{"files":[{"patch":"@@ -123,21 +123,0 @@\n-#ifndef _LP64\n-int AbstractInterpreter::BasicType_as_index(BasicType type) {\n-  int i = 0;\n-  switch (type) {\n-    case T_BOOLEAN: i = 0; break;\n-    case T_CHAR   : i = 1; break;\n-    case T_BYTE   : i = 2; break;\n-    case T_SHORT  : i = 3; break;\n-    case T_INT    : \/\/ fall through\n-    case T_LONG   : \/\/ fall through\n-    case T_VOID   : i = 4; break;\n-    case T_FLOAT  : i = 5; break;  \/\/ have to treat float and double separately for SSE\n-    case T_DOUBLE : i = 6; break;\n-    case T_OBJECT : \/\/ fall through\n-    case T_ARRAY  : i = 7; break;\n-    default       : ShouldNotReachHere();\n-  }\n-  assert(0 <= i && i < AbstractInterpreter::number_of_result_handlers, \"index out of bounds\");\n-  return i;\n-}\n-#else\n@@ -164,1 +143,0 @@\n-#endif \/\/ _LP64\n@@ -176,3 +154,0 @@\n-#ifndef _LP64\n-  const int stub_code = 4;  \/\/ see generate_call_stub\n-#else\n@@ -180,1 +155,0 @@\n-#endif\n","filename":"src\/hotspot\/cpu\/x86\/abstractInterpreter_x86.cpp","additions":0,"deletions":26,"binary":false,"changes":26,"status":"modified"},{"patch":"@@ -2938,1 +2938,0 @@\n-  NOT_LP64(assert(VM_Version::supports_sse2(), \"\"));\n@@ -8074,1 +8073,0 @@\n-  NOT_LP64(assert(VM_Version::supports_sse2(), \"\"));\n","filename":"src\/hotspot\/cpu\/x86\/assembler_x86.cpp","additions":0,"deletions":2,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -48,1 +48,0 @@\n-#ifdef AMD64\n@@ -50,1 +49,1 @@\n-define_pd_global(size_t, NewSizeThreadIncrease,     ScaleForWordSize(4*K));\n+define_pd_global(size_t, NewSizeThreadIncrease,      ScaleForWordSize(4*K));\n@@ -58,11 +57,0 @@\n-#else\n-define_pd_global(intx,  InteriorEntryAlignment,      4);\n-define_pd_global(size_t, NewSizeThreadIncrease,      4*K);\n-define_pd_global(intx,  LoopUnrollLimit,             50);     \/\/ Design center runs on 1.3.1\n-\/\/ InitialCodeCacheSize derived from specjbb2000 run.\n-define_pd_global(size_t, InitialCodeCacheSize,       2304*K); \/\/ Integral multiple of CodeCacheExpansionSize\n-define_pd_global(size_t, CodeCacheExpansionSize,     32*K);\n-\n-\/\/ Ergonomics related flags\n-define_pd_global(uint64_t, MaxRAM,                   4ULL*G);\n-#endif \/\/ AMD64\n","filename":"src\/hotspot\/cpu\/x86\/c2_globals_x86.hpp","additions":1,"deletions":13,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -36,8 +36,0 @@\n-  \/\/ QQQ presumably all 64bit cpu's support this. Seems like the ifdef could\n-  \/\/ simply be left out.\n-#ifndef AMD64\n-  if (!VM_Version::supports_cmov()) {\n-    ConditionalMoveLimit = 0;\n-  }\n-#endif \/\/ AMD64\n-\n","filename":"src\/hotspot\/cpu\/x86\/c2_init_x86.cpp","additions":0,"deletions":8,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -64,2 +64,1 @@\n-  return NOT_LP64(10)    \/\/ movl; jmp\n-         LP64_ONLY(15);  \/\/ movq (1+1+8); jmp (1+4)\n+  return 15;  \/\/ movq (1+1+8); jmp (1+4)\n","filename":"src\/hotspot\/cpu\/x86\/compiledIC_x86.cpp","additions":1,"deletions":2,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -26,2 +26,0 @@\n-#ifdef _LP64\n-\n@@ -57,2 +55,0 @@\n-\n-#endif \/\/ _LP64\n","filename":"src\/hotspot\/cpu\/x86\/compressedKlass_x86.cpp","additions":0,"deletions":4,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2003, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2003, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -31,1 +31,0 @@\n-#ifdef AMD64\n@@ -37,7 +36,0 @@\n-#else\n-  juint* to = (juint*)tohw;\n-  count *= HeapWordSize \/ BytesPerInt;\n-  while (count-- > 0) {\n-    *to++ = value;\n-  }\n-#endif \/\/ AMD64\n@@ -63,1 +55,0 @@\n-#if defined AMD64 || defined _WINDOWS\n@@ -65,40 +56,0 @@\n-#else\n-  \/\/ Includes a zero-count check.\n-  intx temp = 0;\n-  __asm__ volatile(\"        testl   %6,%6         ;\"\n-                   \"        jz      7f            ;\"\n-                   \"        cmpl    %4,%5         ;\"\n-                   \"        leal    -4(%4,%6,4),%3;\"\n-                   \"        jbe     1f            ;\"\n-                   \"        cmpl    %7,%5         ;\"\n-                   \"        jbe     4f            ;\"\n-                   \"1:      cmpl    $32,%6        ;\"\n-                   \"        ja      3f            ;\"\n-                   \"        subl    %4,%1         ;\"\n-                   \"2:      movl    (%4),%3       ;\"\n-                   \"        movl    %7,(%5,%4,1)  ;\"\n-                   \"        addl    $4,%0         ;\"\n-                   \"        subl    $1,%2          ;\"\n-                   \"        jnz     2b            ;\"\n-                   \"        jmp     7f            ;\"\n-                   \"3:      rep;    smovl         ;\"\n-                   \"        jmp     7f            ;\"\n-                   \"4:      cmpl    $32,%2        ;\"\n-                   \"        movl    %7,%0         ;\"\n-                   \"        leal    -4(%5,%6,4),%1;\"\n-                   \"        ja      6f            ;\"\n-                   \"        subl    %4,%1         ;\"\n-                   \"5:      movl    (%4),%3       ;\"\n-                   \"        movl    %7,(%5,%4,1)  ;\"\n-                   \"        subl    $4,%0         ;\"\n-                   \"        subl    $1,%2          ;\"\n-                   \"        jnz     5b            ;\"\n-                   \"        jmp     7f            ;\"\n-                   \"6:      std                   ;\"\n-                   \"        rep;    smovl         ;\"\n-                   \"        cld                   ;\"\n-                   \"7:      nop                    \"\n-                   : \"=S\" (from), \"=D\" (to), \"=c\" (count), \"=r\" (temp)\n-                   : \"0\"  (from), \"1\"  (to), \"2\"  (count), \"3\"  (temp)\n-                   : \"memory\", \"flags\");\n-#endif \/\/ AMD64\n@@ -108,1 +59,0 @@\n-#ifdef AMD64\n@@ -123,24 +73,0 @@\n-#else\n-#if defined _WINDOWS\n-  (void)memcpy(to, from, count * HeapWordSize);\n-#else\n-  \/\/ Includes a zero-count check.\n-  intx temp = 0;\n-  __asm__ volatile(\"        testl   %6,%6       ;\"\n-                   \"        jz      3f          ;\"\n-                   \"        cmpl    $32,%6      ;\"\n-                   \"        ja      2f          ;\"\n-                   \"        subl    %4,%1       ;\"\n-                   \"1:      movl    (%4),%3     ;\"\n-                   \"        movl    %7,(%5,%4,1);\"\n-                   \"        addl    $4,%0       ;\"\n-                   \"        subl    $1,%2        ;\"\n-                   \"        jnz     1b          ;\"\n-                   \"        jmp     3f          ;\"\n-                   \"2:      rep;    smovl       ;\"\n-                   \"3:      nop                  \"\n-                   : \"=S\" (from), \"=D\" (to), \"=c\" (count), \"=r\" (temp)\n-                   : \"0\"  (from), \"1\"  (to), \"2\"  (count), \"3\"  (temp)\n-                   : \"memory\", \"cc\");\n-#endif \/\/ _WINDOWS\n-#endif \/\/ AMD64\n@@ -150,1 +76,0 @@\n-#ifdef AMD64\n@@ -152,4 +77,0 @@\n-#else\n-  \/\/ pd_disjoint_words is word-atomic in this implementation.\n-  pd_disjoint_words(from, to, count);\n-#endif \/\/ AMD64\n@@ -167,1 +88,0 @@\n-#if defined AMD64 || defined _WINDOWS\n@@ -169,74 +89,0 @@\n-#else\n-  \/\/ Includes a zero-count check.\n-  intx temp = 0;\n-  __asm__ volatile(\"        testl   %6,%6          ;\"\n-                   \"        jz      13f            ;\"\n-                   \"        cmpl    %4,%5          ;\"\n-                   \"        leal    -1(%4,%6),%3   ;\"\n-                   \"        jbe     1f             ;\"\n-                   \"        cmpl    %7,%5          ;\"\n-                   \"        jbe     8f             ;\"\n-                   \"1:      cmpl    $3,%6          ;\"\n-                   \"        jbe     6f             ;\"\n-                   \"        movl    %6,%3          ;\"\n-                   \"        movl    $4,%2          ;\"\n-                   \"        subl    %4,%2          ;\"\n-                   \"        andl    $3,%2          ;\"\n-                   \"        jz      2f             ;\"\n-                   \"        subl    %6,%3          ;\"\n-                   \"        rep;    smovb          ;\"\n-                   \"2:      movl    %7,%2          ;\"\n-                   \"        shrl    $2,%2          ;\"\n-                   \"        jz      5f             ;\"\n-                   \"        cmpl    $32,%2         ;\"\n-                   \"        ja      4f             ;\"\n-                   \"        subl    %4,%1          ;\"\n-                   \"3:      movl    (%4),%%edx     ;\"\n-                   \"        movl    %%edx,(%5,%4,1);\"\n-                   \"        addl    $4,%0          ;\"\n-                   \"        subl    $1,%2           ;\"\n-                   \"        jnz     3b             ;\"\n-                   \"        addl    %4,%1          ;\"\n-                   \"        jmp     5f             ;\"\n-                   \"4:      rep;    smovl          ;\"\n-                   \"5:      movl    %7,%2          ;\"\n-                   \"        andl    $3,%2          ;\"\n-                   \"        jz      13f            ;\"\n-                   \"6:      xorl    %7,%3          ;\"\n-                   \"7:      movb    (%4,%7,1),%%dl ;\"\n-                   \"        movb    %%dl,(%5,%7,1) ;\"\n-                   \"        addl    $1,%3          ;\"\n-                   \"        subl    $1,%2           ;\"\n-                   \"        jnz     7b             ;\"\n-                   \"        jmp     13f            ;\"\n-                   \"8:      std                    ;\"\n-                   \"        cmpl    $12,%2         ;\"\n-                   \"        ja      9f             ;\"\n-                   \"        movl    %7,%0          ;\"\n-                   \"        leal    -1(%6,%5),%1   ;\"\n-                   \"        jmp     11f            ;\"\n-                   \"9:      xchgl   %3,%2          ;\"\n-                   \"        movl    %6,%0          ;\"\n-                   \"        addl    $1,%2          ;\"\n-                   \"        leal    -1(%7,%5),%1   ;\"\n-                   \"        andl    $3,%2          ;\"\n-                   \"        jz      10f            ;\"\n-                   \"        subl    %6,%3          ;\"\n-                   \"        rep;    smovb          ;\"\n-                   \"10:     movl    %7,%2          ;\"\n-                   \"        subl    $3,%0          ;\"\n-                   \"        shrl    $2,%2          ;\"\n-                   \"        subl    $3,%1          ;\"\n-                   \"        rep;    smovl          ;\"\n-                   \"        andl    $3,%3          ;\"\n-                   \"        jz      12f            ;\"\n-                   \"        movl    %7,%2          ;\"\n-                   \"        addl    $3,%0          ;\"\n-                   \"        addl    $3,%1          ;\"\n-                   \"11:     rep;    smovb          ;\"\n-                   \"12:     cld                    ;\"\n-                   \"13:     nop                    ;\"\n-                   : \"=S\" (from), \"=D\" (to), \"=c\" (count), \"=r\" (temp)\n-                   : \"0\"  (from), \"1\"  (to), \"2\"  (count), \"3\"  (temp)\n-                   : \"memory\", \"flags\", \"%edx\");\n-#endif \/\/ AMD64\n@@ -256,1 +102,0 @@\n-#ifdef AMD64\n@@ -258,5 +103,0 @@\n-#else\n-  assert(HeapWordSize == BytesPerInt, \"heapwords and jints must be the same size\");\n-  \/\/ pd_conjoint_words is word-atomic in this implementation.\n-  pd_conjoint_words((const HeapWord*)from, (HeapWord*)to, count);\n-#endif \/\/ AMD64\n@@ -266,1 +106,0 @@\n-#ifdef AMD64\n@@ -268,20 +107,0 @@\n-#else\n-  \/\/ Guarantee use of fild\/fistp or xmm regs via some asm code, because compilers won't.\n-  if (from > to) {\n-    while (count-- > 0) {\n-      __asm__ volatile(\"fildll (%0); fistpll (%1)\"\n-                       :\n-                       : \"r\" (from), \"r\" (to)\n-                       : \"memory\" );\n-      ++from;\n-      ++to;\n-    }\n-  } else {\n-    while (count-- > 0) {\n-      __asm__ volatile(\"fildll (%0,%2,8); fistpll (%1,%2,8)\"\n-                       :\n-                       : \"r\" (from), \"r\" (to), \"r\" (count)\n-                       : \"memory\" );\n-    }\n-  }\n-#endif \/\/ AMD64\n@@ -291,1 +110,0 @@\n-#ifdef AMD64\n@@ -294,5 +112,0 @@\n-#else\n-  assert(HeapWordSize == BytesPerOop, \"heapwords and oops must be the same size\");\n-  \/\/ pd_conjoint_words is word-atomic in this implementation.\n-  pd_conjoint_words((const HeapWord*)from, (HeapWord*)to, count);\n-#endif \/\/ AMD64\n@@ -310,1 +123,0 @@\n-#ifdef AMD64\n@@ -312,3 +124,0 @@\n-#else\n-  pd_conjoint_jints_atomic((const jint*)from, (jint*)to, count);\n-#endif \/\/ AMD64\n@@ -318,1 +127,0 @@\n-#ifdef AMD64\n@@ -320,3 +128,0 @@\n-#else\n-  pd_conjoint_jlongs_atomic((const jlong*)from, (jlong*)to, count);\n-#endif \/\/ AMD64\n@@ -326,1 +131,0 @@\n-#ifdef AMD64\n@@ -329,3 +133,0 @@\n-#else\n-  pd_conjoint_oops_atomic((const oop*)from, (oop*)to, count);\n-#endif \/\/ AMD64\n","filename":"src\/hotspot\/cpu\/x86\/copy_x86.hpp","additions":1,"deletions":200,"binary":false,"changes":201,"status":"modified"},{"patch":"@@ -539,2 +539,0 @@\n-    \/\/ QQQ seems like this code is equivalent on the two platforms\n-#ifdef AMD64\n@@ -544,3 +542,0 @@\n-#else\n-      tos_addr += 2;\n-#endif \/\/ AMD64\n@@ -572,13 +567,1 @@\n-    case T_FLOAT   : {\n-#ifdef AMD64\n-        value_result->f = *(jfloat*)tos_addr;\n-#else\n-      if (method->is_native()) {\n-        jdouble d = *(jdouble*)tos_addr;  \/\/ Result was in ST0 so need to convert to jfloat\n-        value_result->f = (jfloat)d;\n-      } else {\n-        value_result->f = *(jfloat*)tos_addr;\n-      }\n-#endif \/\/ AMD64\n-      break;\n-    }\n+    case T_FLOAT   : value_result->f = *(jfloat*)tos_addr; break;\n@@ -614,1 +597,0 @@\n-#ifdef AMD64\n@@ -622,1 +604,0 @@\n-#endif \/\/ AMD64\n","filename":"src\/hotspot\/cpu\/x86\/frame_x86.cpp","additions":1,"deletions":20,"binary":false,"changes":21,"status":"modified"},{"patch":"@@ -83,2 +83,1 @@\n-#ifdef AMD64\n-#ifdef _WIN64\n+#ifdef _WINDOWS\n@@ -94,4 +93,1 @@\n-#endif \/\/ _WIN64\n-#else\n-    entry_frame_call_wrapper_offset                  =  2,\n-#endif \/\/ AMD64\n+#endif \/\/ _WINDOWS\n","filename":"src\/hotspot\/cpu\/x86\/frame_x86.hpp","additions":2,"deletions":6,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -486,1 +486,0 @@\n-#ifdef AMD64\n@@ -494,1 +493,0 @@\n-#endif \/\/ AMD64\n","filename":"src\/hotspot\/cpu\/x86\/frame_x86.inline.hpp","additions":0,"deletions":2,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -521,2 +521,2 @@\n-  LP64_ONLY(assert(Rsub_klass != r14, \"r14 holds locals\");)\n-  LP64_ONLY(assert(Rsub_klass != r13, \"r13 holds bcp\");)\n+  assert(Rsub_klass != r14, \"r14 holds locals\");\n+  assert(Rsub_klass != r13, \"r13 holds bcp\");\n","filename":"src\/hotspot\/cpu\/x86\/interp_masm_x86.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1998, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1998, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -45,9 +45,0 @@\n-private:\n-\n-#ifndef AMD64\n-  \/\/ 32bit Helper routines.\n-  static inline void    put_int2r(jint *from, intptr_t *to)           { *(jint *)(to++) = from[1];\n-                                                                        *(jint *)(to  ) = from[0]; }\n-  static inline void    put_int2r(jint *from, intptr_t *to, int& pos) { put_int2r(from, to + pos); pos += 2; }\n-#endif \/\/ AMD64\n-\n@@ -60,1 +51,0 @@\n-#ifdef AMD64\n@@ -76,7 +66,0 @@\n-#else\n-  \/\/ Longs are stored in big-endian word format in two JavaCallArgument slots at *to.\n-  \/\/ The high half is in *to and the low half in *(to+1).\n-  static inline void    put_long(jlong  from, intptr_t *to)           { put_int2r((jint *)&from, to); }\n-  static inline void    put_long(jlong  from, intptr_t *to, int& pos) { put_int2r((jint *)&from, to, pos); }\n-  static inline void    put_long(jlong *from, intptr_t *to, int& pos) { put_int2r((jint *) from, to, pos); }\n-#endif \/\/ AMD64\n@@ -94,1 +77,0 @@\n-#ifdef AMD64\n@@ -111,8 +93,0 @@\n-#else\n-#define _JNI_SLOT_OFFSET 0\n-  \/\/ Doubles are stored in big-endian word format in two JavaCallArgument slots at *to.\n-  \/\/ The high half is in *to and the low half in *(to+1).\n-  static inline void    put_double(jdouble  from, intptr_t *to)           { put_int2r((jint *)&from, to); }\n-  static inline void    put_double(jdouble  from, intptr_t *to, int& pos) { put_int2r((jint *)&from, to, pos); }\n-  static inline void    put_double(jdouble *from, intptr_t *to, int& pos) { put_int2r((jint *) from, to, pos); }\n-#endif \/\/ AMD64\n","filename":"src\/hotspot\/cpu\/x86\/jniTypes_x86.hpp","additions":1,"deletions":27,"binary":false,"changes":28,"status":"modified"},{"patch":"@@ -80,1 +80,0 @@\n-#ifdef _LP64\n@@ -85,3 +84,0 @@\n-#else\n-    JVMCI_ERROR(\"compressed oop on 32bit\");\n-#endif\n@@ -99,1 +95,0 @@\n-#ifdef _LP64\n@@ -103,3 +98,0 @@\n-#else\n-    JVMCI_ERROR(\"compressed Klass* on 32bit\");\n-#endif\n","filename":"src\/hotspot\/cpu\/x86\/jvmciCodeInstaller_x86.cpp","additions":0,"deletions":8,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -564,1 +564,0 @@\n-#ifdef AMD64\n@@ -573,3 +572,0 @@\n-#else\n-      ls.print(\"%3s=\" PTR_FORMAT, r->name(), saved_regs[((saved_regs_count - 1) - i)]);\n-#endif\n","filename":"src\/hotspot\/cpu\/x86\/methodHandles_x86.cpp","additions":0,"deletions":4,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -39,1 +39,0 @@\n-#ifdef AMD64\n@@ -79,7 +78,0 @@\n-#else\n-  if (verify_only) {\n-    guarantee(*pd_address_in_code() == x, \"instructions must match\");\n-  } else {\n-    *pd_address_in_code() = x;\n-  }\n-#endif \/\/ AMD64\n@@ -153,1 +145,0 @@\n-#ifdef AMD64\n@@ -160,3 +151,0 @@\n-#else\n-  assert(which == Assembler::disp32_operand || which == Assembler::imm_operand, \"format unpacks ok\");\n-#endif \/\/ AMD64\n@@ -168,1 +156,0 @@\n-#ifdef AMD64\n@@ -185,1 +172,0 @@\n-#endif \/\/ AMD64\n","filename":"src\/hotspot\/cpu\/x86\/relocInfo_x86.cpp","additions":0,"deletions":14,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -35,3 +35,0 @@\n-#ifndef AMD64\n-    format_width       =  1\n-#else\n@@ -40,1 +37,0 @@\n-#endif\n","filename":"src\/hotspot\/cpu\/x86\/relocInfo_x86.hpp","additions":1,"deletions":5,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -1263,1 +1263,0 @@\n-#ifdef _LP64\n@@ -1269,1 +1268,0 @@\n-#endif\n","filename":"src\/hotspot\/cpu\/x86\/vm_version_x86.cpp","additions":0,"deletions":2,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -25,1 +25,1 @@\n-\/\/ X86 Common Architecture Description File\n+\/\/ X86 AMD64 Architecture Description File\n@@ -62,0 +62,160 @@\n+\/\/ General Registers\n+\/\/ R8-R15 must be encoded with REX.  (RSP, RBP, RSI, RDI need REX when\n+\/\/ used as byte registers)\n+\n+\/\/ Previously set RBX, RSI, and RDI as save-on-entry for java code\n+\/\/ Turn off SOE in java-code due to frequent use of uncommon-traps.\n+\/\/ Now that allocator is better, turn on RSI and RDI as SOE registers.\n+\n+reg_def RAX  (SOC, SOC, Op_RegI,  0, rax->as_VMReg());\n+reg_def RAX_H(SOC, SOC, Op_RegI,  0, rax->as_VMReg()->next());\n+\n+reg_def RCX  (SOC, SOC, Op_RegI,  1, rcx->as_VMReg());\n+reg_def RCX_H(SOC, SOC, Op_RegI,  1, rcx->as_VMReg()->next());\n+\n+reg_def RDX  (SOC, SOC, Op_RegI,  2, rdx->as_VMReg());\n+reg_def RDX_H(SOC, SOC, Op_RegI,  2, rdx->as_VMReg()->next());\n+\n+reg_def RBX  (SOC, SOE, Op_RegI,  3, rbx->as_VMReg());\n+reg_def RBX_H(SOC, SOE, Op_RegI,  3, rbx->as_VMReg()->next());\n+\n+reg_def RSP  (NS,  NS,  Op_RegI,  4, rsp->as_VMReg());\n+reg_def RSP_H(NS,  NS,  Op_RegI,  4, rsp->as_VMReg()->next());\n+\n+\/\/ now that adapter frames are gone RBP is always saved and restored by the prolog\/epilog code\n+reg_def RBP  (NS, SOE, Op_RegI,  5, rbp->as_VMReg());\n+reg_def RBP_H(NS, SOE, Op_RegI,  5, rbp->as_VMReg()->next());\n+\n+#ifdef _WIN64\n+\n+reg_def RSI  (SOC, SOE, Op_RegI,  6, rsi->as_VMReg());\n+reg_def RSI_H(SOC, SOE, Op_RegI,  6, rsi->as_VMReg()->next());\n+\n+reg_def RDI  (SOC, SOE, Op_RegI,  7, rdi->as_VMReg());\n+reg_def RDI_H(SOC, SOE, Op_RegI,  7, rdi->as_VMReg()->next());\n+\n+#else\n+\n+reg_def RSI  (SOC, SOC, Op_RegI,  6, rsi->as_VMReg());\n+reg_def RSI_H(SOC, SOC, Op_RegI,  6, rsi->as_VMReg()->next());\n+\n+reg_def RDI  (SOC, SOC, Op_RegI,  7, rdi->as_VMReg());\n+reg_def RDI_H(SOC, SOC, Op_RegI,  7, rdi->as_VMReg()->next());\n+\n+#endif\n+\n+reg_def R8   (SOC, SOC, Op_RegI,  8, r8->as_VMReg());\n+reg_def R8_H (SOC, SOC, Op_RegI,  8, r8->as_VMReg()->next());\n+\n+reg_def R9   (SOC, SOC, Op_RegI,  9, r9->as_VMReg());\n+reg_def R9_H (SOC, SOC, Op_RegI,  9, r9->as_VMReg()->next());\n+\n+reg_def R10  (SOC, SOC, Op_RegI, 10, r10->as_VMReg());\n+reg_def R10_H(SOC, SOC, Op_RegI, 10, r10->as_VMReg()->next());\n+\n+reg_def R11  (SOC, SOC, Op_RegI, 11, r11->as_VMReg());\n+reg_def R11_H(SOC, SOC, Op_RegI, 11, r11->as_VMReg()->next());\n+\n+reg_def R12  (SOC, SOE, Op_RegI, 12, r12->as_VMReg());\n+reg_def R12_H(SOC, SOE, Op_RegI, 12, r12->as_VMReg()->next());\n+\n+reg_def R13  (SOC, SOE, Op_RegI, 13, r13->as_VMReg());\n+reg_def R13_H(SOC, SOE, Op_RegI, 13, r13->as_VMReg()->next());\n+\n+reg_def R14  (SOC, SOE, Op_RegI, 14, r14->as_VMReg());\n+reg_def R14_H(SOC, SOE, Op_RegI, 14, r14->as_VMReg()->next());\n+\n+reg_def R15  (SOC, SOE, Op_RegI, 15, r15->as_VMReg());\n+reg_def R15_H(SOC, SOE, Op_RegI, 15, r15->as_VMReg()->next());\n+\n+reg_def R16  (SOC, SOC, Op_RegI, 16, r16->as_VMReg());\n+reg_def R16_H(SOC, SOC, Op_RegI, 16, r16->as_VMReg()->next());\n+\n+reg_def R17  (SOC, SOC, Op_RegI, 17, r17->as_VMReg());\n+reg_def R17_H(SOC, SOC, Op_RegI, 17, r17->as_VMReg()->next());\n+\n+reg_def R18  (SOC, SOC, Op_RegI, 18, r18->as_VMReg());\n+reg_def R18_H(SOC, SOC, Op_RegI, 18, r18->as_VMReg()->next());\n+\n+reg_def R19  (SOC, SOC, Op_RegI, 19, r19->as_VMReg());\n+reg_def R19_H(SOC, SOC, Op_RegI, 19, r19->as_VMReg()->next());\n+\n+reg_def R20  (SOC, SOC, Op_RegI, 20, r20->as_VMReg());\n+reg_def R20_H(SOC, SOC, Op_RegI, 20, r20->as_VMReg()->next());\n+\n+reg_def R21  (SOC, SOC, Op_RegI, 21, r21->as_VMReg());\n+reg_def R21_H(SOC, SOC, Op_RegI, 21, r21->as_VMReg()->next());\n+\n+reg_def R22  (SOC, SOC, Op_RegI, 22, r22->as_VMReg());\n+reg_def R22_H(SOC, SOC, Op_RegI, 22, r22->as_VMReg()->next());\n+\n+reg_def R23  (SOC, SOC, Op_RegI, 23, r23->as_VMReg());\n+reg_def R23_H(SOC, SOC, Op_RegI, 23, r23->as_VMReg()->next());\n+\n+reg_def R24  (SOC, SOC, Op_RegI, 24, r24->as_VMReg());\n+reg_def R24_H(SOC, SOC, Op_RegI, 24, r24->as_VMReg()->next());\n+\n+reg_def R25  (SOC, SOC, Op_RegI, 25, r25->as_VMReg());\n+reg_def R25_H(SOC, SOC, Op_RegI, 25, r25->as_VMReg()->next());\n+\n+reg_def R26  (SOC, SOC, Op_RegI, 26, r26->as_VMReg());\n+reg_def R26_H(SOC, SOC, Op_RegI, 26, r26->as_VMReg()->next());\n+\n+reg_def R27  (SOC, SOC, Op_RegI, 27, r27->as_VMReg());\n+reg_def R27_H(SOC, SOC, Op_RegI, 27, r27->as_VMReg()->next());\n+\n+reg_def R28  (SOC, SOC, Op_RegI, 28, r28->as_VMReg());\n+reg_def R28_H(SOC, SOC, Op_RegI, 28, r28->as_VMReg()->next());\n+\n+reg_def R29  (SOC, SOC, Op_RegI, 29, r29->as_VMReg());\n+reg_def R29_H(SOC, SOC, Op_RegI, 29, r29->as_VMReg()->next());\n+\n+reg_def R30  (SOC, SOC, Op_RegI, 30, r30->as_VMReg());\n+reg_def R30_H(SOC, SOC, Op_RegI, 30, r30->as_VMReg()->next());\n+\n+reg_def R31  (SOC, SOC, Op_RegI, 31, r31->as_VMReg());\n+reg_def R31_H(SOC, SOC, Op_RegI, 31, r31->as_VMReg()->next());\n+\n+\/\/ Floating Point Registers\n+\n+\/\/ Specify priority of register selection within phases of register\n+\/\/ allocation.  Highest priority is first.  A useful heuristic is to\n+\/\/ give registers a low priority when they are required by machine\n+\/\/ instructions, like EAX and EDX on I486, and choose no-save registers\n+\/\/ before save-on-call, & save-on-call before save-on-entry.  Registers\n+\/\/ which participate in fixed calling sequences should come last.\n+\/\/ Registers which are used as pairs must fall on an even boundary.\n+\n+alloc_class chunk0(R10,         R10_H,\n+                   R11,         R11_H,\n+                   R8,          R8_H,\n+                   R9,          R9_H,\n+                   R12,         R12_H,\n+                   RCX,         RCX_H,\n+                   RBX,         RBX_H,\n+                   RDI,         RDI_H,\n+                   RDX,         RDX_H,\n+                   RSI,         RSI_H,\n+                   RAX,         RAX_H,\n+                   RBP,         RBP_H,\n+                   R13,         R13_H,\n+                   R14,         R14_H,\n+                   R15,         R15_H,\n+                   R16,         R16_H,\n+                   R17,         R17_H,\n+                   R18,         R18_H,\n+                   R19,         R19_H,\n+                   R20,         R20_H,\n+                   R21,         R21_H,\n+                   R22,         R22_H,\n+                   R23,         R23_H,\n+                   R24,         R24_H,\n+                   R25,         R25_H,\n+                   R26,         R26_H,\n+                   R27,         R27_H,\n+                   R28,         R28_H,\n+                   R29,         R29_H,\n+                   R30,         R30_H,\n+                   R31,         R31_H,\n+                   RSP,         RSP_H);\n+\n@@ -646,0 +806,192 @@\n+\/\/----------Architecture Description Register Classes--------------------------\n+\/\/ Several register classes are automatically defined based upon information in\n+\/\/ this architecture description.\n+\/\/ 1) reg_class inline_cache_reg           ( \/* as def'd in frame section *\/ )\n+\/\/ 2) reg_class stack_slots( \/* one chunk of stack-based \"registers\" *\/ )\n+\/\/\n+\n+\/\/ Empty register class.\n+reg_class no_reg();\n+\n+\/\/ Class for all pointer\/long registers including APX extended GPRs.\n+reg_class all_reg(RAX, RAX_H,\n+                  RDX, RDX_H,\n+                  RBP, RBP_H,\n+                  RDI, RDI_H,\n+                  RSI, RSI_H,\n+                  RCX, RCX_H,\n+                  RBX, RBX_H,\n+                  RSP, RSP_H,\n+                  R8,  R8_H,\n+                  R9,  R9_H,\n+                  R10, R10_H,\n+                  R11, R11_H,\n+                  R12, R12_H,\n+                  R13, R13_H,\n+                  R14, R14_H,\n+                  R15, R15_H,\n+                  R16, R16_H,\n+                  R17, R17_H,\n+                  R18, R18_H,\n+                  R19, R19_H,\n+                  R20, R20_H,\n+                  R21, R21_H,\n+                  R22, R22_H,\n+                  R23, R23_H,\n+                  R24, R24_H,\n+                  R25, R25_H,\n+                  R26, R26_H,\n+                  R27, R27_H,\n+                  R28, R28_H,\n+                  R29, R29_H,\n+                  R30, R30_H,\n+                  R31, R31_H);\n+\n+\/\/ Class for all int registers including APX extended GPRs.\n+reg_class all_int_reg(RAX\n+                      RDX,\n+                      RBP,\n+                      RDI,\n+                      RSI,\n+                      RCX,\n+                      RBX,\n+                      R8,\n+                      R9,\n+                      R10,\n+                      R11,\n+                      R12,\n+                      R13,\n+                      R14,\n+                      R16,\n+                      R17,\n+                      R18,\n+                      R19,\n+                      R20,\n+                      R21,\n+                      R22,\n+                      R23,\n+                      R24,\n+                      R25,\n+                      R26,\n+                      R27,\n+                      R28,\n+                      R29,\n+                      R30,\n+                      R31);\n+\n+\/\/ Class for all pointer registers\n+reg_class any_reg %{\n+  return _ANY_REG_mask;\n+%}\n+\n+\/\/ Class for all pointer registers (excluding RSP)\n+reg_class ptr_reg %{\n+  return _PTR_REG_mask;\n+%}\n+\n+\/\/ Class for all pointer registers (excluding RSP and RBP)\n+reg_class ptr_reg_no_rbp %{\n+  return _PTR_REG_NO_RBP_mask;\n+%}\n+\n+\/\/ Class for all pointer registers (excluding RAX and RSP)\n+reg_class ptr_no_rax_reg %{\n+  return _PTR_NO_RAX_REG_mask;\n+%}\n+\n+\/\/ Class for all pointer registers (excluding RAX, RBX, and RSP)\n+reg_class ptr_no_rax_rbx_reg %{\n+  return _PTR_NO_RAX_RBX_REG_mask;\n+%}\n+\n+\/\/ Class for all long registers (excluding RSP)\n+reg_class long_reg %{\n+  return _LONG_REG_mask;\n+%}\n+\n+\/\/ Class for all long registers (excluding RAX, RDX and RSP)\n+reg_class long_no_rax_rdx_reg %{\n+  return _LONG_NO_RAX_RDX_REG_mask;\n+%}\n+\n+\/\/ Class for all long registers (excluding RCX and RSP)\n+reg_class long_no_rcx_reg %{\n+  return _LONG_NO_RCX_REG_mask;\n+%}\n+\n+\/\/ Class for all long registers (excluding RBP and R13)\n+reg_class long_no_rbp_r13_reg %{\n+  return _LONG_NO_RBP_R13_REG_mask;\n+%}\n+\n+\/\/ Class for all int registers (excluding RSP)\n+reg_class int_reg %{\n+  return _INT_REG_mask;\n+%}\n+\n+\/\/ Class for all int registers (excluding RAX, RDX, and RSP)\n+reg_class int_no_rax_rdx_reg %{\n+  return _INT_NO_RAX_RDX_REG_mask;\n+%}\n+\n+\/\/ Class for all int registers (excluding RCX and RSP)\n+reg_class int_no_rcx_reg %{\n+  return _INT_NO_RCX_REG_mask;\n+%}\n+\n+\/\/ Class for all int registers (excluding RBP and R13)\n+reg_class int_no_rbp_r13_reg %{\n+  return _INT_NO_RBP_R13_REG_mask;\n+%}\n+\n+\/\/ Singleton class for RAX pointer register\n+reg_class ptr_rax_reg(RAX, RAX_H);\n+\n+\/\/ Singleton class for RBX pointer register\n+reg_class ptr_rbx_reg(RBX, RBX_H);\n+\n+\/\/ Singleton class for RSI pointer register\n+reg_class ptr_rsi_reg(RSI, RSI_H);\n+\n+\/\/ Singleton class for RBP pointer register\n+reg_class ptr_rbp_reg(RBP, RBP_H);\n+\n+\/\/ Singleton class for RDI pointer register\n+reg_class ptr_rdi_reg(RDI, RDI_H);\n+\n+\/\/ Singleton class for stack pointer\n+reg_class ptr_rsp_reg(RSP, RSP_H);\n+\n+\/\/ Singleton class for TLS pointer\n+reg_class ptr_r15_reg(R15, R15_H);\n+\n+\/\/ Singleton class for RAX long register\n+reg_class long_rax_reg(RAX, RAX_H);\n+\n+\/\/ Singleton class for RCX long register\n+reg_class long_rcx_reg(RCX, RCX_H);\n+\n+\/\/ Singleton class for RDX long register\n+reg_class long_rdx_reg(RDX, RDX_H);\n+\n+\/\/ Singleton class for R11 long register\n+reg_class long_r11_reg(R11, R11_H);\n+\n+\/\/ Singleton class for RAX int register\n+reg_class int_rax_reg(RAX);\n+\n+\/\/ Singleton class for RBX int register\n+reg_class int_rbx_reg(RBX);\n+\n+\/\/ Singleton class for RCX int register\n+reg_class int_rcx_reg(RCX);\n+\n+\/\/ Singleton class for RDX int register\n+reg_class int_rdx_reg(RDX);\n+\n+\/\/ Singleton class for RDI int register\n+reg_class int_rdi_reg(RDI);\n+\n+\/\/ Singleton class for instruction pointer\n+\/\/ reg_class ip_reg(RIP);\n+\n@@ -706,1 +1058,0 @@\n-\n@@ -1096,0 +1447,1 @@\n+\n@@ -1104,6 +1456,0 @@\n-\/\/ Header information of the source block.\n-\/\/ Method declarations\/definitions which are used outside\n-\/\/ the ad-scope can conveniently be defined here.\n-\/\/\n-\/\/ To keep related declarations\/definitions\/uses close together,\n-\/\/ we switch between source %{ }% and source_hpp %{ }% freely as needed.\n@@ -1111,1 +1457,1 @@\n-#include \"runtime\/vm_version.hpp\"\n+#include \"peephole_x86_64.hpp\"\n@@ -1113,1 +1459,1 @@\n-class NativeJump;\n+bool castLL_is_imm32(const Node* n);\n@@ -1115,1 +1461,1 @@\n-class CallStubImpl {\n+%}\n@@ -1117,3 +1463,1 @@\n-  \/\/--------------------------------------------------------------\n-  \/\/---<  Used for optimization in Compile::shorten_branches  >---\n-  \/\/--------------------------------------------------------------\n+source %{\n@@ -1121,5 +1465,5 @@\n- public:\n-  \/\/ Size of call trampoline stub.\n-  static uint size_call_trampoline() {\n-    return 0; \/\/ no call trampolines on this platform\n-  }\n+bool castLL_is_imm32(const Node* n) {\n+  assert(n->is_CastLL(), \"must be a CastLL\");\n+  const TypeLong* t = n->bottom_type()->is_long();\n+  return (t->_lo == min_jlong || Assembler::is_simm32(t->_lo)) && (t->_hi == max_jlong || Assembler::is_simm32(t->_hi));\n+}\n@@ -1127,5 +1471,1 @@\n-  \/\/ number of relocations needed by a call trampoline stub\n-  static uint reloc_call_trampoline() {\n-    return 0; \/\/ no call trampolines on this platform\n-  }\n-};\n+%}\n@@ -1133,1 +1473,2 @@\n-class HandlerImpl {\n+\/\/ Register masks\n+source_hpp %{\n@@ -1135,1 +1476,14 @@\n- public:\n+extern RegMask _ANY_REG_mask;\n+extern RegMask _PTR_REG_mask;\n+extern RegMask _PTR_REG_NO_RBP_mask;\n+extern RegMask _PTR_NO_RAX_REG_mask;\n+extern RegMask _PTR_NO_RAX_RBX_REG_mask;\n+extern RegMask _LONG_REG_mask;\n+extern RegMask _LONG_NO_RAX_RDX_REG_mask;\n+extern RegMask _LONG_NO_RCX_REG_mask;\n+extern RegMask _LONG_NO_RBP_R13_REG_mask;\n+extern RegMask _INT_REG_mask;\n+extern RegMask _INT_NO_RAX_RDX_REG_mask;\n+extern RegMask _INT_NO_RCX_REG_mask;\n+extern RegMask _INT_NO_RBP_R13_REG_mask;\n+extern RegMask _FLOAT_REG_mask;\n@@ -1137,2 +1491,3 @@\n-  static int emit_exception_handler(C2_MacroAssembler *masm);\n-  static int emit_deopt_handler(C2_MacroAssembler* masm);\n+extern RegMask _STACK_OR_PTR_REG_mask;\n+extern RegMask _STACK_OR_LONG_REG_mask;\n+extern RegMask _STACK_OR_INT_REG_mask;\n@@ -1140,7 +1495,48 @@\n-  static uint size_exception_handler() {\n-    \/\/ NativeCall instruction size is the same as NativeJump.\n-    \/\/ exception handler starts out as jump and can be patched to\n-    \/\/ a call be deoptimization.  (4932387)\n-    \/\/ Note that this value is also credited (in output.cpp) to\n-    \/\/ the size of the code section.\n-    return NativeJump::instruction_size;\n+inline const RegMask& STACK_OR_PTR_REG_mask()  { return _STACK_OR_PTR_REG_mask;  }\n+inline const RegMask& STACK_OR_LONG_REG_mask() { return _STACK_OR_LONG_REG_mask; }\n+inline const RegMask& STACK_OR_INT_REG_mask()  { return _STACK_OR_INT_REG_mask;  }\n+\n+%}\n+\n+source %{\n+#define   RELOC_IMM64    Assembler::imm_operand\n+#define   RELOC_DISP32   Assembler::disp32_operand\n+\n+#define __ masm->\n+\n+RegMask _ANY_REG_mask;\n+RegMask _PTR_REG_mask;\n+RegMask _PTR_REG_NO_RBP_mask;\n+RegMask _PTR_NO_RAX_REG_mask;\n+RegMask _PTR_NO_RAX_RBX_REG_mask;\n+RegMask _LONG_REG_mask;\n+RegMask _LONG_NO_RAX_RDX_REG_mask;\n+RegMask _LONG_NO_RCX_REG_mask;\n+RegMask _LONG_NO_RBP_R13_REG_mask;\n+RegMask _INT_REG_mask;\n+RegMask _INT_NO_RAX_RDX_REG_mask;\n+RegMask _INT_NO_RCX_REG_mask;\n+RegMask _INT_NO_RBP_R13_REG_mask;\n+RegMask _FLOAT_REG_mask;\n+RegMask _STACK_OR_PTR_REG_mask;\n+RegMask _STACK_OR_LONG_REG_mask;\n+RegMask _STACK_OR_INT_REG_mask;\n+\n+static bool need_r12_heapbase() {\n+  return UseCompressedOops;\n+}\n+\n+void reg_mask_init() {\n+  constexpr Register egprs[] = {r16, r17, r18, r19, r20, r21, r22, r23, r24, r25, r26, r27, r28, r29, r30, r31};\n+\n+  \/\/ _ALL_REG_mask is generated by adlc from the all_reg register class below.\n+  \/\/ We derive a number of subsets from it.\n+  _ANY_REG_mask.assignFrom(_ALL_REG_mask);\n+\n+  if (PreserveFramePointer) {\n+    _ANY_REG_mask.remove(OptoReg::as_OptoReg(rbp->as_VMReg()));\n+    _ANY_REG_mask.remove(OptoReg::as_OptoReg(rbp->as_VMReg()->next()));\n+  }\n+  if (need_r12_heapbase()) {\n+    _ANY_REG_mask.remove(OptoReg::as_OptoReg(r12->as_VMReg()));\n+    _ANY_REG_mask.remove(OptoReg::as_OptoReg(r12->as_VMReg()->next()));\n@@ -1149,3 +1545,10 @@\n-  static uint size_deopt_handler() {\n-    \/\/ three 5 byte instructions plus one move for unreachable address.\n-    return 15+3;\n+  _PTR_REG_mask.assignFrom(_ANY_REG_mask);\n+  _PTR_REG_mask.remove(OptoReg::as_OptoReg(rsp->as_VMReg()));\n+  _PTR_REG_mask.remove(OptoReg::as_OptoReg(rsp->as_VMReg()->next()));\n+  _PTR_REG_mask.remove(OptoReg::as_OptoReg(r15->as_VMReg()));\n+  _PTR_REG_mask.remove(OptoReg::as_OptoReg(r15->as_VMReg()->next()));\n+  if (!UseAPX) {\n+    for (uint i = 0; i < sizeof(egprs)\/sizeof(Register); i++) {\n+      _PTR_REG_mask.remove(OptoReg::as_OptoReg(egprs[i]->as_VMReg()));\n+      _PTR_REG_mask.remove(OptoReg::as_OptoReg(egprs[i]->as_VMReg()->next()));\n+    }\n@@ -1153,1 +1556,0 @@\n-};\n@@ -1155,7 +1557,2 @@\n-inline Assembler::AvxVectorLen vector_length_encoding(int bytes) {\n-  switch(bytes) {\n-    case  4: \/\/ fall-through\n-    case  8: \/\/ fall-through\n-    case 16: return Assembler::AVX_128bit;\n-    case 32: return Assembler::AVX_256bit;\n-    case 64: return Assembler::AVX_512bit;\n+  _STACK_OR_PTR_REG_mask.assignFrom(_PTR_REG_mask);\n+  _STACK_OR_PTR_REG_mask.or_with(STACK_OR_STACK_SLOTS_mask());\n@@ -1163,3 +1560,37 @@\n-    default: {\n-      ShouldNotReachHere();\n-      return Assembler::AVX_NoVec;\n+  _PTR_REG_NO_RBP_mask.assignFrom(_PTR_REG_mask);\n+  _PTR_REG_NO_RBP_mask.remove(OptoReg::as_OptoReg(rbp->as_VMReg()));\n+  _PTR_REG_NO_RBP_mask.remove(OptoReg::as_OptoReg(rbp->as_VMReg()->next()));\n+\n+  _PTR_NO_RAX_REG_mask.assignFrom(_PTR_REG_mask);\n+  _PTR_NO_RAX_REG_mask.remove(OptoReg::as_OptoReg(rax->as_VMReg()));\n+  _PTR_NO_RAX_REG_mask.remove(OptoReg::as_OptoReg(rax->as_VMReg()->next()));\n+\n+  _PTR_NO_RAX_RBX_REG_mask.assignFrom(_PTR_NO_RAX_REG_mask);\n+  _PTR_NO_RAX_RBX_REG_mask.remove(OptoReg::as_OptoReg(rbx->as_VMReg()));\n+  _PTR_NO_RAX_RBX_REG_mask.remove(OptoReg::as_OptoReg(rbx->as_VMReg()->next()));\n+\n+\n+  _LONG_REG_mask.assignFrom(_PTR_REG_mask);\n+  _STACK_OR_LONG_REG_mask.assignFrom(_LONG_REG_mask);\n+  _STACK_OR_LONG_REG_mask.or_with(STACK_OR_STACK_SLOTS_mask());\n+\n+  _LONG_NO_RAX_RDX_REG_mask.assignFrom(_LONG_REG_mask);\n+  _LONG_NO_RAX_RDX_REG_mask.remove(OptoReg::as_OptoReg(rax->as_VMReg()));\n+  _LONG_NO_RAX_RDX_REG_mask.remove(OptoReg::as_OptoReg(rax->as_VMReg()->next()));\n+  _LONG_NO_RAX_RDX_REG_mask.remove(OptoReg::as_OptoReg(rdx->as_VMReg()));\n+  _LONG_NO_RAX_RDX_REG_mask.remove(OptoReg::as_OptoReg(rdx->as_VMReg()->next()));\n+\n+  _LONG_NO_RCX_REG_mask.assignFrom(_LONG_REG_mask);\n+  _LONG_NO_RCX_REG_mask.remove(OptoReg::as_OptoReg(rcx->as_VMReg()));\n+  _LONG_NO_RCX_REG_mask.remove(OptoReg::as_OptoReg(rcx->as_VMReg()->next()));\n+\n+  _LONG_NO_RBP_R13_REG_mask.assignFrom(_LONG_REG_mask);\n+  _LONG_NO_RBP_R13_REG_mask.remove(OptoReg::as_OptoReg(rbp->as_VMReg()));\n+  _LONG_NO_RBP_R13_REG_mask.remove(OptoReg::as_OptoReg(rbp->as_VMReg()->next()));\n+  _LONG_NO_RBP_R13_REG_mask.remove(OptoReg::as_OptoReg(r13->as_VMReg()));\n+  _LONG_NO_RBP_R13_REG_mask.remove(OptoReg::as_OptoReg(r13->as_VMReg()->next()));\n+\n+  _INT_REG_mask.assignFrom(_ALL_INT_REG_mask);\n+  if (!UseAPX) {\n+    for (uint i = 0; i < sizeof(egprs)\/sizeof(Register); i++) {\n+      _INT_REG_mask.remove(OptoReg::as_OptoReg(egprs[i]->as_VMReg()));\n@@ -1168,0 +1599,25 @@\n+\n+  if (PreserveFramePointer) {\n+    _INT_REG_mask.remove(OptoReg::as_OptoReg(rbp->as_VMReg()));\n+  }\n+  if (need_r12_heapbase()) {\n+    _INT_REG_mask.remove(OptoReg::as_OptoReg(r12->as_VMReg()));\n+  }\n+\n+  _STACK_OR_INT_REG_mask.assignFrom(_INT_REG_mask);\n+  _STACK_OR_INT_REG_mask.or_with(STACK_OR_STACK_SLOTS_mask());\n+\n+  _INT_NO_RAX_RDX_REG_mask.assignFrom(_INT_REG_mask);\n+  _INT_NO_RAX_RDX_REG_mask.remove(OptoReg::as_OptoReg(rax->as_VMReg()));\n+  _INT_NO_RAX_RDX_REG_mask.remove(OptoReg::as_OptoReg(rdx->as_VMReg()));\n+\n+  _INT_NO_RCX_REG_mask.assignFrom(_INT_REG_mask);\n+  _INT_NO_RCX_REG_mask.remove(OptoReg::as_OptoReg(rcx->as_VMReg()));\n+\n+  _INT_NO_RBP_R13_REG_mask.assignFrom(_INT_REG_mask);\n+  _INT_NO_RBP_R13_REG_mask.remove(OptoReg::as_OptoReg(rbp->as_VMReg()));\n+  _INT_NO_RBP_R13_REG_mask.remove(OptoReg::as_OptoReg(r13->as_VMReg()));\n+\n+  \/\/ _FLOAT_REG_LEGACY_mask\/_FLOAT_REG_EVEX_mask is generated by adlc\n+  \/\/ from the float_reg_legacy\/float_reg_evex register class.\n+  _FLOAT_REG_mask.assignFrom(VM_Version::supports_evex() ? _FLOAT_REG_EVEX_mask : _FLOAT_REG_LEGACY_mask);\n@@ -1170,2 +1626,2 @@\n-static inline Assembler::AvxVectorLen vector_length_encoding(const Node* n) {\n-  return vector_length_encoding(Matcher::vector_length_in_bytes(n));\n+static bool generate_vzeroupper(Compile* C) {\n+  return (VM_Version::supports_vzeroupper() && (C->max_vector_size() > 16 || C->clear_upper_avx() == true)) ? true: false;  \/\/ Generate vzeroupper\n@@ -1174,4 +1630,2 @@\n-static inline Assembler::AvxVectorLen vector_length_encoding(const MachNode* use, MachOper* opnd) {\n-  uint def_idx = use->operand_index(opnd);\n-  Node* def = use->in(def_idx);\n-  return vector_length_encoding(def);\n+static int clear_avx_size() {\n+  return generate_vzeroupper(Compile::current()) ? 3: 0;  \/\/ vzeroupper\n@@ -1180,3 +1634,8 @@\n-static inline bool is_vector_popcount_predicate(BasicType bt) {\n-  return (is_subword_type(bt) && VM_Version::supports_avx512_bitalg()) ||\n-         (is_non_subword_integral_type(bt) && VM_Version::supports_avx512_vpopcntdq());\n+\/\/ !!!!! Special hack to get all types of calls to specify the byte offset\n+\/\/       from the start of the call to the point where the return address\n+\/\/       will point.\n+int MachCallStaticJavaNode::ret_addr_offset()\n+{\n+  int offset = 5; \/\/ 5 bytes from start of call to where return address points\n+  offset += clear_avx_size();\n+  return offset;\n@@ -1185,3 +1644,5 @@\n-static inline bool is_clz_non_subword_predicate_evex(BasicType bt, int vlen_bytes) {\n-  return is_non_subword_integral_type(bt) && VM_Version::supports_avx512cd() &&\n-           (VM_Version::supports_avx512vl() || vlen_bytes == 64);\n+int MachCallDynamicJavaNode::ret_addr_offset()\n+{\n+  int offset = 15; \/\/ 15 bytes from start of call to where return address points\n+  offset += clear_avx_size();\n+  return offset;\n@@ -1190,17 +1651,10 @@\n-class Node::PD {\n-public:\n-  enum NodeFlags {\n-    Flag_intel_jcc_erratum    = Node::_last_flag << 1,\n-    Flag_sets_carry_flag      = Node::_last_flag << 2,\n-    Flag_sets_parity_flag     = Node::_last_flag << 3,\n-    Flag_sets_zero_flag       = Node::_last_flag << 4,\n-    Flag_sets_overflow_flag   = Node::_last_flag << 5,\n-    Flag_sets_sign_flag       = Node::_last_flag << 6,\n-    Flag_clears_carry_flag    = Node::_last_flag << 7,\n-    Flag_clears_parity_flag   = Node::_last_flag << 8,\n-    Flag_clears_zero_flag     = Node::_last_flag << 9,\n-    Flag_clears_overflow_flag = Node::_last_flag << 10,\n-    Flag_clears_sign_flag     = Node::_last_flag << 11,\n-    _last_flag                = Flag_clears_sign_flag\n-  };\n-};\n+int MachCallRuntimeNode::ret_addr_offset() {\n+  int offset = 13; \/\/ movq r10,#addr; callq (r10)\n+  if (this->ideal_Opcode() != Op_CallLeafVector) {\n+    offset += clear_avx_size();\n+  }\n+  return offset;\n+}\n+\/\/\n+\/\/ Compute padding required for nodes which need alignment\n+\/\/\n@@ -1208,1 +1662,8 @@\n-%} \/\/ end source_hpp\n+\/\/ The address of the call instruction needs to be 4-byte aligned to\n+\/\/ ensure that it does not span a cache line so that it can be patched.\n+int CallStaticJavaDirectNode::compute_padding(int current_offset) const\n+{\n+  current_offset += clear_avx_size(); \/\/ skip vzeroupper\n+  current_offset += 1; \/\/ skip call opcode byte\n+  return align_up(current_offset, alignment_required()) - current_offset;\n+}\n@@ -1210,1 +1671,8 @@\n-source %{\n+\/\/ The address of the call instruction needs to be 4-byte aligned to\n+\/\/ ensure that it does not span a cache line so that it can be patched.\n+int CallDynamicJavaDirectNode::compute_padding(int current_offset) const\n+{\n+  current_offset += clear_avx_size(); \/\/ skip vzeroupper\n+  current_offset += 11; \/\/ skip movq instruction + call opcode byte\n+  return align_up(current_offset, alignment_required()) - current_offset;\n+}\n@@ -1212,2 +1680,20 @@\n-#include \"opto\/addnode.hpp\"\n-#include \"c2_intelJccErratum_x86.hpp\"\n+\/\/ This could be in MacroAssembler but it's fairly C2 specific\n+static void emit_cmpfp_fixup(MacroAssembler* masm) {\n+  Label exit;\n+  __ jccb(Assembler::noParity, exit);\n+  __ pushf();\n+  \/\/\n+  \/\/ comiss\/ucomiss instructions set ZF,PF,CF flags and\n+  \/\/ zero OF,AF,SF for NaN values.\n+  \/\/ Fixup flags by zeroing ZF,PF so that compare of NaN\n+  \/\/ values returns 'less than' result (CF is set).\n+  \/\/ Leave the rest of flags unchanged.\n+  \/\/\n+  \/\/    7 6 5 4 3 2 1 0\n+  \/\/   |S|Z|r|A|r|P|r|C|  (r - reserved bit)\n+  \/\/    0 0 1 0 1 0 1 1   (0x2B)\n+  \/\/\n+  __ andq(Address(rsp, 0), 0xffffff2b);\n+  __ popf();\n+  __ bind(exit);\n+}\n@@ -1215,5 +1701,7 @@\n-void PhaseOutput::pd_perform_mach_node_analysis() {\n-  if (VM_Version::has_intel_jcc_erratum()) {\n-    int extra_padding = IntelJccErratum::tag_affected_machnodes(C, C->cfg(), C->regalloc());\n-    _buf_sizes._code += extra_padding;\n-  }\n+static void emit_cmpfp3(MacroAssembler* masm, Register dst) {\n+  Label done;\n+  __ movl(dst, -1);\n+  __ jcc(Assembler::parity, done);\n+  __ jcc(Assembler::below, done);\n+  __ setcc(Assembler::notEqual, dst);\n+  __ bind(done);\n@@ -1222,6 +1710,37 @@\n-int MachNode::pd_alignment_required() const {\n-  if (VM_Version::has_intel_jcc_erratum() && IntelJccErratum::is_jcc_erratum_branch(this)) {\n-    \/\/ Conservatively add worst case padding. We assume that relocInfo::addr_unit() is 1 on x86.\n-    return IntelJccErratum::largest_jcc_size() + 1;\n-  } else {\n-    return 1;\n+\/\/ Math.min()    # Math.max()\n+\/\/ --------------------------\n+\/\/ ucomis[s\/d]   #\n+\/\/ ja   -> b     # a\n+\/\/ jp   -> NaN   # NaN\n+\/\/ jb   -> a     # b\n+\/\/ je            #\n+\/\/ |-jz -> a | b # a & b\n+\/\/ |    -> a     #\n+static void emit_fp_min_max(MacroAssembler* masm, XMMRegister dst,\n+                            XMMRegister a, XMMRegister b,\n+                            XMMRegister xmmt, Register rt,\n+                            bool min, bool single) {\n+\n+  Label nan, zero, below, above, done;\n+\n+  if (single)\n+    __ ucomiss(a, b);\n+  else\n+    __ ucomisd(a, b);\n+\n+  if (dst->encoding() != (min ? b : a)->encoding())\n+    __ jccb(Assembler::above, above); \/\/ CF=0 & ZF=0\n+  else\n+    __ jccb(Assembler::above, done);\n+\n+  __ jccb(Assembler::parity, nan);  \/\/ PF=1\n+  __ jccb(Assembler::below, below); \/\/ CF=1\n+\n+  \/\/ equal\n+  __ vpxor(xmmt, xmmt, xmmt, Assembler::AVX_128bit);\n+  if (single) {\n+    __ ucomiss(a, xmmt);\n+    __ jccb(Assembler::equal, zero);\n+\n+    __ movflt(dst, a);\n+    __ jmp(done);\n@@ -1229,1 +1748,3 @@\n-}\n+  else {\n+    __ ucomisd(a, xmmt);\n+    __ jccb(Assembler::equal, zero);\n@@ -1231,9 +1752,2 @@\n-int MachNode::compute_padding(int current_offset) const {\n-  if (flags() & Node::PD::Flag_intel_jcc_erratum) {\n-    Compile* C = Compile::current();\n-    PhaseOutput* output = C->output();\n-    Block* block = output->block();\n-    int index = output->index();\n-    return IntelJccErratum::compute_padding(current_offset, this, block, index, C->regalloc());\n-  } else {\n-    return 0;\n+    __ movdbl(dst, a);\n+    __ jmp(done);\n@@ -1241,1 +1755,0 @@\n-}\n@@ -1243,3 +1756,5 @@\n-\/\/ Emit exception handler code.\n-\/\/ Stuff framesize into a register and call a VM stub routine.\n-int HandlerImpl::emit_exception_handler(C2_MacroAssembler* masm) {\n+  __ bind(zero);\n+  if (min)\n+    __ vpor(dst, a, b, Assembler::AVX_128bit);\n+  else\n+    __ vpand(dst, a, b, Assembler::AVX_128bit);\n@@ -1247,13 +1762,1 @@\n-  \/\/ Note that the code buffer's insts_mark is always relative to insts.\n-  \/\/ That's why we must use the macroassembler to generate a handler.\n-  address base = __ start_a_stub(size_exception_handler());\n-  if (base == nullptr) {\n-    ciEnv::current()->record_failure(\"CodeCache is full\");\n-    return 0;  \/\/ CodeBuffer::expand failed\n-  }\n-  int offset = __ offset();\n-  __ jump(RuntimeAddress(OptoRuntime::exception_blob()->entry_point()));\n-  assert(__ offset() - offset <= (int) size_exception_handler(), \"overflow\");\n-  __ end_a_stub();\n-  return offset;\n-}\n+  __ jmp(done);\n@@ -1261,2 +1764,5 @@\n-\/\/ Emit deopt handler code.\n-int HandlerImpl::emit_deopt_handler(C2_MacroAssembler* masm) {\n+  __ bind(above);\n+  if (single)\n+    __ movflt(dst, min ? b : a);\n+  else\n+    __ movdbl(dst, min ? b : a);\n@@ -1264,6 +1770,6 @@\n-  \/\/ Note that the code buffer's insts_mark is always relative to insts.\n-  \/\/ That's why we must use the macroassembler to generate a handler.\n-  address base = __ start_a_stub(size_deopt_handler());\n-  if (base == nullptr) {\n-    ciEnv::current()->record_failure(\"CodeCache is full\");\n-    return 0;  \/\/ CodeBuffer::expand failed\n+  __ jmp(done);\n+\n+  __ bind(nan);\n+  if (single) {\n+    __ movl(rt, 0x7fc00000); \/\/ Float.NaN\n+    __ movdl(dst, rt);\n@@ -1271,1 +1777,5 @@\n-  int offset = __ offset();\n+  else {\n+    __ mov64(rt, 0x7ff8000000000000L); \/\/ Double.NaN\n+    __ movdq(dst, rt);\n+  }\n+  __ jmp(done);\n@@ -1273,4 +1783,5 @@\n-  address the_pc = (address) __ pc();\n-  Label next;\n-  \/\/ push a \"the_pc\" on the stack without destroying any registers\n-  \/\/ as they all may be live.\n+  __ bind(below);\n+  if (single)\n+    __ movflt(dst, min ? a : b);\n+  else\n+    __ movdbl(dst, min ? a : b);\n@@ -1278,5 +1789,2 @@\n-  \/\/ push address of \"next\"\n-  __ call(next, relocInfo::none); \/\/ reloc none is fine since it is a disp32\n-  __ bind(next);\n-  \/\/ adjust it so it matches \"the_pc\"\n-  __ subptr(Address(rsp, 0), __ offset() - offset);\n+  __ bind(done);\n+}\n@@ -1284,4 +1792,5 @@\n-  __ jump(RuntimeAddress(SharedRuntime::deopt_blob()->unpack()));\n-  assert(__ offset() - offset <= (int) size_deopt_handler(), \"overflow %d\", (__ offset() - offset));\n-  __ end_a_stub();\n-  return offset;\n+\/\/=============================================================================\n+const RegMask& MachConstantBaseNode::_out_RegMask = RegMask::EMPTY;\n+\n+int ConstantTable::calculate_table_base_offset() const {\n+  return 0;  \/\/ absolute addressing, no offset\n@@ -1290,7 +1799,44 @@\n-static Assembler::Width widthForType(BasicType bt) {\n-  if (bt == T_BYTE) {\n-    return Assembler::B;\n-  } else if (bt == T_SHORT) {\n-    return Assembler::W;\n-  } else if (bt == T_INT) {\n-    return Assembler::D;\n+bool MachConstantBaseNode::requires_postalloc_expand() const { return false; }\n+void MachConstantBaseNode::postalloc_expand(GrowableArray <Node *> *nodes, PhaseRegAlloc *ra_) {\n+  ShouldNotReachHere();\n+}\n+\n+void MachConstantBaseNode::emit(C2_MacroAssembler* masm, PhaseRegAlloc* ra_) const {\n+  \/\/ Empty encoding\n+}\n+\n+uint MachConstantBaseNode::size(PhaseRegAlloc* ra_) const {\n+  return 0;\n+}\n+\n+#ifndef PRODUCT\n+void MachConstantBaseNode::format(PhaseRegAlloc* ra_, outputStream* st) const {\n+  st->print(\"# MachConstantBaseNode (empty encoding)\");\n+}\n+#endif\n+\n+\n+\/\/=============================================================================\n+#ifndef PRODUCT\n+void MachPrologNode::format(PhaseRegAlloc* ra_, outputStream* st) const {\n+  Compile* C = ra_->C;\n+\n+  int framesize = C->output()->frame_size_in_bytes();\n+  int bangsize = C->output()->bang_size_in_bytes();\n+  assert((framesize & (StackAlignmentInBytes-1)) == 0, \"frame size not aligned\");\n+  \/\/ Remove wordSize for return addr which is already pushed.\n+  framesize -= wordSize;\n+\n+  if (C->output()->need_stack_bang(bangsize)) {\n+    framesize -= wordSize;\n+    st->print(\"# stack bang (%d bytes)\", bangsize);\n+    st->print(\"\\n\\t\");\n+    st->print(\"pushq   rbp\\t# Save rbp\");\n+    if (PreserveFramePointer) {\n+        st->print(\"\\n\\t\");\n+        st->print(\"movq    rbp, rsp\\t# Save the caller's SP into rbp\");\n+    }\n+    if (framesize) {\n+      st->print(\"\\n\\t\");\n+      st->print(\"subq    rsp, #%d\\t# Create frame\",framesize);\n+    }\n@@ -1298,2 +1844,31 @@\n-    assert(bt == T_LONG, \"not a long: %s\", type2name(bt));\n-    return Assembler::Q;\n+    st->print(\"subq    rsp, #%d\\t# Create frame\",framesize);\n+    st->print(\"\\n\\t\");\n+    framesize -= wordSize;\n+    st->print(\"movq    [rsp + #%d], rbp\\t# Save rbp\",framesize);\n+    if (PreserveFramePointer) {\n+      st->print(\"\\n\\t\");\n+      st->print(\"movq    rbp, rsp\\t# Save the caller's SP into rbp\");\n+      if (framesize > 0) {\n+        st->print(\"\\n\\t\");\n+        st->print(\"addq    rbp, #%d\", framesize);\n+      }\n+    }\n+  }\n+\n+  if (VerifyStackAtCalls) {\n+    st->print(\"\\n\\t\");\n+    framesize -= wordSize;\n+    st->print(\"movq    [rsp + #%d], 0xbadb100d\\t# Majik cookie for stack depth check\",framesize);\n+#ifdef ASSERT\n+    st->print(\"\\n\\t\");\n+    st->print(\"# stack alignment check\");\n+#endif\n+  }\n+  if (C->stub_function() != nullptr) {\n+    st->print(\"\\n\\t\");\n+    st->print(\"cmpl    [r15_thread + #disarmed_guard_value_offset], #disarmed_guard_value\\t\");\n+    st->print(\"\\n\\t\");\n+    st->print(\"je      fast_entry\\t\");\n+    st->print(\"\\n\\t\");\n+    st->print(\"call    #nmethod_entry_barrier_stub\\t\");\n+    st->print(\"\\n\\tfast_entry:\");\n@@ -1301,0 +1876,1 @@\n+  st->cr();\n@@ -1302,0 +1878,1 @@\n+#endif\n@@ -1303,1 +1880,2 @@\n-\/\/=============================================================================\n+void MachPrologNode::emit(C2_MacroAssembler *masm, PhaseRegAlloc *ra_) const {\n+  Compile* C = ra_->C;\n@@ -1305,20 +1883,40 @@\n-  \/\/ Float masks come from different places depending on platform.\n-  static address float_signmask()  { return StubRoutines::x86::float_sign_mask(); }\n-  static address float_signflip()  { return StubRoutines::x86::float_sign_flip(); }\n-  static address double_signmask() { return StubRoutines::x86::double_sign_mask(); }\n-  static address double_signflip() { return StubRoutines::x86::double_sign_flip(); }\n-  static address vector_short_to_byte_mask() { return StubRoutines::x86::vector_short_to_byte_mask(); }\n-  static address vector_int_to_byte_mask() { return StubRoutines::x86::vector_int_to_byte_mask(); }\n-  static address vector_byte_perm_mask() { return StubRoutines::x86::vector_byte_perm_mask(); }\n-  static address vector_long_sign_mask() { return StubRoutines::x86::vector_long_sign_mask(); }\n-  static address vector_all_bits_set() { return StubRoutines::x86::vector_all_bits_set(); }\n-  static address vector_int_mask_cmp_bits() { return StubRoutines::x86::vector_int_mask_cmp_bits(); }\n-  static address vector_int_to_short_mask() { return StubRoutines::x86::vector_int_to_short_mask(); }\n-  static address vector_byte_shufflemask() { return StubRoutines::x86::vector_byte_shuffle_mask(); }\n-  static address vector_short_shufflemask() { return StubRoutines::x86::vector_short_shuffle_mask(); }\n-  static address vector_int_shufflemask() { return StubRoutines::x86::vector_int_shuffle_mask(); }\n-  static address vector_long_shufflemask() { return StubRoutines::x86::vector_long_shuffle_mask(); }\n-  static address vector_32_bit_mask() { return StubRoutines::x86::vector_32_bit_mask(); }\n-  static address vector_64_bit_mask() { return StubRoutines::x86::vector_64_bit_mask(); }\n-  static address vector_float_signflip() { return StubRoutines::x86::vector_float_sign_flip();}\n-  static address vector_double_signflip() { return StubRoutines::x86::vector_double_sign_flip();}\n+  int framesize = C->output()->frame_size_in_bytes();\n+  int bangsize = C->output()->bang_size_in_bytes();\n+\n+  if (C->clinit_barrier_on_entry()) {\n+    assert(VM_Version::supports_fast_class_init_checks(), \"sanity\");\n+    assert(!C->method()->holder()->is_not_initialized(), \"initialization should have been started\");\n+\n+    Label L_skip_barrier;\n+    Register klass = rscratch1;\n+\n+    __ mov_metadata(klass, C->method()->holder()->constant_encoding());\n+    __ clinit_barrier(klass, &L_skip_barrier \/*L_fast_path*\/);\n+\n+    __ jump(RuntimeAddress(SharedRuntime::get_handle_wrong_method_stub())); \/\/ slow path\n+\n+    __ bind(L_skip_barrier);\n+  }\n+\n+  __ verified_entry(framesize, C->output()->need_stack_bang(bangsize)?bangsize:0, false, C->stub_function() != nullptr);\n+\n+  C->output()->set_frame_complete(__ offset());\n+\n+  if (C->has_mach_constant_base_node()) {\n+    \/\/ NOTE: We set the table base offset here because users might be\n+    \/\/ emitted before MachConstantBaseNode.\n+    ConstantTable& constant_table = C->output()->constant_table();\n+    constant_table.set_table_base_offset(constant_table.calculate_table_base_offset());\n+  }\n+}\n+\n+uint MachPrologNode::size(PhaseRegAlloc* ra_) const\n+{\n+  return MachNode::size(ra_); \/\/ too many variables; just compute it\n+                              \/\/ the hard way\n+}\n+\n+int MachPrologNode::reloc() const\n+{\n+  return 0; \/\/ a large enough number\n+}\n@@ -1327,3 +1925,7 @@\n-bool Matcher::match_rule_supported(int opcode) {\n-  if (!has_match_rule(opcode)) {\n-    return false; \/\/ no match rule present\n+#ifndef PRODUCT\n+void MachEpilogNode::format(PhaseRegAlloc* ra_, outputStream* st) const\n+{\n+  Compile* C = ra_->C;\n+  if (generate_vzeroupper(C)) {\n+    st->print(\"vzeroupper\");\n+    st->cr(); st->print(\"\\t\");\n@@ -1331,12 +1933,121 @@\n-  switch (opcode) {\n-    case Op_AbsVL:\n-    case Op_StoreVectorScatter:\n-      if (UseAVX < 3) {\n-        return false;\n-      }\n-      break;\n-    case Op_PopCountI:\n-    case Op_PopCountL:\n-      if (!UsePopCountInstruction) {\n-        return false;\n-      }\n+\n+  int framesize = C->output()->frame_size_in_bytes();\n+  assert((framesize & (StackAlignmentInBytes-1)) == 0, \"frame size not aligned\");\n+  \/\/ Remove word for return adr already pushed\n+  \/\/ and RBP\n+  framesize -= 2*wordSize;\n+\n+  if (framesize) {\n+    st->print_cr(\"addq    rsp, %d\\t# Destroy frame\", framesize);\n+    st->print(\"\\t\");\n+  }\n+\n+  st->print_cr(\"popq    rbp\");\n+  if (do_polling() && C->is_method_compilation()) {\n+    st->print(\"\\t\");\n+    st->print_cr(\"cmpq    rsp, poll_offset[r15_thread] \\n\\t\"\n+                 \"ja      #safepoint_stub\\t\"\n+                 \"# Safepoint: poll for GC\");\n+  }\n+}\n+#endif\n+\n+void MachEpilogNode::emit(C2_MacroAssembler* masm, PhaseRegAlloc* ra_) const\n+{\n+  Compile* C = ra_->C;\n+\n+  if (generate_vzeroupper(C)) {\n+    \/\/ Clear upper bits of YMM registers when current compiled code uses\n+    \/\/ wide vectors to avoid AVX <-> SSE transition penalty during call.\n+    __ vzeroupper();\n+  }\n+\n+  int framesize = C->output()->frame_size_in_bytes();\n+  assert((framesize & (StackAlignmentInBytes-1)) == 0, \"frame size not aligned\");\n+  \/\/ Remove word for return adr already pushed\n+  \/\/ and RBP\n+  framesize -= 2*wordSize;\n+\n+  \/\/ Note that VerifyStackAtCalls' Majik cookie does not change the frame size popped here\n+\n+  if (framesize) {\n+    __ addq(rsp, framesize);\n+  }\n+\n+  __ popq(rbp);\n+\n+  if (StackReservedPages > 0 && C->has_reserved_stack_access()) {\n+    __ reserved_stack_check();\n+  }\n+\n+  if (do_polling() && C->is_method_compilation()) {\n+    Label dummy_label;\n+    Label* code_stub = &dummy_label;\n+    if (!C->output()->in_scratch_emit_size()) {\n+      C2SafepointPollStub* stub = new (C->comp_arena()) C2SafepointPollStub(__ offset());\n+      C->output()->add_stub(stub);\n+      code_stub = &stub->entry();\n+    }\n+    __ relocate(relocInfo::poll_return_type);\n+    __ safepoint_poll(*code_stub, true \/* at_return *\/, true \/* in_nmethod *\/);\n+  }\n+}\n+\n+uint MachEpilogNode::size(PhaseRegAlloc* ra_) const\n+{\n+  return MachNode::size(ra_); \/\/ too many variables; just compute it\n+                              \/\/ the hard way\n+}\n+\n+int MachEpilogNode::reloc() const\n+{\n+  return 2; \/\/ a large enough number\n+}\n+\n+const Pipeline* MachEpilogNode::pipeline() const\n+{\n+  return MachNode::pipeline_class();\n+}\n+\n+\/\/=============================================================================\n+\n+enum RC {\n+  rc_bad,\n+  rc_int,\n+  rc_kreg,\n+  rc_float,\n+  rc_stack\n+};\n+\n+static enum RC rc_class(OptoReg::Name reg)\n+{\n+  if( !OptoReg::is_valid(reg)  ) return rc_bad;\n+\n+  if (OptoReg::is_stack(reg)) return rc_stack;\n+\n+  VMReg r = OptoReg::as_VMReg(reg);\n+\n+  if (r->is_Register()) return rc_int;\n+\n+  if (r->is_KRegister()) return rc_kreg;\n+\n+  assert(r->is_XMMRegister(), \"must be\");\n+  return rc_float;\n+}\n+\n+\/\/ Next two methods are shared by 32- and 64-bit VM. They are defined in x86.ad.\n+static void vec_mov_helper(C2_MacroAssembler *masm, int src_lo, int dst_lo,\n+                          int src_hi, int dst_hi, uint ireg, outputStream* st);\n+\n+void vec_spill_helper(C2_MacroAssembler *masm, bool is_load,\n+                     int stack_offset, int reg, uint ireg, outputStream* st);\n+\n+static void vec_stack_to_stack_helper(C2_MacroAssembler *masm, int src_offset,\n+                                      int dst_offset, uint ireg, outputStream* st) {\n+  if (masm) {\n+    switch (ireg) {\n+    case Op_VecS:\n+      __ movq(Address(rsp, -8), rax);\n+      __ movl(rax, Address(rsp, src_offset));\n+      __ movl(Address(rsp, dst_offset), rax);\n+      __ movq(rax, Address(rsp, -8));\n@@ -1344,4 +2055,3 @@\n-    case Op_PopCountVI:\n-      if (UseAVX < 2) {\n-        return false;\n-      }\n+    case Op_VecD:\n+      __ pushq(Address(rsp, src_offset));\n+      __ popq (Address(rsp, dst_offset));\n@@ -1349,6 +2059,5 @@\n-    case Op_CompressV:\n-    case Op_ExpandV:\n-    case Op_PopCountVL:\n-      if (UseAVX < 2) {\n-        return false;\n-      }\n+    case Op_VecX:\n+      __ pushq(Address(rsp, src_offset));\n+      __ popq (Address(rsp, dst_offset));\n+      __ pushq(Address(rsp, src_offset+8));\n+      __ popq (Address(rsp, dst_offset+8));\n@@ -1356,4 +2065,5 @@\n-    case Op_MulVI:\n-      if ((UseSSE < 4) && (UseAVX < 1)) { \/\/ only with SSE4_1 or AVX\n-        return false;\n-      }\n+    case Op_VecY:\n+      __ vmovdqu(Address(rsp, -32), xmm0);\n+      __ vmovdqu(xmm0, Address(rsp, src_offset));\n+      __ vmovdqu(Address(rsp, dst_offset), xmm0);\n+      __ vmovdqu(xmm0, Address(rsp, -32));\n@@ -1361,4 +2071,5 @@\n-    case Op_MulVL:\n-      if (UseSSE < 4) { \/\/ only with SSE4_1 or AVX\n-        return false;\n-      }\n+    case Op_VecZ:\n+      __ evmovdquq(Address(rsp, -64), xmm0, 2);\n+      __ evmovdquq(xmm0, Address(rsp, src_offset), 2);\n+      __ evmovdquq(Address(rsp, dst_offset), xmm0, 2);\n+      __ evmovdquq(xmm0, Address(rsp, -64), 2);\n@@ -1366,4 +2077,12 @@\n-    case Op_MulReductionVL:\n-      if (VM_Version::supports_avx512dq() == false) {\n-        return false;\n-      }\n+    default:\n+      ShouldNotReachHere();\n+    }\n+#ifndef PRODUCT\n+  } else {\n+    switch (ireg) {\n+    case Op_VecS:\n+      st->print(\"movq    [rsp - #8], rax\\t# 32-bit mem-mem spill\\n\\t\"\n+                \"movl    rax, [rsp + #%d]\\n\\t\"\n+                \"movl    [rsp + #%d], rax\\n\\t\"\n+                \"movq    rax, [rsp - #8]\",\n+                src_offset, dst_offset);\n@@ -1371,10 +2090,4 @@\n-    case Op_AbsVB:\n-    case Op_AbsVS:\n-    case Op_AbsVI:\n-    case Op_AddReductionVI:\n-    case Op_AndReductionV:\n-    case Op_OrReductionV:\n-    case Op_XorReductionV:\n-      if (UseSSE < 3) { \/\/ requires at least SSSE3\n-        return false;\n-      }\n+    case Op_VecD:\n+      st->print(\"pushq   [rsp + #%d]\\t# 64-bit mem-mem spill\\n\\t\"\n+                \"popq    [rsp + #%d]\",\n+                src_offset, dst_offset);\n@@ -1382,16 +2095,6 @@\n-    case Op_MaxHF:\n-    case Op_MinHF:\n-      if (!VM_Version::supports_avx512vlbw()) {\n-        return false;\n-      }  \/\/ fallthrough\n-    case Op_AddHF:\n-    case Op_DivHF:\n-    case Op_FmaHF:\n-    case Op_MulHF:\n-    case Op_ReinterpretS2HF:\n-    case Op_ReinterpretHF2S:\n-    case Op_SubHF:\n-    case Op_SqrtHF:\n-      if (!VM_Version::supports_avx512_fp16()) {\n-        return false;\n-      }\n+     case Op_VecX:\n+      st->print(\"pushq   [rsp + #%d]\\t# 128-bit mem-mem spill\\n\\t\"\n+                \"popq    [rsp + #%d]\\n\\t\"\n+                \"pushq   [rsp + #%d]\\n\\t\"\n+                \"popq    [rsp + #%d]\",\n+                src_offset, dst_offset, src_offset+8, dst_offset+8);\n@@ -1399,6 +2102,6 @@\n-    case Op_VectorLoadShuffle:\n-    case Op_VectorRearrange:\n-    case Op_MulReductionVI:\n-      if (UseSSE < 4) { \/\/ requires at least SSE4\n-        return false;\n-      }\n+    case Op_VecY:\n+      st->print(\"vmovdqu [rsp - #32], xmm0\\t# 256-bit mem-mem spill\\n\\t\"\n+                \"vmovdqu xmm0, [rsp + #%d]\\n\\t\"\n+                \"vmovdqu [rsp + #%d], xmm0\\n\\t\"\n+                \"vmovdqu xmm0, [rsp - #32]\",\n+                src_offset, dst_offset);\n@@ -1406,5 +2109,6 @@\n-    case Op_IsInfiniteF:\n-    case Op_IsInfiniteD:\n-      if (!VM_Version::supports_avx512dq()) {\n-        return false;\n-      }\n+    case Op_VecZ:\n+      st->print(\"vmovdqu [rsp - #64], xmm0\\t# 512-bit mem-mem spill\\n\\t\"\n+                \"vmovdqu xmm0, [rsp + #%d]\\n\\t\"\n+                \"vmovdqu [rsp + #%d], xmm0\\n\\t\"\n+                \"vmovdqu xmm0, [rsp - #64]\",\n+                src_offset, dst_offset);\n@@ -1412,15 +2116,93 @@\n-    case Op_SqrtVD:\n-    case Op_SqrtVF:\n-    case Op_VectorMaskCmp:\n-    case Op_VectorCastB2X:\n-    case Op_VectorCastS2X:\n-    case Op_VectorCastI2X:\n-    case Op_VectorCastL2X:\n-    case Op_VectorCastF2X:\n-    case Op_VectorCastD2X:\n-    case Op_VectorUCastB2X:\n-    case Op_VectorUCastS2X:\n-    case Op_VectorUCastI2X:\n-    case Op_VectorMaskCast:\n-      if (UseAVX < 1) { \/\/ enabled for AVX only\n-        return false;\n+    default:\n+      ShouldNotReachHere();\n+    }\n+#endif\n+  }\n+}\n+\n+uint MachSpillCopyNode::implementation(C2_MacroAssembler* masm,\n+                                       PhaseRegAlloc* ra_,\n+                                       bool do_size,\n+                                       outputStream* st) const {\n+  assert(masm != nullptr || st  != nullptr, \"sanity\");\n+  \/\/ Get registers to move\n+  OptoReg::Name src_second = ra_->get_reg_second(in(1));\n+  OptoReg::Name src_first = ra_->get_reg_first(in(1));\n+  OptoReg::Name dst_second = ra_->get_reg_second(this);\n+  OptoReg::Name dst_first = ra_->get_reg_first(this);\n+\n+  enum RC src_second_rc = rc_class(src_second);\n+  enum RC src_first_rc = rc_class(src_first);\n+  enum RC dst_second_rc = rc_class(dst_second);\n+  enum RC dst_first_rc = rc_class(dst_first);\n+\n+  assert(OptoReg::is_valid(src_first) && OptoReg::is_valid(dst_first),\n+         \"must move at least 1 register\" );\n+\n+  if (src_first == dst_first && src_second == dst_second) {\n+    \/\/ Self copy, no move\n+    return 0;\n+  }\n+  if (bottom_type()->isa_vect() != nullptr && bottom_type()->isa_vectmask() == nullptr) {\n+    uint ireg = ideal_reg();\n+    assert((src_first_rc != rc_int && dst_first_rc != rc_int), \"sanity\");\n+    assert((ireg == Op_VecS || ireg == Op_VecD || ireg == Op_VecX || ireg == Op_VecY || ireg == Op_VecZ ), \"sanity\");\n+    if( src_first_rc == rc_stack && dst_first_rc == rc_stack ) {\n+      \/\/ mem -> mem\n+      int src_offset = ra_->reg2offset(src_first);\n+      int dst_offset = ra_->reg2offset(dst_first);\n+      vec_stack_to_stack_helper(masm, src_offset, dst_offset, ireg, st);\n+    } else if (src_first_rc == rc_float && dst_first_rc == rc_float ) {\n+      vec_mov_helper(masm, src_first, dst_first, src_second, dst_second, ireg, st);\n+    } else if (src_first_rc == rc_float && dst_first_rc == rc_stack ) {\n+      int stack_offset = ra_->reg2offset(dst_first);\n+      vec_spill_helper(masm, false, stack_offset, src_first, ireg, st);\n+    } else if (src_first_rc == rc_stack && dst_first_rc == rc_float ) {\n+      int stack_offset = ra_->reg2offset(src_first);\n+      vec_spill_helper(masm, true,  stack_offset, dst_first, ireg, st);\n+    } else {\n+      ShouldNotReachHere();\n+    }\n+    return 0;\n+  }\n+  if (src_first_rc == rc_stack) {\n+    \/\/ mem ->\n+    if (dst_first_rc == rc_stack) {\n+      \/\/ mem -> mem\n+      assert(src_second != dst_first, \"overlap\");\n+      if ((src_first & 1) == 0 && src_first + 1 == src_second &&\n+          (dst_first & 1) == 0 && dst_first + 1 == dst_second) {\n+        \/\/ 64-bit\n+        int src_offset = ra_->reg2offset(src_first);\n+        int dst_offset = ra_->reg2offset(dst_first);\n+        if (masm) {\n+          __ pushq(Address(rsp, src_offset));\n+          __ popq (Address(rsp, dst_offset));\n+#ifndef PRODUCT\n+        } else {\n+          st->print(\"pushq   [rsp + #%d]\\t# 64-bit mem-mem spill\\n\\t\"\n+                    \"popq    [rsp + #%d]\",\n+                     src_offset, dst_offset);\n+#endif\n+        }\n+      } else {\n+        \/\/ 32-bit\n+        assert(!((src_first & 1) == 0 && src_first + 1 == src_second), \"no transform\");\n+        assert(!((dst_first & 1) == 0 && dst_first + 1 == dst_second), \"no transform\");\n+        \/\/ No pushl\/popl, so:\n+        int src_offset = ra_->reg2offset(src_first);\n+        int dst_offset = ra_->reg2offset(dst_first);\n+        if (masm) {\n+          __ movq(Address(rsp, -8), rax);\n+          __ movl(rax, Address(rsp, src_offset));\n+          __ movl(Address(rsp, dst_offset), rax);\n+          __ movq(rax, Address(rsp, -8));\n+#ifndef PRODUCT\n+        } else {\n+          st->print(\"movq    [rsp - #8], rax\\t# 32-bit mem-mem spill\\n\\t\"\n+                    \"movl    rax, [rsp + #%d]\\n\\t\"\n+                    \"movl    [rsp + #%d], rax\\n\\t\"\n+                    \"movq    rax, [rsp - #8]\",\n+                     src_offset, dst_offset);\n+#endif\n+        }\n@@ -1428,4 +2210,30 @@\n-      break;\n-    case Op_PopulateIndex:\n-      if (UseAVX < 2) {\n-        return false;\n+      return 0;\n+    } else if (dst_first_rc == rc_int) {\n+      \/\/ mem -> gpr\n+      if ((src_first & 1) == 0 && src_first + 1 == src_second &&\n+          (dst_first & 1) == 0 && dst_first + 1 == dst_second) {\n+        \/\/ 64-bit\n+        int offset = ra_->reg2offset(src_first);\n+        if (masm) {\n+          __ movq(as_Register(Matcher::_regEncode[dst_first]), Address(rsp, offset));\n+#ifndef PRODUCT\n+        } else {\n+          st->print(\"movq    %s, [rsp + #%d]\\t# spill\",\n+                     Matcher::regName[dst_first],\n+                     offset);\n+#endif\n+        }\n+      } else {\n+        \/\/ 32-bit\n+        assert(!((src_first & 1) == 0 && src_first + 1 == src_second), \"no transform\");\n+        assert(!((dst_first & 1) == 0 && dst_first + 1 == dst_second), \"no transform\");\n+        int offset = ra_->reg2offset(src_first);\n+        if (masm) {\n+          __ movl(as_Register(Matcher::_regEncode[dst_first]), Address(rsp, offset));\n+#ifndef PRODUCT\n+        } else {\n+          st->print(\"movl    %s, [rsp + #%d]\\t# spill\",\n+                     Matcher::regName[dst_first],\n+                     offset);\n+#endif\n+        }\n@@ -1433,67 +2241,31 @@\n-      break;\n-    case Op_RoundVF:\n-      if (UseAVX < 2) { \/\/ enabled for AVX2 only\n-        return false;\n-      }\n-      break;\n-    case Op_RoundVD:\n-      if (UseAVX < 3) {\n-        return false;  \/\/ enabled for AVX3 only\n-      }\n-      break;\n-    case Op_CompareAndSwapL:\n-    case Op_CompareAndSwapP:\n-      break;\n-    case Op_StrIndexOf:\n-      if (!UseSSE42Intrinsics) {\n-        return false;\n-      }\n-      break;\n-    case Op_StrIndexOfChar:\n-      if (!UseSSE42Intrinsics) {\n-        return false;\n-      }\n-      break;\n-    case Op_OnSpinWait:\n-      if (VM_Version::supports_on_spin_wait() == false) {\n-        return false;\n-      }\n-      break;\n-    case Op_MulVB:\n-    case Op_LShiftVB:\n-    case Op_RShiftVB:\n-    case Op_URShiftVB:\n-    case Op_VectorInsert:\n-    case Op_VectorLoadMask:\n-    case Op_VectorStoreMask:\n-    case Op_VectorBlend:\n-      if (UseSSE < 4) {\n-        return false;\n-      }\n-      break;\n-    case Op_MaxD:\n-    case Op_MaxF:\n-    case Op_MinD:\n-    case Op_MinF:\n-      if (UseAVX < 1) { \/\/ enabled for AVX only\n-        return false;\n-      }\n-      break;\n-    case Op_CacheWB:\n-    case Op_CacheWBPreSync:\n-    case Op_CacheWBPostSync:\n-      if (!VM_Version::supports_data_cache_line_flush()) {\n-        return false;\n-      }\n-      break;\n-    case Op_ExtractB:\n-    case Op_ExtractL:\n-    case Op_ExtractI:\n-    case Op_RoundDoubleMode:\n-      if (UseSSE < 4) {\n-        return false;\n-      }\n-      break;\n-    case Op_RoundDoubleModeV:\n-      if (VM_Version::supports_avx() == false) {\n-        return false; \/\/ 128bit vroundpd is not available\n+      return 0;\n+    } else if (dst_first_rc == rc_float) {\n+      \/\/ mem-> xmm\n+      if ((src_first & 1) == 0 && src_first + 1 == src_second &&\n+          (dst_first & 1) == 0 && dst_first + 1 == dst_second) {\n+        \/\/ 64-bit\n+        int offset = ra_->reg2offset(src_first);\n+        if (masm) {\n+          __ movdbl( as_XMMRegister(Matcher::_regEncode[dst_first]), Address(rsp, offset));\n+#ifndef PRODUCT\n+        } else {\n+          st->print(\"%s  %s, [rsp + #%d]\\t# spill\",\n+                     UseXmmLoadAndClearUpper ? \"movsd \" : \"movlpd\",\n+                     Matcher::regName[dst_first],\n+                     offset);\n+#endif\n+        }\n+      } else {\n+        \/\/ 32-bit\n+        assert(!((src_first & 1) == 0 && src_first + 1 == src_second), \"no transform\");\n+        assert(!((dst_first & 1) == 0 && dst_first + 1 == dst_second), \"no transform\");\n+        int offset = ra_->reg2offset(src_first);\n+        if (masm) {\n+          __ movflt( as_XMMRegister(Matcher::_regEncode[dst_first]), Address(rsp, offset));\n+#ifndef PRODUCT\n+        } else {\n+          st->print(\"movss   %s, [rsp + #%d]\\t# spill\",\n+                     Matcher::regName[dst_first],\n+                     offset);\n+#endif\n+        }\n@@ -1501,5 +2273,16 @@\n-      break;\n-    case Op_LoadVectorGather:\n-    case Op_LoadVectorGatherMasked:\n-      if (UseAVX < 2) {\n-        return false;\n+      return 0;\n+    } else if (dst_first_rc == rc_kreg) {\n+      \/\/ mem -> kreg\n+      if ((src_first & 1) == 0 && src_first + 1 == src_second &&\n+          (dst_first & 1) == 0 && dst_first + 1 == dst_second) {\n+        \/\/ 64-bit\n+        int offset = ra_->reg2offset(src_first);\n+        if (masm) {\n+          __ kmov(as_KRegister(Matcher::_regEncode[dst_first]), Address(rsp, offset));\n+#ifndef PRODUCT\n+        } else {\n+          st->print(\"kmovq   %s, [rsp + #%d]\\t# spill\",\n+                     Matcher::regName[dst_first],\n+                     offset);\n+#endif\n+        }\n@@ -1507,7 +2290,33 @@\n-      break;\n-    case Op_FmaF:\n-    case Op_FmaD:\n-    case Op_FmaVD:\n-    case Op_FmaVF:\n-      if (!UseFMA) {\n-        return false;\n+      return 0;\n+    }\n+  } else if (src_first_rc == rc_int) {\n+    \/\/ gpr ->\n+    if (dst_first_rc == rc_stack) {\n+      \/\/ gpr -> mem\n+      if ((src_first & 1) == 0 && src_first + 1 == src_second &&\n+          (dst_first & 1) == 0 && dst_first + 1 == dst_second) {\n+        \/\/ 64-bit\n+        int offset = ra_->reg2offset(dst_first);\n+        if (masm) {\n+          __ movq(Address(rsp, offset), as_Register(Matcher::_regEncode[src_first]));\n+#ifndef PRODUCT\n+        } else {\n+          st->print(\"movq    [rsp + #%d], %s\\t# spill\",\n+                     offset,\n+                     Matcher::regName[src_first]);\n+#endif\n+        }\n+      } else {\n+        \/\/ 32-bit\n+        assert(!((src_first & 1) == 0 && src_first + 1 == src_second), \"no transform\");\n+        assert(!((dst_first & 1) == 0 && dst_first + 1 == dst_second), \"no transform\");\n+        int offset = ra_->reg2offset(dst_first);\n+        if (masm) {\n+          __ movl(Address(rsp, offset), as_Register(Matcher::_regEncode[src_first]));\n+#ifndef PRODUCT\n+        } else {\n+          st->print(\"movl    [rsp + #%d], %s\\t# spill\",\n+                     offset,\n+                     Matcher::regName[src_first]);\n+#endif\n+        }\n@@ -1515,4 +2324,32 @@\n-      break;\n-    case Op_MacroLogicV:\n-      if (UseAVX < 3 || !UseVectorMacroLogic) {\n-        return false;\n+      return 0;\n+    } else if (dst_first_rc == rc_int) {\n+      \/\/ gpr -> gpr\n+      if ((src_first & 1) == 0 && src_first + 1 == src_second &&\n+          (dst_first & 1) == 0 && dst_first + 1 == dst_second) {\n+        \/\/ 64-bit\n+        if (masm) {\n+          __ movq(as_Register(Matcher::_regEncode[dst_first]),\n+                  as_Register(Matcher::_regEncode[src_first]));\n+#ifndef PRODUCT\n+        } else {\n+          st->print(\"movq    %s, %s\\t# spill\",\n+                     Matcher::regName[dst_first],\n+                     Matcher::regName[src_first]);\n+#endif\n+        }\n+        return 0;\n+      } else {\n+        \/\/ 32-bit\n+        assert(!((src_first & 1) == 0 && src_first + 1 == src_second), \"no transform\");\n+        assert(!((dst_first & 1) == 0 && dst_first + 1 == dst_second), \"no transform\");\n+        if (masm) {\n+          __ movl(as_Register(Matcher::_regEncode[dst_first]),\n+                  as_Register(Matcher::_regEncode[src_first]));\n+#ifndef PRODUCT\n+        } else {\n+          st->print(\"movl    %s, %s\\t# spill\",\n+                     Matcher::regName[dst_first],\n+                     Matcher::regName[src_first]);\n+#endif\n+        }\n+        return 0;\n@@ -1520,6 +2357,27 @@\n-      break;\n-\n-    case Op_VectorCmpMasked:\n-    case Op_VectorMaskGen:\n-      if (UseAVX < 3 || !VM_Version::supports_bmi2()) {\n-        return false;\n+    } else if (dst_first_rc == rc_float) {\n+      \/\/ gpr -> xmm\n+      if ((src_first & 1) == 0 && src_first + 1 == src_second &&\n+          (dst_first & 1) == 0 && dst_first + 1 == dst_second) {\n+        \/\/ 64-bit\n+        if (masm) {\n+          __ movdq( as_XMMRegister(Matcher::_regEncode[dst_first]), as_Register(Matcher::_regEncode[src_first]));\n+#ifndef PRODUCT\n+        } else {\n+          st->print(\"movdq   %s, %s\\t# spill\",\n+                     Matcher::regName[dst_first],\n+                     Matcher::regName[src_first]);\n+#endif\n+        }\n+      } else {\n+        \/\/ 32-bit\n+        assert(!((src_first & 1) == 0 && src_first + 1 == src_second), \"no transform\");\n+        assert(!((dst_first & 1) == 0 && dst_first + 1 == dst_second), \"no transform\");\n+        if (masm) {\n+          __ movdl( as_XMMRegister(Matcher::_regEncode[dst_first]), as_Register(Matcher::_regEncode[src_first]));\n+#ifndef PRODUCT\n+        } else {\n+          st->print(\"movdl   %s, %s\\t# spill\",\n+                     Matcher::regName[dst_first],\n+                     Matcher::regName[src_first]);\n+#endif\n+        }\n@@ -1527,7 +2385,14 @@\n-      break;\n-    case Op_VectorMaskFirstTrue:\n-    case Op_VectorMaskLastTrue:\n-    case Op_VectorMaskTrueCount:\n-    case Op_VectorMaskToLong:\n-      if (UseAVX < 1) {\n-         return false;\n+      return 0;\n+    } else if (dst_first_rc == rc_kreg) {\n+      if ((src_first & 1) == 0 && src_first + 1 == src_second &&\n+          (dst_first & 1) == 0 && dst_first + 1 == dst_second) {\n+        \/\/ 64-bit\n+        if (masm) {\n+          __ kmov(as_KRegister(Matcher::_regEncode[dst_first]), as_Register(Matcher::_regEncode[src_first]));\n+  #ifndef PRODUCT\n+        } else {\n+           st->print(\"kmovq   %s, %s\\t# spill\",\n+                       Matcher::regName[dst_first],\n+                       Matcher::regName[src_first]);\n+  #endif\n+        }\n@@ -1535,8 +2400,34 @@\n-      break;\n-    case Op_RoundF:\n-    case Op_RoundD:\n-      break;\n-    case Op_CopySignD:\n-    case Op_CopySignF:\n-      if (UseAVX < 3)  {\n-        return false;\n+      Unimplemented();\n+      return 0;\n+    }\n+  } else if (src_first_rc == rc_float) {\n+    \/\/ xmm ->\n+    if (dst_first_rc == rc_stack) {\n+      \/\/ xmm -> mem\n+      if ((src_first & 1) == 0 && src_first + 1 == src_second &&\n+          (dst_first & 1) == 0 && dst_first + 1 == dst_second) {\n+        \/\/ 64-bit\n+        int offset = ra_->reg2offset(dst_first);\n+        if (masm) {\n+          __ movdbl( Address(rsp, offset), as_XMMRegister(Matcher::_regEncode[src_first]));\n+#ifndef PRODUCT\n+        } else {\n+          st->print(\"movsd   [rsp + #%d], %s\\t# spill\",\n+                     offset,\n+                     Matcher::regName[src_first]);\n+#endif\n+        }\n+      } else {\n+        \/\/ 32-bit\n+        assert(!((src_first & 1) == 0 && src_first + 1 == src_second), \"no transform\");\n+        assert(!((dst_first & 1) == 0 && dst_first + 1 == dst_second), \"no transform\");\n+        int offset = ra_->reg2offset(dst_first);\n+        if (masm) {\n+          __ movflt(Address(rsp, offset), as_XMMRegister(Matcher::_regEncode[src_first]));\n+#ifndef PRODUCT\n+        } else {\n+          st->print(\"movss   [rsp + #%d], %s\\t# spill\",\n+                     offset,\n+                     Matcher::regName[src_first]);\n+#endif\n+        }\n@@ -1544,2 +2435,28 @@\n-      if (!VM_Version::supports_avx512vl()) {\n-        return false;\n+      return 0;\n+    } else if (dst_first_rc == rc_int) {\n+      \/\/ xmm -> gpr\n+      if ((src_first & 1) == 0 && src_first + 1 == src_second &&\n+          (dst_first & 1) == 0 && dst_first + 1 == dst_second) {\n+        \/\/ 64-bit\n+        if (masm) {\n+          __ movdq( as_Register(Matcher::_regEncode[dst_first]), as_XMMRegister(Matcher::_regEncode[src_first]));\n+#ifndef PRODUCT\n+        } else {\n+          st->print(\"movdq   %s, %s\\t# spill\",\n+                     Matcher::regName[dst_first],\n+                     Matcher::regName[src_first]);\n+#endif\n+        }\n+      } else {\n+        \/\/ 32-bit\n+        assert(!((src_first & 1) == 0 && src_first + 1 == src_second), \"no transform\");\n+        assert(!((dst_first & 1) == 0 && dst_first + 1 == dst_second), \"no transform\");\n+        if (masm) {\n+          __ movdl( as_Register(Matcher::_regEncode[dst_first]), as_XMMRegister(Matcher::_regEncode[src_first]));\n+#ifndef PRODUCT\n+        } else {\n+          st->print(\"movdl   %s, %s\\t# spill\",\n+                     Matcher::regName[dst_first],\n+                     Matcher::regName[src_first]);\n+#endif\n+        }\n@@ -1547,5 +2464,30 @@\n-      break;\n-    case Op_CompressBits:\n-    case Op_ExpandBits:\n-      if (!VM_Version::supports_bmi2()) {\n-        return false;\n+      return 0;\n+    } else if (dst_first_rc == rc_float) {\n+      \/\/ xmm -> xmm\n+      if ((src_first & 1) == 0 && src_first + 1 == src_second &&\n+          (dst_first & 1) == 0 && dst_first + 1 == dst_second) {\n+        \/\/ 64-bit\n+        if (masm) {\n+          __ movdbl( as_XMMRegister(Matcher::_regEncode[dst_first]), as_XMMRegister(Matcher::_regEncode[src_first]));\n+#ifndef PRODUCT\n+        } else {\n+          st->print(\"%s  %s, %s\\t# spill\",\n+                     UseXmmRegToRegMoveAll ? \"movapd\" : \"movsd \",\n+                     Matcher::regName[dst_first],\n+                     Matcher::regName[src_first]);\n+#endif\n+        }\n+      } else {\n+        \/\/ 32-bit\n+        assert(!((src_first & 1) == 0 && src_first + 1 == src_second), \"no transform\");\n+        assert(!((dst_first & 1) == 0 && dst_first + 1 == dst_second), \"no transform\");\n+        if (masm) {\n+          __ movflt( as_XMMRegister(Matcher::_regEncode[dst_first]), as_XMMRegister(Matcher::_regEncode[src_first]));\n+#ifndef PRODUCT\n+        } else {\n+          st->print(\"%s  %s, %s\\t# spill\",\n+                     UseXmmRegToRegMoveAll ? \"movaps\" : \"movss \",\n+                     Matcher::regName[dst_first],\n+                     Matcher::regName[src_first]);\n+#endif\n+        }\n@@ -1553,4 +2495,21 @@\n-      break;\n-    case Op_CompressM:\n-      if (!VM_Version::supports_avx512vl() || !VM_Version::supports_bmi2()) {\n-        return false;\n+      return 0;\n+    } else if (dst_first_rc == rc_kreg) {\n+      assert(false, \"Illegal spilling\");\n+      return 0;\n+    }\n+  } else if (src_first_rc == rc_kreg) {\n+    if (dst_first_rc == rc_stack) {\n+      \/\/ mem -> kreg\n+      if ((src_first & 1) == 0 && src_first + 1 == src_second &&\n+          (dst_first & 1) == 0 && dst_first + 1 == dst_second) {\n+        \/\/ 64-bit\n+        int offset = ra_->reg2offset(dst_first);\n+        if (masm) {\n+          __ kmov(Address(rsp, offset), as_KRegister(Matcher::_regEncode[src_first]));\n+#ifndef PRODUCT\n+        } else {\n+          st->print(\"kmovq   [rsp + #%d] , %s\\t# spill\",\n+                     offset,\n+                     Matcher::regName[src_first]);\n+#endif\n+        }\n@@ -1558,5 +2517,14 @@\n-      break;\n-    case Op_ConvF2HF:\n-    case Op_ConvHF2F:\n-      if (!VM_Version::supports_float16()) {\n-        return false;\n+      return 0;\n+    } else if (dst_first_rc == rc_int) {\n+      if ((src_first & 1) == 0 && src_first + 1 == src_second &&\n+          (dst_first & 1) == 0 && dst_first + 1 == dst_second) {\n+        \/\/ 64-bit\n+        if (masm) {\n+          __ kmov(as_Register(Matcher::_regEncode[dst_first]), as_KRegister(Matcher::_regEncode[src_first]));\n+#ifndef PRODUCT\n+        } else {\n+         st->print(\"kmovq   %s, %s\\t# spill\",\n+                     Matcher::regName[dst_first],\n+                     Matcher::regName[src_first]);\n+#endif\n+        }\n@@ -1564,5 +2532,15 @@\n-      break;\n-    case Op_VectorCastF2HF:\n-    case Op_VectorCastHF2F:\n-      if (!VM_Version::supports_f16c() && !VM_Version::supports_evex()) {\n-        return false;\n+      Unimplemented();\n+      return 0;\n+    } else if (dst_first_rc == rc_kreg) {\n+      if ((src_first & 1) == 0 && src_first + 1 == src_second &&\n+          (dst_first & 1) == 0 && dst_first + 1 == dst_second) {\n+        \/\/ 64-bit\n+        if (masm) {\n+          __ kmov(as_KRegister(Matcher::_regEncode[dst_first]), as_KRegister(Matcher::_regEncode[src_first]));\n+#ifndef PRODUCT\n+        } else {\n+         st->print(\"kmovq   %s, %s\\t# spill\",\n+                     Matcher::regName[dst_first],\n+                     Matcher::regName[src_first]);\n+#endif\n+        }\n@@ -1570,1 +2548,5 @@\n-      break;\n+      return 0;\n+    } else if (dst_first_rc == rc_float) {\n+      assert(false, \"Illegal spill\");\n+      return 0;\n+    }\n@@ -1572,1 +2554,4 @@\n-  return true;  \/\/ Match rules are supported by default.\n+\n+  assert(0,\" foo \");\n+  Unimplemented();\n+  return 0;\n@@ -1575,1 +2560,5 @@\n-\/\/------------------------------------------------------------------------\n+#ifndef PRODUCT\n+void MachSpillCopyNode::format(PhaseRegAlloc *ra_, outputStream* st) const {\n+  implementation(nullptr, ra_, false, st);\n+}\n+#endif\n@@ -1577,3 +2566,2 @@\n-static inline bool is_pop_count_instr_target(BasicType bt) {\n-  return (is_subword_type(bt) && VM_Version::supports_avx512_bitalg()) ||\n-         (is_non_subword_integral_type(bt) && VM_Version::supports_avx512_vpopcntdq());\n+void MachSpillCopyNode::emit(C2_MacroAssembler *masm, PhaseRegAlloc *ra_) const {\n+  implementation(masm, ra_, false, nullptr);\n@@ -1582,2 +2570,2 @@\n-bool Matcher::match_rule_supported_auto_vectorization(int opcode, int vlen, BasicType bt) {\n-  return match_rule_supported_vector(opcode, vlen, bt);\n+uint MachSpillCopyNode::size(PhaseRegAlloc *ra_) const {\n+  return MachNode::size(ra_);\n@@ -1586,5 +2574,26 @@\n-\/\/ Identify extra cases that we might want to provide match rules for vector nodes and\n-\/\/ other intrinsics guarded with vector length (vlen) and element type (bt).\n-bool Matcher::match_rule_supported_vector(int opcode, int vlen, BasicType bt) {\n-  if (!match_rule_supported(opcode)) {\n-    return false;\n+\/\/=============================================================================\n+#ifndef PRODUCT\n+void BoxLockNode::format(PhaseRegAlloc* ra_, outputStream* st) const\n+{\n+  int offset = ra_->reg2offset(in_RegMask(0).find_first_elem());\n+  int reg = ra_->get_reg_first(this);\n+  st->print(\"leaq    %s, [rsp + #%d]\\t# box lock\",\n+            Matcher::regName[reg], offset);\n+}\n+#endif\n+\n+void BoxLockNode::emit(C2_MacroAssembler* masm, PhaseRegAlloc* ra_) const\n+{\n+  int offset = ra_->reg2offset(in_RegMask(0).find_first_elem());\n+  int reg = ra_->get_encode(this);\n+\n+  __ lea(as_Register(reg), Address(rsp, offset));\n+}\n+\n+uint BoxLockNode::size(PhaseRegAlloc *ra_) const\n+{\n+  int offset = ra_->reg2offset(in_RegMask(0).find_first_elem());\n+  if (ra_->get_encode(this) > 15) {\n+    return (offset < 0x80) ? 6 : 9; \/\/ REX2\n+  } else {\n+    return (offset < 0x80) ? 5 : 8; \/\/ REX\n@@ -1592,10 +2601,12 @@\n-  \/\/ Matcher::vector_size_supported() restricts vector sizes in the following way (see Matcher::vector_width_in_bytes):\n-  \/\/   * SSE2 supports 128bit vectors for all types;\n-  \/\/   * AVX1 supports 256bit vectors only for FLOAT and DOUBLE types;\n-  \/\/   * AVX2 supports 256bit vectors for all types;\n-  \/\/   * AVX512F supports 512bit vectors only for INT, FLOAT, and DOUBLE types;\n-  \/\/   * AVX512BW supports 512bit vectors for BYTE, SHORT, and CHAR types.\n-  \/\/ There's also a limit on minimum vector size supported: 2 elements (or 4 bytes for BYTE).\n-  \/\/ And MaxVectorSize is taken into account as well.\n-  if (!vector_size_supported(bt, vlen)) {\n-    return false;\n+}\n+\n+\/\/=============================================================================\n+#ifndef PRODUCT\n+void MachUEPNode::format(PhaseRegAlloc* ra_, outputStream* st) const\n+{\n+  if (UseCompressedClassPointers) {\n+    st->print_cr(\"movl    rscratch1, [j_rarg0 + oopDesc::klass_offset_in_bytes()]\\t# compressed klass\");\n+    st->print_cr(\"\\tcmpl    rscratch1, [rax + CompiledICData::speculated_klass_offset()]\\t # Inline cache check\");\n+  } else {\n+    st->print_cr(\"movq    rscratch1, [j_rarg0 + oopDesc::klass_offset_in_bytes()]\\t# compressed klass\");\n+    st->print_cr(\"\\tcmpq    rscratch1, [rax + CompiledICData::speculated_klass_offset()]\\t # Inline cache check\");\n@@ -1603,310 +2614,144 @@\n-  \/\/ Special cases which require vector length follow:\n-  \/\/   * implementation limitations\n-  \/\/   * some 512bit vector operations on FLOAT and DOUBLE types require AVX512DQ\n-  \/\/   * 128bit vroundpd instruction is present only in AVX1\n-  int size_in_bits = vlen * type2aelembytes(bt) * BitsPerByte;\n-  switch (opcode) {\n-    case Op_MaxVHF:\n-    case Op_MinVHF:\n-      if (!VM_Version::supports_avx512bw()) {\n-        return false;\n-      }\n-    case Op_AddVHF:\n-    case Op_DivVHF:\n-    case Op_FmaVHF:\n-    case Op_MulVHF:\n-    case Op_SubVHF:\n-    case Op_SqrtVHF:\n-      if (size_in_bits < 512 && !VM_Version::supports_avx512vl()) {\n-        return false;\n-      }\n-      if (!VM_Version::supports_avx512_fp16()) {\n-        return false;\n-      }\n-      break;\n-    case Op_AbsVF:\n-    case Op_NegVF:\n-      if ((vlen == 16) && (VM_Version::supports_avx512dq() == false)) {\n-        return false; \/\/ 512bit vandps and vxorps are not available\n-      }\n-      break;\n-    case Op_AbsVD:\n-    case Op_NegVD:\n-      if ((vlen == 8) && (VM_Version::supports_avx512dq() == false)) {\n-        return false; \/\/ 512bit vpmullq, vandpd and vxorpd are not available\n-      }\n-      break;\n-    case Op_RotateRightV:\n-    case Op_RotateLeftV:\n-      if (bt != T_INT && bt != T_LONG) {\n-        return false;\n-      } \/\/ fallthrough\n-    case Op_MacroLogicV:\n-      if (!VM_Version::supports_evex() ||\n-          ((size_in_bits != 512) && !VM_Version::supports_avx512vl())) {\n-        return false;\n-      }\n-      break;\n-    case Op_ClearArray:\n-    case Op_VectorMaskGen:\n-    case Op_VectorCmpMasked:\n-      if (!VM_Version::supports_avx512bw()) {\n-        return false;\n-      }\n-      if ((size_in_bits != 512) && !VM_Version::supports_avx512vl()) {\n-        return false;\n-      }\n-      break;\n-    case Op_LoadVectorMasked:\n-    case Op_StoreVectorMasked:\n-      if (!VM_Version::supports_avx512bw() && (is_subword_type(bt) || UseAVX < 1)) {\n-        return false;\n-      }\n-      break;\n-    case Op_UMinV:\n-    case Op_UMaxV:\n-      if (UseAVX == 0) {\n-        return false;\n-      }\n-      break;\n-    case Op_MaxV:\n-    case Op_MinV:\n-      if (UseSSE < 4 && is_integral_type(bt)) {\n-        return false;\n-      }\n-      if ((bt == T_FLOAT || bt == T_DOUBLE)) {\n-          \/\/ Float\/Double intrinsics are enabled for AVX family currently.\n-          if (UseAVX == 0) {\n-            return false;\n-          }\n-          if (UseAVX > 2 && (!VM_Version::supports_avx512dq() && size_in_bits == 512)) { \/\/ 512 bit Float\/Double intrinsics need AVX512DQ\n-            return false;\n-          }\n-      }\n-      break;\n-    case Op_CallLeafVector:\n-      if (size_in_bits == 512 && !VM_Version::supports_avx512vlbwdq()) {\n-        return false;\n-      }\n-      break;\n-    case Op_AddReductionVI:\n-      if (bt == T_INT && (UseSSE < 3 || !VM_Version::supports_ssse3())) {\n-        return false;\n-      }\n-      \/\/ fallthrough\n-    case Op_AndReductionV:\n-    case Op_OrReductionV:\n-    case Op_XorReductionV:\n-      if (is_subword_type(bt) && (UseSSE < 4)) {\n-        return false;\n-      }\n-      break;\n-    case Op_MinReductionV:\n-    case Op_MaxReductionV:\n-      if ((bt == T_INT || is_subword_type(bt)) && UseSSE < 4) {\n-        return false;\n-      } else if (bt == T_LONG && (UseAVX < 3 || !VM_Version::supports_avx512vlbwdq())) {\n-        return false;\n-      }\n-      \/\/ Float\/Double intrinsics enabled for AVX family.\n-      if (UseAVX == 0 && (bt == T_FLOAT || bt == T_DOUBLE)) {\n-        return false;\n-      }\n-      if (UseAVX > 2 && (!VM_Version::supports_avx512dq() && size_in_bits == 512)) {\n-        return false;\n-      }\n-      break;\n-    case Op_VectorTest:\n-      if (UseSSE < 4) {\n-        return false; \/\/ Implementation limitation\n-      } else if (size_in_bits < 32) {\n-        return false; \/\/ Implementation limitation\n-      }\n-      break;\n-    case Op_VectorLoadShuffle:\n-    case Op_VectorRearrange:\n-      if(vlen == 2) {\n-        return false; \/\/ Implementation limitation due to how shuffle is loaded\n-      } else if (size_in_bits == 256 && UseAVX < 2) {\n-        return false; \/\/ Implementation limitation\n-      }\n-      break;\n-    case Op_VectorLoadMask:\n-    case Op_VectorMaskCast:\n-      if (size_in_bits == 256 && UseAVX < 2) {\n-        return false; \/\/ Implementation limitation\n-      }\n-      \/\/ fallthrough\n-    case Op_VectorStoreMask:\n-      if (vlen == 2) {\n-        return false; \/\/ Implementation limitation\n-      }\n-      break;\n-    case Op_PopulateIndex:\n-      if (size_in_bits > 256 && !VM_Version::supports_avx512bw()) {\n-        return false;\n-      }\n-      break;\n-    case Op_VectorCastB2X:\n-    case Op_VectorCastS2X:\n-    case Op_VectorCastI2X:\n-      if (bt != T_DOUBLE && size_in_bits == 256 && UseAVX < 2) {\n-        return false;\n-      }\n-      break;\n-    case Op_VectorCastL2X:\n-      if (is_integral_type(bt) && size_in_bits == 256 && UseAVX < 2) {\n-        return false;\n-      } else if (!is_integral_type(bt) && !VM_Version::supports_avx512dq()) {\n-        return false;\n-      }\n-      break;\n-    case Op_VectorCastF2X: {\n-        \/\/ As per JLS section 5.1.3 narrowing conversion to sub-word types\n-        \/\/ happen after intermediate conversion to integer and special handling\n-        \/\/ code needs AVX2 vpcmpeqd instruction for 256 bit vectors.\n-        int src_size_in_bits = type2aelembytes(T_FLOAT) * vlen * BitsPerByte;\n-        if (is_integral_type(bt) && src_size_in_bits == 256 && UseAVX < 2) {\n-          return false;\n-        }\n-      }\n-      \/\/ fallthrough\n-    case Op_VectorCastD2X:\n-      if (bt == T_LONG && !VM_Version::supports_avx512dq()) {\n-        return false;\n-      }\n-      break;\n-    case Op_VectorCastF2HF:\n-    case Op_VectorCastHF2F:\n-      if (!VM_Version::supports_f16c() &&\n-         ((!VM_Version::supports_evex() ||\n-         ((size_in_bits != 512) && !VM_Version::supports_avx512vl())))) {\n-        return false;\n-      }\n-      break;\n-    case Op_RoundVD:\n-      if (!VM_Version::supports_avx512dq()) {\n-        return false;\n-      }\n-      break;\n-    case Op_MulReductionVI:\n-      if (bt == T_BYTE && size_in_bits == 512 && !VM_Version::supports_avx512bw()) {\n-        return false;\n-      }\n-      break;\n-    case Op_LoadVectorGatherMasked:\n-      if (!is_subword_type(bt) && size_in_bits < 512 && !VM_Version::supports_avx512vl()) {\n-        return false;\n-      }\n-      if (is_subword_type(bt) &&\n-         ((size_in_bits > 256 && !VM_Version::supports_avx512bw()) ||\n-          (size_in_bits < 64)                                      ||\n-          (bt == T_SHORT && !VM_Version::supports_bmi2()))) {\n-        return false;\n-      }\n-      break;\n-    case Op_StoreVectorScatterMasked:\n-    case Op_StoreVectorScatter:\n-      if (is_subword_type(bt)) {\n-        return false;\n-      } else if (size_in_bits < 512 && !VM_Version::supports_avx512vl()) {\n-        return false;\n-      }\n-      \/\/ fallthrough\n-    case Op_LoadVectorGather:\n-      if (!is_subword_type(bt) && size_in_bits == 64) {\n-        return false;\n-      }\n-      if (is_subword_type(bt) && size_in_bits < 64) {\n-        return false;\n-      }\n-      break;\n-    case Op_SaturatingAddV:\n-    case Op_SaturatingSubV:\n-      if (UseAVX < 1) {\n-        return false; \/\/ Implementation limitation\n-      }\n-      if (is_subword_type(bt) && size_in_bits == 512 && !VM_Version::supports_avx512bw()) {\n-        return false;\n-      }\n-      break;\n-    case Op_SelectFromTwoVector:\n-       if (size_in_bits < 128) {\n-         return false;\n-       }\n-       if ((size_in_bits < 512 && !VM_Version::supports_avx512vl())) {\n-         return false;\n-       }\n-       if (bt == T_SHORT && !VM_Version::supports_avx512bw()) {\n-         return false;\n-       }\n-       if (bt == T_BYTE && !VM_Version::supports_avx512_vbmi()) {\n-         return false;\n-       }\n-       if ((bt == T_INT || bt == T_FLOAT || bt == T_DOUBLE) && !VM_Version::supports_evex()) {\n-         return false;\n-       }\n-       break;\n-    case Op_MaskAll:\n-      if (!VM_Version::supports_evex()) {\n-        return false;\n-      }\n-      if ((vlen > 16 || is_subword_type(bt)) && !VM_Version::supports_avx512bw()) {\n-        return false;\n-      }\n-      if (size_in_bits < 512 && !VM_Version::supports_avx512vl()) {\n-        return false;\n-      }\n-      break;\n-    case Op_VectorMaskCmp:\n-      if (vlen < 2 || size_in_bits < 32) {\n-        return false;\n-      }\n-      break;\n-    case Op_CompressM:\n-      if (UseAVX < 3 || !VM_Version::supports_bmi2()) {\n-        return false;\n-      }\n-      break;\n-    case Op_CompressV:\n-    case Op_ExpandV:\n-      if (is_subword_type(bt) && !VM_Version::supports_avx512_vbmi2()) {\n-        return false;\n-      }\n-      if (size_in_bits < 128 ) {\n-        return false;\n-      }\n-    case Op_VectorLongToMask:\n-      if (UseAVX < 1) {\n-        return false;\n-      }\n-      if (UseAVX < 3 && !VM_Version::supports_bmi2()) {\n-        return false;\n-      }\n-      break;\n-    case Op_SignumVD:\n-    case Op_SignumVF:\n-      if (UseAVX < 1) {\n-        return false;\n-      }\n-      break;\n-    case Op_PopCountVI:\n-    case Op_PopCountVL: {\n-        if (!is_pop_count_instr_target(bt) &&\n-            (size_in_bits == 512) && !VM_Version::supports_avx512bw()) {\n-          return false;\n-        }\n-      }\n-      break;\n-    case Op_ReverseV:\n-    case Op_ReverseBytesV:\n-      if (UseAVX < 2) {\n-        return false;\n-      }\n-      break;\n-    case Op_CountTrailingZerosV:\n-    case Op_CountLeadingZerosV:\n-      if (UseAVX < 2) {\n-        return false;\n-      }\n-      break;\n+  st->print_cr(\"\\tjne     SharedRuntime::_ic_miss_stub\");\n+}\n+#endif\n+\n+void MachUEPNode::emit(C2_MacroAssembler* masm, PhaseRegAlloc* ra_) const\n+{\n+  __ ic_check(InteriorEntryAlignment);\n+}\n+\n+uint MachUEPNode::size(PhaseRegAlloc* ra_) const\n+{\n+  return MachNode::size(ra_); \/\/ too many variables; just compute it\n+                              \/\/ the hard way\n+}\n+\n+\n+\/\/=============================================================================\n+\n+bool Matcher::supports_vector_calling_convention(void) {\n+  return EnableVectorSupport;\n+}\n+\n+OptoRegPair Matcher::vector_return_value(uint ideal_reg) {\n+  assert(EnableVectorSupport, \"sanity\");\n+  int lo = XMM0_num;\n+  int hi = XMM0b_num;\n+  if (ideal_reg == Op_VecX) hi = XMM0d_num;\n+  else if (ideal_reg == Op_VecY) hi = XMM0h_num;\n+  else if (ideal_reg == Op_VecZ) hi = XMM0p_num;\n+  return OptoRegPair(hi, lo);\n+}\n+\n+\/\/ Is this branch offset short enough that a short branch can be used?\n+\/\/\n+\/\/ NOTE: If the platform does not provide any short branch variants, then\n+\/\/       this method should return false for offset 0.\n+bool Matcher::is_short_branch_offset(int rule, int br_size, int offset) {\n+  \/\/ The passed offset is relative to address of the branch.\n+  \/\/ On 86 a branch displacement is calculated relative to address\n+  \/\/ of a next instruction.\n+  offset -= br_size;\n+\n+  \/\/ the short version of jmpConUCF2 contains multiple branches,\n+  \/\/ making the reach slightly less\n+  if (rule == jmpConUCF2_rule)\n+    return (-126 <= offset && offset <= 125);\n+  return (-128 <= offset && offset <= 127);\n+}\n+\n+\/\/ Return whether or not this register is ever used as an argument.\n+\/\/ This function is used on startup to build the trampoline stubs in\n+\/\/ generateOptoStub.  Registers not mentioned will be killed by the VM\n+\/\/ call in the trampoline, and arguments in those registers not be\n+\/\/ available to the callee.\n+bool Matcher::can_be_java_arg(int reg)\n+{\n+  return\n+    reg ==  RDI_num || reg == RDI_H_num ||\n+    reg ==  RSI_num || reg == RSI_H_num ||\n+    reg ==  RDX_num || reg == RDX_H_num ||\n+    reg ==  RCX_num || reg == RCX_H_num ||\n+    reg ==   R8_num || reg ==  R8_H_num ||\n+    reg ==   R9_num || reg ==  R9_H_num ||\n+    reg ==  R12_num || reg == R12_H_num ||\n+    reg == XMM0_num || reg == XMM0b_num ||\n+    reg == XMM1_num || reg == XMM1b_num ||\n+    reg == XMM2_num || reg == XMM2b_num ||\n+    reg == XMM3_num || reg == XMM3b_num ||\n+    reg == XMM4_num || reg == XMM4b_num ||\n+    reg == XMM5_num || reg == XMM5b_num ||\n+    reg == XMM6_num || reg == XMM6b_num ||\n+    reg == XMM7_num || reg == XMM7b_num;\n+}\n+\n+bool Matcher::is_spillable_arg(int reg)\n+{\n+  return can_be_java_arg(reg);\n+}\n+\n+uint Matcher::int_pressure_limit()\n+{\n+  return (INTPRESSURE == -1) ? _INT_REG_mask.size() : INTPRESSURE;\n+}\n+\n+uint Matcher::float_pressure_limit()\n+{\n+  \/\/ After experiment around with different values, the following default threshold\n+  \/\/ works best for LCM's register pressure scheduling on x64.\n+  uint dec_count  = VM_Version::supports_evex() ? 4 : 2;\n+  uint default_float_pressure_threshold = _FLOAT_REG_mask.size() - dec_count;\n+  return (FLOATPRESSURE == -1) ? default_float_pressure_threshold : FLOATPRESSURE;\n+}\n+\n+bool Matcher::use_asm_for_ldiv_by_con( jlong divisor ) {\n+  \/\/ In 64 bit mode a code which use multiply when\n+  \/\/ devisor is constant is faster than hardware\n+  \/\/ DIV instruction (it uses MulHiL).\n+  return false;\n+}\n+\n+\/\/ Register for DIVI projection of divmodI\n+const RegMask& Matcher::divI_proj_mask() {\n+  return INT_RAX_REG_mask();\n+}\n+\n+\/\/ Register for MODI projection of divmodI\n+const RegMask& Matcher::modI_proj_mask() {\n+  return INT_RDX_REG_mask();\n+}\n+\n+\/\/ Register for DIVL projection of divmodL\n+const RegMask& Matcher::divL_proj_mask() {\n+  return LONG_RAX_REG_mask();\n+}\n+\n+\/\/ Register for MODL projection of divmodL\n+const RegMask& Matcher::modL_proj_mask() {\n+  return LONG_RDX_REG_mask();\n+}\n+\n+%}\n+\n+source_hpp %{\n+\/\/ Header information of the source block.\n+\/\/ Method declarations\/definitions which are used outside\n+\/\/ the ad-scope can conveniently be defined here.\n+\/\/\n+\/\/ To keep related declarations\/definitions\/uses close together,\n+\/\/ we switch between source %{ }% and source_hpp %{ }% freely as needed.\n+\n+#include \"runtime\/vm_version.hpp\"\n+\n+class NativeJump;\n+\n+class CallStubImpl {\n+\n+  \/\/--------------------------------------------------------------\n+  \/\/---<  Used for optimization in Compile::shorten_branches  >---\n+  \/\/--------------------------------------------------------------\n+\n+ public:\n+  \/\/ Size of call trampoline stub.\n+  static uint size_call_trampoline() {\n+    return 0; \/\/ no call trampolines on this platform\n@@ -1914,1 +2759,133 @@\n-  return true;  \/\/ Per default match rules are supported.\n+\n+  \/\/ number of relocations needed by a call trampoline stub\n+  static uint reloc_call_trampoline() {\n+    return 0; \/\/ no call trampolines on this platform\n+  }\n+};\n+\n+class HandlerImpl {\n+\n+ public:\n+\n+  static int emit_exception_handler(C2_MacroAssembler *masm);\n+  static int emit_deopt_handler(C2_MacroAssembler* masm);\n+\n+  static uint size_exception_handler() {\n+    \/\/ NativeCall instruction size is the same as NativeJump.\n+    \/\/ exception handler starts out as jump and can be patched to\n+    \/\/ a call be deoptimization.  (4932387)\n+    \/\/ Note that this value is also credited (in output.cpp) to\n+    \/\/ the size of the code section.\n+    return NativeJump::instruction_size;\n+  }\n+\n+  static uint size_deopt_handler() {\n+    \/\/ three 5 byte instructions plus one move for unreachable address.\n+    return 15+3;\n+  }\n+};\n+\n+inline Assembler::AvxVectorLen vector_length_encoding(int bytes) {\n+  switch(bytes) {\n+    case  4: \/\/ fall-through\n+    case  8: \/\/ fall-through\n+    case 16: return Assembler::AVX_128bit;\n+    case 32: return Assembler::AVX_256bit;\n+    case 64: return Assembler::AVX_512bit;\n+\n+    default: {\n+      ShouldNotReachHere();\n+      return Assembler::AVX_NoVec;\n+    }\n+  }\n+}\n+\n+static inline Assembler::AvxVectorLen vector_length_encoding(const Node* n) {\n+  return vector_length_encoding(Matcher::vector_length_in_bytes(n));\n+}\n+\n+static inline Assembler::AvxVectorLen vector_length_encoding(const MachNode* use, MachOper* opnd) {\n+  uint def_idx = use->operand_index(opnd);\n+  Node* def = use->in(def_idx);\n+  return vector_length_encoding(def);\n+}\n+\n+static inline bool is_vector_popcount_predicate(BasicType bt) {\n+  return (is_subword_type(bt) && VM_Version::supports_avx512_bitalg()) ||\n+         (is_non_subword_integral_type(bt) && VM_Version::supports_avx512_vpopcntdq());\n+}\n+\n+static inline bool is_clz_non_subword_predicate_evex(BasicType bt, int vlen_bytes) {\n+  return is_non_subword_integral_type(bt) && VM_Version::supports_avx512cd() &&\n+           (VM_Version::supports_avx512vl() || vlen_bytes == 64);\n+}\n+\n+class Node::PD {\n+public:\n+  enum NodeFlags {\n+    Flag_intel_jcc_erratum    = Node::_last_flag << 1,\n+    Flag_sets_carry_flag      = Node::_last_flag << 2,\n+    Flag_sets_parity_flag     = Node::_last_flag << 3,\n+    Flag_sets_zero_flag       = Node::_last_flag << 4,\n+    Flag_sets_overflow_flag   = Node::_last_flag << 5,\n+    Flag_sets_sign_flag       = Node::_last_flag << 6,\n+    Flag_clears_carry_flag    = Node::_last_flag << 7,\n+    Flag_clears_parity_flag   = Node::_last_flag << 8,\n+    Flag_clears_zero_flag     = Node::_last_flag << 9,\n+    Flag_clears_overflow_flag = Node::_last_flag << 10,\n+    Flag_clears_sign_flag     = Node::_last_flag << 11,\n+    _last_flag                = Flag_clears_sign_flag\n+  };\n+};\n+\n+%} \/\/ end source_hpp\n+\n+source %{\n+\n+#include \"opto\/addnode.hpp\"\n+#include \"c2_intelJccErratum_x86.hpp\"\n+\n+void PhaseOutput::pd_perform_mach_node_analysis() {\n+  if (VM_Version::has_intel_jcc_erratum()) {\n+    int extra_padding = IntelJccErratum::tag_affected_machnodes(C, C->cfg(), C->regalloc());\n+    _buf_sizes._code += extra_padding;\n+  }\n+}\n+\n+int MachNode::pd_alignment_required() const {\n+  if (VM_Version::has_intel_jcc_erratum() && IntelJccErratum::is_jcc_erratum_branch(this)) {\n+    \/\/ Conservatively add worst case padding. We assume that relocInfo::addr_unit() is 1 on x86.\n+    return IntelJccErratum::largest_jcc_size() + 1;\n+  } else {\n+    return 1;\n+  }\n+}\n+\n+int MachNode::compute_padding(int current_offset) const {\n+  if (flags() & Node::PD::Flag_intel_jcc_erratum) {\n+    Compile* C = Compile::current();\n+    PhaseOutput* output = C->output();\n+    Block* block = output->block();\n+    int index = output->index();\n+    return IntelJccErratum::compute_padding(current_offset, this, block, index, C->regalloc());\n+  } else {\n+    return 0;\n+  }\n+}\n+\n+\/\/ Emit exception handler code.\n+\/\/ Stuff framesize into a register and call a VM stub routine.\n+int HandlerImpl::emit_exception_handler(C2_MacroAssembler* masm) {\n+\n+  \/\/ Note that the code buffer's insts_mark is always relative to insts.\n+  \/\/ That's why we must use the macroassembler to generate a handler.\n+  address base = __ start_a_stub(size_exception_handler());\n+  if (base == nullptr) {\n+    ciEnv::current()->record_failure(\"CodeCache is full\");\n+    return 0;  \/\/ CodeBuffer::expand failed\n+  }\n+  int offset = __ offset();\n+  __ jump(RuntimeAddress(OptoRuntime::exception_blob()->entry_point()));\n+  assert(__ offset() - offset <= (int) size_exception_handler(), \"overflow\");\n+  __ end_a_stub();\n+  return offset;\n@@ -1917,10 +2894,13251 @@\n-bool Matcher::match_rule_supported_vector_masked(int opcode, int vlen, BasicType bt) {\n-  \/\/ ADLC based match_rule_supported routine checks for the existence of pattern based\n-  \/\/ on IR opcode. Most of the unary\/binary\/ternary masked operation share the IR nodes\n-  \/\/ of their non-masked counterpart with mask edge being the differentiator.\n-  \/\/ This routine does a strict check on the existence of masked operation patterns\n-  \/\/ by returning a default false value for all the other opcodes apart from the\n-  \/\/ ones whose masked instruction patterns are defined in this file.\n-  if (!match_rule_supported_vector(opcode, vlen, bt)) {\n-    return false;\n-  }\n+\/\/ Emit deopt handler code.\n+int HandlerImpl::emit_deopt_handler(C2_MacroAssembler* masm) {\n+\n+  \/\/ Note that the code buffer's insts_mark is always relative to insts.\n+  \/\/ That's why we must use the macroassembler to generate a handler.\n+  address base = __ start_a_stub(size_deopt_handler());\n+  if (base == nullptr) {\n+    ciEnv::current()->record_failure(\"CodeCache is full\");\n+    return 0;  \/\/ CodeBuffer::expand failed\n+  }\n+  int offset = __ offset();\n+\n+  address the_pc = (address) __ pc();\n+  Label next;\n+  \/\/ push a \"the_pc\" on the stack without destroying any registers\n+  \/\/ as they all may be live.\n+\n+  \/\/ push address of \"next\"\n+  __ call(next, relocInfo::none); \/\/ reloc none is fine since it is a disp32\n+  __ bind(next);\n+  \/\/ adjust it so it matches \"the_pc\"\n+  __ subptr(Address(rsp, 0), __ offset() - offset);\n+\n+  __ jump(RuntimeAddress(SharedRuntime::deopt_blob()->unpack()));\n+  assert(__ offset() - offset <= (int) size_deopt_handler(), \"overflow %d\", (__ offset() - offset));\n+  __ end_a_stub();\n+  return offset;\n+}\n+\n+static Assembler::Width widthForType(BasicType bt) {\n+  if (bt == T_BYTE) {\n+    return Assembler::B;\n+  } else if (bt == T_SHORT) {\n+    return Assembler::W;\n+  } else if (bt == T_INT) {\n+    return Assembler::D;\n+  } else {\n+    assert(bt == T_LONG, \"not a long: %s\", type2name(bt));\n+    return Assembler::Q;\n+  }\n+}\n+\n+\/\/=============================================================================\n+\n+  \/\/ Float masks come from different places depending on platform.\n+  static address float_signmask()  { return StubRoutines::x86::float_sign_mask(); }\n+  static address float_signflip()  { return StubRoutines::x86::float_sign_flip(); }\n+  static address double_signmask() { return StubRoutines::x86::double_sign_mask(); }\n+  static address double_signflip() { return StubRoutines::x86::double_sign_flip(); }\n+  static address vector_short_to_byte_mask() { return StubRoutines::x86::vector_short_to_byte_mask(); }\n+  static address vector_int_to_byte_mask() { return StubRoutines::x86::vector_int_to_byte_mask(); }\n+  static address vector_byte_perm_mask() { return StubRoutines::x86::vector_byte_perm_mask(); }\n+  static address vector_long_sign_mask() { return StubRoutines::x86::vector_long_sign_mask(); }\n+  static address vector_all_bits_set() { return StubRoutines::x86::vector_all_bits_set(); }\n+  static address vector_int_mask_cmp_bits() { return StubRoutines::x86::vector_int_mask_cmp_bits(); }\n+  static address vector_int_to_short_mask() { return StubRoutines::x86::vector_int_to_short_mask(); }\n+  static address vector_byte_shufflemask() { return StubRoutines::x86::vector_byte_shuffle_mask(); }\n+  static address vector_short_shufflemask() { return StubRoutines::x86::vector_short_shuffle_mask(); }\n+  static address vector_int_shufflemask() { return StubRoutines::x86::vector_int_shuffle_mask(); }\n+  static address vector_long_shufflemask() { return StubRoutines::x86::vector_long_shuffle_mask(); }\n+  static address vector_32_bit_mask() { return StubRoutines::x86::vector_32_bit_mask(); }\n+  static address vector_64_bit_mask() { return StubRoutines::x86::vector_64_bit_mask(); }\n+  static address vector_float_signflip() { return StubRoutines::x86::vector_float_sign_flip();}\n+  static address vector_double_signflip() { return StubRoutines::x86::vector_double_sign_flip();}\n+\n+\/\/=============================================================================\n+bool Matcher::match_rule_supported(int opcode) {\n+  if (!has_match_rule(opcode)) {\n+    return false; \/\/ no match rule present\n+  }\n+  switch (opcode) {\n+    case Op_AbsVL:\n+    case Op_StoreVectorScatter:\n+      if (UseAVX < 3) {\n+        return false;\n+      }\n+      break;\n+    case Op_PopCountI:\n+    case Op_PopCountL:\n+      if (!UsePopCountInstruction) {\n+        return false;\n+      }\n+      break;\n+    case Op_PopCountVI:\n+      if (UseAVX < 2) {\n+        return false;\n+      }\n+      break;\n+    case Op_CompressV:\n+    case Op_ExpandV:\n+    case Op_PopCountVL:\n+      if (UseAVX < 2) {\n+        return false;\n+      }\n+      break;\n+    case Op_MulVI:\n+      if ((UseSSE < 4) && (UseAVX < 1)) { \/\/ only with SSE4_1 or AVX\n+        return false;\n+      }\n+      break;\n+    case Op_MulVL:\n+      if (UseSSE < 4) { \/\/ only with SSE4_1 or AVX\n+        return false;\n+      }\n+      break;\n+    case Op_MulReductionVL:\n+      if (VM_Version::supports_avx512dq() == false) {\n+        return false;\n+      }\n+      break;\n+    case Op_AbsVB:\n+    case Op_AbsVS:\n+    case Op_AbsVI:\n+    case Op_AddReductionVI:\n+    case Op_AndReductionV:\n+    case Op_OrReductionV:\n+    case Op_XorReductionV:\n+      if (UseSSE < 3) { \/\/ requires at least SSSE3\n+        return false;\n+      }\n+      break;\n+    case Op_MaxHF:\n+    case Op_MinHF:\n+      if (!VM_Version::supports_avx512vlbw()) {\n+        return false;\n+      }  \/\/ fallthrough\n+    case Op_AddHF:\n+    case Op_DivHF:\n+    case Op_FmaHF:\n+    case Op_MulHF:\n+    case Op_ReinterpretS2HF:\n+    case Op_ReinterpretHF2S:\n+    case Op_SubHF:\n+    case Op_SqrtHF:\n+      if (!VM_Version::supports_avx512_fp16()) {\n+        return false;\n+      }\n+      break;\n+    case Op_VectorLoadShuffle:\n+    case Op_VectorRearrange:\n+    case Op_MulReductionVI:\n+      if (UseSSE < 4) { \/\/ requires at least SSE4\n+        return false;\n+      }\n+      break;\n+    case Op_IsInfiniteF:\n+    case Op_IsInfiniteD:\n+      if (!VM_Version::supports_avx512dq()) {\n+        return false;\n+      }\n+      break;\n+    case Op_SqrtVD:\n+    case Op_SqrtVF:\n+    case Op_VectorMaskCmp:\n+    case Op_VectorCastB2X:\n+    case Op_VectorCastS2X:\n+    case Op_VectorCastI2X:\n+    case Op_VectorCastL2X:\n+    case Op_VectorCastF2X:\n+    case Op_VectorCastD2X:\n+    case Op_VectorUCastB2X:\n+    case Op_VectorUCastS2X:\n+    case Op_VectorUCastI2X:\n+    case Op_VectorMaskCast:\n+      if (UseAVX < 1) { \/\/ enabled for AVX only\n+        return false;\n+      }\n+      break;\n+    case Op_PopulateIndex:\n+      if (UseAVX < 2) {\n+        return false;\n+      }\n+      break;\n+    case Op_RoundVF:\n+      if (UseAVX < 2) { \/\/ enabled for AVX2 only\n+        return false;\n+      }\n+      break;\n+    case Op_RoundVD:\n+      if (UseAVX < 3) {\n+        return false;  \/\/ enabled for AVX3 only\n+      }\n+      break;\n+    case Op_CompareAndSwapL:\n+    case Op_CompareAndSwapP:\n+      break;\n+    case Op_StrIndexOf:\n+      if (!UseSSE42Intrinsics) {\n+        return false;\n+      }\n+      break;\n+    case Op_StrIndexOfChar:\n+      if (!UseSSE42Intrinsics) {\n+        return false;\n+      }\n+      break;\n+    case Op_OnSpinWait:\n+      if (VM_Version::supports_on_spin_wait() == false) {\n+        return false;\n+      }\n+      break;\n+    case Op_MulVB:\n+    case Op_LShiftVB:\n+    case Op_RShiftVB:\n+    case Op_URShiftVB:\n+    case Op_VectorInsert:\n+    case Op_VectorLoadMask:\n+    case Op_VectorStoreMask:\n+    case Op_VectorBlend:\n+      if (UseSSE < 4) {\n+        return false;\n+      }\n+      break;\n+    case Op_MaxD:\n+    case Op_MaxF:\n+    case Op_MinD:\n+    case Op_MinF:\n+      if (UseAVX < 1) { \/\/ enabled for AVX only\n+        return false;\n+      }\n+      break;\n+    case Op_CacheWB:\n+    case Op_CacheWBPreSync:\n+    case Op_CacheWBPostSync:\n+      if (!VM_Version::supports_data_cache_line_flush()) {\n+        return false;\n+      }\n+      break;\n+    case Op_ExtractB:\n+    case Op_ExtractL:\n+    case Op_ExtractI:\n+    case Op_RoundDoubleMode:\n+      if (UseSSE < 4) {\n+        return false;\n+      }\n+      break;\n+    case Op_RoundDoubleModeV:\n+      if (VM_Version::supports_avx() == false) {\n+        return false; \/\/ 128bit vroundpd is not available\n+      }\n+      break;\n+    case Op_LoadVectorGather:\n+    case Op_LoadVectorGatherMasked:\n+      if (UseAVX < 2) {\n+        return false;\n+      }\n+      break;\n+    case Op_FmaF:\n+    case Op_FmaD:\n+    case Op_FmaVD:\n+    case Op_FmaVF:\n+      if (!UseFMA) {\n+        return false;\n+      }\n+      break;\n+    case Op_MacroLogicV:\n+      if (UseAVX < 3 || !UseVectorMacroLogic) {\n+        return false;\n+      }\n+      break;\n+\n+    case Op_VectorCmpMasked:\n+    case Op_VectorMaskGen:\n+      if (UseAVX < 3 || !VM_Version::supports_bmi2()) {\n+        return false;\n+      }\n+      break;\n+    case Op_VectorMaskFirstTrue:\n+    case Op_VectorMaskLastTrue:\n+    case Op_VectorMaskTrueCount:\n+    case Op_VectorMaskToLong:\n+      if (UseAVX < 1) {\n+         return false;\n+      }\n+      break;\n+    case Op_RoundF:\n+    case Op_RoundD:\n+      break;\n+    case Op_CopySignD:\n+    case Op_CopySignF:\n+      if (UseAVX < 3)  {\n+        return false;\n+      }\n+      if (!VM_Version::supports_avx512vl()) {\n+        return false;\n+      }\n+      break;\n+    case Op_CompressBits:\n+    case Op_ExpandBits:\n+      if (!VM_Version::supports_bmi2()) {\n+        return false;\n+      }\n+      break;\n+    case Op_CompressM:\n+      if (!VM_Version::supports_avx512vl() || !VM_Version::supports_bmi2()) {\n+        return false;\n+      }\n+      break;\n+    case Op_ConvF2HF:\n+    case Op_ConvHF2F:\n+      if (!VM_Version::supports_float16()) {\n+        return false;\n+      }\n+      break;\n+    case Op_VectorCastF2HF:\n+    case Op_VectorCastHF2F:\n+      if (!VM_Version::supports_f16c() && !VM_Version::supports_evex()) {\n+        return false;\n+      }\n+      break;\n+  }\n+  return true;  \/\/ Match rules are supported by default.\n+}\n+\n+\/\/------------------------------------------------------------------------\n+\n+static inline bool is_pop_count_instr_target(BasicType bt) {\n+  return (is_subword_type(bt) && VM_Version::supports_avx512_bitalg()) ||\n+         (is_non_subword_integral_type(bt) && VM_Version::supports_avx512_vpopcntdq());\n+}\n+\n+bool Matcher::match_rule_supported_auto_vectorization(int opcode, int vlen, BasicType bt) {\n+  return match_rule_supported_vector(opcode, vlen, bt);\n+}\n+\n+\/\/ Identify extra cases that we might want to provide match rules for vector nodes and\n+\/\/ other intrinsics guarded with vector length (vlen) and element type (bt).\n+bool Matcher::match_rule_supported_vector(int opcode, int vlen, BasicType bt) {\n+  if (!match_rule_supported(opcode)) {\n+    return false;\n+  }\n+  \/\/ Matcher::vector_size_supported() restricts vector sizes in the following way (see Matcher::vector_width_in_bytes):\n+  \/\/   * SSE2 supports 128bit vectors for all types;\n+  \/\/   * AVX1 supports 256bit vectors only for FLOAT and DOUBLE types;\n+  \/\/   * AVX2 supports 256bit vectors for all types;\n+  \/\/   * AVX512F supports 512bit vectors only for INT, FLOAT, and DOUBLE types;\n+  \/\/   * AVX512BW supports 512bit vectors for BYTE, SHORT, and CHAR types.\n+  \/\/ There's also a limit on minimum vector size supported: 2 elements (or 4 bytes for BYTE).\n+  \/\/ And MaxVectorSize is taken into account as well.\n+  if (!vector_size_supported(bt, vlen)) {\n+    return false;\n+  }\n+  \/\/ Special cases which require vector length follow:\n+  \/\/   * implementation limitations\n+  \/\/   * some 512bit vector operations on FLOAT and DOUBLE types require AVX512DQ\n+  \/\/   * 128bit vroundpd instruction is present only in AVX1\n+  int size_in_bits = vlen * type2aelembytes(bt) * BitsPerByte;\n+  switch (opcode) {\n+    case Op_MaxVHF:\n+    case Op_MinVHF:\n+      if (!VM_Version::supports_avx512bw()) {\n+        return false;\n+      }\n+    case Op_AddVHF:\n+    case Op_DivVHF:\n+    case Op_FmaVHF:\n+    case Op_MulVHF:\n+    case Op_SubVHF:\n+    case Op_SqrtVHF:\n+      if (size_in_bits < 512 && !VM_Version::supports_avx512vl()) {\n+        return false;\n+      }\n+      if (!VM_Version::supports_avx512_fp16()) {\n+        return false;\n+      }\n+      break;\n+    case Op_AbsVF:\n+    case Op_NegVF:\n+      if ((vlen == 16) && (VM_Version::supports_avx512dq() == false)) {\n+        return false; \/\/ 512bit vandps and vxorps are not available\n+      }\n+      break;\n+    case Op_AbsVD:\n+    case Op_NegVD:\n+      if ((vlen == 8) && (VM_Version::supports_avx512dq() == false)) {\n+        return false; \/\/ 512bit vpmullq, vandpd and vxorpd are not available\n+      }\n+      break;\n+    case Op_RotateRightV:\n+    case Op_RotateLeftV:\n+      if (bt != T_INT && bt != T_LONG) {\n+        return false;\n+      } \/\/ fallthrough\n+    case Op_MacroLogicV:\n+      if (!VM_Version::supports_evex() ||\n+          ((size_in_bits != 512) && !VM_Version::supports_avx512vl())) {\n+        return false;\n+      }\n+      break;\n+    case Op_ClearArray:\n+    case Op_VectorMaskGen:\n+    case Op_VectorCmpMasked:\n+      if (!VM_Version::supports_avx512bw()) {\n+        return false;\n+      }\n+      if ((size_in_bits != 512) && !VM_Version::supports_avx512vl()) {\n+        return false;\n+      }\n+      break;\n+    case Op_LoadVectorMasked:\n+    case Op_StoreVectorMasked:\n+      if (!VM_Version::supports_avx512bw() && (is_subword_type(bt) || UseAVX < 1)) {\n+        return false;\n+      }\n+      break;\n+    case Op_UMinV:\n+    case Op_UMaxV:\n+      if (UseAVX == 0) {\n+        return false;\n+      }\n+      break;\n+    case Op_MaxV:\n+    case Op_MinV:\n+      if (UseSSE < 4 && is_integral_type(bt)) {\n+        return false;\n+      }\n+      if ((bt == T_FLOAT || bt == T_DOUBLE)) {\n+          \/\/ Float\/Double intrinsics are enabled for AVX family currently.\n+          if (UseAVX == 0) {\n+            return false;\n+          }\n+          if (UseAVX > 2 && (!VM_Version::supports_avx512dq() && size_in_bits == 512)) { \/\/ 512 bit Float\/Double intrinsics need AVX512DQ\n+            return false;\n+          }\n+      }\n+      break;\n+    case Op_CallLeafVector:\n+      if (size_in_bits == 512 && !VM_Version::supports_avx512vlbwdq()) {\n+        return false;\n+      }\n+      break;\n+    case Op_AddReductionVI:\n+      if (bt == T_INT && (UseSSE < 3 || !VM_Version::supports_ssse3())) {\n+        return false;\n+      }\n+      \/\/ fallthrough\n+    case Op_AndReductionV:\n+    case Op_OrReductionV:\n+    case Op_XorReductionV:\n+      if (is_subword_type(bt) && (UseSSE < 4)) {\n+        return false;\n+      }\n+      break;\n+    case Op_MinReductionV:\n+    case Op_MaxReductionV:\n+      if ((bt == T_INT || is_subword_type(bt)) && UseSSE < 4) {\n+        return false;\n+      } else if (bt == T_LONG && (UseAVX < 3 || !VM_Version::supports_avx512vlbwdq())) {\n+        return false;\n+      }\n+      \/\/ Float\/Double intrinsics enabled for AVX family.\n+      if (UseAVX == 0 && (bt == T_FLOAT || bt == T_DOUBLE)) {\n+        return false;\n+      }\n+      if (UseAVX > 2 && (!VM_Version::supports_avx512dq() && size_in_bits == 512)) {\n+        return false;\n+      }\n+      break;\n+    case Op_VectorTest:\n+      if (UseSSE < 4) {\n+        return false; \/\/ Implementation limitation\n+      } else if (size_in_bits < 32) {\n+        return false; \/\/ Implementation limitation\n+      }\n+      break;\n+    case Op_VectorLoadShuffle:\n+    case Op_VectorRearrange:\n+      if(vlen == 2) {\n+        return false; \/\/ Implementation limitation due to how shuffle is loaded\n+      } else if (size_in_bits == 256 && UseAVX < 2) {\n+        return false; \/\/ Implementation limitation\n+      }\n+      break;\n+    case Op_VectorLoadMask:\n+    case Op_VectorMaskCast:\n+      if (size_in_bits == 256 && UseAVX < 2) {\n+        return false; \/\/ Implementation limitation\n+      }\n+      \/\/ fallthrough\n+    case Op_VectorStoreMask:\n+      if (vlen == 2) {\n+        return false; \/\/ Implementation limitation\n+      }\n+      break;\n+    case Op_PopulateIndex:\n+      if (size_in_bits > 256 && !VM_Version::supports_avx512bw()) {\n+        return false;\n+      }\n+      break;\n+    case Op_VectorCastB2X:\n+    case Op_VectorCastS2X:\n+    case Op_VectorCastI2X:\n+      if (bt != T_DOUBLE && size_in_bits == 256 && UseAVX < 2) {\n+        return false;\n+      }\n+      break;\n+    case Op_VectorCastL2X:\n+      if (is_integral_type(bt) && size_in_bits == 256 && UseAVX < 2) {\n+        return false;\n+      } else if (!is_integral_type(bt) && !VM_Version::supports_avx512dq()) {\n+        return false;\n+      }\n+      break;\n+    case Op_VectorCastF2X: {\n+        \/\/ As per JLS section 5.1.3 narrowing conversion to sub-word types\n+        \/\/ happen after intermediate conversion to integer and special handling\n+        \/\/ code needs AVX2 vpcmpeqd instruction for 256 bit vectors.\n+        int src_size_in_bits = type2aelembytes(T_FLOAT) * vlen * BitsPerByte;\n+        if (is_integral_type(bt) && src_size_in_bits == 256 && UseAVX < 2) {\n+          return false;\n+        }\n+      }\n+      \/\/ fallthrough\n+    case Op_VectorCastD2X:\n+      if (bt == T_LONG && !VM_Version::supports_avx512dq()) {\n+        return false;\n+      }\n+      break;\n+    case Op_VectorCastF2HF:\n+    case Op_VectorCastHF2F:\n+      if (!VM_Version::supports_f16c() &&\n+         ((!VM_Version::supports_evex() ||\n+         ((size_in_bits != 512) && !VM_Version::supports_avx512vl())))) {\n+        return false;\n+      }\n+      break;\n+    case Op_RoundVD:\n+      if (!VM_Version::supports_avx512dq()) {\n+        return false;\n+      }\n+      break;\n+    case Op_MulReductionVI:\n+      if (bt == T_BYTE && size_in_bits == 512 && !VM_Version::supports_avx512bw()) {\n+        return false;\n+      }\n+      break;\n+    case Op_LoadVectorGatherMasked:\n+      if (!is_subword_type(bt) && size_in_bits < 512 && !VM_Version::supports_avx512vl()) {\n+        return false;\n+      }\n+      if (is_subword_type(bt) &&\n+         ((size_in_bits > 256 && !VM_Version::supports_avx512bw()) ||\n+          (size_in_bits < 64)                                      ||\n+          (bt == T_SHORT && !VM_Version::supports_bmi2()))) {\n+        return false;\n+      }\n+      break;\n+    case Op_StoreVectorScatterMasked:\n+    case Op_StoreVectorScatter:\n+      if (is_subword_type(bt)) {\n+        return false;\n+      } else if (size_in_bits < 512 && !VM_Version::supports_avx512vl()) {\n+        return false;\n+      }\n+      \/\/ fallthrough\n+    case Op_LoadVectorGather:\n+      if (!is_subword_type(bt) && size_in_bits == 64) {\n+        return false;\n+      }\n+      if (is_subword_type(bt) && size_in_bits < 64) {\n+        return false;\n+      }\n+      break;\n+    case Op_SaturatingAddV:\n+    case Op_SaturatingSubV:\n+      if (UseAVX < 1) {\n+        return false; \/\/ Implementation limitation\n+      }\n+      if (is_subword_type(bt) && size_in_bits == 512 && !VM_Version::supports_avx512bw()) {\n+        return false;\n+      }\n+      break;\n+    case Op_SelectFromTwoVector:\n+       if (size_in_bits < 128) {\n+         return false;\n+       }\n+       if ((size_in_bits < 512 && !VM_Version::supports_avx512vl())) {\n+         return false;\n+       }\n+       if (bt == T_SHORT && !VM_Version::supports_avx512bw()) {\n+         return false;\n+       }\n+       if (bt == T_BYTE && !VM_Version::supports_avx512_vbmi()) {\n+         return false;\n+       }\n+       if ((bt == T_INT || bt == T_FLOAT || bt == T_DOUBLE) && !VM_Version::supports_evex()) {\n+         return false;\n+       }\n+       break;\n+    case Op_MaskAll:\n+      if (!VM_Version::supports_evex()) {\n+        return false;\n+      }\n+      if ((vlen > 16 || is_subword_type(bt)) && !VM_Version::supports_avx512bw()) {\n+        return false;\n+      }\n+      if (size_in_bits < 512 && !VM_Version::supports_avx512vl()) {\n+        return false;\n+      }\n+      break;\n+    case Op_VectorMaskCmp:\n+      if (vlen < 2 || size_in_bits < 32) {\n+        return false;\n+      }\n+      break;\n+    case Op_CompressM:\n+      if (UseAVX < 3 || !VM_Version::supports_bmi2()) {\n+        return false;\n+      }\n+      break;\n+    case Op_CompressV:\n+    case Op_ExpandV:\n+      if (is_subword_type(bt) && !VM_Version::supports_avx512_vbmi2()) {\n+        return false;\n+      }\n+      if (size_in_bits < 128 ) {\n+        return false;\n+      }\n+    case Op_VectorLongToMask:\n+      if (UseAVX < 1) {\n+        return false;\n+      }\n+      if (UseAVX < 3 && !VM_Version::supports_bmi2()) {\n+        return false;\n+      }\n+      break;\n+    case Op_SignumVD:\n+    case Op_SignumVF:\n+      if (UseAVX < 1) {\n+        return false;\n+      }\n+      break;\n+    case Op_PopCountVI:\n+    case Op_PopCountVL: {\n+        if (!is_pop_count_instr_target(bt) &&\n+            (size_in_bits == 512) && !VM_Version::supports_avx512bw()) {\n+          return false;\n+        }\n+      }\n+      break;\n+    case Op_ReverseV:\n+    case Op_ReverseBytesV:\n+      if (UseAVX < 2) {\n+        return false;\n+      }\n+      break;\n+    case Op_CountTrailingZerosV:\n+    case Op_CountLeadingZerosV:\n+      if (UseAVX < 2) {\n+        return false;\n+      }\n+      break;\n+  }\n+  return true;  \/\/ Per default match rules are supported.\n+}\n+\n+bool Matcher::match_rule_supported_vector_masked(int opcode, int vlen, BasicType bt) {\n+  \/\/ ADLC based match_rule_supported routine checks for the existence of pattern based\n+  \/\/ on IR opcode. Most of the unary\/binary\/ternary masked operation share the IR nodes\n+  \/\/ of their non-masked counterpart with mask edge being the differentiator.\n+  \/\/ This routine does a strict check on the existence of masked operation patterns\n+  \/\/ by returning a default false value for all the other opcodes apart from the\n+  \/\/ ones whose masked instruction patterns are defined in this file.\n+  if (!match_rule_supported_vector(opcode, vlen, bt)) {\n+    return false;\n+  }\n+\n+  int size_in_bits = vlen * type2aelembytes(bt) * BitsPerByte;\n+  if (size_in_bits != 512 && !VM_Version::supports_avx512vl()) {\n+    return false;\n+  }\n+  switch(opcode) {\n+    \/\/ Unary masked operations\n+    case Op_AbsVB:\n+    case Op_AbsVS:\n+      if(!VM_Version::supports_avx512bw()) {\n+        return false;  \/\/ Implementation limitation\n+      }\n+    case Op_AbsVI:\n+    case Op_AbsVL:\n+      return true;\n+\n+    \/\/ Ternary masked operations\n+    case Op_FmaVF:\n+    case Op_FmaVD:\n+      return true;\n+\n+    case Op_MacroLogicV:\n+      if(bt != T_INT && bt != T_LONG) {\n+        return false;\n+      }\n+      return true;\n+\n+    \/\/ Binary masked operations\n+    case Op_AddVB:\n+    case Op_AddVS:\n+    case Op_SubVB:\n+    case Op_SubVS:\n+    case Op_MulVS:\n+    case Op_LShiftVS:\n+    case Op_RShiftVS:\n+    case Op_URShiftVS:\n+      assert(size_in_bits == 512 || VM_Version::supports_avx512vl(), \"\");\n+      if (!VM_Version::supports_avx512bw()) {\n+        return false;  \/\/ Implementation limitation\n+      }\n+      return true;\n+\n+    case Op_MulVL:\n+      assert(size_in_bits == 512 || VM_Version::supports_avx512vl(), \"\");\n+      if (!VM_Version::supports_avx512dq()) {\n+        return false;  \/\/ Implementation limitation\n+      }\n+      return true;\n+\n+    case Op_AndV:\n+    case Op_OrV:\n+    case Op_XorV:\n+    case Op_RotateRightV:\n+    case Op_RotateLeftV:\n+      if (bt != T_INT && bt != T_LONG) {\n+        return false; \/\/ Implementation limitation\n+      }\n+      return true;\n+\n+    case Op_VectorLoadMask:\n+      assert(size_in_bits == 512 || VM_Version::supports_avx512vl(), \"\");\n+      if (is_subword_type(bt) && !VM_Version::supports_avx512bw()) {\n+        return false;\n+      }\n+      return true;\n+\n+    case Op_AddVI:\n+    case Op_AddVL:\n+    case Op_AddVF:\n+    case Op_AddVD:\n+    case Op_SubVI:\n+    case Op_SubVL:\n+    case Op_SubVF:\n+    case Op_SubVD:\n+    case Op_MulVI:\n+    case Op_MulVF:\n+    case Op_MulVD:\n+    case Op_DivVF:\n+    case Op_DivVD:\n+    case Op_SqrtVF:\n+    case Op_SqrtVD:\n+    case Op_LShiftVI:\n+    case Op_LShiftVL:\n+    case Op_RShiftVI:\n+    case Op_RShiftVL:\n+    case Op_URShiftVI:\n+    case Op_URShiftVL:\n+    case Op_LoadVectorMasked:\n+    case Op_StoreVectorMasked:\n+    case Op_LoadVectorGatherMasked:\n+    case Op_StoreVectorScatterMasked:\n+      return true;\n+\n+    case Op_UMinV:\n+    case Op_UMaxV:\n+      if (size_in_bits < 512 && !VM_Version::supports_avx512vl()) {\n+        return false;\n+      } \/\/ fallthrough\n+    case Op_MaxV:\n+    case Op_MinV:\n+      if (is_subword_type(bt) && !VM_Version::supports_avx512bw()) {\n+        return false; \/\/ Implementation limitation\n+      }\n+      if (is_floating_point_type(bt) && !VM_Version::supports_avx10_2()) {\n+        return false; \/\/ Implementation limitation\n+      }\n+      return true;\n+    case Op_SaturatingAddV:\n+    case Op_SaturatingSubV:\n+      if (!is_subword_type(bt)) {\n+        return false;\n+      }\n+      if (size_in_bits < 128 || !VM_Version::supports_avx512bw()) {\n+        return false; \/\/ Implementation limitation\n+      }\n+      return true;\n+\n+    case Op_VectorMaskCmp:\n+      if (is_subword_type(bt) && !VM_Version::supports_avx512bw()) {\n+        return false; \/\/ Implementation limitation\n+      }\n+      return true;\n+\n+    case Op_VectorRearrange:\n+      if (bt == T_SHORT && !VM_Version::supports_avx512bw()) {\n+        return false; \/\/ Implementation limitation\n+      }\n+      if (bt == T_BYTE && !VM_Version::supports_avx512_vbmi()) {\n+        return false; \/\/ Implementation limitation\n+      } else if ((bt == T_INT || bt == T_FLOAT) && size_in_bits < 256) {\n+        return false; \/\/ Implementation limitation\n+      }\n+      return true;\n+\n+    \/\/ Binary Logical operations\n+    case Op_AndVMask:\n+    case Op_OrVMask:\n+    case Op_XorVMask:\n+      if (vlen > 16 && !VM_Version::supports_avx512bw()) {\n+        return false; \/\/ Implementation limitation\n+      }\n+      return true;\n+\n+    case Op_PopCountVI:\n+    case Op_PopCountVL:\n+      if (!is_pop_count_instr_target(bt)) {\n+        return false;\n+      }\n+      return true;\n+\n+    case Op_MaskAll:\n+      return true;\n+\n+    case Op_CountLeadingZerosV:\n+      if (is_non_subword_integral_type(bt) && VM_Version::supports_avx512cd()) {\n+        return true;\n+      }\n+    default:\n+      return false;\n+  }\n+}\n+\n+bool Matcher::vector_needs_partial_operations(Node* node, const TypeVect* vt) {\n+  return false;\n+}\n+\n+\/\/ Return true if Vector::rearrange needs preparation of the shuffle argument\n+bool Matcher::vector_rearrange_requires_load_shuffle(BasicType elem_bt, int vlen) {\n+  switch (elem_bt) {\n+    case T_BYTE:  return false;\n+    case T_SHORT: return !VM_Version::supports_avx512bw();\n+    case T_INT:   return !VM_Version::supports_avx();\n+    case T_LONG:  return vlen < 8 && !VM_Version::supports_avx512vl();\n+    default:\n+      ShouldNotReachHere();\n+      return false;\n+  }\n+}\n+\n+MachOper* Matcher::pd_specialize_generic_vector_operand(MachOper* generic_opnd, uint ideal_reg, bool is_temp) {\n+  assert(Matcher::is_generic_vector(generic_opnd), \"not generic\");\n+  bool legacy = (generic_opnd->opcode() == LEGVEC);\n+  if (!VM_Version::supports_avx512vlbwdq() && \/\/ KNL\n+      is_temp && !legacy && (ideal_reg == Op_VecZ)) {\n+    \/\/ Conservatively specialize 512bit vec TEMP operands to legVecZ (zmm0-15) on KNL.\n+    return new legVecZOper();\n+  }\n+  if (legacy) {\n+    switch (ideal_reg) {\n+      case Op_VecS: return new legVecSOper();\n+      case Op_VecD: return new legVecDOper();\n+      case Op_VecX: return new legVecXOper();\n+      case Op_VecY: return new legVecYOper();\n+      case Op_VecZ: return new legVecZOper();\n+    }\n+  } else {\n+    switch (ideal_reg) {\n+      case Op_VecS: return new vecSOper();\n+      case Op_VecD: return new vecDOper();\n+      case Op_VecX: return new vecXOper();\n+      case Op_VecY: return new vecYOper();\n+      case Op_VecZ: return new vecZOper();\n+    }\n+  }\n+  ShouldNotReachHere();\n+  return nullptr;\n+}\n+\n+bool Matcher::is_reg2reg_move(MachNode* m) {\n+  switch (m->rule()) {\n+    case MoveVec2Leg_rule:\n+    case MoveLeg2Vec_rule:\n+    case MoveF2VL_rule:\n+    case MoveF2LEG_rule:\n+    case MoveVL2F_rule:\n+    case MoveLEG2F_rule:\n+    case MoveD2VL_rule:\n+    case MoveD2LEG_rule:\n+    case MoveVL2D_rule:\n+    case MoveLEG2D_rule:\n+      return true;\n+    default:\n+      return false;\n+  }\n+}\n+\n+bool Matcher::is_generic_vector(MachOper* opnd) {\n+  switch (opnd->opcode()) {\n+    case VEC:\n+    case LEGVEC:\n+      return true;\n+    default:\n+      return false;\n+  }\n+}\n+\n+\/\/------------------------------------------------------------------------\n+\n+const RegMask* Matcher::predicate_reg_mask(void) {\n+  return &_VECTMASK_REG_mask;\n+}\n+\n+\/\/ Max vector size in bytes. 0 if not supported.\n+int Matcher::vector_width_in_bytes(BasicType bt) {\n+  assert(is_java_primitive(bt), \"only primitive type vectors\");\n+  \/\/ SSE2 supports 128bit vectors for all types.\n+  \/\/ AVX2 supports 256bit vectors for all types.\n+  \/\/ AVX2\/EVEX supports 512bit vectors for all types.\n+  int size = (UseAVX > 1) ? (1 << UseAVX) * 8 : 16;\n+  \/\/ AVX1 supports 256bit vectors only for FLOAT and DOUBLE.\n+  if (UseAVX > 0 && (bt == T_FLOAT || bt == T_DOUBLE))\n+    size = (UseAVX > 2) ? 64 : 32;\n+  if (UseAVX > 2 && (bt == T_BYTE || bt == T_SHORT || bt == T_CHAR))\n+    size = (VM_Version::supports_avx512bw()) ? 64 : 32;\n+  \/\/ Use flag to limit vector size.\n+  size = MIN2(size,(int)MaxVectorSize);\n+  \/\/ Minimum 2 values in vector (or 4 for bytes).\n+  switch (bt) {\n+  case T_DOUBLE:\n+  case T_LONG:\n+    if (size < 16) return 0;\n+    break;\n+  case T_FLOAT:\n+  case T_INT:\n+    if (size < 8) return 0;\n+    break;\n+  case T_BOOLEAN:\n+    if (size < 4) return 0;\n+    break;\n+  case T_CHAR:\n+    if (size < 4) return 0;\n+    break;\n+  case T_BYTE:\n+    if (size < 4) return 0;\n+    break;\n+  case T_SHORT:\n+    if (size < 4) return 0;\n+    break;\n+  default:\n+    ShouldNotReachHere();\n+  }\n+  return size;\n+}\n+\n+\/\/ Limits on vector size (number of elements) loaded into vector.\n+int Matcher::max_vector_size(const BasicType bt) {\n+  return vector_width_in_bytes(bt)\/type2aelembytes(bt);\n+}\n+int Matcher::min_vector_size(const BasicType bt) {\n+  int max_size = max_vector_size(bt);\n+  \/\/ Min size which can be loaded into vector is 4 bytes.\n+  int size = (type2aelembytes(bt) == 1) ? 4 : 2;\n+  \/\/ Support for calling svml double64 vectors\n+  if (bt == T_DOUBLE) {\n+    size = 1;\n+  }\n+  return MIN2(size,max_size);\n+}\n+\n+int Matcher::max_vector_size_auto_vectorization(const BasicType bt) {\n+  \/\/ Limit the max vector size for auto vectorization to 256 bits (32 bytes)\n+  \/\/ by default on Cascade Lake\n+  if (VM_Version::is_default_intel_cascade_lake()) {\n+    return MIN2(Matcher::max_vector_size(bt), 32 \/ type2aelembytes(bt));\n+  }\n+  return Matcher::max_vector_size(bt);\n+}\n+\n+int Matcher::scalable_vector_reg_size(const BasicType bt) {\n+  return -1;\n+}\n+\n+\/\/ Vector ideal reg corresponding to specified size in bytes\n+uint Matcher::vector_ideal_reg(int size) {\n+  assert(MaxVectorSize >= size, \"\");\n+  switch(size) {\n+    case  4: return Op_VecS;\n+    case  8: return Op_VecD;\n+    case 16: return Op_VecX;\n+    case 32: return Op_VecY;\n+    case 64: return Op_VecZ;\n+  }\n+  ShouldNotReachHere();\n+  return 0;\n+}\n+\n+\/\/ Check for shift by small constant as well\n+static bool clone_shift(Node* shift, Matcher* matcher, Matcher::MStack& mstack, VectorSet& address_visited) {\n+  if (shift->Opcode() == Op_LShiftX && shift->in(2)->is_Con() &&\n+      shift->in(2)->get_int() <= 3 &&\n+      \/\/ Are there other uses besides address expressions?\n+      !matcher->is_visited(shift)) {\n+    address_visited.set(shift->_idx); \/\/ Flag as address_visited\n+    mstack.push(shift->in(2), Matcher::Visit);\n+    Node *conv = shift->in(1);\n+    \/\/ Allow Matcher to match the rule which bypass\n+    \/\/ ConvI2L operation for an array index on LP64\n+    \/\/ if the index value is positive.\n+    if (conv->Opcode() == Op_ConvI2L &&\n+        conv->as_Type()->type()->is_long()->_lo >= 0 &&\n+        \/\/ Are there other uses besides address expressions?\n+        !matcher->is_visited(conv)) {\n+      address_visited.set(conv->_idx); \/\/ Flag as address_visited\n+      mstack.push(conv->in(1), Matcher::Pre_Visit);\n+    } else {\n+      mstack.push(conv, Matcher::Pre_Visit);\n+    }\n+    return true;\n+  }\n+  return false;\n+}\n+\n+\/\/ This function identifies sub-graphs in which a 'load' node is\n+\/\/ input to two different nodes, and such that it can be matched\n+\/\/ with BMI instructions like blsi, blsr, etc.\n+\/\/ Example : for b = -a[i] & a[i] can be matched to blsi r32, m32.\n+\/\/ The graph is (AndL (SubL Con0 LoadL*) LoadL*), where LoadL*\n+\/\/ refers to the same node.\n+\/\/\n+\/\/ Match the generic fused operations pattern (op1 (op2 Con{ConType} mop) mop)\n+\/\/ This is a temporary solution until we make DAGs expressible in ADL.\n+template<typename ConType>\n+class FusedPatternMatcher {\n+  Node* _op1_node;\n+  Node* _mop_node;\n+  int _con_op;\n+\n+  static int match_next(Node* n, int next_op, int next_op_idx) {\n+    if (n->in(1) == nullptr || n->in(2) == nullptr) {\n+      return -1;\n+    }\n+\n+    if (next_op_idx == -1) { \/\/ n is commutative, try rotations\n+      if (n->in(1)->Opcode() == next_op) {\n+        return 1;\n+      } else if (n->in(2)->Opcode() == next_op) {\n+        return 2;\n+      }\n+    } else {\n+      assert(next_op_idx > 0 && next_op_idx <= 2, \"Bad argument index\");\n+      if (n->in(next_op_idx)->Opcode() == next_op) {\n+        return next_op_idx;\n+      }\n+    }\n+    return -1;\n+  }\n+\n+ public:\n+  FusedPatternMatcher(Node* op1_node, Node* mop_node, int con_op) :\n+    _op1_node(op1_node), _mop_node(mop_node), _con_op(con_op) { }\n+\n+  bool match(int op1, int op1_op2_idx,  \/\/ op1 and the index of the op1->op2 edge, -1 if op1 is commutative\n+             int op2, int op2_con_idx,  \/\/ op2 and the index of the op2->con edge, -1 if op2 is commutative\n+             typename ConType::NativeType con_value) {\n+    if (_op1_node->Opcode() != op1) {\n+      return false;\n+    }\n+    if (_mop_node->outcnt() > 2) {\n+      return false;\n+    }\n+    op1_op2_idx = match_next(_op1_node, op2, op1_op2_idx);\n+    if (op1_op2_idx == -1) {\n+      return false;\n+    }\n+    \/\/ Memory operation must be the other edge\n+    int op1_mop_idx = (op1_op2_idx & 1) + 1;\n+\n+    \/\/ Check that the mop node is really what we want\n+    if (_op1_node->in(op1_mop_idx) == _mop_node) {\n+      Node* op2_node = _op1_node->in(op1_op2_idx);\n+      if (op2_node->outcnt() > 1) {\n+        return false;\n+      }\n+      assert(op2_node->Opcode() == op2, \"Should be\");\n+      op2_con_idx = match_next(op2_node, _con_op, op2_con_idx);\n+      if (op2_con_idx == -1) {\n+        return false;\n+      }\n+      \/\/ Memory operation must be the other edge\n+      int op2_mop_idx = (op2_con_idx & 1) + 1;\n+      \/\/ Check that the memory operation is the same node\n+      if (op2_node->in(op2_mop_idx) == _mop_node) {\n+        \/\/ Now check the constant\n+        const Type* con_type = op2_node->in(op2_con_idx)->bottom_type();\n+        if (con_type != Type::TOP && ConType::as_self(con_type)->get_con() == con_value) {\n+          return true;\n+        }\n+      }\n+    }\n+    return false;\n+  }\n+};\n+\n+static bool is_bmi_pattern(Node* n, Node* m) {\n+  assert(UseBMI1Instructions, \"sanity\");\n+  if (n != nullptr && m != nullptr) {\n+    if (m->Opcode() == Op_LoadI) {\n+      FusedPatternMatcher<TypeInt> bmii(n, m, Op_ConI);\n+      return bmii.match(Op_AndI, -1, Op_SubI,  1,  0)  ||\n+             bmii.match(Op_AndI, -1, Op_AddI, -1, -1)  ||\n+             bmii.match(Op_XorI, -1, Op_AddI, -1, -1);\n+    } else if (m->Opcode() == Op_LoadL) {\n+      FusedPatternMatcher<TypeLong> bmil(n, m, Op_ConL);\n+      return bmil.match(Op_AndL, -1, Op_SubL,  1,  0) ||\n+             bmil.match(Op_AndL, -1, Op_AddL, -1, -1) ||\n+             bmil.match(Op_XorL, -1, Op_AddL, -1, -1);\n+    }\n+  }\n+  return false;\n+}\n+\n+\/\/ Should the matcher clone input 'm' of node 'n'?\n+bool Matcher::pd_clone_node(Node* n, Node* m, Matcher::MStack& mstack) {\n+  \/\/ If 'n' and 'm' are part of a graph for BMI instruction, clone the input 'm'.\n+  if (UseBMI1Instructions && is_bmi_pattern(n, m)) {\n+    mstack.push(m, Visit);\n+    return true;\n+  }\n+  if (is_vshift_con_pattern(n, m)) { \/\/ ShiftV src (ShiftCntV con)\n+    mstack.push(m, Visit);           \/\/ m = ShiftCntV\n+    return true;\n+  }\n+  if (is_encode_and_store_pattern(n, m)) {\n+    mstack.push(m, Visit);\n+    return true;\n+  }\n+  return false;\n+}\n+\n+\/\/ Should the Matcher clone shifts on addressing modes, expecting them\n+\/\/ to be subsumed into complex addressing expressions or compute them\n+\/\/ into registers?\n+bool Matcher::pd_clone_address_expressions(AddPNode* m, Matcher::MStack& mstack, VectorSet& address_visited) {\n+  Node *off = m->in(AddPNode::Offset);\n+  if (off->is_Con()) {\n+    address_visited.test_set(m->_idx); \/\/ Flag as address_visited\n+    Node *adr = m->in(AddPNode::Address);\n+\n+    \/\/ Intel can handle 2 adds in addressing mode, with one of them using an immediate offset.\n+    \/\/ AtomicAdd is not an addressing expression.\n+    \/\/ Cheap to find it by looking for screwy base.\n+    if (adr->is_AddP() &&\n+        !adr->in(AddPNode::Base)->is_top() &&\n+        !adr->in(AddPNode::Offset)->is_Con() &&\n+        off->get_long() == (int) (off->get_long()) && \/\/ immL32\n+        \/\/ Are there other uses besides address expressions?\n+        !is_visited(adr)) {\n+      address_visited.set(adr->_idx); \/\/ Flag as address_visited\n+      Node *shift = adr->in(AddPNode::Offset);\n+      if (!clone_shift(shift, this, mstack, address_visited)) {\n+        mstack.push(shift, Pre_Visit);\n+      }\n+      mstack.push(adr->in(AddPNode::Address), Pre_Visit);\n+      mstack.push(adr->in(AddPNode::Base), Pre_Visit);\n+    } else {\n+      mstack.push(adr, Pre_Visit);\n+    }\n+\n+    \/\/ Clone X+offset as it also folds into most addressing expressions\n+    mstack.push(off, Visit);\n+    mstack.push(m->in(AddPNode::Base), Pre_Visit);\n+    return true;\n+  } else if (clone_shift(off, this, mstack, address_visited)) {\n+    address_visited.test_set(m->_idx); \/\/ Flag as address_visited\n+    mstack.push(m->in(AddPNode::Address), Pre_Visit);\n+    mstack.push(m->in(AddPNode::Base), Pre_Visit);\n+    return true;\n+  }\n+  return false;\n+}\n+\n+static inline Assembler::ComparisonPredicate booltest_pred_to_comparison_pred(int bt) {\n+  switch (bt) {\n+    case BoolTest::eq:\n+      return Assembler::eq;\n+    case BoolTest::ne:\n+      return Assembler::neq;\n+    case BoolTest::le:\n+    case BoolTest::ule:\n+      return Assembler::le;\n+    case BoolTest::ge:\n+    case BoolTest::uge:\n+      return Assembler::nlt;\n+    case BoolTest::lt:\n+    case BoolTest::ult:\n+      return Assembler::lt;\n+    case BoolTest::gt:\n+    case BoolTest::ugt:\n+      return Assembler::nle;\n+    default : ShouldNotReachHere(); return Assembler::_false;\n+  }\n+}\n+\n+static inline Assembler::ComparisonPredicateFP booltest_pred_to_comparison_pred_fp(int bt) {\n+  switch (bt) {\n+  case BoolTest::eq: return Assembler::EQ_OQ;  \/\/ ordered non-signaling\n+  \/\/ As per JLS 15.21.1, != of NaNs is true. Thus use unordered compare.\n+  case BoolTest::ne: return Assembler::NEQ_UQ; \/\/ unordered non-signaling\n+  case BoolTest::le: return Assembler::LE_OQ;  \/\/ ordered non-signaling\n+  case BoolTest::ge: return Assembler::GE_OQ;  \/\/ ordered non-signaling\n+  case BoolTest::lt: return Assembler::LT_OQ;  \/\/ ordered non-signaling\n+  case BoolTest::gt: return Assembler::GT_OQ;  \/\/ ordered non-signaling\n+  default: ShouldNotReachHere(); return Assembler::FALSE_OS;\n+  }\n+}\n+\n+\/\/ Helper methods for MachSpillCopyNode::implementation().\n+static void vec_mov_helper(C2_MacroAssembler *masm, int src_lo, int dst_lo,\n+                          int src_hi, int dst_hi, uint ireg, outputStream* st) {\n+  assert(ireg == Op_VecS || \/\/ 32bit vector\n+         ((src_lo & 1) == 0 && (src_lo + 1) == src_hi &&\n+          (dst_lo & 1) == 0 && (dst_lo + 1) == dst_hi),\n+         \"no non-adjacent vector moves\" );\n+  if (masm) {\n+    switch (ireg) {\n+    case Op_VecS: \/\/ copy whole register\n+    case Op_VecD:\n+    case Op_VecX:\n+      if ((UseAVX < 3) || VM_Version::supports_avx512vl()) {\n+        __ movdqu(as_XMMRegister(Matcher::_regEncode[dst_lo]), as_XMMRegister(Matcher::_regEncode[src_lo]));\n+      } else {\n+        __ vextractf32x4(as_XMMRegister(Matcher::_regEncode[dst_lo]), as_XMMRegister(Matcher::_regEncode[src_lo]), 0x0);\n+     }\n+      break;\n+    case Op_VecY:\n+      if ((UseAVX < 3) || VM_Version::supports_avx512vl()) {\n+        __ vmovdqu(as_XMMRegister(Matcher::_regEncode[dst_lo]), as_XMMRegister(Matcher::_regEncode[src_lo]));\n+      } else {\n+        __ vextractf64x4(as_XMMRegister(Matcher::_regEncode[dst_lo]), as_XMMRegister(Matcher::_regEncode[src_lo]), 0x0);\n+     }\n+      break;\n+    case Op_VecZ:\n+      __ evmovdquq(as_XMMRegister(Matcher::_regEncode[dst_lo]), as_XMMRegister(Matcher::_regEncode[src_lo]), 2);\n+      break;\n+    default:\n+      ShouldNotReachHere();\n+    }\n+#ifndef PRODUCT\n+  } else {\n+    switch (ireg) {\n+    case Op_VecS:\n+    case Op_VecD:\n+    case Op_VecX:\n+      st->print(\"movdqu  %s,%s\\t# spill\",Matcher::regName[dst_lo],Matcher::regName[src_lo]);\n+      break;\n+    case Op_VecY:\n+    case Op_VecZ:\n+      st->print(\"vmovdqu %s,%s\\t# spill\",Matcher::regName[dst_lo],Matcher::regName[src_lo]);\n+      break;\n+    default:\n+      ShouldNotReachHere();\n+    }\n+#endif\n+  }\n+}\n+\n+void vec_spill_helper(C2_MacroAssembler *masm, bool is_load,\n+                     int stack_offset, int reg, uint ireg, outputStream* st) {\n+  if (masm) {\n+    if (is_load) {\n+      switch (ireg) {\n+      case Op_VecS:\n+        __ movdl(as_XMMRegister(Matcher::_regEncode[reg]), Address(rsp, stack_offset));\n+        break;\n+      case Op_VecD:\n+        __ movq(as_XMMRegister(Matcher::_regEncode[reg]), Address(rsp, stack_offset));\n+        break;\n+      case Op_VecX:\n+        if ((UseAVX < 3) || VM_Version::supports_avx512vl()) {\n+          __ movdqu(as_XMMRegister(Matcher::_regEncode[reg]), Address(rsp, stack_offset));\n+        } else {\n+          __ vpxor(as_XMMRegister(Matcher::_regEncode[reg]), as_XMMRegister(Matcher::_regEncode[reg]), as_XMMRegister(Matcher::_regEncode[reg]), 2);\n+          __ vinsertf32x4(as_XMMRegister(Matcher::_regEncode[reg]), as_XMMRegister(Matcher::_regEncode[reg]), Address(rsp, stack_offset),0x0);\n+        }\n+        break;\n+      case Op_VecY:\n+        if ((UseAVX < 3) || VM_Version::supports_avx512vl()) {\n+          __ vmovdqu(as_XMMRegister(Matcher::_regEncode[reg]), Address(rsp, stack_offset));\n+        } else {\n+          __ vpxor(as_XMMRegister(Matcher::_regEncode[reg]), as_XMMRegister(Matcher::_regEncode[reg]), as_XMMRegister(Matcher::_regEncode[reg]), 2);\n+          __ vinsertf64x4(as_XMMRegister(Matcher::_regEncode[reg]), as_XMMRegister(Matcher::_regEncode[reg]), Address(rsp, stack_offset),0x0);\n+        }\n+        break;\n+      case Op_VecZ:\n+        __ evmovdquq(as_XMMRegister(Matcher::_regEncode[reg]), Address(rsp, stack_offset), 2);\n+        break;\n+      default:\n+        ShouldNotReachHere();\n+      }\n+    } else { \/\/ store\n+      switch (ireg) {\n+      case Op_VecS:\n+        __ movdl(Address(rsp, stack_offset), as_XMMRegister(Matcher::_regEncode[reg]));\n+        break;\n+      case Op_VecD:\n+        __ movq(Address(rsp, stack_offset), as_XMMRegister(Matcher::_regEncode[reg]));\n+        break;\n+      case Op_VecX:\n+        if ((UseAVX < 3) || VM_Version::supports_avx512vl()) {\n+          __ movdqu(Address(rsp, stack_offset), as_XMMRegister(Matcher::_regEncode[reg]));\n+        }\n+        else {\n+          __ vextractf32x4(Address(rsp, stack_offset), as_XMMRegister(Matcher::_regEncode[reg]), 0x0);\n+        }\n+        break;\n+      case Op_VecY:\n+        if ((UseAVX < 3) || VM_Version::supports_avx512vl()) {\n+          __ vmovdqu(Address(rsp, stack_offset), as_XMMRegister(Matcher::_regEncode[reg]));\n+        }\n+        else {\n+          __ vextractf64x4(Address(rsp, stack_offset), as_XMMRegister(Matcher::_regEncode[reg]), 0x0);\n+        }\n+        break;\n+      case Op_VecZ:\n+        __ evmovdquq(Address(rsp, stack_offset), as_XMMRegister(Matcher::_regEncode[reg]), 2);\n+        break;\n+      default:\n+        ShouldNotReachHere();\n+      }\n+    }\n+#ifndef PRODUCT\n+  } else {\n+    if (is_load) {\n+      switch (ireg) {\n+      case Op_VecS:\n+        st->print(\"movd    %s,[rsp + %d]\\t# spill\", Matcher::regName[reg], stack_offset);\n+        break;\n+      case Op_VecD:\n+        st->print(\"movq    %s,[rsp + %d]\\t# spill\", Matcher::regName[reg], stack_offset);\n+        break;\n+       case Op_VecX:\n+        st->print(\"movdqu  %s,[rsp + %d]\\t# spill\", Matcher::regName[reg], stack_offset);\n+        break;\n+      case Op_VecY:\n+      case Op_VecZ:\n+        st->print(\"vmovdqu %s,[rsp + %d]\\t# spill\", Matcher::regName[reg], stack_offset);\n+        break;\n+      default:\n+        ShouldNotReachHere();\n+      }\n+    } else { \/\/ store\n+      switch (ireg) {\n+      case Op_VecS:\n+        st->print(\"movd    [rsp + %d],%s\\t# spill\", stack_offset, Matcher::regName[reg]);\n+        break;\n+      case Op_VecD:\n+        st->print(\"movq    [rsp + %d],%s\\t# spill\", stack_offset, Matcher::regName[reg]);\n+        break;\n+       case Op_VecX:\n+        st->print(\"movdqu  [rsp + %d],%s\\t# spill\", stack_offset, Matcher::regName[reg]);\n+        break;\n+      case Op_VecY:\n+      case Op_VecZ:\n+        st->print(\"vmovdqu [rsp + %d],%s\\t# spill\", stack_offset, Matcher::regName[reg]);\n+        break;\n+      default:\n+        ShouldNotReachHere();\n+      }\n+    }\n+#endif\n+  }\n+}\n+\n+template <class T>\n+static inline GrowableArray<jbyte>* vreplicate_imm(BasicType bt, T con, int len) {\n+  int size = type2aelembytes(bt) * len;\n+  GrowableArray<jbyte>* val = new GrowableArray<jbyte>(size, size, 0);\n+  for (int i = 0; i < len; i++) {\n+    int offset = i * type2aelembytes(bt);\n+    switch (bt) {\n+      case T_BYTE: val->at(i) = con; break;\n+      case T_SHORT: {\n+        jshort c = con;\n+        memcpy(val->adr_at(offset), &c, sizeof(jshort));\n+        break;\n+      }\n+      case T_INT: {\n+        jint c = con;\n+        memcpy(val->adr_at(offset), &c, sizeof(jint));\n+        break;\n+      }\n+      case T_LONG: {\n+        jlong c = con;\n+        memcpy(val->adr_at(offset), &c, sizeof(jlong));\n+        break;\n+      }\n+      case T_FLOAT: {\n+        jfloat c = con;\n+        memcpy(val->adr_at(offset), &c, sizeof(jfloat));\n+        break;\n+      }\n+      case T_DOUBLE: {\n+        jdouble c = con;\n+        memcpy(val->adr_at(offset), &c, sizeof(jdouble));\n+        break;\n+      }\n+      default: assert(false, \"%s\", type2name(bt));\n+    }\n+  }\n+  return val;\n+}\n+\n+static inline jlong high_bit_set(BasicType bt) {\n+  switch (bt) {\n+    case T_BYTE:  return 0x8080808080808080;\n+    case T_SHORT: return 0x8000800080008000;\n+    case T_INT:   return 0x8000000080000000;\n+    case T_LONG:  return 0x8000000000000000;\n+    default:\n+      ShouldNotReachHere();\n+      return 0;\n+  }\n+}\n+\n+#ifndef PRODUCT\n+  void MachNopNode::format(PhaseRegAlloc*, outputStream* st) const {\n+    st->print(\"nop \\t# %d bytes pad for loops and calls\", _count);\n+  }\n+#endif\n+\n+  void MachNopNode::emit(C2_MacroAssembler *masm, PhaseRegAlloc*) const {\n+    __ nop(_count);\n+  }\n+\n+  uint MachNopNode::size(PhaseRegAlloc*) const {\n+    return _count;\n+  }\n+\n+#ifndef PRODUCT\n+  void MachBreakpointNode::format(PhaseRegAlloc*, outputStream* st) const {\n+    st->print(\"# breakpoint\");\n+  }\n+#endif\n+\n+  void MachBreakpointNode::emit(C2_MacroAssembler *masm, PhaseRegAlloc* ra_) const {\n+    __ int3();\n+  }\n+\n+  uint MachBreakpointNode::size(PhaseRegAlloc* ra_) const {\n+    return MachNode::size(ra_);\n+  }\n+\n+%}\n+\n+\/\/----------ENCODING BLOCK-----------------------------------------------------\n+\/\/ This block specifies the encoding classes used by the compiler to\n+\/\/ output byte streams.  Encoding classes are parameterized macros\n+\/\/ used by Machine Instruction Nodes in order to generate the bit\n+\/\/ encoding of the instruction.  Operands specify their base encoding\n+\/\/ interface with the interface keyword.  There are currently\n+\/\/ supported four interfaces, REG_INTER, CONST_INTER, MEMORY_INTER, &\n+\/\/ COND_INTER.  REG_INTER causes an operand to generate a function\n+\/\/ which returns its register number when queried.  CONST_INTER causes\n+\/\/ an operand to generate a function which returns the value of the\n+\/\/ constant when queried.  MEMORY_INTER causes an operand to generate\n+\/\/ four functions which return the Base Register, the Index Register,\n+\/\/ the Scale Value, and the Offset Value of the operand when queried.\n+\/\/ COND_INTER causes an operand to generate six functions which return\n+\/\/ the encoding code (ie - encoding bits for the instruction)\n+\/\/ associated with each basic boolean condition for a conditional\n+\/\/ instruction.\n+\/\/\n+\/\/ Instructions specify two basic values for encoding.  Again, a\n+\/\/ function is available to check if the constant displacement is an\n+\/\/ oop. They use the ins_encode keyword to specify their encoding\n+\/\/ classes (which must be a sequence of enc_class names, and their\n+\/\/ parameters, specified in the encoding block), and they use the\n+\/\/ opcode keyword to specify, in order, their primary, secondary, and\n+\/\/ tertiary opcode.  Only the opcode sections which a particular\n+\/\/ instruction needs for encoding need to be specified.\n+encode %{\n+  enc_class cdql_enc(no_rax_rdx_RegI div)\n+  %{\n+    \/\/ Full implementation of Java idiv and irem; checks for\n+    \/\/ special case as described in JVM spec., p.243 & p.271.\n+    \/\/\n+    \/\/         normal case                           special case\n+    \/\/\n+    \/\/ input : rax: dividend                         min_int\n+    \/\/         reg: divisor                          -1\n+    \/\/\n+    \/\/ output: rax: quotient  (= rax idiv reg)       min_int\n+    \/\/         rdx: remainder (= rax irem reg)       0\n+    \/\/\n+    \/\/  Code sequnce:\n+    \/\/\n+    \/\/    0:   3d 00 00 00 80          cmp    $0x80000000,%eax\n+    \/\/    5:   75 07\/08                jne    e <normal>\n+    \/\/    7:   33 d2                   xor    %edx,%edx\n+    \/\/  [div >= 8 -> offset + 1]\n+    \/\/  [REX_B]\n+    \/\/    9:   83 f9 ff                cmp    $0xffffffffffffffff,$div\n+    \/\/    c:   74 03\/04                je     11 <done>\n+    \/\/ 000000000000000e <normal>:\n+    \/\/    e:   99                      cltd\n+    \/\/  [div >= 8 -> offset + 1]\n+    \/\/  [REX_B]\n+    \/\/    f:   f7 f9                   idiv   $div\n+    \/\/ 0000000000000011 <done>:\n+    Label normal;\n+    Label done;\n+\n+    \/\/ cmp    $0x80000000,%eax\n+    __ cmpl(as_Register(RAX_enc), 0x80000000);\n+\n+    \/\/ jne    e <normal>\n+    __ jccb(Assembler::notEqual, normal);\n+\n+    \/\/ xor    %edx,%edx\n+    __ xorl(as_Register(RDX_enc), as_Register(RDX_enc));\n+\n+    \/\/ cmp    $0xffffffffffffffff,%ecx\n+    __ cmpl($div$$Register, -1);\n+\n+    \/\/ je     11 <done>\n+    __ jccb(Assembler::equal, done);\n+\n+    \/\/ <normal>\n+    \/\/ cltd\n+    __ bind(normal);\n+    __ cdql();\n+\n+    \/\/ idivl\n+    \/\/ <done>\n+    __ idivl($div$$Register);\n+    __ bind(done);\n+  %}\n+\n+  enc_class cdqq_enc(no_rax_rdx_RegL div)\n+  %{\n+    \/\/ Full implementation of Java ldiv and lrem; checks for\n+    \/\/ special case as described in JVM spec., p.243 & p.271.\n+    \/\/\n+    \/\/         normal case                           special case\n+    \/\/\n+    \/\/ input : rax: dividend                         min_long\n+    \/\/         reg: divisor                          -1\n+    \/\/\n+    \/\/ output: rax: quotient  (= rax idiv reg)       min_long\n+    \/\/         rdx: remainder (= rax irem reg)       0\n+    \/\/\n+    \/\/  Code sequnce:\n+    \/\/\n+    \/\/    0:   48 ba 00 00 00 00 00    mov    $0x8000000000000000,%rdx\n+    \/\/    7:   00 00 80\n+    \/\/    a:   48 39 d0                cmp    %rdx,%rax\n+    \/\/    d:   75 08                   jne    17 <normal>\n+    \/\/    f:   33 d2                   xor    %edx,%edx\n+    \/\/   11:   48 83 f9 ff             cmp    $0xffffffffffffffff,$div\n+    \/\/   15:   74 05                   je     1c <done>\n+    \/\/ 0000000000000017 <normal>:\n+    \/\/   17:   48 99                   cqto\n+    \/\/   19:   48 f7 f9                idiv   $div\n+    \/\/ 000000000000001c <done>:\n+    Label normal;\n+    Label done;\n+\n+    \/\/ mov    $0x8000000000000000,%rdx\n+    __ mov64(as_Register(RDX_enc), 0x8000000000000000);\n+\n+    \/\/ cmp    %rdx,%rax\n+    __ cmpq(as_Register(RAX_enc), as_Register(RDX_enc));\n+\n+    \/\/ jne    17 <normal>\n+    __ jccb(Assembler::notEqual, normal);\n+\n+    \/\/ xor    %edx,%edx\n+    __ xorl(as_Register(RDX_enc), as_Register(RDX_enc));\n+\n+    \/\/ cmp    $0xffffffffffffffff,$div\n+    __ cmpq($div$$Register, -1);\n+\n+    \/\/ je     1e <done>\n+    __ jccb(Assembler::equal, done);\n+\n+    \/\/ <normal>\n+    \/\/ cqto\n+    __ bind(normal);\n+    __ cdqq();\n+\n+    \/\/ idivq (note: must be emitted by the user of this rule)\n+    \/\/ <done>\n+    __ idivq($div$$Register);\n+    __ bind(done);\n+  %}\n+\n+  enc_class clear_avx %{\n+    DEBUG_ONLY(int off0 = __ offset());\n+    if (generate_vzeroupper(Compile::current())) {\n+      \/\/ Clear upper bits of YMM registers to avoid AVX <-> SSE transition penalty\n+      \/\/ Clear upper bits of YMM registers when current compiled code uses\n+      \/\/ wide vectors to avoid AVX <-> SSE transition penalty during call.\n+      __ vzeroupper();\n+    }\n+    DEBUG_ONLY(int off1 = __ offset());\n+    assert(off1 - off0 == clear_avx_size(), \"correct size prediction\");\n+  %}\n+\n+  enc_class Java_To_Runtime(method meth) %{\n+    __ lea(r10, RuntimeAddress((address)$meth$$method));\n+    __ call(r10);\n+    __ post_call_nop();\n+  %}\n+\n+  enc_class Java_Static_Call(method meth)\n+  %{\n+    \/\/ JAVA STATIC CALL\n+    \/\/ CALL to fixup routine.  Fixup routine uses ScopeDesc info to\n+    \/\/ determine who we intended to call.\n+    if (!_method) {\n+      __ call(RuntimeAddress(CAST_FROM_FN_PTR(address, $meth$$method)));\n+    } else if (_method->intrinsic_id() == vmIntrinsicID::_ensureMaterializedForStackWalk) {\n+      \/\/ The NOP here is purely to ensure that eliding a call to\n+      \/\/ JVM_EnsureMaterializedForStackWalk doesn't change the code size.\n+      __ addr_nop_5();\n+      __ block_comment(\"call JVM_EnsureMaterializedForStackWalk (elided)\");\n+    } else {\n+      int method_index = resolved_method_index(masm);\n+      RelocationHolder rspec = _optimized_virtual ? opt_virtual_call_Relocation::spec(method_index)\n+                                                  : static_call_Relocation::spec(method_index);\n+      address mark = __ pc();\n+      int call_offset = __ offset();\n+      __ call(AddressLiteral(CAST_FROM_FN_PTR(address, $meth$$method), rspec));\n+      if (CodeBuffer::supports_shared_stubs() && _method->can_be_statically_bound()) {\n+        \/\/ Calls of the same statically bound method can share\n+        \/\/ a stub to the interpreter.\n+        __ code()->shared_stub_to_interp_for(_method, call_offset);\n+      } else {\n+        \/\/ Emit stubs for static call.\n+        address stub = CompiledDirectCall::emit_to_interp_stub(masm, mark);\n+        __ clear_inst_mark();\n+        if (stub == nullptr) {\n+          ciEnv::current()->record_failure(\"CodeCache is full\");\n+          return;\n+        }\n+      }\n+    }\n+    __ post_call_nop();\n+  %}\n+\n+  enc_class Java_Dynamic_Call(method meth) %{\n+    __ ic_call((address)$meth$$method, resolved_method_index(masm));\n+    __ post_call_nop();\n+  %}\n+\n+  enc_class call_epilog %{\n+    if (VerifyStackAtCalls) {\n+      \/\/ Check that stack depth is unchanged: find majik cookie on stack\n+      int framesize = ra_->reg2offset_unchecked(OptoReg::add(ra_->_matcher._old_SP, -3*VMRegImpl::slots_per_word));\n+      Label L;\n+      __ cmpptr(Address(rsp, framesize), (int32_t)0xbadb100d);\n+      __ jccb(Assembler::equal, L);\n+      \/\/ Die if stack mismatch\n+      __ int3();\n+      __ bind(L);\n+    }\n+  %}\n+\n+%}\n+\n+\/\/----------FRAME--------------------------------------------------------------\n+\/\/ Definition of frame structure and management information.\n+\/\/\n+\/\/  S T A C K   L A Y O U T    Allocators stack-slot number\n+\/\/                             |   (to get allocators register number\n+\/\/  G  Owned by    |        |  v    add OptoReg::stack0())\n+\/\/  r   CALLER     |        |\n+\/\/  o     |        +--------+      pad to even-align allocators stack-slot\n+\/\/  w     V        |  pad0  |        numbers; owned by CALLER\n+\/\/  t   -----------+--------+----> Matcher::_in_arg_limit, unaligned\n+\/\/  h     ^        |   in   |  5\n+\/\/        |        |  args  |  4   Holes in incoming args owned by SELF\n+\/\/  |     |        |        |  3\n+\/\/  |     |        +--------+\n+\/\/  V     |        | old out|      Empty on Intel, window on Sparc\n+\/\/        |    old |preserve|      Must be even aligned.\n+\/\/        |     SP-+--------+----> Matcher::_old_SP, even aligned\n+\/\/        |        |   in   |  3   area for Intel ret address\n+\/\/     Owned by    |preserve|      Empty on Sparc.\n+\/\/       SELF      +--------+\n+\/\/        |        |  pad2  |  2   pad to align old SP\n+\/\/        |        +--------+  1\n+\/\/        |        | locks  |  0\n+\/\/        |        +--------+----> OptoReg::stack0(), even aligned\n+\/\/        |        |  pad1  | 11   pad to align new SP\n+\/\/        |        +--------+\n+\/\/        |        |        | 10\n+\/\/        |        | spills |  9   spills\n+\/\/        V        |        |  8   (pad0 slot for callee)\n+\/\/      -----------+--------+----> Matcher::_out_arg_limit, unaligned\n+\/\/        ^        |  out   |  7\n+\/\/        |        |  args  |  6   Holes in outgoing args owned by CALLEE\n+\/\/     Owned by    +--------+\n+\/\/      CALLEE     | new out|  6   Empty on Intel, window on Sparc\n+\/\/        |    new |preserve|      Must be even-aligned.\n+\/\/        |     SP-+--------+----> Matcher::_new_SP, even aligned\n+\/\/        |        |        |\n+\/\/\n+\/\/ Note 1: Only region 8-11 is determined by the allocator.  Region 0-5 is\n+\/\/         known from SELF's arguments and the Java calling convention.\n+\/\/         Region 6-7 is determined per call site.\n+\/\/ Note 2: If the calling convention leaves holes in the incoming argument\n+\/\/         area, those holes are owned by SELF.  Holes in the outgoing area\n+\/\/         are owned by the CALLEE.  Holes should not be necessary in the\n+\/\/         incoming area, as the Java calling convention is completely under\n+\/\/         the control of the AD file.  Doubles can be sorted and packed to\n+\/\/         avoid holes.  Holes in the outgoing arguments may be necessary for\n+\/\/         varargs C calling conventions.\n+\/\/ Note 3: Region 0-3 is even aligned, with pad2 as needed.  Region 3-5 is\n+\/\/         even aligned with pad0 as needed.\n+\/\/         Region 6 is even aligned.  Region 6-7 is NOT even aligned;\n+\/\/         region 6-11 is even aligned; it may be padded out more so that\n+\/\/         the region from SP to FP meets the minimum stack alignment.\n+\/\/ Note 4: For I2C adapters, the incoming FP may not meet the minimum stack\n+\/\/         alignment.  Region 11, pad1, may be dynamically extended so that\n+\/\/         SP meets the minimum alignment.\n+\n+frame\n+%{\n+  \/\/ These three registers define part of the calling convention\n+  \/\/ between compiled code and the interpreter.\n+  inline_cache_reg(RAX);                \/\/ Inline Cache Register\n+\n+  \/\/ Optional: name the operand used by cisc-spilling to access\n+  \/\/ [stack_pointer + offset]\n+  cisc_spilling_operand_name(indOffset32);\n+\n+  \/\/ Number of stack slots consumed by locking an object\n+  sync_stack_slots(2);\n+\n+  \/\/ Compiled code's Frame Pointer\n+  frame_pointer(RSP);\n+\n+  \/\/ Interpreter stores its frame pointer in a register which is\n+  \/\/ stored to the stack by I2CAdaptors.\n+  \/\/ I2CAdaptors convert from interpreted java to compiled java.\n+  interpreter_frame_pointer(RBP);\n+\n+  \/\/ Stack alignment requirement\n+  stack_alignment(StackAlignmentInBytes); \/\/ Alignment size in bytes (128-bit -> 16 bytes)\n+\n+  \/\/ Number of outgoing stack slots killed above the out_preserve_stack_slots\n+  \/\/ for calls to C.  Supports the var-args backing area for register parms.\n+  varargs_C_out_slots_killed(frame::arg_reg_save_area_bytes\/BytesPerInt);\n+\n+  \/\/ The after-PROLOG location of the return address.  Location of\n+  \/\/ return address specifies a type (REG or STACK) and a number\n+  \/\/ representing the register number (i.e. - use a register name) or\n+  \/\/ stack slot.\n+  \/\/ Ret Addr is on stack in slot 0 if no locks or verification or alignment.\n+  \/\/ Otherwise, it is above the locks and verification slot and alignment word\n+  return_addr(STACK - 2 +\n+              align_up((Compile::current()->in_preserve_stack_slots() +\n+                        Compile::current()->fixed_slots()),\n+                       stack_alignment_in_slots()));\n+\n+  \/\/ Location of compiled Java return values.  Same as C for now.\n+  return_value\n+  %{\n+    assert(ideal_reg >= Op_RegI && ideal_reg <= Op_RegL,\n+           \"only return normal values\");\n+\n+    static const int lo[Op_RegL + 1] = {\n+      0,\n+      0,\n+      RAX_num,  \/\/ Op_RegN\n+      RAX_num,  \/\/ Op_RegI\n+      RAX_num,  \/\/ Op_RegP\n+      XMM0_num, \/\/ Op_RegF\n+      XMM0_num, \/\/ Op_RegD\n+      RAX_num   \/\/ Op_RegL\n+    };\n+    static const int hi[Op_RegL + 1] = {\n+      0,\n+      0,\n+      OptoReg::Bad, \/\/ Op_RegN\n+      OptoReg::Bad, \/\/ Op_RegI\n+      RAX_H_num,    \/\/ Op_RegP\n+      OptoReg::Bad, \/\/ Op_RegF\n+      XMM0b_num,    \/\/ Op_RegD\n+      RAX_H_num     \/\/ Op_RegL\n+    };\n+    \/\/ Excluded flags and vector registers.\n+    assert(ARRAY_SIZE(hi) == _last_machine_leaf - 8, \"missing type\");\n+    return OptoRegPair(hi[ideal_reg], lo[ideal_reg]);\n+  %}\n+%}\n+\n+\/\/----------ATTRIBUTES---------------------------------------------------------\n+\/\/----------Operand Attributes-------------------------------------------------\n+op_attrib op_cost(0);        \/\/ Required cost attribute\n+\n+\/\/----------Instruction Attributes---------------------------------------------\n+ins_attrib ins_cost(100);       \/\/ Required cost attribute\n+ins_attrib ins_size(8);         \/\/ Required size attribute (in bits)\n+ins_attrib ins_short_branch(0); \/\/ Required flag: is this instruction\n+                                \/\/ a non-matching short branch variant\n+                                \/\/ of some long branch?\n+ins_attrib ins_alignment(1);    \/\/ Required alignment attribute (must\n+                                \/\/ be a power of 2) specifies the\n+                                \/\/ alignment that some part of the\n+                                \/\/ instruction (not necessarily the\n+                                \/\/ start) requires.  If > 1, a\n+                                \/\/ compute_padding() function must be\n+                                \/\/ provided for the instruction\n+\n+\/\/ Whether this node is expanded during code emission into a sequence of\n+\/\/ instructions and the first instruction can perform an implicit null check.\n+ins_attrib ins_is_late_expanded_null_check_candidate(false);\n+\n+\/\/----------OPERANDS-----------------------------------------------------------\n+\/\/ Operand definitions must precede instruction definitions for correct parsing\n+\/\/ in the ADLC because operands constitute user defined types which are used in\n+\/\/ instruction definitions.\n+\n+\/\/----------Simple Operands----------------------------------------------------\n+\/\/ Immediate Operands\n+\/\/ Integer Immediate\n+operand immI()\n+%{\n+  match(ConI);\n+\n+  op_cost(10);\n+  format %{ %}\n+  interface(CONST_INTER);\n+%}\n+\n+\/\/ Constant for test vs zero\n+operand immI_0()\n+%{\n+  predicate(n->get_int() == 0);\n+  match(ConI);\n+\n+  op_cost(0);\n+  format %{ %}\n+  interface(CONST_INTER);\n+%}\n+\n+\/\/ Constant for increment\n+operand immI_1()\n+%{\n+  predicate(n->get_int() == 1);\n+  match(ConI);\n+\n+  op_cost(0);\n+  format %{ %}\n+  interface(CONST_INTER);\n+%}\n+\n+\/\/ Constant for decrement\n+operand immI_M1()\n+%{\n+  predicate(n->get_int() == -1);\n+  match(ConI);\n+\n+  op_cost(0);\n+  format %{ %}\n+  interface(CONST_INTER);\n+%}\n+\n+operand immI_2()\n+%{\n+  predicate(n->get_int() == 2);\n+  match(ConI);\n+\n+  op_cost(0);\n+  format %{ %}\n+  interface(CONST_INTER);\n+%}\n+\n+operand immI_4()\n+%{\n+  predicate(n->get_int() == 4);\n+  match(ConI);\n+\n+  op_cost(0);\n+  format %{ %}\n+  interface(CONST_INTER);\n+%}\n+\n+operand immI_8()\n+%{\n+  predicate(n->get_int() == 8);\n+  match(ConI);\n+\n+  op_cost(0);\n+  format %{ %}\n+  interface(CONST_INTER);\n+%}\n+\n+\/\/ Valid scale values for addressing modes\n+operand immI2()\n+%{\n+  predicate(0 <= n->get_int() && (n->get_int() <= 3));\n+  match(ConI);\n+\n+  format %{ %}\n+  interface(CONST_INTER);\n+%}\n+\n+operand immU7()\n+%{\n+  predicate((0 <= n->get_int()) && (n->get_int() <= 0x7F));\n+  match(ConI);\n+\n+  op_cost(5);\n+  format %{ %}\n+  interface(CONST_INTER);\n+%}\n+\n+operand immI8()\n+%{\n+  predicate((-0x80 <= n->get_int()) && (n->get_int() < 0x80));\n+  match(ConI);\n+\n+  op_cost(5);\n+  format %{ %}\n+  interface(CONST_INTER);\n+%}\n+\n+operand immU8()\n+%{\n+  predicate((0 <= n->get_int()) && (n->get_int() <= 255));\n+  match(ConI);\n+\n+  op_cost(5);\n+  format %{ %}\n+  interface(CONST_INTER);\n+%}\n+\n+operand immI16()\n+%{\n+  predicate((-32768 <= n->get_int()) && (n->get_int() <= 32767));\n+  match(ConI);\n+\n+  op_cost(10);\n+  format %{ %}\n+  interface(CONST_INTER);\n+%}\n+\n+\/\/ Int Immediate non-negative\n+operand immU31()\n+%{\n+  predicate(n->get_int() >= 0);\n+  match(ConI);\n+\n+  op_cost(0);\n+  format %{ %}\n+  interface(CONST_INTER);\n+%}\n+\n+\/\/ Pointer Immediate\n+operand immP()\n+%{\n+  match(ConP);\n+\n+  op_cost(10);\n+  format %{ %}\n+  interface(CONST_INTER);\n+%}\n+\n+\/\/ Null Pointer Immediate\n+operand immP0()\n+%{\n+  predicate(n->get_ptr() == 0);\n+  match(ConP);\n+\n+  op_cost(5);\n+  format %{ %}\n+  interface(CONST_INTER);\n+%}\n+\n+\/\/ Pointer Immediate\n+operand immN() %{\n+  match(ConN);\n+\n+  op_cost(10);\n+  format %{ %}\n+  interface(CONST_INTER);\n+%}\n+\n+operand immNKlass() %{\n+  match(ConNKlass);\n+\n+  op_cost(10);\n+  format %{ %}\n+  interface(CONST_INTER);\n+%}\n+\n+\/\/ Null Pointer Immediate\n+operand immN0() %{\n+  predicate(n->get_narrowcon() == 0);\n+  match(ConN);\n+\n+  op_cost(5);\n+  format %{ %}\n+  interface(CONST_INTER);\n+%}\n+\n+operand immP31()\n+%{\n+  predicate(n->as_Type()->type()->reloc() == relocInfo::none\n+            && (n->get_ptr() >> 31) == 0);\n+  match(ConP);\n+\n+  op_cost(5);\n+  format %{ %}\n+  interface(CONST_INTER);\n+%}\n+\n+\n+\/\/ Long Immediate\n+operand immL()\n+%{\n+  match(ConL);\n+\n+  op_cost(20);\n+  format %{ %}\n+  interface(CONST_INTER);\n+%}\n+\n+\/\/ Long Immediate 8-bit\n+operand immL8()\n+%{\n+  predicate(-0x80L <= n->get_long() && n->get_long() < 0x80L);\n+  match(ConL);\n+\n+  op_cost(5);\n+  format %{ %}\n+  interface(CONST_INTER);\n+%}\n+\n+\/\/ Long Immediate 32-bit unsigned\n+operand immUL32()\n+%{\n+  predicate(n->get_long() == (unsigned int) (n->get_long()));\n+  match(ConL);\n+\n+  op_cost(10);\n+  format %{ %}\n+  interface(CONST_INTER);\n+%}\n+\n+\/\/ Long Immediate 32-bit signed\n+operand immL32()\n+%{\n+  predicate(n->get_long() == (int) (n->get_long()));\n+  match(ConL);\n+\n+  op_cost(15);\n+  format %{ %}\n+  interface(CONST_INTER);\n+%}\n+\n+operand immL_Pow2()\n+%{\n+  predicate(is_power_of_2((julong)n->get_long()));\n+  match(ConL);\n+\n+  op_cost(15);\n+  format %{ %}\n+  interface(CONST_INTER);\n+%}\n+\n+operand immL_NotPow2()\n+%{\n+  predicate(is_power_of_2((julong)~n->get_long()));\n+  match(ConL);\n+\n+  op_cost(15);\n+  format %{ %}\n+  interface(CONST_INTER);\n+%}\n+\n+\/\/ Long Immediate zero\n+operand immL0()\n+%{\n+  predicate(n->get_long() == 0L);\n+  match(ConL);\n+\n+  op_cost(10);\n+  format %{ %}\n+  interface(CONST_INTER);\n+%}\n+\n+\/\/ Constant for increment\n+operand immL1()\n+%{\n+  predicate(n->get_long() == 1);\n+  match(ConL);\n+\n+  format %{ %}\n+  interface(CONST_INTER);\n+%}\n+\n+\/\/ Constant for decrement\n+operand immL_M1()\n+%{\n+  predicate(n->get_long() == -1);\n+  match(ConL);\n+\n+  format %{ %}\n+  interface(CONST_INTER);\n+%}\n+\n+\/\/ Long Immediate: low 32-bit mask\n+operand immL_32bits()\n+%{\n+  predicate(n->get_long() == 0xFFFFFFFFL);\n+  match(ConL);\n+  op_cost(20);\n+\n+  format %{ %}\n+  interface(CONST_INTER);\n+%}\n+\n+\/\/ Int Immediate: 2^n-1, positive\n+operand immI_Pow2M1()\n+%{\n+  predicate((n->get_int() > 0)\n+            && is_power_of_2((juint)n->get_int() + 1));\n+  match(ConI);\n+\n+  op_cost(20);\n+  format %{ %}\n+  interface(CONST_INTER);\n+%}\n+\n+\/\/ Float Immediate zero\n+operand immF0()\n+%{\n+  predicate(jint_cast(n->getf()) == 0);\n+  match(ConF);\n+\n+  op_cost(5);\n+  format %{ %}\n+  interface(CONST_INTER);\n+%}\n+\n+\/\/ Float Immediate\n+operand immF()\n+%{\n+  match(ConF);\n+\n+  op_cost(15);\n+  format %{ %}\n+  interface(CONST_INTER);\n+%}\n+\n+\/\/ Half Float Immediate\n+operand immH()\n+%{\n+  match(ConH);\n+\n+  op_cost(15);\n+  format %{ %}\n+  interface(CONST_INTER);\n+%}\n+\n+\/\/ Double Immediate zero\n+operand immD0()\n+%{\n+  predicate(jlong_cast(n->getd()) == 0);\n+  match(ConD);\n+\n+  op_cost(5);\n+  format %{ %}\n+  interface(CONST_INTER);\n+%}\n+\n+\/\/ Double Immediate\n+operand immD()\n+%{\n+  match(ConD);\n+\n+  op_cost(15);\n+  format %{ %}\n+  interface(CONST_INTER);\n+%}\n+\n+\/\/ Immediates for special shifts (sign extend)\n+\n+\/\/ Constants for increment\n+operand immI_16()\n+%{\n+  predicate(n->get_int() == 16);\n+  match(ConI);\n+\n+  format %{ %}\n+  interface(CONST_INTER);\n+%}\n+\n+operand immI_24()\n+%{\n+  predicate(n->get_int() == 24);\n+  match(ConI);\n+\n+  format %{ %}\n+  interface(CONST_INTER);\n+%}\n+\n+\/\/ Constant for byte-wide masking\n+operand immI_255()\n+%{\n+  predicate(n->get_int() == 255);\n+  match(ConI);\n+\n+  format %{ %}\n+  interface(CONST_INTER);\n+%}\n+\n+\/\/ Constant for short-wide masking\n+operand immI_65535()\n+%{\n+  predicate(n->get_int() == 65535);\n+  match(ConI);\n+\n+  format %{ %}\n+  interface(CONST_INTER);\n+%}\n+\n+\/\/ Constant for byte-wide masking\n+operand immL_255()\n+%{\n+  predicate(n->get_long() == 255);\n+  match(ConL);\n+\n+  format %{ %}\n+  interface(CONST_INTER);\n+%}\n+\n+\/\/ Constant for short-wide masking\n+operand immL_65535()\n+%{\n+  predicate(n->get_long() == 65535);\n+  match(ConL);\n+\n+  format %{ %}\n+  interface(CONST_INTER);\n+%}\n+\n+operand kReg()\n+%{\n+  constraint(ALLOC_IN_RC(vectmask_reg));\n+  match(RegVectMask);\n+  format %{%}\n+  interface(REG_INTER);\n+%}\n+\n+\/\/ Register Operands\n+\/\/ Integer Register\n+operand rRegI()\n+%{\n+  constraint(ALLOC_IN_RC(int_reg));\n+  match(RegI);\n+\n+  match(rax_RegI);\n+  match(rbx_RegI);\n+  match(rcx_RegI);\n+  match(rdx_RegI);\n+  match(rdi_RegI);\n+\n+  format %{ %}\n+  interface(REG_INTER);\n+%}\n+\n+\/\/ Special Registers\n+operand rax_RegI()\n+%{\n+  constraint(ALLOC_IN_RC(int_rax_reg));\n+  match(RegI);\n+  match(rRegI);\n+\n+  format %{ \"RAX\" %}\n+  interface(REG_INTER);\n+%}\n+\n+\/\/ Special Registers\n+operand rbx_RegI()\n+%{\n+  constraint(ALLOC_IN_RC(int_rbx_reg));\n+  match(RegI);\n+  match(rRegI);\n+\n+  format %{ \"RBX\" %}\n+  interface(REG_INTER);\n+%}\n+\n+operand rcx_RegI()\n+%{\n+  constraint(ALLOC_IN_RC(int_rcx_reg));\n+  match(RegI);\n+  match(rRegI);\n+\n+  format %{ \"RCX\" %}\n+  interface(REG_INTER);\n+%}\n+\n+operand rdx_RegI()\n+%{\n+  constraint(ALLOC_IN_RC(int_rdx_reg));\n+  match(RegI);\n+  match(rRegI);\n+\n+  format %{ \"RDX\" %}\n+  interface(REG_INTER);\n+%}\n+\n+operand rdi_RegI()\n+%{\n+  constraint(ALLOC_IN_RC(int_rdi_reg));\n+  match(RegI);\n+  match(rRegI);\n+\n+  format %{ \"RDI\" %}\n+  interface(REG_INTER);\n+%}\n+\n+operand no_rax_rdx_RegI()\n+%{\n+  constraint(ALLOC_IN_RC(int_no_rax_rdx_reg));\n+  match(RegI);\n+  match(rbx_RegI);\n+  match(rcx_RegI);\n+  match(rdi_RegI);\n+\n+  format %{ %}\n+  interface(REG_INTER);\n+%}\n+\n+operand no_rbp_r13_RegI()\n+%{\n+  constraint(ALLOC_IN_RC(int_no_rbp_r13_reg));\n+  match(RegI);\n+  match(rRegI);\n+  match(rax_RegI);\n+  match(rbx_RegI);\n+  match(rcx_RegI);\n+  match(rdx_RegI);\n+  match(rdi_RegI);\n+\n+  format %{ %}\n+  interface(REG_INTER);\n+%}\n+\n+\/\/ Pointer Register\n+operand any_RegP()\n+%{\n+  constraint(ALLOC_IN_RC(any_reg));\n+  match(RegP);\n+  match(rax_RegP);\n+  match(rbx_RegP);\n+  match(rdi_RegP);\n+  match(rsi_RegP);\n+  match(rbp_RegP);\n+  match(r15_RegP);\n+  match(rRegP);\n+\n+  format %{ %}\n+  interface(REG_INTER);\n+%}\n+\n+operand rRegP()\n+%{\n+  constraint(ALLOC_IN_RC(ptr_reg));\n+  match(RegP);\n+  match(rax_RegP);\n+  match(rbx_RegP);\n+  match(rdi_RegP);\n+  match(rsi_RegP);\n+  match(rbp_RegP);  \/\/ See Q&A below about\n+  match(r15_RegP);  \/\/ r15_RegP and rbp_RegP.\n+\n+  format %{ %}\n+  interface(REG_INTER);\n+%}\n+\n+operand rRegN() %{\n+  constraint(ALLOC_IN_RC(int_reg));\n+  match(RegN);\n+\n+  format %{ %}\n+  interface(REG_INTER);\n+%}\n+\n+\/\/ Question: Why is r15_RegP (the read-only TLS register) a match for rRegP?\n+\/\/ Answer: Operand match rules govern the DFA as it processes instruction inputs.\n+\/\/ It's fine for an instruction input that expects rRegP to match a r15_RegP.\n+\/\/ The output of an instruction is controlled by the allocator, which respects\n+\/\/ register class masks, not match rules.  Unless an instruction mentions\n+\/\/ r15_RegP or any_RegP explicitly as its output, r15 will not be considered\n+\/\/ by the allocator as an input.\n+\/\/ The same logic applies to rbp_RegP being a match for rRegP: If PreserveFramePointer==true,\n+\/\/ the RBP is used as a proper frame pointer and is not included in ptr_reg. As a\n+\/\/ result, RBP is not included in the output of the instruction either.\n+\n+\/\/ This operand is not allowed to use RBP even if\n+\/\/ RBP is not used to hold the frame pointer.\n+operand no_rbp_RegP()\n+%{\n+  constraint(ALLOC_IN_RC(ptr_reg_no_rbp));\n+  match(RegP);\n+  match(rbx_RegP);\n+  match(rsi_RegP);\n+  match(rdi_RegP);\n+\n+  format %{ %}\n+  interface(REG_INTER);\n+%}\n+\n+\/\/ Special Registers\n+\/\/ Return a pointer value\n+operand rax_RegP()\n+%{\n+  constraint(ALLOC_IN_RC(ptr_rax_reg));\n+  match(RegP);\n+  match(rRegP);\n+\n+  format %{ %}\n+  interface(REG_INTER);\n+%}\n+\n+\/\/ Special Registers\n+\/\/ Return a compressed pointer value\n+operand rax_RegN()\n+%{\n+  constraint(ALLOC_IN_RC(int_rax_reg));\n+  match(RegN);\n+  match(rRegN);\n+\n+  format %{ %}\n+  interface(REG_INTER);\n+%}\n+\n+\/\/ Used in AtomicAdd\n+operand rbx_RegP()\n+%{\n+  constraint(ALLOC_IN_RC(ptr_rbx_reg));\n+  match(RegP);\n+  match(rRegP);\n+\n+  format %{ %}\n+  interface(REG_INTER);\n+%}\n+\n+operand rsi_RegP()\n+%{\n+  constraint(ALLOC_IN_RC(ptr_rsi_reg));\n+  match(RegP);\n+  match(rRegP);\n+\n+  format %{ %}\n+  interface(REG_INTER);\n+%}\n+\n+operand rbp_RegP()\n+%{\n+  constraint(ALLOC_IN_RC(ptr_rbp_reg));\n+  match(RegP);\n+  match(rRegP);\n+\n+  format %{ %}\n+  interface(REG_INTER);\n+%}\n+\n+\/\/ Used in rep stosq\n+operand rdi_RegP()\n+%{\n+  constraint(ALLOC_IN_RC(ptr_rdi_reg));\n+  match(RegP);\n+  match(rRegP);\n+\n+  format %{ %}\n+  interface(REG_INTER);\n+%}\n+\n+operand r15_RegP()\n+%{\n+  constraint(ALLOC_IN_RC(ptr_r15_reg));\n+  match(RegP);\n+  match(rRegP);\n+\n+  format %{ %}\n+  interface(REG_INTER);\n+%}\n+\n+operand rRegL()\n+%{\n+  constraint(ALLOC_IN_RC(long_reg));\n+  match(RegL);\n+  match(rax_RegL);\n+  match(rdx_RegL);\n+\n+  format %{ %}\n+  interface(REG_INTER);\n+%}\n+\n+\/\/ Special Registers\n+operand no_rax_rdx_RegL()\n+%{\n+  constraint(ALLOC_IN_RC(long_no_rax_rdx_reg));\n+  match(RegL);\n+  match(rRegL);\n+\n+  format %{ %}\n+  interface(REG_INTER);\n+%}\n+\n+operand rax_RegL()\n+%{\n+  constraint(ALLOC_IN_RC(long_rax_reg));\n+  match(RegL);\n+  match(rRegL);\n+\n+  format %{ \"RAX\" %}\n+  interface(REG_INTER);\n+%}\n+\n+operand rcx_RegL()\n+%{\n+  constraint(ALLOC_IN_RC(long_rcx_reg));\n+  match(RegL);\n+  match(rRegL);\n+\n+  format %{ %}\n+  interface(REG_INTER);\n+%}\n+\n+operand rdx_RegL()\n+%{\n+  constraint(ALLOC_IN_RC(long_rdx_reg));\n+  match(RegL);\n+  match(rRegL);\n+\n+  format %{ %}\n+  interface(REG_INTER);\n+%}\n+\n+operand r11_RegL()\n+%{\n+  constraint(ALLOC_IN_RC(long_r11_reg));\n+  match(RegL);\n+  match(rRegL);\n+\n+  format %{ %}\n+  interface(REG_INTER);\n+%}\n+\n+operand no_rbp_r13_RegL()\n+%{\n+  constraint(ALLOC_IN_RC(long_no_rbp_r13_reg));\n+  match(RegL);\n+  match(rRegL);\n+  match(rax_RegL);\n+  match(rcx_RegL);\n+  match(rdx_RegL);\n+\n+  format %{ %}\n+  interface(REG_INTER);\n+%}\n+\n+\/\/ Flags register, used as output of compare instructions\n+operand rFlagsReg()\n+%{\n+  constraint(ALLOC_IN_RC(int_flags));\n+  match(RegFlags);\n+\n+  format %{ \"RFLAGS\" %}\n+  interface(REG_INTER);\n+%}\n+\n+\/\/ Flags register, used as output of FLOATING POINT compare instructions\n+operand rFlagsRegU()\n+%{\n+  constraint(ALLOC_IN_RC(int_flags));\n+  match(RegFlags);\n+\n+  format %{ \"RFLAGS_U\" %}\n+  interface(REG_INTER);\n+%}\n+\n+operand rFlagsRegUCF() %{\n+  constraint(ALLOC_IN_RC(int_flags));\n+  match(RegFlags);\n+  predicate(false);\n+\n+  format %{ \"RFLAGS_U_CF\" %}\n+  interface(REG_INTER);\n+%}\n+\n+\/\/ Float register operands\n+operand regF() %{\n+   constraint(ALLOC_IN_RC(float_reg));\n+   match(RegF);\n+\n+   format %{ %}\n+   interface(REG_INTER);\n+%}\n+\n+\/\/ Float register operands\n+operand legRegF() %{\n+   constraint(ALLOC_IN_RC(float_reg_legacy));\n+   match(RegF);\n+\n+   format %{ %}\n+   interface(REG_INTER);\n+%}\n+\n+\/\/ Float register operands\n+operand vlRegF() %{\n+   constraint(ALLOC_IN_RC(float_reg_vl));\n+   match(RegF);\n+\n+   format %{ %}\n+   interface(REG_INTER);\n+%}\n+\n+\/\/ Double register operands\n+operand regD() %{\n+   constraint(ALLOC_IN_RC(double_reg));\n+   match(RegD);\n+\n+   format %{ %}\n+   interface(REG_INTER);\n+%}\n+\n+\/\/ Double register operands\n+operand legRegD() %{\n+   constraint(ALLOC_IN_RC(double_reg_legacy));\n+   match(RegD);\n+\n+   format %{ %}\n+   interface(REG_INTER);\n+%}\n+\n+\/\/ Double register operands\n+operand vlRegD() %{\n+   constraint(ALLOC_IN_RC(double_reg_vl));\n+   match(RegD);\n+\n+   format %{ %}\n+   interface(REG_INTER);\n+%}\n+\n+\/\/----------Memory Operands----------------------------------------------------\n+\/\/ Direct Memory Operand\n+\/\/ operand direct(immP addr)\n+\/\/ %{\n+\/\/   match(addr);\n+\n+\/\/   format %{ \"[$addr]\" %}\n+\/\/   interface(MEMORY_INTER) %{\n+\/\/     base(0xFFFFFFFF);\n+\/\/     index(0x4);\n+\/\/     scale(0x0);\n+\/\/     disp($addr);\n+\/\/   %}\n+\/\/ %}\n+\n+\/\/ Indirect Memory Operand\n+operand indirect(any_RegP reg)\n+%{\n+  constraint(ALLOC_IN_RC(ptr_reg));\n+  match(reg);\n+\n+  format %{ \"[$reg]\" %}\n+  interface(MEMORY_INTER) %{\n+    base($reg);\n+    index(0x4);\n+    scale(0x0);\n+    disp(0x0);\n+  %}\n+%}\n+\n+\/\/ Indirect Memory Plus Short Offset Operand\n+operand indOffset8(any_RegP reg, immL8 off)\n+%{\n+  constraint(ALLOC_IN_RC(ptr_reg));\n+  match(AddP reg off);\n+\n+  format %{ \"[$reg + $off (8-bit)]\" %}\n+  interface(MEMORY_INTER) %{\n+    base($reg);\n+    index(0x4);\n+    scale(0x0);\n+    disp($off);\n+  %}\n+%}\n+\n+\/\/ Indirect Memory Plus Long Offset Operand\n+operand indOffset32(any_RegP reg, immL32 off)\n+%{\n+  constraint(ALLOC_IN_RC(ptr_reg));\n+  match(AddP reg off);\n+\n+  format %{ \"[$reg + $off (32-bit)]\" %}\n+  interface(MEMORY_INTER) %{\n+    base($reg);\n+    index(0x4);\n+    scale(0x0);\n+    disp($off);\n+  %}\n+%}\n+\n+\/\/ Indirect Memory Plus Index Register Plus Offset Operand\n+operand indIndexOffset(any_RegP reg, rRegL lreg, immL32 off)\n+%{\n+  constraint(ALLOC_IN_RC(ptr_reg));\n+  match(AddP (AddP reg lreg) off);\n+\n+  op_cost(10);\n+  format %{\"[$reg + $off + $lreg]\" %}\n+  interface(MEMORY_INTER) %{\n+    base($reg);\n+    index($lreg);\n+    scale(0x0);\n+    disp($off);\n+  %}\n+%}\n+\n+\/\/ Indirect Memory Plus Index Register Plus Offset Operand\n+operand indIndex(any_RegP reg, rRegL lreg)\n+%{\n+  constraint(ALLOC_IN_RC(ptr_reg));\n+  match(AddP reg lreg);\n+\n+  op_cost(10);\n+  format %{\"[$reg + $lreg]\" %}\n+  interface(MEMORY_INTER) %{\n+    base($reg);\n+    index($lreg);\n+    scale(0x0);\n+    disp(0x0);\n+  %}\n+%}\n+\n+\/\/ Indirect Memory Times Scale Plus Index Register\n+operand indIndexScale(any_RegP reg, rRegL lreg, immI2 scale)\n+%{\n+  constraint(ALLOC_IN_RC(ptr_reg));\n+  match(AddP reg (LShiftL lreg scale));\n+\n+  op_cost(10);\n+  format %{\"[$reg + $lreg << $scale]\" %}\n+  interface(MEMORY_INTER) %{\n+    base($reg);\n+    index($lreg);\n+    scale($scale);\n+    disp(0x0);\n+  %}\n+%}\n+\n+operand indPosIndexScale(any_RegP reg, rRegI idx, immI2 scale)\n+%{\n+  constraint(ALLOC_IN_RC(ptr_reg));\n+  predicate(n->in(3)->in(1)->as_Type()->type()->is_long()->_lo >= 0);\n+  match(AddP reg (LShiftL (ConvI2L idx) scale));\n+\n+  op_cost(10);\n+  format %{\"[$reg + pos $idx << $scale]\" %}\n+  interface(MEMORY_INTER) %{\n+    base($reg);\n+    index($idx);\n+    scale($scale);\n+    disp(0x0);\n+  %}\n+%}\n+\n+\/\/ Indirect Memory Times Scale Plus Index Register Plus Offset Operand\n+operand indIndexScaleOffset(any_RegP reg, immL32 off, rRegL lreg, immI2 scale)\n+%{\n+  constraint(ALLOC_IN_RC(ptr_reg));\n+  match(AddP (AddP reg (LShiftL lreg scale)) off);\n+\n+  op_cost(10);\n+  format %{\"[$reg + $off + $lreg << $scale]\" %}\n+  interface(MEMORY_INTER) %{\n+    base($reg);\n+    index($lreg);\n+    scale($scale);\n+    disp($off);\n+  %}\n+%}\n+\n+\/\/ Indirect Memory Plus Positive Index Register Plus Offset Operand\n+operand indPosIndexOffset(any_RegP reg, immL32 off, rRegI idx)\n+%{\n+  constraint(ALLOC_IN_RC(ptr_reg));\n+  predicate(n->in(2)->in(3)->as_Type()->type()->is_long()->_lo >= 0);\n+  match(AddP (AddP reg (ConvI2L idx)) off);\n+\n+  op_cost(10);\n+  format %{\"[$reg + $off + $idx]\" %}\n+  interface(MEMORY_INTER) %{\n+    base($reg);\n+    index($idx);\n+    scale(0x0);\n+    disp($off);\n+  %}\n+%}\n+\n+\/\/ Indirect Memory Times Scale Plus Positive Index Register Plus Offset Operand\n+operand indPosIndexScaleOffset(any_RegP reg, immL32 off, rRegI idx, immI2 scale)\n+%{\n+  constraint(ALLOC_IN_RC(ptr_reg));\n+  predicate(n->in(2)->in(3)->in(1)->as_Type()->type()->is_long()->_lo >= 0);\n+  match(AddP (AddP reg (LShiftL (ConvI2L idx) scale)) off);\n+\n+  op_cost(10);\n+  format %{\"[$reg + $off + $idx << $scale]\" %}\n+  interface(MEMORY_INTER) %{\n+    base($reg);\n+    index($idx);\n+    scale($scale);\n+    disp($off);\n+  %}\n+%}\n+\n+\/\/ Indirect Narrow Oop Plus Offset Operand\n+\/\/ Note: x86 architecture doesn't support \"scale * index + offset\" without a base\n+\/\/ we can't free r12 even with CompressedOops::base() == nullptr.\n+operand indCompressedOopOffset(rRegN reg, immL32 off) %{\n+  predicate(UseCompressedOops && (CompressedOops::shift() == Address::times_8));\n+  constraint(ALLOC_IN_RC(ptr_reg));\n+  match(AddP (DecodeN reg) off);\n+\n+  op_cost(10);\n+  format %{\"[R12 + $reg << 3 + $off] (compressed oop addressing)\" %}\n+  interface(MEMORY_INTER) %{\n+    base(0xc); \/\/ R12\n+    index($reg);\n+    scale(0x3);\n+    disp($off);\n+  %}\n+%}\n+\n+\/\/ Indirect Memory Operand\n+operand indirectNarrow(rRegN reg)\n+%{\n+  predicate(CompressedOops::shift() == 0);\n+  constraint(ALLOC_IN_RC(ptr_reg));\n+  match(DecodeN reg);\n+\n+  format %{ \"[$reg]\" %}\n+  interface(MEMORY_INTER) %{\n+    base($reg);\n+    index(0x4);\n+    scale(0x0);\n+    disp(0x0);\n+  %}\n+%}\n+\n+\/\/ Indirect Memory Plus Short Offset Operand\n+operand indOffset8Narrow(rRegN reg, immL8 off)\n+%{\n+  predicate(CompressedOops::shift() == 0);\n+  constraint(ALLOC_IN_RC(ptr_reg));\n+  match(AddP (DecodeN reg) off);\n+\n+  format %{ \"[$reg + $off (8-bit)]\" %}\n+  interface(MEMORY_INTER) %{\n+    base($reg);\n+    index(0x4);\n+    scale(0x0);\n+    disp($off);\n+  %}\n+%}\n+\n+\/\/ Indirect Memory Plus Long Offset Operand\n+operand indOffset32Narrow(rRegN reg, immL32 off)\n+%{\n+  predicate(CompressedOops::shift() == 0);\n+  constraint(ALLOC_IN_RC(ptr_reg));\n+  match(AddP (DecodeN reg) off);\n+\n+  format %{ \"[$reg + $off (32-bit)]\" %}\n+  interface(MEMORY_INTER) %{\n+    base($reg);\n+    index(0x4);\n+    scale(0x0);\n+    disp($off);\n+  %}\n+%}\n+\n+\/\/ Indirect Memory Plus Index Register Plus Offset Operand\n+operand indIndexOffsetNarrow(rRegN reg, rRegL lreg, immL32 off)\n+%{\n+  predicate(CompressedOops::shift() == 0);\n+  constraint(ALLOC_IN_RC(ptr_reg));\n+  match(AddP (AddP (DecodeN reg) lreg) off);\n+\n+  op_cost(10);\n+  format %{\"[$reg + $off + $lreg]\" %}\n+  interface(MEMORY_INTER) %{\n+    base($reg);\n+    index($lreg);\n+    scale(0x0);\n+    disp($off);\n+  %}\n+%}\n+\n+\/\/ Indirect Memory Plus Index Register Plus Offset Operand\n+operand indIndexNarrow(rRegN reg, rRegL lreg)\n+%{\n+  predicate(CompressedOops::shift() == 0);\n+  constraint(ALLOC_IN_RC(ptr_reg));\n+  match(AddP (DecodeN reg) lreg);\n+\n+  op_cost(10);\n+  format %{\"[$reg + $lreg]\" %}\n+  interface(MEMORY_INTER) %{\n+    base($reg);\n+    index($lreg);\n+    scale(0x0);\n+    disp(0x0);\n+  %}\n+%}\n+\n+\/\/ Indirect Memory Times Scale Plus Index Register\n+operand indIndexScaleNarrow(rRegN reg, rRegL lreg, immI2 scale)\n+%{\n+  predicate(CompressedOops::shift() == 0);\n+  constraint(ALLOC_IN_RC(ptr_reg));\n+  match(AddP (DecodeN reg) (LShiftL lreg scale));\n+\n+  op_cost(10);\n+  format %{\"[$reg + $lreg << $scale]\" %}\n+  interface(MEMORY_INTER) %{\n+    base($reg);\n+    index($lreg);\n+    scale($scale);\n+    disp(0x0);\n+  %}\n+%}\n+\n+\/\/ Indirect Memory Times Scale Plus Index Register Plus Offset Operand\n+operand indIndexScaleOffsetNarrow(rRegN reg, immL32 off, rRegL lreg, immI2 scale)\n+%{\n+  predicate(CompressedOops::shift() == 0);\n+  constraint(ALLOC_IN_RC(ptr_reg));\n+  match(AddP (AddP (DecodeN reg) (LShiftL lreg scale)) off);\n+\n+  op_cost(10);\n+  format %{\"[$reg + $off + $lreg << $scale]\" %}\n+  interface(MEMORY_INTER) %{\n+    base($reg);\n+    index($lreg);\n+    scale($scale);\n+    disp($off);\n+  %}\n+%}\n+\n+\/\/ Indirect Memory Times Plus Positive Index Register Plus Offset Operand\n+operand indPosIndexOffsetNarrow(rRegN reg, immL32 off, rRegI idx)\n+%{\n+  constraint(ALLOC_IN_RC(ptr_reg));\n+  predicate(CompressedOops::shift() == 0 && n->in(2)->in(3)->as_Type()->type()->is_long()->_lo >= 0);\n+  match(AddP (AddP (DecodeN reg) (ConvI2L idx)) off);\n+\n+  op_cost(10);\n+  format %{\"[$reg + $off + $idx]\" %}\n+  interface(MEMORY_INTER) %{\n+    base($reg);\n+    index($idx);\n+    scale(0x0);\n+    disp($off);\n+  %}\n+%}\n+\n+\/\/ Indirect Memory Times Scale Plus Positive Index Register Plus Offset Operand\n+operand indPosIndexScaleOffsetNarrow(rRegN reg, immL32 off, rRegI idx, immI2 scale)\n+%{\n+  constraint(ALLOC_IN_RC(ptr_reg));\n+  predicate(CompressedOops::shift() == 0 && n->in(2)->in(3)->in(1)->as_Type()->type()->is_long()->_lo >= 0);\n+  match(AddP (AddP (DecodeN reg) (LShiftL (ConvI2L idx) scale)) off);\n+\n+  op_cost(10);\n+  format %{\"[$reg + $off + $idx << $scale]\" %}\n+  interface(MEMORY_INTER) %{\n+    base($reg);\n+    index($idx);\n+    scale($scale);\n+    disp($off);\n+  %}\n+%}\n+\n+\/\/----------Special Memory Operands--------------------------------------------\n+\/\/ Stack Slot Operand - This operand is used for loading and storing temporary\n+\/\/                      values on the stack where a match requires a value to\n+\/\/                      flow through memory.\n+operand stackSlotP(sRegP reg)\n+%{\n+  constraint(ALLOC_IN_RC(stack_slots));\n+  \/\/ No match rule because this operand is only generated in matching\n+\n+  format %{ \"[$reg]\" %}\n+  interface(MEMORY_INTER) %{\n+    base(0x4);   \/\/ RSP\n+    index(0x4);  \/\/ No Index\n+    scale(0x0);  \/\/ No Scale\n+    disp($reg);  \/\/ Stack Offset\n+  %}\n+%}\n+\n+operand stackSlotI(sRegI reg)\n+%{\n+  constraint(ALLOC_IN_RC(stack_slots));\n+  \/\/ No match rule because this operand is only generated in matching\n+\n+  format %{ \"[$reg]\" %}\n+  interface(MEMORY_INTER) %{\n+    base(0x4);   \/\/ RSP\n+    index(0x4);  \/\/ No Index\n+    scale(0x0);  \/\/ No Scale\n+    disp($reg);  \/\/ Stack Offset\n+  %}\n+%}\n+\n+operand stackSlotF(sRegF reg)\n+%{\n+  constraint(ALLOC_IN_RC(stack_slots));\n+  \/\/ No match rule because this operand is only generated in matching\n+\n+  format %{ \"[$reg]\" %}\n+  interface(MEMORY_INTER) %{\n+    base(0x4);   \/\/ RSP\n+    index(0x4);  \/\/ No Index\n+    scale(0x0);  \/\/ No Scale\n+    disp($reg);  \/\/ Stack Offset\n+  %}\n+%}\n+\n+operand stackSlotD(sRegD reg)\n+%{\n+  constraint(ALLOC_IN_RC(stack_slots));\n+  \/\/ No match rule because this operand is only generated in matching\n+\n+  format %{ \"[$reg]\" %}\n+  interface(MEMORY_INTER) %{\n+    base(0x4);   \/\/ RSP\n+    index(0x4);  \/\/ No Index\n+    scale(0x0);  \/\/ No Scale\n+    disp($reg);  \/\/ Stack Offset\n+  %}\n+%}\n+operand stackSlotL(sRegL reg)\n+%{\n+  constraint(ALLOC_IN_RC(stack_slots));\n+  \/\/ No match rule because this operand is only generated in matching\n+\n+  format %{ \"[$reg]\" %}\n+  interface(MEMORY_INTER) %{\n+    base(0x4);   \/\/ RSP\n+    index(0x4);  \/\/ No Index\n+    scale(0x0);  \/\/ No Scale\n+    disp($reg);  \/\/ Stack Offset\n+  %}\n+%}\n+\n+\/\/----------Conditional Branch Operands----------------------------------------\n+\/\/ Comparison Op  - This is the operation of the comparison, and is limited to\n+\/\/                  the following set of codes:\n+\/\/                  L (<), LE (<=), G (>), GE (>=), E (==), NE (!=)\n+\/\/\n+\/\/ Other attributes of the comparison, such as unsignedness, are specified\n+\/\/ by the comparison instruction that sets a condition code flags register.\n+\/\/ That result is represented by a flags operand whose subtype is appropriate\n+\/\/ to the unsignedness (etc.) of the comparison.\n+\/\/\n+\/\/ Later, the instruction which matches both the Comparison Op (a Bool) and\n+\/\/ the flags (produced by the Cmp) specifies the coding of the comparison op\n+\/\/ by matching a specific subtype of Bool operand below, such as cmpOpU.\n+\n+\/\/ Comparison Code\n+operand cmpOp()\n+%{\n+  match(Bool);\n+\n+  format %{ \"\" %}\n+  interface(COND_INTER) %{\n+    equal(0x4, \"e\");\n+    not_equal(0x5, \"ne\");\n+    less(0xC, \"l\");\n+    greater_equal(0xD, \"ge\");\n+    less_equal(0xE, \"le\");\n+    greater(0xF, \"g\");\n+    overflow(0x0, \"o\");\n+    no_overflow(0x1, \"no\");\n+  %}\n+%}\n+\n+\/\/ Comparison Code, unsigned compare.  Used by FP also, with\n+\/\/ C2 (unordered) turned into GT or LT already.  The other bits\n+\/\/ C0 and C3 are turned into Carry & Zero flags.\n+operand cmpOpU()\n+%{\n+  match(Bool);\n+\n+  format %{ \"\" %}\n+  interface(COND_INTER) %{\n+    equal(0x4, \"e\");\n+    not_equal(0x5, \"ne\");\n+    less(0x2, \"b\");\n+    greater_equal(0x3, \"ae\");\n+    less_equal(0x6, \"be\");\n+    greater(0x7, \"a\");\n+    overflow(0x0, \"o\");\n+    no_overflow(0x1, \"no\");\n+  %}\n+%}\n+\n+\n+\/\/ Floating comparisons that don't require any fixup for the unordered case,\n+\/\/ If both inputs of the comparison are the same, ZF is always set so we\n+\/\/ don't need to use cmpOpUCF2 for eq\/ne\n+operand cmpOpUCF() %{\n+  match(Bool);\n+  predicate(n->as_Bool()->_test._test == BoolTest::lt ||\n+            n->as_Bool()->_test._test == BoolTest::ge ||\n+            n->as_Bool()->_test._test == BoolTest::le ||\n+            n->as_Bool()->_test._test == BoolTest::gt ||\n+            n->in(1)->in(1) == n->in(1)->in(2));\n+  format %{ \"\" %}\n+  interface(COND_INTER) %{\n+    equal(0xb, \"np\");\n+    not_equal(0xa, \"p\");\n+    less(0x2, \"b\");\n+    greater_equal(0x3, \"ae\");\n+    less_equal(0x6, \"be\");\n+    greater(0x7, \"a\");\n+    overflow(0x0, \"o\");\n+    no_overflow(0x1, \"no\");\n+  %}\n+%}\n+\n+\n+\/\/ Floating comparisons that can be fixed up with extra conditional jumps\n+operand cmpOpUCF2() %{\n+  match(Bool);\n+  predicate((n->as_Bool()->_test._test == BoolTest::ne ||\n+             n->as_Bool()->_test._test == BoolTest::eq) &&\n+            n->in(1)->in(1) != n->in(1)->in(2));\n+  format %{ \"\" %}\n+  interface(COND_INTER) %{\n+    equal(0x4, \"e\");\n+    not_equal(0x5, \"ne\");\n+    less(0x2, \"b\");\n+    greater_equal(0x3, \"ae\");\n+    less_equal(0x6, \"be\");\n+    greater(0x7, \"a\");\n+    overflow(0x0, \"o\");\n+    no_overflow(0x1, \"no\");\n+  %}\n+%}\n+\n+\/\/ Operands for bound floating pointer register arguments\n+operand rxmm0() %{\n+  constraint(ALLOC_IN_RC(xmm0_reg));\n+  match(VecX);\n+  format%{%}\n+  interface(REG_INTER);\n+%}\n+\n+\/\/ Vectors\n+\n+\/\/ Dummy generic vector class. Should be used for all vector operands.\n+\/\/ Replaced with vec[SDXYZ] during post-selection pass.\n+operand vec() %{\n+  constraint(ALLOC_IN_RC(dynamic));\n+  match(VecX);\n+  match(VecY);\n+  match(VecZ);\n+  match(VecS);\n+  match(VecD);\n+\n+  format %{ %}\n+  interface(REG_INTER);\n+%}\n+\n+\/\/ Dummy generic legacy vector class. Should be used for all legacy vector operands.\n+\/\/ Replaced with legVec[SDXYZ] during post-selection cleanup.\n+\/\/ Note: legacy register class is used to avoid extra (unneeded in 32-bit VM)\n+\/\/ runtime code generation via reg_class_dynamic.\n+operand legVec() %{\n+  constraint(ALLOC_IN_RC(dynamic));\n+  match(VecX);\n+  match(VecY);\n+  match(VecZ);\n+  match(VecS);\n+  match(VecD);\n+\n+  format %{ %}\n+  interface(REG_INTER);\n+%}\n+\n+\/\/ Replaces vec during post-selection cleanup. See above.\n+operand vecS() %{\n+  constraint(ALLOC_IN_RC(vectors_reg_vlbwdq));\n+  match(VecS);\n+\n+  format %{ %}\n+  interface(REG_INTER);\n+%}\n+\n+\/\/ Replaces legVec during post-selection cleanup. See above.\n+operand legVecS() %{\n+  constraint(ALLOC_IN_RC(vectors_reg_legacy));\n+  match(VecS);\n+\n+  format %{ %}\n+  interface(REG_INTER);\n+%}\n+\n+\/\/ Replaces vec during post-selection cleanup. See above.\n+operand vecD() %{\n+  constraint(ALLOC_IN_RC(vectord_reg_vlbwdq));\n+  match(VecD);\n+\n+  format %{ %}\n+  interface(REG_INTER);\n+%}\n+\n+\/\/ Replaces legVec during post-selection cleanup. See above.\n+operand legVecD() %{\n+  constraint(ALLOC_IN_RC(vectord_reg_legacy));\n+  match(VecD);\n+\n+  format %{ %}\n+  interface(REG_INTER);\n+%}\n+\n+\/\/ Replaces vec during post-selection cleanup. See above.\n+operand vecX() %{\n+  constraint(ALLOC_IN_RC(vectorx_reg_vlbwdq));\n+  match(VecX);\n+\n+  format %{ %}\n+  interface(REG_INTER);\n+%}\n+\n+\/\/ Replaces legVec during post-selection cleanup. See above.\n+operand legVecX() %{\n+  constraint(ALLOC_IN_RC(vectorx_reg_legacy));\n+  match(VecX);\n+\n+  format %{ %}\n+  interface(REG_INTER);\n+%}\n+\n+\/\/ Replaces vec during post-selection cleanup. See above.\n+operand vecY() %{\n+  constraint(ALLOC_IN_RC(vectory_reg_vlbwdq));\n+  match(VecY);\n+\n+  format %{ %}\n+  interface(REG_INTER);\n+%}\n+\n+\/\/ Replaces legVec during post-selection cleanup. See above.\n+operand legVecY() %{\n+  constraint(ALLOC_IN_RC(vectory_reg_legacy));\n+  match(VecY);\n+\n+  format %{ %}\n+  interface(REG_INTER);\n+%}\n+\n+\/\/ Replaces vec during post-selection cleanup. See above.\n+operand vecZ() %{\n+  constraint(ALLOC_IN_RC(vectorz_reg));\n+  match(VecZ);\n+\n+  format %{ %}\n+  interface(REG_INTER);\n+%}\n+\n+\/\/ Replaces legVec during post-selection cleanup. See above.\n+operand legVecZ() %{\n+  constraint(ALLOC_IN_RC(vectorz_reg_legacy));\n+  match(VecZ);\n+\n+  format %{ %}\n+  interface(REG_INTER);\n+%}\n+\n+\/\/----------OPERAND CLASSES----------------------------------------------------\n+\/\/ Operand Classes are groups of operands that are used as to simplify\n+\/\/ instruction definitions by not requiring the AD writer to specify separate\n+\/\/ instructions for every form of operand when the instruction accepts\n+\/\/ multiple operand types with the same basic encoding and format.  The classic\n+\/\/ case of this is memory operands.\n+\n+opclass memory(indirect, indOffset8, indOffset32, indIndexOffset, indIndex,\n+               indIndexScale, indPosIndexScale, indIndexScaleOffset, indPosIndexOffset, indPosIndexScaleOffset,\n+               indCompressedOopOffset,\n+               indirectNarrow, indOffset8Narrow, indOffset32Narrow,\n+               indIndexOffsetNarrow, indIndexNarrow, indIndexScaleNarrow,\n+               indIndexScaleOffsetNarrow, indPosIndexOffsetNarrow, indPosIndexScaleOffsetNarrow);\n+\n+\/\/----------PIPELINE-----------------------------------------------------------\n+\/\/ Rules which define the behavior of the target architectures pipeline.\n+pipeline %{\n+\n+\/\/----------ATTRIBUTES---------------------------------------------------------\n+attributes %{\n+  variable_size_instructions;        \/\/ Fixed size instructions\n+  max_instructions_per_bundle = 3;   \/\/ Up to 3 instructions per bundle\n+  instruction_unit_size = 1;         \/\/ An instruction is 1 bytes long\n+  instruction_fetch_unit_size = 16;  \/\/ The processor fetches one line\n+  instruction_fetch_units = 1;       \/\/ of 16 bytes\n+%}\n+\n+\/\/----------RESOURCES----------------------------------------------------------\n+\/\/ Resources are the functional units available to the machine\n+\n+\/\/ Generic P2\/P3 pipeline\n+\/\/ 3 decoders, only D0 handles big operands; a \"bundle\" is the limit of\n+\/\/ 3 instructions decoded per cycle.\n+\/\/ 2 load\/store ops per cycle, 1 branch, 1 FPU,\n+\/\/ 3 ALU op, only ALU0 handles mul instructions.\n+resources( D0, D1, D2, DECODE = D0 | D1 | D2,\n+           MS0, MS1, MS2, MEM = MS0 | MS1 | MS2,\n+           BR, FPU,\n+           ALU0, ALU1, ALU2, ALU = ALU0 | ALU1 | ALU2);\n+\n+\/\/----------PIPELINE DESCRIPTION-----------------------------------------------\n+\/\/ Pipeline Description specifies the stages in the machine's pipeline\n+\n+\/\/ Generic P2\/P3 pipeline\n+pipe_desc(S0, S1, S2, S3, S4, S5);\n+\n+\/\/----------PIPELINE CLASSES---------------------------------------------------\n+\/\/ Pipeline Classes describe the stages in which input and output are\n+\/\/ referenced by the hardware pipeline.\n+\n+\/\/ Naming convention: ialu or fpu\n+\/\/ Then: _reg\n+\/\/ Then: _reg if there is a 2nd register\n+\/\/ Then: _long if it's a pair of instructions implementing a long\n+\/\/ Then: _fat if it requires the big decoder\n+\/\/   Or: _mem if it requires the big decoder and a memory unit.\n+\n+\/\/ Integer ALU reg operation\n+pipe_class ialu_reg(rRegI dst)\n+%{\n+    single_instruction;\n+    dst    : S4(write);\n+    dst    : S3(read);\n+    DECODE : S0;        \/\/ any decoder\n+    ALU    : S3;        \/\/ any alu\n+%}\n+\n+\/\/ Long ALU reg operation\n+pipe_class ialu_reg_long(rRegL dst)\n+%{\n+    instruction_count(2);\n+    dst    : S4(write);\n+    dst    : S3(read);\n+    DECODE : S0(2);     \/\/ any 2 decoders\n+    ALU    : S3(2);     \/\/ both alus\n+%}\n+\n+\/\/ Integer ALU reg operation using big decoder\n+pipe_class ialu_reg_fat(rRegI dst)\n+%{\n+    single_instruction;\n+    dst    : S4(write);\n+    dst    : S3(read);\n+    D0     : S0;        \/\/ big decoder only\n+    ALU    : S3;        \/\/ any alu\n+%}\n+\n+\/\/ Integer ALU reg-reg operation\n+pipe_class ialu_reg_reg(rRegI dst, rRegI src)\n+%{\n+    single_instruction;\n+    dst    : S4(write);\n+    src    : S3(read);\n+    DECODE : S0;        \/\/ any decoder\n+    ALU    : S3;        \/\/ any alu\n+%}\n+\n+\/\/ Integer ALU reg-reg operation\n+pipe_class ialu_reg_reg_fat(rRegI dst, memory src)\n+%{\n+    single_instruction;\n+    dst    : S4(write);\n+    src    : S3(read);\n+    D0     : S0;        \/\/ big decoder only\n+    ALU    : S3;        \/\/ any alu\n+%}\n+\n+\/\/ Integer ALU reg-mem operation\n+pipe_class ialu_reg_mem(rRegI dst, memory mem)\n+%{\n+    single_instruction;\n+    dst    : S5(write);\n+    mem    : S3(read);\n+    D0     : S0;        \/\/ big decoder only\n+    ALU    : S4;        \/\/ any alu\n+    MEM    : S3;        \/\/ any mem\n+%}\n+\n+\/\/ Integer mem operation (prefetch)\n+pipe_class ialu_mem(memory mem)\n+%{\n+    single_instruction;\n+    mem    : S3(read);\n+    D0     : S0;        \/\/ big decoder only\n+    MEM    : S3;        \/\/ any mem\n+%}\n+\n+\/\/ Integer Store to Memory\n+pipe_class ialu_mem_reg(memory mem, rRegI src)\n+%{\n+    single_instruction;\n+    mem    : S3(read);\n+    src    : S5(read);\n+    D0     : S0;        \/\/ big decoder only\n+    ALU    : S4;        \/\/ any alu\n+    MEM    : S3;\n+%}\n+\n+\/\/ \/\/ Long Store to Memory\n+\/\/ pipe_class ialu_mem_long_reg(memory mem, rRegL src)\n+\/\/ %{\n+\/\/     instruction_count(2);\n+\/\/     mem    : S3(read);\n+\/\/     src    : S5(read);\n+\/\/     D0     : S0(2);          \/\/ big decoder only; twice\n+\/\/     ALU    : S4(2);     \/\/ any 2 alus\n+\/\/     MEM    : S3(2);  \/\/ Both mems\n+\/\/ %}\n+\n+\/\/ Integer Store to Memory\n+pipe_class ialu_mem_imm(memory mem)\n+%{\n+    single_instruction;\n+    mem    : S3(read);\n+    D0     : S0;        \/\/ big decoder only\n+    ALU    : S4;        \/\/ any alu\n+    MEM    : S3;\n+%}\n+\n+\/\/ Integer ALU0 reg-reg operation\n+pipe_class ialu_reg_reg_alu0(rRegI dst, rRegI src)\n+%{\n+    single_instruction;\n+    dst    : S4(write);\n+    src    : S3(read);\n+    D0     : S0;        \/\/ Big decoder only\n+    ALU0   : S3;        \/\/ only alu0\n+%}\n+\n+\/\/ Integer ALU0 reg-mem operation\n+pipe_class ialu_reg_mem_alu0(rRegI dst, memory mem)\n+%{\n+    single_instruction;\n+    dst    : S5(write);\n+    mem    : S3(read);\n+    D0     : S0;        \/\/ big decoder only\n+    ALU0   : S4;        \/\/ ALU0 only\n+    MEM    : S3;        \/\/ any mem\n+%}\n+\n+\/\/ Integer ALU reg-reg operation\n+pipe_class ialu_cr_reg_reg(rFlagsReg cr, rRegI src1, rRegI src2)\n+%{\n+    single_instruction;\n+    cr     : S4(write);\n+    src1   : S3(read);\n+    src2   : S3(read);\n+    DECODE : S0;        \/\/ any decoder\n+    ALU    : S3;        \/\/ any alu\n+%}\n+\n+\/\/ Integer ALU reg-imm operation\n+pipe_class ialu_cr_reg_imm(rFlagsReg cr, rRegI src1)\n+%{\n+    single_instruction;\n+    cr     : S4(write);\n+    src1   : S3(read);\n+    DECODE : S0;        \/\/ any decoder\n+    ALU    : S3;        \/\/ any alu\n+%}\n+\n+\/\/ Integer ALU reg-mem operation\n+pipe_class ialu_cr_reg_mem(rFlagsReg cr, rRegI src1, memory src2)\n+%{\n+    single_instruction;\n+    cr     : S4(write);\n+    src1   : S3(read);\n+    src2   : S3(read);\n+    D0     : S0;        \/\/ big decoder only\n+    ALU    : S4;        \/\/ any alu\n+    MEM    : S3;\n+%}\n+\n+\/\/ Conditional move reg-reg\n+pipe_class pipe_cmplt( rRegI p, rRegI q, rRegI y)\n+%{\n+    instruction_count(4);\n+    y      : S4(read);\n+    q      : S3(read);\n+    p      : S3(read);\n+    DECODE : S0(4);     \/\/ any decoder\n+%}\n+\n+\/\/ Conditional move reg-reg\n+pipe_class pipe_cmov_reg( rRegI dst, rRegI src, rFlagsReg cr)\n+%{\n+    single_instruction;\n+    dst    : S4(write);\n+    src    : S3(read);\n+    cr     : S3(read);\n+    DECODE : S0;        \/\/ any decoder\n+%}\n+\n+\/\/ Conditional move reg-mem\n+pipe_class pipe_cmov_mem( rFlagsReg cr, rRegI dst, memory src)\n+%{\n+    single_instruction;\n+    dst    : S4(write);\n+    src    : S3(read);\n+    cr     : S3(read);\n+    DECODE : S0;        \/\/ any decoder\n+    MEM    : S3;\n+%}\n+\n+\/\/ Conditional move reg-reg long\n+pipe_class pipe_cmov_reg_long( rFlagsReg cr, rRegL dst, rRegL src)\n+%{\n+    single_instruction;\n+    dst    : S4(write);\n+    src    : S3(read);\n+    cr     : S3(read);\n+    DECODE : S0(2);     \/\/ any 2 decoders\n+%}\n+\n+\/\/ Float reg-reg operation\n+pipe_class fpu_reg(regD dst)\n+%{\n+    instruction_count(2);\n+    dst    : S3(read);\n+    DECODE : S0(2);     \/\/ any 2 decoders\n+    FPU    : S3;\n+%}\n+\n+\/\/ Float reg-reg operation\n+pipe_class fpu_reg_reg(regD dst, regD src)\n+%{\n+    instruction_count(2);\n+    dst    : S4(write);\n+    src    : S3(read);\n+    DECODE : S0(2);     \/\/ any 2 decoders\n+    FPU    : S3;\n+%}\n+\n+\/\/ Float reg-reg operation\n+pipe_class fpu_reg_reg_reg(regD dst, regD src1, regD src2)\n+%{\n+    instruction_count(3);\n+    dst    : S4(write);\n+    src1   : S3(read);\n+    src2   : S3(read);\n+    DECODE : S0(3);     \/\/ any 3 decoders\n+    FPU    : S3(2);\n+%}\n+\n+\/\/ Float reg-reg operation\n+pipe_class fpu_reg_reg_reg_reg(regD dst, regD src1, regD src2, regD src3)\n+%{\n+    instruction_count(4);\n+    dst    : S4(write);\n+    src1   : S3(read);\n+    src2   : S3(read);\n+    src3   : S3(read);\n+    DECODE : S0(4);     \/\/ any 3 decoders\n+    FPU    : S3(2);\n+%}\n+\n+\/\/ Float reg-reg operation\n+pipe_class fpu_reg_mem_reg_reg(regD dst, memory src1, regD src2, regD src3)\n+%{\n+    instruction_count(4);\n+    dst    : S4(write);\n+    src1   : S3(read);\n+    src2   : S3(read);\n+    src3   : S3(read);\n+    DECODE : S1(3);     \/\/ any 3 decoders\n+    D0     : S0;        \/\/ Big decoder only\n+    FPU    : S3(2);\n+    MEM    : S3;\n+%}\n+\n+\/\/ Float reg-mem operation\n+pipe_class fpu_reg_mem(regD dst, memory mem)\n+%{\n+    instruction_count(2);\n+    dst    : S5(write);\n+    mem    : S3(read);\n+    D0     : S0;        \/\/ big decoder only\n+    DECODE : S1;        \/\/ any decoder for FPU POP\n+    FPU    : S4;\n+    MEM    : S3;        \/\/ any mem\n+%}\n+\n+\/\/ Float reg-mem operation\n+pipe_class fpu_reg_reg_mem(regD dst, regD src1, memory mem)\n+%{\n+    instruction_count(3);\n+    dst    : S5(write);\n+    src1   : S3(read);\n+    mem    : S3(read);\n+    D0     : S0;        \/\/ big decoder only\n+    DECODE : S1(2);     \/\/ any decoder for FPU POP\n+    FPU    : S4;\n+    MEM    : S3;        \/\/ any mem\n+%}\n+\n+\/\/ Float mem-reg operation\n+pipe_class fpu_mem_reg(memory mem, regD src)\n+%{\n+    instruction_count(2);\n+    src    : S5(read);\n+    mem    : S3(read);\n+    DECODE : S0;        \/\/ any decoder for FPU PUSH\n+    D0     : S1;        \/\/ big decoder only\n+    FPU    : S4;\n+    MEM    : S3;        \/\/ any mem\n+%}\n+\n+pipe_class fpu_mem_reg_reg(memory mem, regD src1, regD src2)\n+%{\n+    instruction_count(3);\n+    src1   : S3(read);\n+    src2   : S3(read);\n+    mem    : S3(read);\n+    DECODE : S0(2);     \/\/ any decoder for FPU PUSH\n+    D0     : S1;        \/\/ big decoder only\n+    FPU    : S4;\n+    MEM    : S3;        \/\/ any mem\n+%}\n+\n+pipe_class fpu_mem_reg_mem(memory mem, regD src1, memory src2)\n+%{\n+    instruction_count(3);\n+    src1   : S3(read);\n+    src2   : S3(read);\n+    mem    : S4(read);\n+    DECODE : S0;        \/\/ any decoder for FPU PUSH\n+    D0     : S0(2);     \/\/ big decoder only\n+    FPU    : S4;\n+    MEM    : S3(2);     \/\/ any mem\n+%}\n+\n+pipe_class fpu_mem_mem(memory dst, memory src1)\n+%{\n+    instruction_count(2);\n+    src1   : S3(read);\n+    dst    : S4(read);\n+    D0     : S0(2);     \/\/ big decoder only\n+    MEM    : S3(2);     \/\/ any mem\n+%}\n+\n+pipe_class fpu_mem_mem_mem(memory dst, memory src1, memory src2)\n+%{\n+    instruction_count(3);\n+    src1   : S3(read);\n+    src2   : S3(read);\n+    dst    : S4(read);\n+    D0     : S0(3);     \/\/ big decoder only\n+    FPU    : S4;\n+    MEM    : S3(3);     \/\/ any mem\n+%}\n+\n+pipe_class fpu_mem_reg_con(memory mem, regD src1)\n+%{\n+    instruction_count(3);\n+    src1   : S4(read);\n+    mem    : S4(read);\n+    DECODE : S0;        \/\/ any decoder for FPU PUSH\n+    D0     : S0(2);     \/\/ big decoder only\n+    FPU    : S4;\n+    MEM    : S3(2);     \/\/ any mem\n+%}\n+\n+\/\/ Float load constant\n+pipe_class fpu_reg_con(regD dst)\n+%{\n+    instruction_count(2);\n+    dst    : S5(write);\n+    D0     : S0;        \/\/ big decoder only for the load\n+    DECODE : S1;        \/\/ any decoder for FPU POP\n+    FPU    : S4;\n+    MEM    : S3;        \/\/ any mem\n+%}\n+\n+\/\/ Float load constant\n+pipe_class fpu_reg_reg_con(regD dst, regD src)\n+%{\n+    instruction_count(3);\n+    dst    : S5(write);\n+    src    : S3(read);\n+    D0     : S0;        \/\/ big decoder only for the load\n+    DECODE : S1(2);     \/\/ any decoder for FPU POP\n+    FPU    : S4;\n+    MEM    : S3;        \/\/ any mem\n+%}\n+\n+\/\/ UnConditional branch\n+pipe_class pipe_jmp(label labl)\n+%{\n+    single_instruction;\n+    BR   : S3;\n+%}\n+\n+\/\/ Conditional branch\n+pipe_class pipe_jcc(cmpOp cmp, rFlagsReg cr, label labl)\n+%{\n+    single_instruction;\n+    cr    : S1(read);\n+    BR    : S3;\n+%}\n+\n+\/\/ Allocation idiom\n+pipe_class pipe_cmpxchg(rRegP dst, rRegP heap_ptr)\n+%{\n+    instruction_count(1); force_serialization;\n+    fixed_latency(6);\n+    heap_ptr : S3(read);\n+    DECODE   : S0(3);\n+    D0       : S2;\n+    MEM      : S3;\n+    ALU      : S3(2);\n+    dst      : S5(write);\n+    BR       : S5;\n+%}\n+\n+\/\/ Generic big\/slow expanded idiom\n+pipe_class pipe_slow()\n+%{\n+    instruction_count(10); multiple_bundles; force_serialization;\n+    fixed_latency(100);\n+    D0  : S0(2);\n+    MEM : S3(2);\n+%}\n+\n+\/\/ The real do-nothing guy\n+pipe_class empty()\n+%{\n+    instruction_count(0);\n+%}\n+\n+\/\/ Define the class for the Nop node\n+define\n+%{\n+   MachNop = empty;\n+%}\n+\n+%}\n+\n+\/\/----------INSTRUCTIONS-------------------------------------------------------\n+\/\/\n+\/\/ match      -- States which machine-independent subtree may be replaced\n+\/\/               by this instruction.\n+\/\/ ins_cost   -- The estimated cost of this instruction is used by instruction\n+\/\/               selection to identify a minimum cost tree of machine\n+\/\/               instructions that matches a tree of machine-independent\n+\/\/               instructions.\n+\/\/ format     -- A string providing the disassembly for this instruction.\n+\/\/               The value of an instruction's operand may be inserted\n+\/\/               by referring to it with a '$' prefix.\n+\/\/ opcode     -- Three instruction opcodes may be provided.  These are referred\n+\/\/               to within an encode class as $primary, $secondary, and $tertiary\n+\/\/               rrspectively.  The primary opcode is commonly used to\n+\/\/               indicate the type of machine instruction, while secondary\n+\/\/               and tertiary are often used for prefix options or addressing\n+\/\/               modes.\n+\/\/ ins_encode -- A list of encode classes with parameters. The encode class\n+\/\/               name must have been defined in an 'enc_class' specification\n+\/\/               in the encode section of the architecture description.\n+\n+\/\/ ============================================================================\n+\n+instruct ShouldNotReachHere() %{\n+  match(Halt);\n+  format %{ \"stop\\t# ShouldNotReachHere\" %}\n+  ins_encode %{\n+    if (is_reachable()) {\n+      const char* str = __ code_string(_halt_reason);\n+      __ stop(str);\n+    }\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ ============================================================================\n+\n+\/\/ Dummy reg-to-reg vector moves. Removed during post-selection cleanup.\n+\/\/ Load Float\n+instruct MoveF2VL(vlRegF dst, regF src) %{\n+  match(Set dst src);\n+  format %{ \"movss $dst,$src\\t! load float (4 bytes)\" %}\n+  ins_encode %{\n+    ShouldNotReachHere();\n+  %}\n+  ins_pipe( fpu_reg_reg );\n+%}\n+\n+\/\/ Load Float\n+instruct MoveF2LEG(legRegF dst, regF src) %{\n+  match(Set dst src);\n+  format %{ \"movss $dst,$src\\t# if src != dst load float (4 bytes)\" %}\n+  ins_encode %{\n+    ShouldNotReachHere();\n+  %}\n+  ins_pipe( fpu_reg_reg );\n+%}\n+\n+\/\/ Load Float\n+instruct MoveVL2F(regF dst, vlRegF src) %{\n+  match(Set dst src);\n+  format %{ \"movss $dst,$src\\t! load float (4 bytes)\" %}\n+  ins_encode %{\n+    ShouldNotReachHere();\n+  %}\n+  ins_pipe( fpu_reg_reg );\n+%}\n+\n+\/\/ Load Float\n+instruct MoveLEG2F(regF dst, legRegF src) %{\n+  match(Set dst src);\n+  format %{ \"movss $dst,$src\\t# if src != dst load float (4 bytes)\" %}\n+  ins_encode %{\n+    ShouldNotReachHere();\n+  %}\n+  ins_pipe( fpu_reg_reg );\n+%}\n+\n+\/\/ Load Double\n+instruct MoveD2VL(vlRegD dst, regD src) %{\n+  match(Set dst src);\n+  format %{ \"movsd $dst,$src\\t! load double (8 bytes)\" %}\n+  ins_encode %{\n+    ShouldNotReachHere();\n+  %}\n+  ins_pipe( fpu_reg_reg );\n+%}\n+\n+\/\/ Load Double\n+instruct MoveD2LEG(legRegD dst, regD src) %{\n+  match(Set dst src);\n+  format %{ \"movsd $dst,$src\\t# if src != dst load double (8 bytes)\" %}\n+  ins_encode %{\n+    ShouldNotReachHere();\n+  %}\n+  ins_pipe( fpu_reg_reg );\n+%}\n+\n+\/\/ Load Double\n+instruct MoveVL2D(regD dst, vlRegD src) %{\n+  match(Set dst src);\n+  format %{ \"movsd $dst,$src\\t! load double (8 bytes)\" %}\n+  ins_encode %{\n+    ShouldNotReachHere();\n+  %}\n+  ins_pipe( fpu_reg_reg );\n+%}\n+\n+\/\/ Load Double\n+instruct MoveLEG2D(regD dst, legRegD src) %{\n+  match(Set dst src);\n+  format %{ \"movsd $dst,$src\\t# if src != dst load double (8 bytes)\" %}\n+  ins_encode %{\n+    ShouldNotReachHere();\n+  %}\n+  ins_pipe( fpu_reg_reg );\n+%}\n+\n+\/\/----------Load\/Store\/Move Instructions---------------------------------------\n+\/\/----------Load Instructions--------------------------------------------------\n+\n+\/\/ Load Byte (8 bit signed)\n+instruct loadB(rRegI dst, memory mem)\n+%{\n+  match(Set dst (LoadB mem));\n+\n+  ins_cost(125);\n+  format %{ \"movsbl  $dst, $mem\\t# byte\" %}\n+\n+  ins_encode %{\n+    __ movsbl($dst$$Register, $mem$$Address);\n+  %}\n+\n+  ins_pipe(ialu_reg_mem);\n+%}\n+\n+\/\/ Load Byte (8 bit signed) into Long Register\n+instruct loadB2L(rRegL dst, memory mem)\n+%{\n+  match(Set dst (ConvI2L (LoadB mem)));\n+\n+  ins_cost(125);\n+  format %{ \"movsbq  $dst, $mem\\t# byte -> long\" %}\n+\n+  ins_encode %{\n+    __ movsbq($dst$$Register, $mem$$Address);\n+  %}\n+\n+  ins_pipe(ialu_reg_mem);\n+%}\n+\n+\/\/ Load Unsigned Byte (8 bit UNsigned)\n+instruct loadUB(rRegI dst, memory mem)\n+%{\n+  match(Set dst (LoadUB mem));\n+\n+  ins_cost(125);\n+  format %{ \"movzbl  $dst, $mem\\t# ubyte\" %}\n+\n+  ins_encode %{\n+    __ movzbl($dst$$Register, $mem$$Address);\n+  %}\n+\n+  ins_pipe(ialu_reg_mem);\n+%}\n+\n+\/\/ Load Unsigned Byte (8 bit UNsigned) into Long Register\n+instruct loadUB2L(rRegL dst, memory mem)\n+%{\n+  match(Set dst (ConvI2L (LoadUB mem)));\n+\n+  ins_cost(125);\n+  format %{ \"movzbq  $dst, $mem\\t# ubyte -> long\" %}\n+\n+  ins_encode %{\n+    __ movzbq($dst$$Register, $mem$$Address);\n+  %}\n+\n+  ins_pipe(ialu_reg_mem);\n+%}\n+\n+\/\/ Load Unsigned Byte (8 bit UNsigned) with 32-bit mask into Long Register\n+instruct loadUB2L_immI(rRegL dst, memory mem, immI mask, rFlagsReg cr) %{\n+  match(Set dst (ConvI2L (AndI (LoadUB mem) mask)));\n+  effect(KILL cr);\n+\n+  format %{ \"movzbq  $dst, $mem\\t# ubyte & 32-bit mask -> long\\n\\t\"\n+            \"andl    $dst, right_n_bits($mask, 8)\" %}\n+  ins_encode %{\n+    Register Rdst = $dst$$Register;\n+    __ movzbq(Rdst, $mem$$Address);\n+    __ andl(Rdst, $mask$$constant & right_n_bits(8));\n+  %}\n+  ins_pipe(ialu_reg_mem);\n+%}\n+\n+\/\/ Load Short (16 bit signed)\n+instruct loadS(rRegI dst, memory mem)\n+%{\n+  match(Set dst (LoadS mem));\n+\n+  ins_cost(125);\n+  format %{ \"movswl $dst, $mem\\t# short\" %}\n+\n+  ins_encode %{\n+    __ movswl($dst$$Register, $mem$$Address);\n+  %}\n+\n+  ins_pipe(ialu_reg_mem);\n+%}\n+\n+\/\/ Load Short (16 bit signed) to Byte (8 bit signed)\n+instruct loadS2B(rRegI dst, memory mem, immI_24 twentyfour) %{\n+  match(Set dst (RShiftI (LShiftI (LoadS mem) twentyfour) twentyfour));\n+\n+  ins_cost(125);\n+  format %{ \"movsbl $dst, $mem\\t# short -> byte\" %}\n+  ins_encode %{\n+    __ movsbl($dst$$Register, $mem$$Address);\n+  %}\n+  ins_pipe(ialu_reg_mem);\n+%}\n+\n+\/\/ Load Short (16 bit signed) into Long Register\n+instruct loadS2L(rRegL dst, memory mem)\n+%{\n+  match(Set dst (ConvI2L (LoadS mem)));\n+\n+  ins_cost(125);\n+  format %{ \"movswq $dst, $mem\\t# short -> long\" %}\n+\n+  ins_encode %{\n+    __ movswq($dst$$Register, $mem$$Address);\n+  %}\n+\n+  ins_pipe(ialu_reg_mem);\n+%}\n+\n+\/\/ Load Unsigned Short\/Char (16 bit UNsigned)\n+instruct loadUS(rRegI dst, memory mem)\n+%{\n+  match(Set dst (LoadUS mem));\n+\n+  ins_cost(125);\n+  format %{ \"movzwl  $dst, $mem\\t# ushort\/char\" %}\n+\n+  ins_encode %{\n+    __ movzwl($dst$$Register, $mem$$Address);\n+  %}\n+\n+  ins_pipe(ialu_reg_mem);\n+%}\n+\n+\/\/ Load Unsigned Short\/Char (16 bit UNsigned) to Byte (8 bit signed)\n+instruct loadUS2B(rRegI dst, memory mem, immI_24 twentyfour) %{\n+  match(Set dst (RShiftI (LShiftI (LoadUS mem) twentyfour) twentyfour));\n+\n+  ins_cost(125);\n+  format %{ \"movsbl $dst, $mem\\t# ushort -> byte\" %}\n+  ins_encode %{\n+    __ movsbl($dst$$Register, $mem$$Address);\n+  %}\n+  ins_pipe(ialu_reg_mem);\n+%}\n+\n+\/\/ Load Unsigned Short\/Char (16 bit UNsigned) into Long Register\n+instruct loadUS2L(rRegL dst, memory mem)\n+%{\n+  match(Set dst (ConvI2L (LoadUS mem)));\n+\n+  ins_cost(125);\n+  format %{ \"movzwq  $dst, $mem\\t# ushort\/char -> long\" %}\n+\n+  ins_encode %{\n+    __ movzwq($dst$$Register, $mem$$Address);\n+  %}\n+\n+  ins_pipe(ialu_reg_mem);\n+%}\n+\n+\/\/ Load Unsigned Short\/Char (16 bit UNsigned) with mask 0xFF into Long Register\n+instruct loadUS2L_immI_255(rRegL dst, memory mem, immI_255 mask) %{\n+  match(Set dst (ConvI2L (AndI (LoadUS mem) mask)));\n+\n+  format %{ \"movzbq  $dst, $mem\\t# ushort\/char & 0xFF -> long\" %}\n+  ins_encode %{\n+    __ movzbq($dst$$Register, $mem$$Address);\n+  %}\n+  ins_pipe(ialu_reg_mem);\n+%}\n+\n+\/\/ Load Unsigned Short\/Char (16 bit UNsigned) with 32-bit mask into Long Register\n+instruct loadUS2L_immI(rRegL dst, memory mem, immI mask, rFlagsReg cr) %{\n+  match(Set dst (ConvI2L (AndI (LoadUS mem) mask)));\n+  effect(KILL cr);\n+\n+  format %{ \"movzwq  $dst, $mem\\t# ushort\/char & 32-bit mask -> long\\n\\t\"\n+            \"andl    $dst, right_n_bits($mask, 16)\" %}\n+  ins_encode %{\n+    Register Rdst = $dst$$Register;\n+    __ movzwq(Rdst, $mem$$Address);\n+    __ andl(Rdst, $mask$$constant & right_n_bits(16));\n+  %}\n+  ins_pipe(ialu_reg_mem);\n+%}\n+\n+\/\/ Load Integer\n+instruct loadI(rRegI dst, memory mem)\n+%{\n+  match(Set dst (LoadI mem));\n+\n+  ins_cost(125);\n+  format %{ \"movl    $dst, $mem\\t# int\" %}\n+\n+  ins_encode %{\n+    __ movl($dst$$Register, $mem$$Address);\n+  %}\n+\n+  ins_pipe(ialu_reg_mem);\n+%}\n+\n+\/\/ Load Integer (32 bit signed) to Byte (8 bit signed)\n+instruct loadI2B(rRegI dst, memory mem, immI_24 twentyfour) %{\n+  match(Set dst (RShiftI (LShiftI (LoadI mem) twentyfour) twentyfour));\n+\n+  ins_cost(125);\n+  format %{ \"movsbl  $dst, $mem\\t# int -> byte\" %}\n+  ins_encode %{\n+    __ movsbl($dst$$Register, $mem$$Address);\n+  %}\n+  ins_pipe(ialu_reg_mem);\n+%}\n+\n+\/\/ Load Integer (32 bit signed) to Unsigned Byte (8 bit UNsigned)\n+instruct loadI2UB(rRegI dst, memory mem, immI_255 mask) %{\n+  match(Set dst (AndI (LoadI mem) mask));\n+\n+  ins_cost(125);\n+  format %{ \"movzbl  $dst, $mem\\t# int -> ubyte\" %}\n+  ins_encode %{\n+    __ movzbl($dst$$Register, $mem$$Address);\n+  %}\n+  ins_pipe(ialu_reg_mem);\n+%}\n+\n+\/\/ Load Integer (32 bit signed) to Short (16 bit signed)\n+instruct loadI2S(rRegI dst, memory mem, immI_16 sixteen) %{\n+  match(Set dst (RShiftI (LShiftI (LoadI mem) sixteen) sixteen));\n+\n+  ins_cost(125);\n+  format %{ \"movswl  $dst, $mem\\t# int -> short\" %}\n+  ins_encode %{\n+    __ movswl($dst$$Register, $mem$$Address);\n+  %}\n+  ins_pipe(ialu_reg_mem);\n+%}\n+\n+\/\/ Load Integer (32 bit signed) to Unsigned Short\/Char (16 bit UNsigned)\n+instruct loadI2US(rRegI dst, memory mem, immI_65535 mask) %{\n+  match(Set dst (AndI (LoadI mem) mask));\n+\n+  ins_cost(125);\n+  format %{ \"movzwl  $dst, $mem\\t# int -> ushort\/char\" %}\n+  ins_encode %{\n+    __ movzwl($dst$$Register, $mem$$Address);\n+  %}\n+  ins_pipe(ialu_reg_mem);\n+%}\n+\n+\/\/ Load Integer into Long Register\n+instruct loadI2L(rRegL dst, memory mem)\n+%{\n+  match(Set dst (ConvI2L (LoadI mem)));\n+\n+  ins_cost(125);\n+  format %{ \"movslq  $dst, $mem\\t# int -> long\" %}\n+\n+  ins_encode %{\n+    __ movslq($dst$$Register, $mem$$Address);\n+  %}\n+\n+  ins_pipe(ialu_reg_mem);\n+%}\n+\n+\/\/ Load Integer with mask 0xFF into Long Register\n+instruct loadI2L_immI_255(rRegL dst, memory mem, immI_255 mask) %{\n+  match(Set dst (ConvI2L (AndI (LoadI mem) mask)));\n+\n+  format %{ \"movzbq  $dst, $mem\\t# int & 0xFF -> long\" %}\n+  ins_encode %{\n+    __ movzbq($dst$$Register, $mem$$Address);\n+  %}\n+  ins_pipe(ialu_reg_mem);\n+%}\n+\n+\/\/ Load Integer with mask 0xFFFF into Long Register\n+instruct loadI2L_immI_65535(rRegL dst, memory mem, immI_65535 mask) %{\n+  match(Set dst (ConvI2L (AndI (LoadI mem) mask)));\n+\n+  format %{ \"movzwq  $dst, $mem\\t# int & 0xFFFF -> long\" %}\n+  ins_encode %{\n+    __ movzwq($dst$$Register, $mem$$Address);\n+  %}\n+  ins_pipe(ialu_reg_mem);\n+%}\n+\n+\/\/ Load Integer with a 31-bit mask into Long Register\n+instruct loadI2L_immU31(rRegL dst, memory mem, immU31 mask, rFlagsReg cr) %{\n+  match(Set dst (ConvI2L (AndI (LoadI mem) mask)));\n+  effect(KILL cr);\n+\n+  format %{ \"movl    $dst, $mem\\t# int & 31-bit mask -> long\\n\\t\"\n+            \"andl    $dst, $mask\" %}\n+  ins_encode %{\n+    Register Rdst = $dst$$Register;\n+    __ movl(Rdst, $mem$$Address);\n+    __ andl(Rdst, $mask$$constant);\n+  %}\n+  ins_pipe(ialu_reg_mem);\n+%}\n+\n+\/\/ Load Unsigned Integer into Long Register\n+instruct loadUI2L(rRegL dst, memory mem, immL_32bits mask)\n+%{\n+  match(Set dst (AndL (ConvI2L (LoadI mem)) mask));\n+\n+  ins_cost(125);\n+  format %{ \"movl    $dst, $mem\\t# uint -> long\" %}\n+\n+  ins_encode %{\n+    __ movl($dst$$Register, $mem$$Address);\n+  %}\n+\n+  ins_pipe(ialu_reg_mem);\n+%}\n+\n+\/\/ Load Long\n+instruct loadL(rRegL dst, memory mem)\n+%{\n+  match(Set dst (LoadL mem));\n+\n+  ins_cost(125);\n+  format %{ \"movq    $dst, $mem\\t# long\" %}\n+\n+  ins_encode %{\n+    __ movq($dst$$Register, $mem$$Address);\n+  %}\n+\n+  ins_pipe(ialu_reg_mem); \/\/ XXX\n+%}\n+\n+\/\/ Load Range\n+instruct loadRange(rRegI dst, memory mem)\n+%{\n+  match(Set dst (LoadRange mem));\n+\n+  ins_cost(125); \/\/ XXX\n+  format %{ \"movl    $dst, $mem\\t# range\" %}\n+  ins_encode %{\n+    __ movl($dst$$Register, $mem$$Address);\n+  %}\n+  ins_pipe(ialu_reg_mem);\n+%}\n+\n+\/\/ Load Pointer\n+instruct loadP(rRegP dst, memory mem)\n+%{\n+  match(Set dst (LoadP mem));\n+  predicate(n->as_Load()->barrier_data() == 0);\n+\n+  ins_cost(125); \/\/ XXX\n+  format %{ \"movq    $dst, $mem\\t# ptr\" %}\n+  ins_encode %{\n+    __ movq($dst$$Register, $mem$$Address);\n+  %}\n+  ins_pipe(ialu_reg_mem); \/\/ XXX\n+%}\n+\n+\/\/ Load Compressed Pointer\n+instruct loadN(rRegN dst, memory mem)\n+%{\n+   predicate(n->as_Load()->barrier_data() == 0);\n+   match(Set dst (LoadN mem));\n+\n+   ins_cost(125); \/\/ XXX\n+   format %{ \"movl    $dst, $mem\\t# compressed ptr\" %}\n+   ins_encode %{\n+     __ movl($dst$$Register, $mem$$Address);\n+   %}\n+   ins_pipe(ialu_reg_mem); \/\/ XXX\n+%}\n+\n+\n+\/\/ Load Klass Pointer\n+instruct loadKlass(rRegP dst, memory mem)\n+%{\n+  match(Set dst (LoadKlass mem));\n+\n+  ins_cost(125); \/\/ XXX\n+  format %{ \"movq    $dst, $mem\\t# class\" %}\n+  ins_encode %{\n+    __ movq($dst$$Register, $mem$$Address);\n+  %}\n+  ins_pipe(ialu_reg_mem); \/\/ XXX\n+%}\n+\n+\/\/ Load narrow Klass Pointer\n+instruct loadNKlass(rRegN dst, memory mem)\n+%{\n+  predicate(!UseCompactObjectHeaders);\n+  match(Set dst (LoadNKlass mem));\n+\n+  ins_cost(125); \/\/ XXX\n+  format %{ \"movl    $dst, $mem\\t# compressed klass ptr\" %}\n+  ins_encode %{\n+    __ movl($dst$$Register, $mem$$Address);\n+  %}\n+  ins_pipe(ialu_reg_mem); \/\/ XXX\n+%}\n+\n+instruct loadNKlassCompactHeaders(rRegN dst, memory mem, rFlagsReg cr)\n+%{\n+  predicate(UseCompactObjectHeaders);\n+  match(Set dst (LoadNKlass mem));\n+  effect(KILL cr);\n+  ins_cost(125);\n+  format %{\n+    \"movl    $dst, $mem\\t# compressed klass ptr, shifted\\n\\t\"\n+    \"shrl    $dst, markWord::klass_shift_at_offset\"\n+  %}\n+  ins_encode %{\n+    if (UseAPX) {\n+      __ eshrl($dst$$Register, $mem$$Address, markWord::klass_shift_at_offset, false);\n+    }\n+    else {\n+      __ movl($dst$$Register, $mem$$Address);\n+      __ shrl($dst$$Register, markWord::klass_shift_at_offset);\n+    }\n+  %}\n+  ins_pipe(ialu_reg_mem);\n+%}\n+\n+\/\/ Load Float\n+instruct loadF(regF dst, memory mem)\n+%{\n+  match(Set dst (LoadF mem));\n+\n+  ins_cost(145); \/\/ XXX\n+  format %{ \"movss   $dst, $mem\\t# float\" %}\n+  ins_encode %{\n+    __ movflt($dst$$XMMRegister, $mem$$Address);\n+  %}\n+  ins_pipe(pipe_slow); \/\/ XXX\n+%}\n+\n+\/\/ Load Double\n+instruct loadD_partial(regD dst, memory mem)\n+%{\n+  predicate(!UseXmmLoadAndClearUpper);\n+  match(Set dst (LoadD mem));\n+\n+  ins_cost(145); \/\/ XXX\n+  format %{ \"movlpd  $dst, $mem\\t# double\" %}\n+  ins_encode %{\n+    __ movdbl($dst$$XMMRegister, $mem$$Address);\n+  %}\n+  ins_pipe(pipe_slow); \/\/ XXX\n+%}\n+\n+instruct loadD(regD dst, memory mem)\n+%{\n+  predicate(UseXmmLoadAndClearUpper);\n+  match(Set dst (LoadD mem));\n+\n+  ins_cost(145); \/\/ XXX\n+  format %{ \"movsd   $dst, $mem\\t# double\" %}\n+  ins_encode %{\n+    __ movdbl($dst$$XMMRegister, $mem$$Address);\n+  %}\n+  ins_pipe(pipe_slow); \/\/ XXX\n+%}\n+\n+\/\/ max = java.lang.Math.max(float a, float b)\n+instruct maxF_avx10_reg(regF dst, regF a, regF b) %{\n+  predicate(VM_Version::supports_avx10_2());\n+  match(Set dst (MaxF a b));\n+  format %{ \"maxF $dst, $a, $b\" %}\n+  ins_encode %{\n+    __ eminmaxss($dst$$XMMRegister, $a$$XMMRegister, $b$$XMMRegister, AVX10_MINMAX_MAX_COMPARE_SIGN);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+\/\/ max = java.lang.Math.max(float a, float b)\n+instruct maxF_reg(legRegF dst, legRegF a, legRegF b, legRegF tmp, legRegF atmp, legRegF btmp) %{\n+  predicate(!VM_Version::supports_avx10_2() && UseAVX > 0 && !VLoopReductions::is_reduction(n));\n+  match(Set dst (MaxF a b));\n+  effect(USE a, USE b, TEMP tmp, TEMP atmp, TEMP btmp);\n+  format %{ \"maxF $dst, $a, $b \\t! using $tmp, $atmp and $btmp as TEMP\" %}\n+  ins_encode %{\n+    __ vminmax_fp(Op_MaxV, T_FLOAT, $dst$$XMMRegister, $a$$XMMRegister, $b$$XMMRegister, $tmp$$XMMRegister, $atmp$$XMMRegister, $btmp$$XMMRegister, Assembler::AVX_128bit);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct maxF_reduction_reg(legRegF dst, legRegF a, legRegF b, legRegF xtmp, rRegI rtmp, rFlagsReg cr) %{\n+  predicate(!VM_Version::supports_avx10_2() && UseAVX > 0 && VLoopReductions::is_reduction(n));\n+  match(Set dst (MaxF a b));\n+  effect(USE a, USE b, TEMP xtmp, TEMP rtmp, KILL cr);\n+\n+  format %{ \"maxF_reduction $dst, $a, $b \\t!using $xtmp and $rtmp as TEMP\" %}\n+  ins_encode %{\n+    emit_fp_min_max(masm, $dst$$XMMRegister, $a$$XMMRegister, $b$$XMMRegister, $xtmp$$XMMRegister, $rtmp$$Register,\n+                    false \/*min*\/, true \/*single*\/);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+\/\/ max = java.lang.Math.max(double a, double b)\n+instruct maxD_avx10_reg(regD dst, regD a, regD b) %{\n+  predicate(VM_Version::supports_avx10_2());\n+  match(Set dst (MaxD a b));\n+  format %{ \"maxD $dst, $a, $b\" %}\n+  ins_encode %{\n+    __ eminmaxsd($dst$$XMMRegister, $a$$XMMRegister, $b$$XMMRegister, AVX10_MINMAX_MAX_COMPARE_SIGN);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+\/\/ max = java.lang.Math.max(double a, double b)\n+instruct maxD_reg(legRegD dst, legRegD a, legRegD b, legRegD tmp, legRegD atmp, legRegD btmp) %{\n+  predicate(!VM_Version::supports_avx10_2() && UseAVX > 0 && !VLoopReductions::is_reduction(n));\n+  match(Set dst (MaxD a b));\n+  effect(USE a, USE b, TEMP atmp, TEMP btmp, TEMP tmp);\n+  format %{ \"maxD $dst, $a, $b \\t! using $tmp, $atmp and $btmp as TEMP\" %}\n+  ins_encode %{\n+    __ vminmax_fp(Op_MaxV, T_DOUBLE, $dst$$XMMRegister, $a$$XMMRegister, $b$$XMMRegister, $tmp$$XMMRegister, $atmp$$XMMRegister, $btmp$$XMMRegister, Assembler::AVX_128bit);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct maxD_reduction_reg(legRegD dst, legRegD a, legRegD b, legRegD xtmp, rRegL rtmp, rFlagsReg cr) %{\n+  predicate(!VM_Version::supports_avx10_2() && UseAVX > 0 && VLoopReductions::is_reduction(n));\n+  match(Set dst (MaxD a b));\n+  effect(USE a, USE b, TEMP xtmp, TEMP rtmp, KILL cr);\n+\n+  format %{ \"maxD_reduction $dst, $a, $b \\t! using $xtmp and $rtmp as TEMP\" %}\n+  ins_encode %{\n+    emit_fp_min_max(masm, $dst$$XMMRegister, $a$$XMMRegister, $b$$XMMRegister, $xtmp$$XMMRegister, $rtmp$$Register,\n+                    false \/*min*\/, false \/*single*\/);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+\/\/ max = java.lang.Math.min(float a, float b)\n+instruct minF_avx10_reg(regF dst, regF a, regF b) %{\n+  predicate(VM_Version::supports_avx10_2());\n+  match(Set dst (MinF a b));\n+  format %{ \"minF $dst, $a, $b\" %}\n+  ins_encode %{\n+    __ eminmaxss($dst$$XMMRegister, $a$$XMMRegister, $b$$XMMRegister, AVX10_MINMAX_MIN_COMPARE_SIGN);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+\/\/ min = java.lang.Math.min(float a, float b)\n+instruct minF_reg(legRegF dst, legRegF a, legRegF b, legRegF tmp, legRegF atmp, legRegF btmp) %{\n+  predicate(!VM_Version::supports_avx10_2() && UseAVX > 0 && !VLoopReductions::is_reduction(n));\n+  match(Set dst (MinF a b));\n+  effect(USE a, USE b, TEMP tmp, TEMP atmp, TEMP btmp);\n+  format %{ \"minF $dst, $a, $b \\t! using $tmp, $atmp and $btmp as TEMP\" %}\n+  ins_encode %{\n+    __ vminmax_fp(Op_MinV, T_FLOAT, $dst$$XMMRegister, $a$$XMMRegister, $b$$XMMRegister, $tmp$$XMMRegister, $atmp$$XMMRegister, $btmp$$XMMRegister, Assembler::AVX_128bit);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct minF_reduction_reg(legRegF dst, legRegF a, legRegF b, legRegF xtmp, rRegI rtmp, rFlagsReg cr) %{\n+  predicate(!VM_Version::supports_avx10_2() && UseAVX > 0 && VLoopReductions::is_reduction(n));\n+  match(Set dst (MinF a b));\n+  effect(USE a, USE b, TEMP xtmp, TEMP rtmp, KILL cr);\n+\n+  format %{ \"minF_reduction $dst, $a, $b \\t! using $xtmp and $rtmp as TEMP\" %}\n+  ins_encode %{\n+    emit_fp_min_max(masm, $dst$$XMMRegister, $a$$XMMRegister, $b$$XMMRegister, $xtmp$$XMMRegister, $rtmp$$Register,\n+                    true \/*min*\/, true \/*single*\/);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+\/\/ max = java.lang.Math.min(double a, double b)\n+instruct minD_avx10_reg(regD dst, regD a, regD b) %{\n+  predicate(VM_Version::supports_avx10_2());\n+  match(Set dst (MinD a b));\n+  format %{ \"minD $dst, $a, $b\" %}\n+  ins_encode %{\n+    __ eminmaxsd($dst$$XMMRegister, $a$$XMMRegister, $b$$XMMRegister, AVX10_MINMAX_MIN_COMPARE_SIGN);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+\/\/ min = java.lang.Math.min(double a, double b)\n+instruct minD_reg(legRegD dst, legRegD a, legRegD b, legRegD tmp, legRegD atmp, legRegD btmp) %{\n+  predicate(!VM_Version::supports_avx10_2() && UseAVX > 0 && !VLoopReductions::is_reduction(n));\n+  match(Set dst (MinD a b));\n+  effect(USE a, USE b, TEMP tmp, TEMP atmp, TEMP btmp);\n+    format %{ \"minD $dst, $a, $b \\t! using $tmp, $atmp and $btmp as TEMP\" %}\n+  ins_encode %{\n+    __ vminmax_fp(Op_MinV, T_DOUBLE, $dst$$XMMRegister, $a$$XMMRegister, $b$$XMMRegister, $tmp$$XMMRegister, $atmp$$XMMRegister, $btmp$$XMMRegister, Assembler::AVX_128bit);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct minD_reduction_reg(legRegD dst, legRegD a, legRegD b, legRegD xtmp, rRegL rtmp, rFlagsReg cr) %{\n+  predicate(!VM_Version::supports_avx10_2() && UseAVX > 0 && VLoopReductions::is_reduction(n));\n+  match(Set dst (MinD a b));\n+  effect(USE a, USE b, TEMP xtmp, TEMP rtmp, KILL cr);\n+\n+  format %{ \"maxD_reduction $dst, $a, $b \\t! using $xtmp and $rtmp as TEMP\" %}\n+  ins_encode %{\n+    emit_fp_min_max(masm, $dst$$XMMRegister, $a$$XMMRegister, $b$$XMMRegister, $xtmp$$XMMRegister, $rtmp$$Register,\n+                    true \/*min*\/, false \/*single*\/);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+\/\/ Load Effective Address\n+instruct leaP8(rRegP dst, indOffset8 mem)\n+%{\n+  match(Set dst mem);\n+\n+  ins_cost(110); \/\/ XXX\n+  format %{ \"leaq    $dst, $mem\\t# ptr 8\" %}\n+  ins_encode %{\n+    __ leaq($dst$$Register, $mem$$Address);\n+  %}\n+  ins_pipe(ialu_reg_reg_fat);\n+%}\n+\n+instruct leaP32(rRegP dst, indOffset32 mem)\n+%{\n+  match(Set dst mem);\n+\n+  ins_cost(110);\n+  format %{ \"leaq    $dst, $mem\\t# ptr 32\" %}\n+  ins_encode %{\n+    __ leaq($dst$$Register, $mem$$Address);\n+  %}\n+  ins_pipe(ialu_reg_reg_fat);\n+%}\n+\n+instruct leaPIdxOff(rRegP dst, indIndexOffset mem)\n+%{\n+  match(Set dst mem);\n+\n+  ins_cost(110);\n+  format %{ \"leaq    $dst, $mem\\t# ptr idxoff\" %}\n+  ins_encode %{\n+    __ leaq($dst$$Register, $mem$$Address);\n+  %}\n+  ins_pipe(ialu_reg_reg_fat);\n+%}\n+\n+instruct leaPIdxScale(rRegP dst, indIndexScale mem)\n+%{\n+  match(Set dst mem);\n+\n+  ins_cost(110);\n+  format %{ \"leaq    $dst, $mem\\t# ptr idxscale\" %}\n+  ins_encode %{\n+    __ leaq($dst$$Register, $mem$$Address);\n+  %}\n+  ins_pipe(ialu_reg_reg_fat);\n+%}\n+\n+instruct leaPPosIdxScale(rRegP dst, indPosIndexScale mem)\n+%{\n+  match(Set dst mem);\n+\n+  ins_cost(110);\n+  format %{ \"leaq    $dst, $mem\\t# ptr idxscale\" %}\n+  ins_encode %{\n+    __ leaq($dst$$Register, $mem$$Address);\n+  %}\n+  ins_pipe(ialu_reg_reg_fat);\n+%}\n+\n+instruct leaPIdxScaleOff(rRegP dst, indIndexScaleOffset mem)\n+%{\n+  match(Set dst mem);\n+\n+  ins_cost(110);\n+  format %{ \"leaq    $dst, $mem\\t# ptr idxscaleoff\" %}\n+  ins_encode %{\n+    __ leaq($dst$$Register, $mem$$Address);\n+  %}\n+  ins_pipe(ialu_reg_reg_fat);\n+%}\n+\n+instruct leaPPosIdxOff(rRegP dst, indPosIndexOffset mem)\n+%{\n+  match(Set dst mem);\n+\n+  ins_cost(110);\n+  format %{ \"leaq    $dst, $mem\\t# ptr posidxoff\" %}\n+  ins_encode %{\n+    __ leaq($dst$$Register, $mem$$Address);\n+  %}\n+  ins_pipe(ialu_reg_reg_fat);\n+%}\n+\n+instruct leaPPosIdxScaleOff(rRegP dst, indPosIndexScaleOffset mem)\n+%{\n+  match(Set dst mem);\n+\n+  ins_cost(110);\n+  format %{ \"leaq    $dst, $mem\\t# ptr posidxscaleoff\" %}\n+  ins_encode %{\n+    __ leaq($dst$$Register, $mem$$Address);\n+  %}\n+  ins_pipe(ialu_reg_reg_fat);\n+%}\n+\n+\/\/ Load Effective Address which uses Narrow (32-bits) oop\n+instruct leaPCompressedOopOffset(rRegP dst, indCompressedOopOffset mem)\n+%{\n+  predicate(UseCompressedOops && (CompressedOops::shift() != 0));\n+  match(Set dst mem);\n+\n+  ins_cost(110);\n+  format %{ \"leaq    $dst, $mem\\t# ptr compressedoopoff32\" %}\n+  ins_encode %{\n+    __ leaq($dst$$Register, $mem$$Address);\n+  %}\n+  ins_pipe(ialu_reg_reg_fat);\n+%}\n+\n+instruct leaP8Narrow(rRegP dst, indOffset8Narrow mem)\n+%{\n+  predicate(CompressedOops::shift() == 0);\n+  match(Set dst mem);\n+\n+  ins_cost(110); \/\/ XXX\n+  format %{ \"leaq    $dst, $mem\\t# ptr off8narrow\" %}\n+  ins_encode %{\n+    __ leaq($dst$$Register, $mem$$Address);\n+  %}\n+  ins_pipe(ialu_reg_reg_fat);\n+%}\n+\n+instruct leaP32Narrow(rRegP dst, indOffset32Narrow mem)\n+%{\n+  predicate(CompressedOops::shift() == 0);\n+  match(Set dst mem);\n+\n+  ins_cost(110);\n+  format %{ \"leaq    $dst, $mem\\t# ptr off32narrow\" %}\n+  ins_encode %{\n+    __ leaq($dst$$Register, $mem$$Address);\n+  %}\n+  ins_pipe(ialu_reg_reg_fat);\n+%}\n+\n+instruct leaPIdxOffNarrow(rRegP dst, indIndexOffsetNarrow mem)\n+%{\n+  predicate(CompressedOops::shift() == 0);\n+  match(Set dst mem);\n+\n+  ins_cost(110);\n+  format %{ \"leaq    $dst, $mem\\t# ptr idxoffnarrow\" %}\n+  ins_encode %{\n+    __ leaq($dst$$Register, $mem$$Address);\n+  %}\n+  ins_pipe(ialu_reg_reg_fat);\n+%}\n+\n+instruct leaPIdxScaleNarrow(rRegP dst, indIndexScaleNarrow mem)\n+%{\n+  predicate(CompressedOops::shift() == 0);\n+  match(Set dst mem);\n+\n+  ins_cost(110);\n+  format %{ \"leaq    $dst, $mem\\t# ptr idxscalenarrow\" %}\n+  ins_encode %{\n+    __ leaq($dst$$Register, $mem$$Address);\n+  %}\n+  ins_pipe(ialu_reg_reg_fat);\n+%}\n+\n+instruct leaPIdxScaleOffNarrow(rRegP dst, indIndexScaleOffsetNarrow mem)\n+%{\n+  predicate(CompressedOops::shift() == 0);\n+  match(Set dst mem);\n+\n+  ins_cost(110);\n+  format %{ \"leaq    $dst, $mem\\t# ptr idxscaleoffnarrow\" %}\n+  ins_encode %{\n+    __ leaq($dst$$Register, $mem$$Address);\n+  %}\n+  ins_pipe(ialu_reg_reg_fat);\n+%}\n+\n+instruct leaPPosIdxOffNarrow(rRegP dst, indPosIndexOffsetNarrow mem)\n+%{\n+  predicate(CompressedOops::shift() == 0);\n+  match(Set dst mem);\n+\n+  ins_cost(110);\n+  format %{ \"leaq    $dst, $mem\\t# ptr posidxoffnarrow\" %}\n+  ins_encode %{\n+    __ leaq($dst$$Register, $mem$$Address);\n+  %}\n+  ins_pipe(ialu_reg_reg_fat);\n+%}\n+\n+instruct leaPPosIdxScaleOffNarrow(rRegP dst, indPosIndexScaleOffsetNarrow mem)\n+%{\n+  predicate(CompressedOops::shift() == 0);\n+  match(Set dst mem);\n+\n+  ins_cost(110);\n+  format %{ \"leaq    $dst, $mem\\t# ptr posidxscaleoffnarrow\" %}\n+  ins_encode %{\n+    __ leaq($dst$$Register, $mem$$Address);\n+  %}\n+  ins_pipe(ialu_reg_reg_fat);\n+%}\n+\n+instruct loadConI(rRegI dst, immI src)\n+%{\n+  match(Set dst src);\n+\n+  format %{ \"movl    $dst, $src\\t# int\" %}\n+  ins_encode %{\n+    __ movl($dst$$Register, $src$$constant);\n+  %}\n+  ins_pipe(ialu_reg_fat); \/\/ XXX\n+%}\n+\n+instruct loadConI0(rRegI dst, immI_0 src, rFlagsReg cr)\n+%{\n+  match(Set dst src);\n+  effect(KILL cr);\n+\n+  ins_cost(50);\n+  format %{ \"xorl    $dst, $dst\\t# int\" %}\n+  ins_encode %{\n+    __ xorl($dst$$Register, $dst$$Register);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+instruct loadConL(rRegL dst, immL src)\n+%{\n+  match(Set dst src);\n+\n+  ins_cost(150);\n+  format %{ \"movq    $dst, $src\\t# long\" %}\n+  ins_encode %{\n+    __ mov64($dst$$Register, $src$$constant);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+instruct loadConL0(rRegL dst, immL0 src, rFlagsReg cr)\n+%{\n+  match(Set dst src);\n+  effect(KILL cr);\n+\n+  ins_cost(50);\n+  format %{ \"xorl    $dst, $dst\\t# long\" %}\n+  ins_encode %{\n+    __ xorl($dst$$Register, $dst$$Register);\n+  %}\n+  ins_pipe(ialu_reg); \/\/ XXX\n+%}\n+\n+instruct loadConUL32(rRegL dst, immUL32 src)\n+%{\n+  match(Set dst src);\n+\n+  ins_cost(60);\n+  format %{ \"movl    $dst, $src\\t# long (unsigned 32-bit)\" %}\n+  ins_encode %{\n+    __ movl($dst$$Register, $src$$constant);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+instruct loadConL32(rRegL dst, immL32 src)\n+%{\n+  match(Set dst src);\n+\n+  ins_cost(70);\n+  format %{ \"movq    $dst, $src\\t# long (32-bit)\" %}\n+  ins_encode %{\n+    __ movq($dst$$Register, $src$$constant);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+instruct loadConP(rRegP dst, immP con) %{\n+  match(Set dst con);\n+\n+  format %{ \"movq    $dst, $con\\t# ptr\" %}\n+  ins_encode %{\n+    __ mov64($dst$$Register, $con$$constant, $con->constant_reloc(), RELOC_IMM64);\n+  %}\n+  ins_pipe(ialu_reg_fat); \/\/ XXX\n+%}\n+\n+instruct loadConP0(rRegP dst, immP0 src, rFlagsReg cr)\n+%{\n+  match(Set dst src);\n+  effect(KILL cr);\n+\n+  ins_cost(50);\n+  format %{ \"xorl    $dst, $dst\\t# ptr\" %}\n+  ins_encode %{\n+    __ xorl($dst$$Register, $dst$$Register);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+instruct loadConP31(rRegP dst, immP31 src, rFlagsReg cr)\n+%{\n+  match(Set dst src);\n+  effect(KILL cr);\n+\n+  ins_cost(60);\n+  format %{ \"movl    $dst, $src\\t# ptr (positive 32-bit)\" %}\n+  ins_encode %{\n+    __ movl($dst$$Register, $src$$constant);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+instruct loadConF(regF dst, immF con) %{\n+  match(Set dst con);\n+  ins_cost(125);\n+  format %{ \"movss   $dst, [$constantaddress]\\t# load from constant table: float=$con\" %}\n+  ins_encode %{\n+    __ movflt($dst$$XMMRegister, $constantaddress($con));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct loadConH(regF dst, immH con) %{\n+  match(Set dst con);\n+  ins_cost(125);\n+  format %{ \"movss   $dst, [$constantaddress]\\t# load from constant table: halffloat=$con\" %}\n+  ins_encode %{\n+    __ movflt($dst$$XMMRegister, $constantaddress($con));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct loadConN0(rRegN dst, immN0 src, rFlagsReg cr) %{\n+  match(Set dst src);\n+  effect(KILL cr);\n+  format %{ \"xorq    $dst, $src\\t# compressed null pointer\" %}\n+  ins_encode %{\n+    __ xorq($dst$$Register, $dst$$Register);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+instruct loadConN(rRegN dst, immN src) %{\n+  match(Set dst src);\n+\n+  ins_cost(125);\n+  format %{ \"movl    $dst, $src\\t# compressed ptr\" %}\n+  ins_encode %{\n+    address con = (address)$src$$constant;\n+    if (con == nullptr) {\n+      ShouldNotReachHere();\n+    } else {\n+      __ set_narrow_oop($dst$$Register, (jobject)$src$$constant);\n+    }\n+  %}\n+  ins_pipe(ialu_reg_fat); \/\/ XXX\n+%}\n+\n+instruct loadConNKlass(rRegN dst, immNKlass src) %{\n+  match(Set dst src);\n+\n+  ins_cost(125);\n+  format %{ \"movl    $dst, $src\\t# compressed klass ptr\" %}\n+  ins_encode %{\n+    address con = (address)$src$$constant;\n+    if (con == nullptr) {\n+      ShouldNotReachHere();\n+    } else {\n+      __ set_narrow_klass($dst$$Register, (Klass*)$src$$constant);\n+    }\n+  %}\n+  ins_pipe(ialu_reg_fat); \/\/ XXX\n+%}\n+\n+instruct loadConF0(regF dst, immF0 src)\n+%{\n+  match(Set dst src);\n+  ins_cost(100);\n+\n+  format %{ \"xorps   $dst, $dst\\t# float 0.0\" %}\n+  ins_encode %{\n+    __ xorps($dst$$XMMRegister, $dst$$XMMRegister);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ Use the same format since predicate() can not be used here.\n+instruct loadConD(regD dst, immD con) %{\n+  match(Set dst con);\n+  ins_cost(125);\n+  format %{ \"movsd   $dst, [$constantaddress]\\t# load from constant table: double=$con\" %}\n+  ins_encode %{\n+    __ movdbl($dst$$XMMRegister, $constantaddress($con));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct loadConD0(regD dst, immD0 src)\n+%{\n+  match(Set dst src);\n+  ins_cost(100);\n+\n+  format %{ \"xorpd   $dst, $dst\\t# double 0.0\" %}\n+  ins_encode %{\n+    __ xorpd($dst$$XMMRegister, $dst$$XMMRegister);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct loadSSI(rRegI dst, stackSlotI src)\n+%{\n+  match(Set dst src);\n+\n+  ins_cost(125);\n+  format %{ \"movl    $dst, $src\\t# int stk\" %}\n+  ins_encode %{\n+    __ movl($dst$$Register, $src$$Address);\n+  %}\n+  ins_pipe(ialu_reg_mem);\n+%}\n+\n+instruct loadSSL(rRegL dst, stackSlotL src)\n+%{\n+  match(Set dst src);\n+\n+  ins_cost(125);\n+  format %{ \"movq    $dst, $src\\t# long stk\" %}\n+  ins_encode %{\n+    __ movq($dst$$Register, $src$$Address);\n+  %}\n+  ins_pipe(ialu_reg_mem);\n+%}\n+\n+instruct loadSSP(rRegP dst, stackSlotP src)\n+%{\n+  match(Set dst src);\n+\n+  ins_cost(125);\n+  format %{ \"movq    $dst, $src\\t# ptr stk\" %}\n+  ins_encode %{\n+    __ movq($dst$$Register, $src$$Address);\n+  %}\n+  ins_pipe(ialu_reg_mem);\n+%}\n+\n+instruct loadSSF(regF dst, stackSlotF src)\n+%{\n+  match(Set dst src);\n+\n+  ins_cost(125);\n+  format %{ \"movss   $dst, $src\\t# float stk\" %}\n+  ins_encode %{\n+    __ movflt($dst$$XMMRegister, Address(rsp, $src$$disp));\n+  %}\n+  ins_pipe(pipe_slow); \/\/ XXX\n+%}\n+\n+\/\/ Use the same format since predicate() can not be used here.\n+instruct loadSSD(regD dst, stackSlotD src)\n+%{\n+  match(Set dst src);\n+\n+  ins_cost(125);\n+  format %{ \"movsd   $dst, $src\\t# double stk\" %}\n+  ins_encode  %{\n+    __ movdbl($dst$$XMMRegister, Address(rsp, $src$$disp));\n+  %}\n+  ins_pipe(pipe_slow); \/\/ XXX\n+%}\n+\n+\/\/ Prefetch instructions for allocation.\n+\/\/ Must be safe to execute with invalid address (cannot fault).\n+\n+instruct prefetchAlloc( memory mem ) %{\n+  predicate(AllocatePrefetchInstr==3);\n+  match(PrefetchAllocation mem);\n+  ins_cost(125);\n+\n+  format %{ \"PREFETCHW $mem\\t# Prefetch allocation into level 1 cache and mark modified\" %}\n+  ins_encode %{\n+    __ prefetchw($mem$$Address);\n+  %}\n+  ins_pipe(ialu_mem);\n+%}\n+\n+instruct prefetchAllocNTA( memory mem ) %{\n+  predicate(AllocatePrefetchInstr==0);\n+  match(PrefetchAllocation mem);\n+  ins_cost(125);\n+\n+  format %{ \"PREFETCHNTA $mem\\t# Prefetch allocation to non-temporal cache for write\" %}\n+  ins_encode %{\n+    __ prefetchnta($mem$$Address);\n+  %}\n+  ins_pipe(ialu_mem);\n+%}\n+\n+instruct prefetchAllocT0( memory mem ) %{\n+  predicate(AllocatePrefetchInstr==1);\n+  match(PrefetchAllocation mem);\n+  ins_cost(125);\n+\n+  format %{ \"PREFETCHT0 $mem\\t# Prefetch allocation to level 1 and 2 caches for write\" %}\n+  ins_encode %{\n+    __ prefetcht0($mem$$Address);\n+  %}\n+  ins_pipe(ialu_mem);\n+%}\n+\n+instruct prefetchAllocT2( memory mem ) %{\n+  predicate(AllocatePrefetchInstr==2);\n+  match(PrefetchAllocation mem);\n+  ins_cost(125);\n+\n+  format %{ \"PREFETCHT2 $mem\\t# Prefetch allocation to level 2 cache for write\" %}\n+  ins_encode %{\n+    __ prefetcht2($mem$$Address);\n+  %}\n+  ins_pipe(ialu_mem);\n+%}\n+\n+\/\/----------Store Instructions-------------------------------------------------\n+\n+\/\/ Store Byte\n+instruct storeB(memory mem, rRegI src)\n+%{\n+  match(Set mem (StoreB mem src));\n+\n+  ins_cost(125); \/\/ XXX\n+  format %{ \"movb    $mem, $src\\t# byte\" %}\n+  ins_encode %{\n+    __ movb($mem$$Address, $src$$Register);\n+  %}\n+  ins_pipe(ialu_mem_reg);\n+%}\n+\n+\/\/ Store Char\/Short\n+instruct storeC(memory mem, rRegI src)\n+%{\n+  match(Set mem (StoreC mem src));\n+\n+  ins_cost(125); \/\/ XXX\n+  format %{ \"movw    $mem, $src\\t# char\/short\" %}\n+  ins_encode %{\n+    __ movw($mem$$Address, $src$$Register);\n+  %}\n+  ins_pipe(ialu_mem_reg);\n+%}\n+\n+\/\/ Store Integer\n+instruct storeI(memory mem, rRegI src)\n+%{\n+  match(Set mem (StoreI mem src));\n+\n+  ins_cost(125); \/\/ XXX\n+  format %{ \"movl    $mem, $src\\t# int\" %}\n+  ins_encode %{\n+    __ movl($mem$$Address, $src$$Register);\n+  %}\n+  ins_pipe(ialu_mem_reg);\n+%}\n+\n+\/\/ Store Long\n+instruct storeL(memory mem, rRegL src)\n+%{\n+  match(Set mem (StoreL mem src));\n+\n+  ins_cost(125); \/\/ XXX\n+  format %{ \"movq    $mem, $src\\t# long\" %}\n+  ins_encode %{\n+    __ movq($mem$$Address, $src$$Register);\n+  %}\n+  ins_pipe(ialu_mem_reg); \/\/ XXX\n+%}\n+\n+\/\/ Store Pointer\n+instruct storeP(memory mem, any_RegP src)\n+%{\n+  predicate(n->as_Store()->barrier_data() == 0);\n+  match(Set mem (StoreP mem src));\n+\n+  ins_cost(125); \/\/ XXX\n+  format %{ \"movq    $mem, $src\\t# ptr\" %}\n+  ins_encode %{\n+    __ movq($mem$$Address, $src$$Register);\n+  %}\n+  ins_pipe(ialu_mem_reg);\n+%}\n+\n+instruct storeImmP0(memory mem, immP0 zero)\n+%{\n+  predicate(UseCompressedOops && (CompressedOops::base() == nullptr) && n->as_Store()->barrier_data() == 0);\n+  match(Set mem (StoreP mem zero));\n+\n+  ins_cost(125); \/\/ XXX\n+  format %{ \"movq    $mem, R12\\t# ptr (R12_heapbase==0)\" %}\n+  ins_encode %{\n+    __ movq($mem$$Address, r12);\n+  %}\n+  ins_pipe(ialu_mem_reg);\n+%}\n+\n+\/\/ Store Null Pointer, mark word, or other simple pointer constant.\n+instruct storeImmP(memory mem, immP31 src)\n+%{\n+  predicate(n->as_Store()->barrier_data() == 0);\n+  match(Set mem (StoreP mem src));\n+\n+  ins_cost(150); \/\/ XXX\n+  format %{ \"movq    $mem, $src\\t# ptr\" %}\n+  ins_encode %{\n+    __ movq($mem$$Address, $src$$constant);\n+  %}\n+  ins_pipe(ialu_mem_imm);\n+%}\n+\n+\/\/ Store Compressed Pointer\n+instruct storeN(memory mem, rRegN src)\n+%{\n+  predicate(n->as_Store()->barrier_data() == 0);\n+  match(Set mem (StoreN mem src));\n+\n+  ins_cost(125); \/\/ XXX\n+  format %{ \"movl    $mem, $src\\t# compressed ptr\" %}\n+  ins_encode %{\n+    __ movl($mem$$Address, $src$$Register);\n+  %}\n+  ins_pipe(ialu_mem_reg);\n+%}\n+\n+instruct storeNKlass(memory mem, rRegN src)\n+%{\n+  match(Set mem (StoreNKlass mem src));\n+\n+  ins_cost(125); \/\/ XXX\n+  format %{ \"movl    $mem, $src\\t# compressed klass ptr\" %}\n+  ins_encode %{\n+    __ movl($mem$$Address, $src$$Register);\n+  %}\n+  ins_pipe(ialu_mem_reg);\n+%}\n+\n+instruct storeImmN0(memory mem, immN0 zero)\n+%{\n+  predicate(CompressedOops::base() == nullptr && n->as_Store()->barrier_data() == 0);\n+  match(Set mem (StoreN mem zero));\n+\n+  ins_cost(125); \/\/ XXX\n+  format %{ \"movl    $mem, R12\\t# compressed ptr (R12_heapbase==0)\" %}\n+  ins_encode %{\n+    __ movl($mem$$Address, r12);\n+  %}\n+  ins_pipe(ialu_mem_reg);\n+%}\n+\n+instruct storeImmN(memory mem, immN src)\n+%{\n+  predicate(n->as_Store()->barrier_data() == 0);\n+  match(Set mem (StoreN mem src));\n+\n+  ins_cost(150); \/\/ XXX\n+  format %{ \"movl    $mem, $src\\t# compressed ptr\" %}\n+  ins_encode %{\n+    address con = (address)$src$$constant;\n+    if (con == nullptr) {\n+      __ movl($mem$$Address, 0);\n+    } else {\n+      __ set_narrow_oop($mem$$Address, (jobject)$src$$constant);\n+    }\n+  %}\n+  ins_pipe(ialu_mem_imm);\n+%}\n+\n+instruct storeImmNKlass(memory mem, immNKlass src)\n+%{\n+  match(Set mem (StoreNKlass mem src));\n+\n+  ins_cost(150); \/\/ XXX\n+  format %{ \"movl    $mem, $src\\t# compressed klass ptr\" %}\n+  ins_encode %{\n+    __ set_narrow_klass($mem$$Address, (Klass*)$src$$constant);\n+  %}\n+  ins_pipe(ialu_mem_imm);\n+%}\n+\n+\/\/ Store Integer Immediate\n+instruct storeImmI0(memory mem, immI_0 zero)\n+%{\n+  predicate(UseCompressedOops && (CompressedOops::base() == nullptr));\n+  match(Set mem (StoreI mem zero));\n+\n+  ins_cost(125); \/\/ XXX\n+  format %{ \"movl    $mem, R12\\t# int (R12_heapbase==0)\" %}\n+  ins_encode %{\n+    __ movl($mem$$Address, r12);\n+  %}\n+  ins_pipe(ialu_mem_reg);\n+%}\n+\n+instruct storeImmI(memory mem, immI src)\n+%{\n+  match(Set mem (StoreI mem src));\n+\n+  ins_cost(150);\n+  format %{ \"movl    $mem, $src\\t# int\" %}\n+  ins_encode %{\n+    __ movl($mem$$Address, $src$$constant);\n+  %}\n+  ins_pipe(ialu_mem_imm);\n+%}\n+\n+\/\/ Store Long Immediate\n+instruct storeImmL0(memory mem, immL0 zero)\n+%{\n+  predicate(UseCompressedOops && (CompressedOops::base() == nullptr));\n+  match(Set mem (StoreL mem zero));\n+\n+  ins_cost(125); \/\/ XXX\n+  format %{ \"movq    $mem, R12\\t# long (R12_heapbase==0)\" %}\n+  ins_encode %{\n+    __ movq($mem$$Address, r12);\n+  %}\n+  ins_pipe(ialu_mem_reg);\n+%}\n+\n+instruct storeImmL(memory mem, immL32 src)\n+%{\n+  match(Set mem (StoreL mem src));\n+\n+  ins_cost(150);\n+  format %{ \"movq    $mem, $src\\t# long\" %}\n+  ins_encode %{\n+    __ movq($mem$$Address, $src$$constant);\n+  %}\n+  ins_pipe(ialu_mem_imm);\n+%}\n+\n+\/\/ Store Short\/Char Immediate\n+instruct storeImmC0(memory mem, immI_0 zero)\n+%{\n+  predicate(UseCompressedOops && (CompressedOops::base() == nullptr));\n+  match(Set mem (StoreC mem zero));\n+\n+  ins_cost(125); \/\/ XXX\n+  format %{ \"movw    $mem, R12\\t# short\/char (R12_heapbase==0)\" %}\n+  ins_encode %{\n+    __ movw($mem$$Address, r12);\n+  %}\n+  ins_pipe(ialu_mem_reg);\n+%}\n+\n+instruct storeImmI16(memory mem, immI16 src)\n+%{\n+  predicate(UseStoreImmI16);\n+  match(Set mem (StoreC mem src));\n+\n+  ins_cost(150);\n+  format %{ \"movw    $mem, $src\\t# short\/char\" %}\n+  ins_encode %{\n+    __ movw($mem$$Address, $src$$constant);\n+  %}\n+  ins_pipe(ialu_mem_imm);\n+%}\n+\n+\/\/ Store Byte Immediate\n+instruct storeImmB0(memory mem, immI_0 zero)\n+%{\n+  predicate(UseCompressedOops && (CompressedOops::base() == nullptr));\n+  match(Set mem (StoreB mem zero));\n+\n+  ins_cost(125); \/\/ XXX\n+  format %{ \"movb    $mem, R12\\t# short\/char (R12_heapbase==0)\" %}\n+  ins_encode %{\n+    __ movb($mem$$Address, r12);\n+  %}\n+  ins_pipe(ialu_mem_reg);\n+%}\n+\n+instruct storeImmB(memory mem, immI8 src)\n+%{\n+  match(Set mem (StoreB mem src));\n+\n+  ins_cost(150); \/\/ XXX\n+  format %{ \"movb    $mem, $src\\t# byte\" %}\n+  ins_encode %{\n+    __ movb($mem$$Address, $src$$constant);\n+  %}\n+  ins_pipe(ialu_mem_imm);\n+%}\n+\n+\/\/ Store Float\n+instruct storeF(memory mem, regF src)\n+%{\n+  match(Set mem (StoreF mem src));\n+\n+  ins_cost(95); \/\/ XXX\n+  format %{ \"movss   $mem, $src\\t# float\" %}\n+  ins_encode %{\n+    __ movflt($mem$$Address, $src$$XMMRegister);\n+  %}\n+  ins_pipe(pipe_slow); \/\/ XXX\n+%}\n+\n+\/\/ Store immediate Float value (it is faster than store from XMM register)\n+instruct storeF0(memory mem, immF0 zero)\n+%{\n+  predicate(UseCompressedOops && (CompressedOops::base() == nullptr));\n+  match(Set mem (StoreF mem zero));\n+\n+  ins_cost(25); \/\/ XXX\n+  format %{ \"movl    $mem, R12\\t# float 0. (R12_heapbase==0)\" %}\n+  ins_encode %{\n+    __ movl($mem$$Address, r12);\n+  %}\n+  ins_pipe(ialu_mem_reg);\n+%}\n+\n+instruct storeF_imm(memory mem, immF src)\n+%{\n+  match(Set mem (StoreF mem src));\n+\n+  ins_cost(50);\n+  format %{ \"movl    $mem, $src\\t# float\" %}\n+  ins_encode %{\n+    __ movl($mem$$Address, jint_cast($src$$constant));\n+  %}\n+  ins_pipe(ialu_mem_imm);\n+%}\n+\n+\/\/ Store Double\n+instruct storeD(memory mem, regD src)\n+%{\n+  match(Set mem (StoreD mem src));\n+\n+  ins_cost(95); \/\/ XXX\n+  format %{ \"movsd   $mem, $src\\t# double\" %}\n+  ins_encode %{\n+    __ movdbl($mem$$Address, $src$$XMMRegister);\n+  %}\n+  ins_pipe(pipe_slow); \/\/ XXX\n+%}\n+\n+\/\/ Store immediate double 0.0 (it is faster than store from XMM register)\n+instruct storeD0_imm(memory mem, immD0 src)\n+%{\n+  predicate(!UseCompressedOops || (CompressedOops::base() != nullptr));\n+  match(Set mem (StoreD mem src));\n+\n+  ins_cost(50);\n+  format %{ \"movq    $mem, $src\\t# double 0.\" %}\n+  ins_encode %{\n+    __ movq($mem$$Address, $src$$constant);\n+  %}\n+  ins_pipe(ialu_mem_imm);\n+%}\n+\n+instruct storeD0(memory mem, immD0 zero)\n+%{\n+  predicate(UseCompressedOops && (CompressedOops::base() == nullptr));\n+  match(Set mem (StoreD mem zero));\n+\n+  ins_cost(25); \/\/ XXX\n+  format %{ \"movq    $mem, R12\\t# double 0. (R12_heapbase==0)\" %}\n+  ins_encode %{\n+    __ movq($mem$$Address, r12);\n+  %}\n+  ins_pipe(ialu_mem_reg);\n+%}\n+\n+instruct storeSSI(stackSlotI dst, rRegI src)\n+%{\n+  match(Set dst src);\n+\n+  ins_cost(100);\n+  format %{ \"movl    $dst, $src\\t# int stk\" %}\n+  ins_encode %{\n+    __ movl($dst$$Address, $src$$Register);\n+  %}\n+  ins_pipe( ialu_mem_reg );\n+%}\n+\n+instruct storeSSL(stackSlotL dst, rRegL src)\n+%{\n+  match(Set dst src);\n+\n+  ins_cost(100);\n+  format %{ \"movq    $dst, $src\\t# long stk\" %}\n+  ins_encode %{\n+    __ movq($dst$$Address, $src$$Register);\n+  %}\n+  ins_pipe(ialu_mem_reg);\n+%}\n+\n+instruct storeSSP(stackSlotP dst, rRegP src)\n+%{\n+  match(Set dst src);\n+\n+  ins_cost(100);\n+  format %{ \"movq    $dst, $src\\t# ptr stk\" %}\n+  ins_encode %{\n+    __ movq($dst$$Address, $src$$Register);\n+  %}\n+  ins_pipe(ialu_mem_reg);\n+%}\n+\n+instruct storeSSF(stackSlotF dst, regF src)\n+%{\n+  match(Set dst src);\n+\n+  ins_cost(95); \/\/ XXX\n+  format %{ \"movss   $dst, $src\\t# float stk\" %}\n+  ins_encode %{\n+    __ movflt(Address(rsp, $dst$$disp), $src$$XMMRegister);\n+  %}\n+  ins_pipe(pipe_slow); \/\/ XXX\n+%}\n+\n+instruct storeSSD(stackSlotD dst, regD src)\n+%{\n+  match(Set dst src);\n+\n+  ins_cost(95); \/\/ XXX\n+  format %{ \"movsd   $dst, $src\\t# double stk\" %}\n+  ins_encode %{\n+    __ movdbl(Address(rsp, $dst$$disp), $src$$XMMRegister);\n+  %}\n+  ins_pipe(pipe_slow); \/\/ XXX\n+%}\n+\n+instruct cacheWB(indirect addr)\n+%{\n+  predicate(VM_Version::supports_data_cache_line_flush());\n+  match(CacheWB addr);\n+\n+  ins_cost(100);\n+  format %{\"cache wb $addr\" %}\n+  ins_encode %{\n+    assert($addr->index_position() < 0, \"should be\");\n+    assert($addr$$disp == 0, \"should be\");\n+    __ cache_wb(Address($addr$$base$$Register, 0));\n+  %}\n+  ins_pipe(pipe_slow); \/\/ XXX\n+%}\n+\n+instruct cacheWBPreSync()\n+%{\n+  predicate(VM_Version::supports_data_cache_line_flush());\n+  match(CacheWBPreSync);\n+\n+  ins_cost(100);\n+  format %{\"cache wb presync\" %}\n+  ins_encode %{\n+    __ cache_wbsync(true);\n+  %}\n+  ins_pipe(pipe_slow); \/\/ XXX\n+%}\n+\n+instruct cacheWBPostSync()\n+%{\n+  predicate(VM_Version::supports_data_cache_line_flush());\n+  match(CacheWBPostSync);\n+\n+  ins_cost(100);\n+  format %{\"cache wb postsync\" %}\n+  ins_encode %{\n+    __ cache_wbsync(false);\n+  %}\n+  ins_pipe(pipe_slow); \/\/ XXX\n+%}\n+\n+\/\/----------BSWAP Instructions-------------------------------------------------\n+instruct bytes_reverse_int(rRegI dst) %{\n+  match(Set dst (ReverseBytesI dst));\n+\n+  format %{ \"bswapl  $dst\" %}\n+  ins_encode %{\n+    __ bswapl($dst$$Register);\n+  %}\n+  ins_pipe( ialu_reg );\n+%}\n+\n+instruct bytes_reverse_long(rRegL dst) %{\n+  match(Set dst (ReverseBytesL dst));\n+\n+  format %{ \"bswapq  $dst\" %}\n+  ins_encode %{\n+    __ bswapq($dst$$Register);\n+  %}\n+  ins_pipe( ialu_reg);\n+%}\n+\n+instruct bytes_reverse_unsigned_short(rRegI dst, rFlagsReg cr) %{\n+  match(Set dst (ReverseBytesUS dst));\n+  effect(KILL cr);\n+\n+  format %{ \"bswapl  $dst\\n\\t\"\n+            \"shrl    $dst,16\\n\\t\" %}\n+  ins_encode %{\n+    __ bswapl($dst$$Register);\n+    __ shrl($dst$$Register, 16);\n+  %}\n+  ins_pipe( ialu_reg );\n+%}\n+\n+instruct bytes_reverse_short(rRegI dst, rFlagsReg cr) %{\n+  match(Set dst (ReverseBytesS dst));\n+  effect(KILL cr);\n+\n+  format %{ \"bswapl  $dst\\n\\t\"\n+            \"sar     $dst,16\\n\\t\" %}\n+  ins_encode %{\n+    __ bswapl($dst$$Register);\n+    __ sarl($dst$$Register, 16);\n+  %}\n+  ins_pipe( ialu_reg );\n+%}\n+\n+\/\/---------- Zeros Count Instructions ------------------------------------------\n+\n+instruct countLeadingZerosI(rRegI dst, rRegI src, rFlagsReg cr) %{\n+  predicate(UseCountLeadingZerosInstruction);\n+  match(Set dst (CountLeadingZerosI src));\n+  effect(KILL cr);\n+\n+  format %{ \"lzcntl  $dst, $src\\t# count leading zeros (int)\" %}\n+  ins_encode %{\n+    __ lzcntl($dst$$Register, $src$$Register);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+instruct countLeadingZerosI_mem(rRegI dst, memory src, rFlagsReg cr) %{\n+  predicate(UseCountLeadingZerosInstruction);\n+  match(Set dst (CountLeadingZerosI (LoadI src)));\n+  effect(KILL cr);\n+  ins_cost(175);\n+  format %{ \"lzcntl  $dst, $src\\t# count leading zeros (int)\" %}\n+  ins_encode %{\n+    __ lzcntl($dst$$Register, $src$$Address);\n+  %}\n+  ins_pipe(ialu_reg_mem);\n+%}\n+\n+instruct countLeadingZerosI_bsr(rRegI dst, rRegI src, rFlagsReg cr) %{\n+  predicate(!UseCountLeadingZerosInstruction);\n+  match(Set dst (CountLeadingZerosI src));\n+  effect(KILL cr);\n+\n+  format %{ \"bsrl    $dst, $src\\t# count leading zeros (int)\\n\\t\"\n+            \"jnz     skip\\n\\t\"\n+            \"movl    $dst, -1\\n\"\n+      \"skip:\\n\\t\"\n+            \"negl    $dst\\n\\t\"\n+            \"addl    $dst, 31\" %}\n+  ins_encode %{\n+    Register Rdst = $dst$$Register;\n+    Register Rsrc = $src$$Register;\n+    Label skip;\n+    __ bsrl(Rdst, Rsrc);\n+    __ jccb(Assembler::notZero, skip);\n+    __ movl(Rdst, -1);\n+    __ bind(skip);\n+    __ negl(Rdst);\n+    __ addl(Rdst, BitsPerInt - 1);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+instruct countLeadingZerosL(rRegI dst, rRegL src, rFlagsReg cr) %{\n+  predicate(UseCountLeadingZerosInstruction);\n+  match(Set dst (CountLeadingZerosL src));\n+  effect(KILL cr);\n+\n+  format %{ \"lzcntq  $dst, $src\\t# count leading zeros (long)\" %}\n+  ins_encode %{\n+    __ lzcntq($dst$$Register, $src$$Register);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+instruct countLeadingZerosL_mem(rRegI dst, memory src, rFlagsReg cr) %{\n+  predicate(UseCountLeadingZerosInstruction);\n+  match(Set dst (CountLeadingZerosL (LoadL src)));\n+  effect(KILL cr);\n+  ins_cost(175);\n+  format %{ \"lzcntq  $dst, $src\\t# count leading zeros (long)\" %}\n+  ins_encode %{\n+    __ lzcntq($dst$$Register, $src$$Address);\n+  %}\n+  ins_pipe(ialu_reg_mem);\n+%}\n+\n+instruct countLeadingZerosL_bsr(rRegI dst, rRegL src, rFlagsReg cr) %{\n+  predicate(!UseCountLeadingZerosInstruction);\n+  match(Set dst (CountLeadingZerosL src));\n+  effect(KILL cr);\n+\n+  format %{ \"bsrq    $dst, $src\\t# count leading zeros (long)\\n\\t\"\n+            \"jnz     skip\\n\\t\"\n+            \"movl    $dst, -1\\n\"\n+      \"skip:\\n\\t\"\n+            \"negl    $dst\\n\\t\"\n+            \"addl    $dst, 63\" %}\n+  ins_encode %{\n+    Register Rdst = $dst$$Register;\n+    Register Rsrc = $src$$Register;\n+    Label skip;\n+    __ bsrq(Rdst, Rsrc);\n+    __ jccb(Assembler::notZero, skip);\n+    __ movl(Rdst, -1);\n+    __ bind(skip);\n+    __ negl(Rdst);\n+    __ addl(Rdst, BitsPerLong - 1);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+instruct countTrailingZerosI(rRegI dst, rRegI src, rFlagsReg cr) %{\n+  predicate(UseCountTrailingZerosInstruction);\n+  match(Set dst (CountTrailingZerosI src));\n+  effect(KILL cr);\n+\n+  format %{ \"tzcntl    $dst, $src\\t# count trailing zeros (int)\" %}\n+  ins_encode %{\n+    __ tzcntl($dst$$Register, $src$$Register);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+instruct countTrailingZerosI_mem(rRegI dst, memory src, rFlagsReg cr) %{\n+  predicate(UseCountTrailingZerosInstruction);\n+  match(Set dst (CountTrailingZerosI (LoadI src)));\n+  effect(KILL cr);\n+  ins_cost(175);\n+  format %{ \"tzcntl    $dst, $src\\t# count trailing zeros (int)\" %}\n+  ins_encode %{\n+    __ tzcntl($dst$$Register, $src$$Address);\n+  %}\n+  ins_pipe(ialu_reg_mem);\n+%}\n+\n+instruct countTrailingZerosI_bsf(rRegI dst, rRegI src, rFlagsReg cr) %{\n+  predicate(!UseCountTrailingZerosInstruction);\n+  match(Set dst (CountTrailingZerosI src));\n+  effect(KILL cr);\n+\n+  format %{ \"bsfl    $dst, $src\\t# count trailing zeros (int)\\n\\t\"\n+            \"jnz     done\\n\\t\"\n+            \"movl    $dst, 32\\n\"\n+      \"done:\" %}\n+  ins_encode %{\n+    Register Rdst = $dst$$Register;\n+    Label done;\n+    __ bsfl(Rdst, $src$$Register);\n+    __ jccb(Assembler::notZero, done);\n+    __ movl(Rdst, BitsPerInt);\n+    __ bind(done);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+instruct countTrailingZerosL(rRegI dst, rRegL src, rFlagsReg cr) %{\n+  predicate(UseCountTrailingZerosInstruction);\n+  match(Set dst (CountTrailingZerosL src));\n+  effect(KILL cr);\n+\n+  format %{ \"tzcntq    $dst, $src\\t# count trailing zeros (long)\" %}\n+  ins_encode %{\n+    __ tzcntq($dst$$Register, $src$$Register);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+instruct countTrailingZerosL_mem(rRegI dst, memory src, rFlagsReg cr) %{\n+  predicate(UseCountTrailingZerosInstruction);\n+  match(Set dst (CountTrailingZerosL (LoadL src)));\n+  effect(KILL cr);\n+  ins_cost(175);\n+  format %{ \"tzcntq    $dst, $src\\t# count trailing zeros (long)\" %}\n+  ins_encode %{\n+    __ tzcntq($dst$$Register, $src$$Address);\n+  %}\n+  ins_pipe(ialu_reg_mem);\n+%}\n+\n+instruct countTrailingZerosL_bsf(rRegI dst, rRegL src, rFlagsReg cr) %{\n+  predicate(!UseCountTrailingZerosInstruction);\n+  match(Set dst (CountTrailingZerosL src));\n+  effect(KILL cr);\n+\n+  format %{ \"bsfq    $dst, $src\\t# count trailing zeros (long)\\n\\t\"\n+            \"jnz     done\\n\\t\"\n+            \"movl    $dst, 64\\n\"\n+      \"done:\" %}\n+  ins_encode %{\n+    Register Rdst = $dst$$Register;\n+    Label done;\n+    __ bsfq(Rdst, $src$$Register);\n+    __ jccb(Assembler::notZero, done);\n+    __ movl(Rdst, BitsPerLong);\n+    __ bind(done);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+\/\/--------------- Reverse Operation Instructions ----------------\n+instruct bytes_reversebit_int(rRegI dst, rRegI src, rRegI rtmp, rFlagsReg cr) %{\n+  predicate(!VM_Version::supports_gfni());\n+  match(Set dst (ReverseI src));\n+  effect(TEMP dst, TEMP rtmp, KILL cr);\n+  format %{ \"reverse_int $dst $src\\t! using $rtmp as TEMP\" %}\n+  ins_encode %{\n+    __ reverseI($dst$$Register, $src$$Register, xnoreg, xnoreg, $rtmp$$Register);\n+  %}\n+  ins_pipe( ialu_reg );\n+%}\n+\n+instruct bytes_reversebit_int_gfni(rRegI dst, rRegI src, vlRegF xtmp1, vlRegF xtmp2, rRegL rtmp, rFlagsReg cr) %{\n+  predicate(VM_Version::supports_gfni());\n+  match(Set dst (ReverseI src));\n+  effect(TEMP dst, TEMP xtmp1, TEMP xtmp2, TEMP rtmp, KILL cr);\n+  format %{ \"reverse_int $dst $src\\t! using $rtmp, $xtmp1 and $xtmp2 as TEMP\" %}\n+  ins_encode %{\n+    __ reverseI($dst$$Register, $src$$Register, $xtmp1$$XMMRegister, $xtmp2$$XMMRegister, $rtmp$$Register);\n+  %}\n+  ins_pipe( ialu_reg );\n+%}\n+\n+instruct bytes_reversebit_long(rRegL dst, rRegL src, rRegL rtmp1, rRegL rtmp2, rFlagsReg cr) %{\n+  predicate(!VM_Version::supports_gfni());\n+  match(Set dst (ReverseL src));\n+  effect(TEMP dst, TEMP rtmp1, TEMP rtmp2, KILL cr);\n+  format %{ \"reverse_long $dst $src\\t! using $rtmp1 and $rtmp2 as TEMP\" %}\n+  ins_encode %{\n+    __ reverseL($dst$$Register, $src$$Register, xnoreg, xnoreg, $rtmp1$$Register, $rtmp2$$Register);\n+  %}\n+  ins_pipe( ialu_reg );\n+%}\n+\n+instruct bytes_reversebit_long_gfni(rRegL dst, rRegL src, vlRegD xtmp1, vlRegD xtmp2, rRegL rtmp, rFlagsReg cr) %{\n+  predicate(VM_Version::supports_gfni());\n+  match(Set dst (ReverseL src));\n+  effect(TEMP dst, TEMP xtmp1, TEMP xtmp2, TEMP rtmp, KILL cr);\n+  format %{ \"reverse_long $dst $src\\t! using $rtmp, $xtmp1 and $xtmp2 as TEMP\" %}\n+  ins_encode %{\n+    __ reverseL($dst$$Register, $src$$Register, $xtmp1$$XMMRegister, $xtmp2$$XMMRegister, $rtmp$$Register, noreg);\n+  %}\n+  ins_pipe( ialu_reg );\n+%}\n+\n+\/\/---------- Population Count Instructions -------------------------------------\n+\n+instruct popCountI(rRegI dst, rRegI src, rFlagsReg cr) %{\n+  predicate(UsePopCountInstruction);\n+  match(Set dst (PopCountI src));\n+  effect(KILL cr);\n+\n+  format %{ \"popcnt  $dst, $src\" %}\n+  ins_encode %{\n+    __ popcntl($dst$$Register, $src$$Register);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+instruct popCountI_mem(rRegI dst, memory mem, rFlagsReg cr) %{\n+  predicate(UsePopCountInstruction);\n+  match(Set dst (PopCountI (LoadI mem)));\n+  effect(KILL cr);\n+\n+  format %{ \"popcnt  $dst, $mem\" %}\n+  ins_encode %{\n+    __ popcntl($dst$$Register, $mem$$Address);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+\/\/ Note: Long.bitCount(long) returns an int.\n+instruct popCountL(rRegI dst, rRegL src, rFlagsReg cr) %{\n+  predicate(UsePopCountInstruction);\n+  match(Set dst (PopCountL src));\n+  effect(KILL cr);\n+\n+  format %{ \"popcnt  $dst, $src\" %}\n+  ins_encode %{\n+    __ popcntq($dst$$Register, $src$$Register);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+\/\/ Note: Long.bitCount(long) returns an int.\n+instruct popCountL_mem(rRegI dst, memory mem, rFlagsReg cr) %{\n+  predicate(UsePopCountInstruction);\n+  match(Set dst (PopCountL (LoadL mem)));\n+  effect(KILL cr);\n+\n+  format %{ \"popcnt  $dst, $mem\" %}\n+  ins_encode %{\n+    __ popcntq($dst$$Register, $mem$$Address);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+\n+\/\/----------MemBar Instructions-----------------------------------------------\n+\/\/ Memory barrier flavors\n+\n+instruct membar_acquire()\n+%{\n+  match(MemBarAcquire);\n+  match(LoadFence);\n+  ins_cost(0);\n+\n+  size(0);\n+  format %{ \"MEMBAR-acquire ! (empty encoding)\" %}\n+  ins_encode();\n+  ins_pipe(empty);\n+%}\n+\n+instruct membar_acquire_lock()\n+%{\n+  match(MemBarAcquireLock);\n+  ins_cost(0);\n+\n+  size(0);\n+  format %{ \"MEMBAR-acquire (prior CMPXCHG in FastLock so empty encoding)\" %}\n+  ins_encode();\n+  ins_pipe(empty);\n+%}\n+\n+instruct membar_release()\n+%{\n+  match(MemBarRelease);\n+  match(StoreFence);\n+  ins_cost(0);\n+\n+  size(0);\n+  format %{ \"MEMBAR-release ! (empty encoding)\" %}\n+  ins_encode();\n+  ins_pipe(empty);\n+%}\n+\n+instruct membar_release_lock()\n+%{\n+  match(MemBarReleaseLock);\n+  ins_cost(0);\n+\n+  size(0);\n+  format %{ \"MEMBAR-release (a FastUnlock follows so empty encoding)\" %}\n+  ins_encode();\n+  ins_pipe(empty);\n+%}\n+\n+instruct membar_volatile(rFlagsReg cr) %{\n+  match(MemBarVolatile);\n+  effect(KILL cr);\n+  ins_cost(400);\n+\n+  format %{\n+    $$template\n+    $$emit$$\"lock addl [rsp + #0], 0\\t! membar_volatile\"\n+  %}\n+  ins_encode %{\n+    __ membar(Assembler::StoreLoad);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct unnecessary_membar_volatile()\n+%{\n+  match(MemBarVolatile);\n+  predicate(Matcher::post_store_load_barrier(n));\n+  ins_cost(0);\n+\n+  size(0);\n+  format %{ \"MEMBAR-volatile (unnecessary so empty encoding)\" %}\n+  ins_encode();\n+  ins_pipe(empty);\n+%}\n+\n+instruct membar_storestore() %{\n+  match(MemBarStoreStore);\n+  match(StoreStoreFence);\n+  ins_cost(0);\n+\n+  size(0);\n+  format %{ \"MEMBAR-storestore (empty encoding)\" %}\n+  ins_encode( );\n+  ins_pipe(empty);\n+%}\n+\n+\/\/----------Move Instructions--------------------------------------------------\n+\n+instruct castX2P(rRegP dst, rRegL src)\n+%{\n+  match(Set dst (CastX2P src));\n+\n+  format %{ \"movq    $dst, $src\\t# long->ptr\" %}\n+  ins_encode %{\n+    if ($dst$$reg != $src$$reg) {\n+      __ movptr($dst$$Register, $src$$Register);\n+    }\n+  %}\n+  ins_pipe(ialu_reg_reg); \/\/ XXX\n+%}\n+\n+instruct castP2X(rRegL dst, rRegP src)\n+%{\n+  match(Set dst (CastP2X src));\n+\n+  format %{ \"movq    $dst, $src\\t# ptr -> long\" %}\n+  ins_encode %{\n+    if ($dst$$reg != $src$$reg) {\n+      __ movptr($dst$$Register, $src$$Register);\n+    }\n+  %}\n+  ins_pipe(ialu_reg_reg); \/\/ XXX\n+%}\n+\n+\/\/ Convert oop into int for vectors alignment masking\n+instruct convP2I(rRegI dst, rRegP src)\n+%{\n+  match(Set dst (ConvL2I (CastP2X src)));\n+\n+  format %{ \"movl    $dst, $src\\t# ptr -> int\" %}\n+  ins_encode %{\n+    __ movl($dst$$Register, $src$$Register);\n+  %}\n+  ins_pipe(ialu_reg_reg); \/\/ XXX\n+%}\n+\n+\/\/ Convert compressed oop into int for vectors alignment masking\n+\/\/ in case of 32bit oops (heap < 4Gb).\n+instruct convN2I(rRegI dst, rRegN src)\n+%{\n+  predicate(CompressedOops::shift() == 0);\n+  match(Set dst (ConvL2I (CastP2X (DecodeN src))));\n+\n+  format %{ \"movl    $dst, $src\\t# compressed ptr -> int\" %}\n+  ins_encode %{\n+    __ movl($dst$$Register, $src$$Register);\n+  %}\n+  ins_pipe(ialu_reg_reg); \/\/ XXX\n+%}\n+\n+\/\/ Convert oop pointer into compressed form\n+instruct encodeHeapOop(rRegN dst, rRegP src, rFlagsReg cr) %{\n+  predicate(n->bottom_type()->make_ptr()->ptr() != TypePtr::NotNull);\n+  match(Set dst (EncodeP src));\n+  effect(KILL cr);\n+  format %{ \"encode_heap_oop $dst,$src\" %}\n+  ins_encode %{\n+    Register s = $src$$Register;\n+    Register d = $dst$$Register;\n+    if (s != d) {\n+      __ movq(d, s);\n+    }\n+    __ encode_heap_oop(d);\n+  %}\n+  ins_pipe(ialu_reg_long);\n+%}\n+\n+instruct encodeHeapOop_not_null(rRegN dst, rRegP src, rFlagsReg cr) %{\n+  predicate(n->bottom_type()->make_ptr()->ptr() == TypePtr::NotNull);\n+  match(Set dst (EncodeP src));\n+  effect(KILL cr);\n+  format %{ \"encode_heap_oop_not_null $dst,$src\" %}\n+  ins_encode %{\n+    __ encode_heap_oop_not_null($dst$$Register, $src$$Register);\n+  %}\n+  ins_pipe(ialu_reg_long);\n+%}\n+\n+instruct decodeHeapOop(rRegP dst, rRegN src, rFlagsReg cr) %{\n+  predicate(n->bottom_type()->is_ptr()->ptr() != TypePtr::NotNull &&\n+            n->bottom_type()->is_ptr()->ptr() != TypePtr::Constant);\n+  match(Set dst (DecodeN src));\n+  effect(KILL cr);\n+  format %{ \"decode_heap_oop $dst,$src\" %}\n+  ins_encode %{\n+    Register s = $src$$Register;\n+    Register d = $dst$$Register;\n+    if (s != d) {\n+      __ movq(d, s);\n+    }\n+    __ decode_heap_oop(d);\n+  %}\n+  ins_pipe(ialu_reg_long);\n+%}\n+\n+instruct decodeHeapOop_not_null(rRegP dst, rRegN src, rFlagsReg cr) %{\n+  predicate(n->bottom_type()->is_ptr()->ptr() == TypePtr::NotNull ||\n+            n->bottom_type()->is_ptr()->ptr() == TypePtr::Constant);\n+  match(Set dst (DecodeN src));\n+  effect(KILL cr);\n+  format %{ \"decode_heap_oop_not_null $dst,$src\" %}\n+  ins_encode %{\n+    Register s = $src$$Register;\n+    Register d = $dst$$Register;\n+    if (s != d) {\n+      __ decode_heap_oop_not_null(d, s);\n+    } else {\n+      __ decode_heap_oop_not_null(d);\n+    }\n+  %}\n+  ins_pipe(ialu_reg_long);\n+%}\n+\n+instruct encodeKlass_not_null(rRegN dst, rRegP src, rFlagsReg cr) %{\n+  match(Set dst (EncodePKlass src));\n+  effect(TEMP dst, KILL cr);\n+  format %{ \"encode_and_move_klass_not_null $dst,$src\" %}\n+  ins_encode %{\n+    __ encode_and_move_klass_not_null($dst$$Register, $src$$Register);\n+  %}\n+  ins_pipe(ialu_reg_long);\n+%}\n+\n+instruct decodeKlass_not_null(rRegP dst, rRegN src, rFlagsReg cr) %{\n+  match(Set dst (DecodeNKlass src));\n+  effect(TEMP dst, KILL cr);\n+  format %{ \"decode_and_move_klass_not_null $dst,$src\" %}\n+  ins_encode %{\n+    __ decode_and_move_klass_not_null($dst$$Register, $src$$Register);\n+  %}\n+  ins_pipe(ialu_reg_long);\n+%}\n+\n+\/\/----------Conditional Move---------------------------------------------------\n+\/\/ Jump\n+\/\/ dummy instruction for generating temp registers\n+instruct jumpXtnd_offset(rRegL switch_val, immI2 shift, rRegI dest) %{\n+  match(Jump (LShiftL switch_val shift));\n+  ins_cost(350);\n+  predicate(false);\n+  effect(TEMP dest);\n+\n+  format %{ \"leaq    $dest, [$constantaddress]\\n\\t\"\n+            \"jmp     [$dest + $switch_val << $shift]\\n\\t\" %}\n+  ins_encode %{\n+    \/\/ We could use jump(ArrayAddress) except that the macro assembler needs to use r10\n+    \/\/ to do that and the compiler is using that register as one it can allocate.\n+    \/\/ So we build it all by hand.\n+    \/\/ Address index(noreg, switch_reg, (Address::ScaleFactor)$shift$$constant);\n+    \/\/ ArrayAddress dispatch(table, index);\n+    Address dispatch($dest$$Register, $switch_val$$Register, (Address::ScaleFactor) $shift$$constant);\n+    __ lea($dest$$Register, $constantaddress);\n+    __ jmp(dispatch);\n+  %}\n+  ins_pipe(pipe_jmp);\n+%}\n+\n+instruct jumpXtnd_addr(rRegL switch_val, immI2 shift, immL32 offset, rRegI dest) %{\n+  match(Jump (AddL (LShiftL switch_val shift) offset));\n+  ins_cost(350);\n+  effect(TEMP dest);\n+\n+  format %{ \"leaq    $dest, [$constantaddress]\\n\\t\"\n+            \"jmp     [$dest + $switch_val << $shift + $offset]\\n\\t\" %}\n+  ins_encode %{\n+    \/\/ We could use jump(ArrayAddress) except that the macro assembler needs to use r10\n+    \/\/ to do that and the compiler is using that register as one it can allocate.\n+    \/\/ So we build it all by hand.\n+    \/\/ Address index(noreg, switch_reg, (Address::ScaleFactor) $shift$$constant, (int) $offset$$constant);\n+    \/\/ ArrayAddress dispatch(table, index);\n+    Address dispatch($dest$$Register, $switch_val$$Register, (Address::ScaleFactor) $shift$$constant, (int) $offset$$constant);\n+    __ lea($dest$$Register, $constantaddress);\n+    __ jmp(dispatch);\n+  %}\n+  ins_pipe(pipe_jmp);\n+%}\n+\n+instruct jumpXtnd(rRegL switch_val, rRegI dest) %{\n+  match(Jump switch_val);\n+  ins_cost(350);\n+  effect(TEMP dest);\n+\n+  format %{ \"leaq    $dest, [$constantaddress]\\n\\t\"\n+            \"jmp     [$dest + $switch_val]\\n\\t\" %}\n+  ins_encode %{\n+    \/\/ We could use jump(ArrayAddress) except that the macro assembler needs to use r10\n+    \/\/ to do that and the compiler is using that register as one it can allocate.\n+    \/\/ So we build it all by hand.\n+    \/\/ Address index(noreg, switch_reg, Address::times_1);\n+    \/\/ ArrayAddress dispatch(table, index);\n+    Address dispatch($dest$$Register, $switch_val$$Register, Address::times_1);\n+    __ lea($dest$$Register, $constantaddress);\n+    __ jmp(dispatch);\n+  %}\n+  ins_pipe(pipe_jmp);\n+%}\n+\n+\/\/ Conditional move\n+instruct cmovI_imm_01(rRegI dst, immI_1 src, rFlagsReg cr, cmpOp cop)\n+%{\n+  predicate(n->in(2)->in(2)->is_Con() && n->in(2)->in(2)->get_int() == 0);\n+  match(Set dst (CMoveI (Binary cop cr) (Binary src dst)));\n+\n+  ins_cost(100); \/\/ XXX\n+  format %{ \"setbn$cop $dst\\t# signed, int\" %}\n+  ins_encode %{\n+    Assembler::Condition cond = (Assembler::Condition)($cop$$cmpcode);\n+    __ setb(MacroAssembler::negate_condition(cond), $dst$$Register);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+instruct cmovI_reg(rRegI dst, rRegI src, rFlagsReg cr, cmpOp cop)\n+%{\n+  predicate(!UseAPX);\n+  match(Set dst (CMoveI (Binary cop cr) (Binary dst src)));\n+\n+  ins_cost(200); \/\/ XXX\n+  format %{ \"cmovl$cop $dst, $src\\t# signed, int\" %}\n+  ins_encode %{\n+    __ cmovl((Assembler::Condition)($cop$$cmpcode), $dst$$Register, $src$$Register);\n+  %}\n+  ins_pipe(pipe_cmov_reg);\n+%}\n+\n+instruct cmovI_reg_ndd(rRegI dst, rRegI src1, rRegI src2, rFlagsReg cr, cmpOp cop)\n+%{\n+  predicate(UseAPX);\n+  match(Set dst (CMoveI (Binary cop cr) (Binary src1 src2)));\n+\n+  ins_cost(200);\n+  format %{ \"ecmovl$cop $dst, $src1, $src2\\t# signed, int ndd\" %}\n+  ins_encode %{\n+    __ ecmovl((Assembler::Condition)($cop$$cmpcode), $dst$$Register, $src1$$Register, $src2$$Register);\n+  %}\n+  ins_pipe(pipe_cmov_reg);\n+%}\n+\n+instruct cmovI_imm_01U(rRegI dst, immI_1 src, rFlagsRegU cr, cmpOpU cop)\n+%{\n+  predicate(n->in(2)->in(2)->is_Con() && n->in(2)->in(2)->get_int() == 0);\n+  match(Set dst (CMoveI (Binary cop cr) (Binary src dst)));\n+\n+  ins_cost(100); \/\/ XXX\n+  format %{ \"setbn$cop $dst\\t# unsigned, int\" %}\n+  ins_encode %{\n+    Assembler::Condition cond = (Assembler::Condition)($cop$$cmpcode);\n+    __ setb(MacroAssembler::negate_condition(cond), $dst$$Register);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+instruct cmovI_regU(cmpOpU cop, rFlagsRegU cr, rRegI dst, rRegI src) %{\n+  predicate(!UseAPX);\n+  match(Set dst (CMoveI (Binary cop cr) (Binary dst src)));\n+\n+  ins_cost(200); \/\/ XXX\n+  format %{ \"cmovl$cop $dst, $src\\t# unsigned, int\" %}\n+  ins_encode %{\n+    __ cmovl((Assembler::Condition)($cop$$cmpcode), $dst$$Register, $src$$Register);\n+  %}\n+  ins_pipe(pipe_cmov_reg);\n+%}\n+\n+instruct cmovI_regU_ndd(rRegI dst, cmpOpU cop, rFlagsRegU cr, rRegI src1, rRegI src2) %{\n+  predicate(UseAPX);\n+  match(Set dst (CMoveI (Binary cop cr) (Binary src1 src2)));\n+\n+  ins_cost(200);\n+  format %{ \"ecmovl$cop $dst, $src1, $src2\\t# unsigned, int ndd\" %}\n+  ins_encode %{\n+    __ ecmovl((Assembler::Condition)($cop$$cmpcode), $dst$$Register, $src1$$Register, $src2$$Register);\n+  %}\n+  ins_pipe(pipe_cmov_reg);\n+%}\n+\n+instruct cmovI_imm_01UCF(rRegI dst, immI_1 src, rFlagsRegUCF cr, cmpOpUCF cop)\n+%{\n+  predicate(n->in(2)->in(2)->is_Con() && n->in(2)->in(2)->get_int() == 0);\n+  match(Set dst (CMoveI (Binary cop cr) (Binary src dst)));\n+\n+  ins_cost(100); \/\/ XXX\n+  format %{ \"setbn$cop $dst\\t# unsigned, int\" %}\n+  ins_encode %{\n+    Assembler::Condition cond = (Assembler::Condition)($cop$$cmpcode);\n+    __ setb(MacroAssembler::negate_condition(cond), $dst$$Register);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+instruct cmovI_regUCF(cmpOpUCF cop, rFlagsRegUCF cr, rRegI dst, rRegI src) %{\n+  predicate(!UseAPX);\n+  match(Set dst (CMoveI (Binary cop cr) (Binary dst src)));\n+  ins_cost(200);\n+  expand %{\n+    cmovI_regU(cop, cr, dst, src);\n+  %}\n+%}\n+\n+instruct cmovI_regUCF_ndd(rRegI dst, cmpOpUCF cop, rFlagsRegUCF cr, rRegI src1, rRegI src2) %{\n+  predicate(UseAPX);\n+  match(Set dst (CMoveI (Binary cop cr) (Binary src1 src2)));\n+  ins_cost(200);\n+  format %{ \"ecmovl$cop $dst, $src1, $src2\\t# unsigned, int ndd\" %}\n+  ins_encode %{\n+    __ ecmovl((Assembler::Condition)($cop$$cmpcode), $dst$$Register, $src1$$Register, $src2$$Register);\n+  %}\n+  ins_pipe(pipe_cmov_reg);\n+%}\n+\n+instruct cmovI_regUCF2_ne(cmpOpUCF2 cop, rFlagsRegUCF cr, rRegI dst, rRegI src) %{\n+  predicate(!UseAPX && n->in(1)->in(1)->as_Bool()->_test._test == BoolTest::ne);\n+  match(Set dst (CMoveI (Binary cop cr) (Binary dst src)));\n+\n+  ins_cost(200); \/\/ XXX\n+  format %{ \"cmovpl  $dst, $src\\n\\t\"\n+            \"cmovnel $dst, $src\" %}\n+  ins_encode %{\n+    __ cmovl(Assembler::parity, $dst$$Register, $src$$Register);\n+    __ cmovl(Assembler::notEqual, $dst$$Register, $src$$Register);\n+  %}\n+  ins_pipe(pipe_cmov_reg);\n+%}\n+\n+instruct cmovI_regUCF2_ne_ndd(cmpOpUCF2 cop, rFlagsRegUCF cr, rRegI dst, rRegI src1, rRegI src2) %{\n+  predicate(UseAPX && n->in(1)->in(1)->as_Bool()->_test._test == BoolTest::ne);\n+  match(Set dst (CMoveI (Binary cop cr) (Binary src1 src2)));\n+  effect(TEMP dst);\n+\n+  ins_cost(200);\n+  format %{ \"ecmovpl  $dst, $src1, $src2\\n\\t\"\n+            \"cmovnel  $dst, $src2\" %}\n+  ins_encode %{\n+    __ ecmovl(Assembler::parity, $dst$$Register, $src1$$Register, $src2$$Register);\n+    __ cmovl(Assembler::notEqual, $dst$$Register, $src2$$Register);\n+  %}\n+  ins_pipe(pipe_cmov_reg);\n+%}\n+\n+\/\/ Since (x == y) == !(x != y), we can flip the sense of the test by flipping the\n+\/\/ inputs of the CMove\n+instruct cmovI_regUCF2_eq(cmpOpUCF2 cop, rFlagsRegUCF cr, rRegI dst, rRegI src) %{\n+  predicate(!UseAPX && n->in(1)->in(1)->as_Bool()->_test._test == BoolTest::eq);\n+  match(Set dst (CMoveI (Binary cop cr) (Binary src dst)));\n+  effect(TEMP dst);\n+\n+  ins_cost(200); \/\/ XXX\n+  format %{ \"cmovpl  $dst, $src\\n\\t\"\n+            \"cmovnel $dst, $src\" %}\n+  ins_encode %{\n+    __ cmovl(Assembler::parity, $dst$$Register, $src$$Register);\n+    __ cmovl(Assembler::notEqual, $dst$$Register, $src$$Register);\n+  %}\n+  ins_pipe(pipe_cmov_reg);\n+%}\n+\n+\/\/ We need this special handling for only eq \/ neq comparison since NaN == NaN is false,\n+\/\/ and parity flag bit is set if any of the operand is a NaN.\n+instruct cmovI_regUCF2_eq_ndd(cmpOpUCF2 cop, rFlagsRegUCF cr, rRegI dst, rRegI src1, rRegI src2) %{\n+  predicate(UseAPX && n->in(1)->in(1)->as_Bool()->_test._test == BoolTest::eq);\n+  match(Set dst (CMoveI (Binary cop cr) (Binary src2 src1)));\n+  effect(TEMP dst);\n+\n+  ins_cost(200);\n+  format %{ \"ecmovpl  $dst, $src1, $src2\\n\\t\"\n+            \"cmovnel  $dst, $src2\" %}\n+  ins_encode %{\n+    __ ecmovl(Assembler::parity, $dst$$Register, $src1$$Register, $src2$$Register);\n+    __ cmovl(Assembler::notEqual, $dst$$Register, $src2$$Register);\n+  %}\n+  ins_pipe(pipe_cmov_reg);\n+%}\n+\n+\/\/ Conditional move\n+instruct cmovI_mem(cmpOp cop, rFlagsReg cr, rRegI dst, memory src) %{\n+  predicate(!UseAPX);\n+  match(Set dst (CMoveI (Binary cop cr) (Binary dst (LoadI src))));\n+\n+  ins_cost(250); \/\/ XXX\n+  format %{ \"cmovl$cop $dst, $src\\t# signed, int\" %}\n+  ins_encode %{\n+    __ cmovl((Assembler::Condition)($cop$$cmpcode), $dst$$Register, $src$$Address);\n+  %}\n+  ins_pipe(pipe_cmov_mem);\n+%}\n+\n+\/\/ Conditional move\n+instruct cmovI_rReg_rReg_mem_ndd(rRegI dst, cmpOp cop, rFlagsReg cr, rRegI src1, memory src2)\n+%{\n+  predicate(UseAPX);\n+  match(Set dst (CMoveI (Binary cop cr) (Binary src1 (LoadI src2))));\n+\n+  ins_cost(250);\n+  format %{ \"ecmovl$cop $dst, $src1, $src2\\t# signed, int ndd\" %}\n+  ins_encode %{\n+    __ ecmovl((Assembler::Condition)($cop$$cmpcode), $dst$$Register, $src1$$Register, $src2$$Address);\n+  %}\n+  ins_pipe(pipe_cmov_mem);\n+%}\n+\n+\/\/ Conditional move\n+instruct cmovI_memU(cmpOpU cop, rFlagsRegU cr, rRegI dst, memory src)\n+%{\n+  predicate(!UseAPX);\n+  match(Set dst (CMoveI (Binary cop cr) (Binary dst (LoadI src))));\n+\n+  ins_cost(250); \/\/ XXX\n+  format %{ \"cmovl$cop $dst, $src\\t# unsigned, int\" %}\n+  ins_encode %{\n+    __ cmovl((Assembler::Condition)($cop$$cmpcode), $dst$$Register, $src$$Address);\n+  %}\n+  ins_pipe(pipe_cmov_mem);\n+%}\n+\n+instruct cmovI_memUCF(cmpOpUCF cop, rFlagsRegUCF cr, rRegI dst, memory src) %{\n+  predicate(!UseAPX);\n+  match(Set dst (CMoveI (Binary cop cr) (Binary dst (LoadI src))));\n+  ins_cost(250);\n+  expand %{\n+    cmovI_memU(cop, cr, dst, src);\n+  %}\n+%}\n+\n+instruct cmovI_rReg_rReg_memU_ndd(rRegI dst, cmpOpU cop, rFlagsRegU cr, rRegI src1, memory src2)\n+%{\n+  predicate(UseAPX);\n+  match(Set dst (CMoveI (Binary cop cr) (Binary src1 (LoadI src2))));\n+\n+  ins_cost(250);\n+  format %{ \"ecmovl$cop $dst, $src1, $src2\\t# unsigned, int ndd\" %}\n+  ins_encode %{\n+    __ ecmovl((Assembler::Condition)($cop$$cmpcode), $dst$$Register, $src1$$Register, $src2$$Address);\n+  %}\n+  ins_pipe(pipe_cmov_mem);\n+%}\n+\n+instruct cmovI_rReg_rReg_memUCF_ndd(rRegI dst, cmpOpUCF cop, rFlagsRegUCF cr, rRegI src1, memory src2)\n+%{\n+  predicate(UseAPX);\n+  match(Set dst (CMoveI (Binary cop cr) (Binary src1 (LoadI src2))));\n+  ins_cost(250);\n+  format %{ \"ecmovl$cop $dst, $src1, $src2\\t# unsigned, int ndd\" %}\n+  ins_encode %{\n+    __ ecmovl((Assembler::Condition)($cop$$cmpcode), $dst$$Register, $src1$$Register, $src2$$Address);\n+  %}\n+  ins_pipe(pipe_cmov_mem);\n+%}\n+\n+\/\/ Conditional move\n+instruct cmovN_reg(rRegN dst, rRegN src, rFlagsReg cr, cmpOp cop)\n+%{\n+  predicate(!UseAPX);\n+  match(Set dst (CMoveN (Binary cop cr) (Binary dst src)));\n+\n+  ins_cost(200); \/\/ XXX\n+  format %{ \"cmovl$cop $dst, $src\\t# signed, compressed ptr\" %}\n+  ins_encode %{\n+    __ cmovl((Assembler::Condition)($cop$$cmpcode), $dst$$Register, $src$$Register);\n+  %}\n+  ins_pipe(pipe_cmov_reg);\n+%}\n+\n+\/\/ Conditional move ndd\n+instruct cmovN_reg_ndd(rRegN dst, rRegN src1, rRegN src2, rFlagsReg cr, cmpOp cop)\n+%{\n+  predicate(UseAPX);\n+  match(Set dst (CMoveN (Binary cop cr) (Binary src1 src2)));\n+\n+  ins_cost(200);\n+  format %{ \"ecmovl$cop $dst, $src1, $src2\\t# signed, compressed ptr ndd\" %}\n+  ins_encode %{\n+    __ ecmovl((Assembler::Condition)($cop$$cmpcode), $dst$$Register, $src1$$Register, $src2$$Register);\n+  %}\n+  ins_pipe(pipe_cmov_reg);\n+%}\n+\n+\/\/ Conditional move\n+instruct cmovN_regU(cmpOpU cop, rFlagsRegU cr, rRegN dst, rRegN src)\n+%{\n+  predicate(!UseAPX);\n+  match(Set dst (CMoveN (Binary cop cr) (Binary dst src)));\n+\n+  ins_cost(200); \/\/ XXX\n+  format %{ \"cmovl$cop $dst, $src\\t# unsigned, compressed ptr\" %}\n+  ins_encode %{\n+    __ cmovl((Assembler::Condition)($cop$$cmpcode), $dst$$Register, $src$$Register);\n+  %}\n+  ins_pipe(pipe_cmov_reg);\n+%}\n+\n+instruct cmovN_regUCF(cmpOpUCF cop, rFlagsRegUCF cr, rRegN dst, rRegN src) %{\n+  predicate(!UseAPX);\n+  match(Set dst (CMoveN (Binary cop cr) (Binary dst src)));\n+  ins_cost(200);\n+  expand %{\n+    cmovN_regU(cop, cr, dst, src);\n+  %}\n+%}\n+\n+\/\/ Conditional move ndd\n+instruct cmovN_regU_ndd(rRegN dst, cmpOpU cop, rFlagsRegU cr, rRegN src1, rRegN src2)\n+%{\n+  predicate(UseAPX);\n+  match(Set dst (CMoveN (Binary cop cr) (Binary src1 src2)));\n+\n+  ins_cost(200);\n+  format %{ \"ecmovl$cop $dst, $src1, $src2\\t# unsigned, compressed ptr ndd\" %}\n+  ins_encode %{\n+    __ ecmovl((Assembler::Condition)($cop$$cmpcode), $dst$$Register, $src1$$Register, $src2$$Register);\n+  %}\n+  ins_pipe(pipe_cmov_reg);\n+%}\n+\n+instruct cmovN_regUCF_ndd(rRegN dst, cmpOpUCF cop, rFlagsRegUCF cr, rRegN src1, rRegN src2) %{\n+  predicate(UseAPX);\n+  match(Set dst (CMoveN (Binary cop cr) (Binary src1 src2)));\n+  ins_cost(200);\n+  format %{ \"ecmovl$cop $dst, $src1, $src2\\t# unsigned, compressed ptr ndd\" %}\n+  ins_encode %{\n+    __ ecmovl((Assembler::Condition)($cop$$cmpcode), $dst$$Register, $src1$$Register, $src2$$Register);\n+  %}\n+  ins_pipe(pipe_cmov_reg);\n+%}\n+\n+instruct cmovN_regUCF2_ne(cmpOpUCF2 cop, rFlagsRegUCF cr, rRegN dst, rRegN src) %{\n+  predicate(n->in(1)->in(1)->as_Bool()->_test._test == BoolTest::ne);\n+  match(Set dst (CMoveN (Binary cop cr) (Binary dst src)));\n+\n+  ins_cost(200); \/\/ XXX\n+  format %{ \"cmovpl  $dst, $src\\n\\t\"\n+            \"cmovnel $dst, $src\" %}\n+  ins_encode %{\n+    __ cmovl(Assembler::parity, $dst$$Register, $src$$Register);\n+    __ cmovl(Assembler::notEqual, $dst$$Register, $src$$Register);\n+  %}\n+  ins_pipe(pipe_cmov_reg);\n+%}\n+\n+\/\/ Since (x == y) == !(x != y), we can flip the sense of the test by flipping the\n+\/\/ inputs of the CMove\n+instruct cmovN_regUCF2_eq(cmpOpUCF2 cop, rFlagsRegUCF cr, rRegN dst, rRegN src) %{\n+  predicate(n->in(1)->in(1)->as_Bool()->_test._test == BoolTest::eq);\n+  match(Set dst (CMoveN (Binary cop cr) (Binary src dst)));\n+\n+  ins_cost(200); \/\/ XXX\n+  format %{ \"cmovpl  $dst, $src\\n\\t\"\n+            \"cmovnel $dst, $src\" %}\n+  ins_encode %{\n+    __ cmovl(Assembler::parity, $dst$$Register, $src$$Register);\n+    __ cmovl(Assembler::notEqual, $dst$$Register, $src$$Register);\n+  %}\n+  ins_pipe(pipe_cmov_reg);\n+%}\n+\n+\/\/ Conditional move\n+instruct cmovP_reg(rRegP dst, rRegP src, rFlagsReg cr, cmpOp cop)\n+%{\n+  predicate(!UseAPX);\n+  match(Set dst (CMoveP (Binary cop cr) (Binary dst src)));\n+\n+  ins_cost(200); \/\/ XXX\n+  format %{ \"cmovq$cop $dst, $src\\t# signed, ptr\" %}\n+  ins_encode %{\n+    __ cmovq((Assembler::Condition)($cop$$cmpcode), $dst$$Register, $src$$Register);\n+  %}\n+  ins_pipe(pipe_cmov_reg);  \/\/ XXX\n+%}\n+\n+\/\/ Conditional move ndd\n+instruct cmovP_reg_ndd(rRegP dst, rRegP src1, rRegP src2, rFlagsReg cr, cmpOp cop)\n+%{\n+  predicate(UseAPX);\n+  match(Set dst (CMoveP (Binary cop cr) (Binary src1 src2)));\n+\n+  ins_cost(200);\n+  format %{ \"ecmovq$cop $dst, $src1, $src2\\t# signed, ptr ndd\" %}\n+  ins_encode %{\n+    __ ecmovq((Assembler::Condition)($cop$$cmpcode), $dst$$Register, $src1$$Register, $src2$$Register);\n+  %}\n+  ins_pipe(pipe_cmov_reg);\n+%}\n+\n+\/\/ Conditional move\n+instruct cmovP_regU(cmpOpU cop, rFlagsRegU cr, rRegP dst, rRegP src)\n+%{\n+  predicate(!UseAPX);\n+  match(Set dst (CMoveP (Binary cop cr) (Binary dst src)));\n+\n+  ins_cost(200); \/\/ XXX\n+  format %{ \"cmovq$cop $dst, $src\\t# unsigned, ptr\" %}\n+  ins_encode %{\n+    __ cmovq((Assembler::Condition)($cop$$cmpcode), $dst$$Register, $src$$Register);\n+  %}\n+  ins_pipe(pipe_cmov_reg); \/\/ XXX\n+%}\n+\n+\/\/ Conditional move ndd\n+instruct cmovP_regU_ndd(rRegP dst, cmpOpU cop, rFlagsRegU cr, rRegP src1, rRegP src2)\n+%{\n+  predicate(UseAPX);\n+  match(Set dst (CMoveP (Binary cop cr) (Binary src1 src2)));\n+\n+  ins_cost(200);\n+  format %{ \"ecmovq$cop $dst, $src1, $src2\\t# unsigned, ptr ndd\" %}\n+  ins_encode %{\n+    __ ecmovq((Assembler::Condition)($cop$$cmpcode), $dst$$Register, $src1$$Register, $src2$$Register);\n+  %}\n+  ins_pipe(pipe_cmov_reg);\n+%}\n+\n+instruct cmovP_regUCF(cmpOpUCF cop, rFlagsRegUCF cr, rRegP dst, rRegP src) %{\n+  predicate(!UseAPX);\n+  match(Set dst (CMoveP (Binary cop cr) (Binary dst src)));\n+  ins_cost(200);\n+  expand %{\n+    cmovP_regU(cop, cr, dst, src);\n+  %}\n+%}\n+\n+instruct cmovP_regUCF_ndd(rRegP dst, cmpOpUCF cop, rFlagsRegUCF cr, rRegP src1, rRegP src2) %{\n+  predicate(UseAPX);\n+  match(Set dst (CMoveP (Binary cop cr) (Binary src1 src2)));\n+  ins_cost(200);\n+  format %{ \"ecmovq$cop $dst, $src1, $src2\\t# unsigned, ptr ndd\" %}\n+  ins_encode %{\n+    __ ecmovq((Assembler::Condition)($cop$$cmpcode), $dst$$Register, $src1$$Register, $src2$$Register);\n+  %}\n+  ins_pipe(pipe_cmov_reg);\n+%}\n+\n+instruct cmovP_regUCF2_ne(cmpOpUCF2 cop, rFlagsRegUCF cr, rRegP dst, rRegP src) %{\n+  predicate(!UseAPX && n->in(1)->in(1)->as_Bool()->_test._test == BoolTest::ne);\n+  match(Set dst (CMoveP (Binary cop cr) (Binary dst src)));\n+\n+  ins_cost(200); \/\/ XXX\n+  format %{ \"cmovpq  $dst, $src\\n\\t\"\n+            \"cmovneq $dst, $src\" %}\n+  ins_encode %{\n+    __ cmovq(Assembler::parity, $dst$$Register, $src$$Register);\n+    __ cmovq(Assembler::notEqual, $dst$$Register, $src$$Register);\n+  %}\n+  ins_pipe(pipe_cmov_reg);\n+%}\n+\n+instruct cmovP_regUCF2_ne_ndd(cmpOpUCF2 cop, rFlagsRegUCF cr, rRegP dst, rRegP src1, rRegP src2) %{\n+  predicate(UseAPX && n->in(1)->in(1)->as_Bool()->_test._test == BoolTest::ne);\n+  match(Set dst (CMoveP (Binary cop cr) (Binary src1 src2)));\n+  effect(TEMP dst);\n+\n+  ins_cost(200);\n+  format %{ \"ecmovpq  $dst, $src1, $src2\\n\\t\"\n+            \"cmovneq  $dst, $src2\" %}\n+  ins_encode %{\n+    __ ecmovq(Assembler::parity, $dst$$Register, $src1$$Register, $src2$$Register);\n+    __ cmovq(Assembler::notEqual, $dst$$Register, $src2$$Register);\n+  %}\n+  ins_pipe(pipe_cmov_reg);\n+%}\n+\n+\/\/ Since (x == y) == !(x != y), we can flip the sense of the test by flipping the\n+\/\/ inputs of the CMove\n+instruct cmovP_regUCF2_eq(cmpOpUCF2 cop, rFlagsRegUCF cr, rRegP dst, rRegP src) %{\n+  predicate(!UseAPX && n->in(1)->in(1)->as_Bool()->_test._test == BoolTest::eq);\n+  match(Set dst (CMoveP (Binary cop cr) (Binary src dst)));\n+\n+  ins_cost(200); \/\/ XXX\n+  format %{ \"cmovpq  $dst, $src\\n\\t\"\n+            \"cmovneq $dst, $src\" %}\n+  ins_encode %{\n+    __ cmovq(Assembler::parity, $dst$$Register, $src$$Register);\n+    __ cmovq(Assembler::notEqual, $dst$$Register, $src$$Register);\n+  %}\n+  ins_pipe(pipe_cmov_reg);\n+%}\n+\n+instruct cmovP_regUCF2_eq_ndd(cmpOpUCF2 cop, rFlagsRegUCF cr, rRegP dst, rRegP src1, rRegP src2) %{\n+  predicate(UseAPX && n->in(1)->in(1)->as_Bool()->_test._test == BoolTest::eq);\n+  match(Set dst (CMoveP (Binary cop cr) (Binary src2 src1)));\n+  effect(TEMP dst);\n+\n+  ins_cost(200);\n+  format %{ \"ecmovpq  $dst, $src1, $src2\\n\\t\"\n+            \"cmovneq  $dst, $src2\" %}\n+  ins_encode %{\n+    __ ecmovq(Assembler::parity, $dst$$Register, $src1$$Register, $src2$$Register);\n+    __ cmovq(Assembler::notEqual, $dst$$Register, $src2$$Register);\n+  %}\n+  ins_pipe(pipe_cmov_reg);\n+%}\n+\n+instruct cmovL_imm_01(rRegL dst, immL1 src, rFlagsReg cr, cmpOp cop)\n+%{\n+  predicate(n->in(2)->in(2)->is_Con() && n->in(2)->in(2)->get_long() == 0);\n+  match(Set dst (CMoveL (Binary cop cr) (Binary src dst)));\n+\n+  ins_cost(100); \/\/ XXX\n+  format %{ \"setbn$cop $dst\\t# signed, long\" %}\n+  ins_encode %{\n+    Assembler::Condition cond = (Assembler::Condition)($cop$$cmpcode);\n+    __ setb(MacroAssembler::negate_condition(cond), $dst$$Register);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+instruct cmovL_reg(cmpOp cop, rFlagsReg cr, rRegL dst, rRegL src)\n+%{\n+  predicate(!UseAPX);\n+  match(Set dst (CMoveL (Binary cop cr) (Binary dst src)));\n+\n+  ins_cost(200); \/\/ XXX\n+  format %{ \"cmovq$cop $dst, $src\\t# signed, long\" %}\n+  ins_encode %{\n+    __ cmovq((Assembler::Condition)($cop$$cmpcode), $dst$$Register, $src$$Register);\n+  %}\n+  ins_pipe(pipe_cmov_reg);  \/\/ XXX\n+%}\n+\n+instruct cmovL_reg_ndd(rRegL dst, cmpOp cop, rFlagsReg cr, rRegL src1, rRegL src2)\n+%{\n+  predicate(UseAPX);\n+  match(Set dst (CMoveL (Binary cop cr) (Binary src1 src2)));\n+\n+  ins_cost(200);\n+  format %{ \"ecmovq$cop $dst, $src1, $src2\\t# signed, long ndd\" %}\n+  ins_encode %{\n+    __ ecmovq((Assembler::Condition)($cop$$cmpcode), $dst$$Register, $src1$$Register, $src2$$Register);\n+  %}\n+  ins_pipe(pipe_cmov_reg);\n+%}\n+\n+instruct cmovL_mem(cmpOp cop, rFlagsReg cr, rRegL dst, memory src)\n+%{\n+  predicate(!UseAPX);\n+  match(Set dst (CMoveL (Binary cop cr) (Binary dst (LoadL src))));\n+\n+  ins_cost(200); \/\/ XXX\n+  format %{ \"cmovq$cop $dst, $src\\t# signed, long\" %}\n+  ins_encode %{\n+    __ cmovq((Assembler::Condition)($cop$$cmpcode), $dst$$Register, $src$$Address);\n+  %}\n+  ins_pipe(pipe_cmov_mem);  \/\/ XXX\n+%}\n+\n+instruct cmovL_rReg_rReg_mem_ndd(rRegL dst, cmpOp cop, rFlagsReg cr, rRegL src1, memory src2)\n+%{\n+  predicate(UseAPX);\n+  match(Set dst (CMoveL (Binary cop cr) (Binary src1 (LoadL src2))));\n+\n+  ins_cost(200);\n+  format %{ \"ecmovq$cop $dst, $src1, $src2\\t# signed, long ndd\" %}\n+  ins_encode %{\n+    __ ecmovq((Assembler::Condition)($cop$$cmpcode), $dst$$Register, $src1$$Register, $src2$$Address);\n+  %}\n+  ins_pipe(pipe_cmov_mem);\n+%}\n+\n+instruct cmovL_imm_01U(rRegL dst, immL1 src, rFlagsRegU cr, cmpOpU cop)\n+%{\n+  predicate(n->in(2)->in(2)->is_Con() && n->in(2)->in(2)->get_long() == 0);\n+  match(Set dst (CMoveL (Binary cop cr) (Binary src dst)));\n+\n+  ins_cost(100); \/\/ XXX\n+  format %{ \"setbn$cop $dst\\t# unsigned, long\" %}\n+  ins_encode %{\n+    Assembler::Condition cond = (Assembler::Condition)($cop$$cmpcode);\n+    __ setb(MacroAssembler::negate_condition(cond), $dst$$Register);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+instruct cmovL_regU(cmpOpU cop, rFlagsRegU cr, rRegL dst, rRegL src)\n+%{\n+  predicate(!UseAPX);\n+  match(Set dst (CMoveL (Binary cop cr) (Binary dst src)));\n+\n+  ins_cost(200); \/\/ XXX\n+  format %{ \"cmovq$cop $dst, $src\\t# unsigned, long\" %}\n+  ins_encode %{\n+    __ cmovq((Assembler::Condition)($cop$$cmpcode), $dst$$Register, $src$$Register);\n+  %}\n+  ins_pipe(pipe_cmov_reg); \/\/ XXX\n+%}\n+\n+instruct cmovL_regU_ndd(rRegL dst, cmpOpU cop, rFlagsRegU cr, rRegL src1, rRegL src2)\n+%{\n+  predicate(UseAPX);\n+  match(Set dst (CMoveL (Binary cop cr) (Binary src1 src2)));\n+\n+  ins_cost(200);\n+  format %{ \"ecmovq$cop $dst, $src1, $src2\\t# unsigned, long ndd\" %}\n+  ins_encode %{\n+    __ ecmovq((Assembler::Condition)($cop$$cmpcode), $dst$$Register, $src1$$Register, $src2$$Register);\n+  %}\n+  ins_pipe(pipe_cmov_reg);\n+%}\n+\n+instruct cmovL_imm_01UCF(rRegL dst, immL1 src, rFlagsRegUCF cr, cmpOpUCF cop)\n+%{\n+  predicate(n->in(2)->in(2)->is_Con() && n->in(2)->in(2)->get_long() == 0);\n+  match(Set dst (CMoveL (Binary cop cr) (Binary src dst)));\n+\n+  ins_cost(100); \/\/ XXX\n+  format %{ \"setbn$cop $dst\\t# unsigned, long\" %}\n+  ins_encode %{\n+    Assembler::Condition cond = (Assembler::Condition)($cop$$cmpcode);\n+    __ setb(MacroAssembler::negate_condition(cond), $dst$$Register);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+instruct cmovL_regUCF(cmpOpUCF cop, rFlagsRegUCF cr, rRegL dst, rRegL src) %{\n+  predicate(!UseAPX);\n+  match(Set dst (CMoveL (Binary cop cr) (Binary dst src)));\n+  ins_cost(200);\n+  expand %{\n+    cmovL_regU(cop, cr, dst, src);\n+  %}\n+%}\n+\n+instruct cmovL_regUCF_ndd(rRegL dst, cmpOpUCF cop, rFlagsRegUCF cr, rRegL src1, rRegL src2)\n+%{\n+  predicate(UseAPX);\n+  match(Set dst (CMoveL (Binary cop cr) (Binary src1 src2)));\n+  ins_cost(200);\n+  format %{ \"ecmovq$cop $dst, $src1, $src2\\t# unsigned, long ndd\" %}\n+  ins_encode %{\n+    __ ecmovq((Assembler::Condition)($cop$$cmpcode), $dst$$Register, $src1$$Register, $src2$$Register);\n+  %}\n+  ins_pipe(pipe_cmov_reg);\n+%}\n+\n+instruct cmovL_regUCF2_ne(cmpOpUCF2 cop, rFlagsRegUCF cr, rRegL dst, rRegL src) %{\n+  predicate(!UseAPX && n->in(1)->in(1)->as_Bool()->_test._test == BoolTest::ne);\n+  match(Set dst (CMoveL (Binary cop cr) (Binary dst src)));\n+\n+  ins_cost(200); \/\/ XXX\n+  format %{ \"cmovpq  $dst, $src\\n\\t\"\n+            \"cmovneq $dst, $src\" %}\n+  ins_encode %{\n+    __ cmovq(Assembler::parity, $dst$$Register, $src$$Register);\n+    __ cmovq(Assembler::notEqual, $dst$$Register, $src$$Register);\n+  %}\n+  ins_pipe(pipe_cmov_reg);\n+%}\n+\n+instruct cmovL_regUCF2_ne_ndd(cmpOpUCF2 cop, rFlagsRegUCF cr, rRegL dst, rRegL src1, rRegL src2) %{\n+  predicate(UseAPX && n->in(1)->in(1)->as_Bool()->_test._test == BoolTest::ne);\n+  match(Set dst (CMoveL (Binary cop cr) (Binary src1 src2)));\n+  effect(TEMP dst);\n+\n+  ins_cost(200);\n+  format %{ \"ecmovpq  $dst, $src1, $src2\\n\\t\"\n+            \"cmovneq  $dst, $src2\" %}\n+  ins_encode %{\n+    __ ecmovq(Assembler::parity, $dst$$Register, $src1$$Register, $src2$$Register);\n+    __ cmovq(Assembler::notEqual, $dst$$Register, $src2$$Register);\n+  %}\n+  ins_pipe(pipe_cmov_reg);\n+%}\n+\n+\/\/ Since (x == y) == !(x != y), we can flip the sense of the test by flipping the\n+\/\/ inputs of the CMove\n+instruct cmovL_regUCF2_eq(cmpOpUCF2 cop, rFlagsRegUCF cr, rRegL dst, rRegL src) %{\n+  predicate(!UseAPX && n->in(1)->in(1)->as_Bool()->_test._test == BoolTest::eq);\n+  match(Set dst (CMoveL (Binary cop cr) (Binary src dst)));\n+\n+  ins_cost(200); \/\/ XXX\n+  format %{ \"cmovpq  $dst, $src\\n\\t\"\n+            \"cmovneq $dst, $src\" %}\n+  ins_encode %{\n+    __ cmovq(Assembler::parity, $dst$$Register, $src$$Register);\n+    __ cmovq(Assembler::notEqual, $dst$$Register, $src$$Register);\n+  %}\n+  ins_pipe(pipe_cmov_reg);\n+%}\n+\n+instruct cmovL_regUCF2_eq_ndd(cmpOpUCF2 cop, rFlagsRegUCF cr, rRegL dst, rRegL src1, rRegL src2) %{\n+  predicate(UseAPX && n->in(1)->in(1)->as_Bool()->_test._test == BoolTest::eq);\n+  match(Set dst (CMoveL (Binary cop cr) (Binary src2 src1)));\n+  effect(TEMP dst);\n+\n+  ins_cost(200);\n+  format %{ \"ecmovpq  $dst, $src1, $src2\\n\\t\"\n+            \"cmovneq $dst, $src2\" %}\n+  ins_encode %{\n+    __ ecmovq(Assembler::parity, $dst$$Register, $src1$$Register, $src2$$Register);\n+    __ cmovq(Assembler::notEqual, $dst$$Register, $src2$$Register);\n+  %}\n+  ins_pipe(pipe_cmov_reg);\n+%}\n+\n+instruct cmovL_memU(cmpOpU cop, rFlagsRegU cr, rRegL dst, memory src)\n+%{\n+  predicate(!UseAPX);\n+  match(Set dst (CMoveL (Binary cop cr) (Binary dst (LoadL src))));\n+\n+  ins_cost(200); \/\/ XXX\n+  format %{ \"cmovq$cop $dst, $src\\t# unsigned, long\" %}\n+  ins_encode %{\n+    __ cmovq((Assembler::Condition)($cop$$cmpcode), $dst$$Register, $src$$Address);\n+  %}\n+  ins_pipe(pipe_cmov_mem); \/\/ XXX\n+%}\n+\n+instruct cmovL_memUCF(cmpOpUCF cop, rFlagsRegUCF cr, rRegL dst, memory src) %{\n+  predicate(!UseAPX);\n+  match(Set dst (CMoveL (Binary cop cr) (Binary dst (LoadL src))));\n+  ins_cost(200);\n+  expand %{\n+    cmovL_memU(cop, cr, dst, src);\n+  %}\n+%}\n+\n+instruct cmovL_rReg_rReg_memU_ndd(rRegL dst, cmpOpU cop, rFlagsRegU cr, rRegL src1, memory src2)\n+%{\n+  predicate(UseAPX);\n+  match(Set dst (CMoveL (Binary cop cr) (Binary src1 (LoadL src2))));\n+\n+  ins_cost(200);\n+  format %{ \"ecmovq$cop $dst, $src1, $src2\\t# unsigned, long ndd\" %}\n+  ins_encode %{\n+    __ ecmovq((Assembler::Condition)($cop$$cmpcode), $dst$$Register, $src1$$Register, $src2$$Address);\n+  %}\n+  ins_pipe(pipe_cmov_mem);\n+%}\n+\n+instruct cmovL_rReg_rReg_memUCF_ndd(rRegL dst, cmpOpUCF cop, rFlagsRegUCF cr, rRegL src1, memory src2)\n+%{\n+  predicate(UseAPX);\n+  match(Set dst (CMoveL (Binary cop cr) (Binary src1 (LoadL src2))));\n+  ins_cost(200);\n+  format %{ \"ecmovq$cop $dst, $src1, $src2\\t# unsigned, long ndd\" %}\n+  ins_encode %{\n+    __ ecmovq((Assembler::Condition)($cop$$cmpcode), $dst$$Register, $src1$$Register, $src2$$Address);\n+  %}\n+  ins_pipe(pipe_cmov_mem);\n+%}\n+\n+instruct cmovF_reg(cmpOp cop, rFlagsReg cr, regF dst, regF src)\n+%{\n+  match(Set dst (CMoveF (Binary cop cr) (Binary dst src)));\n+\n+  ins_cost(200); \/\/ XXX\n+  format %{ \"jn$cop    skip\\t# signed cmove float\\n\\t\"\n+            \"movss     $dst, $src\\n\"\n+    \"skip:\" %}\n+  ins_encode %{\n+    Label Lskip;\n+    \/\/ Invert sense of branch from sense of CMOV\n+    __ jccb((Assembler::Condition)($cop$$cmpcode^1), Lskip);\n+    __ movflt($dst$$XMMRegister, $src$$XMMRegister);\n+    __ bind(Lskip);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct cmovF_regU(cmpOpU cop, rFlagsRegU cr, regF dst, regF src)\n+%{\n+  match(Set dst (CMoveF (Binary cop cr) (Binary dst src)));\n+\n+  ins_cost(200); \/\/ XXX\n+  format %{ \"jn$cop    skip\\t# unsigned cmove float\\n\\t\"\n+            \"movss     $dst, $src\\n\"\n+    \"skip:\" %}\n+  ins_encode %{\n+    Label Lskip;\n+    \/\/ Invert sense of branch from sense of CMOV\n+    __ jccb((Assembler::Condition)($cop$$cmpcode^1), Lskip);\n+    __ movflt($dst$$XMMRegister, $src$$XMMRegister);\n+    __ bind(Lskip);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct cmovF_regUCF(cmpOpUCF cop, rFlagsRegUCF cr, regF dst, regF src) %{\n+  match(Set dst (CMoveF (Binary cop cr) (Binary dst src)));\n+  ins_cost(200);\n+  expand %{\n+    cmovF_regU(cop, cr, dst, src);\n+  %}\n+%}\n+\n+instruct cmovD_reg(cmpOp cop, rFlagsReg cr, regD dst, regD src)\n+%{\n+  match(Set dst (CMoveD (Binary cop cr) (Binary dst src)));\n+\n+  ins_cost(200); \/\/ XXX\n+  format %{ \"jn$cop    skip\\t# signed cmove double\\n\\t\"\n+            \"movsd     $dst, $src\\n\"\n+    \"skip:\" %}\n+  ins_encode %{\n+    Label Lskip;\n+    \/\/ Invert sense of branch from sense of CMOV\n+    __ jccb((Assembler::Condition)($cop$$cmpcode^1), Lskip);\n+    __ movdbl($dst$$XMMRegister, $src$$XMMRegister);\n+    __ bind(Lskip);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct cmovD_regU(cmpOpU cop, rFlagsRegU cr, regD dst, regD src)\n+%{\n+  match(Set dst (CMoveD (Binary cop cr) (Binary dst src)));\n+\n+  ins_cost(200); \/\/ XXX\n+  format %{ \"jn$cop    skip\\t# unsigned cmove double\\n\\t\"\n+            \"movsd     $dst, $src\\n\"\n+    \"skip:\" %}\n+  ins_encode %{\n+    Label Lskip;\n+    \/\/ Invert sense of branch from sense of CMOV\n+    __ jccb((Assembler::Condition)($cop$$cmpcode^1), Lskip);\n+    __ movdbl($dst$$XMMRegister, $src$$XMMRegister);\n+    __ bind(Lskip);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct cmovD_regUCF(cmpOpUCF cop, rFlagsRegUCF cr, regD dst, regD src) %{\n+  match(Set dst (CMoveD (Binary cop cr) (Binary dst src)));\n+  ins_cost(200);\n+  expand %{\n+    cmovD_regU(cop, cr, dst, src);\n+  %}\n+%}\n+\n+\/\/----------Arithmetic Instructions--------------------------------------------\n+\/\/----------Addition Instructions----------------------------------------------\n+\n+instruct addI_rReg(rRegI dst, rRegI src, rFlagsReg cr)\n+%{\n+  predicate(!UseAPX);\n+  match(Set dst (AddI dst src));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_overflow_flag, PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_carry_flag, PD::Flag_sets_parity_flag);\n+  format %{ \"addl    $dst, $src\\t# int\" %}\n+  ins_encode %{\n+    __ addl($dst$$Register, $src$$Register);\n+  %}\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+instruct addI_rReg_ndd(rRegI dst, rRegI src1, rRegI src2, rFlagsReg cr)\n+%{\n+  predicate(UseAPX);\n+  match(Set dst (AddI src1 src2));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_overflow_flag, PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_carry_flag, PD::Flag_sets_parity_flag);\n+\n+  format %{ \"eaddl    $dst, $src1, $src2\\t# int ndd\" %}\n+  ins_encode %{\n+    __ eaddl($dst$$Register, $src1$$Register, $src2$$Register, false);\n+  %}\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+instruct addI_rReg_imm(rRegI dst, immI src, rFlagsReg cr)\n+%{\n+  predicate(!UseAPX);\n+  match(Set dst (AddI dst src));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_overflow_flag, PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_carry_flag, PD::Flag_sets_parity_flag);\n+\n+  format %{ \"addl    $dst, $src\\t# int\" %}\n+  ins_encode %{\n+    __ addl($dst$$Register, $src$$constant);\n+  %}\n+  ins_pipe( ialu_reg );\n+%}\n+\n+instruct addI_rReg_rReg_imm_ndd(rRegI dst, rRegI src1, immI src2, rFlagsReg cr)\n+%{\n+  predicate(UseAPX);\n+  match(Set dst (AddI src1 src2));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_overflow_flag, PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_carry_flag, PD::Flag_sets_parity_flag);\n+\n+  format %{ \"eaddl    $dst, $src1, $src2\\t# int ndd\" %}\n+  ins_encode %{\n+    __ eaddl($dst$$Register, $src1$$Register, $src2$$constant, false);\n+  %}\n+  ins_pipe( ialu_reg );\n+%}\n+\n+instruct addI_rReg_mem_imm_ndd(rRegI dst, memory src1, immI src2, rFlagsReg cr)\n+%{\n+  predicate(UseAPX);\n+  match(Set dst (AddI (LoadI src1) src2));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_overflow_flag, PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_carry_flag, PD::Flag_sets_parity_flag);\n+\n+  format %{ \"eaddl    $dst, $src1, $src2\\t# int ndd\" %}\n+  ins_encode %{\n+    __ eaddl($dst$$Register, $src1$$Address, $src2$$constant, false);\n+  %}\n+  ins_pipe( ialu_reg );\n+%}\n+\n+instruct addI_rReg_mem(rRegI dst, memory src, rFlagsReg cr)\n+%{\n+  predicate(!UseAPX);\n+  match(Set dst (AddI dst (LoadI src)));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_overflow_flag, PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_carry_flag, PD::Flag_sets_parity_flag);\n+\n+  ins_cost(150); \/\/ XXX\n+  format %{ \"addl    $dst, $src\\t# int\" %}\n+  ins_encode %{\n+    __ addl($dst$$Register, $src$$Address);\n+  %}\n+  ins_pipe(ialu_reg_mem);\n+%}\n+\n+instruct addI_rReg_rReg_mem_ndd(rRegI dst, rRegI src1, memory src2, rFlagsReg cr)\n+%{\n+  predicate(UseAPX);\n+  match(Set dst (AddI src1 (LoadI src2)));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_overflow_flag, PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_carry_flag, PD::Flag_sets_parity_flag);\n+\n+  ins_cost(150);\n+  format %{ \"eaddl    $dst, $src1, $src2\\t# int ndd\" %}\n+  ins_encode %{\n+    __ eaddl($dst$$Register, $src1$$Register, $src2$$Address, false);\n+  %}\n+  ins_pipe(ialu_reg_mem);\n+%}\n+\n+instruct addI_mem_rReg(memory dst, rRegI src, rFlagsReg cr)\n+%{\n+  match(Set dst (StoreI dst (AddI (LoadI dst) src)));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_overflow_flag, PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_carry_flag, PD::Flag_sets_parity_flag);\n+\n+  ins_cost(150); \/\/ XXX\n+  format %{ \"addl    $dst, $src\\t# int\" %}\n+  ins_encode %{\n+    __ addl($dst$$Address, $src$$Register);\n+  %}\n+  ins_pipe(ialu_mem_reg);\n+%}\n+\n+instruct addI_mem_imm(memory dst, immI src, rFlagsReg cr)\n+%{\n+  match(Set dst (StoreI dst (AddI (LoadI dst) src)));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_overflow_flag, PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_carry_flag, PD::Flag_sets_parity_flag);\n+\n+\n+  ins_cost(125); \/\/ XXX\n+  format %{ \"addl    $dst, $src\\t# int\" %}\n+  ins_encode %{\n+    __ addl($dst$$Address, $src$$constant);\n+  %}\n+  ins_pipe(ialu_mem_imm);\n+%}\n+\n+instruct incI_rReg(rRegI dst, immI_1 src, rFlagsReg cr)\n+%{\n+  predicate(!UseAPX && UseIncDec);\n+  match(Set dst (AddI dst src));\n+  effect(KILL cr);\n+\n+  format %{ \"incl    $dst\\t# int\" %}\n+  ins_encode %{\n+    __ incrementl($dst$$Register);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+instruct incI_rReg_ndd(rRegI dst, rRegI src, immI_1 val, rFlagsReg cr)\n+%{\n+  predicate(UseAPX && UseIncDec);\n+  match(Set dst (AddI src val));\n+  effect(KILL cr);\n+\n+  format %{ \"eincl    $dst, $src\\t# int ndd\" %}\n+  ins_encode %{\n+    __ eincl($dst$$Register, $src$$Register, false);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+instruct incI_rReg_mem_ndd(rRegI dst, memory src, immI_1 val, rFlagsReg cr)\n+%{\n+  predicate(UseAPX && UseIncDec);\n+  match(Set dst (AddI (LoadI src) val));\n+  effect(KILL cr);\n+\n+  format %{ \"eincl    $dst, $src\\t# int ndd\" %}\n+  ins_encode %{\n+    __ eincl($dst$$Register, $src$$Address, false);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+instruct incI_mem(memory dst, immI_1 src, rFlagsReg cr)\n+%{\n+  predicate(UseIncDec);\n+  match(Set dst (StoreI dst (AddI (LoadI dst) src)));\n+  effect(KILL cr);\n+\n+  ins_cost(125); \/\/ XXX\n+  format %{ \"incl    $dst\\t# int\" %}\n+  ins_encode %{\n+    __ incrementl($dst$$Address);\n+  %}\n+  ins_pipe(ialu_mem_imm);\n+%}\n+\n+\/\/ XXX why does that use AddI\n+instruct decI_rReg(rRegI dst, immI_M1 src, rFlagsReg cr)\n+%{\n+  predicate(!UseAPX && UseIncDec);\n+  match(Set dst (AddI dst src));\n+  effect(KILL cr);\n+\n+  format %{ \"decl    $dst\\t# int\" %}\n+  ins_encode %{\n+    __ decrementl($dst$$Register);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+instruct decI_rReg_ndd(rRegI dst, rRegI src, immI_M1 val, rFlagsReg cr)\n+%{\n+  predicate(UseAPX && UseIncDec);\n+  match(Set dst (AddI src val));\n+  effect(KILL cr);\n+\n+  format %{ \"edecl    $dst, $src\\t# int ndd\" %}\n+  ins_encode %{\n+    __ edecl($dst$$Register, $src$$Register, false);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+instruct decI_rReg_mem_ndd(rRegI dst, memory src, immI_M1 val, rFlagsReg cr)\n+%{\n+  predicate(UseAPX && UseIncDec);\n+  match(Set dst (AddI (LoadI src) val));\n+  effect(KILL cr);\n+\n+  format %{ \"edecl    $dst, $src\\t# int ndd\" %}\n+  ins_encode %{\n+    __ edecl($dst$$Register, $src$$Address, false);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+\/\/ XXX why does that use AddI\n+instruct decI_mem(memory dst, immI_M1 src, rFlagsReg cr)\n+%{\n+  predicate(UseIncDec);\n+  match(Set dst (StoreI dst (AddI (LoadI dst) src)));\n+  effect(KILL cr);\n+\n+  ins_cost(125); \/\/ XXX\n+  format %{ \"decl    $dst\\t# int\" %}\n+  ins_encode %{\n+    __ decrementl($dst$$Address);\n+  %}\n+  ins_pipe(ialu_mem_imm);\n+%}\n+\n+instruct leaI_rReg_immI2_immI(rRegI dst, rRegI index, immI2 scale, immI disp)\n+%{\n+  predicate(VM_Version::supports_fast_2op_lea());\n+  match(Set dst (AddI (LShiftI index scale) disp));\n+\n+  format %{ \"leal $dst, [$index << $scale + $disp]\\t# int\" %}\n+  ins_encode %{\n+    Address::ScaleFactor scale = static_cast<Address::ScaleFactor>($scale$$constant);\n+    __ leal($dst$$Register, Address(noreg, $index$$Register, scale, $disp$$constant));\n+  %}\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+instruct leaI_rReg_rReg_immI(rRegI dst, rRegI base, rRegI index, immI disp)\n+%{\n+  predicate(VM_Version::supports_fast_3op_lea());\n+  match(Set dst (AddI (AddI base index) disp));\n+\n+  format %{ \"leal $dst, [$base + $index + $disp]\\t# int\" %}\n+  ins_encode %{\n+    __ leal($dst$$Register, Address($base$$Register, $index$$Register, Address::times_1, $disp$$constant));\n+  %}\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+instruct leaI_rReg_rReg_immI2(rRegI dst, no_rbp_r13_RegI base, rRegI index, immI2 scale)\n+%{\n+  predicate(VM_Version::supports_fast_2op_lea());\n+  match(Set dst (AddI base (LShiftI index scale)));\n+\n+  format %{ \"leal $dst, [$base + $index << $scale]\\t# int\" %}\n+  ins_encode %{\n+    Address::ScaleFactor scale = static_cast<Address::ScaleFactor>($scale$$constant);\n+    __ leal($dst$$Register, Address($base$$Register, $index$$Register, scale));\n+  %}\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+instruct leaI_rReg_rReg_immI2_immI(rRegI dst, rRegI base, rRegI index, immI2 scale, immI disp)\n+%{\n+  predicate(VM_Version::supports_fast_3op_lea());\n+  match(Set dst (AddI (AddI base (LShiftI index scale)) disp));\n+\n+  format %{ \"leal $dst, [$base + $index << $scale + $disp]\\t# int\" %}\n+  ins_encode %{\n+    Address::ScaleFactor scale = static_cast<Address::ScaleFactor>($scale$$constant);\n+    __ leal($dst$$Register, Address($base$$Register, $index$$Register, scale, $disp$$constant));\n+  %}\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+instruct addL_rReg(rRegL dst, rRegL src, rFlagsReg cr)\n+%{\n+  predicate(!UseAPX);\n+  match(Set dst (AddL dst src));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_overflow_flag, PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_carry_flag, PD::Flag_sets_parity_flag);\n+\n+  format %{ \"addq    $dst, $src\\t# long\" %}\n+  ins_encode %{\n+    __ addq($dst$$Register, $src$$Register);\n+  %}\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+instruct addL_rReg_ndd(rRegL dst, rRegL src1, rRegL src2, rFlagsReg cr)\n+%{\n+  predicate(UseAPX);\n+  match(Set dst (AddL src1 src2));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_overflow_flag, PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_carry_flag, PD::Flag_sets_parity_flag);\n+\n+  format %{ \"eaddq    $dst, $src1, $src2\\t# long ndd\" %}\n+  ins_encode %{\n+    __ eaddq($dst$$Register, $src1$$Register, $src2$$Register, false);\n+  %}\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+instruct addL_rReg_imm(rRegL dst, immL32 src, rFlagsReg cr)\n+%{\n+  predicate(!UseAPX);\n+  match(Set dst (AddL dst src));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_overflow_flag, PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_carry_flag, PD::Flag_sets_parity_flag);\n+\n+  format %{ \"addq    $dst, $src\\t# long\" %}\n+  ins_encode %{\n+    __ addq($dst$$Register, $src$$constant);\n+  %}\n+  ins_pipe( ialu_reg );\n+%}\n+\n+instruct addL_rReg_rReg_imm_ndd(rRegL dst, rRegL src1, immL32 src2, rFlagsReg cr)\n+%{\n+  predicate(UseAPX);\n+  match(Set dst (AddL src1 src2));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_overflow_flag, PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_carry_flag, PD::Flag_sets_parity_flag);\n+\n+  format %{ \"eaddq    $dst, $src1, $src2\\t# long ndd\" %}\n+  ins_encode %{\n+    __ eaddq($dst$$Register, $src1$$Register, $src2$$constant, false);\n+  %}\n+  ins_pipe( ialu_reg );\n+%}\n+\n+instruct addL_rReg_mem_imm_ndd(rRegL dst, memory src1, immL32 src2, rFlagsReg cr)\n+%{\n+  predicate(UseAPX);\n+  match(Set dst (AddL (LoadL src1) src2));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_overflow_flag, PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_carry_flag, PD::Flag_sets_parity_flag);\n+\n+  format %{ \"eaddq    $dst, $src1, $src2\\t# long ndd\" %}\n+  ins_encode %{\n+    __ eaddq($dst$$Register, $src1$$Address, $src2$$constant, false);\n+  %}\n+  ins_pipe( ialu_reg );\n+%}\n+\n+instruct addL_rReg_mem(rRegL dst, memory src, rFlagsReg cr)\n+%{\n+  predicate(!UseAPX);\n+  match(Set dst (AddL dst (LoadL src)));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_overflow_flag, PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_carry_flag, PD::Flag_sets_parity_flag);\n+\n+  ins_cost(150); \/\/ XXX\n+  format %{ \"addq    $dst, $src\\t# long\" %}\n+  ins_encode %{\n+    __ addq($dst$$Register, $src$$Address);\n+  %}\n+  ins_pipe(ialu_reg_mem);\n+%}\n+\n+instruct addL_rReg_rReg_mem_ndd(rRegL dst, rRegL src1, memory src2, rFlagsReg cr)\n+%{\n+  predicate(UseAPX);\n+  match(Set dst (AddL src1 (LoadL src2)));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_overflow_flag, PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_carry_flag, PD::Flag_sets_parity_flag);\n+\n+  ins_cost(150);\n+  format %{ \"eaddq    $dst, $src1, $src2\\t# long ndd\" %}\n+  ins_encode %{\n+    __ eaddq($dst$$Register, $src1$$Register, $src2$$Address, false);\n+  %}\n+  ins_pipe(ialu_reg_mem);\n+%}\n+\n+instruct addL_mem_rReg(memory dst, rRegL src, rFlagsReg cr)\n+%{\n+  match(Set dst (StoreL dst (AddL (LoadL dst) src)));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_overflow_flag, PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_carry_flag, PD::Flag_sets_parity_flag);\n+\n+  ins_cost(150); \/\/ XXX\n+  format %{ \"addq    $dst, $src\\t# long\" %}\n+  ins_encode %{\n+    __ addq($dst$$Address, $src$$Register);\n+  %}\n+  ins_pipe(ialu_mem_reg);\n+%}\n+\n+instruct addL_mem_imm(memory dst, immL32 src, rFlagsReg cr)\n+%{\n+  match(Set dst (StoreL dst (AddL (LoadL dst) src)));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_overflow_flag, PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_carry_flag, PD::Flag_sets_parity_flag);\n+\n+  ins_cost(125); \/\/ XXX\n+  format %{ \"addq    $dst, $src\\t# long\" %}\n+  ins_encode %{\n+    __ addq($dst$$Address, $src$$constant);\n+  %}\n+  ins_pipe(ialu_mem_imm);\n+%}\n+\n+instruct incL_rReg(rRegL dst, immL1 src, rFlagsReg cr)\n+%{\n+  predicate(!UseAPX && UseIncDec);\n+  match(Set dst (AddL dst src));\n+  effect(KILL cr);\n+\n+  format %{ \"incq    $dst\\t# long\" %}\n+  ins_encode %{\n+    __ incrementq($dst$$Register);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+instruct incL_rReg_ndd(rRegL dst, rRegI src, immL1 val, rFlagsReg cr)\n+%{\n+  predicate(UseAPX && UseIncDec);\n+  match(Set dst (AddL src val));\n+  effect(KILL cr);\n+\n+  format %{ \"eincq    $dst, $src\\t# long ndd\" %}\n+  ins_encode %{\n+    __ eincq($dst$$Register, $src$$Register, false);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+instruct incL_rReg_mem_ndd(rRegL dst, memory src, immL1 val, rFlagsReg cr)\n+%{\n+  predicate(UseAPX && UseIncDec);\n+  match(Set dst (AddL (LoadL src) val));\n+  effect(KILL cr);\n+\n+  format %{ \"eincq    $dst, $src\\t# long ndd\" %}\n+  ins_encode %{\n+    __ eincq($dst$$Register, $src$$Address, false);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+instruct incL_mem(memory dst, immL1 src, rFlagsReg cr)\n+%{\n+  predicate(UseIncDec);\n+  match(Set dst (StoreL dst (AddL (LoadL dst) src)));\n+  effect(KILL cr);\n+\n+  ins_cost(125); \/\/ XXX\n+  format %{ \"incq    $dst\\t# long\" %}\n+  ins_encode %{\n+    __ incrementq($dst$$Address);\n+  %}\n+  ins_pipe(ialu_mem_imm);\n+%}\n+\n+\/\/ XXX why does that use AddL\n+instruct decL_rReg(rRegL dst, immL_M1 src, rFlagsReg cr)\n+%{\n+  predicate(!UseAPX && UseIncDec);\n+  match(Set dst (AddL dst src));\n+  effect(KILL cr);\n+\n+  format %{ \"decq    $dst\\t# long\" %}\n+  ins_encode %{\n+    __ decrementq($dst$$Register);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+instruct decL_rReg_ndd(rRegL dst, rRegL src, immL_M1 val, rFlagsReg cr)\n+%{\n+  predicate(UseAPX && UseIncDec);\n+  match(Set dst (AddL src val));\n+  effect(KILL cr);\n+\n+  format %{ \"edecq    $dst, $src\\t# long ndd\" %}\n+  ins_encode %{\n+    __ edecq($dst$$Register, $src$$Register, false);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+instruct decL_rReg_mem_ndd(rRegL dst, memory src, immL_M1 val, rFlagsReg cr)\n+%{\n+  predicate(UseAPX && UseIncDec);\n+  match(Set dst (AddL (LoadL src) val));\n+  effect(KILL cr);\n+\n+  format %{ \"edecq    $dst, $src\\t# long ndd\" %}\n+  ins_encode %{\n+    __ edecq($dst$$Register, $src$$Address, false);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+\/\/ XXX why does that use AddL\n+instruct decL_mem(memory dst, immL_M1 src, rFlagsReg cr)\n+%{\n+  predicate(UseIncDec);\n+  match(Set dst (StoreL dst (AddL (LoadL dst) src)));\n+  effect(KILL cr);\n+\n+  ins_cost(125); \/\/ XXX\n+  format %{ \"decq    $dst\\t# long\" %}\n+  ins_encode %{\n+    __ decrementq($dst$$Address);\n+  %}\n+  ins_pipe(ialu_mem_imm);\n+%}\n+\n+instruct leaL_rReg_immI2_immL32(rRegL dst, rRegL index, immI2 scale, immL32 disp)\n+%{\n+  predicate(VM_Version::supports_fast_2op_lea());\n+  match(Set dst (AddL (LShiftL index scale) disp));\n+\n+  format %{ \"leaq $dst, [$index << $scale + $disp]\\t# long\" %}\n+  ins_encode %{\n+    Address::ScaleFactor scale = static_cast<Address::ScaleFactor>($scale$$constant);\n+    __ leaq($dst$$Register, Address(noreg, $index$$Register, scale, $disp$$constant));\n+  %}\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+instruct leaL_rReg_rReg_immL32(rRegL dst, rRegL base, rRegL index, immL32 disp)\n+%{\n+  predicate(VM_Version::supports_fast_3op_lea());\n+  match(Set dst (AddL (AddL base index) disp));\n+\n+  format %{ \"leaq $dst, [$base + $index + $disp]\\t# long\" %}\n+  ins_encode %{\n+    __ leaq($dst$$Register, Address($base$$Register, $index$$Register, Address::times_1, $disp$$constant));\n+  %}\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+instruct leaL_rReg_rReg_immI2(rRegL dst, no_rbp_r13_RegL base, rRegL index, immI2 scale)\n+%{\n+  predicate(VM_Version::supports_fast_2op_lea());\n+  match(Set dst (AddL base (LShiftL index scale)));\n+\n+  format %{ \"leaq $dst, [$base + $index << $scale]\\t# long\" %}\n+  ins_encode %{\n+    Address::ScaleFactor scale = static_cast<Address::ScaleFactor>($scale$$constant);\n+    __ leaq($dst$$Register, Address($base$$Register, $index$$Register, scale));\n+  %}\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+instruct leaL_rReg_rReg_immI2_immL32(rRegL dst, rRegL base, rRegL index, immI2 scale, immL32 disp)\n+%{\n+  predicate(VM_Version::supports_fast_3op_lea());\n+  match(Set dst (AddL (AddL base (LShiftL index scale)) disp));\n+\n+  format %{ \"leaq $dst, [$base + $index << $scale + $disp]\\t# long\" %}\n+  ins_encode %{\n+    Address::ScaleFactor scale = static_cast<Address::ScaleFactor>($scale$$constant);\n+    __ leaq($dst$$Register, Address($base$$Register, $index$$Register, scale, $disp$$constant));\n+  %}\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+instruct addP_rReg(rRegP dst, rRegL src, rFlagsReg cr)\n+%{\n+  match(Set dst (AddP dst src));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_overflow_flag, PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_carry_flag, PD::Flag_sets_parity_flag);\n+\n+  format %{ \"addq    $dst, $src\\t# ptr\" %}\n+  ins_encode %{\n+    __ addq($dst$$Register, $src$$Register);\n+  %}\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+instruct addP_rReg_imm(rRegP dst, immL32 src, rFlagsReg cr)\n+%{\n+  match(Set dst (AddP dst src));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_overflow_flag, PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_carry_flag, PD::Flag_sets_parity_flag);\n+\n+  format %{ \"addq    $dst, $src\\t# ptr\" %}\n+  ins_encode %{\n+    __ addq($dst$$Register, $src$$constant);\n+  %}\n+  ins_pipe( ialu_reg );\n+%}\n+\n+\/\/ XXX addP mem ops ????\n+\n+instruct checkCastPP(rRegP dst)\n+%{\n+  match(Set dst (CheckCastPP dst));\n+\n+  size(0);\n+  format %{ \"# checkcastPP of $dst\" %}\n+  ins_encode(\/* empty encoding *\/);\n+  ins_pipe(empty);\n+%}\n+\n+instruct castPP(rRegP dst)\n+%{\n+  match(Set dst (CastPP dst));\n+\n+  size(0);\n+  format %{ \"# castPP of $dst\" %}\n+  ins_encode(\/* empty encoding *\/);\n+  ins_pipe(empty);\n+%}\n+\n+instruct castII(rRegI dst)\n+%{\n+  predicate(VerifyConstraintCasts == 0);\n+  match(Set dst (CastII dst));\n+\n+  size(0);\n+  format %{ \"# castII of $dst\" %}\n+  ins_encode(\/* empty encoding *\/);\n+  ins_cost(0);\n+  ins_pipe(empty);\n+%}\n+\n+instruct castII_checked(rRegI dst, rFlagsReg cr)\n+%{\n+  predicate(VerifyConstraintCasts > 0);\n+  match(Set dst (CastII dst));\n+\n+  effect(KILL cr);\n+  format %{ \"# cast_checked_II $dst\" %}\n+  ins_encode %{\n+    __ verify_int_in_range(_idx, bottom_type()->is_int(), $dst$$Register);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct castLL(rRegL dst)\n+%{\n+  predicate(VerifyConstraintCasts == 0);\n+  match(Set dst (CastLL dst));\n+\n+  size(0);\n+  format %{ \"# castLL of $dst\" %}\n+  ins_encode(\/* empty encoding *\/);\n+  ins_cost(0);\n+  ins_pipe(empty);\n+%}\n+\n+instruct castLL_checked_L32(rRegL dst, rFlagsReg cr)\n+%{\n+  predicate(VerifyConstraintCasts > 0 && castLL_is_imm32(n));\n+  match(Set dst (CastLL dst));\n+\n+  effect(KILL cr);\n+  format %{ \"# cast_checked_LL $dst\" %}\n+  ins_encode %{\n+    __ verify_long_in_range(_idx, bottom_type()->is_long(), $dst$$Register, noreg);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct castLL_checked(rRegL dst, rRegL tmp, rFlagsReg cr)\n+%{\n+  predicate(VerifyConstraintCasts > 0 && !castLL_is_imm32(n));\n+  match(Set dst (CastLL dst));\n+\n+  effect(KILL cr, TEMP tmp);\n+  format %{ \"# cast_checked_LL $dst\\tusing $tmp as TEMP\" %}\n+  ins_encode %{\n+    __ verify_long_in_range(_idx, bottom_type()->is_long(), $dst$$Register, $tmp$$Register);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct castFF(regF dst)\n+%{\n+  match(Set dst (CastFF dst));\n+\n+  size(0);\n+  format %{ \"# castFF of $dst\" %}\n+  ins_encode(\/* empty encoding *\/);\n+  ins_cost(0);\n+  ins_pipe(empty);\n+%}\n+\n+instruct castHH(regF dst)\n+%{\n+  match(Set dst (CastHH dst));\n+\n+  size(0);\n+  format %{ \"# castHH of $dst\" %}\n+  ins_encode(\/* empty encoding *\/);\n+  ins_cost(0);\n+  ins_pipe(empty);\n+%}\n+\n+instruct castDD(regD dst)\n+%{\n+  match(Set dst (CastDD dst));\n+\n+  size(0);\n+  format %{ \"# castDD of $dst\" %}\n+  ins_encode(\/* empty encoding *\/);\n+  ins_cost(0);\n+  ins_pipe(empty);\n+%}\n+\n+\/\/ XXX No flag versions for CompareAndSwap{P,I,L} because matcher can't match them\n+instruct compareAndSwapP(rRegI res,\n+                         memory mem_ptr,\n+                         rax_RegP oldval, rRegP newval,\n+                         rFlagsReg cr)\n+%{\n+  predicate(n->as_LoadStore()->barrier_data() == 0);\n+  match(Set res (CompareAndSwapP mem_ptr (Binary oldval newval)));\n+  match(Set res (WeakCompareAndSwapP mem_ptr (Binary oldval newval)));\n+  effect(KILL cr, KILL oldval);\n+\n+  format %{ \"cmpxchgq $mem_ptr,$newval\\t# \"\n+            \"If rax == $mem_ptr then store $newval into $mem_ptr\\n\\t\"\n+            \"setcc $res \\t# emits sete + movzbl or setzue for APX\" %}\n+  ins_encode %{\n+    __ lock();\n+    __ cmpxchgq($newval$$Register, $mem_ptr$$Address);\n+    __ setcc(Assembler::equal, $res$$Register);\n+  %}\n+  ins_pipe( pipe_cmpxchg );\n+%}\n+\n+instruct compareAndSwapL(rRegI res,\n+                         memory mem_ptr,\n+                         rax_RegL oldval, rRegL newval,\n+                         rFlagsReg cr)\n+%{\n+  match(Set res (CompareAndSwapL mem_ptr (Binary oldval newval)));\n+  match(Set res (WeakCompareAndSwapL mem_ptr (Binary oldval newval)));\n+  effect(KILL cr, KILL oldval);\n+\n+  format %{ \"cmpxchgq $mem_ptr,$newval\\t# \"\n+            \"If rax == $mem_ptr then store $newval into $mem_ptr\\n\\t\"\n+            \"setcc $res \\t# emits sete + movzbl or setzue for APX\" %}\n+  ins_encode %{\n+    __ lock();\n+    __ cmpxchgq($newval$$Register, $mem_ptr$$Address);\n+    __ setcc(Assembler::equal, $res$$Register);\n+  %}\n+  ins_pipe( pipe_cmpxchg );\n+%}\n+\n+instruct compareAndSwapI(rRegI res,\n+                         memory mem_ptr,\n+                         rax_RegI oldval, rRegI newval,\n+                         rFlagsReg cr)\n+%{\n+  match(Set res (CompareAndSwapI mem_ptr (Binary oldval newval)));\n+  match(Set res (WeakCompareAndSwapI mem_ptr (Binary oldval newval)));\n+  effect(KILL cr, KILL oldval);\n+\n+  format %{ \"cmpxchgl $mem_ptr,$newval\\t# \"\n+            \"If rax == $mem_ptr then store $newval into $mem_ptr\\n\\t\"\n+            \"setcc $res \\t# emits sete + movzbl or setzue for APX\" %}\n+  ins_encode %{\n+    __ lock();\n+    __ cmpxchgl($newval$$Register, $mem_ptr$$Address);\n+    __ setcc(Assembler::equal, $res$$Register);\n+  %}\n+  ins_pipe( pipe_cmpxchg );\n+%}\n+\n+instruct compareAndSwapB(rRegI res,\n+                         memory mem_ptr,\n+                         rax_RegI oldval, rRegI newval,\n+                         rFlagsReg cr)\n+%{\n+  match(Set res (CompareAndSwapB mem_ptr (Binary oldval newval)));\n+  match(Set res (WeakCompareAndSwapB mem_ptr (Binary oldval newval)));\n+  effect(KILL cr, KILL oldval);\n+\n+  format %{ \"cmpxchgb $mem_ptr,$newval\\t# \"\n+            \"If rax == $mem_ptr then store $newval into $mem_ptr\\n\\t\"\n+            \"setcc $res \\t# emits sete + movzbl or setzue for APX\" %}\n+  ins_encode %{\n+    __ lock();\n+    __ cmpxchgb($newval$$Register, $mem_ptr$$Address);\n+    __ setcc(Assembler::equal, $res$$Register);\n+  %}\n+  ins_pipe( pipe_cmpxchg );\n+%}\n+\n+instruct compareAndSwapS(rRegI res,\n+                         memory mem_ptr,\n+                         rax_RegI oldval, rRegI newval,\n+                         rFlagsReg cr)\n+%{\n+  match(Set res (CompareAndSwapS mem_ptr (Binary oldval newval)));\n+  match(Set res (WeakCompareAndSwapS mem_ptr (Binary oldval newval)));\n+  effect(KILL cr, KILL oldval);\n+\n+  format %{ \"cmpxchgw $mem_ptr,$newval\\t# \"\n+            \"If rax == $mem_ptr then store $newval into $mem_ptr\\n\\t\"\n+            \"setcc $res \\t# emits sete + movzbl or setzue for APX\" %}\n+  ins_encode %{\n+    __ lock();\n+    __ cmpxchgw($newval$$Register, $mem_ptr$$Address);\n+    __ setcc(Assembler::equal, $res$$Register);\n+  %}\n+  ins_pipe( pipe_cmpxchg );\n+%}\n+\n+instruct compareAndSwapN(rRegI res,\n+                          memory mem_ptr,\n+                          rax_RegN oldval, rRegN newval,\n+                          rFlagsReg cr) %{\n+  predicate(n->as_LoadStore()->barrier_data() == 0);\n+  match(Set res (CompareAndSwapN mem_ptr (Binary oldval newval)));\n+  match(Set res (WeakCompareAndSwapN mem_ptr (Binary oldval newval)));\n+  effect(KILL cr, KILL oldval);\n+\n+  format %{ \"cmpxchgl $mem_ptr,$newval\\t# \"\n+            \"If rax == $mem_ptr then store $newval into $mem_ptr\\n\\t\"\n+            \"setcc $res \\t# emits sete + movzbl or setzue for APX\" %}\n+  ins_encode %{\n+    __ lock();\n+    __ cmpxchgl($newval$$Register, $mem_ptr$$Address);\n+    __ setcc(Assembler::equal, $res$$Register);\n+  %}\n+  ins_pipe( pipe_cmpxchg );\n+%}\n+\n+instruct compareAndExchangeB(\n+                         memory mem_ptr,\n+                         rax_RegI oldval, rRegI newval,\n+                         rFlagsReg cr)\n+%{\n+  match(Set oldval (CompareAndExchangeB mem_ptr (Binary oldval newval)));\n+  effect(KILL cr);\n+\n+  format %{ \"cmpxchgb $mem_ptr,$newval\\t# \"\n+            \"If rax == $mem_ptr then store $newval into $mem_ptr\\n\\t\"  %}\n+  ins_encode %{\n+    __ lock();\n+    __ cmpxchgb($newval$$Register, $mem_ptr$$Address);\n+  %}\n+  ins_pipe( pipe_cmpxchg );\n+%}\n+\n+instruct compareAndExchangeS(\n+                         memory mem_ptr,\n+                         rax_RegI oldval, rRegI newval,\n+                         rFlagsReg cr)\n+%{\n+  match(Set oldval (CompareAndExchangeS mem_ptr (Binary oldval newval)));\n+  effect(KILL cr);\n+\n+  format %{ \"cmpxchgw $mem_ptr,$newval\\t# \"\n+            \"If rax == $mem_ptr then store $newval into $mem_ptr\\n\\t\"  %}\n+  ins_encode %{\n+    __ lock();\n+    __ cmpxchgw($newval$$Register, $mem_ptr$$Address);\n+  %}\n+  ins_pipe( pipe_cmpxchg );\n+%}\n+\n+instruct compareAndExchangeI(\n+                         memory mem_ptr,\n+                         rax_RegI oldval, rRegI newval,\n+                         rFlagsReg cr)\n+%{\n+  match(Set oldval (CompareAndExchangeI mem_ptr (Binary oldval newval)));\n+  effect(KILL cr);\n+\n+  format %{ \"cmpxchgl $mem_ptr,$newval\\t# \"\n+            \"If rax == $mem_ptr then store $newval into $mem_ptr\\n\\t\"  %}\n+  ins_encode %{\n+    __ lock();\n+    __ cmpxchgl($newval$$Register, $mem_ptr$$Address);\n+  %}\n+  ins_pipe( pipe_cmpxchg );\n+%}\n+\n+instruct compareAndExchangeL(\n+                         memory mem_ptr,\n+                         rax_RegL oldval, rRegL newval,\n+                         rFlagsReg cr)\n+%{\n+  match(Set oldval (CompareAndExchangeL mem_ptr (Binary oldval newval)));\n+  effect(KILL cr);\n+\n+  format %{ \"cmpxchgq $mem_ptr,$newval\\t# \"\n+            \"If rax == $mem_ptr then store $newval into $mem_ptr\\n\\t\"  %}\n+  ins_encode %{\n+    __ lock();\n+    __ cmpxchgq($newval$$Register, $mem_ptr$$Address);\n+  %}\n+  ins_pipe( pipe_cmpxchg );\n+%}\n+\n+instruct compareAndExchangeN(\n+                          memory mem_ptr,\n+                          rax_RegN oldval, rRegN newval,\n+                          rFlagsReg cr) %{\n+  predicate(n->as_LoadStore()->barrier_data() == 0);\n+  match(Set oldval (CompareAndExchangeN mem_ptr (Binary oldval newval)));\n+  effect(KILL cr);\n+\n+  format %{ \"cmpxchgl $mem_ptr,$newval\\t# \"\n+            \"If rax == $mem_ptr then store $newval into $mem_ptr\\n\\t\" %}\n+  ins_encode %{\n+    __ lock();\n+    __ cmpxchgl($newval$$Register, $mem_ptr$$Address);\n+  %}\n+  ins_pipe( pipe_cmpxchg );\n+%}\n+\n+instruct compareAndExchangeP(\n+                         memory mem_ptr,\n+                         rax_RegP oldval, rRegP newval,\n+                         rFlagsReg cr)\n+%{\n+  predicate(n->as_LoadStore()->barrier_data() == 0);\n+  match(Set oldval (CompareAndExchangeP mem_ptr (Binary oldval newval)));\n+  effect(KILL cr);\n+\n+  format %{ \"cmpxchgq $mem_ptr,$newval\\t# \"\n+            \"If rax == $mem_ptr then store $newval into $mem_ptr\\n\\t\" %}\n+  ins_encode %{\n+    __ lock();\n+    __ cmpxchgq($newval$$Register, $mem_ptr$$Address);\n+  %}\n+  ins_pipe( pipe_cmpxchg );\n+%}\n+\n+instruct xaddB_reg_no_res(memory mem, Universe dummy, rRegI add, rFlagsReg cr) %{\n+  predicate(n->as_LoadStore()->result_not_used());\n+  match(Set dummy (GetAndAddB mem add));\n+  effect(KILL cr);\n+  format %{ \"addb_lock   $mem, $add\" %}\n+  ins_encode %{\n+    __ lock();\n+    __ addb($mem$$Address, $add$$Register);\n+  %}\n+  ins_pipe(pipe_cmpxchg);\n+%}\n+\n+instruct xaddB_imm_no_res(memory mem, Universe dummy, immI add, rFlagsReg cr) %{\n+  predicate(n->as_LoadStore()->result_not_used());\n+  match(Set dummy (GetAndAddB mem add));\n+  effect(KILL cr);\n+  format %{ \"addb_lock   $mem, $add\" %}\n+  ins_encode %{\n+    __ lock();\n+    __ addb($mem$$Address, $add$$constant);\n+  %}\n+  ins_pipe(pipe_cmpxchg);\n+%}\n+\n+instruct xaddB(memory mem, rRegI newval, rFlagsReg cr) %{\n+  predicate(!n->as_LoadStore()->result_not_used());\n+  match(Set newval (GetAndAddB mem newval));\n+  effect(KILL cr);\n+  format %{ \"xaddb_lock  $mem, $newval\" %}\n+  ins_encode %{\n+    __ lock();\n+    __ xaddb($mem$$Address, $newval$$Register);\n+  %}\n+  ins_pipe(pipe_cmpxchg);\n+%}\n+\n+instruct xaddS_reg_no_res(memory mem, Universe dummy, rRegI add, rFlagsReg cr) %{\n+  predicate(n->as_LoadStore()->result_not_used());\n+  match(Set dummy (GetAndAddS mem add));\n+  effect(KILL cr);\n+  format %{ \"addw_lock   $mem, $add\" %}\n+  ins_encode %{\n+    __ lock();\n+    __ addw($mem$$Address, $add$$Register);\n+  %}\n+  ins_pipe(pipe_cmpxchg);\n+%}\n+\n+instruct xaddS_imm_no_res(memory mem, Universe dummy, immI add, rFlagsReg cr) %{\n+  predicate(UseStoreImmI16 && n->as_LoadStore()->result_not_used());\n+  match(Set dummy (GetAndAddS mem add));\n+  effect(KILL cr);\n+  format %{ \"addw_lock   $mem, $add\" %}\n+  ins_encode %{\n+    __ lock();\n+    __ addw($mem$$Address, $add$$constant);\n+  %}\n+  ins_pipe(pipe_cmpxchg);\n+%}\n+\n+instruct xaddS(memory mem, rRegI newval, rFlagsReg cr) %{\n+  predicate(!n->as_LoadStore()->result_not_used());\n+  match(Set newval (GetAndAddS mem newval));\n+  effect(KILL cr);\n+  format %{ \"xaddw_lock  $mem, $newval\" %}\n+  ins_encode %{\n+    __ lock();\n+    __ xaddw($mem$$Address, $newval$$Register);\n+  %}\n+  ins_pipe(pipe_cmpxchg);\n+%}\n+\n+instruct xaddI_reg_no_res(memory mem, Universe dummy, rRegI add, rFlagsReg cr) %{\n+  predicate(n->as_LoadStore()->result_not_used());\n+  match(Set dummy (GetAndAddI mem add));\n+  effect(KILL cr);\n+  format %{ \"addl_lock   $mem, $add\" %}\n+  ins_encode %{\n+    __ lock();\n+    __ addl($mem$$Address, $add$$Register);\n+  %}\n+  ins_pipe(pipe_cmpxchg);\n+%}\n+\n+instruct xaddI_imm_no_res(memory mem, Universe dummy, immI add, rFlagsReg cr) %{\n+  predicate(n->as_LoadStore()->result_not_used());\n+  match(Set dummy (GetAndAddI mem add));\n+  effect(KILL cr);\n+  format %{ \"addl_lock   $mem, $add\" %}\n+  ins_encode %{\n+    __ lock();\n+    __ addl($mem$$Address, $add$$constant);\n+  %}\n+  ins_pipe(pipe_cmpxchg);\n+%}\n+\n+instruct xaddI(memory mem, rRegI newval, rFlagsReg cr) %{\n+  predicate(!n->as_LoadStore()->result_not_used());\n+  match(Set newval (GetAndAddI mem newval));\n+  effect(KILL cr);\n+  format %{ \"xaddl_lock  $mem, $newval\" %}\n+  ins_encode %{\n+    __ lock();\n+    __ xaddl($mem$$Address, $newval$$Register);\n+  %}\n+  ins_pipe(pipe_cmpxchg);\n+%}\n+\n+instruct xaddL_reg_no_res(memory mem, Universe dummy, rRegL add, rFlagsReg cr) %{\n+  predicate(n->as_LoadStore()->result_not_used());\n+  match(Set dummy (GetAndAddL mem add));\n+  effect(KILL cr);\n+  format %{ \"addq_lock   $mem, $add\" %}\n+  ins_encode %{\n+    __ lock();\n+    __ addq($mem$$Address, $add$$Register);\n+  %}\n+  ins_pipe(pipe_cmpxchg);\n+%}\n+\n+instruct xaddL_imm_no_res(memory mem, Universe dummy, immL32 add, rFlagsReg cr) %{\n+  predicate(n->as_LoadStore()->result_not_used());\n+  match(Set dummy (GetAndAddL mem add));\n+  effect(KILL cr);\n+  format %{ \"addq_lock   $mem, $add\" %}\n+  ins_encode %{\n+    __ lock();\n+    __ addq($mem$$Address, $add$$constant);\n+  %}\n+  ins_pipe(pipe_cmpxchg);\n+%}\n+\n+instruct xaddL(memory mem, rRegL newval, rFlagsReg cr) %{\n+  predicate(!n->as_LoadStore()->result_not_used());\n+  match(Set newval (GetAndAddL mem newval));\n+  effect(KILL cr);\n+  format %{ \"xaddq_lock  $mem, $newval\" %}\n+  ins_encode %{\n+    __ lock();\n+    __ xaddq($mem$$Address, $newval$$Register);\n+  %}\n+  ins_pipe(pipe_cmpxchg);\n+%}\n+\n+instruct xchgB( memory mem, rRegI newval) %{\n+  match(Set newval (GetAndSetB mem newval));\n+  format %{ \"XCHGB  $newval,[$mem]\" %}\n+  ins_encode %{\n+    __ xchgb($newval$$Register, $mem$$Address);\n+  %}\n+  ins_pipe( pipe_cmpxchg );\n+%}\n+\n+instruct xchgS( memory mem, rRegI newval) %{\n+  match(Set newval (GetAndSetS mem newval));\n+  format %{ \"XCHGW  $newval,[$mem]\" %}\n+  ins_encode %{\n+    __ xchgw($newval$$Register, $mem$$Address);\n+  %}\n+  ins_pipe( pipe_cmpxchg );\n+%}\n+\n+instruct xchgI( memory mem, rRegI newval) %{\n+  match(Set newval (GetAndSetI mem newval));\n+  format %{ \"XCHGL  $newval,[$mem]\" %}\n+  ins_encode %{\n+    __ xchgl($newval$$Register, $mem$$Address);\n+  %}\n+  ins_pipe( pipe_cmpxchg );\n+%}\n+\n+instruct xchgL( memory mem, rRegL newval) %{\n+  match(Set newval (GetAndSetL mem newval));\n+  format %{ \"XCHGL  $newval,[$mem]\" %}\n+  ins_encode %{\n+    __ xchgq($newval$$Register, $mem$$Address);\n+  %}\n+  ins_pipe( pipe_cmpxchg );\n+%}\n+\n+instruct xchgP( memory mem, rRegP newval) %{\n+  match(Set newval (GetAndSetP mem newval));\n+  predicate(n->as_LoadStore()->barrier_data() == 0);\n+  format %{ \"XCHGQ  $newval,[$mem]\" %}\n+  ins_encode %{\n+    __ xchgq($newval$$Register, $mem$$Address);\n+  %}\n+  ins_pipe( pipe_cmpxchg );\n+%}\n+\n+instruct xchgN( memory mem, rRegN newval) %{\n+  predicate(n->as_LoadStore()->barrier_data() == 0);\n+  match(Set newval (GetAndSetN mem newval));\n+  format %{ \"XCHGL  $newval,$mem]\" %}\n+  ins_encode %{\n+    __ xchgl($newval$$Register, $mem$$Address);\n+  %}\n+  ins_pipe( pipe_cmpxchg );\n+%}\n+\n+\/\/----------Abs Instructions-------------------------------------------\n+\n+\/\/ Integer Absolute Instructions\n+instruct absI_rReg(rRegI dst, rRegI src, rFlagsReg cr)\n+%{\n+  match(Set dst (AbsI src));\n+  effect(TEMP dst, KILL cr);\n+  format %{ \"xorl    $dst, $dst\\t# abs int\\n\\t\"\n+            \"subl    $dst, $src\\n\\t\"\n+            \"cmovll  $dst, $src\" %}\n+  ins_encode %{\n+    __ xorl($dst$$Register, $dst$$Register);\n+    __ subl($dst$$Register, $src$$Register);\n+    __ cmovl(Assembler::less, $dst$$Register, $src$$Register);\n+  %}\n+\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+\/\/ Long Absolute Instructions\n+instruct absL_rReg(rRegL dst, rRegL src, rFlagsReg cr)\n+%{\n+  match(Set dst (AbsL src));\n+  effect(TEMP dst, KILL cr);\n+  format %{ \"xorl    $dst, $dst\\t# abs long\\n\\t\"\n+            \"subq    $dst, $src\\n\\t\"\n+            \"cmovlq  $dst, $src\" %}\n+  ins_encode %{\n+    __ xorl($dst$$Register, $dst$$Register);\n+    __ subq($dst$$Register, $src$$Register);\n+    __ cmovq(Assembler::less, $dst$$Register, $src$$Register);\n+  %}\n+\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+\/\/----------Subtraction Instructions-------------------------------------------\n+\n+\/\/ Integer Subtraction Instructions\n+instruct subI_rReg(rRegI dst, rRegI src, rFlagsReg cr)\n+%{\n+  predicate(!UseAPX);\n+  match(Set dst (SubI dst src));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_overflow_flag, PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_carry_flag, PD::Flag_sets_parity_flag);\n+\n+  format %{ \"subl    $dst, $src\\t# int\" %}\n+  ins_encode %{\n+    __ subl($dst$$Register, $src$$Register);\n+  %}\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+instruct subI_rReg_ndd(rRegI dst, rRegI src1, rRegI src2, rFlagsReg cr)\n+%{\n+  predicate(UseAPX);\n+  match(Set dst (SubI src1 src2));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_overflow_flag, PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_carry_flag, PD::Flag_sets_parity_flag);\n+\n+  format %{ \"esubl    $dst, $src1, $src2\\t# int ndd\" %}\n+  ins_encode %{\n+    __ esubl($dst$$Register, $src1$$Register, $src2$$Register, false);\n+  %}\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+instruct subI_rReg_rReg_imm_ndd(rRegI dst, rRegI src1, immI src2, rFlagsReg cr)\n+%{\n+  predicate(UseAPX);\n+  match(Set dst (SubI src1 src2));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_overflow_flag, PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_carry_flag, PD::Flag_sets_parity_flag);\n+\n+  format %{ \"esubl    $dst, $src1, $src2\\t# int ndd\" %}\n+  ins_encode %{\n+    __ esubl($dst$$Register, $src1$$Register, $src2$$constant, false);\n+  %}\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+instruct subI_rReg_mem_imm_ndd(rRegI dst, memory src1, immI src2, rFlagsReg cr)\n+%{\n+  predicate(UseAPX);\n+  match(Set dst (SubI (LoadI src1) src2));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_overflow_flag, PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_carry_flag, PD::Flag_sets_parity_flag);\n+\n+  format %{ \"esubl    $dst, $src1, $src2\\t# int ndd\" %}\n+  ins_encode %{\n+    __ esubl($dst$$Register, $src1$$Address, $src2$$constant, false);\n+  %}\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+instruct subI_rReg_mem(rRegI dst, memory src, rFlagsReg cr)\n+%{\n+  predicate(!UseAPX);\n+  match(Set dst (SubI dst (LoadI src)));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_overflow_flag, PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_carry_flag, PD::Flag_sets_parity_flag);\n+\n+  ins_cost(150);\n+  format %{ \"subl    $dst, $src\\t# int\" %}\n+  ins_encode %{\n+    __ subl($dst$$Register, $src$$Address);\n+  %}\n+  ins_pipe(ialu_reg_mem);\n+%}\n+\n+instruct subI_rReg_rReg_mem_ndd(rRegI dst, rRegI src1, memory src2, rFlagsReg cr)\n+%{\n+  predicate(UseAPX);\n+  match(Set dst (SubI src1 (LoadI src2)));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_overflow_flag, PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_carry_flag, PD::Flag_sets_parity_flag);\n+\n+  ins_cost(150);\n+  format %{ \"esubl    $dst, $src1, $src2\\t# int ndd\" %}\n+  ins_encode %{\n+    __ esubl($dst$$Register, $src1$$Register, $src2$$Address, false);\n+  %}\n+  ins_pipe(ialu_reg_mem);\n+%}\n+\n+instruct subI_rReg_mem_rReg_ndd(rRegI dst, memory src1, rRegI src2, rFlagsReg cr)\n+%{\n+  predicate(UseAPX);\n+  match(Set dst (SubI (LoadI src1) src2));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_overflow_flag, PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_carry_flag, PD::Flag_sets_parity_flag);\n+\n+  ins_cost(150);\n+  format %{ \"esubl    $dst, $src1, $src2\\t# int ndd\" %}\n+  ins_encode %{\n+    __ esubl($dst$$Register, $src1$$Address, $src2$$Register, false);\n+  %}\n+  ins_pipe(ialu_reg_mem);\n+%}\n+\n+instruct subI_mem_rReg(memory dst, rRegI src, rFlagsReg cr)\n+%{\n+  match(Set dst (StoreI dst (SubI (LoadI dst) src)));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_overflow_flag, PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_carry_flag, PD::Flag_sets_parity_flag);\n+\n+  ins_cost(150);\n+  format %{ \"subl    $dst, $src\\t# int\" %}\n+  ins_encode %{\n+    __ subl($dst$$Address, $src$$Register);\n+  %}\n+  ins_pipe(ialu_mem_reg);\n+%}\n+\n+instruct subL_rReg(rRegL dst, rRegL src, rFlagsReg cr)\n+%{\n+  predicate(!UseAPX);\n+  match(Set dst (SubL dst src));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_overflow_flag, PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_carry_flag, PD::Flag_sets_parity_flag);\n+\n+  format %{ \"subq    $dst, $src\\t# long\" %}\n+  ins_encode %{\n+    __ subq($dst$$Register, $src$$Register);\n+  %}\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+instruct subL_rReg_ndd(rRegL dst, rRegL src1, rRegL src2, rFlagsReg cr)\n+%{\n+  predicate(UseAPX);\n+  match(Set dst (SubL src1 src2));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_overflow_flag, PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_carry_flag, PD::Flag_sets_parity_flag);\n+\n+  format %{ \"esubq    $dst, $src1, $src2\\t# long ndd\" %}\n+  ins_encode %{\n+    __ esubq($dst$$Register, $src1$$Register, $src2$$Register, false);\n+  %}\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+instruct subL_rReg_rReg_imm_ndd(rRegL dst, rRegL src1, immL32 src2, rFlagsReg cr)\n+%{\n+  predicate(UseAPX);\n+  match(Set dst (SubL src1 src2));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_overflow_flag, PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_carry_flag, PD::Flag_sets_parity_flag);\n+\n+  format %{ \"esubq    $dst, $src1, $src2\\t# long ndd\" %}\n+  ins_encode %{\n+    __ esubq($dst$$Register, $src1$$Register, $src2$$constant, false);\n+  %}\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+instruct subL_rReg_mem_imm_ndd(rRegL dst, memory src1, immL32 src2, rFlagsReg cr)\n+%{\n+  predicate(UseAPX);\n+  match(Set dst (SubL (LoadL src1) src2));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_overflow_flag, PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_carry_flag, PD::Flag_sets_parity_flag);\n+\n+  format %{ \"esubq    $dst, $src1, $src2\\t# long ndd\" %}\n+  ins_encode %{\n+    __ esubq($dst$$Register, $src1$$Address, $src2$$constant, false);\n+  %}\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+instruct subL_rReg_mem(rRegL dst, memory src, rFlagsReg cr)\n+%{\n+  predicate(!UseAPX);\n+  match(Set dst (SubL dst (LoadL src)));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_overflow_flag, PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_carry_flag, PD::Flag_sets_parity_flag);\n+\n+  ins_cost(150);\n+  format %{ \"subq    $dst, $src\\t# long\" %}\n+  ins_encode %{\n+    __ subq($dst$$Register, $src$$Address);\n+  %}\n+  ins_pipe(ialu_reg_mem);\n+%}\n+\n+instruct subL_rReg_rReg_mem_ndd(rRegL dst, rRegL src1, memory src2, rFlagsReg cr)\n+%{\n+  predicate(UseAPX);\n+  match(Set dst (SubL src1 (LoadL src2)));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_overflow_flag, PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_carry_flag, PD::Flag_sets_parity_flag);\n+\n+  ins_cost(150);\n+  format %{ \"esubq    $dst, $src1, $src2\\t# long ndd\" %}\n+  ins_encode %{\n+    __ esubq($dst$$Register, $src1$$Register, $src2$$Address, false);\n+  %}\n+  ins_pipe(ialu_reg_mem);\n+%}\n+\n+instruct subL_rReg_mem_rReg_ndd(rRegL dst, memory src1, rRegL src2, rFlagsReg cr)\n+%{\n+  predicate(UseAPX);\n+  match(Set dst (SubL (LoadL src1) src2));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_overflow_flag, PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_carry_flag, PD::Flag_sets_parity_flag);\n+\n+  ins_cost(150);\n+  format %{ \"esubq    $dst, $src1, $src2\\t# long ndd\" %}\n+  ins_encode %{\n+    __ esubq($dst$$Register, $src1$$Address, $src2$$Register, false);\n+  %}\n+  ins_pipe(ialu_reg_mem);\n+%}\n+\n+instruct subL_mem_rReg(memory dst, rRegL src, rFlagsReg cr)\n+%{\n+  match(Set dst (StoreL dst (SubL (LoadL dst) src)));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_overflow_flag, PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_carry_flag, PD::Flag_sets_parity_flag);\n+\n+  ins_cost(150);\n+  format %{ \"subq    $dst, $src\\t# long\" %}\n+  ins_encode %{\n+    __ subq($dst$$Address, $src$$Register);\n+  %}\n+  ins_pipe(ialu_mem_reg);\n+%}\n+\n+\/\/ Subtract from a pointer\n+\/\/ XXX hmpf???\n+instruct subP_rReg(rRegP dst, rRegI src, immI_0 zero, rFlagsReg cr)\n+%{\n+  match(Set dst (AddP dst (SubI zero src)));\n+  effect(KILL cr);\n+\n+  format %{ \"subq    $dst, $src\\t# ptr - int\" %}\n+  ins_encode %{\n+    __ subq($dst$$Register, $src$$Register);\n+  %}\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+instruct negI_rReg(rRegI dst, immI_0 zero, rFlagsReg cr)\n+%{\n+  predicate(!UseAPX);\n+  match(Set dst (SubI zero dst));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_overflow_flag, PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag);\n+\n+  format %{ \"negl    $dst\\t# int\" %}\n+  ins_encode %{\n+    __ negl($dst$$Register);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+instruct negI_rReg_ndd(rRegI dst, rRegI src, immI_0 zero, rFlagsReg cr)\n+%{\n+  predicate(UseAPX);\n+  match(Set dst (SubI zero src));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_overflow_flag, PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag);\n+\n+  format %{ \"enegl    $dst, $src\\t# int ndd\" %}\n+  ins_encode %{\n+    __ enegl($dst$$Register, $src$$Register, false);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+instruct negI_rReg_2(rRegI dst, rFlagsReg cr)\n+%{\n+  predicate(!UseAPX);\n+  match(Set dst (NegI dst));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_overflow_flag, PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag);\n+\n+  format %{ \"negl    $dst\\t# int\" %}\n+  ins_encode %{\n+    __ negl($dst$$Register);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+instruct negI_rReg_2_ndd(rRegI dst, rRegI src, rFlagsReg cr)\n+%{\n+  predicate(UseAPX);\n+  match(Set dst (NegI src));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_overflow_flag, PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag);\n+\n+  format %{ \"enegl    $dst, $src\\t# int ndd\" %}\n+  ins_encode %{\n+    __ enegl($dst$$Register, $src$$Register, false);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+instruct negI_mem(memory dst, immI_0 zero, rFlagsReg cr)\n+%{\n+  match(Set dst (StoreI dst (SubI zero (LoadI dst))));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_overflow_flag, PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag);\n+\n+  format %{ \"negl    $dst\\t# int\" %}\n+  ins_encode %{\n+    __ negl($dst$$Address);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+instruct negL_rReg(rRegL dst, immL0 zero, rFlagsReg cr)\n+%{\n+  predicate(!UseAPX);\n+  match(Set dst (SubL zero dst));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_overflow_flag, PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag);\n+\n+  format %{ \"negq    $dst\\t# long\" %}\n+  ins_encode %{\n+    __ negq($dst$$Register);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+instruct negL_rReg_ndd(rRegL dst, rRegL src, immL0 zero, rFlagsReg cr)\n+%{\n+  predicate(UseAPX);\n+  match(Set dst (SubL zero src));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_overflow_flag, PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag);\n+\n+  format %{ \"enegq    $dst, $src\\t# long ndd\" %}\n+  ins_encode %{\n+    __ enegq($dst$$Register, $src$$Register, false);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+instruct negL_rReg_2(rRegL dst, rFlagsReg cr)\n+%{\n+  predicate(!UseAPX);\n+  match(Set dst (NegL dst));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_overflow_flag, PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag);\n+\n+  format %{ \"negq    $dst\\t# int\" %}\n+  ins_encode %{\n+    __ negq($dst$$Register);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+instruct negL_rReg_2_ndd(rRegL dst, rRegL src, rFlagsReg cr)\n+%{\n+  predicate(UseAPX);\n+  match(Set dst (NegL src));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_overflow_flag, PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag);\n+\n+  format %{ \"enegq    $dst, $src\\t# long ndd\" %}\n+  ins_encode %{\n+    __ enegq($dst$$Register, $src$$Register, false);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+instruct negL_mem(memory dst, immL0 zero, rFlagsReg cr)\n+%{\n+  match(Set dst (StoreL dst (SubL zero (LoadL dst))));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_overflow_flag, PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag);\n+\n+  format %{ \"negq    $dst\\t# long\" %}\n+  ins_encode %{\n+    __ negq($dst$$Address);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+\/\/----------Multiplication\/Division Instructions-------------------------------\n+\/\/ Integer Multiplication Instructions\n+\/\/ Multiply Register\n+\n+instruct mulI_rReg(rRegI dst, rRegI src, rFlagsReg cr)\n+%{\n+  predicate(!UseAPX);\n+  match(Set dst (MulI dst src));\n+  effect(KILL cr);\n+\n+  ins_cost(300);\n+  format %{ \"imull   $dst, $src\\t# int\" %}\n+  ins_encode %{\n+    __ imull($dst$$Register, $src$$Register);\n+  %}\n+  ins_pipe(ialu_reg_reg_alu0);\n+%}\n+\n+instruct mulI_rReg_ndd(rRegI dst, rRegI src1, rRegI src2, rFlagsReg cr)\n+%{\n+  predicate(UseAPX);\n+  match(Set dst (MulI src1 src2));\n+  effect(KILL cr);\n+\n+  ins_cost(300);\n+  format %{ \"eimull   $dst, $src1, $src2\\t# int ndd\" %}\n+  ins_encode %{\n+    __ eimull($dst$$Register, $src1$$Register, $src2$$Register, false);\n+  %}\n+  ins_pipe(ialu_reg_reg_alu0);\n+%}\n+\n+instruct mulI_rReg_imm(rRegI dst, rRegI src, immI imm, rFlagsReg cr)\n+%{\n+  match(Set dst (MulI src imm));\n+  effect(KILL cr);\n+\n+  ins_cost(300);\n+  format %{ \"imull   $dst, $src, $imm\\t# int\" %}\n+  ins_encode %{\n+    __ imull($dst$$Register, $src$$Register, $imm$$constant);\n+  %}\n+  ins_pipe(ialu_reg_reg_alu0);\n+%}\n+\n+instruct mulI_mem(rRegI dst, memory src, rFlagsReg cr)\n+%{\n+  predicate(!UseAPX);\n+  match(Set dst (MulI dst (LoadI src)));\n+  effect(KILL cr);\n+\n+  ins_cost(350);\n+  format %{ \"imull   $dst, $src\\t# int\" %}\n+  ins_encode %{\n+    __ imull($dst$$Register, $src$$Address);\n+  %}\n+  ins_pipe(ialu_reg_mem_alu0);\n+%}\n+\n+instruct mulI_rReg_rReg_mem_ndd(rRegI dst, rRegI src1, memory src2, rFlagsReg cr)\n+%{\n+  predicate(UseAPX);\n+  match(Set dst (MulI src1 (LoadI src2)));\n+  effect(KILL cr);\n+\n+  ins_cost(350);\n+  format %{ \"eimull   $dst, $src1, $src2\\t# int ndd\" %}\n+  ins_encode %{\n+    __ eimull($dst$$Register, $src1$$Register, $src2$$Address, false);\n+  %}\n+  ins_pipe(ialu_reg_mem_alu0);\n+%}\n+\n+instruct mulI_mem_imm(rRegI dst, memory src, immI imm, rFlagsReg cr)\n+%{\n+  match(Set dst (MulI (LoadI src) imm));\n+  effect(KILL cr);\n+\n+  ins_cost(300);\n+  format %{ \"imull   $dst, $src, $imm\\t# int\" %}\n+  ins_encode %{\n+    __ imull($dst$$Register, $src$$Address, $imm$$constant);\n+  %}\n+  ins_pipe(ialu_reg_mem_alu0);\n+%}\n+\n+instruct mulAddS2I_rReg(rRegI dst, rRegI src1, rRegI src2, rRegI src3, rFlagsReg cr)\n+%{\n+  match(Set dst (MulAddS2I (Binary dst src1) (Binary src2 src3)));\n+  effect(KILL cr, KILL src2);\n+\n+  expand %{ mulI_rReg(dst, src1, cr);\n+           mulI_rReg(src2, src3, cr);\n+           addI_rReg(dst, src2, cr); %}\n+%}\n+\n+instruct mulL_rReg(rRegL dst, rRegL src, rFlagsReg cr)\n+%{\n+  predicate(!UseAPX);\n+  match(Set dst (MulL dst src));\n+  effect(KILL cr);\n+\n+  ins_cost(300);\n+  format %{ \"imulq   $dst, $src\\t# long\" %}\n+  ins_encode %{\n+    __ imulq($dst$$Register, $src$$Register);\n+  %}\n+  ins_pipe(ialu_reg_reg_alu0);\n+%}\n+\n+instruct mulL_rReg_ndd(rRegL dst, rRegL src1, rRegL src2, rFlagsReg cr)\n+%{\n+  predicate(UseAPX);\n+  match(Set dst (MulL src1 src2));\n+  effect(KILL cr);\n+\n+  ins_cost(300);\n+  format %{ \"eimulq   $dst, $src1, $src2\\t# long ndd\" %}\n+  ins_encode %{\n+    __ eimulq($dst$$Register, $src1$$Register, $src2$$Register, false);\n+  %}\n+  ins_pipe(ialu_reg_reg_alu0);\n+%}\n+\n+instruct mulL_rReg_imm(rRegL dst, rRegL src, immL32 imm, rFlagsReg cr)\n+%{\n+  match(Set dst (MulL src imm));\n+  effect(KILL cr);\n+\n+  ins_cost(300);\n+  format %{ \"imulq   $dst, $src, $imm\\t# long\" %}\n+  ins_encode %{\n+    __ imulq($dst$$Register, $src$$Register, $imm$$constant);\n+  %}\n+  ins_pipe(ialu_reg_reg_alu0);\n+%}\n+\n+instruct mulL_mem(rRegL dst, memory src, rFlagsReg cr)\n+%{\n+  predicate(!UseAPX);\n+  match(Set dst (MulL dst (LoadL src)));\n+  effect(KILL cr);\n+\n+  ins_cost(350);\n+  format %{ \"imulq   $dst, $src\\t# long\" %}\n+  ins_encode %{\n+    __ imulq($dst$$Register, $src$$Address);\n+  %}\n+  ins_pipe(ialu_reg_mem_alu0);\n+%}\n+\n+instruct mulL_rReg_rReg_mem_ndd(rRegL dst, rRegL src1, memory src2, rFlagsReg cr)\n+%{\n+  predicate(UseAPX);\n+  match(Set dst (MulL src1 (LoadL src2)));\n+  effect(KILL cr);\n+\n+  ins_cost(350);\n+  format %{ \"eimulq   $dst, $src1, $src2 \\t# long\" %}\n+  ins_encode %{\n+    __ eimulq($dst$$Register, $src1$$Register, $src2$$Address, false);\n+  %}\n+  ins_pipe(ialu_reg_mem_alu0);\n+%}\n+\n+instruct mulL_mem_imm(rRegL dst, memory src, immL32 imm, rFlagsReg cr)\n+%{\n+  match(Set dst (MulL (LoadL src) imm));\n+  effect(KILL cr);\n+\n+  ins_cost(300);\n+  format %{ \"imulq   $dst, $src, $imm\\t# long\" %}\n+  ins_encode %{\n+    __ imulq($dst$$Register, $src$$Address, $imm$$constant);\n+  %}\n+  ins_pipe(ialu_reg_mem_alu0);\n+%}\n+\n+instruct mulHiL_rReg(rdx_RegL dst, rRegL src, rax_RegL rax, rFlagsReg cr)\n+%{\n+  match(Set dst (MulHiL src rax));\n+  effect(USE_KILL rax, KILL cr);\n+\n+  ins_cost(300);\n+  format %{ \"imulq   RDX:RAX, RAX, $src\\t# mulhi\" %}\n+  ins_encode %{\n+    __ imulq($src$$Register);\n+  %}\n+  ins_pipe(ialu_reg_reg_alu0);\n+%}\n+\n+instruct umulHiL_rReg(rdx_RegL dst, rRegL src, rax_RegL rax, rFlagsReg cr)\n+%{\n+  match(Set dst (UMulHiL src rax));\n+  effect(USE_KILL rax, KILL cr);\n+\n+  ins_cost(300);\n+  format %{ \"mulq   RDX:RAX, RAX, $src\\t# umulhi\" %}\n+  ins_encode %{\n+    __ mulq($src$$Register);\n+  %}\n+  ins_pipe(ialu_reg_reg_alu0);\n+%}\n+\n+instruct divI_rReg(rax_RegI rax, rdx_RegI rdx, no_rax_rdx_RegI div,\n+                   rFlagsReg cr)\n+%{\n+  match(Set rax (DivI rax div));\n+  effect(KILL rdx, KILL cr);\n+\n+  ins_cost(30*100+10*100); \/\/ XXX\n+  format %{ \"cmpl    rax, 0x80000000\\t# idiv\\n\\t\"\n+            \"jne,s   normal\\n\\t\"\n+            \"xorl    rdx, rdx\\n\\t\"\n+            \"cmpl    $div, -1\\n\\t\"\n+            \"je,s    done\\n\"\n+    \"normal: cdql\\n\\t\"\n+            \"idivl   $div\\n\"\n+    \"done:\"        %}\n+  ins_encode(cdql_enc(div));\n+  ins_pipe(ialu_reg_reg_alu0);\n+%}\n+\n+instruct divL_rReg(rax_RegL rax, rdx_RegL rdx, no_rax_rdx_RegL div,\n+                   rFlagsReg cr)\n+%{\n+  match(Set rax (DivL rax div));\n+  effect(KILL rdx, KILL cr);\n+\n+  ins_cost(30*100+10*100); \/\/ XXX\n+  format %{ \"movq    rdx, 0x8000000000000000\\t# ldiv\\n\\t\"\n+            \"cmpq    rax, rdx\\n\\t\"\n+            \"jne,s   normal\\n\\t\"\n+            \"xorl    rdx, rdx\\n\\t\"\n+            \"cmpq    $div, -1\\n\\t\"\n+            \"je,s    done\\n\"\n+    \"normal: cdqq\\n\\t\"\n+            \"idivq   $div\\n\"\n+    \"done:\"        %}\n+  ins_encode(cdqq_enc(div));\n+  ins_pipe(ialu_reg_reg_alu0);\n+%}\n+\n+instruct udivI_rReg(rax_RegI rax, rdx_RegI rdx, no_rax_rdx_RegI div, rFlagsReg cr)\n+%{\n+  match(Set rax (UDivI rax div));\n+  effect(KILL rdx, KILL cr);\n+\n+  ins_cost(300);\n+  format %{ \"udivl $rax,$rax,$div\\t# UDivI\\n\" %}\n+  ins_encode %{\n+    __ udivI($rax$$Register, $div$$Register, $rdx$$Register);\n+  %}\n+  ins_pipe(ialu_reg_reg_alu0);\n+%}\n+\n+instruct udivL_rReg(rax_RegL rax, rdx_RegL rdx, no_rax_rdx_RegL div, rFlagsReg cr)\n+%{\n+  match(Set rax (UDivL rax div));\n+  effect(KILL rdx, KILL cr);\n+\n+  ins_cost(300);\n+  format %{ \"udivq $rax,$rax,$div\\t# UDivL\\n\" %}\n+  ins_encode %{\n+     __ udivL($rax$$Register, $div$$Register, $rdx$$Register);\n+  %}\n+  ins_pipe(ialu_reg_reg_alu0);\n+%}\n+\n+\/\/ Integer DIVMOD with Register, both quotient and mod results\n+instruct divModI_rReg_divmod(rax_RegI rax, rdx_RegI rdx, no_rax_rdx_RegI div,\n+                             rFlagsReg cr)\n+%{\n+  match(DivModI rax div);\n+  effect(KILL cr);\n+\n+  ins_cost(30*100+10*100); \/\/ XXX\n+  format %{ \"cmpl    rax, 0x80000000\\t# idiv\\n\\t\"\n+            \"jne,s   normal\\n\\t\"\n+            \"xorl    rdx, rdx\\n\\t\"\n+            \"cmpl    $div, -1\\n\\t\"\n+            \"je,s    done\\n\"\n+    \"normal: cdql\\n\\t\"\n+            \"idivl   $div\\n\"\n+    \"done:\"        %}\n+  ins_encode(cdql_enc(div));\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ Long DIVMOD with Register, both quotient and mod results\n+instruct divModL_rReg_divmod(rax_RegL rax, rdx_RegL rdx, no_rax_rdx_RegL div,\n+                             rFlagsReg cr)\n+%{\n+  match(DivModL rax div);\n+  effect(KILL cr);\n+\n+  ins_cost(30*100+10*100); \/\/ XXX\n+  format %{ \"movq    rdx, 0x8000000000000000\\t# ldiv\\n\\t\"\n+            \"cmpq    rax, rdx\\n\\t\"\n+            \"jne,s   normal\\n\\t\"\n+            \"xorl    rdx, rdx\\n\\t\"\n+            \"cmpq    $div, -1\\n\\t\"\n+            \"je,s    done\\n\"\n+    \"normal: cdqq\\n\\t\"\n+            \"idivq   $div\\n\"\n+    \"done:\"        %}\n+  ins_encode(cdqq_enc(div));\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ Unsigned integer DIVMOD with Register, both quotient and mod results\n+instruct udivModI_rReg_divmod(rax_RegI rax, no_rax_rdx_RegI tmp, rdx_RegI rdx,\n+                              no_rax_rdx_RegI div, rFlagsReg cr)\n+%{\n+  match(UDivModI rax div);\n+  effect(TEMP tmp, KILL cr);\n+\n+  ins_cost(300);\n+  format %{ \"udivl $rax,$rax,$div\\t# begin UDivModI\\n\\t\"\n+            \"umodl $rdx,$rax,$div\\t! using $tmp as TEMP # end UDivModI\\n\"\n+          %}\n+  ins_encode %{\n+    __ udivmodI($rax$$Register, $div$$Register, $rdx$$Register, $tmp$$Register);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ Unsigned long DIVMOD with Register, both quotient and mod results\n+instruct udivModL_rReg_divmod(rax_RegL rax, no_rax_rdx_RegL tmp, rdx_RegL rdx,\n+                              no_rax_rdx_RegL div, rFlagsReg cr)\n+%{\n+  match(UDivModL rax div);\n+  effect(TEMP tmp, KILL cr);\n+\n+  ins_cost(300);\n+  format %{ \"udivq $rax,$rax,$div\\t# begin UDivModL\\n\\t\"\n+            \"umodq $rdx,$rax,$div\\t! using $tmp as TEMP # end UDivModL\\n\"\n+          %}\n+  ins_encode %{\n+    __ udivmodL($rax$$Register, $div$$Register, $rdx$$Register, $tmp$$Register);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct modI_rReg(rdx_RegI rdx, rax_RegI rax, no_rax_rdx_RegI div,\n+                   rFlagsReg cr)\n+%{\n+  match(Set rdx (ModI rax div));\n+  effect(KILL rax, KILL cr);\n+\n+  ins_cost(300); \/\/ XXX\n+  format %{ \"cmpl    rax, 0x80000000\\t# irem\\n\\t\"\n+            \"jne,s   normal\\n\\t\"\n+            \"xorl    rdx, rdx\\n\\t\"\n+            \"cmpl    $div, -1\\n\\t\"\n+            \"je,s    done\\n\"\n+    \"normal: cdql\\n\\t\"\n+            \"idivl   $div\\n\"\n+    \"done:\"        %}\n+  ins_encode(cdql_enc(div));\n+  ins_pipe(ialu_reg_reg_alu0);\n+%}\n+\n+instruct modL_rReg(rdx_RegL rdx, rax_RegL rax, no_rax_rdx_RegL div,\n+                   rFlagsReg cr)\n+%{\n+  match(Set rdx (ModL rax div));\n+  effect(KILL rax, KILL cr);\n+\n+  ins_cost(300); \/\/ XXX\n+  format %{ \"movq    rdx, 0x8000000000000000\\t# lrem\\n\\t\"\n+            \"cmpq    rax, rdx\\n\\t\"\n+            \"jne,s   normal\\n\\t\"\n+            \"xorl    rdx, rdx\\n\\t\"\n+            \"cmpq    $div, -1\\n\\t\"\n+            \"je,s    done\\n\"\n+    \"normal: cdqq\\n\\t\"\n+            \"idivq   $div\\n\"\n+    \"done:\"        %}\n+  ins_encode(cdqq_enc(div));\n+  ins_pipe(ialu_reg_reg_alu0);\n+%}\n+\n+instruct umodI_rReg(rdx_RegI rdx, rax_RegI rax, no_rax_rdx_RegI div, rFlagsReg cr)\n+%{\n+  match(Set rdx (UModI rax div));\n+  effect(KILL rax, KILL cr);\n+\n+  ins_cost(300);\n+  format %{ \"umodl $rdx,$rax,$div\\t# UModI\\n\" %}\n+  ins_encode %{\n+    __ umodI($rax$$Register, $div$$Register, $rdx$$Register);\n+  %}\n+  ins_pipe(ialu_reg_reg_alu0);\n+%}\n+\n+instruct umodL_rReg(rdx_RegL rdx, rax_RegL rax, no_rax_rdx_RegL div, rFlagsReg cr)\n+%{\n+  match(Set rdx (UModL rax div));\n+  effect(KILL rax, KILL cr);\n+\n+  ins_cost(300);\n+  format %{ \"umodq $rdx,$rax,$div\\t# UModL\\n\" %}\n+  ins_encode %{\n+    __ umodL($rax$$Register, $div$$Register, $rdx$$Register);\n+  %}\n+  ins_pipe(ialu_reg_reg_alu0);\n+%}\n+\n+\/\/ Integer Shift Instructions\n+\/\/ Shift Left by one, two, three\n+instruct salI_rReg_immI2(rRegI dst, immI2 shift, rFlagsReg cr)\n+%{\n+  predicate(!UseAPX);\n+  match(Set dst (LShiftI dst shift));\n+  effect(KILL cr);\n+\n+  format %{ \"sall    $dst, $shift\" %}\n+  ins_encode %{\n+    __ sall($dst$$Register, $shift$$constant);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+\/\/ Shift Left by one, two, three\n+instruct salI_rReg_immI2_ndd(rRegI dst, rRegI src, immI2 shift, rFlagsReg cr)\n+%{\n+  predicate(UseAPX);\n+  match(Set dst (LShiftI src shift));\n+  effect(KILL cr);\n+\n+  format %{ \"esall    $dst, $src, $shift\\t# int(ndd)\" %}\n+  ins_encode %{\n+    __ esall($dst$$Register, $src$$Register, $shift$$constant, false);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+\/\/ Shift Left by 8-bit immediate\n+instruct salI_rReg_imm(rRegI dst, immI8 shift, rFlagsReg cr)\n+%{\n+  predicate(!UseAPX);\n+  match(Set dst (LShiftI dst shift));\n+  effect(KILL cr);\n+\n+  format %{ \"sall    $dst, $shift\" %}\n+  ins_encode %{\n+    __ sall($dst$$Register, $shift$$constant);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+\/\/ Shift Left by 8-bit immediate\n+instruct salI_rReg_imm_ndd(rRegI dst, rRegI src, immI8 shift, rFlagsReg cr)\n+%{\n+  predicate(UseAPX);\n+  match(Set dst (LShiftI src shift));\n+  effect(KILL cr);\n+\n+  format %{ \"esall    $dst, $src, $shift\\t# int (ndd)\" %}\n+  ins_encode %{\n+    __ esall($dst$$Register, $src$$Register, $shift$$constant, false);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+instruct salI_rReg_mem_imm_ndd(rRegI dst, memory src, immI8 shift, rFlagsReg cr)\n+%{\n+  predicate(UseAPX);\n+  match(Set dst (LShiftI (LoadI src) shift));\n+  effect(KILL cr);\n+\n+  format %{ \"esall    $dst, $src, $shift\\t# int (ndd)\" %}\n+  ins_encode %{\n+    __ esall($dst$$Register, $src$$Address, $shift$$constant, false);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+\/\/ Shift Left by 8-bit immediate\n+instruct salI_mem_imm(memory dst, immI8 shift, rFlagsReg cr)\n+%{\n+  match(Set dst (StoreI dst (LShiftI (LoadI dst) shift)));\n+  effect(KILL cr);\n+\n+  format %{ \"sall    $dst, $shift\" %}\n+  ins_encode %{\n+    __ sall($dst$$Address, $shift$$constant);\n+  %}\n+  ins_pipe(ialu_mem_imm);\n+%}\n+\n+\/\/ Shift Left by variable\n+instruct salI_rReg_CL(rRegI dst, rcx_RegI shift, rFlagsReg cr)\n+%{\n+  predicate(!VM_Version::supports_bmi2());\n+  match(Set dst (LShiftI dst shift));\n+  effect(KILL cr);\n+\n+  format %{ \"sall    $dst, $shift\" %}\n+  ins_encode %{\n+    __ sall($dst$$Register);\n+  %}\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+\/\/ Shift Left by variable\n+instruct salI_mem_CL(memory dst, rcx_RegI shift, rFlagsReg cr)\n+%{\n+  predicate(!VM_Version::supports_bmi2());\n+  match(Set dst (StoreI dst (LShiftI (LoadI dst) shift)));\n+  effect(KILL cr);\n+\n+  format %{ \"sall    $dst, $shift\" %}\n+  ins_encode %{\n+    __ sall($dst$$Address);\n+  %}\n+  ins_pipe(ialu_mem_reg);\n+%}\n+\n+instruct salI_rReg_rReg(rRegI dst, rRegI src, rRegI shift)\n+%{\n+  predicate(VM_Version::supports_bmi2());\n+  match(Set dst (LShiftI src shift));\n+\n+  format %{ \"shlxl   $dst, $src, $shift\" %}\n+  ins_encode %{\n+    __ shlxl($dst$$Register, $src$$Register, $shift$$Register);\n+  %}\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+instruct salI_mem_rReg(rRegI dst, memory src, rRegI shift)\n+%{\n+  predicate(VM_Version::supports_bmi2());\n+  match(Set dst (LShiftI (LoadI src) shift));\n+  ins_cost(175);\n+  format %{ \"shlxl   $dst, $src, $shift\" %}\n+  ins_encode %{\n+    __ shlxl($dst$$Register, $src$$Address, $shift$$Register);\n+  %}\n+  ins_pipe(ialu_reg_mem);\n+%}\n+\n+\/\/ Arithmetic Shift Right by 8-bit immediate\n+instruct sarI_rReg_imm(rRegI dst, immI8 shift, rFlagsReg cr)\n+%{\n+  predicate(!UseAPX);\n+  match(Set dst (RShiftI dst shift));\n+  effect(KILL cr);\n+\n+  format %{ \"sarl    $dst, $shift\" %}\n+  ins_encode %{\n+    __ sarl($dst$$Register, $shift$$constant);\n+  %}\n+  ins_pipe(ialu_mem_imm);\n+%}\n+\n+\/\/ Arithmetic Shift Right by 8-bit immediate\n+instruct sarI_rReg_imm_ndd(rRegI dst, rRegI src, immI8 shift, rFlagsReg cr)\n+%{\n+  predicate(UseAPX);\n+  match(Set dst (RShiftI src shift));\n+  effect(KILL cr);\n+\n+  format %{ \"esarl    $dst, $src, $shift\\t# int (ndd)\" %}\n+  ins_encode %{\n+    __ esarl($dst$$Register, $src$$Register, $shift$$constant, false);\n+  %}\n+  ins_pipe(ialu_mem_imm);\n+%}\n+\n+instruct sarI_rReg_mem_imm_ndd(rRegI dst, memory src, immI8 shift, rFlagsReg cr)\n+%{\n+  predicate(UseAPX);\n+  match(Set dst (RShiftI (LoadI src) shift));\n+  effect(KILL cr);\n+\n+  format %{ \"esarl    $dst, $src, $shift\\t# int (ndd)\" %}\n+  ins_encode %{\n+    __ esarl($dst$$Register, $src$$Address, $shift$$constant, false);\n+  %}\n+  ins_pipe(ialu_mem_imm);\n+%}\n+\n+\/\/ Arithmetic Shift Right by 8-bit immediate\n+instruct sarI_mem_imm(memory dst, immI8 shift, rFlagsReg cr)\n+%{\n+  match(Set dst (StoreI dst (RShiftI (LoadI dst) shift)));\n+  effect(KILL cr);\n+\n+  format %{ \"sarl    $dst, $shift\" %}\n+  ins_encode %{\n+    __ sarl($dst$$Address, $shift$$constant);\n+  %}\n+  ins_pipe(ialu_mem_imm);\n+%}\n+\n+\/\/ Arithmetic Shift Right by variable\n+instruct sarI_rReg_CL(rRegI dst, rcx_RegI shift, rFlagsReg cr)\n+%{\n+  predicate(!VM_Version::supports_bmi2());\n+  match(Set dst (RShiftI dst shift));\n+  effect(KILL cr);\n+\n+  format %{ \"sarl    $dst, $shift\" %}\n+  ins_encode %{\n+    __ sarl($dst$$Register);\n+  %}\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+\/\/ Arithmetic Shift Right by variable\n+instruct sarI_mem_CL(memory dst, rcx_RegI shift, rFlagsReg cr)\n+%{\n+  predicate(!VM_Version::supports_bmi2());\n+  match(Set dst (StoreI dst (RShiftI (LoadI dst) shift)));\n+  effect(KILL cr);\n+\n+  format %{ \"sarl    $dst, $shift\" %}\n+  ins_encode %{\n+    __ sarl($dst$$Address);\n+  %}\n+  ins_pipe(ialu_mem_reg);\n+%}\n+\n+instruct sarI_rReg_rReg(rRegI dst, rRegI src, rRegI shift)\n+%{\n+  predicate(VM_Version::supports_bmi2());\n+  match(Set dst (RShiftI src shift));\n+\n+  format %{ \"sarxl   $dst, $src, $shift\" %}\n+  ins_encode %{\n+    __ sarxl($dst$$Register, $src$$Register, $shift$$Register);\n+  %}\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+instruct sarI_mem_rReg(rRegI dst, memory src, rRegI shift)\n+%{\n+  predicate(VM_Version::supports_bmi2());\n+  match(Set dst (RShiftI (LoadI src) shift));\n+  ins_cost(175);\n+  format %{ \"sarxl   $dst, $src, $shift\" %}\n+  ins_encode %{\n+    __ sarxl($dst$$Register, $src$$Address, $shift$$Register);\n+  %}\n+  ins_pipe(ialu_reg_mem);\n+%}\n+\n+\/\/ Logical Shift Right by 8-bit immediate\n+instruct shrI_rReg_imm(rRegI dst, immI8 shift, rFlagsReg cr)\n+%{\n+  predicate(!UseAPX);\n+  match(Set dst (URShiftI dst shift));\n+  effect(KILL cr);\n+\n+  format %{ \"shrl    $dst, $shift\" %}\n+  ins_encode %{\n+    __ shrl($dst$$Register, $shift$$constant);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+\/\/ Logical Shift Right by 8-bit immediate\n+instruct shrI_rReg_imm_ndd(rRegI dst, rRegI src, immI8 shift, rFlagsReg cr)\n+%{\n+  predicate(UseAPX);\n+  match(Set dst (URShiftI src shift));\n+  effect(KILL cr);\n+\n+  format %{ \"eshrl    $dst, $src, $shift\\t # int (ndd)\" %}\n+  ins_encode %{\n+    __ eshrl($dst$$Register, $src$$Register, $shift$$constant, false);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+instruct shrI_rReg_mem_imm_ndd(rRegI dst, memory src, immI8 shift, rFlagsReg cr)\n+%{\n+  predicate(UseAPX);\n+  match(Set dst (URShiftI (LoadI src) shift));\n+  effect(KILL cr);\n+\n+  format %{ \"eshrl    $dst, $src, $shift\\t # int (ndd)\" %}\n+  ins_encode %{\n+    __ eshrl($dst$$Register, $src$$Address, $shift$$constant, false);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+\/\/ Logical Shift Right by 8-bit immediate\n+instruct shrI_mem_imm(memory dst, immI8 shift, rFlagsReg cr)\n+%{\n+  match(Set dst (StoreI dst (URShiftI (LoadI dst) shift)));\n+  effect(KILL cr);\n+\n+  format %{ \"shrl    $dst, $shift\" %}\n+  ins_encode %{\n+    __ shrl($dst$$Address, $shift$$constant);\n+  %}\n+  ins_pipe(ialu_mem_imm);\n+%}\n+\n+\/\/ Logical Shift Right by variable\n+instruct shrI_rReg_CL(rRegI dst, rcx_RegI shift, rFlagsReg cr)\n+%{\n+  predicate(!VM_Version::supports_bmi2());\n+  match(Set dst (URShiftI dst shift));\n+  effect(KILL cr);\n+\n+  format %{ \"shrl    $dst, $shift\" %}\n+  ins_encode %{\n+    __ shrl($dst$$Register);\n+  %}\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+\/\/ Logical Shift Right by variable\n+instruct shrI_mem_CL(memory dst, rcx_RegI shift, rFlagsReg cr)\n+%{\n+  predicate(!VM_Version::supports_bmi2());\n+  match(Set dst (StoreI dst (URShiftI (LoadI dst) shift)));\n+  effect(KILL cr);\n+\n+  format %{ \"shrl    $dst, $shift\" %}\n+  ins_encode %{\n+    __ shrl($dst$$Address);\n+  %}\n+  ins_pipe(ialu_mem_reg);\n+%}\n+\n+instruct shrI_rReg_rReg(rRegI dst, rRegI src, rRegI shift)\n+%{\n+  predicate(VM_Version::supports_bmi2());\n+  match(Set dst (URShiftI src shift));\n+\n+  format %{ \"shrxl   $dst, $src, $shift\" %}\n+  ins_encode %{\n+    __ shrxl($dst$$Register, $src$$Register, $shift$$Register);\n+  %}\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+instruct shrI_mem_rReg(rRegI dst, memory src, rRegI shift)\n+%{\n+  predicate(VM_Version::supports_bmi2());\n+  match(Set dst (URShiftI (LoadI src) shift));\n+  ins_cost(175);\n+  format %{ \"shrxl   $dst, $src, $shift\" %}\n+  ins_encode %{\n+    __ shrxl($dst$$Register, $src$$Address, $shift$$Register);\n+  %}\n+  ins_pipe(ialu_reg_mem);\n+%}\n+\n+\/\/ Long Shift Instructions\n+\/\/ Shift Left by one, two, three\n+instruct salL_rReg_immI2(rRegL dst, immI2 shift, rFlagsReg cr)\n+%{\n+  predicate(!UseAPX);\n+  match(Set dst (LShiftL dst shift));\n+  effect(KILL cr);\n+\n+  format %{ \"salq    $dst, $shift\" %}\n+  ins_encode %{\n+    __ salq($dst$$Register, $shift$$constant);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+\/\/ Shift Left by one, two, three\n+instruct salL_rReg_immI2_ndd(rRegL dst, rRegL src, immI2 shift, rFlagsReg cr)\n+%{\n+  predicate(UseAPX);\n+  match(Set dst (LShiftL src shift));\n+  effect(KILL cr);\n+\n+  format %{ \"esalq    $dst, $src, $shift\\t# long (ndd)\" %}\n+  ins_encode %{\n+    __ esalq($dst$$Register, $src$$Register, $shift$$constant, false);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+\/\/ Shift Left by 8-bit immediate\n+instruct salL_rReg_imm(rRegL dst, immI8 shift, rFlagsReg cr)\n+%{\n+  predicate(!UseAPX);\n+  match(Set dst (LShiftL dst shift));\n+  effect(KILL cr);\n+\n+  format %{ \"salq    $dst, $shift\" %}\n+  ins_encode %{\n+    __ salq($dst$$Register, $shift$$constant);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+\/\/ Shift Left by 8-bit immediate\n+instruct salL_rReg_imm_ndd(rRegL dst, rRegL src, immI8 shift, rFlagsReg cr)\n+%{\n+  predicate(UseAPX);\n+  match(Set dst (LShiftL src shift));\n+  effect(KILL cr);\n+\n+  format %{ \"esalq    $dst, $src, $shift\\t# long (ndd)\" %}\n+  ins_encode %{\n+    __ esalq($dst$$Register, $src$$Register, $shift$$constant, false);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+instruct salL_rReg_mem_imm_ndd(rRegL dst, memory src, immI8 shift, rFlagsReg cr)\n+%{\n+  predicate(UseAPX);\n+  match(Set dst (LShiftL (LoadL src) shift));\n+  effect(KILL cr);\n+\n+  format %{ \"esalq    $dst, $src, $shift\\t# long (ndd)\" %}\n+  ins_encode %{\n+    __ esalq($dst$$Register, $src$$Address, $shift$$constant, false);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+\/\/ Shift Left by 8-bit immediate\n+instruct salL_mem_imm(memory dst, immI8 shift, rFlagsReg cr)\n+%{\n+  match(Set dst (StoreL dst (LShiftL (LoadL dst) shift)));\n+  effect(KILL cr);\n+\n+  format %{ \"salq    $dst, $shift\" %}\n+  ins_encode %{\n+    __ salq($dst$$Address, $shift$$constant);\n+  %}\n+  ins_pipe(ialu_mem_imm);\n+%}\n+\n+\/\/ Shift Left by variable\n+instruct salL_rReg_CL(rRegL dst, rcx_RegI shift, rFlagsReg cr)\n+%{\n+  predicate(!VM_Version::supports_bmi2());\n+  match(Set dst (LShiftL dst shift));\n+  effect(KILL cr);\n+\n+  format %{ \"salq    $dst, $shift\" %}\n+  ins_encode %{\n+    __ salq($dst$$Register);\n+  %}\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+\/\/ Shift Left by variable\n+instruct salL_mem_CL(memory dst, rcx_RegI shift, rFlagsReg cr)\n+%{\n+  predicate(!VM_Version::supports_bmi2());\n+  match(Set dst (StoreL dst (LShiftL (LoadL dst) shift)));\n+  effect(KILL cr);\n+\n+  format %{ \"salq    $dst, $shift\" %}\n+  ins_encode %{\n+    __ salq($dst$$Address);\n+  %}\n+  ins_pipe(ialu_mem_reg);\n+%}\n+\n+instruct salL_rReg_rReg(rRegL dst, rRegL src, rRegI shift)\n+%{\n+  predicate(VM_Version::supports_bmi2());\n+  match(Set dst (LShiftL src shift));\n+\n+  format %{ \"shlxq   $dst, $src, $shift\" %}\n+  ins_encode %{\n+    __ shlxq($dst$$Register, $src$$Register, $shift$$Register);\n+  %}\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+instruct salL_mem_rReg(rRegL dst, memory src, rRegI shift)\n+%{\n+  predicate(VM_Version::supports_bmi2());\n+  match(Set dst (LShiftL (LoadL src) shift));\n+  ins_cost(175);\n+  format %{ \"shlxq   $dst, $src, $shift\" %}\n+  ins_encode %{\n+    __ shlxq($dst$$Register, $src$$Address, $shift$$Register);\n+  %}\n+  ins_pipe(ialu_reg_mem);\n+%}\n+\n+\/\/ Arithmetic Shift Right by 8-bit immediate\n+instruct sarL_rReg_imm(rRegL dst, immI shift, rFlagsReg cr)\n+%{\n+  predicate(!UseAPX);\n+  match(Set dst (RShiftL dst shift));\n+  effect(KILL cr);\n+\n+  format %{ \"sarq    $dst, $shift\" %}\n+  ins_encode %{\n+    __ sarq($dst$$Register, (unsigned char)($shift$$constant & 0x3F));\n+  %}\n+  ins_pipe(ialu_mem_imm);\n+%}\n+\n+\/\/ Arithmetic Shift Right by 8-bit immediate\n+instruct sarL_rReg_imm_ndd(rRegL dst, rRegL src, immI shift, rFlagsReg cr)\n+%{\n+  predicate(UseAPX);\n+  match(Set dst (RShiftL src shift));\n+  effect(KILL cr);\n+\n+  format %{ \"esarq    $dst, $src, $shift\\t# long (ndd)\" %}\n+  ins_encode %{\n+    __ esarq($dst$$Register, $src$$Register, (unsigned char)($shift$$constant & 0x3F), false);\n+  %}\n+  ins_pipe(ialu_mem_imm);\n+%}\n+\n+instruct sarL_rReg_mem_imm_ndd(rRegL dst, memory src, immI shift, rFlagsReg cr)\n+%{\n+  predicate(UseAPX);\n+  match(Set dst (RShiftL (LoadL src) shift));\n+  effect(KILL cr);\n+\n+  format %{ \"esarq    $dst, $src, $shift\\t# long (ndd)\" %}\n+  ins_encode %{\n+    __ esarq($dst$$Register, $src$$Address, (unsigned char)($shift$$constant & 0x3F), false);\n+  %}\n+  ins_pipe(ialu_mem_imm);\n+%}\n+\n+\/\/ Arithmetic Shift Right by 8-bit immediate\n+instruct sarL_mem_imm(memory dst, immI shift, rFlagsReg cr)\n+%{\n+  match(Set dst (StoreL dst (RShiftL (LoadL dst) shift)));\n+  effect(KILL cr);\n+\n+  format %{ \"sarq    $dst, $shift\" %}\n+  ins_encode %{\n+    __ sarq($dst$$Address, (unsigned char)($shift$$constant & 0x3F));\n+  %}\n+  ins_pipe(ialu_mem_imm);\n+%}\n+\n+\/\/ Arithmetic Shift Right by variable\n+instruct sarL_rReg_CL(rRegL dst, rcx_RegI shift, rFlagsReg cr)\n+%{\n+  predicate(!VM_Version::supports_bmi2());\n+  match(Set dst (RShiftL dst shift));\n+  effect(KILL cr);\n+\n+  format %{ \"sarq    $dst, $shift\" %}\n+  ins_encode %{\n+    __ sarq($dst$$Register);\n+  %}\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+\/\/ Arithmetic Shift Right by variable\n+instruct sarL_mem_CL(memory dst, rcx_RegI shift, rFlagsReg cr)\n+%{\n+  predicate(!VM_Version::supports_bmi2());\n+  match(Set dst (StoreL dst (RShiftL (LoadL dst) shift)));\n+  effect(KILL cr);\n+\n+  format %{ \"sarq    $dst, $shift\" %}\n+  ins_encode %{\n+    __ sarq($dst$$Address);\n+  %}\n+  ins_pipe(ialu_mem_reg);\n+%}\n+\n+instruct sarL_rReg_rReg(rRegL dst, rRegL src, rRegI shift)\n+%{\n+  predicate(VM_Version::supports_bmi2());\n+  match(Set dst (RShiftL src shift));\n+\n+  format %{ \"sarxq   $dst, $src, $shift\" %}\n+  ins_encode %{\n+    __ sarxq($dst$$Register, $src$$Register, $shift$$Register);\n+  %}\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+instruct sarL_mem_rReg(rRegL dst, memory src, rRegI shift)\n+%{\n+  predicate(VM_Version::supports_bmi2());\n+  match(Set dst (RShiftL (LoadL src) shift));\n+  ins_cost(175);\n+  format %{ \"sarxq   $dst, $src, $shift\" %}\n+  ins_encode %{\n+    __ sarxq($dst$$Register, $src$$Address, $shift$$Register);\n+  %}\n+  ins_pipe(ialu_reg_mem);\n+%}\n+\n+\/\/ Logical Shift Right by 8-bit immediate\n+instruct shrL_rReg_imm(rRegL dst, immI8 shift, rFlagsReg cr)\n+%{\n+  predicate(!UseAPX);\n+  match(Set dst (URShiftL dst shift));\n+  effect(KILL cr);\n+\n+  format %{ \"shrq    $dst, $shift\" %}\n+  ins_encode %{\n+    __ shrq($dst$$Register, $shift$$constant);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+\/\/ Logical Shift Right by 8-bit immediate\n+instruct shrL_rReg_imm_ndd(rRegL dst, rRegL src, immI8 shift, rFlagsReg cr)\n+%{\n+  predicate(UseAPX);\n+  match(Set dst (URShiftL src shift));\n+  effect(KILL cr);\n+\n+  format %{ \"eshrq    $dst, $src, $shift\\t# long (ndd)\" %}\n+  ins_encode %{\n+    __ eshrq($dst$$Register, $src$$Register, $shift$$constant, false);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+instruct shrL_rReg_mem_imm_ndd(rRegL dst, memory src, immI8 shift, rFlagsReg cr)\n+%{\n+  predicate(UseAPX);\n+  match(Set dst (URShiftL (LoadL src) shift));\n+  effect(KILL cr);\n+\n+  format %{ \"eshrq    $dst, $src, $shift\\t# long (ndd)\" %}\n+  ins_encode %{\n+    __ eshrq($dst$$Register, $src$$Address, $shift$$constant, false);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+\/\/ Logical Shift Right by 8-bit immediate\n+instruct shrL_mem_imm(memory dst, immI8 shift, rFlagsReg cr)\n+%{\n+  match(Set dst (StoreL dst (URShiftL (LoadL dst) shift)));\n+  effect(KILL cr);\n+\n+  format %{ \"shrq    $dst, $shift\" %}\n+  ins_encode %{\n+    __ shrq($dst$$Address, $shift$$constant);\n+  %}\n+  ins_pipe(ialu_mem_imm);\n+%}\n+\n+\/\/ Logical Shift Right by variable\n+instruct shrL_rReg_CL(rRegL dst, rcx_RegI shift, rFlagsReg cr)\n+%{\n+  predicate(!VM_Version::supports_bmi2());\n+  match(Set dst (URShiftL dst shift));\n+  effect(KILL cr);\n+\n+  format %{ \"shrq    $dst, $shift\" %}\n+  ins_encode %{\n+    __ shrq($dst$$Register);\n+  %}\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+\/\/ Logical Shift Right by variable\n+instruct shrL_mem_CL(memory dst, rcx_RegI shift, rFlagsReg cr)\n+%{\n+  predicate(!VM_Version::supports_bmi2());\n+  match(Set dst (StoreL dst (URShiftL (LoadL dst) shift)));\n+  effect(KILL cr);\n+\n+  format %{ \"shrq    $dst, $shift\" %}\n+  ins_encode %{\n+    __ shrq($dst$$Address);\n+  %}\n+  ins_pipe(ialu_mem_reg);\n+%}\n+\n+instruct shrL_rReg_rReg(rRegL dst, rRegL src, rRegI shift)\n+%{\n+  predicate(VM_Version::supports_bmi2());\n+  match(Set dst (URShiftL src shift));\n+\n+  format %{ \"shrxq   $dst, $src, $shift\" %}\n+  ins_encode %{\n+    __ shrxq($dst$$Register, $src$$Register, $shift$$Register);\n+  %}\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+instruct shrL_mem_rReg(rRegL dst, memory src, rRegI shift)\n+%{\n+  predicate(VM_Version::supports_bmi2());\n+  match(Set dst (URShiftL (LoadL src) shift));\n+  ins_cost(175);\n+  format %{ \"shrxq   $dst, $src, $shift\" %}\n+  ins_encode %{\n+    __ shrxq($dst$$Register, $src$$Address, $shift$$Register);\n+  %}\n+  ins_pipe(ialu_reg_mem);\n+%}\n+\n+\/\/ Logical Shift Right by 24, followed by Arithmetic Shift Left by 24.\n+\/\/ This idiom is used by the compiler for the i2b bytecode.\n+instruct i2b(rRegI dst, rRegI src, immI_24 twentyfour)\n+%{\n+  match(Set dst (RShiftI (LShiftI src twentyfour) twentyfour));\n+\n+  format %{ \"movsbl  $dst, $src\\t# i2b\" %}\n+  ins_encode %{\n+    __ movsbl($dst$$Register, $src$$Register);\n+  %}\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+\/\/ Logical Shift Right by 16, followed by Arithmetic Shift Left by 16.\n+\/\/ This idiom is used by the compiler the i2s bytecode.\n+instruct i2s(rRegI dst, rRegI src, immI_16 sixteen)\n+%{\n+  match(Set dst (RShiftI (LShiftI src sixteen) sixteen));\n+\n+  format %{ \"movswl  $dst, $src\\t# i2s\" %}\n+  ins_encode %{\n+    __ movswl($dst$$Register, $src$$Register);\n+  %}\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+\/\/ ROL\/ROR instructions\n+\n+\/\/ Rotate left by constant.\n+instruct rolI_immI8_legacy(rRegI dst, immI8 shift, rFlagsReg cr)\n+%{\n+  predicate(!VM_Version::supports_bmi2() && n->bottom_type()->basic_type() == T_INT);\n+  match(Set dst (RotateLeft dst shift));\n+  effect(KILL cr);\n+  format %{ \"roll    $dst, $shift\" %}\n+  ins_encode %{\n+    __ roll($dst$$Register, $shift$$constant);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+instruct rolI_immI8(rRegI dst, rRegI src, immI8 shift)\n+%{\n+  predicate(!UseAPX && VM_Version::supports_bmi2() && n->bottom_type()->basic_type() == T_INT);\n+  match(Set dst (RotateLeft src shift));\n+  format %{ \"rolxl   $dst, $src, $shift\" %}\n+  ins_encode %{\n+    int shift = 32 - ($shift$$constant & 31);\n+    __ rorxl($dst$$Register, $src$$Register, shift);\n+  %}\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+instruct rolI_mem_immI8(rRegI dst, memory src, immI8 shift)\n+%{\n+  predicate(VM_Version::supports_bmi2() && n->bottom_type()->basic_type() == T_INT);\n+  match(Set dst (RotateLeft (LoadI src) shift));\n+  ins_cost(175);\n+  format %{ \"rolxl   $dst, $src, $shift\" %}\n+  ins_encode %{\n+    int shift = 32 - ($shift$$constant & 31);\n+    __ rorxl($dst$$Register, $src$$Address, shift);\n+  %}\n+  ins_pipe(ialu_reg_mem);\n+%}\n+\n+\/\/ Rotate Left by variable\n+instruct rolI_rReg_Var(rRegI dst, rcx_RegI shift, rFlagsReg cr)\n+%{\n+  predicate(!UseAPX && n->bottom_type()->basic_type() == T_INT);\n+  match(Set dst (RotateLeft dst shift));\n+  effect(KILL cr);\n+  format %{ \"roll    $dst, $shift\" %}\n+  ins_encode %{\n+    __ roll($dst$$Register);\n+  %}\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+\/\/ Rotate Left by variable\n+instruct rolI_rReg_Var_ndd(rRegI dst, rRegI src, rcx_RegI shift, rFlagsReg cr)\n+%{\n+  predicate(UseAPX && n->bottom_type()->basic_type() == T_INT);\n+  match(Set dst (RotateLeft src shift));\n+  effect(KILL cr);\n+\n+  format %{ \"eroll    $dst, $src, $shift\\t# rotate left (int ndd)\" %}\n+  ins_encode %{\n+    __ eroll($dst$$Register, $src$$Register, false);\n+  %}\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+\/\/ Rotate Right by constant.\n+instruct rorI_immI8_legacy(rRegI dst, immI8 shift, rFlagsReg cr)\n+%{\n+  predicate(!VM_Version::supports_bmi2() && n->bottom_type()->basic_type() == T_INT);\n+  match(Set dst (RotateRight dst shift));\n+  effect(KILL cr);\n+  format %{ \"rorl    $dst, $shift\" %}\n+  ins_encode %{\n+    __ rorl($dst$$Register, $shift$$constant);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+\/\/ Rotate Right by constant.\n+instruct rorI_immI8(rRegI dst, rRegI src, immI8 shift)\n+%{\n+  predicate(!UseAPX && VM_Version::supports_bmi2() && n->bottom_type()->basic_type() == T_INT);\n+  match(Set dst (RotateRight src shift));\n+  format %{ \"rorxl   $dst, $src, $shift\" %}\n+  ins_encode %{\n+    __ rorxl($dst$$Register, $src$$Register, $shift$$constant);\n+  %}\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+instruct rorI_mem_immI8(rRegI dst, memory src, immI8 shift)\n+%{\n+  predicate(VM_Version::supports_bmi2() && n->bottom_type()->basic_type() == T_INT);\n+  match(Set dst (RotateRight (LoadI src) shift));\n+  ins_cost(175);\n+  format %{ \"rorxl   $dst, $src, $shift\" %}\n+  ins_encode %{\n+    __ rorxl($dst$$Register, $src$$Address, $shift$$constant);\n+  %}\n+  ins_pipe(ialu_reg_mem);\n+%}\n+\n+\/\/ Rotate Right by variable\n+instruct rorI_rReg_Var(rRegI dst, rcx_RegI shift, rFlagsReg cr)\n+%{\n+  predicate(!UseAPX && n->bottom_type()->basic_type() == T_INT);\n+  match(Set dst (RotateRight dst shift));\n+  effect(KILL cr);\n+  format %{ \"rorl    $dst, $shift\" %}\n+  ins_encode %{\n+    __ rorl($dst$$Register);\n+  %}\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+\/\/ Rotate Right by variable\n+instruct rorI_rReg_Var_ndd(rRegI dst, rRegI src, rcx_RegI shift, rFlagsReg cr)\n+%{\n+  predicate(UseAPX && n->bottom_type()->basic_type() == T_INT);\n+  match(Set dst (RotateRight src shift));\n+  effect(KILL cr);\n+\n+  format %{ \"erorl    $dst, $src, $shift\\t# rotate right(int ndd)\" %}\n+  ins_encode %{\n+    __ erorl($dst$$Register, $src$$Register, false);\n+  %}\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+\/\/ Rotate Left by constant.\n+instruct rolL_immI8_legacy(rRegL dst, immI8 shift, rFlagsReg cr)\n+%{\n+  predicate(!VM_Version::supports_bmi2() && n->bottom_type()->basic_type() == T_LONG);\n+  match(Set dst (RotateLeft dst shift));\n+  effect(KILL cr);\n+  format %{ \"rolq    $dst, $shift\" %}\n+  ins_encode %{\n+    __ rolq($dst$$Register, $shift$$constant);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+instruct rolL_immI8(rRegL dst, rRegL src, immI8 shift)\n+%{\n+  predicate(!UseAPX && VM_Version::supports_bmi2() && n->bottom_type()->basic_type() == T_LONG);\n+  match(Set dst (RotateLeft src shift));\n+  format %{ \"rolxq   $dst, $src, $shift\" %}\n+  ins_encode %{\n+    int shift = 64 - ($shift$$constant & 63);\n+    __ rorxq($dst$$Register, $src$$Register, shift);\n+  %}\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+instruct rolL_mem_immI8(rRegL dst, memory src, immI8 shift)\n+%{\n+  predicate(VM_Version::supports_bmi2() && n->bottom_type()->basic_type() == T_LONG);\n+  match(Set dst (RotateLeft (LoadL src) shift));\n+  ins_cost(175);\n+  format %{ \"rolxq   $dst, $src, $shift\" %}\n+  ins_encode %{\n+    int shift = 64 - ($shift$$constant & 63);\n+    __ rorxq($dst$$Register, $src$$Address, shift);\n+  %}\n+  ins_pipe(ialu_reg_mem);\n+%}\n+\n+\/\/ Rotate Left by variable\n+instruct rolL_rReg_Var(rRegL dst, rcx_RegI shift, rFlagsReg cr)\n+%{\n+  predicate(!UseAPX && n->bottom_type()->basic_type() == T_LONG);\n+  match(Set dst (RotateLeft dst shift));\n+  effect(KILL cr);\n+  format %{ \"rolq    $dst, $shift\" %}\n+  ins_encode %{\n+    __ rolq($dst$$Register);\n+  %}\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+\/\/ Rotate Left by variable\n+instruct rolL_rReg_Var_ndd(rRegL dst, rRegL src, rcx_RegI shift, rFlagsReg cr)\n+%{\n+  predicate(UseAPX && n->bottom_type()->basic_type() == T_LONG);\n+  match(Set dst (RotateLeft src shift));\n+  effect(KILL cr);\n+\n+  format %{ \"erolq    $dst, $src, $shift\\t# rotate left(long ndd)\" %}\n+  ins_encode %{\n+    __ erolq($dst$$Register, $src$$Register, false);\n+  %}\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+\/\/ Rotate Right by constant.\n+instruct rorL_immI8_legacy(rRegL dst, immI8 shift, rFlagsReg cr)\n+%{\n+  predicate(!VM_Version::supports_bmi2() && n->bottom_type()->basic_type() == T_LONG);\n+  match(Set dst (RotateRight dst shift));\n+  effect(KILL cr);\n+  format %{ \"rorq    $dst, $shift\" %}\n+  ins_encode %{\n+    __ rorq($dst$$Register, $shift$$constant);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+\/\/ Rotate Right by constant\n+instruct rorL_immI8(rRegL dst, rRegL src, immI8 shift)\n+%{\n+  predicate(VM_Version::supports_bmi2() && n->bottom_type()->basic_type() == T_LONG);\n+  match(Set dst (RotateRight src shift));\n+  format %{ \"rorxq   $dst, $src, $shift\" %}\n+  ins_encode %{\n+    __ rorxq($dst$$Register, $src$$Register, $shift$$constant);\n+  %}\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+instruct rorL_mem_immI8(rRegL dst, memory src, immI8 shift)\n+%{\n+  predicate(VM_Version::supports_bmi2() && n->bottom_type()->basic_type() == T_LONG);\n+  match(Set dst (RotateRight (LoadL src) shift));\n+  ins_cost(175);\n+  format %{ \"rorxq   $dst, $src, $shift\" %}\n+  ins_encode %{\n+    __ rorxq($dst$$Register, $src$$Address, $shift$$constant);\n+  %}\n+  ins_pipe(ialu_reg_mem);\n+%}\n+\n+\/\/ Rotate Right by variable\n+instruct rorL_rReg_Var(rRegL dst, rcx_RegI shift, rFlagsReg cr)\n+%{\n+  predicate(!UseAPX && n->bottom_type()->basic_type() == T_LONG);\n+  match(Set dst (RotateRight dst shift));\n+  effect(KILL cr);\n+  format %{ \"rorq    $dst, $shift\" %}\n+  ins_encode %{\n+    __ rorq($dst$$Register);\n+  %}\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+\/\/ Rotate Right by variable\n+instruct rorL_rReg_Var_ndd(rRegL dst, rRegL src, rcx_RegI shift, rFlagsReg cr)\n+%{\n+  predicate(UseAPX && n->bottom_type()->basic_type() == T_LONG);\n+  match(Set dst (RotateRight src shift));\n+  effect(KILL cr);\n+\n+  format %{ \"erorq    $dst, $src, $shift\\t# rotate right(long ndd)\" %}\n+  ins_encode %{\n+    __ erorq($dst$$Register, $src$$Register, false);\n+  %}\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+\/\/----------------------------- CompressBits\/ExpandBits ------------------------\n+\n+instruct compressBitsL_reg(rRegL dst, rRegL src, rRegL mask) %{\n+  predicate(n->bottom_type()->isa_long());\n+  match(Set dst (CompressBits src mask));\n+  format %{ \"pextq  $dst, $src, $mask\\t! parallel bit extract\" %}\n+  ins_encode %{\n+    __ pextq($dst$$Register, $src$$Register, $mask$$Register);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct expandBitsL_reg(rRegL dst, rRegL src, rRegL mask) %{\n+  predicate(n->bottom_type()->isa_long());\n+  match(Set dst (ExpandBits src mask));\n+  format %{ \"pdepq  $dst, $src, $mask\\t! parallel bit deposit\" %}\n+  ins_encode %{\n+    __ pdepq($dst$$Register, $src$$Register, $mask$$Register);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct compressBitsL_mem(rRegL dst, rRegL src, memory mask) %{\n+  predicate(n->bottom_type()->isa_long());\n+  match(Set dst (CompressBits src (LoadL mask)));\n+  format %{ \"pextq  $dst, $src, $mask\\t! parallel bit extract\" %}\n+  ins_encode %{\n+    __ pextq($dst$$Register, $src$$Register, $mask$$Address);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct expandBitsL_mem(rRegL dst, rRegL src, memory mask) %{\n+  predicate(n->bottom_type()->isa_long());\n+  match(Set dst (ExpandBits src (LoadL mask)));\n+  format %{ \"pdepq  $dst, $src, $mask\\t! parallel bit deposit\" %}\n+  ins_encode %{\n+    __ pdepq($dst$$Register, $src$$Register, $mask$$Address);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+\n+\/\/ Logical Instructions\n+\n+\/\/ Integer Logical Instructions\n+\n+\/\/ And Instructions\n+\/\/ And Register with Register\n+instruct andI_rReg(rRegI dst, rRegI src, rFlagsReg cr)\n+%{\n+  predicate(!UseAPX);\n+  match(Set dst (AndI dst src));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);\n+\n+  format %{ \"andl    $dst, $src\\t# int\" %}\n+  ins_encode %{\n+    __ andl($dst$$Register, $src$$Register);\n+  %}\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+\/\/ And Register with Register using New Data Destination (NDD)\n+instruct andI_rReg_ndd(rRegI dst, rRegI src1, rRegI src2, rFlagsReg cr)\n+%{\n+  predicate(UseAPX);\n+  match(Set dst (AndI src1 src2));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);\n+\n+  format %{ \"eandl     $dst, $src1, $src2\\t# int ndd\" %}\n+  ins_encode %{\n+    __ eandl($dst$$Register, $src1$$Register, $src2$$Register, false);\n+\n+  %}\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+\/\/ And Register with Immediate 255\n+instruct andI_rReg_imm255(rRegI dst, rRegI src, immI_255 mask)\n+%{\n+  match(Set dst (AndI src mask));\n+\n+  format %{ \"movzbl  $dst, $src\\t# int & 0xFF\" %}\n+  ins_encode %{\n+    __ movzbl($dst$$Register, $src$$Register);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+\/\/ And Register with Immediate 255 and promote to long\n+instruct andI2L_rReg_imm255(rRegL dst, rRegI src, immI_255 mask)\n+%{\n+  match(Set dst (ConvI2L (AndI src mask)));\n+\n+  format %{ \"movzbl  $dst, $src\\t# int & 0xFF -> long\" %}\n+  ins_encode %{\n+    __ movzbl($dst$$Register, $src$$Register);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+\/\/ And Register with Immediate 65535\n+instruct andI_rReg_imm65535(rRegI dst, rRegI src, immI_65535 mask)\n+%{\n+  match(Set dst (AndI src mask));\n+\n+  format %{ \"movzwl  $dst, $src\\t# int & 0xFFFF\" %}\n+  ins_encode %{\n+    __ movzwl($dst$$Register, $src$$Register);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+\/\/ And Register with Immediate 65535 and promote to long\n+instruct andI2L_rReg_imm65535(rRegL dst, rRegI src, immI_65535 mask)\n+%{\n+  match(Set dst (ConvI2L (AndI src mask)));\n+\n+  format %{ \"movzwl  $dst, $src\\t# int & 0xFFFF -> long\" %}\n+  ins_encode %{\n+    __ movzwl($dst$$Register, $src$$Register);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+\/\/ Can skip int2long conversions after AND with small bitmask\n+instruct convI2LAndI_reg_immIbitmask(rRegL dst, rRegI src,  immI_Pow2M1 mask, rRegI tmp, rFlagsReg cr)\n+%{\n+  predicate(VM_Version::supports_bmi2());\n+  ins_cost(125);\n+  effect(TEMP tmp, KILL cr);\n+  match(Set dst (ConvI2L (AndI src mask)));\n+  format %{ \"bzhiq $dst, $src, $mask \\t# using $tmp as TEMP, int &  immI_Pow2M1 -> long\" %}\n+  ins_encode %{\n+    __ movl($tmp$$Register, exact_log2($mask$$constant + 1));\n+    __ bzhiq($dst$$Register, $src$$Register, $tmp$$Register);\n+  %}\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+\/\/ And Register with Immediate\n+instruct andI_rReg_imm(rRegI dst, immI src, rFlagsReg cr)\n+%{\n+  predicate(!UseAPX);\n+  match(Set dst (AndI dst src));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);\n+\n+  format %{ \"andl    $dst, $src\\t# int\" %}\n+  ins_encode %{\n+    __ andl($dst$$Register, $src$$constant);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+instruct andI_rReg_rReg_imm_ndd(rRegI dst, rRegI src1, immI src2, rFlagsReg cr)\n+%{\n+  predicate(UseAPX);\n+  match(Set dst (AndI src1 src2));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);\n+\n+  format %{ \"eandl    $dst, $src1, $src2\\t# int ndd\" %}\n+  ins_encode %{\n+    __ eandl($dst$$Register, $src1$$Register, $src2$$constant, false);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+instruct andI_rReg_mem_imm_ndd(rRegI dst, memory src1, immI src2, rFlagsReg cr)\n+%{\n+  predicate(UseAPX);\n+  match(Set dst (AndI (LoadI src1) src2));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);\n+\n+  format %{ \"eandl    $dst, $src1, $src2\\t# int ndd\" %}\n+  ins_encode %{\n+    __ eandl($dst$$Register, $src1$$Address, $src2$$constant, false);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+\/\/ And Register with Memory\n+instruct andI_rReg_mem(rRegI dst, memory src, rFlagsReg cr)\n+%{\n+  predicate(!UseAPX);\n+  match(Set dst (AndI dst (LoadI src)));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);\n+\n+  ins_cost(150);\n+  format %{ \"andl    $dst, $src\\t# int\" %}\n+  ins_encode %{\n+    __ andl($dst$$Register, $src$$Address);\n+  %}\n+  ins_pipe(ialu_reg_mem);\n+%}\n+\n+instruct andI_rReg_rReg_mem_ndd(rRegI dst, rRegI src1, memory src2, rFlagsReg cr)\n+%{\n+  predicate(UseAPX);\n+  match(Set dst (AndI src1 (LoadI src2)));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);\n+\n+  ins_cost(150);\n+  format %{ \"eandl    $dst, $src1, $src2\\t# int ndd\" %}\n+  ins_encode %{\n+    __ eandl($dst$$Register, $src1$$Register, $src2$$Address, false);\n+  %}\n+  ins_pipe(ialu_reg_mem);\n+%}\n+\n+\/\/ And Memory with Register\n+instruct andB_mem_rReg(memory dst, rRegI src, rFlagsReg cr)\n+%{\n+  match(Set dst (StoreB dst (AndI (LoadB dst) src)));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);\n+\n+  ins_cost(150);\n+  format %{ \"andb    $dst, $src\\t# byte\" %}\n+  ins_encode %{\n+    __ andb($dst$$Address, $src$$Register);\n+  %}\n+  ins_pipe(ialu_mem_reg);\n+%}\n+\n+instruct andI_mem_rReg(memory dst, rRegI src, rFlagsReg cr)\n+%{\n+  match(Set dst (StoreI dst (AndI (LoadI dst) src)));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);\n+\n+  ins_cost(150);\n+  format %{ \"andl    $dst, $src\\t# int\" %}\n+  ins_encode %{\n+    __ andl($dst$$Address, $src$$Register);\n+  %}\n+  ins_pipe(ialu_mem_reg);\n+%}\n+\n+\/\/ And Memory with Immediate\n+instruct andI_mem_imm(memory dst, immI src, rFlagsReg cr)\n+%{\n+  match(Set dst (StoreI dst (AndI (LoadI dst) src)));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);\n+\n+  ins_cost(125);\n+  format %{ \"andl    $dst, $src\\t# int\" %}\n+  ins_encode %{\n+    __ andl($dst$$Address, $src$$constant);\n+  %}\n+  ins_pipe(ialu_mem_imm);\n+%}\n+\n+\/\/ BMI1 instructions\n+instruct andnI_rReg_rReg_mem(rRegI dst, rRegI src1, memory src2, immI_M1 minus_1, rFlagsReg cr) %{\n+  match(Set dst (AndI (XorI src1 minus_1) (LoadI src2)));\n+  predicate(UseBMI1Instructions);\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);\n+\n+  ins_cost(125);\n+  format %{ \"andnl  $dst, $src1, $src2\" %}\n+\n+  ins_encode %{\n+    __ andnl($dst$$Register, $src1$$Register, $src2$$Address);\n+  %}\n+  ins_pipe(ialu_reg_mem);\n+%}\n+\n+instruct andnI_rReg_rReg_rReg(rRegI dst, rRegI src1, rRegI src2, immI_M1 minus_1, rFlagsReg cr) %{\n+  match(Set dst (AndI (XorI src1 minus_1) src2));\n+  predicate(UseBMI1Instructions);\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);\n+\n+  format %{ \"andnl  $dst, $src1, $src2\" %}\n+\n+  ins_encode %{\n+    __ andnl($dst$$Register, $src1$$Register, $src2$$Register);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+instruct blsiI_rReg_rReg(rRegI dst, rRegI src, immI_0 imm_zero, rFlagsReg cr) %{\n+  match(Set dst (AndI (SubI imm_zero src) src));\n+  predicate(UseBMI1Instructions);\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_clears_overflow_flag);\n+\n+  format %{ \"blsil  $dst, $src\" %}\n+\n+  ins_encode %{\n+    __ blsil($dst$$Register, $src$$Register);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+instruct blsiI_rReg_mem(rRegI dst, memory src, immI_0 imm_zero, rFlagsReg cr) %{\n+  match(Set dst (AndI (SubI imm_zero (LoadI src) ) (LoadI src) ));\n+  predicate(UseBMI1Instructions);\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_clears_overflow_flag);\n+\n+  ins_cost(125);\n+  format %{ \"blsil  $dst, $src\" %}\n+\n+  ins_encode %{\n+    __ blsil($dst$$Register, $src$$Address);\n+  %}\n+  ins_pipe(ialu_reg_mem);\n+%}\n+\n+instruct blsmskI_rReg_mem(rRegI dst, memory src, immI_M1 minus_1, rFlagsReg cr)\n+%{\n+  match(Set dst (XorI (AddI (LoadI src) minus_1) (LoadI src) ) );\n+  predicate(UseBMI1Instructions);\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_sign_flag, PD::Flag_clears_zero_flag, PD::Flag_clears_overflow_flag);\n+\n+  ins_cost(125);\n+  format %{ \"blsmskl $dst, $src\" %}\n+\n+  ins_encode %{\n+    __ blsmskl($dst$$Register, $src$$Address);\n+  %}\n+  ins_pipe(ialu_reg_mem);\n+%}\n+\n+instruct blsmskI_rReg_rReg(rRegI dst, rRegI src, immI_M1 minus_1, rFlagsReg cr)\n+%{\n+  match(Set dst (XorI (AddI src minus_1) src));\n+  predicate(UseBMI1Instructions);\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_sign_flag, PD::Flag_clears_zero_flag, PD::Flag_clears_overflow_flag);\n+\n+  format %{ \"blsmskl $dst, $src\" %}\n+\n+  ins_encode %{\n+    __ blsmskl($dst$$Register, $src$$Register);\n+  %}\n+\n+  ins_pipe(ialu_reg);\n+%}\n+\n+instruct blsrI_rReg_rReg(rRegI dst, rRegI src, immI_M1 minus_1, rFlagsReg cr)\n+%{\n+  match(Set dst (AndI (AddI src minus_1) src) );\n+  predicate(UseBMI1Instructions);\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_clears_overflow_flag);\n+\n+  format %{ \"blsrl  $dst, $src\" %}\n+\n+  ins_encode %{\n+    __ blsrl($dst$$Register, $src$$Register);\n+  %}\n+\n+  ins_pipe(ialu_reg_mem);\n+%}\n+\n+instruct blsrI_rReg_mem(rRegI dst, memory src, immI_M1 minus_1, rFlagsReg cr)\n+%{\n+  match(Set dst (AndI (AddI (LoadI src) minus_1) (LoadI src) ) );\n+  predicate(UseBMI1Instructions);\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_clears_overflow_flag);\n+\n+  ins_cost(125);\n+  format %{ \"blsrl  $dst, $src\" %}\n+\n+  ins_encode %{\n+    __ blsrl($dst$$Register, $src$$Address);\n+  %}\n+\n+  ins_pipe(ialu_reg);\n+%}\n+\n+\/\/ Or Instructions\n+\/\/ Or Register with Register\n+instruct orI_rReg(rRegI dst, rRegI src, rFlagsReg cr)\n+%{\n+  predicate(!UseAPX);\n+  match(Set dst (OrI dst src));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);\n+\n+  format %{ \"orl     $dst, $src\\t# int\" %}\n+  ins_encode %{\n+    __ orl($dst$$Register, $src$$Register);\n+  %}\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+\/\/ Or Register with Register using New Data Destination (NDD)\n+instruct orI_rReg_ndd(rRegI dst, rRegI src1, rRegI src2, rFlagsReg cr)\n+%{\n+  predicate(UseAPX);\n+  match(Set dst (OrI src1 src2));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);\n+\n+  format %{ \"eorl     $dst, $src1, $src2\\t# int ndd\" %}\n+  ins_encode %{\n+    __ eorl($dst$$Register, $src1$$Register, $src2$$Register, false);\n+  %}\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+\/\/ Or Register with Immediate\n+instruct orI_rReg_imm(rRegI dst, immI src, rFlagsReg cr)\n+%{\n+  predicate(!UseAPX);\n+  match(Set dst (OrI dst src));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);\n+\n+  format %{ \"orl     $dst, $src\\t# int\" %}\n+  ins_encode %{\n+    __ orl($dst$$Register, $src$$constant);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+instruct orI_rReg_rReg_imm_ndd(rRegI dst, rRegI src1, immI src2, rFlagsReg cr)\n+%{\n+  predicate(UseAPX);\n+  match(Set dst (OrI src1 src2));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);\n+\n+  format %{ \"eorl     $dst, $src1, $src2\\t# int ndd\" %}\n+  ins_encode %{\n+    __ eorl($dst$$Register, $src1$$Register, $src2$$constant, false);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+instruct orI_rReg_imm_rReg_ndd(rRegI dst, immI src1, rRegI src2, rFlagsReg cr)\n+%{\n+  predicate(UseAPX);\n+  match(Set dst (OrI src1 src2));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);\n+\n+  format %{ \"eorl     $dst, $src2, $src1\\t# int ndd\" %}\n+  ins_encode %{\n+    __ eorl($dst$$Register, $src2$$Register, $src1$$constant, false);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+instruct orI_rReg_mem_imm_ndd(rRegI dst, memory src1, immI src2, rFlagsReg cr)\n+%{\n+  predicate(UseAPX);\n+  match(Set dst (OrI (LoadI src1) src2));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);\n+\n+  format %{ \"eorl     $dst, $src1, $src2\\t# int ndd\" %}\n+  ins_encode %{\n+    __ eorl($dst$$Register, $src1$$Address, $src2$$constant, false);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+\/\/ Or Register with Memory\n+instruct orI_rReg_mem(rRegI dst, memory src, rFlagsReg cr)\n+%{\n+  predicate(!UseAPX);\n+  match(Set dst (OrI dst (LoadI src)));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);\n+\n+  ins_cost(150);\n+  format %{ \"orl     $dst, $src\\t# int\" %}\n+  ins_encode %{\n+    __ orl($dst$$Register, $src$$Address);\n+  %}\n+  ins_pipe(ialu_reg_mem);\n+%}\n+\n+instruct orI_rReg_rReg_mem_ndd(rRegI dst, rRegI src1, memory src2, rFlagsReg cr)\n+%{\n+  predicate(UseAPX);\n+  match(Set dst (OrI src1 (LoadI src2)));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);\n+\n+  ins_cost(150);\n+  format %{ \"eorl     $dst, $src1, $src2\\t# int ndd\" %}\n+  ins_encode %{\n+    __ eorl($dst$$Register, $src1$$Register, $src2$$Address, false);\n+  %}\n+  ins_pipe(ialu_reg_mem);\n+%}\n+\n+\/\/ Or Memory with Register\n+instruct orB_mem_rReg(memory dst, rRegI src, rFlagsReg cr)\n+%{\n+  match(Set dst (StoreB dst (OrI (LoadB dst) src)));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);\n+\n+  ins_cost(150);\n+  format %{ \"orb    $dst, $src\\t# byte\" %}\n+  ins_encode %{\n+    __ orb($dst$$Address, $src$$Register);\n+  %}\n+  ins_pipe(ialu_mem_reg);\n+%}\n+\n+instruct orI_mem_rReg(memory dst, rRegI src, rFlagsReg cr)\n+%{\n+  match(Set dst (StoreI dst (OrI (LoadI dst) src)));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);\n+\n+  ins_cost(150);\n+  format %{ \"orl     $dst, $src\\t# int\" %}\n+  ins_encode %{\n+    __ orl($dst$$Address, $src$$Register);\n+  %}\n+  ins_pipe(ialu_mem_reg);\n+%}\n+\n+\/\/ Or Memory with Immediate\n+instruct orI_mem_imm(memory dst, immI src, rFlagsReg cr)\n+%{\n+  match(Set dst (StoreI dst (OrI (LoadI dst) src)));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);\n+\n+  ins_cost(125);\n+  format %{ \"orl     $dst, $src\\t# int\" %}\n+  ins_encode %{\n+    __ orl($dst$$Address, $src$$constant);\n+  %}\n+  ins_pipe(ialu_mem_imm);\n+%}\n+\n+\/\/ Xor Instructions\n+\/\/ Xor Register with Register\n+instruct xorI_rReg(rRegI dst, rRegI src, rFlagsReg cr)\n+%{\n+  predicate(!UseAPX);\n+  match(Set dst (XorI dst src));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);\n+\n+  format %{ \"xorl    $dst, $src\\t# int\" %}\n+  ins_encode %{\n+    __ xorl($dst$$Register, $src$$Register);\n+  %}\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+\/\/ Xor Register with Register using New Data Destination (NDD)\n+instruct xorI_rReg_ndd(rRegI dst, rRegI src1, rRegI src2, rFlagsReg cr)\n+%{\n+  predicate(UseAPX);\n+  match(Set dst (XorI src1 src2));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);\n+\n+  format %{ \"exorl    $dst, $src1, $src2\\t# int ndd\" %}\n+  ins_encode %{\n+    __ exorl($dst$$Register, $src1$$Register, $src2$$Register, false);\n+  %}\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+\/\/ Xor Register with Immediate -1\n+instruct xorI_rReg_im1(rRegI dst, immI_M1 imm)\n+%{\n+  predicate(!UseAPX);\n+  match(Set dst (XorI dst imm));\n+\n+  format %{ \"notl    $dst\" %}\n+  ins_encode %{\n+     __ notl($dst$$Register);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+instruct xorI_rReg_im1_ndd(rRegI dst, rRegI src, immI_M1 imm)\n+%{\n+  match(Set dst (XorI src imm));\n+  predicate(UseAPX);\n+\n+  format %{ \"enotl    $dst, $src\" %}\n+  ins_encode %{\n+     __ enotl($dst$$Register, $src$$Register);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+\/\/ Xor Register with Immediate\n+instruct xorI_rReg_imm(rRegI dst, immI src, rFlagsReg cr)\n+%{\n+  \/\/ Strict predicate check to make selection of xorI_rReg_im1 cost agnostic if immI src is -1.\n+  predicate(!UseAPX && n->in(2)->bottom_type()->is_int()->get_con() != -1);\n+  match(Set dst (XorI dst src));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);\n+\n+  format %{ \"xorl    $dst, $src\\t# int\" %}\n+  ins_encode %{\n+    __ xorl($dst$$Register, $src$$constant);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+instruct xorI_rReg_rReg_imm_ndd(rRegI dst, rRegI src1, immI src2, rFlagsReg cr)\n+%{\n+  \/\/ Strict predicate check to make selection of xorI_rReg_im1_ndd cost agnostic if immI src2 is -1.\n+  predicate(UseAPX && n->in(2)->bottom_type()->is_int()->get_con() != -1);\n+  match(Set dst (XorI src1 src2));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);\n+\n+  format %{ \"exorl    $dst, $src1, $src2\\t# int ndd\" %}\n+  ins_encode %{\n+    __ exorl($dst$$Register, $src1$$Register, $src2$$constant, false);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+\/\/ Xor Memory with Immediate\n+instruct xorI_rReg_mem_imm_ndd(rRegI dst, memory src1, immI src2, rFlagsReg cr)\n+%{\n+  predicate(UseAPX);\n+  match(Set dst (XorI (LoadI src1) src2));\n+  effect(KILL cr);\n+  ins_cost(150);\n+  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);\n+\n+  format %{ \"exorl    $dst, $src1, $src2\\t# int ndd\" %}\n+  ins_encode %{\n+    __ exorl($dst$$Register, $src1$$Address, $src2$$constant, false);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+\/\/ Xor Register with Memory\n+instruct xorI_rReg_mem(rRegI dst, memory src, rFlagsReg cr)\n+%{\n+  predicate(!UseAPX);\n+  match(Set dst (XorI dst (LoadI src)));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);\n+\n+  ins_cost(150);\n+  format %{ \"xorl    $dst, $src\\t# int\" %}\n+  ins_encode %{\n+    __ xorl($dst$$Register, $src$$Address);\n+  %}\n+  ins_pipe(ialu_reg_mem);\n+%}\n+\n+instruct xorI_rReg_rReg_mem_ndd(rRegI dst, rRegI src1, memory src2, rFlagsReg cr)\n+%{\n+  predicate(UseAPX);\n+  match(Set dst (XorI src1 (LoadI src2)));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);\n+\n+  ins_cost(150);\n+  format %{ \"exorl    $dst, $src1, $src2\\t# int ndd\" %}\n+  ins_encode %{\n+    __ exorl($dst$$Register, $src1$$Register, $src2$$Address, false);\n+  %}\n+  ins_pipe(ialu_reg_mem);\n+%}\n+\n+\/\/ Xor Memory with Register\n+instruct xorB_mem_rReg(memory dst, rRegI src, rFlagsReg cr)\n+%{\n+  match(Set dst (StoreB dst (XorI (LoadB dst) src)));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);\n+\n+  ins_cost(150);\n+  format %{ \"xorb    $dst, $src\\t# byte\" %}\n+  ins_encode %{\n+    __ xorb($dst$$Address, $src$$Register);\n+  %}\n+  ins_pipe(ialu_mem_reg);\n+%}\n+\n+instruct xorI_mem_rReg(memory dst, rRegI src, rFlagsReg cr)\n+%{\n+  match(Set dst (StoreI dst (XorI (LoadI dst) src)));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);\n+\n+  ins_cost(150);\n+  format %{ \"xorl    $dst, $src\\t# int\" %}\n+  ins_encode %{\n+    __ xorl($dst$$Address, $src$$Register);\n+  %}\n+  ins_pipe(ialu_mem_reg);\n+%}\n+\n+\/\/ Xor Memory with Immediate\n+instruct xorI_mem_imm(memory dst, immI src, rFlagsReg cr)\n+%{\n+  match(Set dst (StoreI dst (XorI (LoadI dst) src)));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);\n+\n+  ins_cost(125);\n+  format %{ \"xorl    $dst, $src\\t# int\" %}\n+  ins_encode %{\n+    __ xorl($dst$$Address, $src$$constant);\n+  %}\n+  ins_pipe(ialu_mem_imm);\n+%}\n+\n+\n+\/\/ Long Logical Instructions\n+\n+\/\/ And Instructions\n+\/\/ And Register with Register\n+instruct andL_rReg(rRegL dst, rRegL src, rFlagsReg cr)\n+%{\n+  predicate(!UseAPX);\n+  match(Set dst (AndL dst src));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);\n+\n+  format %{ \"andq    $dst, $src\\t# long\" %}\n+  ins_encode %{\n+    __ andq($dst$$Register, $src$$Register);\n+  %}\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+\/\/ And Register with Register using New Data Destination (NDD)\n+instruct andL_rReg_ndd(rRegL dst, rRegL src1, rRegL src2, rFlagsReg cr)\n+%{\n+  predicate(UseAPX);\n+  match(Set dst (AndL src1 src2));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);\n+\n+  format %{ \"eandq     $dst, $src1, $src2\\t# long ndd\" %}\n+  ins_encode %{\n+    __ eandq($dst$$Register, $src1$$Register, $src2$$Register, false);\n+\n+  %}\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+\/\/ And Register with Immediate 255\n+instruct andL_rReg_imm255(rRegL dst, rRegL src, immL_255 mask)\n+%{\n+  match(Set dst (AndL src mask));\n+\n+  format %{ \"movzbl  $dst, $src\\t# long & 0xFF\" %}\n+  ins_encode %{\n+    \/\/ movzbl zeroes out the upper 32-bit and does not need REX.W\n+    __ movzbl($dst$$Register, $src$$Register);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+\/\/ And Register with Immediate 65535\n+instruct andL_rReg_imm65535(rRegL dst, rRegL src, immL_65535 mask)\n+%{\n+  match(Set dst (AndL src mask));\n+\n+  format %{ \"movzwl  $dst, $src\\t# long & 0xFFFF\" %}\n+  ins_encode %{\n+    \/\/ movzwl zeroes out the upper 32-bit and does not need REX.W\n+    __ movzwl($dst$$Register, $src$$Register);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+\/\/ And Register with Immediate\n+instruct andL_rReg_imm(rRegL dst, immL32 src, rFlagsReg cr)\n+%{\n+  predicate(!UseAPX);\n+  match(Set dst (AndL dst src));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);\n+\n+  format %{ \"andq    $dst, $src\\t# long\" %}\n+  ins_encode %{\n+    __ andq($dst$$Register, $src$$constant);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+instruct andL_rReg_rReg_imm_ndd(rRegL dst, rRegL src1, immL32 src2, rFlagsReg cr)\n+%{\n+  predicate(UseAPX);\n+  match(Set dst (AndL src1 src2));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);\n+\n+  format %{ \"eandq    $dst, $src1, $src2\\t# long ndd\" %}\n+  ins_encode %{\n+    __ eandq($dst$$Register, $src1$$Register, $src2$$constant, false);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+instruct andL_rReg_mem_imm_ndd(rRegL dst, memory src1, immL32 src2, rFlagsReg cr)\n+%{\n+  predicate(UseAPX);\n+  match(Set dst (AndL (LoadL src1) src2));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);\n+\n+  format %{ \"eandq    $dst, $src1, $src2\\t# long ndd\" %}\n+  ins_encode %{\n+    __ eandq($dst$$Register, $src1$$Address, $src2$$constant, false);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+\/\/ And Register with Memory\n+instruct andL_rReg_mem(rRegL dst, memory src, rFlagsReg cr)\n+%{\n+  predicate(!UseAPX);\n+  match(Set dst (AndL dst (LoadL src)));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);\n+\n+  ins_cost(150);\n+  format %{ \"andq    $dst, $src\\t# long\" %}\n+  ins_encode %{\n+    __ andq($dst$$Register, $src$$Address);\n+  %}\n+  ins_pipe(ialu_reg_mem);\n+%}\n+\n+instruct andL_rReg_rReg_mem_ndd(rRegL dst, rRegL src1, memory src2, rFlagsReg cr)\n+%{\n+  predicate(UseAPX);\n+  match(Set dst (AndL src1 (LoadL src2)));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);\n+\n+  ins_cost(150);\n+  format %{ \"eandq    $dst, $src1, $src2\\t# long ndd\" %}\n+  ins_encode %{\n+    __ eandq($dst$$Register, $src1$$Register, $src2$$Address, false);\n+  %}\n+  ins_pipe(ialu_reg_mem);\n+%}\n+\n+\/\/ And Memory with Register\n+instruct andL_mem_rReg(memory dst, rRegL src, rFlagsReg cr)\n+%{\n+  match(Set dst (StoreL dst (AndL (LoadL dst) src)));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);\n+\n+  ins_cost(150);\n+  format %{ \"andq    $dst, $src\\t# long\" %}\n+  ins_encode %{\n+    __ andq($dst$$Address, $src$$Register);\n+  %}\n+  ins_pipe(ialu_mem_reg);\n+%}\n+\n+\/\/ And Memory with Immediate\n+instruct andL_mem_imm(memory dst, immL32 src, rFlagsReg cr)\n+%{\n+  match(Set dst (StoreL dst (AndL (LoadL dst) src)));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);\n+\n+  ins_cost(125);\n+  format %{ \"andq    $dst, $src\\t# long\" %}\n+  ins_encode %{\n+    __ andq($dst$$Address, $src$$constant);\n+  %}\n+  ins_pipe(ialu_mem_imm);\n+%}\n+\n+instruct btrL_mem_imm(memory dst, immL_NotPow2 con, rFlagsReg cr)\n+%{\n+  \/\/ con should be a pure 64-bit immediate given that not(con) is a power of 2\n+  \/\/ because AND\/OR works well enough for 8\/32-bit values.\n+  predicate(log2i_graceful(~n->in(3)->in(2)->get_long()) > 30);\n+\n+  match(Set dst (StoreL dst (AndL (LoadL dst) con)));\n+  effect(KILL cr);\n+\n+  ins_cost(125);\n+  format %{ \"btrq    $dst, log2(not($con))\\t# long\" %}\n+  ins_encode %{\n+    __ btrq($dst$$Address, log2i_exact((julong)~$con$$constant));\n+  %}\n+  ins_pipe(ialu_mem_imm);\n+%}\n+\n+\/\/ BMI1 instructions\n+instruct andnL_rReg_rReg_mem(rRegL dst, rRegL src1, memory src2, immL_M1 minus_1, rFlagsReg cr) %{\n+  match(Set dst (AndL (XorL src1 minus_1) (LoadL src2)));\n+  predicate(UseBMI1Instructions);\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);\n+\n+  ins_cost(125);\n+  format %{ \"andnq  $dst, $src1, $src2\" %}\n+\n+  ins_encode %{\n+    __ andnq($dst$$Register, $src1$$Register, $src2$$Address);\n+  %}\n+  ins_pipe(ialu_reg_mem);\n+%}\n+\n+instruct andnL_rReg_rReg_rReg(rRegL dst, rRegL src1, rRegL src2, immL_M1 minus_1, rFlagsReg cr) %{\n+  match(Set dst (AndL (XorL src1 minus_1) src2));\n+  predicate(UseBMI1Instructions);\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);\n+\n+  format %{ \"andnq  $dst, $src1, $src2\" %}\n+\n+  ins_encode %{\n+  __ andnq($dst$$Register, $src1$$Register, $src2$$Register);\n+  %}\n+  ins_pipe(ialu_reg_mem);\n+%}\n+\n+instruct blsiL_rReg_rReg(rRegL dst, rRegL src, immL0 imm_zero, rFlagsReg cr) %{\n+  match(Set dst (AndL (SubL imm_zero src) src));\n+  predicate(UseBMI1Instructions);\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_clears_overflow_flag);\n+\n+  format %{ \"blsiq  $dst, $src\" %}\n+\n+  ins_encode %{\n+    __ blsiq($dst$$Register, $src$$Register);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+instruct blsiL_rReg_mem(rRegL dst, memory src, immL0 imm_zero, rFlagsReg cr) %{\n+  match(Set dst (AndL (SubL imm_zero (LoadL src) ) (LoadL src) ));\n+  predicate(UseBMI1Instructions);\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_clears_overflow_flag);\n+\n+  ins_cost(125);\n+  format %{ \"blsiq  $dst, $src\" %}\n+\n+  ins_encode %{\n+    __ blsiq($dst$$Register, $src$$Address);\n+  %}\n+  ins_pipe(ialu_reg_mem);\n+%}\n+\n+instruct blsmskL_rReg_mem(rRegL dst, memory src, immL_M1 minus_1, rFlagsReg cr)\n+%{\n+  match(Set dst (XorL (AddL (LoadL src) minus_1) (LoadL src) ) );\n+  predicate(UseBMI1Instructions);\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_sign_flag, PD::Flag_clears_zero_flag, PD::Flag_clears_overflow_flag);\n+\n+  ins_cost(125);\n+  format %{ \"blsmskq $dst, $src\" %}\n+\n+  ins_encode %{\n+    __ blsmskq($dst$$Register, $src$$Address);\n+  %}\n+  ins_pipe(ialu_reg_mem);\n+%}\n+\n+instruct blsmskL_rReg_rReg(rRegL dst, rRegL src, immL_M1 minus_1, rFlagsReg cr)\n+%{\n+  match(Set dst (XorL (AddL src minus_1) src));\n+  predicate(UseBMI1Instructions);\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_sign_flag, PD::Flag_clears_zero_flag, PD::Flag_clears_overflow_flag);\n+\n+  format %{ \"blsmskq $dst, $src\" %}\n+\n+  ins_encode %{\n+    __ blsmskq($dst$$Register, $src$$Register);\n+  %}\n+\n+  ins_pipe(ialu_reg);\n+%}\n+\n+instruct blsrL_rReg_rReg(rRegL dst, rRegL src, immL_M1 minus_1, rFlagsReg cr)\n+%{\n+  match(Set dst (AndL (AddL src minus_1) src) );\n+  predicate(UseBMI1Instructions);\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_clears_overflow_flag);\n+\n+  format %{ \"blsrq  $dst, $src\" %}\n+\n+  ins_encode %{\n+    __ blsrq($dst$$Register, $src$$Register);\n+  %}\n+\n+  ins_pipe(ialu_reg);\n+%}\n+\n+instruct blsrL_rReg_mem(rRegL dst, memory src, immL_M1 minus_1, rFlagsReg cr)\n+%{\n+  match(Set dst (AndL (AddL (LoadL src) minus_1) (LoadL src)) );\n+  predicate(UseBMI1Instructions);\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_clears_overflow_flag);\n+\n+  ins_cost(125);\n+  format %{ \"blsrq  $dst, $src\" %}\n+\n+  ins_encode %{\n+    __ blsrq($dst$$Register, $src$$Address);\n+  %}\n+\n+  ins_pipe(ialu_reg);\n+%}\n+\n+\/\/ Or Instructions\n+\/\/ Or Register with Register\n+instruct orL_rReg(rRegL dst, rRegL src, rFlagsReg cr)\n+%{\n+  predicate(!UseAPX);\n+  match(Set dst (OrL dst src));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);\n+\n+  format %{ \"orq     $dst, $src\\t# long\" %}\n+  ins_encode %{\n+    __ orq($dst$$Register, $src$$Register);\n+  %}\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+\/\/ Or Register with Register using New Data Destination (NDD)\n+instruct orL_rReg_ndd(rRegL dst, rRegL src1, rRegL src2, rFlagsReg cr)\n+%{\n+  predicate(UseAPX);\n+  match(Set dst (OrL src1 src2));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);\n+\n+  format %{ \"eorq     $dst, $src1, $src2\\t# long ndd\" %}\n+  ins_encode %{\n+    __ eorq($dst$$Register, $src1$$Register, $src2$$Register, false);\n+\n+  %}\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+\/\/ Use any_RegP to match R15 (TLS register) without spilling.\n+instruct orL_rReg_castP2X(rRegL dst, any_RegP src, rFlagsReg cr) %{\n+  match(Set dst (OrL dst (CastP2X src)));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);\n+\n+  format %{ \"orq     $dst, $src\\t# long\" %}\n+  ins_encode %{\n+    __ orq($dst$$Register, $src$$Register);\n+  %}\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+instruct orL_rReg_castP2X_ndd(rRegL dst, any_RegP src1, any_RegP src2, rFlagsReg cr) %{\n+  match(Set dst (OrL src1 (CastP2X src2)));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);\n+\n+  format %{ \"eorq     $dst, $src1, $src2\\t# long ndd\" %}\n+  ins_encode %{\n+    __ eorq($dst$$Register, $src1$$Register, $src2$$Register, false);\n+  %}\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+\/\/ Or Register with Immediate\n+instruct orL_rReg_imm(rRegL dst, immL32 src, rFlagsReg cr)\n+%{\n+  predicate(!UseAPX);\n+  match(Set dst (OrL dst src));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);\n+\n+  format %{ \"orq     $dst, $src\\t# long\" %}\n+  ins_encode %{\n+    __ orq($dst$$Register, $src$$constant);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+instruct orL_rReg_rReg_imm_ndd(rRegL dst, rRegL src1, immL32 src2, rFlagsReg cr)\n+%{\n+  predicate(UseAPX);\n+  match(Set dst (OrL src1 src2));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);\n+\n+  format %{ \"eorq     $dst, $src1, $src2\\t# long ndd\" %}\n+  ins_encode %{\n+    __ eorq($dst$$Register, $src1$$Register, $src2$$constant, false);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+instruct orL_rReg_imm_rReg_ndd(rRegL dst, immL32 src1, rRegL src2, rFlagsReg cr)\n+%{\n+  predicate(UseAPX);\n+  match(Set dst (OrL src1 src2));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);\n+\n+  format %{ \"eorq     $dst, $src2, $src1\\t# long ndd\" %}\n+  ins_encode %{\n+    __ eorq($dst$$Register, $src2$$Register, $src1$$constant, false);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+\/\/ Or Memory with Immediate\n+instruct orL_rReg_mem_imm_ndd(rRegL dst, memory src1, immL32 src2, rFlagsReg cr)\n+%{\n+  predicate(UseAPX);\n+  match(Set dst (OrL (LoadL src1) src2));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);\n+\n+  format %{ \"eorq     $dst, $src1, $src2\\t# long ndd\" %}\n+  ins_encode %{\n+    __ eorq($dst$$Register, $src1$$Address, $src2$$constant, false);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+\/\/ Or Register with Memory\n+instruct orL_rReg_mem(rRegL dst, memory src, rFlagsReg cr)\n+%{\n+  predicate(!UseAPX);\n+  match(Set dst (OrL dst (LoadL src)));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);\n+\n+  ins_cost(150);\n+  format %{ \"orq     $dst, $src\\t# long\" %}\n+  ins_encode %{\n+    __ orq($dst$$Register, $src$$Address);\n+  %}\n+  ins_pipe(ialu_reg_mem);\n+%}\n+\n+instruct orL_rReg_rReg_mem_ndd(rRegL dst, rRegL src1, memory src2, rFlagsReg cr)\n+%{\n+  predicate(UseAPX);\n+  match(Set dst (OrL src1 (LoadL src2)));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);\n+\n+  ins_cost(150);\n+  format %{ \"eorq     $dst, $src1, $src2\\t# long ndd\" %}\n+  ins_encode %{\n+    __ eorq($dst$$Register, $src1$$Register, $src2$$Address, false);\n+  %}\n+  ins_pipe(ialu_reg_mem);\n+%}\n+\n+\/\/ Or Memory with Register\n+instruct orL_mem_rReg(memory dst, rRegL src, rFlagsReg cr)\n+%{\n+  match(Set dst (StoreL dst (OrL (LoadL dst) src)));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);\n+\n+  ins_cost(150);\n+  format %{ \"orq     $dst, $src\\t# long\" %}\n+  ins_encode %{\n+    __ orq($dst$$Address, $src$$Register);\n+  %}\n+  ins_pipe(ialu_mem_reg);\n+%}\n+\n+\/\/ Or Memory with Immediate\n+instruct orL_mem_imm(memory dst, immL32 src, rFlagsReg cr)\n+%{\n+  match(Set dst (StoreL dst (OrL (LoadL dst) src)));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);\n+\n+  ins_cost(125);\n+  format %{ \"orq     $dst, $src\\t# long\" %}\n+  ins_encode %{\n+    __ orq($dst$$Address, $src$$constant);\n+  %}\n+  ins_pipe(ialu_mem_imm);\n+%}\n+\n+instruct btsL_mem_imm(memory dst, immL_Pow2 con, rFlagsReg cr)\n+%{\n+  \/\/ con should be a pure 64-bit power of 2 immediate\n+  \/\/ because AND\/OR works well enough for 8\/32-bit values.\n+  predicate(log2i_graceful(n->in(3)->in(2)->get_long()) > 31);\n+\n+  match(Set dst (StoreL dst (OrL (LoadL dst) con)));\n+  effect(KILL cr);\n+\n+  ins_cost(125);\n+  format %{ \"btsq    $dst, log2($con)\\t# long\" %}\n+  ins_encode %{\n+    __ btsq($dst$$Address, log2i_exact((julong)$con$$constant));\n+  %}\n+  ins_pipe(ialu_mem_imm);\n+%}\n+\n+\/\/ Xor Instructions\n+\/\/ Xor Register with Register\n+instruct xorL_rReg(rRegL dst, rRegL src, rFlagsReg cr)\n+%{\n+  predicate(!UseAPX);\n+  match(Set dst (XorL dst src));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);\n+\n+  format %{ \"xorq    $dst, $src\\t# long\" %}\n+  ins_encode %{\n+    __ xorq($dst$$Register, $src$$Register);\n+  %}\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+\/\/ Xor Register with Register using New Data Destination (NDD)\n+instruct xorL_rReg_ndd(rRegL dst, rRegL src1, rRegL src2, rFlagsReg cr)\n+%{\n+  predicate(UseAPX);\n+  match(Set dst (XorL src1 src2));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);\n+\n+  format %{ \"exorq    $dst, $src1, $src2\\t# long ndd\" %}\n+  ins_encode %{\n+    __ exorq($dst$$Register, $src1$$Register, $src2$$Register, false);\n+  %}\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+\/\/ Xor Register with Immediate -1\n+instruct xorL_rReg_im1(rRegL dst, immL_M1 imm)\n+%{\n+  predicate(!UseAPX);\n+  match(Set dst (XorL dst imm));\n+\n+  format %{ \"notq   $dst\" %}\n+  ins_encode %{\n+     __ notq($dst$$Register);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+instruct xorL_rReg_im1_ndd(rRegL dst,rRegL src, immL_M1 imm)\n+%{\n+  predicate(UseAPX);\n+  match(Set dst (XorL src imm));\n+\n+  format %{ \"enotq   $dst, $src\" %}\n+  ins_encode %{\n+    __ enotq($dst$$Register, $src$$Register);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+\/\/ Xor Register with Immediate\n+instruct xorL_rReg_imm(rRegL dst, immL32 src, rFlagsReg cr)\n+%{\n+  \/\/ Strict predicate check to make selection of xorL_rReg_im1 cost agnostic if immL32 src is -1.\n+  predicate(!UseAPX && n->in(2)->bottom_type()->is_long()->get_con() != -1L);\n+  match(Set dst (XorL dst src));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);\n+\n+  format %{ \"xorq    $dst, $src\\t# long\" %}\n+  ins_encode %{\n+    __ xorq($dst$$Register, $src$$constant);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+instruct xorL_rReg_rReg_imm(rRegL dst, rRegL src1, immL32 src2, rFlagsReg cr)\n+%{\n+  \/\/ Strict predicate check to make selection of xorL_rReg_im1_ndd cost agnostic if immL32 src2 is -1.\n+  predicate(UseAPX && n->in(2)->bottom_type()->is_long()->get_con() != -1L);\n+  match(Set dst (XorL src1 src2));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);\n+\n+  format %{ \"exorq    $dst, $src1, $src2\\t# long ndd\" %}\n+  ins_encode %{\n+    __ exorq($dst$$Register, $src1$$Register, $src2$$constant, false);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+\/\/ Xor Memory with Immediate\n+instruct xorL_rReg_mem_imm(rRegL dst, memory src1, immL32 src2, rFlagsReg cr)\n+%{\n+  predicate(UseAPX);\n+  match(Set dst (XorL (LoadL src1) src2));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);\n+  ins_cost(150);\n+\n+  format %{ \"exorq    $dst, $src1, $src2\\t# long ndd\" %}\n+  ins_encode %{\n+    __ exorq($dst$$Register, $src1$$Address, $src2$$constant, false);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+\/\/ Xor Register with Memory\n+instruct xorL_rReg_mem(rRegL dst, memory src, rFlagsReg cr)\n+%{\n+  predicate(!UseAPX);\n+  match(Set dst (XorL dst (LoadL src)));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);\n+\n+  ins_cost(150);\n+  format %{ \"xorq    $dst, $src\\t# long\" %}\n+  ins_encode %{\n+    __ xorq($dst$$Register, $src$$Address);\n+  %}\n+  ins_pipe(ialu_reg_mem);\n+%}\n+\n+instruct xorL_rReg_rReg_mem_ndd(rRegL dst, rRegL src1, memory src2, rFlagsReg cr)\n+%{\n+  predicate(UseAPX);\n+  match(Set dst (XorL src1 (LoadL src2)));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);\n+\n+  ins_cost(150);\n+  format %{ \"exorq    $dst, $src1, $src2\\t# long ndd\" %}\n+  ins_encode %{\n+    __ exorq($dst$$Register, $src1$$Register, $src2$$Address, false);\n+  %}\n+  ins_pipe(ialu_reg_mem);\n+%}\n+\n+\/\/ Xor Memory with Register\n+instruct xorL_mem_rReg(memory dst, rRegL src, rFlagsReg cr)\n+%{\n+  match(Set dst (StoreL dst (XorL (LoadL dst) src)));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);\n+\n+  ins_cost(150);\n+  format %{ \"xorq    $dst, $src\\t# long\" %}\n+  ins_encode %{\n+    __ xorq($dst$$Address, $src$$Register);\n+  %}\n+  ins_pipe(ialu_mem_reg);\n+%}\n+\n+\/\/ Xor Memory with Immediate\n+instruct xorL_mem_imm(memory dst, immL32 src, rFlagsReg cr)\n+%{\n+  match(Set dst (StoreL dst (XorL (LoadL dst) src)));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);\n+\n+  ins_cost(125);\n+  format %{ \"xorq    $dst, $src\\t# long\" %}\n+  ins_encode %{\n+    __ xorq($dst$$Address, $src$$constant);\n+  %}\n+  ins_pipe(ialu_mem_imm);\n+%}\n+\n+instruct cmpLTMask(rRegI dst, rRegI p, rRegI q, rFlagsReg cr)\n+%{\n+  match(Set dst (CmpLTMask p q));\n+  effect(KILL cr);\n+\n+  ins_cost(400);\n+  format %{ \"cmpl    $p, $q\\t# cmpLTMask\\n\\t\"\n+            \"setcc   $dst \\t# emits setlt + movzbl or setzul for APX\"\n+            \"negl    $dst\" %}\n+  ins_encode %{\n+    __ cmpl($p$$Register, $q$$Register);\n+    __ setcc(Assembler::less, $dst$$Register);\n+    __ negl($dst$$Register);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct cmpLTMask0(rRegI dst, immI_0 zero, rFlagsReg cr)\n+%{\n+  match(Set dst (CmpLTMask dst zero));\n+  effect(KILL cr);\n+\n+  ins_cost(100);\n+  format %{ \"sarl    $dst, #31\\t# cmpLTMask0\" %}\n+  ins_encode %{\n+    __ sarl($dst$$Register, 31);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+\/* Better to save a register than avoid a branch *\/\n+instruct cadd_cmpLTMask(rRegI p, rRegI q, rRegI y, rFlagsReg cr)\n+%{\n+  match(Set p (AddI (AndI (CmpLTMask p q) y) (SubI p q)));\n+  effect(KILL cr);\n+  ins_cost(300);\n+  format %{ \"subl    $p,$q\\t# cadd_cmpLTMask\\n\\t\"\n+            \"jge     done\\n\\t\"\n+            \"addl    $p,$y\\n\"\n+            \"done:   \" %}\n+  ins_encode %{\n+    Register Rp = $p$$Register;\n+    Register Rq = $q$$Register;\n+    Register Ry = $y$$Register;\n+    Label done;\n+    __ subl(Rp, Rq);\n+    __ jccb(Assembler::greaterEqual, done);\n+    __ addl(Rp, Ry);\n+    __ bind(done);\n+  %}\n+  ins_pipe(pipe_cmplt);\n+%}\n+\n+\/* Better to save a register than avoid a branch *\/\n+instruct and_cmpLTMask(rRegI p, rRegI q, rRegI y, rFlagsReg cr)\n+%{\n+  match(Set y (AndI (CmpLTMask p q) y));\n+  effect(KILL cr);\n+\n+  ins_cost(300);\n+\n+  format %{ \"cmpl    $p, $q\\t# and_cmpLTMask\\n\\t\"\n+            \"jlt     done\\n\\t\"\n+            \"xorl    $y, $y\\n\"\n+            \"done:   \" %}\n+  ins_encode %{\n+    Register Rp = $p$$Register;\n+    Register Rq = $q$$Register;\n+    Register Ry = $y$$Register;\n+    Label done;\n+    __ cmpl(Rp, Rq);\n+    __ jccb(Assembler::less, done);\n+    __ xorl(Ry, Ry);\n+    __ bind(done);\n+  %}\n+  ins_pipe(pipe_cmplt);\n+%}\n+\n+\n+\/\/---------- FP Instructions------------------------------------------------\n+\n+\/\/ Really expensive, avoid\n+instruct cmpF_cc_reg(rFlagsRegU cr, regF src1, regF src2)\n+%{\n+  match(Set cr (CmpF src1 src2));\n+\n+  ins_cost(500);\n+  format %{ \"ucomiss $src1, $src2\\n\\t\"\n+            \"jnp,s   exit\\n\\t\"\n+            \"pushfq\\t# saw NaN, set CF\\n\\t\"\n+            \"andq    [rsp], #0xffffff2b\\n\\t\"\n+            \"popfq\\n\"\n+    \"exit:\" %}\n+  ins_encode %{\n+    __ ucomiss($src1$$XMMRegister, $src2$$XMMRegister);\n+    emit_cmpfp_fixup(masm);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct cmpF_cc_reg_CF(rFlagsRegUCF cr, regF src1, regF src2) %{\n+  match(Set cr (CmpF src1 src2));\n+\n+  ins_cost(100);\n+  format %{ \"ucomiss $src1, $src2\" %}\n+  ins_encode %{\n+    __ ucomiss($src1$$XMMRegister, $src2$$XMMRegister);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct cmpF_cc_memCF(rFlagsRegUCF cr, regF src1, memory src2) %{\n+  match(Set cr (CmpF src1 (LoadF src2)));\n+\n+  ins_cost(100);\n+  format %{ \"ucomiss $src1, $src2\" %}\n+  ins_encode %{\n+    __ ucomiss($src1$$XMMRegister, $src2$$Address);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct cmpF_cc_immCF(rFlagsRegUCF cr, regF src, immF con) %{\n+  match(Set cr (CmpF src con));\n+  ins_cost(100);\n+  format %{ \"ucomiss $src, [$constantaddress]\\t# load from constant table: float=$con\" %}\n+  ins_encode %{\n+    __ ucomiss($src$$XMMRegister, $constantaddress($con));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ Really expensive, avoid\n+instruct cmpD_cc_reg(rFlagsRegU cr, regD src1, regD src2)\n+%{\n+  match(Set cr (CmpD src1 src2));\n+\n+  ins_cost(500);\n+  format %{ \"ucomisd $src1, $src2\\n\\t\"\n+            \"jnp,s   exit\\n\\t\"\n+            \"pushfq\\t# saw NaN, set CF\\n\\t\"\n+            \"andq    [rsp], #0xffffff2b\\n\\t\"\n+            \"popfq\\n\"\n+    \"exit:\" %}\n+  ins_encode %{\n+    __ ucomisd($src1$$XMMRegister, $src2$$XMMRegister);\n+    emit_cmpfp_fixup(masm);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct cmpD_cc_reg_CF(rFlagsRegUCF cr, regD src1, regD src2) %{\n+  match(Set cr (CmpD src1 src2));\n+\n+  ins_cost(100);\n+  format %{ \"ucomisd $src1, $src2 test\" %}\n+  ins_encode %{\n+    __ ucomisd($src1$$XMMRegister, $src2$$XMMRegister);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct cmpD_cc_memCF(rFlagsRegUCF cr, regD src1, memory src2) %{\n+  match(Set cr (CmpD src1 (LoadD src2)));\n+\n+  ins_cost(100);\n+  format %{ \"ucomisd $src1, $src2\" %}\n+  ins_encode %{\n+    __ ucomisd($src1$$XMMRegister, $src2$$Address);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct cmpD_cc_immCF(rFlagsRegUCF cr, regD src, immD con) %{\n+  match(Set cr (CmpD src con));\n+  ins_cost(100);\n+  format %{ \"ucomisd $src, [$constantaddress]\\t# load from constant table: double=$con\" %}\n+  ins_encode %{\n+    __ ucomisd($src$$XMMRegister, $constantaddress($con));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ Compare into -1,0,1\n+instruct cmpF_reg(rRegI dst, regF src1, regF src2, rFlagsReg cr)\n+%{\n+  match(Set dst (CmpF3 src1 src2));\n+  effect(KILL cr);\n+\n+  ins_cost(275);\n+  format %{ \"ucomiss $src1, $src2\\n\\t\"\n+            \"movl    $dst, #-1\\n\\t\"\n+            \"jp,s    done\\n\\t\"\n+            \"jb,s    done\\n\\t\"\n+            \"setne   $dst\\n\\t\"\n+            \"movzbl  $dst, $dst\\n\"\n+    \"done:\" %}\n+  ins_encode %{\n+    __ ucomiss($src1$$XMMRegister, $src2$$XMMRegister);\n+    emit_cmpfp3(masm, $dst$$Register);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ Compare into -1,0,1\n+instruct cmpF_mem(rRegI dst, regF src1, memory src2, rFlagsReg cr)\n+%{\n+  match(Set dst (CmpF3 src1 (LoadF src2)));\n+  effect(KILL cr);\n+\n+  ins_cost(275);\n+  format %{ \"ucomiss $src1, $src2\\n\\t\"\n+            \"movl    $dst, #-1\\n\\t\"\n+            \"jp,s    done\\n\\t\"\n+            \"jb,s    done\\n\\t\"\n+            \"setne   $dst\\n\\t\"\n+            \"movzbl  $dst, $dst\\n\"\n+    \"done:\" %}\n+  ins_encode %{\n+    __ ucomiss($src1$$XMMRegister, $src2$$Address);\n+    emit_cmpfp3(masm, $dst$$Register);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ Compare into -1,0,1\n+instruct cmpF_imm(rRegI dst, regF src, immF con, rFlagsReg cr) %{\n+  match(Set dst (CmpF3 src con));\n+  effect(KILL cr);\n+\n+  ins_cost(275);\n+  format %{ \"ucomiss $src, [$constantaddress]\\t# load from constant table: float=$con\\n\\t\"\n+            \"movl    $dst, #-1\\n\\t\"\n+            \"jp,s    done\\n\\t\"\n+            \"jb,s    done\\n\\t\"\n+            \"setne   $dst\\n\\t\"\n+            \"movzbl  $dst, $dst\\n\"\n+    \"done:\" %}\n+  ins_encode %{\n+    __ ucomiss($src$$XMMRegister, $constantaddress($con));\n+    emit_cmpfp3(masm, $dst$$Register);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ Compare into -1,0,1\n+instruct cmpD_reg(rRegI dst, regD src1, regD src2, rFlagsReg cr)\n+%{\n+  match(Set dst (CmpD3 src1 src2));\n+  effect(KILL cr);\n+\n+  ins_cost(275);\n+  format %{ \"ucomisd $src1, $src2\\n\\t\"\n+            \"movl    $dst, #-1\\n\\t\"\n+            \"jp,s    done\\n\\t\"\n+            \"jb,s    done\\n\\t\"\n+            \"setne   $dst\\n\\t\"\n+            \"movzbl  $dst, $dst\\n\"\n+    \"done:\" %}\n+  ins_encode %{\n+    __ ucomisd($src1$$XMMRegister, $src2$$XMMRegister);\n+    emit_cmpfp3(masm, $dst$$Register);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ Compare into -1,0,1\n+instruct cmpD_mem(rRegI dst, regD src1, memory src2, rFlagsReg cr)\n+%{\n+  match(Set dst (CmpD3 src1 (LoadD src2)));\n+  effect(KILL cr);\n+\n+  ins_cost(275);\n+  format %{ \"ucomisd $src1, $src2\\n\\t\"\n+            \"movl    $dst, #-1\\n\\t\"\n+            \"jp,s    done\\n\\t\"\n+            \"jb,s    done\\n\\t\"\n+            \"setne   $dst\\n\\t\"\n+            \"movzbl  $dst, $dst\\n\"\n+    \"done:\" %}\n+  ins_encode %{\n+    __ ucomisd($src1$$XMMRegister, $src2$$Address);\n+    emit_cmpfp3(masm, $dst$$Register);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ Compare into -1,0,1\n+instruct cmpD_imm(rRegI dst, regD src, immD con, rFlagsReg cr) %{\n+  match(Set dst (CmpD3 src con));\n+  effect(KILL cr);\n+\n+  ins_cost(275);\n+  format %{ \"ucomisd $src, [$constantaddress]\\t# load from constant table: double=$con\\n\\t\"\n+            \"movl    $dst, #-1\\n\\t\"\n+            \"jp,s    done\\n\\t\"\n+            \"jb,s    done\\n\\t\"\n+            \"setne   $dst\\n\\t\"\n+            \"movzbl  $dst, $dst\\n\"\n+    \"done:\" %}\n+  ins_encode %{\n+    __ ucomisd($src$$XMMRegister, $constantaddress($con));\n+    emit_cmpfp3(masm, $dst$$Register);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/----------Arithmetic Conversion Instructions---------------------------------\n+\n+instruct convF2D_reg_reg(regD dst, regF src)\n+%{\n+  match(Set dst (ConvF2D src));\n+\n+  format %{ \"cvtss2sd $dst, $src\" %}\n+  ins_encode %{\n+    __ cvtss2sd ($dst$$XMMRegister, $src$$XMMRegister);\n+  %}\n+  ins_pipe(pipe_slow); \/\/ XXX\n+%}\n+\n+instruct convF2D_reg_mem(regD dst, memory src)\n+%{\n+  predicate(UseAVX == 0);\n+  match(Set dst (ConvF2D (LoadF src)));\n+\n+  format %{ \"cvtss2sd $dst, $src\" %}\n+  ins_encode %{\n+    __ cvtss2sd ($dst$$XMMRegister, $src$$Address);\n+  %}\n+  ins_pipe(pipe_slow); \/\/ XXX\n+%}\n+\n+instruct convD2F_reg_reg(regF dst, regD src)\n+%{\n+  match(Set dst (ConvD2F src));\n+\n+  format %{ \"cvtsd2ss $dst, $src\" %}\n+  ins_encode %{\n+    __ cvtsd2ss ($dst$$XMMRegister, $src$$XMMRegister);\n+  %}\n+  ins_pipe(pipe_slow); \/\/ XXX\n+%}\n+\n+instruct convD2F_reg_mem(regF dst, memory src)\n+%{\n+  predicate(UseAVX == 0);\n+  match(Set dst (ConvD2F (LoadD src)));\n+\n+  format %{ \"cvtsd2ss $dst, $src\" %}\n+  ins_encode %{\n+    __ cvtsd2ss ($dst$$XMMRegister, $src$$Address);\n+  %}\n+  ins_pipe(pipe_slow); \/\/ XXX\n+%}\n+\n+\/\/ XXX do mem variants\n+instruct convF2I_reg_reg(rRegI dst, regF src, rFlagsReg cr)\n+%{\n+  predicate(!VM_Version::supports_avx10_2());\n+  match(Set dst (ConvF2I src));\n+  effect(KILL cr);\n+  format %{ \"convert_f2i $dst, $src\" %}\n+  ins_encode %{\n+    __ convertF2I(T_INT, T_FLOAT, $dst$$Register, $src$$XMMRegister);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct convF2I_reg_reg_avx10(rRegI dst, regF src)\n+%{\n+  predicate(VM_Version::supports_avx10_2());\n+  match(Set dst (ConvF2I src));\n+  format %{ \"evcvttss2sisl $dst, $src\" %}\n+  ins_encode %{\n+    __ evcvttss2sisl($dst$$Register, $src$$XMMRegister);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct convF2I_reg_mem_avx10(rRegI dst, memory src)\n+%{\n+  predicate(VM_Version::supports_avx10_2());\n+  match(Set dst (ConvF2I (LoadF src)));\n+  format %{ \"evcvttss2sisl $dst, $src\" %}\n+  ins_encode %{\n+    __ evcvttss2sisl($dst$$Register, $src$$Address);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct convF2L_reg_reg(rRegL dst, regF src, rFlagsReg cr)\n+%{\n+  predicate(!VM_Version::supports_avx10_2());\n+  match(Set dst (ConvF2L src));\n+  effect(KILL cr);\n+  format %{ \"convert_f2l $dst, $src\"%}\n+  ins_encode %{\n+    __ convertF2I(T_LONG, T_FLOAT, $dst$$Register, $src$$XMMRegister);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct convF2L_reg_reg_avx10(rRegL dst, regF src)\n+%{\n+  predicate(VM_Version::supports_avx10_2());\n+  match(Set dst (ConvF2L src));\n+  format %{ \"evcvttss2sisq $dst, $src\" %}\n+  ins_encode %{\n+    __ evcvttss2sisq($dst$$Register, $src$$XMMRegister);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct convF2L_reg_mem_avx10(rRegL dst, memory src)\n+%{\n+  predicate(VM_Version::supports_avx10_2());\n+  match(Set dst (ConvF2L (LoadF src)));\n+  format %{ \"evcvttss2sisq $dst, $src\" %}\n+  ins_encode %{\n+    __ evcvttss2sisq($dst$$Register, $src$$Address);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct convD2I_reg_reg(rRegI dst, regD src, rFlagsReg cr)\n+%{\n+  predicate(!VM_Version::supports_avx10_2());\n+  match(Set dst (ConvD2I src));\n+  effect(KILL cr);\n+  format %{ \"convert_d2i $dst, $src\"%}\n+  ins_encode %{\n+    __ convertF2I(T_INT, T_DOUBLE, $dst$$Register, $src$$XMMRegister);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct convD2I_reg_reg_avx10(rRegI dst, regD src)\n+%{\n+  predicate(VM_Version::supports_avx10_2());\n+  match(Set dst (ConvD2I src));\n+  format %{ \"evcvttsd2sisl $dst, $src\" %}\n+  ins_encode %{\n+    __ evcvttsd2sisl($dst$$Register, $src$$XMMRegister);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct convD2I_reg_mem_avx10(rRegI dst, memory src)\n+%{\n+  predicate(VM_Version::supports_avx10_2());\n+  match(Set dst (ConvD2I (LoadD src)));\n+  format %{ \"evcvttsd2sisl $dst, $src\" %}\n+  ins_encode %{\n+    __ evcvttsd2sisl($dst$$Register, $src$$Address);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct convD2L_reg_reg(rRegL dst, regD src, rFlagsReg cr)\n+%{\n+  predicate(!VM_Version::supports_avx10_2());\n+  match(Set dst (ConvD2L src));\n+  effect(KILL cr);\n+  format %{ \"convert_d2l $dst, $src\"%}\n+  ins_encode %{\n+    __ convertF2I(T_LONG, T_DOUBLE, $dst$$Register, $src$$XMMRegister);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct convD2L_reg_reg_avx10(rRegL dst, regD src)\n+%{\n+  predicate(VM_Version::supports_avx10_2());\n+  match(Set dst (ConvD2L src));\n+  format %{ \"evcvttsd2sisq $dst, $src\" %}\n+  ins_encode %{\n+    __ evcvttsd2sisq($dst$$Register, $src$$XMMRegister);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct convD2L_reg_mem_avx10(rRegL dst, memory src)\n+%{\n+  predicate(VM_Version::supports_avx10_2());\n+  match(Set dst (ConvD2L (LoadD src)));\n+  format %{ \"evcvttsd2sisq $dst, $src\" %}\n+  ins_encode %{\n+    __ evcvttsd2sisq($dst$$Register, $src$$Address);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct round_double_reg(rRegL dst, regD src, rRegL rtmp, rcx_RegL rcx, rFlagsReg cr)\n+%{\n+  match(Set dst (RoundD src));\n+  effect(TEMP dst, TEMP rtmp, TEMP rcx, KILL cr);\n+  format %{ \"round_double $dst,$src \\t! using $rtmp and $rcx as TEMP\"%}\n+  ins_encode %{\n+    __ round_double($dst$$Register, $src$$XMMRegister, $rtmp$$Register, $rcx$$Register);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct round_float_reg(rRegI dst, regF src, rRegL rtmp, rcx_RegL rcx, rFlagsReg cr)\n+%{\n+  match(Set dst (RoundF src));\n+  effect(TEMP dst, TEMP rtmp, TEMP rcx, KILL cr);\n+  format %{ \"round_float $dst,$src\" %}\n+  ins_encode %{\n+    __ round_float($dst$$Register, $src$$XMMRegister, $rtmp$$Register, $rcx$$Register);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct convI2F_reg_reg(vlRegF dst, rRegI src)\n+%{\n+  predicate(!UseXmmI2F);\n+  match(Set dst (ConvI2F src));\n+\n+  format %{ \"cvtsi2ssl $dst, $src\\t# i2f\" %}\n+  ins_encode %{\n+    if (UseAVX > 0) {\n+      __ pxor($dst$$XMMRegister, $dst$$XMMRegister);\n+    }\n+    __ cvtsi2ssl ($dst$$XMMRegister, $src$$Register);\n+  %}\n+  ins_pipe(pipe_slow); \/\/ XXX\n+%}\n+\n+instruct convI2F_reg_mem(regF dst, memory src)\n+%{\n+  predicate(UseAVX == 0);\n+  match(Set dst (ConvI2F (LoadI src)));\n+\n+  format %{ \"cvtsi2ssl $dst, $src\\t# i2f\" %}\n+  ins_encode %{\n+    __ cvtsi2ssl ($dst$$XMMRegister, $src$$Address);\n+  %}\n+  ins_pipe(pipe_slow); \/\/ XXX\n+%}\n+\n+instruct convI2D_reg_reg(vlRegD dst, rRegI src)\n+%{\n+  predicate(!UseXmmI2D);\n+  match(Set dst (ConvI2D src));\n+\n+  format %{ \"cvtsi2sdl $dst, $src\\t# i2d\" %}\n+  ins_encode %{\n+    if (UseAVX > 0) {\n+      __ pxor($dst$$XMMRegister, $dst$$XMMRegister);\n+    }\n+    __ cvtsi2sdl ($dst$$XMMRegister, $src$$Register);\n+  %}\n+  ins_pipe(pipe_slow); \/\/ XXX\n+%}\n+\n+instruct convI2D_reg_mem(regD dst, memory src)\n+%{\n+  predicate(UseAVX == 0);\n+  match(Set dst (ConvI2D (LoadI src)));\n+\n+  format %{ \"cvtsi2sdl $dst, $src\\t# i2d\" %}\n+  ins_encode %{\n+    __ cvtsi2sdl ($dst$$XMMRegister, $src$$Address);\n+  %}\n+  ins_pipe(pipe_slow); \/\/ XXX\n+%}\n+\n+instruct convXI2F_reg(regF dst, rRegI src)\n+%{\n+  predicate(UseXmmI2F);\n+  match(Set dst (ConvI2F src));\n+\n+  format %{ \"movdl $dst, $src\\n\\t\"\n+            \"cvtdq2psl $dst, $dst\\t# i2f\" %}\n+  ins_encode %{\n+    __ movdl($dst$$XMMRegister, $src$$Register);\n+    __ cvtdq2ps($dst$$XMMRegister, $dst$$XMMRegister);\n+  %}\n+  ins_pipe(pipe_slow); \/\/ XXX\n+%}\n+\n+instruct convXI2D_reg(regD dst, rRegI src)\n+%{\n+  predicate(UseXmmI2D);\n+  match(Set dst (ConvI2D src));\n+\n+  format %{ \"movdl $dst, $src\\n\\t\"\n+            \"cvtdq2pdl $dst, $dst\\t# i2d\" %}\n+  ins_encode %{\n+    __ movdl($dst$$XMMRegister, $src$$Register);\n+    __ cvtdq2pd($dst$$XMMRegister, $dst$$XMMRegister);\n+  %}\n+  ins_pipe(pipe_slow); \/\/ XXX\n+%}\n+\n+instruct convL2F_reg_reg(vlRegF dst, rRegL src)\n+%{\n+  match(Set dst (ConvL2F src));\n+\n+  format %{ \"cvtsi2ssq $dst, $src\\t# l2f\" %}\n+  ins_encode %{\n+    if (UseAVX > 0) {\n+      __ pxor($dst$$XMMRegister, $dst$$XMMRegister);\n+    }\n+    __ cvtsi2ssq ($dst$$XMMRegister, $src$$Register);\n+  %}\n+  ins_pipe(pipe_slow); \/\/ XXX\n+%}\n+\n+instruct convL2F_reg_mem(regF dst, memory src)\n+%{\n+  predicate(UseAVX == 0);\n+  match(Set dst (ConvL2F (LoadL src)));\n+\n+  format %{ \"cvtsi2ssq $dst, $src\\t# l2f\" %}\n+  ins_encode %{\n+    __ cvtsi2ssq ($dst$$XMMRegister, $src$$Address);\n+  %}\n+  ins_pipe(pipe_slow); \/\/ XXX\n+%}\n+\n+instruct convL2D_reg_reg(vlRegD dst, rRegL src)\n+%{\n+  match(Set dst (ConvL2D src));\n+\n+  format %{ \"cvtsi2sdq $dst, $src\\t# l2d\" %}\n+  ins_encode %{\n+    if (UseAVX > 0) {\n+      __ pxor($dst$$XMMRegister, $dst$$XMMRegister);\n+    }\n+    __ cvtsi2sdq ($dst$$XMMRegister, $src$$Register);\n+  %}\n+  ins_pipe(pipe_slow); \/\/ XXX\n+%}\n+\n+instruct convL2D_reg_mem(regD dst, memory src)\n+%{\n+  predicate(UseAVX == 0);\n+  match(Set dst (ConvL2D (LoadL src)));\n+\n+  format %{ \"cvtsi2sdq $dst, $src\\t# l2d\" %}\n+  ins_encode %{\n+    __ cvtsi2sdq ($dst$$XMMRegister, $src$$Address);\n+  %}\n+  ins_pipe(pipe_slow); \/\/ XXX\n+%}\n+\n+instruct convI2L_reg_reg(rRegL dst, rRegI src)\n+%{\n+  match(Set dst (ConvI2L src));\n+\n+  ins_cost(125);\n+  format %{ \"movslq  $dst, $src\\t# i2l\" %}\n+  ins_encode %{\n+    __ movslq($dst$$Register, $src$$Register);\n+  %}\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+\/\/ Zero-extend convert int to long\n+instruct convI2L_reg_reg_zex(rRegL dst, rRegI src, immL_32bits mask)\n+%{\n+  match(Set dst (AndL (ConvI2L src) mask));\n+\n+  format %{ \"movl    $dst, $src\\t# i2l zero-extend\\n\\t\" %}\n+  ins_encode %{\n+    if ($dst$$reg != $src$$reg) {\n+      __ movl($dst$$Register, $src$$Register);\n+    }\n+  %}\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+\/\/ Zero-extend convert int to long\n+instruct convI2L_reg_mem_zex(rRegL dst, memory src, immL_32bits mask)\n+%{\n+  match(Set dst (AndL (ConvI2L (LoadI src)) mask));\n+\n+  format %{ \"movl    $dst, $src\\t# i2l zero-extend\\n\\t\" %}\n+  ins_encode %{\n+    __ movl($dst$$Register, $src$$Address);\n+  %}\n+  ins_pipe(ialu_reg_mem);\n+%}\n+\n+instruct zerox_long_reg_reg(rRegL dst, rRegL src, immL_32bits mask)\n+%{\n+  match(Set dst (AndL src mask));\n+\n+  format %{ \"movl    $dst, $src\\t# zero-extend long\" %}\n+  ins_encode %{\n+    __ movl($dst$$Register, $src$$Register);\n+  %}\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+instruct convL2I_reg_reg(rRegI dst, rRegL src)\n+%{\n+  match(Set dst (ConvL2I src));\n+\n+  format %{ \"movl    $dst, $src\\t# l2i\" %}\n+  ins_encode %{\n+    __ movl($dst$$Register, $src$$Register);\n+  %}\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+\n+instruct MoveF2I_stack_reg(rRegI dst, stackSlotF src) %{\n+  match(Set dst (MoveF2I src));\n+  effect(DEF dst, USE src);\n+\n+  ins_cost(125);\n+  format %{ \"movl    $dst, $src\\t# MoveF2I_stack_reg\" %}\n+  ins_encode %{\n+    __ movl($dst$$Register, Address(rsp, $src$$disp));\n+  %}\n+  ins_pipe(ialu_reg_mem);\n+%}\n+\n+instruct MoveI2F_stack_reg(regF dst, stackSlotI src) %{\n+  match(Set dst (MoveI2F src));\n+  effect(DEF dst, USE src);\n+\n+  ins_cost(125);\n+  format %{ \"movss   $dst, $src\\t# MoveI2F_stack_reg\" %}\n+  ins_encode %{\n+    __ movflt($dst$$XMMRegister, Address(rsp, $src$$disp));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct MoveD2L_stack_reg(rRegL dst, stackSlotD src) %{\n+  match(Set dst (MoveD2L src));\n+  effect(DEF dst, USE src);\n+\n+  ins_cost(125);\n+  format %{ \"movq    $dst, $src\\t# MoveD2L_stack_reg\" %}\n+  ins_encode %{\n+    __ movq($dst$$Register, Address(rsp, $src$$disp));\n+  %}\n+  ins_pipe(ialu_reg_mem);\n+%}\n+\n+instruct MoveL2D_stack_reg_partial(regD dst, stackSlotL src) %{\n+  predicate(!UseXmmLoadAndClearUpper);\n+  match(Set dst (MoveL2D src));\n+  effect(DEF dst, USE src);\n+\n+  ins_cost(125);\n+  format %{ \"movlpd  $dst, $src\\t# MoveL2D_stack_reg\" %}\n+  ins_encode %{\n+    __ movdbl($dst$$XMMRegister, Address(rsp, $src$$disp));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct MoveL2D_stack_reg(regD dst, stackSlotL src) %{\n+  predicate(UseXmmLoadAndClearUpper);\n+  match(Set dst (MoveL2D src));\n+  effect(DEF dst, USE src);\n+\n+  ins_cost(125);\n+  format %{ \"movsd   $dst, $src\\t# MoveL2D_stack_reg\" %}\n+  ins_encode %{\n+    __ movdbl($dst$$XMMRegister, Address(rsp, $src$$disp));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\n+instruct MoveF2I_reg_stack(stackSlotI dst, regF src) %{\n+  match(Set dst (MoveF2I src));\n+  effect(DEF dst, USE src);\n+\n+  ins_cost(95); \/\/ XXX\n+  format %{ \"movss   $dst, $src\\t# MoveF2I_reg_stack\" %}\n+  ins_encode %{\n+    __ movflt(Address(rsp, $dst$$disp), $src$$XMMRegister);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct MoveI2F_reg_stack(stackSlotF dst, rRegI src) %{\n+  match(Set dst (MoveI2F src));\n+  effect(DEF dst, USE src);\n+\n+  ins_cost(100);\n+  format %{ \"movl    $dst, $src\\t# MoveI2F_reg_stack\" %}\n+  ins_encode %{\n+    __ movl(Address(rsp, $dst$$disp), $src$$Register);\n+  %}\n+  ins_pipe( ialu_mem_reg );\n+%}\n+\n+instruct MoveD2L_reg_stack(stackSlotL dst, regD src) %{\n+  match(Set dst (MoveD2L src));\n+  effect(DEF dst, USE src);\n+\n+  ins_cost(95); \/\/ XXX\n+  format %{ \"movsd   $dst, $src\\t# MoveL2D_reg_stack\" %}\n+  ins_encode %{\n+    __ movdbl(Address(rsp, $dst$$disp), $src$$XMMRegister);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct MoveL2D_reg_stack(stackSlotD dst, rRegL src) %{\n+  match(Set dst (MoveL2D src));\n+  effect(DEF dst, USE src);\n+\n+  ins_cost(100);\n+  format %{ \"movq    $dst, $src\\t# MoveL2D_reg_stack\" %}\n+  ins_encode %{\n+    __ movq(Address(rsp, $dst$$disp), $src$$Register);\n+  %}\n+  ins_pipe(ialu_mem_reg);\n+%}\n+\n+instruct MoveF2I_reg_reg(rRegI dst, regF src) %{\n+  match(Set dst (MoveF2I src));\n+  effect(DEF dst, USE src);\n+  ins_cost(85);\n+  format %{ \"movd    $dst,$src\\t# MoveF2I\" %}\n+  ins_encode %{\n+    __ movdl($dst$$Register, $src$$XMMRegister);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct MoveD2L_reg_reg(rRegL dst, regD src) %{\n+  match(Set dst (MoveD2L src));\n+  effect(DEF dst, USE src);\n+  ins_cost(85);\n+  format %{ \"movd    $dst,$src\\t# MoveD2L\" %}\n+  ins_encode %{\n+    __ movdq($dst$$Register, $src$$XMMRegister);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct MoveI2F_reg_reg(regF dst, rRegI src) %{\n+  match(Set dst (MoveI2F src));\n+  effect(DEF dst, USE src);\n+  ins_cost(100);\n+  format %{ \"movd    $dst,$src\\t# MoveI2F\" %}\n+  ins_encode %{\n+    __ movdl($dst$$XMMRegister, $src$$Register);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct MoveL2D_reg_reg(regD dst, rRegL src) %{\n+  match(Set dst (MoveL2D src));\n+  effect(DEF dst, USE src);\n+  ins_cost(100);\n+  format %{ \"movd    $dst,$src\\t# MoveL2D\" %}\n+  ins_encode %{\n+     __ movdq($dst$$XMMRegister, $src$$Register);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+\/\/ Fast clearing of an array\n+\/\/ Small non-constant lenght ClearArray for non-AVX512 targets.\n+instruct rep_stos(rcx_RegL cnt, rdi_RegP base, regD tmp, rax_RegI zero,\n+                  Universe dummy, rFlagsReg cr)\n+%{\n+  predicate(!((ClearArrayNode*)n)->is_large() && (UseAVX <= 2));\n+  match(Set dummy (ClearArray cnt base));\n+  effect(USE_KILL cnt, USE_KILL base, TEMP tmp, KILL zero, KILL cr);\n+\n+  format %{ $$template\n+    $$emit$$\"xorq    rax, rax\\t# ClearArray:\\n\\t\"\n+    $$emit$$\"cmp     InitArrayShortSize,rcx\\n\\t\"\n+    $$emit$$\"jg      LARGE\\n\\t\"\n+    $$emit$$\"dec     rcx\\n\\t\"\n+    $$emit$$\"js      DONE\\t# Zero length\\n\\t\"\n+    $$emit$$\"mov     rax,(rdi,rcx,8)\\t# LOOP\\n\\t\"\n+    $$emit$$\"dec     rcx\\n\\t\"\n+    $$emit$$\"jge     LOOP\\n\\t\"\n+    $$emit$$\"jmp     DONE\\n\\t\"\n+    $$emit$$\"# LARGE:\\n\\t\"\n+    if (UseFastStosb) {\n+       $$emit$$\"shlq    rcx,3\\t# Convert doublewords to bytes\\n\\t\"\n+       $$emit$$\"rep     stosb\\t# Store rax to *rdi++ while rcx--\\n\\t\"\n+    } else if (UseXMMForObjInit) {\n+       $$emit$$\"mov     rdi,rax\\n\\t\"\n+       $$emit$$\"vpxor   ymm0,ymm0,ymm0\\n\\t\"\n+       $$emit$$\"jmpq    L_zero_64_bytes\\n\\t\"\n+       $$emit$$\"# L_loop:\\t# 64-byte LOOP\\n\\t\"\n+       $$emit$$\"vmovdqu ymm0,(rax)\\n\\t\"\n+       $$emit$$\"vmovdqu ymm0,0x20(rax)\\n\\t\"\n+       $$emit$$\"add     0x40,rax\\n\\t\"\n+       $$emit$$\"# L_zero_64_bytes:\\n\\t\"\n+       $$emit$$\"sub     0x8,rcx\\n\\t\"\n+       $$emit$$\"jge     L_loop\\n\\t\"\n+       $$emit$$\"add     0x4,rcx\\n\\t\"\n+       $$emit$$\"jl      L_tail\\n\\t\"\n+       $$emit$$\"vmovdqu ymm0,(rax)\\n\\t\"\n+       $$emit$$\"add     0x20,rax\\n\\t\"\n+       $$emit$$\"sub     0x4,rcx\\n\\t\"\n+       $$emit$$\"# L_tail:\\t# Clearing tail bytes\\n\\t\"\n+       $$emit$$\"add     0x4,rcx\\n\\t\"\n+       $$emit$$\"jle     L_end\\n\\t\"\n+       $$emit$$\"dec     rcx\\n\\t\"\n+       $$emit$$\"# L_sloop:\\t# 8-byte short loop\\n\\t\"\n+       $$emit$$\"vmovq   xmm0,(rax)\\n\\t\"\n+       $$emit$$\"add     0x8,rax\\n\\t\"\n+       $$emit$$\"dec     rcx\\n\\t\"\n+       $$emit$$\"jge     L_sloop\\n\\t\"\n+       $$emit$$\"# L_end:\\n\\t\"\n+    } else {\n+       $$emit$$\"rep     stosq\\t# Store rax to *rdi++ while rcx--\\n\\t\"\n+    }\n+    $$emit$$\"# DONE\"\n+  %}\n+  ins_encode %{\n+    __ clear_mem($base$$Register, $cnt$$Register, $zero$$Register,\n+                 $tmp$$XMMRegister, false, knoreg);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ Small non-constant length ClearArray for AVX512 targets.\n+instruct rep_stos_evex(rcx_RegL cnt, rdi_RegP base, legRegD tmp, kReg ktmp, rax_RegI zero,\n+                       Universe dummy, rFlagsReg cr)\n+%{\n+  predicate(!((ClearArrayNode*)n)->is_large() && (UseAVX > 2));\n+  match(Set dummy (ClearArray cnt base));\n+  ins_cost(125);\n+  effect(USE_KILL cnt, USE_KILL base, TEMP tmp, TEMP ktmp, KILL zero, KILL cr);\n+\n+  format %{ $$template\n+    $$emit$$\"xorq    rax, rax\\t# ClearArray:\\n\\t\"\n+    $$emit$$\"cmp     InitArrayShortSize,rcx\\n\\t\"\n+    $$emit$$\"jg      LARGE\\n\\t\"\n+    $$emit$$\"dec     rcx\\n\\t\"\n+    $$emit$$\"js      DONE\\t# Zero length\\n\\t\"\n+    $$emit$$\"mov     rax,(rdi,rcx,8)\\t# LOOP\\n\\t\"\n+    $$emit$$\"dec     rcx\\n\\t\"\n+    $$emit$$\"jge     LOOP\\n\\t\"\n+    $$emit$$\"jmp     DONE\\n\\t\"\n+    $$emit$$\"# LARGE:\\n\\t\"\n+    if (UseFastStosb) {\n+       $$emit$$\"shlq    rcx,3\\t# Convert doublewords to bytes\\n\\t\"\n+       $$emit$$\"rep     stosb\\t# Store rax to *rdi++ while rcx--\\n\\t\"\n+    } else if (UseXMMForObjInit) {\n+       $$emit$$\"mov     rdi,rax\\n\\t\"\n+       $$emit$$\"vpxor   ymm0,ymm0,ymm0\\n\\t\"\n+       $$emit$$\"jmpq    L_zero_64_bytes\\n\\t\"\n+       $$emit$$\"# L_loop:\\t# 64-byte LOOP\\n\\t\"\n+       $$emit$$\"vmovdqu ymm0,(rax)\\n\\t\"\n+       $$emit$$\"vmovdqu ymm0,0x20(rax)\\n\\t\"\n+       $$emit$$\"add     0x40,rax\\n\\t\"\n+       $$emit$$\"# L_zero_64_bytes:\\n\\t\"\n+       $$emit$$\"sub     0x8,rcx\\n\\t\"\n+       $$emit$$\"jge     L_loop\\n\\t\"\n+       $$emit$$\"add     0x4,rcx\\n\\t\"\n+       $$emit$$\"jl      L_tail\\n\\t\"\n+       $$emit$$\"vmovdqu ymm0,(rax)\\n\\t\"\n+       $$emit$$\"add     0x20,rax\\n\\t\"\n+       $$emit$$\"sub     0x4,rcx\\n\\t\"\n+       $$emit$$\"# L_tail:\\t# Clearing tail bytes\\n\\t\"\n+       $$emit$$\"add     0x4,rcx\\n\\t\"\n+       $$emit$$\"jle     L_end\\n\\t\"\n+       $$emit$$\"dec     rcx\\n\\t\"\n+       $$emit$$\"# L_sloop:\\t# 8-byte short loop\\n\\t\"\n+       $$emit$$\"vmovq   xmm0,(rax)\\n\\t\"\n+       $$emit$$\"add     0x8,rax\\n\\t\"\n+       $$emit$$\"dec     rcx\\n\\t\"\n+       $$emit$$\"jge     L_sloop\\n\\t\"\n+       $$emit$$\"# L_end:\\n\\t\"\n+    } else {\n+       $$emit$$\"rep     stosq\\t# Store rax to *rdi++ while rcx--\\n\\t\"\n+    }\n+    $$emit$$\"# DONE\"\n+  %}\n+  ins_encode %{\n+    __ clear_mem($base$$Register, $cnt$$Register, $zero$$Register,\n+                 $tmp$$XMMRegister, false, $ktmp$$KRegister);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ Large non-constant length ClearArray for non-AVX512 targets.\n+instruct rep_stos_large(rcx_RegL cnt, rdi_RegP base, regD tmp, rax_RegI zero,\n+                        Universe dummy, rFlagsReg cr)\n+%{\n+  predicate((UseAVX <=2) && ((ClearArrayNode*)n)->is_large());\n+  match(Set dummy (ClearArray cnt base));\n+  effect(USE_KILL cnt, USE_KILL base, TEMP tmp, KILL zero, KILL cr);\n+\n+  format %{ $$template\n+    if (UseFastStosb) {\n+       $$emit$$\"xorq    rax, rax\\t# ClearArray:\\n\\t\"\n+       $$emit$$\"shlq    rcx,3\\t# Convert doublewords to bytes\\n\\t\"\n+       $$emit$$\"rep     stosb\\t# Store rax to *rdi++ while rcx--\"\n+    } else if (UseXMMForObjInit) {\n+       $$emit$$\"mov     rdi,rax\\t# ClearArray:\\n\\t\"\n+       $$emit$$\"vpxor   ymm0,ymm0,ymm0\\n\\t\"\n+       $$emit$$\"jmpq    L_zero_64_bytes\\n\\t\"\n+       $$emit$$\"# L_loop:\\t# 64-byte LOOP\\n\\t\"\n+       $$emit$$\"vmovdqu ymm0,(rax)\\n\\t\"\n+       $$emit$$\"vmovdqu ymm0,0x20(rax)\\n\\t\"\n+       $$emit$$\"add     0x40,rax\\n\\t\"\n+       $$emit$$\"# L_zero_64_bytes:\\n\\t\"\n+       $$emit$$\"sub     0x8,rcx\\n\\t\"\n+       $$emit$$\"jge     L_loop\\n\\t\"\n+       $$emit$$\"add     0x4,rcx\\n\\t\"\n+       $$emit$$\"jl      L_tail\\n\\t\"\n+       $$emit$$\"vmovdqu ymm0,(rax)\\n\\t\"\n+       $$emit$$\"add     0x20,rax\\n\\t\"\n+       $$emit$$\"sub     0x4,rcx\\n\\t\"\n+       $$emit$$\"# L_tail:\\t# Clearing tail bytes\\n\\t\"\n+       $$emit$$\"add     0x4,rcx\\n\\t\"\n+       $$emit$$\"jle     L_end\\n\\t\"\n+       $$emit$$\"dec     rcx\\n\\t\"\n+       $$emit$$\"# L_sloop:\\t# 8-byte short loop\\n\\t\"\n+       $$emit$$\"vmovq   xmm0,(rax)\\n\\t\"\n+       $$emit$$\"add     0x8,rax\\n\\t\"\n+       $$emit$$\"dec     rcx\\n\\t\"\n+       $$emit$$\"jge     L_sloop\\n\\t\"\n+       $$emit$$\"# L_end:\\n\\t\"\n+    } else {\n+       $$emit$$\"xorq    rax, rax\\t# ClearArray:\\n\\t\"\n+       $$emit$$\"rep     stosq\\t# Store rax to *rdi++ while rcx--\"\n+    }\n+  %}\n+  ins_encode %{\n+    __ clear_mem($base$$Register, $cnt$$Register, $zero$$Register,\n+                 $tmp$$XMMRegister, true, knoreg);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ Large non-constant length ClearArray for AVX512 targets.\n+instruct rep_stos_large_evex(rcx_RegL cnt, rdi_RegP base, legRegD tmp, kReg ktmp, rax_RegI zero,\n+                             Universe dummy, rFlagsReg cr)\n+%{\n+  predicate((UseAVX > 2) && ((ClearArrayNode*)n)->is_large());\n+  match(Set dummy (ClearArray cnt base));\n+  effect(USE_KILL cnt, USE_KILL base, TEMP tmp, TEMP ktmp, KILL zero, KILL cr);\n+\n+  format %{ $$template\n+    if (UseFastStosb) {\n+       $$emit$$\"xorq    rax, rax\\t# ClearArray:\\n\\t\"\n+       $$emit$$\"shlq    rcx,3\\t# Convert doublewords to bytes\\n\\t\"\n+       $$emit$$\"rep     stosb\\t# Store rax to *rdi++ while rcx--\"\n+    } else if (UseXMMForObjInit) {\n+       $$emit$$\"mov     rdi,rax\\t# ClearArray:\\n\\t\"\n+       $$emit$$\"vpxor   ymm0,ymm0,ymm0\\n\\t\"\n+       $$emit$$\"jmpq    L_zero_64_bytes\\n\\t\"\n+       $$emit$$\"# L_loop:\\t# 64-byte LOOP\\n\\t\"\n+       $$emit$$\"vmovdqu ymm0,(rax)\\n\\t\"\n+       $$emit$$\"vmovdqu ymm0,0x20(rax)\\n\\t\"\n+       $$emit$$\"add     0x40,rax\\n\\t\"\n+       $$emit$$\"# L_zero_64_bytes:\\n\\t\"\n+       $$emit$$\"sub     0x8,rcx\\n\\t\"\n+       $$emit$$\"jge     L_loop\\n\\t\"\n+       $$emit$$\"add     0x4,rcx\\n\\t\"\n+       $$emit$$\"jl      L_tail\\n\\t\"\n+       $$emit$$\"vmovdqu ymm0,(rax)\\n\\t\"\n+       $$emit$$\"add     0x20,rax\\n\\t\"\n+       $$emit$$\"sub     0x4,rcx\\n\\t\"\n+       $$emit$$\"# L_tail:\\t# Clearing tail bytes\\n\\t\"\n+       $$emit$$\"add     0x4,rcx\\n\\t\"\n+       $$emit$$\"jle     L_end\\n\\t\"\n+       $$emit$$\"dec     rcx\\n\\t\"\n+       $$emit$$\"# L_sloop:\\t# 8-byte short loop\\n\\t\"\n+       $$emit$$\"vmovq   xmm0,(rax)\\n\\t\"\n+       $$emit$$\"add     0x8,rax\\n\\t\"\n+       $$emit$$\"dec     rcx\\n\\t\"\n+       $$emit$$\"jge     L_sloop\\n\\t\"\n+       $$emit$$\"# L_end:\\n\\t\"\n+    } else {\n+       $$emit$$\"xorq    rax, rax\\t# ClearArray:\\n\\t\"\n+       $$emit$$\"rep     stosq\\t# Store rax to *rdi++ while rcx--\"\n+    }\n+  %}\n+  ins_encode %{\n+    __ clear_mem($base$$Register, $cnt$$Register, $zero$$Register,\n+                 $tmp$$XMMRegister, true, $ktmp$$KRegister);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ Small constant length ClearArray for AVX512 targets.\n+instruct rep_stos_im(immL cnt, rRegP base, regD tmp, rRegI zero, kReg ktmp, Universe dummy, rFlagsReg cr)\n+%{\n+  predicate(!((ClearArrayNode*)n)->is_large() && (MaxVectorSize >= 32) && VM_Version::supports_avx512vl());\n+  match(Set dummy (ClearArray cnt base));\n+  ins_cost(100);\n+  effect(TEMP tmp, TEMP zero, TEMP ktmp, KILL cr);\n+  format %{ \"clear_mem_imm $base , $cnt  \\n\\t\" %}\n+  ins_encode %{\n+   __ clear_mem($base$$Register, $cnt$$constant, $zero$$Register, $tmp$$XMMRegister, $ktmp$$KRegister);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct string_compareL(rdi_RegP str1, rcx_RegI cnt1, rsi_RegP str2, rdx_RegI cnt2,\n+                         rax_RegI result, legRegD tmp1, rFlagsReg cr)\n+%{\n+  predicate(!VM_Version::supports_avx512vlbw() && ((StrCompNode*)n)->encoding() == StrIntrinsicNode::LL);\n+  match(Set result (StrComp (Binary str1 cnt1) (Binary str2 cnt2)));\n+  effect(TEMP tmp1, USE_KILL str1, USE_KILL str2, USE_KILL cnt1, USE_KILL cnt2, KILL cr);\n+\n+  format %{ \"String Compare byte[] $str1,$cnt1,$str2,$cnt2 -> $result   \/\/ KILL $tmp1\" %}\n+  ins_encode %{\n+    __ string_compare($str1$$Register, $str2$$Register,\n+                      $cnt1$$Register, $cnt2$$Register, $result$$Register,\n+                      $tmp1$$XMMRegister, StrIntrinsicNode::LL, knoreg);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct string_compareL_evex(rdi_RegP str1, rcx_RegI cnt1, rsi_RegP str2, rdx_RegI cnt2,\n+                              rax_RegI result, legRegD tmp1, kReg ktmp, rFlagsReg cr)\n+%{\n+  predicate(VM_Version::supports_avx512vlbw() && ((StrCompNode*)n)->encoding() == StrIntrinsicNode::LL);\n+  match(Set result (StrComp (Binary str1 cnt1) (Binary str2 cnt2)));\n+  effect(TEMP tmp1, TEMP ktmp, USE_KILL str1, USE_KILL str2, USE_KILL cnt1, USE_KILL cnt2, KILL cr);\n+\n+  format %{ \"String Compare byte[] $str1,$cnt1,$str2,$cnt2 -> $result   \/\/ KILL $tmp1\" %}\n+  ins_encode %{\n+    __ string_compare($str1$$Register, $str2$$Register,\n+                      $cnt1$$Register, $cnt2$$Register, $result$$Register,\n+                      $tmp1$$XMMRegister, StrIntrinsicNode::LL, $ktmp$$KRegister);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct string_compareU(rdi_RegP str1, rcx_RegI cnt1, rsi_RegP str2, rdx_RegI cnt2,\n+                         rax_RegI result, legRegD tmp1, rFlagsReg cr)\n+%{\n+  predicate(!VM_Version::supports_avx512vlbw() && ((StrCompNode*)n)->encoding() == StrIntrinsicNode::UU);\n+  match(Set result (StrComp (Binary str1 cnt1) (Binary str2 cnt2)));\n+  effect(TEMP tmp1, USE_KILL str1, USE_KILL str2, USE_KILL cnt1, USE_KILL cnt2, KILL cr);\n+\n+  format %{ \"String Compare char[] $str1,$cnt1,$str2,$cnt2 -> $result   \/\/ KILL $tmp1\" %}\n+  ins_encode %{\n+    __ string_compare($str1$$Register, $str2$$Register,\n+                      $cnt1$$Register, $cnt2$$Register, $result$$Register,\n+                      $tmp1$$XMMRegister, StrIntrinsicNode::UU, knoreg);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct string_compareU_evex(rdi_RegP str1, rcx_RegI cnt1, rsi_RegP str2, rdx_RegI cnt2,\n+                              rax_RegI result, legRegD tmp1, kReg ktmp, rFlagsReg cr)\n+%{\n+  predicate(VM_Version::supports_avx512vlbw() && ((StrCompNode*)n)->encoding() == StrIntrinsicNode::UU);\n+  match(Set result (StrComp (Binary str1 cnt1) (Binary str2 cnt2)));\n+  effect(TEMP tmp1, TEMP ktmp, USE_KILL str1, USE_KILL str2, USE_KILL cnt1, USE_KILL cnt2, KILL cr);\n+\n+  format %{ \"String Compare char[] $str1,$cnt1,$str2,$cnt2 -> $result   \/\/ KILL $tmp1\" %}\n+  ins_encode %{\n+    __ string_compare($str1$$Register, $str2$$Register,\n+                      $cnt1$$Register, $cnt2$$Register, $result$$Register,\n+                      $tmp1$$XMMRegister, StrIntrinsicNode::UU, $ktmp$$KRegister);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct string_compareLU(rdi_RegP str1, rcx_RegI cnt1, rsi_RegP str2, rdx_RegI cnt2,\n+                          rax_RegI result, legRegD tmp1, rFlagsReg cr)\n+%{\n+  predicate(!VM_Version::supports_avx512vlbw() && ((StrCompNode*)n)->encoding() == StrIntrinsicNode::LU);\n+  match(Set result (StrComp (Binary str1 cnt1) (Binary str2 cnt2)));\n+  effect(TEMP tmp1, USE_KILL str1, USE_KILL str2, USE_KILL cnt1, USE_KILL cnt2, KILL cr);\n+\n+  format %{ \"String Compare byte[] $str1,$cnt1,$str2,$cnt2 -> $result   \/\/ KILL $tmp1\" %}\n+  ins_encode %{\n+    __ string_compare($str1$$Register, $str2$$Register,\n+                      $cnt1$$Register, $cnt2$$Register, $result$$Register,\n+                      $tmp1$$XMMRegister, StrIntrinsicNode::LU, knoreg);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct string_compareLU_evex(rdi_RegP str1, rcx_RegI cnt1, rsi_RegP str2, rdx_RegI cnt2,\n+                               rax_RegI result, legRegD tmp1, kReg ktmp, rFlagsReg cr)\n+%{\n+  predicate(VM_Version::supports_avx512vlbw() && ((StrCompNode*)n)->encoding() == StrIntrinsicNode::LU);\n+  match(Set result (StrComp (Binary str1 cnt1) (Binary str2 cnt2)));\n+  effect(TEMP tmp1, TEMP ktmp, USE_KILL str1, USE_KILL str2, USE_KILL cnt1, USE_KILL cnt2, KILL cr);\n+\n+  format %{ \"String Compare byte[] $str1,$cnt1,$str2,$cnt2 -> $result   \/\/ KILL $tmp1\" %}\n+  ins_encode %{\n+    __ string_compare($str1$$Register, $str2$$Register,\n+                      $cnt1$$Register, $cnt2$$Register, $result$$Register,\n+                      $tmp1$$XMMRegister, StrIntrinsicNode::LU, $ktmp$$KRegister);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct string_compareUL(rsi_RegP str1, rdx_RegI cnt1, rdi_RegP str2, rcx_RegI cnt2,\n+                          rax_RegI result, legRegD tmp1, rFlagsReg cr)\n+%{\n+  predicate(!VM_Version::supports_avx512vlbw() && ((StrCompNode*)n)->encoding() == StrIntrinsicNode::UL);\n+  match(Set result (StrComp (Binary str1 cnt1) (Binary str2 cnt2)));\n+  effect(TEMP tmp1, USE_KILL str1, USE_KILL str2, USE_KILL cnt1, USE_KILL cnt2, KILL cr);\n+\n+  format %{ \"String Compare byte[] $str1,$cnt1,$str2,$cnt2 -> $result   \/\/ KILL $tmp1\" %}\n+  ins_encode %{\n+    __ string_compare($str2$$Register, $str1$$Register,\n+                      $cnt2$$Register, $cnt1$$Register, $result$$Register,\n+                      $tmp1$$XMMRegister, StrIntrinsicNode::UL, knoreg);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct string_compareUL_evex(rsi_RegP str1, rdx_RegI cnt1, rdi_RegP str2, rcx_RegI cnt2,\n+                               rax_RegI result, legRegD tmp1, kReg ktmp, rFlagsReg cr)\n+%{\n+  predicate(VM_Version::supports_avx512vlbw() && ((StrCompNode*)n)->encoding() == StrIntrinsicNode::UL);\n+  match(Set result (StrComp (Binary str1 cnt1) (Binary str2 cnt2)));\n+  effect(TEMP tmp1, TEMP ktmp, USE_KILL str1, USE_KILL str2, USE_KILL cnt1, USE_KILL cnt2, KILL cr);\n+\n+  format %{ \"String Compare byte[] $str1,$cnt1,$str2,$cnt2 -> $result   \/\/ KILL $tmp1\" %}\n+  ins_encode %{\n+    __ string_compare($str2$$Register, $str1$$Register,\n+                      $cnt2$$Register, $cnt1$$Register, $result$$Register,\n+                      $tmp1$$XMMRegister, StrIntrinsicNode::UL, $ktmp$$KRegister);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+\/\/ fast search of substring with known size.\n+instruct string_indexof_conL(rdi_RegP str1, rdx_RegI cnt1, rsi_RegP str2, immI int_cnt2,\n+                             rbx_RegI result, legRegD tmp_vec, rax_RegI cnt2, rcx_RegI tmp, rFlagsReg cr)\n+%{\n+  predicate(UseSSE42Intrinsics && (((StrIndexOfNode*)n)->encoding() == StrIntrinsicNode::LL));\n+  match(Set result (StrIndexOf (Binary str1 cnt1) (Binary str2 int_cnt2)));\n+  effect(TEMP tmp_vec, USE_KILL str1, USE_KILL str2, USE_KILL cnt1, KILL cnt2, KILL tmp, KILL cr);\n+\n+  format %{ \"String IndexOf byte[] $str1,$cnt1,$str2,$int_cnt2 -> $result   \/\/ KILL $tmp_vec, $cnt1, $cnt2, $tmp\" %}\n+  ins_encode %{\n+    int icnt2 = (int)$int_cnt2$$constant;\n+    if (icnt2 >= 16) {\n+      \/\/ IndexOf for constant substrings with size >= 16 elements\n+      \/\/ which don't need to be loaded through stack.\n+      __ string_indexofC8($str1$$Register, $str2$$Register,\n+                          $cnt1$$Register, $cnt2$$Register,\n+                          icnt2, $result$$Register,\n+                          $tmp_vec$$XMMRegister, $tmp$$Register, StrIntrinsicNode::LL);\n+    } else {\n+      \/\/ Small strings are loaded through stack if they cross page boundary.\n+      __ string_indexof($str1$$Register, $str2$$Register,\n+                        $cnt1$$Register, $cnt2$$Register,\n+                        icnt2, $result$$Register,\n+                        $tmp_vec$$XMMRegister, $tmp$$Register, StrIntrinsicNode::LL);\n+    }\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+\/\/ fast search of substring with known size.\n+instruct string_indexof_conU(rdi_RegP str1, rdx_RegI cnt1, rsi_RegP str2, immI int_cnt2,\n+                             rbx_RegI result, legRegD tmp_vec, rax_RegI cnt2, rcx_RegI tmp, rFlagsReg cr)\n+%{\n+  predicate(UseSSE42Intrinsics && (((StrIndexOfNode*)n)->encoding() == StrIntrinsicNode::UU));\n+  match(Set result (StrIndexOf (Binary str1 cnt1) (Binary str2 int_cnt2)));\n+  effect(TEMP tmp_vec, USE_KILL str1, USE_KILL str2, USE_KILL cnt1, KILL cnt2, KILL tmp, KILL cr);\n+\n+  format %{ \"String IndexOf char[] $str1,$cnt1,$str2,$int_cnt2 -> $result   \/\/ KILL $tmp_vec, $cnt1, $cnt2, $tmp\" %}\n+  ins_encode %{\n+    int icnt2 = (int)$int_cnt2$$constant;\n+    if (icnt2 >= 8) {\n+      \/\/ IndexOf for constant substrings with size >= 8 elements\n+      \/\/ which don't need to be loaded through stack.\n+      __ string_indexofC8($str1$$Register, $str2$$Register,\n+                          $cnt1$$Register, $cnt2$$Register,\n+                          icnt2, $result$$Register,\n+                          $tmp_vec$$XMMRegister, $tmp$$Register, StrIntrinsicNode::UU);\n+    } else {\n+      \/\/ Small strings are loaded through stack if they cross page boundary.\n+      __ string_indexof($str1$$Register, $str2$$Register,\n+                        $cnt1$$Register, $cnt2$$Register,\n+                        icnt2, $result$$Register,\n+                        $tmp_vec$$XMMRegister, $tmp$$Register, StrIntrinsicNode::UU);\n+    }\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+\/\/ fast search of substring with known size.\n+instruct string_indexof_conUL(rdi_RegP str1, rdx_RegI cnt1, rsi_RegP str2, immI int_cnt2,\n+                              rbx_RegI result, legRegD tmp_vec, rax_RegI cnt2, rcx_RegI tmp, rFlagsReg cr)\n+%{\n+  predicate(UseSSE42Intrinsics && (((StrIndexOfNode*)n)->encoding() == StrIntrinsicNode::UL));\n+  match(Set result (StrIndexOf (Binary str1 cnt1) (Binary str2 int_cnt2)));\n+  effect(TEMP tmp_vec, USE_KILL str1, USE_KILL str2, USE_KILL cnt1, KILL cnt2, KILL tmp, KILL cr);\n+\n+  format %{ \"String IndexOf char[] $str1,$cnt1,$str2,$int_cnt2 -> $result   \/\/ KILL $tmp_vec, $cnt1, $cnt2, $tmp\" %}\n+  ins_encode %{\n+    int icnt2 = (int)$int_cnt2$$constant;\n+    if (icnt2 >= 8) {\n+      \/\/ IndexOf for constant substrings with size >= 8 elements\n+      \/\/ which don't need to be loaded through stack.\n+      __ string_indexofC8($str1$$Register, $str2$$Register,\n+                          $cnt1$$Register, $cnt2$$Register,\n+                          icnt2, $result$$Register,\n+                          $tmp_vec$$XMMRegister, $tmp$$Register, StrIntrinsicNode::UL);\n+    } else {\n+      \/\/ Small strings are loaded through stack if they cross page boundary.\n+      __ string_indexof($str1$$Register, $str2$$Register,\n+                        $cnt1$$Register, $cnt2$$Register,\n+                        icnt2, $result$$Register,\n+                        $tmp_vec$$XMMRegister, $tmp$$Register, StrIntrinsicNode::UL);\n+    }\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct string_indexofL(rdi_RegP str1, rdx_RegI cnt1, rsi_RegP str2, rax_RegI cnt2,\n+                         rbx_RegI result, legRegD tmp_vec, rcx_RegI tmp, rFlagsReg cr)\n+%{\n+  predicate(UseSSE42Intrinsics && (((StrIndexOfNode*)n)->encoding() == StrIntrinsicNode::LL));\n+  match(Set result (StrIndexOf (Binary str1 cnt1) (Binary str2 cnt2)));\n+  effect(TEMP tmp_vec, USE_KILL str1, USE_KILL str2, USE_KILL cnt1, USE_KILL cnt2, KILL tmp, KILL cr);\n+\n+  format %{ \"String IndexOf byte[] $str1,$cnt1,$str2,$cnt2 -> $result   \/\/ KILL all\" %}\n+  ins_encode %{\n+    __ string_indexof($str1$$Register, $str2$$Register,\n+                      $cnt1$$Register, $cnt2$$Register,\n+                      (-1), $result$$Register,\n+                      $tmp_vec$$XMMRegister, $tmp$$Register, StrIntrinsicNode::LL);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct string_indexofU(rdi_RegP str1, rdx_RegI cnt1, rsi_RegP str2, rax_RegI cnt2,\n+                         rbx_RegI result, legRegD tmp_vec, rcx_RegI tmp, rFlagsReg cr)\n+%{\n+  predicate(UseSSE42Intrinsics && (((StrIndexOfNode*)n)->encoding() == StrIntrinsicNode::UU));\n+  match(Set result (StrIndexOf (Binary str1 cnt1) (Binary str2 cnt2)));\n+  effect(TEMP tmp_vec, USE_KILL str1, USE_KILL str2, USE_KILL cnt1, USE_KILL cnt2, KILL tmp, KILL cr);\n+\n+  format %{ \"String IndexOf char[] $str1,$cnt1,$str2,$cnt2 -> $result   \/\/ KILL all\" %}\n+  ins_encode %{\n+    __ string_indexof($str1$$Register, $str2$$Register,\n+                      $cnt1$$Register, $cnt2$$Register,\n+                      (-1), $result$$Register,\n+                      $tmp_vec$$XMMRegister, $tmp$$Register, StrIntrinsicNode::UU);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct string_indexofUL(rdi_RegP str1, rdx_RegI cnt1, rsi_RegP str2, rax_RegI cnt2,\n+                          rbx_RegI result, legRegD tmp_vec, rcx_RegI tmp, rFlagsReg cr)\n+%{\n+  predicate(UseSSE42Intrinsics && (((StrIndexOfNode*)n)->encoding() == StrIntrinsicNode::UL));\n+  match(Set result (StrIndexOf (Binary str1 cnt1) (Binary str2 cnt2)));\n+  effect(TEMP tmp_vec, USE_KILL str1, USE_KILL str2, USE_KILL cnt1, USE_KILL cnt2, KILL tmp, KILL cr);\n+\n+  format %{ \"String IndexOf char[] $str1,$cnt1,$str2,$cnt2 -> $result   \/\/ KILL all\" %}\n+  ins_encode %{\n+    __ string_indexof($str1$$Register, $str2$$Register,\n+                      $cnt1$$Register, $cnt2$$Register,\n+                      (-1), $result$$Register,\n+                      $tmp_vec$$XMMRegister, $tmp$$Register, StrIntrinsicNode::UL);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct string_indexof_char(rdi_RegP str1, rdx_RegI cnt1, rax_RegI ch,\n+                              rbx_RegI result, legRegD tmp_vec1, legRegD tmp_vec2, legRegD tmp_vec3, rcx_RegI tmp, rFlagsReg cr)\n+%{\n+  predicate(UseSSE42Intrinsics && (((StrIndexOfCharNode*)n)->encoding() == StrIntrinsicNode::U));\n+  match(Set result (StrIndexOfChar (Binary str1 cnt1) ch));\n+  effect(TEMP tmp_vec1, TEMP tmp_vec2, TEMP tmp_vec3, USE_KILL str1, USE_KILL cnt1, USE_KILL ch, TEMP tmp, KILL cr);\n+  format %{ \"StringUTF16 IndexOf char[] $str1,$cnt1,$ch -> $result   \/\/ KILL all\" %}\n+  ins_encode %{\n+    __ string_indexof_char($str1$$Register, $cnt1$$Register, $ch$$Register, $result$$Register,\n+                           $tmp_vec1$$XMMRegister, $tmp_vec2$$XMMRegister, $tmp_vec3$$XMMRegister, $tmp$$Register);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct stringL_indexof_char(rdi_RegP str1, rdx_RegI cnt1, rax_RegI ch,\n+                              rbx_RegI result, legRegD tmp_vec1, legRegD tmp_vec2, legRegD tmp_vec3, rcx_RegI tmp, rFlagsReg cr)\n+%{\n+  predicate(UseSSE42Intrinsics && (((StrIndexOfCharNode*)n)->encoding() == StrIntrinsicNode::L));\n+  match(Set result (StrIndexOfChar (Binary str1 cnt1) ch));\n+  effect(TEMP tmp_vec1, TEMP tmp_vec2, TEMP tmp_vec3, USE_KILL str1, USE_KILL cnt1, USE_KILL ch, TEMP tmp, KILL cr);\n+  format %{ \"StringLatin1 IndexOf char[] $str1,$cnt1,$ch -> $result   \/\/ KILL all\" %}\n+  ins_encode %{\n+    __ stringL_indexof_char($str1$$Register, $cnt1$$Register, $ch$$Register, $result$$Register,\n+                           $tmp_vec1$$XMMRegister, $tmp_vec2$$XMMRegister, $tmp_vec3$$XMMRegister, $tmp$$Register);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+\/\/ fast string equals\n+instruct string_equals(rdi_RegP str1, rsi_RegP str2, rcx_RegI cnt, rax_RegI result,\n+                       legRegD tmp1, legRegD tmp2, rbx_RegI tmp3, rFlagsReg cr)\n+%{\n+  predicate(!VM_Version::supports_avx512vlbw());\n+  match(Set result (StrEquals (Binary str1 str2) cnt));\n+  effect(TEMP tmp1, TEMP tmp2, USE_KILL str1, USE_KILL str2, USE_KILL cnt, KILL tmp3, KILL cr);\n+\n+  format %{ \"String Equals $str1,$str2,$cnt -> $result    \/\/ KILL $tmp1, $tmp2, $tmp3\" %}\n+  ins_encode %{\n+    __ arrays_equals(false, $str1$$Register, $str2$$Register,\n+                     $cnt$$Register, $result$$Register, $tmp3$$Register,\n+                     $tmp1$$XMMRegister, $tmp2$$XMMRegister, false \/* char *\/, knoreg);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct string_equals_evex(rdi_RegP str1, rsi_RegP str2, rcx_RegI cnt, rax_RegI result,\n+                           legRegD tmp1, legRegD tmp2, kReg ktmp, rbx_RegI tmp3, rFlagsReg cr)\n+%{\n+  predicate(VM_Version::supports_avx512vlbw());\n+  match(Set result (StrEquals (Binary str1 str2) cnt));\n+  effect(TEMP tmp1, TEMP tmp2, TEMP ktmp, USE_KILL str1, USE_KILL str2, USE_KILL cnt, KILL tmp3, KILL cr);\n+\n+  format %{ \"String Equals $str1,$str2,$cnt -> $result    \/\/ KILL $tmp1, $tmp2, $tmp3\" %}\n+  ins_encode %{\n+    __ arrays_equals(false, $str1$$Register, $str2$$Register,\n+                     $cnt$$Register, $result$$Register, $tmp3$$Register,\n+                     $tmp1$$XMMRegister, $tmp2$$XMMRegister, false \/* char *\/, $ktmp$$KRegister);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+\/\/ fast array equals\n+instruct array_equalsB(rdi_RegP ary1, rsi_RegP ary2, rax_RegI result,\n+                       legRegD tmp1, legRegD tmp2, rcx_RegI tmp3, rbx_RegI tmp4, rFlagsReg cr)\n+%{\n+  predicate(!VM_Version::supports_avx512vlbw() && ((AryEqNode*)n)->encoding() == StrIntrinsicNode::LL);\n+  match(Set result (AryEq ary1 ary2));\n+  effect(TEMP tmp1, TEMP tmp2, USE_KILL ary1, USE_KILL ary2, KILL tmp3, KILL tmp4, KILL cr);\n+\n+  format %{ \"Array Equals byte[] $ary1,$ary2 -> $result   \/\/ KILL $tmp1, $tmp2, $tmp3, $tmp4\" %}\n+  ins_encode %{\n+    __ arrays_equals(true, $ary1$$Register, $ary2$$Register,\n+                     $tmp3$$Register, $result$$Register, $tmp4$$Register,\n+                     $tmp1$$XMMRegister, $tmp2$$XMMRegister, false \/* char *\/, knoreg);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct array_equalsB_evex(rdi_RegP ary1, rsi_RegP ary2, rax_RegI result,\n+                            legRegD tmp1, legRegD tmp2, kReg ktmp, rcx_RegI tmp3, rbx_RegI tmp4, rFlagsReg cr)\n+%{\n+  predicate(VM_Version::supports_avx512vlbw() && ((AryEqNode*)n)->encoding() == StrIntrinsicNode::LL);\n+  match(Set result (AryEq ary1 ary2));\n+  effect(TEMP tmp1, TEMP tmp2, TEMP ktmp, USE_KILL ary1, USE_KILL ary2, KILL tmp3, KILL tmp4, KILL cr);\n+\n+  format %{ \"Array Equals byte[] $ary1,$ary2 -> $result   \/\/ KILL $tmp1, $tmp2, $tmp3, $tmp4\" %}\n+  ins_encode %{\n+    __ arrays_equals(true, $ary1$$Register, $ary2$$Register,\n+                     $tmp3$$Register, $result$$Register, $tmp4$$Register,\n+                     $tmp1$$XMMRegister, $tmp2$$XMMRegister, false \/* char *\/, $ktmp$$KRegister);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct array_equalsC(rdi_RegP ary1, rsi_RegP ary2, rax_RegI result,\n+                       legRegD tmp1, legRegD tmp2, rcx_RegI tmp3, rbx_RegI tmp4, rFlagsReg cr)\n+%{\n+  predicate(!VM_Version::supports_avx512vlbw() && ((AryEqNode*)n)->encoding() == StrIntrinsicNode::UU);\n+  match(Set result (AryEq ary1 ary2));\n+  effect(TEMP tmp1, TEMP tmp2, USE_KILL ary1, USE_KILL ary2, KILL tmp3, KILL tmp4, KILL cr);\n+\n+  format %{ \"Array Equals char[] $ary1,$ary2 -> $result   \/\/ KILL $tmp1, $tmp2, $tmp3, $tmp4\" %}\n+  ins_encode %{\n+    __ arrays_equals(true, $ary1$$Register, $ary2$$Register,\n+                     $tmp3$$Register, $result$$Register, $tmp4$$Register,\n+                     $tmp1$$XMMRegister, $tmp2$$XMMRegister, true \/* char *\/, knoreg);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct array_equalsC_evex(rdi_RegP ary1, rsi_RegP ary2, rax_RegI result,\n+                            legRegD tmp1, legRegD tmp2, kReg ktmp, rcx_RegI tmp3, rbx_RegI tmp4, rFlagsReg cr)\n+%{\n+  predicate(VM_Version::supports_avx512vlbw() && ((AryEqNode*)n)->encoding() == StrIntrinsicNode::UU);\n+  match(Set result (AryEq ary1 ary2));\n+  effect(TEMP tmp1, TEMP tmp2, TEMP ktmp, USE_KILL ary1, USE_KILL ary2, KILL tmp3, KILL tmp4, KILL cr);\n+\n+  format %{ \"Array Equals char[] $ary1,$ary2 -> $result   \/\/ KILL $tmp1, $tmp2, $tmp3, $tmp4\" %}\n+  ins_encode %{\n+    __ arrays_equals(true, $ary1$$Register, $ary2$$Register,\n+                     $tmp3$$Register, $result$$Register, $tmp4$$Register,\n+                     $tmp1$$XMMRegister, $tmp2$$XMMRegister, true \/* char *\/, $ktmp$$KRegister);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct arrays_hashcode(rdi_RegP ary1, rdx_RegI cnt1, rbx_RegI result, immU8 basic_type,\n+                         legRegD tmp_vec1, legRegD tmp_vec2, legRegD tmp_vec3, legRegD tmp_vec4,\n+                         legRegD tmp_vec5, legRegD tmp_vec6, legRegD tmp_vec7, legRegD tmp_vec8,\n+                         legRegD tmp_vec9, legRegD tmp_vec10, legRegD tmp_vec11, legRegD tmp_vec12,\n+                         legRegD tmp_vec13, rRegI tmp1, rRegI tmp2, rRegI tmp3, rFlagsReg cr)\n+%{\n+  predicate(UseAVX >= 2);\n+  match(Set result (VectorizedHashCode (Binary ary1 cnt1) (Binary result basic_type)));\n+  effect(TEMP tmp_vec1, TEMP tmp_vec2, TEMP tmp_vec3, TEMP tmp_vec4, TEMP tmp_vec5, TEMP tmp_vec6,\n+         TEMP tmp_vec7, TEMP tmp_vec8, TEMP tmp_vec9, TEMP tmp_vec10, TEMP tmp_vec11, TEMP tmp_vec12,\n+         TEMP tmp_vec13, TEMP tmp1, TEMP tmp2, TEMP tmp3, USE_KILL ary1, USE_KILL cnt1,\n+         USE basic_type, KILL cr);\n+\n+  format %{ \"Array HashCode array[] $ary1,$cnt1,$result,$basic_type -> $result   \/\/ KILL all\" %}\n+  ins_encode %{\n+    __ arrays_hashcode($ary1$$Register, $cnt1$$Register, $result$$Register,\n+                       $tmp1$$Register, $tmp2$$Register, $tmp3$$Register,\n+                       $tmp_vec1$$XMMRegister, $tmp_vec2$$XMMRegister, $tmp_vec3$$XMMRegister,\n+                       $tmp_vec4$$XMMRegister, $tmp_vec5$$XMMRegister, $tmp_vec6$$XMMRegister,\n+                       $tmp_vec7$$XMMRegister, $tmp_vec8$$XMMRegister, $tmp_vec9$$XMMRegister,\n+                       $tmp_vec10$$XMMRegister, $tmp_vec11$$XMMRegister, $tmp_vec12$$XMMRegister,\n+                       $tmp_vec13$$XMMRegister, (BasicType)$basic_type$$constant);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct count_positives(rsi_RegP ary1, rcx_RegI len, rax_RegI result,\n+                         legRegD tmp1, legRegD tmp2, rbx_RegI tmp3, rFlagsReg cr,)\n+%{\n+  predicate(!VM_Version::supports_avx512vlbw() || !VM_Version::supports_bmi2());\n+  match(Set result (CountPositives ary1 len));\n+  effect(TEMP tmp1, TEMP tmp2, USE_KILL ary1, USE_KILL len, KILL tmp3, KILL cr);\n+\n+  format %{ \"countPositives byte[] $ary1,$len -> $result   \/\/ KILL $tmp1, $tmp2, $tmp3\" %}\n+  ins_encode %{\n+    __ count_positives($ary1$$Register, $len$$Register,\n+                       $result$$Register, $tmp3$$Register,\n+                       $tmp1$$XMMRegister, $tmp2$$XMMRegister, knoreg, knoreg);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct count_positives_evex(rsi_RegP ary1, rcx_RegI len, rax_RegI result,\n+                              legRegD tmp1, legRegD tmp2, kReg ktmp1, kReg ktmp2, rbx_RegI tmp3, rFlagsReg cr,)\n+%{\n+  predicate(VM_Version::supports_avx512vlbw() && VM_Version::supports_bmi2());\n+  match(Set result (CountPositives ary1 len));\n+  effect(TEMP tmp1, TEMP tmp2, TEMP ktmp1, TEMP ktmp2, USE_KILL ary1, USE_KILL len, KILL tmp3, KILL cr);\n+\n+  format %{ \"countPositives byte[] $ary1,$len -> $result   \/\/ KILL $tmp1, $tmp2, $tmp3\" %}\n+  ins_encode %{\n+    __ count_positives($ary1$$Register, $len$$Register,\n+                       $result$$Register, $tmp3$$Register,\n+                       $tmp1$$XMMRegister, $tmp2$$XMMRegister, $ktmp1$$KRegister, $ktmp2$$KRegister);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+\/\/ fast char[] to byte[] compression\n+instruct string_compress(rsi_RegP src, rdi_RegP dst, rdx_RegI len, legRegD tmp1, legRegD tmp2, legRegD tmp3,\n+                         legRegD tmp4, rcx_RegI tmp5, rax_RegI result, rFlagsReg cr) %{\n+  predicate(!VM_Version::supports_avx512vlbw() || !VM_Version::supports_bmi2());\n+  match(Set result (StrCompressedCopy src (Binary dst len)));\n+  effect(TEMP tmp1, TEMP tmp2, TEMP tmp3, TEMP tmp4, USE_KILL src, USE_KILL dst,\n+         USE_KILL len, KILL tmp5, KILL cr);\n+\n+  format %{ \"String Compress $src,$dst -> $result    \/\/ KILL RAX, RCX, RDX\" %}\n+  ins_encode %{\n+    __ char_array_compress($src$$Register, $dst$$Register, $len$$Register,\n+                           $tmp1$$XMMRegister, $tmp2$$XMMRegister, $tmp3$$XMMRegister,\n+                           $tmp4$$XMMRegister, $tmp5$$Register, $result$$Register,\n+                           knoreg, knoreg);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct string_compress_evex(rsi_RegP src, rdi_RegP dst, rdx_RegI len, legRegD tmp1, legRegD tmp2, legRegD tmp3,\n+                              legRegD tmp4, kReg ktmp1, kReg ktmp2, rcx_RegI tmp5, rax_RegI result, rFlagsReg cr) %{\n+  predicate(VM_Version::supports_avx512vlbw() && VM_Version::supports_bmi2());\n+  match(Set result (StrCompressedCopy src (Binary dst len)));\n+  effect(TEMP tmp1, TEMP tmp2, TEMP tmp3, TEMP tmp4, TEMP ktmp1, TEMP ktmp2, USE_KILL src, USE_KILL dst,\n+         USE_KILL len, KILL tmp5, KILL cr);\n+\n+  format %{ \"String Compress $src,$dst -> $result    \/\/ KILL RAX, RCX, RDX\" %}\n+  ins_encode %{\n+    __ char_array_compress($src$$Register, $dst$$Register, $len$$Register,\n+                           $tmp1$$XMMRegister, $tmp2$$XMMRegister, $tmp3$$XMMRegister,\n+                           $tmp4$$XMMRegister, $tmp5$$Register, $result$$Register,\n+                           $ktmp1$$KRegister, $ktmp2$$KRegister);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\/\/ fast byte[] to char[] inflation\n+instruct string_inflate(Universe dummy, rsi_RegP src, rdi_RegP dst, rdx_RegI len,\n+                        legRegD tmp1, rcx_RegI tmp2, rFlagsReg cr) %{\n+  predicate(!VM_Version::supports_avx512vlbw() || !VM_Version::supports_bmi2());\n+  match(Set dummy (StrInflatedCopy src (Binary dst len)));\n+  effect(TEMP tmp1, TEMP tmp2, USE_KILL src, USE_KILL dst, USE_KILL len, KILL cr);\n+\n+  format %{ \"String Inflate $src,$dst    \/\/ KILL $tmp1, $tmp2\" %}\n+  ins_encode %{\n+    __ byte_array_inflate($src$$Register, $dst$$Register, $len$$Register,\n+                          $tmp1$$XMMRegister, $tmp2$$Register, knoreg);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct string_inflate_evex(Universe dummy, rsi_RegP src, rdi_RegP dst, rdx_RegI len,\n+                             legRegD tmp1, kReg ktmp, rcx_RegI tmp2, rFlagsReg cr) %{\n+  predicate(VM_Version::supports_avx512vlbw() && VM_Version::supports_bmi2());\n+  match(Set dummy (StrInflatedCopy src (Binary dst len)));\n+  effect(TEMP tmp1, TEMP tmp2, TEMP ktmp, USE_KILL src, USE_KILL dst, USE_KILL len, KILL cr);\n+\n+  format %{ \"String Inflate $src,$dst    \/\/ KILL $tmp1, $tmp2\" %}\n+  ins_encode %{\n+    __ byte_array_inflate($src$$Register, $dst$$Register, $len$$Register,\n+                          $tmp1$$XMMRegister, $tmp2$$Register, $ktmp$$KRegister);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+\/\/ encode char[] to byte[] in ISO_8859_1\n+instruct encode_iso_array(rsi_RegP src, rdi_RegP dst, rdx_RegI len,\n+                          legRegD tmp1, legRegD tmp2, legRegD tmp3, legRegD tmp4,\n+                          rcx_RegI tmp5, rax_RegI result, rFlagsReg cr) %{\n+  predicate(!((EncodeISOArrayNode*)n)->is_ascii());\n+  match(Set result (EncodeISOArray src (Binary dst len)));\n+  effect(TEMP tmp1, TEMP tmp2, TEMP tmp3, TEMP tmp4, USE_KILL src, USE_KILL dst, USE_KILL len, KILL tmp5, KILL cr);\n+\n+  format %{ \"Encode iso array $src,$dst,$len -> $result    \/\/ KILL RCX, RDX, $tmp1, $tmp2, $tmp3, $tmp4, RSI, RDI \" %}\n+  ins_encode %{\n+    __ encode_iso_array($src$$Register, $dst$$Register, $len$$Register,\n+                        $tmp1$$XMMRegister, $tmp2$$XMMRegister, $tmp3$$XMMRegister,\n+                        $tmp4$$XMMRegister, $tmp5$$Register, $result$$Register, false);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+\/\/ encode char[] to byte[] in ASCII\n+instruct encode_ascii_array(rsi_RegP src, rdi_RegP dst, rdx_RegI len,\n+                            legRegD tmp1, legRegD tmp2, legRegD tmp3, legRegD tmp4,\n+                            rcx_RegI tmp5, rax_RegI result, rFlagsReg cr) %{\n+  predicate(((EncodeISOArrayNode*)n)->is_ascii());\n+  match(Set result (EncodeISOArray src (Binary dst len)));\n+  effect(TEMP tmp1, TEMP tmp2, TEMP tmp3, TEMP tmp4, USE_KILL src, USE_KILL dst, USE_KILL len, KILL tmp5, KILL cr);\n+\n+  format %{ \"Encode ascii array $src,$dst,$len -> $result    \/\/ KILL RCX, RDX, $tmp1, $tmp2, $tmp3, $tmp4, RSI, RDI \" %}\n+  ins_encode %{\n+    __ encode_iso_array($src$$Register, $dst$$Register, $len$$Register,\n+                        $tmp1$$XMMRegister, $tmp2$$XMMRegister, $tmp3$$XMMRegister,\n+                        $tmp4$$XMMRegister, $tmp5$$Register, $result$$Register, true);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+\/\/----------Overflow Math Instructions-----------------------------------------\n+\n+instruct overflowAddI_rReg(rFlagsReg cr, rax_RegI op1, rRegI op2)\n+%{\n+  match(Set cr (OverflowAddI op1 op2));\n+  effect(DEF cr, USE_KILL op1, USE op2);\n+\n+  format %{ \"addl    $op1, $op2\\t# overflow check int\" %}\n+\n+  ins_encode %{\n+    __ addl($op1$$Register, $op2$$Register);\n+  %}\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+instruct overflowAddI_rReg_imm(rFlagsReg cr, rax_RegI op1, immI op2)\n+%{\n+  match(Set cr (OverflowAddI op1 op2));\n+  effect(DEF cr, USE_KILL op1, USE op2);\n+\n+  format %{ \"addl    $op1, $op2\\t# overflow check int\" %}\n+\n+  ins_encode %{\n+    __ addl($op1$$Register, $op2$$constant);\n+  %}\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+instruct overflowAddL_rReg(rFlagsReg cr, rax_RegL op1, rRegL op2)\n+%{\n+  match(Set cr (OverflowAddL op1 op2));\n+  effect(DEF cr, USE_KILL op1, USE op2);\n+\n+  format %{ \"addq    $op1, $op2\\t# overflow check long\" %}\n+  ins_encode %{\n+    __ addq($op1$$Register, $op2$$Register);\n+  %}\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+instruct overflowAddL_rReg_imm(rFlagsReg cr, rax_RegL op1, immL32 op2)\n+%{\n+  match(Set cr (OverflowAddL op1 op2));\n+  effect(DEF cr, USE_KILL op1, USE op2);\n+\n+  format %{ \"addq    $op1, $op2\\t# overflow check long\" %}\n+  ins_encode %{\n+    __ addq($op1$$Register, $op2$$constant);\n+  %}\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+instruct overflowSubI_rReg(rFlagsReg cr, rRegI op1, rRegI op2)\n+%{\n+  match(Set cr (OverflowSubI op1 op2));\n+\n+  format %{ \"cmpl    $op1, $op2\\t# overflow check int\" %}\n+  ins_encode %{\n+    __ cmpl($op1$$Register, $op2$$Register);\n+  %}\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+instruct overflowSubI_rReg_imm(rFlagsReg cr, rRegI op1, immI op2)\n+%{\n+  match(Set cr (OverflowSubI op1 op2));\n+\n+  format %{ \"cmpl    $op1, $op2\\t# overflow check int\" %}\n+  ins_encode %{\n+    __ cmpl($op1$$Register, $op2$$constant);\n+  %}\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+instruct overflowSubL_rReg(rFlagsReg cr, rRegL op1, rRegL op2)\n+%{\n+  match(Set cr (OverflowSubL op1 op2));\n+\n+  format %{ \"cmpq    $op1, $op2\\t# overflow check long\" %}\n+  ins_encode %{\n+    __ cmpq($op1$$Register, $op2$$Register);\n+  %}\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+instruct overflowSubL_rReg_imm(rFlagsReg cr, rRegL op1, immL32 op2)\n+%{\n+  match(Set cr (OverflowSubL op1 op2));\n+\n+  format %{ \"cmpq    $op1, $op2\\t# overflow check long\" %}\n+  ins_encode %{\n+    __ cmpq($op1$$Register, $op2$$constant);\n+  %}\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+instruct overflowNegI_rReg(rFlagsReg cr, immI_0 zero, rax_RegI op2)\n+%{\n+  match(Set cr (OverflowSubI zero op2));\n+  effect(DEF cr, USE_KILL op2);\n+\n+  format %{ \"negl    $op2\\t# overflow check int\" %}\n+  ins_encode %{\n+    __ negl($op2$$Register);\n+  %}\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+instruct overflowNegL_rReg(rFlagsReg cr, immL0 zero, rax_RegL op2)\n+%{\n+  match(Set cr (OverflowSubL zero op2));\n+  effect(DEF cr, USE_KILL op2);\n+\n+  format %{ \"negq    $op2\\t# overflow check long\" %}\n+  ins_encode %{\n+    __ negq($op2$$Register);\n+  %}\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+instruct overflowMulI_rReg(rFlagsReg cr, rax_RegI op1, rRegI op2)\n+%{\n+  match(Set cr (OverflowMulI op1 op2));\n+  effect(DEF cr, USE_KILL op1, USE op2);\n+\n+  format %{ \"imull    $op1, $op2\\t# overflow check int\" %}\n+  ins_encode %{\n+    __ imull($op1$$Register, $op2$$Register);\n+  %}\n+  ins_pipe(ialu_reg_reg_alu0);\n+%}\n+\n+instruct overflowMulI_rReg_imm(rFlagsReg cr, rRegI op1, immI op2, rRegI tmp)\n+%{\n+  match(Set cr (OverflowMulI op1 op2));\n+  effect(DEF cr, TEMP tmp, USE op1, USE op2);\n+\n+  format %{ \"imull    $tmp, $op1, $op2\\t# overflow check int\" %}\n+  ins_encode %{\n+    __ imull($tmp$$Register, $op1$$Register, $op2$$constant);\n+  %}\n+  ins_pipe(ialu_reg_reg_alu0);\n+%}\n+\n+instruct overflowMulL_rReg(rFlagsReg cr, rax_RegL op1, rRegL op2)\n+%{\n+  match(Set cr (OverflowMulL op1 op2));\n+  effect(DEF cr, USE_KILL op1, USE op2);\n+\n+  format %{ \"imulq    $op1, $op2\\t# overflow check long\" %}\n+  ins_encode %{\n+    __ imulq($op1$$Register, $op2$$Register);\n+  %}\n+  ins_pipe(ialu_reg_reg_alu0);\n+%}\n+\n+instruct overflowMulL_rReg_imm(rFlagsReg cr, rRegL op1, immL32 op2, rRegL tmp)\n+%{\n+  match(Set cr (OverflowMulL op1 op2));\n+  effect(DEF cr, TEMP tmp, USE op1, USE op2);\n+\n+  format %{ \"imulq    $tmp, $op1, $op2\\t# overflow check long\" %}\n+  ins_encode %{\n+    __ imulq($tmp$$Register, $op1$$Register, $op2$$constant);\n+  %}\n+  ins_pipe(ialu_reg_reg_alu0);\n+%}\n+\n+\n+\/\/----------Control Flow Instructions------------------------------------------\n+\/\/ Signed compare Instructions\n+\n+\/\/ XXX more variants!!\n+instruct compI_rReg(rFlagsReg cr, rRegI op1, rRegI op2)\n+%{\n+  match(Set cr (CmpI op1 op2));\n+  effect(DEF cr, USE op1, USE op2);\n+\n+  format %{ \"cmpl    $op1, $op2\" %}\n+  ins_encode %{\n+    __ cmpl($op1$$Register, $op2$$Register);\n+  %}\n+  ins_pipe(ialu_cr_reg_reg);\n+%}\n+\n+instruct compI_rReg_imm(rFlagsReg cr, rRegI op1, immI op2)\n+%{\n+  match(Set cr (CmpI op1 op2));\n+\n+  format %{ \"cmpl    $op1, $op2\" %}\n+  ins_encode %{\n+    __ cmpl($op1$$Register, $op2$$constant);\n+  %}\n+  ins_pipe(ialu_cr_reg_imm);\n+%}\n+\n+instruct compI_rReg_mem(rFlagsReg cr, rRegI op1, memory op2)\n+%{\n+  match(Set cr (CmpI op1 (LoadI op2)));\n+\n+  ins_cost(500); \/\/ XXX\n+  format %{ \"cmpl    $op1, $op2\" %}\n+  ins_encode %{\n+    __ cmpl($op1$$Register, $op2$$Address);\n+  %}\n+  ins_pipe(ialu_cr_reg_mem);\n+%}\n+\n+instruct testI_reg(rFlagsReg cr, rRegI src, immI_0 zero)\n+%{\n+  match(Set cr (CmpI src zero));\n+\n+  format %{ \"testl   $src, $src\" %}\n+  ins_encode %{\n+    __ testl($src$$Register, $src$$Register);\n+  %}\n+  ins_pipe(ialu_cr_reg_imm);\n+%}\n+\n+instruct testI_reg_imm(rFlagsReg cr, rRegI src, immI con, immI_0 zero)\n+%{\n+  match(Set cr (CmpI (AndI src con) zero));\n+\n+  format %{ \"testl   $src, $con\" %}\n+  ins_encode %{\n+    __ testl($src$$Register, $con$$constant);\n+  %}\n+  ins_pipe(ialu_cr_reg_imm);\n+%}\n+\n+instruct testI_reg_reg(rFlagsReg cr, rRegI src1, rRegI src2, immI_0 zero)\n+%{\n+  match(Set cr (CmpI (AndI src1 src2) zero));\n+\n+  format %{ \"testl   $src1, $src2\" %}\n+  ins_encode %{\n+    __ testl($src1$$Register, $src2$$Register);\n+  %}\n+  ins_pipe(ialu_cr_reg_imm);\n+%}\n+\n+instruct testI_reg_mem(rFlagsReg cr, rRegI src, memory mem, immI_0 zero)\n+%{\n+  match(Set cr (CmpI (AndI src (LoadI mem)) zero));\n+\n+  format %{ \"testl   $src, $mem\" %}\n+  ins_encode %{\n+    __ testl($src$$Register, $mem$$Address);\n+  %}\n+  ins_pipe(ialu_cr_reg_mem);\n+%}\n+\n+\/\/ Unsigned compare Instructions; really, same as signed except they\n+\/\/ produce an rFlagsRegU instead of rFlagsReg.\n+instruct compU_rReg(rFlagsRegU cr, rRegI op1, rRegI op2)\n+%{\n+  match(Set cr (CmpU op1 op2));\n+\n+  format %{ \"cmpl    $op1, $op2\\t# unsigned\" %}\n+  ins_encode %{\n+    __ cmpl($op1$$Register, $op2$$Register);\n+  %}\n+  ins_pipe(ialu_cr_reg_reg);\n+%}\n+\n+instruct compU_rReg_imm(rFlagsRegU cr, rRegI op1, immI op2)\n+%{\n+  match(Set cr (CmpU op1 op2));\n+\n+  format %{ \"cmpl    $op1, $op2\\t# unsigned\" %}\n+  ins_encode %{\n+    __ cmpl($op1$$Register, $op2$$constant);\n+  %}\n+  ins_pipe(ialu_cr_reg_imm);\n+%}\n+\n+instruct compU_rReg_mem(rFlagsRegU cr, rRegI op1, memory op2)\n+%{\n+  match(Set cr (CmpU op1 (LoadI op2)));\n+\n+  ins_cost(500); \/\/ XXX\n+  format %{ \"cmpl    $op1, $op2\\t# unsigned\" %}\n+  ins_encode %{\n+    __ cmpl($op1$$Register, $op2$$Address);\n+  %}\n+  ins_pipe(ialu_cr_reg_mem);\n+%}\n+\n+instruct testU_reg(rFlagsRegU cr, rRegI src, immI_0 zero)\n+%{\n+  match(Set cr (CmpU src zero));\n+\n+  format %{ \"testl   $src, $src\\t# unsigned\" %}\n+  ins_encode %{\n+    __ testl($src$$Register, $src$$Register);\n+  %}\n+  ins_pipe(ialu_cr_reg_imm);\n+%}\n+\n+instruct compP_rReg(rFlagsRegU cr, rRegP op1, rRegP op2)\n+%{\n+  match(Set cr (CmpP op1 op2));\n+\n+  format %{ \"cmpq    $op1, $op2\\t# ptr\" %}\n+  ins_encode %{\n+    __ cmpq($op1$$Register, $op2$$Register);\n+  %}\n+  ins_pipe(ialu_cr_reg_reg);\n+%}\n+\n+instruct compP_rReg_mem(rFlagsRegU cr, rRegP op1, memory op2)\n+%{\n+  match(Set cr (CmpP op1 (LoadP op2)));\n+  predicate(n->in(2)->as_Load()->barrier_data() == 0);\n+\n+  ins_cost(500); \/\/ XXX\n+  format %{ \"cmpq    $op1, $op2\\t# ptr\" %}\n+  ins_encode %{\n+    __ cmpq($op1$$Register, $op2$$Address);\n+  %}\n+  ins_pipe(ialu_cr_reg_mem);\n+%}\n+\n+\/\/ XXX this is generalized by compP_rReg_mem???\n+\/\/ Compare raw pointer (used in out-of-heap check).\n+\/\/ Only works because non-oop pointers must be raw pointers\n+\/\/ and raw pointers have no anti-dependencies.\n+instruct compP_mem_rReg(rFlagsRegU cr, rRegP op1, memory op2)\n+%{\n+  predicate(n->in(2)->in(2)->bottom_type()->reloc() == relocInfo::none &&\n+            n->in(2)->as_Load()->barrier_data() == 0);\n+  match(Set cr (CmpP op1 (LoadP op2)));\n+\n+  format %{ \"cmpq    $op1, $op2\\t# raw ptr\" %}\n+  ins_encode %{\n+    __ cmpq($op1$$Register, $op2$$Address);\n+  %}\n+  ins_pipe(ialu_cr_reg_mem);\n+%}\n+\n+\/\/ This will generate a signed flags result. This should be OK since\n+\/\/ any compare to a zero should be eq\/neq.\n+instruct testP_reg(rFlagsReg cr, rRegP src, immP0 zero)\n+%{\n+  match(Set cr (CmpP src zero));\n@@ -1928,14 +16146,6 @@\n-  int size_in_bits = vlen * type2aelembytes(bt) * BitsPerByte;\n-  if (size_in_bits != 512 && !VM_Version::supports_avx512vl()) {\n-    return false;\n-  }\n-  switch(opcode) {\n-    \/\/ Unary masked operations\n-    case Op_AbsVB:\n-    case Op_AbsVS:\n-      if(!VM_Version::supports_avx512bw()) {\n-        return false;  \/\/ Implementation limitation\n-      }\n-    case Op_AbsVI:\n-    case Op_AbsVL:\n-      return true;\n+  format %{ \"testq   $src, $src\\t# ptr\" %}\n+  ins_encode %{\n+    __ testq($src$$Register, $src$$Register);\n+  %}\n+  ins_pipe(ialu_cr_reg_imm);\n+%}\n@@ -1943,4 +16153,7 @@\n-    \/\/ Ternary masked operations\n-    case Op_FmaVF:\n-    case Op_FmaVD:\n-      return true;\n+\/\/ This will generate a signed flags result. This should be OK since\n+\/\/ any compare to a zero should be eq\/neq.\n+instruct testP_mem(rFlagsReg cr, memory op, immP0 zero)\n+%{\n+  predicate((!UseCompressedOops || (CompressedOops::base() != nullptr)) &&\n+            n->in(1)->as_Load()->barrier_data() == 0);\n+  match(Set cr (CmpP (LoadP op) zero));\n@@ -1948,5 +16161,7 @@\n-    case Op_MacroLogicV:\n-      if(bt != T_INT && bt != T_LONG) {\n-        return false;\n-      }\n-      return true;\n+  ins_cost(500); \/\/ XXX\n+  format %{ \"testq   $op, 0xffffffffffffffff\\t# ptr\" %}\n+  ins_encode %{\n+    __ testq($op$$Address, 0xFFFFFFFF);\n+  %}\n+  ins_pipe(ialu_cr_reg_imm);\n+%}\n@@ -1954,14 +16169,5 @@\n-    \/\/ Binary masked operations\n-    case Op_AddVB:\n-    case Op_AddVS:\n-    case Op_SubVB:\n-    case Op_SubVS:\n-    case Op_MulVS:\n-    case Op_LShiftVS:\n-    case Op_RShiftVS:\n-    case Op_URShiftVS:\n-      assert(size_in_bits == 512 || VM_Version::supports_avx512vl(), \"\");\n-      if (!VM_Version::supports_avx512bw()) {\n-        return false;  \/\/ Implementation limitation\n-      }\n-      return true;\n+instruct testP_mem_reg0(rFlagsReg cr, memory mem, immP0 zero)\n+%{\n+  predicate(UseCompressedOops && (CompressedOops::base() == nullptr) &&\n+            n->in(1)->as_Load()->barrier_data() == 0);\n+  match(Set cr (CmpP (LoadP mem) zero));\n@@ -1969,6 +16175,6 @@\n-    case Op_MulVL:\n-      assert(size_in_bits == 512 || VM_Version::supports_avx512vl(), \"\");\n-      if (!VM_Version::supports_avx512dq()) {\n-        return false;  \/\/ Implementation limitation\n-      }\n-      return true;\n+  format %{ \"cmpq    R12, $mem\\t# ptr (R12_heapbase==0)\" %}\n+  ins_encode %{\n+    __ cmpq(r12, $mem$$Address);\n+  %}\n+  ins_pipe(ialu_cr_reg_mem);\n+%}\n@@ -1976,9 +16182,3 @@\n-    case Op_AndV:\n-    case Op_OrV:\n-    case Op_XorV:\n-    case Op_RotateRightV:\n-    case Op_RotateLeftV:\n-      if (bt != T_INT && bt != T_LONG) {\n-        return false; \/\/ Implementation limitation\n-      }\n-      return true;\n+instruct compN_rReg(rFlagsRegU cr, rRegN op1, rRegN op2)\n+%{\n+  match(Set cr (CmpN op1 op2));\n@@ -1986,6 +16186,4 @@\n-    case Op_VectorLoadMask:\n-      assert(size_in_bits == 512 || VM_Version::supports_avx512vl(), \"\");\n-      if (is_subword_type(bt) && !VM_Version::supports_avx512bw()) {\n-        return false;\n-      }\n-      return true;\n+  format %{ \"cmpl    $op1, $op2\\t# compressed ptr\" %}\n+  ins_encode %{ __ cmpl($op1$$Register, $op2$$Register); %}\n+  ins_pipe(ialu_cr_reg_reg);\n+%}\n@@ -1993,26 +16191,4 @@\n-    case Op_AddVI:\n-    case Op_AddVL:\n-    case Op_AddVF:\n-    case Op_AddVD:\n-    case Op_SubVI:\n-    case Op_SubVL:\n-    case Op_SubVF:\n-    case Op_SubVD:\n-    case Op_MulVI:\n-    case Op_MulVF:\n-    case Op_MulVD:\n-    case Op_DivVF:\n-    case Op_DivVD:\n-    case Op_SqrtVF:\n-    case Op_SqrtVD:\n-    case Op_LShiftVI:\n-    case Op_LShiftVL:\n-    case Op_RShiftVI:\n-    case Op_RShiftVL:\n-    case Op_URShiftVI:\n-    case Op_URShiftVL:\n-    case Op_LoadVectorMasked:\n-    case Op_StoreVectorMasked:\n-    case Op_LoadVectorGatherMasked:\n-    case Op_StoreVectorScatterMasked:\n-      return true;\n+instruct compN_rReg_mem(rFlagsRegU cr, rRegN src, memory mem)\n+%{\n+  predicate(n->in(2)->as_Load()->barrier_data() == 0);\n+  match(Set cr (CmpN src (LoadN mem)));\n@@ -2020,23 +16196,283 @@\n-    case Op_UMinV:\n-    case Op_UMaxV:\n-      if (size_in_bits < 512 && !VM_Version::supports_avx512vl()) {\n-        return false;\n-      } \/\/ fallthrough\n-    case Op_MaxV:\n-    case Op_MinV:\n-      if (is_subword_type(bt) && !VM_Version::supports_avx512bw()) {\n-        return false; \/\/ Implementation limitation\n-      }\n-      if (is_floating_point_type(bt) && !VM_Version::supports_avx10_2()) {\n-        return false; \/\/ Implementation limitation\n-      }\n-      return true;\n-    case Op_SaturatingAddV:\n-    case Op_SaturatingSubV:\n-      if (!is_subword_type(bt)) {\n-        return false;\n-      }\n-      if (size_in_bits < 128 || !VM_Version::supports_avx512bw()) {\n-        return false; \/\/ Implementation limitation\n-      }\n-      return true;\n+  format %{ \"cmpl    $src, $mem\\t# compressed ptr\" %}\n+  ins_encode %{\n+    __ cmpl($src$$Register, $mem$$Address);\n+  %}\n+  ins_pipe(ialu_cr_reg_mem);\n+%}\n+\n+instruct compN_rReg_imm(rFlagsRegU cr, rRegN op1, immN op2) %{\n+  match(Set cr (CmpN op1 op2));\n+\n+  format %{ \"cmpl    $op1, $op2\\t# compressed ptr\" %}\n+  ins_encode %{\n+    __ cmp_narrow_oop($op1$$Register, (jobject)$op2$$constant);\n+  %}\n+  ins_pipe(ialu_cr_reg_imm);\n+%}\n+\n+instruct compN_mem_imm(rFlagsRegU cr, memory mem, immN src)\n+%{\n+  predicate(n->in(2)->as_Load()->barrier_data() == 0);\n+  match(Set cr (CmpN src (LoadN mem)));\n+\n+  format %{ \"cmpl    $mem, $src\\t# compressed ptr\" %}\n+  ins_encode %{\n+    __ cmp_narrow_oop($mem$$Address, (jobject)$src$$constant);\n+  %}\n+  ins_pipe(ialu_cr_reg_mem);\n+%}\n+\n+instruct compN_rReg_imm_klass(rFlagsRegU cr, rRegN op1, immNKlass op2) %{\n+  match(Set cr (CmpN op1 op2));\n+\n+  format %{ \"cmpl    $op1, $op2\\t# compressed klass ptr\" %}\n+  ins_encode %{\n+    __ cmp_narrow_klass($op1$$Register, (Klass*)$op2$$constant);\n+  %}\n+  ins_pipe(ialu_cr_reg_imm);\n+%}\n+\n+instruct compN_mem_imm_klass(rFlagsRegU cr, memory mem, immNKlass src)\n+%{\n+  predicate(!UseCompactObjectHeaders);\n+  match(Set cr (CmpN src (LoadNKlass mem)));\n+\n+  format %{ \"cmpl    $mem, $src\\t# compressed klass ptr\" %}\n+  ins_encode %{\n+    __ cmp_narrow_klass($mem$$Address, (Klass*)$src$$constant);\n+  %}\n+  ins_pipe(ialu_cr_reg_mem);\n+%}\n+\n+instruct testN_reg(rFlagsReg cr, rRegN src, immN0 zero) %{\n+  match(Set cr (CmpN src zero));\n+\n+  format %{ \"testl   $src, $src\\t# compressed ptr\" %}\n+  ins_encode %{ __ testl($src$$Register, $src$$Register); %}\n+  ins_pipe(ialu_cr_reg_imm);\n+%}\n+\n+instruct testN_mem(rFlagsReg cr, memory mem, immN0 zero)\n+%{\n+  predicate(CompressedOops::base() != nullptr &&\n+            n->in(1)->as_Load()->barrier_data() == 0);\n+  match(Set cr (CmpN (LoadN mem) zero));\n+\n+  ins_cost(500); \/\/ XXX\n+  format %{ \"testl   $mem, 0xffffffff\\t# compressed ptr\" %}\n+  ins_encode %{\n+    __ cmpl($mem$$Address, (int)0xFFFFFFFF);\n+  %}\n+  ins_pipe(ialu_cr_reg_mem);\n+%}\n+\n+instruct testN_mem_reg0(rFlagsReg cr, memory mem, immN0 zero)\n+%{\n+  predicate(CompressedOops::base() == nullptr &&\n+            n->in(1)->as_Load()->barrier_data() == 0);\n+  match(Set cr (CmpN (LoadN mem) zero));\n+\n+  format %{ \"cmpl    R12, $mem\\t# compressed ptr (R12_heapbase==0)\" %}\n+  ins_encode %{\n+    __ cmpl(r12, $mem$$Address);\n+  %}\n+  ins_pipe(ialu_cr_reg_mem);\n+%}\n+\n+\/\/ Yanked all unsigned pointer compare operations.\n+\/\/ Pointer compares are done with CmpP which is already unsigned.\n+\n+instruct compL_rReg(rFlagsReg cr, rRegL op1, rRegL op2)\n+%{\n+  match(Set cr (CmpL op1 op2));\n+\n+  format %{ \"cmpq    $op1, $op2\" %}\n+  ins_encode %{\n+    __ cmpq($op1$$Register, $op2$$Register);\n+  %}\n+  ins_pipe(ialu_cr_reg_reg);\n+%}\n+\n+instruct compL_rReg_imm(rFlagsReg cr, rRegL op1, immL32 op2)\n+%{\n+  match(Set cr (CmpL op1 op2));\n+\n+  format %{ \"cmpq    $op1, $op2\" %}\n+  ins_encode %{\n+    __ cmpq($op1$$Register, $op2$$constant);\n+  %}\n+  ins_pipe(ialu_cr_reg_imm);\n+%}\n+\n+instruct compL_rReg_mem(rFlagsReg cr, rRegL op1, memory op2)\n+%{\n+  match(Set cr (CmpL op1 (LoadL op2)));\n+\n+  format %{ \"cmpq    $op1, $op2\" %}\n+  ins_encode %{\n+    __ cmpq($op1$$Register, $op2$$Address);\n+  %}\n+  ins_pipe(ialu_cr_reg_mem);\n+%}\n+\n+instruct testL_reg(rFlagsReg cr, rRegL src, immL0 zero)\n+%{\n+  match(Set cr (CmpL src zero));\n+\n+  format %{ \"testq   $src, $src\" %}\n+  ins_encode %{\n+    __ testq($src$$Register, $src$$Register);\n+  %}\n+  ins_pipe(ialu_cr_reg_imm);\n+%}\n+\n+instruct testL_reg_imm(rFlagsReg cr, rRegL src, immL32 con, immL0 zero)\n+%{\n+  match(Set cr (CmpL (AndL src con) zero));\n+\n+  format %{ \"testq   $src, $con\\t# long\" %}\n+  ins_encode %{\n+    __ testq($src$$Register, $con$$constant);\n+  %}\n+  ins_pipe(ialu_cr_reg_imm);\n+%}\n+\n+instruct testL_reg_reg(rFlagsReg cr, rRegL src1, rRegL src2, immL0 zero)\n+%{\n+  match(Set cr (CmpL (AndL src1 src2) zero));\n+\n+  format %{ \"testq   $src1, $src2\\t# long\" %}\n+  ins_encode %{\n+    __ testq($src1$$Register, $src2$$Register);\n+  %}\n+  ins_pipe(ialu_cr_reg_imm);\n+%}\n+\n+instruct testL_reg_mem(rFlagsReg cr, rRegL src, memory mem, immL0 zero)\n+%{\n+  match(Set cr (CmpL (AndL src (LoadL mem)) zero));\n+\n+  format %{ \"testq   $src, $mem\" %}\n+  ins_encode %{\n+    __ testq($src$$Register, $mem$$Address);\n+  %}\n+  ins_pipe(ialu_cr_reg_mem);\n+%}\n+\n+instruct testL_reg_mem2(rFlagsReg cr, rRegP src, memory mem, immL0 zero)\n+%{\n+  match(Set cr (CmpL (AndL (CastP2X src) (LoadL mem)) zero));\n+\n+  format %{ \"testq   $src, $mem\" %}\n+  ins_encode %{\n+    __ testq($src$$Register, $mem$$Address);\n+  %}\n+  ins_pipe(ialu_cr_reg_mem);\n+%}\n+\n+\/\/ Manifest a CmpU result in an integer register.  Very painful.\n+\/\/ This is the test to avoid.\n+instruct cmpU3_reg_reg(rRegI dst, rRegI src1, rRegI src2, rFlagsReg flags)\n+%{\n+  match(Set dst (CmpU3 src1 src2));\n+  effect(KILL flags);\n+\n+  ins_cost(275); \/\/ XXX\n+  format %{ \"cmpl    $src1, $src2\\t# CmpL3\\n\\t\"\n+            \"movl    $dst, -1\\n\\t\"\n+            \"jb,u    done\\n\\t\"\n+            \"setcc   $dst \\t# emits setne + movzbl or setzune for APX\"\n+    \"done:\" %}\n+  ins_encode %{\n+    Label done;\n+    __ cmpl($src1$$Register, $src2$$Register);\n+    __ movl($dst$$Register, -1);\n+    __ jccb(Assembler::below, done);\n+    __ setcc(Assembler::notZero, $dst$$Register);\n+    __ bind(done);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ Manifest a CmpL result in an integer register.  Very painful.\n+\/\/ This is the test to avoid.\n+instruct cmpL3_reg_reg(rRegI dst, rRegL src1, rRegL src2, rFlagsReg flags)\n+%{\n+  match(Set dst (CmpL3 src1 src2));\n+  effect(KILL flags);\n+\n+  ins_cost(275); \/\/ XXX\n+  format %{ \"cmpq    $src1, $src2\\t# CmpL3\\n\\t\"\n+            \"movl    $dst, -1\\n\\t\"\n+            \"jl,s    done\\n\\t\"\n+            \"setcc   $dst \\t# emits setne + movzbl or setzune for APX\"\n+    \"done:\" %}\n+  ins_encode %{\n+    Label done;\n+    __ cmpq($src1$$Register, $src2$$Register);\n+    __ movl($dst$$Register, -1);\n+    __ jccb(Assembler::less, done);\n+    __ setcc(Assembler::notZero, $dst$$Register);\n+    __ bind(done);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ Manifest a CmpUL result in an integer register.  Very painful.\n+\/\/ This is the test to avoid.\n+instruct cmpUL3_reg_reg(rRegI dst, rRegL src1, rRegL src2, rFlagsReg flags)\n+%{\n+  match(Set dst (CmpUL3 src1 src2));\n+  effect(KILL flags);\n+\n+  ins_cost(275); \/\/ XXX\n+  format %{ \"cmpq    $src1, $src2\\t# CmpL3\\n\\t\"\n+            \"movl    $dst, -1\\n\\t\"\n+            \"jb,u    done\\n\\t\"\n+            \"setcc   $dst \\t# emits setne + movzbl or setzune for APX\"\n+    \"done:\" %}\n+  ins_encode %{\n+    Label done;\n+    __ cmpq($src1$$Register, $src2$$Register);\n+    __ movl($dst$$Register, -1);\n+    __ jccb(Assembler::below, done);\n+    __ setcc(Assembler::notZero, $dst$$Register);\n+    __ bind(done);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ Unsigned long compare Instructions; really, same as signed long except they\n+\/\/ produce an rFlagsRegU instead of rFlagsReg.\n+instruct compUL_rReg(rFlagsRegU cr, rRegL op1, rRegL op2)\n+%{\n+  match(Set cr (CmpUL op1 op2));\n+\n+  format %{ \"cmpq    $op1, $op2\\t# unsigned\" %}\n+  ins_encode %{\n+    __ cmpq($op1$$Register, $op2$$Register);\n+  %}\n+  ins_pipe(ialu_cr_reg_reg);\n+%}\n+\n+instruct compUL_rReg_imm(rFlagsRegU cr, rRegL op1, immL32 op2)\n+%{\n+  match(Set cr (CmpUL op1 op2));\n+\n+  format %{ \"cmpq    $op1, $op2\\t# unsigned\" %}\n+  ins_encode %{\n+    __ cmpq($op1$$Register, $op2$$constant);\n+  %}\n+  ins_pipe(ialu_cr_reg_imm);\n+%}\n+\n+instruct compUL_rReg_mem(rFlagsRegU cr, rRegL op1, memory op2)\n+%{\n+  match(Set cr (CmpUL op1 (LoadL op2)));\n+\n+  format %{ \"cmpq    $op1, $op2\\t# unsigned\" %}\n+  ins_encode %{\n+    __ cmpq($op1$$Register, $op2$$Address);\n+  %}\n+  ins_pipe(ialu_cr_reg_mem);\n+%}\n@@ -2044,5 +16480,3 @@\n-    case Op_VectorMaskCmp:\n-      if (is_subword_type(bt) && !VM_Version::supports_avx512bw()) {\n-        return false; \/\/ Implementation limitation\n-      }\n-      return true;\n+instruct testUL_reg(rFlagsRegU cr, rRegL src, immL0 zero)\n+%{\n+  match(Set cr (CmpUL src zero));\n@@ -2050,10 +16484,6 @@\n-    case Op_VectorRearrange:\n-      if (bt == T_SHORT && !VM_Version::supports_avx512bw()) {\n-        return false; \/\/ Implementation limitation\n-      }\n-      if (bt == T_BYTE && !VM_Version::supports_avx512_vbmi()) {\n-        return false; \/\/ Implementation limitation\n-      } else if ((bt == T_INT || bt == T_FLOAT) && size_in_bits < 256) {\n-        return false; \/\/ Implementation limitation\n-      }\n-      return true;\n+  format %{ \"testq   $src, $src\\t# unsigned\" %}\n+  ins_encode %{\n+    __ testq($src$$Register, $src$$Register);\n+  %}\n+  ins_pipe(ialu_cr_reg_imm);\n+%}\n@@ -2061,8 +16491,3 @@\n-    \/\/ Binary Logical operations\n-    case Op_AndVMask:\n-    case Op_OrVMask:\n-    case Op_XorVMask:\n-      if (vlen > 16 && !VM_Version::supports_avx512bw()) {\n-        return false; \/\/ Implementation limitation\n-      }\n-      return true;\n+instruct compB_mem_imm(rFlagsReg cr, memory mem, immI8 imm)\n+%{\n+  match(Set cr (CmpI (LoadB mem) imm));\n@@ -2070,6 +16495,5 @@\n-    case Op_PopCountVI:\n-    case Op_PopCountVL:\n-      if (!is_pop_count_instr_target(bt)) {\n-        return false;\n-      }\n-      return true;\n+  ins_cost(125);\n+  format %{ \"cmpb    $mem, $imm\" %}\n+  ins_encode %{ __ cmpb($mem$$Address, $imm$$constant); %}\n+  ins_pipe(ialu_cr_reg_mem);\n+%}\n@@ -2077,2 +16501,3 @@\n-    case Op_MaskAll:\n-      return true;\n+instruct testUB_mem_imm(rFlagsReg cr, memory mem, immU7 imm, immI_0 zero)\n+%{\n+  match(Set cr (CmpI (AndI (LoadUB mem) imm) zero));\n@@ -2080,8 +16505,5 @@\n-    case Op_CountLeadingZerosV:\n-      if (is_non_subword_integral_type(bt) && VM_Version::supports_avx512cd()) {\n-        return true;\n-      }\n-    default:\n-      return false;\n-  }\n-}\n+  ins_cost(125);\n+  format %{ \"testb   $mem, $imm\\t# ubyte\" %}\n+  ins_encode %{ __ testb($mem$$Address, $imm$$constant); %}\n+  ins_pipe(ialu_cr_reg_mem);\n+%}\n@@ -2089,3 +16511,3 @@\n-bool Matcher::vector_needs_partial_operations(Node* node, const TypeVect* vt) {\n-  return false;\n-}\n+instruct testB_mem_imm(rFlagsReg cr, memory mem, immI8 imm, immI_0 zero)\n+%{\n+  match(Set cr (CmpI (AndI (LoadB mem) imm) zero));\n@@ -2093,12 +16515,5 @@\n-\/\/ Return true if Vector::rearrange needs preparation of the shuffle argument\n-bool Matcher::vector_rearrange_requires_load_shuffle(BasicType elem_bt, int vlen) {\n-  switch (elem_bt) {\n-    case T_BYTE:  return false;\n-    case T_SHORT: return !VM_Version::supports_avx512bw();\n-    case T_INT:   return !VM_Version::supports_avx();\n-    case T_LONG:  return vlen < 8 && !VM_Version::supports_avx512vl();\n-    default:\n-      ShouldNotReachHere();\n-      return false;\n-  }\n-}\n+  ins_cost(125);\n+  format %{ \"testb   $mem, $imm\\t# byte\" %}\n+  ins_encode %{ __ testb($mem$$Address, $imm$$constant); %}\n+  ins_pipe(ialu_cr_reg_mem);\n+%}\n@@ -2106,28 +16521,2 @@\n-MachOper* Matcher::pd_specialize_generic_vector_operand(MachOper* generic_opnd, uint ideal_reg, bool is_temp) {\n-  assert(Matcher::is_generic_vector(generic_opnd), \"not generic\");\n-  bool legacy = (generic_opnd->opcode() == LEGVEC);\n-  if (!VM_Version::supports_avx512vlbwdq() && \/\/ KNL\n-      is_temp && !legacy && (ideal_reg == Op_VecZ)) {\n-    \/\/ Conservatively specialize 512bit vec TEMP operands to legVecZ (zmm0-15) on KNL.\n-    return new legVecZOper();\n-  }\n-  if (legacy) {\n-    switch (ideal_reg) {\n-      case Op_VecS: return new legVecSOper();\n-      case Op_VecD: return new legVecDOper();\n-      case Op_VecX: return new legVecXOper();\n-      case Op_VecY: return new legVecYOper();\n-      case Op_VecZ: return new legVecZOper();\n-    }\n-  } else {\n-    switch (ideal_reg) {\n-      case Op_VecS: return new vecSOper();\n-      case Op_VecD: return new vecDOper();\n-      case Op_VecX: return new vecXOper();\n-      case Op_VecY: return new vecYOper();\n-      case Op_VecZ: return new vecZOper();\n-    }\n-  }\n-  ShouldNotReachHere();\n-  return nullptr;\n-}\n+\/\/----------Max and Min--------------------------------------------------------\n+\/\/ Min Instructions\n@@ -2135,17 +16524,4 @@\n-bool Matcher::is_reg2reg_move(MachNode* m) {\n-  switch (m->rule()) {\n-    case MoveVec2Leg_rule:\n-    case MoveLeg2Vec_rule:\n-    case MoveF2VL_rule:\n-    case MoveF2LEG_rule:\n-    case MoveVL2F_rule:\n-    case MoveLEG2F_rule:\n-    case MoveD2VL_rule:\n-    case MoveD2LEG_rule:\n-    case MoveVL2D_rule:\n-    case MoveLEG2D_rule:\n-      return true;\n-    default:\n-      return false;\n-  }\n-}\n+instruct cmovI_reg_g(rRegI dst, rRegI src, rFlagsReg cr)\n+%{\n+  predicate(!UseAPX);\n+  effect(USE_DEF dst, USE src, USE cr);\n@@ -2153,9 +16529,6 @@\n-bool Matcher::is_generic_vector(MachOper* opnd) {\n-  switch (opnd->opcode()) {\n-    case VEC:\n-    case LEGVEC:\n-      return true;\n-    default:\n-      return false;\n-  }\n-}\n+  format %{ \"cmovlgt $dst, $src\\t# min\" %}\n+  ins_encode %{\n+    __ cmovl(Assembler::greater, $dst$$Register, $src$$Register);\n+  %}\n+  ins_pipe(pipe_cmov_reg);\n+%}\n@@ -2163,1 +16536,4 @@\n-\/\/------------------------------------------------------------------------\n+instruct cmovI_reg_g_ndd(rRegI dst, rRegI src1, rRegI src2, rFlagsReg cr)\n+%{\n+  predicate(UseAPX);\n+  effect(DEF dst, USE src1, USE src2, USE cr);\n@@ -2165,3 +16541,6 @@\n-const RegMask* Matcher::predicate_reg_mask(void) {\n-  return &_VECTMASK_REG_mask;\n-}\n+  format %{ \"ecmovlgt $dst, $src1, $src2\\t# min ndd\" %}\n+  ins_encode %{\n+    __ ecmovl(Assembler::greater, $dst$$Register, $src1$$Register, $src2$$Register);\n+  %}\n+  ins_pipe(pipe_cmov_reg);\n+%}\n@@ -2169,41 +16548,4 @@\n-\/\/ Max vector size in bytes. 0 if not supported.\n-int Matcher::vector_width_in_bytes(BasicType bt) {\n-  assert(is_java_primitive(bt), \"only primitive type vectors\");\n-  \/\/ SSE2 supports 128bit vectors for all types.\n-  \/\/ AVX2 supports 256bit vectors for all types.\n-  \/\/ AVX2\/EVEX supports 512bit vectors for all types.\n-  int size = (UseAVX > 1) ? (1 << UseAVX) * 8 : 16;\n-  \/\/ AVX1 supports 256bit vectors only for FLOAT and DOUBLE.\n-  if (UseAVX > 0 && (bt == T_FLOAT || bt == T_DOUBLE))\n-    size = (UseAVX > 2) ? 64 : 32;\n-  if (UseAVX > 2 && (bt == T_BYTE || bt == T_SHORT || bt == T_CHAR))\n-    size = (VM_Version::supports_avx512bw()) ? 64 : 32;\n-  \/\/ Use flag to limit vector size.\n-  size = MIN2(size,(int)MaxVectorSize);\n-  \/\/ Minimum 2 values in vector (or 4 for bytes).\n-  switch (bt) {\n-  case T_DOUBLE:\n-  case T_LONG:\n-    if (size < 16) return 0;\n-    break;\n-  case T_FLOAT:\n-  case T_INT:\n-    if (size < 8) return 0;\n-    break;\n-  case T_BOOLEAN:\n-    if (size < 4) return 0;\n-    break;\n-  case T_CHAR:\n-    if (size < 4) return 0;\n-    break;\n-  case T_BYTE:\n-    if (size < 4) return 0;\n-    break;\n-  case T_SHORT:\n-    if (size < 4) return 0;\n-    break;\n-  default:\n-    ShouldNotReachHere();\n-  }\n-  return size;\n-}\n+instruct minI_rReg(rRegI dst, rRegI src)\n+%{\n+  predicate(!UseAPX);\n+  match(Set dst (MinI dst src));\n@@ -2211,14 +16553,7 @@\n-\/\/ Limits on vector size (number of elements) loaded into vector.\n-int Matcher::max_vector_size(const BasicType bt) {\n-  return vector_width_in_bytes(bt)\/type2aelembytes(bt);\n-}\n-int Matcher::min_vector_size(const BasicType bt) {\n-  int max_size = max_vector_size(bt);\n-  \/\/ Min size which can be loaded into vector is 4 bytes.\n-  int size = (type2aelembytes(bt) == 1) ? 4 : 2;\n-  \/\/ Support for calling svml double64 vectors\n-  if (bt == T_DOUBLE) {\n-    size = 1;\n-  }\n-  return MIN2(size,max_size);\n-}\n+  ins_cost(200);\n+  expand %{\n+    rFlagsReg cr;\n+    compI_rReg(cr, dst, src);\n+    cmovI_reg_g(dst, src, cr);\n+  %}\n+%}\n@@ -2226,8 +16561,5 @@\n-int Matcher::max_vector_size_auto_vectorization(const BasicType bt) {\n-  \/\/ Limit the max vector size for auto vectorization to 256 bits (32 bytes)\n-  \/\/ by default on Cascade Lake\n-  if (VM_Version::is_default_intel_cascade_lake()) {\n-    return MIN2(Matcher::max_vector_size(bt), 32 \/ type2aelembytes(bt));\n-  }\n-  return Matcher::max_vector_size(bt);\n-}\n+instruct minI_rReg_ndd(rRegI dst, rRegI src1, rRegI src2)\n+%{\n+  predicate(UseAPX);\n+  match(Set dst (MinI src1 src2));\n+  effect(DEF dst, USE src1, USE src2);\n@@ -2235,3 +16567,7 @@\n-int Matcher::scalable_vector_reg_size(const BasicType bt) {\n-  return -1;\n-}\n+  ins_cost(200);\n+  expand %{\n+    rFlagsReg cr;\n+    compI_rReg(cr, src1, src2);\n+    cmovI_reg_g_ndd(dst, src1, src2, cr);\n+  %}\n+%}\n@@ -2239,13 +16575,4 @@\n-\/\/ Vector ideal reg corresponding to specified size in bytes\n-uint Matcher::vector_ideal_reg(int size) {\n-  assert(MaxVectorSize >= size, \"\");\n-  switch(size) {\n-    case  4: return Op_VecS;\n-    case  8: return Op_VecD;\n-    case 16: return Op_VecX;\n-    case 32: return Op_VecY;\n-    case 64: return Op_VecZ;\n-  }\n-  ShouldNotReachHere();\n-  return 0;\n-}\n+instruct cmovI_reg_l(rRegI dst, rRegI src, rFlagsReg cr)\n+%{\n+  predicate(!UseAPX);\n+  effect(USE_DEF dst, USE src, USE cr);\n@@ -2253,25 +16580,23 @@\n-\/\/ Check for shift by small constant as well\n-static bool clone_shift(Node* shift, Matcher* matcher, Matcher::MStack& mstack, VectorSet& address_visited) {\n-  if (shift->Opcode() == Op_LShiftX && shift->in(2)->is_Con() &&\n-      shift->in(2)->get_int() <= 3 &&\n-      \/\/ Are there other uses besides address expressions?\n-      !matcher->is_visited(shift)) {\n-    address_visited.set(shift->_idx); \/\/ Flag as address_visited\n-    mstack.push(shift->in(2), Matcher::Visit);\n-    Node *conv = shift->in(1);\n-    \/\/ Allow Matcher to match the rule which bypass\n-    \/\/ ConvI2L operation for an array index on LP64\n-    \/\/ if the index value is positive.\n-    if (conv->Opcode() == Op_ConvI2L &&\n-        conv->as_Type()->type()->is_long()->_lo >= 0 &&\n-        \/\/ Are there other uses besides address expressions?\n-        !matcher->is_visited(conv)) {\n-      address_visited.set(conv->_idx); \/\/ Flag as address_visited\n-      mstack.push(conv->in(1), Matcher::Pre_Visit);\n-    } else {\n-      mstack.push(conv, Matcher::Pre_Visit);\n-    }\n-    return true;\n-  }\n-  return false;\n-}\n+  format %{ \"cmovllt $dst, $src\\t# max\" %}\n+  ins_encode %{\n+    __ cmovl(Assembler::less, $dst$$Register, $src$$Register);\n+  %}\n+  ins_pipe(pipe_cmov_reg);\n+%}\n+\n+instruct cmovI_reg_l_ndd(rRegI dst, rRegI src1, rRegI src2, rFlagsReg cr)\n+%{\n+  predicate(UseAPX);\n+  effect(DEF dst, USE src1, USE src2, USE cr);\n+\n+  format %{ \"ecmovllt $dst, $src1, $src2\\t# max ndd\" %}\n+  ins_encode %{\n+    __ ecmovl(Assembler::less, $dst$$Register, $src1$$Register, $src2$$Register);\n+  %}\n+  ins_pipe(pipe_cmov_reg);\n+%}\n+\n+instruct maxI_rReg(rRegI dst, rRegI src)\n+%{\n+  predicate(!UseAPX);\n+  match(Set dst (MaxI dst src));\n@@ -2279,14 +16604,7 @@\n-\/\/ This function identifies sub-graphs in which a 'load' node is\n-\/\/ input to two different nodes, and such that it can be matched\n-\/\/ with BMI instructions like blsi, blsr, etc.\n-\/\/ Example : for b = -a[i] & a[i] can be matched to blsi r32, m32.\n-\/\/ The graph is (AndL (SubL Con0 LoadL*) LoadL*), where LoadL*\n-\/\/ refers to the same node.\n-\/\/\n-\/\/ Match the generic fused operations pattern (op1 (op2 Con{ConType} mop) mop)\n-\/\/ This is a temporary solution until we make DAGs expressible in ADL.\n-template<typename ConType>\n-class FusedPatternMatcher {\n-  Node* _op1_node;\n-  Node* _mop_node;\n-  int _con_op;\n+  ins_cost(200);\n+  expand %{\n+    rFlagsReg cr;\n+    compI_rReg(cr, dst, src);\n+    cmovI_reg_l(dst, src, cr);\n+  %}\n+%}\n@@ -2294,4 +16612,5 @@\n-  static int match_next(Node* n, int next_op, int next_op_idx) {\n-    if (n->in(1) == nullptr || n->in(2) == nullptr) {\n-      return -1;\n-    }\n+instruct maxI_rReg_ndd(rRegI dst, rRegI src1, rRegI src2)\n+%{\n+  predicate(UseAPX);\n+  match(Set dst (MaxI src1 src2));\n+  effect(DEF dst, USE src1, USE src2);\n@@ -2299,14 +16618,7 @@\n-    if (next_op_idx == -1) { \/\/ n is commutative, try rotations\n-      if (n->in(1)->Opcode() == next_op) {\n-        return 1;\n-      } else if (n->in(2)->Opcode() == next_op) {\n-        return 2;\n-      }\n-    } else {\n-      assert(next_op_idx > 0 && next_op_idx <= 2, \"Bad argument index\");\n-      if (n->in(next_op_idx)->Opcode() == next_op) {\n-        return next_op_idx;\n-      }\n-    }\n-    return -1;\n-  }\n+  ins_cost(200);\n+  expand %{\n+    rFlagsReg cr;\n+    compI_rReg(cr, src1, src2);\n+    cmovI_reg_l_ndd(dst, src1, src2, cr);\n+  %}\n+%}\n@@ -2314,3 +16626,2 @@\n- public:\n-  FusedPatternMatcher(Node* op1_node, Node* mop_node, int con_op) :\n-    _op1_node(op1_node), _mop_node(mop_node), _con_op(con_op) { }\n+\/\/ ============================================================================\n+\/\/ Branch Instructions\n@@ -2318,15 +16629,5 @@\n-  bool match(int op1, int op1_op2_idx,  \/\/ op1 and the index of the op1->op2 edge, -1 if op1 is commutative\n-             int op2, int op2_con_idx,  \/\/ op2 and the index of the op2->con edge, -1 if op2 is commutative\n-             typename ConType::NativeType con_value) {\n-    if (_op1_node->Opcode() != op1) {\n-      return false;\n-    }\n-    if (_mop_node->outcnt() > 2) {\n-      return false;\n-    }\n-    op1_op2_idx = match_next(_op1_node, op2, op1_op2_idx);\n-    if (op1_op2_idx == -1) {\n-      return false;\n-    }\n-    \/\/ Memory operation must be the other edge\n-    int op1_mop_idx = (op1_op2_idx & 1) + 1;\n+\/\/ Jump Direct - Label defines a relative address from JMP+1\n+instruct jmpDir(label labl)\n+%{\n+  match(Goto);\n+  effect(USE labl);\n@@ -2334,25 +16635,9 @@\n-    \/\/ Check that the mop node is really what we want\n-    if (_op1_node->in(op1_mop_idx) == _mop_node) {\n-      Node* op2_node = _op1_node->in(op1_op2_idx);\n-      if (op2_node->outcnt() > 1) {\n-        return false;\n-      }\n-      assert(op2_node->Opcode() == op2, \"Should be\");\n-      op2_con_idx = match_next(op2_node, _con_op, op2_con_idx);\n-      if (op2_con_idx == -1) {\n-        return false;\n-      }\n-      \/\/ Memory operation must be the other edge\n-      int op2_mop_idx = (op2_con_idx & 1) + 1;\n-      \/\/ Check that the memory operation is the same node\n-      if (op2_node->in(op2_mop_idx) == _mop_node) {\n-        \/\/ Now check the constant\n-        const Type* con_type = op2_node->in(op2_con_idx)->bottom_type();\n-        if (con_type != Type::TOP && ConType::as_self(con_type)->get_con() == con_value) {\n-          return true;\n-        }\n-      }\n-    }\n-    return false;\n-  }\n-};\n+  ins_cost(300);\n+  format %{ \"jmp     $labl\" %}\n+  size(5);\n+  ins_encode %{\n+    Label* L = $labl$$label;\n+    __ jmp(*L, false); \/\/ Always long jump\n+  %}\n+  ins_pipe(pipe_jmp);\n+%}\n@@ -2360,17 +16645,5 @@\n-static bool is_bmi_pattern(Node* n, Node* m) {\n-  assert(UseBMI1Instructions, \"sanity\");\n-  if (n != nullptr && m != nullptr) {\n-    if (m->Opcode() == Op_LoadI) {\n-      FusedPatternMatcher<TypeInt> bmii(n, m, Op_ConI);\n-      return bmii.match(Op_AndI, -1, Op_SubI,  1,  0)  ||\n-             bmii.match(Op_AndI, -1, Op_AddI, -1, -1)  ||\n-             bmii.match(Op_XorI, -1, Op_AddI, -1, -1);\n-    } else if (m->Opcode() == Op_LoadL) {\n-      FusedPatternMatcher<TypeLong> bmil(n, m, Op_ConL);\n-      return bmil.match(Op_AndL, -1, Op_SubL,  1,  0) ||\n-             bmil.match(Op_AndL, -1, Op_AddL, -1, -1) ||\n-             bmil.match(Op_XorL, -1, Op_AddL, -1, -1);\n-    }\n-  }\n-  return false;\n-}\n+\/\/ Jump Direct Conditional - Label defines a relative address from Jcc+1\n+instruct jmpCon(cmpOp cop, rFlagsReg cr, label labl)\n+%{\n+  match(If cop cr);\n+  effect(USE labl);\n@@ -2378,17 +16651,9 @@\n-\/\/ Should the matcher clone input 'm' of node 'n'?\n-bool Matcher::pd_clone_node(Node* n, Node* m, Matcher::MStack& mstack) {\n-  \/\/ If 'n' and 'm' are part of a graph for BMI instruction, clone the input 'm'.\n-  if (UseBMI1Instructions && is_bmi_pattern(n, m)) {\n-    mstack.push(m, Visit);\n-    return true;\n-  }\n-  if (is_vshift_con_pattern(n, m)) { \/\/ ShiftV src (ShiftCntV con)\n-    mstack.push(m, Visit);           \/\/ m = ShiftCntV\n-    return true;\n-  }\n-  if (is_encode_and_store_pattern(n, m)) {\n-    mstack.push(m, Visit);\n-    return true;\n-  }\n-  return false;\n-}\n+  ins_cost(300);\n+  format %{ \"j$cop     $labl\" %}\n+  size(6);\n+  ins_encode %{\n+    Label* L = $labl$$label;\n+    __ jcc((Assembler::Condition)($cop$$cmpcode), *L, false); \/\/ Always long jump\n+  %}\n+  ins_pipe(pipe_jcc);\n+%}\n@@ -2396,8 +16661,5 @@\n-\/\/ Should the Matcher clone shifts on addressing modes, expecting them\n-\/\/ to be subsumed into complex addressing expressions or compute them\n-\/\/ into registers?\n-bool Matcher::pd_clone_address_expressions(AddPNode* m, Matcher::MStack& mstack, VectorSet& address_visited) {\n-  Node *off = m->in(AddPNode::Offset);\n-  if (off->is_Con()) {\n-    address_visited.test_set(m->_idx); \/\/ Flag as address_visited\n-    Node *adr = m->in(AddPNode::Address);\n+\/\/ Jump Direct Conditional - Label defines a relative address from Jcc+1\n+instruct jmpLoopEnd(cmpOp cop, rFlagsReg cr, label labl)\n+%{\n+  match(CountedLoopEnd cop cr);\n+  effect(USE labl);\n@@ -2405,19 +16667,9 @@\n-    \/\/ Intel can handle 2 adds in addressing mode, with one of them using an immediate offset.\n-    \/\/ AtomicAdd is not an addressing expression.\n-    \/\/ Cheap to find it by looking for screwy base.\n-    if (adr->is_AddP() &&\n-        !adr->in(AddPNode::Base)->is_top() &&\n-        !adr->in(AddPNode::Offset)->is_Con() &&\n-        off->get_long() == (int) (off->get_long()) && \/\/ immL32\n-        \/\/ Are there other uses besides address expressions?\n-        !is_visited(adr)) {\n-      address_visited.set(adr->_idx); \/\/ Flag as address_visited\n-      Node *shift = adr->in(AddPNode::Offset);\n-      if (!clone_shift(shift, this, mstack, address_visited)) {\n-        mstack.push(shift, Pre_Visit);\n-      }\n-      mstack.push(adr->in(AddPNode::Address), Pre_Visit);\n-      mstack.push(adr->in(AddPNode::Base), Pre_Visit);\n-    } else {\n-      mstack.push(adr, Pre_Visit);\n-    }\n+  ins_cost(300);\n+  format %{ \"j$cop     $labl\\t# loop end\" %}\n+  size(6);\n+  ins_encode %{\n+    Label* L = $labl$$label;\n+    __ jcc((Assembler::Condition)($cop$$cmpcode), *L, false); \/\/ Always long jump\n+  %}\n+  ins_pipe(pipe_jcc);\n+%}\n@@ -2425,12 +16677,4 @@\n-    \/\/ Clone X+offset as it also folds into most addressing expressions\n-    mstack.push(off, Visit);\n-    mstack.push(m->in(AddPNode::Base), Pre_Visit);\n-    return true;\n-  } else if (clone_shift(off, this, mstack, address_visited)) {\n-    address_visited.test_set(m->_idx); \/\/ Flag as address_visited\n-    mstack.push(m->in(AddPNode::Address), Pre_Visit);\n-    mstack.push(m->in(AddPNode::Base), Pre_Visit);\n-    return true;\n-  }\n-  return false;\n-}\n+\/\/ Jump Direct Conditional - using unsigned comparison\n+instruct jmpConU(cmpOpU cop, rFlagsRegU cmp, label labl) %{\n+  match(If cop cmp);\n+  effect(USE labl);\n@@ -2438,21 +16682,9 @@\n-static inline Assembler::ComparisonPredicate booltest_pred_to_comparison_pred(int bt) {\n-  switch (bt) {\n-    case BoolTest::eq:\n-      return Assembler::eq;\n-    case BoolTest::ne:\n-      return Assembler::neq;\n-    case BoolTest::le:\n-    case BoolTest::ule:\n-      return Assembler::le;\n-    case BoolTest::ge:\n-    case BoolTest::uge:\n-      return Assembler::nlt;\n-    case BoolTest::lt:\n-    case BoolTest::ult:\n-      return Assembler::lt;\n-    case BoolTest::gt:\n-    case BoolTest::ugt:\n-      return Assembler::nle;\n-    default : ShouldNotReachHere(); return Assembler::_false;\n-  }\n-}\n+  ins_cost(300);\n+  format %{ \"j$cop,u   $labl\" %}\n+  size(6);\n+  ins_encode %{\n+    Label* L = $labl$$label;\n+    __ jcc((Assembler::Condition)($cop$$cmpcode), *L, false); \/\/ Always long jump\n+  %}\n+  ins_pipe(pipe_jcc);\n+%}\n@@ -2460,12 +16692,3 @@\n-static inline Assembler::ComparisonPredicateFP booltest_pred_to_comparison_pred_fp(int bt) {\n-  switch (bt) {\n-  case BoolTest::eq: return Assembler::EQ_OQ;  \/\/ ordered non-signaling\n-  \/\/ As per JLS 15.21.1, != of NaNs is true. Thus use unordered compare.\n-  case BoolTest::ne: return Assembler::NEQ_UQ; \/\/ unordered non-signaling\n-  case BoolTest::le: return Assembler::LE_OQ;  \/\/ ordered non-signaling\n-  case BoolTest::ge: return Assembler::GE_OQ;  \/\/ ordered non-signaling\n-  case BoolTest::lt: return Assembler::LT_OQ;  \/\/ ordered non-signaling\n-  case BoolTest::gt: return Assembler::GT_OQ;  \/\/ ordered non-signaling\n-  default: ShouldNotReachHere(); return Assembler::FALSE_OS;\n-  }\n-}\n+instruct jmpConUCF(cmpOpUCF cop, rFlagsRegUCF cmp, label labl) %{\n+  match(If cop cmp);\n+  effect(USE labl);\n@@ -2473,49 +16696,9 @@\n-\/\/ Helper methods for MachSpillCopyNode::implementation().\n-static void vec_mov_helper(C2_MacroAssembler *masm, int src_lo, int dst_lo,\n-                          int src_hi, int dst_hi, uint ireg, outputStream* st) {\n-  assert(ireg == Op_VecS || \/\/ 32bit vector\n-         ((src_lo & 1) == 0 && (src_lo + 1) == src_hi &&\n-          (dst_lo & 1) == 0 && (dst_lo + 1) == dst_hi),\n-         \"no non-adjacent vector moves\" );\n-  if (masm) {\n-    switch (ireg) {\n-    case Op_VecS: \/\/ copy whole register\n-    case Op_VecD:\n-    case Op_VecX:\n-      if ((UseAVX < 3) || VM_Version::supports_avx512vl()) {\n-        __ movdqu(as_XMMRegister(Matcher::_regEncode[dst_lo]), as_XMMRegister(Matcher::_regEncode[src_lo]));\n-      } else {\n-        __ vextractf32x4(as_XMMRegister(Matcher::_regEncode[dst_lo]), as_XMMRegister(Matcher::_regEncode[src_lo]), 0x0);\n-     }\n-      break;\n-    case Op_VecY:\n-      if ((UseAVX < 3) || VM_Version::supports_avx512vl()) {\n-        __ vmovdqu(as_XMMRegister(Matcher::_regEncode[dst_lo]), as_XMMRegister(Matcher::_regEncode[src_lo]));\n-      } else {\n-        __ vextractf64x4(as_XMMRegister(Matcher::_regEncode[dst_lo]), as_XMMRegister(Matcher::_regEncode[src_lo]), 0x0);\n-     }\n-      break;\n-    case Op_VecZ:\n-      __ evmovdquq(as_XMMRegister(Matcher::_regEncode[dst_lo]), as_XMMRegister(Matcher::_regEncode[src_lo]), 2);\n-      break;\n-    default:\n-      ShouldNotReachHere();\n-    }\n-#ifndef PRODUCT\n-  } else {\n-    switch (ireg) {\n-    case Op_VecS:\n-    case Op_VecD:\n-    case Op_VecX:\n-      st->print(\"movdqu  %s,%s\\t# spill\",Matcher::regName[dst_lo],Matcher::regName[src_lo]);\n-      break;\n-    case Op_VecY:\n-    case Op_VecZ:\n-      st->print(\"vmovdqu %s,%s\\t# spill\",Matcher::regName[dst_lo],Matcher::regName[src_lo]);\n-      break;\n-    default:\n-      ShouldNotReachHere();\n-    }\n-#endif\n-  }\n-}\n+  ins_cost(200);\n+  format %{ \"j$cop,u   $labl\" %}\n+  size(6);\n+  ins_encode %{\n+    Label* L = $labl$$label;\n+    __ jcc((Assembler::Condition)($cop$$cmpcode), *L, false); \/\/ Always long jump\n+  %}\n+  ins_pipe(pipe_jcc);\n+%}\n@@ -2523,63 +16706,13 @@\n-void vec_spill_helper(C2_MacroAssembler *masm, bool is_load,\n-                     int stack_offset, int reg, uint ireg, outputStream* st) {\n-  if (masm) {\n-    if (is_load) {\n-      switch (ireg) {\n-      case Op_VecS:\n-        __ movdl(as_XMMRegister(Matcher::_regEncode[reg]), Address(rsp, stack_offset));\n-        break;\n-      case Op_VecD:\n-        __ movq(as_XMMRegister(Matcher::_regEncode[reg]), Address(rsp, stack_offset));\n-        break;\n-      case Op_VecX:\n-        if ((UseAVX < 3) || VM_Version::supports_avx512vl()) {\n-          __ movdqu(as_XMMRegister(Matcher::_regEncode[reg]), Address(rsp, stack_offset));\n-        } else {\n-          __ vpxor(as_XMMRegister(Matcher::_regEncode[reg]), as_XMMRegister(Matcher::_regEncode[reg]), as_XMMRegister(Matcher::_regEncode[reg]), 2);\n-          __ vinsertf32x4(as_XMMRegister(Matcher::_regEncode[reg]), as_XMMRegister(Matcher::_regEncode[reg]), Address(rsp, stack_offset),0x0);\n-        }\n-        break;\n-      case Op_VecY:\n-        if ((UseAVX < 3) || VM_Version::supports_avx512vl()) {\n-          __ vmovdqu(as_XMMRegister(Matcher::_regEncode[reg]), Address(rsp, stack_offset));\n-        } else {\n-          __ vpxor(as_XMMRegister(Matcher::_regEncode[reg]), as_XMMRegister(Matcher::_regEncode[reg]), as_XMMRegister(Matcher::_regEncode[reg]), 2);\n-          __ vinsertf64x4(as_XMMRegister(Matcher::_regEncode[reg]), as_XMMRegister(Matcher::_regEncode[reg]), Address(rsp, stack_offset),0x0);\n-        }\n-        break;\n-      case Op_VecZ:\n-        __ evmovdquq(as_XMMRegister(Matcher::_regEncode[reg]), Address(rsp, stack_offset), 2);\n-        break;\n-      default:\n-        ShouldNotReachHere();\n-      }\n-    } else { \/\/ store\n-      switch (ireg) {\n-      case Op_VecS:\n-        __ movdl(Address(rsp, stack_offset), as_XMMRegister(Matcher::_regEncode[reg]));\n-        break;\n-      case Op_VecD:\n-        __ movq(Address(rsp, stack_offset), as_XMMRegister(Matcher::_regEncode[reg]));\n-        break;\n-      case Op_VecX:\n-        if ((UseAVX < 3) || VM_Version::supports_avx512vl()) {\n-          __ movdqu(Address(rsp, stack_offset), as_XMMRegister(Matcher::_regEncode[reg]));\n-        }\n-        else {\n-          __ vextractf32x4(Address(rsp, stack_offset), as_XMMRegister(Matcher::_regEncode[reg]), 0x0);\n-        }\n-        break;\n-      case Op_VecY:\n-        if ((UseAVX < 3) || VM_Version::supports_avx512vl()) {\n-          __ vmovdqu(Address(rsp, stack_offset), as_XMMRegister(Matcher::_regEncode[reg]));\n-        }\n-        else {\n-          __ vextractf64x4(Address(rsp, stack_offset), as_XMMRegister(Matcher::_regEncode[reg]), 0x0);\n-        }\n-        break;\n-      case Op_VecZ:\n-        __ evmovdquq(Address(rsp, stack_offset), as_XMMRegister(Matcher::_regEncode[reg]), 2);\n-        break;\n-      default:\n-        ShouldNotReachHere();\n-      }\n+instruct jmpConUCF2(cmpOpUCF2 cop, rFlagsRegUCF cmp, label labl) %{\n+  match(If cop cmp);\n+  effect(USE labl);\n+\n+  ins_cost(200);\n+  format %{ $$template\n+    if ($cop$$cmpcode == Assembler::notEqual) {\n+      $$emit$$\"jp,u    $labl\\n\\t\"\n+      $$emit$$\"j$cop,u   $labl\"\n+    } else {\n+      $$emit$$\"jp,u    done\\n\\t\"\n+      $$emit$$\"j$cop,u   $labl\\n\\t\"\n+      $$emit$$\"done:\"\n@@ -2587,38 +16720,13 @@\n-#ifndef PRODUCT\n-  } else {\n-    if (is_load) {\n-      switch (ireg) {\n-      case Op_VecS:\n-        st->print(\"movd    %s,[rsp + %d]\\t# spill\", Matcher::regName[reg], stack_offset);\n-        break;\n-      case Op_VecD:\n-        st->print(\"movq    %s,[rsp + %d]\\t# spill\", Matcher::regName[reg], stack_offset);\n-        break;\n-       case Op_VecX:\n-        st->print(\"movdqu  %s,[rsp + %d]\\t# spill\", Matcher::regName[reg], stack_offset);\n-        break;\n-      case Op_VecY:\n-      case Op_VecZ:\n-        st->print(\"vmovdqu %s,[rsp + %d]\\t# spill\", Matcher::regName[reg], stack_offset);\n-        break;\n-      default:\n-        ShouldNotReachHere();\n-      }\n-    } else { \/\/ store\n-      switch (ireg) {\n-      case Op_VecS:\n-        st->print(\"movd    [rsp + %d],%s\\t# spill\", stack_offset, Matcher::regName[reg]);\n-        break;\n-      case Op_VecD:\n-        st->print(\"movq    [rsp + %d],%s\\t# spill\", stack_offset, Matcher::regName[reg]);\n-        break;\n-       case Op_VecX:\n-        st->print(\"movdqu  [rsp + %d],%s\\t# spill\", stack_offset, Matcher::regName[reg]);\n-        break;\n-      case Op_VecY:\n-      case Op_VecZ:\n-        st->print(\"vmovdqu [rsp + %d],%s\\t# spill\", stack_offset, Matcher::regName[reg]);\n-        break;\n-      default:\n-        ShouldNotReachHere();\n-      }\n+  %}\n+  ins_encode %{\n+    Label* l = $labl$$label;\n+    if ($cop$$cmpcode == Assembler::notEqual) {\n+      __ jcc(Assembler::parity, *l, false);\n+      __ jcc(Assembler::notEqual, *l, false);\n+    } else if ($cop$$cmpcode == Assembler::equal) {\n+      Label done;\n+      __ jccb(Assembler::parity, done);\n+      __ jcc(Assembler::equal, *l, false);\n+      __ bind(done);\n+    } else {\n+       ShouldNotReachHere();\n@@ -2626,3 +16734,3 @@\n-#endif\n-  }\n-}\n+  %}\n+  ins_pipe(pipe_jcc);\n+%}\n@@ -2630,34 +16738,87 @@\n-template <class T>\n-static inline GrowableArray<jbyte>* vreplicate_imm(BasicType bt, T con, int len) {\n-  int size = type2aelembytes(bt) * len;\n-  GrowableArray<jbyte>* val = new GrowableArray<jbyte>(size, size, 0);\n-  for (int i = 0; i < len; i++) {\n-    int offset = i * type2aelembytes(bt);\n-    switch (bt) {\n-      case T_BYTE: val->at(i) = con; break;\n-      case T_SHORT: {\n-        jshort c = con;\n-        memcpy(val->adr_at(offset), &c, sizeof(jshort));\n-        break;\n-      }\n-      case T_INT: {\n-        jint c = con;\n-        memcpy(val->adr_at(offset), &c, sizeof(jint));\n-        break;\n-      }\n-      case T_LONG: {\n-        jlong c = con;\n-        memcpy(val->adr_at(offset), &c, sizeof(jlong));\n-        break;\n-      }\n-      case T_FLOAT: {\n-        jfloat c = con;\n-        memcpy(val->adr_at(offset), &c, sizeof(jfloat));\n-        break;\n-      }\n-      case T_DOUBLE: {\n-        jdouble c = con;\n-        memcpy(val->adr_at(offset), &c, sizeof(jdouble));\n-        break;\n-      }\n-      default: assert(false, \"%s\", type2name(bt));\n+\/\/ ============================================================================\n+\/\/ The 2nd slow-half of a subtype check.  Scan the subklass's 2ndary\n+\/\/ superklass array for an instance of the superklass.  Set a hidden\n+\/\/ internal cache on a hit (cache is checked with exposed code in\n+\/\/ gen_subtype_check()).  Return NZ for a miss or zero for a hit.  The\n+\/\/ encoding ALSO sets flags.\n+\n+instruct partialSubtypeCheck(rdi_RegP result,\n+                             rsi_RegP sub, rax_RegP super, rcx_RegI rcx,\n+                             rFlagsReg cr)\n+%{\n+  match(Set result (PartialSubtypeCheck sub super));\n+  predicate(!UseSecondarySupersTable);\n+  effect(KILL rcx, KILL cr);\n+\n+  ins_cost(1100);  \/\/ slightly larger than the next version\n+  format %{ \"movq    rdi, [$sub + in_bytes(Klass::secondary_supers_offset())]\\n\\t\"\n+            \"movl    rcx, [rdi + Array<Klass*>::length_offset_in_bytes()]\\t# length to scan\\n\\t\"\n+            \"addq    rdi, Array<Klass*>::base_offset_in_bytes()\\t# Skip to start of data; set NZ in case count is zero\\n\\t\"\n+            \"repne   scasq\\t# Scan *rdi++ for a match with rax while rcx--\\n\\t\"\n+            \"jne,s   miss\\t\\t# Missed: rdi not-zero\\n\\t\"\n+            \"movq    [$sub + in_bytes(Klass::secondary_super_cache_offset())], $super\\t# Hit: update cache\\n\\t\"\n+            \"xorq    $result, $result\\t\\t Hit: rdi zero\\n\\t\"\n+    \"miss:\\t\" %}\n+\n+  ins_encode %{\n+    Label miss;\n+    \/\/ NB: Callers may assume that, when $result is a valid register,\n+    \/\/ check_klass_subtype_slow_path_linear sets it to a nonzero\n+    \/\/ value.\n+    __ check_klass_subtype_slow_path_linear($sub$$Register, $super$$Register,\n+                                            $rcx$$Register, $result$$Register,\n+                                            nullptr, &miss,\n+                                            \/*set_cond_codes:*\/ true);\n+    __ xorptr($result$$Register, $result$$Register);\n+    __ bind(miss);\n+  %}\n+\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ ============================================================================\n+\/\/ Two versions of hashtable-based partialSubtypeCheck, both used when\n+\/\/ we need to search for a super class in the secondary supers array.\n+\/\/ The first is used when we don't know _a priori_ the class being\n+\/\/ searched for. The second, far more common, is used when we do know:\n+\/\/ this is used for instanceof, checkcast, and any case where C2 can\n+\/\/ determine it by constant propagation.\n+\n+instruct partialSubtypeCheckVarSuper(rsi_RegP sub, rax_RegP super, rdi_RegP result,\n+                                       rdx_RegL temp1, rcx_RegL temp2, rbx_RegP temp3, r11_RegL temp4,\n+                                       rFlagsReg cr)\n+%{\n+  match(Set result (PartialSubtypeCheck sub super));\n+  predicate(UseSecondarySupersTable);\n+  effect(KILL cr, TEMP temp1, TEMP temp2, TEMP temp3, TEMP temp4);\n+\n+  ins_cost(1000);\n+  format %{ \"partialSubtypeCheck $result, $sub, $super\" %}\n+\n+  ins_encode %{\n+    __ lookup_secondary_supers_table_var($sub$$Register, $super$$Register, $temp1$$Register, $temp2$$Register,\n+\t\t\t\t\t $temp3$$Register, $temp4$$Register, $result$$Register);\n+  %}\n+\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct partialSubtypeCheckConstSuper(rsi_RegP sub, rax_RegP super_reg, immP super_con, rdi_RegP result,\n+                                       rdx_RegL temp1, rcx_RegL temp2, rbx_RegP temp3, r11_RegL temp4,\n+                                       rFlagsReg cr)\n+%{\n+  match(Set result (PartialSubtypeCheck sub (Binary super_reg super_con)));\n+  predicate(UseSecondarySupersTable);\n+  effect(KILL cr, TEMP temp1, TEMP temp2, TEMP temp3, TEMP temp4);\n+\n+  ins_cost(700);  \/\/ smaller than the next version\n+  format %{ \"partialSubtypeCheck $result, $sub, $super_reg, $super_con\" %}\n+\n+  ins_encode %{\n+    u1 super_klass_slot = ((Klass*)$super_con$$constant)->hash_slot();\n+    if (InlineSecondarySupersTest) {\n+      __ lookup_secondary_supers_table_const($sub$$Register, $super_reg$$Register, $temp1$$Register, $temp2$$Register,\n+                                       $temp3$$Register, $temp4$$Register, $result$$Register,\n+                                       super_klass_slot);\n+    } else {\n+      __ call(RuntimeAddress(StubRoutines::lookup_secondary_supers_table_stub(super_klass_slot)));\n@@ -2665,3 +16826,1 @@\n-  }\n-  return val;\n-}\n+  %}\n@@ -2669,11 +16828,2 @@\n-static inline jlong high_bit_set(BasicType bt) {\n-  switch (bt) {\n-    case T_BYTE:  return 0x8080808080808080;\n-    case T_SHORT: return 0x8000800080008000;\n-    case T_INT:   return 0x8000000080000000;\n-    case T_LONG:  return 0x8000000000000000;\n-    default:\n-      ShouldNotReachHere();\n-      return 0;\n-  }\n-}\n+  ins_pipe(pipe_slow);\n+%}\n@@ -2681,5 +16831,11 @@\n-#ifndef PRODUCT\n-  void MachNopNode::format(PhaseRegAlloc*, outputStream* st) const {\n-    st->print(\"nop \\t# %d bytes pad for loops and calls\", _count);\n-  }\n-#endif\n+\/\/ ============================================================================\n+\/\/ Branch Instructions -- short offset versions\n+\/\/\n+\/\/ These instructions are used to replace jumps of a long offset (the default\n+\/\/ match) with jumps of a shorter offset.  These instructions are all tagged\n+\/\/ with the ins_short_branch attribute, which causes the ADLC to suppress the\n+\/\/ match rules in general matching.  Instead, the ADLC generates a conversion\n+\/\/ method in the MachNode which can be used to do in-place replacement of the\n+\/\/ long variant with the shorter variant.  The compiler will determine if a\n+\/\/ branch can be taken by the is_short_branch_offset() predicate in the machine\n+\/\/ specific code section of the file.\n@@ -2687,3 +16843,4 @@\n-  void MachNopNode::emit(C2_MacroAssembler *masm, PhaseRegAlloc*) const {\n-    __ nop(_count);\n-  }\n+\/\/ Jump Direct - Label defines a relative address from JMP+1\n+instruct jmpDir_short(label labl) %{\n+  match(Goto);\n+  effect(USE labl);\n@@ -2691,3 +16848,10 @@\n-  uint MachNopNode::size(PhaseRegAlloc*) const {\n-    return _count;\n-  }\n+  ins_cost(300);\n+  format %{ \"jmp,s   $labl\" %}\n+  size(2);\n+  ins_encode %{\n+    Label* L = $labl$$label;\n+    __ jmpb(*L);\n+  %}\n+  ins_pipe(pipe_jmp);\n+  ins_short_branch(1);\n+%}\n@@ -2695,5 +16859,4 @@\n-#ifndef PRODUCT\n-  void MachBreakpointNode::format(PhaseRegAlloc*, outputStream* st) const {\n-    st->print(\"# breakpoint\");\n-  }\n-#endif\n+\/\/ Jump Direct Conditional - Label defines a relative address from Jcc+1\n+instruct jmpCon_short(cmpOp cop, rFlagsReg cr, label labl) %{\n+  match(If cop cr);\n+  effect(USE labl);\n@@ -2701,3 +16864,10 @@\n-  void MachBreakpointNode::emit(C2_MacroAssembler *masm, PhaseRegAlloc* ra_) const {\n-    __ int3();\n-  }\n+  ins_cost(300);\n+  format %{ \"j$cop,s   $labl\" %}\n+  size(2);\n+  ins_encode %{\n+    Label* L = $labl$$label;\n+    __ jccb((Assembler::Condition)($cop$$cmpcode), *L);\n+  %}\n+  ins_pipe(pipe_jcc);\n+  ins_short_branch(1);\n+%}\n@@ -2705,3 +16875,4 @@\n-  uint MachBreakpointNode::size(PhaseRegAlloc* ra_) const {\n-    return MachNode::size(ra_);\n-  }\n+\/\/ Jump Direct Conditional - Label defines a relative address from Jcc+1\n+instruct jmpLoopEnd_short(cmpOp cop, rFlagsReg cr, label labl) %{\n+  match(CountedLoopEnd cop cr);\n+  effect(USE labl);\n@@ -2709,0 +16880,9 @@\n+  ins_cost(300);\n+  format %{ \"j$cop,s   $labl\\t# loop end\" %}\n+  size(2);\n+  ins_encode %{\n+    Label* L = $labl$$label;\n+    __ jccb((Assembler::Condition)($cop$$cmpcode), *L);\n+  %}\n+  ins_pipe(pipe_jcc);\n+  ins_short_branch(1);\n@@ -2711,1 +16891,4 @@\n-encode %{\n+\/\/ Jump Direct Conditional - using unsigned comparison\n+instruct jmpConU_short(cmpOpU cop, rFlagsRegU cmp, label labl) %{\n+  match(If cop cmp);\n+  effect(USE labl);\n@@ -2713,11 +16896,6 @@\n-  enc_class call_epilog %{\n-    if (VerifyStackAtCalls) {\n-      \/\/ Check that stack depth is unchanged: find majik cookie on stack\n-      int framesize = ra_->reg2offset_unchecked(OptoReg::add(ra_->_matcher._old_SP, -3*VMRegImpl::slots_per_word));\n-      Label L;\n-      __ cmpptr(Address(rsp, framesize), (int32_t)0xbadb100d);\n-      __ jccb(Assembler::equal, L);\n-      \/\/ Die if stack mismatch\n-      __ int3();\n-      __ bind(L);\n-    }\n+  ins_cost(300);\n+  format %{ \"j$cop,us  $labl\" %}\n+  size(2);\n+  ins_encode %{\n+    Label* L = $labl$$label;\n+    __ jccb((Assembler::Condition)($cop$$cmpcode), *L);\n@@ -2725,0 +16903,7 @@\n+  ins_pipe(pipe_jcc);\n+  ins_short_branch(1);\n+%}\n+\n+instruct jmpConUCF_short(cmpOpUCF cop, rFlagsRegUCF cmp, label labl) %{\n+  match(If cop cmp);\n+  effect(USE labl);\n@@ -2726,0 +16911,9 @@\n+  ins_cost(300);\n+  format %{ \"j$cop,us  $labl\" %}\n+  size(2);\n+  ins_encode %{\n+    Label* L = $labl$$label;\n+    __ jccb((Assembler::Condition)($cop$$cmpcode), *L);\n+  %}\n+  ins_pipe(pipe_jcc);\n+  ins_short_branch(1);\n@@ -2728,6 +16922,46 @@\n-\/\/ Operands for bound floating pointer register arguments\n-operand rxmm0() %{\n-  constraint(ALLOC_IN_RC(xmm0_reg));\n-  match(VecX);\n-  format%{%}\n-  interface(REG_INTER);\n+instruct jmpConUCF2_short(cmpOpUCF2 cop, rFlagsRegUCF cmp, label labl) %{\n+  match(If cop cmp);\n+  effect(USE labl);\n+\n+  ins_cost(300);\n+  format %{ $$template\n+    if ($cop$$cmpcode == Assembler::notEqual) {\n+      $$emit$$\"jp,u,s  $labl\\n\\t\"\n+      $$emit$$\"j$cop,u,s  $labl\"\n+    } else {\n+      $$emit$$\"jp,u,s  done\\n\\t\"\n+      $$emit$$\"j$cop,u,s  $labl\\n\\t\"\n+      $$emit$$\"done:\"\n+    }\n+  %}\n+  size(4);\n+  ins_encode %{\n+    Label* l = $labl$$label;\n+    if ($cop$$cmpcode == Assembler::notEqual) {\n+      __ jccb(Assembler::parity, *l);\n+      __ jccb(Assembler::notEqual, *l);\n+    } else if ($cop$$cmpcode == Assembler::equal) {\n+      Label done;\n+      __ jccb(Assembler::parity, done);\n+      __ jccb(Assembler::equal, *l);\n+      __ bind(done);\n+    } else {\n+       ShouldNotReachHere();\n+    }\n+  %}\n+  ins_pipe(pipe_jcc);\n+  ins_short_branch(1);\n+%}\n+\n+\/\/ ============================================================================\n+\/\/ inlined locking and unlocking\n+\n+instruct cmpFastLockLightweight(rFlagsReg cr, rRegP object, rbx_RegP box, rax_RegI rax_reg, rRegP tmp) %{\n+  match(Set cr (FastLock object box));\n+  effect(TEMP rax_reg, TEMP tmp, USE_KILL box);\n+  ins_cost(300);\n+  format %{ \"fastlock $object,$box\\t! kills $box,$rax_reg,$tmp\" %}\n+  ins_encode %{\n+    __ fast_lock_lightweight($object$$Register, $box$$Register, $rax_reg$$Register, $tmp$$Register, r15_thread);\n+  %}\n+  ins_pipe(pipe_slow);\n@@ -2736,4 +16970,10 @@\n-\/\/----------OPERANDS-----------------------------------------------------------\n-\/\/ Operand definitions must precede instruction definitions for correct parsing\n-\/\/ in the ADLC because operands constitute user defined types which are used in\n-\/\/ instruction definitions.\n+instruct cmpFastUnlockLightweight(rFlagsReg cr, rRegP object, rax_RegP rax_reg, rRegP tmp) %{\n+  match(Set cr (FastUnlock object rax_reg));\n+  effect(TEMP tmp, USE_KILL rax_reg);\n+  ins_cost(300);\n+  format %{ \"fastunlock $object,$rax_reg\\t! kills $rax_reg,$tmp\" %}\n+  ins_encode %{\n+    __ fast_unlock_lightweight($object$$Register, $rax_reg$$Register, $tmp$$Register, r15_thread);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n@@ -2741,1 +16981,0 @@\n-\/\/ Vectors\n@@ -2743,9 +16982,6 @@\n-\/\/ Dummy generic vector class. Should be used for all vector operands.\n-\/\/ Replaced with vec[SDXYZ] during post-selection pass.\n-operand vec() %{\n-  constraint(ALLOC_IN_RC(dynamic));\n-  match(VecX);\n-  match(VecY);\n-  match(VecZ);\n-  match(VecS);\n-  match(VecD);\n+\/\/ ============================================================================\n+\/\/ Safepoint Instructions\n+instruct safePoint_poll_tls(rFlagsReg cr, rRegP poll)\n+%{\n+  match(SafePoint poll);\n+  effect(KILL cr, USE poll);\n@@ -2753,2 +16989,10 @@\n-  format %{ %}\n-  interface(REG_INTER);\n+  format %{ \"testl   rax, [$poll]\\t\"\n+            \"# Safepoint: poll for GC\" %}\n+  ins_cost(125);\n+  ins_encode %{\n+    __ relocate(relocInfo::poll_type);\n+    address pre_pc = __ pc();\n+    __ testl(rax, Address($poll$$Register, 0));\n+    assert(nativeInstruction_at(pre_pc)->is_safepoint_poll(), \"must emit test %%eax [reg]\");\n+  %}\n+  ins_pipe(ialu_reg_mem);\n@@ -2757,14 +17001,8 @@\n-\/\/ Dummy generic legacy vector class. Should be used for all legacy vector operands.\n-\/\/ Replaced with legVec[SDXYZ] during post-selection cleanup.\n-\/\/ Note: legacy register class is used to avoid extra (unneeded in 32-bit VM)\n-\/\/ runtime code generation via reg_class_dynamic.\n-operand legVec() %{\n-  constraint(ALLOC_IN_RC(dynamic));\n-  match(VecX);\n-  match(VecY);\n-  match(VecZ);\n-  match(VecS);\n-  match(VecD);\n-\n-  format %{ %}\n-  interface(REG_INTER);\n+instruct mask_all_evexL(kReg dst, rRegL src) %{\n+  match(Set dst (MaskAll src));\n+  format %{ \"mask_all_evexL $dst, $src \\t! mask all operation\" %}\n+  ins_encode %{\n+    int mask_len = Matcher::vector_length(this);\n+    __ vector_maskall_operation($dst$$KRegister, $src$$Register, mask_len);\n+  %}\n+  ins_pipe( pipe_slow );\n@@ -2773,4 +17011,12 @@\n-\/\/ Replaces vec during post-selection cleanup. See above.\n-operand vecS() %{\n-  constraint(ALLOC_IN_RC(vectors_reg_vlbwdq));\n-  match(VecS);\n+instruct mask_all_evexI_GT32(kReg dst, rRegI src, rRegL tmp) %{\n+  predicate(Matcher::vector_length(n) > 32);\n+  match(Set dst (MaskAll src));\n+  effect(TEMP tmp);\n+  format %{ \"mask_all_evexI_GT32 $dst, $src \\t! using $tmp as TEMP\" %}\n+  ins_encode %{\n+    int mask_len = Matcher::vector_length(this);\n+    __ movslq($tmp$$Register, $src$$Register);\n+    __ vector_maskall_operation($dst$$KRegister, $tmp$$Register, mask_len);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n@@ -2778,2 +17024,15 @@\n-  format %{ %}\n-  interface(REG_INTER);\n+\/\/ ============================================================================\n+\/\/ Procedure Call\/Return Instructions\n+\/\/ Call Java Static Instruction\n+\/\/ Note: If this code changes, the corresponding ret_addr_offset() and\n+\/\/       compute_padding() functions will have to be adjusted.\n+instruct CallStaticJavaDirect(method meth) %{\n+  match(CallStaticJava);\n+  effect(USE meth);\n+\n+  ins_cost(300);\n+  format %{ \"call,static \" %}\n+  opcode(0xE8); \/* E8 cd *\/\n+  ins_encode(clear_avx, Java_Static_Call(meth), call_epilog);\n+  ins_pipe(pipe_slow);\n+  ins_alignment(4);\n@@ -2782,4 +17041,7 @@\n-\/\/ Replaces legVec during post-selection cleanup. See above.\n-operand legVecS() %{\n-  constraint(ALLOC_IN_RC(vectors_reg_legacy));\n-  match(VecS);\n+\/\/ Call Java Dynamic Instruction\n+\/\/ Note: If this code changes, the corresponding ret_addr_offset() and\n+\/\/       compute_padding() functions will have to be adjusted.\n+instruct CallDynamicJavaDirect(method meth)\n+%{\n+  match(CallDynamicJava);\n+  effect(USE meth);\n@@ -2787,2 +17049,6 @@\n-  format %{ %}\n-  interface(REG_INTER);\n+  ins_cost(300);\n+  format %{ \"movq    rax, #Universe::non_oop_word()\\n\\t\"\n+            \"call,dynamic \" %}\n+  ins_encode(clear_avx, Java_Dynamic_Call(meth), call_epilog);\n+  ins_pipe(pipe_slow);\n+  ins_alignment(4);\n@@ -2791,4 +17057,5 @@\n-\/\/ Replaces vec during post-selection cleanup. See above.\n-operand vecD() %{\n-  constraint(ALLOC_IN_RC(vectord_reg_vlbwdq));\n-  match(VecD);\n+\/\/ Call Runtime Instruction\n+instruct CallRuntimeDirect(method meth)\n+%{\n+  match(CallRuntime);\n+  effect(USE meth);\n@@ -2796,2 +17063,4 @@\n-  format %{ %}\n-  interface(REG_INTER);\n+  ins_cost(300);\n+  format %{ \"call,runtime \" %}\n+  ins_encode(clear_avx, Java_To_Runtime(meth));\n+  ins_pipe(pipe_slow);\n@@ -2800,4 +17069,5 @@\n-\/\/ Replaces legVec during post-selection cleanup. See above.\n-operand legVecD() %{\n-  constraint(ALLOC_IN_RC(vectord_reg_legacy));\n-  match(VecD);\n+\/\/ Call runtime without safepoint\n+instruct CallLeafDirect(method meth)\n+%{\n+  match(CallLeaf);\n+  effect(USE meth);\n@@ -2805,2 +17075,4 @@\n-  format %{ %}\n-  interface(REG_INTER);\n+  ins_cost(300);\n+  format %{ \"call_leaf,runtime \" %}\n+  ins_encode(clear_avx, Java_To_Runtime(meth));\n+  ins_pipe(pipe_slow);\n@@ -2809,4 +17081,5 @@\n-\/\/ Replaces vec during post-selection cleanup. See above.\n-operand vecX() %{\n-  constraint(ALLOC_IN_RC(vectorx_reg_vlbwdq));\n-  match(VecX);\n+\/\/ Call runtime without safepoint and with vector arguments\n+instruct CallLeafDirectVector(method meth)\n+%{\n+  match(CallLeafVector);\n+  effect(USE meth);\n@@ -2814,2 +17087,4 @@\n-  format %{ %}\n-  interface(REG_INTER);\n+  ins_cost(300);\n+  format %{ \"call_leaf,vector \" %}\n+  ins_encode(Java_To_Runtime(meth));\n+  ins_pipe(pipe_slow);\n@@ -2818,4 +17093,5 @@\n-\/\/ Replaces legVec during post-selection cleanup. See above.\n-operand legVecX() %{\n-  constraint(ALLOC_IN_RC(vectorx_reg_legacy));\n-  match(VecX);\n+\/\/ Call runtime without safepoint\n+instruct CallLeafNoFPDirect(method meth)\n+%{\n+  match(CallLeafNoFP);\n+  effect(USE meth);\n@@ -2823,2 +17099,4 @@\n-  format %{ %}\n-  interface(REG_INTER);\n+  ins_cost(300);\n+  format %{ \"call_leaf_nofp,runtime \" %}\n+  ins_encode(clear_avx, Java_To_Runtime(meth));\n+  ins_pipe(pipe_slow);\n@@ -2827,4 +17105,7 @@\n-\/\/ Replaces vec during post-selection cleanup. See above.\n-operand vecY() %{\n-  constraint(ALLOC_IN_RC(vectory_reg_vlbwdq));\n-  match(VecY);\n+\/\/ Return Instruction\n+\/\/ Remove the return address & jump to it.\n+\/\/ Notice: We always emit a nop after a ret to make sure there is room\n+\/\/ for safepoint patching\n+instruct Ret()\n+%{\n+  match(Return);\n@@ -2832,2 +17113,5 @@\n-  format %{ %}\n-  interface(REG_INTER);\n+  format %{ \"ret\" %}\n+  ins_encode %{\n+    __ ret(0);\n+  %}\n+  ins_pipe(pipe_jmp);\n@@ -2836,4 +17120,9 @@\n-\/\/ Replaces legVec during post-selection cleanup. See above.\n-operand legVecY() %{\n-  constraint(ALLOC_IN_RC(vectory_reg_legacy));\n-  match(VecY);\n+\/\/ Tail Call; Jump from runtime stub to Java code.\n+\/\/ Also known as an 'interprocedural jump'.\n+\/\/ Target of jump will eventually return to caller.\n+\/\/ TailJump below removes the return address.\n+\/\/ Don't use rbp for 'jump_target' because a MachEpilogNode has already been\n+\/\/ emitted just above the TailCall which has reset rbp to the caller state.\n+instruct TailCalljmpInd(no_rbp_RegP jump_target, rbx_RegP method_ptr)\n+%{\n+  match(TailCall jump_target method_ptr);\n@@ -2841,2 +17130,6 @@\n-  format %{ %}\n-  interface(REG_INTER);\n+  ins_cost(300);\n+  format %{ \"jmp     $jump_target\\t# rbx holds method\" %}\n+  ins_encode %{\n+    __ jmp($jump_target$$Register);\n+  %}\n+  ins_pipe(pipe_jmp);\n@@ -2845,4 +17138,5 @@\n-\/\/ Replaces vec during post-selection cleanup. See above.\n-operand vecZ() %{\n-  constraint(ALLOC_IN_RC(vectorz_reg));\n-  match(VecZ);\n+\/\/ Tail Jump; remove the return address; jump to target.\n+\/\/ TailCall above leaves the return address around.\n+instruct tailjmpInd(no_rbp_RegP jump_target, rax_RegP ex_oop)\n+%{\n+  match(TailJump jump_target ex_oop);\n@@ -2850,2 +17144,8 @@\n-  format %{ %}\n-  interface(REG_INTER);\n+  ins_cost(300);\n+  format %{ \"popq    rdx\\t# pop return address\\n\\t\"\n+            \"jmp     $jump_target\" %}\n+  ins_encode %{\n+    __ popq(as_Register(RDX_enc));\n+    __ jmp($jump_target$$Register);\n+  %}\n+  ins_pipe(pipe_jmp);\n@@ -2854,4 +17154,4 @@\n-\/\/ Replaces legVec during post-selection cleanup. See above.\n-operand legVecZ() %{\n-  constraint(ALLOC_IN_RC(vectorz_reg_legacy));\n-  match(VecZ);\n+\/\/ Forward exception.\n+instruct ForwardExceptionjmp()\n+%{\n+  match(ForwardException);\n@@ -2859,2 +17159,5 @@\n-  format %{ %}\n-  interface(REG_INTER);\n+  format %{ \"jmp     forward_exception_stub\" %}\n+  ins_encode %{\n+    __ jump(RuntimeAddress(StubRoutines::forward_exception_entry()), noreg);\n+  %}\n+  ins_pipe(pipe_jmp);\n@@ -2863,1 +17166,6 @@\n-\/\/ INSTRUCTIONS -- Platform independent definitions (same for 32- and 64-bit)\n+\/\/ Create exception oop: created by stack-crawling runtime code.\n+\/\/ Created exception is now available to this handler, and is setup\n+\/\/ just prior to jumping to this handler.  No code emitted.\n+instruct CreateException(rax_RegP ex_oop)\n+%{\n+  match(Set ex_oop (CreateEx));\n@@ -2865,1 +17173,6 @@\n-\/\/ ============================================================================\n+  size(0);\n+  \/\/ use the following format syntax\n+  format %{ \"# exception oop is in rax; no code emitted\" %}\n+  ins_encode();\n+  ins_pipe(empty);\n+%}\n@@ -2867,3 +17180,9 @@\n-instruct ShouldNotReachHere() %{\n-  match(Halt);\n-  format %{ \"stop\\t# ShouldNotReachHere\" %}\n+\/\/ Rethrow exception:\n+\/\/ The exception oop will come in the first argument position.\n+\/\/ Then JUMP (not call) to the rethrow stub code.\n+instruct RethrowException()\n+%{\n+  match(Rethrow);\n+\n+  \/\/ use the following format syntax\n+  format %{ \"jmp     rethrow_stub\" %}\n@@ -2871,4 +17190,1 @@\n-    if (is_reachable()) {\n-      const char* str = __ code_string(_halt_reason);\n-      __ stop(str);\n-    }\n+    __ jump(RuntimeAddress(OptoRuntime::rethrow_stub()), noreg);\n@@ -2876,1 +17192,1 @@\n-  ins_pipe(pipe_slow);\n+  ins_pipe(pipe_jmp);\n@@ -2880,0 +17196,12 @@\n+\/\/ This name is KNOWN by the ADLC and cannot be changed.\n+\/\/ The ADLC forces a 'TypeRawPtr::BOTTOM' output type\n+\/\/ for this guy.\n+instruct tlsLoadP(r15_RegP dst) %{\n+  match(Set dst (ThreadLocal));\n+  effect(DEF dst);\n+\n+  size(0);\n+  format %{ \"# TLS is in R15\" %}\n+  ins_encode( \/*empty encoding*\/ );\n+  ins_pipe(ialu_reg_reg);\n+%}\n@@ -10942,0 +25270,326 @@\n+\n+\/\/----------PEEPHOLE RULES-----------------------------------------------------\n+\/\/ These must follow all instruction definitions as they use the names\n+\/\/ defined in the instructions definitions.\n+\/\/\n+\/\/ peeppredicate ( rule_predicate );\n+\/\/ \/\/ the predicate unless which the peephole rule will be ignored\n+\/\/\n+\/\/ peepmatch ( root_instr_name [preceding_instruction]* );\n+\/\/\n+\/\/ peepprocedure ( procedure_name );\n+\/\/ \/\/ provide a procedure name to perform the optimization, the procedure should\n+\/\/ \/\/ reside in the architecture dependent peephole file, the method has the\n+\/\/ \/\/ signature of MachNode* (Block*, int, PhaseRegAlloc*, (MachNode*)(*)(), int...)\n+\/\/ \/\/ with the arguments being the basic block, the current node index inside the\n+\/\/ \/\/ block, the register allocator, the functions upon invoked return a new node\n+\/\/ \/\/ defined in peepreplace, and the rules of the nodes appearing in the\n+\/\/ \/\/ corresponding peepmatch, the function return true if successful, else\n+\/\/ \/\/ return false\n+\/\/\n+\/\/ peepconstraint %{\n+\/\/ (instruction_number.operand_name relational_op instruction_number.operand_name\n+\/\/  [, ...] );\n+\/\/ \/\/ instruction numbers are zero-based using left to right order in peepmatch\n+\/\/\n+\/\/ peepreplace ( instr_name  ( [instruction_number.operand_name]* ) );\n+\/\/ \/\/ provide an instruction_number.operand_name for each operand that appears\n+\/\/ \/\/ in the replacement instruction's match rule\n+\/\/\n+\/\/ ---------VM FLAGS---------------------------------------------------------\n+\/\/\n+\/\/ All peephole optimizations can be turned off using -XX:-OptoPeephole\n+\/\/\n+\/\/ Each peephole rule is given an identifying number starting with zero and\n+\/\/ increasing by one in the order seen by the parser.  An individual peephole\n+\/\/ can be enabled, and all others disabled, by using -XX:OptoPeepholeAt=#\n+\/\/ on the command-line.\n+\/\/\n+\/\/ ---------CURRENT LIMITATIONS----------------------------------------------\n+\/\/\n+\/\/ Only transformations inside a basic block (do we need more for peephole)\n+\/\/\n+\/\/ ---------EXAMPLE----------------------------------------------------------\n+\/\/\n+\/\/ \/\/ pertinent parts of existing instructions in architecture description\n+\/\/ instruct movI(rRegI dst, rRegI src)\n+\/\/ %{\n+\/\/   match(Set dst (CopyI src));\n+\/\/ %}\n+\/\/\n+\/\/ instruct incI_rReg(rRegI dst, immI_1 src, rFlagsReg cr)\n+\/\/ %{\n+\/\/   match(Set dst (AddI dst src));\n+\/\/   effect(KILL cr);\n+\/\/ %}\n+\/\/\n+\/\/ instruct leaI_rReg_immI(rRegI dst, immI_1 src)\n+\/\/ %{\n+\/\/   match(Set dst (AddI dst src));\n+\/\/ %}\n+\/\/\n+\/\/ 1. Simple replacement\n+\/\/ - Only match adjacent instructions in same basic block\n+\/\/ - Only equality constraints\n+\/\/ - Only constraints between operands, not (0.dest_reg == RAX_enc)\n+\/\/ - Only one replacement instruction\n+\/\/\n+\/\/ \/\/ Change (inc mov) to lea\n+\/\/ peephole %{\n+\/\/   \/\/ lea should only be emitted when beneficial\n+\/\/   peeppredicate( VM_Version::supports_fast_2op_lea() );\n+\/\/   \/\/ increment preceded by register-register move\n+\/\/   peepmatch ( incI_rReg movI );\n+\/\/   \/\/ require that the destination register of the increment\n+\/\/   \/\/ match the destination register of the move\n+\/\/   peepconstraint ( 0.dst == 1.dst );\n+\/\/   \/\/ construct a replacement instruction that sets\n+\/\/   \/\/ the destination to ( move's source register + one )\n+\/\/   peepreplace ( leaI_rReg_immI( 0.dst 1.src 0.src ) );\n+\/\/ %}\n+\/\/\n+\/\/ 2. Procedural replacement\n+\/\/ - More flexible finding relevent nodes\n+\/\/ - More flexible constraints\n+\/\/ - More flexible transformations\n+\/\/ - May utilise architecture-dependent API more effectively\n+\/\/ - Currently only one replacement instruction due to adlc parsing capabilities\n+\/\/\n+\/\/ \/\/ Change (inc mov) to lea\n+\/\/ peephole %{\n+\/\/   \/\/ lea should only be emitted when beneficial\n+\/\/   peeppredicate( VM_Version::supports_fast_2op_lea() );\n+\/\/   \/\/ the rule numbers of these nodes inside are passed into the function below\n+\/\/   peepmatch ( incI_rReg movI );\n+\/\/   \/\/ the method that takes the responsibility of transformation\n+\/\/   peepprocedure ( inc_mov_to_lea );\n+\/\/   \/\/ the replacement is a leaI_rReg_immI, a lambda upon invoked creating this\n+\/\/   \/\/ node is passed into the function above\n+\/\/   peepreplace ( leaI_rReg_immI() );\n+\/\/ %}\n+\n+\/\/ These instructions is not matched by the matcher but used by the peephole\n+instruct leaI_rReg_rReg_peep(rRegI dst, rRegI src1, rRegI src2)\n+%{\n+  predicate(false);\n+  match(Set dst (AddI src1 src2));\n+  format %{ \"leal    $dst, [$src1 + $src2]\" %}\n+  ins_encode %{\n+    Register dst = $dst$$Register;\n+    Register src1 = $src1$$Register;\n+    Register src2 = $src2$$Register;\n+    if (src1 != rbp && src1 != r13) {\n+      __ leal(dst, Address(src1, src2, Address::times_1));\n+    } else {\n+      assert(src2 != rbp && src2 != r13, \"\");\n+      __ leal(dst, Address(src2, src1, Address::times_1));\n+    }\n+  %}\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+instruct leaI_rReg_immI_peep(rRegI dst, rRegI src1, immI src2)\n+%{\n+  predicate(false);\n+  match(Set dst (AddI src1 src2));\n+  format %{ \"leal    $dst, [$src1 + $src2]\" %}\n+  ins_encode %{\n+    __ leal($dst$$Register, Address($src1$$Register, $src2$$constant));\n+  %}\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+instruct leaI_rReg_immI2_peep(rRegI dst, rRegI src, immI2 shift)\n+%{\n+  predicate(false);\n+  match(Set dst (LShiftI src shift));\n+  format %{ \"leal    $dst, [$src << $shift]\" %}\n+  ins_encode %{\n+    Address::ScaleFactor scale = static_cast<Address::ScaleFactor>($shift$$constant);\n+    Register src = $src$$Register;\n+    if (scale == Address::times_2 && src != rbp && src != r13) {\n+      __ leal($dst$$Register, Address(src, src, Address::times_1));\n+    } else {\n+      __ leal($dst$$Register, Address(noreg, src, scale));\n+    }\n+  %}\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+instruct leaL_rReg_rReg_peep(rRegL dst, rRegL src1, rRegL src2)\n+%{\n+  predicate(false);\n+  match(Set dst (AddL src1 src2));\n+  format %{ \"leaq    $dst, [$src1 + $src2]\" %}\n+  ins_encode %{\n+    Register dst = $dst$$Register;\n+    Register src1 = $src1$$Register;\n+    Register src2 = $src2$$Register;\n+    if (src1 != rbp && src1 != r13) {\n+      __ leaq(dst, Address(src1, src2, Address::times_1));\n+    } else {\n+      assert(src2 != rbp && src2 != r13, \"\");\n+      __ leaq(dst, Address(src2, src1, Address::times_1));\n+    }\n+  %}\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+instruct leaL_rReg_immL32_peep(rRegL dst, rRegL src1, immL32 src2)\n+%{\n+  predicate(false);\n+  match(Set dst (AddL src1 src2));\n+  format %{ \"leaq    $dst, [$src1 + $src2]\" %}\n+  ins_encode %{\n+    __ leaq($dst$$Register, Address($src1$$Register, $src2$$constant));\n+  %}\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+instruct leaL_rReg_immI2_peep(rRegL dst, rRegL src, immI2 shift)\n+%{\n+  predicate(false);\n+  match(Set dst (LShiftL src shift));\n+  format %{ \"leaq    $dst, [$src << $shift]\" %}\n+  ins_encode %{\n+    Address::ScaleFactor scale = static_cast<Address::ScaleFactor>($shift$$constant);\n+    Register src = $src$$Register;\n+    if (scale == Address::times_2 && src != rbp && src != r13) {\n+      __ leaq($dst$$Register, Address(src, src, Address::times_1));\n+    } else {\n+      __ leaq($dst$$Register, Address(noreg, src, scale));\n+    }\n+  %}\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+\/\/ These peephole rules replace mov + I pairs (where I is one of {add, inc, dec,\n+\/\/ sal}) with lea instructions. The {add, sal} rules are beneficial in\n+\/\/ processors with at least partial ALU support for lea\n+\/\/ (supports_fast_2op_lea()), whereas the {inc, dec} rules are only generally\n+\/\/ beneficial for processors with full ALU support\n+\/\/ (VM_Version::supports_fast_3op_lea()) and Intel Cascade Lake.\n+\n+peephole\n+%{\n+  peeppredicate(VM_Version::supports_fast_2op_lea());\n+  peepmatch (addI_rReg);\n+  peepprocedure (lea_coalesce_reg);\n+  peepreplace (leaI_rReg_rReg_peep());\n+%}\n+\n+peephole\n+%{\n+  peeppredicate(VM_Version::supports_fast_2op_lea());\n+  peepmatch (addI_rReg_imm);\n+  peepprocedure (lea_coalesce_imm);\n+  peepreplace (leaI_rReg_immI_peep());\n+%}\n+\n+peephole\n+%{\n+  peeppredicate(VM_Version::supports_fast_3op_lea() ||\n+                VM_Version::is_intel_cascade_lake());\n+  peepmatch (incI_rReg);\n+  peepprocedure (lea_coalesce_imm);\n+  peepreplace (leaI_rReg_immI_peep());\n+%}\n+\n+peephole\n+%{\n+  peeppredicate(VM_Version::supports_fast_3op_lea() ||\n+                VM_Version::is_intel_cascade_lake());\n+  peepmatch (decI_rReg);\n+  peepprocedure (lea_coalesce_imm);\n+  peepreplace (leaI_rReg_immI_peep());\n+%}\n+\n+peephole\n+%{\n+  peeppredicate(VM_Version::supports_fast_2op_lea());\n+  peepmatch (salI_rReg_immI2);\n+  peepprocedure (lea_coalesce_imm);\n+  peepreplace (leaI_rReg_immI2_peep());\n+%}\n+\n+peephole\n+%{\n+  peeppredicate(VM_Version::supports_fast_2op_lea());\n+  peepmatch (addL_rReg);\n+  peepprocedure (lea_coalesce_reg);\n+  peepreplace (leaL_rReg_rReg_peep());\n+%}\n+\n+peephole\n+%{\n+  peeppredicate(VM_Version::supports_fast_2op_lea());\n+  peepmatch (addL_rReg_imm);\n+  peepprocedure (lea_coalesce_imm);\n+  peepreplace (leaL_rReg_immL32_peep());\n+%}\n+\n+peephole\n+%{\n+  peeppredicate(VM_Version::supports_fast_3op_lea() ||\n+                VM_Version::is_intel_cascade_lake());\n+  peepmatch (incL_rReg);\n+  peepprocedure (lea_coalesce_imm);\n+  peepreplace (leaL_rReg_immL32_peep());\n+%}\n+\n+peephole\n+%{\n+  peeppredicate(VM_Version::supports_fast_3op_lea() ||\n+                VM_Version::is_intel_cascade_lake());\n+  peepmatch (decL_rReg);\n+  peepprocedure (lea_coalesce_imm);\n+  peepreplace (leaL_rReg_immL32_peep());\n+%}\n+\n+peephole\n+%{\n+  peeppredicate(VM_Version::supports_fast_2op_lea());\n+  peepmatch (salL_rReg_immI2);\n+  peepprocedure (lea_coalesce_imm);\n+  peepreplace (leaL_rReg_immI2_peep());\n+%}\n+\n+peephole\n+%{\n+  peepmatch (leaPCompressedOopOffset);\n+  peepprocedure (lea_remove_redundant);\n+%}\n+\n+peephole\n+%{\n+  peepmatch (leaP8Narrow);\n+  peepprocedure (lea_remove_redundant);\n+%}\n+\n+peephole\n+%{\n+  peepmatch (leaP32Narrow);\n+  peepprocedure (lea_remove_redundant);\n+%}\n+\n+\/\/ These peephole rules matches instructions which set flags and are followed by a testI\/L_reg\n+\/\/ The test instruction is redudanent in case the downstream instuctions (like JCC or CMOV) only use flags that are already set by the previous instruction\n+\n+\/\/int variant\n+peephole\n+%{\n+  peepmatch (testI_reg);\n+  peepprocedure (test_may_remove);\n+%}\n+\n+\/\/long variant\n+peephole\n+%{\n+  peepmatch (testL_reg);\n+  peepprocedure (test_may_remove);\n+%}\n+\n+\n+\/\/----------SMARTSPILL RULES---------------------------------------------------\n+\/\/ These must follow all instruction definitions as they use the names\n+\/\/ defined in the instructions definitions.\n","filename":"src\/hotspot\/cpu\/x86\/x86.ad","additions":16233,"deletions":1579,"binary":false,"changes":17812,"status":"modified"},{"patch":"@@ -1,14735 +0,0 @@\n-\/\/\n-\/\/ Copyright (c) 2003, 2025, Oracle and\/or its affiliates. All rights reserved.\n-\/\/ DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n-\/\/\n-\/\/ This code is free software; you can redistribute it and\/or modify it\n-\/\/ under the terms of the GNU General Public License version 2 only, as\n-\/\/ published by the Free Software Foundation.\n-\/\/\n-\/\/ This code is distributed in the hope that it will be useful, but WITHOUT\n-\/\/ ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n-\/\/ FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n-\/\/ version 2 for more details (a copy is included in the LICENSE file that\n-\/\/ accompanied this code).\n-\/\/\n-\/\/ You should have received a copy of the GNU General Public License version\n-\/\/ 2 along with this work; if not, write to the Free Software Foundation,\n-\/\/ Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n-\/\/\n-\/\/ Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n-\/\/ or visit www.oracle.com if you need additional information or have any\n-\/\/ questions.\n-\/\/\n-\/\/\n-\n-\/\/ AMD64 Architecture Description File\n-\n-\/\/----------REGISTER DEFINITION BLOCK------------------------------------------\n-\/\/ This information is used by the matcher and the register allocator to\n-\/\/ describe individual registers and classes of registers within the target\n-\/\/ architecture.\n-\n-register %{\n-\/\/----------Architecture Description Register Definitions----------------------\n-\/\/ General Registers\n-\/\/ \"reg_def\"  name ( register save type, C convention save type,\n-\/\/                   ideal register type, encoding );\n-\/\/ Register Save Types:\n-\/\/\n-\/\/ NS  = No-Save:       The register allocator assumes that these registers\n-\/\/                      can be used without saving upon entry to the method, &\n-\/\/                      that they do not need to be saved at call sites.\n-\/\/\n-\/\/ SOC = Save-On-Call:  The register allocator assumes that these registers\n-\/\/                      can be used without saving upon entry to the method,\n-\/\/                      but that they must be saved at call sites.\n-\/\/\n-\/\/ SOE = Save-On-Entry: The register allocator assumes that these registers\n-\/\/                      must be saved before using them upon entry to the\n-\/\/                      method, but they do not need to be saved at call\n-\/\/                      sites.\n-\/\/\n-\/\/ AS  = Always-Save:   The register allocator assumes that these registers\n-\/\/                      must be saved before using them upon entry to the\n-\/\/                      method, & that they must be saved at call sites.\n-\/\/\n-\/\/ Ideal Register Type is used to determine how to save & restore a\n-\/\/ register.  Op_RegI will get spilled with LoadI\/StoreI, Op_RegP will get\n-\/\/ spilled with LoadP\/StoreP.  If the register supports both, use Op_RegI.\n-\/\/\n-\/\/ The encoding number is the actual bit-pattern placed into the opcodes.\n-\n-\/\/ General Registers\n-\/\/ R8-R15 must be encoded with REX.  (RSP, RBP, RSI, RDI need REX when\n-\/\/ used as byte registers)\n-\n-\/\/ Previously set RBX, RSI, and RDI as save-on-entry for java code\n-\/\/ Turn off SOE in java-code due to frequent use of uncommon-traps.\n-\/\/ Now that allocator is better, turn on RSI and RDI as SOE registers.\n-\n-reg_def RAX  (SOC, SOC, Op_RegI,  0, rax->as_VMReg());\n-reg_def RAX_H(SOC, SOC, Op_RegI,  0, rax->as_VMReg()->next());\n-\n-reg_def RCX  (SOC, SOC, Op_RegI,  1, rcx->as_VMReg());\n-reg_def RCX_H(SOC, SOC, Op_RegI,  1, rcx->as_VMReg()->next());\n-\n-reg_def RDX  (SOC, SOC, Op_RegI,  2, rdx->as_VMReg());\n-reg_def RDX_H(SOC, SOC, Op_RegI,  2, rdx->as_VMReg()->next());\n-\n-reg_def RBX  (SOC, SOE, Op_RegI,  3, rbx->as_VMReg());\n-reg_def RBX_H(SOC, SOE, Op_RegI,  3, rbx->as_VMReg()->next());\n-\n-reg_def RSP  (NS,  NS,  Op_RegI,  4, rsp->as_VMReg());\n-reg_def RSP_H(NS,  NS,  Op_RegI,  4, rsp->as_VMReg()->next());\n-\n-\/\/ now that adapter frames are gone RBP is always saved and restored by the prolog\/epilog code\n-reg_def RBP  (NS, SOE, Op_RegI,  5, rbp->as_VMReg());\n-reg_def RBP_H(NS, SOE, Op_RegI,  5, rbp->as_VMReg()->next());\n-\n-#ifdef _WIN64\n-\n-reg_def RSI  (SOC, SOE, Op_RegI,  6, rsi->as_VMReg());\n-reg_def RSI_H(SOC, SOE, Op_RegI,  6, rsi->as_VMReg()->next());\n-\n-reg_def RDI  (SOC, SOE, Op_RegI,  7, rdi->as_VMReg());\n-reg_def RDI_H(SOC, SOE, Op_RegI,  7, rdi->as_VMReg()->next());\n-\n-#else\n-\n-reg_def RSI  (SOC, SOC, Op_RegI,  6, rsi->as_VMReg());\n-reg_def RSI_H(SOC, SOC, Op_RegI,  6, rsi->as_VMReg()->next());\n-\n-reg_def RDI  (SOC, SOC, Op_RegI,  7, rdi->as_VMReg());\n-reg_def RDI_H(SOC, SOC, Op_RegI,  7, rdi->as_VMReg()->next());\n-\n-#endif\n-\n-reg_def R8   (SOC, SOC, Op_RegI,  8, r8->as_VMReg());\n-reg_def R8_H (SOC, SOC, Op_RegI,  8, r8->as_VMReg()->next());\n-\n-reg_def R9   (SOC, SOC, Op_RegI,  9, r9->as_VMReg());\n-reg_def R9_H (SOC, SOC, Op_RegI,  9, r9->as_VMReg()->next());\n-\n-reg_def R10  (SOC, SOC, Op_RegI, 10, r10->as_VMReg());\n-reg_def R10_H(SOC, SOC, Op_RegI, 10, r10->as_VMReg()->next());\n-\n-reg_def R11  (SOC, SOC, Op_RegI, 11, r11->as_VMReg());\n-reg_def R11_H(SOC, SOC, Op_RegI, 11, r11->as_VMReg()->next());\n-\n-reg_def R12  (SOC, SOE, Op_RegI, 12, r12->as_VMReg());\n-reg_def R12_H(SOC, SOE, Op_RegI, 12, r12->as_VMReg()->next());\n-\n-reg_def R13  (SOC, SOE, Op_RegI, 13, r13->as_VMReg());\n-reg_def R13_H(SOC, SOE, Op_RegI, 13, r13->as_VMReg()->next());\n-\n-reg_def R14  (SOC, SOE, Op_RegI, 14, r14->as_VMReg());\n-reg_def R14_H(SOC, SOE, Op_RegI, 14, r14->as_VMReg()->next());\n-\n-reg_def R15  (SOC, SOE, Op_RegI, 15, r15->as_VMReg());\n-reg_def R15_H(SOC, SOE, Op_RegI, 15, r15->as_VMReg()->next());\n-\n-reg_def R16  (SOC, SOC, Op_RegI, 16, r16->as_VMReg());\n-reg_def R16_H(SOC, SOC, Op_RegI, 16, r16->as_VMReg()->next());\n-\n-reg_def R17  (SOC, SOC, Op_RegI, 17, r17->as_VMReg());\n-reg_def R17_H(SOC, SOC, Op_RegI, 17, r17->as_VMReg()->next());\n-\n-reg_def R18  (SOC, SOC, Op_RegI, 18, r18->as_VMReg());\n-reg_def R18_H(SOC, SOC, Op_RegI, 18, r18->as_VMReg()->next());\n-\n-reg_def R19  (SOC, SOC, Op_RegI, 19, r19->as_VMReg());\n-reg_def R19_H(SOC, SOC, Op_RegI, 19, r19->as_VMReg()->next());\n-\n-reg_def R20  (SOC, SOC, Op_RegI, 20, r20->as_VMReg());\n-reg_def R20_H(SOC, SOC, Op_RegI, 20, r20->as_VMReg()->next());\n-\n-reg_def R21  (SOC, SOC, Op_RegI, 21, r21->as_VMReg());\n-reg_def R21_H(SOC, SOC, Op_RegI, 21, r21->as_VMReg()->next());\n-\n-reg_def R22  (SOC, SOC, Op_RegI, 22, r22->as_VMReg());\n-reg_def R22_H(SOC, SOC, Op_RegI, 22, r22->as_VMReg()->next());\n-\n-reg_def R23  (SOC, SOC, Op_RegI, 23, r23->as_VMReg());\n-reg_def R23_H(SOC, SOC, Op_RegI, 23, r23->as_VMReg()->next());\n-\n-reg_def R24  (SOC, SOC, Op_RegI, 24, r24->as_VMReg());\n-reg_def R24_H(SOC, SOC, Op_RegI, 24, r24->as_VMReg()->next());\n-\n-reg_def R25  (SOC, SOC, Op_RegI, 25, r25->as_VMReg());\n-reg_def R25_H(SOC, SOC, Op_RegI, 25, r25->as_VMReg()->next());\n-\n-reg_def R26  (SOC, SOC, Op_RegI, 26, r26->as_VMReg());\n-reg_def R26_H(SOC, SOC, Op_RegI, 26, r26->as_VMReg()->next());\n-\n-reg_def R27  (SOC, SOC, Op_RegI, 27, r27->as_VMReg());\n-reg_def R27_H(SOC, SOC, Op_RegI, 27, r27->as_VMReg()->next());\n-\n-reg_def R28  (SOC, SOC, Op_RegI, 28, r28->as_VMReg());\n-reg_def R28_H(SOC, SOC, Op_RegI, 28, r28->as_VMReg()->next());\n-\n-reg_def R29  (SOC, SOC, Op_RegI, 29, r29->as_VMReg());\n-reg_def R29_H(SOC, SOC, Op_RegI, 29, r29->as_VMReg()->next());\n-\n-reg_def R30  (SOC, SOC, Op_RegI, 30, r30->as_VMReg());\n-reg_def R30_H(SOC, SOC, Op_RegI, 30, r30->as_VMReg()->next());\n-\n-reg_def R31  (SOC, SOC, Op_RegI, 31, r31->as_VMReg());\n-reg_def R31_H(SOC, SOC, Op_RegI, 31, r31->as_VMReg()->next());\n-\n-\/\/ Floating Point Registers\n-\n-\/\/ Specify priority of register selection within phases of register\n-\/\/ allocation.  Highest priority is first.  A useful heuristic is to\n-\/\/ give registers a low priority when they are required by machine\n-\/\/ instructions, like EAX and EDX on I486, and choose no-save registers\n-\/\/ before save-on-call, & save-on-call before save-on-entry.  Registers\n-\/\/ which participate in fixed calling sequences should come last.\n-\/\/ Registers which are used as pairs must fall on an even boundary.\n-\n-alloc_class chunk0(R10,         R10_H,\n-                   R11,         R11_H,\n-                   R8,          R8_H,\n-                   R9,          R9_H,\n-                   R12,         R12_H,\n-                   RCX,         RCX_H,\n-                   RBX,         RBX_H,\n-                   RDI,         RDI_H,\n-                   RDX,         RDX_H,\n-                   RSI,         RSI_H,\n-                   RAX,         RAX_H,\n-                   RBP,         RBP_H,\n-                   R13,         R13_H,\n-                   R14,         R14_H,\n-                   R15,         R15_H,\n-                   R16,         R16_H,\n-                   R17,         R17_H,\n-                   R18,         R18_H,\n-                   R19,         R19_H,\n-                   R20,         R20_H,\n-                   R21,         R21_H,\n-                   R22,         R22_H,\n-                   R23,         R23_H,\n-                   R24,         R24_H,\n-                   R25,         R25_H,\n-                   R26,         R26_H,\n-                   R27,         R27_H,\n-                   R28,         R28_H,\n-                   R29,         R29_H,\n-                   R30,         R30_H,\n-                   R31,         R31_H,\n-                   RSP,         RSP_H);\n-\n-\n-\/\/----------Architecture Description Register Classes--------------------------\n-\/\/ Several register classes are automatically defined based upon information in\n-\/\/ this architecture description.\n-\/\/ 1) reg_class inline_cache_reg           ( \/* as def'd in frame section *\/ )\n-\/\/ 2) reg_class stack_slots( \/* one chunk of stack-based \"registers\" *\/ )\n-\/\/\n-\n-\/\/ Empty register class.\n-reg_class no_reg();\n-\n-\/\/ Class for all pointer\/long registers including APX extended GPRs.\n-reg_class all_reg(RAX, RAX_H,\n-                  RDX, RDX_H,\n-                  RBP, RBP_H,\n-                  RDI, RDI_H,\n-                  RSI, RSI_H,\n-                  RCX, RCX_H,\n-                  RBX, RBX_H,\n-                  RSP, RSP_H,\n-                  R8,  R8_H,\n-                  R9,  R9_H,\n-                  R10, R10_H,\n-                  R11, R11_H,\n-                  R12, R12_H,\n-                  R13, R13_H,\n-                  R14, R14_H,\n-                  R15, R15_H,\n-                  R16, R16_H,\n-                  R17, R17_H,\n-                  R18, R18_H,\n-                  R19, R19_H,\n-                  R20, R20_H,\n-                  R21, R21_H,\n-                  R22, R22_H,\n-                  R23, R23_H,\n-                  R24, R24_H,\n-                  R25, R25_H,\n-                  R26, R26_H,\n-                  R27, R27_H,\n-                  R28, R28_H,\n-                  R29, R29_H,\n-                  R30, R30_H,\n-                  R31, R31_H);\n-\n-\/\/ Class for all int registers including APX extended GPRs.\n-reg_class all_int_reg(RAX\n-                      RDX,\n-                      RBP,\n-                      RDI,\n-                      RSI,\n-                      RCX,\n-                      RBX,\n-                      R8,\n-                      R9,\n-                      R10,\n-                      R11,\n-                      R12,\n-                      R13,\n-                      R14,\n-                      R16,\n-                      R17,\n-                      R18,\n-                      R19,\n-                      R20,\n-                      R21,\n-                      R22,\n-                      R23,\n-                      R24,\n-                      R25,\n-                      R26,\n-                      R27,\n-                      R28,\n-                      R29,\n-                      R30,\n-                      R31);\n-\n-\/\/ Class for all pointer registers\n-reg_class any_reg %{\n-  return _ANY_REG_mask;\n-%}\n-\n-\/\/ Class for all pointer registers (excluding RSP)\n-reg_class ptr_reg %{\n-  return _PTR_REG_mask;\n-%}\n-\n-\/\/ Class for all pointer registers (excluding RSP and RBP)\n-reg_class ptr_reg_no_rbp %{\n-  return _PTR_REG_NO_RBP_mask;\n-%}\n-\n-\/\/ Class for all pointer registers (excluding RAX and RSP)\n-reg_class ptr_no_rax_reg %{\n-  return _PTR_NO_RAX_REG_mask;\n-%}\n-\n-\/\/ Class for all pointer registers (excluding RAX, RBX, and RSP)\n-reg_class ptr_no_rax_rbx_reg %{\n-  return _PTR_NO_RAX_RBX_REG_mask;\n-%}\n-\n-\/\/ Class for all long registers (excluding RSP)\n-reg_class long_reg %{\n-  return _LONG_REG_mask;\n-%}\n-\n-\/\/ Class for all long registers (excluding RAX, RDX and RSP)\n-reg_class long_no_rax_rdx_reg %{\n-  return _LONG_NO_RAX_RDX_REG_mask;\n-%}\n-\n-\/\/ Class for all long registers (excluding RCX and RSP)\n-reg_class long_no_rcx_reg %{\n-  return _LONG_NO_RCX_REG_mask;\n-%}\n-\n-\/\/ Class for all long registers (excluding RBP and R13)\n-reg_class long_no_rbp_r13_reg %{\n-  return _LONG_NO_RBP_R13_REG_mask;\n-%}\n-\n-\/\/ Class for all int registers (excluding RSP)\n-reg_class int_reg %{\n-  return _INT_REG_mask;\n-%}\n-\n-\/\/ Class for all int registers (excluding RAX, RDX, and RSP)\n-reg_class int_no_rax_rdx_reg %{\n-  return _INT_NO_RAX_RDX_REG_mask;\n-%}\n-\n-\/\/ Class for all int registers (excluding RCX and RSP)\n-reg_class int_no_rcx_reg %{\n-  return _INT_NO_RCX_REG_mask;\n-%}\n-\n-\/\/ Class for all int registers (excluding RBP and R13)\n-reg_class int_no_rbp_r13_reg %{\n-  return _INT_NO_RBP_R13_REG_mask;\n-%}\n-\n-\/\/ Singleton class for RAX pointer register\n-reg_class ptr_rax_reg(RAX, RAX_H);\n-\n-\/\/ Singleton class for RBX pointer register\n-reg_class ptr_rbx_reg(RBX, RBX_H);\n-\n-\/\/ Singleton class for RSI pointer register\n-reg_class ptr_rsi_reg(RSI, RSI_H);\n-\n-\/\/ Singleton class for RBP pointer register\n-reg_class ptr_rbp_reg(RBP, RBP_H);\n-\n-\/\/ Singleton class for RDI pointer register\n-reg_class ptr_rdi_reg(RDI, RDI_H);\n-\n-\/\/ Singleton class for stack pointer\n-reg_class ptr_rsp_reg(RSP, RSP_H);\n-\n-\/\/ Singleton class for TLS pointer\n-reg_class ptr_r15_reg(R15, R15_H);\n-\n-\/\/ Singleton class for RAX long register\n-reg_class long_rax_reg(RAX, RAX_H);\n-\n-\/\/ Singleton class for RCX long register\n-reg_class long_rcx_reg(RCX, RCX_H);\n-\n-\/\/ Singleton class for RDX long register\n-reg_class long_rdx_reg(RDX, RDX_H);\n-\n-\/\/ Singleton class for R11 long register\n-reg_class long_r11_reg(R11, R11_H);\n-\n-\/\/ Singleton class for RAX int register\n-reg_class int_rax_reg(RAX);\n-\n-\/\/ Singleton class for RBX int register\n-reg_class int_rbx_reg(RBX);\n-\n-\/\/ Singleton class for RCX int register\n-reg_class int_rcx_reg(RCX);\n-\n-\/\/ Singleton class for RDX int register\n-reg_class int_rdx_reg(RDX);\n-\n-\/\/ Singleton class for RDI int register\n-reg_class int_rdi_reg(RDI);\n-\n-\/\/ Singleton class for instruction pointer\n-\/\/ reg_class ip_reg(RIP);\n-\n-%}\n-\n-\/\/----------SOURCE BLOCK-------------------------------------------------------\n-\/\/ This is a block of C++ code which provides values, functions, and\n-\/\/ definitions necessary in the rest of the architecture description\n-\n-source_hpp %{\n-\n-#include \"peephole_x86_64.hpp\"\n-\n-bool castLL_is_imm32(const Node* n);\n-\n-%}\n-\n-source %{\n-\n-bool castLL_is_imm32(const Node* n) {\n-  assert(n->is_CastLL(), \"must be a CastLL\");\n-  const TypeLong* t = n->bottom_type()->is_long();\n-  return (t->_lo == min_jlong || Assembler::is_simm32(t->_lo)) && (t->_hi == max_jlong || Assembler::is_simm32(t->_hi));\n-}\n-\n-%}\n-\n-\/\/ Register masks\n-source_hpp %{\n-\n-extern RegMask _ANY_REG_mask;\n-extern RegMask _PTR_REG_mask;\n-extern RegMask _PTR_REG_NO_RBP_mask;\n-extern RegMask _PTR_NO_RAX_REG_mask;\n-extern RegMask _PTR_NO_RAX_RBX_REG_mask;\n-extern RegMask _LONG_REG_mask;\n-extern RegMask _LONG_NO_RAX_RDX_REG_mask;\n-extern RegMask _LONG_NO_RCX_REG_mask;\n-extern RegMask _LONG_NO_RBP_R13_REG_mask;\n-extern RegMask _INT_REG_mask;\n-extern RegMask _INT_NO_RAX_RDX_REG_mask;\n-extern RegMask _INT_NO_RCX_REG_mask;\n-extern RegMask _INT_NO_RBP_R13_REG_mask;\n-extern RegMask _FLOAT_REG_mask;\n-\n-extern RegMask _STACK_OR_PTR_REG_mask;\n-extern RegMask _STACK_OR_LONG_REG_mask;\n-extern RegMask _STACK_OR_INT_REG_mask;\n-\n-inline const RegMask& STACK_OR_PTR_REG_mask()  { return _STACK_OR_PTR_REG_mask;  }\n-inline const RegMask& STACK_OR_LONG_REG_mask() { return _STACK_OR_LONG_REG_mask; }\n-inline const RegMask& STACK_OR_INT_REG_mask()  { return _STACK_OR_INT_REG_mask;  }\n-\n-%}\n-\n-source %{\n-#define   RELOC_IMM64    Assembler::imm_operand\n-#define   RELOC_DISP32   Assembler::disp32_operand\n-\n-#define __ masm->\n-\n-RegMask _ANY_REG_mask;\n-RegMask _PTR_REG_mask;\n-RegMask _PTR_REG_NO_RBP_mask;\n-RegMask _PTR_NO_RAX_REG_mask;\n-RegMask _PTR_NO_RAX_RBX_REG_mask;\n-RegMask _LONG_REG_mask;\n-RegMask _LONG_NO_RAX_RDX_REG_mask;\n-RegMask _LONG_NO_RCX_REG_mask;\n-RegMask _LONG_NO_RBP_R13_REG_mask;\n-RegMask _INT_REG_mask;\n-RegMask _INT_NO_RAX_RDX_REG_mask;\n-RegMask _INT_NO_RCX_REG_mask;\n-RegMask _INT_NO_RBP_R13_REG_mask;\n-RegMask _FLOAT_REG_mask;\n-RegMask _STACK_OR_PTR_REG_mask;\n-RegMask _STACK_OR_LONG_REG_mask;\n-RegMask _STACK_OR_INT_REG_mask;\n-\n-static bool need_r12_heapbase() {\n-  return UseCompressedOops;\n-}\n-\n-void reg_mask_init() {\n-  constexpr Register egprs[] = {r16, r17, r18, r19, r20, r21, r22, r23, r24, r25, r26, r27, r28, r29, r30, r31};\n-\n-  \/\/ _ALL_REG_mask is generated by adlc from the all_reg register class below.\n-  \/\/ We derive a number of subsets from it.\n-  _ANY_REG_mask.assignFrom(_ALL_REG_mask);\n-\n-  if (PreserveFramePointer) {\n-    _ANY_REG_mask.remove(OptoReg::as_OptoReg(rbp->as_VMReg()));\n-    _ANY_REG_mask.remove(OptoReg::as_OptoReg(rbp->as_VMReg()->next()));\n-  }\n-  if (need_r12_heapbase()) {\n-    _ANY_REG_mask.remove(OptoReg::as_OptoReg(r12->as_VMReg()));\n-    _ANY_REG_mask.remove(OptoReg::as_OptoReg(r12->as_VMReg()->next()));\n-  }\n-\n-  _PTR_REG_mask.assignFrom(_ANY_REG_mask);\n-  _PTR_REG_mask.remove(OptoReg::as_OptoReg(rsp->as_VMReg()));\n-  _PTR_REG_mask.remove(OptoReg::as_OptoReg(rsp->as_VMReg()->next()));\n-  _PTR_REG_mask.remove(OptoReg::as_OptoReg(r15->as_VMReg()));\n-  _PTR_REG_mask.remove(OptoReg::as_OptoReg(r15->as_VMReg()->next()));\n-  if (!UseAPX) {\n-    for (uint i = 0; i < sizeof(egprs)\/sizeof(Register); i++) {\n-      _PTR_REG_mask.remove(OptoReg::as_OptoReg(egprs[i]->as_VMReg()));\n-      _PTR_REG_mask.remove(OptoReg::as_OptoReg(egprs[i]->as_VMReg()->next()));\n-    }\n-  }\n-\n-  _STACK_OR_PTR_REG_mask.assignFrom(_PTR_REG_mask);\n-  _STACK_OR_PTR_REG_mask.or_with(STACK_OR_STACK_SLOTS_mask());\n-\n-  _PTR_REG_NO_RBP_mask.assignFrom(_PTR_REG_mask);\n-  _PTR_REG_NO_RBP_mask.remove(OptoReg::as_OptoReg(rbp->as_VMReg()));\n-  _PTR_REG_NO_RBP_mask.remove(OptoReg::as_OptoReg(rbp->as_VMReg()->next()));\n-\n-  _PTR_NO_RAX_REG_mask.assignFrom(_PTR_REG_mask);\n-  _PTR_NO_RAX_REG_mask.remove(OptoReg::as_OptoReg(rax->as_VMReg()));\n-  _PTR_NO_RAX_REG_mask.remove(OptoReg::as_OptoReg(rax->as_VMReg()->next()));\n-\n-  _PTR_NO_RAX_RBX_REG_mask.assignFrom(_PTR_NO_RAX_REG_mask);\n-  _PTR_NO_RAX_RBX_REG_mask.remove(OptoReg::as_OptoReg(rbx->as_VMReg()));\n-  _PTR_NO_RAX_RBX_REG_mask.remove(OptoReg::as_OptoReg(rbx->as_VMReg()->next()));\n-\n-\n-  _LONG_REG_mask.assignFrom(_PTR_REG_mask);\n-  _STACK_OR_LONG_REG_mask.assignFrom(_LONG_REG_mask);\n-  _STACK_OR_LONG_REG_mask.or_with(STACK_OR_STACK_SLOTS_mask());\n-\n-  _LONG_NO_RAX_RDX_REG_mask.assignFrom(_LONG_REG_mask);\n-  _LONG_NO_RAX_RDX_REG_mask.remove(OptoReg::as_OptoReg(rax->as_VMReg()));\n-  _LONG_NO_RAX_RDX_REG_mask.remove(OptoReg::as_OptoReg(rax->as_VMReg()->next()));\n-  _LONG_NO_RAX_RDX_REG_mask.remove(OptoReg::as_OptoReg(rdx->as_VMReg()));\n-  _LONG_NO_RAX_RDX_REG_mask.remove(OptoReg::as_OptoReg(rdx->as_VMReg()->next()));\n-\n-  _LONG_NO_RCX_REG_mask.assignFrom(_LONG_REG_mask);\n-  _LONG_NO_RCX_REG_mask.remove(OptoReg::as_OptoReg(rcx->as_VMReg()));\n-  _LONG_NO_RCX_REG_mask.remove(OptoReg::as_OptoReg(rcx->as_VMReg()->next()));\n-\n-  _LONG_NO_RBP_R13_REG_mask.assignFrom(_LONG_REG_mask);\n-  _LONG_NO_RBP_R13_REG_mask.remove(OptoReg::as_OptoReg(rbp->as_VMReg()));\n-  _LONG_NO_RBP_R13_REG_mask.remove(OptoReg::as_OptoReg(rbp->as_VMReg()->next()));\n-  _LONG_NO_RBP_R13_REG_mask.remove(OptoReg::as_OptoReg(r13->as_VMReg()));\n-  _LONG_NO_RBP_R13_REG_mask.remove(OptoReg::as_OptoReg(r13->as_VMReg()->next()));\n-\n-  _INT_REG_mask.assignFrom(_ALL_INT_REG_mask);\n-  if (!UseAPX) {\n-    for (uint i = 0; i < sizeof(egprs)\/sizeof(Register); i++) {\n-      _INT_REG_mask.remove(OptoReg::as_OptoReg(egprs[i]->as_VMReg()));\n-    }\n-  }\n-\n-  if (PreserveFramePointer) {\n-    _INT_REG_mask.remove(OptoReg::as_OptoReg(rbp->as_VMReg()));\n-  }\n-  if (need_r12_heapbase()) {\n-    _INT_REG_mask.remove(OptoReg::as_OptoReg(r12->as_VMReg()));\n-  }\n-\n-  _STACK_OR_INT_REG_mask.assignFrom(_INT_REG_mask);\n-  _STACK_OR_INT_REG_mask.or_with(STACK_OR_STACK_SLOTS_mask());\n-\n-  _INT_NO_RAX_RDX_REG_mask.assignFrom(_INT_REG_mask);\n-  _INT_NO_RAX_RDX_REG_mask.remove(OptoReg::as_OptoReg(rax->as_VMReg()));\n-  _INT_NO_RAX_RDX_REG_mask.remove(OptoReg::as_OptoReg(rdx->as_VMReg()));\n-\n-  _INT_NO_RCX_REG_mask.assignFrom(_INT_REG_mask);\n-  _INT_NO_RCX_REG_mask.remove(OptoReg::as_OptoReg(rcx->as_VMReg()));\n-\n-  _INT_NO_RBP_R13_REG_mask.assignFrom(_INT_REG_mask);\n-  _INT_NO_RBP_R13_REG_mask.remove(OptoReg::as_OptoReg(rbp->as_VMReg()));\n-  _INT_NO_RBP_R13_REG_mask.remove(OptoReg::as_OptoReg(r13->as_VMReg()));\n-\n-  \/\/ _FLOAT_REG_LEGACY_mask\/_FLOAT_REG_EVEX_mask is generated by adlc\n-  \/\/ from the float_reg_legacy\/float_reg_evex register class.\n-  _FLOAT_REG_mask.assignFrom(VM_Version::supports_evex() ? _FLOAT_REG_EVEX_mask : _FLOAT_REG_LEGACY_mask);\n-}\n-\n-static bool generate_vzeroupper(Compile* C) {\n-  return (VM_Version::supports_vzeroupper() && (C->max_vector_size() > 16 || C->clear_upper_avx() == true)) ? true: false;  \/\/ Generate vzeroupper\n-}\n-\n-static int clear_avx_size() {\n-  return generate_vzeroupper(Compile::current()) ? 3: 0;  \/\/ vzeroupper\n-}\n-\n-\/\/ !!!!! Special hack to get all types of calls to specify the byte offset\n-\/\/       from the start of the call to the point where the return address\n-\/\/       will point.\n-int MachCallStaticJavaNode::ret_addr_offset()\n-{\n-  int offset = 5; \/\/ 5 bytes from start of call to where return address points\n-  offset += clear_avx_size();\n-  return offset;\n-}\n-\n-int MachCallDynamicJavaNode::ret_addr_offset()\n-{\n-  int offset = 15; \/\/ 15 bytes from start of call to where return address points\n-  offset += clear_avx_size();\n-  return offset;\n-}\n-\n-int MachCallRuntimeNode::ret_addr_offset() {\n-  int offset = 13; \/\/ movq r10,#addr; callq (r10)\n-  if (this->ideal_Opcode() != Op_CallLeafVector) {\n-    offset += clear_avx_size();\n-  }\n-  return offset;\n-}\n-\/\/\n-\/\/ Compute padding required for nodes which need alignment\n-\/\/\n-\n-\/\/ The address of the call instruction needs to be 4-byte aligned to\n-\/\/ ensure that it does not span a cache line so that it can be patched.\n-int CallStaticJavaDirectNode::compute_padding(int current_offset) const\n-{\n-  current_offset += clear_avx_size(); \/\/ skip vzeroupper\n-  current_offset += 1; \/\/ skip call opcode byte\n-  return align_up(current_offset, alignment_required()) - current_offset;\n-}\n-\n-\/\/ The address of the call instruction needs to be 4-byte aligned to\n-\/\/ ensure that it does not span a cache line so that it can be patched.\n-int CallDynamicJavaDirectNode::compute_padding(int current_offset) const\n-{\n-  current_offset += clear_avx_size(); \/\/ skip vzeroupper\n-  current_offset += 11; \/\/ skip movq instruction + call opcode byte\n-  return align_up(current_offset, alignment_required()) - current_offset;\n-}\n-\n-\/\/ This could be in MacroAssembler but it's fairly C2 specific\n-static void emit_cmpfp_fixup(MacroAssembler* masm) {\n-  Label exit;\n-  __ jccb(Assembler::noParity, exit);\n-  __ pushf();\n-  \/\/\n-  \/\/ comiss\/ucomiss instructions set ZF,PF,CF flags and\n-  \/\/ zero OF,AF,SF for NaN values.\n-  \/\/ Fixup flags by zeroing ZF,PF so that compare of NaN\n-  \/\/ values returns 'less than' result (CF is set).\n-  \/\/ Leave the rest of flags unchanged.\n-  \/\/\n-  \/\/    7 6 5 4 3 2 1 0\n-  \/\/   |S|Z|r|A|r|P|r|C|  (r - reserved bit)\n-  \/\/    0 0 1 0 1 0 1 1   (0x2B)\n-  \/\/\n-  __ andq(Address(rsp, 0), 0xffffff2b);\n-  __ popf();\n-  __ bind(exit);\n-}\n-\n-static void emit_cmpfp3(MacroAssembler* masm, Register dst) {\n-  Label done;\n-  __ movl(dst, -1);\n-  __ jcc(Assembler::parity, done);\n-  __ jcc(Assembler::below, done);\n-  __ setcc(Assembler::notEqual, dst);\n-  __ bind(done);\n-}\n-\n-\/\/ Math.min()    # Math.max()\n-\/\/ --------------------------\n-\/\/ ucomis[s\/d]   #\n-\/\/ ja   -> b     # a\n-\/\/ jp   -> NaN   # NaN\n-\/\/ jb   -> a     # b\n-\/\/ je            #\n-\/\/ |-jz -> a | b # a & b\n-\/\/ |    -> a     #\n-static void emit_fp_min_max(MacroAssembler* masm, XMMRegister dst,\n-                            XMMRegister a, XMMRegister b,\n-                            XMMRegister xmmt, Register rt,\n-                            bool min, bool single) {\n-\n-  Label nan, zero, below, above, done;\n-\n-  if (single)\n-    __ ucomiss(a, b);\n-  else\n-    __ ucomisd(a, b);\n-\n-  if (dst->encoding() != (min ? b : a)->encoding())\n-    __ jccb(Assembler::above, above); \/\/ CF=0 & ZF=0\n-  else\n-    __ jccb(Assembler::above, done);\n-\n-  __ jccb(Assembler::parity, nan);  \/\/ PF=1\n-  __ jccb(Assembler::below, below); \/\/ CF=1\n-\n-  \/\/ equal\n-  __ vpxor(xmmt, xmmt, xmmt, Assembler::AVX_128bit);\n-  if (single) {\n-    __ ucomiss(a, xmmt);\n-    __ jccb(Assembler::equal, zero);\n-\n-    __ movflt(dst, a);\n-    __ jmp(done);\n-  }\n-  else {\n-    __ ucomisd(a, xmmt);\n-    __ jccb(Assembler::equal, zero);\n-\n-    __ movdbl(dst, a);\n-    __ jmp(done);\n-  }\n-\n-  __ bind(zero);\n-  if (min)\n-    __ vpor(dst, a, b, Assembler::AVX_128bit);\n-  else\n-    __ vpand(dst, a, b, Assembler::AVX_128bit);\n-\n-  __ jmp(done);\n-\n-  __ bind(above);\n-  if (single)\n-    __ movflt(dst, min ? b : a);\n-  else\n-    __ movdbl(dst, min ? b : a);\n-\n-  __ jmp(done);\n-\n-  __ bind(nan);\n-  if (single) {\n-    __ movl(rt, 0x7fc00000); \/\/ Float.NaN\n-    __ movdl(dst, rt);\n-  }\n-  else {\n-    __ mov64(rt, 0x7ff8000000000000L); \/\/ Double.NaN\n-    __ movdq(dst, rt);\n-  }\n-  __ jmp(done);\n-\n-  __ bind(below);\n-  if (single)\n-    __ movflt(dst, min ? a : b);\n-  else\n-    __ movdbl(dst, min ? a : b);\n-\n-  __ bind(done);\n-}\n-\n-\/\/=============================================================================\n-const RegMask& MachConstantBaseNode::_out_RegMask = RegMask::EMPTY;\n-\n-int ConstantTable::calculate_table_base_offset() const {\n-  return 0;  \/\/ absolute addressing, no offset\n-}\n-\n-bool MachConstantBaseNode::requires_postalloc_expand() const { return false; }\n-void MachConstantBaseNode::postalloc_expand(GrowableArray <Node *> *nodes, PhaseRegAlloc *ra_) {\n-  ShouldNotReachHere();\n-}\n-\n-void MachConstantBaseNode::emit(C2_MacroAssembler* masm, PhaseRegAlloc* ra_) const {\n-  \/\/ Empty encoding\n-}\n-\n-uint MachConstantBaseNode::size(PhaseRegAlloc* ra_) const {\n-  return 0;\n-}\n-\n-#ifndef PRODUCT\n-void MachConstantBaseNode::format(PhaseRegAlloc* ra_, outputStream* st) const {\n-  st->print(\"# MachConstantBaseNode (empty encoding)\");\n-}\n-#endif\n-\n-\n-\/\/=============================================================================\n-#ifndef PRODUCT\n-void MachPrologNode::format(PhaseRegAlloc* ra_, outputStream* st) const {\n-  Compile* C = ra_->C;\n-\n-  int framesize = C->output()->frame_size_in_bytes();\n-  int bangsize = C->output()->bang_size_in_bytes();\n-  assert((framesize & (StackAlignmentInBytes-1)) == 0, \"frame size not aligned\");\n-  \/\/ Remove wordSize for return addr which is already pushed.\n-  framesize -= wordSize;\n-\n-  if (C->output()->need_stack_bang(bangsize)) {\n-    framesize -= wordSize;\n-    st->print(\"# stack bang (%d bytes)\", bangsize);\n-    st->print(\"\\n\\t\");\n-    st->print(\"pushq   rbp\\t# Save rbp\");\n-    if (PreserveFramePointer) {\n-        st->print(\"\\n\\t\");\n-        st->print(\"movq    rbp, rsp\\t# Save the caller's SP into rbp\");\n-    }\n-    if (framesize) {\n-      st->print(\"\\n\\t\");\n-      st->print(\"subq    rsp, #%d\\t# Create frame\",framesize);\n-    }\n-  } else {\n-    st->print(\"subq    rsp, #%d\\t# Create frame\",framesize);\n-    st->print(\"\\n\\t\");\n-    framesize -= wordSize;\n-    st->print(\"movq    [rsp + #%d], rbp\\t# Save rbp\",framesize);\n-    if (PreserveFramePointer) {\n-      st->print(\"\\n\\t\");\n-      st->print(\"movq    rbp, rsp\\t# Save the caller's SP into rbp\");\n-      if (framesize > 0) {\n-        st->print(\"\\n\\t\");\n-        st->print(\"addq    rbp, #%d\", framesize);\n-      }\n-    }\n-  }\n-\n-  if (VerifyStackAtCalls) {\n-    st->print(\"\\n\\t\");\n-    framesize -= wordSize;\n-    st->print(\"movq    [rsp + #%d], 0xbadb100d\\t# Majik cookie for stack depth check\",framesize);\n-#ifdef ASSERT\n-    st->print(\"\\n\\t\");\n-    st->print(\"# stack alignment check\");\n-#endif\n-  }\n-  if (C->stub_function() != nullptr) {\n-    st->print(\"\\n\\t\");\n-    st->print(\"cmpl    [r15_thread + #disarmed_guard_value_offset], #disarmed_guard_value\\t\");\n-    st->print(\"\\n\\t\");\n-    st->print(\"je      fast_entry\\t\");\n-    st->print(\"\\n\\t\");\n-    st->print(\"call    #nmethod_entry_barrier_stub\\t\");\n-    st->print(\"\\n\\tfast_entry:\");\n-  }\n-  st->cr();\n-}\n-#endif\n-\n-void MachPrologNode::emit(C2_MacroAssembler *masm, PhaseRegAlloc *ra_) const {\n-  Compile* C = ra_->C;\n-\n-  int framesize = C->output()->frame_size_in_bytes();\n-  int bangsize = C->output()->bang_size_in_bytes();\n-\n-  if (C->clinit_barrier_on_entry()) {\n-    assert(VM_Version::supports_fast_class_init_checks(), \"sanity\");\n-    assert(!C->method()->holder()->is_not_initialized(), \"initialization should have been started\");\n-\n-    Label L_skip_barrier;\n-    Register klass = rscratch1;\n-\n-    __ mov_metadata(klass, C->method()->holder()->constant_encoding());\n-    __ clinit_barrier(klass, &L_skip_barrier \/*L_fast_path*\/);\n-\n-    __ jump(RuntimeAddress(SharedRuntime::get_handle_wrong_method_stub())); \/\/ slow path\n-\n-    __ bind(L_skip_barrier);\n-  }\n-\n-  __ verified_entry(framesize, C->output()->need_stack_bang(bangsize)?bangsize:0, false, C->stub_function() != nullptr);\n-\n-  C->output()->set_frame_complete(__ offset());\n-\n-  if (C->has_mach_constant_base_node()) {\n-    \/\/ NOTE: We set the table base offset here because users might be\n-    \/\/ emitted before MachConstantBaseNode.\n-    ConstantTable& constant_table = C->output()->constant_table();\n-    constant_table.set_table_base_offset(constant_table.calculate_table_base_offset());\n-  }\n-}\n-\n-uint MachPrologNode::size(PhaseRegAlloc* ra_) const\n-{\n-  return MachNode::size(ra_); \/\/ too many variables; just compute it\n-                              \/\/ the hard way\n-}\n-\n-int MachPrologNode::reloc() const\n-{\n-  return 0; \/\/ a large enough number\n-}\n-\n-\/\/=============================================================================\n-#ifndef PRODUCT\n-void MachEpilogNode::format(PhaseRegAlloc* ra_, outputStream* st) const\n-{\n-  Compile* C = ra_->C;\n-  if (generate_vzeroupper(C)) {\n-    st->print(\"vzeroupper\");\n-    st->cr(); st->print(\"\\t\");\n-  }\n-\n-  int framesize = C->output()->frame_size_in_bytes();\n-  assert((framesize & (StackAlignmentInBytes-1)) == 0, \"frame size not aligned\");\n-  \/\/ Remove word for return adr already pushed\n-  \/\/ and RBP\n-  framesize -= 2*wordSize;\n-\n-  if (framesize) {\n-    st->print_cr(\"addq    rsp, %d\\t# Destroy frame\", framesize);\n-    st->print(\"\\t\");\n-  }\n-\n-  st->print_cr(\"popq    rbp\");\n-  if (do_polling() && C->is_method_compilation()) {\n-    st->print(\"\\t\");\n-    st->print_cr(\"cmpq    rsp, poll_offset[r15_thread] \\n\\t\"\n-                 \"ja      #safepoint_stub\\t\"\n-                 \"# Safepoint: poll for GC\");\n-  }\n-}\n-#endif\n-\n-void MachEpilogNode::emit(C2_MacroAssembler* masm, PhaseRegAlloc* ra_) const\n-{\n-  Compile* C = ra_->C;\n-\n-  if (generate_vzeroupper(C)) {\n-    \/\/ Clear upper bits of YMM registers when current compiled code uses\n-    \/\/ wide vectors to avoid AVX <-> SSE transition penalty during call.\n-    __ vzeroupper();\n-  }\n-\n-  int framesize = C->output()->frame_size_in_bytes();\n-  assert((framesize & (StackAlignmentInBytes-1)) == 0, \"frame size not aligned\");\n-  \/\/ Remove word for return adr already pushed\n-  \/\/ and RBP\n-  framesize -= 2*wordSize;\n-\n-  \/\/ Note that VerifyStackAtCalls' Majik cookie does not change the frame size popped here\n-\n-  if (framesize) {\n-    __ addq(rsp, framesize);\n-  }\n-\n-  __ popq(rbp);\n-\n-  if (StackReservedPages > 0 && C->has_reserved_stack_access()) {\n-    __ reserved_stack_check();\n-  }\n-\n-  if (do_polling() && C->is_method_compilation()) {\n-    Label dummy_label;\n-    Label* code_stub = &dummy_label;\n-    if (!C->output()->in_scratch_emit_size()) {\n-      C2SafepointPollStub* stub = new (C->comp_arena()) C2SafepointPollStub(__ offset());\n-      C->output()->add_stub(stub);\n-      code_stub = &stub->entry();\n-    }\n-    __ relocate(relocInfo::poll_return_type);\n-    __ safepoint_poll(*code_stub, true \/* at_return *\/, true \/* in_nmethod *\/);\n-  }\n-}\n-\n-uint MachEpilogNode::size(PhaseRegAlloc* ra_) const\n-{\n-  return MachNode::size(ra_); \/\/ too many variables; just compute it\n-                              \/\/ the hard way\n-}\n-\n-int MachEpilogNode::reloc() const\n-{\n-  return 2; \/\/ a large enough number\n-}\n-\n-const Pipeline* MachEpilogNode::pipeline() const\n-{\n-  return MachNode::pipeline_class();\n-}\n-\n-\/\/=============================================================================\n-\n-enum RC {\n-  rc_bad,\n-  rc_int,\n-  rc_kreg,\n-  rc_float,\n-  rc_stack\n-};\n-\n-static enum RC rc_class(OptoReg::Name reg)\n-{\n-  if( !OptoReg::is_valid(reg)  ) return rc_bad;\n-\n-  if (OptoReg::is_stack(reg)) return rc_stack;\n-\n-  VMReg r = OptoReg::as_VMReg(reg);\n-\n-  if (r->is_Register()) return rc_int;\n-\n-  if (r->is_KRegister()) return rc_kreg;\n-\n-  assert(r->is_XMMRegister(), \"must be\");\n-  return rc_float;\n-}\n-\n-\/\/ Next two methods are shared by 32- and 64-bit VM. They are defined in x86.ad.\n-static void vec_mov_helper(C2_MacroAssembler *masm, int src_lo, int dst_lo,\n-                          int src_hi, int dst_hi, uint ireg, outputStream* st);\n-\n-void vec_spill_helper(C2_MacroAssembler *masm, bool is_load,\n-                     int stack_offset, int reg, uint ireg, outputStream* st);\n-\n-static void vec_stack_to_stack_helper(C2_MacroAssembler *masm, int src_offset,\n-                                      int dst_offset, uint ireg, outputStream* st) {\n-  if (masm) {\n-    switch (ireg) {\n-    case Op_VecS:\n-      __ movq(Address(rsp, -8), rax);\n-      __ movl(rax, Address(rsp, src_offset));\n-      __ movl(Address(rsp, dst_offset), rax);\n-      __ movq(rax, Address(rsp, -8));\n-      break;\n-    case Op_VecD:\n-      __ pushq(Address(rsp, src_offset));\n-      __ popq (Address(rsp, dst_offset));\n-      break;\n-    case Op_VecX:\n-      __ pushq(Address(rsp, src_offset));\n-      __ popq (Address(rsp, dst_offset));\n-      __ pushq(Address(rsp, src_offset+8));\n-      __ popq (Address(rsp, dst_offset+8));\n-      break;\n-    case Op_VecY:\n-      __ vmovdqu(Address(rsp, -32), xmm0);\n-      __ vmovdqu(xmm0, Address(rsp, src_offset));\n-      __ vmovdqu(Address(rsp, dst_offset), xmm0);\n-      __ vmovdqu(xmm0, Address(rsp, -32));\n-      break;\n-    case Op_VecZ:\n-      __ evmovdquq(Address(rsp, -64), xmm0, 2);\n-      __ evmovdquq(xmm0, Address(rsp, src_offset), 2);\n-      __ evmovdquq(Address(rsp, dst_offset), xmm0, 2);\n-      __ evmovdquq(xmm0, Address(rsp, -64), 2);\n-      break;\n-    default:\n-      ShouldNotReachHere();\n-    }\n-#ifndef PRODUCT\n-  } else {\n-    switch (ireg) {\n-    case Op_VecS:\n-      st->print(\"movq    [rsp - #8], rax\\t# 32-bit mem-mem spill\\n\\t\"\n-                \"movl    rax, [rsp + #%d]\\n\\t\"\n-                \"movl    [rsp + #%d], rax\\n\\t\"\n-                \"movq    rax, [rsp - #8]\",\n-                src_offset, dst_offset);\n-      break;\n-    case Op_VecD:\n-      st->print(\"pushq   [rsp + #%d]\\t# 64-bit mem-mem spill\\n\\t\"\n-                \"popq    [rsp + #%d]\",\n-                src_offset, dst_offset);\n-      break;\n-     case Op_VecX:\n-      st->print(\"pushq   [rsp + #%d]\\t# 128-bit mem-mem spill\\n\\t\"\n-                \"popq    [rsp + #%d]\\n\\t\"\n-                \"pushq   [rsp + #%d]\\n\\t\"\n-                \"popq    [rsp + #%d]\",\n-                src_offset, dst_offset, src_offset+8, dst_offset+8);\n-      break;\n-    case Op_VecY:\n-      st->print(\"vmovdqu [rsp - #32], xmm0\\t# 256-bit mem-mem spill\\n\\t\"\n-                \"vmovdqu xmm0, [rsp + #%d]\\n\\t\"\n-                \"vmovdqu [rsp + #%d], xmm0\\n\\t\"\n-                \"vmovdqu xmm0, [rsp - #32]\",\n-                src_offset, dst_offset);\n-      break;\n-    case Op_VecZ:\n-      st->print(\"vmovdqu [rsp - #64], xmm0\\t# 512-bit mem-mem spill\\n\\t\"\n-                \"vmovdqu xmm0, [rsp + #%d]\\n\\t\"\n-                \"vmovdqu [rsp + #%d], xmm0\\n\\t\"\n-                \"vmovdqu xmm0, [rsp - #64]\",\n-                src_offset, dst_offset);\n-      break;\n-    default:\n-      ShouldNotReachHere();\n-    }\n-#endif\n-  }\n-}\n-\n-uint MachSpillCopyNode::implementation(C2_MacroAssembler* masm,\n-                                       PhaseRegAlloc* ra_,\n-                                       bool do_size,\n-                                       outputStream* st) const {\n-  assert(masm != nullptr || st  != nullptr, \"sanity\");\n-  \/\/ Get registers to move\n-  OptoReg::Name src_second = ra_->get_reg_second(in(1));\n-  OptoReg::Name src_first = ra_->get_reg_first(in(1));\n-  OptoReg::Name dst_second = ra_->get_reg_second(this);\n-  OptoReg::Name dst_first = ra_->get_reg_first(this);\n-\n-  enum RC src_second_rc = rc_class(src_second);\n-  enum RC src_first_rc = rc_class(src_first);\n-  enum RC dst_second_rc = rc_class(dst_second);\n-  enum RC dst_first_rc = rc_class(dst_first);\n-\n-  assert(OptoReg::is_valid(src_first) && OptoReg::is_valid(dst_first),\n-         \"must move at least 1 register\" );\n-\n-  if (src_first == dst_first && src_second == dst_second) {\n-    \/\/ Self copy, no move\n-    return 0;\n-  }\n-  if (bottom_type()->isa_vect() != nullptr && bottom_type()->isa_vectmask() == nullptr) {\n-    uint ireg = ideal_reg();\n-    assert((src_first_rc != rc_int && dst_first_rc != rc_int), \"sanity\");\n-    assert((ireg == Op_VecS || ireg == Op_VecD || ireg == Op_VecX || ireg == Op_VecY || ireg == Op_VecZ ), \"sanity\");\n-    if( src_first_rc == rc_stack && dst_first_rc == rc_stack ) {\n-      \/\/ mem -> mem\n-      int src_offset = ra_->reg2offset(src_first);\n-      int dst_offset = ra_->reg2offset(dst_first);\n-      vec_stack_to_stack_helper(masm, src_offset, dst_offset, ireg, st);\n-    } else if (src_first_rc == rc_float && dst_first_rc == rc_float ) {\n-      vec_mov_helper(masm, src_first, dst_first, src_second, dst_second, ireg, st);\n-    } else if (src_first_rc == rc_float && dst_first_rc == rc_stack ) {\n-      int stack_offset = ra_->reg2offset(dst_first);\n-      vec_spill_helper(masm, false, stack_offset, src_first, ireg, st);\n-    } else if (src_first_rc == rc_stack && dst_first_rc == rc_float ) {\n-      int stack_offset = ra_->reg2offset(src_first);\n-      vec_spill_helper(masm, true,  stack_offset, dst_first, ireg, st);\n-    } else {\n-      ShouldNotReachHere();\n-    }\n-    return 0;\n-  }\n-  if (src_first_rc == rc_stack) {\n-    \/\/ mem ->\n-    if (dst_first_rc == rc_stack) {\n-      \/\/ mem -> mem\n-      assert(src_second != dst_first, \"overlap\");\n-      if ((src_first & 1) == 0 && src_first + 1 == src_second &&\n-          (dst_first & 1) == 0 && dst_first + 1 == dst_second) {\n-        \/\/ 64-bit\n-        int src_offset = ra_->reg2offset(src_first);\n-        int dst_offset = ra_->reg2offset(dst_first);\n-        if (masm) {\n-          __ pushq(Address(rsp, src_offset));\n-          __ popq (Address(rsp, dst_offset));\n-#ifndef PRODUCT\n-        } else {\n-          st->print(\"pushq   [rsp + #%d]\\t# 64-bit mem-mem spill\\n\\t\"\n-                    \"popq    [rsp + #%d]\",\n-                     src_offset, dst_offset);\n-#endif\n-        }\n-      } else {\n-        \/\/ 32-bit\n-        assert(!((src_first & 1) == 0 && src_first + 1 == src_second), \"no transform\");\n-        assert(!((dst_first & 1) == 0 && dst_first + 1 == dst_second), \"no transform\");\n-        \/\/ No pushl\/popl, so:\n-        int src_offset = ra_->reg2offset(src_first);\n-        int dst_offset = ra_->reg2offset(dst_first);\n-        if (masm) {\n-          __ movq(Address(rsp, -8), rax);\n-          __ movl(rax, Address(rsp, src_offset));\n-          __ movl(Address(rsp, dst_offset), rax);\n-          __ movq(rax, Address(rsp, -8));\n-#ifndef PRODUCT\n-        } else {\n-          st->print(\"movq    [rsp - #8], rax\\t# 32-bit mem-mem spill\\n\\t\"\n-                    \"movl    rax, [rsp + #%d]\\n\\t\"\n-                    \"movl    [rsp + #%d], rax\\n\\t\"\n-                    \"movq    rax, [rsp - #8]\",\n-                     src_offset, dst_offset);\n-#endif\n-        }\n-      }\n-      return 0;\n-    } else if (dst_first_rc == rc_int) {\n-      \/\/ mem -> gpr\n-      if ((src_first & 1) == 0 && src_first + 1 == src_second &&\n-          (dst_first & 1) == 0 && dst_first + 1 == dst_second) {\n-        \/\/ 64-bit\n-        int offset = ra_->reg2offset(src_first);\n-        if (masm) {\n-          __ movq(as_Register(Matcher::_regEncode[dst_first]), Address(rsp, offset));\n-#ifndef PRODUCT\n-        } else {\n-          st->print(\"movq    %s, [rsp + #%d]\\t# spill\",\n-                     Matcher::regName[dst_first],\n-                     offset);\n-#endif\n-        }\n-      } else {\n-        \/\/ 32-bit\n-        assert(!((src_first & 1) == 0 && src_first + 1 == src_second), \"no transform\");\n-        assert(!((dst_first & 1) == 0 && dst_first + 1 == dst_second), \"no transform\");\n-        int offset = ra_->reg2offset(src_first);\n-        if (masm) {\n-          __ movl(as_Register(Matcher::_regEncode[dst_first]), Address(rsp, offset));\n-#ifndef PRODUCT\n-        } else {\n-          st->print(\"movl    %s, [rsp + #%d]\\t# spill\",\n-                     Matcher::regName[dst_first],\n-                     offset);\n-#endif\n-        }\n-      }\n-      return 0;\n-    } else if (dst_first_rc == rc_float) {\n-      \/\/ mem-> xmm\n-      if ((src_first & 1) == 0 && src_first + 1 == src_second &&\n-          (dst_first & 1) == 0 && dst_first + 1 == dst_second) {\n-        \/\/ 64-bit\n-        int offset = ra_->reg2offset(src_first);\n-        if (masm) {\n-          __ movdbl( as_XMMRegister(Matcher::_regEncode[dst_first]), Address(rsp, offset));\n-#ifndef PRODUCT\n-        } else {\n-          st->print(\"%s  %s, [rsp + #%d]\\t# spill\",\n-                     UseXmmLoadAndClearUpper ? \"movsd \" : \"movlpd\",\n-                     Matcher::regName[dst_first],\n-                     offset);\n-#endif\n-        }\n-      } else {\n-        \/\/ 32-bit\n-        assert(!((src_first & 1) == 0 && src_first + 1 == src_second), \"no transform\");\n-        assert(!((dst_first & 1) == 0 && dst_first + 1 == dst_second), \"no transform\");\n-        int offset = ra_->reg2offset(src_first);\n-        if (masm) {\n-          __ movflt( as_XMMRegister(Matcher::_regEncode[dst_first]), Address(rsp, offset));\n-#ifndef PRODUCT\n-        } else {\n-          st->print(\"movss   %s, [rsp + #%d]\\t# spill\",\n-                     Matcher::regName[dst_first],\n-                     offset);\n-#endif\n-        }\n-      }\n-      return 0;\n-    } else if (dst_first_rc == rc_kreg) {\n-      \/\/ mem -> kreg\n-      if ((src_first & 1) == 0 && src_first + 1 == src_second &&\n-          (dst_first & 1) == 0 && dst_first + 1 == dst_second) {\n-        \/\/ 64-bit\n-        int offset = ra_->reg2offset(src_first);\n-        if (masm) {\n-          __ kmov(as_KRegister(Matcher::_regEncode[dst_first]), Address(rsp, offset));\n-#ifndef PRODUCT\n-        } else {\n-          st->print(\"kmovq   %s, [rsp + #%d]\\t# spill\",\n-                     Matcher::regName[dst_first],\n-                     offset);\n-#endif\n-        }\n-      }\n-      return 0;\n-    }\n-  } else if (src_first_rc == rc_int) {\n-    \/\/ gpr ->\n-    if (dst_first_rc == rc_stack) {\n-      \/\/ gpr -> mem\n-      if ((src_first & 1) == 0 && src_first + 1 == src_second &&\n-          (dst_first & 1) == 0 && dst_first + 1 == dst_second) {\n-        \/\/ 64-bit\n-        int offset = ra_->reg2offset(dst_first);\n-        if (masm) {\n-          __ movq(Address(rsp, offset), as_Register(Matcher::_regEncode[src_first]));\n-#ifndef PRODUCT\n-        } else {\n-          st->print(\"movq    [rsp + #%d], %s\\t# spill\",\n-                     offset,\n-                     Matcher::regName[src_first]);\n-#endif\n-        }\n-      } else {\n-        \/\/ 32-bit\n-        assert(!((src_first & 1) == 0 && src_first + 1 == src_second), \"no transform\");\n-        assert(!((dst_first & 1) == 0 && dst_first + 1 == dst_second), \"no transform\");\n-        int offset = ra_->reg2offset(dst_first);\n-        if (masm) {\n-          __ movl(Address(rsp, offset), as_Register(Matcher::_regEncode[src_first]));\n-#ifndef PRODUCT\n-        } else {\n-          st->print(\"movl    [rsp + #%d], %s\\t# spill\",\n-                     offset,\n-                     Matcher::regName[src_first]);\n-#endif\n-        }\n-      }\n-      return 0;\n-    } else if (dst_first_rc == rc_int) {\n-      \/\/ gpr -> gpr\n-      if ((src_first & 1) == 0 && src_first + 1 == src_second &&\n-          (dst_first & 1) == 0 && dst_first + 1 == dst_second) {\n-        \/\/ 64-bit\n-        if (masm) {\n-          __ movq(as_Register(Matcher::_regEncode[dst_first]),\n-                  as_Register(Matcher::_regEncode[src_first]));\n-#ifndef PRODUCT\n-        } else {\n-          st->print(\"movq    %s, %s\\t# spill\",\n-                     Matcher::regName[dst_first],\n-                     Matcher::regName[src_first]);\n-#endif\n-        }\n-        return 0;\n-      } else {\n-        \/\/ 32-bit\n-        assert(!((src_first & 1) == 0 && src_first + 1 == src_second), \"no transform\");\n-        assert(!((dst_first & 1) == 0 && dst_first + 1 == dst_second), \"no transform\");\n-        if (masm) {\n-          __ movl(as_Register(Matcher::_regEncode[dst_first]),\n-                  as_Register(Matcher::_regEncode[src_first]));\n-#ifndef PRODUCT\n-        } else {\n-          st->print(\"movl    %s, %s\\t# spill\",\n-                     Matcher::regName[dst_first],\n-                     Matcher::regName[src_first]);\n-#endif\n-        }\n-        return 0;\n-      }\n-    } else if (dst_first_rc == rc_float) {\n-      \/\/ gpr -> xmm\n-      if ((src_first & 1) == 0 && src_first + 1 == src_second &&\n-          (dst_first & 1) == 0 && dst_first + 1 == dst_second) {\n-        \/\/ 64-bit\n-        if (masm) {\n-          __ movdq( as_XMMRegister(Matcher::_regEncode[dst_first]), as_Register(Matcher::_regEncode[src_first]));\n-#ifndef PRODUCT\n-        } else {\n-          st->print(\"movdq   %s, %s\\t# spill\",\n-                     Matcher::regName[dst_first],\n-                     Matcher::regName[src_first]);\n-#endif\n-        }\n-      } else {\n-        \/\/ 32-bit\n-        assert(!((src_first & 1) == 0 && src_first + 1 == src_second), \"no transform\");\n-        assert(!((dst_first & 1) == 0 && dst_first + 1 == dst_second), \"no transform\");\n-        if (masm) {\n-          __ movdl( as_XMMRegister(Matcher::_regEncode[dst_first]), as_Register(Matcher::_regEncode[src_first]));\n-#ifndef PRODUCT\n-        } else {\n-          st->print(\"movdl   %s, %s\\t# spill\",\n-                     Matcher::regName[dst_first],\n-                     Matcher::regName[src_first]);\n-#endif\n-        }\n-      }\n-      return 0;\n-    } else if (dst_first_rc == rc_kreg) {\n-      if ((src_first & 1) == 0 && src_first + 1 == src_second &&\n-          (dst_first & 1) == 0 && dst_first + 1 == dst_second) {\n-        \/\/ 64-bit\n-        if (masm) {\n-          __ kmov(as_KRegister(Matcher::_regEncode[dst_first]), as_Register(Matcher::_regEncode[src_first]));\n-  #ifndef PRODUCT\n-        } else {\n-           st->print(\"kmovq   %s, %s\\t# spill\",\n-                       Matcher::regName[dst_first],\n-                       Matcher::regName[src_first]);\n-  #endif\n-        }\n-      }\n-      Unimplemented();\n-      return 0;\n-    }\n-  } else if (src_first_rc == rc_float) {\n-    \/\/ xmm ->\n-    if (dst_first_rc == rc_stack) {\n-      \/\/ xmm -> mem\n-      if ((src_first & 1) == 0 && src_first + 1 == src_second &&\n-          (dst_first & 1) == 0 && dst_first + 1 == dst_second) {\n-        \/\/ 64-bit\n-        int offset = ra_->reg2offset(dst_first);\n-        if (masm) {\n-          __ movdbl( Address(rsp, offset), as_XMMRegister(Matcher::_regEncode[src_first]));\n-#ifndef PRODUCT\n-        } else {\n-          st->print(\"movsd   [rsp + #%d], %s\\t# spill\",\n-                     offset,\n-                     Matcher::regName[src_first]);\n-#endif\n-        }\n-      } else {\n-        \/\/ 32-bit\n-        assert(!((src_first & 1) == 0 && src_first + 1 == src_second), \"no transform\");\n-        assert(!((dst_first & 1) == 0 && dst_first + 1 == dst_second), \"no transform\");\n-        int offset = ra_->reg2offset(dst_first);\n-        if (masm) {\n-          __ movflt(Address(rsp, offset), as_XMMRegister(Matcher::_regEncode[src_first]));\n-#ifndef PRODUCT\n-        } else {\n-          st->print(\"movss   [rsp + #%d], %s\\t# spill\",\n-                     offset,\n-                     Matcher::regName[src_first]);\n-#endif\n-        }\n-      }\n-      return 0;\n-    } else if (dst_first_rc == rc_int) {\n-      \/\/ xmm -> gpr\n-      if ((src_first & 1) == 0 && src_first + 1 == src_second &&\n-          (dst_first & 1) == 0 && dst_first + 1 == dst_second) {\n-        \/\/ 64-bit\n-        if (masm) {\n-          __ movdq( as_Register(Matcher::_regEncode[dst_first]), as_XMMRegister(Matcher::_regEncode[src_first]));\n-#ifndef PRODUCT\n-        } else {\n-          st->print(\"movdq   %s, %s\\t# spill\",\n-                     Matcher::regName[dst_first],\n-                     Matcher::regName[src_first]);\n-#endif\n-        }\n-      } else {\n-        \/\/ 32-bit\n-        assert(!((src_first & 1) == 0 && src_first + 1 == src_second), \"no transform\");\n-        assert(!((dst_first & 1) == 0 && dst_first + 1 == dst_second), \"no transform\");\n-        if (masm) {\n-          __ movdl( as_Register(Matcher::_regEncode[dst_first]), as_XMMRegister(Matcher::_regEncode[src_first]));\n-#ifndef PRODUCT\n-        } else {\n-          st->print(\"movdl   %s, %s\\t# spill\",\n-                     Matcher::regName[dst_first],\n-                     Matcher::regName[src_first]);\n-#endif\n-        }\n-      }\n-      return 0;\n-    } else if (dst_first_rc == rc_float) {\n-      \/\/ xmm -> xmm\n-      if ((src_first & 1) == 0 && src_first + 1 == src_second &&\n-          (dst_first & 1) == 0 && dst_first + 1 == dst_second) {\n-        \/\/ 64-bit\n-        if (masm) {\n-          __ movdbl( as_XMMRegister(Matcher::_regEncode[dst_first]), as_XMMRegister(Matcher::_regEncode[src_first]));\n-#ifndef PRODUCT\n-        } else {\n-          st->print(\"%s  %s, %s\\t# spill\",\n-                     UseXmmRegToRegMoveAll ? \"movapd\" : \"movsd \",\n-                     Matcher::regName[dst_first],\n-                     Matcher::regName[src_first]);\n-#endif\n-        }\n-      } else {\n-        \/\/ 32-bit\n-        assert(!((src_first & 1) == 0 && src_first + 1 == src_second), \"no transform\");\n-        assert(!((dst_first & 1) == 0 && dst_first + 1 == dst_second), \"no transform\");\n-        if (masm) {\n-          __ movflt( as_XMMRegister(Matcher::_regEncode[dst_first]), as_XMMRegister(Matcher::_regEncode[src_first]));\n-#ifndef PRODUCT\n-        } else {\n-          st->print(\"%s  %s, %s\\t# spill\",\n-                     UseXmmRegToRegMoveAll ? \"movaps\" : \"movss \",\n-                     Matcher::regName[dst_first],\n-                     Matcher::regName[src_first]);\n-#endif\n-        }\n-      }\n-      return 0;\n-    } else if (dst_first_rc == rc_kreg) {\n-      assert(false, \"Illegal spilling\");\n-      return 0;\n-    }\n-  } else if (src_first_rc == rc_kreg) {\n-    if (dst_first_rc == rc_stack) {\n-      \/\/ mem -> kreg\n-      if ((src_first & 1) == 0 && src_first + 1 == src_second &&\n-          (dst_first & 1) == 0 && dst_first + 1 == dst_second) {\n-        \/\/ 64-bit\n-        int offset = ra_->reg2offset(dst_first);\n-        if (masm) {\n-          __ kmov(Address(rsp, offset), as_KRegister(Matcher::_regEncode[src_first]));\n-#ifndef PRODUCT\n-        } else {\n-          st->print(\"kmovq   [rsp + #%d] , %s\\t# spill\",\n-                     offset,\n-                     Matcher::regName[src_first]);\n-#endif\n-        }\n-      }\n-      return 0;\n-    } else if (dst_first_rc == rc_int) {\n-      if ((src_first & 1) == 0 && src_first + 1 == src_second &&\n-          (dst_first & 1) == 0 && dst_first + 1 == dst_second) {\n-        \/\/ 64-bit\n-        if (masm) {\n-          __ kmov(as_Register(Matcher::_regEncode[dst_first]), as_KRegister(Matcher::_regEncode[src_first]));\n-#ifndef PRODUCT\n-        } else {\n-         st->print(\"kmovq   %s, %s\\t# spill\",\n-                     Matcher::regName[dst_first],\n-                     Matcher::regName[src_first]);\n-#endif\n-        }\n-      }\n-      Unimplemented();\n-      return 0;\n-    } else if (dst_first_rc == rc_kreg) {\n-      if ((src_first & 1) == 0 && src_first + 1 == src_second &&\n-          (dst_first & 1) == 0 && dst_first + 1 == dst_second) {\n-        \/\/ 64-bit\n-        if (masm) {\n-          __ kmov(as_KRegister(Matcher::_regEncode[dst_first]), as_KRegister(Matcher::_regEncode[src_first]));\n-#ifndef PRODUCT\n-        } else {\n-         st->print(\"kmovq   %s, %s\\t# spill\",\n-                     Matcher::regName[dst_first],\n-                     Matcher::regName[src_first]);\n-#endif\n-        }\n-      }\n-      return 0;\n-    } else if (dst_first_rc == rc_float) {\n-      assert(false, \"Illegal spill\");\n-      return 0;\n-    }\n-  }\n-\n-  assert(0,\" foo \");\n-  Unimplemented();\n-  return 0;\n-}\n-\n-#ifndef PRODUCT\n-void MachSpillCopyNode::format(PhaseRegAlloc *ra_, outputStream* st) const {\n-  implementation(nullptr, ra_, false, st);\n-}\n-#endif\n-\n-void MachSpillCopyNode::emit(C2_MacroAssembler *masm, PhaseRegAlloc *ra_) const {\n-  implementation(masm, ra_, false, nullptr);\n-}\n-\n-uint MachSpillCopyNode::size(PhaseRegAlloc *ra_) const {\n-  return MachNode::size(ra_);\n-}\n-\n-\/\/=============================================================================\n-#ifndef PRODUCT\n-void BoxLockNode::format(PhaseRegAlloc* ra_, outputStream* st) const\n-{\n-  int offset = ra_->reg2offset(in_RegMask(0).find_first_elem());\n-  int reg = ra_->get_reg_first(this);\n-  st->print(\"leaq    %s, [rsp + #%d]\\t# box lock\",\n-            Matcher::regName[reg], offset);\n-}\n-#endif\n-\n-void BoxLockNode::emit(C2_MacroAssembler* masm, PhaseRegAlloc* ra_) const\n-{\n-  int offset = ra_->reg2offset(in_RegMask(0).find_first_elem());\n-  int reg = ra_->get_encode(this);\n-\n-  __ lea(as_Register(reg), Address(rsp, offset));\n-}\n-\n-uint BoxLockNode::size(PhaseRegAlloc *ra_) const\n-{\n-  int offset = ra_->reg2offset(in_RegMask(0).find_first_elem());\n-  if (ra_->get_encode(this) > 15) {\n-    return (offset < 0x80) ? 6 : 9; \/\/ REX2\n-  } else {\n-    return (offset < 0x80) ? 5 : 8; \/\/ REX\n-  }\n-}\n-\n-\/\/=============================================================================\n-#ifndef PRODUCT\n-void MachUEPNode::format(PhaseRegAlloc* ra_, outputStream* st) const\n-{\n-  if (UseCompressedClassPointers) {\n-    st->print_cr(\"movl    rscratch1, [j_rarg0 + oopDesc::klass_offset_in_bytes()]\\t# compressed klass\");\n-    st->print_cr(\"\\tcmpl    rscratch1, [rax + CompiledICData::speculated_klass_offset()]\\t # Inline cache check\");\n-  } else {\n-    st->print_cr(\"movq    rscratch1, [j_rarg0 + oopDesc::klass_offset_in_bytes()]\\t# compressed klass\");\n-    st->print_cr(\"\\tcmpq    rscratch1, [rax + CompiledICData::speculated_klass_offset()]\\t # Inline cache check\");\n-  }\n-  st->print_cr(\"\\tjne     SharedRuntime::_ic_miss_stub\");\n-}\n-#endif\n-\n-void MachUEPNode::emit(C2_MacroAssembler* masm, PhaseRegAlloc* ra_) const\n-{\n-  __ ic_check(InteriorEntryAlignment);\n-}\n-\n-uint MachUEPNode::size(PhaseRegAlloc* ra_) const\n-{\n-  return MachNode::size(ra_); \/\/ too many variables; just compute it\n-                              \/\/ the hard way\n-}\n-\n-\n-\/\/=============================================================================\n-\n-bool Matcher::supports_vector_calling_convention(void) {\n-  return EnableVectorSupport;\n-}\n-\n-OptoRegPair Matcher::vector_return_value(uint ideal_reg) {\n-  assert(EnableVectorSupport, \"sanity\");\n-  int lo = XMM0_num;\n-  int hi = XMM0b_num;\n-  if (ideal_reg == Op_VecX) hi = XMM0d_num;\n-  else if (ideal_reg == Op_VecY) hi = XMM0h_num;\n-  else if (ideal_reg == Op_VecZ) hi = XMM0p_num;\n-  return OptoRegPair(hi, lo);\n-}\n-\n-\/\/ Is this branch offset short enough that a short branch can be used?\n-\/\/\n-\/\/ NOTE: If the platform does not provide any short branch variants, then\n-\/\/       this method should return false for offset 0.\n-bool Matcher::is_short_branch_offset(int rule, int br_size, int offset) {\n-  \/\/ The passed offset is relative to address of the branch.\n-  \/\/ On 86 a branch displacement is calculated relative to address\n-  \/\/ of a next instruction.\n-  offset -= br_size;\n-\n-  \/\/ the short version of jmpConUCF2 contains multiple branches,\n-  \/\/ making the reach slightly less\n-  if (rule == jmpConUCF2_rule)\n-    return (-126 <= offset && offset <= 125);\n-  return (-128 <= offset && offset <= 127);\n-}\n-\n-\/\/ Return whether or not this register is ever used as an argument.\n-\/\/ This function is used on startup to build the trampoline stubs in\n-\/\/ generateOptoStub.  Registers not mentioned will be killed by the VM\n-\/\/ call in the trampoline, and arguments in those registers not be\n-\/\/ available to the callee.\n-bool Matcher::can_be_java_arg(int reg)\n-{\n-  return\n-    reg ==  RDI_num || reg == RDI_H_num ||\n-    reg ==  RSI_num || reg == RSI_H_num ||\n-    reg ==  RDX_num || reg == RDX_H_num ||\n-    reg ==  RCX_num || reg == RCX_H_num ||\n-    reg ==   R8_num || reg ==  R8_H_num ||\n-    reg ==   R9_num || reg ==  R9_H_num ||\n-    reg ==  R12_num || reg == R12_H_num ||\n-    reg == XMM0_num || reg == XMM0b_num ||\n-    reg == XMM1_num || reg == XMM1b_num ||\n-    reg == XMM2_num || reg == XMM2b_num ||\n-    reg == XMM3_num || reg == XMM3b_num ||\n-    reg == XMM4_num || reg == XMM4b_num ||\n-    reg == XMM5_num || reg == XMM5b_num ||\n-    reg == XMM6_num || reg == XMM6b_num ||\n-    reg == XMM7_num || reg == XMM7b_num;\n-}\n-\n-bool Matcher::is_spillable_arg(int reg)\n-{\n-  return can_be_java_arg(reg);\n-}\n-\n-uint Matcher::int_pressure_limit()\n-{\n-  return (INTPRESSURE == -1) ? _INT_REG_mask.size() : INTPRESSURE;\n-}\n-\n-uint Matcher::float_pressure_limit()\n-{\n-  \/\/ After experiment around with different values, the following default threshold\n-  \/\/ works best for LCM's register pressure scheduling on x64.\n-  uint dec_count  = VM_Version::supports_evex() ? 4 : 2;\n-  uint default_float_pressure_threshold = _FLOAT_REG_mask.size() - dec_count;\n-  return (FLOATPRESSURE == -1) ? default_float_pressure_threshold : FLOATPRESSURE;\n-}\n-\n-bool Matcher::use_asm_for_ldiv_by_con( jlong divisor ) {\n-  \/\/ In 64 bit mode a code which use multiply when\n-  \/\/ devisor is constant is faster than hardware\n-  \/\/ DIV instruction (it uses MulHiL).\n-  return false;\n-}\n-\n-\/\/ Register for DIVI projection of divmodI\n-const RegMask& Matcher::divI_proj_mask() {\n-  return INT_RAX_REG_mask();\n-}\n-\n-\/\/ Register for MODI projection of divmodI\n-const RegMask& Matcher::modI_proj_mask() {\n-  return INT_RDX_REG_mask();\n-}\n-\n-\/\/ Register for DIVL projection of divmodL\n-const RegMask& Matcher::divL_proj_mask() {\n-  return LONG_RAX_REG_mask();\n-}\n-\n-\/\/ Register for MODL projection of divmodL\n-const RegMask& Matcher::modL_proj_mask() {\n-  return LONG_RDX_REG_mask();\n-}\n-\n-%}\n-\n-\/\/----------ENCODING BLOCK-----------------------------------------------------\n-\/\/ This block specifies the encoding classes used by the compiler to\n-\/\/ output byte streams.  Encoding classes are parameterized macros\n-\/\/ used by Machine Instruction Nodes in order to generate the bit\n-\/\/ encoding of the instruction.  Operands specify their base encoding\n-\/\/ interface with the interface keyword.  There are currently\n-\/\/ supported four interfaces, REG_INTER, CONST_INTER, MEMORY_INTER, &\n-\/\/ COND_INTER.  REG_INTER causes an operand to generate a function\n-\/\/ which returns its register number when queried.  CONST_INTER causes\n-\/\/ an operand to generate a function which returns the value of the\n-\/\/ constant when queried.  MEMORY_INTER causes an operand to generate\n-\/\/ four functions which return the Base Register, the Index Register,\n-\/\/ the Scale Value, and the Offset Value of the operand when queried.\n-\/\/ COND_INTER causes an operand to generate six functions which return\n-\/\/ the encoding code (ie - encoding bits for the instruction)\n-\/\/ associated with each basic boolean condition for a conditional\n-\/\/ instruction.\n-\/\/\n-\/\/ Instructions specify two basic values for encoding.  Again, a\n-\/\/ function is available to check if the constant displacement is an\n-\/\/ oop. They use the ins_encode keyword to specify their encoding\n-\/\/ classes (which must be a sequence of enc_class names, and their\n-\/\/ parameters, specified in the encoding block), and they use the\n-\/\/ opcode keyword to specify, in order, their primary, secondary, and\n-\/\/ tertiary opcode.  Only the opcode sections which a particular\n-\/\/ instruction needs for encoding need to be specified.\n-encode %{\n-  enc_class cdql_enc(no_rax_rdx_RegI div)\n-  %{\n-    \/\/ Full implementation of Java idiv and irem; checks for\n-    \/\/ special case as described in JVM spec., p.243 & p.271.\n-    \/\/\n-    \/\/         normal case                           special case\n-    \/\/\n-    \/\/ input : rax: dividend                         min_int\n-    \/\/         reg: divisor                          -1\n-    \/\/\n-    \/\/ output: rax: quotient  (= rax idiv reg)       min_int\n-    \/\/         rdx: remainder (= rax irem reg)       0\n-    \/\/\n-    \/\/  Code sequnce:\n-    \/\/\n-    \/\/    0:   3d 00 00 00 80          cmp    $0x80000000,%eax\n-    \/\/    5:   75 07\/08                jne    e <normal>\n-    \/\/    7:   33 d2                   xor    %edx,%edx\n-    \/\/  [div >= 8 -> offset + 1]\n-    \/\/  [REX_B]\n-    \/\/    9:   83 f9 ff                cmp    $0xffffffffffffffff,$div\n-    \/\/    c:   74 03\/04                je     11 <done>\n-    \/\/ 000000000000000e <normal>:\n-    \/\/    e:   99                      cltd\n-    \/\/  [div >= 8 -> offset + 1]\n-    \/\/  [REX_B]\n-    \/\/    f:   f7 f9                   idiv   $div\n-    \/\/ 0000000000000011 <done>:\n-    Label normal;\n-    Label done;\n-\n-    \/\/ cmp    $0x80000000,%eax\n-    __ cmpl(as_Register(RAX_enc), 0x80000000);\n-\n-    \/\/ jne    e <normal>\n-    __ jccb(Assembler::notEqual, normal);\n-\n-    \/\/ xor    %edx,%edx\n-    __ xorl(as_Register(RDX_enc), as_Register(RDX_enc));\n-\n-    \/\/ cmp    $0xffffffffffffffff,%ecx\n-    __ cmpl($div$$Register, -1);\n-\n-    \/\/ je     11 <done>\n-    __ jccb(Assembler::equal, done);\n-\n-    \/\/ <normal>\n-    \/\/ cltd\n-    __ bind(normal);\n-    __ cdql();\n-\n-    \/\/ idivl\n-    \/\/ <done>\n-    __ idivl($div$$Register);\n-    __ bind(done);\n-  %}\n-\n-  enc_class cdqq_enc(no_rax_rdx_RegL div)\n-  %{\n-    \/\/ Full implementation of Java ldiv and lrem; checks for\n-    \/\/ special case as described in JVM spec., p.243 & p.271.\n-    \/\/\n-    \/\/         normal case                           special case\n-    \/\/\n-    \/\/ input : rax: dividend                         min_long\n-    \/\/         reg: divisor                          -1\n-    \/\/\n-    \/\/ output: rax: quotient  (= rax idiv reg)       min_long\n-    \/\/         rdx: remainder (= rax irem reg)       0\n-    \/\/\n-    \/\/  Code sequnce:\n-    \/\/\n-    \/\/    0:   48 ba 00 00 00 00 00    mov    $0x8000000000000000,%rdx\n-    \/\/    7:   00 00 80\n-    \/\/    a:   48 39 d0                cmp    %rdx,%rax\n-    \/\/    d:   75 08                   jne    17 <normal>\n-    \/\/    f:   33 d2                   xor    %edx,%edx\n-    \/\/   11:   48 83 f9 ff             cmp    $0xffffffffffffffff,$div\n-    \/\/   15:   74 05                   je     1c <done>\n-    \/\/ 0000000000000017 <normal>:\n-    \/\/   17:   48 99                   cqto\n-    \/\/   19:   48 f7 f9                idiv   $div\n-    \/\/ 000000000000001c <done>:\n-    Label normal;\n-    Label done;\n-\n-    \/\/ mov    $0x8000000000000000,%rdx\n-    __ mov64(as_Register(RDX_enc), 0x8000000000000000);\n-\n-    \/\/ cmp    %rdx,%rax\n-    __ cmpq(as_Register(RAX_enc), as_Register(RDX_enc));\n-\n-    \/\/ jne    17 <normal>\n-    __ jccb(Assembler::notEqual, normal);\n-\n-    \/\/ xor    %edx,%edx\n-    __ xorl(as_Register(RDX_enc), as_Register(RDX_enc));\n-\n-    \/\/ cmp    $0xffffffffffffffff,$div\n-    __ cmpq($div$$Register, -1);\n-\n-    \/\/ je     1e <done>\n-    __ jccb(Assembler::equal, done);\n-\n-    \/\/ <normal>\n-    \/\/ cqto\n-    __ bind(normal);\n-    __ cdqq();\n-\n-    \/\/ idivq (note: must be emitted by the user of this rule)\n-    \/\/ <done>\n-    __ idivq($div$$Register);\n-    __ bind(done);\n-  %}\n-\n-  enc_class clear_avx %{\n-    DEBUG_ONLY(int off0 = __ offset());\n-    if (generate_vzeroupper(Compile::current())) {\n-      \/\/ Clear upper bits of YMM registers to avoid AVX <-> SSE transition penalty\n-      \/\/ Clear upper bits of YMM registers when current compiled code uses\n-      \/\/ wide vectors to avoid AVX <-> SSE transition penalty during call.\n-      __ vzeroupper();\n-    }\n-    DEBUG_ONLY(int off1 = __ offset());\n-    assert(off1 - off0 == clear_avx_size(), \"correct size prediction\");\n-  %}\n-\n-  enc_class Java_To_Runtime(method meth) %{\n-    __ lea(r10, RuntimeAddress((address)$meth$$method));\n-    __ call(r10);\n-    __ post_call_nop();\n-  %}\n-\n-  enc_class Java_Static_Call(method meth)\n-  %{\n-    \/\/ JAVA STATIC CALL\n-    \/\/ CALL to fixup routine.  Fixup routine uses ScopeDesc info to\n-    \/\/ determine who we intended to call.\n-    if (!_method) {\n-      __ call(RuntimeAddress(CAST_FROM_FN_PTR(address, $meth$$method)));\n-    } else if (_method->intrinsic_id() == vmIntrinsicID::_ensureMaterializedForStackWalk) {\n-      \/\/ The NOP here is purely to ensure that eliding a call to\n-      \/\/ JVM_EnsureMaterializedForStackWalk doesn't change the code size.\n-      __ addr_nop_5();\n-      __ block_comment(\"call JVM_EnsureMaterializedForStackWalk (elided)\");\n-    } else {\n-      int method_index = resolved_method_index(masm);\n-      RelocationHolder rspec = _optimized_virtual ? opt_virtual_call_Relocation::spec(method_index)\n-                                                  : static_call_Relocation::spec(method_index);\n-      address mark = __ pc();\n-      int call_offset = __ offset();\n-      __ call(AddressLiteral(CAST_FROM_FN_PTR(address, $meth$$method), rspec));\n-      if (CodeBuffer::supports_shared_stubs() && _method->can_be_statically_bound()) {\n-        \/\/ Calls of the same statically bound method can share\n-        \/\/ a stub to the interpreter.\n-        __ code()->shared_stub_to_interp_for(_method, call_offset);\n-      } else {\n-        \/\/ Emit stubs for static call.\n-        address stub = CompiledDirectCall::emit_to_interp_stub(masm, mark);\n-        __ clear_inst_mark();\n-        if (stub == nullptr) {\n-          ciEnv::current()->record_failure(\"CodeCache is full\");\n-          return;\n-        }\n-      }\n-    }\n-    __ post_call_nop();\n-  %}\n-\n-  enc_class Java_Dynamic_Call(method meth) %{\n-    __ ic_call((address)$meth$$method, resolved_method_index(masm));\n-    __ post_call_nop();\n-  %}\n-\n-%}\n-\n-\n-\n-\/\/----------FRAME--------------------------------------------------------------\n-\/\/ Definition of frame structure and management information.\n-\/\/\n-\/\/  S T A C K   L A Y O U T    Allocators stack-slot number\n-\/\/                             |   (to get allocators register number\n-\/\/  G  Owned by    |        |  v    add OptoReg::stack0())\n-\/\/  r   CALLER     |        |\n-\/\/  o     |        +--------+      pad to even-align allocators stack-slot\n-\/\/  w     V        |  pad0  |        numbers; owned by CALLER\n-\/\/  t   -----------+--------+----> Matcher::_in_arg_limit, unaligned\n-\/\/  h     ^        |   in   |  5\n-\/\/        |        |  args  |  4   Holes in incoming args owned by SELF\n-\/\/  |     |        |        |  3\n-\/\/  |     |        +--------+\n-\/\/  V     |        | old out|      Empty on Intel, window on Sparc\n-\/\/        |    old |preserve|      Must be even aligned.\n-\/\/        |     SP-+--------+----> Matcher::_old_SP, even aligned\n-\/\/        |        |   in   |  3   area for Intel ret address\n-\/\/     Owned by    |preserve|      Empty on Sparc.\n-\/\/       SELF      +--------+\n-\/\/        |        |  pad2  |  2   pad to align old SP\n-\/\/        |        +--------+  1\n-\/\/        |        | locks  |  0\n-\/\/        |        +--------+----> OptoReg::stack0(), even aligned\n-\/\/        |        |  pad1  | 11   pad to align new SP\n-\/\/        |        +--------+\n-\/\/        |        |        | 10\n-\/\/        |        | spills |  9   spills\n-\/\/        V        |        |  8   (pad0 slot for callee)\n-\/\/      -----------+--------+----> Matcher::_out_arg_limit, unaligned\n-\/\/        ^        |  out   |  7\n-\/\/        |        |  args  |  6   Holes in outgoing args owned by CALLEE\n-\/\/     Owned by    +--------+\n-\/\/      CALLEE     | new out|  6   Empty on Intel, window on Sparc\n-\/\/        |    new |preserve|      Must be even-aligned.\n-\/\/        |     SP-+--------+----> Matcher::_new_SP, even aligned\n-\/\/        |        |        |\n-\/\/\n-\/\/ Note 1: Only region 8-11 is determined by the allocator.  Region 0-5 is\n-\/\/         known from SELF's arguments and the Java calling convention.\n-\/\/         Region 6-7 is determined per call site.\n-\/\/ Note 2: If the calling convention leaves holes in the incoming argument\n-\/\/         area, those holes are owned by SELF.  Holes in the outgoing area\n-\/\/         are owned by the CALLEE.  Holes should not be necessary in the\n-\/\/         incoming area, as the Java calling convention is completely under\n-\/\/         the control of the AD file.  Doubles can be sorted and packed to\n-\/\/         avoid holes.  Holes in the outgoing arguments may be necessary for\n-\/\/         varargs C calling conventions.\n-\/\/ Note 3: Region 0-3 is even aligned, with pad2 as needed.  Region 3-5 is\n-\/\/         even aligned with pad0 as needed.\n-\/\/         Region 6 is even aligned.  Region 6-7 is NOT even aligned;\n-\/\/         region 6-11 is even aligned; it may be padded out more so that\n-\/\/         the region from SP to FP meets the minimum stack alignment.\n-\/\/ Note 4: For I2C adapters, the incoming FP may not meet the minimum stack\n-\/\/         alignment.  Region 11, pad1, may be dynamically extended so that\n-\/\/         SP meets the minimum alignment.\n-\n-frame\n-%{\n-  \/\/ These three registers define part of the calling convention\n-  \/\/ between compiled code and the interpreter.\n-  inline_cache_reg(RAX);                \/\/ Inline Cache Register\n-\n-  \/\/ Optional: name the operand used by cisc-spilling to access\n-  \/\/ [stack_pointer + offset]\n-  cisc_spilling_operand_name(indOffset32);\n-\n-  \/\/ Number of stack slots consumed by locking an object\n-  sync_stack_slots(2);\n-\n-  \/\/ Compiled code's Frame Pointer\n-  frame_pointer(RSP);\n-\n-  \/\/ Interpreter stores its frame pointer in a register which is\n-  \/\/ stored to the stack by I2CAdaptors.\n-  \/\/ I2CAdaptors convert from interpreted java to compiled java.\n-  interpreter_frame_pointer(RBP);\n-\n-  \/\/ Stack alignment requirement\n-  stack_alignment(StackAlignmentInBytes); \/\/ Alignment size in bytes (128-bit -> 16 bytes)\n-\n-  \/\/ Number of outgoing stack slots killed above the out_preserve_stack_slots\n-  \/\/ for calls to C.  Supports the var-args backing area for register parms.\n-  varargs_C_out_slots_killed(frame::arg_reg_save_area_bytes\/BytesPerInt);\n-\n-  \/\/ The after-PROLOG location of the return address.  Location of\n-  \/\/ return address specifies a type (REG or STACK) and a number\n-  \/\/ representing the register number (i.e. - use a register name) or\n-  \/\/ stack slot.\n-  \/\/ Ret Addr is on stack in slot 0 if no locks or verification or alignment.\n-  \/\/ Otherwise, it is above the locks and verification slot and alignment word\n-  return_addr(STACK - 2 +\n-              align_up((Compile::current()->in_preserve_stack_slots() +\n-                        Compile::current()->fixed_slots()),\n-                       stack_alignment_in_slots()));\n-\n-  \/\/ Location of compiled Java return values.  Same as C for now.\n-  return_value\n-  %{\n-    assert(ideal_reg >= Op_RegI && ideal_reg <= Op_RegL,\n-           \"only return normal values\");\n-\n-    static const int lo[Op_RegL + 1] = {\n-      0,\n-      0,\n-      RAX_num,  \/\/ Op_RegN\n-      RAX_num,  \/\/ Op_RegI\n-      RAX_num,  \/\/ Op_RegP\n-      XMM0_num, \/\/ Op_RegF\n-      XMM0_num, \/\/ Op_RegD\n-      RAX_num   \/\/ Op_RegL\n-    };\n-    static const int hi[Op_RegL + 1] = {\n-      0,\n-      0,\n-      OptoReg::Bad, \/\/ Op_RegN\n-      OptoReg::Bad, \/\/ Op_RegI\n-      RAX_H_num,    \/\/ Op_RegP\n-      OptoReg::Bad, \/\/ Op_RegF\n-      XMM0b_num,    \/\/ Op_RegD\n-      RAX_H_num     \/\/ Op_RegL\n-    };\n-    \/\/ Excluded flags and vector registers.\n-    assert(ARRAY_SIZE(hi) == _last_machine_leaf - 8, \"missing type\");\n-    return OptoRegPair(hi[ideal_reg], lo[ideal_reg]);\n-  %}\n-%}\n-\n-\/\/----------ATTRIBUTES---------------------------------------------------------\n-\/\/----------Operand Attributes-------------------------------------------------\n-op_attrib op_cost(0);        \/\/ Required cost attribute\n-\n-\/\/----------Instruction Attributes---------------------------------------------\n-ins_attrib ins_cost(100);       \/\/ Required cost attribute\n-ins_attrib ins_size(8);         \/\/ Required size attribute (in bits)\n-ins_attrib ins_short_branch(0); \/\/ Required flag: is this instruction\n-                                \/\/ a non-matching short branch variant\n-                                \/\/ of some long branch?\n-ins_attrib ins_alignment(1);    \/\/ Required alignment attribute (must\n-                                \/\/ be a power of 2) specifies the\n-                                \/\/ alignment that some part of the\n-                                \/\/ instruction (not necessarily the\n-                                \/\/ start) requires.  If > 1, a\n-                                \/\/ compute_padding() function must be\n-                                \/\/ provided for the instruction\n-\n-\/\/ Whether this node is expanded during code emission into a sequence of\n-\/\/ instructions and the first instruction can perform an implicit null check.\n-ins_attrib ins_is_late_expanded_null_check_candidate(false);\n-\n-\/\/----------OPERANDS-----------------------------------------------------------\n-\/\/ Operand definitions must precede instruction definitions for correct parsing\n-\/\/ in the ADLC because operands constitute user defined types which are used in\n-\/\/ instruction definitions.\n-\n-\/\/----------Simple Operands----------------------------------------------------\n-\/\/ Immediate Operands\n-\/\/ Integer Immediate\n-operand immI()\n-%{\n-  match(ConI);\n-\n-  op_cost(10);\n-  format %{ %}\n-  interface(CONST_INTER);\n-%}\n-\n-\/\/ Constant for test vs zero\n-operand immI_0()\n-%{\n-  predicate(n->get_int() == 0);\n-  match(ConI);\n-\n-  op_cost(0);\n-  format %{ %}\n-  interface(CONST_INTER);\n-%}\n-\n-\/\/ Constant for increment\n-operand immI_1()\n-%{\n-  predicate(n->get_int() == 1);\n-  match(ConI);\n-\n-  op_cost(0);\n-  format %{ %}\n-  interface(CONST_INTER);\n-%}\n-\n-\/\/ Constant for decrement\n-operand immI_M1()\n-%{\n-  predicate(n->get_int() == -1);\n-  match(ConI);\n-\n-  op_cost(0);\n-  format %{ %}\n-  interface(CONST_INTER);\n-%}\n-\n-operand immI_2()\n-%{\n-  predicate(n->get_int() == 2);\n-  match(ConI);\n-\n-  op_cost(0);\n-  format %{ %}\n-  interface(CONST_INTER);\n-%}\n-\n-operand immI_4()\n-%{\n-  predicate(n->get_int() == 4);\n-  match(ConI);\n-\n-  op_cost(0);\n-  format %{ %}\n-  interface(CONST_INTER);\n-%}\n-\n-operand immI_8()\n-%{\n-  predicate(n->get_int() == 8);\n-  match(ConI);\n-\n-  op_cost(0);\n-  format %{ %}\n-  interface(CONST_INTER);\n-%}\n-\n-\/\/ Valid scale values for addressing modes\n-operand immI2()\n-%{\n-  predicate(0 <= n->get_int() && (n->get_int() <= 3));\n-  match(ConI);\n-\n-  format %{ %}\n-  interface(CONST_INTER);\n-%}\n-\n-operand immU7()\n-%{\n-  predicate((0 <= n->get_int()) && (n->get_int() <= 0x7F));\n-  match(ConI);\n-\n-  op_cost(5);\n-  format %{ %}\n-  interface(CONST_INTER);\n-%}\n-\n-operand immI8()\n-%{\n-  predicate((-0x80 <= n->get_int()) && (n->get_int() < 0x80));\n-  match(ConI);\n-\n-  op_cost(5);\n-  format %{ %}\n-  interface(CONST_INTER);\n-%}\n-\n-operand immU8()\n-%{\n-  predicate((0 <= n->get_int()) && (n->get_int() <= 255));\n-  match(ConI);\n-\n-  op_cost(5);\n-  format %{ %}\n-  interface(CONST_INTER);\n-%}\n-\n-operand immI16()\n-%{\n-  predicate((-32768 <= n->get_int()) && (n->get_int() <= 32767));\n-  match(ConI);\n-\n-  op_cost(10);\n-  format %{ %}\n-  interface(CONST_INTER);\n-%}\n-\n-\/\/ Int Immediate non-negative\n-operand immU31()\n-%{\n-  predicate(n->get_int() >= 0);\n-  match(ConI);\n-\n-  op_cost(0);\n-  format %{ %}\n-  interface(CONST_INTER);\n-%}\n-\n-\/\/ Pointer Immediate\n-operand immP()\n-%{\n-  match(ConP);\n-\n-  op_cost(10);\n-  format %{ %}\n-  interface(CONST_INTER);\n-%}\n-\n-\/\/ Null Pointer Immediate\n-operand immP0()\n-%{\n-  predicate(n->get_ptr() == 0);\n-  match(ConP);\n-\n-  op_cost(5);\n-  format %{ %}\n-  interface(CONST_INTER);\n-%}\n-\n-\/\/ Pointer Immediate\n-operand immN() %{\n-  match(ConN);\n-\n-  op_cost(10);\n-  format %{ %}\n-  interface(CONST_INTER);\n-%}\n-\n-operand immNKlass() %{\n-  match(ConNKlass);\n-\n-  op_cost(10);\n-  format %{ %}\n-  interface(CONST_INTER);\n-%}\n-\n-\/\/ Null Pointer Immediate\n-operand immN0() %{\n-  predicate(n->get_narrowcon() == 0);\n-  match(ConN);\n-\n-  op_cost(5);\n-  format %{ %}\n-  interface(CONST_INTER);\n-%}\n-\n-operand immP31()\n-%{\n-  predicate(n->as_Type()->type()->reloc() == relocInfo::none\n-            && (n->get_ptr() >> 31) == 0);\n-  match(ConP);\n-\n-  op_cost(5);\n-  format %{ %}\n-  interface(CONST_INTER);\n-%}\n-\n-\n-\/\/ Long Immediate\n-operand immL()\n-%{\n-  match(ConL);\n-\n-  op_cost(20);\n-  format %{ %}\n-  interface(CONST_INTER);\n-%}\n-\n-\/\/ Long Immediate 8-bit\n-operand immL8()\n-%{\n-  predicate(-0x80L <= n->get_long() && n->get_long() < 0x80L);\n-  match(ConL);\n-\n-  op_cost(5);\n-  format %{ %}\n-  interface(CONST_INTER);\n-%}\n-\n-\/\/ Long Immediate 32-bit unsigned\n-operand immUL32()\n-%{\n-  predicate(n->get_long() == (unsigned int) (n->get_long()));\n-  match(ConL);\n-\n-  op_cost(10);\n-  format %{ %}\n-  interface(CONST_INTER);\n-%}\n-\n-\/\/ Long Immediate 32-bit signed\n-operand immL32()\n-%{\n-  predicate(n->get_long() == (int) (n->get_long()));\n-  match(ConL);\n-\n-  op_cost(15);\n-  format %{ %}\n-  interface(CONST_INTER);\n-%}\n-\n-operand immL_Pow2()\n-%{\n-  predicate(is_power_of_2((julong)n->get_long()));\n-  match(ConL);\n-\n-  op_cost(15);\n-  format %{ %}\n-  interface(CONST_INTER);\n-%}\n-\n-operand immL_NotPow2()\n-%{\n-  predicate(is_power_of_2((julong)~n->get_long()));\n-  match(ConL);\n-\n-  op_cost(15);\n-  format %{ %}\n-  interface(CONST_INTER);\n-%}\n-\n-\/\/ Long Immediate zero\n-operand immL0()\n-%{\n-  predicate(n->get_long() == 0L);\n-  match(ConL);\n-\n-  op_cost(10);\n-  format %{ %}\n-  interface(CONST_INTER);\n-%}\n-\n-\/\/ Constant for increment\n-operand immL1()\n-%{\n-  predicate(n->get_long() == 1);\n-  match(ConL);\n-\n-  format %{ %}\n-  interface(CONST_INTER);\n-%}\n-\n-\/\/ Constant for decrement\n-operand immL_M1()\n-%{\n-  predicate(n->get_long() == -1);\n-  match(ConL);\n-\n-  format %{ %}\n-  interface(CONST_INTER);\n-%}\n-\n-\/\/ Long Immediate: low 32-bit mask\n-operand immL_32bits()\n-%{\n-  predicate(n->get_long() == 0xFFFFFFFFL);\n-  match(ConL);\n-  op_cost(20);\n-\n-  format %{ %}\n-  interface(CONST_INTER);\n-%}\n-\n-\/\/ Int Immediate: 2^n-1, positive\n-operand immI_Pow2M1()\n-%{\n-  predicate((n->get_int() > 0)\n-            && is_power_of_2((juint)n->get_int() + 1));\n-  match(ConI);\n-\n-  op_cost(20);\n-  format %{ %}\n-  interface(CONST_INTER);\n-%}\n-\n-\/\/ Float Immediate zero\n-operand immF0()\n-%{\n-  predicate(jint_cast(n->getf()) == 0);\n-  match(ConF);\n-\n-  op_cost(5);\n-  format %{ %}\n-  interface(CONST_INTER);\n-%}\n-\n-\/\/ Float Immediate\n-operand immF()\n-%{\n-  match(ConF);\n-\n-  op_cost(15);\n-  format %{ %}\n-  interface(CONST_INTER);\n-%}\n-\n-\/\/ Half Float Immediate\n-operand immH()\n-%{\n-  match(ConH);\n-\n-  op_cost(15);\n-  format %{ %}\n-  interface(CONST_INTER);\n-%}\n-\n-\/\/ Double Immediate zero\n-operand immD0()\n-%{\n-  predicate(jlong_cast(n->getd()) == 0);\n-  match(ConD);\n-\n-  op_cost(5);\n-  format %{ %}\n-  interface(CONST_INTER);\n-%}\n-\n-\/\/ Double Immediate\n-operand immD()\n-%{\n-  match(ConD);\n-\n-  op_cost(15);\n-  format %{ %}\n-  interface(CONST_INTER);\n-%}\n-\n-\/\/ Immediates for special shifts (sign extend)\n-\n-\/\/ Constants for increment\n-operand immI_16()\n-%{\n-  predicate(n->get_int() == 16);\n-  match(ConI);\n-\n-  format %{ %}\n-  interface(CONST_INTER);\n-%}\n-\n-operand immI_24()\n-%{\n-  predicate(n->get_int() == 24);\n-  match(ConI);\n-\n-  format %{ %}\n-  interface(CONST_INTER);\n-%}\n-\n-\/\/ Constant for byte-wide masking\n-operand immI_255()\n-%{\n-  predicate(n->get_int() == 255);\n-  match(ConI);\n-\n-  format %{ %}\n-  interface(CONST_INTER);\n-%}\n-\n-\/\/ Constant for short-wide masking\n-operand immI_65535()\n-%{\n-  predicate(n->get_int() == 65535);\n-  match(ConI);\n-\n-  format %{ %}\n-  interface(CONST_INTER);\n-%}\n-\n-\/\/ Constant for byte-wide masking\n-operand immL_255()\n-%{\n-  predicate(n->get_long() == 255);\n-  match(ConL);\n-\n-  format %{ %}\n-  interface(CONST_INTER);\n-%}\n-\n-\/\/ Constant for short-wide masking\n-operand immL_65535()\n-%{\n-  predicate(n->get_long() == 65535);\n-  match(ConL);\n-\n-  format %{ %}\n-  interface(CONST_INTER);\n-%}\n-\n-operand kReg()\n-%{\n-  constraint(ALLOC_IN_RC(vectmask_reg));\n-  match(RegVectMask);\n-  format %{%}\n-  interface(REG_INTER);\n-%}\n-\n-\/\/ Register Operands\n-\/\/ Integer Register\n-operand rRegI()\n-%{\n-  constraint(ALLOC_IN_RC(int_reg));\n-  match(RegI);\n-\n-  match(rax_RegI);\n-  match(rbx_RegI);\n-  match(rcx_RegI);\n-  match(rdx_RegI);\n-  match(rdi_RegI);\n-\n-  format %{ %}\n-  interface(REG_INTER);\n-%}\n-\n-\/\/ Special Registers\n-operand rax_RegI()\n-%{\n-  constraint(ALLOC_IN_RC(int_rax_reg));\n-  match(RegI);\n-  match(rRegI);\n-\n-  format %{ \"RAX\" %}\n-  interface(REG_INTER);\n-%}\n-\n-\/\/ Special Registers\n-operand rbx_RegI()\n-%{\n-  constraint(ALLOC_IN_RC(int_rbx_reg));\n-  match(RegI);\n-  match(rRegI);\n-\n-  format %{ \"RBX\" %}\n-  interface(REG_INTER);\n-%}\n-\n-operand rcx_RegI()\n-%{\n-  constraint(ALLOC_IN_RC(int_rcx_reg));\n-  match(RegI);\n-  match(rRegI);\n-\n-  format %{ \"RCX\" %}\n-  interface(REG_INTER);\n-%}\n-\n-operand rdx_RegI()\n-%{\n-  constraint(ALLOC_IN_RC(int_rdx_reg));\n-  match(RegI);\n-  match(rRegI);\n-\n-  format %{ \"RDX\" %}\n-  interface(REG_INTER);\n-%}\n-\n-operand rdi_RegI()\n-%{\n-  constraint(ALLOC_IN_RC(int_rdi_reg));\n-  match(RegI);\n-  match(rRegI);\n-\n-  format %{ \"RDI\" %}\n-  interface(REG_INTER);\n-%}\n-\n-operand no_rax_rdx_RegI()\n-%{\n-  constraint(ALLOC_IN_RC(int_no_rax_rdx_reg));\n-  match(RegI);\n-  match(rbx_RegI);\n-  match(rcx_RegI);\n-  match(rdi_RegI);\n-\n-  format %{ %}\n-  interface(REG_INTER);\n-%}\n-\n-operand no_rbp_r13_RegI()\n-%{\n-  constraint(ALLOC_IN_RC(int_no_rbp_r13_reg));\n-  match(RegI);\n-  match(rRegI);\n-  match(rax_RegI);\n-  match(rbx_RegI);\n-  match(rcx_RegI);\n-  match(rdx_RegI);\n-  match(rdi_RegI);\n-\n-  format %{ %}\n-  interface(REG_INTER);\n-%}\n-\n-\/\/ Pointer Register\n-operand any_RegP()\n-%{\n-  constraint(ALLOC_IN_RC(any_reg));\n-  match(RegP);\n-  match(rax_RegP);\n-  match(rbx_RegP);\n-  match(rdi_RegP);\n-  match(rsi_RegP);\n-  match(rbp_RegP);\n-  match(r15_RegP);\n-  match(rRegP);\n-\n-  format %{ %}\n-  interface(REG_INTER);\n-%}\n-\n-operand rRegP()\n-%{\n-  constraint(ALLOC_IN_RC(ptr_reg));\n-  match(RegP);\n-  match(rax_RegP);\n-  match(rbx_RegP);\n-  match(rdi_RegP);\n-  match(rsi_RegP);\n-  match(rbp_RegP);  \/\/ See Q&A below about\n-  match(r15_RegP);  \/\/ r15_RegP and rbp_RegP.\n-\n-  format %{ %}\n-  interface(REG_INTER);\n-%}\n-\n-operand rRegN() %{\n-  constraint(ALLOC_IN_RC(int_reg));\n-  match(RegN);\n-\n-  format %{ %}\n-  interface(REG_INTER);\n-%}\n-\n-\/\/ Question: Why is r15_RegP (the read-only TLS register) a match for rRegP?\n-\/\/ Answer: Operand match rules govern the DFA as it processes instruction inputs.\n-\/\/ It's fine for an instruction input that expects rRegP to match a r15_RegP.\n-\/\/ The output of an instruction is controlled by the allocator, which respects\n-\/\/ register class masks, not match rules.  Unless an instruction mentions\n-\/\/ r15_RegP or any_RegP explicitly as its output, r15 will not be considered\n-\/\/ by the allocator as an input.\n-\/\/ The same logic applies to rbp_RegP being a match for rRegP: If PreserveFramePointer==true,\n-\/\/ the RBP is used as a proper frame pointer and is not included in ptr_reg. As a\n-\/\/ result, RBP is not included in the output of the instruction either.\n-\n-\/\/ This operand is not allowed to use RBP even if\n-\/\/ RBP is not used to hold the frame pointer.\n-operand no_rbp_RegP()\n-%{\n-  constraint(ALLOC_IN_RC(ptr_reg_no_rbp));\n-  match(RegP);\n-  match(rbx_RegP);\n-  match(rsi_RegP);\n-  match(rdi_RegP);\n-\n-  format %{ %}\n-  interface(REG_INTER);\n-%}\n-\n-\/\/ Special Registers\n-\/\/ Return a pointer value\n-operand rax_RegP()\n-%{\n-  constraint(ALLOC_IN_RC(ptr_rax_reg));\n-  match(RegP);\n-  match(rRegP);\n-\n-  format %{ %}\n-  interface(REG_INTER);\n-%}\n-\n-\/\/ Special Registers\n-\/\/ Return a compressed pointer value\n-operand rax_RegN()\n-%{\n-  constraint(ALLOC_IN_RC(int_rax_reg));\n-  match(RegN);\n-  match(rRegN);\n-\n-  format %{ %}\n-  interface(REG_INTER);\n-%}\n-\n-\/\/ Used in AtomicAdd\n-operand rbx_RegP()\n-%{\n-  constraint(ALLOC_IN_RC(ptr_rbx_reg));\n-  match(RegP);\n-  match(rRegP);\n-\n-  format %{ %}\n-  interface(REG_INTER);\n-%}\n-\n-operand rsi_RegP()\n-%{\n-  constraint(ALLOC_IN_RC(ptr_rsi_reg));\n-  match(RegP);\n-  match(rRegP);\n-\n-  format %{ %}\n-  interface(REG_INTER);\n-%}\n-\n-operand rbp_RegP()\n-%{\n-  constraint(ALLOC_IN_RC(ptr_rbp_reg));\n-  match(RegP);\n-  match(rRegP);\n-\n-  format %{ %}\n-  interface(REG_INTER);\n-%}\n-\n-\/\/ Used in rep stosq\n-operand rdi_RegP()\n-%{\n-  constraint(ALLOC_IN_RC(ptr_rdi_reg));\n-  match(RegP);\n-  match(rRegP);\n-\n-  format %{ %}\n-  interface(REG_INTER);\n-%}\n-\n-operand r15_RegP()\n-%{\n-  constraint(ALLOC_IN_RC(ptr_r15_reg));\n-  match(RegP);\n-  match(rRegP);\n-\n-  format %{ %}\n-  interface(REG_INTER);\n-%}\n-\n-operand rRegL()\n-%{\n-  constraint(ALLOC_IN_RC(long_reg));\n-  match(RegL);\n-  match(rax_RegL);\n-  match(rdx_RegL);\n-\n-  format %{ %}\n-  interface(REG_INTER);\n-%}\n-\n-\/\/ Special Registers\n-operand no_rax_rdx_RegL()\n-%{\n-  constraint(ALLOC_IN_RC(long_no_rax_rdx_reg));\n-  match(RegL);\n-  match(rRegL);\n-\n-  format %{ %}\n-  interface(REG_INTER);\n-%}\n-\n-operand rax_RegL()\n-%{\n-  constraint(ALLOC_IN_RC(long_rax_reg));\n-  match(RegL);\n-  match(rRegL);\n-\n-  format %{ \"RAX\" %}\n-  interface(REG_INTER);\n-%}\n-\n-operand rcx_RegL()\n-%{\n-  constraint(ALLOC_IN_RC(long_rcx_reg));\n-  match(RegL);\n-  match(rRegL);\n-\n-  format %{ %}\n-  interface(REG_INTER);\n-%}\n-\n-operand rdx_RegL()\n-%{\n-  constraint(ALLOC_IN_RC(long_rdx_reg));\n-  match(RegL);\n-  match(rRegL);\n-\n-  format %{ %}\n-  interface(REG_INTER);\n-%}\n-\n-operand r11_RegL()\n-%{\n-  constraint(ALLOC_IN_RC(long_r11_reg));\n-  match(RegL);\n-  match(rRegL);\n-\n-  format %{ %}\n-  interface(REG_INTER);\n-%}\n-\n-operand no_rbp_r13_RegL()\n-%{\n-  constraint(ALLOC_IN_RC(long_no_rbp_r13_reg));\n-  match(RegL);\n-  match(rRegL);\n-  match(rax_RegL);\n-  match(rcx_RegL);\n-  match(rdx_RegL);\n-\n-  format %{ %}\n-  interface(REG_INTER);\n-%}\n-\n-\/\/ Flags register, used as output of compare instructions\n-operand rFlagsReg()\n-%{\n-  constraint(ALLOC_IN_RC(int_flags));\n-  match(RegFlags);\n-\n-  format %{ \"RFLAGS\" %}\n-  interface(REG_INTER);\n-%}\n-\n-\/\/ Flags register, used as output of FLOATING POINT compare instructions\n-operand rFlagsRegU()\n-%{\n-  constraint(ALLOC_IN_RC(int_flags));\n-  match(RegFlags);\n-\n-  format %{ \"RFLAGS_U\" %}\n-  interface(REG_INTER);\n-%}\n-\n-operand rFlagsRegUCF() %{\n-  constraint(ALLOC_IN_RC(int_flags));\n-  match(RegFlags);\n-  predicate(false);\n-\n-  format %{ \"RFLAGS_U_CF\" %}\n-  interface(REG_INTER);\n-%}\n-\n-\/\/ Float register operands\n-operand regF() %{\n-   constraint(ALLOC_IN_RC(float_reg));\n-   match(RegF);\n-\n-   format %{ %}\n-   interface(REG_INTER);\n-%}\n-\n-\/\/ Float register operands\n-operand legRegF() %{\n-   constraint(ALLOC_IN_RC(float_reg_legacy));\n-   match(RegF);\n-\n-   format %{ %}\n-   interface(REG_INTER);\n-%}\n-\n-\/\/ Float register operands\n-operand vlRegF() %{\n-   constraint(ALLOC_IN_RC(float_reg_vl));\n-   match(RegF);\n-\n-   format %{ %}\n-   interface(REG_INTER);\n-%}\n-\n-\/\/ Double register operands\n-operand regD() %{\n-   constraint(ALLOC_IN_RC(double_reg));\n-   match(RegD);\n-\n-   format %{ %}\n-   interface(REG_INTER);\n-%}\n-\n-\/\/ Double register operands\n-operand legRegD() %{\n-   constraint(ALLOC_IN_RC(double_reg_legacy));\n-   match(RegD);\n-\n-   format %{ %}\n-   interface(REG_INTER);\n-%}\n-\n-\/\/ Double register operands\n-operand vlRegD() %{\n-   constraint(ALLOC_IN_RC(double_reg_vl));\n-   match(RegD);\n-\n-   format %{ %}\n-   interface(REG_INTER);\n-%}\n-\n-\/\/----------Memory Operands----------------------------------------------------\n-\/\/ Direct Memory Operand\n-\/\/ operand direct(immP addr)\n-\/\/ %{\n-\/\/   match(addr);\n-\n-\/\/   format %{ \"[$addr]\" %}\n-\/\/   interface(MEMORY_INTER) %{\n-\/\/     base(0xFFFFFFFF);\n-\/\/     index(0x4);\n-\/\/     scale(0x0);\n-\/\/     disp($addr);\n-\/\/   %}\n-\/\/ %}\n-\n-\/\/ Indirect Memory Operand\n-operand indirect(any_RegP reg)\n-%{\n-  constraint(ALLOC_IN_RC(ptr_reg));\n-  match(reg);\n-\n-  format %{ \"[$reg]\" %}\n-  interface(MEMORY_INTER) %{\n-    base($reg);\n-    index(0x4);\n-    scale(0x0);\n-    disp(0x0);\n-  %}\n-%}\n-\n-\/\/ Indirect Memory Plus Short Offset Operand\n-operand indOffset8(any_RegP reg, immL8 off)\n-%{\n-  constraint(ALLOC_IN_RC(ptr_reg));\n-  match(AddP reg off);\n-\n-  format %{ \"[$reg + $off (8-bit)]\" %}\n-  interface(MEMORY_INTER) %{\n-    base($reg);\n-    index(0x4);\n-    scale(0x0);\n-    disp($off);\n-  %}\n-%}\n-\n-\/\/ Indirect Memory Plus Long Offset Operand\n-operand indOffset32(any_RegP reg, immL32 off)\n-%{\n-  constraint(ALLOC_IN_RC(ptr_reg));\n-  match(AddP reg off);\n-\n-  format %{ \"[$reg + $off (32-bit)]\" %}\n-  interface(MEMORY_INTER) %{\n-    base($reg);\n-    index(0x4);\n-    scale(0x0);\n-    disp($off);\n-  %}\n-%}\n-\n-\/\/ Indirect Memory Plus Index Register Plus Offset Operand\n-operand indIndexOffset(any_RegP reg, rRegL lreg, immL32 off)\n-%{\n-  constraint(ALLOC_IN_RC(ptr_reg));\n-  match(AddP (AddP reg lreg) off);\n-\n-  op_cost(10);\n-  format %{\"[$reg + $off + $lreg]\" %}\n-  interface(MEMORY_INTER) %{\n-    base($reg);\n-    index($lreg);\n-    scale(0x0);\n-    disp($off);\n-  %}\n-%}\n-\n-\/\/ Indirect Memory Plus Index Register Plus Offset Operand\n-operand indIndex(any_RegP reg, rRegL lreg)\n-%{\n-  constraint(ALLOC_IN_RC(ptr_reg));\n-  match(AddP reg lreg);\n-\n-  op_cost(10);\n-  format %{\"[$reg + $lreg]\" %}\n-  interface(MEMORY_INTER) %{\n-    base($reg);\n-    index($lreg);\n-    scale(0x0);\n-    disp(0x0);\n-  %}\n-%}\n-\n-\/\/ Indirect Memory Times Scale Plus Index Register\n-operand indIndexScale(any_RegP reg, rRegL lreg, immI2 scale)\n-%{\n-  constraint(ALLOC_IN_RC(ptr_reg));\n-  match(AddP reg (LShiftL lreg scale));\n-\n-  op_cost(10);\n-  format %{\"[$reg + $lreg << $scale]\" %}\n-  interface(MEMORY_INTER) %{\n-    base($reg);\n-    index($lreg);\n-    scale($scale);\n-    disp(0x0);\n-  %}\n-%}\n-\n-operand indPosIndexScale(any_RegP reg, rRegI idx, immI2 scale)\n-%{\n-  constraint(ALLOC_IN_RC(ptr_reg));\n-  predicate(n->in(3)->in(1)->as_Type()->type()->is_long()->_lo >= 0);\n-  match(AddP reg (LShiftL (ConvI2L idx) scale));\n-\n-  op_cost(10);\n-  format %{\"[$reg + pos $idx << $scale]\" %}\n-  interface(MEMORY_INTER) %{\n-    base($reg);\n-    index($idx);\n-    scale($scale);\n-    disp(0x0);\n-  %}\n-%}\n-\n-\/\/ Indirect Memory Times Scale Plus Index Register Plus Offset Operand\n-operand indIndexScaleOffset(any_RegP reg, immL32 off, rRegL lreg, immI2 scale)\n-%{\n-  constraint(ALLOC_IN_RC(ptr_reg));\n-  match(AddP (AddP reg (LShiftL lreg scale)) off);\n-\n-  op_cost(10);\n-  format %{\"[$reg + $off + $lreg << $scale]\" %}\n-  interface(MEMORY_INTER) %{\n-    base($reg);\n-    index($lreg);\n-    scale($scale);\n-    disp($off);\n-  %}\n-%}\n-\n-\/\/ Indirect Memory Plus Positive Index Register Plus Offset Operand\n-operand indPosIndexOffset(any_RegP reg, immL32 off, rRegI idx)\n-%{\n-  constraint(ALLOC_IN_RC(ptr_reg));\n-  predicate(n->in(2)->in(3)->as_Type()->type()->is_long()->_lo >= 0);\n-  match(AddP (AddP reg (ConvI2L idx)) off);\n-\n-  op_cost(10);\n-  format %{\"[$reg + $off + $idx]\" %}\n-  interface(MEMORY_INTER) %{\n-    base($reg);\n-    index($idx);\n-    scale(0x0);\n-    disp($off);\n-  %}\n-%}\n-\n-\/\/ Indirect Memory Times Scale Plus Positive Index Register Plus Offset Operand\n-operand indPosIndexScaleOffset(any_RegP reg, immL32 off, rRegI idx, immI2 scale)\n-%{\n-  constraint(ALLOC_IN_RC(ptr_reg));\n-  predicate(n->in(2)->in(3)->in(1)->as_Type()->type()->is_long()->_lo >= 0);\n-  match(AddP (AddP reg (LShiftL (ConvI2L idx) scale)) off);\n-\n-  op_cost(10);\n-  format %{\"[$reg + $off + $idx << $scale]\" %}\n-  interface(MEMORY_INTER) %{\n-    base($reg);\n-    index($idx);\n-    scale($scale);\n-    disp($off);\n-  %}\n-%}\n-\n-\/\/ Indirect Narrow Oop Plus Offset Operand\n-\/\/ Note: x86 architecture doesn't support \"scale * index + offset\" without a base\n-\/\/ we can't free r12 even with CompressedOops::base() == nullptr.\n-operand indCompressedOopOffset(rRegN reg, immL32 off) %{\n-  predicate(UseCompressedOops && (CompressedOops::shift() == Address::times_8));\n-  constraint(ALLOC_IN_RC(ptr_reg));\n-  match(AddP (DecodeN reg) off);\n-\n-  op_cost(10);\n-  format %{\"[R12 + $reg << 3 + $off] (compressed oop addressing)\" %}\n-  interface(MEMORY_INTER) %{\n-    base(0xc); \/\/ R12\n-    index($reg);\n-    scale(0x3);\n-    disp($off);\n-  %}\n-%}\n-\n-\/\/ Indirect Memory Operand\n-operand indirectNarrow(rRegN reg)\n-%{\n-  predicate(CompressedOops::shift() == 0);\n-  constraint(ALLOC_IN_RC(ptr_reg));\n-  match(DecodeN reg);\n-\n-  format %{ \"[$reg]\" %}\n-  interface(MEMORY_INTER) %{\n-    base($reg);\n-    index(0x4);\n-    scale(0x0);\n-    disp(0x0);\n-  %}\n-%}\n-\n-\/\/ Indirect Memory Plus Short Offset Operand\n-operand indOffset8Narrow(rRegN reg, immL8 off)\n-%{\n-  predicate(CompressedOops::shift() == 0);\n-  constraint(ALLOC_IN_RC(ptr_reg));\n-  match(AddP (DecodeN reg) off);\n-\n-  format %{ \"[$reg + $off (8-bit)]\" %}\n-  interface(MEMORY_INTER) %{\n-    base($reg);\n-    index(0x4);\n-    scale(0x0);\n-    disp($off);\n-  %}\n-%}\n-\n-\/\/ Indirect Memory Plus Long Offset Operand\n-operand indOffset32Narrow(rRegN reg, immL32 off)\n-%{\n-  predicate(CompressedOops::shift() == 0);\n-  constraint(ALLOC_IN_RC(ptr_reg));\n-  match(AddP (DecodeN reg) off);\n-\n-  format %{ \"[$reg + $off (32-bit)]\" %}\n-  interface(MEMORY_INTER) %{\n-    base($reg);\n-    index(0x4);\n-    scale(0x0);\n-    disp($off);\n-  %}\n-%}\n-\n-\/\/ Indirect Memory Plus Index Register Plus Offset Operand\n-operand indIndexOffsetNarrow(rRegN reg, rRegL lreg, immL32 off)\n-%{\n-  predicate(CompressedOops::shift() == 0);\n-  constraint(ALLOC_IN_RC(ptr_reg));\n-  match(AddP (AddP (DecodeN reg) lreg) off);\n-\n-  op_cost(10);\n-  format %{\"[$reg + $off + $lreg]\" %}\n-  interface(MEMORY_INTER) %{\n-    base($reg);\n-    index($lreg);\n-    scale(0x0);\n-    disp($off);\n-  %}\n-%}\n-\n-\/\/ Indirect Memory Plus Index Register Plus Offset Operand\n-operand indIndexNarrow(rRegN reg, rRegL lreg)\n-%{\n-  predicate(CompressedOops::shift() == 0);\n-  constraint(ALLOC_IN_RC(ptr_reg));\n-  match(AddP (DecodeN reg) lreg);\n-\n-  op_cost(10);\n-  format %{\"[$reg + $lreg]\" %}\n-  interface(MEMORY_INTER) %{\n-    base($reg);\n-    index($lreg);\n-    scale(0x0);\n-    disp(0x0);\n-  %}\n-%}\n-\n-\/\/ Indirect Memory Times Scale Plus Index Register\n-operand indIndexScaleNarrow(rRegN reg, rRegL lreg, immI2 scale)\n-%{\n-  predicate(CompressedOops::shift() == 0);\n-  constraint(ALLOC_IN_RC(ptr_reg));\n-  match(AddP (DecodeN reg) (LShiftL lreg scale));\n-\n-  op_cost(10);\n-  format %{\"[$reg + $lreg << $scale]\" %}\n-  interface(MEMORY_INTER) %{\n-    base($reg);\n-    index($lreg);\n-    scale($scale);\n-    disp(0x0);\n-  %}\n-%}\n-\n-\/\/ Indirect Memory Times Scale Plus Index Register Plus Offset Operand\n-operand indIndexScaleOffsetNarrow(rRegN reg, immL32 off, rRegL lreg, immI2 scale)\n-%{\n-  predicate(CompressedOops::shift() == 0);\n-  constraint(ALLOC_IN_RC(ptr_reg));\n-  match(AddP (AddP (DecodeN reg) (LShiftL lreg scale)) off);\n-\n-  op_cost(10);\n-  format %{\"[$reg + $off + $lreg << $scale]\" %}\n-  interface(MEMORY_INTER) %{\n-    base($reg);\n-    index($lreg);\n-    scale($scale);\n-    disp($off);\n-  %}\n-%}\n-\n-\/\/ Indirect Memory Times Plus Positive Index Register Plus Offset Operand\n-operand indPosIndexOffsetNarrow(rRegN reg, immL32 off, rRegI idx)\n-%{\n-  constraint(ALLOC_IN_RC(ptr_reg));\n-  predicate(CompressedOops::shift() == 0 && n->in(2)->in(3)->as_Type()->type()->is_long()->_lo >= 0);\n-  match(AddP (AddP (DecodeN reg) (ConvI2L idx)) off);\n-\n-  op_cost(10);\n-  format %{\"[$reg + $off + $idx]\" %}\n-  interface(MEMORY_INTER) %{\n-    base($reg);\n-    index($idx);\n-    scale(0x0);\n-    disp($off);\n-  %}\n-%}\n-\n-\/\/ Indirect Memory Times Scale Plus Positive Index Register Plus Offset Operand\n-operand indPosIndexScaleOffsetNarrow(rRegN reg, immL32 off, rRegI idx, immI2 scale)\n-%{\n-  constraint(ALLOC_IN_RC(ptr_reg));\n-  predicate(CompressedOops::shift() == 0 && n->in(2)->in(3)->in(1)->as_Type()->type()->is_long()->_lo >= 0);\n-  match(AddP (AddP (DecodeN reg) (LShiftL (ConvI2L idx) scale)) off);\n-\n-  op_cost(10);\n-  format %{\"[$reg + $off + $idx << $scale]\" %}\n-  interface(MEMORY_INTER) %{\n-    base($reg);\n-    index($idx);\n-    scale($scale);\n-    disp($off);\n-  %}\n-%}\n-\n-\/\/----------Special Memory Operands--------------------------------------------\n-\/\/ Stack Slot Operand - This operand is used for loading and storing temporary\n-\/\/                      values on the stack where a match requires a value to\n-\/\/                      flow through memory.\n-operand stackSlotP(sRegP reg)\n-%{\n-  constraint(ALLOC_IN_RC(stack_slots));\n-  \/\/ No match rule because this operand is only generated in matching\n-\n-  format %{ \"[$reg]\" %}\n-  interface(MEMORY_INTER) %{\n-    base(0x4);   \/\/ RSP\n-    index(0x4);  \/\/ No Index\n-    scale(0x0);  \/\/ No Scale\n-    disp($reg);  \/\/ Stack Offset\n-  %}\n-%}\n-\n-operand stackSlotI(sRegI reg)\n-%{\n-  constraint(ALLOC_IN_RC(stack_slots));\n-  \/\/ No match rule because this operand is only generated in matching\n-\n-  format %{ \"[$reg]\" %}\n-  interface(MEMORY_INTER) %{\n-    base(0x4);   \/\/ RSP\n-    index(0x4);  \/\/ No Index\n-    scale(0x0);  \/\/ No Scale\n-    disp($reg);  \/\/ Stack Offset\n-  %}\n-%}\n-\n-operand stackSlotF(sRegF reg)\n-%{\n-  constraint(ALLOC_IN_RC(stack_slots));\n-  \/\/ No match rule because this operand is only generated in matching\n-\n-  format %{ \"[$reg]\" %}\n-  interface(MEMORY_INTER) %{\n-    base(0x4);   \/\/ RSP\n-    index(0x4);  \/\/ No Index\n-    scale(0x0);  \/\/ No Scale\n-    disp($reg);  \/\/ Stack Offset\n-  %}\n-%}\n-\n-operand stackSlotD(sRegD reg)\n-%{\n-  constraint(ALLOC_IN_RC(stack_slots));\n-  \/\/ No match rule because this operand is only generated in matching\n-\n-  format %{ \"[$reg]\" %}\n-  interface(MEMORY_INTER) %{\n-    base(0x4);   \/\/ RSP\n-    index(0x4);  \/\/ No Index\n-    scale(0x0);  \/\/ No Scale\n-    disp($reg);  \/\/ Stack Offset\n-  %}\n-%}\n-operand stackSlotL(sRegL reg)\n-%{\n-  constraint(ALLOC_IN_RC(stack_slots));\n-  \/\/ No match rule because this operand is only generated in matching\n-\n-  format %{ \"[$reg]\" %}\n-  interface(MEMORY_INTER) %{\n-    base(0x4);   \/\/ RSP\n-    index(0x4);  \/\/ No Index\n-    scale(0x0);  \/\/ No Scale\n-    disp($reg);  \/\/ Stack Offset\n-  %}\n-%}\n-\n-\/\/----------Conditional Branch Operands----------------------------------------\n-\/\/ Comparison Op  - This is the operation of the comparison, and is limited to\n-\/\/                  the following set of codes:\n-\/\/                  L (<), LE (<=), G (>), GE (>=), E (==), NE (!=)\n-\/\/\n-\/\/ Other attributes of the comparison, such as unsignedness, are specified\n-\/\/ by the comparison instruction that sets a condition code flags register.\n-\/\/ That result is represented by a flags operand whose subtype is appropriate\n-\/\/ to the unsignedness (etc.) of the comparison.\n-\/\/\n-\/\/ Later, the instruction which matches both the Comparison Op (a Bool) and\n-\/\/ the flags (produced by the Cmp) specifies the coding of the comparison op\n-\/\/ by matching a specific subtype of Bool operand below, such as cmpOpU.\n-\n-\/\/ Comparison Code\n-operand cmpOp()\n-%{\n-  match(Bool);\n-\n-  format %{ \"\" %}\n-  interface(COND_INTER) %{\n-    equal(0x4, \"e\");\n-    not_equal(0x5, \"ne\");\n-    less(0xC, \"l\");\n-    greater_equal(0xD, \"ge\");\n-    less_equal(0xE, \"le\");\n-    greater(0xF, \"g\");\n-    overflow(0x0, \"o\");\n-    no_overflow(0x1, \"no\");\n-  %}\n-%}\n-\n-\/\/ Comparison Code, unsigned compare.  Used by FP also, with\n-\/\/ C2 (unordered) turned into GT or LT already.  The other bits\n-\/\/ C0 and C3 are turned into Carry & Zero flags.\n-operand cmpOpU()\n-%{\n-  match(Bool);\n-\n-  format %{ \"\" %}\n-  interface(COND_INTER) %{\n-    equal(0x4, \"e\");\n-    not_equal(0x5, \"ne\");\n-    less(0x2, \"b\");\n-    greater_equal(0x3, \"ae\");\n-    less_equal(0x6, \"be\");\n-    greater(0x7, \"a\");\n-    overflow(0x0, \"o\");\n-    no_overflow(0x1, \"no\");\n-  %}\n-%}\n-\n-\n-\/\/ Floating comparisons that don't require any fixup for the unordered case,\n-\/\/ If both inputs of the comparison are the same, ZF is always set so we\n-\/\/ don't need to use cmpOpUCF2 for eq\/ne\n-operand cmpOpUCF() %{\n-  match(Bool);\n-  predicate(n->as_Bool()->_test._test == BoolTest::lt ||\n-            n->as_Bool()->_test._test == BoolTest::ge ||\n-            n->as_Bool()->_test._test == BoolTest::le ||\n-            n->as_Bool()->_test._test == BoolTest::gt ||\n-            n->in(1)->in(1) == n->in(1)->in(2));\n-  format %{ \"\" %}\n-  interface(COND_INTER) %{\n-    equal(0xb, \"np\");\n-    not_equal(0xa, \"p\");\n-    less(0x2, \"b\");\n-    greater_equal(0x3, \"ae\");\n-    less_equal(0x6, \"be\");\n-    greater(0x7, \"a\");\n-    overflow(0x0, \"o\");\n-    no_overflow(0x1, \"no\");\n-  %}\n-%}\n-\n-\n-\/\/ Floating comparisons that can be fixed up with extra conditional jumps\n-operand cmpOpUCF2() %{\n-  match(Bool);\n-  predicate((n->as_Bool()->_test._test == BoolTest::ne ||\n-             n->as_Bool()->_test._test == BoolTest::eq) &&\n-            n->in(1)->in(1) != n->in(1)->in(2));\n-  format %{ \"\" %}\n-  interface(COND_INTER) %{\n-    equal(0x4, \"e\");\n-    not_equal(0x5, \"ne\");\n-    less(0x2, \"b\");\n-    greater_equal(0x3, \"ae\");\n-    less_equal(0x6, \"be\");\n-    greater(0x7, \"a\");\n-    overflow(0x0, \"o\");\n-    no_overflow(0x1, \"no\");\n-  %}\n-%}\n-\n-\/\/----------OPERAND CLASSES----------------------------------------------------\n-\/\/ Operand Classes are groups of operands that are used as to simplify\n-\/\/ instruction definitions by not requiring the AD writer to specify separate\n-\/\/ instructions for every form of operand when the instruction accepts\n-\/\/ multiple operand types with the same basic encoding and format.  The classic\n-\/\/ case of this is memory operands.\n-\n-opclass memory(indirect, indOffset8, indOffset32, indIndexOffset, indIndex,\n-               indIndexScale, indPosIndexScale, indIndexScaleOffset, indPosIndexOffset, indPosIndexScaleOffset,\n-               indCompressedOopOffset,\n-               indirectNarrow, indOffset8Narrow, indOffset32Narrow,\n-               indIndexOffsetNarrow, indIndexNarrow, indIndexScaleNarrow,\n-               indIndexScaleOffsetNarrow, indPosIndexOffsetNarrow, indPosIndexScaleOffsetNarrow);\n-\n-\/\/----------PIPELINE-----------------------------------------------------------\n-\/\/ Rules which define the behavior of the target architectures pipeline.\n-pipeline %{\n-\n-\/\/----------ATTRIBUTES---------------------------------------------------------\n-attributes %{\n-  variable_size_instructions;        \/\/ Fixed size instructions\n-  max_instructions_per_bundle = 3;   \/\/ Up to 3 instructions per bundle\n-  instruction_unit_size = 1;         \/\/ An instruction is 1 bytes long\n-  instruction_fetch_unit_size = 16;  \/\/ The processor fetches one line\n-  instruction_fetch_units = 1;       \/\/ of 16 bytes\n-%}\n-\n-\/\/----------RESOURCES----------------------------------------------------------\n-\/\/ Resources are the functional units available to the machine\n-\n-\/\/ Generic P2\/P3 pipeline\n-\/\/ 3 decoders, only D0 handles big operands; a \"bundle\" is the limit of\n-\/\/ 3 instructions decoded per cycle.\n-\/\/ 2 load\/store ops per cycle, 1 branch, 1 FPU,\n-\/\/ 3 ALU op, only ALU0 handles mul instructions.\n-resources( D0, D1, D2, DECODE = D0 | D1 | D2,\n-           MS0, MS1, MS2, MEM = MS0 | MS1 | MS2,\n-           BR, FPU,\n-           ALU0, ALU1, ALU2, ALU = ALU0 | ALU1 | ALU2);\n-\n-\/\/----------PIPELINE DESCRIPTION-----------------------------------------------\n-\/\/ Pipeline Description specifies the stages in the machine's pipeline\n-\n-\/\/ Generic P2\/P3 pipeline\n-pipe_desc(S0, S1, S2, S3, S4, S5);\n-\n-\/\/----------PIPELINE CLASSES---------------------------------------------------\n-\/\/ Pipeline Classes describe the stages in which input and output are\n-\/\/ referenced by the hardware pipeline.\n-\n-\/\/ Naming convention: ialu or fpu\n-\/\/ Then: _reg\n-\/\/ Then: _reg if there is a 2nd register\n-\/\/ Then: _long if it's a pair of instructions implementing a long\n-\/\/ Then: _fat if it requires the big decoder\n-\/\/   Or: _mem if it requires the big decoder and a memory unit.\n-\n-\/\/ Integer ALU reg operation\n-pipe_class ialu_reg(rRegI dst)\n-%{\n-    single_instruction;\n-    dst    : S4(write);\n-    dst    : S3(read);\n-    DECODE : S0;        \/\/ any decoder\n-    ALU    : S3;        \/\/ any alu\n-%}\n-\n-\/\/ Long ALU reg operation\n-pipe_class ialu_reg_long(rRegL dst)\n-%{\n-    instruction_count(2);\n-    dst    : S4(write);\n-    dst    : S3(read);\n-    DECODE : S0(2);     \/\/ any 2 decoders\n-    ALU    : S3(2);     \/\/ both alus\n-%}\n-\n-\/\/ Integer ALU reg operation using big decoder\n-pipe_class ialu_reg_fat(rRegI dst)\n-%{\n-    single_instruction;\n-    dst    : S4(write);\n-    dst    : S3(read);\n-    D0     : S0;        \/\/ big decoder only\n-    ALU    : S3;        \/\/ any alu\n-%}\n-\n-\/\/ Integer ALU reg-reg operation\n-pipe_class ialu_reg_reg(rRegI dst, rRegI src)\n-%{\n-    single_instruction;\n-    dst    : S4(write);\n-    src    : S3(read);\n-    DECODE : S0;        \/\/ any decoder\n-    ALU    : S3;        \/\/ any alu\n-%}\n-\n-\/\/ Integer ALU reg-reg operation\n-pipe_class ialu_reg_reg_fat(rRegI dst, memory src)\n-%{\n-    single_instruction;\n-    dst    : S4(write);\n-    src    : S3(read);\n-    D0     : S0;        \/\/ big decoder only\n-    ALU    : S3;        \/\/ any alu\n-%}\n-\n-\/\/ Integer ALU reg-mem operation\n-pipe_class ialu_reg_mem(rRegI dst, memory mem)\n-%{\n-    single_instruction;\n-    dst    : S5(write);\n-    mem    : S3(read);\n-    D0     : S0;        \/\/ big decoder only\n-    ALU    : S4;        \/\/ any alu\n-    MEM    : S3;        \/\/ any mem\n-%}\n-\n-\/\/ Integer mem operation (prefetch)\n-pipe_class ialu_mem(memory mem)\n-%{\n-    single_instruction;\n-    mem    : S3(read);\n-    D0     : S0;        \/\/ big decoder only\n-    MEM    : S3;        \/\/ any mem\n-%}\n-\n-\/\/ Integer Store to Memory\n-pipe_class ialu_mem_reg(memory mem, rRegI src)\n-%{\n-    single_instruction;\n-    mem    : S3(read);\n-    src    : S5(read);\n-    D0     : S0;        \/\/ big decoder only\n-    ALU    : S4;        \/\/ any alu\n-    MEM    : S3;\n-%}\n-\n-\/\/ \/\/ Long Store to Memory\n-\/\/ pipe_class ialu_mem_long_reg(memory mem, rRegL src)\n-\/\/ %{\n-\/\/     instruction_count(2);\n-\/\/     mem    : S3(read);\n-\/\/     src    : S5(read);\n-\/\/     D0     : S0(2);          \/\/ big decoder only; twice\n-\/\/     ALU    : S4(2);     \/\/ any 2 alus\n-\/\/     MEM    : S3(2);  \/\/ Both mems\n-\/\/ %}\n-\n-\/\/ Integer Store to Memory\n-pipe_class ialu_mem_imm(memory mem)\n-%{\n-    single_instruction;\n-    mem    : S3(read);\n-    D0     : S0;        \/\/ big decoder only\n-    ALU    : S4;        \/\/ any alu\n-    MEM    : S3;\n-%}\n-\n-\/\/ Integer ALU0 reg-reg operation\n-pipe_class ialu_reg_reg_alu0(rRegI dst, rRegI src)\n-%{\n-    single_instruction;\n-    dst    : S4(write);\n-    src    : S3(read);\n-    D0     : S0;        \/\/ Big decoder only\n-    ALU0   : S3;        \/\/ only alu0\n-%}\n-\n-\/\/ Integer ALU0 reg-mem operation\n-pipe_class ialu_reg_mem_alu0(rRegI dst, memory mem)\n-%{\n-    single_instruction;\n-    dst    : S5(write);\n-    mem    : S3(read);\n-    D0     : S0;        \/\/ big decoder only\n-    ALU0   : S4;        \/\/ ALU0 only\n-    MEM    : S3;        \/\/ any mem\n-%}\n-\n-\/\/ Integer ALU reg-reg operation\n-pipe_class ialu_cr_reg_reg(rFlagsReg cr, rRegI src1, rRegI src2)\n-%{\n-    single_instruction;\n-    cr     : S4(write);\n-    src1   : S3(read);\n-    src2   : S3(read);\n-    DECODE : S0;        \/\/ any decoder\n-    ALU    : S3;        \/\/ any alu\n-%}\n-\n-\/\/ Integer ALU reg-imm operation\n-pipe_class ialu_cr_reg_imm(rFlagsReg cr, rRegI src1)\n-%{\n-    single_instruction;\n-    cr     : S4(write);\n-    src1   : S3(read);\n-    DECODE : S0;        \/\/ any decoder\n-    ALU    : S3;        \/\/ any alu\n-%}\n-\n-\/\/ Integer ALU reg-mem operation\n-pipe_class ialu_cr_reg_mem(rFlagsReg cr, rRegI src1, memory src2)\n-%{\n-    single_instruction;\n-    cr     : S4(write);\n-    src1   : S3(read);\n-    src2   : S3(read);\n-    D0     : S0;        \/\/ big decoder only\n-    ALU    : S4;        \/\/ any alu\n-    MEM    : S3;\n-%}\n-\n-\/\/ Conditional move reg-reg\n-pipe_class pipe_cmplt( rRegI p, rRegI q, rRegI y)\n-%{\n-    instruction_count(4);\n-    y      : S4(read);\n-    q      : S3(read);\n-    p      : S3(read);\n-    DECODE : S0(4);     \/\/ any decoder\n-%}\n-\n-\/\/ Conditional move reg-reg\n-pipe_class pipe_cmov_reg( rRegI dst, rRegI src, rFlagsReg cr)\n-%{\n-    single_instruction;\n-    dst    : S4(write);\n-    src    : S3(read);\n-    cr     : S3(read);\n-    DECODE : S0;        \/\/ any decoder\n-%}\n-\n-\/\/ Conditional move reg-mem\n-pipe_class pipe_cmov_mem( rFlagsReg cr, rRegI dst, memory src)\n-%{\n-    single_instruction;\n-    dst    : S4(write);\n-    src    : S3(read);\n-    cr     : S3(read);\n-    DECODE : S0;        \/\/ any decoder\n-    MEM    : S3;\n-%}\n-\n-\/\/ Conditional move reg-reg long\n-pipe_class pipe_cmov_reg_long( rFlagsReg cr, rRegL dst, rRegL src)\n-%{\n-    single_instruction;\n-    dst    : S4(write);\n-    src    : S3(read);\n-    cr     : S3(read);\n-    DECODE : S0(2);     \/\/ any 2 decoders\n-%}\n-\n-\/\/ Float reg-reg operation\n-pipe_class fpu_reg(regD dst)\n-%{\n-    instruction_count(2);\n-    dst    : S3(read);\n-    DECODE : S0(2);     \/\/ any 2 decoders\n-    FPU    : S3;\n-%}\n-\n-\/\/ Float reg-reg operation\n-pipe_class fpu_reg_reg(regD dst, regD src)\n-%{\n-    instruction_count(2);\n-    dst    : S4(write);\n-    src    : S3(read);\n-    DECODE : S0(2);     \/\/ any 2 decoders\n-    FPU    : S3;\n-%}\n-\n-\/\/ Float reg-reg operation\n-pipe_class fpu_reg_reg_reg(regD dst, regD src1, regD src2)\n-%{\n-    instruction_count(3);\n-    dst    : S4(write);\n-    src1   : S3(read);\n-    src2   : S3(read);\n-    DECODE : S0(3);     \/\/ any 3 decoders\n-    FPU    : S3(2);\n-%}\n-\n-\/\/ Float reg-reg operation\n-pipe_class fpu_reg_reg_reg_reg(regD dst, regD src1, regD src2, regD src3)\n-%{\n-    instruction_count(4);\n-    dst    : S4(write);\n-    src1   : S3(read);\n-    src2   : S3(read);\n-    src3   : S3(read);\n-    DECODE : S0(4);     \/\/ any 3 decoders\n-    FPU    : S3(2);\n-%}\n-\n-\/\/ Float reg-reg operation\n-pipe_class fpu_reg_mem_reg_reg(regD dst, memory src1, regD src2, regD src3)\n-%{\n-    instruction_count(4);\n-    dst    : S4(write);\n-    src1   : S3(read);\n-    src2   : S3(read);\n-    src3   : S3(read);\n-    DECODE : S1(3);     \/\/ any 3 decoders\n-    D0     : S0;        \/\/ Big decoder only\n-    FPU    : S3(2);\n-    MEM    : S3;\n-%}\n-\n-\/\/ Float reg-mem operation\n-pipe_class fpu_reg_mem(regD dst, memory mem)\n-%{\n-    instruction_count(2);\n-    dst    : S5(write);\n-    mem    : S3(read);\n-    D0     : S0;        \/\/ big decoder only\n-    DECODE : S1;        \/\/ any decoder for FPU POP\n-    FPU    : S4;\n-    MEM    : S3;        \/\/ any mem\n-%}\n-\n-\/\/ Float reg-mem operation\n-pipe_class fpu_reg_reg_mem(regD dst, regD src1, memory mem)\n-%{\n-    instruction_count(3);\n-    dst    : S5(write);\n-    src1   : S3(read);\n-    mem    : S3(read);\n-    D0     : S0;        \/\/ big decoder only\n-    DECODE : S1(2);     \/\/ any decoder for FPU POP\n-    FPU    : S4;\n-    MEM    : S3;        \/\/ any mem\n-%}\n-\n-\/\/ Float mem-reg operation\n-pipe_class fpu_mem_reg(memory mem, regD src)\n-%{\n-    instruction_count(2);\n-    src    : S5(read);\n-    mem    : S3(read);\n-    DECODE : S0;        \/\/ any decoder for FPU PUSH\n-    D0     : S1;        \/\/ big decoder only\n-    FPU    : S4;\n-    MEM    : S3;        \/\/ any mem\n-%}\n-\n-pipe_class fpu_mem_reg_reg(memory mem, regD src1, regD src2)\n-%{\n-    instruction_count(3);\n-    src1   : S3(read);\n-    src2   : S3(read);\n-    mem    : S3(read);\n-    DECODE : S0(2);     \/\/ any decoder for FPU PUSH\n-    D0     : S1;        \/\/ big decoder only\n-    FPU    : S4;\n-    MEM    : S3;        \/\/ any mem\n-%}\n-\n-pipe_class fpu_mem_reg_mem(memory mem, regD src1, memory src2)\n-%{\n-    instruction_count(3);\n-    src1   : S3(read);\n-    src2   : S3(read);\n-    mem    : S4(read);\n-    DECODE : S0;        \/\/ any decoder for FPU PUSH\n-    D0     : S0(2);     \/\/ big decoder only\n-    FPU    : S4;\n-    MEM    : S3(2);     \/\/ any mem\n-%}\n-\n-pipe_class fpu_mem_mem(memory dst, memory src1)\n-%{\n-    instruction_count(2);\n-    src1   : S3(read);\n-    dst    : S4(read);\n-    D0     : S0(2);     \/\/ big decoder only\n-    MEM    : S3(2);     \/\/ any mem\n-%}\n-\n-pipe_class fpu_mem_mem_mem(memory dst, memory src1, memory src2)\n-%{\n-    instruction_count(3);\n-    src1   : S3(read);\n-    src2   : S3(read);\n-    dst    : S4(read);\n-    D0     : S0(3);     \/\/ big decoder only\n-    FPU    : S4;\n-    MEM    : S3(3);     \/\/ any mem\n-%}\n-\n-pipe_class fpu_mem_reg_con(memory mem, regD src1)\n-%{\n-    instruction_count(3);\n-    src1   : S4(read);\n-    mem    : S4(read);\n-    DECODE : S0;        \/\/ any decoder for FPU PUSH\n-    D0     : S0(2);     \/\/ big decoder only\n-    FPU    : S4;\n-    MEM    : S3(2);     \/\/ any mem\n-%}\n-\n-\/\/ Float load constant\n-pipe_class fpu_reg_con(regD dst)\n-%{\n-    instruction_count(2);\n-    dst    : S5(write);\n-    D0     : S0;        \/\/ big decoder only for the load\n-    DECODE : S1;        \/\/ any decoder for FPU POP\n-    FPU    : S4;\n-    MEM    : S3;        \/\/ any mem\n-%}\n-\n-\/\/ Float load constant\n-pipe_class fpu_reg_reg_con(regD dst, regD src)\n-%{\n-    instruction_count(3);\n-    dst    : S5(write);\n-    src    : S3(read);\n-    D0     : S0;        \/\/ big decoder only for the load\n-    DECODE : S1(2);     \/\/ any decoder for FPU POP\n-    FPU    : S4;\n-    MEM    : S3;        \/\/ any mem\n-%}\n-\n-\/\/ UnConditional branch\n-pipe_class pipe_jmp(label labl)\n-%{\n-    single_instruction;\n-    BR   : S3;\n-%}\n-\n-\/\/ Conditional branch\n-pipe_class pipe_jcc(cmpOp cmp, rFlagsReg cr, label labl)\n-%{\n-    single_instruction;\n-    cr    : S1(read);\n-    BR    : S3;\n-%}\n-\n-\/\/ Allocation idiom\n-pipe_class pipe_cmpxchg(rRegP dst, rRegP heap_ptr)\n-%{\n-    instruction_count(1); force_serialization;\n-    fixed_latency(6);\n-    heap_ptr : S3(read);\n-    DECODE   : S0(3);\n-    D0       : S2;\n-    MEM      : S3;\n-    ALU      : S3(2);\n-    dst      : S5(write);\n-    BR       : S5;\n-%}\n-\n-\/\/ Generic big\/slow expanded idiom\n-pipe_class pipe_slow()\n-%{\n-    instruction_count(10); multiple_bundles; force_serialization;\n-    fixed_latency(100);\n-    D0  : S0(2);\n-    MEM : S3(2);\n-%}\n-\n-\/\/ The real do-nothing guy\n-pipe_class empty()\n-%{\n-    instruction_count(0);\n-%}\n-\n-\/\/ Define the class for the Nop node\n-define\n-%{\n-   MachNop = empty;\n-%}\n-\n-%}\n-\n-\/\/----------INSTRUCTIONS-------------------------------------------------------\n-\/\/\n-\/\/ match      -- States which machine-independent subtree may be replaced\n-\/\/               by this instruction.\n-\/\/ ins_cost   -- The estimated cost of this instruction is used by instruction\n-\/\/               selection to identify a minimum cost tree of machine\n-\/\/               instructions that matches a tree of machine-independent\n-\/\/               instructions.\n-\/\/ format     -- A string providing the disassembly for this instruction.\n-\/\/               The value of an instruction's operand may be inserted\n-\/\/               by referring to it with a '$' prefix.\n-\/\/ opcode     -- Three instruction opcodes may be provided.  These are referred\n-\/\/               to within an encode class as $primary, $secondary, and $tertiary\n-\/\/               rrspectively.  The primary opcode is commonly used to\n-\/\/               indicate the type of machine instruction, while secondary\n-\/\/               and tertiary are often used for prefix options or addressing\n-\/\/               modes.\n-\/\/ ins_encode -- A list of encode classes with parameters. The encode class\n-\/\/               name must have been defined in an 'enc_class' specification\n-\/\/               in the encode section of the architecture description.\n-\n-\/\/ Dummy reg-to-reg vector moves. Removed during post-selection cleanup.\n-\/\/ Load Float\n-instruct MoveF2VL(vlRegF dst, regF src) %{\n-  match(Set dst src);\n-  format %{ \"movss $dst,$src\\t! load float (4 bytes)\" %}\n-  ins_encode %{\n-    ShouldNotReachHere();\n-  %}\n-  ins_pipe( fpu_reg_reg );\n-%}\n-\n-\/\/ Load Float\n-instruct MoveF2LEG(legRegF dst, regF src) %{\n-  match(Set dst src);\n-  format %{ \"movss $dst,$src\\t# if src != dst load float (4 bytes)\" %}\n-  ins_encode %{\n-    ShouldNotReachHere();\n-  %}\n-  ins_pipe( fpu_reg_reg );\n-%}\n-\n-\/\/ Load Float\n-instruct MoveVL2F(regF dst, vlRegF src) %{\n-  match(Set dst src);\n-  format %{ \"movss $dst,$src\\t! load float (4 bytes)\" %}\n-  ins_encode %{\n-    ShouldNotReachHere();\n-  %}\n-  ins_pipe( fpu_reg_reg );\n-%}\n-\n-\/\/ Load Float\n-instruct MoveLEG2F(regF dst, legRegF src) %{\n-  match(Set dst src);\n-  format %{ \"movss $dst,$src\\t# if src != dst load float (4 bytes)\" %}\n-  ins_encode %{\n-    ShouldNotReachHere();\n-  %}\n-  ins_pipe( fpu_reg_reg );\n-%}\n-\n-\/\/ Load Double\n-instruct MoveD2VL(vlRegD dst, regD src) %{\n-  match(Set dst src);\n-  format %{ \"movsd $dst,$src\\t! load double (8 bytes)\" %}\n-  ins_encode %{\n-    ShouldNotReachHere();\n-  %}\n-  ins_pipe( fpu_reg_reg );\n-%}\n-\n-\/\/ Load Double\n-instruct MoveD2LEG(legRegD dst, regD src) %{\n-  match(Set dst src);\n-  format %{ \"movsd $dst,$src\\t# if src != dst load double (8 bytes)\" %}\n-  ins_encode %{\n-    ShouldNotReachHere();\n-  %}\n-  ins_pipe( fpu_reg_reg );\n-%}\n-\n-\/\/ Load Double\n-instruct MoveVL2D(regD dst, vlRegD src) %{\n-  match(Set dst src);\n-  format %{ \"movsd $dst,$src\\t! load double (8 bytes)\" %}\n-  ins_encode %{\n-    ShouldNotReachHere();\n-  %}\n-  ins_pipe( fpu_reg_reg );\n-%}\n-\n-\/\/ Load Double\n-instruct MoveLEG2D(regD dst, legRegD src) %{\n-  match(Set dst src);\n-  format %{ \"movsd $dst,$src\\t# if src != dst load double (8 bytes)\" %}\n-  ins_encode %{\n-    ShouldNotReachHere();\n-  %}\n-  ins_pipe( fpu_reg_reg );\n-%}\n-\n-\/\/----------Load\/Store\/Move Instructions---------------------------------------\n-\/\/----------Load Instructions--------------------------------------------------\n-\n-\/\/ Load Byte (8 bit signed)\n-instruct loadB(rRegI dst, memory mem)\n-%{\n-  match(Set dst (LoadB mem));\n-\n-  ins_cost(125);\n-  format %{ \"movsbl  $dst, $mem\\t# byte\" %}\n-\n-  ins_encode %{\n-    __ movsbl($dst$$Register, $mem$$Address);\n-  %}\n-\n-  ins_pipe(ialu_reg_mem);\n-%}\n-\n-\/\/ Load Byte (8 bit signed) into Long Register\n-instruct loadB2L(rRegL dst, memory mem)\n-%{\n-  match(Set dst (ConvI2L (LoadB mem)));\n-\n-  ins_cost(125);\n-  format %{ \"movsbq  $dst, $mem\\t# byte -> long\" %}\n-\n-  ins_encode %{\n-    __ movsbq($dst$$Register, $mem$$Address);\n-  %}\n-\n-  ins_pipe(ialu_reg_mem);\n-%}\n-\n-\/\/ Load Unsigned Byte (8 bit UNsigned)\n-instruct loadUB(rRegI dst, memory mem)\n-%{\n-  match(Set dst (LoadUB mem));\n-\n-  ins_cost(125);\n-  format %{ \"movzbl  $dst, $mem\\t# ubyte\" %}\n-\n-  ins_encode %{\n-    __ movzbl($dst$$Register, $mem$$Address);\n-  %}\n-\n-  ins_pipe(ialu_reg_mem);\n-%}\n-\n-\/\/ Load Unsigned Byte (8 bit UNsigned) into Long Register\n-instruct loadUB2L(rRegL dst, memory mem)\n-%{\n-  match(Set dst (ConvI2L (LoadUB mem)));\n-\n-  ins_cost(125);\n-  format %{ \"movzbq  $dst, $mem\\t# ubyte -> long\" %}\n-\n-  ins_encode %{\n-    __ movzbq($dst$$Register, $mem$$Address);\n-  %}\n-\n-  ins_pipe(ialu_reg_mem);\n-%}\n-\n-\/\/ Load Unsigned Byte (8 bit UNsigned) with 32-bit mask into Long Register\n-instruct loadUB2L_immI(rRegL dst, memory mem, immI mask, rFlagsReg cr) %{\n-  match(Set dst (ConvI2L (AndI (LoadUB mem) mask)));\n-  effect(KILL cr);\n-\n-  format %{ \"movzbq  $dst, $mem\\t# ubyte & 32-bit mask -> long\\n\\t\"\n-            \"andl    $dst, right_n_bits($mask, 8)\" %}\n-  ins_encode %{\n-    Register Rdst = $dst$$Register;\n-    __ movzbq(Rdst, $mem$$Address);\n-    __ andl(Rdst, $mask$$constant & right_n_bits(8));\n-  %}\n-  ins_pipe(ialu_reg_mem);\n-%}\n-\n-\/\/ Load Short (16 bit signed)\n-instruct loadS(rRegI dst, memory mem)\n-%{\n-  match(Set dst (LoadS mem));\n-\n-  ins_cost(125);\n-  format %{ \"movswl $dst, $mem\\t# short\" %}\n-\n-  ins_encode %{\n-    __ movswl($dst$$Register, $mem$$Address);\n-  %}\n-\n-  ins_pipe(ialu_reg_mem);\n-%}\n-\n-\/\/ Load Short (16 bit signed) to Byte (8 bit signed)\n-instruct loadS2B(rRegI dst, memory mem, immI_24 twentyfour) %{\n-  match(Set dst (RShiftI (LShiftI (LoadS mem) twentyfour) twentyfour));\n-\n-  ins_cost(125);\n-  format %{ \"movsbl $dst, $mem\\t# short -> byte\" %}\n-  ins_encode %{\n-    __ movsbl($dst$$Register, $mem$$Address);\n-  %}\n-  ins_pipe(ialu_reg_mem);\n-%}\n-\n-\/\/ Load Short (16 bit signed) into Long Register\n-instruct loadS2L(rRegL dst, memory mem)\n-%{\n-  match(Set dst (ConvI2L (LoadS mem)));\n-\n-  ins_cost(125);\n-  format %{ \"movswq $dst, $mem\\t# short -> long\" %}\n-\n-  ins_encode %{\n-    __ movswq($dst$$Register, $mem$$Address);\n-  %}\n-\n-  ins_pipe(ialu_reg_mem);\n-%}\n-\n-\/\/ Load Unsigned Short\/Char (16 bit UNsigned)\n-instruct loadUS(rRegI dst, memory mem)\n-%{\n-  match(Set dst (LoadUS mem));\n-\n-  ins_cost(125);\n-  format %{ \"movzwl  $dst, $mem\\t# ushort\/char\" %}\n-\n-  ins_encode %{\n-    __ movzwl($dst$$Register, $mem$$Address);\n-  %}\n-\n-  ins_pipe(ialu_reg_mem);\n-%}\n-\n-\/\/ Load Unsigned Short\/Char (16 bit UNsigned) to Byte (8 bit signed)\n-instruct loadUS2B(rRegI dst, memory mem, immI_24 twentyfour) %{\n-  match(Set dst (RShiftI (LShiftI (LoadUS mem) twentyfour) twentyfour));\n-\n-  ins_cost(125);\n-  format %{ \"movsbl $dst, $mem\\t# ushort -> byte\" %}\n-  ins_encode %{\n-    __ movsbl($dst$$Register, $mem$$Address);\n-  %}\n-  ins_pipe(ialu_reg_mem);\n-%}\n-\n-\/\/ Load Unsigned Short\/Char (16 bit UNsigned) into Long Register\n-instruct loadUS2L(rRegL dst, memory mem)\n-%{\n-  match(Set dst (ConvI2L (LoadUS mem)));\n-\n-  ins_cost(125);\n-  format %{ \"movzwq  $dst, $mem\\t# ushort\/char -> long\" %}\n-\n-  ins_encode %{\n-    __ movzwq($dst$$Register, $mem$$Address);\n-  %}\n-\n-  ins_pipe(ialu_reg_mem);\n-%}\n-\n-\/\/ Load Unsigned Short\/Char (16 bit UNsigned) with mask 0xFF into Long Register\n-instruct loadUS2L_immI_255(rRegL dst, memory mem, immI_255 mask) %{\n-  match(Set dst (ConvI2L (AndI (LoadUS mem) mask)));\n-\n-  format %{ \"movzbq  $dst, $mem\\t# ushort\/char & 0xFF -> long\" %}\n-  ins_encode %{\n-    __ movzbq($dst$$Register, $mem$$Address);\n-  %}\n-  ins_pipe(ialu_reg_mem);\n-%}\n-\n-\/\/ Load Unsigned Short\/Char (16 bit UNsigned) with 32-bit mask into Long Register\n-instruct loadUS2L_immI(rRegL dst, memory mem, immI mask, rFlagsReg cr) %{\n-  match(Set dst (ConvI2L (AndI (LoadUS mem) mask)));\n-  effect(KILL cr);\n-\n-  format %{ \"movzwq  $dst, $mem\\t# ushort\/char & 32-bit mask -> long\\n\\t\"\n-            \"andl    $dst, right_n_bits($mask, 16)\" %}\n-  ins_encode %{\n-    Register Rdst = $dst$$Register;\n-    __ movzwq(Rdst, $mem$$Address);\n-    __ andl(Rdst, $mask$$constant & right_n_bits(16));\n-  %}\n-  ins_pipe(ialu_reg_mem);\n-%}\n-\n-\/\/ Load Integer\n-instruct loadI(rRegI dst, memory mem)\n-%{\n-  match(Set dst (LoadI mem));\n-\n-  ins_cost(125);\n-  format %{ \"movl    $dst, $mem\\t# int\" %}\n-\n-  ins_encode %{\n-    __ movl($dst$$Register, $mem$$Address);\n-  %}\n-\n-  ins_pipe(ialu_reg_mem);\n-%}\n-\n-\/\/ Load Integer (32 bit signed) to Byte (8 bit signed)\n-instruct loadI2B(rRegI dst, memory mem, immI_24 twentyfour) %{\n-  match(Set dst (RShiftI (LShiftI (LoadI mem) twentyfour) twentyfour));\n-\n-  ins_cost(125);\n-  format %{ \"movsbl  $dst, $mem\\t# int -> byte\" %}\n-  ins_encode %{\n-    __ movsbl($dst$$Register, $mem$$Address);\n-  %}\n-  ins_pipe(ialu_reg_mem);\n-%}\n-\n-\/\/ Load Integer (32 bit signed) to Unsigned Byte (8 bit UNsigned)\n-instruct loadI2UB(rRegI dst, memory mem, immI_255 mask) %{\n-  match(Set dst (AndI (LoadI mem) mask));\n-\n-  ins_cost(125);\n-  format %{ \"movzbl  $dst, $mem\\t# int -> ubyte\" %}\n-  ins_encode %{\n-    __ movzbl($dst$$Register, $mem$$Address);\n-  %}\n-  ins_pipe(ialu_reg_mem);\n-%}\n-\n-\/\/ Load Integer (32 bit signed) to Short (16 bit signed)\n-instruct loadI2S(rRegI dst, memory mem, immI_16 sixteen) %{\n-  match(Set dst (RShiftI (LShiftI (LoadI mem) sixteen) sixteen));\n-\n-  ins_cost(125);\n-  format %{ \"movswl  $dst, $mem\\t# int -> short\" %}\n-  ins_encode %{\n-    __ movswl($dst$$Register, $mem$$Address);\n-  %}\n-  ins_pipe(ialu_reg_mem);\n-%}\n-\n-\/\/ Load Integer (32 bit signed) to Unsigned Short\/Char (16 bit UNsigned)\n-instruct loadI2US(rRegI dst, memory mem, immI_65535 mask) %{\n-  match(Set dst (AndI (LoadI mem) mask));\n-\n-  ins_cost(125);\n-  format %{ \"movzwl  $dst, $mem\\t# int -> ushort\/char\" %}\n-  ins_encode %{\n-    __ movzwl($dst$$Register, $mem$$Address);\n-  %}\n-  ins_pipe(ialu_reg_mem);\n-%}\n-\n-\/\/ Load Integer into Long Register\n-instruct loadI2L(rRegL dst, memory mem)\n-%{\n-  match(Set dst (ConvI2L (LoadI mem)));\n-\n-  ins_cost(125);\n-  format %{ \"movslq  $dst, $mem\\t# int -> long\" %}\n-\n-  ins_encode %{\n-    __ movslq($dst$$Register, $mem$$Address);\n-  %}\n-\n-  ins_pipe(ialu_reg_mem);\n-%}\n-\n-\/\/ Load Integer with mask 0xFF into Long Register\n-instruct loadI2L_immI_255(rRegL dst, memory mem, immI_255 mask) %{\n-  match(Set dst (ConvI2L (AndI (LoadI mem) mask)));\n-\n-  format %{ \"movzbq  $dst, $mem\\t# int & 0xFF -> long\" %}\n-  ins_encode %{\n-    __ movzbq($dst$$Register, $mem$$Address);\n-  %}\n-  ins_pipe(ialu_reg_mem);\n-%}\n-\n-\/\/ Load Integer with mask 0xFFFF into Long Register\n-instruct loadI2L_immI_65535(rRegL dst, memory mem, immI_65535 mask) %{\n-  match(Set dst (ConvI2L (AndI (LoadI mem) mask)));\n-\n-  format %{ \"movzwq  $dst, $mem\\t# int & 0xFFFF -> long\" %}\n-  ins_encode %{\n-    __ movzwq($dst$$Register, $mem$$Address);\n-  %}\n-  ins_pipe(ialu_reg_mem);\n-%}\n-\n-\/\/ Load Integer with a 31-bit mask into Long Register\n-instruct loadI2L_immU31(rRegL dst, memory mem, immU31 mask, rFlagsReg cr) %{\n-  match(Set dst (ConvI2L (AndI (LoadI mem) mask)));\n-  effect(KILL cr);\n-\n-  format %{ \"movl    $dst, $mem\\t# int & 31-bit mask -> long\\n\\t\"\n-            \"andl    $dst, $mask\" %}\n-  ins_encode %{\n-    Register Rdst = $dst$$Register;\n-    __ movl(Rdst, $mem$$Address);\n-    __ andl(Rdst, $mask$$constant);\n-  %}\n-  ins_pipe(ialu_reg_mem);\n-%}\n-\n-\/\/ Load Unsigned Integer into Long Register\n-instruct loadUI2L(rRegL dst, memory mem, immL_32bits mask)\n-%{\n-  match(Set dst (AndL (ConvI2L (LoadI mem)) mask));\n-\n-  ins_cost(125);\n-  format %{ \"movl    $dst, $mem\\t# uint -> long\" %}\n-\n-  ins_encode %{\n-    __ movl($dst$$Register, $mem$$Address);\n-  %}\n-\n-  ins_pipe(ialu_reg_mem);\n-%}\n-\n-\/\/ Load Long\n-instruct loadL(rRegL dst, memory mem)\n-%{\n-  match(Set dst (LoadL mem));\n-\n-  ins_cost(125);\n-  format %{ \"movq    $dst, $mem\\t# long\" %}\n-\n-  ins_encode %{\n-    __ movq($dst$$Register, $mem$$Address);\n-  %}\n-\n-  ins_pipe(ialu_reg_mem); \/\/ XXX\n-%}\n-\n-\/\/ Load Range\n-instruct loadRange(rRegI dst, memory mem)\n-%{\n-  match(Set dst (LoadRange mem));\n-\n-  ins_cost(125); \/\/ XXX\n-  format %{ \"movl    $dst, $mem\\t# range\" %}\n-  ins_encode %{\n-    __ movl($dst$$Register, $mem$$Address);\n-  %}\n-  ins_pipe(ialu_reg_mem);\n-%}\n-\n-\/\/ Load Pointer\n-instruct loadP(rRegP dst, memory mem)\n-%{\n-  match(Set dst (LoadP mem));\n-  predicate(n->as_Load()->barrier_data() == 0);\n-\n-  ins_cost(125); \/\/ XXX\n-  format %{ \"movq    $dst, $mem\\t# ptr\" %}\n-  ins_encode %{\n-    __ movq($dst$$Register, $mem$$Address);\n-  %}\n-  ins_pipe(ialu_reg_mem); \/\/ XXX\n-%}\n-\n-\/\/ Load Compressed Pointer\n-instruct loadN(rRegN dst, memory mem)\n-%{\n-   predicate(n->as_Load()->barrier_data() == 0);\n-   match(Set dst (LoadN mem));\n-\n-   ins_cost(125); \/\/ XXX\n-   format %{ \"movl    $dst, $mem\\t# compressed ptr\" %}\n-   ins_encode %{\n-     __ movl($dst$$Register, $mem$$Address);\n-   %}\n-   ins_pipe(ialu_reg_mem); \/\/ XXX\n-%}\n-\n-\n-\/\/ Load Klass Pointer\n-instruct loadKlass(rRegP dst, memory mem)\n-%{\n-  match(Set dst (LoadKlass mem));\n-\n-  ins_cost(125); \/\/ XXX\n-  format %{ \"movq    $dst, $mem\\t# class\" %}\n-  ins_encode %{\n-    __ movq($dst$$Register, $mem$$Address);\n-  %}\n-  ins_pipe(ialu_reg_mem); \/\/ XXX\n-%}\n-\n-\/\/ Load narrow Klass Pointer\n-instruct loadNKlass(rRegN dst, memory mem)\n-%{\n-  predicate(!UseCompactObjectHeaders);\n-  match(Set dst (LoadNKlass mem));\n-\n-  ins_cost(125); \/\/ XXX\n-  format %{ \"movl    $dst, $mem\\t# compressed klass ptr\" %}\n-  ins_encode %{\n-    __ movl($dst$$Register, $mem$$Address);\n-  %}\n-  ins_pipe(ialu_reg_mem); \/\/ XXX\n-%}\n-\n-instruct loadNKlassCompactHeaders(rRegN dst, memory mem, rFlagsReg cr)\n-%{\n-  predicate(UseCompactObjectHeaders);\n-  match(Set dst (LoadNKlass mem));\n-  effect(KILL cr);\n-  ins_cost(125);\n-  format %{\n-    \"movl    $dst, $mem\\t# compressed klass ptr, shifted\\n\\t\"\n-    \"shrl    $dst, markWord::klass_shift_at_offset\"\n-  %}\n-  ins_encode %{\n-    if (UseAPX) {\n-      __ eshrl($dst$$Register, $mem$$Address, markWord::klass_shift_at_offset, false);\n-    }\n-    else {\n-      __ movl($dst$$Register, $mem$$Address);\n-      __ shrl($dst$$Register, markWord::klass_shift_at_offset);\n-    }\n-  %}\n-  ins_pipe(ialu_reg_mem);\n-%}\n-\n-\/\/ Load Float\n-instruct loadF(regF dst, memory mem)\n-%{\n-  match(Set dst (LoadF mem));\n-\n-  ins_cost(145); \/\/ XXX\n-  format %{ \"movss   $dst, $mem\\t# float\" %}\n-  ins_encode %{\n-    __ movflt($dst$$XMMRegister, $mem$$Address);\n-  %}\n-  ins_pipe(pipe_slow); \/\/ XXX\n-%}\n-\n-\/\/ Load Double\n-instruct loadD_partial(regD dst, memory mem)\n-%{\n-  predicate(!UseXmmLoadAndClearUpper);\n-  match(Set dst (LoadD mem));\n-\n-  ins_cost(145); \/\/ XXX\n-  format %{ \"movlpd  $dst, $mem\\t# double\" %}\n-  ins_encode %{\n-    __ movdbl($dst$$XMMRegister, $mem$$Address);\n-  %}\n-  ins_pipe(pipe_slow); \/\/ XXX\n-%}\n-\n-instruct loadD(regD dst, memory mem)\n-%{\n-  predicate(UseXmmLoadAndClearUpper);\n-  match(Set dst (LoadD mem));\n-\n-  ins_cost(145); \/\/ XXX\n-  format %{ \"movsd   $dst, $mem\\t# double\" %}\n-  ins_encode %{\n-    __ movdbl($dst$$XMMRegister, $mem$$Address);\n-  %}\n-  ins_pipe(pipe_slow); \/\/ XXX\n-%}\n-\n-\/\/ max = java.lang.Math.max(float a, float b)\n-instruct maxF_avx10_reg(regF dst, regF a, regF b) %{\n-  predicate(VM_Version::supports_avx10_2());\n-  match(Set dst (MaxF a b));\n-  format %{ \"maxF $dst, $a, $b\" %}\n-  ins_encode %{\n-    __ eminmaxss($dst$$XMMRegister, $a$$XMMRegister, $b$$XMMRegister, AVX10_MINMAX_MAX_COMPARE_SIGN);\n-  %}\n-  ins_pipe( pipe_slow );\n-%}\n-\n-\/\/ max = java.lang.Math.max(float a, float b)\n-instruct maxF_reg(legRegF dst, legRegF a, legRegF b, legRegF tmp, legRegF atmp, legRegF btmp) %{\n-  predicate(!VM_Version::supports_avx10_2() && UseAVX > 0 && !VLoopReductions::is_reduction(n));\n-  match(Set dst (MaxF a b));\n-  effect(USE a, USE b, TEMP tmp, TEMP atmp, TEMP btmp);\n-  format %{ \"maxF $dst, $a, $b \\t! using $tmp, $atmp and $btmp as TEMP\" %}\n-  ins_encode %{\n-    __ vminmax_fp(Op_MaxV, T_FLOAT, $dst$$XMMRegister, $a$$XMMRegister, $b$$XMMRegister, $tmp$$XMMRegister, $atmp$$XMMRegister, $btmp$$XMMRegister, Assembler::AVX_128bit);\n-  %}\n-  ins_pipe( pipe_slow );\n-%}\n-\n-instruct maxF_reduction_reg(legRegF dst, legRegF a, legRegF b, legRegF xtmp, rRegI rtmp, rFlagsReg cr) %{\n-  predicate(!VM_Version::supports_avx10_2() && UseAVX > 0 && VLoopReductions::is_reduction(n));\n-  match(Set dst (MaxF a b));\n-  effect(USE a, USE b, TEMP xtmp, TEMP rtmp, KILL cr);\n-\n-  format %{ \"maxF_reduction $dst, $a, $b \\t!using $xtmp and $rtmp as TEMP\" %}\n-  ins_encode %{\n-    emit_fp_min_max(masm, $dst$$XMMRegister, $a$$XMMRegister, $b$$XMMRegister, $xtmp$$XMMRegister, $rtmp$$Register,\n-                    false \/*min*\/, true \/*single*\/);\n-  %}\n-  ins_pipe( pipe_slow );\n-%}\n-\n-\/\/ max = java.lang.Math.max(double a, double b)\n-instruct maxD_avx10_reg(regD dst, regD a, regD b) %{\n-  predicate(VM_Version::supports_avx10_2());\n-  match(Set dst (MaxD a b));\n-  format %{ \"maxD $dst, $a, $b\" %}\n-  ins_encode %{\n-    __ eminmaxsd($dst$$XMMRegister, $a$$XMMRegister, $b$$XMMRegister, AVX10_MINMAX_MAX_COMPARE_SIGN);\n-  %}\n-  ins_pipe( pipe_slow );\n-%}\n-\n-\/\/ max = java.lang.Math.max(double a, double b)\n-instruct maxD_reg(legRegD dst, legRegD a, legRegD b, legRegD tmp, legRegD atmp, legRegD btmp) %{\n-  predicate(!VM_Version::supports_avx10_2() && UseAVX > 0 && !VLoopReductions::is_reduction(n));\n-  match(Set dst (MaxD a b));\n-  effect(USE a, USE b, TEMP atmp, TEMP btmp, TEMP tmp);\n-  format %{ \"maxD $dst, $a, $b \\t! using $tmp, $atmp and $btmp as TEMP\" %}\n-  ins_encode %{\n-    __ vminmax_fp(Op_MaxV, T_DOUBLE, $dst$$XMMRegister, $a$$XMMRegister, $b$$XMMRegister, $tmp$$XMMRegister, $atmp$$XMMRegister, $btmp$$XMMRegister, Assembler::AVX_128bit);\n-  %}\n-  ins_pipe( pipe_slow );\n-%}\n-\n-instruct maxD_reduction_reg(legRegD dst, legRegD a, legRegD b, legRegD xtmp, rRegL rtmp, rFlagsReg cr) %{\n-  predicate(!VM_Version::supports_avx10_2() && UseAVX > 0 && VLoopReductions::is_reduction(n));\n-  match(Set dst (MaxD a b));\n-  effect(USE a, USE b, TEMP xtmp, TEMP rtmp, KILL cr);\n-\n-  format %{ \"maxD_reduction $dst, $a, $b \\t! using $xtmp and $rtmp as TEMP\" %}\n-  ins_encode %{\n-    emit_fp_min_max(masm, $dst$$XMMRegister, $a$$XMMRegister, $b$$XMMRegister, $xtmp$$XMMRegister, $rtmp$$Register,\n-                    false \/*min*\/, false \/*single*\/);\n-  %}\n-  ins_pipe( pipe_slow );\n-%}\n-\n-\/\/ max = java.lang.Math.min(float a, float b)\n-instruct minF_avx10_reg(regF dst, regF a, regF b) %{\n-  predicate(VM_Version::supports_avx10_2());\n-  match(Set dst (MinF a b));\n-  format %{ \"minF $dst, $a, $b\" %}\n-  ins_encode %{\n-    __ eminmaxss($dst$$XMMRegister, $a$$XMMRegister, $b$$XMMRegister, AVX10_MINMAX_MIN_COMPARE_SIGN);\n-  %}\n-  ins_pipe( pipe_slow );\n-%}\n-\n-\/\/ min = java.lang.Math.min(float a, float b)\n-instruct minF_reg(legRegF dst, legRegF a, legRegF b, legRegF tmp, legRegF atmp, legRegF btmp) %{\n-  predicate(!VM_Version::supports_avx10_2() && UseAVX > 0 && !VLoopReductions::is_reduction(n));\n-  match(Set dst (MinF a b));\n-  effect(USE a, USE b, TEMP tmp, TEMP atmp, TEMP btmp);\n-  format %{ \"minF $dst, $a, $b \\t! using $tmp, $atmp and $btmp as TEMP\" %}\n-  ins_encode %{\n-    __ vminmax_fp(Op_MinV, T_FLOAT, $dst$$XMMRegister, $a$$XMMRegister, $b$$XMMRegister, $tmp$$XMMRegister, $atmp$$XMMRegister, $btmp$$XMMRegister, Assembler::AVX_128bit);\n-  %}\n-  ins_pipe( pipe_slow );\n-%}\n-\n-instruct minF_reduction_reg(legRegF dst, legRegF a, legRegF b, legRegF xtmp, rRegI rtmp, rFlagsReg cr) %{\n-  predicate(!VM_Version::supports_avx10_2() && UseAVX > 0 && VLoopReductions::is_reduction(n));\n-  match(Set dst (MinF a b));\n-  effect(USE a, USE b, TEMP xtmp, TEMP rtmp, KILL cr);\n-\n-  format %{ \"minF_reduction $dst, $a, $b \\t! using $xtmp and $rtmp as TEMP\" %}\n-  ins_encode %{\n-    emit_fp_min_max(masm, $dst$$XMMRegister, $a$$XMMRegister, $b$$XMMRegister, $xtmp$$XMMRegister, $rtmp$$Register,\n-                    true \/*min*\/, true \/*single*\/);\n-  %}\n-  ins_pipe( pipe_slow );\n-%}\n-\n-\/\/ max = java.lang.Math.min(double a, double b)\n-instruct minD_avx10_reg(regD dst, regD a, regD b) %{\n-  predicate(VM_Version::supports_avx10_2());\n-  match(Set dst (MinD a b));\n-  format %{ \"minD $dst, $a, $b\" %}\n-  ins_encode %{\n-    __ eminmaxsd($dst$$XMMRegister, $a$$XMMRegister, $b$$XMMRegister, AVX10_MINMAX_MIN_COMPARE_SIGN);\n-  %}\n-  ins_pipe( pipe_slow );\n-%}\n-\n-\/\/ min = java.lang.Math.min(double a, double b)\n-instruct minD_reg(legRegD dst, legRegD a, legRegD b, legRegD tmp, legRegD atmp, legRegD btmp) %{\n-  predicate(!VM_Version::supports_avx10_2() && UseAVX > 0 && !VLoopReductions::is_reduction(n));\n-  match(Set dst (MinD a b));\n-  effect(USE a, USE b, TEMP tmp, TEMP atmp, TEMP btmp);\n-    format %{ \"minD $dst, $a, $b \\t! using $tmp, $atmp and $btmp as TEMP\" %}\n-  ins_encode %{\n-    __ vminmax_fp(Op_MinV, T_DOUBLE, $dst$$XMMRegister, $a$$XMMRegister, $b$$XMMRegister, $tmp$$XMMRegister, $atmp$$XMMRegister, $btmp$$XMMRegister, Assembler::AVX_128bit);\n-  %}\n-  ins_pipe( pipe_slow );\n-%}\n-\n-instruct minD_reduction_reg(legRegD dst, legRegD a, legRegD b, legRegD xtmp, rRegL rtmp, rFlagsReg cr) %{\n-  predicate(!VM_Version::supports_avx10_2() && UseAVX > 0 && VLoopReductions::is_reduction(n));\n-  match(Set dst (MinD a b));\n-  effect(USE a, USE b, TEMP xtmp, TEMP rtmp, KILL cr);\n-\n-  format %{ \"maxD_reduction $dst, $a, $b \\t! using $xtmp and $rtmp as TEMP\" %}\n-  ins_encode %{\n-    emit_fp_min_max(masm, $dst$$XMMRegister, $a$$XMMRegister, $b$$XMMRegister, $xtmp$$XMMRegister, $rtmp$$Register,\n-                    true \/*min*\/, false \/*single*\/);\n-  %}\n-  ins_pipe( pipe_slow );\n-%}\n-\n-\/\/ Load Effective Address\n-instruct leaP8(rRegP dst, indOffset8 mem)\n-%{\n-  match(Set dst mem);\n-\n-  ins_cost(110); \/\/ XXX\n-  format %{ \"leaq    $dst, $mem\\t# ptr 8\" %}\n-  ins_encode %{\n-    __ leaq($dst$$Register, $mem$$Address);\n-  %}\n-  ins_pipe(ialu_reg_reg_fat);\n-%}\n-\n-instruct leaP32(rRegP dst, indOffset32 mem)\n-%{\n-  match(Set dst mem);\n-\n-  ins_cost(110);\n-  format %{ \"leaq    $dst, $mem\\t# ptr 32\" %}\n-  ins_encode %{\n-    __ leaq($dst$$Register, $mem$$Address);\n-  %}\n-  ins_pipe(ialu_reg_reg_fat);\n-%}\n-\n-instruct leaPIdxOff(rRegP dst, indIndexOffset mem)\n-%{\n-  match(Set dst mem);\n-\n-  ins_cost(110);\n-  format %{ \"leaq    $dst, $mem\\t# ptr idxoff\" %}\n-  ins_encode %{\n-    __ leaq($dst$$Register, $mem$$Address);\n-  %}\n-  ins_pipe(ialu_reg_reg_fat);\n-%}\n-\n-instruct leaPIdxScale(rRegP dst, indIndexScale mem)\n-%{\n-  match(Set dst mem);\n-\n-  ins_cost(110);\n-  format %{ \"leaq    $dst, $mem\\t# ptr idxscale\" %}\n-  ins_encode %{\n-    __ leaq($dst$$Register, $mem$$Address);\n-  %}\n-  ins_pipe(ialu_reg_reg_fat);\n-%}\n-\n-instruct leaPPosIdxScale(rRegP dst, indPosIndexScale mem)\n-%{\n-  match(Set dst mem);\n-\n-  ins_cost(110);\n-  format %{ \"leaq    $dst, $mem\\t# ptr idxscale\" %}\n-  ins_encode %{\n-    __ leaq($dst$$Register, $mem$$Address);\n-  %}\n-  ins_pipe(ialu_reg_reg_fat);\n-%}\n-\n-instruct leaPIdxScaleOff(rRegP dst, indIndexScaleOffset mem)\n-%{\n-  match(Set dst mem);\n-\n-  ins_cost(110);\n-  format %{ \"leaq    $dst, $mem\\t# ptr idxscaleoff\" %}\n-  ins_encode %{\n-    __ leaq($dst$$Register, $mem$$Address);\n-  %}\n-  ins_pipe(ialu_reg_reg_fat);\n-%}\n-\n-instruct leaPPosIdxOff(rRegP dst, indPosIndexOffset mem)\n-%{\n-  match(Set dst mem);\n-\n-  ins_cost(110);\n-  format %{ \"leaq    $dst, $mem\\t# ptr posidxoff\" %}\n-  ins_encode %{\n-    __ leaq($dst$$Register, $mem$$Address);\n-  %}\n-  ins_pipe(ialu_reg_reg_fat);\n-%}\n-\n-instruct leaPPosIdxScaleOff(rRegP dst, indPosIndexScaleOffset mem)\n-%{\n-  match(Set dst mem);\n-\n-  ins_cost(110);\n-  format %{ \"leaq    $dst, $mem\\t# ptr posidxscaleoff\" %}\n-  ins_encode %{\n-    __ leaq($dst$$Register, $mem$$Address);\n-  %}\n-  ins_pipe(ialu_reg_reg_fat);\n-%}\n-\n-\/\/ Load Effective Address which uses Narrow (32-bits) oop\n-instruct leaPCompressedOopOffset(rRegP dst, indCompressedOopOffset mem)\n-%{\n-  predicate(UseCompressedOops && (CompressedOops::shift() != 0));\n-  match(Set dst mem);\n-\n-  ins_cost(110);\n-  format %{ \"leaq    $dst, $mem\\t# ptr compressedoopoff32\" %}\n-  ins_encode %{\n-    __ leaq($dst$$Register, $mem$$Address);\n-  %}\n-  ins_pipe(ialu_reg_reg_fat);\n-%}\n-\n-instruct leaP8Narrow(rRegP dst, indOffset8Narrow mem)\n-%{\n-  predicate(CompressedOops::shift() == 0);\n-  match(Set dst mem);\n-\n-  ins_cost(110); \/\/ XXX\n-  format %{ \"leaq    $dst, $mem\\t# ptr off8narrow\" %}\n-  ins_encode %{\n-    __ leaq($dst$$Register, $mem$$Address);\n-  %}\n-  ins_pipe(ialu_reg_reg_fat);\n-%}\n-\n-instruct leaP32Narrow(rRegP dst, indOffset32Narrow mem)\n-%{\n-  predicate(CompressedOops::shift() == 0);\n-  match(Set dst mem);\n-\n-  ins_cost(110);\n-  format %{ \"leaq    $dst, $mem\\t# ptr off32narrow\" %}\n-  ins_encode %{\n-    __ leaq($dst$$Register, $mem$$Address);\n-  %}\n-  ins_pipe(ialu_reg_reg_fat);\n-%}\n-\n-instruct leaPIdxOffNarrow(rRegP dst, indIndexOffsetNarrow mem)\n-%{\n-  predicate(CompressedOops::shift() == 0);\n-  match(Set dst mem);\n-\n-  ins_cost(110);\n-  format %{ \"leaq    $dst, $mem\\t# ptr idxoffnarrow\" %}\n-  ins_encode %{\n-    __ leaq($dst$$Register, $mem$$Address);\n-  %}\n-  ins_pipe(ialu_reg_reg_fat);\n-%}\n-\n-instruct leaPIdxScaleNarrow(rRegP dst, indIndexScaleNarrow mem)\n-%{\n-  predicate(CompressedOops::shift() == 0);\n-  match(Set dst mem);\n-\n-  ins_cost(110);\n-  format %{ \"leaq    $dst, $mem\\t# ptr idxscalenarrow\" %}\n-  ins_encode %{\n-    __ leaq($dst$$Register, $mem$$Address);\n-  %}\n-  ins_pipe(ialu_reg_reg_fat);\n-%}\n-\n-instruct leaPIdxScaleOffNarrow(rRegP dst, indIndexScaleOffsetNarrow mem)\n-%{\n-  predicate(CompressedOops::shift() == 0);\n-  match(Set dst mem);\n-\n-  ins_cost(110);\n-  format %{ \"leaq    $dst, $mem\\t# ptr idxscaleoffnarrow\" %}\n-  ins_encode %{\n-    __ leaq($dst$$Register, $mem$$Address);\n-  %}\n-  ins_pipe(ialu_reg_reg_fat);\n-%}\n-\n-instruct leaPPosIdxOffNarrow(rRegP dst, indPosIndexOffsetNarrow mem)\n-%{\n-  predicate(CompressedOops::shift() == 0);\n-  match(Set dst mem);\n-\n-  ins_cost(110);\n-  format %{ \"leaq    $dst, $mem\\t# ptr posidxoffnarrow\" %}\n-  ins_encode %{\n-    __ leaq($dst$$Register, $mem$$Address);\n-  %}\n-  ins_pipe(ialu_reg_reg_fat);\n-%}\n-\n-instruct leaPPosIdxScaleOffNarrow(rRegP dst, indPosIndexScaleOffsetNarrow mem)\n-%{\n-  predicate(CompressedOops::shift() == 0);\n-  match(Set dst mem);\n-\n-  ins_cost(110);\n-  format %{ \"leaq    $dst, $mem\\t# ptr posidxscaleoffnarrow\" %}\n-  ins_encode %{\n-    __ leaq($dst$$Register, $mem$$Address);\n-  %}\n-  ins_pipe(ialu_reg_reg_fat);\n-%}\n-\n-instruct loadConI(rRegI dst, immI src)\n-%{\n-  match(Set dst src);\n-\n-  format %{ \"movl    $dst, $src\\t# int\" %}\n-  ins_encode %{\n-    __ movl($dst$$Register, $src$$constant);\n-  %}\n-  ins_pipe(ialu_reg_fat); \/\/ XXX\n-%}\n-\n-instruct loadConI0(rRegI dst, immI_0 src, rFlagsReg cr)\n-%{\n-  match(Set dst src);\n-  effect(KILL cr);\n-\n-  ins_cost(50);\n-  format %{ \"xorl    $dst, $dst\\t# int\" %}\n-  ins_encode %{\n-    __ xorl($dst$$Register, $dst$$Register);\n-  %}\n-  ins_pipe(ialu_reg);\n-%}\n-\n-instruct loadConL(rRegL dst, immL src)\n-%{\n-  match(Set dst src);\n-\n-  ins_cost(150);\n-  format %{ \"movq    $dst, $src\\t# long\" %}\n-  ins_encode %{\n-    __ mov64($dst$$Register, $src$$constant);\n-  %}\n-  ins_pipe(ialu_reg);\n-%}\n-\n-instruct loadConL0(rRegL dst, immL0 src, rFlagsReg cr)\n-%{\n-  match(Set dst src);\n-  effect(KILL cr);\n-\n-  ins_cost(50);\n-  format %{ \"xorl    $dst, $dst\\t# long\" %}\n-  ins_encode %{\n-    __ xorl($dst$$Register, $dst$$Register);\n-  %}\n-  ins_pipe(ialu_reg); \/\/ XXX\n-%}\n-\n-instruct loadConUL32(rRegL dst, immUL32 src)\n-%{\n-  match(Set dst src);\n-\n-  ins_cost(60);\n-  format %{ \"movl    $dst, $src\\t# long (unsigned 32-bit)\" %}\n-  ins_encode %{\n-    __ movl($dst$$Register, $src$$constant);\n-  %}\n-  ins_pipe(ialu_reg);\n-%}\n-\n-instruct loadConL32(rRegL dst, immL32 src)\n-%{\n-  match(Set dst src);\n-\n-  ins_cost(70);\n-  format %{ \"movq    $dst, $src\\t# long (32-bit)\" %}\n-  ins_encode %{\n-    __ movq($dst$$Register, $src$$constant);\n-  %}\n-  ins_pipe(ialu_reg);\n-%}\n-\n-instruct loadConP(rRegP dst, immP con) %{\n-  match(Set dst con);\n-\n-  format %{ \"movq    $dst, $con\\t# ptr\" %}\n-  ins_encode %{\n-    __ mov64($dst$$Register, $con$$constant, $con->constant_reloc(), RELOC_IMM64);\n-  %}\n-  ins_pipe(ialu_reg_fat); \/\/ XXX\n-%}\n-\n-instruct loadConP0(rRegP dst, immP0 src, rFlagsReg cr)\n-%{\n-  match(Set dst src);\n-  effect(KILL cr);\n-\n-  ins_cost(50);\n-  format %{ \"xorl    $dst, $dst\\t# ptr\" %}\n-  ins_encode %{\n-    __ xorl($dst$$Register, $dst$$Register);\n-  %}\n-  ins_pipe(ialu_reg);\n-%}\n-\n-instruct loadConP31(rRegP dst, immP31 src, rFlagsReg cr)\n-%{\n-  match(Set dst src);\n-  effect(KILL cr);\n-\n-  ins_cost(60);\n-  format %{ \"movl    $dst, $src\\t# ptr (positive 32-bit)\" %}\n-  ins_encode %{\n-    __ movl($dst$$Register, $src$$constant);\n-  %}\n-  ins_pipe(ialu_reg);\n-%}\n-\n-instruct loadConF(regF dst, immF con) %{\n-  match(Set dst con);\n-  ins_cost(125);\n-  format %{ \"movss   $dst, [$constantaddress]\\t# load from constant table: float=$con\" %}\n-  ins_encode %{\n-    __ movflt($dst$$XMMRegister, $constantaddress($con));\n-  %}\n-  ins_pipe(pipe_slow);\n-%}\n-\n-instruct loadConH(regF dst, immH con) %{\n-  match(Set dst con);\n-  ins_cost(125);\n-  format %{ \"movss   $dst, [$constantaddress]\\t# load from constant table: halffloat=$con\" %}\n-  ins_encode %{\n-    __ movflt($dst$$XMMRegister, $constantaddress($con));\n-  %}\n-  ins_pipe(pipe_slow);\n-%}\n-\n-instruct loadConN0(rRegN dst, immN0 src, rFlagsReg cr) %{\n-  match(Set dst src);\n-  effect(KILL cr);\n-  format %{ \"xorq    $dst, $src\\t# compressed null pointer\" %}\n-  ins_encode %{\n-    __ xorq($dst$$Register, $dst$$Register);\n-  %}\n-  ins_pipe(ialu_reg);\n-%}\n-\n-instruct loadConN(rRegN dst, immN src) %{\n-  match(Set dst src);\n-\n-  ins_cost(125);\n-  format %{ \"movl    $dst, $src\\t# compressed ptr\" %}\n-  ins_encode %{\n-    address con = (address)$src$$constant;\n-    if (con == nullptr) {\n-      ShouldNotReachHere();\n-    } else {\n-      __ set_narrow_oop($dst$$Register, (jobject)$src$$constant);\n-    }\n-  %}\n-  ins_pipe(ialu_reg_fat); \/\/ XXX\n-%}\n-\n-instruct loadConNKlass(rRegN dst, immNKlass src) %{\n-  match(Set dst src);\n-\n-  ins_cost(125);\n-  format %{ \"movl    $dst, $src\\t# compressed klass ptr\" %}\n-  ins_encode %{\n-    address con = (address)$src$$constant;\n-    if (con == nullptr) {\n-      ShouldNotReachHere();\n-    } else {\n-      __ set_narrow_klass($dst$$Register, (Klass*)$src$$constant);\n-    }\n-  %}\n-  ins_pipe(ialu_reg_fat); \/\/ XXX\n-%}\n-\n-instruct loadConF0(regF dst, immF0 src)\n-%{\n-  match(Set dst src);\n-  ins_cost(100);\n-\n-  format %{ \"xorps   $dst, $dst\\t# float 0.0\" %}\n-  ins_encode %{\n-    __ xorps($dst$$XMMRegister, $dst$$XMMRegister);\n-  %}\n-  ins_pipe(pipe_slow);\n-%}\n-\n-\/\/ Use the same format since predicate() can not be used here.\n-instruct loadConD(regD dst, immD con) %{\n-  match(Set dst con);\n-  ins_cost(125);\n-  format %{ \"movsd   $dst, [$constantaddress]\\t# load from constant table: double=$con\" %}\n-  ins_encode %{\n-    __ movdbl($dst$$XMMRegister, $constantaddress($con));\n-  %}\n-  ins_pipe(pipe_slow);\n-%}\n-\n-instruct loadConD0(regD dst, immD0 src)\n-%{\n-  match(Set dst src);\n-  ins_cost(100);\n-\n-  format %{ \"xorpd   $dst, $dst\\t# double 0.0\" %}\n-  ins_encode %{\n-    __ xorpd($dst$$XMMRegister, $dst$$XMMRegister);\n-  %}\n-  ins_pipe(pipe_slow);\n-%}\n-\n-instruct loadSSI(rRegI dst, stackSlotI src)\n-%{\n-  match(Set dst src);\n-\n-  ins_cost(125);\n-  format %{ \"movl    $dst, $src\\t# int stk\" %}\n-  ins_encode %{\n-    __ movl($dst$$Register, $src$$Address);\n-  %}\n-  ins_pipe(ialu_reg_mem);\n-%}\n-\n-instruct loadSSL(rRegL dst, stackSlotL src)\n-%{\n-  match(Set dst src);\n-\n-  ins_cost(125);\n-  format %{ \"movq    $dst, $src\\t# long stk\" %}\n-  ins_encode %{\n-    __ movq($dst$$Register, $src$$Address);\n-  %}\n-  ins_pipe(ialu_reg_mem);\n-%}\n-\n-instruct loadSSP(rRegP dst, stackSlotP src)\n-%{\n-  match(Set dst src);\n-\n-  ins_cost(125);\n-  format %{ \"movq    $dst, $src\\t# ptr stk\" %}\n-  ins_encode %{\n-    __ movq($dst$$Register, $src$$Address);\n-  %}\n-  ins_pipe(ialu_reg_mem);\n-%}\n-\n-instruct loadSSF(regF dst, stackSlotF src)\n-%{\n-  match(Set dst src);\n-\n-  ins_cost(125);\n-  format %{ \"movss   $dst, $src\\t# float stk\" %}\n-  ins_encode %{\n-    __ movflt($dst$$XMMRegister, Address(rsp, $src$$disp));\n-  %}\n-  ins_pipe(pipe_slow); \/\/ XXX\n-%}\n-\n-\/\/ Use the same format since predicate() can not be used here.\n-instruct loadSSD(regD dst, stackSlotD src)\n-%{\n-  match(Set dst src);\n-\n-  ins_cost(125);\n-  format %{ \"movsd   $dst, $src\\t# double stk\" %}\n-  ins_encode  %{\n-    __ movdbl($dst$$XMMRegister, Address(rsp, $src$$disp));\n-  %}\n-  ins_pipe(pipe_slow); \/\/ XXX\n-%}\n-\n-\/\/ Prefetch instructions for allocation.\n-\/\/ Must be safe to execute with invalid address (cannot fault).\n-\n-instruct prefetchAlloc( memory mem ) %{\n-  predicate(AllocatePrefetchInstr==3);\n-  match(PrefetchAllocation mem);\n-  ins_cost(125);\n-\n-  format %{ \"PREFETCHW $mem\\t# Prefetch allocation into level 1 cache and mark modified\" %}\n-  ins_encode %{\n-    __ prefetchw($mem$$Address);\n-  %}\n-  ins_pipe(ialu_mem);\n-%}\n-\n-instruct prefetchAllocNTA( memory mem ) %{\n-  predicate(AllocatePrefetchInstr==0);\n-  match(PrefetchAllocation mem);\n-  ins_cost(125);\n-\n-  format %{ \"PREFETCHNTA $mem\\t# Prefetch allocation to non-temporal cache for write\" %}\n-  ins_encode %{\n-    __ prefetchnta($mem$$Address);\n-  %}\n-  ins_pipe(ialu_mem);\n-%}\n-\n-instruct prefetchAllocT0( memory mem ) %{\n-  predicate(AllocatePrefetchInstr==1);\n-  match(PrefetchAllocation mem);\n-  ins_cost(125);\n-\n-  format %{ \"PREFETCHT0 $mem\\t# Prefetch allocation to level 1 and 2 caches for write\" %}\n-  ins_encode %{\n-    __ prefetcht0($mem$$Address);\n-  %}\n-  ins_pipe(ialu_mem);\n-%}\n-\n-instruct prefetchAllocT2( memory mem ) %{\n-  predicate(AllocatePrefetchInstr==2);\n-  match(PrefetchAllocation mem);\n-  ins_cost(125);\n-\n-  format %{ \"PREFETCHT2 $mem\\t# Prefetch allocation to level 2 cache for write\" %}\n-  ins_encode %{\n-    __ prefetcht2($mem$$Address);\n-  %}\n-  ins_pipe(ialu_mem);\n-%}\n-\n-\/\/----------Store Instructions-------------------------------------------------\n-\n-\/\/ Store Byte\n-instruct storeB(memory mem, rRegI src)\n-%{\n-  match(Set mem (StoreB mem src));\n-\n-  ins_cost(125); \/\/ XXX\n-  format %{ \"movb    $mem, $src\\t# byte\" %}\n-  ins_encode %{\n-    __ movb($mem$$Address, $src$$Register);\n-  %}\n-  ins_pipe(ialu_mem_reg);\n-%}\n-\n-\/\/ Store Char\/Short\n-instruct storeC(memory mem, rRegI src)\n-%{\n-  match(Set mem (StoreC mem src));\n-\n-  ins_cost(125); \/\/ XXX\n-  format %{ \"movw    $mem, $src\\t# char\/short\" %}\n-  ins_encode %{\n-    __ movw($mem$$Address, $src$$Register);\n-  %}\n-  ins_pipe(ialu_mem_reg);\n-%}\n-\n-\/\/ Store Integer\n-instruct storeI(memory mem, rRegI src)\n-%{\n-  match(Set mem (StoreI mem src));\n-\n-  ins_cost(125); \/\/ XXX\n-  format %{ \"movl    $mem, $src\\t# int\" %}\n-  ins_encode %{\n-    __ movl($mem$$Address, $src$$Register);\n-  %}\n-  ins_pipe(ialu_mem_reg);\n-%}\n-\n-\/\/ Store Long\n-instruct storeL(memory mem, rRegL src)\n-%{\n-  match(Set mem (StoreL mem src));\n-\n-  ins_cost(125); \/\/ XXX\n-  format %{ \"movq    $mem, $src\\t# long\" %}\n-  ins_encode %{\n-    __ movq($mem$$Address, $src$$Register);\n-  %}\n-  ins_pipe(ialu_mem_reg); \/\/ XXX\n-%}\n-\n-\/\/ Store Pointer\n-instruct storeP(memory mem, any_RegP src)\n-%{\n-  predicate(n->as_Store()->barrier_data() == 0);\n-  match(Set mem (StoreP mem src));\n-\n-  ins_cost(125); \/\/ XXX\n-  format %{ \"movq    $mem, $src\\t# ptr\" %}\n-  ins_encode %{\n-    __ movq($mem$$Address, $src$$Register);\n-  %}\n-  ins_pipe(ialu_mem_reg);\n-%}\n-\n-instruct storeImmP0(memory mem, immP0 zero)\n-%{\n-  predicate(UseCompressedOops && (CompressedOops::base() == nullptr) && n->as_Store()->barrier_data() == 0);\n-  match(Set mem (StoreP mem zero));\n-\n-  ins_cost(125); \/\/ XXX\n-  format %{ \"movq    $mem, R12\\t# ptr (R12_heapbase==0)\" %}\n-  ins_encode %{\n-    __ movq($mem$$Address, r12);\n-  %}\n-  ins_pipe(ialu_mem_reg);\n-%}\n-\n-\/\/ Store Null Pointer, mark word, or other simple pointer constant.\n-instruct storeImmP(memory mem, immP31 src)\n-%{\n-  predicate(n->as_Store()->barrier_data() == 0);\n-  match(Set mem (StoreP mem src));\n-\n-  ins_cost(150); \/\/ XXX\n-  format %{ \"movq    $mem, $src\\t# ptr\" %}\n-  ins_encode %{\n-    __ movq($mem$$Address, $src$$constant);\n-  %}\n-  ins_pipe(ialu_mem_imm);\n-%}\n-\n-\/\/ Store Compressed Pointer\n-instruct storeN(memory mem, rRegN src)\n-%{\n-  predicate(n->as_Store()->barrier_data() == 0);\n-  match(Set mem (StoreN mem src));\n-\n-  ins_cost(125); \/\/ XXX\n-  format %{ \"movl    $mem, $src\\t# compressed ptr\" %}\n-  ins_encode %{\n-    __ movl($mem$$Address, $src$$Register);\n-  %}\n-  ins_pipe(ialu_mem_reg);\n-%}\n-\n-instruct storeNKlass(memory mem, rRegN src)\n-%{\n-  match(Set mem (StoreNKlass mem src));\n-\n-  ins_cost(125); \/\/ XXX\n-  format %{ \"movl    $mem, $src\\t# compressed klass ptr\" %}\n-  ins_encode %{\n-    __ movl($mem$$Address, $src$$Register);\n-  %}\n-  ins_pipe(ialu_mem_reg);\n-%}\n-\n-instruct storeImmN0(memory mem, immN0 zero)\n-%{\n-  predicate(CompressedOops::base() == nullptr && n->as_Store()->barrier_data() == 0);\n-  match(Set mem (StoreN mem zero));\n-\n-  ins_cost(125); \/\/ XXX\n-  format %{ \"movl    $mem, R12\\t# compressed ptr (R12_heapbase==0)\" %}\n-  ins_encode %{\n-    __ movl($mem$$Address, r12);\n-  %}\n-  ins_pipe(ialu_mem_reg);\n-%}\n-\n-instruct storeImmN(memory mem, immN src)\n-%{\n-  predicate(n->as_Store()->barrier_data() == 0);\n-  match(Set mem (StoreN mem src));\n-\n-  ins_cost(150); \/\/ XXX\n-  format %{ \"movl    $mem, $src\\t# compressed ptr\" %}\n-  ins_encode %{\n-    address con = (address)$src$$constant;\n-    if (con == nullptr) {\n-      __ movl($mem$$Address, 0);\n-    } else {\n-      __ set_narrow_oop($mem$$Address, (jobject)$src$$constant);\n-    }\n-  %}\n-  ins_pipe(ialu_mem_imm);\n-%}\n-\n-instruct storeImmNKlass(memory mem, immNKlass src)\n-%{\n-  match(Set mem (StoreNKlass mem src));\n-\n-  ins_cost(150); \/\/ XXX\n-  format %{ \"movl    $mem, $src\\t# compressed klass ptr\" %}\n-  ins_encode %{\n-    __ set_narrow_klass($mem$$Address, (Klass*)$src$$constant);\n-  %}\n-  ins_pipe(ialu_mem_imm);\n-%}\n-\n-\/\/ Store Integer Immediate\n-instruct storeImmI0(memory mem, immI_0 zero)\n-%{\n-  predicate(UseCompressedOops && (CompressedOops::base() == nullptr));\n-  match(Set mem (StoreI mem zero));\n-\n-  ins_cost(125); \/\/ XXX\n-  format %{ \"movl    $mem, R12\\t# int (R12_heapbase==0)\" %}\n-  ins_encode %{\n-    __ movl($mem$$Address, r12);\n-  %}\n-  ins_pipe(ialu_mem_reg);\n-%}\n-\n-instruct storeImmI(memory mem, immI src)\n-%{\n-  match(Set mem (StoreI mem src));\n-\n-  ins_cost(150);\n-  format %{ \"movl    $mem, $src\\t# int\" %}\n-  ins_encode %{\n-    __ movl($mem$$Address, $src$$constant);\n-  %}\n-  ins_pipe(ialu_mem_imm);\n-%}\n-\n-\/\/ Store Long Immediate\n-instruct storeImmL0(memory mem, immL0 zero)\n-%{\n-  predicate(UseCompressedOops && (CompressedOops::base() == nullptr));\n-  match(Set mem (StoreL mem zero));\n-\n-  ins_cost(125); \/\/ XXX\n-  format %{ \"movq    $mem, R12\\t# long (R12_heapbase==0)\" %}\n-  ins_encode %{\n-    __ movq($mem$$Address, r12);\n-  %}\n-  ins_pipe(ialu_mem_reg);\n-%}\n-\n-instruct storeImmL(memory mem, immL32 src)\n-%{\n-  match(Set mem (StoreL mem src));\n-\n-  ins_cost(150);\n-  format %{ \"movq    $mem, $src\\t# long\" %}\n-  ins_encode %{\n-    __ movq($mem$$Address, $src$$constant);\n-  %}\n-  ins_pipe(ialu_mem_imm);\n-%}\n-\n-\/\/ Store Short\/Char Immediate\n-instruct storeImmC0(memory mem, immI_0 zero)\n-%{\n-  predicate(UseCompressedOops && (CompressedOops::base() == nullptr));\n-  match(Set mem (StoreC mem zero));\n-\n-  ins_cost(125); \/\/ XXX\n-  format %{ \"movw    $mem, R12\\t# short\/char (R12_heapbase==0)\" %}\n-  ins_encode %{\n-    __ movw($mem$$Address, r12);\n-  %}\n-  ins_pipe(ialu_mem_reg);\n-%}\n-\n-instruct storeImmI16(memory mem, immI16 src)\n-%{\n-  predicate(UseStoreImmI16);\n-  match(Set mem (StoreC mem src));\n-\n-  ins_cost(150);\n-  format %{ \"movw    $mem, $src\\t# short\/char\" %}\n-  ins_encode %{\n-    __ movw($mem$$Address, $src$$constant);\n-  %}\n-  ins_pipe(ialu_mem_imm);\n-%}\n-\n-\/\/ Store Byte Immediate\n-instruct storeImmB0(memory mem, immI_0 zero)\n-%{\n-  predicate(UseCompressedOops && (CompressedOops::base() == nullptr));\n-  match(Set mem (StoreB mem zero));\n-\n-  ins_cost(125); \/\/ XXX\n-  format %{ \"movb    $mem, R12\\t# short\/char (R12_heapbase==0)\" %}\n-  ins_encode %{\n-    __ movb($mem$$Address, r12);\n-  %}\n-  ins_pipe(ialu_mem_reg);\n-%}\n-\n-instruct storeImmB(memory mem, immI8 src)\n-%{\n-  match(Set mem (StoreB mem src));\n-\n-  ins_cost(150); \/\/ XXX\n-  format %{ \"movb    $mem, $src\\t# byte\" %}\n-  ins_encode %{\n-    __ movb($mem$$Address, $src$$constant);\n-  %}\n-  ins_pipe(ialu_mem_imm);\n-%}\n-\n-\/\/ Store Float\n-instruct storeF(memory mem, regF src)\n-%{\n-  match(Set mem (StoreF mem src));\n-\n-  ins_cost(95); \/\/ XXX\n-  format %{ \"movss   $mem, $src\\t# float\" %}\n-  ins_encode %{\n-    __ movflt($mem$$Address, $src$$XMMRegister);\n-  %}\n-  ins_pipe(pipe_slow); \/\/ XXX\n-%}\n-\n-\/\/ Store immediate Float value (it is faster than store from XMM register)\n-instruct storeF0(memory mem, immF0 zero)\n-%{\n-  predicate(UseCompressedOops && (CompressedOops::base() == nullptr));\n-  match(Set mem (StoreF mem zero));\n-\n-  ins_cost(25); \/\/ XXX\n-  format %{ \"movl    $mem, R12\\t# float 0. (R12_heapbase==0)\" %}\n-  ins_encode %{\n-    __ movl($mem$$Address, r12);\n-  %}\n-  ins_pipe(ialu_mem_reg);\n-%}\n-\n-instruct storeF_imm(memory mem, immF src)\n-%{\n-  match(Set mem (StoreF mem src));\n-\n-  ins_cost(50);\n-  format %{ \"movl    $mem, $src\\t# float\" %}\n-  ins_encode %{\n-    __ movl($mem$$Address, jint_cast($src$$constant));\n-  %}\n-  ins_pipe(ialu_mem_imm);\n-%}\n-\n-\/\/ Store Double\n-instruct storeD(memory mem, regD src)\n-%{\n-  match(Set mem (StoreD mem src));\n-\n-  ins_cost(95); \/\/ XXX\n-  format %{ \"movsd   $mem, $src\\t# double\" %}\n-  ins_encode %{\n-    __ movdbl($mem$$Address, $src$$XMMRegister);\n-  %}\n-  ins_pipe(pipe_slow); \/\/ XXX\n-%}\n-\n-\/\/ Store immediate double 0.0 (it is faster than store from XMM register)\n-instruct storeD0_imm(memory mem, immD0 src)\n-%{\n-  predicate(!UseCompressedOops || (CompressedOops::base() != nullptr));\n-  match(Set mem (StoreD mem src));\n-\n-  ins_cost(50);\n-  format %{ \"movq    $mem, $src\\t# double 0.\" %}\n-  ins_encode %{\n-    __ movq($mem$$Address, $src$$constant);\n-  %}\n-  ins_pipe(ialu_mem_imm);\n-%}\n-\n-instruct storeD0(memory mem, immD0 zero)\n-%{\n-  predicate(UseCompressedOops && (CompressedOops::base() == nullptr));\n-  match(Set mem (StoreD mem zero));\n-\n-  ins_cost(25); \/\/ XXX\n-  format %{ \"movq    $mem, R12\\t# double 0. (R12_heapbase==0)\" %}\n-  ins_encode %{\n-    __ movq($mem$$Address, r12);\n-  %}\n-  ins_pipe(ialu_mem_reg);\n-%}\n-\n-instruct storeSSI(stackSlotI dst, rRegI src)\n-%{\n-  match(Set dst src);\n-\n-  ins_cost(100);\n-  format %{ \"movl    $dst, $src\\t# int stk\" %}\n-  ins_encode %{\n-    __ movl($dst$$Address, $src$$Register);\n-  %}\n-  ins_pipe( ialu_mem_reg );\n-%}\n-\n-instruct storeSSL(stackSlotL dst, rRegL src)\n-%{\n-  match(Set dst src);\n-\n-  ins_cost(100);\n-  format %{ \"movq    $dst, $src\\t# long stk\" %}\n-  ins_encode %{\n-    __ movq($dst$$Address, $src$$Register);\n-  %}\n-  ins_pipe(ialu_mem_reg);\n-%}\n-\n-instruct storeSSP(stackSlotP dst, rRegP src)\n-%{\n-  match(Set dst src);\n-\n-  ins_cost(100);\n-  format %{ \"movq    $dst, $src\\t# ptr stk\" %}\n-  ins_encode %{\n-    __ movq($dst$$Address, $src$$Register);\n-  %}\n-  ins_pipe(ialu_mem_reg);\n-%}\n-\n-instruct storeSSF(stackSlotF dst, regF src)\n-%{\n-  match(Set dst src);\n-\n-  ins_cost(95); \/\/ XXX\n-  format %{ \"movss   $dst, $src\\t# float stk\" %}\n-  ins_encode %{\n-    __ movflt(Address(rsp, $dst$$disp), $src$$XMMRegister);\n-  %}\n-  ins_pipe(pipe_slow); \/\/ XXX\n-%}\n-\n-instruct storeSSD(stackSlotD dst, regD src)\n-%{\n-  match(Set dst src);\n-\n-  ins_cost(95); \/\/ XXX\n-  format %{ \"movsd   $dst, $src\\t# double stk\" %}\n-  ins_encode %{\n-    __ movdbl(Address(rsp, $dst$$disp), $src$$XMMRegister);\n-  %}\n-  ins_pipe(pipe_slow); \/\/ XXX\n-%}\n-\n-instruct cacheWB(indirect addr)\n-%{\n-  predicate(VM_Version::supports_data_cache_line_flush());\n-  match(CacheWB addr);\n-\n-  ins_cost(100);\n-  format %{\"cache wb $addr\" %}\n-  ins_encode %{\n-    assert($addr->index_position() < 0, \"should be\");\n-    assert($addr$$disp == 0, \"should be\");\n-    __ cache_wb(Address($addr$$base$$Register, 0));\n-  %}\n-  ins_pipe(pipe_slow); \/\/ XXX\n-%}\n-\n-instruct cacheWBPreSync()\n-%{\n-  predicate(VM_Version::supports_data_cache_line_flush());\n-  match(CacheWBPreSync);\n-\n-  ins_cost(100);\n-  format %{\"cache wb presync\" %}\n-  ins_encode %{\n-    __ cache_wbsync(true);\n-  %}\n-  ins_pipe(pipe_slow); \/\/ XXX\n-%}\n-\n-instruct cacheWBPostSync()\n-%{\n-  predicate(VM_Version::supports_data_cache_line_flush());\n-  match(CacheWBPostSync);\n-\n-  ins_cost(100);\n-  format %{\"cache wb postsync\" %}\n-  ins_encode %{\n-    __ cache_wbsync(false);\n-  %}\n-  ins_pipe(pipe_slow); \/\/ XXX\n-%}\n-\n-\/\/----------BSWAP Instructions-------------------------------------------------\n-instruct bytes_reverse_int(rRegI dst) %{\n-  match(Set dst (ReverseBytesI dst));\n-\n-  format %{ \"bswapl  $dst\" %}\n-  ins_encode %{\n-    __ bswapl($dst$$Register);\n-  %}\n-  ins_pipe( ialu_reg );\n-%}\n-\n-instruct bytes_reverse_long(rRegL dst) %{\n-  match(Set dst (ReverseBytesL dst));\n-\n-  format %{ \"bswapq  $dst\" %}\n-  ins_encode %{\n-    __ bswapq($dst$$Register);\n-  %}\n-  ins_pipe( ialu_reg);\n-%}\n-\n-instruct bytes_reverse_unsigned_short(rRegI dst, rFlagsReg cr) %{\n-  match(Set dst (ReverseBytesUS dst));\n-  effect(KILL cr);\n-\n-  format %{ \"bswapl  $dst\\n\\t\"\n-            \"shrl    $dst,16\\n\\t\" %}\n-  ins_encode %{\n-    __ bswapl($dst$$Register);\n-    __ shrl($dst$$Register, 16);\n-  %}\n-  ins_pipe( ialu_reg );\n-%}\n-\n-instruct bytes_reverse_short(rRegI dst, rFlagsReg cr) %{\n-  match(Set dst (ReverseBytesS dst));\n-  effect(KILL cr);\n-\n-  format %{ \"bswapl  $dst\\n\\t\"\n-            \"sar     $dst,16\\n\\t\" %}\n-  ins_encode %{\n-    __ bswapl($dst$$Register);\n-    __ sarl($dst$$Register, 16);\n-  %}\n-  ins_pipe( ialu_reg );\n-%}\n-\n-\/\/---------- Zeros Count Instructions ------------------------------------------\n-\n-instruct countLeadingZerosI(rRegI dst, rRegI src, rFlagsReg cr) %{\n-  predicate(UseCountLeadingZerosInstruction);\n-  match(Set dst (CountLeadingZerosI src));\n-  effect(KILL cr);\n-\n-  format %{ \"lzcntl  $dst, $src\\t# count leading zeros (int)\" %}\n-  ins_encode %{\n-    __ lzcntl($dst$$Register, $src$$Register);\n-  %}\n-  ins_pipe(ialu_reg);\n-%}\n-\n-instruct countLeadingZerosI_mem(rRegI dst, memory src, rFlagsReg cr) %{\n-  predicate(UseCountLeadingZerosInstruction);\n-  match(Set dst (CountLeadingZerosI (LoadI src)));\n-  effect(KILL cr);\n-  ins_cost(175);\n-  format %{ \"lzcntl  $dst, $src\\t# count leading zeros (int)\" %}\n-  ins_encode %{\n-    __ lzcntl($dst$$Register, $src$$Address);\n-  %}\n-  ins_pipe(ialu_reg_mem);\n-%}\n-\n-instruct countLeadingZerosI_bsr(rRegI dst, rRegI src, rFlagsReg cr) %{\n-  predicate(!UseCountLeadingZerosInstruction);\n-  match(Set dst (CountLeadingZerosI src));\n-  effect(KILL cr);\n-\n-  format %{ \"bsrl    $dst, $src\\t# count leading zeros (int)\\n\\t\"\n-            \"jnz     skip\\n\\t\"\n-            \"movl    $dst, -1\\n\"\n-      \"skip:\\n\\t\"\n-            \"negl    $dst\\n\\t\"\n-            \"addl    $dst, 31\" %}\n-  ins_encode %{\n-    Register Rdst = $dst$$Register;\n-    Register Rsrc = $src$$Register;\n-    Label skip;\n-    __ bsrl(Rdst, Rsrc);\n-    __ jccb(Assembler::notZero, skip);\n-    __ movl(Rdst, -1);\n-    __ bind(skip);\n-    __ negl(Rdst);\n-    __ addl(Rdst, BitsPerInt - 1);\n-  %}\n-  ins_pipe(ialu_reg);\n-%}\n-\n-instruct countLeadingZerosL(rRegI dst, rRegL src, rFlagsReg cr) %{\n-  predicate(UseCountLeadingZerosInstruction);\n-  match(Set dst (CountLeadingZerosL src));\n-  effect(KILL cr);\n-\n-  format %{ \"lzcntq  $dst, $src\\t# count leading zeros (long)\" %}\n-  ins_encode %{\n-    __ lzcntq($dst$$Register, $src$$Register);\n-  %}\n-  ins_pipe(ialu_reg);\n-%}\n-\n-instruct countLeadingZerosL_mem(rRegI dst, memory src, rFlagsReg cr) %{\n-  predicate(UseCountLeadingZerosInstruction);\n-  match(Set dst (CountLeadingZerosL (LoadL src)));\n-  effect(KILL cr);\n-  ins_cost(175);\n-  format %{ \"lzcntq  $dst, $src\\t# count leading zeros (long)\" %}\n-  ins_encode %{\n-    __ lzcntq($dst$$Register, $src$$Address);\n-  %}\n-  ins_pipe(ialu_reg_mem);\n-%}\n-\n-instruct countLeadingZerosL_bsr(rRegI dst, rRegL src, rFlagsReg cr) %{\n-  predicate(!UseCountLeadingZerosInstruction);\n-  match(Set dst (CountLeadingZerosL src));\n-  effect(KILL cr);\n-\n-  format %{ \"bsrq    $dst, $src\\t# count leading zeros (long)\\n\\t\"\n-            \"jnz     skip\\n\\t\"\n-            \"movl    $dst, -1\\n\"\n-      \"skip:\\n\\t\"\n-            \"negl    $dst\\n\\t\"\n-            \"addl    $dst, 63\" %}\n-  ins_encode %{\n-    Register Rdst = $dst$$Register;\n-    Register Rsrc = $src$$Register;\n-    Label skip;\n-    __ bsrq(Rdst, Rsrc);\n-    __ jccb(Assembler::notZero, skip);\n-    __ movl(Rdst, -1);\n-    __ bind(skip);\n-    __ negl(Rdst);\n-    __ addl(Rdst, BitsPerLong - 1);\n-  %}\n-  ins_pipe(ialu_reg);\n-%}\n-\n-instruct countTrailingZerosI(rRegI dst, rRegI src, rFlagsReg cr) %{\n-  predicate(UseCountTrailingZerosInstruction);\n-  match(Set dst (CountTrailingZerosI src));\n-  effect(KILL cr);\n-\n-  format %{ \"tzcntl    $dst, $src\\t# count trailing zeros (int)\" %}\n-  ins_encode %{\n-    __ tzcntl($dst$$Register, $src$$Register);\n-  %}\n-  ins_pipe(ialu_reg);\n-%}\n-\n-instruct countTrailingZerosI_mem(rRegI dst, memory src, rFlagsReg cr) %{\n-  predicate(UseCountTrailingZerosInstruction);\n-  match(Set dst (CountTrailingZerosI (LoadI src)));\n-  effect(KILL cr);\n-  ins_cost(175);\n-  format %{ \"tzcntl    $dst, $src\\t# count trailing zeros (int)\" %}\n-  ins_encode %{\n-    __ tzcntl($dst$$Register, $src$$Address);\n-  %}\n-  ins_pipe(ialu_reg_mem);\n-%}\n-\n-instruct countTrailingZerosI_bsf(rRegI dst, rRegI src, rFlagsReg cr) %{\n-  predicate(!UseCountTrailingZerosInstruction);\n-  match(Set dst (CountTrailingZerosI src));\n-  effect(KILL cr);\n-\n-  format %{ \"bsfl    $dst, $src\\t# count trailing zeros (int)\\n\\t\"\n-            \"jnz     done\\n\\t\"\n-            \"movl    $dst, 32\\n\"\n-      \"done:\" %}\n-  ins_encode %{\n-    Register Rdst = $dst$$Register;\n-    Label done;\n-    __ bsfl(Rdst, $src$$Register);\n-    __ jccb(Assembler::notZero, done);\n-    __ movl(Rdst, BitsPerInt);\n-    __ bind(done);\n-  %}\n-  ins_pipe(ialu_reg);\n-%}\n-\n-instruct countTrailingZerosL(rRegI dst, rRegL src, rFlagsReg cr) %{\n-  predicate(UseCountTrailingZerosInstruction);\n-  match(Set dst (CountTrailingZerosL src));\n-  effect(KILL cr);\n-\n-  format %{ \"tzcntq    $dst, $src\\t# count trailing zeros (long)\" %}\n-  ins_encode %{\n-    __ tzcntq($dst$$Register, $src$$Register);\n-  %}\n-  ins_pipe(ialu_reg);\n-%}\n-\n-instruct countTrailingZerosL_mem(rRegI dst, memory src, rFlagsReg cr) %{\n-  predicate(UseCountTrailingZerosInstruction);\n-  match(Set dst (CountTrailingZerosL (LoadL src)));\n-  effect(KILL cr);\n-  ins_cost(175);\n-  format %{ \"tzcntq    $dst, $src\\t# count trailing zeros (long)\" %}\n-  ins_encode %{\n-    __ tzcntq($dst$$Register, $src$$Address);\n-  %}\n-  ins_pipe(ialu_reg_mem);\n-%}\n-\n-instruct countTrailingZerosL_bsf(rRegI dst, rRegL src, rFlagsReg cr) %{\n-  predicate(!UseCountTrailingZerosInstruction);\n-  match(Set dst (CountTrailingZerosL src));\n-  effect(KILL cr);\n-\n-  format %{ \"bsfq    $dst, $src\\t# count trailing zeros (long)\\n\\t\"\n-            \"jnz     done\\n\\t\"\n-            \"movl    $dst, 64\\n\"\n-      \"done:\" %}\n-  ins_encode %{\n-    Register Rdst = $dst$$Register;\n-    Label done;\n-    __ bsfq(Rdst, $src$$Register);\n-    __ jccb(Assembler::notZero, done);\n-    __ movl(Rdst, BitsPerLong);\n-    __ bind(done);\n-  %}\n-  ins_pipe(ialu_reg);\n-%}\n-\n-\/\/--------------- Reverse Operation Instructions ----------------\n-instruct bytes_reversebit_int(rRegI dst, rRegI src, rRegI rtmp, rFlagsReg cr) %{\n-  predicate(!VM_Version::supports_gfni());\n-  match(Set dst (ReverseI src));\n-  effect(TEMP dst, TEMP rtmp, KILL cr);\n-  format %{ \"reverse_int $dst $src\\t! using $rtmp as TEMP\" %}\n-  ins_encode %{\n-    __ reverseI($dst$$Register, $src$$Register, xnoreg, xnoreg, $rtmp$$Register);\n-  %}\n-  ins_pipe( ialu_reg );\n-%}\n-\n-instruct bytes_reversebit_int_gfni(rRegI dst, rRegI src, vlRegF xtmp1, vlRegF xtmp2, rRegL rtmp, rFlagsReg cr) %{\n-  predicate(VM_Version::supports_gfni());\n-  match(Set dst (ReverseI src));\n-  effect(TEMP dst, TEMP xtmp1, TEMP xtmp2, TEMP rtmp, KILL cr);\n-  format %{ \"reverse_int $dst $src\\t! using $rtmp, $xtmp1 and $xtmp2 as TEMP\" %}\n-  ins_encode %{\n-    __ reverseI($dst$$Register, $src$$Register, $xtmp1$$XMMRegister, $xtmp2$$XMMRegister, $rtmp$$Register);\n-  %}\n-  ins_pipe( ialu_reg );\n-%}\n-\n-instruct bytes_reversebit_long(rRegL dst, rRegL src, rRegL rtmp1, rRegL rtmp2, rFlagsReg cr) %{\n-  predicate(!VM_Version::supports_gfni());\n-  match(Set dst (ReverseL src));\n-  effect(TEMP dst, TEMP rtmp1, TEMP rtmp2, KILL cr);\n-  format %{ \"reverse_long $dst $src\\t! using $rtmp1 and $rtmp2 as TEMP\" %}\n-  ins_encode %{\n-    __ reverseL($dst$$Register, $src$$Register, xnoreg, xnoreg, $rtmp1$$Register, $rtmp2$$Register);\n-  %}\n-  ins_pipe( ialu_reg );\n-%}\n-\n-instruct bytes_reversebit_long_gfni(rRegL dst, rRegL src, vlRegD xtmp1, vlRegD xtmp2, rRegL rtmp, rFlagsReg cr) %{\n-  predicate(VM_Version::supports_gfni());\n-  match(Set dst (ReverseL src));\n-  effect(TEMP dst, TEMP xtmp1, TEMP xtmp2, TEMP rtmp, KILL cr);\n-  format %{ \"reverse_long $dst $src\\t! using $rtmp, $xtmp1 and $xtmp2 as TEMP\" %}\n-  ins_encode %{\n-    __ reverseL($dst$$Register, $src$$Register, $xtmp1$$XMMRegister, $xtmp2$$XMMRegister, $rtmp$$Register, noreg);\n-  %}\n-  ins_pipe( ialu_reg );\n-%}\n-\n-\/\/---------- Population Count Instructions -------------------------------------\n-\n-instruct popCountI(rRegI dst, rRegI src, rFlagsReg cr) %{\n-  predicate(UsePopCountInstruction);\n-  match(Set dst (PopCountI src));\n-  effect(KILL cr);\n-\n-  format %{ \"popcnt  $dst, $src\" %}\n-  ins_encode %{\n-    __ popcntl($dst$$Register, $src$$Register);\n-  %}\n-  ins_pipe(ialu_reg);\n-%}\n-\n-instruct popCountI_mem(rRegI dst, memory mem, rFlagsReg cr) %{\n-  predicate(UsePopCountInstruction);\n-  match(Set dst (PopCountI (LoadI mem)));\n-  effect(KILL cr);\n-\n-  format %{ \"popcnt  $dst, $mem\" %}\n-  ins_encode %{\n-    __ popcntl($dst$$Register, $mem$$Address);\n-  %}\n-  ins_pipe(ialu_reg);\n-%}\n-\n-\/\/ Note: Long.bitCount(long) returns an int.\n-instruct popCountL(rRegI dst, rRegL src, rFlagsReg cr) %{\n-  predicate(UsePopCountInstruction);\n-  match(Set dst (PopCountL src));\n-  effect(KILL cr);\n-\n-  format %{ \"popcnt  $dst, $src\" %}\n-  ins_encode %{\n-    __ popcntq($dst$$Register, $src$$Register);\n-  %}\n-  ins_pipe(ialu_reg);\n-%}\n-\n-\/\/ Note: Long.bitCount(long) returns an int.\n-instruct popCountL_mem(rRegI dst, memory mem, rFlagsReg cr) %{\n-  predicate(UsePopCountInstruction);\n-  match(Set dst (PopCountL (LoadL mem)));\n-  effect(KILL cr);\n-\n-  format %{ \"popcnt  $dst, $mem\" %}\n-  ins_encode %{\n-    __ popcntq($dst$$Register, $mem$$Address);\n-  %}\n-  ins_pipe(ialu_reg);\n-%}\n-\n-\n-\/\/----------MemBar Instructions-----------------------------------------------\n-\/\/ Memory barrier flavors\n-\n-instruct membar_acquire()\n-%{\n-  match(MemBarAcquire);\n-  match(LoadFence);\n-  ins_cost(0);\n-\n-  size(0);\n-  format %{ \"MEMBAR-acquire ! (empty encoding)\" %}\n-  ins_encode();\n-  ins_pipe(empty);\n-%}\n-\n-instruct membar_acquire_lock()\n-%{\n-  match(MemBarAcquireLock);\n-  ins_cost(0);\n-\n-  size(0);\n-  format %{ \"MEMBAR-acquire (prior CMPXCHG in FastLock so empty encoding)\" %}\n-  ins_encode();\n-  ins_pipe(empty);\n-%}\n-\n-instruct membar_release()\n-%{\n-  match(MemBarRelease);\n-  match(StoreFence);\n-  ins_cost(0);\n-\n-  size(0);\n-  format %{ \"MEMBAR-release ! (empty encoding)\" %}\n-  ins_encode();\n-  ins_pipe(empty);\n-%}\n-\n-instruct membar_release_lock()\n-%{\n-  match(MemBarReleaseLock);\n-  ins_cost(0);\n-\n-  size(0);\n-  format %{ \"MEMBAR-release (a FastUnlock follows so empty encoding)\" %}\n-  ins_encode();\n-  ins_pipe(empty);\n-%}\n-\n-instruct membar_volatile(rFlagsReg cr) %{\n-  match(MemBarVolatile);\n-  effect(KILL cr);\n-  ins_cost(400);\n-\n-  format %{\n-    $$template\n-    $$emit$$\"lock addl [rsp + #0], 0\\t! membar_volatile\"\n-  %}\n-  ins_encode %{\n-    __ membar(Assembler::StoreLoad);\n-  %}\n-  ins_pipe(pipe_slow);\n-%}\n-\n-instruct unnecessary_membar_volatile()\n-%{\n-  match(MemBarVolatile);\n-  predicate(Matcher::post_store_load_barrier(n));\n-  ins_cost(0);\n-\n-  size(0);\n-  format %{ \"MEMBAR-volatile (unnecessary so empty encoding)\" %}\n-  ins_encode();\n-  ins_pipe(empty);\n-%}\n-\n-instruct membar_storestore() %{\n-  match(MemBarStoreStore);\n-  match(StoreStoreFence);\n-  ins_cost(0);\n-\n-  size(0);\n-  format %{ \"MEMBAR-storestore (empty encoding)\" %}\n-  ins_encode( );\n-  ins_pipe(empty);\n-%}\n-\n-\/\/----------Move Instructions--------------------------------------------------\n-\n-instruct castX2P(rRegP dst, rRegL src)\n-%{\n-  match(Set dst (CastX2P src));\n-\n-  format %{ \"movq    $dst, $src\\t# long->ptr\" %}\n-  ins_encode %{\n-    if ($dst$$reg != $src$$reg) {\n-      __ movptr($dst$$Register, $src$$Register);\n-    }\n-  %}\n-  ins_pipe(ialu_reg_reg); \/\/ XXX\n-%}\n-\n-instruct castP2X(rRegL dst, rRegP src)\n-%{\n-  match(Set dst (CastP2X src));\n-\n-  format %{ \"movq    $dst, $src\\t# ptr -> long\" %}\n-  ins_encode %{\n-    if ($dst$$reg != $src$$reg) {\n-      __ movptr($dst$$Register, $src$$Register);\n-    }\n-  %}\n-  ins_pipe(ialu_reg_reg); \/\/ XXX\n-%}\n-\n-\/\/ Convert oop into int for vectors alignment masking\n-instruct convP2I(rRegI dst, rRegP src)\n-%{\n-  match(Set dst (ConvL2I (CastP2X src)));\n-\n-  format %{ \"movl    $dst, $src\\t# ptr -> int\" %}\n-  ins_encode %{\n-    __ movl($dst$$Register, $src$$Register);\n-  %}\n-  ins_pipe(ialu_reg_reg); \/\/ XXX\n-%}\n-\n-\/\/ Convert compressed oop into int for vectors alignment masking\n-\/\/ in case of 32bit oops (heap < 4Gb).\n-instruct convN2I(rRegI dst, rRegN src)\n-%{\n-  predicate(CompressedOops::shift() == 0);\n-  match(Set dst (ConvL2I (CastP2X (DecodeN src))));\n-\n-  format %{ \"movl    $dst, $src\\t# compressed ptr -> int\" %}\n-  ins_encode %{\n-    __ movl($dst$$Register, $src$$Register);\n-  %}\n-  ins_pipe(ialu_reg_reg); \/\/ XXX\n-%}\n-\n-\/\/ Convert oop pointer into compressed form\n-instruct encodeHeapOop(rRegN dst, rRegP src, rFlagsReg cr) %{\n-  predicate(n->bottom_type()->make_ptr()->ptr() != TypePtr::NotNull);\n-  match(Set dst (EncodeP src));\n-  effect(KILL cr);\n-  format %{ \"encode_heap_oop $dst,$src\" %}\n-  ins_encode %{\n-    Register s = $src$$Register;\n-    Register d = $dst$$Register;\n-    if (s != d) {\n-      __ movq(d, s);\n-    }\n-    __ encode_heap_oop(d);\n-  %}\n-  ins_pipe(ialu_reg_long);\n-%}\n-\n-instruct encodeHeapOop_not_null(rRegN dst, rRegP src, rFlagsReg cr) %{\n-  predicate(n->bottom_type()->make_ptr()->ptr() == TypePtr::NotNull);\n-  match(Set dst (EncodeP src));\n-  effect(KILL cr);\n-  format %{ \"encode_heap_oop_not_null $dst,$src\" %}\n-  ins_encode %{\n-    __ encode_heap_oop_not_null($dst$$Register, $src$$Register);\n-  %}\n-  ins_pipe(ialu_reg_long);\n-%}\n-\n-instruct decodeHeapOop(rRegP dst, rRegN src, rFlagsReg cr) %{\n-  predicate(n->bottom_type()->is_ptr()->ptr() != TypePtr::NotNull &&\n-            n->bottom_type()->is_ptr()->ptr() != TypePtr::Constant);\n-  match(Set dst (DecodeN src));\n-  effect(KILL cr);\n-  format %{ \"decode_heap_oop $dst,$src\" %}\n-  ins_encode %{\n-    Register s = $src$$Register;\n-    Register d = $dst$$Register;\n-    if (s != d) {\n-      __ movq(d, s);\n-    }\n-    __ decode_heap_oop(d);\n-  %}\n-  ins_pipe(ialu_reg_long);\n-%}\n-\n-instruct decodeHeapOop_not_null(rRegP dst, rRegN src, rFlagsReg cr) %{\n-  predicate(n->bottom_type()->is_ptr()->ptr() == TypePtr::NotNull ||\n-            n->bottom_type()->is_ptr()->ptr() == TypePtr::Constant);\n-  match(Set dst (DecodeN src));\n-  effect(KILL cr);\n-  format %{ \"decode_heap_oop_not_null $dst,$src\" %}\n-  ins_encode %{\n-    Register s = $src$$Register;\n-    Register d = $dst$$Register;\n-    if (s != d) {\n-      __ decode_heap_oop_not_null(d, s);\n-    } else {\n-      __ decode_heap_oop_not_null(d);\n-    }\n-  %}\n-  ins_pipe(ialu_reg_long);\n-%}\n-\n-instruct encodeKlass_not_null(rRegN dst, rRegP src, rFlagsReg cr) %{\n-  match(Set dst (EncodePKlass src));\n-  effect(TEMP dst, KILL cr);\n-  format %{ \"encode_and_move_klass_not_null $dst,$src\" %}\n-  ins_encode %{\n-    __ encode_and_move_klass_not_null($dst$$Register, $src$$Register);\n-  %}\n-  ins_pipe(ialu_reg_long);\n-%}\n-\n-instruct decodeKlass_not_null(rRegP dst, rRegN src, rFlagsReg cr) %{\n-  match(Set dst (DecodeNKlass src));\n-  effect(TEMP dst, KILL cr);\n-  format %{ \"decode_and_move_klass_not_null $dst,$src\" %}\n-  ins_encode %{\n-    __ decode_and_move_klass_not_null($dst$$Register, $src$$Register);\n-  %}\n-  ins_pipe(ialu_reg_long);\n-%}\n-\n-\/\/----------Conditional Move---------------------------------------------------\n-\/\/ Jump\n-\/\/ dummy instruction for generating temp registers\n-instruct jumpXtnd_offset(rRegL switch_val, immI2 shift, rRegI dest) %{\n-  match(Jump (LShiftL switch_val shift));\n-  ins_cost(350);\n-  predicate(false);\n-  effect(TEMP dest);\n-\n-  format %{ \"leaq    $dest, [$constantaddress]\\n\\t\"\n-            \"jmp     [$dest + $switch_val << $shift]\\n\\t\" %}\n-  ins_encode %{\n-    \/\/ We could use jump(ArrayAddress) except that the macro assembler needs to use r10\n-    \/\/ to do that and the compiler is using that register as one it can allocate.\n-    \/\/ So we build it all by hand.\n-    \/\/ Address index(noreg, switch_reg, (Address::ScaleFactor)$shift$$constant);\n-    \/\/ ArrayAddress dispatch(table, index);\n-    Address dispatch($dest$$Register, $switch_val$$Register, (Address::ScaleFactor) $shift$$constant);\n-    __ lea($dest$$Register, $constantaddress);\n-    __ jmp(dispatch);\n-  %}\n-  ins_pipe(pipe_jmp);\n-%}\n-\n-instruct jumpXtnd_addr(rRegL switch_val, immI2 shift, immL32 offset, rRegI dest) %{\n-  match(Jump (AddL (LShiftL switch_val shift) offset));\n-  ins_cost(350);\n-  effect(TEMP dest);\n-\n-  format %{ \"leaq    $dest, [$constantaddress]\\n\\t\"\n-            \"jmp     [$dest + $switch_val << $shift + $offset]\\n\\t\" %}\n-  ins_encode %{\n-    \/\/ We could use jump(ArrayAddress) except that the macro assembler needs to use r10\n-    \/\/ to do that and the compiler is using that register as one it can allocate.\n-    \/\/ So we build it all by hand.\n-    \/\/ Address index(noreg, switch_reg, (Address::ScaleFactor) $shift$$constant, (int) $offset$$constant);\n-    \/\/ ArrayAddress dispatch(table, index);\n-    Address dispatch($dest$$Register, $switch_val$$Register, (Address::ScaleFactor) $shift$$constant, (int) $offset$$constant);\n-    __ lea($dest$$Register, $constantaddress);\n-    __ jmp(dispatch);\n-  %}\n-  ins_pipe(pipe_jmp);\n-%}\n-\n-instruct jumpXtnd(rRegL switch_val, rRegI dest) %{\n-  match(Jump switch_val);\n-  ins_cost(350);\n-  effect(TEMP dest);\n-\n-  format %{ \"leaq    $dest, [$constantaddress]\\n\\t\"\n-            \"jmp     [$dest + $switch_val]\\n\\t\" %}\n-  ins_encode %{\n-    \/\/ We could use jump(ArrayAddress) except that the macro assembler needs to use r10\n-    \/\/ to do that and the compiler is using that register as one it can allocate.\n-    \/\/ So we build it all by hand.\n-    \/\/ Address index(noreg, switch_reg, Address::times_1);\n-    \/\/ ArrayAddress dispatch(table, index);\n-    Address dispatch($dest$$Register, $switch_val$$Register, Address::times_1);\n-    __ lea($dest$$Register, $constantaddress);\n-    __ jmp(dispatch);\n-  %}\n-  ins_pipe(pipe_jmp);\n-%}\n-\n-\/\/ Conditional move\n-instruct cmovI_imm_01(rRegI dst, immI_1 src, rFlagsReg cr, cmpOp cop)\n-%{\n-  predicate(n->in(2)->in(2)->is_Con() && n->in(2)->in(2)->get_int() == 0);\n-  match(Set dst (CMoveI (Binary cop cr) (Binary src dst)));\n-\n-  ins_cost(100); \/\/ XXX\n-  format %{ \"setbn$cop $dst\\t# signed, int\" %}\n-  ins_encode %{\n-    Assembler::Condition cond = (Assembler::Condition)($cop$$cmpcode);\n-    __ setb(MacroAssembler::negate_condition(cond), $dst$$Register);\n-  %}\n-  ins_pipe(ialu_reg);\n-%}\n-\n-instruct cmovI_reg(rRegI dst, rRegI src, rFlagsReg cr, cmpOp cop)\n-%{\n-  predicate(!UseAPX);\n-  match(Set dst (CMoveI (Binary cop cr) (Binary dst src)));\n-\n-  ins_cost(200); \/\/ XXX\n-  format %{ \"cmovl$cop $dst, $src\\t# signed, int\" %}\n-  ins_encode %{\n-    __ cmovl((Assembler::Condition)($cop$$cmpcode), $dst$$Register, $src$$Register);\n-  %}\n-  ins_pipe(pipe_cmov_reg);\n-%}\n-\n-instruct cmovI_reg_ndd(rRegI dst, rRegI src1, rRegI src2, rFlagsReg cr, cmpOp cop)\n-%{\n-  predicate(UseAPX);\n-  match(Set dst (CMoveI (Binary cop cr) (Binary src1 src2)));\n-\n-  ins_cost(200);\n-  format %{ \"ecmovl$cop $dst, $src1, $src2\\t# signed, int ndd\" %}\n-  ins_encode %{\n-    __ ecmovl((Assembler::Condition)($cop$$cmpcode), $dst$$Register, $src1$$Register, $src2$$Register);\n-  %}\n-  ins_pipe(pipe_cmov_reg);\n-%}\n-\n-instruct cmovI_imm_01U(rRegI dst, immI_1 src, rFlagsRegU cr, cmpOpU cop)\n-%{\n-  predicate(n->in(2)->in(2)->is_Con() && n->in(2)->in(2)->get_int() == 0);\n-  match(Set dst (CMoveI (Binary cop cr) (Binary src dst)));\n-\n-  ins_cost(100); \/\/ XXX\n-  format %{ \"setbn$cop $dst\\t# unsigned, int\" %}\n-  ins_encode %{\n-    Assembler::Condition cond = (Assembler::Condition)($cop$$cmpcode);\n-    __ setb(MacroAssembler::negate_condition(cond), $dst$$Register);\n-  %}\n-  ins_pipe(ialu_reg);\n-%}\n-\n-instruct cmovI_regU(cmpOpU cop, rFlagsRegU cr, rRegI dst, rRegI src) %{\n-  predicate(!UseAPX);\n-  match(Set dst (CMoveI (Binary cop cr) (Binary dst src)));\n-\n-  ins_cost(200); \/\/ XXX\n-  format %{ \"cmovl$cop $dst, $src\\t# unsigned, int\" %}\n-  ins_encode %{\n-    __ cmovl((Assembler::Condition)($cop$$cmpcode), $dst$$Register, $src$$Register);\n-  %}\n-  ins_pipe(pipe_cmov_reg);\n-%}\n-\n-instruct cmovI_regU_ndd(rRegI dst, cmpOpU cop, rFlagsRegU cr, rRegI src1, rRegI src2) %{\n-  predicate(UseAPX);\n-  match(Set dst (CMoveI (Binary cop cr) (Binary src1 src2)));\n-\n-  ins_cost(200);\n-  format %{ \"ecmovl$cop $dst, $src1, $src2\\t# unsigned, int ndd\" %}\n-  ins_encode %{\n-    __ ecmovl((Assembler::Condition)($cop$$cmpcode), $dst$$Register, $src1$$Register, $src2$$Register);\n-  %}\n-  ins_pipe(pipe_cmov_reg);\n-%}\n-\n-instruct cmovI_imm_01UCF(rRegI dst, immI_1 src, rFlagsRegUCF cr, cmpOpUCF cop)\n-%{\n-  predicate(n->in(2)->in(2)->is_Con() && n->in(2)->in(2)->get_int() == 0);\n-  match(Set dst (CMoveI (Binary cop cr) (Binary src dst)));\n-\n-  ins_cost(100); \/\/ XXX\n-  format %{ \"setbn$cop $dst\\t# unsigned, int\" %}\n-  ins_encode %{\n-    Assembler::Condition cond = (Assembler::Condition)($cop$$cmpcode);\n-    __ setb(MacroAssembler::negate_condition(cond), $dst$$Register);\n-  %}\n-  ins_pipe(ialu_reg);\n-%}\n-\n-instruct cmovI_regUCF(cmpOpUCF cop, rFlagsRegUCF cr, rRegI dst, rRegI src) %{\n-  predicate(!UseAPX);\n-  match(Set dst (CMoveI (Binary cop cr) (Binary dst src)));\n-  ins_cost(200);\n-  expand %{\n-    cmovI_regU(cop, cr, dst, src);\n-  %}\n-%}\n-\n-instruct cmovI_regUCF_ndd(rRegI dst, cmpOpUCF cop, rFlagsRegUCF cr, rRegI src1, rRegI src2) %{\n-  predicate(UseAPX);\n-  match(Set dst (CMoveI (Binary cop cr) (Binary src1 src2)));\n-  ins_cost(200);\n-  format %{ \"ecmovl$cop $dst, $src1, $src2\\t# unsigned, int ndd\" %}\n-  ins_encode %{\n-    __ ecmovl((Assembler::Condition)($cop$$cmpcode), $dst$$Register, $src1$$Register, $src2$$Register);\n-  %}\n-  ins_pipe(pipe_cmov_reg);\n-%}\n-\n-instruct cmovI_regUCF2_ne(cmpOpUCF2 cop, rFlagsRegUCF cr, rRegI dst, rRegI src) %{\n-  predicate(!UseAPX && n->in(1)->in(1)->as_Bool()->_test._test == BoolTest::ne);\n-  match(Set dst (CMoveI (Binary cop cr) (Binary dst src)));\n-\n-  ins_cost(200); \/\/ XXX\n-  format %{ \"cmovpl  $dst, $src\\n\\t\"\n-            \"cmovnel $dst, $src\" %}\n-  ins_encode %{\n-    __ cmovl(Assembler::parity, $dst$$Register, $src$$Register);\n-    __ cmovl(Assembler::notEqual, $dst$$Register, $src$$Register);\n-  %}\n-  ins_pipe(pipe_cmov_reg);\n-%}\n-\n-instruct cmovI_regUCF2_ne_ndd(cmpOpUCF2 cop, rFlagsRegUCF cr, rRegI dst, rRegI src1, rRegI src2) %{\n-  predicate(UseAPX && n->in(1)->in(1)->as_Bool()->_test._test == BoolTest::ne);\n-  match(Set dst (CMoveI (Binary cop cr) (Binary src1 src2)));\n-  effect(TEMP dst);\n-\n-  ins_cost(200);\n-  format %{ \"ecmovpl  $dst, $src1, $src2\\n\\t\"\n-            \"cmovnel  $dst, $src2\" %}\n-  ins_encode %{\n-    __ ecmovl(Assembler::parity, $dst$$Register, $src1$$Register, $src2$$Register);\n-    __ cmovl(Assembler::notEqual, $dst$$Register, $src2$$Register);\n-  %}\n-  ins_pipe(pipe_cmov_reg);\n-%}\n-\n-\/\/ Since (x == y) == !(x != y), we can flip the sense of the test by flipping the\n-\/\/ inputs of the CMove\n-instruct cmovI_regUCF2_eq(cmpOpUCF2 cop, rFlagsRegUCF cr, rRegI dst, rRegI src) %{\n-  predicate(!UseAPX && n->in(1)->in(1)->as_Bool()->_test._test == BoolTest::eq);\n-  match(Set dst (CMoveI (Binary cop cr) (Binary src dst)));\n-  effect(TEMP dst);\n-\n-  ins_cost(200); \/\/ XXX\n-  format %{ \"cmovpl  $dst, $src\\n\\t\"\n-            \"cmovnel $dst, $src\" %}\n-  ins_encode %{\n-    __ cmovl(Assembler::parity, $dst$$Register, $src$$Register);\n-    __ cmovl(Assembler::notEqual, $dst$$Register, $src$$Register);\n-  %}\n-  ins_pipe(pipe_cmov_reg);\n-%}\n-\n-\/\/ We need this special handling for only eq \/ neq comparison since NaN == NaN is false,\n-\/\/ and parity flag bit is set if any of the operand is a NaN.\n-instruct cmovI_regUCF2_eq_ndd(cmpOpUCF2 cop, rFlagsRegUCF cr, rRegI dst, rRegI src1, rRegI src2) %{\n-  predicate(UseAPX && n->in(1)->in(1)->as_Bool()->_test._test == BoolTest::eq);\n-  match(Set dst (CMoveI (Binary cop cr) (Binary src2 src1)));\n-  effect(TEMP dst);\n-\n-  ins_cost(200);\n-  format %{ \"ecmovpl  $dst, $src1, $src2\\n\\t\"\n-            \"cmovnel  $dst, $src2\" %}\n-  ins_encode %{\n-    __ ecmovl(Assembler::parity, $dst$$Register, $src1$$Register, $src2$$Register);\n-    __ cmovl(Assembler::notEqual, $dst$$Register, $src2$$Register);\n-  %}\n-  ins_pipe(pipe_cmov_reg);\n-%}\n-\n-\/\/ Conditional move\n-instruct cmovI_mem(cmpOp cop, rFlagsReg cr, rRegI dst, memory src) %{\n-  predicate(!UseAPX);\n-  match(Set dst (CMoveI (Binary cop cr) (Binary dst (LoadI src))));\n-\n-  ins_cost(250); \/\/ XXX\n-  format %{ \"cmovl$cop $dst, $src\\t# signed, int\" %}\n-  ins_encode %{\n-    __ cmovl((Assembler::Condition)($cop$$cmpcode), $dst$$Register, $src$$Address);\n-  %}\n-  ins_pipe(pipe_cmov_mem);\n-%}\n-\n-\/\/ Conditional move\n-instruct cmovI_rReg_rReg_mem_ndd(rRegI dst, cmpOp cop, rFlagsReg cr, rRegI src1, memory src2)\n-%{\n-  predicate(UseAPX);\n-  match(Set dst (CMoveI (Binary cop cr) (Binary src1 (LoadI src2))));\n-\n-  ins_cost(250);\n-  format %{ \"ecmovl$cop $dst, $src1, $src2\\t# signed, int ndd\" %}\n-  ins_encode %{\n-    __ ecmovl((Assembler::Condition)($cop$$cmpcode), $dst$$Register, $src1$$Register, $src2$$Address);\n-  %}\n-  ins_pipe(pipe_cmov_mem);\n-%}\n-\n-\/\/ Conditional move\n-instruct cmovI_memU(cmpOpU cop, rFlagsRegU cr, rRegI dst, memory src)\n-%{\n-  predicate(!UseAPX);\n-  match(Set dst (CMoveI (Binary cop cr) (Binary dst (LoadI src))));\n-\n-  ins_cost(250); \/\/ XXX\n-  format %{ \"cmovl$cop $dst, $src\\t# unsigned, int\" %}\n-  ins_encode %{\n-    __ cmovl((Assembler::Condition)($cop$$cmpcode), $dst$$Register, $src$$Address);\n-  %}\n-  ins_pipe(pipe_cmov_mem);\n-%}\n-\n-instruct cmovI_memUCF(cmpOpUCF cop, rFlagsRegUCF cr, rRegI dst, memory src) %{\n-  predicate(!UseAPX);\n-  match(Set dst (CMoveI (Binary cop cr) (Binary dst (LoadI src))));\n-  ins_cost(250);\n-  expand %{\n-    cmovI_memU(cop, cr, dst, src);\n-  %}\n-%}\n-\n-instruct cmovI_rReg_rReg_memU_ndd(rRegI dst, cmpOpU cop, rFlagsRegU cr, rRegI src1, memory src2)\n-%{\n-  predicate(UseAPX);\n-  match(Set dst (CMoveI (Binary cop cr) (Binary src1 (LoadI src2))));\n-\n-  ins_cost(250);\n-  format %{ \"ecmovl$cop $dst, $src1, $src2\\t# unsigned, int ndd\" %}\n-  ins_encode %{\n-    __ ecmovl((Assembler::Condition)($cop$$cmpcode), $dst$$Register, $src1$$Register, $src2$$Address);\n-  %}\n-  ins_pipe(pipe_cmov_mem);\n-%}\n-\n-instruct cmovI_rReg_rReg_memUCF_ndd(rRegI dst, cmpOpUCF cop, rFlagsRegUCF cr, rRegI src1, memory src2)\n-%{\n-  predicate(UseAPX);\n-  match(Set dst (CMoveI (Binary cop cr) (Binary src1 (LoadI src2))));\n-  ins_cost(250);\n-  format %{ \"ecmovl$cop $dst, $src1, $src2\\t# unsigned, int ndd\" %}\n-  ins_encode %{\n-    __ ecmovl((Assembler::Condition)($cop$$cmpcode), $dst$$Register, $src1$$Register, $src2$$Address);\n-  %}\n-  ins_pipe(pipe_cmov_mem);\n-%}\n-\n-\/\/ Conditional move\n-instruct cmovN_reg(rRegN dst, rRegN src, rFlagsReg cr, cmpOp cop)\n-%{\n-  predicate(!UseAPX);\n-  match(Set dst (CMoveN (Binary cop cr) (Binary dst src)));\n-\n-  ins_cost(200); \/\/ XXX\n-  format %{ \"cmovl$cop $dst, $src\\t# signed, compressed ptr\" %}\n-  ins_encode %{\n-    __ cmovl((Assembler::Condition)($cop$$cmpcode), $dst$$Register, $src$$Register);\n-  %}\n-  ins_pipe(pipe_cmov_reg);\n-%}\n-\n-\/\/ Conditional move ndd\n-instruct cmovN_reg_ndd(rRegN dst, rRegN src1, rRegN src2, rFlagsReg cr, cmpOp cop)\n-%{\n-  predicate(UseAPX);\n-  match(Set dst (CMoveN (Binary cop cr) (Binary src1 src2)));\n-\n-  ins_cost(200);\n-  format %{ \"ecmovl$cop $dst, $src1, $src2\\t# signed, compressed ptr ndd\" %}\n-  ins_encode %{\n-    __ ecmovl((Assembler::Condition)($cop$$cmpcode), $dst$$Register, $src1$$Register, $src2$$Register);\n-  %}\n-  ins_pipe(pipe_cmov_reg);\n-%}\n-\n-\/\/ Conditional move\n-instruct cmovN_regU(cmpOpU cop, rFlagsRegU cr, rRegN dst, rRegN src)\n-%{\n-  predicate(!UseAPX);\n-  match(Set dst (CMoveN (Binary cop cr) (Binary dst src)));\n-\n-  ins_cost(200); \/\/ XXX\n-  format %{ \"cmovl$cop $dst, $src\\t# unsigned, compressed ptr\" %}\n-  ins_encode %{\n-    __ cmovl((Assembler::Condition)($cop$$cmpcode), $dst$$Register, $src$$Register);\n-  %}\n-  ins_pipe(pipe_cmov_reg);\n-%}\n-\n-instruct cmovN_regUCF(cmpOpUCF cop, rFlagsRegUCF cr, rRegN dst, rRegN src) %{\n-  predicate(!UseAPX);\n-  match(Set dst (CMoveN (Binary cop cr) (Binary dst src)));\n-  ins_cost(200);\n-  expand %{\n-    cmovN_regU(cop, cr, dst, src);\n-  %}\n-%}\n-\n-\/\/ Conditional move ndd\n-instruct cmovN_regU_ndd(rRegN dst, cmpOpU cop, rFlagsRegU cr, rRegN src1, rRegN src2)\n-%{\n-  predicate(UseAPX);\n-  match(Set dst (CMoveN (Binary cop cr) (Binary src1 src2)));\n-\n-  ins_cost(200);\n-  format %{ \"ecmovl$cop $dst, $src1, $src2\\t# unsigned, compressed ptr ndd\" %}\n-  ins_encode %{\n-    __ ecmovl((Assembler::Condition)($cop$$cmpcode), $dst$$Register, $src1$$Register, $src2$$Register);\n-  %}\n-  ins_pipe(pipe_cmov_reg);\n-%}\n-\n-instruct cmovN_regUCF_ndd(rRegN dst, cmpOpUCF cop, rFlagsRegUCF cr, rRegN src1, rRegN src2) %{\n-  predicate(UseAPX);\n-  match(Set dst (CMoveN (Binary cop cr) (Binary src1 src2)));\n-  ins_cost(200);\n-  format %{ \"ecmovl$cop $dst, $src1, $src2\\t# unsigned, compressed ptr ndd\" %}\n-  ins_encode %{\n-    __ ecmovl((Assembler::Condition)($cop$$cmpcode), $dst$$Register, $src1$$Register, $src2$$Register);\n-  %}\n-  ins_pipe(pipe_cmov_reg);\n-%}\n-\n-instruct cmovN_regUCF2_ne(cmpOpUCF2 cop, rFlagsRegUCF cr, rRegN dst, rRegN src) %{\n-  predicate(n->in(1)->in(1)->as_Bool()->_test._test == BoolTest::ne);\n-  match(Set dst (CMoveN (Binary cop cr) (Binary dst src)));\n-\n-  ins_cost(200); \/\/ XXX\n-  format %{ \"cmovpl  $dst, $src\\n\\t\"\n-            \"cmovnel $dst, $src\" %}\n-  ins_encode %{\n-    __ cmovl(Assembler::parity, $dst$$Register, $src$$Register);\n-    __ cmovl(Assembler::notEqual, $dst$$Register, $src$$Register);\n-  %}\n-  ins_pipe(pipe_cmov_reg);\n-%}\n-\n-\/\/ Since (x == y) == !(x != y), we can flip the sense of the test by flipping the\n-\/\/ inputs of the CMove\n-instruct cmovN_regUCF2_eq(cmpOpUCF2 cop, rFlagsRegUCF cr, rRegN dst, rRegN src) %{\n-  predicate(n->in(1)->in(1)->as_Bool()->_test._test == BoolTest::eq);\n-  match(Set dst (CMoveN (Binary cop cr) (Binary src dst)));\n-\n-  ins_cost(200); \/\/ XXX\n-  format %{ \"cmovpl  $dst, $src\\n\\t\"\n-            \"cmovnel $dst, $src\" %}\n-  ins_encode %{\n-    __ cmovl(Assembler::parity, $dst$$Register, $src$$Register);\n-    __ cmovl(Assembler::notEqual, $dst$$Register, $src$$Register);\n-  %}\n-  ins_pipe(pipe_cmov_reg);\n-%}\n-\n-\/\/ Conditional move\n-instruct cmovP_reg(rRegP dst, rRegP src, rFlagsReg cr, cmpOp cop)\n-%{\n-  predicate(!UseAPX);\n-  match(Set dst (CMoveP (Binary cop cr) (Binary dst src)));\n-\n-  ins_cost(200); \/\/ XXX\n-  format %{ \"cmovq$cop $dst, $src\\t# signed, ptr\" %}\n-  ins_encode %{\n-    __ cmovq((Assembler::Condition)($cop$$cmpcode), $dst$$Register, $src$$Register);\n-  %}\n-  ins_pipe(pipe_cmov_reg);  \/\/ XXX\n-%}\n-\n-\/\/ Conditional move ndd\n-instruct cmovP_reg_ndd(rRegP dst, rRegP src1, rRegP src2, rFlagsReg cr, cmpOp cop)\n-%{\n-  predicate(UseAPX);\n-  match(Set dst (CMoveP (Binary cop cr) (Binary src1 src2)));\n-\n-  ins_cost(200);\n-  format %{ \"ecmovq$cop $dst, $src1, $src2\\t# signed, ptr ndd\" %}\n-  ins_encode %{\n-    __ ecmovq((Assembler::Condition)($cop$$cmpcode), $dst$$Register, $src1$$Register, $src2$$Register);\n-  %}\n-  ins_pipe(pipe_cmov_reg);\n-%}\n-\n-\/\/ Conditional move\n-instruct cmovP_regU(cmpOpU cop, rFlagsRegU cr, rRegP dst, rRegP src)\n-%{\n-  predicate(!UseAPX);\n-  match(Set dst (CMoveP (Binary cop cr) (Binary dst src)));\n-\n-  ins_cost(200); \/\/ XXX\n-  format %{ \"cmovq$cop $dst, $src\\t# unsigned, ptr\" %}\n-  ins_encode %{\n-    __ cmovq((Assembler::Condition)($cop$$cmpcode), $dst$$Register, $src$$Register);\n-  %}\n-  ins_pipe(pipe_cmov_reg); \/\/ XXX\n-%}\n-\n-\/\/ Conditional move ndd\n-instruct cmovP_regU_ndd(rRegP dst, cmpOpU cop, rFlagsRegU cr, rRegP src1, rRegP src2)\n-%{\n-  predicate(UseAPX);\n-  match(Set dst (CMoveP (Binary cop cr) (Binary src1 src2)));\n-\n-  ins_cost(200);\n-  format %{ \"ecmovq$cop $dst, $src1, $src2\\t# unsigned, ptr ndd\" %}\n-  ins_encode %{\n-    __ ecmovq((Assembler::Condition)($cop$$cmpcode), $dst$$Register, $src1$$Register, $src2$$Register);\n-  %}\n-  ins_pipe(pipe_cmov_reg);\n-%}\n-\n-instruct cmovP_regUCF(cmpOpUCF cop, rFlagsRegUCF cr, rRegP dst, rRegP src) %{\n-  predicate(!UseAPX);\n-  match(Set dst (CMoveP (Binary cop cr) (Binary dst src)));\n-  ins_cost(200);\n-  expand %{\n-    cmovP_regU(cop, cr, dst, src);\n-  %}\n-%}\n-\n-instruct cmovP_regUCF_ndd(rRegP dst, cmpOpUCF cop, rFlagsRegUCF cr, rRegP src1, rRegP src2) %{\n-  predicate(UseAPX);\n-  match(Set dst (CMoveP (Binary cop cr) (Binary src1 src2)));\n-  ins_cost(200);\n-  format %{ \"ecmovq$cop $dst, $src1, $src2\\t# unsigned, ptr ndd\" %}\n-  ins_encode %{\n-    __ ecmovq((Assembler::Condition)($cop$$cmpcode), $dst$$Register, $src1$$Register, $src2$$Register);\n-  %}\n-  ins_pipe(pipe_cmov_reg);\n-%}\n-\n-instruct cmovP_regUCF2_ne(cmpOpUCF2 cop, rFlagsRegUCF cr, rRegP dst, rRegP src) %{\n-  predicate(!UseAPX && n->in(1)->in(1)->as_Bool()->_test._test == BoolTest::ne);\n-  match(Set dst (CMoveP (Binary cop cr) (Binary dst src)));\n-\n-  ins_cost(200); \/\/ XXX\n-  format %{ \"cmovpq  $dst, $src\\n\\t\"\n-            \"cmovneq $dst, $src\" %}\n-  ins_encode %{\n-    __ cmovq(Assembler::parity, $dst$$Register, $src$$Register);\n-    __ cmovq(Assembler::notEqual, $dst$$Register, $src$$Register);\n-  %}\n-  ins_pipe(pipe_cmov_reg);\n-%}\n-\n-instruct cmovP_regUCF2_ne_ndd(cmpOpUCF2 cop, rFlagsRegUCF cr, rRegP dst, rRegP src1, rRegP src2) %{\n-  predicate(UseAPX && n->in(1)->in(1)->as_Bool()->_test._test == BoolTest::ne);\n-  match(Set dst (CMoveP (Binary cop cr) (Binary src1 src2)));\n-  effect(TEMP dst);\n-\n-  ins_cost(200);\n-  format %{ \"ecmovpq  $dst, $src1, $src2\\n\\t\"\n-            \"cmovneq  $dst, $src2\" %}\n-  ins_encode %{\n-    __ ecmovq(Assembler::parity, $dst$$Register, $src1$$Register, $src2$$Register);\n-    __ cmovq(Assembler::notEqual, $dst$$Register, $src2$$Register);\n-  %}\n-  ins_pipe(pipe_cmov_reg);\n-%}\n-\n-\/\/ Since (x == y) == !(x != y), we can flip the sense of the test by flipping the\n-\/\/ inputs of the CMove\n-instruct cmovP_regUCF2_eq(cmpOpUCF2 cop, rFlagsRegUCF cr, rRegP dst, rRegP src) %{\n-  predicate(!UseAPX && n->in(1)->in(1)->as_Bool()->_test._test == BoolTest::eq);\n-  match(Set dst (CMoveP (Binary cop cr) (Binary src dst)));\n-\n-  ins_cost(200); \/\/ XXX\n-  format %{ \"cmovpq  $dst, $src\\n\\t\"\n-            \"cmovneq $dst, $src\" %}\n-  ins_encode %{\n-    __ cmovq(Assembler::parity, $dst$$Register, $src$$Register);\n-    __ cmovq(Assembler::notEqual, $dst$$Register, $src$$Register);\n-  %}\n-  ins_pipe(pipe_cmov_reg);\n-%}\n-\n-instruct cmovP_regUCF2_eq_ndd(cmpOpUCF2 cop, rFlagsRegUCF cr, rRegP dst, rRegP src1, rRegP src2) %{\n-  predicate(UseAPX && n->in(1)->in(1)->as_Bool()->_test._test == BoolTest::eq);\n-  match(Set dst (CMoveP (Binary cop cr) (Binary src2 src1)));\n-  effect(TEMP dst);\n-\n-  ins_cost(200);\n-  format %{ \"ecmovpq  $dst, $src1, $src2\\n\\t\"\n-            \"cmovneq  $dst, $src2\" %}\n-  ins_encode %{\n-    __ ecmovq(Assembler::parity, $dst$$Register, $src1$$Register, $src2$$Register);\n-    __ cmovq(Assembler::notEqual, $dst$$Register, $src2$$Register);\n-  %}\n-  ins_pipe(pipe_cmov_reg);\n-%}\n-\n-instruct cmovL_imm_01(rRegL dst, immL1 src, rFlagsReg cr, cmpOp cop)\n-%{\n-  predicate(n->in(2)->in(2)->is_Con() && n->in(2)->in(2)->get_long() == 0);\n-  match(Set dst (CMoveL (Binary cop cr) (Binary src dst)));\n-\n-  ins_cost(100); \/\/ XXX\n-  format %{ \"setbn$cop $dst\\t# signed, long\" %}\n-  ins_encode %{\n-    Assembler::Condition cond = (Assembler::Condition)($cop$$cmpcode);\n-    __ setb(MacroAssembler::negate_condition(cond), $dst$$Register);\n-  %}\n-  ins_pipe(ialu_reg);\n-%}\n-\n-instruct cmovL_reg(cmpOp cop, rFlagsReg cr, rRegL dst, rRegL src)\n-%{\n-  predicate(!UseAPX);\n-  match(Set dst (CMoveL (Binary cop cr) (Binary dst src)));\n-\n-  ins_cost(200); \/\/ XXX\n-  format %{ \"cmovq$cop $dst, $src\\t# signed, long\" %}\n-  ins_encode %{\n-    __ cmovq((Assembler::Condition)($cop$$cmpcode), $dst$$Register, $src$$Register);\n-  %}\n-  ins_pipe(pipe_cmov_reg);  \/\/ XXX\n-%}\n-\n-instruct cmovL_reg_ndd(rRegL dst, cmpOp cop, rFlagsReg cr, rRegL src1, rRegL src2)\n-%{\n-  predicate(UseAPX);\n-  match(Set dst (CMoveL (Binary cop cr) (Binary src1 src2)));\n-\n-  ins_cost(200);\n-  format %{ \"ecmovq$cop $dst, $src1, $src2\\t# signed, long ndd\" %}\n-  ins_encode %{\n-    __ ecmovq((Assembler::Condition)($cop$$cmpcode), $dst$$Register, $src1$$Register, $src2$$Register);\n-  %}\n-  ins_pipe(pipe_cmov_reg);\n-%}\n-\n-instruct cmovL_mem(cmpOp cop, rFlagsReg cr, rRegL dst, memory src)\n-%{\n-  predicate(!UseAPX);\n-  match(Set dst (CMoveL (Binary cop cr) (Binary dst (LoadL src))));\n-\n-  ins_cost(200); \/\/ XXX\n-  format %{ \"cmovq$cop $dst, $src\\t# signed, long\" %}\n-  ins_encode %{\n-    __ cmovq((Assembler::Condition)($cop$$cmpcode), $dst$$Register, $src$$Address);\n-  %}\n-  ins_pipe(pipe_cmov_mem);  \/\/ XXX\n-%}\n-\n-instruct cmovL_rReg_rReg_mem_ndd(rRegL dst, cmpOp cop, rFlagsReg cr, rRegL src1, memory src2)\n-%{\n-  predicate(UseAPX);\n-  match(Set dst (CMoveL (Binary cop cr) (Binary src1 (LoadL src2))));\n-\n-  ins_cost(200);\n-  format %{ \"ecmovq$cop $dst, $src1, $src2\\t# signed, long ndd\" %}\n-  ins_encode %{\n-    __ ecmovq((Assembler::Condition)($cop$$cmpcode), $dst$$Register, $src1$$Register, $src2$$Address);\n-  %}\n-  ins_pipe(pipe_cmov_mem);\n-%}\n-\n-instruct cmovL_imm_01U(rRegL dst, immL1 src, rFlagsRegU cr, cmpOpU cop)\n-%{\n-  predicate(n->in(2)->in(2)->is_Con() && n->in(2)->in(2)->get_long() == 0);\n-  match(Set dst (CMoveL (Binary cop cr) (Binary src dst)));\n-\n-  ins_cost(100); \/\/ XXX\n-  format %{ \"setbn$cop $dst\\t# unsigned, long\" %}\n-  ins_encode %{\n-    Assembler::Condition cond = (Assembler::Condition)($cop$$cmpcode);\n-    __ setb(MacroAssembler::negate_condition(cond), $dst$$Register);\n-  %}\n-  ins_pipe(ialu_reg);\n-%}\n-\n-instruct cmovL_regU(cmpOpU cop, rFlagsRegU cr, rRegL dst, rRegL src)\n-%{\n-  predicate(!UseAPX);\n-  match(Set dst (CMoveL (Binary cop cr) (Binary dst src)));\n-\n-  ins_cost(200); \/\/ XXX\n-  format %{ \"cmovq$cop $dst, $src\\t# unsigned, long\" %}\n-  ins_encode %{\n-    __ cmovq((Assembler::Condition)($cop$$cmpcode), $dst$$Register, $src$$Register);\n-  %}\n-  ins_pipe(pipe_cmov_reg); \/\/ XXX\n-%}\n-\n-instruct cmovL_regU_ndd(rRegL dst, cmpOpU cop, rFlagsRegU cr, rRegL src1, rRegL src2)\n-%{\n-  predicate(UseAPX);\n-  match(Set dst (CMoveL (Binary cop cr) (Binary src1 src2)));\n-\n-  ins_cost(200);\n-  format %{ \"ecmovq$cop $dst, $src1, $src2\\t# unsigned, long ndd\" %}\n-  ins_encode %{\n-    __ ecmovq((Assembler::Condition)($cop$$cmpcode), $dst$$Register, $src1$$Register, $src2$$Register);\n-  %}\n-  ins_pipe(pipe_cmov_reg);\n-%}\n-\n-instruct cmovL_imm_01UCF(rRegL dst, immL1 src, rFlagsRegUCF cr, cmpOpUCF cop)\n-%{\n-  predicate(n->in(2)->in(2)->is_Con() && n->in(2)->in(2)->get_long() == 0);\n-  match(Set dst (CMoveL (Binary cop cr) (Binary src dst)));\n-\n-  ins_cost(100); \/\/ XXX\n-  format %{ \"setbn$cop $dst\\t# unsigned, long\" %}\n-  ins_encode %{\n-    Assembler::Condition cond = (Assembler::Condition)($cop$$cmpcode);\n-    __ setb(MacroAssembler::negate_condition(cond), $dst$$Register);\n-  %}\n-  ins_pipe(ialu_reg);\n-%}\n-\n-instruct cmovL_regUCF(cmpOpUCF cop, rFlagsRegUCF cr, rRegL dst, rRegL src) %{\n-  predicate(!UseAPX);\n-  match(Set dst (CMoveL (Binary cop cr) (Binary dst src)));\n-  ins_cost(200);\n-  expand %{\n-    cmovL_regU(cop, cr, dst, src);\n-  %}\n-%}\n-\n-instruct cmovL_regUCF_ndd(rRegL dst, cmpOpUCF cop, rFlagsRegUCF cr, rRegL src1, rRegL src2)\n-%{\n-  predicate(UseAPX);\n-  match(Set dst (CMoveL (Binary cop cr) (Binary src1 src2)));\n-  ins_cost(200);\n-  format %{ \"ecmovq$cop $dst, $src1, $src2\\t# unsigned, long ndd\" %}\n-  ins_encode %{\n-    __ ecmovq((Assembler::Condition)($cop$$cmpcode), $dst$$Register, $src1$$Register, $src2$$Register);\n-  %}\n-  ins_pipe(pipe_cmov_reg);\n-%}\n-\n-instruct cmovL_regUCF2_ne(cmpOpUCF2 cop, rFlagsRegUCF cr, rRegL dst, rRegL src) %{\n-  predicate(!UseAPX && n->in(1)->in(1)->as_Bool()->_test._test == BoolTest::ne);\n-  match(Set dst (CMoveL (Binary cop cr) (Binary dst src)));\n-\n-  ins_cost(200); \/\/ XXX\n-  format %{ \"cmovpq  $dst, $src\\n\\t\"\n-            \"cmovneq $dst, $src\" %}\n-  ins_encode %{\n-    __ cmovq(Assembler::parity, $dst$$Register, $src$$Register);\n-    __ cmovq(Assembler::notEqual, $dst$$Register, $src$$Register);\n-  %}\n-  ins_pipe(pipe_cmov_reg);\n-%}\n-\n-instruct cmovL_regUCF2_ne_ndd(cmpOpUCF2 cop, rFlagsRegUCF cr, rRegL dst, rRegL src1, rRegL src2) %{\n-  predicate(UseAPX && n->in(1)->in(1)->as_Bool()->_test._test == BoolTest::ne);\n-  match(Set dst (CMoveL (Binary cop cr) (Binary src1 src2)));\n-  effect(TEMP dst);\n-\n-  ins_cost(200);\n-  format %{ \"ecmovpq  $dst, $src1, $src2\\n\\t\"\n-            \"cmovneq  $dst, $src2\" %}\n-  ins_encode %{\n-    __ ecmovq(Assembler::parity, $dst$$Register, $src1$$Register, $src2$$Register);\n-    __ cmovq(Assembler::notEqual, $dst$$Register, $src2$$Register);\n-  %}\n-  ins_pipe(pipe_cmov_reg);\n-%}\n-\n-\/\/ Since (x == y) == !(x != y), we can flip the sense of the test by flipping the\n-\/\/ inputs of the CMove\n-instruct cmovL_regUCF2_eq(cmpOpUCF2 cop, rFlagsRegUCF cr, rRegL dst, rRegL src) %{\n-  predicate(!UseAPX && n->in(1)->in(1)->as_Bool()->_test._test == BoolTest::eq);\n-  match(Set dst (CMoveL (Binary cop cr) (Binary src dst)));\n-\n-  ins_cost(200); \/\/ XXX\n-  format %{ \"cmovpq  $dst, $src\\n\\t\"\n-            \"cmovneq $dst, $src\" %}\n-  ins_encode %{\n-    __ cmovq(Assembler::parity, $dst$$Register, $src$$Register);\n-    __ cmovq(Assembler::notEqual, $dst$$Register, $src$$Register);\n-  %}\n-  ins_pipe(pipe_cmov_reg);\n-%}\n-\n-instruct cmovL_regUCF2_eq_ndd(cmpOpUCF2 cop, rFlagsRegUCF cr, rRegL dst, rRegL src1, rRegL src2) %{\n-  predicate(UseAPX && n->in(1)->in(1)->as_Bool()->_test._test == BoolTest::eq);\n-  match(Set dst (CMoveL (Binary cop cr) (Binary src2 src1)));\n-  effect(TEMP dst);\n-\n-  ins_cost(200);\n-  format %{ \"ecmovpq  $dst, $src1, $src2\\n\\t\"\n-            \"cmovneq $dst, $src2\" %}\n-  ins_encode %{\n-    __ ecmovq(Assembler::parity, $dst$$Register, $src1$$Register, $src2$$Register);\n-    __ cmovq(Assembler::notEqual, $dst$$Register, $src2$$Register);\n-  %}\n-  ins_pipe(pipe_cmov_reg);\n-%}\n-\n-instruct cmovL_memU(cmpOpU cop, rFlagsRegU cr, rRegL dst, memory src)\n-%{\n-  predicate(!UseAPX);\n-  match(Set dst (CMoveL (Binary cop cr) (Binary dst (LoadL src))));\n-\n-  ins_cost(200); \/\/ XXX\n-  format %{ \"cmovq$cop $dst, $src\\t# unsigned, long\" %}\n-  ins_encode %{\n-    __ cmovq((Assembler::Condition)($cop$$cmpcode), $dst$$Register, $src$$Address);\n-  %}\n-  ins_pipe(pipe_cmov_mem); \/\/ XXX\n-%}\n-\n-instruct cmovL_memUCF(cmpOpUCF cop, rFlagsRegUCF cr, rRegL dst, memory src) %{\n-  predicate(!UseAPX);\n-  match(Set dst (CMoveL (Binary cop cr) (Binary dst (LoadL src))));\n-  ins_cost(200);\n-  expand %{\n-    cmovL_memU(cop, cr, dst, src);\n-  %}\n-%}\n-\n-instruct cmovL_rReg_rReg_memU_ndd(rRegL dst, cmpOpU cop, rFlagsRegU cr, rRegL src1, memory src2)\n-%{\n-  predicate(UseAPX);\n-  match(Set dst (CMoveL (Binary cop cr) (Binary src1 (LoadL src2))));\n-\n-  ins_cost(200);\n-  format %{ \"ecmovq$cop $dst, $src1, $src2\\t# unsigned, long ndd\" %}\n-  ins_encode %{\n-    __ ecmovq((Assembler::Condition)($cop$$cmpcode), $dst$$Register, $src1$$Register, $src2$$Address);\n-  %}\n-  ins_pipe(pipe_cmov_mem);\n-%}\n-\n-instruct cmovL_rReg_rReg_memUCF_ndd(rRegL dst, cmpOpUCF cop, rFlagsRegUCF cr, rRegL src1, memory src2)\n-%{\n-  predicate(UseAPX);\n-  match(Set dst (CMoveL (Binary cop cr) (Binary src1 (LoadL src2))));\n-  ins_cost(200);\n-  format %{ \"ecmovq$cop $dst, $src1, $src2\\t# unsigned, long ndd\" %}\n-  ins_encode %{\n-    __ ecmovq((Assembler::Condition)($cop$$cmpcode), $dst$$Register, $src1$$Register, $src2$$Address);\n-  %}\n-  ins_pipe(pipe_cmov_mem);\n-%}\n-\n-instruct cmovF_reg(cmpOp cop, rFlagsReg cr, regF dst, regF src)\n-%{\n-  match(Set dst (CMoveF (Binary cop cr) (Binary dst src)));\n-\n-  ins_cost(200); \/\/ XXX\n-  format %{ \"jn$cop    skip\\t# signed cmove float\\n\\t\"\n-            \"movss     $dst, $src\\n\"\n-    \"skip:\" %}\n-  ins_encode %{\n-    Label Lskip;\n-    \/\/ Invert sense of branch from sense of CMOV\n-    __ jccb((Assembler::Condition)($cop$$cmpcode^1), Lskip);\n-    __ movflt($dst$$XMMRegister, $src$$XMMRegister);\n-    __ bind(Lskip);\n-  %}\n-  ins_pipe(pipe_slow);\n-%}\n-\n-instruct cmovF_regU(cmpOpU cop, rFlagsRegU cr, regF dst, regF src)\n-%{\n-  match(Set dst (CMoveF (Binary cop cr) (Binary dst src)));\n-\n-  ins_cost(200); \/\/ XXX\n-  format %{ \"jn$cop    skip\\t# unsigned cmove float\\n\\t\"\n-            \"movss     $dst, $src\\n\"\n-    \"skip:\" %}\n-  ins_encode %{\n-    Label Lskip;\n-    \/\/ Invert sense of branch from sense of CMOV\n-    __ jccb((Assembler::Condition)($cop$$cmpcode^1), Lskip);\n-    __ movflt($dst$$XMMRegister, $src$$XMMRegister);\n-    __ bind(Lskip);\n-  %}\n-  ins_pipe(pipe_slow);\n-%}\n-\n-instruct cmovF_regUCF(cmpOpUCF cop, rFlagsRegUCF cr, regF dst, regF src) %{\n-  match(Set dst (CMoveF (Binary cop cr) (Binary dst src)));\n-  ins_cost(200);\n-  expand %{\n-    cmovF_regU(cop, cr, dst, src);\n-  %}\n-%}\n-\n-instruct cmovD_reg(cmpOp cop, rFlagsReg cr, regD dst, regD src)\n-%{\n-  match(Set dst (CMoveD (Binary cop cr) (Binary dst src)));\n-\n-  ins_cost(200); \/\/ XXX\n-  format %{ \"jn$cop    skip\\t# signed cmove double\\n\\t\"\n-            \"movsd     $dst, $src\\n\"\n-    \"skip:\" %}\n-  ins_encode %{\n-    Label Lskip;\n-    \/\/ Invert sense of branch from sense of CMOV\n-    __ jccb((Assembler::Condition)($cop$$cmpcode^1), Lskip);\n-    __ movdbl($dst$$XMMRegister, $src$$XMMRegister);\n-    __ bind(Lskip);\n-  %}\n-  ins_pipe(pipe_slow);\n-%}\n-\n-instruct cmovD_regU(cmpOpU cop, rFlagsRegU cr, regD dst, regD src)\n-%{\n-  match(Set dst (CMoveD (Binary cop cr) (Binary dst src)));\n-\n-  ins_cost(200); \/\/ XXX\n-  format %{ \"jn$cop    skip\\t# unsigned cmove double\\n\\t\"\n-            \"movsd     $dst, $src\\n\"\n-    \"skip:\" %}\n-  ins_encode %{\n-    Label Lskip;\n-    \/\/ Invert sense of branch from sense of CMOV\n-    __ jccb((Assembler::Condition)($cop$$cmpcode^1), Lskip);\n-    __ movdbl($dst$$XMMRegister, $src$$XMMRegister);\n-    __ bind(Lskip);\n-  %}\n-  ins_pipe(pipe_slow);\n-%}\n-\n-instruct cmovD_regUCF(cmpOpUCF cop, rFlagsRegUCF cr, regD dst, regD src) %{\n-  match(Set dst (CMoveD (Binary cop cr) (Binary dst src)));\n-  ins_cost(200);\n-  expand %{\n-    cmovD_regU(cop, cr, dst, src);\n-  %}\n-%}\n-\n-\/\/----------Arithmetic Instructions--------------------------------------------\n-\/\/----------Addition Instructions----------------------------------------------\n-\n-instruct addI_rReg(rRegI dst, rRegI src, rFlagsReg cr)\n-%{\n-  predicate(!UseAPX);\n-  match(Set dst (AddI dst src));\n-  effect(KILL cr);\n-  flag(PD::Flag_sets_overflow_flag, PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_carry_flag, PD::Flag_sets_parity_flag);\n-  format %{ \"addl    $dst, $src\\t# int\" %}\n-  ins_encode %{\n-    __ addl($dst$$Register, $src$$Register);\n-  %}\n-  ins_pipe(ialu_reg_reg);\n-%}\n-\n-instruct addI_rReg_ndd(rRegI dst, rRegI src1, rRegI src2, rFlagsReg cr)\n-%{\n-  predicate(UseAPX);\n-  match(Set dst (AddI src1 src2));\n-  effect(KILL cr);\n-  flag(PD::Flag_sets_overflow_flag, PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_carry_flag, PD::Flag_sets_parity_flag);\n-\n-  format %{ \"eaddl    $dst, $src1, $src2\\t# int ndd\" %}\n-  ins_encode %{\n-    __ eaddl($dst$$Register, $src1$$Register, $src2$$Register, false);\n-  %}\n-  ins_pipe(ialu_reg_reg);\n-%}\n-\n-instruct addI_rReg_imm(rRegI dst, immI src, rFlagsReg cr)\n-%{\n-  predicate(!UseAPX);\n-  match(Set dst (AddI dst src));\n-  effect(KILL cr);\n-  flag(PD::Flag_sets_overflow_flag, PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_carry_flag, PD::Flag_sets_parity_flag);\n-\n-  format %{ \"addl    $dst, $src\\t# int\" %}\n-  ins_encode %{\n-    __ addl($dst$$Register, $src$$constant);\n-  %}\n-  ins_pipe( ialu_reg );\n-%}\n-\n-instruct addI_rReg_rReg_imm_ndd(rRegI dst, rRegI src1, immI src2, rFlagsReg cr)\n-%{\n-  predicate(UseAPX);\n-  match(Set dst (AddI src1 src2));\n-  effect(KILL cr);\n-  flag(PD::Flag_sets_overflow_flag, PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_carry_flag, PD::Flag_sets_parity_flag);\n-\n-  format %{ \"eaddl    $dst, $src1, $src2\\t# int ndd\" %}\n-  ins_encode %{\n-    __ eaddl($dst$$Register, $src1$$Register, $src2$$constant, false);\n-  %}\n-  ins_pipe( ialu_reg );\n-%}\n-\n-instruct addI_rReg_mem_imm_ndd(rRegI dst, memory src1, immI src2, rFlagsReg cr)\n-%{\n-  predicate(UseAPX);\n-  match(Set dst (AddI (LoadI src1) src2));\n-  effect(KILL cr);\n-  flag(PD::Flag_sets_overflow_flag, PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_carry_flag, PD::Flag_sets_parity_flag);\n-\n-  format %{ \"eaddl    $dst, $src1, $src2\\t# int ndd\" %}\n-  ins_encode %{\n-    __ eaddl($dst$$Register, $src1$$Address, $src2$$constant, false);\n-  %}\n-  ins_pipe( ialu_reg );\n-%}\n-\n-instruct addI_rReg_mem(rRegI dst, memory src, rFlagsReg cr)\n-%{\n-  predicate(!UseAPX);\n-  match(Set dst (AddI dst (LoadI src)));\n-  effect(KILL cr);\n-  flag(PD::Flag_sets_overflow_flag, PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_carry_flag, PD::Flag_sets_parity_flag);\n-\n-  ins_cost(150); \/\/ XXX\n-  format %{ \"addl    $dst, $src\\t# int\" %}\n-  ins_encode %{\n-    __ addl($dst$$Register, $src$$Address);\n-  %}\n-  ins_pipe(ialu_reg_mem);\n-%}\n-\n-instruct addI_rReg_rReg_mem_ndd(rRegI dst, rRegI src1, memory src2, rFlagsReg cr)\n-%{\n-  predicate(UseAPX);\n-  match(Set dst (AddI src1 (LoadI src2)));\n-  effect(KILL cr);\n-  flag(PD::Flag_sets_overflow_flag, PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_carry_flag, PD::Flag_sets_parity_flag);\n-\n-  ins_cost(150);\n-  format %{ \"eaddl    $dst, $src1, $src2\\t# int ndd\" %}\n-  ins_encode %{\n-    __ eaddl($dst$$Register, $src1$$Register, $src2$$Address, false);\n-  %}\n-  ins_pipe(ialu_reg_mem);\n-%}\n-\n-instruct addI_mem_rReg(memory dst, rRegI src, rFlagsReg cr)\n-%{\n-  match(Set dst (StoreI dst (AddI (LoadI dst) src)));\n-  effect(KILL cr);\n-  flag(PD::Flag_sets_overflow_flag, PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_carry_flag, PD::Flag_sets_parity_flag);\n-\n-  ins_cost(150); \/\/ XXX\n-  format %{ \"addl    $dst, $src\\t# int\" %}\n-  ins_encode %{\n-    __ addl($dst$$Address, $src$$Register);\n-  %}\n-  ins_pipe(ialu_mem_reg);\n-%}\n-\n-instruct addI_mem_imm(memory dst, immI src, rFlagsReg cr)\n-%{\n-  match(Set dst (StoreI dst (AddI (LoadI dst) src)));\n-  effect(KILL cr);\n-  flag(PD::Flag_sets_overflow_flag, PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_carry_flag, PD::Flag_sets_parity_flag);\n-\n-\n-  ins_cost(125); \/\/ XXX\n-  format %{ \"addl    $dst, $src\\t# int\" %}\n-  ins_encode %{\n-    __ addl($dst$$Address, $src$$constant);\n-  %}\n-  ins_pipe(ialu_mem_imm);\n-%}\n-\n-instruct incI_rReg(rRegI dst, immI_1 src, rFlagsReg cr)\n-%{\n-  predicate(!UseAPX && UseIncDec);\n-  match(Set dst (AddI dst src));\n-  effect(KILL cr);\n-\n-  format %{ \"incl    $dst\\t# int\" %}\n-  ins_encode %{\n-    __ incrementl($dst$$Register);\n-  %}\n-  ins_pipe(ialu_reg);\n-%}\n-\n-instruct incI_rReg_ndd(rRegI dst, rRegI src, immI_1 val, rFlagsReg cr)\n-%{\n-  predicate(UseAPX && UseIncDec);\n-  match(Set dst (AddI src val));\n-  effect(KILL cr);\n-\n-  format %{ \"eincl    $dst, $src\\t# int ndd\" %}\n-  ins_encode %{\n-    __ eincl($dst$$Register, $src$$Register, false);\n-  %}\n-  ins_pipe(ialu_reg);\n-%}\n-\n-instruct incI_rReg_mem_ndd(rRegI dst, memory src, immI_1 val, rFlagsReg cr)\n-%{\n-  predicate(UseAPX && UseIncDec);\n-  match(Set dst (AddI (LoadI src) val));\n-  effect(KILL cr);\n-\n-  format %{ \"eincl    $dst, $src\\t# int ndd\" %}\n-  ins_encode %{\n-    __ eincl($dst$$Register, $src$$Address, false);\n-  %}\n-  ins_pipe(ialu_reg);\n-%}\n-\n-instruct incI_mem(memory dst, immI_1 src, rFlagsReg cr)\n-%{\n-  predicate(UseIncDec);\n-  match(Set dst (StoreI dst (AddI (LoadI dst) src)));\n-  effect(KILL cr);\n-\n-  ins_cost(125); \/\/ XXX\n-  format %{ \"incl    $dst\\t# int\" %}\n-  ins_encode %{\n-    __ incrementl($dst$$Address);\n-  %}\n-  ins_pipe(ialu_mem_imm);\n-%}\n-\n-\/\/ XXX why does that use AddI\n-instruct decI_rReg(rRegI dst, immI_M1 src, rFlagsReg cr)\n-%{\n-  predicate(!UseAPX && UseIncDec);\n-  match(Set dst (AddI dst src));\n-  effect(KILL cr);\n-\n-  format %{ \"decl    $dst\\t# int\" %}\n-  ins_encode %{\n-    __ decrementl($dst$$Register);\n-  %}\n-  ins_pipe(ialu_reg);\n-%}\n-\n-instruct decI_rReg_ndd(rRegI dst, rRegI src, immI_M1 val, rFlagsReg cr)\n-%{\n-  predicate(UseAPX && UseIncDec);\n-  match(Set dst (AddI src val));\n-  effect(KILL cr);\n-\n-  format %{ \"edecl    $dst, $src\\t# int ndd\" %}\n-  ins_encode %{\n-    __ edecl($dst$$Register, $src$$Register, false);\n-  %}\n-  ins_pipe(ialu_reg);\n-%}\n-\n-instruct decI_rReg_mem_ndd(rRegI dst, memory src, immI_M1 val, rFlagsReg cr)\n-%{\n-  predicate(UseAPX && UseIncDec);\n-  match(Set dst (AddI (LoadI src) val));\n-  effect(KILL cr);\n-\n-  format %{ \"edecl    $dst, $src\\t# int ndd\" %}\n-  ins_encode %{\n-    __ edecl($dst$$Register, $src$$Address, false);\n-  %}\n-  ins_pipe(ialu_reg);\n-%}\n-\n-\/\/ XXX why does that use AddI\n-instruct decI_mem(memory dst, immI_M1 src, rFlagsReg cr)\n-%{\n-  predicate(UseIncDec);\n-  match(Set dst (StoreI dst (AddI (LoadI dst) src)));\n-  effect(KILL cr);\n-\n-  ins_cost(125); \/\/ XXX\n-  format %{ \"decl    $dst\\t# int\" %}\n-  ins_encode %{\n-    __ decrementl($dst$$Address);\n-  %}\n-  ins_pipe(ialu_mem_imm);\n-%}\n-\n-instruct leaI_rReg_immI2_immI(rRegI dst, rRegI index, immI2 scale, immI disp)\n-%{\n-  predicate(VM_Version::supports_fast_2op_lea());\n-  match(Set dst (AddI (LShiftI index scale) disp));\n-\n-  format %{ \"leal $dst, [$index << $scale + $disp]\\t# int\" %}\n-  ins_encode %{\n-    Address::ScaleFactor scale = static_cast<Address::ScaleFactor>($scale$$constant);\n-    __ leal($dst$$Register, Address(noreg, $index$$Register, scale, $disp$$constant));\n-  %}\n-  ins_pipe(ialu_reg_reg);\n-%}\n-\n-instruct leaI_rReg_rReg_immI(rRegI dst, rRegI base, rRegI index, immI disp)\n-%{\n-  predicate(VM_Version::supports_fast_3op_lea());\n-  match(Set dst (AddI (AddI base index) disp));\n-\n-  format %{ \"leal $dst, [$base + $index + $disp]\\t# int\" %}\n-  ins_encode %{\n-    __ leal($dst$$Register, Address($base$$Register, $index$$Register, Address::times_1, $disp$$constant));\n-  %}\n-  ins_pipe(ialu_reg_reg);\n-%}\n-\n-instruct leaI_rReg_rReg_immI2(rRegI dst, no_rbp_r13_RegI base, rRegI index, immI2 scale)\n-%{\n-  predicate(VM_Version::supports_fast_2op_lea());\n-  match(Set dst (AddI base (LShiftI index scale)));\n-\n-  format %{ \"leal $dst, [$base + $index << $scale]\\t# int\" %}\n-  ins_encode %{\n-    Address::ScaleFactor scale = static_cast<Address::ScaleFactor>($scale$$constant);\n-    __ leal($dst$$Register, Address($base$$Register, $index$$Register, scale));\n-  %}\n-  ins_pipe(ialu_reg_reg);\n-%}\n-\n-instruct leaI_rReg_rReg_immI2_immI(rRegI dst, rRegI base, rRegI index, immI2 scale, immI disp)\n-%{\n-  predicate(VM_Version::supports_fast_3op_lea());\n-  match(Set dst (AddI (AddI base (LShiftI index scale)) disp));\n-\n-  format %{ \"leal $dst, [$base + $index << $scale + $disp]\\t# int\" %}\n-  ins_encode %{\n-    Address::ScaleFactor scale = static_cast<Address::ScaleFactor>($scale$$constant);\n-    __ leal($dst$$Register, Address($base$$Register, $index$$Register, scale, $disp$$constant));\n-  %}\n-  ins_pipe(ialu_reg_reg);\n-%}\n-\n-instruct addL_rReg(rRegL dst, rRegL src, rFlagsReg cr)\n-%{\n-  predicate(!UseAPX);\n-  match(Set dst (AddL dst src));\n-  effect(KILL cr);\n-  flag(PD::Flag_sets_overflow_flag, PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_carry_flag, PD::Flag_sets_parity_flag);\n-\n-  format %{ \"addq    $dst, $src\\t# long\" %}\n-  ins_encode %{\n-    __ addq($dst$$Register, $src$$Register);\n-  %}\n-  ins_pipe(ialu_reg_reg);\n-%}\n-\n-instruct addL_rReg_ndd(rRegL dst, rRegL src1, rRegL src2, rFlagsReg cr)\n-%{\n-  predicate(UseAPX);\n-  match(Set dst (AddL src1 src2));\n-  effect(KILL cr);\n-  flag(PD::Flag_sets_overflow_flag, PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_carry_flag, PD::Flag_sets_parity_flag);\n-\n-  format %{ \"eaddq    $dst, $src1, $src2\\t# long ndd\" %}\n-  ins_encode %{\n-    __ eaddq($dst$$Register, $src1$$Register, $src2$$Register, false);\n-  %}\n-  ins_pipe(ialu_reg_reg);\n-%}\n-\n-instruct addL_rReg_imm(rRegL dst, immL32 src, rFlagsReg cr)\n-%{\n-  predicate(!UseAPX);\n-  match(Set dst (AddL dst src));\n-  effect(KILL cr);\n-  flag(PD::Flag_sets_overflow_flag, PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_carry_flag, PD::Flag_sets_parity_flag);\n-\n-  format %{ \"addq    $dst, $src\\t# long\" %}\n-  ins_encode %{\n-    __ addq($dst$$Register, $src$$constant);\n-  %}\n-  ins_pipe( ialu_reg );\n-%}\n-\n-instruct addL_rReg_rReg_imm_ndd(rRegL dst, rRegL src1, immL32 src2, rFlagsReg cr)\n-%{\n-  predicate(UseAPX);\n-  match(Set dst (AddL src1 src2));\n-  effect(KILL cr);\n-  flag(PD::Flag_sets_overflow_flag, PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_carry_flag, PD::Flag_sets_parity_flag);\n-\n-  format %{ \"eaddq    $dst, $src1, $src2\\t# long ndd\" %}\n-  ins_encode %{\n-    __ eaddq($dst$$Register, $src1$$Register, $src2$$constant, false);\n-  %}\n-  ins_pipe( ialu_reg );\n-%}\n-\n-instruct addL_rReg_mem_imm_ndd(rRegL dst, memory src1, immL32 src2, rFlagsReg cr)\n-%{\n-  predicate(UseAPX);\n-  match(Set dst (AddL (LoadL src1) src2));\n-  effect(KILL cr);\n-  flag(PD::Flag_sets_overflow_flag, PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_carry_flag, PD::Flag_sets_parity_flag);\n-\n-  format %{ \"eaddq    $dst, $src1, $src2\\t# long ndd\" %}\n-  ins_encode %{\n-    __ eaddq($dst$$Register, $src1$$Address, $src2$$constant, false);\n-  %}\n-  ins_pipe( ialu_reg );\n-%}\n-\n-instruct addL_rReg_mem(rRegL dst, memory src, rFlagsReg cr)\n-%{\n-  predicate(!UseAPX);\n-  match(Set dst (AddL dst (LoadL src)));\n-  effect(KILL cr);\n-  flag(PD::Flag_sets_overflow_flag, PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_carry_flag, PD::Flag_sets_parity_flag);\n-\n-  ins_cost(150); \/\/ XXX\n-  format %{ \"addq    $dst, $src\\t# long\" %}\n-  ins_encode %{\n-    __ addq($dst$$Register, $src$$Address);\n-  %}\n-  ins_pipe(ialu_reg_mem);\n-%}\n-\n-instruct addL_rReg_rReg_mem_ndd(rRegL dst, rRegL src1, memory src2, rFlagsReg cr)\n-%{\n-  predicate(UseAPX);\n-  match(Set dst (AddL src1 (LoadL src2)));\n-  effect(KILL cr);\n-  flag(PD::Flag_sets_overflow_flag, PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_carry_flag, PD::Flag_sets_parity_flag);\n-\n-  ins_cost(150);\n-  format %{ \"eaddq    $dst, $src1, $src2\\t# long ndd\" %}\n-  ins_encode %{\n-    __ eaddq($dst$$Register, $src1$$Register, $src2$$Address, false);\n-  %}\n-  ins_pipe(ialu_reg_mem);\n-%}\n-\n-instruct addL_mem_rReg(memory dst, rRegL src, rFlagsReg cr)\n-%{\n-  match(Set dst (StoreL dst (AddL (LoadL dst) src)));\n-  effect(KILL cr);\n-  flag(PD::Flag_sets_overflow_flag, PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_carry_flag, PD::Flag_sets_parity_flag);\n-\n-  ins_cost(150); \/\/ XXX\n-  format %{ \"addq    $dst, $src\\t# long\" %}\n-  ins_encode %{\n-    __ addq($dst$$Address, $src$$Register);\n-  %}\n-  ins_pipe(ialu_mem_reg);\n-%}\n-\n-instruct addL_mem_imm(memory dst, immL32 src, rFlagsReg cr)\n-%{\n-  match(Set dst (StoreL dst (AddL (LoadL dst) src)));\n-  effect(KILL cr);\n-  flag(PD::Flag_sets_overflow_flag, PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_carry_flag, PD::Flag_sets_parity_flag);\n-\n-  ins_cost(125); \/\/ XXX\n-  format %{ \"addq    $dst, $src\\t# long\" %}\n-  ins_encode %{\n-    __ addq($dst$$Address, $src$$constant);\n-  %}\n-  ins_pipe(ialu_mem_imm);\n-%}\n-\n-instruct incL_rReg(rRegL dst, immL1 src, rFlagsReg cr)\n-%{\n-  predicate(!UseAPX && UseIncDec);\n-  match(Set dst (AddL dst src));\n-  effect(KILL cr);\n-\n-  format %{ \"incq    $dst\\t# long\" %}\n-  ins_encode %{\n-    __ incrementq($dst$$Register);\n-  %}\n-  ins_pipe(ialu_reg);\n-%}\n-\n-instruct incL_rReg_ndd(rRegL dst, rRegI src, immL1 val, rFlagsReg cr)\n-%{\n-  predicate(UseAPX && UseIncDec);\n-  match(Set dst (AddL src val));\n-  effect(KILL cr);\n-\n-  format %{ \"eincq    $dst, $src\\t# long ndd\" %}\n-  ins_encode %{\n-    __ eincq($dst$$Register, $src$$Register, false);\n-  %}\n-  ins_pipe(ialu_reg);\n-%}\n-\n-instruct incL_rReg_mem_ndd(rRegL dst, memory src, immL1 val, rFlagsReg cr)\n-%{\n-  predicate(UseAPX && UseIncDec);\n-  match(Set dst (AddL (LoadL src) val));\n-  effect(KILL cr);\n-\n-  format %{ \"eincq    $dst, $src\\t# long ndd\" %}\n-  ins_encode %{\n-    __ eincq($dst$$Register, $src$$Address, false);\n-  %}\n-  ins_pipe(ialu_reg);\n-%}\n-\n-instruct incL_mem(memory dst, immL1 src, rFlagsReg cr)\n-%{\n-  predicate(UseIncDec);\n-  match(Set dst (StoreL dst (AddL (LoadL dst) src)));\n-  effect(KILL cr);\n-\n-  ins_cost(125); \/\/ XXX\n-  format %{ \"incq    $dst\\t# long\" %}\n-  ins_encode %{\n-    __ incrementq($dst$$Address);\n-  %}\n-  ins_pipe(ialu_mem_imm);\n-%}\n-\n-\/\/ XXX why does that use AddL\n-instruct decL_rReg(rRegL dst, immL_M1 src, rFlagsReg cr)\n-%{\n-  predicate(!UseAPX && UseIncDec);\n-  match(Set dst (AddL dst src));\n-  effect(KILL cr);\n-\n-  format %{ \"decq    $dst\\t# long\" %}\n-  ins_encode %{\n-    __ decrementq($dst$$Register);\n-  %}\n-  ins_pipe(ialu_reg);\n-%}\n-\n-instruct decL_rReg_ndd(rRegL dst, rRegL src, immL_M1 val, rFlagsReg cr)\n-%{\n-  predicate(UseAPX && UseIncDec);\n-  match(Set dst (AddL src val));\n-  effect(KILL cr);\n-\n-  format %{ \"edecq    $dst, $src\\t# long ndd\" %}\n-  ins_encode %{\n-    __ edecq($dst$$Register, $src$$Register, false);\n-  %}\n-  ins_pipe(ialu_reg);\n-%}\n-\n-instruct decL_rReg_mem_ndd(rRegL dst, memory src, immL_M1 val, rFlagsReg cr)\n-%{\n-  predicate(UseAPX && UseIncDec);\n-  match(Set dst (AddL (LoadL src) val));\n-  effect(KILL cr);\n-\n-  format %{ \"edecq    $dst, $src\\t# long ndd\" %}\n-  ins_encode %{\n-    __ edecq($dst$$Register, $src$$Address, false);\n-  %}\n-  ins_pipe(ialu_reg);\n-%}\n-\n-\/\/ XXX why does that use AddL\n-instruct decL_mem(memory dst, immL_M1 src, rFlagsReg cr)\n-%{\n-  predicate(UseIncDec);\n-  match(Set dst (StoreL dst (AddL (LoadL dst) src)));\n-  effect(KILL cr);\n-\n-  ins_cost(125); \/\/ XXX\n-  format %{ \"decq    $dst\\t# long\" %}\n-  ins_encode %{\n-    __ decrementq($dst$$Address);\n-  %}\n-  ins_pipe(ialu_mem_imm);\n-%}\n-\n-instruct leaL_rReg_immI2_immL32(rRegL dst, rRegL index, immI2 scale, immL32 disp)\n-%{\n-  predicate(VM_Version::supports_fast_2op_lea());\n-  match(Set dst (AddL (LShiftL index scale) disp));\n-\n-  format %{ \"leaq $dst, [$index << $scale + $disp]\\t# long\" %}\n-  ins_encode %{\n-    Address::ScaleFactor scale = static_cast<Address::ScaleFactor>($scale$$constant);\n-    __ leaq($dst$$Register, Address(noreg, $index$$Register, scale, $disp$$constant));\n-  %}\n-  ins_pipe(ialu_reg_reg);\n-%}\n-\n-instruct leaL_rReg_rReg_immL32(rRegL dst, rRegL base, rRegL index, immL32 disp)\n-%{\n-  predicate(VM_Version::supports_fast_3op_lea());\n-  match(Set dst (AddL (AddL base index) disp));\n-\n-  format %{ \"leaq $dst, [$base + $index + $disp]\\t# long\" %}\n-  ins_encode %{\n-    __ leaq($dst$$Register, Address($base$$Register, $index$$Register, Address::times_1, $disp$$constant));\n-  %}\n-  ins_pipe(ialu_reg_reg);\n-%}\n-\n-instruct leaL_rReg_rReg_immI2(rRegL dst, no_rbp_r13_RegL base, rRegL index, immI2 scale)\n-%{\n-  predicate(VM_Version::supports_fast_2op_lea());\n-  match(Set dst (AddL base (LShiftL index scale)));\n-\n-  format %{ \"leaq $dst, [$base + $index << $scale]\\t# long\" %}\n-  ins_encode %{\n-    Address::ScaleFactor scale = static_cast<Address::ScaleFactor>($scale$$constant);\n-    __ leaq($dst$$Register, Address($base$$Register, $index$$Register, scale));\n-  %}\n-  ins_pipe(ialu_reg_reg);\n-%}\n-\n-instruct leaL_rReg_rReg_immI2_immL32(rRegL dst, rRegL base, rRegL index, immI2 scale, immL32 disp)\n-%{\n-  predicate(VM_Version::supports_fast_3op_lea());\n-  match(Set dst (AddL (AddL base (LShiftL index scale)) disp));\n-\n-  format %{ \"leaq $dst, [$base + $index << $scale + $disp]\\t# long\" %}\n-  ins_encode %{\n-    Address::ScaleFactor scale = static_cast<Address::ScaleFactor>($scale$$constant);\n-    __ leaq($dst$$Register, Address($base$$Register, $index$$Register, scale, $disp$$constant));\n-  %}\n-  ins_pipe(ialu_reg_reg);\n-%}\n-\n-instruct addP_rReg(rRegP dst, rRegL src, rFlagsReg cr)\n-%{\n-  match(Set dst (AddP dst src));\n-  effect(KILL cr);\n-  flag(PD::Flag_sets_overflow_flag, PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_carry_flag, PD::Flag_sets_parity_flag);\n-\n-  format %{ \"addq    $dst, $src\\t# ptr\" %}\n-  ins_encode %{\n-    __ addq($dst$$Register, $src$$Register);\n-  %}\n-  ins_pipe(ialu_reg_reg);\n-%}\n-\n-instruct addP_rReg_imm(rRegP dst, immL32 src, rFlagsReg cr)\n-%{\n-  match(Set dst (AddP dst src));\n-  effect(KILL cr);\n-  flag(PD::Flag_sets_overflow_flag, PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_carry_flag, PD::Flag_sets_parity_flag);\n-\n-  format %{ \"addq    $dst, $src\\t# ptr\" %}\n-  ins_encode %{\n-    __ addq($dst$$Register, $src$$constant);\n-  %}\n-  ins_pipe( ialu_reg );\n-%}\n-\n-\/\/ XXX addP mem ops ????\n-\n-instruct checkCastPP(rRegP dst)\n-%{\n-  match(Set dst (CheckCastPP dst));\n-\n-  size(0);\n-  format %{ \"# checkcastPP of $dst\" %}\n-  ins_encode(\/* empty encoding *\/);\n-  ins_pipe(empty);\n-%}\n-\n-instruct castPP(rRegP dst)\n-%{\n-  match(Set dst (CastPP dst));\n-\n-  size(0);\n-  format %{ \"# castPP of $dst\" %}\n-  ins_encode(\/* empty encoding *\/);\n-  ins_pipe(empty);\n-%}\n-\n-instruct castII(rRegI dst)\n-%{\n-  predicate(VerifyConstraintCasts == 0);\n-  match(Set dst (CastII dst));\n-\n-  size(0);\n-  format %{ \"# castII of $dst\" %}\n-  ins_encode(\/* empty encoding *\/);\n-  ins_cost(0);\n-  ins_pipe(empty);\n-%}\n-\n-instruct castII_checked(rRegI dst, rFlagsReg cr)\n-%{\n-  predicate(VerifyConstraintCasts > 0);\n-  match(Set dst (CastII dst));\n-\n-  effect(KILL cr);\n-  format %{ \"# cast_checked_II $dst\" %}\n-  ins_encode %{\n-    __ verify_int_in_range(_idx, bottom_type()->is_int(), $dst$$Register);\n-  %}\n-  ins_pipe(pipe_slow);\n-%}\n-\n-instruct castLL(rRegL dst)\n-%{\n-  predicate(VerifyConstraintCasts == 0);\n-  match(Set dst (CastLL dst));\n-\n-  size(0);\n-  format %{ \"# castLL of $dst\" %}\n-  ins_encode(\/* empty encoding *\/);\n-  ins_cost(0);\n-  ins_pipe(empty);\n-%}\n-\n-instruct castLL_checked_L32(rRegL dst, rFlagsReg cr)\n-%{\n-  predicate(VerifyConstraintCasts > 0 && castLL_is_imm32(n));\n-  match(Set dst (CastLL dst));\n-\n-  effect(KILL cr);\n-  format %{ \"# cast_checked_LL $dst\" %}\n-  ins_encode %{\n-    __ verify_long_in_range(_idx, bottom_type()->is_long(), $dst$$Register, noreg);\n-  %}\n-  ins_pipe(pipe_slow);\n-%}\n-\n-instruct castLL_checked(rRegL dst, rRegL tmp, rFlagsReg cr)\n-%{\n-  predicate(VerifyConstraintCasts > 0 && !castLL_is_imm32(n));\n-  match(Set dst (CastLL dst));\n-\n-  effect(KILL cr, TEMP tmp);\n-  format %{ \"# cast_checked_LL $dst\\tusing $tmp as TEMP\" %}\n-  ins_encode %{\n-    __ verify_long_in_range(_idx, bottom_type()->is_long(), $dst$$Register, $tmp$$Register);\n-  %}\n-  ins_pipe(pipe_slow);\n-%}\n-\n-instruct castFF(regF dst)\n-%{\n-  match(Set dst (CastFF dst));\n-\n-  size(0);\n-  format %{ \"# castFF of $dst\" %}\n-  ins_encode(\/* empty encoding *\/);\n-  ins_cost(0);\n-  ins_pipe(empty);\n-%}\n-\n-instruct castHH(regF dst)\n-%{\n-  match(Set dst (CastHH dst));\n-\n-  size(0);\n-  format %{ \"# castHH of $dst\" %}\n-  ins_encode(\/* empty encoding *\/);\n-  ins_cost(0);\n-  ins_pipe(empty);\n-%}\n-\n-instruct castDD(regD dst)\n-%{\n-  match(Set dst (CastDD dst));\n-\n-  size(0);\n-  format %{ \"# castDD of $dst\" %}\n-  ins_encode(\/* empty encoding *\/);\n-  ins_cost(0);\n-  ins_pipe(empty);\n-%}\n-\n-\/\/ XXX No flag versions for CompareAndSwap{P,I,L} because matcher can't match them\n-instruct compareAndSwapP(rRegI res,\n-                         memory mem_ptr,\n-                         rax_RegP oldval, rRegP newval,\n-                         rFlagsReg cr)\n-%{\n-  predicate(n->as_LoadStore()->barrier_data() == 0);\n-  match(Set res (CompareAndSwapP mem_ptr (Binary oldval newval)));\n-  match(Set res (WeakCompareAndSwapP mem_ptr (Binary oldval newval)));\n-  effect(KILL cr, KILL oldval);\n-\n-  format %{ \"cmpxchgq $mem_ptr,$newval\\t# \"\n-            \"If rax == $mem_ptr then store $newval into $mem_ptr\\n\\t\"\n-            \"setcc $res \\t# emits sete + movzbl or setzue for APX\" %}\n-  ins_encode %{\n-    __ lock();\n-    __ cmpxchgq($newval$$Register, $mem_ptr$$Address);\n-    __ setcc(Assembler::equal, $res$$Register);\n-  %}\n-  ins_pipe( pipe_cmpxchg );\n-%}\n-\n-instruct compareAndSwapL(rRegI res,\n-                         memory mem_ptr,\n-                         rax_RegL oldval, rRegL newval,\n-                         rFlagsReg cr)\n-%{\n-  match(Set res (CompareAndSwapL mem_ptr (Binary oldval newval)));\n-  match(Set res (WeakCompareAndSwapL mem_ptr (Binary oldval newval)));\n-  effect(KILL cr, KILL oldval);\n-\n-  format %{ \"cmpxchgq $mem_ptr,$newval\\t# \"\n-            \"If rax == $mem_ptr then store $newval into $mem_ptr\\n\\t\"\n-            \"setcc $res \\t# emits sete + movzbl or setzue for APX\" %}\n-  ins_encode %{\n-    __ lock();\n-    __ cmpxchgq($newval$$Register, $mem_ptr$$Address);\n-    __ setcc(Assembler::equal, $res$$Register);\n-  %}\n-  ins_pipe( pipe_cmpxchg );\n-%}\n-\n-instruct compareAndSwapI(rRegI res,\n-                         memory mem_ptr,\n-                         rax_RegI oldval, rRegI newval,\n-                         rFlagsReg cr)\n-%{\n-  match(Set res (CompareAndSwapI mem_ptr (Binary oldval newval)));\n-  match(Set res (WeakCompareAndSwapI mem_ptr (Binary oldval newval)));\n-  effect(KILL cr, KILL oldval);\n-\n-  format %{ \"cmpxchgl $mem_ptr,$newval\\t# \"\n-            \"If rax == $mem_ptr then store $newval into $mem_ptr\\n\\t\"\n-            \"setcc $res \\t# emits sete + movzbl or setzue for APX\" %}\n-  ins_encode %{\n-    __ lock();\n-    __ cmpxchgl($newval$$Register, $mem_ptr$$Address);\n-    __ setcc(Assembler::equal, $res$$Register);\n-  %}\n-  ins_pipe( pipe_cmpxchg );\n-%}\n-\n-instruct compareAndSwapB(rRegI res,\n-                         memory mem_ptr,\n-                         rax_RegI oldval, rRegI newval,\n-                         rFlagsReg cr)\n-%{\n-  match(Set res (CompareAndSwapB mem_ptr (Binary oldval newval)));\n-  match(Set res (WeakCompareAndSwapB mem_ptr (Binary oldval newval)));\n-  effect(KILL cr, KILL oldval);\n-\n-  format %{ \"cmpxchgb $mem_ptr,$newval\\t# \"\n-            \"If rax == $mem_ptr then store $newval into $mem_ptr\\n\\t\"\n-            \"setcc $res \\t# emits sete + movzbl or setzue for APX\" %}\n-  ins_encode %{\n-    __ lock();\n-    __ cmpxchgb($newval$$Register, $mem_ptr$$Address);\n-    __ setcc(Assembler::equal, $res$$Register);\n-  %}\n-  ins_pipe( pipe_cmpxchg );\n-%}\n-\n-instruct compareAndSwapS(rRegI res,\n-                         memory mem_ptr,\n-                         rax_RegI oldval, rRegI newval,\n-                         rFlagsReg cr)\n-%{\n-  match(Set res (CompareAndSwapS mem_ptr (Binary oldval newval)));\n-  match(Set res (WeakCompareAndSwapS mem_ptr (Binary oldval newval)));\n-  effect(KILL cr, KILL oldval);\n-\n-  format %{ \"cmpxchgw $mem_ptr,$newval\\t# \"\n-            \"If rax == $mem_ptr then store $newval into $mem_ptr\\n\\t\"\n-            \"setcc $res \\t# emits sete + movzbl or setzue for APX\" %}\n-  ins_encode %{\n-    __ lock();\n-    __ cmpxchgw($newval$$Register, $mem_ptr$$Address);\n-    __ setcc(Assembler::equal, $res$$Register);\n-  %}\n-  ins_pipe( pipe_cmpxchg );\n-%}\n-\n-instruct compareAndSwapN(rRegI res,\n-                          memory mem_ptr,\n-                          rax_RegN oldval, rRegN newval,\n-                          rFlagsReg cr) %{\n-  predicate(n->as_LoadStore()->barrier_data() == 0);\n-  match(Set res (CompareAndSwapN mem_ptr (Binary oldval newval)));\n-  match(Set res (WeakCompareAndSwapN mem_ptr (Binary oldval newval)));\n-  effect(KILL cr, KILL oldval);\n-\n-  format %{ \"cmpxchgl $mem_ptr,$newval\\t# \"\n-            \"If rax == $mem_ptr then store $newval into $mem_ptr\\n\\t\"\n-            \"setcc $res \\t# emits sete + movzbl or setzue for APX\" %}\n-  ins_encode %{\n-    __ lock();\n-    __ cmpxchgl($newval$$Register, $mem_ptr$$Address);\n-    __ setcc(Assembler::equal, $res$$Register);\n-  %}\n-  ins_pipe( pipe_cmpxchg );\n-%}\n-\n-instruct compareAndExchangeB(\n-                         memory mem_ptr,\n-                         rax_RegI oldval, rRegI newval,\n-                         rFlagsReg cr)\n-%{\n-  match(Set oldval (CompareAndExchangeB mem_ptr (Binary oldval newval)));\n-  effect(KILL cr);\n-\n-  format %{ \"cmpxchgb $mem_ptr,$newval\\t# \"\n-            \"If rax == $mem_ptr then store $newval into $mem_ptr\\n\\t\"  %}\n-  ins_encode %{\n-    __ lock();\n-    __ cmpxchgb($newval$$Register, $mem_ptr$$Address);\n-  %}\n-  ins_pipe( pipe_cmpxchg );\n-%}\n-\n-instruct compareAndExchangeS(\n-                         memory mem_ptr,\n-                         rax_RegI oldval, rRegI newval,\n-                         rFlagsReg cr)\n-%{\n-  match(Set oldval (CompareAndExchangeS mem_ptr (Binary oldval newval)));\n-  effect(KILL cr);\n-\n-  format %{ \"cmpxchgw $mem_ptr,$newval\\t# \"\n-            \"If rax == $mem_ptr then store $newval into $mem_ptr\\n\\t\"  %}\n-  ins_encode %{\n-    __ lock();\n-    __ cmpxchgw($newval$$Register, $mem_ptr$$Address);\n-  %}\n-  ins_pipe( pipe_cmpxchg );\n-%}\n-\n-instruct compareAndExchangeI(\n-                         memory mem_ptr,\n-                         rax_RegI oldval, rRegI newval,\n-                         rFlagsReg cr)\n-%{\n-  match(Set oldval (CompareAndExchangeI mem_ptr (Binary oldval newval)));\n-  effect(KILL cr);\n-\n-  format %{ \"cmpxchgl $mem_ptr,$newval\\t# \"\n-            \"If rax == $mem_ptr then store $newval into $mem_ptr\\n\\t\"  %}\n-  ins_encode %{\n-    __ lock();\n-    __ cmpxchgl($newval$$Register, $mem_ptr$$Address);\n-  %}\n-  ins_pipe( pipe_cmpxchg );\n-%}\n-\n-instruct compareAndExchangeL(\n-                         memory mem_ptr,\n-                         rax_RegL oldval, rRegL newval,\n-                         rFlagsReg cr)\n-%{\n-  match(Set oldval (CompareAndExchangeL mem_ptr (Binary oldval newval)));\n-  effect(KILL cr);\n-\n-  format %{ \"cmpxchgq $mem_ptr,$newval\\t# \"\n-            \"If rax == $mem_ptr then store $newval into $mem_ptr\\n\\t\"  %}\n-  ins_encode %{\n-    __ lock();\n-    __ cmpxchgq($newval$$Register, $mem_ptr$$Address);\n-  %}\n-  ins_pipe( pipe_cmpxchg );\n-%}\n-\n-instruct compareAndExchangeN(\n-                          memory mem_ptr,\n-                          rax_RegN oldval, rRegN newval,\n-                          rFlagsReg cr) %{\n-  predicate(n->as_LoadStore()->barrier_data() == 0);\n-  match(Set oldval (CompareAndExchangeN mem_ptr (Binary oldval newval)));\n-  effect(KILL cr);\n-\n-  format %{ \"cmpxchgl $mem_ptr,$newval\\t# \"\n-            \"If rax == $mem_ptr then store $newval into $mem_ptr\\n\\t\" %}\n-  ins_encode %{\n-    __ lock();\n-    __ cmpxchgl($newval$$Register, $mem_ptr$$Address);\n-  %}\n-  ins_pipe( pipe_cmpxchg );\n-%}\n-\n-instruct compareAndExchangeP(\n-                         memory mem_ptr,\n-                         rax_RegP oldval, rRegP newval,\n-                         rFlagsReg cr)\n-%{\n-  predicate(n->as_LoadStore()->barrier_data() == 0);\n-  match(Set oldval (CompareAndExchangeP mem_ptr (Binary oldval newval)));\n-  effect(KILL cr);\n-\n-  format %{ \"cmpxchgq $mem_ptr,$newval\\t# \"\n-            \"If rax == $mem_ptr then store $newval into $mem_ptr\\n\\t\" %}\n-  ins_encode %{\n-    __ lock();\n-    __ cmpxchgq($newval$$Register, $mem_ptr$$Address);\n-  %}\n-  ins_pipe( pipe_cmpxchg );\n-%}\n-\n-instruct xaddB_reg_no_res(memory mem, Universe dummy, rRegI add, rFlagsReg cr) %{\n-  predicate(n->as_LoadStore()->result_not_used());\n-  match(Set dummy (GetAndAddB mem add));\n-  effect(KILL cr);\n-  format %{ \"addb_lock   $mem, $add\" %}\n-  ins_encode %{\n-    __ lock();\n-    __ addb($mem$$Address, $add$$Register);\n-  %}\n-  ins_pipe(pipe_cmpxchg);\n-%}\n-\n-instruct xaddB_imm_no_res(memory mem, Universe dummy, immI add, rFlagsReg cr) %{\n-  predicate(n->as_LoadStore()->result_not_used());\n-  match(Set dummy (GetAndAddB mem add));\n-  effect(KILL cr);\n-  format %{ \"addb_lock   $mem, $add\" %}\n-  ins_encode %{\n-    __ lock();\n-    __ addb($mem$$Address, $add$$constant);\n-  %}\n-  ins_pipe(pipe_cmpxchg);\n-%}\n-\n-instruct xaddB(memory mem, rRegI newval, rFlagsReg cr) %{\n-  predicate(!n->as_LoadStore()->result_not_used());\n-  match(Set newval (GetAndAddB mem newval));\n-  effect(KILL cr);\n-  format %{ \"xaddb_lock  $mem, $newval\" %}\n-  ins_encode %{\n-    __ lock();\n-    __ xaddb($mem$$Address, $newval$$Register);\n-  %}\n-  ins_pipe(pipe_cmpxchg);\n-%}\n-\n-instruct xaddS_reg_no_res(memory mem, Universe dummy, rRegI add, rFlagsReg cr) %{\n-  predicate(n->as_LoadStore()->result_not_used());\n-  match(Set dummy (GetAndAddS mem add));\n-  effect(KILL cr);\n-  format %{ \"addw_lock   $mem, $add\" %}\n-  ins_encode %{\n-    __ lock();\n-    __ addw($mem$$Address, $add$$Register);\n-  %}\n-  ins_pipe(pipe_cmpxchg);\n-%}\n-\n-instruct xaddS_imm_no_res(memory mem, Universe dummy, immI add, rFlagsReg cr) %{\n-  predicate(UseStoreImmI16 && n->as_LoadStore()->result_not_used());\n-  match(Set dummy (GetAndAddS mem add));\n-  effect(KILL cr);\n-  format %{ \"addw_lock   $mem, $add\" %}\n-  ins_encode %{\n-    __ lock();\n-    __ addw($mem$$Address, $add$$constant);\n-  %}\n-  ins_pipe(pipe_cmpxchg);\n-%}\n-\n-instruct xaddS(memory mem, rRegI newval, rFlagsReg cr) %{\n-  predicate(!n->as_LoadStore()->result_not_used());\n-  match(Set newval (GetAndAddS mem newval));\n-  effect(KILL cr);\n-  format %{ \"xaddw_lock  $mem, $newval\" %}\n-  ins_encode %{\n-    __ lock();\n-    __ xaddw($mem$$Address, $newval$$Register);\n-  %}\n-  ins_pipe(pipe_cmpxchg);\n-%}\n-\n-instruct xaddI_reg_no_res(memory mem, Universe dummy, rRegI add, rFlagsReg cr) %{\n-  predicate(n->as_LoadStore()->result_not_used());\n-  match(Set dummy (GetAndAddI mem add));\n-  effect(KILL cr);\n-  format %{ \"addl_lock   $mem, $add\" %}\n-  ins_encode %{\n-    __ lock();\n-    __ addl($mem$$Address, $add$$Register);\n-  %}\n-  ins_pipe(pipe_cmpxchg);\n-%}\n-\n-instruct xaddI_imm_no_res(memory mem, Universe dummy, immI add, rFlagsReg cr) %{\n-  predicate(n->as_LoadStore()->result_not_used());\n-  match(Set dummy (GetAndAddI mem add));\n-  effect(KILL cr);\n-  format %{ \"addl_lock   $mem, $add\" %}\n-  ins_encode %{\n-    __ lock();\n-    __ addl($mem$$Address, $add$$constant);\n-  %}\n-  ins_pipe(pipe_cmpxchg);\n-%}\n-\n-instruct xaddI(memory mem, rRegI newval, rFlagsReg cr) %{\n-  predicate(!n->as_LoadStore()->result_not_used());\n-  match(Set newval (GetAndAddI mem newval));\n-  effect(KILL cr);\n-  format %{ \"xaddl_lock  $mem, $newval\" %}\n-  ins_encode %{\n-    __ lock();\n-    __ xaddl($mem$$Address, $newval$$Register);\n-  %}\n-  ins_pipe(pipe_cmpxchg);\n-%}\n-\n-instruct xaddL_reg_no_res(memory mem, Universe dummy, rRegL add, rFlagsReg cr) %{\n-  predicate(n->as_LoadStore()->result_not_used());\n-  match(Set dummy (GetAndAddL mem add));\n-  effect(KILL cr);\n-  format %{ \"addq_lock   $mem, $add\" %}\n-  ins_encode %{\n-    __ lock();\n-    __ addq($mem$$Address, $add$$Register);\n-  %}\n-  ins_pipe(pipe_cmpxchg);\n-%}\n-\n-instruct xaddL_imm_no_res(memory mem, Universe dummy, immL32 add, rFlagsReg cr) %{\n-  predicate(n->as_LoadStore()->result_not_used());\n-  match(Set dummy (GetAndAddL mem add));\n-  effect(KILL cr);\n-  format %{ \"addq_lock   $mem, $add\" %}\n-  ins_encode %{\n-    __ lock();\n-    __ addq($mem$$Address, $add$$constant);\n-  %}\n-  ins_pipe(pipe_cmpxchg);\n-%}\n-\n-instruct xaddL(memory mem, rRegL newval, rFlagsReg cr) %{\n-  predicate(!n->as_LoadStore()->result_not_used());\n-  match(Set newval (GetAndAddL mem newval));\n-  effect(KILL cr);\n-  format %{ \"xaddq_lock  $mem, $newval\" %}\n-  ins_encode %{\n-    __ lock();\n-    __ xaddq($mem$$Address, $newval$$Register);\n-  %}\n-  ins_pipe(pipe_cmpxchg);\n-%}\n-\n-instruct xchgB( memory mem, rRegI newval) %{\n-  match(Set newval (GetAndSetB mem newval));\n-  format %{ \"XCHGB  $newval,[$mem]\" %}\n-  ins_encode %{\n-    __ xchgb($newval$$Register, $mem$$Address);\n-  %}\n-  ins_pipe( pipe_cmpxchg );\n-%}\n-\n-instruct xchgS( memory mem, rRegI newval) %{\n-  match(Set newval (GetAndSetS mem newval));\n-  format %{ \"XCHGW  $newval,[$mem]\" %}\n-  ins_encode %{\n-    __ xchgw($newval$$Register, $mem$$Address);\n-  %}\n-  ins_pipe( pipe_cmpxchg );\n-%}\n-\n-instruct xchgI( memory mem, rRegI newval) %{\n-  match(Set newval (GetAndSetI mem newval));\n-  format %{ \"XCHGL  $newval,[$mem]\" %}\n-  ins_encode %{\n-    __ xchgl($newval$$Register, $mem$$Address);\n-  %}\n-  ins_pipe( pipe_cmpxchg );\n-%}\n-\n-instruct xchgL( memory mem, rRegL newval) %{\n-  match(Set newval (GetAndSetL mem newval));\n-  format %{ \"XCHGL  $newval,[$mem]\" %}\n-  ins_encode %{\n-    __ xchgq($newval$$Register, $mem$$Address);\n-  %}\n-  ins_pipe( pipe_cmpxchg );\n-%}\n-\n-instruct xchgP( memory mem, rRegP newval) %{\n-  match(Set newval (GetAndSetP mem newval));\n-  predicate(n->as_LoadStore()->barrier_data() == 0);\n-  format %{ \"XCHGQ  $newval,[$mem]\" %}\n-  ins_encode %{\n-    __ xchgq($newval$$Register, $mem$$Address);\n-  %}\n-  ins_pipe( pipe_cmpxchg );\n-%}\n-\n-instruct xchgN( memory mem, rRegN newval) %{\n-  predicate(n->as_LoadStore()->barrier_data() == 0);\n-  match(Set newval (GetAndSetN mem newval));\n-  format %{ \"XCHGL  $newval,$mem]\" %}\n-  ins_encode %{\n-    __ xchgl($newval$$Register, $mem$$Address);\n-  %}\n-  ins_pipe( pipe_cmpxchg );\n-%}\n-\n-\/\/----------Abs Instructions-------------------------------------------\n-\n-\/\/ Integer Absolute Instructions\n-instruct absI_rReg(rRegI dst, rRegI src, rFlagsReg cr)\n-%{\n-  match(Set dst (AbsI src));\n-  effect(TEMP dst, KILL cr);\n-  format %{ \"xorl    $dst, $dst\\t# abs int\\n\\t\"\n-            \"subl    $dst, $src\\n\\t\"\n-            \"cmovll  $dst, $src\" %}\n-  ins_encode %{\n-    __ xorl($dst$$Register, $dst$$Register);\n-    __ subl($dst$$Register, $src$$Register);\n-    __ cmovl(Assembler::less, $dst$$Register, $src$$Register);\n-  %}\n-\n-  ins_pipe(ialu_reg_reg);\n-%}\n-\n-\/\/ Long Absolute Instructions\n-instruct absL_rReg(rRegL dst, rRegL src, rFlagsReg cr)\n-%{\n-  match(Set dst (AbsL src));\n-  effect(TEMP dst, KILL cr);\n-  format %{ \"xorl    $dst, $dst\\t# abs long\\n\\t\"\n-            \"subq    $dst, $src\\n\\t\"\n-            \"cmovlq  $dst, $src\" %}\n-  ins_encode %{\n-    __ xorl($dst$$Register, $dst$$Register);\n-    __ subq($dst$$Register, $src$$Register);\n-    __ cmovq(Assembler::less, $dst$$Register, $src$$Register);\n-  %}\n-\n-  ins_pipe(ialu_reg_reg);\n-%}\n-\n-\/\/----------Subtraction Instructions-------------------------------------------\n-\n-\/\/ Integer Subtraction Instructions\n-instruct subI_rReg(rRegI dst, rRegI src, rFlagsReg cr)\n-%{\n-  predicate(!UseAPX);\n-  match(Set dst (SubI dst src));\n-  effect(KILL cr);\n-  flag(PD::Flag_sets_overflow_flag, PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_carry_flag, PD::Flag_sets_parity_flag);\n-\n-  format %{ \"subl    $dst, $src\\t# int\" %}\n-  ins_encode %{\n-    __ subl($dst$$Register, $src$$Register);\n-  %}\n-  ins_pipe(ialu_reg_reg);\n-%}\n-\n-instruct subI_rReg_ndd(rRegI dst, rRegI src1, rRegI src2, rFlagsReg cr)\n-%{\n-  predicate(UseAPX);\n-  match(Set dst (SubI src1 src2));\n-  effect(KILL cr);\n-  flag(PD::Flag_sets_overflow_flag, PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_carry_flag, PD::Flag_sets_parity_flag);\n-\n-  format %{ \"esubl    $dst, $src1, $src2\\t# int ndd\" %}\n-  ins_encode %{\n-    __ esubl($dst$$Register, $src1$$Register, $src2$$Register, false);\n-  %}\n-  ins_pipe(ialu_reg_reg);\n-%}\n-\n-instruct subI_rReg_rReg_imm_ndd(rRegI dst, rRegI src1, immI src2, rFlagsReg cr)\n-%{\n-  predicate(UseAPX);\n-  match(Set dst (SubI src1 src2));\n-  effect(KILL cr);\n-  flag(PD::Flag_sets_overflow_flag, PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_carry_flag, PD::Flag_sets_parity_flag);\n-\n-  format %{ \"esubl    $dst, $src1, $src2\\t# int ndd\" %}\n-  ins_encode %{\n-    __ esubl($dst$$Register, $src1$$Register, $src2$$constant, false);\n-  %}\n-  ins_pipe(ialu_reg_reg);\n-%}\n-\n-instruct subI_rReg_mem_imm_ndd(rRegI dst, memory src1, immI src2, rFlagsReg cr)\n-%{\n-  predicate(UseAPX);\n-  match(Set dst (SubI (LoadI src1) src2));\n-  effect(KILL cr);\n-  flag(PD::Flag_sets_overflow_flag, PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_carry_flag, PD::Flag_sets_parity_flag);\n-\n-  format %{ \"esubl    $dst, $src1, $src2\\t# int ndd\" %}\n-  ins_encode %{\n-    __ esubl($dst$$Register, $src1$$Address, $src2$$constant, false);\n-  %}\n-  ins_pipe(ialu_reg_reg);\n-%}\n-\n-instruct subI_rReg_mem(rRegI dst, memory src, rFlagsReg cr)\n-%{\n-  predicate(!UseAPX);\n-  match(Set dst (SubI dst (LoadI src)));\n-  effect(KILL cr);\n-  flag(PD::Flag_sets_overflow_flag, PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_carry_flag, PD::Flag_sets_parity_flag);\n-\n-  ins_cost(150);\n-  format %{ \"subl    $dst, $src\\t# int\" %}\n-  ins_encode %{\n-    __ subl($dst$$Register, $src$$Address);\n-  %}\n-  ins_pipe(ialu_reg_mem);\n-%}\n-\n-instruct subI_rReg_rReg_mem_ndd(rRegI dst, rRegI src1, memory src2, rFlagsReg cr)\n-%{\n-  predicate(UseAPX);\n-  match(Set dst (SubI src1 (LoadI src2)));\n-  effect(KILL cr);\n-  flag(PD::Flag_sets_overflow_flag, PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_carry_flag, PD::Flag_sets_parity_flag);\n-\n-  ins_cost(150);\n-  format %{ \"esubl    $dst, $src1, $src2\\t# int ndd\" %}\n-  ins_encode %{\n-    __ esubl($dst$$Register, $src1$$Register, $src2$$Address, false);\n-  %}\n-  ins_pipe(ialu_reg_mem);\n-%}\n-\n-instruct subI_rReg_mem_rReg_ndd(rRegI dst, memory src1, rRegI src2, rFlagsReg cr)\n-%{\n-  predicate(UseAPX);\n-  match(Set dst (SubI (LoadI src1) src2));\n-  effect(KILL cr);\n-  flag(PD::Flag_sets_overflow_flag, PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_carry_flag, PD::Flag_sets_parity_flag);\n-\n-  ins_cost(150);\n-  format %{ \"esubl    $dst, $src1, $src2\\t# int ndd\" %}\n-  ins_encode %{\n-    __ esubl($dst$$Register, $src1$$Address, $src2$$Register, false);\n-  %}\n-  ins_pipe(ialu_reg_mem);\n-%}\n-\n-instruct subI_mem_rReg(memory dst, rRegI src, rFlagsReg cr)\n-%{\n-  match(Set dst (StoreI dst (SubI (LoadI dst) src)));\n-  effect(KILL cr);\n-  flag(PD::Flag_sets_overflow_flag, PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_carry_flag, PD::Flag_sets_parity_flag);\n-\n-  ins_cost(150);\n-  format %{ \"subl    $dst, $src\\t# int\" %}\n-  ins_encode %{\n-    __ subl($dst$$Address, $src$$Register);\n-  %}\n-  ins_pipe(ialu_mem_reg);\n-%}\n-\n-instruct subL_rReg(rRegL dst, rRegL src, rFlagsReg cr)\n-%{\n-  predicate(!UseAPX);\n-  match(Set dst (SubL dst src));\n-  effect(KILL cr);\n-  flag(PD::Flag_sets_overflow_flag, PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_carry_flag, PD::Flag_sets_parity_flag);\n-\n-  format %{ \"subq    $dst, $src\\t# long\" %}\n-  ins_encode %{\n-    __ subq($dst$$Register, $src$$Register);\n-  %}\n-  ins_pipe(ialu_reg_reg);\n-%}\n-\n-instruct subL_rReg_ndd(rRegL dst, rRegL src1, rRegL src2, rFlagsReg cr)\n-%{\n-  predicate(UseAPX);\n-  match(Set dst (SubL src1 src2));\n-  effect(KILL cr);\n-  flag(PD::Flag_sets_overflow_flag, PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_carry_flag, PD::Flag_sets_parity_flag);\n-\n-  format %{ \"esubq    $dst, $src1, $src2\\t# long ndd\" %}\n-  ins_encode %{\n-    __ esubq($dst$$Register, $src1$$Register, $src2$$Register, false);\n-  %}\n-  ins_pipe(ialu_reg_reg);\n-%}\n-\n-instruct subL_rReg_rReg_imm_ndd(rRegL dst, rRegL src1, immL32 src2, rFlagsReg cr)\n-%{\n-  predicate(UseAPX);\n-  match(Set dst (SubL src1 src2));\n-  effect(KILL cr);\n-  flag(PD::Flag_sets_overflow_flag, PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_carry_flag, PD::Flag_sets_parity_flag);\n-\n-  format %{ \"esubq    $dst, $src1, $src2\\t# long ndd\" %}\n-  ins_encode %{\n-    __ esubq($dst$$Register, $src1$$Register, $src2$$constant, false);\n-  %}\n-  ins_pipe(ialu_reg_reg);\n-%}\n-\n-instruct subL_rReg_mem_imm_ndd(rRegL dst, memory src1, immL32 src2, rFlagsReg cr)\n-%{\n-  predicate(UseAPX);\n-  match(Set dst (SubL (LoadL src1) src2));\n-  effect(KILL cr);\n-  flag(PD::Flag_sets_overflow_flag, PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_carry_flag, PD::Flag_sets_parity_flag);\n-\n-  format %{ \"esubq    $dst, $src1, $src2\\t# long ndd\" %}\n-  ins_encode %{\n-    __ esubq($dst$$Register, $src1$$Address, $src2$$constant, false);\n-  %}\n-  ins_pipe(ialu_reg_reg);\n-%}\n-\n-instruct subL_rReg_mem(rRegL dst, memory src, rFlagsReg cr)\n-%{\n-  predicate(!UseAPX);\n-  match(Set dst (SubL dst (LoadL src)));\n-  effect(KILL cr);\n-  flag(PD::Flag_sets_overflow_flag, PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_carry_flag, PD::Flag_sets_parity_flag);\n-\n-  ins_cost(150);\n-  format %{ \"subq    $dst, $src\\t# long\" %}\n-  ins_encode %{\n-    __ subq($dst$$Register, $src$$Address);\n-  %}\n-  ins_pipe(ialu_reg_mem);\n-%}\n-\n-instruct subL_rReg_rReg_mem_ndd(rRegL dst, rRegL src1, memory src2, rFlagsReg cr)\n-%{\n-  predicate(UseAPX);\n-  match(Set dst (SubL src1 (LoadL src2)));\n-  effect(KILL cr);\n-  flag(PD::Flag_sets_overflow_flag, PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_carry_flag, PD::Flag_sets_parity_flag);\n-\n-  ins_cost(150);\n-  format %{ \"esubq    $dst, $src1, $src2\\t# long ndd\" %}\n-  ins_encode %{\n-    __ esubq($dst$$Register, $src1$$Register, $src2$$Address, false);\n-  %}\n-  ins_pipe(ialu_reg_mem);\n-%}\n-\n-instruct subL_rReg_mem_rReg_ndd(rRegL dst, memory src1, rRegL src2, rFlagsReg cr)\n-%{\n-  predicate(UseAPX);\n-  match(Set dst (SubL (LoadL src1) src2));\n-  effect(KILL cr);\n-  flag(PD::Flag_sets_overflow_flag, PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_carry_flag, PD::Flag_sets_parity_flag);\n-\n-  ins_cost(150);\n-  format %{ \"esubq    $dst, $src1, $src2\\t# long ndd\" %}\n-  ins_encode %{\n-    __ esubq($dst$$Register, $src1$$Address, $src2$$Register, false);\n-  %}\n-  ins_pipe(ialu_reg_mem);\n-%}\n-\n-instruct subL_mem_rReg(memory dst, rRegL src, rFlagsReg cr)\n-%{\n-  match(Set dst (StoreL dst (SubL (LoadL dst) src)));\n-  effect(KILL cr);\n-  flag(PD::Flag_sets_overflow_flag, PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_carry_flag, PD::Flag_sets_parity_flag);\n-\n-  ins_cost(150);\n-  format %{ \"subq    $dst, $src\\t# long\" %}\n-  ins_encode %{\n-    __ subq($dst$$Address, $src$$Register);\n-  %}\n-  ins_pipe(ialu_mem_reg);\n-%}\n-\n-\/\/ Subtract from a pointer\n-\/\/ XXX hmpf???\n-instruct subP_rReg(rRegP dst, rRegI src, immI_0 zero, rFlagsReg cr)\n-%{\n-  match(Set dst (AddP dst (SubI zero src)));\n-  effect(KILL cr);\n-\n-  format %{ \"subq    $dst, $src\\t# ptr - int\" %}\n-  ins_encode %{\n-    __ subq($dst$$Register, $src$$Register);\n-  %}\n-  ins_pipe(ialu_reg_reg);\n-%}\n-\n-instruct negI_rReg(rRegI dst, immI_0 zero, rFlagsReg cr)\n-%{\n-  predicate(!UseAPX);\n-  match(Set dst (SubI zero dst));\n-  effect(KILL cr);\n-  flag(PD::Flag_sets_overflow_flag, PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag);\n-\n-  format %{ \"negl    $dst\\t# int\" %}\n-  ins_encode %{\n-    __ negl($dst$$Register);\n-  %}\n-  ins_pipe(ialu_reg);\n-%}\n-\n-instruct negI_rReg_ndd(rRegI dst, rRegI src, immI_0 zero, rFlagsReg cr)\n-%{\n-  predicate(UseAPX);\n-  match(Set dst (SubI zero src));\n-  effect(KILL cr);\n-  flag(PD::Flag_sets_overflow_flag, PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag);\n-\n-  format %{ \"enegl    $dst, $src\\t# int ndd\" %}\n-  ins_encode %{\n-    __ enegl($dst$$Register, $src$$Register, false);\n-  %}\n-  ins_pipe(ialu_reg);\n-%}\n-\n-instruct negI_rReg_2(rRegI dst, rFlagsReg cr)\n-%{\n-  predicate(!UseAPX);\n-  match(Set dst (NegI dst));\n-  effect(KILL cr);\n-  flag(PD::Flag_sets_overflow_flag, PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag);\n-\n-  format %{ \"negl    $dst\\t# int\" %}\n-  ins_encode %{\n-    __ negl($dst$$Register);\n-  %}\n-  ins_pipe(ialu_reg);\n-%}\n-\n-instruct negI_rReg_2_ndd(rRegI dst, rRegI src, rFlagsReg cr)\n-%{\n-  predicate(UseAPX);\n-  match(Set dst (NegI src));\n-  effect(KILL cr);\n-  flag(PD::Flag_sets_overflow_flag, PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag);\n-\n-  format %{ \"enegl    $dst, $src\\t# int ndd\" %}\n-  ins_encode %{\n-    __ enegl($dst$$Register, $src$$Register, false);\n-  %}\n-  ins_pipe(ialu_reg);\n-%}\n-\n-instruct negI_mem(memory dst, immI_0 zero, rFlagsReg cr)\n-%{\n-  match(Set dst (StoreI dst (SubI zero (LoadI dst))));\n-  effect(KILL cr);\n-  flag(PD::Flag_sets_overflow_flag, PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag);\n-\n-  format %{ \"negl    $dst\\t# int\" %}\n-  ins_encode %{\n-    __ negl($dst$$Address);\n-  %}\n-  ins_pipe(ialu_reg);\n-%}\n-\n-instruct negL_rReg(rRegL dst, immL0 zero, rFlagsReg cr)\n-%{\n-  predicate(!UseAPX);\n-  match(Set dst (SubL zero dst));\n-  effect(KILL cr);\n-  flag(PD::Flag_sets_overflow_flag, PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag);\n-\n-  format %{ \"negq    $dst\\t# long\" %}\n-  ins_encode %{\n-    __ negq($dst$$Register);\n-  %}\n-  ins_pipe(ialu_reg);\n-%}\n-\n-instruct negL_rReg_ndd(rRegL dst, rRegL src, immL0 zero, rFlagsReg cr)\n-%{\n-  predicate(UseAPX);\n-  match(Set dst (SubL zero src));\n-  effect(KILL cr);\n-  flag(PD::Flag_sets_overflow_flag, PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag);\n-\n-  format %{ \"enegq    $dst, $src\\t# long ndd\" %}\n-  ins_encode %{\n-    __ enegq($dst$$Register, $src$$Register, false);\n-  %}\n-  ins_pipe(ialu_reg);\n-%}\n-\n-instruct negL_rReg_2(rRegL dst, rFlagsReg cr)\n-%{\n-  predicate(!UseAPX);\n-  match(Set dst (NegL dst));\n-  effect(KILL cr);\n-  flag(PD::Flag_sets_overflow_flag, PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag);\n-\n-  format %{ \"negq    $dst\\t# int\" %}\n-  ins_encode %{\n-    __ negq($dst$$Register);\n-  %}\n-  ins_pipe(ialu_reg);\n-%}\n-\n-instruct negL_rReg_2_ndd(rRegL dst, rRegL src, rFlagsReg cr)\n-%{\n-  predicate(UseAPX);\n-  match(Set dst (NegL src));\n-  effect(KILL cr);\n-  flag(PD::Flag_sets_overflow_flag, PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag);\n-\n-  format %{ \"enegq    $dst, $src\\t# long ndd\" %}\n-  ins_encode %{\n-    __ enegq($dst$$Register, $src$$Register, false);\n-  %}\n-  ins_pipe(ialu_reg);\n-%}\n-\n-instruct negL_mem(memory dst, immL0 zero, rFlagsReg cr)\n-%{\n-  match(Set dst (StoreL dst (SubL zero (LoadL dst))));\n-  effect(KILL cr);\n-  flag(PD::Flag_sets_overflow_flag, PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag);\n-\n-  format %{ \"negq    $dst\\t# long\" %}\n-  ins_encode %{\n-    __ negq($dst$$Address);\n-  %}\n-  ins_pipe(ialu_reg);\n-%}\n-\n-\/\/----------Multiplication\/Division Instructions-------------------------------\n-\/\/ Integer Multiplication Instructions\n-\/\/ Multiply Register\n-\n-instruct mulI_rReg(rRegI dst, rRegI src, rFlagsReg cr)\n-%{\n-  predicate(!UseAPX);\n-  match(Set dst (MulI dst src));\n-  effect(KILL cr);\n-\n-  ins_cost(300);\n-  format %{ \"imull   $dst, $src\\t# int\" %}\n-  ins_encode %{\n-    __ imull($dst$$Register, $src$$Register);\n-  %}\n-  ins_pipe(ialu_reg_reg_alu0);\n-%}\n-\n-instruct mulI_rReg_ndd(rRegI dst, rRegI src1, rRegI src2, rFlagsReg cr)\n-%{\n-  predicate(UseAPX);\n-  match(Set dst (MulI src1 src2));\n-  effect(KILL cr);\n-\n-  ins_cost(300);\n-  format %{ \"eimull   $dst, $src1, $src2\\t# int ndd\" %}\n-  ins_encode %{\n-    __ eimull($dst$$Register, $src1$$Register, $src2$$Register, false);\n-  %}\n-  ins_pipe(ialu_reg_reg_alu0);\n-%}\n-\n-instruct mulI_rReg_imm(rRegI dst, rRegI src, immI imm, rFlagsReg cr)\n-%{\n-  match(Set dst (MulI src imm));\n-  effect(KILL cr);\n-\n-  ins_cost(300);\n-  format %{ \"imull   $dst, $src, $imm\\t# int\" %}\n-  ins_encode %{\n-    __ imull($dst$$Register, $src$$Register, $imm$$constant);\n-  %}\n-  ins_pipe(ialu_reg_reg_alu0);\n-%}\n-\n-instruct mulI_mem(rRegI dst, memory src, rFlagsReg cr)\n-%{\n-  predicate(!UseAPX);\n-  match(Set dst (MulI dst (LoadI src)));\n-  effect(KILL cr);\n-\n-  ins_cost(350);\n-  format %{ \"imull   $dst, $src\\t# int\" %}\n-  ins_encode %{\n-    __ imull($dst$$Register, $src$$Address);\n-  %}\n-  ins_pipe(ialu_reg_mem_alu0);\n-%}\n-\n-instruct mulI_rReg_rReg_mem_ndd(rRegI dst, rRegI src1, memory src2, rFlagsReg cr)\n-%{\n-  predicate(UseAPX);\n-  match(Set dst (MulI src1 (LoadI src2)));\n-  effect(KILL cr);\n-\n-  ins_cost(350);\n-  format %{ \"eimull   $dst, $src1, $src2\\t# int ndd\" %}\n-  ins_encode %{\n-    __ eimull($dst$$Register, $src1$$Register, $src2$$Address, false);\n-  %}\n-  ins_pipe(ialu_reg_mem_alu0);\n-%}\n-\n-instruct mulI_mem_imm(rRegI dst, memory src, immI imm, rFlagsReg cr)\n-%{\n-  match(Set dst (MulI (LoadI src) imm));\n-  effect(KILL cr);\n-\n-  ins_cost(300);\n-  format %{ \"imull   $dst, $src, $imm\\t# int\" %}\n-  ins_encode %{\n-    __ imull($dst$$Register, $src$$Address, $imm$$constant);\n-  %}\n-  ins_pipe(ialu_reg_mem_alu0);\n-%}\n-\n-instruct mulAddS2I_rReg(rRegI dst, rRegI src1, rRegI src2, rRegI src3, rFlagsReg cr)\n-%{\n-  match(Set dst (MulAddS2I (Binary dst src1) (Binary src2 src3)));\n-  effect(KILL cr, KILL src2);\n-\n-  expand %{ mulI_rReg(dst, src1, cr);\n-           mulI_rReg(src2, src3, cr);\n-           addI_rReg(dst, src2, cr); %}\n-%}\n-\n-instruct mulL_rReg(rRegL dst, rRegL src, rFlagsReg cr)\n-%{\n-  predicate(!UseAPX);\n-  match(Set dst (MulL dst src));\n-  effect(KILL cr);\n-\n-  ins_cost(300);\n-  format %{ \"imulq   $dst, $src\\t# long\" %}\n-  ins_encode %{\n-    __ imulq($dst$$Register, $src$$Register);\n-  %}\n-  ins_pipe(ialu_reg_reg_alu0);\n-%}\n-\n-instruct mulL_rReg_ndd(rRegL dst, rRegL src1, rRegL src2, rFlagsReg cr)\n-%{\n-  predicate(UseAPX);\n-  match(Set dst (MulL src1 src2));\n-  effect(KILL cr);\n-\n-  ins_cost(300);\n-  format %{ \"eimulq   $dst, $src1, $src2\\t# long ndd\" %}\n-  ins_encode %{\n-    __ eimulq($dst$$Register, $src1$$Register, $src2$$Register, false);\n-  %}\n-  ins_pipe(ialu_reg_reg_alu0);\n-%}\n-\n-instruct mulL_rReg_imm(rRegL dst, rRegL src, immL32 imm, rFlagsReg cr)\n-%{\n-  match(Set dst (MulL src imm));\n-  effect(KILL cr);\n-\n-  ins_cost(300);\n-  format %{ \"imulq   $dst, $src, $imm\\t# long\" %}\n-  ins_encode %{\n-    __ imulq($dst$$Register, $src$$Register, $imm$$constant);\n-  %}\n-  ins_pipe(ialu_reg_reg_alu0);\n-%}\n-\n-instruct mulL_mem(rRegL dst, memory src, rFlagsReg cr)\n-%{\n-  predicate(!UseAPX);\n-  match(Set dst (MulL dst (LoadL src)));\n-  effect(KILL cr);\n-\n-  ins_cost(350);\n-  format %{ \"imulq   $dst, $src\\t# long\" %}\n-  ins_encode %{\n-    __ imulq($dst$$Register, $src$$Address);\n-  %}\n-  ins_pipe(ialu_reg_mem_alu0);\n-%}\n-\n-instruct mulL_rReg_rReg_mem_ndd(rRegL dst, rRegL src1, memory src2, rFlagsReg cr)\n-%{\n-  predicate(UseAPX);\n-  match(Set dst (MulL src1 (LoadL src2)));\n-  effect(KILL cr);\n-\n-  ins_cost(350);\n-  format %{ \"eimulq   $dst, $src1, $src2 \\t# long\" %}\n-  ins_encode %{\n-    __ eimulq($dst$$Register, $src1$$Register, $src2$$Address, false);\n-  %}\n-  ins_pipe(ialu_reg_mem_alu0);\n-%}\n-\n-instruct mulL_mem_imm(rRegL dst, memory src, immL32 imm, rFlagsReg cr)\n-%{\n-  match(Set dst (MulL (LoadL src) imm));\n-  effect(KILL cr);\n-\n-  ins_cost(300);\n-  format %{ \"imulq   $dst, $src, $imm\\t# long\" %}\n-  ins_encode %{\n-    __ imulq($dst$$Register, $src$$Address, $imm$$constant);\n-  %}\n-  ins_pipe(ialu_reg_mem_alu0);\n-%}\n-\n-instruct mulHiL_rReg(rdx_RegL dst, rRegL src, rax_RegL rax, rFlagsReg cr)\n-%{\n-  match(Set dst (MulHiL src rax));\n-  effect(USE_KILL rax, KILL cr);\n-\n-  ins_cost(300);\n-  format %{ \"imulq   RDX:RAX, RAX, $src\\t# mulhi\" %}\n-  ins_encode %{\n-    __ imulq($src$$Register);\n-  %}\n-  ins_pipe(ialu_reg_reg_alu0);\n-%}\n-\n-instruct umulHiL_rReg(rdx_RegL dst, rRegL src, rax_RegL rax, rFlagsReg cr)\n-%{\n-  match(Set dst (UMulHiL src rax));\n-  effect(USE_KILL rax, KILL cr);\n-\n-  ins_cost(300);\n-  format %{ \"mulq   RDX:RAX, RAX, $src\\t# umulhi\" %}\n-  ins_encode %{\n-    __ mulq($src$$Register);\n-  %}\n-  ins_pipe(ialu_reg_reg_alu0);\n-%}\n-\n-instruct divI_rReg(rax_RegI rax, rdx_RegI rdx, no_rax_rdx_RegI div,\n-                   rFlagsReg cr)\n-%{\n-  match(Set rax (DivI rax div));\n-  effect(KILL rdx, KILL cr);\n-\n-  ins_cost(30*100+10*100); \/\/ XXX\n-  format %{ \"cmpl    rax, 0x80000000\\t# idiv\\n\\t\"\n-            \"jne,s   normal\\n\\t\"\n-            \"xorl    rdx, rdx\\n\\t\"\n-            \"cmpl    $div, -1\\n\\t\"\n-            \"je,s    done\\n\"\n-    \"normal: cdql\\n\\t\"\n-            \"idivl   $div\\n\"\n-    \"done:\"        %}\n-  ins_encode(cdql_enc(div));\n-  ins_pipe(ialu_reg_reg_alu0);\n-%}\n-\n-instruct divL_rReg(rax_RegL rax, rdx_RegL rdx, no_rax_rdx_RegL div,\n-                   rFlagsReg cr)\n-%{\n-  match(Set rax (DivL rax div));\n-  effect(KILL rdx, KILL cr);\n-\n-  ins_cost(30*100+10*100); \/\/ XXX\n-  format %{ \"movq    rdx, 0x8000000000000000\\t# ldiv\\n\\t\"\n-            \"cmpq    rax, rdx\\n\\t\"\n-            \"jne,s   normal\\n\\t\"\n-            \"xorl    rdx, rdx\\n\\t\"\n-            \"cmpq    $div, -1\\n\\t\"\n-            \"je,s    done\\n\"\n-    \"normal: cdqq\\n\\t\"\n-            \"idivq   $div\\n\"\n-    \"done:\"        %}\n-  ins_encode(cdqq_enc(div));\n-  ins_pipe(ialu_reg_reg_alu0);\n-%}\n-\n-instruct udivI_rReg(rax_RegI rax, rdx_RegI rdx, no_rax_rdx_RegI div, rFlagsReg cr)\n-%{\n-  match(Set rax (UDivI rax div));\n-  effect(KILL rdx, KILL cr);\n-\n-  ins_cost(300);\n-  format %{ \"udivl $rax,$rax,$div\\t# UDivI\\n\" %}\n-  ins_encode %{\n-    __ udivI($rax$$Register, $div$$Register, $rdx$$Register);\n-  %}\n-  ins_pipe(ialu_reg_reg_alu0);\n-%}\n-\n-instruct udivL_rReg(rax_RegL rax, rdx_RegL rdx, no_rax_rdx_RegL div, rFlagsReg cr)\n-%{\n-  match(Set rax (UDivL rax div));\n-  effect(KILL rdx, KILL cr);\n-\n-  ins_cost(300);\n-  format %{ \"udivq $rax,$rax,$div\\t# UDivL\\n\" %}\n-  ins_encode %{\n-     __ udivL($rax$$Register, $div$$Register, $rdx$$Register);\n-  %}\n-  ins_pipe(ialu_reg_reg_alu0);\n-%}\n-\n-\/\/ Integer DIVMOD with Register, both quotient and mod results\n-instruct divModI_rReg_divmod(rax_RegI rax, rdx_RegI rdx, no_rax_rdx_RegI div,\n-                             rFlagsReg cr)\n-%{\n-  match(DivModI rax div);\n-  effect(KILL cr);\n-\n-  ins_cost(30*100+10*100); \/\/ XXX\n-  format %{ \"cmpl    rax, 0x80000000\\t# idiv\\n\\t\"\n-            \"jne,s   normal\\n\\t\"\n-            \"xorl    rdx, rdx\\n\\t\"\n-            \"cmpl    $div, -1\\n\\t\"\n-            \"je,s    done\\n\"\n-    \"normal: cdql\\n\\t\"\n-            \"idivl   $div\\n\"\n-    \"done:\"        %}\n-  ins_encode(cdql_enc(div));\n-  ins_pipe(pipe_slow);\n-%}\n-\n-\/\/ Long DIVMOD with Register, both quotient and mod results\n-instruct divModL_rReg_divmod(rax_RegL rax, rdx_RegL rdx, no_rax_rdx_RegL div,\n-                             rFlagsReg cr)\n-%{\n-  match(DivModL rax div);\n-  effect(KILL cr);\n-\n-  ins_cost(30*100+10*100); \/\/ XXX\n-  format %{ \"movq    rdx, 0x8000000000000000\\t# ldiv\\n\\t\"\n-            \"cmpq    rax, rdx\\n\\t\"\n-            \"jne,s   normal\\n\\t\"\n-            \"xorl    rdx, rdx\\n\\t\"\n-            \"cmpq    $div, -1\\n\\t\"\n-            \"je,s    done\\n\"\n-    \"normal: cdqq\\n\\t\"\n-            \"idivq   $div\\n\"\n-    \"done:\"        %}\n-  ins_encode(cdqq_enc(div));\n-  ins_pipe(pipe_slow);\n-%}\n-\n-\/\/ Unsigned integer DIVMOD with Register, both quotient and mod results\n-instruct udivModI_rReg_divmod(rax_RegI rax, no_rax_rdx_RegI tmp, rdx_RegI rdx,\n-                              no_rax_rdx_RegI div, rFlagsReg cr)\n-%{\n-  match(UDivModI rax div);\n-  effect(TEMP tmp, KILL cr);\n-\n-  ins_cost(300);\n-  format %{ \"udivl $rax,$rax,$div\\t# begin UDivModI\\n\\t\"\n-            \"umodl $rdx,$rax,$div\\t! using $tmp as TEMP # end UDivModI\\n\"\n-          %}\n-  ins_encode %{\n-    __ udivmodI($rax$$Register, $div$$Register, $rdx$$Register, $tmp$$Register);\n-  %}\n-  ins_pipe(pipe_slow);\n-%}\n-\n-\/\/ Unsigned long DIVMOD with Register, both quotient and mod results\n-instruct udivModL_rReg_divmod(rax_RegL rax, no_rax_rdx_RegL tmp, rdx_RegL rdx,\n-                              no_rax_rdx_RegL div, rFlagsReg cr)\n-%{\n-  match(UDivModL rax div);\n-  effect(TEMP tmp, KILL cr);\n-\n-  ins_cost(300);\n-  format %{ \"udivq $rax,$rax,$div\\t# begin UDivModL\\n\\t\"\n-            \"umodq $rdx,$rax,$div\\t! using $tmp as TEMP # end UDivModL\\n\"\n-          %}\n-  ins_encode %{\n-    __ udivmodL($rax$$Register, $div$$Register, $rdx$$Register, $tmp$$Register);\n-  %}\n-  ins_pipe(pipe_slow);\n-%}\n-\n-instruct modI_rReg(rdx_RegI rdx, rax_RegI rax, no_rax_rdx_RegI div,\n-                   rFlagsReg cr)\n-%{\n-  match(Set rdx (ModI rax div));\n-  effect(KILL rax, KILL cr);\n-\n-  ins_cost(300); \/\/ XXX\n-  format %{ \"cmpl    rax, 0x80000000\\t# irem\\n\\t\"\n-            \"jne,s   normal\\n\\t\"\n-            \"xorl    rdx, rdx\\n\\t\"\n-            \"cmpl    $div, -1\\n\\t\"\n-            \"je,s    done\\n\"\n-    \"normal: cdql\\n\\t\"\n-            \"idivl   $div\\n\"\n-    \"done:\"        %}\n-  ins_encode(cdql_enc(div));\n-  ins_pipe(ialu_reg_reg_alu0);\n-%}\n-\n-instruct modL_rReg(rdx_RegL rdx, rax_RegL rax, no_rax_rdx_RegL div,\n-                   rFlagsReg cr)\n-%{\n-  match(Set rdx (ModL rax div));\n-  effect(KILL rax, KILL cr);\n-\n-  ins_cost(300); \/\/ XXX\n-  format %{ \"movq    rdx, 0x8000000000000000\\t# lrem\\n\\t\"\n-            \"cmpq    rax, rdx\\n\\t\"\n-            \"jne,s   normal\\n\\t\"\n-            \"xorl    rdx, rdx\\n\\t\"\n-            \"cmpq    $div, -1\\n\\t\"\n-            \"je,s    done\\n\"\n-    \"normal: cdqq\\n\\t\"\n-            \"idivq   $div\\n\"\n-    \"done:\"        %}\n-  ins_encode(cdqq_enc(div));\n-  ins_pipe(ialu_reg_reg_alu0);\n-%}\n-\n-instruct umodI_rReg(rdx_RegI rdx, rax_RegI rax, no_rax_rdx_RegI div, rFlagsReg cr)\n-%{\n-  match(Set rdx (UModI rax div));\n-  effect(KILL rax, KILL cr);\n-\n-  ins_cost(300);\n-  format %{ \"umodl $rdx,$rax,$div\\t# UModI\\n\" %}\n-  ins_encode %{\n-    __ umodI($rax$$Register, $div$$Register, $rdx$$Register);\n-  %}\n-  ins_pipe(ialu_reg_reg_alu0);\n-%}\n-\n-instruct umodL_rReg(rdx_RegL rdx, rax_RegL rax, no_rax_rdx_RegL div, rFlagsReg cr)\n-%{\n-  match(Set rdx (UModL rax div));\n-  effect(KILL rax, KILL cr);\n-\n-  ins_cost(300);\n-  format %{ \"umodq $rdx,$rax,$div\\t# UModL\\n\" %}\n-  ins_encode %{\n-    __ umodL($rax$$Register, $div$$Register, $rdx$$Register);\n-  %}\n-  ins_pipe(ialu_reg_reg_alu0);\n-%}\n-\n-\/\/ Integer Shift Instructions\n-\/\/ Shift Left by one, two, three\n-instruct salI_rReg_immI2(rRegI dst, immI2 shift, rFlagsReg cr)\n-%{\n-  predicate(!UseAPX);\n-  match(Set dst (LShiftI dst shift));\n-  effect(KILL cr);\n-\n-  format %{ \"sall    $dst, $shift\" %}\n-  ins_encode %{\n-    __ sall($dst$$Register, $shift$$constant);\n-  %}\n-  ins_pipe(ialu_reg);\n-%}\n-\n-\/\/ Shift Left by one, two, three\n-instruct salI_rReg_immI2_ndd(rRegI dst, rRegI src, immI2 shift, rFlagsReg cr)\n-%{\n-  predicate(UseAPX);\n-  match(Set dst (LShiftI src shift));\n-  effect(KILL cr);\n-\n-  format %{ \"esall    $dst, $src, $shift\\t# int(ndd)\" %}\n-  ins_encode %{\n-    __ esall($dst$$Register, $src$$Register, $shift$$constant, false);\n-  %}\n-  ins_pipe(ialu_reg);\n-%}\n-\n-\/\/ Shift Left by 8-bit immediate\n-instruct salI_rReg_imm(rRegI dst, immI8 shift, rFlagsReg cr)\n-%{\n-  predicate(!UseAPX);\n-  match(Set dst (LShiftI dst shift));\n-  effect(KILL cr);\n-\n-  format %{ \"sall    $dst, $shift\" %}\n-  ins_encode %{\n-    __ sall($dst$$Register, $shift$$constant);\n-  %}\n-  ins_pipe(ialu_reg);\n-%}\n-\n-\/\/ Shift Left by 8-bit immediate\n-instruct salI_rReg_imm_ndd(rRegI dst, rRegI src, immI8 shift, rFlagsReg cr)\n-%{\n-  predicate(UseAPX);\n-  match(Set dst (LShiftI src shift));\n-  effect(KILL cr);\n-\n-  format %{ \"esall    $dst, $src, $shift\\t# int (ndd)\" %}\n-  ins_encode %{\n-    __ esall($dst$$Register, $src$$Register, $shift$$constant, false);\n-  %}\n-  ins_pipe(ialu_reg);\n-%}\n-\n-instruct salI_rReg_mem_imm_ndd(rRegI dst, memory src, immI8 shift, rFlagsReg cr)\n-%{\n-  predicate(UseAPX);\n-  match(Set dst (LShiftI (LoadI src) shift));\n-  effect(KILL cr);\n-\n-  format %{ \"esall    $dst, $src, $shift\\t# int (ndd)\" %}\n-  ins_encode %{\n-    __ esall($dst$$Register, $src$$Address, $shift$$constant, false);\n-  %}\n-  ins_pipe(ialu_reg);\n-%}\n-\n-\/\/ Shift Left by 8-bit immediate\n-instruct salI_mem_imm(memory dst, immI8 shift, rFlagsReg cr)\n-%{\n-  match(Set dst (StoreI dst (LShiftI (LoadI dst) shift)));\n-  effect(KILL cr);\n-\n-  format %{ \"sall    $dst, $shift\" %}\n-  ins_encode %{\n-    __ sall($dst$$Address, $shift$$constant);\n-  %}\n-  ins_pipe(ialu_mem_imm);\n-%}\n-\n-\/\/ Shift Left by variable\n-instruct salI_rReg_CL(rRegI dst, rcx_RegI shift, rFlagsReg cr)\n-%{\n-  predicate(!VM_Version::supports_bmi2());\n-  match(Set dst (LShiftI dst shift));\n-  effect(KILL cr);\n-\n-  format %{ \"sall    $dst, $shift\" %}\n-  ins_encode %{\n-    __ sall($dst$$Register);\n-  %}\n-  ins_pipe(ialu_reg_reg);\n-%}\n-\n-\/\/ Shift Left by variable\n-instruct salI_mem_CL(memory dst, rcx_RegI shift, rFlagsReg cr)\n-%{\n-  predicate(!VM_Version::supports_bmi2());\n-  match(Set dst (StoreI dst (LShiftI (LoadI dst) shift)));\n-  effect(KILL cr);\n-\n-  format %{ \"sall    $dst, $shift\" %}\n-  ins_encode %{\n-    __ sall($dst$$Address);\n-  %}\n-  ins_pipe(ialu_mem_reg);\n-%}\n-\n-instruct salI_rReg_rReg(rRegI dst, rRegI src, rRegI shift)\n-%{\n-  predicate(VM_Version::supports_bmi2());\n-  match(Set dst (LShiftI src shift));\n-\n-  format %{ \"shlxl   $dst, $src, $shift\" %}\n-  ins_encode %{\n-    __ shlxl($dst$$Register, $src$$Register, $shift$$Register);\n-  %}\n-  ins_pipe(ialu_reg_reg);\n-%}\n-\n-instruct salI_mem_rReg(rRegI dst, memory src, rRegI shift)\n-%{\n-  predicate(VM_Version::supports_bmi2());\n-  match(Set dst (LShiftI (LoadI src) shift));\n-  ins_cost(175);\n-  format %{ \"shlxl   $dst, $src, $shift\" %}\n-  ins_encode %{\n-    __ shlxl($dst$$Register, $src$$Address, $shift$$Register);\n-  %}\n-  ins_pipe(ialu_reg_mem);\n-%}\n-\n-\/\/ Arithmetic Shift Right by 8-bit immediate\n-instruct sarI_rReg_imm(rRegI dst, immI8 shift, rFlagsReg cr)\n-%{\n-  predicate(!UseAPX);\n-  match(Set dst (RShiftI dst shift));\n-  effect(KILL cr);\n-\n-  format %{ \"sarl    $dst, $shift\" %}\n-  ins_encode %{\n-    __ sarl($dst$$Register, $shift$$constant);\n-  %}\n-  ins_pipe(ialu_mem_imm);\n-%}\n-\n-\/\/ Arithmetic Shift Right by 8-bit immediate\n-instruct sarI_rReg_imm_ndd(rRegI dst, rRegI src, immI8 shift, rFlagsReg cr)\n-%{\n-  predicate(UseAPX);\n-  match(Set dst (RShiftI src shift));\n-  effect(KILL cr);\n-\n-  format %{ \"esarl    $dst, $src, $shift\\t# int (ndd)\" %}\n-  ins_encode %{\n-    __ esarl($dst$$Register, $src$$Register, $shift$$constant, false);\n-  %}\n-  ins_pipe(ialu_mem_imm);\n-%}\n-\n-instruct sarI_rReg_mem_imm_ndd(rRegI dst, memory src, immI8 shift, rFlagsReg cr)\n-%{\n-  predicate(UseAPX);\n-  match(Set dst (RShiftI (LoadI src) shift));\n-  effect(KILL cr);\n-\n-  format %{ \"esarl    $dst, $src, $shift\\t# int (ndd)\" %}\n-  ins_encode %{\n-    __ esarl($dst$$Register, $src$$Address, $shift$$constant, false);\n-  %}\n-  ins_pipe(ialu_mem_imm);\n-%}\n-\n-\/\/ Arithmetic Shift Right by 8-bit immediate\n-instruct sarI_mem_imm(memory dst, immI8 shift, rFlagsReg cr)\n-%{\n-  match(Set dst (StoreI dst (RShiftI (LoadI dst) shift)));\n-  effect(KILL cr);\n-\n-  format %{ \"sarl    $dst, $shift\" %}\n-  ins_encode %{\n-    __ sarl($dst$$Address, $shift$$constant);\n-  %}\n-  ins_pipe(ialu_mem_imm);\n-%}\n-\n-\/\/ Arithmetic Shift Right by variable\n-instruct sarI_rReg_CL(rRegI dst, rcx_RegI shift, rFlagsReg cr)\n-%{\n-  predicate(!VM_Version::supports_bmi2());\n-  match(Set dst (RShiftI dst shift));\n-  effect(KILL cr);\n-\n-  format %{ \"sarl    $dst, $shift\" %}\n-  ins_encode %{\n-    __ sarl($dst$$Register);\n-  %}\n-  ins_pipe(ialu_reg_reg);\n-%}\n-\n-\/\/ Arithmetic Shift Right by variable\n-instruct sarI_mem_CL(memory dst, rcx_RegI shift, rFlagsReg cr)\n-%{\n-  predicate(!VM_Version::supports_bmi2());\n-  match(Set dst (StoreI dst (RShiftI (LoadI dst) shift)));\n-  effect(KILL cr);\n-\n-  format %{ \"sarl    $dst, $shift\" %}\n-  ins_encode %{\n-    __ sarl($dst$$Address);\n-  %}\n-  ins_pipe(ialu_mem_reg);\n-%}\n-\n-instruct sarI_rReg_rReg(rRegI dst, rRegI src, rRegI shift)\n-%{\n-  predicate(VM_Version::supports_bmi2());\n-  match(Set dst (RShiftI src shift));\n-\n-  format %{ \"sarxl   $dst, $src, $shift\" %}\n-  ins_encode %{\n-    __ sarxl($dst$$Register, $src$$Register, $shift$$Register);\n-  %}\n-  ins_pipe(ialu_reg_reg);\n-%}\n-\n-instruct sarI_mem_rReg(rRegI dst, memory src, rRegI shift)\n-%{\n-  predicate(VM_Version::supports_bmi2());\n-  match(Set dst (RShiftI (LoadI src) shift));\n-  ins_cost(175);\n-  format %{ \"sarxl   $dst, $src, $shift\" %}\n-  ins_encode %{\n-    __ sarxl($dst$$Register, $src$$Address, $shift$$Register);\n-  %}\n-  ins_pipe(ialu_reg_mem);\n-%}\n-\n-\/\/ Logical Shift Right by 8-bit immediate\n-instruct shrI_rReg_imm(rRegI dst, immI8 shift, rFlagsReg cr)\n-%{\n-  predicate(!UseAPX);\n-  match(Set dst (URShiftI dst shift));\n-  effect(KILL cr);\n-\n-  format %{ \"shrl    $dst, $shift\" %}\n-  ins_encode %{\n-    __ shrl($dst$$Register, $shift$$constant);\n-  %}\n-  ins_pipe(ialu_reg);\n-%}\n-\n-\/\/ Logical Shift Right by 8-bit immediate\n-instruct shrI_rReg_imm_ndd(rRegI dst, rRegI src, immI8 shift, rFlagsReg cr)\n-%{\n-  predicate(UseAPX);\n-  match(Set dst (URShiftI src shift));\n-  effect(KILL cr);\n-\n-  format %{ \"eshrl    $dst, $src, $shift\\t # int (ndd)\" %}\n-  ins_encode %{\n-    __ eshrl($dst$$Register, $src$$Register, $shift$$constant, false);\n-  %}\n-  ins_pipe(ialu_reg);\n-%}\n-\n-instruct shrI_rReg_mem_imm_ndd(rRegI dst, memory src, immI8 shift, rFlagsReg cr)\n-%{\n-  predicate(UseAPX);\n-  match(Set dst (URShiftI (LoadI src) shift));\n-  effect(KILL cr);\n-\n-  format %{ \"eshrl    $dst, $src, $shift\\t # int (ndd)\" %}\n-  ins_encode %{\n-    __ eshrl($dst$$Register, $src$$Address, $shift$$constant, false);\n-  %}\n-  ins_pipe(ialu_reg);\n-%}\n-\n-\/\/ Logical Shift Right by 8-bit immediate\n-instruct shrI_mem_imm(memory dst, immI8 shift, rFlagsReg cr)\n-%{\n-  match(Set dst (StoreI dst (URShiftI (LoadI dst) shift)));\n-  effect(KILL cr);\n-\n-  format %{ \"shrl    $dst, $shift\" %}\n-  ins_encode %{\n-    __ shrl($dst$$Address, $shift$$constant);\n-  %}\n-  ins_pipe(ialu_mem_imm);\n-%}\n-\n-\/\/ Logical Shift Right by variable\n-instruct shrI_rReg_CL(rRegI dst, rcx_RegI shift, rFlagsReg cr)\n-%{\n-  predicate(!VM_Version::supports_bmi2());\n-  match(Set dst (URShiftI dst shift));\n-  effect(KILL cr);\n-\n-  format %{ \"shrl    $dst, $shift\" %}\n-  ins_encode %{\n-    __ shrl($dst$$Register);\n-  %}\n-  ins_pipe(ialu_reg_reg);\n-%}\n-\n-\/\/ Logical Shift Right by variable\n-instruct shrI_mem_CL(memory dst, rcx_RegI shift, rFlagsReg cr)\n-%{\n-  predicate(!VM_Version::supports_bmi2());\n-  match(Set dst (StoreI dst (URShiftI (LoadI dst) shift)));\n-  effect(KILL cr);\n-\n-  format %{ \"shrl    $dst, $shift\" %}\n-  ins_encode %{\n-    __ shrl($dst$$Address);\n-  %}\n-  ins_pipe(ialu_mem_reg);\n-%}\n-\n-instruct shrI_rReg_rReg(rRegI dst, rRegI src, rRegI shift)\n-%{\n-  predicate(VM_Version::supports_bmi2());\n-  match(Set dst (URShiftI src shift));\n-\n-  format %{ \"shrxl   $dst, $src, $shift\" %}\n-  ins_encode %{\n-    __ shrxl($dst$$Register, $src$$Register, $shift$$Register);\n-  %}\n-  ins_pipe(ialu_reg_reg);\n-%}\n-\n-instruct shrI_mem_rReg(rRegI dst, memory src, rRegI shift)\n-%{\n-  predicate(VM_Version::supports_bmi2());\n-  match(Set dst (URShiftI (LoadI src) shift));\n-  ins_cost(175);\n-  format %{ \"shrxl   $dst, $src, $shift\" %}\n-  ins_encode %{\n-    __ shrxl($dst$$Register, $src$$Address, $shift$$Register);\n-  %}\n-  ins_pipe(ialu_reg_mem);\n-%}\n-\n-\/\/ Long Shift Instructions\n-\/\/ Shift Left by one, two, three\n-instruct salL_rReg_immI2(rRegL dst, immI2 shift, rFlagsReg cr)\n-%{\n-  predicate(!UseAPX);\n-  match(Set dst (LShiftL dst shift));\n-  effect(KILL cr);\n-\n-  format %{ \"salq    $dst, $shift\" %}\n-  ins_encode %{\n-    __ salq($dst$$Register, $shift$$constant);\n-  %}\n-  ins_pipe(ialu_reg);\n-%}\n-\n-\/\/ Shift Left by one, two, three\n-instruct salL_rReg_immI2_ndd(rRegL dst, rRegL src, immI2 shift, rFlagsReg cr)\n-%{\n-  predicate(UseAPX);\n-  match(Set dst (LShiftL src shift));\n-  effect(KILL cr);\n-\n-  format %{ \"esalq    $dst, $src, $shift\\t# long (ndd)\" %}\n-  ins_encode %{\n-    __ esalq($dst$$Register, $src$$Register, $shift$$constant, false);\n-  %}\n-  ins_pipe(ialu_reg);\n-%}\n-\n-\/\/ Shift Left by 8-bit immediate\n-instruct salL_rReg_imm(rRegL dst, immI8 shift, rFlagsReg cr)\n-%{\n-  predicate(!UseAPX);\n-  match(Set dst (LShiftL dst shift));\n-  effect(KILL cr);\n-\n-  format %{ \"salq    $dst, $shift\" %}\n-  ins_encode %{\n-    __ salq($dst$$Register, $shift$$constant);\n-  %}\n-  ins_pipe(ialu_reg);\n-%}\n-\n-\/\/ Shift Left by 8-bit immediate\n-instruct salL_rReg_imm_ndd(rRegL dst, rRegL src, immI8 shift, rFlagsReg cr)\n-%{\n-  predicate(UseAPX);\n-  match(Set dst (LShiftL src shift));\n-  effect(KILL cr);\n-\n-  format %{ \"esalq    $dst, $src, $shift\\t# long (ndd)\" %}\n-  ins_encode %{\n-    __ esalq($dst$$Register, $src$$Register, $shift$$constant, false);\n-  %}\n-  ins_pipe(ialu_reg);\n-%}\n-\n-instruct salL_rReg_mem_imm_ndd(rRegL dst, memory src, immI8 shift, rFlagsReg cr)\n-%{\n-  predicate(UseAPX);\n-  match(Set dst (LShiftL (LoadL src) shift));\n-  effect(KILL cr);\n-\n-  format %{ \"esalq    $dst, $src, $shift\\t# long (ndd)\" %}\n-  ins_encode %{\n-    __ esalq($dst$$Register, $src$$Address, $shift$$constant, false);\n-  %}\n-  ins_pipe(ialu_reg);\n-%}\n-\n-\/\/ Shift Left by 8-bit immediate\n-instruct salL_mem_imm(memory dst, immI8 shift, rFlagsReg cr)\n-%{\n-  match(Set dst (StoreL dst (LShiftL (LoadL dst) shift)));\n-  effect(KILL cr);\n-\n-  format %{ \"salq    $dst, $shift\" %}\n-  ins_encode %{\n-    __ salq($dst$$Address, $shift$$constant);\n-  %}\n-  ins_pipe(ialu_mem_imm);\n-%}\n-\n-\/\/ Shift Left by variable\n-instruct salL_rReg_CL(rRegL dst, rcx_RegI shift, rFlagsReg cr)\n-%{\n-  predicate(!VM_Version::supports_bmi2());\n-  match(Set dst (LShiftL dst shift));\n-  effect(KILL cr);\n-\n-  format %{ \"salq    $dst, $shift\" %}\n-  ins_encode %{\n-    __ salq($dst$$Register);\n-  %}\n-  ins_pipe(ialu_reg_reg);\n-%}\n-\n-\/\/ Shift Left by variable\n-instruct salL_mem_CL(memory dst, rcx_RegI shift, rFlagsReg cr)\n-%{\n-  predicate(!VM_Version::supports_bmi2());\n-  match(Set dst (StoreL dst (LShiftL (LoadL dst) shift)));\n-  effect(KILL cr);\n-\n-  format %{ \"salq    $dst, $shift\" %}\n-  ins_encode %{\n-    __ salq($dst$$Address);\n-  %}\n-  ins_pipe(ialu_mem_reg);\n-%}\n-\n-instruct salL_rReg_rReg(rRegL dst, rRegL src, rRegI shift)\n-%{\n-  predicate(VM_Version::supports_bmi2());\n-  match(Set dst (LShiftL src shift));\n-\n-  format %{ \"shlxq   $dst, $src, $shift\" %}\n-  ins_encode %{\n-    __ shlxq($dst$$Register, $src$$Register, $shift$$Register);\n-  %}\n-  ins_pipe(ialu_reg_reg);\n-%}\n-\n-instruct salL_mem_rReg(rRegL dst, memory src, rRegI shift)\n-%{\n-  predicate(VM_Version::supports_bmi2());\n-  match(Set dst (LShiftL (LoadL src) shift));\n-  ins_cost(175);\n-  format %{ \"shlxq   $dst, $src, $shift\" %}\n-  ins_encode %{\n-    __ shlxq($dst$$Register, $src$$Address, $shift$$Register);\n-  %}\n-  ins_pipe(ialu_reg_mem);\n-%}\n-\n-\/\/ Arithmetic Shift Right by 8-bit immediate\n-instruct sarL_rReg_imm(rRegL dst, immI shift, rFlagsReg cr)\n-%{\n-  predicate(!UseAPX);\n-  match(Set dst (RShiftL dst shift));\n-  effect(KILL cr);\n-\n-  format %{ \"sarq    $dst, $shift\" %}\n-  ins_encode %{\n-    __ sarq($dst$$Register, (unsigned char)($shift$$constant & 0x3F));\n-  %}\n-  ins_pipe(ialu_mem_imm);\n-%}\n-\n-\/\/ Arithmetic Shift Right by 8-bit immediate\n-instruct sarL_rReg_imm_ndd(rRegL dst, rRegL src, immI shift, rFlagsReg cr)\n-%{\n-  predicate(UseAPX);\n-  match(Set dst (RShiftL src shift));\n-  effect(KILL cr);\n-\n-  format %{ \"esarq    $dst, $src, $shift\\t# long (ndd)\" %}\n-  ins_encode %{\n-    __ esarq($dst$$Register, $src$$Register, (unsigned char)($shift$$constant & 0x3F), false);\n-  %}\n-  ins_pipe(ialu_mem_imm);\n-%}\n-\n-instruct sarL_rReg_mem_imm_ndd(rRegL dst, memory src, immI shift, rFlagsReg cr)\n-%{\n-  predicate(UseAPX);\n-  match(Set dst (RShiftL (LoadL src) shift));\n-  effect(KILL cr);\n-\n-  format %{ \"esarq    $dst, $src, $shift\\t# long (ndd)\" %}\n-  ins_encode %{\n-    __ esarq($dst$$Register, $src$$Address, (unsigned char)($shift$$constant & 0x3F), false);\n-  %}\n-  ins_pipe(ialu_mem_imm);\n-%}\n-\n-\/\/ Arithmetic Shift Right by 8-bit immediate\n-instruct sarL_mem_imm(memory dst, immI shift, rFlagsReg cr)\n-%{\n-  match(Set dst (StoreL dst (RShiftL (LoadL dst) shift)));\n-  effect(KILL cr);\n-\n-  format %{ \"sarq    $dst, $shift\" %}\n-  ins_encode %{\n-    __ sarq($dst$$Address, (unsigned char)($shift$$constant & 0x3F));\n-  %}\n-  ins_pipe(ialu_mem_imm);\n-%}\n-\n-\/\/ Arithmetic Shift Right by variable\n-instruct sarL_rReg_CL(rRegL dst, rcx_RegI shift, rFlagsReg cr)\n-%{\n-  predicate(!VM_Version::supports_bmi2());\n-  match(Set dst (RShiftL dst shift));\n-  effect(KILL cr);\n-\n-  format %{ \"sarq    $dst, $shift\" %}\n-  ins_encode %{\n-    __ sarq($dst$$Register);\n-  %}\n-  ins_pipe(ialu_reg_reg);\n-%}\n-\n-\/\/ Arithmetic Shift Right by variable\n-instruct sarL_mem_CL(memory dst, rcx_RegI shift, rFlagsReg cr)\n-%{\n-  predicate(!VM_Version::supports_bmi2());\n-  match(Set dst (StoreL dst (RShiftL (LoadL dst) shift)));\n-  effect(KILL cr);\n-\n-  format %{ \"sarq    $dst, $shift\" %}\n-  ins_encode %{\n-    __ sarq($dst$$Address);\n-  %}\n-  ins_pipe(ialu_mem_reg);\n-%}\n-\n-instruct sarL_rReg_rReg(rRegL dst, rRegL src, rRegI shift)\n-%{\n-  predicate(VM_Version::supports_bmi2());\n-  match(Set dst (RShiftL src shift));\n-\n-  format %{ \"sarxq   $dst, $src, $shift\" %}\n-  ins_encode %{\n-    __ sarxq($dst$$Register, $src$$Register, $shift$$Register);\n-  %}\n-  ins_pipe(ialu_reg_reg);\n-%}\n-\n-instruct sarL_mem_rReg(rRegL dst, memory src, rRegI shift)\n-%{\n-  predicate(VM_Version::supports_bmi2());\n-  match(Set dst (RShiftL (LoadL src) shift));\n-  ins_cost(175);\n-  format %{ \"sarxq   $dst, $src, $shift\" %}\n-  ins_encode %{\n-    __ sarxq($dst$$Register, $src$$Address, $shift$$Register);\n-  %}\n-  ins_pipe(ialu_reg_mem);\n-%}\n-\n-\/\/ Logical Shift Right by 8-bit immediate\n-instruct shrL_rReg_imm(rRegL dst, immI8 shift, rFlagsReg cr)\n-%{\n-  predicate(!UseAPX);\n-  match(Set dst (URShiftL dst shift));\n-  effect(KILL cr);\n-\n-  format %{ \"shrq    $dst, $shift\" %}\n-  ins_encode %{\n-    __ shrq($dst$$Register, $shift$$constant);\n-  %}\n-  ins_pipe(ialu_reg);\n-%}\n-\n-\/\/ Logical Shift Right by 8-bit immediate\n-instruct shrL_rReg_imm_ndd(rRegL dst, rRegL src, immI8 shift, rFlagsReg cr)\n-%{\n-  predicate(UseAPX);\n-  match(Set dst (URShiftL src shift));\n-  effect(KILL cr);\n-\n-  format %{ \"eshrq    $dst, $src, $shift\\t# long (ndd)\" %}\n-  ins_encode %{\n-    __ eshrq($dst$$Register, $src$$Register, $shift$$constant, false);\n-  %}\n-  ins_pipe(ialu_reg);\n-%}\n-\n-instruct shrL_rReg_mem_imm_ndd(rRegL dst, memory src, immI8 shift, rFlagsReg cr)\n-%{\n-  predicate(UseAPX);\n-  match(Set dst (URShiftL (LoadL src) shift));\n-  effect(KILL cr);\n-\n-  format %{ \"eshrq    $dst, $src, $shift\\t# long (ndd)\" %}\n-  ins_encode %{\n-    __ eshrq($dst$$Register, $src$$Address, $shift$$constant, false);\n-  %}\n-  ins_pipe(ialu_reg);\n-%}\n-\n-\/\/ Logical Shift Right by 8-bit immediate\n-instruct shrL_mem_imm(memory dst, immI8 shift, rFlagsReg cr)\n-%{\n-  match(Set dst (StoreL dst (URShiftL (LoadL dst) shift)));\n-  effect(KILL cr);\n-\n-  format %{ \"shrq    $dst, $shift\" %}\n-  ins_encode %{\n-    __ shrq($dst$$Address, $shift$$constant);\n-  %}\n-  ins_pipe(ialu_mem_imm);\n-%}\n-\n-\/\/ Logical Shift Right by variable\n-instruct shrL_rReg_CL(rRegL dst, rcx_RegI shift, rFlagsReg cr)\n-%{\n-  predicate(!VM_Version::supports_bmi2());\n-  match(Set dst (URShiftL dst shift));\n-  effect(KILL cr);\n-\n-  format %{ \"shrq    $dst, $shift\" %}\n-  ins_encode %{\n-    __ shrq($dst$$Register);\n-  %}\n-  ins_pipe(ialu_reg_reg);\n-%}\n-\n-\/\/ Logical Shift Right by variable\n-instruct shrL_mem_CL(memory dst, rcx_RegI shift, rFlagsReg cr)\n-%{\n-  predicate(!VM_Version::supports_bmi2());\n-  match(Set dst (StoreL dst (URShiftL (LoadL dst) shift)));\n-  effect(KILL cr);\n-\n-  format %{ \"shrq    $dst, $shift\" %}\n-  ins_encode %{\n-    __ shrq($dst$$Address);\n-  %}\n-  ins_pipe(ialu_mem_reg);\n-%}\n-\n-instruct shrL_rReg_rReg(rRegL dst, rRegL src, rRegI shift)\n-%{\n-  predicate(VM_Version::supports_bmi2());\n-  match(Set dst (URShiftL src shift));\n-\n-  format %{ \"shrxq   $dst, $src, $shift\" %}\n-  ins_encode %{\n-    __ shrxq($dst$$Register, $src$$Register, $shift$$Register);\n-  %}\n-  ins_pipe(ialu_reg_reg);\n-%}\n-\n-instruct shrL_mem_rReg(rRegL dst, memory src, rRegI shift)\n-%{\n-  predicate(VM_Version::supports_bmi2());\n-  match(Set dst (URShiftL (LoadL src) shift));\n-  ins_cost(175);\n-  format %{ \"shrxq   $dst, $src, $shift\" %}\n-  ins_encode %{\n-    __ shrxq($dst$$Register, $src$$Address, $shift$$Register);\n-  %}\n-  ins_pipe(ialu_reg_mem);\n-%}\n-\n-\/\/ Logical Shift Right by 24, followed by Arithmetic Shift Left by 24.\n-\/\/ This idiom is used by the compiler for the i2b bytecode.\n-instruct i2b(rRegI dst, rRegI src, immI_24 twentyfour)\n-%{\n-  match(Set dst (RShiftI (LShiftI src twentyfour) twentyfour));\n-\n-  format %{ \"movsbl  $dst, $src\\t# i2b\" %}\n-  ins_encode %{\n-    __ movsbl($dst$$Register, $src$$Register);\n-  %}\n-  ins_pipe(ialu_reg_reg);\n-%}\n-\n-\/\/ Logical Shift Right by 16, followed by Arithmetic Shift Left by 16.\n-\/\/ This idiom is used by the compiler the i2s bytecode.\n-instruct i2s(rRegI dst, rRegI src, immI_16 sixteen)\n-%{\n-  match(Set dst (RShiftI (LShiftI src sixteen) sixteen));\n-\n-  format %{ \"movswl  $dst, $src\\t# i2s\" %}\n-  ins_encode %{\n-    __ movswl($dst$$Register, $src$$Register);\n-  %}\n-  ins_pipe(ialu_reg_reg);\n-%}\n-\n-\/\/ ROL\/ROR instructions\n-\n-\/\/ Rotate left by constant.\n-instruct rolI_immI8_legacy(rRegI dst, immI8 shift, rFlagsReg cr)\n-%{\n-  predicate(!VM_Version::supports_bmi2() && n->bottom_type()->basic_type() == T_INT);\n-  match(Set dst (RotateLeft dst shift));\n-  effect(KILL cr);\n-  format %{ \"roll    $dst, $shift\" %}\n-  ins_encode %{\n-    __ roll($dst$$Register, $shift$$constant);\n-  %}\n-  ins_pipe(ialu_reg);\n-%}\n-\n-instruct rolI_immI8(rRegI dst, rRegI src, immI8 shift)\n-%{\n-  predicate(!UseAPX && VM_Version::supports_bmi2() && n->bottom_type()->basic_type() == T_INT);\n-  match(Set dst (RotateLeft src shift));\n-  format %{ \"rolxl   $dst, $src, $shift\" %}\n-  ins_encode %{\n-    int shift = 32 - ($shift$$constant & 31);\n-    __ rorxl($dst$$Register, $src$$Register, shift);\n-  %}\n-  ins_pipe(ialu_reg_reg);\n-%}\n-\n-instruct rolI_mem_immI8(rRegI dst, memory src, immI8 shift)\n-%{\n-  predicate(VM_Version::supports_bmi2() && n->bottom_type()->basic_type() == T_INT);\n-  match(Set dst (RotateLeft (LoadI src) shift));\n-  ins_cost(175);\n-  format %{ \"rolxl   $dst, $src, $shift\" %}\n-  ins_encode %{\n-    int shift = 32 - ($shift$$constant & 31);\n-    __ rorxl($dst$$Register, $src$$Address, shift);\n-  %}\n-  ins_pipe(ialu_reg_mem);\n-%}\n-\n-\/\/ Rotate Left by variable\n-instruct rolI_rReg_Var(rRegI dst, rcx_RegI shift, rFlagsReg cr)\n-%{\n-  predicate(!UseAPX && n->bottom_type()->basic_type() == T_INT);\n-  match(Set dst (RotateLeft dst shift));\n-  effect(KILL cr);\n-  format %{ \"roll    $dst, $shift\" %}\n-  ins_encode %{\n-    __ roll($dst$$Register);\n-  %}\n-  ins_pipe(ialu_reg_reg);\n-%}\n-\n-\/\/ Rotate Left by variable\n-instruct rolI_rReg_Var_ndd(rRegI dst, rRegI src, rcx_RegI shift, rFlagsReg cr)\n-%{\n-  predicate(UseAPX && n->bottom_type()->basic_type() == T_INT);\n-  match(Set dst (RotateLeft src shift));\n-  effect(KILL cr);\n-\n-  format %{ \"eroll    $dst, $src, $shift\\t# rotate left (int ndd)\" %}\n-  ins_encode %{\n-    __ eroll($dst$$Register, $src$$Register, false);\n-  %}\n-  ins_pipe(ialu_reg_reg);\n-%}\n-\n-\/\/ Rotate Right by constant.\n-instruct rorI_immI8_legacy(rRegI dst, immI8 shift, rFlagsReg cr)\n-%{\n-  predicate(!VM_Version::supports_bmi2() && n->bottom_type()->basic_type() == T_INT);\n-  match(Set dst (RotateRight dst shift));\n-  effect(KILL cr);\n-  format %{ \"rorl    $dst, $shift\" %}\n-  ins_encode %{\n-    __ rorl($dst$$Register, $shift$$constant);\n-  %}\n-  ins_pipe(ialu_reg);\n-%}\n-\n-\/\/ Rotate Right by constant.\n-instruct rorI_immI8(rRegI dst, rRegI src, immI8 shift)\n-%{\n-  predicate(!UseAPX && VM_Version::supports_bmi2() && n->bottom_type()->basic_type() == T_INT);\n-  match(Set dst (RotateRight src shift));\n-  format %{ \"rorxl   $dst, $src, $shift\" %}\n-  ins_encode %{\n-    __ rorxl($dst$$Register, $src$$Register, $shift$$constant);\n-  %}\n-  ins_pipe(ialu_reg_reg);\n-%}\n-\n-instruct rorI_mem_immI8(rRegI dst, memory src, immI8 shift)\n-%{\n-  predicate(VM_Version::supports_bmi2() && n->bottom_type()->basic_type() == T_INT);\n-  match(Set dst (RotateRight (LoadI src) shift));\n-  ins_cost(175);\n-  format %{ \"rorxl   $dst, $src, $shift\" %}\n-  ins_encode %{\n-    __ rorxl($dst$$Register, $src$$Address, $shift$$constant);\n-  %}\n-  ins_pipe(ialu_reg_mem);\n-%}\n-\n-\/\/ Rotate Right by variable\n-instruct rorI_rReg_Var(rRegI dst, rcx_RegI shift, rFlagsReg cr)\n-%{\n-  predicate(!UseAPX && n->bottom_type()->basic_type() == T_INT);\n-  match(Set dst (RotateRight dst shift));\n-  effect(KILL cr);\n-  format %{ \"rorl    $dst, $shift\" %}\n-  ins_encode %{\n-    __ rorl($dst$$Register);\n-  %}\n-  ins_pipe(ialu_reg_reg);\n-%}\n-\n-\/\/ Rotate Right by variable\n-instruct rorI_rReg_Var_ndd(rRegI dst, rRegI src, rcx_RegI shift, rFlagsReg cr)\n-%{\n-  predicate(UseAPX && n->bottom_type()->basic_type() == T_INT);\n-  match(Set dst (RotateRight src shift));\n-  effect(KILL cr);\n-\n-  format %{ \"erorl    $dst, $src, $shift\\t# rotate right(int ndd)\" %}\n-  ins_encode %{\n-    __ erorl($dst$$Register, $src$$Register, false);\n-  %}\n-  ins_pipe(ialu_reg_reg);\n-%}\n-\n-\/\/ Rotate Left by constant.\n-instruct rolL_immI8_legacy(rRegL dst, immI8 shift, rFlagsReg cr)\n-%{\n-  predicate(!VM_Version::supports_bmi2() && n->bottom_type()->basic_type() == T_LONG);\n-  match(Set dst (RotateLeft dst shift));\n-  effect(KILL cr);\n-  format %{ \"rolq    $dst, $shift\" %}\n-  ins_encode %{\n-    __ rolq($dst$$Register, $shift$$constant);\n-  %}\n-  ins_pipe(ialu_reg);\n-%}\n-\n-instruct rolL_immI8(rRegL dst, rRegL src, immI8 shift)\n-%{\n-  predicate(!UseAPX && VM_Version::supports_bmi2() && n->bottom_type()->basic_type() == T_LONG);\n-  match(Set dst (RotateLeft src shift));\n-  format %{ \"rolxq   $dst, $src, $shift\" %}\n-  ins_encode %{\n-    int shift = 64 - ($shift$$constant & 63);\n-    __ rorxq($dst$$Register, $src$$Register, shift);\n-  %}\n-  ins_pipe(ialu_reg_reg);\n-%}\n-\n-instruct rolL_mem_immI8(rRegL dst, memory src, immI8 shift)\n-%{\n-  predicate(VM_Version::supports_bmi2() && n->bottom_type()->basic_type() == T_LONG);\n-  match(Set dst (RotateLeft (LoadL src) shift));\n-  ins_cost(175);\n-  format %{ \"rolxq   $dst, $src, $shift\" %}\n-  ins_encode %{\n-    int shift = 64 - ($shift$$constant & 63);\n-    __ rorxq($dst$$Register, $src$$Address, shift);\n-  %}\n-  ins_pipe(ialu_reg_mem);\n-%}\n-\n-\/\/ Rotate Left by variable\n-instruct rolL_rReg_Var(rRegL dst, rcx_RegI shift, rFlagsReg cr)\n-%{\n-  predicate(!UseAPX && n->bottom_type()->basic_type() == T_LONG);\n-  match(Set dst (RotateLeft dst shift));\n-  effect(KILL cr);\n-  format %{ \"rolq    $dst, $shift\" %}\n-  ins_encode %{\n-    __ rolq($dst$$Register);\n-  %}\n-  ins_pipe(ialu_reg_reg);\n-%}\n-\n-\/\/ Rotate Left by variable\n-instruct rolL_rReg_Var_ndd(rRegL dst, rRegL src, rcx_RegI shift, rFlagsReg cr)\n-%{\n-  predicate(UseAPX && n->bottom_type()->basic_type() == T_LONG);\n-  match(Set dst (RotateLeft src shift));\n-  effect(KILL cr);\n-\n-  format %{ \"erolq    $dst, $src, $shift\\t# rotate left(long ndd)\" %}\n-  ins_encode %{\n-    __ erolq($dst$$Register, $src$$Register, false);\n-  %}\n-  ins_pipe(ialu_reg_reg);\n-%}\n-\n-\/\/ Rotate Right by constant.\n-instruct rorL_immI8_legacy(rRegL dst, immI8 shift, rFlagsReg cr)\n-%{\n-  predicate(!VM_Version::supports_bmi2() && n->bottom_type()->basic_type() == T_LONG);\n-  match(Set dst (RotateRight dst shift));\n-  effect(KILL cr);\n-  format %{ \"rorq    $dst, $shift\" %}\n-  ins_encode %{\n-    __ rorq($dst$$Register, $shift$$constant);\n-  %}\n-  ins_pipe(ialu_reg);\n-%}\n-\n-\/\/ Rotate Right by constant\n-instruct rorL_immI8(rRegL dst, rRegL src, immI8 shift)\n-%{\n-  predicate(VM_Version::supports_bmi2() && n->bottom_type()->basic_type() == T_LONG);\n-  match(Set dst (RotateRight src shift));\n-  format %{ \"rorxq   $dst, $src, $shift\" %}\n-  ins_encode %{\n-    __ rorxq($dst$$Register, $src$$Register, $shift$$constant);\n-  %}\n-  ins_pipe(ialu_reg_reg);\n-%}\n-\n-instruct rorL_mem_immI8(rRegL dst, memory src, immI8 shift)\n-%{\n-  predicate(VM_Version::supports_bmi2() && n->bottom_type()->basic_type() == T_LONG);\n-  match(Set dst (RotateRight (LoadL src) shift));\n-  ins_cost(175);\n-  format %{ \"rorxq   $dst, $src, $shift\" %}\n-  ins_encode %{\n-    __ rorxq($dst$$Register, $src$$Address, $shift$$constant);\n-  %}\n-  ins_pipe(ialu_reg_mem);\n-%}\n-\n-\/\/ Rotate Right by variable\n-instruct rorL_rReg_Var(rRegL dst, rcx_RegI shift, rFlagsReg cr)\n-%{\n-  predicate(!UseAPX && n->bottom_type()->basic_type() == T_LONG);\n-  match(Set dst (RotateRight dst shift));\n-  effect(KILL cr);\n-  format %{ \"rorq    $dst, $shift\" %}\n-  ins_encode %{\n-    __ rorq($dst$$Register);\n-  %}\n-  ins_pipe(ialu_reg_reg);\n-%}\n-\n-\/\/ Rotate Right by variable\n-instruct rorL_rReg_Var_ndd(rRegL dst, rRegL src, rcx_RegI shift, rFlagsReg cr)\n-%{\n-  predicate(UseAPX && n->bottom_type()->basic_type() == T_LONG);\n-  match(Set dst (RotateRight src shift));\n-  effect(KILL cr);\n-\n-  format %{ \"erorq    $dst, $src, $shift\\t# rotate right(long ndd)\" %}\n-  ins_encode %{\n-    __ erorq($dst$$Register, $src$$Register, false);\n-  %}\n-  ins_pipe(ialu_reg_reg);\n-%}\n-\n-\/\/----------------------------- CompressBits\/ExpandBits ------------------------\n-\n-instruct compressBitsL_reg(rRegL dst, rRegL src, rRegL mask) %{\n-  predicate(n->bottom_type()->isa_long());\n-  match(Set dst (CompressBits src mask));\n-  format %{ \"pextq  $dst, $src, $mask\\t! parallel bit extract\" %}\n-  ins_encode %{\n-    __ pextq($dst$$Register, $src$$Register, $mask$$Register);\n-  %}\n-  ins_pipe( pipe_slow );\n-%}\n-\n-instruct expandBitsL_reg(rRegL dst, rRegL src, rRegL mask) %{\n-  predicate(n->bottom_type()->isa_long());\n-  match(Set dst (ExpandBits src mask));\n-  format %{ \"pdepq  $dst, $src, $mask\\t! parallel bit deposit\" %}\n-  ins_encode %{\n-    __ pdepq($dst$$Register, $src$$Register, $mask$$Register);\n-  %}\n-  ins_pipe( pipe_slow );\n-%}\n-\n-instruct compressBitsL_mem(rRegL dst, rRegL src, memory mask) %{\n-  predicate(n->bottom_type()->isa_long());\n-  match(Set dst (CompressBits src (LoadL mask)));\n-  format %{ \"pextq  $dst, $src, $mask\\t! parallel bit extract\" %}\n-  ins_encode %{\n-    __ pextq($dst$$Register, $src$$Register, $mask$$Address);\n-  %}\n-  ins_pipe( pipe_slow );\n-%}\n-\n-instruct expandBitsL_mem(rRegL dst, rRegL src, memory mask) %{\n-  predicate(n->bottom_type()->isa_long());\n-  match(Set dst (ExpandBits src (LoadL mask)));\n-  format %{ \"pdepq  $dst, $src, $mask\\t! parallel bit deposit\" %}\n-  ins_encode %{\n-    __ pdepq($dst$$Register, $src$$Register, $mask$$Address);\n-  %}\n-  ins_pipe( pipe_slow );\n-%}\n-\n-\n-\/\/ Logical Instructions\n-\n-\/\/ Integer Logical Instructions\n-\n-\/\/ And Instructions\n-\/\/ And Register with Register\n-instruct andI_rReg(rRegI dst, rRegI src, rFlagsReg cr)\n-%{\n-  predicate(!UseAPX);\n-  match(Set dst (AndI dst src));\n-  effect(KILL cr);\n-  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);\n-\n-  format %{ \"andl    $dst, $src\\t# int\" %}\n-  ins_encode %{\n-    __ andl($dst$$Register, $src$$Register);\n-  %}\n-  ins_pipe(ialu_reg_reg);\n-%}\n-\n-\/\/ And Register with Register using New Data Destination (NDD)\n-instruct andI_rReg_ndd(rRegI dst, rRegI src1, rRegI src2, rFlagsReg cr)\n-%{\n-  predicate(UseAPX);\n-  match(Set dst (AndI src1 src2));\n-  effect(KILL cr);\n-  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);\n-\n-  format %{ \"eandl     $dst, $src1, $src2\\t# int ndd\" %}\n-  ins_encode %{\n-    __ eandl($dst$$Register, $src1$$Register, $src2$$Register, false);\n-\n-  %}\n-  ins_pipe(ialu_reg_reg);\n-%}\n-\n-\/\/ And Register with Immediate 255\n-instruct andI_rReg_imm255(rRegI dst, rRegI src, immI_255 mask)\n-%{\n-  match(Set dst (AndI src mask));\n-\n-  format %{ \"movzbl  $dst, $src\\t# int & 0xFF\" %}\n-  ins_encode %{\n-    __ movzbl($dst$$Register, $src$$Register);\n-  %}\n-  ins_pipe(ialu_reg);\n-%}\n-\n-\/\/ And Register with Immediate 255 and promote to long\n-instruct andI2L_rReg_imm255(rRegL dst, rRegI src, immI_255 mask)\n-%{\n-  match(Set dst (ConvI2L (AndI src mask)));\n-\n-  format %{ \"movzbl  $dst, $src\\t# int & 0xFF -> long\" %}\n-  ins_encode %{\n-    __ movzbl($dst$$Register, $src$$Register);\n-  %}\n-  ins_pipe(ialu_reg);\n-%}\n-\n-\/\/ And Register with Immediate 65535\n-instruct andI_rReg_imm65535(rRegI dst, rRegI src, immI_65535 mask)\n-%{\n-  match(Set dst (AndI src mask));\n-\n-  format %{ \"movzwl  $dst, $src\\t# int & 0xFFFF\" %}\n-  ins_encode %{\n-    __ movzwl($dst$$Register, $src$$Register);\n-  %}\n-  ins_pipe(ialu_reg);\n-%}\n-\n-\/\/ And Register with Immediate 65535 and promote to long\n-instruct andI2L_rReg_imm65535(rRegL dst, rRegI src, immI_65535 mask)\n-%{\n-  match(Set dst (ConvI2L (AndI src mask)));\n-\n-  format %{ \"movzwl  $dst, $src\\t# int & 0xFFFF -> long\" %}\n-  ins_encode %{\n-    __ movzwl($dst$$Register, $src$$Register);\n-  %}\n-  ins_pipe(ialu_reg);\n-%}\n-\n-\/\/ Can skip int2long conversions after AND with small bitmask\n-instruct convI2LAndI_reg_immIbitmask(rRegL dst, rRegI src,  immI_Pow2M1 mask, rRegI tmp, rFlagsReg cr)\n-%{\n-  predicate(VM_Version::supports_bmi2());\n-  ins_cost(125);\n-  effect(TEMP tmp, KILL cr);\n-  match(Set dst (ConvI2L (AndI src mask)));\n-  format %{ \"bzhiq $dst, $src, $mask \\t# using $tmp as TEMP, int &  immI_Pow2M1 -> long\" %}\n-  ins_encode %{\n-    __ movl($tmp$$Register, exact_log2($mask$$constant + 1));\n-    __ bzhiq($dst$$Register, $src$$Register, $tmp$$Register);\n-  %}\n-  ins_pipe(ialu_reg_reg);\n-%}\n-\n-\/\/ And Register with Immediate\n-instruct andI_rReg_imm(rRegI dst, immI src, rFlagsReg cr)\n-%{\n-  predicate(!UseAPX);\n-  match(Set dst (AndI dst src));\n-  effect(KILL cr);\n-  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);\n-\n-  format %{ \"andl    $dst, $src\\t# int\" %}\n-  ins_encode %{\n-    __ andl($dst$$Register, $src$$constant);\n-  %}\n-  ins_pipe(ialu_reg);\n-%}\n-\n-instruct andI_rReg_rReg_imm_ndd(rRegI dst, rRegI src1, immI src2, rFlagsReg cr)\n-%{\n-  predicate(UseAPX);\n-  match(Set dst (AndI src1 src2));\n-  effect(KILL cr);\n-  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);\n-\n-  format %{ \"eandl    $dst, $src1, $src2\\t# int ndd\" %}\n-  ins_encode %{\n-    __ eandl($dst$$Register, $src1$$Register, $src2$$constant, false);\n-  %}\n-  ins_pipe(ialu_reg);\n-%}\n-\n-instruct andI_rReg_mem_imm_ndd(rRegI dst, memory src1, immI src2, rFlagsReg cr)\n-%{\n-  predicate(UseAPX);\n-  match(Set dst (AndI (LoadI src1) src2));\n-  effect(KILL cr);\n-  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);\n-\n-  format %{ \"eandl    $dst, $src1, $src2\\t# int ndd\" %}\n-  ins_encode %{\n-    __ eandl($dst$$Register, $src1$$Address, $src2$$constant, false);\n-  %}\n-  ins_pipe(ialu_reg);\n-%}\n-\n-\/\/ And Register with Memory\n-instruct andI_rReg_mem(rRegI dst, memory src, rFlagsReg cr)\n-%{\n-  predicate(!UseAPX);\n-  match(Set dst (AndI dst (LoadI src)));\n-  effect(KILL cr);\n-  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);\n-\n-  ins_cost(150);\n-  format %{ \"andl    $dst, $src\\t# int\" %}\n-  ins_encode %{\n-    __ andl($dst$$Register, $src$$Address);\n-  %}\n-  ins_pipe(ialu_reg_mem);\n-%}\n-\n-instruct andI_rReg_rReg_mem_ndd(rRegI dst, rRegI src1, memory src2, rFlagsReg cr)\n-%{\n-  predicate(UseAPX);\n-  match(Set dst (AndI src1 (LoadI src2)));\n-  effect(KILL cr);\n-  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);\n-\n-  ins_cost(150);\n-  format %{ \"eandl    $dst, $src1, $src2\\t# int ndd\" %}\n-  ins_encode %{\n-    __ eandl($dst$$Register, $src1$$Register, $src2$$Address, false);\n-  %}\n-  ins_pipe(ialu_reg_mem);\n-%}\n-\n-\/\/ And Memory with Register\n-instruct andB_mem_rReg(memory dst, rRegI src, rFlagsReg cr)\n-%{\n-  match(Set dst (StoreB dst (AndI (LoadB dst) src)));\n-  effect(KILL cr);\n-  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);\n-\n-  ins_cost(150);\n-  format %{ \"andb    $dst, $src\\t# byte\" %}\n-  ins_encode %{\n-    __ andb($dst$$Address, $src$$Register);\n-  %}\n-  ins_pipe(ialu_mem_reg);\n-%}\n-\n-instruct andI_mem_rReg(memory dst, rRegI src, rFlagsReg cr)\n-%{\n-  match(Set dst (StoreI dst (AndI (LoadI dst) src)));\n-  effect(KILL cr);\n-  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);\n-\n-  ins_cost(150);\n-  format %{ \"andl    $dst, $src\\t# int\" %}\n-  ins_encode %{\n-    __ andl($dst$$Address, $src$$Register);\n-  %}\n-  ins_pipe(ialu_mem_reg);\n-%}\n-\n-\/\/ And Memory with Immediate\n-instruct andI_mem_imm(memory dst, immI src, rFlagsReg cr)\n-%{\n-  match(Set dst (StoreI dst (AndI (LoadI dst) src)));\n-  effect(KILL cr);\n-  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);\n-\n-  ins_cost(125);\n-  format %{ \"andl    $dst, $src\\t# int\" %}\n-  ins_encode %{\n-    __ andl($dst$$Address, $src$$constant);\n-  %}\n-  ins_pipe(ialu_mem_imm);\n-%}\n-\n-\/\/ BMI1 instructions\n-instruct andnI_rReg_rReg_mem(rRegI dst, rRegI src1, memory src2, immI_M1 minus_1, rFlagsReg cr) %{\n-  match(Set dst (AndI (XorI src1 minus_1) (LoadI src2)));\n-  predicate(UseBMI1Instructions);\n-  effect(KILL cr);\n-  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);\n-\n-  ins_cost(125);\n-  format %{ \"andnl  $dst, $src1, $src2\" %}\n-\n-  ins_encode %{\n-    __ andnl($dst$$Register, $src1$$Register, $src2$$Address);\n-  %}\n-  ins_pipe(ialu_reg_mem);\n-%}\n-\n-instruct andnI_rReg_rReg_rReg(rRegI dst, rRegI src1, rRegI src2, immI_M1 minus_1, rFlagsReg cr) %{\n-  match(Set dst (AndI (XorI src1 minus_1) src2));\n-  predicate(UseBMI1Instructions);\n-  effect(KILL cr);\n-  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);\n-\n-  format %{ \"andnl  $dst, $src1, $src2\" %}\n-\n-  ins_encode %{\n-    __ andnl($dst$$Register, $src1$$Register, $src2$$Register);\n-  %}\n-  ins_pipe(ialu_reg);\n-%}\n-\n-instruct blsiI_rReg_rReg(rRegI dst, rRegI src, immI_0 imm_zero, rFlagsReg cr) %{\n-  match(Set dst (AndI (SubI imm_zero src) src));\n-  predicate(UseBMI1Instructions);\n-  effect(KILL cr);\n-  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_clears_overflow_flag);\n-\n-  format %{ \"blsil  $dst, $src\" %}\n-\n-  ins_encode %{\n-    __ blsil($dst$$Register, $src$$Register);\n-  %}\n-  ins_pipe(ialu_reg);\n-%}\n-\n-instruct blsiI_rReg_mem(rRegI dst, memory src, immI_0 imm_zero, rFlagsReg cr) %{\n-  match(Set dst (AndI (SubI imm_zero (LoadI src) ) (LoadI src) ));\n-  predicate(UseBMI1Instructions);\n-  effect(KILL cr);\n-  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_clears_overflow_flag);\n-\n-  ins_cost(125);\n-  format %{ \"blsil  $dst, $src\" %}\n-\n-  ins_encode %{\n-    __ blsil($dst$$Register, $src$$Address);\n-  %}\n-  ins_pipe(ialu_reg_mem);\n-%}\n-\n-instruct blsmskI_rReg_mem(rRegI dst, memory src, immI_M1 minus_1, rFlagsReg cr)\n-%{\n-  match(Set dst (XorI (AddI (LoadI src) minus_1) (LoadI src) ) );\n-  predicate(UseBMI1Instructions);\n-  effect(KILL cr);\n-  flag(PD::Flag_sets_sign_flag, PD::Flag_clears_zero_flag, PD::Flag_clears_overflow_flag);\n-\n-  ins_cost(125);\n-  format %{ \"blsmskl $dst, $src\" %}\n-\n-  ins_encode %{\n-    __ blsmskl($dst$$Register, $src$$Address);\n-  %}\n-  ins_pipe(ialu_reg_mem);\n-%}\n-\n-instruct blsmskI_rReg_rReg(rRegI dst, rRegI src, immI_M1 minus_1, rFlagsReg cr)\n-%{\n-  match(Set dst (XorI (AddI src minus_1) src));\n-  predicate(UseBMI1Instructions);\n-  effect(KILL cr);\n-  flag(PD::Flag_sets_sign_flag, PD::Flag_clears_zero_flag, PD::Flag_clears_overflow_flag);\n-\n-  format %{ \"blsmskl $dst, $src\" %}\n-\n-  ins_encode %{\n-    __ blsmskl($dst$$Register, $src$$Register);\n-  %}\n-\n-  ins_pipe(ialu_reg);\n-%}\n-\n-instruct blsrI_rReg_rReg(rRegI dst, rRegI src, immI_M1 minus_1, rFlagsReg cr)\n-%{\n-  match(Set dst (AndI (AddI src minus_1) src) );\n-  predicate(UseBMI1Instructions);\n-  effect(KILL cr);\n-  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_clears_overflow_flag);\n-\n-  format %{ \"blsrl  $dst, $src\" %}\n-\n-  ins_encode %{\n-    __ blsrl($dst$$Register, $src$$Register);\n-  %}\n-\n-  ins_pipe(ialu_reg_mem);\n-%}\n-\n-instruct blsrI_rReg_mem(rRegI dst, memory src, immI_M1 minus_1, rFlagsReg cr)\n-%{\n-  match(Set dst (AndI (AddI (LoadI src) minus_1) (LoadI src) ) );\n-  predicate(UseBMI1Instructions);\n-  effect(KILL cr);\n-  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_clears_overflow_flag);\n-\n-  ins_cost(125);\n-  format %{ \"blsrl  $dst, $src\" %}\n-\n-  ins_encode %{\n-    __ blsrl($dst$$Register, $src$$Address);\n-  %}\n-\n-  ins_pipe(ialu_reg);\n-%}\n-\n-\/\/ Or Instructions\n-\/\/ Or Register with Register\n-instruct orI_rReg(rRegI dst, rRegI src, rFlagsReg cr)\n-%{\n-  predicate(!UseAPX);\n-  match(Set dst (OrI dst src));\n-  effect(KILL cr);\n-  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);\n-\n-  format %{ \"orl     $dst, $src\\t# int\" %}\n-  ins_encode %{\n-    __ orl($dst$$Register, $src$$Register);\n-  %}\n-  ins_pipe(ialu_reg_reg);\n-%}\n-\n-\/\/ Or Register with Register using New Data Destination (NDD)\n-instruct orI_rReg_ndd(rRegI dst, rRegI src1, rRegI src2, rFlagsReg cr)\n-%{\n-  predicate(UseAPX);\n-  match(Set dst (OrI src1 src2));\n-  effect(KILL cr);\n-  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);\n-\n-  format %{ \"eorl     $dst, $src1, $src2\\t# int ndd\" %}\n-  ins_encode %{\n-    __ eorl($dst$$Register, $src1$$Register, $src2$$Register, false);\n-  %}\n-  ins_pipe(ialu_reg_reg);\n-%}\n-\n-\/\/ Or Register with Immediate\n-instruct orI_rReg_imm(rRegI dst, immI src, rFlagsReg cr)\n-%{\n-  predicate(!UseAPX);\n-  match(Set dst (OrI dst src));\n-  effect(KILL cr);\n-  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);\n-\n-  format %{ \"orl     $dst, $src\\t# int\" %}\n-  ins_encode %{\n-    __ orl($dst$$Register, $src$$constant);\n-  %}\n-  ins_pipe(ialu_reg);\n-%}\n-\n-instruct orI_rReg_rReg_imm_ndd(rRegI dst, rRegI src1, immI src2, rFlagsReg cr)\n-%{\n-  predicate(UseAPX);\n-  match(Set dst (OrI src1 src2));\n-  effect(KILL cr);\n-  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);\n-\n-  format %{ \"eorl     $dst, $src1, $src2\\t# int ndd\" %}\n-  ins_encode %{\n-    __ eorl($dst$$Register, $src1$$Register, $src2$$constant, false);\n-  %}\n-  ins_pipe(ialu_reg);\n-%}\n-\n-instruct orI_rReg_imm_rReg_ndd(rRegI dst, immI src1, rRegI src2, rFlagsReg cr)\n-%{\n-  predicate(UseAPX);\n-  match(Set dst (OrI src1 src2));\n-  effect(KILL cr);\n-  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);\n-\n-  format %{ \"eorl     $dst, $src2, $src1\\t# int ndd\" %}\n-  ins_encode %{\n-    __ eorl($dst$$Register, $src2$$Register, $src1$$constant, false);\n-  %}\n-  ins_pipe(ialu_reg);\n-%}\n-\n-instruct orI_rReg_mem_imm_ndd(rRegI dst, memory src1, immI src2, rFlagsReg cr)\n-%{\n-  predicate(UseAPX);\n-  match(Set dst (OrI (LoadI src1) src2));\n-  effect(KILL cr);\n-  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);\n-\n-  format %{ \"eorl     $dst, $src1, $src2\\t# int ndd\" %}\n-  ins_encode %{\n-    __ eorl($dst$$Register, $src1$$Address, $src2$$constant, false);\n-  %}\n-  ins_pipe(ialu_reg);\n-%}\n-\n-\/\/ Or Register with Memory\n-instruct orI_rReg_mem(rRegI dst, memory src, rFlagsReg cr)\n-%{\n-  predicate(!UseAPX);\n-  match(Set dst (OrI dst (LoadI src)));\n-  effect(KILL cr);\n-  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);\n-\n-  ins_cost(150);\n-  format %{ \"orl     $dst, $src\\t# int\" %}\n-  ins_encode %{\n-    __ orl($dst$$Register, $src$$Address);\n-  %}\n-  ins_pipe(ialu_reg_mem);\n-%}\n-\n-instruct orI_rReg_rReg_mem_ndd(rRegI dst, rRegI src1, memory src2, rFlagsReg cr)\n-%{\n-  predicate(UseAPX);\n-  match(Set dst (OrI src1 (LoadI src2)));\n-  effect(KILL cr);\n-  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);\n-\n-  ins_cost(150);\n-  format %{ \"eorl     $dst, $src1, $src2\\t# int ndd\" %}\n-  ins_encode %{\n-    __ eorl($dst$$Register, $src1$$Register, $src2$$Address, false);\n-  %}\n-  ins_pipe(ialu_reg_mem);\n-%}\n-\n-\/\/ Or Memory with Register\n-instruct orB_mem_rReg(memory dst, rRegI src, rFlagsReg cr)\n-%{\n-  match(Set dst (StoreB dst (OrI (LoadB dst) src)));\n-  effect(KILL cr);\n-  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);\n-\n-  ins_cost(150);\n-  format %{ \"orb    $dst, $src\\t# byte\" %}\n-  ins_encode %{\n-    __ orb($dst$$Address, $src$$Register);\n-  %}\n-  ins_pipe(ialu_mem_reg);\n-%}\n-\n-instruct orI_mem_rReg(memory dst, rRegI src, rFlagsReg cr)\n-%{\n-  match(Set dst (StoreI dst (OrI (LoadI dst) src)));\n-  effect(KILL cr);\n-  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);\n-\n-  ins_cost(150);\n-  format %{ \"orl     $dst, $src\\t# int\" %}\n-  ins_encode %{\n-    __ orl($dst$$Address, $src$$Register);\n-  %}\n-  ins_pipe(ialu_mem_reg);\n-%}\n-\n-\/\/ Or Memory with Immediate\n-instruct orI_mem_imm(memory dst, immI src, rFlagsReg cr)\n-%{\n-  match(Set dst (StoreI dst (OrI (LoadI dst) src)));\n-  effect(KILL cr);\n-  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);\n-\n-  ins_cost(125);\n-  format %{ \"orl     $dst, $src\\t# int\" %}\n-  ins_encode %{\n-    __ orl($dst$$Address, $src$$constant);\n-  %}\n-  ins_pipe(ialu_mem_imm);\n-%}\n-\n-\/\/ Xor Instructions\n-\/\/ Xor Register with Register\n-instruct xorI_rReg(rRegI dst, rRegI src, rFlagsReg cr)\n-%{\n-  predicate(!UseAPX);\n-  match(Set dst (XorI dst src));\n-  effect(KILL cr);\n-  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);\n-\n-  format %{ \"xorl    $dst, $src\\t# int\" %}\n-  ins_encode %{\n-    __ xorl($dst$$Register, $src$$Register);\n-  %}\n-  ins_pipe(ialu_reg_reg);\n-%}\n-\n-\/\/ Xor Register with Register using New Data Destination (NDD)\n-instruct xorI_rReg_ndd(rRegI dst, rRegI src1, rRegI src2, rFlagsReg cr)\n-%{\n-  predicate(UseAPX);\n-  match(Set dst (XorI src1 src2));\n-  effect(KILL cr);\n-  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);\n-\n-  format %{ \"exorl    $dst, $src1, $src2\\t# int ndd\" %}\n-  ins_encode %{\n-    __ exorl($dst$$Register, $src1$$Register, $src2$$Register, false);\n-  %}\n-  ins_pipe(ialu_reg_reg);\n-%}\n-\n-\/\/ Xor Register with Immediate -1\n-instruct xorI_rReg_im1(rRegI dst, immI_M1 imm)\n-%{\n-  predicate(!UseAPX);\n-  match(Set dst (XorI dst imm));\n-\n-  format %{ \"notl    $dst\" %}\n-  ins_encode %{\n-     __ notl($dst$$Register);\n-  %}\n-  ins_pipe(ialu_reg);\n-%}\n-\n-instruct xorI_rReg_im1_ndd(rRegI dst, rRegI src, immI_M1 imm)\n-%{\n-  match(Set dst (XorI src imm));\n-  predicate(UseAPX);\n-\n-  format %{ \"enotl    $dst, $src\" %}\n-  ins_encode %{\n-     __ enotl($dst$$Register, $src$$Register);\n-  %}\n-  ins_pipe(ialu_reg);\n-%}\n-\n-\/\/ Xor Register with Immediate\n-instruct xorI_rReg_imm(rRegI dst, immI src, rFlagsReg cr)\n-%{\n-  \/\/ Strict predicate check to make selection of xorI_rReg_im1 cost agnostic if immI src is -1.\n-  predicate(!UseAPX && n->in(2)->bottom_type()->is_int()->get_con() != -1);\n-  match(Set dst (XorI dst src));\n-  effect(KILL cr);\n-  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);\n-\n-  format %{ \"xorl    $dst, $src\\t# int\" %}\n-  ins_encode %{\n-    __ xorl($dst$$Register, $src$$constant);\n-  %}\n-  ins_pipe(ialu_reg);\n-%}\n-\n-instruct xorI_rReg_rReg_imm_ndd(rRegI dst, rRegI src1, immI src2, rFlagsReg cr)\n-%{\n-  \/\/ Strict predicate check to make selection of xorI_rReg_im1_ndd cost agnostic if immI src2 is -1.\n-  predicate(UseAPX && n->in(2)->bottom_type()->is_int()->get_con() != -1);\n-  match(Set dst (XorI src1 src2));\n-  effect(KILL cr);\n-  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);\n-\n-  format %{ \"exorl    $dst, $src1, $src2\\t# int ndd\" %}\n-  ins_encode %{\n-    __ exorl($dst$$Register, $src1$$Register, $src2$$constant, false);\n-  %}\n-  ins_pipe(ialu_reg);\n-%}\n-\n-\/\/ Xor Memory with Immediate\n-instruct xorI_rReg_mem_imm_ndd(rRegI dst, memory src1, immI src2, rFlagsReg cr)\n-%{\n-  predicate(UseAPX);\n-  match(Set dst (XorI (LoadI src1) src2));\n-  effect(KILL cr);\n-  ins_cost(150);\n-  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);\n-\n-  format %{ \"exorl    $dst, $src1, $src2\\t# int ndd\" %}\n-  ins_encode %{\n-    __ exorl($dst$$Register, $src1$$Address, $src2$$constant, false);\n-  %}\n-  ins_pipe(ialu_reg);\n-%}\n-\n-\/\/ Xor Register with Memory\n-instruct xorI_rReg_mem(rRegI dst, memory src, rFlagsReg cr)\n-%{\n-  predicate(!UseAPX);\n-  match(Set dst (XorI dst (LoadI src)));\n-  effect(KILL cr);\n-  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);\n-\n-  ins_cost(150);\n-  format %{ \"xorl    $dst, $src\\t# int\" %}\n-  ins_encode %{\n-    __ xorl($dst$$Register, $src$$Address);\n-  %}\n-  ins_pipe(ialu_reg_mem);\n-%}\n-\n-instruct xorI_rReg_rReg_mem_ndd(rRegI dst, rRegI src1, memory src2, rFlagsReg cr)\n-%{\n-  predicate(UseAPX);\n-  match(Set dst (XorI src1 (LoadI src2)));\n-  effect(KILL cr);\n-  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);\n-\n-  ins_cost(150);\n-  format %{ \"exorl    $dst, $src1, $src2\\t# int ndd\" %}\n-  ins_encode %{\n-    __ exorl($dst$$Register, $src1$$Register, $src2$$Address, false);\n-  %}\n-  ins_pipe(ialu_reg_mem);\n-%}\n-\n-\/\/ Xor Memory with Register\n-instruct xorB_mem_rReg(memory dst, rRegI src, rFlagsReg cr)\n-%{\n-  match(Set dst (StoreB dst (XorI (LoadB dst) src)));\n-  effect(KILL cr);\n-  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);\n-\n-  ins_cost(150);\n-  format %{ \"xorb    $dst, $src\\t# byte\" %}\n-  ins_encode %{\n-    __ xorb($dst$$Address, $src$$Register);\n-  %}\n-  ins_pipe(ialu_mem_reg);\n-%}\n-\n-instruct xorI_mem_rReg(memory dst, rRegI src, rFlagsReg cr)\n-%{\n-  match(Set dst (StoreI dst (XorI (LoadI dst) src)));\n-  effect(KILL cr);\n-  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);\n-\n-  ins_cost(150);\n-  format %{ \"xorl    $dst, $src\\t# int\" %}\n-  ins_encode %{\n-    __ xorl($dst$$Address, $src$$Register);\n-  %}\n-  ins_pipe(ialu_mem_reg);\n-%}\n-\n-\/\/ Xor Memory with Immediate\n-instruct xorI_mem_imm(memory dst, immI src, rFlagsReg cr)\n-%{\n-  match(Set dst (StoreI dst (XorI (LoadI dst) src)));\n-  effect(KILL cr);\n-  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);\n-\n-  ins_cost(125);\n-  format %{ \"xorl    $dst, $src\\t# int\" %}\n-  ins_encode %{\n-    __ xorl($dst$$Address, $src$$constant);\n-  %}\n-  ins_pipe(ialu_mem_imm);\n-%}\n-\n-\n-\/\/ Long Logical Instructions\n-\n-\/\/ And Instructions\n-\/\/ And Register with Register\n-instruct andL_rReg(rRegL dst, rRegL src, rFlagsReg cr)\n-%{\n-  predicate(!UseAPX);\n-  match(Set dst (AndL dst src));\n-  effect(KILL cr);\n-  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);\n-\n-  format %{ \"andq    $dst, $src\\t# long\" %}\n-  ins_encode %{\n-    __ andq($dst$$Register, $src$$Register);\n-  %}\n-  ins_pipe(ialu_reg_reg);\n-%}\n-\n-\/\/ And Register with Register using New Data Destination (NDD)\n-instruct andL_rReg_ndd(rRegL dst, rRegL src1, rRegL src2, rFlagsReg cr)\n-%{\n-  predicate(UseAPX);\n-  match(Set dst (AndL src1 src2));\n-  effect(KILL cr);\n-  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);\n-\n-  format %{ \"eandq     $dst, $src1, $src2\\t# long ndd\" %}\n-  ins_encode %{\n-    __ eandq($dst$$Register, $src1$$Register, $src2$$Register, false);\n-\n-  %}\n-  ins_pipe(ialu_reg_reg);\n-%}\n-\n-\/\/ And Register with Immediate 255\n-instruct andL_rReg_imm255(rRegL dst, rRegL src, immL_255 mask)\n-%{\n-  match(Set dst (AndL src mask));\n-\n-  format %{ \"movzbl  $dst, $src\\t# long & 0xFF\" %}\n-  ins_encode %{\n-    \/\/ movzbl zeroes out the upper 32-bit and does not need REX.W\n-    __ movzbl($dst$$Register, $src$$Register);\n-  %}\n-  ins_pipe(ialu_reg);\n-%}\n-\n-\/\/ And Register with Immediate 65535\n-instruct andL_rReg_imm65535(rRegL dst, rRegL src, immL_65535 mask)\n-%{\n-  match(Set dst (AndL src mask));\n-\n-  format %{ \"movzwl  $dst, $src\\t# long & 0xFFFF\" %}\n-  ins_encode %{\n-    \/\/ movzwl zeroes out the upper 32-bit and does not need REX.W\n-    __ movzwl($dst$$Register, $src$$Register);\n-  %}\n-  ins_pipe(ialu_reg);\n-%}\n-\n-\/\/ And Register with Immediate\n-instruct andL_rReg_imm(rRegL dst, immL32 src, rFlagsReg cr)\n-%{\n-  predicate(!UseAPX);\n-  match(Set dst (AndL dst src));\n-  effect(KILL cr);\n-  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);\n-\n-  format %{ \"andq    $dst, $src\\t# long\" %}\n-  ins_encode %{\n-    __ andq($dst$$Register, $src$$constant);\n-  %}\n-  ins_pipe(ialu_reg);\n-%}\n-\n-instruct andL_rReg_rReg_imm_ndd(rRegL dst, rRegL src1, immL32 src2, rFlagsReg cr)\n-%{\n-  predicate(UseAPX);\n-  match(Set dst (AndL src1 src2));\n-  effect(KILL cr);\n-  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);\n-\n-  format %{ \"eandq    $dst, $src1, $src2\\t# long ndd\" %}\n-  ins_encode %{\n-    __ eandq($dst$$Register, $src1$$Register, $src2$$constant, false);\n-  %}\n-  ins_pipe(ialu_reg);\n-%}\n-\n-instruct andL_rReg_mem_imm_ndd(rRegL dst, memory src1, immL32 src2, rFlagsReg cr)\n-%{\n-  predicate(UseAPX);\n-  match(Set dst (AndL (LoadL src1) src2));\n-  effect(KILL cr);\n-  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);\n-\n-  format %{ \"eandq    $dst, $src1, $src2\\t# long ndd\" %}\n-  ins_encode %{\n-    __ eandq($dst$$Register, $src1$$Address, $src2$$constant, false);\n-  %}\n-  ins_pipe(ialu_reg);\n-%}\n-\n-\/\/ And Register with Memory\n-instruct andL_rReg_mem(rRegL dst, memory src, rFlagsReg cr)\n-%{\n-  predicate(!UseAPX);\n-  match(Set dst (AndL dst (LoadL src)));\n-  effect(KILL cr);\n-  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);\n-\n-  ins_cost(150);\n-  format %{ \"andq    $dst, $src\\t# long\" %}\n-  ins_encode %{\n-    __ andq($dst$$Register, $src$$Address);\n-  %}\n-  ins_pipe(ialu_reg_mem);\n-%}\n-\n-instruct andL_rReg_rReg_mem_ndd(rRegL dst, rRegL src1, memory src2, rFlagsReg cr)\n-%{\n-  predicate(UseAPX);\n-  match(Set dst (AndL src1 (LoadL src2)));\n-  effect(KILL cr);\n-  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);\n-\n-  ins_cost(150);\n-  format %{ \"eandq    $dst, $src1, $src2\\t# long ndd\" %}\n-  ins_encode %{\n-    __ eandq($dst$$Register, $src1$$Register, $src2$$Address, false);\n-  %}\n-  ins_pipe(ialu_reg_mem);\n-%}\n-\n-\/\/ And Memory with Register\n-instruct andL_mem_rReg(memory dst, rRegL src, rFlagsReg cr)\n-%{\n-  match(Set dst (StoreL dst (AndL (LoadL dst) src)));\n-  effect(KILL cr);\n-  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);\n-\n-  ins_cost(150);\n-  format %{ \"andq    $dst, $src\\t# long\" %}\n-  ins_encode %{\n-    __ andq($dst$$Address, $src$$Register);\n-  %}\n-  ins_pipe(ialu_mem_reg);\n-%}\n-\n-\/\/ And Memory with Immediate\n-instruct andL_mem_imm(memory dst, immL32 src, rFlagsReg cr)\n-%{\n-  match(Set dst (StoreL dst (AndL (LoadL dst) src)));\n-  effect(KILL cr);\n-  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);\n-\n-  ins_cost(125);\n-  format %{ \"andq    $dst, $src\\t# long\" %}\n-  ins_encode %{\n-    __ andq($dst$$Address, $src$$constant);\n-  %}\n-  ins_pipe(ialu_mem_imm);\n-%}\n-\n-instruct btrL_mem_imm(memory dst, immL_NotPow2 con, rFlagsReg cr)\n-%{\n-  \/\/ con should be a pure 64-bit immediate given that not(con) is a power of 2\n-  \/\/ because AND\/OR works well enough for 8\/32-bit values.\n-  predicate(log2i_graceful(~n->in(3)->in(2)->get_long()) > 30);\n-\n-  match(Set dst (StoreL dst (AndL (LoadL dst) con)));\n-  effect(KILL cr);\n-\n-  ins_cost(125);\n-  format %{ \"btrq    $dst, log2(not($con))\\t# long\" %}\n-  ins_encode %{\n-    __ btrq($dst$$Address, log2i_exact((julong)~$con$$constant));\n-  %}\n-  ins_pipe(ialu_mem_imm);\n-%}\n-\n-\/\/ BMI1 instructions\n-instruct andnL_rReg_rReg_mem(rRegL dst, rRegL src1, memory src2, immL_M1 minus_1, rFlagsReg cr) %{\n-  match(Set dst (AndL (XorL src1 minus_1) (LoadL src2)));\n-  predicate(UseBMI1Instructions);\n-  effect(KILL cr);\n-  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);\n-\n-  ins_cost(125);\n-  format %{ \"andnq  $dst, $src1, $src2\" %}\n-\n-  ins_encode %{\n-    __ andnq($dst$$Register, $src1$$Register, $src2$$Address);\n-  %}\n-  ins_pipe(ialu_reg_mem);\n-%}\n-\n-instruct andnL_rReg_rReg_rReg(rRegL dst, rRegL src1, rRegL src2, immL_M1 minus_1, rFlagsReg cr) %{\n-  match(Set dst (AndL (XorL src1 minus_1) src2));\n-  predicate(UseBMI1Instructions);\n-  effect(KILL cr);\n-  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);\n-\n-  format %{ \"andnq  $dst, $src1, $src2\" %}\n-\n-  ins_encode %{\n-  __ andnq($dst$$Register, $src1$$Register, $src2$$Register);\n-  %}\n-  ins_pipe(ialu_reg_mem);\n-%}\n-\n-instruct blsiL_rReg_rReg(rRegL dst, rRegL src, immL0 imm_zero, rFlagsReg cr) %{\n-  match(Set dst (AndL (SubL imm_zero src) src));\n-  predicate(UseBMI1Instructions);\n-  effect(KILL cr);\n-  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_clears_overflow_flag);\n-\n-  format %{ \"blsiq  $dst, $src\" %}\n-\n-  ins_encode %{\n-    __ blsiq($dst$$Register, $src$$Register);\n-  %}\n-  ins_pipe(ialu_reg);\n-%}\n-\n-instruct blsiL_rReg_mem(rRegL dst, memory src, immL0 imm_zero, rFlagsReg cr) %{\n-  match(Set dst (AndL (SubL imm_zero (LoadL src) ) (LoadL src) ));\n-  predicate(UseBMI1Instructions);\n-  effect(KILL cr);\n-  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_clears_overflow_flag);\n-\n-  ins_cost(125);\n-  format %{ \"blsiq  $dst, $src\" %}\n-\n-  ins_encode %{\n-    __ blsiq($dst$$Register, $src$$Address);\n-  %}\n-  ins_pipe(ialu_reg_mem);\n-%}\n-\n-instruct blsmskL_rReg_mem(rRegL dst, memory src, immL_M1 minus_1, rFlagsReg cr)\n-%{\n-  match(Set dst (XorL (AddL (LoadL src) minus_1) (LoadL src) ) );\n-  predicate(UseBMI1Instructions);\n-  effect(KILL cr);\n-  flag(PD::Flag_sets_sign_flag, PD::Flag_clears_zero_flag, PD::Flag_clears_overflow_flag);\n-\n-  ins_cost(125);\n-  format %{ \"blsmskq $dst, $src\" %}\n-\n-  ins_encode %{\n-    __ blsmskq($dst$$Register, $src$$Address);\n-  %}\n-  ins_pipe(ialu_reg_mem);\n-%}\n-\n-instruct blsmskL_rReg_rReg(rRegL dst, rRegL src, immL_M1 minus_1, rFlagsReg cr)\n-%{\n-  match(Set dst (XorL (AddL src minus_1) src));\n-  predicate(UseBMI1Instructions);\n-  effect(KILL cr);\n-  flag(PD::Flag_sets_sign_flag, PD::Flag_clears_zero_flag, PD::Flag_clears_overflow_flag);\n-\n-  format %{ \"blsmskq $dst, $src\" %}\n-\n-  ins_encode %{\n-    __ blsmskq($dst$$Register, $src$$Register);\n-  %}\n-\n-  ins_pipe(ialu_reg);\n-%}\n-\n-instruct blsrL_rReg_rReg(rRegL dst, rRegL src, immL_M1 minus_1, rFlagsReg cr)\n-%{\n-  match(Set dst (AndL (AddL src minus_1) src) );\n-  predicate(UseBMI1Instructions);\n-  effect(KILL cr);\n-  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_clears_overflow_flag);\n-\n-  format %{ \"blsrq  $dst, $src\" %}\n-\n-  ins_encode %{\n-    __ blsrq($dst$$Register, $src$$Register);\n-  %}\n-\n-  ins_pipe(ialu_reg);\n-%}\n-\n-instruct blsrL_rReg_mem(rRegL dst, memory src, immL_M1 minus_1, rFlagsReg cr)\n-%{\n-  match(Set dst (AndL (AddL (LoadL src) minus_1) (LoadL src)) );\n-  predicate(UseBMI1Instructions);\n-  effect(KILL cr);\n-  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_clears_overflow_flag);\n-\n-  ins_cost(125);\n-  format %{ \"blsrq  $dst, $src\" %}\n-\n-  ins_encode %{\n-    __ blsrq($dst$$Register, $src$$Address);\n-  %}\n-\n-  ins_pipe(ialu_reg);\n-%}\n-\n-\/\/ Or Instructions\n-\/\/ Or Register with Register\n-instruct orL_rReg(rRegL dst, rRegL src, rFlagsReg cr)\n-%{\n-  predicate(!UseAPX);\n-  match(Set dst (OrL dst src));\n-  effect(KILL cr);\n-  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);\n-\n-  format %{ \"orq     $dst, $src\\t# long\" %}\n-  ins_encode %{\n-    __ orq($dst$$Register, $src$$Register);\n-  %}\n-  ins_pipe(ialu_reg_reg);\n-%}\n-\n-\/\/ Or Register with Register using New Data Destination (NDD)\n-instruct orL_rReg_ndd(rRegL dst, rRegL src1, rRegL src2, rFlagsReg cr)\n-%{\n-  predicate(UseAPX);\n-  match(Set dst (OrL src1 src2));\n-  effect(KILL cr);\n-  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);\n-\n-  format %{ \"eorq     $dst, $src1, $src2\\t# long ndd\" %}\n-  ins_encode %{\n-    __ eorq($dst$$Register, $src1$$Register, $src2$$Register, false);\n-\n-  %}\n-  ins_pipe(ialu_reg_reg);\n-%}\n-\n-\/\/ Use any_RegP to match R15 (TLS register) without spilling.\n-instruct orL_rReg_castP2X(rRegL dst, any_RegP src, rFlagsReg cr) %{\n-  match(Set dst (OrL dst (CastP2X src)));\n-  effect(KILL cr);\n-  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);\n-\n-  format %{ \"orq     $dst, $src\\t# long\" %}\n-  ins_encode %{\n-    __ orq($dst$$Register, $src$$Register);\n-  %}\n-  ins_pipe(ialu_reg_reg);\n-%}\n-\n-instruct orL_rReg_castP2X_ndd(rRegL dst, any_RegP src1, any_RegP src2, rFlagsReg cr) %{\n-  match(Set dst (OrL src1 (CastP2X src2)));\n-  effect(KILL cr);\n-  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);\n-\n-  format %{ \"eorq     $dst, $src1, $src2\\t# long ndd\" %}\n-  ins_encode %{\n-    __ eorq($dst$$Register, $src1$$Register, $src2$$Register, false);\n-  %}\n-  ins_pipe(ialu_reg_reg);\n-%}\n-\n-\/\/ Or Register with Immediate\n-instruct orL_rReg_imm(rRegL dst, immL32 src, rFlagsReg cr)\n-%{\n-  predicate(!UseAPX);\n-  match(Set dst (OrL dst src));\n-  effect(KILL cr);\n-  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);\n-\n-  format %{ \"orq     $dst, $src\\t# long\" %}\n-  ins_encode %{\n-    __ orq($dst$$Register, $src$$constant);\n-  %}\n-  ins_pipe(ialu_reg);\n-%}\n-\n-instruct orL_rReg_rReg_imm_ndd(rRegL dst, rRegL src1, immL32 src2, rFlagsReg cr)\n-%{\n-  predicate(UseAPX);\n-  match(Set dst (OrL src1 src2));\n-  effect(KILL cr);\n-  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);\n-\n-  format %{ \"eorq     $dst, $src1, $src2\\t# long ndd\" %}\n-  ins_encode %{\n-    __ eorq($dst$$Register, $src1$$Register, $src2$$constant, false);\n-  %}\n-  ins_pipe(ialu_reg);\n-%}\n-\n-instruct orL_rReg_imm_rReg_ndd(rRegL dst, immL32 src1, rRegL src2, rFlagsReg cr)\n-%{\n-  predicate(UseAPX);\n-  match(Set dst (OrL src1 src2));\n-  effect(KILL cr);\n-  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);\n-\n-  format %{ \"eorq     $dst, $src2, $src1\\t# long ndd\" %}\n-  ins_encode %{\n-    __ eorq($dst$$Register, $src2$$Register, $src1$$constant, false);\n-  %}\n-  ins_pipe(ialu_reg);\n-%}\n-\n-\/\/ Or Memory with Immediate\n-instruct orL_rReg_mem_imm_ndd(rRegL dst, memory src1, immL32 src2, rFlagsReg cr)\n-%{\n-  predicate(UseAPX);\n-  match(Set dst (OrL (LoadL src1) src2));\n-  effect(KILL cr);\n-  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);\n-\n-  format %{ \"eorq     $dst, $src1, $src2\\t# long ndd\" %}\n-  ins_encode %{\n-    __ eorq($dst$$Register, $src1$$Address, $src2$$constant, false);\n-  %}\n-  ins_pipe(ialu_reg);\n-%}\n-\n-\/\/ Or Register with Memory\n-instruct orL_rReg_mem(rRegL dst, memory src, rFlagsReg cr)\n-%{\n-  predicate(!UseAPX);\n-  match(Set dst (OrL dst (LoadL src)));\n-  effect(KILL cr);\n-  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);\n-\n-  ins_cost(150);\n-  format %{ \"orq     $dst, $src\\t# long\" %}\n-  ins_encode %{\n-    __ orq($dst$$Register, $src$$Address);\n-  %}\n-  ins_pipe(ialu_reg_mem);\n-%}\n-\n-instruct orL_rReg_rReg_mem_ndd(rRegL dst, rRegL src1, memory src2, rFlagsReg cr)\n-%{\n-  predicate(UseAPX);\n-  match(Set dst (OrL src1 (LoadL src2)));\n-  effect(KILL cr);\n-  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);\n-\n-  ins_cost(150);\n-  format %{ \"eorq     $dst, $src1, $src2\\t# long ndd\" %}\n-  ins_encode %{\n-    __ eorq($dst$$Register, $src1$$Register, $src2$$Address, false);\n-  %}\n-  ins_pipe(ialu_reg_mem);\n-%}\n-\n-\/\/ Or Memory with Register\n-instruct orL_mem_rReg(memory dst, rRegL src, rFlagsReg cr)\n-%{\n-  match(Set dst (StoreL dst (OrL (LoadL dst) src)));\n-  effect(KILL cr);\n-  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);\n-\n-  ins_cost(150);\n-  format %{ \"orq     $dst, $src\\t# long\" %}\n-  ins_encode %{\n-    __ orq($dst$$Address, $src$$Register);\n-  %}\n-  ins_pipe(ialu_mem_reg);\n-%}\n-\n-\/\/ Or Memory with Immediate\n-instruct orL_mem_imm(memory dst, immL32 src, rFlagsReg cr)\n-%{\n-  match(Set dst (StoreL dst (OrL (LoadL dst) src)));\n-  effect(KILL cr);\n-  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);\n-\n-  ins_cost(125);\n-  format %{ \"orq     $dst, $src\\t# long\" %}\n-  ins_encode %{\n-    __ orq($dst$$Address, $src$$constant);\n-  %}\n-  ins_pipe(ialu_mem_imm);\n-%}\n-\n-instruct btsL_mem_imm(memory dst, immL_Pow2 con, rFlagsReg cr)\n-%{\n-  \/\/ con should be a pure 64-bit power of 2 immediate\n-  \/\/ because AND\/OR works well enough for 8\/32-bit values.\n-  predicate(log2i_graceful(n->in(3)->in(2)->get_long()) > 31);\n-\n-  match(Set dst (StoreL dst (OrL (LoadL dst) con)));\n-  effect(KILL cr);\n-\n-  ins_cost(125);\n-  format %{ \"btsq    $dst, log2($con)\\t# long\" %}\n-  ins_encode %{\n-    __ btsq($dst$$Address, log2i_exact((julong)$con$$constant));\n-  %}\n-  ins_pipe(ialu_mem_imm);\n-%}\n-\n-\/\/ Xor Instructions\n-\/\/ Xor Register with Register\n-instruct xorL_rReg(rRegL dst, rRegL src, rFlagsReg cr)\n-%{\n-  predicate(!UseAPX);\n-  match(Set dst (XorL dst src));\n-  effect(KILL cr);\n-  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);\n-\n-  format %{ \"xorq    $dst, $src\\t# long\" %}\n-  ins_encode %{\n-    __ xorq($dst$$Register, $src$$Register);\n-  %}\n-  ins_pipe(ialu_reg_reg);\n-%}\n-\n-\/\/ Xor Register with Register using New Data Destination (NDD)\n-instruct xorL_rReg_ndd(rRegL dst, rRegL src1, rRegL src2, rFlagsReg cr)\n-%{\n-  predicate(UseAPX);\n-  match(Set dst (XorL src1 src2));\n-  effect(KILL cr);\n-  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);\n-\n-  format %{ \"exorq    $dst, $src1, $src2\\t# long ndd\" %}\n-  ins_encode %{\n-    __ exorq($dst$$Register, $src1$$Register, $src2$$Register, false);\n-  %}\n-  ins_pipe(ialu_reg_reg);\n-%}\n-\n-\/\/ Xor Register with Immediate -1\n-instruct xorL_rReg_im1(rRegL dst, immL_M1 imm)\n-%{\n-  predicate(!UseAPX);\n-  match(Set dst (XorL dst imm));\n-\n-  format %{ \"notq   $dst\" %}\n-  ins_encode %{\n-     __ notq($dst$$Register);\n-  %}\n-  ins_pipe(ialu_reg);\n-%}\n-\n-instruct xorL_rReg_im1_ndd(rRegL dst,rRegL src, immL_M1 imm)\n-%{\n-  predicate(UseAPX);\n-  match(Set dst (XorL src imm));\n-\n-  format %{ \"enotq   $dst, $src\" %}\n-  ins_encode %{\n-    __ enotq($dst$$Register, $src$$Register);\n-  %}\n-  ins_pipe(ialu_reg);\n-%}\n-\n-\/\/ Xor Register with Immediate\n-instruct xorL_rReg_imm(rRegL dst, immL32 src, rFlagsReg cr)\n-%{\n-  \/\/ Strict predicate check to make selection of xorL_rReg_im1 cost agnostic if immL32 src is -1.\n-  predicate(!UseAPX && n->in(2)->bottom_type()->is_long()->get_con() != -1L);\n-  match(Set dst (XorL dst src));\n-  effect(KILL cr);\n-  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);\n-\n-  format %{ \"xorq    $dst, $src\\t# long\" %}\n-  ins_encode %{\n-    __ xorq($dst$$Register, $src$$constant);\n-  %}\n-  ins_pipe(ialu_reg);\n-%}\n-\n-instruct xorL_rReg_rReg_imm(rRegL dst, rRegL src1, immL32 src2, rFlagsReg cr)\n-%{\n-  \/\/ Strict predicate check to make selection of xorL_rReg_im1_ndd cost agnostic if immL32 src2 is -1.\n-  predicate(UseAPX && n->in(2)->bottom_type()->is_long()->get_con() != -1L);\n-  match(Set dst (XorL src1 src2));\n-  effect(KILL cr);\n-  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);\n-\n-  format %{ \"exorq    $dst, $src1, $src2\\t# long ndd\" %}\n-  ins_encode %{\n-    __ exorq($dst$$Register, $src1$$Register, $src2$$constant, false);\n-  %}\n-  ins_pipe(ialu_reg);\n-%}\n-\n-\/\/ Xor Memory with Immediate\n-instruct xorL_rReg_mem_imm(rRegL dst, memory src1, immL32 src2, rFlagsReg cr)\n-%{\n-  predicate(UseAPX);\n-  match(Set dst (XorL (LoadL src1) src2));\n-  effect(KILL cr);\n-  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);\n-  ins_cost(150);\n-\n-  format %{ \"exorq    $dst, $src1, $src2\\t# long ndd\" %}\n-  ins_encode %{\n-    __ exorq($dst$$Register, $src1$$Address, $src2$$constant, false);\n-  %}\n-  ins_pipe(ialu_reg);\n-%}\n-\n-\/\/ Xor Register with Memory\n-instruct xorL_rReg_mem(rRegL dst, memory src, rFlagsReg cr)\n-%{\n-  predicate(!UseAPX);\n-  match(Set dst (XorL dst (LoadL src)));\n-  effect(KILL cr);\n-  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);\n-\n-  ins_cost(150);\n-  format %{ \"xorq    $dst, $src\\t# long\" %}\n-  ins_encode %{\n-    __ xorq($dst$$Register, $src$$Address);\n-  %}\n-  ins_pipe(ialu_reg_mem);\n-%}\n-\n-instruct xorL_rReg_rReg_mem_ndd(rRegL dst, rRegL src1, memory src2, rFlagsReg cr)\n-%{\n-  predicate(UseAPX);\n-  match(Set dst (XorL src1 (LoadL src2)));\n-  effect(KILL cr);\n-  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);\n-\n-  ins_cost(150);\n-  format %{ \"exorq    $dst, $src1, $src2\\t# long ndd\" %}\n-  ins_encode %{\n-    __ exorq($dst$$Register, $src1$$Register, $src2$$Address, false);\n-  %}\n-  ins_pipe(ialu_reg_mem);\n-%}\n-\n-\/\/ Xor Memory with Register\n-instruct xorL_mem_rReg(memory dst, rRegL src, rFlagsReg cr)\n-%{\n-  match(Set dst (StoreL dst (XorL (LoadL dst) src)));\n-  effect(KILL cr);\n-  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);\n-\n-  ins_cost(150);\n-  format %{ \"xorq    $dst, $src\\t# long\" %}\n-  ins_encode %{\n-    __ xorq($dst$$Address, $src$$Register);\n-  %}\n-  ins_pipe(ialu_mem_reg);\n-%}\n-\n-\/\/ Xor Memory with Immediate\n-instruct xorL_mem_imm(memory dst, immL32 src, rFlagsReg cr)\n-%{\n-  match(Set dst (StoreL dst (XorL (LoadL dst) src)));\n-  effect(KILL cr);\n-  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);\n-\n-  ins_cost(125);\n-  format %{ \"xorq    $dst, $src\\t# long\" %}\n-  ins_encode %{\n-    __ xorq($dst$$Address, $src$$constant);\n-  %}\n-  ins_pipe(ialu_mem_imm);\n-%}\n-\n-instruct cmpLTMask(rRegI dst, rRegI p, rRegI q, rFlagsReg cr)\n-%{\n-  match(Set dst (CmpLTMask p q));\n-  effect(KILL cr);\n-\n-  ins_cost(400);\n-  format %{ \"cmpl    $p, $q\\t# cmpLTMask\\n\\t\"\n-            \"setcc   $dst \\t# emits setlt + movzbl or setzul for APX\"\n-            \"negl    $dst\" %}\n-  ins_encode %{\n-    __ cmpl($p$$Register, $q$$Register);\n-    __ setcc(Assembler::less, $dst$$Register);\n-    __ negl($dst$$Register);\n-  %}\n-  ins_pipe(pipe_slow);\n-%}\n-\n-instruct cmpLTMask0(rRegI dst, immI_0 zero, rFlagsReg cr)\n-%{\n-  match(Set dst (CmpLTMask dst zero));\n-  effect(KILL cr);\n-\n-  ins_cost(100);\n-  format %{ \"sarl    $dst, #31\\t# cmpLTMask0\" %}\n-  ins_encode %{\n-    __ sarl($dst$$Register, 31);\n-  %}\n-  ins_pipe(ialu_reg);\n-%}\n-\n-\/* Better to save a register than avoid a branch *\/\n-instruct cadd_cmpLTMask(rRegI p, rRegI q, rRegI y, rFlagsReg cr)\n-%{\n-  match(Set p (AddI (AndI (CmpLTMask p q) y) (SubI p q)));\n-  effect(KILL cr);\n-  ins_cost(300);\n-  format %{ \"subl    $p,$q\\t# cadd_cmpLTMask\\n\\t\"\n-            \"jge     done\\n\\t\"\n-            \"addl    $p,$y\\n\"\n-            \"done:   \" %}\n-  ins_encode %{\n-    Register Rp = $p$$Register;\n-    Register Rq = $q$$Register;\n-    Register Ry = $y$$Register;\n-    Label done;\n-    __ subl(Rp, Rq);\n-    __ jccb(Assembler::greaterEqual, done);\n-    __ addl(Rp, Ry);\n-    __ bind(done);\n-  %}\n-  ins_pipe(pipe_cmplt);\n-%}\n-\n-\/* Better to save a register than avoid a branch *\/\n-instruct and_cmpLTMask(rRegI p, rRegI q, rRegI y, rFlagsReg cr)\n-%{\n-  match(Set y (AndI (CmpLTMask p q) y));\n-  effect(KILL cr);\n-\n-  ins_cost(300);\n-\n-  format %{ \"cmpl    $p, $q\\t# and_cmpLTMask\\n\\t\"\n-            \"jlt     done\\n\\t\"\n-            \"xorl    $y, $y\\n\"\n-            \"done:   \" %}\n-  ins_encode %{\n-    Register Rp = $p$$Register;\n-    Register Rq = $q$$Register;\n-    Register Ry = $y$$Register;\n-    Label done;\n-    __ cmpl(Rp, Rq);\n-    __ jccb(Assembler::less, done);\n-    __ xorl(Ry, Ry);\n-    __ bind(done);\n-  %}\n-  ins_pipe(pipe_cmplt);\n-%}\n-\n-\n-\/\/---------- FP Instructions------------------------------------------------\n-\n-\/\/ Really expensive, avoid\n-instruct cmpF_cc_reg(rFlagsRegU cr, regF src1, regF src2)\n-%{\n-  match(Set cr (CmpF src1 src2));\n-\n-  ins_cost(500);\n-  format %{ \"ucomiss $src1, $src2\\n\\t\"\n-            \"jnp,s   exit\\n\\t\"\n-            \"pushfq\\t# saw NaN, set CF\\n\\t\"\n-            \"andq    [rsp], #0xffffff2b\\n\\t\"\n-            \"popfq\\n\"\n-    \"exit:\" %}\n-  ins_encode %{\n-    __ ucomiss($src1$$XMMRegister, $src2$$XMMRegister);\n-    emit_cmpfp_fixup(masm);\n-  %}\n-  ins_pipe(pipe_slow);\n-%}\n-\n-instruct cmpF_cc_reg_CF(rFlagsRegUCF cr, regF src1, regF src2) %{\n-  match(Set cr (CmpF src1 src2));\n-\n-  ins_cost(100);\n-  format %{ \"ucomiss $src1, $src2\" %}\n-  ins_encode %{\n-    __ ucomiss($src1$$XMMRegister, $src2$$XMMRegister);\n-  %}\n-  ins_pipe(pipe_slow);\n-%}\n-\n-instruct cmpF_cc_memCF(rFlagsRegUCF cr, regF src1, memory src2) %{\n-  match(Set cr (CmpF src1 (LoadF src2)));\n-\n-  ins_cost(100);\n-  format %{ \"ucomiss $src1, $src2\" %}\n-  ins_encode %{\n-    __ ucomiss($src1$$XMMRegister, $src2$$Address);\n-  %}\n-  ins_pipe(pipe_slow);\n-%}\n-\n-instruct cmpF_cc_immCF(rFlagsRegUCF cr, regF src, immF con) %{\n-  match(Set cr (CmpF src con));\n-  ins_cost(100);\n-  format %{ \"ucomiss $src, [$constantaddress]\\t# load from constant table: float=$con\" %}\n-  ins_encode %{\n-    __ ucomiss($src$$XMMRegister, $constantaddress($con));\n-  %}\n-  ins_pipe(pipe_slow);\n-%}\n-\n-\/\/ Really expensive, avoid\n-instruct cmpD_cc_reg(rFlagsRegU cr, regD src1, regD src2)\n-%{\n-  match(Set cr (CmpD src1 src2));\n-\n-  ins_cost(500);\n-  format %{ \"ucomisd $src1, $src2\\n\\t\"\n-            \"jnp,s   exit\\n\\t\"\n-            \"pushfq\\t# saw NaN, set CF\\n\\t\"\n-            \"andq    [rsp], #0xffffff2b\\n\\t\"\n-            \"popfq\\n\"\n-    \"exit:\" %}\n-  ins_encode %{\n-    __ ucomisd($src1$$XMMRegister, $src2$$XMMRegister);\n-    emit_cmpfp_fixup(masm);\n-  %}\n-  ins_pipe(pipe_slow);\n-%}\n-\n-instruct cmpD_cc_reg_CF(rFlagsRegUCF cr, regD src1, regD src2) %{\n-  match(Set cr (CmpD src1 src2));\n-\n-  ins_cost(100);\n-  format %{ \"ucomisd $src1, $src2 test\" %}\n-  ins_encode %{\n-    __ ucomisd($src1$$XMMRegister, $src2$$XMMRegister);\n-  %}\n-  ins_pipe(pipe_slow);\n-%}\n-\n-instruct cmpD_cc_memCF(rFlagsRegUCF cr, regD src1, memory src2) %{\n-  match(Set cr (CmpD src1 (LoadD src2)));\n-\n-  ins_cost(100);\n-  format %{ \"ucomisd $src1, $src2\" %}\n-  ins_encode %{\n-    __ ucomisd($src1$$XMMRegister, $src2$$Address);\n-  %}\n-  ins_pipe(pipe_slow);\n-%}\n-\n-instruct cmpD_cc_immCF(rFlagsRegUCF cr, regD src, immD con) %{\n-  match(Set cr (CmpD src con));\n-  ins_cost(100);\n-  format %{ \"ucomisd $src, [$constantaddress]\\t# load from constant table: double=$con\" %}\n-  ins_encode %{\n-    __ ucomisd($src$$XMMRegister, $constantaddress($con));\n-  %}\n-  ins_pipe(pipe_slow);\n-%}\n-\n-\/\/ Compare into -1,0,1\n-instruct cmpF_reg(rRegI dst, regF src1, regF src2, rFlagsReg cr)\n-%{\n-  match(Set dst (CmpF3 src1 src2));\n-  effect(KILL cr);\n-\n-  ins_cost(275);\n-  format %{ \"ucomiss $src1, $src2\\n\\t\"\n-            \"movl    $dst, #-1\\n\\t\"\n-            \"jp,s    done\\n\\t\"\n-            \"jb,s    done\\n\\t\"\n-            \"setne   $dst\\n\\t\"\n-            \"movzbl  $dst, $dst\\n\"\n-    \"done:\" %}\n-  ins_encode %{\n-    __ ucomiss($src1$$XMMRegister, $src2$$XMMRegister);\n-    emit_cmpfp3(masm, $dst$$Register);\n-  %}\n-  ins_pipe(pipe_slow);\n-%}\n-\n-\/\/ Compare into -1,0,1\n-instruct cmpF_mem(rRegI dst, regF src1, memory src2, rFlagsReg cr)\n-%{\n-  match(Set dst (CmpF3 src1 (LoadF src2)));\n-  effect(KILL cr);\n-\n-  ins_cost(275);\n-  format %{ \"ucomiss $src1, $src2\\n\\t\"\n-            \"movl    $dst, #-1\\n\\t\"\n-            \"jp,s    done\\n\\t\"\n-            \"jb,s    done\\n\\t\"\n-            \"setne   $dst\\n\\t\"\n-            \"movzbl  $dst, $dst\\n\"\n-    \"done:\" %}\n-  ins_encode %{\n-    __ ucomiss($src1$$XMMRegister, $src2$$Address);\n-    emit_cmpfp3(masm, $dst$$Register);\n-  %}\n-  ins_pipe(pipe_slow);\n-%}\n-\n-\/\/ Compare into -1,0,1\n-instruct cmpF_imm(rRegI dst, regF src, immF con, rFlagsReg cr) %{\n-  match(Set dst (CmpF3 src con));\n-  effect(KILL cr);\n-\n-  ins_cost(275);\n-  format %{ \"ucomiss $src, [$constantaddress]\\t# load from constant table: float=$con\\n\\t\"\n-            \"movl    $dst, #-1\\n\\t\"\n-            \"jp,s    done\\n\\t\"\n-            \"jb,s    done\\n\\t\"\n-            \"setne   $dst\\n\\t\"\n-            \"movzbl  $dst, $dst\\n\"\n-    \"done:\" %}\n-  ins_encode %{\n-    __ ucomiss($src$$XMMRegister, $constantaddress($con));\n-    emit_cmpfp3(masm, $dst$$Register);\n-  %}\n-  ins_pipe(pipe_slow);\n-%}\n-\n-\/\/ Compare into -1,0,1\n-instruct cmpD_reg(rRegI dst, regD src1, regD src2, rFlagsReg cr)\n-%{\n-  match(Set dst (CmpD3 src1 src2));\n-  effect(KILL cr);\n-\n-  ins_cost(275);\n-  format %{ \"ucomisd $src1, $src2\\n\\t\"\n-            \"movl    $dst, #-1\\n\\t\"\n-            \"jp,s    done\\n\\t\"\n-            \"jb,s    done\\n\\t\"\n-            \"setne   $dst\\n\\t\"\n-            \"movzbl  $dst, $dst\\n\"\n-    \"done:\" %}\n-  ins_encode %{\n-    __ ucomisd($src1$$XMMRegister, $src2$$XMMRegister);\n-    emit_cmpfp3(masm, $dst$$Register);\n-  %}\n-  ins_pipe(pipe_slow);\n-%}\n-\n-\/\/ Compare into -1,0,1\n-instruct cmpD_mem(rRegI dst, regD src1, memory src2, rFlagsReg cr)\n-%{\n-  match(Set dst (CmpD3 src1 (LoadD src2)));\n-  effect(KILL cr);\n-\n-  ins_cost(275);\n-  format %{ \"ucomisd $src1, $src2\\n\\t\"\n-            \"movl    $dst, #-1\\n\\t\"\n-            \"jp,s    done\\n\\t\"\n-            \"jb,s    done\\n\\t\"\n-            \"setne   $dst\\n\\t\"\n-            \"movzbl  $dst, $dst\\n\"\n-    \"done:\" %}\n-  ins_encode %{\n-    __ ucomisd($src1$$XMMRegister, $src2$$Address);\n-    emit_cmpfp3(masm, $dst$$Register);\n-  %}\n-  ins_pipe(pipe_slow);\n-%}\n-\n-\/\/ Compare into -1,0,1\n-instruct cmpD_imm(rRegI dst, regD src, immD con, rFlagsReg cr) %{\n-  match(Set dst (CmpD3 src con));\n-  effect(KILL cr);\n-\n-  ins_cost(275);\n-  format %{ \"ucomisd $src, [$constantaddress]\\t# load from constant table: double=$con\\n\\t\"\n-            \"movl    $dst, #-1\\n\\t\"\n-            \"jp,s    done\\n\\t\"\n-            \"jb,s    done\\n\\t\"\n-            \"setne   $dst\\n\\t\"\n-            \"movzbl  $dst, $dst\\n\"\n-    \"done:\" %}\n-  ins_encode %{\n-    __ ucomisd($src$$XMMRegister, $constantaddress($con));\n-    emit_cmpfp3(masm, $dst$$Register);\n-  %}\n-  ins_pipe(pipe_slow);\n-%}\n-\n-\/\/----------Arithmetic Conversion Instructions---------------------------------\n-\n-instruct convF2D_reg_reg(regD dst, regF src)\n-%{\n-  match(Set dst (ConvF2D src));\n-\n-  format %{ \"cvtss2sd $dst, $src\" %}\n-  ins_encode %{\n-    __ cvtss2sd ($dst$$XMMRegister, $src$$XMMRegister);\n-  %}\n-  ins_pipe(pipe_slow); \/\/ XXX\n-%}\n-\n-instruct convF2D_reg_mem(regD dst, memory src)\n-%{\n-  predicate(UseAVX == 0);\n-  match(Set dst (ConvF2D (LoadF src)));\n-\n-  format %{ \"cvtss2sd $dst, $src\" %}\n-  ins_encode %{\n-    __ cvtss2sd ($dst$$XMMRegister, $src$$Address);\n-  %}\n-  ins_pipe(pipe_slow); \/\/ XXX\n-%}\n-\n-instruct convD2F_reg_reg(regF dst, regD src)\n-%{\n-  match(Set dst (ConvD2F src));\n-\n-  format %{ \"cvtsd2ss $dst, $src\" %}\n-  ins_encode %{\n-    __ cvtsd2ss ($dst$$XMMRegister, $src$$XMMRegister);\n-  %}\n-  ins_pipe(pipe_slow); \/\/ XXX\n-%}\n-\n-instruct convD2F_reg_mem(regF dst, memory src)\n-%{\n-  predicate(UseAVX == 0);\n-  match(Set dst (ConvD2F (LoadD src)));\n-\n-  format %{ \"cvtsd2ss $dst, $src\" %}\n-  ins_encode %{\n-    __ cvtsd2ss ($dst$$XMMRegister, $src$$Address);\n-  %}\n-  ins_pipe(pipe_slow); \/\/ XXX\n-%}\n-\n-\/\/ XXX do mem variants\n-instruct convF2I_reg_reg(rRegI dst, regF src, rFlagsReg cr)\n-%{\n-  predicate(!VM_Version::supports_avx10_2());\n-  match(Set dst (ConvF2I src));\n-  effect(KILL cr);\n-  format %{ \"convert_f2i $dst, $src\" %}\n-  ins_encode %{\n-    __ convertF2I(T_INT, T_FLOAT, $dst$$Register, $src$$XMMRegister);\n-  %}\n-  ins_pipe(pipe_slow);\n-%}\n-\n-instruct convF2I_reg_reg_avx10(rRegI dst, regF src)\n-%{\n-  predicate(VM_Version::supports_avx10_2());\n-  match(Set dst (ConvF2I src));\n-  format %{ \"evcvttss2sisl $dst, $src\" %}\n-  ins_encode %{\n-    __ evcvttss2sisl($dst$$Register, $src$$XMMRegister);\n-  %}\n-  ins_pipe(pipe_slow);\n-%}\n-\n-instruct convF2I_reg_mem_avx10(rRegI dst, memory src)\n-%{\n-  predicate(VM_Version::supports_avx10_2());\n-  match(Set dst (ConvF2I (LoadF src)));\n-  format %{ \"evcvttss2sisl $dst, $src\" %}\n-  ins_encode %{\n-    __ evcvttss2sisl($dst$$Register, $src$$Address);\n-  %}\n-  ins_pipe(pipe_slow);\n-%}\n-\n-instruct convF2L_reg_reg(rRegL dst, regF src, rFlagsReg cr)\n-%{\n-  predicate(!VM_Version::supports_avx10_2());\n-  match(Set dst (ConvF2L src));\n-  effect(KILL cr);\n-  format %{ \"convert_f2l $dst, $src\"%}\n-  ins_encode %{\n-    __ convertF2I(T_LONG, T_FLOAT, $dst$$Register, $src$$XMMRegister);\n-  %}\n-  ins_pipe(pipe_slow);\n-%}\n-\n-instruct convF2L_reg_reg_avx10(rRegL dst, regF src)\n-%{\n-  predicate(VM_Version::supports_avx10_2());\n-  match(Set dst (ConvF2L src));\n-  format %{ \"evcvttss2sisq $dst, $src\" %}\n-  ins_encode %{\n-    __ evcvttss2sisq($dst$$Register, $src$$XMMRegister);\n-  %}\n-  ins_pipe(pipe_slow);\n-%}\n-\n-instruct convF2L_reg_mem_avx10(rRegL dst, memory src)\n-%{\n-  predicate(VM_Version::supports_avx10_2());\n-  match(Set dst (ConvF2L (LoadF src)));\n-  format %{ \"evcvttss2sisq $dst, $src\" %}\n-  ins_encode %{\n-    __ evcvttss2sisq($dst$$Register, $src$$Address);\n-  %}\n-  ins_pipe(pipe_slow);\n-%}\n-\n-instruct convD2I_reg_reg(rRegI dst, regD src, rFlagsReg cr)\n-%{\n-  predicate(!VM_Version::supports_avx10_2());\n-  match(Set dst (ConvD2I src));\n-  effect(KILL cr);\n-  format %{ \"convert_d2i $dst, $src\"%}\n-  ins_encode %{\n-    __ convertF2I(T_INT, T_DOUBLE, $dst$$Register, $src$$XMMRegister);\n-  %}\n-  ins_pipe(pipe_slow);\n-%}\n-\n-instruct convD2I_reg_reg_avx10(rRegI dst, regD src)\n-%{\n-  predicate(VM_Version::supports_avx10_2());\n-  match(Set dst (ConvD2I src));\n-  format %{ \"evcvttsd2sisl $dst, $src\" %}\n-  ins_encode %{\n-    __ evcvttsd2sisl($dst$$Register, $src$$XMMRegister);\n-  %}\n-  ins_pipe(pipe_slow);\n-%}\n-\n-instruct convD2I_reg_mem_avx10(rRegI dst, memory src)\n-%{\n-  predicate(VM_Version::supports_avx10_2());\n-  match(Set dst (ConvD2I (LoadD src)));\n-  format %{ \"evcvttsd2sisl $dst, $src\" %}\n-  ins_encode %{\n-    __ evcvttsd2sisl($dst$$Register, $src$$Address);\n-  %}\n-  ins_pipe(pipe_slow);\n-%}\n-\n-instruct convD2L_reg_reg(rRegL dst, regD src, rFlagsReg cr)\n-%{\n-  predicate(!VM_Version::supports_avx10_2());\n-  match(Set dst (ConvD2L src));\n-  effect(KILL cr);\n-  format %{ \"convert_d2l $dst, $src\"%}\n-  ins_encode %{\n-    __ convertF2I(T_LONG, T_DOUBLE, $dst$$Register, $src$$XMMRegister);\n-  %}\n-  ins_pipe(pipe_slow);\n-%}\n-\n-instruct convD2L_reg_reg_avx10(rRegL dst, regD src)\n-%{\n-  predicate(VM_Version::supports_avx10_2());\n-  match(Set dst (ConvD2L src));\n-  format %{ \"evcvttsd2sisq $dst, $src\" %}\n-  ins_encode %{\n-    __ evcvttsd2sisq($dst$$Register, $src$$XMMRegister);\n-  %}\n-  ins_pipe(pipe_slow);\n-%}\n-\n-instruct convD2L_reg_mem_avx10(rRegL dst, memory src)\n-%{\n-  predicate(VM_Version::supports_avx10_2());\n-  match(Set dst (ConvD2L (LoadD src)));\n-  format %{ \"evcvttsd2sisq $dst, $src\" %}\n-  ins_encode %{\n-    __ evcvttsd2sisq($dst$$Register, $src$$Address);\n-  %}\n-  ins_pipe(pipe_slow);\n-%}\n-\n-instruct round_double_reg(rRegL dst, regD src, rRegL rtmp, rcx_RegL rcx, rFlagsReg cr)\n-%{\n-  match(Set dst (RoundD src));\n-  effect(TEMP dst, TEMP rtmp, TEMP rcx, KILL cr);\n-  format %{ \"round_double $dst,$src \\t! using $rtmp and $rcx as TEMP\"%}\n-  ins_encode %{\n-    __ round_double($dst$$Register, $src$$XMMRegister, $rtmp$$Register, $rcx$$Register);\n-  %}\n-  ins_pipe(pipe_slow);\n-%}\n-\n-instruct round_float_reg(rRegI dst, regF src, rRegL rtmp, rcx_RegL rcx, rFlagsReg cr)\n-%{\n-  match(Set dst (RoundF src));\n-  effect(TEMP dst, TEMP rtmp, TEMP rcx, KILL cr);\n-  format %{ \"round_float $dst,$src\" %}\n-  ins_encode %{\n-    __ round_float($dst$$Register, $src$$XMMRegister, $rtmp$$Register, $rcx$$Register);\n-  %}\n-  ins_pipe(pipe_slow);\n-%}\n-\n-instruct convI2F_reg_reg(vlRegF dst, rRegI src)\n-%{\n-  predicate(!UseXmmI2F);\n-  match(Set dst (ConvI2F src));\n-\n-  format %{ \"cvtsi2ssl $dst, $src\\t# i2f\" %}\n-  ins_encode %{\n-    if (UseAVX > 0) {\n-      __ pxor($dst$$XMMRegister, $dst$$XMMRegister);\n-    }\n-    __ cvtsi2ssl ($dst$$XMMRegister, $src$$Register);\n-  %}\n-  ins_pipe(pipe_slow); \/\/ XXX\n-%}\n-\n-instruct convI2F_reg_mem(regF dst, memory src)\n-%{\n-  predicate(UseAVX == 0);\n-  match(Set dst (ConvI2F (LoadI src)));\n-\n-  format %{ \"cvtsi2ssl $dst, $src\\t# i2f\" %}\n-  ins_encode %{\n-    __ cvtsi2ssl ($dst$$XMMRegister, $src$$Address);\n-  %}\n-  ins_pipe(pipe_slow); \/\/ XXX\n-%}\n-\n-instruct convI2D_reg_reg(vlRegD dst, rRegI src)\n-%{\n-  predicate(!UseXmmI2D);\n-  match(Set dst (ConvI2D src));\n-\n-  format %{ \"cvtsi2sdl $dst, $src\\t# i2d\" %}\n-  ins_encode %{\n-    if (UseAVX > 0) {\n-      __ pxor($dst$$XMMRegister, $dst$$XMMRegister);\n-    }\n-    __ cvtsi2sdl ($dst$$XMMRegister, $src$$Register);\n-  %}\n-  ins_pipe(pipe_slow); \/\/ XXX\n-%}\n-\n-instruct convI2D_reg_mem(regD dst, memory src)\n-%{\n-  predicate(UseAVX == 0);\n-  match(Set dst (ConvI2D (LoadI src)));\n-\n-  format %{ \"cvtsi2sdl $dst, $src\\t# i2d\" %}\n-  ins_encode %{\n-    __ cvtsi2sdl ($dst$$XMMRegister, $src$$Address);\n-  %}\n-  ins_pipe(pipe_slow); \/\/ XXX\n-%}\n-\n-instruct convXI2F_reg(regF dst, rRegI src)\n-%{\n-  predicate(UseXmmI2F);\n-  match(Set dst (ConvI2F src));\n-\n-  format %{ \"movdl $dst, $src\\n\\t\"\n-            \"cvtdq2psl $dst, $dst\\t# i2f\" %}\n-  ins_encode %{\n-    __ movdl($dst$$XMMRegister, $src$$Register);\n-    __ cvtdq2ps($dst$$XMMRegister, $dst$$XMMRegister);\n-  %}\n-  ins_pipe(pipe_slow); \/\/ XXX\n-%}\n-\n-instruct convXI2D_reg(regD dst, rRegI src)\n-%{\n-  predicate(UseXmmI2D);\n-  match(Set dst (ConvI2D src));\n-\n-  format %{ \"movdl $dst, $src\\n\\t\"\n-            \"cvtdq2pdl $dst, $dst\\t# i2d\" %}\n-  ins_encode %{\n-    __ movdl($dst$$XMMRegister, $src$$Register);\n-    __ cvtdq2pd($dst$$XMMRegister, $dst$$XMMRegister);\n-  %}\n-  ins_pipe(pipe_slow); \/\/ XXX\n-%}\n-\n-instruct convL2F_reg_reg(vlRegF dst, rRegL src)\n-%{\n-  match(Set dst (ConvL2F src));\n-\n-  format %{ \"cvtsi2ssq $dst, $src\\t# l2f\" %}\n-  ins_encode %{\n-    if (UseAVX > 0) {\n-      __ pxor($dst$$XMMRegister, $dst$$XMMRegister);\n-    }\n-    __ cvtsi2ssq ($dst$$XMMRegister, $src$$Register);\n-  %}\n-  ins_pipe(pipe_slow); \/\/ XXX\n-%}\n-\n-instruct convL2F_reg_mem(regF dst, memory src)\n-%{\n-  predicate(UseAVX == 0);\n-  match(Set dst (ConvL2F (LoadL src)));\n-\n-  format %{ \"cvtsi2ssq $dst, $src\\t# l2f\" %}\n-  ins_encode %{\n-    __ cvtsi2ssq ($dst$$XMMRegister, $src$$Address);\n-  %}\n-  ins_pipe(pipe_slow); \/\/ XXX\n-%}\n-\n-instruct convL2D_reg_reg(vlRegD dst, rRegL src)\n-%{\n-  match(Set dst (ConvL2D src));\n-\n-  format %{ \"cvtsi2sdq $dst, $src\\t# l2d\" %}\n-  ins_encode %{\n-    if (UseAVX > 0) {\n-      __ pxor($dst$$XMMRegister, $dst$$XMMRegister);\n-    }\n-    __ cvtsi2sdq ($dst$$XMMRegister, $src$$Register);\n-  %}\n-  ins_pipe(pipe_slow); \/\/ XXX\n-%}\n-\n-instruct convL2D_reg_mem(regD dst, memory src)\n-%{\n-  predicate(UseAVX == 0);\n-  match(Set dst (ConvL2D (LoadL src)));\n-\n-  format %{ \"cvtsi2sdq $dst, $src\\t# l2d\" %}\n-  ins_encode %{\n-    __ cvtsi2sdq ($dst$$XMMRegister, $src$$Address);\n-  %}\n-  ins_pipe(pipe_slow); \/\/ XXX\n-%}\n-\n-instruct convI2L_reg_reg(rRegL dst, rRegI src)\n-%{\n-  match(Set dst (ConvI2L src));\n-\n-  ins_cost(125);\n-  format %{ \"movslq  $dst, $src\\t# i2l\" %}\n-  ins_encode %{\n-    __ movslq($dst$$Register, $src$$Register);\n-  %}\n-  ins_pipe(ialu_reg_reg);\n-%}\n-\n-\/\/ Zero-extend convert int to long\n-instruct convI2L_reg_reg_zex(rRegL dst, rRegI src, immL_32bits mask)\n-%{\n-  match(Set dst (AndL (ConvI2L src) mask));\n-\n-  format %{ \"movl    $dst, $src\\t# i2l zero-extend\\n\\t\" %}\n-  ins_encode %{\n-    if ($dst$$reg != $src$$reg) {\n-      __ movl($dst$$Register, $src$$Register);\n-    }\n-  %}\n-  ins_pipe(ialu_reg_reg);\n-%}\n-\n-\/\/ Zero-extend convert int to long\n-instruct convI2L_reg_mem_zex(rRegL dst, memory src, immL_32bits mask)\n-%{\n-  match(Set dst (AndL (ConvI2L (LoadI src)) mask));\n-\n-  format %{ \"movl    $dst, $src\\t# i2l zero-extend\\n\\t\" %}\n-  ins_encode %{\n-    __ movl($dst$$Register, $src$$Address);\n-  %}\n-  ins_pipe(ialu_reg_mem);\n-%}\n-\n-instruct zerox_long_reg_reg(rRegL dst, rRegL src, immL_32bits mask)\n-%{\n-  match(Set dst (AndL src mask));\n-\n-  format %{ \"movl    $dst, $src\\t# zero-extend long\" %}\n-  ins_encode %{\n-    __ movl($dst$$Register, $src$$Register);\n-  %}\n-  ins_pipe(ialu_reg_reg);\n-%}\n-\n-instruct convL2I_reg_reg(rRegI dst, rRegL src)\n-%{\n-  match(Set dst (ConvL2I src));\n-\n-  format %{ \"movl    $dst, $src\\t# l2i\" %}\n-  ins_encode %{\n-    __ movl($dst$$Register, $src$$Register);\n-  %}\n-  ins_pipe(ialu_reg_reg);\n-%}\n-\n-\n-instruct MoveF2I_stack_reg(rRegI dst, stackSlotF src) %{\n-  match(Set dst (MoveF2I src));\n-  effect(DEF dst, USE src);\n-\n-  ins_cost(125);\n-  format %{ \"movl    $dst, $src\\t# MoveF2I_stack_reg\" %}\n-  ins_encode %{\n-    __ movl($dst$$Register, Address(rsp, $src$$disp));\n-  %}\n-  ins_pipe(ialu_reg_mem);\n-%}\n-\n-instruct MoveI2F_stack_reg(regF dst, stackSlotI src) %{\n-  match(Set dst (MoveI2F src));\n-  effect(DEF dst, USE src);\n-\n-  ins_cost(125);\n-  format %{ \"movss   $dst, $src\\t# MoveI2F_stack_reg\" %}\n-  ins_encode %{\n-    __ movflt($dst$$XMMRegister, Address(rsp, $src$$disp));\n-  %}\n-  ins_pipe(pipe_slow);\n-%}\n-\n-instruct MoveD2L_stack_reg(rRegL dst, stackSlotD src) %{\n-  match(Set dst (MoveD2L src));\n-  effect(DEF dst, USE src);\n-\n-  ins_cost(125);\n-  format %{ \"movq    $dst, $src\\t# MoveD2L_stack_reg\" %}\n-  ins_encode %{\n-    __ movq($dst$$Register, Address(rsp, $src$$disp));\n-  %}\n-  ins_pipe(ialu_reg_mem);\n-%}\n-\n-instruct MoveL2D_stack_reg_partial(regD dst, stackSlotL src) %{\n-  predicate(!UseXmmLoadAndClearUpper);\n-  match(Set dst (MoveL2D src));\n-  effect(DEF dst, USE src);\n-\n-  ins_cost(125);\n-  format %{ \"movlpd  $dst, $src\\t# MoveL2D_stack_reg\" %}\n-  ins_encode %{\n-    __ movdbl($dst$$XMMRegister, Address(rsp, $src$$disp));\n-  %}\n-  ins_pipe(pipe_slow);\n-%}\n-\n-instruct MoveL2D_stack_reg(regD dst, stackSlotL src) %{\n-  predicate(UseXmmLoadAndClearUpper);\n-  match(Set dst (MoveL2D src));\n-  effect(DEF dst, USE src);\n-\n-  ins_cost(125);\n-  format %{ \"movsd   $dst, $src\\t# MoveL2D_stack_reg\" %}\n-  ins_encode %{\n-    __ movdbl($dst$$XMMRegister, Address(rsp, $src$$disp));\n-  %}\n-  ins_pipe(pipe_slow);\n-%}\n-\n-\n-instruct MoveF2I_reg_stack(stackSlotI dst, regF src) %{\n-  match(Set dst (MoveF2I src));\n-  effect(DEF dst, USE src);\n-\n-  ins_cost(95); \/\/ XXX\n-  format %{ \"movss   $dst, $src\\t# MoveF2I_reg_stack\" %}\n-  ins_encode %{\n-    __ movflt(Address(rsp, $dst$$disp), $src$$XMMRegister);\n-  %}\n-  ins_pipe(pipe_slow);\n-%}\n-\n-instruct MoveI2F_reg_stack(stackSlotF dst, rRegI src) %{\n-  match(Set dst (MoveI2F src));\n-  effect(DEF dst, USE src);\n-\n-  ins_cost(100);\n-  format %{ \"movl    $dst, $src\\t# MoveI2F_reg_stack\" %}\n-  ins_encode %{\n-    __ movl(Address(rsp, $dst$$disp), $src$$Register);\n-  %}\n-  ins_pipe( ialu_mem_reg );\n-%}\n-\n-instruct MoveD2L_reg_stack(stackSlotL dst, regD src) %{\n-  match(Set dst (MoveD2L src));\n-  effect(DEF dst, USE src);\n-\n-  ins_cost(95); \/\/ XXX\n-  format %{ \"movsd   $dst, $src\\t# MoveL2D_reg_stack\" %}\n-  ins_encode %{\n-    __ movdbl(Address(rsp, $dst$$disp), $src$$XMMRegister);\n-  %}\n-  ins_pipe(pipe_slow);\n-%}\n-\n-instruct MoveL2D_reg_stack(stackSlotD dst, rRegL src) %{\n-  match(Set dst (MoveL2D src));\n-  effect(DEF dst, USE src);\n-\n-  ins_cost(100);\n-  format %{ \"movq    $dst, $src\\t# MoveL2D_reg_stack\" %}\n-  ins_encode %{\n-    __ movq(Address(rsp, $dst$$disp), $src$$Register);\n-  %}\n-  ins_pipe(ialu_mem_reg);\n-%}\n-\n-instruct MoveF2I_reg_reg(rRegI dst, regF src) %{\n-  match(Set dst (MoveF2I src));\n-  effect(DEF dst, USE src);\n-  ins_cost(85);\n-  format %{ \"movd    $dst,$src\\t# MoveF2I\" %}\n-  ins_encode %{\n-    __ movdl($dst$$Register, $src$$XMMRegister);\n-  %}\n-  ins_pipe( pipe_slow );\n-%}\n-\n-instruct MoveD2L_reg_reg(rRegL dst, regD src) %{\n-  match(Set dst (MoveD2L src));\n-  effect(DEF dst, USE src);\n-  ins_cost(85);\n-  format %{ \"movd    $dst,$src\\t# MoveD2L\" %}\n-  ins_encode %{\n-    __ movdq($dst$$Register, $src$$XMMRegister);\n-  %}\n-  ins_pipe( pipe_slow );\n-%}\n-\n-instruct MoveI2F_reg_reg(regF dst, rRegI src) %{\n-  match(Set dst (MoveI2F src));\n-  effect(DEF dst, USE src);\n-  ins_cost(100);\n-  format %{ \"movd    $dst,$src\\t# MoveI2F\" %}\n-  ins_encode %{\n-    __ movdl($dst$$XMMRegister, $src$$Register);\n-  %}\n-  ins_pipe( pipe_slow );\n-%}\n-\n-instruct MoveL2D_reg_reg(regD dst, rRegL src) %{\n-  match(Set dst (MoveL2D src));\n-  effect(DEF dst, USE src);\n-  ins_cost(100);\n-  format %{ \"movd    $dst,$src\\t# MoveL2D\" %}\n-  ins_encode %{\n-     __ movdq($dst$$XMMRegister, $src$$Register);\n-  %}\n-  ins_pipe( pipe_slow );\n-%}\n-\n-\/\/ Fast clearing of an array\n-\/\/ Small non-constant lenght ClearArray for non-AVX512 targets.\n-instruct rep_stos(rcx_RegL cnt, rdi_RegP base, regD tmp, rax_RegI zero,\n-                  Universe dummy, rFlagsReg cr)\n-%{\n-  predicate(!((ClearArrayNode*)n)->is_large() && (UseAVX <= 2));\n-  match(Set dummy (ClearArray cnt base));\n-  effect(USE_KILL cnt, USE_KILL base, TEMP tmp, KILL zero, KILL cr);\n-\n-  format %{ $$template\n-    $$emit$$\"xorq    rax, rax\\t# ClearArray:\\n\\t\"\n-    $$emit$$\"cmp     InitArrayShortSize,rcx\\n\\t\"\n-    $$emit$$\"jg      LARGE\\n\\t\"\n-    $$emit$$\"dec     rcx\\n\\t\"\n-    $$emit$$\"js      DONE\\t# Zero length\\n\\t\"\n-    $$emit$$\"mov     rax,(rdi,rcx,8)\\t# LOOP\\n\\t\"\n-    $$emit$$\"dec     rcx\\n\\t\"\n-    $$emit$$\"jge     LOOP\\n\\t\"\n-    $$emit$$\"jmp     DONE\\n\\t\"\n-    $$emit$$\"# LARGE:\\n\\t\"\n-    if (UseFastStosb) {\n-       $$emit$$\"shlq    rcx,3\\t# Convert doublewords to bytes\\n\\t\"\n-       $$emit$$\"rep     stosb\\t# Store rax to *rdi++ while rcx--\\n\\t\"\n-    } else if (UseXMMForObjInit) {\n-       $$emit$$\"mov     rdi,rax\\n\\t\"\n-       $$emit$$\"vpxor   ymm0,ymm0,ymm0\\n\\t\"\n-       $$emit$$\"jmpq    L_zero_64_bytes\\n\\t\"\n-       $$emit$$\"# L_loop:\\t# 64-byte LOOP\\n\\t\"\n-       $$emit$$\"vmovdqu ymm0,(rax)\\n\\t\"\n-       $$emit$$\"vmovdqu ymm0,0x20(rax)\\n\\t\"\n-       $$emit$$\"add     0x40,rax\\n\\t\"\n-       $$emit$$\"# L_zero_64_bytes:\\n\\t\"\n-       $$emit$$\"sub     0x8,rcx\\n\\t\"\n-       $$emit$$\"jge     L_loop\\n\\t\"\n-       $$emit$$\"add     0x4,rcx\\n\\t\"\n-       $$emit$$\"jl      L_tail\\n\\t\"\n-       $$emit$$\"vmovdqu ymm0,(rax)\\n\\t\"\n-       $$emit$$\"add     0x20,rax\\n\\t\"\n-       $$emit$$\"sub     0x4,rcx\\n\\t\"\n-       $$emit$$\"# L_tail:\\t# Clearing tail bytes\\n\\t\"\n-       $$emit$$\"add     0x4,rcx\\n\\t\"\n-       $$emit$$\"jle     L_end\\n\\t\"\n-       $$emit$$\"dec     rcx\\n\\t\"\n-       $$emit$$\"# L_sloop:\\t# 8-byte short loop\\n\\t\"\n-       $$emit$$\"vmovq   xmm0,(rax)\\n\\t\"\n-       $$emit$$\"add     0x8,rax\\n\\t\"\n-       $$emit$$\"dec     rcx\\n\\t\"\n-       $$emit$$\"jge     L_sloop\\n\\t\"\n-       $$emit$$\"# L_end:\\n\\t\"\n-    } else {\n-       $$emit$$\"rep     stosq\\t# Store rax to *rdi++ while rcx--\\n\\t\"\n-    }\n-    $$emit$$\"# DONE\"\n-  %}\n-  ins_encode %{\n-    __ clear_mem($base$$Register, $cnt$$Register, $zero$$Register,\n-                 $tmp$$XMMRegister, false, knoreg);\n-  %}\n-  ins_pipe(pipe_slow);\n-%}\n-\n-\/\/ Small non-constant length ClearArray for AVX512 targets.\n-instruct rep_stos_evex(rcx_RegL cnt, rdi_RegP base, legRegD tmp, kReg ktmp, rax_RegI zero,\n-                       Universe dummy, rFlagsReg cr)\n-%{\n-  predicate(!((ClearArrayNode*)n)->is_large() && (UseAVX > 2));\n-  match(Set dummy (ClearArray cnt base));\n-  ins_cost(125);\n-  effect(USE_KILL cnt, USE_KILL base, TEMP tmp, TEMP ktmp, KILL zero, KILL cr);\n-\n-  format %{ $$template\n-    $$emit$$\"xorq    rax, rax\\t# ClearArray:\\n\\t\"\n-    $$emit$$\"cmp     InitArrayShortSize,rcx\\n\\t\"\n-    $$emit$$\"jg      LARGE\\n\\t\"\n-    $$emit$$\"dec     rcx\\n\\t\"\n-    $$emit$$\"js      DONE\\t# Zero length\\n\\t\"\n-    $$emit$$\"mov     rax,(rdi,rcx,8)\\t# LOOP\\n\\t\"\n-    $$emit$$\"dec     rcx\\n\\t\"\n-    $$emit$$\"jge     LOOP\\n\\t\"\n-    $$emit$$\"jmp     DONE\\n\\t\"\n-    $$emit$$\"# LARGE:\\n\\t\"\n-    if (UseFastStosb) {\n-       $$emit$$\"shlq    rcx,3\\t# Convert doublewords to bytes\\n\\t\"\n-       $$emit$$\"rep     stosb\\t# Store rax to *rdi++ while rcx--\\n\\t\"\n-    } else if (UseXMMForObjInit) {\n-       $$emit$$\"mov     rdi,rax\\n\\t\"\n-       $$emit$$\"vpxor   ymm0,ymm0,ymm0\\n\\t\"\n-       $$emit$$\"jmpq    L_zero_64_bytes\\n\\t\"\n-       $$emit$$\"# L_loop:\\t# 64-byte LOOP\\n\\t\"\n-       $$emit$$\"vmovdqu ymm0,(rax)\\n\\t\"\n-       $$emit$$\"vmovdqu ymm0,0x20(rax)\\n\\t\"\n-       $$emit$$\"add     0x40,rax\\n\\t\"\n-       $$emit$$\"# L_zero_64_bytes:\\n\\t\"\n-       $$emit$$\"sub     0x8,rcx\\n\\t\"\n-       $$emit$$\"jge     L_loop\\n\\t\"\n-       $$emit$$\"add     0x4,rcx\\n\\t\"\n-       $$emit$$\"jl      L_tail\\n\\t\"\n-       $$emit$$\"vmovdqu ymm0,(rax)\\n\\t\"\n-       $$emit$$\"add     0x20,rax\\n\\t\"\n-       $$emit$$\"sub     0x4,rcx\\n\\t\"\n-       $$emit$$\"# L_tail:\\t# Clearing tail bytes\\n\\t\"\n-       $$emit$$\"add     0x4,rcx\\n\\t\"\n-       $$emit$$\"jle     L_end\\n\\t\"\n-       $$emit$$\"dec     rcx\\n\\t\"\n-       $$emit$$\"# L_sloop:\\t# 8-byte short loop\\n\\t\"\n-       $$emit$$\"vmovq   xmm0,(rax)\\n\\t\"\n-       $$emit$$\"add     0x8,rax\\n\\t\"\n-       $$emit$$\"dec     rcx\\n\\t\"\n-       $$emit$$\"jge     L_sloop\\n\\t\"\n-       $$emit$$\"# L_end:\\n\\t\"\n-    } else {\n-       $$emit$$\"rep     stosq\\t# Store rax to *rdi++ while rcx--\\n\\t\"\n-    }\n-    $$emit$$\"# DONE\"\n-  %}\n-  ins_encode %{\n-    __ clear_mem($base$$Register, $cnt$$Register, $zero$$Register,\n-                 $tmp$$XMMRegister, false, $ktmp$$KRegister);\n-  %}\n-  ins_pipe(pipe_slow);\n-%}\n-\n-\/\/ Large non-constant length ClearArray for non-AVX512 targets.\n-instruct rep_stos_large(rcx_RegL cnt, rdi_RegP base, regD tmp, rax_RegI zero,\n-                        Universe dummy, rFlagsReg cr)\n-%{\n-  predicate((UseAVX <=2) && ((ClearArrayNode*)n)->is_large());\n-  match(Set dummy (ClearArray cnt base));\n-  effect(USE_KILL cnt, USE_KILL base, TEMP tmp, KILL zero, KILL cr);\n-\n-  format %{ $$template\n-    if (UseFastStosb) {\n-       $$emit$$\"xorq    rax, rax\\t# ClearArray:\\n\\t\"\n-       $$emit$$\"shlq    rcx,3\\t# Convert doublewords to bytes\\n\\t\"\n-       $$emit$$\"rep     stosb\\t# Store rax to *rdi++ while rcx--\"\n-    } else if (UseXMMForObjInit) {\n-       $$emit$$\"mov     rdi,rax\\t# ClearArray:\\n\\t\"\n-       $$emit$$\"vpxor   ymm0,ymm0,ymm0\\n\\t\"\n-       $$emit$$\"jmpq    L_zero_64_bytes\\n\\t\"\n-       $$emit$$\"# L_loop:\\t# 64-byte LOOP\\n\\t\"\n-       $$emit$$\"vmovdqu ymm0,(rax)\\n\\t\"\n-       $$emit$$\"vmovdqu ymm0,0x20(rax)\\n\\t\"\n-       $$emit$$\"add     0x40,rax\\n\\t\"\n-       $$emit$$\"# L_zero_64_bytes:\\n\\t\"\n-       $$emit$$\"sub     0x8,rcx\\n\\t\"\n-       $$emit$$\"jge     L_loop\\n\\t\"\n-       $$emit$$\"add     0x4,rcx\\n\\t\"\n-       $$emit$$\"jl      L_tail\\n\\t\"\n-       $$emit$$\"vmovdqu ymm0,(rax)\\n\\t\"\n-       $$emit$$\"add     0x20,rax\\n\\t\"\n-       $$emit$$\"sub     0x4,rcx\\n\\t\"\n-       $$emit$$\"# L_tail:\\t# Clearing tail bytes\\n\\t\"\n-       $$emit$$\"add     0x4,rcx\\n\\t\"\n-       $$emit$$\"jle     L_end\\n\\t\"\n-       $$emit$$\"dec     rcx\\n\\t\"\n-       $$emit$$\"# L_sloop:\\t# 8-byte short loop\\n\\t\"\n-       $$emit$$\"vmovq   xmm0,(rax)\\n\\t\"\n-       $$emit$$\"add     0x8,rax\\n\\t\"\n-       $$emit$$\"dec     rcx\\n\\t\"\n-       $$emit$$\"jge     L_sloop\\n\\t\"\n-       $$emit$$\"# L_end:\\n\\t\"\n-    } else {\n-       $$emit$$\"xorq    rax, rax\\t# ClearArray:\\n\\t\"\n-       $$emit$$\"rep     stosq\\t# Store rax to *rdi++ while rcx--\"\n-    }\n-  %}\n-  ins_encode %{\n-    __ clear_mem($base$$Register, $cnt$$Register, $zero$$Register,\n-                 $tmp$$XMMRegister, true, knoreg);\n-  %}\n-  ins_pipe(pipe_slow);\n-%}\n-\n-\/\/ Large non-constant length ClearArray for AVX512 targets.\n-instruct rep_stos_large_evex(rcx_RegL cnt, rdi_RegP base, legRegD tmp, kReg ktmp, rax_RegI zero,\n-                             Universe dummy, rFlagsReg cr)\n-%{\n-  predicate((UseAVX > 2) && ((ClearArrayNode*)n)->is_large());\n-  match(Set dummy (ClearArray cnt base));\n-  effect(USE_KILL cnt, USE_KILL base, TEMP tmp, TEMP ktmp, KILL zero, KILL cr);\n-\n-  format %{ $$template\n-    if (UseFastStosb) {\n-       $$emit$$\"xorq    rax, rax\\t# ClearArray:\\n\\t\"\n-       $$emit$$\"shlq    rcx,3\\t# Convert doublewords to bytes\\n\\t\"\n-       $$emit$$\"rep     stosb\\t# Store rax to *rdi++ while rcx--\"\n-    } else if (UseXMMForObjInit) {\n-       $$emit$$\"mov     rdi,rax\\t# ClearArray:\\n\\t\"\n-       $$emit$$\"vpxor   ymm0,ymm0,ymm0\\n\\t\"\n-       $$emit$$\"jmpq    L_zero_64_bytes\\n\\t\"\n-       $$emit$$\"# L_loop:\\t# 64-byte LOOP\\n\\t\"\n-       $$emit$$\"vmovdqu ymm0,(rax)\\n\\t\"\n-       $$emit$$\"vmovdqu ymm0,0x20(rax)\\n\\t\"\n-       $$emit$$\"add     0x40,rax\\n\\t\"\n-       $$emit$$\"# L_zero_64_bytes:\\n\\t\"\n-       $$emit$$\"sub     0x8,rcx\\n\\t\"\n-       $$emit$$\"jge     L_loop\\n\\t\"\n-       $$emit$$\"add     0x4,rcx\\n\\t\"\n-       $$emit$$\"jl      L_tail\\n\\t\"\n-       $$emit$$\"vmovdqu ymm0,(rax)\\n\\t\"\n-       $$emit$$\"add     0x20,rax\\n\\t\"\n-       $$emit$$\"sub     0x4,rcx\\n\\t\"\n-       $$emit$$\"# L_tail:\\t# Clearing tail bytes\\n\\t\"\n-       $$emit$$\"add     0x4,rcx\\n\\t\"\n-       $$emit$$\"jle     L_end\\n\\t\"\n-       $$emit$$\"dec     rcx\\n\\t\"\n-       $$emit$$\"# L_sloop:\\t# 8-byte short loop\\n\\t\"\n-       $$emit$$\"vmovq   xmm0,(rax)\\n\\t\"\n-       $$emit$$\"add     0x8,rax\\n\\t\"\n-       $$emit$$\"dec     rcx\\n\\t\"\n-       $$emit$$\"jge     L_sloop\\n\\t\"\n-       $$emit$$\"# L_end:\\n\\t\"\n-    } else {\n-       $$emit$$\"xorq    rax, rax\\t# ClearArray:\\n\\t\"\n-       $$emit$$\"rep     stosq\\t# Store rax to *rdi++ while rcx--\"\n-    }\n-  %}\n-  ins_encode %{\n-    __ clear_mem($base$$Register, $cnt$$Register, $zero$$Register,\n-                 $tmp$$XMMRegister, true, $ktmp$$KRegister);\n-  %}\n-  ins_pipe(pipe_slow);\n-%}\n-\n-\/\/ Small constant length ClearArray for AVX512 targets.\n-instruct rep_stos_im(immL cnt, rRegP base, regD tmp, rRegI zero, kReg ktmp, Universe dummy, rFlagsReg cr)\n-%{\n-  predicate(!((ClearArrayNode*)n)->is_large() && (MaxVectorSize >= 32) && VM_Version::supports_avx512vl());\n-  match(Set dummy (ClearArray cnt base));\n-  ins_cost(100);\n-  effect(TEMP tmp, TEMP zero, TEMP ktmp, KILL cr);\n-  format %{ \"clear_mem_imm $base , $cnt  \\n\\t\" %}\n-  ins_encode %{\n-   __ clear_mem($base$$Register, $cnt$$constant, $zero$$Register, $tmp$$XMMRegister, $ktmp$$KRegister);\n-  %}\n-  ins_pipe(pipe_slow);\n-%}\n-\n-instruct string_compareL(rdi_RegP str1, rcx_RegI cnt1, rsi_RegP str2, rdx_RegI cnt2,\n-                         rax_RegI result, legRegD tmp1, rFlagsReg cr)\n-%{\n-  predicate(!VM_Version::supports_avx512vlbw() && ((StrCompNode*)n)->encoding() == StrIntrinsicNode::LL);\n-  match(Set result (StrComp (Binary str1 cnt1) (Binary str2 cnt2)));\n-  effect(TEMP tmp1, USE_KILL str1, USE_KILL str2, USE_KILL cnt1, USE_KILL cnt2, KILL cr);\n-\n-  format %{ \"String Compare byte[] $str1,$cnt1,$str2,$cnt2 -> $result   \/\/ KILL $tmp1\" %}\n-  ins_encode %{\n-    __ string_compare($str1$$Register, $str2$$Register,\n-                      $cnt1$$Register, $cnt2$$Register, $result$$Register,\n-                      $tmp1$$XMMRegister, StrIntrinsicNode::LL, knoreg);\n-  %}\n-  ins_pipe( pipe_slow );\n-%}\n-\n-instruct string_compareL_evex(rdi_RegP str1, rcx_RegI cnt1, rsi_RegP str2, rdx_RegI cnt2,\n-                              rax_RegI result, legRegD tmp1, kReg ktmp, rFlagsReg cr)\n-%{\n-  predicate(VM_Version::supports_avx512vlbw() && ((StrCompNode*)n)->encoding() == StrIntrinsicNode::LL);\n-  match(Set result (StrComp (Binary str1 cnt1) (Binary str2 cnt2)));\n-  effect(TEMP tmp1, TEMP ktmp, USE_KILL str1, USE_KILL str2, USE_KILL cnt1, USE_KILL cnt2, KILL cr);\n-\n-  format %{ \"String Compare byte[] $str1,$cnt1,$str2,$cnt2 -> $result   \/\/ KILL $tmp1\" %}\n-  ins_encode %{\n-    __ string_compare($str1$$Register, $str2$$Register,\n-                      $cnt1$$Register, $cnt2$$Register, $result$$Register,\n-                      $tmp1$$XMMRegister, StrIntrinsicNode::LL, $ktmp$$KRegister);\n-  %}\n-  ins_pipe( pipe_slow );\n-%}\n-\n-instruct string_compareU(rdi_RegP str1, rcx_RegI cnt1, rsi_RegP str2, rdx_RegI cnt2,\n-                         rax_RegI result, legRegD tmp1, rFlagsReg cr)\n-%{\n-  predicate(!VM_Version::supports_avx512vlbw() && ((StrCompNode*)n)->encoding() == StrIntrinsicNode::UU);\n-  match(Set result (StrComp (Binary str1 cnt1) (Binary str2 cnt2)));\n-  effect(TEMP tmp1, USE_KILL str1, USE_KILL str2, USE_KILL cnt1, USE_KILL cnt2, KILL cr);\n-\n-  format %{ \"String Compare char[] $str1,$cnt1,$str2,$cnt2 -> $result   \/\/ KILL $tmp1\" %}\n-  ins_encode %{\n-    __ string_compare($str1$$Register, $str2$$Register,\n-                      $cnt1$$Register, $cnt2$$Register, $result$$Register,\n-                      $tmp1$$XMMRegister, StrIntrinsicNode::UU, knoreg);\n-  %}\n-  ins_pipe( pipe_slow );\n-%}\n-\n-instruct string_compareU_evex(rdi_RegP str1, rcx_RegI cnt1, rsi_RegP str2, rdx_RegI cnt2,\n-                              rax_RegI result, legRegD tmp1, kReg ktmp, rFlagsReg cr)\n-%{\n-  predicate(VM_Version::supports_avx512vlbw() && ((StrCompNode*)n)->encoding() == StrIntrinsicNode::UU);\n-  match(Set result (StrComp (Binary str1 cnt1) (Binary str2 cnt2)));\n-  effect(TEMP tmp1, TEMP ktmp, USE_KILL str1, USE_KILL str2, USE_KILL cnt1, USE_KILL cnt2, KILL cr);\n-\n-  format %{ \"String Compare char[] $str1,$cnt1,$str2,$cnt2 -> $result   \/\/ KILL $tmp1\" %}\n-  ins_encode %{\n-    __ string_compare($str1$$Register, $str2$$Register,\n-                      $cnt1$$Register, $cnt2$$Register, $result$$Register,\n-                      $tmp1$$XMMRegister, StrIntrinsicNode::UU, $ktmp$$KRegister);\n-  %}\n-  ins_pipe( pipe_slow );\n-%}\n-\n-instruct string_compareLU(rdi_RegP str1, rcx_RegI cnt1, rsi_RegP str2, rdx_RegI cnt2,\n-                          rax_RegI result, legRegD tmp1, rFlagsReg cr)\n-%{\n-  predicate(!VM_Version::supports_avx512vlbw() && ((StrCompNode*)n)->encoding() == StrIntrinsicNode::LU);\n-  match(Set result (StrComp (Binary str1 cnt1) (Binary str2 cnt2)));\n-  effect(TEMP tmp1, USE_KILL str1, USE_KILL str2, USE_KILL cnt1, USE_KILL cnt2, KILL cr);\n-\n-  format %{ \"String Compare byte[] $str1,$cnt1,$str2,$cnt2 -> $result   \/\/ KILL $tmp1\" %}\n-  ins_encode %{\n-    __ string_compare($str1$$Register, $str2$$Register,\n-                      $cnt1$$Register, $cnt2$$Register, $result$$Register,\n-                      $tmp1$$XMMRegister, StrIntrinsicNode::LU, knoreg);\n-  %}\n-  ins_pipe( pipe_slow );\n-%}\n-\n-instruct string_compareLU_evex(rdi_RegP str1, rcx_RegI cnt1, rsi_RegP str2, rdx_RegI cnt2,\n-                               rax_RegI result, legRegD tmp1, kReg ktmp, rFlagsReg cr)\n-%{\n-  predicate(VM_Version::supports_avx512vlbw() && ((StrCompNode*)n)->encoding() == StrIntrinsicNode::LU);\n-  match(Set result (StrComp (Binary str1 cnt1) (Binary str2 cnt2)));\n-  effect(TEMP tmp1, TEMP ktmp, USE_KILL str1, USE_KILL str2, USE_KILL cnt1, USE_KILL cnt2, KILL cr);\n-\n-  format %{ \"String Compare byte[] $str1,$cnt1,$str2,$cnt2 -> $result   \/\/ KILL $tmp1\" %}\n-  ins_encode %{\n-    __ string_compare($str1$$Register, $str2$$Register,\n-                      $cnt1$$Register, $cnt2$$Register, $result$$Register,\n-                      $tmp1$$XMMRegister, StrIntrinsicNode::LU, $ktmp$$KRegister);\n-  %}\n-  ins_pipe( pipe_slow );\n-%}\n-\n-instruct string_compareUL(rsi_RegP str1, rdx_RegI cnt1, rdi_RegP str2, rcx_RegI cnt2,\n-                          rax_RegI result, legRegD tmp1, rFlagsReg cr)\n-%{\n-  predicate(!VM_Version::supports_avx512vlbw() && ((StrCompNode*)n)->encoding() == StrIntrinsicNode::UL);\n-  match(Set result (StrComp (Binary str1 cnt1) (Binary str2 cnt2)));\n-  effect(TEMP tmp1, USE_KILL str1, USE_KILL str2, USE_KILL cnt1, USE_KILL cnt2, KILL cr);\n-\n-  format %{ \"String Compare byte[] $str1,$cnt1,$str2,$cnt2 -> $result   \/\/ KILL $tmp1\" %}\n-  ins_encode %{\n-    __ string_compare($str2$$Register, $str1$$Register,\n-                      $cnt2$$Register, $cnt1$$Register, $result$$Register,\n-                      $tmp1$$XMMRegister, StrIntrinsicNode::UL, knoreg);\n-  %}\n-  ins_pipe( pipe_slow );\n-%}\n-\n-instruct string_compareUL_evex(rsi_RegP str1, rdx_RegI cnt1, rdi_RegP str2, rcx_RegI cnt2,\n-                               rax_RegI result, legRegD tmp1, kReg ktmp, rFlagsReg cr)\n-%{\n-  predicate(VM_Version::supports_avx512vlbw() && ((StrCompNode*)n)->encoding() == StrIntrinsicNode::UL);\n-  match(Set result (StrComp (Binary str1 cnt1) (Binary str2 cnt2)));\n-  effect(TEMP tmp1, TEMP ktmp, USE_KILL str1, USE_KILL str2, USE_KILL cnt1, USE_KILL cnt2, KILL cr);\n-\n-  format %{ \"String Compare byte[] $str1,$cnt1,$str2,$cnt2 -> $result   \/\/ KILL $tmp1\" %}\n-  ins_encode %{\n-    __ string_compare($str2$$Register, $str1$$Register,\n-                      $cnt2$$Register, $cnt1$$Register, $result$$Register,\n-                      $tmp1$$XMMRegister, StrIntrinsicNode::UL, $ktmp$$KRegister);\n-  %}\n-  ins_pipe( pipe_slow );\n-%}\n-\n-\/\/ fast search of substring with known size.\n-instruct string_indexof_conL(rdi_RegP str1, rdx_RegI cnt1, rsi_RegP str2, immI int_cnt2,\n-                             rbx_RegI result, legRegD tmp_vec, rax_RegI cnt2, rcx_RegI tmp, rFlagsReg cr)\n-%{\n-  predicate(UseSSE42Intrinsics && (((StrIndexOfNode*)n)->encoding() == StrIntrinsicNode::LL));\n-  match(Set result (StrIndexOf (Binary str1 cnt1) (Binary str2 int_cnt2)));\n-  effect(TEMP tmp_vec, USE_KILL str1, USE_KILL str2, USE_KILL cnt1, KILL cnt2, KILL tmp, KILL cr);\n-\n-  format %{ \"String IndexOf byte[] $str1,$cnt1,$str2,$int_cnt2 -> $result   \/\/ KILL $tmp_vec, $cnt1, $cnt2, $tmp\" %}\n-  ins_encode %{\n-    int icnt2 = (int)$int_cnt2$$constant;\n-    if (icnt2 >= 16) {\n-      \/\/ IndexOf for constant substrings with size >= 16 elements\n-      \/\/ which don't need to be loaded through stack.\n-      __ string_indexofC8($str1$$Register, $str2$$Register,\n-                          $cnt1$$Register, $cnt2$$Register,\n-                          icnt2, $result$$Register,\n-                          $tmp_vec$$XMMRegister, $tmp$$Register, StrIntrinsicNode::LL);\n-    } else {\n-      \/\/ Small strings are loaded through stack if they cross page boundary.\n-      __ string_indexof($str1$$Register, $str2$$Register,\n-                        $cnt1$$Register, $cnt2$$Register,\n-                        icnt2, $result$$Register,\n-                        $tmp_vec$$XMMRegister, $tmp$$Register, StrIntrinsicNode::LL);\n-    }\n-  %}\n-  ins_pipe( pipe_slow );\n-%}\n-\n-\/\/ fast search of substring with known size.\n-instruct string_indexof_conU(rdi_RegP str1, rdx_RegI cnt1, rsi_RegP str2, immI int_cnt2,\n-                             rbx_RegI result, legRegD tmp_vec, rax_RegI cnt2, rcx_RegI tmp, rFlagsReg cr)\n-%{\n-  predicate(UseSSE42Intrinsics && (((StrIndexOfNode*)n)->encoding() == StrIntrinsicNode::UU));\n-  match(Set result (StrIndexOf (Binary str1 cnt1) (Binary str2 int_cnt2)));\n-  effect(TEMP tmp_vec, USE_KILL str1, USE_KILL str2, USE_KILL cnt1, KILL cnt2, KILL tmp, KILL cr);\n-\n-  format %{ \"String IndexOf char[] $str1,$cnt1,$str2,$int_cnt2 -> $result   \/\/ KILL $tmp_vec, $cnt1, $cnt2, $tmp\" %}\n-  ins_encode %{\n-    int icnt2 = (int)$int_cnt2$$constant;\n-    if (icnt2 >= 8) {\n-      \/\/ IndexOf for constant substrings with size >= 8 elements\n-      \/\/ which don't need to be loaded through stack.\n-      __ string_indexofC8($str1$$Register, $str2$$Register,\n-                          $cnt1$$Register, $cnt2$$Register,\n-                          icnt2, $result$$Register,\n-                          $tmp_vec$$XMMRegister, $tmp$$Register, StrIntrinsicNode::UU);\n-    } else {\n-      \/\/ Small strings are loaded through stack if they cross page boundary.\n-      __ string_indexof($str1$$Register, $str2$$Register,\n-                        $cnt1$$Register, $cnt2$$Register,\n-                        icnt2, $result$$Register,\n-                        $tmp_vec$$XMMRegister, $tmp$$Register, StrIntrinsicNode::UU);\n-    }\n-  %}\n-  ins_pipe( pipe_slow );\n-%}\n-\n-\/\/ fast search of substring with known size.\n-instruct string_indexof_conUL(rdi_RegP str1, rdx_RegI cnt1, rsi_RegP str2, immI int_cnt2,\n-                              rbx_RegI result, legRegD tmp_vec, rax_RegI cnt2, rcx_RegI tmp, rFlagsReg cr)\n-%{\n-  predicate(UseSSE42Intrinsics && (((StrIndexOfNode*)n)->encoding() == StrIntrinsicNode::UL));\n-  match(Set result (StrIndexOf (Binary str1 cnt1) (Binary str2 int_cnt2)));\n-  effect(TEMP tmp_vec, USE_KILL str1, USE_KILL str2, USE_KILL cnt1, KILL cnt2, KILL tmp, KILL cr);\n-\n-  format %{ \"String IndexOf char[] $str1,$cnt1,$str2,$int_cnt2 -> $result   \/\/ KILL $tmp_vec, $cnt1, $cnt2, $tmp\" %}\n-  ins_encode %{\n-    int icnt2 = (int)$int_cnt2$$constant;\n-    if (icnt2 >= 8) {\n-      \/\/ IndexOf for constant substrings with size >= 8 elements\n-      \/\/ which don't need to be loaded through stack.\n-      __ string_indexofC8($str1$$Register, $str2$$Register,\n-                          $cnt1$$Register, $cnt2$$Register,\n-                          icnt2, $result$$Register,\n-                          $tmp_vec$$XMMRegister, $tmp$$Register, StrIntrinsicNode::UL);\n-    } else {\n-      \/\/ Small strings are loaded through stack if they cross page boundary.\n-      __ string_indexof($str1$$Register, $str2$$Register,\n-                        $cnt1$$Register, $cnt2$$Register,\n-                        icnt2, $result$$Register,\n-                        $tmp_vec$$XMMRegister, $tmp$$Register, StrIntrinsicNode::UL);\n-    }\n-  %}\n-  ins_pipe( pipe_slow );\n-%}\n-\n-instruct string_indexofL(rdi_RegP str1, rdx_RegI cnt1, rsi_RegP str2, rax_RegI cnt2,\n-                         rbx_RegI result, legRegD tmp_vec, rcx_RegI tmp, rFlagsReg cr)\n-%{\n-  predicate(UseSSE42Intrinsics && (((StrIndexOfNode*)n)->encoding() == StrIntrinsicNode::LL));\n-  match(Set result (StrIndexOf (Binary str1 cnt1) (Binary str2 cnt2)));\n-  effect(TEMP tmp_vec, USE_KILL str1, USE_KILL str2, USE_KILL cnt1, USE_KILL cnt2, KILL tmp, KILL cr);\n-\n-  format %{ \"String IndexOf byte[] $str1,$cnt1,$str2,$cnt2 -> $result   \/\/ KILL all\" %}\n-  ins_encode %{\n-    __ string_indexof($str1$$Register, $str2$$Register,\n-                      $cnt1$$Register, $cnt2$$Register,\n-                      (-1), $result$$Register,\n-                      $tmp_vec$$XMMRegister, $tmp$$Register, StrIntrinsicNode::LL);\n-  %}\n-  ins_pipe( pipe_slow );\n-%}\n-\n-instruct string_indexofU(rdi_RegP str1, rdx_RegI cnt1, rsi_RegP str2, rax_RegI cnt2,\n-                         rbx_RegI result, legRegD tmp_vec, rcx_RegI tmp, rFlagsReg cr)\n-%{\n-  predicate(UseSSE42Intrinsics && (((StrIndexOfNode*)n)->encoding() == StrIntrinsicNode::UU));\n-  match(Set result (StrIndexOf (Binary str1 cnt1) (Binary str2 cnt2)));\n-  effect(TEMP tmp_vec, USE_KILL str1, USE_KILL str2, USE_KILL cnt1, USE_KILL cnt2, KILL tmp, KILL cr);\n-\n-  format %{ \"String IndexOf char[] $str1,$cnt1,$str2,$cnt2 -> $result   \/\/ KILL all\" %}\n-  ins_encode %{\n-    __ string_indexof($str1$$Register, $str2$$Register,\n-                      $cnt1$$Register, $cnt2$$Register,\n-                      (-1), $result$$Register,\n-                      $tmp_vec$$XMMRegister, $tmp$$Register, StrIntrinsicNode::UU);\n-  %}\n-  ins_pipe( pipe_slow );\n-%}\n-\n-instruct string_indexofUL(rdi_RegP str1, rdx_RegI cnt1, rsi_RegP str2, rax_RegI cnt2,\n-                          rbx_RegI result, legRegD tmp_vec, rcx_RegI tmp, rFlagsReg cr)\n-%{\n-  predicate(UseSSE42Intrinsics && (((StrIndexOfNode*)n)->encoding() == StrIntrinsicNode::UL));\n-  match(Set result (StrIndexOf (Binary str1 cnt1) (Binary str2 cnt2)));\n-  effect(TEMP tmp_vec, USE_KILL str1, USE_KILL str2, USE_KILL cnt1, USE_KILL cnt2, KILL tmp, KILL cr);\n-\n-  format %{ \"String IndexOf char[] $str1,$cnt1,$str2,$cnt2 -> $result   \/\/ KILL all\" %}\n-  ins_encode %{\n-    __ string_indexof($str1$$Register, $str2$$Register,\n-                      $cnt1$$Register, $cnt2$$Register,\n-                      (-1), $result$$Register,\n-                      $tmp_vec$$XMMRegister, $tmp$$Register, StrIntrinsicNode::UL);\n-  %}\n-  ins_pipe( pipe_slow );\n-%}\n-\n-instruct string_indexof_char(rdi_RegP str1, rdx_RegI cnt1, rax_RegI ch,\n-                              rbx_RegI result, legRegD tmp_vec1, legRegD tmp_vec2, legRegD tmp_vec3, rcx_RegI tmp, rFlagsReg cr)\n-%{\n-  predicate(UseSSE42Intrinsics && (((StrIndexOfCharNode*)n)->encoding() == StrIntrinsicNode::U));\n-  match(Set result (StrIndexOfChar (Binary str1 cnt1) ch));\n-  effect(TEMP tmp_vec1, TEMP tmp_vec2, TEMP tmp_vec3, USE_KILL str1, USE_KILL cnt1, USE_KILL ch, TEMP tmp, KILL cr);\n-  format %{ \"StringUTF16 IndexOf char[] $str1,$cnt1,$ch -> $result   \/\/ KILL all\" %}\n-  ins_encode %{\n-    __ string_indexof_char($str1$$Register, $cnt1$$Register, $ch$$Register, $result$$Register,\n-                           $tmp_vec1$$XMMRegister, $tmp_vec2$$XMMRegister, $tmp_vec3$$XMMRegister, $tmp$$Register);\n-  %}\n-  ins_pipe( pipe_slow );\n-%}\n-\n-instruct stringL_indexof_char(rdi_RegP str1, rdx_RegI cnt1, rax_RegI ch,\n-                              rbx_RegI result, legRegD tmp_vec1, legRegD tmp_vec2, legRegD tmp_vec3, rcx_RegI tmp, rFlagsReg cr)\n-%{\n-  predicate(UseSSE42Intrinsics && (((StrIndexOfCharNode*)n)->encoding() == StrIntrinsicNode::L));\n-  match(Set result (StrIndexOfChar (Binary str1 cnt1) ch));\n-  effect(TEMP tmp_vec1, TEMP tmp_vec2, TEMP tmp_vec3, USE_KILL str1, USE_KILL cnt1, USE_KILL ch, TEMP tmp, KILL cr);\n-  format %{ \"StringLatin1 IndexOf char[] $str1,$cnt1,$ch -> $result   \/\/ KILL all\" %}\n-  ins_encode %{\n-    __ stringL_indexof_char($str1$$Register, $cnt1$$Register, $ch$$Register, $result$$Register,\n-                           $tmp_vec1$$XMMRegister, $tmp_vec2$$XMMRegister, $tmp_vec3$$XMMRegister, $tmp$$Register);\n-  %}\n-  ins_pipe( pipe_slow );\n-%}\n-\n-\/\/ fast string equals\n-instruct string_equals(rdi_RegP str1, rsi_RegP str2, rcx_RegI cnt, rax_RegI result,\n-                       legRegD tmp1, legRegD tmp2, rbx_RegI tmp3, rFlagsReg cr)\n-%{\n-  predicate(!VM_Version::supports_avx512vlbw());\n-  match(Set result (StrEquals (Binary str1 str2) cnt));\n-  effect(TEMP tmp1, TEMP tmp2, USE_KILL str1, USE_KILL str2, USE_KILL cnt, KILL tmp3, KILL cr);\n-\n-  format %{ \"String Equals $str1,$str2,$cnt -> $result    \/\/ KILL $tmp1, $tmp2, $tmp3\" %}\n-  ins_encode %{\n-    __ arrays_equals(false, $str1$$Register, $str2$$Register,\n-                     $cnt$$Register, $result$$Register, $tmp3$$Register,\n-                     $tmp1$$XMMRegister, $tmp2$$XMMRegister, false \/* char *\/, knoreg);\n-  %}\n-  ins_pipe( pipe_slow );\n-%}\n-\n-instruct string_equals_evex(rdi_RegP str1, rsi_RegP str2, rcx_RegI cnt, rax_RegI result,\n-                           legRegD tmp1, legRegD tmp2, kReg ktmp, rbx_RegI tmp3, rFlagsReg cr)\n-%{\n-  predicate(VM_Version::supports_avx512vlbw());\n-  match(Set result (StrEquals (Binary str1 str2) cnt));\n-  effect(TEMP tmp1, TEMP tmp2, TEMP ktmp, USE_KILL str1, USE_KILL str2, USE_KILL cnt, KILL tmp3, KILL cr);\n-\n-  format %{ \"String Equals $str1,$str2,$cnt -> $result    \/\/ KILL $tmp1, $tmp2, $tmp3\" %}\n-  ins_encode %{\n-    __ arrays_equals(false, $str1$$Register, $str2$$Register,\n-                     $cnt$$Register, $result$$Register, $tmp3$$Register,\n-                     $tmp1$$XMMRegister, $tmp2$$XMMRegister, false \/* char *\/, $ktmp$$KRegister);\n-  %}\n-  ins_pipe( pipe_slow );\n-%}\n-\n-\/\/ fast array equals\n-instruct array_equalsB(rdi_RegP ary1, rsi_RegP ary2, rax_RegI result,\n-                       legRegD tmp1, legRegD tmp2, rcx_RegI tmp3, rbx_RegI tmp4, rFlagsReg cr)\n-%{\n-  predicate(!VM_Version::supports_avx512vlbw() && ((AryEqNode*)n)->encoding() == StrIntrinsicNode::LL);\n-  match(Set result (AryEq ary1 ary2));\n-  effect(TEMP tmp1, TEMP tmp2, USE_KILL ary1, USE_KILL ary2, KILL tmp3, KILL tmp4, KILL cr);\n-\n-  format %{ \"Array Equals byte[] $ary1,$ary2 -> $result   \/\/ KILL $tmp1, $tmp2, $tmp3, $tmp4\" %}\n-  ins_encode %{\n-    __ arrays_equals(true, $ary1$$Register, $ary2$$Register,\n-                     $tmp3$$Register, $result$$Register, $tmp4$$Register,\n-                     $tmp1$$XMMRegister, $tmp2$$XMMRegister, false \/* char *\/, knoreg);\n-  %}\n-  ins_pipe( pipe_slow );\n-%}\n-\n-instruct array_equalsB_evex(rdi_RegP ary1, rsi_RegP ary2, rax_RegI result,\n-                            legRegD tmp1, legRegD tmp2, kReg ktmp, rcx_RegI tmp3, rbx_RegI tmp4, rFlagsReg cr)\n-%{\n-  predicate(VM_Version::supports_avx512vlbw() && ((AryEqNode*)n)->encoding() == StrIntrinsicNode::LL);\n-  match(Set result (AryEq ary1 ary2));\n-  effect(TEMP tmp1, TEMP tmp2, TEMP ktmp, USE_KILL ary1, USE_KILL ary2, KILL tmp3, KILL tmp4, KILL cr);\n-\n-  format %{ \"Array Equals byte[] $ary1,$ary2 -> $result   \/\/ KILL $tmp1, $tmp2, $tmp3, $tmp4\" %}\n-  ins_encode %{\n-    __ arrays_equals(true, $ary1$$Register, $ary2$$Register,\n-                     $tmp3$$Register, $result$$Register, $tmp4$$Register,\n-                     $tmp1$$XMMRegister, $tmp2$$XMMRegister, false \/* char *\/, $ktmp$$KRegister);\n-  %}\n-  ins_pipe( pipe_slow );\n-%}\n-\n-instruct array_equalsC(rdi_RegP ary1, rsi_RegP ary2, rax_RegI result,\n-                       legRegD tmp1, legRegD tmp2, rcx_RegI tmp3, rbx_RegI tmp4, rFlagsReg cr)\n-%{\n-  predicate(!VM_Version::supports_avx512vlbw() && ((AryEqNode*)n)->encoding() == StrIntrinsicNode::UU);\n-  match(Set result (AryEq ary1 ary2));\n-  effect(TEMP tmp1, TEMP tmp2, USE_KILL ary1, USE_KILL ary2, KILL tmp3, KILL tmp4, KILL cr);\n-\n-  format %{ \"Array Equals char[] $ary1,$ary2 -> $result   \/\/ KILL $tmp1, $tmp2, $tmp3, $tmp4\" %}\n-  ins_encode %{\n-    __ arrays_equals(true, $ary1$$Register, $ary2$$Register,\n-                     $tmp3$$Register, $result$$Register, $tmp4$$Register,\n-                     $tmp1$$XMMRegister, $tmp2$$XMMRegister, true \/* char *\/, knoreg);\n-  %}\n-  ins_pipe( pipe_slow );\n-%}\n-\n-instruct array_equalsC_evex(rdi_RegP ary1, rsi_RegP ary2, rax_RegI result,\n-                            legRegD tmp1, legRegD tmp2, kReg ktmp, rcx_RegI tmp3, rbx_RegI tmp4, rFlagsReg cr)\n-%{\n-  predicate(VM_Version::supports_avx512vlbw() && ((AryEqNode*)n)->encoding() == StrIntrinsicNode::UU);\n-  match(Set result (AryEq ary1 ary2));\n-  effect(TEMP tmp1, TEMP tmp2, TEMP ktmp, USE_KILL ary1, USE_KILL ary2, KILL tmp3, KILL tmp4, KILL cr);\n-\n-  format %{ \"Array Equals char[] $ary1,$ary2 -> $result   \/\/ KILL $tmp1, $tmp2, $tmp3, $tmp4\" %}\n-  ins_encode %{\n-    __ arrays_equals(true, $ary1$$Register, $ary2$$Register,\n-                     $tmp3$$Register, $result$$Register, $tmp4$$Register,\n-                     $tmp1$$XMMRegister, $tmp2$$XMMRegister, true \/* char *\/, $ktmp$$KRegister);\n-  %}\n-  ins_pipe( pipe_slow );\n-%}\n-\n-instruct arrays_hashcode(rdi_RegP ary1, rdx_RegI cnt1, rbx_RegI result, immU8 basic_type,\n-                         legRegD tmp_vec1, legRegD tmp_vec2, legRegD tmp_vec3, legRegD tmp_vec4,\n-                         legRegD tmp_vec5, legRegD tmp_vec6, legRegD tmp_vec7, legRegD tmp_vec8,\n-                         legRegD tmp_vec9, legRegD tmp_vec10, legRegD tmp_vec11, legRegD tmp_vec12,\n-                         legRegD tmp_vec13, rRegI tmp1, rRegI tmp2, rRegI tmp3, rFlagsReg cr)\n-%{\n-  predicate(UseAVX >= 2);\n-  match(Set result (VectorizedHashCode (Binary ary1 cnt1) (Binary result basic_type)));\n-  effect(TEMP tmp_vec1, TEMP tmp_vec2, TEMP tmp_vec3, TEMP tmp_vec4, TEMP tmp_vec5, TEMP tmp_vec6,\n-         TEMP tmp_vec7, TEMP tmp_vec8, TEMP tmp_vec9, TEMP tmp_vec10, TEMP tmp_vec11, TEMP tmp_vec12,\n-         TEMP tmp_vec13, TEMP tmp1, TEMP tmp2, TEMP tmp3, USE_KILL ary1, USE_KILL cnt1,\n-         USE basic_type, KILL cr);\n-\n-  format %{ \"Array HashCode array[] $ary1,$cnt1,$result,$basic_type -> $result   \/\/ KILL all\" %}\n-  ins_encode %{\n-    __ arrays_hashcode($ary1$$Register, $cnt1$$Register, $result$$Register,\n-                       $tmp1$$Register, $tmp2$$Register, $tmp3$$Register,\n-                       $tmp_vec1$$XMMRegister, $tmp_vec2$$XMMRegister, $tmp_vec3$$XMMRegister,\n-                       $tmp_vec4$$XMMRegister, $tmp_vec5$$XMMRegister, $tmp_vec6$$XMMRegister,\n-                       $tmp_vec7$$XMMRegister, $tmp_vec8$$XMMRegister, $tmp_vec9$$XMMRegister,\n-                       $tmp_vec10$$XMMRegister, $tmp_vec11$$XMMRegister, $tmp_vec12$$XMMRegister,\n-                       $tmp_vec13$$XMMRegister, (BasicType)$basic_type$$constant);\n-  %}\n-  ins_pipe( pipe_slow );\n-%}\n-\n-instruct count_positives(rsi_RegP ary1, rcx_RegI len, rax_RegI result,\n-                         legRegD tmp1, legRegD tmp2, rbx_RegI tmp3, rFlagsReg cr,)\n-%{\n-  predicate(!VM_Version::supports_avx512vlbw() || !VM_Version::supports_bmi2());\n-  match(Set result (CountPositives ary1 len));\n-  effect(TEMP tmp1, TEMP tmp2, USE_KILL ary1, USE_KILL len, KILL tmp3, KILL cr);\n-\n-  format %{ \"countPositives byte[] $ary1,$len -> $result   \/\/ KILL $tmp1, $tmp2, $tmp3\" %}\n-  ins_encode %{\n-    __ count_positives($ary1$$Register, $len$$Register,\n-                       $result$$Register, $tmp3$$Register,\n-                       $tmp1$$XMMRegister, $tmp2$$XMMRegister, knoreg, knoreg);\n-  %}\n-  ins_pipe( pipe_slow );\n-%}\n-\n-instruct count_positives_evex(rsi_RegP ary1, rcx_RegI len, rax_RegI result,\n-                              legRegD tmp1, legRegD tmp2, kReg ktmp1, kReg ktmp2, rbx_RegI tmp3, rFlagsReg cr,)\n-%{\n-  predicate(VM_Version::supports_avx512vlbw() && VM_Version::supports_bmi2());\n-  match(Set result (CountPositives ary1 len));\n-  effect(TEMP tmp1, TEMP tmp2, TEMP ktmp1, TEMP ktmp2, USE_KILL ary1, USE_KILL len, KILL tmp3, KILL cr);\n-\n-  format %{ \"countPositives byte[] $ary1,$len -> $result   \/\/ KILL $tmp1, $tmp2, $tmp3\" %}\n-  ins_encode %{\n-    __ count_positives($ary1$$Register, $len$$Register,\n-                       $result$$Register, $tmp3$$Register,\n-                       $tmp1$$XMMRegister, $tmp2$$XMMRegister, $ktmp1$$KRegister, $ktmp2$$KRegister);\n-  %}\n-  ins_pipe( pipe_slow );\n-%}\n-\n-\/\/ fast char[] to byte[] compression\n-instruct string_compress(rsi_RegP src, rdi_RegP dst, rdx_RegI len, legRegD tmp1, legRegD tmp2, legRegD tmp3,\n-                         legRegD tmp4, rcx_RegI tmp5, rax_RegI result, rFlagsReg cr) %{\n-  predicate(!VM_Version::supports_avx512vlbw() || !VM_Version::supports_bmi2());\n-  match(Set result (StrCompressedCopy src (Binary dst len)));\n-  effect(TEMP tmp1, TEMP tmp2, TEMP tmp3, TEMP tmp4, USE_KILL src, USE_KILL dst,\n-         USE_KILL len, KILL tmp5, KILL cr);\n-\n-  format %{ \"String Compress $src,$dst -> $result    \/\/ KILL RAX, RCX, RDX\" %}\n-  ins_encode %{\n-    __ char_array_compress($src$$Register, $dst$$Register, $len$$Register,\n-                           $tmp1$$XMMRegister, $tmp2$$XMMRegister, $tmp3$$XMMRegister,\n-                           $tmp4$$XMMRegister, $tmp5$$Register, $result$$Register,\n-                           knoreg, knoreg);\n-  %}\n-  ins_pipe( pipe_slow );\n-%}\n-\n-instruct string_compress_evex(rsi_RegP src, rdi_RegP dst, rdx_RegI len, legRegD tmp1, legRegD tmp2, legRegD tmp3,\n-                              legRegD tmp4, kReg ktmp1, kReg ktmp2, rcx_RegI tmp5, rax_RegI result, rFlagsReg cr) %{\n-  predicate(VM_Version::supports_avx512vlbw() && VM_Version::supports_bmi2());\n-  match(Set result (StrCompressedCopy src (Binary dst len)));\n-  effect(TEMP tmp1, TEMP tmp2, TEMP tmp3, TEMP tmp4, TEMP ktmp1, TEMP ktmp2, USE_KILL src, USE_KILL dst,\n-         USE_KILL len, KILL tmp5, KILL cr);\n-\n-  format %{ \"String Compress $src,$dst -> $result    \/\/ KILL RAX, RCX, RDX\" %}\n-  ins_encode %{\n-    __ char_array_compress($src$$Register, $dst$$Register, $len$$Register,\n-                           $tmp1$$XMMRegister, $tmp2$$XMMRegister, $tmp3$$XMMRegister,\n-                           $tmp4$$XMMRegister, $tmp5$$Register, $result$$Register,\n-                           $ktmp1$$KRegister, $ktmp2$$KRegister);\n-  %}\n-  ins_pipe( pipe_slow );\n-%}\n-\/\/ fast byte[] to char[] inflation\n-instruct string_inflate(Universe dummy, rsi_RegP src, rdi_RegP dst, rdx_RegI len,\n-                        legRegD tmp1, rcx_RegI tmp2, rFlagsReg cr) %{\n-  predicate(!VM_Version::supports_avx512vlbw() || !VM_Version::supports_bmi2());\n-  match(Set dummy (StrInflatedCopy src (Binary dst len)));\n-  effect(TEMP tmp1, TEMP tmp2, USE_KILL src, USE_KILL dst, USE_KILL len, KILL cr);\n-\n-  format %{ \"String Inflate $src,$dst    \/\/ KILL $tmp1, $tmp2\" %}\n-  ins_encode %{\n-    __ byte_array_inflate($src$$Register, $dst$$Register, $len$$Register,\n-                          $tmp1$$XMMRegister, $tmp2$$Register, knoreg);\n-  %}\n-  ins_pipe( pipe_slow );\n-%}\n-\n-instruct string_inflate_evex(Universe dummy, rsi_RegP src, rdi_RegP dst, rdx_RegI len,\n-                             legRegD tmp1, kReg ktmp, rcx_RegI tmp2, rFlagsReg cr) %{\n-  predicate(VM_Version::supports_avx512vlbw() && VM_Version::supports_bmi2());\n-  match(Set dummy (StrInflatedCopy src (Binary dst len)));\n-  effect(TEMP tmp1, TEMP tmp2, TEMP ktmp, USE_KILL src, USE_KILL dst, USE_KILL len, KILL cr);\n-\n-  format %{ \"String Inflate $src,$dst    \/\/ KILL $tmp1, $tmp2\" %}\n-  ins_encode %{\n-    __ byte_array_inflate($src$$Register, $dst$$Register, $len$$Register,\n-                          $tmp1$$XMMRegister, $tmp2$$Register, $ktmp$$KRegister);\n-  %}\n-  ins_pipe( pipe_slow );\n-%}\n-\n-\/\/ encode char[] to byte[] in ISO_8859_1\n-instruct encode_iso_array(rsi_RegP src, rdi_RegP dst, rdx_RegI len,\n-                          legRegD tmp1, legRegD tmp2, legRegD tmp3, legRegD tmp4,\n-                          rcx_RegI tmp5, rax_RegI result, rFlagsReg cr) %{\n-  predicate(!((EncodeISOArrayNode*)n)->is_ascii());\n-  match(Set result (EncodeISOArray src (Binary dst len)));\n-  effect(TEMP tmp1, TEMP tmp2, TEMP tmp3, TEMP tmp4, USE_KILL src, USE_KILL dst, USE_KILL len, KILL tmp5, KILL cr);\n-\n-  format %{ \"Encode iso array $src,$dst,$len -> $result    \/\/ KILL RCX, RDX, $tmp1, $tmp2, $tmp3, $tmp4, RSI, RDI \" %}\n-  ins_encode %{\n-    __ encode_iso_array($src$$Register, $dst$$Register, $len$$Register,\n-                        $tmp1$$XMMRegister, $tmp2$$XMMRegister, $tmp3$$XMMRegister,\n-                        $tmp4$$XMMRegister, $tmp5$$Register, $result$$Register, false);\n-  %}\n-  ins_pipe( pipe_slow );\n-%}\n-\n-\/\/ encode char[] to byte[] in ASCII\n-instruct encode_ascii_array(rsi_RegP src, rdi_RegP dst, rdx_RegI len,\n-                            legRegD tmp1, legRegD tmp2, legRegD tmp3, legRegD tmp4,\n-                            rcx_RegI tmp5, rax_RegI result, rFlagsReg cr) %{\n-  predicate(((EncodeISOArrayNode*)n)->is_ascii());\n-  match(Set result (EncodeISOArray src (Binary dst len)));\n-  effect(TEMP tmp1, TEMP tmp2, TEMP tmp3, TEMP tmp4, USE_KILL src, USE_KILL dst, USE_KILL len, KILL tmp5, KILL cr);\n-\n-  format %{ \"Encode ascii array $src,$dst,$len -> $result    \/\/ KILL RCX, RDX, $tmp1, $tmp2, $tmp3, $tmp4, RSI, RDI \" %}\n-  ins_encode %{\n-    __ encode_iso_array($src$$Register, $dst$$Register, $len$$Register,\n-                        $tmp1$$XMMRegister, $tmp2$$XMMRegister, $tmp3$$XMMRegister,\n-                        $tmp4$$XMMRegister, $tmp5$$Register, $result$$Register, true);\n-  %}\n-  ins_pipe( pipe_slow );\n-%}\n-\n-\/\/----------Overflow Math Instructions-----------------------------------------\n-\n-instruct overflowAddI_rReg(rFlagsReg cr, rax_RegI op1, rRegI op2)\n-%{\n-  match(Set cr (OverflowAddI op1 op2));\n-  effect(DEF cr, USE_KILL op1, USE op2);\n-\n-  format %{ \"addl    $op1, $op2\\t# overflow check int\" %}\n-\n-  ins_encode %{\n-    __ addl($op1$$Register, $op2$$Register);\n-  %}\n-  ins_pipe(ialu_reg_reg);\n-%}\n-\n-instruct overflowAddI_rReg_imm(rFlagsReg cr, rax_RegI op1, immI op2)\n-%{\n-  match(Set cr (OverflowAddI op1 op2));\n-  effect(DEF cr, USE_KILL op1, USE op2);\n-\n-  format %{ \"addl    $op1, $op2\\t# overflow check int\" %}\n-\n-  ins_encode %{\n-    __ addl($op1$$Register, $op2$$constant);\n-  %}\n-  ins_pipe(ialu_reg_reg);\n-%}\n-\n-instruct overflowAddL_rReg(rFlagsReg cr, rax_RegL op1, rRegL op2)\n-%{\n-  match(Set cr (OverflowAddL op1 op2));\n-  effect(DEF cr, USE_KILL op1, USE op2);\n-\n-  format %{ \"addq    $op1, $op2\\t# overflow check long\" %}\n-  ins_encode %{\n-    __ addq($op1$$Register, $op2$$Register);\n-  %}\n-  ins_pipe(ialu_reg_reg);\n-%}\n-\n-instruct overflowAddL_rReg_imm(rFlagsReg cr, rax_RegL op1, immL32 op2)\n-%{\n-  match(Set cr (OverflowAddL op1 op2));\n-  effect(DEF cr, USE_KILL op1, USE op2);\n-\n-  format %{ \"addq    $op1, $op2\\t# overflow check long\" %}\n-  ins_encode %{\n-    __ addq($op1$$Register, $op2$$constant);\n-  %}\n-  ins_pipe(ialu_reg_reg);\n-%}\n-\n-instruct overflowSubI_rReg(rFlagsReg cr, rRegI op1, rRegI op2)\n-%{\n-  match(Set cr (OverflowSubI op1 op2));\n-\n-  format %{ \"cmpl    $op1, $op2\\t# overflow check int\" %}\n-  ins_encode %{\n-    __ cmpl($op1$$Register, $op2$$Register);\n-  %}\n-  ins_pipe(ialu_reg_reg);\n-%}\n-\n-instruct overflowSubI_rReg_imm(rFlagsReg cr, rRegI op1, immI op2)\n-%{\n-  match(Set cr (OverflowSubI op1 op2));\n-\n-  format %{ \"cmpl    $op1, $op2\\t# overflow check int\" %}\n-  ins_encode %{\n-    __ cmpl($op1$$Register, $op2$$constant);\n-  %}\n-  ins_pipe(ialu_reg_reg);\n-%}\n-\n-instruct overflowSubL_rReg(rFlagsReg cr, rRegL op1, rRegL op2)\n-%{\n-  match(Set cr (OverflowSubL op1 op2));\n-\n-  format %{ \"cmpq    $op1, $op2\\t# overflow check long\" %}\n-  ins_encode %{\n-    __ cmpq($op1$$Register, $op2$$Register);\n-  %}\n-  ins_pipe(ialu_reg_reg);\n-%}\n-\n-instruct overflowSubL_rReg_imm(rFlagsReg cr, rRegL op1, immL32 op2)\n-%{\n-  match(Set cr (OverflowSubL op1 op2));\n-\n-  format %{ \"cmpq    $op1, $op2\\t# overflow check long\" %}\n-  ins_encode %{\n-    __ cmpq($op1$$Register, $op2$$constant);\n-  %}\n-  ins_pipe(ialu_reg_reg);\n-%}\n-\n-instruct overflowNegI_rReg(rFlagsReg cr, immI_0 zero, rax_RegI op2)\n-%{\n-  match(Set cr (OverflowSubI zero op2));\n-  effect(DEF cr, USE_KILL op2);\n-\n-  format %{ \"negl    $op2\\t# overflow check int\" %}\n-  ins_encode %{\n-    __ negl($op2$$Register);\n-  %}\n-  ins_pipe(ialu_reg_reg);\n-%}\n-\n-instruct overflowNegL_rReg(rFlagsReg cr, immL0 zero, rax_RegL op2)\n-%{\n-  match(Set cr (OverflowSubL zero op2));\n-  effect(DEF cr, USE_KILL op2);\n-\n-  format %{ \"negq    $op2\\t# overflow check long\" %}\n-  ins_encode %{\n-    __ negq($op2$$Register);\n-  %}\n-  ins_pipe(ialu_reg_reg);\n-%}\n-\n-instruct overflowMulI_rReg(rFlagsReg cr, rax_RegI op1, rRegI op2)\n-%{\n-  match(Set cr (OverflowMulI op1 op2));\n-  effect(DEF cr, USE_KILL op1, USE op2);\n-\n-  format %{ \"imull    $op1, $op2\\t# overflow check int\" %}\n-  ins_encode %{\n-    __ imull($op1$$Register, $op2$$Register);\n-  %}\n-  ins_pipe(ialu_reg_reg_alu0);\n-%}\n-\n-instruct overflowMulI_rReg_imm(rFlagsReg cr, rRegI op1, immI op2, rRegI tmp)\n-%{\n-  match(Set cr (OverflowMulI op1 op2));\n-  effect(DEF cr, TEMP tmp, USE op1, USE op2);\n-\n-  format %{ \"imull    $tmp, $op1, $op2\\t# overflow check int\" %}\n-  ins_encode %{\n-    __ imull($tmp$$Register, $op1$$Register, $op2$$constant);\n-  %}\n-  ins_pipe(ialu_reg_reg_alu0);\n-%}\n-\n-instruct overflowMulL_rReg(rFlagsReg cr, rax_RegL op1, rRegL op2)\n-%{\n-  match(Set cr (OverflowMulL op1 op2));\n-  effect(DEF cr, USE_KILL op1, USE op2);\n-\n-  format %{ \"imulq    $op1, $op2\\t# overflow check long\" %}\n-  ins_encode %{\n-    __ imulq($op1$$Register, $op2$$Register);\n-  %}\n-  ins_pipe(ialu_reg_reg_alu0);\n-%}\n-\n-instruct overflowMulL_rReg_imm(rFlagsReg cr, rRegL op1, immL32 op2, rRegL tmp)\n-%{\n-  match(Set cr (OverflowMulL op1 op2));\n-  effect(DEF cr, TEMP tmp, USE op1, USE op2);\n-\n-  format %{ \"imulq    $tmp, $op1, $op2\\t# overflow check long\" %}\n-  ins_encode %{\n-    __ imulq($tmp$$Register, $op1$$Register, $op2$$constant);\n-  %}\n-  ins_pipe(ialu_reg_reg_alu0);\n-%}\n-\n-\n-\/\/----------Control Flow Instructions------------------------------------------\n-\/\/ Signed compare Instructions\n-\n-\/\/ XXX more variants!!\n-instruct compI_rReg(rFlagsReg cr, rRegI op1, rRegI op2)\n-%{\n-  match(Set cr (CmpI op1 op2));\n-  effect(DEF cr, USE op1, USE op2);\n-\n-  format %{ \"cmpl    $op1, $op2\" %}\n-  ins_encode %{\n-    __ cmpl($op1$$Register, $op2$$Register);\n-  %}\n-  ins_pipe(ialu_cr_reg_reg);\n-%}\n-\n-instruct compI_rReg_imm(rFlagsReg cr, rRegI op1, immI op2)\n-%{\n-  match(Set cr (CmpI op1 op2));\n-\n-  format %{ \"cmpl    $op1, $op2\" %}\n-  ins_encode %{\n-    __ cmpl($op1$$Register, $op2$$constant);\n-  %}\n-  ins_pipe(ialu_cr_reg_imm);\n-%}\n-\n-instruct compI_rReg_mem(rFlagsReg cr, rRegI op1, memory op2)\n-%{\n-  match(Set cr (CmpI op1 (LoadI op2)));\n-\n-  ins_cost(500); \/\/ XXX\n-  format %{ \"cmpl    $op1, $op2\" %}\n-  ins_encode %{\n-    __ cmpl($op1$$Register, $op2$$Address);\n-  %}\n-  ins_pipe(ialu_cr_reg_mem);\n-%}\n-\n-instruct testI_reg(rFlagsReg cr, rRegI src, immI_0 zero)\n-%{\n-  match(Set cr (CmpI src zero));\n-\n-  format %{ \"testl   $src, $src\" %}\n-  ins_encode %{\n-    __ testl($src$$Register, $src$$Register);\n-  %}\n-  ins_pipe(ialu_cr_reg_imm);\n-%}\n-\n-instruct testI_reg_imm(rFlagsReg cr, rRegI src, immI con, immI_0 zero)\n-%{\n-  match(Set cr (CmpI (AndI src con) zero));\n-\n-  format %{ \"testl   $src, $con\" %}\n-  ins_encode %{\n-    __ testl($src$$Register, $con$$constant);\n-  %}\n-  ins_pipe(ialu_cr_reg_imm);\n-%}\n-\n-instruct testI_reg_reg(rFlagsReg cr, rRegI src1, rRegI src2, immI_0 zero)\n-%{\n-  match(Set cr (CmpI (AndI src1 src2) zero));\n-\n-  format %{ \"testl   $src1, $src2\" %}\n-  ins_encode %{\n-    __ testl($src1$$Register, $src2$$Register);\n-  %}\n-  ins_pipe(ialu_cr_reg_imm);\n-%}\n-\n-instruct testI_reg_mem(rFlagsReg cr, rRegI src, memory mem, immI_0 zero)\n-%{\n-  match(Set cr (CmpI (AndI src (LoadI mem)) zero));\n-\n-  format %{ \"testl   $src, $mem\" %}\n-  ins_encode %{\n-    __ testl($src$$Register, $mem$$Address);\n-  %}\n-  ins_pipe(ialu_cr_reg_mem);\n-%}\n-\n-\/\/ Unsigned compare Instructions; really, same as signed except they\n-\/\/ produce an rFlagsRegU instead of rFlagsReg.\n-instruct compU_rReg(rFlagsRegU cr, rRegI op1, rRegI op2)\n-%{\n-  match(Set cr (CmpU op1 op2));\n-\n-  format %{ \"cmpl    $op1, $op2\\t# unsigned\" %}\n-  ins_encode %{\n-    __ cmpl($op1$$Register, $op2$$Register);\n-  %}\n-  ins_pipe(ialu_cr_reg_reg);\n-%}\n-\n-instruct compU_rReg_imm(rFlagsRegU cr, rRegI op1, immI op2)\n-%{\n-  match(Set cr (CmpU op1 op2));\n-\n-  format %{ \"cmpl    $op1, $op2\\t# unsigned\" %}\n-  ins_encode %{\n-    __ cmpl($op1$$Register, $op2$$constant);\n-  %}\n-  ins_pipe(ialu_cr_reg_imm);\n-%}\n-\n-instruct compU_rReg_mem(rFlagsRegU cr, rRegI op1, memory op2)\n-%{\n-  match(Set cr (CmpU op1 (LoadI op2)));\n-\n-  ins_cost(500); \/\/ XXX\n-  format %{ \"cmpl    $op1, $op2\\t# unsigned\" %}\n-  ins_encode %{\n-    __ cmpl($op1$$Register, $op2$$Address);\n-  %}\n-  ins_pipe(ialu_cr_reg_mem);\n-%}\n-\n-instruct testU_reg(rFlagsRegU cr, rRegI src, immI_0 zero)\n-%{\n-  match(Set cr (CmpU src zero));\n-\n-  format %{ \"testl   $src, $src\\t# unsigned\" %}\n-  ins_encode %{\n-    __ testl($src$$Register, $src$$Register);\n-  %}\n-  ins_pipe(ialu_cr_reg_imm);\n-%}\n-\n-instruct compP_rReg(rFlagsRegU cr, rRegP op1, rRegP op2)\n-%{\n-  match(Set cr (CmpP op1 op2));\n-\n-  format %{ \"cmpq    $op1, $op2\\t# ptr\" %}\n-  ins_encode %{\n-    __ cmpq($op1$$Register, $op2$$Register);\n-  %}\n-  ins_pipe(ialu_cr_reg_reg);\n-%}\n-\n-instruct compP_rReg_mem(rFlagsRegU cr, rRegP op1, memory op2)\n-%{\n-  match(Set cr (CmpP op1 (LoadP op2)));\n-  predicate(n->in(2)->as_Load()->barrier_data() == 0);\n-\n-  ins_cost(500); \/\/ XXX\n-  format %{ \"cmpq    $op1, $op2\\t# ptr\" %}\n-  ins_encode %{\n-    __ cmpq($op1$$Register, $op2$$Address);\n-  %}\n-  ins_pipe(ialu_cr_reg_mem);\n-%}\n-\n-\/\/ XXX this is generalized by compP_rReg_mem???\n-\/\/ Compare raw pointer (used in out-of-heap check).\n-\/\/ Only works because non-oop pointers must be raw pointers\n-\/\/ and raw pointers have no anti-dependencies.\n-instruct compP_mem_rReg(rFlagsRegU cr, rRegP op1, memory op2)\n-%{\n-  predicate(n->in(2)->in(2)->bottom_type()->reloc() == relocInfo::none &&\n-            n->in(2)->as_Load()->barrier_data() == 0);\n-  match(Set cr (CmpP op1 (LoadP op2)));\n-\n-  format %{ \"cmpq    $op1, $op2\\t# raw ptr\" %}\n-  ins_encode %{\n-    __ cmpq($op1$$Register, $op2$$Address);\n-  %}\n-  ins_pipe(ialu_cr_reg_mem);\n-%}\n-\n-\/\/ This will generate a signed flags result. This should be OK since\n-\/\/ any compare to a zero should be eq\/neq.\n-instruct testP_reg(rFlagsReg cr, rRegP src, immP0 zero)\n-%{\n-  match(Set cr (CmpP src zero));\n-\n-  format %{ \"testq   $src, $src\\t# ptr\" %}\n-  ins_encode %{\n-    __ testq($src$$Register, $src$$Register);\n-  %}\n-  ins_pipe(ialu_cr_reg_imm);\n-%}\n-\n-\/\/ This will generate a signed flags result. This should be OK since\n-\/\/ any compare to a zero should be eq\/neq.\n-instruct testP_mem(rFlagsReg cr, memory op, immP0 zero)\n-%{\n-  predicate((!UseCompressedOops || (CompressedOops::base() != nullptr)) &&\n-            n->in(1)->as_Load()->barrier_data() == 0);\n-  match(Set cr (CmpP (LoadP op) zero));\n-\n-  ins_cost(500); \/\/ XXX\n-  format %{ \"testq   $op, 0xffffffffffffffff\\t# ptr\" %}\n-  ins_encode %{\n-    __ testq($op$$Address, 0xFFFFFFFF);\n-  %}\n-  ins_pipe(ialu_cr_reg_imm);\n-%}\n-\n-instruct testP_mem_reg0(rFlagsReg cr, memory mem, immP0 zero)\n-%{\n-  predicate(UseCompressedOops && (CompressedOops::base() == nullptr) &&\n-            n->in(1)->as_Load()->barrier_data() == 0);\n-  match(Set cr (CmpP (LoadP mem) zero));\n-\n-  format %{ \"cmpq    R12, $mem\\t# ptr (R12_heapbase==0)\" %}\n-  ins_encode %{\n-    __ cmpq(r12, $mem$$Address);\n-  %}\n-  ins_pipe(ialu_cr_reg_mem);\n-%}\n-\n-instruct compN_rReg(rFlagsRegU cr, rRegN op1, rRegN op2)\n-%{\n-  match(Set cr (CmpN op1 op2));\n-\n-  format %{ \"cmpl    $op1, $op2\\t# compressed ptr\" %}\n-  ins_encode %{ __ cmpl($op1$$Register, $op2$$Register); %}\n-  ins_pipe(ialu_cr_reg_reg);\n-%}\n-\n-instruct compN_rReg_mem(rFlagsRegU cr, rRegN src, memory mem)\n-%{\n-  predicate(n->in(2)->as_Load()->barrier_data() == 0);\n-  match(Set cr (CmpN src (LoadN mem)));\n-\n-  format %{ \"cmpl    $src, $mem\\t# compressed ptr\" %}\n-  ins_encode %{\n-    __ cmpl($src$$Register, $mem$$Address);\n-  %}\n-  ins_pipe(ialu_cr_reg_mem);\n-%}\n-\n-instruct compN_rReg_imm(rFlagsRegU cr, rRegN op1, immN op2) %{\n-  match(Set cr (CmpN op1 op2));\n-\n-  format %{ \"cmpl    $op1, $op2\\t# compressed ptr\" %}\n-  ins_encode %{\n-    __ cmp_narrow_oop($op1$$Register, (jobject)$op2$$constant);\n-  %}\n-  ins_pipe(ialu_cr_reg_imm);\n-%}\n-\n-instruct compN_mem_imm(rFlagsRegU cr, memory mem, immN src)\n-%{\n-  predicate(n->in(2)->as_Load()->barrier_data() == 0);\n-  match(Set cr (CmpN src (LoadN mem)));\n-\n-  format %{ \"cmpl    $mem, $src\\t# compressed ptr\" %}\n-  ins_encode %{\n-    __ cmp_narrow_oop($mem$$Address, (jobject)$src$$constant);\n-  %}\n-  ins_pipe(ialu_cr_reg_mem);\n-%}\n-\n-instruct compN_rReg_imm_klass(rFlagsRegU cr, rRegN op1, immNKlass op2) %{\n-  match(Set cr (CmpN op1 op2));\n-\n-  format %{ \"cmpl    $op1, $op2\\t# compressed klass ptr\" %}\n-  ins_encode %{\n-    __ cmp_narrow_klass($op1$$Register, (Klass*)$op2$$constant);\n-  %}\n-  ins_pipe(ialu_cr_reg_imm);\n-%}\n-\n-instruct compN_mem_imm_klass(rFlagsRegU cr, memory mem, immNKlass src)\n-%{\n-  predicate(!UseCompactObjectHeaders);\n-  match(Set cr (CmpN src (LoadNKlass mem)));\n-\n-  format %{ \"cmpl    $mem, $src\\t# compressed klass ptr\" %}\n-  ins_encode %{\n-    __ cmp_narrow_klass($mem$$Address, (Klass*)$src$$constant);\n-  %}\n-  ins_pipe(ialu_cr_reg_mem);\n-%}\n-\n-instruct testN_reg(rFlagsReg cr, rRegN src, immN0 zero) %{\n-  match(Set cr (CmpN src zero));\n-\n-  format %{ \"testl   $src, $src\\t# compressed ptr\" %}\n-  ins_encode %{ __ testl($src$$Register, $src$$Register); %}\n-  ins_pipe(ialu_cr_reg_imm);\n-%}\n-\n-instruct testN_mem(rFlagsReg cr, memory mem, immN0 zero)\n-%{\n-  predicate(CompressedOops::base() != nullptr &&\n-            n->in(1)->as_Load()->barrier_data() == 0);\n-  match(Set cr (CmpN (LoadN mem) zero));\n-\n-  ins_cost(500); \/\/ XXX\n-  format %{ \"testl   $mem, 0xffffffff\\t# compressed ptr\" %}\n-  ins_encode %{\n-    __ cmpl($mem$$Address, (int)0xFFFFFFFF);\n-  %}\n-  ins_pipe(ialu_cr_reg_mem);\n-%}\n-\n-instruct testN_mem_reg0(rFlagsReg cr, memory mem, immN0 zero)\n-%{\n-  predicate(CompressedOops::base() == nullptr &&\n-            n->in(1)->as_Load()->barrier_data() == 0);\n-  match(Set cr (CmpN (LoadN mem) zero));\n-\n-  format %{ \"cmpl    R12, $mem\\t# compressed ptr (R12_heapbase==0)\" %}\n-  ins_encode %{\n-    __ cmpl(r12, $mem$$Address);\n-  %}\n-  ins_pipe(ialu_cr_reg_mem);\n-%}\n-\n-\/\/ Yanked all unsigned pointer compare operations.\n-\/\/ Pointer compares are done with CmpP which is already unsigned.\n-\n-instruct compL_rReg(rFlagsReg cr, rRegL op1, rRegL op2)\n-%{\n-  match(Set cr (CmpL op1 op2));\n-\n-  format %{ \"cmpq    $op1, $op2\" %}\n-  ins_encode %{\n-    __ cmpq($op1$$Register, $op2$$Register);\n-  %}\n-  ins_pipe(ialu_cr_reg_reg);\n-%}\n-\n-instruct compL_rReg_imm(rFlagsReg cr, rRegL op1, immL32 op2)\n-%{\n-  match(Set cr (CmpL op1 op2));\n-\n-  format %{ \"cmpq    $op1, $op2\" %}\n-  ins_encode %{\n-    __ cmpq($op1$$Register, $op2$$constant);\n-  %}\n-  ins_pipe(ialu_cr_reg_imm);\n-%}\n-\n-instruct compL_rReg_mem(rFlagsReg cr, rRegL op1, memory op2)\n-%{\n-  match(Set cr (CmpL op1 (LoadL op2)));\n-\n-  format %{ \"cmpq    $op1, $op2\" %}\n-  ins_encode %{\n-    __ cmpq($op1$$Register, $op2$$Address);\n-  %}\n-  ins_pipe(ialu_cr_reg_mem);\n-%}\n-\n-instruct testL_reg(rFlagsReg cr, rRegL src, immL0 zero)\n-%{\n-  match(Set cr (CmpL src zero));\n-\n-  format %{ \"testq   $src, $src\" %}\n-  ins_encode %{\n-    __ testq($src$$Register, $src$$Register);\n-  %}\n-  ins_pipe(ialu_cr_reg_imm);\n-%}\n-\n-instruct testL_reg_imm(rFlagsReg cr, rRegL src, immL32 con, immL0 zero)\n-%{\n-  match(Set cr (CmpL (AndL src con) zero));\n-\n-  format %{ \"testq   $src, $con\\t# long\" %}\n-  ins_encode %{\n-    __ testq($src$$Register, $con$$constant);\n-  %}\n-  ins_pipe(ialu_cr_reg_imm);\n-%}\n-\n-instruct testL_reg_reg(rFlagsReg cr, rRegL src1, rRegL src2, immL0 zero)\n-%{\n-  match(Set cr (CmpL (AndL src1 src2) zero));\n-\n-  format %{ \"testq   $src1, $src2\\t# long\" %}\n-  ins_encode %{\n-    __ testq($src1$$Register, $src2$$Register);\n-  %}\n-  ins_pipe(ialu_cr_reg_imm);\n-%}\n-\n-instruct testL_reg_mem(rFlagsReg cr, rRegL src, memory mem, immL0 zero)\n-%{\n-  match(Set cr (CmpL (AndL src (LoadL mem)) zero));\n-\n-  format %{ \"testq   $src, $mem\" %}\n-  ins_encode %{\n-    __ testq($src$$Register, $mem$$Address);\n-  %}\n-  ins_pipe(ialu_cr_reg_mem);\n-%}\n-\n-instruct testL_reg_mem2(rFlagsReg cr, rRegP src, memory mem, immL0 zero)\n-%{\n-  match(Set cr (CmpL (AndL (CastP2X src) (LoadL mem)) zero));\n-\n-  format %{ \"testq   $src, $mem\" %}\n-  ins_encode %{\n-    __ testq($src$$Register, $mem$$Address);\n-  %}\n-  ins_pipe(ialu_cr_reg_mem);\n-%}\n-\n-\/\/ Manifest a CmpU result in an integer register.  Very painful.\n-\/\/ This is the test to avoid.\n-instruct cmpU3_reg_reg(rRegI dst, rRegI src1, rRegI src2, rFlagsReg flags)\n-%{\n-  match(Set dst (CmpU3 src1 src2));\n-  effect(KILL flags);\n-\n-  ins_cost(275); \/\/ XXX\n-  format %{ \"cmpl    $src1, $src2\\t# CmpL3\\n\\t\"\n-            \"movl    $dst, -1\\n\\t\"\n-            \"jb,u    done\\n\\t\"\n-            \"setcc   $dst \\t# emits setne + movzbl or setzune for APX\"\n-    \"done:\" %}\n-  ins_encode %{\n-    Label done;\n-    __ cmpl($src1$$Register, $src2$$Register);\n-    __ movl($dst$$Register, -1);\n-    __ jccb(Assembler::below, done);\n-    __ setcc(Assembler::notZero, $dst$$Register);\n-    __ bind(done);\n-  %}\n-  ins_pipe(pipe_slow);\n-%}\n-\n-\/\/ Manifest a CmpL result in an integer register.  Very painful.\n-\/\/ This is the test to avoid.\n-instruct cmpL3_reg_reg(rRegI dst, rRegL src1, rRegL src2, rFlagsReg flags)\n-%{\n-  match(Set dst (CmpL3 src1 src2));\n-  effect(KILL flags);\n-\n-  ins_cost(275); \/\/ XXX\n-  format %{ \"cmpq    $src1, $src2\\t# CmpL3\\n\\t\"\n-            \"movl    $dst, -1\\n\\t\"\n-            \"jl,s    done\\n\\t\"\n-            \"setcc   $dst \\t# emits setne + movzbl or setzune for APX\"\n-    \"done:\" %}\n-  ins_encode %{\n-    Label done;\n-    __ cmpq($src1$$Register, $src2$$Register);\n-    __ movl($dst$$Register, -1);\n-    __ jccb(Assembler::less, done);\n-    __ setcc(Assembler::notZero, $dst$$Register);\n-    __ bind(done);\n-  %}\n-  ins_pipe(pipe_slow);\n-%}\n-\n-\/\/ Manifest a CmpUL result in an integer register.  Very painful.\n-\/\/ This is the test to avoid.\n-instruct cmpUL3_reg_reg(rRegI dst, rRegL src1, rRegL src2, rFlagsReg flags)\n-%{\n-  match(Set dst (CmpUL3 src1 src2));\n-  effect(KILL flags);\n-\n-  ins_cost(275); \/\/ XXX\n-  format %{ \"cmpq    $src1, $src2\\t# CmpL3\\n\\t\"\n-            \"movl    $dst, -1\\n\\t\"\n-            \"jb,u    done\\n\\t\"\n-            \"setcc   $dst \\t# emits setne + movzbl or setzune for APX\"\n-    \"done:\" %}\n-  ins_encode %{\n-    Label done;\n-    __ cmpq($src1$$Register, $src2$$Register);\n-    __ movl($dst$$Register, -1);\n-    __ jccb(Assembler::below, done);\n-    __ setcc(Assembler::notZero, $dst$$Register);\n-    __ bind(done);\n-  %}\n-  ins_pipe(pipe_slow);\n-%}\n-\n-\/\/ Unsigned long compare Instructions; really, same as signed long except they\n-\/\/ produce an rFlagsRegU instead of rFlagsReg.\n-instruct compUL_rReg(rFlagsRegU cr, rRegL op1, rRegL op2)\n-%{\n-  match(Set cr (CmpUL op1 op2));\n-\n-  format %{ \"cmpq    $op1, $op2\\t# unsigned\" %}\n-  ins_encode %{\n-    __ cmpq($op1$$Register, $op2$$Register);\n-  %}\n-  ins_pipe(ialu_cr_reg_reg);\n-%}\n-\n-instruct compUL_rReg_imm(rFlagsRegU cr, rRegL op1, immL32 op2)\n-%{\n-  match(Set cr (CmpUL op1 op2));\n-\n-  format %{ \"cmpq    $op1, $op2\\t# unsigned\" %}\n-  ins_encode %{\n-    __ cmpq($op1$$Register, $op2$$constant);\n-  %}\n-  ins_pipe(ialu_cr_reg_imm);\n-%}\n-\n-instruct compUL_rReg_mem(rFlagsRegU cr, rRegL op1, memory op2)\n-%{\n-  match(Set cr (CmpUL op1 (LoadL op2)));\n-\n-  format %{ \"cmpq    $op1, $op2\\t# unsigned\" %}\n-  ins_encode %{\n-    __ cmpq($op1$$Register, $op2$$Address);\n-  %}\n-  ins_pipe(ialu_cr_reg_mem);\n-%}\n-\n-instruct testUL_reg(rFlagsRegU cr, rRegL src, immL0 zero)\n-%{\n-  match(Set cr (CmpUL src zero));\n-\n-  format %{ \"testq   $src, $src\\t# unsigned\" %}\n-  ins_encode %{\n-    __ testq($src$$Register, $src$$Register);\n-  %}\n-  ins_pipe(ialu_cr_reg_imm);\n-%}\n-\n-instruct compB_mem_imm(rFlagsReg cr, memory mem, immI8 imm)\n-%{\n-  match(Set cr (CmpI (LoadB mem) imm));\n-\n-  ins_cost(125);\n-  format %{ \"cmpb    $mem, $imm\" %}\n-  ins_encode %{ __ cmpb($mem$$Address, $imm$$constant); %}\n-  ins_pipe(ialu_cr_reg_mem);\n-%}\n-\n-instruct testUB_mem_imm(rFlagsReg cr, memory mem, immU7 imm, immI_0 zero)\n-%{\n-  match(Set cr (CmpI (AndI (LoadUB mem) imm) zero));\n-\n-  ins_cost(125);\n-  format %{ \"testb   $mem, $imm\\t# ubyte\" %}\n-  ins_encode %{ __ testb($mem$$Address, $imm$$constant); %}\n-  ins_pipe(ialu_cr_reg_mem);\n-%}\n-\n-instruct testB_mem_imm(rFlagsReg cr, memory mem, immI8 imm, immI_0 zero)\n-%{\n-  match(Set cr (CmpI (AndI (LoadB mem) imm) zero));\n-\n-  ins_cost(125);\n-  format %{ \"testb   $mem, $imm\\t# byte\" %}\n-  ins_encode %{ __ testb($mem$$Address, $imm$$constant); %}\n-  ins_pipe(ialu_cr_reg_mem);\n-%}\n-\n-\/\/----------Max and Min--------------------------------------------------------\n-\/\/ Min Instructions\n-\n-instruct cmovI_reg_g(rRegI dst, rRegI src, rFlagsReg cr)\n-%{\n-  predicate(!UseAPX);\n-  effect(USE_DEF dst, USE src, USE cr);\n-\n-  format %{ \"cmovlgt $dst, $src\\t# min\" %}\n-  ins_encode %{\n-    __ cmovl(Assembler::greater, $dst$$Register, $src$$Register);\n-  %}\n-  ins_pipe(pipe_cmov_reg);\n-%}\n-\n-instruct cmovI_reg_g_ndd(rRegI dst, rRegI src1, rRegI src2, rFlagsReg cr)\n-%{\n-  predicate(UseAPX);\n-  effect(DEF dst, USE src1, USE src2, USE cr);\n-\n-  format %{ \"ecmovlgt $dst, $src1, $src2\\t# min ndd\" %}\n-  ins_encode %{\n-    __ ecmovl(Assembler::greater, $dst$$Register, $src1$$Register, $src2$$Register);\n-  %}\n-  ins_pipe(pipe_cmov_reg);\n-%}\n-\n-instruct minI_rReg(rRegI dst, rRegI src)\n-%{\n-  predicate(!UseAPX);\n-  match(Set dst (MinI dst src));\n-\n-  ins_cost(200);\n-  expand %{\n-    rFlagsReg cr;\n-    compI_rReg(cr, dst, src);\n-    cmovI_reg_g(dst, src, cr);\n-  %}\n-%}\n-\n-instruct minI_rReg_ndd(rRegI dst, rRegI src1, rRegI src2)\n-%{\n-  predicate(UseAPX);\n-  match(Set dst (MinI src1 src2));\n-  effect(DEF dst, USE src1, USE src2);\n-\n-  ins_cost(200);\n-  expand %{\n-    rFlagsReg cr;\n-    compI_rReg(cr, src1, src2);\n-    cmovI_reg_g_ndd(dst, src1, src2, cr);\n-  %}\n-%}\n-\n-instruct cmovI_reg_l(rRegI dst, rRegI src, rFlagsReg cr)\n-%{\n-  predicate(!UseAPX);\n-  effect(USE_DEF dst, USE src, USE cr);\n-\n-  format %{ \"cmovllt $dst, $src\\t# max\" %}\n-  ins_encode %{\n-    __ cmovl(Assembler::less, $dst$$Register, $src$$Register);\n-  %}\n-  ins_pipe(pipe_cmov_reg);\n-%}\n-\n-instruct cmovI_reg_l_ndd(rRegI dst, rRegI src1, rRegI src2, rFlagsReg cr)\n-%{\n-  predicate(UseAPX);\n-  effect(DEF dst, USE src1, USE src2, USE cr);\n-\n-  format %{ \"ecmovllt $dst, $src1, $src2\\t# max ndd\" %}\n-  ins_encode %{\n-    __ ecmovl(Assembler::less, $dst$$Register, $src1$$Register, $src2$$Register);\n-  %}\n-  ins_pipe(pipe_cmov_reg);\n-%}\n-\n-instruct maxI_rReg(rRegI dst, rRegI src)\n-%{\n-  predicate(!UseAPX);\n-  match(Set dst (MaxI dst src));\n-\n-  ins_cost(200);\n-  expand %{\n-    rFlagsReg cr;\n-    compI_rReg(cr, dst, src);\n-    cmovI_reg_l(dst, src, cr);\n-  %}\n-%}\n-\n-instruct maxI_rReg_ndd(rRegI dst, rRegI src1, rRegI src2)\n-%{\n-  predicate(UseAPX);\n-  match(Set dst (MaxI src1 src2));\n-  effect(DEF dst, USE src1, USE src2);\n-\n-  ins_cost(200);\n-  expand %{\n-    rFlagsReg cr;\n-    compI_rReg(cr, src1, src2);\n-    cmovI_reg_l_ndd(dst, src1, src2, cr);\n-  %}\n-%}\n-\n-\/\/ ============================================================================\n-\/\/ Branch Instructions\n-\n-\/\/ Jump Direct - Label defines a relative address from JMP+1\n-instruct jmpDir(label labl)\n-%{\n-  match(Goto);\n-  effect(USE labl);\n-\n-  ins_cost(300);\n-  format %{ \"jmp     $labl\" %}\n-  size(5);\n-  ins_encode %{\n-    Label* L = $labl$$label;\n-    __ jmp(*L, false); \/\/ Always long jump\n-  %}\n-  ins_pipe(pipe_jmp);\n-%}\n-\n-\/\/ Jump Direct Conditional - Label defines a relative address from Jcc+1\n-instruct jmpCon(cmpOp cop, rFlagsReg cr, label labl)\n-%{\n-  match(If cop cr);\n-  effect(USE labl);\n-\n-  ins_cost(300);\n-  format %{ \"j$cop     $labl\" %}\n-  size(6);\n-  ins_encode %{\n-    Label* L = $labl$$label;\n-    __ jcc((Assembler::Condition)($cop$$cmpcode), *L, false); \/\/ Always long jump\n-  %}\n-  ins_pipe(pipe_jcc);\n-%}\n-\n-\/\/ Jump Direct Conditional - Label defines a relative address from Jcc+1\n-instruct jmpLoopEnd(cmpOp cop, rFlagsReg cr, label labl)\n-%{\n-  match(CountedLoopEnd cop cr);\n-  effect(USE labl);\n-\n-  ins_cost(300);\n-  format %{ \"j$cop     $labl\\t# loop end\" %}\n-  size(6);\n-  ins_encode %{\n-    Label* L = $labl$$label;\n-    __ jcc((Assembler::Condition)($cop$$cmpcode), *L, false); \/\/ Always long jump\n-  %}\n-  ins_pipe(pipe_jcc);\n-%}\n-\n-\/\/ Jump Direct Conditional - using unsigned comparison\n-instruct jmpConU(cmpOpU cop, rFlagsRegU cmp, label labl) %{\n-  match(If cop cmp);\n-  effect(USE labl);\n-\n-  ins_cost(300);\n-  format %{ \"j$cop,u   $labl\" %}\n-  size(6);\n-  ins_encode %{\n-    Label* L = $labl$$label;\n-    __ jcc((Assembler::Condition)($cop$$cmpcode), *L, false); \/\/ Always long jump\n-  %}\n-  ins_pipe(pipe_jcc);\n-%}\n-\n-instruct jmpConUCF(cmpOpUCF cop, rFlagsRegUCF cmp, label labl) %{\n-  match(If cop cmp);\n-  effect(USE labl);\n-\n-  ins_cost(200);\n-  format %{ \"j$cop,u   $labl\" %}\n-  size(6);\n-  ins_encode %{\n-    Label* L = $labl$$label;\n-    __ jcc((Assembler::Condition)($cop$$cmpcode), *L, false); \/\/ Always long jump\n-  %}\n-  ins_pipe(pipe_jcc);\n-%}\n-\n-instruct jmpConUCF2(cmpOpUCF2 cop, rFlagsRegUCF cmp, label labl) %{\n-  match(If cop cmp);\n-  effect(USE labl);\n-\n-  ins_cost(200);\n-  format %{ $$template\n-    if ($cop$$cmpcode == Assembler::notEqual) {\n-      $$emit$$\"jp,u    $labl\\n\\t\"\n-      $$emit$$\"j$cop,u   $labl\"\n-    } else {\n-      $$emit$$\"jp,u    done\\n\\t\"\n-      $$emit$$\"j$cop,u   $labl\\n\\t\"\n-      $$emit$$\"done:\"\n-    }\n-  %}\n-  ins_encode %{\n-    Label* l = $labl$$label;\n-    if ($cop$$cmpcode == Assembler::notEqual) {\n-      __ jcc(Assembler::parity, *l, false);\n-      __ jcc(Assembler::notEqual, *l, false);\n-    } else if ($cop$$cmpcode == Assembler::equal) {\n-      Label done;\n-      __ jccb(Assembler::parity, done);\n-      __ jcc(Assembler::equal, *l, false);\n-      __ bind(done);\n-    } else {\n-       ShouldNotReachHere();\n-    }\n-  %}\n-  ins_pipe(pipe_jcc);\n-%}\n-\n-\/\/ ============================================================================\n-\/\/ The 2nd slow-half of a subtype check.  Scan the subklass's 2ndary\n-\/\/ superklass array for an instance of the superklass.  Set a hidden\n-\/\/ internal cache on a hit (cache is checked with exposed code in\n-\/\/ gen_subtype_check()).  Return NZ for a miss or zero for a hit.  The\n-\/\/ encoding ALSO sets flags.\n-\n-instruct partialSubtypeCheck(rdi_RegP result,\n-                             rsi_RegP sub, rax_RegP super, rcx_RegI rcx,\n-                             rFlagsReg cr)\n-%{\n-  match(Set result (PartialSubtypeCheck sub super));\n-  predicate(!UseSecondarySupersTable);\n-  effect(KILL rcx, KILL cr);\n-\n-  ins_cost(1100);  \/\/ slightly larger than the next version\n-  format %{ \"movq    rdi, [$sub + in_bytes(Klass::secondary_supers_offset())]\\n\\t\"\n-            \"movl    rcx, [rdi + Array<Klass*>::length_offset_in_bytes()]\\t# length to scan\\n\\t\"\n-            \"addq    rdi, Array<Klass*>::base_offset_in_bytes()\\t# Skip to start of data; set NZ in case count is zero\\n\\t\"\n-            \"repne   scasq\\t# Scan *rdi++ for a match with rax while rcx--\\n\\t\"\n-            \"jne,s   miss\\t\\t# Missed: rdi not-zero\\n\\t\"\n-            \"movq    [$sub + in_bytes(Klass::secondary_super_cache_offset())], $super\\t# Hit: update cache\\n\\t\"\n-            \"xorq    $result, $result\\t\\t Hit: rdi zero\\n\\t\"\n-    \"miss:\\t\" %}\n-\n-  ins_encode %{\n-    Label miss;\n-    \/\/ NB: Callers may assume that, when $result is a valid register,\n-    \/\/ check_klass_subtype_slow_path_linear sets it to a nonzero\n-    \/\/ value.\n-    __ check_klass_subtype_slow_path_linear($sub$$Register, $super$$Register,\n-                                            $rcx$$Register, $result$$Register,\n-                                            nullptr, &miss,\n-                                            \/*set_cond_codes:*\/ true);\n-    __ xorptr($result$$Register, $result$$Register);\n-    __ bind(miss);\n-  %}\n-\n-  ins_pipe(pipe_slow);\n-%}\n-\n-\/\/ ============================================================================\n-\/\/ Two versions of hashtable-based partialSubtypeCheck, both used when\n-\/\/ we need to search for a super class in the secondary supers array.\n-\/\/ The first is used when we don't know _a priori_ the class being\n-\/\/ searched for. The second, far more common, is used when we do know:\n-\/\/ this is used for instanceof, checkcast, and any case where C2 can\n-\/\/ determine it by constant propagation.\n-\n-instruct partialSubtypeCheckVarSuper(rsi_RegP sub, rax_RegP super, rdi_RegP result,\n-                                       rdx_RegL temp1, rcx_RegL temp2, rbx_RegP temp3, r11_RegL temp4,\n-                                       rFlagsReg cr)\n-%{\n-  match(Set result (PartialSubtypeCheck sub super));\n-  predicate(UseSecondarySupersTable);\n-  effect(KILL cr, TEMP temp1, TEMP temp2, TEMP temp3, TEMP temp4);\n-\n-  ins_cost(1000);\n-  format %{ \"partialSubtypeCheck $result, $sub, $super\" %}\n-\n-  ins_encode %{\n-    __ lookup_secondary_supers_table_var($sub$$Register, $super$$Register, $temp1$$Register, $temp2$$Register,\n-\t\t\t\t\t $temp3$$Register, $temp4$$Register, $result$$Register);\n-  %}\n-\n-  ins_pipe(pipe_slow);\n-%}\n-\n-instruct partialSubtypeCheckConstSuper(rsi_RegP sub, rax_RegP super_reg, immP super_con, rdi_RegP result,\n-                                       rdx_RegL temp1, rcx_RegL temp2, rbx_RegP temp3, r11_RegL temp4,\n-                                       rFlagsReg cr)\n-%{\n-  match(Set result (PartialSubtypeCheck sub (Binary super_reg super_con)));\n-  predicate(UseSecondarySupersTable);\n-  effect(KILL cr, TEMP temp1, TEMP temp2, TEMP temp3, TEMP temp4);\n-\n-  ins_cost(700);  \/\/ smaller than the next version\n-  format %{ \"partialSubtypeCheck $result, $sub, $super_reg, $super_con\" %}\n-\n-  ins_encode %{\n-    u1 super_klass_slot = ((Klass*)$super_con$$constant)->hash_slot();\n-    if (InlineSecondarySupersTest) {\n-      __ lookup_secondary_supers_table_const($sub$$Register, $super_reg$$Register, $temp1$$Register, $temp2$$Register,\n-                                       $temp3$$Register, $temp4$$Register, $result$$Register,\n-                                       super_klass_slot);\n-    } else {\n-      __ call(RuntimeAddress(StubRoutines::lookup_secondary_supers_table_stub(super_klass_slot)));\n-    }\n-  %}\n-\n-  ins_pipe(pipe_slow);\n-%}\n-\n-\/\/ ============================================================================\n-\/\/ Branch Instructions -- short offset versions\n-\/\/\n-\/\/ These instructions are used to replace jumps of a long offset (the default\n-\/\/ match) with jumps of a shorter offset.  These instructions are all tagged\n-\/\/ with the ins_short_branch attribute, which causes the ADLC to suppress the\n-\/\/ match rules in general matching.  Instead, the ADLC generates a conversion\n-\/\/ method in the MachNode which can be used to do in-place replacement of the\n-\/\/ long variant with the shorter variant.  The compiler will determine if a\n-\/\/ branch can be taken by the is_short_branch_offset() predicate in the machine\n-\/\/ specific code section of the file.\n-\n-\/\/ Jump Direct - Label defines a relative address from JMP+1\n-instruct jmpDir_short(label labl) %{\n-  match(Goto);\n-  effect(USE labl);\n-\n-  ins_cost(300);\n-  format %{ \"jmp,s   $labl\" %}\n-  size(2);\n-  ins_encode %{\n-    Label* L = $labl$$label;\n-    __ jmpb(*L);\n-  %}\n-  ins_pipe(pipe_jmp);\n-  ins_short_branch(1);\n-%}\n-\n-\/\/ Jump Direct Conditional - Label defines a relative address from Jcc+1\n-instruct jmpCon_short(cmpOp cop, rFlagsReg cr, label labl) %{\n-  match(If cop cr);\n-  effect(USE labl);\n-\n-  ins_cost(300);\n-  format %{ \"j$cop,s   $labl\" %}\n-  size(2);\n-  ins_encode %{\n-    Label* L = $labl$$label;\n-    __ jccb((Assembler::Condition)($cop$$cmpcode), *L);\n-  %}\n-  ins_pipe(pipe_jcc);\n-  ins_short_branch(1);\n-%}\n-\n-\/\/ Jump Direct Conditional - Label defines a relative address from Jcc+1\n-instruct jmpLoopEnd_short(cmpOp cop, rFlagsReg cr, label labl) %{\n-  match(CountedLoopEnd cop cr);\n-  effect(USE labl);\n-\n-  ins_cost(300);\n-  format %{ \"j$cop,s   $labl\\t# loop end\" %}\n-  size(2);\n-  ins_encode %{\n-    Label* L = $labl$$label;\n-    __ jccb((Assembler::Condition)($cop$$cmpcode), *L);\n-  %}\n-  ins_pipe(pipe_jcc);\n-  ins_short_branch(1);\n-%}\n-\n-\/\/ Jump Direct Conditional - using unsigned comparison\n-instruct jmpConU_short(cmpOpU cop, rFlagsRegU cmp, label labl) %{\n-  match(If cop cmp);\n-  effect(USE labl);\n-\n-  ins_cost(300);\n-  format %{ \"j$cop,us  $labl\" %}\n-  size(2);\n-  ins_encode %{\n-    Label* L = $labl$$label;\n-    __ jccb((Assembler::Condition)($cop$$cmpcode), *L);\n-  %}\n-  ins_pipe(pipe_jcc);\n-  ins_short_branch(1);\n-%}\n-\n-instruct jmpConUCF_short(cmpOpUCF cop, rFlagsRegUCF cmp, label labl) %{\n-  match(If cop cmp);\n-  effect(USE labl);\n-\n-  ins_cost(300);\n-  format %{ \"j$cop,us  $labl\" %}\n-  size(2);\n-  ins_encode %{\n-    Label* L = $labl$$label;\n-    __ jccb((Assembler::Condition)($cop$$cmpcode), *L);\n-  %}\n-  ins_pipe(pipe_jcc);\n-  ins_short_branch(1);\n-%}\n-\n-instruct jmpConUCF2_short(cmpOpUCF2 cop, rFlagsRegUCF cmp, label labl) %{\n-  match(If cop cmp);\n-  effect(USE labl);\n-\n-  ins_cost(300);\n-  format %{ $$template\n-    if ($cop$$cmpcode == Assembler::notEqual) {\n-      $$emit$$\"jp,u,s  $labl\\n\\t\"\n-      $$emit$$\"j$cop,u,s  $labl\"\n-    } else {\n-      $$emit$$\"jp,u,s  done\\n\\t\"\n-      $$emit$$\"j$cop,u,s  $labl\\n\\t\"\n-      $$emit$$\"done:\"\n-    }\n-  %}\n-  size(4);\n-  ins_encode %{\n-    Label* l = $labl$$label;\n-    if ($cop$$cmpcode == Assembler::notEqual) {\n-      __ jccb(Assembler::parity, *l);\n-      __ jccb(Assembler::notEqual, *l);\n-    } else if ($cop$$cmpcode == Assembler::equal) {\n-      Label done;\n-      __ jccb(Assembler::parity, done);\n-      __ jccb(Assembler::equal, *l);\n-      __ bind(done);\n-    } else {\n-       ShouldNotReachHere();\n-    }\n-  %}\n-  ins_pipe(pipe_jcc);\n-  ins_short_branch(1);\n-%}\n-\n-\/\/ ============================================================================\n-\/\/ inlined locking and unlocking\n-\n-instruct cmpFastLockLightweight(rFlagsReg cr, rRegP object, rbx_RegP box, rax_RegI rax_reg, rRegP tmp) %{\n-  match(Set cr (FastLock object box));\n-  effect(TEMP rax_reg, TEMP tmp, USE_KILL box);\n-  ins_cost(300);\n-  format %{ \"fastlock $object,$box\\t! kills $box,$rax_reg,$tmp\" %}\n-  ins_encode %{\n-    __ fast_lock_lightweight($object$$Register, $box$$Register, $rax_reg$$Register, $tmp$$Register, r15_thread);\n-  %}\n-  ins_pipe(pipe_slow);\n-%}\n-\n-instruct cmpFastUnlockLightweight(rFlagsReg cr, rRegP object, rax_RegP rax_reg, rRegP tmp) %{\n-  match(Set cr (FastUnlock object rax_reg));\n-  effect(TEMP tmp, USE_KILL rax_reg);\n-  ins_cost(300);\n-  format %{ \"fastunlock $object,$rax_reg\\t! kills $rax_reg,$tmp\" %}\n-  ins_encode %{\n-    __ fast_unlock_lightweight($object$$Register, $rax_reg$$Register, $tmp$$Register, r15_thread);\n-  %}\n-  ins_pipe(pipe_slow);\n-%}\n-\n-\n-\/\/ ============================================================================\n-\/\/ Safepoint Instructions\n-instruct safePoint_poll_tls(rFlagsReg cr, rRegP poll)\n-%{\n-  match(SafePoint poll);\n-  effect(KILL cr, USE poll);\n-\n-  format %{ \"testl   rax, [$poll]\\t\"\n-            \"# Safepoint: poll for GC\" %}\n-  ins_cost(125);\n-  ins_encode %{\n-    __ relocate(relocInfo::poll_type);\n-    address pre_pc = __ pc();\n-    __ testl(rax, Address($poll$$Register, 0));\n-    assert(nativeInstruction_at(pre_pc)->is_safepoint_poll(), \"must emit test %%eax [reg]\");\n-  %}\n-  ins_pipe(ialu_reg_mem);\n-%}\n-\n-instruct mask_all_evexL(kReg dst, rRegL src) %{\n-  match(Set dst (MaskAll src));\n-  format %{ \"mask_all_evexL $dst, $src \\t! mask all operation\" %}\n-  ins_encode %{\n-    int mask_len = Matcher::vector_length(this);\n-    __ vector_maskall_operation($dst$$KRegister, $src$$Register, mask_len);\n-  %}\n-  ins_pipe( pipe_slow );\n-%}\n-\n-instruct mask_all_evexI_GT32(kReg dst, rRegI src, rRegL tmp) %{\n-  predicate(Matcher::vector_length(n) > 32);\n-  match(Set dst (MaskAll src));\n-  effect(TEMP tmp);\n-  format %{ \"mask_all_evexI_GT32 $dst, $src \\t! using $tmp as TEMP\" %}\n-  ins_encode %{\n-    int mask_len = Matcher::vector_length(this);\n-    __ movslq($tmp$$Register, $src$$Register);\n-    __ vector_maskall_operation($dst$$KRegister, $tmp$$Register, mask_len);\n-  %}\n-  ins_pipe( pipe_slow );\n-%}\n-\n-\/\/ ============================================================================\n-\/\/ Procedure Call\/Return Instructions\n-\/\/ Call Java Static Instruction\n-\/\/ Note: If this code changes, the corresponding ret_addr_offset() and\n-\/\/       compute_padding() functions will have to be adjusted.\n-instruct CallStaticJavaDirect(method meth) %{\n-  match(CallStaticJava);\n-  effect(USE meth);\n-\n-  ins_cost(300);\n-  format %{ \"call,static \" %}\n-  opcode(0xE8); \/* E8 cd *\/\n-  ins_encode(clear_avx, Java_Static_Call(meth), call_epilog);\n-  ins_pipe(pipe_slow);\n-  ins_alignment(4);\n-%}\n-\n-\/\/ Call Java Dynamic Instruction\n-\/\/ Note: If this code changes, the corresponding ret_addr_offset() and\n-\/\/       compute_padding() functions will have to be adjusted.\n-instruct CallDynamicJavaDirect(method meth)\n-%{\n-  match(CallDynamicJava);\n-  effect(USE meth);\n-\n-  ins_cost(300);\n-  format %{ \"movq    rax, #Universe::non_oop_word()\\n\\t\"\n-            \"call,dynamic \" %}\n-  ins_encode(clear_avx, Java_Dynamic_Call(meth), call_epilog);\n-  ins_pipe(pipe_slow);\n-  ins_alignment(4);\n-%}\n-\n-\/\/ Call Runtime Instruction\n-instruct CallRuntimeDirect(method meth)\n-%{\n-  match(CallRuntime);\n-  effect(USE meth);\n-\n-  ins_cost(300);\n-  format %{ \"call,runtime \" %}\n-  ins_encode(clear_avx, Java_To_Runtime(meth));\n-  ins_pipe(pipe_slow);\n-%}\n-\n-\/\/ Call runtime without safepoint\n-instruct CallLeafDirect(method meth)\n-%{\n-  match(CallLeaf);\n-  effect(USE meth);\n-\n-  ins_cost(300);\n-  format %{ \"call_leaf,runtime \" %}\n-  ins_encode(clear_avx, Java_To_Runtime(meth));\n-  ins_pipe(pipe_slow);\n-%}\n-\n-\/\/ Call runtime without safepoint and with vector arguments\n-instruct CallLeafDirectVector(method meth)\n-%{\n-  match(CallLeafVector);\n-  effect(USE meth);\n-\n-  ins_cost(300);\n-  format %{ \"call_leaf,vector \" %}\n-  ins_encode(Java_To_Runtime(meth));\n-  ins_pipe(pipe_slow);\n-%}\n-\n-\/\/ Call runtime without safepoint\n-instruct CallLeafNoFPDirect(method meth)\n-%{\n-  match(CallLeafNoFP);\n-  effect(USE meth);\n-\n-  ins_cost(300);\n-  format %{ \"call_leaf_nofp,runtime \" %}\n-  ins_encode(clear_avx, Java_To_Runtime(meth));\n-  ins_pipe(pipe_slow);\n-%}\n-\n-\/\/ Return Instruction\n-\/\/ Remove the return address & jump to it.\n-\/\/ Notice: We always emit a nop after a ret to make sure there is room\n-\/\/ for safepoint patching\n-instruct Ret()\n-%{\n-  match(Return);\n-\n-  format %{ \"ret\" %}\n-  ins_encode %{\n-    __ ret(0);\n-  %}\n-  ins_pipe(pipe_jmp);\n-%}\n-\n-\/\/ Tail Call; Jump from runtime stub to Java code.\n-\/\/ Also known as an 'interprocedural jump'.\n-\/\/ Target of jump will eventually return to caller.\n-\/\/ TailJump below removes the return address.\n-\/\/ Don't use rbp for 'jump_target' because a MachEpilogNode has already been\n-\/\/ emitted just above the TailCall which has reset rbp to the caller state.\n-instruct TailCalljmpInd(no_rbp_RegP jump_target, rbx_RegP method_ptr)\n-%{\n-  match(TailCall jump_target method_ptr);\n-\n-  ins_cost(300);\n-  format %{ \"jmp     $jump_target\\t# rbx holds method\" %}\n-  ins_encode %{\n-    __ jmp($jump_target$$Register);\n-  %}\n-  ins_pipe(pipe_jmp);\n-%}\n-\n-\/\/ Tail Jump; remove the return address; jump to target.\n-\/\/ TailCall above leaves the return address around.\n-instruct tailjmpInd(no_rbp_RegP jump_target, rax_RegP ex_oop)\n-%{\n-  match(TailJump jump_target ex_oop);\n-\n-  ins_cost(300);\n-  format %{ \"popq    rdx\\t# pop return address\\n\\t\"\n-            \"jmp     $jump_target\" %}\n-  ins_encode %{\n-    __ popq(as_Register(RDX_enc));\n-    __ jmp($jump_target$$Register);\n-  %}\n-  ins_pipe(pipe_jmp);\n-%}\n-\n-\/\/ Forward exception.\n-instruct ForwardExceptionjmp()\n-%{\n-  match(ForwardException);\n-\n-  format %{ \"jmp     forward_exception_stub\" %}\n-  ins_encode %{\n-    __ jump(RuntimeAddress(StubRoutines::forward_exception_entry()), noreg);\n-  %}\n-  ins_pipe(pipe_jmp);\n-%}\n-\n-\/\/ Create exception oop: created by stack-crawling runtime code.\n-\/\/ Created exception is now available to this handler, and is setup\n-\/\/ just prior to jumping to this handler.  No code emitted.\n-instruct CreateException(rax_RegP ex_oop)\n-%{\n-  match(Set ex_oop (CreateEx));\n-\n-  size(0);\n-  \/\/ use the following format syntax\n-  format %{ \"# exception oop is in rax; no code emitted\" %}\n-  ins_encode();\n-  ins_pipe(empty);\n-%}\n-\n-\/\/ Rethrow exception:\n-\/\/ The exception oop will come in the first argument position.\n-\/\/ Then JUMP (not call) to the rethrow stub code.\n-instruct RethrowException()\n-%{\n-  match(Rethrow);\n-\n-  \/\/ use the following format syntax\n-  format %{ \"jmp     rethrow_stub\" %}\n-  ins_encode %{\n-    __ jump(RuntimeAddress(OptoRuntime::rethrow_stub()), noreg);\n-  %}\n-  ins_pipe(pipe_jmp);\n-%}\n-\n-\/\/ ============================================================================\n-\/\/ This name is KNOWN by the ADLC and cannot be changed.\n-\/\/ The ADLC forces a 'TypeRawPtr::BOTTOM' output type\n-\/\/ for this guy.\n-instruct tlsLoadP(r15_RegP dst) %{\n-  match(Set dst (ThreadLocal));\n-  effect(DEF dst);\n-\n-  size(0);\n-  format %{ \"# TLS is in R15\" %}\n-  ins_encode( \/*empty encoding*\/ );\n-  ins_pipe(ialu_reg_reg);\n-%}\n-\n-\n-\/\/----------PEEPHOLE RULES-----------------------------------------------------\n-\/\/ These must follow all instruction definitions as they use the names\n-\/\/ defined in the instructions definitions.\n-\/\/\n-\/\/ peeppredicate ( rule_predicate );\n-\/\/ \/\/ the predicate unless which the peephole rule will be ignored\n-\/\/\n-\/\/ peepmatch ( root_instr_name [preceding_instruction]* );\n-\/\/\n-\/\/ peepprocedure ( procedure_name );\n-\/\/ \/\/ provide a procedure name to perform the optimization, the procedure should\n-\/\/ \/\/ reside in the architecture dependent peephole file, the method has the\n-\/\/ \/\/ signature of MachNode* (Block*, int, PhaseRegAlloc*, (MachNode*)(*)(), int...)\n-\/\/ \/\/ with the arguments being the basic block, the current node index inside the\n-\/\/ \/\/ block, the register allocator, the functions upon invoked return a new node\n-\/\/ \/\/ defined in peepreplace, and the rules of the nodes appearing in the\n-\/\/ \/\/ corresponding peepmatch, the function return true if successful, else\n-\/\/ \/\/ return false\n-\/\/\n-\/\/ peepconstraint %{\n-\/\/ (instruction_number.operand_name relational_op instruction_number.operand_name\n-\/\/  [, ...] );\n-\/\/ \/\/ instruction numbers are zero-based using left to right order in peepmatch\n-\/\/\n-\/\/ peepreplace ( instr_name  ( [instruction_number.operand_name]* ) );\n-\/\/ \/\/ provide an instruction_number.operand_name for each operand that appears\n-\/\/ \/\/ in the replacement instruction's match rule\n-\/\/\n-\/\/ ---------VM FLAGS---------------------------------------------------------\n-\/\/\n-\/\/ All peephole optimizations can be turned off using -XX:-OptoPeephole\n-\/\/\n-\/\/ Each peephole rule is given an identifying number starting with zero and\n-\/\/ increasing by one in the order seen by the parser.  An individual peephole\n-\/\/ can be enabled, and all others disabled, by using -XX:OptoPeepholeAt=#\n-\/\/ on the command-line.\n-\/\/\n-\/\/ ---------CURRENT LIMITATIONS----------------------------------------------\n-\/\/\n-\/\/ Only transformations inside a basic block (do we need more for peephole)\n-\/\/\n-\/\/ ---------EXAMPLE----------------------------------------------------------\n-\/\/\n-\/\/ \/\/ pertinent parts of existing instructions in architecture description\n-\/\/ instruct movI(rRegI dst, rRegI src)\n-\/\/ %{\n-\/\/   match(Set dst (CopyI src));\n-\/\/ %}\n-\/\/\n-\/\/ instruct incI_rReg(rRegI dst, immI_1 src, rFlagsReg cr)\n-\/\/ %{\n-\/\/   match(Set dst (AddI dst src));\n-\/\/   effect(KILL cr);\n-\/\/ %}\n-\/\/\n-\/\/ instruct leaI_rReg_immI(rRegI dst, immI_1 src)\n-\/\/ %{\n-\/\/   match(Set dst (AddI dst src));\n-\/\/ %}\n-\/\/\n-\/\/ 1. Simple replacement\n-\/\/ - Only match adjacent instructions in same basic block\n-\/\/ - Only equality constraints\n-\/\/ - Only constraints between operands, not (0.dest_reg == RAX_enc)\n-\/\/ - Only one replacement instruction\n-\/\/\n-\/\/ \/\/ Change (inc mov) to lea\n-\/\/ peephole %{\n-\/\/   \/\/ lea should only be emitted when beneficial\n-\/\/   peeppredicate( VM_Version::supports_fast_2op_lea() );\n-\/\/   \/\/ increment preceded by register-register move\n-\/\/   peepmatch ( incI_rReg movI );\n-\/\/   \/\/ require that the destination register of the increment\n-\/\/   \/\/ match the destination register of the move\n-\/\/   peepconstraint ( 0.dst == 1.dst );\n-\/\/   \/\/ construct a replacement instruction that sets\n-\/\/   \/\/ the destination to ( move's source register + one )\n-\/\/   peepreplace ( leaI_rReg_immI( 0.dst 1.src 0.src ) );\n-\/\/ %}\n-\/\/\n-\/\/ 2. Procedural replacement\n-\/\/ - More flexible finding relevent nodes\n-\/\/ - More flexible constraints\n-\/\/ - More flexible transformations\n-\/\/ - May utilise architecture-dependent API more effectively\n-\/\/ - Currently only one replacement instruction due to adlc parsing capabilities\n-\/\/\n-\/\/ \/\/ Change (inc mov) to lea\n-\/\/ peephole %{\n-\/\/   \/\/ lea should only be emitted when beneficial\n-\/\/   peeppredicate( VM_Version::supports_fast_2op_lea() );\n-\/\/   \/\/ the rule numbers of these nodes inside are passed into the function below\n-\/\/   peepmatch ( incI_rReg movI );\n-\/\/   \/\/ the method that takes the responsibility of transformation\n-\/\/   peepprocedure ( inc_mov_to_lea );\n-\/\/   \/\/ the replacement is a leaI_rReg_immI, a lambda upon invoked creating this\n-\/\/   \/\/ node is passed into the function above\n-\/\/   peepreplace ( leaI_rReg_immI() );\n-\/\/ %}\n-\n-\/\/ These instructions is not matched by the matcher but used by the peephole\n-instruct leaI_rReg_rReg_peep(rRegI dst, rRegI src1, rRegI src2)\n-%{\n-  predicate(false);\n-  match(Set dst (AddI src1 src2));\n-  format %{ \"leal    $dst, [$src1 + $src2]\" %}\n-  ins_encode %{\n-    Register dst = $dst$$Register;\n-    Register src1 = $src1$$Register;\n-    Register src2 = $src2$$Register;\n-    if (src1 != rbp && src1 != r13) {\n-      __ leal(dst, Address(src1, src2, Address::times_1));\n-    } else {\n-      assert(src2 != rbp && src2 != r13, \"\");\n-      __ leal(dst, Address(src2, src1, Address::times_1));\n-    }\n-  %}\n-  ins_pipe(ialu_reg_reg);\n-%}\n-\n-instruct leaI_rReg_immI_peep(rRegI dst, rRegI src1, immI src2)\n-%{\n-  predicate(false);\n-  match(Set dst (AddI src1 src2));\n-  format %{ \"leal    $dst, [$src1 + $src2]\" %}\n-  ins_encode %{\n-    __ leal($dst$$Register, Address($src1$$Register, $src2$$constant));\n-  %}\n-  ins_pipe(ialu_reg_reg);\n-%}\n-\n-instruct leaI_rReg_immI2_peep(rRegI dst, rRegI src, immI2 shift)\n-%{\n-  predicate(false);\n-  match(Set dst (LShiftI src shift));\n-  format %{ \"leal    $dst, [$src << $shift]\" %}\n-  ins_encode %{\n-    Address::ScaleFactor scale = static_cast<Address::ScaleFactor>($shift$$constant);\n-    Register src = $src$$Register;\n-    if (scale == Address::times_2 && src != rbp && src != r13) {\n-      __ leal($dst$$Register, Address(src, src, Address::times_1));\n-    } else {\n-      __ leal($dst$$Register, Address(noreg, src, scale));\n-    }\n-  %}\n-  ins_pipe(ialu_reg_reg);\n-%}\n-\n-instruct leaL_rReg_rReg_peep(rRegL dst, rRegL src1, rRegL src2)\n-%{\n-  predicate(false);\n-  match(Set dst (AddL src1 src2));\n-  format %{ \"leaq    $dst, [$src1 + $src2]\" %}\n-  ins_encode %{\n-    Register dst = $dst$$Register;\n-    Register src1 = $src1$$Register;\n-    Register src2 = $src2$$Register;\n-    if (src1 != rbp && src1 != r13) {\n-      __ leaq(dst, Address(src1, src2, Address::times_1));\n-    } else {\n-      assert(src2 != rbp && src2 != r13, \"\");\n-      __ leaq(dst, Address(src2, src1, Address::times_1));\n-    }\n-  %}\n-  ins_pipe(ialu_reg_reg);\n-%}\n-\n-instruct leaL_rReg_immL32_peep(rRegL dst, rRegL src1, immL32 src2)\n-%{\n-  predicate(false);\n-  match(Set dst (AddL src1 src2));\n-  format %{ \"leaq    $dst, [$src1 + $src2]\" %}\n-  ins_encode %{\n-    __ leaq($dst$$Register, Address($src1$$Register, $src2$$constant));\n-  %}\n-  ins_pipe(ialu_reg_reg);\n-%}\n-\n-instruct leaL_rReg_immI2_peep(rRegL dst, rRegL src, immI2 shift)\n-%{\n-  predicate(false);\n-  match(Set dst (LShiftL src shift));\n-  format %{ \"leaq    $dst, [$src << $shift]\" %}\n-  ins_encode %{\n-    Address::ScaleFactor scale = static_cast<Address::ScaleFactor>($shift$$constant);\n-    Register src = $src$$Register;\n-    if (scale == Address::times_2 && src != rbp && src != r13) {\n-      __ leaq($dst$$Register, Address(src, src, Address::times_1));\n-    } else {\n-      __ leaq($dst$$Register, Address(noreg, src, scale));\n-    }\n-  %}\n-  ins_pipe(ialu_reg_reg);\n-%}\n-\n-\/\/ These peephole rules replace mov + I pairs (where I is one of {add, inc, dec,\n-\/\/ sal}) with lea instructions. The {add, sal} rules are beneficial in\n-\/\/ processors with at least partial ALU support for lea\n-\/\/ (supports_fast_2op_lea()), whereas the {inc, dec} rules are only generally\n-\/\/ beneficial for processors with full ALU support\n-\/\/ (VM_Version::supports_fast_3op_lea()) and Intel Cascade Lake.\n-\n-peephole\n-%{\n-  peeppredicate(VM_Version::supports_fast_2op_lea());\n-  peepmatch (addI_rReg);\n-  peepprocedure (lea_coalesce_reg);\n-  peepreplace (leaI_rReg_rReg_peep());\n-%}\n-\n-peephole\n-%{\n-  peeppredicate(VM_Version::supports_fast_2op_lea());\n-  peepmatch (addI_rReg_imm);\n-  peepprocedure (lea_coalesce_imm);\n-  peepreplace (leaI_rReg_immI_peep());\n-%}\n-\n-peephole\n-%{\n-  peeppredicate(VM_Version::supports_fast_3op_lea() ||\n-                VM_Version::is_intel_cascade_lake());\n-  peepmatch (incI_rReg);\n-  peepprocedure (lea_coalesce_imm);\n-  peepreplace (leaI_rReg_immI_peep());\n-%}\n-\n-peephole\n-%{\n-  peeppredicate(VM_Version::supports_fast_3op_lea() ||\n-                VM_Version::is_intel_cascade_lake());\n-  peepmatch (decI_rReg);\n-  peepprocedure (lea_coalesce_imm);\n-  peepreplace (leaI_rReg_immI_peep());\n-%}\n-\n-peephole\n-%{\n-  peeppredicate(VM_Version::supports_fast_2op_lea());\n-  peepmatch (salI_rReg_immI2);\n-  peepprocedure (lea_coalesce_imm);\n-  peepreplace (leaI_rReg_immI2_peep());\n-%}\n-\n-peephole\n-%{\n-  peeppredicate(VM_Version::supports_fast_2op_lea());\n-  peepmatch (addL_rReg);\n-  peepprocedure (lea_coalesce_reg);\n-  peepreplace (leaL_rReg_rReg_peep());\n-%}\n-\n-peephole\n-%{\n-  peeppredicate(VM_Version::supports_fast_2op_lea());\n-  peepmatch (addL_rReg_imm);\n-  peepprocedure (lea_coalesce_imm);\n-  peepreplace (leaL_rReg_immL32_peep());\n-%}\n-\n-peephole\n-%{\n-  peeppredicate(VM_Version::supports_fast_3op_lea() ||\n-                VM_Version::is_intel_cascade_lake());\n-  peepmatch (incL_rReg);\n-  peepprocedure (lea_coalesce_imm);\n-  peepreplace (leaL_rReg_immL32_peep());\n-%}\n-\n-peephole\n-%{\n-  peeppredicate(VM_Version::supports_fast_3op_lea() ||\n-                VM_Version::is_intel_cascade_lake());\n-  peepmatch (decL_rReg);\n-  peepprocedure (lea_coalesce_imm);\n-  peepreplace (leaL_rReg_immL32_peep());\n-%}\n-\n-peephole\n-%{\n-  peeppredicate(VM_Version::supports_fast_2op_lea());\n-  peepmatch (salL_rReg_immI2);\n-  peepprocedure (lea_coalesce_imm);\n-  peepreplace (leaL_rReg_immI2_peep());\n-%}\n-\n-peephole\n-%{\n-  peepmatch (leaPCompressedOopOffset);\n-  peepprocedure (lea_remove_redundant);\n-%}\n-\n-peephole\n-%{\n-  peepmatch (leaP8Narrow);\n-  peepprocedure (lea_remove_redundant);\n-%}\n-\n-peephole\n-%{\n-  peepmatch (leaP32Narrow);\n-  peepprocedure (lea_remove_redundant);\n-%}\n-\n-\/\/ These peephole rules matches instructions which set flags and are followed by a testI\/L_reg\n-\/\/ The test instruction is redudanent in case the downstream instuctions (like JCC or CMOV) only use flags that are already set by the previous instruction\n-\n-\/\/int variant\n-peephole\n-%{\n-  peepmatch (testI_reg);\n-  peepprocedure (test_may_remove);\n-%}\n-\n-\/\/long variant\n-peephole\n-%{\n-  peepmatch (testL_reg);\n-  peepprocedure (test_may_remove);\n-%}\n-\n-\n-\/\/----------SMARTSPILL RULES---------------------------------------------------\n-\/\/ These must follow all instruction definitions as they use the names\n-\/\/ defined in the instructions definitions.\n","filename":"src\/hotspot\/cpu\/x86\/x86_64.ad","additions":0,"deletions":14735,"binary":false,"changes":14735,"status":"deleted"},{"patch":"@@ -96,1 +96,0 @@\n-#ifdef AMD64\n@@ -138,45 +137,0 @@\n-#else \/\/ !AMD64\n-\n-extern \"C\" {\n-  \/\/ defined in bsd_x86.s\n-  int64_t _Atomic_cmpxchg_long(int64_t, volatile int64_t*, int64_t);\n-  void _Atomic_move_long(const volatile int64_t* src, volatile int64_t* dst);\n-}\n-\n-template<>\n-template<typename T>\n-inline T AtomicAccess::PlatformCmpxchg<8>::operator()(T volatile* dest,\n-                                                      T compare_value,\n-                                                      T exchange_value,\n-                                                      atomic_memory_order \/* order *\/) const {\n-  STATIC_ASSERT(8 == sizeof(T));\n-  return cmpxchg_using_helper<int64_t>(_Atomic_cmpxchg_long, dest, compare_value, exchange_value);\n-}\n-\n-\/\/ No direct support for 8-byte xchg; emulate using cmpxchg.\n-template<>\n-struct AtomicAccess::PlatformXchg<8> : AtomicAccess::XchgUsingCmpxchg<8> {};\n-\n-\/\/ No direct support for 8-byte add; emulate using cmpxchg.\n-template<>\n-struct AtomicAccess::PlatformAdd<8> : AtomicAccess::AddUsingCmpxchg<8> {};\n-\n-template<>\n-template<typename T>\n-inline T AtomicAccess::PlatformLoad<8>::operator()(T const volatile* src) const {\n-  STATIC_ASSERT(8 == sizeof(T));\n-  volatile int64_t dest;\n-  _Atomic_move_long(reinterpret_cast<const volatile int64_t*>(src), reinterpret_cast<volatile int64_t*>(&dest));\n-  return PrimitiveConversions::cast<T>(dest);\n-}\n-\n-template<>\n-template<typename T>\n-inline void AtomicAccess::PlatformStore<8>::operator()(T volatile* dest,\n-                                                       T store_value) const {\n-  STATIC_ASSERT(8 == sizeof(T));\n-  _Atomic_move_long(reinterpret_cast<const volatile int64_t*>(&store_value), reinterpret_cast<volatile int64_t*>(dest));\n-}\n-\n-#endif \/\/ AMD64\n-\n@@ -219,1 +173,0 @@\n-#ifdef AMD64\n@@ -231,1 +184,0 @@\n-#endif \/\/ AMD64\n","filename":"src\/hotspot\/os_cpu\/bsd_x86\/atomicAccess_bsd_x86.hpp","additions":0,"deletions":48,"binary":false,"changes":48,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2000, 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2000, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -32,1 +32,0 @@\n-#ifdef AMD64\n@@ -36,9 +35,0 @@\n-#else\n-define_pd_global(intx, CompilerThreadStackSize,  512);\n-\/\/ ThreadStackSize 320 allows a couple of test cases to run while\n-\/\/ keeping the number of threads that can be created high.  System\n-\/\/ default ThreadStackSize appears to be 512 which is too big.\n-define_pd_global(intx, ThreadStackSize,          320);\n-define_pd_global(intx, VMThreadStackSize,        512);\n-#endif \/\/ AMD64\n-\n","filename":"src\/hotspot\/os_cpu\/bsd_x86\/globals_bsd_x86.hpp","additions":1,"deletions":11,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2003, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2003, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -54,1 +54,0 @@\n-#ifdef AMD64\n@@ -56,3 +55,0 @@\n-#else\n-  __asm__ volatile (\"lock; addl $0,0(%%esp)\" : : : \"cc\", \"memory\");\n-#endif\n","filename":"src\/hotspot\/os_cpu\/bsd_x86\/orderAccess_bsd_x86.hpp","additions":1,"deletions":5,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -89,1 +89,0 @@\n-#ifdef AMD64\n@@ -93,4 +92,0 @@\n-#else\n-#define SPELL_REG_SP \"esp\"\n-#define SPELL_REG_FP \"ebp\"\n-#endif \/\/ AMD64\n@@ -100,39 +95,22 @@\n-# ifdef AMD64\n-#  define context_pc uc_mcontext.mc_rip\n-#  define context_sp uc_mcontext.mc_rsp\n-#  define context_fp uc_mcontext.mc_rbp\n-#  define context_rip uc_mcontext.mc_rip\n-#  define context_rsp uc_mcontext.mc_rsp\n-#  define context_rbp uc_mcontext.mc_rbp\n-#  define context_rax uc_mcontext.mc_rax\n-#  define context_rbx uc_mcontext.mc_rbx\n-#  define context_rcx uc_mcontext.mc_rcx\n-#  define context_rdx uc_mcontext.mc_rdx\n-#  define context_rsi uc_mcontext.mc_rsi\n-#  define context_rdi uc_mcontext.mc_rdi\n-#  define context_r8  uc_mcontext.mc_r8\n-#  define context_r9  uc_mcontext.mc_r9\n-#  define context_r10 uc_mcontext.mc_r10\n-#  define context_r11 uc_mcontext.mc_r11\n-#  define context_r12 uc_mcontext.mc_r12\n-#  define context_r13 uc_mcontext.mc_r13\n-#  define context_r14 uc_mcontext.mc_r14\n-#  define context_r15 uc_mcontext.mc_r15\n-#  define context_flags uc_mcontext.mc_flags\n-#  define context_err uc_mcontext.mc_err\n-# else\n-#  define context_pc uc_mcontext.mc_eip\n-#  define context_sp uc_mcontext.mc_esp\n-#  define context_fp uc_mcontext.mc_ebp\n-#  define context_eip uc_mcontext.mc_eip\n-#  define context_esp uc_mcontext.mc_esp\n-#  define context_eax uc_mcontext.mc_eax\n-#  define context_ebx uc_mcontext.mc_ebx\n-#  define context_ecx uc_mcontext.mc_ecx\n-#  define context_edx uc_mcontext.mc_edx\n-#  define context_ebp uc_mcontext.mc_ebp\n-#  define context_esi uc_mcontext.mc_esi\n-#  define context_edi uc_mcontext.mc_edi\n-#  define context_eflags uc_mcontext.mc_eflags\n-#  define context_trapno uc_mcontext.mc_trapno\n-# endif\n+# define context_pc uc_mcontext.mc_rip\n+# define context_sp uc_mcontext.mc_rsp\n+# define context_fp uc_mcontext.mc_rbp\n+# define context_rip uc_mcontext.mc_rip\n+# define context_rsp uc_mcontext.mc_rsp\n+# define context_rbp uc_mcontext.mc_rbp\n+# define context_rax uc_mcontext.mc_rax\n+# define context_rbx uc_mcontext.mc_rbx\n+# define context_rcx uc_mcontext.mc_rcx\n+# define context_rdx uc_mcontext.mc_rdx\n+# define context_rsi uc_mcontext.mc_rsi\n+# define context_rdi uc_mcontext.mc_rdi\n+# define context_r8  uc_mcontext.mc_r8\n+# define context_r9  uc_mcontext.mc_r9\n+# define context_r10 uc_mcontext.mc_r10\n+# define context_r11 uc_mcontext.mc_r11\n+# define context_r12 uc_mcontext.mc_r12\n+# define context_r13 uc_mcontext.mc_r13\n+# define context_r14 uc_mcontext.mc_r14\n+# define context_r15 uc_mcontext.mc_r15\n+# define context_flags uc_mcontext.mc_flags\n+# define context_err uc_mcontext.mc_err\n@@ -149,40 +127,23 @@\n-# ifdef AMD64\n-#  define context_pc context_rip\n-#  define context_sp context_rsp\n-#  define context_fp context_rbp\n-#  define context_rip uc_mcontext->DU3_PREFIX(ss,rip)\n-#  define context_rsp uc_mcontext->DU3_PREFIX(ss,rsp)\n-#  define context_rax uc_mcontext->DU3_PREFIX(ss,rax)\n-#  define context_rbx uc_mcontext->DU3_PREFIX(ss,rbx)\n-#  define context_rcx uc_mcontext->DU3_PREFIX(ss,rcx)\n-#  define context_rdx uc_mcontext->DU3_PREFIX(ss,rdx)\n-#  define context_rbp uc_mcontext->DU3_PREFIX(ss,rbp)\n-#  define context_rsi uc_mcontext->DU3_PREFIX(ss,rsi)\n-#  define context_rdi uc_mcontext->DU3_PREFIX(ss,rdi)\n-#  define context_r8  uc_mcontext->DU3_PREFIX(ss,r8)\n-#  define context_r9  uc_mcontext->DU3_PREFIX(ss,r9)\n-#  define context_r10 uc_mcontext->DU3_PREFIX(ss,r10)\n-#  define context_r11 uc_mcontext->DU3_PREFIX(ss,r11)\n-#  define context_r12 uc_mcontext->DU3_PREFIX(ss,r12)\n-#  define context_r13 uc_mcontext->DU3_PREFIX(ss,r13)\n-#  define context_r14 uc_mcontext->DU3_PREFIX(ss,r14)\n-#  define context_r15 uc_mcontext->DU3_PREFIX(ss,r15)\n-#  define context_flags uc_mcontext->DU3_PREFIX(ss,rflags)\n-#  define context_trapno uc_mcontext->DU3_PREFIX(es,trapno)\n-#  define context_err uc_mcontext->DU3_PREFIX(es,err)\n-# else\n-#  define context_pc context_eip\n-#  define context_sp context_esp\n-#  define context_fp context_ebp\n-#  define context_eip uc_mcontext->DU3_PREFIX(ss,eip)\n-#  define context_esp uc_mcontext->DU3_PREFIX(ss,esp)\n-#  define context_eax uc_mcontext->DU3_PREFIX(ss,eax)\n-#  define context_ebx uc_mcontext->DU3_PREFIX(ss,ebx)\n-#  define context_ecx uc_mcontext->DU3_PREFIX(ss,ecx)\n-#  define context_edx uc_mcontext->DU3_PREFIX(ss,edx)\n-#  define context_ebp uc_mcontext->DU3_PREFIX(ss,ebp)\n-#  define context_esi uc_mcontext->DU3_PREFIX(ss,esi)\n-#  define context_edi uc_mcontext->DU3_PREFIX(ss,edi)\n-#  define context_eflags uc_mcontext->DU3_PREFIX(ss,eflags)\n-#  define context_trapno uc_mcontext->DU3_PREFIX(es,trapno)\n-# endif\n+# define context_pc context_rip\n+# define context_sp context_rsp\n+# define context_fp context_rbp\n+# define context_rip uc_mcontext->DU3_PREFIX(ss,rip)\n+# define context_rsp uc_mcontext->DU3_PREFIX(ss,rsp)\n+# define context_rax uc_mcontext->DU3_PREFIX(ss,rax)\n+# define context_rbx uc_mcontext->DU3_PREFIX(ss,rbx)\n+# define context_rcx uc_mcontext->DU3_PREFIX(ss,rcx)\n+# define context_rdx uc_mcontext->DU3_PREFIX(ss,rdx)\n+# define context_rbp uc_mcontext->DU3_PREFIX(ss,rbp)\n+# define context_rsi uc_mcontext->DU3_PREFIX(ss,rsi)\n+# define context_rdi uc_mcontext->DU3_PREFIX(ss,rdi)\n+# define context_r8  uc_mcontext->DU3_PREFIX(ss,r8)\n+# define context_r9  uc_mcontext->DU3_PREFIX(ss,r9)\n+# define context_r10 uc_mcontext->DU3_PREFIX(ss,r10)\n+# define context_r11 uc_mcontext->DU3_PREFIX(ss,r11)\n+# define context_r12 uc_mcontext->DU3_PREFIX(ss,r12)\n+# define context_r13 uc_mcontext->DU3_PREFIX(ss,r13)\n+# define context_r14 uc_mcontext->DU3_PREFIX(ss,r14)\n+# define context_r15 uc_mcontext->DU3_PREFIX(ss,r15)\n+# define context_flags uc_mcontext->DU3_PREFIX(ss,rflags)\n+# define context_trapno uc_mcontext->DU3_PREFIX(es,trapno)\n+# define context_err uc_mcontext->DU3_PREFIX(es,err)\n@@ -193,39 +154,22 @@\n-# ifdef AMD64\n-#  define context_pc sc_rip\n-#  define context_sp sc_rsp\n-#  define context_fp sc_rbp\n-#  define context_rip sc_rip\n-#  define context_rsp sc_rsp\n-#  define context_rbp sc_rbp\n-#  define context_rax sc_rax\n-#  define context_rbx sc_rbx\n-#  define context_rcx sc_rcx\n-#  define context_rdx sc_rdx\n-#  define context_rsi sc_rsi\n-#  define context_rdi sc_rdi\n-#  define context_r8  sc_r8\n-#  define context_r9  sc_r9\n-#  define context_r10 sc_r10\n-#  define context_r11 sc_r11\n-#  define context_r12 sc_r12\n-#  define context_r13 sc_r13\n-#  define context_r14 sc_r14\n-#  define context_r15 sc_r15\n-#  define context_flags sc_rflags\n-#  define context_err sc_err\n-# else\n-#  define context_pc sc_eip\n-#  define context_sp sc_esp\n-#  define context_fp sc_ebp\n-#  define context_eip sc_eip\n-#  define context_esp sc_esp\n-#  define context_eax sc_eax\n-#  define context_ebx sc_ebx\n-#  define context_ecx sc_ecx\n-#  define context_edx sc_edx\n-#  define context_ebp sc_ebp\n-#  define context_esi sc_esi\n-#  define context_edi sc_edi\n-#  define context_eflags sc_eflags\n-#  define context_trapno sc_trapno\n-# endif\n+# define context_pc sc_rip\n+# define context_sp sc_rsp\n+# define context_fp sc_rbp\n+# define context_rip sc_rip\n+# define context_rsp sc_rsp\n+# define context_rbp sc_rbp\n+# define context_rax sc_rax\n+# define context_rbx sc_rbx\n+# define context_rcx sc_rcx\n+# define context_rdx sc_rdx\n+# define context_rsi sc_rsi\n+# define context_rdi sc_rdi\n+# define context_r8  sc_r8\n+# define context_r9  sc_r9\n+# define context_r10 sc_r10\n+# define context_r11 sc_r11\n+# define context_r12 sc_r12\n+# define context_r13 sc_r13\n+# define context_r14 sc_r14\n+# define context_r15 sc_r15\n+# define context_flags sc_rflags\n+# define context_err sc_err\n@@ -236,40 +180,23 @@\n-# ifdef AMD64\n-#  define __register_t __greg_t\n-#  define context_pc uc_mcontext.__gregs[_REG_RIP]\n-#  define context_sp uc_mcontext.__gregs[_REG_URSP]\n-#  define context_fp uc_mcontext.__gregs[_REG_RBP]\n-#  define context_rip uc_mcontext.__gregs[_REG_RIP]\n-#  define context_rsp uc_mcontext.__gregs[_REG_URSP]\n-#  define context_rax uc_mcontext.__gregs[_REG_RAX]\n-#  define context_rbx uc_mcontext.__gregs[_REG_RBX]\n-#  define context_rcx uc_mcontext.__gregs[_REG_RCX]\n-#  define context_rdx uc_mcontext.__gregs[_REG_RDX]\n-#  define context_rbp uc_mcontext.__gregs[_REG_RBP]\n-#  define context_rsi uc_mcontext.__gregs[_REG_RSI]\n-#  define context_rdi uc_mcontext.__gregs[_REG_RDI]\n-#  define context_r8  uc_mcontext.__gregs[_REG_R8]\n-#  define context_r9  uc_mcontext.__gregs[_REG_R9]\n-#  define context_r10 uc_mcontext.__gregs[_REG_R10]\n-#  define context_r11 uc_mcontext.__gregs[_REG_R11]\n-#  define context_r12 uc_mcontext.__gregs[_REG_R12]\n-#  define context_r13 uc_mcontext.__gregs[_REG_R13]\n-#  define context_r14 uc_mcontext.__gregs[_REG_R14]\n-#  define context_r15 uc_mcontext.__gregs[_REG_R15]\n-#  define context_flags uc_mcontext.__gregs[_REG_RFL]\n-#  define context_err uc_mcontext.__gregs[_REG_ERR]\n-# else\n-#  define context_pc uc_mcontext.__gregs[_REG_EIP]\n-#  define context_sp uc_mcontext.__gregs[_REG_UESP]\n-#  define context_fp uc_mcontext.__gregs[_REG_EBP]\n-#  define context_eip uc_mcontext.__gregs[_REG_EIP]\n-#  define context_esp uc_mcontext.__gregs[_REG_UESP]\n-#  define context_eax uc_mcontext.__gregs[_REG_EAX]\n-#  define context_ebx uc_mcontext.__gregs[_REG_EBX]\n-#  define context_ecx uc_mcontext.__gregs[_REG_ECX]\n-#  define context_edx uc_mcontext.__gregs[_REG_EDX]\n-#  define context_ebp uc_mcontext.__gregs[_REG_EBP]\n-#  define context_esi uc_mcontext.__gregs[_REG_ESI]\n-#  define context_edi uc_mcontext.__gregs[_REG_EDI]\n-#  define context_eflags uc_mcontext.__gregs[_REG_EFL]\n-#  define context_trapno uc_mcontext.__gregs[_REG_TRAPNO]\n-# endif\n+# define __register_t __greg_t\n+# define context_pc uc_mcontext.__gregs[_REG_RIP]\n+# define context_sp uc_mcontext.__gregs[_REG_URSP]\n+# define context_fp uc_mcontext.__gregs[_REG_RBP]\n+# define context_rip uc_mcontext.__gregs[_REG_RIP]\n+# define context_rsp uc_mcontext.__gregs[_REG_URSP]\n+# define context_rax uc_mcontext.__gregs[_REG_RAX]\n+# define context_rbx uc_mcontext.__gregs[_REG_RBX]\n+# define context_rcx uc_mcontext.__gregs[_REG_RCX]\n+# define context_rdx uc_mcontext.__gregs[_REG_RDX]\n+# define context_rbp uc_mcontext.__gregs[_REG_RBP]\n+# define context_rsi uc_mcontext.__gregs[_REG_RSI]\n+# define context_rdi uc_mcontext.__gregs[_REG_RDI]\n+# define context_r8  uc_mcontext.__gregs[_REG_R8]\n+# define context_r9  uc_mcontext.__gregs[_REG_R9]\n+# define context_r10 uc_mcontext.__gregs[_REG_R10]\n+# define context_r11 uc_mcontext.__gregs[_REG_R11]\n+# define context_r12 uc_mcontext.__gregs[_REG_R12]\n+# define context_r13 uc_mcontext.__gregs[_REG_R13]\n+# define context_r14 uc_mcontext.__gregs[_REG_R14]\n+# define context_r15 uc_mcontext.__gregs[_REG_R15]\n+# define context_flags uc_mcontext.__gregs[_REG_RFL]\n+# define context_err uc_mcontext.__gregs[_REG_ERR]\n@@ -471,7 +398,5 @@\n-      } else\n-#ifdef AMD64\n-      if (sig == SIGFPE &&\n-          (info->si_code == FPE_INTDIV || info->si_code == FPE_FLTDIV\n-           \/\/ Workaround for macOS ARM incorrectly reporting FPE_FLTINV for \"div by 0\"\n-           \/\/ instead of the expected FPE_FLTDIV when running x86_64 binary under Rosetta emulation\n-           MACOS_ONLY(|| (VM_Version::is_cpu_emulated() && info->si_code == FPE_FLTINV)))) {\n+      } else if (sig == SIGFPE &&\n+                 (info->si_code == FPE_INTDIV || info->si_code == FPE_FLTDIV\n+                 \/\/ Workaround for macOS ARM incorrectly reporting FPE_FLTINV for \"div by 0\"\n+                 \/\/ instead of the expected FPE_FLTDIV when running x86_64 binary under Rosetta emulation\n+                 MACOS_ONLY(|| (VM_Version::is_cpu_emulated() && info->si_code == FPE_FLTINV)))) {\n@@ -505,28 +430,0 @@\n-\n-#else\n-      if (sig == SIGFPE \/* && info->si_code == FPE_INTDIV *\/) {\n-        \/\/ HACK: si_code does not work on bsd 2.2.12-20!!!\n-        int op = pc[0];\n-        if (op == 0xDB) {\n-          \/\/ FIST\n-          \/\/ TODO: The encoding of D2I in x86_32.ad can cause an exception\n-          \/\/ prior to the fist instruction if there was an invalid operation\n-          \/\/ pending. We want to dismiss that exception. From the win_32\n-          \/\/ side it also seems that if it really was the fist causing\n-          \/\/ the exception that we do the d2i by hand with different\n-          \/\/ rounding. Seems kind of weird.\n-          \/\/ NOTE: that we take the exception at the NEXT floating point instruction.\n-          assert(pc[0] == 0xDB, \"not a FIST opcode\");\n-          assert(pc[1] == 0x14, \"not a FIST opcode\");\n-          assert(pc[2] == 0x24, \"not a FIST opcode\");\n-          return true;\n-        } else if (op == 0xF7) {\n-          \/\/ IDIV\n-          stub = SharedRuntime::continuation_for_implicit_exception(thread, pc, SharedRuntime::IMPLICIT_DIVIDE_BY_ZERO);\n-        } else {\n-          \/\/ TODO: handle more cases if we are using other x86 instructions\n-          \/\/   that can generate SIGFPE signal on bsd.\n-          tty->print_cr(\"unknown opcode 0x%X with SIGFPE.\", op);\n-          fatal(\"please update this code.\");\n-        }\n-#endif \/\/ AMD64\n@@ -559,75 +456,0 @@\n-#ifndef AMD64\n-  \/\/ Execution protection violation\n-  \/\/\n-  \/\/ This should be kept as the last step in the triage.  We don't\n-  \/\/ have a dedicated trap number for a no-execute fault, so be\n-  \/\/ conservative and allow other handlers the first shot.\n-  \/\/\n-  \/\/ Note: We don't test that info->si_code == SEGV_ACCERR here.\n-  \/\/ this si_code is so generic that it is almost meaningless; and\n-  \/\/ the si_code for this condition may change in the future.\n-  \/\/ Furthermore, a false-positive should be harmless.\n-  if (UnguardOnExecutionViolation > 0 &&\n-      stub == nullptr &&\n-      (sig == SIGSEGV || sig == SIGBUS) &&\n-      uc->context_trapno == trap_page_fault) {\n-    size_t page_size = os::vm_page_size();\n-    address addr = (address) info->si_addr;\n-    address pc = os::Posix::ucontext_get_pc(uc);\n-    \/\/ Make sure the pc and the faulting address are sane.\n-    \/\/\n-    \/\/ If an instruction spans a page boundary, and the page containing\n-    \/\/ the beginning of the instruction is executable but the following\n-    \/\/ page is not, the pc and the faulting address might be slightly\n-    \/\/ different - we still want to unguard the 2nd page in this case.\n-    \/\/\n-    \/\/ 15 bytes seems to be a (very) safe value for max instruction size.\n-    bool pc_is_near_addr =\n-      (pointer_delta((void*) addr, (void*) pc, sizeof(char)) < 15);\n-    bool instr_spans_page_boundary =\n-      (align_down((intptr_t) pc ^ (intptr_t) addr,\n-                       (intptr_t) page_size) > 0);\n-\n-    if (pc == addr || (pc_is_near_addr && instr_spans_page_boundary)) {\n-      static volatile address last_addr =\n-        (address) os::non_memory_address_word();\n-\n-      \/\/ In conservative mode, don't unguard unless the address is in the VM\n-      if (addr != last_addr &&\n-          (UnguardOnExecutionViolation > 1 || os::address_is_in_vm(addr))) {\n-\n-        \/\/ Set memory to RWX and retry\n-        address page_start = align_down(addr, page_size);\n-        bool res = os::protect_memory((char*) page_start, page_size,\n-                                      os::MEM_PROT_RWX);\n-\n-        log_debug(os)(\"Execution protection violation \"\n-                      \"at \" INTPTR_FORMAT\n-                      \", unguarding \" INTPTR_FORMAT \": %s, errno=%d\", p2i(addr),\n-                      p2i(page_start), (res ? \"success\" : \"failed\"), errno);\n-        stub = pc;\n-\n-        \/\/ Set last_addr so if we fault again at the same address, we don't end\n-        \/\/ up in an endless loop.\n-        \/\/\n-        \/\/ There are two potential complications here.  Two threads trapping at\n-        \/\/ the same address at the same time could cause one of the threads to\n-        \/\/ think it already unguarded, and abort the VM.  Likely very rare.\n-        \/\/\n-        \/\/ The other race involves two threads alternately trapping at\n-        \/\/ different addresses and failing to unguard the page, resulting in\n-        \/\/ an endless loop.  This condition is probably even more unlikely than\n-        \/\/ the first.\n-        \/\/\n-        \/\/ Although both cases could be avoided by using locks or thread local\n-        \/\/ last_addr, these solutions are unnecessary complication: this\n-        \/\/ handler is a best-effort safety net, not a complete solution.  It is\n-        \/\/ disabled by default and should only be used as a workaround in case\n-        \/\/ we missed any no-execute-unsafe VM code.\n-\n-        last_addr = addr;\n-      }\n-    }\n-  }\n-#endif \/\/ !AMD64\n-\n@@ -649,4 +471,0 @@\n-#ifndef AMD64\n-  \/\/ Set fpu to 53 bit precision. This happens too early to use a stub.\n-  fixcw();\n-#endif \/\/ !AMD64\n@@ -674,1 +492,0 @@\n-#ifdef _LP64\n@@ -676,9 +493,0 @@\n-#else\n-size_t os::_vm_internal_thread_min_stack_allowed = (48 DEBUG_ONLY(+ 4)) * K;\n-#endif \/\/ _LP64\n-\n-#ifndef AMD64\n-#ifdef __GNUC__\n-#define GET_GS() ({int gs; __asm__ volatile(\"movw %%gs, %w0\":\"=q\"(gs)); gs&0xffff;})\n-#endif\n-#endif \/\/ AMD64\n@@ -689,1 +497,0 @@\n-#ifdef AMD64\n@@ -691,3 +498,0 @@\n-#else\n-  size_t s = (thr_type == os::compiler_thread ? 2 * M : 512 * K);\n-#endif \/\/ AMD64\n@@ -806,1 +610,0 @@\n-#ifdef AMD64\n@@ -832,14 +635,0 @@\n-#else\n-  st->print(  \"EAX=\" INTPTR_FORMAT, (intptr_t)uc->context_eax);\n-  st->print(\", EBX=\" INTPTR_FORMAT, (intptr_t)uc->context_ebx);\n-  st->print(\", ECX=\" INTPTR_FORMAT, (intptr_t)uc->context_ecx);\n-  st->print(\", EDX=\" INTPTR_FORMAT, (intptr_t)uc->context_edx);\n-  st->cr();\n-  st->print(  \"ESP=\" INTPTR_FORMAT, (intptr_t)uc->context_esp);\n-  st->print(\", EBP=\" INTPTR_FORMAT, (intptr_t)uc->context_ebp);\n-  st->print(\", ESI=\" INTPTR_FORMAT, (intptr_t)uc->context_esi);\n-  st->print(\", EDI=\" INTPTR_FORMAT, (intptr_t)uc->context_edi);\n-  st->cr();\n-  st->print(  \"EIP=\" INTPTR_FORMAT, (intptr_t)uc->context_eip);\n-  st->print(\", EFLAGS=\" INTPTR_FORMAT, (intptr_t)uc->context_eflags);\n-#endif \/\/ AMD64\n@@ -851,1 +640,1 @@\n-  const int register_count = AMD64_ONLY(16) NOT_AMD64(8);\n+  const int register_count = 16;\n@@ -864,1 +653,0 @@\n-#ifdef AMD64\n@@ -881,11 +669,1 @@\n-#else\n-    CASE_PRINT_REG(0, \"EAX=\", eax); break;\n-    CASE_PRINT_REG(1, \"EBX=\", ebx); break;\n-    CASE_PRINT_REG(2, \"ECX=\", ecx); break;\n-    CASE_PRINT_REG(3, \"EDX=\", edx); break;\n-    CASE_PRINT_REG(4, \"ESP=\", esp); break;\n-    CASE_PRINT_REG(5, \"EBP=\", ebp); break;\n-    CASE_PRINT_REG(6, \"ESI=\", esi); break;\n-    CASE_PRINT_REG(7, \"EDI=\", edi); break;\n-#endif \/\/ AMD64\n-    }\n+  }\n@@ -898,5 +676,0 @@\n-#ifndef AMD64\n-  address fpu_cntrl = StubRoutines::addr_fpu_cntrl_wrd_std();\n-  __asm__ volatile (  \"fldcw (%0)\" :\n-                      : \"r\" (fpu_cntrl) : \"memory\");\n-#endif \/\/ !AMD64\n","filename":"src\/hotspot\/os_cpu\/bsd_x86\/os_bsd_x86.cpp","additions":97,"deletions":324,"binary":false,"changes":421,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2011, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2011, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -42,6 +42,0 @@\n-#ifndef AMD64\n-  \/\/ 64 bit result in edx:eax\n-  uint64_t res;\n-  __asm__ __volatile__ (\"rdtsc\" : \"=A\" (res));\n-  return (jlong)res;\n-#else\n@@ -53,1 +47,0 @@\n-#endif \/\/ AMD64\n","filename":"src\/hotspot\/os_cpu\/bsd_x86\/os_bsd_x86.inline.hpp","additions":1,"deletions":8,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2003, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2003, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -32,1 +32,0 @@\n-#ifdef AMD64\n@@ -34,1 +33,0 @@\n-#endif \/\/ AMD64\n@@ -38,2 +36,0 @@\n-#ifdef AMD64\n-\n@@ -43,2 +39,0 @@\n-\n-#endif \/\/ AMD64\n","filename":"src\/hotspot\/os_cpu\/bsd_x86\/prefetch_bsd_x86.inline.hpp","additions":1,"deletions":7,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -96,2 +96,0 @@\n-#ifdef AMD64\n-\n@@ -138,45 +136,0 @@\n-#else \/\/ !AMD64\n-\n-extern \"C\" {\n-  \/\/ defined in linux_x86.s\n-  int64_t _Atomic_cmpxchg_long(int64_t, volatile int64_t*, int64_t);\n-  void _Atomic_move_long(const volatile int64_t* src, volatile int64_t* dst);\n-}\n-\n-template<>\n-template<typename T>\n-inline T AtomicAccess::PlatformCmpxchg<8>::operator()(T volatile* dest,\n-                                                      T compare_value,\n-                                                      T exchange_value,\n-                                                      atomic_memory_order order) const {\n-  STATIC_ASSERT(8 == sizeof(T));\n-  return cmpxchg_using_helper<int64_t>(_Atomic_cmpxchg_long, dest, compare_value, exchange_value);\n-}\n-\n-\/\/ No direct support for 8-byte xchg; emulate using cmpxchg.\n-template<>\n-struct AtomicAccess::PlatformXchg<8> : AtomicAccess::XchgUsingCmpxchg<8> {};\n-\n-\/\/ No direct support for 8-byte add; emulate using cmpxchg.\n-template<>\n-struct AtomicAccess::PlatformAdd<8> : AtomicAccess::AddUsingCmpxchg<8> {};\n-\n-template<>\n-template<typename T>\n-inline T AtomicAccess::PlatformLoad<8>::operator()(T const volatile* src) const {\n-  STATIC_ASSERT(8 == sizeof(T));\n-  volatile int64_t dest;\n-  _Atomic_move_long(reinterpret_cast<const volatile int64_t*>(src), reinterpret_cast<volatile int64_t*>(&dest));\n-  return PrimitiveConversions::cast<T>(dest);\n-}\n-\n-template<>\n-template<typename T>\n-inline void AtomicAccess::PlatformStore<8>::operator()(T volatile* dest,\n-                                                       T store_value) const {\n-  STATIC_ASSERT(8 == sizeof(T));\n-  _Atomic_move_long(reinterpret_cast<const volatile int64_t*>(&store_value), reinterpret_cast<volatile int64_t*>(dest));\n-}\n-\n-#endif \/\/ AMD64\n-\n@@ -219,1 +172,0 @@\n-#ifdef AMD64\n@@ -231,1 +183,0 @@\n-#endif \/\/ AMD64\n","filename":"src\/hotspot\/os_cpu\/linux_x86\/atomicAccess_linux_x86.hpp","additions":0,"deletions":49,"binary":false,"changes":49,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2000, 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2000, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -31,1 +31,0 @@\n-#ifdef AMD64\n@@ -35,14 +34,0 @@\n-#else\n-\/\/ Some tests in debug VM mode run out of compile thread stack.\n-\/\/ Observed on some x86_32 VarHandles tests during escape analysis.\n-#ifdef ASSERT\n-define_pd_global(intx, CompilerThreadStackSize,   768);\n-#else\n-define_pd_global(intx, CompilerThreadStackSize,   512);\n-#endif\n-\/\/ ThreadStackSize 320 allows a couple of test cases to run while\n-\/\/ keeping the number of threads that can be created high.  System\n-\/\/ default ThreadStackSize appears to be 512 which is too big.\n-define_pd_global(intx, ThreadStackSize,          320);\n-define_pd_global(intx, VMThreadStackSize,        512);\n-#endif \/\/ AMD64\n","filename":"src\/hotspot\/os_cpu\/linux_x86\/globals_linux_x86.hpp","additions":1,"deletions":16,"binary":false,"changes":17,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2003, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2003, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -49,2 +49,1 @@\n-   \/\/ always use locked addl since mfence is sometimes expensive\n-#ifdef AMD64\n+  \/\/ always use locked addl since mfence is sometimes expensive\n@@ -52,3 +51,0 @@\n-#else\n-  __asm__ volatile (\"lock; addl $0,0(%%esp)\" : : : \"cc\", \"memory\");\n-#endif\n@@ -63,1 +59,0 @@\n-#ifdef AMD64\n@@ -65,5 +60,0 @@\n-#else\n-    \/\/ On some x86 systems EBX is a reserved register that cannot be\n-    \/\/ clobbered, so we must protect it around the CPUID.\n-    __asm__ volatile (\"xchg %%esi, %%ebx; cpuid; xchg %%esi, %%ebx \" : \"+a\" (idx) : : \"esi\", \"ecx\", \"edx\", \"memory\");\n-#endif\n","filename":"src\/hotspot\/os_cpu\/linux_x86\/orderAccess_linux_x86.hpp","additions":2,"deletions":12,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -75,3 +75,0 @@\n-#ifndef AMD64\n-# include <fpu_control.h>\n-#endif\n@@ -79,1 +76,0 @@\n-#ifdef AMD64\n@@ -86,7 +82,0 @@\n-#else\n-#define REG_SP REG_UESP\n-#define REG_PC REG_EIP\n-#define REG_FP REG_EBP\n-#define SPELL_REG_SP \"esp\"\n-#define SPELL_REG_FP \"ebp\"\n-#endif \/\/ AMD64\n@@ -284,4 +273,2 @@\n-      } else\n-#ifdef AMD64\n-      if (sig == SIGFPE &&\n-          (info->si_code == FPE_INTDIV || info->si_code == FPE_FLTDIV)) {\n+      } else if (sig == SIGFPE &&\n+                 (info->si_code == FPE_INTDIV || info->si_code == FPE_FLTDIV)) {\n@@ -294,27 +281,0 @@\n-#else\n-      if (sig == SIGFPE \/* && info->si_code == FPE_INTDIV *\/) {\n-        \/\/ HACK: si_code does not work on linux 2.2.12-20!!!\n-        int op = pc[0];\n-        if (op == 0xDB) {\n-          \/\/ FIST\n-          \/\/ TODO: The encoding of D2I in x86_32.ad can cause an exception\n-          \/\/ prior to the fist instruction if there was an invalid operation\n-          \/\/ pending. We want to dismiss that exception. From the win_32\n-          \/\/ side it also seems that if it really was the fist causing\n-          \/\/ the exception that we do the d2i by hand with different\n-          \/\/ rounding. Seems kind of weird.\n-          \/\/ NOTE: that we take the exception at the NEXT floating point instruction.\n-          assert(pc[0] == 0xDB, \"not a FIST opcode\");\n-          assert(pc[1] == 0x14, \"not a FIST opcode\");\n-          assert(pc[2] == 0x24, \"not a FIST opcode\");\n-          return true;\n-        } else if (op == 0xF7) {\n-          \/\/ IDIV\n-          stub = SharedRuntime::continuation_for_implicit_exception(thread, pc, SharedRuntime::IMPLICIT_DIVIDE_BY_ZERO);\n-        } else {\n-          \/\/ TODO: handle more cases if we are using other x86 instructions\n-          \/\/   that can generate SIGFPE signal on linux.\n-          tty->print_cr(\"unknown opcode 0x%X with SIGFPE.\", op);\n-          fatal(\"please update this code.\");\n-        }\n-#endif \/\/ AMD64\n@@ -347,75 +307,0 @@\n-#ifndef AMD64\n-  \/\/ Execution protection violation\n-  \/\/\n-  \/\/ This should be kept as the last step in the triage.  We don't\n-  \/\/ have a dedicated trap number for a no-execute fault, so be\n-  \/\/ conservative and allow other handlers the first shot.\n-  \/\/\n-  \/\/ Note: We don't test that info->si_code == SEGV_ACCERR here.\n-  \/\/ this si_code is so generic that it is almost meaningless; and\n-  \/\/ the si_code for this condition may change in the future.\n-  \/\/ Furthermore, a false-positive should be harmless.\n-  if (UnguardOnExecutionViolation > 0 &&\n-      stub == nullptr &&\n-      (sig == SIGSEGV || sig == SIGBUS) &&\n-      uc->uc_mcontext.gregs[REG_TRAPNO] == trap_page_fault) {\n-    size_t page_size = os::vm_page_size();\n-    address addr = (address) info->si_addr;\n-    address pc = os::Posix::ucontext_get_pc(uc);\n-    \/\/ Make sure the pc and the faulting address are sane.\n-    \/\/\n-    \/\/ If an instruction spans a page boundary, and the page containing\n-    \/\/ the beginning of the instruction is executable but the following\n-    \/\/ page is not, the pc and the faulting address might be slightly\n-    \/\/ different - we still want to unguard the 2nd page in this case.\n-    \/\/\n-    \/\/ 15 bytes seems to be a (very) safe value for max instruction size.\n-    bool pc_is_near_addr =\n-      (pointer_delta((void*) addr, (void*) pc, sizeof(char)) < 15);\n-    bool instr_spans_page_boundary =\n-      (align_down((intptr_t) pc ^ (intptr_t) addr,\n-                       (intptr_t) page_size) > 0);\n-\n-    if (pc == addr || (pc_is_near_addr && instr_spans_page_boundary)) {\n-      static volatile address last_addr =\n-        (address) os::non_memory_address_word();\n-\n-      \/\/ In conservative mode, don't unguard unless the address is in the VM\n-      if (addr != last_addr &&\n-          (UnguardOnExecutionViolation > 1 || os::address_is_in_vm(addr))) {\n-\n-        \/\/ Set memory to RWX and retry\n-        address page_start = align_down(addr, page_size);\n-        bool res = os::protect_memory((char*) page_start, page_size,\n-                                      os::MEM_PROT_RWX);\n-\n-        log_debug(os)(\"Execution protection violation \"\n-                      \"at \" INTPTR_FORMAT\n-                      \", unguarding \" INTPTR_FORMAT \": %s, errno=%d\", p2i(addr),\n-                      p2i(page_start), (res ? \"success\" : \"failed\"), errno);\n-        stub = pc;\n-\n-        \/\/ Set last_addr so if we fault again at the same address, we don't end\n-        \/\/ up in an endless loop.\n-        \/\/\n-        \/\/ There are two potential complications here.  Two threads trapping at\n-        \/\/ the same address at the same time could cause one of the threads to\n-        \/\/ think it already unguarded, and abort the VM.  Likely very rare.\n-        \/\/\n-        \/\/ The other race involves two threads alternately trapping at\n-        \/\/ different addresses and failing to unguard the page, resulting in\n-        \/\/ an endless loop.  This condition is probably even more unlikely than\n-        \/\/ the first.\n-        \/\/\n-        \/\/ Although both cases could be avoided by using locks or thread local\n-        \/\/ last_addr, these solutions are unnecessary complication: this\n-        \/\/ handler is a best-effort safety net, not a complete solution.  It is\n-        \/\/ disabled by default and should only be used as a workaround in case\n-        \/\/ we missed any no-execute-unsafe VM code.\n-\n-        last_addr = addr;\n-      }\n-    }\n-  }\n-#endif \/\/ !AMD64\n-\n@@ -434,4 +319,0 @@\n-#ifndef AMD64\n-  \/\/ set fpu to 53 bit precision\n-  set_fpu_control_word(0x27f);\n-#endif \/\/ !AMD64\n@@ -441,1 +322,0 @@\n-#ifdef AMD64\n@@ -443,5 +323,0 @@\n-#else\n-  int fpu_control;\n-  _FPU_GETCW(fpu_control);\n-  return fpu_control & 0xffff;\n-#endif \/\/ AMD64\n@@ -451,3 +326,0 @@\n-#ifndef AMD64\n-  _FPU_SETCW(fpu_control);\n-#endif \/\/ !AMD64\n@@ -499,1 +371,0 @@\n-#ifdef _LP64\n@@ -501,3 +372,0 @@\n-#else\n-size_t os::_vm_internal_thread_min_stack_allowed = (48 DEBUG_ONLY(+ 4)) * K;\n-#endif \/\/ _LP64\n@@ -508,1 +376,0 @@\n-#ifdef AMD64\n@@ -510,3 +377,0 @@\n-#else\n-  size_t s = (thr_type == os::compiler_thread ? 2 * M : 512 * K);\n-#endif \/\/ AMD64\n@@ -525,1 +389,0 @@\n-#ifdef AMD64\n@@ -567,15 +430,0 @@\n-#else\n-  st->print(  \"EAX=\" INTPTR_FORMAT, uc->uc_mcontext.gregs[REG_EAX]);\n-  st->print(\", EBX=\" INTPTR_FORMAT, uc->uc_mcontext.gregs[REG_EBX]);\n-  st->print(\", ECX=\" INTPTR_FORMAT, uc->uc_mcontext.gregs[REG_ECX]);\n-  st->print(\", EDX=\" INTPTR_FORMAT, uc->uc_mcontext.gregs[REG_EDX]);\n-  st->cr();\n-  st->print(  \"ESP=\" INTPTR_FORMAT, uc->uc_mcontext.gregs[REG_UESP]);\n-  st->print(\", EBP=\" INTPTR_FORMAT, uc->uc_mcontext.gregs[REG_EBP]);\n-  st->print(\", ESI=\" INTPTR_FORMAT, uc->uc_mcontext.gregs[REG_ESI]);\n-  st->print(\", EDI=\" INTPTR_FORMAT, uc->uc_mcontext.gregs[REG_EDI]);\n-  st->cr();\n-  st->print(  \"EIP=\" INTPTR_FORMAT, uc->uc_mcontext.gregs[REG_EIP]);\n-  st->print(\", EFLAGS=\" INTPTR_FORMAT, uc->uc_mcontext.gregs[REG_EFL]);\n-  st->print(\", CR2=\" UINT64_FORMAT_X_0, (uint64_t)uc->uc_mcontext.cr2);\n-#endif \/\/ AMD64\n@@ -587,1 +435,1 @@\n-  const int register_count = AMD64_ONLY(16) NOT_AMD64(8);\n+  const int register_count = 16;\n@@ -600,1 +448,0 @@\n-#ifdef AMD64\n@@ -617,10 +464,0 @@\n-#else\n-    CASE_PRINT_REG(0, \"EAX=\", EAX); break;\n-    CASE_PRINT_REG(1, \"EBX=\", EBX); break;\n-    CASE_PRINT_REG(2, \"ECX=\", ECX); break;\n-    CASE_PRINT_REG(3, \"EDX=\", EDX); break;\n-    CASE_PRINT_REG(4, \"ESP=\", ESP); break;\n-    CASE_PRINT_REG(5, \"EBP=\", EBP); break;\n-    CASE_PRINT_REG(6, \"ESI=\", ESI); break;\n-    CASE_PRINT_REG(7, \"EDI=\", EDI); break;\n-#endif \/\/ AMD64\n@@ -634,5 +471,0 @@\n-#ifndef AMD64\n-  address fpu_cntrl = StubRoutines::x86::addr_fpu_cntrl_wrd_std();\n-  __asm__ volatile (  \"fldcw (%0)\" :\n-                      : \"r\" (fpu_cntrl) : \"memory\");\n-#endif \/\/ !AMD64\n@@ -643,1 +475,0 @@\n-#ifdef AMD64\n@@ -645,1 +476,0 @@\n-#endif\n","filename":"src\/hotspot\/os_cpu\/linux_x86\/os_linux_x86.cpp","additions":3,"deletions":173,"binary":false,"changes":176,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2011, 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2011, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -32,6 +32,0 @@\n-#ifndef AMD64\n-  \/\/ 64 bit result in edx:eax\n-  uint64_t res;\n-  __asm__ __volatile__ (\"rdtsc\" : \"=A\" (res));\n-  return (jlong)res;\n-#else\n@@ -43,1 +37,0 @@\n-#endif \/\/ AMD64\n","filename":"src\/hotspot\/os_cpu\/linux_x86\/os_linux_x86.inline.hpp","additions":1,"deletions":8,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2003, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2003, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -32,1 +32,0 @@\n-#ifdef AMD64\n@@ -34,1 +33,0 @@\n-#endif \/\/ AMD64\n@@ -38,2 +36,0 @@\n-#ifdef AMD64\n-\n@@ -43,2 +39,0 @@\n-\n-#endif \/\/ AMD64\n","filename":"src\/hotspot\/os_cpu\/linux_x86\/prefetch_linux_x86.inline.hpp","additions":1,"deletions":7,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -163,1 +163,0 @@\n-#if defined(_M_AMD64)\n@@ -198,1 +197,0 @@\n-#endif\n","filename":"src\/hotspot\/os_cpu\/windows_x86\/os_windows_x86.cpp","additions":0,"deletions":2,"binary":false,"changes":2,"status":"modified"}]}