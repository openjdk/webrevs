{"files":[{"patch":"@@ -97,0 +97,2 @@\n+  product(bool, UseSIMDForArrayFill, false,                             \\\n+          \"Use SIMD instructions in generated array fill code\")         \\\n","filename":"src\/hotspot\/cpu\/aarch64\/globals_aarch64.hpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -5475,72 +5475,0 @@\n-\/\/ base:   Address of a buffer to be filled, 8 bytes aligned.\n-\/\/ cnt:    Count in 8-byte unit.\n-\/\/ value:  Value to be filled with.\n-\/\/ base will point to the end of the buffer after filling.\n-void MacroAssembler::fill_words(Register base, Register cnt, Register value)\n-{\n-\/\/  Algorithm:\n-\/\/\n-\/\/    if (cnt == 0) {\n-\/\/      return;\n-\/\/    }\n-\/\/    if ((p & 8) != 0) {\n-\/\/      *p++ = v;\n-\/\/    }\n-\/\/\n-\/\/    scratch1 = cnt & 14;\n-\/\/    cnt -= scratch1;\n-\/\/    p += scratch1;\n-\/\/    switch (scratch1 \/ 2) {\n-\/\/      do {\n-\/\/        cnt -= 16;\n-\/\/          p[-16] = v;\n-\/\/          p[-15] = v;\n-\/\/        case 7:\n-\/\/          p[-14] = v;\n-\/\/          p[-13] = v;\n-\/\/        case 6:\n-\/\/          p[-12] = v;\n-\/\/          p[-11] = v;\n-\/\/          \/\/ ...\n-\/\/        case 1:\n-\/\/          p[-2] = v;\n-\/\/          p[-1] = v;\n-\/\/        case 0:\n-\/\/          p += 16;\n-\/\/      } while (cnt);\n-\/\/    }\n-\/\/    if ((cnt & 1) == 1) {\n-\/\/      *p++ = v;\n-\/\/    }\n-\n-  assert_different_registers(base, cnt, value, rscratch1, rscratch2);\n-\n-  Label fini, skip, entry, loop;\n-  const int unroll = 8; \/\/ Number of stp instructions we'll unroll\n-\n-  cbz(cnt, fini);\n-  tbz(base, 3, skip);\n-  str(value, Address(post(base, 8)));\n-  sub(cnt, cnt, 1);\n-  bind(skip);\n-\n-  andr(rscratch1, cnt, (unroll-1) * 2);\n-  sub(cnt, cnt, rscratch1);\n-  add(base, base, rscratch1, Assembler::LSL, 3);\n-  adr(rscratch2, entry);\n-  sub(rscratch2, rscratch2, rscratch1, Assembler::LSL, 1);\n-  br(rscratch2);\n-\n-  bind(loop);\n-  add(base, base, unroll * 16);\n-  for (int i = -unroll; i < 0; i++)\n-    stp(value, value, Address(base, i * 16));\n-  bind(entry);\n-  subs(cnt, cnt, unroll * 2);\n-  br(Assembler::GE, loop);\n-\n-  tbz(cnt, 0, fini);\n-  str(value, Address(post(base, 8)));\n-  bind(fini);\n-}\n-\n","filename":"src\/hotspot\/cpu\/aarch64\/macroAssembler_aarch64.cpp","additions":0,"deletions":72,"binary":false,"changes":72,"status":"modified"},{"patch":"@@ -626,0 +626,70 @@\n+  void generate_arrays_fill() {\n+    const Register to        = c_rarg0;   \/\/ source array address\n+    const Register value     = c_rarg1;   \/\/ value\n+    const Register count     = c_rarg2;   \/\/ elements count\n+    const Register end       = rscratch1; \/\/ register to keep end of array\n+    const FloatRegister simdValue = v0;   \/\/ simd register to use as tmp\n+    const Register tmpReg1 = r10;         \/\/ tmp register\n+    const Register tmpReg2 = r11;         \/\/ another tmp register\n+\n+    const int IMPL_SIZE = (UseSIMDForArrayFill ? 32 : 64); \/\/ table implementation size limit\n+    const int IMPL_MAX = 160; \/\/ max array byte size supported by table implementations\n+    const int MAX_STORE = (UseSIMDForArrayFill ? 32 : 16); \/\/ max available store\n+\n+    address implTable = StubRoutines::aarch64::_array_fill_fixed_implementations\n+        = generate_fill_table(to, value, end, simdValue, IMPL_SIZE, IMPL_MAX, MAX_STORE);\n+\n+    StubRoutines::_jbyte_fill = generate_fill(T_BYTE, false, \"jbyte_fill\", to,\n+        value, count, simdValue, end, tmpReg1, tmpReg2, IMPL_SIZE, IMPL_MAX, implTable, MAX_STORE);\n+\n+    StubRoutines::_jshort_fill = generate_fill(T_SHORT, false, \"jshort_fill\", to,\n+        value, count, simdValue, end, tmpReg1, tmpReg2, IMPL_SIZE, IMPL_MAX, implTable, MAX_STORE);\n+\n+    StubRoutines::_jint_fill = generate_fill(T_INT, false, \"jint_fill\", to,\n+        value, count, simdValue, end, tmpReg1, tmpReg2, IMPL_SIZE, IMPL_MAX, implTable, MAX_STORE);\n+\n+    StubRoutines::_arrayof_jbyte_fill = generate_fill(T_BYTE, true, \"arrayof_jbyte_fill\", to,\n+        value, count, simdValue, end, tmpReg1, tmpReg2, IMPL_SIZE, IMPL_MAX, implTable, MAX_STORE);\n+\n+    StubRoutines::_arrayof_jshort_fill = generate_fill(T_SHORT, true, \"arrayof_jshort_fill\", to,\n+        value, count, simdValue, end, tmpReg1, tmpReg2, IMPL_SIZE, IMPL_MAX, implTable, MAX_STORE);\n+\n+    StubRoutines::_arrayof_jint_fill = generate_fill(T_INT, true, \"arrayof_jint_fill\", to,\n+        value, count, simdValue, end, tmpReg1, tmpReg2, IMPL_SIZE, IMPL_MAX, implTable, MAX_STORE);\n+  }\n+\n+  \/\/ Generate table with fill implementations for various fixed lengths.\n+  \/\/\n+  \/\/ <table start>:\n+  \/\/ <implementation for 0 bytes length>;\n+  \/\/ <implementation for 1 bytes length>;\n+  \/\/ ...\n+  \/\/ <implementation for IMPL_MAX bytes length>;\n+  \/\/\n+  \/\/ Arguments:\n+  \/\/    to:        Start address.\n+  \/\/    value:     GPR register with cloned fill pattern. Initialized when\n+  \/\/                UseSIMDForArrayFill is false.\n+  \/\/    end:       End address.\n+  \/\/    simdValue: SIMD register with cloned fill pattern. Initialized when\n+  \/\/                UseSIMDForArrayFill is true.\n+  \/\/    implSize:  Size limit for implementation in bytes. Also used as an alignment.\n+  \/\/    implMax:   The maximum size of the array, in bytes, for which the code is generated.\n+  \/\/    maxStore:  The maximum available store size in bytes.\n+  address generate_fill_table(const Register to, const Register value,\n+      const Register end, const FloatRegister simdValue, const int implSize,\n+      const int implMax, const int maxStore) {\n+\n+    assert(is_power_of_2(implSize), \"arrays fill table implementation requirement\");\n+\n+    __ align(implSize);\n+    StubCodeMark mark(this, \"StubRoutines\", \"fill_table\");\n+    address start = __ pc();\n+    for (int i = 0; i <= implMax; i++) {\n+      __ align(implSize);\n+      generate_fixed_fill_tail_implementation(i, to, value, simdValue, end, maxStore,\n+          implSize, start);\n+    }\n+    return start;\n+  }\n+\n@@ -2363,0 +2433,149 @@\n+  \/\/ Generate store of size bytes at address \"a\" using gpr or simd register\n+  \/\/ depending on the value of \"UseSIMDForArrayFill\".\n+  void gen_fill_store(int size, Register gpr, FloatRegister simd, const Address &a) {\n+    if (UseSIMDForArrayFill) {\n+      switch (size) {\n+        case 1: __ str(simd, __ B, a); break;\n+        case 2: __ str(simd, __ H, a); break;\n+        case 4: __ strs(simd, a); break;\n+        case 8: __ strd(simd, a); break;\n+        case 16: __ strq(simd, a); break;\n+        case 32: __ stpq(simd, simd, a); break;\n+        default: ShouldNotReachHere();\n+      }\n+    } else {\n+      switch (size) {\n+        case 1: __ strb(gpr, a); break;\n+        case 2: __ strh(gpr, a); break;\n+        case 4: __ strw(gpr, a); break;\n+        case 8: __ str(gpr, a); break;\n+        case 16: __ stp(gpr, gpr, a); break;\n+        default: ShouldNotReachHere();\n+      }\n+    }\n+  }\n+\n+  \/\/ Generate array tail fill implementation. A set of stores with an epilogue,\n+  \/\/ where the last store will overlap with the previous store if there are more\n+  \/\/ than one stores and the remaining amount of bytes is not a power of 2.\n+  \/\/\n+  \/\/ Arguments:\n+  \/\/    tail:        Total store size in bytes.\n+  \/\/    to:          Register with start address.\n+  \/\/    value:       Register with fill pattern.\n+  \/\/    simdValue:   SIMD register with fill pattern.\n+  \/\/    endAddr:     Register with end address (for simd implementation only).\n+  \/\/    maxStore:    The maximum available store size in bytes.\n+  \/\/    maxImplSize: Implementation code size limit.\n+  \/\/    tableStart:  First (0 byte size) implementation address.\n+  void generate_fixed_fill_tail_implementation(int tail, Register to, Register value,\n+      FloatRegister simdValue, Register endAddr, const int maxStore,\n+      const int maxImplSize, address tableStart) {\n+    int initialTail = tail;\n+    int offset = 0;\n+    address start = __ pc();\n+    while (tail >= maxStore) {\n+      gen_fill_store(maxStore, value, simdValue, Address(to, offset));\n+      offset += maxStore;\n+      tail -= maxStore;\n+    }\n+    if (tail != 0) {\n+      int end = offset + tail;\n+      int storeSize = round_up_power_of_2(tail);\n+      if (storeSize > initialTail) {\n+        int halfStore = (storeSize >> 1);\n+        gen_fill_store(halfStore, value, simdValue, Address(to, offset));\n+        offset += halfStore;\n+        tail -= halfStore;\n+        storeSize = round_up_power_of_2(tail); \/\/ recalculate\n+      }\n+      int finalOffset = end - storeSize;\n+      if (storeSize < maxStore || ((finalOffset & ((maxStore >> 1) - 1)) == 0)) {\n+        gen_fill_store(storeSize, value, simdValue, Address(to, finalOffset));\n+      } else {\n+        if (UseSIMDForArrayFill) {\n+          gen_fill_store(storeSize, value, simdValue, Address(endAddr, -storeSize));\n+        } else {\n+          __ add(to, to, finalOffset);\n+          gen_fill_store(storeSize, value, simdValue, Address(to));\n+        }\n+      }\n+    }\n+    int bytesLimitLeft = maxImplSize + (__ pc() - start);\n+    int instructionsLimitLeft = bytesLimitLeft \/ NativeInstruction::instruction_size;\n+    assert(instructionsLimitLeft >= 1, \"out of table size\");\n+    int epilogueInstructionsCount = (VM_Version::use_rop_protection() ? 5 : 3);\n+    if (epilogueInstructionsCount > instructionsLimitLeft) {\n+      \/\/ reuse epilogue from \"0-byte-case\"\n+      __ b(tableStart);\n+    } else {\n+      \/\/ normal epilogue generation\n+      __ leave();\n+      __ ret(lr);\n+    }\n+  }\n+\n+  \/\/ Generate code that fills in the tail by checking count bit and using the appropriate store\n+  \/\/ Assume less than 64 bytes are available for store. Also suppose that leave() + ret(lr)\n+  \/\/ are generated later by the caller. The last store will overwrite one element in case\n+  \/\/ when there is an even number of array elements left.\n+  \/\/\n+  \/\/ Arguments:\n+  \/\/    t:         Array element type.\n+  \/\/    to:        Register with start address.\n+  \/\/    end:       Register with end address.\n+  \/\/    value:     Register with fill pattern.\n+  \/\/    count:     Counter register.\n+  \/\/    simdvalue: SIMD register with fill pattern.\n+  \/\/    maxStore:  max available store to use\n+  void generate_fill_tail_impl(BasicType t, const Register to, const Register end,\n+      const Register value, const Register count, const FloatRegister simdValue,\n+      const int maxStore) {\n+\n+    Label L_tailCheck16, L_tailCheck8, L_tailCheck4, L_tailCheck2, L_done;\n+\n+    __ tbz(count, exact_log2(32), L_tailCheck16);\n+    if (maxStore == 32) {\n+      gen_fill_store(maxStore, value, simdValue, Address(__ post(to, maxStore)));\n+    } else {\n+      gen_fill_store(maxStore, value, simdValue, Address(__ post(to, maxStore)));\n+      gen_fill_store(maxStore, value, simdValue, Address(__ post(to, maxStore)));\n+    }\n+    __ bind(L_tailCheck16);\n+    __ tbz(count, exact_log2(16), L_tailCheck8);\n+    gen_fill_store(16, value, simdValue, Address(__ post(to, 16)));\n+    __ bind(L_tailCheck8);\n+    __ tbz(count, exact_log2(8), L_tailCheck4);\n+    gen_fill_store(8, value, simdValue, Address(to));\n+    int under8bStore = (t == T_INT ? 4 : 8);\n+    gen_fill_store(under8bStore, value, simdValue, Address(end, -under8bStore));\n+    __ leave();\n+    __ ret(lr);\n+\n+    __ bind(L_tailCheck4);\n+    __ tbz(count, exact_log2(4), t == T_INT ? L_done : L_tailCheck2);\n+    gen_fill_store(4, value, simdValue, Address(to));\n+\n+    if (t != T_INT) {\n+      int under4bStore = ((t == T_BYTE) ? 4 : 2); \/\/ 0..1 short or 0..3 bytes\n+      gen_fill_store(under4bStore, value, simdValue, Address(end, -under4bStore));\n+      __ leave();\n+      __ ret(lr);\n+\n+      Label L_tailCheck1;\n+      __ bind(L_tailCheck2);\n+      __ tbz(count, exact_log2(2), t == T_SHORT ? L_done : L_tailCheck1);\n+      gen_fill_store(2, value, simdValue, Address(to));\n+      if (t == T_BYTE) {\n+        Label L_lastByte;\n+        __ bind(L_lastByte);\n+        gen_fill_store(1, value, simdValue, Address(end, -1));\n+        __ leave();\n+        __ ret(lr);\n+        __ bind(L_tailCheck1);\n+        __ tbnz(count, exact_log2(1), L_lastByte);\n+      }\n+    }\n+    __ bind(L_done);\n+  }\n+\n@@ -2364,2 +2583,1 @@\n-  \/\/ Generate stub for array fill. If \"aligned\" is true, the\n-  \/\/ \"to\" address is assumed to be heapword aligned.\n+  \/\/ Generate stub for array fill.\n@@ -2368,5 +2586,104 @@\n-  \/\/   to:    c_rarg0\n-  \/\/   value: c_rarg1\n-  \/\/   count: c_rarg2 treated as signed\n-  \/\/\n-  address generate_fill(BasicType t, bool aligned, const char *name) {\n+  \/\/   t:                Array element type.\n+  \/\/   aligned:          True if data is 8 bytes aligned.\n+  \/\/   name:             Stub name.\n+  \/\/   to:               Start address.\n+  \/\/   value:            Value to fill array with.\n+  \/\/   count:            Treated as signed, number of elements to fill.\n+  \/\/   simdValue:        Tmp simd register to use for simd implementation.\n+  \/\/   end:              Tmp register to keep the end address.\n+  \/\/   implTableAddress: Tmp register to keep the implementation table address.\n+  \/\/   tmpReg:           Tmp register for various purposes.\n+  \/\/   implSize:         Size of the table implementation.\n+  \/\/   implMax:          Maximum array byte size supported by table implementation.\n+  \/\/   implTable:        Implementation table address.\n+  \/\/   maxStore:         Maximum available store byte size.\n+  address generate_fill(BasicType t, bool aligned, const char *name, const Register to,\n+      const Register value, const Register count, const FloatRegister simdValue,\n+      const Register end, const Register implTableAddress, const Register tmpReg,\n+      const int implSize, const int implMax, address implTable, const int maxStore) {\n+    \/\/ Stub code below is implementation of Arrays.fill intrinsic for 12 cases:\n+    \/\/   1) gpr implementation for int array, start is 8-byte aligned\n+    \/\/   2) gpr implementation for short array, start is 8-byte aligned\n+    \/\/   3) gpr implementation for byte array, start is 8-byte aligned\n+    \/\/   4) gpr implementation for int array, start alignment is unknown\n+    \/\/   5) gpr implementation for short array, start alignment is unknown\n+    \/\/   6) gpr implementation for byte array, start alignment is unknown\n+    \/\/   7) simd implementation for int array, start is 8-byte aligned\n+    \/\/   8) simd implementation for short array, start is 8-byte aligned\n+    \/\/   9) simd implementation for byte array, start is 8-byte aligned\n+    \/\/   10) simd implementation for int array, start alignment is unknown\n+    \/\/   11) simd implementation for short array, start alignment is unknown\n+    \/\/   12) simd implementation for byte array, start alignment is unknown\n+    \/\/\n+    \/\/ GPR and SIMD implementations are almost identical. Most different parts are\n+    \/\/   gpr\/simd register initialization and gpr\/simd stores usage. Another natural\n+    \/\/   difference is the maximum possible store.\n+    \/\/\n+    \/\/ Very small arrays are handled separately for all cases except 9) and 12) where\n+    \/\/   not having separate code for small case gives better results according to benchmarks.\n+    \/\/\n+    \/\/ Medium size arrays are handled uniformly by using a table of implementations\n+    \/\/   for each medium size in bytes. Thresholds for considering size as medium are\n+    \/\/   selected based on benchmarking on various platforms and are:\n+    \/\/   1) 160 bytes for cases 1,2,3,7,8,9.\n+    \/\/   2) 127 bytes for cases 5, 11\n+    \/\/   3) 63 bytes for cases 6, 12\n+    \/\/\n+    \/\/ All larger than medium arrays are accessed in an aligned way by applying one of the\n+    \/\/   alignment procedures selected after benchmarking:\n+    \/\/     1) single branch checking 16-byte alignment for cases 1,2,3,7,8,9\n+    \/\/     2) 2 consequent branches with 8-byte alignment and 16-byte alignment for cases 4, 10\n+    \/\/     3) large branchless store for cases 5, 6, 11, 12\n+    \/\/\n+    \/\/ Large arrays are handled differently depending on the pattern value. For zero a separate\n+    \/\/   implementation with cache line zeroing (dc zva instruction) is used, dc zva loop is\n+    \/\/   unrolled by factor 2. For other values a common loop unrolled to store 128 bytes is used.\n+    \/\/\n+    \/\/ Post-loop processing based on branching, which shows better results in benchmarking\n+    \/\/   than existing table implementation.\n+    \/\/\n+    \/\/\n+    \/\/\n+    \/\/ High-level pseudo-code of the algorithm:\n+    \/\/\n+    \/\/ int smallThreshold = 0;\n+    \/\/ if (UseSIMDForArrayFill) {\n+    \/\/   if (t == T_INT) smallThreshold = 1;\n+    \/\/   if (t == T_SHORT) smallthreshold = 2;\n+    \/\/ } else {\n+    \/\/   if (t == T_INT) smallThreshold = 1;\n+    \/\/   if (t == T_SHORT) smallthreshold = 3;\n+    \/\/   if (t == T_BYTE) smallThreshold = 6;\n+    \/\/ }\n+    \/\/ if (smallThreshold != 0 && count <= smallThreshold) {\n+    \/\/   goto implTable + count * implSize; \/\/ see generate_fixed_fill_tail_implementation(...)\n+    \/\/ }\n+    \/\/ [align data by 16 bytes];\n+    \/\/ count = count * ELEMENT_SIZE; \/\/ count in bytes\n+    \/\/ if (UseBlockZeroing && value == 0 && count >= BlockZeroingLowLimit) {\n+    \/\/   goto specialZeroingImplementation;\n+    \/\/ }\n+    \/\/ while(count > 128) {\n+    \/\/   [store 128 bytes];\n+    \/\/   count -= 128;\n+    \/\/ }\n+    \/\/ [store tail of 0..127 bytes]; \/\/ see generate_fill_tail_impl(...)\n+    \/\/ return;\n+    \/\/\n+    \/\/ specialZeroingImplementation:\n+    \/\/ [align by zva_length];\n+    \/\/ while(count >= 2 * zva_length) {\n+    \/\/   [clear cache line with zva_length bytes];\n+    \/\/   [clear cache line with zva_length bytes];\n+    \/\/   count -= 2 * zva_length;\n+    \/\/ }\n+    \/\/ if (count >= zva_length) {\n+    \/\/   [clear cache line with zva_length bytes];\n+    \/\/ }\n+    \/\/ [store tail of 0..zva_length-1 bytes]; \/\/ see generate_fill_tail_impl(...)\n+    \/\/\n+\n+    assert(t == T_BYTE || t == T_SHORT || t == T_INT, \"unsupported\");\n+\n+    assert_different_registers(to, value, count, end, rscratch2, implTableAddress, tmpReg);\n+\n@@ -2379,3 +2696,16 @@\n-    const Register to        = c_rarg0;  \/\/ source array address\n-    const Register value     = c_rarg1;  \/\/ value\n-    const Register count     = c_rarg2;  \/\/ elements count\n+    const int LOOP_BYTE_SIZE = 128; \/\/ bytes stored in loop iteration\n+    const int LOG_MAX_STORE = exact_log2(maxStore);\n+    const int LOG_INSTR_SIZE = exact_log2(NativeInstruction::instruction_size);\n+    \/\/ Use different threshold for different element types. Selected according\n+    \/\/ to benchmarking.\n+    const int ALIGNMENT_THRESHOLD = ((aligned || t == T_INT) ? implMax : (t == T_SHORT ? 127 : 63));\n+    const int MAX_SUPPORTED_TAIL_SIZE = 63; \/\/ generate_fill_tail_impl limitation\n+    const int ELEMENT_SIZE = type2aelembytes(t);\n+    const int ZVA_LENGTH = VM_Version::zva_length();\n+\n+    assert(is_power_of_2(LOOP_BYTE_SIZE), \"check size of data stored in loop\");\n+    assert(implMax >= maxStore, \"code requirement\");\n+    assert(ALIGNMENT_THRESHOLD > (32 + 16), \"code requirement\");\n+    assert(BlockZeroingLowLimit >= 2 * ZVA_LENGTH, \"code requirement\");\n+\n+    Label L_loop, L_under64bytes, L_under128bytes,  L_small, L_large, L_zero, L_done;\n@@ -2383,2 +2713,1 @@\n-    const Register bz_base = r10;        \/\/ base for block_zero routine\n-    const Register cnt_words = r11;      \/\/ temp register\n+    int shift = exact_log2(ELEMENT_SIZE);\n@@ -2387,0 +2716,72 @@\n+    if (UseSIMDForArrayFill) {\n+      if (t == T_INT) {\n+        \/\/ Handle size=1 specifically to avoid performance degradation in this case\n+        \/\/ at the cost of some performance for all other cases.\n+        Label L_not1;\n+        __ cmp(count, (u1)1);\n+        __ br(__ NE, L_not1);\n+        __ strw(value, Address(to));\n+        __ leave();\n+        __ ret(lr);\n+        __ bind(L_not1);\n+      }\n+      __ dup(simdValue, __ esize2arrangement(ELEMENT_SIZE, true), value);\n+      if (t == T_SHORT) {\n+        \/\/ As above, handle short arrays of one or two elements to avoid performance\n+        \/\/ degradation of this case, despite some performance loss for all other cases.\n+        Label L_proceed;\n+        __ cmp(count, (u1)2);\n+        __ br(__ GT, L_proceed);\n+        __ br(__ EQ, implTable + 2 * ELEMENT_SIZE * implSize);\n+        __ tbnz(count, 0, implTable + ELEMENT_SIZE * implSize);\n+        __ bind(L_proceed);\n+      }\n+      __ adr(implTableAddress, implTable);\n+      __ add(end, to, count, __ LSL, shift); \/\/ keep end address, because simd-version table uses it\n+      __ cmp(count, (u1)(ALIGNMENT_THRESHOLD >> shift));\n+    } else {\n+      \/\/ Similar to cases above, handle small byte arrays.\n+      if (t == T_BYTE) {\n+        __ cmp(count, (u1)6);\n+        __ bfi(value, value, BitsPerByte, BitsPerByte);\n+        __ bfi(value, value, 2 * BitsPerByte, 2 * BitsPerByte);\n+        __ br(__ LE, L_small);\n+      } else if (t == T_SHORT) {\n+        __ cmp(count, (u1)4);\n+        __ bfi(value, value, ELEMENT_SIZE * BitsPerByte, ELEMENT_SIZE * BitsPerByte);\n+        __ br(__ LT, L_small);\n+      } else {\n+        __ cmp(count, (u1)1);\n+        __ br(__ EQ, L_small);\n+      }\n+      __ cmp(count, (u1)(ALIGNMENT_THRESHOLD >> shift));\n+      __ adr(implTableAddress, implTable);\n+      __ bfi(value, value, 4 * BitsPerByte, 4 * BitsPerByte);\n+    }\n+    __ br(__ GT, L_large);\n+    \/\/ jump to table implementation\n+    __ add(implTableAddress, implTableAddress, count, __ LSL, exact_log2(implSize) + shift);\n+    __ br(implTableAddress);\n+\n+    if (!UseSIMDForArrayFill) {\n+      \/\/ separate block with small-size implementation for non-simd case\n+      Label L_check4bytes, L_smallDone;\n+      __ align(OptoLoopAlignment);\n+      __ bind(L_small);\n+      if (t == T_BYTE) {\n+        Label L_check2bytes;\n+        __ tbz(count, 0, L_check2bytes);\n+        __ strb(value, Address(__ post(to, 1)));\n+        __ bind(L_check2bytes);\n+      }\n+      if (t != T_INT) {\n+        __ tbz(count, 1 - shift, L_check4bytes);\n+        __ strh(value, Address(__ post(to, 2)));\n+        __ bind(L_check4bytes);\n+        __ tbz(count, 2 - shift, L_smallDone);\n+      }\n+      __ strw(value, Address(to));\n+      __ bind(L_smallDone);\n+      __ leave();\n+      __ ret(lr);\n+    }\n@@ -2388,51 +2789,14 @@\n-    Label L_fill_elements, L_exit1;\n-\n-    int shift = -1;\n-    switch (t) {\n-      case T_BYTE:\n-        shift = 0;\n-        __ cmpw(count, 8 >> shift); \/\/ Short arrays (< 8 bytes) fill by element\n-        __ bfi(value, value, 8, 8);   \/\/ 8 bit -> 16 bit\n-        __ bfi(value, value, 16, 16); \/\/ 16 bit -> 32 bit\n-        __ br(Assembler::LO, L_fill_elements);\n-        break;\n-      case T_SHORT:\n-        shift = 1;\n-        __ cmpw(count, 8 >> shift); \/\/ Short arrays (< 8 bytes) fill by element\n-        __ bfi(value, value, 16, 16); \/\/ 16 bit -> 32 bit\n-        __ br(Assembler::LO, L_fill_elements);\n-        break;\n-      case T_INT:\n-        shift = 2;\n-        __ cmpw(count, 8 >> shift); \/\/ Short arrays (< 8 bytes) fill by element\n-        __ br(Assembler::LO, L_fill_elements);\n-        break;\n-      default: ShouldNotReachHere();\n-    }\n-\n-    \/\/ Align source address at 8 bytes address boundary.\n-    Label L_skip_align1, L_skip_align2, L_skip_align4;\n-    if (!aligned) {\n-      switch (t) {\n-        case T_BYTE:\n-          \/\/ One byte misalignment happens only for byte arrays.\n-          __ tbz(to, 0, L_skip_align1);\n-          __ strb(value, Address(__ post(to, 1)));\n-          __ subw(count, count, 1);\n-          __ bind(L_skip_align1);\n-          \/\/ Fallthrough\n-        case T_SHORT:\n-          \/\/ Two bytes misalignment happens only for byte and short (char) arrays.\n-          __ tbz(to, 1, L_skip_align2);\n-          __ strh(value, Address(__ post(to, 2)));\n-          __ subw(count, count, 2 >> shift);\n-          __ bind(L_skip_align2);\n-          \/\/ Fallthrough\n-        case T_INT:\n-          \/\/ Align to 8 bytes, we know we are 4 byte aligned to start.\n-          __ tbz(to, 2, L_skip_align4);\n-          __ strw(value, Address(__ post(to, 4)));\n-          __ subw(count, count, 4 >> shift);\n-          __ bind(L_skip_align4);\n-          break;\n-        default: ShouldNotReachHere();\n+    \/\/ align data by 16 bytes\n+    __ align(OptoLoopAlignment);\n+    __ bind(L_large);\n+    if (!UseSIMDForArrayFill) {\n+      __ add(end, to, count, __ LSL, shift);\n+    }\n+    if (aligned) {\n+      Label L_aligned16;\n+      __ tbz(to, exact_log2(8), L_aligned16);\n+      gen_fill_store(8, value, simdValue, Address(__ post(to, 8)));\n+      __ sub(count, count, 8 >> shift);\n+      __ bind(L_aligned16);\n+      if (shift != 0) {\n+        __ lslw(count, count, shift);\n@@ -2440,0 +2804,17 @@\n+    } else {\n+      if (t == T_INT) {\n+        Label L_aligned8;\n+        __ tbz(to, exact_log2(4), L_aligned8);\n+        gen_fill_store(4, value, simdValue, Address(__ post(to, 4)));\n+        __ sub(count, count, 4 >> shift);\n+        __ bind(L_aligned8);\n+        Label L_aligned16;\n+        __ tbz(to, exact_log2(8), L_aligned16);\n+        gen_fill_store(8, value, simdValue, Address(__ post(to, 8)));\n+        __ bind(L_aligned16);\n+      } else {\n+        gen_fill_store(16, value, simdValue, Address(__ post(to, 16)));\n+        __ bfi(to, zr, 0, exact_log2(16));\n+      }\n+      __ sub(count, end, to);\n+      shift = 0;\n@@ -2442,6 +2823,1 @@\n-    \/\/\n-    \/\/  Fill large chunks\n-    \/\/\n-    __ lsrw(cnt_words, count, 3 - shift); \/\/ number of words\n-    __ bfi(value, value, 32, 32);         \/\/ 32 bit -> 64 bit\n-    __ subw(count, count, cnt_words, Assembler::LSL, 3 - shift);\n+    \/\/ check and jump to block zeroing code if applicable\n@@ -2449,8 +2825,8 @@\n-      Label non_block_zeroing, rest;\n-      \/\/ If the fill value is zero we can use the fast zero_words().\n-      __ cbnz(value, non_block_zeroing);\n-      __ mov(bz_base, to);\n-      __ add(to, to, cnt_words, Assembler::LSL, LogBytesPerWord);\n-      address tpc = __ zero_words(bz_base, cnt_words);\n-      if (tpc == nullptr) {\n-        fatal(\"CodeCache is full at generate_fill\");\n+      if (ALIGNMENT_THRESHOLD < BlockZeroingLowLimit) {\n+        Label L_non_zero;\n+        __ cbnz(value, L_non_zero);\n+        __ subs(zr, count, BlockZeroingLowLimit);\n+        __ br(__ GE, L_zero);\n+        __ bind(L_non_zero);\n+      } else {\n+        __ cbz(value, L_zero);\n@@ -2458,6 +2834,0 @@\n-      __ b(rest);\n-      __ bind(non_block_zeroing);\n-      __ fill_words(to, cnt_words, value);\n-      __ bind(rest);\n-    } else {\n-      __ fill_words(to, cnt_words, value);\n@@ -2466,10 +2836,7 @@\n-    \/\/ Remaining count is less than 8 bytes. Fill it by a single store.\n-    \/\/ Note that the total length is no less than 8 bytes.\n-    if (t == T_BYTE || t == T_SHORT) {\n-      Label L_exit1;\n-      __ cbzw(count, L_exit1);\n-      __ add(to, to, count, Assembler::LSL, shift); \/\/ points to the end\n-      __ str(value, Address(to, -8));    \/\/ overwrite some elements\n-      __ bind(L_exit1);\n-      __ leave();\n-      __ ret(lr);\n+    \/\/ main non-zero fill code for large arrays\n+    int minRemainingBytes = ALIGNMENT_THRESHOLD - 16;\n+    if (minRemainingBytes > LOOP_BYTE_SIZE) {\n+      __ lsrw(rscratch2, count, exact_log2(LOOP_BYTE_SIZE));\n+    } else {\n+      __ adds(rscratch2, zr, count, __ LSR, exact_log2(LOOP_BYTE_SIZE));\n+      __ br(__ EQ, L_under128bytes);\n@@ -2478,26 +2845,12 @@\n-    \/\/ Handle copies less than 8 bytes.\n-    Label L_fill_2, L_fill_4, L_exit2;\n-    __ bind(L_fill_elements);\n-    switch (t) {\n-      case T_BYTE:\n-        __ tbz(count, 0, L_fill_2);\n-        __ strb(value, Address(__ post(to, 1)));\n-        __ bind(L_fill_2);\n-        __ tbz(count, 1, L_fill_4);\n-        __ strh(value, Address(__ post(to, 2)));\n-        __ bind(L_fill_4);\n-        __ tbz(count, 2, L_exit2);\n-        __ strw(value, Address(to));\n-        break;\n-      case T_SHORT:\n-        __ tbz(count, 0, L_fill_4);\n-        __ strh(value, Address(__ post(to, 2)));\n-        __ bind(L_fill_4);\n-        __ tbz(count, 1, L_exit2);\n-        __ strw(value, Address(to));\n-        break;\n-      case T_INT:\n-        __ cbzw(count, L_exit2);\n-        __ strw(value, Address(to));\n-        break;\n-      default: ShouldNotReachHere();\n+    __ bind(L_loop); \/\/ loop begin\n+    __ add(to, to, LOOP_BYTE_SIZE);\n+    __ sub(rscratch2, rscratch2, 1);\n+    for (int i = 0; i < LOOP_BYTE_SIZE; i += maxStore) {\n+      gen_fill_store(maxStore, value, simdValue, Address(to, i - LOOP_BYTE_SIZE));\n+    }\n+    __ cbnz(rscratch2, L_loop); \/\/ loop end\n+    \/\/ 0..(LOOP_BYTE_SIZE\/2 - 1) bytes left\n+    if (aligned) {\n+      \/\/ Take a chance that the large array is a power of two long and already properly aligned.\n+      __ tst(count, LOOP_BYTE_SIZE - 1);\n+      __ br(__ EQ, L_done);\n@@ -2505,1 +2858,9 @@\n-    __ bind(L_exit2);\n+    __ bind(L_under128bytes);\n+    __ tbz(count, exact_log2(LOOP_BYTE_SIZE) - 1, L_under64bytes);\n+    for (int i = 0; i < LOOP_BYTE_SIZE\/2; i += maxStore) {\n+      gen_fill_store(maxStore, value, simdValue, Address(to, i));\n+    }\n+    __ add(to, to, LOOP_BYTE_SIZE\/2);\n+    __ bind(L_under64bytes);\n+    generate_fill_tail_impl(t, to, end, value, count, simdValue, maxStore);\n+    __ bind(L_done);\n@@ -2508,0 +2869,81 @@\n+\n+    \/\/ block zeroing code\n+    if (UseBlockZeroing) {\n+      Label L_loop_zva, L_post_loop_zva, L_zva_tail, L_aligned;\n+      __ align(OptoLoopAlignment);\n+      __ bind(L_zero);\n+\n+      \/\/ align by zva_length\n+      if (ZVA_LENGTH == 64) {\n+        Label L_aligned32;\n+        __ tbz(to, exact_log2(16), L_aligned32);\n+        gen_fill_store(16, zr, simdValue, Address(__ post(to, 16)));\n+        __ bind(L_aligned32);\n+        __ tbz(to, exact_log2(32), L_aligned);\n+        if (maxStore == 32) {\n+          gen_fill_store(maxStore, zr, simdValue, Address(__ post(to, maxStore)));\n+        } else {\n+          gen_fill_store(maxStore, zr, simdValue, Address(__ post(to, maxStore)));\n+          gen_fill_store(maxStore, zr, simdValue, Address(__ post(to, maxStore)));\n+        }\n+      } else { \/\/ generic code\n+        if (maxStore == 32) {\n+          \/\/ align to maxStore\n+          Label L_aligned32;\n+          __ tbz(to, exact_log2(16), L_aligned32);\n+          gen_fill_store(16, zr, simdValue, Address(__ post(to, 16)));\n+          __ bind(L_aligned32);\n+        }\n+        __ neg(tmpReg, to);\n+        __ andr(tmpReg, tmpReg, ZVA_LENGTH - 1);\n+        __ adr(rscratch2, L_aligned);\n+        __ add(to, to, tmpReg);\n+        __ sub(rscratch2, rscratch2, tmpReg, __ LSR, exact_log2(maxStore) - LOG_INSTR_SIZE);\n+        __ br(rscratch2);\n+        for (int i = -ZVA_LENGTH + maxStore; i < 0; i += maxStore) {\n+          gen_fill_store(maxStore, zr, simdValue, Address(to, i));\n+        }\n+      }\n+      __ bind(L_aligned);\n+      if (BlockZeroingLowLimit < 3 * ZVA_LENGTH) { \/\/ no known platform so far\n+        __ sub(count, end, to);\n+        \/\/ zva loop is unrolled by 2 zva instructions\n+        __ lsrw(tmpReg, count, exact_log2(ZVA_LENGTH) + 1);\n+        __ cbz(tmpReg, L_post_loop_zva);\n+      } else {\n+        \/\/ at least 2 zva needed. Issue as early as possible\n+        __ dc(Assembler::ZVA, to);\n+        __ add(to, to, ZVA_LENGTH);\n+        __ dc(Assembler::ZVA, to);\n+        __ add(to, to, ZVA_LENGTH);\n+        __ sub(count, end, to);\n+        __ lsrw(tmpReg, count, exact_log2(ZVA_LENGTH) + 1);\n+        __ cbz(tmpReg, L_post_loop_zva);\n+      }\n+      __ bind(L_loop_zva);\n+      __ dc(Assembler::ZVA, to);\n+      __ add(to, to, ZVA_LENGTH);\n+      __ dc(Assembler::ZVA, to);\n+      __ sub(tmpReg, tmpReg, 1);\n+      __ add(to, to, ZVA_LENGTH);\n+      __ cbnz(tmpReg, L_loop_zva);\n+      __ bind(L_post_loop_zva);\n+      __ tbz(count, exact_log2(ZVA_LENGTH), L_zva_tail);\n+      __ dc(Assembler::ZVA, to);\n+      __ add(to, to, ZVA_LENGTH);\n+      __ bind(L_zva_tail);\n+      if ((ZVA_LENGTH - 1) > MAX_SUPPORTED_TAIL_SIZE) { \/\/ most CPUs have zva_length == 64\n+        __ sub(count, end, to);\n+        Label L_proceed, L_recheck;\n+        __ bind(L_recheck);\n+        __ cmp(count, (u1)MAX_SUPPORTED_TAIL_SIZE);\n+        __ br(__ LT, L_proceed);\n+        gen_fill_store(maxStore, zr, simdValue, Address(__ post(to, maxStore)));\n+        __ sub(count, count, maxStore);\n+        __ b(L_recheck);\n+        __ bind(L_proceed);\n+      }\n+      generate_fill_tail_impl(t, to, end, zr, count, simdValue, maxStore);\n+      __ leave();\n+      __ ret(lr);\n+    }\n@@ -2660,6 +3102,1 @@\n-    StubRoutines::_jbyte_fill = generate_fill(T_BYTE, false, \"jbyte_fill\");\n-    StubRoutines::_jshort_fill = generate_fill(T_SHORT, false, \"jshort_fill\");\n-    StubRoutines::_jint_fill = generate_fill(T_INT, false, \"jint_fill\");\n-    StubRoutines::_arrayof_jbyte_fill = generate_fill(T_BYTE, true, \"arrayof_jbyte_fill\");\n-    StubRoutines::_arrayof_jshort_fill = generate_fill(T_SHORT, true, \"arrayof_jshort_fill\");\n-    StubRoutines::_arrayof_jint_fill = generate_fill(T_INT, true, \"arrayof_jint_fill\");\n+    generate_arrays_fill();\n","filename":"src\/hotspot\/cpu\/aarch64\/stubGenerator_aarch64.cpp","additions":563,"deletions":126,"binary":false,"changes":689,"status":"modified"},{"patch":"@@ -48,0 +48,1 @@\n+address StubRoutines::aarch64::_array_fill_fixed_implementations = NULL;\n","filename":"src\/hotspot\/cpu\/aarch64\/stubRoutines_aarch64.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2003, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2003, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -39,1 +39,1 @@\n-  code_size2 = 45000           \/\/ simply increase if too small (assembler will crash if too small)\n+  code_size2 = 53000           \/\/ simply increase if too small (assembler will crash if too small)\n@@ -60,0 +60,1 @@\n+  static address _array_fill_fixed_implementations;\n@@ -135,0 +136,4 @@\n+  static address array_fill_fixed_implementations() {\n+    return _array_fill_fixed_implementations;\n+  }\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/stubRoutines_aarch64.hpp","additions":7,"deletions":2,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -223,0 +223,6 @@\n+\n+    if (FLAG_IS_DEFAULT(UseSIMDForArrayFill)) {\n+      if (_model == 0xd40 || _model2 == 0xd40) { \/\/ Neoverse N2\n+        FLAG_SET_DEFAULT(UseSIMDForArrayFill, true);\n+      }\n+    }\n","filename":"src\/hotspot\/cpu\/aarch64\/vm_version_aarch64.cpp","additions":6,"deletions":0,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -0,0 +1,202 @@\n+\/*\n+ * Copyright (c) 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2023, BELLSOFT. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+package compiler.intrinsics;\n+\n+import java.lang.reflect.Method;\n+import java.util.function.IntConsumer;\n+import java.util.Arrays;\n+import jdk.test.whitebox.WhiteBox;\n+import jdk.test.lib.Asserts;\n+\n+import static jdk.test.lib.Asserts.assertEQ;\n+import static jdk.test.lib.Asserts.assertTrue;\n+import static compiler.whitebox.CompilerWhiteBoxTest.COMP_LEVEL_FULL_OPTIMIZATION;\n+\n+\/*\n+ * @test\n+ * @bug 8300669\n+ * @summary check Arrays.fill methods for byte length up to 600 with aligned, unaligned\n+ *          and zero-nonzero cases.\n+ * @requires vm.flavor == \"server\" & (vm.opt.TieredStopAtLevel == null | vm.opt.TieredStopAtLevel == 4)\n+ * @library \/test\/lib \/\n+ * @modules java.base\/jdk.internal.misc\n+ *\n+ * @build jdk.test.whitebox.WhiteBox\n+ * @run driver jdk.test.lib.helpers.ClassFileInstaller jdk.test.whitebox.WhiteBox\n+ *\n+ * @run main\/othervm -Xbootclasspath\/a:. -XX:+UnlockDiagnosticVMOptions -XX:+WhiteBoxAPI\n+ *                   -server -XX:-BackgroundCompilation -XX:-UseOnStackReplacement\n+ *                   -XX:+IgnoreUnrecognizedVMOptions -XX:-UseSIMDForArrayEquals\n+ *                   compiler.intrinsics.TestArraysFill\n+ *\n+ * @run main\/othervm -Xbootclasspath\/a:. -XX:+UnlockDiagnosticVMOptions -XX:+WhiteBoxAPI\n+ *                   -server -XX:-BackgroundCompilation -XX:-UseOnStackReplacement\n+ *                   -XX:+IgnoreUnrecognizedVMOptions -XX:+UseSIMDForArrayEquals\n+ *                   compiler.intrinsics.TestArraysFill\n+ *\/\n+\n+public class TestArraysFill {\n+    static final int MAX_BYTES_LENGTH_CHECK = 600;\n+    static final int SIZE = MAX_BYTES_LENGTH_CHECK + 150;\n+    static final int START_BYTE = 64 + 4;\n+    static final byte BYTE_PATTERN = (byte) 42;\n+    static final short SHORT_PATTERN = (short) 0x1234;\n+    static final int INT_PATTERN = 0xCAFEBABE;\n+\n+    WhiteBox wb = WhiteBox.getWhiteBox();\n+\n+    public static void main(String[] args) throws Exception {\n+        var t = new TestArraysFill();\n+        t.testByteAll();\n+        t.testShortAll();\n+        t.testIntAll();\n+    }\n+\n+    void testByteAll() throws Exception {\n+        testAll(byte[].class, this::testByteForLength, MAX_BYTES_LENGTH_CHECK);\n+    }\n+\n+    void testShortAll() throws Exception {\n+        testAll(short[].class, this::testShortForLength, MAX_BYTES_LENGTH_CHECK \/ 2);\n+    }\n+\n+    void testIntAll() throws Exception {\n+        testAll(int[].class, this::testIntForLength, MAX_BYTES_LENGTH_CHECK \/ 4);\n+    }\n+\n+    void testAll(Class arrayType, IntConsumer testForLength, int maxArrayLength) throws NoSuchMethodException {\n+        var elemType = arrayType.getComponentType();\n+        var m2 = Arrays.class.getDeclaredMethod(\"fill\", arrayType, elemType);\n+        var m4 = Arrays.class.getDeclaredMethod(\"fill\", arrayType, int.class, int.class, elemType);\n+\n+        testForLength.accept(maxArrayLength); \/\/ warmup\n+\n+        assertTrue(wb.enqueueMethodForCompilation(m2, COMP_LEVEL_FULL_OPTIMIZATION));\n+        assertTrue(wb.enqueueMethodForCompilation(m4, COMP_LEVEL_FULL_OPTIMIZATION));\n+\n+        for (int length = 0; length < maxArrayLength; length++) {\n+            assertEQ(wb.getMethodCompilationLevel(m2), COMP_LEVEL_FULL_OPTIMIZATION);\n+            assertEQ(wb.getMethodCompilationLevel(m4), COMP_LEVEL_FULL_OPTIMIZATION);\n+            testForLength.accept(length);\n+            assertEQ(wb.getMethodCompilationLevel(m2), COMP_LEVEL_FULL_OPTIMIZATION);\n+            assertEQ(wb.getMethodCompilationLevel(m4), COMP_LEVEL_FULL_OPTIMIZATION);\n+        }\n+    }\n+\n+    void testByteForLength(int length) {\n+        testByte2(length, (byte) 0, BYTE_PATTERN);\n+        testByte2(length, BYTE_PATTERN, (byte) 0);\n+\n+        testByte4(length, (byte) 0, BYTE_PATTERN);\n+        testByte4(length, BYTE_PATTERN, (byte) 0);\n+    }\n+\n+    void testShortForLength(int length) {\n+        testShort2(length, (short) 0, SHORT_PATTERN);\n+        testShort2(length, SHORT_PATTERN, (short) 0);\n+\n+        testShort4(length, (short) 0, SHORT_PATTERN);\n+        testShort4(length, SHORT_PATTERN, (short) 0);\n+    }\n+\n+    void testIntForLength(int length) {\n+        testInt2(length, 0, INT_PATTERN);\n+        testInt2(length, INT_PATTERN, 0);\n+\n+        testInt4(length, 0, INT_PATTERN);\n+        testInt4(length, INT_PATTERN, 0);\n+    }\n+\n+    void testByte2(int length, byte initVal, byte fillVal) {\n+        var data = new byte[length];\n+        for (int i = 0; i < data.length; i++)\n+            data[i] = initVal;\n+        Arrays.fill(data, fillVal);\n+        for (int i = 0; i < length; i++)\n+            check(i, data[i], 0, length, initVal, fillVal);\n+    }\n+\n+    void testByte4(int length, byte initVal, byte fillVal) {\n+        var data = new byte[SIZE];\n+        for (int i = 0; i < data.length; i++)\n+            data[i] = initVal;\n+        int start = START_BYTE;\n+        int end = start + length;\n+        Arrays.fill(data, start, end, fillVal);\n+        for (int i = Math.max(0, start - 64); i < Math.min(end + 64, data.length); i++)\n+            check(i, data[i], start, end, initVal, fillVal);\n+    }\n+\n+    void testShort2(int length, short initVal, short fillVal) {\n+        var data = new short[length];\n+        for (int i = 0; i < data.length; i++)\n+            data[i] = initVal;\n+        Arrays.fill(data, fillVal);\n+        for (int i = 0; i < length; i++)\n+            check(i, data[i], 0, length, initVal, fillVal);\n+    }\n+\n+    void testShort4(int length, short initVal, short fillVal) {\n+        var data = new short[SIZE \/ 2];\n+        for (int i = 0; i < data.length; i++)\n+            data[i] = initVal;\n+        int start = START_BYTE \/ 2;\n+        int end = start + length;\n+        Arrays.fill(data, start, end, fillVal);\n+        for (int i = Math.max(0, start - 64); i < Math.min(end + 64, data.length); i++)\n+            check(i, data[i], start, end, initVal, fillVal);\n+    }\n+\n+    void testInt2(int length, int initVal, int fillVal) {\n+        var data = new int[length];\n+        for (int i = 0; i < data.length; i++)\n+            data[i] = initVal;\n+        Arrays.fill(data, fillVal);\n+        for (int i = 0; i < length; i++)\n+            check(i, data[i], 0, length, initVal, fillVal);\n+    }\n+\n+    void testInt4(int length, int initVal, int fillVal) {\n+        var data = new int[SIZE \/ 4];\n+        for (int i = 0; i < data.length; i++)\n+            data[i] = initVal;\n+        int start = START_BYTE \/ 4;\n+        int end = start + length;\n+        Arrays.fill(data, start, end, fillVal);\n+        for (int i = Math.max(0, start - 64); i < Math.min(end + 64, data.length); i++)\n+            check(i, data[i], start, end, initVal, fillVal);\n+    }\n+\n+    void check(int i, int actual, int start, int end, int initVal, int fillVal) {\n+        int len = end - start;\n+        if (i < start) {\n+            assertEQ(actual, initVal, String.format(\"Corrupted value at %d before [%d x %d]\", i, len, fillVal));\n+        } else if (i >= end) {\n+            assertEQ(actual, initVal, String.format(\"Corrupted value at %d after [%d x %d]\", i, len, fillVal));\n+        } else {\n+            assertEQ(actual, fillVal, String.format(\"Wrong value at %d in [%d x %d]\", i - start, len, fillVal));\n+        }\n+    }\n+}\n","filename":"test\/hotspot\/jtreg\/compiler\/intrinsics\/TestArraysFill.java","additions":202,"deletions":0,"binary":false,"changes":202,"status":"added"}]}