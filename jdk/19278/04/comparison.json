{"files":[{"patch":"@@ -7783,1 +7783,1 @@\n-            \"dmb ish\" %}\n+            \"dmb ishld\" %}\n@@ -7837,1 +7837,1 @@\n-            \"dmb ish\" %}\n+            \"dmb ishst\\n\\tdmb ishld\" %}\n@@ -7841,1 +7841,2 @@\n-    __ membar(Assembler::LoadStore|Assembler::StoreStore);\n+    __ membar(Assembler::StoreStore);\n+    __ membar(Assembler::LoadStore);\n","filename":"src\/hotspot\/cpu\/aarch64\/aarch64.ad","additions":4,"deletions":3,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -127,0 +127,2 @@\n+  product(bool, AlwaysMergeDMB, true, DIAGNOSTIC,                       \\\n+          \"Always merge DMB instructions in code emission\")             \\\n","filename":"src\/hotspot\/cpu\/aarch64\/globals_aarch64.hpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -2353,7 +2353,27 @@\n-    \/\/ We are merging two memory barrier instructions.  On AArch64 we\n-    \/\/ can do this simply by ORing them together.\n-    bar->set_kind(bar->get_kind() | order_constraint);\n-    BLOCK_COMMENT(\"merged membar\");\n-  } else {\n-    code()->set_last_insn(pc());\n-    dmb(Assembler::barrier(order_constraint));\n+    if (AlwaysMergeDMB) {\n+      bar->set_kind(bar->get_kind() | order_constraint);\n+      BLOCK_COMMENT(\"merged membar(always)\");\n+      return;\n+    }\n+    \/\/ Don't promote DMB ST|DMB LD to DMB (a full barrier) because\n+    \/\/ doing so would introduce a StoreLoad which the caller did not\n+    \/\/ intend\n+    if (bar->get_kind() == order_constraint\n+        || bar->get_kind() == AnyAny\n+        || order_constraint == AnyAny) {\n+      \/\/ We are merging two memory barrier instructions.  On AArch64 we\n+      \/\/ can do this simply by ORing them together.\n+      bar->set_kind(bar->get_kind() | order_constraint);\n+      BLOCK_COMMENT(\"merged membar\");\n+      return;\n+    } else {\n+      \/\/ A special case like \"DMB ST;DMB LD;DMB ST\", the last DMB can be skipped\n+      \/\/ We need check the last 2 instructions\n+      address prev2 = prev - NativeMembar::instruction_size;\n+      if (last != code()->last_label() && nativeInstruction_at(prev2)->is_Membar()) {\n+        NativeMembar *bar2 = NativeMembar_at(prev2);\n+        assert(bar2->get_kind() == order_constraint, \"it should be merged before\");\n+        BLOCK_COMMENT(\"merged membar(elided)\");\n+        return;\n+      }\n+    }\n@@ -2361,0 +2381,2 @@\n+  code()->set_last_insn(pc());\n+  dmb(Assembler::barrier(order_constraint));\n","filename":"src\/hotspot\/cpu\/aarch64\/macroAssembler_aarch64.cpp","additions":29,"deletions":7,"binary":false,"changes":36,"status":"modified"},{"patch":"@@ -153,0 +153,1 @@\n+    code()->set_last_label(pc());\n","filename":"src\/hotspot\/cpu\/aarch64\/macroAssembler_aarch64.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -233,0 +233,3 @@\n+    if (FLAG_IS_DEFAULT(AlwaysMergeDMB)) {\n+      FLAG_SET_DEFAULT(AlwaysMergeDMB, false);\n+    }\n","filename":"src\/hotspot\/cpu\/aarch64\/vm_version_aarch64.cpp","additions":4,"deletions":1,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -933,0 +933,4 @@\n+  \/\/ some internal addresses, _last_insn _last_label, are used during code emission,\n+  \/\/ adjust them in expansion\n+  adjust_internal_address(insts_begin(), cb.insts_begin());\n+\n@@ -954,0 +958,9 @@\n+void CodeBuffer::adjust_internal_address(address from, address to) {\n+  if (_last_insn != nullptr) {\n+    _last_insn += to - from;\n+  }\n+  if (_last_label != nullptr) {\n+    _last_label += to - from;\n+  }\n+}\n+\n","filename":"src\/hotspot\/share\/asm\/codeBuffer.cpp","additions":14,"deletions":1,"binary":false,"changes":15,"status":"modified"},{"patch":"@@ -439,0 +439,1 @@\n+  address      _last_label;     \/\/ record last bind label address, it's also the start of current bb.\n@@ -463,0 +464,1 @@\n+    _last_label      = nullptr;\n@@ -517,0 +519,3 @@\n+  \/\/ adjust some internal address during expand\n+  void adjust_internal_address(address from, address to);\n+\n@@ -689,0 +694,3 @@\n+  address last_label() const { return _last_label; }\n+  void set_last_label(address a) { _last_label = a; }\n+\n","filename":"src\/hotspot\/share\/asm\/codeBuffer.hpp","additions":8,"deletions":0,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -31,0 +31,1 @@\n+#include \"asm\/macroAssembler.hpp\"\n@@ -33,0 +34,1 @@\n+#include \"nativeInst_aarch64.hpp\"\n@@ -84,0 +86,371 @@\n+static void asm_dump(address start, address end) {\n+  ResourceMark rm;\n+  stringStream ss;\n+  ss.print_cr(\"Insns:\");\n+  Disassembler::decode(start, end, &ss);\n+  printf(\"%s\\n\", ss.as_string());\n+}\n+\n+void test_merge_dmb() {\n+  BufferBlob* b = BufferBlob::create(\"aarch64Test\", 400);\n+  CodeBuffer code(b);\n+  MacroAssembler _masm(&code);\n+\n+  {\n+    \/\/ merge with same type\n+    __ membar(Assembler::Membar_mask_bits::StoreStore);\n+    __ membar(Assembler::Membar_mask_bits::StoreStore);\n+    __ membar(Assembler::Membar_mask_bits::StoreStore);\n+    __ nop();\n+    __ membar(Assembler::Membar_mask_bits::LoadStore);\n+    __ membar(Assembler::Membar_mask_bits::LoadStore);\n+    __ membar(Assembler::Membar_mask_bits::LoadStore);\n+    __ membar(Assembler::Membar_mask_bits::LoadStore);\n+    __ nop();\n+    \/\/ merge with high rank\n+    __ membar(Assembler::Membar_mask_bits::LoadStore);\n+    __ membar(Assembler::Membar_mask_bits::LoadStore);\n+    __ membar(Assembler::Membar_mask_bits::AnyAny);\n+    __ membar(Assembler::Membar_mask_bits::StoreStore);\n+    __ membar(Assembler::Membar_mask_bits::StoreStore);\n+    __ nop();\n+    \/\/ merge with different type\n+    __ membar(Assembler::Membar_mask_bits::LoadStore);\n+    __ membar(Assembler::Membar_mask_bits::StoreStore);\n+    __ membar(Assembler::Membar_mask_bits::LoadStore);\n+    __ membar(Assembler::Membar_mask_bits::StoreStore);\n+  }\n+  asm_dump(code.insts()->start(), code.insts()->end());\n+  \/\/ AlwaysMergeDMB\n+  static const unsigned int insns1[] = {\n+    0xd5033abf, \/\/ dmb.ishst\n+    0xd503201f, \/\/ nop\n+    0xd50339bf, \/\/ dmb.ishld\n+    0xd503201f, \/\/ nop\n+    0xd5033bbf, \/\/ dmb.ish\n+    0xd503201f, \/\/ nop\n+    0xd5033bbf, \/\/ dmb.ish\n+  };\n+  \/\/ !AlwaysMergeDMB\n+  static const unsigned int insns2[] = {\n+    0xd5033abf, \/\/ dmb.ishst\n+    0xd503201f, \/\/ nop\n+    0xd50339bf, \/\/ dmb.ishld\n+    0xd503201f, \/\/ nop\n+    0xd5033bbf, \/\/ dmb.ish\n+    0xd503201f, \/\/ nop\n+    0xd50339bf, \/\/ dmb.ishld\n+    0xd5033abf, \/\/ dmb.ishst\n+  };\n+  if (AlwaysMergeDMB) {\n+    EXPECT_EQ(code.insts()->size(), (CodeSection::csize_t)(sizeof insns1));\n+    asm_check((const unsigned int *)code.insts()->start(), insns1, sizeof insns1 \/ sizeof insns1[0]);\n+  } else {\n+    EXPECT_EQ(code.insts()->size(), (CodeSection::csize_t)(sizeof insns2));\n+    asm_check((const unsigned int *)code.insts()->start(), insns2, sizeof insns2 \/ sizeof insns2[0]);\n+  }\n+\n+  BufferBlob::free(b);\n+}\n+\n+TEST_VM(AssemblerAArch64, merge_dmb_1) {\n+  FlagSetting fs(AlwaysMergeDMB, true);\n+  test_merge_dmb();\n+}\n+\n+TEST_VM(AssemblerAArch64, merge_dmb_2) {\n+  FlagSetting fs(AlwaysMergeDMB, false);\n+  test_merge_dmb();\n+}\n+\n+TEST_VM(AssemblerAArch64, merge_dmb_block_by_label) {\n+  BufferBlob* b = BufferBlob::create(\"aarch64Test\", 400);\n+  CodeBuffer code(b);\n+  MacroAssembler _masm(&code);\n+\n+  {\n+    Label l;\n+    \/\/ merge can not cross the label\n+    __ membar(Assembler::Membar_mask_bits::StoreStore);\n+    __ bind(l);\n+    __ membar(Assembler::Membar_mask_bits::StoreStore);\n+  }\n+  asm_dump(code.insts()->start(), code.insts()->end());\n+  static const unsigned int insns[] = {\n+    0xd5033abf, \/\/ dmb.ishst\n+    0xd5033abf, \/\/ dmb.ishst\n+  };\n+  EXPECT_EQ(code.insts()->size(), (CodeSection::csize_t)(sizeof insns));\n+  asm_check((const unsigned int *)code.insts()->start(), insns, sizeof insns \/ sizeof insns[0]);\n+\n+  BufferBlob::free(b);\n+}\n+\n+TEST_VM(AssemblerAArch64, merge_dmb_after_expand) {\n+  ResourceMark rm;\n+  BufferBlob* b = BufferBlob::create(\"aarch64Test\", 400);\n+  CodeBuffer code(b);\n+  code.set_blob(b);\n+  MacroAssembler _masm(&code);\n+\n+  {\n+    __ membar(Assembler::Membar_mask_bits::StoreStore);\n+    code.insts()->maybe_expand_to_ensure_remaining(50000);\n+    __ membar(Assembler::Membar_mask_bits::StoreStore);\n+  }\n+  asm_dump(code.insts()->start(), code.insts()->end());\n+  static const unsigned int insns[] = {\n+    0xd5033abf, \/\/ dmb.ishst\n+  };\n+  EXPECT_EQ(code.insts()->size(), (CodeSection::csize_t)(sizeof insns));\n+  asm_check((const unsigned int *)code.insts()->start(), insns, sizeof insns \/ sizeof insns[0]);\n+}\n+\n+constexpr uint32_t test_encode_dmb_ld = 0xd50339bf;\n+constexpr uint32_t test_encode_dmb_st = 0xd5033abf;\n+constexpr uint32_t test_encode_dmb    = 0xd5033bbf;\n+\n+void expect_dmbld(void* addr) {\n+  if (*((uint32_t *) addr) != test_encode_dmb_ld) {\n+    tty->print_cr(\"Expected dmb.ld\");\n+    FAIL();\n+  }\n+}\n+\n+void expect_dmbst(void* addr) {\n+  if (*((uint32_t *) addr) != test_encode_dmb_st) {\n+    tty->print_cr(\"Expected dmb.st\");\n+    FAIL();\n+  }\n+}\n+\n+void expect_dmb(void* addr) {\n+  if (*((uint32_t *) addr) != test_encode_dmb) {\n+    tty->print_cr(\"Expected dmb\");\n+    FAIL();\n+  }\n+}\n+\n+void expect_any_dmb(void* addr) {\n+  uint32_t encode = *((uint32_t *) addr);\n+  if (encode != test_encode_dmb && encode != test_encode_dmb_ld && encode != test_encode_dmb_st) {\n+    tty->print_cr(\"Expected a dmb.* instruction\");\n+    FAIL();\n+  }\n+}\n+\n+void expect_different_dmb_kind(void* addr) {\n+  uint32_t pos1 = *((uint32_t *) addr);\n+  uint32_t pos2 = *(((uint32_t *) addr) + 1);\n+  if (pos1 == pos2) {\n+    tty->print_cr(\"Expected different dmb kind\");\n+    FAIL();\n+  }\n+}\n+\n+void expect_dmb_at_least_one(void* addr) {\n+  uint32_t pos1 = *((uint32_t *) addr);\n+  uint32_t pos2 = *(((uint32_t *) addr) + 1);\n+  if (pos1 != test_encode_dmb && pos2 != test_encode_dmb) {\n+    tty->print_cr(\"Expected at least one dmb\");\n+    FAIL();\n+  }\n+}\n+\n+void expect_dmb_none(void* addr) {\n+  uint32_t pos1 = *((uint32_t *) addr);\n+  uint32_t pos2 = *(((uint32_t *) addr) + 1);\n+  if (pos1 == test_encode_dmb || pos2 == test_encode_dmb) {\n+    tty->print_cr(\"Expected no dmb\");\n+    FAIL();\n+  }\n+}\n+\n+void test_merge_dmb_all_kinds() {\n+  BufferBlob* b = BufferBlob::create(\"aarch64Test\", 20000);\n+  CodeBuffer code(b);\n+  MacroAssembler _masm(&code);\n+\n+  constexpr int count = 5;\n+  struct {\n+    const char* label;\n+    Assembler::Membar_mask_bits flavor;\n+    \/\/ Two groups of two bits describing the ordering, can be OR-ed to figure out composite semantics.\n+    \/\/ First group describes ops before the barrier. Second group describes ops after the barrier.\n+    \/\/ \"01\" means \"load\", \"10\" means \"store\", \"100\" means \"any\".\n+    int mask;\n+  } kind[count] = {\n+          {\"storestore\", Assembler::StoreStore, 0b010010},\n+          {\"loadstore\",  Assembler::LoadStore,  0b001010},\n+          {\"loadload\",   Assembler::LoadLoad,   0b001001},\n+          {\"storeload\",  Assembler::StoreLoad,  0b100100}, \/\/ quirk: StoreLoad is as powerful as AnyAny\n+          {\"anyany\",     Assembler::AnyAny,     0b100100},\n+  };\n+\n+  for (int b1 = 0; b1 < count; b1++) {\n+    for (int b2 = 0; b2 < count; b2++) {\n+      for (int b3 = 0; b3 < count; b3++) {\n+        for (int b4 = 0; b4 < count; b4++) {\n+          tty->print_cr(\"%s + %s + %s + %s\", kind[b1].label, kind[b2].label, kind[b3].label, kind[b4].label);\n+\n+          address start = __ pc();\n+          __ membar(kind[b1].flavor);\n+          __ membar(kind[b2].flavor);\n+          __ membar(kind[b3].flavor);\n+          __ membar(kind[b4].flavor);\n+          address end = __ pc();\n+          __ nop();\n+\n+          size_t size = pointer_delta(end, start, 1);\n+          if (AlwaysMergeDMB) {\n+            \/\/ Expect only a single barrier.\n+            EXPECT_EQ(size, (size_t) NativeMembar::instruction_size);\n+          } else {\n+            EXPECT_LE(size, (size_t) NativeMembar::instruction_size * 2);\n+          }\n+\n+          \/\/ Composite ordering for this group of barriers.\n+          int composite_mask = kind[b1].mask | kind[b2].mask | kind[b3].mask | kind[b4].mask;\n+\n+          if (size == NativeMembar::instruction_size) {\n+            \/\/ If there is a single barrier, we can easily test its type.\n+            switch (composite_mask) {\n+              case 0b001001:\n+              case 0b001010:\n+              case 0b001011:\n+              case 0b001101:\n+              case 0b001110:\n+              case 0b001111:\n+                \/\/ Any combination of Load(Load|Store|Any) gets dmb.ld\n+                expect_dmbld(start);\n+                break;\n+              case 0b010010:\n+                \/\/ Only StoreStore gets dmb.st\n+                expect_dmbst(start);\n+                break;\n+              default:\n+                \/\/ Everything else gets folded into full dmb\n+                expect_dmb(start);\n+                break;\n+            }\n+          } else if (size == 2 * NativeMembar::instruction_size) {\n+            \/\/ There are two barriers. Make a few sanity checks.\n+            \/\/ They must be different kind\n+            expect_any_dmb(start);\n+            expect_any_dmb(start + NativeMembar::instruction_size);\n+            expect_different_dmb_kind(start);\n+            if ((composite_mask & 0b100100) != 0) {\n+              \/\/ There was \"any\" barrier in the group, a full dmb is expected\n+              expect_dmb_at_least_one(start);\n+            } else {\n+              \/\/ Otherwise expect no full dmb\n+              expect_dmb_none(start);\n+            }\n+          } else {\n+            \/\/ Merging code does not produce this result.\n+            FAIL();\n+          }\n+        }\n+      }\n+    }\n+  }\n+\n+  BufferBlob::free(b);\n+}\n+\n+TEST_VM(AssemblerAArch64, merge_dmb_all_kinds_1) {\n+  FlagSetting fs(AlwaysMergeDMB, true);\n+  test_merge_dmb_all_kinds();\n+}\n+\n+TEST_VM(AssemblerAArch64, merge_dmb_all_kinds_2) {\n+  FlagSetting fs(AlwaysMergeDMB, false);\n+  test_merge_dmb_all_kinds();\n+}\n+\n+TEST_VM(AssemblerAArch64, merge_ldst) {\n+  BufferBlob* b = BufferBlob::create(\"aarch64Test\", 400);\n+  CodeBuffer code(b);\n+  MacroAssembler _masm(&code);\n+\n+  {\n+    Label l;\n+    \/\/ merge ld\/st into ldp\/stp\n+    __ ldr(r0, Address(sp, 8));\n+    __ ldr(r1, Address(sp, 0));\n+    __ nop();\n+    __ str(r0, Address(sp, 0));\n+    __ str(r1, Address(sp, 8));\n+    __ nop();\n+    __ ldrw(r0, Address(sp, 0));\n+    __ ldrw(r1, Address(sp, 4));\n+    __ nop();\n+    __ strw(r0, Address(sp, 4));\n+    __ strw(r1, Address(sp, 0));\n+    __ nop();\n+    \/\/ can not merge\n+    __ ldrw(r0, Address(sp, 4));\n+    __ ldr(r1, Address(sp, 8));\n+    __ nop();\n+    __ ldrw(r0, Address(sp, 0));\n+    __ ldrw(r1, Address(sp, 8));\n+    __ nop();\n+    __ str(r0, Address(sp, 0));\n+    __ bind(l);                     \/\/ block by label\n+    __ str(r1, Address(sp, 8));\n+    __ nop();\n+  }\n+  asm_dump(code.insts()->start(), code.insts()->end());\n+  static const unsigned int insns1[] = {\n+    0xa94003e1, \/\/ ldp x1, x0, [sp]\n+    0xd503201f, \/\/ nop\n+    0xa90007e0, \/\/ stp x0, x1, [sp]\n+    0xd503201f, \/\/ nop\n+    0x294007e0, \/\/ ldp w0, w1, [sp]\n+    0xd503201f, \/\/ nop\n+    0x290003e1, \/\/ stp w1, w0, [sp]\n+    0xd503201f, \/\/ nop\n+    0xb94007e0, \/\/ ldr w0, [sp, 4]\n+    0xf94007e1, \/\/ ldr x1, [sp, 8]\n+    0xd503201f, \/\/ nop\n+    0xb94003e0, \/\/ ldr w0, [sp]\n+    0xb9400be1, \/\/ ldr w1, [sp, 8]\n+    0xd503201f, \/\/ nop\n+    0xf90003e0, \/\/ str x0, [sp]\n+    0xf90007e1, \/\/ str x1, [sp, 8]\n+    0xd503201f, \/\/ nop\n+  };\n+  EXPECT_EQ(code.insts()->size(), (CodeSection::csize_t)(sizeof insns1));\n+  asm_check((const unsigned int *)code.insts()->start(), insns1, sizeof insns1 \/ sizeof insns1[0]);\n+\n+  BufferBlob::free(b);\n+}\n+\n+TEST_VM(AssemblerAArch64, merge_ldst_after_expand) {\n+  ResourceMark rm;\n+  BufferBlob* b = BufferBlob::create(\"aarch64Test\", 400);\n+  CodeBuffer code(b);\n+  code.set_blob(b);\n+  MacroAssembler _masm(&code);\n+\n+  {\n+    __ ldr(r0, Address(sp, 8));\n+    code.insts()->maybe_expand_to_ensure_remaining(10000);\n+    __ ldr(r1, Address(sp, 0));\n+    __ nop();\n+    __ str(r0, Address(sp, 0));\n+    code.insts()->maybe_expand_to_ensure_remaining(100000);\n+    __ str(r1, Address(sp, 8));\n+    __ nop();\n+  }\n+  asm_dump(code.insts()->start(), code.insts()->end());\n+  static const unsigned int insns[] = {\n+    0xa94003e1, \/\/ ldp x1, x0, [sp]\n+    0xd503201f, \/\/ nop\n+    0xa90007e0, \/\/ stp x0, x1, [sp]\n+    0xd503201f, \/\/ nop\n+  };\n+  EXPECT_EQ(code.insts()->size(), (CodeSection::csize_t)(sizeof insns));\n+  asm_check((const unsigned int *)code.insts()->start(), insns, sizeof insns \/ sizeof insns[0]);\n+}\n+\n","filename":"test\/hotspot\/gtest\/aarch64\/test_assembler_aarch64.cpp","additions":374,"deletions":1,"binary":false,"changes":375,"status":"modified"},{"patch":"@@ -0,0 +1,85 @@\n+\/*\n+ * Copyright (c) 2024, Alibaba Group Co., Ltd. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+package org.openjdk.bench.vm.compiler;\n+\n+import org.openjdk.jmh.annotations.Benchmark;\n+import org.openjdk.jmh.annotations.*;\n+\n+import java.util.concurrent.TimeUnit;\n+import org.openjdk.jmh.infra.Blackhole;\n+\n+\/* test allocation speed of object with final field *\/\n+@BenchmarkMode(Mode.Throughput)\n+@OutputTimeUnit(TimeUnit.SECONDS)\n+@State(Scope.Benchmark)\n+@Warmup(iterations = 5, time = 3, timeUnit = TimeUnit.SECONDS)\n+@Measurement(iterations = 3, time = 3, timeUnit = TimeUnit.SECONDS)\n+@Fork(value = 3)\n+public class FinalFieldInitialize {\n+  final static int LEN = 100_000;\n+  Object arr[] = null;\n+  @Setup\n+  public void setup(){\n+    arr = new Object[LEN];\n+  }\n+\n+  @Benchmark\n+  public void testAlloc(Blackhole bh) {\n+    for (int i=0; i<LEN; i++) {\n+      arr[i] = new TObj();\n+    }\n+    bh.consume(arr);\n+  }\n+\n+  @Benchmark\n+  public void testAllocWithFinal(Blackhole bh) {\n+    for (int i=0; i<LEN; i++) {\n+      arr[i] = new TObjWithFinal();\n+    }\n+    bh.consume(arr);\n+  }\n+}\n+\n+class TObj {\n+  private int i;\n+  private long l;\n+  private boolean b;\n+\n+  public TObj() {\n+    i = 10;\n+    l = 100L;\n+    b = true;\n+  }\n+}\n+\n+class TObjWithFinal {\n+  private int i;\n+  private long l;\n+  private final boolean b;\n+\n+  public TObjWithFinal() {\n+    i = 10;\n+    l = 100L;\n+    b = true;\n+  }\n+}\n","filename":"test\/micro\/org\/openjdk\/bench\/vm\/compiler\/FinalFieldInitialize.java","additions":85,"deletions":0,"binary":false,"changes":85,"status":"added"}]}