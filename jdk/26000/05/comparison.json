{"files":[{"patch":"@@ -4,0 +4,1 @@\n+ * Copyright 2025 Arm Limited and\/or its affiliates.\n@@ -1479,1 +1480,1 @@\n-  __ cmpxchg(addr, cmpval, newval, Assembler::word, \/* acquire*\/ true, \/* release*\/ true, \/* weak*\/ false, rscratch1);\n+  __ cmpxchg_barrier(addr, cmpval, newval, Assembler::word, \/* acquire*\/ true, \/* release*\/ true, \/* weak*\/ false, rscratch1);\n@@ -1481,1 +1482,0 @@\n-  __ membar(__ AnyAny);\n@@ -1485,1 +1485,1 @@\n-  __ cmpxchg(addr, cmpval, newval, Assembler::xword, \/* acquire*\/ true, \/* release*\/ true, \/* weak*\/ false, rscratch1);\n+  __ cmpxchg_barrier(addr, cmpval, newval, Assembler::xword, \/* acquire*\/ true, \/* release*\/ true, \/* weak*\/ false, rscratch1);\n@@ -1487,1 +1487,0 @@\n-  __ membar(__ AnyAny);\n@@ -3056,2 +3055,2 @@\n-    xchg = &MacroAssembler::atomic_xchgalw;\n-    add = &MacroAssembler::atomic_addalw;\n+    xchg = &MacroAssembler::atomic_xchgalw_barrier;\n+    add = &MacroAssembler::atomic_addalw_barrier;\n@@ -3060,2 +3059,2 @@\n-    xchg = &MacroAssembler::atomic_xchgal;\n-    add = &MacroAssembler::atomic_addal;\n+    xchg = &MacroAssembler::atomic_xchgal_barrier;\n+    add = &MacroAssembler::atomic_addal_barrier;\n@@ -3066,2 +3065,2 @@\n-      xchg = &MacroAssembler::atomic_xchgalw;\n-      add = &MacroAssembler::atomic_addalw;\n+      xchg = &MacroAssembler::atomic_xchgalw_barrier;\n+      add = &MacroAssembler::atomic_addalw_barrier;\n@@ -3069,2 +3068,2 @@\n-      xchg = &MacroAssembler::atomic_xchgal;\n-      add = &MacroAssembler::atomic_addal;\n+      xchg = &MacroAssembler::atomic_xchgal_barrier;\n+      add = &MacroAssembler::atomic_addal_barrier;\n@@ -3075,2 +3074,2 @@\n-    xchg = &MacroAssembler::atomic_xchgal;\n-    add = &MacroAssembler::atomic_addal; \/\/ unreachable\n+    xchg = &MacroAssembler::atomic_xchgal_barrier;\n+    add = &MacroAssembler::atomic_addal_barrier; \/\/ unreachable\n@@ -3119,3 +3118,0 @@\n-  if(!UseLSE) {\n-    __ membar(__ AnyAny);\n-  }\n","filename":"src\/hotspot\/cpu\/aarch64\/c1_LIRAssembler_aarch64.cpp","additions":13,"deletions":17,"binary":false,"changes":30,"status":"modified"},{"patch":"@@ -4,0 +4,1 @@\n+ * Copyright 2025 Arm Limited and\/or its affiliates.\n@@ -3420,0 +3421,4 @@\n+\/\/\n+\/\/ When `with_barrier` is set:\n+\/\/   Provides acquire barrier semantics, and, when CAS is successful,\n+\/\/   both acquire and release barrier semantics.\n@@ -3427,1 +3432,1 @@\n-                             Register result) {\n+                             Register result, bool with_barrier) {\n@@ -3452,0 +3457,4 @@\n+    if (with_barrier) {\n+      \/\/ Prevent a later volatile load from being reordered with the STLXR.\n+      membar(AnyAny);\n+    }\n@@ -3456,0 +3465,20 @@\n+void MacroAssembler::cmpxchg(Register addr, Register expected,\n+                             Register new_val,\n+                             enum operand_size size,\n+                             bool acquire, bool release,\n+                             bool weak,\n+                             Register result) {\n+  cmpxchg(addr, expected, new_val, size, acquire, release, weak, result, \/*with_barrier*\/ false);\n+}\n+\n+\/\/ Provides acquire barrier semantics, and, when CAS is successful,\n+\/\/ both acquire and release barrier (trailing membar) semantics.\n+void MacroAssembler::cmpxchg_barrier(Register addr, Register expected,\n+                             Register new_val,\n+                             enum operand_size size,\n+                             bool acquire, bool release,\n+                             bool weak,\n+                             Register result) {\n+  cmpxchg(addr, expected, new_val, size, acquire, release, weak, result, \/*with_barrier*\/ true);\n+}\n+\n@@ -3481,1 +3510,1 @@\n-#define ATOMIC_OP(NAME, LDXR, OP, IOP, AOP, STXR, sz)                   \\\n+#define ATOMIC_OP(NAME, LDXR, OP, IOP, AOP, STXR, sz, with_barrier)     \\\n@@ -3507,0 +3536,3 @@\n+  if (with_barrier) {                                                   \\\n+    membar(AnyAny);                                                     \\\n+  }                                                                     \\\n@@ -3509,4 +3541,8 @@\n-ATOMIC_OP(add, ldxr, add, sub, ldadd, stxr, Assembler::xword)\n-ATOMIC_OP(addw, ldxrw, addw, subw, ldadd, stxrw, Assembler::word)\n-ATOMIC_OP(addal, ldaxr, add, sub, ldaddal, stlxr, Assembler::xword)\n-ATOMIC_OP(addalw, ldaxrw, addw, subw, ldaddal, stlxrw, Assembler::word)\n+ATOMIC_OP(add,    ldxr,   add,  sub,  ldadd,   stxr,   Assembler::xword, \/*with_barrier*\/ false)\n+ATOMIC_OP(addw,   ldxrw,  addw, subw, ldadd,   stxrw,  Assembler::word,  \/*with_barrier*\/ false)\n+ATOMIC_OP(addal,  ldaxr,  add,  sub,  ldaddal, stlxr,  Assembler::xword, \/*with_barrier*\/ false)\n+ATOMIC_OP(addalw, ldaxrw, addw, subw, ldaddal, stlxrw, Assembler::word,  \/*with_barrier*\/ false)\n+\n+\/\/ These versions provide trailing membar semantics.\n+ATOMIC_OP(addal_barrier,  ldaxr,  add,  sub,  ldaddal, stlxr,  Assembler::xword, \/*with_barrier*\/ true)\n+ATOMIC_OP(addalw_barrier, ldaxrw, addw, subw, ldaddal, stlxrw, Assembler::word,  \/*with_barrier*\/ true)\n@@ -3516,1 +3552,1 @@\n-#define ATOMIC_XCHG(OP, AOP, LDXR, STXR, sz)                            \\\n+#define ATOMIC_XCHG(OP, AOP, LDXR, STXR, sz, with_barrier)              \\\n@@ -3535,0 +3571,3 @@\n+  if (with_barrier) {                                                   \\\n+    membar(AnyAny);                                                     \\\n+  }                                                                     \\\n@@ -3537,6 +3576,10 @@\n-ATOMIC_XCHG(xchg, swp, ldxr, stxr, Assembler::xword)\n-ATOMIC_XCHG(xchgw, swp, ldxrw, stxrw, Assembler::word)\n-ATOMIC_XCHG(xchgl, swpl, ldxr, stlxr, Assembler::xword)\n-ATOMIC_XCHG(xchglw, swpl, ldxrw, stlxrw, Assembler::word)\n-ATOMIC_XCHG(xchgal, swpal, ldaxr, stlxr, Assembler::xword)\n-ATOMIC_XCHG(xchgalw, swpal, ldaxrw, stlxrw, Assembler::word)\n+ATOMIC_XCHG(xchg,    swp,   ldxr,   stxr,   Assembler::xword, \/*with_barrier*\/ false)\n+ATOMIC_XCHG(xchgw,   swp,   ldxrw,  stxrw,  Assembler::word,  \/*with_barrier*\/ false)\n+ATOMIC_XCHG(xchgl,   swpl,  ldxr,   stlxr,  Assembler::xword, \/*with_barrier*\/ false)\n+ATOMIC_XCHG(xchglw,  swpl,  ldxrw,  stlxrw, Assembler::word,  \/*with_barrier*\/ false)\n+ATOMIC_XCHG(xchgal,  swpal, ldaxr,  stlxr,  Assembler::xword, \/*with_barrier*\/ false)\n+ATOMIC_XCHG(xchgalw, swpal, ldaxrw, stlxrw, Assembler::word,  \/*with_barrier*\/ false)\n+\n+\/\/ These versions provide trailing membar semantics.\n+ATOMIC_XCHG(xchgal_barrier,  swpal, ldaxr,  stlxr, Assembler::xword, \/*with_barrier*\/ true)\n+ATOMIC_XCHG(xchgalw_barrier, swpal, ldaxrw, stlxrw, Assembler::word, \/*with_barrier*\/ true)\n","filename":"src\/hotspot\/cpu\/aarch64\/macroAssembler_aarch64.cpp","additions":56,"deletions":13,"binary":false,"changes":69,"status":"modified"},{"patch":"@@ -4,0 +4,1 @@\n+ * Copyright 2025 Arm Limited and\/or its affiliates.\n@@ -1224,0 +1225,6 @@\n+  void atomic_addal_barrier(Register prev, RegisterOrConstant incr, Register addr);\n+  void atomic_addalw_barrier(Register prev, RegisterOrConstant incr, Register addr);\n+\n+  void atomic_xchgal_barrier(Register prev, Register newv, Register addr);\n+  void atomic_xchgalw_barrier(Register prev, Register newv, Register addr);\n+\n@@ -1235,0 +1242,5 @@\n+  void cmpxchg(Register addr, Register expected, Register new_val,\n+               enum operand_size size,\n+               bool acquire, bool release, bool weak,\n+               Register result, bool with_barrier);\n+\n@@ -1239,0 +1251,4 @@\n+  void cmpxchg_barrier(Register addr, Register expected,\n+                       Register new_val, enum operand_size size,\n+                       bool acquire, bool release, bool weak,\n+                       Register result);\n","filename":"src\/hotspot\/cpu\/aarch64\/macroAssembler_aarch64.hpp","additions":16,"deletions":0,"binary":false,"changes":16,"status":"modified"}]}