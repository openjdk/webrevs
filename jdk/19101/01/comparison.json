{"files":[{"patch":"@@ -40,3 +40,0 @@\n-  \/\/ The bits will be divided evenly between two bitmaps; each of them should be\n-  \/\/ an integral number of words.\n-  assert(is_aligned(bits, (BitsPerWord * 2)), \"region size unaligned\");\n@@ -64,2 +61,1 @@\n-    _beg_bits = BitMapView(map,             bits \/ 2);\n-    _end_bits = BitMapView(map + words \/ 2, bits \/ 2);\n+    _beg_bits = BitMapView(map, bits);\n@@ -80,170 +76,0 @@\n-bool\n-ParMarkBitMap::mark_obj(HeapWord* addr, size_t size)\n-{\n-  const idx_t beg_bit = addr_to_bit(addr);\n-  if (_beg_bits.par_set_bit(beg_bit)) {\n-    const idx_t end_bit = addr_to_bit(addr + size - 1);\n-    bool end_bit_ok = _end_bits.par_set_bit(end_bit);\n-    assert(end_bit_ok, \"concurrency problem\");\n-    return true;\n-  }\n-  return false;\n-}\n-\n-inline bool\n-ParMarkBitMap::is_live_words_in_range_in_cache(ParCompactionManager* cm, HeapWord* beg_addr) const {\n-  return cm->last_query_begin() == beg_addr;\n-}\n-\n-inline void\n-ParMarkBitMap::update_live_words_in_range_cache(ParCompactionManager* cm, HeapWord* beg_addr, oop end_obj, size_t result) const {\n-  cm->set_last_query_begin(beg_addr);\n-  cm->set_last_query_object(end_obj);\n-  cm->set_last_query_return(result);\n-}\n-\n-size_t\n-ParMarkBitMap::live_words_in_range_helper(HeapWord* beg_addr, oop end_obj) const\n-{\n-  assert(beg_addr <= cast_from_oop<HeapWord*>(end_obj), \"bad range\");\n-  assert(is_marked(end_obj), \"end_obj must be live\");\n-\n-  idx_t live_bits = 0;\n-\n-  \/\/ The bitmap routines require the right boundary to be word-aligned.\n-  const idx_t end_bit = addr_to_bit(cast_from_oop<HeapWord*>(end_obj));\n-  const idx_t range_end = align_range_end(end_bit);\n-\n-  idx_t beg_bit = find_obj_beg(addr_to_bit(beg_addr), range_end);\n-  while (beg_bit < end_bit) {\n-    idx_t tmp_end = find_obj_end(beg_bit, range_end);\n-    assert(tmp_end < end_bit, \"missing end bit\");\n-    live_bits += tmp_end - beg_bit + 1;\n-    beg_bit = find_obj_beg(tmp_end + 1, range_end);\n-  }\n-  return bits_to_words(live_bits);\n-}\n-\n-size_t\n-ParMarkBitMap::live_words_in_range_use_cache(ParCompactionManager* cm, HeapWord* beg_addr, oop end_oop) const\n-{\n-  HeapWord* last_beg = cm->last_query_begin();\n-  HeapWord* last_obj = cast_from_oop<HeapWord*>(cm->last_query_object());\n-  HeapWord* end_obj  = cast_from_oop<HeapWord*>(end_oop);\n-\n-  size_t last_ret = cm->last_query_return();\n-  if (end_obj > last_obj) {\n-    last_ret = last_ret + live_words_in_range_helper(last_obj, end_oop);\n-    last_obj = end_obj;\n-  } else if (end_obj < last_obj) {\n-    \/\/ The cached value is for an object that is to the left (lower address) of the current\n-    \/\/ end_obj. Calculate back from that cached value.\n-    if (pointer_delta(end_obj, beg_addr) > pointer_delta(last_obj, end_obj)) {\n-      last_ret = last_ret - live_words_in_range_helper(end_obj, cast_to_oop(last_obj));\n-    } else {\n-      last_ret = live_words_in_range_helper(beg_addr, end_oop);\n-    }\n-    last_obj = end_obj;\n-  }\n-\n-  update_live_words_in_range_cache(cm, last_beg, cast_to_oop(last_obj), last_ret);\n-  return last_ret;\n-}\n-\n-size_t\n-ParMarkBitMap::live_words_in_range(ParCompactionManager* cm, HeapWord* beg_addr, oop end_obj) const\n-{\n-  \/\/ Try to reuse result from ParCompactionManager cache first.\n-  if (is_live_words_in_range_in_cache(cm, beg_addr)) {\n-    return live_words_in_range_use_cache(cm, beg_addr, end_obj);\n-  }\n-  size_t ret = live_words_in_range_helper(beg_addr, end_obj);\n-  update_live_words_in_range_cache(cm, beg_addr, end_obj, ret);\n-  return ret;\n-}\n-\n-ParMarkBitMap::IterationStatus\n-ParMarkBitMap::iterate(ParMarkBitMapClosure* live_closure,\n-                       idx_t range_beg, idx_t range_end) const\n-{\n-  DEBUG_ONLY(verify_bit(range_beg);)\n-  DEBUG_ONLY(verify_bit(range_end);)\n-  assert(range_beg <= range_end, \"live range invalid\");\n-\n-  \/\/ The bitmap routines require the right boundary to be word-aligned.\n-  const idx_t search_end = align_range_end(range_end);\n-\n-  idx_t cur_beg = range_beg;\n-  while (true) {\n-    cur_beg = find_obj_beg(cur_beg, search_end);\n-    if (cur_beg >= range_end) {\n-      break;\n-    }\n-\n-    const size_t size = obj_size(cur_beg);\n-    IterationStatus status = live_closure->do_addr(bit_to_addr(cur_beg), size);\n-    if (status != incomplete) {\n-      assert(status == would_overflow || status == full, \"sanity\");\n-      return status;\n-    }\n-\n-    cur_beg += words_to_bits(size);\n-    if (cur_beg >= range_end) {\n-      break;\n-    }\n-  }\n-\n-  return complete;\n-}\n-\n-ParMarkBitMap::IterationStatus\n-ParMarkBitMap::iterate(ParMarkBitMapClosure* live_closure,\n-                       ParMarkBitMapClosure* dead_closure,\n-                       idx_t range_beg, idx_t range_end,\n-                       idx_t dead_range_end) const\n-{\n-  DEBUG_ONLY(verify_bit(range_beg);)\n-  DEBUG_ONLY(verify_bit(range_end);)\n-  DEBUG_ONLY(verify_bit(dead_range_end);)\n-  assert(range_beg <= range_end, \"live range invalid\");\n-  assert(range_end <= dead_range_end, \"dead range invalid\");\n-\n-  \/\/ The bitmap routines require the right boundary to be word-aligned.\n-  const idx_t dead_search_end = align_range_end(dead_range_end);\n-\n-  idx_t cur_beg = range_beg;\n-  if (range_beg < range_end && is_unmarked(range_beg)) {\n-    \/\/ The range starts with dead space.  Look for the next object, then fill.\n-    \/\/ This must be the beginning of old\/eden\/from\/to-space, so it's must be\n-    \/\/ large enough for a filler.\n-    cur_beg = find_obj_beg(range_beg + 1, dead_search_end);\n-    const idx_t dead_space_end = cur_beg - 1;\n-    const size_t size = obj_size(range_beg, dead_space_end);\n-    dead_closure->do_addr(bit_to_addr(range_beg), size);\n-  }\n-\n-  while (cur_beg < range_end) {\n-    const size_t size = obj_size(cur_beg);\n-    IterationStatus status = live_closure->do_addr(bit_to_addr(cur_beg), size);\n-    if (status != incomplete) {\n-      assert(status == would_overflow || status == full, \"sanity\");\n-      return status;\n-    }\n-\n-    const idx_t dead_space_beg = cur_beg + words_to_bits(size);\n-    if (dead_space_beg >= dead_search_end) {\n-      break;\n-    }\n-    \/\/ Look for the start of the next object.\n-    cur_beg = find_obj_beg(dead_space_beg, dead_search_end);\n-    if (cur_beg > dead_space_beg) {\n-      \/\/ Found dead space; compute the size and invoke the dead closure.\n-      const idx_t dead_space_end = cur_beg - 1;\n-      dead_closure->do_addr(bit_to_addr(dead_space_beg),\n-                            obj_size(dead_space_beg, dead_space_end));\n-    }\n-  }\n-\n-  return complete;\n-}\n-\n","filename":"src\/hotspot\/share\/gc\/parallel\/parMarkBitMap.cpp","additions":1,"deletions":175,"binary":false,"changes":176,"status":"modified"},{"patch":"@@ -42,1 +42,1 @@\n-  enum IterationStatus { incomplete, complete, full, would_overflow };\n+  enum IterationStatus { incomplete, complete, full };\n@@ -48,6 +48,2 @@\n-  bool mark_obj(HeapWord* addr, size_t size);\n-  inline bool mark_obj(oop obj, size_t size);\n-\n-  \/\/ Return whether the specified begin or end bit is set.\n-  inline bool is_obj_beg(idx_t bit) const;\n-  inline bool is_obj_end(idx_t bit) const;\n+  inline bool mark_obj(HeapWord* addr);\n+  inline bool mark_obj(oop obj);\n@@ -71,55 +67,0 @@\n-  \/\/ Return the size in words of an object given a begin bit and an end bit, or\n-  \/\/ the equivalent beg_addr and end_addr.\n-  inline size_t obj_size(idx_t beg_bit, idx_t end_bit) const;\n-  inline size_t obj_size(HeapWord* beg_addr, HeapWord* end_addr) const;\n-\n-  \/\/ Return the size in words of the object (a search is done for the end bit).\n-  inline size_t obj_size(idx_t beg_bit)  const;\n-  inline size_t obj_size(HeapWord* addr) const;\n-\n-  \/\/ Apply live_closure to each live object that lies completely within the\n-  \/\/ range [live_range_beg, live_range_end).  This is used to iterate over the\n-  \/\/ compacted region of the heap.  Return values:\n-  \/\/\n-  \/\/ complete           The iteration is complete.  All objects in the range\n-  \/\/                    were processed and the closure is not full;\n-  \/\/                    closure->source() is set one past the end of the range.\n-  \/\/\n-  \/\/ full               The closure is full; closure->source() is set to one\n-  \/\/                    past the end of the last object processed.\n-  \/\/\n-  \/\/ would_overflow     The next object in the range would overflow the closure;\n-  \/\/                    closure->source() is set to the start of that object.\n-  IterationStatus iterate(ParMarkBitMapClosure* live_closure,\n-                          idx_t range_beg, idx_t range_end) const;\n-  inline IterationStatus iterate(ParMarkBitMapClosure* live_closure,\n-                                 HeapWord* range_beg,\n-                                 HeapWord* range_end) const;\n-\n-  \/\/ Apply live closure as above and additionally apply dead_closure to all dead\n-  \/\/ space in the range [range_beg, dead_range_end).  Note that dead_range_end\n-  \/\/ must be >= range_end.  This is used to iterate over the dense prefix.\n-  \/\/\n-  \/\/ This method assumes that if the first bit in the range (range_beg) is not\n-  \/\/ marked, then dead space begins at that point and the dead_closure is\n-  \/\/ applied.  Thus callers must ensure that range_beg is not in the middle of a\n-  \/\/ live object.\n-  IterationStatus iterate(ParMarkBitMapClosure* live_closure,\n-                          ParMarkBitMapClosure* dead_closure,\n-                          idx_t range_beg, idx_t range_end,\n-                          idx_t dead_range_end) const;\n-  inline IterationStatus iterate(ParMarkBitMapClosure* live_closure,\n-                                 ParMarkBitMapClosure* dead_closure,\n-                                 HeapWord* range_beg,\n-                                 HeapWord* range_end,\n-                                 HeapWord* dead_range_end) const;\n-\n-  \/\/ Return the number of live words in the range [beg_addr, end_obj) due to\n-  \/\/ objects that start in the range.  If a live object extends onto the range,\n-  \/\/ the caller must detect and account for any live words due to that object.\n-  \/\/ If a live object extends beyond the end of the range, only the words within\n-  \/\/ the range are included in the result. The end of the range must be a live object,\n-  \/\/ which is the case when updating pointers.  This allows a branch to be removed\n-  \/\/ from inside the loop.\n-  size_t live_words_in_range(ParCompactionManager* cm, HeapWord* beg_addr, oop end_obj) const;\n-\n@@ -144,1 +85,0 @@\n-  inline idx_t find_obj_end(idx_t beg, idx_t end) const;\n@@ -147,1 +87,0 @@\n-  inline HeapWord* find_obj_end(HeapWord* beg, HeapWord* end) const;\n@@ -149,0 +88,3 @@\n+  \/\/ Return the address of the last obj-start in the range [beg, end).  If no\n+  \/\/ object is found, return end.\n+  inline HeapWord* find_obj_beg_reverse(HeapWord* beg, HeapWord* end) const;\n@@ -161,1 +103,0 @@\n-    _end_bits.print_on_error(st, \" End Bits:   \");\n@@ -171,5 +112,0 @@\n-  size_t live_words_in_range_helper(HeapWord* beg_addr, oop end_obj) const;\n-\n-  bool is_live_words_in_range_in_cache(ParCompactionManager* cm, HeapWord* beg_addr) const;\n-  size_t live_words_in_range_use_cache(ParCompactionManager* cm, HeapWord* beg_addr, oop end_obj) const;\n-  void update_live_words_in_range_cache(ParCompactionManager* cm, HeapWord* beg_addr, oop end_obj, size_t result) const;\n@@ -186,1 +122,0 @@\n-  BitMapView      _end_bits;\n","filename":"src\/hotspot\/share\/gc\/parallel\/parMarkBitMap.hpp","additions":6,"deletions":71,"binary":false,"changes":77,"status":"modified"},{"patch":"@@ -34,1 +34,1 @@\n-  _region_start(nullptr), _region_size(0), _beg_bits(), _end_bits(), _virtual_space(nullptr), _reserved_byte_size(0)\n+  _region_start(nullptr), _region_size(0), _beg_bits(), _virtual_space(nullptr), _reserved_byte_size(0)\n@@ -39,1 +39,0 @@\n-  _end_bits.clear_range(beg, end);\n@@ -43,3 +42,1 @@\n-  \/\/ Need two bits (one begin bit, one end bit) for each unit of 'object\n-  \/\/ granularity' in the heap.\n-  return words_to_bits(words * 2);\n+  return words_to_bits(words);\n@@ -68,8 +65,0 @@\n-inline bool ParMarkBitMap::is_obj_beg(idx_t bit) const {\n-  return _beg_bits.at(bit);\n-}\n-\n-inline bool ParMarkBitMap::is_obj_end(idx_t bit) const {\n-  return _end_bits.at(bit);\n-}\n-\n@@ -77,1 +66,1 @@\n-  return is_obj_beg(bit);\n+  return _beg_bits.at(bit);\n@@ -108,17 +97,2 @@\n-inline size_t ParMarkBitMap::obj_size(idx_t beg_bit, idx_t end_bit) const {\n-  DEBUG_ONLY(verify_bit(beg_bit);)\n-  DEBUG_ONLY(verify_bit(end_bit);)\n-  return bits_to_words(end_bit - beg_bit + 1);\n-}\n-\n-inline size_t ParMarkBitMap::obj_size(HeapWord* beg_addr, HeapWord* end_addr) const {\n-  DEBUG_ONLY(verify_addr(beg_addr);)\n-  DEBUG_ONLY(verify_addr(end_addr);)\n-  return pointer_delta(end_addr, beg_addr) + obj_granularity();\n-}\n-\n-inline size_t ParMarkBitMap::obj_size(idx_t beg_bit) const {\n-  const idx_t end_bit = _end_bits.find_first_set_bit(beg_bit, size());\n-  assert(is_marked(beg_bit), \"obj not marked\");\n-  assert(end_bit < size(), \"end bit missing\");\n-  return obj_size(beg_bit, end_bit);\n+inline bool ParMarkBitMap::mark_obj(HeapWord* addr) {\n+  return _beg_bits.par_set_bit(addr_to_bit(addr));\n@@ -127,22 +101,2 @@\n-inline size_t ParMarkBitMap::obj_size(HeapWord* addr) const {\n-  return obj_size(addr_to_bit(addr));\n-}\n-\n-inline ParMarkBitMap::IterationStatus ParMarkBitMap::iterate(ParMarkBitMapClosure* live_closure,\n-                                                             HeapWord* range_beg,\n-                                                             HeapWord* range_end) const {\n-  return iterate(live_closure, addr_to_bit(range_beg), addr_to_bit(range_end));\n-}\n-\n-inline ParMarkBitMap::IterationStatus ParMarkBitMap::iterate(ParMarkBitMapClosure* live_closure,\n-                                                             ParMarkBitMapClosure* dead_closure,\n-                                                             HeapWord* range_beg,\n-                                                             HeapWord* range_end,\n-                                                             HeapWord* dead_range_end) const {\n-  return iterate(live_closure, dead_closure,\n-                 addr_to_bit(range_beg), addr_to_bit(range_end),\n-                 addr_to_bit(dead_range_end));\n-}\n-\n-inline bool ParMarkBitMap::mark_obj(oop obj, size_t size) {\n-  return mark_obj(cast_from_oop<HeapWord*>(obj), size);\n+inline bool ParMarkBitMap::mark_obj(oop obj) {\n+  return mark_obj(cast_from_oop<HeapWord*>(obj));\n@@ -171,4 +125,0 @@\n-inline ParMarkBitMap::idx_t ParMarkBitMap::find_obj_end(idx_t beg, idx_t end) const {\n-  return _end_bits.find_first_set_bit_aligned_right(beg, end);\n-}\n-\n@@ -183,1 +133,1 @@\n-inline HeapWord* ParMarkBitMap::find_obj_end(HeapWord* beg, HeapWord* end) const {\n+inline HeapWord* ParMarkBitMap::find_obj_beg_reverse(HeapWord* beg, HeapWord* end) const {\n@@ -186,2 +136,1 @@\n-  const idx_t search_end = align_range_end(end_bit);\n-  const idx_t res_bit = MIN2(find_obj_end(beg_bit, search_end), end_bit);\n+  const idx_t res_bit = _beg_bits.find_last_set_bit_aligned_left(beg_bit, end_bit);\n","filename":"src\/hotspot\/share\/gc\/parallel\/parMarkBitMap.inline.hpp","additions":9,"deletions":60,"binary":false,"changes":69,"status":"modified"},{"patch":"@@ -32,0 +32,1 @@\n+#include \"gc\/shared\/preservedMarks.inline.hpp\"\n@@ -54,1 +55,3 @@\n-ParCompactionManager::ParCompactionManager() {\n+PreservedMarksSet* ParCompactionManager::_preserved_marks_set = nullptr;\n+\n+ParCompactionManager::ParCompactionManager(PreservedMarks* preserved_marks) {\n@@ -61,3 +64,1 @@\n-  reset_bitmap_query_cache();\n-\n-  _deferred_obj_array = new (mtGC) GrowableArray<HeapWord*>(10, mtGC);\n+  _preserved_marks = preserved_marks;\n@@ -82,0 +83,3 @@\n+  _preserved_marks_set = new PreservedMarksSet(true);\n+  _preserved_marks_set->init(parallel_gc_threads);\n+\n@@ -84,1 +88,1 @@\n-    _manager_array[i] = new ParCompactionManager();\n+    _manager_array[i] = new ParCompactionManager(_preserved_marks_set->get(i));\n@@ -96,1 +100,0 @@\n-}\n@@ -98,5 +101,0 @@\n-void ParCompactionManager::reset_all_bitmap_query_caches() {\n-  uint parallel_gc_threads = ParallelScavengeHeap::heap()->workers().max_workers();\n-  for (uint i=0; i<parallel_gc_threads; i++) {\n-    _manager_array[i]->reset_bitmap_query_cache();\n-  }\n@@ -171,9 +169,0 @@\n-void ParCompactionManager::drain_deferred_objects() {\n-  while (!_deferred_obj_array->is_empty()) {\n-    HeapWord* addr = _deferred_obj_array->pop();\n-    assert(addr != nullptr, \"expected a deferred object\");\n-    PSParallelCompact::update_deferred_object(this, addr);\n-  }\n-  _deferred_obj_array->clear_and_deallocate();\n-}\n-\n@@ -210,4 +199,0 @@\n-void ParCompactionManager::push_deferred_object(HeapWord* addr) {\n-  _deferred_obj_array->push(addr);\n-}\n-\n","filename":"src\/hotspot\/share\/gc\/parallel\/psCompactionManager.cpp","additions":9,"deletions":24,"binary":false,"changes":33,"status":"modified"},{"patch":"@@ -29,0 +29,1 @@\n+#include \"gc\/shared\/preservedMarks.hpp\"\n@@ -48,1 +49,1 @@\n-  friend class UpdateDensePrefixAndCompactionTask;\n+  friend class FillDensePrefixAndCompactionTask;\n@@ -78,1 +79,2 @@\n-  GrowableArray<HeapWord*>*    _deferred_obj_array;\n+  static PreservedMarksSet* _preserved_marks_set;\n+  PreservedMarks* _preserved_marks;\n@@ -90,4 +92,0 @@\n-  HeapWord* _last_query_beg;\n-  oop _last_query_obj;\n-  size_t _last_query_ret;\n-\n@@ -109,1 +107,1 @@\n-  ParCompactionManager();\n+  ParCompactionManager(PreservedMarks* preserved_marks);\n@@ -156,8 +154,0 @@\n-  void push_deferred_object(HeapWord* addr);\n-\n-  void reset_bitmap_query_cache() {\n-    _last_query_beg = nullptr;\n-    _last_query_obj = nullptr;\n-    _last_query_ret = 0;\n-  }\n-\n@@ -168,11 +158,0 @@\n-  \/\/ Bitmap query support, cache last query and result\n-  HeapWord* last_query_begin() { return _last_query_beg; }\n-  oop last_query_object() { return _last_query_obj; }\n-  size_t last_query_return() { return _last_query_ret; }\n-\n-  void set_last_query_begin(HeapWord *new_beg) { _last_query_beg = new_beg; }\n-  void set_last_query_object(oop new_obj) { _last_query_obj = new_obj; }\n-  void set_last_query_return(size_t new_ret) { _last_query_ret = new_ret; }\n-\n-  static void reset_all_bitmap_query_caches();\n-\n@@ -187,0 +166,3 @@\n+  PreservedMarks* preserved_marks() const {\n+    return _preserved_marks;\n+  }\n@@ -211,1 +193,0 @@\n-  void drain_deferred_objects();\n@@ -216,2 +197,0 @@\n-  void update_contents(oop obj);\n-\n","filename":"src\/hotspot\/share\/gc\/parallel\/psCompactionManager.hpp","additions":8,"deletions":29,"binary":false,"changes":37,"status":"modified"},{"patch":"@@ -160,7 +160,0 @@\n-inline void ParCompactionManager::update_contents(oop obj) {\n-  if (!obj->klass()->is_typeArray_klass()) {\n-    PCAdjustPointerClosure apc(this);\n-    obj->oop_iterate(&apc);\n-  }\n-}\n-\n","filename":"src\/hotspot\/share\/gc\/parallel\/psCompactionManager.inline.hpp","additions":0,"deletions":7,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -33,0 +33,1 @@\n+#include \"gc\/parallel\/objectStartArray.inline.hpp\"\n@@ -57,0 +58,1 @@\n+#include \"gc\/shared\/preservedMarks.inline.hpp\"\n@@ -100,0 +102,1 @@\n+\/\/ todo: smaller region size for debugging only\n@@ -101,0 +104,1 @@\n+\/\/ const size_t ParallelCompactData::Log2RegionSize  = 10; \/\/ 64K words\n@@ -102,0 +106,1 @@\n+static_assert(ParallelCompactData::RegionSize >= BitsPerWord, \"region-start bit word-aligned\");\n@@ -108,12 +113,0 @@\n-const size_t ParallelCompactData::Log2BlockSize   = 7; \/\/ 128 words\n-const size_t ParallelCompactData::BlockSize       = (size_t)1 << Log2BlockSize;\n-const size_t ParallelCompactData::BlockSizeBytes  =\n-  BlockSize << LogHeapWordSize;\n-const size_t ParallelCompactData::BlockSizeOffsetMask = BlockSize - 1;\n-const size_t ParallelCompactData::BlockAddrOffsetMask = BlockSizeBytes - 1;\n-const size_t ParallelCompactData::BlockAddrMask       = ~BlockAddrOffsetMask;\n-\n-const size_t ParallelCompactData::BlocksPerRegion = RegionSize \/ BlockSize;\n-const size_t ParallelCompactData::Log2BlocksPerRegion =\n-  Log2RegionSize - Log2BlockSize;\n-\n@@ -416,4 +409,1 @@\n-  _region_count(0),\n-  _block_vspace(nullptr),\n-  _block_data(nullptr),\n-  _block_count(0) {}\n+  _region_count(0) {}\n@@ -430,2 +420,1 @@\n-  bool result = initialize_region_data(heap_size) && initialize_block_data();\n-  return result;\n+  return initialize_region_data(heap_size);\n@@ -477,13 +466,0 @@\n-bool ParallelCompactData::initialize_block_data()\n-{\n-  assert(_region_count != 0, \"region data must be initialized first\");\n-  const size_t count = _region_count << Log2BlocksPerRegion;\n-  _block_vspace = create_vspace(count, sizeof(BlockData));\n-  if (_block_vspace != 0) {\n-    _block_data = (BlockData*)_block_vspace->reserved_low_addr();\n-    _block_count = count;\n-    return true;\n-  }\n-  return false;\n-}\n-\n@@ -493,1 +469,0 @@\n-  assert(RegionSize % BlockSize == 0, \"RegionSize not a multiple of BlockSize\");\n@@ -497,18 +472,0 @@\n-\n-  const size_t beg_block = beg_region * BlocksPerRegion;\n-  const size_t block_cnt = region_cnt * BlocksPerRegion;\n-  memset(_block_data + beg_block, 0, block_cnt * sizeof(BlockData));\n-}\n-\n-HeapWord* ParallelCompactData::partial_obj_end(size_t region_idx) const\n-{\n-  const RegionData* cur_cp = region(region_idx);\n-  const RegionData* const end_cp = region(region_count() - 1);\n-\n-  HeapWord* result = region_to_addr(region_idx);\n-  if (cur_cp < end_cp) {\n-    do {\n-      result += cur_cp->partial_obj_size();\n-    } while (cur_cp->partial_obj_size() == RegionSize && ++cur_cp < end_cp);\n-  }\n-  return result;\n@@ -765,43 +722,0 @@\n-HeapWord* ParallelCompactData::calc_new_pointer(HeapWord* addr, ParCompactionManager* cm) const {\n-  assert(addr != nullptr, \"Should detect null oop earlier\");\n-  assert(ParallelScavengeHeap::heap()->is_in(addr), \"not in heap\");\n-  assert(PSParallelCompact::mark_bitmap()->is_marked(addr), \"not marked\");\n-\n-  \/\/ Region covering the object.\n-  RegionData* const region_ptr = addr_to_region_ptr(addr);\n-  HeapWord* result = region_ptr->destination();\n-\n-  \/\/ If the entire Region is live, the new location is region->destination + the\n-  \/\/ offset of the object within in the Region.\n-\n-  \/\/ Run some performance tests to determine if this special case pays off.  It\n-  \/\/ is worth it for pointers into the dense prefix.  If the optimization to\n-  \/\/ avoid pointer updates in regions that only point to the dense prefix is\n-  \/\/ ever implemented, this should be revisited.\n-  if (region_ptr->data_size() == RegionSize) {\n-    result += region_offset(addr);\n-    return result;\n-  }\n-\n-  \/\/ Otherwise, the new location is region->destination + block offset + the\n-  \/\/ number of live words in the Block that are (a) to the left of addr and (b)\n-  \/\/ due to objects that start in the Block.\n-\n-  \/\/ Fill in the block table if necessary.  This is unsynchronized, so multiple\n-  \/\/ threads may fill the block table for a region (harmless, since it is\n-  \/\/ idempotent).\n-  if (!region_ptr->blocks_filled()) {\n-    PSParallelCompact::fill_blocks(addr_to_region_idx(addr));\n-    region_ptr->set_blocks_filled();\n-  }\n-\n-  HeapWord* const search_start = block_align_down(addr);\n-  const size_t block_offset = addr_to_block_ptr(addr)->offset();\n-\n-  const ParMarkBitMap* bitmap = PSParallelCompact::mark_bitmap();\n-  const size_t live = bitmap->live_words_in_range(cm, search_start, cast_to_oop(addr));\n-  result += block_offset + live;\n-  DEBUG_ONLY(PSParallelCompact::check_new_location(addr, result));\n-  return result;\n-}\n-\n@@ -821,1 +735,0 @@\n-  verify_clear(_block_vspace);\n@@ -835,0 +748,13 @@\n+class PCAdjustPointerClosure: public BasicOopIterateClosure {\n+  template <typename T>\n+  void do_oop_work(T* p) { PSParallelCompact::adjust_pointer(p); }\n+\n+public:\n+  virtual void do_oop(oop* p)                { do_oop_work(p); }\n+  virtual void do_oop(narrowOop* p)          { do_oop_work(p); }\n+\n+  virtual ReferenceIterationMode reference_iteration_mode() { return DO_FIELDS; }\n+};\n+\n+static PCAdjustPointerClosure pc_adjust_pointer_closure;\n+\n@@ -951,2 +877,0 @@\n-\n-  ParCompactionManager::reset_all_bitmap_query_caches();\n@@ -1082,1 +1006,1 @@\n-      _mark_bitmap.is_obj_beg(dense_prefix_bit)) {\n+      _mark_bitmap.is_marked(dense_prefix_bit)) {\n@@ -1087,1 +1011,3 @@\n-  if (_mark_bitmap.is_obj_end(dense_prefix_bit - 2)) {\n+  HeapWord* block_start = start_array(id)->block_start_reaching_into_card(dense_prefix_end);\n+  if (block_start == dense_prefix_end - 1) {\n+    assert(!_mark_bitmap.is_marked(block_start), \"inv\");\n@@ -1089,1 +1015,1 @@\n-    \/\/ The filler object will extend into the region after the last dense prefix region.\n+    \/\/ The filler object will extend into region_after_dense_prefix.\n@@ -1093,1 +1019,1 @@\n-    _mark_bitmap.mark_obj(obj_beg, obj_len);\n+    _mark_bitmap.mark_obj(obj_beg);\n@@ -1364,1 +1290,3 @@\n-    \/\/ adjust_roots() updates Universe::_intArrayKlass which is\n+    forward_to_new_addr();\n+\n+    \/\/ adjust_pointers() updates Universe::_intArrayKlass which is\n@@ -1366,1 +1294,1 @@\n-    adjust_roots();\n+    adjust_pointers();\n@@ -1370,0 +1298,2 @@\n+    ParCompactionManager::_preserved_marks_set->restore(&ParallelScavengeHeap::heap()->workers());\n+\n@@ -1694,0 +1624,78 @@\n+template<typename Func>\n+void PSParallelCompact::adjust_in_space_helper(SpaceId id, volatile uint* claim_counter, Func&& on_stripe) {\n+  MutableSpace* sp = PSParallelCompact::space(id);\n+  HeapWord* const bottom = sp->bottom();\n+  HeapWord* const top = sp->top();\n+  if (bottom == top) {\n+    return;\n+  }\n+\n+  const uint num_regions_per_stripe = 2;\n+  const size_t region_size = ParallelCompactData::RegionSize;\n+  const size_t stripe_size = num_regions_per_stripe * region_size;\n+\n+  while (true) {\n+    uint counter = Atomic::fetch_then_add(claim_counter, num_regions_per_stripe);\n+    HeapWord* cur_stripe = bottom + counter * region_size;\n+    if (cur_stripe >= top) {\n+      break;\n+    }\n+    HeapWord* stripe_end = MIN2(cur_stripe + stripe_size, top);\n+    on_stripe(cur_stripe, stripe_end);\n+  }\n+}\n+\n+void PSParallelCompact::adjust_in_old_space(volatile uint* claim_counter) {\n+  \/\/ Regions in old-space shouldn't be split.\n+  assert(!_space_info[old_space_id].split_info().is_valid(), \"inv\");\n+\n+  auto scan_obj_with_limit = [&] (HeapWord* obj_start, HeapWord* left, HeapWord* right) {\n+    assert(mark_bitmap()->is_marked(obj_start), \"inv\");\n+    oop obj = cast_to_oop(obj_start);\n+    return obj->oop_iterate_size(&pc_adjust_pointer_closure, MemRegion(left, right));\n+  };\n+\n+  adjust_in_space_helper(old_space_id, claim_counter, [&] (HeapWord* stripe_start, HeapWord* stripe_end) {\n+    assert(_summary_data.is_region_aligned(stripe_start), \"inv\");\n+    RegionData* cur_region = _summary_data.addr_to_region_ptr(stripe_start);\n+    HeapWord* obj_start;\n+    if (cur_region->partial_obj_size() != 0) {\n+      obj_start = cur_region->partial_obj_addr();\n+      obj_start += scan_obj_with_limit(obj_start, stripe_start, stripe_end);\n+    } else {\n+      obj_start = stripe_start;\n+    }\n+\n+    while (obj_start < stripe_end) {\n+      obj_start = mark_bitmap()->find_obj_beg(obj_start, stripe_end);\n+      if (obj_start >= stripe_end) {\n+        break;\n+      }\n+      obj_start += scan_obj_with_limit(obj_start, stripe_start, stripe_end);\n+    }\n+  });\n+}\n+\n+void PSParallelCompact::adjust_in_young_space(SpaceId id, volatile uint* claim_counter) {\n+  adjust_in_space_helper(id, claim_counter, [](HeapWord* stripe_start, HeapWord* stripe_end) {\n+    HeapWord* obj_start = stripe_start;\n+    while (obj_start < stripe_end) {\n+      obj_start = mark_bitmap()->find_obj_beg(obj_start, stripe_end);\n+      if (obj_start >= stripe_end) {\n+        break;\n+      }\n+      oop obj = cast_to_oop(obj_start);\n+      obj_start += obj->oop_iterate_size(&pc_adjust_pointer_closure);\n+    }\n+  });\n+}\n+\n+void PSParallelCompact::adjust_pointers_in_spaces(uint worker_id, volatile uint* claim_counters) {\n+  auto start_time = Ticks::now();\n+  adjust_in_old_space(&claim_counters[0]);\n+  for (uint id = eden_space_id; id < last_space_id; ++id) {\n+    adjust_in_young_space(SpaceId(id), &claim_counters[id]);\n+  }\n+  log_trace(gc, phases)(\"adjust_pointers_in_spaces worker %u: %.3f ms\", worker_id, (Ticks::now() - start_time).seconds() * 1000);\n+}\n+\n@@ -1699,0 +1707,1 @@\n+  volatile uint _claim_counters[PSParallelCompact::last_space_id] = {};\n@@ -1725,1 +1734,5 @@\n-    PCAdjustPointerClosure adjust(cm);\n+    cm->preserved_marks()->adjust_during_full_gc();\n+    {\n+      \/\/ adjust pointers in all spaces\n+      PSParallelCompact::adjust_pointers_in_spaces(worker_id, _claim_counters);\n+    }\n@@ -1728,1 +1741,1 @@\n-      Threads::possibly_parallel_oops_do(_nworkers > 1, &adjust, nullptr);\n+      Threads::possibly_parallel_oops_do(_nworkers > 1, &pc_adjust_pointer_closure, nullptr);\n@@ -1730,1 +1743,1 @@\n-    _oop_storage_iter.oops_do(&adjust);\n+    _oop_storage_iter.oops_do(&pc_adjust_pointer_closure);\n@@ -1732,1 +1745,1 @@\n-      CLDToOopClosure cld_closure(&adjust, ClassLoaderData::_claim_stw_fullgc_adjust);\n+      CLDToOopClosure cld_closure(&pc_adjust_pointer_closure, ClassLoaderData::_claim_stw_fullgc_adjust);\n@@ -1737,1 +1750,1 @@\n-      _weak_proc_task.work(worker_id, &always_alive, &adjust);\n+      _weak_proc_task.work(worker_id, &always_alive, &pc_adjust_pointer_closure);\n@@ -1740,1 +1753,1 @@\n-      NMethodToOopClosure adjust_code(&adjust, NMethodToOopClosure::FixRelocations);\n+      NMethodToOopClosure adjust_code(&pc_adjust_pointer_closure, NMethodToOopClosure::FixRelocations);\n@@ -1747,1 +1760,1 @@\n-void PSParallelCompact::adjust_roots() {\n+void PSParallelCompact::adjust_pointers() {\n@@ -1749,1 +1762,1 @@\n-  GCTraceTime(Info, gc, phases) tm(\"Adjust Roots\", &_gc_timer);\n+  GCTraceTime(Info, gc, phases) tm(\"Adjust Pointers\", &_gc_timer);\n@@ -1755,0 +1768,125 @@\n+\/\/ Split [start, end) evenly for a number of workers and return the\n+\/\/ range for worker_id.\n+static void split_regions_for_worker(size_t start, size_t end,\n+                                     uint worker_id, uint num_workers,\n+                                     size_t* worker_start, size_t* worker_end) {\n+  assert(start < end, \"precondition\");\n+  assert(num_workers > 0, \"precondition\");\n+  assert(worker_id < num_workers, \"precondition\");\n+\n+  size_t num_regions = end - start;\n+  size_t num_regions_per_worker = num_regions \/ num_workers;\n+  size_t remainder = num_regions % num_workers;\n+  \/\/ The first few workers will get one extra.\n+  *worker_start = start + worker_id * num_regions_per_worker\n+                  + MIN2(checked_cast<size_t>(worker_id), remainder);\n+  *worker_end = *worker_start + num_regions_per_worker\n+                + (worker_id < remainder ? 1 : 0);\n+}\n+\n+void PSParallelCompact::forward_to_new_addr() {\n+  GCTraceTime(Info, gc, phases) tm(\"Forward\", &_gc_timer);\n+  uint nworkers = ParallelScavengeHeap::heap()->workers().active_workers();\n+\n+  struct ForwardTask final : public WorkerTask {\n+    uint _num_workers;\n+\n+    explicit ForwardTask(uint num_workers) :\n+      WorkerTask(\"PSForward task\"),\n+      _num_workers(num_workers) {}\n+\n+    void work(uint worker_id) override {\n+      ParCompactionManager* cm = ParCompactionManager::gc_thread_compaction_manager(worker_id);\n+      for (uint id = old_space_id; id < last_space_id; ++id) {\n+        MutableSpace* sp = PSParallelCompact::space(SpaceId(id));\n+        HeapWord* dense_prefix_addr = dense_prefix(SpaceId(id));\n+        HeapWord* top = sp->top();\n+\n+        if (dense_prefix_addr == top) {\n+          continue;\n+        }\n+\n+        size_t dense_prefix_region = _summary_data.addr_to_region_idx(dense_prefix_addr);\n+        size_t top_region = _summary_data.addr_to_region_idx(_summary_data.region_align_up(top));\n+        size_t start_region;\n+        size_t end_region;\n+        split_regions_for_worker(dense_prefix_region, top_region,\n+                                 worker_id, _num_workers,\n+                                 &start_region, &end_region);\n+        for (size_t cur_region = start_region; cur_region < end_region; ++cur_region) {\n+          RegionData* region_ptr = _summary_data.region(cur_region);\n+          size_t live_words = region_ptr->partial_obj_size();\n+\n+          if (live_words == ParallelCompactData::RegionSize) {\n+            \/\/ No obj-start\n+            continue;\n+          }\n+\n+          HeapWord* region_start = _summary_data.region_to_addr(cur_region);\n+          HeapWord* region_end = region_start + ParallelCompactData::RegionSize;\n+\n+          HeapWord* cur_addr = region_start + live_words;\n+\n+          HeapWord* destination = region_ptr->destination();\n+          while (cur_addr < region_end) {\n+            cur_addr = mark_bitmap()->find_obj_beg(cur_addr, region_end);\n+            if (cur_addr >= region_end) {\n+              break;\n+            }\n+            assert(mark_bitmap()->is_marked(cur_addr), \"inv\");\n+            HeapWord* new_addr = destination + live_words;\n+            oop obj = cast_to_oop(cur_addr);\n+            if (new_addr != cur_addr) {\n+              cm->preserved_marks()->push_if_necessary(obj, obj->mark());\n+              obj->forward_to(cast_to_oop(new_addr));\n+            }\n+            size_t obj_size = obj->size();\n+            live_words += obj_size;\n+            cur_addr += obj_size;\n+          }\n+        }\n+      }\n+    }\n+  } task(nworkers);\n+\n+  ParallelScavengeHeap::heap()->workers().run_task(&task);\n+  debug_only(verify_forward();)\n+}\n+\n+#ifdef ASSERT\n+void PSParallelCompact::verify_forward() {\n+  HeapWord* old_dense_prefix_addr = dense_prefix(SpaceId(old_space_id));\n+  RegionData* old_region = _summary_data.region(_summary_data.addr_to_region_idx(old_dense_prefix_addr));\n+  HeapWord* bump_ptr = old_region->partial_obj_size() != 0\n+                       ? old_dense_prefix_addr + old_region->partial_obj_size()\n+                       : old_dense_prefix_addr;\n+  SpaceId bump_ptr_space = old_space_id;\n+\n+  for (uint id = old_space_id; id < last_space_id; ++id) {\n+    MutableSpace* sp = PSParallelCompact::space(SpaceId(id));\n+    HeapWord* dense_prefix_addr = dense_prefix(SpaceId(id));\n+    HeapWord* top = sp->top();\n+    HeapWord* cur_addr = dense_prefix_addr;\n+\n+    while (cur_addr < top) {\n+      cur_addr = mark_bitmap()->find_obj_beg(cur_addr, top);\n+      if (cur_addr >= top) {\n+        break;\n+      }\n+      assert(mark_bitmap()->is_marked(cur_addr), \"inv\");\n+      \/\/ Move to the space containing cur_addr\n+      if (bump_ptr == _space_info[bump_ptr_space].new_top()) {\n+        bump_ptr = space(space_id(cur_addr))->bottom();\n+        bump_ptr_space = space_id(bump_ptr);\n+      }\n+      oop obj = cast_to_oop(cur_addr);\n+      if (cur_addr != bump_ptr) {\n+        assert(obj->forwardee() == cast_to_oop(bump_ptr), \"inv\");\n+      }\n+      bump_ptr += obj->size();\n+      cur_addr += obj->size();\n+    }\n+  }\n+}\n+#endif\n+\n@@ -1835,154 +1973,0 @@\n-class TaskQueue : StackObj {\n-  volatile uint _counter;\n-  uint _size;\n-  uint _insert_index;\n-  PSParallelCompact::UpdateDensePrefixTask* _backing_array;\n-public:\n-  explicit TaskQueue(uint size) : _counter(0), _size(size), _insert_index(0), _backing_array(nullptr) {\n-    _backing_array = NEW_C_HEAP_ARRAY(PSParallelCompact::UpdateDensePrefixTask, _size, mtGC);\n-  }\n-  ~TaskQueue() {\n-    assert(_counter >= _insert_index, \"not all queue elements were claimed\");\n-    FREE_C_HEAP_ARRAY(T, _backing_array);\n-  }\n-\n-  void push(const PSParallelCompact::UpdateDensePrefixTask& value) {\n-    assert(_insert_index < _size, \"too small backing array\");\n-    _backing_array[_insert_index++] = value;\n-  }\n-\n-  bool try_claim(PSParallelCompact::UpdateDensePrefixTask& reference) {\n-    uint claimed = Atomic::fetch_then_add(&_counter, 1u);\n-    if (claimed < _insert_index) {\n-      reference = _backing_array[claimed];\n-      return true;\n-    } else {\n-      return false;\n-    }\n-  }\n-};\n-\n-#define PAR_OLD_DENSE_PREFIX_OVER_PARTITIONING 4\n-\n-void PSParallelCompact::enqueue_dense_prefix_tasks(TaskQueue& task_queue,\n-                                                   uint parallel_gc_threads) {\n-  GCTraceTime(Trace, gc, phases) tm(\"Dense Prefix Task Setup\", &_gc_timer);\n-\n-  ParallelCompactData& sd = PSParallelCompact::summary_data();\n-\n-  \/\/ Iterate over all the spaces adding tasks for updating\n-  \/\/ regions in the dense prefix.  Assume that 1 gc thread\n-  \/\/ will work on opening the gaps and the remaining gc threads\n-  \/\/ will work on the dense prefix.\n-  unsigned int space_id;\n-  for (space_id = old_space_id; space_id < last_space_id; ++ space_id) {\n-    HeapWord* const dense_prefix_end = _space_info[space_id].dense_prefix();\n-    const MutableSpace* const space = _space_info[space_id].space();\n-\n-    if (dense_prefix_end == space->bottom()) {\n-      \/\/ There is no dense prefix for this space.\n-      continue;\n-    }\n-\n-    \/\/ The dense prefix is before this region.\n-    size_t region_index_end_dense_prefix =\n-        sd.addr_to_region_idx(dense_prefix_end);\n-    RegionData* const dense_prefix_cp =\n-      sd.region(region_index_end_dense_prefix);\n-    assert(dense_prefix_end == space->end() ||\n-           dense_prefix_cp->available() ||\n-           dense_prefix_cp->claimed(),\n-           \"The region after the dense prefix should always be ready to fill\");\n-\n-    size_t region_index_start = sd.addr_to_region_idx(space->bottom());\n-\n-    \/\/ Is there dense prefix work?\n-    size_t total_dense_prefix_regions =\n-      region_index_end_dense_prefix - region_index_start;\n-    \/\/ How many regions of the dense prefix should be given to\n-    \/\/ each thread?\n-    if (total_dense_prefix_regions > 0) {\n-      uint tasks_for_dense_prefix = 1;\n-      if (total_dense_prefix_regions <=\n-          (parallel_gc_threads * PAR_OLD_DENSE_PREFIX_OVER_PARTITIONING)) {\n-        \/\/ Don't over partition.  This assumes that\n-        \/\/ PAR_OLD_DENSE_PREFIX_OVER_PARTITIONING is a small integer value\n-        \/\/ so there are not many regions to process.\n-        tasks_for_dense_prefix = parallel_gc_threads;\n-      } else {\n-        \/\/ Over partition\n-        tasks_for_dense_prefix = parallel_gc_threads *\n-          PAR_OLD_DENSE_PREFIX_OVER_PARTITIONING;\n-      }\n-      size_t regions_per_thread = total_dense_prefix_regions \/\n-        tasks_for_dense_prefix;\n-      \/\/ Give each thread at least 1 region.\n-      if (regions_per_thread == 0) {\n-        regions_per_thread = 1;\n-      }\n-\n-      for (uint k = 0; k < tasks_for_dense_prefix; k++) {\n-        if (region_index_start >= region_index_end_dense_prefix) {\n-          break;\n-        }\n-        \/\/ region_index_end is not processed\n-        size_t region_index_end = MIN2(region_index_start + regions_per_thread,\n-                                       region_index_end_dense_prefix);\n-        task_queue.push(UpdateDensePrefixTask(SpaceId(space_id),\n-                                              region_index_start,\n-                                              region_index_end));\n-        region_index_start = region_index_end;\n-      }\n-    }\n-    \/\/ This gets any part of the dense prefix that did not\n-    \/\/ fit evenly.\n-    if (region_index_start < region_index_end_dense_prefix) {\n-      task_queue.push(UpdateDensePrefixTask(SpaceId(space_id),\n-                                            region_index_start,\n-                                            region_index_end_dense_prefix));\n-    }\n-  }\n-}\n-\n-#ifdef ASSERT\n-\/\/ Write a histogram of the number of times the block table was filled for a\n-\/\/ region.\n-void PSParallelCompact::write_block_fill_histogram()\n-{\n-  if (!log_develop_is_enabled(Trace, gc, compaction)) {\n-    return;\n-  }\n-\n-  Log(gc, compaction) log;\n-  ResourceMark rm;\n-  LogStream ls(log.trace());\n-  outputStream* out = &ls;\n-\n-  typedef ParallelCompactData::RegionData rd_t;\n-  ParallelCompactData& sd = summary_data();\n-\n-  for (unsigned int id = old_space_id; id < last_space_id; ++id) {\n-    MutableSpace* const spc = _space_info[id].space();\n-    if (spc->bottom() != spc->top()) {\n-      const rd_t* const beg = sd.addr_to_region_ptr(spc->bottom());\n-      HeapWord* const top_aligned_up = sd.region_align_up(spc->top());\n-      const rd_t* const end = sd.addr_to_region_ptr(top_aligned_up);\n-\n-      size_t histo[5] = { 0, 0, 0, 0, 0 };\n-      const size_t histo_len = sizeof(histo) \/ sizeof(size_t);\n-      const size_t region_cnt = pointer_delta(end, beg, sizeof(rd_t));\n-\n-      for (const rd_t* cur = beg; cur < end; ++cur) {\n-        ++histo[MIN2(cur->blocks_filled_count(), histo_len - 1)];\n-      }\n-      out->print(\"Block fill histogram: %u %-4s\" SIZE_FORMAT_W(5), id, space_names[id], region_cnt);\n-      for (size_t i = 0; i < histo_len; ++i) {\n-        out->print(\" \" SIZE_FORMAT_W(5) \" %5.1f%%\",\n-                   histo[i], 100.0 * histo[i] \/ region_cnt);\n-      }\n-      out->cr();\n-    }\n-  }\n-}\n-#endif \/\/ #ifdef ASSERT\n-\n@@ -2021,2 +2005,2 @@\n-class UpdateDensePrefixAndCompactionTask: public WorkerTask {\n-  TaskQueue& _tq;\n+class FillDensePrefixAndCompactionTask: public WorkerTask {\n+  uint _num_workers;\n@@ -2026,3 +2010,3 @@\n-  UpdateDensePrefixAndCompactionTask(TaskQueue& tq, uint active_workers) :\n-      WorkerTask(\"UpdateDensePrefixAndCompactionTask\"),\n-      _tq(tq),\n+  FillDensePrefixAndCompactionTask(uint active_workers) :\n+      WorkerTask(\"FillDensePrefixAndCompactionTask\"),\n+      _num_workers(active_workers),\n@@ -2031,0 +2015,1 @@\n+\n@@ -2032,1 +2017,8 @@\n-    ParCompactionManager* cm = ParCompactionManager::gc_thread_compaction_manager(worker_id);\n+    {\n+      auto start = Ticks::now();\n+      PSParallelCompact::fill_dead_objs_in_dense_prefix(worker_id, _num_workers);\n+      log_trace(gc, phases)(\"Fill dense prefix by worker %u: %.3f ms\", worker_id, (Ticks::now() - start).seconds() * 1000);\n+    }\n+    compaction_with_stealing_work(&_terminator, worker_id);\n+  }\n+};\n@@ -2034,5 +2026,10 @@\n-    for (PSParallelCompact::UpdateDensePrefixTask task; _tq.try_claim(task); \/* empty *\/) {\n-      PSParallelCompact::update_and_deadwood_in_dense_prefix(cm,\n-                                                             task._space_id,\n-                                                             task._region_index_start,\n-                                                             task._region_index_end);\n+void PSParallelCompact::fill_range_in_dense_prefix(HeapWord* start, HeapWord* end) {\n+#ifdef ASSERT\n+  {\n+    assert(start < end, \"precondition\");\n+    assert(mark_bitmap()->find_obj_beg(start, end) == end, \"precondition\");\n+    HeapWord* bottom = _space_info[old_space_id].space()->bottom();\n+    if (start != bottom) {\n+      HeapWord* obj_start = mark_bitmap()->find_obj_beg_reverse(bottom, start);\n+      HeapWord* after_obj = obj_start + cast_to_oop(obj_start)->size();\n+      assert(after_obj == start, \"precondition\");\n@@ -2040,0 +2037,2 @@\n+  }\n+#endif\n@@ -2041,3 +2040,14 @@\n-    \/\/ Once a thread has drained it's stack, it should try to steal regions from\n-    \/\/ other threads.\n-    compaction_with_stealing_work(&_terminator, worker_id);\n+  CollectedHeap::fill_with_objects(start, pointer_delta(end, start));\n+  HeapWord* addr = start;\n+  do {\n+    size_t size = cast_to_oop(addr)->size();\n+    start_array(old_space_id)->update_for_block(addr, addr + size);\n+    addr += size;\n+  } while (addr < end);\n+}\n+\n+void PSParallelCompact::fill_dead_objs_in_dense_prefix(uint worker_id, uint num_workers) {\n+  ParMarkBitMap* bitmap = mark_bitmap();\n+\n+  HeapWord* const bottom = _space_info[old_space_id].space()->bottom();\n+  HeapWord* const prefix_end = dense_prefix(old_space_id);\n@@ -2045,3 +2055,2 @@\n-    \/\/ At this point all regions have been compacted, so it's now safe\n-    \/\/ to update the deferred objects that cross region boundaries.\n-    cm->drain_deferred_objects();\n+  if (bottom == prefix_end) {\n+    return;\n@@ -2049,1 +2058,45 @@\n-};\n+\n+  size_t bottom_region = _summary_data.addr_to_region_idx(bottom);\n+  size_t prefix_end_region = _summary_data.addr_to_region_idx(prefix_end);\n+\n+  size_t start_region;\n+  size_t end_region;\n+  split_regions_for_worker(bottom_region, prefix_end_region,\n+                           worker_id, num_workers,\n+                           &start_region, &end_region);\n+\n+  if (start_region == end_region) {\n+    return;\n+  }\n+\n+  HeapWord* const start_addr = _summary_data.region_to_addr(start_region);\n+  HeapWord* const end_addr = _summary_data.region_to_addr(end_region);\n+\n+  \/\/ Skip live partial obj (if any) from previous region.\n+  HeapWord* cur_addr;\n+  RegionData* start_region_ptr = _summary_data.region(start_region);\n+  if (start_region_ptr->partial_obj_size() != 0) {\n+    HeapWord* partial_obj_start = start_region_ptr->partial_obj_addr();\n+    assert(bitmap->is_marked(partial_obj_start), \"inv\");\n+    cur_addr = partial_obj_start + cast_to_oop(partial_obj_start)->size();\n+  } else {\n+    cur_addr = start_addr;\n+  }\n+\n+  \/\/ end_addr is inclusive to handle regions starting with dead space.\n+  while (cur_addr <= end_addr) {\n+    \/\/ Use prefix_end to handle trailing obj in each worker region-chunk.\n+    HeapWord* live_start = bitmap->find_obj_beg(cur_addr, prefix_end);\n+    if (cur_addr != live_start) {\n+      \/\/ Only worker 0 handles proceeding dead space.\n+      if (cur_addr != start_addr || worker_id == 0) {\n+        fill_range_in_dense_prefix(cur_addr, live_start);\n+      }\n+    }\n+    if (live_start >= end_addr) {\n+      break;\n+    }\n+    assert(bitmap->is_marked(live_start), \"inv\");\n+    cur_addr = live_start + cast_to_oop(live_start)->size();\n+  }\n+}\n@@ -2054,2 +2107,0 @@\n-  ParallelScavengeHeap* heap = ParallelScavengeHeap::heap();\n-  PSOldGen* old_gen = heap->old_gen();\n@@ -2058,7 +2109,0 @@\n-  \/\/ for [0..last_space_id)\n-  \/\/     for [0..active_gc_threads * PAR_OLD_DENSE_PREFIX_OVER_PARTITIONING)\n-  \/\/         push\n-  \/\/     push\n-  \/\/\n-  \/\/ max push count is thus: last_space_id * (active_gc_threads * PAR_OLD_DENSE_PREFIX_OVER_PARTITIONING + 1)\n-  TaskQueue task_queue(last_space_id * (active_gc_threads * PAR_OLD_DENSE_PREFIX_OVER_PARTITIONING + 1));\n@@ -2067,1 +2111,0 @@\n-  enqueue_dense_prefix_tasks(task_queue, active_gc_threads);\n@@ -2072,1 +2115,1 @@\n-    UpdateDensePrefixAndCompactionTask task(task_queue, active_gc_threads);\n+    FillDensePrefixAndCompactionTask task(active_gc_threads);\n@@ -2076,0 +2119,2 @@\n+    verify_filler_in_dense_prefix();\n+\n@@ -2082,2 +2127,0 @@\n-\n-  DEBUG_ONLY(write_block_fill_histogram());\n@@ -2087,0 +2130,15 @@\n+void PSParallelCompact::verify_filler_in_dense_prefix() {\n+  HeapWord* bottom = _space_info[old_space_id].space()->bottom();\n+  HeapWord* dense_prefix_end = dense_prefix(old_space_id);\n+  HeapWord* cur_addr = bottom;\n+  while (cur_addr < dense_prefix_end) {\n+    oop obj = cast_to_oop(cur_addr);\n+    oopDesc::verify(obj);\n+    if (!mark_bitmap()->is_marked(cur_addr)) {\n+      Klass* k = cast_to_oop(cur_addr)->klass_without_asserts();\n+      assert(k == Universe::fillerArrayKlass() || k == vmClasses::FillerObject_klass(), \"inv\");\n+    }\n+    cur_addr += obj->size();\n+  }\n+}\n+\n@@ -2126,66 +2184,0 @@\n-inline void UpdateOnlyClosure::do_addr(HeapWord* addr) {\n-  compaction_manager()->update_contents(cast_to_oop(addr));\n-}\n-\n-\/\/ Update interior oops in the ranges of regions [beg_region, end_region).\n-void\n-PSParallelCompact::update_and_deadwood_in_dense_prefix(ParCompactionManager* cm,\n-                                                       SpaceId space_id,\n-                                                       size_t beg_region,\n-                                                       size_t end_region) {\n-  ParallelCompactData& sd = summary_data();\n-  ParMarkBitMap* const mbm = mark_bitmap();\n-\n-  HeapWord* beg_addr = sd.region_to_addr(beg_region);\n-  HeapWord* const end_addr = sd.region_to_addr(end_region);\n-  assert(beg_region <= end_region, \"bad region range\");\n-  assert(end_addr <= dense_prefix(space_id), \"not in the dense prefix\");\n-\n-#ifdef  ASSERT\n-  \/\/ Claim the regions to avoid triggering an assert when they are marked as\n-  \/\/ filled.\n-  for (size_t claim_region = beg_region; claim_region < end_region; ++claim_region) {\n-    assert(sd.region(claim_region)->claim_unsafe(), \"claim() failed\");\n-  }\n-#endif  \/\/ #ifdef ASSERT\n-  HeapWord* const space_bottom = space(space_id)->bottom();\n-\n-  \/\/ Check if it's the first region in this space.\n-  if (beg_addr != space_bottom) {\n-    \/\/ Find the first live object or block of dead space that *starts* in this\n-    \/\/ range of regions.  If a partial object crosses onto the region, skip it;\n-    \/\/ it will be marked for 'deferred update' when the object head is\n-    \/\/ processed.  If dead space crosses onto the region, it is also skipped; it\n-    \/\/ will be filled when the prior region is processed.  If neither of those\n-    \/\/ apply, the first word in the region is the start of a live object or dead\n-    \/\/ space.\n-    assert(beg_addr > space(space_id)->bottom(), \"sanity\");\n-    const RegionData* const cp = sd.region(beg_region);\n-    if (cp->partial_obj_size() != 0) {\n-      beg_addr = sd.partial_obj_end(beg_region);\n-    } else {\n-      idx_t beg_bit = mbm->addr_to_bit(beg_addr);\n-      if (!mbm->is_obj_beg(beg_bit) && !mbm->is_obj_end(beg_bit - 1)) {\n-        beg_addr = mbm->find_obj_beg(beg_addr, end_addr);\n-      }\n-    }\n-  }\n-\n-  if (beg_addr < end_addr) {\n-    \/\/ A live object or block of dead space starts in this range of Regions.\n-     HeapWord* const dense_prefix_end = dense_prefix(space_id);\n-\n-    \/\/ Create closures and iterate.\n-    UpdateOnlyClosure update_closure(mbm, cm, space_id);\n-    FillClosure fill_closure(cm, space_id);\n-    mbm->iterate(&update_closure, &fill_closure, beg_addr, end_addr, dense_prefix_end);\n-  }\n-\n-  \/\/ Mark the regions as filled.\n-  RegionData* const beg_cp = sd.region(beg_region);\n-  RegionData* const end_cp = sd.region(end_region);\n-  for (RegionData* cp = beg_cp; cp < end_cp; ++cp) {\n-    cp->set_completed();\n-  }\n-}\n-\n@@ -2208,18 +2200,0 @@\n-void PSParallelCompact::update_deferred_object(ParCompactionManager* cm, HeapWord *addr) {\n-#ifdef ASSERT\n-  ParallelCompactData& sd = summary_data();\n-  size_t region_idx = sd.addr_to_region_idx(addr);\n-  assert(sd.region(region_idx)->completed(), \"first region must be completed before deferred updates\");\n-  assert(sd.region(region_idx + 1)->completed(), \"second region must be completed before deferred updates\");\n-#endif\n-\n-  const SpaceInfo* const space_info = _space_info + space_id(addr);\n-  ObjectStartArray* const start_array = space_info->start_array();\n-  if (start_array != nullptr) {\n-    start_array->update_for_block(addr, addr + cast_to_oop(addr)->size());\n-  }\n-\n-  cm->update_contents(cast_to_oop(addr));\n-  assert(oopDesc::is_oop(cast_to_oop(addr)), \"Expected an oop at \" PTR_FORMAT, p2i(cast_to_oop(addr)));\n-}\n-\n@@ -2237,10 +2211,8 @@\n-  idx_t bits_to_skip = m->words_to_bits(count);\n-  idx_t cur_beg = m->addr_to_bit(beg);\n-  const idx_t search_end = m->align_range_end(m->addr_to_bit(end));\n-\n-  do {\n-    cur_beg = m->find_obj_beg(cur_beg, search_end);\n-    idx_t cur_end = m->find_obj_end(cur_beg, search_end);\n-    const size_t obj_bits = cur_end - cur_beg + 1;\n-    if (obj_bits > bits_to_skip) {\n-      return m->bit_to_addr(cur_beg + bits_to_skip);\n+  HeapWord* cur_addr = beg;\n+  while (true) {\n+    cur_addr = m->find_obj_beg(cur_addr, end);\n+    assert(cur_addr < end, \"inv\");\n+    size_t obj_size = cast_to_oop(cur_addr)->size();\n+    \/\/ Strictly greater-than\n+    if (obj_size > count) {\n+      return cur_addr + count;\n@@ -2248,9 +2220,3 @@\n-    bits_to_skip -= obj_bits;\n-    cur_beg = cur_end + 1;\n-  } while (bits_to_skip > 0);\n-\n-  \/\/ Skipping the desired number of words landed just past the end of an object.\n-  \/\/ Find the start of the next object.\n-  cur_beg = m->find_obj_beg(cur_beg, search_end);\n-  assert(cur_beg < m->addr_to_bit(end), \"not enough live words to skip\");\n-  return m->bit_to_addr(cur_beg);\n+    count -= obj_size;\n+    cur_addr += obj_size;\n+  }\n@@ -2440,0 +2406,23 @@\n+HeapWord* PSParallelCompact::partial_obj_end(HeapWord* region_start_addr) {\n+  ParallelCompactData& sd = summary_data();\n+  assert(sd.is_region_aligned(region_start_addr), \"precondition\");\n+\n+  \/\/ Use per-region partial_obj_size to locate the end of the obj, that extends to region_start_addr.\n+  SplitInfo& split_info = _space_info[space_id(region_start_addr)].split_info();\n+  size_t start_region_idx = sd.addr_to_region_idx(region_start_addr);\n+  size_t end_region_idx = sd.region_count();\n+  size_t accumulated_size = 0;\n+  for (size_t region_idx = start_region_idx; region_idx < end_region_idx; ++region_idx) {\n+    if (split_info.is_split(region_idx)) {\n+      accumulated_size += split_info.partial_obj_size();\n+      break;\n+    }\n+    size_t cur_partial_obj_size = sd.region(region_idx)->partial_obj_size();\n+    accumulated_size += cur_partial_obj_size;\n+    if (cur_partial_obj_size != ParallelCompactData::RegionSize) {\n+      break;\n+    }\n+  }\n+  return region_start_addr + accumulated_size;\n+}\n+\n@@ -2442,1 +2431,0 @@\n-  typedef ParMarkBitMap::IterationStatus IterationStatus;\n@@ -2466,1 +2454,24 @@\n-    closure.copy_partial_obj();\n+    {\n+      HeapWord* region_start = sd.region_align_down(closure.source());\n+      HeapWord* obj_start = bitmap->find_obj_beg_reverse(region_start, closure.source());\n+      HeapWord* obj_end;\n+      if (bitmap->is_marked(obj_start)) {\n+        HeapWord* next_region_start = region_start + ParallelCompactData::RegionSize;\n+        HeapWord* partial_obj_start = (next_region_start >= src_space_top)\n+                                      ? nullptr\n+                                      : sd.addr_to_region_ptr(next_region_start)->partial_obj_addr();\n+        if (partial_obj_start == obj_start) {\n+          \/\/ This obj extends to next region.\n+          obj_end = partial_obj_end(next_region_start);\n+        } else {\n+          \/\/ Completely contained in this region; safe to use size().\n+          obj_end = obj_start + cast_to_oop(obj_start)->size();\n+        }\n+      } else {\n+        \/\/ This obj extends to current region.\n+        obj_end = partial_obj_end(region_start);\n+      }\n+      size_t partial_obj_size = pointer_delta(obj_end, closure.source());\n+      closure.copy_partial_obj(partial_obj_size);\n+    }\n+\n@@ -2487,1 +2498,1 @@\n-    HeapWord* const cur_addr = closure.source();\n+    HeapWord* cur_addr = closure.source();\n@@ -2490,13 +2501,19 @@\n-    IterationStatus status = bitmap->iterate(&closure, cur_addr, end_addr);\n-\n-    if (status == ParMarkBitMap::would_overflow) {\n-      \/\/ The last object did not fit.  Note that interior oop updates were\n-      \/\/ deferred, then copy enough of the object to fill the region.\n-      cm->push_deferred_object(closure.destination());\n-      status = closure.copy_until_full(); \/\/ copies from closure.source()\n-\n-      decrement_destination_counts(cm, src_space_id, src_region_idx,\n-                                   closure.source());\n-      closure.complete_region(cm, dest_addr, region_ptr);\n-      return;\n-    }\n+    HeapWord* partial_obj_start = (end_addr == src_space_top)\n+                                ? nullptr\n+                                : sd.addr_to_region_ptr(end_addr)->partial_obj_addr();\n+    \/\/ apply closure on objs inside [cur_addr, end_addr)\n+    do {\n+      cur_addr = bitmap->find_obj_beg(cur_addr, end_addr);\n+      if (cur_addr == end_addr) {\n+        break;\n+      }\n+      size_t obj_size;\n+      if (partial_obj_start == cur_addr) {\n+        obj_size = pointer_delta(partial_obj_end(end_addr), cur_addr);\n+      } else {\n+        \/\/ This obj doesn't extend into next region; size() is safe to use.\n+        obj_size = cast_to_oop(cur_addr)->size();\n+      }\n+      closure.do_addr(cur_addr, obj_size);\n+      cur_addr += obj_size;\n+    } while (cur_addr < end_addr && !closure.is_full());\n@@ -2504,1 +2521,1 @@\n-    if (status == ParMarkBitMap::full) {\n+    if (closure.is_full()) {\n@@ -2604,1 +2621,1 @@\n-void PSParallelCompact::fill_blocks(size_t region_idx)\n+void MoveAndUpdateClosure::copy_partial_obj(size_t partial_obj_size)\n@@ -2606,69 +2623,1 @@\n-  \/\/ Fill in the block table elements for the specified region.  Each block\n-  \/\/ table element holds the number of live words in the region that are to the\n-  \/\/ left of the first object that starts in the block.  Thus only blocks in\n-  \/\/ which an object starts need to be filled.\n-  \/\/\n-  \/\/ The algorithm scans the section of the bitmap that corresponds to the\n-  \/\/ region, keeping a running total of the live words.  When an object start is\n-  \/\/ found, if it's the first to start in the block that contains it, the\n-  \/\/ current total is written to the block table element.\n-  const size_t Log2BlockSize = ParallelCompactData::Log2BlockSize;\n-  const size_t Log2RegionSize = ParallelCompactData::Log2RegionSize;\n-  const size_t RegionSize = ParallelCompactData::RegionSize;\n-\n-  ParallelCompactData& sd = summary_data();\n-  const size_t partial_obj_size = sd.region(region_idx)->partial_obj_size();\n-  if (partial_obj_size >= RegionSize) {\n-    return; \/\/ No objects start in this region.\n-  }\n-\n-  \/\/ Ensure the first loop iteration decides that the block has changed.\n-  size_t cur_block = sd.block_count();\n-\n-  const ParMarkBitMap* const bitmap = mark_bitmap();\n-\n-  const size_t Log2BitsPerBlock = Log2BlockSize - LogMinObjAlignment;\n-  assert((size_t)1 << Log2BitsPerBlock ==\n-         bitmap->words_to_bits(ParallelCompactData::BlockSize), \"sanity\");\n-\n-  size_t beg_bit = bitmap->words_to_bits(region_idx << Log2RegionSize);\n-  const size_t range_end = beg_bit + bitmap->words_to_bits(RegionSize);\n-  size_t live_bits = bitmap->words_to_bits(partial_obj_size);\n-  beg_bit = bitmap->find_obj_beg(beg_bit + live_bits, range_end);\n-  while (beg_bit < range_end) {\n-    const size_t new_block = beg_bit >> Log2BitsPerBlock;\n-    if (new_block != cur_block) {\n-      cur_block = new_block;\n-      sd.block(cur_block)->set_offset(bitmap->bits_to_words(live_bits));\n-    }\n-\n-    const size_t end_bit = bitmap->find_obj_end(beg_bit, range_end);\n-    if (end_bit < range_end - 1) {\n-      live_bits += end_bit - beg_bit + 1;\n-      beg_bit = bitmap->find_obj_beg(end_bit + 1, range_end);\n-    } else {\n-      return;\n-    }\n-  }\n-}\n-\n-ParMarkBitMap::IterationStatus MoveAndUpdateClosure::copy_until_full()\n-{\n-  if (source() != copy_destination()) {\n-    DEBUG_ONLY(PSParallelCompact::check_new_location(source(), destination());)\n-    Copy::aligned_conjoint_words(source(), copy_destination(), words_remaining());\n-  }\n-  update_state(words_remaining());\n-  assert(is_full(), \"sanity\");\n-  return ParMarkBitMap::full;\n-}\n-\n-void MoveAndUpdateClosure::copy_partial_obj()\n-{\n-  size_t words = words_remaining();\n-\n-  HeapWord* const range_end = MIN2(source() + words, bitmap()->region_end());\n-  HeapWord* const end_addr = bitmap()->find_obj_end(source(), range_end);\n-  if (end_addr < range_end) {\n-    words = bitmap()->obj_size(source(), end_addr);\n-  }\n+  size_t words = MIN2(partial_obj_size, words_remaining());\n@@ -2694,2 +2643,0 @@\n-  assert(bitmap()->obj_size(addr) == words, \"bad size\");\n-\n@@ -2697,6 +2644,0 @@\n-  assert(PSParallelCompact::summary_data().calc_new_pointer(source(), compaction_manager()) ==\n-         destination(), \"wrong destination\");\n-\n-  if (words > words_remaining()) {\n-    return ParMarkBitMap::would_overflow;\n-  }\n@@ -2709,0 +2650,4 @@\n+  \/\/ Avoid overflow\n+  words = MIN2(words, words_remaining());\n+  assert(words > 0, \"inv\");\n+\n@@ -2711,0 +2656,3 @@\n+    assert(source() != destination(), \"inv\");\n+    assert(cast_to_oop(source())->is_forwarded(), \"inv\");\n+    assert(cast_to_oop(source())->forwardee() == cast_to_oop(destination()), \"inv\");\n@@ -2712,0 +2660,1 @@\n+    cast_to_oop(copy_destination())->init_mark();\n@@ -2714,4 +2663,0 @@\n-  oop moved_oop = cast_to_oop(copy_destination());\n-  compaction_manager()->update_contents(moved_oop);\n-  assert(oopDesc::is_oop_or_null(moved_oop), \"Expected an oop or null at \" PTR_FORMAT, p2i(moved_oop));\n-\n@@ -2719,1 +2664,0 @@\n-  assert(copy_destination() == cast_from_oop<HeapWord*>(moved_oop) + moved_oop->size(), \"sanity\");\n@@ -2742,34 +2686,0 @@\n-UpdateOnlyClosure::UpdateOnlyClosure(ParMarkBitMap* mbm,\n-                                     ParCompactionManager* cm,\n-                                     PSParallelCompact::SpaceId space_id) :\n-  ParMarkBitMapClosure(mbm, cm),\n-  _start_array(PSParallelCompact::start_array(space_id))\n-{\n-}\n-\n-\/\/ Updates the references in the object to their new values.\n-ParMarkBitMapClosure::IterationStatus\n-UpdateOnlyClosure::do_addr(HeapWord* addr, size_t words) {\n-  do_addr(addr);\n-  return ParMarkBitMap::incomplete;\n-}\n-\n-FillClosure::FillClosure(ParCompactionManager* cm, PSParallelCompact::SpaceId space_id) :\n-  ParMarkBitMapClosure(PSParallelCompact::mark_bitmap(), cm),\n-  _start_array(PSParallelCompact::start_array(space_id))\n-{\n-  assert(space_id == PSParallelCompact::old_space_id,\n-         \"cannot use FillClosure in the young gen\");\n-}\n-\n-ParMarkBitMapClosure::IterationStatus\n-FillClosure::do_addr(HeapWord* addr, size_t size) {\n-  CollectedHeap::fill_with_objects(addr, size);\n-  HeapWord* const end = addr + size;\n-  do {\n-    size_t size = cast_to_oop(addr)->size();\n-    _start_array->update_for_block(addr, addr + size);\n-    addr += size;\n-  } while (addr < end);\n-  return ParMarkBitMap::incomplete;\n-}\n","filename":"src\/hotspot\/share\/gc\/parallel\/psParallelCompact.cpp","additions":442,"deletions":532,"binary":false,"changes":974,"status":"modified"},{"patch":"@@ -218,11 +218,0 @@\n-  static const size_t Log2BlockSize;\n-  static const size_t BlockSize;\n-  static const size_t BlockSizeBytes;\n-\n-  static const size_t BlockSizeOffsetMask;\n-  static const size_t BlockAddrOffsetMask;\n-  static const size_t BlockAddrMask;\n-\n-  static const size_t BlocksPerRegion;\n-  static const size_t Log2BlocksPerRegion;\n-\n@@ -277,6 +266,0 @@\n-    \/\/ Whether the block table for this region has been filled.\n-    inline bool blocks_filled() const;\n-\n-    \/\/ Number of times the block table was filled.\n-    DEBUG_ONLY(inline size_t blocks_filled_count() const;)\n-\n@@ -301,1 +284,0 @@\n-    inline void set_blocks_filled();\n@@ -359,1 +341,0 @@\n-    bool        volatile _blocks_filled;\n@@ -362,4 +343,0 @@\n-#ifdef ASSERT\n-    size_t               _blocks_filled_count;   \/\/ Number of block table fills.\n-#endif  \/\/ #ifdef ASSERT\n-\n@@ -373,15 +350,0 @@\n-  \/\/ \"Blocks\" allow shorter sections of the bitmap to be searched.  Each Block\n-  \/\/ holds an offset, which is the amount of live data in the Region to the left\n-  \/\/ of the first live object that starts in the Block.\n-  class BlockData\n-  {\n-  public:\n-    typedef unsigned short int blk_ofs_t;\n-\n-    blk_ofs_t offset() const    { return _offset; }\n-    void set_offset(size_t val) { _offset = (blk_ofs_t)val; }\n-\n-  private:\n-    blk_ofs_t _offset;\n-  };\n-\n@@ -399,3 +361,0 @@\n-  size_t block_count() const { return _block_count; }\n-  inline BlockData* block(size_t block_idx) const;\n-\n@@ -439,15 +398,0 @@\n-  size_t     addr_to_block_idx(const HeapWord* addr) const;\n-  inline BlockData* addr_to_block_ptr(const HeapWord* addr) const;\n-\n-  inline HeapWord*  block_align_down(HeapWord* addr) const;\n-\n-  \/\/ Return the address one past the end of the partial object.\n-  HeapWord* partial_obj_end(size_t region_idx) const;\n-\n-  \/\/ Return the location of the object after compaction.\n-  HeapWord* calc_new_pointer(HeapWord* addr, ParCompactionManager* cm) const;\n-\n-  HeapWord* calc_new_pointer(oop p, ParCompactionManager* cm) const {\n-    return calc_new_pointer(cast_from_oop<HeapWord*>(p), cm);\n-  }\n-\n@@ -460,1 +404,0 @@\n-  bool initialize_block_data();\n@@ -473,4 +416,0 @@\n-\n-  PSVirtualSpace* _block_vspace;\n-  BlockData*      _block_data;\n-  size_t          _block_count;\n@@ -491,25 +430,0 @@\n-inline bool\n-ParallelCompactData::RegionData::blocks_filled() const\n-{\n-  bool result = _blocks_filled;\n-  OrderAccess::acquire();\n-  return result;\n-}\n-\n-#ifdef ASSERT\n-inline size_t\n-ParallelCompactData::RegionData::blocks_filled_count() const\n-{\n-  return _blocks_filled_count;\n-}\n-#endif \/\/ #ifdef ASSERT\n-\n-inline void\n-ParallelCompactData::RegionData::set_blocks_filled()\n-{\n-  OrderAccess::release();\n-  _blocks_filled = true;\n-  \/\/ Debug builds count the number of times the table was filled.\n-  DEBUG_ONLY(Atomic::inc(&_blocks_filled_count));\n-}\n-\n@@ -605,6 +519,0 @@\n-inline ParallelCompactData::BlockData*\n-ParallelCompactData::block(size_t n) const {\n-  assert(n < block_count(), \"bad arg\");\n-  return _block_data + n;\n-}\n-\n@@ -670,22 +578,0 @@\n-inline size_t\n-ParallelCompactData::addr_to_block_idx(const HeapWord* addr) const\n-{\n-  assert(addr >= _heap_start, \"bad addr\");\n-  assert(addr <= _heap_end, \"bad addr\");\n-  return pointer_delta(addr, _heap_start) >> Log2BlockSize;\n-}\n-\n-inline ParallelCompactData::BlockData*\n-ParallelCompactData::addr_to_block_ptr(const HeapWord* addr) const\n-{\n-  return block(addr_to_block_idx(addr));\n-}\n-\n-inline HeapWord*\n-ParallelCompactData::block_align_down(HeapWord* addr) const\n-{\n-  assert(addr >= _heap_start, \"bad addr\");\n-  assert(addr < _heap_end + RegionSize, \"bad addr\");\n-  return (HeapWord*)(size_t(addr) & BlockAddrMask);\n-}\n-\n@@ -872,2 +758,0 @@\n-class TaskQueue;\n-\n@@ -879,1 +763,0 @@\n-  typedef ParallelCompactData::BlockData BlockData;\n@@ -886,19 +769,1 @@\n-  struct UpdateDensePrefixTask : public CHeapObj<mtGC> {\n-    SpaceId _space_id;\n-    size_t _region_index_start;\n-    size_t _region_index_end;\n-\n-    UpdateDensePrefixTask() :\n-        _space_id(SpaceId(0)),\n-        _region_index_start(0),\n-        _region_index_end(0) {}\n-\n-    UpdateDensePrefixTask(SpaceId space_id,\n-                          size_t region_index_start,\n-                          size_t region_index_end) :\n-        _space_id(space_id),\n-        _region_index_start(region_index_start),\n-        _region_index_end(region_index_end) {}\n-  };\n-\n- public:\n+public:\n@@ -912,1 +777,0 @@\n-  friend class RefProcTaskProxy;\n@@ -961,2 +825,2 @@\n-  \/\/ Adjust addresses in roots.  Does not adjust addresses in heap.\n-  static void adjust_roots();\n+  static void adjust_pointers();\n+  static void forward_to_new_addr();\n@@ -964,1 +828,2 @@\n-  DEBUG_ONLY(static void write_block_fill_histogram();)\n+  static void verify_forward() NOT_DEBUG_RETURN;\n+  static void verify_filler_in_dense_prefix() NOT_DEBUG_RETURN;\n@@ -972,4 +837,0 @@\n-  \/\/ Add dense prefix update tasks to the task queue.\n-  static void enqueue_dense_prefix_tasks(TaskQueue& task_queue,\n-                                         uint parallel_gc_threads);\n-\n@@ -983,0 +844,2 @@\n+  static void fill_range_in_dense_prefix(HeapWord* start, HeapWord* end);\n+\n@@ -984,0 +847,2 @@\n+  static void fill_dead_objs_in_dense_prefix(uint worker_id, uint num_workers);\n+\n@@ -987,0 +852,9 @@\n+  template<typename Func>\n+  static void adjust_in_space_helper(SpaceId id, volatile uint* claim_counter, Func&& on_stripe);\n+\n+  static void adjust_in_old_space(volatile uint* claim_counter);\n+\n+  static void adjust_in_young_space(SpaceId id, volatile uint* claim_counter);\n+\n+  static void adjust_pointers_in_spaces(uint worker_id, volatile uint* claim_counter);\n+\n@@ -1006,1 +880,1 @@\n-  template <class T> static inline void adjust_pointer(T* p, ParCompactionManager* cm);\n+  template <class T> static inline void adjust_pointer(T* p);\n@@ -1019,13 +893,0 @@\n-  \/\/ Update a region in the dense prefix.  For each live object\n-  \/\/ in the region, update it's interior references.  For each\n-  \/\/ dead object, fill it with deadwood. Dead space at the end\n-  \/\/ of a region range will be filled to the start of the next\n-  \/\/ live object regardless of the region_index_end.  None of the\n-  \/\/ objects in the dense prefix move and dead space is dead\n-  \/\/ (holds only dead objects that don't need any processing), so\n-  \/\/ dead space can be filled in any order.\n-  static void update_and_deadwood_in_dense_prefix(ParCompactionManager* cm,\n-                                                  SpaceId space_id,\n-                                                  size_t region_index_start,\n-                                                  size_t region_index_end);\n-\n@@ -1059,0 +920,2 @@\n+  static HeapWord* partial_obj_end(HeapWord* region_start_addr);\n+\n@@ -1070,6 +933,0 @@\n-  \/\/ Fill in the block table for the specified region.\n-  static void fill_blocks(size_t region_idx);\n-\n-  \/\/ Update a single deferred object.\n-  static void update_deferred_object(ParCompactionManager* cm, HeapWord* addr);\n-\n@@ -1123,4 +980,0 @@\n-  \/\/ Copy enough words to fill this closure, starting at source().  Interior\n-  \/\/ oops and the start array are not updated.  Return full.\n-  IterationStatus copy_until_full();\n-\n@@ -1128,3 +981,3 @@\n-  \/\/ whichever is smaller, starting at source().  Interior oops and the start\n-  \/\/ array are not updated.\n-  void copy_partial_obj();\n+  \/\/ whichever is smaller, starting at source(). The start array is not\n+  \/\/ updated.\n+  void copy_partial_obj(size_t partial_obj_size);\n@@ -1201,25 +1054,0 @@\n-class UpdateOnlyClosure: public ParMarkBitMapClosure {\n- private:\n-  ObjectStartArray* const          _start_array;\n-\n- public:\n-  UpdateOnlyClosure(ParMarkBitMap* mbm,\n-                    ParCompactionManager* cm,\n-                    PSParallelCompact::SpaceId space_id);\n-\n-  \/\/ Update the object.\n-  virtual IterationStatus do_addr(HeapWord* addr, size_t words);\n-\n-  inline void do_addr(HeapWord* addr);\n-};\n-\n-class FillClosure: public ParMarkBitMapClosure {\n- public:\n-  FillClosure(ParCompactionManager* cm, PSParallelCompact::SpaceId space_id);\n-\n-  virtual IterationStatus do_addr(HeapWord* addr, size_t size);\n-\n- private:\n-  ObjectStartArray* const _start_array;\n-};\n-\n","filename":"src\/hotspot\/share\/gc\/parallel\/psParallelCompact.hpp","additions":24,"deletions":196,"binary":false,"changes":220,"status":"modified"},{"patch":"@@ -81,2 +81,1 @@\n-  const size_t obj_size = obj->size();\n-  if (mark_bitmap()->mark_obj(obj, obj_size)) {\n+  if (mark_bitmap()->mark_obj(obj)) {\n@@ -91,1 +90,1 @@\n-inline void PSParallelCompact::adjust_pointer(T* p, ParCompactionManager* cm) {\n+inline void PSParallelCompact::adjust_pointer(T* p) {\n@@ -97,7 +96,2 @@\n-    oop new_obj = cast_to_oop(summary_data().calc_new_pointer(obj, cm));\n-    assert(new_obj != nullptr, \"non-null address for live objects\");\n-    \/\/ Is it actually relocated at all?\n-    if (new_obj != obj) {\n-      assert(ParallelScavengeHeap::heap()->is_in_reserved(new_obj),\n-             \"should be in object space\");\n-      RawAccess<IS_NOT_NULL>::oop_store(p, new_obj);\n+    if (!obj->is_forwarded()) {\n+      return;\n@@ -105,0 +99,6 @@\n+    oop new_obj = obj->forwardee();\n+    assert(new_obj != nullptr, \"non-null address for live objects\");\n+    assert(new_obj != obj, \"inv\");\n+    assert(ParallelScavengeHeap::heap()->is_in_reserved(new_obj),\n+           \"should be in object space\");\n+    RawAccess<IS_NOT_NULL>::oop_store(p, new_obj);\n@@ -108,13 +108,0 @@\n-class PCAdjustPointerClosure: public BasicOopIterateClosure {\n-public:\n-  PCAdjustPointerClosure(ParCompactionManager* cm) : _cm(cm) {\n-  }\n-  template <typename T> void do_oop_work(T* p) { PSParallelCompact::adjust_pointer(p, _cm); }\n-  virtual void do_oop(oop* p)                { do_oop_work(p); }\n-  virtual void do_oop(narrowOop* p)          { do_oop_work(p); }\n-\n-  virtual ReferenceIterationMode reference_iteration_mode() { return DO_FIELDS; }\n-private:\n-  ParCompactionManager* _cm;\n-};\n-\n","filename":"src\/hotspot\/share\/gc\/parallel\/psParallelCompact.inline.hpp","additions":10,"deletions":23,"binary":false,"changes":33,"status":"modified"}]}