{"files":[{"patch":"@@ -35,0 +35,19 @@\n+class ThreadPriorityAdjuster : public StackObj {\n+private:\n+  Thread* const _thread;\n+  ThreadPriority const _new_prio;\n+  ThreadPriority _old_prio;\n+public:\n+  ThreadPriorityAdjuster(ThreadPriority new_prio) : _thread(Thread::current()), _new_prio(new_prio) {\n+    os::get_priority(_thread, _old_prio);\n+    if (_old_prio != _new_prio) {\n+      os::set_priority(_thread, new_prio);\n+    }\n+  }\n+  ~ThreadPriorityAdjuster() {\n+    if (_old_prio != _new_prio) {\n+      os::set_priority(_thread, _old_prio);\n+    }\n+  }\n+};\n+\n@@ -43,0 +62,4 @@\n+  bool use_caller = task->caller_can_run();\n+  bool use_workers = !use_caller || (num_workers > 1);\n+  uint num_worker_tasks = use_caller ? (num_workers - 1) : num_workers;\n+\n@@ -45,1 +68,11 @@\n-  _not_finished = num_workers;\n+  Atomic::store(&_not_finished, num_worker_tasks);\n+\n+  if (use_workers) {\n+    if (use_caller) {\n+      \/\/ Claim worker_id = 0 for caller.\n+      Atomic::inc(&_started);\n+    }\n+\n+    \/\/ Dispatch 'num_worker_tasks' number of tasks.\n+    _start_semaphore.signal(num_worker_tasks);\n+  }\n@@ -47,2 +80,4 @@\n-  \/\/ Dispatch 'num_workers' number of tasks.\n-  _start_semaphore.signal(num_workers);\n+  if (use_caller) {\n+    \/\/ Execute task in caller.\n+    caller_run_task();\n+  }\n@@ -50,2 +85,4 @@\n-  \/\/ Wait for the last worker to signal the coordinator.\n-  _end_semaphore.wait();\n+  if (use_workers) {\n+    \/\/ Wait for the last worker to signal the coordinator.\n+    _end_semaphore.wait();\n+  }\n@@ -59,0 +96,12 @@\n+void WorkerTaskDispatcher::caller_run_task() {\n+  \/\/ Execute the task in the same context and with the same priority\n+  \/\/ it would be executed by a worker.\n+  ThreadPriorityAdjuster tp(NearMaxPriority);\n+  if (Thread::current()->is_Named_thread()) {\n+    GCIdMark gc_id_mark(_task->gc_id());\n+    _task->work(0);\n+  } else {\n+    _task->work(0);\n+  }\n+}\n+\n","filename":"src\/hotspot\/share\/gc\/shared\/workerThread.cpp","additions":54,"deletions":5,"binary":false,"changes":59,"status":"modified"},{"patch":"@@ -44,0 +44,1 @@\n+  const bool _caller_can_run;\n@@ -45,2 +46,2 @@\n- public:\n-  explicit WorkerTask(const char* name) :\n+public:\n+  explicit WorkerTask(const char* name, bool caller_can_run = false) :\n@@ -48,1 +49,2 @@\n-    _gc_id(GCId::current_or_undefined()) {}\n+    _gc_id(GCId::current_or_undefined()),\n+    _caller_can_run(caller_can_run) {}\n@@ -52,0 +54,1 @@\n+  bool caller_can_run() { return _caller_can_run; }\n@@ -69,0 +72,3 @@\n+  \/\/ Runs task in caller.\n+  void caller_run_task();\n+\n","filename":"src\/hotspot\/share\/gc\/shared\/workerThread.hpp","additions":9,"deletions":3,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -0,0 +1,159 @@\n+\/*\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"gc\/shared\/workerThread.hpp\"\n+#include \"memory\/universe.hpp\"\n+#include \"utilities\/spinYield.hpp\"\n+#include \"unittest.hpp\"\n+\n+\n+class ParallelTask : public WorkerTask {\n+protected:\n+  const uint _expected_workers;\n+  volatile uint _actual_workers;\n+  volatile uint _actual_ids_bitset;\n+  const Thread* _caller_thread;\n+  bool _seen_caller;\n+\n+public:\n+  ParallelTask(int expected_workers, bool can_caller_execute) :\n+    WorkerTask(\"Parallel Task\", can_caller_execute),\n+    _expected_workers(expected_workers),\n+    _actual_workers(0),\n+    _actual_ids_bitset(0),\n+    _caller_thread(Thread::current()),\n+    _seen_caller(false)\n+    {};\n+\n+  void record_worker(uint worker_id) {\n+    if (!_seen_caller && Thread::current() == _caller_thread) {\n+      _seen_caller = true;\n+    }\n+    while (true) {\n+      uint cur_ids = Atomic::load(&_actual_ids_bitset);\n+      uint new_ids = cur_ids | (1 << worker_id);\n+      if (cur_ids == new_ids) {\n+        return;\n+      }\n+      if (Atomic::cmpxchg(&_actual_ids_bitset, cur_ids, new_ids) == cur_ids) {\n+        return;\n+      }\n+    }\n+  }\n+\n+  void work(uint worker_id) {\n+    record_worker(worker_id);\n+\n+    Atomic::inc(&_actual_workers);\n+    SpinYield sp;\n+    while (Atomic::load(&_actual_workers) < _expected_workers) {\n+      sp.wait();\n+    }\n+  }\n+\n+  uint actual_workers() {\n+    return Atomic::load(&_actual_workers);\n+  }\n+\n+  uint actual_ids_bitset() {\n+    return Atomic::load(&_actual_ids_bitset);\n+  }\n+\n+  bool seen_caller() {\n+    return _seen_caller;\n+  }\n+};\n+\n+static uint expected_ids_bitset(int expected_workers) {\n+  return (1 << expected_workers) - 1;\n+}\n+\n+TEST_VM(WorkerThreads, basic) {\n+  static const int TRIES = 1000;\n+  static const uint max_workers = 4;\n+  static const uint half_workers = max_workers \/ 2;\n+\n+  WorkerThreads* workers = new WorkerThreads(\"test\", max_workers);\n+  workers->initialize_workers();\n+\n+  \/\/ Full parallelism\n+  workers->set_active_workers(max_workers);\n+  for (int t = 0; t < TRIES; t++) {\n+    ParallelTask task(max_workers, false);\n+    workers->run_task(&task);\n+    EXPECT_EQ(max_workers, task.actual_workers());\n+    EXPECT_EQ(expected_ids_bitset(max_workers), task.actual_ids_bitset());\n+    EXPECT_FALSE(task.seen_caller());\n+  }\n+\n+  \/\/ Full parallelism, can execute in caller\n+  workers->set_active_workers(max_workers);\n+  for (int t = 0; t < TRIES; t++) {\n+    ParallelTask task(max_workers, true);\n+    workers->run_task(&task);\n+    EXPECT_EQ(max_workers, task.actual_workers());\n+    EXPECT_EQ(expected_ids_bitset(max_workers), task.actual_ids_bitset());\n+    EXPECT_TRUE(task.seen_caller());\n+  }\n+\n+  \/\/ Half parallelism\n+  workers->set_active_workers(half_workers);\n+  for (int t = 0; t < TRIES; t++) {\n+    ParallelTask task(half_workers, false);\n+    workers->run_task(&task);\n+    EXPECT_EQ(half_workers, task.actual_workers());\n+    EXPECT_EQ(expected_ids_bitset(half_workers), task.actual_ids_bitset());\n+    EXPECT_FALSE(task.seen_caller());\n+  }\n+\n+  \/\/ Half parallelism, can execute in caller\n+  workers->set_active_workers(half_workers);\n+  for (int t = 0; t < TRIES; t++) {\n+    ParallelTask task(half_workers, true);\n+    workers->run_task(&task);\n+    EXPECT_EQ(half_workers, task.actual_workers());\n+    EXPECT_EQ(expected_ids_bitset(half_workers), task.actual_ids_bitset());\n+    EXPECT_TRUE(task.seen_caller());\n+  }\n+\n+  \/\/ Lowest parallelism\n+  workers->set_active_workers(1);\n+  for (int t = 0; t < TRIES; t++) {\n+    ParallelTask task(1, false);\n+    workers->run_task(&task);\n+    EXPECT_EQ(1u, task.actual_workers());\n+    EXPECT_EQ(expected_ids_bitset(1), task.actual_ids_bitset());\n+    EXPECT_FALSE(task.seen_caller());\n+  }\n+\n+  \/\/ Lowest parallelism, can execute in caller\n+  workers->set_active_workers(1);\n+  for (int t = 0; t < TRIES; t++) {\n+    ParallelTask task(1, true);\n+    workers->run_task(&task);\n+    EXPECT_EQ(1u, task.actual_workers());\n+    EXPECT_EQ(expected_ids_bitset(1), task.actual_ids_bitset());\n+    EXPECT_TRUE(task.seen_caller());\n+  }\n+}\n","filename":"test\/hotspot\/gtest\/gc\/shared\/test_workerThreads.cpp","additions":159,"deletions":0,"binary":false,"changes":159,"status":"added"}]}