{"files":[{"patch":"@@ -2326,0 +2326,1 @@\n+  INSN(vandn_vx,   0b1010111, 0b100, 0b000001);\n","filename":"src\/hotspot\/cpu\/riscv\/assembler_riscv.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -416,1 +416,1 @@\n-instruct vadd_immI(vReg dst, vReg src1, immI5 con) %{\n+instruct vaddI_vi(vReg dst, vReg src1, immI5 con) %{\n@@ -420,1 +420,1 @@\n-  format %{ \"vadd_immI $dst, $src1, $con\" %}\n+  format %{ \"vaddI_vi $dst, $src1, $con\" %}\n@@ -431,1 +431,1 @@\n-instruct vadd_immL(vReg dst, vReg src1, immL5 con) %{\n+instruct vaddL_vi(vReg dst, vReg src1, immL5 con) %{\n@@ -433,1 +433,1 @@\n-  format %{ \"vadd_immL $dst, $src1, $con\" %}\n+  format %{ \"vaddL_vi $dst, $src1, $con\" %}\n@@ -445,1 +445,1 @@\n-instruct vadd_regI(vReg dst, vReg src1, iRegIorL2I src2) %{\n+instruct vaddI_vx(vReg dst, vReg src1, iRegIorL2I src2) %{\n@@ -449,1 +449,1 @@\n-  format %{ \"vadd_regI $dst, $src1, $src2\" %}\n+  format %{ \"vaddI_vx $dst, $src1, $src2\" %}\n@@ -460,1 +460,1 @@\n-instruct vadd_regL(vReg dst, vReg src1, iRegL src2) %{\n+instruct vaddL_vx(vReg dst, vReg src1, iRegL src2) %{\n@@ -462,1 +462,1 @@\n-  format %{ \"vadd_regL $dst, $src1, $src2\" %}\n+  format %{ \"vaddL_vx $dst, $src1, $src2\" %}\n@@ -474,1 +474,1 @@\n-instruct vadd_immI_masked(vReg dst_src, immI5 con, vRegMask_V0 v0) %{\n+instruct vaddI_vi_masked(vReg dst_src, immI5 con, vRegMask_V0 v0) %{\n@@ -478,1 +478,1 @@\n-  format %{ \"vadd_immI_masked $dst_src, $dst_src, $con\" %}\n+  format %{ \"vaddI_vi_masked $dst_src, $dst_src, $con, $v0\" %}\n@@ -489,1 +489,1 @@\n-instruct vadd_immL_masked(vReg dst_src, immL5 con, vRegMask_V0 v0) %{\n+instruct vaddL_vi_masked(vReg dst_src, immL5 con, vRegMask_V0 v0) %{\n@@ -491,1 +491,1 @@\n-  format %{ \"vadd_immL_masked $dst_src, $dst_src, $con\" %}\n+  format %{ \"vaddL_vi_masked $dst_src, $dst_src, $con, $v0\" %}\n@@ -503,1 +503,1 @@\n-instruct vadd_regI_masked(vReg dst_src, iRegIorL2I src2, vRegMask_V0 v0) %{\n+instruct vaddI_vx_masked(vReg dst_src, iRegIorL2I src2, vRegMask_V0 v0) %{\n@@ -507,1 +507,1 @@\n-  format %{ \"vadd_regI_masked $dst_src, $dst_src, $src2\" %}\n+  format %{ \"vaddI_vx_masked $dst_src, $dst_src, $src2, $v0\" %}\n@@ -518,1 +518,1 @@\n-instruct vadd_regL_masked(vReg dst_src, iRegL src2, vRegMask_V0 v0) %{\n+instruct vaddL_vx_masked(vReg dst_src, iRegL src2, vRegMask_V0 v0) %{\n@@ -520,1 +520,1 @@\n-  format %{ \"vadd_regL_masked $dst_src, $dst_src, $src2\" %}\n+  format %{ \"vaddL_vx_masked $dst_src, $dst_src, $src2, $v0\" %}\n@@ -596,1 +596,1 @@\n-instruct vsub_regI(vReg dst, vReg src1, iRegIorL2I src2) %{\n+instruct vsubI_vx(vReg dst, vReg src1, iRegIorL2I src2) %{\n@@ -600,1 +600,1 @@\n-  format %{ \"vsub_regI $dst, $src1, $src2\" %}\n+  format %{ \"vsubI_vx $dst, $src1, $src2\" %}\n@@ -611,1 +611,1 @@\n-instruct vsub_regL(vReg dst, vReg src1, iRegL src2) %{\n+instruct vsubL_vx(vReg dst, vReg src1, iRegL src2) %{\n@@ -613,1 +613,1 @@\n-  format %{ \"vsub_regL $dst, $src1, $src2\" %}\n+  format %{ \"vsubL_vx $dst, $src1, $src2\" %}\n@@ -625,1 +625,1 @@\n-instruct vsub_regI_masked(vReg dst_src, iRegIorL2I src2, vRegMask_V0 v0) %{\n+instruct vsubI_vx_masked(vReg dst_src, iRegIorL2I src2, vRegMask_V0 v0) %{\n@@ -629,1 +629,1 @@\n-  format %{ \"vsub_regI_masked $dst_src, $dst_src, $src2\" %}\n+  format %{ \"vsubI_vx_masked $dst_src, $dst_src, $src2, $v0\" %}\n@@ -640,1 +640,1 @@\n-instruct vsub_regL_masked(vReg dst_src, iRegL src2, vRegMask_V0 v0) %{\n+instruct vsubL_vx_masked(vReg dst_src, iRegL src2, vRegMask_V0 v0) %{\n@@ -642,1 +642,1 @@\n-  format %{ \"vsub_regL_masked $dst_src, $dst_src, $src2\" %}\n+  format %{ \"vsub_vx_masked $dst_src, $dst_src, $src2, $v0\" %}\n@@ -686,1 +686,1 @@\n-instruct vand_immI(vReg dst_src, immI5 con) %{\n+instruct vandI_vi(vReg dst_src, immI5 con) %{\n@@ -691,1 +691,1 @@\n-  format %{ \"vand_immI $dst_src, $dst_src, $con\" %}\n+  format %{ \"vandI_vi $dst_src, $dst_src, $con\" %}\n@@ -702,1 +702,1 @@\n-instruct vand_immL(vReg dst_src, immL5 con) %{\n+instruct vandL_vi(vReg dst_src, immL5 con) %{\n@@ -705,1 +705,1 @@\n-  format %{ \"vand_immL $dst_src, $dst_src, $con\" %}\n+  format %{ \"vandL_vi $dst_src, $dst_src, $con\" %}\n@@ -717,1 +717,1 @@\n-instruct vand_regI(vReg dst_src, iRegIorL2I src) %{\n+instruct vandI_vx(vReg dst_src, iRegIorL2I src) %{\n@@ -722,1 +722,1 @@\n-  format %{ \"vand_regI $dst_src, $dst_src, $src\" %}\n+  format %{ \"vandI_vx $dst_src, $dst_src, $src\" %}\n@@ -733,1 +733,1 @@\n-instruct vand_regL(vReg dst_src, iRegL src) %{\n+instruct vandL_vx(vReg dst_src, iRegL src) %{\n@@ -736,1 +736,1 @@\n-  format %{ \"vand_regL $dst_src, $dst_src, $src\" %}\n+  format %{ \"vandL_vx $dst_src, $dst_src, $src\" %}\n@@ -748,1 +748,1 @@\n-instruct vand_immI_masked(vReg dst_src, immI5 con, vRegMask_V0 v0) %{\n+instruct vandI_vi_masked(vReg dst_src, immI5 con, vRegMask_V0 v0) %{\n@@ -753,1 +753,1 @@\n-  format %{ \"vand_immI_masked $dst_src, $dst_src, $con\" %}\n+  format %{ \"vandI_vi_masked $dst_src, $dst_src, $con, $v0\" %}\n@@ -764,1 +764,1 @@\n-instruct vand_immL_masked(vReg dst_src, immL5 con, vRegMask_V0 v0) %{\n+instruct vandL_vi_masked(vReg dst_src, immL5 con, vRegMask_V0 v0) %{\n@@ -767,1 +767,1 @@\n-  format %{ \"vand_immL_masked $dst_src, $dst_src, $con\" %}\n+  format %{ \"vandL_vi_masked $dst_src, $dst_src, $con, $v0\" %}\n@@ -779,1 +779,1 @@\n-instruct vand_regI_masked(vReg dst_src, iRegIorL2I src, vRegMask_V0 v0) %{\n+instruct vandI_vx_masked(vReg dst_src, iRegIorL2I src, vRegMask_V0 v0) %{\n@@ -784,1 +784,1 @@\n-  format %{ \"vand_regI_masked $dst_src, $dst_src, $src\" %}\n+  format %{ \"vandI_vx_masked $dst_src, $dst_src, $src, $v0\" %}\n@@ -795,1 +795,1 @@\n-instruct vand_regL_masked(vReg dst_src, iRegL src, vRegMask_V0 v0) %{\n+instruct vandL_vx_masked(vReg dst_src, iRegL src, vRegMask_V0 v0) %{\n@@ -798,1 +798,1 @@\n-  format %{ \"vand_regL_masked $dst_src, $dst_src, $src\" %}\n+  format %{ \"vandL_vx_masked $dst_src, $dst_src, $src, $v0\" %}\n@@ -842,1 +842,1 @@\n-instruct vor_immI(vReg dst_src, immI5 con) %{\n+instruct vorI_vi(vReg dst_src, immI5 con) %{\n@@ -847,1 +847,1 @@\n-  format %{ \"vor_immI $dst_src, $dst_src, $con\" %}\n+  format %{ \"vorI_vi $dst_src, $dst_src, $con\" %}\n@@ -858,1 +858,1 @@\n-instruct vor_immL(vReg dst_src, immL5 con) %{\n+instruct vorL_vi(vReg dst_src, immL5 con) %{\n@@ -861,1 +861,1 @@\n-  format %{ \"vor_immL $dst_src, $dst_src, $con\" %}\n+  format %{ \"vorL_vi $dst_src, $dst_src, $con\" %}\n@@ -873,1 +873,1 @@\n-instruct vor_regI(vReg dst_src, iRegIorL2I src) %{\n+instruct vorI_vx(vReg dst_src, iRegIorL2I src) %{\n@@ -878,1 +878,1 @@\n-  format %{ \"vor_regI $dst_src, $dst_src, $src\" %}\n+  format %{ \"vorI_vx $dst_src, $dst_src, $src\" %}\n@@ -889,1 +889,1 @@\n-instruct vor_regL(vReg dst_src, iRegL src) %{\n+instruct vorL_vx(vReg dst_src, iRegL src) %{\n@@ -892,1 +892,1 @@\n-  format %{ \"vor_regL $dst_src, $dst_src, $src\" %}\n+  format %{ \"vorL_vx $dst_src, $dst_src, $src\" %}\n@@ -904,1 +904,1 @@\n-instruct vor_immI_masked(vReg dst_src, immI5 con, vRegMask_V0 v0) %{\n+instruct vorI_vi_masked(vReg dst_src, immI5 con, vRegMask_V0 v0) %{\n@@ -909,1 +909,1 @@\n-  format %{ \"vor_immI_masked $dst_src, $dst_src, $con\" %}\n+  format %{ \"vorI_vi_masked $dst_src, $dst_src, $con, $v0\" %}\n@@ -920,1 +920,1 @@\n-instruct vor_immL_masked(vReg dst_src, immL5 con, vRegMask_V0 v0) %{\n+instruct vorL_vi_masked(vReg dst_src, immL5 con, vRegMask_V0 v0) %{\n@@ -923,1 +923,1 @@\n-  format %{ \"vor_immL_masked $dst_src, $dst_src, $con\" %}\n+  format %{ \"vorL_vi_masked $dst_src, $dst_src, $con, $v0\" %}\n@@ -935,1 +935,1 @@\n-instruct vor_regI_masked(vReg dst_src, iRegIorL2I src, vRegMask_V0 v0) %{\n+instruct vorI_vx_masked(vReg dst_src, iRegIorL2I src, vRegMask_V0 v0) %{\n@@ -940,1 +940,1 @@\n-  format %{ \"vor_regI_masked $dst_src, $dst_src, $src\" %}\n+  format %{ \"vorI_vx_masked $dst_src, $dst_src, $src, $v0\" %}\n@@ -951,1 +951,1 @@\n-instruct vor_regL_masked(vReg dst_src, iRegL src, vRegMask_V0 v0) %{\n+instruct vorL_vx_masked(vReg dst_src, iRegL src, vRegMask_V0 v0) %{\n@@ -954,1 +954,1 @@\n-  format %{ \"vor_regL_masked $dst_src, $dst_src, $src\" %}\n+  format %{ \"vorL_vx_masked $dst_src, $dst_src, $src, $v0\" %}\n@@ -998,1 +998,1 @@\n-instruct vxor_immI(vReg dst_src, immI5 con) %{\n+instruct vxorI_vi(vReg dst_src, immI5 con) %{\n@@ -1003,1 +1003,1 @@\n-  format %{ \"vxor_immI $dst_src, $dst_src, $con\" %}\n+  format %{ \"vxorI_vi $dst_src, $dst_src, $con\" %}\n@@ -1014,1 +1014,1 @@\n-instruct vxor_immL(vReg dst_src, immL5 con) %{\n+instruct vxorL_vi(vReg dst_src, immL5 con) %{\n@@ -1017,1 +1017,1 @@\n-  format %{ \"vxor_immL $dst_src, $dst_src, $con\" %}\n+  format %{ \"vxorL_vi $dst_src, $dst_src, $con\" %}\n@@ -1029,1 +1029,1 @@\n-instruct vxor_regI(vReg dst_src, iRegIorL2I src) %{\n+instruct vxorI_vx(vReg dst_src, iRegIorL2I src) %{\n@@ -1034,1 +1034,1 @@\n-  format %{ \"vxor_regI $dst_src, $dst_src, $src\" %}\n+  format %{ \"vxorI_vx $dst_src, $dst_src, $src\" %}\n@@ -1045,1 +1045,1 @@\n-instruct vxor_regL(vReg dst_src, iRegL src) %{\n+instruct vxorL_vx(vReg dst_src, iRegL src) %{\n@@ -1048,1 +1048,1 @@\n-  format %{ \"vxor_regL $dst_src, $dst_src, $src\" %}\n+  format %{ \"vxorL_vx $dst_src, $dst_src, $src\" %}\n@@ -1060,1 +1060,1 @@\n-instruct vxor_immI_masked(vReg dst_src, immI5 con, vRegMask_V0 v0) %{\n+instruct vxorI_vi_masked(vReg dst_src, immI5 con, vRegMask_V0 v0) %{\n@@ -1065,1 +1065,1 @@\n-  format %{ \"vxor_immI_masked $dst_src, $dst_src, $con\" %}\n+  format %{ \"vxorI_vi_masked $dst_src, $dst_src, $con, $v0\" %}\n@@ -1076,1 +1076,1 @@\n-instruct vxor_immL_masked(vReg dst_src, immL5 con, vRegMask_V0 v0) %{\n+instruct vxorL_vi_masked(vReg dst_src, immL5 con, vRegMask_V0 v0) %{\n@@ -1079,1 +1079,1 @@\n-  format %{ \"vxor_immL_masked $dst_src, $dst_src, $con\" %}\n+  format %{ \"vxorL_vi_masked $dst_src, $dst_src, $con, $v0\" %}\n@@ -1091,1 +1091,1 @@\n-instruct vxor_regI_masked(vReg dst_src, iRegIorL2I src, vRegMask_V0 v0) %{\n+instruct vxorI_vx_masked(vReg dst_src, iRegIorL2I src, vRegMask_V0 v0) %{\n@@ -1096,1 +1096,1 @@\n-  format %{ \"vxor_regI_masked $dst_src, $dst_src, $src\" %}\n+  format %{ \"vxorI_vx_masked $dst_src, $dst_src, $src, $v0\" %}\n@@ -1107,1 +1107,1 @@\n-instruct vxor_regL_masked(vReg dst_src, iRegL src, vRegMask_V0 v0) %{\n+instruct vxorL_vx_masked(vReg dst_src, iRegL src, vRegMask_V0 v0) %{\n@@ -1110,1 +1110,1 @@\n-  format %{ \"vxor_regL_masked $dst_src, $dst_src, $src\" %}\n+  format %{ \"vxorL_vx_masked $dst_src, $dst_src, $src, $v0\" %}\n@@ -1188,0 +1188,64 @@\n+instruct vand_notI_vx(vReg dst, vReg src1, iRegIorL2I src2, immI_M1 m1) %{\n+  predicate(UseZvbb);\n+  predicate(Matcher::vector_element_basic_type(n) == T_INT ||\n+            Matcher::vector_element_basic_type(n) == T_BYTE ||\n+            Matcher::vector_element_basic_type(n) == T_SHORT);\n+  match(Set dst (AndV src1 (Replicate (XorI src2 m1))));\n+  format %{ \"vand_notI_vx $dst, $src1, $src2\" %}\n+  ins_encode %{\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    __ vsetvli_helper(bt, Matcher::vector_length(this));\n+    __ vandn_vx(as_VectorRegister($dst$$reg),\n+                as_VectorRegister($src1$$reg),\n+                as_Register($src2$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vand_notL_vx(vReg dst, vReg src1, iRegL src2, immL_M1 m1) %{\n+  predicate(UseZvbb);\n+  predicate(Matcher::vector_element_basic_type(n) == T_LONG);\n+  match(Set dst (AndV src1 (Replicate (XorL src2 m1))));\n+  format %{ \"vand_notL_vx $dst, $src1, $src2\" %}\n+  ins_encode %{\n+    __ vsetvli_helper(T_LONG, Matcher::vector_length(this));\n+    __ vandn_vx(as_VectorRegister($dst$$reg),\n+                as_VectorRegister($src1$$reg),\n+                as_Register($src2$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vand_notI_vx_masked(vReg dst_src1, iRegIorL2I src2, immI_M1 m1, vRegMask_V0 v0) %{\n+  predicate(UseZvbb);\n+  predicate(Matcher::vector_element_basic_type(n) == T_INT ||\n+            Matcher::vector_element_basic_type(n) == T_BYTE ||\n+            Matcher::vector_element_basic_type(n) == T_SHORT);\n+  match(Set dst_src1 (AndV (Binary dst_src1 (Replicate (XorI src2 m1))) v0));\n+  format %{ \"vand_notI_vx_masked $dst_src1, $dst_src1, $src2, $v0\" %}\n+  ins_encode %{\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    __ vsetvli_helper(bt, Matcher::vector_length(this));\n+    __ vandn_vx(as_VectorRegister($dst_src1$$reg),\n+                as_VectorRegister($dst_src1$$reg),\n+                as_Register($src2$$reg),\n+                Assembler::v0_t);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vand_notL_vx_masked(vReg dst_src1, iRegL src2, immL_M1 m1, vRegMask_V0 v0) %{\n+  predicate(UseZvbb);\n+  predicate(Matcher::vector_element_basic_type(n) == T_LONG);\n+  match(Set dst_src1 (AndV (Binary dst_src1 (Replicate (XorL src2 m1))) v0));\n+  format %{ \"vand_notL_vx_masked $dst_src1, $dst_src1, $src2, $v0\" %}\n+  ins_encode %{\n+    __ vsetvli_helper(T_LONG, Matcher::vector_length(this));\n+    __ vandn_vx(as_VectorRegister($dst_src1$$reg),\n+                as_VectorRegister($dst_src1$$reg),\n+                as_Register($src2$$reg),\n+                Assembler::v0_t);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n@@ -1738,1 +1802,1 @@\n-instruct vmul_regI(vReg dst, vReg src1, iRegIorL2I src2) %{\n+instruct vmulI_vx(vReg dst, vReg src1, iRegIorL2I src2) %{\n@@ -1742,1 +1806,1 @@\n-  format %{ \"vmul_regI $dst, $src1, $src2\" %}\n+  format %{ \"vmulI_vx $dst, $src1, $src2\" %}\n@@ -1753,1 +1817,1 @@\n-instruct vmul_regL(vReg dst, vReg src1, iRegL src2) %{\n+instruct vmulL_vx(vReg dst, vReg src1, iRegL src2) %{\n@@ -1755,1 +1819,1 @@\n-  format %{ \"vmul_regL $dst, $src1, $src2\" %}\n+  format %{ \"vmulL_vx $dst, $src1, $src2\" %}\n@@ -1767,1 +1831,1 @@\n-instruct vmul_regI_masked(vReg dst_src, iRegIorL2I src2, vRegMask_V0 v0) %{\n+instruct vmulI_vx_masked(vReg dst_src, iRegIorL2I src2, vRegMask_V0 v0) %{\n@@ -1771,1 +1835,1 @@\n-  format %{ \"vmul_regI_masked $dst_src, $dst_src, $src2\" %}\n+  format %{ \"vmulI_vx_masked $dst_src, $dst_src, $src2, $v0\" %}\n@@ -1782,1 +1846,1 @@\n-instruct vmul_regL_masked(vReg dst_src, iRegL src2, vRegMask_V0 v0) %{\n+instruct vmulL_vx_masked(vReg dst_src, iRegL src2, vRegMask_V0 v0) %{\n@@ -1784,1 +1848,1 @@\n-  format %{ \"vmul_regL_masked $dst_src, $dst_src, $src2\" %}\n+  format %{ \"vmulL_vx_masked $dst_src, $dst_src, $src2, $v0\" %}\n@@ -3059,1 +3123,1 @@\n-instruct vasrB_imm(vReg dst, vReg src, immI shift) %{\n+instruct vasrB_vi(vReg dst, vReg src, immI shift) %{\n@@ -3062,1 +3126,1 @@\n-  format %{ \"vasrB_imm $dst, $src, $shift\" %}\n+  format %{ \"vasrB_vi $dst, $src, $shift\" %}\n@@ -3077,1 +3141,1 @@\n-instruct vasrS_imm(vReg dst, vReg src, immI shift) %{\n+instruct vasrS_vi(vReg dst, vReg src, immI shift) %{\n@@ -3080,1 +3144,1 @@\n-  format %{ \"vasrS_imm $dst, $src, $shift\" %}\n+  format %{ \"vasrS_vi $dst, $src, $shift\" %}\n@@ -3095,1 +3159,1 @@\n-instruct vasrI_imm(vReg dst, vReg src, immI shift) %{\n+instruct vasrI_vi(vReg dst, vReg src, immI shift) %{\n@@ -3098,1 +3162,1 @@\n-  format %{ \"vasrI_imm $dst, $src, $shift\" %}\n+  format %{ \"vasrI_vi $dst, $src, $shift\" %}\n@@ -3112,1 +3176,1 @@\n-instruct vasrL_imm(vReg dst, vReg src, immI shift) %{\n+instruct vasrL_vi(vReg dst, vReg src, immI shift) %{\n@@ -3116,1 +3180,1 @@\n-  format %{ \"vasrL_imm $dst, $src, $shift\" %}\n+  format %{ \"vasrL_vi $dst, $src, $shift\" %}\n@@ -3130,1 +3194,1 @@\n-instruct vasrB_imm_masked(vReg dst_src, immI shift, vRegMask_V0 v0) %{\n+instruct vasrB_vi_masked(vReg dst_src, immI shift, vRegMask_V0 v0) %{\n@@ -3133,1 +3197,1 @@\n-  format %{ \"vasrB_imm_masked $dst_src, $dst_src, $shift, $v0\" %}\n+  format %{ \"vasrB_vi_masked $dst_src, $dst_src, $shift, $v0\" %}\n@@ -3147,1 +3211,1 @@\n-instruct vasrS_imm_masked(vReg dst_src, immI shift, vRegMask_V0 v0) %{\n+instruct vasrS_vi_masked(vReg dst_src, immI shift, vRegMask_V0 v0) %{\n@@ -3150,1 +3214,1 @@\n-  format %{ \"vasrS_imm_masked $dst_src, $dst_src, $shift, $v0\" %}\n+  format %{ \"vasrS_vi_masked $dst_src, $dst_src, $shift, $v0\" %}\n@@ -3164,1 +3228,1 @@\n-instruct vasrI_imm_masked(vReg dst_src, immI shift, vRegMask_V0 v0) %{\n+instruct vasrI_vi_masked(vReg dst_src, immI shift, vRegMask_V0 v0) %{\n@@ -3167,1 +3231,1 @@\n-  format %{ \"vasrI_imm_masked $dst_src, $dst_src, $shift, $v0\" %}\n+  format %{ \"vasrI_vi_masked $dst_src, $dst_src, $shift, $v0\" %}\n@@ -3180,1 +3244,1 @@\n-instruct vasrL_imm_masked(vReg dst_src, immI shift, vRegMask_V0 v0) %{\n+instruct vasrL_vi_masked(vReg dst_src, immI shift, vRegMask_V0 v0) %{\n@@ -3184,1 +3248,1 @@\n-  format %{ \"vasrL_imm_masked $dst_src, $dst_src, $shift, $v0\" %}\n+  format %{ \"vasrL_vi_masked $dst_src, $dst_src, $shift, $v0\" %}\n@@ -3197,1 +3261,1 @@\n-instruct vlsrB_imm(vReg dst, vReg src, immI shift) %{\n+instruct vlsrB_vi(vReg dst, vReg src, immI shift) %{\n@@ -3200,1 +3264,1 @@\n-  format %{ \"vlsrB_imm $dst, $src, $shift\" %}\n+  format %{ \"vlsrB_vi $dst, $src, $shift\" %}\n@@ -3219,1 +3283,1 @@\n-instruct vlsrS_imm(vReg dst, vReg src, immI shift) %{\n+instruct vlsrS_vi(vReg dst, vReg src, immI shift) %{\n@@ -3222,1 +3286,1 @@\n-  format %{ \"vlsrS_imm $dst, $src, $shift\" %}\n+  format %{ \"vlsrS_vi $dst, $src, $shift\" %}\n@@ -3241,1 +3305,1 @@\n-instruct vlsrI_imm(vReg dst, vReg src, immI shift) %{\n+instruct vlsrI_vi(vReg dst, vReg src, immI shift) %{\n@@ -3244,1 +3308,1 @@\n-  format %{ \"vlsrI_imm $dst, $src, $shift\" %}\n+  format %{ \"vlsrI_vi $dst, $src, $shift\" %}\n@@ -3258,1 +3322,1 @@\n-instruct vlsrL_imm(vReg dst, vReg src, immI shift) %{\n+instruct vlsrL_vi(vReg dst, vReg src, immI shift) %{\n@@ -3262,1 +3326,1 @@\n-  format %{ \"vlsrL_imm $dst, $src, $shift\" %}\n+  format %{ \"vlsrL_vi $dst, $src, $shift\" %}\n@@ -3276,1 +3340,1 @@\n-instruct vlsrB_imm_masked(vReg dst_src, immI shift, vRegMask_V0 v0) %{\n+instruct vlsrB_vi_masked(vReg dst_src, immI shift, vRegMask_V0 v0) %{\n@@ -3279,1 +3343,1 @@\n-  format %{ \"vlsrB_imm_masked $dst_src, $dst_src, $shift, $v0\" %}\n+  format %{ \"vlsrB_vi_masked $dst_src, $dst_src, $shift, $v0\" %}\n@@ -3297,1 +3361,1 @@\n-instruct vlsrS_imm_masked(vReg dst_src, immI shift, vRegMask_V0 v0) %{\n+instruct vlsrS_vi_masked(vReg dst_src, immI shift, vRegMask_V0 v0) %{\n@@ -3300,1 +3364,1 @@\n-  format %{ \"vlsrS_imm_masked $dst_src, $dst_src, $shift, $v0\" %}\n+  format %{ \"vlsrS_vi_masked $dst_src, $dst_src, $shift, $v0\" %}\n@@ -3318,1 +3382,1 @@\n-instruct vlsrI_imm_masked(vReg dst_src, immI shift, vRegMask_V0 v0) %{\n+instruct vlsrI_vi_masked(vReg dst_src, immI shift, vRegMask_V0 v0) %{\n@@ -3321,1 +3385,1 @@\n-  format %{ \"vlsrI_imm_masked $dst_src, $dst_src, $shift, $v0\" %}\n+  format %{ \"vlsrI_vi_masked $dst_src, $dst_src, $shift, $v0\" %}\n@@ -3334,1 +3398,1 @@\n-instruct vlsrL_imm_masked(vReg dst_src, immI shift, vRegMask_V0 v0) %{\n+instruct vlsrL_vi_masked(vReg dst_src, immI shift, vRegMask_V0 v0) %{\n@@ -3338,1 +3402,1 @@\n-  format %{ \"vlsrL_imm_masked $dst_src, $dst_src, $shift, $v0\" %}\n+  format %{ \"vlsrL_vi_masked $dst_src, $dst_src, $shift, $v0\" %}\n@@ -3351,1 +3415,1 @@\n-instruct vlslB_imm(vReg dst, vReg src, immI shift) %{\n+instruct vlslB_vi(vReg dst, vReg src, immI shift) %{\n@@ -3354,1 +3418,1 @@\n-  format %{ \"vlslB_imm $dst, $src, $shift\" %}\n+  format %{ \"vlslB_vi $dst, $src, $shift\" %}\n@@ -3368,1 +3432,1 @@\n-instruct vlslS_imm(vReg dst, vReg src, immI shift) %{\n+instruct vlslS_vi(vReg dst, vReg src, immI shift) %{\n@@ -3371,1 +3435,1 @@\n-  format %{ \"vlslS_imm $dst, $src, $shift\" %}\n+  format %{ \"vlslS_vi $dst, $src, $shift\" %}\n@@ -3385,1 +3449,1 @@\n-instruct vlslI_imm(vReg dst, vReg src, immI shift) %{\n+instruct vlslI_vi(vReg dst, vReg src, immI shift) %{\n@@ -3388,1 +3452,1 @@\n-  format %{ \"vlslI_imm $dst, $src, $shift\" %}\n+  format %{ \"vlslI_vi $dst, $src, $shift\" %}\n@@ -3397,1 +3461,1 @@\n-instruct vlslL_imm(vReg dst, vReg src, immI shift) %{\n+instruct vlslL_vi(vReg dst, vReg src, immI shift) %{\n@@ -3401,1 +3465,1 @@\n-  format %{ \"vlslL_imm $dst, $src, $shift\" %}\n+  format %{ \"vlslL_vi $dst, $src, $shift\" %}\n@@ -3410,1 +3474,1 @@\n-instruct vlslB_imm_masked(vReg dst_src, immI shift, vRegMask_V0 v0) %{\n+instruct vlslB_vi_masked(vReg dst_src, immI shift, vRegMask_V0 v0) %{\n@@ -3413,1 +3477,1 @@\n-  format %{ \"vlslB_imm_masked $dst_src, $dst_src, $shift, $v0\" %}\n+  format %{ \"vlslB_vi_masked $dst_src, $dst_src, $shift, $v0\" %}\n@@ -3428,1 +3492,1 @@\n-instruct vlslS_imm_masked(vReg dst_src, immI shift, vRegMask_V0 v0) %{\n+instruct vlslS_vi_masked(vReg dst_src, immI shift, vRegMask_V0 v0) %{\n@@ -3431,1 +3495,1 @@\n-  format %{ \"vlslS_imm_masked $dst_src, $dst_src, $shift, $v0\" %}\n+  format %{ \"vlslS_vi_masked $dst_src, $dst_src, $shift, $v0\" %}\n@@ -3446,1 +3510,1 @@\n-instruct vlslI_imm_masked(vReg dst_src, immI shift, vRegMask_V0 v0) %{\n+instruct vlslI_vi_masked(vReg dst_src, immI shift, vRegMask_V0 v0) %{\n@@ -3449,1 +3513,1 @@\n-  format %{ \"vlslI_imm_masked $dst_src, $dst_src, $shift, $v0\" %}\n+  format %{ \"vlslI_vi_masked $dst_src, $dst_src, $shift, $v0\" %}\n@@ -3459,1 +3523,1 @@\n-instruct vlslL_imm_masked(vReg dst_src, immI shift, vRegMask_V0 v0) %{\n+instruct vlslL_vi_masked(vReg dst_src, immI shift, vRegMask_V0 v0) %{\n@@ -3463,1 +3527,1 @@\n-  format %{ \"vlslL_imm_masked $dst_src, $dst_src, $shift, $v0\" %}\n+  format %{ \"vlslL_vi_masked $dst_src, $dst_src, $shift, $v0\" %}\n@@ -3503,1 +3567,1 @@\n-instruct vrotate_right_reg(vReg dst, vReg src, iRegIorL2I shift) %{\n+instruct vrotate_right_vx(vReg dst, vReg src, iRegIorL2I shift) %{\n@@ -3505,1 +3569,1 @@\n-  format %{ \"vrotate_right_reg $dst, $src, $shift\\t\" %}\n+  format %{ \"vrotate_right_vx $dst, $src, $shift\\t\" %}\n@@ -3515,1 +3579,1 @@\n-instruct vrotate_right_imm(vReg dst, vReg src, immI shift) %{\n+instruct vrotate_right_vi(vReg dst, vReg src, immI shift) %{\n@@ -3517,1 +3581,1 @@\n-  format %{ \"vrotate_right_imm $dst, $src, $shift\\t\" %}\n+  format %{ \"vrotate_right_vi $dst, $src, $shift\\t\" %}\n@@ -3535,1 +3599,1 @@\n-  format %{ \"vrotate_right_masked $dst_src, $dst_src, $shift, v0.t\\t\" %}\n+  format %{ \"vrotate_right_masked $dst_src, $dst_src, $shift, $v0\\t\" %}\n@@ -3546,1 +3610,1 @@\n-instruct vrotate_right_reg_masked(vReg dst_src, iRegIorL2I shift, vRegMask_V0 v0) %{\n+instruct vrotate_right_vx_masked(vReg dst_src, iRegIorL2I shift, vRegMask_V0 v0) %{\n@@ -3548,1 +3612,1 @@\n-  format %{ \"vrotate_right_reg_masked $dst_src, $dst_src, $shift, v0.t\\t\" %}\n+  format %{ \"vrotate_right_vx_masked $dst_src, $dst_src, $shift, $v0\\t\" %}\n@@ -3558,1 +3622,1 @@\n-instruct vrotate_right_imm_masked(vReg dst_src, immI shift, vRegMask_V0 v0) %{\n+instruct vrotate_right_vi_masked(vReg dst_src, immI shift, vRegMask_V0 v0) %{\n@@ -3560,1 +3624,1 @@\n-  format %{ \"vrotate_right_imm_masked $dst_src, $dst_src, $shift, v0.t\\t\" %}\n+  format %{ \"vrotate_right_vi_masked $dst_src, $dst_src, $shift, $v0\\t\" %}\n@@ -3590,1 +3654,1 @@\n-instruct vrotate_left_reg(vReg dst, vReg src, iRegIorL2I shift) %{\n+instruct vrotate_left_vx(vReg dst, vReg src, iRegIorL2I shift) %{\n@@ -3592,1 +3656,1 @@\n-  format %{ \"vrotate_left_reg $dst, $src, $shift\\t\" %}\n+  format %{ \"vrotate_left_vx $dst, $src, $shift\\t\" %}\n@@ -3602,1 +3666,1 @@\n-instruct vrotate_left_imm(vReg dst, vReg src, immI shift) %{\n+instruct vrotate_left_vi(vReg dst, vReg src, immI shift) %{\n@@ -3604,1 +3668,1 @@\n-  format %{ \"vrotate_left_imm $dst, $src, $shift\\t\" %}\n+  format %{ \"vrotate_left_vi $dst, $src, $shift\\t\" %}\n@@ -3623,1 +3687,1 @@\n-  format %{ \"vrotate_left_masked $dst_src, $dst_src, $shift, v0.t\\t\" %}\n+  format %{ \"vrotate_left_masked $dst_src, $dst_src, $shift, $v0\\t\" %}\n@@ -3634,1 +3698,1 @@\n-instruct vrotate_left_reg_masked(vReg dst_src, iRegIorL2I shift, vRegMask_V0 v0) %{\n+instruct vrotate_left_vx_masked(vReg dst_src, iRegIorL2I shift, vRegMask_V0 v0) %{\n@@ -3636,1 +3700,1 @@\n-  format %{ \"vrotate_left_reg_masked $dst_src, $dst_src, $shift, v0.t\\t\" %}\n+  format %{ \"vrotate_left_vx_masked $dst_src, $dst_src, $shift, $v0\\t\" %}\n@@ -3646,1 +3710,1 @@\n-instruct vrotate_left_imm_masked(vReg dst_src, immI shift, vRegMask_V0 v0) %{\n+instruct vrotate_left_vi_masked(vReg dst_src, immI shift, vRegMask_V0 v0) %{\n@@ -3648,1 +3712,1 @@\n-  format %{ \"vrotate_left_imm_masked $dst_src, $dst_src, $shift, v0.t\\t\" %}\n+  format %{ \"vrotate_left_vi_masked $dst_src, $dst_src, $shift, $v0\\t\" %}\n","filename":"src\/hotspot\/cpu\/riscv\/riscv_v.ad","additions":210,"deletions":146,"binary":false,"changes":356,"status":"modified"},{"patch":"@@ -2129,0 +2129,20 @@\n+    public static final String VAND_NOTI_VX = PREFIX + \"VAND_NOTI_VX\" + POSTFIX;\n+    static {\n+        machOnlyNameRegex(VAND_NOTI_VX, \"vand_notI_vx\");\n+    }\n+\n+    public static final String VAND_NOTL_VX = PREFIX + \"VAND_NOTL_VX\" + POSTFIX;\n+    static {\n+        machOnlyNameRegex(VAND_NOTL_VX, \"vand_notL_vx\");\n+    }\n+\n+    public static final String VAND_NOTI_VX_MASKED = PREFIX + \"VAND_NOTI_VX_MASKED\" + POSTFIX;\n+    static {\n+        machOnlyNameRegex(VAND_NOTI_VX_MASKED, \"vand_notI_vx_masked\");\n+    }\n+\n+    public static final String VAND_NOTL_VX_MASKED = PREFIX + \"VAND_NOTL_VX_MASKED\" + POSTFIX;\n+    static {\n+        machOnlyNameRegex(VAND_NOTL_VX_MASKED, \"vand_notL_vx_masked\");\n+    }\n+\n","filename":"test\/hotspot\/jtreg\/compiler\/lib\/ir_framework\/IRNode.java","additions":20,"deletions":0,"binary":false,"changes":20,"status":"modified"},{"patch":"@@ -157,0 +157,62 @@\n+    @Test\n+    @Warmup(10000)\n+    @IR(counts = { IRNode.VAND_NOTI_VX, \" >= 1\" }, applyIfPlatform = {\"riscv64\", \"true\"})\n+    public static void testAllBitsSetVectorRegI() {\n+        IntVector av = IntVector.fromArray(I_SPECIES, ia, 0);\n+        int bs = ib[0];\n+        av.not().lanewise(VectorOperators.AND_NOT, bs).intoArray(ir, 0);\n+\n+        \/\/ Verify results\n+        for (int i = 0; i < I_SPECIES.length(); i++) {\n+            Asserts.assertEquals((~ia[i]) & (~bs), ir[i]);\n+        }\n+    }\n+\n+    @Test\n+    @Warmup(10000)\n+    @IR(counts = { IRNode.VAND_NOTL_VX, \" >= 1\" }, applyIfPlatform = {\"riscv64\", \"true\"})\n+    public static void testAllBitsSetVectorRegL() {\n+        LongVector av = LongVector.fromArray(L_SPECIES, la, 0);\n+        long bs = lb[0];\n+        av.not().lanewise(VectorOperators.AND_NOT, bs).intoArray(lr, 0);\n+\n+        \/\/ Verify results\n+        for (int i = 0; i < L_SPECIES.length(); i++) {\n+            Asserts.assertEquals((~la[i]) & (~bs), lr[i]);\n+        }\n+    }\n+\n+    @Test\n+    @Warmup(10000)\n+    @IR(counts = { IRNode.VAND_NOTI_VX_MASKED, \" >= 1\" }, applyIfPlatform = {\"riscv64\", \"true\"})\n+    public static void testAllBitsSetVectorRegIMask() {\n+        VectorMask<Integer> avm = VectorMask.fromArray(I_SPECIES, ma, 0);\n+        IntVector av = IntVector.fromArray(I_SPECIES, ia, 0);\n+        int bs = ib[0];\n+        av.not().lanewise(VectorOperators.AND_NOT, bs, avm).intoArray(ir, 0);\n+\n+        \/\/ Verify results\n+        for (int i = 0; i < I_SPECIES.length(); i++) {\n+            if (ma[i] == true) {\n+                Asserts.assertEquals((~ia[i]) & (~bs), ir[i]);\n+            }\n+        }\n+    }\n+\n+    @Test\n+    @Warmup(10000)\n+    @IR(counts = { IRNode.VAND_NOTL_VX_MASKED, \" >= 1\" }, applyIfPlatform = {\"riscv64\", \"true\"})\n+    public static void testAllBitsSetVectorRegLMask() {\n+        VectorMask<Long> avm = VectorMask.fromArray(L_SPECIES, ma, 0);\n+        LongVector av = LongVector.fromArray(L_SPECIES, la, 0);\n+        long bs = lb[0];\n+        av.not().lanewise(VectorOperators.AND_NOT, bs, avm).intoArray(lr, 0);\n+\n+        \/\/ Verify results\n+        for (int i = 0; i < L_SPECIES.length(); i++) {\n+            if (ma[i] == true) {\n+                Asserts.assertEquals((~la[i]) & (~bs), lr[i]);\n+            }\n+        }\n+    }\n+\n","filename":"test\/hotspot\/jtreg\/compiler\/vectorapi\/AllBitsSetVectorMatchRuleTest.java","additions":62,"deletions":0,"binary":false,"changes":62,"status":"modified"}]}