{"files":[{"patch":"@@ -820,2 +820,8 @@\n-    AC_MSG_CHECKING([for binutils to use with hsdis])\n-    if test \"x$BINUTILS_DIR\" != x; then\n+    binutils_system_error=\"\"\n+    HSDIS_LIBS=\"\"\n+    if test \"x$BINUTILS_DIR\" = xsystem; then\n+      AC_CHECK_LIB(bfd, bfd_openr, [ HSDIS_LIBS=\"-lbfd\" ], [ binutils_system_error=\"libbfd not found\" ])\n+      AC_CHECK_LIB(opcodes, disassembler, [ HSDIS_LIBS=\"$HSDIS_LIBS -lopcodes\" ], [ binutils_system_error=\"libopcodes not found\" ])\n+      AC_CHECK_LIB(iberty, xmalloc, [ HSDIS_LIBS=\"$HSDIS_LIBS -liberty\" ], [ binutils_system_error=\"libiberty not found\" ])\n+      AC_CHECK_LIB(z, deflate, [ HSDIS_LIBS=\"$HSDIS_LIBS -lz\" ], [ binutils_system_error=\"libz not found\" ])\n+    elif test \"x$BINUTILS_DIR\" != x; then\n@@ -825,1 +831,0 @@\n-        AC_MSG_RESULT([$BINUTILS_DIR])\n@@ -828,3 +833,0 @@\n-      else\n-        AC_MSG_RESULT([invalid])\n-        AC_MSG_ERROR([$BINUTILS_DIR does not contain a proper binutils installation])\n@@ -832,7 +834,0 @@\n-    else\n-      AC_MSG_RESULT([missing])\n-      AC_MSG_NOTICE([--with-hsdis=binutils requires specifying a binutils installation.])\n-      AC_MSG_NOTICE([Download binutils from https:\/\/www.gnu.org\/software\/binutils and unpack it,])\n-      AC_MSG_NOTICE([and point --with-binutils-src to the resulting directory, or use])\n-      AC_MSG_NOTICE([--with-binutils to point to a pre-built binutils installation.])\n-      AC_MSG_ERROR([Cannot continue])\n@@ -840,0 +835,32 @@\n+\n+    AC_MSG_CHECKING([for binutils to use with hsdis])\n+    case \"x$BINUTILS_DIR\" in\n+      xsystem)\n+        if test \"x$OPENJDK_TARGET_OS\" != xlinux; then\n+          AC_MSG_RESULT([invalid])\n+          AC_MSG_ERROR([binutils on system is supported for Linux only])\n+        elif test \"x$binutils_system_error\" = x; then\n+          AC_MSG_RESULT([system])\n+          HSDIS_CFLAGS=\"-DSYSTEM_BINUTILS\"\n+        else\n+          AC_MSG_RESULT([invalid])\n+          AC_MSG_ERROR([$binutils_system_error])\n+        fi\n+        ;;\n+      x)\n+        AC_MSG_RESULT([missing])\n+        AC_MSG_NOTICE([--with-hsdis=binutils requires specifying a binutils installation.])\n+        AC_MSG_NOTICE([Download binutils from https:\/\/www.gnu.org\/software\/binutils and unpack it,])\n+        AC_MSG_NOTICE([and point --with-binutils-src to the resulting directory, or use])\n+        AC_MSG_NOTICE([--with-binutils to point to a pre-built binutils installation.])\n+        AC_MSG_ERROR([Cannot continue])\n+        ;;\n+      *)\n+        if test \"x$HSDIS_LIBS\" != x; then\n+          AC_MSG_RESULT([$BINUTILS_DIR])\n+        else\n+          AC_MSG_RESULT([invalid])\n+          AC_MSG_ERROR([$BINUTILS_DIR does not contain a proper binutils installation])\n+        fi\n+        ;;\n+    esac\n","filename":"make\/autoconf\/jdk-options.m4","additions":40,"deletions":13,"binary":false,"changes":53,"status":"modified"},{"patch":"@@ -310,1 +310,2 @@\n-        test \"x$OPENJDK_TARGET_CPU\" = \"xaarch64\" ; then\n+        test \"x$OPENJDK_TARGET_CPU\" = \"xaarch64\" || \\\n+        test \"x$OPENJDK_TARGET_CPU\" = \"xppc64le\"; then\n","filename":"make\/autoconf\/jvm-features.m4","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -152,0 +152,1 @@\n+        $d\/cpu\/$(HOTSPOT_TARGET_CPU_ARCH)\/gc\/shenandoah\/shenandoah_$(HOTSPOT_TARGET_CPU_ARCH).ad \\\n","filename":"make\/hotspot\/gensrc\/GensrcAdlc.gmk","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -837,0 +837,13 @@\n+# MACOSX_METAL_VERSION_MIN specifies the lowest version of Macosx\n+# that should be used to compile Metal shaders. We support Metal\n+# pipeline only on Macosx >=10.14. For Macosx versions <10.14 even if\n+# we enable Metal pipeline using -Dsun.java2d.metal=true, at\n+# runtime we force it to use OpenGL pipeline. And MACOSX_VERSION_MIN\n+# for aarch64 has always been >10.14 so we use continue to use\n+# MACOSX_VERSION_MIN for aarch64.\n+ifeq ($(OPENJDK_TARGET_CPU_ARCH), xaarch64)\n+    MACOSX_METAL_VERSION_MIN=$(MACOSX_VERSION_MIN)\n+else\n+    MACOSX_METAL_VERSION_MIN=10.14.0\n+endif\n+\n@@ -848,1 +861,3 @@\n-      COMMAND := $(METAL) -c -std=osx-metal2.0 -o $(SHADERS_AIR) $(SHADERS_SRC), \\\n+      COMMAND := $(METAL) -c -std=osx-metal2.0 \\\n+          -mmacosx-version-min=$(MACOSX_METAL_VERSION_MIN) \\\n+          -o $(SHADERS_AIR) $(SHADERS_SRC), \\\n","filename":"make\/modules\/java.desktop\/lib\/Awt2dLibraries.gmk","additions":16,"deletions":1,"binary":false,"changes":17,"status":"modified"},{"patch":"@@ -2062,1 +2062,1 @@\n-  if (src_hi != OptoReg::Bad) {\n+  if (src_hi != OptoReg::Bad && !bottom_type()->isa_vectmask()) {\n@@ -2077,1 +2077,1 @@\n-  if (bottom_type()->isa_vect() != NULL) {\n+  if (bottom_type()->isa_vect() && !bottom_type()->isa_vectmask()) {\n@@ -2183,0 +2183,3 @@\n+      } else if (dst_lo_rc == rc_predicate) {\n+        __ unspill_sve_predicate(as_PRegister(Matcher::_regEncode[dst_lo]), ra_->reg2offset(src_lo),\n+                                 Matcher::scalable_vector_reg_size(T_BYTE) >> 3);\n@@ -2185,2 +2188,18 @@\n-        __ unspill(rscratch1, is64, src_offset);\n-        __ spill(rscratch1, is64, dst_offset);\n+        if (ideal_reg() == Op_RegVectMask) {\n+          __ spill_copy_sve_predicate_stack_to_stack(src_offset, dst_offset,\n+                                                     Matcher::scalable_vector_reg_size(T_BYTE) >> 3);\n+        } else {\n+          __ unspill(rscratch1, is64, src_offset);\n+          __ spill(rscratch1, is64, dst_offset);\n+        }\n+      }\n+      break;\n+    case rc_predicate:\n+      if (dst_lo_rc == rc_predicate) {\n+        __ sve_mov(as_PRegister(Matcher::_regEncode[dst_lo]), as_PRegister(Matcher::_regEncode[src_lo]));\n+      } else if (dst_lo_rc == rc_stack) {\n+        __ spill_sve_predicate(as_PRegister(Matcher::_regEncode[src_lo]), ra_->reg2offset(dst_lo),\n+                               Matcher::scalable_vector_reg_size(T_BYTE) >> 3);\n+      } else {\n+        assert(false, \"bad src and dst rc_class combination.\");\n+        ShouldNotReachHere();\n@@ -2207,1 +2226,1 @@\n-    if (bottom_type()->isa_vect() != NULL) {\n+    if (bottom_type()->isa_vect() && !bottom_type()->isa_vectmask()) {\n@@ -2224,0 +2243,4 @@\n+    } else if (ideal_reg() == Op_RegVectMask) {\n+      assert(Matcher::supports_scalable_vector(), \"bad register type for spill\");\n+      int vsize = Matcher::scalable_predicate_reg_slots() * 32;\n+      st->print(\"\\t# predicate spill size = %d\", vsize);\n@@ -2376,0 +2399,2 @@\n+    case Op_OnSpinWait:\n+      return VM_Version::supports_on_spin_wait();\n@@ -2383,0 +2408,12 @@\n+    case Op_LoadVectorMasked:\n+    case Op_StoreVectorMasked:\n+    case Op_LoadVectorGatherMasked:\n+    case Op_StoreVectorScatterMasked:\n+    case Op_MaskAll:\n+    case Op_AndVMask:\n+    case Op_OrVMask:\n+    case Op_XorVMask:\n+      if (UseSVE == 0) {\n+        ret_value = false;\n+      }\n+      break;\n@@ -2431,0 +2468,9 @@\n+const bool Matcher::match_rule_supported_vector_masked(int opcode, int vlen, BasicType bt) {\n+  \/\/ Only SVE supports masked operations.\n+  if (UseSVE == 0) {\n+    return false;\n+  }\n+  return match_rule_supported(opcode) &&\n+         masked_op_sve_supported(opcode, vlen, bt);\n+}\n+\n@@ -2642,0 +2688,41 @@\n+bool can_combine_with_imm(Node* binary_node, Node* replicate_node) {\n+  if (UseSVE == 0 || !VectorNode::is_invariant_vector(replicate_node)){\n+    return false;\n+  }\n+  Node* imm_node = replicate_node->in(1);\n+  if (!imm_node->is_Con()) {\n+    return false;\n+  }\n+\n+  const Type* t = imm_node->bottom_type();\n+  if (!(t->isa_int() || t->isa_long())) {\n+    return false;\n+  }\n+\n+  switch (binary_node->Opcode()) {\n+  case Op_AndV:\n+  case Op_OrV:\n+  case Op_XorV: {\n+    Assembler::SIMD_RegVariant T = Assembler::elemType_to_regVariant(Matcher::vector_element_basic_type(binary_node));\n+    uint64_t value = t->isa_long() ? (uint64_t)imm_node->get_long() : (uint64_t)imm_node->get_int();\n+    return Assembler::operand_valid_for_sve_logical_immediate(Assembler::regVariant_to_elemBits(T), value);\n+  }\n+  case Op_AddVB:\n+    return (imm_node->get_int() <= 255 && imm_node->get_int() >= -255);\n+  case Op_AddVS:\n+  case Op_AddVI:\n+    return Assembler::operand_valid_for_sve_add_sub_immediate((int64_t)imm_node->get_int());\n+  case Op_AddVL:\n+    return Assembler::operand_valid_for_sve_add_sub_immediate(imm_node->get_long());\n+  default:\n+    return false;\n+  }\n+}\n+\n+bool is_vector_arith_imm_pattern(Node* n, Node* m) {\n+  if (n != NULL && m != NULL) {\n+    return can_combine_with_imm(n, m);\n+  }\n+  return false;\n+}\n+\n@@ -2644,2 +2731,7 @@\n-  if (is_vshift_con_pattern(n, m)) { \/\/ ShiftV src (ShiftCntV con)\n-    mstack.push(m, Visit);           \/\/ m = ShiftCntV\n+  \/\/ ShiftV src (ShiftCntV con)\n+  \/\/ StoreVector (VectorStoreMask src)\n+  \/\/ Binary src (Replicate con)\n+  if (is_vshift_con_pattern(n, m) ||\n+      (UseSVE > 0 && m->Opcode() == Op_VectorStoreMask && n->Opcode() == Op_StoreVector) ||\n+      is_vector_arith_imm_pattern(n, m)) {\n+    mstack.push(m, Visit);\n@@ -2648,0 +2740,1 @@\n+\n@@ -3855,1 +3948,1 @@\n-               \/*release*\/ true, \/*weak*\/ false, noreg); \/\/ Sets flags for result\n+               \/*release*\/ true, \/*weak*\/ false, rscratch1); \/\/ Sets flags for result\n@@ -3864,0 +3957,9 @@\n+    __ br(Assembler::EQ, cont); \/\/ CAS success means locking succeeded\n+\n+    __ cmp(rscratch1, rthread);\n+    __ br(Assembler::NE, cont); \/\/ Check for recursive locking\n+\n+    \/\/ Recursive lock case\n+    __ increment(Address(disp_hdr, ObjectMonitor::recursions_offset_in_bytes() - markWord::monitor_value), 1);\n+    \/\/ flag == EQ still from the cmp above, checking if this is a reentrant lock\n+\n@@ -3907,3 +4009,3 @@\n-    __ eor(rscratch1, rscratch1, rthread); \/\/ Will be 0 if we are the owner.\n-    __ orr(rscratch1, rscratch1, disp_hdr); \/\/ Will be 0 if there are 0 recursions\n-    __ cmp(rscratch1, zr); \/\/ Sets flags for result\n+\n+    Label notRecursive;\n+    __ cmp(rscratch1, rthread);\n@@ -3912,0 +4014,9 @@\n+    __ cbz(disp_hdr, notRecursive);\n+\n+    \/\/ Recursive lock\n+    __ sub(disp_hdr, disp_hdr, 1u);\n+    __ str(disp_hdr, Address(tmp, ObjectMonitor::recursions_offset_in_bytes()));\n+    \/\/ flag == EQ was set in the ownership check above\n+    __ b(cont);\n+\n+    __ bind(notRecursive);\n@@ -4564,0 +4675,11 @@\n+\/\/ 8 bit integer valid for vector add sub immediate\n+operand immBAddSubV()\n+%{\n+  predicate(n->get_int() <= 255 && n->get_int() >= -255);\n+  match(ConI);\n+\n+  op_cost(0);\n+  format %{ %}\n+  interface(CONST_INTER);\n+%}\n+\n@@ -4574,0 +4696,11 @@\n+\/\/ 32 bit integer valid for vector add sub immediate\n+operand immIAddSubV()\n+%{\n+  predicate(Assembler::operand_valid_for_sve_add_sub_immediate((int64_t)n->get_int()));\n+  match(ConI);\n+\n+  op_cost(0);\n+  format %{ %}\n+  interface(CONST_INTER);\n+%}\n+\n@@ -4575,1 +4708,21 @@\n-\/\/ TODO -- check this is right when e.g the mask is 0x80000000\n+\n+operand immBLog()\n+%{\n+  predicate(Assembler::operand_valid_for_sve_logical_immediate(BitsPerByte, (uint64_t)n->get_int()));\n+  match(ConI);\n+\n+  op_cost(0);\n+  format %{ %}\n+  interface(CONST_INTER);\n+%}\n+\n+operand immSLog()\n+%{\n+  predicate(Assembler::operand_valid_for_sve_logical_immediate(BitsPerShort, (uint64_t)n->get_int()));\n+  match(ConI);\n+\n+  op_cost(0);\n+  format %{ %}\n+  interface(CONST_INTER);\n+%}\n+\n@@ -4653,0 +4806,11 @@\n+\/\/ 64 bit integer valid for addv subv immediate\n+operand immLAddSubV()\n+%{\n+  predicate(Assembler::operand_valid_for_sve_add_sub_immediate(n->get_long()));\n+  match(ConL);\n+\n+  op_cost(0);\n+  format %{ %}\n+  interface(CONST_INTER);\n+%}\n+\n@@ -5506,0 +5670,1 @@\n+  match(pRegGov);\n@@ -8855,0 +9020,11 @@\n+instruct castVVMask(pRegGov dst)\n+%{\n+  match(Set dst (CastVV dst));\n+\n+  size(0);\n+  format %{ \"# castVV of $dst\" %}\n+  ins_encode(\/* empty encoding *\/);\n+  ins_cost(0);\n+  ins_pipe(pipe_class_empty);\n+%}\n+\n@@ -14336,0 +14512,12 @@\n+instruct onspinwait() %{\n+  match(OnSpinWait);\n+  ins_cost(INSN_COST);\n+\n+  format %{ \"onspinwait\" %}\n+\n+  ins_encode %{\n+    __ spin_wait();\n+  %}\n+  ins_pipe(pipe_class_empty);\n+%}\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/aarch64.ad","additions":200,"deletions":12,"binary":false,"changes":212,"status":"modified"},{"patch":"@@ -91,0 +91,1 @@\n+  bool masked_op_sve_supported(int opcode, int vlen, BasicType bt);\n@@ -147,5 +148,1 @@\n-        if (vlen < 4 || length_in_bytes > MaxVectorSize) {\n-          return false;\n-        } else {\n-          return true;\n-        }\n+        return vlen >= 4 && length_in_bytes <= MaxVectorSize;\n@@ -161,0 +158,8 @@\n+\n+  bool masked_op_sve_supported(int opcode, int vlen, BasicType bt) {\n+    if (opcode == Op_VectorRearrange) {\n+      return false;\n+    }\n+    return op_sve_supported(opcode, vlen, bt);\n+  }\n+\n@@ -297,1 +302,1 @@\n-instruct loadV_partial(vReg dst, vmemA mem, pRegGov pTmp, rFlagsReg cr) %{\n+instruct loadV_partial(vReg dst, vmemA mem, pRegGov pgtmp, rFlagsReg cr) %{\n@@ -301,1 +306,1 @@\n-  effect(TEMP pTmp, KILL cr);\n+  effect(TEMP pgtmp, KILL cr);\n@@ -303,2 +308,2 @@\n-  format %{ \"sve_whilelo_zr_imm $pTmp, vector_length\\n\\t\"\n-            \"sve_ldr $dst, $pTmp, $mem\\t# load vector predicated\" %}\n+  format %{ \"sve_whilelo_zr_imm $pgtmp, vector_length\\n\\t\"\n+            \"sve_ldr $dst, $pgtmp, $mem\\t# load vector partial\" %}\n@@ -307,1 +312,1 @@\n-    __ sve_whilelo_zr_imm(as_PRegister($pTmp$$reg), __ elemType_to_regVariant(bt),\n+    __ sve_whilelo_zr_imm(as_PRegister($pgtmp$$reg), __ elemType_to_regVariant(bt),\n@@ -311,1 +316,1 @@\n-                          as_PRegister($pTmp$$reg), bt, bt, $mem->opcode(),\n+                          as_PRegister($pgtmp$$reg), bt, bt, $mem->opcode(),\n@@ -317,1 +322,1 @@\n-instruct storeV_partial(vReg src, vmemA mem, pRegGov pTmp, rFlagsReg cr) %{\n+instruct storeV_partial(vReg src, vmemA mem, pRegGov pgtmp, rFlagsReg cr) %{\n@@ -321,1 +326,1 @@\n-  effect(TEMP pTmp, KILL cr);\n+  effect(TEMP pgtmp, KILL cr);\n@@ -323,2 +328,2 @@\n-  format %{ \"sve_whilelo_zr_imm $pTmp, vector_length\\n\\t\"\n-            \"sve_str $src, $pTmp, $mem\\t# store vector predicated\" %}\n+  format %{ \"sve_whilelo_zr_imm $pgtmp, vector_length\\n\\t\"\n+            \"sve_str $src, $pgtmp, $mem\\t# store vector partial\" %}\n@@ -327,1 +332,1 @@\n-    __ sve_whilelo_zr_imm(as_PRegister($pTmp$$reg), __ elemType_to_regVariant(bt),\n+    __ sve_whilelo_zr_imm(as_PRegister($pgtmp$$reg), __ elemType_to_regVariant(bt),\n@@ -331,1 +336,73 @@\n-                          as_PRegister($pTmp$$reg), bt, bt, $mem->opcode(),\n+                          as_PRegister($pgtmp$$reg), bt, bt, $mem->opcode(),\n+                          as_Register($mem$$base), $mem$$index, $mem$$scale, $mem$$disp);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ vector load\/store - predicated\n+\n+instruct loadV_masked(vReg dst, vmemA mem, pRegGov pg) %{\n+  predicate(UseSVE > 0 &&\n+            n->as_LoadVector()->memory_size() == MaxVectorSize);\n+  match(Set dst (LoadVectorMasked mem pg));\n+  ins_cost(4 * SVE_COST);\n+  format %{ \"sve_ldr $dst, $pg, $mem\\t# load vector predicated (sve)\" %}\n+  ins_encode %{\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    loadStoreA_predicated(C2_MacroAssembler(&cbuf), false, as_FloatRegister($dst$$reg),\n+                          as_PRegister($pg$$reg), bt, bt, $mem->opcode(),\n+                          as_Register($mem$$base), $mem$$index, $mem$$scale, $mem$$disp);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct loadV_masked_partial(vReg dst, vmemA mem, pRegGov pg, pRegGov pgtmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->as_LoadVector()->memory_size() < MaxVectorSize);\n+  match(Set dst (LoadVectorMasked mem pg));\n+  effect(TEMP pgtmp, KILL cr);\n+  ins_cost(6 * SVE_COST);\n+  format %{ \"sve_ldr $dst, $pg, $mem\\t# load vector predicated partial (sve)\" %}\n+  ins_encode %{\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    __ sve_whilelo_zr_imm(as_PRegister($pgtmp$$reg), __ elemType_to_regVariant(bt),\n+                          Matcher::vector_length(this));\n+    __ sve_and(as_PRegister($pgtmp$$reg), as_PRegister($pgtmp$$reg),\n+               as_PRegister($pg$$reg), as_PRegister($pg$$reg));\n+    loadStoreA_predicated(C2_MacroAssembler(&cbuf), false, as_FloatRegister($dst$$reg),\n+                          as_PRegister($pgtmp$$reg), bt, bt, $mem->opcode(),\n+                          as_Register($mem$$base), $mem$$index, $mem$$scale, $mem$$disp);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct storeV_masked(vReg src, vmemA mem, pRegGov pg) %{\n+  predicate(UseSVE > 0 &&\n+            n->as_StoreVector()->memory_size() == MaxVectorSize);\n+  match(Set mem (StoreVectorMasked mem (Binary src pg)));\n+  ins_cost(4 * SVE_COST);\n+  format %{ \"sve_str $mem, $pg, $src\\t# store vector predicated (sve)\" %}\n+  ins_encode %{\n+    BasicType bt = Matcher::vector_element_basic_type(this, $src);\n+    loadStoreA_predicated(C2_MacroAssembler(&cbuf), true, as_FloatRegister($src$$reg),\n+                          as_PRegister($pg$$reg), bt, bt, $mem->opcode(),\n+                          as_Register($mem$$base), $mem$$index, $mem$$scale, $mem$$disp);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct storeV_masked_partial(vReg src, vmemA mem, pRegGov pg, pRegGov pgtmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->as_StoreVector()->memory_size() < MaxVectorSize);\n+  match(Set mem (StoreVectorMasked mem (Binary src pg)));\n+  effect(TEMP pgtmp, KILL cr);\n+  ins_cost(6 * SVE_COST);\n+  format %{ \"sve_str $mem, $pg, $src\\t# store vector predicated partial (sve)\" %}\n+  ins_encode %{\n+    BasicType bt = Matcher::vector_element_basic_type(this, $src);\n+    __ sve_whilelo_zr_imm(as_PRegister($pgtmp$$reg), __ elemType_to_regVariant(bt),\n+                          Matcher::vector_length(this, $src));\n+    __ sve_and(as_PRegister($pgtmp$$reg), as_PRegister($pgtmp$$reg),\n+               as_PRegister($pg$$reg), as_PRegister($pg$$reg));\n+    loadStoreA_predicated(C2_MacroAssembler(&cbuf), true, as_FloatRegister($src$$reg),\n+                          as_PRegister($pgtmp$$reg), bt, bt, $mem->opcode(),\n@@ -337,0 +414,134 @@\n+\/\/ maskAll\n+\n+instruct vmaskAll_immI(pRegGov dst, immI src) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst (MaskAll src));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_ptrue\/sve_pfalse $dst\\t# mask all (sve) (B\/H\/S)\" %}\n+  ins_encode %{\n+    int con = (int)$src$$constant;\n+    if (con == 0) {\n+      __ sve_pfalse(as_PRegister($dst$$reg));\n+    } else {\n+      assert(con == -1, \"invalid constant value for mask\");\n+      BasicType bt = Matcher::vector_element_basic_type(this);\n+      __ sve_ptrue(as_PRegister($dst$$reg), __ elemType_to_regVariant(bt));\n+    }\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vmaskAllI(pRegGov dst, iRegIorL2I src, vReg tmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst (MaskAll src));\n+  effect(TEMP tmp, KILL cr);\n+  ins_cost(2 * SVE_COST);\n+  format %{ \"sve_dup $tmp, $src\\n\\t\"\n+            \"sve_cmpne $dst, $tmp, 0\\t# mask all (sve) (B\/H\/S)\" %}\n+  ins_encode %{\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    Assembler::SIMD_RegVariant size = __ elemType_to_regVariant(bt);\n+    __ sve_dup(as_FloatRegister($tmp$$reg), size, as_Register($src$$reg));\n+    __ sve_cmp(Assembler::NE, as_PRegister($dst$$reg), size, ptrue, as_FloatRegister($tmp$$reg), 0);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vmaskAll_immL(pRegGov dst, immL src) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst (MaskAll src));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_ptrue\/sve_pfalse $dst\\t# mask all (sve) (D)\" %}\n+  ins_encode %{\n+    long con = (long)$src$$constant;\n+    if (con == 0) {\n+      __ sve_pfalse(as_PRegister($dst$$reg));\n+    } else {\n+      assert(con == -1, \"invalid constant value for mask\");\n+      BasicType bt = Matcher::vector_element_basic_type(this);\n+      __ sve_ptrue(as_PRegister($dst$$reg), __ elemType_to_regVariant(bt));\n+    }\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vmaskAllL(pRegGov dst, iRegL src, vReg tmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst (MaskAll src));\n+  effect(TEMP tmp, KILL cr);\n+  ins_cost(2 * SVE_COST);\n+  format %{ \"sve_dup $tmp, $src\\n\\t\"\n+            \"sve_cmpne $dst, $tmp, 0\\t# mask all (sve) (D)\" %}\n+  ins_encode %{\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    Assembler::SIMD_RegVariant size = __ elemType_to_regVariant(bt);\n+    __ sve_dup(as_FloatRegister($tmp$$reg), size, as_Register($src$$reg));\n+    __ sve_cmp(Assembler::NE, as_PRegister($dst$$reg), size, ptrue, as_FloatRegister($tmp$$reg), 0);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ mask logical and\/or\/xor\n+\n+instruct vmask_and(pRegGov pd, pRegGov pn, pRegGov pm) %{\n+  predicate(UseSVE > 0);\n+  match(Set pd (AndVMask pn pm));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_and $pd, $pn, $pm\\t# predicate (sve)\" %}\n+  ins_encode %{\n+    __ sve_and(as_PRegister($pd$$reg), ptrue,\n+               as_PRegister($pn$$reg), as_PRegister($pm$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vmask_or(pRegGov pd, pRegGov pn, pRegGov pm) %{\n+  predicate(UseSVE > 0);\n+  match(Set pd (OrVMask pn pm));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_orr $pd, $pn, $pm\\t# predicate (sve)\" %}\n+  ins_encode %{\n+    __ sve_orr(as_PRegister($pd$$reg), ptrue,\n+               as_PRegister($pn$$reg), as_PRegister($pm$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vmask_xor(pRegGov pd, pRegGov pn, pRegGov pm) %{\n+  predicate(UseSVE > 0);\n+  match(Set pd (XorVMask pn pm));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_eor $pd, $pn, $pm\\t# predicate (sve)\" %}\n+  ins_encode %{\n+    __ sve_eor(as_PRegister($pd$$reg), ptrue,\n+               as_PRegister($pn$$reg), as_PRegister($pm$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ mask logical and_not\n+\n+instruct vmask_and_notI(pRegGov pd, pRegGov pn, pRegGov pm, immI_M1 m1) %{\n+  predicate(UseSVE > 0);\n+  match(Set pd (AndVMask pn (XorVMask pm (MaskAll m1))));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_bic $pd, $pn, $pm\\t# predciate (sve) (B\/H\/S)\" %}\n+  ins_encode %{\n+    __ sve_bic(as_PRegister($pd$$reg), ptrue,\n+               as_PRegister($pn$$reg), as_PRegister($pm$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vmask_and_notL(pRegGov pd, pRegGov pn, pRegGov pm, immL_M1 m1) %{\n+  predicate(UseSVE > 0);\n+  match(Set pd (AndVMask pn (XorVMask pm (MaskAll m1))));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_bic $pd, $pn, $pm\\t# predciate (sve) (D)\" %}\n+  ins_encode %{\n+    __ sve_bic(as_PRegister($pd$$reg), ptrue,\n+               as_PRegister($pn$$reg), as_PRegister($pm$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n@@ -351,1 +562,1 @@\n-instruct reinterpretResize(vReg dst, vReg src, pRegGov pTmp, rFlagsReg cr) %{\n+instruct reinterpretResize(vReg dst, vReg src, pRegGov pgtmp, rFlagsReg cr) %{\n@@ -355,1 +566,1 @@\n-  effect(TEMP_DEF dst, TEMP pTmp, KILL cr);\n+  effect(TEMP_DEF dst, TEMP pgtmp, KILL cr);\n@@ -365,1 +576,1 @@\n-    __ sve_whilelo_zr_imm(as_PRegister($pTmp$$reg), __ B, length_in_bytes_resize);\n+    __ sve_whilelo_zr_imm(as_PRegister($pgtmp$$reg), __ B, length_in_bytes_resize);\n@@ -367,1 +578,1 @@\n-    __ sve_sel(as_FloatRegister($dst$$reg), __ B, as_PRegister($pTmp$$reg),\n+    __ sve_sel(as_FloatRegister($dst$$reg), __ B, as_PRegister($pgtmp$$reg),\n@@ -373,0 +584,34 @@\n+\/\/ vector mask reinterpret\n+\n+instruct vmask_reinterpret_same_esize(pRegGov dst_src) %{\n+  predicate(UseSVE > 0 &&\n+            n->as_Vector()->length() == n->in(1)->bottom_type()->is_vect()->length() &&\n+            n->as_Vector()->length_in_bytes() == n->in(1)->bottom_type()->is_vect()->length_in_bytes());\n+  match(Set dst_src (VectorReinterpret dst_src));\n+  ins_cost(0);\n+  format %{ \"# vmask_reinterpret $dst_src\\t# do nothing\" %}\n+  ins_encode %{\n+    \/\/ empty\n+  %}\n+  ins_pipe(pipe_class_empty);\n+%}\n+\n+instruct vmask_reinterpret_diff_esize(pRegGov dst, pRegGov src, vReg tmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->as_Vector()->length() != n->in(1)->bottom_type()->is_vect()->length() &&\n+            n->as_Vector()->length_in_bytes() == n->in(1)->bottom_type()->is_vect()->length_in_bytes());\n+  match(Set dst (VectorReinterpret src));\n+  effect(TEMP tmp, KILL cr);\n+  ins_cost(2 * SVE_COST);\n+  format %{ \"# vmask_reinterpret $dst, $src\\t# vector (sve)\" %}\n+  ins_encode %{\n+    BasicType from_bt = Matcher::vector_element_basic_type(this, $src);\n+    Assembler::SIMD_RegVariant from_size = __ elemType_to_regVariant(from_bt);\n+    BasicType to_bt = Matcher::vector_element_basic_type(this);\n+    Assembler::SIMD_RegVariant to_size = __ elemType_to_regVariant(to_bt);\n+    __ sve_cpy(as_FloatRegister($tmp$$reg), from_size, as_PRegister($src$$reg), -1, false);\n+    __ sve_cmp(Assembler::EQ, as_PRegister($dst$$reg), to_size, ptrue, as_FloatRegister($tmp$$reg), -1);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n@@ -377,1 +622,1 @@\n-            n->bottom_type()->is_vect()->element_basic_type() == T_BYTE);\n+            !n->as_Vector()->is_predicated_vector());\n@@ -390,1 +635,1 @@\n-            n->bottom_type()->is_vect()->element_basic_type() == T_SHORT);\n+            !n->as_Vector()->is_predicated_vector());\n@@ -403,1 +648,1 @@\n-            n->bottom_type()->is_vect()->element_basic_type() == T_INT);\n+            !n->as_Vector()->is_predicated_vector());\n@@ -416,1 +661,1 @@\n-            n->bottom_type()->is_vect()->element_basic_type() == T_LONG);\n+            !n->as_Vector()->is_predicated_vector());\n@@ -429,1 +674,1 @@\n-            n->bottom_type()->is_vect()->element_basic_type() == T_FLOAT);\n+            !n->as_Vector()->is_predicated_vector());\n@@ -442,1 +687,1 @@\n-            n->bottom_type()->is_vect()->element_basic_type() == T_DOUBLE);\n+            !n->as_Vector()->is_predicated_vector());\n@@ -453,0 +698,80 @@\n+\/\/ vector abs - predicated\n+\n+instruct vabsB_masked(vReg dst_src, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src (AbsVB dst_src pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_abs $dst_src, $pg, $dst_src\\t# vector (sve) (B)\" %}\n+  ins_encode %{\n+    __ sve_abs(as_FloatRegister($dst_src$$reg), __ B,\n+            as_PRegister($pg$$reg),\n+            as_FloatRegister($dst_src$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vabsS_masked(vReg dst_src, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src (AbsVS dst_src pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_abs $dst_src, $pg, $dst_src\\t# vector (sve) (H)\" %}\n+  ins_encode %{\n+    __ sve_abs(as_FloatRegister($dst_src$$reg), __ H,\n+            as_PRegister($pg$$reg),\n+            as_FloatRegister($dst_src$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vabsI_masked(vReg dst_src, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src (AbsVI dst_src pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_abs $dst_src, $pg, $dst_src\\t# vector (sve) (S)\" %}\n+  ins_encode %{\n+    __ sve_abs(as_FloatRegister($dst_src$$reg), __ S,\n+            as_PRegister($pg$$reg),\n+            as_FloatRegister($dst_src$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vabsL_masked(vReg dst_src, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src (AbsVL dst_src pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_abs $dst_src, $pg, $dst_src\\t# vector (sve) (D)\" %}\n+  ins_encode %{\n+    __ sve_abs(as_FloatRegister($dst_src$$reg), __ D,\n+            as_PRegister($pg$$reg),\n+            as_FloatRegister($dst_src$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vabsF_masked(vReg dst_src, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src (AbsVF dst_src pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_fabs $dst_src, $pg, $dst_src\\t# vector (sve) (S)\" %}\n+  ins_encode %{\n+    __ sve_fabs(as_FloatRegister($dst_src$$reg), __ S,\n+            as_PRegister($pg$$reg),\n+            as_FloatRegister($dst_src$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vabsD_masked(vReg dst_src, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src (AbsVD dst_src pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_fabs $dst_src, $pg, $dst_src\\t# vector (sve) (D)\" %}\n+  ins_encode %{\n+    __ sve_fabs(as_FloatRegister($dst_src$$reg), __ D,\n+            as_PRegister($pg$$reg),\n+            as_FloatRegister($dst_src$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n@@ -533,1 +858,1 @@\n-\/\/ vector and\n+\/\/ vector add - predicated\n@@ -535,1 +860,1 @@\n-instruct vand(vReg dst, vReg src1, vReg src2) %{\n+instruct vaddB_masked(vReg dst_src1, vReg src2, pRegGov pg) %{\n@@ -537,1 +862,1 @@\n-  match(Set dst (AndV src1 src2));\n+  match(Set dst_src1 (AddVB (Binary dst_src1 src2) pg));\n@@ -539,1 +864,1 @@\n-  format %{ \"sve_and  $dst, $src1, $src2\\t# vector (sve)\" %}\n+  format %{ \"sve_add $dst_src1, $pg, $dst_src1, $src2\\t# vector (sve) (B)\" %}\n@@ -541,3 +866,3 @@\n-    __ sve_and(as_FloatRegister($dst$$reg),\n-         as_FloatRegister($src1$$reg),\n-         as_FloatRegister($src2$$reg));\n+    __ sve_add(as_FloatRegister($dst_src1$$reg), __ B,\n+            as_PRegister($pg$$reg),\n+            as_FloatRegister($src2$$reg));\n@@ -548,3 +873,1 @@\n-\/\/ vector or\n-\n-instruct vor(vReg dst, vReg src1, vReg src2) %{\n+instruct vaddS_masked(vReg dst_src1, vReg src2, pRegGov pg) %{\n@@ -552,1 +875,1 @@\n-  match(Set dst (OrV src1 src2));\n+  match(Set dst_src1 (AddVS (Binary dst_src1 src2) pg));\n@@ -554,1 +877,1 @@\n-  format %{ \"sve_orr  $dst, $src1, $src2\\t# vector (sve)\" %}\n+  format %{ \"sve_add $dst_src1, $pg, $dst_src1, $src2\\t# vector (sve) (H)\" %}\n@@ -556,3 +879,3 @@\n-    __ sve_orr(as_FloatRegister($dst$$reg),\n-         as_FloatRegister($src1$$reg),\n-         as_FloatRegister($src2$$reg));\n+    __ sve_add(as_FloatRegister($dst_src1$$reg), __ H,\n+            as_PRegister($pg$$reg),\n+            as_FloatRegister($src2$$reg));\n@@ -563,3 +886,1 @@\n-\/\/ vector xor\n-\n-instruct vxor(vReg dst, vReg src1, vReg src2) %{\n+instruct vaddI_masked(vReg dst_src1, vReg src2, pRegGov pg) %{\n@@ -567,1 +888,1 @@\n-  match(Set dst (XorV src1 src2));\n+  match(Set dst_src1 (AddVI (Binary dst_src1 src2) pg));\n@@ -569,1 +890,1 @@\n-  format %{ \"sve_eor  $dst, $src1, $src2\\t# vector (sve)\" %}\n+  format %{ \"sve_add $dst_src1, $pg, $dst_src1, $src2\\t# vector (sve) (S)\" %}\n@@ -571,3 +892,3 @@\n-    __ sve_eor(as_FloatRegister($dst$$reg),\n-         as_FloatRegister($src1$$reg),\n-         as_FloatRegister($src2$$reg));\n+    __ sve_add(as_FloatRegister($dst_src1$$reg), __ S,\n+            as_PRegister($pg$$reg),\n+            as_FloatRegister($src2$$reg));\n@@ -578,3 +899,1 @@\n-\/\/ vector not\n-\n-instruct vnotI(vReg dst, vReg src, immI_M1 m1) %{\n+instruct vaddL_masked(vReg dst_src1, vReg src2, pRegGov pg) %{\n@@ -582,3 +901,1 @@\n-  match(Set dst (XorV src (ReplicateB m1)));\n-  match(Set dst (XorV src (ReplicateS m1)));\n-  match(Set dst (XorV src (ReplicateI m1)));\n+  match(Set dst_src1 (AddVL (Binary dst_src1 src2) pg));\n@@ -586,1 +903,1 @@\n-  format %{ \"sve_not $dst, $src\\t# vector (sve) B\/H\/S\" %}\n+  format %{ \"sve_add $dst_src1, $pg, $dst_src1, $src2\\t# vector (sve) (D)\" %}\n@@ -588,2 +905,3 @@\n-    __ sve_not(as_FloatRegister($dst$$reg), __ D,\n-               ptrue, as_FloatRegister($src$$reg));\n+    __ sve_add(as_FloatRegister($dst_src1$$reg), __ D,\n+            as_PRegister($pg$$reg),\n+            as_FloatRegister($src2$$reg));\n@@ -594,1 +912,1 @@\n-instruct vnotL(vReg dst, vReg src, immL_M1 m1) %{\n+instruct vaddF_masked(vReg dst_src1, vReg src2, pRegGov pg) %{\n@@ -596,1 +914,1 @@\n-  match(Set dst (XorV src (ReplicateL m1)));\n+  match(Set dst_src1 (AddVF (Binary dst_src1 src2) pg));\n@@ -598,1 +916,1 @@\n-  format %{ \"sve_not $dst, $src\\t# vector (sve) D\" %}\n+  format %{ \"sve_fadd $dst_src1, $pg, $dst_src1, $src2\\t# vector (sve) (S)\" %}\n@@ -600,2 +918,3 @@\n-    __ sve_not(as_FloatRegister($dst$$reg), __ D,\n-               ptrue, as_FloatRegister($src$$reg));\n+    __ sve_fadd(as_FloatRegister($dst_src1$$reg), __ S,\n+            as_PRegister($pg$$reg),\n+            as_FloatRegister($src2$$reg));\n@@ -606,4 +925,1 @@\n-\n-\/\/ vector and_not\n-\n-instruct vand_notI(vReg dst, vReg src1, vReg src2, immI_M1 m1) %{\n+instruct vaddD_masked(vReg dst_src1, vReg src2, pRegGov pg) %{\n@@ -611,3 +927,1 @@\n-  match(Set dst (AndV src1 (XorV src2 (ReplicateB m1))));\n-  match(Set dst (AndV src1 (XorV src2 (ReplicateS m1))));\n-  match(Set dst (AndV src1 (XorV src2 (ReplicateI m1))));\n+  match(Set dst_src1 (AddVD (Binary dst_src1 src2) pg));\n@@ -615,1 +929,1 @@\n-  format %{ \"sve_bic $dst, $src1, $src2\\t# vector (sve) B\/H\/S\" %}\n+  format %{ \"sve_fadd $dst_src1, $pg, $dst_src1, $src2\\t# vector (sve) (D)\" %}\n@@ -617,3 +931,3 @@\n-    __ sve_bic(as_FloatRegister($dst$$reg),\n-               as_FloatRegister($src1$$reg),\n-               as_FloatRegister($src2$$reg));\n+    __ sve_fadd(as_FloatRegister($dst_src1$$reg), __ D,\n+            as_PRegister($pg$$reg),\n+            as_FloatRegister($src2$$reg));\n@@ -624,1 +938,3 @@\n-instruct vand_notL(vReg dst, vReg src1, vReg src2, immL_M1 m1) %{\n+\/\/ vector add reg imm (unpredicated)\n+\n+instruct vaddImmB(vReg dst_src, immBAddSubV con) %{\n@@ -626,1 +942,1 @@\n-  match(Set dst (AndV src1 (XorV src2 (ReplicateL m1))));\n+  match(Set dst_src (AddVB dst_src (ReplicateB con)));\n@@ -628,1 +944,1 @@\n-  format %{ \"sve_bic $dst, $src1, $src2\\t# vector (sve) D\" %}\n+  format %{ \"sve_add $dst_src, $dst_src, $con\\t # vector (sve) (B)\" %}\n@@ -630,3 +946,6 @@\n-    __ sve_bic(as_FloatRegister($dst$$reg),\n-               as_FloatRegister($src1$$reg),\n-               as_FloatRegister($src2$$reg));\n+    int32_t val = $con$$constant;\n+    if (val > 0){\n+      __ sve_add(as_FloatRegister($dst_src$$reg), __ B, val);\n+    } else if (val < 0){\n+      __ sve_sub(as_FloatRegister($dst_src$$reg), __ B, -val);\n+    }\n@@ -637,4 +956,1 @@\n-\n-\/\/ vector float div\n-\n-instruct vdivF(vReg dst_src1, vReg src2) %{\n+instruct vaddImmS(vReg dst_src, immIAddSubV con) %{\n@@ -642,1 +958,1 @@\n-  match(Set dst_src1 (DivVF dst_src1 src2));\n+  match(Set dst_src (AddVS dst_src (ReplicateS con)));\n@@ -644,1 +960,1 @@\n-  format %{ \"sve_fdiv  $dst_src1, $dst_src1, $src2\\t# vector (sve) (S)\" %}\n+  format %{ \"sve_add $dst_src, $dst_src, $con\\t # vector (sve) (H)\" %}\n@@ -646,2 +962,6 @@\n-    __ sve_fdiv(as_FloatRegister($dst_src1$$reg), __ S,\n-         ptrue, as_FloatRegister($src2$$reg));\n+    int32_t val = $con$$constant;\n+    if (val > 0){\n+      __ sve_add(as_FloatRegister($dst_src$$reg), __ H, val);\n+    } else if (val < 0){\n+      __ sve_sub(as_FloatRegister($dst_src$$reg), __ H, -val);\n+    }\n@@ -652,1 +972,1 @@\n-instruct vdivD(vReg dst_src1, vReg src2) %{\n+instruct vaddImmI(vReg dst_src, immIAddSubV con) %{\n@@ -654,1 +974,1 @@\n-  match(Set dst_src1 (DivVD dst_src1 src2));\n+  match(Set dst_src (AddVI dst_src (ReplicateI con)));\n@@ -656,1 +976,1 @@\n-  format %{ \"sve_fdiv  $dst_src1, $dst_src1, $src2\\t# vector (sve) (D)\" %}\n+  format %{ \"sve_add $dst_src, $dst_src, $con\\t # vector (sve) (S)\" %}\n@@ -658,2 +978,6 @@\n-    __ sve_fdiv(as_FloatRegister($dst_src1$$reg), __ D,\n-         ptrue, as_FloatRegister($src2$$reg));\n+    int32_t val = $con$$constant;\n+    if (val > 0){\n+      __ sve_add(as_FloatRegister($dst_src$$reg), __ S, val);\n+    } else if (val < 0){\n+      __ sve_sub(as_FloatRegister($dst_src$$reg), __ S, -val);\n+    }\n@@ -664,3 +988,1 @@\n-\/\/ vector min\/max\n-\n-instruct vmin(vReg dst_src1, vReg src2) %{\n+instruct vaddImmL(vReg dst_src, immLAddSubV con) %{\n@@ -668,1 +990,1 @@\n-  match(Set dst_src1 (MinV dst_src1 src2));\n+  match(Set dst_src (AddVL dst_src (ReplicateL con)));\n@@ -670,1 +992,1 @@\n-  format %{ \"sve_min $dst_src1, $dst_src1, $src2\\t # vector (sve)\" %}\n+  format %{ \"sve_add $dst_src, $dst_src, $con\\t # vector (sve) (D)\" %}\n@@ -672,9 +994,5 @@\n-    BasicType bt = Matcher::vector_element_basic_type(this);\n-    Assembler::SIMD_RegVariant size = __ elemType_to_regVariant(bt);\n-    if (is_floating_point_type(bt)) {\n-      __ sve_fmin(as_FloatRegister($dst_src1$$reg), size,\n-                  ptrue, as_FloatRegister($src2$$reg));\n-    } else {\n-      assert(is_integral_type(bt), \"Unsupported type\");\n-      __ sve_smin(as_FloatRegister($dst_src1$$reg), size,\n-                  ptrue, as_FloatRegister($src2$$reg));\n+    int32_t val = $con$$constant;\n+    if (val > 0){\n+      __ sve_add(as_FloatRegister($dst_src$$reg), __ D, val);\n+    } else if (val < 0){\n+      __ sve_sub(as_FloatRegister($dst_src$$reg), __ D, -val);\n@@ -686,1 +1004,3 @@\n-instruct vmax(vReg dst_src1, vReg src2) %{\n+\/\/ vector binary op reg imm (unpredicated)\n+\n+instruct vandB(vReg dst_src, immBLog con) %{\n@@ -688,1 +1008,1 @@\n-  match(Set dst_src1 (MaxV dst_src1 src2));\n+  match(Set dst_src (AndV dst_src (ReplicateB con)));\n@@ -690,1 +1010,1 @@\n-  format %{ \"sve_max $dst_src1, $dst_src1, $src2\\t # vector (sve)\" %}\n+  format %{ \"sve_and $dst_src, $dst_src, $con\\t # vector (sve) (B)\" %}\n@@ -692,10 +1012,2 @@\n-    BasicType bt = Matcher::vector_element_basic_type(this);\n-    Assembler::SIMD_RegVariant size = __ elemType_to_regVariant(bt);\n-    if (is_floating_point_type(bt)) {\n-      __ sve_fmax(as_FloatRegister($dst_src1$$reg), size,\n-                  ptrue, as_FloatRegister($src2$$reg));\n-    } else {\n-      assert(is_integral_type(bt), \"Unsupported type\");\n-      __ sve_smax(as_FloatRegister($dst_src1$$reg), size,\n-                  ptrue, as_FloatRegister($src2$$reg));\n-    }\n+    __ sve_and(as_FloatRegister($dst_src$$reg), __ B,\n+         (uint64_t)($con$$constant));\n@@ -706,6 +1018,3 @@\n-\/\/ vector fmla\n-\n-\/\/ dst_src1 = dst_src1 + src2 * src3\n-instruct vfmlaF(vReg dst_src1, vReg src2, vReg src3) %{\n-  predicate(UseFMA && UseSVE > 0);\n-  match(Set dst_src1 (FmaVF dst_src1 (Binary src2 src3)));\n+instruct vandH(vReg dst_src, immSLog con) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src (AndV dst_src (ReplicateS con)));\n@@ -713,1 +1022,1 @@\n-  format %{ \"sve_fmla $dst_src1, $src2, $src3\\t # vector (sve) (S)\" %}\n+  format %{ \"sve_and $dst_src, $dst_src, $con\\t # vector (sve) (H)\" %}\n@@ -715,2 +1024,2 @@\n-    __ sve_fmla(as_FloatRegister($dst_src1$$reg), __ S,\n-         ptrue, as_FloatRegister($src2$$reg), as_FloatRegister($src3$$reg));\n+    __ sve_and(as_FloatRegister($dst_src$$reg), __ H,\n+         (uint64_t)($con$$constant));\n@@ -721,4 +1030,3 @@\n-\/\/ dst_src1 = dst_src1 + src2 * src3\n-instruct vfmlaD(vReg dst_src1, vReg src2, vReg src3) %{\n-  predicate(UseFMA && UseSVE > 0);\n-  match(Set dst_src1 (FmaVD dst_src1 (Binary src2 src3)));\n+instruct vandS(vReg dst_src, immILog con) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src (AndV dst_src (ReplicateI con)));\n@@ -726,1 +1034,1 @@\n-  format %{ \"sve_fmla $dst_src1, $src2, $src3\\t # vector (sve) (D)\" %}\n+  format %{ \"sve_and $dst_src, $dst_src, $con\\t # vector (sve) (S)\" %}\n@@ -728,2 +1036,2 @@\n-    __ sve_fmla(as_FloatRegister($dst_src1$$reg), __ D,\n-         ptrue, as_FloatRegister($src2$$reg), as_FloatRegister($src3$$reg));\n+    __ sve_and(as_FloatRegister($dst_src$$reg), __ S,\n+         (uint64_t)($con$$constant));\n@@ -734,8 +1042,3 @@\n-\/\/ vector fmls\n-\n-\/\/ dst_src1 = dst_src1 + -src2 * src3\n-\/\/ dst_src1 = dst_src1 + src2 * -src3\n-instruct vfmlsF(vReg dst_src1, vReg src2, vReg src3) %{\n-  predicate(UseFMA && UseSVE > 0);\n-  match(Set dst_src1 (FmaVF dst_src1 (Binary (NegVF src2) src3)));\n-  match(Set dst_src1 (FmaVF dst_src1 (Binary src2 (NegVF src3))));\n+instruct vandD(vReg dst_src, immLLog con) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src (AndV dst_src (ReplicateL con)));\n@@ -743,1 +1046,1 @@\n-  format %{ \"sve_fmls $dst_src1, $src2, $src3\\t # vector (sve) (S)\" %}\n+  format %{ \"sve_and $dst_src, $dst_src, $con\\t # vector (sve) (D)\" %}\n@@ -745,2 +1048,2 @@\n-    __ sve_fmls(as_FloatRegister($dst_src1$$reg), __ S,\n-         ptrue, as_FloatRegister($src2$$reg), as_FloatRegister($src3$$reg));\n+    __ sve_and(as_FloatRegister($dst_src$$reg), __ D,\n+         (uint64_t)($con$$constant));\n@@ -751,6 +1054,3 @@\n-\/\/ dst_src1 = dst_src1 + -src2 * src3\n-\/\/ dst_src1 = dst_src1 + src2 * -src3\n-instruct vfmlsD(vReg dst_src1, vReg src2, vReg src3) %{\n-  predicate(UseFMA && UseSVE > 0);\n-  match(Set dst_src1 (FmaVD dst_src1 (Binary (NegVD src2) src3)));\n-  match(Set dst_src1 (FmaVD dst_src1 (Binary src2 (NegVD src3))));\n+instruct vorB(vReg dst_src, immBLog con) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src (OrV dst_src (ReplicateB con)));\n@@ -758,1 +1058,1 @@\n-  format %{ \"sve_fmls $dst_src1, $src2, $src3\\t # vector (sve) (D)\" %}\n+  format %{ \"sve_orr $dst_src, $dst_src, $con\\t # vector (sve) (B)\" %}\n@@ -760,2 +1060,2 @@\n-    __ sve_fmls(as_FloatRegister($dst_src1$$reg), __ D,\n-         ptrue, as_FloatRegister($src2$$reg), as_FloatRegister($src3$$reg));\n+    __ sve_orr(as_FloatRegister($dst_src$$reg), __ B,\n+         (uint64_t)($con$$constant));\n@@ -766,8 +1066,3 @@\n-\/\/ vector fnmla\n-\n-\/\/ dst_src1 = -dst_src1 + -src2 * src3\n-\/\/ dst_src1 = -dst_src1 + src2 * -src3\n-instruct vfnmlaF(vReg dst_src1, vReg src2, vReg src3) %{\n-  predicate(UseFMA && UseSVE > 0);\n-  match(Set dst_src1 (FmaVF (NegVF dst_src1) (Binary (NegVF src2) src3)));\n-  match(Set dst_src1 (FmaVF (NegVF dst_src1) (Binary src2 (NegVF src3))));\n+instruct vorH(vReg dst_src, immSLog con) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src (OrV dst_src (ReplicateS con)));\n@@ -775,1 +1070,1 @@\n-  format %{ \"sve_fnmla $dst_src1, $src2, $src3\\t # vector (sve) (S)\" %}\n+  format %{ \"sve_orr $dst_src, $dst_src, $con\\t # vector (sve) (H)\" %}\n@@ -777,2 +1072,2 @@\n-    __ sve_fnmla(as_FloatRegister($dst_src1$$reg), __ S,\n-         ptrue, as_FloatRegister($src2$$reg), as_FloatRegister($src3$$reg));\n+    __ sve_orr(as_FloatRegister($dst_src$$reg), __ H,\n+         (uint64_t)($con$$constant));\n@@ -783,6 +1078,3 @@\n-\/\/ dst_src1 = -dst_src1 + -src2 * src3\n-\/\/ dst_src1 = -dst_src1 + src2 * -src3\n-instruct vfnmlaD(vReg dst_src1, vReg src2, vReg src3) %{\n-  predicate(UseFMA && UseSVE > 0);\n-  match(Set dst_src1 (FmaVD (NegVD dst_src1) (Binary (NegVD src2) src3)));\n-  match(Set dst_src1 (FmaVD (NegVD dst_src1) (Binary src2 (NegVD src3))));\n+instruct vorS(vReg dst_src, immILog con) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src (OrV dst_src (ReplicateI con)));\n@@ -790,1 +1082,1 @@\n-  format %{ \"sve_fnmla $dst_src1, $src2, $src3\\t # vector (sve) (D)\" %}\n+  format %{ \"sve_orr $dst_src, $dst_src, $con\\t # vector (sve) (S)\" %}\n@@ -792,2 +1084,2 @@\n-    __ sve_fnmla(as_FloatRegister($dst_src1$$reg), __ D,\n-         ptrue, as_FloatRegister($src2$$reg), as_FloatRegister($src3$$reg));\n+    __ sve_orr(as_FloatRegister($dst_src$$reg), __ S,\n+         (uint64_t)($con$$constant));\n@@ -798,6 +1090,3 @@\n-\/\/ vector fnmls\n-\n-\/\/ dst_src1 = -dst_src1 + src2 * src3\n-instruct vfnmlsF(vReg dst_src1, vReg src2, vReg src3) %{\n-  predicate(UseFMA && UseSVE > 0);\n-  match(Set dst_src1 (FmaVF (NegVF dst_src1) (Binary src2 src3)));\n+instruct vorD(vReg dst_src, immLLog con) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src (OrV dst_src (ReplicateL con)));\n@@ -805,1 +1094,1 @@\n-  format %{ \"sve_fnmls $dst_src1, $src2, $src3\\t # vector (sve) (S)\" %}\n+  format %{ \"sve_orr $dst_src, $dst_src, $con\\t # vector (sve) (D)\" %}\n@@ -807,2 +1096,2 @@\n-    __ sve_fnmls(as_FloatRegister($dst_src1$$reg), __ S,\n-         ptrue, as_FloatRegister($src2$$reg), as_FloatRegister($src3$$reg));\n+    __ sve_orr(as_FloatRegister($dst_src$$reg), __ D,\n+         (uint64_t)($con$$constant));\n@@ -813,4 +1102,3 @@\n-\/\/ dst_src1 = -dst_src1 + src2 * src3\n-instruct vfnmlsD(vReg dst_src1, vReg src2, vReg src3) %{\n-  predicate(UseFMA && UseSVE > 0);\n-  match(Set dst_src1 (FmaVD (NegVD dst_src1) (Binary src2 src3)));\n+instruct vxorB(vReg dst_src, immBLog con) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src (XorV dst_src (ReplicateB con)));\n@@ -818,1 +1106,1 @@\n-  format %{ \"sve_fnmls $dst_src1, $src2, $src3\\t # vector (sve) (D)\" %}\n+  format %{ \"sve_eor $dst_src, $dst_src, $con\\t # vector (sve) (B)\" %}\n@@ -820,2 +1108,2 @@\n-    __ sve_fnmls(as_FloatRegister($dst_src1$$reg), __ D,\n-         ptrue, as_FloatRegister($src2$$reg), as_FloatRegister($src3$$reg));\n+    __ sve_eor(as_FloatRegister($dst_src$$reg), __ B,\n+         (uint64_t)($con$$constant));\n@@ -826,5 +1114,1 @@\n-\/\/ vector mla\n-\n-\/\/ dst_src1 = dst_src1 + src2 * src3\n-instruct vmlaB(vReg dst_src1, vReg src2, vReg src3)\n-%{\n+instruct vxorH(vReg dst_src, immSLog con) %{\n@@ -832,1 +1116,1 @@\n-  match(Set dst_src1 (AddVB dst_src1 (MulVB src2 src3)));\n+  match(Set dst_src (XorV dst_src (ReplicateS con)));\n@@ -834,1 +1118,1 @@\n-  format %{ \"sve_mla $dst_src1, src2, src3\\t # vector (sve) (B)\" %}\n+  format %{ \"sve_eor $dst_src, $dst_src, $con\\t # vector (sve) (H)\" %}\n@@ -836,2 +1120,2 @@\n-    __ sve_mla(as_FloatRegister($dst_src1$$reg), __ B,\n-      ptrue, as_FloatRegister($src2$$reg), as_FloatRegister($src3$$reg));\n+    __ sve_eor(as_FloatRegister($dst_src$$reg), __ H,\n+         (uint64_t)($con$$constant));\n@@ -842,3 +1126,1 @@\n-\/\/ dst_src1 = dst_src1 + src2 * src3\n-instruct vmlaS(vReg dst_src1, vReg src2, vReg src3)\n-%{\n+instruct vxorS(vReg dst_src, immILog con) %{\n@@ -846,1 +1128,1 @@\n-  match(Set dst_src1 (AddVS dst_src1 (MulVS src2 src3)));\n+  match(Set dst_src (XorV dst_src (ReplicateI con)));\n@@ -848,1 +1130,1 @@\n-  format %{ \"sve_mla $dst_src1, src2, src3\\t # vector (sve) (H)\" %}\n+  format %{ \"sve_eor $dst_src, $dst_src, $con\\t # vector (sve) (S)\" %}\n@@ -850,2 +1132,2 @@\n-    __ sve_mla(as_FloatRegister($dst_src1$$reg), __ H,\n-      ptrue, as_FloatRegister($src2$$reg), as_FloatRegister($src3$$reg));\n+    __ sve_eor(as_FloatRegister($dst_src$$reg), __ S,\n+         (uint64_t)($con$$constant));\n@@ -856,3 +1138,1 @@\n-\/\/ dst_src1 = dst_src1 + src2 * src3\n-instruct vmlaI(vReg dst_src1, vReg src2, vReg src3)\n-%{\n+instruct vxorD(vReg dst_src, immLLog con) %{\n@@ -860,1 +1140,1 @@\n-  match(Set dst_src1 (AddVI dst_src1 (MulVI src2 src3)));\n+  match(Set dst_src (XorV dst_src (ReplicateL con)));\n@@ -862,1 +1142,1 @@\n-  format %{ \"sve_mla $dst_src1, src2, src3\\t # vector (sve) (S)\" %}\n+  format %{ \"sve_eor $dst_src, $dst_src, $con\\t # vector (sve) (D)\" %}\n@@ -864,2 +1144,2 @@\n-    __ sve_mla(as_FloatRegister($dst_src1$$reg), __ S,\n-      ptrue, as_FloatRegister($src2$$reg), as_FloatRegister($src3$$reg));\n+    __ sve_eor(as_FloatRegister($dst_src$$reg), __ D,\n+         (uint64_t)($con$$constant));\n@@ -869,0 +1149,1 @@\n+\/\/ vector and\n@@ -870,3 +1151,1 @@\n-\/\/ dst_src1 = dst_src1 + src2 * src3\n-instruct vmlaL(vReg dst_src1, vReg src2, vReg src3)\n-%{\n+instruct vand(vReg dst, vReg src1, vReg src2) %{\n@@ -874,1 +1153,1 @@\n-  match(Set dst_src1 (AddVL dst_src1 (MulVL src2 src3)));\n+  match(Set dst (AndV src1 src2));\n@@ -876,1 +1155,1 @@\n-  format %{ \"sve_mla $dst_src1, src2, src3\\t # vector (sve) (D)\" %}\n+  format %{ \"sve_and  $dst, $src1, $src2\\t# vector (sve)\" %}\n@@ -878,2 +1157,3 @@\n-    __ sve_mla(as_FloatRegister($dst_src1$$reg), __ D,\n-      ptrue, as_FloatRegister($src2$$reg), as_FloatRegister($src3$$reg));\n+    __ sve_and(as_FloatRegister($dst$$reg),\n+         as_FloatRegister($src1$$reg),\n+         as_FloatRegister($src2$$reg));\n@@ -884,1 +1164,1 @@\n-\/\/ vector mls\n+\/\/ vector or\n@@ -886,3 +1166,1 @@\n-\/\/ dst_src1 = dst_src1 - src2 * src3\n-instruct vmlsB(vReg dst_src1, vReg src2, vReg src3)\n-%{\n+instruct vor(vReg dst, vReg src1, vReg src2) %{\n@@ -890,1 +1168,1 @@\n-  match(Set dst_src1 (SubVB dst_src1 (MulVB src2 src3)));\n+  match(Set dst (OrV src1 src2));\n@@ -892,1 +1170,1 @@\n-  format %{ \"sve_mls $dst_src1, src2, src3\\t # vector (sve) (B)\" %}\n+  format %{ \"sve_orr  $dst, $src1, $src2\\t# vector (sve)\" %}\n@@ -894,2 +1172,3 @@\n-    __ sve_mls(as_FloatRegister($dst_src1$$reg), __ B,\n-      ptrue, as_FloatRegister($src2$$reg), as_FloatRegister($src3$$reg));\n+    __ sve_orr(as_FloatRegister($dst$$reg),\n+         as_FloatRegister($src1$$reg),\n+         as_FloatRegister($src2$$reg));\n@@ -900,3 +1179,3 @@\n-\/\/ dst_src1 = dst_src1 - src2 * src3\n-instruct vmlsS(vReg dst_src1, vReg src2, vReg src3)\n-%{\n+\/\/ vector xor\n+\n+instruct vxor(vReg dst, vReg src1, vReg src2) %{\n@@ -904,1 +1183,1 @@\n-  match(Set dst_src1 (SubVS dst_src1 (MulVS src2 src3)));\n+  match(Set dst (XorV src1 src2));\n@@ -906,1 +1185,1 @@\n-  format %{ \"sve_mls $dst_src1, src2, src3\\t # vector (sve) (H)\" %}\n+  format %{ \"sve_eor  $dst, $src1, $src2\\t# vector (sve)\" %}\n@@ -908,2 +1187,3 @@\n-    __ sve_mls(as_FloatRegister($dst_src1$$reg), __ H,\n-      ptrue, as_FloatRegister($src2$$reg), as_FloatRegister($src3$$reg));\n+    __ sve_eor(as_FloatRegister($dst$$reg),\n+         as_FloatRegister($src1$$reg),\n+         as_FloatRegister($src2$$reg));\n@@ -914,3 +1194,3 @@\n-\/\/ dst_src1 = dst_src1 - src2 * src3\n-instruct vmlsI(vReg dst_src1, vReg src2, vReg src3)\n-%{\n+\/\/ vector and - predicated\n+\n+instruct vand_masked(vReg dst_src1, vReg src2, pRegGov pg) %{\n@@ -918,1 +1198,1 @@\n-  match(Set dst_src1 (SubVI dst_src1 (MulVI src2 src3)));\n+  match(Set dst_src1 (AndV (Binary dst_src1 src2) pg));\n@@ -920,1 +1200,1 @@\n-  format %{ \"sve_mls $dst_src1, src2, src3\\t # vector (sve) (S)\" %}\n+  format %{ \"sve_and $dst_src1, $pg, $dst_src1, $src2\\t # vector (sve)\" %}\n@@ -922,2 +1202,5 @@\n-    __ sve_mls(as_FloatRegister($dst_src1$$reg), __ S,\n-      ptrue, as_FloatRegister($src2$$reg), as_FloatRegister($src3$$reg));\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    Assembler::SIMD_RegVariant size = __ elemType_to_regVariant(bt);\n+    __ sve_and(as_FloatRegister($dst_src1$$reg), size,\n+          as_PRegister($pg$$reg),\n+          as_FloatRegister($src2$$reg));\n@@ -928,3 +1211,3 @@\n-\/\/ dst_src1 = dst_src1 - src2 * src3\n-instruct vmlsL(vReg dst_src1, vReg src2, vReg src3)\n-%{\n+\/\/ vector or - predicated\n+\n+instruct vor_masked(vReg dst_src1, vReg src2, pRegGov pg) %{\n@@ -932,1 +1215,1 @@\n-  match(Set dst_src1 (SubVL dst_src1 (MulVL src2 src3)));\n+  match(Set dst_src1 (OrV (Binary dst_src1 src2) pg));\n@@ -934,1 +1217,1 @@\n-  format %{ \"sve_mls $dst_src1, src2, src3\\t # vector (sve) (D)\" %}\n+  format %{ \"sve_orr $dst_src1, $pg, $dst_src1, $src2\\t # vector (sve)\" %}\n@@ -936,2 +1219,5 @@\n-    __ sve_mls(as_FloatRegister($dst_src1$$reg), __ D,\n-      ptrue, as_FloatRegister($src2$$reg), as_FloatRegister($src3$$reg));\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    Assembler::SIMD_RegVariant size = __ elemType_to_regVariant(bt);\n+    __ sve_orr(as_FloatRegister($dst_src1$$reg), size,\n+          as_PRegister($pg$$reg),\n+          as_FloatRegister($src2$$reg));\n@@ -942,0 +1228,1 @@\n+\/\/ vector xor - predicated\n@@ -943,3 +1230,1 @@\n-\/\/ vector mul\n-\n-instruct vmulB(vReg dst_src1, vReg src2) %{\n+instruct vxor_masked(vReg dst_src1, vReg src2, pRegGov pg) %{\n@@ -947,1 +1232,1 @@\n-  match(Set dst_src1 (MulVB dst_src1 src2));\n+  match(Set dst_src1 (XorV (Binary dst_src1 src2) pg));\n@@ -949,1 +1234,1 @@\n-  format %{ \"sve_mul $dst_src1, $dst_src1, $src2\\t # vector (sve) (B)\" %}\n+  format %{ \"sve_eor $dst_src1, $pg, $dst_src1, $src2\\t # vector (sve)\" %}\n@@ -951,2 +1236,5 @@\n-    __ sve_mul(as_FloatRegister($dst_src1$$reg), __ B,\n-         ptrue, as_FloatRegister($src2$$reg));\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    Assembler::SIMD_RegVariant size = __ elemType_to_regVariant(bt);\n+    __ sve_eor(as_FloatRegister($dst_src1$$reg), size,\n+          as_PRegister($pg$$reg),\n+          as_FloatRegister($src2$$reg));\n@@ -957,1 +1245,3 @@\n-instruct vmulS(vReg dst_src1, vReg src2) %{\n+\/\/ vector not\n+\n+instruct vnotI(vReg dst, vReg src, immI_M1 m1) %{\n@@ -959,1 +1249,3 @@\n-  match(Set dst_src1 (MulVS dst_src1 src2));\n+  match(Set dst (XorV src (ReplicateB m1)));\n+  match(Set dst (XorV src (ReplicateS m1)));\n+  match(Set dst (XorV src (ReplicateI m1)));\n@@ -961,1 +1253,1 @@\n-  format %{ \"sve_mul $dst_src1, $dst_src1, $src2\\t # vector (sve) (H)\" %}\n+  format %{ \"sve_not $dst, $src\\t# vector (sve) B\/H\/S\" %}\n@@ -963,2 +1255,2 @@\n-    __ sve_mul(as_FloatRegister($dst_src1$$reg), __ H,\n-         ptrue, as_FloatRegister($src2$$reg));\n+    __ sve_not(as_FloatRegister($dst$$reg), __ D,\n+               ptrue, as_FloatRegister($src$$reg));\n@@ -969,1 +1261,1 @@\n-instruct vmulI(vReg dst_src1, vReg src2) %{\n+instruct vnotL(vReg dst, vReg src, immL_M1 m1) %{\n@@ -971,1 +1263,1 @@\n-  match(Set dst_src1 (MulVI dst_src1 src2));\n+  match(Set dst (XorV src (ReplicateL m1)));\n@@ -973,1 +1265,1 @@\n-  format %{ \"sve_mul $dst_src1, $dst_src1, $src2\\t # vector (sve) (S)\" %}\n+  format %{ \"sve_not $dst, $src\\t# vector (sve) D\" %}\n@@ -975,2 +1267,2 @@\n-    __ sve_mul(as_FloatRegister($dst_src1$$reg), __ S,\n-         ptrue, as_FloatRegister($src2$$reg));\n+    __ sve_not(as_FloatRegister($dst$$reg), __ D,\n+               ptrue, as_FloatRegister($src$$reg));\n@@ -981,1 +1273,3 @@\n-instruct vmulL(vReg dst_src1, vReg src2) %{\n+\/\/ vector and_not\n+\n+instruct vand_notI(vReg dst, vReg src1, vReg src2, immI_M1 m1) %{\n@@ -983,1 +1277,3 @@\n-  match(Set dst_src1 (MulVL dst_src1 src2));\n+  match(Set dst (AndV src1 (XorV src2 (ReplicateB m1))));\n+  match(Set dst (AndV src1 (XorV src2 (ReplicateS m1))));\n+  match(Set dst (AndV src1 (XorV src2 (ReplicateI m1))));\n@@ -985,1 +1281,1 @@\n-  format %{ \"sve_mul $dst_src1, $dst_src1, $src2\\t # vector (sve) (D)\" %}\n+  format %{ \"sve_bic $dst, $src1, $src2\\t# vector (sve) B\/H\/S\" %}\n@@ -987,2 +1283,3 @@\n-    __ sve_mul(as_FloatRegister($dst_src1$$reg), __ D,\n-         ptrue, as_FloatRegister($src2$$reg));\n+    __ sve_bic(as_FloatRegister($dst$$reg),\n+               as_FloatRegister($src1$$reg),\n+               as_FloatRegister($src2$$reg));\n@@ -993,1 +1290,1 @@\n-instruct vmulF(vReg dst, vReg src1, vReg src2) %{\n+instruct vand_notL(vReg dst, vReg src1, vReg src2, immL_M1 m1) %{\n@@ -995,1 +1292,1 @@\n-  match(Set dst (MulVF src1 src2));\n+  match(Set dst (AndV src1 (XorV src2 (ReplicateL m1))));\n@@ -997,1 +1294,1 @@\n-  format %{ \"sve_fmul $dst, $src1, $src2\\t # vector (sve) (S)\" %}\n+  format %{ \"sve_bic $dst, $src1, $src2\\t# vector (sve) D\" %}\n@@ -999,3 +1296,3 @@\n-    __ sve_fmul(as_FloatRegister($dst$$reg), __ S,\n-         as_FloatRegister($src1$$reg),\n-         as_FloatRegister($src2$$reg));\n+    __ sve_bic(as_FloatRegister($dst$$reg),\n+               as_FloatRegister($src1$$reg),\n+               as_FloatRegister($src2$$reg));\n@@ -1006,1 +1303,3 @@\n-instruct vmulD(vReg dst, vReg src1, vReg src2) %{\n+\/\/ vector float div\n+\n+instruct vdivF(vReg dst_src1, vReg src2) %{\n@@ -1008,1 +1307,1 @@\n-  match(Set dst (MulVD src1 src2));\n+  match(Set dst_src1 (DivVF dst_src1 src2));\n@@ -1010,1 +1309,1 @@\n-  format %{ \"sve_fmul $dst, $src1, $src2\\t # vector (sve) (D)\" %}\n+  format %{ \"sve_fdiv  $dst_src1, $dst_src1, $src2\\t# vector (sve) (S)\" %}\n@@ -1012,3 +1311,2 @@\n-    __ sve_fmul(as_FloatRegister($dst$$reg), __ D,\n-         as_FloatRegister($src1$$reg),\n-         as_FloatRegister($src2$$reg));\n+    __ sve_fdiv(as_FloatRegister($dst_src1$$reg), __ S,\n+         ptrue, as_FloatRegister($src2$$reg));\n@@ -1019,1 +1317,11 @@\n-\/\/ vector fneg\n+instruct vdivD(vReg dst_src1, vReg src2) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src1 (DivVD dst_src1 src2));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_fdiv  $dst_src1, $dst_src1, $src2\\t# vector (sve) (D)\" %}\n+  ins_encode %{\n+    __ sve_fdiv(as_FloatRegister($dst_src1$$reg), __ D,\n+         ptrue, as_FloatRegister($src2$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n@@ -1021,1 +1329,3 @@\n-instruct vnegF(vReg dst, vReg src) %{\n+\/\/ vector float div - predicated\n+\n+instruct vfdivF_masked(vReg dst_src1, vReg src2, pRegGov pg) %{\n@@ -1023,1 +1333,1 @@\n-  match(Set dst (NegVF src));\n+  match(Set dst_src1 (DivVF (Binary dst_src1 src2) pg));\n@@ -1025,1 +1335,1 @@\n-  format %{ \"sve_fneg $dst, $src\\t# vector (sve) (S)\" %}\n+  format %{ \"sve_fdiv $dst_src1, $pg, $dst_src1, $src2\\t# vector (sve) (S)\" %}\n@@ -1027,2 +1337,3 @@\n-    __ sve_fneg(as_FloatRegister($dst$$reg), __ S,\n-         ptrue, as_FloatRegister($src$$reg));\n+    __ sve_fdiv(as_FloatRegister($dst_src1$$reg), __ S,\n+            as_PRegister($pg$$reg),\n+            as_FloatRegister($src2$$reg));\n@@ -1033,1 +1344,1 @@\n-instruct vnegD(vReg dst, vReg src) %{\n+instruct vfdivD_masked(vReg dst_src1, vReg src2, pRegGov pg) %{\n@@ -1035,1 +1346,1 @@\n-  match(Set dst (NegVD src));\n+  match(Set dst_src1 (DivVD (Binary dst_src1 src2) pg));\n@@ -1037,1 +1348,1 @@\n-  format %{ \"sve_fneg $dst, $src\\t# vector (sve) (D)\" %}\n+  format %{ \"sve_fdiv $dst_src1, $pg, $dst_src1, $src2\\t# vector (sve) (D)\" %}\n@@ -1039,2 +1350,3 @@\n-    __ sve_fneg(as_FloatRegister($dst$$reg), __ D,\n-         ptrue, as_FloatRegister($src$$reg));\n+    __ sve_fdiv(as_FloatRegister($dst_src1$$reg), __ D,\n+            as_PRegister($pg$$reg),\n+            as_FloatRegister($src2$$reg));\n@@ -1045,1 +1357,1 @@\n-\/\/ popcount vector\n+\/\/ vector min\/max\n@@ -1047,1 +1359,1 @@\n-instruct vpopcountI(vReg dst, vReg src) %{\n+instruct vmin(vReg dst_src1, vReg src2) %{\n@@ -1049,2 +1361,3 @@\n-  match(Set dst (PopCountVI src));\n-  format %{ \"sve_cnt $dst, $src\\t# vector (sve) (S)\\n\\t\" %}\n+  match(Set dst_src1 (MinV dst_src1 src2));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_min $dst_src1, $dst_src1, $src2\\t # vector (sve)\" %}\n@@ -1052,1 +1365,10 @@\n-     __ sve_cnt(as_FloatRegister($dst$$reg), __ S, ptrue, as_FloatRegister($src$$reg));\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    Assembler::SIMD_RegVariant size = __ elemType_to_regVariant(bt);\n+    if (is_floating_point_type(bt)) {\n+      __ sve_fmin(as_FloatRegister($dst_src1$$reg), size,\n+                  ptrue, as_FloatRegister($src2$$reg));\n+    } else {\n+      assert(is_integral_type(bt), \"unsupported type\");\n+      __ sve_smin(as_FloatRegister($dst_src1$$reg), size,\n+                  ptrue, as_FloatRegister($src2$$reg));\n+    }\n@@ -1057,3 +1379,1 @@\n-\/\/ vector mask compare\n-\n-instruct vmaskcmp(vReg dst, vReg src1, vReg src2, immI cond, pRegGov pTmp, rFlagsReg cr) %{\n+instruct vmax(vReg dst_src1, vReg src2) %{\n@@ -1061,5 +1381,3 @@\n-  match(Set dst (VectorMaskCmp (Binary src1 src2) cond));\n-  effect(TEMP pTmp, KILL cr);\n-  ins_cost(2 * SVE_COST);\n-  format %{ \"sve_cmp $pTmp, $src1, $src2\\n\\t\"\n-            \"sve_cpy $dst, $pTmp, -1\\t# vector mask cmp (sve)\" %}\n+  match(Set dst_src1 (MaxV dst_src1 src2));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_max $dst_src1, $dst_src1, $src2\\t # vector (sve)\" %}\n@@ -1068,4 +1386,9 @@\n-    __ sve_compare(as_PRegister($pTmp$$reg), bt, ptrue, as_FloatRegister($src1$$reg),\n-                   as_FloatRegister($src2$$reg), (int)$cond$$constant);\n-    __ sve_cpy(as_FloatRegister($dst$$reg), __ elemType_to_regVariant(bt),\n-               as_PRegister($pTmp$$reg), -1, false);\n+    Assembler::SIMD_RegVariant size = __ elemType_to_regVariant(bt);\n+    if (is_floating_point_type(bt)) {\n+      __ sve_fmax(as_FloatRegister($dst_src1$$reg), size,\n+                  ptrue, as_FloatRegister($src2$$reg));\n+    } else {\n+      assert(is_integral_type(bt), \"unsupported type\");\n+      __ sve_smax(as_FloatRegister($dst_src1$$reg), size,\n+                  ptrue, as_FloatRegister($src2$$reg));\n+    }\n@@ -1076,1 +1399,1 @@\n-\/\/ vector blend\n+\/\/ vector min\/max - predicated\n@@ -1078,1 +1401,1 @@\n-instruct vblend(vReg dst, vReg src1, vReg src2, vReg src3, pRegGov pTmp, rFlagsReg cr) %{\n+instruct vmin_masked(vReg dst_src1, vReg src2, pRegGov pg) %{\n@@ -1080,5 +1403,3 @@\n-  match(Set dst (VectorBlend (Binary src1 src2) src3));\n-  effect(TEMP pTmp, KILL cr);\n-  ins_cost(2 * SVE_COST);\n-  format %{ \"sve_cmpeq $pTmp, $src3, -1\\n\\t\"\n-            \"sve_sel $dst, $pTmp, $src2, $src1\\t# vector blend (sve)\" %}\n+  match(Set dst_src1 (MinV (Binary dst_src1 src2) pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_min $dst_src1, $pg, $dst_src1, $src2\\t# vector (sve)\" %}\n@@ -1086,6 +1407,10 @@\n-    Assembler::SIMD_RegVariant size =\n-      __ elemType_to_regVariant(Matcher::vector_element_basic_type(this));\n-    __ sve_cmp(Assembler::EQ, as_PRegister($pTmp$$reg), size,\n-               ptrue, as_FloatRegister($src3$$reg), -1);\n-    __ sve_sel(as_FloatRegister($dst$$reg), size, as_PRegister($pTmp$$reg),\n-               as_FloatRegister($src2$$reg), as_FloatRegister($src1$$reg));\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    Assembler::SIMD_RegVariant size = __ elemType_to_regVariant(bt);\n+    if (is_floating_point_type(bt)) {\n+      __ sve_fmin(as_FloatRegister($dst_src1$$reg), size,\n+                  as_PRegister($pg$$reg), as_FloatRegister($src2$$reg));\n+    } else {\n+      assert(is_integral_type(bt), \"unsupported type\");\n+      __ sve_smin(as_FloatRegister($dst_src1$$reg), size,\n+                  as_PRegister($pg$$reg), as_FloatRegister($src2$$reg));\n+    }\n@@ -1096,4 +1421,1 @@\n-\/\/ vector blend with compare\n-\n-instruct vblend_maskcmp(vReg dst, vReg src1, vReg src2, vReg src3,\n-                        vReg src4, pRegGov pTmp, immI cond, rFlagsReg cr) %{\n+instruct vmax_masked(vReg dst_src1, vReg src2, pRegGov pg) %{\n@@ -1101,5 +1423,3 @@\n-  match(Set dst (VectorBlend (Binary src1 src2) (VectorMaskCmp (Binary src3 src4) cond)));\n-  effect(TEMP pTmp, KILL cr);\n-  ins_cost(2 * SVE_COST);\n-  format %{ \"sve_cmp $pTmp, $src3, $src4\\t# vector cmp (sve)\\n\\t\"\n-            \"sve_sel $dst, $pTmp, $src2, $src1\\t# vector blend (sve)\" %}\n+  match(Set dst_src1 (MaxV (Binary dst_src1 src2) pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_max $dst_src1, $pg, $dst_src1, $src2\\t# vector (sve)\" %}\n@@ -1108,5 +1428,9 @@\n-    __ sve_compare(as_PRegister($pTmp$$reg), bt, ptrue, as_FloatRegister($src3$$reg),\n-                   as_FloatRegister($src4$$reg), (int)$cond$$constant);\n-    __ sve_sel(as_FloatRegister($dst$$reg), __ elemType_to_regVariant(bt),\n-               as_PRegister($pTmp$$reg), as_FloatRegister($src2$$reg),\n-               as_FloatRegister($src1$$reg));\n+    Assembler::SIMD_RegVariant size = __ elemType_to_regVariant(bt);\n+    if (is_floating_point_type(bt)) {\n+      __ sve_fmax(as_FloatRegister($dst_src1$$reg), size,\n+                  as_PRegister($pg$$reg), as_FloatRegister($src2$$reg));\n+    } else {\n+      assert(is_integral_type(bt), \"unsupported type\");\n+      __ sve_smax(as_FloatRegister($dst_src1$$reg), size,\n+                  as_PRegister($pg$$reg), as_FloatRegister($src2$$reg));\n+    }\n@@ -1117,1 +1441,1 @@\n-\/\/ vector load mask\n+\/\/ vector fmla\n@@ -1119,4 +1443,4 @@\n-instruct vloadmaskB(vReg dst, vReg src) %{\n-  predicate(UseSVE > 0 &&\n-            n->bottom_type()->is_vect()->element_basic_type() == T_BYTE);\n-  match(Set dst (VectorLoadMask src));\n+\/\/ dst_src1 = dst_src1 + src2 * src3\n+instruct vfmlaF(vReg dst_src1, vReg src2, vReg src3) %{\n+  predicate(UseFMA && UseSVE > 0);\n+  match(Set dst_src1 (FmaVF dst_src1 (Binary src2 src3)));\n@@ -1124,1 +1448,1 @@\n-  format %{ \"sve_neg $dst, $src\\t# vector load mask (B)\" %}\n+  format %{ \"sve_fmla $dst_src1, $src2, $src3\\t # vector (sve) (S)\" %}\n@@ -1126,1 +1450,2 @@\n-    __ sve_neg(as_FloatRegister($dst$$reg), __ B, ptrue, as_FloatRegister($src$$reg));\n+    __ sve_fmla(as_FloatRegister($dst_src1$$reg), __ S,\n+         ptrue, as_FloatRegister($src2$$reg), as_FloatRegister($src3$$reg));\n@@ -1131,7 +1456,6 @@\n-instruct vloadmaskS(vReg dst, vReg src) %{\n-  predicate(UseSVE > 0 &&\n-            n->bottom_type()->is_vect()->element_basic_type() == T_SHORT);\n-  match(Set dst (VectorLoadMask src));\n-  ins_cost(2 * SVE_COST);\n-  format %{ \"sve_uunpklo $dst, H, $src\\n\\t\"\n-            \"sve_neg $dst, $dst\\t# vector load mask (B to H)\" %}\n+\/\/ dst_src1 = dst_src1 + src2 * src3\n+instruct vfmlaD(vReg dst_src1, vReg src2, vReg src3) %{\n+  predicate(UseFMA && UseSVE > 0);\n+  match(Set dst_src1 (FmaVD dst_src1 (Binary src2 src3)));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_fmla $dst_src1, $src2, $src3\\t # vector (sve) (D)\" %}\n@@ -1139,2 +1463,2 @@\n-    __ sve_uunpklo(as_FloatRegister($dst$$reg), __ H, as_FloatRegister($src$$reg));\n-    __ sve_neg(as_FloatRegister($dst$$reg), __ H, ptrue, as_FloatRegister($dst$$reg));\n+    __ sve_fmla(as_FloatRegister($dst_src1$$reg), __ D,\n+         ptrue, as_FloatRegister($src2$$reg), as_FloatRegister($src3$$reg));\n@@ -1145,9 +1469,8 @@\n-instruct vloadmaskI(vReg dst, vReg src) %{\n-  predicate(UseSVE > 0 &&\n-            (n->bottom_type()->is_vect()->element_basic_type() == T_INT ||\n-             n->bottom_type()->is_vect()->element_basic_type() == T_FLOAT));\n-  match(Set dst (VectorLoadMask src));\n-  ins_cost(3 * SVE_COST);\n-  format %{ \"sve_uunpklo $dst, H, $src\\n\\t\"\n-            \"sve_uunpklo $dst, S, $dst\\n\\t\"\n-            \"sve_neg $dst, $dst\\t# vector load mask (B to S)\" %}\n+\/\/ vector fmla - predicated\n+\n+\/\/ dst_src1 = dst_src1 * src2 + src3\n+instruct vfmlaF_masked(vReg dst_src1, vReg src2, vReg src3, pRegGov pg) %{\n+  predicate(UseFMA && UseSVE > 0);\n+  match(Set dst_src1 (FmaVF (Binary dst_src1 src2) (Binary src3 pg)));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_fmad $dst_src1, $pg, $src2, $src3\\t# vector (sve) (S)\" %}\n@@ -1155,3 +1478,2 @@\n-    __ sve_uunpklo(as_FloatRegister($dst$$reg), __ H, as_FloatRegister($src$$reg));\n-    __ sve_uunpklo(as_FloatRegister($dst$$reg), __ S, as_FloatRegister($dst$$reg));\n-    __ sve_neg(as_FloatRegister($dst$$reg), __ S, ptrue, as_FloatRegister($dst$$reg));\n+    __ sve_fmad(as_FloatRegister($dst_src1$$reg), __ S, as_PRegister($pg$$reg),\n+         as_FloatRegister($src2$$reg), as_FloatRegister($src3$$reg));\n@@ -1162,10 +1484,6 @@\n-instruct vloadmaskL(vReg dst, vReg src) %{\n-  predicate(UseSVE > 0 &&\n-            (n->bottom_type()->is_vect()->element_basic_type() == T_LONG ||\n-             n->bottom_type()->is_vect()->element_basic_type() == T_DOUBLE));\n-  match(Set dst (VectorLoadMask src));\n-  ins_cost(4 * SVE_COST);\n-  format %{ \"sve_uunpklo $dst, H, $src\\n\\t\"\n-            \"sve_uunpklo $dst, S, $dst\\n\\t\"\n-            \"sve_uunpklo $dst, D, $dst\\n\\t\"\n-            \"sve_neg $dst, $dst\\t# vector load mask (B to D)\" %}\n+\/\/ dst_src1 = dst_src1 * src2 + src3\n+instruct vfmlaD_masked(vReg dst_src1, vReg src2, vReg src3, pRegGov pg) %{\n+  predicate(UseFMA && UseSVE > 0);\n+  match(Set dst_src1 (FmaVD (Binary dst_src1 src2) (Binary src3 pg)));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_fmad $dst_src1, $pg, $src2, $src3\\t# vector (sve) (D)\" %}\n@@ -1173,4 +1491,2 @@\n-    __ sve_uunpklo(as_FloatRegister($dst$$reg), __ H, as_FloatRegister($src$$reg));\n-    __ sve_uunpklo(as_FloatRegister($dst$$reg), __ S, as_FloatRegister($dst$$reg));\n-    __ sve_uunpklo(as_FloatRegister($dst$$reg), __ D, as_FloatRegister($dst$$reg));\n-    __ sve_neg(as_FloatRegister($dst$$reg), __ D, ptrue, as_FloatRegister($dst$$reg));\n+    __ sve_fmad(as_FloatRegister($dst_src1$$reg), __ D, as_PRegister($pg$$reg),\n+         as_FloatRegister($src2$$reg), as_FloatRegister($src3$$reg));\n@@ -1181,1 +1497,1 @@\n-\/\/ vector store mask\n+\/\/ vector fmls\n@@ -1183,3 +1499,6 @@\n-instruct vstoremaskB(vReg dst, vReg src, immI_1 size) %{\n-  predicate(UseSVE > 0);\n-  match(Set dst (VectorStoreMask src size));\n+\/\/ dst_src1 = dst_src1 + -src2 * src3\n+\/\/ dst_src1 = dst_src1 + src2 * -src3\n+instruct vfmlsF(vReg dst_src1, vReg src2, vReg src3) %{\n+  predicate(UseFMA && UseSVE > 0);\n+  match(Set dst_src1 (FmaVF dst_src1 (Binary (NegVF src2) src3)));\n+  match(Set dst_src1 (FmaVF dst_src1 (Binary src2 (NegVF src3))));\n@@ -1187,1 +1506,1 @@\n-  format %{ \"sve_neg $dst, $src\\t# vector store mask (B)\" %}\n+  format %{ \"sve_fmls $dst_src1, $src2, $src3\\t # vector (sve) (S)\" %}\n@@ -1189,2 +1508,2 @@\n-    __ sve_neg(as_FloatRegister($dst$$reg), __ B, ptrue,\n-               as_FloatRegister($src$$reg));\n+    __ sve_fmls(as_FloatRegister($dst_src1$$reg), __ S,\n+         ptrue, as_FloatRegister($src2$$reg), as_FloatRegister($src3$$reg));\n@@ -1195,8 +1514,8 @@\n-instruct vstoremaskS(vReg dst, vReg src, vReg tmp, immI_2 size) %{\n-  predicate(UseSVE > 0);\n-  match(Set dst (VectorStoreMask src size));\n-  effect(TEMP_DEF dst, TEMP tmp);\n-  ins_cost(3 * SVE_COST);\n-  format %{ \"sve_dup $tmp, H, 0\\n\\t\"\n-            \"sve_uzp1 $dst, B, $src, $tmp\\n\\t\"\n-            \"sve_neg $dst, B, $dst\\t# vector store mask (sve) (H to B)\" %}\n+\/\/ dst_src1 = dst_src1 + -src2 * src3\n+\/\/ dst_src1 = dst_src1 + src2 * -src3\n+instruct vfmlsD(vReg dst_src1, vReg src2, vReg src3) %{\n+  predicate(UseFMA && UseSVE > 0);\n+  match(Set dst_src1 (FmaVD dst_src1 (Binary (NegVD src2) src3)));\n+  match(Set dst_src1 (FmaVD dst_src1 (Binary src2 (NegVD src3))));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_fmls $dst_src1, $src2, $src3\\t # vector (sve) (D)\" %}\n@@ -1204,5 +1523,7 @@\n-    __ sve_dup(as_FloatRegister($tmp$$reg), __ H, 0);\n-    __ sve_uzp1(as_FloatRegister($dst$$reg), __ B,\n-                as_FloatRegister($src$$reg), as_FloatRegister($tmp$$reg));\n-    __ sve_neg(as_FloatRegister($dst$$reg), __ B, ptrue,\n-               as_FloatRegister($dst$$reg));\n+    __ sve_fmls(as_FloatRegister($dst_src1$$reg), __ D,\n+         ptrue, as_FloatRegister($src2$$reg), as_FloatRegister($src3$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ vector fnmla\n@@ -1210,0 +1531,11 @@\n+\/\/ dst_src1 = -dst_src1 + -src2 * src3\n+\/\/ dst_src1 = -dst_src1 + src2 * -src3\n+instruct vfnmlaF(vReg dst_src1, vReg src2, vReg src3) %{\n+  predicate(UseFMA && UseSVE > 0);\n+  match(Set dst_src1 (FmaVF (NegVF dst_src1) (Binary (NegVF src2) src3)));\n+  match(Set dst_src1 (FmaVF (NegVF dst_src1) (Binary src2 (NegVF src3))));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_fnmla $dst_src1, $src2, $src3\\t # vector (sve) (S)\" %}\n+  ins_encode %{\n+    __ sve_fnmla(as_FloatRegister($dst_src1$$reg), __ S,\n+         ptrue, as_FloatRegister($src2$$reg), as_FloatRegister($src3$$reg));\n@@ -1214,9 +1546,36 @@\n-instruct vstoremaskI(vReg dst, vReg src, vReg tmp, immI_4 size) %{\n-  predicate(UseSVE > 0);\n-  match(Set dst (VectorStoreMask src size));\n-  effect(TEMP_DEF dst, TEMP tmp);\n-  ins_cost(4 * SVE_COST);\n-  format %{ \"sve_dup $tmp, S, 0\\n\\t\"\n-            \"sve_uzp1 $dst, H, $src, $tmp\\n\\t\"\n-            \"sve_uzp1 $dst, B, $dst, $tmp\\n\\t\"\n-            \"sve_neg $dst, B, $dst\\t# vector store mask (sve) (S to B)\" %}\n+\/\/ dst_src1 = -dst_src1 + -src2 * src3\n+\/\/ dst_src1 = -dst_src1 + src2 * -src3\n+instruct vfnmlaD(vReg dst_src1, vReg src2, vReg src3) %{\n+  predicate(UseFMA && UseSVE > 0);\n+  match(Set dst_src1 (FmaVD (NegVD dst_src1) (Binary (NegVD src2) src3)));\n+  match(Set dst_src1 (FmaVD (NegVD dst_src1) (Binary src2 (NegVD src3))));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_fnmla $dst_src1, $src2, $src3\\t # vector (sve) (D)\" %}\n+  ins_encode %{\n+    __ sve_fnmla(as_FloatRegister($dst_src1$$reg), __ D,\n+         ptrue, as_FloatRegister($src2$$reg), as_FloatRegister($src3$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ vector fnmls\n+\n+\/\/ dst_src1 = -dst_src1 + src2 * src3\n+instruct vfnmlsF(vReg dst_src1, vReg src2, vReg src3) %{\n+  predicate(UseFMA && UseSVE > 0);\n+  match(Set dst_src1 (FmaVF (NegVF dst_src1) (Binary src2 src3)));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_fnmls $dst_src1, $src2, $src3\\t # vector (sve) (S)\" %}\n+  ins_encode %{\n+    __ sve_fnmls(as_FloatRegister($dst_src1$$reg), __ S,\n+         ptrue, as_FloatRegister($src2$$reg), as_FloatRegister($src3$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ dst_src1 = -dst_src1 + src2 * src3\n+instruct vfnmlsD(vReg dst_src1, vReg src2, vReg src3) %{\n+  predicate(UseFMA && UseSVE > 0);\n+  match(Set dst_src1 (FmaVD (NegVD dst_src1) (Binary src2 src3)));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_fnmls $dst_src1, $src2, $src3\\t # vector (sve) (D)\" %}\n@@ -1224,7 +1583,2 @@\n-    __ sve_dup(as_FloatRegister($tmp$$reg), __ S, 0);\n-    __ sve_uzp1(as_FloatRegister($dst$$reg), __ H,\n-                as_FloatRegister($src$$reg), as_FloatRegister($tmp$$reg));\n-    __ sve_uzp1(as_FloatRegister($dst$$reg), __ B,\n-                as_FloatRegister($dst$$reg), as_FloatRegister($tmp$$reg));\n-    __ sve_neg(as_FloatRegister($dst$$reg), __ B, ptrue,\n-               as_FloatRegister($dst$$reg));\n+    __ sve_fnmls(as_FloatRegister($dst_src1$$reg), __ D,\n+         ptrue, as_FloatRegister($src2$$reg), as_FloatRegister($src3$$reg));\n@@ -1235,1 +1589,5 @@\n-instruct vstoremaskL(vReg dst, vReg src, vReg tmp, immI_8 size) %{\n+\/\/ vector mla\n+\n+\/\/ dst_src1 = dst_src1 + src2 * src3\n+instruct vmlaB(vReg dst_src1, vReg src2, vReg src3)\n+%{\n@@ -1237,8 +1595,3 @@\n-  match(Set dst (VectorStoreMask src size));\n-  effect(TEMP_DEF dst, TEMP tmp);\n-  ins_cost(5 * SVE_COST);\n-  format %{ \"sve_dup $tmp, D, 0\\n\\t\"\n-            \"sve_uzp1 $dst, S, $src, $tmp\\n\\t\"\n-            \"sve_uzp1 $dst, H, $dst, $tmp\\n\\t\"\n-            \"sve_uzp1 $dst, B, $dst, $tmp\\n\\t\"\n-            \"sve_neg $dst, B, $dst\\t# vector store mask (sve) (D to B)\" %}\n+  match(Set dst_src1 (AddVB dst_src1 (MulVB src2 src3)));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_mla $dst_src1, src2, src3\\t # vector (sve) (B)\" %}\n@@ -1246,9 +1599,2 @@\n-    __ sve_dup(as_FloatRegister($tmp$$reg), __ D, 0);\n-    __ sve_uzp1(as_FloatRegister($dst$$reg), __ S,\n-                as_FloatRegister($src$$reg), as_FloatRegister($tmp$$reg));\n-    __ sve_uzp1(as_FloatRegister($dst$$reg), __ H,\n-                as_FloatRegister($dst$$reg), as_FloatRegister($tmp$$reg));\n-    __ sve_uzp1(as_FloatRegister($dst$$reg), __ B,\n-                as_FloatRegister($dst$$reg), as_FloatRegister($tmp$$reg));\n-    __ sve_neg(as_FloatRegister($dst$$reg), __ B, ptrue,\n-               as_FloatRegister($dst$$reg));\n+    __ sve_mla(as_FloatRegister($dst_src1$$reg), __ B,\n+      ptrue, as_FloatRegister($src2$$reg), as_FloatRegister($src3$$reg));\n@@ -1259,1 +1605,13 @@\n-\/\/ load\/store mask vector\n+\/\/ dst_src1 = dst_src1 + src2 * src3\n+instruct vmlaS(vReg dst_src1, vReg src2, vReg src3)\n+%{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src1 (AddVS dst_src1 (MulVS src2 src3)));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_mla $dst_src1, src2, src3\\t # vector (sve) (H)\" %}\n+  ins_encode %{\n+    __ sve_mla(as_FloatRegister($dst_src1$$reg), __ H,\n+      ptrue, as_FloatRegister($src2$$reg), as_FloatRegister($src3$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n@@ -1261,7 +1619,7 @@\n-instruct vloadmask_loadV_byte(vReg dst, vmemA mem) %{\n-  predicate(UseSVE > 0 && n->as_Vector()->length_in_bytes() == MaxVectorSize &&\n-            type2aelembytes(n->bottom_type()->is_vect()->element_basic_type()) == 1);\n-  match(Set dst (VectorLoadMask (LoadVector mem)));\n-  ins_cost(5 * SVE_COST);\n-  format %{ \"sve_ld1b $dst, $mem\\n\\t\"\n-            \"sve_neg $dst, $dst\\t# load vector mask (sve)\" %}\n+\/\/ dst_src1 = dst_src1 + src2 * src3\n+instruct vmlaI(vReg dst_src1, vReg src2, vReg src3)\n+%{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src1 (AddVI dst_src1 (MulVI src2 src3)));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_mla $dst_src1, src2, src3\\t # vector (sve) (S)\" %}\n@@ -1269,1 +1627,421 @@\n-    FloatRegister dst_reg = as_FloatRegister($dst$$reg);\n+    __ sve_mla(as_FloatRegister($dst_src1$$reg), __ S,\n+      ptrue, as_FloatRegister($src2$$reg), as_FloatRegister($src3$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ dst_src1 = dst_src1 + src2 * src3\n+instruct vmlaL(vReg dst_src1, vReg src2, vReg src3)\n+%{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src1 (AddVL dst_src1 (MulVL src2 src3)));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_mla $dst_src1, src2, src3\\t # vector (sve) (D)\" %}\n+  ins_encode %{\n+    __ sve_mla(as_FloatRegister($dst_src1$$reg), __ D,\n+      ptrue, as_FloatRegister($src2$$reg), as_FloatRegister($src3$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ vector mls\n+\n+\/\/ dst_src1 = dst_src1 - src2 * src3\n+instruct vmlsB(vReg dst_src1, vReg src2, vReg src3)\n+%{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src1 (SubVB dst_src1 (MulVB src2 src3)));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_mls $dst_src1, src2, src3\\t # vector (sve) (B)\" %}\n+  ins_encode %{\n+    __ sve_mls(as_FloatRegister($dst_src1$$reg), __ B,\n+      ptrue, as_FloatRegister($src2$$reg), as_FloatRegister($src3$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ dst_src1 = dst_src1 - src2 * src3\n+instruct vmlsS(vReg dst_src1, vReg src2, vReg src3)\n+%{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src1 (SubVS dst_src1 (MulVS src2 src3)));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_mls $dst_src1, src2, src3\\t # vector (sve) (H)\" %}\n+  ins_encode %{\n+    __ sve_mls(as_FloatRegister($dst_src1$$reg), __ H,\n+      ptrue, as_FloatRegister($src2$$reg), as_FloatRegister($src3$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ dst_src1 = dst_src1 - src2 * src3\n+instruct vmlsI(vReg dst_src1, vReg src2, vReg src3)\n+%{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src1 (SubVI dst_src1 (MulVI src2 src3)));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_mls $dst_src1, src2, src3\\t # vector (sve) (S)\" %}\n+  ins_encode %{\n+    __ sve_mls(as_FloatRegister($dst_src1$$reg), __ S,\n+      ptrue, as_FloatRegister($src2$$reg), as_FloatRegister($src3$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ dst_src1 = dst_src1 - src2 * src3\n+instruct vmlsL(vReg dst_src1, vReg src2, vReg src3)\n+%{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src1 (SubVL dst_src1 (MulVL src2 src3)));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_mls $dst_src1, src2, src3\\t # vector (sve) (D)\" %}\n+  ins_encode %{\n+    __ sve_mls(as_FloatRegister($dst_src1$$reg), __ D,\n+      ptrue, as_FloatRegister($src2$$reg), as_FloatRegister($src3$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ vector mul\n+\n+instruct vmulB(vReg dst_src1, vReg src2) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src1 (MulVB dst_src1 src2));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_mul $dst_src1, $dst_src1, $src2\\t # vector (sve) (B)\" %}\n+  ins_encode %{\n+    __ sve_mul(as_FloatRegister($dst_src1$$reg), __ B,\n+         ptrue, as_FloatRegister($src2$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vmulS(vReg dst_src1, vReg src2) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src1 (MulVS dst_src1 src2));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_mul $dst_src1, $dst_src1, $src2\\t # vector (sve) (H)\" %}\n+  ins_encode %{\n+    __ sve_mul(as_FloatRegister($dst_src1$$reg), __ H,\n+         ptrue, as_FloatRegister($src2$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vmulI(vReg dst_src1, vReg src2) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src1 (MulVI dst_src1 src2));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_mul $dst_src1, $dst_src1, $src2\\t # vector (sve) (S)\" %}\n+  ins_encode %{\n+    __ sve_mul(as_FloatRegister($dst_src1$$reg), __ S,\n+         ptrue, as_FloatRegister($src2$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vmulL(vReg dst_src1, vReg src2) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src1 (MulVL dst_src1 src2));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_mul $dst_src1, $dst_src1, $src2\\t # vector (sve) (D)\" %}\n+  ins_encode %{\n+    __ sve_mul(as_FloatRegister($dst_src1$$reg), __ D,\n+         ptrue, as_FloatRegister($src2$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vmulF(vReg dst, vReg src1, vReg src2) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst (MulVF src1 src2));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_fmul $dst, $src1, $src2\\t # vector (sve) (S)\" %}\n+  ins_encode %{\n+    __ sve_fmul(as_FloatRegister($dst$$reg), __ S,\n+         as_FloatRegister($src1$$reg),\n+         as_FloatRegister($src2$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vmulD(vReg dst, vReg src1, vReg src2) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst (MulVD src1 src2));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_fmul $dst, $src1, $src2\\t # vector (sve) (D)\" %}\n+  ins_encode %{\n+    __ sve_fmul(as_FloatRegister($dst$$reg), __ D,\n+         as_FloatRegister($src1$$reg),\n+         as_FloatRegister($src2$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ vector mul - predicated\n+\n+instruct vmulB_masked(vReg dst_src1, vReg src2, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src1 (MulVB (Binary dst_src1 src2) pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_mul $dst_src1, $pg, $dst_src1, $src2\\t# vector (sve) (B)\" %}\n+  ins_encode %{\n+    __ sve_mul(as_FloatRegister($dst_src1$$reg), __ B,\n+            as_PRegister($pg$$reg),\n+            as_FloatRegister($src2$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vmulS_masked(vReg dst_src1, vReg src2, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src1 (MulVS (Binary dst_src1 src2) pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_mul $dst_src1, $pg, $dst_src1, $src2\\t# vector (sve) (H)\" %}\n+  ins_encode %{\n+    __ sve_mul(as_FloatRegister($dst_src1$$reg), __ H,\n+            as_PRegister($pg$$reg),\n+            as_FloatRegister($src2$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vmulI_masked(vReg dst_src1, vReg src2, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src1 (MulVI (Binary dst_src1 src2) pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_mul $dst_src1, $pg, $dst_src1, $src2\\t# vector (sve) (S)\" %}\n+  ins_encode %{\n+    __ sve_mul(as_FloatRegister($dst_src1$$reg), __ S,\n+            as_PRegister($pg$$reg),\n+            as_FloatRegister($src2$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vmulL_masked(vReg dst_src1, vReg src2, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src1 (MulVL (Binary dst_src1 src2) pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_mul $dst_src1, $pg, $dst_src1, $src2\\t# vector (sve) (D)\" %}\n+  ins_encode %{\n+    __ sve_mul(as_FloatRegister($dst_src1$$reg), __ D,\n+            as_PRegister($pg$$reg),\n+            as_FloatRegister($src2$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vmulF_masked(vReg dst_src1, vReg src2, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src1 (MulVF (Binary dst_src1 src2) pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_fmul $dst_src1, $pg, $dst_src1, $src2\\t# vector (sve) (S)\" %}\n+  ins_encode %{\n+    __ sve_fmul(as_FloatRegister($dst_src1$$reg), __ S,\n+            as_PRegister($pg$$reg),\n+            as_FloatRegister($src2$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vmulD_masked(vReg dst_src1, vReg src2, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src1 (MulVD (Binary dst_src1 src2) pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_fmul $dst_src1, $pg, $dst_src1, $src2\\t# vector (sve) (D)\" %}\n+  ins_encode %{\n+    __ sve_fmul(as_FloatRegister($dst_src1$$reg), __ D,\n+            as_PRegister($pg$$reg),\n+            as_FloatRegister($src2$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ vector fneg\n+\n+instruct vnegF(vReg dst, vReg src) %{\n+  predicate(UseSVE > 0 &&\n+            !n->as_Vector()->is_predicated_vector());\n+  match(Set dst (NegVF src));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_fneg $dst, $src\\t# vector (sve) (S)\" %}\n+  ins_encode %{\n+    __ sve_fneg(as_FloatRegister($dst$$reg), __ S,\n+         ptrue, as_FloatRegister($src$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vnegD(vReg dst, vReg src) %{\n+  predicate(UseSVE > 0 &&\n+            !n->as_Vector()->is_predicated_vector());\n+  match(Set dst (NegVD src));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_fneg $dst, $src\\t# vector (sve) (D)\" %}\n+  ins_encode %{\n+    __ sve_fneg(as_FloatRegister($dst$$reg), __ D,\n+         ptrue, as_FloatRegister($src$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ vector fneg - predicated\n+\n+instruct vnegF_masked(vReg dst_src, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src (NegVF dst_src pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_fneg $dst_src, $pg, $dst_src\\t# vector (sve) (S)\" %}\n+  ins_encode %{\n+    __ sve_fneg(as_FloatRegister($dst_src$$reg), __ S,\n+            as_PRegister($pg$$reg),\n+            as_FloatRegister($dst_src$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vnegD_masked(vReg dst_src, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src (NegVD dst_src pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_fneg $dst_src, $pg, $dst_src\\t# vector (sve) (D)\" %}\n+  ins_encode %{\n+    __ sve_fneg(as_FloatRegister($dst_src$$reg), __ D,\n+            as_PRegister($pg$$reg),\n+            as_FloatRegister($dst_src$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ popcount vector\n+\n+instruct vpopcountI(vReg dst, vReg src) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst (PopCountVI src));\n+  format %{ \"sve_cnt $dst, $src\\t# vector (sve) (S)\\n\\t\" %}\n+  ins_encode %{\n+     __ sve_cnt(as_FloatRegister($dst$$reg), __ S, ptrue, as_FloatRegister($src$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ vector mask compare\n+\n+instruct vmaskcmp(pRegGov dst, vReg src1, vReg src2, immI cond, rFlagsReg cr) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst (VectorMaskCmp (Binary src1 src2) cond));\n+  effect(KILL cr);\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_cmp $dst, $src1, $src2\\t# vector mask cmp (sve)\" %}\n+  ins_encode %{\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    __ sve_compare(as_PRegister($dst$$reg), bt, ptrue, as_FloatRegister($src1$$reg),\n+                   as_FloatRegister($src2$$reg), (int)$cond$$constant);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vmaskcmp_masked(pRegGov dst, vReg src1, vReg src2, immI cond, pRegGov pg, rFlagsReg cr) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst (VectorMaskCmp (Binary src1 src2) (Binary cond pg)));\n+  effect(KILL cr);\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_cmp $dst, $pg, $src1, $src2\\t# vector mask cmp (sve)\" %}\n+  ins_encode %{\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    __ sve_compare(as_PRegister($dst$$reg), bt, as_PRegister($pg$$reg), as_FloatRegister($src1$$reg),\n+                   as_FloatRegister($src2$$reg), (int)$cond$$constant);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ vector blend\n+\n+instruct vblend(vReg dst, vReg src1, vReg src2, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst (VectorBlend (Binary src1 src2) pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_sel $dst, $pg, $src2, $src1\\t# vector blend (sve)\" %}\n+  ins_encode %{\n+    Assembler::SIMD_RegVariant size =\n+               __ elemType_to_regVariant(Matcher::vector_element_basic_type(this));\n+    __ sve_sel(as_FloatRegister($dst$$reg), size, as_PRegister($pg$$reg),\n+               as_FloatRegister($src2$$reg), as_FloatRegister($src1$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ vector load mask\n+\n+instruct vloadmaskB(pRegGov dst, vReg src, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->bottom_type()->is_vect()->element_basic_type() == T_BYTE);\n+  match(Set dst (VectorLoadMask src));\n+  effect(KILL cr);\n+  ins_cost(SVE_COST);\n+  format %{ \"vloadmaskB $dst, $src\\t# vector load mask (sve) (B)\" %}\n+  ins_encode %{\n+    __ sve_cmp(Assembler::NE, as_PRegister($dst$$reg), __ B,\n+               ptrue, as_FloatRegister($src$$reg), 0);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vloadmask_extend(pRegGov dst, vReg src, vReg tmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 && n->bottom_type()->is_vect()->element_basic_type() != T_BYTE);\n+  match(Set dst (VectorLoadMask src));\n+  effect(TEMP tmp, KILL cr);\n+  ins_cost(3 * SVE_COST);\n+  format %{ \"vloadmask $dst, $src\\t# vector load mask (sve) (H\/S\/D)\" %}\n+  ins_encode %{\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    Assembler::SIMD_RegVariant size = __ elemType_to_regVariant(bt);\n+    __ sve_vector_extend(as_FloatRegister($tmp$$reg), size, as_FloatRegister($src$$reg), __ B);\n+    __ sve_cmp(Assembler::NE, as_PRegister($dst$$reg), size, ptrue, as_FloatRegister($tmp$$reg), 0);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ vector store mask\n+\n+instruct vstoremaskB(vReg dst, pRegGov src, immI_1 size) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst (VectorStoreMask src size));\n+  ins_cost(SVE_COST);\n+  format %{ \"vstoremask $dst, $src\\t# vector store mask (sve) (B)\" %}\n+  ins_encode %{\n+    __ sve_cpy(as_FloatRegister($dst$$reg), __ B, as_PRegister($src$$reg), 1, false);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vstoremask_narrow(vReg dst, pRegGov src, vReg tmp, immI_gt_1 size) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst (VectorStoreMask src size));\n+  effect(TEMP_DEF dst, TEMP tmp);\n+  ins_cost(3 * SVE_COST);\n+  format %{ \"vstoremask $dst, $src\\t# vector store mask (sve) (H\/S\/D)\" %}\n+  ins_encode %{\n+    Assembler::SIMD_RegVariant size = __ elemBytes_to_regVariant((int)$size$$constant);\n+    __ sve_cpy(as_FloatRegister($dst$$reg), size, as_PRegister($src$$reg), 1, false);\n+    __ sve_vector_narrow(as_FloatRegister($dst$$reg), __ B,\n+                         as_FloatRegister($dst$$reg), size, as_FloatRegister($tmp$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ Combine LoadVector+VectorLoadMask when the vector element type is not T_BYTE\n+\n+instruct vloadmask_loadV(pRegGov dst, indirect mem, vReg tmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->as_Vector()->length_in_bytes() == MaxVectorSize &&\n+            type2aelembytes(n->bottom_type()->is_vect()->element_basic_type()) > 1);\n+  match(Set dst (VectorLoadMask (LoadVector mem)));\n+  effect(TEMP tmp, KILL cr);\n+  ins_cost(3 * SVE_COST);\n+  format %{ \"sve_ld1b $tmp, $mem\\n\\t\"\n+            \"sve_cmpne $dst, $tmp, 0\\t# load vector mask (sve) (H\/S\/D)\" %}\n+  ins_encode %{\n+    \/\/ Load mask values which are boolean type, and extend them to the\n+    \/\/ expected vector element type. Convert the vector to predicate.\n@@ -1271,3 +2049,73 @@\n-    Assembler::SIMD_RegVariant to_vect_variant = __ elemType_to_regVariant(to_vect_bt);\n-    loadStoreA_predicated(C2_MacroAssembler(&cbuf), false, dst_reg, ptrue,\n-                          T_BOOLEAN, to_vect_bt, $mem->opcode(),\n+    loadStoreA_predicated(C2_MacroAssembler(&cbuf), false, as_FloatRegister($tmp$$reg),\n+                          ptrue, T_BOOLEAN, to_vect_bt, $mem->opcode(),\n+                          as_Register($mem$$base), $mem$$index, $mem$$scale, $mem$$disp);\n+    __ sve_cmp(Assembler::NE, as_PRegister($dst$$reg), __ elemType_to_regVariant(to_vect_bt),\n+               ptrue, as_FloatRegister($tmp$$reg), 0);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vloadmask_loadV_partial(pRegGov dst, indirect mem, vReg vtmp, pRegGov ptmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->as_Vector()->length_in_bytes() > 16 &&\n+            n->as_Vector()->length_in_bytes() < MaxVectorSize &&\n+            type2aelembytes(n->bottom_type()->is_vect()->element_basic_type()) > 1);\n+  match(Set dst (VectorLoadMask (LoadVector mem)));\n+  effect(TEMP vtmp, TEMP ptmp, KILL cr);\n+  ins_cost(6 * SVE_COST);\n+  format %{ \"vloadmask_loadV $dst, $mem\\t# load vector mask partial (sve) (H\/S\/D)\" %}\n+  ins_encode %{\n+    \/\/ Load valid mask values which are boolean type, and extend them to the\n+    \/\/ expected vector element type. Convert the vector to predicate.\n+    BasicType to_vect_bt = Matcher::vector_element_basic_type(this);\n+    Assembler::SIMD_RegVariant size = __ elemType_to_regVariant(to_vect_bt);\n+    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), size, Matcher::vector_length(this));\n+    loadStoreA_predicated(C2_MacroAssembler(&cbuf), false, as_FloatRegister($vtmp$$reg),\n+                          as_PRegister($ptmp$$reg), T_BOOLEAN, to_vect_bt, $mem->opcode(),\n+                          as_Register($mem$$base), $mem$$index, $mem$$scale, $mem$$disp);\n+    __ sve_cmp(Assembler::NE, as_PRegister($dst$$reg), size, ptrue, as_FloatRegister($vtmp$$reg), 0);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ Combine VectorStoreMask+StoreVector when the vector element type is not T_BYTE\n+\n+instruct storeV_vstoremask(indirect mem, pRegGov src, vReg tmp, immI_gt_1 esize) %{\n+  predicate(UseSVE > 0 &&\n+            Matcher::vector_length_in_bytes(n->as_StoreVector()->in(MemNode::ValueIn)->in(1)) == MaxVectorSize);\n+  match(Set mem (StoreVector mem (VectorStoreMask src esize)));\n+  effect(TEMP tmp);\n+  ins_cost(3 * SVE_COST);\n+  format %{ \"sve_cpy $tmp, $src, 1\\n\\t\"\n+            \"sve_st1b $tmp, $mem\\t# store vector mask (sve) (H\/S\/D)\" %}\n+  ins_encode %{\n+    BasicType from_vect_bt = Matcher::vector_element_basic_type(this, $src);\n+    assert(type2aelembytes(from_vect_bt) == (int)$esize$$constant, \"unsupported type.\");\n+    Assembler::SIMD_RegVariant size = __ elemBytes_to_regVariant($esize$$constant);\n+    __ sve_cpy(as_FloatRegister($tmp$$reg), size, as_PRegister($src$$reg), 1, false);\n+    loadStoreA_predicated(C2_MacroAssembler(&cbuf), true, as_FloatRegister($tmp$$reg),\n+                          ptrue, T_BOOLEAN, from_vect_bt, $mem->opcode(),\n+                          as_Register($mem$$base), $mem$$index, $mem$$scale, $mem$$disp);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct storeV_vstoremask_partial(indirect mem, pRegGov src, vReg vtmp,\n+                                   immI_gt_1 esize, pRegGov ptmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->as_StoreVector()->memory_size() > 16 &&\n+            type2aelembytes(n->as_StoreVector()->vect_type()->element_basic_type()) > 1 &&\n+            Matcher::vector_length_in_bytes(n->as_StoreVector()->in(MemNode::ValueIn)->in(1)) < MaxVectorSize);\n+  match(Set mem (StoreVector mem (VectorStoreMask src esize)));\n+  effect(TEMP vtmp, TEMP ptmp, KILL cr);\n+  format %{ \"storeV_vstoremask $src, $mem\\t# store vector mask partial (sve) (H\/S\/D)\" %}\n+  ins_cost(6 * SVE_COST);\n+  ins_encode %{\n+    \/\/ Convert the valid src predicate to vector, and store the vector\n+    \/\/ elements as boolean values.\n+    BasicType from_vect_bt = Matcher::vector_element_basic_type(this, $src);\n+    Assembler::SIMD_RegVariant size = __ elemType_to_regVariant(from_vect_bt);\n+    __ sve_cpy(as_FloatRegister($vtmp$$reg), size, as_PRegister($src$$reg), 1, false);\n+    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), size, Matcher::vector_length(this, $src));\n+    loadStoreA_predicated(C2_MacroAssembler(&cbuf), true, as_FloatRegister($vtmp$$reg),\n+                          as_PRegister($ptmp$$reg), T_BOOLEAN, from_vect_bt, $mem->opcode(),\n@@ -1275,1 +2123,0 @@\n-    __ sve_neg(dst_reg, to_vect_variant, ptrue, dst_reg);\n@@ -1280,7 +2127,293 @@\n-instruct vloadmask_loadV_non_byte(vReg dst, indirect mem) %{\n-  predicate(UseSVE > 0 && n->as_Vector()->length_in_bytes() == MaxVectorSize &&\n-            type2aelembytes(n->bottom_type()->is_vect()->element_basic_type()) > 1);\n-  match(Set dst (VectorLoadMask (LoadVector mem)));\n-  ins_cost(5 * SVE_COST);\n-  format %{ \"sve_ld1b $dst, $mem\\n\\t\"\n-            \"sve_neg $dst, $dst\\t# load vector mask (sve)\" %}\n+\/\/ vector add reduction\n+\n+instruct reduce_addI(iRegINoSp dst, iRegIorL2I src1, vReg src2, vRegD tmp) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize);\n+  match(Set dst (AddReductionVI src1 src2));\n+  effect(TEMP_DEF dst, TEMP tmp);\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_reduce_addI $dst, $src1, $src2\\t# addI reduction (sve) (may extend)\" %}\n+  ins_encode %{\n+    BasicType bt = Matcher::vector_element_basic_type(this, $src2);\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, bt,\n+                           $src1$$Register, as_FloatRegister($src2$$reg),\n+                           ptrue, as_FloatRegister($tmp$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct reduce_addL(iRegLNoSp dst, iRegL src1, vReg src2, vRegD tmp) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize);\n+  match(Set dst (AddReductionVL src1 src2));\n+  effect(TEMP_DEF dst, TEMP tmp);\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_reduce_addL $dst, $src1, $src2\\t# addL reduction (sve)\" %}\n+  ins_encode %{\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, T_LONG,\n+                           $src1$$Register, as_FloatRegister($src2$$reg),\n+                           ptrue, as_FloatRegister($tmp$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct reduce_addF(vRegF src1_dst, vReg src2) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize);\n+  match(Set src1_dst (AddReductionVF src1_dst src2));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_fadda $src1_dst, $src1_dst, $src2\\t# vector (sve) (S)\" %}\n+  ins_encode %{\n+    __ sve_fadda(as_FloatRegister($src1_dst$$reg), __ S,\n+         ptrue, as_FloatRegister($src2$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct reduce_addD(vRegD src1_dst, vReg src2) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize);\n+  match(Set src1_dst (AddReductionVD src1_dst src2));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_fadda $src1_dst, $src1_dst, $src2\\t# vector (sve) (D)\" %}\n+  ins_encode %{\n+    __ sve_fadda(as_FloatRegister($src1_dst$$reg), __ D,\n+         ptrue, as_FloatRegister($src2$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct reduce_addI_partial(iRegINoSp dst, iRegIorL2I src1, vReg src2, vRegD vtmp,\n+                             pRegGov ptmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize);\n+  match(Set dst (AddReductionVI src1 src2));\n+  effect(TEMP_DEF dst, TEMP vtmp, TEMP ptmp, KILL cr);\n+  ins_cost(2 * SVE_COST);\n+  format %{ \"sve_reduce_addI $dst, $src1, $src2\\t# addI reduction partial (sve) (may extend)\" %}\n+  ins_encode %{\n+    BasicType bt = Matcher::vector_element_basic_type(this, $src2);\n+    Assembler::SIMD_RegVariant variant = __ elemType_to_regVariant(bt);\n+    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), variant,\n+                          Matcher::vector_length(this, $src2));\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, bt,\n+                           $src1$$Register, as_FloatRegister($src2$$reg),\n+                           as_PRegister($ptmp$$reg), as_FloatRegister($vtmp$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct reduce_addL_partial(iRegLNoSp dst, iRegL src1, vReg src2, vRegD vtmp,\n+                             pRegGov ptmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize);\n+  match(Set dst (AddReductionVL src1 src2));\n+  effect(TEMP_DEF dst, TEMP vtmp, TEMP ptmp, KILL cr);\n+  ins_cost(2 * SVE_COST);\n+  format %{ \"sve_reduce_addL $dst, $src1, $src2\\t# addL reduction partial (sve)\" %}\n+  ins_encode %{\n+    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), __ D,\n+                          Matcher::vector_length(this, $src2));\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, T_LONG,\n+                           $src1$$Register, as_FloatRegister($src2$$reg),\n+                           as_PRegister($ptmp$$reg), as_FloatRegister($vtmp$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct reduce_addF_partial(vRegF src1_dst, vReg src2, pRegGov ptmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize);\n+  match(Set src1_dst (AddReductionVF src1_dst src2));\n+  ins_cost(SVE_COST);\n+  effect(TEMP ptmp, KILL cr);\n+  format %{ \"sve_reduce_addF $src1_dst, $src1_dst, $src2\\t# addF reduction partial (sve) (S)\" %}\n+  ins_encode %{\n+    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), __ S,\n+                          Matcher::vector_length(this, $src2));\n+    __ sve_fadda(as_FloatRegister($src1_dst$$reg), __ S,\n+                 as_PRegister($ptmp$$reg), as_FloatRegister($src2$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct reduce_addD_partial(vRegD src1_dst, vReg src2, pRegGov ptmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize);\n+  match(Set src1_dst (AddReductionVD src1_dst src2));\n+  ins_cost(SVE_COST);\n+  effect(TEMP ptmp, KILL cr);\n+  format %{ \"sve_reduce_addD $src1_dst, $src1_dst, $src2\\t# addD reduction partial (sve) (D)\" %}\n+  ins_encode %{\n+    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), __ D,\n+                          Matcher::vector_length(this, $src2));\n+    __ sve_fadda(as_FloatRegister($src1_dst$$reg), __ D,\n+                 as_PRegister($ptmp$$reg), as_FloatRegister($src2$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ vector add reduction - predicated\n+\n+instruct reduce_addI_masked(iRegINoSp dst, iRegIorL2I src1, vReg src2, vRegD tmp, pRegGov pg) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize);\n+  match(Set dst (AddReductionVI (Binary src1 src2) pg));\n+  effect(TEMP_DEF dst, TEMP tmp);\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_reduce_addI $dst, $src1, $pg, $src2\\t# addI reduction predicated (sve) (may extend)\" %}\n+  ins_encode %{\n+    BasicType bt = Matcher::vector_element_basic_type(this, $src2);\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, bt,\n+                           $src1$$Register, as_FloatRegister($src2$$reg),\n+                           as_PRegister($pg$$reg), as_FloatRegister($tmp$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct reduce_addL_masked(iRegLNoSp dst, iRegL src1, vReg src2, vRegD tmp, pRegGov pg) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize);\n+  match(Set dst (AddReductionVL (Binary src1 src2) pg));\n+  effect(TEMP_DEF dst, TEMP tmp);\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_reduce_addL $dst, $src1, $pg, $src2\\t# addL reduction predicated (sve)\" %}\n+  ins_encode %{\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, T_LONG,\n+                           $src1$$Register, as_FloatRegister($src2$$reg),\n+                           as_PRegister($pg$$reg), as_FloatRegister($tmp$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct reduce_addF_masked(vRegF src1_dst, vReg src2, pRegGov pg) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize);\n+  match(Set src1_dst (AddReductionVF (Binary src1_dst src2) pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_reduce_addF $src1_dst, $pg, $src2\\t# addF reduction predicated (sve)\" %}\n+  ins_encode %{\n+    __ sve_fadda(as_FloatRegister($src1_dst$$reg), __ S,\n+                 as_PRegister($pg$$reg), as_FloatRegister($src2$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct reduce_addD_masked(vRegD src1_dst, vReg src2, pRegGov pg) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize);\n+  match(Set src1_dst (AddReductionVD (Binary src1_dst src2) pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_reduce_addD $src1_dst, $pg, $src2\\t# addD reduction predicated (sve)\" %}\n+  ins_encode %{\n+    __ sve_fadda(as_FloatRegister($src1_dst$$reg), __ D,\n+                 as_PRegister($pg$$reg), as_FloatRegister($src2$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct reduce_addI_masked_partial(iRegINoSp dst, iRegIorL2I src1, vReg src2, vRegD vtmp,\n+                                  pRegGov pg, pRegGov ptmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize);\n+  match(Set dst (AddReductionVI (Binary src1 src2) pg));\n+  effect(TEMP_DEF dst, TEMP vtmp, TEMP ptmp, KILL cr);\n+  ins_cost(3 * SVE_COST);\n+  format %{ \"sve_reduce_addI $dst, $src1, $pg, $src2\\t# addI reduction predicated partial (sve) (may extend)\" %}\n+  ins_encode %{\n+    BasicType bt = Matcher::vector_element_basic_type(this, $src2);\n+    Assembler::SIMD_RegVariant variant = __ elemType_to_regVariant(bt);\n+    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), variant,\n+                          Matcher::vector_length(this, $src2));\n+    __ sve_and(as_PRegister($ptmp$$reg), as_PRegister($ptmp$$reg),\n+               as_PRegister($pg$$reg), as_PRegister($pg$$reg));\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, bt,\n+                           $src1$$Register, as_FloatRegister($src2$$reg),\n+                           as_PRegister($ptmp$$reg), as_FloatRegister($vtmp$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct reduce_addL_masked_partial(iRegLNoSp dst, iRegL src1, vReg src2, vRegD vtmp,\n+                                  pRegGov pg, pRegGov ptmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize);\n+  match(Set dst (AddReductionVL (Binary src1 src2) pg));\n+  effect(TEMP_DEF dst, TEMP vtmp, TEMP ptmp, KILL cr);\n+  ins_cost(3 * SVE_COST);\n+  format %{ \"sve_reduce_addL $dst, $src1, $pg, $src2\\t# addL reduction predicated partial (sve)\" %}\n+  ins_encode %{\n+    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), __ D,\n+                          Matcher::vector_length(this, $src2));\n+    __ sve_and(as_PRegister($ptmp$$reg), as_PRegister($ptmp$$reg),\n+               as_PRegister($pg$$reg), as_PRegister($pg$$reg));\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, T_LONG,\n+                           $src1$$Register, as_FloatRegister($src2$$reg),\n+                           as_PRegister($ptmp$$reg), as_FloatRegister($vtmp$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct reduce_addF_masked_partial(vRegF src1_dst, vReg src2, pRegGov pg, pRegGov ptmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize);\n+  match(Set src1_dst (AddReductionVF (Binary src1_dst src2) pg));\n+  effect(TEMP ptmp, KILL cr);\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_reduce_addF $src1_dst, $pg, $src2\\t# addF reduction predicated partial (sve)\" %}\n+  ins_encode %{\n+    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), __ S,\n+                          Matcher::vector_length(this, $src2));\n+    __ sve_and(as_PRegister($ptmp$$reg), as_PRegister($ptmp$$reg),\n+               as_PRegister($pg$$reg), as_PRegister($pg$$reg));\n+    __ sve_fadda(as_FloatRegister($src1_dst$$reg), __ S,\n+                 as_PRegister($ptmp$$reg), as_FloatRegister($src2$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct reduce_addD_masked_partial(vRegD src1_dst, vReg src2, pRegGov pg, pRegGov ptmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize);\n+  match(Set src1_dst (AddReductionVD (Binary src1_dst src2) pg));\n+  effect(TEMP ptmp, KILL cr);\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_reduce_addD $src1_dst, $pg, $src2\\t# addD reduction predicated partial (sve)\" %}\n+  ins_encode %{\n+    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), __ D,\n+                          Matcher::vector_length(this, $src2));\n+    __ sve_and(as_PRegister($ptmp$$reg), as_PRegister($ptmp$$reg),\n+               as_PRegister($pg$$reg), as_PRegister($pg$$reg));\n+    __ sve_fadda(as_FloatRegister($src1_dst$$reg), __ D,\n+                 as_PRegister($ptmp$$reg), as_FloatRegister($src2$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ vector and reduction\n+\n+instruct reduce_andI(iRegINoSp dst, iRegIorL2I src1, vReg src2, vRegD tmp) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(2)->bottom_type()->is_vect()->element_basic_type() != T_LONG &&\n+            n->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize);\n+  match(Set dst (AndReductionV src1 src2));\n+  effect(TEMP_DEF dst, TEMP tmp);\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_reduce_andI $dst, $src1, $src2\\t# andI reduction (sve) (may extend)\" %}\n+  ins_encode %{\n+    BasicType bt = Matcher::vector_element_basic_type(this, $src2);\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, bt,\n+                           $src1$$Register, as_FloatRegister($src2$$reg),\n+                           ptrue, as_FloatRegister($tmp$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct reduce_andL(iRegLNoSp dst, iRegL src1, vReg src2, vRegD tmp) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_LONG &&\n+            n->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize);\n+  match(Set dst (AndReductionV src1 src2));\n+  effect(TEMP_DEF dst, TEMP tmp);\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_reduce_andL $dst, $src1, $src2\\t# andL reduction (sve)\" %}\n@@ -1288,7 +2421,3 @@\n-    FloatRegister dst_reg = as_FloatRegister($dst$$reg);\n-    BasicType to_vect_bt = Matcher::vector_element_basic_type(this);\n-    Assembler::SIMD_RegVariant to_vect_variant = __ elemType_to_regVariant(to_vect_bt);\n-    loadStoreA_predicated(C2_MacroAssembler(&cbuf), false, dst_reg, ptrue,\n-                          T_BOOLEAN, to_vect_bt, $mem->opcode(),\n-                          as_Register($mem$$base), $mem$$index, $mem$$scale, $mem$$disp);\n-    __ sve_neg(dst_reg, to_vect_variant, ptrue, dst_reg);\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, T_LONG,\n+                           $src1$$Register, as_FloatRegister($src2$$reg),\n+                           ptrue, as_FloatRegister($tmp$$reg));\n@@ -1299,8 +2428,9 @@\n-instruct storeV_vstoremask_byte(vmemA mem, vReg src, vReg tmp, immI_1 esize) %{\n-  predicate(UseSVE > 0 && n->as_StoreVector()->memory_size() *\n-                          n->as_StoreVector()->in(MemNode::ValueIn)->in(2)->get_int() == MaxVectorSize);\n-  match(Set mem (StoreVector mem (VectorStoreMask src esize)));\n-  effect(TEMP tmp);\n-  ins_cost(5 * SVE_COST);\n-  format %{ \"sve_neg $tmp, $src\\n\\t\"\n-            \"sve_st1b $tmp, $mem\\t# store vector mask (sve)\" %}\n+instruct reduce_andI_partial(iRegINoSp dst, iRegIorL2I src1, vReg src2, vRegD vtmp,\n+                             pRegGov ptmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(2)->bottom_type()->is_vect()->element_basic_type() != T_LONG &&\n+            n->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize);\n+  match(Set dst (AndReductionV src1 src2));\n+  effect(TEMP_DEF dst, TEMP vtmp, TEMP ptmp, KILL cr);\n+  ins_cost(2 * SVE_COST);\n+  format %{ \"sve_reduce_andI $dst, $src1, $src2\\t# andI reduction partial (sve) (may extend)\" %}\n@@ -1308,8 +2438,7 @@\n-    BasicType from_vect_bt = Matcher::vector_element_basic_type(this, $src);\n-    assert(type2aelembytes(from_vect_bt) == (int)$esize$$constant, \"unsupported type.\");\n-    Assembler::SIMD_RegVariant from_vect_variant = __ elemBytes_to_regVariant($esize$$constant);\n-    __ sve_neg(as_FloatRegister($tmp$$reg), from_vect_variant, ptrue,\n-               as_FloatRegister($src$$reg));\n-    loadStoreA_predicated(C2_MacroAssembler(&cbuf), true, as_FloatRegister($tmp$$reg),\n-                          ptrue, T_BOOLEAN, from_vect_bt, $mem->opcode(),\n-                          as_Register($mem$$base), $mem$$index, $mem$$scale, $mem$$disp);\n+    BasicType bt = Matcher::vector_element_basic_type(this, $src2);\n+    Assembler::SIMD_RegVariant variant = __ elemType_to_regVariant(bt);\n+    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), variant,\n+                          Matcher::vector_length(this, $src2));\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, bt,\n+                           $src1$$Register, as_FloatRegister($src2$$reg),\n+                           as_PRegister($ptmp$$reg), as_FloatRegister($vtmp$$reg));\n@@ -1320,8 +2449,9 @@\n-instruct storeV_vstoremask_non_byte(indirect mem, vReg src, vReg tmp, immI_gt_1 esize) %{\n-  predicate(UseSVE > 0 && n->as_StoreVector()->memory_size() *\n-                          n->as_StoreVector()->in(MemNode::ValueIn)->in(2)->get_int() == MaxVectorSize);\n-  match(Set mem (StoreVector mem (VectorStoreMask src esize)));\n-  effect(TEMP tmp);\n-  ins_cost(5 * SVE_COST);\n-  format %{ \"sve_neg $tmp, $src\\n\\t\"\n-            \"sve_st1b $tmp, $mem\\t# store vector mask (sve)\" %}\n+instruct reduce_andL_partial(iRegLNoSp dst, iRegL src1, vReg src2, vRegD vtmp,\n+                             pRegGov ptmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_LONG &&\n+            n->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize);\n+  match(Set dst (AndReductionV src1 src2));\n+  effect(TEMP_DEF dst, TEMP vtmp, TEMP ptmp, KILL cr);\n+  ins_cost(2 * SVE_COST);\n+  format %{ \"sve_reduce_andL $dst, $src1, $src2\\t# andL reduction partial (sve)\" %}\n@@ -1329,8 +2459,5 @@\n-    BasicType from_vect_bt = Matcher::vector_element_basic_type(this, $src);\n-    assert(type2aelembytes(from_vect_bt) == (int)$esize$$constant, \"unsupported type.\");\n-    Assembler::SIMD_RegVariant from_vect_variant = __ elemBytes_to_regVariant($esize$$constant);\n-    __ sve_neg(as_FloatRegister($tmp$$reg), from_vect_variant, ptrue,\n-               as_FloatRegister($src$$reg));\n-    loadStoreA_predicated(C2_MacroAssembler(&cbuf), true, as_FloatRegister($tmp$$reg),\n-                          ptrue, T_BOOLEAN, from_vect_bt, $mem->opcode(),\n-                          as_Register($mem$$base), $mem$$index, $mem$$scale, $mem$$disp);\n+    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), __ D,\n+                          Matcher::vector_length(this, $src2));\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, T_LONG,\n+                           $src1$$Register, as_FloatRegister($src2$$reg),\n+                           as_PRegister($ptmp$$reg), as_FloatRegister($vtmp$$reg));\n@@ -1341,1 +2468,1 @@\n-\/\/ vector add reduction\n+\/\/ vector and reduction - predicated\n@@ -1343,4 +2470,6 @@\n-instruct reduce_addI(iRegINoSp dst, iRegIorL2I src1, vReg src2, vRegD vtmp) %{\n-  predicate(UseSVE > 0 && n->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize);\n-  match(Set dst (AddReductionVI src1 src2));\n-  effect(TEMP_DEF dst, TEMP vtmp);\n+instruct reduce_andI_masked(iRegINoSp dst, iRegIorL2I src1, vReg src2, vRegD tmp, pRegGov pg) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->element_basic_type() != T_LONG &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize);\n+  match(Set dst (AndReductionV (Binary src1 src2) pg));\n+  effect(TEMP_DEF dst, TEMP tmp);\n@@ -1348,1 +2477,1 @@\n-  format %{ \"sve_reduce_addI $dst, $src1, $src2\\t# addB\/S\/I reduction (sve) (may extend)\" %}\n+  format %{ \"sve_reduce_andI $dst, $src1, $pg, $src2\\t# andI reduction predicated (sve) (may extend)\" %}\n@@ -1351,11 +2480,3 @@\n-    Assembler::SIMD_RegVariant variant = __ elemType_to_regVariant(bt);\n-    __ sve_uaddv(as_FloatRegister($vtmp$$reg), variant, ptrue, as_FloatRegister($src2$$reg));\n-    __ umov($dst$$Register, as_FloatRegister($vtmp$$reg), variant, 0);\n-    __ addw($dst$$Register, $dst$$Register, $src1$$Register);\n-    if (bt == T_BYTE) {\n-      __ sxtb($dst$$Register, $dst$$Register);\n-    } else if (bt == T_SHORT) {\n-      __ sxth($dst$$Register, $dst$$Register);\n-    } else {\n-      assert(bt == T_INT, \"unsupported type\");\n-    }\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, bt,\n+                           $src1$$Register, as_FloatRegister($src2$$reg),\n+                           as_PRegister($pg$$reg), as_FloatRegister($tmp$$reg));\n@@ -1366,5 +2487,6 @@\n-instruct reduce_addI_partial(iRegINoSp dst, iRegIorL2I src1, vReg src2, vRegD vtmp,\n-                             pRegGov ptmp, rFlagsReg cr) %{\n-  predicate(UseSVE > 0 && n->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize);\n-  match(Set dst (AddReductionVI src1 src2));\n-  effect(TEMP_DEF dst, TEMP vtmp, TEMP ptmp, KILL cr);\n+instruct reduce_andL_masked(iRegLNoSp dst, iRegL src1, vReg src2, vRegD tmp, pRegGov pg) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->element_basic_type() == T_LONG &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize);\n+  match(Set dst (AndReductionV (Binary src1 src2) pg));\n+  effect(TEMP_DEF dst, TEMP tmp);\n@@ -1372,1 +2494,1 @@\n-  format %{ \"sve_reduce_addI $dst, $src1, $src2\\t# addI reduction partial (sve) (may extend)\" %}\n+  format %{ \"sve_reduce_andL $dst, $src1, $pg, $src2\\t# andL reduction predicated (sve)\" %}\n@@ -1374,15 +2496,3 @@\n-    BasicType bt = Matcher::vector_element_basic_type(this, $src2);\n-    Assembler::SIMD_RegVariant variant = __ elemType_to_regVariant(bt);\n-    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), variant,\n-                          Matcher::vector_length(this, $src2));\n-    __ sve_uaddv(as_FloatRegister($vtmp$$reg), variant,\n-                 as_PRegister($ptmp$$reg), as_FloatRegister($src2$$reg));\n-    __ umov($dst$$Register, as_FloatRegister($vtmp$$reg), variant, 0);\n-    __ addw($dst$$Register, $dst$$Register, $src1$$Register);\n-    if (bt == T_BYTE) {\n-      __ sxtb($dst$$Register, $dst$$Register);\n-    } else if (bt == T_SHORT) {\n-      __ sxth($dst$$Register, $dst$$Register);\n-    } else {\n-      assert(bt == T_INT, \"unsupported type\");\n-    }\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, T_LONG,\n+                           $src1$$Register, as_FloatRegister($src2$$reg),\n+                           as_PRegister($pg$$reg), as_FloatRegister($tmp$$reg));\n@@ -1393,6 +2503,9 @@\n-instruct reduce_addL(iRegLNoSp dst, iRegL src1, vReg src2, vRegD vtmp) %{\n-  predicate(UseSVE > 0 && n->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize);\n-  match(Set dst (AddReductionVL src1 src2));\n-  effect(TEMP_DEF dst, TEMP vtmp);\n-  ins_cost(SVE_COST);\n-  format %{ \"sve_reduce_addL $dst, $src1, $src2\\t# addL reduction (sve)\" %}\n+instruct reduce_andI_masked_partial(iRegINoSp dst, iRegIorL2I src1, vReg src2, vRegD vtmp,\n+                                  pRegGov pg, pRegGov ptmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->element_basic_type() != T_LONG &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize);\n+  match(Set dst (AndReductionV (Binary src1 src2) pg));\n+  effect(TEMP_DEF dst, TEMP vtmp, TEMP ptmp, KILL cr);\n+  ins_cost(3 * SVE_COST);\n+  format %{ \"sve_reduce_andI $dst, $src1, $pg, $src2\\t# andI reduction predicated partial (sve) (may extend)\" %}\n@@ -1400,3 +2513,9 @@\n-    __ sve_uaddv(as_FloatRegister($vtmp$$reg), __ D, ptrue, as_FloatRegister($src2$$reg));\n-    __ umov($dst$$Register, as_FloatRegister($vtmp$$reg), __ D, 0);\n-    __ add($dst$$Register, $dst$$Register, $src1$$Register);\n+    BasicType bt = Matcher::vector_element_basic_type(this, $src2);\n+    Assembler::SIMD_RegVariant variant = __ elemType_to_regVariant(bt);\n+    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), variant,\n+                          Matcher::vector_length(this, $src2));\n+    __ sve_and(as_PRegister($ptmp$$reg), as_PRegister($ptmp$$reg),\n+               as_PRegister($pg$$reg), as_PRegister($pg$$reg));\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, bt,\n+                           $src1$$Register, as_FloatRegister($src2$$reg),\n+                           as_PRegister($ptmp$$reg), as_FloatRegister($vtmp$$reg));\n@@ -1407,4 +2526,6 @@\n-instruct reduce_addL_partial(iRegLNoSp dst, iRegL src1, vReg src2, vRegD vtmp,\n-                             pRegGov ptmp, rFlagsReg cr) %{\n-  predicate(UseSVE > 0 && n->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize);\n-  match(Set dst (AddReductionVL src1 src2));\n+instruct reduce_andL_masked_partial(iRegLNoSp dst, iRegL src1, vReg src2, vRegD vtmp,\n+                                  pRegGov pg, pRegGov ptmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->element_basic_type() == T_LONG &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize);\n+  match(Set dst (AndReductionV (Binary src1 src2) pg));\n@@ -1412,2 +2533,2 @@\n-  ins_cost(SVE_COST);\n-  format %{ \"sve_reduce_addL $dst, $src1, $src2\\t# addL reduction partial (sve)\" %}\n+  ins_cost(3 * SVE_COST);\n+  format %{ \"sve_reduce_andL $dst, $src1, $pg, $src2\\t# andL reduction predicated partial (sve)\" %}\n@@ -1417,4 +2538,5 @@\n-    __ sve_uaddv(as_FloatRegister($vtmp$$reg), __ D,\n-                 as_PRegister($ptmp$$reg), as_FloatRegister($src2$$reg));\n-    __ umov($dst$$Register, as_FloatRegister($vtmp$$reg), __ D, 0);\n-    __ add($dst$$Register, $dst$$Register, $src1$$Register);\n+    __ sve_and(as_PRegister($ptmp$$reg), as_PRegister($ptmp$$reg),\n+               as_PRegister($pg$$reg), as_PRegister($pg$$reg));\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, T_LONG,\n+                           $src1$$Register, as_FloatRegister($src2$$reg),\n+                           as_PRegister($ptmp$$reg), as_FloatRegister($vtmp$$reg));\n@@ -1425,0 +2547,1 @@\n+\/\/ vector or reduction\n@@ -1426,3 +2549,6 @@\n-instruct reduce_addF(vRegF src1_dst, vReg src2) %{\n-  predicate(UseSVE > 0 && n->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize);\n-  match(Set src1_dst (AddReductionVF src1_dst src2));\n+instruct reduce_orI(iRegINoSp dst, iRegIorL2I src1, vReg src2, vRegD tmp) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(2)->bottom_type()->is_vect()->element_basic_type() != T_LONG &&\n+            n->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize);\n+  match(Set dst (OrReductionV src1 src2));\n+  effect(TEMP_DEF dst, TEMP tmp);\n@@ -1430,1 +2556,1 @@\n-  format %{ \"sve_fadda $src1_dst, $src1_dst, $src2\\t# vector (sve) (S)\" %}\n+  format %{ \"sve_reduce_orI $dst, $src1, $src2\\t# orI reduction (sve) (may extend)\" %}\n@@ -1432,2 +2558,4 @@\n-    __ sve_fadda(as_FloatRegister($src1_dst$$reg), __ S,\n-         ptrue, as_FloatRegister($src2$$reg));\n+    BasicType bt = Matcher::vector_element_basic_type(this, $src2);\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, bt,\n+                           $src1$$Register, as_FloatRegister($src2$$reg),\n+                           ptrue, as_FloatRegister($tmp$$reg));\n@@ -1438,3 +2566,6 @@\n-instruct reduce_addF_partial(vRegF src1_dst, vReg src2, pRegGov ptmp, rFlagsReg cr) %{\n-  predicate(UseSVE > 0 && n->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize);\n-  match(Set src1_dst (AddReductionVF src1_dst src2));\n+instruct reduce_orL(iRegLNoSp dst, iRegL src1, vReg src2, vRegD tmp) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_LONG &&\n+            n->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize);\n+  match(Set dst (OrReductionV src1 src2));\n+  effect(TEMP_DEF dst, TEMP tmp);\n@@ -1442,2 +2573,1 @@\n-  effect(TEMP ptmp, KILL cr);\n-  format %{ \"sve_reduce_addF $src1_dst, $src1_dst, $src2\\t# addF reduction partial (sve) (S)\" %}\n+  format %{ \"sve_reduce_orL $dst, $src1, $src2\\t# orL reduction (sve)\" %}\n@@ -1445,4 +2575,3 @@\n-    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), __ S,\n-                          Matcher::vector_length(this, $src2));\n-    __ sve_fadda(as_FloatRegister($src1_dst$$reg), __ S,\n-                 as_PRegister($ptmp$$reg), as_FloatRegister($src2$$reg));\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, T_LONG,\n+                           $src1$$Register, as_FloatRegister($src2$$reg),\n+                           ptrue, as_FloatRegister($tmp$$reg));\n@@ -1453,5 +2582,9 @@\n-instruct reduce_addD(vRegD src1_dst, vReg src2) %{\n-  predicate(UseSVE > 0 && n->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize);\n-  match(Set src1_dst (AddReductionVD src1_dst src2));\n-  ins_cost(SVE_COST);\n-  format %{ \"sve_fadda $src1_dst, $src1_dst, $src2\\t# vector (sve) (D)\" %}\n+instruct reduce_orI_partial(iRegINoSp dst, iRegIorL2I src1, vReg src2, vRegD vtmp,\n+                             pRegGov ptmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(2)->bottom_type()->is_vect()->element_basic_type() != T_LONG &&\n+            n->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize);\n+  match(Set dst (OrReductionV src1 src2));\n+  effect(TEMP_DEF dst, TEMP vtmp, TEMP ptmp, KILL cr);\n+  ins_cost(2 * SVE_COST);\n+  format %{ \"sve_reduce_orI $dst, $src1, $src2\\t# orI reduction partial (sve) (may extend)\" %}\n@@ -1459,2 +2592,7 @@\n-    __ sve_fadda(as_FloatRegister($src1_dst$$reg), __ D,\n-         ptrue, as_FloatRegister($src2$$reg));\n+    BasicType bt = Matcher::vector_element_basic_type(this, $src2);\n+    Assembler::SIMD_RegVariant variant = __ elemType_to_regVariant(bt);\n+    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), variant,\n+                          Matcher::vector_length(this, $src2));\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, bt,\n+                           $src1$$Register, as_FloatRegister($src2$$reg),\n+                           as_PRegister($ptmp$$reg), as_FloatRegister($vtmp$$reg));\n@@ -1465,6 +2603,9 @@\n-instruct reduce_addD_partial(vRegD src1_dst, vReg src2, pRegGov ptmp, rFlagsReg cr) %{\n-  predicate(UseSVE > 0 && n->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize);\n-  match(Set src1_dst (AddReductionVD src1_dst src2));\n-  ins_cost(SVE_COST);\n-  effect(TEMP ptmp, KILL cr);\n-  format %{ \"sve_reduce_addD $src1_dst, $src1_dst, $src2\\t# addD reduction partial (sve) (D)\" %}\n+instruct reduce_orL_partial(iRegLNoSp dst, iRegL src1, vReg src2, vRegD vtmp,\n+                             pRegGov ptmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_LONG &&\n+            n->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize);\n+  match(Set dst (OrReductionV src1 src2));\n+  effect(TEMP_DEF dst, TEMP vtmp, TEMP ptmp, KILL cr);\n+  ins_cost(2 * SVE_COST);\n+  format %{ \"sve_reduce_orL $dst, $src1, $src2\\t# orL reduction partial (sve)\" %}\n@@ -1474,2 +2615,3 @@\n-    __ sve_fadda(as_FloatRegister($src1_dst$$reg), __ D,\n-                 as_PRegister($ptmp$$reg), as_FloatRegister($src2$$reg));\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, T_LONG,\n+                           $src1$$Register, as_FloatRegister($src2$$reg),\n+                           as_PRegister($ptmp$$reg), as_FloatRegister($vtmp$$reg));\n@@ -1480,1 +2622,1 @@\n-\/\/ vector and reduction\n+\/\/ vector or reduction - predicated\n@@ -1482,5 +2624,6 @@\n-instruct reduce_andI(iRegINoSp dst, iRegIorL2I src1, vReg src2, vRegD vtmp) %{\n-  predicate(UseSVE > 0 && n->in(2)->bottom_type()->is_vect()->element_basic_type() != T_LONG &&\n-            n->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize);\n-  match(Set dst (AndReductionV src1 src2));\n-  effect(TEMP_DEF dst, TEMP vtmp);\n+instruct reduce_orI_masked(iRegINoSp dst, iRegIorL2I src1, vReg src2, vRegD tmp, pRegGov pg) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->element_basic_type() != T_LONG &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize);\n+  match(Set dst (OrReductionV (Binary src1 src2) pg));\n+  effect(TEMP_DEF dst, TEMP tmp);\n@@ -1488,1 +2631,1 @@\n-  format %{ \"sve_reduce_andI $dst, $src1, $src2\\t# andB\/S\/I reduction (sve) (may extend)\" %}\n+  format %{ \"sve_reduce_orI $dst, $src1, $pg, $src2\\t# orI reduction predicated (sve) (may extend)\" %}\n@@ -1491,11 +2634,3 @@\n-    Assembler::SIMD_RegVariant variant = __ elemType_to_regVariant(bt);\n-    __ sve_andv(as_FloatRegister($vtmp$$reg), variant, ptrue, as_FloatRegister($src2$$reg));\n-    __ smov($dst$$Register, as_FloatRegister($vtmp$$reg), variant, 0);\n-    __ andw($dst$$Register, $dst$$Register, $src1$$Register);\n-    if (bt == T_BYTE) {\n-      __ sxtb($dst$$Register, $dst$$Register);\n-    } else if (bt == T_SHORT) {\n-      __ sxth($dst$$Register, $dst$$Register);\n-    } else {\n-      assert(bt == T_INT, \"unsupported type\");\n-    }\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, bt,\n+                           $src1$$Register, as_FloatRegister($src2$$reg),\n+                           as_PRegister($pg$$reg), as_FloatRegister($tmp$$reg));\n@@ -1506,6 +2641,6 @@\n-instruct reduce_andI_partial(iRegINoSp dst, iRegIorL2I src1, vReg src2, vRegD vtmp,\n-                             pRegGov ptmp, rFlagsReg cr) %{\n-  predicate(UseSVE > 0 && n->in(2)->bottom_type()->is_vect()->element_basic_type() != T_LONG &&\n-            n->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize);\n-  match(Set dst (AndReductionV src1 src2));\n-  effect(TEMP_DEF dst, TEMP vtmp, TEMP ptmp, KILL cr);\n+instruct reduce_orL_masked(iRegLNoSp dst, iRegL src1, vReg src2, vRegD tmp, pRegGov pg) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->element_basic_type() == T_LONG &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize);\n+  match(Set dst (OrReductionV (Binary src1 src2) pg));\n+  effect(TEMP_DEF dst, TEMP tmp);\n@@ -1513,1 +2648,1 @@\n-  format %{ \"sve_reduce_andI $dst, $src1, $src2\\t# andI reduction partial (sve) (may extend)\" %}\n+  format %{ \"sve_reduce_orL $dst, $src1, $pg, $src2\\t# orL reduction predicated (sve)\" %}\n@@ -1515,15 +2650,3 @@\n-    BasicType bt = Matcher::vector_element_basic_type(this, $src2);\n-    Assembler::SIMD_RegVariant variant = __ elemType_to_regVariant(bt);\n-    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), variant,\n-                          Matcher::vector_length(this, $src2));\n-    __ sve_andv(as_FloatRegister($vtmp$$reg), variant,\n-                as_PRegister($ptmp$$reg), as_FloatRegister($src2$$reg));\n-    __ smov($dst$$Register, as_FloatRegister($vtmp$$reg), variant, 0);\n-    __ andw($dst$$Register, $dst$$Register, $src1$$Register);\n-    if (bt == T_BYTE) {\n-      __ sxtb($dst$$Register, $dst$$Register);\n-    } else if (bt == T_SHORT) {\n-      __ sxth($dst$$Register, $dst$$Register);\n-    } else {\n-      assert(bt == T_INT, \"unsupported type\");\n-    }\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, T_LONG,\n+                           $src1$$Register, as_FloatRegister($src2$$reg),\n+                           as_PRegister($pg$$reg), as_FloatRegister($tmp$$reg));\n@@ -1534,7 +2657,9 @@\n-instruct reduce_andL(iRegLNoSp dst, iRegL src1, vReg src2, vRegD vtmp) %{\n-  predicate(UseSVE > 0 && n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_LONG &&\n-            n->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize);\n-  match(Set dst (AndReductionV src1 src2));\n-  effect(TEMP_DEF dst, TEMP vtmp);\n-  ins_cost(SVE_COST);\n-  format %{ \"sve_reduce_andL $dst, $src1, $src2\\t# andL reduction (sve)\" %}\n+instruct reduce_orI_masked_partial(iRegINoSp dst, iRegIorL2I src1, vReg src2, vRegD vtmp,\n+                                  pRegGov pg, pRegGov ptmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->element_basic_type() != T_LONG &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize);\n+  match(Set dst (OrReductionV (Binary src1 src2) pg));\n+  effect(TEMP_DEF dst, TEMP vtmp, TEMP ptmp, KILL cr);\n+  ins_cost(3 * SVE_COST);\n+  format %{ \"sve_reduce_orI $dst, $src1, $pg, $src2\\t# orI reduction predicated partial (sve) (may extend)\" %}\n@@ -1542,3 +2667,9 @@\n-    __ sve_andv(as_FloatRegister($vtmp$$reg), __ D, ptrue, as_FloatRegister($src2$$reg));\n-    __ umov($dst$$Register, as_FloatRegister($vtmp$$reg), __ D, 0);\n-    __ andr($dst$$Register, $dst$$Register, $src1$$Register);\n+    BasicType bt = Matcher::vector_element_basic_type(this, $src2);\n+    Assembler::SIMD_RegVariant variant = __ elemType_to_regVariant(bt);\n+    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), variant,\n+                          Matcher::vector_length(this, $src2));\n+    __ sve_and(as_PRegister($ptmp$$reg), as_PRegister($ptmp$$reg),\n+               as_PRegister($pg$$reg), as_PRegister($pg$$reg));\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, bt,\n+                           $src1$$Register, as_FloatRegister($src2$$reg),\n+                           as_PRegister($ptmp$$reg), as_FloatRegister($vtmp$$reg));\n@@ -1549,5 +2680,6 @@\n-instruct reduce_andL_partial(iRegLNoSp dst, iRegL src1, vReg src2, vRegD vtmp,\n-                             pRegGov ptmp, rFlagsReg cr) %{\n-  predicate(UseSVE > 0 && n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_LONG &&\n-            n->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize);\n-  match(Set dst (AndReductionV src1 src2));\n+instruct reduce_orL_masked_partial(iRegLNoSp dst, iRegL src1, vReg src2, vRegD vtmp,\n+                                  pRegGov pg, pRegGov ptmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->element_basic_type() == T_LONG &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize);\n+  match(Set dst (OrReductionV (Binary src1 src2) pg));\n@@ -1555,2 +2687,2 @@\n-  ins_cost(SVE_COST);\n-  format %{ \"sve_reduce_andL $dst, $src1, $src2\\t# andL reduction partial (sve)\" %}\n+  ins_cost(3 * SVE_COST);\n+  format %{ \"sve_reduce_orL $dst, $src1, $pg, $src2\\t# orL reduction predicated partial (sve)\" %}\n@@ -1560,4 +2692,5 @@\n-    __ sve_andv(as_FloatRegister($vtmp$$reg), __ D,\n-                as_PRegister($ptmp$$reg), as_FloatRegister($src2$$reg));\n-    __ umov($dst$$Register, as_FloatRegister($vtmp$$reg), __ D, 0);\n-    __ andr($dst$$Register, $dst$$Register, $src1$$Register);\n+    __ sve_and(as_PRegister($ptmp$$reg), as_PRegister($ptmp$$reg),\n+               as_PRegister($pg$$reg), as_PRegister($pg$$reg));\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, T_LONG,\n+                           $src1$$Register, as_FloatRegister($src2$$reg),\n+                           as_PRegister($ptmp$$reg), as_FloatRegister($vtmp$$reg));\n@@ -1568,1 +2701,1 @@\n-\/\/ vector or reduction\n+\/\/ vector xor reduction\n@@ -1570,2 +2703,3 @@\n-instruct reduce_orI(iRegINoSp dst, iRegIorL2I src1, vReg src2, vRegD vtmp) %{\n-  predicate(UseSVE > 0 && n->in(2)->bottom_type()->is_vect()->element_basic_type() != T_LONG &&\n+instruct reduce_eorI(iRegINoSp dst, iRegIorL2I src1, vReg src2, vRegD tmp) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(2)->bottom_type()->is_vect()->element_basic_type() != T_LONG &&\n@@ -1573,2 +2707,2 @@\n-  match(Set dst (OrReductionV src1 src2));\n-  effect(TEMP_DEF dst, TEMP vtmp);\n+  match(Set dst (XorReductionV src1 src2));\n+  effect(TEMP_DEF dst, TEMP tmp);\n@@ -1576,1 +2710,1 @@\n-  format %{ \"sve_reduce_orI $dst, $src1, $src2\\t# orB\/S\/I reduction (sve) (may extend)\" %}\n+  format %{ \"sve_reduce_eorI $dst, $src1, $src2\\t# eorI reduction (sve) (may extend)\" %}\n@@ -1579,11 +2713,3 @@\n-    Assembler::SIMD_RegVariant variant = __ elemType_to_regVariant(bt);\n-    __ sve_orv(as_FloatRegister($vtmp$$reg), variant, ptrue, as_FloatRegister($src2$$reg));\n-    __ smov($dst$$Register, as_FloatRegister($vtmp$$reg), variant, 0);\n-    __ orrw($dst$$Register, $dst$$Register, $src1$$Register);\n-    if (bt == T_BYTE) {\n-      __ sxtb($dst$$Register, $dst$$Register);\n-    } else if (bt == T_SHORT) {\n-      __ sxth($dst$$Register, $dst$$Register);\n-    } else {\n-      assert(bt == T_INT, \"unsupported type\");\n-    }\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, bt,\n+                           $src1$$Register, as_FloatRegister($src2$$reg),\n+                           ptrue, as_FloatRegister($tmp$$reg));\n@@ -1594,1 +2720,17 @@\n-instruct reduce_orI_partial(iRegINoSp dst, iRegIorL2I src1, vReg src2, vRegD vtmp,\n+instruct reduce_eorL(iRegLNoSp dst, iRegL src1, vReg src2, vRegD tmp) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_LONG &&\n+            n->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize);\n+  match(Set dst (XorReductionV src1 src2));\n+  effect(TEMP_DEF dst, TEMP tmp);\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_reduce_eorL $dst, $src1, $src2\\t# eorL reduction (sve)\" %}\n+  ins_encode %{\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, T_LONG,\n+                           $src1$$Register, as_FloatRegister($src2$$reg),\n+                           ptrue, as_FloatRegister($tmp$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct reduce_eorI_partial(iRegINoSp dst, iRegIorL2I src1, vReg src2, vRegD vtmp,\n@@ -1596,1 +2738,2 @@\n-  predicate(UseSVE > 0 && n->in(2)->bottom_type()->is_vect()->element_basic_type() != T_LONG &&\n+  predicate(UseSVE > 0 &&\n+            n->in(2)->bottom_type()->is_vect()->element_basic_type() != T_LONG &&\n@@ -1598,1 +2741,1 @@\n-  match(Set dst (OrReductionV src1 src2));\n+  match(Set dst (XorReductionV src1 src2));\n@@ -1600,2 +2743,2 @@\n-  ins_cost(SVE_COST);\n-  format %{ \"sve_reduce_orI $dst, $src1, $src2\\t# orI reduction partial (sve) (may extend)\" %}\n+  ins_cost(2 * SVE_COST);\n+  format %{ \"sve_reduce_eorI $dst, $src1, $src2\\t# eorI reduction partial (sve) (may extend)\" %}\n@@ -1607,26 +2750,3 @@\n-    __ sve_orv(as_FloatRegister($vtmp$$reg), variant,\n-               as_PRegister($ptmp$$reg), as_FloatRegister($src2$$reg));\n-    __ smov($dst$$Register, as_FloatRegister($vtmp$$reg), variant, 0);\n-    __ orrw($dst$$Register, $dst$$Register, $src1$$Register);\n-    if (bt == T_BYTE) {\n-      __ sxtb($dst$$Register, $dst$$Register);\n-    } else if (bt == T_SHORT) {\n-      __ sxth($dst$$Register, $dst$$Register);\n-    } else {\n-      assert(bt == T_INT, \"unsupported type\");\n-    }\n-  %}\n-  ins_pipe(pipe_slow);\n-%}\n-\n-instruct reduce_orL(iRegLNoSp dst, iRegL src1, vReg src2, vRegD vtmp) %{\n-  predicate(UseSVE > 0 && n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_LONG &&\n-            n->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize);\n-  match(Set dst (OrReductionV src1 src2));\n-  effect(TEMP_DEF dst, TEMP vtmp);\n-  ins_cost(SVE_COST);\n-  format %{ \"sve_reduce_orL $dst, $src1, $src2\\t# orL reduction (sve)\" %}\n-  ins_encode %{\n-    __ sve_orv(as_FloatRegister($vtmp$$reg), __ D, ptrue, as_FloatRegister($src2$$reg));\n-    __ umov($dst$$Register, as_FloatRegister($vtmp$$reg), __ D, 0);\n-    __ orr($dst$$Register, $dst$$Register, $src1$$Register);\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, bt,\n+                           $src1$$Register, as_FloatRegister($src2$$reg),\n+                           as_PRegister($ptmp$$reg), as_FloatRegister($vtmp$$reg));\n@@ -1637,1 +2757,1 @@\n-instruct reduce_orL_partial(iRegLNoSp dst, iRegL src1, vReg src2, vRegD vtmp,\n+instruct reduce_eorL_partial(iRegLNoSp dst, iRegL src1, vReg src2, vRegD vtmp,\n@@ -1639,1 +2759,2 @@\n-  predicate(UseSVE > 0 && n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_LONG &&\n+  predicate(UseSVE > 0 &&\n+            n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_LONG &&\n@@ -1641,1 +2762,1 @@\n-  match(Set dst (OrReductionV src1 src2));\n+  match(Set dst (XorReductionV src1 src2));\n@@ -1643,2 +2764,2 @@\n-  ins_cost(SVE_COST);\n-  format %{ \"sve_reduce_orL $dst, $src1, $src2\\t# orL reduction partial (sve)\" %}\n+  ins_cost(2 * SVE_COST);\n+  format %{ \"sve_reduce_eorL $dst, $src1, $src2\\t# eorL reduction partial (sve)\" %}\n@@ -1648,4 +2769,3 @@\n-    __ sve_orv(as_FloatRegister($vtmp$$reg), __ D,\n-               as_PRegister($ptmp$$reg), as_FloatRegister($src2$$reg));\n-    __ umov($dst$$Register, as_FloatRegister($vtmp$$reg), __ D, 0);\n-    __ orr($dst$$Register, $dst$$Register, $src1$$Register);\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, T_LONG,\n+                           $src1$$Register, as_FloatRegister($src2$$reg),\n+                           as_PRegister($ptmp$$reg), as_FloatRegister($vtmp$$reg));\n@@ -1656,1 +2776,18 @@\n-\/\/ vector xor reduction\n+\/\/ vector xor reduction - predicated\n+\n+instruct reduce_eorI_masked(iRegINoSp dst, iRegIorL2I src1, vReg src2, vRegD tmp, pRegGov pg) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->element_basic_type() != T_LONG &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize);\n+  match(Set dst (XorReductionV (Binary src1 src2) pg));\n+  effect(TEMP_DEF dst, TEMP tmp);\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_reduce_eorI $dst, $src1, $pg, $src2\\t# eorI reduction predicated (sve) (may extend)\" %}\n+  ins_encode %{\n+    BasicType bt = Matcher::vector_element_basic_type(this, $src2);\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, bt,\n+                           $src1$$Register, as_FloatRegister($src2$$reg),\n+                           as_PRegister($pg$$reg), as_FloatRegister($tmp$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n@@ -1658,5 +2795,6 @@\n-instruct reduce_eorI(iRegINoSp dst, iRegIorL2I src1, vReg src2, vRegD vtmp) %{\n-  predicate(UseSVE > 0 && n->in(2)->bottom_type()->is_vect()->element_basic_type() != T_LONG &&\n-            n->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize);\n-  match(Set dst (XorReductionV src1 src2));\n-  effect(TEMP_DEF dst, TEMP vtmp);\n+instruct reduce_eorL_masked(iRegLNoSp dst, iRegL src1, vReg src2, vRegD tmp, pRegGov pg) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->element_basic_type() == T_LONG &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize);\n+  match(Set dst (XorReductionV (Binary src1 src2) pg));\n+  effect(TEMP_DEF dst, TEMP tmp);\n@@ -1664,1 +2802,1 @@\n-  format %{ \"sve_reduce_eorI $dst, $src1, $src2\\t# xorB\/H\/I reduction (sve) (may extend)\" %}\n+  format %{ \"sve_reduce_eorL $dst, $src1, $pg, $src2\\t# eorL reduction predicated (sve)\" %}\n@@ -1666,12 +2804,3 @@\n-    BasicType bt = Matcher::vector_element_basic_type(this, $src2);\n-    Assembler::SIMD_RegVariant variant = __ elemType_to_regVariant(bt);\n-    __ sve_eorv(as_FloatRegister($vtmp$$reg), variant, ptrue, as_FloatRegister($src2$$reg));\n-    __ smov($dst$$Register, as_FloatRegister($vtmp$$reg), variant, 0);\n-    __ eorw($dst$$Register, $dst$$Register, $src1$$Register);\n-    if (bt == T_BYTE) {\n-      __ sxtb($dst$$Register, $dst$$Register);\n-    } else if (bt == T_SHORT) {\n-      __ sxth($dst$$Register, $dst$$Register);\n-    } else {\n-      assert(bt == T_INT, \"unsupported type\");\n-    }\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, T_LONG,\n+                           $src1$$Register, as_FloatRegister($src2$$reg),\n+                           as_PRegister($pg$$reg), as_FloatRegister($tmp$$reg));\n@@ -1682,5 +2811,6 @@\n-instruct reduce_eorI_partial(iRegINoSp dst, iRegIorL2I src1, vReg src2, vRegD vtmp,\n-                             pRegGov ptmp, rFlagsReg cr) %{\n-  predicate(UseSVE > 0 && n->in(2)->bottom_type()->is_vect()->element_basic_type() != T_LONG &&\n-            n->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize);\n-  match(Set dst (XorReductionV src1 src2));\n+instruct reduce_eorI_masked_partial(iRegINoSp dst, iRegIorL2I src1, vReg src2, vRegD vtmp,\n+                                  pRegGov pg, pRegGov ptmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->element_basic_type() != T_LONG &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize);\n+  match(Set dst (XorReductionV (Binary src1 src2) pg));\n@@ -1688,2 +2818,2 @@\n-  ins_cost(SVE_COST);\n-  format %{ \"sve_reduce_eorI $dst, $src1, $src2\\t# xorI reduction partial (sve) (may extend)\" %}\n+  ins_cost(3 * SVE_COST);\n+  format %{ \"sve_reduce_eorI $dst, $src1, $pg, $src2\\t# eorI reduction predicated partial (sve) (may extend)\" %}\n@@ -1695,26 +2825,5 @@\n-    __ sve_eorv(as_FloatRegister($vtmp$$reg), variant,\n-                as_PRegister($ptmp$$reg), as_FloatRegister($src2$$reg));\n-    __ smov($dst$$Register, as_FloatRegister($vtmp$$reg), variant, 0);\n-    __ eorw($dst$$Register, $dst$$Register, $src1$$Register);\n-    if (bt == T_BYTE) {\n-      __ sxtb($dst$$Register, $dst$$Register);\n-    } else if (bt == T_SHORT) {\n-      __ sxth($dst$$Register, $dst$$Register);\n-    } else {\n-      assert(bt == T_INT, \"unsupported type\");\n-    }\n-  %}\n-  ins_pipe(pipe_slow);\n-%}\n-\n-instruct reduce_eorL(iRegLNoSp dst, iRegL src1, vReg src2, vRegD vtmp) %{\n-  predicate(UseSVE > 0 && n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_LONG &&\n-            n->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize);\n-  match(Set dst (XorReductionV src1 src2));\n-  effect(TEMP_DEF dst, TEMP vtmp);\n-  ins_cost(SVE_COST);\n-  format %{ \"sve_reduce_eorL $dst, $src1, $src2\\t# xorL reduction (sve)\" %}\n-  ins_encode %{\n-    __ sve_eorv(as_FloatRegister($vtmp$$reg), __ D, ptrue, as_FloatRegister($src2$$reg));\n-    __ umov($dst$$Register, as_FloatRegister($vtmp$$reg), __ D, 0);\n-    __ eor($dst$$Register, $dst$$Register, $src1$$Register);\n+    __ sve_and(as_PRegister($ptmp$$reg), as_PRegister($ptmp$$reg),\n+               as_PRegister($pg$$reg), as_PRegister($pg$$reg));\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, bt,\n+                           $src1$$Register, as_FloatRegister($src2$$reg),\n+                           as_PRegister($ptmp$$reg), as_FloatRegister($vtmp$$reg));\n@@ -1725,5 +2834,6 @@\n-instruct reduce_eorL_partial(iRegLNoSp dst, iRegL src1, vReg src2, vRegD vtmp,\n-                             pRegGov ptmp, rFlagsReg cr) %{\n-  predicate(UseSVE > 0 && n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_LONG &&\n-            n->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize);\n-  match(Set dst (XorReductionV src1 src2));\n+instruct reduce_eorL_masked_partial(iRegLNoSp dst, iRegL src1, vReg src2, vRegD vtmp,\n+                                  pRegGov pg, pRegGov ptmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->element_basic_type() == T_LONG &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize);\n+  match(Set dst (XorReductionV (Binary src1 src2) pg));\n@@ -1731,2 +2841,2 @@\n-  ins_cost(SVE_COST);\n-  format %{ \"sve_reduce_eorL $dst, $src1, $src2\\t# xorL reduction partial (sve)\" %}\n+  ins_cost(3 * SVE_COST);\n+  format %{ \"sve_reduce_eorL $dst, $src1, $pg, $src2\\t# eorL reduction predicated partial (sve)\" %}\n@@ -1736,4 +2846,5 @@\n-    __ sve_eorv(as_FloatRegister($vtmp$$reg), __ D,\n-                as_PRegister($ptmp$$reg), as_FloatRegister($src2$$reg));\n-    __ umov($dst$$Register, as_FloatRegister($vtmp$$reg), __ D, 0);\n-    __ eor($dst$$Register, $dst$$Register, $src1$$Register);\n+    __ sve_and(as_PRegister($ptmp$$reg), as_PRegister($ptmp$$reg),\n+               as_PRegister($pg$$reg), as_PRegister($pg$$reg));\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, T_LONG,\n+                           $src1$$Register, as_FloatRegister($src2$$reg),\n+                           as_PRegister($ptmp$$reg), as_FloatRegister($vtmp$$reg));\n@@ -1744,1 +2855,0 @@\n-\n@@ -1747,5 +2857,5 @@\n-instruct reduce_maxI(iRegINoSp dst, iRegIorL2I src1, vReg src2, vRegD vtmp) %{\n-  predicate(UseSVE > 0 && n->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize &&\n-            (n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_BYTE ||\n-             n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_SHORT ||\n-             n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_INT));\n+instruct reduce_maxI(iRegINoSp dst, iRegIorL2I src1, vReg src2, vRegD tmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize &&\n+            n->in(2)->bottom_type()->is_vect()->element_basic_type() != T_LONG &&\n+            is_integral_type(n->in(2)->bottom_type()->is_vect()->element_basic_type()));\n@@ -1753,1 +2863,1 @@\n-  effect(TEMP_DEF dst, TEMP vtmp);\n+  effect(TEMP_DEF dst, TEMP tmp, KILL cr);\n@@ -1755,1 +2865,1 @@\n-  format %{ \"sve_reduce_maxI $dst, $src1, $src2\\t# reduce maxB\/S\/I (sve)\" %}\n+  format %{ \"sve_reduce_maxI $dst, $src1, $src2\\t# maxI reduction (sve)\" %}\n@@ -1758,5 +2868,3 @@\n-    Assembler::SIMD_RegVariant variant = __ elemType_to_regVariant(bt);\n-    __ sve_smaxv(as_FloatRegister($vtmp$$reg), variant, ptrue, as_FloatRegister($src2$$reg));\n-    __ smov($dst$$Register, as_FloatRegister($vtmp$$reg), variant, 0);\n-    __ cmpw($dst$$Register, $src1$$Register);\n-    __ cselw(as_Register($dst$$reg), as_Register($dst$$reg), as_Register($src1$$reg), Assembler::GT);\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, bt,\n+                           $src1$$Register, as_FloatRegister($src2$$reg),\n+                           ptrue, as_FloatRegister($tmp$$reg));\n@@ -1767,6 +2875,4 @@\n-instruct reduce_maxI_partial(iRegINoSp dst, iRegIorL2I src1, vReg src2, vRegD vtmp,\n-                             pRegGov ptmp, rFlagsReg cr) %{\n-  predicate(UseSVE > 0 && n->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize &&\n-            (n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_BYTE ||\n-             n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_SHORT ||\n-             n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_INT));\n+instruct reduce_maxL(iRegLNoSp dst, iRegL src1, vReg src2, vRegD tmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize &&\n+            n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_LONG);\n@@ -1774,1 +2880,1 @@\n-  effect(TEMP_DEF dst, TEMP vtmp, TEMP ptmp, KILL cr);\n+  effect(TEMP_DEF dst, TEMP tmp, KILL cr);\n@@ -1776,1 +2882,1 @@\n-  format %{ \"sve_reduce_maxI $dst, $src1, $src2\\t# reduce maxI partial (sve)\" %}\n+  format %{ \"sve_reduce_maxL $dst, $src1, $src2\\t# maxL reduction (sve)\" %}\n@@ -1778,9 +2884,3 @@\n-    BasicType bt = Matcher::vector_element_basic_type(this, $src2);\n-    Assembler::SIMD_RegVariant variant = __ elemType_to_regVariant(bt);\n-    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), variant,\n-                          Matcher::vector_length(this, $src2));\n-    __ sve_smaxv(as_FloatRegister($vtmp$$reg), variant,\n-                 as_PRegister($ptmp$$reg), as_FloatRegister($src2$$reg));\n-    __ smov($dst$$Register, as_FloatRegister($vtmp$$reg), variant, 0);\n-    __ cmpw($dst$$Register, $src1$$Register);\n-    __ cselw(as_Register($dst$$reg), as_Register($dst$$reg), as_Register($src1$$reg), Assembler::GT);\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, T_LONG,\n+                           $src1$$Register, as_FloatRegister($src2$$reg),\n+                           ptrue, as_FloatRegister($tmp$$reg));\n@@ -1791,3 +2891,6 @@\n-instruct reduce_maxL(iRegLNoSp dst, iRegL src1, vReg src2, vRegD vtmp) %{\n-  predicate(UseSVE > 0 && n->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize &&\n-            n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_LONG);\n+instruct reduce_maxI_partial(iRegINoSp dst, iRegIorL2I src1, vReg src2, vRegD vtmp,\n+                             pRegGov ptmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize &&\n+            n->in(2)->bottom_type()->is_vect()->element_basic_type() != T_LONG &&\n+            is_integral_type(n->in(2)->bottom_type()->is_vect()->element_basic_type()));\n@@ -1795,3 +2898,3 @@\n-  effect(TEMP_DEF dst, TEMP vtmp);\n-  ins_cost(SVE_COST);\n-  format %{ \"sve_reduce_maxL $dst, $src1, $src2\\t# reduce maxL partial (sve)\" %}\n+  effect(TEMP_DEF dst, TEMP vtmp, TEMP ptmp, KILL cr);\n+  ins_cost(2 * SVE_COST);\n+  format %{ \"sve_reduce_maxI $dst, $src1, $src2\\t# maxI reduction partial (sve)\" %}\n@@ -1799,4 +2902,7 @@\n-    __ sve_smaxv(as_FloatRegister($vtmp$$reg), __ D, ptrue, as_FloatRegister($src2$$reg));\n-    __ umov($dst$$Register, as_FloatRegister($vtmp$$reg), __ D, 0);\n-    __ cmp($dst$$Register, $src1$$Register);\n-    __ csel(as_Register($dst$$reg), as_Register($dst$$reg), as_Register($src1$$reg), Assembler::GT);\n+    BasicType bt = Matcher::vector_element_basic_type(this, $src2);\n+    Assembler::SIMD_RegVariant variant = __ elemType_to_regVariant(bt);\n+    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), variant,\n+                          Matcher::vector_length(this, $src2));\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, bt,\n+                           $src1$$Register, as_FloatRegister($src2$$reg),\n+                           as_PRegister($ptmp$$reg), as_FloatRegister($vtmp$$reg));\n@@ -1809,1 +2915,2 @@\n-  predicate(UseSVE > 0 && n->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize &&\n+  predicate(UseSVE > 0 &&\n+            n->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize &&\n@@ -1813,2 +2920,2 @@\n-  ins_cost(SVE_COST);\n-  format %{ \"sve_reduce_maxL $dst, $src1, $src2\\t# reduce maxL partial (sve)\" %}\n+  ins_cost(2 * SVE_COST);\n+  format %{ \"sve_reduce_maxL $dst, $src1, $src2\\t# maxL reduction  partial (sve)\" %}\n@@ -1818,5 +2925,3 @@\n-    __ sve_smaxv(as_FloatRegister($vtmp$$reg), __ D,\n-                 as_PRegister($ptmp$$reg), as_FloatRegister($src2$$reg));\n-    __ umov($dst$$Register, as_FloatRegister($vtmp$$reg), __ D, 0);\n-    __ cmp($dst$$Register, $src1$$Register);\n-    __ csel(as_Register($dst$$reg), as_Register($dst$$reg), as_Register($src1$$reg), Assembler::GT);\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, T_LONG,\n+                           $src1$$Register, as_FloatRegister($src2$$reg),\n+                           as_PRegister($ptmp$$reg), as_FloatRegister($vtmp$$reg));\n@@ -1828,1 +2933,2 @@\n-  predicate(UseSVE > 0 && n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_FLOAT &&\n+  predicate(UseSVE > 0 &&\n+            n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_FLOAT &&\n@@ -1833,2 +2939,1 @@\n-  format %{ \"sve_fmaxv $dst, $src2 # vector (sve) (S)\\n\\t\"\n-            \"fmaxs $dst, $dst, $src1\\t# max reduction F\" %}\n+  format %{ \"sve_reduce_maxF $dst, $src1, $src2\\t# maxF reduction (sve)\" %}\n@@ -1836,2 +2941,1 @@\n-    __ sve_fmaxv(as_FloatRegister($dst$$reg), __ S,\n-         ptrue, as_FloatRegister($src2$$reg));\n+    __ sve_fmaxv(as_FloatRegister($dst$$reg), __ S, ptrue, as_FloatRegister($src2$$reg));\n@@ -1845,1 +2949,2 @@\n-  predicate(UseSVE > 0 && n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_FLOAT &&\n+  predicate(UseSVE > 0 &&\n+            n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_FLOAT &&\n@@ -1850,1 +2955,1 @@\n-  format %{ \"sve_reduce_maxF $dst, $src1, $src2\\t# reduce max S partial (sve)\" %}\n+  format %{ \"sve_reduce_maxF $dst, $src1, $src2\\t# maxF reduction partial (sve)\" %}\n@@ -1854,2 +2959,1 @@\n-    __ sve_fmaxv(as_FloatRegister($dst$$reg), __ S,\n-         as_PRegister($ptmp$$reg), as_FloatRegister($src2$$reg));\n+    __ sve_fmaxv(as_FloatRegister($dst$$reg), __ S, as_PRegister($ptmp$$reg), as_FloatRegister($src2$$reg));\n@@ -1862,1 +2966,2 @@\n-  predicate(UseSVE > 0 && n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_DOUBLE &&\n+  predicate(UseSVE > 0 &&\n+            n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_DOUBLE &&\n@@ -1867,2 +2972,1 @@\n-  format %{ \"sve_fmaxv $dst, $src2 # vector (sve) (D)\\n\\t\"\n-            \"fmaxs $dst, $dst, $src1\\t# max reduction D\" %}\n+  format %{ \"sve_reduce_maxD $dst, $src1, $src2\\t# maxD reduction (sve)\" %}\n@@ -1870,2 +2974,1 @@\n-    __ sve_fmaxv(as_FloatRegister($dst$$reg), __ D,\n-         ptrue, as_FloatRegister($src2$$reg));\n+    __ sve_fmaxv(as_FloatRegister($dst$$reg), __ D, ptrue, as_FloatRegister($src2$$reg));\n@@ -1879,1 +2982,2 @@\n-  predicate(UseSVE > 0 && n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_DOUBLE &&\n+  predicate(UseSVE > 0 &&\n+            n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_DOUBLE &&\n@@ -1884,1 +2988,1 @@\n-  format %{ \"sve_reduce_maxD $dst, $src1, $src2\\t# reduce max D partial (sve)\" %}\n+  format %{ \"sve_reduce_maxD $dst, $src1, $src2\\t# maxD reduction partial (sve)\" %}\n@@ -1888,2 +2992,1 @@\n-    __ sve_fmaxv(as_FloatRegister($dst$$reg), __ D,\n-         as_PRegister($ptmp$$reg), as_FloatRegister($src2$$reg));\n+    __ sve_fmaxv(as_FloatRegister($dst$$reg), __ D, as_PRegister($ptmp$$reg), as_FloatRegister($src2$$reg));\n@@ -1895,1 +2998,1 @@\n-\/\/ vector min reduction\n+\/\/ vector max reduction - predicated\n@@ -1897,7 +3000,8 @@\n-instruct reduce_minI(iRegINoSp dst, iRegIorL2I src1, vReg src2, vRegD vtmp) %{\n-  predicate(UseSVE > 0 && n->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize &&\n-            (n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_BYTE ||\n-             n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_SHORT ||\n-             n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_INT));\n-  match(Set dst (MinReductionV src1 src2));\n-  effect(TEMP_DEF dst, TEMP vtmp);\n+instruct reduce_maxI_masked(iRegINoSp dst, iRegIorL2I src1, vReg src2, vRegD tmp,\n+                           pRegGov pg, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->element_basic_type() != T_LONG &&\n+            is_integral_type(n->in(1)->in(2)->bottom_type()->is_vect()->element_basic_type()));\n+  match(Set dst (MaxReductionV (Binary src1 src2) pg));\n+  effect(TEMP_DEF dst, TEMP tmp, KILL cr);\n@@ -1905,1 +3009,1 @@\n-  format %{ \"sve_reduce_minI $dst, $src1, $src2\\t# reduce minB\/S\/I (sve)\" %}\n+  format %{ \"sve_reduce_maxI $dst, $src1, $pg, $src2\\t# maxI reduction predicated (sve)\" %}\n@@ -1908,5 +3012,3 @@\n-    Assembler::SIMD_RegVariant variant = __ elemType_to_regVariant(bt);\n-    __ sve_sminv(as_FloatRegister($vtmp$$reg), variant, ptrue, as_FloatRegister($src2$$reg));\n-    __ smov($dst$$Register, as_FloatRegister($vtmp$$reg), variant, 0);\n-    __ cmpw($dst$$Register, $src1$$Register);\n-    __ cselw(as_Register($dst$$reg), as_Register($dst$$reg), as_Register($src1$$reg), Assembler::LT);\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, bt,\n+                           $src1$$Register, as_FloatRegister($src2$$reg),\n+                           as_PRegister($pg$$reg), as_FloatRegister($tmp$$reg));\n@@ -1917,8 +3019,7 @@\n-instruct reduce_minI_partial(iRegINoSp dst, iRegIorL2I src1, vReg src2, vRegD vtmp,\n-                             pRegGov ptmp, rFlagsReg cr) %{\n-  predicate(UseSVE > 0 && n->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize &&\n-            (n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_BYTE ||\n-             n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_SHORT ||\n-             n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_INT));\n-  match(Set dst (MinReductionV src1 src2));\n-  effect(TEMP_DEF dst, TEMP vtmp, TEMP ptmp, KILL cr);\n+instruct reduce_maxL_masked(iRegLNoSp dst, iRegL src1, vReg src2, vRegD tmp,\n+                          pRegGov pg, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->element_basic_type() == T_LONG);\n+  match(Set dst (MaxReductionV (Binary src1 src2) pg));\n+  effect(TEMP_DEF dst, TEMP tmp, KILL cr);\n@@ -1926,1 +3027,19 @@\n-  format %{ \"sve_reduce_minI $dst, $src1, $src2\\t# reduce minI partial (sve)\" %}\n+  format %{ \"sve_reduce_maxL $dst, $src1, $pg, $src2\\t# maxL reduction predicated (sve)\" %}\n+  ins_encode %{\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, T_LONG,\n+                           $src1$$Register, as_FloatRegister($src2$$reg),\n+                           as_PRegister($pg$$reg), as_FloatRegister($tmp$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct reduce_maxI_masked_partial(iRegINoSp dst, iRegIorL2I src1, vReg src2, vRegD vtmp,\n+                                  pRegGov pg, pRegGov ptmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->element_basic_type() != T_LONG &&\n+            is_integral_type(n->in(1)->in(2)->bottom_type()->is_vect()->element_basic_type()));\n+  match(Set dst (MaxReductionV (Binary src1 src2) pg));\n+  effect(TEMP_DEF dst, TEMP vtmp, TEMP ptmp, KILL cr);\n+  ins_cost(3 * SVE_COST);\n+  format %{ \"sve_reduce_maxI $dst, $src1, $pg, $src2\\t# maxI reduction predicated partial (sve)\" %}\n@@ -1932,5 +3051,116 @@\n-    __ sve_sminv(as_FloatRegister($vtmp$$reg), variant,\n-                 as_PRegister($ptmp$$reg), as_FloatRegister($src2$$reg));\n-    __ smov($dst$$Register, as_FloatRegister($vtmp$$reg), variant, 0);\n-    __ cmpw($dst$$Register, $src1$$Register);\n-    __ cselw(as_Register($dst$$reg), as_Register($dst$$reg), as_Register($src1$$reg), Assembler::LT);\n+    __ sve_and(as_PRegister($ptmp$$reg), as_PRegister($ptmp$$reg),\n+               as_PRegister($pg$$reg), as_PRegister($pg$$reg));\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, bt,\n+                           $src1$$Register, as_FloatRegister($src2$$reg),\n+                           as_PRegister($ptmp$$reg), as_FloatRegister($vtmp$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct reduce_maxL_masked_partial(iRegLNoSp dst, iRegL src1, vReg src2, vRegD vtmp,\n+                                  pRegGov pg, pRegGov ptmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->element_basic_type() == T_LONG);\n+  match(Set dst (MaxReductionV (Binary src1 src2) pg));\n+  effect(TEMP_DEF dst, TEMP vtmp, TEMP ptmp, KILL cr);\n+  ins_cost(3 * SVE_COST);\n+  format %{ \"sve_reduce_maxL $dst, $src1, $pg, $src2\\t# maxL reduction predicated partial (sve)\" %}\n+  ins_encode %{\n+    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), __ D,\n+                          Matcher::vector_length(this, $src2));\n+    __ sve_and(as_PRegister($ptmp$$reg), as_PRegister($ptmp$$reg),\n+               as_PRegister($pg$$reg), as_PRegister($pg$$reg));\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, T_LONG,\n+                           $src1$$Register, as_FloatRegister($src2$$reg),\n+                           as_PRegister($ptmp$$reg), as_FloatRegister($vtmp$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct reduce_maxF_masked(vRegF dst, vRegF src1, vReg src2, pRegGov pg) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->element_basic_type() == T_FLOAT &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize);\n+  match(Set dst (MaxReductionV (Binary src1 src2) pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_reduce_maxF $dst, $src1, $pg, $src2\\t# maxF reduction predicated (sve)\" %}\n+  ins_encode %{\n+    __ sve_fmaxv(as_FloatRegister($dst$$reg), __ S, as_PRegister($pg$$reg), as_FloatRegister($src2$$reg));\n+    __ fmaxs(as_FloatRegister($dst$$reg), as_FloatRegister($dst$$reg), as_FloatRegister($src1$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct reduce_maxD_masked(vRegD dst, vRegD src1, vReg src2, pRegGov pg) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->element_basic_type() == T_DOUBLE &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize);\n+  match(Set dst (MaxReductionV (Binary src1 src2) pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_reduce_maxD $dst, $src1, $pg, $src2\\t# maxD reduction predicated (sve)\" %}\n+  ins_encode %{\n+    __ sve_fmaxv(as_FloatRegister($dst$$reg), __ D, as_PRegister($pg$$reg), as_FloatRegister($src2$$reg));\n+    __ fmaxd(as_FloatRegister($dst$$reg), as_FloatRegister($dst$$reg), as_FloatRegister($src1$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct reduce_maxF_masked_partial(vRegF dst, vRegF src1, vReg src2, pRegGov pg,\n+                                    pRegGov ptmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->element_basic_type() == T_FLOAT &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize);\n+  match(Set dst (MaxReductionV (Binary src1 src2) pg));\n+  effect(TEMP_DEF dst, TEMP ptmp, KILL cr);\n+  ins_cost(3 * SVE_COST);\n+  format %{ \"sve_reduce_maxF $dst, $src1, $pg, $src2\\t# maxF reduction predicated partial (sve)\" %}\n+  ins_encode %{\n+    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), __ S,\n+                          Matcher::vector_length(this, $src2));\n+    __ sve_and(as_PRegister($ptmp$$reg), as_PRegister($ptmp$$reg),\n+               as_PRegister($pg$$reg), as_PRegister($pg$$reg));\n+    __ sve_fmaxv(as_FloatRegister($dst$$reg), __ S,\n+               as_PRegister($ptmp$$reg), as_FloatRegister($src2$$reg));\n+    __ fmaxs(as_FloatRegister($dst$$reg), as_FloatRegister($dst$$reg), as_FloatRegister($src1$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct reduce_maxD_masked_partial(vRegD dst, vRegD src1, vReg src2, pRegGov pg,\n+                                    pRegGov ptmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->element_basic_type() == T_DOUBLE &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize);\n+  match(Set dst (MaxReductionV (Binary src1 src2) pg));\n+  effect(TEMP_DEF dst, TEMP ptmp, KILL cr);\n+  ins_cost(3 * SVE_COST);\n+  format %{ \"sve_reduce_maxD $dst, $src1, $pg, $src2\\t# maxD reduction predicated partial (sve)\" %}\n+  ins_encode %{\n+    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), __ D,\n+                          Matcher::vector_length(this, $src2));\n+    __ sve_and(as_PRegister($ptmp$$reg), as_PRegister($ptmp$$reg),\n+               as_PRegister($pg$$reg), as_PRegister($pg$$reg));\n+    __ sve_fmaxv(as_FloatRegister($dst$$reg), __ D,\n+               as_PRegister($ptmp$$reg), as_FloatRegister($src2$$reg));\n+    __ fmaxd(as_FloatRegister($dst$$reg), as_FloatRegister($dst$$reg), as_FloatRegister($src1$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ vector min reduction\n+\n+instruct reduce_minI(iRegINoSp dst, iRegIorL2I src1, vReg src2, vRegD tmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize &&\n+            n->in(2)->bottom_type()->is_vect()->element_basic_type() != T_LONG &&\n+            is_integral_type(n->in(2)->bottom_type()->is_vect()->element_basic_type()));\n+  match(Set dst (MinReductionV src1 src2));\n+  effect(TEMP_DEF dst, TEMP tmp, KILL cr);\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_reduce_minI $dst, $src1, $src2\\t# minI reduction (sve)\" %}\n+  ins_encode %{\n+    BasicType bt = Matcher::vector_element_basic_type(this, $src2);\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, bt,\n+                           $src1$$Register, as_FloatRegister($src2$$reg),\n+                           ptrue, as_FloatRegister($tmp$$reg));\n@@ -1941,2 +3171,3 @@\n-instruct reduce_minL(iRegLNoSp dst, iRegL src1, vReg src2, vRegD vtmp) %{\n-  predicate(UseSVE > 0 && n->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize &&\n+instruct reduce_minL(iRegLNoSp dst, iRegL src1, vReg src2, vRegD tmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize &&\n@@ -1945,1 +3176,1 @@\n-  effect(TEMP_DEF dst, TEMP vtmp);\n+  effect(TEMP_DEF dst, TEMP tmp, KILL cr);\n@@ -1947,1 +3178,19 @@\n-  format %{ \"sve_reduce_minL $dst, $src1, $src2\\t# reduce minL partial (sve)\" %}\n+  format %{ \"sve_reduce_minL $dst, $src1, $src2\\t# minL reduction (sve)\" %}\n+  ins_encode %{\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, T_LONG,\n+                           $src1$$Register, as_FloatRegister($src2$$reg),\n+                           ptrue, as_FloatRegister($tmp$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct reduce_minI_partial(iRegINoSp dst, iRegIorL2I src1, vReg src2, vRegD vtmp,\n+                             pRegGov ptmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize &&\n+            n->in(2)->bottom_type()->is_vect()->element_basic_type() != T_LONG &&\n+            is_integral_type(n->in(2)->bottom_type()->is_vect()->element_basic_type()));\n+  match(Set dst (MinReductionV src1 src2));\n+  effect(TEMP_DEF dst, TEMP vtmp, TEMP ptmp, KILL cr);\n+  ins_cost(2 * SVE_COST);\n+  format %{ \"sve_reduce_minI $dst, $src1, $src2\\t# minI reduction partial (sve)\" %}\n@@ -1949,4 +3198,7 @@\n-    __ sve_sminv(as_FloatRegister($vtmp$$reg), __ D, ptrue, as_FloatRegister($src2$$reg));\n-    __ umov($dst$$Register, as_FloatRegister($vtmp$$reg), __ D, 0);\n-    __ cmp($dst$$Register, $src1$$Register);\n-    __ csel(as_Register($dst$$reg), as_Register($dst$$reg), as_Register($src1$$reg), Assembler::LT);\n+    BasicType bt = Matcher::vector_element_basic_type(this, $src2);\n+    Assembler::SIMD_RegVariant variant = __ elemType_to_regVariant(bt);\n+    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), variant,\n+                          Matcher::vector_length(this, $src2));\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, bt,\n+                           $src1$$Register, as_FloatRegister($src2$$reg),\n+                           as_PRegister($ptmp$$reg), as_FloatRegister($vtmp$$reg));\n@@ -1959,1 +3211,2 @@\n-  predicate(UseSVE > 0 && n->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize &&\n+  predicate(UseSVE > 0 &&\n+            n->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize &&\n@@ -1963,2 +3216,2 @@\n-  ins_cost(SVE_COST);\n-  format %{ \"sve_reduce_minL $dst, $src1, $src2\\t# reduce minL partial (sve)\" %}\n+  ins_cost(2 * SVE_COST);\n+  format %{ \"sve_reduce_minL $dst, $src1, $src2\\t# minL reduction  partial (sve)\" %}\n@@ -1968,5 +3221,3 @@\n-    __ sve_sminv(as_FloatRegister($vtmp$$reg), __ D,\n-                 as_PRegister($ptmp$$reg), as_FloatRegister($src2$$reg));\n-    __ umov($dst$$Register, as_FloatRegister($vtmp$$reg), __ D, 0);\n-    __ cmp($dst$$Register, $src1$$Register);\n-    __ csel(as_Register($dst$$reg), as_Register($dst$$reg), as_Register($src1$$reg), Assembler::LT);\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, T_LONG,\n+                           $src1$$Register, as_FloatRegister($src2$$reg),\n+                           as_PRegister($ptmp$$reg), as_FloatRegister($vtmp$$reg));\n@@ -1978,1 +3229,2 @@\n-  predicate(UseSVE > 0 && n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_FLOAT &&\n+  predicate(UseSVE > 0 &&\n+            n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_FLOAT &&\n@@ -1983,2 +3235,1 @@\n-  format %{ \"sve_fminv $dst, $src2 # vector (sve) (S)\\n\\t\"\n-            \"fmins $dst, $dst, $src1\\t# min reduction F\" %}\n+  format %{ \"sve_reduce_minF $dst, $src1, $src2\\t# minF reduction (sve)\" %}\n@@ -1986,2 +3237,1 @@\n-    __ sve_fminv(as_FloatRegister($dst$$reg), __ S,\n-         ptrue, as_FloatRegister($src2$$reg));\n+    __ sve_fminv(as_FloatRegister($dst$$reg), __ S, ptrue, as_FloatRegister($src2$$reg));\n@@ -1995,1 +3245,2 @@\n-  predicate(UseSVE > 0 && n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_FLOAT &&\n+  predicate(UseSVE > 0 &&\n+            n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_FLOAT &&\n@@ -2000,1 +3251,1 @@\n-  format %{ \"sve_reduce_minF $dst, $src1, $src2\\t# reduce min S partial (sve)\" %}\n+  format %{ \"sve_reduce_minF $dst, $src1, $src2\\t# minF reduction partial (sve)\" %}\n@@ -2004,2 +3255,1 @@\n-    __ sve_fminv(as_FloatRegister($dst$$reg), __ S,\n-         as_PRegister($ptmp$$reg), as_FloatRegister($src2$$reg));\n+    __ sve_fminv(as_FloatRegister($dst$$reg), __ S, as_PRegister($ptmp$$reg), as_FloatRegister($src2$$reg));\n@@ -2012,1 +3262,2 @@\n-  predicate(UseSVE > 0 && n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_DOUBLE &&\n+  predicate(UseSVE > 0 &&\n+            n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_DOUBLE &&\n@@ -2017,2 +3268,1 @@\n-  format %{ \"sve_fminv $dst, $src2 # vector (sve) (D)\\n\\t\"\n-            \"fmins $dst, $dst, $src1\\t# min reduction D\" %}\n+  format %{ \"sve_reduce_minD $dst, $src1, $src2\\t# minD reduction (sve)\" %}\n@@ -2020,2 +3270,130 @@\n-    __ sve_fminv(as_FloatRegister($dst$$reg), __ D,\n-         ptrue, as_FloatRegister($src2$$reg));\n+    __ sve_fminv(as_FloatRegister($dst$$reg), __ D, ptrue, as_FloatRegister($src2$$reg));\n+    __ fmind(as_FloatRegister($dst$$reg), as_FloatRegister($dst$$reg), as_FloatRegister($src1$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct reduce_minD_partial(vRegD dst, vRegD src1, vReg src2,\n+                             pRegGov ptmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_DOUBLE &&\n+            n->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize);\n+  match(Set dst (MinReductionV src1 src2));\n+  ins_cost(INSN_COST);\n+  effect(TEMP_DEF dst, TEMP ptmp, KILL cr);\n+  format %{ \"sve_reduce_minD $dst, $src1, $src2\\t# minD reduction partial (sve)\" %}\n+  ins_encode %{\n+    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), __ D,\n+                          Matcher::vector_length(this, $src2));\n+    __ sve_fminv(as_FloatRegister($dst$$reg), __ D, as_PRegister($ptmp$$reg), as_FloatRegister($src2$$reg));\n+    __ fmind(as_FloatRegister($dst$$reg), as_FloatRegister($dst$$reg), as_FloatRegister($src1$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ vector min reduction - predicated\n+\n+instruct reduce_minI_masked(iRegINoSp dst, iRegIorL2I src1, vReg src2, vRegD tmp,\n+                           pRegGov pg, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->element_basic_type() != T_LONG &&\n+            is_integral_type(n->in(1)->in(2)->bottom_type()->is_vect()->element_basic_type()));\n+  match(Set dst (MinReductionV (Binary src1 src2) pg));\n+  effect(TEMP_DEF dst, TEMP tmp, KILL cr);\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_reduce_minI $dst, $src1, $pg, $src2\\t# minI reduction predicated (sve)\" %}\n+  ins_encode %{\n+    BasicType bt = Matcher::vector_element_basic_type(this, $src2);\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, bt,\n+                           $src1$$Register, as_FloatRegister($src2$$reg),\n+                           as_PRegister($pg$$reg), as_FloatRegister($tmp$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct reduce_minL_masked(iRegLNoSp dst, iRegL src1, vReg src2, vRegD tmp,\n+                          pRegGov pg, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->element_basic_type() == T_LONG);\n+  match(Set dst (MinReductionV (Binary src1 src2) pg));\n+  effect(TEMP_DEF dst, TEMP tmp, KILL cr);\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_reduce_minL $dst, $src1, $pg, $src2\\t# minL reduction predicated (sve)\" %}\n+  ins_encode %{\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, T_LONG,\n+                           $src1$$Register, as_FloatRegister($src2$$reg),\n+                           as_PRegister($pg$$reg), as_FloatRegister($tmp$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct reduce_minI_masked_partial(iRegINoSp dst, iRegIorL2I src1, vReg src2, vRegD vtmp,\n+                                  pRegGov pg, pRegGov ptmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->element_basic_type() != T_LONG &&\n+            is_integral_type(n->in(1)->in(2)->bottom_type()->is_vect()->element_basic_type()));\n+  match(Set dst (MinReductionV (Binary src1 src2) pg));\n+  effect(TEMP_DEF dst, TEMP vtmp, TEMP ptmp, KILL cr);\n+  ins_cost(3 * SVE_COST);\n+  format %{ \"sve_reduce_minI $dst, $src1, $pg, $src2\\t# minI reduction predicated partial (sve)\" %}\n+  ins_encode %{\n+    BasicType bt = Matcher::vector_element_basic_type(this, $src2);\n+    Assembler::SIMD_RegVariant variant = __ elemType_to_regVariant(bt);\n+    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), variant,\n+                          Matcher::vector_length(this, $src2));\n+    __ sve_and(as_PRegister($ptmp$$reg), as_PRegister($ptmp$$reg),\n+               as_PRegister($pg$$reg), as_PRegister($pg$$reg));\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, bt,\n+                           $src1$$Register, as_FloatRegister($src2$$reg),\n+                           as_PRegister($ptmp$$reg), as_FloatRegister($vtmp$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct reduce_minL_masked_partial(iRegLNoSp dst, iRegL src1, vReg src2, vRegD vtmp,\n+                                  pRegGov pg, pRegGov ptmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->element_basic_type() == T_LONG);\n+  match(Set dst (MinReductionV (Binary src1 src2) pg));\n+  effect(TEMP_DEF dst, TEMP vtmp, TEMP ptmp, KILL cr);\n+  ins_cost(3 * SVE_COST);\n+  format %{ \"sve_reduce_minL $dst, $src1, $pg, $src2\\t# minL reduction predicated partial (sve)\" %}\n+  ins_encode %{\n+    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), __ D,\n+                          Matcher::vector_length(this, $src2));\n+    __ sve_and(as_PRegister($ptmp$$reg), as_PRegister($ptmp$$reg),\n+               as_PRegister($pg$$reg), as_PRegister($pg$$reg));\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, T_LONG,\n+                           $src1$$Register, as_FloatRegister($src2$$reg),\n+                           as_PRegister($ptmp$$reg), as_FloatRegister($vtmp$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct reduce_minF_masked(vRegF dst, vRegF src1, vReg src2, pRegGov pg) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->element_basic_type() == T_FLOAT &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize);\n+  match(Set dst (MinReductionV (Binary src1 src2) pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_reduce_minF $dst, $src1, $pg, $src2\\t# minF reduction predicated (sve)\" %}\n+  ins_encode %{\n+    __ sve_fminv(as_FloatRegister($dst$$reg), __ S, as_PRegister($pg$$reg), as_FloatRegister($src2$$reg));\n+    __ fmins(as_FloatRegister($dst$$reg), as_FloatRegister($dst$$reg), as_FloatRegister($src1$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct reduce_minD_masked(vRegD dst, vRegD src1, vReg src2, pRegGov pg) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->element_basic_type() == T_DOUBLE &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize);\n+  match(Set dst (MinReductionV (Binary src1 src2) pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_reduce_minD $dst, $src1, $pg, $src2\\t# minD reduction predicated (sve)\" %}\n+  ins_encode %{\n+    __ sve_fminv(as_FloatRegister($dst$$reg), __ D, as_PRegister($pg$$reg), as_FloatRegister($src2$$reg));\n@@ -2027,6 +3405,27 @@\n-instruct reduce_minD_partial(vRegD dst, vRegD src1, vReg src2,\n-                             pRegGov ptmp, rFlagsReg cr) %{\n-  predicate(UseSVE > 0 && n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_DOUBLE &&\n-            n->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize);\n-  match(Set dst (MinReductionV src1 src2));\n-  ins_cost(INSN_COST);\n+instruct reduce_minF_masked_partial(vRegF dst, vRegF src1, vReg src2, pRegGov pg,\n+                                    pRegGov ptmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->element_basic_type() == T_FLOAT &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize);\n+  match(Set dst (MinReductionV (Binary src1 src2) pg));\n+  effect(TEMP_DEF dst, TEMP ptmp, KILL cr);\n+  ins_cost(3 * SVE_COST);\n+  format %{ \"sve_reduce_minF $dst, $src1, $pg, $src2\\t# minF reduction predicated partial (sve)\" %}\n+  ins_encode %{\n+    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), __ S,\n+                          Matcher::vector_length(this, $src2));\n+    __ sve_and(as_PRegister($ptmp$$reg), as_PRegister($ptmp$$reg),\n+               as_PRegister($pg$$reg), as_PRegister($pg$$reg));\n+    __ sve_fminv(as_FloatRegister($dst$$reg), __ S,\n+               as_PRegister($ptmp$$reg), as_FloatRegister($src2$$reg));\n+    __ fmins(as_FloatRegister($dst$$reg), as_FloatRegister($dst$$reg), as_FloatRegister($src1$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct reduce_minD_masked_partial(vRegD dst, vRegD src1, vReg src2, pRegGov pg,\n+                                    pRegGov ptmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->element_basic_type() == T_DOUBLE &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize);\n+  match(Set dst (MinReductionV (Binary src1 src2) pg));\n@@ -2034,1 +3433,2 @@\n-  format %{ \"sve_reduce_minD $dst, $src1, $src2\\t# reduce min D partial (sve)\" %}\n+  ins_cost(3 * SVE_COST);\n+  format %{ \"sve_reduce_minD $dst, $src1, $pg, $src2\\t# minD reduction predicated partial (sve)\" %}\n@@ -2038,0 +3438,2 @@\n+    __ sve_and(as_PRegister($ptmp$$reg), as_PRegister($ptmp$$reg),\n+               as_PRegister($pg$$reg), as_PRegister($pg$$reg));\n@@ -2039,1 +3441,1 @@\n-         as_PRegister($ptmp$$reg), as_FloatRegister($src2$$reg));\n+               as_PRegister($ptmp$$reg), as_FloatRegister($src2$$reg));\n@@ -2451,1 +3853,332 @@\n-instruct vlsrI_imm(vReg dst, vReg src, immI shift) %{\n+instruct vlsrI_imm(vReg dst, vReg src, immI shift) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst (URShiftVI src (RShiftCntV shift)));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_lsr $dst, $src, $shift\\t# vector (sve) (S)\" %}\n+  ins_encode %{\n+    int con = (int)$shift$$constant;\n+    if (con == 0) {\n+      __ sve_orr(as_FloatRegister($dst$$reg), as_FloatRegister($src$$reg),\n+           as_FloatRegister($src$$reg));\n+      return;\n+    }\n+    __ sve_lsr(as_FloatRegister($dst$$reg), __ S,\n+         as_FloatRegister($src$$reg), con);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vlsrL_imm(vReg dst, vReg src, immI shift) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst (URShiftVL src (RShiftCntV shift)));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_lsr $dst, $src, $shift\\t# vector (sve) (D)\" %}\n+  ins_encode %{\n+    int con = (int)$shift$$constant;\n+    if (con == 0) {\n+      __ sve_orr(as_FloatRegister($dst$$reg), as_FloatRegister($src$$reg),\n+           as_FloatRegister($src$$reg));\n+      return;\n+    }\n+    __ sve_lsr(as_FloatRegister($dst$$reg), __ D,\n+         as_FloatRegister($src$$reg), con);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vlslB_imm(vReg dst, vReg src, immI shift) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst (LShiftVB src (LShiftCntV shift)));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_lsl $dst, $src, $shift\\t# vector (sve) (B)\" %}\n+  ins_encode %{\n+    int con = (int)$shift$$constant;\n+    if (con >= 8) {\n+      __ sve_eor(as_FloatRegister($dst$$reg), as_FloatRegister($src$$reg),\n+           as_FloatRegister($src$$reg));\n+      return;\n+    }\n+    __ sve_lsl(as_FloatRegister($dst$$reg), __ B,\n+         as_FloatRegister($src$$reg), con);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vlslS_imm(vReg dst, vReg src, immI shift) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst (LShiftVS src (LShiftCntV shift)));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_lsl $dst, $src, $shift\\t# vector (sve) (H)\" %}\n+  ins_encode %{\n+    int con = (int)$shift$$constant;\n+    if (con >= 16) {\n+      __ sve_eor(as_FloatRegister($dst$$reg), as_FloatRegister($src$$reg),\n+           as_FloatRegister($src$$reg));\n+      return;\n+    }\n+    __ sve_lsl(as_FloatRegister($dst$$reg), __ H,\n+         as_FloatRegister($src$$reg), con);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vlslI_imm(vReg dst, vReg src, immI shift) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst (LShiftVI src (LShiftCntV shift)));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_lsl $dst, $src, $shift\\t# vector (sve) (S)\" %}\n+  ins_encode %{\n+    int con = (int)$shift$$constant;\n+    __ sve_lsl(as_FloatRegister($dst$$reg), __ S,\n+         as_FloatRegister($src$$reg), con);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vlslL_imm(vReg dst, vReg src, immI shift) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst (LShiftVL src (LShiftCntV shift)));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_lsl $dst, $src, $shift\\t# vector (sve) (D)\" %}\n+  ins_encode %{\n+    int con = (int)$shift$$constant;\n+    __ sve_lsl(as_FloatRegister($dst$$reg), __ D,\n+         as_FloatRegister($src$$reg), con);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vshiftcntB(vReg dst, iRegIorL2I cnt) %{\n+  predicate(UseSVE > 0 &&\n+            (n->bottom_type()->is_vect()->element_basic_type() == T_BYTE));\n+  match(Set dst (LShiftCntV cnt));\n+  match(Set dst (RShiftCntV cnt));\n+  format %{ \"sve_dup $dst, $cnt\\t# vector shift count (sve) (B)\" %}\n+  ins_encode %{\n+    __ sve_dup(as_FloatRegister($dst$$reg), __ B, as_Register($cnt$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vshiftcntS(vReg dst, iRegIorL2I cnt) %{\n+  predicate(UseSVE > 0 &&\n+            (n->bottom_type()->is_vect()->element_basic_type() == T_SHORT ||\n+            (n->bottom_type()->is_vect()->element_basic_type() == T_CHAR)));\n+  match(Set dst (LShiftCntV cnt));\n+  match(Set dst (RShiftCntV cnt));\n+  format %{ \"sve_dup $dst, $cnt\\t# vector shift count (sve) (H)\" %}\n+  ins_encode %{\n+    __ sve_dup(as_FloatRegister($dst$$reg), __ H, as_Register($cnt$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vshiftcntI(vReg dst, iRegIorL2I cnt) %{\n+  predicate(UseSVE > 0 &&\n+            (n->bottom_type()->is_vect()->element_basic_type() == T_INT));\n+  match(Set dst (LShiftCntV cnt));\n+  match(Set dst (RShiftCntV cnt));\n+  format %{ \"sve_dup $dst, $cnt\\t# vector shift count (sve) (S)\" %}\n+  ins_encode %{\n+    __ sve_dup(as_FloatRegister($dst$$reg), __ S, as_Register($cnt$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vshiftcntL(vReg dst, iRegIorL2I cnt) %{\n+  predicate(UseSVE > 0 &&\n+            (n->bottom_type()->is_vect()->element_basic_type() == T_LONG));\n+  match(Set dst (LShiftCntV cnt));\n+  match(Set dst (RShiftCntV cnt));\n+  format %{ \"sve_dup $dst, $cnt\\t# vector shift count (sve) (D)\" %}\n+  ins_encode %{\n+    __ sve_dup(as_FloatRegister($dst$$reg), __ D, as_Register($cnt$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ vector shift - predicated\n+\n+instruct vasrB_masked(vReg dst_src1, vReg src2, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src1 (RShiftVB (Binary dst_src1 src2) pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_asr $dst_src1, $pg, $dst_src1, $src2\\t# vector (sve) (B)\" %}\n+  ins_encode %{\n+    __ sve_asr(as_FloatRegister($dst_src1$$reg), __ B,\n+            as_PRegister($pg$$reg),\n+            as_FloatRegister($src2$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vasrS_masked(vReg dst_src1, vReg src2, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src1 (RShiftVS (Binary dst_src1 src2) pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_asr $dst_src1, $pg, $dst_src1, $src2\\t# vector (sve) (H)\" %}\n+  ins_encode %{\n+    __ sve_asr(as_FloatRegister($dst_src1$$reg), __ H,\n+            as_PRegister($pg$$reg),\n+            as_FloatRegister($src2$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vasrI_masked(vReg dst_src1, vReg src2, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src1 (RShiftVI (Binary dst_src1 src2) pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_asr $dst_src1, $pg, $dst_src1, $src2\\t# vector (sve) (S)\" %}\n+  ins_encode %{\n+    __ sve_asr(as_FloatRegister($dst_src1$$reg), __ S,\n+            as_PRegister($pg$$reg),\n+            as_FloatRegister($src2$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vasrL_masked(vReg dst_src1, vReg src2, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src1 (RShiftVL (Binary dst_src1 src2) pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_asr $dst_src1, $pg, $dst_src1, $src2\\t# vector (sve) (D)\" %}\n+  ins_encode %{\n+    __ sve_asr(as_FloatRegister($dst_src1$$reg), __ D,\n+            as_PRegister($pg$$reg),\n+            as_FloatRegister($src2$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vlslB_masked(vReg dst_src1, vReg src2, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src1 (LShiftVB (Binary dst_src1 src2) pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_lsl $dst_src1, $pg, $dst_src1, $src2\\t# vector (sve) (B)\" %}\n+  ins_encode %{\n+    __ sve_lsl(as_FloatRegister($dst_src1$$reg), __ B,\n+            as_PRegister($pg$$reg),\n+            as_FloatRegister($src2$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vlslS_masked(vReg dst_src1, vReg src2, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src1 (LShiftVS (Binary dst_src1 src2) pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_lsl $dst_src1, $pg, $dst_src1, $src2\\t# vector (sve) (H)\" %}\n+  ins_encode %{\n+    __ sve_lsl(as_FloatRegister($dst_src1$$reg), __ H,\n+            as_PRegister($pg$$reg),\n+            as_FloatRegister($src2$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vlslI_masked(vReg dst_src1, vReg src2, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src1 (LShiftVI (Binary dst_src1 src2) pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_lsl $dst_src1, $pg, $dst_src1, $src2\\t# vector (sve) (S)\" %}\n+  ins_encode %{\n+    __ sve_lsl(as_FloatRegister($dst_src1$$reg), __ S,\n+            as_PRegister($pg$$reg),\n+            as_FloatRegister($src2$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vlslL_masked(vReg dst_src1, vReg src2, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src1 (LShiftVL (Binary dst_src1 src2) pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_lsl $dst_src1, $pg, $dst_src1, $src2\\t# vector (sve) (D)\" %}\n+  ins_encode %{\n+    __ sve_lsl(as_FloatRegister($dst_src1$$reg), __ D,\n+            as_PRegister($pg$$reg),\n+            as_FloatRegister($src2$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vlsrB_masked(vReg dst_src1, vReg src2, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src1 (URShiftVB (Binary dst_src1 src2) pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_lsr $dst_src1, $pg, $dst_src1, $src2\\t# vector (sve) (B)\" %}\n+  ins_encode %{\n+    __ sve_lsr(as_FloatRegister($dst_src1$$reg), __ B,\n+            as_PRegister($pg$$reg),\n+            as_FloatRegister($src2$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vlsrS_masked(vReg dst_src1, vReg src2, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src1 (URShiftVS (Binary dst_src1 src2) pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_lsr $dst_src1, $pg, $dst_src1, $src2\\t# vector (sve) (H)\" %}\n+  ins_encode %{\n+    __ sve_lsr(as_FloatRegister($dst_src1$$reg), __ H,\n+            as_PRegister($pg$$reg),\n+            as_FloatRegister($src2$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vlsrI_masked(vReg dst_src1, vReg src2, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src1 (URShiftVI (Binary dst_src1 src2) pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_lsr $dst_src1, $pg, $dst_src1, $src2\\t# vector (sve) (S)\" %}\n+  ins_encode %{\n+    __ sve_lsr(as_FloatRegister($dst_src1$$reg), __ S,\n+            as_PRegister($pg$$reg),\n+            as_FloatRegister($src2$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vlsrL_masked(vReg dst_src1, vReg src2, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src1 (URShiftVL (Binary dst_src1 src2) pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_lsr $dst_src1, $pg, $dst_src1, $src2\\t# vector (sve) (D)\" %}\n+  ins_encode %{\n+    __ sve_lsr(as_FloatRegister($dst_src1$$reg), __ D,\n+            as_PRegister($pg$$reg),\n+            as_FloatRegister($src2$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vasrB_imm_masked(vReg dst_src, immI shift, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src (RShiftVB (Binary dst_src (RShiftCntV shift)) pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_asr $dst_src, $pg, $dst_src, $shift\\t# vector (sve) (B)\" %}\n+  ins_encode %{\n+    int con = (int)$shift$$constant;\n+    assert(con > 0 && con < 8, \"invalid shift immediate\");\n+    __ sve_asr(as_FloatRegister($dst_src$$reg), __ B, as_PRegister($pg$$reg), con);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vasrS_imm_masked(vReg dst_src, immI shift, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src (RShiftVS (Binary dst_src (RShiftCntV shift)) pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_asr $dst_src, $pg, $dst_src, $shift\\t# vector (sve) (H)\" %}\n+  ins_encode %{\n+    int con = (int)$shift$$constant;\n+    assert(con > 0 && con < 16, \"invalid shift immediate\");\n+    __ sve_asr(as_FloatRegister($dst_src$$reg), __ H, as_PRegister($pg$$reg), con);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vasrI_imm_masked(vReg dst_src, immI shift, pRegGov pg) %{\n@@ -2453,1 +4186,1 @@\n-  match(Set dst (URShiftVI src (RShiftCntV shift)));\n+  match(Set dst_src (RShiftVI (Binary dst_src (RShiftCntV shift)) pg));\n@@ -2455,1 +4188,1 @@\n-  format %{ \"sve_lsr $dst, $src, $shift\\t# vector (sve) (S)\" %}\n+  format %{ \"sve_asr $dst_src, $pg, $dst_src, $shift\\t# vector (sve) (S)\" %}\n@@ -2458,7 +4191,2 @@\n-    if (con == 0) {\n-      __ sve_orr(as_FloatRegister($dst$$reg), as_FloatRegister($src$$reg),\n-           as_FloatRegister($src$$reg));\n-      return;\n-    }\n-    __ sve_lsr(as_FloatRegister($dst$$reg), __ S,\n-         as_FloatRegister($src$$reg), con);\n+    assert(con > 0 && con < 32, \"invalid shift immediate\");\n+    __ sve_asr(as_FloatRegister($dst_src$$reg), __ S, as_PRegister($pg$$reg), con);\n@@ -2469,1 +4197,1 @@\n-instruct vlsrL_imm(vReg dst, vReg src, immI shift) %{\n+instruct vasrL_imm_masked(vReg dst_src, immI shift, pRegGov pg) %{\n@@ -2471,1 +4199,1 @@\n-  match(Set dst (URShiftVL src (RShiftCntV shift)));\n+  match(Set dst_src (RShiftVL (Binary dst_src (RShiftCntV shift)) pg));\n@@ -2473,1 +4201,1 @@\n-  format %{ \"sve_lsr $dst, $src, $shift\\t# vector (sve) (D)\" %}\n+  format %{ \"sve_asr $dst_src, $pg, $dst_src, $shift\\t# vector (sve) (D)\" %}\n@@ -2476,7 +4204,2 @@\n-    if (con == 0) {\n-      __ sve_orr(as_FloatRegister($dst$$reg), as_FloatRegister($src$$reg),\n-           as_FloatRegister($src$$reg));\n-      return;\n-    }\n-    __ sve_lsr(as_FloatRegister($dst$$reg), __ D,\n-         as_FloatRegister($src$$reg), con);\n+    assert(con > 0 && con < 64, \"invalid shift immediate\");\n+    __ sve_asr(as_FloatRegister($dst_src$$reg), __ D, as_PRegister($pg$$reg), con);\n@@ -2487,1 +4210,1 @@\n-instruct vlslB_imm(vReg dst, vReg src, immI shift) %{\n+instruct vlsrB_imm_masked(vReg dst_src, immI shift, pRegGov pg) %{\n@@ -2489,1 +4212,1 @@\n-  match(Set dst (LShiftVB src (LShiftCntV shift)));\n+  match(Set dst_src (URShiftVB (Binary dst_src (RShiftCntV shift)) pg));\n@@ -2491,1 +4214,1 @@\n-  format %{ \"sve_lsl $dst, $src, $shift\\t# vector (sve) (B)\" %}\n+  format %{ \"sve_lsr $dst_src, $pg, $dst_src, $shift\\t# vector (sve) (B)\" %}\n@@ -2494,7 +4217,2 @@\n-    if (con >= 8) {\n-      __ sve_eor(as_FloatRegister($dst$$reg), as_FloatRegister($src$$reg),\n-           as_FloatRegister($src$$reg));\n-      return;\n-    }\n-    __ sve_lsl(as_FloatRegister($dst$$reg), __ B,\n-         as_FloatRegister($src$$reg), con);\n+    assert(con > 0 && con < 8, \"invalid shift immediate\");\n+    __ sve_lsr(as_FloatRegister($dst_src$$reg), __ B, as_PRegister($pg$$reg), con);\n@@ -2505,1 +4223,1 @@\n-instruct vlslS_imm(vReg dst, vReg src, immI shift) %{\n+instruct vlsrS_imm_masked(vReg dst_src, immI shift, pRegGov pg) %{\n@@ -2507,1 +4225,1 @@\n-  match(Set dst (LShiftVS src (LShiftCntV shift)));\n+  match(Set dst_src (URShiftVS (Binary dst_src (RShiftCntV shift)) pg));\n@@ -2509,1 +4227,1 @@\n-  format %{ \"sve_lsl $dst, $src, $shift\\t# vector (sve) (H)\" %}\n+  format %{ \"sve_lsr $dst_src, $pg, $dst_src, $shift\\t# vector (sve) (H)\" %}\n@@ -2512,7 +4230,2 @@\n-    if (con >= 16) {\n-      __ sve_eor(as_FloatRegister($dst$$reg), as_FloatRegister($src$$reg),\n-           as_FloatRegister($src$$reg));\n-      return;\n-    }\n-    __ sve_lsl(as_FloatRegister($dst$$reg), __ H,\n-         as_FloatRegister($src$$reg), con);\n+    assert(con > 0 && con < 16, \"invalid shift immediate\");\n+    __ sve_lsr(as_FloatRegister($dst_src$$reg), __ H, as_PRegister($pg$$reg), con);\n@@ -2523,1 +4236,1 @@\n-instruct vlslI_imm(vReg dst, vReg src, immI shift) %{\n+instruct vlsrI_imm_masked(vReg dst_src, immI shift, pRegGov pg) %{\n@@ -2525,1 +4238,1 @@\n-  match(Set dst (LShiftVI src (LShiftCntV shift)));\n+  match(Set dst_src (URShiftVI (Binary dst_src (RShiftCntV shift)) pg));\n@@ -2527,1 +4240,1 @@\n-  format %{ \"sve_lsl $dst, $src, $shift\\t# vector (sve) (S)\" %}\n+  format %{ \"sve_lsr $dst_src, $pg, $dst_src, $shift\\t# vector (sve) (S)\" %}\n@@ -2530,2 +4243,2 @@\n-    __ sve_lsl(as_FloatRegister($dst$$reg), __ S,\n-         as_FloatRegister($src$$reg), con);\n+    assert(con > 0 && con < 32, \"invalid shift immediate\");\n+    __ sve_lsr(as_FloatRegister($dst_src$$reg), __ S, as_PRegister($pg$$reg), con);\n@@ -2536,1 +4249,1 @@\n-instruct vlslL_imm(vReg dst, vReg src, immI shift) %{\n+instruct vlsrL_imm_masked(vReg dst_src, immI shift, pRegGov pg) %{\n@@ -2538,1 +4251,1 @@\n-  match(Set dst (LShiftVL src (LShiftCntV shift)));\n+  match(Set dst_src (URShiftVL (Binary dst_src (RShiftCntV shift)) pg));\n@@ -2540,1 +4253,1 @@\n-  format %{ \"sve_lsl $dst, $src, $shift\\t# vector (sve) (D)\" %}\n+  format %{ \"sve_lsr $dst_src, $pg, $dst_src, $shift\\t# vector (sve) (D)\" %}\n@@ -2543,2 +4256,2 @@\n-    __ sve_lsl(as_FloatRegister($dst$$reg), __ D,\n-         as_FloatRegister($src$$reg), con);\n+    assert(con > 0 && con < 64, \"invalid shift immediate\");\n+    __ sve_lsr(as_FloatRegister($dst_src$$reg), __ D, as_PRegister($pg$$reg), con);\n@@ -2549,6 +4262,5 @@\n-instruct vshiftcntB(vReg dst, iRegIorL2I cnt) %{\n-  predicate(UseSVE > 0 &&\n-            (n->bottom_type()->is_vect()->element_basic_type() == T_BYTE));\n-  match(Set dst (LShiftCntV cnt));\n-  match(Set dst (RShiftCntV cnt));\n-  format %{ \"sve_dup $dst, $cnt\\t# vector shift count (sve) (B)\" %}\n+instruct vlslB_imm_masked(vReg dst_src, immI shift, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src (LShiftVB (Binary dst_src (LShiftCntV shift)) pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_lsl $dst_src, $pg, $dst_src, $shift\\t# vector (sve) (B)\" %}\n@@ -2556,1 +4268,3 @@\n-    __ sve_dup(as_FloatRegister($dst$$reg), __ B, as_Register($cnt$$reg));\n+    int con = (int)$shift$$constant;\n+    assert(con >= 0 && con < 8, \"invalid shift immediate\");\n+    __ sve_lsl(as_FloatRegister($dst_src$$reg), __ B, as_PRegister($pg$$reg), con);\n@@ -2561,7 +4275,5 @@\n-instruct vshiftcntS(vReg dst, iRegIorL2I cnt) %{\n-  predicate(UseSVE > 0 &&\n-            (n->bottom_type()->is_vect()->element_basic_type() == T_SHORT ||\n-            (n->bottom_type()->is_vect()->element_basic_type() == T_CHAR)));\n-  match(Set dst (LShiftCntV cnt));\n-  match(Set dst (RShiftCntV cnt));\n-  format %{ \"sve_dup $dst, $cnt\\t# vector shift count (sve) (H)\" %}\n+instruct vlslS_imm_masked(vReg dst_src, immI shift, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src (LShiftVS (Binary dst_src (LShiftCntV shift)) pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_lsl $dst_src, $pg, $dst_src, $shift\\t# vector (sve) (H)\" %}\n@@ -2569,1 +4281,3 @@\n-    __ sve_dup(as_FloatRegister($dst$$reg), __ H, as_Register($cnt$$reg));\n+    int con = (int)$shift$$constant;\n+    assert(con >= 0 && con < 16, \"invalid shift immediate\");\n+    __ sve_lsl(as_FloatRegister($dst_src$$reg), __ H, as_PRegister($pg$$reg), con);\n@@ -2574,6 +4288,5 @@\n-instruct vshiftcntI(vReg dst, iRegIorL2I cnt) %{\n-  predicate(UseSVE > 0 &&\n-            (n->bottom_type()->is_vect()->element_basic_type() == T_INT));\n-  match(Set dst (LShiftCntV cnt));\n-  match(Set dst (RShiftCntV cnt));\n-  format %{ \"sve_dup $dst, $cnt\\t# vector shift count (sve) (S)\" %}\n+instruct vlslI_imm_masked(vReg dst_src, immI shift, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src (LShiftVI (Binary dst_src (LShiftCntV shift)) pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_lsl $dst_src, $pg, $dst_src, $shift\\t# vector (sve) (S)\" %}\n@@ -2581,1 +4294,3 @@\n-    __ sve_dup(as_FloatRegister($dst$$reg), __ S, as_Register($cnt$$reg));\n+    int con = (int)$shift$$constant;\n+    assert(con >= 0 && con < 32, \"invalid shift immediate\");\n+    __ sve_lsl(as_FloatRegister($dst_src$$reg), __ S, as_PRegister($pg$$reg), con);\n@@ -2586,6 +4301,5 @@\n-instruct vshiftcntL(vReg dst, iRegIorL2I cnt) %{\n-  predicate(UseSVE > 0 &&\n-            (n->bottom_type()->is_vect()->element_basic_type() == T_LONG));\n-  match(Set dst (LShiftCntV cnt));\n-  match(Set dst (RShiftCntV cnt));\n-  format %{ \"sve_dup $dst, $cnt\\t# vector shift count (sve) (D)\" %}\n+instruct vlslL_imm_masked(vReg dst_src, immI shift, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src (LShiftVL (Binary dst_src (LShiftCntV shift)) pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_lsl $dst_src, $pg, $dst_src, $shift\\t# vector (sve) (D)\" %}\n@@ -2593,1 +4307,3 @@\n-    __ sve_dup(as_FloatRegister($dst$$reg), __ D, as_Register($cnt$$reg));\n+    int con = (int)$shift$$constant;\n+    assert(con >= 0 && con < 64, \"invalid shift immediate\");\n+    __ sve_lsl(as_FloatRegister($dst_src$$reg), __ D, as_PRegister($pg$$reg), con);\n@@ -2601,1 +4317,2 @@\n-  predicate(UseSVE > 0);\n+  predicate(UseSVE > 0 &&\n+            !n->as_Vector()->is_predicated_vector());\n@@ -2613,1 +4330,2 @@\n-  predicate(UseSVE > 0);\n+  predicate(UseSVE > 0 &&\n+            !n->as_Vector()->is_predicated_vector());\n@@ -2624,0 +4342,28 @@\n+\/\/ vector sqrt - predicated\n+\n+instruct vsqrtF_masked(vReg dst_src, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src (SqrtVF dst_src pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_fsqrt $dst_src, $pg, $dst_src\\t# vector (sve) (S)\" %}\n+  ins_encode %{\n+    __ sve_fsqrt(as_FloatRegister($dst_src$$reg), __ S,\n+            as_PRegister($pg$$reg),\n+            as_FloatRegister($dst_src$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vsqrtD_masked(vReg dst_src, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src (SqrtVD dst_src pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_fsqrt $dst_src, $pg, $dst_src\\t# vector (sve) (D)\" %}\n+  ins_encode %{\n+    __ sve_fsqrt(as_FloatRegister($dst_src$$reg), __ D,\n+            as_PRegister($pg$$reg),\n+            as_FloatRegister($dst_src$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n@@ -2704,1 +4450,1 @@\n-\/\/ vector mask cast\n+\/\/ vector sub - predicated\n@@ -2706,6 +4452,5 @@\n-instruct vmaskcast(vReg dst) %{\n-  predicate(UseSVE > 0 && n->bottom_type()->is_vect()->length() == n->in(1)->bottom_type()->is_vect()->length() &&\n-            n->bottom_type()->is_vect()->length_in_bytes() == n->in(1)->bottom_type()->is_vect()->length_in_bytes());\n-  match(Set dst (VectorMaskCast dst));\n-  ins_cost(0);\n-  format %{ \"vmaskcast $dst\\t# empty (sve)\" %}\n+instruct vsubB_masked(vReg dst_src1, vReg src2, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src1 (SubVB (Binary dst_src1 src2) pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_sub $dst_src1, $pg, $dst_src1, $src2\\t# vector (sve) (B)\" %}\n@@ -2713,1 +4458,3 @@\n-    \/\/ empty\n+    __ sve_sub(as_FloatRegister($dst_src1$$reg), __ B,\n+            as_PRegister($pg$$reg),\n+            as_FloatRegister($src2$$reg));\n@@ -2715,1 +4462,1 @@\n-  ins_pipe(pipe_class_empty);\n+  ins_pipe(pipe_slow);\n@@ -2718,1 +4465,12 @@\n-\/\/ ------------------------------ Vector cast -------------------------------\n+instruct vsubS_masked(vReg dst_src1, vReg src2, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src1 (SubVS (Binary dst_src1 src2) pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_sub $dst_src1, $pg, $dst_src1, $src2\\t# vector (sve) (H)\" %}\n+  ins_encode %{\n+    __ sve_sub(as_FloatRegister($dst_src1$$reg), __ H,\n+            as_PRegister($pg$$reg),\n+            as_FloatRegister($src2$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n@@ -2720,5 +4478,3 @@\n-instruct vcvtBtoS(vReg dst, vReg src)\n-%{\n-  predicate(UseSVE > 0 &&\n-            n->bottom_type()->is_vect()->element_basic_type() == T_SHORT);\n-  match(Set dst (VectorCastB2X src));\n+instruct vsubI_masked(vReg dst_src1, vReg src2, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src1 (SubVI (Binary dst_src1 src2) pg));\n@@ -2726,1 +4482,1 @@\n-  format %{ \"sve_sunpklo  $dst, H, $src\\t# convert B to S vector\" %}\n+  format %{ \"sve_sub $dst_src1, $pg, $dst_src1, $src2\\t# vector (sve) (S)\" %}\n@@ -2728,1 +4484,3 @@\n-    __ sve_sunpklo(as_FloatRegister($dst$$reg), __ H, as_FloatRegister($src$$reg));\n+    __ sve_sub(as_FloatRegister($dst_src1$$reg), __ S,\n+            as_PRegister($pg$$reg),\n+            as_FloatRegister($src2$$reg));\n@@ -2733,8 +4491,5 @@\n-instruct vcvtBtoI(vReg dst, vReg src)\n-%{\n-  predicate(UseSVE > 0 &&\n-            n->bottom_type()->is_vect()->element_basic_type() == T_INT);\n-  match(Set dst (VectorCastB2X src));\n-  ins_cost(2 * SVE_COST);\n-  format %{ \"sve_sunpklo  $dst, H, $src\\n\\t\"\n-            \"sve_sunpklo  $dst, S, $dst\\t# convert B to I vector\" %}\n+instruct vsubL_masked(vReg dst_src1, vReg src2, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src1 (SubVL (Binary dst_src1 src2) pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_sub $dst_src1, $pg, $dst_src1, $src2\\t# vector (sve) (D)\" %}\n@@ -2742,2 +4497,3 @@\n-    __ sve_sunpklo(as_FloatRegister($dst$$reg), __ H, as_FloatRegister($src$$reg));\n-    __ sve_sunpklo(as_FloatRegister($dst$$reg), __ S, as_FloatRegister($dst$$reg));\n+    __ sve_sub(as_FloatRegister($dst_src1$$reg), __ D,\n+            as_PRegister($pg$$reg),\n+            as_FloatRegister($src2$$reg));\n@@ -2748,9 +4504,5 @@\n-instruct vcvtBtoL(vReg dst, vReg src)\n-%{\n-  predicate(UseSVE > 0 &&\n-            n->bottom_type()->is_vect()->element_basic_type() == T_LONG);\n-  match(Set dst (VectorCastB2X src));\n-  ins_cost(3 * SVE_COST);\n-  format %{ \"sve_sunpklo  $dst, H, $src\\n\\t\"\n-            \"sve_sunpklo  $dst, S, $dst\\n\\t\"\n-            \"sve_sunpklo  $dst, D, $dst\\t# convert B to L vector\" %}\n+instruct vsubF_masked(vReg dst_src1, vReg src2, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src1 (SubVF (Binary dst_src1 src2) pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_fsub $dst_src1, $pg, $dst_src1, $src2\\t# vector (sve) (S)\" %}\n@@ -2758,3 +4510,3 @@\n-    __ sve_sunpklo(as_FloatRegister($dst$$reg), __ H, as_FloatRegister($src$$reg));\n-    __ sve_sunpklo(as_FloatRegister($dst$$reg), __ S, as_FloatRegister($dst$$reg));\n-    __ sve_sunpklo(as_FloatRegister($dst$$reg), __ D, as_FloatRegister($dst$$reg));\n+    __ sve_fsub(as_FloatRegister($dst_src1$$reg), __ S,\n+            as_PRegister($pg$$reg),\n+            as_FloatRegister($src2$$reg));\n@@ -2765,9 +4517,5 @@\n-instruct vcvtBtoF(vReg dst, vReg src)\n-%{\n-  predicate(UseSVE > 0 &&\n-            n->bottom_type()->is_vect()->element_basic_type() == T_FLOAT);\n-  match(Set dst (VectorCastB2X src));\n-  ins_cost(3 * SVE_COST);\n-  format %{ \"sve_sunpklo  $dst, H, $src\\n\\t\"\n-            \"sve_sunpklo  $dst, S, $dst\\n\\t\"\n-            \"sve_scvtf  $dst, S, $dst, S\\t# convert B to F vector\" %}\n+instruct vsubD_masked(vReg dst_src1, vReg src2, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src1 (SubVD (Binary dst_src1 src2) pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_fsub $dst_src1, $pg, $dst_src1, $src2\\t# vector (sve) (D)\" %}\n@@ -2775,3 +4523,3 @@\n-    __ sve_sunpklo(as_FloatRegister($dst$$reg), __ H, as_FloatRegister($src$$reg));\n-    __ sve_sunpklo(as_FloatRegister($dst$$reg), __ S, as_FloatRegister($dst$$reg));\n-    __ sve_scvtf(as_FloatRegister($dst$$reg), __ S, ptrue, as_FloatRegister($dst$$reg), __ S);\n+    __ sve_fsub(as_FloatRegister($dst_src1$$reg), __ D,\n+            as_PRegister($pg$$reg),\n+            as_FloatRegister($src2$$reg));\n@@ -2782,2 +4530,3 @@\n-instruct vcvtBtoD(vReg dst, vReg src)\n-%{\n+\/\/ ------------------------------ Vector mask cast --------------------------\n+\n+instruct vmaskcast(pRegGov dst_src) %{\n@@ -2785,12 +4534,7 @@\n-            n->bottom_type()->is_vect()->element_basic_type() == T_DOUBLE);\n-  match(Set dst (VectorCastB2X src));\n-  ins_cost(4 * SVE_COST);\n-  format %{ \"sve_sunpklo  $dst, H, $src\\n\\t\"\n-            \"sve_sunpklo  $dst, S, $dst\\n\\t\"\n-            \"sve_sunpklo  $dst, D, $dst\\n\\t\"\n-            \"sve_scvtf  $dst, D, $dst, D\\t# convert B to D vector\" %}\n-  ins_encode %{\n-    __ sve_sunpklo(as_FloatRegister($dst$$reg), __ H, as_FloatRegister($src$$reg));\n-    __ sve_sunpklo(as_FloatRegister($dst$$reg), __ S, as_FloatRegister($dst$$reg));\n-    __ sve_sunpklo(as_FloatRegister($dst$$reg), __ D, as_FloatRegister($dst$$reg));\n-    __ sve_scvtf(as_FloatRegister($dst$$reg), __ D, ptrue, as_FloatRegister($dst$$reg), __ D);\n+            n->bottom_type()->is_vect()->length() == n->in(1)->bottom_type()->is_vect()->length() &&\n+            n->bottom_type()->is_vect()->length_in_bytes() == n->in(1)->bottom_type()->is_vect()->length_in_bytes());\n+  match(Set dst_src (VectorMaskCast dst_src));\n+  ins_cost(0);\n+  format %{ \"vmaskcast $dst_src\\t# empty (sve)\" %}\n+  ins_encode %{\n+    \/\/ empty\n@@ -2798,1 +4542,1 @@\n-  ins_pipe(pipe_slow);\n+  ins_pipe(pipe_class_empty);\n@@ -2801,1 +4545,1 @@\n-instruct vcvtStoB(vReg dst, vReg src, vReg tmp)\n+instruct vmaskcast_extend(pRegGov dst, pReg src)\n@@ -2804,6 +4548,6 @@\n-            n->bottom_type()->is_vect()->element_basic_type() == T_BYTE);\n-  match(Set dst (VectorCastS2X src));\n-  effect(TEMP tmp);\n-  ins_cost(2 * SVE_COST);\n-  format %{ \"sve_dup  $tmp, B, 0\\n\\t\"\n-            \"sve_uzp1  $dst, B, $src, tmp\\t# convert S to B vector\" %}\n+            (Matcher::vector_length_in_bytes(n) == 2 * Matcher::vector_length_in_bytes(n->in(1)) ||\n+             Matcher::vector_length_in_bytes(n) == 4 * Matcher::vector_length_in_bytes(n->in(1)) ||\n+             Matcher::vector_length_in_bytes(n) == 8 * Matcher::vector_length_in_bytes(n->in(1))));\n+  match(Set dst (VectorMaskCast src));\n+  ins_cost(SVE_COST * 3);\n+  format %{ \"sve_vmaskcast_extend  $dst, $src\\t# extend predicate $src\" %}\n@@ -2811,2 +4555,2 @@\n-    __ sve_dup(as_FloatRegister($tmp$$reg), __ B, 0);\n-    __ sve_uzp1(as_FloatRegister($dst$$reg), __ B, as_FloatRegister($src$$reg), as_FloatRegister($tmp$$reg));\n+    __ sve_vmaskcast_extend(as_PRegister($dst$$reg), as_PRegister($src$$reg),\n+                            Matcher::vector_length_in_bytes(this), Matcher::vector_length_in_bytes(this, $src));\n@@ -2817,1 +4561,1 @@\n-instruct vcvtStoI(vReg dst, vReg src)\n+instruct vmaskcast_narrow(pRegGov dst, pReg src)\n@@ -2820,4 +4564,6 @@\n-            n->bottom_type()->is_vect()->element_basic_type() == T_INT);\n-  match(Set dst (VectorCastS2X src));\n-  ins_cost(SVE_COST);\n-  format %{ \"sve_sunpklo  $dst, S, $src\\t# convert S to I vector\" %}\n+            (Matcher::vector_length_in_bytes(n) * 2 == Matcher::vector_length_in_bytes(n->in(1)) ||\n+             Matcher::vector_length_in_bytes(n) * 4 == Matcher::vector_length_in_bytes(n->in(1)) ||\n+             Matcher::vector_length_in_bytes(n) * 8 == Matcher::vector_length_in_bytes(n->in(1))));\n+  match(Set dst (VectorMaskCast src));\n+  ins_cost(SVE_COST * 3);\n+  format %{ \"sve_vmaskcast_narrow  $dst, $src\\t# narrow predicate $src\" %}\n@@ -2825,1 +4571,2 @@\n-    __ sve_sunpklo(as_FloatRegister($dst$$reg), __ S, as_FloatRegister($src$$reg));\n+    __ sve_vmaskcast_narrow(as_PRegister($dst$$reg), as_PRegister($src$$reg),\n+                            Matcher::vector_length_in_bytes(this), Matcher::vector_length_in_bytes(this, $src));\n@@ -2830,1 +4577,3 @@\n-instruct vcvtStoL(vReg dst, vReg src)\n+\/\/ ------------------------------ Vector cast -------------------------------\n+\n+instruct vcvtBtoX_extend(vReg dst, vReg src)\n@@ -2832,3 +4581,2 @@\n-  predicate(UseSVE > 0 &&\n-            n->bottom_type()->is_vect()->element_basic_type() == T_LONG);\n-  match(Set dst (VectorCastS2X src));\n+  predicate(UseSVE > 0);\n+  match(Set dst (VectorCastB2X src));\n@@ -2836,2 +4584,1 @@\n-  format %{ \"sve_sunpklo  $dst, S, $src\\n\\t\"\n-            \"sve_sunpklo  $dst, D, $dst\\t# convert S to L vector\" %}\n+  format %{ \"sve_vectorcast_b2x  $dst, $src\\t# convert B to X vector (extend)\" %}\n@@ -2839,2 +4586,6 @@\n-    __ sve_sunpklo(as_FloatRegister($dst$$reg), __ S, as_FloatRegister($src$$reg));\n-    __ sve_sunpklo(as_FloatRegister($dst$$reg), __ D, as_FloatRegister($dst$$reg));\n+    BasicType to_bt = Matcher::vector_element_basic_type(this);\n+    Assembler::SIMD_RegVariant to_size = __ elemType_to_regVariant(to_bt);\n+    __ sve_vector_extend(as_FloatRegister($dst$$reg), to_size, as_FloatRegister($src$$reg), __ B);\n+    if (to_bt == T_FLOAT || to_bt == T_DOUBLE) {\n+      __ sve_scvtf(as_FloatRegister($dst$$reg), to_size, ptrue, as_FloatRegister($dst$$reg), to_size);\n+    }\n@@ -2845,1 +4596,1 @@\n-instruct vcvtStoF(vReg dst, vReg src)\n+instruct vcvtStoB(vReg dst, vReg src, vReg tmp)\n@@ -2848,1 +4599,1 @@\n-            n->bottom_type()->is_vect()->element_basic_type() == T_FLOAT);\n+            n->bottom_type()->is_vect()->element_basic_type() == T_BYTE);\n@@ -2850,0 +4601,1 @@\n+  effect(TEMP tmp);\n@@ -2851,2 +4603,1 @@\n-  format %{ \"sve_sunpklo  $dst, S, $src\\n\\t\"\n-            \"sve_scvtf  $dst, S, $dst, S\\t# convert S to F vector\" %}\n+  format %{ \"sve_vectorcast_s2b  $dst, $src\\t# convert H to B vector\" %}\n@@ -2854,2 +4605,2 @@\n-    __ sve_sunpklo(as_FloatRegister($dst$$reg), __ S, as_FloatRegister($src$$reg));\n-    __ sve_scvtf(as_FloatRegister($dst$$reg), __ S, ptrue, as_FloatRegister($dst$$reg), __ S);\n+    __ sve_vector_narrow(as_FloatRegister($dst$$reg), __ B,\n+                         as_FloatRegister($src$$reg), __ H, as_FloatRegister($tmp$$reg));\n@@ -2860,1 +4611,1 @@\n-instruct vcvtStoD(vReg dst, vReg src)\n+instruct vcvtStoX_extend(vReg dst, vReg src)\n@@ -2863,1 +4614,1 @@\n-            n->bottom_type()->is_vect()->element_basic_type() == T_DOUBLE);\n+            type2aelembytes(Matcher::vector_element_basic_type(n)) > 2);\n@@ -2865,4 +4616,2 @@\n-  ins_cost(3 * SVE_COST);\n-  format %{ \"sve_sunpklo  $dst, S, $src\\n\\t\"\n-            \"sve_sunpklo  $dst, D, $dst\\n\\t\"\n-            \"sve_scvtf  $dst, D, $dst, D\\t# convert S to D vector\" %}\n+  ins_cost(2 * SVE_COST);\n+  format %{ \"sve_vectorcast_s2x  $dst, $src\\t# convert H to X vector (extend)\" %}\n@@ -2870,3 +4619,6 @@\n-    __ sve_sunpklo(as_FloatRegister($dst$$reg), __ S, as_FloatRegister($src$$reg));\n-    __ sve_sunpklo(as_FloatRegister($dst$$reg), __ D, as_FloatRegister($dst$$reg));\n-    __ sve_scvtf(as_FloatRegister($dst$$reg), __ D, ptrue, as_FloatRegister($dst$$reg), __ D);\n+    BasicType to_bt = Matcher::vector_element_basic_type(this);\n+    Assembler::SIMD_RegVariant to_size = __ elemType_to_regVariant(to_bt);\n+    __ sve_vector_extend(as_FloatRegister($dst$$reg), to_size, as_FloatRegister($src$$reg), __ H);\n+    if (to_bt == T_FLOAT || to_bt == T_DOUBLE) {\n+      __ sve_scvtf(as_FloatRegister($dst$$reg), to_size, ptrue, as_FloatRegister($dst$$reg), to_size);\n+    }\n@@ -2884,3 +4636,1 @@\n-  format %{ \"sve_dup  $tmp, H, 0\\n\\t\"\n-            \"sve_uzp1  $dst, H, $src, tmp\\n\\t\"\n-            \"sve_uzp1  $dst, B, $dst, tmp\\n\\t# convert I to B vector\" %}\n+  format %{ \"sve_vectorcast_i2b  $dst, $src\\t# convert I to B vector\" %}\n@@ -2888,3 +4638,2 @@\n-    __ sve_dup(as_FloatRegister($tmp$$reg), __ H, 0);\n-    __ sve_uzp1(as_FloatRegister($dst$$reg), __ H, as_FloatRegister($src$$reg), as_FloatRegister($tmp$$reg));\n-    __ sve_uzp1(as_FloatRegister($dst$$reg), __ B, as_FloatRegister($dst$$reg), as_FloatRegister($tmp$$reg));\n+    __ sve_vector_narrow(as_FloatRegister($dst$$reg), __ B,\n+                         as_FloatRegister($src$$reg), __ S, as_FloatRegister($tmp$$reg));\n@@ -2902,2 +4651,1 @@\n-  format %{ \"sve_dup  $tmp, H, 0\\n\\t\"\n-            \"sve_uzp1  $dst, H, $src, tmp\\t# convert I to S vector\" %}\n+  format %{ \"sve_vectorcast_i2s $dst, $src\\t# convert I to H vector\" %}\n@@ -2905,2 +4653,2 @@\n-    __ sve_dup(as_FloatRegister($tmp$$reg), __ H, 0);\n-    __ sve_uzp1(as_FloatRegister($dst$$reg), __ H, as_FloatRegister($src$$reg), as_FloatRegister($tmp$$reg));\n+    __ sve_vector_narrow(as_FloatRegister($dst$$reg), __ H,\n+                         as_FloatRegister($src$$reg), __ S, as_FloatRegister($tmp$$reg));\n@@ -2917,1 +4665,1 @@\n-  format %{ \"sve_sunpklo  $dst, D, $src\\t# convert I to L vector\" %}\n+  format %{ \"sve_vectorcast_i2l  $dst, $src\\t# convert I to L vector\" %}\n@@ -2919,1 +4667,1 @@\n-    __ sve_sunpklo(as_FloatRegister($dst$$reg), __ D, as_FloatRegister($src$$reg));\n+    __ sve_vector_extend(as_FloatRegister($dst$$reg), __ D, as_FloatRegister($src$$reg), __ S);\n@@ -2930,1 +4678,1 @@\n-  format %{ \"sve_scvtf  $dst, S, $src, S\\t# convert I to F vector\" %}\n+  format %{ \"sve_vectorcast_i2f  $dst, $src\\t# convert I to F vector\" %}\n@@ -2943,2 +4691,1 @@\n-  format %{ \"sve_sunpklo  $dst, D, $src\\n\\t\"\n-            \"sve_scvtf  $dst, D, $dst, D\\t# convert I to D vector\" %}\n+  format %{ \"sve_vectorcast_i2d  $dst, $src\\t# convert I to D vector\" %}\n@@ -2952,21 +4699,1 @@\n-instruct vcvtLtoB(vReg dst, vReg src, vReg tmp)\n-%{\n-  predicate(UseSVE > 0 &&\n-            n->bottom_type()->is_vect()->element_basic_type() == T_BYTE);\n-  match(Set dst (VectorCastL2X src));\n-  effect(TEMP_DEF dst, TEMP tmp);\n-  ins_cost(4 * SVE_COST);\n-  format %{ \"sve_dup  $tmp, S, 0\\n\\t\"\n-            \"sve_uzp1  $dst, S, $src, tmp\\n\\t\"\n-            \"sve_uzp1  $dst, H, $dst, tmp\\n\\t\"\n-            \"sve_uzp1  $dst, B, $dst, tmp\\n\\t# convert L to B vector\" %}\n-  ins_encode %{\n-    __ sve_dup(as_FloatRegister($tmp$$reg), __ S, 0);\n-    __ sve_uzp1(as_FloatRegister($dst$$reg), __ S, as_FloatRegister($src$$reg), as_FloatRegister($tmp$$reg));\n-    __ sve_uzp1(as_FloatRegister($dst$$reg), __ H, as_FloatRegister($dst$$reg), as_FloatRegister($tmp$$reg));\n-    __ sve_uzp1(as_FloatRegister($dst$$reg), __ B, as_FloatRegister($dst$$reg), as_FloatRegister($tmp$$reg));\n-  %}\n-  ins_pipe(pipe_slow);\n-%}\n-\n-instruct vcvtLtoS(vReg dst, vReg src, vReg tmp)\n+instruct vcvtLtoX_narrow(vReg dst, vReg src, vReg tmp)\n@@ -2974,2 +4701,1 @@\n-  predicate(UseSVE > 0 &&\n-            n->bottom_type()->is_vect()->element_basic_type() == T_SHORT);\n+  predicate(UseSVE > 0 && is_integral_type(Matcher::vector_element_basic_type(n)));\n@@ -2978,18 +4704,0 @@\n-  ins_cost(3 * SVE_COST);\n-  format %{ \"sve_dup  $tmp, S, 0\\n\\t\"\n-            \"sve_uzp1  $dst, S, $src, tmp\\n\\t\"\n-            \"sve_uzp1  $dst, H, $dst, tmp\\n\\t# convert L to S vector\" %}\n-  ins_encode %{\n-    __ sve_dup(as_FloatRegister($tmp$$reg), __ S, 0);\n-    __ sve_uzp1(as_FloatRegister($dst$$reg), __ S, as_FloatRegister($src$$reg), as_FloatRegister($tmp$$reg));\n-    __ sve_uzp1(as_FloatRegister($dst$$reg), __ H, as_FloatRegister($dst$$reg), as_FloatRegister($tmp$$reg));\n-  %}\n-  ins_pipe(pipe_slow);\n-%}\n-\n-instruct vcvtLtoI(vReg dst, vReg src, vReg tmp)\n-%{\n-  predicate(UseSVE > 0 &&\n-            n->bottom_type()->is_vect()->element_basic_type() == T_INT);\n-  match(Set dst (VectorCastL2X src));\n-  effect(TEMP tmp);\n@@ -2997,2 +4705,1 @@\n-  format %{ \"sve_dup  $tmp, S, 0\\n\\t\"\n-            \"sve_uzp1  $dst, S, $src, tmp\\t# convert L to I vector\" %}\n+  format %{ \"sve_vectorcast_l2x  $dst, $src\\t# convert L to B\/H\/S vector (narrow)\" %}\n@@ -3000,2 +4707,4 @@\n-    __ sve_dup(as_FloatRegister($tmp$$reg), __ S, 0);\n-    __ sve_uzp1(as_FloatRegister($dst$$reg), __ S, as_FloatRegister($src$$reg), as_FloatRegister($tmp$$reg));\n+    BasicType to_bt = Matcher::vector_element_basic_type(this);\n+    Assembler::SIMD_RegVariant to_size = __ elemType_to_regVariant(to_bt);\n+    __ sve_vector_narrow(as_FloatRegister($dst$$reg), to_size,\n+                         as_FloatRegister($src$$reg), __ D, as_FloatRegister($tmp$$reg));\n@@ -3013,3 +4722,1 @@\n-  format %{ \"sve_scvtf  $dst, S, $src, D\\n\\t\"\n-            \"sve_dup  $tmp, S, 0\\n\\t\"\n-            \"sve_uzp1  $dst, S, $dst, $tmp\\t# convert L to F vector\" %}\n+  format %{ \"sve_vectorcast_l2f  $dst, $src\\t# convert L to F vector\" %}\n@@ -3018,2 +4725,3 @@\n-    __ sve_dup(as_FloatRegister($tmp$$reg), __ S, 0);\n-    __ sve_uzp1(as_FloatRegister($dst$$reg), __ S, as_FloatRegister($dst$$reg), as_FloatRegister($tmp$$reg));\n+    __ sve_vector_narrow(as_FloatRegister($dst$$reg), __ S,\n+                         as_FloatRegister($dst$$reg), __ D, as_FloatRegister($tmp$$reg));\n+\n@@ -3030,1 +4738,1 @@\n-  format %{ \"sve_scvtf  $dst, D, $src, D\\t# convert L to D vector\" %}\n+  format %{ \"sve_vectorcast_l2d  $dst, $src\\t# convert L to D vector\" %}\n@@ -3037,21 +4745,1 @@\n-instruct vcvtFtoB(vReg dst, vReg src, vReg tmp)\n-%{\n-  predicate(UseSVE > 0 &&\n-            n->bottom_type()->is_vect()->element_basic_type() == T_BYTE);\n-  match(Set dst (VectorCastF2X src));\n-  effect(TEMP_DEF dst, TEMP tmp);\n-  ins_cost(4 * SVE_COST);\n-  format %{ \"sve_fcvtzs  $dst, S, $src, S\\n\\t\"\n-            \"sve_dup  $tmp, H, 0\\n\\t\"\n-            \"sve_uzp1  $dst, H, $dst, tmp\\n\\t\"\n-            \"sve_uzp1  $dst, B, $dst, tmp\\n\\t# convert F to B vector\" %}\n-  ins_encode %{\n-    __ sve_fcvtzs(as_FloatRegister($dst$$reg), __ S, ptrue, as_FloatRegister($src$$reg), __ S);\n-    __ sve_dup(as_FloatRegister($tmp$$reg), __ H, 0);\n-    __ sve_uzp1(as_FloatRegister($dst$$reg), __ H, as_FloatRegister($dst$$reg), as_FloatRegister($tmp$$reg));\n-    __ sve_uzp1(as_FloatRegister($dst$$reg), __ B, as_FloatRegister($dst$$reg), as_FloatRegister($tmp$$reg));\n-  %}\n-  ins_pipe(pipe_slow);\n-%}\n-\n-instruct vcvtFtoS(vReg dst, vReg src, vReg tmp)\n+instruct vcvtFtoX_narrow(vReg dst, vReg src, vReg tmp)\n@@ -3060,1 +4748,2 @@\n-            n->bottom_type()->is_vect()->element_basic_type() == T_SHORT);\n+            (n->bottom_type()->is_vect()->element_basic_type() == T_BYTE ||\n+             n->bottom_type()->is_vect()->element_basic_type() == T_SHORT));\n@@ -3064,3 +4753,1 @@\n-  format %{ \"sve_fcvtzs  $dst, S, $src, S\\n\\t\"\n-            \"sve_dup  $tmp, H, 0\\n\\t\"\n-            \"sve_uzp1  $dst, H, $dst, tmp\\t# convert F to S vector\" %}\n+  format %{ \"sve_vectorcast_f2x  $dst, $src\\t# convert F to B\/H vector\" %}\n@@ -3068,0 +4755,2 @@\n+    BasicType to_bt = Matcher::vector_element_basic_type(this);\n+    Assembler::SIMD_RegVariant to_size = __ elemType_to_regVariant(to_bt);\n@@ -3069,2 +4758,2 @@\n-    __ sve_dup(as_FloatRegister($tmp$$reg), __ H, 0);\n-    __ sve_uzp1(as_FloatRegister($dst$$reg), __ H, as_FloatRegister($dst$$reg), as_FloatRegister($tmp$$reg));\n+    __ sve_vector_narrow(as_FloatRegister($dst$$reg), to_size,\n+                         as_FloatRegister($dst$$reg), __ S, as_FloatRegister($tmp$$reg));\n@@ -3078,1 +4767,1 @@\n-            n->bottom_type()->is_vect()->element_basic_type() == T_INT);\n+            (n->bottom_type()->is_vect()->element_basic_type() == T_INT));\n@@ -3081,1 +4770,1 @@\n-  format %{ \"sve_fcvtzs  $dst, S, $src, S\\t# convert F to I vector\" %}\n+  format %{ \"sve_vectorcast_f2x  $dst, $src\\t# convert F to I vector\" %}\n@@ -3091,39 +4780,4 @@\n-            n->bottom_type()->is_vect()->element_basic_type() == T_LONG);\n-  match(Set dst (VectorCastF2X src));\n-  ins_cost(2 * SVE_COST);\n-  format %{ \"sve_fcvtzs  $dst, S, $src, S\\n\\t\"\n-            \"sve_sunpklo  $dst, D, $dst\\t# convert F to L vector\" %}\n-  ins_encode %{\n-    __ sve_fcvtzs(as_FloatRegister($dst$$reg), __ S, ptrue, as_FloatRegister($src$$reg), __ S);\n-    __ sve_sunpklo(as_FloatRegister($dst$$reg), __ D, as_FloatRegister($dst$$reg));\n-  %}\n-  ins_pipe(pipe_slow);\n-%}\n-\n-instruct vcvtFtoD(vReg dst, vReg src)\n-%{\n-  predicate(UseSVE > 0 &&\n-            n->bottom_type()->is_vect()->element_basic_type() == T_DOUBLE);\n-  match(Set dst (VectorCastF2X src));\n-  ins_cost(2 * SVE_COST);\n-  format %{ \"sve_sunpklo  $dst, D, $src\\n\\t\"\n-            \"sve_fcvt  $dst, D, $dst, S\\t# convert F to D vector\" %}\n-  ins_encode %{\n-    __ sve_sunpklo(as_FloatRegister($dst$$reg), __ D, as_FloatRegister($src$$reg));\n-    __ sve_fcvt(as_FloatRegister($dst$$reg), __ D, ptrue, as_FloatRegister($dst$$reg), __ S);\n-  %}\n-  ins_pipe(pipe_slow);\n-%}\n-\n-instruct vcvtDtoB(vReg dst, vReg src, vReg tmp)\n-%{\n-  predicate(UseSVE > 0 &&\n-            n->bottom_type()->is_vect()->element_basic_type() == T_BYTE);\n-  match(Set dst (VectorCastD2X src));\n-  effect(TEMP_DEF dst, TEMP tmp);\n-  ins_cost(5 * SVE_COST);\n-  format %{ \"sve_fcvtzs  $dst, D, $src, D\\n\\t\"\n-            \"sve_dup  $tmp, S, 0\\n\\t\"\n-            \"sve_uzp1  $dst, S, $dst, tmp\\n\\t\"\n-            \"sve_uzp1  $dst, H, $dst, tmp\\n\\t\"\n-            \"sve_uzp1  $dst, B, $dst, tmp\\n\\t# convert D to B vector\" %}\n+            (n->bottom_type()->is_vect()->element_basic_type() == T_LONG));\n+  match(Set dst (VectorCastF2X src));\n+  ins_cost(SVE_COST * 2);\n+  format %{ \"sve_vectorcast_f2x  $dst, $src\\t# convert F to L vector\" %}\n@@ -3131,5 +4785,2 @@\n-    __ sve_fcvtzs(as_FloatRegister($dst$$reg), __ D, ptrue, as_FloatRegister($src$$reg), __ D);\n-    __ sve_dup(as_FloatRegister($tmp$$reg), __ S, 0);\n-    __ sve_uzp1(as_FloatRegister($dst$$reg), __ S, as_FloatRegister($dst$$reg), as_FloatRegister($tmp$$reg));\n-    __ sve_uzp1(as_FloatRegister($dst$$reg), __ H, as_FloatRegister($dst$$reg), as_FloatRegister($tmp$$reg));\n-    __ sve_uzp1(as_FloatRegister($dst$$reg), __ B, as_FloatRegister($dst$$reg), as_FloatRegister($tmp$$reg));\n+    __ sve_sunpklo(as_FloatRegister($dst$$reg), __ D, as_FloatRegister($src$$reg));\n+    __ sve_fcvtzs(as_FloatRegister($dst$$reg), __ D, ptrue, as_FloatRegister($dst$$reg), __ S);\n@@ -3140,1 +4791,1 @@\n-instruct vcvtDtoS(vReg dst, vReg src, vReg tmp)\n+instruct vcvtFtoD(vReg dst, vReg src)\n@@ -3143,8 +4794,4 @@\n-            n->bottom_type()->is_vect()->element_basic_type() == T_SHORT);\n-  match(Set dst (VectorCastD2X src));\n-  effect(TEMP_DEF dst, TEMP tmp);\n-  ins_cost(4 * SVE_COST);\n-  format %{ \"sve_fcvtzs  $dst, D, $src, D\\n\\t\"\n-            \"sve_dup  $tmp, S, 0\\n\\t\"\n-            \"sve_uzp1  $dst, S, $dst, tmp\\n\\t\"\n-            \"sve_uzp1  $dst, H, $dst, tmp\\n\\t# convert D to S vector\" %}\n+            n->bottom_type()->is_vect()->element_basic_type() == T_DOUBLE);\n+  match(Set dst (VectorCastF2X src));\n+  ins_cost(2 * SVE_COST);\n+  format %{ \"sve_vectorcast_f2d  $dst, $dst\\t# convert F to D vector\" %}\n@@ -3152,4 +4799,2 @@\n-    __ sve_fcvtzs(as_FloatRegister($dst$$reg), __ D, ptrue, as_FloatRegister($src$$reg), __ D);\n-    __ sve_dup(as_FloatRegister($tmp$$reg), __ S, 0);\n-    __ sve_uzp1(as_FloatRegister($dst$$reg), __ S, as_FloatRegister($dst$$reg), as_FloatRegister($tmp$$reg));\n-    __ sve_uzp1(as_FloatRegister($dst$$reg), __ H, as_FloatRegister($dst$$reg), as_FloatRegister($tmp$$reg));\n+    __ sve_vector_extend(as_FloatRegister($dst$$reg), __ D, as_FloatRegister($src$$reg), __ S);\n+    __ sve_fcvt(as_FloatRegister($dst$$reg), __ D, ptrue, as_FloatRegister($dst$$reg), __ S);\n@@ -3160,1 +4805,1 @@\n-instruct vcvtDtoI(vReg dst, vReg src, vReg tmp)\n+instruct vcvtDtoX_narrow(vReg dst, vReg src, vReg tmp)\n@@ -3163,1 +4808,3 @@\n-            n->bottom_type()->is_vect()->element_basic_type() == T_INT);\n+            (n->bottom_type()->is_vect()->element_basic_type() == T_BYTE ||\n+             n->bottom_type()->is_vect()->element_basic_type() == T_SHORT ||\n+             n->bottom_type()->is_vect()->element_basic_type() == T_INT));\n@@ -3167,3 +4814,1 @@\n-  format %{ \"sve_fcvtzs  $dst, D, $src, D\\n\\t\"\n-            \"sve_dup  $tmp, S, 0\\n\\t\"\n-            \"sve_uzp1  $dst, S, $dst, tmp\\t# convert D to I vector\" %}\n+  format %{ \"sve_vectorcast_d2x  $dst, $src\\t# convert D to X vector (narrow)\" %}\n@@ -3171,3 +4816,5 @@\n-    __ sve_fcvtzs(as_FloatRegister($dst$$reg), __ D, ptrue, as_FloatRegister($src$$reg), __ D);\n-    __ sve_dup(as_FloatRegister($tmp$$reg), __ S, 0);\n-    __ sve_uzp1(as_FloatRegister($dst$$reg), __ S, as_FloatRegister($dst$$reg), as_FloatRegister($tmp$$reg));\n+    BasicType to_bt = Matcher::vector_element_basic_type(this);\n+    Assembler::SIMD_RegVariant to_size = __ elemType_to_regVariant(to_bt);\n+    __ sve_fcvtzs(as_FloatRegister($dst$$reg), __ S, ptrue, as_FloatRegister($src$$reg), __ D);\n+    __ sve_vector_narrow(as_FloatRegister($dst$$reg), to_size,\n+                         as_FloatRegister($dst$$reg), __ D, as_FloatRegister($tmp$$reg));\n@@ -3184,1 +4831,1 @@\n-  format %{ \"sve_fcvtzs  $dst, D, $src, D\\t# convert D to L vector\" %}\n+  format %{ \"sve_vectorcast_d2l  $dst, $src\\t# convert D to L vector\" %}\n@@ -3198,3 +4845,1 @@\n-  format %{ \"sve_fcvt  $dst, S, $src, D\\n\\t\"\n-            \"sve_dup  $tmp, S, 0\\n\\t\"\n-            \"sve_uzp1  $dst, S, $dst, $tmp\\t# convert D to F vector\" %}\n+  format %{ \"sve_vectorcast_d2f  $dst, S, $dst\\t# convert D to F vector\" %}\n@@ -3203,2 +4848,2 @@\n-    __ sve_dup(as_FloatRegister($tmp$$reg), __ S, 0);\n-    __ sve_uzp1(as_FloatRegister($dst$$reg), __ S, as_FloatRegister($dst$$reg), as_FloatRegister($tmp$$reg));\n+    __ sve_vector_narrow(as_FloatRegister($dst$$reg), __ S,\n+                         as_FloatRegister($dst$$reg), __ D, as_FloatRegister($tmp$$reg));\n@@ -3208,0 +4853,1 @@\n+\n@@ -3210,1 +4856,1 @@\n-instruct extractB(iRegINoSp dst, vReg src, immI idx, pRegGov pTmp, rFlagsReg cr)\n+instruct extractB(iRegINoSp dst, vReg src, immI idx, pRegGov pgtmp, rFlagsReg cr)\n@@ -3214,1 +4860,1 @@\n-  effect(TEMP pTmp, KILL cr);\n+  effect(TEMP pgtmp, KILL cr);\n@@ -3216,1 +4862,1 @@\n-  format %{ \"sve_extract $dst, B, $pTmp, $src, $idx\\n\\t\"\n+  format %{ \"sve_extract $dst, B, $pgtmp, $src, $idx\\n\\t\"\n@@ -3219,1 +4865,1 @@\n-    __ sve_extract(as_Register($dst$$reg), __ B, as_PRegister($pTmp$$reg),\n+    __ sve_extract(as_Register($dst$$reg), __ B, as_PRegister($pgtmp$$reg),\n@@ -3226,1 +4872,1 @@\n-instruct extractS(iRegINoSp dst, vReg src, immI idx, pRegGov pTmp, rFlagsReg cr)\n+instruct extractS(iRegINoSp dst, vReg src, immI idx, pRegGov pgtmp, rFlagsReg cr)\n@@ -3230,1 +4876,1 @@\n-  effect(TEMP pTmp, KILL cr);\n+  effect(TEMP pgtmp, KILL cr);\n@@ -3232,1 +4878,1 @@\n-  format %{ \"sve_extract $dst, H, $pTmp, $src, $idx\\n\\t\"\n+  format %{ \"sve_extract $dst, H, $pgtmp, $src, $idx\\n\\t\"\n@@ -3235,1 +4881,1 @@\n-    __ sve_extract(as_Register($dst$$reg), __ H, as_PRegister($pTmp$$reg),\n+    __ sve_extract(as_Register($dst$$reg), __ H, as_PRegister($pgtmp$$reg),\n@@ -3243,1 +4889,1 @@\n-instruct extractI(iRegINoSp dst, vReg src, immI idx, pRegGov pTmp, rFlagsReg cr)\n+instruct extractI(iRegINoSp dst, vReg src, immI idx, pRegGov pgtmp, rFlagsReg cr)\n@@ -3247,1 +4893,1 @@\n-  effect(TEMP pTmp, KILL cr);\n+  effect(TEMP pgtmp, KILL cr);\n@@ -3249,1 +4895,1 @@\n-  format %{ \"sve_extract $dst, S, $pTmp, $src, $idx\\t# extract from vector(I)\" %}\n+  format %{ \"sve_extract $dst, S, $pgtmp, $src, $idx\\t# extract from vector(I)\" %}\n@@ -3251,1 +4897,1 @@\n-    __ sve_extract(as_Register($dst$$reg), __ S, as_PRegister($pTmp$$reg),\n+    __ sve_extract(as_Register($dst$$reg), __ S, as_PRegister($pgtmp$$reg),\n@@ -3257,1 +4903,1 @@\n-instruct extractL(iRegLNoSp dst, vReg src, immI idx, pRegGov pTmp, rFlagsReg cr)\n+instruct extractL(iRegLNoSp dst, vReg src, immI idx, pRegGov pgtmp, rFlagsReg cr)\n@@ -3261,1 +4907,1 @@\n-  effect(TEMP pTmp, KILL cr);\n+  effect(TEMP pgtmp, KILL cr);\n@@ -3263,1 +4909,1 @@\n-  format %{ \"sve_extract $dst, D, $pTmp, $src, $idx\\t# extract from vector(L)\" %}\n+  format %{ \"sve_extract $dst, D, $pgtmp, $src, $idx\\t# extract from vector(L)\" %}\n@@ -3265,1 +4911,1 @@\n-    __ sve_extract(as_Register($dst$$reg), __ D, as_PRegister($pTmp$$reg),\n+    __ sve_extract(as_Register($dst$$reg), __ D, as_PRegister($pgtmp$$reg),\n@@ -3271,1 +4917,1 @@\n-instruct extractF(vRegF dst, vReg src, immI idx, pRegGov pTmp, rFlagsReg cr)\n+instruct extractF(vRegF dst, vReg src, immI idx, pRegGov pgtmp, rFlagsReg cr)\n@@ -3275,1 +4921,1 @@\n-  effect(TEMP pTmp, KILL cr);\n+  effect(TEMP pgtmp, KILL cr);\n@@ -3277,1 +4923,1 @@\n-  format %{ \"sve_extract $dst, S, $pTmp, $src, $idx\\t# extract from vector(F)\" %}\n+  format %{ \"sve_extract $dst, S, $pgtmp, $src, $idx\\t# extract from vector(F)\" %}\n@@ -3279,1 +4925,1 @@\n-    __ sve_extract(as_FloatRegister($dst$$reg), __ S, as_PRegister($pTmp$$reg),\n+    __ sve_extract(as_FloatRegister($dst$$reg), __ S, as_PRegister($pgtmp$$reg),\n@@ -3285,1 +4931,1 @@\n-instruct extractD(vRegD dst, vReg src, immI idx, pRegGov pTmp, rFlagsReg cr)\n+instruct extractD(vRegD dst, vReg src, immI idx, pRegGov pgtmp, rFlagsReg cr)\n@@ -3289,1 +4935,1 @@\n-  effect(TEMP pTmp, KILL cr);\n+  effect(TEMP pgtmp, KILL cr);\n@@ -3291,1 +4937,1 @@\n-  format %{ \"sve_extract $dst, D, $pTmp, $src, $idx\\t# extract from vector(D)\" %}\n+  format %{ \"sve_extract $dst, D, $pgtmp, $src, $idx\\t# extract from vector(D)\" %}\n@@ -3293,1 +4939,1 @@\n-    __ sve_extract(as_FloatRegister($dst$$reg), __ D, as_PRegister($pTmp$$reg),\n+    __ sve_extract(as_FloatRegister($dst$$reg), __ D, as_PRegister($pgtmp$$reg),\n@@ -3301,1 +4947,1 @@\n-instruct vtest_alltrue(iRegINoSp dst, vReg src1, vReg src2, pReg pTmp, rFlagsReg cr)\n+instruct vtest_alltrue(iRegINoSp dst, pRegGov src1, pRegGov src2, pReg ptmp, rFlagsReg cr)\n@@ -3303,1 +4949,2 @@\n-  predicate(UseSVE > 0 && n->in(1)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize &&\n+  predicate(UseSVE > 0 &&\n+            n->in(1)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize &&\n@@ -3306,1 +4953,1 @@\n-  effect(TEMP pTmp, KILL cr);\n+  effect(TEMP ptmp, KILL cr);\n@@ -3308,1 +4955,1 @@\n-  format %{ \"sve_cmpeq $pTmp, $src1, 0\\n\\t\"\n+  format %{ \"sve_eors $ptmp, $src1, $src2\\t# $src2 is all true mask\\n\"\n@@ -3311,5 +4958,2 @@\n-    \/\/ \"src2\" is not used for sve.\n-    BasicType bt = Matcher::vector_element_basic_type(this, $src1);\n-    Assembler::SIMD_RegVariant size = __ elemType_to_regVariant(bt);\n-    __ sve_cmp(Assembler::EQ, as_PRegister($pTmp$$reg), size,\n-               ptrue, as_FloatRegister($src1$$reg), 0);\n+    __ sve_eors(as_PRegister($ptmp$$reg), ptrue,\n+                as_PRegister($src1$$reg), as_PRegister($src2$$reg));\n@@ -3321,1 +4965,1 @@\n-instruct vtest_anytrue(iRegINoSp dst, vReg src1, vReg src2, pReg pTmp, rFlagsReg cr)\n+instruct vtest_anytrue(iRegINoSp dst, pRegGov src1, pRegGov src2, rFlagsReg cr)\n@@ -3323,1 +4967,2 @@\n-  predicate(UseSVE > 0 && n->in(1)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize &&\n+  predicate(UseSVE > 0 &&\n+            n->in(1)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize &&\n@@ -3326,1 +4971,1 @@\n-  effect(TEMP pTmp, KILL cr);\n+  effect(KILL cr);\n@@ -3328,1 +4973,1 @@\n-  format %{ \"sve_cmpeq $pTmp, $src1, -1\\n\\t\"\n+  format %{ \"sve_ptest $src1\\n\\t\"\n@@ -3332,4 +4977,1 @@\n-    BasicType bt = Matcher::vector_element_basic_type(this, $src1);\n-    Assembler::SIMD_RegVariant size = __ elemType_to_regVariant(bt);\n-    __ sve_cmp(Assembler::EQ, as_PRegister($pTmp$$reg), size,\n-               ptrue, as_FloatRegister($src1$$reg), -1);\n+    __ sve_ptest(ptrue, as_PRegister($src1$$reg));\n@@ -3341,1 +4983,1 @@\n-instruct vtest_alltrue_partial(iRegINoSp dst, vReg src1, vReg src2, pRegGov pTmp, rFlagsReg cr)\n+instruct vtest_alltrue_partial(iRegINoSp dst, pRegGov src1, pRegGov src2, pRegGov ptmp, rFlagsReg cr)\n@@ -3343,1 +4985,2 @@\n-  predicate(UseSVE > 0 && n->in(1)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize &&\n+  predicate(UseSVE > 0 &&\n+            n->in(1)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize &&\n@@ -3346,1 +4989,1 @@\n-  effect(TEMP pTmp, KILL cr);\n+  effect(TEMP ptmp, KILL cr);\n@@ -3350,1 +4993,0 @@\n-    \/\/ \"src2\" is not used for sve.\n@@ -3353,1 +4995,1 @@\n-    __ sve_whilelo_zr_imm(as_PRegister($pTmp$$reg), size,\n+    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), size,\n@@ -3355,2 +4997,2 @@\n-    __ sve_cmp(Assembler::EQ, as_PRegister($pTmp$$reg), size,\n-               as_PRegister($pTmp$$reg), as_FloatRegister($src1$$reg), 0);\n+    __ sve_eors(as_PRegister($ptmp$$reg), as_PRegister($ptmp$$reg),\n+          as_PRegister($src1$$reg), as_PRegister($src2$$reg));\n@@ -3362,1 +5004,1 @@\n-instruct vtest_anytrue_partial(iRegINoSp dst, vReg src1, vReg src2, pRegGov pTmp, rFlagsReg cr)\n+instruct vtest_anytrue_partial(iRegINoSp dst, pRegGov src1, pRegGov src2, pRegGov ptmp, rFlagsReg cr)\n@@ -3364,1 +5006,2 @@\n-  predicate(UseSVE > 0 && n->in(1)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize &&\n+  predicate(UseSVE > 0 &&\n+            n->in(1)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize &&\n@@ -3367,1 +5010,1 @@\n-  effect(TEMP pTmp, KILL cr);\n+  effect(TEMP ptmp, KILL cr);\n@@ -3371,1 +5014,0 @@\n-    \/\/ \"src2\" is not used for sve.\n@@ -3374,1 +5016,1 @@\n-    __ sve_whilelo_zr_imm(as_PRegister($pTmp$$reg), size,\n+    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), size,\n@@ -3376,2 +5018,2 @@\n-    __ sve_cmp(Assembler::EQ, as_PRegister($pTmp$$reg), size,\n-               as_PRegister($pTmp$$reg), as_FloatRegister($src1$$reg), -1);\n+    __ sve_ands(as_PRegister($ptmp$$reg), as_PRegister($ptmp$$reg),\n+          as_PRegister($src1$$reg), as_PRegister($src2$$reg));\n@@ -3385,1 +5027,1 @@\n-instruct insertI_small(vReg dst, vReg src, iRegIorL2I val, immI idx, pRegGov pTmp, rFlagsReg cr)\n+instruct insertI_small(vReg dst, vReg src, iRegIorL2I val, immI idx, pRegGov pgtmp, rFlagsReg cr)\n@@ -3392,1 +5034,1 @@\n-  effect(TEMP_DEF dst, TEMP pTmp, KILL cr);\n+  effect(TEMP_DEF dst, TEMP pgtmp, KILL cr);\n@@ -3394,2 +5036,2 @@\n-  format %{ \"sve_index $dst, -16, 1\\t# (B\/S\/I)\\n\\t\"\n-            \"sve_cmpeq $pTmp, $dst, ($idx-#16) # shift from [0, 31] to [-16, 15]\\n\\t\"\n+  format %{ \"sve_index $dst, -16, 1\\t# (B\/H\/S)\\n\\t\"\n+            \"sve_cmpeq $pgtmp, $dst, ($idx-#16) # shift from [0, 31] to [-16, 15]\\n\\t\"\n@@ -3397,1 +5039,1 @@\n-            \"sve_cpy $dst, $pTmp, $val\\t# insert into vector (B\/S\/I)\" %}\n+            \"sve_cpy $dst, $pgtmp, $val\\t# insert into vector (B\/H\/S)\" %}\n@@ -3402,1 +5044,1 @@\n-    __ sve_cmp(Assembler::EQ, as_PRegister($pTmp$$reg), size, ptrue,\n+    __ sve_cmp(Assembler::EQ, as_PRegister($pgtmp$$reg), size, ptrue,\n@@ -3405,1 +5047,1 @@\n-    __ sve_cpy(as_FloatRegister($dst$$reg), size, as_PRegister($pTmp$$reg), as_Register($val$$reg));\n+    __ sve_cpy(as_FloatRegister($dst$$reg), size, as_PRegister($pgtmp$$reg), as_Register($val$$reg));\n@@ -3410,1 +5052,1 @@\n-instruct insertF_small(vReg dst, vReg src, vRegF val, immI idx, pRegGov pTmp, rFlagsReg cr)\n+instruct insertF_small(vReg dst, vReg src, vRegF val, immI idx, pRegGov pgtmp, rFlagsReg cr)\n@@ -3415,1 +5057,1 @@\n-  effect(TEMP_DEF dst, TEMP pTmp, KILL cr);\n+  effect(TEMP_DEF dst, TEMP pgtmp, KILL cr);\n@@ -3418,1 +5060,1 @@\n-            \"sve_cmpeq $pTmp, $dst, ($idx-#16) # shift from [0, 31] to [-16, 15]\\n\\t\"\n+            \"sve_cmpeq $pgtmp, $dst, ($idx-#16) # shift from [0, 31] to [-16, 15]\\n\\t\"\n@@ -3420,1 +5062,1 @@\n-            \"sve_cpy $dst, $pTmp, $val\\t# insert into vector (F)\" %}\n+            \"sve_cpy $dst, $pgtmp, $val\\t# insert into vector (F)\" %}\n@@ -3423,1 +5065,1 @@\n-    __ sve_cmp(Assembler::EQ, as_PRegister($pTmp$$reg), __ S, ptrue,\n+    __ sve_cmp(Assembler::EQ, as_PRegister($pgtmp$$reg), __ S, ptrue,\n@@ -3426,1 +5068,1 @@\n-    __ sve_cpy(as_FloatRegister($dst$$reg), __ S, as_PRegister($pTmp$$reg), as_FloatRegister($val$$reg));\n+    __ sve_cpy(as_FloatRegister($dst$$reg), __ S, as_PRegister($pgtmp$$reg), as_FloatRegister($val$$reg));\n@@ -3431,1 +5073,1 @@\n-instruct insertI(vReg dst, vReg src, iRegIorL2I val, immI idx, vReg tmp1, pRegGov pTmp, rFlagsReg cr)\n+instruct insertI(vReg dst, vReg src, iRegIorL2I val, immI idx, vReg tmp1, pRegGov pgtmp, rFlagsReg cr)\n@@ -3438,1 +5080,1 @@\n-  effect(TEMP_DEF dst, TEMP tmp1, TEMP pTmp, KILL cr);\n+  effect(TEMP_DEF dst, TEMP tmp1, TEMP pgtmp, KILL cr);\n@@ -3440,3 +5082,3 @@\n-  format %{ \"sve_index $tmp1, 0, 1\\t# (B\/S\/I)\\n\\t\"\n-            \"sve_dup $dst, $idx\\t# (B\/S\/I)\\n\\t\"\n-            \"sve_cmpeq $pTmp, $tmp1, $dst\\n\\t\"\n+  format %{ \"sve_index $tmp1, 0, 1\\t# (B\/H\/S)\\n\\t\"\n+            \"sve_dup $dst, $idx\\t# (B\/H\/S)\\n\\t\"\n+            \"sve_cmpeq $pgtmp, $tmp1, $dst\\n\\t\"\n@@ -3444,1 +5086,1 @@\n-            \"sve_cpy $dst, $pTmp, $val\\t# insert into vector (B\/S\/I)\" %}\n+            \"sve_cpy $dst, $pgtmp, $val\\t# insert into vector (B\/H\/S)\" %}\n@@ -3450,1 +5092,1 @@\n-    __ sve_cmp(Assembler::EQ, as_PRegister($pTmp$$reg), size, ptrue,\n+    __ sve_cmp(Assembler::EQ, as_PRegister($pgtmp$$reg), size, ptrue,\n@@ -3453,1 +5095,1 @@\n-    __ sve_cpy(as_FloatRegister($dst$$reg), size, as_PRegister($pTmp$$reg), as_Register($val$$reg));\n+    __ sve_cpy(as_FloatRegister($dst$$reg), size, as_PRegister($pgtmp$$reg), as_Register($val$$reg));\n@@ -3458,1 +5100,1 @@\n-instruct insertL(vReg dst, vReg src, iRegL val, immI idx, pRegGov pTmp, rFlagsReg cr)\n+instruct insertL(vReg dst, vReg src, iRegL val, immI idx, pRegGov pgtmp, rFlagsReg cr)\n@@ -3463,1 +5105,1 @@\n-  effect(TEMP_DEF dst, TEMP pTmp, KILL cr);\n+  effect(TEMP_DEF dst, TEMP pgtmp, KILL cr);\n@@ -3466,1 +5108,1 @@\n-            \"sve_cmpeq $pTmp, $dst, ($idx-#16) # shift from [0, 31] to [-16, 15]\\n\\t\"\n+            \"sve_cmpeq $pgtmp, $dst, ($idx-#16) # shift from [0, 31] to [-16, 15]\\n\\t\"\n@@ -3468,1 +5110,1 @@\n-            \"sve_cpy $dst, $pTmp, $val\\t# insert into vector (L)\" %}\n+            \"sve_cpy $dst, $pgtmp, $val\\t# insert into vector (L)\" %}\n@@ -3471,1 +5113,1 @@\n-    __ sve_cmp(Assembler::EQ, as_PRegister($pTmp$$reg), __ D, ptrue,\n+    __ sve_cmp(Assembler::EQ, as_PRegister($pgtmp$$reg), __ D, ptrue,\n@@ -3474,1 +5116,1 @@\n-    __ sve_cpy(as_FloatRegister($dst$$reg), __ D, as_PRegister($pTmp$$reg), as_Register($val$$reg));\n+    __ sve_cpy(as_FloatRegister($dst$$reg), __ D, as_PRegister($pgtmp$$reg), as_Register($val$$reg));\n@@ -3479,1 +5121,1 @@\n-instruct insertD(vReg dst, vReg src, vRegD val, immI idx, pRegGov pTmp, rFlagsReg cr)\n+instruct insertD(vReg dst, vReg src, vRegD val, immI idx, pRegGov pgtmp, rFlagsReg cr)\n@@ -3484,1 +5126,1 @@\n-  effect(TEMP_DEF dst, TEMP pTmp, KILL cr);\n+  effect(TEMP_DEF dst, TEMP pgtmp, KILL cr);\n@@ -3487,1 +5129,1 @@\n-            \"sve_cmpeq $pTmp, $dst, ($idx-#16) # shift from [0, 31] to [-16, 15]\\n\\t\"\n+            \"sve_cmpeq $pgtmp, $dst, ($idx-#16) # shift from [0, 31] to [-16, 15]\\n\\t\"\n@@ -3489,1 +5131,1 @@\n-            \"sve_cpy $dst, $pTmp, $val\\t# insert into vector (D)\" %}\n+            \"sve_cpy $dst, $pgtmp, $val\\t# insert into vector (D)\" %}\n@@ -3492,1 +5134,1 @@\n-    __ sve_cmp(Assembler::EQ, as_PRegister($pTmp$$reg), __ D, ptrue,\n+    __ sve_cmp(Assembler::EQ, as_PRegister($pgtmp$$reg), __ D, ptrue,\n@@ -3495,1 +5137,1 @@\n-    __ sve_cpy(as_FloatRegister($dst$$reg), __ D, as_PRegister($pTmp$$reg), as_FloatRegister($val$$reg));\n+    __ sve_cpy(as_FloatRegister($dst$$reg), __ D, as_PRegister($pgtmp$$reg), as_FloatRegister($val$$reg));\n@@ -3500,1 +5142,1 @@\n-instruct insertF(vReg dst, vReg src, vRegF val, immI idx, vReg tmp1, pRegGov pTmp, rFlagsReg cr)\n+instruct insertF(vReg dst, vReg src, vRegF val, immI idx, vReg tmp1, pRegGov pgtmp, rFlagsReg cr)\n@@ -3505,1 +5147,1 @@\n-  effect(TEMP_DEF dst, TEMP tmp1, TEMP pTmp, KILL cr);\n+  effect(TEMP_DEF dst, TEMP tmp1, TEMP pgtmp, KILL cr);\n@@ -3509,1 +5151,1 @@\n-            \"sve_cmpeq $pTmp, $tmp1, $dst\\n\\t\"\n+            \"sve_cmpeq $pgtmp, $tmp1, $dst\\n\\t\"\n@@ -3511,1 +5153,1 @@\n-            \"sve_cpy $dst, $pTmp, $val\\t# insert into vector (F)\" %}\n+            \"sve_cpy $dst, $pgtmp, $val\\t# insert into vector (F)\" %}\n@@ -3515,1 +5157,1 @@\n-    __ sve_cmp(Assembler::EQ, as_PRegister($pTmp$$reg), __ S, ptrue,\n+    __ sve_cmp(Assembler::EQ, as_PRegister($pgtmp$$reg), __ S, ptrue,\n@@ -3521,1 +5163,1 @@\n-               as_PRegister($pTmp$$reg), as_FloatRegister($val$$reg));\n+               as_PRegister($pgtmp$$reg), as_FloatRegister($val$$reg));\n@@ -3528,3 +5170,2 @@\n-instruct loadshuffleB(vReg dst, vReg src)\n-%{\n-  predicate(UseSVE > 0 && n->bottom_type()->is_vect()->element_basic_type() == T_BYTE);\n+instruct loadshuffle(vReg dst, vReg src) %{\n+  predicate(UseSVE > 0);\n@@ -3533,1 +5174,1 @@\n-  format %{ \"sve_orr $dst, $src, $src\\t# vector load shuffle (B)\" %}\n+  format %{ \"sve_loadshuffle $dst, $src\\t# vector load shuffle (B\/H\/S\/D)\" %}\n@@ -3535,4 +5176,9 @@\n-    if (as_FloatRegister($dst$$reg) != as_FloatRegister($src$$reg)) {\n-      __ sve_orr(as_FloatRegister($dst$$reg),\n-                 as_FloatRegister($src$$reg),\n-                 as_FloatRegister($src$$reg));\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    if (bt == T_BYTE) {\n+      if (as_FloatRegister($dst$$reg) != as_FloatRegister($src$$reg)) {\n+        __ sve_orr(as_FloatRegister($dst$$reg), as_FloatRegister($src$$reg),\n+                   as_FloatRegister($src$$reg));\n+      }\n+    } else {\n+      __ sve_vector_extend(as_FloatRegister($dst$$reg),  __ elemType_to_regVariant(bt),\n+                           as_FloatRegister($src$$reg), __ B);\n@@ -3544,46 +5190,0 @@\n-instruct loadshuffleS(vReg dst, vReg src)\n-%{\n-  predicate(UseSVE > 0 && n->bottom_type()->is_vect()->element_basic_type() == T_SHORT);\n-  match(Set dst (VectorLoadShuffle src));\n-  ins_cost(SVE_COST);\n-  format %{ \"sve_uunpklo $dst, $src\\t# vector load shuffle (B to H)\" %}\n-  ins_encode %{\n-    __ sve_uunpklo(as_FloatRegister($dst$$reg), __ H, as_FloatRegister($src$$reg));\n-  %}\n-  ins_pipe(pipe_slow);\n-%}\n-\n-instruct loadshuffleI(vReg dst, vReg src)\n-%{\n-  predicate(UseSVE > 0 &&\n-           (n->bottom_type()->is_vect()->element_basic_type() == T_INT ||\n-            n->bottom_type()->is_vect()->element_basic_type() == T_FLOAT));\n-  match(Set dst (VectorLoadShuffle src));\n-  ins_cost(2 * SVE_COST);\n-  format %{ \"sve_uunpklo $dst, H, $src\\n\\t\"\n-            \"sve_uunpklo $dst, S, $dst\\t# vector load shuffle (B to S)\" %}\n-  ins_encode %{\n-    __ sve_uunpklo(as_FloatRegister($dst$$reg), __ H, as_FloatRegister($src$$reg));\n-    __ sve_uunpklo(as_FloatRegister($dst$$reg), __ S, as_FloatRegister($dst$$reg));\n-  %}\n-  ins_pipe(pipe_slow);\n-%}\n-\n-instruct loadshuffleL(vReg dst, vReg src)\n-%{\n-  predicate(UseSVE > 0 &&\n-           (n->bottom_type()->is_vect()->element_basic_type() == T_LONG ||\n-            n->bottom_type()->is_vect()->element_basic_type() == T_DOUBLE));\n-  match(Set dst (VectorLoadShuffle src));\n-  ins_cost(3 * SVE_COST);\n-  format %{ \"sve_uunpklo $dst, H, $src\\n\\t\"\n-            \"sve_uunpklo $dst, S, $dst\\n\\t\"\n-            \"sve_uunpklo $dst, D, $dst\\t# vector load shuffle (B to D)\" %}\n-  ins_encode %{\n-    __ sve_uunpklo(as_FloatRegister($dst$$reg), __ H, as_FloatRegister($src$$reg));\n-    __ sve_uunpklo(as_FloatRegister($dst$$reg), __ S, as_FloatRegister($dst$$reg));\n-    __ sve_uunpklo(as_FloatRegister($dst$$reg), __ D, as_FloatRegister($dst$$reg));\n-  %}\n-  ins_pipe(pipe_slow);\n-%}\n-\n@@ -3616,1 +5216,1 @@\n-  format %{ \"load_vector_gather $dst, $mem, $idx\\t# vector load gather (I\/F)\" %}\n+  format %{ \"load_vector_gather $dst, $mem, $idx\\t# vector load gather (S)\" %}\n@@ -3631,2 +5231,1 @@\n-  format %{ \"sve_uunpklo $idx, $idx\\n\\t\"\n-            \"load_vector_gather $dst, $mem, $idx\\t# vector load gather (L\/D)\" %}\n+  format %{ \"load_vector_gather $dst, $mem, $idx\\t# vector load gather (D)\" %}\n@@ -3635,1 +5234,2 @@\n-    __ sve_ld1d_gather(as_FloatRegister($dst$$reg), ptrue, as_Register($mem$$base), as_FloatRegister($idx$$reg));\n+    __ sve_ld1d_gather(as_FloatRegister($dst$$reg), ptrue, as_Register($mem$$base),\n+                       as_FloatRegister($idx$$reg));\n@@ -3642,1 +5242,1 @@\n-instruct gatherI_partial(vReg dst, indirect mem, vReg idx, pRegGov pTmp, rFlagsReg cr) %{\n+instruct gatherI_partial(vReg dst, indirect mem, vReg idx, pRegGov ptmp, rFlagsReg cr) %{\n@@ -3648,1 +5248,1 @@\n-  effect(TEMP pTmp, KILL cr);\n+  effect(TEMP ptmp, KILL cr);\n@@ -3650,2 +5250,1 @@\n-  format %{ \"sve_whilelo_zr_imm $pTmp, vector_length\\n\\t\"\n-            \"load_vector_gather $dst, $pTmp, $mem, $idx\\t# vector load gather partial (I\/F)\" %}\n+  format %{ \"load_vector_gather $dst, $ptmp, $mem, $idx\\t# vector load gather partial (S)\" %}\n@@ -3653,3 +5252,2 @@\n-    __ sve_whilelo_zr_imm(as_PRegister($pTmp$$reg), __ S,\n-                          Matcher::vector_length(this));\n-    __ sve_ld1w_gather(as_FloatRegister($dst$$reg), as_PRegister($pTmp$$reg),\n+    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), __ S, Matcher::vector_length(this));\n+    __ sve_ld1w_gather(as_FloatRegister($dst$$reg), as_PRegister($ptmp$$reg),\n@@ -3661,1 +5259,1 @@\n-instruct gatherL_partial(vReg dst, indirect mem, vReg idx, pRegGov pTmp, rFlagsReg cr) %{\n+instruct gatherL_partial(vReg dst, indirect mem, vReg idx, pRegGov ptmp, rFlagsReg cr) %{\n@@ -3667,1 +5265,1 @@\n-  effect(TEMP pTmp, KILL cr);\n+  effect(TEMP ptmp, KILL cr);\n@@ -3669,3 +5267,36 @@\n-  format %{ \"sve_whilelo_zr_imm $pTmp, vector_length\\n\\t\"\n-            \"sve_uunpklo $idx, $idx\\n\\t\"\n-            \"load_vector_gather $dst, $pTmp, $mem, $idx\\t# vector load gather partial (L\/D)\" %}\n+  format %{ \"load_vector_gather $dst, $ptmp, $mem, $idx\\t# vector load gather partial (D)\" %}\n+  ins_encode %{\n+    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), __ D,\n+                          Matcher::vector_length(this));\n+    __ sve_uunpklo(as_FloatRegister($idx$$reg), __ D, as_FloatRegister($idx$$reg));\n+    __ sve_ld1d_gather(as_FloatRegister($dst$$reg), as_PRegister($ptmp$$reg),\n+                       as_Register($mem$$base), as_FloatRegister($idx$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ ------------------------------ Vector Load Gather Predicated -------------------------------\n+\n+instruct gatherI_masked(vReg dst, indirect mem, vReg idx, pRegGov pg) %{\n+  predicate(UseSVE > 0 &&\n+            n->as_LoadVector()->memory_size() == MaxVectorSize &&\n+            (n->bottom_type()->is_vect()->element_basic_type() == T_INT ||\n+             n->bottom_type()->is_vect()->element_basic_type() == T_FLOAT));\n+  match(Set dst (LoadVectorGatherMasked mem (Binary idx pg)));\n+  ins_cost(SVE_COST);\n+  format %{ \"load_vector_gather $dst, $pg, $mem, $idx\\t# vector load gather predicated (S)\" %}\n+  ins_encode %{\n+    __ sve_ld1w_gather(as_FloatRegister($dst$$reg), as_PRegister($pg$$reg),\n+                       as_Register($mem$$base), as_FloatRegister($idx$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct gatherL_masked(vReg dst, indirect mem, vReg idx, pRegGov pg) %{\n+  predicate(UseSVE > 0 &&\n+            n->as_LoadVector()->memory_size() == MaxVectorSize &&\n+            (n->bottom_type()->is_vect()->element_basic_type() == T_LONG ||\n+             n->bottom_type()->is_vect()->element_basic_type() == T_DOUBLE));\n+  match(Set dst (LoadVectorGatherMasked mem (Binary idx pg)));\n+  ins_cost(2 * SVE_COST);\n+  format %{ \"load_vector_gather $dst, $pg, $mem, $idx\\t# vector load gather predicated (D)\" %}\n@@ -3673,1 +5304,20 @@\n-    __ sve_whilelo_zr_imm(as_PRegister($pTmp$$reg), __ D,\n+    __ sve_uunpklo(as_FloatRegister($idx$$reg), __ D, as_FloatRegister($idx$$reg));\n+    __ sve_ld1d_gather(as_FloatRegister($dst$$reg), as_PRegister($pg$$reg),\n+                       as_Register($mem$$base), as_FloatRegister($idx$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ ------------------------------ Vector Load Gather Predicated Partial -------------------------------\n+\n+instruct gatherI_masked_partial(vReg dst, indirect mem, vReg idx, pRegGov pg, pRegGov ptmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->as_LoadVector()->memory_size() < MaxVectorSize &&\n+            (n->bottom_type()->is_vect()->element_basic_type() == T_INT ||\n+             n->bottom_type()->is_vect()->element_basic_type() == T_FLOAT));\n+  match(Set dst (LoadVectorGatherMasked mem (Binary idx pg)));\n+  effect(TEMP ptmp, KILL cr);\n+  ins_cost(3 * SVE_COST);\n+  format %{ \"load_vector_gather $dst, $pg, $mem, $idx\\t# vector load gather predicated partial (S)\" %}\n+  ins_encode %{\n+    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), __ S,\n@@ -3675,0 +5325,21 @@\n+    __ sve_and(as_PRegister($ptmp$$reg), as_PRegister($ptmp$$reg),\n+               as_PRegister($pg$$reg), as_PRegister($pg$$reg));\n+    __ sve_ld1w_gather(as_FloatRegister($dst$$reg), as_PRegister($ptmp$$reg),\n+                       as_Register($mem$$base), as_FloatRegister($idx$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct gatherL_masked_partial(vReg dst, indirect mem, vReg idx, pRegGov pg, pRegGov ptmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->as_LoadVector()->memory_size() < MaxVectorSize &&\n+            (n->bottom_type()->is_vect()->element_basic_type() == T_LONG ||\n+             n->bottom_type()->is_vect()->element_basic_type() == T_DOUBLE));\n+  match(Set dst (LoadVectorGatherMasked mem (Binary idx pg)));\n+  effect(TEMP ptmp, KILL cr);\n+  ins_cost(4 * SVE_COST);\n+  format %{ \"load_vector_gather $dst, $pg, $mem, $idx\\t# vector load gather predicated partial (D)\" %}\n+  ins_encode %{\n+    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), __ D, Matcher::vector_length(this));\n+    __ sve_and(as_PRegister($ptmp$$reg), as_PRegister($ptmp$$reg),\n+               as_PRegister($pg$$reg), as_PRegister($pg$$reg));\n@@ -3676,1 +5347,1 @@\n-    __ sve_ld1d_gather(as_FloatRegister($dst$$reg), as_PRegister($pTmp$$reg),\n+    __ sve_ld1d_gather(as_FloatRegister($dst$$reg), as_PRegister($ptmp$$reg),\n@@ -3691,1 +5362,1 @@\n-  format %{ \"store_vector_scatter $mem, $idx, $src\\t# vector store scatter (I\/F)\" %}\n+  format %{ \"store_vector_scatter $mem, $idx, $src\\t# vector store scatter (S)\" %}\n@@ -3706,2 +5377,1 @@\n-  format %{ \"sve_uunpklo $idx, $idx\\n\\t\"\n-            \"store_vector_scatter $mem, $idx, $src\\t# vector store scatter (L\/D)\" %}\n+  format %{ \"store_vector_scatter $mem, $idx, $src\\t# vector store scatter (D)\" %}\n@@ -3709,2 +5379,1 @@\n-    __ sve_uunpklo(as_FloatRegister($idx$$reg), __ D,\n-                   as_FloatRegister($idx$$reg));\n+    __ sve_uunpklo(as_FloatRegister($idx$$reg), __ D, as_FloatRegister($idx$$reg));\n@@ -3717,1 +5386,1 @@\n-\/\/ ------------------------------ Vector Store Scatter Partial-------------------------------\n+\/\/ ------------------------------ Vector Store Scatter Partial -------------------------------\n@@ -3719,1 +5388,1 @@\n-instruct scatterI_partial(indirect mem, vReg src, vReg idx, pRegGov pTmp, rFlagsReg cr) %{\n+instruct scatterI_partial(indirect mem, vReg src, vReg idx, pRegGov ptmp, rFlagsReg cr) %{\n@@ -3725,1 +5394,1 @@\n-  effect(TEMP pTmp, KILL cr);\n+  effect(TEMP ptmp, KILL cr);\n@@ -3727,2 +5396,1 @@\n-  format %{ \"sve_whilelo_zr_imm $pTmp, vector_length\\n\\t\"\n-            \"store_vector_scatter $mem, $pTmp, $idx, $src\\t# vector store scatter partial (I\/F)\" %}\n+  format %{ \"store_vector_scatter $mem, $ptmp, $idx, $src\\t# vector store scatter partial (S)\" %}\n@@ -3730,1 +5398,1 @@\n-    __ sve_whilelo_zr_imm(as_PRegister($pTmp$$reg), __ S,\n+    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), __ S,\n@@ -3732,1 +5400,1 @@\n-    __ sve_st1w_scatter(as_FloatRegister($src$$reg), as_PRegister($pTmp$$reg),\n+    __ sve_st1w_scatter(as_FloatRegister($src$$reg), as_PRegister($ptmp$$reg),\n@@ -3738,1 +5406,1 @@\n-instruct scatterL_partial(indirect mem, vReg src, vReg idx, pRegGov pTmp, rFlagsReg cr) %{\n+instruct scatterL_partial(indirect mem, vReg src, vReg idx, pRegGov ptmp, rFlagsReg cr) %{\n@@ -3744,1 +5412,1 @@\n-  effect(TEMP pTmp, KILL cr);\n+  effect(TEMP ptmp, KILL cr);\n@@ -3746,3 +5414,1 @@\n-  format %{ \"sve_whilelo_zr_imm $pTmp, vector_length\\n\\t\"\n-            \"sve_uunpklo $idx, $idx\\n\\t\"\n-            \"store_vector_scatter $mem, $pTmp, $idx, $src\\t# vector store scatter partial (L\/D)\" %}\n+  format %{ \"store_vector_scatter $mem, $ptmp, $idx, $src\\t# vector store scatter partial (D)\" %}\n@@ -3750,1 +5416,1 @@\n-    __ sve_whilelo_zr_imm(as_PRegister($pTmp$$reg), __ D,\n+    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), __ D,\n@@ -3753,1 +5419,56 @@\n-    __ sve_st1d_scatter(as_FloatRegister($src$$reg), as_PRegister($pTmp$$reg),\n+    __ sve_st1d_scatter(as_FloatRegister($src$$reg), as_PRegister($ptmp$$reg),\n+                        as_Register($mem$$base), as_FloatRegister($idx$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ ------------------------------ Vector Store Scatter Predicated -------------------------------\n+\n+instruct scatterI_masked(indirect mem, vReg src, vReg idx, pRegGov pg) %{\n+  predicate(UseSVE > 0 &&\n+            n->as_StoreVector()->memory_size() == MaxVectorSize &&\n+            (n->in(3)->in(1)->bottom_type()->is_vect()->element_basic_type() == T_INT ||\n+             n->in(3)->in(1)->bottom_type()->is_vect()->element_basic_type() == T_FLOAT));\n+  match(Set mem (StoreVectorScatterMasked mem (Binary src (Binary idx pg))));\n+  ins_cost(SVE_COST);\n+  format %{ \"store_vector_scatter $mem, $pg, $idx, $src\\t# vector store scatter predicate (S)\" %}\n+  ins_encode %{\n+    __ sve_st1w_scatter(as_FloatRegister($src$$reg), as_PRegister($pg$$reg),\n+                        as_Register($mem$$base), as_FloatRegister($idx$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct scatterL_masked(indirect mem, vReg src, vReg idx, pRegGov pg) %{\n+  predicate(UseSVE > 0 &&\n+            n->as_StoreVector()->memory_size() == MaxVectorSize &&\n+            (n->in(3)->in(1)->bottom_type()->is_vect()->element_basic_type() == T_LONG ||\n+             n->in(3)->in(1)->bottom_type()->is_vect()->element_basic_type() == T_DOUBLE));\n+  match(Set mem (StoreVectorScatterMasked mem (Binary src (Binary idx pg))));\n+  ins_cost(2 * SVE_COST);\n+  format %{ \"store_vector_scatter $mem, $pg, $idx, $src\\t# vector store scatter predicated (D)\" %}\n+  ins_encode %{\n+    __ sve_uunpklo(as_FloatRegister($idx$$reg), __ D, as_FloatRegister($idx$$reg));\n+    __ sve_st1d_scatter(as_FloatRegister($src$$reg), as_PRegister($pg$$reg),\n+                        as_Register($mem$$base), as_FloatRegister($idx$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ ------------------------------ Vector Store Scatter Predicated Partial -------------------------------\n+\n+instruct scatterI_masked_partial(indirect mem, vReg src, vReg idx, pRegGov pg, pRegGov ptmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->as_StoreVector()->memory_size() < MaxVectorSize &&\n+            (n->in(3)->in(1)->bottom_type()->is_vect()->element_basic_type() == T_INT ||\n+             n->in(3)->in(1)->bottom_type()->is_vect()->element_basic_type() == T_FLOAT));\n+  match(Set mem (StoreVectorScatterMasked mem (Binary src (Binary idx pg))));\n+  effect(TEMP ptmp, KILL cr);\n+  ins_cost(3 * SVE_COST);\n+  format %{ \"store_vector_scatter $mem, $pg, $idx, $src\\t# vector store scatter predicated partial (S)\" %}\n+  ins_encode %{\n+    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), __ S,\n+                          Matcher::vector_length(this, $src));\n+    __ sve_and(as_PRegister($ptmp$$reg), as_PRegister($ptmp$$reg),\n+               as_PRegister($pg$$reg), as_PRegister($pg$$reg));\n+    __ sve_st1w_scatter(as_FloatRegister($src$$reg), as_PRegister($ptmp$$reg),\n@@ -3759,0 +5480,20 @@\n+instruct scatterL_masked_partial(indirect mem, vReg src, vReg idx, pRegGov pg, pRegGov ptmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->as_StoreVector()->memory_size() < MaxVectorSize &&\n+            (n->in(3)->in(1)->bottom_type()->is_vect()->element_basic_type() == T_LONG ||\n+             n->in(3)->in(1)->bottom_type()->is_vect()->element_basic_type() == T_DOUBLE));\n+  match(Set mem (StoreVectorScatterMasked mem (Binary src (Binary idx pg))));\n+  effect(TEMP ptmp, KILL cr);\n+  ins_cost(4 * SVE_COST);\n+  format %{ \"store_vector_scatter $mem, $pg, $idx, $src\\t# vector store scatter predicated partial (D)\" %}\n+  ins_encode %{\n+    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), __ D,\n+                          Matcher::vector_length(this, $src));\n+    __ sve_and(as_PRegister($ptmp$$reg), as_PRegister($ptmp$$reg),\n+               as_PRegister($pg$$reg), as_PRegister($pg$$reg));\n+    __ sve_uunpklo(as_FloatRegister($idx$$reg), __ D, as_FloatRegister($idx$$reg));\n+    __ sve_st1d_scatter(as_FloatRegister($src$$reg), as_PRegister($ptmp$$reg),\n+                        as_Register($mem$$base), as_FloatRegister($idx$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n@@ -3814,2 +5555,1 @@\n-\n-instruct vmask_truecount(iRegINoSp dst, vReg src, pReg ptmp, rFlagsReg cr) %{\n+instruct vmask_truecount(iRegINoSp dst, pReg src) %{\n@@ -3819,2 +5559,1 @@\n-  effect(TEMP ptmp, KILL cr);\n-  ins_cost(2 * SVE_COST);\n+  ins_cost(SVE_COST);\n@@ -3823,2 +5562,3 @@\n-    __ sve_vmask_reduction(this->ideal_Opcode(), $dst$$Register, __ B,\n-                           as_FloatRegister($src$$reg), ptrue, as_PRegister($ptmp$$reg));\n+    BasicType bt = Matcher::vector_element_basic_type(this, $src);\n+    Assembler::SIMD_RegVariant size = __ elemType_to_regVariant(bt);\n+    __ sve_cntp($dst$$Register, size, ptrue, as_PRegister($src$$reg));\n@@ -3829,1 +5569,1 @@\n-instruct vmask_firsttrue(iRegINoSp dst, vReg src, pReg ptmp, rFlagsReg cr) %{\n+instruct vmask_firsttrue(iRegINoSp dst, pReg src, pReg ptmp) %{\n@@ -3833,2 +5573,2 @@\n-  effect(TEMP ptmp, KILL cr);\n-  ins_cost(3 * SVE_COST);\n+  effect(TEMP ptmp);\n+  ins_cost(2 * SVE_COST);\n@@ -3837,2 +5577,4 @@\n-    __ sve_vmask_reduction(this->ideal_Opcode(), $dst$$Register, __ B,\n-                           as_FloatRegister($src$$reg), ptrue, as_PRegister($ptmp$$reg));\n+    BasicType bt = Matcher::vector_element_basic_type(this, $src);\n+    Assembler::SIMD_RegVariant size = __ elemType_to_regVariant(bt);\n+    __ sve_brkb(as_PRegister($ptmp$$reg), ptrue, as_PRegister($src$$reg), false);\n+    __ sve_cntp($dst$$Register, size, ptrue, as_PRegister($ptmp$$reg));\n@@ -3843,1 +5585,1 @@\n-instruct vmask_lasttrue(iRegINoSp dst, vReg src, pReg ptmp, rFlagsReg cr) %{\n+instruct vmask_lasttrue(iRegINoSp dst, pReg src, pReg ptmp) %{\n@@ -3847,2 +5589,2 @@\n-  effect(TEMP ptmp, KILL cr);\n-  ins_cost(4 * SVE_COST);\n+  effect(TEMP ptmp);\n+  ins_cost(3 * SVE_COST);\n@@ -3851,2 +5593,2 @@\n-    __ sve_vmask_reduction(this->ideal_Opcode(), $dst$$Register, __ B,\n-                           as_FloatRegister($src$$reg), ptrue, as_PRegister($ptmp$$reg));\n+    BasicType bt = Matcher::vector_element_basic_type(this, $src);\n+    __ sve_vmask_lasttrue($dst$$Register, bt, as_PRegister($src$$reg), as_PRegister($ptmp$$reg));\n@@ -3857,1 +5599,1 @@\n-instruct vmask_truecount_partial(iRegINoSp dst, vReg src, pRegGov ptmp, rFlagsReg cr) %{\n+instruct vmask_truecount_partial(iRegINoSp dst, pReg src, pReg ptmp, rFlagsReg cr) %{\n@@ -3862,2 +5604,2 @@\n-  ins_cost(3 * SVE_COST);\n-  format %{ \"vmask_truecount $dst, $src\\t# vector mask truecount partial (sve)\" %}\n+  ins_cost(2 * SVE_COST);\n+  format %{ \"vmask_truecount_partial $dst, $src\\t# vector mask truecount partial (sve)\" %}\n@@ -3865,4 +5607,4 @@\n-    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), __ B,\n-                          Matcher::vector_length(this, $src));\n-    __ sve_vmask_reduction(this->ideal_Opcode(), $dst$$Register, __ B, as_FloatRegister($src$$reg),\n-                           as_PRegister($ptmp$$reg), as_PRegister($ptmp$$reg));\n+    BasicType bt = Matcher::vector_element_basic_type(this, $src);\n+    Assembler::SIMD_RegVariant size = __ elemType_to_regVariant(bt);\n+    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), size, Matcher::vector_length(this, $src));\n+    __ sve_cntp($dst$$Register, size, as_PRegister($ptmp$$reg), as_PRegister($src$$reg));\n@@ -3873,1 +5615,1 @@\n-instruct vmask_firsttrue_partial(iRegINoSp dst, vReg src, pRegGov pgtmp, pReg ptmp, rFlagsReg cr) %{\n+instruct vmask_firsttrue_partial(iRegINoSp dst, pReg src, pReg ptmp1, pReg ptmp2, rFlagsReg cr) %{\n@@ -3877,3 +5619,3 @@\n-  effect(TEMP pgtmp, TEMP ptmp, KILL cr);\n-  ins_cost(4 * SVE_COST);\n-  format %{ \"vmask_firsttrue $dst, $src\\t# vector mask firsttrue partial (sve)\" %}\n+  effect(TEMP ptmp1, TEMP ptmp2, KILL cr);\n+  ins_cost(3 * SVE_COST);\n+  format %{ \"vmask_firsttrue_partial $dst, $src\\t# vector mask firsttrue partial (sve)\" %}\n@@ -3881,1 +5623,3 @@\n-    __ sve_whilelo_zr_imm(as_PRegister($pgtmp$$reg), __ B,\n+    BasicType bt = Matcher::vector_element_basic_type(this, $src);\n+    Assembler::SIMD_RegVariant size = __ elemType_to_regVariant(bt);\n+    __ sve_whilelo_zr_imm(as_PRegister($ptmp1$$reg), size,\n@@ -3883,2 +5627,2 @@\n-    __ sve_vmask_reduction(this->ideal_Opcode(), $dst$$Register, __ B, as_FloatRegister($src$$reg),\n-                           as_PRegister($pgtmp$$reg), as_PRegister($ptmp$$reg));\n+    __ sve_brkb(as_PRegister($ptmp2$$reg), as_PRegister($ptmp1$$reg), as_PRegister($src$$reg), false);\n+    __ sve_cntp($dst$$Register, size, as_PRegister($ptmp1$$reg), as_PRegister($ptmp2$$reg));\n@@ -3889,1 +5633,1 @@\n-instruct vmask_lasttrue_partial(iRegINoSp dst, vReg src, pRegGov ptmp, rFlagsReg cr) %{\n+instruct vmask_lasttrue_partial(iRegINoSp dst, pReg src, pReg ptmp, rFlagsReg cr) %{\n@@ -3895,108 +5639,1 @@\n-  format %{ \"vmask_lasttrue $dst, $src\\t# vector mask lasttrue partial (sve)\" %}\n-  ins_encode %{\n-    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), __ B,\n-                          Matcher::vector_length(this, $src));\n-    __ sve_vmask_reduction(this->ideal_Opcode(), $dst$$Register, __ B, as_FloatRegister($src$$reg),\n-                           as_PRegister($ptmp$$reg), as_PRegister($ptmp$$reg));\n-  %}\n-  ins_pipe(pipe_slow);\n-%}\n-\n-\/\/ ----------------- Vector mask reductions combined with VectorMaskStore ---------------\n-\n-instruct vstoremask_truecount(iRegINoSp dst, vReg src, immI esize, pReg ptmp, rFlagsReg cr) %{\n-  predicate(UseSVE > 0 &&\n-            n->in(1)->in(1)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize);\n-  match(Set dst (VectorMaskTrueCount (VectorStoreMask src esize)));\n-  effect(TEMP ptmp, KILL cr);\n-  ins_cost(2 * SVE_COST);\n-  format %{ \"vstoremask_truecount $dst, $src\\t# vector mask truecount (sve)\" %}\n-  ins_encode %{\n-    unsigned size = $esize$$constant;\n-    assert(size == 1 || size == 2 || size == 4 || size == 8, \"unsupported element size\");\n-    Assembler::SIMD_RegVariant variant = __ elemBytes_to_regVariant(size);\n-    __ sve_vmask_reduction(this->ideal_Opcode(), $dst$$Register, variant, as_FloatRegister($src$$reg),\n-                           ptrue, as_PRegister($ptmp$$reg), Matcher::vector_length(this, $src));\n-  %}\n-  ins_pipe(pipe_slow);\n-%}\n-\n-instruct vstoremask_firsttrue(iRegINoSp dst, vReg src, immI esize, pReg ptmp, rFlagsReg cr) %{\n-  predicate(UseSVE > 0 &&\n-            n->in(1)->in(1)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize);\n-  match(Set dst (VectorMaskFirstTrue (VectorStoreMask src esize)));\n-  effect(TEMP ptmp, KILL cr);\n-  ins_cost(3 * SVE_COST);\n-  format %{ \"vstoremask_firsttrue $dst, $src\\t# vector mask firsttrue (sve)\" %}\n-  ins_encode %{\n-    unsigned size = $esize$$constant;\n-    assert(size == 1 || size == 2 || size == 4 || size == 8, \"unsupported element size\");\n-    Assembler::SIMD_RegVariant variant = __ elemBytes_to_regVariant(size);\n-    __ sve_vmask_reduction(this->ideal_Opcode(), $dst$$Register, variant, as_FloatRegister($src$$reg),\n-                           ptrue, as_PRegister($ptmp$$reg), Matcher::vector_length(this, $src));\n-  %}\n-  ins_pipe(pipe_slow);\n-%}\n-\n-instruct vstoremask_lasttrue(iRegINoSp dst, vReg src, immI esize, pReg ptmp, rFlagsReg cr) %{\n-  predicate(UseSVE > 0 &&\n-            n->in(1)->in(1)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize);\n-  match(Set dst (VectorMaskLastTrue (VectorStoreMask src esize)));\n-  effect(TEMP ptmp, KILL cr);\n-  ins_cost(4 * SVE_COST);\n-  format %{ \"vstoremask_lasttrue $dst, $src\\t# vector mask lasttrue (sve)\" %}\n-  ins_encode %{\n-    unsigned size = $esize$$constant;\n-    assert(size == 1 || size == 2 || size == 4 || size == 8, \"unsupported element size\");\n-    Assembler::SIMD_RegVariant variant = __ elemBytes_to_regVariant(size);\n-    __ sve_vmask_reduction(this->ideal_Opcode(), $dst$$Register, variant, as_FloatRegister($src$$reg),\n-                           ptrue, as_PRegister($ptmp$$reg), Matcher::vector_length(this, $src));\n-  %}\n-  ins_pipe(pipe_slow);\n-%}\n-\n-instruct vstoremask_truecount_partial(iRegINoSp dst, vReg src, immI esize, pRegGov ptmp, rFlagsReg cr) %{\n-  predicate(UseSVE > 0 &&\n-            n->in(1)->in(1)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize);\n-  match(Set dst (VectorMaskTrueCount (VectorStoreMask src esize)));\n-  effect(TEMP ptmp, KILL cr);\n-  ins_cost(3 * SVE_COST);\n-  format %{ \"vstoremask_truecount $dst, $src\\t# vector mask truecount partial (sve)\" %}\n-  ins_encode %{\n-    unsigned size = $esize$$constant;\n-    assert(size == 1 || size == 2 || size == 4 || size == 8, \"unsupported element size\");\n-    Assembler::SIMD_RegVariant variant = __ elemBytes_to_regVariant(size);\n-    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), variant,\n-                          Matcher::vector_length(this, $src));\n-    __ sve_vmask_reduction(this->ideal_Opcode(), $dst$$Register, variant, as_FloatRegister($src$$reg),\n-                           as_PRegister($ptmp$$reg), as_PRegister($ptmp$$reg), MaxVectorSize \/ size);\n-  %}\n-  ins_pipe(pipe_slow);\n-%}\n-\n-instruct vstoremask_firsttrue_partial(iRegINoSp dst, vReg src, immI esize, pRegGov pgtmp, pReg ptmp, rFlagsReg cr) %{\n-  predicate(UseSVE > 0 &&\n-            n->in(1)->in(1)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize);\n-  match(Set dst (VectorMaskFirstTrue (VectorStoreMask src esize)));\n-  effect(TEMP pgtmp, TEMP ptmp, KILL cr);\n-  ins_cost(4 * SVE_COST);\n-  format %{ \"vstoremask_firsttrue $dst, $src\\t# vector mask firsttrue partial (sve)\" %}\n-  ins_encode %{\n-    unsigned size = $esize$$constant;\n-    assert(size == 1 || size == 2 || size == 4 || size == 8, \"unsupported element size\");\n-    Assembler::SIMD_RegVariant variant = __ elemBytes_to_regVariant(size);\n-    __ sve_whilelo_zr_imm(as_PRegister($pgtmp$$reg), variant,\n-                          Matcher::vector_length(this, $src));\n-    __ sve_vmask_reduction(this->ideal_Opcode(), $dst$$Register, variant, as_FloatRegister($src$$reg),\n-                           as_PRegister($pgtmp$$reg), as_PRegister($ptmp$$reg), MaxVectorSize \/ size);\n-  %}\n-  ins_pipe(pipe_slow);\n-%}\n-\n-instruct vstoremask_lasttrue_partial(iRegINoSp dst, vReg src, immI esize, pRegGov ptmp, rFlagsReg cr) %{\n-  predicate(UseSVE > 0 &&\n-            n->in(1)->in(1)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize);\n-  match(Set dst (VectorMaskLastTrue (VectorStoreMask src esize)));\n-  effect(TEMP ptmp, KILL cr);\n-  ins_cost(5 * SVE_COST);\n-  format %{ \"vstoremask_lasttrue $dst, $src\\t# vector mask lasttrue partial (sve)\" %}\n+  format %{ \"vmask_lasttrue_partial $dst, $src\\t# vector mask lasttrue partial (sve)\" %}\n@@ -4004,7 +5641,5 @@\n-    unsigned size = $esize$$constant;\n-    assert(size == 1 || size == 2 || size == 4 || size == 8, \"unsupported element size\");\n-    Assembler::SIMD_RegVariant variant = __ elemBytes_to_regVariant(size);\n-    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), variant,\n-                          Matcher::vector_length(this, $src));\n-    __ sve_vmask_reduction(this->ideal_Opcode(), $dst$$Register, variant, as_FloatRegister($src$$reg),\n-                           as_PRegister($ptmp$$reg), as_PRegister($ptmp$$reg), MaxVectorSize \/ size);\n+    BasicType bt = Matcher::vector_element_basic_type(this, $src);\n+    Assembler::SIMD_RegVariant size = __ elemType_to_regVariant(bt);\n+    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), size, Matcher::vector_length(this, $src));\n+    __ sve_and(as_PRegister($ptmp$$reg), ptrue, as_PRegister($ptmp$$reg), as_PRegister($src$$reg));\n+    __ sve_vmask_lasttrue($dst$$Register, bt, as_PRegister($ptmp$$reg), as_PRegister($ptmp$$reg));\n@@ -4013,1 +5648,1 @@\n-%}\n+%}\n\\ No newline at end of file\n","filename":"src\/hotspot\/cpu\/aarch64\/aarch64_sve.ad","additions":3214,"deletions":1579,"binary":false,"changes":4793,"status":"modified"},{"patch":"@@ -86,0 +86,1 @@\n+  bool masked_op_sve_supported(int opcode, int vlen, BasicType bt);\n@@ -142,5 +143,1 @@\n-        if (vlen < 4 || length_in_bytes > MaxVectorSize) {\n-          return false;\n-        } else {\n-          return true;\n-        }\n+        return vlen >= 4 && length_in_bytes <= MaxVectorSize;\n@@ -156,0 +153,8 @@\n+\n+  bool masked_op_sve_supported(int opcode, int vlen, BasicType bt) {\n+    if (opcode == Op_VectorRearrange) {\n+      return false;\n+    }\n+    return op_sve_supported(opcode, vlen, bt);\n+  }\n+\n@@ -234,1 +239,1 @@\n-instruct loadV_partial(vReg dst, vmemA mem, pRegGov pTmp, rFlagsReg cr) %{\n+instruct loadV_partial(vReg dst, vmemA mem, pRegGov pgtmp, rFlagsReg cr) %{\n@@ -238,1 +243,1 @@\n-  effect(TEMP pTmp, KILL cr);\n+  effect(TEMP pgtmp, KILL cr);\n@@ -240,2 +245,2 @@\n-  format %{ \"sve_whilelo_zr_imm $pTmp, vector_length\\n\\t\"\n-            \"sve_ldr $dst, $pTmp, $mem\\t# load vector predicated\" %}\n+  format %{ \"sve_whilelo_zr_imm $pgtmp, vector_length\\n\\t\"\n+            \"sve_ldr $dst, $pgtmp, $mem\\t# load vector partial\" %}\n@@ -244,1 +249,1 @@\n-    __ sve_whilelo_zr_imm(as_PRegister($pTmp$$reg), __ elemType_to_regVariant(bt),\n+    __ sve_whilelo_zr_imm(as_PRegister($pgtmp$$reg), __ elemType_to_regVariant(bt),\n@@ -248,1 +253,1 @@\n-                          as_PRegister($pTmp$$reg), bt, bt, $mem->opcode(),\n+                          as_PRegister($pgtmp$$reg), bt, bt, $mem->opcode(),\n@@ -254,1 +259,1 @@\n-instruct storeV_partial(vReg src, vmemA mem, pRegGov pTmp, rFlagsReg cr) %{\n+instruct storeV_partial(vReg src, vmemA mem, pRegGov pgtmp, rFlagsReg cr) %{\n@@ -258,1 +263,1 @@\n-  effect(TEMP pTmp, KILL cr);\n+  effect(TEMP pgtmp, KILL cr);\n@@ -260,2 +265,2 @@\n-  format %{ \"sve_whilelo_zr_imm $pTmp, vector_length\\n\\t\"\n-            \"sve_str $src, $pTmp, $mem\\t# store vector predicated\" %}\n+  format %{ \"sve_whilelo_zr_imm $pgtmp, vector_length\\n\\t\"\n+            \"sve_str $src, $pgtmp, $mem\\t# store vector partial\" %}\n@@ -264,1 +269,1 @@\n-    __ sve_whilelo_zr_imm(as_PRegister($pTmp$$reg), __ elemType_to_regVariant(bt),\n+    __ sve_whilelo_zr_imm(as_PRegister($pgtmp$$reg), __ elemType_to_regVariant(bt),\n@@ -268,1 +273,1 @@\n-                          as_PRegister($pTmp$$reg), bt, bt, $mem->opcode(),\n+                          as_PRegister($pgtmp$$reg), bt, bt, $mem->opcode(),\n@@ -272,1 +277,141 @@\n-%}dnl\n+%}\n+\n+\/\/ vector load\/store - predicated\n+\n+instruct loadV_masked(vReg dst, vmemA mem, pRegGov pg) %{\n+  predicate(UseSVE > 0 &&\n+            n->as_LoadVector()->memory_size() == MaxVectorSize);\n+  match(Set dst (LoadVectorMasked mem pg));\n+  ins_cost(4 * SVE_COST);\n+  format %{ \"sve_ldr $dst, $pg, $mem\\t# load vector predicated (sve)\" %}\n+  ins_encode %{\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    loadStoreA_predicated(C2_MacroAssembler(&cbuf), false, as_FloatRegister($dst$$reg),\n+                          as_PRegister($pg$$reg), bt, bt, $mem->opcode(),\n+                          as_Register($mem$$base), $mem$$index, $mem$$scale, $mem$$disp);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct loadV_masked_partial(vReg dst, vmemA mem, pRegGov pg, pRegGov pgtmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->as_LoadVector()->memory_size() < MaxVectorSize);\n+  match(Set dst (LoadVectorMasked mem pg));\n+  effect(TEMP pgtmp, KILL cr);\n+  ins_cost(6 * SVE_COST);\n+  format %{ \"sve_ldr $dst, $pg, $mem\\t# load vector predicated partial (sve)\" %}\n+  ins_encode %{\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    __ sve_whilelo_zr_imm(as_PRegister($pgtmp$$reg), __ elemType_to_regVariant(bt),\n+                          Matcher::vector_length(this));\n+    __ sve_and(as_PRegister($pgtmp$$reg), as_PRegister($pgtmp$$reg),\n+               as_PRegister($pg$$reg), as_PRegister($pg$$reg));\n+    loadStoreA_predicated(C2_MacroAssembler(&cbuf), false, as_FloatRegister($dst$$reg),\n+                          as_PRegister($pgtmp$$reg), bt, bt, $mem->opcode(),\n+                          as_Register($mem$$base), $mem$$index, $mem$$scale, $mem$$disp);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct storeV_masked(vReg src, vmemA mem, pRegGov pg) %{\n+  predicate(UseSVE > 0 &&\n+            n->as_StoreVector()->memory_size() == MaxVectorSize);\n+  match(Set mem (StoreVectorMasked mem (Binary src pg)));\n+  ins_cost(4 * SVE_COST);\n+  format %{ \"sve_str $mem, $pg, $src\\t# store vector predicated (sve)\" %}\n+  ins_encode %{\n+    BasicType bt = Matcher::vector_element_basic_type(this, $src);\n+    loadStoreA_predicated(C2_MacroAssembler(&cbuf), true, as_FloatRegister($src$$reg),\n+                          as_PRegister($pg$$reg), bt, bt, $mem->opcode(),\n+                          as_Register($mem$$base), $mem$$index, $mem$$scale, $mem$$disp);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct storeV_masked_partial(vReg src, vmemA mem, pRegGov pg, pRegGov pgtmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->as_StoreVector()->memory_size() < MaxVectorSize);\n+  match(Set mem (StoreVectorMasked mem (Binary src pg)));\n+  effect(TEMP pgtmp, KILL cr);\n+  ins_cost(6 * SVE_COST);\n+  format %{ \"sve_str $mem, $pg, $src\\t# store vector predicated partial (sve)\" %}\n+  ins_encode %{\n+    BasicType bt = Matcher::vector_element_basic_type(this, $src);\n+    __ sve_whilelo_zr_imm(as_PRegister($pgtmp$$reg), __ elemType_to_regVariant(bt),\n+                          Matcher::vector_length(this, $src));\n+    __ sve_and(as_PRegister($pgtmp$$reg), as_PRegister($pgtmp$$reg),\n+               as_PRegister($pg$$reg), as_PRegister($pg$$reg));\n+    loadStoreA_predicated(C2_MacroAssembler(&cbuf), true, as_FloatRegister($src$$reg),\n+                          as_PRegister($pgtmp$$reg), bt, bt, $mem->opcode(),\n+                          as_Register($mem$$base), $mem$$index, $mem$$scale, $mem$$disp);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+dnl\n+dnl MASKALL_IMM($1,   $2  )\n+dnl MASKALL_IMM(type, size)\n+define(`MASKALL_IMM', `\n+instruct vmaskAll_imm$1(pRegGov dst, imm$1 src) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst (MaskAll src));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_ptrue\/sve_pfalse $dst\\t# mask all (sve) ($2)\" %}\n+  ins_encode %{\n+    ifelse($1, `I', int, long) con = (ifelse($1, `I', int, long))$src$$constant;\n+    if (con == 0) {\n+      __ sve_pfalse(as_PRegister($dst$$reg));\n+    } else {\n+      assert(con == -1, \"invalid constant value for mask\");\n+      BasicType bt = Matcher::vector_element_basic_type(this);\n+      __ sve_ptrue(as_PRegister($dst$$reg), __ elemType_to_regVariant(bt));\n+    }\n+  %}\n+  ins_pipe(pipe_slow);\n+%}')dnl\n+dnl\n+dnl MASKALL($1,   $2  )\n+dnl MASKALL(type, size)\n+define(`MASKALL', `\n+instruct vmaskAll$1(pRegGov dst, ifelse($1, `I', iRegIorL2I, iRegL) src, vReg tmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst (MaskAll src));\n+  effect(TEMP tmp, KILL cr);\n+  ins_cost(2 * SVE_COST);\n+  format %{ \"sve_dup $tmp, $src\\n\\t\"\n+            \"sve_cmpne $dst, $tmp, 0\\t# mask all (sve) ($2)\" %}\n+  ins_encode %{\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    Assembler::SIMD_RegVariant size = __ elemType_to_regVariant(bt);\n+    __ sve_dup(as_FloatRegister($tmp$$reg), size, as_Register($src$$reg));\n+    __ sve_cmp(Assembler::NE, as_PRegister($dst$$reg), size, ptrue, as_FloatRegister($tmp$$reg), 0);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}')dnl\n+dnl\n+\/\/ maskAll\n+MASKALL_IMM(I, B\/H\/S)\n+MASKALL(I, B\/H\/S)\n+MASKALL_IMM(L, D)\n+MASKALL(L, D)\n+\n+dnl\n+dnl MASK_LOGICAL_OP($1,        $2,      $3  )\n+dnl MASK_LOGICAL_OP(insn_name, op_name, insn)\n+define(`MASK_LOGICAL_OP', `\n+instruct vmask_$1(pRegGov pd, pRegGov pn, pRegGov pm) %{\n+  predicate(UseSVE > 0);\n+  match(Set pd ($2 pn pm));\n+  ins_cost(SVE_COST);\n+  format %{ \"$3 $pd, $pn, $pm\\t# predicate (sve)\" %}\n+  ins_encode %{\n+    __ $3(as_PRegister($pd$$reg), ptrue,\n+               as_PRegister($pn$$reg), as_PRegister($pm$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}')dnl\n+dnl\n+\/\/ mask logical and\/or\/xor\n+MASK_LOGICAL_OP(and, AndVMask, sve_and)\n+MASK_LOGICAL_OP(or, OrVMask, sve_orr)\n+MASK_LOGICAL_OP(xor, XorVMask, sve_eor)\n@@ -274,0 +419,19 @@\n+dnl\n+dnl MASK_LOGICAL_AND_NOT($1,   $2  )\n+dnl MASK_LOGICAL_AND_NOT(type, size)\n+define(`MASK_LOGICAL_AND_NOT', `\n+instruct vmask_and_not$1(pRegGov pd, pRegGov pn, pRegGov pm, imm$1_M1 m1) %{\n+  predicate(UseSVE > 0);\n+  match(Set pd (AndVMask pn (XorVMask pm (MaskAll m1))));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_bic $pd, $pn, $pm\\t# predciate (sve) ($2)\" %}\n+  ins_encode %{\n+    __ sve_bic(as_PRegister($pd$$reg), ptrue,\n+               as_PRegister($pn$$reg), as_PRegister($pm$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}')dnl\n+dnl\n+\/\/ mask logical and_not\n+MASK_LOGICAL_AND_NOT(I, B\/H\/S)\n+MASK_LOGICAL_AND_NOT(L, D)\n@@ -289,1 +453,1 @@\n-instruct reinterpretResize(vReg dst, vReg src, pRegGov pTmp, rFlagsReg cr) %{\n+instruct reinterpretResize(vReg dst, vReg src, pRegGov pgtmp, rFlagsReg cr) %{\n@@ -293,1 +457,1 @@\n-  effect(TEMP_DEF dst, TEMP pTmp, KILL cr);\n+  effect(TEMP_DEF dst, TEMP pgtmp, KILL cr);\n@@ -303,1 +467,1 @@\n-    __ sve_whilelo_zr_imm(as_PRegister($pTmp$$reg), __ B, length_in_bytes_resize);\n+    __ sve_whilelo_zr_imm(as_PRegister($pgtmp$$reg), __ B, length_in_bytes_resize);\n@@ -305,1 +469,1 @@\n-    __ sve_sel(as_FloatRegister($dst$$reg), __ B, as_PRegister($pTmp$$reg),\n+    __ sve_sel(as_FloatRegister($dst$$reg), __ B, as_PRegister($pgtmp$$reg),\n@@ -310,0 +474,34 @@\n+\n+\/\/ vector mask reinterpret\n+\n+instruct vmask_reinterpret_same_esize(pRegGov dst_src) %{\n+  predicate(UseSVE > 0 &&\n+            n->as_Vector()->length() == n->in(1)->bottom_type()->is_vect()->length() &&\n+            n->as_Vector()->length_in_bytes() == n->in(1)->bottom_type()->is_vect()->length_in_bytes());\n+  match(Set dst_src (VectorReinterpret dst_src));\n+  ins_cost(0);\n+  format %{ \"# vmask_reinterpret $dst_src\\t# do nothing\" %}\n+  ins_encode %{\n+    \/\/ empty\n+  %}\n+  ins_pipe(pipe_class_empty);\n+%}\n+\n+instruct vmask_reinterpret_diff_esize(pRegGov dst, pRegGov src, vReg tmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->as_Vector()->length() != n->in(1)->bottom_type()->is_vect()->length() &&\n+            n->as_Vector()->length_in_bytes() == n->in(1)->bottom_type()->is_vect()->length_in_bytes());\n+  match(Set dst (VectorReinterpret src));\n+  effect(TEMP tmp, KILL cr);\n+  ins_cost(2 * SVE_COST);\n+  format %{ \"# vmask_reinterpret $dst, $src\\t# vector (sve)\" %}\n+  ins_encode %{\n+    BasicType from_bt = Matcher::vector_element_basic_type(this, $src);\n+    Assembler::SIMD_RegVariant from_size = __ elemType_to_regVariant(from_bt);\n+    BasicType to_bt = Matcher::vector_element_basic_type(this);\n+    Assembler::SIMD_RegVariant to_size = __ elemType_to_regVariant(to_bt);\n+    __ sve_cpy(as_FloatRegister($tmp$$reg), from_size, as_PRegister($src$$reg), -1, false);\n+    __ sve_cmp(Assembler::EQ, as_PRegister($dst$$reg), to_size, ptrue, as_FloatRegister($tmp$$reg), -1);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n@@ -311,3 +509,3 @@\n-dnl UNARY_OP_TRUE_PREDICATE_ETYPE($1,        $2,      $3,           $4,   $5,          %6  )\n-dnl UNARY_OP_TRUE_PREDICATE_ETYPE(insn_name, op_name, element_type, size, min_vec_len, insn)\n-define(`UNARY_OP_TRUE_PREDICATE_ETYPE', `\n+dnl UNARY_OP_TRUE_PREDICATE($1,        $2,      $3,   $4  )\n+dnl UNARY_OP_TRUE_PREDICATE(insn_name, op_name, size, insn)\n+define(`UNARY_OP_TRUE_PREDICATE', `\n@@ -316,1 +514,1 @@\n-            n->bottom_type()->is_vect()->element_basic_type() == $3);\n+            !n->as_Vector()->is_predicated_vector());\n@@ -319,1 +517,1 @@\n-  format %{ \"$6 $dst, $src\\t# vector (sve) ($4)\" %}\n+  format %{ \"$4 $dst, $src\\t# vector (sve) ($3)\" %}\n@@ -321,1 +519,1 @@\n-    __ $6(as_FloatRegister($dst$$reg), __ $4,\n+    __ $4(as_FloatRegister($dst$$reg), __ $3,\n@@ -329,10 +527,34 @@\n-UNARY_OP_TRUE_PREDICATE_ETYPE(vabsB, AbsVB, T_BYTE,   B, 16, sve_abs)\n-UNARY_OP_TRUE_PREDICATE_ETYPE(vabsS, AbsVS, T_SHORT,  H, 8,  sve_abs)\n-UNARY_OP_TRUE_PREDICATE_ETYPE(vabsI, AbsVI, T_INT,    S, 4,  sve_abs)\n-UNARY_OP_TRUE_PREDICATE_ETYPE(vabsL, AbsVL, T_LONG,   D, 2,  sve_abs)\n-UNARY_OP_TRUE_PREDICATE_ETYPE(vabsF, AbsVF, T_FLOAT,  S, 4,  sve_fabs)\n-UNARY_OP_TRUE_PREDICATE_ETYPE(vabsD, AbsVD, T_DOUBLE, D, 2,  sve_fabs)\n-dnl\n-dnl BINARY_OP_UNPREDICATED($1,        $2       $3,   $4           $5  )\n-dnl BINARY_OP_UNPREDICATED(insn_name, op_name, size, min_vec_len, insn)\n-define(`BINARY_OP_UNPREDICATED', `\n+UNARY_OP_TRUE_PREDICATE(vabsB, AbsVB, B, sve_abs)\n+UNARY_OP_TRUE_PREDICATE(vabsS, AbsVS, H, sve_abs)\n+UNARY_OP_TRUE_PREDICATE(vabsI, AbsVI, S, sve_abs)\n+UNARY_OP_TRUE_PREDICATE(vabsL, AbsVL, D, sve_abs)\n+UNARY_OP_TRUE_PREDICATE(vabsF, AbsVF, S, sve_fabs)\n+UNARY_OP_TRUE_PREDICATE(vabsD, AbsVD, D, sve_fabs)\n+\n+dnl UNARY_OP_PREDICATE($1,        $2,      $3,   $4  )\n+dnl UNARY_OP_PREDICATE(insn_name, op_name, size, insn)\n+define(`UNARY_OP_PREDICATE', `\n+instruct $1_masked(vReg dst_src, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src ($2 dst_src pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"$4 $dst_src, $pg, $dst_src\\t# vector (sve) ($3)\" %}\n+  ins_encode %{\n+    __ $4(as_FloatRegister($dst_src$$reg), __ $3,\n+            as_PRegister($pg$$reg),\n+            as_FloatRegister($dst_src$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}')dnl\n+\/\/ vector abs - predicated\n+UNARY_OP_PREDICATE(vabsB, AbsVB, B, sve_abs)\n+UNARY_OP_PREDICATE(vabsS, AbsVS, H, sve_abs)\n+UNARY_OP_PREDICATE(vabsI, AbsVI, S, sve_abs)\n+UNARY_OP_PREDICATE(vabsL, AbsVL, D, sve_abs)\n+UNARY_OP_PREDICATE(vabsF, AbsVF, S, sve_fabs)\n+UNARY_OP_PREDICATE(vabsD, AbsVD, D, sve_fabs)\n+\n+dnl\n+dnl BINARY_OP_UNPREDICATE($1,        $2       $3,   $4           $5  )\n+dnl BINARY_OP_UNPREDICATE(insn_name, op_name, size, min_vec_len, insn)\n+define(`BINARY_OP_UNPREDICATE', `\n@@ -351,1 +573,18 @@\n-\n+dnl\n+dnl\n+dnl BINARY_OP_PREDICATE($1,        $2,      $3,   $4  )\n+dnl BINARY_OP_PREDICATE(insn_name, op_name, size, insn)\n+define(`BINARY_OP_PREDICATE', `\n+instruct $1_masked(vReg dst_src1, vReg src2, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src1 ($2 (Binary dst_src1 src2) pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"$4 $dst_src1, $pg, $dst_src1, $src2\\t# vector (sve) ($3)\" %}\n+  ins_encode %{\n+    __ $4(as_FloatRegister($dst_src1$$reg), __ $3,\n+            as_PRegister($pg$$reg),\n+            as_FloatRegister($src2$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}')dnl\n+dnl\n@@ -353,9 +592,72 @@\n-BINARY_OP_UNPREDICATED(vaddB, AddVB, B, 16, sve_add)\n-BINARY_OP_UNPREDICATED(vaddS, AddVS, H, 8,  sve_add)\n-BINARY_OP_UNPREDICATED(vaddI, AddVI, S, 4,  sve_add)\n-BINARY_OP_UNPREDICATED(vaddL, AddVL, D, 2,  sve_add)\n-BINARY_OP_UNPREDICATED(vaddF, AddVF, S, 4,  sve_fadd)\n-BINARY_OP_UNPREDICATED(vaddD, AddVD, D, 2,  sve_fadd)\n-dnl\n-dnl BINARY_OP_UNSIZED($1,        $2,      $3,          $4  )\n-dnl BINARY_OP_UNSIZED(insn_name, op_name, min_vec_len, insn)\n+BINARY_OP_UNPREDICATE(vaddB, AddVB, B, 16, sve_add)\n+BINARY_OP_UNPREDICATE(vaddS, AddVS, H, 8,  sve_add)\n+BINARY_OP_UNPREDICATE(vaddI, AddVI, S, 4,  sve_add)\n+BINARY_OP_UNPREDICATE(vaddL, AddVL, D, 2,  sve_add)\n+BINARY_OP_UNPREDICATE(vaddF, AddVF, S, 4,  sve_fadd)\n+BINARY_OP_UNPREDICATE(vaddD, AddVD, D, 2,  sve_fadd)\n+\n+\/\/ vector add - predicated\n+BINARY_OP_PREDICATE(vaddB, AddVB, B, sve_add)\n+BINARY_OP_PREDICATE(vaddS, AddVS, H, sve_add)\n+BINARY_OP_PREDICATE(vaddI, AddVI, S, sve_add)\n+BINARY_OP_PREDICATE(vaddL, AddVL, D, sve_add)\n+BINARY_OP_PREDICATE(vaddF, AddVF, S, sve_fadd)\n+BINARY_OP_PREDICATE(vaddD, AddVD, D, sve_fadd)\n+dnl\n+dnl ADD_IMM($1,          $2,   $3      )\n+dnl ADD_IMM(name_suffix, size, imm_type)\n+define(`ADD_IMM', `\n+instruct vaddImm$1(vReg dst_src, $3 con) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src (AddV$1 dst_src (Replicate$1 con)));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_add $dst_src, $dst_src, $con\\t # vector (sve) ($2)\" %}\n+  ins_encode %{\n+    int32_t val = $con$$constant;\n+    if (val > 0){\n+      __ sve_add(as_FloatRegister($dst_src$$reg), __ $2, val);\n+    } else if (val < 0){\n+      __ sve_sub(as_FloatRegister($dst_src$$reg), __ $2, -val);\n+    }\n+  %}\n+  ins_pipe(pipe_slow);\n+%}')dnl\n+\n+\/\/ vector add reg imm (unpredicated)\n+ADD_IMM(B, B, immBAddSubV)\n+ADD_IMM(S, H, immIAddSubV)\n+ADD_IMM(I, S, immIAddSubV)\n+ADD_IMM(L, D, immLAddSubV)\n+dnl\n+dnl BITWISE_OP_IMM($1,        $2        $3,   $4    $5      )\n+dnl BITWISE_OP_IMM(insn_name, op_name1, size, type, op_name2)\n+define(`BITWISE_OP_IMM', `\n+instruct $1(vReg dst_src, imm$4Log con) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src ($2 dst_src (Replicate$4 con)));\n+  ins_cost(SVE_COST);\n+  format %{ \"$5 $dst_src, $dst_src, $con\\t # vector (sve) ($3)\" %}\n+  ins_encode %{\n+    __ $5(as_FloatRegister($dst_src$$reg), __ $3,\n+         (uint64_t)($con$$constant));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}')dnl\n+\n+\/\/ vector binary op reg imm (unpredicated)\n+BITWISE_OP_IMM(vandB, AndV, B, B, sve_and)\n+BITWISE_OP_IMM(vandH, AndV, H, S, sve_and)\n+BITWISE_OP_IMM(vandS, AndV, S, I, sve_and)\n+BITWISE_OP_IMM(vandD, AndV, D, L, sve_and)\n+BITWISE_OP_IMM(vorB,  OrV,  B, B, sve_orr)\n+BITWISE_OP_IMM(vorH,  OrV,  H, S, sve_orr)\n+BITWISE_OP_IMM(vorS,  OrV,  S, I, sve_orr)\n+BITWISE_OP_IMM(vorD,  OrV,  D, L, sve_orr)\n+BITWISE_OP_IMM(vxorB, XorV, B, B, sve_eor)\n+BITWISE_OP_IMM(vxorH, XorV, H, S, sve_eor)\n+BITWISE_OP_IMM(vxorS, XorV, S, I, sve_eor)\n+BITWISE_OP_IMM(vxorD, XorV, D, L, sve_eor)\n+dnl\n+dnl\n+dnl BINARY_OP_UNSIZED($1,        $2,      $3  )\n+dnl BINARY_OP_UNSIZED(insn_name, op_name, insn)\n@@ -367,1 +669,1 @@\n-  format %{ \"$4  $dst, $src1, $src2\\t# vector (sve)\" %}\n+  format %{ \"$3  $dst, $src1, $src2\\t# vector (sve)\" %}\n@@ -369,1 +671,1 @@\n-    __ $4(as_FloatRegister($dst$$reg),\n+    __ $3(as_FloatRegister($dst$$reg),\n@@ -375,1 +677,1 @@\n-\n+dnl\n@@ -377,1 +679,1 @@\n-BINARY_OP_UNSIZED(vand, AndV, 16, sve_and)\n+BINARY_OP_UNSIZED(vand, AndV, sve_and)\n@@ -380,1 +682,1 @@\n-BINARY_OP_UNSIZED(vor, OrV, 16, sve_orr)\n+BINARY_OP_UNSIZED(vor, OrV, sve_orr)\n@@ -383,1 +685,28 @@\n-BINARY_OP_UNSIZED(vxor, XorV, 16, sve_eor)\n+BINARY_OP_UNSIZED(vxor, XorV, sve_eor)\n+\n+dnl BINARY_LOGIC_OP_PREDICATE($1,        $2,      $3  )\n+dnl BINARY_LOGIC_OP_PREDICATE(insn_name, op_name, insn)\n+define(`BINARY_LOGIC_OP_PREDICATE', `\n+instruct $1_masked(vReg dst_src1, vReg src2, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src1 ($2 (Binary dst_src1 src2) pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"$3 $dst_src1, $pg, $dst_src1, $src2\\t # vector (sve)\" %}\n+  ins_encode %{\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    Assembler::SIMD_RegVariant size = __ elemType_to_regVariant(bt);\n+    __ $3(as_FloatRegister($dst_src1$$reg), size,\n+          as_PRegister($pg$$reg),\n+          as_FloatRegister($src2$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}')dnl\n+dnl\n+\/\/ vector and - predicated\n+BINARY_LOGIC_OP_PREDICATE(vand, AndV, sve_and)\n+\n+\/\/ vector or - predicated\n+BINARY_LOGIC_OP_PREDICATE(vor, OrV, sve_orr)\n+\n+\/\/ vector xor - predicated\n+BINARY_LOGIC_OP_PREDICATE(vxor, XorV, sve_eor)\n@@ -409,1 +738,1 @@\n-\n+dnl\n@@ -450,1 +779,1 @@\n-\n+dnl\n@@ -455,1 +784,3 @@\n-\/\/ vector min\/max\n+\/\/ vector float div - predicated\n+BINARY_OP_PREDICATE(vfdivF, DivVF, S, sve_fdiv)\n+BINARY_OP_PREDICATE(vfdivD, DivVD, D, sve_fdiv)\n@@ -457,1 +788,5 @@\n-instruct vmin(vReg dst_src1, vReg src2) %{\n+dnl\n+dnl VMINMAX($1     , $2, $3   , $4  )\n+dnl VMINMAX(op_name, op, finsn, insn)\n+define(`VMINMAX', `\n+instruct v$1(vReg dst_src1, vReg src2) %{\n@@ -459,1 +794,1 @@\n-  match(Set dst_src1 (MinV dst_src1 src2));\n+  match(Set dst_src1 ($2 dst_src1 src2));\n@@ -461,1 +796,1 @@\n-  format %{ \"sve_min $dst_src1, $dst_src1, $src2\\t # vector (sve)\" %}\n+  format %{ \"sve_$1 $dst_src1, $dst_src1, $src2\\t # vector (sve)\" %}\n@@ -466,1 +801,1 @@\n-      __ sve_fmin(as_FloatRegister($dst_src1$$reg), size,\n+      __ $3(as_FloatRegister($dst_src1$$reg), size,\n@@ -469,2 +804,2 @@\n-      assert(is_integral_type(bt), \"Unsupported type\");\n-      __ sve_smin(as_FloatRegister($dst_src1$$reg), size,\n+      assert(is_integral_type(bt), \"unsupported type\");\n+      __ $4(as_FloatRegister($dst_src1$$reg), size,\n@@ -475,1 +810,5 @@\n-%}\n+%}')dnl\n+dnl\n+\/\/ vector min\/max\n+VMINMAX(min, MinV, sve_fmin, sve_smin)\n+VMINMAX(max, MaxV, sve_fmax, sve_smax)\n@@ -477,1 +816,5 @@\n-instruct vmax(vReg dst_src1, vReg src2) %{\n+dnl\n+dnl VMINMAX_PREDICATE($1     , $2, $3   , $4  )\n+dnl VMINMAX_PREDICATE(op_name, op, finsn, insn)\n+define(`VMINMAX_PREDICATE', `\n+instruct v$1_masked(vReg dst_src1, vReg src2, pRegGov pg) %{\n@@ -479,1 +822,1 @@\n-  match(Set dst_src1 (MaxV dst_src1 src2));\n+  match(Set dst_src1 ($2 (Binary dst_src1 src2) pg));\n@@ -481,1 +824,1 @@\n-  format %{ \"sve_max $dst_src1, $dst_src1, $src2\\t # vector (sve)\" %}\n+  format %{ \"sve_$1 $dst_src1, $pg, $dst_src1, $src2\\t# vector (sve)\" %}\n@@ -486,2 +829,2 @@\n-      __ sve_fmax(as_FloatRegister($dst_src1$$reg), size,\n-                  ptrue, as_FloatRegister($src2$$reg));\n+      __ $3(as_FloatRegister($dst_src1$$reg), size,\n+                  as_PRegister($pg$$reg), as_FloatRegister($src2$$reg));\n@@ -489,3 +832,3 @@\n-      assert(is_integral_type(bt), \"Unsupported type\");\n-      __ sve_smax(as_FloatRegister($dst_src1$$reg), size,\n-                  ptrue, as_FloatRegister($src2$$reg));\n+      assert(is_integral_type(bt), \"unsupported type\");\n+      __ $4(as_FloatRegister($dst_src1$$reg), size,\n+                  as_PRegister($pg$$reg), as_FloatRegister($src2$$reg));\n@@ -495,1 +838,5 @@\n-%}\n+%}')dnl\n+dnl\n+\/\/ vector min\/max - predicated\n+VMINMAX_PREDICATE(min, MinV, sve_fmin, sve_smin)\n+VMINMAX_PREDICATE(max, MaxV, sve_fmax, sve_smax)\n@@ -518,0 +865,21 @@\n+dnl\n+dnl VFMLA_PREDICATE($1,   $2  )\n+dnl VFMLA_PREDICATE(type, size)\n+define(`VFMLA_PREDICATE', `\n+\/\/ dst_src1 = dst_src1 * src2 + src3\n+instruct vfmla$1_masked(vReg dst_src1, vReg src2, vReg src3, pRegGov pg) %{\n+  predicate(UseFMA && UseSVE > 0);\n+  match(Set dst_src1 (FmaV$1 (Binary dst_src1 src2) (Binary src3 pg)));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_fmad $dst_src1, $pg, $src2, $src3\\t# vector (sve) ($2)\" %}\n+  ins_encode %{\n+    __ sve_fmad(as_FloatRegister($dst_src1$$reg), __ $2, as_PRegister($pg$$reg),\n+         as_FloatRegister($src2$$reg), as_FloatRegister($src3$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}')dnl\n+dnl\n+\/\/ vector fmla - predicated\n+VFMLA_PREDICATE(F, S)\n+VFMLA_PREDICATE(D, D)\n+\n@@ -648,1 +1016,1 @@\n-\n+dnl\n@@ -654,2 +1022,10 @@\n-BINARY_OP_UNPREDICATED(vmulF, MulVF, S, 4, sve_fmul)\n-BINARY_OP_UNPREDICATED(vmulD, MulVD, D, 2, sve_fmul)\n+BINARY_OP_UNPREDICATE(vmulF, MulVF, S, 4, sve_fmul)\n+BINARY_OP_UNPREDICATE(vmulD, MulVD, D, 2, sve_fmul)\n+\n+\/\/ vector mul - predicated\n+BINARY_OP_PREDICATE(vmulB, MulVB, B, sve_mul)\n+BINARY_OP_PREDICATE(vmulS, MulVS, H, sve_mul)\n+BINARY_OP_PREDICATE(vmulI, MulVI, S, sve_mul)\n+BINARY_OP_PREDICATE(vmulL, MulVL, D, sve_mul)\n+BINARY_OP_PREDICATE(vmulF, MulVF, S, sve_fmul)\n+BINARY_OP_PREDICATE(vmulD, MulVD, D, sve_fmul)\n@@ -657,16 +1033,0 @@\n-dnl\n-dnl UNARY_OP_TRUE_PREDICATE($1,        $2,      $3,   $4,            $5  )\n-dnl UNARY_OP_TRUE_PREDICATE(insn_name, op_name, size, min_vec_bytes, insn)\n-define(`UNARY_OP_TRUE_PREDICATE', `\n-instruct $1(vReg dst, vReg src) %{\n-  predicate(UseSVE > 0);\n-  match(Set dst ($2 src));\n-  ins_cost(SVE_COST);\n-  format %{ \"$5 $dst, $src\\t# vector (sve) ($3)\" %}\n-  ins_encode %{\n-    __ $5(as_FloatRegister($dst$$reg), __ $3,\n-         ptrue, as_FloatRegister($src$$reg));\n-  %}\n-  ins_pipe(pipe_slow);\n-%}')dnl\n-dnl\n@@ -674,2 +1034,6 @@\n-UNARY_OP_TRUE_PREDICATE(vnegF, NegVF, S, 16, sve_fneg)\n-UNARY_OP_TRUE_PREDICATE(vnegD, NegVD, D, 16, sve_fneg)\n+UNARY_OP_TRUE_PREDICATE(vnegF, NegVF, S, sve_fneg)\n+UNARY_OP_TRUE_PREDICATE(vnegD, NegVD, D, sve_fneg)\n+\n+\/\/ vector fneg - predicated\n+UNARY_OP_PREDICATE(vnegF, NegVF, S, sve_fneg)\n+UNARY_OP_PREDICATE(vnegD, NegVD, D, sve_fneg)\n@@ -691,1 +1055,1 @@\n-instruct vmaskcmp(vReg dst, vReg src1, vReg src2, immI cond, pRegGov pTmp, rFlagsReg cr) %{\n+instruct vmaskcmp(pRegGov dst, vReg src1, vReg src2, immI cond, rFlagsReg cr) %{\n@@ -694,4 +1058,3 @@\n-  effect(TEMP pTmp, KILL cr);\n-  ins_cost(2 * SVE_COST);\n-  format %{ \"sve_cmp $pTmp, $src1, $src2\\n\\t\"\n-            \"sve_cpy $dst, $pTmp, -1\\t# vector mask cmp (sve)\" %}\n+  effect(KILL cr);\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_cmp $dst, $src1, $src2\\t# vector mask cmp (sve)\" %}\n@@ -700,1 +1063,1 @@\n-    __ sve_compare(as_PRegister($pTmp$$reg), bt, ptrue, as_FloatRegister($src1$$reg),\n+    __ sve_compare(as_PRegister($dst$$reg), bt, ptrue, as_FloatRegister($src1$$reg),\n@@ -702,2 +1065,0 @@\n-    __ sve_cpy(as_FloatRegister($dst$$reg), __ elemType_to_regVariant(bt),\n-               as_PRegister($pTmp$$reg), -1, false);\n@@ -708,3 +1069,1 @@\n-\/\/ vector blend\n-\n-instruct vblend(vReg dst, vReg src1, vReg src2, vReg src3, pRegGov pTmp, rFlagsReg cr) %{\n+instruct vmaskcmp_masked(pRegGov dst, vReg src1, vReg src2, immI cond, pRegGov pg, rFlagsReg cr) %{\n@@ -712,5 +1071,4 @@\n-  match(Set dst (VectorBlend (Binary src1 src2) src3));\n-  effect(TEMP pTmp, KILL cr);\n-  ins_cost(2 * SVE_COST);\n-  format %{ \"sve_cmpeq $pTmp, $src3, -1\\n\\t\"\n-            \"sve_sel $dst, $pTmp, $src2, $src1\\t# vector blend (sve)\" %}\n+  match(Set dst (VectorMaskCmp (Binary src1 src2) (Binary cond pg)));\n+  effect(KILL cr);\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_cmp $dst, $pg, $src1, $src2\\t# vector mask cmp (sve)\" %}\n@@ -718,6 +1076,3 @@\n-    Assembler::SIMD_RegVariant size =\n-      __ elemType_to_regVariant(Matcher::vector_element_basic_type(this));\n-    __ sve_cmp(Assembler::EQ, as_PRegister($pTmp$$reg), size,\n-               ptrue, as_FloatRegister($src3$$reg), -1);\n-    __ sve_sel(as_FloatRegister($dst$$reg), size, as_PRegister($pTmp$$reg),\n-               as_FloatRegister($src2$$reg), as_FloatRegister($src1$$reg));\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    __ sve_compare(as_PRegister($dst$$reg), bt, as_PRegister($pg$$reg), as_FloatRegister($src1$$reg),\n+                   as_FloatRegister($src2$$reg), (int)$cond$$constant);\n@@ -728,1 +1083,1 @@\n-\/\/ vector blend with compare\n+\/\/ vector blend\n@@ -730,2 +1085,1 @@\n-instruct vblend_maskcmp(vReg dst, vReg src1, vReg src2, vReg src3,\n-                        vReg src4, pRegGov pTmp, immI cond, rFlagsReg cr) %{\n+instruct vblend(vReg dst, vReg src1, vReg src2, pRegGov pg) %{\n@@ -733,5 +1087,3 @@\n-  match(Set dst (VectorBlend (Binary src1 src2) (VectorMaskCmp (Binary src3 src4) cond)));\n-  effect(TEMP pTmp, KILL cr);\n-  ins_cost(2 * SVE_COST);\n-  format %{ \"sve_cmp $pTmp, $src3, $src4\\t# vector cmp (sve)\\n\\t\"\n-            \"sve_sel $dst, $pTmp, $src2, $src1\\t# vector blend (sve)\" %}\n+  match(Set dst (VectorBlend (Binary src1 src2) pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_sel $dst, $pg, $src2, $src1\\t# vector blend (sve)\" %}\n@@ -739,6 +1091,4 @@\n-    BasicType bt = Matcher::vector_element_basic_type(this);\n-    __ sve_compare(as_PRegister($pTmp$$reg), bt, ptrue, as_FloatRegister($src3$$reg),\n-                   as_FloatRegister($src4$$reg), (int)$cond$$constant);\n-    __ sve_sel(as_FloatRegister($dst$$reg), __ elemType_to_regVariant(bt),\n-               as_PRegister($pTmp$$reg), as_FloatRegister($src2$$reg),\n-               as_FloatRegister($src1$$reg));\n+    Assembler::SIMD_RegVariant size =\n+               __ elemType_to_regVariant(Matcher::vector_element_basic_type(this));\n+    __ sve_sel(as_FloatRegister($dst$$reg), size, as_PRegister($pg$$reg),\n+               as_FloatRegister($src2$$reg), as_FloatRegister($src1$$reg));\n@@ -751,1 +1101,1 @@\n-instruct vloadmaskB(vReg dst, vReg src) %{\n+instruct vloadmaskB(pRegGov dst, vReg src, rFlagsReg cr) %{\n@@ -755,0 +1105,1 @@\n+  effect(KILL cr);\n@@ -756,14 +1107,1 @@\n-  format %{ \"sve_neg $dst, $src\\t# vector load mask (B)\" %}\n-  ins_encode %{\n-    __ sve_neg(as_FloatRegister($dst$$reg), __ B, ptrue, as_FloatRegister($src$$reg));\n-  %}\n-  ins_pipe(pipe_slow);\n-%}\n-\n-instruct vloadmaskS(vReg dst, vReg src) %{\n-  predicate(UseSVE > 0 &&\n-            n->bottom_type()->is_vect()->element_basic_type() == T_SHORT);\n-  match(Set dst (VectorLoadMask src));\n-  ins_cost(2 * SVE_COST);\n-  format %{ \"sve_uunpklo $dst, H, $src\\n\\t\"\n-            \"sve_neg $dst, $dst\\t# vector load mask (B to H)\" %}\n+  format %{ \"vloadmaskB $dst, $src\\t# vector load mask (sve) (B)\" %}\n@@ -771,2 +1109,2 @@\n-    __ sve_uunpklo(as_FloatRegister($dst$$reg), __ H, as_FloatRegister($src$$reg));\n-    __ sve_neg(as_FloatRegister($dst$$reg), __ H, ptrue, as_FloatRegister($dst$$reg));\n+    __ sve_cmp(Assembler::NE, as_PRegister($dst$$reg), __ B,\n+               ptrue, as_FloatRegister($src$$reg), 0);\n@@ -777,4 +1115,2 @@\n-instruct vloadmaskI(vReg dst, vReg src) %{\n-  predicate(UseSVE > 0 &&\n-            (n->bottom_type()->is_vect()->element_basic_type() == T_INT ||\n-             n->bottom_type()->is_vect()->element_basic_type() == T_FLOAT));\n+instruct vloadmask_extend(pRegGov dst, vReg src, vReg tmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 && n->bottom_type()->is_vect()->element_basic_type() != T_BYTE);\n@@ -782,0 +1118,1 @@\n+  effect(TEMP tmp, KILL cr);\n@@ -783,21 +1120,1 @@\n-  format %{ \"sve_uunpklo $dst, H, $src\\n\\t\"\n-            \"sve_uunpklo $dst, S, $dst\\n\\t\"\n-            \"sve_neg $dst, $dst\\t# vector load mask (B to S)\" %}\n-  ins_encode %{\n-    __ sve_uunpklo(as_FloatRegister($dst$$reg), __ H, as_FloatRegister($src$$reg));\n-    __ sve_uunpklo(as_FloatRegister($dst$$reg), __ S, as_FloatRegister($dst$$reg));\n-    __ sve_neg(as_FloatRegister($dst$$reg), __ S, ptrue, as_FloatRegister($dst$$reg));\n-  %}\n-  ins_pipe(pipe_slow);\n-%}\n-\n-instruct vloadmaskL(vReg dst, vReg src) %{\n-  predicate(UseSVE > 0 &&\n-            (n->bottom_type()->is_vect()->element_basic_type() == T_LONG ||\n-             n->bottom_type()->is_vect()->element_basic_type() == T_DOUBLE));\n-  match(Set dst (VectorLoadMask src));\n-  ins_cost(4 * SVE_COST);\n-  format %{ \"sve_uunpklo $dst, H, $src\\n\\t\"\n-            \"sve_uunpklo $dst, S, $dst\\n\\t\"\n-            \"sve_uunpklo $dst, D, $dst\\n\\t\"\n-            \"sve_neg $dst, $dst\\t# vector load mask (B to D)\" %}\n+  format %{ \"vloadmask $dst, $src\\t# vector load mask (sve) (H\/S\/D)\" %}\n@@ -805,4 +1122,4 @@\n-    __ sve_uunpklo(as_FloatRegister($dst$$reg), __ H, as_FloatRegister($src$$reg));\n-    __ sve_uunpklo(as_FloatRegister($dst$$reg), __ S, as_FloatRegister($dst$$reg));\n-    __ sve_uunpklo(as_FloatRegister($dst$$reg), __ D, as_FloatRegister($dst$$reg));\n-    __ sve_neg(as_FloatRegister($dst$$reg), __ D, ptrue, as_FloatRegister($dst$$reg));\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    Assembler::SIMD_RegVariant size = __ elemType_to_regVariant(bt);\n+    __ sve_vector_extend(as_FloatRegister($tmp$$reg), size, as_FloatRegister($src$$reg), __ B);\n+    __ sve_cmp(Assembler::NE, as_PRegister($dst$$reg), size, ptrue, as_FloatRegister($tmp$$reg), 0);\n@@ -815,1 +1132,1 @@\n-instruct vstoremaskB(vReg dst, vReg src, immI_1 size) %{\n+instruct vstoremaskB(vReg dst, pRegGov src, immI_1 size) %{\n@@ -819,1 +1136,1 @@\n-  format %{ \"sve_neg $dst, $src\\t# vector store mask (B)\" %}\n+  format %{ \"vstoremask $dst, $src\\t# vector store mask (sve) (B)\" %}\n@@ -821,2 +1138,1 @@\n-    __ sve_neg(as_FloatRegister($dst$$reg), __ B, ptrue,\n-               as_FloatRegister($src$$reg));\n+    __ sve_cpy(as_FloatRegister($dst$$reg), __ B, as_PRegister($src$$reg), 1, false);\n@@ -827,1 +1143,1 @@\n-instruct vstoremaskS(vReg dst, vReg src, vReg tmp, immI_2 size) %{\n+instruct vstoremask_narrow(vReg dst, pRegGov src, vReg tmp, immI_gt_1 size) %{\n@@ -832,3 +1148,1 @@\n-  format %{ \"sve_dup $tmp, H, 0\\n\\t\"\n-            \"sve_uzp1 $dst, B, $src, $tmp\\n\\t\"\n-            \"sve_neg $dst, B, $dst\\t# vector store mask (sve) (H to B)\" %}\n+  format %{ \"vstoremask $dst, $src\\t# vector store mask (sve) (H\/S\/D)\" %}\n@@ -836,6 +1150,4 @@\n-    __ sve_dup(as_FloatRegister($tmp$$reg), __ H, 0);\n-    __ sve_uzp1(as_FloatRegister($dst$$reg), __ B,\n-                as_FloatRegister($src$$reg), as_FloatRegister($tmp$$reg));\n-    __ sve_neg(as_FloatRegister($dst$$reg), __ B, ptrue,\n-               as_FloatRegister($dst$$reg));\n-\n+    Assembler::SIMD_RegVariant size = __ elemBytes_to_regVariant((int)$size$$constant);\n+    __ sve_cpy(as_FloatRegister($dst$$reg), size, as_PRegister($src$$reg), 1, false);\n+    __ sve_vector_narrow(as_FloatRegister($dst$$reg), __ B,\n+                         as_FloatRegister($dst$$reg), size, as_FloatRegister($tmp$$reg));\n@@ -846,9 +1158,11 @@\n-instruct vstoremaskI(vReg dst, vReg src, vReg tmp, immI_4 size) %{\n-  predicate(UseSVE > 0);\n-  match(Set dst (VectorStoreMask src size));\n-  effect(TEMP_DEF dst, TEMP tmp);\n-  ins_cost(4 * SVE_COST);\n-  format %{ \"sve_dup $tmp, S, 0\\n\\t\"\n-            \"sve_uzp1 $dst, H, $src, $tmp\\n\\t\"\n-            \"sve_uzp1 $dst, B, $dst, $tmp\\n\\t\"\n-            \"sve_neg $dst, B, $dst\\t# vector store mask (sve) (S to B)\" %}\n+\/\/ Combine LoadVector+VectorLoadMask when the vector element type is not T_BYTE\n+\n+instruct vloadmask_loadV(pRegGov dst, indirect mem, vReg tmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->as_Vector()->length_in_bytes() == MaxVectorSize &&\n+            type2aelembytes(n->bottom_type()->is_vect()->element_basic_type()) > 1);\n+  match(Set dst (VectorLoadMask (LoadVector mem)));\n+  effect(TEMP tmp, KILL cr);\n+  ins_cost(3 * SVE_COST);\n+  format %{ \"sve_ld1b $tmp, $mem\\n\\t\"\n+            \"sve_cmpne $dst, $tmp, 0\\t# load vector mask (sve) (H\/S\/D)\" %}\n@@ -856,7 +1170,8 @@\n-    __ sve_dup(as_FloatRegister($tmp$$reg), __ S, 0);\n-    __ sve_uzp1(as_FloatRegister($dst$$reg), __ H,\n-                as_FloatRegister($src$$reg), as_FloatRegister($tmp$$reg));\n-    __ sve_uzp1(as_FloatRegister($dst$$reg), __ B,\n-                as_FloatRegister($dst$$reg), as_FloatRegister($tmp$$reg));\n-    __ sve_neg(as_FloatRegister($dst$$reg), __ B, ptrue,\n-               as_FloatRegister($dst$$reg));\n+    \/\/ Load mask values which are boolean type, and extend them to the\n+    \/\/ expected vector element type. Convert the vector to predicate.\n+    BasicType to_vect_bt = Matcher::vector_element_basic_type(this);\n+    loadStoreA_predicated(C2_MacroAssembler(&cbuf), false, as_FloatRegister($tmp$$reg),\n+                          ptrue, T_BOOLEAN, to_vect_bt, $mem->opcode(),\n+                          as_Register($mem$$base), $mem$$index, $mem$$scale, $mem$$disp);\n+    __ sve_cmp(Assembler::NE, as_PRegister($dst$$reg), __ elemType_to_regVariant(to_vect_bt),\n+               ptrue, as_FloatRegister($tmp$$reg), 0);\n@@ -867,10 +1182,9 @@\n-instruct vstoremaskL(vReg dst, vReg src, vReg tmp, immI_8 size) %{\n-  predicate(UseSVE > 0);\n-  match(Set dst (VectorStoreMask src size));\n-  effect(TEMP_DEF dst, TEMP tmp);\n-  ins_cost(5 * SVE_COST);\n-  format %{ \"sve_dup $tmp, D, 0\\n\\t\"\n-            \"sve_uzp1 $dst, S, $src, $tmp\\n\\t\"\n-            \"sve_uzp1 $dst, H, $dst, $tmp\\n\\t\"\n-            \"sve_uzp1 $dst, B, $dst, $tmp\\n\\t\"\n-            \"sve_neg $dst, B, $dst\\t# vector store mask (sve) (D to B)\" %}\n+instruct vloadmask_loadV_partial(pRegGov dst, indirect mem, vReg vtmp, pRegGov ptmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->as_Vector()->length_in_bytes() > 16 &&\n+            n->as_Vector()->length_in_bytes() < MaxVectorSize &&\n+            type2aelembytes(n->bottom_type()->is_vect()->element_basic_type()) > 1);\n+  match(Set dst (VectorLoadMask (LoadVector mem)));\n+  effect(TEMP vtmp, TEMP ptmp, KILL cr);\n+  ins_cost(6 * SVE_COST);\n+  format %{ \"vloadmask_loadV $dst, $mem\\t# load vector mask partial (sve) (H\/S\/D)\" %}\n@@ -878,9 +1192,9 @@\n-    __ sve_dup(as_FloatRegister($tmp$$reg), __ D, 0);\n-    __ sve_uzp1(as_FloatRegister($dst$$reg), __ S,\n-                as_FloatRegister($src$$reg), as_FloatRegister($tmp$$reg));\n-    __ sve_uzp1(as_FloatRegister($dst$$reg), __ H,\n-                as_FloatRegister($dst$$reg), as_FloatRegister($tmp$$reg));\n-    __ sve_uzp1(as_FloatRegister($dst$$reg), __ B,\n-                as_FloatRegister($dst$$reg), as_FloatRegister($tmp$$reg));\n-    __ sve_neg(as_FloatRegister($dst$$reg), __ B, ptrue,\n-               as_FloatRegister($dst$$reg));\n+    \/\/ Load valid mask values which are boolean type, and extend them to the\n+    \/\/ expected vector element type. Convert the vector to predicate.\n+    BasicType to_vect_bt = Matcher::vector_element_basic_type(this);\n+    Assembler::SIMD_RegVariant size = __ elemType_to_regVariant(to_vect_bt);\n+    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), size, Matcher::vector_length(this));\n+    loadStoreA_predicated(C2_MacroAssembler(&cbuf), false, as_FloatRegister($vtmp$$reg),\n+                          as_PRegister($ptmp$$reg), T_BOOLEAN, to_vect_bt, $mem->opcode(),\n+                          as_Register($mem$$base), $mem$$index, $mem$$scale, $mem$$disp);\n+    __ sve_cmp(Assembler::NE, as_PRegister($dst$$reg), size, ptrue, as_FloatRegister($vtmp$$reg), 0);\n@@ -890,33 +1204,6 @@\n-dnl\n-dnl\n-dnl VLOADMASK_LOADV($1,    $2  )\n-dnl VLOADMASK_LOADV(esize, cond)\n-define(`VLOADMASK_LOADV', `\n-instruct vloadmask_loadV_$1(vReg dst, ifelse($1, `byte', vmemA, indirect) mem) %{\n-  predicate(UseSVE > 0 && n->as_Vector()->length_in_bytes() == MaxVectorSize &&\n-            type2aelembytes(n->bottom_type()->is_vect()->element_basic_type()) $2);\n-  match(Set dst (VectorLoadMask (LoadVector mem)));\n-  ins_cost(5 * SVE_COST);\n-  format %{ \"sve_ld1b $dst, $mem\\n\\t\"\n-            \"sve_neg $dst, $dst\\t# load vector mask (sve)\" %}\n-  ins_encode %{\n-    FloatRegister dst_reg = as_FloatRegister($dst$$reg);\n-    BasicType to_vect_bt = Matcher::vector_element_basic_type(this);\n-    Assembler::SIMD_RegVariant to_vect_variant = __ elemType_to_regVariant(to_vect_bt);\n-    loadStoreA_predicated(C2_MacroAssembler(&cbuf), false, dst_reg, ptrue,\n-                          T_BOOLEAN, to_vect_bt, $mem->opcode(),\n-                          as_Register($mem$$base), $mem$$index, $mem$$scale, $mem$$disp);\n-    __ sve_neg(dst_reg, to_vect_variant, ptrue, dst_reg);\n-  %}\n-  ins_pipe(pipe_slow);\n-%}')dnl\n-dnl\n-define(`ARGLIST',\n-`ifelse($1, `byte', vmemA, indirect) mem, vReg src, vReg tmp, ifelse($1, `byte', immI_1, immI_gt_1) esize')\n-dnl\n-dnl STOREV_VSTOREMASK($1,  )\n-dnl STOREV_VSTOREMASK(esize)\n-define(`STOREV_VSTOREMASK', `\n-instruct storeV_vstoremask_$1(ARGLIST($1)) %{\n-  predicate(UseSVE > 0 && n->as_StoreVector()->memory_size() *\n-                          n->as_StoreVector()->in(MemNode::ValueIn)->in(2)->get_int() == MaxVectorSize);\n+\n+\/\/ Combine VectorStoreMask+StoreVector when the vector element type is not T_BYTE\n+\n+instruct storeV_vstoremask(indirect mem, pRegGov src, vReg tmp, immI_gt_1 esize) %{\n+  predicate(UseSVE > 0 &&\n+            Matcher::vector_length_in_bytes(n->as_StoreVector()->in(MemNode::ValueIn)->in(1)) == MaxVectorSize);\n@@ -925,3 +1212,3 @@\n-  ins_cost(5 * SVE_COST);\n-  format %{ \"sve_neg $tmp, $src\\n\\t\"\n-            \"sve_st1b $tmp, $mem\\t# store vector mask (sve)\" %}\n+  ins_cost(3 * SVE_COST);\n+  format %{ \"sve_cpy $tmp, $src, 1\\n\\t\"\n+            \"sve_st1b $tmp, $mem\\t# store vector mask (sve) (H\/S\/D)\" %}\n@@ -931,3 +1218,2 @@\n-    Assembler::SIMD_RegVariant from_vect_variant = __ elemBytes_to_regVariant($esize$$constant);\n-    __ sve_neg(as_FloatRegister($tmp$$reg), from_vect_variant, ptrue,\n-               as_FloatRegister($src$$reg));\n+    Assembler::SIMD_RegVariant size = __ elemBytes_to_regVariant($esize$$constant);\n+    __ sve_cpy(as_FloatRegister($tmp$$reg), size, as_PRegister($src$$reg), 1, false);\n@@ -939,10 +1225,1 @@\n-%}')dnl\n-undefine(ARGLIST)dnl\n-dnl\n-\/\/ load\/store mask vector\n-VLOADMASK_LOADV(byte, == 1)\n-VLOADMASK_LOADV(non_byte, > 1)\n-STOREV_VSTOREMASK(byte)\n-STOREV_VSTOREMASK(non_byte)\n-\n-\/\/ vector add reduction\n+%}\n@@ -950,4 +1227,36 @@\n-instruct reduce_addI(iRegINoSp dst, iRegIorL2I src1, vReg src2, vRegD vtmp) %{\n-  predicate(UseSVE > 0 && n->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize);\n-  match(Set dst (AddReductionVI src1 src2));\n-  effect(TEMP_DEF dst, TEMP vtmp);\n+instruct storeV_vstoremask_partial(indirect mem, pRegGov src, vReg vtmp,\n+                                   immI_gt_1 esize, pRegGov ptmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->as_StoreVector()->memory_size() > 16 &&\n+            type2aelembytes(n->as_StoreVector()->vect_type()->element_basic_type()) > 1 &&\n+            Matcher::vector_length_in_bytes(n->as_StoreVector()->in(MemNode::ValueIn)->in(1)) < MaxVectorSize);\n+  match(Set mem (StoreVector mem (VectorStoreMask src esize)));\n+  effect(TEMP vtmp, TEMP ptmp, KILL cr);\n+  format %{ \"storeV_vstoremask $src, $mem\\t# store vector mask partial (sve) (H\/S\/D)\" %}\n+  ins_cost(6 * SVE_COST);\n+  ins_encode %{\n+    \/\/ Convert the valid src predicate to vector, and store the vector\n+    \/\/ elements as boolean values.\n+    BasicType from_vect_bt = Matcher::vector_element_basic_type(this, $src);\n+    Assembler::SIMD_RegVariant size = __ elemType_to_regVariant(from_vect_bt);\n+    __ sve_cpy(as_FloatRegister($vtmp$$reg), size, as_PRegister($src$$reg), 1, false);\n+    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), size, Matcher::vector_length(this, $src));\n+    loadStoreA_predicated(C2_MacroAssembler(&cbuf), true, as_FloatRegister($vtmp$$reg),\n+                          as_PRegister($ptmp$$reg), T_BOOLEAN, from_vect_bt, $mem->opcode(),\n+                          as_Register($mem$$base), $mem$$index, $mem$$scale, $mem$$disp);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+dnl\n+dnl REDUCE_I($1,        $2     )\n+dnl REDUCE_I(insn_name, op_name)\n+define(`REDUCE_I', `\n+instruct reduce_$1I(iRegINoSp dst, iRegIorL2I src1, vReg src2, vRegD tmp) %{\n+  ifelse($2, AddReductionVI,\n+       `predicate(UseSVE > 0 &&\n+            n->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize);',\n+       `predicate(UseSVE > 0 &&\n+            n->in(2)->bottom_type()->is_vect()->element_basic_type() != T_LONG &&\n+            n->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize);')\n+  match(Set dst ($2 src1 src2));\n+  effect(TEMP_DEF dst, TEMP tmp);\n@@ -955,1 +1264,1 @@\n-  format %{ \"sve_reduce_addI $dst, $src1, $src2\\t# addB\/S\/I reduction (sve) (may extend)\" %}\n+  format %{ \"sve_reduce_$1I $dst, $src1, $src2\\t# $1I reduction (sve) (may extend)\" %}\n@@ -958,11 +1267,3 @@\n-    Assembler::SIMD_RegVariant variant = __ elemType_to_regVariant(bt);\n-    __ sve_uaddv(as_FloatRegister($vtmp$$reg), variant, ptrue, as_FloatRegister($src2$$reg));\n-    __ umov($dst$$Register, as_FloatRegister($vtmp$$reg), variant, 0);\n-    __ addw($dst$$Register, $dst$$Register, $src1$$Register);\n-    if (bt == T_BYTE) {\n-      __ sxtb($dst$$Register, $dst$$Register);\n-    } else if (bt == T_SHORT) {\n-      __ sxth($dst$$Register, $dst$$Register);\n-    } else {\n-      assert(bt == T_INT, \"unsupported type\");\n-    }\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, bt,\n+                           $src1$$Register, as_FloatRegister($src2$$reg),\n+                           ptrue, as_FloatRegister($tmp$$reg));\n@@ -971,3 +1272,29 @@\n-%}\n-\n-instruct reduce_addI_partial(iRegINoSp dst, iRegIorL2I src1, vReg src2, vRegD vtmp,\n+%}')dnl\n+dnl\n+dnl\n+dnl REDUCE_L($1,        $2    )\n+dnl REDUCE_L(insn_name, op_name)\n+define(`REDUCE_L', `\n+instruct reduce_$1L(iRegLNoSp dst, iRegL src1, vReg src2, vRegD tmp) %{\n+  ifelse($2, AddReductionVL,\n+       `predicate(UseSVE > 0 &&\n+            n->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize);',\n+       `predicate(UseSVE > 0 &&\n+            n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_LONG &&\n+            n->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize);')\n+  match(Set dst ($2 src1 src2));\n+  effect(TEMP_DEF dst, TEMP tmp);\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_reduce_$1L $dst, $src1, $src2\\t# $1L reduction (sve)\" %}\n+  ins_encode %{\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, T_LONG,\n+                           $src1$$Register, as_FloatRegister($src2$$reg),\n+                           ptrue, as_FloatRegister($tmp$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}')dnl\n+dnl\n+dnl REDUCE_I_PARTIAL($1,        $2     )\n+dnl REDUCE_I_PARTIAL(insn_name, op_name)\n+define(`REDUCE_I_PARTIAL', `\n+instruct reduce_$1I_partial(iRegINoSp dst, iRegIorL2I src1, vReg src2, vRegD vtmp,\n@@ -975,2 +1302,7 @@\n-  predicate(UseSVE > 0 && n->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize);\n-  match(Set dst (AddReductionVI src1 src2));\n+  ifelse($2, AddReductionVI,\n+       `predicate(UseSVE > 0 &&\n+            n->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize);',\n+       `predicate(UseSVE > 0 &&\n+            n->in(2)->bottom_type()->is_vect()->element_basic_type() != T_LONG &&\n+            n->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize);')\n+  match(Set dst ($2 src1 src2));\n@@ -978,2 +1310,2 @@\n-  ins_cost(SVE_COST);\n-  format %{ \"sve_reduce_addI $dst, $src1, $src2\\t# addI reduction partial (sve) (may extend)\" %}\n+  ins_cost(2 * SVE_COST);\n+  format %{ \"sve_reduce_$1I $dst, $src1, $src2\\t# $1I reduction partial (sve) (may extend)\" %}\n@@ -985,25 +1317,3 @@\n-    __ sve_uaddv(as_FloatRegister($vtmp$$reg), variant,\n-                 as_PRegister($ptmp$$reg), as_FloatRegister($src2$$reg));\n-    __ umov($dst$$Register, as_FloatRegister($vtmp$$reg), variant, 0);\n-    __ addw($dst$$Register, $dst$$Register, $src1$$Register);\n-    if (bt == T_BYTE) {\n-      __ sxtb($dst$$Register, $dst$$Register);\n-    } else if (bt == T_SHORT) {\n-      __ sxth($dst$$Register, $dst$$Register);\n-    } else {\n-      assert(bt == T_INT, \"unsupported type\");\n-    }\n-  %}\n-  ins_pipe(pipe_slow);\n-%}\n-\n-instruct reduce_addL(iRegLNoSp dst, iRegL src1, vReg src2, vRegD vtmp) %{\n-  predicate(UseSVE > 0 && n->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize);\n-  match(Set dst (AddReductionVL src1 src2));\n-  effect(TEMP_DEF dst, TEMP vtmp);\n-  ins_cost(SVE_COST);\n-  format %{ \"sve_reduce_addL $dst, $src1, $src2\\t# addL reduction (sve)\" %}\n-  ins_encode %{\n-    __ sve_uaddv(as_FloatRegister($vtmp$$reg), __ D, ptrue, as_FloatRegister($src2$$reg));\n-    __ umov($dst$$Register, as_FloatRegister($vtmp$$reg), __ D, 0);\n-    __ add($dst$$Register, $dst$$Register, $src1$$Register);\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, bt,\n+                           $src1$$Register, as_FloatRegister($src2$$reg),\n+                           as_PRegister($ptmp$$reg), as_FloatRegister($vtmp$$reg));\n@@ -1012,3 +1322,6 @@\n-%}\n-\n-instruct reduce_addL_partial(iRegLNoSp dst, iRegL src1, vReg src2, vRegD vtmp,\n+%}')dnl\n+dnl\n+dnl REDUCE_L_PARTIAL($1,        $2    )\n+dnl REDUCE_L_PARTIAL(insn_name, op_name)\n+define(`REDUCE_L_PARTIAL', `\n+instruct reduce_$1L_partial(iRegLNoSp dst, iRegL src1, vReg src2, vRegD vtmp,\n@@ -1016,2 +1329,7 @@\n-  predicate(UseSVE > 0 && n->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize);\n-  match(Set dst (AddReductionVL src1 src2));\n+  ifelse($2, AddReductionVL,\n+       `predicate(UseSVE > 0 &&\n+            n->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize);',\n+       `predicate(UseSVE > 0 &&\n+            n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_LONG &&\n+            n->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize);')\n+  match(Set dst ($2 src1 src2));\n@@ -1019,2 +1337,2 @@\n-  ins_cost(SVE_COST);\n-  format %{ \"sve_reduce_addL $dst, $src1, $src2\\t# addL reduction partial (sve)\" %}\n+  ins_cost(2 * SVE_COST);\n+  format %{ \"sve_reduce_$1L $dst, $src1, $src2\\t# $1L reduction partial (sve)\" %}\n@@ -1024,4 +1342,3 @@\n-    __ sve_uaddv(as_FloatRegister($vtmp$$reg), __ D,\n-                 as_PRegister($ptmp$$reg), as_FloatRegister($src2$$reg));\n-    __ umov($dst$$Register, as_FloatRegister($vtmp$$reg), __ D, 0);\n-    __ add($dst$$Register, $dst$$Register, $src1$$Register);\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, T_LONG,\n+                           $src1$$Register, as_FloatRegister($src2$$reg),\n+                           as_PRegister($ptmp$$reg), as_FloatRegister($vtmp$$reg));\n@@ -1030,2 +1347,1 @@\n-%}\n-\n+%}')dnl\n@@ -1036,3 +1352,4 @@\n-instruct $1($3 src1_dst, vReg src2) %{\n-  predicate(UseSVE > 0 && n->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize);\n-  match(Set src1_dst (AddReductionV$2 src1_dst src2));\n+instruct reduce_$1($3 src1_dst, vReg src2) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize);\n+  match(Set src1_dst ($2 src1_dst src2));\n@@ -1052,3 +1369,4 @@\n-instruct $1($3 src1_dst, vReg src2, pRegGov ptmp, rFlagsReg cr) %{\n-  predicate(UseSVE > 0 && n->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize);\n-  match(Set src1_dst (AddReductionV$2 src1_dst src2));\n+instruct reduce_$1_partial($3 src1_dst, vReg src2, pRegGov ptmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize);\n+  match(Set src1_dst ($2 src1_dst src2));\n@@ -1057,1 +1375,1 @@\n-  format %{ \"sve_reduce_add$2 $src1_dst, $src1_dst, $src2\\t# add$2 reduction partial (sve) ($4)\" %}\n+  format %{ \"sve_reduce_$1 $src1_dst, $src1_dst, $src2\\t# $1 reduction partial (sve) ($4)\" %}\n@@ -1067,12 +1385,13 @@\n-REDUCE_ADDF(reduce_addF, F, vRegF, S)\n-REDUCE_ADDF_PARTIAL(reduce_addF_partial, F, vRegF, S)\n-REDUCE_ADDF(reduce_addD, D, vRegD, D)\n-REDUCE_ADDF_PARTIAL(reduce_addD_partial, D, vRegD, D)\n-\n-\/\/ vector and reduction\n-\n-instruct reduce_andI(iRegINoSp dst, iRegIorL2I src1, vReg src2, vRegD vtmp) %{\n-  predicate(UseSVE > 0 && n->in(2)->bottom_type()->is_vect()->element_basic_type() != T_LONG &&\n-            n->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize);\n-  match(Set dst (AndReductionV src1 src2));\n-  effect(TEMP_DEF dst, TEMP vtmp);\n+dnl\n+dnl REDUCE_I_PREDICATE($1,        $2     )\n+dnl REDUCE_I_PREDICATE(insn_name, op_name)\n+define(`REDUCE_I_PREDICATE', `\n+instruct reduce_$1I_masked(iRegINoSp dst, iRegIorL2I src1, vReg src2, vRegD tmp, pRegGov pg) %{\n+  ifelse($2, AddReductionVI,\n+       `predicate(UseSVE > 0 &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize);',\n+       `predicate(UseSVE > 0 &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->element_basic_type() != T_LONG &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize);')\n+  match(Set dst ($2 (Binary src1 src2) pg));\n+  effect(TEMP_DEF dst, TEMP tmp);\n@@ -1080,1 +1399,1 @@\n-  format %{ \"sve_reduce_andI $dst, $src1, $src2\\t# andB\/S\/I reduction (sve) (may extend)\" %}\n+  format %{ \"sve_reduce_$1I $dst, $src1, $pg, $src2\\t# $1I reduction predicated (sve) (may extend)\" %}\n@@ -1083,11 +1402,3 @@\n-    Assembler::SIMD_RegVariant variant = __ elemType_to_regVariant(bt);\n-    __ sve_andv(as_FloatRegister($vtmp$$reg), variant, ptrue, as_FloatRegister($src2$$reg));\n-    __ smov($dst$$Register, as_FloatRegister($vtmp$$reg), variant, 0);\n-    __ andw($dst$$Register, $dst$$Register, $src1$$Register);\n-    if (bt == T_BYTE) {\n-      __ sxtb($dst$$Register, $dst$$Register);\n-    } else if (bt == T_SHORT) {\n-      __ sxth($dst$$Register, $dst$$Register);\n-    } else {\n-      assert(bt == T_INT, \"unsupported type\");\n-    }\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, bt,\n+                           $src1$$Register, as_FloatRegister($src2$$reg),\n+                           as_PRegister($pg$$reg), as_FloatRegister($tmp$$reg));\n@@ -1096,8 +1407,14 @@\n-%}\n-\n-instruct reduce_andI_partial(iRegINoSp dst, iRegIorL2I src1, vReg src2, vRegD vtmp,\n-                             pRegGov ptmp, rFlagsReg cr) %{\n-  predicate(UseSVE > 0 && n->in(2)->bottom_type()->is_vect()->element_basic_type() != T_LONG &&\n-            n->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize);\n-  match(Set dst (AndReductionV src1 src2));\n-  effect(TEMP_DEF dst, TEMP vtmp, TEMP ptmp, KILL cr);\n+%}')dnl\n+dnl\n+dnl REDUCE_L_PREDICATE($1,        $2    )\n+dnl REDUCE_L_PREDICATE(insn_name, op_name)\n+define(`REDUCE_L_PREDICATE', `\n+instruct reduce_$1L_masked(iRegLNoSp dst, iRegL src1, vReg src2, vRegD tmp, pRegGov pg) %{\n+  ifelse($2, AddReductionVL,\n+       `predicate(UseSVE > 0 &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize);',\n+       `predicate(UseSVE > 0 &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->element_basic_type() == T_LONG &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize);')\n+  match(Set dst ($2 (Binary src1 src2) pg));\n+  effect(TEMP_DEF dst, TEMP tmp);\n@@ -1105,1 +1422,24 @@\n-  format %{ \"sve_reduce_andI $dst, $src1, $src2\\t# andI reduction partial (sve) (may extend)\" %}\n+  format %{ \"sve_reduce_$1L $dst, $src1, $pg, $src2\\t# $1L reduction predicated (sve)\" %}\n+  ins_encode %{\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, T_LONG,\n+                           $src1$$Register, as_FloatRegister($src2$$reg),\n+                           as_PRegister($pg$$reg), as_FloatRegister($tmp$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}')dnl\n+dnl\n+dnl REDUCE_I_PREDICATE_PARTIAL($1,        $2     )\n+dnl REDUCE_I_PREDICATE_PARTIAL(insn_name, op_name)\n+define(`REDUCE_I_PREDICATE_PARTIAL', `\n+instruct reduce_$1I_masked_partial(iRegINoSp dst, iRegIorL2I src1, vReg src2, vRegD vtmp,\n+                                  pRegGov pg, pRegGov ptmp, rFlagsReg cr) %{\n+  ifelse($2, AddReductionVI,\n+       `predicate(UseSVE > 0 &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize);',\n+       `predicate(UseSVE > 0 &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->element_basic_type() != T_LONG &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize);')\n+  match(Set dst ($2 (Binary src1 src2) pg));\n+  effect(TEMP_DEF dst, TEMP vtmp, TEMP ptmp, KILL cr);\n+  ins_cost(3 * SVE_COST);\n+  format %{ \"sve_reduce_$1I $dst, $src1, $pg, $src2\\t# $1I reduction predicated partial (sve) (may extend)\" %}\n@@ -1111,26 +1451,5 @@\n-    __ sve_andv(as_FloatRegister($vtmp$$reg), variant,\n-                as_PRegister($ptmp$$reg), as_FloatRegister($src2$$reg));\n-    __ smov($dst$$Register, as_FloatRegister($vtmp$$reg), variant, 0);\n-    __ andw($dst$$Register, $dst$$Register, $src1$$Register);\n-    if (bt == T_BYTE) {\n-      __ sxtb($dst$$Register, $dst$$Register);\n-    } else if (bt == T_SHORT) {\n-      __ sxth($dst$$Register, $dst$$Register);\n-    } else {\n-      assert(bt == T_INT, \"unsupported type\");\n-    }\n-  %}\n-  ins_pipe(pipe_slow);\n-%}\n-\n-instruct reduce_andL(iRegLNoSp dst, iRegL src1, vReg src2, vRegD vtmp) %{\n-  predicate(UseSVE > 0 && n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_LONG &&\n-            n->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize);\n-  match(Set dst (AndReductionV src1 src2));\n-  effect(TEMP_DEF dst, TEMP vtmp);\n-  ins_cost(SVE_COST);\n-  format %{ \"sve_reduce_andL $dst, $src1, $src2\\t# andL reduction (sve)\" %}\n-  ins_encode %{\n-    __ sve_andv(as_FloatRegister($vtmp$$reg), __ D, ptrue, as_FloatRegister($src2$$reg));\n-    __ umov($dst$$Register, as_FloatRegister($vtmp$$reg), __ D, 0);\n-    __ andr($dst$$Register, $dst$$Register, $src1$$Register);\n+    __ sve_and(as_PRegister($ptmp$$reg), as_PRegister($ptmp$$reg),\n+               as_PRegister($pg$$reg), as_PRegister($pg$$reg));\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, bt,\n+                           $src1$$Register, as_FloatRegister($src2$$reg),\n+                           as_PRegister($ptmp$$reg), as_FloatRegister($vtmp$$reg));\n@@ -1139,7 +1458,14 @@\n-%}\n-\n-instruct reduce_andL_partial(iRegLNoSp dst, iRegL src1, vReg src2, vRegD vtmp,\n-                             pRegGov ptmp, rFlagsReg cr) %{\n-  predicate(UseSVE > 0 && n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_LONG &&\n-            n->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize);\n-  match(Set dst (AndReductionV src1 src2));\n+%}')dnl\n+dnl\n+dnl REDUCE_L_PREDICATE_PARTIAL($1,        $2    )\n+dnl REDUCE_L_PREDICATE_PARTIAL(insn_name, op_name)\n+define(`REDUCE_L_PREDICATE_PARTIAL', `\n+instruct reduce_$1L_masked_partial(iRegLNoSp dst, iRegL src1, vReg src2, vRegD vtmp,\n+                                  pRegGov pg, pRegGov ptmp, rFlagsReg cr) %{\n+  ifelse($2, AddReductionVL,\n+       `predicate(UseSVE > 0 &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize);',\n+       `predicate(UseSVE > 0 &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->element_basic_type() == T_LONG &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize);')\n+  match(Set dst ($2 (Binary src1 src2) pg));\n@@ -1147,2 +1473,2 @@\n-  ins_cost(SVE_COST);\n-  format %{ \"sve_reduce_andL $dst, $src1, $src2\\t# andL reduction partial (sve)\" %}\n+  ins_cost(3 * SVE_COST);\n+  format %{ \"sve_reduce_$1L $dst, $src1, $pg, $src2\\t# $1L reduction predicated partial (sve)\" %}\n@@ -1152,4 +1478,5 @@\n-    __ sve_andv(as_FloatRegister($vtmp$$reg), __ D,\n-                as_PRegister($ptmp$$reg), as_FloatRegister($src2$$reg));\n-    __ umov($dst$$Register, as_FloatRegister($vtmp$$reg), __ D, 0);\n-    __ andr($dst$$Register, $dst$$Register, $src1$$Register);\n+    __ sve_and(as_PRegister($ptmp$$reg), as_PRegister($ptmp$$reg),\n+               as_PRegister($pg$$reg), as_PRegister($pg$$reg));\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, T_LONG,\n+                           $src1$$Register, as_FloatRegister($src2$$reg),\n+                           as_PRegister($ptmp$$reg), as_FloatRegister($vtmp$$reg));\n@@ -1158,9 +1485,9 @@\n-%}\n-\n-\/\/ vector or reduction\n-\n-instruct reduce_orI(iRegINoSp dst, iRegIorL2I src1, vReg src2, vRegD vtmp) %{\n-  predicate(UseSVE > 0 && n->in(2)->bottom_type()->is_vect()->element_basic_type() != T_LONG &&\n-            n->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize);\n-  match(Set dst (OrReductionV src1 src2));\n-  effect(TEMP_DEF dst, TEMP vtmp);\n+%}')dnl\n+dnl\n+dnl REDUCE_ADDF_PREDICATE($1,        $2,      $3,      $4  )\n+dnl REDUCE_ADDF_PREDICATE(insn_name, op_name, reg_dst, size)\n+define(`REDUCE_ADDF_PREDICATE', `\n+instruct reduce_$1_masked($3 src1_dst, vReg src2, pRegGov pg) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize);\n+  match(Set src1_dst ($2 (Binary src1_dst src2) pg));\n@@ -1168,1 +1495,1 @@\n-  format %{ \"sve_reduce_orI $dst, $src1, $src2\\t# orB\/S\/I reduction (sve) (may extend)\" %}\n+  format %{ \"sve_reduce_$1 $src1_dst, $pg, $src2\\t# $1 reduction predicated (sve)\" %}\n@@ -1170,12 +1497,2 @@\n-    BasicType bt = Matcher::vector_element_basic_type(this, $src2);\n-    Assembler::SIMD_RegVariant variant = __ elemType_to_regVariant(bt);\n-    __ sve_orv(as_FloatRegister($vtmp$$reg), variant, ptrue, as_FloatRegister($src2$$reg));\n-    __ smov($dst$$Register, as_FloatRegister($vtmp$$reg), variant, 0);\n-    __ orrw($dst$$Register, $dst$$Register, $src1$$Register);\n-    if (bt == T_BYTE) {\n-      __ sxtb($dst$$Register, $dst$$Register);\n-    } else if (bt == T_SHORT) {\n-      __ sxth($dst$$Register, $dst$$Register);\n-    } else {\n-      assert(bt == T_INT, \"unsupported type\");\n-    }\n+    __ sve_fadda(as_FloatRegister($src1_dst$$reg), __ $4,\n+                 as_PRegister($pg$$reg), as_FloatRegister($src2$$reg));\n@@ -1184,8 +1501,10 @@\n-%}\n-\n-instruct reduce_orI_partial(iRegINoSp dst, iRegIorL2I src1, vReg src2, vRegD vtmp,\n-                             pRegGov ptmp, rFlagsReg cr) %{\n-  predicate(UseSVE > 0 && n->in(2)->bottom_type()->is_vect()->element_basic_type() != T_LONG &&\n-            n->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize);\n-  match(Set dst (OrReductionV src1 src2));\n-  effect(TEMP_DEF dst, TEMP vtmp, TEMP ptmp, KILL cr);\n+%}')dnl\n+dnl\n+dnl REDUCE_ADDF_PREDICATE_PARTIAL($1,        $2,      $3,      $4  )\n+dnl REDUCE_ADDF_PREDICATE_PARTIAL(insn_name, op_name, reg_dst, size)\n+define(`REDUCE_ADDF_PREDICATE_PARTIAL', `\n+instruct reduce_$1_masked_partial($3 src1_dst, vReg src2, pRegGov pg, pRegGov ptmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize);\n+  match(Set src1_dst ($2 (Binary src1_dst src2) pg));\n+  effect(TEMP ptmp, KILL cr);\n@@ -1193,1 +1512,1 @@\n-  format %{ \"sve_reduce_orI $dst, $src1, $src2\\t# orI reduction partial (sve) (may extend)\" %}\n+  format %{ \"sve_reduce_$1 $src1_dst, $pg, $src2\\t# $1 reduction predicated partial (sve)\" %}\n@@ -1195,3 +1514,1 @@\n-    BasicType bt = Matcher::vector_element_basic_type(this, $src2);\n-    Assembler::SIMD_RegVariant variant = __ elemType_to_regVariant(bt);\n-    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), variant,\n+    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), __ $4,\n@@ -1199,11 +1516,4 @@\n-    __ sve_orv(as_FloatRegister($vtmp$$reg), variant,\n-               as_PRegister($ptmp$$reg), as_FloatRegister($src2$$reg));\n-    __ smov($dst$$Register, as_FloatRegister($vtmp$$reg), variant, 0);\n-    __ orrw($dst$$Register, $dst$$Register, $src1$$Register);\n-    if (bt == T_BYTE) {\n-      __ sxtb($dst$$Register, $dst$$Register);\n-    } else if (bt == T_SHORT) {\n-      __ sxth($dst$$Register, $dst$$Register);\n-    } else {\n-      assert(bt == T_INT, \"unsupported type\");\n-    }\n+    __ sve_and(as_PRegister($ptmp$$reg), as_PRegister($ptmp$$reg),\n+               as_PRegister($pg$$reg), as_PRegister($pg$$reg));\n+    __ sve_fadda(as_FloatRegister($src1_dst$$reg), __ $4,\n+                 as_PRegister($ptmp$$reg), as_FloatRegister($src2$$reg));\n@@ -1212,1 +1522,2 @@\n-%}\n+%}')dnl\n+dnl\n@@ -1214,14 +1525,19 @@\n-instruct reduce_orL(iRegLNoSp dst, iRegL src1, vReg src2, vRegD vtmp) %{\n-  predicate(UseSVE > 0 && n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_LONG &&\n-            n->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize);\n-  match(Set dst (OrReductionV src1 src2));\n-  effect(TEMP_DEF dst, TEMP vtmp);\n-  ins_cost(SVE_COST);\n-  format %{ \"sve_reduce_orL $dst, $src1, $src2\\t# orL reduction (sve)\" %}\n-  ins_encode %{\n-    __ sve_orv(as_FloatRegister($vtmp$$reg), __ D, ptrue, as_FloatRegister($src2$$reg));\n-    __ umov($dst$$Register, as_FloatRegister($vtmp$$reg), __ D, 0);\n-    __ orr($dst$$Register, $dst$$Register, $src1$$Register);\n-  %}\n-  ins_pipe(pipe_slow);\n-%}\n+\/\/ vector add reduction\n+REDUCE_I(add, AddReductionVI)\n+REDUCE_L(add, AddReductionVL)\n+REDUCE_ADDF(addF, AddReductionVF, vRegF, S)\n+REDUCE_ADDF(addD, AddReductionVD, vRegD, D)\n+REDUCE_I_PARTIAL(add, AddReductionVI)\n+REDUCE_L_PARTIAL(add, AddReductionVL)\n+REDUCE_ADDF_PARTIAL(addF, AddReductionVF, vRegF, S)\n+REDUCE_ADDF_PARTIAL(addD, AddReductionVD, vRegD, D)\n+\n+\/\/ vector add reduction - predicated\n+REDUCE_I_PREDICATE(add, AddReductionVI)\n+REDUCE_L_PREDICATE(add, AddReductionVL)\n+REDUCE_ADDF_PREDICATE(addF, AddReductionVF, vRegF, S)\n+REDUCE_ADDF_PREDICATE(addD, AddReductionVD, vRegD, D)\n+REDUCE_I_PREDICATE_PARTIAL(add, AddReductionVI)\n+REDUCE_L_PREDICATE_PARTIAL(add, AddReductionVL)\n+REDUCE_ADDF_PREDICATE_PARTIAL(addF, AddReductionVF, vRegF, S)\n+REDUCE_ADDF_PREDICATE_PARTIAL(addD, AddReductionVD, vRegD, D)\n@@ -1229,18 +1545,23 @@\n-instruct reduce_orL_partial(iRegLNoSp dst, iRegL src1, vReg src2, vRegD vtmp,\n-                             pRegGov ptmp, rFlagsReg cr) %{\n-  predicate(UseSVE > 0 && n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_LONG &&\n-            n->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize);\n-  match(Set dst (OrReductionV src1 src2));\n-  effect(TEMP_DEF dst, TEMP vtmp, TEMP ptmp, KILL cr);\n-  ins_cost(SVE_COST);\n-  format %{ \"sve_reduce_orL $dst, $src1, $src2\\t# orL reduction partial (sve)\" %}\n-  ins_encode %{\n-    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), __ D,\n-                          Matcher::vector_length(this, $src2));\n-    __ sve_orv(as_FloatRegister($vtmp$$reg), __ D,\n-               as_PRegister($ptmp$$reg), as_FloatRegister($src2$$reg));\n-    __ umov($dst$$Register, as_FloatRegister($vtmp$$reg), __ D, 0);\n-    __ orr($dst$$Register, $dst$$Register, $src1$$Register);\n-  %}\n-  ins_pipe(pipe_slow);\n-%}\n+\/\/ vector and reduction\n+REDUCE_I(and, AndReductionV)\n+REDUCE_L(and, AndReductionV)\n+REDUCE_I_PARTIAL(and, AndReductionV)\n+REDUCE_L_PARTIAL(and, AndReductionV)\n+\n+\/\/ vector and reduction - predicated\n+REDUCE_I_PREDICATE(and, AndReductionV)\n+REDUCE_L_PREDICATE(and, AndReductionV)\n+REDUCE_I_PREDICATE_PARTIAL(and, AndReductionV)\n+REDUCE_L_PREDICATE_PARTIAL(and, AndReductionV)\n+\n+\/\/ vector or reduction\n+REDUCE_I(or, OrReductionV)\n+REDUCE_L(or, OrReductionV)\n+REDUCE_I_PARTIAL(or, OrReductionV)\n+REDUCE_L_PARTIAL(or, OrReductionV)\n+\n+\/\/ vector or reduction - predicated\n+REDUCE_I_PREDICATE(or, OrReductionV)\n+REDUCE_L_PREDICATE(or, OrReductionV)\n+REDUCE_I_PREDICATE_PARTIAL(or, OrReductionV)\n+REDUCE_L_PREDICATE_PARTIAL(or, OrReductionV)\n@@ -1249,0 +1570,4 @@\n+REDUCE_I(eor, XorReductionV)\n+REDUCE_L(eor, XorReductionV)\n+REDUCE_I_PARTIAL(eor, XorReductionV)\n+REDUCE_L_PARTIAL(eor, XorReductionV)\n@@ -1250,5 +1575,17 @@\n-instruct reduce_eorI(iRegINoSp dst, iRegIorL2I src1, vReg src2, vRegD vtmp) %{\n-  predicate(UseSVE > 0 && n->in(2)->bottom_type()->is_vect()->element_basic_type() != T_LONG &&\n-            n->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize);\n-  match(Set dst (XorReductionV src1 src2));\n-  effect(TEMP_DEF dst, TEMP vtmp);\n+\/\/ vector xor reduction - predicated\n+REDUCE_I_PREDICATE(eor, XorReductionV)\n+REDUCE_L_PREDICATE(eor, XorReductionV)\n+REDUCE_I_PREDICATE_PARTIAL(eor, XorReductionV)\n+REDUCE_L_PREDICATE_PARTIAL(eor, XorReductionV)\n+\n+dnl\n+dnl REDUCE_MAXMIN_I($1,        $2     )\n+dnl REDUCE_MAXMIN_I(insn_name, op_name)\n+define(`REDUCE_MAXMIN_I', `\n+instruct reduce_$1I(iRegINoSp dst, iRegIorL2I src1, vReg src2, vRegD tmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize &&\n+            n->in(2)->bottom_type()->is_vect()->element_basic_type() != T_LONG &&\n+            is_integral_type(n->in(2)->bottom_type()->is_vect()->element_basic_type()));\n+  match(Set dst ($2 src1 src2));\n+  effect(TEMP_DEF dst, TEMP tmp, KILL cr);\n@@ -1256,1 +1593,1 @@\n-  format %{ \"sve_reduce_eorI $dst, $src1, $src2\\t# xorB\/H\/I reduction (sve) (may extend)\" %}\n+  format %{ \"sve_reduce_$1I $dst, $src1, $src2\\t# $1I reduction (sve)\" %}\n@@ -1259,11 +1596,3 @@\n-    Assembler::SIMD_RegVariant variant = __ elemType_to_regVariant(bt);\n-    __ sve_eorv(as_FloatRegister($vtmp$$reg), variant, ptrue, as_FloatRegister($src2$$reg));\n-    __ smov($dst$$Register, as_FloatRegister($vtmp$$reg), variant, 0);\n-    __ eorw($dst$$Register, $dst$$Register, $src1$$Register);\n-    if (bt == T_BYTE) {\n-      __ sxtb($dst$$Register, $dst$$Register);\n-    } else if (bt == T_SHORT) {\n-      __ sxth($dst$$Register, $dst$$Register);\n-    } else {\n-      assert(bt == T_INT, \"unsupported type\");\n-    }\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, bt,\n+                           $src1$$Register, as_FloatRegister($src2$$reg),\n+                           ptrue, as_FloatRegister($tmp$$reg));\n@@ -1272,3 +1601,25 @@\n-%}\n-\n-instruct reduce_eorI_partial(iRegINoSp dst, iRegIorL2I src1, vReg src2, vRegD vtmp,\n+%}')dnl\n+dnl\n+dnl REDUCE_MAXMIN_L($1,        $2     )\n+dnl REDUCE_MAXMIN_L(insn_name, op_name)\n+define(`REDUCE_MAXMIN_L', `\n+instruct reduce_$1L(iRegLNoSp dst, iRegL src1, vReg src2, vRegD tmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize &&\n+            n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_LONG);\n+  match(Set dst ($2 src1 src2));\n+  effect(TEMP_DEF dst, TEMP tmp, KILL cr);\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_reduce_$1L $dst, $src1, $src2\\t# $1L reduction (sve)\" %}\n+  ins_encode %{\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, T_LONG,\n+                           $src1$$Register, as_FloatRegister($src2$$reg),\n+                           ptrue, as_FloatRegister($tmp$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}')dnl\n+dnl\n+dnl REDUCE_MAXMIN_I_PARTIAL($1     , $2     )\n+dnl REDUCE_MAXMIN_I_PARTIAL(min_max, op_name)\n+define(`REDUCE_MAXMIN_I_PARTIAL', `\n+instruct reduce_$1I_partial(iRegINoSp dst, iRegIorL2I src1, vReg src2, vRegD vtmp,\n@@ -1276,3 +1627,5 @@\n-  predicate(UseSVE > 0 && n->in(2)->bottom_type()->is_vect()->element_basic_type() != T_LONG &&\n-            n->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize);\n-  match(Set dst (XorReductionV src1 src2));\n+  predicate(UseSVE > 0 &&\n+            n->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize &&\n+            n->in(2)->bottom_type()->is_vect()->element_basic_type() != T_LONG &&\n+            is_integral_type(n->in(2)->bottom_type()->is_vect()->element_basic_type()));\n+  match(Set dst ($2 src1 src2));\n@@ -1280,2 +1633,2 @@\n-  ins_cost(SVE_COST);\n-  format %{ \"sve_reduce_eorI $dst, $src1, $src2\\t# xorI reduction partial (sve) (may extend)\" %}\n+  ins_cost(2 * SVE_COST);\n+  format %{ \"sve_reduce_$1I $dst, $src1, $src2\\t# $1I reduction partial (sve)\" %}\n@@ -1287,26 +1640,3 @@\n-    __ sve_eorv(as_FloatRegister($vtmp$$reg), variant,\n-                as_PRegister($ptmp$$reg), as_FloatRegister($src2$$reg));\n-    __ smov($dst$$Register, as_FloatRegister($vtmp$$reg), variant, 0);\n-    __ eorw($dst$$Register, $dst$$Register, $src1$$Register);\n-    if (bt == T_BYTE) {\n-      __ sxtb($dst$$Register, $dst$$Register);\n-    } else if (bt == T_SHORT) {\n-      __ sxth($dst$$Register, $dst$$Register);\n-    } else {\n-      assert(bt == T_INT, \"unsupported type\");\n-    }\n-  %}\n-  ins_pipe(pipe_slow);\n-%}\n-\n-instruct reduce_eorL(iRegLNoSp dst, iRegL src1, vReg src2, vRegD vtmp) %{\n-  predicate(UseSVE > 0 && n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_LONG &&\n-            n->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize);\n-  match(Set dst (XorReductionV src1 src2));\n-  effect(TEMP_DEF dst, TEMP vtmp);\n-  ins_cost(SVE_COST);\n-  format %{ \"sve_reduce_eorL $dst, $src1, $src2\\t# xorL reduction (sve)\" %}\n-  ins_encode %{\n-    __ sve_eorv(as_FloatRegister($vtmp$$reg), __ D, ptrue, as_FloatRegister($src2$$reg));\n-    __ umov($dst$$Register, as_FloatRegister($vtmp$$reg), __ D, 0);\n-    __ eor($dst$$Register, $dst$$Register, $src1$$Register);\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, bt,\n+                           $src1$$Register, as_FloatRegister($src2$$reg),\n+                           as_PRegister($ptmp$$reg), as_FloatRegister($vtmp$$reg));\n@@ -1315,3 +1645,6 @@\n-%}\n-\n-instruct reduce_eorL_partial(iRegLNoSp dst, iRegL src1, vReg src2, vRegD vtmp,\n+%}')dnl\n+dnl\n+dnl REDUCE_MAXMIN_L_PARTIAL($1     , $2     )\n+dnl REDUCE_MAXMIN_L_PARTIAL(min_max, op_name)\n+define(`REDUCE_MAXMIN_L_PARTIAL', `\n+instruct reduce_$1L_partial(iRegLNoSp dst, iRegL src1, vReg src2, vRegD vtmp,\n@@ -1319,3 +1652,4 @@\n-  predicate(UseSVE > 0 && n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_LONG &&\n-            n->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize);\n-  match(Set dst (XorReductionV src1 src2));\n+  predicate(UseSVE > 0 &&\n+            n->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize &&\n+            n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_LONG);\n+  match(Set dst ($2 src1 src2));\n@@ -1323,2 +1657,2 @@\n-  ins_cost(SVE_COST);\n-  format %{ \"sve_reduce_eorL $dst, $src1, $src2\\t# xorL reduction partial (sve)\" %}\n+  ins_cost(2 * SVE_COST);\n+  format %{ \"sve_reduce_$1L $dst, $src1, $src2\\t# $1L reduction  partial (sve)\" %}\n@@ -1328,4 +1662,3 @@\n-    __ sve_eorv(as_FloatRegister($vtmp$$reg), __ D,\n-                as_PRegister($ptmp$$reg), as_FloatRegister($src2$$reg));\n-    __ umov($dst$$Register, as_FloatRegister($vtmp$$reg), __ D, 0);\n-    __ eor($dst$$Register, $dst$$Register, $src1$$Register);\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, T_LONG,\n+                           $src1$$Register, as_FloatRegister($src2$$reg),\n+                           as_PRegister($ptmp$$reg), as_FloatRegister($vtmp$$reg));\n@@ -1334,2 +1667,1 @@\n-%}\n-\n+%}')dnl\n@@ -1337,10 +1669,11 @@\n-dnl REDUCE_MAXMIN_I($1,      $2,      $3 )\n-dnl REDUCE_MAXMIN_I(min_max, op_mame, cmp)\n-define(`REDUCE_MAXMIN_I', `\n-instruct reduce_$1I(iRegINoSp dst, iRegIorL2I src1, vReg src2, vRegD vtmp) %{\n-  predicate(UseSVE > 0 && n->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize &&\n-            (n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_BYTE ||\n-             n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_SHORT ||\n-             n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_INT));\n-  match(Set dst ($2 src1 src2));\n-  effect(TEMP_DEF dst, TEMP vtmp);\n+dnl REDUCE_MAXMIN_I_PREDICATE($1     , $2     )\n+dnl REDUCE_MAXMIN_I_PREDICATE(min_max, op_name)\n+define(`REDUCE_MAXMIN_I_PREDICATE', `\n+instruct reduce_$1I_masked(iRegINoSp dst, iRegIorL2I src1, vReg src2, vRegD tmp,\n+                           pRegGov pg, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->element_basic_type() != T_LONG &&\n+            is_integral_type(n->in(1)->in(2)->bottom_type()->is_vect()->element_basic_type()));\n+  match(Set dst ($2 (Binary src1 src2) pg));\n+  effect(TEMP_DEF dst, TEMP tmp, KILL cr);\n@@ -1348,1 +1681,1 @@\n-  format %{ \"sve_reduce_$1I $dst, $src1, $src2\\t# reduce $1B\/S\/I (sve)\" %}\n+  format %{ \"sve_reduce_$1I $dst, $src1, $pg, $src2\\t# $1I reduction predicated (sve)\" %}\n@@ -1351,5 +1684,3 @@\n-    Assembler::SIMD_RegVariant variant = __ elemType_to_regVariant(bt);\n-    __ sve_s$1v(as_FloatRegister($vtmp$$reg), variant, ptrue, as_FloatRegister($src2$$reg));\n-    __ smov($dst$$Register, as_FloatRegister($vtmp$$reg), variant, 0);\n-    __ cmpw($dst$$Register, $src1$$Register);\n-    __ cselw(as_Register($dst$$reg), as_Register($dst$$reg), as_Register($src1$$reg), Assembler::$3);\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, bt,\n+                           $src1$$Register, as_FloatRegister($src2$$reg),\n+                           as_PRegister($pg$$reg), as_FloatRegister($tmp$$reg));\n@@ -1360,8 +1691,10 @@\n-dnl REDUCE_MAXMIN_L($1,      $2,      $3 )\n-dnl REDUCE_MAXMIN_L(min_max, op_name, cmp)\n-define(`REDUCE_MAXMIN_L', `\n-instruct reduce_$1L(iRegLNoSp dst, iRegL src1, vReg src2, vRegD vtmp) %{\n-  predicate(UseSVE > 0 && n->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize &&\n-            n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_LONG);\n-  match(Set dst ($2 src1 src2));\n-  effect(TEMP_DEF dst, TEMP vtmp);\n+dnl REDUCE_MAXMIN_L_PREDICATE($1     , $2     )\n+dnl REDUCE_MAXMIN_L_PREDICATE(min_max, op_name)\n+define(`REDUCE_MAXMIN_L_PREDICATE', `\n+instruct reduce_$1L_masked(iRegLNoSp dst, iRegL src1, vReg src2, vRegD tmp,\n+                          pRegGov pg, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->element_basic_type() == T_LONG);\n+  match(Set dst ($2 (Binary src1 src2) pg));\n+  effect(TEMP_DEF dst, TEMP tmp, KILL cr);\n@@ -1369,1 +1702,1 @@\n-  format %{ \"sve_reduce_$1L $dst, $src1, $src2\\t# reduce $1L partial (sve)\" %}\n+  format %{ \"sve_reduce_$1L $dst, $src1, $pg, $src2\\t# $1L reduction predicated (sve)\" %}\n@@ -1371,4 +1704,3 @@\n-    __ sve_s$1v(as_FloatRegister($vtmp$$reg), __ D, ptrue, as_FloatRegister($src2$$reg));\n-    __ umov($dst$$Register, as_FloatRegister($vtmp$$reg), __ D, 0);\n-    __ cmp($dst$$Register, $src1$$Register);\n-    __ csel(as_Register($dst$$reg), as_Register($dst$$reg), as_Register($src1$$reg), Assembler::$3);\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, T_LONG,\n+                           $src1$$Register, as_FloatRegister($src2$$reg),\n+                           as_PRegister($pg$$reg), as_FloatRegister($tmp$$reg));\n@@ -1379,10 +1711,10 @@\n-dnl REDUCE_MAXMIN_I_PARTIAL($1,      $2,      $3 )\n-dnl REDUCE_MAXMIN_I_PARTIAL(min_max, op_mame, cmp)\n-define(`REDUCE_MAXMIN_I_PARTIAL', `\n-instruct reduce_$1I_partial(iRegINoSp dst, iRegIorL2I src1, vReg src2, vRegD vtmp,\n-                             pRegGov ptmp, rFlagsReg cr) %{\n-  predicate(UseSVE > 0 && n->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize &&\n-            (n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_BYTE ||\n-             n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_SHORT ||\n-             n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_INT));\n-  match(Set dst ($2 src1 src2));\n+dnl REDUCE_MAXMIN_I_PREDICATE_PARTIAL($1     , $2     )\n+dnl REDUCE_MAXMIN_I_PREDICATE_PARTIAL(min_max, op_name)\n+define(`REDUCE_MAXMIN_I_PREDICATE_PARTIAL', `\n+instruct reduce_$1I_masked_partial(iRegINoSp dst, iRegIorL2I src1, vReg src2, vRegD vtmp,\n+                                  pRegGov pg, pRegGov ptmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->element_basic_type() != T_LONG &&\n+            is_integral_type(n->in(1)->in(2)->bottom_type()->is_vect()->element_basic_type()));\n+  match(Set dst ($2 (Binary src1 src2) pg));\n@@ -1390,2 +1722,2 @@\n-  ins_cost(SVE_COST);\n-  format %{ \"sve_reduce_$1I $dst, $src1, $src2\\t# reduce $1I partial (sve)\" %}\n+  ins_cost(3 * SVE_COST);\n+  format %{ \"sve_reduce_$1I $dst, $src1, $pg, $src2\\t# $1I reduction predicated partial (sve)\" %}\n@@ -1397,5 +1729,5 @@\n-    __ sve_s$1v(as_FloatRegister($vtmp$$reg), variant,\n-                 as_PRegister($ptmp$$reg), as_FloatRegister($src2$$reg));\n-    __ smov($dst$$Register, as_FloatRegister($vtmp$$reg), variant, 0);\n-    __ cmpw($dst$$Register, $src1$$Register);\n-    __ cselw(as_Register($dst$$reg), as_Register($dst$$reg), as_Register($src1$$reg), Assembler::$3);\n+    __ sve_and(as_PRegister($ptmp$$reg), as_PRegister($ptmp$$reg),\n+               as_PRegister($pg$$reg), as_PRegister($pg$$reg));\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, bt,\n+                           $src1$$Register, as_FloatRegister($src2$$reg),\n+                           as_PRegister($ptmp$$reg), as_FloatRegister($vtmp$$reg));\n@@ -1406,8 +1738,9 @@\n-dnl REDUCE_MAXMIN_L_PARTIAL($1,      $2,      $3 )\n-dnl REDUCE_MAXMIN_L_PARTIAL(min_max, op_name, cmp)\n-define(`REDUCE_MAXMIN_L_PARTIAL', `\n-instruct reduce_$1L_partial(iRegLNoSp dst, iRegL src1, vReg src2, vRegD vtmp,\n-                             pRegGov ptmp, rFlagsReg cr) %{\n-  predicate(UseSVE > 0 && n->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize &&\n-            n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_LONG);\n-  match(Set dst ($2 src1 src2));\n+dnl REDUCE_MAXMIN_L_PREDICATE_PARTIAL($1     , $2     )\n+dnl REDUCE_MAXMIN_L_PREDICATE_PARTIAL(min_max, op_name)\n+define(`REDUCE_MAXMIN_L_PREDICATE_PARTIAL', `\n+instruct reduce_$1L_masked_partial(iRegLNoSp dst, iRegL src1, vReg src2, vRegD vtmp,\n+                                  pRegGov pg, pRegGov ptmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->element_basic_type() == T_LONG);\n+  match(Set dst ($2 (Binary src1 src2) pg));\n@@ -1415,2 +1748,2 @@\n-  ins_cost(SVE_COST);\n-  format %{ \"sve_reduce_$1L $dst, $src1, $src2\\t# reduce $1L partial (sve)\" %}\n+  ins_cost(3 * SVE_COST);\n+  format %{ \"sve_reduce_$1L $dst, $src1, $pg, $src2\\t# $1L reduction predicated partial (sve)\" %}\n@@ -1420,5 +1753,5 @@\n-    __ sve_s$1v(as_FloatRegister($vtmp$$reg), __ D,\n-                 as_PRegister($ptmp$$reg), as_FloatRegister($src2$$reg));\n-    __ umov($dst$$Register, as_FloatRegister($vtmp$$reg), __ D, 0);\n-    __ cmp($dst$$Register, $src1$$Register);\n-    __ csel(as_Register($dst$$reg), as_Register($dst$$reg), as_Register($src1$$reg), Assembler::$3);\n+    __ sve_and(as_PRegister($ptmp$$reg), as_PRegister($ptmp$$reg),\n+               as_PRegister($pg$$reg), as_PRegister($pg$$reg));\n+    __ sve_reduce_integral(this->ideal_Opcode(), $dst$$Register, T_LONG,\n+                           $src1$$Register, as_FloatRegister($src2$$reg),\n+                           as_PRegister($ptmp$$reg), as_FloatRegister($vtmp$$reg));\n@@ -1433,1 +1766,2 @@\n-  predicate(UseSVE > 0 && n->in(2)->bottom_type()->is_vect()->element_basic_type() == $3 &&\n+  predicate(UseSVE > 0 &&\n+            n->in(2)->bottom_type()->is_vect()->element_basic_type() == $3 &&\n@@ -1438,2 +1772,1 @@\n-  format %{ \"sve_f$1v $dst, $src2 # vector (sve) ($4)\\n\\t\"\n-            \"f$1s $dst, $dst, $src1\\t# $1 reduction $2\" %}\n+  format %{ \"sve_reduce_$1$2 $dst, $src1, $src2\\t# $1$2 reduction (sve)\" %}\n@@ -1441,2 +1774,1 @@\n-    __ sve_f$1v(as_FloatRegister($dst$$reg), __ $4,\n-         ptrue, as_FloatRegister($src2$$reg));\n+    __ sve_f$1v(as_FloatRegister($dst$$reg), __ $4, ptrue, as_FloatRegister($src2$$reg));\n@@ -1448,1 +1780,0 @@\n-dnl\n@@ -1454,1 +1785,2 @@\n-  predicate(UseSVE > 0 && n->in(2)->bottom_type()->is_vect()->element_basic_type() == $3 &&\n+  predicate(UseSVE > 0 &&\n+            n->in(2)->bottom_type()->is_vect()->element_basic_type() == $3 &&\n@@ -1459,1 +1791,1 @@\n-  format %{ \"sve_reduce_$1$2 $dst, $src1, $src2\\t# reduce $1 $4 partial (sve)\" %}\n+  format %{ \"sve_reduce_$1$2 $dst, $src1, $src2\\t# $1$2 reduction partial (sve)\" %}\n@@ -1463,0 +1795,40 @@\n+    __ sve_f$1v(as_FloatRegister($dst$$reg), __ $4, as_PRegister($ptmp$$reg), as_FloatRegister($src2$$reg));\n+    __ f`$1'translit($4, `SD', `sd')(as_FloatRegister($dst$$reg), as_FloatRegister($dst$$reg), as_FloatRegister($src1$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}')dnl\n+dnl\n+dnl REDUCE_FMINMAX_PREDICATE($1,      $2,          $3,           $4,   $5         )\n+dnl REDUCE_FMINMAX_PREDICATE(min_max, name_suffix, element_type, size, reg_src_dst)\n+define(`REDUCE_FMINMAX_PREDICATE', `\n+instruct reduce_$1$2_masked($5 dst, $5 src1, vReg src2, pRegGov pg) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->element_basic_type() == $3 &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize);\n+  match(Set dst (translit($1, `m', `M')ReductionV (Binary src1 src2) pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_reduce_$1$2 $dst, $src1, $pg, $src2\\t# $1$2 reduction predicated (sve)\" %}\n+  ins_encode %{\n+    __ sve_f$1v(as_FloatRegister($dst$$reg), __ $4, as_PRegister($pg$$reg), as_FloatRegister($src2$$reg));\n+    __ f`$1'translit($4, `SD', `sd')(as_FloatRegister($dst$$reg), as_FloatRegister($dst$$reg), as_FloatRegister($src1$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}')dnl\n+dnl\n+dnl REDUCE_FMINMAX_PREDICATE_PARTIAL($1,      $2,          $3,           $4,   $5         )\n+dnl REDUCE_FMINMAX_PREDICATE_PARTIAL(min_max, name_suffix, element_type, size, reg_src_dst)\n+define(`REDUCE_FMINMAX_PREDICATE_PARTIAL', `\n+instruct reduce_$1$2_masked_partial($5 dst, $5 src1, vReg src2, pRegGov pg,\n+                                    pRegGov ptmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->element_basic_type() == $3 &&\n+            n->in(1)->in(2)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize);\n+  match(Set dst (translit($1, `m', `M')ReductionV (Binary src1 src2) pg));\n+  effect(TEMP_DEF dst, TEMP ptmp, KILL cr);\n+  ins_cost(3 * SVE_COST);\n+  format %{ \"sve_reduce_$1$2 $dst, $src1, $pg, $src2\\t# $1$2 reduction predicated partial (sve)\" %}\n+  ins_encode %{\n+    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), __ $4,\n+                          Matcher::vector_length(this, $src2));\n+    __ sve_and(as_PRegister($ptmp$$reg), as_PRegister($ptmp$$reg),\n+               as_PRegister($pg$$reg), as_PRegister($pg$$reg));\n@@ -1464,1 +1836,1 @@\n-         as_PRegister($ptmp$$reg), as_FloatRegister($src2$$reg));\n+               as_PRegister($ptmp$$reg), as_FloatRegister($src2$$reg));\n@@ -1469,1 +1841,0 @@\n-\n@@ -1471,4 +1842,4 @@\n-REDUCE_MAXMIN_I(max, MaxReductionV, GT)\n-REDUCE_MAXMIN_I_PARTIAL(max, MaxReductionV, GT)\n-REDUCE_MAXMIN_L(max, MaxReductionV, GT)\n-REDUCE_MAXMIN_L_PARTIAL(max, MaxReductionV, GT)\n+REDUCE_MAXMIN_I(max, MaxReductionV)\n+REDUCE_MAXMIN_L(max, MaxReductionV)\n+REDUCE_MAXMIN_I_PARTIAL(max, MaxReductionV)\n+REDUCE_MAXMIN_L_PARTIAL(max, MaxReductionV)\n@@ -1480,0 +1851,10 @@\n+\/\/ vector max reduction - predicated\n+REDUCE_MAXMIN_I_PREDICATE(max, MaxReductionV)\n+REDUCE_MAXMIN_L_PREDICATE(max, MaxReductionV)\n+REDUCE_MAXMIN_I_PREDICATE_PARTIAL(max, MaxReductionV)\n+REDUCE_MAXMIN_L_PREDICATE_PARTIAL(max, MaxReductionV)\n+REDUCE_FMINMAX_PREDICATE(max, F, T_FLOAT,  S, vRegF)\n+REDUCE_FMINMAX_PREDICATE(max, D, T_DOUBLE, D, vRegD)\n+REDUCE_FMINMAX_PREDICATE_PARTIAL(max, F, T_FLOAT,  S, vRegF)\n+REDUCE_FMINMAX_PREDICATE_PARTIAL(max, D, T_DOUBLE, D, vRegD)\n+\n@@ -1481,4 +1862,4 @@\n-REDUCE_MAXMIN_I(min, MinReductionV, LT)\n-REDUCE_MAXMIN_I_PARTIAL(min, MinReductionV, LT)\n-REDUCE_MAXMIN_L(min, MinReductionV, LT)\n-REDUCE_MAXMIN_L_PARTIAL(min, MinReductionV, LT)\n+REDUCE_MAXMIN_I(min, MinReductionV)\n+REDUCE_MAXMIN_L(min, MinReductionV)\n+REDUCE_MAXMIN_I_PARTIAL(min, MinReductionV)\n+REDUCE_MAXMIN_L_PARTIAL(min, MinReductionV)\n@@ -1490,0 +1871,10 @@\n+\/\/ vector min reduction - predicated\n+REDUCE_MAXMIN_I_PREDICATE(min, MinReductionV)\n+REDUCE_MAXMIN_L_PREDICATE(min, MinReductionV)\n+REDUCE_MAXMIN_I_PREDICATE_PARTIAL(min, MinReductionV)\n+REDUCE_MAXMIN_L_PREDICATE_PARTIAL(min, MinReductionV)\n+REDUCE_FMINMAX_PREDICATE(min, F, T_FLOAT,  S, vRegF)\n+REDUCE_FMINMAX_PREDICATE(min, D, T_DOUBLE, D, vRegD)\n+REDUCE_FMINMAX_PREDICATE_PARTIAL(min, F, T_FLOAT,  S, vRegF)\n+REDUCE_FMINMAX_PREDICATE_PARTIAL(min, D, T_DOUBLE, D, vRegD)\n+\n@@ -1667,3 +2058,42 @@\n-\/\/ vector sqrt\n-UNARY_OP_TRUE_PREDICATE(vsqrtF, SqrtVF, S, 16, sve_fsqrt)\n-UNARY_OP_TRUE_PREDICATE(vsqrtD, SqrtVD, D, 16, sve_fsqrt)\n+\/\/ vector shift - predicated\n+BINARY_OP_PREDICATE(vasrB, RShiftVB,  B, sve_asr)\n+BINARY_OP_PREDICATE(vasrS, RShiftVS,  H, sve_asr)\n+BINARY_OP_PREDICATE(vasrI, RShiftVI,  S, sve_asr)\n+BINARY_OP_PREDICATE(vasrL, RShiftVL,  D, sve_asr)\n+BINARY_OP_PREDICATE(vlslB, LShiftVB,  B, sve_lsl)\n+BINARY_OP_PREDICATE(vlslS, LShiftVS,  H, sve_lsl)\n+BINARY_OP_PREDICATE(vlslI, LShiftVI,  S, sve_lsl)\n+BINARY_OP_PREDICATE(vlslL, LShiftVL,  D, sve_lsl)\n+BINARY_OP_PREDICATE(vlsrB, URShiftVB, B, sve_lsr)\n+BINARY_OP_PREDICATE(vlsrS, URShiftVS, H, sve_lsr)\n+BINARY_OP_PREDICATE(vlsrI, URShiftVI, S, sve_lsr)\n+BINARY_OP_PREDICATE(vlsrL, URShiftVL, D, sve_lsr)\n+dnl\n+dnl VSHIFT_IMM_PREDICATED($1,        $2,      $3,       $4,   $5,   $6  )\n+dnl VSHIFT_IMM_PREDICATED(insn_name, op_name, op_name2, type, size, insn)\n+define(`VSHIFT_IMM_PREDICATED', `\n+instruct $1_imm_masked(vReg dst_src, immI shift, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src ($2 (Binary dst_src ($3 shift)) pg));\n+  ins_cost(SVE_COST);\n+  format %{ \"$6 $dst_src, $pg, $dst_src, $shift\\t# vector (sve) ($4)\" %}\n+  ins_encode %{\n+    int con = (int)$shift$$constant;\n+    assert(con ifelse(index(`$1', `vlsl'), 0, `>=', `>') 0 && con < $5, \"invalid shift immediate\");\n+    __ $6(as_FloatRegister($dst_src$$reg), __ $4, as_PRegister($pg$$reg), con);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}')dnl\n+dnl\n+VSHIFT_IMM_PREDICATED(vasrB, RShiftVB,  RShiftCntV, B, 8,  sve_asr)\n+VSHIFT_IMM_PREDICATED(vasrS, RShiftVS,  RShiftCntV, H, 16, sve_asr)\n+VSHIFT_IMM_PREDICATED(vasrI, RShiftVI,  RShiftCntV, S, 32, sve_asr)\n+VSHIFT_IMM_PREDICATED(vasrL, RShiftVL,  RShiftCntV, D, 64, sve_asr)\n+VSHIFT_IMM_PREDICATED(vlsrB, URShiftVB, RShiftCntV, B, 8,  sve_lsr)\n+VSHIFT_IMM_PREDICATED(vlsrS, URShiftVS, RShiftCntV, H, 16, sve_lsr)\n+VSHIFT_IMM_PREDICATED(vlsrI, URShiftVI, RShiftCntV, S, 32, sve_lsr)\n+VSHIFT_IMM_PREDICATED(vlsrL, URShiftVL, RShiftCntV, D, 64, sve_lsr)\n+VSHIFT_IMM_PREDICATED(vlslB, LShiftVB,  LShiftCntV, B, 8,  sve_lsl)\n+VSHIFT_IMM_PREDICATED(vlslS, LShiftVS,  LShiftCntV, H, 16, sve_lsl)\n+VSHIFT_IMM_PREDICATED(vlslI, LShiftVI,  LShiftCntV, S, 32, sve_lsl)\n+VSHIFT_IMM_PREDICATED(vlslL, LShiftVL,  LShiftCntV, D, 64, sve_lsl)\n@@ -1671,7 +2101,3 @@\n-\/\/ vector sub\n-BINARY_OP_UNPREDICATED(vsubB, SubVB, B, 16, sve_sub)\n-BINARY_OP_UNPREDICATED(vsubS, SubVS, H, 8, sve_sub)\n-BINARY_OP_UNPREDICATED(vsubI, SubVI, S, 4, sve_sub)\n-BINARY_OP_UNPREDICATED(vsubL, SubVL, D, 2, sve_sub)\n-BINARY_OP_UNPREDICATED(vsubF, SubVF, S, 4, sve_fsub)\n-BINARY_OP_UNPREDICATED(vsubD, SubVD, D, 2, sve_fsub)\n+\/\/ vector sqrt\n+UNARY_OP_TRUE_PREDICATE(vsqrtF, SqrtVF, S, sve_fsqrt)\n+UNARY_OP_TRUE_PREDICATE(vsqrtD, SqrtVD, D, sve_fsqrt)\n@@ -1679,1 +2105,3 @@\n-\/\/ vector mask cast\n+\/\/ vector sqrt - predicated\n+UNARY_OP_PREDICATE(vsqrtF, SqrtVF, S, sve_fsqrt)\n+UNARY_OP_PREDICATE(vsqrtD, SqrtVD, D, sve_fsqrt)\n@@ -1681,2 +2109,21 @@\n-instruct vmaskcast(vReg dst) %{\n-  predicate(UseSVE > 0 && n->bottom_type()->is_vect()->length() == n->in(1)->bottom_type()->is_vect()->length() &&\n+\/\/ vector sub\n+BINARY_OP_UNPREDICATE(vsubB, SubVB, B, 16, sve_sub)\n+BINARY_OP_UNPREDICATE(vsubS, SubVS, H, 8, sve_sub)\n+BINARY_OP_UNPREDICATE(vsubI, SubVI, S, 4, sve_sub)\n+BINARY_OP_UNPREDICATE(vsubL, SubVL, D, 2, sve_sub)\n+BINARY_OP_UNPREDICATE(vsubF, SubVF, S, 4, sve_fsub)\n+BINARY_OP_UNPREDICATE(vsubD, SubVD, D, 2, sve_fsub)\n+\n+\/\/ vector sub - predicated\n+BINARY_OP_PREDICATE(vsubB, SubVB, B, sve_sub)\n+BINARY_OP_PREDICATE(vsubS, SubVS, H, sve_sub)\n+BINARY_OP_PREDICATE(vsubI, SubVI, S, sve_sub)\n+BINARY_OP_PREDICATE(vsubL, SubVL, D, sve_sub)\n+BINARY_OP_PREDICATE(vsubF, SubVF, S, sve_fsub)\n+BINARY_OP_PREDICATE(vsubD, SubVD, D, sve_fsub)\n+\n+\/\/ ------------------------------ Vector mask cast --------------------------\n+\n+instruct vmaskcast(pRegGov dst_src) %{\n+  predicate(UseSVE > 0 &&\n+            n->bottom_type()->is_vect()->length() == n->in(1)->bottom_type()->is_vect()->length() &&\n@@ -1684,1 +2131,1 @@\n-  match(Set dst (VectorMaskCast dst));\n+  match(Set dst_src (VectorMaskCast dst_src));\n@@ -1686,1 +2133,1 @@\n-  format %{ \"vmaskcast $dst\\t# empty (sve)\" %}\n+  format %{ \"vmaskcast $dst_src\\t# empty (sve)\" %}\n@@ -1693,0 +2140,33 @@\n+instruct vmaskcast_extend(pRegGov dst, pReg src)\n+%{\n+  predicate(UseSVE > 0 &&\n+            (Matcher::vector_length_in_bytes(n) == 2 * Matcher::vector_length_in_bytes(n->in(1)) ||\n+             Matcher::vector_length_in_bytes(n) == 4 * Matcher::vector_length_in_bytes(n->in(1)) ||\n+             Matcher::vector_length_in_bytes(n) == 8 * Matcher::vector_length_in_bytes(n->in(1))));\n+  match(Set dst (VectorMaskCast src));\n+  ins_cost(SVE_COST * 3);\n+  format %{ \"sve_vmaskcast_extend  $dst, $src\\t# extend predicate $src\" %}\n+  ins_encode %{\n+    __ sve_vmaskcast_extend(as_PRegister($dst$$reg), as_PRegister($src$$reg),\n+                            Matcher::vector_length_in_bytes(this), Matcher::vector_length_in_bytes(this, $src));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vmaskcast_narrow(pRegGov dst, pReg src)\n+%{\n+  predicate(UseSVE > 0 &&\n+            (Matcher::vector_length_in_bytes(n) * 2 == Matcher::vector_length_in_bytes(n->in(1)) ||\n+             Matcher::vector_length_in_bytes(n) * 4 == Matcher::vector_length_in_bytes(n->in(1)) ||\n+             Matcher::vector_length_in_bytes(n) * 8 == Matcher::vector_length_in_bytes(n->in(1))));\n+  match(Set dst (VectorMaskCast src));\n+  ins_cost(SVE_COST * 3);\n+  format %{ \"sve_vmaskcast_narrow  $dst, $src\\t# narrow predicate $src\" %}\n+  ins_encode %{\n+    __ sve_vmaskcast_narrow(as_PRegister($dst$$reg), as_PRegister($src$$reg),\n+                            Matcher::vector_length_in_bytes(this), Matcher::vector_length_in_bytes(this, $src));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+dnl\n+\n@@ -1696,1 +2176,1 @@\n-define(`VECTOR_CAST_EXTEND1', `\n+define(`VECTOR_CAST_X2X', `\n@@ -1703,1 +2183,1 @@\n-  format %{ \"sve_$3  $dst, $4, $src\\t# convert $1 to $2 vector\" %}\n+  format %{ \"sve_vectorcast_$5  $dst, $src\\t# convert $1 to $2 vector\" %}\n@@ -1705,1 +2185,1 @@\n-    __ sve_$3(as_FloatRegister($dst$$reg), __ $4, as_FloatRegister($src$$reg));\n+    __ sve_$3(as_FloatRegister($dst$$reg), __ $4, ptrue, as_FloatRegister($src$$reg), __ $4);\n@@ -1709,0 +2189,1 @@\n+\n@@ -1710,0 +2191,1 @@\n+dnl Start of vector cast rules\n@@ -1711,2 +2193,1 @@\n-define(`VECTOR_CAST_EXTEND2', `\n-instruct vcvt$1to$2`'(vReg dst, vReg src)\n+instruct vcvtBtoX_extend(vReg dst, vReg src)\n@@ -1714,3 +2195,2 @@\n-  predicate(UseSVE > 0 &&\n-            n->bottom_type()->is_vect()->element_basic_type() == T_`'TYPE2DATATYPE($2));\n-  match(Set dst (VectorCast$1`'2X src));\n+  predicate(UseSVE > 0);\n+  match(Set dst (VectorCastB2X src));\n@@ -1718,2 +2198,1 @@\n-  format %{ \"sve_$3  $dst, $4, $src\\n\\t\"\n-            \"sve_$3  $dst, $5, $dst\\t# convert $1 to $2 vector\" %}\n+  format %{ \"sve_vectorcast_b2x  $dst, $src\\t# convert B to X vector (extend)\" %}\n@@ -1721,2 +2200,6 @@\n-    __ sve_$3(as_FloatRegister($dst$$reg), __ $4, as_FloatRegister($src$$reg));\n-    __ sve_$3(as_FloatRegister($dst$$reg), __ $5, as_FloatRegister($dst$$reg));\n+    BasicType to_bt = Matcher::vector_element_basic_type(this);\n+    Assembler::SIMD_RegVariant to_size = __ elemType_to_regVariant(to_bt);\n+    __ sve_vector_extend(as_FloatRegister($dst$$reg), to_size, as_FloatRegister($src$$reg), __ B);\n+    if (to_bt == T_FLOAT || to_bt == T_DOUBLE) {\n+      __ sve_scvtf(as_FloatRegister($dst$$reg), to_size, ptrue, as_FloatRegister($dst$$reg), to_size);\n+    }\n@@ -1725,5 +2208,3 @@\n-%}')dnl\n-dnl\n-dnl\n-define(`VECTOR_CAST_EXTEND3', `\n-instruct vcvt$1to$2`'(vReg dst, vReg src)\n+%}\n+\n+instruct vcvtStoB(vReg dst, vReg src, vReg tmp)\n@@ -1732,6 +2213,5 @@\n-            n->bottom_type()->is_vect()->element_basic_type() == T_`'TYPE2DATATYPE($2));\n-  match(Set dst (VectorCast$1`'2X src));\n-  ins_cost(3 * SVE_COST);\n-  format %{ \"sve_$3  $dst, $4, $src\\n\\t\"\n-            \"sve_$3  $dst, $5, $dst\\n\\t\"\n-            \"sve_$3  $dst, $6, $dst\\t# convert $1 to $2 vector\" %}\n+            n->bottom_type()->is_vect()->element_basic_type() == T_BYTE);\n+  match(Set dst (VectorCastS2X src));\n+  effect(TEMP tmp);\n+  ins_cost(2 * SVE_COST);\n+  format %{ \"sve_vectorcast_s2b  $dst, $src\\t# convert H to B vector\" %}\n@@ -1739,3 +2219,2 @@\n-    __ sve_$3(as_FloatRegister($dst$$reg), __ $4, as_FloatRegister($src$$reg));\n-    __ sve_$3(as_FloatRegister($dst$$reg), __ $5, as_FloatRegister($dst$$reg));\n-    __ sve_$3(as_FloatRegister($dst$$reg), __ $6, as_FloatRegister($dst$$reg));\n+    __ sve_vector_narrow(as_FloatRegister($dst$$reg), __ B,\n+                         as_FloatRegister($src$$reg), __ H, as_FloatRegister($tmp$$reg));\n@@ -1744,5 +2223,3 @@\n-%}')dnl\n-dnl\n-dnl\n-define(`VECTOR_CAST_NARROW1', `\n-instruct vcvt$1to$2`'(vReg dst, vReg src, vReg tmp)\n+%}\n+\n+instruct vcvtStoX_extend(vReg dst, vReg src)\n@@ -1751,3 +2228,2 @@\n-            n->bottom_type()->is_vect()->element_basic_type() == T_`'TYPE2DATATYPE($2));\n-  match(Set dst (VectorCast$1`'2X src));\n-  effect(TEMP tmp);\n+            type2aelembytes(Matcher::vector_element_basic_type(n)) > 2);\n+  match(Set dst (VectorCastS2X src));\n@@ -1755,2 +2231,1 @@\n-  format %{ \"sve_$3  $tmp, $4, 0\\n\\t\"\n-            \"sve_$5  $dst, $4, $src, tmp\\t# convert $1 to $2 vector\" %}\n+  format %{ \"sve_vectorcast_s2x  $dst, $src\\t# convert H to X vector (extend)\" %}\n@@ -1758,2 +2233,6 @@\n-    __ sve_$3(as_FloatRegister($tmp$$reg), __ $4, 0);\n-    __ sve_$5(as_FloatRegister($dst$$reg), __ $4, as_FloatRegister($src$$reg), as_FloatRegister($tmp$$reg));\n+    BasicType to_bt = Matcher::vector_element_basic_type(this);\n+    Assembler::SIMD_RegVariant to_size = __ elemType_to_regVariant(to_bt);\n+    __ sve_vector_extend(as_FloatRegister($dst$$reg), to_size, as_FloatRegister($src$$reg), __ H);\n+    if (to_bt == T_FLOAT || to_bt == T_DOUBLE) {\n+      __ sve_scvtf(as_FloatRegister($dst$$reg), to_size, ptrue, as_FloatRegister($dst$$reg), to_size);\n+    }\n@@ -1762,5 +2241,3 @@\n-%}')dnl\n-dnl\n-dnl\n-define(`VECTOR_CAST_NARROW2', `\n-instruct vcvt$1to$2`'(vReg dst, vReg src, vReg tmp)\n+%}\n+\n+instruct vcvtItoB(vReg dst, vReg src, vReg tmp)\n@@ -1769,2 +2246,2 @@\n-            n->bottom_type()->is_vect()->element_basic_type() == T_`'TYPE2DATATYPE($2));\n-  match(Set dst (VectorCast$1`'2X src));\n+            n->bottom_type()->is_vect()->element_basic_type() == T_BYTE);\n+  match(Set dst (VectorCastI2X src));\n@@ -1773,3 +2250,1 @@\n-  format %{ \"sve_$3  $tmp, $4, 0\\n\\t\"\n-            \"sve_$5  $dst, $4, $src, tmp\\n\\t\"\n-            \"sve_$5  $dst, $6, $dst, tmp\\n\\t# convert $1 to $2 vector\" %}\n+  format %{ \"sve_vectorcast_i2b  $dst, $src\\t# convert I to B vector\" %}\n@@ -1777,3 +2252,2 @@\n-    __ sve_$3(as_FloatRegister($tmp$$reg), __ $4, 0);\n-    __ sve_$5(as_FloatRegister($dst$$reg), __ $4, as_FloatRegister($src$$reg), as_FloatRegister($tmp$$reg));\n-    __ sve_$5(as_FloatRegister($dst$$reg), __ $6, as_FloatRegister($dst$$reg), as_FloatRegister($tmp$$reg));\n+    __ sve_vector_narrow(as_FloatRegister($dst$$reg), __ B,\n+                         as_FloatRegister($src$$reg), __ S, as_FloatRegister($tmp$$reg));\n@@ -1782,5 +2256,3 @@\n-%}')dnl\n-dnl\n-dnl\n-define(`VECTOR_CAST_NARROW3', `\n-instruct vcvt$1to$2`'(vReg dst, vReg src, vReg tmp)\n+%}\n+\n+instruct vcvtItoS(vReg dst, vReg src, vReg tmp)\n@@ -1789,8 +2261,5 @@\n-            n->bottom_type()->is_vect()->element_basic_type() == T_`'TYPE2DATATYPE($2));\n-  match(Set dst (VectorCast$1`'2X src));\n-  effect(TEMP_DEF dst, TEMP tmp);\n-  ins_cost(4 * SVE_COST);\n-  format %{ \"sve_$3  $tmp, $4, 0\\n\\t\"\n-            \"sve_$5  $dst, $4, $src, tmp\\n\\t\"\n-            \"sve_$5  $dst, $6, $dst, tmp\\n\\t\"\n-            \"sve_$5  $dst, $7, $dst, tmp\\n\\t# convert $1 to $2 vector\" %}\n+            n->bottom_type()->is_vect()->element_basic_type() == T_SHORT);\n+  match(Set dst (VectorCastI2X src));\n+  effect(TEMP tmp);\n+  ins_cost(2 * SVE_COST);\n+  format %{ \"sve_vectorcast_i2s $dst, $src\\t# convert I to H vector\" %}\n@@ -1798,4 +2267,2 @@\n-    __ sve_$3(as_FloatRegister($tmp$$reg), __ $4, 0);\n-    __ sve_$5(as_FloatRegister($dst$$reg), __ $4, as_FloatRegister($src$$reg), as_FloatRegister($tmp$$reg));\n-    __ sve_$5(as_FloatRegister($dst$$reg), __ $6, as_FloatRegister($dst$$reg), as_FloatRegister($tmp$$reg));\n-    __ sve_$5(as_FloatRegister($dst$$reg), __ $7, as_FloatRegister($dst$$reg), as_FloatRegister($tmp$$reg));\n+    __ sve_vector_narrow(as_FloatRegister($dst$$reg), __ H,\n+                         as_FloatRegister($src$$reg), __ S, as_FloatRegister($tmp$$reg));\n@@ -1804,5 +2271,3 @@\n-%}')dnl\n-dnl\n-dnl\n-define(`VECTOR_CAST_I2F_EXTEND2', `\n-instruct vcvt$1to$2`'(vReg dst, vReg src)\n+%}\n+\n+instruct vcvtItoL(vReg dst, vReg src)\n@@ -1811,6 +2276,4 @@\n-            n->bottom_type()->is_vect()->element_basic_type() == T_`'TYPE2DATATYPE($2));\n-  match(Set dst (VectorCast$1`'2X src));\n-  ins_cost(3 * SVE_COST);\n-  format %{ \"sve_$3  $dst, $4, $src\\n\\t\"\n-            \"sve_$3  $dst, $5, $dst\\n\\t\"\n-            \"sve_$6  $dst, $5, $dst, $5\\t# convert $1 to $2 vector\" %}\n+            n->bottom_type()->is_vect()->element_basic_type() == T_LONG);\n+  match(Set dst (VectorCastI2X src));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_vectorcast_i2l  $dst, $src\\t# convert I to L vector\" %}\n@@ -1818,3 +2281,1 @@\n-    __ sve_$3(as_FloatRegister($dst$$reg), __ $4, as_FloatRegister($src$$reg));\n-    __ sve_$3(as_FloatRegister($dst$$reg), __ $5, as_FloatRegister($dst$$reg));\n-    __ sve_$6(as_FloatRegister($dst$$reg), __ $5, ptrue, as_FloatRegister($dst$$reg), __ $5);\n+    __ sve_vector_extend(as_FloatRegister($dst$$reg), __ D, as_FloatRegister($src$$reg), __ S);\n@@ -1823,2 +2284,1 @@\n-%}')dnl\n-dnl\n+%}\n@@ -1826,2 +2286,4 @@\n-define(`VECTOR_CAST_I2F_EXTEND3', `\n-instruct vcvt$1to$2`'(vReg dst, vReg src)\n+dnl vcvtItoF\n+VECTOR_CAST_X2X(I, F, scvtf, S, i2f)\n+\n+instruct vcvtItoD(vReg dst, vReg src)\n@@ -1830,7 +2292,4 @@\n-            n->bottom_type()->is_vect()->element_basic_type() == T_`'TYPE2DATATYPE($2));\n-  match(Set dst (VectorCast$1`'2X src));\n-  ins_cost(4 * SVE_COST);\n-  format %{ \"sve_$3  $dst, $4, $src\\n\\t\"\n-            \"sve_$3  $dst, $5, $dst\\n\\t\"\n-            \"sve_$3  $dst, $6, $dst\\n\\t\"\n-            \"sve_$7  $dst, $6, $dst, $6\\t# convert $1 to $2 vector\" %}\n+            n->bottom_type()->is_vect()->element_basic_type() == T_DOUBLE);\n+  match(Set dst (VectorCastI2X src));\n+  ins_cost(2 * SVE_COST);\n+  format %{ \"sve_vectorcast_i2d  $dst, $src\\t# convert I to D vector\" %}\n@@ -1838,4 +2297,2 @@\n-    __ sve_$3(as_FloatRegister($dst$$reg), __ $4, as_FloatRegister($src$$reg));\n-    __ sve_$3(as_FloatRegister($dst$$reg), __ $5, as_FloatRegister($dst$$reg));\n-    __ sve_$3(as_FloatRegister($dst$$reg), __ $6, as_FloatRegister($dst$$reg));\n-    __ sve_$7(as_FloatRegister($dst$$reg), __ $6, ptrue, as_FloatRegister($dst$$reg), __ $6);\n+    __ sve_sunpklo(as_FloatRegister($dst$$reg), __ D, as_FloatRegister($src$$reg));\n+    __ sve_scvtf(as_FloatRegister($dst$$reg), __ D, ptrue, as_FloatRegister($dst$$reg), __ D);\n@@ -1844,5 +2301,19 @@\n-%}')dnl\n-dnl\n-dnl\n-define(`VECTOR_CAST_X2F_NARROW1', `\n-instruct vcvt$1to$2`'(vReg dst, vReg src, vReg tmp)\n+%}\n+\n+instruct vcvtLtoX_narrow(vReg dst, vReg src, vReg tmp)\n+%{\n+  predicate(UseSVE > 0 && is_integral_type(Matcher::vector_element_basic_type(n)));\n+  match(Set dst (VectorCastL2X src));\n+  effect(TEMP_DEF dst, TEMP tmp);\n+  ins_cost(2 * SVE_COST);\n+  format %{ \"sve_vectorcast_l2x  $dst, $src\\t# convert L to B\/H\/S vector (narrow)\" %}\n+  ins_encode %{\n+    BasicType to_bt = Matcher::vector_element_basic_type(this);\n+    Assembler::SIMD_RegVariant to_size = __ elemType_to_regVariant(to_bt);\n+    __ sve_vector_narrow(as_FloatRegister($dst$$reg), to_size,\n+                         as_FloatRegister($src$$reg), __ D, as_FloatRegister($tmp$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vcvtLtoF(vReg dst, vReg src, vReg tmp)\n@@ -1851,2 +2322,2 @@\n-            n->bottom_type()->is_vect()->element_basic_type() == T_`'TYPE2DATATYPE($2));\n-  match(Set dst (VectorCast$1`'2X src));\n+            n->bottom_type()->is_vect()->element_basic_type() == T_FLOAT);\n+  match(Set dst (VectorCastL2X src));\n@@ -1855,3 +2326,1 @@\n-  format %{ \"sve_$3  $dst, $4, $src, $5\\n\\t\"\n-            \"sve_$6  $tmp, $7, 0\\n\\t\"\n-            \"sve_$8  $dst, $7, $dst, $tmp\\t# convert $1 to $2 vector\" %}\n+  format %{ \"sve_vectorcast_l2f  $dst, $src\\t# convert L to F vector\" %}\n@@ -1859,3 +2328,4 @@\n-    __ sve_$3(as_FloatRegister($dst$$reg), __ $4, ptrue, as_FloatRegister($src$$reg), __ $5);\n-    __ sve_$6(as_FloatRegister($tmp$$reg), __ $7, 0);\n-    __ sve_$8(as_FloatRegister($dst$$reg), __ $7, as_FloatRegister($dst$$reg), as_FloatRegister($tmp$$reg));\n+    __ sve_scvtf(as_FloatRegister($dst$$reg), __ S, ptrue, as_FloatRegister($src$$reg), __ D);\n+    __ sve_vector_narrow(as_FloatRegister($dst$$reg), __ S,\n+                         as_FloatRegister($dst$$reg), __ D, as_FloatRegister($tmp$$reg));\n+\n@@ -1864,2 +2334,1 @@\n-%}')dnl\n-dnl\n+%}\n@@ -1867,2 +2336,4 @@\n-define(`VECTOR_CAST_X2X', `\n-instruct vcvt$1to$2`'(vReg dst, vReg src)\n+dnl vcvtLtoD\n+VECTOR_CAST_X2X(L, D, scvtf, D, l2d)\n+\n+instruct vcvtFtoX_narrow(vReg dst, vReg src, vReg tmp)\n@@ -1871,4 +2342,6 @@\n-            n->bottom_type()->is_vect()->element_basic_type() == T_`'TYPE2DATATYPE($2));\n-  match(Set dst (VectorCast$1`'2X src));\n-  ins_cost(SVE_COST);\n-  format %{ \"sve_$3  $dst, $4, $src, $4\\t# convert $1 to $2 vector\" %}\n+            (n->bottom_type()->is_vect()->element_basic_type() == T_BYTE ||\n+             n->bottom_type()->is_vect()->element_basic_type() == T_SHORT));\n+  match(Set dst (VectorCastF2X src));\n+  effect(TEMP_DEF dst, TEMP tmp);\n+  ins_cost(3 * SVE_COST);\n+  format %{ \"sve_vectorcast_f2x  $dst, $src\\t# convert F to B\/H vector\" %}\n@@ -1876,1 +2349,5 @@\n-    __ sve_$3(as_FloatRegister($dst$$reg), __ $4, ptrue, as_FloatRegister($src$$reg), __ $4);\n+    BasicType to_bt = Matcher::vector_element_basic_type(this);\n+    Assembler::SIMD_RegVariant to_size = __ elemType_to_regVariant(to_bt);\n+    __ sve_fcvtzs(as_FloatRegister($dst$$reg), __ S, ptrue, as_FloatRegister($src$$reg), __ S);\n+    __ sve_vector_narrow(as_FloatRegister($dst$$reg), to_size,\n+                         as_FloatRegister($dst$$reg), __ S, as_FloatRegister($tmp$$reg));\n@@ -1879,5 +2356,3 @@\n-%}')dnl\n-dnl\n-dnl\n-define(`VECTOR_CAST_X2F_EXTEND1', `\n-instruct vcvt$1to$2`'(vReg dst, vReg src)\n+%}\n+\n+instruct vcvtFtoI(vReg dst, vReg src)\n@@ -1886,5 +2361,4 @@\n-            n->bottom_type()->is_vect()->element_basic_type() == T_`'TYPE2DATATYPE($2));\n-  match(Set dst (VectorCast$1`'2X src));\n-  ins_cost(2 * SVE_COST);\n-  format %{ \"sve_$3  $dst, $4, $src\\n\\t\"\n-            \"sve_$5  $dst, $4, $dst, $6\\t# convert $1 to $2 vector\" %}\n+            (n->bottom_type()->is_vect()->element_basic_type() == T_INT));\n+  match(Set dst (VectorCastF2X src));\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_vectorcast_f2x  $dst, $src\\t# convert F to I vector\" %}\n@@ -1892,2 +2366,1 @@\n-    __ sve_$3(as_FloatRegister($dst$$reg), __ $4, as_FloatRegister($src$$reg));\n-    __ sve_$5(as_FloatRegister($dst$$reg), __ $4, ptrue, as_FloatRegister($dst$$reg), __ $6);\n+    __ sve_fcvtzs(as_FloatRegister($dst$$reg), __ S, ptrue, as_FloatRegister($src$$reg), __ S);\n@@ -1896,5 +2369,3 @@\n-%}')dnl\n-dnl\n-dnl\n-define(`VECTOR_CAST_F2X_NARROW1', `\n-instruct vcvt$1to$2`'(vReg dst, vReg src, vReg tmp)\n+%}\n+\n+instruct vcvtFtoL(vReg dst, vReg src)\n@@ -1903,7 +2374,4 @@\n-            n->bottom_type()->is_vect()->element_basic_type() == T_`'TYPE2DATATYPE($2));\n-  match(Set dst (VectorCast$1`'2X src));\n-  effect(TEMP_DEF dst, TEMP tmp);\n-  ins_cost(3 * SVE_COST);\n-  format %{ \"sve_$3  $dst, $4, $src, $4\\n\\t\"\n-            \"sve_$5  $tmp, $6, 0\\n\\t\"\n-            \"sve_$7  $dst, $6, $dst, tmp\\t# convert $1 to $2 vector\" %}\n+            (n->bottom_type()->is_vect()->element_basic_type() == T_LONG));\n+  match(Set dst (VectorCastF2X src));\n+  ins_cost(SVE_COST * 2);\n+  format %{ \"sve_vectorcast_f2x  $dst, $src\\t# convert F to L vector\" %}\n@@ -1911,3 +2379,2 @@\n-    __ sve_$3(as_FloatRegister($dst$$reg), __ $4, ptrue, as_FloatRegister($src$$reg), __ $4);\n-    __ sve_$5(as_FloatRegister($tmp$$reg), __ $6, 0);\n-    __ sve_$7(as_FloatRegister($dst$$reg), __ $6, as_FloatRegister($dst$$reg), as_FloatRegister($tmp$$reg));\n+    __ sve_sunpklo(as_FloatRegister($dst$$reg), __ D, as_FloatRegister($src$$reg));\n+    __ sve_fcvtzs(as_FloatRegister($dst$$reg), __ D, ptrue, as_FloatRegister($dst$$reg), __ S);\n@@ -1916,5 +2383,3 @@\n-%}')dnl\n-dnl\n-dnl\n-define(`VECTOR_CAST_F2X_NARROW2', `\n-instruct vcvt$1to$2`'(vReg dst, vReg src, vReg tmp)\n+%}\n+\n+instruct vcvtFtoD(vReg dst, vReg src)\n@@ -1922,9 +2387,5 @@\n-  predicate(UseSVE > 0 &&\n-            n->bottom_type()->is_vect()->element_basic_type() == T_`'TYPE2DATATYPE($2));\n-  match(Set dst (VectorCast$1`'2X src));\n-  effect(TEMP_DEF dst, TEMP tmp);\n-  ins_cost(4 * SVE_COST);\n-  format %{ \"sve_$3  $dst, $4, $src, $4\\n\\t\"\n-            \"sve_$5  $tmp, $6, 0\\n\\t\"\n-            \"sve_$7  $dst, $6, $dst, tmp\\n\\t\"\n-            \"sve_$7  $dst, $8, $dst, tmp\\n\\t# convert $1 to $2 vector\" %}\n+  predicate(UseSVE > 0 &&\n+            n->bottom_type()->is_vect()->element_basic_type() == T_DOUBLE);\n+  match(Set dst (VectorCastF2X src));\n+  ins_cost(2 * SVE_COST);\n+  format %{ \"sve_vectorcast_f2d  $dst, $dst\\t# convert F to D vector\" %}\n@@ -1932,4 +2393,2 @@\n-    __ sve_$3(as_FloatRegister($dst$$reg), __ $4, ptrue, as_FloatRegister($src$$reg), __ $4);\n-    __ sve_$5(as_FloatRegister($tmp$$reg), __ $6, 0);\n-    __ sve_$7(as_FloatRegister($dst$$reg), __ $6, as_FloatRegister($dst$$reg), as_FloatRegister($tmp$$reg));\n-    __ sve_$7(as_FloatRegister($dst$$reg), __ $8, as_FloatRegister($dst$$reg), as_FloatRegister($tmp$$reg));\n+    __ sve_vector_extend(as_FloatRegister($dst$$reg), __ D, as_FloatRegister($src$$reg), __ S);\n+    __ sve_fcvt(as_FloatRegister($dst$$reg), __ D, ptrue, as_FloatRegister($dst$$reg), __ S);\n@@ -1938,5 +2397,3 @@\n-%}')dnl\n-dnl\n-dnl\n-define(`VECTOR_CAST_F2X_EXTEND1', `\n-instruct vcvt$1to$2`'(vReg dst, vReg src)\n+%}\n+\n+instruct vcvtDtoX_narrow(vReg dst, vReg src, vReg tmp)\n@@ -1945,5 +2402,7 @@\n-            n->bottom_type()->is_vect()->element_basic_type() == T_`'TYPE2DATATYPE($2));\n-  match(Set dst (VectorCast$1`'2X src));\n-  ins_cost(2 * SVE_COST);\n-  format %{ \"sve_$3  $dst, $4, $src, $4\\n\\t\"\n-            \"sve_$5  $dst, $6, $dst\\t# convert $1 to $2 vector\" %}\n+            (n->bottom_type()->is_vect()->element_basic_type() == T_BYTE ||\n+             n->bottom_type()->is_vect()->element_basic_type() == T_SHORT ||\n+             n->bottom_type()->is_vect()->element_basic_type() == T_INT));\n+  match(Set dst (VectorCastD2X src));\n+  effect(TEMP_DEF dst, TEMP tmp);\n+  ins_cost(3 * SVE_COST);\n+  format %{ \"sve_vectorcast_d2x  $dst, $src\\t# convert D to X vector (narrow)\" %}\n@@ -1951,2 +2410,5 @@\n-    __ sve_$3(as_FloatRegister($dst$$reg), __ $4, ptrue, as_FloatRegister($src$$reg), __ $4);\n-    __ sve_$5(as_FloatRegister($dst$$reg), __ $6, as_FloatRegister($dst$$reg));\n+    BasicType to_bt = Matcher::vector_element_basic_type(this);\n+    Assembler::SIMD_RegVariant to_size = __ elemType_to_regVariant(to_bt);\n+    __ sve_fcvtzs(as_FloatRegister($dst$$reg), __ S, ptrue, as_FloatRegister($src$$reg), __ D);\n+    __ sve_vector_narrow(as_FloatRegister($dst$$reg), to_size,\n+                         as_FloatRegister($dst$$reg), __ D, as_FloatRegister($tmp$$reg));\n@@ -1955,2 +2417,1 @@\n-%}')dnl\n-dnl\n+%}\n@@ -1958,2 +2419,4 @@\n-define(`VECTOR_CAST_F2X_NARROW3', `\n-instruct vcvt$1to$2`'(vReg dst, vReg src, vReg tmp)\n+dnl vcvtDtoL\n+VECTOR_CAST_X2X(D, L, fcvtzs, D, d2l)\n+\n+instruct vcvtDtoF(vReg dst, vReg src, vReg tmp)\n@@ -1962,2 +2425,2 @@\n-            n->bottom_type()->is_vect()->element_basic_type() == T_`'TYPE2DATATYPE($2));\n-  match(Set dst (VectorCast$1`'2X src));\n+            n->bottom_type()->is_vect()->element_basic_type() == T_FLOAT);\n+  match(Set dst (VectorCastD2X src));\n@@ -1965,6 +2428,2 @@\n-  ins_cost(5 * SVE_COST);\n-  format %{ \"sve_$3  $dst, $4, $src, $4\\n\\t\"\n-            \"sve_$5  $tmp, $6, 0\\n\\t\"\n-            \"sve_$7  $dst, $6, $dst, tmp\\n\\t\"\n-            \"sve_$7  $dst, $8, $dst, tmp\\n\\t\"\n-            \"sve_$7  $dst, $9, $dst, tmp\\n\\t# convert $1 to $2 vector\" %}\n+  ins_cost(3 * SVE_COST);\n+  format %{ \"sve_vectorcast_d2f  $dst, S, $dst\\t# convert D to F vector\" %}\n@@ -1972,5 +2431,3 @@\n-    __ sve_$3(as_FloatRegister($dst$$reg), __ $4, ptrue, as_FloatRegister($src$$reg), __ $4);\n-    __ sve_$5(as_FloatRegister($tmp$$reg), __ $6, 0);\n-    __ sve_$7(as_FloatRegister($dst$$reg), __ $6, as_FloatRegister($dst$$reg), as_FloatRegister($tmp$$reg));\n-    __ sve_$7(as_FloatRegister($dst$$reg), __ $8, as_FloatRegister($dst$$reg), as_FloatRegister($tmp$$reg));\n-    __ sve_$7(as_FloatRegister($dst$$reg), __ $9, as_FloatRegister($dst$$reg), as_FloatRegister($tmp$$reg));\n+    __ sve_fcvt(as_FloatRegister($dst$$reg), __ S, ptrue, as_FloatRegister($src$$reg), __ D);\n+    __ sve_vector_narrow(as_FloatRegister($dst$$reg), __ S,\n+                         as_FloatRegister($dst$$reg), __ D, as_FloatRegister($tmp$$reg));\n@@ -1979,37 +2436,2 @@\n-%}')dnl\n-dnl\n-VECTOR_CAST_EXTEND1(B, S, sunpklo, H)\n-VECTOR_CAST_EXTEND2(B, I, sunpklo, H, S)\n-VECTOR_CAST_EXTEND3(B, L, sunpklo, H, S, D)\n-VECTOR_CAST_I2F_EXTEND2(B, F, sunpklo, H, S, scvtf)\n-VECTOR_CAST_I2F_EXTEND3(B, D, sunpklo, H, S, D, scvtf)\n-dnl\n-VECTOR_CAST_NARROW1(S, B, dup, B, uzp1)\n-VECTOR_CAST_EXTEND1(S, I, sunpklo, S)\n-VECTOR_CAST_EXTEND2(S, L, sunpklo, S, D)\n-VECTOR_CAST_X2F_EXTEND1(S, F, sunpklo, S, scvtf, S)\n-VECTOR_CAST_I2F_EXTEND2(S, D, sunpklo, S, D, scvtf)\n-dnl\n-VECTOR_CAST_NARROW2(I, B, dup, H, uzp1, B)\n-VECTOR_CAST_NARROW1(I, S, dup, H, uzp1)\n-VECTOR_CAST_EXTEND1(I, L, sunpklo, D)\n-VECTOR_CAST_X2X(I, F, scvtf, S)\n-VECTOR_CAST_X2F_EXTEND1(I, D, sunpklo, D, scvtf, D)\n-dnl\n-VECTOR_CAST_NARROW3(L, B, dup, S, uzp1, H, B)\n-VECTOR_CAST_NARROW2(L, S, dup, S, uzp1, H)\n-VECTOR_CAST_NARROW1(L, I, dup, S, uzp1)\n-VECTOR_CAST_X2F_NARROW1(L, F, scvtf, S, D, dup, S, uzp1)\n-VECTOR_CAST_X2X(L, D, scvtf, D)\n-dnl\n-VECTOR_CAST_F2X_NARROW2(F, B, fcvtzs, S, dup, H, uzp1, B)\n-VECTOR_CAST_F2X_NARROW1(F, S, fcvtzs, S, dup, H, uzp1)\n-VECTOR_CAST_X2X(F, I, fcvtzs, S)\n-VECTOR_CAST_F2X_EXTEND1(F, L, fcvtzs, S, sunpklo, D)\n-VECTOR_CAST_X2F_EXTEND1(F, D, sunpklo, D, fcvt, S)\n-dnl\n-VECTOR_CAST_F2X_NARROW3(D, B, fcvtzs, D, dup, S, uzp1, H, B)\n-VECTOR_CAST_F2X_NARROW2(D, S, fcvtzs, D, dup, S, uzp1, H)\n-VECTOR_CAST_F2X_NARROW1(D, I, fcvtzs, D, dup, S, uzp1)\n-VECTOR_CAST_X2X(D, L, fcvtzs, D)\n-VECTOR_CAST_X2F_NARROW1(D, F, fcvt, S, D, dup, S, uzp1)\n+%}\n+\n@@ -2020,1 +2442,1 @@\n-instruct extract$1`'($2 dst, vReg src, immI idx, pRegGov pTmp, rFlagsReg cr)\n+instruct extract$1`'($2 dst, vReg src, immI idx, pRegGov pgtmp, rFlagsReg cr)\n@@ -2024,1 +2446,1 @@\n-  effect(TEMP pTmp, KILL cr);\n+  effect(TEMP pgtmp, KILL cr);\n@@ -2026,1 +2448,1 @@\n-  format %{ \"sve_extract $dst, $3, $pTmp, $src, $idx\\n\\t\"\n+  format %{ \"sve_extract $dst, $3, $pgtmp, $src, $idx\\n\\t\"\n@@ -2029,1 +2451,1 @@\n-    __ sve_extract(as_$4($dst$$reg), __ $3, as_PRegister($pTmp$$reg),\n+    __ sve_extract(as_$4($dst$$reg), __ $3, as_PRegister($pgtmp$$reg),\n@@ -2041,1 +2463,1 @@\n-instruct extract$1`'($2 dst, vReg src, immI idx, pRegGov pTmp, rFlagsReg cr)\n+instruct extract$1`'($2 dst, vReg src, immI idx, pRegGov pgtmp, rFlagsReg cr)\n@@ -2045,1 +2467,1 @@\n-  effect(TEMP pTmp, KILL cr);\n+  effect(TEMP pgtmp, KILL cr);\n@@ -2047,1 +2469,1 @@\n-  format %{ \"sve_extract $dst, $3, $pTmp, $src, $idx\\t# extract from vector($1)\" %}\n+  format %{ \"sve_extract $dst, $3, $pgtmp, $src, $idx\\t# extract from vector($1)\" %}\n@@ -2049,1 +2471,1 @@\n-    __ sve_extract(as_$4($dst$$reg), __ $3, as_PRegister($pTmp$$reg),\n+    __ sve_extract(as_$4($dst$$reg), __ $3, as_PRegister($pgtmp$$reg),\n@@ -2061,5 +2483,2 @@\n-dnl\n-dnl VTEST($1,      $2,   $3,  $4  )\n-dnl VTEST(op_name, pred, imm, cond)\n-define(`VTEST', `\n-instruct vtest_$1`'(iRegINoSp dst, vReg src1, vReg src2, pReg pTmp, rFlagsReg cr)\n+\n+instruct vtest_alltrue(iRegINoSp dst, pRegGov src1, pRegGov src2, pReg ptmp, rFlagsReg cr)\n@@ -2067,2 +2486,21 @@\n-  predicate(UseSVE > 0 && n->in(1)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize &&\n-            static_cast<const VectorTestNode*>(n)->get_predicate() == BoolTest::$2);\n+  predicate(UseSVE > 0 &&\n+            n->in(1)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize &&\n+            static_cast<const VectorTestNode*>(n)->get_predicate() == BoolTest::overflow);\n+  match(Set dst (VectorTest src1 src2));\n+  effect(TEMP ptmp, KILL cr);\n+  ins_cost(SVE_COST);\n+  format %{ \"sve_eors $ptmp, $src1, $src2\\t# $src2 is all true mask\\n\"\n+            \"csetw $dst, EQ\\t# VectorTest (sve) - alltrue\" %}\n+  ins_encode %{\n+    __ sve_eors(as_PRegister($ptmp$$reg), ptrue,\n+                as_PRegister($src1$$reg), as_PRegister($src2$$reg));\n+    __ csetw(as_Register($dst$$reg), Assembler::EQ);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vtest_anytrue(iRegINoSp dst, pRegGov src1, pRegGov src2, rFlagsReg cr)\n+%{\n+  predicate(UseSVE > 0 &&\n+            n->in(1)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize &&\n+            static_cast<const VectorTestNode*>(n)->get_predicate() == BoolTest::ne);\n@@ -2070,1 +2508,1 @@\n-  effect(TEMP pTmp, KILL cr);\n+  effect(KILL cr);\n@@ -2072,2 +2510,2 @@\n-  format %{ \"sve_cmpeq $pTmp, $src1, $3\\n\\t\"\n-            \"csetw $dst, $4\\t# VectorTest (sve) - $1\" %}\n+  format %{ \"sve_ptest $src1\\n\\t\"\n+            \"csetw $dst, NE\\t# VectorTest (sve) - anytrue\" %}\n@@ -2076,5 +2514,2 @@\n-    BasicType bt = Matcher::vector_element_basic_type(this, $src1);\n-    Assembler::SIMD_RegVariant size = __ elemType_to_regVariant(bt);\n-    __ sve_cmp(Assembler::EQ, as_PRegister($pTmp$$reg), size,\n-               ptrue, as_FloatRegister($src1$$reg), $3);\n-    __ csetw(as_Register($dst$$reg), Assembler::$4);\n+    __ sve_ptest(ptrue, as_PRegister($src1$$reg));\n+    __ csetw(as_Register($dst$$reg), Assembler::NE);\n@@ -2083,4 +2518,1 @@\n-%}')dnl\n-dnl\n-VTEST(alltrue, overflow, 0, EQ)\n-VTEST(anytrue, ne,      -1, NE)\n+%}\n@@ -2089,2 +2521,2 @@\n-dnl VTEST_PARTIAL($1,      $2,   $3,  $4  )\n-dnl VTEST_PARTIAL(op_name, pred, imm, cond)\n+dnl VTEST_PARTIAL($1,      $2,   $3,   $4  )\n+dnl VTEST_PARTIAL(op_name, pred, inst, cond)\n@@ -2092,1 +2524,1 @@\n-instruct vtest_$1_partial`'(iRegINoSp dst, vReg src1, vReg src2, pRegGov pTmp, rFlagsReg cr)\n+instruct vtest_$1_partial`'(iRegINoSp dst, pRegGov src1, pRegGov src2, pRegGov ptmp, rFlagsReg cr)\n@@ -2094,1 +2526,2 @@\n-  predicate(UseSVE > 0 && n->in(1)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize &&\n+  predicate(UseSVE > 0 &&\n+            n->in(1)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize &&\n@@ -2097,1 +2530,1 @@\n-  effect(TEMP pTmp, KILL cr);\n+  effect(TEMP ptmp, KILL cr);\n@@ -2101,1 +2534,0 @@\n-    \/\/ \"src2\" is not used for sve.\n@@ -2104,1 +2536,1 @@\n-    __ sve_whilelo_zr_imm(as_PRegister($pTmp$$reg), size,\n+    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), size,\n@@ -2106,2 +2538,2 @@\n-    __ sve_cmp(Assembler::EQ, as_PRegister($pTmp$$reg), size,\n-               as_PRegister($pTmp$$reg), as_FloatRegister($src1$$reg), $3);\n+    __ $3(as_PRegister($ptmp$$reg), as_PRegister($ptmp$$reg),\n+          as_PRegister($src1$$reg), as_PRegister($src2$$reg));\n@@ -2113,2 +2545,2 @@\n-VTEST_PARTIAL(alltrue, overflow, 0, EQ)\n-VTEST_PARTIAL(anytrue, ne,      -1, NE)\n+VTEST_PARTIAL(alltrue, overflow, sve_eors, EQ)\n+VTEST_PARTIAL(anytrue, ne,       sve_ands, NE)\n@@ -2118,1 +2550,1 @@\n-instruct insertI_small(vReg dst, vReg src, iRegIorL2I val, immI idx, pRegGov pTmp, rFlagsReg cr)\n+instruct insertI_small(vReg dst, vReg src, iRegIorL2I val, immI idx, pRegGov pgtmp, rFlagsReg cr)\n@@ -2125,1 +2557,1 @@\n-  effect(TEMP_DEF dst, TEMP pTmp, KILL cr);\n+  effect(TEMP_DEF dst, TEMP pgtmp, KILL cr);\n@@ -2127,2 +2559,2 @@\n-  format %{ \"sve_index $dst, -16, 1\\t# (B\/S\/I)\\n\\t\"\n-            \"sve_cmpeq $pTmp, $dst, ($idx-#16) # shift from [0, 31] to [-16, 15]\\n\\t\"\n+  format %{ \"sve_index $dst, -16, 1\\t# (B\/H\/S)\\n\\t\"\n+            \"sve_cmpeq $pgtmp, $dst, ($idx-#16) # shift from [0, 31] to [-16, 15]\\n\\t\"\n@@ -2130,1 +2562,1 @@\n-            \"sve_cpy $dst, $pTmp, $val\\t# insert into vector (B\/S\/I)\" %}\n+            \"sve_cpy $dst, $pgtmp, $val\\t# insert into vector (B\/H\/S)\" %}\n@@ -2135,1 +2567,1 @@\n-    __ sve_cmp(Assembler::EQ, as_PRegister($pTmp$$reg), size, ptrue,\n+    __ sve_cmp(Assembler::EQ, as_PRegister($pgtmp$$reg), size, ptrue,\n@@ -2138,1 +2570,1 @@\n-    __ sve_cpy(as_FloatRegister($dst$$reg), size, as_PRegister($pTmp$$reg), as_Register($val$$reg));\n+    __ sve_cpy(as_FloatRegister($dst$$reg), size, as_PRegister($pgtmp$$reg), as_Register($val$$reg));\n@@ -2143,1 +2575,1 @@\n-instruct insertF_small(vReg dst, vReg src, vRegF val, immI idx, pRegGov pTmp, rFlagsReg cr)\n+instruct insertF_small(vReg dst, vReg src, vRegF val, immI idx, pRegGov pgtmp, rFlagsReg cr)\n@@ -2148,1 +2580,1 @@\n-  effect(TEMP_DEF dst, TEMP pTmp, KILL cr);\n+  effect(TEMP_DEF dst, TEMP pgtmp, KILL cr);\n@@ -2151,1 +2583,1 @@\n-            \"sve_cmpeq $pTmp, $dst, ($idx-#16) # shift from [0, 31] to [-16, 15]\\n\\t\"\n+            \"sve_cmpeq $pgtmp, $dst, ($idx-#16) # shift from [0, 31] to [-16, 15]\\n\\t\"\n@@ -2153,1 +2585,1 @@\n-            \"sve_cpy $dst, $pTmp, $val\\t# insert into vector (F)\" %}\n+            \"sve_cpy $dst, $pgtmp, $val\\t# insert into vector (F)\" %}\n@@ -2156,1 +2588,1 @@\n-    __ sve_cmp(Assembler::EQ, as_PRegister($pTmp$$reg), __ S, ptrue,\n+    __ sve_cmp(Assembler::EQ, as_PRegister($pgtmp$$reg), __ S, ptrue,\n@@ -2159,1 +2591,1 @@\n-    __ sve_cpy(as_FloatRegister($dst$$reg), __ S, as_PRegister($pTmp$$reg), as_FloatRegister($val$$reg));\n+    __ sve_cpy(as_FloatRegister($dst$$reg), __ S, as_PRegister($pgtmp$$reg), as_FloatRegister($val$$reg));\n@@ -2164,1 +2596,1 @@\n-instruct insertI(vReg dst, vReg src, iRegIorL2I val, immI idx, vReg tmp1, pRegGov pTmp, rFlagsReg cr)\n+instruct insertI(vReg dst, vReg src, iRegIorL2I val, immI idx, vReg tmp1, pRegGov pgtmp, rFlagsReg cr)\n@@ -2171,1 +2603,1 @@\n-  effect(TEMP_DEF dst, TEMP tmp1, TEMP pTmp, KILL cr);\n+  effect(TEMP_DEF dst, TEMP tmp1, TEMP pgtmp, KILL cr);\n@@ -2173,3 +2605,3 @@\n-  format %{ \"sve_index $tmp1, 0, 1\\t# (B\/S\/I)\\n\\t\"\n-            \"sve_dup $dst, $idx\\t# (B\/S\/I)\\n\\t\"\n-            \"sve_cmpeq $pTmp, $tmp1, $dst\\n\\t\"\n+  format %{ \"sve_index $tmp1, 0, 1\\t# (B\/H\/S)\\n\\t\"\n+            \"sve_dup $dst, $idx\\t# (B\/H\/S)\\n\\t\"\n+            \"sve_cmpeq $pgtmp, $tmp1, $dst\\n\\t\"\n@@ -2177,1 +2609,1 @@\n-            \"sve_cpy $dst, $pTmp, $val\\t# insert into vector (B\/S\/I)\" %}\n+            \"sve_cpy $dst, $pgtmp, $val\\t# insert into vector (B\/H\/S)\" %}\n@@ -2183,1 +2615,1 @@\n-    __ sve_cmp(Assembler::EQ, as_PRegister($pTmp$$reg), size, ptrue,\n+    __ sve_cmp(Assembler::EQ, as_PRegister($pgtmp$$reg), size, ptrue,\n@@ -2186,1 +2618,1 @@\n-    __ sve_cpy(as_FloatRegister($dst$$reg), size, as_PRegister($pTmp$$reg), as_Register($val$$reg));\n+    __ sve_cpy(as_FloatRegister($dst$$reg), size, as_PRegister($pgtmp$$reg), as_Register($val$$reg));\n@@ -2193,1 +2625,1 @@\n-instruct insert$1`'(vReg dst, vReg src, $2 val, immI idx, pRegGov pTmp, rFlagsReg cr)\n+instruct insert$1`'(vReg dst, vReg src, $2 val, immI idx, pRegGov pgtmp, rFlagsReg cr)\n@@ -2198,1 +2630,1 @@\n-  effect(TEMP_DEF dst, TEMP pTmp, KILL cr);\n+  effect(TEMP_DEF dst, TEMP pgtmp, KILL cr);\n@@ -2201,1 +2633,1 @@\n-            \"sve_cmpeq $pTmp, $dst, ($idx-#16) # shift from [0, 31] to [-16, 15]\\n\\t\"\n+            \"sve_cmpeq $pgtmp, $dst, ($idx-#16) # shift from [0, 31] to [-16, 15]\\n\\t\"\n@@ -2203,1 +2635,1 @@\n-            \"sve_cpy $dst, $pTmp, $val\\t# insert into vector ($1)\" %}\n+            \"sve_cpy $dst, $pgtmp, $val\\t# insert into vector ($1)\" %}\n@@ -2206,1 +2638,1 @@\n-    __ sve_cmp(Assembler::EQ, as_PRegister($pTmp$$reg), __ $3, ptrue,\n+    __ sve_cmp(Assembler::EQ, as_PRegister($pgtmp$$reg), __ $3, ptrue,\n@@ -2209,1 +2641,1 @@\n-    __ sve_cpy(as_FloatRegister($dst$$reg), __ $3, as_PRegister($pTmp$$reg), as_$4($val$$reg));\n+    __ sve_cpy(as_FloatRegister($dst$$reg), __ $3, as_PRegister($pgtmp$$reg), as_$4($val$$reg));\n@@ -2217,1 +2649,1 @@\n-instruct insertF(vReg dst, vReg src, vRegF val, immI idx, vReg tmp1, pRegGov pTmp, rFlagsReg cr)\n+instruct insertF(vReg dst, vReg src, vRegF val, immI idx, vReg tmp1, pRegGov pgtmp, rFlagsReg cr)\n@@ -2222,1 +2654,1 @@\n-  effect(TEMP_DEF dst, TEMP tmp1, TEMP pTmp, KILL cr);\n+  effect(TEMP_DEF dst, TEMP tmp1, TEMP pgtmp, KILL cr);\n@@ -2226,1 +2658,1 @@\n-            \"sve_cmpeq $pTmp, $tmp1, $dst\\n\\t\"\n+            \"sve_cmpeq $pgtmp, $tmp1, $dst\\n\\t\"\n@@ -2228,1 +2660,1 @@\n-            \"sve_cpy $dst, $pTmp, $val\\t# insert into vector (F)\" %}\n+            \"sve_cpy $dst, $pgtmp, $val\\t# insert into vector (F)\" %}\n@@ -2232,1 +2664,1 @@\n-    __ sve_cmp(Assembler::EQ, as_PRegister($pTmp$$reg), __ S, ptrue,\n+    __ sve_cmp(Assembler::EQ, as_PRegister($pgtmp$$reg), __ S, ptrue,\n@@ -2238,1 +2670,1 @@\n-               as_PRegister($pTmp$$reg), as_FloatRegister($val$$reg));\n+               as_PRegister($pgtmp$$reg), as_FloatRegister($val$$reg));\n@@ -2245,3 +2677,2 @@\n-instruct loadshuffleB(vReg dst, vReg src)\n-%{\n-  predicate(UseSVE > 0 && n->bottom_type()->is_vect()->element_basic_type() == T_BYTE);\n+instruct loadshuffle(vReg dst, vReg src) %{\n+  predicate(UseSVE > 0);\n@@ -2250,1 +2681,1 @@\n-  format %{ \"sve_orr $dst, $src, $src\\t# vector load shuffle (B)\" %}\n+  format %{ \"sve_loadshuffle $dst, $src\\t# vector load shuffle (B\/H\/S\/D)\" %}\n@@ -2252,4 +2683,9 @@\n-    if (as_FloatRegister($dst$$reg) != as_FloatRegister($src$$reg)) {\n-      __ sve_orr(as_FloatRegister($dst$$reg),\n-                 as_FloatRegister($src$$reg),\n-                 as_FloatRegister($src$$reg));\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    if (bt == T_BYTE) {\n+      if (as_FloatRegister($dst$$reg) != as_FloatRegister($src$$reg)) {\n+        __ sve_orr(as_FloatRegister($dst$$reg), as_FloatRegister($src$$reg),\n+                   as_FloatRegister($src$$reg));\n+      }\n+    } else {\n+      __ sve_vector_extend(as_FloatRegister($dst$$reg),  __ elemType_to_regVariant(bt),\n+                           as_FloatRegister($src$$reg), __ B);\n@@ -2261,46 +2697,0 @@\n-instruct loadshuffleS(vReg dst, vReg src)\n-%{\n-  predicate(UseSVE > 0 && n->bottom_type()->is_vect()->element_basic_type() == T_SHORT);\n-  match(Set dst (VectorLoadShuffle src));\n-  ins_cost(SVE_COST);\n-  format %{ \"sve_uunpklo $dst, $src\\t# vector load shuffle (B to H)\" %}\n-  ins_encode %{\n-    __ sve_uunpklo(as_FloatRegister($dst$$reg), __ H, as_FloatRegister($src$$reg));\n-  %}\n-  ins_pipe(pipe_slow);\n-%}\n-\n-instruct loadshuffleI(vReg dst, vReg src)\n-%{\n-  predicate(UseSVE > 0 &&\n-           (n->bottom_type()->is_vect()->element_basic_type() == T_INT ||\n-            n->bottom_type()->is_vect()->element_basic_type() == T_FLOAT));\n-  match(Set dst (VectorLoadShuffle src));\n-  ins_cost(2 * SVE_COST);\n-  format %{ \"sve_uunpklo $dst, H, $src\\n\\t\"\n-            \"sve_uunpklo $dst, S, $dst\\t# vector load shuffle (B to S)\" %}\n-  ins_encode %{\n-    __ sve_uunpklo(as_FloatRegister($dst$$reg), __ H, as_FloatRegister($src$$reg));\n-    __ sve_uunpklo(as_FloatRegister($dst$$reg), __ S, as_FloatRegister($dst$$reg));\n-  %}\n-  ins_pipe(pipe_slow);\n-%}\n-\n-instruct loadshuffleL(vReg dst, vReg src)\n-%{\n-  predicate(UseSVE > 0 &&\n-           (n->bottom_type()->is_vect()->element_basic_type() == T_LONG ||\n-            n->bottom_type()->is_vect()->element_basic_type() == T_DOUBLE));\n-  match(Set dst (VectorLoadShuffle src));\n-  ins_cost(3 * SVE_COST);\n-  format %{ \"sve_uunpklo $dst, H, $src\\n\\t\"\n-            \"sve_uunpklo $dst, S, $dst\\n\\t\"\n-            \"sve_uunpklo $dst, D, $dst\\t# vector load shuffle (B to D)\" %}\n-  ins_encode %{\n-    __ sve_uunpklo(as_FloatRegister($dst$$reg), __ H, as_FloatRegister($src$$reg));\n-    __ sve_uunpklo(as_FloatRegister($dst$$reg), __ S, as_FloatRegister($dst$$reg));\n-    __ sve_uunpklo(as_FloatRegister($dst$$reg), __ D, as_FloatRegister($dst$$reg));\n-  %}\n-  ins_pipe(pipe_slow);\n-%}\n-\n@@ -2333,1 +2723,1 @@\n-  format %{ \"load_vector_gather $dst, $mem, $idx\\t# vector load gather (I\/F)\" %}\n+  format %{ \"load_vector_gather $dst, $mem, $idx\\t# vector load gather (S)\" %}\n@@ -2348,2 +2738,1 @@\n-  format %{ \"sve_uunpklo $idx, $idx\\n\\t\"\n-            \"load_vector_gather $dst, $mem, $idx\\t# vector load gather (L\/D)\" %}\n+  format %{ \"load_vector_gather $dst, $mem, $idx\\t# vector load gather (D)\" %}\n@@ -2352,1 +2741,2 @@\n-    __ sve_ld1d_gather(as_FloatRegister($dst$$reg), ptrue, as_Register($mem$$base), as_FloatRegister($idx$$reg));\n+    __ sve_ld1d_gather(as_FloatRegister($dst$$reg), ptrue, as_Register($mem$$base),\n+                       as_FloatRegister($idx$$reg));\n@@ -2359,1 +2749,1 @@\n-instruct gatherI_partial(vReg dst, indirect mem, vReg idx, pRegGov pTmp, rFlagsReg cr) %{\n+instruct gatherI_partial(vReg dst, indirect mem, vReg idx, pRegGov ptmp, rFlagsReg cr) %{\n@@ -2365,1 +2755,1 @@\n-  effect(TEMP pTmp, KILL cr);\n+  effect(TEMP ptmp, KILL cr);\n@@ -2367,2 +2757,1 @@\n-  format %{ \"sve_whilelo_zr_imm $pTmp, vector_length\\n\\t\"\n-            \"load_vector_gather $dst, $pTmp, $mem, $idx\\t# vector load gather partial (I\/F)\" %}\n+  format %{ \"load_vector_gather $dst, $ptmp, $mem, $idx\\t# vector load gather partial (S)\" %}\n@@ -2370,3 +2759,2 @@\n-    __ sve_whilelo_zr_imm(as_PRegister($pTmp$$reg), __ S,\n-                          Matcher::vector_length(this));\n-    __ sve_ld1w_gather(as_FloatRegister($dst$$reg), as_PRegister($pTmp$$reg),\n+    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), __ S, Matcher::vector_length(this));\n+    __ sve_ld1w_gather(as_FloatRegister($dst$$reg), as_PRegister($ptmp$$reg),\n@@ -2378,1 +2766,1 @@\n-instruct gatherL_partial(vReg dst, indirect mem, vReg idx, pRegGov pTmp, rFlagsReg cr) %{\n+instruct gatherL_partial(vReg dst, indirect mem, vReg idx, pRegGov ptmp, rFlagsReg cr) %{\n@@ -2384,1 +2772,1 @@\n-  effect(TEMP pTmp, KILL cr);\n+  effect(TEMP ptmp, KILL cr);\n@@ -2386,3 +2774,55 @@\n-  format %{ \"sve_whilelo_zr_imm $pTmp, vector_length\\n\\t\"\n-            \"sve_uunpklo $idx, $idx\\n\\t\"\n-            \"load_vector_gather $dst, $pTmp, $mem, $idx\\t# vector load gather partial (L\/D)\" %}\n+  format %{ \"load_vector_gather $dst, $ptmp, $mem, $idx\\t# vector load gather partial (D)\" %}\n+  ins_encode %{\n+    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), __ D,\n+                          Matcher::vector_length(this));\n+    __ sve_uunpklo(as_FloatRegister($idx$$reg), __ D, as_FloatRegister($idx$$reg));\n+    __ sve_ld1d_gather(as_FloatRegister($dst$$reg), as_PRegister($ptmp$$reg),\n+                       as_Register($mem$$base), as_FloatRegister($idx$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ ------------------------------ Vector Load Gather Predicated -------------------------------\n+\n+instruct gatherI_masked(vReg dst, indirect mem, vReg idx, pRegGov pg) %{\n+  predicate(UseSVE > 0 &&\n+            n->as_LoadVector()->memory_size() == MaxVectorSize &&\n+            (n->bottom_type()->is_vect()->element_basic_type() == T_INT ||\n+             n->bottom_type()->is_vect()->element_basic_type() == T_FLOAT));\n+  match(Set dst (LoadVectorGatherMasked mem (Binary idx pg)));\n+  ins_cost(SVE_COST);\n+  format %{ \"load_vector_gather $dst, $pg, $mem, $idx\\t# vector load gather predicated (S)\" %}\n+  ins_encode %{\n+    __ sve_ld1w_gather(as_FloatRegister($dst$$reg), as_PRegister($pg$$reg),\n+                       as_Register($mem$$base), as_FloatRegister($idx$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct gatherL_masked(vReg dst, indirect mem, vReg idx, pRegGov pg) %{\n+  predicate(UseSVE > 0 &&\n+            n->as_LoadVector()->memory_size() == MaxVectorSize &&\n+            (n->bottom_type()->is_vect()->element_basic_type() == T_LONG ||\n+             n->bottom_type()->is_vect()->element_basic_type() == T_DOUBLE));\n+  match(Set dst (LoadVectorGatherMasked mem (Binary idx pg)));\n+  ins_cost(2 * SVE_COST);\n+  format %{ \"load_vector_gather $dst, $pg, $mem, $idx\\t# vector load gather predicated (D)\" %}\n+  ins_encode %{\n+    __ sve_uunpklo(as_FloatRegister($idx$$reg), __ D, as_FloatRegister($idx$$reg));\n+    __ sve_ld1d_gather(as_FloatRegister($dst$$reg), as_PRegister($pg$$reg),\n+                       as_Register($mem$$base), as_FloatRegister($idx$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ ------------------------------ Vector Load Gather Predicated Partial -------------------------------\n+\n+instruct gatherI_masked_partial(vReg dst, indirect mem, vReg idx, pRegGov pg, pRegGov ptmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->as_LoadVector()->memory_size() < MaxVectorSize &&\n+            (n->bottom_type()->is_vect()->element_basic_type() == T_INT ||\n+             n->bottom_type()->is_vect()->element_basic_type() == T_FLOAT));\n+  match(Set dst (LoadVectorGatherMasked mem (Binary idx pg)));\n+  effect(TEMP ptmp, KILL cr);\n+  ins_cost(3 * SVE_COST);\n+  format %{ \"load_vector_gather $dst, $pg, $mem, $idx\\t# vector load gather predicated partial (S)\" %}\n@@ -2390,1 +2830,1 @@\n-    __ sve_whilelo_zr_imm(as_PRegister($pTmp$$reg), __ D,\n+    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), __ S,\n@@ -2392,0 +2832,21 @@\n+    __ sve_and(as_PRegister($ptmp$$reg), as_PRegister($ptmp$$reg),\n+               as_PRegister($pg$$reg), as_PRegister($pg$$reg));\n+    __ sve_ld1w_gather(as_FloatRegister($dst$$reg), as_PRegister($ptmp$$reg),\n+                       as_Register($mem$$base), as_FloatRegister($idx$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct gatherL_masked_partial(vReg dst, indirect mem, vReg idx, pRegGov pg, pRegGov ptmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->as_LoadVector()->memory_size() < MaxVectorSize &&\n+            (n->bottom_type()->is_vect()->element_basic_type() == T_LONG ||\n+             n->bottom_type()->is_vect()->element_basic_type() == T_DOUBLE));\n+  match(Set dst (LoadVectorGatherMasked mem (Binary idx pg)));\n+  effect(TEMP ptmp, KILL cr);\n+  ins_cost(4 * SVE_COST);\n+  format %{ \"load_vector_gather $dst, $pg, $mem, $idx\\t# vector load gather predicated partial (D)\" %}\n+  ins_encode %{\n+    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), __ D, Matcher::vector_length(this));\n+    __ sve_and(as_PRegister($ptmp$$reg), as_PRegister($ptmp$$reg),\n+               as_PRegister($pg$$reg), as_PRegister($pg$$reg));\n@@ -2393,1 +2854,1 @@\n-    __ sve_ld1d_gather(as_FloatRegister($dst$$reg), as_PRegister($pTmp$$reg),\n+    __ sve_ld1d_gather(as_FloatRegister($dst$$reg), as_PRegister($ptmp$$reg),\n@@ -2408,1 +2869,1 @@\n-  format %{ \"store_vector_scatter $mem, $idx, $src\\t# vector store scatter (I\/F)\" %}\n+  format %{ \"store_vector_scatter $mem, $idx, $src\\t# vector store scatter (S)\" %}\n@@ -2423,2 +2884,1 @@\n-  format %{ \"sve_uunpklo $idx, $idx\\n\\t\"\n-            \"store_vector_scatter $mem, $idx, $src\\t# vector store scatter (L\/D)\" %}\n+  format %{ \"store_vector_scatter $mem, $idx, $src\\t# vector store scatter (D)\" %}\n@@ -2426,2 +2886,1 @@\n-    __ sve_uunpklo(as_FloatRegister($idx$$reg), __ D,\n-                   as_FloatRegister($idx$$reg));\n+    __ sve_uunpklo(as_FloatRegister($idx$$reg), __ D, as_FloatRegister($idx$$reg));\n@@ -2434,1 +2893,1 @@\n-\/\/ ------------------------------ Vector Store Scatter Partial-------------------------------\n+\/\/ ------------------------------ Vector Store Scatter Partial -------------------------------\n@@ -2436,1 +2895,1 @@\n-instruct scatterI_partial(indirect mem, vReg src, vReg idx, pRegGov pTmp, rFlagsReg cr) %{\n+instruct scatterI_partial(indirect mem, vReg src, vReg idx, pRegGov ptmp, rFlagsReg cr) %{\n@@ -2442,1 +2901,1 @@\n-  effect(TEMP pTmp, KILL cr);\n+  effect(TEMP ptmp, KILL cr);\n@@ -2444,2 +2903,1 @@\n-  format %{ \"sve_whilelo_zr_imm $pTmp, vector_length\\n\\t\"\n-            \"store_vector_scatter $mem, $pTmp, $idx, $src\\t# vector store scatter partial (I\/F)\" %}\n+  format %{ \"store_vector_scatter $mem, $ptmp, $idx, $src\\t# vector store scatter partial (S)\" %}\n@@ -2447,1 +2905,1 @@\n-    __ sve_whilelo_zr_imm(as_PRegister($pTmp$$reg), __ S,\n+    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), __ S,\n@@ -2449,1 +2907,1 @@\n-    __ sve_st1w_scatter(as_FloatRegister($src$$reg), as_PRegister($pTmp$$reg),\n+    __ sve_st1w_scatter(as_FloatRegister($src$$reg), as_PRegister($ptmp$$reg),\n@@ -2455,1 +2913,1 @@\n-instruct scatterL_partial(indirect mem, vReg src, vReg idx, pRegGov pTmp, rFlagsReg cr) %{\n+instruct scatterL_partial(indirect mem, vReg src, vReg idx, pRegGov ptmp, rFlagsReg cr) %{\n@@ -2461,1 +2919,1 @@\n-  effect(TEMP pTmp, KILL cr);\n+  effect(TEMP ptmp, KILL cr);\n@@ -2463,3 +2921,1 @@\n-  format %{ \"sve_whilelo_zr_imm $pTmp, vector_length\\n\\t\"\n-            \"sve_uunpklo $idx, $idx\\n\\t\"\n-            \"store_vector_scatter $mem, $pTmp, $idx, $src\\t# vector store scatter partial (L\/D)\" %}\n+  format %{ \"store_vector_scatter $mem, $ptmp, $idx, $src\\t# vector store scatter partial (D)\" %}\n@@ -2467,1 +2923,1 @@\n-    __ sve_whilelo_zr_imm(as_PRegister($pTmp$$reg), __ D,\n+    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), __ D,\n@@ -2470,1 +2926,56 @@\n-    __ sve_st1d_scatter(as_FloatRegister($src$$reg), as_PRegister($pTmp$$reg),\n+    __ sve_st1d_scatter(as_FloatRegister($src$$reg), as_PRegister($ptmp$$reg),\n+                        as_Register($mem$$base), as_FloatRegister($idx$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ ------------------------------ Vector Store Scatter Predicated -------------------------------\n+\n+instruct scatterI_masked(indirect mem, vReg src, vReg idx, pRegGov pg) %{\n+  predicate(UseSVE > 0 &&\n+            n->as_StoreVector()->memory_size() == MaxVectorSize &&\n+            (n->in(3)->in(1)->bottom_type()->is_vect()->element_basic_type() == T_INT ||\n+             n->in(3)->in(1)->bottom_type()->is_vect()->element_basic_type() == T_FLOAT));\n+  match(Set mem (StoreVectorScatterMasked mem (Binary src (Binary idx pg))));\n+  ins_cost(SVE_COST);\n+  format %{ \"store_vector_scatter $mem, $pg, $idx, $src\\t# vector store scatter predicate (S)\" %}\n+  ins_encode %{\n+    __ sve_st1w_scatter(as_FloatRegister($src$$reg), as_PRegister($pg$$reg),\n+                        as_Register($mem$$base), as_FloatRegister($idx$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct scatterL_masked(indirect mem, vReg src, vReg idx, pRegGov pg) %{\n+  predicate(UseSVE > 0 &&\n+            n->as_StoreVector()->memory_size() == MaxVectorSize &&\n+            (n->in(3)->in(1)->bottom_type()->is_vect()->element_basic_type() == T_LONG ||\n+             n->in(3)->in(1)->bottom_type()->is_vect()->element_basic_type() == T_DOUBLE));\n+  match(Set mem (StoreVectorScatterMasked mem (Binary src (Binary idx pg))));\n+  ins_cost(2 * SVE_COST);\n+  format %{ \"store_vector_scatter $mem, $pg, $idx, $src\\t# vector store scatter predicated (D)\" %}\n+  ins_encode %{\n+    __ sve_uunpklo(as_FloatRegister($idx$$reg), __ D, as_FloatRegister($idx$$reg));\n+    __ sve_st1d_scatter(as_FloatRegister($src$$reg), as_PRegister($pg$$reg),\n+                        as_Register($mem$$base), as_FloatRegister($idx$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ ------------------------------ Vector Store Scatter Predicated Partial -------------------------------\n+\n+instruct scatterI_masked_partial(indirect mem, vReg src, vReg idx, pRegGov pg, pRegGov ptmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->as_StoreVector()->memory_size() < MaxVectorSize &&\n+            (n->in(3)->in(1)->bottom_type()->is_vect()->element_basic_type() == T_INT ||\n+             n->in(3)->in(1)->bottom_type()->is_vect()->element_basic_type() == T_FLOAT));\n+  match(Set mem (StoreVectorScatterMasked mem (Binary src (Binary idx pg))));\n+  effect(TEMP ptmp, KILL cr);\n+  ins_cost(3 * SVE_COST);\n+  format %{ \"store_vector_scatter $mem, $pg, $idx, $src\\t# vector store scatter predicated partial (S)\" %}\n+  ins_encode %{\n+    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), __ S,\n+                          Matcher::vector_length(this, $src));\n+    __ sve_and(as_PRegister($ptmp$$reg), as_PRegister($ptmp$$reg),\n+               as_PRegister($pg$$reg), as_PRegister($pg$$reg));\n+    __ sve_st1w_scatter(as_FloatRegister($src$$reg), as_PRegister($ptmp$$reg),\n@@ -2476,0 +2987,20 @@\n+instruct scatterL_masked_partial(indirect mem, vReg src, vReg idx, pRegGov pg, pRegGov ptmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->as_StoreVector()->memory_size() < MaxVectorSize &&\n+            (n->in(3)->in(1)->bottom_type()->is_vect()->element_basic_type() == T_LONG ||\n+             n->in(3)->in(1)->bottom_type()->is_vect()->element_basic_type() == T_DOUBLE));\n+  match(Set mem (StoreVectorScatterMasked mem (Binary src (Binary idx pg))));\n+  effect(TEMP ptmp, KILL cr);\n+  ins_cost(4 * SVE_COST);\n+  format %{ \"store_vector_scatter $mem, $pg, $idx, $src\\t# vector store scatter predicated partial (D)\" %}\n+  ins_encode %{\n+    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), __ D,\n+                          Matcher::vector_length(this, $src));\n+    __ sve_and(as_PRegister($ptmp$$reg), as_PRegister($ptmp$$reg),\n+               as_PRegister($pg$$reg), as_PRegister($pg$$reg));\n+    __ sve_uunpklo(as_FloatRegister($idx$$reg), __ D, as_FloatRegister($idx$$reg));\n+    __ sve_st1d_scatter(as_FloatRegister($src$$reg), as_PRegister($ptmp$$reg),\n+                        as_Register($mem$$base), as_FloatRegister($idx$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n@@ -2516,5 +3047,2 @@\n-dnl\n-dnl VMASK_REDUCTION($1,     $2,      $3  )\n-dnl VMASK_REDUCTION(suffix, op_name, cost)\n-define(`VMASK_REDUCTION', `\n-instruct vmask_$1(iRegINoSp dst, vReg src, pReg ptmp, rFlagsReg cr) %{\n+\/\/ ---------------------------- Vector mask reductions ---------------------------\n+instruct vmask_truecount(iRegINoSp dst, pReg src) %{\n@@ -2523,4 +3051,3 @@\n-  match(Set dst ($2 src));\n-  effect(TEMP ptmp, KILL cr);\n-  ins_cost($3 * SVE_COST);\n-  format %{ \"vmask_$1 $dst, $src\\t# vector mask $1 (sve)\" %}\n+  match(Set dst (VectorMaskTrueCount src));\n+  ins_cost(SVE_COST);\n+  format %{ \"vmask_truecount $dst, $src\\t# vector mask truecount (sve)\" %}\n@@ -2528,2 +3055,3 @@\n-    __ sve_vmask_reduction(this->ideal_Opcode(), $dst$$Register, __ B,\n-                           as_FloatRegister($src$$reg), ptrue, as_PRegister($ptmp$$reg));\n+    BasicType bt = Matcher::vector_element_basic_type(this, $src);\n+    Assembler::SIMD_RegVariant size = __ elemType_to_regVariant(bt);\n+    __ sve_cntp($dst$$Register, size, ptrue, as_PRegister($src$$reg));\n@@ -2532,11 +3060,3 @@\n-%}')dnl\n-dnl\n-\/\/ ---------------------------- Vector mask reductions ---------------------------\n-VMASK_REDUCTION(truecount, VectorMaskTrueCount, 2)\n-VMASK_REDUCTION(firsttrue, VectorMaskFirstTrue, 3)\n-VMASK_REDUCTION(lasttrue,  VectorMaskLastTrue, 4)\n-dnl\n-dnl VMASK_REDUCTION_PARTIAL($1,     $2,      $3  )\n-dnl VMASK_REDUCTION_PARTIAL(suffix, op_name, cost)\n-define(`VMASK_REDUCTION_PARTIAL', `\n-instruct vmask_$1_partial(iRegINoSp dst, vReg src, pRegGov ifelse($1, `firsttrue', `pgtmp, pReg ptmp', `ptmp'), rFlagsReg cr) %{\n+%}\n+\n+instruct vmask_firsttrue(iRegINoSp dst, pReg src, pReg ptmp) %{\n@@ -2544,5 +3064,5 @@\n-            n->in(1)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize);\n-  match(Set dst ($2 src));\n-  effect(TEMP ifelse($1, `firsttrue', `pgtmp, TEMP ptmp', `ptmp'), KILL cr);\n-  ins_cost($3 * SVE_COST);\n-  format %{ \"vmask_$1 $dst, $src\\t# vector mask $1 partial (sve)\" %}\n+            n->in(1)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize);\n+  match(Set dst (VectorMaskFirstTrue src));\n+  effect(TEMP ptmp);\n+  ins_cost(2 * SVE_COST);\n+  format %{ \"vmask_firsttrue $dst, $src\\t# vector mask firsttrue (sve)\" %}\n@@ -2550,4 +3070,4 @@\n-    __ sve_whilelo_zr_imm(as_PRegister(ifelse($1, `firsttrue', `$pgtmp', `$ptmp')$$reg), __ B,\n-                          Matcher::vector_length(this, $src));\n-    __ sve_vmask_reduction(this->ideal_Opcode(), $dst$$Register, __ B, as_FloatRegister($src$$reg),\n-                           as_PRegister(ifelse($1, `firsttrue', `$pgtmp', `$ptmp')$$reg), as_PRegister($ptmp$$reg));\n+    BasicType bt = Matcher::vector_element_basic_type(this, $src);\n+    Assembler::SIMD_RegVariant size = __ elemType_to_regVariant(bt);\n+    __ sve_brkb(as_PRegister($ptmp$$reg), ptrue, as_PRegister($src$$reg), false);\n+    __ sve_cntp($dst$$Register, size, ptrue, as_PRegister($ptmp$$reg));\n@@ -2556,5 +3076,1 @@\n-%}')dnl\n-dnl\n-VMASK_REDUCTION_PARTIAL(truecount, VectorMaskTrueCount, 3)\n-VMASK_REDUCTION_PARTIAL(firsttrue, VectorMaskFirstTrue, 4)\n-VMASK_REDUCTION_PARTIAL(lasttrue,  VectorMaskLastTrue, 5)\n+%}\n@@ -2562,5 +3078,15 @@\n-dnl\n-dnl VSTOREMASK_REDUCTION($1,     $2,      $3  )\n-dnl VSTOREMASK_REDUCTION(suffix, op_name, cost)\n-define(`VSTOREMASK_REDUCTION', `\n-instruct vstoremask_$1(iRegINoSp dst, vReg src, immI esize, pReg ptmp, rFlagsReg cr) %{\n+instruct vmask_lasttrue(iRegINoSp dst, pReg src, pReg ptmp) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(1)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize);\n+  match(Set dst (VectorMaskLastTrue src));\n+  effect(TEMP ptmp);\n+  ins_cost(3 * SVE_COST);\n+  format %{ \"vmask_lasttrue $dst, $src\\t# vector mask lasttrue (sve)\" %}\n+  ins_encode %{\n+    BasicType bt = Matcher::vector_element_basic_type(this, $src);\n+    __ sve_vmask_lasttrue($dst$$Register, bt, as_PRegister($src$$reg), as_PRegister($ptmp$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vmask_truecount_partial(iRegINoSp dst, pReg src, pReg ptmp, rFlagsReg cr) %{\n@@ -2568,2 +3094,2 @@\n-            n->in(1)->in(1)->bottom_type()->is_vect()->length_in_bytes() == MaxVectorSize);\n-  match(Set dst ($2 (VectorStoreMask src esize)));\n+            n->in(1)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize);\n+  match(Set dst (VectorMaskTrueCount src));\n@@ -2571,2 +3097,2 @@\n-  ins_cost($3 * SVE_COST);\n-  format %{ \"vstoremask_$1 $dst, $src\\t# vector mask $1 (sve)\" %}\n+  ins_cost(2 * SVE_COST);\n+  format %{ \"vmask_truecount_partial $dst, $src\\t# vector mask truecount partial (sve)\" %}\n@@ -2574,5 +3100,4 @@\n-    unsigned size = $esize$$constant;\n-    assert(size == 1 || size == 2 || size == 4 || size == 8, \"unsupported element size\");\n-    Assembler::SIMD_RegVariant variant = __ elemBytes_to_regVariant(size);\n-    __ sve_vmask_reduction(this->ideal_Opcode(), $dst$$Register, variant, as_FloatRegister($src$$reg),\n-                           ptrue, as_PRegister($ptmp$$reg), Matcher::vector_length(this, $src));\n+    BasicType bt = Matcher::vector_element_basic_type(this, $src);\n+    Assembler::SIMD_RegVariant size = __ elemType_to_regVariant(bt);\n+    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), size, Matcher::vector_length(this, $src));\n+    __ sve_cntp($dst$$Register, size, as_PRegister($ptmp$$reg), as_PRegister($src$$reg));\n@@ -2581,11 +3106,3 @@\n-%}')dnl\n-dnl\n-\/\/ ----------------- Vector mask reductions combined with VectorMaskStore ---------------\n-VSTOREMASK_REDUCTION(truecount, VectorMaskTrueCount, 2)\n-VSTOREMASK_REDUCTION(firsttrue, VectorMaskFirstTrue, 3)\n-VSTOREMASK_REDUCTION(lasttrue,  VectorMaskLastTrue, 4)\n-dnl\n-dnl VSTOREMASK_REDUCTION_PARTIAL($1,     $2,      $3  )\n-dnl VSTOREMASK_REDUCTION_PARTIAL(suffix, op_name, cost)\n-define(`VSTOREMASK_REDUCTION_PARTIAL', `\n-instruct vstoremask_$1_partial(iRegINoSp dst, vReg src, immI esize, pRegGov ifelse($1, `firsttrue', `pgtmp, pReg ptmp', `ptmp'), rFlagsReg cr) %{\n+%}\n+\n+instruct vmask_firsttrue_partial(iRegINoSp dst, pReg src, pReg ptmp1, pReg ptmp2, rFlagsReg cr) %{\n@@ -2593,5 +3110,5 @@\n-            n->in(1)->in(1)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize);\n-  match(Set dst ($2 (VectorStoreMask src esize)));\n-  effect(TEMP ifelse($1, `firsttrue', `pgtmp, TEMP ptmp', `ptmp'), KILL cr);\n-  ins_cost($3 * SVE_COST);\n-  format %{ \"vstoremask_$1 $dst, $src\\t# vector mask $1 partial (sve)\" %}\n+            n->in(1)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize);\n+  match(Set dst (VectorMaskFirstTrue src));\n+  effect(TEMP ptmp1, TEMP ptmp2, KILL cr);\n+  ins_cost(3 * SVE_COST);\n+  format %{ \"vmask_firsttrue_partial $dst, $src\\t# vector mask firsttrue partial (sve)\" %}\n@@ -2599,4 +3116,3 @@\n-    unsigned size = $esize$$constant;\n-    assert(size == 1 || size == 2 || size == 4 || size == 8, \"unsupported element size\");\n-    Assembler::SIMD_RegVariant variant = __ elemBytes_to_regVariant(size);\n-    __ sve_whilelo_zr_imm(as_PRegister(ifelse($1, `firsttrue', `$pgtmp', `$ptmp')$$reg), variant,\n+    BasicType bt = Matcher::vector_element_basic_type(this, $src);\n+    Assembler::SIMD_RegVariant size = __ elemType_to_regVariant(bt);\n+    __ sve_whilelo_zr_imm(as_PRegister($ptmp1$$reg), size,\n@@ -2604,2 +3120,2 @@\n-    __ sve_vmask_reduction(this->ideal_Opcode(), $dst$$Register, variant, as_FloatRegister($src$$reg),\n-                           as_PRegister(ifelse($1, `firsttrue', `$pgtmp', `$ptmp')$$reg), as_PRegister($ptmp$$reg), MaxVectorSize \/ size);\n+    __ sve_brkb(as_PRegister($ptmp2$$reg), as_PRegister($ptmp1$$reg), as_PRegister($src$$reg), false);\n+    __ sve_cntp($dst$$Register, size, as_PRegister($ptmp1$$reg), as_PRegister($ptmp2$$reg));\n@@ -2608,5 +3124,18 @@\n-%}')dnl\n-dnl\n-VSTOREMASK_REDUCTION_PARTIAL(truecount, VectorMaskTrueCount, 3)\n-VSTOREMASK_REDUCTION_PARTIAL(firsttrue, VectorMaskFirstTrue, 4)\n-VSTOREMASK_REDUCTION_PARTIAL(lasttrue,  VectorMaskLastTrue, 5)\n+%}\n+\n+instruct vmask_lasttrue_partial(iRegINoSp dst, pReg src, pReg ptmp, rFlagsReg cr) %{\n+  predicate(UseSVE > 0 &&\n+            n->in(1)->bottom_type()->is_vect()->length_in_bytes() < MaxVectorSize);\n+  match(Set dst (VectorMaskLastTrue src));\n+  effect(TEMP ptmp, KILL cr);\n+  ins_cost(5 * SVE_COST);\n+  format %{ \"vmask_lasttrue_partial $dst, $src\\t# vector mask lasttrue partial (sve)\" %}\n+  ins_encode %{\n+    BasicType bt = Matcher::vector_element_basic_type(this, $src);\n+    Assembler::SIMD_RegVariant size = __ elemType_to_regVariant(bt);\n+    __ sve_whilelo_zr_imm(as_PRegister($ptmp$$reg), size, Matcher::vector_length(this, $src));\n+    __ sve_and(as_PRegister($ptmp$$reg), ptrue, as_PRegister($ptmp$$reg), as_PRegister($src$$reg));\n+    __ sve_vmask_lasttrue($dst$$Register, bt, as_PRegister($ptmp$$reg), as_PRegister($ptmp$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}dnl\n","filename":"src\/hotspot\/cpu\/aarch64\/aarch64_sve_ad.m4","additions":1649,"deletions":1120,"binary":false,"changes":2769,"status":"modified"},{"patch":"@@ -85,0 +85,5 @@\n+unsigned Assembler::regVariant_to_elemBits(Assembler::SIMD_RegVariant T){\n+  guarantee(T != Q, \"Invalid register variant\");\n+  return 1 << (T + 3);\n+}\n+\n@@ -342,9 +347,5 @@\n-  bool shift = false;\n-  uint64_t uimm = (uint64_t)uabs((jlong)imm);\n-  if (uimm < (1 << 12))\n-    return true;\n-  if (uimm < (1 << 24)\n-      && ((uimm >> 12) << 12 == uimm)) {\n-    return true;\n-  }\n-  return false;\n+  return operand_valid_for_immediate_bits(imm, 12);\n+}\n+\n+bool Assembler::operand_valid_for_sve_add_sub_immediate(int64_t imm) {\n+  return operand_valid_for_immediate_bits(imm, 8);\n@@ -357,0 +358,4 @@\n+bool Assembler::operand_valid_for_sve_logical_immediate(unsigned elembits, uint64_t imm) {\n+  return encode_sve_logical_immediate(elembits, imm) != 0xffffffff;\n+}\n+\n@@ -386,0 +391,11 @@\n+bool asm_util::operand_valid_for_immediate_bits(int64_t imm, unsigned nbits) {\n+  guarantee(nbits == 8 || nbits == 12, \"invalid nbits value\");\n+  uint64_t uimm = (uint64_t)uabs((jlong)imm);\n+  if (uimm < (UCONST64(1) << nbits))\n+    return true;\n+  if (uimm < (UCONST64(1) << (2 * nbits))\n+      && ((uimm >> nbits) << nbits == uimm)) {\n+    return true;\n+  }\n+  return false;\n+}\n@@ -406,0 +422,19 @@\n+uint32_t\n+asm_util::encode_sve_logical_immediate(unsigned elembits, uint64_t imm) {\n+  guarantee(elembits == 8 || elembits == 16 ||\n+            elembits == 32 || elembits == 64, \"unsupported element size\");\n+  uint64_t upper = UCONST64(-1) << (elembits\/2) << (elembits\/2);\n+  \/* Allow all zeros or all ones in top bits, so that\n+   * constant expressions like ~1 are permitted. *\/\n+  if ((imm & ~upper) != imm && (imm | upper) != imm)\n+    return 0xffffffff;\n+\n+  \/\/ Replicate the immediate in different element sizes to 64 bits.\n+  imm &= ~upper;\n+  for (unsigned i = elembits; i < 64; i *= 2) {\n+    imm |= (imm << i);\n+  }\n+\n+  return encoding_for_logical_immediate(imm);\n+}\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/assembler_aarch64.cpp","additions":44,"deletions":9,"binary":false,"changes":53,"status":"modified"},{"patch":"@@ -162,0 +162,2 @@\n+  uint32_t encode_sve_logical_immediate(unsigned elembits, uint64_t imm);\n+  bool operand_valid_for_immediate_bits(int64_t imm, unsigned nbits);\n@@ -586,1 +588,1 @@\n-  static bool offset_ok_for_sve_immed(long offset, int shift, int vl \/* sve vector length *\/) {\n+  static bool offset_ok_for_sve_immed(int64_t offset, int shift, int vl \/* sve vector length *\/) {\n@@ -1519,0 +1521,2 @@\n+  \/\/ Return the corresponding bits for different SIMD_RegVariant value.\n+  static unsigned regVariant_to_elemBits(SIMD_RegVariant T);\n@@ -2956,0 +2960,26 @@\n+\/\/ SVE integer add\/subtract immediate (unpredicated)\n+#define INSN(NAME, op)                                                  \\\n+  void NAME(FloatRegister Zd, SIMD_RegVariant T, unsigned imm8) {       \\\n+    starti;                                                             \\\n+    \/* The immediate is an unsigned value in the range 0 to 255, and    \\\n+     * for element width of 16 bits or higher it may also be a          \\\n+     * positive multiple of 256 in the range 256 to 65280.              \\\n+     *\/                                                                 \\\n+    assert(T != Q, \"invalid size\");                                     \\\n+    int sh = 0;                                                         \\\n+    if (imm8 <= 0xff) {                                                 \\\n+      sh = 0;                                                           \\\n+    } else if (T != B && imm8 <= 0xff00 && (imm8 & 0xff) == 0) {        \\\n+      sh = 1;                                                           \\\n+      imm8 = (imm8 >> 8);                                               \\\n+    } else {                                                            \\\n+      guarantee(false, \"invalid immediate\");                            \\\n+    }                                                                   \\\n+    f(0b00100101, 31, 24), f(T, 23, 22), f(0b10000, 21, 17);            \\\n+    f(op, 16, 14), f(sh, 13), f(imm8, 12, 5), rf(Zd, 0);                \\\n+  }\n+\n+  INSN(sve_add, 0b011);\n+  INSN(sve_sub, 0b111);\n+#undef INSN\n+\n@@ -2979,0 +3009,26 @@\n+  void sve_shift_imm_encoding(SIMD_RegVariant T, int shift, bool isSHR,\n+                              int& tszh, int& tszl_imm) {\n+    \/* The encodings for the tszh:tszl:imm3 fields\n+     * for shift right is calculated as:\n+     *   0001 xxx       B, shift = 16  - UInt(tszh:tszl:imm3)\n+     *   001x xxx       H, shift = 32  - UInt(tszh:tszl:imm3)\n+     *   01xx xxx       S, shift = 64  - UInt(tszh:tszl:imm3)\n+     *   1xxx xxx       D, shift = 128 - UInt(tszh:tszl:imm3)\n+     * for shift left is calculated as:\n+     *   0001 xxx       B, shift = UInt(tszh:tszl:imm3) - 8\n+     *   001x xxx       H, shift = UInt(tszh:tszl:imm3) - 16\n+     *   01xx xxx       S, shift = UInt(tszh:tszl:imm3) - 32\n+     *   1xxx xxx       D, shift = UInt(tszh:tszl:imm3) - 64\n+     *\/\n+    assert(T != Q, \"Invalid register variant\");\n+    if (isSHR) {\n+      assert(((1 << (T + 3)) >= shift) && (shift > 0) , \"Invalid shift value\");\n+    } else {\n+      assert(((1 << (T + 3)) > shift) && (shift >= 0) , \"Invalid shift value\");\n+    }\n+    int cVal = (1 << ((T + 3) + (isSHR ? 1 : 0)));\n+    int encodedShift = isSHR ? cVal - shift : cVal + shift;\n+    tszh = encodedShift >> 5;\n+    tszl_imm = encodedShift & 0x1f;\n+  }\n+\n@@ -2990,0 +3046,1 @@\n+  INSN(sve_and,  0b00000100, 0b011010000); \/\/ vector and\n@@ -2992,1 +3049,1 @@\n-  INSN(sve_cnt,  0b00000100, 0b011010101)  \/\/ count non-zero bits\n+  INSN(sve_cnt,  0b00000100, 0b011010101); \/\/ count non-zero bits\n@@ -2994,0 +3051,1 @@\n+  INSN(sve_eor,  0b00000100, 0b011001000); \/\/ vector eor\n@@ -3000,0 +3058,1 @@\n+  INSN(sve_orr,  0b00000100, 0b011000000); \/\/ vector or\n@@ -3042,1 +3101,1 @@\n-  INSN(sve_fmla,  0b01100101, 1, 0b000); \/\/ floating-point fused multiply-add: Zda = Zda + Zn * Zm\n+  INSN(sve_fmla,  0b01100101, 1, 0b000); \/\/ floating-point fused multiply-add, writing addend: Zda = Zda + Zn * Zm\n@@ -3046,0 +3105,1 @@\n+  INSN(sve_fmad,  0b01100101, 1, 0b100); \/\/ floating-point fused multiply-add, writing multiplicand: Zda = Zm + Zda * Zn\n@@ -3063,0 +3123,14 @@\n+\/\/ SVE bitwise logical with immediate (unpredicated)\n+#define INSN(NAME, opc)                                                      \\\n+  void NAME(FloatRegister Zd, SIMD_RegVariant T, uint64_t imm) {             \\\n+    starti;                                                                  \\\n+    unsigned elembits = regVariant_to_elemBits(T);                           \\\n+    uint32_t val = encode_sve_logical_immediate(elembits, imm);              \\\n+    f(0b00000101, 31, 24), f(opc, 23, 22), f(0b0000, 21, 18);                \\\n+    f(val, 17, 5), rf(Zd, 0);                                                \\\n+  }\n+  INSN(sve_and, 0b10);\n+  INSN(sve_eor, 0b01);\n+  INSN(sve_orr, 0b00);\n+#undef INSN\n+\n@@ -3067,22 +3141,2 @@\n-    \/* The encodings for the tszh:tszl:imm3 fields (bits 23:22 20:19 18:16)     \\\n-     * for shift right is calculated as:                                        \\\n-     *   0001 xxx       B, shift = 16  - UInt(tszh:tszl:imm3)                   \\\n-     *   001x xxx       H, shift = 32  - UInt(tszh:tszl:imm3)                   \\\n-     *   01xx xxx       S, shift = 64  - UInt(tszh:tszl:imm3)                   \\\n-     *   1xxx xxx       D, shift = 128 - UInt(tszh:tszl:imm3)                   \\\n-     * for shift left is calculated as:                                         \\\n-     *   0001 xxx       B, shift = UInt(tszh:tszl:imm3) - 8                     \\\n-     *   001x xxx       H, shift = UInt(tszh:tszl:imm3) - 16                    \\\n-     *   01xx xxx       S, shift = UInt(tszh:tszl:imm3) - 32                    \\\n-     *   1xxx xxx       D, shift = UInt(tszh:tszl:imm3) - 64                    \\\n-     *\/                                                                         \\\n-    assert(T != Q, \"Invalid register variant\");                                 \\\n-    if (isSHR) {                                                                \\\n-      assert(((1 << (T + 3)) >= shift) && (shift > 0) , \"Invalid shift value\"); \\\n-    } else {                                                                    \\\n-      assert(((1 << (T + 3)) > shift) && (shift >= 0) , \"Invalid shift value\"); \\\n-    }                                                                           \\\n-    int cVal = (1 << ((T + 3) + (isSHR ? 1 : 0)));                              \\\n-    int encodedShift = isSHR ? cVal - shift : cVal + shift;                     \\\n-    int tszh = encodedShift >> 5;                                               \\\n-    int tszl_imm = encodedShift & 0x1f;                                         \\\n+    int tszh, tszl_imm;                                                         \\\n+    sve_shift_imm_encoding(T, shift, isSHR, tszh, tszl_imm);                    \\\n@@ -3099,0 +3153,15 @@\n+\/\/ SVE bitwise shift by immediate (predicated)\n+#define INSN(NAME, opc, isSHR)                                                  \\\n+  void NAME(FloatRegister Zdn, SIMD_RegVariant T, PRegister Pg, int shift) {    \\\n+    starti;                                                                     \\\n+    int tszh, tszl_imm;                                                         \\\n+    sve_shift_imm_encoding(T, shift, isSHR, tszh, tszl_imm);                    \\\n+    f(0b00000100, 31, 24), f(tszh, 23, 22), f(0b00, 21, 20), f(opc, 19, 16);    \\\n+    f(0b100, 15, 13), pgrf(Pg, 10), f(tszl_imm, 9, 5), rf(Zdn, 0);              \\\n+  }\n+\n+  INSN(sve_asr, 0b0000, \/* isSHR = *\/ true);\n+  INSN(sve_lsl, 0b0011, \/* isSHR = *\/ false);\n+  INSN(sve_lsr, 0b0001, \/* isSHR = *\/ true);\n+#undef INSN\n+\n@@ -3210,0 +3279,18 @@\n+\/\/ SVE predicate logical operations\n+#define INSN(NAME, op1, op2, op3) \\\n+  void NAME(PRegister Pd, PRegister Pg, PRegister Pn, PRegister Pm) { \\\n+    starti;                                                           \\\n+    f(0b00100101, 31, 24), f(op1, 23, 22), f(0b00, 21, 20);           \\\n+    prf(Pm, 16), f(0b01, 15, 14), prf(Pg, 10), f(op2, 9);             \\\n+    prf(Pn, 5), f(op3, 4), prf(Pd, 0);                                \\\n+  }\n+\n+  INSN(sve_and,  0b00, 0b0, 0b0);\n+  INSN(sve_ands, 0b01, 0b0, 0b0);\n+  INSN(sve_eor,  0b00, 0b1, 0b0);\n+  INSN(sve_eors, 0b01, 0b1, 0b0);\n+  INSN(sve_orr,  0b10, 0b0, 0b0);\n+  INSN(sve_orrs, 0b11, 0b0, 0b0);\n+  INSN(sve_bic,  0b00, 0b0, 0b1);\n+#undef INSN\n+\n@@ -3243,0 +3330,7 @@\n+  \/\/ SVE predicate test\n+  void sve_ptest(PRegister Pg, PRegister Pn) {\n+    starti;\n+    f(0b001001010101000011, 31, 14), prf(Pg, 10), f(0, 9), prf(Pn, 5), f(0, 4, 0);\n+  }\n+\n+  \/\/ SVE predicate initialize\n@@ -3249,0 +3343,28 @@\n+  \/\/ SVE predicate zero\n+  void sve_pfalse(PRegister pd) {\n+    starti;\n+    f(0b00100101, 31, 24), f(0b00, 23, 22), f(0b011000111001, 21, 10);\n+    f(0b000000, 9, 4), prf(pd, 0);\n+  }\n+\n+\/\/ SVE load\/store predicate register\n+#define INSN(NAME, op1)                                                  \\\n+  void NAME(PRegister Pt, const Address &a)  {                           \\\n+    starti;                                                              \\\n+    assert(a.index() == noreg, \"invalid address variant\");               \\\n+    f(op1, 31, 29), f(0b0010110, 28, 22), sf(a.offset() >> 3, 21, 16),   \\\n+    f(0b000, 15, 13), f(a.offset() & 0x7, 12, 10), srf(a.base(), 5),     \\\n+    f(0, 4), prf(Pt, 0);                                                 \\\n+  }\n+\n+  INSN(sve_ldr, 0b100); \/\/ LDR (predicate)\n+  INSN(sve_str, 0b111); \/\/ STR (predicate)\n+#undef INSN\n+\n+  \/\/ SVE move predicate register\n+  void sve_mov(PRegister Pd, PRegister Pn) {\n+    starti;\n+    f(0b001001011000, 31, 20), prf(Pn, 16), f(0b01, 15, 14), prf(Pn, 10);\n+    f(0, 9), prf(Pn, 5), f(0, 4), prf(Pd, 0);\n+  }\n+\n@@ -3351,0 +3473,12 @@\n+\/\/ SVE unpack predicate elements\n+#define INSN(NAME, op) \\\n+  void NAME(PRegister Pd, PRegister Pn) { \\\n+    starti;                                                          \\\n+    f(0b000001010011000, 31, 17), f(op, 16), f(0b0100000, 15, 9);    \\\n+    prf(Pn, 5), f(0b0, 4), prf(Pd, 0);                               \\\n+  }\n+\n+  INSN(sve_punpkhi, 0b1); \/\/ Unpack and widen high half of predicate\n+  INSN(sve_punpklo, 0b0); \/\/ Unpack and widen low half of predicate\n+#undef INSN\n+\n@@ -3364,0 +3498,13 @@\n+\/\/ SVE permute predicate elements\n+#define INSN(NAME, op) \\\n+  void NAME(PRegister Pd, SIMD_RegVariant T, PRegister Pn, PRegister Pm) {             \\\n+    starti;                                                                            \\\n+    assert(T != Q, \"invalid size\");                                                    \\\n+    f(0b00000101, 31, 24), f(T, 23, 22), f(0b10, 21, 20), prf(Pm, 16);                 \\\n+    f(0b01001, 15, 11), f(op, 10), f(0b0, 9), prf(Pn, 5), f(0b0, 4), prf(Pd, 0);       \\\n+  }\n+\n+  INSN(sve_uzp1, 0b0); \/\/ Concatenate even elements from two predicates\n+  INSN(sve_uzp2, 0b1); \/\/ Concatenate odd elements from two predicates\n+#undef INSN\n+\n@@ -3539,0 +3686,1 @@\n+  static bool operand_valid_for_sve_logical_immediate(unsigned elembits, uint64_t imm);\n@@ -3540,0 +3688,1 @@\n+  static bool operand_valid_for_sve_add_sub_immediate(int64_t imm);\n","filename":"src\/hotspot\/cpu\/aarch64\/assembler_aarch64.hpp","additions":174,"deletions":25,"binary":false,"changes":199,"status":"modified"},{"patch":"@@ -73,1 +73,1 @@\n-  : _index(index), _array(NULL), _throw_index_out_of_bounds_exception(true) {\n+  : _index(index), _array(), _throw_index_out_of_bounds_exception(true) {\n@@ -98,1 +98,1 @@\n-    assert(_array != NULL, \"sanity\");\n+    assert(_array != LIR_Opr::nullOpr(), \"sanity\");\n","filename":"src\/hotspot\/cpu\/aarch64\/c1_CodeStubs_aarch64.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -157,2 +157,2 @@\n-LIR_Opr FrameMap::_caller_save_cpu_regs[] = { 0, };\n-LIR_Opr FrameMap::_caller_save_fpu_regs[] = { 0, };\n+LIR_Opr FrameMap::_caller_save_cpu_regs[] = {};\n+LIR_Opr FrameMap::_caller_save_fpu_regs[] = {};\n","filename":"src\/hotspot\/cpu\/aarch64\/c1_FrameMap_aarch64.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -1567,1 +1567,1 @@\n-    assert(addr_ptr->index() == LIR_OprDesc::illegalOpr(), \"need 0 index\");\n+    assert(addr_ptr->index() == LIR_Opr::illegalOpr(), \"need 0 index\");\n@@ -2987,1 +2987,1 @@\n-  Unimplemented();\n+  __ spin_wait();\n","filename":"src\/hotspot\/cpu\/aarch64\/c1_LIRAssembler_aarch64.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -248,1 +248,0 @@\n-    r = NULL;  \/\/ unreachable\n@@ -264,1 +263,1 @@\n-  LIR_Opr imm = NULL;\n+  LIR_Opr imm;\n","filename":"src\/hotspot\/cpu\/aarch64\/c1_LIRGenerator_aarch64.cpp","additions":1,"deletions":2,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -29,1 +29,1 @@\n-FloatRegister LIR_OprDesc::as_float_reg() const {\n+FloatRegister LIR_Opr::as_float_reg() const {\n@@ -33,1 +33,1 @@\n-FloatRegister LIR_OprDesc::as_double_reg() const {\n+FloatRegister LIR_Opr::as_double_reg() const {\n@@ -40,5 +40,5 @@\n-  return (LIR_Opr)(intptr_t)((reg1 << LIR_OprDesc::reg1_shift) |\n-                             (reg1 << LIR_OprDesc::reg2_shift) |\n-                             LIR_OprDesc::double_type          |\n-                             LIR_OprDesc::fpu_register         |\n-                             LIR_OprDesc::double_size);\n+  return (LIR_Opr)(intptr_t)((reg1 << LIR_Opr::reg1_shift) |\n+                             (reg1 << LIR_Opr::reg2_shift) |\n+                             LIR_Opr::double_type          |\n+                             LIR_Opr::fpu_register         |\n+                             LIR_Opr::double_size);\n","filename":"src\/hotspot\/cpu\/aarch64\/c1_LIR_aarch64.cpp","additions":7,"deletions":7,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -975,2 +975,121 @@\n-void C2_MacroAssembler::sve_vmask_reduction(int opc, Register dst, SIMD_RegVariant size, FloatRegister src,\n-                                            PRegister pg, PRegister pn, int length) {\n+\/\/ Get index of the last mask lane that is set\n+void C2_MacroAssembler::sve_vmask_lasttrue(Register dst, BasicType bt, PRegister src, PRegister ptmp) {\n+  SIMD_RegVariant size = elemType_to_regVariant(bt);\n+  sve_rev(ptmp, size, src);\n+  sve_brkb(ptmp, ptrue, ptmp, false);\n+  sve_cntp(dst, size, ptrue, ptmp);\n+  movw(rscratch1, MaxVectorSize \/ type2aelembytes(bt) - 1);\n+  subw(dst, rscratch1, dst);\n+}\n+\n+void C2_MacroAssembler::sve_vector_extend(FloatRegister dst, SIMD_RegVariant dst_size,\n+                                          FloatRegister src, SIMD_RegVariant src_size) {\n+  assert(dst_size > src_size && dst_size <= D && src_size <= S, \"invalid element size\");\n+  if (src_size == B) {\n+    switch (dst_size) {\n+    case H:\n+      sve_sunpklo(dst, H, src);\n+      break;\n+    case S:\n+      sve_sunpklo(dst, H, src);\n+      sve_sunpklo(dst, S, dst);\n+      break;\n+    case D:\n+      sve_sunpklo(dst, H, src);\n+      sve_sunpklo(dst, S, dst);\n+      sve_sunpklo(dst, D, dst);\n+      break;\n+    default:\n+      ShouldNotReachHere();\n+    }\n+  } else if (src_size == H) {\n+    if (dst_size == S) {\n+      sve_sunpklo(dst, S, src);\n+    } else { \/\/ D\n+      sve_sunpklo(dst, S, src);\n+      sve_sunpklo(dst, D, dst);\n+    }\n+  } else if (src_size == S) {\n+    sve_sunpklo(dst, D, src);\n+  }\n+}\n+\n+\/\/ Vector narrow from src to dst with specified element sizes.\n+\/\/ High part of dst vector will be filled with zero.\n+void C2_MacroAssembler::sve_vector_narrow(FloatRegister dst, SIMD_RegVariant dst_size,\n+                                          FloatRegister src, SIMD_RegVariant src_size,\n+                                          FloatRegister tmp) {\n+  assert(dst_size < src_size && dst_size <= S && src_size <= D, \"invalid element size\");\n+  sve_dup(tmp, src_size, 0);\n+  if (src_size == D) {\n+    switch (dst_size) {\n+    case S:\n+      sve_uzp1(dst, S, src, tmp);\n+      break;\n+    case H:\n+      sve_uzp1(dst, S, src, tmp);\n+      sve_uzp1(dst, H, dst, tmp);\n+      break;\n+    case B:\n+      sve_uzp1(dst, S, src, tmp);\n+      sve_uzp1(dst, H, dst, tmp);\n+      sve_uzp1(dst, B, dst, tmp);\n+      break;\n+    default:\n+      ShouldNotReachHere();\n+    }\n+  } else if (src_size == S) {\n+    if (dst_size == H) {\n+      sve_uzp1(dst, H, src, tmp);\n+    } else { \/\/ B\n+      sve_uzp1(dst, H, src, tmp);\n+      sve_uzp1(dst, B, dst, tmp);\n+    }\n+  } else if (src_size == H) {\n+    sve_uzp1(dst, B, src, tmp);\n+  }\n+}\n+\n+\/\/ Extend src predicate to dst predicate with the same lane count but larger\n+\/\/ element size, e.g. 64Byte -> 512Long\n+void C2_MacroAssembler::sve_vmaskcast_extend(PRegister dst, PRegister src,\n+                                             uint dst_element_length_in_bytes,\n+                                             uint src_element_length_in_bytes) {\n+  if (dst_element_length_in_bytes == 2 * src_element_length_in_bytes) {\n+    sve_punpklo(dst, src);\n+  } else if (dst_element_length_in_bytes == 4 * src_element_length_in_bytes) {\n+    sve_punpklo(dst, src);\n+    sve_punpklo(dst, dst);\n+  } else if (dst_element_length_in_bytes == 8 * src_element_length_in_bytes) {\n+    sve_punpklo(dst, src);\n+    sve_punpklo(dst, dst);\n+    sve_punpklo(dst, dst);\n+  } else {\n+    assert(false, \"unsupported\");\n+    ShouldNotReachHere();\n+  }\n+}\n+\n+\/\/ Narrow src predicate to dst predicate with the same lane count but\n+\/\/ smaller element size, e.g. 512Long -> 64Byte\n+void C2_MacroAssembler::sve_vmaskcast_narrow(PRegister dst, PRegister src,\n+                                             uint dst_element_length_in_bytes, uint src_element_length_in_bytes) {\n+  \/\/ The insignificant bits in src predicate are expected to be zero.\n+  if (dst_element_length_in_bytes * 2 == src_element_length_in_bytes) {\n+    sve_uzp1(dst, B, src, src);\n+  } else if (dst_element_length_in_bytes * 4 == src_element_length_in_bytes) {\n+    sve_uzp1(dst, H, src, src);\n+    sve_uzp1(dst, B, dst, dst);\n+  } else if (dst_element_length_in_bytes * 8 == src_element_length_in_bytes) {\n+    sve_uzp1(dst, S, src, src);\n+    sve_uzp1(dst, H, dst, dst);\n+    sve_uzp1(dst, B, dst, dst);\n+  } else {\n+    assert(false, \"unsupported\");\n+    ShouldNotReachHere();\n+  }\n+}\n+\n+void C2_MacroAssembler::sve_reduce_integral(int opc, Register dst, BasicType bt, Register src1,\n+                                            FloatRegister src2, PRegister pg, FloatRegister tmp) {\n+  assert(bt == T_BYTE || bt == T_SHORT || bt == T_INT || bt == T_LONG, \"unsupported element type\");\n@@ -978,2 +1097,3 @@\n-  \/\/ The conditional flags will be clobbered by this function\n-  sve_cmp(Assembler::NE, pn, size, pg, src, 0);\n+  assert_different_registers(src1, dst);\n+  \/\/ Register \"dst\" and \"tmp\" are to be clobbered, and \"src1\" and \"src2\" should be preserved.\n+  Assembler::SIMD_RegVariant size = elemType_to_regVariant(bt);\n@@ -981,2 +1101,38 @@\n-    case Op_VectorMaskTrueCount:\n-      sve_cntp(dst, size, ptrue, pn);\n+    case Op_AddReductionVI: {\n+      sve_uaddv(tmp, size, pg, src2);\n+      smov(dst, tmp, size, 0);\n+      if (bt == T_BYTE) {\n+        addw(dst, src1, dst, ext::sxtb);\n+      } else if (bt == T_SHORT) {\n+        addw(dst, src1, dst, ext::sxth);\n+      } else {\n+        addw(dst, dst, src1);\n+      }\n+      break;\n+    }\n+    case Op_AddReductionVL: {\n+      sve_uaddv(tmp, size, pg, src2);\n+      umov(dst, tmp, size, 0);\n+      add(dst, dst, src1);\n+      break;\n+    }\n+    case Op_AndReductionV: {\n+      sve_andv(tmp, size, pg, src2);\n+      if (bt == T_LONG) {\n+        umov(dst, tmp, size, 0);\n+        andr(dst, dst, src1);\n+      } else {\n+        smov(dst, tmp, size, 0);\n+        andw(dst, dst, src1);\n+      }\n+      break;\n+    }\n+    case Op_OrReductionV: {\n+      sve_orv(tmp, size, pg, src2);\n+      if (bt == T_LONG) {\n+        umov(dst, tmp, size, 0);\n+        orr(dst, dst, src1);\n+      } else {\n+        smov(dst, tmp, size, 0);\n+        orrw(dst, dst, src1);\n+      }\n@@ -984,3 +1140,23 @@\n-    case Op_VectorMaskFirstTrue:\n-      sve_brkb(pn, pg, pn, false);\n-      sve_cntp(dst, size, ptrue, pn);\n+    }\n+    case Op_XorReductionV: {\n+      sve_eorv(tmp, size, pg, src2);\n+      if (bt == T_LONG) {\n+        umov(dst, tmp, size, 0);\n+        eor(dst, dst, src1);\n+      } else {\n+        smov(dst, tmp, size, 0);\n+        eorw(dst, dst, src1);\n+      }\n+      break;\n+    }\n+    case Op_MaxReductionV: {\n+      sve_smaxv(tmp, size, pg, src2);\n+      if (bt == T_LONG) {\n+        umov(dst, tmp, size, 0);\n+        cmp(dst, src1);\n+        csel(dst, dst, src1, Assembler::GT);\n+      } else {\n+        smov(dst, tmp, size, 0);\n+        cmpw(dst, src1);\n+        cselw(dst, dst, src1, Assembler::GT);\n+      }\n@@ -988,6 +1164,12 @@\n-    case Op_VectorMaskLastTrue:\n-      sve_rev(pn, size, pn);\n-      sve_brkb(pn, ptrue, pn, false);\n-      sve_cntp(dst, size, ptrue, pn);\n-      movw(rscratch1, length - 1);\n-      subw(dst, rscratch1, dst);\n+    }\n+    case Op_MinReductionV: {\n+      sve_sminv(tmp, size, pg, src2);\n+      if (bt == T_LONG) {\n+        umov(dst, tmp, size, 0);\n+        cmp(dst, src1);\n+        csel(dst, dst, src1, Assembler::LT);\n+      } else {\n+        smov(dst, tmp, size, 0);\n+        cmpw(dst, src1);\n+        cselw(dst, dst, src1, Assembler::LT);\n+      }\n@@ -995,0 +1177,1 @@\n+    }\n@@ -999,0 +1182,8 @@\n+\n+  if (opc == Op_AndReductionV || opc == Op_OrReductionV || opc == Op_XorReductionV) {\n+    if (bt == T_BYTE) {\n+      sxtb(dst, dst);\n+    } else if (bt == T_SHORT) {\n+      sxth(dst, dst);\n+    }\n+  }\n","filename":"src\/hotspot\/cpu\/aarch64\/c2_MacroAssembler_aarch64.cpp","additions":206,"deletions":15,"binary":false,"changes":221,"status":"modified"},{"patch":"@@ -64,2 +64,16 @@\n-  void sve_vmask_reduction(int opc, Register dst, SIMD_RegVariant size, FloatRegister src,\n-                           PRegister pg, PRegister pn, int length = MaxVectorSize);\n+  void sve_vmask_lasttrue(Register dst, BasicType bt, PRegister src, PRegister ptmp);\n+\n+  void sve_vector_extend(FloatRegister dst, SIMD_RegVariant dst_size,\n+                         FloatRegister src, SIMD_RegVariant src_size);\n+\n+  void sve_vector_narrow(FloatRegister dst, SIMD_RegVariant dst_size,\n+                         FloatRegister src, SIMD_RegVariant src_size, FloatRegister tmp);\n+\n+  void sve_vmaskcast_extend(PRegister dst, PRegister src,\n+                            uint dst_element_length_in_bytes, uint src_element_lenght_in_bytes);\n+\n+  void sve_vmaskcast_narrow(PRegister dst, PRegister src,\n+                            uint dst_element_length_in_bytes, uint src_element_lenght_in_bytes);\n+\n+  void sve_reduce_integral(int opc, Register dst, BasicType bt, Register src1,\n+                           FloatRegister src2, PRegister pg, FloatRegister tmp);\n","filename":"src\/hotspot\/cpu\/aarch64\/c2_MacroAssembler_aarch64.hpp","additions":16,"deletions":2,"binary":false,"changes":18,"status":"modified"},{"patch":"@@ -318,0 +318,1 @@\n+  PRegSet               _p_regs;\n@@ -331,0 +332,2 @@\n+        } else if (vm_reg->is_PRegister()) {\n+          _p_regs += PRegSet::of(vm_reg->as_PRegister());\n@@ -344,1 +347,2 @@\n-      _fp_regs() {\n+      _fp_regs(),\n+      _p_regs() {\n@@ -352,0 +356,1 @@\n+    __ push_p(_p_regs, sp);\n@@ -356,0 +361,1 @@\n+    __ pop_p(_p_regs, sp);\n","filename":"src\/hotspot\/cpu\/aarch64\/gc\/z\/zBarrierSetAssembler_aarch64.cpp","additions":7,"deletions":1,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -35,2 +35,1 @@\n-class LIR_OprDesc;\n-typedef LIR_OprDesc* LIR_Opr;\n+class LIR_Opr;\n","filename":"src\/hotspot\/cpu\/aarch64\/gc\/z\/zBarrierSetAssembler_aarch64.hpp","additions":1,"deletions":2,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -113,1 +113,9 @@\n-          range(-1, 4096)\n+          range(-1, 4096)                                               \\\n+  product(ccstr, OnSpinWaitInst, \"none\", DIAGNOSTIC,                    \\\n+          \"The instruction to use to implement \"                        \\\n+          \"java.lang.Thread.onSpinWait().\"                              \\\n+          \"Options: none, nop, isb, yield.\")                            \\\n+  product(uint, OnSpinWaitInstCount, 1, DIAGNOSTIC,                     \\\n+          \"The number of OnSpinWaitInst instructions to generate.\"      \\\n+          \"It cannot be used with OnSpinWaitInst=none.\")                \\\n+          range(1, 99)\n","filename":"src\/hotspot\/cpu\/aarch64\/globals_aarch64.hpp","additions":9,"deletions":1,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -1981,1 +1981,1 @@\n-\/\/ Return the number of dwords poped\n+\/\/ Return the number of dwords popped\n@@ -2040,0 +2040,74 @@\n+\/\/ Return the number of dwords pushed\n+int MacroAssembler::push_p(unsigned int bitset, Register stack) {\n+  bool use_sve = false;\n+  int sve_predicate_size_in_slots = 0;\n+\n+#ifdef COMPILER2\n+  use_sve = Matcher::supports_scalable_vector();\n+  if (use_sve) {\n+    sve_predicate_size_in_slots = Matcher::scalable_predicate_reg_slots();\n+  }\n+#endif\n+\n+  if (!use_sve) {\n+    return 0;\n+  }\n+\n+  unsigned char regs[PRegisterImpl::number_of_saved_registers];\n+  int count = 0;\n+  for (int reg = 0; reg < PRegisterImpl::number_of_saved_registers; reg++) {\n+    if (1 & bitset)\n+      regs[count++] = reg;\n+    bitset >>= 1;\n+  }\n+\n+  if (count == 0) {\n+    return 0;\n+  }\n+\n+  int total_push_bytes = align_up(sve_predicate_size_in_slots *\n+                                  VMRegImpl::stack_slot_size * count, 16);\n+  sub(stack, stack, total_push_bytes);\n+  for (int i = 0; i < count; i++) {\n+    sve_str(as_PRegister(regs[i]), Address(stack, i));\n+  }\n+  return total_push_bytes \/ 8;\n+}\n+\n+\/\/ Return the number of dwords popped\n+int MacroAssembler::pop_p(unsigned int bitset, Register stack) {\n+  bool use_sve = false;\n+  int sve_predicate_size_in_slots = 0;\n+\n+#ifdef COMPILER2\n+  use_sve = Matcher::supports_scalable_vector();\n+  if (use_sve) {\n+    sve_predicate_size_in_slots = Matcher::scalable_predicate_reg_slots();\n+  }\n+#endif\n+\n+  if (!use_sve) {\n+    return 0;\n+  }\n+\n+  unsigned char regs[PRegisterImpl::number_of_saved_registers];\n+  int count = 0;\n+  for (int reg = 0; reg < PRegisterImpl::number_of_saved_registers; reg++) {\n+    if (1 & bitset)\n+      regs[count++] = reg;\n+    bitset >>= 1;\n+  }\n+\n+  if (count == 0) {\n+    return 0;\n+  }\n+\n+  int total_pop_bytes = align_up(sve_predicate_size_in_slots *\n+                                 VMRegImpl::stack_slot_size * count, 16);\n+  for (int i = count - 1; i >= 0; i--) {\n+    sve_ldr(as_PRegister(regs[i]), Address(stack, i));\n+  }\n+  add(stack, stack, total_pop_bytes);\n+  return total_pop_bytes \/ 8;\n+}\n+\n@@ -2498,1 +2572,1 @@\n-                                    int sve_vector_size_in_bytes) {\n+                                    int sve_vector_size_in_bytes, int total_predicate_in_bytes) {\n@@ -2515,0 +2589,6 @@\n+  if (save_vectors && use_sve && total_predicate_in_bytes > 0) {\n+    sub(sp, sp, total_predicate_in_bytes);\n+    for (int i = 0; i < PRegisterImpl::number_of_saved_registers; i++) {\n+      sve_str(as_PRegister(i), Address(sp, i));\n+    }\n+  }\n@@ -2518,1 +2598,7 @@\n-                                   int sve_vector_size_in_bytes) {\n+                                   int sve_vector_size_in_bytes, int total_predicate_in_bytes) {\n+  if (restore_vectors && use_sve && total_predicate_in_bytes > 0) {\n+    for (int i = PRegisterImpl::number_of_saved_registers - 1; i >= 0; i--) {\n+      sve_ldr(as_PRegister(i), Address(sp, i));\n+    }\n+    add(sp, sp, total_predicate_in_bytes);\n+  }\n@@ -5157,0 +5243,18 @@\n+\n+void MacroAssembler::spin_wait() {\n+  for (int i = 0; i < VM_Version::spin_wait_desc().inst_count(); ++i) {\n+    switch (VM_Version::spin_wait_desc().inst()) {\n+      case SpinWait::NOP:\n+        nop();\n+        break;\n+      case SpinWait::ISB:\n+        isb();\n+        break;\n+      case SpinWait::YIELD:\n+        yield();\n+        break;\n+      default:\n+        ShouldNotReachHere();\n+    }\n+  }\n+}\n","filename":"src\/hotspot\/cpu\/aarch64\/macroAssembler_aarch64.cpp","additions":107,"deletions":3,"binary":false,"changes":110,"status":"modified"},{"patch":"@@ -458,0 +458,3 @@\n+  int push_p(unsigned int bitset, Register stack);\n+  int pop_p(unsigned int bitset, Register stack);\n+\n@@ -469,0 +472,3 @@\n+  void push_p(PRegSet regs, Register stack) { if (regs.bits()) push_p(regs.bits(), stack); }\n+  void pop_p(PRegSet regs, Register stack) { if (regs.bits()) pop_p(regs.bits(), stack); }\n+\n@@ -868,1 +874,1 @@\n-                      int sve_vector_size_in_bytes = 0);\n+                      int sve_vector_size_in_bytes = 0, int total_predicate_in_bytes = 0);\n@@ -870,1 +876,1 @@\n-                      int sve_vector_size_in_bytes = 0);\n+                     int sve_vector_size_in_bytes = 0, int total_predicate_in_bytes = 0);\n@@ -1364,0 +1370,1 @@\n+\n@@ -1367,0 +1374,4 @@\n+  void spill_sve_predicate(PRegister pr, int offset, int predicate_reg_size_in_bytes) {\n+    sve_str(pr, sve_spill_address(predicate_reg_size_in_bytes, offset));\n+  }\n+\n@@ -1377,0 +1388,1 @@\n+\n@@ -1380,0 +1392,4 @@\n+  void unspill_sve_predicate(PRegister pr, int offset, int predicate_reg_size_in_bytes) {\n+    sve_ldr(pr, sve_spill_address(predicate_reg_size_in_bytes, offset));\n+  }\n+\n@@ -1402,0 +1418,6 @@\n+  void spill_copy_sve_predicate_stack_to_stack(int src_offset, int dst_offset,\n+                                               int sve_predicate_reg_size_in_bytes) {\n+    sve_ldr(ptrue, sve_spill_address(sve_predicate_reg_size_in_bytes, src_offset));\n+    sve_str(ptrue, sve_spill_address(sve_predicate_reg_size_in_bytes, dst_offset));\n+    reinitialize_ptrue();\n+  }\n@@ -1405,0 +1427,3 @@\n+  \/\/ Code for java.lang.Thread::onSpinWait() intrinsic.\n+  void spin_wait();\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/macroAssembler_aarch64.hpp","additions":27,"deletions":2,"binary":false,"changes":29,"status":"modified"},{"patch":"@@ -2,2 +2,2 @@\n- * Copyright (c) 2000, 2020, Oracle and\/or its affiliates. All rights reserved.\n- * Copyright (c) 2014, 2020, Red Hat Inc. All rights reserved.\n+ * Copyright (c) 2000, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2014, 2021, Red Hat Inc. All rights reserved.\n@@ -37,1 +37,2 @@\n-  = ConcreteRegisterImpl::max_fpr + PRegisterImpl::number_of_registers;\n+  = ConcreteRegisterImpl::max_fpr +\n+    PRegisterImpl::number_of_registers * PRegisterImpl::max_slots_per_register;\n","filename":"src\/hotspot\/cpu\/aarch64\/register_aarch64.cpp","additions":4,"deletions":3,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -3,1 +3,1 @@\n- * Copyright (c) 2014, 2020, Red Hat Inc. All rights reserved.\n+ * Copyright (c) 2014, 2021, Red Hat Inc. All rights reserved.\n@@ -246,0 +246,5 @@\n+    \/\/ p0-p7 are governing predicates for load\/store and arithmetic, but p7 is\n+    \/\/ preserved as an all-true predicate in OpenJDK. And since we don't support\n+    \/\/ non-governing predicate registers allocation for non-temp register, the\n+    \/\/ predicate registers to be saved are p0-p6.\n+    number_of_saved_registers = number_of_governing_registers - 1,\n@@ -380,0 +385,1 @@\n+typedef AbstractRegSet<PRegister> PRegSet;\n","filename":"src\/hotspot\/cpu\/aarch64\/register_aarch64.hpp","additions":7,"deletions":1,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -104,1 +104,4 @@\n-  int v0_offset_in_bytes(void)   { return 0; }\n+  int v0_offset_in_bytes();\n+\n+  \/\/ Total stack size in bytes for saving sve predicate registers.\n+  int total_sve_predicate_in_bytes();\n@@ -142,1 +145,1 @@\n-  int r0_offset = (slots_per_vect * FloatRegisterImpl::number_of_registers) * BytesPerInt;\n+  int r0_offset = v0_offset_in_bytes() + (slots_per_vect * FloatRegisterImpl::number_of_registers) * BytesPerInt;\n@@ -146,0 +149,20 @@\n+int RegisterSaver::v0_offset_in_bytes() {\n+  \/\/ The floating point registers are located above the predicate registers if\n+  \/\/ they are present in the stack frame pushed by save_live_registers(). So the\n+  \/\/ offset depends on the saved total predicate vectors in the stack frame.\n+  return (total_sve_predicate_in_bytes() \/ VMRegImpl::stack_slot_size) * BytesPerInt;\n+}\n+\n+int RegisterSaver::total_sve_predicate_in_bytes() {\n+#ifdef COMPILER2\n+  if (_save_vectors && Matcher::supports_scalable_vector()) {\n+    \/\/ The number of total predicate bytes is unlikely to be a multiple\n+    \/\/ of 16 bytes so we manually align it up.\n+    return align_up(Matcher::scalable_predicate_reg_slots() *\n+                    VMRegImpl::stack_slot_size *\n+                    PRegisterImpl::number_of_saved_registers, 16);\n+  }\n+#endif\n+  return 0;\n+}\n+\n@@ -150,0 +173,3 @@\n+  int sve_predicate_size_in_slots = 0;\n+  int total_predicate_in_bytes = total_sve_predicate_in_bytes();\n+  int total_predicate_in_slots = total_predicate_in_bytes \/ VMRegImpl::stack_slot_size;\n@@ -153,2 +179,5 @@\n-  sve_vector_size_in_bytes = Matcher::scalable_vector_reg_size(T_BYTE);\n-  sve_vector_size_in_slots = Matcher::scalable_vector_reg_size(T_FLOAT);\n+  if (use_sve) {\n+    sve_vector_size_in_bytes = Matcher::scalable_vector_reg_size(T_BYTE);\n+    sve_vector_size_in_slots = Matcher::scalable_vector_reg_size(T_FLOAT);\n+    sve_predicate_size_in_slots = Matcher::scalable_predicate_reg_slots();\n+  }\n@@ -159,1 +188,0 @@\n-    int vect_words = 0;\n@@ -167,3 +195,4 @@\n-    vect_words = FloatRegisterImpl::number_of_registers * extra_save_slots_per_register \/\n-                 VMRegImpl::slots_per_word;\n-    additional_frame_words += vect_words;\n+    int extra_vector_bytes = extra_save_slots_per_register *\n+                             VMRegImpl::stack_slot_size *\n+                             FloatRegisterImpl::number_of_registers;\n+    additional_frame_words += ((extra_vector_bytes + total_predicate_in_bytes) \/ wordSize);\n@@ -187,1 +216,1 @@\n-  __ push_CPU_state(_save_vectors, use_sve, sve_vector_size_in_bytes);\n+  __ push_CPU_state(_save_vectors, use_sve, sve_vector_size_in_bytes, total_predicate_in_bytes);\n@@ -204,2 +233,1 @@\n-      oop_map->set_callee_saved(VMRegImpl::stack2reg(sp_offset + additional_frame_slots),\n-                                r->as_VMReg());\n+      oop_map->set_callee_saved(VMRegImpl::stack2reg(sp_offset + additional_frame_slots), r->as_VMReg());\n@@ -213,1 +241,1 @@\n-      sp_offset = use_sve ? (sve_vector_size_in_slots * i) :\n+      sp_offset = use_sve ? (total_predicate_in_slots + sve_vector_size_in_slots * i) :\n@@ -218,2 +246,9 @@\n-    oop_map->set_callee_saved(VMRegImpl::stack2reg(sp_offset),\n-                              r->as_VMReg());\n+    oop_map->set_callee_saved(VMRegImpl::stack2reg(sp_offset), r->as_VMReg());\n+  }\n+\n+  if (_save_vectors && use_sve) {\n+    for (int i = 0; i < PRegisterImpl::number_of_saved_registers; i++) {\n+      PRegister r = as_PRegister(i);\n+      int sp_offset = sve_predicate_size_in_slots * i;\n+      oop_map->set_callee_saved(VMRegImpl::stack2reg(sp_offset), r->as_VMReg());\n+    }\n@@ -228,1 +263,1 @@\n-                   Matcher::scalable_vector_reg_size(T_BYTE));\n+                   Matcher::scalable_vector_reg_size(T_BYTE), total_sve_predicate_in_bytes());\n@@ -241,0 +276,2 @@\n+\/\/ The SVE supported min vector size is 8 bytes and we need to save\n+\/\/ predicate registers when the vector size is 8 bytes as well.\n@@ -242,1 +279,1 @@\n-  return size > 8;\n+  return size > 8 || (UseSVE > 0 && size >= 8);\n@@ -1115,63 +1152,0 @@\n-\/\/ Unpack an array argument into a pointer to the body and the length\n-\/\/ if the array is non-null, otherwise pass 0 for both.\n-static void unpack_array_argument(MacroAssembler* masm, VMRegPair reg, BasicType in_elem_type, VMRegPair body_arg, VMRegPair length_arg) { Unimplemented(); }\n-\n-\n-class ComputeMoveOrder: public StackObj {\n-  class MoveOperation: public ResourceObj {\n-    friend class ComputeMoveOrder;\n-   private:\n-    VMRegPair        _src;\n-    VMRegPair        _dst;\n-    int              _src_index;\n-    int              _dst_index;\n-    bool             _processed;\n-    MoveOperation*  _next;\n-    MoveOperation*  _prev;\n-\n-    static int get_id(VMRegPair r) { Unimplemented(); return 0; }\n-\n-   public:\n-    MoveOperation(int src_index, VMRegPair src, int dst_index, VMRegPair dst):\n-      _src(src)\n-    , _dst(dst)\n-    , _src_index(src_index)\n-    , _dst_index(dst_index)\n-    , _processed(false)\n-    , _next(NULL)\n-    , _prev(NULL) { Unimplemented(); }\n-\n-    VMRegPair src() const              { Unimplemented(); return _src; }\n-    int src_id() const                 { Unimplemented(); return 0; }\n-    int src_index() const              { Unimplemented(); return 0; }\n-    VMRegPair dst() const              { Unimplemented(); return _src; }\n-    void set_dst(int i, VMRegPair dst) { Unimplemented(); }\n-    int dst_index() const              { Unimplemented(); return 0; }\n-    int dst_id() const                 { Unimplemented(); return 0; }\n-    MoveOperation* next() const        { Unimplemented(); return 0; }\n-    MoveOperation* prev() const        { Unimplemented(); return 0; }\n-    void set_processed()               { Unimplemented(); }\n-    bool is_processed() const          { Unimplemented(); return 0; }\n-\n-    \/\/ insert\n-    void break_cycle(VMRegPair temp_register) { Unimplemented(); }\n-\n-    void link(GrowableArray<MoveOperation*>& killer) { Unimplemented(); }\n-  };\n-\n- private:\n-  GrowableArray<MoveOperation*> edges;\n-\n- public:\n-  ComputeMoveOrder(int total_in_args, VMRegPair* in_regs, int total_c_args, VMRegPair* out_regs,\n-                    BasicType* in_sig_bt, GrowableArray<int>& arg_order, VMRegPair tmp_vmreg) { Unimplemented(); }\n-\n-  \/\/ Collected all the move operations\n-  void add_edge(int src_index, VMRegPair src, int dst_index, VMRegPair dst) { Unimplemented(); }\n-\n-  \/\/ Walk the edges breaking cycles between moves.  The result list\n-  \/\/ can be walked in order to produce the proper set of loads\n-  GrowableArray<MoveOperation*>* get_store_order(VMRegPair temp_register) { Unimplemented(); return 0; }\n-};\n-\n-\n@@ -1290,2 +1264,1 @@\n-                                                BasicType ret_type,\n-                                                address critical_entry) {\n+                                                BasicType ret_type) {\n@@ -1316,6 +1289,1 @@\n-  bool is_critical_native = true;\n-  address native_func = critical_entry;\n-  if (native_func == NULL) {\n-    native_func = method->native_function();\n-    is_critical_native = false;\n-  }\n+  address native_func = method->native_function();\n@@ -1335,13 +1303,1 @@\n-  int total_c_args = total_in_args;\n-  if (!is_critical_native) {\n-    total_c_args += 1;\n-    if (method->is_static()) {\n-      total_c_args++;\n-    }\n-  } else {\n-    for (int i = 0; i < total_in_args; i++) {\n-      if (in_sig_bt[i] == T_ARRAY) {\n-        total_c_args++;\n-      }\n-    }\n-  }\n+  int total_c_args = total_in_args + (method->is_static() ? 2 : 1);\n@@ -1354,5 +1310,4 @@\n-  if (!is_critical_native) {\n-    out_sig_bt[argc++] = T_ADDRESS;\n-    if (method->is_static()) {\n-      out_sig_bt[argc++] = T_OBJECT;\n-    }\n+  out_sig_bt[argc++] = T_ADDRESS;\n+  if (method->is_static()) {\n+    out_sig_bt[argc++] = T_OBJECT;\n+  }\n@@ -1360,24 +1315,2 @@\n-    for (int i = 0; i < total_in_args ; i++ ) {\n-      out_sig_bt[argc++] = in_sig_bt[i];\n-    }\n-  } else {\n-    in_elem_bt = NEW_RESOURCE_ARRAY(BasicType, total_in_args);\n-    SignatureStream ss(method->signature());\n-    for (int i = 0; i < total_in_args ; i++ ) {\n-      if (in_sig_bt[i] == T_ARRAY) {\n-        \/\/ Arrays are passed as int, elem* pair\n-        out_sig_bt[argc++] = T_INT;\n-        out_sig_bt[argc++] = T_ADDRESS;\n-        ss.skip_array_prefix(1);  \/\/ skip one '['\n-        assert(ss.is_primitive(), \"primitive type expected\");\n-        in_elem_bt[i] = ss.type();\n-      } else {\n-        out_sig_bt[argc++] = in_sig_bt[i];\n-        in_elem_bt[i] = T_VOID;\n-      }\n-      if (in_sig_bt[i] != T_VOID) {\n-        assert(in_sig_bt[i] == ss.type() ||\n-               in_sig_bt[i] == T_ARRAY, \"must match\");\n-        ss.next();\n-      }\n-    }\n+  for (int i = 0; i < total_in_args ; i++ ) {\n+    out_sig_bt[argc++] = in_sig_bt[i];\n@@ -1405,28 +1338,0 @@\n-  if (is_critical_native) {\n-    \/\/ Critical natives may have to call out so they need a save area\n-    \/\/ for register arguments.\n-    int double_slots = 0;\n-    int single_slots = 0;\n-    for ( int i = 0; i < total_in_args; i++) {\n-      if (in_regs[i].first()->is_Register()) {\n-        const Register reg = in_regs[i].first()->as_Register();\n-        switch (in_sig_bt[i]) {\n-          case T_BOOLEAN:\n-          case T_BYTE:\n-          case T_SHORT:\n-          case T_CHAR:\n-          case T_INT:  single_slots++; break;\n-          case T_ARRAY:  \/\/ specific to LP64 (7145024)\n-          case T_LONG: double_slots++; break;\n-          default:  ShouldNotReachHere();\n-        }\n-      } else if (in_regs[i].first()->is_FloatRegister()) {\n-        ShouldNotReachHere();\n-      }\n-    }\n-    total_save_slots = double_slots * 2 + single_slots;\n-    \/\/ align the save area\n-    if (double_slots != 0) {\n-      stack_slots = align_up(stack_slots, 2);\n-    }\n-  }\n@@ -1599,4 +1504,1 @@\n-  \/\/ This may iterate in two different directions depending on the\n-  \/\/ kind of native it is.  The reason is that for regular JNI natives\n-  \/\/ the incoming and outgoing registers are offset upwards and for\n-  \/\/ critical natives they are offset down.\n+  \/\/ For JNI natives the incoming and outgoing registers are offset upwards.\n@@ -1607,8 +1509,3 @@\n-  if (!is_critical_native) {\n-    for (int i = total_in_args - 1, c_arg = total_c_args - 1; i >= 0; i--, c_arg--) {\n-      arg_order.push(i);\n-      arg_order.push(c_arg);\n-    }\n-  } else {\n-    \/\/ Compute a valid move order, using tmp_vmreg to break any cycles\n-    ComputeMoveOrder cmo(total_in_args, in_regs, total_c_args, out_regs, in_sig_bt, arg_order, tmp_vmreg);\n+  for (int i = total_in_args - 1, c_arg = total_c_args - 1; i >= 0; i--, c_arg--) {\n+    arg_order.push(i);\n+    arg_order.push(c_arg);\n@@ -1622,14 +1519,1 @@\n-    if (c_arg == -1) {\n-      assert(is_critical_native, \"should only be required for critical natives\");\n-      \/\/ This arg needs to be moved to a temporary\n-      __ mov(tmp_vmreg.first()->as_Register(), in_regs[i].first()->as_Register());\n-      in_regs[i] = tmp_vmreg;\n-      temploc = i;\n-      continue;\n-    } else if (i == -1) {\n-      assert(is_critical_native, \"should only be required for critical natives\");\n-      \/\/ Read from the temporary location\n-      assert(temploc != -1, \"must be valid\");\n-      i = temploc;\n-      temploc = -1;\n-    }\n+    assert(c_arg != -1 && i != -1, \"wrong order\");\n@@ -1650,13 +1534,0 @@\n-        if (is_critical_native) {\n-          unpack_array_argument(masm, in_regs[i], in_elem_bt[i], out_regs[c_arg + 1], out_regs[c_arg]);\n-          c_arg++;\n-#ifdef ASSERT\n-          if (out_regs[c_arg].first()->is_Register()) {\n-            reg_destroyed[out_regs[c_arg].first()->as_Register()->encoding()] = true;\n-          } else if (out_regs[c_arg].first()->is_FloatRegister()) {\n-            freg_destroyed[out_regs[c_arg].first()->as_FloatRegister()->encoding()] = true;\n-          }\n-#endif\n-          int_args++;\n-          break;\n-        }\n@@ -1664,1 +1535,0 @@\n-        assert(!is_critical_native, \"no oop arguments\");\n@@ -1704,1 +1574,1 @@\n-  if (method->is_static() && !is_critical_native) {\n+  if (method->is_static()) {\n@@ -1762,1 +1632,0 @@\n-    assert(!is_critical_native, \"unhandled\");\n@@ -1816,2 +1685,1 @@\n-  if (!is_critical_native) {\n-    __ lea(c_rarg0, Address(rthread, in_bytes(JavaThread::jni_environment_offset())));\n+  __ lea(c_rarg0, Address(rthread, in_bytes(JavaThread::jni_environment_offset())));\n@@ -1819,5 +1687,4 @@\n-    \/\/ Now set thread in native\n-    __ mov(rscratch1, _thread_in_native);\n-    __ lea(rscratch2, Address(rthread, JavaThread::thread_state_offset()));\n-    __ stlrw(rscratch1, rscratch2);\n-  }\n+  \/\/ Now set thread in native\n+  __ mov(rscratch1, _thread_in_native);\n+  __ lea(rscratch2, Address(rthread, JavaThread::thread_state_offset()));\n+  __ stlrw(rscratch1, rscratch2);\n@@ -1854,12 +1721,0 @@\n-  \/\/ If this is a critical native, check for a safepoint or suspend request after the call.\n-  \/\/ If a safepoint is needed, transition to native, then to native_trans to handle\n-  \/\/ safepoints like the native methods that are not critical natives.\n-  if (is_critical_native) {\n-    Label needs_safepoint;\n-    __ safepoint_poll(needs_safepoint, false \/* at_return *\/, true \/* acquire *\/, false \/* in_nmethod *\/);\n-    __ ldrw(rscratch1, Address(rthread, JavaThread::suspend_flags_offset()));\n-    __ cbnzw(rscratch1, needs_safepoint);\n-    __ b(after_transition);\n-    __ bind(needs_safepoint);\n-  }\n-\n@@ -1974,5 +1829,3 @@\n-  if (!is_critical_native) {\n-    \/\/ reset handle block\n-    __ ldr(r2, Address(rthread, JavaThread::active_handles_offset()));\n-    __ str(zr, Address(r2, JNIHandleBlock::top_offset_in_bytes()));\n-  }\n+  \/\/ reset handle block\n+  __ ldr(r2, Address(rthread, JavaThread::active_handles_offset()));\n+  __ str(zr, Address(r2, JNIHandleBlock::top_offset_in_bytes()));\n@@ -1982,5 +1835,3 @@\n-  if (!is_critical_native) {\n-    \/\/ Any exception pending?\n-    __ ldr(rscratch1, Address(rthread, in_bytes(Thread::pending_exception_offset())));\n-    __ cbnz(rscratch1, exception_pending);\n-  }\n+  \/\/ Any exception pending?\n+  __ ldr(rscratch1, Address(rthread, in_bytes(Thread::pending_exception_offset())));\n+  __ cbnz(rscratch1, exception_pending);\n@@ -1993,3 +1844,2 @@\n-  if (!is_critical_native) {\n-    \/\/ forward the exception\n-    __ bind(exception_pending);\n+  \/\/ forward the exception\n+  __ bind(exception_pending);\n@@ -1997,3 +1847,2 @@\n-    \/\/ and forward the exception\n-    __ far_jump(RuntimeAddress(StubRoutines::forward_exception_entry()));\n-  }\n+  \/\/ and forward the exception\n+  __ far_jump(RuntimeAddress(StubRoutines::forward_exception_entry()));\n","filename":"src\/hotspot\/cpu\/aarch64\/sharedRuntime_aarch64.cpp","additions":83,"deletions":234,"binary":false,"changes":317,"status":"modified"},{"patch":"@@ -0,0 +1,48 @@\n+\/*\n+ * Copyright (c) 2021, Amazon.com Inc. or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef CPU_AARCH64_SPIN_WAIT_AARCH64_HPP\n+#define CPU_AARCH64_SPIN_WAIT_AARCH64_HPP\n+\n+class SpinWait {\n+public:\n+  enum Inst {\n+    NONE = -1,\n+    NOP,\n+    ISB,\n+    YIELD\n+  };\n+\n+private:\n+  Inst _inst;\n+  int _count;\n+\n+public:\n+  SpinWait(Inst inst = NONE, int count = 0) : _inst(inst), _count(count) {}\n+\n+  Inst inst() const { return _inst; }\n+  int inst_count() const { return _count; }\n+};\n+\n+#endif \/\/ CPU_AARCH64_SPIN_WAIT_AARCH64_HPP\n","filename":"src\/hotspot\/cpu\/aarch64\/spin_wait_aarch64.hpp","additions":48,"deletions":0,"binary":false,"changes":48,"status":"added"},{"patch":"@@ -49,0 +49,20 @@\n+SpinWait VM_Version::_spin_wait;\n+\n+static SpinWait get_spin_wait_desc() {\n+  if (strcmp(OnSpinWaitInst, \"nop\") == 0) {\n+    return SpinWait(SpinWait::NOP, OnSpinWaitInstCount);\n+  } else if (strcmp(OnSpinWaitInst, \"isb\") == 0) {\n+    return SpinWait(SpinWait::ISB, OnSpinWaitInstCount);\n+  } else if (strcmp(OnSpinWaitInst, \"yield\") == 0) {\n+    return SpinWait(SpinWait::YIELD, OnSpinWaitInstCount);\n+  } else if (strcmp(OnSpinWaitInst, \"none\") != 0) {\n+    vm_exit_during_initialization(\"The options for OnSpinWaitInst are nop, isb, yield, and none\", OnSpinWaitInst);\n+  }\n+\n+  if (!FLAG_IS_DEFAULT(OnSpinWaitInstCount) && OnSpinWaitInstCount > 0) {\n+    vm_exit_during_initialization(\"OnSpinWaitInstCount cannot be used for OnSpinWaitInst 'none'\");\n+  }\n+\n+  return SpinWait{};\n+}\n+\n@@ -185,0 +205,8 @@\n+\n+    if (FLAG_IS_DEFAULT(OnSpinWaitInst)) {\n+      FLAG_SET_DEFAULT(OnSpinWaitInst, \"isb\");\n+    }\n+\n+    if (FLAG_IS_DEFAULT(OnSpinWaitInstCount)) {\n+      FLAG_SET_DEFAULT(OnSpinWaitInstCount, 1);\n+    }\n@@ -454,1 +482,1 @@\n-  UNSUPPORTED_OPTION(CriticalJNINatives);\n+  _spin_wait = get_spin_wait_desc();\n","filename":"src\/hotspot\/cpu\/aarch64\/vm_version_aarch64.cpp","additions":29,"deletions":1,"binary":false,"changes":30,"status":"modified"},{"patch":"@@ -29,0 +29,1 @@\n+#include \"spin_wait_aarch64.hpp\"\n@@ -48,0 +49,2 @@\n+  static SpinWait _spin_wait;\n+\n@@ -145,0 +148,4 @@\n+  static const SpinWait& spin_wait_desc() { return _spin_wait; }\n+\n+  static bool supports_on_spin_wait() { return _spin_wait.inst() != SpinWait::NONE; }\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/vm_version_aarch64.hpp","additions":7,"deletions":0,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -986,0 +986,4 @@\n+const bool Matcher::match_rule_supported_vector_masked(int opcode, int vlen, BasicType bt) {\n+  return false;\n+}\n+\n","filename":"src\/hotspot\/cpu\/arm\/arm.ad","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -67,1 +67,1 @@\n-  : _index(index), _array(NULL), _throw_index_out_of_bounds_exception(true) {\n+  : _index(index), _array(), _throw_index_out_of_bounds_exception(true) {\n","filename":"src\/hotspot\/cpu\/arm\/c1_CodeStubs_arm.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -69,2 +69,2 @@\n-LIR_Opr FrameMap::_caller_save_cpu_regs[] = { 0 };\n-LIR_Opr FrameMap::_caller_save_fpu_regs[];  \/\/ same as initialize to zero\n+LIR_Opr FrameMap::_caller_save_cpu_regs[] = {};\n+LIR_Opr FrameMap::_caller_save_fpu_regs[] = {};\n","filename":"src\/hotspot\/cpu\/arm\/c1_FrameMap_arm.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -1384,1 +1384,1 @@\n-  assert(op->addr()->is_register() || op->addr()->as_address_ptr()->index() == LIR_OprDesc::illegalOpr(), \"unexpected index\");\n+  assert(op->addr()->is_register() || op->addr()->as_address_ptr()->index() == LIR_Opr::illegalOpr(), \"unexpected index\");\n","filename":"src\/hotspot\/cpu\/arm\/c1_LIRAssembler_arm.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -366,1 +366,1 @@\n-void LIRGenerator::CardTableBarrierSet_post_barrier_helper(LIR_OprDesc* addr, LIR_Const* card_table_base) {\n+void LIRGenerator::CardTableBarrierSet_post_barrier_helper(LIR_Opr addr, LIR_Const* card_table_base) {\n@@ -633,1 +633,1 @@\n-    arithmetic_op_int(x->op(), x->operand(), left_arg->result(), right_arg->result(), NULL);\n+    arithmetic_op_int(x->op(), x->operand(), left_arg->result(), right_arg->result(), LIR_OprFact::nullOpr);\n","filename":"src\/hotspot\/cpu\/arm\/c1_LIRGenerator_arm.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -28,1 +28,1 @@\n-FloatRegister LIR_OprDesc::as_float_reg() const {\n+FloatRegister LIR_Opr::as_float_reg() const {\n@@ -32,1 +32,1 @@\n-FloatRegister LIR_OprDesc::as_double_reg() const {\n+FloatRegister LIR_Opr::as_double_reg() const {\n@@ -38,5 +38,5 @@\n-  return (LIR_Opr)(intptr_t)((reg1 << LIR_OprDesc::reg1_shift) |\n-                             (reg2 << LIR_OprDesc::reg2_shift) |\n-                             LIR_OprDesc::double_type          |\n-                             LIR_OprDesc::fpu_register         |\n-                             LIR_OprDesc::double_size);\n+  return (LIR_Opr)(intptr_t)((reg1 << LIR_Opr::reg1_shift) |\n+                             (reg2 << LIR_Opr::reg2_shift) |\n+                             LIR_Opr::double_type          |\n+                             LIR_Opr::fpu_register         |\n+                             LIR_Opr::double_size);\n","filename":"src\/hotspot\/cpu\/arm\/c1_LIR_arm.cpp","additions":7,"deletions":7,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -753,2 +753,1 @@\n-                                                BasicType ret_type,\n-                                                address critical_entry) {\n+                                                BasicType ret_type) {\n@@ -780,1 +779,1 @@\n-  bool is_static = method->is_static();\n+  bool method_is_static = method->is_static();\n@@ -783,4 +782,1 @@\n-  int total_c_args = total_in_args + 1;\n-  if (is_static) {\n-    total_c_args++;\n-  }\n+  int total_c_args = total_in_args + (method_is_static ? 2 : 1);\n@@ -793,1 +789,1 @@\n-  if (is_static) {\n+  if (method_is_static) {\n@@ -884,1 +880,1 @@\n-  const int extra_args = is_static ? 2 : 1;\n+  const int extra_args = method_is_static ? 2 : 1;\n@@ -907,1 +903,1 @@\n-        if ((i == 0) && (!is_static)) {\n+        if ((i == 0) && (!method_is_static)) {\n@@ -1119,1 +1115,1 @@\n-  if (is_static) {\n+  if (method_is_static) {\n@@ -1335,1 +1331,1 @@\n-                                     in_ByteSize(is_static ? klass_offset : receiver_offset),\n+                                     in_ByteSize(method_is_static ? klass_offset : receiver_offset),\n","filename":"src\/hotspot\/cpu\/arm\/sharedRuntime_arm.cpp","additions":8,"deletions":12,"binary":false,"changes":20,"status":"modified"},{"patch":"@@ -338,1 +338,0 @@\n-  UNSUPPORTED_OPTION(CriticalJNINatives);\n","filename":"src\/hotspot\/cpu\/arm\/vm_version_arm_32.cpp","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -74,1 +74,1 @@\n-  : _index(index), _array(NULL), _throw_index_out_of_bounds_exception(true) {\n+  : _index(index), _array(), _throw_index_out_of_bounds_exception(true) {\n","filename":"src\/hotspot\/cpu\/ppc\/c1_CodeStubs_ppc.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -188,2 +188,2 @@\n-LIR_Opr FrameMap::_caller_save_cpu_regs[] = { 0, };\n-LIR_Opr FrameMap::_caller_save_fpu_regs[] = { 0, };\n+LIR_Opr FrameMap::_caller_save_cpu_regs[] = {};\n+LIR_Opr FrameMap::_caller_save_fpu_regs[] = {};\n","filename":"src\/hotspot\/cpu\/ppc\/c1_FrameMap_ppc.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -246,1 +246,1 @@\n-  LIR_Opr r = NULL;\n+  LIR_Opr r;\n","filename":"src\/hotspot\/cpu\/ppc\/c1_LIRGenerator_ppc.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -30,1 +30,1 @@\n-FloatRegister LIR_OprDesc::as_float_reg() const {\n+FloatRegister LIR_Opr::as_float_reg() const {\n@@ -34,1 +34,1 @@\n-FloatRegister LIR_OprDesc::as_double_reg() const {\n+FloatRegister LIR_Opr::as_double_reg() const {\n@@ -41,5 +41,5 @@\n-  return (LIR_Opr)(intptr_t)((reg1 << LIR_OprDesc::reg1_shift) |\n-                             (reg1 << LIR_OprDesc::reg2_shift) |\n-                             LIR_OprDesc::double_type          |\n-                             LIR_OprDesc::fpu_register         |\n-                             LIR_OprDesc::double_size);\n+  return (LIR_Opr)(intptr_t)((reg1 << LIR_Opr::reg1_shift) |\n+                             (reg1 << LIR_Opr::reg2_shift) |\n+                             LIR_Opr::double_type          |\n+                             LIR_Opr::fpu_register         |\n+                             LIR_Opr::double_size);\n","filename":"src\/hotspot\/cpu\/ppc\/c1_LIR_ppc.cpp","additions":7,"deletions":7,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -154,0 +154,2 @@\n+  __ block_comment(\"nmethod_entry_barrier (nmethod_entry_barrier) {\");\n+\n@@ -170,0 +172,2 @@\n+\n+  __ block_comment(\"} nmethod_entry_barrier (nmethod_entry_barrier)\");\n@@ -180,0 +184,2 @@\n+  __ block_comment(\"c2i_entry_barrier (c2i_entry_barrier) {\");\n+\n@@ -210,0 +216,2 @@\n+\n+  __ block_comment(\"} c2i_entry_barrier (c2i_entry_barrier)\");\n","filename":"src\/hotspot\/cpu\/ppc\/gc\/shared\/barrierSetAssembler_ppc.cpp","additions":8,"deletions":0,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -0,0 +1,162 @@\n+\/*\n+ * Copyright (c) 2018, 2021, Red Hat, Inc. All rights reserved.\n+ * Copyright (c) 2012, 2021 SAP SE. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"asm\/macroAssembler.inline.hpp\"\n+#include \"c1\/c1_LIRAssembler.hpp\"\n+#include \"c1\/c1_MacroAssembler.hpp\"\n+#include \"gc\/shenandoah\/shenandoahBarrierSet.hpp\"\n+#include \"gc\/shenandoah\/shenandoahBarrierSetAssembler.hpp\"\n+#include \"gc\/shenandoah\/c1\/shenandoahBarrierSetC1.hpp\"\n+\n+#define __ masm->masm()->\n+\n+void LIR_OpShenandoahCompareAndSwap::emit_code(LIR_Assembler *masm) {\n+  __ block_comment(\"LIR_OpShenandoahCompareAndSwap (shenandaohgc) {\");\n+\n+  Register addr = _addr->as_register_lo();\n+  Register new_val = _new_value->as_register();\n+  Register cmp_val = _cmp_value->as_register();\n+  Register tmp1 = _tmp1->as_register();\n+  Register tmp2 = _tmp2->as_register();\n+  Register result = result_opr()->as_register();\n+\n+  if (ShenandoahIUBarrier) {\n+    ShenandoahBarrierSet::assembler()->iu_barrier(masm->masm(), new_val, tmp1, tmp2,\n+                                                  MacroAssembler::PRESERVATION_FRAME_LR_GP_FP_REGS);\n+  }\n+\n+  if (UseCompressedOops) {\n+    __ encode_heap_oop(cmp_val, cmp_val);\n+    __ encode_heap_oop(new_val, new_val);\n+  }\n+\n+  \/\/ Due to the memory barriers emitted in ShenandoahBarrierSetC1::atomic_cmpxchg_at_resolved,\n+  \/\/ there is no need to specify stronger memory semantics.\n+  ShenandoahBarrierSet::assembler()->cmpxchg_oop(masm->masm(), addr, cmp_val, new_val, tmp1, tmp2,\n+                                                 false, result);\n+\n+  if (UseCompressedOops) {\n+    __ decode_heap_oop(cmp_val);\n+    __ decode_heap_oop(new_val);\n+  }\n+\n+  __ block_comment(\"} LIR_OpShenandoahCompareAndSwap (shenandaohgc)\");\n+}\n+\n+#undef __\n+\n+#ifdef ASSERT\n+#define __ gen->lir(__FILE__, __LINE__)->\n+#else\n+#define __ gen->lir()->\n+#endif\n+\n+LIR_Opr ShenandoahBarrierSetC1::atomic_cmpxchg_at_resolved(LIRAccess &access, LIRItem &cmp_value, LIRItem &new_value) {\n+  BasicType bt = access.type();\n+\n+  if (access.is_oop()) {\n+    LIRGenerator* gen = access.gen();\n+\n+    if (ShenandoahCASBarrier) {\n+      if (support_IRIW_for_not_multiple_copy_atomic_cpu) {\n+        __ membar();\n+      } else {\n+        __ membar_release();\n+      }\n+    }\n+\n+    if (ShenandoahSATBBarrier) {\n+      pre_barrier(gen, access.access_emit_info(), access.decorators(), access.resolved_addr(),\n+                  LIR_OprFact::illegalOpr);\n+    }\n+\n+    if (ShenandoahCASBarrier) {\n+      cmp_value.load_item();\n+      new_value.load_item();\n+\n+      LIR_Opr t1 = gen->new_register(T_OBJECT);\n+      LIR_Opr t2 = gen->new_register(T_OBJECT);\n+      LIR_Opr addr = access.resolved_addr()->as_address_ptr()->base();\n+      LIR_Opr result = gen->new_register(T_INT);\n+\n+      __ append(new LIR_OpShenandoahCompareAndSwap(addr, cmp_value.result(), new_value.result(), t1, t2, result));\n+\n+      if (support_IRIW_for_not_multiple_copy_atomic_cpu) {\n+        __ membar_acquire();\n+      } else {\n+        __ membar();\n+      }\n+\n+      return result;\n+    }\n+  }\n+\n+  return BarrierSetC1::atomic_cmpxchg_at_resolved(access, cmp_value, new_value);\n+}\n+\n+LIR_Opr ShenandoahBarrierSetC1::atomic_xchg_at_resolved(LIRAccess &access, LIRItem &value) {\n+  LIRGenerator* gen = access.gen();\n+  BasicType type = access.type();\n+\n+  LIR_Opr result = gen->new_register(type);\n+  value.load_item();\n+  LIR_Opr value_opr = value.result();\n+\n+  if (support_IRIW_for_not_multiple_copy_atomic_cpu) {\n+    __ membar();\n+  } else {\n+    __ membar_release();\n+  }\n+\n+  if (access.is_oop()) {\n+    value_opr = iu_barrier(access.gen(), value_opr, access.access_emit_info(), access.decorators());\n+  }\n+\n+  assert(type == T_INT || is_reference_type(type) LP64_ONLY( || type == T_LONG ), \"unexpected type\");\n+  LIR_Opr tmp_xchg = gen->new_register(T_INT);\n+  __ xchg(access.resolved_addr(), value_opr, result, tmp_xchg);\n+\n+  if (access.is_oop()) {\n+    result = load_reference_barrier_impl(access.gen(), result, LIR_OprFact::addressConst(0),\n+                                         access.decorators());\n+\n+    LIR_Opr tmp_barrier = gen->new_register(type);\n+    __ move(result, tmp_barrier);\n+    result = tmp_barrier;\n+\n+    if (ShenandoahSATBBarrier) {\n+      pre_barrier(access.gen(), access.access_emit_info(), access.decorators(), LIR_OprFact::illegalOpr, result);\n+    }\n+  }\n+\n+  if (support_IRIW_for_not_multiple_copy_atomic_cpu) {\n+    __ membar_acquire();\n+  } else {\n+    __ membar();\n+  }\n+\n+  return result;\n+}\n","filename":"src\/hotspot\/cpu\/ppc\/gc\/shenandoah\/c1\/shenandoahBarrierSetC1_ppc.cpp","additions":162,"deletions":0,"binary":false,"changes":162,"status":"added"},{"patch":"@@ -0,0 +1,1012 @@\n+\/*\n+ * Copyright (c) 2018, 2021, Red Hat, Inc. All rights reserved.\n+ * Copyright (c) 2012, 2021 SAP SE. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#include \"gc\/shared\/gcArguments.hpp\"\n+#include \"gc\/shared\/gc_globals.hpp\"\n+#include \"macroAssembler_ppc.hpp\"\n+#include \"precompiled.hpp\"\n+#include \"asm\/macroAssembler.inline.hpp\"\n+#include \"gc\/shenandoah\/shenandoahBarrierSet.hpp\"\n+#include \"gc\/shenandoah\/shenandoahBarrierSetAssembler.hpp\"\n+#include \"gc\/shenandoah\/shenandoahForwarding.hpp\"\n+#include \"gc\/shenandoah\/shenandoahHeap.hpp\"\n+#include \"gc\/shenandoah\/shenandoahHeap.inline.hpp\"\n+#include \"gc\/shenandoah\/shenandoahHeapRegion.hpp\"\n+#include \"gc\/shenandoah\/shenandoahRuntime.hpp\"\n+#include \"gc\/shenandoah\/shenandoahThreadLocalData.hpp\"\n+#include \"gc\/shenandoah\/heuristics\/shenandoahHeuristics.hpp\"\n+#include \"interpreter\/interpreter.hpp\"\n+#include \"runtime\/sharedRuntime.hpp\"\n+#include \"runtime\/thread.hpp\"\n+#include \"utilities\/globalDefinitions.hpp\"\n+#include \"vm_version_ppc.hpp\"\n+\n+#ifdef COMPILER1\n+\n+#include \"c1\/c1_LIRAssembler.hpp\"\n+#include \"c1\/c1_MacroAssembler.hpp\"\n+#include \"gc\/shenandoah\/c1\/shenandoahBarrierSetC1.hpp\"\n+\n+#endif\n+\n+#define __ masm->\n+\n+void ShenandoahBarrierSetAssembler::satb_write_barrier(MacroAssembler *masm,\n+                                                       Register base, RegisterOrConstant ind_or_offs,\n+                                                       Register tmp1, Register tmp2, Register tmp3,\n+                                                       MacroAssembler::PreservationLevel preservation_level) {\n+  if (ShenandoahSATBBarrier) {\n+    __ block_comment(\"satb_write_barrier (shenandoahgc) {\");\n+    satb_write_barrier_impl(masm, 0, base, ind_or_offs, tmp1, tmp2, tmp3, preservation_level);\n+    __ block_comment(\"} satb_write_barrier (shenandoahgc)\");\n+  }\n+}\n+\n+void ShenandoahBarrierSetAssembler::iu_barrier(MacroAssembler *masm,\n+                                               Register val,\n+                                               Register tmp1, Register tmp2,\n+                                               MacroAssembler::PreservationLevel preservation_level,\n+                                               DecoratorSet decorators) {\n+  \/\/ IU barriers are also employed to avoid resurrection of weak references,\n+  \/\/ even if Shenandoah does not operate in incremental update mode.\n+  if (ShenandoahIUBarrier || ShenandoahSATBBarrier) {\n+    __ block_comment(\"iu_barrier (shenandoahgc) {\");\n+    satb_write_barrier_impl(masm, decorators, noreg, noreg, val, tmp1, tmp2, preservation_level);\n+    __ block_comment(\"} iu_barrier (shenandoahgc)\");\n+  }\n+}\n+\n+void ShenandoahBarrierSetAssembler::load_reference_barrier(MacroAssembler *masm, DecoratorSet decorators,\n+                                                           Register base, RegisterOrConstant ind_or_offs,\n+                                                           Register dst,\n+                                                           Register tmp1, Register tmp2,\n+                                                           MacroAssembler::PreservationLevel preservation_level) {\n+  if (ShenandoahLoadRefBarrier) {\n+    __ block_comment(\"load_reference_barrier (shenandoahgc) {\");\n+    load_reference_barrier_impl(masm, decorators, base, ind_or_offs, dst, tmp1, tmp2, preservation_level);\n+    __ block_comment(\"} load_reference_barrier (shenandoahgc)\");\n+  }\n+}\n+\n+void ShenandoahBarrierSetAssembler::arraycopy_prologue(MacroAssembler *masm, DecoratorSet decorators, BasicType type,\n+                                                       Register src, Register dst, Register count,\n+                                                       Register preserve1, Register preserve2) {\n+  __ block_comment(\"arraycopy_prologue (shenandoahgc) {\");\n+\n+  Register R11_tmp = R11_scratch1;\n+\n+  assert_different_registers(src, dst, count, R11_tmp, noreg);\n+  if (preserve1 != noreg) {\n+    \/\/ Technically not required, but likely to indicate an error.\n+    assert_different_registers(preserve1, preserve2);\n+  }\n+\n+  \/* ==== Check whether barrier is required (optimizations) ==== *\/\n+  \/\/ Fast path: Component type of array is not a reference type.\n+  if (!is_reference_type(type)) {\n+    return;\n+  }\n+\n+  bool dest_uninitialized = (decorators & IS_DEST_UNINITIALIZED) != 0;\n+\n+  \/\/ Fast path: No barrier required if for every barrier type, it is either disabled or would not store\n+  \/\/ any useful information.\n+  if ((!ShenandoahSATBBarrier || dest_uninitialized) && !ShenandoahIUBarrier && !ShenandoahLoadRefBarrier) {\n+    return;\n+  }\n+\n+  Label skip_prologue;\n+\n+  \/\/ Fast path: Array is of length zero.\n+  __ cmpdi(CCR0, count, 0);\n+  __ beq(CCR0, skip_prologue);\n+\n+  \/* ==== Check whether barrier is required (gc state) ==== *\/\n+  __ lbz(R11_tmp, in_bytes(ShenandoahThreadLocalData::gc_state_offset()),\n+         R16_thread);\n+\n+  \/\/ The set of garbage collection states requiring barriers depends on the available barrier types and the\n+  \/\/ type of the reference in question.\n+  \/\/ For instance, satb barriers may be skipped if it is certain that the overridden values are not relevant\n+  \/\/ for the garbage collector.\n+  const int required_states = ShenandoahSATBBarrier && dest_uninitialized\n+                              ? ShenandoahHeap::HAS_FORWARDED\n+                              : ShenandoahHeap::HAS_FORWARDED | ShenandoahHeap::MARKING;\n+\n+  __ andi_(R11_tmp, R11_tmp, required_states);\n+  __ beq(CCR0, skip_prologue);\n+\n+  \/* ==== Invoke runtime ==== *\/\n+  \/\/ Save to-be-preserved registers.\n+  int highest_preserve_register_index = 0;\n+  {\n+    if (preserve1 != noreg && preserve1->is_volatile()) {\n+      __ std(preserve1, -BytesPerWord * ++highest_preserve_register_index, R1_SP);\n+    }\n+    if (preserve2 != noreg && preserve2 != preserve1 && preserve2->is_volatile()) {\n+      __ std(preserve2, -BytesPerWord * ++highest_preserve_register_index, R1_SP);\n+    }\n+\n+    __ std(src, -BytesPerWord * ++highest_preserve_register_index, R1_SP);\n+    __ std(dst, -BytesPerWord * ++highest_preserve_register_index, R1_SP);\n+    __ std(count, -BytesPerWord * ++highest_preserve_register_index, R1_SP);\n+\n+    __ save_LR_CR(R11_tmp);\n+    __ push_frame_reg_args(-BytesPerWord * highest_preserve_register_index,\n+                           R11_tmp);\n+  }\n+\n+  \/\/ Invoke runtime.\n+  address jrt_address = NULL;\n+  if (UseCompressedOops) {\n+    jrt_address = CAST_FROM_FN_PTR(address, ShenandoahRuntime::arraycopy_barrier_narrow_oop_entry);\n+  } else {\n+    jrt_address = CAST_FROM_FN_PTR(address, ShenandoahRuntime::arraycopy_barrier_oop_entry);\n+  }\n+  assert(jrt_address != nullptr, \"jrt routine cannot be found\");\n+\n+  __ call_VM_leaf(jrt_address, src, dst, count);\n+\n+  \/\/ Restore to-be-preserved registers.\n+  {\n+    __ pop_frame();\n+    __ restore_LR_CR(R11_tmp);\n+\n+    __ ld(count, -BytesPerWord * highest_preserve_register_index--, R1_SP);\n+    __ ld(dst, -BytesPerWord * highest_preserve_register_index--, R1_SP);\n+    __ ld(src, -BytesPerWord * highest_preserve_register_index--, R1_SP);\n+\n+    if (preserve2 != noreg && preserve2 != preserve1 && preserve2->is_volatile()) {\n+      __ ld(preserve2, -BytesPerWord * highest_preserve_register_index--, R1_SP);\n+    }\n+    if (preserve1 != noreg && preserve1->is_volatile()) {\n+      __ ld(preserve1, -BytesPerWord * highest_preserve_register_index--, R1_SP);\n+    }\n+  }\n+\n+  __ bind(skip_prologue);\n+  __ block_comment(\"} arraycopy_prologue (shenandoahgc)\");\n+}\n+\n+\/\/ The to-be-enqueued value can either be determined\n+\/\/ - dynamically by passing the reference's address information (load mode) or\n+\/\/ - statically by passing a register the value is stored in (preloaded mode)\n+\/\/   - for performance optimizations in cases where the previous value is known (currently not implemented) and\n+\/\/   - for incremental-update barriers.\n+\/\/\n+\/\/ decorators:  The previous value's decorator set.\n+\/\/              In \"load mode\", the value must equal '0'.\n+\/\/ base:        Base register of the reference's address (load mode).\n+\/\/              In \"preloaded mode\", the register must equal 'noreg'.\n+\/\/ ind_or_offs: Index or offset of the reference's address (load mode).\n+\/\/              If 'base' equals 'noreg' (preloaded mode), the passed value is ignored.\n+\/\/ pre_val:     Register holding the to-be-stored value (preloaded mode).\n+\/\/              In \"load mode\", this register acts as a temporary register and must\n+\/\/              thus not be 'noreg'.  In \"preloaded mode\", its content will be sustained.\n+\/\/ tmp1\/tmp2:   Temporary registers, one of which must be non-volatile in \"preloaded mode\".\n+void ShenandoahBarrierSetAssembler::satb_write_barrier_impl(MacroAssembler *masm, DecoratorSet decorators,\n+                                                            Register base, RegisterOrConstant ind_or_offs,\n+                                                            Register pre_val,\n+                                                            Register tmp1, Register tmp2,\n+                                                            MacroAssembler::PreservationLevel preservation_level) {\n+  assert_different_registers(tmp1, tmp2, pre_val, noreg);\n+\n+  Label skip_barrier;\n+\n+  \/* ==== Determine necessary runtime invocation preservation measures ==== *\/\n+  const bool needs_frame           = preservation_level >= MacroAssembler::PRESERVATION_FRAME_LR;\n+  const bool preserve_gp_registers = preservation_level >= MacroAssembler::PRESERVATION_FRAME_LR_GP_REGS;\n+  const bool preserve_fp_registers = preservation_level >= MacroAssembler::PRESERVATION_FRAME_LR_GP_FP_REGS;\n+\n+  \/\/ Check whether marking is active.\n+  __ lbz(tmp1, in_bytes(ShenandoahThreadLocalData::gc_state_offset()), R16_thread);\n+\n+  __ andi_(tmp1, tmp1, ShenandoahHeap::MARKING);\n+  __ beq(CCR0, skip_barrier);\n+\n+  \/* ==== Determine the reference's previous value ==== *\/\n+  bool preloaded_mode = base == noreg;\n+  Register pre_val_save = noreg;\n+\n+  if (preloaded_mode) {\n+    \/\/ Previous value has been passed to the method, so it must not be determined manually.\n+    \/\/ In case 'pre_val' is a volatile register, it must be saved across the C-call\n+    \/\/ as callers may depend on its value.\n+    \/\/ Unless the general purposes registers are saved anyway, one of the temporary registers\n+    \/\/ (i.e., 'tmp1' and 'tmp2') is used to the preserve 'pre_val'.\n+    if (!preserve_gp_registers && pre_val->is_volatile()) {\n+      pre_val_save = !tmp1->is_volatile() ? tmp1 : tmp2;\n+      assert(!pre_val_save->is_volatile(), \"at least one of the temporary registers must be non-volatile\");\n+    }\n+\n+    if ((decorators & IS_NOT_NULL) != 0) {\n+#ifdef ASSERT\n+      __ cmpdi(CCR0, pre_val, 0);\n+      __ asm_assert_ne(\"null oop is not allowed\");\n+#endif \/\/ ASSERT\n+    } else {\n+      __ cmpdi(CCR0, pre_val, 0);\n+      __ beq(CCR0, skip_barrier);\n+    }\n+  } else {\n+    \/\/ Load from the reference address to determine the reference's current value (before the store is being performed).\n+    \/\/ Contrary to the given value in \"preloaded mode\", it is not necessary to preserve it.\n+    assert(decorators == 0, \"decorator set must be empty\");\n+    assert(base != noreg, \"base must be a register\");\n+    assert(!ind_or_offs.is_register() || ind_or_offs.as_register() != noreg, \"ind_or_offs must be a register\");\n+    if (UseCompressedOops) {\n+      __ lwz(pre_val, ind_or_offs, base);\n+    } else {\n+      __ ld(pre_val, ind_or_offs, base);\n+    }\n+\n+    __ cmpdi(CCR0, pre_val, 0);\n+    __ beq(CCR0, skip_barrier);\n+\n+    if (UseCompressedOops) {\n+      __ decode_heap_oop_not_null(pre_val);\n+    }\n+  }\n+\n+  \/* ==== Try to enqueue the to-be-stored value directly into thread's local SATB mark queue ==== *\/\n+  {\n+    Label runtime;\n+    Register Rbuffer = tmp1, Rindex = tmp2;\n+\n+    \/\/ Check whether the queue has enough capacity to store another oop.\n+    \/\/ If not, jump to the runtime to commit the buffer and to allocate a new one.\n+    \/\/ (The buffer's index corresponds to the amount of remaining free space.)\n+    __ ld(Rindex, in_bytes(ShenandoahThreadLocalData::satb_mark_queue_index_offset()), R16_thread);\n+    __ cmpdi(CCR0, Rindex, 0);\n+    __ beq(CCR0, runtime); \/\/ If index == 0 (buffer is full), goto runtime.\n+\n+    \/\/ Capacity suffices.  Decrement the queue's size by the size of one oop.\n+    \/\/ (The buffer is filled contrary to the heap's growing direction, i.e., it is filled downwards.)\n+    __ addi(Rindex, Rindex, -wordSize);\n+    __ std(Rindex, in_bytes(ShenandoahThreadLocalData::satb_mark_queue_index_offset()), R16_thread);\n+\n+    \/\/ Enqueue the previous value and skip the invocation of the runtime.\n+    __ ld(Rbuffer, in_bytes(ShenandoahThreadLocalData::satb_mark_queue_buffer_offset()), R16_thread);\n+    __ stdx(pre_val, Rbuffer, Rindex);\n+    __ b(skip_barrier);\n+\n+    __ bind(runtime);\n+  }\n+\n+  \/* ==== Invoke runtime to commit SATB mark queue to gc and allocate a new buffer ==== *\/\n+  \/\/ Save to-be-preserved registers.\n+  int nbytes_save = 0;\n+\n+  if (needs_frame) {\n+    if (preserve_gp_registers) {\n+      nbytes_save = (preserve_fp_registers\n+                     ? MacroAssembler::num_volatile_gp_regs + MacroAssembler::num_volatile_fp_regs\n+                     : MacroAssembler::num_volatile_gp_regs) * BytesPerWord;\n+      __ save_volatile_gprs(R1_SP, -nbytes_save, preserve_fp_registers);\n+    }\n+\n+    __ save_LR_CR(tmp1);\n+    __ push_frame_reg_args(nbytes_save, tmp2);\n+  }\n+\n+  if (!preserve_gp_registers && preloaded_mode && pre_val->is_volatile()) {\n+    assert(pre_val_save != noreg, \"nv_save must not be noreg\");\n+\n+    \/\/ 'pre_val' register must be saved manually unless general-purpose are preserved in general.\n+    __ mr(pre_val_save, pre_val);\n+  }\n+\n+  \/\/ Invoke runtime.\n+  __ call_VM_leaf(CAST_FROM_FN_PTR(address, ShenandoahRuntime::write_ref_field_pre_entry), pre_val, R16_thread);\n+\n+  \/\/ Restore to-be-preserved registers.\n+  if (!preserve_gp_registers && preloaded_mode && pre_val->is_volatile()) {\n+    __ mr(pre_val, pre_val_save);\n+  }\n+\n+  if (needs_frame) {\n+    __ pop_frame();\n+    __ restore_LR_CR(tmp1);\n+\n+    if (preserve_gp_registers) {\n+      __ restore_volatile_gprs(R1_SP, -nbytes_save, preserve_fp_registers);\n+    }\n+  }\n+\n+  __ bind(skip_barrier);\n+}\n+\n+void ShenandoahBarrierSetAssembler::resolve_forward_pointer_not_null(MacroAssembler *masm, Register dst, Register tmp) {\n+  __ block_comment(\"resolve_forward_pointer_not_null (shenandoahgc) {\");\n+\n+  Register tmp1 = tmp,\n+           R0_tmp2 = R0;\n+  assert_different_registers(dst, tmp1, R0_tmp2, noreg);\n+\n+  \/\/ If the object has been evacuated, the mark word layout is as follows:\n+  \/\/ | forwarding pointer (62-bit) | '11' (2-bit) |\n+\n+  \/\/ The invariant that stack\/thread pointers have the lowest two bits cleared permits retrieving\n+  \/\/ the forwarding pointer solely by inversing the lowest two bits.\n+  \/\/ This invariant follows inevitably from hotspot's minimal alignment.\n+  assert(markWord::marked_value <= (unsigned long) MinObjAlignmentInBytes,\n+         \"marked value must not be higher than hotspot's minimal alignment\");\n+\n+  Label done;\n+\n+  \/\/ Load the object's mark word.\n+  __ ld(tmp1, oopDesc::mark_offset_in_bytes(), dst);\n+\n+  \/\/ Load the bit mask for the lock bits.\n+  __ li(R0_tmp2, markWord::lock_mask_in_place);\n+\n+  \/\/ Check whether all bits matching the bit mask are set.\n+  \/\/ If that is the case, the object has been evacuated and the most significant bits form the forward pointer.\n+  __ andc_(R0_tmp2, R0_tmp2, tmp1);\n+\n+  assert(markWord::lock_mask_in_place == markWord::marked_value,\n+         \"marked value must equal the value obtained when all lock bits are being set\");\n+  if (VM_Version::has_isel()) {\n+    __ xori(tmp1, tmp1, markWord::lock_mask_in_place);\n+    __ isel(dst, CCR0, Assembler::equal, false, tmp1);\n+  } else {\n+    __ bne(CCR0, done);\n+    __ xori(dst, tmp1, markWord::lock_mask_in_place);\n+  }\n+\n+  __ bind(done);\n+  __ block_comment(\"} resolve_forward_pointer_not_null (shenandoahgc)\");\n+}\n+\n+\/\/ base:        Base register of the reference's address.\n+\/\/ ind_or_offs: Index or offset of the reference's address (load mode).\n+\/\/ dst:         Reference's address.  In case the object has been evacuated, this is the to-space version\n+\/\/              of that object.\n+void ShenandoahBarrierSetAssembler::load_reference_barrier_impl(\n+    MacroAssembler *masm, DecoratorSet decorators,\n+    Register base, RegisterOrConstant ind_or_offs,\n+    Register dst,\n+    Register tmp1, Register tmp2,\n+    MacroAssembler::PreservationLevel preservation_level) {\n+  if (ind_or_offs.is_register()) {\n+    assert_different_registers(tmp1, tmp2, base, ind_or_offs.as_register(), dst, noreg);\n+  } else {\n+    assert_different_registers(tmp1, tmp2, base, dst, noreg);\n+  }\n+\n+  Label skip_barrier;\n+\n+  bool is_strong  = ShenandoahBarrierSet::is_strong_access(decorators);\n+  bool is_weak    = ShenandoahBarrierSet::is_weak_access(decorators);\n+  bool is_phantom = ShenandoahBarrierSet::is_phantom_access(decorators);\n+  bool is_native  = ShenandoahBarrierSet::is_native_access(decorators);\n+  bool is_narrow  = UseCompressedOops && !is_native;\n+\n+  \/* ==== Check whether heap is stable ==== *\/\n+  __ lbz(tmp2, in_bytes(ShenandoahThreadLocalData::gc_state_offset()), R16_thread);\n+\n+  if (is_strong) {\n+    \/\/ For strong references, the heap is considered stable if \"has forwarded\" is not active.\n+    __ andi_(tmp1, tmp2, ShenandoahHeap::HAS_FORWARDED | ShenandoahHeap::EVACUATION);\n+    __ beq(CCR0, skip_barrier);\n+#ifdef ASSERT\n+    \/\/ \"evacuation\" -> (implies) \"has forwarded\".  If we reach this code, \"has forwarded\" must thus be set.\n+    __ andi_(tmp1, tmp1, ShenandoahHeap::HAS_FORWARDED);\n+    __ asm_assert_ne(\"'has forwarded' is missing\");\n+#endif \/\/ ASSERT\n+  } else {\n+    \/\/ For all non-strong references, the heap is considered stable if not any of \"has forwarded\",\n+    \/\/ \"root set processing\", and \"weak reference processing\" is active.\n+    \/\/ The additional phase conditions are in place to avoid the resurrection of weak references (see JDK-8266440).\n+    Label skip_fastpath;\n+    __ andi_(tmp1, tmp2, ShenandoahHeap::WEAK_ROOTS);\n+    __ bne(CCR0, skip_fastpath);\n+\n+    __ andi_(tmp1, tmp2, ShenandoahHeap::HAS_FORWARDED | ShenandoahHeap::EVACUATION);\n+    __ beq(CCR0, skip_barrier);\n+#ifdef ASSERT\n+    \/\/ \"evacuation\" -> (implies) \"has forwarded\".  If we reach this code, \"has forwarded\" must thus be set.\n+    __ andi_(tmp1, tmp1, ShenandoahHeap::HAS_FORWARDED);\n+    __ asm_assert_ne(\"'has forwarded' is missing\");\n+#endif \/\/ ASSERT\n+\n+    __ bind(skip_fastpath);\n+  }\n+\n+  \/* ==== Check whether region is in collection set ==== *\/\n+  if (is_strong) {\n+    \/\/ Shenandoah stores metadata on regions in a continuous area of memory in which a single byte corresponds to\n+    \/\/ an entire region of the shenandoah heap.  At present, only the least significant bit is of significance\n+    \/\/ and indicates whether the region is part of the collection set.\n+    \/\/\n+    \/\/ All regions are of the same size and are always aligned by a power of two.\n+    \/\/ Any address can thus be shifted by a fixed number of bits to retrieve the address prefix shared by\n+    \/\/ all objects within that region (region identification bits).\n+    \/\/\n+    \/\/  | unused bits | region identification bits | object identification bits |\n+    \/\/  (Region size depends on a couple of criteria, such as page size, user-provided arguments and the max heap size.\n+    \/\/   The number of object identification bits can thus not be determined at compile time.)\n+    \/\/\n+    \/\/ -------------------------------------------------------  <--- cs (collection set) base address\n+    \/\/ | lost space due to heap space base address                   -> 'ShenandoahHeap::in_cset_fast_test_addr()'\n+    \/\/ | (region identification bits contain heap base offset)\n+    \/\/ |------------------------------------------------------  <--- cs base address + (heap_base >> region size shift)\n+    \/\/ | collection set in the proper                                -> shift: 'region_size_bytes_shift_jint()'\n+    \/\/ |\n+    \/\/ |------------------------------------------------------  <--- cs base address + (heap_base >> region size shift)\n+    \/\/                                                                               + number of regions\n+    __ load_const_optimized(tmp2, ShenandoahHeap::in_cset_fast_test_addr(), tmp1);\n+    __ srdi(tmp1, dst, ShenandoahHeapRegion::region_size_bytes_shift_jint());\n+    __ lbzx(tmp2, tmp1, tmp2);\n+    __ andi_(tmp2, tmp2, 1);\n+    __ beq(CCR0, skip_barrier);\n+  }\n+\n+  \/* ==== Invoke runtime ==== *\/\n+  \/\/ Save to-be-preserved registers.\n+  int nbytes_save = 0;\n+\n+  const bool needs_frame           = preservation_level >= MacroAssembler::PRESERVATION_FRAME_LR;\n+  const bool preserve_gp_registers = preservation_level >= MacroAssembler::PRESERVATION_FRAME_LR_GP_REGS;\n+  const bool preserve_fp_registers = preservation_level >= MacroAssembler::PRESERVATION_FRAME_LR_GP_FP_REGS;\n+\n+  if (needs_frame) {\n+    if (preserve_gp_registers) {\n+      nbytes_save = (preserve_fp_registers\n+                     ? MacroAssembler::num_volatile_gp_regs + MacroAssembler::num_volatile_fp_regs\n+                     : MacroAssembler::num_volatile_gp_regs) * BytesPerWord;\n+      __ save_volatile_gprs(R1_SP, -nbytes_save, preserve_fp_registers);\n+    }\n+\n+    __ save_LR_CR(tmp1);\n+    __ push_frame_reg_args(nbytes_save, tmp1);\n+  }\n+\n+  \/\/ Calculate the reference's absolute address.\n+  __ add(R4_ARG2, ind_or_offs, base);\n+\n+  \/\/ Invoke runtime.\n+  address jrt_address = nullptr;\n+\n+  if (is_strong) {\n+    if (is_narrow) {\n+      jrt_address = CAST_FROM_FN_PTR(address, ShenandoahRuntime::load_reference_barrier_strong_narrow);\n+    } else {\n+      jrt_address = CAST_FROM_FN_PTR(address, ShenandoahRuntime::load_reference_barrier_strong);\n+    }\n+  } else if (is_weak) {\n+    if (is_narrow) {\n+      jrt_address = CAST_FROM_FN_PTR(address, ShenandoahRuntime::load_reference_barrier_weak_narrow);\n+    } else {\n+      jrt_address = CAST_FROM_FN_PTR(address, ShenandoahRuntime::load_reference_barrier_weak);\n+    }\n+  } else {\n+    assert(is_phantom, \"only remaining strength\");\n+    assert(!is_narrow, \"phantom access cannot be narrow\");\n+    jrt_address = CAST_FROM_FN_PTR(address, ShenandoahRuntime::load_reference_barrier_phantom);\n+  }\n+  assert(jrt_address != nullptr, \"jrt routine cannot be found\");\n+\n+  __ call_VM_leaf(jrt_address, dst \/* reference *\/, R4_ARG2 \/* reference address *\/);\n+\n+  \/\/ Restore to-be-preserved registers.\n+  if (preserve_gp_registers) {\n+    __ mr(R0, R3_RET);\n+  } else {\n+    __ mr_if_needed(dst, R3_RET);\n+  }\n+\n+  if (needs_frame) {\n+    __ pop_frame();\n+    __ restore_LR_CR(tmp1);\n+\n+    if (preserve_gp_registers) {\n+      __ restore_volatile_gprs(R1_SP, -nbytes_save, preserve_fp_registers);\n+      __ mr(dst, R0);\n+    }\n+  }\n+\n+  __ bind(skip_barrier);\n+}\n+\n+\/\/ base:           Base register of the reference's address.\n+\/\/ ind_or_offs:    Index or offset of the reference's address.\n+\/\/ L_handle_null:  An optional label that will be jumped to if the reference is null.\n+void ShenandoahBarrierSetAssembler::load_at(\n+    MacroAssembler *masm, DecoratorSet decorators, BasicType type,\n+    Register base, RegisterOrConstant ind_or_offs, Register dst,\n+    Register tmp1, Register tmp2,\n+    MacroAssembler::PreservationLevel preservation_level, Label *L_handle_null) {\n+  \/\/ Register must not clash, except 'base' and 'dst'.\n+  if (ind_or_offs.is_register()) {\n+    if (base != noreg) {\n+      assert_different_registers(tmp1, tmp2, base, ind_or_offs.register_or_noreg(), R0, noreg);\n+    }\n+    assert_different_registers(tmp1, tmp2, dst, ind_or_offs.register_or_noreg(), R0, noreg);\n+  } else {\n+    if (base == noreg) {\n+      assert_different_registers(tmp1, tmp2, base, R0, noreg);\n+    }\n+    assert_different_registers(tmp1, tmp2, dst, R0, noreg);\n+  }\n+\n+  \/* ==== Apply load barrier, if required ==== *\/\n+  if (ShenandoahBarrierSet::need_load_reference_barrier(decorators, type)) {\n+    assert(is_reference_type(type), \"need_load_reference_barrier must check whether type is a reference type\");\n+\n+    \/\/ If 'dst' clashes with either 'base' or 'ind_or_offs', use an intermediate result register\n+    \/\/ to keep the values of those alive until the load reference barrier is applied.\n+    Register intermediate_dst = (dst == base || (ind_or_offs.is_register() && dst == ind_or_offs.as_register()))\n+                                ? tmp2\n+                                : dst;\n+\n+    BarrierSetAssembler::load_at(masm, decorators, type,\n+                                 base, ind_or_offs,\n+                                 intermediate_dst,\n+                                 tmp1, noreg,\n+                                 preservation_level, L_handle_null);\n+\n+    load_reference_barrier(masm, decorators,\n+                           base, ind_or_offs,\n+                           intermediate_dst,\n+                           tmp1, R0,\n+                           preservation_level);\n+\n+    __ mr_if_needed(dst, intermediate_dst);\n+  } else {\n+    BarrierSetAssembler::load_at(masm, decorators, type,\n+                                 base, ind_or_offs,\n+                                 dst,\n+                                 tmp1, tmp2,\n+                                 preservation_level, L_handle_null);\n+  }\n+\n+  \/* ==== Apply keep-alive barrier, if required (e.g., to inhibit weak reference resurrection) ==== *\/\n+  if (ShenandoahBarrierSet::need_keep_alive_barrier(decorators, type)) {\n+    iu_barrier(masm, dst, tmp1, tmp2, preservation_level);\n+  }\n+}\n+\n+\/\/ base:        Base register of the reference's address.\n+\/\/ ind_or_offs: Index or offset of the reference's address.\n+\/\/ val:         To-be-stored value\/reference's new value.\n+void ShenandoahBarrierSetAssembler::store_at(MacroAssembler *masm, DecoratorSet decorators, BasicType type,\n+                                             Register base, RegisterOrConstant ind_or_offs, Register val,\n+                                             Register tmp1, Register tmp2, Register tmp3,\n+                                             MacroAssembler::PreservationLevel preservation_level) {\n+  if (is_reference_type(type)) {\n+    if (ShenandoahSATBBarrier) {\n+      satb_write_barrier(masm, base, ind_or_offs, tmp1, tmp2, tmp3, preservation_level);\n+    }\n+\n+    if (ShenandoahIUBarrier && val != noreg) {\n+      iu_barrier(masm, val, tmp1, tmp2, preservation_level, decorators);\n+    }\n+  }\n+\n+  BarrierSetAssembler::store_at(masm, decorators, type,\n+                                base, ind_or_offs,\n+                                val,\n+                                tmp1, tmp2, tmp3,\n+                                preservation_level);\n+}\n+\n+void ShenandoahBarrierSetAssembler::try_resolve_jobject_in_native(MacroAssembler *masm,\n+                                                                  Register dst, Register jni_env, Register obj,\n+                                                                  Register tmp, Label &slowpath) {\n+  __ block_comment(\"try_resolve_jobject_in_native (shenandoahgc) {\");\n+\n+  assert_different_registers(jni_env, obj, tmp);\n+\n+  Label done;\n+\n+  \/\/ Fast path: Reference is null (JNI tags are zero for null pointers).\n+  __ cmpdi(CCR0, obj, 0);\n+  __ beq(CCR0, done);\n+\n+  \/\/ Resolve jobject using standard implementation.\n+  BarrierSetAssembler::try_resolve_jobject_in_native(masm, dst, jni_env, obj, tmp, slowpath);\n+\n+  \/\/ Check whether heap is stable.\n+  __ lbz(tmp,\n+         in_bytes(ShenandoahThreadLocalData::gc_state_offset() - JavaThread::jni_environment_offset()),\n+         jni_env);\n+\n+  __ andi_(tmp, tmp, ShenandoahHeap::EVACUATION | ShenandoahHeap::HAS_FORWARDED);\n+  __ bne(CCR0, slowpath);\n+\n+  __ bind(done);\n+  __ block_comment(\"} try_resolve_jobject_in_native (shenandoahgc)\");\n+}\n+\n+\/\/ Special shenandoah CAS implementation that handles false negatives due\n+\/\/ to concurrent evacuation.  That is, the CAS operation is intended to succeed in\n+\/\/ the following scenarios (success criteria):\n+\/\/  s1) The reference pointer ('base_addr') equals the expected ('expected') pointer.\n+\/\/  s2) The reference pointer refers to the from-space version of an already-evacuated\n+\/\/      object, whereas the expected pointer refers to the to-space version of the same object.\n+\/\/ Situations in which the reference pointer refers to the to-space version of an object\n+\/\/ and the expected pointer refers to the from-space version of the same object can not occur due to\n+\/\/ shenandoah's strong to-space invariant.  This also implies that the reference stored in 'new_val'\n+\/\/ can not refer to the from-space version of an already-evacuated object.\n+\/\/\n+\/\/ To guarantee correct behavior in concurrent environments, two races must be addressed:\n+\/\/  r1) A concurrent thread may heal the reference pointer (i.e., it is no longer referring to the\n+\/\/      from-space version but to the to-space version of the object in question).\n+\/\/      In this case, the CAS operation should succeed.\n+\/\/  r2) A concurrent thread may mutate the reference (i.e., the reference pointer refers to an entirely different object).\n+\/\/      In this case, the CAS operation should fail.\n+\/\/\n+\/\/ By default, the value held in the 'result' register is zero to indicate failure of CAS,\n+\/\/ non-zero to indicate success.  If 'is_cae' is set, the result is the most recently fetched\n+\/\/ value from 'base_addr' rather than a boolean success indicator.\n+void ShenandoahBarrierSetAssembler::cmpxchg_oop(MacroAssembler *masm, Register base_addr,\n+                                                Register expected, Register new_val, Register tmp1, Register tmp2,\n+                                                bool is_cae, Register result) {\n+  __ block_comment(\"cmpxchg_oop (shenandoahgc) {\");\n+\n+  assert_different_registers(base_addr, new_val, tmp1, tmp2, result, R0);\n+  assert_different_registers(base_addr, expected, tmp1, tmp2, result, R0);\n+\n+  \/\/ Potential clash of 'success_flag' and 'tmp' is being accounted for.\n+  Register success_flag  = is_cae ? noreg  : result,\n+           current_value = is_cae ? result : tmp1,\n+           tmp           = is_cae ? tmp1   : result,\n+           initial_value = tmp2;\n+\n+  Label done, step_four;\n+\n+  __ bind(step_four);\n+\n+  \/* ==== Step 1 (\"Standard\" CAS) ==== *\/\n+  \/\/ Fast path: The values stored in 'expected' and 'base_addr' are equal.\n+  \/\/ Given that 'expected' must refer to the to-space object of an evacuated object (strong to-space invariant),\n+  \/\/ no special processing is required.\n+  if (UseCompressedOops) {\n+    __ cmpxchgw(CCR0, current_value, expected, new_val, base_addr, MacroAssembler::MemBarNone,\n+                false, success_flag, true);\n+  } else {\n+    __ cmpxchgd(CCR0, current_value, expected, new_val, base_addr, MacroAssembler::MemBarNone,\n+                false, success_flag, NULL, true);\n+  }\n+\n+  \/\/ Skip the rest of the barrier if the CAS operation succeeds immediately.\n+  \/\/ If it does not, the value stored at the address is either the from-space pointer of the\n+  \/\/ referenced object (success criteria s2)) or simply another object.\n+  __ beq(CCR0, done);\n+\n+  \/* ==== Step 2 (Null check) ==== *\/\n+  \/\/ The success criteria s2) cannot be matched with a null pointer\n+  \/\/ (null pointers cannot be subject to concurrent evacuation).  The failure of the CAS operation is thus legitimate.\n+  __ cmpdi(CCR0, current_value, 0);\n+  __ beq(CCR0, done);\n+\n+  \/* ==== Step 3 (reference pointer refers to from-space version; success criteria s2)) ==== *\/\n+  \/\/ To check whether the reference pointer refers to the from-space version, the forward\n+  \/\/ pointer of the object referred to by the reference is resolved and compared against the expected pointer.\n+  \/\/ If this check succeed, another CAS operation is issued with the from-space pointer being the expected pointer.\n+  \/\/\n+  \/\/ Save the potential from-space pointer.\n+  __ mr(initial_value, current_value);\n+\n+  \/\/ Resolve forward pointer.\n+  if (UseCompressedOops) { __ decode_heap_oop_not_null(current_value); }\n+  resolve_forward_pointer_not_null(masm, current_value, tmp);\n+  if (UseCompressedOops) { __ encode_heap_oop_not_null(current_value); }\n+\n+  if (!is_cae) {\n+    \/\/ 'success_flag' was overwritten by call to 'resovle_forward_pointer_not_null'.\n+    \/\/ Load zero into register for the potential failure case.\n+    __ li(success_flag, 0);\n+  }\n+  __ cmpd(CCR0, current_value, expected);\n+  __ bne(CCR0, done);\n+\n+  \/\/ Discard fetched value as it might be a reference to the from-space version of an object.\n+  if (UseCompressedOops) {\n+    __ cmpxchgw(CCR0, R0, initial_value, new_val, base_addr, MacroAssembler::MemBarNone,\n+                false, success_flag);\n+  } else {\n+    __ cmpxchgd(CCR0, R0, initial_value, new_val, base_addr, MacroAssembler::MemBarNone,\n+                false, success_flag);\n+  }\n+\n+  \/* ==== Step 4 (Retry CAS with to-space pointer (success criteria s2) under race r1)) ==== *\/\n+  \/\/ The reference pointer could have been healed whilst the previous CAS operation was being performed.\n+  \/\/ Another CAS operation must thus be issued with the to-space pointer being the expected pointer.\n+  \/\/ If that CAS operation fails as well, race r2) must have occurred, indicating that\n+  \/\/ the operation failure is legitimate.\n+  \/\/\n+  \/\/ To keep the code's size small and thus improving cache (icache) performance, this highly\n+  \/\/ unlikely case should be handled by the smallest possible code.  Instead of emitting a third,\n+  \/\/ explicit CAS operation, the code jumps back and reuses the first CAS operation (step 1)\n+  \/\/ (passed arguments are identical).\n+  \/\/\n+  \/\/ A failure of the CAS operation in step 1 would imply that the overall CAS operation is supposed\n+  \/\/ to fail.  Jumping back to step 1 requires, however, that step 2 and step 3 are re-executed as well.\n+  \/\/ It is thus important to ensure that a re-execution of those steps does not put program correctness\n+  \/\/ at risk:\n+  \/\/ - Step 2: Either terminates in failure (desired result) or falls through to step 3.\n+  \/\/ - Step 3: Terminates if the comparison between the forwarded, fetched pointer and the expected value\n+  \/\/           fails.  Unless the reference has been updated in the meanwhile once again, this is\n+  \/\/           guaranteed to be the case.\n+  \/\/           In case of a concurrent update, the CAS would be retried again. This is legitimate\n+  \/\/           in terms of program correctness (even though it is not desired).\n+  __ bne(CCR0, step_four);\n+\n+  __ bind(done);\n+  __ block_comment(\"} cmpxchg_oop (shenandoahgc)\");\n+}\n+\n+#undef __\n+\n+#ifdef COMPILER1\n+\n+#define __ ce->masm()->\n+\n+void ShenandoahBarrierSetAssembler::gen_pre_barrier_stub(LIR_Assembler *ce, ShenandoahPreBarrierStub *stub) {\n+  __ block_comment(\"gen_pre_barrier_stub (shenandoahgc) {\");\n+\n+  ShenandoahBarrierSetC1 *bs = (ShenandoahBarrierSetC1*) BarrierSet::barrier_set()->barrier_set_c1();\n+  __ bind(*stub->entry());\n+\n+  \/\/ GC status has already been verified by 'ShenandoahBarrierSetC1::pre_barrier'.\n+  \/\/ This stub is the slowpath of that function.\n+\n+  assert(stub->pre_val()->is_register(), \"pre_val must be a register\");\n+  Register pre_val = stub->pre_val()->as_register();\n+\n+  \/\/ If 'do_load()' returns false, the to-be-stored value is already available in 'stub->pre_val()'\n+  \/\/ (\"preloaded mode\" of the store barrier).\n+  if (stub->do_load()) {\n+    ce->mem2reg(stub->addr(), stub->pre_val(), T_OBJECT, stub->patch_code(), stub->info(), false);\n+  }\n+\n+  \/\/ Fast path: Reference is null.\n+  __ cmpdi(CCR0, pre_val, 0);\n+  __ bc_far_optimized(Assembler::bcondCRbiIs1_bhintNoHint, __ bi0(CCR0, Assembler::equal), *stub->continuation());\n+\n+  \/\/ Argument passing via the stack.\n+  __ std(pre_val, -8, R1_SP);\n+\n+  __ load_const_optimized(R0, bs->pre_barrier_c1_runtime_code_blob()->code_begin());\n+  __ call_stub(R0);\n+\n+  __ b(*stub->continuation());\n+  __ block_comment(\"} gen_pre_barrier_stub (shenandoahgc)\");\n+}\n+\n+void ShenandoahBarrierSetAssembler::gen_load_reference_barrier_stub(LIR_Assembler *ce,\n+                                                                    ShenandoahLoadReferenceBarrierStub *stub) {\n+  __ block_comment(\"gen_load_reference_barrier_stub (shenandoahgc) {\");\n+\n+  ShenandoahBarrierSetC1 *bs = (ShenandoahBarrierSetC1*) BarrierSet::barrier_set()->barrier_set_c1();\n+  __ bind(*stub->entry());\n+\n+  Register obj  = stub->obj()->as_register();\n+  Register res  = stub->result()->as_register();\n+  Register addr = stub->addr()->as_pointer_register();\n+  Register tmp1 = stub->tmp1()->as_register();\n+  Register tmp2 = stub->tmp2()->as_register();\n+  assert_different_registers(addr, res, tmp1, tmp2);\n+\n+#ifdef ASSERT\n+  \/\/ Ensure that 'res' is 'R3_ARG1' and contains the same value as 'obj' to reduce the number of required\n+  \/\/ copy instructions.\n+  assert(R3_RET == res, \"res must be r3\");\n+  __ cmpd(CCR0, res, obj);\n+  __ asm_assert_eq(\"result register must contain the reference stored in obj\");\n+#endif\n+\n+  DecoratorSet decorators = stub->decorators();\n+\n+  \/* ==== Check whether region is in collection set ==== *\/\n+  \/\/ GC status (unstable) has already been verified by 'ShenandoahBarrierSetC1::load_reference_barrier_impl'.\n+  \/\/ This stub is the slowpath of that function.\n+\n+  bool is_strong  = ShenandoahBarrierSet::is_strong_access(decorators);\n+  bool is_weak    = ShenandoahBarrierSet::is_weak_access(decorators);\n+  bool is_phantom = ShenandoahBarrierSet::is_phantom_access(decorators);\n+  bool is_native  = ShenandoahBarrierSet::is_native_access(decorators);\n+\n+  if (is_strong) {\n+    \/\/ Check whether object is in collection set.\n+    __ load_const_optimized(tmp2, ShenandoahHeap::in_cset_fast_test_addr(), tmp1);\n+    __ srdi(tmp1, obj, ShenandoahHeapRegion::region_size_bytes_shift_jint());\n+    __ lbzx(tmp2, tmp1, tmp2);\n+\n+    __ andi_(tmp2, tmp2, 1);\n+    __ bc_far_optimized(Assembler::bcondCRbiIs1_bhintNoHint, __ bi0(CCR0, Assembler::equal), *stub->continuation());\n+  }\n+\n+  address blob_addr = nullptr;\n+\n+  if (is_strong) {\n+    if (is_native) {\n+      blob_addr = bs->load_reference_barrier_strong_native_rt_code_blob()->code_begin();\n+    } else {\n+      blob_addr = bs->load_reference_barrier_strong_rt_code_blob()->code_begin();\n+    }\n+  } else if (is_weak) {\n+    blob_addr = bs->load_reference_barrier_weak_rt_code_blob()->code_begin();\n+  } else {\n+    assert(is_phantom, \"only remaining strength\");\n+    blob_addr = bs->load_reference_barrier_phantom_rt_code_blob()->code_begin();\n+  }\n+\n+  assert(blob_addr != nullptr, \"code blob cannot be found\");\n+\n+  \/\/ Argument passing via the stack.  'obj' is passed implicitly (as asserted above).\n+  __ std(addr, -8, R1_SP);\n+\n+  __ load_const_optimized(tmp1, blob_addr, tmp2);\n+  __ call_stub(tmp1);\n+\n+  \/\/ 'res' is 'R3_RET'.  The result is thus already in the correct register.\n+\n+  __ b(*stub->continuation());\n+  __ block_comment(\"} gen_load_reference_barrier_stub (shenandoahgc)\");\n+}\n+\n+#undef __\n+\n+#define __ sasm->\n+\n+void ShenandoahBarrierSetAssembler::generate_c1_pre_barrier_runtime_stub(StubAssembler *sasm) {\n+  __ block_comment(\"generate_c1_pre_barrier_runtime_stub (shenandoahgc) {\");\n+\n+  Label runtime, skip_barrier;\n+  BarrierSet *bs = BarrierSet::barrier_set();\n+\n+  \/\/ Argument passing via the stack.\n+  const int caller_stack_slots = 3;\n+\n+  Register R0_pre_val = R0;\n+  __ ld(R0, -8, R1_SP);\n+  Register R11_tmp1 = R11_scratch1;\n+  __ std(R11_tmp1, -16, R1_SP);\n+  Register R12_tmp2 = R12_scratch2;\n+  __ std(R12_tmp2, -24, R1_SP);\n+\n+  \/* ==== Check whether marking is active ==== *\/\n+  \/\/ Even though gc status was checked in 'ShenandoahBarrierSetAssembler::gen_pre_barrier_stub',\n+  \/\/ another check is required as a safepoint might have been reached in the meantime (JDK-8140588).\n+  __ lbz(R12_tmp2, in_bytes(ShenandoahThreadLocalData::gc_state_offset()), R16_thread);\n+\n+  __ andi_(R12_tmp2, R12_tmp2, ShenandoahHeap::MARKING);\n+  __ beq(CCR0, skip_barrier);\n+\n+  \/* ==== Add previous value directly to thread-local SATB mark queue ==== *\/\n+  \/\/ Check queue's capacity.  Jump to runtime if no free slot is available.\n+  __ ld(R12_tmp2, in_bytes(ShenandoahThreadLocalData::satb_mark_queue_index_offset()), R16_thread);\n+  __ cmpdi(CCR0, R12_tmp2, 0);\n+  __ beq(CCR0, runtime);\n+\n+  \/\/ Capacity suffices.  Decrement the queue's size by one slot (size of one oop).\n+  __ addi(R12_tmp2, R12_tmp2, -wordSize);\n+  __ std(R12_tmp2, in_bytes(ShenandoahThreadLocalData::satb_mark_queue_index_offset()), R16_thread);\n+\n+  \/\/ Enqueue the previous value and skip the runtime invocation.\n+  __ ld(R11_tmp1, in_bytes(ShenandoahThreadLocalData::satb_mark_queue_buffer_offset()), R16_thread);\n+  __ stdx(R0_pre_val, R11_tmp1, R12_tmp2);\n+  __ b(skip_barrier);\n+\n+  __ bind(runtime);\n+\n+  \/* ==== Invoke runtime to commit SATB mark queue to gc and allocate a new buffer ==== *\/\n+  \/\/ Save to-be-preserved registers.\n+  const int nbytes_save = (MacroAssembler::num_volatile_regs + caller_stack_slots) * BytesPerWord;\n+  __ save_volatile_gprs(R1_SP, -nbytes_save);\n+  __ save_LR_CR(R11_tmp1);\n+  __ push_frame_reg_args(nbytes_save, R11_tmp1);\n+\n+  \/\/ Invoke runtime.\n+  __ call_VM_leaf(CAST_FROM_FN_PTR(address, ShenandoahRuntime::write_ref_field_pre_entry), R0_pre_val, R16_thread);\n+\n+  \/\/ Restore to-be-preserved registers.\n+  __ pop_frame();\n+  __ restore_LR_CR(R11_tmp1);\n+  __ restore_volatile_gprs(R1_SP, -nbytes_save);\n+\n+  __ bind(skip_barrier);\n+\n+  \/\/ Restore spilled registers.\n+  __ ld(R11_tmp1, -16, R1_SP);\n+  __ ld(R12_tmp2, -24, R1_SP);\n+\n+  __ blr();\n+  __ block_comment(\"} generate_c1_pre_barrier_runtime_stub (shenandoahgc)\");\n+}\n+\n+void ShenandoahBarrierSetAssembler::generate_c1_load_reference_barrier_runtime_stub(StubAssembler *sasm,\n+                                                                                    DecoratorSet decorators) {\n+  __ block_comment(\"generate_c1_load_reference_barrier_runtime_stub (shenandoahgc) {\");\n+\n+  \/\/ Argument passing via the stack.\n+  const int caller_stack_slots = 1;\n+\n+  \/\/ Save to-be-preserved registers.\n+  const int nbytes_save = (MacroAssembler::num_volatile_regs - 1 \/\/ 'R3_ARG1' is skipped\n+                           + caller_stack_slots) * BytesPerWord;\n+  __ save_volatile_gprs(R1_SP, -nbytes_save, true, false);\n+\n+  \/\/ Load arguments from stack.\n+  \/\/ No load required, as assured by assertions in 'ShenandoahBarrierSetAssembler::gen_load_reference_barrier_stub'.\n+  Register R3_obj = R3_ARG1;\n+  Register R4_load_addr = R4_ARG2;\n+  __ ld(R4_load_addr, -8, R1_SP);\n+\n+  Register R11_tmp = R11_scratch1;\n+\n+  \/* ==== Invoke runtime ==== *\/\n+  bool is_strong  = ShenandoahBarrierSet::is_strong_access(decorators);\n+  bool is_weak    = ShenandoahBarrierSet::is_weak_access(decorators);\n+  bool is_phantom = ShenandoahBarrierSet::is_phantom_access(decorators);\n+  bool is_native  = ShenandoahBarrierSet::is_native_access(decorators);\n+\n+  address jrt_address = NULL;\n+\n+  if (is_strong) {\n+    if (is_native) {\n+      jrt_address = CAST_FROM_FN_PTR(address, ShenandoahRuntime::load_reference_barrier_strong);\n+    } else {\n+      if (UseCompressedOops) {\n+        jrt_address = CAST_FROM_FN_PTR(address, ShenandoahRuntime::load_reference_barrier_strong_narrow);\n+      } else {\n+        jrt_address = CAST_FROM_FN_PTR(address, ShenandoahRuntime::load_reference_barrier_strong);\n+      }\n+    }\n+  } else if (is_weak) {\n+    assert(!is_native, \"weak load reference barrier must not be called off-heap\");\n+    if (UseCompressedOops) {\n+      jrt_address = CAST_FROM_FN_PTR(address, ShenandoahRuntime::load_reference_barrier_weak_narrow);\n+    } else {\n+      jrt_address = CAST_FROM_FN_PTR(address, ShenandoahRuntime::load_reference_barrier_weak);\n+    }\n+  } else {\n+    assert(is_phantom, \"reference type must be phantom\");\n+    assert(is_native, \"phantom load reference barrier must be called off-heap\");\n+    jrt_address = CAST_FROM_FN_PTR(address, ShenandoahRuntime::load_reference_barrier_phantom);\n+  }\n+  assert(jrt_address != NULL, \"load reference barrier runtime routine cannot be found\");\n+\n+  __ save_LR_CR(R11_tmp);\n+  __ push_frame_reg_args(nbytes_save, R11_tmp);\n+\n+  \/\/ Invoke runtime.  Arguments are already stored in the corresponding registers.\n+  __ call_VM_leaf(jrt_address, R3_obj, R4_load_addr);\n+\n+  \/\/ Restore to-be-preserved registers.\n+  __ pop_frame();\n+  __ restore_LR_CR(R11_tmp);\n+  __ restore_volatile_gprs(R1_SP, -nbytes_save, true, false); \/\/ Skip 'R3_RET' register.\n+\n+  __ blr();\n+  __ block_comment(\"} generate_c1_load_reference_barrier_runtime_stub (shenandoahgc)\");\n+}\n+\n+#undef __\n+\n+#endif \/\/ COMPILER1\n","filename":"src\/hotspot\/cpu\/ppc\/gc\/shenandoah\/shenandoahBarrierSetAssembler_ppc.cpp","additions":1012,"deletions":0,"binary":false,"changes":1012,"status":"added"},{"patch":"@@ -0,0 +1,118 @@\n+\/*\n+ * Copyright (c) 2018, 2021, Red Hat, Inc. All rights reserved.\n+ * Copyright (c) 2012, 2021 SAP SE. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef CPU_PPC_GC_SHENANDOAH_SHENANDOAHBARRIERSETASSEMBLER_PPC_HPP\n+#define CPU_PPC_GC_SHENANDOAH_SHENANDOAHBARRIERSETASSEMBLER_PPC_HPP\n+\n+#include \"asm\/macroAssembler.hpp\"\n+#include \"gc\/shared\/barrierSetAssembler.hpp\"\n+#include \"gc\/shenandoah\/shenandoahBarrierSet.hpp\"\n+\n+#ifdef COMPILER1\n+\n+class LIR_Assembler;\n+class ShenandoahPreBarrierStub;\n+class ShenandoahLoadReferenceBarrierStub;\n+class StubAssembler;\n+\n+#endif\n+\n+class StubCodeGenerator;\n+\n+class ShenandoahBarrierSetAssembler: public BarrierSetAssembler {\n+private:\n+\n+  \/* ==== Actual barrier implementations ==== *\/\n+  void satb_write_barrier_impl(MacroAssembler* masm, DecoratorSet decorators,\n+                               Register base, RegisterOrConstant ind_or_offs,\n+                               Register pre_val,\n+                               Register tmp1, Register tmp2,\n+                               MacroAssembler::PreservationLevel preservation_level);\n+\n+  void load_reference_barrier_impl(MacroAssembler* masm, DecoratorSet decorators,\n+                                   Register base, RegisterOrConstant ind_or_offs,\n+                                   Register dst,\n+                                   Register tmp1, Register tmp2,\n+                                   MacroAssembler::PreservationLevel preservation_level);\n+\n+  \/* ==== Helper methods for barrier implementations ==== *\/\n+  void resolve_forward_pointer_not_null(MacroAssembler* masm, Register dst, Register tmp);\n+\n+public:\n+\n+  \/* ==== C1 stubs ==== *\/\n+#ifdef COMPILER1\n+\n+  void gen_pre_barrier_stub(LIR_Assembler* ce, ShenandoahPreBarrierStub* stub);\n+\n+  void gen_load_reference_barrier_stub(LIR_Assembler* ce, ShenandoahLoadReferenceBarrierStub* stub);\n+\n+  void generate_c1_pre_barrier_runtime_stub(StubAssembler* sasm);\n+\n+  void generate_c1_load_reference_barrier_runtime_stub(StubAssembler* sasm, DecoratorSet decorators);\n+\n+#endif\n+\n+  \/* ==== Available barriers (facades of the actual implementations) ==== *\/\n+  void satb_write_barrier(MacroAssembler* masm,\n+                          Register base, RegisterOrConstant ind_or_offs,\n+                          Register tmp1, Register tmp2, Register tmp3,\n+                          MacroAssembler::PreservationLevel preservation_level);\n+\n+  void iu_barrier(MacroAssembler* masm,\n+                        Register val,\n+                        Register tmp1, Register tmp2,\n+                        MacroAssembler::PreservationLevel preservation_level, DecoratorSet decorators = 0);\n+\n+  void load_reference_barrier(MacroAssembler* masm, DecoratorSet decorators,\n+                              Register base, RegisterOrConstant ind_or_offs,\n+                              Register dst,\n+                              Register tmp1, Register tmp2,\n+                              MacroAssembler::PreservationLevel preservation_level);\n+\n+  \/* ==== Helper methods used by C1 and C2 ==== *\/\n+  void cmpxchg_oop(MacroAssembler* masm, Register base_addr, Register expected, Register new_val,\n+                   Register tmp1, Register tmp2,\n+                   bool is_cae, Register result);\n+\n+  \/* ==== Access api ==== *\/\n+  virtual void arraycopy_prologue(MacroAssembler* masm, DecoratorSet decorators, BasicType type,\n+                          Register src, Register dst, Register count, Register preserve1, Register preserve2);\n+\n+  virtual void store_at(MacroAssembler* masm, DecoratorSet decorators, BasicType type,\n+                        Register base, RegisterOrConstant ind_or_offs, Register val,\n+                        Register tmp1, Register tmp2, Register tmp3,\n+                        MacroAssembler::PreservationLevel preservation_level);\n+\n+  virtual void load_at(MacroAssembler* masm, DecoratorSet decorators, BasicType type,\n+                       Register base, RegisterOrConstant ind_or_offs, Register dst,\n+                       Register tmp1, Register tmp2,\n+                       MacroAssembler::PreservationLevel preservation_level, Label* L_handle_null = NULL);\n+\n+  virtual void try_resolve_jobject_in_native(MacroAssembler* masm, Register dst, Register jni_env,\n+                                             Register obj, Register tmp, Label& slowpath);\n+};\n+\n+#endif \/\/ CPU_PPC_GC_SHENANDOAH_SHENANDOAHBARRIERSETASSEMBLER_PPC_HPP\n","filename":"src\/hotspot\/cpu\/ppc\/gc\/shenandoah\/shenandoahBarrierSetAssembler_ppc.hpp","additions":118,"deletions":0,"binary":false,"changes":118,"status":"added"},{"patch":"@@ -0,0 +1,217 @@\n+\/\/\n+\/\/ Copyright (c) 2018, 2021, Red Hat, Inc. All rights reserved.\n+\/\/ Copyright (c) 2012, 2021 SAP SE. All rights reserved.\n+\/\/ DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+\/\/\n+\/\/ This code is free software; you can redistribute it and\/or modify it\n+\/\/ under the terms of the GNU General Public License version 2 only, as\n+\/\/ published by the Free Software Foundation.\n+\/\/\n+\/\/ This code is distributed in the hope that it will be useful, but WITHOUT\n+\/\/ ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+\/\/ FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+\/\/ version 2 for more details (a copy is included in the LICENSE file that\n+\/\/ accompanied this code).\n+\/\/\n+\/\/ You should have received a copy of the GNU General Public License version\n+\/\/ 2 along with this work; if not, write to the Free Software Foundation,\n+\/\/ Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+\/\/\n+\/\/ Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+\/\/ or visit www.oracle.com if you need additional information or have any\n+\/\/ questions.\n+\/\/\n+\/\/\n+\n+source_hpp %{\n+#include \"gc\/shenandoah\/shenandoahBarrierSet.hpp\"\n+#include \"gc\/shenandoah\/shenandoahBarrierSetAssembler.hpp\"\n+%}\n+\n+\/\/ Weak compareAndSwap operations are treated as strong compareAndSwap operations.\n+\/\/ This is motivated by the retry logic of ShenandoahBarrierSetAssembler::cmpxchg_oop which is hard to realise\n+\/\/ using weak CAS operations.\n+\n+instruct compareAndSwapP_shenandoah(iRegIdst res, indirect mem, iRegPsrc oldval, iRegPsrc newval,\n+                                    iRegPdst tmp1, iRegPdst tmp2, flagsRegCR0 cr) %{\n+  match(Set res (ShenandoahCompareAndSwapP mem (Binary oldval newval)));\n+  match(Set res (ShenandoahWeakCompareAndSwapP mem (Binary oldval newval)));\n+  effect(TEMP_DEF res, TEMP tmp1, TEMP tmp2, KILL cr);\n+\n+  predicate(((CompareAndSwapNode*)n)->order() != MemNode::acquire\n+            && ((CompareAndSwapNode*)n)->order() != MemNode::seqcst);\n+\n+  format %{ \"CMPXCHG $res, $mem, $oldval, $newval; as bool; ptr\" %}\n+  ins_encode %{\n+    ShenandoahBarrierSet::assembler()->cmpxchg_oop(\n+        &_masm,\n+        $mem$$Register, $oldval$$Register, $newval$$Register,\n+        $tmp1$$Register, $tmp2$$Register,\n+        false, $res$$Register\n+    );\n+  %}\n+  ins_pipe(pipe_class_default);\n+%}\n+\n+instruct compareAndSwapN_shenandoah(iRegIdst res, indirect mem, iRegNsrc oldval, iRegNsrc newval,\n+                                    iRegNdst tmp1, iRegNdst tmp2, flagsRegCR0 cr) %{\n+  match(Set res (ShenandoahCompareAndSwapN mem (Binary oldval newval)));\n+  match(Set res (ShenandoahWeakCompareAndSwapN mem (Binary oldval newval)));\n+  effect(TEMP_DEF res, TEMP tmp1, TEMP tmp2, KILL cr);\n+\n+  predicate(((CompareAndSwapNode*)n)->order() != MemNode::acquire\n+            && ((CompareAndSwapNode*)n)->order() != MemNode::seqcst);\n+\n+  format %{ \"CMPXCHG $res, $mem, $oldval, $newval; as bool; ptr\" %}\n+  ins_encode %{\n+    ShenandoahBarrierSet::assembler()->cmpxchg_oop(\n+        &_masm,\n+        $mem$$Register, $oldval$$Register, $newval$$Register,\n+        $tmp1$$Register, $tmp2$$Register,\n+        false, $res$$Register\n+    );\n+  %}\n+  ins_pipe(pipe_class_default);\n+%}\n+\n+instruct compareAndSwapP_acq_shenandoah(iRegIdst res, indirect mem, iRegPsrc oldval, iRegPsrc newval,\n+                                       iRegPdst tmp1, iRegPdst tmp2, flagsRegCR0 cr) %{\n+  match(Set res (ShenandoahCompareAndSwapP mem (Binary oldval newval)));\n+  match(Set res (ShenandoahWeakCompareAndSwapP mem (Binary oldval newval)));\n+  effect(TEMP_DEF res, TEMP tmp1, TEMP tmp2, KILL cr);\n+\n+  predicate(((CompareAndSwapNode*)n)->order() == MemNode::acquire\n+            || ((CompareAndSwapNode*)n)->order() == MemNode::seqcst);\n+\n+  format %{ \"CMPXCHGD acq $res, $mem, $oldval, $newval; as bool; ptr\" %}\n+  ins_encode %{\n+    ShenandoahBarrierSet::assembler()->cmpxchg_oop(\n+        &_masm,\n+        $mem$$Register, $oldval$$Register, $newval$$Register,\n+        $tmp1$$Register, $tmp2$$Register,\n+        false, $res$$Register\n+    );\n+    if (support_IRIW_for_not_multiple_copy_atomic_cpu) {\n+      __ isync();\n+    } else {\n+      __ sync();\n+    }\n+  %}\n+  ins_pipe(pipe_class_default);\n+%}\n+\n+instruct compareAndSwapN_acq_shenandoah(iRegIdst res, indirect mem, iRegNsrc oldval, iRegNsrc newval,\n+                                        iRegNdst tmp1, iRegNdst tmp2, flagsRegCR0 cr) %{\n+  match(Set res (ShenandoahCompareAndSwapN mem (Binary oldval newval)));\n+  match(Set res (ShenandoahWeakCompareAndSwapN mem (Binary oldval newval)));\n+  effect(TEMP_DEF res, TEMP tmp1, TEMP tmp2, KILL cr);\n+\n+  predicate(((CompareAndSwapNode*)n)->order() == MemNode::acquire\n+            || ((CompareAndSwapNode*)n)->order() == MemNode::seqcst);\n+\n+  format %{ \"CMPXCHGD acq $res, $mem, $oldval, $newval; as bool; ptr\" %}\n+  ins_encode %{\n+    ShenandoahBarrierSet::assembler()->cmpxchg_oop(\n+        &_masm,\n+        $mem$$Register, $oldval$$Register, $newval$$Register,\n+        $tmp1$$Register, $tmp2$$Register,\n+        false, $res$$Register\n+    );\n+    if (support_IRIW_for_not_multiple_copy_atomic_cpu) {\n+      __ isync();\n+    } else {\n+      __ sync();\n+    }\n+  %}\n+  ins_pipe(pipe_class_default);\n+%}\n+\n+instruct compareAndExchangeP_shenandoah(iRegPdst res, indirect mem, iRegPsrc oldval, iRegPsrc newval,\n+                                        iRegPdst tmp1, iRegPdst tmp2, flagsRegCR0 cr) %{\n+  match(Set res (ShenandoahCompareAndExchangeP mem (Binary oldval newval)));\n+  effect(TEMP_DEF res, TEMP tmp1, TEMP tmp2, KILL cr);\n+\n+  predicate(((CompareAndSwapNode*)n)->order() != MemNode::acquire\n+            && ((CompareAndSwapNode*)n)->order() != MemNode::seqcst);\n+\n+  format %{ \"CMPXCHGD $res, $mem, $oldval, $newval; as ptr; ptr\" %}\n+  ins_encode %{\n+    ShenandoahBarrierSet::assembler()->cmpxchg_oop(\n+        &_masm,\n+        $mem$$Register, $oldval$$Register, $newval$$Register,\n+        $tmp1$$Register, $tmp2$$Register,\n+        true, $res$$Register\n+    );\n+  %}\n+  ins_pipe(pipe_class_default);\n+%}\n+\n+instruct compareAndExchangeN_shenandoah(iRegNdst res, indirect mem, iRegNsrc oldval, iRegNsrc newval,\n+                                        iRegNdst tmp1, iRegNdst tmp2, flagsRegCR0 cr) %{\n+  match(Set res (ShenandoahCompareAndExchangeN mem (Binary oldval newval)));\n+  effect(TEMP_DEF res, TEMP tmp1, TEMP tmp2, KILL cr);\n+\n+  predicate(((CompareAndSwapNode*)n)->order() != MemNode::acquire\n+            && ((CompareAndSwapNode*)n)->order() != MemNode::seqcst);\n+\n+  format %{ \"CMPXCHGD $res, $mem, $oldval, $newval; as ptr; ptr\" %}\n+  ins_encode %{\n+    ShenandoahBarrierSet::assembler()->cmpxchg_oop(\n+        &_masm,\n+        $mem$$Register, $oldval$$Register, $newval$$Register,\n+        $tmp1$$Register, $tmp2$$Register,\n+        true, $res$$Register\n+    );\n+  %}\n+  ins_pipe(pipe_class_default);\n+%}\n+\n+instruct compareAndExchangePAcq_shenandoah(iRegPdst res, indirect mem, iRegPsrc oldval, iRegPsrc newval,\n+                                           iRegPdst tmp1, iRegPdst tmp2, flagsRegCR0 cr) %{\n+  match(Set res (ShenandoahCompareAndExchangeP mem (Binary oldval newval)));\n+  effect(TEMP_DEF res, TEMP tmp1, TEMP tmp2, KILL cr);\n+\n+  predicate(((CompareAndSwapNode*)n)->order() == MemNode::acquire\n+            || ((CompareAndSwapNode*)n)->order() == MemNode::seqcst);\n+\n+  format %{ \"CMPXCHGD acq $res, $mem, $oldval, $newval; as ptr; ptr\" %}\n+  ins_encode %{\n+    ShenandoahBarrierSet::assembler()->cmpxchg_oop(\n+        &_masm,\n+        $mem$$Register, $oldval$$Register, $newval$$Register,\n+        $tmp1$$Register, $tmp2$$Register,\n+        true, $res$$Register\n+    );\n+    if (support_IRIW_for_not_multiple_copy_atomic_cpu) {\n+      __ isync();\n+    } else {\n+      __ sync();\n+    }\n+  %}\n+  ins_pipe(pipe_class_default);\n+%}\n+\n+instruct compareAndExchangeNAcq_shenandoah(iRegNdst res, indirect mem, iRegNsrc oldval, iRegNsrc newval,\n+                                           iRegNdst tmp1, iRegNdst tmp2, flagsRegCR0 cr) %{\n+  match(Set res (ShenandoahCompareAndExchangeN mem (Binary oldval newval)));\n+  effect(TEMP_DEF res, TEMP tmp1, TEMP tmp2, KILL cr);\n+\n+  predicate(((CompareAndSwapNode*)n)->order() == MemNode::acquire\n+            || ((CompareAndSwapNode*)n)->order() == MemNode::seqcst);\n+\n+  format %{ \"CMPXCHGD acq $res, $mem, $oldval, $newval; as ptr; ptr\" %}\n+  ins_encode %{\n+    ShenandoahBarrierSet::assembler()->cmpxchg_oop(\n+        &_masm,\n+        $mem$$Register, $oldval$$Register, $newval$$Register,\n+        $tmp1$$Register, $tmp2$$Register,\n+        true, $res$$Register\n+    );\n+    if (support_IRIW_for_not_multiple_copy_atomic_cpu) {\n+      __ isync();\n+    } else {\n+      __ sync();\n+    }\n+  %}\n+  ins_pipe(pipe_class_default);\n+%}\n","filename":"src\/hotspot\/cpu\/ppc\/gc\/shenandoah\/shenandoah_ppc.ad","additions":217,"deletions":0,"binary":false,"changes":217,"status":"added"},{"patch":"@@ -36,2 +36,1 @@\n-class LIR_OprDesc;\n-typedef LIR_OprDesc* LIR_Opr;\n+class LIR_Opr;\n","filename":"src\/hotspot\/cpu\/ppc\/gc\/z\/zBarrierSetAssembler_ppc.hpp","additions":1,"deletions":2,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -2180,0 +2180,4 @@\n+const bool Matcher::match_rule_supported_vector_masked(int opcode, int vlen, BasicType bt) {\n+  return false;\n+}\n+\n","filename":"src\/hotspot\/cpu\/ppc\/ppc.ad","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -1536,51 +1536,0 @@\n-static void move_ptr(MacroAssembler* masm, VMRegPair src, VMRegPair dst, Register r_caller_sp, Register r_temp) {\n-  if (src.first()->is_stack()) {\n-    if (dst.first()->is_stack()) {\n-      \/\/ stack to stack\n-      __ ld(r_temp, reg2offset(src.first()), r_caller_sp);\n-      __ std(r_temp, reg2offset(dst.first()), R1_SP);\n-    } else {\n-      \/\/ stack to reg\n-      __ ld(dst.first()->as_Register(), reg2offset(src.first()), r_caller_sp);\n-    }\n-  } else if (dst.first()->is_stack()) {\n-    \/\/ reg to stack\n-    __ std(src.first()->as_Register(), reg2offset(dst.first()), R1_SP);\n-  } else {\n-    if (dst.first() != src.first()) {\n-      __ mr(dst.first()->as_Register(), src.first()->as_Register());\n-    }\n-  }\n-}\n-\n-\/\/ Unpack an array argument into a pointer to the body and the length\n-\/\/ if the array is non-null, otherwise pass 0 for both.\n-static void unpack_array_argument(MacroAssembler* masm, VMRegPair reg, BasicType in_elem_type,\n-                                  VMRegPair body_arg, VMRegPair length_arg, Register r_caller_sp,\n-                                  Register tmp_reg, Register tmp2_reg) {\n-  assert(!body_arg.first()->is_Register() || body_arg.first()->as_Register() != tmp_reg,\n-         \"possible collision\");\n-  assert(!length_arg.first()->is_Register() || length_arg.first()->as_Register() != tmp_reg,\n-         \"possible collision\");\n-\n-  \/\/ Pass the length, ptr pair.\n-  Label set_out_args;\n-  VMRegPair tmp, tmp2;\n-  tmp.set_ptr(tmp_reg->as_VMReg());\n-  tmp2.set_ptr(tmp2_reg->as_VMReg());\n-  if (reg.first()->is_stack()) {\n-    \/\/ Load the arg up from the stack.\n-    move_ptr(masm, reg, tmp, r_caller_sp, \/*unused*\/ R0);\n-    reg = tmp;\n-  }\n-  __ li(tmp2_reg, 0); \/\/ Pass zeros if Array=null.\n-  if (tmp_reg != reg.first()->as_Register()) __ li(tmp_reg, 0);\n-  __ cmpdi(CCR0, reg.first()->as_Register(), 0);\n-  __ beq(CCR0, set_out_args);\n-  __ lwa(tmp2_reg, arrayOopDesc::length_offset_in_bytes(), reg.first()->as_Register());\n-  __ addi(tmp_reg, reg.first()->as_Register(), arrayOopDesc::base_offset_in_bytes(in_elem_type));\n-  __ bind(set_out_args);\n-  move_ptr(masm, tmp, body_arg, r_caller_sp, \/*unused*\/ R0);\n-  move_ptr(masm, tmp2, length_arg, r_caller_sp, \/*unused*\/ R0); \/\/ Same as move32_64 on PPC64.\n-}\n-\n@@ -1688,2 +1637,1 @@\n-                                                BasicType ret_type,\n-                                                address critical_entry) {\n+                                                BasicType ret_type) {\n@@ -1712,6 +1660,1 @@\n-  bool is_critical_native = true;\n-  address native_func = critical_entry;\n-  if (native_func == NULL) {\n-    native_func = method->native_function();\n-    is_critical_native = false;\n-  }\n+  address native_func = method->native_function();\n@@ -1736,13 +1679,1 @@\n-  int  total_c_args     = total_in_args;\n-\n-  if (!is_critical_native) {\n-    int n_hidden_args = method_is_static ? 2 : 1;\n-    total_c_args += n_hidden_args;\n-  } else {\n-    \/\/ No JNIEnv*, no this*, but unpacked arrays (base+length).\n-    for (int i = 0; i < total_in_args; i++) {\n-      if (in_sig_bt[i] == T_ARRAY) {\n-        total_c_args++;\n-      }\n-    }\n-  }\n+  int  total_c_args     = total_in_args + (method_is_static ? 2 : 1);\n@@ -1762,28 +1693,4 @@\n-  if (!is_critical_native) {\n-    out_sig_bt[argc++] = T_ADDRESS;\n-    if (method->is_static()) {\n-      out_sig_bt[argc++] = T_OBJECT;\n-    }\n-\n-    for (int i = 0; i < total_in_args ; i++ ) {\n-      out_sig_bt[argc++] = in_sig_bt[i];\n-    }\n-  } else {\n-    in_elem_bt = NEW_RESOURCE_ARRAY(BasicType, total_c_args);\n-    SignatureStream ss(method->signature());\n-    int o = 0;\n-    for (int i = 0; i < total_in_args ; i++, o++) {\n-      if (in_sig_bt[i] == T_ARRAY) {\n-        \/\/ Arrays are passed as int, elem* pair\n-        ss.skip_array_prefix(1);  \/\/ skip one '['\n-        assert(ss.is_primitive(), \"primitive type expected\");\n-        in_elem_bt[o] = ss.type();\n-      } else {\n-        in_elem_bt[o] = T_VOID;\n-      }\n-      if (in_sig_bt[i] != T_VOID) {\n-        assert(in_sig_bt[i] == ss.type() ||\n-               in_sig_bt[i] == T_ARRAY, \"must match\");\n-        ss.next();\n-      }\n-    }\n+  out_sig_bt[argc++] = T_ADDRESS;\n+  if (method->is_static()) {\n+    out_sig_bt[argc++] = T_OBJECT;\n+  }\n@@ -1791,9 +1698,2 @@\n-    for (int i = 0; i < total_in_args ; i++ ) {\n-      if (in_sig_bt[i] == T_ARRAY) {\n-        \/\/ Arrays are passed as int, elem* pair.\n-        out_sig_bt[argc++] = T_INT;\n-        out_sig_bt[argc++] = T_ADDRESS;\n-      } else {\n-        out_sig_bt[argc++] = in_sig_bt[i];\n-      }\n-    }\n+  for (int i = 0; i < total_in_args ; i++ ) {\n+    out_sig_bt[argc++] = in_sig_bt[i];\n@@ -1826,1 +1726,1 @@\n-  \/\/        [oopHandle area]           <-- 3) R1_SP + oop_handle_offset (save area for critical natives)\n+  \/\/        [oopHandle area]           <-- 3) R1_SP + oop_handle_offset\n@@ -1841,29 +1741,0 @@\n-  if (is_critical_native) {\n-    \/\/ Critical natives may have to call out so they need a save area\n-    \/\/ for register arguments.\n-    int double_slots = 0;\n-    int single_slots = 0;\n-    for (int i = 0; i < total_in_args; i++) {\n-      if (in_regs[i].first()->is_Register()) {\n-        const Register reg = in_regs[i].first()->as_Register();\n-        switch (in_sig_bt[i]) {\n-          case T_BOOLEAN:\n-          case T_BYTE:\n-          case T_SHORT:\n-          case T_CHAR:\n-          case T_INT:\n-          \/\/ Fall through.\n-          case T_ARRAY:\n-          case T_LONG: double_slots++; break;\n-          default:  ShouldNotReachHere();\n-        }\n-      } else if (in_regs[i].first()->is_FloatRegister()) {\n-        switch (in_sig_bt[i]) {\n-          case T_FLOAT:  single_slots++; break;\n-          case T_DOUBLE: double_slots++; break;\n-          default:  ShouldNotReachHere();\n-        }\n-      }\n-    }\n-    total_save_slots = double_slots * 2 + align_up(single_slots, 2); \/\/ round to even\n-  }\n@@ -1876,1 +1747,1 @@\n-  if (method_is_static && !is_critical_native) {                                  \/\/ 4)\n+  if (method_is_static) {                                                         \/\/ 4)\n@@ -1922,4 +1793,2 @@\n-  if (!is_critical_native) {\n-    r_carg1_jnienv        = out_regs[0].first()->as_Register();\n-    r_carg2_classorobject = out_regs[1].first()->as_Register();\n-  }\n+  r_carg1_jnienv        = out_regs[0].first()->as_Register();\n+  r_carg2_classorobject = out_regs[1].first()->as_Register();\n@@ -2061,7 +1930,0 @@\n-        if (is_critical_native) {\n-          int body_arg = out;\n-          out -= 1; \/\/ Point to length arg.\n-          unpack_array_argument(masm, in_regs[in], in_elem_bt[in], out_regs[body_arg], out_regs[out],\n-                                r_callers_sp, r_temp_1, r_temp_2);\n-          break;\n-        }\n@@ -2069,1 +1931,0 @@\n-        assert(!is_critical_native, \"no oop arguments\");\n@@ -2101,1 +1962,1 @@\n-  if (method_is_static && !is_critical_native) {\n+  if (method_is_static) {\n@@ -2112,3 +1973,1 @@\n-  if (!is_critical_native) {\n-    __ addi(r_carg1_jnienv, R16_thread, in_bytes(JavaThread::jni_environment_offset()));\n-  }\n+  __ addi(r_carg1_jnienv, R16_thread, in_bytes(JavaThread::jni_environment_offset()));\n@@ -2143,1 +2002,0 @@\n-    assert(!is_critical_native, \"unhandled\");\n@@ -2188,3 +2046,2 @@\n-  if (!is_critical_native) {\n-    \/\/ Publish thread state\n-    \/\/ --------------------------------------------------------------------------\n+  \/\/ Publish thread state\n+  \/\/ --------------------------------------------------------------------------\n@@ -2192,6 +2049,5 @@\n-    \/\/ Transition from _thread_in_Java to _thread_in_native.\n-    __ li(R0, _thread_in_native);\n-    __ release();\n-    \/\/ TODO: PPC port assert(4 == JavaThread::sz_thread_state(), \"unexpected field size\");\n-    __ stw(R0, thread_(thread_state));\n-  }\n+  \/\/ Transition from _thread_in_Java to _thread_in_native.\n+  __ li(R0, _thread_in_native);\n+  __ release();\n+  \/\/ TODO: PPC port assert(4 == JavaThread::sz_thread_state(), \"unexpected field size\");\n+  __ stw(R0, thread_(thread_state));\n@@ -2259,18 +2115,0 @@\n-  \/\/ If this is a critical native, check for a safepoint or suspend request after the call.\n-  \/\/ If a safepoint is needed, transition to native, then to native_trans to handle\n-  \/\/ safepoints like the native methods that are not critical natives.\n-  if (is_critical_native) {\n-    Label needs_safepoint;\n-    Register sync_state      = r_temp_5;\n-    \/\/ Note: We should not reach here with active stack watermark. There's no safepoint between\n-    \/\/       start of the native wrapper and this check where it could have been added.\n-    \/\/       We don't check the watermark in the fast path.\n-    __ safepoint_poll(needs_safepoint, sync_state, false \/* at_return *\/, false \/* in_nmethod *\/);\n-\n-    Register suspend_flags   = r_temp_6;\n-    __ lwz(suspend_flags, thread_(suspend_flags));\n-    __ cmpwi(CCR1, suspend_flags, 0);\n-    __ beq(CCR1, after_transition);\n-    __ bind(needs_safepoint);\n-  }\n-\n@@ -2446,1 +2284,0 @@\n-  if (!is_critical_native) {\n@@ -2458,1 +2295,0 @@\n-  }\n@@ -2473,1 +2309,0 @@\n-  if (!is_critical_native) {\n@@ -2480,1 +2315,0 @@\n-  }\n","filename":"src\/hotspot\/cpu\/ppc\/sharedRuntime_ppc.cpp","additions":22,"deletions":188,"binary":false,"changes":210,"status":"modified"},{"patch":"@@ -55,1 +55,1 @@\n-  : _index(index), _array(NULL), _throw_index_out_of_bounds_exception(true) {\n+  : _index(index), _array(), _throw_index_out_of_bounds_exception(true) {\n","filename":"src\/hotspot\/cpu\/s390\/c1_CodeStubs_s390.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -141,2 +141,2 @@\n-LIR_Opr FrameMap::_caller_save_cpu_regs[] = { 0, };\n-LIR_Opr FrameMap::_caller_save_fpu_regs[] = { 0, };\n+LIR_Opr FrameMap::_caller_save_cpu_regs[] = {};\n+LIR_Opr FrameMap::_caller_save_fpu_regs[] = {};\n","filename":"src\/hotspot\/cpu\/s390\/c1_FrameMap_s390.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -32,1 +32,1 @@\n-FloatRegister LIR_OprDesc::as_float_reg() const {\n+FloatRegister LIR_Opr::as_float_reg() const {\n@@ -36,1 +36,1 @@\n-FloatRegister LIR_OprDesc::as_double_reg() const {\n+FloatRegister LIR_Opr::as_double_reg() const {\n@@ -43,5 +43,5 @@\n-  return (LIR_Opr)(intptr_t)((reg1 << LIR_OprDesc::reg1_shift) |\n-                             (reg1 << LIR_OprDesc::reg2_shift) |\n-                             LIR_OprDesc::double_type          |\n-                             LIR_OprDesc::fpu_register         |\n-                             LIR_OprDesc::double_size);\n+  return (LIR_Opr)(intptr_t)((reg1 << LIR_Opr::reg1_shift) |\n+                             (reg1 << LIR_Opr::reg2_shift) |\n+                             LIR_Opr::double_type          |\n+                             LIR_Opr::fpu_register         |\n+                             LIR_Opr::double_size);\n","filename":"src\/hotspot\/cpu\/s390\/c1_LIR_s390.cpp","additions":7,"deletions":7,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -1539,0 +1539,4 @@\n+const bool Matcher::match_rule_supported_vector_masked(int opcode, int vlen, BasicType bt) {\n+  return false;\n+}\n+\n","filename":"src\/hotspot\/cpu\/s390\/s390.ad","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -1289,70 +1289,0 @@\n-static void move_ptr(MacroAssembler *masm,\n-                     VMRegPair src,\n-                     VMRegPair dst,\n-                     int framesize_in_slots) {\n-  int frame_offset = framesize_in_slots * VMRegImpl::stack_slot_size;\n-\n-  if (src.first()->is_stack()) {\n-    if (dst.first()->is_stack()) {\n-      \/\/ stack to stack\n-      __ mem2reg_opt(Z_R0_scratch, Address(Z_SP, reg2offset(src.first()) + frame_offset));\n-      __ reg2mem_opt(Z_R0_scratch, Address(Z_SP, reg2offset(dst.first())));\n-    } else {\n-      \/\/ stack to reg\n-      __ mem2reg_opt(dst.first()->as_Register(),\n-                     Address(Z_SP, reg2offset(src.first()) + frame_offset));\n-    }\n-  } else {\n-    if (dst.first()->is_stack()) {\n-      \/\/ reg to stack\n-    __ reg2mem_opt(src.first()->as_Register(), Address(Z_SP, reg2offset(dst.first())));\n-    } else {\n-    __ lgr_if_needed(dst.first()->as_Register(), src.first()->as_Register());\n-    }\n-  }\n-}\n-\n-\/\/ Unpack an array argument into a pointer to the body and the length\n-\/\/ if the array is non-null, otherwise pass 0 for both.\n-static void unpack_array_argument(MacroAssembler *masm,\n-                                   VMRegPair reg,\n-                                   BasicType in_elem_type,\n-                                   VMRegPair body_arg,\n-                                   VMRegPair length_arg,\n-                                   int framesize_in_slots) {\n-  Register tmp_reg = Z_tmp_2;\n-  Register tmp2_reg = Z_tmp_1;\n-\n-  assert(!body_arg.first()->is_Register() || body_arg.first()->as_Register() != tmp_reg,\n-         \"possible collision\");\n-  assert(!length_arg.first()->is_Register() || length_arg.first()->as_Register() != tmp_reg,\n-         \"possible collision\");\n-\n-  \/\/ Pass the length, ptr pair.\n-  NearLabel set_out_args;\n-  VMRegPair tmp, tmp2;\n-\n-  tmp.set_ptr(tmp_reg->as_VMReg());\n-  tmp2.set_ptr(tmp2_reg->as_VMReg());\n-  if (reg.first()->is_stack()) {\n-    \/\/ Load the arg up from the stack.\n-    move_ptr(masm, reg, tmp, framesize_in_slots);\n-    reg = tmp;\n-  }\n-\n-  const Register first = reg.first()->as_Register();\n-\n-  \/\/ Don't set CC, indicate unused result.\n-  (void) __ clear_reg(tmp2_reg, true, false);\n-  if (tmp_reg != first) {\n-    __ clear_reg(tmp_reg, true, false);  \/\/ Don't set CC.\n-  }\n-  __ compare64_and_branch(first, (RegisterOrConstant)0L, Assembler::bcondEqual, set_out_args);\n-  __ z_lgf(tmp2_reg, Address(first, arrayOopDesc::length_offset_in_bytes()));\n-  __ add2reg(tmp_reg, arrayOopDesc::base_offset_in_bytes(in_elem_type), first);\n-\n-  __ bind(set_out_args);\n-  move_ptr(masm, tmp, body_arg, framesize_in_slots);\n-  move32_64(masm, tmp2, length_arg, framesize_in_slots);\n-}\n-\n@@ -1368,2 +1298,1 @@\n-                                                BasicType ret_type,\n-                                                address critical_entry) {\n+                                                BasicType ret_type) {\n@@ -1403,6 +1332,1 @@\n-  bool is_critical_native = true;\n-  address native_func = critical_entry;\n-  if (native_func == NULL) {\n-    native_func = method->native_function();\n-    is_critical_native = false;\n-  }\n+  address native_func = method->native_function();\n@@ -1433,13 +1357,1 @@\n-  int  total_c_args     = total_in_args;\n-\n-  if (!is_critical_native) {\n-    int n_hidden_args = method_is_static ? 2 : 1;\n-    total_c_args += n_hidden_args;\n-  } else {\n-    \/\/ No JNIEnv*, no this*, but unpacked arrays (base+length).\n-    for (int i = 0; i < total_in_args; i++) {\n-      if (in_sig_bt[i] == T_ARRAY) {\n-        total_c_args ++;\n-      }\n-    }\n-  }\n+  int  total_c_args     = total_in_args + (method_is_static ? 2 : 1);\n@@ -1458,29 +1370,4 @@\n-  if (!is_critical_native) {\n-    out_sig_bt[argc++] = T_ADDRESS;\n-    if (method->is_static()) {\n-      out_sig_bt[argc++] = T_OBJECT;\n-    }\n-\n-    for (int i = 0; i < total_in_args; i++) {\n-      out_sig_bt[argc++] = in_sig_bt[i];\n-    }\n-  } else {\n-    in_elem_bt = NEW_RESOURCE_ARRAY(BasicType, total_in_args);\n-    SignatureStream ss(method->signature());\n-    int o = 0;\n-    for (int i = 0; i < total_in_args; i++, o++) {\n-      if (in_sig_bt[i] == T_ARRAY) {\n-        \/\/ Arrays are passed as tuples (int, elem*).\n-        ss.skip_array_prefix(1);  \/\/ skip one '['\n-        assert(ss.is_primitive(), \"primitive type expected\");\n-        in_elem_bt[o] = ss.type();\n-      } else {\n-        in_elem_bt[o] = T_VOID;\n-      }\n-      if (in_sig_bt[i] != T_VOID) {\n-        assert(in_sig_bt[i] == ss.type() ||\n-               in_sig_bt[i] == T_ARRAY, \"must match\");\n-        ss.next();\n-      }\n-    }\n-    assert(total_in_args == o, \"must match\");\n+  out_sig_bt[argc++] = T_ADDRESS;\n+  if (method->is_static()) {\n+    out_sig_bt[argc++] = T_OBJECT;\n+  }\n@@ -1488,9 +1375,2 @@\n-    for (int i = 0; i < total_in_args; i++) {\n-      if (in_sig_bt[i] == T_ARRAY) {\n-        \/\/ Arrays are passed as tuples (int, elem*).\n-        out_sig_bt[argc++] = T_INT;\n-        out_sig_bt[argc++] = T_ADDRESS;\n-      } else {\n-        out_sig_bt[argc++] = in_sig_bt[i];\n-      }\n-    }\n+  for (int i = 0; i < total_in_args; i++) {\n+    out_sig_bt[argc++] = in_sig_bt[i];\n@@ -1553,2 +1433,0 @@\n-  \/\/      | (save area for      |\n-  \/\/      |  critical natives)  |\n@@ -1582,31 +1460,0 @@\n-  if (is_critical_native) {\n-    \/\/ Critical natives may have to call out so they need a save area\n-    \/\/ for register arguments.\n-    int double_slots = 0;\n-    int single_slots = 0;\n-    for (int i = 0; i < total_in_args; i++) {\n-      if (in_regs[i].first()->is_Register()) {\n-        const Register reg = in_regs[i].first()->as_Register();\n-        switch (in_sig_bt[i]) {\n-          case T_BOOLEAN:\n-          case T_BYTE:\n-          case T_SHORT:\n-          case T_CHAR:\n-          case T_INT:\n-          \/\/ Fall through.\n-          case T_ARRAY:\n-          case T_LONG: double_slots++; break;\n-          default:  ShouldNotReachHere();\n-        }\n-      } else {\n-        if (in_regs[i].first()->is_FloatRegister()) {\n-          switch (in_sig_bt[i]) {\n-            case T_FLOAT:  single_slots++; break;\n-            case T_DOUBLE: double_slots++; break;\n-            default:  ShouldNotReachHere();\n-          }\n-        }\n-      }\n-    }  \/\/ for\n-    total_save_slots = double_slots * 2 + align_up(single_slots, 2); \/\/ Round to even.\n-  }\n@@ -1619,1 +1466,1 @@\n-  if (method_is_static && !is_critical_native) {                          \/\/ 4)\n+  if (method_is_static) {                                                 \/\/ 4)\n@@ -1786,7 +1633,0 @@\n-        if (is_critical_native) {\n-          int body_arg = cix;\n-          cix -= 1; \/\/ Point to length arg.\n-          unpack_array_argument(masm, in_regs[jix], in_elem_bt[jix], out_regs[body_arg], out_regs[cix], stack_slots);\n-          break;\n-        }\n-        \/\/ else fallthrough\n@@ -1794,1 +1634,0 @@\n-        assert(!is_critical_native, \"no oop arguments\");\n@@ -1824,1 +1663,1 @@\n-  if (method_is_static && !is_critical_native) {\n+  if (method_is_static) {\n@@ -1834,3 +1673,1 @@\n-  if (!is_critical_native) {\n-    __ add2reg(Z_ARG1, in_bytes(JavaThread::jni_environment_offset()), Z_thread);\n-  }\n+  __ add2reg(Z_ARG1, in_bytes(JavaThread::jni_environment_offset()), Z_thread);\n@@ -1858,1 +1695,0 @@\n-    assert(!is_critical_native, \"unhandled\");\n@@ -1926,4 +1762,2 @@\n-  if (!is_critical_native) {\n-    \/\/ Transition from _thread_in_Java to _thread_in_native.\n-    __ set_thread_state(_thread_in_native);\n-  }\n+  \/\/ Transition from _thread_in_Java to _thread_in_native.\n+  __ set_thread_state(_thread_in_native);\n@@ -1977,12 +1811,0 @@\n-  \/\/ If this is a critical native, check for a safepoint or suspend request after the call.\n-  \/\/ If a safepoint is needed, transition to native, then to native_trans to handle\n-  \/\/ safepoints like the native methods that are not critical natives.\n-  if (is_critical_native) {\n-    Label needs_safepoint;\n-    \/\/ Does this need to save_native_result and fences?\n-    __ safepoint_poll(needs_safepoint, Z_R1);\n-    __ load_and_test_int(Z_R0, Address(Z_thread, JavaThread::suspend_flags_offset()));\n-    __ z_bre(after_transition);\n-    __ bind(needs_safepoint);\n-  }\n-\n@@ -2159,3 +1981,2 @@\n-  if (!is_critical_native) {\n-    __ z_lg(Z_R1_scratch, Address(Z_thread, JavaThread::active_handles_offset()));\n-    __ clear_mem(Address(Z_R1_scratch, JNIHandleBlock::top_offset_in_bytes()), 4);\n+  __ z_lg(Z_R1_scratch, Address(Z_thread, JavaThread::active_handles_offset()));\n+  __ clear_mem(Address(Z_R1_scratch, JNIHandleBlock::top_offset_in_bytes()), 4);\n@@ -2163,4 +1984,3 @@\n-    \/\/ Check for pending exceptions.\n-    __ load_and_test_long(Z_R0, Address(Z_thread, Thread::pending_exception_offset()));\n-    __ z_brne(handle_pending_exception);\n-  }\n+  \/\/ Check for pending exceptions.\n+  __ load_and_test_long(Z_R0, Address(Z_thread, Thread::pending_exception_offset()));\n+  __ z_brne(handle_pending_exception);\n@@ -2188,20 +2008,17 @@\n-  if (!is_critical_native) {\n-\n-    \/\/---------------------------------------------------------------------\n-    \/\/ Handler for pending exceptions (out-of-line).\n-    \/\/---------------------------------------------------------------------\n-    \/\/ Since this is a native call, we know the proper exception handler\n-    \/\/ is the empty function. We just pop this frame and then jump to\n-    \/\/ forward_exception_entry. Z_R14 will contain the native caller's\n-    \/\/ return PC.\n-    __ bind(handle_pending_exception);\n-    __ pop_frame();\n-    __ load_const_optimized(Z_R1_scratch, StubRoutines::forward_exception_entry());\n-    __ restore_return_pc();\n-    __ z_br(Z_R1_scratch);\n-\n-    \/\/---------------------------------------------------------------------\n-    \/\/ Handler for a cache miss (out-of-line)\n-    \/\/---------------------------------------------------------------------\n-    __ call_ic_miss_handler(ic_miss, 0x77, 0, Z_R1_scratch);\n-  }\n+  \/\/---------------------------------------------------------------------\n+  \/\/ Handler for pending exceptions (out-of-line).\n+  \/\/---------------------------------------------------------------------\n+  \/\/ Since this is a native call, we know the proper exception handler\n+  \/\/ is the empty function. We just pop this frame and then jump to\n+  \/\/ forward_exception_entry. Z_R14 will contain the native caller's\n+  \/\/ return PC.\n+  __ bind(handle_pending_exception);\n+  __ pop_frame();\n+  __ load_const_optimized(Z_R1_scratch, StubRoutines::forward_exception_entry());\n+  __ restore_return_pc();\n+  __ z_br(Z_R1_scratch);\n+\n+  \/\/---------------------------------------------------------------------\n+  \/\/ Handler for a cache miss (out-of-line)\n+  \/\/---------------------------------------------------------------------\n+  __ call_ic_miss_handler(ic_miss, 0x77, 0, Z_R1_scratch);\n","filename":"src\/hotspot\/cpu\/s390\/sharedRuntime_s390.cpp","additions":36,"deletions":219,"binary":false,"changes":255,"status":"modified"},{"patch":"@@ -2461,0 +2461,7 @@\n+void Assembler::kmovbl(KRegister dst, KRegister src) {\n+  assert(VM_Version::supports_avx512dq(), \"\");\n+  InstructionAttr attributes(AVX_128bit, \/* rex_w *\/ false, \/* legacy_mode *\/ true, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n+  int encode = vex_prefix_and_encode(dst->encoding(), 0, src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int16((unsigned char)0x90, (0xC0 | encode));\n+}\n+\n@@ -2508,1 +2515,1 @@\n-  assert(VM_Version::supports_avx512bw(), \"\");\n+  assert(VM_Version::supports_evex(), \"\");\n@@ -2574,0 +2581,98 @@\n+void Assembler::knotbl(KRegister dst, KRegister src) {\n+  assert(VM_Version::supports_evex(), \"\");\n+  InstructionAttr attributes(AVX_128bit, \/* rex_w *\/ false, \/* legacy_mode *\/ true, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n+  int encode = vex_prefix_and_encode(dst->encoding(), 0, src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int16(0x44, (0xC0 | encode));\n+}\n+\n+void Assembler::korbl(KRegister dst, KRegister src1, KRegister src2) {\n+  assert(VM_Version::supports_avx512dq(), \"\");\n+  InstructionAttr attributes(AVX_256bit, \/* rex_w *\/ false, \/* legacy_mode *\/ true, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n+  int encode = vex_prefix_and_encode(dst->encoding(), src1->encoding(), src2->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int16(0x45, (0xC0 | encode));\n+}\n+\n+void Assembler::korwl(KRegister dst, KRegister src1, KRegister src2) {\n+  assert(VM_Version::supports_evex(), \"\");\n+  InstructionAttr attributes(AVX_256bit, \/* rex_w *\/ false, \/* legacy_mode *\/ true, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n+  int encode = vex_prefix_and_encode(dst->encoding(), src1->encoding(), src2->encoding(), VEX_SIMD_NONE, VEX_OPCODE_0F, &attributes);\n+  emit_int16(0x45, (0xC0 | encode));\n+}\n+\n+void Assembler::kordl(KRegister dst, KRegister src1, KRegister src2) {\n+  assert(VM_Version::supports_avx512bw(), \"\");\n+  InstructionAttr attributes(AVX_256bit, \/* rex_w *\/ true, \/* legacy_mode *\/ true, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n+  int encode = vex_prefix_and_encode(dst->encoding(), src1->encoding(), src2->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int16(0x45, (0xC0 | encode));\n+}\n+\n+void Assembler::korql(KRegister dst, KRegister src1, KRegister src2) {\n+  assert(VM_Version::supports_avx512bw(), \"\");\n+  InstructionAttr attributes(AVX_256bit, \/* rex_w *\/ true, \/* legacy_mode *\/ true, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n+  int encode = vex_prefix_and_encode(dst->encoding(), src1->encoding(), src2->encoding(), VEX_SIMD_NONE, VEX_OPCODE_0F, &attributes);\n+  emit_int16(0x45, (0xC0 | encode));\n+}\n+\n+void Assembler::kxorbl(KRegister dst, KRegister src1, KRegister src2) {\n+  assert(VM_Version::supports_avx512dq(), \"\");\n+  InstructionAttr attributes(AVX_256bit, \/* rex_w *\/ false, \/* legacy_mode *\/ true, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n+  int encode = vex_prefix_and_encode(dst->encoding(), src1->encoding(), src2->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int16(0x47, (0xC0 | encode));\n+}\n+\n+void Assembler::kxorwl(KRegister dst, KRegister src1, KRegister src2) {\n+  assert(VM_Version::supports_evex(), \"\");\n+  InstructionAttr attributes(AVX_256bit, \/* rex_w *\/ false, \/* legacy_mode *\/ true, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n+  int encode = vex_prefix_and_encode(dst->encoding(), src1->encoding(), src2->encoding(), VEX_SIMD_NONE, VEX_OPCODE_0F, &attributes);\n+  emit_int16(0x47, (0xC0 | encode));\n+}\n+\n+void Assembler::kxordl(KRegister dst, KRegister src1, KRegister src2) {\n+  assert(VM_Version::supports_avx512bw(), \"\");\n+  InstructionAttr attributes(AVX_256bit, \/* rex_w *\/ true, \/* legacy_mode *\/ true, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n+  int encode = vex_prefix_and_encode(dst->encoding(), src1->encoding(), src2->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int16(0x47, (0xC0 | encode));\n+}\n+\n+void Assembler::kxorql(KRegister dst, KRegister src1, KRegister src2) {\n+  assert(VM_Version::supports_avx512bw(), \"\");\n+  InstructionAttr attributes(AVX_256bit, \/* rex_w *\/ true, \/* legacy_mode *\/ true, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n+  int encode = vex_prefix_and_encode(dst->encoding(), src1->encoding(), src2->encoding(), VEX_SIMD_NONE, VEX_OPCODE_0F, &attributes);\n+  emit_int16(0x47, (0xC0 | encode));\n+}\n+\n+void Assembler::kandbl(KRegister dst, KRegister src1, KRegister src2) {\n+  assert(VM_Version::supports_avx512dq(), \"\");\n+  InstructionAttr attributes(AVX_256bit, \/* rex_w *\/ false, \/* legacy_mode *\/ true, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n+  int encode = vex_prefix_and_encode(dst->encoding(), src1->encoding(), src2->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int16(0x41, (0xC0 | encode));\n+}\n+\n+void Assembler::kandwl(KRegister dst, KRegister src1, KRegister src2) {\n+  assert(VM_Version::supports_evex(), \"\");\n+  InstructionAttr attributes(AVX_256bit, \/* rex_w *\/ false, \/* legacy_mode *\/ true, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n+  int encode = vex_prefix_and_encode(dst->encoding(), src1->encoding(), src2->encoding(), VEX_SIMD_NONE, VEX_OPCODE_0F, &attributes);\n+  emit_int16(0x41, (0xC0 | encode));\n+}\n+\n+void Assembler::kanddl(KRegister dst, KRegister src1, KRegister src2) {\n+  assert(VM_Version::supports_avx512bw(), \"\");\n+  InstructionAttr attributes(AVX_256bit, \/* rex_w *\/ true, \/* legacy_mode *\/ true, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n+  int encode = vex_prefix_and_encode(dst->encoding(), src1->encoding(), src2->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int16(0x41, (0xC0 | encode));\n+}\n+\n+void Assembler::kandql(KRegister dst, KRegister src1, KRegister src2) {\n+  assert(VM_Version::supports_avx512bw(), \"\");\n+  InstructionAttr attributes(AVX_256bit, \/* rex_w *\/ true, \/* legacy_mode *\/ true, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n+  int encode = vex_prefix_and_encode(dst->encoding(), src1->encoding(), src2->encoding(), VEX_SIMD_NONE, VEX_OPCODE_0F, &attributes);\n+  emit_int16(0x41, (0xC0 | encode));\n+}\n+\n+void Assembler::knotdl(KRegister dst, KRegister src) {\n+  assert(VM_Version::supports_avx512bw(), \"\");\n+  InstructionAttr attributes(AVX_128bit, \/* rex_w *\/ true, \/* legacy_mode *\/ true, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n+  int encode = vex_prefix_and_encode(dst->encoding(), 0, src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int16(0x44, (0xC0 | encode));\n+}\n+\n@@ -2621,0 +2726,21 @@\n+void Assembler::ktestdl(KRegister src1, KRegister src2) {\n+  assert(VM_Version::supports_avx512bw(), \"\");\n+  InstructionAttr attributes(AVX_128bit, \/* rex_w *\/ true, \/* legacy_mode *\/ true, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n+  int encode = vex_prefix_and_encode(src1->encoding(), 0, src2->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int16((unsigned char)0x99, (0xC0 | encode));\n+}\n+\n+void Assembler::ktestwl(KRegister src1, KRegister src2) {\n+  assert(VM_Version::supports_avx512dq(), \"\");\n+  InstructionAttr attributes(AVX_128bit, \/* rex_w *\/ false, \/* legacy_mode *\/ true, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n+  int encode = vex_prefix_and_encode(src1->encoding(), 0, src2->encoding(), VEX_SIMD_NONE, VEX_OPCODE_0F, &attributes);\n+  emit_int16((unsigned char)0x99, (0xC0 | encode));\n+}\n+\n+void Assembler::ktestbl(KRegister src1, KRegister src2) {\n+  assert(VM_Version::supports_avx512dq(), \"\");\n+  InstructionAttr attributes(AVX_128bit, \/* rex_w *\/ false, \/* legacy_mode *\/ true, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n+  int encode = vex_prefix_and_encode(src1->encoding(), 0, src2->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int16((unsigned char)0x99, (0xC0 | encode));\n+}\n+\n@@ -2635,0 +2761,46 @@\n+void Assembler::kxnorbl(KRegister dst, KRegister src1, KRegister src2) {\n+  assert(VM_Version::supports_avx512dq(), \"\");\n+  InstructionAttr attributes(AVX_256bit, \/* rex_w *\/ false, \/* legacy_mode *\/ true, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n+  int encode = vex_prefix_and_encode(dst->encoding(), src1->encoding(), src2->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int16(0x46, (0xC0 | encode));\n+}\n+\n+void Assembler::kshiftlbl(KRegister dst, KRegister src, int imm8) {\n+  assert(VM_Version::supports_avx512dq(), \"\");\n+  InstructionAttr attributes(AVX_128bit, \/* rex_w *\/ false, \/* legacy_mode *\/ true, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n+  int encode = vex_prefix_and_encode(dst->encoding(), 0 , src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_3A, &attributes);\n+  emit_int16(0x32, (0xC0 | encode));\n+  emit_int8(imm8);\n+}\n+\n+void Assembler::kshiftrbl(KRegister dst, KRegister src, int imm8) {\n+  assert(VM_Version::supports_avx512dq(), \"\");\n+  InstructionAttr attributes(AVX_128bit, \/* rex_w *\/ false, \/* legacy_mode *\/ true, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n+  int encode = vex_prefix_and_encode(dst->encoding(), 0 , src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_3A, &attributes);\n+  emit_int16(0x30, (0xC0 | encode));\n+}\n+\n+void Assembler::kshiftrwl(KRegister dst, KRegister src, int imm8) {\n+  assert(VM_Version::supports_evex(), \"\");\n+  InstructionAttr attributes(AVX_128bit, \/* rex_w *\/ true, \/* legacy_mode *\/ true, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n+  int encode = vex_prefix_and_encode(dst->encoding(), 0 , src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_3A, &attributes);\n+  emit_int16(0x30, (0xC0 | encode));\n+  emit_int8(imm8);\n+}\n+\n+void Assembler::kshiftrdl(KRegister dst, KRegister src, int imm8) {\n+  assert(VM_Version::supports_avx512bw(), \"\");\n+  InstructionAttr attributes(AVX_128bit, \/* rex_w *\/ false, \/* legacy_mode *\/ true, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n+  int encode = vex_prefix_and_encode(dst->encoding(), 0 , src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_3A, &attributes);\n+  emit_int16(0x31, (0xC0 | encode));\n+  emit_int8(imm8);\n+}\n+\n+void Assembler::kshiftrql(KRegister dst, KRegister src, int imm8) {\n+  assert(VM_Version::supports_avx512bw(), \"\");\n+  InstructionAttr attributes(AVX_128bit, \/* rex_w *\/ true, \/* legacy_mode *\/ true, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n+  int encode = vex_prefix_and_encode(dst->encoding(), 0 , src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_3A, &attributes);\n+  emit_int16(0x31, (0xC0 | encode));\n+  emit_int8(imm8);\n+}\n+\n@@ -4115,18 +4287,0 @@\n-void Assembler::evpmovd2m(KRegister kdst, XMMRegister src, int vector_len) {\n-  assert(UseAVX > 2  && VM_Version::supports_avx512dq(), \"\");\n-  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n-  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ true);\n-  attributes.set_is_evex_instruction();\n-  int encode = vex_prefix_and_encode(kdst->encoding(), 0, src->encoding(), VEX_SIMD_F3, VEX_OPCODE_0F_38, &attributes);\n-  emit_int16(0x39, (0xC0 | encode));\n-}\n-\n-void Assembler::evpmovq2m(KRegister kdst, XMMRegister src, int vector_len) {\n-  assert(UseAVX > 2  && VM_Version::supports_avx512dq(), \"\");\n-  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n-  InstructionAttr attributes(vector_len, \/* vex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ true);\n-  attributes.set_is_evex_instruction();\n-  int encode = vex_prefix_and_encode(kdst->encoding(), 0, src->encoding(), VEX_SIMD_F3, VEX_OPCODE_0F_38, &attributes);\n-  emit_int16(0x39, (0xC0 | encode));\n-}\n-\n@@ -7422,1 +7576,0 @@\n-  assert(VM_Version::supports_evex(), \"\");\n@@ -7424,0 +7577,1 @@\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n@@ -7434,0 +7588,112 @@\n+void Assembler::evpxord(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len) {\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionMark im(this);\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false,\/* legacy_mode *\/ false, \/* no_mask_reg *\/ false,\/* uses_vl *\/ true);\n+  attributes.set_address_attributes(\/* tuple_type *\/ EVEX_FV,\/* input_size_in_bits *\/ EVEX_32bit);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  vex_prefix(src, nds->encoding(), dst->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int8((unsigned char)0xEF);\n+  emit_operand(dst, src);\n+}\n+\n+void Assembler::evpxorq(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  \/\/ Encoding: EVEX.NDS.XXX.66.0F.W1 EF \/r\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int16((unsigned char)0xEF, (0xC0 | encode));\n+}\n+\n+void Assembler::evpxorq(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len) {\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionMark im(this);\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ true,\/* legacy_mode *\/ false, \/* no_mask_reg *\/ false,\/* uses_vl *\/ true);\n+  attributes.set_address_attributes(\/* tuple_type *\/ EVEX_FV,\/* input_size_in_bits *\/ EVEX_32bit);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  vex_prefix(src, nds->encoding(), dst->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int8((unsigned char)0xEF);\n+  emit_operand(dst, src);\n+}\n+\n+void Assembler::evpandd(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len) {\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionMark im(this);\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false,\/* legacy_mode *\/ false, \/* no_mask_reg *\/ false,\/* uses_vl *\/ true);\n+  attributes.set_address_attributes(\/* tuple_type *\/ EVEX_FV,\/* input_size_in_bits *\/ EVEX_32bit);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  vex_prefix(src, nds->encoding(), dst->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int8((unsigned char)0xDB);\n+  emit_operand(dst, src);\n+}\n+\n+void Assembler::evpandq(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  assert(VM_Version::supports_evex(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int16((unsigned char)0xDB, (0xC0 | encode));\n+}\n+\n+void Assembler::evpandq(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len) {\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionMark im(this);\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ true,\/* legacy_mode *\/ false, \/* no_mask_reg *\/ false,\/* uses_vl *\/ true);\n+  attributes.set_address_attributes(\/* tuple_type *\/ EVEX_FV,\/* input_size_in_bits *\/ EVEX_32bit);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  vex_prefix(src, nds->encoding(), dst->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int8((unsigned char)0xDB);\n+  emit_operand(dst, src);\n+}\n+\n+void Assembler::evporq(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  assert(VM_Version::supports_evex(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int16((unsigned char)0xEB, (0xC0 | encode));\n+}\n+\n+void Assembler::evporq(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len) {\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionMark im(this);\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ true,\/* legacy_mode *\/ false, \/* no_mask_reg *\/ false,\/* uses_vl *\/ true);\n+  attributes.set_address_attributes(\/* tuple_type *\/ EVEX_FV,\/* input_size_in_bits *\/ EVEX_32bit);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  vex_prefix(src, nds->encoding(), dst->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int8((unsigned char)0xEB);\n+  emit_operand(dst, src);\n+}\n+\n@@ -7978,6 +8244,10 @@\n-\/\/ duplicate 4-byte integer data from src into programmed locations in dest : requires AVX512VL\n-void Assembler::vpbroadcastd(XMMRegister dst, XMMRegister src, int vector_len) {\n-  assert(UseAVX >= 2, \"\");\n-  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ true);\n-  int encode = vex_prefix_and_encode(dst->encoding(), 0, src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n-  emit_int16(0x58, (0xC0 | encode));\n+void Assembler::evpaddb(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  assert(VM_Version::supports_avx512bw() && (vector_len == AVX_512bit || VM_Version::supports_avx512vl()), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false,\/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int16((unsigned char)0xFC, (0xC0 | encode));\n@@ -7986,3 +8256,1 @@\n-void Assembler::vpbroadcastd(XMMRegister dst, Address src, int vector_len) {\n-  assert(VM_Version::supports_avx2(), \"\");\n-  assert(dst != xnoreg, \"sanity\");\n+void Assembler::evpaddb(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len) {\n@@ -7990,11 +8258,1448 @@\n-  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ true);\n-  attributes.set_address_attributes(\/* tuple_type *\/ EVEX_T1S, \/* input_size_in_bits *\/ EVEX_32bit);\n-  \/\/ swap src<->dst for encoding\n-  vex_prefix(src, 0, dst->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n-  emit_int8(0x58);\n-  emit_operand(dst, src);\n-}\n-\n-\/\/ duplicate 8-byte integer data from src into programmed locations in dest : requires AVX512VL\n-void Assembler::vpbroadcastq(XMMRegister dst, XMMRegister src, int vector_len) {\n-  assert(VM_Version::supports_avx2(), \"\");\n+  assert(VM_Version::supports_avx512bw() && (vector_len == AVX_512bit || VM_Version::supports_avx512vl()), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false,\/* legacy_mode *\/ false, \/* no_mask_reg *\/ false,\/* uses_vl *\/ true);\n+  attributes.set_address_attributes(\/* tuple_type *\/ EVEX_FV,\/* input_size_in_bits *\/ EVEX_32bit);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  vex_prefix(src, nds->encoding(), dst->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int8((unsigned char)0xFC);\n+  emit_operand(dst, src);\n+}\n+\n+void Assembler::evpaddw(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  assert(VM_Version::supports_avx512bw() && (vector_len == AVX_512bit || VM_Version::supports_avx512vl()), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false,\/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int16((unsigned char)0xFD, (0xC0 | encode));\n+}\n+\n+void Assembler::evpaddw(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len) {\n+  InstructionMark im(this);\n+  assert(VM_Version::supports_avx512bw() && (vector_len == AVX_512bit || VM_Version::supports_avx512vl()), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false,\/* legacy_mode *\/ false, \/* no_mask_reg *\/ false,\/* uses_vl *\/ true);\n+  attributes.set_address_attributes(\/* tuple_type *\/ EVEX_FV,\/* input_size_in_bits *\/ EVEX_32bit);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  vex_prefix(src, nds->encoding(), dst->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int8((unsigned char)0xFD);\n+  emit_operand(dst, src);\n+}\n+\n+void Assembler::evpaddd(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  assert(VM_Version::supports_evex(), \"\");\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false,\/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int16((unsigned char)0xFE, (0xC0 | encode));\n+}\n+\n+void Assembler::evpaddd(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len) {\n+  InstructionMark im(this);\n+  assert(VM_Version::supports_evex(), \"\");\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false,\/* legacy_mode *\/ false, \/* no_mask_reg *\/ false,\/* uses_vl *\/ true);\n+  attributes.set_address_attributes(\/* tuple_type *\/ EVEX_FV,\/* input_size_in_bits *\/ EVEX_32bit);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  vex_prefix(src, nds->encoding(), dst->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int8((unsigned char)0xFE);\n+  emit_operand(dst, src);\n+}\n+\n+void Assembler::evpaddq(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  assert(VM_Version::supports_evex(), \"\");\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false,\/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int16((unsigned char)0xD4, (0xC0 | encode));\n+}\n+\n+void Assembler::evpaddq(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len) {\n+  InstructionMark im(this);\n+  assert(VM_Version::supports_evex(), \"\");\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ true,\/* legacy_mode *\/ false, \/* no_mask_reg *\/ false,\/* uses_vl *\/ true);\n+  attributes.set_address_attributes(\/* tuple_type *\/ EVEX_FV,\/* input_size_in_bits *\/ EVEX_32bit);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  vex_prefix(src, nds->encoding(), dst->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int8((unsigned char)0xD4);\n+  emit_operand(dst, src);\n+}\n+\n+void Assembler::evaddps(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  assert(VM_Version::supports_evex(), \"\");\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false,\/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_NONE, VEX_OPCODE_0F, &attributes);\n+  emit_int16(0x58, (0xC0 | encode));\n+}\n+\n+void Assembler::evaddps(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len) {\n+  InstructionMark im(this);\n+  assert(VM_Version::supports_evex(), \"\");\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false,\/* legacy_mode *\/ false, \/* no_mask_reg *\/ false,\/* uses_vl *\/ true);\n+  attributes.set_address_attributes(\/* tuple_type *\/ EVEX_FV,\/* input_size_in_bits *\/ EVEX_32bit);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  vex_prefix(src, nds->encoding(), dst->encoding(), VEX_SIMD_NONE, VEX_OPCODE_0F, &attributes);\n+  emit_int8(0x58);\n+  emit_operand(dst, src);\n+}\n+\n+void Assembler::evaddpd(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  assert(VM_Version::supports_evex(), \"\");\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false,\/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int16(0x58, (0xC0 | encode));\n+}\n+\n+void Assembler::evaddpd(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len) {\n+  InstructionMark im(this);\n+  assert(VM_Version::supports_evex(), \"\");\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ true,\/* legacy_mode *\/ false, \/* no_mask_reg *\/ false,\/* uses_vl *\/ true);\n+  attributes.set_address_attributes(\/* tuple_type *\/ EVEX_FV,\/* input_size_in_bits *\/ EVEX_32bit);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  vex_prefix(src, nds->encoding(), dst->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int8(0x58);\n+  emit_operand(dst, src);\n+}\n+\n+void Assembler::evpsubb(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  assert(VM_Version::supports_avx512bw() && (vector_len == AVX_512bit || VM_Version::supports_avx512vl()), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false,\/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int16((unsigned char)0xF8, (0xC0 | encode));\n+}\n+\n+void Assembler::evpsubb(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len) {\n+  InstructionMark im(this);\n+  assert(VM_Version::supports_avx512bw() && (vector_len == AVX_512bit || VM_Version::supports_avx512vl()), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false,\/* legacy_mode *\/ false, \/* no_mask_reg *\/ false,\/* uses_vl *\/ true);\n+  attributes.set_address_attributes(\/* tuple_type *\/ EVEX_FV,\/* input_size_in_bits *\/ EVEX_32bit);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  vex_prefix(src, nds->encoding(), dst->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int8((unsigned char)0xF8);\n+  emit_operand(dst, src);\n+}\n+\n+void Assembler::evpsubw(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  assert(VM_Version::supports_avx512bw() && (vector_len == AVX_512bit || VM_Version::supports_avx512vl()), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false,\/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int16((unsigned char)0xF9, (0xC0 | encode));\n+}\n+\n+void Assembler::evpsubw(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len) {\n+  InstructionMark im(this);\n+  assert(VM_Version::supports_avx512bw() && (vector_len == AVX_512bit || VM_Version::supports_avx512vl()), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false,\/* legacy_mode *\/ false, \/* no_mask_reg *\/ false,\/* uses_vl *\/ true);\n+  attributes.set_address_attributes(\/* tuple_type *\/ EVEX_FV,\/* input_size_in_bits *\/ EVEX_32bit);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  vex_prefix(src, nds->encoding(), dst->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int8((unsigned char)0xF9);\n+  emit_operand(dst, src);\n+}\n+\n+void Assembler::evpsubd(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  assert(VM_Version::supports_evex(), \"\");\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false,\/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int16((unsigned char)0xFA, (0xC0 | encode));\n+}\n+\n+void Assembler::evpsubd(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len) {\n+  InstructionMark im(this);\n+  assert(VM_Version::supports_evex(), \"\");\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false,\/* legacy_mode *\/ false, \/* no_mask_reg *\/ false,\/* uses_vl *\/ true);\n+  attributes.set_address_attributes(\/* tuple_type *\/ EVEX_FV,\/* input_size_in_bits *\/ EVEX_32bit);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  vex_prefix(src, nds->encoding(), dst->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int8((unsigned char)0xFA);\n+  emit_operand(dst, src);\n+}\n+\n+void Assembler::evpsubq(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  assert(VM_Version::supports_evex(), \"\");\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false,\/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int16((unsigned char)0xFB, (0xC0 | encode));\n+}\n+\n+void Assembler::evpsubq(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len) {\n+  InstructionMark im(this);\n+  assert(VM_Version::supports_evex(), \"\");\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ true,\/* legacy_mode *\/ false, \/* no_mask_reg *\/ false,\/* uses_vl *\/ true);\n+  attributes.set_address_attributes(\/* tuple_type *\/ EVEX_FV,\/* input_size_in_bits *\/ EVEX_32bit);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  vex_prefix(src, nds->encoding(), dst->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int8((unsigned char)0xFB);\n+  emit_operand(dst, src);\n+}\n+\n+void Assembler::evsubps(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  assert(VM_Version::supports_evex(), \"\");\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false,\/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_NONE, VEX_OPCODE_0F, &attributes);\n+  emit_int16(0x5C, (0xC0 | encode));\n+}\n+\n+void Assembler::evsubps(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len) {\n+  InstructionMark im(this);\n+  assert(VM_Version::supports_evex(), \"\");\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false,\/* legacy_mode *\/ false, \/* no_mask_reg *\/ false,\/* uses_vl *\/ true);\n+  attributes.set_address_attributes(\/* tuple_type *\/ EVEX_FV,\/* input_size_in_bits *\/ EVEX_32bit);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  vex_prefix(src, nds->encoding(), dst->encoding(), VEX_SIMD_NONE, VEX_OPCODE_0F, &attributes);\n+  emit_int8(0x5C);\n+  emit_operand(dst, src);\n+}\n+\n+void Assembler::evsubpd(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  assert(VM_Version::supports_evex(), \"\");\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false,\/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int16(0x5C, (0xC0 | encode));\n+}\n+\n+void Assembler::evsubpd(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len) {\n+  InstructionMark im(this);\n+  assert(VM_Version::supports_evex(), \"\");\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ true,\/* legacy_mode *\/ false, \/* no_mask_reg *\/ false,\/* uses_vl *\/ true);\n+  attributes.set_address_attributes(\/* tuple_type *\/ EVEX_FV,\/* input_size_in_bits *\/ EVEX_32bit);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  vex_prefix(src, nds->encoding(), dst->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int8(0x5C);\n+  emit_operand(dst, src);\n+}\n+\n+void Assembler::evpmullw(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  assert(VM_Version::supports_avx512bw() && (vector_len == AVX_512bit || VM_Version::supports_avx512vl()), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false,\/* legacy_mode *\/ false, \/* no_mask_reg *\/ false,\/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int16((unsigned char)0xD5, (0xC0 | encode));\n+}\n+\n+void Assembler::evpmullw(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len) {\n+  InstructionMark im(this);\n+  assert(VM_Version::supports_avx512bw() && (vector_len == AVX_512bit || VM_Version::supports_avx512vl()), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false,\/* legacy_mode *\/ false, \/* no_mask_reg *\/ false,\/* uses_vl *\/ true);\n+  attributes.set_address_attributes(\/* tuple_type *\/ EVEX_FV,\/* input_size_in_bits *\/ EVEX_32bit);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  vex_prefix(src, nds->encoding(), dst->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int8((unsigned char)0xD5);\n+  emit_operand(dst, src);\n+}\n+\n+void Assembler::evpmulld(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  assert(VM_Version::supports_evex(), \"\");\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false,\/* legacy_mode *\/ false, \/* no_mask_reg *\/ false,\/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int16(0x40, (0xC0 | encode));\n+}\n+\n+void Assembler::evpmulld(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len) {\n+  InstructionMark im(this);\n+  assert(VM_Version::supports_evex(), \"\");\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false,\/* legacy_mode *\/ false, \/* no_mask_reg *\/ false,\/* uses_vl *\/ true);\n+  attributes.set_address_attributes(\/* tuple_type *\/ EVEX_FV,\/* input_size_in_bits *\/ EVEX_32bit);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  vex_prefix(src, nds->encoding(), dst->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int8(0x40);\n+  emit_operand(dst, src);\n+}\n+\n+void Assembler::evpmullq(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  assert(VM_Version::supports_avx512dq() && (vector_len == AVX_512bit || VM_Version::supports_avx512vl()), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ true,\/* legacy_mode *\/ false, \/* no_mask_reg *\/ false,\/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int16(0x40, (0xC0 | encode));\n+}\n+\n+void Assembler::evpmullq(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len) {\n+  InstructionMark im(this);\n+  assert(VM_Version::supports_avx512dq() && (vector_len == AVX_512bit || VM_Version::supports_avx512vl()), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ true,\/* legacy_mode *\/ false, \/* no_mask_reg *\/ false,\/* uses_vl *\/ true);\n+  attributes.set_address_attributes(\/* tuple_type *\/ EVEX_FV,\/* input_size_in_bits *\/ EVEX_32bit);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  vex_prefix(src, nds->encoding(), dst->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int8(0x40);\n+  emit_operand(dst, src);\n+}\n+\n+void Assembler::evmulps(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  assert(VM_Version::supports_evex(), \"\");\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false,\/* legacy_mode *\/ false, \/* no_mask_reg *\/ false,\/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_NONE, VEX_OPCODE_0F, &attributes);\n+  emit_int16(0x59, (0xC0 | encode));\n+}\n+\n+void Assembler::evmulps(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len) {\n+  InstructionMark im(this);\n+  assert(VM_Version::supports_evex(), \"\");\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false,\/* legacy_mode *\/ false, \/* no_mask_reg *\/ false,\/* uses_vl *\/ true);\n+  attributes.set_address_attributes(\/* tuple_type *\/ EVEX_FV,\/* input_size_in_bits *\/ EVEX_32bit);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  vex_prefix(src, nds->encoding(), dst->encoding(), VEX_SIMD_NONE, VEX_OPCODE_0F, &attributes);\n+  emit_int8(0x59);\n+  emit_operand(dst, src);\n+}\n+\n+void Assembler::evmulpd(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  assert(VM_Version::supports_evex(), \"\");\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ true,\/* legacy_mode *\/ false, \/* no_mask_reg *\/ false,\/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int16(0x59, (0xC0 | encode));\n+}\n+\n+void Assembler::evmulpd(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len) {\n+  InstructionMark im(this);\n+  assert(VM_Version::supports_evex(), \"\");\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ true,\/* legacy_mode *\/ false, \/* no_mask_reg *\/ false,\/* uses_vl *\/ true);\n+  attributes.set_address_attributes(\/* tuple_type *\/ EVEX_FV,\/* input_size_in_bits *\/ EVEX_32bit);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  vex_prefix(src, nds->encoding(), dst->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int8(0x59);\n+  emit_operand(dst, src);\n+}\n+\n+void Assembler::evsqrtps(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  assert(VM_Version::supports_evex(), \"\");\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len,\/* vex_w *\/ false,\/* legacy_mode *\/ false, \/* no_mask_reg *\/ false,\/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), 0, src->encoding(), VEX_SIMD_NONE, VEX_OPCODE_0F, &attributes);\n+  emit_int16(0x51, (0xC0 | encode));\n+}\n+\n+void Assembler::evsqrtps(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len) {\n+  InstructionMark im(this);\n+  assert(VM_Version::supports_evex(), \"\");\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false,\/* legacy_mode *\/ false, \/* no_mask_reg *\/ false,\/* uses_vl *\/ true);\n+  attributes.set_address_attributes(\/* tuple_type *\/ EVEX_FV,\/* input_size_in_bits *\/ EVEX_32bit);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  vex_prefix(src, 0, dst->encoding(), VEX_SIMD_NONE, VEX_OPCODE_0F, &attributes);\n+  emit_int8(0x51);\n+  emit_operand(dst, src);\n+}\n+\n+void Assembler::evsqrtpd(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  assert(VM_Version::supports_evex(), \"\");\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len,\/* vex_w *\/ true,\/* legacy_mode *\/ false, \/* no_mask_reg *\/ false,\/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), 0, src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int16(0x51, (0xC0 | encode));\n+}\n+\n+void Assembler::evsqrtpd(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len) {\n+  InstructionMark im(this);\n+  assert(VM_Version::supports_evex(), \"\");\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ true,\/* legacy_mode *\/ false, \/* no_mask_reg *\/ false,\/* uses_vl *\/ true);\n+  attributes.set_address_attributes(\/* tuple_type *\/ EVEX_FV,\/* input_size_in_bits *\/ EVEX_32bit);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  vex_prefix(src, 0, dst->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int8(0x51);\n+  emit_operand(dst, src);\n+}\n+\n+\n+void Assembler::evdivps(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  assert(VM_Version::supports_evex(), \"\");\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len,\/* vex_w *\/ false,\/* legacy_mode *\/ false, \/* no_mask_reg *\/ false,\/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_NONE, VEX_OPCODE_0F, &attributes);\n+  emit_int16(0x5E, (0xC0 | encode));\n+}\n+\n+void Assembler::evdivps(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len) {\n+  InstructionMark im(this);\n+  assert(VM_Version::supports_evex(), \"\");\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false,\/* legacy_mode *\/ false, \/* no_mask_reg *\/ false,\/* uses_vl *\/ true);\n+  attributes.set_address_attributes(\/* tuple_type *\/ EVEX_FV,\/* input_size_in_bits *\/ EVEX_32bit);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  vex_prefix(src, nds->encoding(), dst->encoding(), VEX_SIMD_NONE, VEX_OPCODE_0F, &attributes);\n+  emit_int8(0x5E);\n+  emit_operand(dst, src);\n+}\n+\n+void Assembler::evdivpd(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  assert(VM_Version::supports_evex(), \"\");\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len,\/* vex_w *\/ true,\/* legacy_mode *\/ false, \/* no_mask_reg *\/ false,\/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int16(0x5E, (0xC0 | encode));\n+}\n+\n+void Assembler::evdivpd(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len) {\n+  InstructionMark im(this);\n+  assert(VM_Version::supports_evex(), \"\");\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ true,\/* legacy_mode *\/ false, \/* no_mask_reg *\/ false,\/* uses_vl *\/ true);\n+  attributes.set_address_attributes(\/* tuple_type *\/ EVEX_FV,\/* input_size_in_bits *\/ EVEX_32bit);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  vex_prefix(src, nds->encoding(), dst->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int8(0x5E);\n+  emit_operand(dst, src);\n+}\n+\n+void Assembler::evpabsb(XMMRegister dst, KRegister mask, XMMRegister src, bool merge, int vector_len) {\n+  assert(VM_Version::supports_avx512bw() && (vector_len == AVX_512bit || VM_Version::supports_avx512vl()), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false,\/* legacy_mode *\/ false, \/* no_mask_reg *\/ false,\/* uses_vl *\/ true);\n+  attributes.set_address_attributes(\/* tuple_type *\/ EVEX_FV,\/* input_size_in_bits *\/ EVEX_32bit);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), 0, src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int16(0x1C, (0xC0 | encode));\n+}\n+\n+\n+void Assembler::evpabsb(XMMRegister dst, KRegister mask, Address src, bool merge, int vector_len) {\n+  InstructionMark im(this);\n+  assert(VM_Version::supports_avx512bw() && (vector_len == AVX_512bit || VM_Version::supports_avx512vl()), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false,\/* legacy_mode *\/ false, \/* no_mask_reg *\/ false,\/* uses_vl *\/ true);\n+  attributes.set_address_attributes(\/* tuple_type *\/ EVEX_FV,\/* input_size_in_bits *\/ EVEX_32bit);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  vex_prefix(src, 0, dst->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int8(0x1C);\n+  emit_operand(dst, src);\n+}\n+\n+void Assembler::evpabsw(XMMRegister dst, KRegister mask, XMMRegister src, bool merge, int vector_len) {\n+  assert(VM_Version::supports_avx512bw() && (vector_len == AVX_512bit || VM_Version::supports_avx512vl()), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false,\/* legacy_mode *\/ false, \/* no_mask_reg *\/ false,\/* uses_vl *\/ true);\n+  attributes.set_address_attributes(\/* tuple_type *\/ EVEX_FV,\/* input_size_in_bits *\/ EVEX_32bit);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), 0, src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int16(0x1D, (0xC0 | encode));\n+}\n+\n+\n+void Assembler::evpabsw(XMMRegister dst, KRegister mask, Address src, bool merge, int vector_len) {\n+  InstructionMark im(this);\n+  assert(VM_Version::supports_avx512bw() && (vector_len == AVX_512bit || VM_Version::supports_avx512vl()), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false,\/* legacy_mode *\/ false, \/* no_mask_reg *\/ false,\/* uses_vl *\/ true);\n+  attributes.set_address_attributes(\/* tuple_type *\/ EVEX_FV,\/* input_size_in_bits *\/ EVEX_32bit);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  vex_prefix(src, 0, dst->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int8(0x1D);\n+  emit_operand(dst, src);\n+}\n+\n+void Assembler::evpabsd(XMMRegister dst, KRegister mask, XMMRegister src, bool merge, int vector_len) {\n+  assert(VM_Version::supports_evex(), \"\");\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false,\/* legacy_mode *\/ false, \/* no_mask_reg *\/ false,\/* uses_vl *\/ true);\n+  attributes.set_address_attributes(\/* tuple_type *\/ EVEX_FV,\/* input_size_in_bits *\/ EVEX_32bit);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), 0, src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int16(0x1E, (0xC0 | encode));\n+}\n+\n+\n+void Assembler::evpabsd(XMMRegister dst, KRegister mask, Address src, bool merge, int vector_len) {\n+  InstructionMark im(this);\n+  assert(VM_Version::supports_evex(), \"\");\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false,\/* legacy_mode *\/ false, \/* no_mask_reg *\/ false,\/* uses_vl *\/ true);\n+  attributes.set_address_attributes(\/* tuple_type *\/ EVEX_FV,\/* input_size_in_bits *\/ EVEX_32bit);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  vex_prefix(src, 0, dst->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int8(0x1E);\n+  emit_operand(dst, src);\n+}\n+\n+void Assembler::evpabsq(XMMRegister dst, KRegister mask, XMMRegister src, bool merge, int vector_len) {\n+  assert(VM_Version::supports_evex(), \"\");\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ true,\/* legacy_mode *\/ false, \/* no_mask_reg *\/ false,\/* uses_vl *\/ true);\n+  attributes.set_address_attributes(\/* tuple_type *\/ EVEX_FV,\/* input_size_in_bits *\/ EVEX_32bit);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), 0, src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int16(0x1F, (0xC0 | encode));\n+}\n+\n+\n+void Assembler::evpabsq(XMMRegister dst, KRegister mask, Address src, bool merge, int vector_len) {\n+  InstructionMark im(this);\n+  assert(VM_Version::supports_evex(), \"\");\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ true,\/* legacy_mode *\/ false, \/* no_mask_reg *\/ false,\/* uses_vl *\/ true);\n+  attributes.set_address_attributes(\/* tuple_type *\/ EVEX_FV,\/* input_size_in_bits *\/ EVEX_32bit);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  vex_prefix(src, 0, dst->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int8(0x1F);\n+  emit_operand(dst, src);\n+}\n+\n+void Assembler::evpfma213ps(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  assert(VM_Version::supports_evex(), \"\");\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false,\/* legacy_mode *\/ false, \/* no_mask_reg *\/ false,\/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int16((unsigned char)0xA8, (0xC0 | encode));\n+}\n+\n+void Assembler::evpfma213ps(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len) {\n+  InstructionMark im(this);\n+  assert(VM_Version::supports_evex(), \"\");\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false,\/* legacy_mode *\/ false, \/* no_mask_reg *\/ false,\/* uses_vl *\/ true);\n+  attributes.set_address_attributes(\/* tuple_type *\/ EVEX_FV,\/* input_size_in_bits *\/ EVEX_32bit);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  vex_prefix(src, nds->encoding(), dst->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int8((unsigned char)0xA8);\n+  emit_operand(dst, src);\n+}\n+\n+void Assembler::evpfma213pd(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  assert(VM_Version::supports_evex(), \"\");\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ true,\/* legacy_mode *\/ false, \/* no_mask_reg *\/ false,\/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int16((unsigned char)0xA8, (0xC0 | encode));\n+}\n+\n+void Assembler::evpfma213pd(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len) {\n+  InstructionMark im(this);\n+  assert(VM_Version::supports_evex(), \"\");\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ true,\/* legacy_mode *\/ false, \/* no_mask_reg *\/ false,\/* uses_vl *\/ true);\n+  attributes.set_address_attributes(\/* tuple_type *\/ EVEX_FV,\/* input_size_in_bits *\/ EVEX_32bit);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  vex_prefix(src, nds->encoding(), dst->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int8((unsigned char)0xA8);\n+  emit_operand(dst, src);\n+}\n+\n+void Assembler::evpermb(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  assert(VM_Version::supports_avx512_vbmi() && (vector_len == AVX_512bit || VM_Version::supports_avx512vl()), \"\");\n+  InstructionAttr attributes(vector_len, \/* rex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int16((unsigned char)0x8D, (0xC0 | encode));\n+}\n+\n+void Assembler::evpermb(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len) {\n+  assert(VM_Version::supports_avx512_vbmi() && (vector_len == AVX_512bit || VM_Version::supports_avx512vl()), \"\");\n+  InstructionMark im(this);\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  vex_prefix(src, nds->encoding(), dst->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int8((unsigned char)0x8D);\n+  emit_operand(dst, src);\n+}\n+\n+void Assembler::evpermw(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  assert(VM_Version::supports_avx512bw() && (vector_len == AVX_512bit || VM_Version::supports_avx512vl()), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int16((unsigned char)0x8D, (0xC0 | encode));\n+}\n+\n+void Assembler::evpermw(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len) {\n+  assert(VM_Version::supports_avx512bw() && (vector_len == AVX_512bit || VM_Version::supports_avx512vl()), \"\");\n+  InstructionMark im(this);\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  vex_prefix(src, nds->encoding(), dst->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int8((unsigned char)0x8D);\n+  emit_operand(dst, src);\n+}\n+\n+void Assembler::evpermd(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  assert(VM_Version::supports_evex() && vector_len > AVX_128bit, \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int16(0x36, (0xC0 | encode));\n+}\n+\n+void Assembler::evpermd(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len) {\n+  assert(VM_Version::supports_evex() && vector_len > AVX_128bit, \"\");\n+  InstructionMark im(this);\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  vex_prefix(src, nds->encoding(), dst->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int8(0x36);\n+  emit_operand(dst, src);\n+}\n+\n+void Assembler::evpermq(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  assert(VM_Version::supports_evex() && vector_len > AVX_128bit, \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int16(0x36, (0xC0 | encode));\n+}\n+\n+void Assembler::evpermq(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len) {\n+  assert(VM_Version::supports_evex() && vector_len > AVX_128bit, \"\");\n+  InstructionMark im(this);\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  vex_prefix(src, nds->encoding(), dst->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int8(0x36);\n+  emit_operand(dst, src);\n+}\n+\n+void Assembler::evpsllw(XMMRegister dst, KRegister mask, XMMRegister src, int shift, bool merge, int vector_len) {\n+  assert(VM_Version::supports_avx512bw() && (vector_len == AVX_512bit || VM_Version::supports_avx512vl()), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(xmm6->encoding(), dst->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int24(0x71, (0xC0 | encode), shift & 0xFF);\n+}\n+\n+void Assembler::evpslld(XMMRegister dst, KRegister mask, XMMRegister src, int shift, bool merge, int vector_len) {\n+  assert(VM_Version::supports_evex(), \"\");\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(xmm6->encoding(), dst->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int24(0x72, (0xC0 | encode), shift & 0xFF);\n+}\n+\n+void Assembler::evpsllq(XMMRegister dst, KRegister mask, XMMRegister src, int shift, bool merge, int vector_len) {\n+  assert(VM_Version::supports_evex(), \"\");\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(xmm6->encoding(), dst->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int24(0x73, (0xC0 | encode), shift & 0xFF);\n+}\n+\n+void Assembler::evpsrlw(XMMRegister dst, KRegister mask, XMMRegister src, int shift, bool merge, int vector_len) {\n+  assert(VM_Version::supports_avx512bw() && (vector_len == AVX_512bit || VM_Version::supports_avx512vl()), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(xmm2->encoding(), dst->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int24(0x71, (0xC0 | encode), shift & 0xFF);\n+}\n+\n+void Assembler::evpsrld(XMMRegister dst, KRegister mask, XMMRegister src, int shift, bool merge, int vector_len) {\n+  assert(VM_Version::supports_evex(), \"\");\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(xmm2->encoding(), dst->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int24(0x72, (0xC0 | encode), shift & 0xFF);\n+}\n+\n+void Assembler::evpsrlq(XMMRegister dst, KRegister mask, XMMRegister src, int shift, bool merge, int vector_len) {\n+  assert(VM_Version::supports_evex(), \"\");\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(xmm2->encoding(), dst->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int24(0x73, (0xC0 | encode), shift & 0xFF);\n+}\n+\n+void Assembler::evpsraw(XMMRegister dst, KRegister mask, XMMRegister src, int shift, bool merge, int vector_len) {\n+  assert(VM_Version::supports_avx512bw() && (vector_len == AVX_512bit || VM_Version::supports_avx512vl()), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(xmm4->encoding(), dst->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int24(0x71, (0xC0 | encode), shift & 0xFF);\n+}\n+\n+void Assembler::evpsrad(XMMRegister dst, KRegister mask, XMMRegister src, int shift, bool merge, int vector_len) {\n+  assert(VM_Version::supports_evex(), \"\");\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(xmm4->encoding(), dst->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int24(0x72, (0xC0 | encode), shift & 0xFF);\n+}\n+\n+void Assembler::evpsraq(XMMRegister dst, KRegister mask, XMMRegister src, int shift, bool merge, int vector_len) {\n+  assert(VM_Version::supports_evex(), \"\");\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(xmm4->encoding(), dst->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int24(0x73, (0xC0 | encode), shift & 0xFF);\n+}\n+\n+void Assembler::evpsllw(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  assert(VM_Version::supports_avx512bw() && (vector_len == AVX_512bit || VM_Version::supports_avx512vl()), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int16((unsigned char)0xF1, (0xC0 | encode));\n+}\n+\n+void Assembler::evpslld(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  assert(VM_Version::supports_evex(), \"\");\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int16((unsigned char)0xF2, (0xC0 | encode));\n+}\n+\n+void Assembler::evpsllq(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  assert(VM_Version::supports_evex(), \"\");\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int16((unsigned char)0xF3, (0xC0 | encode));\n+}\n+\n+void Assembler::evpsrlw(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  assert(VM_Version::supports_avx512bw() && (vector_len == AVX_512bit || VM_Version::supports_avx512vl()), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int16((unsigned char)0xD1, (0xC0 | encode));\n+}\n+\n+void Assembler::evpsrld(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  assert(VM_Version::supports_evex(), \"\");\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int16((unsigned char)0xD2, (0xC0 | encode));\n+}\n+\n+void Assembler::evpsrlq(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  assert(VM_Version::supports_evex(), \"\");\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int16((unsigned char)0xD3, (0xC0 | encode));\n+}\n+\n+void Assembler::evpsraw(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  assert(VM_Version::supports_avx512bw() && (vector_len == AVX_512bit || VM_Version::supports_avx512vl()), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int16((unsigned char)0xE1, (0xC0 | encode));\n+}\n+\n+void Assembler::evpsrad(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  assert(VM_Version::supports_evex(), \"\");\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int16((unsigned char)0xE2, (0xC0 | encode));\n+}\n+\n+void Assembler::evpsraq(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  assert(VM_Version::supports_evex(), \"\");\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int16((unsigned char)0xE2, (0xC0 | encode));\n+}\n+\n+void Assembler::evpsllvw(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  assert(VM_Version::supports_avx512bw() && (vector_len == AVX_512bit || VM_Version::supports_avx512vl()), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int16(0x12, (0xC0 | encode));\n+}\n+\n+void Assembler::evpsllvd(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  assert(VM_Version::supports_evex(), \"\");\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int16(0x47, (0xC0 | encode));\n+}\n+\n+void Assembler::evpsllvq(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  assert(VM_Version::supports_evex(), \"\");\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int16(0x47, (0xC0 | encode));\n+}\n+\n+void Assembler::evpsrlvw(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  assert(VM_Version::supports_avx512bw() && (vector_len == AVX_512bit || VM_Version::supports_avx512vl()), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int16(0x10, (0xC0 | encode));\n+}\n+\n+void Assembler::evpsrlvd(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  assert(VM_Version::supports_evex(), \"\");\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int16(0x45, (0xC0 | encode));\n+}\n+\n+void Assembler::evpsrlvq(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  assert(VM_Version::supports_evex(), \"\");\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int16(0x45, (0xC0 | encode));\n+}\n+\n+void Assembler::evpsravw(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  assert(VM_Version::supports_avx512bw() && (vector_len == AVX_512bit || VM_Version::supports_avx512vl()), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int16(0x11, (0xC0 | encode));\n+}\n+\n+void Assembler::evpsravd(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  assert(VM_Version::supports_evex(), \"\");\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int16(0x46, (0xC0 | encode));\n+}\n+\n+void Assembler::evpsravq(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  assert(VM_Version::supports_evex(), \"\");\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int16(0x46, (0xC0 | encode));\n+}\n+\n+void Assembler::evpminsb(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  assert(VM_Version::supports_avx512bw() && (vector_len == AVX_512bit || VM_Version::supports_avx512vl()), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int16(0x38, (0xC0 | encode));\n+}\n+\n+void Assembler::evpminsb(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len) {\n+  assert(VM_Version::supports_avx512bw(), \"\");\n+  InstructionMark im(this);\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  vex_prefix(src, nds->encoding(), dst->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int8(0x38);\n+  emit_operand(dst, src);\n+}\n+\n+void Assembler::evpminsw(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  assert(VM_Version::supports_avx512bw() && (vector_len == AVX_512bit || VM_Version::supports_avx512vl()), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int16((unsigned char)0xEA, (0xC0 | encode));\n+}\n+\n+void Assembler::evpminsw(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len) {\n+  assert(VM_Version::supports_avx512bw() && (vector_len == AVX_512bit || VM_Version::supports_avx512vl()), \"\");\n+  InstructionMark im(this);\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  vex_prefix(src, nds->encoding(), dst->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int8((unsigned char)0xEA);\n+  emit_operand(dst, src);\n+}\n+\n+void Assembler::evpminsd(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  assert(VM_Version::supports_evex(), \"\");\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int16(0x39, (0xC0 | encode));\n+}\n+\n+void Assembler::evpminsd(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len) {\n+  assert(VM_Version::supports_evex(), \"\");\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionMark im(this);\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  vex_prefix(src, nds->encoding(), dst->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int8(0x39);\n+  emit_operand(dst, src);\n+}\n+\n+void Assembler::evpminsq(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  assert(VM_Version::supports_evex(), \"\");\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int16(0x39, (0xC0 | encode));\n+}\n+\n+void Assembler::evpminsq(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len) {\n+  assert(VM_Version::supports_evex(), \"\");\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionMark im(this);\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  vex_prefix(src, nds->encoding(), dst->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int8(0x39);\n+  emit_operand(dst, src);\n+}\n+\n+\n+void Assembler::evpmaxsb(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  assert(VM_Version::supports_avx512bw() && (vector_len == AVX_512bit || VM_Version::supports_avx512vl()), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int16(0x3C, (0xC0 | encode));\n+}\n+\n+void Assembler::evpmaxsb(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len) {\n+  assert(VM_Version::supports_avx512bw() && (vector_len == AVX_512bit || VM_Version::supports_avx512vl()), \"\");\n+  InstructionMark im(this);\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  vex_prefix(src, nds->encoding(), dst->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int8(0x3C);\n+  emit_operand(dst, src);\n+}\n+\n+void Assembler::evpmaxsw(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  assert(VM_Version::supports_avx512bw() && (vector_len == AVX_512bit || VM_Version::supports_avx512vl()), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int16((unsigned char)0xEE, (0xC0 | encode));\n+}\n+\n+void Assembler::evpmaxsw(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len) {\n+  assert(VM_Version::supports_avx512bw() && (vector_len == AVX_512bit || VM_Version::supports_avx512vl()), \"\");\n+  InstructionMark im(this);\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  vex_prefix(src, nds->encoding(), dst->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int8((unsigned char)0xEE);\n+  emit_operand(dst, src);\n+}\n+\n+void Assembler::evpmaxsd(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  assert(VM_Version::supports_evex(), \"\");\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int16(0x3D, (0xC0 | encode));\n+}\n+\n+void Assembler::evpmaxsd(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len) {\n+  assert(VM_Version::supports_evex(), \"\");\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionMark im(this);\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  vex_prefix(src, nds->encoding(), dst->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int8(0x3D);\n+  emit_operand(dst, src);\n+}\n+\n+void Assembler::evpmaxsq(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  assert(VM_Version::supports_evex(), \"\");\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int16(0x3D, (0xC0 | encode));\n+}\n+\n+void Assembler::evpmaxsq(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len) {\n+  assert(VM_Version::supports_evex(), \"\");\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionMark im(this);\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  vex_prefix(src, nds->encoding(), dst->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int8(0x3D);\n+  emit_operand(dst, src);\n+}\n+\n+\/\/ duplicate 4-byte integer data from src into programmed locations in dest : requires AVX512VL\n+void Assembler::vpbroadcastd(XMMRegister dst, XMMRegister src, int vector_len) {\n+  assert(UseAVX >= 2, \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ true);\n+  int encode = vex_prefix_and_encode(dst->encoding(), 0, src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int16(0x58, (0xC0 | encode));\n+}\n+\n+void Assembler::vpbroadcastd(XMMRegister dst, Address src, int vector_len) {\n+  assert(VM_Version::supports_avx2(), \"\");\n+  assert(dst != xnoreg, \"sanity\");\n+  InstructionMark im(this);\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ true);\n+  attributes.set_address_attributes(\/* tuple_type *\/ EVEX_T1S, \/* input_size_in_bits *\/ EVEX_32bit);\n+  \/\/ swap src<->dst for encoding\n+  vex_prefix(src, 0, dst->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int8(0x58);\n+  emit_operand(dst, src);\n+}\n+\n+\/\/ duplicate 8-byte integer data from src into programmed locations in dest : requires AVX512VL\n+void Assembler::vpbroadcastq(XMMRegister dst, XMMRegister src, int vector_len) {\n+  assert(VM_Version::supports_avx2(), \"\");\n@@ -9370,0 +11075,96 @@\n+void Assembler::evprord(XMMRegister dst, KRegister mask, XMMRegister src, int shift, bool merge, int vector_len) {\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(xmm0->encoding(), dst->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int24(0x72, (0xC0 | encode), shift & 0xFF);\n+}\n+\n+void Assembler::evprorq(XMMRegister dst, KRegister mask, XMMRegister src, int shift, bool merge, int vector_len) {\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(xmm0->encoding(), dst->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int24(0x72, (0xC0 | encode), shift & 0xFF);\n+}\n+\n+void Assembler::evprorvd(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int16(0x14, (0xC0 | encode));\n+}\n+\n+void Assembler::evprorvq(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int16(0x14, (0xC0 | encode));\n+}\n+\n+void Assembler::evprold(XMMRegister dst, KRegister mask, XMMRegister src, int shift, bool merge, int vector_len) {\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(xmm1->encoding(), dst->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int24(0x72, (0xC0 | encode), shift & 0xFF);\n+}\n+\n+void Assembler::evprolq(XMMRegister dst, KRegister mask, XMMRegister src, int shift, bool merge, int vector_len) {\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(xmm1->encoding(), dst->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int24(0x72, (0xC0 | encode), shift & 0xFF);\n+}\n+\n+void Assembler::evprolvd(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int16(0x15, (0xC0 | encode));\n+}\n+\n+void Assembler::evprolvq(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int16(0x15, (0xC0 | encode));\n+}\n+\n@@ -9493,0 +11294,24 @@\n+void Assembler::evpmovq2m(KRegister dst, XMMRegister src, int vector_len) {\n+  assert(VM_Version::supports_avx512vldq(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  int encode = vex_prefix_and_encode(dst->encoding(), 0, src->encoding(), VEX_SIMD_F3, VEX_OPCODE_0F_38, &attributes);\n+  emit_int16(0x39, (0xC0 | encode));\n+}\n+\n+void Assembler::evpmovd2m(KRegister dst, XMMRegister src, int vector_len) {\n+  assert(VM_Version::supports_avx512vldq(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  int encode = vex_prefix_and_encode(dst->encoding(), 0, src->encoding(), VEX_SIMD_F3, VEX_OPCODE_0F_38, &attributes);\n+  emit_int16(0x39, (0xC0 | encode));\n+}\n+\n+void Assembler::evpmovw2m(KRegister dst, XMMRegister src, int vector_len) {\n+  assert(VM_Version::supports_avx512vlbw(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  int encode = vex_prefix_and_encode(dst->encoding(), 0, src->encoding(), VEX_SIMD_F3, VEX_OPCODE_0F_38, &attributes);\n+  emit_int16(0x29, (0xC0 | encode));\n+}\n+\n@@ -9501,0 +11326,31 @@\n+void Assembler::evpmovm2q(XMMRegister dst, KRegister src, int vector_len) {\n+  assert(VM_Version::supports_avx512vldq(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  int encode = vex_prefix_and_encode(dst->encoding(), 0, src->encoding(), VEX_SIMD_F3, VEX_OPCODE_0F_38, &attributes);\n+  emit_int16(0x38, (0xC0 | encode));\n+}\n+\n+void Assembler::evpmovm2d(XMMRegister dst, KRegister src, int vector_len) {\n+  assert(VM_Version::supports_avx512vldq(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  int encode = vex_prefix_and_encode(dst->encoding(), 0, src->encoding(), VEX_SIMD_F3, VEX_OPCODE_0F_38, &attributes);\n+  emit_int16(0x38, (0xC0 | encode));\n+}\n+\n+void Assembler::evpmovm2w(XMMRegister dst, KRegister src, int vector_len) {\n+  assert(VM_Version::supports_avx512vlbw(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  int encode = vex_prefix_and_encode(dst->encoding(), 0, src->encoding(), VEX_SIMD_F3, VEX_OPCODE_0F_38, &attributes);\n+  emit_int16(0x28, (0xC0 | encode));\n+}\n+\n+void Assembler::evpmovm2b(XMMRegister dst, KRegister src, int vector_len) {\n+  assert(VM_Version::supports_avx512vlbw(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  int encode = vex_prefix_and_encode(dst->encoding(), 0, src->encoding(), VEX_SIMD_F3, VEX_OPCODE_0F_38, &attributes);\n+  emit_int16(0x28, (0xC0 | encode));\n+}\n","filename":"src\/hotspot\/cpu\/x86\/assembler_x86.cpp","additions":1896,"deletions":40,"binary":false,"changes":1936,"status":"modified"},{"patch":"@@ -1465,0 +1465,14 @@\n+  void kandbl(KRegister dst, KRegister src1, KRegister src2);\n+  void kandwl(KRegister dst, KRegister src1, KRegister src2);\n+  void kanddl(KRegister dst, KRegister src1, KRegister src2);\n+  void kandql(KRegister dst, KRegister src1, KRegister src2);\n+\n+  void korbl(KRegister dst, KRegister src1, KRegister src2);\n+  void korwl(KRegister dst, KRegister src1, KRegister src2);\n+  void kordl(KRegister dst, KRegister src1, KRegister src2);\n+  void korql(KRegister dst, KRegister src1, KRegister src2);\n+\n+  void kxorbl(KRegister dst, KRegister src1, KRegister src2);\n+  void kxorwl(KRegister dst, KRegister src1, KRegister src2);\n+  void kxordl(KRegister dst, KRegister src1, KRegister src2);\n+  void kxorql(KRegister dst, KRegister src1, KRegister src2);\n@@ -1467,0 +1481,1 @@\n+  void kmovbl(KRegister dst, KRegister src);\n@@ -1480,0 +1495,1 @@\n+  void knotbl(KRegister dst, KRegister src);\n@@ -1481,0 +1497,1 @@\n+  void knotdl(KRegister dst, KRegister src);\n@@ -1488,0 +1505,6 @@\n+  void kxnorbl(KRegister dst, KRegister src1, KRegister src2);\n+  void kshiftlbl(KRegister dst, KRegister src, int imm8);\n+  void kshiftrbl(KRegister dst, KRegister src, int imm8);\n+  void kshiftrwl(KRegister dst, KRegister src, int imm8);\n+  void kshiftrdl(KRegister dst, KRegister src, int imm8);\n+  void kshiftrql(KRegister dst, KRegister src, int imm8);\n@@ -1492,0 +1515,3 @@\n+  void ktestdl(KRegister dst, KRegister src);\n+  void ktestwl(KRegister dst, KRegister src);\n+  void ktestbl(KRegister dst, KRegister src);\n@@ -2155,3 +2181,0 @@\n-  void evpmovd2m(KRegister kdst, XMMRegister src, int vector_len);\n-  void evpmovq2m(KRegister kdst, XMMRegister src, int vector_len);\n-\n@@ -2249,0 +2272,130 @@\n+  \/\/ Leaf level assembler routines for masked operations.\n+  void evpaddb(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evpaddb(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len);\n+  void evpaddw(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evpaddw(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len);\n+  void evpaddd(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evpaddd(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len);\n+  void evpaddq(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evpaddq(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len);\n+  void evaddps(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evaddps(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len);\n+  void evaddpd(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evaddpd(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len);\n+  void evpsubb(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evpsubb(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len);\n+  void evpsubw(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evpsubw(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len);\n+  void evpsubd(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evpsubd(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len);\n+  void evpsubq(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evpsubq(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len);\n+  void evsubps(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evsubps(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len);\n+  void evsubpd(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evsubpd(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len);\n+  void evpmullw(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evpmullw(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len);\n+  void evpmulld(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evpmulld(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len);\n+  void evpmullq(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evpmullq(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len);\n+  void evmulps(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evmulps(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len);\n+  void evmulpd(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evmulpd(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len);\n+  void evdivps(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evdivps(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len);\n+  void evdivpd(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evdivpd(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len);\n+  void evpabsb(XMMRegister dst, KRegister mask, XMMRegister src, bool merge, int vector_len);\n+  void evpabsb(XMMRegister dst, KRegister mask, Address src, bool merge, int vector_len);\n+  void evpabsw(XMMRegister dst, KRegister mask, XMMRegister src, bool merge, int vector_len);\n+  void evpabsw(XMMRegister dst, KRegister mask, Address src, bool merge, int vector_len);\n+  void evpabsd(XMMRegister dst, KRegister mask, XMMRegister src, bool merge, int vector_len);\n+  void evpabsd(XMMRegister dst, KRegister mask, Address src, bool merge, int vector_len);\n+  void evpabsq(XMMRegister dst, KRegister mask, XMMRegister src, bool merge, int vector_len);\n+  void evpabsq(XMMRegister dst, KRegister mask, Address src, bool merge, int vector_len);\n+  void evpfma213ps(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evpfma213ps(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len);\n+  void evpfma213pd(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evpfma213pd(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len);\n+  void evpermb(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evpermb(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len);\n+  void evpermw(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evpermw(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len);\n+  void evpermd(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evpermd(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len);\n+  void evpermq(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evpermq(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len);\n+  void evpsllw(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evpslld(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evpsllq(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evpsrlw(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evpsrld(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evpsrlq(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evpsraw(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evpsrad(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evpsraq(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evsqrtps(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evsqrtps(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len);\n+  void evsqrtpd(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evsqrtpd(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len);\n+\n+  void evpsllw(XMMRegister dst, KRegister mask, XMMRegister src, int shift, bool merge, int vector_len);\n+  void evpslld(XMMRegister dst, KRegister mask, XMMRegister src, int shift, bool merge, int vector_len);\n+  void evpsllq(XMMRegister dst, KRegister mask, XMMRegister src, int shift, bool merge, int vector_len);\n+  void evpsrlw(XMMRegister dst, KRegister mask, XMMRegister src, int shift, bool merge, int vector_len);\n+  void evpsrld(XMMRegister dst, KRegister mask, XMMRegister src, int shift, bool merge, int vector_len);\n+  void evpsrlq(XMMRegister dst, KRegister mask, XMMRegister src, int shift, bool merge, int vector_len);\n+  void evpsraw(XMMRegister dst, KRegister mask, XMMRegister src, int shift, bool merge, int vector_len);\n+  void evpsrad(XMMRegister dst, KRegister mask, XMMRegister src, int shift, bool merge, int vector_len);\n+  void evpsraq(XMMRegister dst, KRegister mask, XMMRegister src, int shift, bool merge, int vector_len);\n+\n+  void evpsllvw(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evpsllvd(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evpsllvq(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evpsrlvw(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evpsrlvd(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evpsrlvq(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evpsravw(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evpsravd(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evpsravq(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evpmaxsb(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evpmaxsw(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evpmaxsd(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evpmaxsq(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evpminsb(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evpminsw(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evpminsd(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evpminsq(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evpmaxsb(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len);\n+  void evpmaxsw(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len);\n+  void evpmaxsd(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len);\n+  void evpmaxsq(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len);\n+  void evpminsb(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len);\n+  void evpminsw(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len);\n+  void evpminsd(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len);\n+  void evpminsq(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len);\n+  void evpord(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evpord(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len);\n+  void evporq(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evporq(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len);\n+  void evpandd(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evpandd(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len);\n+  void evpandq(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evpandq(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len);\n+  void evpxord(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evpxord(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len);\n+  void evpxorq(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evpxorq(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len);\n+\n+  void evprold(XMMRegister dst, KRegister mask, XMMRegister src, int shift, bool merge, int vector_len);\n+  void evprolq(XMMRegister dst, KRegister mask, XMMRegister src, int shift, bool merge, int vector_len);\n+  void evprolvd(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evprolvq(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evprord(XMMRegister dst, KRegister mask, XMMRegister src, int shift, bool merge, int vector_len);\n+  void evprorq(XMMRegister dst, KRegister mask, XMMRegister src, int shift, bool merge, int vector_len);\n+  void evprorvd(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evprorvq(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+\n@@ -2367,1 +2520,0 @@\n-  void evpandd(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n@@ -2380,3 +2532,0 @@\n-  void evpord(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n-  void evpord(XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len);\n-\n@@ -2388,1 +2537,0 @@\n-  void evpxord(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n@@ -2530,0 +2678,7 @@\n+  void evpmovw2m(KRegister dst, XMMRegister src, int vector_len);\n+  void evpmovd2m(KRegister dst, XMMRegister src, int vector_len);\n+  void evpmovq2m(KRegister dst, XMMRegister src, int vector_len);\n+  void evpmovm2b(XMMRegister dst, KRegister src, int vector_len);\n+  void evpmovm2w(XMMRegister dst, KRegister src, int vector_len);\n+  void evpmovm2d(XMMRegister dst, KRegister src, int vector_len);\n+  void evpmovm2q(XMMRegister dst, KRegister src, int vector_len);\n","filename":"src\/hotspot\/cpu\/x86\/assembler_x86.hpp","additions":163,"deletions":8,"binary":false,"changes":171,"status":"modified"},{"patch":"@@ -126,1 +126,1 @@\n-  : _index(index), _array(NULL), _throw_index_out_of_bounds_exception(true) {\n+  : _index(index), _array(), _throw_index_out_of_bounds_exception(true) {\n","filename":"src\/hotspot\/cpu\/x86\/c1_CodeStubs_x86.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -145,3 +145,3 @@\n-LIR_Opr FrameMap::_caller_save_cpu_regs[] = { 0, };\n-LIR_Opr FrameMap::_caller_save_fpu_regs[] = { 0, };\n-LIR_Opr FrameMap::_caller_save_xmm_regs[] = { 0, };\n+LIR_Opr FrameMap::_caller_save_cpu_regs[] = {};\n+LIR_Opr FrameMap::_caller_save_fpu_regs[] = {};\n+LIR_Opr FrameMap::_caller_save_xmm_regs[] = {};\n","filename":"src\/hotspot\/cpu\/x86\/c1_FrameMap_x86.cpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -215,1 +215,1 @@\n-  LIR_Opr r = NULL;\n+  LIR_Opr r;\n","filename":"src\/hotspot\/cpu\/x86\/c1_LIRGenerator_x86.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -31,1 +31,1 @@\n-FloatRegister LIR_OprDesc::as_float_reg() const {\n+FloatRegister LIR_Opr::as_float_reg() const {\n@@ -36,1 +36,1 @@\n-FloatRegister LIR_OprDesc::as_double_reg() const {\n+FloatRegister LIR_Opr::as_double_reg() const {\n@@ -41,1 +41,1 @@\n-XMMRegister LIR_OprDesc::as_xmm_float_reg() const {\n+XMMRegister LIR_Opr::as_xmm_float_reg() const {\n@@ -45,1 +45,1 @@\n-XMMRegister LIR_OprDesc::as_xmm_double_reg() const {\n+XMMRegister LIR_Opr::as_xmm_double_reg() const {\n@@ -53,5 +53,5 @@\n-  return (LIR_Opr)(intptr_t)((reg1 << LIR_OprDesc::reg1_shift) |\n-                             (reg1 << LIR_OprDesc::reg2_shift) |\n-                             LIR_OprDesc::double_type          |\n-                             LIR_OprDesc::fpu_register         |\n-                             LIR_OprDesc::double_size);\n+  return (LIR_Opr)(intptr_t)((reg1 << LIR_Opr::reg1_shift) |\n+                             (reg1 << LIR_Opr::reg2_shift) |\n+                             LIR_Opr::double_type          |\n+                             LIR_Opr::fpu_register         |\n+                             LIR_Opr::double_size);\n","filename":"src\/hotspot\/cpu\/x86\/c1_LIR_x86.cpp","additions":9,"deletions":9,"binary":false,"changes":18,"status":"modified"},{"patch":"@@ -577,1 +577,0 @@\n-  \/\/ Intentional fall-through into DONE_LABEL ...\n@@ -579,0 +578,6 @@\n+  jcc(Assembler::equal, DONE_LABEL);           \/\/ CAS above succeeded; propagate ZF = 1 (success)\n+\n+  cmpptr(r15_thread, rax);                     \/\/ Check if we are already the owner (recursive lock)\n+  jcc(Assembler::notEqual, DONE_LABEL);        \/\/ If not recursive, ZF = 0 at this point (fail)\n+  incq(Address(scrReg, OM_OFFSET_NO_MONITOR_VALUE_TAG(recursions)));\n+  xorq(rax, rax); \/\/ Set ZF = 1 (success) for recursive lock, denoting locking success\n@@ -673,4 +678,0 @@\n-  \/\/ I'd like to add more cases in fast_lock() and fast_unlock() --\n-  \/\/ such as recursive enter and exit -- but we have to be wary of\n-  \/\/ I$ bloat, T$ effects and BP$ effects.\n-  \/\/\n@@ -724,3 +725,10 @@\n-  xorptr(boxReg, boxReg);\n-  orptr(boxReg, Address(tmpReg, OM_OFFSET_NO_MONITOR_VALUE_TAG(recursions)));\n-  jccb  (Assembler::notZero, DONE_LABEL);\n+  Label LNotRecursive, LSuccess, LGoSlowPath;\n+\n+  cmpptr(Address(tmpReg, OM_OFFSET_NO_MONITOR_VALUE_TAG(recursions)), 0);\n+  jccb(Assembler::equal, LNotRecursive);\n+\n+  \/\/ Recursive inflated unlock\n+  decq(Address(tmpReg, OM_OFFSET_NO_MONITOR_VALUE_TAG(recursions)));\n+  jmpb(LSuccess);\n+\n+  bind(LNotRecursive);\n@@ -735,1 +743,0 @@\n-  Label LSuccess, LGoSlowPath ;\n@@ -1464,0 +1471,13 @@\n+void C2_MacroAssembler::load_vector_mask(KRegister dst, XMMRegister src, XMMRegister xtmp,\n+                                         Register tmp, bool novlbwdq, int vlen_enc) {\n+  if (novlbwdq) {\n+    vpmovsxbd(xtmp, src, vlen_enc);\n+    evpcmpd(dst, k0, xtmp, ExternalAddress(StubRoutines::x86::vector_int_mask_cmp_bits()),\n+            Assembler::eq, true, vlen_enc, tmp);\n+  } else {\n+    vpxor(xtmp, xtmp, xtmp, vlen_enc);\n+    vpsubb(xtmp, xtmp, src, vlen_enc);\n+    evpmovb2m(dst, xtmp, vlen_enc);\n+  }\n+}\n+\n@@ -3830,0 +3850,212 @@\n+void C2_MacroAssembler::evmasked_op(int ideal_opc, BasicType eType, KRegister mask, XMMRegister dst,\n+                                    XMMRegister src1, int imm8, bool merge, int vlen_enc) {\n+  switch(ideal_opc) {\n+    case Op_LShiftVS:\n+      Assembler::evpsllw(dst, mask, src1, imm8, merge, vlen_enc); break;\n+    case Op_LShiftVI:\n+      Assembler::evpslld(dst, mask, src1, imm8, merge, vlen_enc); break;\n+    case Op_LShiftVL:\n+      Assembler::evpsllq(dst, mask, src1, imm8, merge, vlen_enc); break;\n+    case Op_RShiftVS:\n+      Assembler::evpsraw(dst, mask, src1, imm8, merge, vlen_enc); break;\n+    case Op_RShiftVI:\n+      Assembler::evpsrad(dst, mask, src1, imm8, merge, vlen_enc); break;\n+    case Op_RShiftVL:\n+      Assembler::evpsraq(dst, mask, src1, imm8, merge, vlen_enc); break;\n+    case Op_URShiftVS:\n+      Assembler::evpsrlw(dst, mask, src1, imm8, merge, vlen_enc); break;\n+    case Op_URShiftVI:\n+      Assembler::evpsrld(dst, mask, src1, imm8, merge, vlen_enc); break;\n+    case Op_URShiftVL:\n+      Assembler::evpsrlq(dst, mask, src1, imm8, merge, vlen_enc); break;\n+    case Op_RotateRightV:\n+      evrord(eType, dst, mask, src1, imm8, merge, vlen_enc); break;\n+    case Op_RotateLeftV:\n+      evrold(eType, dst, mask, src1, imm8, merge, vlen_enc); break;\n+    default:\n+      fatal(\"Unsupported masked operation\"); break;\n+  }\n+}\n+\n+void C2_MacroAssembler::evmasked_op(int ideal_opc, BasicType eType, KRegister mask, XMMRegister dst,\n+                                    XMMRegister src1, XMMRegister src2, bool merge, int vlen_enc,\n+                                    bool is_varshift) {\n+  switch (ideal_opc) {\n+    case Op_AddVB:\n+      evpaddb(dst, mask, src1, src2, merge, vlen_enc); break;\n+    case Op_AddVS:\n+      evpaddw(dst, mask, src1, src2, merge, vlen_enc); break;\n+    case Op_AddVI:\n+      evpaddd(dst, mask, src1, src2, merge, vlen_enc); break;\n+    case Op_AddVL:\n+      evpaddq(dst, mask, src1, src2, merge, vlen_enc); break;\n+    case Op_AddVF:\n+      evaddps(dst, mask, src1, src2, merge, vlen_enc); break;\n+    case Op_AddVD:\n+      evaddpd(dst, mask, src1, src2, merge, vlen_enc); break;\n+    case Op_SubVB:\n+      evpsubb(dst, mask, src1, src2, merge, vlen_enc); break;\n+    case Op_SubVS:\n+      evpsubw(dst, mask, src1, src2, merge, vlen_enc); break;\n+    case Op_SubVI:\n+      evpsubd(dst, mask, src1, src2, merge, vlen_enc); break;\n+    case Op_SubVL:\n+      evpsubq(dst, mask, src1, src2, merge, vlen_enc); break;\n+    case Op_SubVF:\n+      evsubps(dst, mask, src1, src2, merge, vlen_enc); break;\n+    case Op_SubVD:\n+      evsubpd(dst, mask, src1, src2, merge, vlen_enc); break;\n+    case Op_MulVS:\n+      evpmullw(dst, mask, src1, src2, merge, vlen_enc); break;\n+    case Op_MulVI:\n+      evpmulld(dst, mask, src1, src2, merge, vlen_enc); break;\n+    case Op_MulVL:\n+      evpmullq(dst, mask, src1, src2, merge, vlen_enc); break;\n+    case Op_MulVF:\n+      evmulps(dst, mask, src1, src2, merge, vlen_enc); break;\n+    case Op_MulVD:\n+      evmulpd(dst, mask, src1, src2, merge, vlen_enc); break;\n+    case Op_DivVF:\n+      evdivps(dst, mask, src1, src2, merge, vlen_enc); break;\n+    case Op_DivVD:\n+      evdivpd(dst, mask, src1, src2, merge, vlen_enc); break;\n+    case Op_SqrtVF:\n+      evsqrtps(dst, mask, src1, src2, merge, vlen_enc); break;\n+    case Op_SqrtVD:\n+      evsqrtpd(dst, mask, src1, src2, merge, vlen_enc); break;\n+    case Op_AbsVB:\n+      evpabsb(dst, mask, src2, merge, vlen_enc); break;\n+    case Op_AbsVS:\n+      evpabsw(dst, mask, src2, merge, vlen_enc); break;\n+    case Op_AbsVI:\n+      evpabsd(dst, mask, src2, merge, vlen_enc); break;\n+    case Op_AbsVL:\n+      evpabsq(dst, mask, src2, merge, vlen_enc); break;\n+    case Op_FmaVF:\n+      evpfma213ps(dst, mask, src1, src2, merge, vlen_enc); break;\n+    case Op_FmaVD:\n+      evpfma213pd(dst, mask, src1, src2, merge, vlen_enc); break;\n+    case Op_VectorRearrange:\n+      evperm(eType, dst, mask, src2, src1, merge, vlen_enc); break;\n+    case Op_LShiftVS:\n+      evpsllw(dst, mask, src1, src2, merge, vlen_enc, is_varshift); break;\n+    case Op_LShiftVI:\n+      evpslld(dst, mask, src1, src2, merge, vlen_enc, is_varshift); break;\n+    case Op_LShiftVL:\n+      evpsllq(dst, mask, src1, src2, merge, vlen_enc, is_varshift); break;\n+    case Op_RShiftVS:\n+      evpsraw(dst, mask, src1, src2, merge, vlen_enc, is_varshift); break;\n+    case Op_RShiftVI:\n+      evpsrad(dst, mask, src1, src2, merge, vlen_enc, is_varshift); break;\n+    case Op_RShiftVL:\n+      evpsraq(dst, mask, src1, src2, merge, vlen_enc, is_varshift); break;\n+    case Op_URShiftVS:\n+      evpsrlw(dst, mask, src1, src2, merge, vlen_enc, is_varshift); break;\n+    case Op_URShiftVI:\n+      evpsrld(dst, mask, src1, src2, merge, vlen_enc, is_varshift); break;\n+    case Op_URShiftVL:\n+      evpsrlq(dst, mask, src1, src2, merge, vlen_enc, is_varshift); break;\n+    case Op_RotateLeftV:\n+      evrold(eType, dst, mask, src1, src2, merge, vlen_enc); break;\n+    case Op_RotateRightV:\n+      evrord(eType, dst, mask, src1, src2, merge, vlen_enc); break;\n+    case Op_MaxV:\n+      evpmaxs(eType, dst, mask, src1, src2, merge, vlen_enc); break;\n+    case Op_MinV:\n+      evpmins(eType, dst, mask, src1, src2, merge, vlen_enc); break;\n+    case Op_XorV:\n+      evxor(eType, dst, mask, src1, src2, merge, vlen_enc); break;\n+    case Op_OrV:\n+      evor(eType, dst, mask, src1, src2, merge, vlen_enc); break;\n+    case Op_AndV:\n+      evand(eType, dst, mask, src1, src2, merge, vlen_enc); break;\n+    default:\n+      fatal(\"Unsupported masked operation\"); break;\n+  }\n+}\n+\n+void C2_MacroAssembler::evmasked_op(int ideal_opc, BasicType eType, KRegister mask, XMMRegister dst,\n+                                    XMMRegister src1, Address src2, bool merge, int vlen_enc) {\n+  switch (ideal_opc) {\n+    case Op_AddVB:\n+      evpaddb(dst, mask, src1, src2, merge, vlen_enc); break;\n+    case Op_AddVS:\n+      evpaddw(dst, mask, src1, src2, merge, vlen_enc); break;\n+    case Op_AddVI:\n+      evpaddd(dst, mask, src1, src2, merge, vlen_enc); break;\n+    case Op_AddVL:\n+      evpaddq(dst, mask, src1, src2, merge, vlen_enc); break;\n+    case Op_AddVF:\n+      evaddps(dst, mask, src1, src2, merge, vlen_enc); break;\n+    case Op_AddVD:\n+      evaddpd(dst, mask, src1, src2, merge, vlen_enc); break;\n+    case Op_SubVB:\n+      evpsubb(dst, mask, src1, src2, merge, vlen_enc); break;\n+    case Op_SubVS:\n+      evpsubw(dst, mask, src1, src2, merge, vlen_enc); break;\n+    case Op_SubVI:\n+      evpsubd(dst, mask, src1, src2, merge, vlen_enc); break;\n+    case Op_SubVL:\n+      evpsubq(dst, mask, src1, src2, merge, vlen_enc); break;\n+    case Op_SubVF:\n+      evsubps(dst, mask, src1, src2, merge, vlen_enc); break;\n+    case Op_SubVD:\n+      evsubpd(dst, mask, src1, src2, merge, vlen_enc); break;\n+    case Op_MulVS:\n+      evpmullw(dst, mask, src1, src2, merge, vlen_enc); break;\n+    case Op_MulVI:\n+      evpmulld(dst, mask, src1, src2, merge, vlen_enc); break;\n+    case Op_MulVL:\n+      evpmullq(dst, mask, src1, src2, merge, vlen_enc); break;\n+    case Op_MulVF:\n+      evmulps(dst, mask, src1, src2, merge, vlen_enc); break;\n+    case Op_MulVD:\n+      evmulpd(dst, mask, src1, src2, merge, vlen_enc); break;\n+    case Op_DivVF:\n+      evdivps(dst, mask, src1, src2, merge, vlen_enc); break;\n+    case Op_DivVD:\n+      evdivpd(dst, mask, src1, src2, merge, vlen_enc); break;\n+    case Op_FmaVF:\n+      evpfma213ps(dst, mask, src1, src2, merge, vlen_enc); break;\n+    case Op_FmaVD:\n+      evpfma213pd(dst, mask, src1, src2, merge, vlen_enc); break;\n+    case Op_MaxV:\n+      evpmaxs(eType, dst, mask, src1, src2, merge, vlen_enc); break;\n+    case Op_MinV:\n+      evpmins(eType, dst, mask, src1, src2, merge, vlen_enc); break;\n+    case Op_XorV:\n+      evxor(eType, dst, mask, src1, src2, merge, vlen_enc); break;\n+    case Op_OrV:\n+      evor(eType, dst, mask, src1, src2, merge, vlen_enc); break;\n+    case Op_AndV:\n+      evand(eType, dst, mask, src1, src2, merge, vlen_enc); break;\n+    default:\n+      fatal(\"Unsupported masked operation\"); break;\n+  }\n+}\n+\n+void C2_MacroAssembler::masked_op(int ideal_opc, int mask_len, KRegister dst,\n+                                  KRegister src1, KRegister src2) {\n+  BasicType etype = T_ILLEGAL;\n+  switch(mask_len) {\n+    case 2:\n+    case 4:\n+    case 8:  etype = T_BYTE; break;\n+    case 16: etype = T_SHORT; break;\n+    case 32: etype = T_INT; break;\n+    case 64: etype = T_LONG; break;\n+    default: fatal(\"Unsupported type\"); break;\n+  }\n+  assert(etype != T_ILLEGAL, \"\");\n+  switch(ideal_opc) {\n+    case Op_AndVMask:\n+      kand(etype, dst, src1, src2); break;\n+    case Op_OrVMask:\n+      kor(etype, dst, src1, src2); break;\n+    case Op_XorVMask:\n+      kxor(etype, dst, src1, src2); break;\n+    default:\n+      fatal(\"Unsupported masked operation\"); break;\n+  }\n+}\n+\n@@ -3831,7 +4063,12 @@\n-void C2_MacroAssembler::vector_mask_operation(int opc, Register dst, XMMRegister mask, XMMRegister xtmp,\n-                                              Register tmp, KRegister ktmp, int masklen, int vec_enc) {\n-  assert(VM_Version::supports_avx512vlbw(), \"\");\n-  vpxor(xtmp, xtmp, xtmp, vec_enc);\n-  vpsubb(xtmp, xtmp, mask, vec_enc);\n-  evpmovb2m(ktmp, xtmp, vec_enc);\n-  kmovql(tmp, ktmp);\n+void C2_MacroAssembler::vector_mask_operation(int opc, Register dst, KRegister mask,\n+                                              Register tmp, int masklen, int masksize,\n+                                              int vec_enc) {\n+  if(VM_Version::supports_avx512bw()) {\n+    kmovql(tmp, mask);\n+  } else {\n+    assert(masklen <= 16, \"\");\n+    kmovwl(tmp, mask);\n+  }\n+  if (masksize < 16) {\n+    andq(tmp, (((jlong)1 << masklen) - 1));\n+  }\n@@ -3857,1 +4094,2 @@\n-                                              XMMRegister xtmp1, Register tmp, int masklen, int vec_enc) {\n+                                              XMMRegister xtmp1, Register tmp, int masklen, int masksize,\n+                                              int vec_enc) {\n@@ -3862,1 +4100,1 @@\n-  if (masklen < 64) {\n+  if (masksize < 16) {\n","filename":"src\/hotspot\/cpu\/x86\/c2_MacroAssembler_x86.cpp","additions":256,"deletions":18,"binary":false,"changes":274,"status":"modified"},{"patch":"@@ -145,0 +145,2 @@\n+  void load_vector_mask(KRegister dst, XMMRegister src, XMMRegister xtmp, Register tmp, bool novlbwdq, int vlen_enc);\n+\n@@ -225,2 +227,1 @@\n-  void vector_mask_operation(int opc, Register dst, XMMRegister mask, XMMRegister xtmp, Register tmp,\n-                             KRegister ktmp, int masklen, int vec_enc);\n+  void vector_mask_operation(int opc, Register dst, KRegister mask, Register tmp, int masklen, int masksize, int vec_enc);\n@@ -229,1 +230,1 @@\n-                             Register tmp, int masklen, int vec_enc);\n+                             Register tmp, int masklen, int masksize, int vec_enc);\n@@ -276,0 +277,14 @@\n+\n+  void evmasked_op(int ideal_opc, BasicType eType, KRegister mask,\n+                   XMMRegister dst, XMMRegister src1, XMMRegister src2,\n+                   bool merge, int vlen_enc, bool is_varshift = false);\n+\n+  void evmasked_op(int ideal_opc, BasicType eType, KRegister mask,\n+                   XMMRegister dst, XMMRegister src1, Address src2,\n+                   bool merge, int vlen_enc);\n+\n+  void evmasked_op(int ideal_opc, BasicType eType, KRegister mask, XMMRegister dst,\n+                   XMMRegister src1, int imm8, bool merge, int vlen_enc);\n+\n+  void masked_op(int ideal_opc, int mask_len, KRegister dst,\n+                 KRegister src1, KRegister src2);\n","filename":"src\/hotspot\/cpu\/x86\/c2_MacroAssembler_x86.hpp","additions":18,"deletions":3,"binary":false,"changes":21,"status":"modified"},{"patch":"@@ -37,2 +37,1 @@\n-class LIR_OprDesc;\n-typedef LIR_OprDesc* LIR_Opr;\n+class LIR_Opr;\n","filename":"src\/hotspot\/cpu\/x86\/gc\/z\/zBarrierSetAssembler_x86.hpp","additions":1,"deletions":2,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -158,0 +158,1 @@\n+  NativeCall* call = NULL;\n@@ -160,1 +161,1 @@\n-      break;\n+      return;\n@@ -165,1 +166,1 @@\n-      NativeCall* call = nativeCall_at(_instructions->start() + pc_offset);\n+      call = nativeCall_at(_instructions->start() + pc_offset);\n@@ -175,1 +176,1 @@\n-      NativeCall* call = nativeCall_at(_instructions->start() + pc_offset);\n+      call = nativeCall_at(_instructions->start() + pc_offset);\n@@ -183,1 +184,1 @@\n-      NativeCall* call = nativeCall_at(_instructions->start() + pc_offset);\n+      call = nativeCall_at(_instructions->start() + pc_offset);\n@@ -190,2 +191,5 @@\n-      JVMCI_ERROR(\"invalid _next_call_type value\");\n-      break;\n+      JVMCI_ERROR(\"invalid _next_call_type value: %d\", _next_call_type);\n+      return;\n+  }\n+  if (!call->is_displacement_aligned()) {\n+    JVMCI_ERROR(\"unaligned displacement for call at offset %d\", pc_offset);\n","filename":"src\/hotspot\/cpu\/x86\/jvmciCodeInstaller_x86.cpp","additions":10,"deletions":6,"binary":false,"changes":16,"status":"modified"},{"patch":"@@ -4789,2 +4789,0 @@\n-\/\/ !!! If the instructions that get generated here change then function\n-\/\/ instr_size_for_decode_klass_not_null() needs to get updated.\n@@ -8270,0 +8268,373 @@\n+void MacroAssembler::knot(uint masklen, KRegister dst, KRegister src, KRegister ktmp, Register rtmp) {\n+  switch(masklen) {\n+    case 2:\n+       knotbl(dst, src);\n+       movl(rtmp, 3);\n+       kmovbl(ktmp, rtmp);\n+       kandbl(dst, ktmp, dst);\n+       break;\n+    case 4:\n+       knotbl(dst, src);\n+       movl(rtmp, 15);\n+       kmovbl(ktmp, rtmp);\n+       kandbl(dst, ktmp, dst);\n+       break;\n+    case 8:\n+       knotbl(dst, src);\n+       break;\n+    case 16:\n+       knotwl(dst, src);\n+       break;\n+    case 32:\n+       knotdl(dst, src);\n+       break;\n+    case 64:\n+       knotql(dst, src);\n+       break;\n+    default:\n+      fatal(\"Unexpected vector length %d\", masklen);\n+      break;\n+  }\n+}\n+\n+void MacroAssembler::kand(BasicType type, KRegister dst, KRegister src1, KRegister src2) {\n+  switch(type) {\n+    case T_BOOLEAN:\n+    case T_BYTE:\n+       kandbl(dst, src1, src2);\n+       break;\n+    case T_CHAR:\n+    case T_SHORT:\n+       kandwl(dst, src1, src2);\n+       break;\n+    case T_INT:\n+    case T_FLOAT:\n+       kanddl(dst, src1, src2);\n+       break;\n+    case T_LONG:\n+    case T_DOUBLE:\n+       kandql(dst, src1, src2);\n+       break;\n+    default:\n+      fatal(\"Unexpected type argument %s\", type2name(type));\n+      break;\n+  }\n+}\n+\n+void MacroAssembler::kor(BasicType type, KRegister dst, KRegister src1, KRegister src2) {\n+  switch(type) {\n+    case T_BOOLEAN:\n+    case T_BYTE:\n+       korbl(dst, src1, src2);\n+       break;\n+    case T_CHAR:\n+    case T_SHORT:\n+       korwl(dst, src1, src2);\n+       break;\n+    case T_INT:\n+    case T_FLOAT:\n+       kordl(dst, src1, src2);\n+       break;\n+    case T_LONG:\n+    case T_DOUBLE:\n+       korql(dst, src1, src2);\n+       break;\n+    default:\n+      fatal(\"Unexpected type argument %s\", type2name(type));\n+      break;\n+  }\n+}\n+\n+void MacroAssembler::kxor(BasicType type, KRegister dst, KRegister src1, KRegister src2) {\n+  switch(type) {\n+    case T_BOOLEAN:\n+    case T_BYTE:\n+       kxorbl(dst, src1, src2);\n+       break;\n+    case T_CHAR:\n+    case T_SHORT:\n+       kxorwl(dst, src1, src2);\n+       break;\n+    case T_INT:\n+    case T_FLOAT:\n+       kxordl(dst, src1, src2);\n+       break;\n+    case T_LONG:\n+    case T_DOUBLE:\n+       kxorql(dst, src1, src2);\n+       break;\n+    default:\n+      fatal(\"Unexpected type argument %s\", type2name(type));\n+      break;\n+  }\n+}\n+\n+void MacroAssembler::evperm(BasicType type, XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  switch(type) {\n+    case T_BOOLEAN:\n+    case T_BYTE:\n+      evpermb(dst, mask, nds, src, merge, vector_len); break;\n+    case T_CHAR:\n+    case T_SHORT:\n+      evpermw(dst, mask, nds, src, merge, vector_len); break;\n+    case T_INT:\n+    case T_FLOAT:\n+      evpermd(dst, mask, nds, src, merge, vector_len); break;\n+    case T_LONG:\n+    case T_DOUBLE:\n+      evpermq(dst, mask, nds, src, merge, vector_len); break;\n+    default:\n+      fatal(\"Unexpected type argument %s\", type2name(type)); break;\n+  }\n+}\n+\n+void MacroAssembler::evperm(BasicType type, XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len) {\n+  switch(type) {\n+    case T_BOOLEAN:\n+    case T_BYTE:\n+      evpermb(dst, mask, nds, src, merge, vector_len); break;\n+    case T_CHAR:\n+    case T_SHORT:\n+      evpermw(dst, mask, nds, src, merge, vector_len); break;\n+    case T_INT:\n+    case T_FLOAT:\n+      evpermd(dst, mask, nds, src, merge, vector_len); break;\n+    case T_LONG:\n+    case T_DOUBLE:\n+      evpermq(dst, mask, nds, src, merge, vector_len); break;\n+    default:\n+      fatal(\"Unexpected type argument %s\", type2name(type)); break;\n+  }\n+}\n+\n+void MacroAssembler::evpmins(BasicType type, XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len) {\n+  switch(type) {\n+    case T_BYTE:\n+      evpminsb(dst, mask, nds, src, merge, vector_len); break;\n+    case T_SHORT:\n+      evpminsw(dst, mask, nds, src, merge, vector_len); break;\n+    case T_INT:\n+      evpminsd(dst, mask, nds, src, merge, vector_len); break;\n+    case T_LONG:\n+      evpminsq(dst, mask, nds, src, merge, vector_len); break;\n+    default:\n+      fatal(\"Unexpected type argument %s\", type2name(type)); break;\n+  }\n+}\n+\n+void MacroAssembler::evpmaxs(BasicType type, XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len) {\n+  switch(type) {\n+    case T_BYTE:\n+      evpmaxsb(dst, mask, nds, src, merge, vector_len); break;\n+    case T_SHORT:\n+      evpmaxsw(dst, mask, nds, src, merge, vector_len); break;\n+    case T_INT:\n+      evpmaxsd(dst, mask, nds, src, merge, vector_len); break;\n+    case T_LONG:\n+      evpmaxsq(dst, mask, nds, src, merge, vector_len); break;\n+    default:\n+      fatal(\"Unexpected type argument %s\", type2name(type)); break;\n+  }\n+}\n+\n+void MacroAssembler::evpmins(BasicType type, XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  switch(type) {\n+    case T_BYTE:\n+      evpminsb(dst, mask, nds, src, merge, vector_len); break;\n+    case T_SHORT:\n+      evpminsw(dst, mask, nds, src, merge, vector_len); break;\n+    case T_INT:\n+      evpminsd(dst, mask, nds, src, merge, vector_len); break;\n+    case T_LONG:\n+      evpminsq(dst, mask, nds, src, merge, vector_len); break;\n+    default:\n+      fatal(\"Unexpected type argument %s\", type2name(type)); break;\n+  }\n+}\n+\n+void MacroAssembler::evpmaxs(BasicType type, XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  switch(type) {\n+    case T_BYTE:\n+      evpmaxsb(dst, mask, nds, src, merge, vector_len); break;\n+    case T_SHORT:\n+      evpmaxsw(dst, mask, nds, src, merge, vector_len); break;\n+    case T_INT:\n+      evpmaxsd(dst, mask, nds, src, merge, vector_len); break;\n+    case T_LONG:\n+      evpmaxsq(dst, mask, nds, src, merge, vector_len); break;\n+    default:\n+      fatal(\"Unexpected type argument %s\", type2name(type)); break;\n+  }\n+}\n+\n+void MacroAssembler::evxor(BasicType type, XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  switch(type) {\n+    case T_INT:\n+      evpxord(dst, mask, nds, src, merge, vector_len); break;\n+    case T_LONG:\n+      evpxorq(dst, mask, nds, src, merge, vector_len); break;\n+    default:\n+      fatal(\"Unexpected type argument %s\", type2name(type)); break;\n+  }\n+}\n+\n+void MacroAssembler::evxor(BasicType type, XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len) {\n+  switch(type) {\n+    case T_INT:\n+      evpxord(dst, mask, nds, src, merge, vector_len); break;\n+    case T_LONG:\n+      evpxorq(dst, mask, nds, src, merge, vector_len); break;\n+    default:\n+      fatal(\"Unexpected type argument %s\", type2name(type)); break;\n+  }\n+}\n+\n+void MacroAssembler::evor(BasicType type, XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  switch(type) {\n+    case T_INT:\n+      Assembler::evpord(dst, mask, nds, src, merge, vector_len); break;\n+    case T_LONG:\n+      evporq(dst, mask, nds, src, merge, vector_len); break;\n+    default:\n+      fatal(\"Unexpected type argument %s\", type2name(type)); break;\n+  }\n+}\n+\n+void MacroAssembler::evor(BasicType type, XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len) {\n+  switch(type) {\n+    case T_INT:\n+      Assembler::evpord(dst, mask, nds, src, merge, vector_len); break;\n+    case T_LONG:\n+      evporq(dst, mask, nds, src, merge, vector_len); break;\n+    default:\n+      fatal(\"Unexpected type argument %s\", type2name(type)); break;\n+  }\n+}\n+\n+void MacroAssembler::evand(BasicType type, XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  switch(type) {\n+    case T_INT:\n+      evpandd(dst, mask, nds, src, merge, vector_len); break;\n+    case T_LONG:\n+      evpandq(dst, mask, nds, src, merge, vector_len); break;\n+    default:\n+      fatal(\"Unexpected type argument %s\", type2name(type)); break;\n+  }\n+}\n+\n+void MacroAssembler::evand(BasicType type, XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len) {\n+  switch(type) {\n+    case T_INT:\n+      evpandd(dst, mask, nds, src, merge, vector_len); break;\n+    case T_LONG:\n+      evpandq(dst, mask, nds, src, merge, vector_len); break;\n+    default:\n+      fatal(\"Unexpected type argument %s\", type2name(type)); break;\n+  }\n+}\n+\n+void MacroAssembler::anytrue(Register dst, uint masklen, KRegister src1, KRegister src2) {\n+   masklen = masklen < 8 ? 8 : masklen;\n+   ktest(masklen, src1, src2);\n+   setb(Assembler::notZero, dst);\n+   movzbl(dst, dst);\n+}\n+\n+void MacroAssembler::alltrue(Register dst, uint masklen, KRegister src1, KRegister src2, KRegister kscratch) {\n+  if (masklen < 8) {\n+    knotbl(kscratch, src2);\n+    kortestbl(src1, kscratch);\n+    setb(Assembler::carrySet, dst);\n+    movzbl(dst, dst);\n+  } else {\n+    ktest(masklen, src1, src2);\n+    setb(Assembler::carrySet, dst);\n+    movzbl(dst, dst);\n+  }\n+}\n+\n+void MacroAssembler::kortest(uint masklen, KRegister src1, KRegister src2) {\n+  switch(masklen) {\n+    case 8:\n+       kortestbl(src1, src2);\n+       break;\n+    case 16:\n+       kortestwl(src1, src2);\n+       break;\n+    case 32:\n+       kortestdl(src1, src2);\n+       break;\n+    case 64:\n+       kortestql(src1, src2);\n+       break;\n+    default:\n+      fatal(\"Unexpected mask length %d\", masklen);\n+      break;\n+  }\n+}\n+\n+\n+void MacroAssembler::ktest(uint masklen, KRegister src1, KRegister src2) {\n+  switch(masklen)  {\n+    case 8:\n+       ktestbl(src1, src2);\n+       break;\n+    case 16:\n+       ktestwl(src1, src2);\n+       break;\n+    case 32:\n+       ktestdl(src1, src2);\n+       break;\n+    case 64:\n+       ktestql(src1, src2);\n+       break;\n+    default:\n+      fatal(\"Unexpected mask length %d\", masklen);\n+      break;\n+  }\n+}\n+\n+void MacroAssembler::evrold(BasicType type, XMMRegister dst, KRegister mask, XMMRegister src, int shift, bool merge, int vlen_enc) {\n+  switch(type) {\n+    case T_INT:\n+      evprold(dst, mask, src, shift, merge, vlen_enc); break;\n+    case T_LONG:\n+      evprolq(dst, mask, src, shift, merge, vlen_enc); break;\n+    default:\n+      fatal(\"Unexpected type argument %s\", type2name(type)); break;\n+      break;\n+  }\n+}\n+\n+void MacroAssembler::evrord(BasicType type, XMMRegister dst, KRegister mask, XMMRegister src, int shift, bool merge, int vlen_enc) {\n+  switch(type) {\n+    case T_INT:\n+      evprord(dst, mask, src, shift, merge, vlen_enc); break;\n+    case T_LONG:\n+      evprorq(dst, mask, src, shift, merge, vlen_enc); break;\n+    default:\n+      fatal(\"Unexpected type argument %s\", type2name(type)); break;\n+  }\n+}\n+\n+void MacroAssembler::evrold(BasicType type, XMMRegister dst, KRegister mask, XMMRegister src1, XMMRegister src2, bool merge, int vlen_enc) {\n+  switch(type) {\n+    case T_INT:\n+      evprolvd(dst, mask, src1, src2, merge, vlen_enc); break;\n+    case T_LONG:\n+      evprolvq(dst, mask, src1, src2, merge, vlen_enc); break;\n+    default:\n+      fatal(\"Unexpected type argument %s\", type2name(type)); break;\n+  }\n+}\n+\n+void MacroAssembler::evrord(BasicType type, XMMRegister dst, KRegister mask, XMMRegister src1, XMMRegister src2, bool merge, int vlen_enc) {\n+  switch(type) {\n+    case T_INT:\n+      evprorvd(dst, mask, src1, src2, merge, vlen_enc); break;\n+    case T_LONG:\n+      evprorvq(dst, mask, src1, src2, merge, vlen_enc); break;\n+    default:\n+      fatal(\"Unexpected type argument %s\", type2name(type)); break;\n+  }\n+}\n","filename":"src\/hotspot\/cpu\/x86\/macroAssembler_x86.cpp","additions":373,"deletions":2,"binary":false,"changes":375,"status":"modified"},{"patch":"@@ -1341,0 +1341,69 @@\n+  void evpsllw(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len, bool is_varshift) {\n+    if (!is_varshift) {\n+      Assembler::evpsllw(dst, mask, nds, src, merge, vector_len);\n+    } else {\n+      Assembler::evpsllvw(dst, mask, nds, src, merge, vector_len);\n+    }\n+  }\n+  void evpslld(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len, bool is_varshift) {\n+    if (!is_varshift) {\n+      Assembler::evpslld(dst, mask, nds, src, merge, vector_len);\n+    } else {\n+      Assembler::evpsllvd(dst, mask, nds, src, merge, vector_len);\n+    }\n+  }\n+  void evpsllq(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len, bool is_varshift) {\n+    if (!is_varshift) {\n+      Assembler::evpsllq(dst, mask, nds, src, merge, vector_len);\n+    } else {\n+      Assembler::evpsllvq(dst, mask, nds, src, merge, vector_len);\n+    }\n+  }\n+  void evpsrlw(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len, bool is_varshift) {\n+    if (!is_varshift) {\n+      Assembler::evpsrlw(dst, mask, nds, src, merge, vector_len);\n+    } else {\n+      Assembler::evpsrlvw(dst, mask, nds, src, merge, vector_len);\n+    }\n+  }\n+  void evpsrld(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len, bool is_varshift) {\n+    if (!is_varshift) {\n+      Assembler::evpsrld(dst, mask, nds, src, merge, vector_len);\n+    } else {\n+      Assembler::evpsrlvd(dst, mask, nds, src, merge, vector_len);\n+    }\n+  }\n+  void evpsrlq(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len, bool is_varshift) {\n+    if (!is_varshift) {\n+      Assembler::evpsrlq(dst, mask, nds, src, merge, vector_len);\n+    } else {\n+      Assembler::evpsrlvq(dst, mask, nds, src, merge, vector_len);\n+    }\n+  }\n+  void evpsraw(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len, bool is_varshift) {\n+    if (!is_varshift) {\n+      Assembler::evpsraw(dst, mask, nds, src, merge, vector_len);\n+    } else {\n+      Assembler::evpsravw(dst, mask, nds, src, merge, vector_len);\n+    }\n+  }\n+  void evpsrad(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len, bool is_varshift) {\n+    if (!is_varshift) {\n+      Assembler::evpsrad(dst, mask, nds, src, merge, vector_len);\n+    } else {\n+      Assembler::evpsravd(dst, mask, nds, src, merge, vector_len);\n+    }\n+  }\n+  void evpsraq(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len, bool is_varshift) {\n+    if (!is_varshift) {\n+      Assembler::evpsraq(dst, mask, nds, src, merge, vector_len);\n+    } else {\n+      Assembler::evpsravq(dst, mask, nds, src, merge, vector_len);\n+    }\n+  }\n+\n+  void evpmins(BasicType type, XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evpmaxs(BasicType type, XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evpmins(BasicType type, XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len);\n+  void evpmaxs(BasicType type, XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len);\n+\n@@ -1630,1 +1699,27 @@\n-  \/\/ Data\n+  \/\/ AVX-512 mask operations.\n+  void kand(BasicType etype, KRegister dst, KRegister src1, KRegister src2);\n+  void kor(BasicType type, KRegister dst, KRegister src1, KRegister src2);\n+  void knot(uint masklen, KRegister dst, KRegister src, KRegister ktmp = knoreg, Register rtmp = noreg);\n+  void kxor(BasicType type, KRegister dst, KRegister src1, KRegister src2);\n+  void kortest(uint masklen, KRegister src1, KRegister src2);\n+  void ktest(uint masklen, KRegister src1, KRegister src2);\n+\n+  void evperm(BasicType type, XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evperm(BasicType type, XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len);\n+\n+  void evor(BasicType type, XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evor(BasicType type, XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len);\n+\n+  void evand(BasicType type, XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evand(BasicType type, XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len);\n+\n+  void evxor(BasicType type, XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n+  void evxor(BasicType type, XMMRegister dst, KRegister mask, XMMRegister nds, Address src, bool merge, int vector_len);\n+\n+  void evrold(BasicType type, XMMRegister dst, KRegister mask, XMMRegister src, int shift, bool merge, int vlen_enc);\n+  void evrold(BasicType type, XMMRegister dst, KRegister mask, XMMRegister src1, XMMRegister src2, bool merge, int vlen_enc);\n+  void evrord(BasicType type, XMMRegister dst, KRegister mask, XMMRegister src, int shift, bool merge, int vlen_enc);\n+  void evrord(BasicType type, XMMRegister dst, KRegister mask, XMMRegister src1, XMMRegister src2, bool merge, int vlen_enc);\n+\n+  void alltrue(Register dst, uint masklen, KRegister src1, KRegister src2, KRegister kscratch);\n+  void anytrue(Register dst, uint masklen, KRegister src, KRegister kscratch);\n","filename":"src\/hotspot\/cpu\/x86\/macroAssembler_x86.hpp","additions":96,"deletions":1,"binary":false,"changes":97,"status":"modified"},{"patch":"@@ -263,0 +263,3 @@\n+bool NativeCall::is_displacement_aligned() {\n+  return (uintptr_t) displacement_address() % 4 == 0;\n+}\n@@ -285,2 +288,1 @@\n-  bool is_aligned = ((uintptr_t)displacement_address() + 0) \/ cache_line_size ==\n-                    ((uintptr_t)displacement_address() + 3) \/ cache_line_size;\n+  bool is_aligned = is_displacement_aligned();\n","filename":"src\/hotspot\/cpu\/x86\/nativeInst_x86.cpp","additions":4,"deletions":2,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -163,2 +163,0 @@\n-  enum { cache_line_size = BytesPerWord };  \/\/ conservative estimate!\n-\n@@ -178,0 +176,2 @@\n+  \/\/ Returns whether the 4-byte displacement operand is 4-byte aligned.\n+  bool  is_displacement_aligned();\n@@ -180,1 +180,1 @@\n-  void  verify_alignment() { assert((intptr_t)addr_at(displacement_offset) % BytesPerInt == 0, \"must be aligned\"); }\n+  void  verify_alignment() { assert(is_displacement_aligned(), \"displacement of call is not aligned\"); }\n","filename":"src\/hotspot\/cpu\/x86\/nativeInst_x86.hpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -1239,34 +1239,0 @@\n-\/\/ Unpack an array argument into a pointer to the body and the length\n-\/\/ if the array is non-null, otherwise pass 0 for both.\n-static void unpack_array_argument(MacroAssembler* masm, VMRegPair reg, BasicType in_elem_type, VMRegPair body_arg, VMRegPair length_arg) {\n-  Register tmp_reg = rax;\n-  assert(!body_arg.first()->is_Register() || body_arg.first()->as_Register() != tmp_reg,\n-         \"possible collision\");\n-  assert(!length_arg.first()->is_Register() || length_arg.first()->as_Register() != tmp_reg,\n-         \"possible collision\");\n-\n-  \/\/ Pass the length, ptr pair\n-  Label is_null, done;\n-  VMRegPair tmp(tmp_reg->as_VMReg());\n-  if (reg.first()->is_stack()) {\n-    \/\/ Load the arg up from the stack\n-    simple_move32(masm, reg, tmp);\n-    reg = tmp;\n-  }\n-  __ testptr(reg.first()->as_Register(), reg.first()->as_Register());\n-  __ jccb(Assembler::equal, is_null);\n-  __ lea(tmp_reg, Address(reg.first()->as_Register(), arrayOopDesc::base_offset_in_bytes(in_elem_type)));\n-  simple_move32(masm, tmp, body_arg);\n-  \/\/ load the length relative to the body.\n-  __ movl(tmp_reg, Address(tmp_reg, arrayOopDesc::length_offset_in_bytes() -\n-                           arrayOopDesc::base_offset_in_bytes(in_elem_type)));\n-  simple_move32(masm, tmp, length_arg);\n-  __ jmpb(done);\n-  __ bind(is_null);\n-  \/\/ Pass zeros\n-  __ xorptr(tmp_reg, tmp_reg);\n-  simple_move32(masm, tmp, body_arg);\n-  simple_move32(masm, tmp, length_arg);\n-  __ bind(done);\n-}\n-\n@@ -1375,2 +1341,1 @@\n-                                                BasicType ret_type,\n-                                                address critical_entry) {\n+                                                BasicType ret_type) {\n@@ -1398,6 +1363,1 @@\n-  bool is_critical_native = true;\n-  address native_func = critical_entry;\n-  if (native_func == NULL) {\n-    native_func = method->native_function();\n-    is_critical_native = false;\n-  }\n+  address native_func = method->native_function();\n@@ -1416,13 +1376,1 @@\n-  int total_c_args = total_in_args;\n-  if (!is_critical_native) {\n-    total_c_args += 1;\n-    if (method->is_static()) {\n-      total_c_args++;\n-    }\n-  } else {\n-    for (int i = 0; i < total_in_args; i++) {\n-      if (in_sig_bt[i] == T_ARRAY) {\n-        total_c_args++;\n-      }\n-    }\n-  }\n+  int  total_c_args       = total_in_args + (method->is_static() ? 2 : 1);\n@@ -1435,5 +1383,4 @@\n-  if (!is_critical_native) {\n-    out_sig_bt[argc++] = T_ADDRESS;\n-    if (method->is_static()) {\n-      out_sig_bt[argc++] = T_OBJECT;\n-    }\n+  out_sig_bt[argc++] = T_ADDRESS;\n+  if (method->is_static()) {\n+    out_sig_bt[argc++] = T_OBJECT;\n+  }\n@@ -1441,24 +1388,2 @@\n-    for (int i = 0; i < total_in_args ; i++ ) {\n-      out_sig_bt[argc++] = in_sig_bt[i];\n-    }\n-  } else {\n-    in_elem_bt = NEW_RESOURCE_ARRAY(BasicType, total_in_args);\n-    SignatureStream ss(method->signature());\n-    for (int i = 0; i < total_in_args ; i++ ) {\n-      if (in_sig_bt[i] == T_ARRAY) {\n-        \/\/ Arrays are passed as int, elem* pair\n-        out_sig_bt[argc++] = T_INT;\n-        out_sig_bt[argc++] = T_ADDRESS;\n-        ss.skip_array_prefix(1);  \/\/ skip one '['\n-        assert(ss.is_primitive(), \"primitive type expected\");\n-        in_elem_bt[i] = ss.type();\n-      } else {\n-        out_sig_bt[argc++] = in_sig_bt[i];\n-        in_elem_bt[i] = T_VOID;\n-      }\n-      if (in_sig_bt[i] != T_VOID) {\n-        assert(in_sig_bt[i] == ss.type() ||\n-               in_sig_bt[i] == T_ARRAY, \"must match\");\n-        ss.next();\n-      }\n-    }\n+  for (int i = 0; i < total_in_args ; i++ ) {\n+    out_sig_bt[argc++] = in_sig_bt[i];\n@@ -1482,34 +1407,0 @@\n-  if (is_critical_native) {\n-    \/\/ Critical natives may have to call out so they need a save area\n-    \/\/ for register arguments.\n-    int double_slots = 0;\n-    int single_slots = 0;\n-    for ( int i = 0; i < total_in_args; i++) {\n-      if (in_regs[i].first()->is_Register()) {\n-        const Register reg = in_regs[i].first()->as_Register();\n-        switch (in_sig_bt[i]) {\n-          case T_ARRAY:  \/\/ critical array (uses 2 slots on LP64)\n-          case T_BOOLEAN:\n-          case T_BYTE:\n-          case T_SHORT:\n-          case T_CHAR:\n-          case T_INT:  single_slots++; break;\n-          case T_LONG: double_slots++; break;\n-          default:  ShouldNotReachHere();\n-        }\n-      } else if (in_regs[i].first()->is_XMMRegister()) {\n-        switch (in_sig_bt[i]) {\n-          case T_FLOAT:  single_slots++; break;\n-          case T_DOUBLE: double_slots++; break;\n-          default:  ShouldNotReachHere();\n-        }\n-      } else if (in_regs[i].first()->is_FloatRegister()) {\n-        ShouldNotReachHere();\n-      }\n-    }\n-    total_save_slots = double_slots * 2 + single_slots;\n-    \/\/ align the save area\n-    if (double_slots != 0) {\n-      stack_slots = align_up(stack_slots, 2);\n-    }\n-  }\n@@ -1694,1 +1585,1 @@\n-  int c_arg = is_critical_native ? 0 : (method->is_static() ? 2 : 1 );\n+  int c_arg = method->is_static() ? 2 : 1;\n@@ -1717,6 +1608,0 @@\n-        if (is_critical_native) {\n-          VMRegPair in_arg = in_regs[i];\n-          unpack_array_argument(masm, in_arg, in_elem_bt[i], out_regs[c_arg + 1], out_regs[c_arg]);\n-          c_arg++;\n-          break;\n-        }\n@@ -1724,1 +1609,0 @@\n-        assert(!is_critical_native, \"no oop arguments\");\n@@ -1756,1 +1640,1 @@\n-  if (method->is_static() && !is_critical_native) {\n+  if (method->is_static()) {\n@@ -1811,2 +1695,0 @@\n-    assert(!is_critical_native, \"unhandled\");\n-\n@@ -1864,3 +1746,2 @@\n-  if (!is_critical_native) {\n-    __ lea(rdx, Address(thread, in_bytes(JavaThread::jni_environment_offset())));\n-    __ movptr(Address(rsp, 0), rdx);\n+  __ lea(rdx, Address(thread, in_bytes(JavaThread::jni_environment_offset())));\n+  __ movptr(Address(rsp, 0), rdx);\n@@ -1868,3 +1749,2 @@\n-    \/\/ Now set thread in native\n-    __ movl(Address(thread, JavaThread::thread_state_offset()), _thread_in_native);\n-  }\n+  \/\/ Now set thread in native\n+  __ movl(Address(thread, JavaThread::thread_state_offset()), _thread_in_native);\n@@ -1903,11 +1783,0 @@\n-  \/\/ If this is a critical native, check for a safepoint or suspend request after the call.\n-  \/\/ If a safepoint is needed, transition to native, then to native_trans to handle\n-  \/\/ safepoints like the native methods that are not critical natives.\n-  if (is_critical_native) {\n-    Label needs_safepoint;\n-    __ safepoint_poll(needs_safepoint, thread, false \/* at_return *\/, false \/* in_nmethod *\/);\n-    __ cmpl(Address(thread, JavaThread::suspend_flags_offset()), 0);\n-    __ jcc(Assembler::equal, after_transition);\n-    __ bind(needs_safepoint);\n-  }\n-\n@@ -2046,4 +1915,3 @@\n-  if (!is_critical_native) {\n-    \/\/ reset handle block\n-    __ movptr(rcx, Address(thread, JavaThread::active_handles_offset()));\n-    __ movl(Address(rcx, JNIHandleBlock::top_offset_in_bytes()), NULL_WORD);\n+  \/\/ reset handle block\n+  __ movptr(rcx, Address(thread, JavaThread::active_handles_offset()));\n+  __ movl(Address(rcx, JNIHandleBlock::top_offset_in_bytes()), NULL_WORD);\n@@ -2051,4 +1919,3 @@\n-    \/\/ Any exception pending?\n-    __ cmpptr(Address(thread, in_bytes(Thread::pending_exception_offset())), (int32_t)NULL_WORD);\n-    __ jcc(Assembler::notEqual, exception_pending);\n-  }\n+  \/\/ Any exception pending?\n+  __ cmpptr(Address(thread, in_bytes(Thread::pending_exception_offset())), (int32_t)NULL_WORD);\n+  __ jcc(Assembler::notEqual, exception_pending);\n@@ -2168,3 +2035,2 @@\n-  if (!is_critical_native) {\n-    \/\/ Forward  the exception\n-    __ bind(exception_pending);\n+  \/\/ Forward  the exception\n+  __ bind(exception_pending);\n@@ -2172,2 +2038,2 @@\n-    \/\/ remove possible return value from FPU register stack\n-    __ empty_FPU_stack();\n+  \/\/ remove possible return value from FPU register stack\n+  __ empty_FPU_stack();\n@@ -2175,5 +2041,4 @@\n-    \/\/ pop our frame\n-    __ leave();\n-    \/\/ and forward the exception\n-    __ jump(RuntimeAddress(StubRoutines::forward_exception_entry()));\n-  }\n+  \/\/ pop our frame\n+  __ leave();\n+  \/\/ and forward the exception\n+  __ jump(RuntimeAddress(StubRoutines::forward_exception_entry()));\n","filename":"src\/hotspot\/cpu\/x86\/sharedRuntime_x86_32.cpp","additions":29,"deletions":164,"binary":false,"changes":193,"status":"modified"},{"patch":"@@ -1238,40 +1238,0 @@\n-\/\/ Unpack an array argument into a pointer to the body and the length\n-\/\/ if the array is non-null, otherwise pass 0 for both.\n-static void unpack_array_argument(MacroAssembler* masm, VMRegPair reg, BasicType in_elem_type, VMRegPair body_arg, VMRegPair length_arg) {\n-  Register tmp_reg = rax;\n-  assert(!body_arg.first()->is_Register() || body_arg.first()->as_Register() != tmp_reg,\n-         \"possible collision\");\n-  assert(!length_arg.first()->is_Register() || length_arg.first()->as_Register() != tmp_reg,\n-         \"possible collision\");\n-\n-  __ block_comment(\"unpack_array_argument {\");\n-\n-  \/\/ Pass the length, ptr pair\n-  Label is_null, done;\n-  VMRegPair tmp;\n-  tmp.set_ptr(tmp_reg->as_VMReg());\n-  if (reg.first()->is_stack()) {\n-    \/\/ Load the arg up from the stack\n-    __ move_ptr(reg, tmp);\n-    reg = tmp;\n-  }\n-  __ testptr(reg.first()->as_Register(), reg.first()->as_Register());\n-  __ jccb(Assembler::equal, is_null);\n-  __ lea(tmp_reg, Address(reg.first()->as_Register(), arrayOopDesc::base_offset_in_bytes(in_elem_type)));\n-  __ move_ptr(tmp, body_arg);\n-  \/\/ load the length relative to the body.\n-  __ movl(tmp_reg, Address(tmp_reg, arrayOopDesc::length_offset_in_bytes() -\n-                           arrayOopDesc::base_offset_in_bytes(in_elem_type)));\n-  __ move32_64(tmp, length_arg);\n-  __ jmpb(done);\n-  __ bind(is_null);\n-  \/\/ Pass zeros\n-  __ xorptr(tmp_reg, tmp_reg);\n-  __ move_ptr(tmp, body_arg);\n-  __ move32_64(tmp, length_arg);\n-  __ bind(done);\n-\n-  __ block_comment(\"} unpack_array_argument\");\n-}\n-\n-\n@@ -1553,2 +1513,1 @@\n-                                                BasicType ret_type,\n-                                                address critical_entry) {\n+                                                BasicType ret_type) {\n@@ -1576,6 +1535,1 @@\n-  bool is_critical_native = true;\n-  address native_func = critical_entry;\n-  if (native_func == NULL) {\n-    native_func = method->native_function();\n-    is_critical_native = false;\n-  }\n+  address native_func = method->native_function();\n@@ -1595,13 +1549,1 @@\n-  int total_c_args = total_in_args;\n-  if (!is_critical_native) {\n-    total_c_args += 1;\n-    if (method->is_static()) {\n-      total_c_args++;\n-    }\n-  } else {\n-    for (int i = 0; i < total_in_args; i++) {\n-      if (in_sig_bt[i] == T_ARRAY) {\n-        total_c_args++;\n-      }\n-    }\n-  }\n+  int total_c_args = total_in_args + (method->is_static() ? 2 : 1);\n@@ -1614,5 +1556,4 @@\n-  if (!is_critical_native) {\n-    out_sig_bt[argc++] = T_ADDRESS;\n-    if (method->is_static()) {\n-      out_sig_bt[argc++] = T_OBJECT;\n-    }\n+  out_sig_bt[argc++] = T_ADDRESS;\n+  if (method->is_static()) {\n+    out_sig_bt[argc++] = T_OBJECT;\n+  }\n@@ -1620,24 +1561,2 @@\n-    for (int i = 0; i < total_in_args ; i++ ) {\n-      out_sig_bt[argc++] = in_sig_bt[i];\n-    }\n-  } else {\n-    in_elem_bt = NEW_RESOURCE_ARRAY(BasicType, total_in_args);\n-    SignatureStream ss(method->signature());\n-    for (int i = 0; i < total_in_args ; i++ ) {\n-      if (in_sig_bt[i] == T_ARRAY) {\n-        \/\/ Arrays are passed as int, elem* pair\n-        out_sig_bt[argc++] = T_INT;\n-        out_sig_bt[argc++] = T_ADDRESS;\n-        ss.skip_array_prefix(1);  \/\/ skip one '['\n-        assert(ss.is_primitive(), \"primitive type expected\");\n-        in_elem_bt[i] = ss.type();\n-      } else {\n-        out_sig_bt[argc++] = in_sig_bt[i];\n-        in_elem_bt[i] = T_VOID;\n-      }\n-      if (in_sig_bt[i] != T_VOID) {\n-        assert(in_sig_bt[i] == ss.type() ||\n-               in_sig_bt[i] == T_ARRAY, \"must match\");\n-        ss.next();\n-      }\n-    }\n+  for (int i = 0; i < total_in_args ; i++ ) {\n+    out_sig_bt[argc++] = in_sig_bt[i];\n@@ -1661,34 +1580,0 @@\n-  if (is_critical_native) {\n-    \/\/ Critical natives may have to call out so they need a save area\n-    \/\/ for register arguments.\n-    int double_slots = 0;\n-    int single_slots = 0;\n-    for ( int i = 0; i < total_in_args; i++) {\n-      if (in_regs[i].first()->is_Register()) {\n-        const Register reg = in_regs[i].first()->as_Register();\n-        switch (in_sig_bt[i]) {\n-          case T_BOOLEAN:\n-          case T_BYTE:\n-          case T_SHORT:\n-          case T_CHAR:\n-          case T_INT:  single_slots++; break;\n-          case T_ARRAY:  \/\/ specific to LP64 (7145024)\n-          case T_LONG: double_slots++; break;\n-          default:  ShouldNotReachHere();\n-        }\n-      } else if (in_regs[i].first()->is_XMMRegister()) {\n-        switch (in_sig_bt[i]) {\n-          case T_FLOAT:  single_slots++; break;\n-          case T_DOUBLE: double_slots++; break;\n-          default:  ShouldNotReachHere();\n-        }\n-      } else if (in_regs[i].first()->is_FloatRegister()) {\n-        ShouldNotReachHere();\n-      }\n-    }\n-    total_save_slots = double_slots * 2 + single_slots;\n-    \/\/ align the save area\n-    if (double_slots != 0) {\n-      stack_slots = align_up(stack_slots, 2);\n-    }\n-  }\n@@ -1889,4 +1774,1 @@\n-  \/\/ This may iterate in two different directions depending on the\n-  \/\/ kind of native it is.  The reason is that for regular JNI natives\n-  \/\/ the incoming and outgoing registers are offset upwards and for\n-  \/\/ critical natives they are offset down.\n+  \/\/ For JNI natives the incoming and outgoing registers are offset upwards.\n@@ -1898,8 +1780,3 @@\n-  if (!is_critical_native) {\n-    for (int i = total_in_args - 1, c_arg = total_c_args - 1; i >= 0; i--, c_arg--) {\n-      arg_order.push(i);\n-      arg_order.push(c_arg);\n-    }\n-  } else {\n-    \/\/ Compute a valid move order, using tmp_vmreg to break any cycles\n-    ComputeMoveOrder cmo(total_in_args, in_regs, total_c_args, out_regs, in_sig_bt, arg_order, tmp_vmreg);\n+  for (int i = total_in_args - 1, c_arg = total_c_args - 1; i >= 0; i--, c_arg--) {\n+    arg_order.push(i);\n+    arg_order.push(c_arg);\n@@ -1913,14 +1790,0 @@\n-    if (c_arg == -1) {\n-      assert(is_critical_native, \"should only be required for critical natives\");\n-      \/\/ This arg needs to be moved to a temporary\n-      __ mov(tmp_vmreg.first()->as_Register(), in_regs[i].first()->as_Register());\n-      in_regs[i] = tmp_vmreg;\n-      temploc = i;\n-      continue;\n-    } else if (i == -1) {\n-      assert(is_critical_native, \"should only be required for critical natives\");\n-      \/\/ Read from the temporary location\n-      assert(temploc != -1, \"must be valid\");\n-      i = temploc;\n-      temploc = -1;\n-    }\n@@ -1941,12 +1804,0 @@\n-        if (is_critical_native) {\n-          unpack_array_argument(masm, in_regs[i], in_elem_bt[i], out_regs[c_arg + 1], out_regs[c_arg]);\n-          c_arg++;\n-#ifdef ASSERT\n-          if (out_regs[c_arg].first()->is_Register()) {\n-            reg_destroyed[out_regs[c_arg].first()->as_Register()->encoding()] = true;\n-          } else if (out_regs[c_arg].first()->is_XMMRegister()) {\n-            freg_destroyed[out_regs[c_arg].first()->as_XMMRegister()->encoding()] = true;\n-          }\n-#endif\n-          break;\n-        }\n@@ -1954,1 +1805,0 @@\n-        assert(!is_critical_native, \"no oop arguments\");\n@@ -1988,24 +1838,19 @@\n-  if (!is_critical_native) {\n-    \/\/ point c_arg at the first arg that is already loaded in case we\n-    \/\/ need to spill before we call out\n-    c_arg = total_c_args - total_in_args;\n-\n-    if (method->is_static()) {\n-\n-      \/\/  load oop into a register\n-      __ movoop(oop_handle_reg, JNIHandles::make_local(method->method_holder()->java_mirror()));\n-\n-      \/\/ Now handlize the static class mirror it's known not-null.\n-      __ movptr(Address(rsp, klass_offset), oop_handle_reg);\n-      map->set_oop(VMRegImpl::stack2reg(klass_slot_offset));\n-\n-      \/\/ Now get the handle\n-      __ lea(oop_handle_reg, Address(rsp, klass_offset));\n-      \/\/ store the klass handle as second argument\n-      __ movptr(c_rarg1, oop_handle_reg);\n-      \/\/ and protect the arg if we must spill\n-      c_arg--;\n-    }\n-  } else {\n-    \/\/ For JNI critical methods we need to save all registers in save_args.\n-    c_arg = 0;\n+  \/\/ point c_arg at the first arg that is already loaded in case we\n+  \/\/ need to spill before we call out\n+  c_arg = total_c_args - total_in_args;\n+\n+  if (method->is_static()) {\n+\n+    \/\/  load oop into a register\n+    __ movoop(oop_handle_reg, JNIHandles::make_local(method->method_holder()->java_mirror()));\n+\n+    \/\/ Now handlize the static class mirror it's known not-null.\n+    __ movptr(Address(rsp, klass_offset), oop_handle_reg);\n+    map->set_oop(VMRegImpl::stack2reg(klass_slot_offset));\n+\n+    \/\/ Now get the handle\n+    __ lea(oop_handle_reg, Address(rsp, klass_offset));\n+    \/\/ store the klass handle as second argument\n+    __ movptr(c_rarg1, oop_handle_reg);\n+    \/\/ and protect the arg if we must spill\n+    c_arg--;\n@@ -2063,2 +1908,0 @@\n-    assert(!is_critical_native, \"unhandled\");\n-\n@@ -2118,2 +1961,1 @@\n-  if (!is_critical_native) {\n-    __ lea(c_rarg0, Address(r15_thread, in_bytes(JavaThread::jni_environment_offset())));\n+  __ lea(c_rarg0, Address(r15_thread, in_bytes(JavaThread::jni_environment_offset())));\n@@ -2121,3 +1963,2 @@\n-    \/\/ Now set thread in native\n-    __ movl(Address(r15_thread, JavaThread::thread_state_offset()), _thread_in_native);\n-  }\n+  \/\/ Now set thread in native\n+  __ movl(Address(r15_thread, JavaThread::thread_state_offset()), _thread_in_native);\n@@ -2151,11 +1992,0 @@\n-  \/\/ If this is a critical native, check for a safepoint or suspend request after the call.\n-  \/\/ If a safepoint is needed, transition to native, then to native_trans to handle\n-  \/\/ safepoints like the native methods that are not critical natives.\n-  if (is_critical_native) {\n-    Label needs_safepoint;\n-    __ safepoint_poll(needs_safepoint, r15_thread, false \/* at_return *\/, false \/* in_nmethod *\/);\n-    __ cmpl(Address(r15_thread, JavaThread::suspend_flags_offset()), 0);\n-    __ jcc(Assembler::equal, after_transition);\n-    __ bind(needs_safepoint);\n-  }\n-\n@@ -2282,5 +2112,3 @@\n-  if (!is_critical_native) {\n-    \/\/ reset handle block\n-    __ movptr(rcx, Address(r15_thread, JavaThread::active_handles_offset()));\n-    __ movl(Address(rcx, JNIHandleBlock::top_offset_in_bytes()), (int32_t)NULL_WORD);\n-  }\n+  \/\/ reset handle block\n+  __ movptr(rcx, Address(r15_thread, JavaThread::active_handles_offset()));\n+  __ movl(Address(rcx, JNIHandleBlock::top_offset_in_bytes()), (int32_t)NULL_WORD);\n@@ -2292,5 +2120,3 @@\n-  if (!is_critical_native) {\n-    \/\/ Any exception pending?\n-    __ cmpptr(Address(r15_thread, in_bytes(Thread::pending_exception_offset())), (int32_t)NULL_WORD);\n-    __ jcc(Assembler::notEqual, exception_pending);\n-  }\n+  \/\/ Any exception pending?\n+  __ cmpptr(Address(r15_thread, in_bytes(Thread::pending_exception_offset())), (int32_t)NULL_WORD);\n+  __ jcc(Assembler::notEqual, exception_pending);\n@@ -2304,3 +2130,2 @@\n-  if (!is_critical_native) {\n-    \/\/ forward the exception\n-    __ bind(exception_pending);\n+  \/\/ forward the exception\n+  __ bind(exception_pending);\n@@ -2308,3 +2133,2 @@\n-    \/\/ and forward the exception\n-    __ jump(RuntimeAddress(StubRoutines::forward_exception_entry()));\n-  }\n+  \/\/ and forward the exception\n+  __ jump(RuntimeAddress(StubRoutines::forward_exception_entry()));\n","filename":"src\/hotspot\/cpu\/x86\/sharedRuntime_x86_64.cpp","additions":45,"deletions":221,"binary":false,"changes":266,"status":"modified"},{"patch":"@@ -4004,0 +4004,1 @@\n+    StubRoutines::x86::_vector_int_mask_cmp_bits = generate_vector_mask(\"vector_int_mask_cmp_bits\", 0x00000001);\n","filename":"src\/hotspot\/cpu\/x86\/stubGenerator_x86_32.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -7679,0 +7679,1 @@\n+    StubRoutines::x86::_vector_int_mask_cmp_bits = generate_vector_mask(\"vector_int_mask_cmp_bits\", 0x0000000100000001);\n","filename":"src\/hotspot\/cpu\/x86\/stubGenerator_x86_64.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -51,0 +51,1 @@\n+address StubRoutines::x86::_vector_int_mask_cmp_bits = NULL;\n","filename":"src\/hotspot\/cpu\/x86\/stubRoutines_x86.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -168,0 +168,1 @@\n+  static address _vector_int_mask_cmp_bits;\n@@ -292,0 +293,4 @@\n+  static address vector_int_mask_cmp_bits() {\n+    return _vector_int_mask_cmp_bits;\n+  }\n+\n","filename":"src\/hotspot\/cpu\/x86\/stubRoutines_x86.hpp","additions":5,"deletions":0,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -887,0 +887,1 @@\n+  static bool supports_avx512bwdq()   { return (supports_evex() && supports_avx512bw() && supports_avx512dq()); }\n","filename":"src\/hotspot\/cpu\/x86\/vm_version_x86.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -1377,0 +1377,1 @@\n+  static address vector_int_mask_cmp_bits() { return StubRoutines::x86::vector_int_mask_cmp_bits(); }\n@@ -1559,0 +1560,1 @@\n+    case Op_VectorMaskToLong:\n@@ -1805,0 +1807,2 @@\n+    case Op_LoadVectorGatherMasked:\n+    case Op_StoreVectorScatterMasked:\n@@ -1806,1 +1810,1 @@\n-      if(bt == T_BYTE || bt == T_SHORT) {\n+      if(is_subword_type(bt)) {\n@@ -1817,0 +1821,11 @@\n+    case Op_MaskAll:\n+      if (!is_LP64 || !VM_Version::supports_evex()) {\n+        return false;\n+      }\n+      if ((vlen > 16 || is_subword_type(bt)) && !VM_Version::supports_avx512bw()) {\n+        return false;\n+      }\n+      if (size_in_bits < 512 && !VM_Version::supports_avx512vl()) {\n+        return false;\n+      }\n+      break;\n@@ -1826,0 +1841,142 @@\n+const bool Matcher::match_rule_supported_vector_masked(int opcode, int vlen, BasicType bt) {\n+  \/\/ ADLC based match_rule_supported routine checks for the existence of pattern based\n+  \/\/ on IR opcode. Most of the unary\/binary\/ternary masked operation share the IR nodes\n+  \/\/ of their non-masked counterpart with mask edge being the differentiator.\n+  \/\/ This routine does a strict check on the existence of masked operation patterns\n+  \/\/ by returning a default false value for all the other opcodes apart from the\n+  \/\/ ones whose masked instruction patterns are defined in this file.\n+  if (!match_rule_supported_vector(opcode, vlen, bt)) {\n+    return false;\n+  }\n+\n+  const bool is_LP64 = LP64_ONLY(true) NOT_LP64(false);\n+  int size_in_bits = vlen * type2aelembytes(bt) * BitsPerByte;\n+  if (size_in_bits != 512 && !VM_Version::supports_avx512vl()) {\n+    return false;\n+  }\n+  switch(opcode) {\n+    \/\/ Unary masked operations\n+    case Op_AbsVB:\n+    case Op_AbsVS:\n+      if(!VM_Version::supports_avx512bw()) {\n+        return false;  \/\/ Implementation limitation\n+      }\n+    case Op_AbsVI:\n+    case Op_AbsVL:\n+      return true;\n+\n+    \/\/ Ternary masked operations\n+    case Op_FmaVF:\n+    case Op_FmaVD:\n+      return true;\n+\n+    \/\/ Binary masked operations\n+    case Op_AddVB:\n+    case Op_AddVS:\n+    case Op_SubVB:\n+    case Op_SubVS:\n+    case Op_MulVS:\n+    case Op_LShiftVS:\n+    case Op_RShiftVS:\n+    case Op_URShiftVS:\n+      assert(size_in_bits == 512 || VM_Version::supports_avx512vl(), \"\");\n+      if (!VM_Version::supports_avx512bw()) {\n+        return false;  \/\/ Implementation limitation\n+      }\n+      return true;\n+\n+    case Op_MulVL:\n+      assert(size_in_bits == 512 || VM_Version::supports_avx512vl(), \"\");\n+      if (!VM_Version::supports_avx512dq()) {\n+        return false;  \/\/ Implementation limitation\n+      }\n+      return true;\n+\n+    case Op_AndV:\n+    case Op_OrV:\n+    case Op_XorV:\n+    case Op_RotateRightV:\n+    case Op_RotateLeftV:\n+      if (bt != T_INT && bt != T_LONG) {\n+        return false; \/\/ Implementation limitation\n+      }\n+      return true;\n+\n+    case Op_VectorLoadMask:\n+      assert(size_in_bits == 512 || VM_Version::supports_avx512vl(), \"\");\n+      if (is_subword_type(bt) && !VM_Version::supports_avx512bw()) {\n+        return false;\n+      }\n+      return true;\n+\n+    case Op_AddVI:\n+    case Op_AddVL:\n+    case Op_AddVF:\n+    case Op_AddVD:\n+    case Op_SubVI:\n+    case Op_SubVL:\n+    case Op_SubVF:\n+    case Op_SubVD:\n+    case Op_MulVI:\n+    case Op_MulVF:\n+    case Op_MulVD:\n+    case Op_DivVF:\n+    case Op_DivVD:\n+    case Op_SqrtVF:\n+    case Op_SqrtVD:\n+    case Op_LShiftVI:\n+    case Op_LShiftVL:\n+    case Op_RShiftVI:\n+    case Op_RShiftVL:\n+    case Op_URShiftVI:\n+    case Op_URShiftVL:\n+    case Op_LoadVectorMasked:\n+    case Op_StoreVectorMasked:\n+    case Op_LoadVectorGatherMasked:\n+    case Op_StoreVectorScatterMasked:\n+      return true;\n+\n+    case Op_MaxV:\n+    case Op_MinV:\n+      if (is_subword_type(bt) && !VM_Version::supports_avx512bw()) {\n+        return false; \/\/ Implementation limitation\n+      }\n+      if (is_floating_point_type(bt)) {\n+        return false; \/\/ Implementation limitation\n+      }\n+      return true;\n+\n+    case Op_VectorMaskCmp:\n+      if (is_subword_type(bt) && !VM_Version::supports_avx512bw()) {\n+        return false; \/\/ Implementation limitation\n+      }\n+      return true;\n+\n+    case Op_VectorRearrange:\n+      if (bt == T_SHORT && !VM_Version::supports_avx512bw()) {\n+        return false; \/\/ Implementation limitation\n+      }\n+      if (bt == T_BYTE && !VM_Version::supports_avx512_vbmi()) {\n+        return false; \/\/ Implementation limitation\n+      } else if ((bt == T_INT || bt == T_FLOAT) && size_in_bits < 256) {\n+        return false; \/\/ Implementation limitation\n+      }\n+      return true;\n+\n+    \/\/ Binary Logical operations\n+    case Op_AndVMask:\n+    case Op_OrVMask:\n+    case Op_XorVMask:\n+      if (vlen > 16 && !VM_Version::supports_avx512bw()) {\n+        return false; \/\/ Implementation limitation\n+      }\n+      return true;\n+\n+    case Op_MaskAll:\n+      return true;\n+\n+    default:\n+      return false;\n+  }\n+}\n+\n@@ -1890,1 +2047,1 @@\n-  return new TypeVectMask(TypeInt::BOOL, length);\n+  return new TypeVectMask(elemTy, length);\n@@ -3313,0 +3470,1 @@\n+\n@@ -3314,0 +3472,73 @@\n+instruct reinterpret_mask(kReg dst) %{\n+  predicate(n->bottom_type()->isa_vectmask() &&\n+            Matcher::vector_length(n) == Matcher::vector_length(n->in(1))); \/\/ dst == src\n+  match(Set dst (VectorReinterpret dst));\n+  ins_cost(125);\n+  format %{ \"vector_reinterpret $dst\\t!\" %}\n+  ins_encode %{\n+    \/\/ empty\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct reinterpret_mask_W2B(kReg dst, kReg src, vec xtmp) %{\n+  predicate(UseAVX > 2 && Matcher::vector_length(n) != Matcher::vector_length(n->in(1)) &&\n+            n->bottom_type()->isa_vectmask() &&\n+            n->in(1)->bottom_type()->isa_vectmask() &&\n+            n->in(1)->bottom_type()->is_vectmask()->element_basic_type() == T_SHORT &&\n+            n->bottom_type()->is_vectmask()->element_basic_type() == T_BYTE); \/\/ dst == src\n+  match(Set dst (VectorReinterpret src));\n+  effect(TEMP xtmp);\n+  format %{ \"vector_mask_reinterpret_W2B $dst $src\\t!\" %}\n+  ins_encode %{\n+     int src_sz = Matcher::vector_length(this, $src)*type2aelembytes(T_SHORT);\n+     int dst_sz = Matcher::vector_length(this)*type2aelembytes(T_BYTE);\n+     assert(src_sz == dst_sz , \"src and dst size mismatch\");\n+     int vlen_enc = vector_length_encoding(src_sz);\n+     __  evpmovm2w($xtmp$$XMMRegister, $src$$KRegister, vlen_enc);\n+     __  evpmovb2m($dst$$KRegister, $xtmp$$XMMRegister, vlen_enc);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct reinterpret_mask_D2B(kReg dst, kReg src, vec xtmp) %{\n+  predicate(UseAVX > 2 && Matcher::vector_length(n) != Matcher::vector_length(n->in(1)) &&\n+            n->bottom_type()->isa_vectmask() &&\n+            n->in(1)->bottom_type()->isa_vectmask() &&\n+            (n->in(1)->bottom_type()->is_vectmask()->element_basic_type() == T_INT ||\n+             n->in(1)->bottom_type()->is_vectmask()->element_basic_type() == T_FLOAT) &&\n+            n->bottom_type()->is_vectmask()->element_basic_type() == T_BYTE); \/\/ dst == src\n+  match(Set dst (VectorReinterpret src));\n+  effect(TEMP xtmp);\n+  format %{ \"vector_mask_reinterpret_D2B $dst $src\\t!\" %}\n+  ins_encode %{\n+     int src_sz = Matcher::vector_length(this, $src)*type2aelembytes(T_INT);\n+     int dst_sz = Matcher::vector_length(this)*type2aelembytes(T_BYTE);\n+     assert(src_sz == dst_sz , \"src and dst size mismatch\");\n+     int vlen_enc = vector_length_encoding(src_sz);\n+     __  evpmovm2d($xtmp$$XMMRegister, $src$$KRegister, vlen_enc);\n+     __  evpmovb2m($dst$$KRegister, $xtmp$$XMMRegister, vlen_enc);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct reinterpret_mask_Q2B(kReg dst, kReg src, vec xtmp) %{\n+  predicate(UseAVX > 2 && Matcher::vector_length(n) != Matcher::vector_length(n->in(1)) &&\n+            n->bottom_type()->isa_vectmask() &&\n+            n->in(1)->bottom_type()->isa_vectmask() &&\n+            (n->in(1)->bottom_type()->is_vectmask()->element_basic_type() == T_LONG ||\n+             n->in(1)->bottom_type()->is_vectmask()->element_basic_type() == T_DOUBLE) &&\n+            n->bottom_type()->is_vectmask()->element_basic_type() == T_BYTE); \/\/ dst == src\n+  match(Set dst (VectorReinterpret src));\n+  effect(TEMP xtmp);\n+  format %{ \"vector_mask_reinterpret_Q2B $dst $src\\t!\" %}\n+  ins_encode %{\n+     int src_sz = Matcher::vector_length(this, $src)*type2aelembytes(T_LONG);\n+     int dst_sz = Matcher::vector_length(this)*type2aelembytes(T_BYTE);\n+     assert(src_sz == dst_sz , \"src and dst size mismatch\");\n+     int vlen_enc = vector_length_encoding(src_sz);\n+     __  evpmovm2q($xtmp$$XMMRegister, $src$$KRegister, vlen_enc);\n+     __  evpmovb2m($dst$$KRegister, $xtmp$$XMMRegister, vlen_enc);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n@@ -3316,1 +3547,2 @@\n-  predicate(Matcher::vector_length_in_bytes(n) == Matcher::vector_length_in_bytes(n->in(1))); \/\/ dst == src\n+  predicate(!n->bottom_type()->isa_vectmask() &&\n+            Matcher::vector_length_in_bytes(n) == Matcher::vector_length_in_bytes(n->in(1))); \/\/ dst == src\n@@ -3351,0 +3583,1 @@\n+            !n->bottom_type()->isa_vectmask() &&\n@@ -3366,0 +3599,1 @@\n+            !n->bottom_type()->isa_vectmask() &&\n@@ -3383,1 +3617,2 @@\n-  predicate(Matcher::vector_length_in_bytes(n->in(1)) > Matcher::vector_length_in_bytes(n)); \/\/ src > dst\n+  predicate(!n->bottom_type()->isa_vectmask() &&\n+            Matcher::vector_length_in_bytes(n->in(1)) > Matcher::vector_length_in_bytes(n)); \/\/ src > dst\n@@ -3585,1 +3820,1 @@\n-  predicate(Matcher::vector_length_in_bytes(n) <= 32);\n+  predicate(!VM_Version::supports_avx512vl() && Matcher::vector_length_in_bytes(n) <= 32);\n@@ -3610,1 +3845,1 @@\n-  predicate(Matcher::vector_length_in_bytes(n) == 64);\n+  predicate(VM_Version::supports_avx512vl() || Matcher::vector_length_in_bytes(n) == 64);\n@@ -3613,1 +3848,1 @@\n-  format %{ \"load_vector_gather $dst, $mem, $idx\\t! using $tmp and k2 as TEMP\" %}\n+  format %{ \"load_vector_gather $dst, $mem, $idx\\t! using $tmp and ktmp as TEMP\" %}\n@@ -3629,0 +3864,18 @@\n+instruct evgather_masked(vec dst, memory mem, vec idx, kReg mask, kReg ktmp, rRegP tmp) %{\n+  match(Set dst (LoadVectorGatherMasked mem (Binary idx mask)));\n+  effect(TEMP_DEF dst, TEMP tmp, TEMP ktmp);\n+  format %{ \"load_vector_gather_masked $dst, $mem, $idx, $mask\\t! using $tmp and ktmp as TEMP\" %}\n+  ins_encode %{\n+    assert(UseAVX > 2, \"sanity\");\n+    int vlen_enc = vector_length_encoding(this);\n+    BasicType elem_bt = Matcher::vector_element_basic_type(this);\n+    assert(!is_subword_type(elem_bt), \"sanity\"); \/\/ T_INT, T_LONG, T_FLOAT, T_DOUBLE\n+    \/\/ Note: Since gather instruction partially updates the opmask register used\n+    \/\/ for predication hense moving mask operand to a temporary.\n+    __ kmovwl($ktmp$$KRegister, $mask$$KRegister);\n+    __ vpxor($dst$$XMMRegister, $dst$$XMMRegister, $dst$$XMMRegister, vlen_enc);\n+    __ lea($tmp$$Register, $mem$$Address);\n+    __ evgather(elem_bt, $dst$$XMMRegister, $ktmp$$KRegister, $tmp$$Register, $idx$$XMMRegister, vlen_enc);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n@@ -3652,0 +3905,18 @@\n+instruct scatter_masked(memory mem, vec src, vec idx, kReg mask, kReg ktmp, rRegP tmp) %{\n+  match(Set mem (StoreVectorScatterMasked mem (Binary src (Binary idx mask))));\n+  effect(TEMP tmp, TEMP ktmp);\n+  format %{ \"store_vector_scatter_masked $mem, $idx, $src, $mask\\t!\" %}\n+  ins_encode %{\n+    int vlen_enc = vector_length_encoding(this, $src);\n+    BasicType elem_bt = Matcher::vector_element_basic_type(this, $src);\n+    assert(Matcher::vector_length_in_bytes(this, $src) >= 16, \"sanity\");\n+    assert(!is_subword_type(elem_bt), \"sanity\"); \/\/ T_INT, T_LONG, T_FLOAT, T_DOUBLE\n+    \/\/ Note: Since scatter instruction partially updates the opmask register used\n+    \/\/ for predication hense moving mask operand to a temporary.\n+    __ kmovwl($ktmp$$KRegister, $mask$$KRegister);\n+    __ lea($tmp$$Register, $mem$$Address);\n+    __ evscatter(elem_bt, $tmp$$Register, $idx$$XMMRegister, $ktmp$$KRegister, $src$$XMMRegister, vlen_enc);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n@@ -3897,1 +4168,1 @@\n-  predicate(UseAVX > 0);\n+  predicate(UseAVX > 0 && Matcher::vector_length_in_bytes(n) >= 16);\n@@ -5863,0 +6134,1 @@\n+  ins_cost(400);\n@@ -5875,0 +6147,1 @@\n+  ins_cost(400);\n@@ -5887,0 +6160,1 @@\n+  ins_cost(400);\n@@ -5899,0 +6173,1 @@\n+  ins_cost(400);\n@@ -6907,1 +7182,2 @@\n-  predicate(Matcher::vector_length_in_bytes(n->in(1)->in(1)) >=  8 && \/\/ src1\n+  predicate(n->bottom_type()->isa_vectmask() == NULL &&\n+            Matcher::vector_length_in_bytes(n->in(1)->in(1)) >=  8 && \/\/ src1\n@@ -6924,1 +7200,1 @@\n-instruct evcmpFD(vec dst, vec src1, vec src2, immI8 cond, rRegP scratch, kReg ktmp) %{\n+instruct evcmpFD64(vec dst, vec src1, vec src2, immI8 cond, rRegP scratch, kReg ktmp) %{\n@@ -6926,0 +7202,1 @@\n+            n->bottom_type()->isa_vectmask() == NULL &&\n@@ -6945,0 +7222,19 @@\n+instruct evcmpFD(kReg dst, vec src1, vec src2, immI8 cond) %{\n+  predicate(n->bottom_type()->isa_vectmask() &&\n+            is_floating_point_type(Matcher::vector_element_basic_type(n->in(1)->in(1)))); \/\/ src1 T_FLOAT, T_DOUBLE\n+  match(Set dst (VectorMaskCmp (Binary src1 src2) cond));\n+  format %{ \"vector_compare_evex $dst,$src1,$src2,$cond\\t!\" %}\n+  ins_encode %{\n+    assert(bottom_type()->isa_vectmask(), \"TypeVectMask expected\");\n+    int vlen_enc = vector_length_encoding(this, $src1);\n+    Assembler::ComparisonPredicateFP cmp = booltest_pred_to_comparison_pred_fp($cond$$constant);\n+    KRegister mask = k0; \/\/ The comparison itself is not being masked.\n+    if (Matcher::vector_element_basic_type(this, $src1) == T_FLOAT) {\n+      __ evcmpps($dst$$KRegister, mask, $src1$$XMMRegister, $src2$$XMMRegister, cmp, vlen_enc);\n+    } else {\n+      __ evcmppd($dst$$KRegister, mask, $src1$$XMMRegister, $src2$$XMMRegister, cmp, vlen_enc);\n+    }\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n@@ -6946,1 +7242,1 @@\n-  predicate((UseAVX <= 2 || !VM_Version::supports_avx512vl()) &&\n+  predicate(n->bottom_type()->isa_vectmask() == NULL &&\n@@ -6964,1 +7260,1 @@\n-  predicate((UseAVX == 2 || !VM_Version::supports_avx512vl()) &&\n+  predicate(n->bottom_type()->isa_vectmask() == NULL &&\n@@ -6983,1 +7279,1 @@\n-  predicate((UseAVX == 2 || !VM_Version::supports_avx512vl()) &&\n+  predicate(n->bottom_type()->isa_vectmask() == NULL &&\n@@ -7000,3 +7296,2 @@\n-instruct evcmp(vec dst, vec src1, vec src2, immI8 cond, rRegP scratch, kReg ktmp) %{\n-  predicate(UseAVX > 2 &&\n-            (VM_Version::supports_avx512vl() ||\n+instruct vcmpu64(vec dst, vec src1, vec src2, immI8 cond, rRegP scratch, kReg ktmp) %{\n+  predicate((n->bottom_type()->isa_vectmask() == NULL &&\n@@ -7018,0 +7313,33 @@\n+    switch (src1_elem_bt) {\n+      case T_INT: {\n+        __ evpcmpd($ktmp$$KRegister, mask, $src1$$XMMRegister, $src2$$XMMRegister, cmp, !is_unsigned, vlen_enc);\n+        __ evmovdqul($dst$$XMMRegister, $ktmp$$KRegister, ExternalAddress(vector_all_bits_set()), merge, vlen_enc, $scratch$$Register);\n+        break;\n+      }\n+      case T_LONG: {\n+        __ evpcmpq($ktmp$$KRegister, mask, $src1$$XMMRegister, $src2$$XMMRegister, cmp, !is_unsigned, vlen_enc);\n+        __ evmovdquq($dst$$XMMRegister, $ktmp$$KRegister, ExternalAddress(vector_all_bits_set()), merge, vlen_enc, $scratch$$Register);\n+        break;\n+      }\n+      default: assert(false, \"%s\", type2name(src1_elem_bt));\n+    }\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+\n+instruct evcmp(kReg dst, vec src1, vec src2, immI8 cond) %{\n+  predicate(n->bottom_type()->isa_vectmask() &&\n+            is_integral_type(Matcher::vector_element_basic_type(n->in(1)->in(1)))); \/\/ src1\n+  match(Set dst (VectorMaskCmp (Binary src1 src2) cond));\n+  format %{ \"vector_compared_evex $dst,$src1,$src2,$cond\\t!\" %}\n+  ins_encode %{\n+    assert(UseAVX > 2, \"required\");\n+    assert(bottom_type()->isa_vectmask(), \"TypeVectMask expected\");\n+\n+    int vlen_enc = vector_length_encoding(this, $src1);\n+    Assembler::ComparisonPredicate cmp = booltest_pred_to_comparison_pred($cond$$constant);\n+    bool is_unsigned = is_unsigned_booltest_pred($cond$$constant);\n+    BasicType src1_elem_bt = Matcher::vector_element_basic_type(this, $src1);\n+\n+    \/\/ Comparison i\n@@ -7020,2 +7348,1 @@\n-        __ evpcmpb($ktmp$$KRegister, mask, $src1$$XMMRegister, $src2$$XMMRegister, cmp, !is_unsigned, vlen_enc);\n-        __ evmovdqub($dst$$XMMRegister, $ktmp$$KRegister, ExternalAddress(vector_all_bits_set()), merge, vlen_enc, $scratch$$Register);\n+        __ evpcmpb($dst$$KRegister, k0, $src1$$XMMRegister, $src2$$XMMRegister, cmp, !is_unsigned, vlen_enc);\n@@ -7025,2 +7352,1 @@\n-        __ evpcmpw($ktmp$$KRegister, mask, $src1$$XMMRegister, $src2$$XMMRegister, cmp, !is_unsigned, vlen_enc);\n-        __ evmovdquw($dst$$XMMRegister, $ktmp$$KRegister, ExternalAddress(vector_all_bits_set()), merge, vlen_enc, $scratch$$Register);\n+        __ evpcmpw($dst$$KRegister, k0, $src1$$XMMRegister, $src2$$XMMRegister, cmp, !is_unsigned, vlen_enc);\n@@ -7030,2 +7356,1 @@\n-        __ evpcmpd($ktmp$$KRegister, mask, $src1$$XMMRegister, $src2$$XMMRegister, cmp, !is_unsigned, vlen_enc);\n-        __ evmovdqul($dst$$XMMRegister, $ktmp$$KRegister, ExternalAddress(vector_all_bits_set()), merge, vlen_enc, $scratch$$Register);\n+        __ evpcmpd($dst$$KRegister, k0, $src1$$XMMRegister, $src2$$XMMRegister, cmp, !is_unsigned, vlen_enc);\n@@ -7035,2 +7360,1 @@\n-        __ evpcmpq($ktmp$$KRegister, mask, $src1$$XMMRegister, $src2$$XMMRegister, cmp, !is_unsigned, vlen_enc);\n-        __ evmovdquq($dst$$XMMRegister, $ktmp$$KRegister, ExternalAddress(vector_all_bits_set()), merge, vlen_enc, $scratch$$Register);\n+        __ evpcmpq($dst$$KRegister, k0, $src1$$XMMRegister, $src2$$XMMRegister, cmp, !is_unsigned, vlen_enc);\n@@ -7189,0 +7513,1 @@\n+            n->in(2)->bottom_type()->isa_vectmask() == NULL &&\n@@ -7202,0 +7527,1 @@\n+            n->in(2)->bottom_type()->isa_vectmask() == NULL &&\n@@ -7214,1 +7540,2 @@\n-  predicate(Matcher::vector_length_in_bytes(n) == 64);\n+  predicate(Matcher::vector_length_in_bytes(n) == 64 &&\n+            n->in(2)->bottom_type()->isa_vectmask() == NULL);\n@@ -7227,0 +7554,16 @@\n+\n+instruct evblendvp64_masked(vec dst, vec src1, vec src2, kReg mask, rRegP scratch) %{\n+  predicate(n->in(2)->bottom_type()->isa_vectmask() &&\n+            (!is_subword_type(Matcher::vector_element_basic_type(n)) ||\n+             VM_Version::supports_avx512bw()));\n+  match(Set dst (VectorBlend (Binary src1 src2) mask));\n+  format %{ \"vector_blend  $dst,$src1,$src2,$mask\\t! using $scratch and k2 as TEMP\" %}\n+  effect(TEMP scratch);\n+  ins_encode %{\n+    int vlen_enc = vector_length_encoding(this);\n+    BasicType elem_bt = Matcher::vector_element_basic_type(this);\n+    __ evpblend(elem_bt, $dst$$XMMRegister, $mask$$KRegister, $src1$$XMMRegister, $src2$$XMMRegister, true, vlen_enc);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n@@ -7231,0 +7574,1 @@\n+  ins_cost(450);\n@@ -7246,0 +7590,1 @@\n+  ins_cost(450);\n@@ -7262,0 +7607,1 @@\n+  ins_cost(250);\n@@ -7276,0 +7622,1 @@\n+  ins_cost(450);\n@@ -7348,1 +7695,2 @@\n-  predicate(Matcher::vector_length_in_bytes(n->in(1)) >= 4 &&\n+  predicate(!VM_Version::supports_avx512bwdq() &&\n+            Matcher::vector_length_in_bytes(n->in(1)) >= 4 &&\n@@ -7353,1 +7701,1 @@\n-  format %{ \"vector_test $dst,$src1, $src2\\t! using $vtmp1, $vtmp2 and $cr as TEMP\" %}\n+  format %{ \"vptest_alltrue_lt16 $dst,$src1, $src2\\t! using $vtmp1, $vtmp2 and $cr as TEMP\" %}\n@@ -7363,2 +7711,3 @@\n-instruct vptest_alltrue(rRegI dst, legVec src1, legVec src2, rFlagsReg cr) %{\n-  predicate(Matcher::vector_length_in_bytes(n->in(1)) >= 16 &&\n+instruct vptest_alltrue_ge16(rRegI dst, legVec src1, legVec src2, rFlagsReg cr) %{\n+  predicate(!VM_Version::supports_avx512bwdq() &&\n+            Matcher::vector_length_in_bytes(n->in(1)) >= 16 &&\n@@ -7369,1 +7718,1 @@\n-  format %{ \"vector_test $dst,$src1, $src2\\t! using $cr as TEMP\" %}\n+  format %{ \"vptest_alltrue_ge16  $dst,$src1, $src2\\t! using $cr as TEMP\" %}\n@@ -7379,6 +7728,8 @@\n-instruct vptest_alltrue_evex(rRegI dst, legVec src1, legVec src2, kReg ktmp, rFlagsReg cr) %{\n-  predicate(Matcher::vector_length_in_bytes(n->in(1)) == 64 &&\n-            static_cast<const VectorTestNode*>(n)->get_predicate() == BoolTest::overflow);\n-  match(Set dst (VectorTest src1 src2 ));\n-  effect(KILL cr, TEMP ktmp);\n-  format %{ \"vector_test $dst,$src1, $src2\\t! using $cr as TEMP\" %}\n+instruct vptest_alltrue_lt8_evex(rRegI dst, kReg src1, kReg src2, kReg kscratch, rFlagsReg cr) %{\n+  predicate(VM_Version::supports_avx512bwdq() &&\n+            static_cast<const VectorTestNode*>(n)->get_predicate() == BoolTest::overflow &&\n+            n->in(1)->bottom_type()->isa_vectmask() &&\n+            Matcher::vector_length(n->in(1)) < 8);\n+  match(Set dst (VectorTest src1 src2));\n+  effect(KILL cr, TEMP kscratch);\n+  format %{ \"vptest_alltrue_lt8_evex $dst,$src1,$src2\\t! using $cr as TEMP\" %}\n@@ -7386,4 +7737,24 @@\n-    int vlen = Matcher::vector_length_in_bytes(this, $src1);\n-    __ vectortest(BoolTest::overflow, vlen, $src1$$XMMRegister, $src2$$XMMRegister, xnoreg, xnoreg, $ktmp$$KRegister);\n-    __ setb(Assembler::carrySet, $dst$$Register);\n-    __ movzbl($dst$$Register, $dst$$Register);\n+    const MachNode* mask1 = static_cast<const MachNode*>(this->in(this->operand_index($src1)));\n+    const MachNode* mask2 = static_cast<const MachNode*>(this->in(this->operand_index($src2)));\n+    assert(0 == Type::cmp(mask1->bottom_type(), mask2->bottom_type()), \"\");\n+    uint masklen = Matcher::vector_length(this, $src1);\n+    __ alltrue($dst$$Register, masklen, $src1$$KRegister, $src2$$KRegister, $kscratch$$KRegister);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+\n+instruct vptest_alltrue_ge8_evex(rRegI dst, kReg src1, kReg src2, rFlagsReg cr) %{\n+  predicate(VM_Version::supports_avx512bwdq() &&\n+            static_cast<const VectorTestNode*>(n)->get_predicate() == BoolTest::overflow &&\n+            n->in(1)->bottom_type()->isa_vectmask() &&\n+            Matcher::vector_length(n->in(1)) >= 8);\n+  match(Set dst (VectorTest src1 src2));\n+  effect(KILL cr);\n+  format %{ \"vptest_alltrue_ge8_evex $dst,$src1,$src2\\t! using $cr as TEMP\" %}\n+  ins_encode %{\n+    const MachNode* mask1 = static_cast<const MachNode*>(this->in(this->operand_index($src1)));\n+    const MachNode* mask2 = static_cast<const MachNode*>(this->in(this->operand_index($src2)));\n+    assert(0 == Type::cmp(mask1->bottom_type(), mask2->bottom_type()), \"\");\n+    uint masklen = Matcher::vector_length(this, $src1);\n+    __ alltrue($dst$$Register, masklen, $src1$$KRegister, $src2$$KRegister, knoreg);\n@@ -7394,0 +7765,1 @@\n+\n@@ -7395,1 +7767,2 @@\n-  predicate(Matcher::vector_length_in_bytes(n->in(1)) >= 4 &&\n+  predicate(!VM_Version::supports_avx512bwdq() &&\n+            Matcher::vector_length_in_bytes(n->in(1)) >= 4 &&\n@@ -7400,1 +7773,1 @@\n-  format %{ \"vector_test_any_true $dst,$src1,$src2\\t! using $vtmp, $cr as TEMP\" %}\n+  format %{ \"vptest_anytrue_lt16 $dst,$src1,$src2\\t! using $vtmp, $cr as TEMP\" %}\n@@ -7410,2 +7783,3 @@\n-instruct vptest_anytrue(rRegI dst, legVec src1, legVec src2, rFlagsReg cr) %{\n-  predicate(Matcher::vector_length_in_bytes(n->in(1)) >= 16 &&\n+instruct vptest_anytrue_ge16(rRegI dst, legVec src1, legVec src2, rFlagsReg cr) %{\n+  predicate(!VM_Version::supports_avx512bwdq() &&\n+            Matcher::vector_length_in_bytes(n->in(1)) >= 16 &&\n@@ -7416,1 +7790,1 @@\n-  format %{ \"vector_test_any_true $dst,$src1,$src2\\t! using $cr as TEMP\" %}\n+  format %{ \"vptest_anytrue_ge16 $dst,$src1,$src2\\t! using $cr as TEMP\" %}\n@@ -7426,2 +7800,2 @@\n-instruct vptest_anytrue_evex(rRegI dst, legVec src1, legVec src2, kReg ktmp, rFlagsReg cr) %{\n-  predicate(Matcher::vector_length_in_bytes(n->in(1)) == 64 &&\n+instruct vptest_anytrue_evex(rRegI dst, kReg src1, kReg src2, rFlagsReg cr) %{\n+  predicate(VM_Version::supports_avx512bwdq() &&\n@@ -7429,3 +7803,3 @@\n-  match(Set dst (VectorTest src1 src2 ));\n-  effect(KILL cr, TEMP ktmp);\n-  format %{ \"vector_test_any_true $dst,$src1,$src2\\t! using $cr as TEMP\" %}\n+  match(Set dst (VectorTest src1 src2));\n+  effect(KILL cr);\n+  format %{ \"vptest_anytrue_lt8_evex $dst,$src1,$src2\\t! using $cr as TEMP\" %}\n@@ -7433,4 +7807,5 @@\n-    int vlen = Matcher::vector_length_in_bytes(this, $src1);\n-    __ vectortest(BoolTest::ne, vlen, $src1$$XMMRegister, $src2$$XMMRegister, xnoreg, xnoreg, $ktmp$$KRegister);\n-    __ setb(Assembler::notZero, $dst$$Register);\n-    __ movzbl($dst$$Register, $dst$$Register);\n+    const MachNode* mask1 = static_cast<const MachNode*>(this->in(this->operand_index($src1)));\n+    const MachNode* mask2 = static_cast<const MachNode*>(this->in(this->operand_index($src2)));\n+    assert(0 == Type::cmp(mask1->bottom_type(), mask2->bottom_type()), \"\");\n+    uint  masklen = Matcher::vector_length(this, $src1);\n+    __ anytrue($dst$$Register, masklen, $src1$$KRegister, $src2$$KRegister);\n@@ -7442,1 +7817,2 @@\n-  predicate(Matcher::vector_length_in_bytes(n->in(1)->in(1)) >= 4 &&\n+  predicate(!VM_Version::supports_avx512bwdq() &&\n+            Matcher::vector_length_in_bytes(n->in(1)->in(1)) >= 4 &&\n@@ -7447,1 +7823,1 @@\n-  format %{ \"cmp_vector_test_any_true $src1,$src2\\t! using $vtmp as TEMP\" %}\n+  format %{ \"cmpvptest_anytrue_lt16 $src1,$src2\\t! using $vtmp as TEMP\" %}\n@@ -7455,2 +7831,3 @@\n-instruct cmpvptest_anytrue(rFlagsReg cr, legVec src1, legVec src2, immI_0 zero) %{\n-  predicate(Matcher::vector_length_in_bytes(n->in(1)->in(1)) >= 16 &&\n+instruct cmpvptest_anytrue_ge16(rFlagsReg cr, legVec src1, legVec src2, immI_0 zero) %{\n+  predicate(!VM_Version::supports_avx512bwdq() &&\n+            Matcher::vector_length_in_bytes(n->in(1)->in(1)) >= 16 &&\n@@ -7460,1 +7837,1 @@\n-  format %{ \"cmp_vector_test_any_true $src1,$src2\\t!\" %}\n+  format %{ \"cmpvptest_anytrue_ge16 $src1,$src2\\t!\" %}\n@@ -7468,2 +7845,2 @@\n-instruct cmpvptest_anytrue_evex(rFlagsReg cr, legVec src1, legVec src2, immI_0 zero, kReg ktmp) %{\n-  predicate(Matcher::vector_length_in_bytes(n->in(1)->in(1)) == 64 &&\n+instruct cmpvptest_anytrue_evex(rFlagsReg cr, kReg src1, kReg src2, immI_0 zero) %{\n+  predicate(VM_Version::supports_avx512bwdq() &&\n@@ -7472,2 +7849,1 @@\n-  effect(TEMP ktmp);\n-  format %{ \"cmp_vector_test_any_true $src1,$src2\\t!\" %}\n+  format %{ \"cmpvptest_anytrue_evex $src1,$src2\\t!\" %}\n@@ -7475,2 +7851,6 @@\n-    int vlen = Matcher::vector_length_in_bytes(this, $src1);\n-    __ vectortest(BoolTest::ne, vlen, $src1$$XMMRegister, $src2$$XMMRegister, xnoreg, xnoreg, $ktmp$$KRegister);\n+    uint masklen = Matcher::vector_length(this, $src1);\n+    const MachNode* mask1 = static_cast<const MachNode*>(this->in(this->operand_index($src1)));\n+    const MachNode* mask2 = static_cast<const MachNode*>(this->in(this->operand_index($src2)));\n+    assert(0 == Type::cmp(mask1->bottom_type(), mask2->bottom_type()), \"\");\n+    masklen = masklen < 8 ? 8 : masklen;\n+    __ ktest(masklen, $src1$$KRegister, $src2$$KRegister);\n@@ -7485,1 +7865,1 @@\n-  predicate(!VM_Version::supports_avx512vlbw());\n+  predicate(n->bottom_type()->isa_vectmask() == NULL && !VM_Version::supports_avx512vlbw());\n@@ -7488,1 +7868,1 @@\n-  format %{ \"vector_loadmask_byte $dst,$src\\n\\t\" %}\n+  format %{ \"vector_loadmask_byte $dst, $src\\n\\t\" %}\n@@ -7492,1 +7872,0 @@\n-\n@@ -7498,2 +7877,2 @@\n-instruct loadMask_evex(vec dst, vec src) %{\n-  predicate(VM_Version::supports_avx512vlbw());\n+instruct loadMask64(kReg dst, vec src, vec xtmp, rRegI tmp) %{\n+  predicate(n->bottom_type()->isa_vectmask() && !VM_Version::supports_avx512vlbw());\n@@ -7501,2 +7880,2 @@\n-  effect(TEMP dst);\n-  format %{ \"vector_loadmask_byte $dst,$src\\n\\t\" %}\n+  effect(TEMP xtmp, TEMP tmp);\n+  format %{ \"vector_loadmask_64byte $dst, $src\\t! using $xtmp and $tmp as TEMP\" %}\n@@ -7504,2 +7883,5 @@\n-    int vlen_in_bytes = Matcher::vector_length_in_bytes(this);\n-    BasicType elem_bt = Matcher::vector_element_basic_type(this);\n+    __ load_vector_mask($dst$$KRegister, $src$$XMMRegister, $xtmp$$XMMRegister,\n+                        $tmp$$Register, true, Assembler::AVX_512bit);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n@@ -7507,1 +7889,9 @@\n-    __ load_vector_mask($dst$$XMMRegister, $src$$XMMRegister, vlen_in_bytes, elem_bt, false);\n+instruct loadMask_evex(kReg dst, vec src,  vec xtmp) %{\n+  predicate(n->bottom_type()->isa_vectmask() && VM_Version::supports_avx512vlbw());\n+  match(Set dst (VectorLoadMask src));\n+  effect(TEMP xtmp);\n+  format %{ \"vector_loadmask_byte $dst, $src\\t! using $xtmp as TEMP\" %}\n+  ins_encode %{\n+    int vlen_enc = vector_length_encoding(in(1));\n+    __ load_vector_mask($dst$$KRegister, $src$$XMMRegister, $xtmp$$XMMRegister,\n+                        noreg, false, vlen_enc);\n@@ -7514,2 +7904,2 @@\n-instruct storeMask1B(vec dst, vec src, immI_1 size) %{\n-  predicate(Matcher::vector_length(n) < 64 || VM_Version::supports_avx512vlbw());\n+instruct vstoreMask1B(vec dst, vec src, immI_1 size) %{\n+  predicate(Matcher::vector_length(n) < 64 && n->in(1)->bottom_type()->isa_vectmask() == NULL);\n@@ -7517,1 +7907,1 @@\n-  format %{ \"vector_store_mask $dst,$src\\t!\" %}\n+  format %{ \"vector_store_mask $dst, $src \\t! elem size is $size byte[s]\" %}\n@@ -7519,2 +7909,3 @@\n-    assert(UseSSE >= 3, \"required\");\n-    if (Matcher::vector_length_in_bytes(this) <= 16) {\n+    int vlen = Matcher::vector_length(this);\n+    if (vlen <= 16 && UseAVX <= 2) {\n+      assert(UseSSE >= 3, \"required\");\n@@ -7523,1 +7914,1 @@\n-      assert(UseAVX >= 2, \"required\");\n+      assert(UseAVX > 0, \"required\");\n@@ -7531,2 +7922,2 @@\n-instruct storeMask2B(vec dst, vec src, immI_2 size) %{\n-  predicate(Matcher::vector_length(n) <= 8);\n+instruct vstoreMask2B(vec dst, vec src, vec xtmp, immI_2 size) %{\n+  predicate(Matcher::vector_length(n) <= 16 && n->in(1)->bottom_type()->isa_vectmask() == NULL);\n@@ -7534,14 +7925,2 @@\n-  format %{ \"vector_store_mask $dst,$src\\n\\t\" %}\n-  ins_encode %{\n-    assert(UseSSE >= 3, \"required\");\n-    __ pabsw($dst$$XMMRegister, $src$$XMMRegister);\n-    __ packsswb($dst$$XMMRegister, $dst$$XMMRegister);\n-  %}\n-  ins_pipe( pipe_slow );\n-%}\n-\n-instruct vstoreMask2B(vec dst, vec src, immI_2 size) %{\n-  predicate(Matcher::vector_length(n) == 16 && !VM_Version::supports_avx512bw());\n-  match(Set dst (VectorStoreMask src size));\n-  effect(TEMP dst);\n-  format %{ \"vector_store_mask $dst,$src\\t!\" %}\n+  effect(TEMP_DEF dst, TEMP xtmp);\n+  format %{ \"vector_store_mask $dst, $src \\t! elem size is $size byte[s]\" %}\n@@ -7550,3 +7929,12 @@\n-    __ vextracti128($dst$$XMMRegister, $src$$XMMRegister, 0x1);\n-    __ vpacksswb($dst$$XMMRegister, $src$$XMMRegister, $dst$$XMMRegister,vlen_enc);\n-    __ vpabsb($dst$$XMMRegister, $dst$$XMMRegister, vlen_enc);\n+    int vlen = Matcher::vector_length(this);\n+    if (vlen <= 8) {\n+      assert(UseSSE >= 3, \"required\");\n+      __ pxor($xtmp$$XMMRegister, $xtmp$$XMMRegister);\n+      __ pabsw($dst$$XMMRegister, $src$$XMMRegister);\n+      __ packuswb($dst$$XMMRegister, $xtmp$$XMMRegister);\n+    } else {\n+      assert(UseAVX > 0, \"required\");\n+      __ vextracti128($dst$$XMMRegister, $src$$XMMRegister, 0x1);\n+      __ vpacksswb($dst$$XMMRegister, $src$$XMMRegister, $dst$$XMMRegister, vlen_enc);\n+      __ vpabsb($dst$$XMMRegister, $dst$$XMMRegister, vlen_enc);\n+    }\n@@ -7557,2 +7945,2 @@\n-instruct vstoreMask2B_evex(vec dst, vec src, immI_2 size) %{\n-  predicate(VM_Version::supports_avx512bw());\n+instruct vstoreMask4B(vec dst, vec src, vec xtmp, immI_4 size) %{\n+  predicate(UseAVX <= 2 && Matcher::vector_length(n) <= 8 && n->in(1)->bottom_type()->isa_vectmask() == NULL);\n@@ -7560,1 +7948,2 @@\n-  format %{ \"vector_store_mask $dst,$src\\t!\" %}\n+  format %{ \"vector_store_mask $dst, $src \\t! elem size is $size byte[s]\" %}\n+  effect(TEMP_DEF dst, TEMP xtmp);\n@@ -7562,4 +7951,16 @@\n-    int src_vlen_enc = vector_length_encoding(this, $src);\n-    int dst_vlen_enc = vector_length_encoding(this);\n-    __ evpmovwb($dst$$XMMRegister, $src$$XMMRegister, src_vlen_enc);\n-    __ vpabsb($dst$$XMMRegister, $dst$$XMMRegister, dst_vlen_enc);\n+    int vlen_enc = Assembler::AVX_128bit;\n+    int vlen = Matcher::vector_length(this);\n+    if (vlen <= 4) {\n+      assert(UseSSE >= 3, \"required\");\n+      __ pxor($xtmp$$XMMRegister, $xtmp$$XMMRegister);\n+      __ pabsd($dst$$XMMRegister, $src$$XMMRegister);\n+      __ packusdw($dst$$XMMRegister, $xtmp$$XMMRegister);\n+      __ packuswb($dst$$XMMRegister, $xtmp$$XMMRegister);\n+    } else {\n+      assert(UseAVX > 0, \"required\");\n+      __ vpxor($xtmp$$XMMRegister, $xtmp$$XMMRegister, $xtmp$$XMMRegister, vlen_enc);\n+      __ vextracti128($dst$$XMMRegister, $src$$XMMRegister, 0x1);\n+      __ vpackssdw($dst$$XMMRegister, $src$$XMMRegister, $dst$$XMMRegister, vlen_enc);\n+      __ vpacksswb($dst$$XMMRegister, $dst$$XMMRegister, $xtmp$$XMMRegister, vlen_enc);\n+      __ vpabsb($dst$$XMMRegister, $dst$$XMMRegister, vlen_enc);\n+    }\n@@ -7570,2 +7971,2 @@\n-instruct storeMask4B(vec dst, vec src, immI_4 size) %{\n-  predicate(Matcher::vector_length(n) <= 4 && UseAVX <= 2);\n+instruct storeMask8B(vec dst, vec src, vec xtmp, immI_8 size) %{\n+  predicate(UseAVX <= 2 && Matcher::vector_length(n) == 2);\n@@ -7573,1 +7974,2 @@\n-  format %{ \"vector_store_mask $dst,$src\\t!\" %}\n+  effect(TEMP_DEF dst, TEMP xtmp);\n+  format %{ \"vector_store_mask $dst, $src \\t! elem size is $size byte[s]\" %}\n@@ -7576,3 +7978,5 @@\n-    __ pabsd($dst$$XMMRegister, $src$$XMMRegister);\n-    __ packssdw($dst$$XMMRegister, $dst$$XMMRegister);\n-    __ packsswb($dst$$XMMRegister, $dst$$XMMRegister);\n+    __ pxor($xtmp$$XMMRegister, $xtmp$$XMMRegister);\n+    __ pshufd($dst$$XMMRegister, $src$$XMMRegister, 0x8);\n+    __ pabsd($dst$$XMMRegister, $dst$$XMMRegister);\n+    __ packusdw($dst$$XMMRegister, $xtmp$$XMMRegister);\n+    __ packuswb($dst$$XMMRegister, $xtmp$$XMMRegister);\n@@ -7583,2 +7987,2 @@\n-instruct vstoreMask4B(vec dst, vec src, immI_4 size) %{\n-  predicate(Matcher::vector_length(n) == 8 && UseAVX <= 2);\n+instruct storeMask8B_avx(vec dst, vec src, immI_8 size, vec vtmp) %{\n+  predicate(UseAVX <= 2 && Matcher::vector_length(n) == 4);\n@@ -7586,2 +7990,2 @@\n-  format %{ \"vector_store_mask $dst,$src\\t!\" %}\n-  effect(TEMP dst);\n+  format %{ \"vector_store_mask $dst, $src \\t! elem size is $size byte[s], using $vtmp as TEMP\" %}\n+  effect(TEMP_DEF dst, TEMP vtmp);\n@@ -7590,3 +7994,6 @@\n-    __ vextracti128($dst$$XMMRegister, $src$$XMMRegister, 0x1);\n-    __ vpackssdw($dst$$XMMRegister, $src$$XMMRegister, $dst$$XMMRegister, vlen_enc);\n-    __ vpacksswb($dst$$XMMRegister, $dst$$XMMRegister, $dst$$XMMRegister, vlen_enc);\n+    __ vpshufps($dst$$XMMRegister, $src$$XMMRegister, $src$$XMMRegister, 0x88, Assembler::AVX_256bit);\n+    __ vextracti128($vtmp$$XMMRegister, $dst$$XMMRegister, 0x1);\n+    __ vblendps($dst$$XMMRegister, $dst$$XMMRegister, $vtmp$$XMMRegister, 0xC, vlen_enc);\n+    __ vpxor($vtmp$$XMMRegister, $vtmp$$XMMRegister, $vtmp$$XMMRegister, vlen_enc);\n+    __ vpackssdw($dst$$XMMRegister, $dst$$XMMRegister, $vtmp$$XMMRegister, vlen_enc);\n+    __ vpacksswb($dst$$XMMRegister, $dst$$XMMRegister, $vtmp$$XMMRegister, vlen_enc);\n@@ -7598,2 +8005,2 @@\n-instruct vstoreMask4B_evex(vec dst, vec src, immI_4 size) %{\n-  predicate(UseAVX > 2);\n+instruct vstoreMask4B_evex_novectmask(vec dst, vec src, immI_4 size) %{\n+  predicate(UseAVX > 2 && n->in(1)->bottom_type()->isa_vectmask() == NULL);\n@@ -7601,1 +8008,1 @@\n-  format %{ \"vector_store_mask $dst,$src\\t!\" %}\n+  format %{ \"vector_store_mask $dst, $src \\t! elem size is $size byte[s]\" %}\n@@ -7614,2 +8021,2 @@\n-instruct storeMask8B(vec dst, vec src, immI_8 size) %{\n-  predicate(Matcher::vector_length(n) == 2 && UseAVX <= 2);\n+instruct vstoreMask8B_evex_novectmask(vec dst, vec src, immI_8 size) %{\n+  predicate(UseAVX > 2 && n->in(1)->bottom_type()->isa_vectmask() == NULL);\n@@ -7617,1 +8024,1 @@\n-  format %{ \"vector_store_mask $dst,$src\\t!\" %}\n+  format %{ \"vector_store_mask $dst, $src \\t! elem size is $size byte[s]\" %}\n@@ -7619,6 +8026,8 @@\n-    assert(UseSSE >= 3, \"required\");\n-    __ pshufd($dst$$XMMRegister, $src$$XMMRegister, 0x8);\n-    __ packssdw($dst$$XMMRegister, $dst$$XMMRegister);\n-    __ packsswb($dst$$XMMRegister, $dst$$XMMRegister);\n-    __ pabsb($dst$$XMMRegister, $dst$$XMMRegister);\n-  %}\n+    int src_vlen_enc = vector_length_encoding(this, $src);\n+    int dst_vlen_enc = vector_length_encoding(this);\n+    if (!VM_Version::supports_avx512vl()) {\n+      src_vlen_enc = Assembler::AVX_512bit;\n+    }\n+    __ evpmovqb($dst$$XMMRegister, $src$$XMMRegister, src_vlen_enc);\n+    __ vpabsb($dst$$XMMRegister, $dst$$XMMRegister, dst_vlen_enc);\n+  %}\n@@ -7628,5 +8037,5 @@\n-instruct storeMask8B_avx(vec dst, vec src, immI_8 size, legVec vtmp) %{\n-  predicate(Matcher::vector_length(n) == 4 && UseAVX <= 2);\n-  match(Set dst (VectorStoreMask src size));\n-  format %{ \"vector_store_mask $dst,$src\\t! using $vtmp as TEMP\" %}\n-  effect(TEMP dst, TEMP vtmp);\n+instruct vstoreMask_evex_vectmask(vec dst, kReg mask, immI size, rRegI tmp) %{\n+  predicate(n->in(1)->bottom_type()->isa_vectmask() && !VM_Version::supports_avx512vlbw());\n+  match(Set dst (VectorStoreMask mask size));\n+  effect(TEMP_DEF dst, TEMP tmp);\n+  format %{ \"vector_store_mask $dst, $mask \\t! elem size is $size byte[s]\" %}\n@@ -7634,7 +8043,4 @@\n-    int vlen_enc = Assembler::AVX_128bit;\n-    __ vpshufps($dst$$XMMRegister, $src$$XMMRegister, $src$$XMMRegister, 0x88, Assembler::AVX_256bit);\n-    __ vextracti128($vtmp$$XMMRegister, $dst$$XMMRegister, 0x1);\n-    __ vblendps($dst$$XMMRegister, $dst$$XMMRegister, $vtmp$$XMMRegister, 0xC, vlen_enc);\n-    __ vpackssdw($dst$$XMMRegister, $dst$$XMMRegister, $dst$$XMMRegister, vlen_enc);\n-    __ vpacksswb($dst$$XMMRegister, $dst$$XMMRegister, $dst$$XMMRegister, vlen_enc);\n-    __ vpabsb($dst$$XMMRegister, $dst$$XMMRegister, vlen_enc);\n+    assert(Matcher::vector_length_in_bytes(this, $mask) == 64, \"\");\n+    __ evmovdqul($dst$$XMMRegister, $mask$$KRegister, ExternalAddress(vector_int_mask_cmp_bits()),\n+                 false, Assembler::AVX_512bit, $tmp$$Register);\n+    __ evpmovdb($dst$$XMMRegister, $dst$$XMMRegister, Assembler::AVX_512bit);\n@@ -7645,4 +8051,5 @@\n-instruct vstoreMask8B_evex(vec dst, vec src, immI_8 size) %{\n-  predicate(UseAVX > 2);\n-  match(Set dst (VectorStoreMask src size));\n-  format %{ \"vector_store_mask $dst,$src\\t!\" %}\n+instruct vstoreMask_evex(vec dst, kReg mask, immI size) %{\n+  predicate(n->in(1)->bottom_type()->isa_vectmask() && VM_Version::supports_avx512vlbw());\n+  match(Set dst (VectorStoreMask mask size));\n+  effect(TEMP_DEF dst);\n+  format %{ \"vector_store_mask $dst, $mask \\t! elem size is $size byte[s]\" %}\n@@ -7650,1 +8057,0 @@\n-    int src_vlen_enc = vector_length_encoding(this, $src);\n@@ -7652,4 +8058,1 @@\n-    if (!VM_Version::supports_avx512vl()) {\n-      src_vlen_enc = Assembler::AVX_512bit;\n-    }\n-    __ evpmovqb($dst$$XMMRegister, $src$$XMMRegister, src_vlen_enc);\n+    __ evpmovm2b($dst$$XMMRegister, $mask$$KRegister, dst_vlen_enc);\n@@ -7661,0 +8064,11 @@\n+instruct vmaskcast_evex(kReg dst) %{\n+  predicate(Matcher::vector_length(n) == Matcher::vector_length(n->in(1)));\n+  match(Set dst (VectorMaskCast dst));\n+  ins_cost(0);\n+  format %{ \"vector_mask_cast $dst\" %}\n+  ins_encode %{\n+    \/\/ empty\n+  %}\n+  ins_pipe(empty);\n+%}\n+\n@@ -8230,2 +8644,49 @@\n-instruct vmask_truecount_evex(rRegI dst, vec mask, rRegL tmp, kReg ktmp, vec xtmp) %{\n-  predicate(VM_Version::supports_avx512vlbw());\n+instruct vmask_tolong_evex(rRegL dst, kReg mask, rFlagsReg cr) %{\n+  predicate(n->in(1)->bottom_type()->isa_vectmask());\n+  match(Set dst (VectorMaskToLong mask));\n+  effect(TEMP dst, KILL cr);\n+  format %{ \"vector_tolong_evex $dst, $mask \\t! vector mask tolong\" %}\n+  ins_encode %{\n+    int mask_len = Matcher::vector_length(this, $mask);\n+    BasicType mbt = Matcher::vector_element_basic_type(this, $mask);\n+    if (VM_Version::supports_avx512vlbw()) {\n+      __ kmovql($dst$$Register, $mask$$KRegister);\n+    } else {\n+      assert(mask_len <= 16, \"\");\n+      __ kmovwl($dst$$Register, $mask$$KRegister);\n+    }\n+    \/\/ Mask generated out of partial vector comparisons\/replicate\/mask manipulation\n+    \/\/ operations needs to be clipped.\n+    int mask_size = mask_len * type2aelembytes(mbt);\n+    if (mask_size < 16) {\n+      __ andq($dst$$Register, (((jlong)1 << mask_len) - 1));\n+    }\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct vmask_tolong_avx(rRegL dst, vec mask, vec xtmp, rFlagsReg cr) %{\n+  predicate(n->in(1)->bottom_type()->isa_vectmask() == NULL &&\n+            n->in(1)->bottom_type()->is_vect()->element_basic_type() == T_BOOLEAN);\n+  match(Set dst (VectorMaskToLong mask));\n+  format %{ \"vector_tolong_avx $dst, $mask \\t! using $xtmp as TEMP\" %}\n+  effect(TEMP_DEF dst, TEMP xtmp, KILL cr);\n+  ins_encode %{\n+    int mask_len = Matcher::vector_length(this, $mask);\n+    BasicType mbt = Matcher::vector_element_basic_type(this, $mask);\n+    int vlen_enc = vector_length_encoding(this, $mask);\n+    __ vpxor($xtmp$$XMMRegister, $xtmp$$XMMRegister, $xtmp$$XMMRegister, vlen_enc);\n+    __ vpsubb($xtmp$$XMMRegister, $xtmp$$XMMRegister, $mask$$XMMRegister, vlen_enc);\n+    __ vpmovmskb($dst$$Register, $xtmp$$XMMRegister, vlen_enc);\n+    \/\/ Mask generated out of partial vector comparisons\/replicate\/mask manipulation\n+    \/\/ operations needs to be clipped.\n+    int mask_size = mask_len * type2aelembytes(mbt);\n+    if (mask_size < 16) {\n+      __ andq($dst$$Register, (((jlong)1 << mask_len) - 1));\n+    }\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct vmask_truecount_evex(rRegI dst, kReg mask, rRegL tmp, rFlagsReg cr) %{\n+  predicate(n->in(1)->bottom_type()->isa_vectmask());\n@@ -8233,2 +8694,2 @@\n-  effect(TEMP_DEF dst, TEMP tmp, TEMP ktmp, TEMP xtmp);\n-  format %{ \"vector_truecount_evex $mask \\t! vector mask true count\" %}\n+  effect(TEMP_DEF dst, TEMP tmp, KILL cr);\n+  format %{ \"vector_truecount_evex $dst, $mask \\t! using $tmp as TEMP\" %}\n@@ -8237,1 +8698,1 @@\n-    int vlen_enc = vector_length_encoding(this, $mask);\n+    BasicType mbt = Matcher::vector_element_basic_type(this, $mask);\n@@ -8239,2 +8700,4 @@\n-    __ vector_mask_operation(opcode, $dst$$Register, $mask$$XMMRegister, $xtmp$$XMMRegister,\n-                             $tmp$$Register, $ktmp$$KRegister, mask_len, vlen_enc);\n+    int mask_size = mask_len * type2aelembytes(mbt);\n+    int vlen_enc = vector_length_encoding(this, $mask);\n+    __ vector_mask_operation(opcode, $dst$$Register, $mask$$KRegister, $tmp$$Register,\n+                             mask_len, mask_size, vlen_enc);\n@@ -8245,6 +8708,5 @@\n-instruct vmask_first_or_last_true_evex(rRegI dst, vec mask, rRegL tmp, kReg ktmp, vec xtmp, rFlagsReg cr) %{\n-  predicate(VM_Version::supports_avx512vlbw());\n-  match(Set dst (VectorMaskFirstTrue mask));\n-  match(Set dst (VectorMaskLastTrue mask));\n-  effect(TEMP_DEF dst, TEMP tmp, TEMP ktmp, TEMP xtmp, KILL cr);\n-  format %{ \"vector_mask_first_or_last_true_evex $mask \\t! vector first\/last true location\" %}\n+instruct vmask_truecount_avx(rRegI dst, vec mask, rRegL tmp, vec xtmp, vec xtmp1, rFlagsReg cr) %{\n+  predicate(n->in(1)->bottom_type()->isa_vectmask() == NULL);\n+  match(Set dst (VectorMaskTrueCount mask));\n+  effect(TEMP_DEF dst, TEMP tmp, TEMP xtmp, TEMP xtmp1, KILL cr);\n+  format %{ \"vector_truecount_avx $dst, $mask \\t! using $tmp, $xtmp and $xtmp1 as TEMP\" %}\n@@ -8253,1 +8715,1 @@\n-    int vlen_enc = vector_length_encoding(this, $mask);\n+    BasicType mbt = Matcher::vector_element_basic_type(this, $mask);\n@@ -8255,0 +8717,2 @@\n+    int mask_size = mask_len * type2aelembytes(mbt);\n+    int vlen_enc = vector_length_encoding(this, $mask);\n@@ -8256,1 +8720,1 @@\n-                             $tmp$$Register, $ktmp$$KRegister, mask_len, vlen_enc);\n+                             $xtmp1$$XMMRegister, $tmp$$Register, mask_len, mask_size, vlen_enc);\n@@ -8261,5 +8725,6 @@\n-instruct vmask_truecount_avx(rRegI dst, vec mask, rRegL tmp, vec xtmp, vec xtmp1) %{\n-  predicate(!VM_Version::supports_avx512vlbw());\n-  match(Set dst (VectorMaskTrueCount mask));\n-  effect(TEMP_DEF dst, TEMP tmp, TEMP xtmp, TEMP xtmp1);\n-  format %{ \"vector_truecount_avx $mask \\t! vector mask true count\" %}\n+instruct vmask_first_or_last_true_evex(rRegI dst, kReg mask, rRegL tmp, rFlagsReg cr) %{\n+  predicate(n->in(1)->bottom_type()->isa_vectmask());\n+  match(Set dst (VectorMaskFirstTrue mask));\n+  match(Set dst (VectorMaskLastTrue mask));\n+  effect(TEMP_DEF dst, TEMP tmp, KILL cr);\n+  format %{ \"vector_mask_first_or_last_true_evex $dst, $mask \\t! using $tmp as TEMP\" %}\n@@ -8268,1 +8733,1 @@\n-    int vlen_enc = vector_length_encoding(this, $mask);\n+    BasicType mbt = Matcher::vector_element_basic_type(this, $mask);\n@@ -8270,2 +8735,4 @@\n-    __ vector_mask_operation(opcode, $dst$$Register, $mask$$XMMRegister, $xtmp$$XMMRegister,\n-                             $xtmp1$$XMMRegister, $tmp$$Register, mask_len, vlen_enc);\n+    int mask_size = mask_len * type2aelembytes(mbt);\n+    int vlen_enc = vector_length_encoding(this, $mask);\n+    __ vector_mask_operation(opcode, $dst$$Register, $mask$$KRegister, $tmp$$Register, mask_len,\n+                             mask_size, vlen_enc);\n@@ -8277,1 +8744,1 @@\n-  predicate(!VM_Version::supports_avx512vlbw());\n+  predicate(n->in(1)->bottom_type()->isa_vectmask() == NULL);\n@@ -8281,1 +8748,1 @@\n-  format %{ \"vector_mask_first_or_last_true_avx $mask \\t! vector first\/last true location\" %}\n+  format %{ \"vector_mask_first_or_last_true_avx $dst, $mask \\t! using $tmp, $xtmp and $xtmp1 as TEMP\" %}\n@@ -8284,1 +8751,1 @@\n-    int vlen_enc = vector_length_encoding(this, $mask);\n+    BasicType mbt = Matcher::vector_element_basic_type(this, $mask);\n@@ -8286,0 +8753,2 @@\n+    int mask_size = mask_len * type2aelembytes(mbt);\n+    int vlen_enc = vector_length_encoding(this, $mask);\n@@ -8287,1 +8756,1 @@\n-                             $xtmp1$$XMMRegister, $tmp$$Register, mask_len, vlen_enc);\n+                             $xtmp1$$XMMRegister, $tmp$$Register, mask_len, mask_size, vlen_enc);\n@@ -8293,0 +8762,669 @@\n+\/\/ ---------------------------------- Vector Masked Operations ------------------------------------\n+\n+instruct vadd_reg_masked(vec dst, vec src2, kReg mask) %{\n+  match(Set dst (AddVB (Binary dst src2) mask));\n+  match(Set dst (AddVS (Binary dst src2) mask));\n+  match(Set dst (AddVI (Binary dst src2) mask));\n+  match(Set dst (AddVL (Binary dst src2) mask));\n+  match(Set dst (AddVF (Binary dst src2) mask));\n+  match(Set dst (AddVD (Binary dst src2) mask));\n+  format %{ \"vpadd_masked $dst, $dst, $src2, $mask\\t! add masked operation\" %}\n+  ins_encode %{\n+    int vlen_enc = vector_length_encoding(this);\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    int opc = this->ideal_Opcode();\n+    __ evmasked_op(opc, bt, $mask$$KRegister, $dst$$XMMRegister,\n+                   $dst$$XMMRegister, $src2$$XMMRegister, true, vlen_enc);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct vadd_mem_masked(vec dst, memory src2, kReg mask) %{\n+  match(Set dst (AddVB (Binary dst (LoadVector src2)) mask));\n+  match(Set dst (AddVS (Binary dst (LoadVector src2)) mask));\n+  match(Set dst (AddVI (Binary dst (LoadVector src2)) mask));\n+  match(Set dst (AddVL (Binary dst (LoadVector src2)) mask));\n+  match(Set dst (AddVF (Binary dst (LoadVector src2)) mask));\n+  match(Set dst (AddVD (Binary dst (LoadVector src2)) mask));\n+  format %{ \"vpadd_masked $dst, $dst, $src2, $mask\\t! add masked operation\" %}\n+  ins_encode %{\n+    int vlen_enc = vector_length_encoding(this);\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    int opc = this->ideal_Opcode();\n+    __ evmasked_op(opc, bt, $mask$$KRegister, $dst$$XMMRegister,\n+                   $dst$$XMMRegister, $src2$$Address, true, vlen_enc);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct vxor_reg_masked(vec dst, vec src2, kReg mask) %{\n+  match(Set dst (XorV (Binary dst src2) mask));\n+  format %{ \"vxor_masked $dst, $dst, $src2, $mask\\t! xor masked operation\" %}\n+  ins_encode %{\n+    int vlen_enc = vector_length_encoding(this);\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    int opc = this->ideal_Opcode();\n+    __ evmasked_op(opc, bt, $mask$$KRegister, $dst$$XMMRegister,\n+                   $dst$$XMMRegister, $src2$$XMMRegister, true, vlen_enc);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct vxor_mem_masked(vec dst, memory src2, kReg mask) %{\n+  match(Set dst (XorV (Binary dst (LoadVector src2)) mask));\n+  format %{ \"vxor_masked $dst, $dst, $src2, $mask\\t! xor masked operation\" %}\n+  ins_encode %{\n+    int vlen_enc = vector_length_encoding(this);\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    int opc = this->ideal_Opcode();\n+    __ evmasked_op(opc, bt, $mask$$KRegister, $dst$$XMMRegister,\n+                   $dst$$XMMRegister, $src2$$Address, true, vlen_enc);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct vor_reg_masked(vec dst, vec src2, kReg mask) %{\n+  match(Set dst (OrV (Binary dst src2) mask));\n+  format %{ \"vor_masked $dst, $dst, $src2, $mask\\t! or masked operation\" %}\n+  ins_encode %{\n+    int vlen_enc = vector_length_encoding(this);\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    int opc = this->ideal_Opcode();\n+    __ evmasked_op(opc, bt, $mask$$KRegister, $dst$$XMMRegister,\n+                   $dst$$XMMRegister, $src2$$XMMRegister, true, vlen_enc);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct vor_mem_masked(vec dst, memory src2, kReg mask) %{\n+  match(Set dst (OrV (Binary dst (LoadVector src2)) mask));\n+  format %{ \"vor_masked $dst, $dst, $src2, $mask\\t! or masked operation\" %}\n+  ins_encode %{\n+    int vlen_enc = vector_length_encoding(this);\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    int opc = this->ideal_Opcode();\n+    __ evmasked_op(opc, bt, $mask$$KRegister, $dst$$XMMRegister,\n+                   $dst$$XMMRegister, $src2$$Address, true, vlen_enc);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct vand_reg_masked(vec dst, vec src2, kReg mask) %{\n+  match(Set dst (AndV (Binary dst src2) mask));\n+  format %{ \"vand_masked $dst, $dst, $src2, $mask\\t! and masked operation\" %}\n+  ins_encode %{\n+    int vlen_enc = vector_length_encoding(this);\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    int opc = this->ideal_Opcode();\n+    __ evmasked_op(opc, bt, $mask$$KRegister, $dst$$XMMRegister,\n+                   $dst$$XMMRegister, $src2$$XMMRegister, true, vlen_enc);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct vand_mem_masked(vec dst, memory src2, kReg mask) %{\n+  match(Set dst (AndV (Binary dst (LoadVector src2)) mask));\n+  format %{ \"vand_masked $dst, $dst, $src2, $mask\\t! and masked operation\" %}\n+  ins_encode %{\n+    int vlen_enc = vector_length_encoding(this);\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    int opc = this->ideal_Opcode();\n+    __ evmasked_op(opc, bt, $mask$$KRegister, $dst$$XMMRegister,\n+                   $dst$$XMMRegister, $src2$$Address, true, vlen_enc);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct vsub_reg_masked(vec dst, vec src2, kReg mask) %{\n+  match(Set dst (SubVB (Binary dst src2) mask));\n+  match(Set dst (SubVS (Binary dst src2) mask));\n+  match(Set dst (SubVI (Binary dst src2) mask));\n+  match(Set dst (SubVL (Binary dst src2) mask));\n+  match(Set dst (SubVF (Binary dst src2) mask));\n+  match(Set dst (SubVD (Binary dst src2) mask));\n+  format %{ \"vpsub_masked $dst, $dst, $src2, $mask\\t! sub masked operation\" %}\n+  ins_encode %{\n+    int vlen_enc = vector_length_encoding(this);\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    int opc = this->ideal_Opcode();\n+    __ evmasked_op(opc, bt, $mask$$KRegister, $dst$$XMMRegister,\n+                   $dst$$XMMRegister, $src2$$XMMRegister, true, vlen_enc);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct vsub_mem_masked(vec dst, memory src2, kReg mask) %{\n+  match(Set dst (SubVB (Binary dst (LoadVector src2)) mask));\n+  match(Set dst (SubVS (Binary dst (LoadVector src2)) mask));\n+  match(Set dst (SubVI (Binary dst (LoadVector src2)) mask));\n+  match(Set dst (SubVL (Binary dst (LoadVector src2)) mask));\n+  match(Set dst (SubVF (Binary dst (LoadVector src2)) mask));\n+  match(Set dst (SubVD (Binary dst (LoadVector src2)) mask));\n+  format %{ \"vpsub_masked $dst, $dst, $src2, $mask\\t! sub masked operation\" %}\n+  ins_encode %{\n+    int vlen_enc = vector_length_encoding(this);\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    int opc = this->ideal_Opcode();\n+    __ evmasked_op(opc, bt, $mask$$KRegister, $dst$$XMMRegister,\n+                   $dst$$XMMRegister, $src2$$Address, true, vlen_enc);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct vmul_reg_masked(vec dst, vec src2, kReg mask) %{\n+  match(Set dst (MulVS (Binary dst src2) mask));\n+  match(Set dst (MulVI (Binary dst src2) mask));\n+  match(Set dst (MulVL (Binary dst src2) mask));\n+  match(Set dst (MulVF (Binary dst src2) mask));\n+  match(Set dst (MulVD (Binary dst src2) mask));\n+  format %{ \"vpmul_masked $dst, $dst, $src2, $mask\\t! mul masked operation\" %}\n+  ins_encode %{\n+    int vlen_enc = vector_length_encoding(this);\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    int opc = this->ideal_Opcode();\n+    __ evmasked_op(opc, bt, $mask$$KRegister, $dst$$XMMRegister,\n+                   $dst$$XMMRegister, $src2$$XMMRegister, true, vlen_enc);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct vmul_mem_masked(vec dst, memory src2, kReg mask) %{\n+  match(Set dst (MulVS (Binary dst (LoadVector src2)) mask));\n+  match(Set dst (MulVI (Binary dst (LoadVector src2)) mask));\n+  match(Set dst (MulVL (Binary dst (LoadVector src2)) mask));\n+  match(Set dst (MulVF (Binary dst (LoadVector src2)) mask));\n+  match(Set dst (MulVD (Binary dst (LoadVector src2)) mask));\n+  format %{ \"vpmul_masked $dst, $dst, $src2, $mask\\t! mul masked operation\" %}\n+  ins_encode %{\n+    int vlen_enc = vector_length_encoding(this);\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    int opc = this->ideal_Opcode();\n+    __ evmasked_op(opc, bt, $mask$$KRegister, $dst$$XMMRegister,\n+                   $dst$$XMMRegister, $src2$$Address, true, vlen_enc);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct vsqrt_reg_masked(vec dst, kReg mask) %{\n+  match(Set dst (SqrtVF dst mask));\n+  match(Set dst (SqrtVD dst mask));\n+  ins_cost(100);\n+  format %{ \"vpsqrt_masked $dst, $mask\\t! sqrt masked operation\" %}\n+  ins_encode %{\n+    int vlen_enc = vector_length_encoding(this);\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    int opc = this->ideal_Opcode();\n+    __ evmasked_op(opc, bt, $mask$$KRegister, $dst$$XMMRegister,\n+                   $dst$$XMMRegister, $dst$$XMMRegister, true, vlen_enc);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct vdiv_reg_masked(vec dst, vec src2, kReg mask) %{\n+  match(Set dst (DivVF (Binary dst src2) mask));\n+  match(Set dst (DivVD (Binary dst src2) mask));\n+  format %{ \"vpdiv_masked $dst, $dst, $src2, $mask\\t! div masked operation\" %}\n+  ins_encode %{\n+    int vlen_enc = vector_length_encoding(this);\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    int opc = this->ideal_Opcode();\n+    __ evmasked_op(opc, bt, $mask$$KRegister, $dst$$XMMRegister,\n+                   $dst$$XMMRegister, $src2$$XMMRegister, true, vlen_enc);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct vdiv_mem_masked(vec dst, memory src2, kReg mask) %{\n+  match(Set dst (DivVF (Binary dst (LoadVector src2)) mask));\n+  match(Set dst (DivVD (Binary dst (LoadVector src2)) mask));\n+  format %{ \"vpdiv_masked $dst, $dst, $src2, $mask\\t! div masked operation\" %}\n+  ins_encode %{\n+    int vlen_enc = vector_length_encoding(this);\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    int opc = this->ideal_Opcode();\n+    __ evmasked_op(opc, bt, $mask$$KRegister, $dst$$XMMRegister,\n+                   $dst$$XMMRegister, $src2$$Address, true, vlen_enc);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+\n+instruct vrol_imm_masked(vec dst, immI8 shift, kReg mask) %{\n+  match(Set dst (RotateLeftV (Binary dst shift) mask));\n+  match(Set dst (RotateRightV (Binary dst shift) mask));\n+  format %{ \"vprotate_imm_masked $dst, $dst, $shift, $mask\\t! rotate masked operation\" %}\n+  ins_encode %{\n+    int vlen_enc = vector_length_encoding(this);\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    int opc = this->ideal_Opcode();\n+    __ evmasked_op(opc, bt, $mask$$KRegister, $dst$$XMMRegister,\n+                   $dst$$XMMRegister, $shift$$constant, true, vlen_enc);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct vrol_reg_masked(vec dst, vec src2, kReg mask) %{\n+  match(Set dst (RotateLeftV (Binary dst src2) mask));\n+  match(Set dst (RotateRightV (Binary dst src2) mask));\n+  format %{ \"vrotate_masked $dst, $dst, $src2, $mask\\t! rotate masked operation\" %}\n+  ins_encode %{\n+    int vlen_enc = vector_length_encoding(this);\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    int opc = this->ideal_Opcode();\n+    __ evmasked_op(opc, bt, $mask$$KRegister, $dst$$XMMRegister,\n+                   $dst$$XMMRegister, $src2$$XMMRegister, true, vlen_enc);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct vlshift_imm_masked(vec dst, immI8 shift, kReg mask) %{\n+  match(Set dst (LShiftVS (Binary dst (LShiftCntV shift)) mask));\n+  match(Set dst (LShiftVI (Binary dst (LShiftCntV shift)) mask));\n+  match(Set dst (LShiftVL (Binary dst (LShiftCntV shift)) mask));\n+  format %{ \"vplshift_imm_masked $dst, $dst, $shift, $mask\\t! lshift masked operation\" %}\n+  ins_encode %{\n+    int vlen_enc = vector_length_encoding(this);\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    int opc = this->ideal_Opcode();\n+    __ evmasked_op(opc, bt, $mask$$KRegister, $dst$$XMMRegister,\n+                   $dst$$XMMRegister, $shift$$constant, true, vlen_enc);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct vlshift_reg_masked(vec dst, vec src2, kReg mask) %{\n+  match(Set dst (LShiftVS (Binary dst src2) mask));\n+  match(Set dst (LShiftVI (Binary dst src2) mask));\n+  match(Set dst (LShiftVL (Binary dst src2) mask));\n+  format %{ \"vplshift_masked $dst, $dst, $src2, $mask\\t! lshift masked operation\" %}\n+  ins_encode %{\n+    int vlen_enc = vector_length_encoding(this);\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    int opc = this->ideal_Opcode();\n+    bool is_varshift = !VectorNode::is_vshift_cnt_opcode(in(2)->isa_Mach()->ideal_Opcode());\n+    __ evmasked_op(opc, bt, $mask$$KRegister, $dst$$XMMRegister,\n+                   $dst$$XMMRegister, $src2$$XMMRegister, true, vlen_enc, is_varshift);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct vlshift_mem_masked(vec dst, memory src2, kReg mask) %{\n+  match(Set dst (LShiftVS (Binary dst (LoadVector src2)) mask));\n+  match(Set dst (LShiftVI (Binary dst (LoadVector src2)) mask));\n+  match(Set dst (LShiftVL (Binary dst (LoadVector src2)) mask));\n+  format %{ \"vplshift_masked $dst, $dst, $src2, $mask\\t! lshift masked operation\" %}\n+  ins_encode %{\n+    int vlen_enc = vector_length_encoding(this);\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    int opc = this->ideal_Opcode();\n+    __ evmasked_op(opc, bt, $mask$$KRegister, $dst$$XMMRegister,\n+                   $dst$$XMMRegister, $src2$$Address, true, vlen_enc);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct vrshift_imm_masked(vec dst, immI8 shift, kReg mask) %{\n+  match(Set dst (RShiftVS (Binary dst (RShiftCntV shift)) mask));\n+  match(Set dst (RShiftVI (Binary dst (RShiftCntV shift)) mask));\n+  match(Set dst (RShiftVL (Binary dst (RShiftCntV shift)) mask));\n+  format %{ \"vprshift_imm_masked $dst, $dst, $shift, $mask\\t! rshift masked operation\" %}\n+  ins_encode %{\n+    int vlen_enc = vector_length_encoding(this);\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    int opc = this->ideal_Opcode();\n+    __ evmasked_op(opc, bt, $mask$$KRegister, $dst$$XMMRegister,\n+                   $dst$$XMMRegister, $shift$$constant, true, vlen_enc);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct vrshift_reg_masked(vec dst, vec src2, kReg mask) %{\n+  match(Set dst (RShiftVS (Binary dst src2) mask));\n+  match(Set dst (RShiftVI (Binary dst src2) mask));\n+  match(Set dst (RShiftVL (Binary dst src2) mask));\n+  format %{ \"vprshift_masked $dst, $dst, $src2, $mask\\t! rshift masked operation\" %}\n+  ins_encode %{\n+    int vlen_enc = vector_length_encoding(this);\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    int opc = this->ideal_Opcode();\n+    bool is_varshift = !VectorNode::is_vshift_cnt_opcode(in(2)->isa_Mach()->ideal_Opcode());\n+    __ evmasked_op(opc, bt, $mask$$KRegister, $dst$$XMMRegister,\n+                   $dst$$XMMRegister, $src2$$XMMRegister, true, vlen_enc, is_varshift);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct vrshift_mem_masked(vec dst, memory src2, kReg mask) %{\n+  match(Set dst (RShiftVS (Binary dst (LoadVector src2)) mask));\n+  match(Set dst (RShiftVI (Binary dst (LoadVector src2)) mask));\n+  match(Set dst (RShiftVL (Binary dst (LoadVector src2)) mask));\n+  format %{ \"vprshift_masked $dst, $dst, $src2, $mask\\t! rshift masked operation\" %}\n+  ins_encode %{\n+    int vlen_enc = vector_length_encoding(this);\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    int opc = this->ideal_Opcode();\n+    __ evmasked_op(opc, bt, $mask$$KRegister, $dst$$XMMRegister,\n+                   $dst$$XMMRegister, $src2$$Address, true, vlen_enc);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct vurshift_imm_masked(vec dst, immI8 shift, kReg mask) %{\n+  match(Set dst (URShiftVS (Binary dst (RShiftCntV shift)) mask));\n+  match(Set dst (URShiftVI (Binary dst (RShiftCntV shift)) mask));\n+  match(Set dst (URShiftVL (Binary dst (RShiftCntV shift)) mask));\n+  format %{ \"vpurshift_imm_masked $dst, $dst, $shift, $mask\\t! urshift masked operation\" %}\n+  ins_encode %{\n+    int vlen_enc = vector_length_encoding(this);\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    int opc = this->ideal_Opcode();\n+    __ evmasked_op(opc, bt, $mask$$KRegister, $dst$$XMMRegister,\n+                   $dst$$XMMRegister, $shift$$constant, true, vlen_enc);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct vurshift_reg_masked(vec dst, vec src2, kReg mask) %{\n+  match(Set dst (URShiftVS (Binary dst src2) mask));\n+  match(Set dst (URShiftVI (Binary dst src2) mask));\n+  match(Set dst (URShiftVL (Binary dst src2) mask));\n+  format %{ \"vpurshift_masked $dst, $dst, $src2, $mask\\t! urshift masked operation\" %}\n+  ins_encode %{\n+    int vlen_enc = vector_length_encoding(this);\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    int opc = this->ideal_Opcode();\n+    bool is_varshift = !VectorNode::is_vshift_cnt_opcode(in(2)->isa_Mach()->ideal_Opcode());\n+    __ evmasked_op(opc, bt, $mask$$KRegister, $dst$$XMMRegister,\n+                   $dst$$XMMRegister, $src2$$XMMRegister, true, vlen_enc, is_varshift);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct vurshift_mem_masked(vec dst, memory src2, kReg mask) %{\n+  match(Set dst (URShiftVS (Binary dst (LoadVector src2)) mask));\n+  match(Set dst (URShiftVI (Binary dst (LoadVector src2)) mask));\n+  match(Set dst (URShiftVL (Binary dst (LoadVector src2)) mask));\n+  format %{ \"vpurshift_masked $dst, $dst, $src2, $mask\\t! urshift masked operation\" %}\n+  ins_encode %{\n+    int vlen_enc = vector_length_encoding(this);\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    int opc = this->ideal_Opcode();\n+    __ evmasked_op(opc, bt, $mask$$KRegister, $dst$$XMMRegister,\n+                   $dst$$XMMRegister, $src2$$Address, true, vlen_enc);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct vmaxv_reg_masked(vec dst, vec src2, kReg mask) %{\n+  match(Set dst (MaxV (Binary dst src2) mask));\n+  format %{ \"vpmax_masked $dst, $dst, $src2, $mask\\t! max masked operation\" %}\n+  ins_encode %{\n+    int vlen_enc = vector_length_encoding(this);\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    int opc = this->ideal_Opcode();\n+    __ evmasked_op(opc, bt, $mask$$KRegister, $dst$$XMMRegister,\n+                   $dst$$XMMRegister, $src2$$XMMRegister, true, vlen_enc);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct vmaxv_mem_masked(vec dst, memory src2, kReg mask) %{\n+  match(Set dst (MaxV (Binary dst (LoadVector src2)) mask));\n+  format %{ \"vpmax_masked $dst, $dst, $src2, $mask\\t! max masked operation\" %}\n+  ins_encode %{\n+    int vlen_enc = vector_length_encoding(this);\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    int opc = this->ideal_Opcode();\n+    __ evmasked_op(opc, bt, $mask$$KRegister, $dst$$XMMRegister,\n+                   $dst$$XMMRegister, $src2$$Address, true, vlen_enc);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct vminv_reg_masked(vec dst, vec src2, kReg mask) %{\n+  match(Set dst (MinV (Binary dst src2) mask));\n+  format %{ \"vpmin_masked $dst, $dst, $src2, $mask\\t! min masked operation\" %}\n+  ins_encode %{\n+    int vlen_enc = vector_length_encoding(this);\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    int opc = this->ideal_Opcode();\n+    __ evmasked_op(opc, bt, $mask$$KRegister, $dst$$XMMRegister,\n+                   $dst$$XMMRegister, $src2$$XMMRegister, true, vlen_enc);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct vminv_mem_masked(vec dst, memory src2, kReg mask) %{\n+  match(Set dst (MinV (Binary dst (LoadVector src2)) mask));\n+  format %{ \"vpmin_masked $dst, $dst, $src2, $mask\\t! min masked operation\" %}\n+  ins_encode %{\n+    int vlen_enc = vector_length_encoding(this);\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    int opc = this->ideal_Opcode();\n+    __ evmasked_op(opc, bt, $mask$$KRegister, $dst$$XMMRegister,\n+                   $dst$$XMMRegister, $src2$$Address, true, vlen_enc);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct vrearrangev_reg_masked(vec dst, vec src2, kReg mask) %{\n+  match(Set dst (VectorRearrange (Binary dst src2) mask));\n+  format %{ \"vprearrange_masked $dst, $dst, $src2, $mask\\t! rearrange masked operation\" %}\n+  ins_encode %{\n+    int vlen_enc = vector_length_encoding(this);\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    int opc = this->ideal_Opcode();\n+    __ evmasked_op(opc, bt, $mask$$KRegister, $dst$$XMMRegister,\n+                   $dst$$XMMRegister, $src2$$XMMRegister, false, vlen_enc);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct vabs_masked(vec dst, kReg mask) %{\n+  match(Set dst (AbsVB dst mask));\n+  match(Set dst (AbsVS dst mask));\n+  match(Set dst (AbsVI dst mask));\n+  match(Set dst (AbsVL dst mask));\n+  format %{ \"vabs_masked $dst, $mask \\t! vabs masked operation\" %}\n+  ins_cost(100);\n+  ins_encode %{\n+    int vlen_enc = vector_length_encoding(this);\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    int opc = this->ideal_Opcode();\n+    __ evmasked_op(opc, bt, $mask$$KRegister, $dst$$XMMRegister,\n+                   $dst$$XMMRegister, $dst$$XMMRegister, true, vlen_enc);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct vfma_reg_masked(vec dst, vec src2, vec src3, kReg mask) %{\n+  match(Set dst (FmaVF (Binary dst src2) (Binary src3 mask)));\n+  match(Set dst (FmaVD (Binary dst src2) (Binary src3 mask)));\n+  format %{ \"vfma_masked $dst, $src2, $src3, $mask \\t! vfma masked operation\" %}\n+  ins_encode %{\n+    int vlen_enc = vector_length_encoding(this);\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    int opc = this->ideal_Opcode();\n+    __ evmasked_op(opc, bt, $mask$$KRegister, $dst$$XMMRegister,\n+                   $src2$$XMMRegister, $src3$$XMMRegister, true, vlen_enc);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct vfma_mem_masked(vec dst, vec src2, memory src3, kReg mask) %{\n+  match(Set dst (FmaVF (Binary dst src2) (Binary (LoadVector src3) mask)));\n+  match(Set dst (FmaVD (Binary dst src2) (Binary (LoadVector src3) mask)));\n+  format %{ \"vfma_masked $dst, $src2, $src3, $mask \\t! vfma masked operation\" %}\n+  ins_encode %{\n+    int vlen_enc = vector_length_encoding(this);\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    int opc = this->ideal_Opcode();\n+    __ evmasked_op(opc, bt, $mask$$KRegister, $dst$$XMMRegister,\n+                   $src2$$XMMRegister, $src3$$Address, true, vlen_enc);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct evcmp_masked(kReg dst, vec src1, vec src2, immI8 cond, kReg mask, rRegP scratch) %{\n+  match(Set dst (VectorMaskCmp (Binary src1 src2) (Binary cond mask)));\n+  effect(TEMP scratch);\n+  format %{ \"vcmp_masked $dst, $src1, $src2, $cond, $mask\\t! using $scratch as TEMP\" %}\n+  ins_encode %{\n+    assert(bottom_type()->isa_vectmask(), \"TypeVectMask expected\");\n+    int vlen_enc = vector_length_encoding(this, $src1);\n+    BasicType src1_elem_bt = Matcher::vector_element_basic_type(this, $src1);\n+\n+    \/\/ Comparison i\n+    switch (src1_elem_bt) {\n+      case T_BYTE: {\n+        bool is_unsigned = is_unsigned_booltest_pred($cond$$constant);\n+        Assembler::ComparisonPredicate cmp = booltest_pred_to_comparison_pred($cond$$constant);\n+        __ evpcmpb($dst$$KRegister, $mask$$KRegister, $src1$$XMMRegister, $src2$$XMMRegister, cmp, !is_unsigned, vlen_enc);\n+        break;\n+      }\n+      case T_SHORT: {\n+        bool is_unsigned = is_unsigned_booltest_pred($cond$$constant);\n+        Assembler::ComparisonPredicate cmp = booltest_pred_to_comparison_pred($cond$$constant);\n+        __ evpcmpw($dst$$KRegister, $mask$$KRegister, $src1$$XMMRegister, $src2$$XMMRegister, cmp, !is_unsigned, vlen_enc);\n+        break;\n+      }\n+      case T_INT: {\n+        bool is_unsigned = is_unsigned_booltest_pred($cond$$constant);\n+        Assembler::ComparisonPredicate cmp = booltest_pred_to_comparison_pred($cond$$constant);\n+        __ evpcmpd($dst$$KRegister, $mask$$KRegister, $src1$$XMMRegister, $src2$$XMMRegister, cmp, !is_unsigned, vlen_enc);\n+        break;\n+      }\n+      case T_LONG: {\n+        bool is_unsigned = is_unsigned_booltest_pred($cond$$constant);\n+        Assembler::ComparisonPredicate cmp = booltest_pred_to_comparison_pred($cond$$constant);\n+        __ evpcmpq($dst$$KRegister, $mask$$KRegister, $src1$$XMMRegister, $src2$$XMMRegister, cmp, !is_unsigned, vlen_enc);\n+        break;\n+      }\n+      case T_FLOAT: {\n+        Assembler::ComparisonPredicateFP cmp = booltest_pred_to_comparison_pred_fp($cond$$constant);\n+        __ evcmpps($dst$$KRegister, $mask$$KRegister, $src1$$XMMRegister, $src2$$XMMRegister, cmp, vlen_enc);\n+        break;\n+      }\n+      case T_DOUBLE: {\n+        Assembler::ComparisonPredicateFP cmp = booltest_pred_to_comparison_pred_fp($cond$$constant);\n+        __ evcmppd($dst$$KRegister, $mask$$KRegister, $src1$$XMMRegister, $src2$$XMMRegister, cmp, vlen_enc);\n+        break;\n+      }\n+      default: assert(false, \"%s\", type2name(src1_elem_bt)); break;\n+    }\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+#ifdef _LP64\n+instruct mask_all_evexI_imm(kReg dst, immI cnt, rRegL tmp) %{\n+  match(Set dst (MaskAll cnt));\n+  effect(TEMP_DEF dst, TEMP tmp);\n+  format %{ \"mask_all_evexI $dst, $cnt \\t! using $tmp as TEMP\" %}\n+  ins_encode %{\n+    int vec_len = Matcher::vector_length(this);\n+    if (VM_Version::supports_avx512bw()) {\n+      __ movq($tmp$$Register, $cnt$$constant);\n+      __ kmovql($dst$$KRegister, $tmp$$Register);\n+      __ kshiftrql($dst$$KRegister, $dst$$KRegister, 64 - vec_len);\n+    } else {\n+      assert(vec_len <= 16, \"\");\n+      __ movq($tmp$$Register, $cnt$$constant);\n+      __ kmovwl($dst$$KRegister, $tmp$$Register);\n+      __ kshiftrwl($dst$$KRegister, $dst$$KRegister, 16 - vec_len);\n+    }\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct mask_all_evexI(kReg dst, rRegI src, rRegL tmp) %{\n+  match(Set dst (MaskAll src));\n+  effect(TEMP_DEF dst, TEMP tmp);\n+  format %{ \"mask_all_evexI $dst, $src \\t! using $tmp as TEMP\" %}\n+  ins_encode %{\n+    int vec_len = Matcher::vector_length(this);\n+    if (VM_Version::supports_avx512bw()) {\n+      __ movslq($tmp$$Register, $src$$Register);\n+      __ kmovql($dst$$KRegister, $tmp$$Register);\n+      __ kshiftrql($dst$$KRegister, $dst$$KRegister, 64 - vec_len);\n+    } else {\n+      assert(vec_len <= 16, \"\");\n+      __ kmovwl($dst$$KRegister, $src$$Register);\n+      __ kshiftrwl($dst$$KRegister, $dst$$KRegister, 16 - vec_len);\n+    }\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct mask_all_evexL(kReg dst, rRegL src) %{\n+  match(Set dst (MaskAll src));\n+  effect(TEMP_DEF dst);\n+  format %{ \"mask_all_evexL $dst, $src \\t! mask all operation\" %}\n+  ins_encode %{\n+    int vec_len = Matcher::vector_length(this);\n+    if (VM_Version::supports_avx512bw()) {\n+      __ kmovql($dst$$KRegister, $src$$Register);\n+      __ kshiftrql($dst$$KRegister, $dst$$KRegister, 64 - vec_len);\n+    } else {\n+      assert(vec_len <= 16, \"\");\n+      __ kmovwl($dst$$KRegister, $src$$Register);\n+      __ kshiftrwl($dst$$KRegister, $dst$$KRegister, 16 - vec_len);\n+    }\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct mask_not_immLT8(kReg dst, kReg src, rRegI rtmp, kReg ktmp, immI_M1 cnt) %{\n+  predicate(Matcher::vector_length(n) < 8 && VM_Version::supports_avx512dq());\n+  match(Set dst (XorVMask src (MaskAll cnt)));\n+  effect(TEMP_DEF dst, TEMP rtmp, TEMP ktmp);\n+  format %{ \"mask_not_LT8 $dst, $src, $cnt \\t!using $ktmp and $rtmp as TEMP\" %}\n+  ins_encode %{\n+    uint masklen = Matcher::vector_length(this);\n+    __ knot(masklen, $dst$$KRegister, $src$$KRegister, $ktmp$$KRegister, $rtmp$$Register);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct mask_not_imm(kReg dst, kReg src, immI_M1 cnt) %{\n+  predicate((Matcher::vector_length(n) == 8 && VM_Version::supports_avx512dq()) ||\n+            (Matcher::vector_length(n) == 16) ||\n+            (Matcher::vector_length(n) > 16 && VM_Version::supports_avx512bw()));\n+  match(Set dst (XorVMask src (MaskAll cnt)));\n+  format %{ \"mask_not $dst, $src, $cnt \\t! mask not operation\" %}\n+  ins_encode %{\n+    uint masklen = Matcher::vector_length(this);\n+    __ knot(masklen, $dst$$KRegister, $src$$KRegister);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+#endif\n+\n+instruct mask_opers_evex(kReg dst, kReg src1, kReg src2, kReg kscratch) %{\n+  match(Set dst (AndVMask src1 src2));\n+  match(Set dst (OrVMask src1 src2));\n+  match(Set dst (XorVMask src1 src2));\n+  effect(TEMP kscratch);\n+  format %{ \"mask_opers_evex $dst, $src1, $src2\\t! using $kscratch as TEMP\" %}\n+  ins_encode %{\n+    const MachNode* mask1 = static_cast<const MachNode*>(this->in(this->operand_index($src1)));\n+    const MachNode* mask2 = static_cast<const MachNode*>(this->in(this->operand_index($src2)));\n+    assert(0 == Type::cmp(mask1->bottom_type(), mask2->bottom_type()), \"\");\n+    uint masklen = Matcher::vector_length(this);\n+    masklen = (masklen < 16 && !VM_Version::supports_avx512dq()) ? 16 : masklen;\n+    __ masked_op(this->ideal_Opcode(), masklen, $dst$$KRegister, $src1$$KRegister, $src2$$KRegister);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct castMM(kReg dst)\n+%{\n+  match(Set dst (CastVV dst));\n+\n+  size(0);\n+  format %{ \"# castVV of $dst\" %}\n+  ins_encode(\/* empty encoding *\/);\n+  ins_cost(0);\n+  ins_pipe(empty);\n+%}\n+\n","filename":"src\/hotspot\/cpu\/x86\/x86.ad","additions":1318,"deletions":180,"binary":false,"changes":1498,"status":"modified"},{"patch":"@@ -13153,1 +13153,19 @@\n-\/\/ Compare 2 longs and CMOVE ints.\n+instruct cmovII_reg_LTGE_U(cmpOpU cmp, flagsReg_ulong_LTGE flags, rRegI dst, rRegI src) %{\n+  predicate(VM_Version::supports_cmov() && ( _kids[0]->_kids[0]->_leaf->as_Bool()->_test._test == BoolTest::lt || _kids[0]->_kids[0]->_leaf->as_Bool()->_test._test == BoolTest::ge ));\n+  match(Set dst (CMoveI (Binary cmp flags) (Binary dst src)));\n+  ins_cost(200);\n+  expand %{\n+    cmovII_reg_LTGE(cmp, flags, dst, src);\n+  %}\n+%}\n+\n+instruct cmovII_mem_LTGE_U(cmpOpU cmp, flagsReg_ulong_LTGE flags, rRegI dst, memory src) %{\n+  predicate(VM_Version::supports_cmov() && ( _kids[0]->_kids[0]->_leaf->as_Bool()->_test._test == BoolTest::lt || _kids[0]->_kids[0]->_leaf->as_Bool()->_test._test == BoolTest::ge ));\n+  match(Set dst (CMoveI (Binary cmp flags) (Binary dst (LoadI src))));\n+  ins_cost(250);\n+  expand %{\n+    cmovII_mem_LTGE(cmp, flags, dst, src);\n+  %}\n+%}\n+\n+\/\/ Compare 2 longs and CMOVE ptrs.\n@@ -13316,1 +13334,19 @@\n-\/\/ Compare 2 longs and CMOVE ints.\n+instruct cmovII_reg_EQNE_U(cmpOpU cmp, flagsReg_ulong_EQNE flags, rRegI dst, rRegI src) %{\n+  predicate(VM_Version::supports_cmov() && ( _kids[0]->_kids[0]->_leaf->as_Bool()->_test._test == BoolTest::eq || _kids[0]->_kids[0]->_leaf->as_Bool()->_test._test == BoolTest::ne ));\n+  match(Set dst (CMoveI (Binary cmp flags) (Binary dst src)));\n+  ins_cost(200);\n+  expand %{\n+    cmovII_reg_EQNE(cmp, flags, dst, src);\n+  %}\n+%}\n+\n+instruct cmovII_mem_EQNE_U(cmpOpU cmp, flagsReg_ulong_EQNE flags, rRegI dst, memory src) %{\n+  predicate(VM_Version::supports_cmov() && ( _kids[0]->_kids[0]->_leaf->as_Bool()->_test._test == BoolTest::eq || _kids[0]->_kids[0]->_leaf->as_Bool()->_test._test == BoolTest::ne ));\n+  match(Set dst (CMoveI (Binary cmp flags) (Binary dst (LoadI src))));\n+  ins_cost(250);\n+  expand %{\n+    cmovII_mem_EQNE(cmp, flags, dst, src);\n+  %}\n+%}\n+\n+\/\/ Compare 2 longs and CMOVE ptrs.\n@@ -13472,5 +13508,3 @@\n-  format %{ \"CMOV$cmp $dst.lo,$src.lo\\n\\t\"\n-            \"CMOV$cmp $dst.hi,$src.hi\" %}\n-  opcode(0x0F,0x40);\n-  ins_encode( enc_cmov(cmp), RegReg_Lo2( dst, src ), enc_cmov(cmp), RegReg_Hi2( dst, src ) );\n-  ins_pipe( pipe_cmov_reg_long );\n+  expand %{\n+    cmovLL_reg_LEGT(cmp, flags, dst, src);\n+  %}\n@@ -13483,5 +13517,3 @@\n-  format %{ \"CMOV$cmp $dst.lo,$src.lo\\n\\t\"\n-            \"CMOV$cmp $dst.hi,$src.hi+4\" %}\n-  opcode(0x0F,0x40);\n-  ins_encode( enc_cmov(cmp), RegMem(dst, src), enc_cmov(cmp), RegMem_Hi(dst, src) );\n-  ins_pipe( pipe_cmov_reg_long );\n+  expand %{\n+    cmovLL_mem_LEGT(cmp, flags, dst, src);\n+  %}\n@@ -13511,0 +13543,18 @@\n+instruct cmovII_reg_LEGT_U(cmpOpU_commute cmp, flagsReg_ulong_LEGT flags, rRegI dst, rRegI src) %{\n+  predicate(VM_Version::supports_cmov() && ( _kids[0]->_kids[0]->_leaf->as_Bool()->_test._test == BoolTest::le || _kids[0]->_kids[0]->_leaf->as_Bool()->_test._test == BoolTest::gt ));\n+  match(Set dst (CMoveI (Binary cmp flags) (Binary dst src)));\n+  ins_cost(200);\n+  expand %{\n+    cmovII_reg_LEGT(cmp, flags, dst, src);\n+  %}\n+%}\n+\n+instruct cmovII_mem_LEGT_U(cmpOpU_commute cmp, flagsReg_ulong_LEGT flags, rRegI dst, memory src) %{\n+  predicate(VM_Version::supports_cmov() && ( _kids[0]->_kids[0]->_leaf->as_Bool()->_test._test == BoolTest::le || _kids[0]->_kids[0]->_leaf->as_Bool()->_test._test == BoolTest::gt ));\n+  match(Set dst (CMoveI (Binary cmp flags) (Binary dst (LoadI src))));\n+  ins_cost(250);\n+  expand %{\n+    cmovII_mem_LEGT(cmp, flags, dst, src);\n+  %}\n+%}\n+\n","filename":"src\/hotspot\/cpu\/x86\/x86_32.ad","additions":62,"deletions":12,"binary":false,"changes":74,"status":"modified"},{"patch":"@@ -76,2 +76,1 @@\n-                                                BasicType ret_type,\n-                                                address critical_entry) {\n+                                                BasicType ret_type) {\n","filename":"src\/hotspot\/cpu\/zero\/sharedRuntime_zero.cpp","additions":1,"deletions":2,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -119,1 +119,0 @@\n-  UNSUPPORTED_OPTION(CriticalJNINatives);\n","filename":"src\/hotspot\/cpu\/zero\/vm_version_zero.cpp","additions":1,"deletions":2,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -189,1 +189,5 @@\n-      BytecodeInterpreter::run<true>(istate);\n+      if (RewriteBytecodes) {\n+        BytecodeInterpreter::run<true, true>(istate);\n+      } else {\n+        BytecodeInterpreter::run<true, false>(istate);\n+      }\n@@ -191,1 +195,5 @@\n-      BytecodeInterpreter::run<false>(istate);\n+      if (RewriteBytecodes) {\n+        BytecodeInterpreter::run<false, true>(istate);\n+      } else {\n+        BytecodeInterpreter::run<false, false>(istate);\n+      }\n","filename":"src\/hotspot\/cpu\/zero\/zeroInterpreter_zero.cpp","additions":10,"deletions":2,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2012, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -272,0 +272,1 @@\n+  if( strcmp(opType,\"LoadVectorGatherMasked\")==0 )  return Form::idealV;\n@@ -290,0 +291,1 @@\n+  if( strcmp(opType,\"StoreVectorScatterMasked\")==0 )  return Form::idealV;\n","filename":"src\/hotspot\/share\/adlc\/forms.cpp","additions":3,"deletions":1,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -2284,0 +2284,1 @@\n+  if (strcmp(name, \"RegVectMask\") == 0) size = globalAD->get_preproc_def(\"AARCH64\") ? 1 : 2;\n@@ -3517,1 +3518,2 @@\n-    \"StoreVector\", \"LoadVector\", \"LoadVectorGather\", \"StoreVectorScatter\", \"LoadVectorMasked\", \"StoreVectorMasked\",\n+    \"StoreVector\", \"LoadVector\", \"LoadVectorMasked\", \"StoreVectorMasked\",\n+    \"LoadVectorGather\", \"StoreVectorScatter\", \"LoadVectorGatherMasked\", \"StoreVectorScatterMasked\",\n@@ -3821,1 +3823,1 @@\n-\/\/-------------------------- has_commutative_op -------------------------------\n+\/\/-------------------------- count_commutative_op -------------------------------\n@@ -3827,1 +3829,0 @@\n-    \"AddVB\",\"AddVS\",\"AddVI\",\"AddVL\",\"AddVF\",\"AddVD\",\n@@ -3829,1 +3830,0 @@\n-    \"AndV\",\n@@ -3831,1 +3831,0 @@\n-    \"MaxV\", \"MinV\",\n@@ -3833,1 +3832,0 @@\n-    \"MulVB\",\"MulVS\",\"MulVI\",\"MulVL\",\"MulVF\",\"MulVD\",\n@@ -3835,3 +3833,1 @@\n-    \"OrV\",\n-    \"XorI\",\"XorL\",\n-    \"XorV\"\n+    \"XorI\",\"XorL\"\n@@ -3839,1 +3835,0 @@\n-  int cnt = sizeof(commut_op_list)\/sizeof(char*);\n@@ -3841,1 +3836,8 @@\n-  if( _lChild && _rChild && (_lChild->_lChild || _rChild->_lChild) ) {\n+  static const char *commut_vector_op_list[] = {\n+    \"AddVB\", \"AddVS\", \"AddVI\", \"AddVL\", \"AddVF\", \"AddVD\",\n+    \"MulVB\", \"MulVS\", \"MulVI\", \"MulVL\", \"MulVF\", \"MulVD\",\n+    \"AndV\", \"OrV\", \"XorV\",\n+    \"MaxV\", \"MinV\"\n+  };\n+\n+  if (_lChild && _rChild && (_lChild->_lChild || _rChild->_lChild)) {\n@@ -3844,1 +3846,1 @@\n-    if( _rChild->_lChild == NULL && _rChild->_rChild == NULL ) {\n+    if (_rChild->_lChild == NULL && _rChild->_rChild == NULL) {\n@@ -3847,3 +3849,3 @@\n-      if ( form ) {\n-        OperandForm  *oper = form->is_operand();\n-        if( oper && oper->interface_type(globals) == Form::constant_interface )\n+      if (form) {\n+        OperandForm *oper = form->is_operand();\n+        if (oper && oper->interface_type(globals) == Form::constant_interface)\n@@ -3853,5 +3855,19 @@\n-    if( !is_const ) {\n-      for( int i=0; i<cnt; i++ ) {\n-        if( strcmp(_opType, commut_op_list[i]) == 0 ) {\n-          count++;\n-          _commutative_id = count; \/\/ id should be > 0\n+\n+    if (!is_const) {\n+      int scalar_cnt = sizeof(commut_op_list)\/sizeof(char*);\n+      int vector_cnt = sizeof(commut_vector_op_list)\/sizeof(char*);\n+      bool matched = false;\n+\n+      \/\/ Check the commutative vector op first. It's noncommutative if\n+      \/\/ the current node is a masked vector op, since a mask value\n+      \/\/ is added to the original vector node's input list and the original\n+      \/\/ first two inputs are packed into one BinaryNode. So don't swap\n+      \/\/ if one of the operands is a BinaryNode.\n+      for (int i = 0; i < vector_cnt; i++) {\n+        if (strcmp(_opType, commut_vector_op_list[i]) == 0) {\n+          if (strcmp(_lChild->_opType, \"Binary\") != 0 &&\n+              strcmp(_rChild->_opType, \"Binary\") != 0) {\n+            count++;\n+            _commutative_id = count; \/\/ id should be > 0\n+          }\n+          matched = true;\n@@ -3861,0 +3877,12 @@\n+\n+      \/\/ Then check the scalar op if the current op is not in\n+      \/\/ the commut_vector_op_list.\n+      if (!matched) {\n+        for (int i = 0; i < scalar_cnt; i++) {\n+          if (strcmp(_opType, commut_op_list[i]) == 0) {\n+            count++;\n+            _commutative_id = count; \/\/ id should be > 0\n+            break;\n+          }\n+        }\n+      }\n@@ -3863,1 +3891,1 @@\n-  if( _lChild )\n+  if (_lChild)\n@@ -3865,1 +3893,1 @@\n-  if( _rChild )\n+  if (_rChild)\n@@ -4091,0 +4119,1 @@\n+        strcmp(opType,\"MaskAll\")==0 ||\n@@ -4203,1 +4232,1 @@\n-    \"LoadVectorGather\", \"StoreVectorScatter\",\n+    \"LoadVectorGather\", \"StoreVectorScatter\", \"LoadVectorGatherMasked\", \"StoreVectorScatterMasked\",\n@@ -4210,0 +4239,2 @@\n+    \/\/ Next are vector mask ops.\n+    \"MaskAll\", \"AndVMask\", \"OrVMask\", \"XorVMask\", \"VectorMaskCast\",\n@@ -4212,2 +4243,1 @@\n-    \"ExtractB\",\"ExtractUB\",\"ExtractC\",\"ExtractS\",\"ExtractI\",\"ExtractL\",\"ExtractF\",\"ExtractD\",\n-    \"VectorMaskCast\"\n+    \"ExtractB\",\"ExtractUB\",\"ExtractC\",\"ExtractS\",\"ExtractI\",\"ExtractL\",\"ExtractF\",\"ExtractD\"\n","filename":"src\/hotspot\/share\/adlc\/formssel.cpp","additions":55,"deletions":25,"binary":false,"changes":80,"status":"modified"},{"patch":"@@ -52,1 +52,0 @@\n-class LIR_OprDesc;\n@@ -56,1 +55,0 @@\n-typedef LIR_OprDesc* LIR_Opr;\n","filename":"src\/hotspot\/share\/c1\/c1_Compilation.hpp","additions":0,"deletions":2,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -65,4 +65,0 @@\n-class LIR_OprDesc;\n-typedef LIR_OprDesc* LIR_Opr;\n-\n-\n@@ -86,1 +82,1 @@\n-  friend class LIR_OprDesc;\n+  friend class LIR_Opr;\n","filename":"src\/hotspot\/share\/c1\/c1_FrameMap.hpp","additions":1,"deletions":5,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -38,2 +38,0 @@\n-class LIR_OprDesc;\n-typedef LIR_OprDesc* LIR_Opr;\n","filename":"src\/hotspot\/share\/c1\/c1_Instruction.hpp","additions":0,"deletions":2,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -36,1 +36,1 @@\n-Register LIR_OprDesc::as_register() const {\n+Register LIR_Opr::as_register() const {\n@@ -40,1 +40,1 @@\n-Register LIR_OprDesc::as_register_lo() const {\n+Register LIR_Opr::as_register_lo() const {\n@@ -44,1 +44,1 @@\n-Register LIR_OprDesc::as_register_hi() const {\n+Register LIR_Opr::as_register_hi() const {\n@@ -49,0 +49,1 @@\n+LIR_Opr LIR_OprFact::nullOpr = LIR_Opr();\n@@ -95,1 +96,1 @@\n-char LIR_OprDesc::type_char(BasicType t) {\n+char LIR_Opr::type_char(BasicType t) {\n@@ -123,1 +124,1 @@\n-void LIR_OprDesc::validate_type() const {\n+void LIR_Opr::validate_type() const {\n@@ -175,1 +176,1 @@\n-bool LIR_OprDesc::is_oop() const {\n+bool LIR_Opr::is_oop() const {\n@@ -1375,1 +1376,1 @@\n-  \/\/ guarantee(sizeof(LIR_OprDesc) == wordSize, \"may not have a v-table\");\n+  \/\/ guarantee(sizeof(LIR_Opr) == wordSize, \"may not have a v-table\");\n@@ -1450,2 +1451,2 @@\n-\/\/ LIR_OprDesc\n-void LIR_OprDesc::print() const {\n+\/\/ LIR_Opr\n+void LIR_Opr::print() const {\n@@ -1455,1 +1456,1 @@\n-void LIR_OprDesc::print(outputStream* out) const {\n+void LIR_Opr::print(outputStream* out) const {\n","filename":"src\/hotspot\/share\/c1\/c1_LIR.cpp","additions":11,"deletions":10,"binary":false,"changes":21,"status":"modified"},{"patch":"@@ -49,1 +49,0 @@\n-\/\/  LIR_OprDesc\n@@ -54,1 +53,0 @@\n-class LIR_OprDesc;\n@@ -59,0 +57,1 @@\n+class LIR_Opr;\n@@ -60,2 +59,0 @@\n-\n-typedef LIR_OprDesc* LIR_Opr;\n@@ -68,1 +65,1 @@\n-\/\/ define LIR_OprPtr early so LIR_OprDesc can refer to it\n+\/\/ define LIR_OprPtr early so LIR_Opr can refer to it\n@@ -187,1 +184,1 @@\n-\/\/ The class LIR_OprDesc represents a LIR instruction operand;\n+\/\/ The class LIR_Opr represents a LIR instruction operand;\n@@ -190,2 +187,3 @@\n-\/\/ structures (see above).\n-\/\/ Registers and stack locations are inlined into the this pointer\n+\/\/ structures (see above), and pointers are stored in the _value field (cast to\n+\/\/ an intptr_t).\n+\/\/ Registers and stack locations are represented inline as integers.\n@@ -194,1 +192,7 @@\n-class LIR_OprDesc: public CompilationResourceObj {\n+\/\/ Previously, this class was derived from CompilationResourceObj.\n+\/\/ However, deriving from any of the \"Obj\" types in allocation.hpp seems\n+\/\/ detrimental, since in some build modes it would add a vtable to this class,\n+\/\/ which make it no longer be a 1-word trivially-copyable wrapper object,\n+\/\/ which is the entire point of it.\n+\n+class LIR_Opr {\n@@ -209,0 +213,1 @@\n+  intptr_t _value;\n@@ -210,1 +215,1 @@\n-  intptr_t value() const                         { return (intptr_t) this; }\n+  intptr_t value() const                         { return _value; }\n@@ -282,0 +287,13 @@\n+  LIR_Opr() : _value(0) {}\n+  LIR_Opr(intptr_t val) : _value(val) {}\n+  LIR_Opr(LIR_OprPtr *val) : _value(reinterpret_cast<intptr_t>(val)) {}\n+  bool operator==(const LIR_Opr &other) const { return _value == other._value; }\n+  bool operator!=(const LIR_Opr &other) const { return _value != other._value; }\n+  explicit operator bool() const { return _value != 0; }\n+\n+  \/\/ UGLY HACK: make this value object look like a pointer (to itself). This\n+  \/\/ operator overload should be removed, and all callers updated from\n+  \/\/ `opr->fn()` to `opr.fn()`.\n+  const LIR_Opr* operator->() const { return this; }\n+  LIR_Opr* operator->() { return this; }\n+\n@@ -288,0 +306,1 @@\n+  static inline LIR_Opr nullOpr();\n@@ -346,1 +365,1 @@\n-  bool is_equal(LIR_Opr opr) const         { return this == opr; }\n+  bool is_equal(LIR_Opr opr) const         { return *this == opr; }\n@@ -425,1 +444,1 @@\n-  LIR_OprPtr* pointer()  const                   { assert(is_pointer(), \"type check\");      return (LIR_OprPtr*)this; }\n+  LIR_OprPtr* pointer() const { assert(_value != 0 && is_pointer(), \"nullness and type check\"); return (LIR_OprPtr*)_value; }\n@@ -462,2 +481,1 @@\n-\n-inline LIR_OprDesc::OprType as_OprType(BasicType type) {\n+inline LIR_Opr::OprType as_OprType(BasicType type) {\n@@ -465,4 +483,4 @@\n-  case T_INT:      return LIR_OprDesc::int_type;\n-  case T_LONG:     return LIR_OprDesc::long_type;\n-  case T_FLOAT:    return LIR_OprDesc::float_type;\n-  case T_DOUBLE:   return LIR_OprDesc::double_type;\n+  case T_INT:      return LIR_Opr::int_type;\n+  case T_LONG:     return LIR_Opr::long_type;\n+  case T_FLOAT:    return LIR_Opr::float_type;\n+  case T_DOUBLE:   return LIR_Opr::double_type;\n@@ -470,3 +488,3 @@\n-  case T_ARRAY:    return LIR_OprDesc::object_type;\n-  case T_ADDRESS:  return LIR_OprDesc::address_type;\n-  case T_METADATA: return LIR_OprDesc::metadata_type;\n+  case T_ARRAY:    return LIR_Opr::object_type;\n+  case T_ADDRESS:  return LIR_Opr::address_type;\n+  case T_METADATA: return LIR_Opr::metadata_type;\n@@ -474,1 +492,1 @@\n-  default: ShouldNotReachHere(); return LIR_OprDesc::unknown_type;\n+  default: ShouldNotReachHere(); return LIR_Opr::unknown_type;\n@@ -478,1 +496,1 @@\n-inline BasicType as_BasicType(LIR_OprDesc::OprType t) {\n+inline BasicType as_BasicType(LIR_Opr::OprType t) {\n@@ -480,8 +498,8 @@\n-  case LIR_OprDesc::int_type:     return T_INT;\n-  case LIR_OprDesc::long_type:    return T_LONG;\n-  case LIR_OprDesc::float_type:   return T_FLOAT;\n-  case LIR_OprDesc::double_type:  return T_DOUBLE;\n-  case LIR_OprDesc::object_type:  return T_OBJECT;\n-  case LIR_OprDesc::address_type: return T_ADDRESS;\n-  case LIR_OprDesc::metadata_type:return T_METADATA;\n-  case LIR_OprDesc::unknown_type: \/\/ fall through\n+  case LIR_Opr::int_type:     return T_INT;\n+  case LIR_Opr::long_type:    return T_LONG;\n+  case LIR_Opr::float_type:   return T_FLOAT;\n+  case LIR_Opr::double_type:  return T_DOUBLE;\n+  case LIR_Opr::object_type:  return T_OBJECT;\n+  case LIR_Opr::address_type: return T_ADDRESS;\n+  case LIR_Opr::metadata_type:return T_METADATA;\n+  case LIR_Opr::unknown_type: \/\/ fall through\n@@ -525,1 +543,1 @@\n-     , _index(LIR_OprDesc::illegalOpr())\n+     , _index(LIR_Opr::illegalOpr())\n@@ -532,1 +550,1 @@\n-     , _index(LIR_OprDesc::illegalOpr())\n+     , _index(LIR_Opr::illegalOpr())\n@@ -573,0 +591,1 @@\n+  static LIR_Opr nullOpr;\n@@ -575,4 +594,4 @@\n-    return (LIR_Opr)(intptr_t)((reg  << LIR_OprDesc::reg1_shift) |\n-                               LIR_OprDesc::int_type             |\n-                               LIR_OprDesc::cpu_register         |\n-                               LIR_OprDesc::single_size);\n+    return (LIR_Opr)(intptr_t)((reg  << LIR_Opr::reg1_shift) |\n+                               LIR_Opr::int_type             |\n+                               LIR_Opr::cpu_register         |\n+                               LIR_Opr::single_size);\n@@ -581,4 +600,4 @@\n-    return (LIR_Opr)(intptr_t)((reg  << LIR_OprDesc::reg1_shift) |\n-                               LIR_OprDesc::object_type          |\n-                               LIR_OprDesc::cpu_register         |\n-                               LIR_OprDesc::single_size);\n+    return (LIR_Opr)(intptr_t)((reg  << LIR_Opr::reg1_shift) |\n+                               LIR_Opr::object_type          |\n+                               LIR_Opr::cpu_register         |\n+                               LIR_Opr::single_size);\n@@ -587,4 +606,4 @@\n-    return (LIR_Opr)(intptr_t)((reg  << LIR_OprDesc::reg1_shift) |\n-                               LIR_OprDesc::address_type         |\n-                               LIR_OprDesc::cpu_register         |\n-                               LIR_OprDesc::single_size);\n+    return (LIR_Opr)(intptr_t)((reg  << LIR_Opr::reg1_shift) |\n+                               LIR_Opr::address_type         |\n+                               LIR_Opr::cpu_register         |\n+                               LIR_Opr::single_size);\n@@ -593,4 +612,4 @@\n-    return (LIR_Opr)(intptr_t)((reg  << LIR_OprDesc::reg1_shift) |\n-                               LIR_OprDesc::metadata_type        |\n-                               LIR_OprDesc::cpu_register         |\n-                               LIR_OprDesc::single_size);\n+    return (LIR_Opr)(intptr_t)((reg  << LIR_Opr::reg1_shift) |\n+                               LIR_Opr::metadata_type        |\n+                               LIR_Opr::cpu_register         |\n+                               LIR_Opr::single_size);\n@@ -600,5 +619,5 @@\n-    return (LIR_Opr)(intptr_t)((reg1 << LIR_OprDesc::reg1_shift) |\n-                               (reg2 << LIR_OprDesc::reg2_shift) |\n-                               LIR_OprDesc::long_type            |\n-                               LIR_OprDesc::cpu_register         |\n-                               LIR_OprDesc::double_size);\n+    return (LIR_Opr)(intptr_t)((reg1 << LIR_Opr::reg1_shift) |\n+                               (reg2 << LIR_Opr::reg2_shift) |\n+                               LIR_Opr::long_type            |\n+                               LIR_Opr::cpu_register         |\n+                               LIR_Opr::double_size);\n@@ -608,4 +627,4 @@\n-    return (LIR_Opr)(intptr_t)((reg  << LIR_OprDesc::reg1_shift) |\n-                               LIR_OprDesc::float_type           |\n-                               LIR_OprDesc::fpu_register         |\n-                               LIR_OprDesc::single_size);\n+    return (LIR_Opr)(intptr_t)((reg  << LIR_Opr::reg1_shift) |\n+                               LIR_Opr::float_type           |\n+                               LIR_Opr::fpu_register         |\n+                               LIR_Opr::single_size);\n@@ -619,4 +638,4 @@\n-    return (LIR_Opr)(intptr_t)((reg  << LIR_OprDesc::reg1_shift) |\n-                               LIR_OprDesc::float_type           |\n-                               LIR_OprDesc::cpu_register         |\n-                               LIR_OprDesc::single_size);\n+    return (LIR_Opr)(intptr_t)((reg  << LIR_Opr::reg1_shift) |\n+                               LIR_Opr::float_type           |\n+                               LIR_Opr::cpu_register         |\n+                               LIR_Opr::single_size);\n@@ -625,5 +644,5 @@\n-    return (LIR_Opr)(intptr_t)((reg1 << LIR_OprDesc::reg1_shift) |\n-                               (reg2 << LIR_OprDesc::reg2_shift) |\n-                               LIR_OprDesc::double_type          |\n-                               LIR_OprDesc::cpu_register         |\n-                               LIR_OprDesc::double_size);\n+    return (LIR_Opr)(intptr_t)((reg1 << LIR_Opr::reg1_shift) |\n+                               (reg2 << LIR_Opr::reg2_shift) |\n+                               LIR_Opr::double_type          |\n+                               LIR_Opr::cpu_register         |\n+                               LIR_Opr::double_size);\n@@ -635,5 +654,5 @@\n-    return (LIR_Opr)(intptr_t)((reg << LIR_OprDesc::reg1_shift) |\n-                               LIR_OprDesc::float_type          |\n-                               LIR_OprDesc::fpu_register        |\n-                               LIR_OprDesc::single_size         |\n-                               LIR_OprDesc::is_xmm_mask);\n+    return (LIR_Opr)(intptr_t)((reg << LIR_Opr::reg1_shift) |\n+                               LIR_Opr::float_type          |\n+                               LIR_Opr::fpu_register        |\n+                               LIR_Opr::single_size         |\n+                               LIR_Opr::is_xmm_mask);\n@@ -642,6 +661,6 @@\n-    return (LIR_Opr)(intptr_t)((reg << LIR_OprDesc::reg1_shift) |\n-                               (reg << LIR_OprDesc::reg2_shift) |\n-                               LIR_OprDesc::double_type         |\n-                               LIR_OprDesc::fpu_register        |\n-                               LIR_OprDesc::double_size         |\n-                               LIR_OprDesc::is_xmm_mask);\n+    return (LIR_Opr)(intptr_t)((reg << LIR_Opr::reg1_shift) |\n+                               (reg << LIR_Opr::reg2_shift) |\n+                               LIR_Opr::double_type         |\n+                               LIR_Opr::fpu_register        |\n+                               LIR_Opr::double_size         |\n+                               LIR_Opr::is_xmm_mask);\n@@ -652,1 +671,1 @@\n-    if (index > LIR_OprDesc::vreg_max) {\n+    if (index > LIR_Opr::vreg_max) {\n@@ -661,5 +680,5 @@\n-        res = (LIR_Opr)(intptr_t)((index << LIR_OprDesc::data_shift)  |\n-                                            LIR_OprDesc::object_type  |\n-                                            LIR_OprDesc::cpu_register |\n-                                            LIR_OprDesc::single_size  |\n-                                            LIR_OprDesc::virtual_mask);\n+        res = (LIR_Opr)(intptr_t)((index << LIR_Opr::data_shift)  |\n+                                            LIR_Opr::object_type  |\n+                                            LIR_Opr::cpu_register |\n+                                            LIR_Opr::single_size  |\n+                                            LIR_Opr::virtual_mask);\n@@ -669,5 +688,5 @@\n-        res = (LIR_Opr)(intptr_t)((index << LIR_OprDesc::data_shift)  |\n-                                            LIR_OprDesc::metadata_type|\n-                                            LIR_OprDesc::cpu_register |\n-                                            LIR_OprDesc::single_size  |\n-                                            LIR_OprDesc::virtual_mask);\n+        res = (LIR_Opr)(intptr_t)((index << LIR_Opr::data_shift)  |\n+                                            LIR_Opr::metadata_type|\n+                                            LIR_Opr::cpu_register |\n+                                            LIR_Opr::single_size  |\n+                                            LIR_Opr::virtual_mask);\n@@ -677,5 +696,5 @@\n-        res = (LIR_Opr)(intptr_t)((index << LIR_OprDesc::data_shift) |\n-                                  LIR_OprDesc::int_type              |\n-                                  LIR_OprDesc::cpu_register          |\n-                                  LIR_OprDesc::single_size           |\n-                                  LIR_OprDesc::virtual_mask);\n+        res = (LIR_Opr)(intptr_t)((index << LIR_Opr::data_shift) |\n+                                  LIR_Opr::int_type              |\n+                                  LIR_Opr::cpu_register          |\n+                                  LIR_Opr::single_size           |\n+                                  LIR_Opr::virtual_mask);\n@@ -685,5 +704,5 @@\n-        res = (LIR_Opr)(intptr_t)((index << LIR_OprDesc::data_shift) |\n-                                  LIR_OprDesc::address_type          |\n-                                  LIR_OprDesc::cpu_register          |\n-                                  LIR_OprDesc::single_size           |\n-                                  LIR_OprDesc::virtual_mask);\n+        res = (LIR_Opr)(intptr_t)((index << LIR_Opr::data_shift) |\n+                                  LIR_Opr::address_type          |\n+                                  LIR_Opr::cpu_register          |\n+                                  LIR_Opr::single_size           |\n+                                  LIR_Opr::virtual_mask);\n@@ -693,5 +712,5 @@\n-        res = (LIR_Opr)(intptr_t)((index << LIR_OprDesc::data_shift) |\n-                                  LIR_OprDesc::long_type             |\n-                                  LIR_OprDesc::cpu_register          |\n-                                  LIR_OprDesc::double_size           |\n-                                  LIR_OprDesc::virtual_mask);\n+        res = (LIR_Opr)(intptr_t)((index << LIR_Opr::data_shift) |\n+                                  LIR_Opr::long_type             |\n+                                  LIR_Opr::cpu_register          |\n+                                  LIR_Opr::double_size           |\n+                                  LIR_Opr::virtual_mask);\n@@ -702,5 +721,5 @@\n-        res = (LIR_Opr)(intptr_t)((index << LIR_OprDesc::data_shift) |\n-                                  LIR_OprDesc::float_type  |\n-                                  LIR_OprDesc::cpu_register |\n-                                  LIR_OprDesc::single_size |\n-                                  LIR_OprDesc::virtual_mask);\n+        res = (LIR_Opr)(intptr_t)((index << LIR_Opr::data_shift) |\n+                                  LIR_Opr::float_type  |\n+                                  LIR_Opr::cpu_register |\n+                                  LIR_Opr::single_size |\n+                                  LIR_Opr::virtual_mask);\n@@ -709,5 +728,5 @@\n-        res = (LIR_Opr)(intptr_t)((index << LIR_OprDesc::data_shift) |\n-                                  LIR_OprDesc::double_type |\n-                                  LIR_OprDesc::cpu_register |\n-                                  LIR_OprDesc::double_size |\n-                                  LIR_OprDesc::virtual_mask);\n+        res = (LIR_Opr)(intptr_t)((index << LIR_Opr::data_shift) |\n+                                  LIR_Opr::double_type |\n+                                  LIR_Opr::cpu_register |\n+                                  LIR_Opr::double_size |\n+                                  LIR_Opr::virtual_mask);\n@@ -717,5 +736,5 @@\n-        res = (LIR_Opr)(intptr_t)((index << LIR_OprDesc::data_shift) |\n-                                  LIR_OprDesc::float_type           |\n-                                  LIR_OprDesc::fpu_register         |\n-                                  LIR_OprDesc::single_size          |\n-                                  LIR_OprDesc::virtual_mask);\n+        res = (LIR_Opr)(intptr_t)((index << LIR_Opr::data_shift) |\n+                                  LIR_Opr::float_type           |\n+                                  LIR_Opr::fpu_register         |\n+                                  LIR_Opr::single_size          |\n+                                  LIR_Opr::virtual_mask);\n@@ -725,5 +744,5 @@\n-        T_DOUBLE: res = (LIR_Opr)(intptr_t)((index << LIR_OprDesc::data_shift) |\n-                                            LIR_OprDesc::double_type           |\n-                                            LIR_OprDesc::fpu_register          |\n-                                            LIR_OprDesc::double_size           |\n-                                            LIR_OprDesc::virtual_mask);\n+        T_DOUBLE: res = (LIR_Opr)(intptr_t)((index << LIR_Opr::data_shift) |\n+                                            LIR_Opr::double_type           |\n+                                            LIR_Opr::fpu_register          |\n+                                            LIR_Opr::double_size           |\n+                                            LIR_Opr::virtual_mask);\n@@ -738,2 +757,2 @@\n-    assert(index >= LIR_OprDesc::vreg_base, \"must start at vreg_base\");\n-    assert(index <= (max_jint >> LIR_OprDesc::data_shift), \"index is too big\");\n+    assert(index >= LIR_Opr::vreg_base, \"must start at vreg_base\");\n+    assert(index <= (max_jint >> LIR_Opr::data_shift), \"index is too big\");\n@@ -742,1 +761,1 @@\n-    LIR_OprDesc::OprType t = as_OprType(type);\n+    LIR_Opr::OprType t = as_OprType(type);\n@@ -744,1 +763,1 @@\n-    LIR_Opr old_res = (LIR_Opr)(intptr_t)((index << LIR_OprDesc::data_shift) |\n+    LIR_Opr old_res = (LIR_Opr)(intptr_t)((index << LIR_Opr::data_shift) |\n@@ -746,2 +765,2 @@\n-                               LIR_OprDesc::cpu_register |\n-                               LIR_OprDesc::size_for(type) | LIR_OprDesc::virtual_mask);\n+                               LIR_Opr::cpu_register |\n+                               LIR_Opr::size_for(type) | LIR_Opr::virtual_mask);\n@@ -749,3 +768,3 @@\n-    LIR_Opr old_res = (LIR_Opr)(intptr_t)((index << LIR_OprDesc::data_shift) | t |\n-                                          ((type == T_FLOAT || type == T_DOUBLE) ?  LIR_OprDesc::fpu_register : LIR_OprDesc::cpu_register) |\n-                               LIR_OprDesc::size_for(type) | LIR_OprDesc::virtual_mask);\n+    LIR_Opr old_res = (LIR_Opr)(intptr_t)((index << LIR_Opr::data_shift) | t |\n+                                          ((type == T_FLOAT || type == T_DOUBLE) ?  LIR_Opr::fpu_register : LIR_Opr::cpu_register) |\n+                               LIR_Opr::size_for(type) | LIR_Opr::virtual_mask);\n@@ -767,4 +786,4 @@\n-        res = (LIR_Opr)(intptr_t)((index << LIR_OprDesc::data_shift) |\n-                                  LIR_OprDesc::object_type           |\n-                                  LIR_OprDesc::stack_value           |\n-                                  LIR_OprDesc::single_size);\n+        res = (LIR_Opr)(intptr_t)((index << LIR_Opr::data_shift) |\n+                                  LIR_Opr::object_type           |\n+                                  LIR_Opr::stack_value           |\n+                                  LIR_Opr::single_size);\n@@ -774,4 +793,4 @@\n-        res = (LIR_Opr)(intptr_t)((index << LIR_OprDesc::data_shift) |\n-                                  LIR_OprDesc::metadata_type         |\n-                                  LIR_OprDesc::stack_value           |\n-                                  LIR_OprDesc::single_size);\n+        res = (LIR_Opr)(intptr_t)((index << LIR_Opr::data_shift) |\n+                                  LIR_Opr::metadata_type         |\n+                                  LIR_Opr::stack_value           |\n+                                  LIR_Opr::single_size);\n@@ -780,4 +799,4 @@\n-        res = (LIR_Opr)(intptr_t)((index << LIR_OprDesc::data_shift) |\n-                                  LIR_OprDesc::int_type              |\n-                                  LIR_OprDesc::stack_value           |\n-                                  LIR_OprDesc::single_size);\n+        res = (LIR_Opr)(intptr_t)((index << LIR_Opr::data_shift) |\n+                                  LIR_Opr::int_type              |\n+                                  LIR_Opr::stack_value           |\n+                                  LIR_Opr::single_size);\n@@ -787,4 +806,4 @@\n-        res = (LIR_Opr)(intptr_t)((index << LIR_OprDesc::data_shift) |\n-                                  LIR_OprDesc::address_type          |\n-                                  LIR_OprDesc::stack_value           |\n-                                  LIR_OprDesc::single_size);\n+        res = (LIR_Opr)(intptr_t)((index << LIR_Opr::data_shift) |\n+                                  LIR_Opr::address_type          |\n+                                  LIR_Opr::stack_value           |\n+                                  LIR_Opr::single_size);\n@@ -794,4 +813,4 @@\n-        res = (LIR_Opr)(intptr_t)((index << LIR_OprDesc::data_shift) |\n-                                  LIR_OprDesc::long_type             |\n-                                  LIR_OprDesc::stack_value           |\n-                                  LIR_OprDesc::double_size);\n+        res = (LIR_Opr)(intptr_t)((index << LIR_Opr::data_shift) |\n+                                  LIR_Opr::long_type             |\n+                                  LIR_Opr::stack_value           |\n+                                  LIR_Opr::double_size);\n@@ -801,4 +820,4 @@\n-        res = (LIR_Opr)(intptr_t)((index << LIR_OprDesc::data_shift) |\n-                                  LIR_OprDesc::float_type            |\n-                                  LIR_OprDesc::stack_value           |\n-                                  LIR_OprDesc::single_size);\n+        res = (LIR_Opr)(intptr_t)((index << LIR_Opr::data_shift) |\n+                                  LIR_Opr::float_type            |\n+                                  LIR_Opr::stack_value           |\n+                                  LIR_Opr::single_size);\n@@ -807,4 +826,4 @@\n-        res = (LIR_Opr)(intptr_t)((index << LIR_OprDesc::data_shift) |\n-                                  LIR_OprDesc::double_type           |\n-                                  LIR_OprDesc::stack_value           |\n-                                  LIR_OprDesc::double_size);\n+        res = (LIR_Opr)(intptr_t)((index << LIR_Opr::data_shift) |\n+                                  LIR_Opr::double_type           |\n+                                  LIR_Opr::stack_value           |\n+                                  LIR_Opr::double_size);\n@@ -818,1 +837,1 @@\n-    assert(index <= (max_jint >> LIR_OprDesc::data_shift), \"index is too big\");\n+    assert(index <= (max_jint >> LIR_Opr::data_shift), \"index is too big\");\n@@ -820,2 +839,2 @@\n-    LIR_Opr old_res = (LIR_Opr)(intptr_t)((index << LIR_OprDesc::data_shift) |\n-                                          LIR_OprDesc::stack_value           |\n+    LIR_Opr old_res = (LIR_Opr)(intptr_t)((index << LIR_Opr::data_shift) |\n+                                          LIR_Opr::stack_value           |\n@@ -823,1 +842,1 @@\n-                                          LIR_OprDesc::size_for(type));\n+                                          LIR_Opr::size_for(type));\n@@ -2449,1 +2468,3 @@\n-inline LIR_Opr LIR_OprDesc::illegalOpr()   { return LIR_OprFact::illegalOpr; };\n+inline LIR_Opr LIR_Opr::illegalOpr()   { return LIR_OprFact::illegalOpr; };\n+\n+inline LIR_Opr LIR_Opr::nullOpr()   { return LIR_OprFact::nullOpr; };\n","filename":"src\/hotspot\/share\/c1\/c1_LIR.hpp","additions":187,"deletions":166,"binary":false,"changes":353,"status":"modified"},{"patch":"@@ -1022,1 +1022,1 @@\n-  if (vreg_num + 20 >= LIR_OprDesc::vreg_max) {\n+  if (vreg_num + 20 >= LIR_Opr::vreg_max) {\n@@ -1024,1 +1024,1 @@\n-    if (vreg_num + 2 >= LIR_OprDesc::vreg_max) {\n+    if (vreg_num + 2 >= LIR_Opr::vreg_max) {\n@@ -1026,2 +1026,2 @@\n-      _virtual_register_number = LIR_OprDesc::vreg_base;\n-      vreg_num = LIR_OprDesc::vreg_base;\n+      _virtual_register_number = LIR_Opr::vreg_base;\n+      vreg_num = LIR_Opr::vreg_base;\n@@ -1868,1 +1868,1 @@\n-  LIR_Opr zero = NULL;\n+  LIR_Opr zero;\n@@ -3274,1 +3274,1 @@\n-  LIR_Opr counter_holder = NULL;\n+  LIR_Opr counter_holder;\n","filename":"src\/hotspot\/share\/c1\/c1_LIRGenerator.cpp","additions":6,"deletions":6,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -312,1 +312,1 @@\n-  virtual void CardTableBarrierSet_post_barrier_helper(LIR_OprDesc* addr, LIR_Const* card_table_base);\n+  virtual void CardTableBarrierSet_post_barrier_helper(LIR_Opr addr, LIR_Const* card_table_base);\n@@ -505,1 +505,1 @@\n-    , _virtual_register_number(LIR_OprDesc::vreg_base)\n+    , _virtual_register_number(LIR_Opr::vreg_base)\n","filename":"src\/hotspot\/share\/c1\/c1_LIRGenerator.hpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -176,1 +176,1 @@\n-  return i->reg_num() >= LIR_OprDesc::vreg_base;\n+  return i->reg_num() >= LIR_Opr::vreg_base;\n@@ -185,1 +185,1 @@\n-  return i->reg_num() >= LIR_OprDesc::vreg_base;\n+  return i->reg_num() >= LIR_Opr::vreg_base;\n@@ -187,1 +187,1 @@\n-  return i->reg_num() >= LIR_OprDesc::vreg_base && (i->type() != T_FLOAT && i->type() != T_DOUBLE);\n+  return i->reg_num() >= LIR_Opr::vreg_base && (i->type() != T_FLOAT && i->type() != T_DOUBLE);\n@@ -199,1 +199,1 @@\n-  return i->reg_num() >= LIR_OprDesc::vreg_base && (i->type() == T_FLOAT || i->type() == T_DOUBLE);\n+  return i->reg_num() >= LIR_Opr::vreg_base && (i->type() == T_FLOAT || i->type() == T_DOUBLE);\n@@ -277,1 +277,1 @@\n-  if (reg_num < LIR_OprDesc::vreg_base) {\n+  if (reg_num < LIR_Opr::vreg_base) {\n@@ -822,1 +822,1 @@\n-    for (int j = 0; j < LIR_OprDesc::vreg_base; j++) {\n+    for (int j = 0; j < LIR_Opr::vreg_base; j++) {\n@@ -1336,1 +1336,1 @@\n-      assert(number >= LIR_OprDesc::vreg_base, \"fixed intervals must not be live on block bounds\");\n+      assert(number >= LIR_Opr::vreg_base, \"fixed intervals must not be live on block bounds\");\n@@ -1709,1 +1709,1 @@\n-  result = new Interval(LIR_OprDesc::vreg_base);\n+  result = new Interval(LIR_Opr::vreg_base);\n@@ -2438,1 +2438,1 @@\n-    assert(interval->reg_num() >= LIR_OprDesc::vreg_base, \"fixed interval found\");\n+    assert(interval->reg_num() >= LIR_Opr::vreg_base, \"fixed interval found\");\n@@ -3221,1 +3221,1 @@\n-  } else if (reg_num >= LIR_OprDesc::vreg_base) {\n+  } else if (reg_num >= LIR_Opr::vreg_base) {\n@@ -3301,1 +3301,1 @@\n-    if (i1->reg_num() >= LIR_OprDesc::vreg_base && i1->type() == T_ILLEGAL) {\n+    if (i1->reg_num() >= LIR_Opr::vreg_base && i1->type() == T_ILLEGAL) {\n@@ -3972,1 +3972,1 @@\n-  if (reg_num + 20 >= LIR_OprDesc::vreg_max) {\n+  if (reg_num + 20 >= LIR_Opr::vreg_max) {\n@@ -3974,1 +3974,1 @@\n-    if (reg_num + 2 >= LIR_OprDesc::vreg_max) {\n+    if (reg_num + 2 >= LIR_Opr::vreg_max) {\n@@ -3976,1 +3976,1 @@\n-      reg_num = LIR_OprDesc::vreg_base;\n+      reg_num = LIR_Opr::vreg_base;\n@@ -4408,1 +4408,1 @@\n-  if (use_kind != noUse && reg_num() >= LIR_OprDesc::vreg_base) {\n+  if (use_kind != noUse && reg_num() >= LIR_Opr::vreg_base) {\n@@ -4638,1 +4638,1 @@\n-  if (reg_num() < LIR_OprDesc::vreg_base) {\n+  if (reg_num() < LIR_Opr::vreg_base) {\n@@ -4655,1 +4655,1 @@\n-    if (reg_num() < LIR_OprDesc::vreg_base) {\n+    if (reg_num() < LIR_Opr::vreg_base) {\n","filename":"src\/hotspot\/share\/c1\/c1_LinearScan.cpp","additions":17,"deletions":17,"binary":false,"changes":34,"status":"modified"},{"patch":"@@ -550,2 +550,2 @@\n-  BasicType        type() const                  { assert(_reg_num == -1 || _reg_num >= LIR_OprDesc::vreg_base, \"cannot access type for fixed interval\"); return _type; }\n-  void             set_type(BasicType type)      { assert(_reg_num < LIR_OprDesc::vreg_base || _type == T_ILLEGAL || _type == type, \"overwriting existing type\"); _type = type; }\n+  BasicType        type() const                  { assert(_reg_num == -1 || _reg_num >= LIR_Opr::vreg_base, \"cannot access type for fixed interval\"); return _type; }\n+  void             set_type(BasicType type)      { assert(_reg_num < LIR_Opr::vreg_base || _type == T_ILLEGAL || _type == type, \"overwriting existing type\"); _type = type; }\n","filename":"src\/hotspot\/share\/c1\/c1_LinearScan.hpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -45,8 +45,0 @@\n-  \/\/ prevent divide-by-zero\n-  if (ro_all < 1) {\n-    ro_all = 1;\n-  }\n-  if (rw_all < 1) {\n-    rw_all = 1;\n-  }\n-\n@@ -105,2 +97,5 @@\n-  assert(all_ro_bytes == ro_all, \"everything should have been counted\");\n-  assert(all_rw_bytes == rw_all, \"everything should have been counted\");\n+  msg.flush();\n+\n+  assert(all_ro_bytes == ro_all && all_rw_bytes == rw_all,\n+         \"everything should have been counted (used\/counted: ro %d\/%d, rw %d\/%d\",\n+         ro_all, all_ro_bytes, rw_all, all_rw_bytes);\n","filename":"src\/hotspot\/share\/cds\/dumpAllocStats.cpp","additions":5,"deletions":10,"binary":false,"changes":15,"status":"modified"},{"patch":"@@ -53,0 +53,1 @@\n+  const char* _archive_name;\n@@ -54,0 +55,1 @@\n+  DynamicArchiveBuilder(const char* archive_name) : _archive_name(archive_name) {}\n@@ -115,1 +117,0 @@\n-    SystemDictionaryShared::cleanup_lambda_proxy_class_dictionary();\n@@ -322,1 +323,1 @@\n-  dynamic_info->open_for_write(Arguments::GetSharedDynamicArchivePath());\n+  dynamic_info->open_for_write(_archive_name);\n@@ -337,1 +338,1 @@\n-  DynamicArchiveBuilder builder;\n+  DynamicArchiveBuilder _builder;\n@@ -339,1 +340,2 @@\n-  VM_PopulateDynamicDumpSharedSpace() : VM_GC_Sync_Operation() {}\n+  VM_PopulateDynamicDumpSharedSpace(const char* archive_name)\n+  : VM_GC_Sync_Operation(), _builder(archive_name) {}\n@@ -353,1 +355,1 @@\n-    builder.doit();\n+    _builder.doit();\n@@ -357,1 +359,20 @@\n-void DynamicArchive::prepare_for_dynamic_dumping() {\n+void DynamicArchive::check_for_dynamic_dump() {\n+  if (DynamicDumpSharedSpaces && !UseSharedSpaces) {\n+    \/\/ This could happen if SharedArchiveFile has failed to load:\n+    \/\/ - -Xshare:off was specified\n+    \/\/ - SharedArchiveFile points to an non-existent file.\n+    \/\/ - SharedArchiveFile points to an archive that has failed CRC check\n+    \/\/ - SharedArchiveFile is not specified and the VM doesn't have a compatible default archive\n+\n+#define __THEMSG \" is unsupported when base CDS archive is not loaded. Run with -Xlog:cds for more info.\"\n+    if (RecordDynamicDumpInfo) {\n+      vm_exit_during_initialization(\"-XX:+RecordDynamicDumpInfo\" __THEMSG, NULL);\n+    } else {\n+      assert(ArchiveClassesAtExit != nullptr, \"sanity\");\n+      vm_exit_during_initialization(\"-XX:ArchiveClassesAtExit\" __THEMSG, NULL);\n+#undef __THEMSG\n+    }\n+  }\n+}\n+\n+void DynamicArchive::prepare_for_dump_at_exit() {\n@@ -371,20 +392,7 @@\n-void DynamicArchive::dump(const char* archive_name, TRAPS) {\n-  assert(UseSharedSpaces && RecordDynamicDumpInfo, \"already checked in arguments.cpp?\");\n-  assert(ArchiveClassesAtExit == nullptr, \"already checked in arguments.cpp?\");\n-  ArchiveClassesAtExit = archive_name;\n-  if (Arguments::init_shared_archive_paths()) {\n-    prepare_for_dynamic_dumping();\n-    if (DynamicDumpSharedSpaces) {\n-      dump(CHECK);\n-    }\n-  } else {\n-    ArchiveClassesAtExit = nullptr;\n-    THROW_MSG(vmSymbols::java_lang_RuntimeException(),\n-              \"Could not setup SharedDynamicArchivePath\");\n-  }\n-  \/\/ prevent do dynamic dump at exit.\n-  ArchiveClassesAtExit = nullptr;\n-  if (!Arguments::init_shared_archive_paths()) {\n-    THROW_MSG(vmSymbols::java_lang_RuntimeException(),\n-              \"Could not restore SharedDynamicArchivePath\");\n-  }\n+\/\/ This is called by \"jcmd VM.cds dynamic_dump\"\n+void DynamicArchive::dump_for_jcmd(const char* archive_name, TRAPS) {\n+  assert(UseSharedSpaces && RecordDynamicDumpInfo, \"already checked in arguments.cpp\");\n+  assert(ArchiveClassesAtExit == nullptr, \"already checked in arguments.cpp\");\n+  assert(DynamicDumpSharedSpaces, \"already checked by check_for_dynamic_dump() during VM startup\");\n+  MetaspaceShared::link_shared_classes(CHECK);\n+  dump(archive_name, THREAD);\n@@ -393,6 +401,1 @@\n-void DynamicArchive::dump(TRAPS) {\n-  if (Arguments::GetSharedDynamicArchivePath() == NULL) {\n-    log_warning(cds, dynamic)(\"SharedDynamicArchivePath is not specified\");\n-    return;\n-  }\n-\n+void DynamicArchive::dump(const char* archive_name, TRAPS) {\n@@ -402,1 +405,1 @@\n-  VM_PopulateDynamicDumpSharedSpace op;\n+  VM_PopulateDynamicDumpSharedSpace op(archive_name);\n@@ -406,0 +409,4 @@\n+bool DynamicArchive::should_dump_at_vm_exit() {\n+  return DynamicDumpSharedSpaces && (ArchiveClassesAtExit != nullptr);\n+}\n+\n","filename":"src\/hotspot\/share\/cds\/dynamicArchive.cpp","additions":40,"deletions":33,"binary":false,"changes":73,"status":"modified"},{"patch":"@@ -62,1 +62,4 @@\n-  static void prepare_for_dynamic_dumping();\n+  static void check_for_dynamic_dump();\n+  static bool should_dump_at_vm_exit();\n+  static void prepare_for_dump_at_exit();\n+  static void dump_for_jcmd(const char* archive_name, TRAPS);\n@@ -64,1 +67,0 @@\n-  static void dump(TRAPS);\n","filename":"src\/hotspot\/share\/cds\/dynamicArchive.hpp","additions":4,"deletions":2,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -524,1 +524,0 @@\n-  SystemDictionaryShared::cleanup_lambda_proxy_class_dictionary();\n","filename":"src\/hotspot\/share\/cds\/metaspaceShared.cpp","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -1646,0 +1646,1 @@\n+  out->print_cr(\"version %d\", REPLAY_VERSION);\n@@ -1683,2 +1684,2 @@\n-  static char buffer[O_BUFLEN];\n-  int ret = jio_snprintf(buffer, O_BUFLEN, \"replay_pid%p_compid%d.log\", os::current_process_id(), compile_id);\n+  char buffer[64];\n+  int ret = jio_snprintf(buffer, sizeof(buffer), \"replay_pid%d_compid%d.log\", os::current_process_id(), compile_id);\n@@ -1701,2 +1702,2 @@\n-  static char buffer[O_BUFLEN];\n-  int ret = jio_snprintf(buffer, O_BUFLEN, \"inline_pid%p_compid%d.log\", os::current_process_id(), compile_id);\n+  char buffer[64];\n+  int ret = jio_snprintf(buffer, sizeof(buffer), \"inline_pid%d_compid%d.log\", os::current_process_id(), compile_id);\n","filename":"src\/hotspot\/share\/ci\/ciEnv.cpp","additions":5,"deletions":4,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -90,1 +90,1 @@\n-  Thread *thread = Thread::current();\n+  JavaThread *thread = JavaThread::current();\n","filename":"src\/hotspot\/share\/ci\/ciInstanceKlass.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -52,2 +52,0 @@\n-  _creation_mileage(0),\n-  _current_mileage(0),\n@@ -55,1 +53,0 @@\n-  _backedge_counter(0),\n@@ -253,2 +250,0 @@\n-  _creation_mileage = mdo->creation_mileage();\n-  _current_mileage = MethodData::mileage_of(mdo->method());\n@@ -256,1 +251,0 @@\n-  _backedge_counter = mdo->backedge_count();\n@@ -709,1 +703,1 @@\n-  out->print(\" %d %d\", _state, current_mileage());\n+  out->print(\" %d %d\", _state, _invocation_counter);\n","filename":"src\/hotspot\/share\/ci\/ciMethodData.cpp","additions":1,"deletions":7,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -398,5 +398,0 @@\n-  int _creation_mileage; \/\/ method mileage at MDO creation\n-\n-  \/\/ Maturity of the oop when the snapshot is taken.\n-  int _current_mileage;\n-\n@@ -407,1 +402,0 @@\n-  int _backedge_counter;\n@@ -480,3 +474,0 @@\n-  int creation_mileage() { return _creation_mileage; }\n-  int current_mileage()  { return _current_mileage; }\n-\n@@ -484,1 +475,0 @@\n-  int backedge_count()   { return _backedge_counter;   }\n","filename":"src\/hotspot\/share\/ci\/ciMethodData.hpp","additions":0,"deletions":10,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -68,1 +68,1 @@\n-  int _current_mileage;\n+  int _invocation_counter;\n@@ -118,0 +118,1 @@\n+  int     _version;\n@@ -162,0 +163,1 @@\n+    _version = 0;\n@@ -437,1 +439,1 @@\n-        obj = cp->resolve_possibly_cached_constant_at(bootstrap_specifier.bsm_index(), thread);\n+        obj = cp->resolve_possibly_cached_constant_at(bootstrap_specifier.bsm_index(), CHECK_NULL);\n@@ -466,4 +468,6 @@\n-      {\n-        bool found_it;\n-        obj = cp->find_cached_constant_at(cpi, found_it, thread);\n-      }\n+      ik->link_class(CHECK_NULL);\n+      obj = cp->resolve_possibly_cached_constant_at(cpi, CHECK_NULL);\n+    }\n+    if (obj == NULL) {\n+      report_error(\"null cp object found\");\n+      return NULL;\n@@ -472,8 +476,18 @@\n-    if (obj != NULL) {\n-      skip_ws();\n-      \/\/ loop: read fields\n-      char* field = NULL;\n-      do {\n-        field = parse_string();\n-        if (field == NULL) {\n-          report_error(\"no field found\");\n+    skip_ws();\n+    \/\/ loop: read fields\n+    char* field = NULL;\n+    do {\n+      field = parse_string();\n+      if (field == NULL) {\n+        report_error(\"no field found\");\n+        return NULL;\n+      }\n+      if (strcmp(field, \";\") == 0) {\n+        break;\n+      }\n+      \/\/ raw Method*\n+      if (strcmp(field, \"<vmtarget>\") == 0) {\n+        Method* vmtarget = java_lang_invoke_MemberName::vmtarget(obj);\n+        k = (vmtarget == NULL) ? NULL : vmtarget->method_holder();\n+        if (k == NULL) {\n+          report_error(\"null vmtarget found\");\n@@ -482,16 +496,3 @@\n-        if (strcmp(field, \";\") == 0) {\n-          break;\n-        }\n-        \/\/ raw Method*\n-        if (strcmp(field, \"<vmtarget>\") == 0) {\n-          Method* vmtarget = java_lang_invoke_MemberName::vmtarget(obj);\n-          k = (vmtarget == NULL) ? NULL : vmtarget->method_holder();\n-          if (k == NULL) {\n-            report_error(\"null vmtarget found\");\n-            return NULL;\n-          }\n-          if (!parse_terminator()) {\n-            report_error(\"missing terminator\");\n-            return NULL;\n-          }\n-          return k;\n+        if (!parse_terminator()) {\n+          report_error(\"missing terminator\");\n+          return NULL;\n@@ -499,10 +500,10 @@\n-        obj = ciReplay::obj_field(obj, field);\n-        \/\/ array\n-        if (obj != NULL && obj->is_objArray()) {\n-          objArrayOop arr = (objArrayOop)obj;\n-          int index = parse_int(\"index\");\n-          if (index >= arr->length()) {\n-            report_error(\"bad array index\");\n-            return NULL;\n-          }\n-          obj = arr->obj_at(index);\n+        return k;\n+      }\n+      obj = ciReplay::obj_field(obj, field);\n+      \/\/ array\n+      if (obj != NULL && obj->is_objArray()) {\n+        objArrayOop arr = (objArrayOop)obj;\n+        int index = parse_int(\"index\");\n+        if (index >= arr->length()) {\n+          report_error(\"bad array index\");\n+          return NULL;\n@@ -510,4 +511,1 @@\n-      } while (obj != NULL);\n-      if (obj == NULL) {\n-        report_error(\"null field found\");\n-        return NULL;\n+        obj = arr->obj_at(index);\n@@ -515,1 +513,4 @@\n-      k = obj->klass();\n+    } while (obj != NULL);\n+    if (obj == NULL) {\n+      report_error(\"null field found\");\n+      return NULL;\n@@ -517,0 +518,1 @@\n+    k = obj->klass();\n@@ -641,0 +643,5 @@\n+    } else if (strcmp(\"version\", cmd) == 0) {\n+      _version = parse_int(\"version\");\n+      if (_version < 0 || _version > REPLAY_VERSION) {\n+        tty->print_cr(\"# unrecognized version %d, expected 0 <= version <= %d\", _version, REPLAY_VERSION);\n+      }\n@@ -805,1 +812,1 @@\n-  \/\/ ciMethodData <klass> <name> <signature> <state> <current_mileage> orig <length> <byte>* data <length> <ptr>* oops <length> (<offset> <klass>)* methods <length> (<offset> <klass> <name> <signature>)*\n+  \/\/ ciMethodData <klass> <name> <signature> <state> <invocation_counter> orig <length> <byte>* data <length> <ptr>* oops <length> (<offset> <klass>)* methods <length> (<offset> <klass> <name> <signature>)*\n@@ -830,1 +837,5 @@\n-    rec->_current_mileage = parse_int(\"current_mileage\");\n+    if (_version < 1) {\n+      parse_int(\"current_mileage\");\n+    } else {\n+      rec->_invocation_counter = parse_int(\"invocation_counter\");\n+    }\n@@ -879,7 +890,0 @@\n-    if (!_protection_domain_initialized && k != NULL) {\n-      assert(_protection_domain() == NULL, \"must be uninitialized\");\n-      \/\/ The first entry is the holder class of the method for which a replay compilation is requested.\n-      \/\/ Use the same protection domain to load all subsequent classes in order to resolve all classes\n-      \/\/ in signatures of inlinees. This ensures that inlining can be done as stated in the replay file.\n-      _protection_domain = Handle(_thread, k->protection_domain());\n-    }\n@@ -887,3 +891,11 @@\n-    \/\/ Only initialize the protection domain handle with the protection domain of the very first entry.\n-    \/\/ This also ensures that older replay files work.\n-    _protection_domain_initialized = true;\n+    if (_version >= 1) {\n+      if (!_protection_domain_initialized && k != NULL) {\n+        assert(_protection_domain() == NULL, \"must be uninitialized\");\n+        \/\/ The first entry is the holder class of the method for which a replay compilation is requested.\n+        \/\/ Use the same protection domain to load all subsequent classes in order to resolve all classes\n+        \/\/ in signatures of inlinees. This ensures that inlining can be done as stated in the replay file.\n+        _protection_domain = Handle(_thread, k->protection_domain());\n+      }\n+\n+      _protection_domain_initialized = true;\n+    }\n@@ -918,0 +930,1 @@\n+      skip_remaining();\n@@ -1416,1 +1429,1 @@\n-    m->_current_mileage = rec->_current_mileage;\n+    m->_invocation_counter = rec->_invocation_counter;\n","filename":"src\/hotspot\/share\/ci\/ciReplay.cpp","additions":71,"deletions":58,"binary":false,"changes":129,"status":"modified"},{"patch":"@@ -134,0 +134,6 @@\n+\/\/ Replay file format version history\n+\/\/ 0: legacy (no version number)\n+\/\/ 1: first instanceKlass sets protection domain (8275868)\n+\/\/    replace current_mileage with invocation_count (8276095)\n+#define REPLAY_VERSION 1 \/\/ current version, bump up for incompatible changes\n+\n","filename":"src\/hotspot\/share\/ci\/ciReplay.hpp","additions":6,"deletions":0,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -248,0 +248,8 @@\n+void SystemDictionaryShared::reset_registered_lambda_proxy_class(InstanceKlass* ik) {\n+  DumpTimeClassInfo* info = _dumptime_table->get(ik);\n+  if (info != NULL) {\n+    info->_is_archived_lambda_proxy = false;\n+    info->set_excluded();\n+  }\n+}\n+\n@@ -328,0 +336,1 @@\n+      ResourceMark rm;\n@@ -666,0 +675,2 @@\n+\n+  cleanup_lambda_proxy_class_dictionary();\n@@ -1605,1 +1616,10 @@\n-    for (int i = 0; i < info._proxy_klasses->length(); i++) {\n+    InstanceKlass* caller_ik = key.caller_ik();\n+    if (SystemDictionaryShared::check_for_exclusion(caller_ik, NULL)) {\n+      \/\/ If the caller class is excluded, unregister all the associated lambda proxy classes\n+      \/\/ so that they will not be included in the CDS archive.\n+      for (int i = info._proxy_klasses->length() - 1; i >= 0; i--) {\n+        SystemDictionaryShared::reset_registered_lambda_proxy_class(info._proxy_klasses->at(i));\n+        info._proxy_klasses->remove_at(i);\n+      }\n+    }\n+    for (int i = info._proxy_klasses->length() - 1; i >= 0; i--) {\n@@ -1607,1 +1627,2 @@\n-      if (!ik->can_be_verified_at_dumptime()) {\n+      if (SystemDictionaryShared::check_for_exclusion(ik, NULL)) {\n+        SystemDictionaryShared::reset_registered_lambda_proxy_class(ik);\n","filename":"src\/hotspot\/share\/classfile\/systemDictionaryShared.cpp","additions":23,"deletions":2,"binary":false,"changes":25,"status":"modified"},{"patch":"@@ -139,0 +139,1 @@\n+  friend class CleanupDumpTimeLambdaProxyClassTable;\n@@ -176,0 +177,2 @@\n+  static void cleanup_lambda_proxy_class_dictionary();\n+  static void reset_registered_lambda_proxy_class(InstanceKlass* ik);\n@@ -291,1 +294,0 @@\n-  static void cleanup_lambda_proxy_class_dictionary();\n","filename":"src\/hotspot\/share\/classfile\/systemDictionaryShared.hpp","additions":3,"deletions":1,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -835,1 +835,8 @@\n-   do_signature(vector_unary_op_sig, \"(ILjava\/lang\/Class;Ljava\/lang\/Class;ILjava\/lang\/Object;Ljava\/util\/function\/Function;)Ljava\/lang\/Object;\") \\\n+   do_signature(vector_unary_op_sig, \"(I\"                                                                                                      \\\n+                                      \"Ljava\/lang\/Class;\"                                                                                      \\\n+                                      \"Ljava\/lang\/Class;Ljava\/lang\/Class;\"                                                                     \\\n+                                      \"I\"                                                                                                      \\\n+                                      \"Ljdk\/internal\/vm\/vector\/VectorSupport$Vector;\"                                                          \\\n+                                      \"Ljdk\/internal\/vm\/vector\/VectorSupport$VectorMask;\"                                                      \\\n+                                      \"Ljdk\/internal\/vm\/vector\/VectorSupport$UnaryOperation;)\"                                                 \\\n+                                      \"Ljdk\/internal\/vm\/vector\/VectorSupport$Vector;\")                                                         \\\n@@ -839,2 +846,10 @@\n-   do_signature(vector_binary_op_sig, \"(ILjava\/lang\/Class;Ljava\/lang\/Class;ILjava\/lang\/Object;Ljava\/lang\/Object;\"                              \\\n-                                       \"Ljava\/util\/function\/BiFunction;)Ljava\/lang\/Object;\")                                                   \\\n+   do_signature(vector_binary_op_sig, \"(I\"                                                                                                     \\\n+                                       \"Ljava\/lang\/Class;\"                                                                                     \\\n+                                       \"Ljava\/lang\/Class;\"                                                                                     \\\n+                                       \"Ljava\/lang\/Class;\"                                                                                     \\\n+                                       \"I\"                                                                                                     \\\n+                                       \"Ljdk\/internal\/vm\/vector\/VectorSupport$VectorPayload;\"                                                  \\\n+                                       \"Ljdk\/internal\/vm\/vector\/VectorSupport$VectorPayload;\"                                                  \\\n+                                       \"Ljdk\/internal\/vm\/vector\/VectorSupport$VectorMask;\"                                                     \\\n+                                       \"Ljdk\/internal\/vm\/vector\/VectorSupport$BinaryOperation;)\"                                               \\\n+                                       \"Ljdk\/internal\/vm\/vector\/VectorSupport$VectorPayload;\")                                                 \\\n@@ -844,2 +859,11 @@\n-   do_signature(vector_ternary_op_sig, \"(ILjava\/lang\/Class;Ljava\/lang\/Class;ILjava\/lang\/Object;Ljava\/lang\/Object;\"                             \\\n-                                        \"Ljava\/lang\/Object;Ljdk\/internal\/vm\/vector\/VectorSupport$TernaryOperation;)Ljava\/lang\/Object;\")        \\\n+   do_signature(vector_ternary_op_sig, \"(I\"                                                                                                    \\\n+                                        \"Ljava\/lang\/Class;\"                                                                                    \\\n+                                        \"Ljava\/lang\/Class;\"                                                                                    \\\n+                                        \"Ljava\/lang\/Class;\"                                                                                    \\\n+                                        \"I\"                                                                                                    \\\n+                                        \"Ljdk\/internal\/vm\/vector\/VectorSupport$Vector;\"                                                        \\\n+                                        \"Ljdk\/internal\/vm\/vector\/VectorSupport$Vector;\"                                                        \\\n+                                        \"Ljdk\/internal\/vm\/vector\/VectorSupport$Vector;\"                                                        \\\n+                                        \"Ljdk\/internal\/vm\/vector\/VectorSupport$VectorMask;\"                                                    \\\n+                                        \"Ljdk\/internal\/vm\/vector\/VectorSupport$TernaryOperation;)\"                                             \\\n+                                        \"Ljdk\/internal\/vm\/vector\/VectorSupport$Vector;\")                                                       \\\n@@ -849,2 +873,7 @@\n-   do_signature(vector_broadcast_coerced_sig, \"(Ljava\/lang\/Class;Ljava\/lang\/Class;IJLjdk\/internal\/vm\/vector\/VectorSupport$VectorSpecies;\"      \\\n-                                               \"Ljdk\/internal\/vm\/vector\/VectorSupport$BroadcastOperation;)Ljava\/lang\/Object;\")                 \\\n+   do_signature(vector_broadcast_coerced_sig, \"(Ljava\/lang\/Class;\"                                                                             \\\n+                                               \"Ljava\/lang\/Class;\"                                                                             \\\n+                                               \"I\"                                                                                             \\\n+                                               \"J\"                                                                                             \\\n+                                               \"Ljdk\/internal\/vm\/vector\/VectorSupport$VectorSpecies;\"                                          \\\n+                                               \"Ljdk\/internal\/vm\/vector\/VectorSupport$BroadcastOperation;)\"                                    \\\n+                                               \"Ljdk\/internal\/vm\/vector\/VectorSupport$VectorPayload;\")                                         \\\n@@ -854,2 +883,6 @@\n-   do_signature(vector_shuffle_step_iota_sig, \"(Ljava\/lang\/Class;Ljava\/lang\/Class;Ljdk\/internal\/vm\/vector\/VectorSupport$VectorSpecies;\"        \\\n-                                               \"IIIILjdk\/internal\/vm\/vector\/VectorSupport$ShuffleIotaOperation;)Ljdk\/internal\/vm\/vector\/VectorSupport$VectorShuffle;\") \\\n+   do_signature(vector_shuffle_step_iota_sig, \"(Ljava\/lang\/Class;\"                                                                             \\\n+                                               \"Ljava\/lang\/Class;\"                                                                             \\\n+                                               \"Ljdk\/internal\/vm\/vector\/VectorSupport$VectorSpecies;\"                                          \\\n+                                               \"IIII\"                                                                                          \\\n+                                               \"Ljdk\/internal\/vm\/vector\/VectorSupport$ShuffleIotaOperation;)\"                                  \\\n+                                               \"Ljdk\/internal\/vm\/vector\/VectorSupport$VectorShuffle;\")                                         \\\n@@ -859,2 +892,6 @@\n-   do_signature(vector_shuffle_to_vector_sig, \"(Ljava\/lang\/Class;Ljava\/lang\/Class;Ljava\/lang\/Class;Ljdk\/internal\/vm\/vector\/VectorSupport$VectorShuffle;\" \\\n-                                               \"ILjdk\/internal\/vm\/vector\/VectorSupport$ShuffleToVectorOperation;)Ljava\/lang\/Object;\")          \\\n+   do_signature(vector_shuffle_to_vector_sig, \"(Ljava\/lang\/Class;\"                                                                             \\\n+                                               \"Ljava\/lang\/Class;\"                                                                             \\\n+                                               \"Ljava\/lang\/Class;\"                                                                             \\\n+                                               \"Ljdk\/internal\/vm\/vector\/VectorSupport$VectorShuffle;\"                                          \\\n+                                               \"ILjdk\/internal\/vm\/vector\/VectorSupport$ShuffleToVectorOperation;)\"                             \\\n+                                               \"Ljdk\/internal\/vm\/vector\/VectorSupport$Vector;\")                                                \\\n@@ -864,2 +901,10 @@\n-   do_signature(vector_load_op_sig, \"(Ljava\/lang\/Class;Ljava\/lang\/Class;ILjava\/lang\/Object;JLjava\/lang\/Object;\"                                \\\n-                                     \"ILjdk\/internal\/vm\/vector\/VectorSupport$VectorSpecies;Ljdk\/internal\/vm\/vector\/VectorSupport$LoadOperation;)Ljava\/lang\/Object;\") \\\n+   do_signature(vector_load_op_sig, \"(Ljava\/lang\/Class;\"                                                                                       \\\n+                                     \"Ljava\/lang\/Class;\"                                                                                       \\\n+                                     \"I\"                                                                                                       \\\n+                                     \"Ljava\/lang\/Object;\"                                                                                      \\\n+                                     \"J\"                                                                                                       \\\n+                                     \"Ljava\/lang\/Object;\"                                                                                      \\\n+                                     \"I\"                                                                                                       \\\n+                                     \"Ljdk\/internal\/vm\/vector\/VectorSupport$VectorSpecies;\"                                                    \\\n+                                     \"Ljdk\/internal\/vm\/vector\/VectorSupport$LoadOperation;)\"                                                   \\\n+                                     \"Ljdk\/internal\/vm\/vector\/VectorSupport$VectorPayload;\")                                                   \\\n@@ -868,0 +913,15 @@\n+  do_intrinsic(_VectorLoadMaskedOp, jdk_internal_vm_vector_VectorSupport, vector_load_masked_op_name, vector_load_masked_op_sig, F_S)          \\\n+   do_signature(vector_load_masked_op_sig, \"(Ljava\/lang\/Class;\"                                                                                \\\n+                                            \"Ljava\/lang\/Class;\"                                                                                \\\n+                                            \"Ljava\/lang\/Class;\"                                                                                \\\n+                                            \"I\"                                                                                                \\\n+                                            \"Ljava\/lang\/Object;\"                                                                               \\\n+                                            \"J\"                                                                                                \\\n+                                            \"Ljdk\/internal\/vm\/vector\/VectorSupport$VectorMask;\"                                                \\\n+                                            \"Ljava\/lang\/Object;\"                                                                               \\\n+                                            \"I\"                                                                                                \\\n+                                            \"Ljdk\/internal\/vm\/vector\/VectorSupport$VectorSpecies;\"                                             \\\n+                                            \"Ljdk\/internal\/vm\/vector\/VectorSupport$LoadVectorMaskedOperation;)\"                                \\\n+                                            \"Ljdk\/internal\/vm\/vector\/VectorSupport$Vector;\")                                                   \\\n+   do_name(vector_load_masked_op_name,     \"loadMasked\")                                                                                       \\\n+                                                                                                                                               \\\n@@ -869,2 +929,8 @@\n-   do_signature(vector_store_op_sig, \"(Ljava\/lang\/Class;Ljava\/lang\/Class;ILjava\/lang\/Object;JLjdk\/internal\/vm\/vector\/VectorSupport$Vector;\"    \\\n-                                      \"Ljava\/lang\/Object;ILjdk\/internal\/vm\/vector\/VectorSupport$StoreVectorOperation;)V\")                      \\\n+   do_signature(vector_store_op_sig, \"(Ljava\/lang\/Class;\"                                                                                      \\\n+                                      \"Ljava\/lang\/Class;\"                                                                                      \\\n+                                      \"I\"                                                                                                      \\\n+                                      \"Ljava\/lang\/Object;\"                                                                                     \\\n+                                      \"J\"                                                                                                      \\\n+                                      \"Ljdk\/internal\/vm\/vector\/VectorSupport$Vector;\"                                                          \\\n+                                      \"Ljava\/lang\/Object;ILjdk\/internal\/vm\/vector\/VectorSupport$StoreVectorOperation;)\"                        \\\n+                                      \"V\")                                                                                                     \\\n@@ -873,2 +939,25 @@\n-  do_intrinsic(_VectorReductionCoerced, jdk_internal_vm_vector_VectorSupport, vector_reduction_coerced_name, vector_reduction_coerced_sig, F_S) \\\n-   do_signature(vector_reduction_coerced_sig, \"(ILjava\/lang\/Class;Ljava\/lang\/Class;ILjdk\/internal\/vm\/vector\/VectorSupport$Vector;Ljava\/util\/function\/Function;)J\") \\\n+  do_intrinsic(_VectorStoreMaskedOp, jdk_internal_vm_vector_VectorSupport, vector_store_masked_op_name, vector_store_masked_op_sig, F_S)       \\\n+   do_signature(vector_store_masked_op_sig, \"(Ljava\/lang\/Class;\"                                                                               \\\n+                                             \"Ljava\/lang\/Class;\"                                                                               \\\n+                                             \"Ljava\/lang\/Class;\"                                                                               \\\n+                                             \"I\"                                                                                               \\\n+                                             \"Ljava\/lang\/Object;\"                                                                              \\\n+                                             \"J\"                                                                                               \\\n+                                             \"Ljdk\/internal\/vm\/vector\/VectorSupport$Vector;\"                                                   \\\n+                                             \"Ljdk\/internal\/vm\/vector\/VectorSupport$VectorMask;\"                                               \\\n+                                             \"Ljava\/lang\/Object;\"                                                                              \\\n+                                             \"I\"                                                                                               \\\n+                                             \"Ljdk\/internal\/vm\/vector\/VectorSupport$StoreVectorMaskedOperation;)\"                              \\\n+                                             \"V\")                                                                                              \\\n+   do_name(vector_store_masked_op_name,     \"storeMasked\")                                                                                     \\\n+                                                                                                                                               \\\n+  do_intrinsic(_VectorReductionCoerced, jdk_internal_vm_vector_VectorSupport, vector_reduction_coerced_name, vector_reduction_coerced_sig, F_S)\\\n+   do_signature(vector_reduction_coerced_sig, \"(I\"                                                                                             \\\n+                                               \"Ljava\/lang\/Class;\"                                                                             \\\n+                                               \"Ljava\/lang\/Class;\"                                                                             \\\n+                                               \"Ljava\/lang\/Class;\"                                                                             \\\n+                                               \"I\"                                                                                             \\\n+                                               \"Ljdk\/internal\/vm\/vector\/VectorSupport$Vector;\"                                                 \\\n+                                               \"Ljdk\/internal\/vm\/vector\/VectorSupport$VectorMask;\"                                             \\\n+                                               \"Ljdk\/internal\/vm\/vector\/VectorSupport$ReductionOperation;)\"                                    \\\n+                                               \"J\")                                                                                            \\\n@@ -878,1 +967,8 @@\n-   do_signature(vector_test_sig, \"(ILjava\/lang\/Class;Ljava\/lang\/Class;ILjava\/lang\/Object;Ljava\/lang\/Object;Ljava\/util\/function\/BiFunction;)Z\") \\\n+   do_signature(vector_test_sig, \"(I\"                                                                                                          \\\n+                                  \"Ljava\/lang\/Class;\"                                                                                          \\\n+                                  \"Ljava\/lang\/Class;\"                                                                                          \\\n+                                  \"I\"                                                                                                          \\\n+                                  \"Ljdk\/internal\/vm\/vector\/VectorSupport$VectorMask;\"                                                          \\\n+                                  \"Ljdk\/internal\/vm\/vector\/VectorSupport$VectorMask;\"                                                          \\\n+                                  \"Ljava\/util\/function\/BiFunction;)\"                                                                           \\\n+                                  \"Z\")                                                                                                         \\\n@@ -882,3 +978,9 @@\n-   do_signature(vector_blend_sig, \"(Ljava\/lang\/Class;Ljava\/lang\/Class;Ljava\/lang\/Class;I\"                                                      \\\n-                                   \"Ljdk\/internal\/vm\/vector\/VectorSupport$Vector;Ljdk\/internal\/vm\/vector\/VectorSupport$Vector;Ljdk\/internal\/vm\/vector\/VectorSupport$VectorMask;\" \\\n-                                   \"Ljdk\/internal\/vm\/vector\/VectorSupport$VectorBlendOp;)Ljdk\/internal\/vm\/vector\/VectorSupport$Vector;\")       \\\n+   do_signature(vector_blend_sig, \"(Ljava\/lang\/Class;\"                                                                                         \\\n+                                   \"Ljava\/lang\/Class;\"                                                                                         \\\n+                                   \"Ljava\/lang\/Class;\"                                                                                         \\\n+                                   \"I\"                                                                                                         \\\n+                                   \"Ljdk\/internal\/vm\/vector\/VectorSupport$Vector;\"                                                             \\\n+                                   \"Ljdk\/internal\/vm\/vector\/VectorSupport$Vector;\"                                                             \\\n+                                   \"Ljdk\/internal\/vm\/vector\/VectorSupport$VectorMask;\"                                                         \\\n+                                   \"Ljdk\/internal\/vm\/vector\/VectorSupport$VectorBlendOp;)\"                                                     \\\n+                                   \"Ljdk\/internal\/vm\/vector\/VectorSupport$Vector;\")                                                            \\\n@@ -888,3 +990,9 @@\n-   do_signature(vector_compare_sig, \"(ILjava\/lang\/Class;Ljava\/lang\/Class;Ljava\/lang\/Class;I\"                                                   \\\n-                                     \"Ljdk\/internal\/vm\/vector\/VectorSupport$Vector;\" \"Ljdk\/internal\/vm\/vector\/VectorSupport$Vector;\"           \\\n-                                     \"Ljdk\/internal\/vm\/vector\/VectorSupport$VectorCompareOp;\" \")\" \"Ljdk\/internal\/vm\/vector\/VectorSupport$VectorMask;\") \\\n+   do_signature(vector_compare_sig, \"(I\"                                                                                                       \\\n+                                     \"Ljava\/lang\/Class;\"                                                                                       \\\n+                                     \"Ljava\/lang\/Class;Ljava\/lang\/Class;\"                                                                      \\\n+                                     \"I\"                                                                                                       \\\n+                                     \"Ljdk\/internal\/vm\/vector\/VectorSupport$Vector;\"                                                           \\\n+                                     \"Ljdk\/internal\/vm\/vector\/VectorSupport$Vector;\"                                                           \\\n+                                     \"Ljdk\/internal\/vm\/vector\/VectorSupport$VectorMask;\"                                                       \\\n+                                     \"Ljdk\/internal\/vm\/vector\/VectorSupport$VectorCompareOp;)\"                                                 \\\n+                                     \"Ljdk\/internal\/vm\/vector\/VectorSupport$VectorMask;\")                                                      \\\n@@ -894,3 +1002,10 @@\n-   do_signature(vector_rearrange_sig, \"(Ljava\/lang\/Class;Ljava\/lang\/Class;Ljava\/lang\/Class;I\"                                                  \\\n-                                       \"Ljdk\/internal\/vm\/vector\/VectorSupport$Vector;Ljdk\/internal\/vm\/vector\/VectorSupport$VectorShuffle;\"     \\\n-                                       \"Ljdk\/internal\/vm\/vector\/VectorSupport$VectorRearrangeOp;)Ljdk\/internal\/vm\/vector\/VectorSupport$Vector;\") \\\n+   do_signature(vector_rearrange_sig, \"(Ljava\/lang\/Class;\"                                                                                     \\\n+                                       \"Ljava\/lang\/Class;\"                                                                                     \\\n+                                       \"Ljava\/lang\/Class;\"                                                                                     \\\n+                                       \"Ljava\/lang\/Class;\"                                                                                     \\\n+                                       \"I\"                                                                                                     \\\n+                                       \"Ljdk\/internal\/vm\/vector\/VectorSupport$Vector;\"                                                         \\\n+                                       \"Ljdk\/internal\/vm\/vector\/VectorSupport$VectorShuffle;\"                                                  \\\n+                                       \"Ljdk\/internal\/vm\/vector\/VectorSupport$VectorMask;\"                                                     \\\n+                                       \"Ljdk\/internal\/vm\/vector\/VectorSupport$VectorRearrangeOp;)\"                                             \\\n+                                       \"Ljdk\/internal\/vm\/vector\/VectorSupport$Vector;\")                                                        \\\n@@ -900,3 +1015,7 @@\n-   do_signature(vector_extract_sig, \"(Ljava\/lang\/Class;Ljava\/lang\/Class;I\"                                                                     \\\n-                                     \"Ljdk\/internal\/vm\/vector\/VectorSupport$Vector;I\"                                                          \\\n-                                     \"Ljdk\/internal\/vm\/vector\/VectorSupport$VecExtractOp;)J\")                                                  \\\n+   do_signature(vector_extract_sig, \"(Ljava\/lang\/Class;\"                                                                                       \\\n+                                     \"Ljava\/lang\/Class;\"                                                                                       \\\n+                                     \"I\"                                                                                                       \\\n+                                     \"Ljdk\/internal\/vm\/vector\/VectorSupport$Vector;\"                                                           \\\n+                                     \"I\"                                                                                                       \\\n+                                     \"Ljdk\/internal\/vm\/vector\/VectorSupport$VecExtractOp;)\"                                                    \\\n+                                     \"J\")                                                                                                      \\\n@@ -906,3 +1025,7 @@\n-   do_signature(vector_insert_sig, \"(Ljava\/lang\/Class;Ljava\/lang\/Class;I\"                                                                      \\\n-                                    \"Ljdk\/internal\/vm\/vector\/VectorSupport$Vector;IJ\"                                                          \\\n-                                    \"Ljdk\/internal\/vm\/vector\/VectorSupport$VecInsertOp;)Ljdk\/internal\/vm\/vector\/VectorSupport$Vector;\")        \\\n+   do_signature(vector_insert_sig, \"(Ljava\/lang\/Class;\"                                                                                        \\\n+                                    \"Ljava\/lang\/Class;\"                                                                                        \\\n+                                    \"I\"                                                                                                        \\\n+                                    \"Ljdk\/internal\/vm\/vector\/VectorSupport$Vector;\"                                                            \\\n+                                    \"IJ\"                                                                                                       \\\n+                                    \"Ljdk\/internal\/vm\/vector\/VectorSupport$VecInsertOp;)\"                                                      \\\n+                                    \"Ljdk\/internal\/vm\/vector\/VectorSupport$Vector;\")                                                           \\\n@@ -912,3 +1035,10 @@\n-   do_signature(vector_broadcast_int_sig, \"(ILjava\/lang\/Class;Ljava\/lang\/Class;I\"                                                              \\\n-                                           \"Ljdk\/internal\/vm\/vector\/VectorSupport$Vector;I\"                                                    \\\n-                                           \"Ljdk\/internal\/vm\/vector\/VectorSupport$VectorBroadcastIntOp;)Ljdk\/internal\/vm\/vector\/VectorSupport$Vector;\") \\\n+   do_signature(vector_broadcast_int_sig, \"(I\"                                                                                                 \\\n+                                           \"Ljava\/lang\/Class;\"                                                                                 \\\n+                                           \"Ljava\/lang\/Class;\"                                                                                 \\\n+                                           \"Ljava\/lang\/Class;\"                                                                                 \\\n+                                           \"I\"                                                                                                 \\\n+                                           \"Ljdk\/internal\/vm\/vector\/VectorSupport$Vector;\"                                                     \\\n+                                           \"I\"                                                                                                 \\\n+                                           \"Ljdk\/internal\/vm\/vector\/VectorSupport$VectorMask;\"                                                 \\\n+                                           \"Ljdk\/internal\/vm\/vector\/VectorSupport$VectorBroadcastIntOp;)\"                                      \\\n+                                           \"Ljdk\/internal\/vm\/vector\/VectorSupport$Vector;\")                                                    \\\n@@ -918,2 +1048,7 @@\n-   do_signature(vector_convert_sig, \"(ILjava\/lang\/Class;Ljava\/lang\/Class;I\"                                                                    \\\n-                                     \"Ljava\/lang\/Class;Ljava\/lang\/Class;I\"                                                                     \\\n+   do_signature(vector_convert_sig, \"(I\"                                                                                                       \\\n+                                     \"Ljava\/lang\/Class;\"                                                                                       \\\n+                                     \"Ljava\/lang\/Class;\"                                                                                       \\\n+                                     \"I\"                                                                                                       \\\n+                                     \"Ljava\/lang\/Class;\"                                                                                       \\\n+                                     \"Ljava\/lang\/Class;\"                                                                                       \\\n+                                     \"I\"                                                                                                       \\\n@@ -922,1 +1057,2 @@\n-                                     \"Ljdk\/internal\/vm\/vector\/VectorSupport$VectorConvertOp;)Ljdk\/internal\/vm\/vector\/VectorSupport$VectorPayload;\") \\\n+                                     \"Ljdk\/internal\/vm\/vector\/VectorSupport$VectorConvertOp;)\"                                                 \\\n+                                     \"Ljdk\/internal\/vm\/vector\/VectorSupport$VectorPayload;\")                                                   \\\n@@ -926,2 +1062,7 @@\n-    do_signature(vector_gather_sig, \"(Ljava\/lang\/Class;Ljava\/lang\/Class;ILjava\/lang\/Class;\"                                                    \\\n-                                     \"Ljava\/lang\/Object;J\"                                                                                     \\\n+    do_signature(vector_gather_sig, \"(Ljava\/lang\/Class;\"                                                                                       \\\n+                                     \"Ljava\/lang\/Class;\"                                                                                       \\\n+                                     \"Ljava\/lang\/Class;\"                                                                                       \\\n+                                     \"I\"                                                                                                       \\\n+                                     \"Ljava\/lang\/Class;\"                                                                                       \\\n+                                     \"Ljava\/lang\/Object;\"                                                                                      \\\n+                                     \"J\"                                                                                                       \\\n@@ -929,1 +1070,3 @@\n-                                     \"Ljava\/lang\/Object;I[II\"                                                                                  \\\n+                                     \"Ljdk\/internal\/vm\/vector\/VectorSupport$VectorMask;\"                                                       \\\n+                                     \"Ljava\/lang\/Object;\"                                                                                      \\\n+                                     \"I[II\"                                                                                                    \\\n@@ -936,5 +1079,13 @@\n-    do_signature(vector_scatter_sig, \"(Ljava\/lang\/Class;Ljava\/lang\/Class;ILjava\/lang\/Class;\"                                                   \\\n-                                      \"Ljava\/lang\/Object;J\"                                                                                    \\\n-                                      \"Ljdk\/internal\/vm\/vector\/VectorSupport$Vector;Ljdk\/internal\/vm\/vector\/VectorSupport$Vector;\"             \\\n-                                      \"Ljava\/lang\/Object;I[II\"                                                                                 \\\n-                                      \"Ljdk\/internal\/vm\/vector\/VectorSupport$StoreVectorOperationWithMap;)V\")                                  \\\n+    do_signature(vector_scatter_sig, \"(Ljava\/lang\/Class;\"                                                                                      \\\n+                                      \"Ljava\/lang\/Class;\"                                                                                      \\\n+                                      \"Ljava\/lang\/Class;\"                                                                                      \\\n+                                      \"I\"                                                                                                      \\\n+                                      \"Ljava\/lang\/Class;\"                                                                                      \\\n+                                      \"Ljava\/lang\/Object;\"                                                                                     \\\n+                                      \"J\"                                                                                                      \\\n+                                      \"Ljdk\/internal\/vm\/vector\/VectorSupport$Vector;\"                                                          \\\n+                                      \"Ljdk\/internal\/vm\/vector\/VectorSupport$Vector;\"                                                          \\\n+                                      \"Ljdk\/internal\/vm\/vector\/VectorSupport$VectorMask;Ljava\/lang\/Object;\"                                    \\\n+                                      \"I[II\"                                                                                                   \\\n+                                      \"Ljdk\/internal\/vm\/vector\/VectorSupport$StoreVectorOperationWithMap;)\"                                    \\\n+                                      \"V\")                                                                                                     \\\n@@ -944,1 +1095,2 @@\n-   do_alias(vector_rebox_sig, object_object_signature)                                                                                         \\\n+    do_signature(vector_rebox_sig, \"(Ljdk\/internal\/vm\/vector\/VectorSupport$VectorPayload;)\"                                                    \\\n+                                    \"Ljdk\/internal\/vm\/vector\/VectorSupport$VectorPayload;\")                                                    \\\n@@ -948,2 +1100,7 @@\n-    do_signature(vector_mask_oper_sig, \"(ILjava\/lang\/Class;Ljava\/lang\/Class;ILjava\/lang\/Object;\"                                               \\\n-                                        \"Ljdk\/internal\/vm\/vector\/VectorSupport$VectorMaskOp;)I\")                                               \\\n+    do_signature(vector_mask_oper_sig, \"(I\"                                                                                                    \\\n+                                        \"Ljava\/lang\/Class;\"                                                                                    \\\n+                                        \"Ljava\/lang\/Class;\"                                                                                    \\\n+                                        \"I\"                                                                                                    \\\n+                                        \"Ljdk\/internal\/vm\/vector\/VectorSupport$VectorMask;\"                                                    \\\n+                                        \"Ljdk\/internal\/vm\/vector\/VectorSupport$VectorMaskOp;)\"                                                 \\\n+                                        \"J\")                                                                                                   \\\n","filename":"src\/hotspot\/share\/classfile\/vmIntrinsics.hpp","additions":207,"deletions":50,"binary":false,"changes":257,"status":"modified"},{"patch":"@@ -927,1 +927,1 @@\n-      CompileTask::print(st, this, msg, \/*short_form:*\/ false, \/* cr *\/ true, \/* timestamp *\/ false);\n+      CompileTask::print(st, this, msg, \/*short_form:*\/ false);\n","filename":"src\/hotspot\/share\/code\/nmethod.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -2203,1 +2203,1 @@\n-  push_jni_handle_block();\n+  JNIHandleMark jhm(thread);\n@@ -2322,3 +2322,0 @@\n-  \/\/ Remove the JNI handle block after the ciEnv destructor has run in\n-  \/\/ the previous block.\n-  pop_jni_handle_block();\n@@ -2485,32 +2482,0 @@\n-\/\/ ------------------------------------------------------------------\n-\/\/ CompileBroker::push_jni_handle_block\n-\/\/\n-\/\/ Push on a new block of JNI handles.\n-void CompileBroker::push_jni_handle_block() {\n-  JavaThread* thread = JavaThread::current();\n-\n-  \/\/ Allocate a new block for JNI handles.\n-  \/\/ Inlined code from jni_PushLocalFrame()\n-  JNIHandleBlock* java_handles = thread->active_handles();\n-  JNIHandleBlock* compile_handles = JNIHandleBlock::allocate_block(thread);\n-  assert(compile_handles != NULL && java_handles != NULL, \"should not be NULL\");\n-  compile_handles->set_pop_frame_link(java_handles);  \/\/ make sure java handles get gc'd.\n-  thread->set_active_handles(compile_handles);\n-}\n-\n-\n-\/\/ ------------------------------------------------------------------\n-\/\/ CompileBroker::pop_jni_handle_block\n-\/\/\n-\/\/ Pop off the current block of JNI handles.\n-void CompileBroker::pop_jni_handle_block() {\n-  JavaThread* thread = JavaThread::current();\n-\n-  \/\/ Release our JNI handle block\n-  JNIHandleBlock* compile_handles = thread->active_handles();\n-  JNIHandleBlock* java_handles = compile_handles->pop_frame_link();\n-  thread->set_active_handles(java_handles);\n-  compile_handles->set_pop_frame_link(NULL);\n-  JNIHandleBlock::release_block(compile_handles, thread); \/\/ may block\n-}\n-\n","filename":"src\/hotspot\/share\/compiler\/compileBroker.cpp","additions":1,"deletions":36,"binary":false,"changes":37,"status":"modified"},{"patch":"@@ -262,2 +262,0 @@\n-  static void push_jni_handle_block();\n-  static void pop_jni_handle_block();\n","filename":"src\/hotspot\/share\/compiler\/compileBroker.hpp","additions":0,"deletions":2,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -239,1 +239,1 @@\n-                             const char* msg, bool short_form, bool cr, bool timestamp,\n+                             const char* msg, bool short_form, bool cr,\n@@ -242,4 +242,2 @@\n-    if (timestamp) {\n-      \/\/ Print current time\n-      st->print(\"%7d \", (int)tty->time_stamp().milliseconds());\n-    }\n+    \/\/ Print current time\n+    st->print(\"%7d \", (int)tty->time_stamp().milliseconds());\n","filename":"src\/hotspot\/share\/compiler\/compileTask.cpp","additions":3,"deletions":5,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -107,1 +107,2 @@\n-    _lock = new Monitor(Mutex::safepoint, \"CompileTask_lock\");\n+    \/\/ May hold MethodCompileQueue_lock\n+    _lock = new Monitor(Mutex::safepoint-1, \"CompileTask_lock\");\n@@ -190,1 +191,1 @@\n-                                      const char* msg = NULL, bool short_form = false, bool cr = true, bool timestamp = true,\n+                                      const char* msg = NULL, bool short_form = false, bool cr = true,\n@@ -196,1 +197,1 @@\n-  static void  print(outputStream* st, const nmethod* nm, const char* msg = NULL, bool short_form = false, bool cr = true, bool timestamp = true) {\n+  static void  print(outputStream* st, const nmethod* nm, const char* msg = NULL, bool short_form = false, bool cr = true) {\n@@ -199,1 +200,1 @@\n-                           msg, short_form, cr, timestamp);\n+                           msg, short_form, cr);\n","filename":"src\/hotspot\/share\/compiler\/compileTask.hpp","additions":5,"deletions":4,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -110,1 +110,1 @@\n-void G1BarrierSetC1::post_barrier(LIRAccess& access, LIR_OprDesc* addr, LIR_OprDesc* new_val) {\n+void G1BarrierSetC1::post_barrier(LIRAccess& access, LIR_Opr addr, LIR_Opr new_val) {\n@@ -155,1 +155,1 @@\n-                            LIR_OprDesc::illegalOpr());\n+                            LIR_Opr::illegalOpr());\n@@ -161,1 +161,1 @@\n-                            LIR_OprDesc::illegalOpr());\n+                            LIR_Opr::illegalOpr());\n","filename":"src\/hotspot\/share\/gc\/g1\/c1\/g1BarrierSetC1.cpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -123,1 +123,1 @@\n-  virtual void post_barrier(LIRAccess& access, LIR_OprDesc* addr, LIR_OprDesc* new_val);\n+  virtual void post_barrier(LIRAccess& access, LIR_Opr addr, LIR_Opr new_val);\n","filename":"src\/hotspot\/share\/gc\/g1\/c1\/g1BarrierSetC1.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -216,36 +216,0 @@\n-HeapWord* G1BlockOffsetTablePart::forward_to_block_containing_addr_slow(HeapWord* q,\n-                                                                        HeapWord* n,\n-                                                                        const void* addr) {\n-  \/\/ We're not in the normal case.  We need to handle an important subcase\n-  \/\/ here: LAB allocation.  An allocation previously recorded in the\n-  \/\/ offset table was actually a lab allocation, and was divided into\n-  \/\/ several objects subsequently.  Fix this situation as we answer the\n-  \/\/ query, by updating entries as we cross them.\n-\n-  \/\/ If the fist object's end q is at the card boundary. Start refining\n-  \/\/ with the corresponding card (the value of the entry will be basically\n-  \/\/ set to 0). If the object crosses the boundary -- start from the next card.\n-  size_t n_index = _bot->index_for(n);\n-  size_t next_index = _bot->index_for(n) + !_bot->is_card_boundary(n);\n-  \/\/ Calculate a consistent next boundary.  If \"n\" is not at the boundary\n-  \/\/ already, step to the boundary.\n-  HeapWord* next_boundary = _bot->address_for_index(n_index) +\n-                            (n_index == next_index ? 0 : BOTConstants::N_words);\n-  assert(next_boundary <= _bot->_reserved.end(),\n-         \"next_boundary is beyond the end of the covered region \"\n-         \" next_boundary \" PTR_FORMAT \" _array->_end \" PTR_FORMAT,\n-         p2i(next_boundary), p2i(_bot->_reserved.end()));\n-  while (next_boundary < addr) {\n-    while (n <= next_boundary) {\n-      q = n;\n-      oop obj = cast_to_oop(q);\n-      if (obj->klass_or_null_acquire() == NULL) return q;\n-      n += block_size(q);\n-    }\n-    assert(q <= next_boundary && n > next_boundary, \"Consequence of loop\");\n-    \/\/ [q, n) is the block that crosses the boundary.\n-    alloc_block_work(&next_boundary, q, n);\n-  }\n-  return forward_to_block_containing_addr_const(q, n, addr);\n-}\n-\n","filename":"src\/hotspot\/share\/gc\/g1\/g1BlockOffsetTable.cpp","additions":0,"deletions":36,"binary":false,"changes":36,"status":"modified"},{"patch":"@@ -144,0 +144,1 @@\n+  \/\/ Return the address of the beginning of the block that contains \"addr\".\n@@ -145,19 +146,3 @@\n-  \/\/ next block (or the end of the space.)  Return the address of the\n-  \/\/ beginning of the block that contains \"addr\".  Does so without side\n-  \/\/ effects (see, e.g., spec of  block_start.)\n-  inline HeapWord* forward_to_block_containing_addr_const(HeapWord* q, HeapWord* n,\n-                                                          const void* addr) const;\n-\n-  \/\/ \"q\" is a block boundary that is <= \"addr\"; return the address of the\n-  \/\/ beginning of the block that contains \"addr\".  May have side effects\n-  \/\/ on \"this\", by updating imprecise entries.\n-  inline HeapWord* forward_to_block_containing_addr(HeapWord* q,\n-                                                    const void* addr);\n-\n-  \/\/ \"q\" is a block boundary that is <= \"addr\"; \"n\" is the address of the\n-  \/\/ next block (or the end of the space.)  Return the address of the\n-  \/\/ beginning of the block that contains \"addr\".  May have side effects\n-  \/\/ on \"this\", by updating imprecise entries.\n-  HeapWord* forward_to_block_containing_addr_slow(HeapWord* q,\n-                                                  HeapWord* n,\n-                                                  const void* addr);\n+  \/\/ next block (or the end of the space.)\n+  inline HeapWord* forward_to_block_containing_addr(HeapWord* q, HeapWord* n,\n+                                                    const void* addr) const;\n","filename":"src\/hotspot\/share\/gc\/g1\/g1BlockOffsetTable.hpp","additions":4,"deletions":19,"binary":false,"changes":23,"status":"modified"},{"patch":"@@ -49,6 +49,0 @@\n-  assert(addr >= _hr->bottom() && addr < _hr->top(), \"invalid address\");\n-  HeapWord* q = block_at_or_preceding(addr);\n-  return forward_to_block_containing_addr(q, addr);\n-}\n-\n-inline HeapWord* G1BlockOffsetTablePart::block_start_const(const void* addr) const {\n@@ -58,1 +52,1 @@\n-  return forward_to_block_containing_addr_const(q, n, addr);\n+  return forward_to_block_containing_addr(q, n, addr);\n@@ -144,2 +138,2 @@\n-inline HeapWord* G1BlockOffsetTablePart::forward_to_block_containing_addr_const(HeapWord* q, HeapWord* n,\n-                                                                                const void* addr) const {\n+inline HeapWord* G1BlockOffsetTablePart::forward_to_block_containing_addr(HeapWord* q, HeapWord* n,\n+                                                                          const void* addr) const {\n@@ -147,0 +141,7 @@\n+    \/\/ When addr is not covered by the block starting at q we need to\n+    \/\/ step forward until we find the correct block. With the BOT\n+    \/\/ being precise, we should never have to step through more than\n+    \/\/ a single card.\n+    assert(_bot->index_for(n) == _bot->index_for(addr),\n+           \"BOT not precise. Index for n: \" SIZE_FORMAT \" must be equal to the index for addr: \" SIZE_FORMAT,\n+           _bot->index_for(n), _bot->index_for(addr));\n@@ -159,17 +160,0 @@\n-inline HeapWord* G1BlockOffsetTablePart::forward_to_block_containing_addr(HeapWord* q,\n-                                                                          const void* addr) {\n-  if (cast_to_oop(q)->klass_or_null_acquire() == NULL) {\n-    return q;\n-  }\n-  HeapWord* n = q + block_size(q);\n-  \/\/ In the normal case, where the query \"addr\" is a card boundary, and the\n-  \/\/ offset table chunks are the same size as cards, the block starting at\n-  \/\/ \"q\" will contain addr, so the test below will fail, and we'll fall\n-  \/\/ through quickly.\n-  if (n <= addr) {\n-    q = forward_to_block_containing_addr_slow(q, n, addr);\n-  }\n-  assert(q <= addr, \"wrong order for current and arg\");\n-  return q;\n-}\n-\n","filename":"src\/hotspot\/share\/gc\/g1\/g1BlockOffsetTable.inline.hpp","additions":10,"deletions":26,"binary":false,"changes":36,"status":"modified"},{"patch":"@@ -47,0 +47,11 @@\n+static uint default_log2_card_region_per_region() {\n+  uint log2_card_region_per_heap_region = 0;\n+\n+  const uint card_container_limit = G1CardSetContainer::LogCardsPerRegionLimit;\n+  if (card_container_limit < (uint)HeapRegion::LogCardsPerRegion) {\n+    log2_card_region_per_heap_region = (uint)HeapRegion::LogCardsPerRegion - card_container_limit;\n+  }\n+\n+  return log2_card_region_per_heap_region;\n+}\n+\n@@ -53,2 +64,6 @@\n-                         (uint)HeapRegion::CardsPerRegion)                          \/* max_cards_in_cardset *\/\n-                         { }\n+                         (uint)HeapRegion::CardsPerRegion,                          \/* max_cards_in_cardset *\/\n+                         default_log2_card_region_per_region())                     \/* log2_card_region_per_region *\/\n+{\n+  assert((_log2_card_region_per_heap_region + _log2_cards_per_card_region) == (uint)HeapRegion::LogCardsPerRegion,\n+         \"inconsistent heap region virtualization setup\");\n+}\n@@ -60,1 +75,2 @@\n-                                               uint max_cards_in_card_set) :\n+                                               uint max_cards_in_card_set,\n+                                               uint log2_card_region_per_region) :\n@@ -68,2 +84,3 @@\n-                         max_cards_in_card_set)                                \/* max_cards_in_cardset *\/\n-                         { }\n+                         max_cards_in_card_set,                                \/* max_cards_in_cardset *\/\n+                         log2_card_region_per_region)\n+{ }\n@@ -76,1 +93,2 @@\n-                                               uint max_cards_in_card_set) :\n+                                               uint max_cards_in_card_set,\n+                                               uint log2_card_region_per_heap_region) :\n@@ -85,1 +103,3 @@\n-  _bitmap_hash_mask(~(~(0) << _log2_num_cards_in_howl_bitmap)) {\n+  _bitmap_hash_mask(~(~(0) << _log2_num_cards_in_howl_bitmap)),\n+  _log2_card_region_per_heap_region(log2_card_region_per_heap_region),\n+  _log2_cards_per_card_region(log2i_exact(_max_cards_in_card_set) - _log2_card_region_per_heap_region) {\n@@ -91,1 +111,0 @@\n-\n@@ -112,1 +131,2 @@\n-                          \"Howl Bitmap #elems %u size %zu coarsen threshold %u\",\n+                          \"Howl Bitmap #elems %u size %zu coarsen threshold %u \"\n+                          \"Card regions per heap region %u cards per card region %u\",\n@@ -116,1 +136,3 @@\n-                          num_cards_in_howl_bitmap(), G1CardSetBitMap::size_in_bytes(num_cards_in_howl_bitmap()), cards_in_howl_bitmap_threshold());\n+                          num_cards_in_howl_bitmap(), G1CardSetBitMap::size_in_bytes(num_cards_in_howl_bitmap()), cards_in_howl_bitmap_threshold(),\n+                          (uint)1 << log2_card_region_per_heap_region(),\n+                          (uint)1 << log2_cards_per_card_region());\n@@ -212,1 +234,1 @@\n-    G1CardSet::G1CardSetPtrIterator* _scan_f;\n+    G1CardSet::CardSetPtrClosure* _scan_f;\n@@ -214,1 +236,1 @@\n-    explicit G1CardSetHashTableScan(G1CardSet::G1CardSetPtrIterator* f) : _scan_f(f) { }\n+    explicit G1CardSetHashTableScan(G1CardSet::CardSetPtrClosure* f) : _scan_f(f) { }\n@@ -265,1 +287,1 @@\n-  void iterate_safepoint(G1CardSet::G1CardSetPtrIterator* cl2) {\n+  void iterate_safepoint(G1CardSet::CardSetPtrClosure* cl2) {\n@@ -270,1 +292,1 @@\n-  void iterate(G1CardSet::G1CardSetPtrIterator* cl2) {\n+  void iterate(G1CardSet::CardSetPtrClosure* cl2) {\n@@ -781,1 +803,1 @@\n-void G1CardSet::iterate_cards_during_transfer(CardSetPtr const card_set, CardVisitor& found) {\n+void G1CardSet::iterate_cards_during_transfer(CardSetPtr const card_set, CardVisitor& cl) {\n@@ -790,1 +812,1 @@\n-      ptr.iterate(found, _config->inline_ptr_bits_per_card());\n+      ptr.iterate(cl, _config->inline_ptr_bits_per_card());\n@@ -794,1 +816,1 @@\n-      card_set_ptr<G1CardSetArray>(card_set)->iterate(found);\n+      card_set_ptr<G1CardSetArray>(card_set)->iterate(cl);\n@@ -802,1 +824,1 @@\n-void G1CardSet::iterate_containers(G1CardSetPtrIterator* found, bool at_safepoint) {\n+void G1CardSet::iterate_containers(CardSetPtrClosure* cl, bool at_safepoint) {\n@@ -804,1 +826,1 @@\n-    _table->iterate_safepoint(found);\n+    _table->iterate_safepoint(cl);\n@@ -806,1 +828,1 @@\n-    _table->iterate(found);\n+    _table->iterate(cl);\n@@ -810,0 +832,1 @@\n+\/\/ Applied to all card (ranges) of the containers.\n@@ -811,2 +834,2 @@\n-class G1ContainerCards {\n-  Closure& _iter;\n+class G1ContainerCardsClosure {\n+  Closure& _cl;\n@@ -816,1 +839,1 @@\n-  G1ContainerCards(Closure& iter, uint region_idx) : _iter(iter), _region_idx(region_idx) { }\n+  G1ContainerCardsClosure(Closure& cl, uint region_idx) : _cl(cl), _region_idx(region_idx) { }\n@@ -821,1 +844,1 @@\n-    _iter.do_card(_region_idx, card_idx);\n+    _cl.do_card(_region_idx, card_idx);\n@@ -826,1 +849,1 @@\n-      _iter.do_card(_region_idx, card_idx);\n+      _cl.do_card(_region_idx, card_idx);\n@@ -831,3 +854,21 @@\n-void G1CardSet::iterate_cards(G1CardSetCardIterator& iter) {\n-  G1CardSetMergeCardIterator<G1CardSetCardIterator, G1ContainerCards> cl(this, iter);\n-  iterate_containers(&cl);\n+template <typename Closure, template <typename> class CardOrRanges>\n+class G1CardSetContainersClosure : public G1CardSet::CardSetPtrClosure {\n+  G1CardSet* _card_set;\n+  Closure& _cl;\n+\n+public:\n+\n+  G1CardSetContainersClosure(G1CardSet* card_set,\n+                             Closure& cl) :\n+    _card_set(card_set),\n+    _cl(cl) { }\n+\n+  void do_cardsetptr(uint region_idx, size_t num_occupied, G1CardSet::CardSetPtr card_set) override {\n+    CardOrRanges<Closure> cl(_cl, region_idx);\n+    _card_set->iterate_cards_or_ranges_in_container(card_set, cl);\n+  }\n+};\n+\n+void G1CardSet::iterate_cards(CardClosure& cl) {\n+  G1CardSetContainersClosure<CardClosure, G1ContainerCardsClosure> cl2(this, cl);\n+  iterate_containers(&cl2);\n@@ -849,1 +890,1 @@\n-  class GetNumberOfContainers : public G1CardSetPtrIterator {\n+  class GetNumberOfContainers : public CardSetPtrClosure {\n@@ -853,1 +894,1 @@\n-    GetNumberOfContainers() : G1CardSetPtrIterator(), _count(0) { }\n+    GetNumberOfContainers() : CardSetPtrClosure(), _count(0) { }\n","filename":"src\/hotspot\/share\/gc\/g1\/g1CardSet.cpp","additions":71,"deletions":30,"binary":false,"changes":101,"status":"modified"},{"patch":"@@ -61,0 +61,2 @@\n+  uint _log2_card_region_per_heap_region;\n+  uint _log2_cards_per_card_region;\n@@ -69,1 +71,2 @@\n-                         uint max_cards_in_card_set);\n+                         uint max_cards_in_card_set,\n+                         uint log2_card_region_per_heap_region);\n@@ -78,1 +81,1 @@\n-  \/\/ Only for test\n+  \/\/ Testing only.\n@@ -83,1 +86,2 @@\n-                         uint max_cards_in_card_set);\n+                         uint max_cards_in_cardset,\n+                         uint log2_card_region_per_region);\n@@ -118,0 +122,14 @@\n+  \/\/ Heap region virtualization: there are some limitations to how many cards the\n+  \/\/ containers can cover to save memory for the common case. Heap region virtualization\n+  \/\/ allows to use multiple entries in the G1CardSet hash table per area covered\n+  \/\/ by the remembered set (e.g. heap region); each such entry is called \"card_region\".\n+  \/\/\n+  \/\/ The next two members give information about how many card regions are there\n+  \/\/ per area (heap region) and how many cards each card region has.\n+\n+  \/\/ The log2 of the amount of card regions per heap region configured.\n+  uint log2_card_region_per_heap_region() const { return _log2_card_region_per_heap_region; }\n+  \/\/ The log2 of the number of cards per card region. This is calculated from max_cards_in_region()\n+  \/\/ and above.\n+  uint log2_cards_per_card_region() const { return _log2_cards_per_card_region; }\n+\n@@ -174,3 +192,0 @@\n-  template <typename Closure, template <typename> class CardorRanges>\n-  friend class G1CardSetMergeCardIterator;\n-\n@@ -279,18 +294,1 @@\n-  void iterate_cards_during_transfer(CardSetPtr const card_set, CardVisitor& found);\n-\n-  \/\/ Iterate over the container, calling a method on every card or card range contained\n-  \/\/ in the card container.\n-  \/\/ For every container, first calls\n-  \/\/\n-  \/\/   void start_iterate(uint tag, uint region_idx);\n-  \/\/\n-  \/\/ Then for every card or card range it calls\n-  \/\/\n-  \/\/   void do_card(uint card_idx);\n-  \/\/   void do_card_range(uint card_idx, uint length);\n-  \/\/\n-  \/\/ where card_idx is the card index within that region_idx passed before in\n-  \/\/ start_iterate().\n-  \/\/\n-  template <class CardOrRangeVisitor>\n-  void iterate_cards_or_ranges_in_container(CardSetPtr const card_set, CardOrRangeVisitor& found);\n+  void iterate_cards_during_transfer(CardSetPtr const card_set, CardVisitor& vl);\n@@ -343,2 +341,18 @@\n-  \/\/ Various iterators - should be made inlineable somehow.\n-  class G1CardSetPtrIterator {\n+  \/\/ Iterate over the container, calling a method on every card or card range contained\n+  \/\/ in the card container.\n+  \/\/ For every container, first calls\n+  \/\/\n+  \/\/   void start_iterate(uint tag, uint region_idx);\n+  \/\/\n+  \/\/ Then for every card or card range it calls\n+  \/\/\n+  \/\/   void do_card(uint card_idx);\n+  \/\/   void do_card_range(uint card_idx, uint length);\n+  \/\/\n+  \/\/ where card_idx is the card index within that region_idx passed before in\n+  \/\/ start_iterate().\n+  \/\/\n+  template <class CardOrRangeVisitor>\n+  void iterate_cards_or_ranges_in_container(CardSetPtr const card_set, CardOrRangeVisitor& cl);\n+\n+  class CardSetPtrClosure {\n@@ -349,1 +363,1 @@\n-  void iterate_containers(G1CardSetPtrIterator* iter, bool safepoint = false);\n+  void iterate_containers(CardSetPtrClosure* cl, bool safepoint = false);\n@@ -351,1 +365,1 @@\n-  class G1CardSetCardIterator {\n+  class CardClosure {\n@@ -356,6 +370,1 @@\n-  void iterate_cards(G1CardSetCardIterator& iter);\n-\n-  \/\/ Iterate all cards for card set merging. Must be a CardOrRangeVisitor as\n-  \/\/ explained above.\n-  template <class CardOrRangeVisitor>\n-  void iterate_for_merge(CardOrRangeVisitor& cl);\n+  void iterate_cards(CardClosure& cl);\n","filename":"src\/hotspot\/share\/gc\/g1\/g1CardSet.hpp","additions":43,"deletions":34,"binary":false,"changes":77,"status":"modified"},{"patch":"@@ -45,1 +45,1 @@\n-inline void G1CardSet::iterate_cards_or_ranges_in_container(CardSetPtr const card_set, CardOrRangeVisitor& found) {\n+inline void G1CardSet::iterate_cards_or_ranges_in_container(CardSetPtr const card_set, CardOrRangeVisitor& cl) {\n@@ -48,1 +48,1 @@\n-      if (found.start_iterate(G1GCPhaseTimes::MergeRSMergedInline)) {\n+      if (cl.start_iterate(G1GCPhaseTimes::MergeRSMergedInline)) {\n@@ -50,1 +50,1 @@\n-        ptr.iterate(found, _config->inline_ptr_bits_per_card());\n+        ptr.iterate(cl, _config->inline_ptr_bits_per_card());\n@@ -55,2 +55,2 @@\n-      if (found.start_iterate(G1GCPhaseTimes::MergeRSMergedArrayOfCards)) {\n-        card_set_ptr<G1CardSetArray>(card_set)->iterate(found);\n+      if (cl.start_iterate(G1GCPhaseTimes::MergeRSMergedArrayOfCards)) {\n+        card_set_ptr<G1CardSetArray>(card_set)->iterate(cl);\n@@ -68,2 +68,2 @@\n-        if (found.start_iterate(G1GCPhaseTimes::MergeRSMergedFull)) {\n-          found(0, _config->max_cards_in_region());\n+        if (cl.start_iterate(G1GCPhaseTimes::MergeRSMergedFull)) {\n+          cl(0, _config->max_cards_in_region());\n@@ -73,2 +73,2 @@\n-      if (found.start_iterate(G1GCPhaseTimes::MergeRSMergedHowl)) {\n-        card_set_ptr<G1CardSetHowl>(card_set)->iterate(found, _config);\n+      if (cl.start_iterate(G1GCPhaseTimes::MergeRSMergedHowl)) {\n+        card_set_ptr<G1CardSetHowl>(card_set)->iterate(cl, _config);\n@@ -83,42 +83,0 @@\n-template <typename Closure>\n-class G1ContainerCardsOrRanges {\n-  Closure& _iter;\n-  uint _region_idx;\n-\n-public:\n-  G1ContainerCardsOrRanges(Closure& iter, uint region_idx) : _iter(iter), _region_idx(region_idx) { }\n-\n-  bool start_iterate(uint tag) {\n-    return _iter.start_iterate(tag, _region_idx);\n-  }\n-\n-  void operator()(uint card_idx) {\n-    _iter.do_card(card_idx);\n-  }\n-\n-  void operator()(uint card_idx, uint length) {\n-    _iter.do_card_range(card_idx, length);\n-  }\n-};\n-\n-template <typename Closure, template <typename> class CardOrRanges>\n-class G1CardSetMergeCardIterator : public G1CardSet::G1CardSetPtrIterator {\n-  G1CardSet* _card_set;\n-  Closure& _iter;\n-\n-public:\n-\n-  G1CardSetMergeCardIterator(G1CardSet* card_set, Closure& iter) : _card_set(card_set), _iter(iter) { }\n-\n-  void do_cardsetptr(uint region_idx, size_t num_occupied, G1CardSet::CardSetPtr card_set) override {\n-    CardOrRanges<Closure> cl(_iter, region_idx);\n-    _card_set->iterate_cards_or_ranges_in_container(card_set, cl);\n-  }\n-};\n-\n-template <class CardOrRangeVisitor>\n-inline void G1CardSet::iterate_for_merge(CardOrRangeVisitor& cl) {\n-  G1CardSetMergeCardIterator<CardOrRangeVisitor, G1ContainerCardsOrRanges> cl2(this, cl);\n-  iterate_containers(&cl2, true \/* at_safepoint *\/);\n-}\n-\n","filename":"src\/hotspot\/share\/gc\/g1\/g1CardSet.inline.hpp","additions":9,"deletions":51,"binary":false,"changes":60,"status":"modified"},{"patch":"@@ -1831,1 +1831,0 @@\n-                           (ParallelGCThreads > 1) || (ConcGCThreads > 1), \/\/ mt discovery\n@@ -1840,1 +1839,0 @@\n-                           (ParallelGCThreads > 1),              \/\/ mt discovery\n","filename":"src\/hotspot\/share\/gc\/g1\/g1CollectedHeap.cpp","additions":0,"deletions":2,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -103,1 +103,1 @@\n-  bool do_object_b(oop p);\n+  bool do_object_b(oop p) override;\n@@ -110,1 +110,1 @@\n-  bool do_object_b(oop p);\n+  bool do_object_b(oop p) override;\n@@ -117,1 +117,1 @@\n-  virtual void on_commit(uint start_idx, size_t num_regions, bool zero_filled);\n+  void on_commit(uint start_idx, size_t num_regions, bool zero_filled) override;\n@@ -238,1 +238,1 @@\n-    bool default_value() const { return false; }\n+    bool default_value() const override { return false; }\n@@ -309,1 +309,1 @@\n-  void trace_heap(GCWhen::Type when, const GCTracer* tracer);\n+  void trace_heap(GCWhen::Type when, const GCTracer* tracer) override;\n@@ -442,3 +442,3 @@\n-  virtual HeapWord* allocate_new_tlab(size_t min_size,\n-                                      size_t requested_size,\n-                                      size_t* actual_size);\n+  HeapWord* allocate_new_tlab(size_t min_size,\n+                              size_t requested_size,\n+                              size_t* actual_size) override;\n@@ -446,2 +446,2 @@\n-  virtual HeapWord* mem_allocate(size_t word_size,\n-                                 bool*  gc_overhead_limit_was_exceeded);\n+  HeapWord* mem_allocate(size_t word_size,\n+                         bool*  gc_overhead_limit_was_exceeded) override;\n@@ -498,1 +498,1 @@\n-  virtual void do_full_collection(bool clear_all_soft_refs);\n+  void do_full_collection(bool clear_all_soft_refs) override;\n@@ -780,2 +780,0 @@\n-  void wait_for_root_region_scanning();\n-\n@@ -894,1 +892,1 @@\n-  jint initialize();\n+  jint initialize() override;\n@@ -899,3 +897,3 @@\n-  virtual void stop();\n-  virtual void safepoint_synchronize_begin();\n-  virtual void safepoint_synchronize_end();\n+  void stop() override;\n+  void safepoint_synchronize_begin() override;\n+  void safepoint_synchronize_end() override;\n@@ -904,1 +902,1 @@\n-  void post_initialize();\n+  void post_initialize() override;\n@@ -909,1 +907,1 @@\n-  virtual Name kind() const {\n+  Name kind() const override {\n@@ -913,1 +911,1 @@\n-  virtual const char* name() const {\n+  const char* name() const override {\n@@ -930,6 +928,1 @@\n-  virtual SoftRefPolicy* soft_ref_policy();\n-\n-  virtual void initialize_serviceability();\n-  virtual MemoryUsage memory_usage();\n-  virtual GrowableArray<GCMemoryManager*> memory_managers();\n-  virtual GrowableArray<MemoryPool*> memory_pools();\n+  SoftRefPolicy* soft_ref_policy() override;\n@@ -937,1 +930,4 @@\n-  virtual void fill_with_dummy_object(HeapWord* start, HeapWord* end, bool zap);\n+  void initialize_serviceability() override;\n+  MemoryUsage memory_usage() override;\n+  GrowableArray<GCMemoryManager*> memory_managers() override;\n+  GrowableArray<MemoryPool*> memory_pools() override;\n@@ -939,2 +935,1 @@\n-  \/\/ Try to minimize the remembered set.\n-  void scrub_rem_set();\n+  void fill_with_dummy_object(HeapWord* start, HeapWord* end, bool zap) override;\n@@ -961,2 +956,2 @@\n-  virtual size_t capacity() const;\n-  virtual size_t used() const;\n+  size_t capacity() const override;\n+  size_t used() const override;\n@@ -974,1 +969,1 @@\n-  virtual bool is_maximal_no_gc() const {\n+  bool is_maximal_no_gc() const override {\n@@ -1029,1 +1024,1 @@\n-  virtual void collect(GCCause::Cause cause);\n+  void collect(GCCause::Cause cause) override;\n@@ -1043,1 +1038,1 @@\n-  virtual bool is_in(const void* p) const;\n+  bool is_in(const void* p) const override;\n@@ -1083,1 +1078,1 @@\n-  virtual void object_iterate(ObjectClosure* cl);\n+  void object_iterate(ObjectClosure* cl) override;\n@@ -1085,1 +1080,1 @@\n-  virtual ParallelObjectIterator* parallel_object_iterator(uint thread_num);\n+  ParallelObjectIterator* parallel_object_iterator(uint thread_num) override;\n@@ -1088,1 +1083,1 @@\n-  virtual void keep_alive(oop obj);\n+  void keep_alive(oop obj) override;\n@@ -1179,4 +1174,4 @@\n-  size_t tlab_capacity(Thread* ignored) const;\n-  size_t tlab_used(Thread* ignored) const;\n-  size_t max_tlab_size() const;\n-  size_t unsafe_max_tlab_alloc(Thread* ignored) const;\n+  size_t tlab_capacity(Thread* ignored) const override;\n+  size_t tlab_used(Thread* ignored) const override;\n+  size_t max_tlab_size() const override;\n+  size_t unsafe_max_tlab_alloc(Thread* ignored) const override;\n@@ -1206,1 +1201,1 @@\n-  virtual size_t max_capacity() const;\n+  size_t max_capacity() const override;\n@@ -1269,1 +1264,1 @@\n-  virtual void register_nmethod(nmethod* nm);\n+  void register_nmethod(nmethod* nm) override;\n@@ -1272,1 +1267,1 @@\n-  virtual void unregister_nmethod(nmethod* nm);\n+  void unregister_nmethod(nmethod* nm) override;\n@@ -1275,1 +1270,1 @@\n-  virtual void flush_nmethod(nmethod* nm) {}\n+  void flush_nmethod(nmethod* nm) override {}\n@@ -1278,1 +1273,1 @@\n-  virtual void verify_nmethod(nmethod* nm) {}\n+  void verify_nmethod(nmethod* nm) override {}\n@@ -1300,1 +1295,1 @@\n-  virtual void prepare_for_verify();\n+  void prepare_for_verify() override;\n@@ -1317,1 +1312,1 @@\n-  void verify(VerifyOption vo);\n+  void verify(VerifyOption vo) override;\n@@ -1320,1 +1315,1 @@\n-  virtual bool supports_concurrent_gc_breakpoints() const;\n+  bool supports_concurrent_gc_breakpoints() const override;\n@@ -1322,1 +1317,1 @@\n-  virtual WorkerThreads* safepoint_workers() { return _workers; }\n+  WorkerThreads* safepoint_workers() override { return _workers; }\n@@ -1324,1 +1319,1 @@\n-  virtual bool is_archived_object(oop object) const;\n+  bool is_archived_object(oop object) const override;\n@@ -1347,3 +1342,3 @@\n-  virtual void print_on(outputStream* st) const;\n-  virtual void print_extended_on(outputStream* st) const;\n-  virtual void print_on_error(outputStream* st) const;\n+  void print_on(outputStream* st) const override;\n+  void print_extended_on(outputStream* st) const override;\n+  void print_on_error(outputStream* st) const override;\n@@ -1351,1 +1346,1 @@\n-  virtual void gc_threads_do(ThreadClosure* tc) const;\n+  void gc_threads_do(ThreadClosure* tc) const override;\n@@ -1354,1 +1349,1 @@\n-  void print_tracing_info() const;\n+  void print_tracing_info() const override;\n@@ -1361,1 +1356,1 @@\n-  virtual bool print_location(outputStream* st, void* addr) const;\n+  bool print_location(outputStream* st, void* addr) const override;\n","filename":"src\/hotspot\/share\/gc\/g1\/g1CollectedHeap.hpp","additions":52,"deletions":57,"binary":false,"changes":109,"status":"modified"},{"patch":"@@ -33,1 +33,1 @@\n-  return _active.at(index);\n+  return _active.par_at(index);\n","filename":"src\/hotspot\/share\/gc\/g1\/g1CommittedRegionMap.inline.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -1621,4 +1621,0 @@\n-    \/\/ Set the concurrency level. The phase was already set prior to\n-    \/\/ executing the remark task.\n-    set_concurrency(active_workers);\n-\n","filename":"src\/hotspot\/share\/gc\/g1\/g1ConcurrentMark.cpp","additions":0,"deletions":4,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -149,1 +149,0 @@\n-  uint volatile* _num_failed_regions;\n@@ -154,1 +153,0 @@\n-                                uint volatile* num_failed_regions,\n@@ -158,1 +156,0 @@\n-    _num_failed_regions(num_failed_regions),\n@@ -197,2 +194,0 @@\n-\n-      Atomic::inc(_num_failed_regions, memory_order_relaxed);\n@@ -208,2 +203,1 @@\n-  _evac_failure_regions(evac_failure_regions),\n-  _num_failed_regions(0) { }\n+  _evac_failure_regions(evac_failure_regions) { }\n@@ -212,1 +206,1 @@\n-  RemoveSelfForwardPtrHRClosure rsfp_cl(worker_id, &_num_failed_regions, _evac_failure_regions);\n+  RemoveSelfForwardPtrHRClosure rsfp_cl(worker_id, _evac_failure_regions);\n@@ -219,1 +213,1 @@\n-  return Atomic::load(&_num_failed_regions);\n+  return _evac_failure_regions->num_regions_failed_evacuation();\n","filename":"src\/hotspot\/share\/gc\/g1\/g1EvacFailure.cpp","additions":3,"deletions":9,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -29,1 +29,0 @@\n-#include \"gc\/g1\/heapRegion.hpp\"\n@@ -65,7 +64,0 @@\n-void G1EvacFailureObjectsSet::record(oop obj) {\n-  assert(obj != NULL, \"must be\");\n-  assert(_region_idx == G1CollectedHeap::heap()->heap_region_containing(obj)->hrm_index(), \"must be\");\n-  OffsetInRegion* e = _offsets.allocate();\n-  *e = to_offset(obj);\n-}\n-\n","filename":"src\/hotspot\/share\/gc\/g1\/g1EvacFailureObjectsSet.cpp","additions":0,"deletions":8,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -25,2 +25,2 @@\n-#ifndef SHARE_GC_G1_G1EVACUATIONFAILUREOBJSINHR_HPP\n-#define SHARE_GC_G1_G1EVACUATIONFAILUREOBJSINHR_HPP\n+#ifndef SHARE_GC_G1_G1EVACFAILUREOBJECTSSET_HPP\n+#define SHARE_GC_G1_G1EVACFAILUREOBJECTSSET_HPP\n@@ -73,1 +73,1 @@\n-  void record(oop obj);\n+  inline void record(oop obj);\n@@ -82,1 +82,1 @@\n-#endif \/\/SHARE_GC_G1_G1EVACUATIONFAILUREOBJSINHR_HPP\n+#endif \/\/SHARE_GC_G1_G1EVACFAILUREOBJECTSSET_HPP\n","filename":"src\/hotspot\/share\/gc\/g1\/g1EvacFailureObjectsSet.hpp","additions":4,"deletions":4,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -0,0 +1,40 @@\n+\/*\n+ * Copyright (c) 2021, Huawei and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef SHARE_GC_G1_G1EVACFAILUREOBJECTSSET_INLINE_HPP\n+#define SHARE_GC_G1_G1EVACFAILUREOBJECTSSET_INLINE_HPP\n+\n+#include \"gc\/g1\/g1EvacFailureObjectsSet.hpp\"\n+#include \"gc\/g1\/g1CollectedHeap.hpp\"\n+#include \"gc\/g1\/g1SegmentedArray.inline.hpp\"\n+#include \"gc\/g1\/heapRegion.inline.hpp\"\n+\n+void G1EvacFailureObjectsSet::record(oop obj) {\n+  assert(obj != NULL, \"must be\");\n+  assert(_region_idx == G1CollectedHeap::heap()->heap_region_containing(obj)->hrm_index(), \"must be\");\n+  OffsetInRegion* e = _offsets.allocate();\n+  *e = to_offset(obj);\n+}\n+\n+#endif \/\/SHARE_GC_G1_G1EVACFAILUREOBJECTSSET_INLINE_HPP\n","filename":"src\/hotspot\/share\/gc\/g1\/g1EvacFailureObjectsSet.inline.hpp","additions":40,"deletions":0,"binary":false,"changes":40,"status":"added"},{"patch":"@@ -63,2 +63,1 @@\n-  HeapWord* destination = cast_from_oop<HeapWord*>(obj->forwardee());\n-  if (destination == NULL) {\n+  if (!obj->is_forwarded()) {\n@@ -69,0 +68,2 @@\n+  HeapWord* destination = cast_from_oop<HeapWord*>(obj->forwardee());\n+\n","filename":"src\/hotspot\/share\/gc\/g1\/g1FullGCCompactTask.cpp","additions":3,"deletions":2,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -106,0 +106,1 @@\n+    assert(object->is_forwarded(), \"must be forwarded\");\n@@ -107,14 +108,1 @@\n-    if (object->forwardee() != NULL) {\n-      \/\/ Object should not move but mark-word is used so it looks like the\n-      \/\/ object is forwarded. Need to clear the mark and it's no problem\n-      \/\/ since it will be restored by preserved marks.\n-      object->init_mark();\n-    } else {\n-      \/\/ Make sure object has the correct mark-word set or that it will be\n-      \/\/ fixed when restoring the preserved marks.\n-      assert(object->mark() == markWord::prototype() || \/\/ Correct mark\n-             object->mark_must_be_preserved(), \/\/ Will be restored by PreservedMarksSet\n-             \"should have correct prototype obj: \" PTR_FORMAT \" mark: \" PTR_FORMAT \" prototype: \" PTR_FORMAT,\n-             p2i(object), object->mark().value(), markWord::prototype().value());\n-    }\n-    assert(object->forwardee() == NULL, \"should be forwarded to NULL\");\n+    assert(!object->is_forwarded(), \"must not be forwarded\");\n","filename":"src\/hotspot\/share\/gc\/g1\/g1FullGCCompactionPoint.cpp","additions":2,"deletions":14,"binary":false,"changes":16,"status":"modified"},{"patch":"@@ -80,8 +80,5 @@\n-  oop forwardee = obj->forwardee();\n-  if (forwardee == NULL) {\n-    \/\/ Not forwarded, return current reference.\n-    assert(obj->mark() == markWord::prototype() || \/\/ Correct mark\n-           obj->mark_must_be_preserved(), \/\/ Will be restored by PreservedMarksSet\n-           \"Must have correct prototype or be preserved, obj: \" PTR_FORMAT \", mark: \" PTR_FORMAT \", prototype: \" PTR_FORMAT,\n-           p2i(obj), obj->mark().value(), markWord::prototype().value());\n-    return;\n+  if (obj->is_forwarded()) {\n+    oop forwardee = obj->forwardee();\n+    \/\/ Forwarded, just update.\n+    assert(G1CollectedHeap::heap()->is_in_reserved(forwardee), \"should be in object space\");\n+    RawAccess<IS_NOT_NULL>::oop_store(p, forwardee);\n@@ -90,3 +87,0 @@\n-  \/\/ Forwarded, just update.\n-  assert(G1CollectedHeap::heap()->is_in_reserved(forwardee), \"should be in object space\");\n-  RawAccess<IS_NOT_NULL>::oop_store(p, forwardee);\n","filename":"src\/hotspot\/share\/gc\/g1\/g1FullGCOopClosures.inline.hpp","additions":5,"deletions":11,"binary":false,"changes":16,"status":"modified"},{"patch":"@@ -171,2 +171,1 @@\n-  oop forwarded_to = obj->forwardee();\n-  if (forwarded_to != NULL && !_current->is_in(forwarded_to)) {\n+  if (obj->is_forwarded() && !_current->is_in(obj->forwardee())) {\n","filename":"src\/hotspot\/share\/gc\/g1\/g1FullGCPrepareTask.cpp","additions":1,"deletions":2,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2001, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2001, 2021, Oracle and\/or its affiliates. All rights reserved.\n","filename":"src\/hotspot\/share\/gc\/g1\/g1OopClosures.inline.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -109,2 +109,2 @@\n-  \/\/ Testing showed that 8 for 1M\/2M region, 16 for 4M\/8M regions, 32 for 16\/32M regions\n-  \/\/ seems to be such a good trade-off.\n+  \/\/ Testing showed that 8 for 1M\/2M region, 16 for 4M\/8M regions, 32 for 16\/32M regions,\n+  \/\/ and so on seems to be such a good trade-off.\n@@ -115,2 +115,2 @@\n-    assert(log_region_size >= 20 && log_region_size <= 25,\n-           \"expected value in [20,25], but got %u\", log_region_size);\n+    assert(log_region_size >= 20 && log_region_size <= 29,\n+           \"expected value in [20,29], but got %u\", log_region_size);\n","filename":"src\/hotspot\/share\/gc\/g1\/g1RemSet.cpp","additions":4,"deletions":4,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -859,1 +859,1 @@\n-      assert( obj->is_forwarded(), \"invariant\" );\n+      assert(obj->is_forwarded(), \"invariant\" );\n","filename":"src\/hotspot\/share\/gc\/g1\/g1YoungCollector.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -110,6 +110,0 @@\n-  ~RemoveSelfForwardPtrsTask() {\n-    assert(_task.num_failed_regions() == _evac_failure_regions->num_regions_failed_evacuation(),\n-           \"Removed regions %u inconsistent with expected %u\",\n-           _task.num_failed_regions(), _evac_failure_regions->num_regions_failed_evacuation());\n-  }\n-\n","filename":"src\/hotspot\/share\/gc\/g1\/g1YoungGCPostEvacuateTasks.cpp","additions":0,"deletions":6,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -260,1 +260,1 @@\n-          range(0, 32*M)                                                    \\\n+          range(0, NOT_LP64(32*M) LP64_ONLY(512*M))                         \\\n","filename":"src\/hotspot\/share\/gc\/g1\/g1_globals.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -68,2 +68,3 @@\n-    region_size = MAX2(max_heap_size \/ HeapRegionBounds::target_number(),\n-                       HeapRegionBounds::min_size());\n+    region_size = clamp(max_heap_size \/ HeapRegionBounds::target_number(),\n+                        HeapRegionBounds::min_size(),\n+                        HeapRegionBounds::max_ergonomics_size());\n","filename":"src\/hotspot\/share\/gc\/g1\/heapRegion.cpp","additions":3,"deletions":2,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -33,0 +33,1 @@\n+#include \"gc\/g1\/g1EvacFailureObjectsSet.inline.hpp\"\n@@ -86,4 +87,0 @@\n-inline HeapWord* HeapRegion::block_start_const(const void* p) const {\n-  return _bot_part.block_start_const(p);\n-}\n-\n@@ -358,2 +355,0 @@\n-  \/\/ Update BOT as needed while finding start of (possibly dead)\n-  \/\/ object containing the start of the region.\n","filename":"src\/hotspot\/share\/gc\/g1\/heapRegion.inline.hpp","additions":1,"deletions":6,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -37,0 +37,2 @@\n+  \/\/ Maximum region size determined ergonomically.\n+  static const size_t MAX_ERGONOMICS_SIZE = 32 * 1024 * 1024;\n@@ -42,1 +44,1 @@\n-  static const size_t MAX_REGION_SIZE = 32 * 1024 * 1024;\n+  static const size_t MAX_REGION_SIZE = 512 * 1024 * 1024;\n@@ -50,0 +52,1 @@\n+  static inline size_t max_ergonomics_size();\n","filename":"src\/hotspot\/share\/gc\/g1\/heapRegionBounds.hpp","additions":4,"deletions":1,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -34,0 +34,4 @@\n+size_t HeapRegionBounds::max_ergonomics_size() {\n+  return MAX_ERGONOMICS_SIZE;\n+}\n+\n","filename":"src\/hotspot\/share\/gc\/g1\/heapRegionBounds.inline.hpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -29,0 +29,1 @@\n+#include \"gc\/g1\/g1CardSetContainers.inline.hpp\"\n","filename":"src\/hotspot\/share\/gc\/g1\/heapRegionRemSet.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -58,0 +58,50 @@\n+template <typename Closure>\n+class G1ContainerCardsOrRanges {\n+  Closure& _cl;\n+  uint _region_idx;\n+  uint _offset;\n+\n+public:\n+  G1ContainerCardsOrRanges(Closure& cl, uint region_idx, uint offset) : _cl(cl), _region_idx(region_idx), _offset(offset) { }\n+\n+  bool start_iterate(uint tag) {\n+    return _cl.start_iterate(tag, _region_idx);\n+  }\n+\n+  void operator()(uint card_idx) {\n+    _cl.do_card(card_idx + _offset);\n+  }\n+\n+  void operator()(uint card_idx, uint length) {\n+    _cl.do_card_range(card_idx + _offset, length);\n+  }\n+};\n+\n+template <typename Closure, template <typename> class CardOrRanges>\n+class G1HeapRegionRemSetMergeCardClosure : public G1CardSet::CardSetPtrClosure {\n+  G1CardSet* _card_set;\n+  Closure& _cl;\n+  uint _log_card_regions_per_region;\n+  uint _card_regions_per_region_mask;\n+  uint _log_card_region_size;\n+\n+public:\n+\n+  G1HeapRegionRemSetMergeCardClosure(G1CardSet* card_set,\n+                                      Closure& cl,\n+                                      uint log_card_regions_per_region,\n+                                      uint log_card_region_size) :\n+    _card_set(card_set),\n+    _cl(cl),\n+    _log_card_regions_per_region(log_card_regions_per_region),\n+    _card_regions_per_region_mask((1 << log_card_regions_per_region) - 1),\n+    _log_card_region_size(log_card_region_size) {\n+  }\n+\n+  void do_cardsetptr(uint card_region_idx, size_t num_occupied, G1CardSet::CardSetPtr card_set) override {\n+    CardOrRanges<Closure> cl(_cl,\n+                             card_region_idx >> _log_card_regions_per_region,\n+                             (card_region_idx & _card_regions_per_region_mask) << _log_card_region_size);\n+    _card_set->iterate_cards_or_ranges_in_container(card_set, cl);\n+  }\n+};\n@@ -61,1 +111,5 @@\n-  _card_set.iterate_for_merge(cl);\n+  G1HeapRegionRemSetMergeCardClosure<CardOrRangeVisitor, G1ContainerCardsOrRanges> cl2(&_card_set,\n+                                                                                       cl,\n+                                                                                       _card_set.config()->log2_card_region_per_heap_region(),\n+                                                                                       _card_set.config()->log2_cards_per_card_region());\n+  _card_set.iterate_containers(&cl2, true \/* at_safepoint *\/);\n","filename":"src\/hotspot\/share\/gc\/g1\/heapRegionRemSet.inline.hpp","additions":55,"deletions":1,"binary":false,"changes":56,"status":"modified"},{"patch":"@@ -855,1 +855,0 @@\n-      true,                \/\/ mt discovery\n","filename":"src\/hotspot\/share\/gc\/parallel\/psParallelCompact.cpp","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -802,1 +802,0 @@\n-                           true,                       \/\/ mt discovery\n","filename":"src\/hotspot\/share\/gc\/parallel\/psScavenge.cpp","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -881,5 +881,0 @@\n-void DefNewGeneration::ref_processor_init() {\n-  Generation::ref_processor_init();\n-}\n-\n-\n","filename":"src\/hotspot\/share\/gc\/serial\/defNewGeneration.cpp","additions":0,"deletions":5,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -205,2 +205,0 @@\n-  virtual void ref_processor_init();\n-\n","filename":"src\/hotspot\/share\/gc\/serial\/defNewGeneration.hpp","additions":0,"deletions":2,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -91,7 +91,2 @@\n-    oop new_obj = cast_to_oop(obj->mark().decode_pointer());\n-\n-    assert(new_obj != NULL ||                      \/\/ is forwarding ptr?\n-           obj->mark() == markWord::prototype(), \/\/ not gc marked?\n-           \"should be forwarded\");\n-\n-    if (new_obj != NULL) {\n+    if (obj->is_forwarded()) {\n+      oop new_obj = obj->forwardee();\n","filename":"src\/hotspot\/share\/gc\/serial\/markSweep.inline.hpp","additions":2,"deletions":7,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -44,1 +44,1 @@\n-  LIRAddressOpr(LIRItem& item) : _item(&item), _opr(NULL) {}\n+  LIRAddressOpr(LIRItem& item) : _item(&item), _opr() {}\n@@ -83,1 +83,1 @@\n-    _resolved_addr(NULL),\n+    _resolved_addr(),\n","filename":"src\/hotspot\/share\/gc\/shared\/c1\/barrierSetC1.hpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -38,1 +38,1 @@\n-void CardTableBarrierSetC1::post_barrier(LIRAccess& access, LIR_OprDesc* addr, LIR_OprDesc* new_val) {\n+void CardTableBarrierSetC1::post_barrier(LIRAccess& access, LIR_Opr addr, LIR_Opr new_val) {\n","filename":"src\/hotspot\/share\/gc\/shared\/c1\/cardTableBarrierSetC1.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -32,1 +32,1 @@\n-  virtual void post_barrier(LIRAccess& access, LIR_OprDesc* addr, LIR_OprDesc* new_val);\n+  virtual void post_barrier(LIRAccess& access, LIR_Opr addr, LIR_Opr new_val);\n","filename":"src\/hotspot\/share\/gc\/shared\/c1\/cardTableBarrierSetC1.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -38,2 +38,2 @@\n-  virtual void post_barrier(LIRAccess& access, LIR_OprDesc* addr,\n-                            LIR_OprDesc* new_val) {}\n+  virtual void post_barrier(LIRAccess& access, LIR_Opr addr,\n+                            LIR_Opr new_val) {}\n","filename":"src\/hotspot\/share\/gc\/shared\/c1\/modRefBarrierSetC1.hpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -464,8 +464,9 @@\n-  \/\/ Provides a thread pool to SafepointSynchronize to use\n-  \/\/ for parallel safepoint cleanup.\n-  \/\/ GCs that use a GC worker thread pool may want to share\n-  \/\/ it for use during safepoint cleanup. This is only possible\n-  \/\/ if the GC can pause and resume concurrent work (e.g. G1\n-  \/\/ concurrent marking) for an intermittent non-GC safepoint.\n-  \/\/ If this method returns NULL, SafepointSynchronize will\n-  \/\/ perform cleanup tasks serially in the VMThread.\n+  \/\/ Workers used in non-GC safepoints for parallel safepoint cleanup. If this\n+  \/\/ method returns NULL, cleanup tasks are done serially in the VMThread. See\n+  \/\/ `SafepointSynchronize::do_cleanup_tasks` for details.\n+  \/\/ GCs using a GC worker thread pool inside GC safepoints may opt to share\n+  \/\/ that pool with non-GC safepoints, avoiding creating extraneous threads.\n+  \/\/ Such sharing is safe, because GC safepoints and non-GC safepoints never\n+  \/\/ overlap. For example, `G1CollectedHeap::workers()` (for GC safepoints) and\n+  \/\/ `G1CollectedHeap::safepoint_workers()` (for non-GC safepoints) return the\n+  \/\/ same thread-pool.\n","filename":"src\/hotspot\/share\/gc\/shared\/collectedHeap.hpp","additions":9,"deletions":8,"binary":false,"changes":17,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2001, 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2001, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -45,3 +45,0 @@\n-  \/\/ Setup handle area\n-  set_active_handles(JNIHandleBlock::allocate_block());\n-\n","filename":"src\/hotspot\/share\/gc\/shared\/concurrentGCThread.cpp","additions":1,"deletions":4,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -27,0 +27,1 @@\n+#include \"gc\/shared\/gc_globals.hpp\"\n@@ -133,1 +134,1 @@\n-  assert(_phases->length() <= 1000, \"Too many recorded phases? (count: %d)\", _phases->length());\n+  assert(UseZGC || _phases->length() <= 1000, \"Too many recorded phases? (count: %d)\", _phases->length());\n","filename":"src\/hotspot\/share\/gc\/shared\/gcTimer.cpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -121,1 +121,1 @@\n-  virtual void ref_processor_init();\n+  void ref_processor_init();\n","filename":"src\/hotspot\/share\/gc\/shared\/generation.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -90,1 +90,0 @@\n-                                       bool      mt_discovery,\n@@ -102,1 +101,1 @@\n-  _discovery_is_mt         = mt_discovery;\n+  _discovery_is_mt         = (mt_discovery_degree > 1);\n","filename":"src\/hotspot\/share\/gc\/shared\/referenceProcessor.cpp","additions":1,"deletions":2,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -377,1 +377,1 @@\n-                     bool mt_discovery  = false, uint mt_discovery_degree  = 1,\n+                     uint mt_discovery_degree  = 1,\n","filename":"src\/hotspot\/share\/gc\/shared\/referenceProcessor.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -379,1 +379,1 @@\n-    assert(q->forwardee() == NULL, \"should be forwarded to NULL\");\n+    assert(!q->is_forwarded(), \"should not be forwarded\");\n@@ -539,1 +539,1 @@\n-    if (!cast_to_oop(cur_obj)->is_gc_marked()) {\n+    if (!cast_to_oop(cur_obj)->is_forwarded()) {\n","filename":"src\/hotspot\/share\/gc\/shared\/space.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -486,0 +486,1 @@\n+  assert(i < _n, \"index out of range.\");\n","filename":"src\/hotspot\/share\/gc\/shared\/taskqueue.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -323,0 +323,1 @@\n+  assert(queue_num < _n, \"index out of range.\");\n","filename":"src\/hotspot\/share\/gc\/shared\/taskqueue.inline.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -38,1 +38,1 @@\n-#if !(defined AARCH64 || defined AMD64 || defined IA32)\n+#if !(defined AARCH64 || defined AMD64 || defined IA32 || defined PPC64)\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahArguments.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -41,0 +41,1 @@\n+#include \"memory\/metaspaceCriticalAllocation.hpp\"\n@@ -156,2 +157,0 @@\n-  MetaWord* result;\n-\n@@ -162,16 +161,1 @@\n-  result = loader_data->metaspace_non_null()->expand_and_allocate(size, mdtype);\n-  if (result != NULL) {\n-    return result;\n-  }\n-\n-  \/\/ Start synchronous GC\n-  collect(GCCause::_metadata_GC_clear_soft_refs);\n-\n-  \/\/ Retry allocation\n-  result = loader_data->metaspace_non_null()->allocate(size, mdtype);\n-  if (result != NULL) {\n-    return result;\n-  }\n-\n-  \/\/ Expand and retry allocation\n-  result = loader_data->metaspace_non_null()->expand_and_allocate(size, mdtype);\n+  MetaWord* const result = loader_data->metaspace_non_null()->expand_and_allocate(size, mdtype);\n@@ -182,2 +166,2 @@\n-  \/\/ Out of memory\n-  return NULL;\n+  \/\/ As a last resort, try a critical allocation, riding on a synchronous full GC\n+  return MetaspaceCriticalAllocation::allocate(loader_data, size, mdtype);\n","filename":"src\/hotspot\/share\/gc\/z\/zCollectedHeap.cpp","additions":4,"deletions":20,"binary":false,"changes":24,"status":"modified"},{"patch":"@@ -336,1 +336,2 @@\n-                                                  dcmdArgInfo *infoArray);\n+                                                  dcmdArgInfo *infoArray,\n+                                                  jint count);\n","filename":"src\/hotspot\/share\/include\/jmm.h","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -296,0 +296,2 @@\n+#define REWRITE_AT_PC(val) \\\n+    *pc = val;\n@@ -392,0 +394,75 @@\n+#define MAYBE_POST_FIELD_ACCESS(obj) {                              \\\n+  if (JVMTI_ENABLED) {                                              \\\n+    int* count_addr;                                                \\\n+    \/* Check to see if a field modification watch has been set *\/   \\\n+    \/* before we take the time to call into the VM. *\/              \\\n+    count_addr = (int*)JvmtiExport::get_field_access_count_addr();  \\\n+    if (*count_addr > 0) {                                          \\\n+      oop target;                                                   \\\n+      if ((Bytecodes::Code)opcode == Bytecodes::_getstatic) {       \\\n+        target = NULL;                                              \\\n+      } else {                                                      \\\n+        target = obj;                                               \\\n+      }                                                             \\\n+      CALL_VM(InterpreterRuntime::post_field_access(THREAD,         \\\n+                                  target, cache),                   \\\n+                                  handle_exception);                \\\n+    }                                                               \\\n+  }                                                                 \\\n+}\n+\n+#define MAYBE_POST_FIELD_MODIFICATION(obj) {                        \\\n+  if (JVMTI_ENABLED) {                                              \\\n+    int* count_addr;                                                \\\n+    \/* Check to see if a field modification watch has been set *\/   \\\n+    \/* before we take the time to call into the VM.            *\/   \\\n+    count_addr = (int*)JvmtiExport::get_field_modification_count_addr(); \\\n+    if (*count_addr > 0) {                                          \\\n+      oop target;                                                   \\\n+      if ((Bytecodes::Code)opcode == Bytecodes::_putstatic) {       \\\n+        target = NULL;                                              \\\n+      } else {                                                      \\\n+        target = obj;                                               \\\n+      }                                                             \\\n+      CALL_VM(InterpreterRuntime::post_field_modification(THREAD,   \\\n+                                  target, cache,                    \\\n+                                  (jvalue*)STACK_SLOT(-1)),         \\\n+                                  handle_exception);                \\\n+    }                                                               \\\n+  }                                                                 \\\n+}\n+\n+static inline int fast_get_type(TosState tos) {\n+  switch (tos) {\n+    case ztos:\n+    case btos: return Bytecodes::_fast_bgetfield;\n+    case ctos: return Bytecodes::_fast_cgetfield;\n+    case stos: return Bytecodes::_fast_sgetfield;\n+    case itos: return Bytecodes::_fast_igetfield;\n+    case ltos: return Bytecodes::_fast_lgetfield;\n+    case ftos: return Bytecodes::_fast_fgetfield;\n+    case dtos: return Bytecodes::_fast_dgetfield;\n+    case atos: return Bytecodes::_fast_agetfield;\n+    default:\n+      ShouldNotReachHere();\n+      return -1;\n+  }\n+}\n+\n+static inline int fast_put_type(TosState tos) {\n+  switch (tos) {\n+    case ztos: return Bytecodes::_fast_zputfield;\n+    case btos: return Bytecodes::_fast_bputfield;\n+    case ctos: return Bytecodes::_fast_cputfield;\n+    case stos: return Bytecodes::_fast_sputfield;\n+    case itos: return Bytecodes::_fast_iputfield;\n+    case ltos: return Bytecodes::_fast_lputfield;\n+    case ftos: return Bytecodes::_fast_fputfield;\n+    case dtos: return Bytecodes::_fast_dputfield;\n+    case atos: return Bytecodes::_fast_aputfield;\n+    default:\n+      ShouldNotReachHere();\n+      return -1;\n+  }\n+}\n+\n@@ -400,3 +477,5 @@\n-\/\/ Instantiate two variants of the method for future linking.\n-template void BytecodeInterpreter::run<true>(interpreterState istate);\n-template void BytecodeInterpreter::run<false>(interpreterState istate);\n+\/\/ Instantiate variants of the method for future linking.\n+template void BytecodeInterpreter::run<false, false>(interpreterState istate);\n+template void BytecodeInterpreter::run<false,  true>(interpreterState istate);\n+template void BytecodeInterpreter::run< true, false>(interpreterState istate);\n+template void BytecodeInterpreter::run< true,  true>(interpreterState istate);\n@@ -404,1 +483,1 @@\n-template<bool JVMTI_ENABLED>\n+template<bool JVMTI_ENABLED, bool REWRITE_BYTECODES>\n@@ -500,2 +579,2 @@\n-\/* 0xC8 *\/ &&opc_goto_w,        &&opc_jsr_w,            &&opc_breakpoint,     &&opc_default,\n-\/* 0xCC *\/ &&opc_default,       &&opc_default,          &&opc_default,        &&opc_default,\n+\/* 0xC8 *\/ &&opc_goto_w,        &&opc_jsr_w,            &&opc_breakpoint,     &&opc_fast_agetfield,\n+\/* 0xCC *\/ &&opc_fast_bgetfield,&&opc_fast_cgetfield,   &&opc_fast_dgetfield, &&opc_fast_fgetfield,\n@@ -503,4 +582,4 @@\n-\/* 0xD0 *\/ &&opc_default,       &&opc_default,          &&opc_default,        &&opc_default,\n-\/* 0xD4 *\/ &&opc_default,       &&opc_default,          &&opc_default,        &&opc_default,\n-\/* 0xD8 *\/ &&opc_default,       &&opc_default,          &&opc_default,        &&opc_default,\n-\/* 0xDC *\/ &&opc_default,       &&opc_default,          &&opc_default,        &&opc_default,\n+\/* 0xD0 *\/ &&opc_fast_igetfield,&&opc_fast_lgetfield,   &&opc_fast_sgetfield, &&opc_fast_aputfield,\n+\/* 0xD4 *\/ &&opc_fast_bputfield,&&opc_fast_zputfield,   &&opc_fast_cputfield, &&opc_fast_dputfield,\n+\/* 0xD8 *\/ &&opc_fast_fputfield,&&opc_fast_iputfield,   &&opc_fast_lputfield, &&opc_fast_sputfield,\n+\/* 0xDC *\/ &&opc_fast_aload_0,  &&opc_fast_iaccess_0,   &&opc_fast_aaccess_0, &&opc_fast_faccess_0,\n@@ -508,1 +587,1 @@\n-\/* 0xE0 *\/ &&opc_default,       &&opc_default,          &&opc_default,        &&opc_default,\n+\/* 0xE0 *\/ &&opc_fast_iload,    &&opc_fast_iload2,      &&opc_fast_icaload,   &&opc_fast_invokevfinal,\n@@ -750,0 +829,26 @@\n+      {\n+        if (REWRITE_BYTECODES) {\n+          \/\/ Attempt to rewrite iload, iload -> fast_iload2\n+          \/\/                    iload, caload -> fast_icaload\n+          \/\/ Normal iloads will be rewritten to fast_iload to avoid checking again.\n+          switch (*(pc + 2)) {\n+            case Bytecodes::_fast_iload:\n+              REWRITE_AT_PC(Bytecodes::_fast_iload2);\n+              break;\n+            case Bytecodes::_caload:\n+              REWRITE_AT_PC(Bytecodes::_fast_icaload);\n+              break;\n+            case Bytecodes::_iload:\n+              \/\/ Wait until rewritten to _fast_iload.\n+              break;\n+            default:\n+              \/\/ Last iload in a (potential) series, don't check again.\n+              REWRITE_AT_PC(Bytecodes::_fast_iload);\n+          }\n+        }\n+        \/\/ Normal iload handling.\n+        SET_STACK_SLOT(LOCALS_SLOT(pc[1]), 0);\n+        UPDATE_PC_AND_TOS_AND_CONTINUE(2, 1);\n+      }\n+\n+      CASE(_fast_iload):\n@@ -754,0 +859,5 @@\n+      CASE(_fast_iload2):\n+          SET_STACK_SLOT(LOCALS_SLOT(pc[1]), 0);\n+          SET_STACK_SLOT(LOCALS_SLOT(pc[3]), 1);\n+          UPDATE_PC_AND_TOS_AND_CONTINUE(4, 2);\n+\n@@ -764,5 +874,0 @@\n-      CASE(_aload_##num):                                               \\\n-          VERIFY_OOP(LOCALS_OBJECT(num));                               \\\n-          SET_STACK_OBJECT(LOCALS_OBJECT(num), 0);                      \\\n-          UPDATE_PC_AND_TOS_AND_CONTINUE(1, 1);                         \\\n-                                                                        \\\n@@ -781,4 +886,47 @@\n-          OPC_LOAD_n(0);\n-          OPC_LOAD_n(1);\n-          OPC_LOAD_n(2);\n-          OPC_LOAD_n(3);\n+      OPC_LOAD_n(0);\n+      OPC_LOAD_n(1);\n+      OPC_LOAD_n(2);\n+      OPC_LOAD_n(3);\n+\n+#undef  OPC_ALOAD_n\n+#define OPC_ALOAD_n(num)                                                \\\n+      CASE(_aload_##num): {                                             \\\n+          oop obj = LOCALS_OBJECT(num);                                 \\\n+          VERIFY_OOP(obj);                                              \\\n+          SET_STACK_OBJECT(obj, 0);                                     \\\n+          UPDATE_PC_AND_TOS_AND_CONTINUE(1, 1);                         \\\n+      }\n+\n+      CASE(_aload_0):\n+      {\n+        \/* Maybe rewrite if following bytecode is one of the supported _fast_Xgetfield bytecodes. *\/\n+        if (REWRITE_BYTECODES) {\n+          switch (*(pc + 1)) {\n+            case Bytecodes::_fast_agetfield:\n+              REWRITE_AT_PC(Bytecodes::_fast_aaccess_0);\n+              break;\n+            case Bytecodes::_fast_fgetfield:\n+              REWRITE_AT_PC(Bytecodes::_fast_faccess_0);\n+              break;\n+            case Bytecodes::_fast_igetfield:\n+              REWRITE_AT_PC(Bytecodes::_fast_iaccess_0);\n+              break;\n+            case Bytecodes::_getfield: {\n+              \/* Otherwise, do nothing here, wait until it gets rewritten to _fast_Xgetfield.\n+               * Unfortunately, this punishes volatile field access, because it never gets\n+               * rewritten. *\/\n+              break;\n+            }\n+            default:\n+              REWRITE_AT_PC(Bytecodes::_fast_aload_0);\n+              break;\n+          }\n+        }\n+        VERIFY_OOP(LOCALS_OBJECT(0));\n+        SET_STACK_OBJECT(LOCALS_OBJECT(0), 0);\n+        UPDATE_PC_AND_TOS_AND_CONTINUE(1, 1);\n+      }\n+\n+      OPC_ALOAD_n(1);\n+      OPC_ALOAD_n(2);\n+      OPC_ALOAD_n(3);\n@@ -1316,5 +1464,1 @@\n-      \/* Every array access byte-code starts out like this *\/\n-\/\/        arrayOopDesc* arrObj = (arrayOopDesc*)STACK_OBJECT(arrayOff);\n-#define ARRAY_INTRO(arrayOff)                                                  \\\n-      arrayOop arrObj = (arrayOop)STACK_OBJECT(arrayOff);                      \\\n-      jint     index  = STACK_INT(arrayOff + 1);                               \\\n+#define ARRAY_INDEX_CHECK(arrObj, index)                                       \\\n@@ -1332,0 +1476,7 @@\n+      \/* Every array access byte-code starts out like this *\/\n+\/\/        arrayOopDesc* arrObj = (arrayOopDesc*)STACK_OBJECT(arrayOff);\n+#define ARRAY_INTRO(arrayOff)                                                  \\\n+      arrayOop arrObj = (arrayOop)STACK_OBJECT(arrayOff);                      \\\n+      jint     index  = STACK_INT(arrayOff + 1);                               \\\n+      ARRAY_INDEX_CHECK(arrObj, index)\n+\n@@ -1371,0 +1522,9 @@\n+      CASE(_fast_icaload): {\n+          \/\/ Custom fast access for iload,caload pair.\n+          arrayOop arrObj = (arrayOop) STACK_OBJECT(-1);\n+          jint index = LOCALS_INT(pc[1]);\n+          ARRAY_INDEX_CHECK(arrObj, index);\n+          SET_STACK_INT(*(jchar *)(((address) arrObj->base(T_CHAR)) + index * sizeof(jchar)), -1);\n+          UPDATE_PC_AND_TOS_AND_CONTINUE(3, 0);\n+      }\n+\n@@ -1540,20 +1700,0 @@\n-          if (JVMTI_ENABLED) {\n-            int *count_addr;\n-            oop obj;\n-            \/\/ Check to see if a field modification watch has been set\n-            \/\/ before we take the time to call into the VM.\n-            count_addr = (int *)JvmtiExport::get_field_access_count_addr();\n-            if ( *count_addr > 0 ) {\n-              if ((Bytecodes::Code)opcode == Bytecodes::_getstatic) {\n-                obj = NULL;\n-              } else {\n-                obj = STACK_OBJECT(-1);\n-                VERIFY_OOP(obj);\n-              }\n-              CALL_VM(InterpreterRuntime::post_field_access(THREAD,\n-                                          obj,\n-                                          cache),\n-                                          handle_exception);\n-            }\n-          }\n-\n@@ -1568,0 +1708,5 @@\n+            \/\/ Check if we can rewrite non-volatile _getfield to one of the _fast_Xgetfield.\n+            if (REWRITE_BYTECODES && !cache->is_volatile()) {\n+              \/\/ Rewrite current BC to _fast_Xgetfield.\n+              REWRITE_AT_PC(fast_get_type(cache->flag_state()));\n+            }\n@@ -1570,0 +1715,2 @@\n+          MAYBE_POST_FIELD_ACCESS(obj);\n+\n@@ -1664,27 +1811,0 @@\n-          if (JVMTI_ENABLED) {\n-            int *count_addr;\n-            oop obj;\n-            \/\/ Check to see if a field modification watch has been set\n-            \/\/ before we take the time to call into the VM.\n-            count_addr = (int *)JvmtiExport::get_field_modification_count_addr();\n-            if ( *count_addr > 0 ) {\n-              if ((Bytecodes::Code)opcode == Bytecodes::_putstatic) {\n-                obj = NULL;\n-              }\n-              else {\n-                if (cache->is_long() || cache->is_double()) {\n-                  obj = STACK_OBJECT(-3);\n-                } else {\n-                  obj = STACK_OBJECT(-2);\n-                }\n-                VERIFY_OOP(obj);\n-              }\n-\n-              CALL_VM(InterpreterRuntime::post_field_modification(THREAD,\n-                                          obj,\n-                                          cache,\n-                                          (jvalue *)STACK_SLOT(-1)),\n-                                          handle_exception);\n-            }\n-          }\n-\n@@ -1709,0 +1829,6 @@\n+\n+            \/\/ Check if we can rewrite non-volatile _putfield to one of the _fast_Xputfield.\n+            if (REWRITE_BYTECODES && !cache->is_volatile()) {\n+              \/\/ Rewrite current BC to _fast_Xputfield.\n+              REWRITE_AT_PC(fast_put_type(cache->flag_state()));\n+            }\n@@ -1711,0 +1837,2 @@\n+          MAYBE_POST_FIELD_MODIFICATION(obj);\n+\n@@ -2269,0 +2397,4 @@\n+              if (REWRITE_BYTECODES) {\n+                \/\/ Rewrite to _fast_invokevfinal.\n+                REWRITE_AT_PC(Bytecodes::_fast_invokevfinal);\n+              }\n@@ -2403,0 +2535,323 @@\n+      CASE(_fast_agetfield): {\n+        u2 index = Bytes::get_native_u2(pc+1);\n+        ConstantPoolCacheEntry* cache = cp->entry_at(index);\n+        int field_offset = cache->f2_as_index();\n+\n+        oop obj = STACK_OBJECT(-1);\n+        CHECK_NULL(obj);\n+\n+        MAYBE_POST_FIELD_ACCESS(obj);\n+\n+        VERIFY_OOP(obj->obj_field(field_offset));\n+        SET_STACK_OBJECT(obj->obj_field(field_offset), -1);\n+        UPDATE_PC_AND_CONTINUE(3);\n+      }\n+\n+      CASE(_fast_bgetfield): {\n+        u2 index = Bytes::get_native_u2(pc+1);\n+        ConstantPoolCacheEntry* cache = cp->entry_at(index);\n+        int field_offset = cache->f2_as_index();\n+\n+        oop obj = STACK_OBJECT(-1);\n+        CHECK_NULL(obj);\n+\n+        MAYBE_POST_FIELD_ACCESS(obj);\n+\n+        SET_STACK_INT(obj->byte_field(field_offset), -1);\n+        UPDATE_PC_AND_CONTINUE(3);\n+      }\n+\n+      CASE(_fast_cgetfield): {\n+        u2 index = Bytes::get_native_u2(pc+1);\n+        ConstantPoolCacheEntry* cache = cp->entry_at(index);\n+        int field_offset = cache->f2_as_index();\n+\n+        oop obj = STACK_OBJECT(-1);\n+        CHECK_NULL(obj);\n+\n+        MAYBE_POST_FIELD_ACCESS(obj);\n+\n+        SET_STACK_INT(obj->char_field(field_offset), -1);\n+        UPDATE_PC_AND_CONTINUE(3);\n+      }\n+\n+      CASE(_fast_dgetfield): {\n+        u2 index = Bytes::get_native_u2(pc+1);\n+        ConstantPoolCacheEntry* cache = cp->entry_at(index);\n+        int field_offset = cache->f2_as_index();\n+\n+        oop obj = STACK_OBJECT(-1);\n+        CHECK_NULL(obj);\n+\n+        MAYBE_POST_FIELD_ACCESS(obj);\n+\n+        SET_STACK_DOUBLE(obj->double_field(field_offset), 0);\n+        MORE_STACK(1);\n+        UPDATE_PC_AND_CONTINUE(3);\n+      }\n+\n+      CASE(_fast_fgetfield): {\n+        u2 index = Bytes::get_native_u2(pc+1);\n+        ConstantPoolCacheEntry* cache = cp->entry_at(index);\n+        int field_offset = cache->f2_as_index();\n+\n+        oop obj = STACK_OBJECT(-1);\n+        CHECK_NULL(obj);\n+\n+        MAYBE_POST_FIELD_ACCESS(obj);\n+\n+        SET_STACK_FLOAT(obj->float_field(field_offset), -1);\n+        UPDATE_PC_AND_CONTINUE(3);\n+      }\n+\n+      CASE(_fast_igetfield): {\n+        u2 index = Bytes::get_native_u2(pc+1);\n+        ConstantPoolCacheEntry* cache = cp->entry_at(index);\n+        int field_offset = cache->f2_as_index();\n+\n+        oop obj = STACK_OBJECT(-1);\n+        CHECK_NULL(obj);\n+\n+        MAYBE_POST_FIELD_ACCESS(obj);\n+\n+        SET_STACK_INT(obj->int_field(field_offset), -1);\n+        UPDATE_PC_AND_CONTINUE(3);\n+      }\n+\n+      CASE(_fast_lgetfield): {\n+        u2 index = Bytes::get_native_u2(pc+1);\n+        ConstantPoolCacheEntry* cache = cp->entry_at(index);\n+        int field_offset = cache->f2_as_index();\n+\n+        oop obj = STACK_OBJECT(-1);\n+        CHECK_NULL(obj);\n+\n+        MAYBE_POST_FIELD_ACCESS(obj);\n+\n+        SET_STACK_LONG(obj->long_field(field_offset), 0);\n+        MORE_STACK(1);\n+        UPDATE_PC_AND_CONTINUE(3);\n+      }\n+\n+      CASE(_fast_sgetfield): {\n+        u2 index = Bytes::get_native_u2(pc+1);\n+        ConstantPoolCacheEntry* cache = cp->entry_at(index);\n+        int field_offset = cache->f2_as_index();\n+\n+        oop obj = STACK_OBJECT(-1);\n+        CHECK_NULL(obj);\n+\n+        MAYBE_POST_FIELD_ACCESS(obj);\n+\n+        SET_STACK_INT(obj->short_field(field_offset), -1);\n+        UPDATE_PC_AND_CONTINUE(3);\n+      }\n+\n+      CASE(_fast_aputfield): {\n+        u2 index = Bytes::get_native_u2(pc+1);\n+        ConstantPoolCacheEntry* cache = cp->entry_at(index);\n+\n+        oop obj = STACK_OBJECT(-2);\n+        CHECK_NULL(obj);\n+\n+        MAYBE_POST_FIELD_MODIFICATION(obj);\n+\n+        int field_offset = cache->f2_as_index();\n+        obj->obj_field_put(field_offset, STACK_OBJECT(-1));\n+\n+        UPDATE_PC_AND_TOS_AND_CONTINUE(3, -2);\n+      }\n+\n+      CASE(_fast_bputfield): {\n+        u2 index = Bytes::get_native_u2(pc+1);\n+        ConstantPoolCacheEntry* cache = cp->entry_at(index);\n+\n+        oop obj = STACK_OBJECT(-2);\n+        CHECK_NULL(obj);\n+\n+        MAYBE_POST_FIELD_MODIFICATION(obj);\n+\n+        int field_offset = cache->f2_as_index();\n+        obj->byte_field_put(field_offset, STACK_INT(-1));\n+\n+        UPDATE_PC_AND_TOS_AND_CONTINUE(3, -2);\n+      }\n+\n+      CASE(_fast_zputfield): {\n+        u2 index = Bytes::get_native_u2(pc+1);\n+        ConstantPoolCacheEntry* cache = cp->entry_at(index);\n+\n+        oop obj = STACK_OBJECT(-2);\n+        CHECK_NULL(obj);\n+\n+        MAYBE_POST_FIELD_MODIFICATION(obj);\n+\n+        int field_offset = cache->f2_as_index();\n+        obj->byte_field_put(field_offset, (STACK_INT(-1) & 1)); \/\/ only store LSB\n+\n+        UPDATE_PC_AND_TOS_AND_CONTINUE(3, -2);\n+      }\n+\n+      CASE(_fast_cputfield): {\n+        u2 index = Bytes::get_native_u2(pc+1);\n+        ConstantPoolCacheEntry* cache = cp->entry_at(index);\n+\n+        oop obj = STACK_OBJECT(-2);\n+        CHECK_NULL(obj);\n+\n+        MAYBE_POST_FIELD_MODIFICATION(obj);\n+\n+        int field_offset = cache->f2_as_index();\n+        obj->char_field_put(field_offset, STACK_INT(-1));\n+\n+        UPDATE_PC_AND_TOS_AND_CONTINUE(3, -2);\n+      }\n+\n+      CASE(_fast_dputfield): {\n+        u2 index = Bytes::get_native_u2(pc+1);\n+        ConstantPoolCacheEntry* cache = cp->entry_at(index);\n+\n+        oop obj = STACK_OBJECT(-3);\n+        CHECK_NULL(obj);\n+\n+        MAYBE_POST_FIELD_MODIFICATION(obj);\n+\n+        int field_offset = cache->f2_as_index();\n+        obj->double_field_put(field_offset, STACK_DOUBLE(-1));\n+\n+        UPDATE_PC_AND_TOS_AND_CONTINUE(3, -3);\n+      }\n+\n+      CASE(_fast_fputfield): {\n+        u2 index = Bytes::get_native_u2(pc+1);\n+        ConstantPoolCacheEntry* cache = cp->entry_at(index);\n+\n+        oop obj = STACK_OBJECT(-2);\n+        CHECK_NULL(obj);\n+\n+        MAYBE_POST_FIELD_MODIFICATION(obj);\n+\n+        int field_offset = cache->f2_as_index();\n+        obj->float_field_put(field_offset, STACK_FLOAT(-1));\n+\n+        UPDATE_PC_AND_TOS_AND_CONTINUE(3, -2);\n+      }\n+\n+      CASE(_fast_iputfield): {\n+        u2 index = Bytes::get_native_u2(pc+1);\n+        ConstantPoolCacheEntry* cache = cp->entry_at(index);\n+\n+        oop obj = STACK_OBJECT(-2);\n+        CHECK_NULL(obj);\n+\n+        MAYBE_POST_FIELD_MODIFICATION(obj);\n+\n+        int field_offset = cache->f2_as_index();\n+        obj->int_field_put(field_offset, STACK_INT(-1));\n+\n+        UPDATE_PC_AND_TOS_AND_CONTINUE(3, -2);\n+      }\n+\n+      CASE(_fast_lputfield): {\n+        u2 index = Bytes::get_native_u2(pc+1);\n+        ConstantPoolCacheEntry* cache = cp->entry_at(index);\n+\n+        oop obj = STACK_OBJECT(-3);\n+        CHECK_NULL(obj);\n+\n+        MAYBE_POST_FIELD_MODIFICATION(obj);\n+\n+        int field_offset = cache->f2_as_index();\n+        obj->long_field_put(field_offset, STACK_LONG(-1));\n+\n+        UPDATE_PC_AND_TOS_AND_CONTINUE(3, -3);\n+      }\n+\n+      CASE(_fast_sputfield): {\n+        u2 index = Bytes::get_native_u2(pc+1);\n+        ConstantPoolCacheEntry* cache = cp->entry_at(index);\n+\n+        oop obj = STACK_OBJECT(-2);\n+        CHECK_NULL(obj);\n+\n+        MAYBE_POST_FIELD_MODIFICATION(obj);\n+\n+        int field_offset = cache->f2_as_index();\n+        obj->short_field_put(field_offset, STACK_INT(-1));\n+\n+        UPDATE_PC_AND_TOS_AND_CONTINUE(3, -2);\n+      }\n+\n+      CASE(_fast_aload_0): {\n+        oop obj = LOCALS_OBJECT(0);\n+        VERIFY_OOP(obj);\n+        SET_STACK_OBJECT(obj, 0);\n+        UPDATE_PC_AND_TOS_AND_CONTINUE(1, 1);\n+      }\n+\n+      CASE(_fast_aaccess_0): {\n+        u2 index = Bytes::get_native_u2(pc+2);\n+        ConstantPoolCacheEntry* cache = cp->entry_at(index);\n+        int field_offset = cache->f2_as_index();\n+\n+        oop obj = LOCALS_OBJECT(0);\n+        CHECK_NULL(obj);\n+        VERIFY_OOP(obj);\n+\n+        MAYBE_POST_FIELD_ACCESS(obj);\n+\n+        VERIFY_OOP(obj->obj_field(field_offset));\n+        SET_STACK_OBJECT(obj->obj_field(field_offset), 0);\n+        UPDATE_PC_AND_TOS_AND_CONTINUE(4, 1);\n+      }\n+\n+      CASE(_fast_faccess_0): {\n+        u2 index = Bytes::get_native_u2(pc+2);\n+        ConstantPoolCacheEntry* cache = cp->entry_at(index);\n+        int field_offset = cache->f2_as_index();\n+\n+        oop obj = LOCALS_OBJECT(0);\n+        CHECK_NULL(obj);\n+        VERIFY_OOP(obj);\n+\n+        MAYBE_POST_FIELD_ACCESS(obj);\n+\n+        SET_STACK_INT(obj->int_field(field_offset), 0);\n+        UPDATE_PC_AND_TOS_AND_CONTINUE(4, 1);\n+      }\n+\n+      CASE(_fast_iaccess_0): {\n+        u2 index = Bytes::get_native_u2(pc+2);\n+        ConstantPoolCacheEntry* cache = cp->entry_at(index);\n+        int field_offset = cache->f2_as_index();\n+\n+        oop obj = LOCALS_OBJECT(0);\n+        CHECK_NULL(obj);\n+        VERIFY_OOP(obj);\n+\n+        MAYBE_POST_FIELD_ACCESS(obj);\n+\n+        SET_STACK_FLOAT(obj->float_field(field_offset), 0);\n+        UPDATE_PC_AND_TOS_AND_CONTINUE(4, 1);\n+      }\n+\n+      CASE(_fast_invokevfinal): {\n+        u2 index = Bytes::get_native_u2(pc+1);\n+        ConstantPoolCacheEntry* cache = cp->entry_at(index);\n+\n+        assert(cache->is_resolved(Bytecodes::_invokevirtual), \"Should be resolved before rewriting\");\n+\n+        istate->set_msg(call_method);\n+\n+        CHECK_NULL(STACK_OBJECT(-(cache->parameter_size())));\n+        Method* callee = cache->f2_as_vfinal_method();\n+        istate->set_callee(callee);\n+        if (JVMTI_ENABLED && THREAD->is_interp_only_mode()) {\n+          istate->set_callee_entry_point(callee->interpreter_entry());\n+        } else {\n+          istate->set_callee_entry_point(callee->from_interpreted_entry());\n+        }\n+        istate->set_bcp_advance(3);\n+        UPDATE_PC_AND_RETURN(0);\n+      }\n+\n","filename":"src\/hotspot\/share\/interpreter\/zero\/bytecodeInterpreter.cpp","additions":527,"deletions":72,"binary":false,"changes":599,"status":"modified"},{"patch":"@@ -506,1 +506,1 @@\n-template<bool JVMTI_ENABLED>\n+template<bool JVMTI_ENABLED, bool REWRITE_BYTECODES>\n","filename":"src\/hotspot\/share\/interpreter\/zero\/bytecodeInterpreter.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -58,47 +58,0 @@\n-\/\/ JNIHandle management\n-\n-\/\/ ------------------------------------------------------------------\n-\/\/ push_jni_handle_block\n-\/\/\n-\/\/ Push on a new block of JNI handles.\n-static void push_jni_handle_block(JavaThread* const thread) {\n-  DEBUG_ONLY(JfrJavaSupport::check_java_thread_in_vm(thread));\n-\n-  \/\/ Allocate a new block for JNI handles.\n-  \/\/ Inlined code from jni_PushLocalFrame()\n-  JNIHandleBlock* prev_handles = thread->active_handles();\n-  JNIHandleBlock* entry_handles = JNIHandleBlock::allocate_block(thread);\n-  assert(entry_handles != NULL && prev_handles != NULL, \"should not be NULL\");\n-  entry_handles->set_pop_frame_link(prev_handles);  \/\/ make sure prev handles get gc'd.\n-  thread->set_active_handles(entry_handles);\n-}\n-\n-\/\/ ------------------------------------------------------------------\n-\/\/ pop_jni_handle_block\n-\/\/\n-\/\/ Pop off the current block of JNI handles.\n-static void pop_jni_handle_block(JavaThread* const thread) {\n-  DEBUG_ONLY(JfrJavaSupport::check_java_thread_in_vm(thread));\n-\n-  \/\/ Release our JNI handle block\n-  JNIHandleBlock* entry_handles = thread->active_handles();\n-  JNIHandleBlock* prev_handles = entry_handles->pop_frame_link();\n-  \/\/ restore\n-  thread->set_active_handles(prev_handles);\n-  entry_handles->set_pop_frame_link(NULL);\n-  JNIHandleBlock::release_block(entry_handles, thread); \/\/ may block\n-}\n-\n-class JNIHandleBlockManager : public StackObj {\n- private:\n-  JavaThread* const _thread;\n- public:\n-  JNIHandleBlockManager(JavaThread* thread) : _thread(thread) {\n-    push_jni_handle_block(_thread);\n-  }\n-\n-  ~JNIHandleBlockManager() {\n-    pop_jni_handle_block(_thread);\n-  }\n-};\n-\n@@ -226,1 +179,1 @@\n-  JNIHandleBlockManager jni_handle_management(THREAD);\n+  JNIHandleMark jni_handle_management(THREAD);\n@@ -497,1 +450,1 @@\n-  JNIHandleBlockManager jni_handle_management(THREAD);\n+  JNIHandleMark jni_handle_management(THREAD);\n","filename":"src\/hotspot\/share\/jfr\/dcmd\/jfrDcmds.cpp","additions":2,"deletions":49,"binary":false,"changes":51,"status":"modified"},{"patch":"@@ -74,1 +74,1 @@\n-  return t->active_handles()->allocate_handle(obj);\n+  return t->active_handles()->allocate_handle(t, obj);\n","filename":"src\/hotspot\/share\/jfr\/jni\/jfrJavaSupport.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -902,1 +902,1 @@\n-    ScopeValue *klass_sv = new ConstantOopWriteValue(JNIHandles::make_local(Thread::current(), javaMirror));\n+    ScopeValue *klass_sv = new ConstantOopWriteValue(JNIHandles::make_local(javaMirror));\n","filename":"src\/hotspot\/share\/jvmci\/jvmciCodeInstaller.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -89,23 +89,0 @@\n-void JNIHandleMark::push_jni_handle_block(JavaThread* thread) {\n-  if (thread != NULL) {\n-    \/\/ Allocate a new block for JNI handles.\n-    \/\/ Inlined code from jni_PushLocalFrame()\n-    JNIHandleBlock* java_handles = thread->active_handles();\n-    JNIHandleBlock* compile_handles = JNIHandleBlock::allocate_block(thread);\n-    assert(compile_handles != NULL && java_handles != NULL, \"should not be NULL\");\n-    compile_handles->set_pop_frame_link(java_handles);\n-    thread->set_active_handles(compile_handles);\n-  }\n-}\n-\n-void JNIHandleMark::pop_jni_handle_block(JavaThread* thread) {\n-  if (thread != NULL) {\n-    \/\/ Release our JNI handle block\n-    JNIHandleBlock* compile_handles = thread->active_handles();\n-    JNIHandleBlock* java_handles = compile_handles->pop_frame_link();\n-    thread->set_active_handles(java_handles);\n-    compile_handles->set_pop_frame_link(NULL);\n-    JNIHandleBlock::release_block(compile_handles, thread); \/\/ may block\n-  }\n-}\n-\n","filename":"src\/hotspot\/share\/jvmci\/jvmciCompilerToVM.cpp","additions":0,"deletions":23,"binary":false,"changes":23,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2011, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2011, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -176,11 +176,0 @@\n-class JNIHandleMark : public StackObj {\n-  JavaThread* _thread;\n-  public:\n-    JNIHandleMark(JavaThread* thread) : _thread(thread) { push_jni_handle_block(thread); }\n-    ~JNIHandleMark() { pop_jni_handle_block(_thread); }\n-\n-  private:\n-    static void push_jni_handle_block(JavaThread* thread);\n-    static void pop_jni_handle_block(JavaThread* thread);\n-};\n-\n","filename":"src\/hotspot\/share\/jvmci\/jvmciCompilerToVM.hpp","additions":1,"deletions":12,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -34,0 +34,1 @@\n+#include \"memory\/metaspaceCriticalAllocation.hpp\"\n@@ -752,4 +753,0 @@\n-\n-  if (DynamicDumpSharedSpaces && !UseSharedSpaces) {\n-    vm_exit_during_initialization(\"DynamicDumpSharedSpaces is unsupported when base CDS archive is not loaded\", NULL);\n-  }\n@@ -884,0 +881,3 @@\n+  \/\/ Deal with concurrent unloading failed allocation starvation\n+  MetaspaceCriticalAllocation::block_if_concurrent_purge();\n+\n@@ -999,0 +999,4 @@\n+  \/\/ The MetaspaceCritical_lock is used by a concurrent GC to block out concurrent metaspace\n+  \/\/ allocations, that would starve critical metaspace allocations, that are about to throw\n+  \/\/ OOM if they fail; they need precedence for correctness.\n+  MutexLocker ml(MetaspaceCritical_lock, Mutex::_no_safepoint_check_flag);\n@@ -1009,0 +1013,2 @@\n+\n+  MetaspaceCriticalAllocation::satisfy();\n","filename":"src\/hotspot\/share\/memory\/metaspace.cpp","additions":10,"deletions":4,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -0,0 +1,178 @@\n+\/*\n+ * Copyright (c) 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"classfile\/classLoaderData.hpp\"\n+#include \"gc\/shared\/collectedHeap.hpp\"\n+#include \"logging\/log.hpp\"\n+#include \"memory\/classLoaderMetaspace.hpp\"\n+#include \"memory\/metaspaceCriticalAllocation.hpp\"\n+#include \"memory\/universe.hpp\"\n+#include \"runtime\/interfaceSupport.inline.hpp\"\n+#include \"runtime\/mutexLocker.hpp\"\n+\n+class MetadataAllocationRequest {\n+  ClassLoaderData*           _loader_data;\n+  size_t                     _word_size;\n+  Metaspace::MetadataType    _type;\n+  MetadataAllocationRequest* _next;\n+  MetaWord*                  _result;\n+  bool                       _has_result;\n+\n+public:\n+  MetadataAllocationRequest(ClassLoaderData* loader_data,\n+                            size_t word_size,\n+                            Metaspace::MetadataType type)\n+    : _loader_data(loader_data),\n+      _word_size(word_size),\n+      _type(type),\n+      _next(NULL),\n+      _result(NULL),\n+      _has_result(false) {\n+    MetaspaceCriticalAllocation::add(this);\n+  }\n+\n+  ~MetadataAllocationRequest() {\n+    MetaspaceCriticalAllocation::remove(this);\n+  }\n+\n+  ClassLoaderData*           loader_data() const { return _loader_data; }\n+  size_t                     word_size() const   { return _word_size; }\n+  Metaspace::MetadataType    type() const        { return _type; }\n+  MetadataAllocationRequest* next() const        { return _next; }\n+  MetaWord*                  result() const      { return _result; }\n+  bool                       has_result() const  { return _has_result; }\n+\n+  void set_next(MetadataAllocationRequest* next) { _next = next; }\n+  void set_result(MetaWord* result) {\n+    _result = result;\n+    _has_result = true;\n+  }\n+};\n+\n+volatile bool MetaspaceCriticalAllocation::_has_critical_allocation = false;\n+MetadataAllocationRequest* MetaspaceCriticalAllocation::_requests_head = NULL;\n+MetadataAllocationRequest* MetaspaceCriticalAllocation::_requests_tail = NULL;\n+\n+void MetaspaceCriticalAllocation::add(MetadataAllocationRequest* request) {\n+  MutexLocker ml(MetaspaceCritical_lock, Mutex::_no_safepoint_check_flag);\n+  log_info(metaspace)(\"Requesting critical metaspace allocation; almost out of memory\");\n+  Atomic::store(&_has_critical_allocation, true);\n+  if (_requests_head == NULL) {\n+    _requests_head = _requests_tail = request;\n+  } else {\n+    _requests_tail->set_next(request);\n+    _requests_tail = request;\n+  }\n+}\n+\n+void MetaspaceCriticalAllocation::unlink(MetadataAllocationRequest* curr, MetadataAllocationRequest* prev) {\n+  if (_requests_head == curr) {\n+    _requests_head = curr->next();\n+  }\n+  if (_requests_tail == curr) {\n+    _requests_tail = prev;\n+  }\n+  if (prev != NULL) {\n+    prev->set_next(curr->next());\n+  }\n+}\n+\n+void MetaspaceCriticalAllocation::remove(MetadataAllocationRequest* request) {\n+  MutexLocker ml(MetaspaceCritical_lock, Mutex::_no_safepoint_check_flag);\n+  MetadataAllocationRequest* prev = NULL;\n+  for (MetadataAllocationRequest* curr = _requests_head; curr != NULL; curr = curr->next()) {\n+    if (curr == request) {\n+      unlink(curr, prev);\n+      break;\n+    } else {\n+      prev = curr;\n+    }\n+  }\n+}\n+\n+bool MetaspaceCriticalAllocation::try_allocate_critical(MetadataAllocationRequest* request) {\n+  MutexLocker ml(MetaspaceCritical_lock, Mutex::_no_safepoint_check_flag);\n+  if (_requests_head == request) {\n+    \/\/ The first request can't opportunistically ride on a previous GC\n+    return false;\n+  }\n+  \/\/ Try to ride on a previous GC and hope for early satisfaction\n+  wait_for_purge(request);\n+  return request->result() != NULL;\n+}\n+\n+void MetaspaceCriticalAllocation::wait_for_purge(MetadataAllocationRequest* request) {\n+  while (!request->has_result()) {\n+    ThreadBlockInVM tbivm(JavaThread::current());\n+    MetaspaceCritical_lock->wait_without_safepoint_check();\n+  }\n+}\n+\n+void MetaspaceCriticalAllocation::block_if_concurrent_purge() {\n+  if (Atomic::load(&_has_critical_allocation)) {\n+    \/\/ If there is a concurrent Metaspace::purge() operation, we will block here,\n+    \/\/ to make sure critical allocations get precedence and don't get starved.\n+    MutexLocker ml(MetaspaceCritical_lock, Mutex::_no_safepoint_check_flag);\n+  }\n+}\n+\n+void MetaspaceCriticalAllocation::satisfy() {\n+  assert_lock_strong(MetaspaceCritical_lock);\n+  bool all_satisfied = true;\n+  for (MetadataAllocationRequest* curr = _requests_head; curr != NULL; curr = curr->next()) {\n+    if (curr->result() != NULL) {\n+      \/\/ Don't satisfy twice\n+      continue;\n+    }\n+    \/\/ Try to allocate metadata.\n+    MetaWord* result = curr->loader_data()->metaspace_non_null()->allocate(curr->word_size(), curr->type());\n+    if (result == NULL) {\n+      result = curr->loader_data()->metaspace_non_null()->expand_and_allocate(curr->word_size(), curr->type());\n+    }\n+    if (result == NULL) {\n+      all_satisfied = false;\n+    }\n+    curr->set_result(result);\n+  }\n+  if (all_satisfied) {\n+    Atomic::store(&_has_critical_allocation, false);\n+  }\n+  MetaspaceCritical_lock->notify_all();\n+}\n+\n+MetaWord* MetaspaceCriticalAllocation::allocate(ClassLoaderData* loader_data, size_t word_size, Metaspace::MetadataType type) {\n+  MetadataAllocationRequest request(loader_data, word_size, type);\n+\n+  if (try_allocate_critical(&request)) {\n+    \/\/ Try to allocate on a previous concurrent GC if there was one, and return if successful\n+    return request.result();\n+  }\n+\n+  \/\/ Always perform a synchronous full GC before bailing\n+  Universe::heap()->collect(GCCause::_metadata_GC_clear_soft_refs);\n+\n+  \/\/ Return the result, be that success or failure\n+  return request.result();\n+}\n","filename":"src\/hotspot\/share\/memory\/metaspaceCriticalAllocation.cpp","additions":178,"deletions":0,"binary":false,"changes":178,"status":"added"},{"patch":"@@ -0,0 +1,84 @@\n+\/*\n+ * Copyright (c) 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef SHARE_MEMORY_METASPACECRITICALALLOCATION_HPP\n+#define SHARE_MEMORY_METASPACECRITICALALLOCATION_HPP\n+\n+#include \"memory\/allocation.hpp\"\n+#include \"memory\/metaspace.hpp\"\n+\n+class MetadataAllocationRequest;\n+class ClassLoaderData;\n+\n+\/\/ == Critical allocation support ==\n+\/\/\n+\/\/ The critical allocation support has the purpose of preventing starvation of failed\n+\/\/ metadata allocations that need a GC, in particular for concurrent GCs.\n+\/\/ A \"critical\" allocation request is registered, then a concurrent full GC is executed.\n+\/\/ When there is any critical allocation present in the system, allocations compete for\n+\/\/ a global lock, so that allocations can be shut out from the concurrent purge() call,\n+\/\/ which takes the same lock. The reasoning is that we gather all the critical allocations\n+\/\/ that are one more failure away from throwing metaspace OOM, in a queue before the GC,\n+\/\/ then free up metaspace due to class unloading in the purge() operation of that GC,\n+\/\/ and satisfy the registered critical allocations. This allows the critical allocations\n+\/\/ to get precedence over normal metaspace allocations, so that the critical allocations\n+\/\/ that are about to throw, do not get starved by other metaspace allocations that have\n+\/\/ not gone through the same dance.\n+\/\/\n+\/\/ The solution has an intended accuracy of not one allocation, but one per thread. What\n+\/\/ I mean by that, is that the allocations are allowed to throw if they got starved by\n+\/\/ one metaspace allocation per thread, even though a more complicated dance could have\n+\/\/ survived that situation in theory. The motivation is that we are at this point so close\n+\/\/ to being out of memory, and the VM is not having a good time, so the user really ought\n+\/\/ to increase the amount of available metaspace anyway, instead of GC:ing around more\n+\/\/ to satisfy a very small number of additional allocations. But it does solve pathologial\n+\/\/ unbounded starvation scenarios where OOM can get thrown even though most of metaspace\n+\/\/ is full of dead metadata.\n+\/\/\n+\/\/ The contract for this to work for a given GC is that GCCause::_metadata_GC_clear_soft_refs\n+\/\/ yields a full synchronous GC that unloads metaspace. And it is only intended to be used\n+\/\/ by GCs with concurrent class unloading.\n+\n+class MetaspaceCriticalAllocation : public AllStatic {\n+  friend class MetadataAllocationRequest;\n+\n+  static volatile bool _has_critical_allocation;\n+  static MetadataAllocationRequest* _requests_head;\n+  static MetadataAllocationRequest* _requests_tail;\n+\n+  static void unlink(MetadataAllocationRequest* curr, MetadataAllocationRequest* prev);\n+\n+  static void add(MetadataAllocationRequest* request);\n+  static void remove(MetadataAllocationRequest* request);\n+\n+  static bool try_allocate_critical(MetadataAllocationRequest* request);\n+  static void wait_for_purge(MetadataAllocationRequest* request);\n+\n+public:\n+  static void block_if_concurrent_purge();\n+  static void satisfy();\n+  static MetaWord* allocate(ClassLoaderData* loader_data, size_t word_size, Metaspace::MetadataType type);\n+};\n+\n+#endif \/\/ SHARE_MEMORY_METASPACECRITICALALLOCATION_HPP\n","filename":"src\/hotspot\/share\/memory\/metaspaceCriticalAllocation.hpp","additions":84,"deletions":0,"binary":false,"changes":84,"status":"added"},{"patch":"@@ -26,0 +26,1 @@\n+#include \"cds\/dynamicArchive.hpp\"\n@@ -768,0 +769,1 @@\n+  DynamicArchive::check_for_dynamic_dump();\n","filename":"src\/hotspot\/share\/memory\/universe.cpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -2601,0 +2601,5 @@\n+  if (MetaspaceShared::is_in_shared_metaspace(this)) {\n+    \/\/ This is a class that was dumped into the base archive, so we know\n+    \/\/ it was verified at dump time.\n+    return true;\n+  }\n","filename":"src\/hotspot\/share\/oops\/instanceKlass.cpp","additions":5,"deletions":0,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -257,2 +257,1 @@\n-    _misc_is_being_redefined                  = 1 << 14, \/\/ used for locking redefinition\n-    _misc_has_contended_annotations           = 1 << 15  \/\/ has @Contended annotation\n+    _misc_has_contended_annotations           = 1 << 14  \/\/ has @Contended annotation\n@@ -736,0 +735,2 @@\n+  \/\/ The flag is in access_flags so that it can be set and reset using atomic\n+  \/\/ operations, and not be reset by other misc_flag settings.\n@@ -737,1 +738,1 @@\n-    return ((_misc_flags & _misc_is_being_redefined) != 0);\n+    return _access_flags.is_being_redefined();\n@@ -741,1 +742,1 @@\n-      _misc_flags |= _misc_is_being_redefined;\n+      _access_flags.set_is_being_redefined();\n@@ -743,1 +744,1 @@\n-      _misc_flags &= ~_misc_is_being_redefined;\n+      _access_flags.clear_is_being_redefined();\n","filename":"src\/hotspot\/share\/oops\/instanceKlass.hpp","additions":6,"deletions":5,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -292,0 +292,1 @@\n+  assert(is_forwarded(), \"only decode when actually forwarded\");\n@@ -297,1 +298,1 @@\n-  assert(!is_forwarded(), \"Attempt to read age from forwarded mark\");\n+  assert(!mark().is_marked(), \"Attempt to read age from forwarded mark\");\n@@ -306,1 +307,1 @@\n-  assert(!is_forwarded(), \"Attempt to increment age of forwarded mark\");\n+  assert(!mark().is_marked(), \"Attempt to increment age of forwarded mark\");\n","filename":"src\/hotspot\/share\/oops\/oop.inline.hpp","additions":3,"deletions":2,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -697,0 +697,1 @@\n+  case vmIntrinsics::_VectorLoadMaskedOp:\n@@ -698,0 +699,1 @@\n+  case vmIntrinsics::_VectorStoreMaskedOp:\n","filename":"src\/hotspot\/share\/opto\/c2compiler.cpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -678,6 +678,7 @@\n-    if (is_boxing_late_inline() && callprojs.resproj != nullptr) {\n-        \/\/ replace box node to scalar node only in case it is directly referenced by debug info\n-        assert(call->as_CallStaticJava()->is_boxing_method(), \"sanity\");\n-        if (!has_non_debug_usages(callprojs.resproj) && is_box_cache_valid(call)) {\n-          scalarize_debug_usages(call, callprojs.resproj);\n-        }\n+    \/\/ Disabled due to JDK-8276112\n+    if (false && is_boxing_late_inline() && callprojs.resproj != nullptr) {\n+      \/\/ replace box node to scalar node only in case it is directly referenced by debug info\n+      assert(call->as_CallStaticJava()->is_boxing_method(), \"sanity\");\n+      if (!has_non_debug_usages(callprojs.resproj) && is_box_cache_valid(call)) {\n+        scalarize_debug_usages(call, callprojs.resproj);\n+      }\n","filename":"src\/hotspot\/share\/opto\/callGenerator.cpp","additions":7,"deletions":6,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -2401,21 +2401,32 @@\n-  if (EnableVectorReboxing && can_reshape && progress == NULL) {\n-    PhaseIterGVN* igvn = phase->is_IterGVN();\n-\n-    bool all_inputs_are_equiv_vboxes = true;\n-    for (uint i = 1; i < req(); ++i) {\n-      Node* n = in(i);\n-      if (in(i)->Opcode() != Op_VectorBox) {\n-        all_inputs_are_equiv_vboxes = false;\n-        break;\n-      }\n-      \/\/ Check that vector type of vboxes is equivalent\n-      if (i != 1) {\n-        if (Type::cmp(in(i-0)->in(VectorBoxNode::Value)->bottom_type(),\n-                      in(i-1)->in(VectorBoxNode::Value)->bottom_type()) != 0) {\n-          all_inputs_are_equiv_vboxes = false;\n-          break;\n-        }\n-        if (Type::cmp(in(i-0)->in(VectorBoxNode::Box)->bottom_type(),\n-                      in(i-1)->in(VectorBoxNode::Box)->bottom_type()) != 0) {\n-          all_inputs_are_equiv_vboxes = false;\n-          break;\n+  if (EnableVectorReboxing && can_reshape && progress == NULL && type()->isa_oopptr()) {\n+    progress = merge_through_phi(this, phase->is_IterGVN());\n+  }\n+\n+  return progress;              \/\/ Return any progress\n+}\n+\n+Node* PhiNode::clone_through_phi(Node* root_phi, const Type* t, uint c, PhaseIterGVN* igvn) {\n+  Node_Stack stack(1);\n+  VectorSet  visited;\n+  Node_List  node_map;\n+\n+  stack.push(root_phi, 1); \/\/ ignore control\n+  visited.set(root_phi->_idx);\n+\n+  Node* new_phi = new PhiNode(root_phi->in(0), t);\n+  node_map.map(root_phi->_idx, new_phi);\n+\n+  while (stack.is_nonempty()) {\n+    Node* n   = stack.node();\n+    uint  idx = stack.index();\n+    assert(n->is_Phi(), \"not a phi\");\n+    if (idx < n->req()) {\n+      stack.set_index(idx + 1);\n+      Node* def = n->in(idx);\n+      if (def == NULL) {\n+        continue; \/\/ ignore dead path\n+      } else if (def->is_Phi()) { \/\/ inner node\n+        Node* new_phi = node_map[n->_idx];\n+        if (!visited.test_set(def->_idx)) { \/\/ not visited yet\n+          node_map.map(def->_idx, new PhiNode(def->in(0), t));\n+          stack.push(def, 1); \/\/ ignore control\n@@ -2423,0 +2434,9 @@\n+        Node* new_in = node_map[def->_idx];\n+        new_phi->set_req(idx, new_in);\n+      } else if (def->Opcode() == Op_VectorBox) { \/\/ leaf\n+        assert(n->is_Phi(), \"not a phi\");\n+        Node* new_phi = node_map[n->_idx];\n+        new_phi->set_req(idx, def->in(c));\n+      } else {\n+        assert(false, \"not optimizeable\");\n+        return NULL;\n@@ -2424,0 +2444,4 @@\n+    } else {\n+      Node* new_phi = node_map[n->_idx];\n+      igvn->register_new_node_with_optimizer(new_phi, n);\n+      stack.pop();\n@@ -2425,0 +2449,3 @@\n+  }\n+  return new_phi;\n+}\n@@ -2426,8 +2453,34 @@\n-    if (all_inputs_are_equiv_vboxes) {\n-      VectorBoxNode* vbox = static_cast<VectorBoxNode*>(in(1));\n-      PhiNode* new_vbox_phi = new PhiNode(r, vbox->box_type());\n-      PhiNode* new_vect_phi = new PhiNode(r, vbox->vec_type());\n-      for (uint i = 1; i < req(); ++i) {\n-        VectorBoxNode* old_vbox = static_cast<VectorBoxNode*>(in(i));\n-        new_vbox_phi->set_req(i, old_vbox->in(VectorBoxNode::Box));\n-        new_vect_phi->set_req(i, old_vbox->in(VectorBoxNode::Value));\n+Node* PhiNode::merge_through_phi(Node* root_phi, PhaseIterGVN* igvn) {\n+  Node_Stack stack(1);\n+  VectorSet  visited;\n+\n+  stack.push(root_phi, 1); \/\/ ignore control\n+  visited.set(root_phi->_idx);\n+\n+  VectorBoxNode* cached_vbox = NULL;\n+  while (stack.is_nonempty()) {\n+    Node* n   = stack.node();\n+    uint  idx = stack.index();\n+    if (idx < n->req()) {\n+      stack.set_index(idx + 1);\n+      Node* in = n->in(idx);\n+      if (in == NULL) {\n+        continue; \/\/ ignore dead path\n+      } else if (in->isa_Phi()) {\n+        if (!visited.test_set(in->_idx)) {\n+          stack.push(in, 1); \/\/ ignore control\n+        }\n+      } else if (in->Opcode() == Op_VectorBox) {\n+        VectorBoxNode* vbox = static_cast<VectorBoxNode*>(in);\n+        if (cached_vbox == NULL) {\n+          cached_vbox = vbox;\n+        } else if (vbox->vec_type() != cached_vbox->vec_type()) {\n+          \/\/ TODO: vector type mismatch can be handled with additional reinterpret casts\n+          assert(Type::cmp(vbox->vec_type(), cached_vbox->vec_type()) != 0, \"inconsistent\");\n+          return NULL; \/\/ not optimizable: vector type mismatch\n+        } else if (vbox->box_type() != cached_vbox->box_type()) {\n+          assert(Type::cmp(vbox->box_type(), cached_vbox->box_type()) != 0, \"inconsistent\");\n+          return NULL; \/\/ not optimizable: box type mismatch\n+        }\n+      } else {\n+        return NULL; \/\/ not optimizable: neither Phi nor VectorBox\n@@ -2435,3 +2488,2 @@\n-      igvn->register_new_node_with_optimizer(new_vbox_phi, this);\n-      igvn->register_new_node_with_optimizer(new_vect_phi, this);\n-      progress = new VectorBoxNode(igvn->C, new_vbox_phi, new_vect_phi, vbox->box_type(), vbox->vec_type());\n+    } else {\n+      stack.pop();\n@@ -2440,2 +2492,6 @@\n-\n-  return progress;              \/\/ Return any progress\n+  assert(cached_vbox != NULL, \"sanity\");\n+  const TypeInstPtr* btype = cached_vbox->box_type();\n+  const TypeVect*    vtype = cached_vbox->vec_type();\n+  Node* new_vbox_phi = clone_through_phi(root_phi, btype, VectorBoxNode::Box,   igvn);\n+  Node* new_vect_phi = clone_through_phi(root_phi, vtype, VectorBoxNode::Value, igvn);\n+  return new VectorBoxNode(igvn->C, new_vbox_phi, new_vect_phi, btype, vtype);\n","filename":"src\/hotspot\/share\/opto\/cfgnode.cpp","additions":90,"deletions":34,"binary":false,"changes":124,"status":"modified"},{"patch":"@@ -146,0 +146,3 @@\n+  static Node* clone_through_phi(Node* root_phi, const Type* t, uint c, PhaseIterGVN* igvn);\n+  static Node* merge_through_phi(Node* root_phi, PhaseIterGVN* igvn);\n+\n","filename":"src\/hotspot\/share\/opto\/cfgnode.hpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -80,0 +80,1 @@\n+  if( _is_predicate ) tty->print(\"Predicate \");\n@@ -641,1 +642,2 @@\n-      } else if (lrg.num_regs() == 1) {\n+      } else if ((lrg.num_regs() == 1 && !lrg.is_scalable()) ||\n+                 (lrg.is_scalable() && lrg.scalable_reg_slots() == 1)) {\n@@ -656,9 +658,13 @@\n-          OptoReg::Name lo = OptoReg::add(hi, (1-num_regs)); \/\/ Find lo\n-          \/\/ We have to use pair [lo,lo+1] even for wide vectors because\n-          \/\/ the rest of code generation works only with pairs. It is safe\n-          \/\/ since for registers encoding only 'lo' is used.\n-          \/\/ Second reg from pair is used in ScheduleAndBundle on SPARC where\n-          \/\/ vector max size is 8 which corresponds to registers pair.\n-          \/\/ It is also used in BuildOopMaps but oop operations are not\n-          \/\/ vectorized.\n-          set2(i, lo);\n+          if (num_regs == 1) {\n+            set1(i, hi);\n+          } else {\n+            OptoReg::Name lo = OptoReg::add(hi, (1 - num_regs)); \/\/ Find lo\n+            \/\/ We have to use pair [lo,lo+1] even for wide vectors\/vmasks because\n+            \/\/ the rest of code generation works only with pairs. It is safe\n+            \/\/ since for registers encoding only 'lo' is used.\n+            \/\/ Second reg from pair is used in ScheduleAndBundle with vector max\n+            \/\/ size 8 which corresponds to registers pair.\n+            \/\/ It is also used in BuildOopMaps but oop operations are not\n+            \/\/ vectorized.\n+            set2(i, lo);\n+          }\n@@ -827,0 +833,14 @@\n+\n+        if (ireg == Op_RegVectMask) {\n+          assert(Matcher::has_predicated_vectors(), \"predicated vector should be supported\");\n+          lrg._is_predicate = 1;\n+          if (Matcher::supports_scalable_vector()) {\n+            lrg._is_scalable = 1;\n+            \/\/ For scalable predicate, when it is allocated in physical register,\n+            \/\/ num_regs is RegMask::SlotsPerRegVectMask for reg mask,\n+            \/\/ which may not be the actual physical register size.\n+            \/\/ If it is allocated in stack, we need to get the actual\n+            \/\/ physical length of scalable predicate register.\n+            lrg.set_scalable_reg_slots(Matcher::scalable_predicate_reg_slots());\n+          }\n+        }\n@@ -828,1 +848,1 @@\n-               ireg == Op_RegD || ireg == Op_RegL  || ireg == Op_RegVectMask,\n+               ireg == Op_RegD || ireg == Op_RegL || ireg == Op_RegVectMask,\n@@ -922,0 +942,2 @@\n+          assert(Matcher::has_predicated_vectors(), \"sanity\");\n+          assert(RegMask::num_registers(Op_RegVectMask) == RegMask::SlotsPerRegVectMask, \"sanity\");\n@@ -1374,0 +1396,5 @@\n+    } else if (lrg._is_predicate) {\n+      assert(num_regs == RegMask::SlotsPerRegVectMask, \"scalable predicate register\");\n+      num_regs = lrg.scalable_reg_slots();\n+      mask.clear_to_sets(num_regs);\n+      return mask.find_first_set(lrg, num_regs);\n@@ -1420,1 +1447,1 @@\n-  if (lrg._is_vector || lrg.num_regs() == 2) {\n+  if (lrg._is_vector || lrg.num_regs() == 2 || lrg.is_scalable()) {\n","filename":"src\/hotspot\/share\/opto\/chaitin.cpp","additions":39,"deletions":12,"binary":false,"changes":51,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -166,2 +166,2 @@\n-      \/\/ Should only be a vector for now, but it could also be a RegVectMask in future.\n-      assert(_is_vector && (_num_regs == RegMask::SlotsPerVecA), \"unexpected scalable reg\");\n+      assert(_is_vector && (_num_regs == RegMask::SlotsPerVecA) ||\n+             _is_predicate && (_num_regs == RegMask::SlotsPerRegVectMask), \"unexpected scalable reg\");\n@@ -198,0 +198,1 @@\n+         _is_predicate:1,       \/\/ True if in mask\/predicate registers\n","filename":"src\/hotspot\/share\/opto\/chaitin.hpp","additions":4,"deletions":3,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -418,0 +418,1 @@\n+macro(LoadVectorGatherMasked)\n@@ -420,0 +421,1 @@\n+macro(StoreVectorScatterMasked)\n@@ -428,0 +430,1 @@\n+macro(VectorMaskToLong)\n@@ -478,0 +481,4 @@\n+macro(MaskAll)\n+macro(AndVMask)\n+macro(OrVMask)\n+macro(XorVMask)\n","filename":"src\/hotspot\/share\/opto\/classes.hpp","additions":7,"deletions":0,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -2137,1 +2137,2 @@\n-    for_igvn()->clear();\n+    Unique_Node_List* old_worklist = for_igvn();\n+    old_worklist->clear();\n@@ -2147,1 +2148,1 @@\n-    set_for_igvn(save_for_igvn);\n+    set_for_igvn(old_worklist); \/\/ new_worklist is dead beyond this point\n@@ -2361,0 +2362,1 @@\n+         n->req() == 2 &&\n@@ -2368,1 +2370,1 @@\n-      return true;\n+      return n->req() == 2;\n@@ -3427,0 +3429,2 @@\n+  case Op_LoadVectorGatherMasked:\n+  case Op_StoreVectorScatterMasked:\n","filename":"src\/hotspot\/share\/opto\/compile.cpp","additions":7,"deletions":3,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -1724,0 +1724,10 @@\n+    if (in(0)->is_BaseCountedLoopEnd()) {\n+      \/\/ CountedLoopEndNode may be eliminated by if subsuming, replace CountedLoopNode with LoopNode to\n+      \/\/ avoid mismatching between CountedLoopNode and CountedLoopEndNode in the following optimization.\n+      Node* head = unique_ctrl_out();\n+      if (head != NULL && head->is_BaseCountedLoop() && head->in(LoopNode::LoopBackControl) == this) {\n+        Node* new_head = new LoopNode(head->in(LoopNode::EntryControl), this);\n+        phase->is_IterGVN()->register_new_node_with_optimizer(new_head);\n+        phase->is_IterGVN()->replace_node(head, new_head);\n+      }\n+    }\n","filename":"src\/hotspot\/share\/opto\/ifnode.cpp","additions":10,"deletions":0,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -705,1 +705,0 @@\n-        case Op_StoreVectorScatter:\n@@ -707,0 +706,2 @@\n+        case Op_StoreVectorScatter:\n+        case Op_StoreVectorScatterMasked:\n","filename":"src\/hotspot\/share\/opto\/lcm.cpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -665,0 +665,2 @@\n+  case vmIntrinsics::_VectorLoadMaskedOp:\n+    return inline_vector_mem_masked_operation(\/*is_store*\/false);\n@@ -667,0 +669,2 @@\n+  case vmIntrinsics::_VectorStoreMaskedOp:\n+    return inline_vector_mem_masked_operation(\/*is_store=*\/true);\n","filename":"src\/hotspot\/share\/opto\/library_call.cpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -322,0 +322,1 @@\n+  bool inline_vector_mem_masked_operation(bool is_store);\n@@ -335,4 +336,5 @@\n-    VecMaskUseLoad,\n-    VecMaskUseStore,\n-    VecMaskUseAll,\n-    VecMaskNotUsed\n+    VecMaskUseLoad  = 1 << 0,\n+    VecMaskUseStore = 1 << 1,\n+    VecMaskUseAll   = VecMaskUseLoad | VecMaskUseStore,\n+    VecMaskUsePred  = 1 << 2,\n+    VecMaskNotUsed  = 1 << 3\n@@ -342,1 +344,1 @@\n-  bool arch_supports_vector_rotate(int opc, int num_elem, BasicType elem_bt, bool has_scalar_args = false);\n+  bool arch_supports_vector_rotate(int opc, int num_elem, BasicType elem_bt, VectorMaskUseType mask_use_type, bool has_scalar_args = false);\n","filename":"src\/hotspot\/share\/opto\/library_call.hpp","additions":7,"deletions":5,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -805,1 +805,1 @@\n-                                       Node* range, bool upper, bool &overflow) {\n+                                       Node* range, bool upper, bool &overflow, bool negate) {\n@@ -931,1 +931,1 @@\n-  BoolNode* bol = new BoolNode(cmp, BoolTest::lt);\n+  BoolNode* bol = new BoolNode(cmp, negate ? BoolTest::ge : BoolTest::lt);\n@@ -1298,0 +1298,1 @@\n+    bool negate = (proj->_con != predicate_proj->_con);\n@@ -1300,8 +1301,2 @@\n-    BoolNode* lower_bound_bol = rc_predicate(loop, ctrl, scale, offset, init, limit, stride, rng, false, overflow);\n-    \/\/ Negate test if necessary\n-    bool negated = false;\n-    if (proj->_con != predicate_proj->_con) {\n-      lower_bound_bol = new BoolNode(lower_bound_bol->in(1), lower_bound_bol->_test.negate());\n-      register_new_node(lower_bound_bol, ctrl);\n-      negated = true;\n-    }\n+    BoolNode* lower_bound_bol = rc_predicate(loop, ctrl, scale, offset, init, limit, stride, rng, false, overflow, negate);\n+\n@@ -1312,1 +1307,1 @@\n-    if (TraceLoopPredicate) tty->print_cr(\"lower bound check if: %s %d \", negated ? \" negated\" : \"\", lower_bound_iff->_idx);\n+    if (TraceLoopPredicate) tty->print_cr(\"lower bound check if: %s %d \", negate ? \" negated\" : \"\", lower_bound_iff->_idx);\n@@ -1315,7 +1310,2 @@\n-    BoolNode* upper_bound_bol = rc_predicate(loop, lower_bound_proj, scale, offset, init, limit, stride, rng, true, overflow);\n-    negated = false;\n-    if (proj->_con != predicate_proj->_con) {\n-      upper_bound_bol = new BoolNode(upper_bound_bol->in(1), upper_bound_bol->_test.negate());\n-      register_new_node(upper_bound_bol, ctrl);\n-      negated = true;\n-    }\n+    BoolNode* upper_bound_bol = rc_predicate(loop, lower_bound_proj, scale, offset, init, limit, stride, rng, true, overflow, negate);\n+\n@@ -1327,1 +1317,1 @@\n-    if (TraceLoopPredicate) tty->print_cr(\"upper bound check if: %s %d \", negated ? \" negated\" : \"\", lower_bound_iff->_idx);\n+    if (TraceLoopPredicate) tty->print_cr(\"upper bound check if: %s %d \", negate ? \" negated\" : \"\", lower_bound_iff->_idx);\n@@ -1373,1 +1363,0 @@\n-  assert(proj->_con && predicate_proj->_con, \"not a range check?\");\n@@ -1376,1 +1365,2 @@\n-  BoolNode* bol = rc_predicate(loop, upper_bound_proj, scale, offset, opaque_init, limit, stride, rng, (stride > 0) != (scale > 0), overflow);\n+  bool negate = (proj->_con != predicate_proj->_con);\n+  BoolNode* bol = rc_predicate(loop, upper_bound_proj, scale, offset, opaque_init, limit, stride, rng, (stride > 0) != (scale > 0), overflow, negate);\n@@ -1394,1 +1384,1 @@\n-  bol = rc_predicate(loop, new_proj, scale, offset, max_value, limit, stride, rng, (stride > 0) != (scale > 0), overflow);\n+  bol = rc_predicate(loop, new_proj, scale, offset, max_value, limit, stride, rng, (stride > 0) != (scale > 0), overflow, negate);\n","filename":"src\/hotspot\/share\/opto\/loopPredicate.cpp","additions":12,"deletions":22,"binary":false,"changes":34,"status":"modified"},{"patch":"@@ -2592,1 +2592,1 @@\n-  BoolNode* bol = rc_predicate(loop, predicate_proj, scale_con, offset, value, NULL, stride_con, limit, (stride_con > 0) != (scale_con > 0), overflow);\n+  BoolNode* bol = rc_predicate(loop, predicate_proj, scale_con, offset, value, NULL, stride_con, limit, (stride_con > 0) != (scale_con > 0), overflow, false);\n","filename":"src\/hotspot\/share\/opto\/loopTransform.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -1319,1 +1319,2 @@\n-                         Node* range, bool upper, bool &overflow);\n+                         Node* range, bool upper, bool &overflow,\n+                         bool negate);\n","filename":"src\/hotspot\/share\/opto\/loopnode.hpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -189,1 +189,0 @@\n-  void copy_call_debug_info(CallNode *oldcall, CallNode * newcall);\n","filename":"src\/hotspot\/share\/opto\/macro.hpp","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -437,0 +437,18 @@\n+const int Matcher::scalable_predicate_reg_slots() {\n+  assert(Matcher::has_predicated_vectors() && Matcher::supports_scalable_vector(),\n+        \"scalable predicate vector should be supported\");\n+  int vector_reg_bit_size = Matcher::scalable_vector_reg_size(T_BYTE) << LogBitsPerByte;\n+  \/\/ We assume each predicate register is one-eighth of the size of\n+  \/\/ scalable vector register, one mask bit per vector byte.\n+  int predicate_reg_bit_size = vector_reg_bit_size >> 3;\n+  \/\/ Compute number of slots which is required when scalable predicate\n+  \/\/ register is spilled. E.g. if scalable vector register is 640 bits,\n+  \/\/ predicate register is 80 bits, which is 2.5 * slots.\n+  \/\/ We will round up the slot number to power of 2, which is required\n+  \/\/ by find_first_set().\n+  int slots = predicate_reg_bit_size & (BitsPerInt - 1)\n+              ? (predicate_reg_bit_size >> LogBitsPerInt) + 1\n+              : predicate_reg_bit_size >> LogBitsPerInt;\n+  return round_up_power_of_2(slots);\n+}\n+\n@@ -545,0 +563,2 @@\n+  } else {\n+    *idealreg2spillmask[Op_RegVectMask] = RegMask::Empty;\n@@ -617,0 +637,13 @@\n+    \/\/ Exclude last input arg stack slots to avoid spilling vector register there,\n+    \/\/ otherwise RegVectMask spills could stomp over stack slots in caller frame.\n+    for (; (in >= init_in) && (k < scalable_predicate_reg_slots()); k++) {\n+      scalable_stack_mask.Remove(in);\n+      in = OptoReg::add(in, -1);\n+    }\n+\n+    \/\/ For RegVectMask\n+    scalable_stack_mask.clear_to_sets(scalable_predicate_reg_slots());\n+    assert(scalable_stack_mask.is_AllStack(), \"should be infinite stack\");\n+    *idealreg2spillmask[Op_RegVectMask] = *idealreg2regmask[Op_RegVectMask];\n+    idealreg2spillmask[Op_RegVectMask]->OR(scalable_stack_mask);\n+\n@@ -2231,0 +2264,1 @@\n+    case Op_VectorLoadMask:\n@@ -2276,0 +2310,15 @@\n+  if (n->is_predicated_vector()) {\n+    \/\/ Restructure into binary trees for Matching.\n+    if (n->req() == 4) {\n+      n->set_req(1, new BinaryNode(n->in(1), n->in(2)));\n+      n->set_req(2, n->in(3));\n+      n->del_req(3);\n+    } else if (n->req() == 5) {\n+      n->set_req(1, new BinaryNode(n->in(1), n->in(2)));\n+      n->set_req(2, new BinaryNode(n->in(3), n->in(4)));\n+      n->del_req(4);\n+      n->del_req(3);\n+    }\n+    return;\n+  }\n+\n@@ -2415,0 +2464,1 @@\n+    case Op_LoadVectorGatherMasked:\n@@ -2421,0 +2471,9 @@\n+    case Op_StoreVectorScatterMasked: {\n+      Node* pair = new BinaryNode(n->in(MemNode::ValueIn+1), n->in(MemNode::ValueIn+2));\n+      n->set_req(MemNode::ValueIn+1, pair);\n+      n->del_req(MemNode::ValueIn+2);\n+      pair = new BinaryNode(n->in(MemNode::ValueIn), n->in(MemNode::ValueIn+1));\n+      n->set_req(MemNode::ValueIn, pair);\n+      n->del_req(MemNode::ValueIn+1);\n+      break;\n+    }\n","filename":"src\/hotspot\/share\/opto\/matcher.cpp","additions":59,"deletions":0,"binary":false,"changes":59,"status":"modified"},{"patch":"@@ -332,0 +332,2 @@\n+  static const bool match_rule_supported_vector_masked(int opcode, int vlen, BasicType bt);\n+\n@@ -348,0 +350,2 @@\n+  \/\/ Actual max scalable predicate register length.\n+  static const int scalable_predicate_reg_slots();\n","filename":"src\/hotspot\/share\/opto\/matcher.hpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -1139,1 +1139,1 @@\n-      if (store_Opcode() == Op_StoreVector) {\n+      if (st->is_StoreVector()) {\n","filename":"src\/hotspot\/share\/opto\/memnode.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -175,0 +175,1 @@\n+class VectorUnboxNode;\n@@ -176,0 +177,1 @@\n+class VectorReinterpretNode;\n@@ -710,0 +712,2 @@\n+        DEFINE_CLASS_ID(VectorUnbox, Vector, 1)\n+        DEFINE_CLASS_ID(VectorReinterpret, Vector, 2)\n@@ -781,1 +785,2 @@\n-    Flag_for_post_loop_opts_igvn     = 1 << 15,\n+    Flag_is_predicated_vector        = 1 << 15,\n+    Flag_for_post_loop_opts_igvn     = 1 << 16,\n@@ -936,0 +941,3 @@\n+  DEFINE_CLASS_QUERY(VectorMaskCmp)\n+  DEFINE_CLASS_QUERY(VectorUnbox)\n+  DEFINE_CLASS_QUERY(VectorReinterpret);\n@@ -940,1 +948,0 @@\n-  DEFINE_CLASS_QUERY(VectorMaskCmp)\n@@ -991,0 +998,2 @@\n+  bool is_predicated_vector() const { return (_flags & Flag_is_predicated_vector) != 0; }\n+\n","filename":"src\/hotspot\/share\/opto\/node.hpp","additions":11,"deletions":2,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1998, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1998, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -312,1 +312,1 @@\n-      assert(val->ideal_reg() == Op_VecA, \"scalable vector register\");\n+      assert(val->ideal_reg() == Op_VecA || val->ideal_reg() == Op_RegVectMask, \"scalable register\");\n@@ -316,1 +316,1 @@\n-        n_regs = RegMask::SlotsPerVecA;\n+        n_regs = lrgs(val_idx)._is_predicate ? RegMask::SlotsPerRegVectMask : RegMask::SlotsPerVecA;\n@@ -321,2 +321,1 @@\n-      if (lrgs(val_idx).is_scalable()) {\n-        assert(val->ideal_reg() == Op_VecA, \"scalable vector register\");\n+      if (lrgs(val_idx).is_scalable() && val->ideal_reg() == Op_VecA) {\n","filename":"src\/hotspot\/share\/opto\/postaloc.cpp","additions":4,"deletions":5,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -240,1 +240,1 @@\n-  if (lrg.is_scalable()) {\n+  if (lrg.is_scalable() && lrg._is_vector) {\n","filename":"src\/hotspot\/share\/opto\/regmask.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -1283,0 +1283,5 @@\n+  \/\/ The frame we rethrow the exception to might not have been processed by the GC yet.\n+  \/\/ The stack watermark barrier takes care of detecting that and ensuring the frame\n+  \/\/ has updated oops.\n+  StackWatermarkSet::after_unwind(current);\n+\n@@ -1425,1 +1430,1 @@\n-    RegisterMap map(current, false);\n+    RegisterMap map(current, false \/* update_map *\/, false \/* process_frames *\/);\n@@ -1464,5 +1469,0 @@\n-  \/\/ The frame we rethrow the exception to might not have been processed by the GC yet.\n-  \/\/ The stack watermark barrier takes care of detecting that and ensuring the frame\n-  \/\/ has updated oops.\n-  StackWatermarkSet::after_unwind(thread);\n-\n","filename":"src\/hotspot\/share\/opto\/runtime.cpp","additions":6,"deletions":6,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -1483,0 +1483,3 @@\n+  const int cmp1_op = cmp1->Opcode();\n+  const int cmp2_op = cmp2->Opcode();\n+\n@@ -1485,1 +1488,0 @@\n-  uint op2 = cmp2->Opcode();\n@@ -1489,1 +1491,1 @@\n-  if( con->is_Con() && !cmp2->is_Con() && op2 != Op_Opaque1 &&\n+  if( con->is_Con() && !cmp2->is_Con() && cmp2_op != Op_Opaque1 &&\n@@ -1507,1 +1509,1 @@\n-      cmp1->Opcode() == Op_AndI && cmp2->Opcode() == Op_ConI &&\n+      cmp1_op == Op_AndI && cmp2_op == Op_ConI &&\n@@ -1521,1 +1523,1 @@\n-      cmp1->Opcode() == Op_AndL && cmp2->Opcode() == Op_ConL &&\n+      cmp1_op == Op_AndL && cmp2_op == Op_ConL &&\n@@ -1532,0 +1534,32 @@\n+  \/\/ Change \"cmp (add X min_jint) (add Y min_jint)\" into \"cmpu X Y\"\n+  \/\/ and    \"cmp (add X min_jint) c\" into \"cmpu X (c + min_jint)\"\n+  if (cop == Op_CmpI &&\n+      cmp1_op == Op_AddI &&\n+      phase->type(cmp1->in(2)) == TypeInt::MIN) {\n+    if (cmp2_op == Op_ConI) {\n+      Node* ncmp2 = phase->intcon(java_add(cmp2->get_int(), min_jint));\n+      Node* ncmp = phase->transform(new CmpUNode(cmp1->in(1), ncmp2));\n+      return new BoolNode(ncmp, _test._test);\n+    } else if (cmp2_op == Op_AddI &&\n+               phase->type(cmp2->in(2)) == TypeInt::MIN) {\n+      Node* ncmp = phase->transform(new CmpUNode(cmp1->in(1), cmp2->in(1)));\n+      return new BoolNode(ncmp, _test._test);\n+    }\n+  }\n+\n+  \/\/ Change \"cmp (add X min_jlong) (add Y min_jlong)\" into \"cmpu X Y\"\n+  \/\/ and    \"cmp (add X min_jlong) c\" into \"cmpu X (c + min_jlong)\"\n+  if (cop == Op_CmpL &&\n+      cmp1_op == Op_AddL &&\n+      phase->type(cmp1->in(2)) == TypeLong::MIN) {\n+    if (cmp2_op == Op_ConL) {\n+      Node* ncmp2 = phase->longcon(java_add(cmp2->get_long(), min_jlong));\n+      Node* ncmp = phase->transform(new CmpULNode(cmp1->in(1), ncmp2));\n+      return new BoolNode(ncmp, _test._test);\n+    } else if (cmp2_op == Op_AddL &&\n+               phase->type(cmp2->in(2)) == TypeLong::MIN) {\n+      Node* ncmp = phase->transform(new CmpULNode(cmp1->in(1), cmp2->in(1)));\n+      return new BoolNode(ncmp, _test._test);\n+    }\n+  }\n+\n@@ -1535,1 +1569,0 @@\n-  int cmp1_op = cmp1->Opcode();\n","filename":"src\/hotspot\/share\/opto\/subnode.cpp","additions":38,"deletions":5,"binary":false,"changes":43,"status":"modified"},{"patch":"@@ -2561,0 +2561,8 @@\n+      } else if (opc == Op_ConvI2F || opc == Op_ConvL2D ||\n+                 opc == Op_ConvF2I || opc == Op_ConvD2L) {\n+        assert(n->req() == 2, \"only one input expected\");\n+        BasicType bt = velt_basic_type(n);\n+        int vopc = VectorNode::opcode(opc, bt);\n+        Node* in = vector_opd(p, 1);\n+        vn = VectorCastNode::make(vopc, in, bt, vlen);\n+        vlen_in_bytes = vn->as_Vector()->length_in_bytes();\n","filename":"src\/hotspot\/share\/opto\/superword.cpp","additions":8,"deletions":0,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -1000,1 +1000,0 @@\n-\n@@ -2362,1 +2361,4 @@\n-const TypeVect* TypeVect::make(const Type *elem, uint length) {\n+const TypeVect* TypeVect::make(const Type *elem, uint length, bool is_mask) {\n+  if (is_mask) {\n+    return makemask(elem, length);\n+  }\n@@ -2388,1 +2390,3 @@\n-  if (Matcher::has_predicated_vectors()) {\n+  BasicType elem_bt = elem->array_element_basic_type();\n+  if (Matcher::has_predicated_vectors() &&\n+      Matcher::match_rule_supported_vector_masked(Op_VectorLoadMask, length, elem_bt)) {\n","filename":"src\/hotspot\/share\/opto\/type.cpp","additions":7,"deletions":3,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -807,1 +807,1 @@\n-  static const TypeVect *make(const BasicType elem_bt, uint length) {\n+  static const TypeVect *make(const BasicType elem_bt, uint length, bool is_mask = false) {\n@@ -809,1 +809,1 @@\n-    return make(get_const_basic_type(elem_bt), length);\n+    return make(get_const_basic_type(elem_bt), length, is_mask);\n@@ -812,1 +812,1 @@\n-  static const TypeVect *make(const Type* elem, uint length);\n+  static const TypeVect *make(const Type* elem, uint length, bool is_mask = false);\n","filename":"src\/hotspot\/share\/opto\/type.hpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2020, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -356,1 +356,4 @@\n-  if (is_mask && bt != T_BOOLEAN) {\n+  \/\/ If boxed mask value is present in a predicate register, it must be\n+  \/\/ spilled to a vector though a VectorStoreMaskOperation before actual StoreVector\n+  \/\/ operation to vector payload field.\n+  if (is_mask && (value->bottom_type()->isa_vectmask() || bt != T_BOOLEAN)) {\n@@ -472,1 +475,1 @@\n-      vec_val_load = gvn.transform(new VectorLoadMaskNode(vec_val_load, TypeVect::make(masktype, num_elem)));\n+      vec_val_load = gvn.transform(new VectorLoadMaskNode(vec_val_load, TypeVect::makemask(masktype, num_elem)));\n","filename":"src\/hotspot\/share\/opto\/vector.cpp","additions":6,"deletions":3,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -62,40 +62,78 @@\n-bool LibraryCallKit::arch_supports_vector_rotate(int opc, int num_elem, BasicType elem_bt, bool has_scalar_args) {\n-    bool is_supported = true;\n-    \/\/ has_scalar_args flag is true only for non-constant scalar shift count,\n-    \/\/ since in this case shift needs to be broadcasted.\n-    if (!Matcher::match_rule_supported_vector(opc, num_elem, elem_bt) ||\n-         (has_scalar_args &&\n-           !arch_supports_vector(VectorNode::replicate_opcode(elem_bt), num_elem, elem_bt, VecMaskNotUsed))) {\n-      is_supported = false;\n-    }\n-\n-    int lshiftopc, rshiftopc;\n-    switch(elem_bt) {\n-      case T_BYTE:\n-        lshiftopc = Op_LShiftI;\n-        rshiftopc = Op_URShiftB;\n-        break;\n-      case T_SHORT:\n-        lshiftopc = Op_LShiftI;\n-        rshiftopc = Op_URShiftS;\n-        break;\n-      case T_INT:\n-        lshiftopc = Op_LShiftI;\n-        rshiftopc = Op_URShiftI;\n-        break;\n-      case T_LONG:\n-        lshiftopc = Op_LShiftL;\n-        rshiftopc = Op_URShiftL;\n-        break;\n-      default:\n-        assert(false, \"Unexpected type\");\n-    }\n-    int lshiftvopc = VectorNode::opcode(lshiftopc, elem_bt);\n-    int rshiftvopc = VectorNode::opcode(rshiftopc, elem_bt);\n-    if (!is_supported &&\n-        arch_supports_vector(lshiftvopc, num_elem, elem_bt, VecMaskNotUsed, has_scalar_args) &&\n-        arch_supports_vector(rshiftvopc, num_elem, elem_bt, VecMaskNotUsed, has_scalar_args) &&\n-        arch_supports_vector(Op_OrV, num_elem, elem_bt, VecMaskNotUsed)) {\n-      is_supported = true;\n-    }\n-    return is_supported;\n+static bool is_vector_mask(ciKlass* klass) {\n+  return klass->is_subclass_of(ciEnv::current()->vector_VectorMask_klass());\n+}\n+\n+static bool is_vector_shuffle(ciKlass* klass) {\n+  return klass->is_subclass_of(ciEnv::current()->vector_VectorShuffle_klass());\n+}\n+\n+bool LibraryCallKit::arch_supports_vector_rotate(int opc, int num_elem, BasicType elem_bt,\n+                                                 VectorMaskUseType mask_use_type, bool has_scalar_args) {\n+  bool is_supported = true;\n+\n+  \/\/ has_scalar_args flag is true only for non-constant scalar shift count,\n+  \/\/ since in this case shift needs to be broadcasted.\n+  if (!Matcher::match_rule_supported_vector(opc, num_elem, elem_bt) ||\n+       (has_scalar_args &&\n+         !arch_supports_vector(VectorNode::replicate_opcode(elem_bt), num_elem, elem_bt, VecMaskNotUsed))) {\n+    is_supported = false;\n+  }\n+\n+  if (is_supported) {\n+    \/\/ Check whether mask unboxing is supported.\n+    if ((mask_use_type & VecMaskUseLoad) != 0) {\n+      if (!Matcher::match_rule_supported_vector(Op_VectorLoadMask, num_elem, elem_bt)) {\n+      #ifndef PRODUCT\n+        if (C->print_intrinsics()) {\n+          tty->print_cr(\"  ** Rejected vector mask loading (%s,%s,%d) because architecture does not support it\",\n+                        NodeClassNames[Op_VectorLoadMask], type2name(elem_bt), num_elem);\n+        }\n+      #endif\n+        return false;\n+      }\n+    }\n+\n+    if ((mask_use_type & VecMaskUsePred) != 0) {\n+      if (!Matcher::has_predicated_vectors() ||\n+          !Matcher::match_rule_supported_vector_masked(opc, num_elem, elem_bt)) {\n+      #ifndef PRODUCT\n+        if (C->print_intrinsics()) {\n+          tty->print_cr(\"Rejected vector mask predicate using (%s,%s,%d) because architecture does not support it\",\n+                        NodeClassNames[opc], type2name(elem_bt), num_elem);\n+        }\n+      #endif\n+        return false;\n+      }\n+    }\n+  }\n+\n+  int lshiftopc, rshiftopc;\n+  switch(elem_bt) {\n+    case T_BYTE:\n+      lshiftopc = Op_LShiftI;\n+      rshiftopc = Op_URShiftB;\n+      break;\n+    case T_SHORT:\n+      lshiftopc = Op_LShiftI;\n+      rshiftopc = Op_URShiftS;\n+      break;\n+    case T_INT:\n+      lshiftopc = Op_LShiftI;\n+      rshiftopc = Op_URShiftI;\n+      break;\n+    case T_LONG:\n+      lshiftopc = Op_LShiftL;\n+      rshiftopc = Op_URShiftL;\n+      break;\n+    default:\n+      assert(false, \"Unexpected type\");\n+  }\n+  int lshiftvopc = VectorNode::opcode(lshiftopc, elem_bt);\n+  int rshiftvopc = VectorNode::opcode(rshiftopc, elem_bt);\n+  if (!is_supported &&\n+      arch_supports_vector(lshiftvopc, num_elem, elem_bt, VecMaskNotUsed, has_scalar_args) &&\n+      arch_supports_vector(rshiftvopc, num_elem, elem_bt, VecMaskNotUsed, has_scalar_args) &&\n+      arch_supports_vector(Op_OrV, num_elem, elem_bt, VecMaskNotUsed)) {\n+    is_supported = true;\n+  }\n+  return is_supported;\n@@ -118,1 +156,1 @@\n-  const TypeVect* vt = TypeVect::make(elem_bt, num_elem);\n+  const TypeVect* vt = TypeVect::make(elem_bt, num_elem, is_vector_mask(vbox_type->klass()));\n@@ -133,1 +171,1 @@\n-  const TypeVect* vt = TypeVect::make(elem_bt, num_elem);\n+  const TypeVect* vt = TypeVect::make(elem_bt, num_elem, is_vector_mask(vbox_type->klass()));\n@@ -158,1 +196,1 @@\n-    if(!arch_supports_vector_rotate(sopc, num_elem, type, has_scalar_args)) {\n+    if(!arch_supports_vector_rotate(sopc, num_elem, type, mask_use_type, has_scalar_args)) {\n@@ -216,1 +254,1 @@\n-  if (mask_use_type == VecMaskUseAll || mask_use_type == VecMaskUseLoad) {\n+  if ((mask_use_type & VecMaskUseLoad) != 0) {\n@@ -229,1 +267,1 @@\n-  if (mask_use_type == VecMaskUseAll || mask_use_type == VecMaskUseStore) {\n+  if ((mask_use_type & VecMaskUseStore) != 0) {\n@@ -241,6 +279,12 @@\n-  return true;\n-}\n-\n-static bool is_vector_mask(ciKlass* klass) {\n-  return klass->is_subclass_of(ciEnv::current()->vector_VectorMask_klass());\n-}\n+  if ((mask_use_type & VecMaskUsePred) != 0) {\n+    if (!Matcher::has_predicated_vectors() ||\n+        !Matcher::match_rule_supported_vector_masked(sopc, num_elem, type)) {\n+    #ifndef PRODUCT\n+      if (C->print_intrinsics()) {\n+        tty->print_cr(\"Rejected vector mask predicate using (%s,%s,%d) because architecture does not support it\",\n+                      NodeClassNames[sopc], type2name(type), num_elem);\n+      }\n+    #endif\n+      return false;\n+    }\n+  }\n@@ -248,2 +292,1 @@\n-static bool is_vector_shuffle(ciKlass* klass) {\n-  return klass->is_subclass_of(ciEnv::current()->vector_VectorShuffle_klass());\n+  return true;\n@@ -262,4 +305,6 @@\n-\/\/ <VM>\n-\/\/ VM unaryOp(int oprId, Class<? extends VM> vmClass, Class<?> elementType, int length,\n-\/\/            VM vm,\n-\/\/            Function<VM, VM> defaultImpl) {\n+\/\/ <V extends Vector<E>,\n+\/\/  M extends VectorMask<E>,\n+\/\/  E>\n+\/\/ V unaryOp(int oprId, Class<? extends V> vmClass, Class<? extends M> maskClass, Class<E> elementType,\n+\/\/           int length, V v, M m,\n+\/\/           UnaryOperation<V, M> defaultImpl)\n@@ -268,4 +313,6 @@\n-\/\/ <VM>\n-\/\/ VM binaryOp(int oprId, Class<? extends VM> vmClass, Class<?> elementType, int length,\n-\/\/             VM vm1, VM vm2,\n-\/\/             BiFunction<VM, VM, VM> defaultImpl) {\n+\/\/ <V,\n+\/\/  M extends VectorMask<E>,\n+\/\/  E>\n+\/\/ V binaryOp(int oprId, Class<? extends V> vmClass, Class<? extends M> maskClass, Class<E> elementType,\n+\/\/            int length, V v1, V v2, M m,\n+\/\/            BinaryOperation<V, M> defaultImpl)\n@@ -274,4 +321,6 @@\n-\/\/ <VM>\n-\/\/ VM ternaryOp(int oprId, Class<? extends VM> vmClass, Class<?> elementType, int length,\n-\/\/              VM vm1, VM vm2, VM vm3,\n-\/\/              TernaryOperation<VM> defaultImpl) {\n+\/\/ <V extends Vector<E>,\n+\/\/  M extends VectorMask<E>,\n+\/\/  E>\n+\/\/ V ternaryOp(int oprId, Class<? extends V> vmClass, Class<? extends M> maskClass, Class<E> elementType,\n+\/\/             int length, V v1, V v2, V v3, M m,\n+\/\/             TernaryOperation<V, M> defaultImpl)\n@@ -282,2 +331,3 @@\n-  const TypeInstPtr* elem_klass   = gvn().type(argument(2))->isa_instptr();\n-  const TypeInt*     vlen         = gvn().type(argument(3))->isa_int();\n+  const TypeInstPtr* mask_klass   = gvn().type(argument(2))->isa_instptr();\n+  const TypeInstPtr* elem_klass   = gvn().type(argument(3))->isa_instptr();\n+  const TypeInt*     vlen         = gvn().type(argument(4))->isa_int();\n@@ -291,2 +341,2 @@\n-                    NodeClassNames[argument(2)->Opcode()],\n-                    NodeClassNames[argument(3)->Opcode()]);\n+                    NodeClassNames[argument(3)->Opcode()],\n+                    NodeClassNames[argument(4)->Opcode()]);\n@@ -296,0 +346,1 @@\n+\n@@ -309,0 +360,28 @@\n+\n+  \/\/ \"argument(n + 5)\" should be the mask object. We assume it is \"null\" when no mask\n+  \/\/ is used to control this operation.\n+  const Type* vmask_type = gvn().type(argument(n + 5));\n+  bool is_masked_op = vmask_type != TypePtr::NULL_PTR;\n+  if (is_masked_op) {\n+    if (mask_klass == NULL || mask_klass->const_oop() == NULL) {\n+      if (C->print_intrinsics()) {\n+        tty->print_cr(\"  ** missing constant: maskclass=%s\", NodeClassNames[argument(2)->Opcode()]);\n+      }\n+      return false; \/\/ not enough info for intrinsification\n+    }\n+\n+    if (!is_klass_initialized(mask_klass)) {\n+      if (C->print_intrinsics()) {\n+        tty->print_cr(\"  ** mask klass argument not initialized\");\n+      }\n+      return false;\n+    }\n+\n+    if (vmask_type->maybe_null()) {\n+      if (C->print_intrinsics()) {\n+        tty->print_cr(\"  ** null mask values are not allowed for masked op\");\n+      }\n+      return false;\n+    }\n+  }\n+\n@@ -331,0 +410,4 @@\n+  if (is_vector_mask(vbox_klass)) {\n+    assert(!is_masked_op, \"mask operations do not need mask to control\");\n+  }\n+\n@@ -353,3 +436,4 @@\n-  \/\/ TODO When mask usage is supported, VecMaskNotUsed needs to be VecMaskUseLoad.\n-  if ((sopc != 0) &&\n-      !arch_supports_vector(sopc, num_elem, elem_bt, is_vector_mask(vbox_klass) ? VecMaskUseAll : VecMaskNotUsed)) {\n+  \/\/ When using mask, mask use type needs to be VecMaskUseLoad.\n+  VectorMaskUseType mask_use_type = is_vector_mask(vbox_klass) ? VecMaskUseAll\n+                                      : is_masked_op ? VecMaskUseLoad : VecMaskNotUsed;\n+  if ((sopc != 0) && !arch_supports_vector(sopc, num_elem, elem_bt, mask_use_type)) {\n@@ -357,1 +441,1 @@\n-      tty->print_cr(\"  ** not supported: arity=%d opc=%d vlen=%d etype=%s ismask=%d\",\n+      tty->print_cr(\"  ** not supported: arity=%d opc=%d vlen=%d etype=%s ismask=%d is_masked_op=%d\",\n@@ -359,1 +443,1 @@\n-                    is_vector_mask(vbox_klass) ? 1 : 0);\n+                    is_vector_mask(vbox_klass) ? 1 : 0, is_masked_op ? 1 : 0);\n@@ -364,0 +448,10 @@\n+  \/\/ Return true if current platform has implemented the masked operation with predicate feature.\n+  bool use_predicate = is_masked_op && sopc != 0 && arch_supports_vector(sopc, num_elem, elem_bt, VecMaskUsePred);\n+  if (is_masked_op && !use_predicate && !arch_supports_vector(Op_VectorBlend, num_elem, elem_bt, VecMaskUseLoad)) {\n+    if (C->print_intrinsics()) {\n+      tty->print_cr(\"  ** not supported: arity=%d opc=%d vlen=%d etype=%s ismask=0 is_masked_op=1\",\n+                    n, sopc, num_elem, type2name(elem_bt));\n+    }\n+    return false;\n+  }\n+\n@@ -367,1 +461,1 @@\n-      opd3 = unbox_vector(argument(6), vbox_type, elem_bt, num_elem);\n+      opd3 = unbox_vector(argument(7), vbox_type, elem_bt, num_elem);\n@@ -371,1 +465,1 @@\n-                        NodeClassNames[argument(6)->Opcode()]);\n+                        NodeClassNames[argument(7)->Opcode()]);\n@@ -378,1 +472,1 @@\n-      opd2 = unbox_vector(argument(5), vbox_type, elem_bt, num_elem);\n+      opd2 = unbox_vector(argument(6), vbox_type, elem_bt, num_elem);\n@@ -382,1 +476,1 @@\n-                        NodeClassNames[argument(5)->Opcode()]);\n+                        NodeClassNames[argument(6)->Opcode()]);\n@@ -389,1 +483,1 @@\n-      opd1 = unbox_vector(argument(4), vbox_type, elem_bt, num_elem);\n+      opd1 = unbox_vector(argument(5), vbox_type, elem_bt, num_elem);\n@@ -393,1 +487,1 @@\n-                        NodeClassNames[argument(4)->Opcode()]);\n+                        NodeClassNames[argument(5)->Opcode()]);\n@@ -402,0 +496,15 @@\n+  Node* mask = NULL;\n+  if (is_masked_op) {\n+    ciKlass* mbox_klass = mask_klass->const_oop()->as_instance()->java_lang_Class_klass();\n+    assert(is_vector_mask(mbox_klass), \"argument(2) should be a mask class\");\n+    const TypeInstPtr* mbox_type = TypeInstPtr::make_exact(TypePtr::NotNull, mbox_klass);\n+    mask = unbox_vector(argument(n + 5), mbox_type, elem_bt, num_elem);\n+    if (mask == NULL) {\n+      if (C->print_intrinsics()) {\n+        tty->print_cr(\"  ** unbox failed mask=%s\",\n+                      NodeClassNames[argument(n + 5)->Opcode()]);\n+      }\n+      return false;\n+    }\n+  }\n+\n@@ -416,1 +525,1 @@\n-    const TypeVect* vt = TypeVect::make(elem_bt, num_elem);\n+    const TypeVect* vt = TypeVect::make(elem_bt, num_elem, is_vector_mask(vbox_klass));\n@@ -420,1 +529,1 @@\n-        operation = gvn().transform(VectorNode::make(sopc, opd1, opd2, vt));\n+        operation = VectorNode::make(sopc, opd1, opd2, vt, is_vector_mask(vbox_klass));\n@@ -424,1 +533,1 @@\n-        operation = gvn().transform(VectorNode::make(sopc, opd1, opd2, opd3, vt));\n+        operation = VectorNode::make(sopc, opd1, opd2, opd3, vt);\n@@ -430,0 +539,12 @@\n+\n+  if (is_masked_op && mask != NULL) {\n+    if (use_predicate) {\n+      operation->add_req(mask);\n+      operation->add_flag(Node::Flag_is_predicated_vector);\n+    } else {\n+      operation = gvn().transform(operation);\n+      operation = new VectorBlendNode(opd1, operation, mask);\n+    }\n+  }\n+  operation = gvn().transform(operation);\n+\n@@ -438,1 +559,1 @@\n-\/\/  Sh ShuffleIota(Class<?> E, Class<?> ShuffleClass, Vector.Species<E> s, int length,\n+\/\/  Sh ShuffleIota(Class<?> E, Class<?> shuffleClass, Vector.Species<E> s, int length,\n@@ -512,1 +633,1 @@\n-    ConINode* pred_node = (ConINode*)gvn().makecon(TypeInt::make(1));\n+    ConINode* pred_node = (ConINode*)gvn().makecon(TypeInt::make(BoolTest::ge));\n@@ -515,1 +636,2 @@\n-    Node* mask = gvn().transform(new VectorMaskCmpNode(BoolTest::ge, bcast_lane_cnt, res, pred_node, vt));\n+    const TypeVect* vmask_type = TypeVect::makemask(elem_bt, num_elem);\n+    Node* mask = gvn().transform(new VectorMaskCmpNode(BoolTest::ge, bcast_lane_cnt, res, pred_node, vmask_type));\n@@ -534,1 +656,1 @@\n-\/\/ int maskReductionCoerced(int oper, Class<? extends M> maskClass, Class<?> elemClass,\n+\/\/ long maskReductionCoerced(int oper, Class<? extends M> maskClass, Class<?> elemClass,\n@@ -579,2 +701,16 @@\n-  Node* store_mask = gvn().transform(VectorStoreMaskNode::make(gvn(), mask_vec, elem_bt, num_elem));\n-  Node* maskoper = gvn().transform(VectorMaskOpNode::make(store_mask, TypeInt::INT, mopc));\n+  if (mask_vec == NULL) {\n+    if (C->print_intrinsics()) {\n+        tty->print_cr(\"  ** unbox failed mask=%s\",\n+                      NodeClassNames[argument(4)->Opcode()]);\n+    }\n+    return false;\n+  }\n+\n+  if (mask_vec->bottom_type()->isa_vectmask() == NULL) {\n+    mask_vec = gvn().transform(VectorStoreMaskNode::make(gvn(), mask_vec, elem_bt, num_elem));\n+  }\n+  const Type* maskoper_ty = mopc == Op_VectorMaskToLong ? (const Type*)TypeLong::LONG : (const Type*)TypeInt::INT;\n+  Node* maskoper = gvn().transform(VectorMaskOpNode::make(mask_vec, maskoper_ty, mopc));\n+  if (mopc != Op_VectorMaskToLong) {\n+    maskoper = ConvI2L(maskoper);\n+  }\n@@ -587,3 +723,7 @@\n-\/\/ <VM ,Sh extends VectorShuffle<E>, E>\n-\/\/ VM shuffleToVector(Class<VM> VecClass, Class<?>E , Class<?> ShuffleClass, Sh s, int length,\n-\/\/                    ShuffleToVectorOperation<VM,Sh,E> defaultImpl)\n+\/\/ public static\n+\/\/ <V,\n+\/\/  Sh extends VectorShuffle<E>,\n+\/\/  E>\n+\/\/ V shuffleToVector(Class<? extends Vector<E>> vclass, Class<E> elementType,\n+\/\/                   Class<? extends Sh> shuffleClass, Sh s, int length,\n+\/\/                   ShuffleToVectorOperation<V, Sh, E> defaultImpl)\n@@ -648,4 +788,7 @@\n-\/\/ <V extends Vector<?,?>>\n-\/\/ V broadcastCoerced(Class<?> vectorClass, Class<?> elementType, int vlen,\n-\/\/                    long bits,\n-\/\/                    LongFunction<V> defaultImpl)\n+\/\/ public static\n+\/\/ <M,\n+\/\/  S extends VectorSpecies<E>,\n+\/\/  E>\n+\/\/ M broadcastCoerced(Class<? extends M> vmClass, Class<E> elementType, int length,\n+\/\/                    long bits, S s,\n+\/\/                    BroadcastOperation<M, E, S> defaultImpl)\n@@ -698,1 +841,0 @@\n-\n@@ -725,1 +867,1 @@\n-  Node* broadcast = VectorNode::scalar2vector(elem, num_elem, Type::get_const_basic_type(elem_bt));\n+  Node* broadcast = VectorNode::scalar2vector(elem, num_elem, Type::get_const_basic_type(elem_bt), is_vector_mask(vbox_klass));\n@@ -750,6 +892,9 @@\n-\/\/    <C, V extends Vector<?,?>>\n-\/\/    V load(Class<?> vectorClass, Class<?> elementType, int vlen,\n-\/\/           Object base, long offset,\n-\/\/           \/* Vector.Mask<E,S> m*\/\n-\/\/           Object container, int index,\n-\/\/           LoadOperation<C, VM> defaultImpl) {\n+\/\/ public static\n+\/\/ <C,\n+\/\/  VM,\n+\/\/  E,\n+\/\/  S extends VectorSpecies<E>>\n+\/\/ VM load(Class<? extends VM> vmClass, Class<E> elementType, int length,\n+\/\/         Object base, long offset,    \/\/ Unsafe addressing\n+\/\/         C container, int index, S s,     \/\/ Arguments for default implementation\n+\/\/         LoadOperation<C, VM, E, S> defaultImpl)\n@@ -757,6 +902,8 @@\n-\/\/    <C, V extends Vector<?,?>>\n-\/\/    void store(Class<?> vectorClass, Class<?> elementType, int vlen,\n-\/\/               Object base, long offset,\n-\/\/               V v, \/*Vector.Mask<E,S> m*\/\n-\/\/               Object container, int index,\n-\/\/               StoreVectorOperation<C, V> defaultImpl) {\n+\/\/ public static\n+\/\/ <C,\n+\/\/  V extends Vector<?>>\n+\/\/ void store(Class<?> vectorClass, Class<?> elementType, int length,\n+\/\/            Object base, long offset,    \/\/ Unsafe addressing\n+\/\/            V v,\n+\/\/            C container, int index,      \/\/ Arguments for default implementation\n+\/\/            StoreVectorOperation<C, V> defaultImpl)\n@@ -817,2 +964,4 @@\n-  \/\/ Can base be NULL? Otherwise, always on-heap access.\n-  bool can_access_non_heap = TypePtr::NULL_PTR->higher_equal(gvn().type(base));\n+\n+  \/\/ The memory barrier checks are based on ones for unsafe access.\n+  \/\/ This is not 1-1 implementation.\n+  const Type *const base_type = gvn().type(base);\n@@ -823,0 +972,9 @@\n+  const bool in_native = TypePtr::NULL_PTR == base_type; \/\/ base always null\n+  const bool in_heap   = !TypePtr::NULL_PTR->higher_equal(base_type); \/\/ base never null\n+\n+  const bool is_mixed_access = !in_heap && !in_native;\n+\n+  const bool is_mismatched_access = in_heap && (addr_type->isa_aryptr() == NULL);\n+\n+  const bool needs_cpu_membar = is_mixed_access || is_mismatched_access;\n+\n@@ -880,1 +1038,1 @@\n-  if (can_access_non_heap) {\n+  if (needs_cpu_membar) {\n@@ -915,2 +1073,1 @@\n-        const TypeVect* to_vect_type = TypeVect::make(elem_bt, num_elem);\n-        vload = gvn().transform(new VectorLoadMaskNode(vload, to_vect_type));\n+        vload = gvn().transform(new VectorLoadMaskNode(vload, TypeVect::makemask(elem_bt, num_elem)));\n@@ -927,0 +1084,237 @@\n+  if (needs_cpu_membar) {\n+    insert_mem_bar(Op_MemBarCPUOrder);\n+  }\n+\n+  C->set_max_vector_size(MAX2(C->max_vector_size(), (uint)(num_elem * type2aelembytes(elem_bt))));\n+  return true;\n+}\n+\n+\/\/ public static\n+\/\/ <C,\n+\/\/  V extends Vector<?>,\n+\/\/  E,\n+\/\/  S extends VectorSpecies<E>,\n+\/\/  M extends VectorMask<E>>\n+\/\/ V loadMasked(Class<? extends V> vectorClass, Class<M> maskClass, Class<E> elementType,\n+\/\/              int length, Object base, long offset, M m,\n+\/\/              C container, int index, S s,  \/\/ Arguments for default implementation\n+\/\/              LoadVectorMaskedOperation<C, V, S, M> defaultImpl) {\n+\/\/\n+\/\/ public static\n+\/\/ <C,\n+\/\/  V extends Vector<E>,\n+\/\/  M extends VectorMask<E>,\n+\/\/  E>\n+\/\/ void storeMasked(Class<? extends V> vectorClass, Class<M> maskClass, Class<E> elementType,\n+\/\/                  int length, Object base, long offset,\n+\/\/                  V v, M m,\n+\/\/                  C container, int index,  \/\/ Arguments for default implementation\n+\/\/                  StoreVectorMaskedOperation<C, V, M, E> defaultImpl) {\n+\/\/\n+bool LibraryCallKit::inline_vector_mem_masked_operation(bool is_store) {\n+  const TypeInstPtr* vector_klass = gvn().type(argument(0))->isa_instptr();\n+  const TypeInstPtr* mask_klass   = gvn().type(argument(1))->isa_instptr();\n+  const TypeInstPtr* elem_klass   = gvn().type(argument(2))->isa_instptr();\n+  const TypeInt*     vlen         = gvn().type(argument(3))->isa_int();\n+\n+  if (vector_klass == NULL || mask_klass == NULL || elem_klass == NULL || vlen == NULL ||\n+      vector_klass->const_oop() == NULL || mask_klass->const_oop() == NULL ||\n+      elem_klass->const_oop() == NULL || !vlen->is_con()) {\n+    if (C->print_intrinsics()) {\n+      tty->print_cr(\"  ** missing constant: vclass=%s mclass=%s etype=%s vlen=%s\",\n+                    NodeClassNames[argument(0)->Opcode()],\n+                    NodeClassNames[argument(1)->Opcode()],\n+                    NodeClassNames[argument(2)->Opcode()],\n+                    NodeClassNames[argument(3)->Opcode()]);\n+    }\n+    return false; \/\/ not enough info for intrinsification\n+  }\n+  if (!is_klass_initialized(vector_klass)) {\n+    if (C->print_intrinsics()) {\n+      tty->print_cr(\"  ** klass argument not initialized\");\n+    }\n+    return false;\n+  }\n+\n+  if (!is_klass_initialized(mask_klass)) {\n+    if (C->print_intrinsics()) {\n+      tty->print_cr(\"  ** mask klass argument not initialized\");\n+    }\n+    return false;\n+  }\n+\n+  ciType* elem_type = elem_klass->const_oop()->as_instance()->java_mirror_type();\n+  if (!elem_type->is_primitive_type()) {\n+    if (C->print_intrinsics()) {\n+      tty->print_cr(\"  ** not a primitive bt=%d\", elem_type->basic_type());\n+    }\n+    return false; \/\/ should be primitive type\n+  }\n+\n+  BasicType elem_bt = elem_type->basic_type();\n+  int num_elem = vlen->get_con();\n+\n+  Node* base = argument(4);\n+  Node* offset = ConvL2X(argument(5));\n+\n+  \/\/ Save state and restore on bailout\n+  uint old_sp = sp();\n+  SafePointNode* old_map = clone_map();\n+\n+  Node* addr = make_unsafe_address(base, offset, elem_bt, true);\n+  const TypePtr *addr_type = gvn().type(addr)->isa_ptr();\n+  const TypeAryPtr* arr_type = addr_type->isa_aryptr();\n+\n+  \/\/ Now handle special case where load\/store happens from\/to byte array but element type is not byte.\n+  bool using_byte_array = arr_type != NULL && arr_type->elem()->array_element_basic_type() == T_BYTE && elem_bt != T_BYTE;\n+  \/\/ If there is no consistency between array and vector element types, it must be special byte array case\n+  if (arr_type != NULL && !using_byte_array && !elem_consistent_with_arr(elem_bt, arr_type)) {\n+    if (C->print_intrinsics()) {\n+      tty->print_cr(\"  ** not supported: arity=%d op=%s vlen=%d etype=%s atype=%s\",\n+                    is_store, is_store ? \"storeMasked\" : \"loadMasked\",\n+                    num_elem, type2name(elem_bt), type2name(arr_type->elem()->array_element_basic_type()));\n+    }\n+    set_map(old_map);\n+    set_sp(old_sp);\n+    return false;\n+  }\n+\n+  int mem_num_elem = using_byte_array ? num_elem * type2aelembytes(elem_bt) : num_elem;\n+  BasicType mem_elem_bt = using_byte_array ? T_BYTE : elem_bt;\n+  bool use_predicate = arch_supports_vector(is_store ? Op_StoreVectorMasked : Op_LoadVectorMasked,\n+                                            mem_num_elem, mem_elem_bt,\n+                                            (VectorMaskUseType) (VecMaskUseLoad | VecMaskUsePred));\n+  \/\/ Masked vector store operation needs the architecture predicate feature. We need to check\n+  \/\/ whether the predicated vector operation is supported by backend.\n+  if (is_store && !use_predicate) {\n+    if (C->print_intrinsics()) {\n+      tty->print_cr(\"  ** not supported: op=storeMasked vlen=%d etype=%s using_byte_array=%d\",\n+                    num_elem, type2name(elem_bt), using_byte_array ? 1 : 0);\n+    }\n+    set_map(old_map);\n+    set_sp(old_sp);\n+    return false;\n+  }\n+\n+  \/\/ This only happens for masked vector load. If predicate is not supported, then check whether\n+  \/\/ the normal vector load and blend operations are supported by backend.\n+  if (!use_predicate && (!arch_supports_vector(Op_LoadVector, mem_num_elem, mem_elem_bt, VecMaskNotUsed) ||\n+      !arch_supports_vector(Op_VectorBlend, mem_num_elem, mem_elem_bt, VecMaskUseLoad))) {\n+    if (C->print_intrinsics()) {\n+      tty->print_cr(\"  ** not supported: op=loadMasked vlen=%d etype=%s using_byte_array=%d\",\n+                    num_elem, type2name(elem_bt), using_byte_array ? 1 : 0);\n+    }\n+    set_map(old_map);\n+    set_sp(old_sp);\n+    return false;\n+  }\n+\n+  \/\/ Since we are using byte array, we need to double check that the vector reinterpret operation\n+  \/\/ with byte type is supported by backend.\n+  if (using_byte_array) {\n+    if (!arch_supports_vector(Op_VectorReinterpret, mem_num_elem, T_BYTE, VecMaskNotUsed)) {\n+      if (C->print_intrinsics()) {\n+        tty->print_cr(\"  ** not supported: arity=%d op=%s vlen=%d etype=%s using_byte_array=1\",\n+                      is_store, is_store ? \"storeMasked\" : \"loadMasked\",\n+                      num_elem, type2name(elem_bt));\n+      }\n+      set_map(old_map);\n+      set_sp(old_sp);\n+      return false;\n+    }\n+  }\n+\n+  \/\/ Since it needs to unbox the mask, we need to double check that the related load operations\n+  \/\/ for mask are supported by backend.\n+  if (!arch_supports_vector(Op_LoadVector, num_elem, elem_bt, VecMaskUseLoad)) {\n+    if (C->print_intrinsics()) {\n+      tty->print_cr(\"  ** not supported: arity=%d op=%s vlen=%d etype=%s\",\n+                      is_store, is_store ? \"storeMasked\" : \"loadMasked\",\n+                      num_elem, type2name(elem_bt));\n+    }\n+    set_map(old_map);\n+    set_sp(old_sp);\n+    return false;\n+  }\n+\n+  \/\/ Can base be NULL? Otherwise, always on-heap access.\n+  bool can_access_non_heap = TypePtr::NULL_PTR->higher_equal(gvn().type(base));\n+  if (can_access_non_heap) {\n+    insert_mem_bar(Op_MemBarCPUOrder);\n+  }\n+\n+  ciKlass* vbox_klass = vector_klass->const_oop()->as_instance()->java_lang_Class_klass();\n+  ciKlass* mbox_klass = mask_klass->const_oop()->as_instance()->java_lang_Class_klass();\n+  assert(!is_vector_mask(vbox_klass) && is_vector_mask(mbox_klass), \"Invalid class type\");\n+  const TypeInstPtr* vbox_type = TypeInstPtr::make_exact(TypePtr::NotNull, vbox_klass);\n+  const TypeInstPtr* mbox_type = TypeInstPtr::make_exact(TypePtr::NotNull, mbox_klass);\n+\n+  Node* mask = unbox_vector(is_store ? argument(8) : argument(7), mbox_type, elem_bt, num_elem);\n+  if (mask == NULL) {\n+    if (C->print_intrinsics()) {\n+      tty->print_cr(\"  ** unbox failed mask=%s\",\n+                    is_store ? NodeClassNames[argument(8)->Opcode()]\n+                             : NodeClassNames[argument(7)->Opcode()]);\n+    }\n+    set_map(old_map);\n+    set_sp(old_sp);\n+    return false;\n+  }\n+\n+  if (is_store) {\n+    Node* val = unbox_vector(argument(7), vbox_type, elem_bt, num_elem);\n+    if (val == NULL) {\n+      if (C->print_intrinsics()) {\n+        tty->print_cr(\"  ** unbox failed vector=%s\",\n+                      NodeClassNames[argument(7)->Opcode()]);\n+      }\n+      set_map(old_map);\n+      set_sp(old_sp);\n+      return false; \/\/ operand unboxing failed\n+    }\n+    set_all_memory(reset_memory());\n+\n+    if (using_byte_array) {\n+      \/\/ Reinterpret the incoming vector to byte vector.\n+      const TypeVect* to_vect_type = TypeVect::make(mem_elem_bt, mem_num_elem);\n+      val = gvn().transform(new VectorReinterpretNode(val, val->bottom_type()->is_vect(), to_vect_type));\n+      \/\/ Reinterpret the vector mask to byte type.\n+      const TypeVect* from_mask_type = TypeVect::makemask(elem_bt, num_elem);\n+      const TypeVect* to_mask_type = TypeVect::makemask(mem_elem_bt, mem_num_elem);\n+      mask = gvn().transform(new VectorReinterpretNode(mask, from_mask_type, to_mask_type));\n+    }\n+    Node* vstore = gvn().transform(new StoreVectorMaskedNode(control(), memory(addr), addr, val, addr_type, mask));\n+    set_memory(vstore, addr_type);\n+  } else {\n+    Node* vload = NULL;\n+\n+    if (using_byte_array) {\n+      \/\/ Reinterpret the vector mask to byte type.\n+      const TypeVect* from_mask_type = TypeVect::makemask(elem_bt, num_elem);\n+      const TypeVect* to_mask_type = TypeVect::makemask(mem_elem_bt, mem_num_elem);\n+      mask = gvn().transform(new VectorReinterpretNode(mask, from_mask_type, to_mask_type));\n+    }\n+\n+    if (use_predicate) {\n+      \/\/ Generate masked load vector node if predicate feature is supported.\n+      const TypeVect* vt = TypeVect::make(mem_elem_bt, mem_num_elem);\n+      vload = gvn().transform(new LoadVectorMaskedNode(control(), memory(addr), addr, addr_type, vt, mask));\n+    } else {\n+      \/\/ Use the vector blend to implement the masked load vector. The biased elements are zeros.\n+      Node* zero = gvn().transform(gvn().zerocon(mem_elem_bt));\n+      zero = gvn().transform(VectorNode::scalar2vector(zero, mem_num_elem, Type::get_const_basic_type(mem_elem_bt)));\n+      vload = gvn().transform(LoadVectorNode::make(0, control(), memory(addr), addr, addr_type, mem_num_elem, mem_elem_bt));\n+      vload = gvn().transform(new VectorBlendNode(zero, vload, mask));\n+    }\n+\n+    if (using_byte_array) {\n+      const TypeVect* to_vect_type = TypeVect::make(elem_bt, num_elem);\n+      vload = gvn().transform(new VectorReinterpretNode(vload, vload->bottom_type()->is_vect(), to_vect_type));\n+    }\n+\n+    Node* box = box_vector(vload, vbox_type, elem_bt, num_elem);\n+    set_result(box);\n+  }\n+\n+  old_map->destruct(&_gvn);\n+\n@@ -935,6 +1329,12 @@\n-\/\/   <C, V extends Vector<?>, W extends IntVector, E, S extends VectorSpecies<E>>\n-\/\/   void loadWithMap(Class<?> vectorClass, Class<E> E, int length, Class<?> vectorIndexClass,\n-\/\/                    Object base, long offset, \/\/ Unsafe addressing\n-\/\/                    W index_vector,\n-\/\/                    C container, int index, int[] indexMap, int indexM, S s, \/\/ Arguments for default implementation\n-\/\/                    LoadVectorOperationWithMap<C, V, E, S> defaultImpl)\n+\/\/ <C,\n+\/\/  V extends Vector<?>,\n+\/\/  W extends Vector<Integer>,\n+\/\/  S extends VectorSpecies<E>,\n+\/\/  M extends VectorMask<E>,\n+\/\/  E>\n+\/\/ V loadWithMap(Class<? extends V> vectorClass, Class<M> maskClass, Class<E> elementType, int length,\n+\/\/               Class<? extends Vector<Integer>> vectorIndexClass,\n+\/\/               Object base, long offset, \/\/ Unsafe addressing\n+\/\/               W index_vector, M m,\n+\/\/               C container, int index, int[] indexMap, int indexM, S s, \/\/ Arguments for default implementation\n+\/\/               LoadVectorOperationWithMap<C, V, E, S, M> defaultImpl)\n@@ -942,6 +1342,10 @@\n-\/\/    <C, V extends Vector<?>, W extends IntVector>\n-\/\/    void storeWithMap(Class<?> vectorClass, Class<?> elementType, int length, Class<?> vectorIndexClass,\n-\/\/                      Object base, long offset,    \/\/ Unsafe addressing\n-\/\/                      W index_vector, V v,\n-\/\/                      C container, int index, int[] indexMap, int indexM, \/\/ Arguments for default implementation\n-\/\/                      StoreVectorOperationWithMap<C, V> defaultImpl) {\n+\/\/  <C,\n+\/\/   V extends Vector<E>,\n+\/\/   W extends Vector<Integer>,\n+\/\/   M extends VectorMask<E>,\n+\/\/   E>\n+\/\/  void storeWithMap(Class<? extends V> vectorClass, Class<M> maskClass, Class<E> elementType,\n+\/\/                    int length, Class<? extends Vector<Integer>> vectorIndexClass, Object base, long offset,    \/\/ Unsafe addressing\n+\/\/                    W index_vector, V v, M m,\n+\/\/                    C container, int index, int[] indexMap, int indexM, \/\/ Arguments for default implementation\n+\/\/                    StoreVectorOperationWithMap<C, V, M, E> defaultImpl)\n@@ -951,3 +1355,4 @@\n-  const TypeInstPtr* elem_klass       = gvn().type(argument(1))->isa_instptr();\n-  const TypeInt*     vlen             = gvn().type(argument(2))->isa_int();\n-  const TypeInstPtr* vector_idx_klass = gvn().type(argument(3))->isa_instptr();\n+  const TypeInstPtr* mask_klass       = gvn().type(argument(1))->isa_instptr();\n+  const TypeInstPtr* elem_klass       = gvn().type(argument(2))->isa_instptr();\n+  const TypeInt*     vlen             = gvn().type(argument(3))->isa_int();\n+  const TypeInstPtr* vector_idx_klass = gvn().type(argument(4))->isa_instptr();\n@@ -960,1 +1365,0 @@\n-                    NodeClassNames[argument(1)->Opcode()],\n@@ -962,1 +1366,2 @@\n-                    NodeClassNames[argument(3)->Opcode()]);\n+                    NodeClassNames[argument(3)->Opcode()],\n+                    NodeClassNames[argument(4)->Opcode()]);\n@@ -973,0 +1378,1 @@\n+\n@@ -980,0 +1386,1 @@\n+\n@@ -983,5 +1390,43 @@\n-  if (!arch_supports_vector(is_scatter ? Op_StoreVectorScatter : Op_LoadVectorGather, num_elem, elem_bt, VecMaskNotUsed)) {\n-    if (C->print_intrinsics()) {\n-      tty->print_cr(\"  ** not supported: arity=%d op=%s vlen=%d etype=%s ismask=no\",\n-                    is_scatter, is_scatter ? \"scatter\" : \"gather\",\n-                    num_elem, type2name(elem_bt));\n+  const Type* vmask_type = gvn().type(is_scatter ? argument(10) : argument(9));\n+  bool is_masked_op = vmask_type != TypePtr::NULL_PTR;\n+  if (is_masked_op) {\n+    if (mask_klass == NULL || mask_klass->const_oop() == NULL) {\n+      if (C->print_intrinsics()) {\n+        tty->print_cr(\"  ** missing constant: maskclass=%s\", NodeClassNames[argument(1)->Opcode()]);\n+      }\n+      return false; \/\/ not enough info for intrinsification\n+    }\n+\n+    if (!is_klass_initialized(mask_klass)) {\n+      if (C->print_intrinsics()) {\n+        tty->print_cr(\"  ** mask klass argument not initialized\");\n+      }\n+      return false;\n+    }\n+\n+    if (vmask_type->maybe_null()) {\n+      if (C->print_intrinsics()) {\n+        tty->print_cr(\"  ** null mask values are not allowed for masked op\");\n+      }\n+      return false;\n+    }\n+\n+    \/\/ Check whether the predicated gather\/scatter node is supported by architecture.\n+    if (!arch_supports_vector(is_scatter ? Op_StoreVectorScatterMasked : Op_LoadVectorGatherMasked, num_elem, elem_bt,\n+                              (VectorMaskUseType) (VecMaskUseLoad | VecMaskUsePred))) {\n+      if (C->print_intrinsics()) {\n+        tty->print_cr(\"  ** not supported: arity=%d op=%s vlen=%d etype=%s is_masked_op=1\",\n+                      is_scatter, is_scatter ? \"scatterMasked\" : \"gatherMasked\",\n+                      num_elem, type2name(elem_bt));\n+      }\n+      return false; \/\/ not supported\n+    }\n+  } else {\n+    \/\/ Check whether the normal gather\/scatter node is supported for non-masked operation.\n+    if (!arch_supports_vector(is_scatter ? Op_StoreVectorScatter : Op_LoadVectorGather, num_elem, elem_bt, VecMaskNotUsed)) {\n+      if (C->print_intrinsics()) {\n+        tty->print_cr(\"  ** not supported: arity=%d op=%s vlen=%d etype=%s is_masked_op=0\",\n+                      is_scatter, is_scatter ? \"scatter\" : \"gather\",\n+                      num_elem, type2name(elem_bt));\n+      }\n+      return false; \/\/ not supported\n@@ -989,1 +1434,0 @@\n-    return false; \/\/ not supported\n@@ -995,1 +1439,1 @@\n-        tty->print_cr(\"  ** not supported: arity=%d op=%s\/loadindex vlen=%d etype=int ismask=no\",\n+        tty->print_cr(\"  ** not supported: arity=%d op=%s\/loadindex vlen=%d etype=int is_masked_op=%d\",\n@@ -997,1 +1441,1 @@\n-                      num_elem);\n+                      num_elem, is_masked_op ? 1 : 0);\n@@ -1000,1 +1444,1 @@\n-    }\n+  }\n@@ -1002,2 +1446,2 @@\n-  Node* base = argument(4);\n-  Node* offset = ConvL2X(argument(5));\n+  Node* base = argument(5);\n+  Node* offset = ConvL2X(argument(6));\n@@ -1025,0 +1469,1 @@\n+\n@@ -1027,1 +1472,0 @@\n-\n@@ -1029,1 +1473,0 @@\n-\n@@ -1037,2 +1480,1 @@\n-\n-  Node* index_vect = unbox_vector(argument(7), vbox_idx_type, T_INT, num_elem);\n+  Node* index_vect = unbox_vector(argument(8), vbox_idx_type, T_INT, num_elem);\n@@ -1044,0 +1486,18 @@\n+\n+  Node* mask = NULL;\n+  if (is_masked_op) {\n+    ciKlass* mbox_klass = mask_klass->const_oop()->as_instance()->java_lang_Class_klass();\n+    const TypeInstPtr* mbox_type = TypeInstPtr::make_exact(TypePtr::NotNull, mbox_klass);\n+    mask = unbox_vector(is_scatter ? argument(10) : argument(9), mbox_type, elem_bt, num_elem);\n+    if (mask == NULL) {\n+      if (C->print_intrinsics()) {\n+        tty->print_cr(\"  ** unbox failed mask=%s\",\n+                    is_scatter ? NodeClassNames[argument(10)->Opcode()]\n+                               : NodeClassNames[argument(9)->Opcode()]);\n+      }\n+      set_map(old_map);\n+      set_sp(old_sp);\n+      return false;\n+    }\n+  }\n+\n@@ -1046,1 +1506,1 @@\n-    Node* val = unbox_vector(argument(8), vbox_type, elem_bt, num_elem);\n+    Node* val = unbox_vector(argument(9), vbox_type, elem_bt, num_elem);\n@@ -1054,1 +1514,6 @@\n-    Node* vstore = gvn().transform(new StoreVectorScatterNode(control(), memory(addr), addr, addr_type, val, index_vect));\n+    Node* vstore = NULL;\n+    if (mask != NULL) {\n+      vstore = gvn().transform(new StoreVectorScatterMaskedNode(control(), memory(addr), addr, addr_type, val, index_vect, mask));\n+    } else {\n+      vstore = gvn().transform(new StoreVectorScatterNode(control(), memory(addr), addr, addr_type, val, index_vect));\n+    }\n@@ -1057,2 +1522,6 @@\n-    Node* vload = gvn().transform(new LoadVectorGatherNode(control(), memory(addr), addr, addr_type, vector_type, index_vect));\n-\n+    Node* vload = NULL;\n+    if (mask != NULL) {\n+      vload = gvn().transform(new LoadVectorGatherMaskedNode(control(), memory(addr), addr, addr_type, vector_type, index_vect, mask));\n+    } else {\n+      vload = gvn().transform(new LoadVectorGatherNode(control(), memory(addr), addr, addr_type, vector_type, index_vect));\n+    }\n@@ -1069,5 +1538,7 @@\n-\/\/ <V extends Vector<?,?>>\n-\/\/ long reductionCoerced(int oprId, Class<?> vectorClass, Class<?> elementType, int vlen,\n-\/\/                       V v,\n-\/\/                       Function<V,Long> defaultImpl)\n-\n+\/\/ public static\n+\/\/ <V extends Vector<E>,\n+\/\/  M extends VectorMask<E>,\n+\/\/  E>\n+\/\/ long reductionCoerced(int oprId, Class<? extends V> vectorClass, Class<? extends M> maskClass,\n+\/\/                       Class<E> elementType, int length, V v, M m,\n+\/\/                       ReductionOperation<V, M> defaultImpl)\n@@ -1077,2 +1548,3 @@\n-  const TypeInstPtr* elem_klass   = gvn().type(argument(2))->isa_instptr();\n-  const TypeInt*     vlen         = gvn().type(argument(3))->isa_int();\n+  const TypeInstPtr* mask_klass   = gvn().type(argument(2))->isa_instptr();\n+  const TypeInstPtr* elem_klass   = gvn().type(argument(3))->isa_instptr();\n+  const TypeInt*     vlen         = gvn().type(argument(4))->isa_int();\n@@ -1086,2 +1558,2 @@\n-                    NodeClassNames[argument(2)->Opcode()],\n-                    NodeClassNames[argument(3)->Opcode()]);\n+                    NodeClassNames[argument(3)->Opcode()],\n+                    NodeClassNames[argument(4)->Opcode()]);\n@@ -1104,0 +1576,26 @@\n+\n+  const Type* vmask_type = gvn().type(argument(6));\n+  bool is_masked_op = vmask_type != TypePtr::NULL_PTR;\n+  if (is_masked_op) {\n+    if (mask_klass == NULL || mask_klass->const_oop() == NULL) {\n+      if (C->print_intrinsics()) {\n+        tty->print_cr(\"  ** missing constant: maskclass=%s\", NodeClassNames[argument(2)->Opcode()]);\n+      }\n+      return false; \/\/ not enough info for intrinsification\n+    }\n+\n+    if (!is_klass_initialized(mask_klass)) {\n+      if (C->print_intrinsics()) {\n+        tty->print_cr(\"  ** mask klass argument not initialized\");\n+      }\n+      return false;\n+    }\n+\n+    if (vmask_type->maybe_null()) {\n+      if (C->print_intrinsics()) {\n+        tty->print_cr(\"  ** null mask values are not allowed for masked op\");\n+      }\n+      return false;\n+    }\n+  }\n+\n@@ -1106,1 +1604,0 @@\n-\n@@ -1110,2 +1607,2 @@\n-  \/\/ TODO When mask usage is supported, VecMaskNotUsed needs to be VecMaskUseLoad.\n-  if (!arch_supports_vector(sopc, num_elem, elem_bt, VecMaskNotUsed)) {\n+  \/\/ When using mask, mask use type needs to be VecMaskUseLoad.\n+  if (!arch_supports_vector(sopc, num_elem, elem_bt, is_masked_op ? VecMaskUseLoad : VecMaskNotUsed)) {\n@@ -1113,1 +1610,11 @@\n-      tty->print_cr(\"  ** not supported: arity=1 op=%d\/reduce vlen=%d etype=%s ismask=no\",\n+      tty->print_cr(\"  ** not supported: arity=1 op=%d\/reduce vlen=%d etype=%s is_masked_op=%d\",\n+                    sopc, num_elem, type2name(elem_bt), is_masked_op ? 1 : 0);\n+    }\n+    return false;\n+  }\n+\n+  \/\/ Return true if current platform has implemented the masked operation with predicate feature.\n+  bool use_predicate = is_masked_op && arch_supports_vector(sopc, num_elem, elem_bt, VecMaskUsePred);\n+  if (is_masked_op && !use_predicate && !arch_supports_vector(Op_VectorBlend, num_elem, elem_bt, VecMaskUseLoad)) {\n+    if (C->print_intrinsics()) {\n+      tty->print_cr(\"  ** not supported: arity=1 op=%d\/reduce vlen=%d etype=%s is_masked_op=1\",\n@@ -1122,1 +1629,1 @@\n-  Node* opd = unbox_vector(argument(4), vbox_type, elem_bt, num_elem);\n+  Node* opd = unbox_vector(argument(5), vbox_type, elem_bt, num_elem);\n@@ -1127,0 +1634,15 @@\n+  Node* mask = NULL;\n+  if (is_masked_op) {\n+    ciKlass* mbox_klass = mask_klass->const_oop()->as_instance()->java_lang_Class_klass();\n+    assert(is_vector_mask(mbox_klass), \"argument(2) should be a mask class\");\n+    const TypeInstPtr* mbox_type = TypeInstPtr::make_exact(TypePtr::NotNull, mbox_klass);\n+    mask = unbox_vector(argument(6), mbox_type, elem_bt, num_elem);\n+    if (mask == NULL) {\n+      if (C->print_intrinsics()) {\n+        tty->print_cr(\"  ** unbox failed mask=%s\",\n+                      NodeClassNames[argument(6)->Opcode()]);\n+      }\n+      return false;\n+    }\n+  }\n+\n@@ -1128,1 +1650,16 @@\n-  Node* rn = gvn().transform(ReductionNode::make(opc, NULL, init, opd, elem_bt));\n+  Node* value = NULL;\n+  if (mask == NULL) {\n+    assert(!is_masked_op, \"Masked op needs the mask value never null\");\n+    value = ReductionNode::make(opc, NULL, init, opd, elem_bt);\n+  } else {\n+    if (use_predicate) {\n+      value = ReductionNode::make(opc, NULL, init, opd, elem_bt);\n+      value->add_req(mask);\n+      value->add_flag(Node::Flag_is_predicated_vector);\n+    } else {\n+      Node* reduce_identity = gvn().transform(VectorNode::scalar2vector(init, num_elem, Type::get_const_basic_type(elem_bt)));\n+      value = gvn().transform(new VectorBlendNode(reduce_identity, opd, mask));\n+      value = ReductionNode::make(opc, NULL, init, value, elem_bt);\n+    }\n+  }\n+  value = gvn().transform(value);\n@@ -1135,1 +1672,1 @@\n-      bits = gvn().transform(new ConvI2LNode(rn));\n+      bits = gvn().transform(new ConvI2LNode(value));\n@@ -1139,2 +1676,2 @@\n-      rn   = gvn().transform(new MoveF2INode(rn));\n-      bits = gvn().transform(new ConvI2LNode(rn));\n+      value = gvn().transform(new MoveF2INode(value));\n+      bits  = gvn().transform(new ConvI2LNode(value));\n@@ -1144,1 +1681,1 @@\n-      bits = gvn().transform(new MoveD2LNode(rn));\n+      bits = gvn().transform(new MoveD2LNode(value));\n@@ -1148,1 +1685,1 @@\n-      bits = rn; \/\/ no conversion needed\n+      bits = value; \/\/ no conversion needed\n@@ -1160,1 +1697,1 @@\n-\/\/                                BiFunction<V, V, Boolean> defaultImpl) {\n+\/\/                                BiFunction<V, V, Boolean> defaultImpl)\n@@ -1221,2 +1758,4 @@\n-\/\/ <V extends Vector, M extends Mask>\n-\/\/ V blend(Class<V> vectorClass, Class<M> maskClass, Class<?> elementType, int vlen,\n+\/\/ <V extends Vector<E>,\n+\/\/  M extends VectorMask<E>,\n+\/\/  E>\n+\/\/ V blend(Class<? extends V> vectorClass, Class<M> maskClass, Class<E> elementType, int vlen,\n@@ -1224,2 +1763,1 @@\n-\/\/         VectorBlendOp<V,M> defaultImpl) { ...\n-\/\/\n+\/\/         VectorBlendOp<V, M, E> defaultImpl)\n@@ -1292,7 +1830,7 @@\n-\/\/  public static <V extends Vector<E,S>,\n-\/\/          M extends Vector.Mask<E,S>,\n-\/\/          S extends Vector.Shape, E>\n-\/\/  M compare(int cond, Class<V> vectorClass, Class<M> maskClass, Class<?> elementType, int vlen,\n-\/\/            V v1, V v2,\n-\/\/            VectorCompareOp<V,M> defaultImpl) { ...\n-\/\/\n+\/\/  public static\n+\/\/  <V extends Vector<E>,\n+\/\/   M extends VectorMask<E>,\n+\/\/   E>\n+\/\/  M compare(int cond, Class<? extends V> vectorClass, Class<M> maskClass, Class<E> elementType, int vlen,\n+\/\/            V v1, V v2, M m,\n+\/\/            VectorCompareOp<V,M> defaultImpl)\n@@ -1366,0 +1904,19 @@\n+  bool is_masked_op = argument(7)->bottom_type() != TypePtr::NULL_PTR;\n+  Node* mask = is_masked_op ? unbox_vector(argument(7), mbox_type, elem_bt, num_elem) : NULL;\n+  if (is_masked_op && mask == NULL) {\n+    if (C->print_intrinsics()) {\n+      tty->print_cr(\"  ** not supported: mask = null arity=2 op=comp\/%d vlen=%d etype=%s ismask=usestore is_masked_op=1\",\n+                    cond->get_con(), num_elem, type2name(elem_bt));\n+    }\n+    return false;\n+  }\n+\n+  bool use_predicate = is_masked_op && arch_supports_vector(Op_VectorMaskCmp, num_elem, elem_bt, VecMaskUsePred);\n+  if (is_masked_op && !use_predicate && !arch_supports_vector(Op_AndV, num_elem, elem_bt, VecMaskUseLoad)) {\n+    if (C->print_intrinsics()) {\n+      tty->print_cr(\"  ** not supported: arity=2 op=comp\/%d vlen=%d etype=%s ismask=usestore is_masked_op=1\",\n+                    cond->get_con(), num_elem, type2name(elem_bt));\n+    }\n+    return false;\n+  }\n+\n@@ -1372,2 +1929,14 @@\n-  const TypeVect* vt = TypeVect::make(mask_bt, num_elem);\n-  Node* operation = gvn().transform(new VectorMaskCmpNode(pred, v1, v2, pred_node, vt));\n+  const TypeVect* vmask_type = TypeVect::makemask(mask_bt, num_elem);\n+  Node* operation = new VectorMaskCmpNode(pred, v1, v2, pred_node, vmask_type);\n+\n+  if (is_masked_op) {\n+    if (use_predicate) {\n+      operation->add_req(mask);\n+      operation->add_flag(Node::Flag_is_predicated_vector);\n+    } else {\n+      operation = gvn().transform(operation);\n+      operation = VectorNode::make(Op_AndV, operation, mask, vmask_type);\n+    }\n+  }\n+\n+  operation = gvn().transform(operation);\n@@ -1382,5 +1951,7 @@\n-\/\/ <V extends Vector, Sh extends Shuffle>\n-\/\/  V rearrangeOp(Class<V> vectorClass, Class<Sh> shuffleClass, Class< ? > elementType, int vlen,\n-\/\/    V v1, Sh sh,\n-\/\/    VectorSwizzleOp<V, Sh, S, E> defaultImpl) { ...\n-\n+\/\/ <V extends Vector<E>,\n+\/\/  Sh extends VectorShuffle<E>,\n+\/\/  M extends VectorMask<E>,\n+\/\/  E>\n+\/\/ V rearrangeOp(Class<? extends V> vectorClass, Class<Sh> shuffleClass, Class<M> maskClass, Class<E> elementType, int vlen,\n+\/\/               V v1, Sh sh, M m,\n+\/\/               VectorRearrangeOp<V, Sh, M, E> defaultImpl)\n@@ -1390,2 +1961,3 @@\n-  const TypeInstPtr* elem_klass    = gvn().type(argument(2))->isa_instptr();\n-  const TypeInt*     vlen          = gvn().type(argument(3))->isa_int();\n+  const TypeInstPtr* mask_klass    = gvn().type(argument(2))->isa_instptr();\n+  const TypeInstPtr* elem_klass    = gvn().type(argument(3))->isa_instptr();\n+  const TypeInt*     vlen          = gvn().type(argument(4))->isa_int();\n@@ -1393,1 +1965,1 @@\n-  if (vector_klass == NULL || shuffle_klass == NULL || elem_klass == NULL || vlen == NULL) {\n+  if (vector_klass == NULL  || shuffle_klass == NULL ||  elem_klass == NULL || vlen == NULL) {\n@@ -1396,2 +1968,4 @@\n-  if (shuffle_klass->const_oop() == NULL || vector_klass->const_oop() == NULL ||\n-    elem_klass->const_oop() == NULL || !vlen->is_con()) {\n+  if (shuffle_klass->const_oop() == NULL ||\n+      vector_klass->const_oop()  == NULL ||\n+      elem_klass->const_oop()    == NULL ||\n+      !vlen->is_con()) {\n@@ -1402,2 +1976,2 @@\n-                    NodeClassNames[argument(2)->Opcode()],\n-                    NodeClassNames[argument(3)->Opcode()]);\n+                    NodeClassNames[argument(3)->Opcode()],\n+                    NodeClassNames[argument(4)->Opcode()]);\n@@ -1407,1 +1981,2 @@\n-  if (!is_klass_initialized(vector_klass) || !is_klass_initialized(shuffle_klass)) {\n+  if (!is_klass_initialized(vector_klass)  ||\n+      !is_klass_initialized(shuffle_klass)) {\n@@ -1431,1 +2006,7 @@\n-  if (!arch_supports_vector(Op_VectorRearrange, num_elem, elem_bt, VecMaskNotUsed)) {\n+\n+  bool is_masked_op = argument(7)->bottom_type() != TypePtr::NULL_PTR;\n+  bool use_predicate = is_masked_op;\n+  if (is_masked_op &&\n+      (mask_klass == NULL ||\n+       mask_klass->const_oop() == NULL ||\n+       !is_klass_initialized(mask_klass))) {\n@@ -1433,2 +2014,15 @@\n-      tty->print_cr(\"  ** not supported: arity=2 op=shuffle\/rearrange vlen=%d etype=%s ismask=no\",\n-                    num_elem, type2name(elem_bt));\n+      tty->print_cr(\"  ** mask_klass argument not initialized\");\n+    }\n+  }\n+  VectorMaskUseType checkFlags = (VectorMaskUseType)(is_masked_op ? (VecMaskUseLoad | VecMaskUsePred) : VecMaskNotUsed);\n+  if (!arch_supports_vector(Op_VectorRearrange, num_elem, elem_bt, checkFlags)) {\n+    use_predicate = false;\n+    if(!is_masked_op ||\n+       (!arch_supports_vector(Op_VectorRearrange, num_elem, elem_bt, VecMaskNotUsed) ||\n+        !arch_supports_vector(Op_VectorBlend, num_elem, elem_bt, VecMaskUseLoad)     ||\n+        !arch_supports_vector(VectorNode::replicate_opcode(elem_bt), num_elem, elem_bt, VecMaskNotUsed))) {\n+      if (C->print_intrinsics()) {\n+        tty->print_cr(\"  ** not supported: arity=2 op=shuffle\/rearrange vlen=%d etype=%s ismask=no\",\n+                      num_elem, type2name(elem_bt));\n+      }\n+      return false; \/\/ not supported\n@@ -1436,1 +2030,0 @@\n-    return false; \/\/ not supported\n@@ -1444,2 +2037,2 @@\n-  Node* v1 = unbox_vector(argument(4), vbox_type, elem_bt, num_elem);\n-  Node* shuffle = unbox_vector(argument(5), shbox_type, shuffle_bt, num_elem);\n+  Node* v1 = unbox_vector(argument(5), vbox_type, elem_bt, num_elem);\n+  Node* shuffle = unbox_vector(argument(6), shbox_type, shuffle_bt, num_elem);\n@@ -1451,1 +2044,28 @@\n-  Node* rearrange = gvn().transform(new VectorRearrangeNode(v1, shuffle));\n+  Node* mask = NULL;\n+  if (is_masked_op) {\n+    ciKlass* mbox_klass = mask_klass->const_oop()->as_instance()->java_lang_Class_klass();\n+    const TypeInstPtr* mbox_type = TypeInstPtr::make_exact(TypePtr::NotNull, mbox_klass);\n+    mask = unbox_vector(argument(7), mbox_type, elem_bt, num_elem);\n+    if (mask == NULL) {\n+      if (C->print_intrinsics()) {\n+        tty->print_cr(\"  ** not supported: arity=3 op=shuffle\/rearrange vlen=%d etype=%s ismask=useload is_masked_op=1\",\n+                      num_elem, type2name(elem_bt));\n+      }\n+      return false;\n+    }\n+  }\n+\n+  Node* rearrange = new VectorRearrangeNode(v1, shuffle);\n+  if (is_masked_op) {\n+    if (use_predicate) {\n+      rearrange->add_req(mask);\n+      rearrange->add_flag(Node::Flag_is_predicated_vector);\n+    } else {\n+      const TypeVect* vt = v1->bottom_type()->is_vect();\n+      rearrange = gvn().transform(rearrange);\n+      Node* zero = gvn().makecon(Type::get_zero_type(elem_bt));\n+      Node* zerovec = gvn().transform(VectorNode::scalar2vector(zero, num_elem, Type::get_const_basic_type(elem_bt)));\n+      rearrange = new VectorBlendNode(zerovec, rearrange, mask);\n+    }\n+  }\n+  rearrange = gvn().transform(rearrange);\n@@ -1517,5 +2137,7 @@\n-\/\/  <V extends Vector<?,?>>\n-\/\/  V broadcastInt(int opr, Class<V> vectorClass, Class<?> elementType, int vlen,\n-\/\/                 V v, int i,\n-\/\/                 VectorBroadcastIntOp<V> defaultImpl) {\n-\/\/\n+\/\/  <V extends Vector<E>,\n+\/\/   M extends VectorMask<E>,\n+\/\/   E>\n+\/\/  V broadcastInt(int opr, Class<? extends V> vectorClass, Class<? extends M> maskClass,\n+\/\/                 Class<E> elementType, int length,\n+\/\/                 V v, int n, M m,\n+\/\/                 VectorBroadcastIntOp<V, M> defaultImpl)\n@@ -1525,2 +2147,3 @@\n-  const TypeInstPtr* elem_klass   = gvn().type(argument(2))->isa_instptr();\n-  const TypeInt*     vlen         = gvn().type(argument(3))->isa_int();\n+  const TypeInstPtr* mask_klass   = gvn().type(argument(2))->isa_instptr();\n+  const TypeInstPtr* elem_klass   = gvn().type(argument(3))->isa_instptr();\n+  const TypeInt*     vlen         = gvn().type(argument(4))->isa_int();\n@@ -1536,2 +2159,2 @@\n-                    NodeClassNames[argument(2)->Opcode()],\n-                    NodeClassNames[argument(3)->Opcode()]);\n+                    NodeClassNames[argument(3)->Opcode()],\n+                    NodeClassNames[argument(4)->Opcode()]);\n@@ -1547,0 +2170,26 @@\n+\n+  const Type* vmask_type = gvn().type(argument(7));\n+  bool is_masked_op = vmask_type != TypePtr::NULL_PTR;\n+  if (is_masked_op) {\n+    if (mask_klass == NULL || mask_klass->const_oop() == NULL) {\n+      if (C->print_intrinsics()) {\n+        tty->print_cr(\"  ** missing constant: maskclass=%s\", NodeClassNames[argument(2)->Opcode()]);\n+      }\n+      return false; \/\/ not enough info for intrinsification\n+    }\n+\n+    if (!is_klass_initialized(mask_klass)) {\n+      if (C->print_intrinsics()) {\n+        tty->print_cr(\"  ** mask klass argument not initialized\");\n+      }\n+      return false;\n+    }\n+\n+    if (vmask_type->maybe_null()) {\n+      if (C->print_intrinsics()) {\n+        tty->print_cr(\"  ** null mask values are not allowed for masked op\");\n+      }\n+      return false;\n+    }\n+  }\n+\n@@ -1554,1 +2203,1 @@\n-  BasicType elem_bt = elem_type->basic_type();\n+\n@@ -1556,0 +2205,1 @@\n+  BasicType elem_bt = elem_type->basic_type();\n@@ -1557,0 +2207,1 @@\n+\n@@ -1559,0 +2210,1 @@\n+\n@@ -1565,0 +2217,1 @@\n+\n@@ -1572,1 +2225,2 @@\n-  Node* cnt  = argument(5);\n+\n+  Node* cnt  = argument(6);\n@@ -1581,4 +2235,15 @@\n-  if (!arch_supports_vector(sopc, num_elem, elem_bt, VecMaskNotUsed, has_scalar_args)) {\n-    if (C->print_intrinsics()) {\n-      tty->print_cr(\"  ** not supported: arity=0 op=int\/%d vlen=%d etype=%s ismask=no\",\n-                    sopc, num_elem, type2name(elem_bt));\n+\n+  VectorMaskUseType checkFlags = (VectorMaskUseType)(is_masked_op ? (VecMaskUseLoad | VecMaskUsePred) : VecMaskNotUsed);\n+  bool use_predicate = is_masked_op;\n+\n+  if (!arch_supports_vector(sopc, num_elem, elem_bt, checkFlags, has_scalar_args)) {\n+    use_predicate = false;\n+    if (!is_masked_op ||\n+        (!arch_supports_vector(sopc, num_elem, elem_bt, VecMaskNotUsed, has_scalar_args) ||\n+         !arch_supports_vector(Op_VectorBlend, num_elem, elem_bt, VecMaskUseLoad))) {\n+\n+      if (C->print_intrinsics()) {\n+        tty->print_cr(\"  ** not supported: arity=0 op=int\/%d vlen=%d etype=%s is_masked_op=%d\",\n+                      sopc, num_elem, type2name(elem_bt), is_masked_op ? 1 : 0);\n+      }\n+      return false; \/\/ not supported\n@@ -1586,1 +2251,0 @@\n-    return false; \/\/ not supported\n@@ -1588,1 +2252,2 @@\n-  Node* opd1 = unbox_vector(argument(4), vbox_type, elem_bt, num_elem);\n+\n+  Node* opd1 = unbox_vector(argument(5), vbox_type, elem_bt, num_elem);\n@@ -1603,0 +2268,1 @@\n+\n@@ -1606,1 +2272,0 @@\n-  Node* operation = gvn().transform(VectorNode::make(opc, opd1, opd2, num_elem, elem_bt));\n@@ -1608,0 +2273,24 @@\n+  Node* mask = NULL;\n+  if (is_masked_op) {\n+    ciKlass* mbox_klass = mask_klass->const_oop()->as_instance()->java_lang_Class_klass();\n+    const TypeInstPtr* mbox_type = TypeInstPtr::make_exact(TypePtr::NotNull, mbox_klass);\n+    mask = unbox_vector(argument(7), mbox_type, elem_bt, num_elem);\n+    if (mask == NULL) {\n+      if (C->print_intrinsics()) {\n+        tty->print_cr(\"  ** unbox failed mask=%s\", NodeClassNames[argument(7)->Opcode()]);\n+      }\n+      return false;\n+    }\n+  }\n+\n+  Node* operation = VectorNode::make(opc, opd1, opd2, num_elem, elem_bt);\n+  if (is_masked_op && mask != NULL) {\n+    if (use_predicate) {\n+      operation->add_req(mask);\n+      operation->add_flag(Node::Flag_is_predicated_vector);\n+    } else {\n+      operation = gvn().transform(operation);\n+      operation = new VectorBlendNode(opd1, operation, mask);\n+    }\n+  }\n+  operation = gvn().transform(operation);\n@@ -1621,1 +2310,1 @@\n-\/\/           VectorConvertOp<VOUT, VIN, S> defaultImpl) {\n+\/\/           VectorConvertOp<VOUT, VIN, S> defaultImpl)\n@@ -1682,3 +2371,0 @@\n-  if (is_mask && (type2aelembytes(elem_bt_from) != type2aelembytes(elem_bt_to))) {\n-    return false; \/\/ elem size mismatch\n-  }\n@@ -1730,2 +2416,13 @@\n-  const TypeVect* src_type = TypeVect::make(elem_bt_from, num_elem_from);\n-  const TypeVect* dst_type = TypeVect::make(elem_bt_to,   num_elem_to);\n+  const TypeVect* src_type = TypeVect::make(elem_bt_from, num_elem_from, is_mask);\n+  const TypeVect* dst_type = TypeVect::make(elem_bt_to, num_elem_to, is_mask);\n+\n+  \/\/ Safety check to prevent casting if source mask is of type vector\n+  \/\/ and destination mask of type predicate vector and vice-versa.\n+  \/\/ From X86 standpoint, this case will only arise over KNL target,\n+  \/\/ where certain masks (depending on the species) are either propagated\n+  \/\/ through a vector or predicate register.\n+  if (is_mask &&\n+      ((src_type->isa_vectmask() == NULL && dst_type->isa_vectmask()) ||\n+       (dst_type->isa_vectmask() == NULL && src_type->isa_vectmask()))) {\n+    return false;\n+  }\n@@ -1735,2 +2432,6 @@\n-    assert(!is_mask, \"masks cannot be casted\");\n-    int cast_vopc = VectorCastNode::opcode(elem_bt_from);\n+    BasicType new_elem_bt_to = elem_bt_to;\n+    BasicType new_elem_bt_from = elem_bt_from;\n+    if (is_mask && is_floating_point_type(elem_bt_from)) {\n+      new_elem_bt_from = elem_bt_from == T_FLOAT ? T_INT : T_LONG;\n+    }\n+    int cast_vopc = VectorCastNode::opcode(new_elem_bt_from);\n@@ -1790,3 +2491,26 @@\n-      \/\/ Since input and output number of elements match, and since we know this vector size is\n-      \/\/ supported, simply do a cast with no resize needed.\n-      op = gvn().transform(VectorCastNode::make(cast_vopc, op, elem_bt_to, num_elem_to));\n+      if (is_mask) {\n+        if ((dst_type->isa_vectmask() && src_type->isa_vectmask()) ||\n+            (type2aelembytes(elem_bt_from) == type2aelembytes(elem_bt_to))) {\n+          op = gvn().transform(new VectorMaskCastNode(op, dst_type));\n+        } else {\n+          \/\/ Special handling for casting operation involving floating point types.\n+          \/\/ Case A) F -> X :=  F -> VectorMaskCast (F->I\/L [NOP]) -> VectorCast[I\/L]2X\n+          \/\/ Case B) X -> F :=  X -> VectorCastX2[I\/L] -> VectorMaskCast ([I\/L]->F [NOP])\n+          \/\/ Case C) F -> F :=  VectorMaskCast (F->I\/L [NOP]) -> VectorCast[I\/L]2[L\/I] -> VectotMaskCast (L\/I->F [NOP])\n+          if (is_floating_point_type(elem_bt_from)) {\n+            const TypeVect* new_src_type = TypeVect::make(new_elem_bt_from, num_elem_to, is_mask);\n+            op = gvn().transform(new VectorMaskCastNode(op, new_src_type));\n+          }\n+          if (is_floating_point_type(elem_bt_to)) {\n+            new_elem_bt_to = elem_bt_to == T_FLOAT ? T_INT : T_LONG;\n+          }\n+          op = gvn().transform(VectorCastNode::make(cast_vopc, op, new_elem_bt_to, num_elem_to));\n+          if (new_elem_bt_to != elem_bt_to) {\n+            op = gvn().transform(new VectorMaskCastNode(op, dst_type));\n+          }\n+        }\n+      } else {\n+        \/\/ Since input and output number of elements match, and since we know this vector size is\n+        \/\/ supported, simply do a cast with no resize needed.\n+        op = gvn().transform(VectorCastNode::make(cast_vopc, op, elem_bt_to, num_elem_to));\n+      }\n@@ -1807,2 +2531,3 @@\n-\/\/  <V extends Vector<?>>\n-\/\/  V insert(Class<? extends V> vectorClass, Class<?> elementType, int vlen,\n+\/\/  <V extends Vector<E>,\n+\/\/   E>\n+\/\/  V insert(Class<? extends V> vectorClass, Class<E> elementType, int vlen,\n@@ -1810,2 +2535,1 @@\n-\/\/           VecInsertOp<V> defaultImpl) {\n-\/\/\n+\/\/           VecInsertOp<V> defaultImpl)\n@@ -1900,2 +2624,3 @@\n-\/\/  <V extends Vector<?>>\n-\/\/  long extract(Class<?> vectorClass, Class<?> elementType, int vlen,\n+\/\/  <V extends Vector<E>,\n+\/\/   E>\n+\/\/  long extract(Class<? extends V> vectorClass, Class<E> elementType, int vlen,\n@@ -1903,2 +2628,1 @@\n-\/\/               VecExtractOp<V> defaultImpl) {\n-\/\/\n+\/\/               VecExtractOp<V> defaultImpl)\n","filename":"src\/hotspot\/share\/opto\/vectorIntrinsics.cpp","additions":966,"deletions":242,"binary":false,"changes":1208,"status":"modified"},{"patch":"@@ -227,0 +227,8 @@\n+  case Op_ConvI2F:\n+    return Op_VectorCastI2X;\n+  case Op_ConvL2D:\n+    return Op_VectorCastL2X;\n+  case Op_ConvF2I:\n+    return Op_VectorCastF2X;\n+  case Op_ConvD2L:\n+    return Op_VectorCastD2X;\n@@ -369,2 +377,2 @@\n-bool VectorNode::is_vshift_cnt(Node* n) {\n-  switch (n->Opcode()) {\n+bool VectorNode::is_vshift_cnt_opcode(int opc) {\n+  switch (opc) {\n@@ -379,0 +387,4 @@\n+bool VectorNode::is_vshift_cnt(Node* n) {\n+  return is_vshift_cnt_opcode(n->Opcode());\n+}\n+\n@@ -445,0 +457,25 @@\n+VectorNode* VectorNode::make_mask_node(int vopc, Node* n1, Node* n2, uint vlen, BasicType bt) {\n+  guarantee(vopc > 0, \"vopc must be > 0\");\n+  const TypeVect* vmask_type = TypeVect::makemask(bt, vlen);\n+  switch (vopc) {\n+    case Op_AndV:\n+      if (Matcher::match_rule_supported_vector_masked(Op_AndVMask, vlen, bt)) {\n+        return new AndVMaskNode(n1, n2, vmask_type);\n+      }\n+      return new AndVNode(n1, n2, vmask_type);\n+    case Op_OrV:\n+      if (Matcher::match_rule_supported_vector_masked(Op_OrVMask, vlen, bt)) {\n+        return new OrVMaskNode(n1, n2, vmask_type);\n+      }\n+      return new OrVNode(n1, n2, vmask_type);\n+    case Op_XorV:\n+      if (Matcher::match_rule_supported_vector_masked(Op_XorVMask, vlen, bt)) {\n+        return new XorVMaskNode(n1, n2, vmask_type);\n+      }\n+      return new XorVNode(n1, n2, vmask_type);\n+    default:\n+      fatal(\"Unsupported mask vector creation for '%s'\", NodeClassNames[vopc]);\n+      return NULL;\n+  }\n+}\n+\n@@ -446,1 +483,1 @@\n-VectorNode* VectorNode::make(int vopc, Node* n1, Node* n2, const TypeVect* vt) {\n+VectorNode* VectorNode::make(int vopc, Node* n1, Node* n2, const TypeVect* vt, bool is_mask) {\n@@ -449,0 +486,5 @@\n+\n+  if (is_mask) {\n+    return make_mask_node(vopc, n1, n2, vt->length(), vt->element_basic_type());\n+  }\n+\n@@ -555,1 +597,1 @@\n-VectorNode* VectorNode::scalar2vector(Node* s, uint vlen, const Type* opd_t) {\n+VectorNode* VectorNode::scalar2vector(Node* s, uint vlen, const Type* opd_t, bool is_mask) {\n@@ -557,2 +599,7 @@\n-  const TypeVect* vt = opd_t->singleton() ? TypeVect::make(opd_t, vlen)\n-                                          : TypeVect::make(bt, vlen);\n+  const TypeVect* vt = opd_t->singleton() ? TypeVect::make(opd_t, vlen, is_mask)\n+                                          : TypeVect::make(bt, vlen, is_mask);\n+\n+  if (is_mask && Matcher::match_rule_supported_vector(Op_MaskAll, vlen, bt)) {\n+    return new MaskAllNode(s, vt);\n+  }\n+\n@@ -1009,1 +1056,1 @@\n-  if (out_bt == T_BOOLEAN) {\n+  if (!Matcher::has_predicated_vectors() && out_bt == T_BOOLEAN) {\n@@ -1012,0 +1059,1 @@\n+\n@@ -1108,0 +1156,1 @@\n+          return gvn.makecon(TypeInt::make(max_jbyte));\n@@ -1109,0 +1158,1 @@\n+          return gvn.makecon(TypeInt::make(max_jshort));\n@@ -1123,0 +1173,1 @@\n+          return gvn.makecon(TypeInt::make(min_jbyte));\n@@ -1124,0 +1175,1 @@\n+          return gvn.makecon(TypeInt::make(min_jshort));\n@@ -1316,0 +1368,1 @@\n+          const TypeVect* vmask_type = TypeVect::makemask(out_vt->element_basic_type(), out_vt->length());\n@@ -1321,1 +1374,1 @@\n-            return new VectorMaskCastNode(value, out_vt);\n+            return new VectorMaskCastNode(value, vmask_type);\n@@ -1325,1 +1378,1 @@\n-          return new VectorLoadMaskNode(value, out_vt);\n+          return new VectorLoadMaskNode(value, vmask_type);\n@@ -1383,0 +1436,2 @@\n+    case Op_VectorMaskToLong:\n+      return new VectorMaskToLongNode(mask, ty);\n@@ -1389,1 +1444,0 @@\n-\n","filename":"src\/hotspot\/share\/opto\/vectornode.cpp","additions":64,"deletions":10,"binary":false,"changes":74,"status":"modified"},{"patch":"@@ -69,1 +69,3 @@\n-  virtual uint ideal_reg() const { return Matcher::vector_ideal_reg(vect_type()->length_in_bytes()); }\n+  virtual uint ideal_reg() const {\n+    return type()->ideal_reg();\n+  }\n@@ -71,1 +73,1 @@\n-  static VectorNode* scalar2vector(Node* s, uint vlen, const Type* opd_t);\n+  static VectorNode* scalar2vector(Node* s, uint vlen, const Type* opd_t, bool is_mask = false);\n@@ -74,1 +76,1 @@\n-  static VectorNode* make(int vopc, Node* n1, Node* n2, const TypeVect* vt);\n+  static VectorNode* make(int vopc, Node* n1, Node* n2, const TypeVect* vt, bool is_mask = false);\n@@ -77,0 +79,1 @@\n+  static VectorNode* make_mask_node(int vopc, Node* n1, Node* n2, uint vlen, BasicType bt);\n@@ -79,0 +82,3 @@\n+\n+  static bool is_vshift_cnt_opcode(int opc);\n+\n@@ -801,2 +807,2 @@\n-                                                     idx == MemNode::ValueIn ||\n-                                                     idx == MemNode::ValueIn + 1; }\n+                                                    idx == MemNode::ValueIn ||\n+                                                    idx == MemNode::ValueIn + 1; }\n@@ -811,1 +817,1 @@\n-    assert(mask->bottom_type()->is_vectmask(), \"sanity\");\n+    assert(mask->bottom_type()->isa_vectmask(), \"sanity\");\n@@ -831,1 +837,1 @@\n-    assert(mask->bottom_type()->is_vectmask(), \"sanity\");\n+    assert(mask->bottom_type()->isa_vectmask(), \"sanity\");\n@@ -845,0 +851,39 @@\n+\/\/-------------------------------LoadVectorGatherMaskedNode---------------------------------\n+\/\/ Load Vector from memory via index map under the influence of a predicate register(mask).\n+class LoadVectorGatherMaskedNode : public LoadVectorNode {\n+ public:\n+  LoadVectorGatherMaskedNode(Node* c, Node* mem, Node* adr, const TypePtr* at, const TypeVect* vt, Node* indices, Node* mask)\n+    : LoadVectorNode(c, mem, adr, at, vt) {\n+    init_class_id(Class_LoadVector);\n+    assert(indices->bottom_type()->is_vect(), \"indices must be in vector\");\n+    assert(mask->bottom_type()->isa_vectmask(), \"sanity\");\n+    add_req(indices);\n+    add_req(mask);\n+    assert(req() == MemNode::ValueIn + 2, \"match_edge expects that last input is in MemNode::ValueIn+1\");\n+  }\n+\n+  virtual int Opcode() const;\n+  virtual uint match_edge(uint idx) const { return idx == MemNode::Address ||\n+                                                   idx == MemNode::ValueIn ||\n+                                                   idx == MemNode::ValueIn + 1; }\n+};\n+\n+\/\/------------------------------StoreVectorScatterMaskedNode--------------------------------\n+\/\/ Store Vector into memory via index map under the influence of a predicate register(mask).\n+class StoreVectorScatterMaskedNode : public StoreVectorNode {\n+  public:\n+   StoreVectorScatterMaskedNode(Node* c, Node* mem, Node* adr, const TypePtr* at, Node* val, Node* indices, Node* mask)\n+     : StoreVectorNode(c, mem, adr, at, val) {\n+     init_class_id(Class_StoreVector);\n+     assert(indices->bottom_type()->is_vect(), \"indices must be in vector\");\n+     assert(mask->bottom_type()->isa_vectmask(), \"sanity\");\n+     add_req(indices);\n+     add_req(mask);\n+     assert(req() == MemNode::ValueIn + 3, \"match_edge expects that last input is in MemNode::ValueIn+2\");\n+   }\n+   virtual int Opcode() const;\n+   virtual uint match_edge(uint idx) const { return idx == MemNode::Address ||\n+                                                    idx == MemNode::ValueIn ||\n+                                                    idx == MemNode::ValueIn + 1 ||\n+                                                    idx == MemNode::ValueIn + 2; }\n+};\n@@ -859,1 +904,0 @@\n-\n@@ -881,1 +925,1 @@\n-    assert(mask->bottom_type()->is_vect()->element_basic_type() == T_BOOLEAN, \"\");\n+    assert(Matcher::has_predicated_vectors() || mask->bottom_type()->is_vect()->element_basic_type() == T_BOOLEAN, \"\");\n@@ -916,0 +960,36 @@\n+class VectorMaskToLongNode : public VectorMaskOpNode {\n+ public:\n+  VectorMaskToLongNode(Node* mask, const Type* ty):\n+    VectorMaskOpNode(mask, ty, Op_VectorMaskToLong) {}\n+  virtual int Opcode() const;\n+  virtual uint  ideal_reg() const { return Op_RegL; }\n+};\n+\n+\/\/-------------------------- Vector mask broadcast -----------------------------------\n+class MaskAllNode : public VectorNode {\n+ public:\n+  MaskAllNode(Node* in, const TypeVect* vt) : VectorNode(in, vt) {}\n+  virtual int Opcode() const;\n+};\n+\n+\/\/--------------------------- Vector mask logical and --------------------------------\n+class AndVMaskNode : public VectorNode {\n+ public:\n+  AndVMaskNode(Node* in1, Node* in2, const TypeVect* vt) : VectorNode(in1, in2, vt) {}\n+  virtual int Opcode() const;\n+};\n+\n+\/\/--------------------------- Vector mask logical or ---------------------------------\n+class OrVMaskNode : public VectorNode {\n+ public:\n+  OrVMaskNode(Node* in1, Node* in2, const TypeVect* vt) : VectorNode(in1, in2, vt) {}\n+  virtual int Opcode() const;\n+};\n+\n+\/\/--------------------------- Vector mask logical xor --------------------------------\n+class XorVMaskNode : public VectorNode {\n+ public:\n+  XorVMaskNode(Node* in1, Node* in2, const TypeVect* vt) : VectorNode(in1, in2, vt) {}\n+  virtual int Opcode() const;\n+};\n+\n@@ -1187,1 +1267,1 @@\n-  uint size_of() const { return sizeof(*this); }\n+  virtual  uint size_of() const { return sizeof(VectorMaskCmpNode); }\n@@ -1197,0 +1277,1 @@\n+    assert((BoolTest::mask)predicate_node->get_int() == predicate, \"Unmatched predicates\");\n@@ -1308,1 +1389,0 @@\n-    assert(type2aelembytes(in_vt->element_basic_type()) == type2aelembytes(vt->element_basic_type()), \"element size must match\");\n@@ -1318,0 +1398,1 @@\n+\n@@ -1319,1 +1400,1 @@\n-  uint size_of() const { return sizeof(*this); }\n+  uint size_of() const { return sizeof(VectorReinterpretNode); }\n@@ -1322,1 +1403,6 @@\n-      : VectorNode(in, dst_vt), _src_vt(src_vt) { }\n+     : VectorNode(in, dst_vt), _src_vt(src_vt) {\n+     assert((!dst_vt->isa_vectmask() && !src_vt->isa_vectmask()) ||\n+            (type2aelembytes(src_vt->element_basic_type()) >= type2aelembytes(dst_vt->element_basic_type())),\n+            \"unsupported mask widening reinterpretation\");\n+     init_class_id(Class_VectorReinterpret);\n+  }\n@@ -1324,0 +1410,1 @@\n+  const TypeVect* src_type() { return _src_vt; }\n@@ -1456,0 +1543,1 @@\n+    init_class_id(Class_VectorUnbox);\n@@ -1485,1 +1573,0 @@\n-\n","filename":"src\/hotspot\/share\/opto\/vectornode.hpp","additions":101,"deletions":14,"binary":false,"changes":115,"status":"modified"},{"patch":"@@ -642,5 +642,2 @@\n-  JNIHandleBlock* old_handles = thread->active_handles();\n-  JNIHandleBlock* new_handles = JNIHandleBlock::allocate_block(thread);\n-  assert(new_handles != NULL, \"should not be NULL\");\n-  new_handles->set_pop_frame_link(old_handles);\n-  thread->set_active_handles(new_handles);\n+\n+  thread->push_jni_handle_block();\n","filename":"src\/hotspot\/share\/prims\/jni.cpp","additions":2,"deletions":5,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -429,2 +429,2 @@\n-  if (DynamicDumpSharedSpaces) {\n-    DynamicArchive::prepare_for_dynamic_dumping();\n+  if (DynamicArchive::should_dump_at_vm_exit()) {\n+    DynamicArchive::prepare_for_dump_at_exit();\n@@ -3709,1 +3709,1 @@\n-  DynamicArchive::dump(archive_name, CHECK);\n+  DynamicArchive::dump_for_jcmd(archive_name, CHECK);\n","filename":"src\/hotspot\/share\/prims\/jvm.cpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -1513,1 +1513,1 @@\n-    array[idx] = JNIHandles::make_local(Thread::current(), _tbl->at(idx).resolve());\n+    array[idx] = JNIHandles::make_local(_tbl->at(idx).resolve());\n","filename":"src\/hotspot\/share\/prims\/jvmtiEnvBase.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -144,3 +144,0 @@\n-#if 0\n-  JNIHandleBlock* _hblock;\n-#endif\n@@ -152,6 +149,0 @@\n-#if 0\n-    _hblock = thread->active_handles();\n-    _hblock->clear_thoroughly(); \/\/ so we can be safe\n-#else\n-    \/\/ we want to use the code above - but that needs the JNIHandle changes - later...\n-    \/\/ for now, steal JNI push local frame code\n@@ -165,6 +156,1 @@\n-    JNIHandleBlock* old_handles = thread->active_handles();\n-    JNIHandleBlock* new_handles = JNIHandleBlock::allocate_block(thread);\n-    assert(new_handles != NULL, \"should not be NULL\");\n-    new_handles->set_pop_frame_link(old_handles);\n-    thread->set_active_handles(new_handles);\n-#endif\n+    thread->push_jni_handle_block();\n@@ -176,14 +162,1 @@\n-#if 0\n-    _hblock->clear(); \/\/ for consistency with future correct behavior\n-#else\n-    \/\/ we want to use the code above - but that needs the JNIHandle changes - later...\n-    \/\/ for now, steal JNI pop local frame code\n-    JNIHandleBlock* old_handles = _thread->active_handles();\n-    JNIHandleBlock* new_handles = old_handles->pop_frame_link();\n-    assert(new_handles != NULL, \"should not be NULL\");\n-    _thread->set_active_handles(new_handles);\n-    \/\/ Note that we set the pop_frame_link to NULL explicitly, otherwise\n-    \/\/ the release_block call will release the blocks.\n-    old_handles->set_pop_frame_link(NULL);\n-    JNIHandleBlock::release_block(old_handles, _thread); \/\/ may block\n-#endif\n+    _thread->pop_jni_handle_block();\n@@ -199,5 +172,0 @@\n-#if 0\n-  jobject to_jobject(oop obj) { return obj == NULL? NULL : _hblock->allocate_handle_fast(obj); }\n-#else\n-  \/\/ we want to use the code above - but that needs the JNIHandle changes - later...\n-  \/\/ for now, use regular make_local\n@@ -205,1 +173,0 @@\n-#endif\n","filename":"src\/hotspot\/share\/prims\/jvmtiExport.cpp","additions":2,"deletions":35,"binary":false,"changes":37,"status":"modified"},{"patch":"@@ -329,0 +329,6 @@\n+#ifdef ZERO\n+    \/\/ The BytecodeInterpreter is specialized only with RewriteBytecodes\n+    \/\/ for simplicity. If we want to disable RewriteFrequentPairs, we\n+    \/\/ need to disable RewriteBytecodes as well.\n+    RewriteBytecodes = false;\n+#endif\n","filename":"src\/hotspot\/share\/prims\/jvmtiManageCapabilities.cpp","additions":6,"deletions":0,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -185,18 +185,0 @@\n-\n-char* NativeLookup::critical_jni_name(const methodHandle& method) {\n-  stringStream st;\n-  \/\/ Prefix\n-  st.print(\"JavaCritical_\");\n-  \/\/ Klass name\n-  if (!map_escaped_name_on(&st, method->klass_name())) {\n-    return NULL;\n-  }\n-  st.print(\"_\");\n-  \/\/ Method name\n-  if (!map_escaped_name_on(&st, method->name())) {\n-    return NULL;\n-  }\n-  return st.as_string();\n-}\n-\n-\n@@ -335,6 +317,0 @@\n-address NativeLookup::lookup_critical_style(void* dll, const char* pure_name, const char* long_name, int args_size, bool os_style) {\n-  const char* jni_name = compute_complete_jni_name(pure_name, long_name, args_size, os_style);\n-  assert(dll != NULL, \"dll must be loaded\");\n-  return (address)os::dll_lookup(dll, jni_name);\n-}\n-\n@@ -384,47 +360,0 @@\n-\/\/ Check all the formats of native implementation name to see if there is one\n-\/\/ for the specified method.\n-address NativeLookup::lookup_critical_entry(const methodHandle& method) {\n-  assert(CriticalJNINatives, \"or should not be here\");\n-\n-  if (method->is_synchronized() ||\n-      !method->is_static()) {\n-    \/\/ Only static non-synchronized methods are allowed\n-    return NULL;\n-  }\n-\n-  ResourceMark rm;\n-\n-  Symbol* signature = method->signature();\n-  for (int end = 0; end < signature->utf8_length(); end++) {\n-    if (signature->char_at(end) == 'L') {\n-      \/\/ Don't allow object types\n-      return NULL;\n-    }\n-  }\n-\n-  \/\/ Compute argument size\n-  int args_size = method->size_of_parameters();\n-  for (SignatureStream ss(signature); !ss.at_return_type(); ss.next()) {\n-    if (ss.is_array()) {\n-      args_size += T_INT_size; \/\/ array length parameter\n-    }\n-  }\n-\n-  \/\/ dll handling requires I\/O. Don't do that while in _thread_in_vm (safepoint may get requested).\n-  ThreadToNativeFromVM thread_in_native(JavaThread::current());\n-\n-  void* dll = dll_load(method);\n-  address entry = NULL;\n-\n-  if (dll != NULL) {\n-    entry = lookup_critical_style(dll, method, args_size);\n-    \/\/ Close the handle to avoid keeping the library alive if the native method holder is unloaded.\n-    \/\/ This is fine because the library is still kept alive by JNI (see JVM_LoadLibrary). As soon\n-    \/\/ as the holder class and the library are unloaded (see JVM_UnloadLibrary), the native wrapper\n-    \/\/ that calls 'critical_entry' becomes unreachable and is unloaded as well.\n-    os::dll_unload(dll);\n-  }\n-\n-  return entry; \/\/ NULL indicates not found\n-}\n-\n@@ -449,38 +378,0 @@\n-address NativeLookup::lookup_critical_style(void* dll, const methodHandle& method, int args_size) {\n-  address entry = NULL;\n-  const char* critical_name = critical_jni_name(method);\n-  if (critical_name == NULL) {\n-    \/\/ JNI name mapping rejected this method so return\n-    \/\/ NULL to indicate UnsatisfiedLinkError should be thrown.\n-    return NULL;\n-  }\n-\n-  \/\/ 1) Try JNI short style\n-  entry = lookup_critical_style(dll, critical_name, \"\",        args_size, true);\n-  if (entry != NULL) {\n-    return entry;\n-  }\n-\n-  const char* long_name = long_jni_name(method);\n-  if (long_name == NULL) {\n-    \/\/ JNI name mapping rejected this method so return\n-    \/\/ NULL to indicate UnsatisfiedLinkError should be thrown.\n-    return NULL;\n-  }\n-\n-  \/\/ 2) Try JNI long style\n-  entry = lookup_critical_style(dll, critical_name, long_name, args_size, true);\n-  if (entry != NULL) {\n-    return entry;\n-  }\n-\n-  \/\/ 3) Try JNI short style without os prefix\/suffix\n-  entry = lookup_critical_style(dll, critical_name, \"\",        args_size, false);\n-  if (entry != NULL) {\n-    return entry;\n-  }\n-\n-  \/\/ 4) Try JNI long style without os prefix\/suffix\n-  return lookup_critical_style(dll, critical_name, long_name, args_size, false);\n-}\n-\n","filename":"src\/hotspot\/share\/prims\/nativeLookup.cpp","additions":0,"deletions":109,"binary":false,"changes":109,"status":"modified"},{"patch":"@@ -38,2 +38,0 @@\n-  static address lookup_critical_style(void* dll, const char* pure_name, const char* long_name, int args_size, bool os_style);\n-  static address lookup_critical_style(void* dll, const methodHandle& method, int args_size);\n@@ -50,1 +48,0 @@\n-  static char* critical_jni_name(const methodHandle& method);\n@@ -54,1 +51,0 @@\n-  static address lookup_critical_entry(const methodHandle& method);\n","filename":"src\/hotspot\/share\/prims\/nativeLookup.hpp","additions":0,"deletions":4,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -295,0 +295,1 @@\n+        case T_LONG:   return Op_NegL;\n@@ -433,0 +434,12 @@\n+    case VECTOR_OP_MASK_TOLONG: {\n+      switch (bt) {\n+        case T_BYTE:  \/\/ fall-through\n+        case T_SHORT: \/\/ fall-through\n+        case T_INT:   \/\/ fall-through\n+        case T_LONG:  \/\/ fall-through\n+        case T_FLOAT: \/\/ fall-through\n+        case T_DOUBLE: return Op_VectorMaskToLong;\n+        default: fatal(\"MASK_TOLONG: %s\", type2name(bt));\n+      }\n+      break;\n+    }\n","filename":"src\/hotspot\/share\/prims\/vectorSupport.cpp","additions":13,"deletions":0,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -85,0 +85,1 @@\n+    VECTOR_OP_MASK_TOLONG    = 22,\n@@ -87,2 +88,2 @@\n-    VECTOR_OP_LROTATE = 22,\n-    VECTOR_OP_RROTATE = 23,\n+    VECTOR_OP_LROTATE = 23,\n+    VECTOR_OP_RROTATE = 24,\n","filename":"src\/hotspot\/share\/prims\/vectorSupport.hpp","additions":3,"deletions":2,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -531,0 +531,4 @@\n+  { \"DumpSharedSpaces\",             JDK_Version::jdk(18), JDK_Version::jdk(19), JDK_Version::undefined() },\n+  { \"DynamicDumpSharedSpaces\",      JDK_Version::jdk(18), JDK_Version::jdk(19), JDK_Version::undefined() },\n+  { \"RequireSharedSpaces\",          JDK_Version::jdk(18), JDK_Version::jdk(19), JDK_Version::undefined() },\n+  { \"UseSharedSpaces\",              JDK_Version::jdk(18), JDK_Version::jdk(19), JDK_Version::undefined() },\n@@ -1442,0 +1446,2 @@\n+      } else {\n+        log_info(cds)(\"CDS is disabled when the %s option is specified.\", unsupported_options[i]);\n@@ -3120,7 +3126,0 @@\n-  if (DumpSharedSpaces || ArchiveClassesAtExit != NULL) {\n-    \/\/ Always verify non-system classes during CDS dump\n-    if (!BytecodeVerificationRemote) {\n-      BytecodeVerificationRemote = true;\n-      log_info(cds)(\"All non-system classes will be verified (-Xverify:remote) during CDS dump time.\");\n-    }\n-  }\n@@ -3130,1 +3129,2 @@\n-    log_info(cds)(\"RecordDynamicDumpInfo is for jcmd only, could not set with -XX:ArchiveClassesAtExit.\");\n+    jio_fprintf(defaultStream::output_stream(),\n+                \"-XX:+RecordDynamicDumpInfo cannot be used with -XX:ArchiveClassesAtExit.\\n\");\n@@ -3146,0 +3146,8 @@\n+\n+  if (DumpSharedSpaces || DynamicDumpSharedSpaces) {\n+    \/\/ Always verify non-system classes during CDS dump\n+    if (!BytecodeVerificationRemote) {\n+      BytecodeVerificationRemote = true;\n+      log_info(cds)(\"All non-system classes will be verified (-Xverify:remote) during CDS dump time.\");\n+    }\n+  }\n@@ -3425,3 +3433,1 @@\n-  if (!init_shared_archive_paths()) {\n-    return JNI_ENOMEM;\n-  }\n+  init_shared_archive_paths();\n@@ -3490,1 +3496,0 @@\n-  \/\/cur_path[len] = '\\0';\n@@ -3495,2 +3500,3 @@\n-bool Arguments::init_shared_archive_paths() {\n-  if (ArchiveClassesAtExit != NULL) {\n+void Arguments::init_shared_archive_paths() {\n+  if (ArchiveClassesAtExit != nullptr) {\n+    assert(!RecordDynamicDumpInfo, \"already checked\");\n@@ -3500,3 +3506,0 @@\n-    if (FLAG_SET_CMDLINE(DynamicDumpSharedSpaces, true) != JVMFlag::SUCCESS) {\n-      return false;\n-    }\n@@ -3504,6 +3507,0 @@\n-    SharedDynamicArchivePath = os::strdup_check_oom(ArchiveClassesAtExit, mtArguments);\n-  } else {\n-    if (SharedDynamicArchivePath != nullptr) {\n-      os::free(SharedDynamicArchivePath);\n-      SharedDynamicArchivePath = nullptr;\n-    }\n@@ -3511,1 +3508,2 @@\n-  if (SharedArchiveFile == NULL) {\n+\n+  if (SharedArchiveFile == nullptr) {\n@@ -3515,12 +3513,5 @@\n-    if (is_dumping_archive()) {\n-      if (archives > 1) {\n-        vm_exit_during_initialization(\n-          \"Cannot have more than 1 archive file specified in -XX:SharedArchiveFile during CDS dumping\");\n-      }\n-      if (DynamicDumpSharedSpaces) {\n-        if (os::same_files(SharedArchiveFile, ArchiveClassesAtExit)) {\n-          vm_exit_during_initialization(\n-            \"Cannot have the same archive file specified for -XX:SharedArchiveFile and -XX:ArchiveClassesAtExit\",\n-            SharedArchiveFile);\n-        }\n-      }\n+    assert(archives > 0, \"must be\");\n+\n+    if (is_dumping_archive() && archives > 1) {\n+      vm_exit_during_initialization(\n+        \"Cannot have more than 1 archive file specified in -XX:SharedArchiveFile during CDS dumping\");\n@@ -3528,1 +3519,16 @@\n-    if (!is_dumping_archive()){\n+\n+    if (DumpSharedSpaces) {\n+      assert(archives == 1, \"must be\");\n+      \/\/ Static dump is simple: only one archive is allowed in SharedArchiveFile. This file\n+      \/\/ will be overwritten no matter regardless of its contents\n+      SharedArchivePath = os::strdup_check_oom(SharedArchiveFile, mtArguments);\n+    } else {\n+      \/\/ SharedArchiveFile may specify one or two files. In case (c), the path for base.jsa\n+      \/\/ is read from top.jsa\n+      \/\/    (a) 1 file:  -XX:SharedArchiveFile=base.jsa\n+      \/\/    (b) 2 files: -XX:SharedArchiveFile=base.jsa:top.jsa\n+      \/\/    (c) 2 files: -XX:SharedArchiveFile=top.jsa\n+      \/\/\n+      \/\/ However, if either RecordDynamicDumpInfo or ArchiveClassesAtExit is used, we do not\n+      \/\/ allow cases (b) and (c). Case (b) is already checked above.\n+\n@@ -3546,2 +3552,18 @@\n-    } else { \/\/ CDS dumping\n-      SharedArchivePath = os::strdup_check_oom(SharedArchiveFile, mtArguments);\n+\n+      if (SharedDynamicArchivePath != nullptr) {\n+        \/\/ Check for case (c)\n+        if (RecordDynamicDumpInfo) {\n+          vm_exit_during_initialization(\"-XX:+RecordDynamicDumpInfo is unsupported when a dynamic CDS archive is specified in -XX:SharedArchiveFile\",\n+                                        SharedArchiveFile);\n+        }\n+        if (ArchiveClassesAtExit != nullptr) {\n+          vm_exit_during_initialization(\"-XX:ArchiveClassesAtExit is unsupported when a dynamic CDS archive is specified in -XX:SharedArchiveFile\",\n+                                        SharedArchiveFile);\n+        }\n+      }\n+\n+      if (ArchiveClassesAtExit != nullptr && os::same_files(SharedArchiveFile, ArchiveClassesAtExit)) {\n+          vm_exit_during_initialization(\n+            \"Cannot have the same archive file specified for -XX:SharedArchiveFile and -XX:ArchiveClassesAtExit\",\n+            SharedArchiveFile);\n+      }\n@@ -3550,1 +3572,0 @@\n-  return (SharedArchivePath != NULL);\n","filename":"src\/hotspot\/share\/runtime\/arguments.cpp","additions":61,"deletions":40,"binary":false,"changes":101,"status":"modified"},{"patch":"@@ -621,1 +621,1 @@\n-  static bool  init_shared_archive_paths() NOT_CDS_RETURN_(false);\n+  static void  init_shared_archive_paths() NOT_CDS_RETURN;\n","filename":"src\/hotspot\/share\/runtime\/arguments.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -317,3 +317,0 @@\n-  product(bool, CriticalJNINatives, false,                                  \\\n-          \"(Deprecated) Check for critical JNI entry points\")               \\\n-                                                                            \\\n@@ -1807,1 +1804,1 @@\n-          \"Use shared spaces for metadata\")                                 \\\n+          \"(Deprecated) Use shared spaces for metadata\")                    \\\n@@ -1813,1 +1810,1 @@\n-          \"Require shared spaces for metadata\")                             \\\n+          \"(Deprecated) Require shared spaces for metadata\")                \\\n@@ -1816,3 +1813,3 @@\n-          \"Special mode: JVM reads a class list, loads classes, builds \"    \\\n-          \"shared spaces, and dumps the shared spaces to a file to be \"     \\\n-          \"used in future JVM runs\")                                        \\\n+          \"(Deprecated) Special mode: JVM reads a class list, loads \"       \\\n+          \"classes, builds shared spaces, and dumps the shared spaces to \"  \\\n+          \"a file to be used in future JVM runs\")                           \\\n@@ -1821,1 +1818,1 @@\n-          \"Dynamic archive\")                                                \\\n+          \"(Deprecated) Dynamic archive\")                                   \\\n","filename":"src\/hotspot\/share\/runtime\/globals.hpp","additions":6,"deletions":9,"binary":false,"changes":15,"status":"modified"},{"patch":"@@ -358,1 +358,1 @@\n-    guarantee(Thread::is_JavaThread_protected(target, \/* checkTLHOnly *\/ true),\n+    guarantee(Thread::is_JavaThread_protected_by_TLH(target),\n@@ -415,1 +415,1 @@\n-    guarantee(Thread::is_JavaThread_protected(target, \/* checkTLHOnly *\/ true),\n+    guarantee(Thread::is_JavaThread_protected_by_TLH(target),\n","filename":"src\/hotspot\/share\/runtime\/handshake.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -506,1 +506,2 @@\n-  if (DynamicDumpSharedSpaces) {\n+  if (DynamicArchive::should_dump_at_vm_exit()) {\n+    assert(ArchiveClassesAtExit != NULL, \"Must be already set\");\n@@ -508,1 +509,1 @@\n-    DynamicArchive::dump(thread);\n+    DynamicArchive::dump(ArchiveClassesAtExit, thread);\n","filename":"src\/hotspot\/share\/runtime\/java.cpp","additions":3,"deletions":2,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -59,1 +59,1 @@\n-  return make_local(Thread::current(), obj);\n+  return make_local(JavaThread::current(), obj);\n@@ -63,1 +63,1 @@\n-jobject JNIHandles::make_local(Thread* thread, oop obj, AllocFailType alloc_failmode) {\n+jobject JNIHandles::make_local(JavaThread* thread, oop obj, AllocFailType alloc_failmode) {\n@@ -68,1 +68,0 @@\n-    assert(thread->is_Java_thread(), \"not a Java thread\");\n@@ -70,1 +69,1 @@\n-    return thread->active_handles()->allocate_handle(obj, alloc_failmode);\n+    return thread->active_handles()->allocate_handle(thread, obj, alloc_failmode);\n@@ -190,1 +189,1 @@\n-jobjectRefType JNIHandles::handle_type(Thread* thread, jobject handle) {\n+jobjectRefType JNIHandles::handle_type(JavaThread* thread, jobject handle) {\n@@ -208,3 +207,1 @@\n-      if (is_local_handle(thread, handle) ||\n-          (thread->is_Java_thread() &&\n-           is_frame_handle(JavaThread::cast(thread), handle))) {\n+      if (is_local_handle(thread, handle) || is_frame_handle(thread, handle)) {\n@@ -223,1 +220,1 @@\n-bool JNIHandles::is_local_handle(Thread* thread, jobject handle) {\n+bool JNIHandles::is_local_handle(JavaThread* thread, jobject handle) {\n@@ -308,6 +305,1 @@\n-\n-int             JNIHandleBlock::_blocks_allocated     = 0;\n-JNIHandleBlock* JNIHandleBlock::_block_free_list      = NULL;\n-#ifndef PRODUCT\n-JNIHandleBlock* JNIHandleBlock::_block_list           = NULL;\n-#endif\n+int JNIHandleBlock::_blocks_allocated = 0;\n@@ -346,2 +338,4 @@\n-JNIHandleBlock* JNIHandleBlock::allocate_block(Thread* thread, AllocFailType alloc_failmode)  {\n-  assert(thread == NULL || thread == Thread::current(), \"sanity check\");\n+JNIHandleBlock* JNIHandleBlock::allocate_block(JavaThread* thread, AllocFailType alloc_failmode)  {\n+  \/\/ The VM thread can allocate a handle block in behalf of another thread during a safepoint.\n+  assert(thread == NULL || thread == Thread::current() || SafepointSynchronize::is_at_safepoint(),\n+         \"sanity check\");\n@@ -354,17 +348,6 @@\n-  }\n-  else {\n-    \/\/ locking with safepoint checking introduces a potential deadlock:\n-    \/\/ - we would hold JNIHandleBlockFreeList_lock and then Threads_lock\n-    \/\/ - another would hold Threads_lock (jni_AttachCurrentThread) and then\n-    \/\/   JNIHandleBlockFreeList_lock (JNIHandleBlock::allocate_block)\n-    MutexLocker ml(JNIHandleBlockFreeList_lock,\n-                   Mutex::_no_safepoint_check_flag);\n-    if (_block_free_list == NULL) {\n-      \/\/ Allocate new block\n-      if (alloc_failmode == AllocFailStrategy::RETURN_NULL) {\n-        block = new (std::nothrow) JNIHandleBlock();\n-        if (block == NULL) {\n-          return NULL;\n-        }\n-      } else {\n-        block = new JNIHandleBlock();\n+  } else {\n+    \/\/ Allocate new block\n+    if (alloc_failmode == AllocFailStrategy::RETURN_NULL) {\n+      block = new (std::nothrow) JNIHandleBlock();\n+      if (block == NULL) {\n+        return NULL;\n@@ -372,7 +355,0 @@\n-      _blocks_allocated++;\n-      block->zap();\n-      #ifndef PRODUCT\n-      \/\/ Link new block to list of all allocated blocks\n-      block->_block_list_link = _block_list;\n-      _block_list = block;\n-      #endif\n@@ -380,3 +356,1 @@\n-      \/\/ Get block from free list\n-      block = _block_free_list;\n-      _block_free_list = _block_free_list->_next;\n+      block = new JNIHandleBlock();\n@@ -384,0 +358,2 @@\n+    Atomic::inc(&_blocks_allocated);\n+    block->zap();\n@@ -397,1 +373,1 @@\n-void JNIHandleBlock::release_block(JNIHandleBlock* block, Thread* thread) {\n+void JNIHandleBlock::release_block(JNIHandleBlock* block, JavaThread* thread) {\n@@ -418,14 +394,2 @@\n-    \/\/ Return blocks to free list\n-    \/\/ locking with safepoint checking introduces a potential deadlock:\n-    \/\/ - we would hold JNIHandleBlockFreeList_lock and then Threads_lock\n-    \/\/ - another would hold Threads_lock (jni_AttachCurrentThread) and then\n-    \/\/   JNIHandleBlockFreeList_lock (JNIHandleBlock::allocate_block)\n-    MutexLocker ml(JNIHandleBlockFreeList_lock,\n-                   Mutex::_no_safepoint_check_flag);\n-    while (block != NULL) {\n-      block->zap();\n-      JNIHandleBlock* next = block->_next;\n-      block->_next = _block_free_list;\n-      _block_free_list = block;\n-      block = next;\n-    }\n+    Atomic::dec(&_blocks_allocated);\n+    delete block;\n@@ -471,1 +435,1 @@\n-jobject JNIHandleBlock::allocate_handle(oop obj, AllocFailType alloc_failmode) {\n+jobject JNIHandleBlock::allocate_handle(JavaThread* caller, oop obj, AllocFailType alloc_failmode) {\n@@ -519,1 +483,1 @@\n-    return allocate_handle(obj, alloc_failmode);\n+    return allocate_handle(caller, obj, alloc_failmode);\n@@ -526,5 +490,1 @@\n-    \/\/ Append new block\n-    Thread* thread = Thread::current();\n-    Handle obj_handle(thread, obj);\n-    \/\/ This can block, so we need to preserve obj across call.\n-    _last->_next = JNIHandleBlock::allocate_block(thread, alloc_failmode);\n+    _last->_next = JNIHandleBlock::allocate_block(caller, alloc_failmode);\n@@ -536,1 +496,0 @@\n-    obj = obj_handle();\n@@ -538,1 +497,1 @@\n-  return allocate_handle(obj, alloc_failmode);  \/\/ retry\n+  return allocate_handle(caller, obj, alloc_failmode);  \/\/ retry\n@@ -615,43 +574,0 @@\n-\n-\n-#ifndef PRODUCT\n-\n-bool JNIHandles::is_local_handle(jobject handle) {\n-  return JNIHandleBlock::any_contains(handle);\n-}\n-\n-bool JNIHandleBlock::any_contains(jobject handle) {\n-  assert(handle != NULL, \"precondition\");\n-  for (JNIHandleBlock* current = _block_list; current != NULL; current = current->_block_list_link) {\n-    if (current->contains(handle)) {\n-      return true;\n-    }\n-  }\n-  return false;\n-}\n-\n-void JNIHandleBlock::print_statistics() {\n-  int used_blocks = 0;\n-  int free_blocks = 0;\n-  int used_handles = 0;\n-  int free_handles = 0;\n-  JNIHandleBlock* block = _block_list;\n-  while (block != NULL) {\n-    if (block->_top > 0) {\n-      used_blocks++;\n-    } else {\n-      free_blocks++;\n-    }\n-    used_handles += block->_top;\n-    free_handles += (block_size_in_oops - block->_top);\n-    block = block->_block_list_link;\n-  }\n-  tty->print_cr(\"JNIHandleBlocks statistics\");\n-  tty->print_cr(\"- blocks allocated: %d\", used_blocks + free_blocks);\n-  tty->print_cr(\"- blocks in use:    %d\", used_blocks);\n-  tty->print_cr(\"- blocks free:      %d\", free_blocks);\n-  tty->print_cr(\"- handles in use:   %d\", used_handles);\n-  tty->print_cr(\"- handles free:     %d\", free_handles);\n-}\n-\n-#endif\n","filename":"src\/hotspot\/share\/runtime\/jniHandles.cpp","additions":27,"deletions":111,"binary":false,"changes":138,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1998, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1998, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -87,1 +87,1 @@\n-  static jobject make_local(Thread* thread, oop obj,  \/\/ Faster version when current thread is known\n+  static jobject make_local(JavaThread* thread, oop obj,  \/\/ Faster version when current thread is known\n@@ -107,1 +107,1 @@\n-  static bool is_local_handle(Thread* thread, jobject handle);\n+  static bool is_local_handle(JavaThread* thread, jobject handle);\n@@ -114,5 +114,0 @@\n-#ifndef PRODUCT\n-  \/\/ Is handle from any local block of any thread?\n-  static bool is_local_handle(jobject handle);\n-#endif\n-\n@@ -120,1 +115,1 @@\n-  static jobjectRefType handle_type(Thread* thread, jobject handle);\n+  static jobjectRefType handle_type(JavaThread* thread, jobject handle);\n@@ -148,0 +143,1 @@\n+  int             _allocate_before_rebuild;     \/\/ Number of blocks to allocate before rebuilding free list\n@@ -155,1 +151,0 @@\n-  int             _allocate_before_rebuild;     \/\/ Number of blocks to allocate before rebuilding free list\n@@ -159,7 +154,0 @@\n-\n-  #ifndef PRODUCT\n-  JNIHandleBlock* _block_list_link;             \/\/ Link for list below\n-  static JNIHandleBlock* _block_list;           \/\/ List of all allocated blocks (for debugging only)\n-  #endif\n-\n-  static JNIHandleBlock* _block_free_list;      \/\/ Free list of currently unused blocks\n@@ -179,1 +167,1 @@\n-  jobject allocate_handle(oop obj, AllocFailType alloc_failmode = AllocFailStrategy::EXIT_OOM);\n+  jobject allocate_handle(JavaThread* caller, oop obj, AllocFailType alloc_failmode = AllocFailStrategy::EXIT_OOM);\n@@ -182,2 +170,2 @@\n-  static JNIHandleBlock* allocate_block(Thread* thread = NULL, AllocFailType alloc_failmode = AllocFailStrategy::EXIT_OOM);\n-  static void release_block(JNIHandleBlock* block, Thread* thread = NULL);\n+  static JNIHandleBlock* allocate_block(JavaThread* thread = NULL, AllocFailType alloc_failmode = AllocFailStrategy::EXIT_OOM);\n+  static void release_block(JNIHandleBlock* block, JavaThread* thread = NULL);\n@@ -206,4 +194,0 @@\n-  #ifndef PRODUCT\n-  static bool any_contains(jobject handle);     \/\/ Does any block currently in use contain handle\n-  static void print_statistics();\n-  #endif\n","filename":"src\/hotspot\/share\/runtime\/jniHandles.hpp","additions":8,"deletions":24,"binary":false,"changes":32,"status":"modified"},{"patch":"@@ -51,1 +51,0 @@\n-Mutex*   JNIHandleBlockFreeList_lock  = NULL;\n@@ -135,0 +134,1 @@\n+Monitor* MetaspaceCritical_lock       = NULL;\n@@ -247,0 +247,1 @@\n+  def(MetaspaceCritical_lock       , PaddedMonitor, nosafepoint-1);\n@@ -262,1 +263,0 @@\n-  def(JNIHandleBlockFreeList_lock  , PaddedMutex  , nosafepoint-1);      \/\/ handles are used by VM thread\n","filename":"src\/hotspot\/share\/runtime\/mutexLocker.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -43,1 +43,0 @@\n-extern Mutex*   JNIHandleBlockFreeList_lock;     \/\/ a lock on the JNI handle block free list\n@@ -143,1 +142,2 @@\n-extern Mutex*   Metaspace_lock;            \/\/ protects Metaspace virtualspace and chunk expansions\n+extern Mutex*   Metaspace_lock;                  \/\/ protects Metaspace virtualspace and chunk expansions\n+extern Monitor* MetaspaceCritical_lock;          \/\/ synchronizes failed metaspace allocations that risk throwing metaspace OOM\n","filename":"src\/hotspot\/share\/runtime\/mutexLocker.hpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -237,1 +237,0 @@\n-  this->set_active_handles(JNIHandleBlock::allocate_block());\n","filename":"src\/hotspot\/share\/runtime\/nonJavaThread.cpp","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -766,1 +766,4 @@\n-    size_t memblock_size = guarded.get_user_size() - MemTracker::malloc_header_size(memblock);\n+    NMT_TrackingLevel level = MemTracker::tracking_level();\n+    const size_t nmt_overhead =\n+        MemTracker::malloc_header_size(level) + MemTracker::malloc_footer_size(level);\n+    size_t memblock_size = guarded.get_user_size() - nmt_overhead;\n@@ -1174,7 +1177,0 @@\n-#ifndef PRODUCT\n-    \/\/ we don't keep the block list in product mode\n-    if (JNIHandles::is_local_handle((jobject) addr)) {\n-      st->print_cr(INTPTR_FORMAT \" is a local jni handle\", p2i(addr));\n-      return;\n-    }\n-#endif\n","filename":"src\/hotspot\/share\/runtime\/os.cpp","additions":4,"deletions":8,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -500,1 +500,1 @@\n-      \/\/ * OptoRuntime::rethrow_C for C2 code\n+      \/\/ * OptoRuntime::handle_exception_C_helper for C2 code\n@@ -3011,1 +3011,0 @@\n-  address critical_entry = NULL;\n@@ -3017,5 +3016,0 @@\n-  if (CriticalJNINatives && !method->is_method_handle_intrinsic()) {\n-    \/\/ We perform the I\/O with transition to native before acquiring AdapterHandlerLibrary_lock.\n-    critical_entry = NativeLookup::lookup_critical_entry(method);\n-  }\n-\n@@ -3064,1 +3058,1 @@\n-      nm = SharedRuntime::generate_native_wrapper(&_masm, method, compile_id, sig_bt, regs, ret_type, critical_entry);\n+      nm = SharedRuntime::generate_native_wrapper(&_masm, method, compile_id, sig_bt, regs, ret_type);\n","filename":"src\/hotspot\/share\/runtime\/sharedRuntime.cpp","additions":2,"deletions":8,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -477,2 +477,1 @@\n-  \/\/ is a JNI critical method, or a compiled method handle adapter,\n-  \/\/ such as _invokeBasic, _linkToVirtual, etc.\n+  \/\/ is a compiled method handle adapter, such as _invokeBasic, _linkToVirtual, etc.\n@@ -484,2 +483,1 @@\n-                                          BasicType ret_type,\n-                                          address critical_entry);\n+                                          BasicType ret_type);\n","filename":"src\/hotspot\/share\/runtime\/sharedRuntime.hpp","additions":2,"deletions":4,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -223,2 +223,0 @@\n-  set_active_handles(NULL);\n-  set_free_handle_block(NULL);\n@@ -439,3 +437,1 @@\n-\/\/ mechanism? If checkTLHOnly is true (default is false), then we only check\n-\/\/ if the target JavaThread is protected by a ThreadsList (if any) associated\n-\/\/ with the calling Thread.\n+\/\/ mechanism?\n@@ -443,1 +439,1 @@\n-bool Thread::is_JavaThread_protected(const JavaThread* p, bool checkTLHOnly) {\n+bool Thread::is_JavaThread_protected(const JavaThread* target) {\n@@ -445,7 +441,0 @@\n-  if (!checkTLHOnly) {\n-    \/\/ Do the simplest check first:\n-    if (SafepointSynchronize::is_at_safepoint()) {\n-      \/\/ The target is protected since JavaThreads cannot exit\n-      \/\/ while we're at a safepoint.\n-      return true;\n-    }\n@@ -453,6 +442,6 @@\n-    \/\/ If the target hasn't been started yet then it is trivially\n-    \/\/ \"protected\". We assume the caller is the thread that will do\n-    \/\/ the starting.\n-    if (p->osthread() == NULL || p->osthread()->get_state() <= INITIALIZED) {\n-      return true;\n-    }\n+  \/\/ Do the simplest check first:\n+  if (SafepointSynchronize::is_at_safepoint()) {\n+    \/\/ The target is protected since JavaThreads cannot exit\n+    \/\/ while we're at a safepoint.\n+    return true;\n+  }\n@@ -460,7 +449,13 @@\n-    \/\/ Now make the simple checks based on who the caller is:\n-    if (current_thread == p || Threads_lock->owner() == current_thread) {\n-      \/\/ Target JavaThread is self or calling thread owns the Threads_lock.\n-      \/\/ Second check is the same as Threads_lock->owner_is_self(),\n-      \/\/ but we already have the current thread so check directly.\n-      return true;\n-    }\n+  \/\/ If the target hasn't been started yet then it is trivially\n+  \/\/ \"protected\". We assume the caller is the thread that will do\n+  \/\/ the starting.\n+  if (target->osthread() == NULL || target->osthread()->get_state() <= INITIALIZED) {\n+    return true;\n+  }\n+\n+  \/\/ Now make the simple checks based on who the caller is:\n+  if (current_thread == target || Threads_lock->owner() == current_thread) {\n+    \/\/ Target JavaThread is self or calling thread owns the Threads_lock.\n+    \/\/ Second check is the same as Threads_lock->owner_is_self(),\n+    \/\/ but we already have the current thread so check directly.\n+    return true;\n@@ -469,0 +464,27 @@\n+  \/\/ Check the ThreadsLists associated with the calling thread (if any)\n+  \/\/ to see if one of them protects the target JavaThread:\n+  if (is_JavaThread_protected_by_TLH(target)) {\n+    return true;\n+  }\n+\n+  \/\/ Use this debug code with -XX:+UseNewCode to diagnose locations that\n+  \/\/ are missing a ThreadsListHandle or other protection mechanism:\n+  \/\/ guarantee(!UseNewCode, \"current_thread=\" INTPTR_FORMAT \" is not protecting target=\"\n+  \/\/           INTPTR_FORMAT, p2i(current_thread), p2i(target));\n+\n+  \/\/ Note: Since 'target' isn't protected by a TLH, the call to\n+  \/\/ target->is_handshake_safe_for() may crash, but we have debug bits so\n+  \/\/ we'll be able to figure out what protection mechanism is missing.\n+  assert(target->is_handshake_safe_for(current_thread), \"JavaThread=\" INTPTR_FORMAT\n+         \" is not protected and not handshake safe.\", p2i(target));\n+\n+  \/\/ The target JavaThread is not protected so it is not safe to query:\n+  return false;\n+}\n+\n+\/\/ Is the target JavaThread protected by a ThreadsListHandle (TLH) associated\n+\/\/ with the calling Thread?\n+\/\/\n+bool Thread::is_JavaThread_protected_by_TLH(const JavaThread* target) {\n+  Thread* current_thread = Thread::current();\n+\n@@ -473,1 +495,1 @@\n-    if (stlp->list()->includes(p)) {\n+    if (stlp->list()->includes(target)) {\n@@ -479,14 +501,1 @@\n-  if (!checkTLHOnly) {\n-    \/\/ Use this debug code with -XX:+UseNewCode to diagnose locations that\n-    \/\/ are missing a ThreadsListHandle or other protection mechanism:\n-    \/\/ guarantee(!UseNewCode, \"current_thread=\" INTPTR_FORMAT \" is not protecting p=\"\n-    \/\/           INTPTR_FORMAT, p2i(current_thread), p2i(p));\n-\n-    \/\/ Note: Since 'p' isn't protected by a TLH, the call to\n-    \/\/ p->is_handshake_safe_for() may crash, but we have debug bits so\n-    \/\/ we'll be able to figure out what protection mechanism is missing.\n-    assert(p->is_handshake_safe_for(current_thread), \"JavaThread=\" INTPTR_FORMAT\n-           \" is not protected and not handshake safe.\", p2i(p));\n-  }\n-\n-  \/\/ The target JavaThread is not protected so it is not safe to query:\n+  \/\/ The target JavaThread is not protected by a TLH so it is not safe to query:\n@@ -539,3 +548,0 @@\n-  if (active_handles() != NULL) {\n-    active_handles()->oops_do(f);\n-  }\n@@ -1006,0 +1012,2 @@\n+  _active_handles(NULL),\n+  _free_handle_block(NULL),\n@@ -1753,1 +1761,1 @@\n-  guarantee(Thread::is_JavaThread_protected(this, \/* checkTLHOnly *\/ true),\n+  guarantee(Thread::is_JavaThread_protected_by_TLH(\/* target *\/ this),\n@@ -1759,1 +1767,1 @@\n-  guarantee(Thread::is_JavaThread_protected(this, \/* checkTLHOnly *\/ true),\n+  guarantee(Thread::is_JavaThread_protected_by_TLH(\/* target *\/ this),\n@@ -1933,0 +1941,22 @@\n+\/\/ Push on a new block of JNI handles.\n+void JavaThread::push_jni_handle_block() {\n+  \/\/ Allocate a new block for JNI handles.\n+  \/\/ Inlined code from jni_PushLocalFrame()\n+  JNIHandleBlock* old_handles = active_handles();\n+  JNIHandleBlock* new_handles = JNIHandleBlock::allocate_block(this);\n+  assert(old_handles != NULL && new_handles != NULL, \"should not be NULL\");\n+  new_handles->set_pop_frame_link(old_handles);  \/\/ make sure java handles get gc'd.\n+  set_active_handles(new_handles);\n+}\n+\n+\/\/ Pop off the current block of JNI handles.\n+void JavaThread::pop_jni_handle_block() {\n+  \/\/ Release our JNI handle block\n+  JNIHandleBlock* old_handles = active_handles();\n+  JNIHandleBlock* new_handles = old_handles->pop_frame_link();\n+  assert(new_handles != nullptr, \"should never set active handles to null\");\n+  set_active_handles(new_handles);\n+  old_handles->set_pop_frame_link(NULL);\n+  JNIHandleBlock::release_block(old_handles, this);\n+}\n+\n@@ -1940,0 +1970,4 @@\n+  if (active_handles() != NULL) {\n+    active_handles()->oops_do(f);\n+  }\n+\n@@ -2140,1 +2174,1 @@\n-  if (Thread::is_JavaThread_protected(this)) {\n+  if (Thread::is_JavaThread_protected(\/* target *\/ this)) {\n@@ -2833,1 +2867,1 @@\n-    Thread* vmthread = VMThread::vm_thread();\n+    VMThread* vmthread = VMThread::vm_thread();\n@@ -2845,1 +2879,1 @@\n-      while (vmthread->active_handles() == NULL) {\n+      while (!vmthread->is_running()) {\n@@ -3272,2 +3306,2 @@\n-  if (DynamicDumpSharedSpaces) {\n-    DynamicArchive::prepare_for_dynamic_dumping();\n+  if (DynamicArchive::should_dump_at_vm_exit()) {\n+    DynamicArchive::prepare_for_dump_at_exit();\n","filename":"src\/hotspot\/share\/runtime\/thread.cpp","additions":85,"deletions":51,"binary":false,"changes":136,"status":"modified"},{"patch":"@@ -203,4 +203,5 @@\n-  \/\/ mechanism? If checkTLHOnly is true (default is false), then we only check\n-  \/\/ if the target JavaThread is protected by a ThreadsList (if any) associated\n-  \/\/ with the calling Thread.\n-  static bool is_JavaThread_protected(const JavaThread* p, bool checkTLHOnly = false);\n+  \/\/ mechanism?\n+  static bool is_JavaThread_protected(const JavaThread* target);\n+  \/\/ Is the target JavaThread protected by a ThreadsListHandle (TLH) associated\n+  \/\/ with the calling Thread?\n+  static bool is_JavaThread_protected_by_TLH(const JavaThread* target);\n@@ -241,6 +242,0 @@\n-  \/\/ Active_handles points to a block of handles\n-  JNIHandleBlock* _active_handles;\n-\n-  \/\/ One-element thread local free list\n-  JNIHandleBlock* _free_handle_block;\n-\n@@ -419,6 +414,0 @@\n-  \/\/ JNI handle support\n-  JNIHandleBlock* active_handles() const         { return _active_handles; }\n-  void set_active_handles(JNIHandleBlock* block) { _active_handles = block; }\n-  JNIHandleBlock* free_handle_block() const      { return _free_handle_block; }\n-  void set_free_handle_block(JNIHandleBlock* block) { _free_handle_block = block; }\n-\n@@ -610,1 +599,0 @@\n-  static ByteSize active_handles_offset()        { return byte_offset_of(Thread, _active_handles); }\n@@ -749,0 +737,7 @@\n+\n+  \/\/ Active_handles points to a block of handles\n+  JNIHandleBlock* _active_handles;\n+\n+  \/\/ One-element thread local free list\n+  JNIHandleBlock* _free_handle_block;\n+\n@@ -776,0 +771,9 @@\n+  \/\/ JNI handle support\n+  JNIHandleBlock* active_handles() const         { return _active_handles; }\n+  void set_active_handles(JNIHandleBlock* block) { _active_handles = block; }\n+  JNIHandleBlock* free_handle_block() const      { return _free_handle_block; }\n+  void set_free_handle_block(JNIHandleBlock* block) { _free_handle_block = block; }\n+\n+  void push_jni_handle_block();\n+  void pop_jni_handle_block();\n+\n@@ -1286,0 +1290,2 @@\n+  static ByteSize active_handles_offset()        { return byte_offset_of(JavaThread, _active_handles); }\n+\n@@ -1748,0 +1754,9 @@\n+class JNIHandleMark : public StackObj {\n+  JavaThread* _thread;\n+ public:\n+  JNIHandleMark(JavaThread* thread) : _thread(thread) {\n+    thread->push_jni_handle_block();\n+  }\n+  ~JNIHandleMark() { _thread->pop_jni_handle_block(); }\n+};\n+\n","filename":"src\/hotspot\/share\/runtime\/thread.hpp","additions":32,"deletions":17,"binary":false,"changes":49,"status":"modified"},{"patch":"@@ -707,1 +707,0 @@\n-  nonstatic_field(Thread,                      _active_handles,                               JNIHandleBlock*)                       \\\n@@ -731,0 +730,1 @@\n+  nonstatic_field(JavaThread,                  _active_handles,                               JNIHandleBlock*)                       \\\n@@ -827,1 +827,0 @@\n-  nonstatic_field(ciMethodData,                _current_mileage,                              int)                                   \\\n@@ -1853,0 +1852,4 @@\n+  declare_c2_type(MaskAllNode, VectorNode)                                \\\n+  declare_c2_type(AndVMaskNode, VectorNode)                               \\\n+  declare_c2_type(OrVMaskNode, VectorNode)                                \\\n+  declare_c2_type(XorVMaskNode, VectorNode)                               \\\n","filename":"src\/hotspot\/share\/runtime\/vmStructs.cpp","additions":5,"deletions":2,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -142,1 +142,1 @@\n-VMThread::VMThread() : NamedThread() {\n+VMThread::VMThread() : NamedThread(), _is_running(false) {\n@@ -155,1 +155,1 @@\n-  \/\/ Notify_lock wait checks on active_handles() to rewait in\n+  \/\/ Notify_lock wait checks on is_running() to rewait in\n@@ -158,1 +158,1 @@\n-  this->set_active_handles(JNIHandleBlock::allocate_block());\n+  Atomic::store(&_is_running, true);\n","filename":"src\/hotspot\/share\/runtime\/vmThread.cpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -28,0 +28,1 @@\n+#include \"runtime\/atomic.hpp\"\n@@ -62,0 +63,2 @@\n+  volatile bool _is_running;\n+\n@@ -87,0 +90,1 @@\n+  bool is_running() const { return Atomic::load(&_is_running); }\n","filename":"src\/hotspot\/share\/runtime\/vmThread.hpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -112,1 +112,1 @@\n-  set_footer_byte(_footer_canary_dead_mark);\n+  set_footer(_footer_canary_dead_mark);\n@@ -215,1 +215,1 @@\n-  if (get_footer_byte() != _footer_canary_life_mark) {\n+  if (get_footer() != _footer_canary_life_mark) {\n","filename":"src\/hotspot\/share\/services\/mallocTracker.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -246,1 +246,2 @@\n- * The user allocation is preceded by a header and followed by a single footer canary byte:\n+ * The user allocation is preceded by a header and is immediately followed by a (possibly unaligned)\n+ *  footer canary:\n@@ -252,1 +253,1 @@\n- *     16 bytes              user size                      1 byte\n+ *     16 bytes              user size                      2 byte\n@@ -256,6 +257,3 @@\n- * The start of the user allocation needs to adhere to malloc alignment. We assume 2 words\n- * (16 bytes\/8 bytes on 64-bit\/32-bit) to be enough for that (todo: it may not be enough on 32bit).\n- *\n- * That dictates a minimum alignment of the size of a malloc header of 2 words. However, since\n- * 8 bytes are not enough to hold all information we need, we have to enlarge the malloc header on\n- * 32-bit. So the malloc header is 16 bytes long on both 32-bit and 64-bit.\n+ * The start of the user allocation needs to adhere to malloc alignment. We assume 128 bits\n+ * on both 64-bit\/32-bit to be enough for that. So the malloc header is 16 bytes long on both\n+ * 32-bit and 64-bit.\n@@ -292,2 +290,2 @@\n- * - The footer canary is just one byte since this is still enough to catch overflows, and it\n- *   allows us to not care about aligned accesses.\n+ * - The footer canary consists of two bytes. Since the footer location may be unaligned to 16 bits,\n+ *   the bytes are stored individually.\n@@ -309,6 +307,6 @@\n-  static const uint16_t _header_canary_life_mark = 0xFA1F;\n-  static const uint16_t _header_canary_dead_mark = 0xFB1F;\n-  static const uint8_t  _footer_canary_life_mark = 0xFA;\n-  static const uint8_t  _footer_canary_dead_mark = 0xFB;\n-  NOT_LP64(static const uint32_t _header_alt_canary_life_mark = 0xFAFA1F1F;)\n-  NOT_LP64(static const uint32_t _header_alt_canary_dead_mark = 0xFBFB1F1F;)\n+  static const uint16_t _header_canary_life_mark = 0xE99E;\n+  static const uint16_t _header_canary_dead_mark = 0xD99D;\n+  static const uint16_t _footer_canary_life_mark = 0xE88E;\n+  static const uint16_t _footer_canary_dead_mark = 0xD88D;\n+  NOT_LP64(static const uint32_t _header_alt_canary_life_mark = 0xE99EE99E;)\n+  NOT_LP64(static const uint32_t _header_alt_canary_dead_mark = 0xD88DD88D;)\n@@ -326,3 +324,5 @@\n-  uint8_t* footer_address() const { return ((address)this) + sizeof(MallocHeader) + _size; }\n-  uint8_t get_footer_byte() const { return *footer_address(); }\n-  void set_footer_byte(uint8_t b) { (*footer_address()) = b; }\n+  static uint16_t build_footer(uint8_t b1, uint8_t b2) { return ((uint16_t)b1 << 8) | (uint16_t)b2; }\n+\n+  uint8_t* footer_address() const   { return ((address)this) + sizeof(MallocHeader) + _size; }\n+  uint16_t get_footer() const       { return build_footer(footer_address()[0], footer_address()[1]); }\n+  void set_footer(uint16_t v)       { footer_address()[0] = v >> 8; footer_address()[1] = (uint8_t)v; }\n@@ -358,1 +358,1 @@\n-    set_footer_byte(_footer_canary_life_mark); \/\/ set after initializing _size\n+    set_footer(_footer_canary_life_mark); \/\/ set after initializing _size\n@@ -398,1 +398,1 @@\n-    return (level == NMT_off) ? 0 : 1;\n+    return (level == NMT_off) ? 0 : sizeof(uint16_t);\n@@ -435,5 +435,0 @@\n-  \/\/ Get header size\n-  static inline size_t get_header_size(void* memblock) {\n-    return (memblock == NULL) ? 0 : sizeof(MallocHeader);\n-  }\n-\n","filename":"src\/hotspot\/share\/services\/mallocTracker.hpp","additions":21,"deletions":26,"binary":false,"changes":47,"status":"modified"},{"patch":"@@ -2018,1 +2018,1 @@\n-          jstring command, dcmdArgInfo* infoArray))\n+          jstring command, dcmdArgInfo* infoArray, jint count))\n@@ -2042,2 +2042,4 @@\n-  if (array->length() == 0) {\n-    return;\n+  const int num_args = array->length();\n+  if (num_args != count) {\n+    assert(false, \"jmm_GetDiagnosticCommandArgumentsInfo count mismatch (%d vs %d)\", count, num_args);\n+    THROW_MSG(vmSymbols::java_lang_InternalError(), \"jmm_GetDiagnosticCommandArgumentsInfo count mismatch\");\n@@ -2045,1 +2047,1 @@\n-  for (int i = 0; i < array->length(); i++) {\n+  for (int i = 0; i < num_args; i++) {\n","filename":"src\/hotspot\/share\/services\/management.cpp","additions":6,"deletions":4,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -166,7 +166,0 @@\n-  static size_t malloc_header_size(void* memblock) {\n-    if (tracking_level() != NMT_off) {\n-      return MallocTracker::get_header_size(memblock);\n-    }\n-    return 0;\n-  }\n-\n","filename":"src\/hotspot\/share\/services\/memTracker.hpp","additions":0,"deletions":7,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -72,0 +72,1 @@\n+  JVM_ACC_IS_BEING_REDEFINED      = 0x00100000,     \/\/ True if the klass is being redefined.\n@@ -162,0 +163,4 @@\n+  bool is_being_redefined() const       { return (_flags & JVM_ACC_IS_BEING_REDEFINED) != 0; }\n+  void set_is_being_redefined()         { atomic_set_bits(JVM_ACC_IS_BEING_REDEFINED); }\n+  void clear_is_being_redefined()       { atomic_clear_bits(JVM_ACC_IS_BEING_REDEFINED); }\n+\n","filename":"src\/hotspot\/share\/utilities\/accessFlags.hpp","additions":5,"deletions":0,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -323,0 +323,11 @@\n+  void truncate_to(int idx) {\n+    for (int i = 0, j = idx; j < length(); i++, j++) {\n+      at_put(i, at(j));\n+    }\n+    trunc_to(length() - idx);\n+  }\n+\n+  void truncate_from(int idx) {\n+    trunc_to(idx);\n+  }\n+\n","filename":"src\/hotspot\/share\/utilities\/growableArray.hpp","additions":11,"deletions":0,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -526,0 +526,2 @@\n+     * The filter is created as if {@link #createFilter(String) createFilter} is called;\n+     * if the filter string is invalid, an {@link ExceptionInInitializerError} is thrown.\n","filename":"src\/java.base\/share\/classes\/java\/io\/ObjectInputFilter.java","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -102,1 +102,3 @@\n-     * Creates an OutputStreamWriter that uses the default character encoding.\n+     * Creates an OutputStreamWriter that uses the default character encoding, or\n+     * where {@code out} is a {@code PrintStream}, the charset used by the print\n+     * stream.\n@@ -110,1 +112,1 @@\n-                Charset.defaultCharset());\n+                out instanceof PrintStream ps ? ps.charset() : Charset.defaultCharset());\n","filename":"src\/java.base\/share\/classes\/java\/io\/OutputStreamWriter.java","additions":4,"deletions":2,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -71,0 +71,1 @@\n+    private final Charset charset;\n@@ -111,1 +112,2 @@\n-        this.charOut = new OutputStreamWriter(this);\n+        this.charset = out instanceof PrintStream ps ? ps.charset() : Charset.defaultCharset();\n+        this.charOut = new OutputStreamWriter(this, charset);\n@@ -127,1 +129,2 @@\n-     * to bytes using the default charset.\n+     * to bytes using the default charset, or where {@code out} is a\n+     * {@code PrintStream}, the charset used by the print stream.\n@@ -142,1 +145,2 @@\n-     * the default charset.\n+     * the default charset, or where {@code out} is a {@code PrintStream},\n+     * the charset used by the print stream.\n@@ -204,0 +208,1 @@\n+        this.charset = charset;\n@@ -1377,0 +1382,8 @@\n+    \/**\n+     * {@return the charset used in this {@code PrintStream} instance}\n+     *\n+     * @since 18\n+     *\/\n+    public Charset charset() {\n+        return charset;\n+    }\n","filename":"src\/java.base\/share\/classes\/java\/io\/PrintStream.java","additions":16,"deletions":3,"binary":false,"changes":19,"status":"modified"},{"patch":"@@ -121,1 +121,2 @@\n-     * into bytes using the default charset.\n+     * into bytes using the default charset, or where {@code out} is a\n+     * {@code PrintStream}, the charset used by the print stream.\n@@ -135,2 +136,3 @@\n-     * OutputStreamWriter, which will convert characters into bytes using the\n-     * default charset.\n+     * OutputStreamWriter, which will convert characters into bytes using\n+     * the default charset, or where {@code out} is a {@code PrintStream},\n+     * the charset used by the print stream.\n@@ -147,1 +149,1 @@\n-        this(out, autoFlush, Charset.defaultCharset());\n+        this(out, autoFlush, out instanceof PrintStream ps ? ps.charset() : Charset.defaultCharset());\n","filename":"src\/java.base\/share\/classes\/java\/io\/PrintWriter.java","additions":6,"deletions":4,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1994, 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1994, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -101,1 +101,1 @@\n-            in = (InputStream) e.nextElement();\n+            in = e.nextElement();\n","filename":"src\/java.base\/share\/classes\/java\/io\/SequenceInputStream.java","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2010, 2013, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2010, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -688,1 +688,1 @@\n-                    cache[replacementPos & mask] = (Entry<?>) Entry.DEAD_ENTRY;\n+                    cache[replacementPos & mask] = Entry.DEAD_ENTRY;\n","filename":"src\/java.base\/share\/classes\/java\/lang\/ClassValue.java","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2003, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2003, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -196,1 +196,1 @@\n-        Enum<?> other = (Enum<?>)o;\n+        Enum<?> other = o;\n","filename":"src\/java.base\/share\/classes\/java\/lang\/Enum.java","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -381,0 +381,10 @@\n+ * <tr>\n+ *   <th scope=\"row\">inetAddressResolverProvider<\/th>\n+ *   <td>This {@code RuntimePermission} is required to be granted to\n+ *   classes which subclass and implement {@code java.net.spi.InetAddressResolverProvider}.\n+ *   The permission is checked during invocation of the abstract base class constructor.\n+ *   This permission ensures trust in classes which provide resolvers used by\n+ *   {@link java.net.InetAddress} hostname and address resolution methods.<\/td>\n+ *   <td>See {@link java.net.spi.InetAddressResolverProvider} for more information.<\/td>\n+ * <\/tr>\n+ *\n","filename":"src\/java.base\/share\/classes\/java\/lang\/RuntimePermission.java","additions":11,"deletions":1,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2000, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2000, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -446,1 +446,1 @@\n-            Class<?> cls = (Class<?>) declaringClassObject;\n+            Class<?> cls = declaringClassObject;\n","filename":"src\/java.base\/share\/classes\/java\/lang\/StackTraceElement.java","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -36,1 +36,0 @@\n-import java.io.UnsupportedEncodingException;\n@@ -48,1 +47,0 @@\n-import java.nio.charset.CharacterCodingException;\n@@ -51,0 +49,1 @@\n+import java.nio.charset.CharacterCodingException;\n@@ -87,0 +86,1 @@\n+import sun.nio.cs.UTF_8;\n@@ -191,0 +191,5 @@\n+    \/\/ `sun.jnu.encoding` if it is not supported. Otherwise null.\n+    \/\/ It is initialized in `initPhase1()` before any charset providers\n+    \/\/ are initialized.\n+    private static String notSupportedJnuEncoding;\n+\n@@ -2020,4 +2025,3 @@\n-       if (enc != null) {\n-            try {\n-                return new PrintStream(new BufferedOutputStream(fos, 128), true, enc);\n-            } catch (UnsupportedEncodingException uee) {}\n+        if (enc != null) {\n+            return new PrintStream(new BufferedOutputStream(fos, 128), true,\n+                                   Charset.forName(enc, UTF_8.INSTANCE));\n@@ -2116,0 +2120,7 @@\n+        \/\/ Check if sun.jnu.encoding is supported. If not, replace it with UTF-8.\n+        var jnuEncoding = props.getProperty(\"sun.jnu.encoding\");\n+        if (jnuEncoding == null || !Charset.isSupported(jnuEncoding)) {\n+            notSupportedJnuEncoding = jnuEncoding == null ? \"null\" : jnuEncoding;\n+            props.setProperty(\"sun.jnu.encoding\", \"UTF-8\");\n+        }\n+\n@@ -2144,1 +2155,0 @@\n-\n@@ -2251,0 +2261,8 @@\n+        \/\/ Emit a warning if `sun.jnu.encoding` is not supported.\n+        if (notSupportedJnuEncoding != null) {\n+            System.err.println(\n+                    \"WARNING: The encoding of the underlying platform's\" +\n+                    \" file system is not supported: \" +\n+                    notSupportedJnuEncoding);\n+        }\n+\n","filename":"src\/java.base\/share\/classes\/java\/lang\/System.java","additions":25,"deletions":7,"binary":false,"changes":32,"status":"modified"},{"patch":"@@ -27,0 +27,3 @@\n+import java.net.spi.InetAddressResolver.LookupPolicy;\n+\n+import static java.net.spi.InetAddressResolver.LookupPolicy.IPV4;\n@@ -35,2 +38,8 @@\n-    public native InetAddress[]\n-        lookupAllHostAddr(String hostname) throws UnknownHostException;\n+    public InetAddress[] lookupAllHostAddr(String hostname, LookupPolicy lookupPolicy)\n+            throws UnknownHostException {\n+        if ((lookupPolicy.characteristics() & IPV4) == 0) {\n+            throw new UnknownHostException(hostname);\n+        }\n+        return lookupAllHostAddr(hostname);\n+    }\n+    private native InetAddress[] lookupAllHostAddr(String hostname) throws UnknownHostException;\n","filename":"src\/java.base\/share\/classes\/java\/net\/Inet4AddressImpl.java","additions":11,"deletions":2,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -28,0 +28,1 @@\n+import java.net.spi.InetAddressResolver.LookupPolicy;\n@@ -29,3 +30,1 @@\n-import static java.net.InetAddress.IPv6;\n-import static java.net.InetAddress.PREFER_IPV6_VALUE;\n-import static java.net.InetAddress.PREFER_SYSTEM_VALUE;\n+import static java.net.InetAddress.PLATFORM_LOOKUP_POLICY;\n@@ -51,2 +50,7 @@\n-    public native InetAddress[] lookupAllHostAddr(String hostname)\n-        throws UnknownHostException;\n+    public InetAddress[] lookupAllHostAddr(String hostname, LookupPolicy lookupPolicy)\n+            throws UnknownHostException {\n+        return lookupAllHostAddr(hostname, lookupPolicy.characteristics());\n+    }\n+\n+    private native InetAddress[] lookupAllHostAddr(String hostname, int characteristics)\n+            throws UnknownHostException;\n@@ -99,2 +103,3 @@\n-            if (InetAddress.preferIPv6Address == PREFER_IPV6_VALUE ||\n-                InetAddress.preferIPv6Address == PREFER_SYSTEM_VALUE) {\n+            int flags = PLATFORM_LOOKUP_POLICY.characteristics();\n+            if (InetAddress.ipv6AddressesFirst(flags) ||\n+                InetAddress.systemAddressesOrder(flags)) {\n@@ -112,3 +117,3 @@\n-            boolean preferIPv6Address =\n-                InetAddress.preferIPv6Address == PREFER_IPV6_VALUE ||\n-                InetAddress.preferIPv6Address == PREFER_SYSTEM_VALUE;\n+            int flags = PLATFORM_LOOKUP_POLICY.characteristics();\n+            boolean preferIPv6Address = InetAddress.ipv6AddressesFirst(flags) ||\n+                    InetAddress.systemAddressesOrder(flags);\n","filename":"src\/java.base\/share\/classes\/java\/net\/Inet6AddressImpl.java","additions":15,"deletions":10,"binary":false,"changes":25,"status":"modified"},{"patch":"@@ -28,0 +28,5 @@\n+import java.net.spi.InetAddressResolver;\n+import java.net.spi.InetAddressResolverProvider;\n+import java.net.spi.InetAddressResolver.LookupPolicy;\n+import java.security.AccessController;\n+import java.security.PrivilegedAction;\n@@ -43,0 +48,1 @@\n+import java.util.ServiceLoader;\n@@ -48,0 +54,4 @@\n+import java.util.concurrent.locks.ReentrantLock;\n+import java.util.stream.Stream;\n+\n+import jdk.internal.misc.VM;\n@@ -51,0 +61,2 @@\n+import jdk.internal.vm.annotation.Stable;\n+import sun.net.ResolverProviderConfiguration;\n@@ -56,0 +68,5 @@\n+import static java.net.spi.InetAddressResolver.LookupPolicy.IPV4;\n+import static java.net.spi.InetAddressResolver.LookupPolicy.IPV4_FIRST;\n+import static java.net.spi.InetAddressResolver.LookupPolicy.IPV6;\n+import static java.net.spi.InetAddressResolver.LookupPolicy.IPV6_FIRST;\n+\n@@ -131,2 +148,2 @@\n- * <P>There is a <a href=\"doc-files\/net-properties.html#Ipv4IPv6\">couple of\n- * System Properties<\/a> affecting how IPv4 and IPv6 addresses are used.<\/P>\n+ * <p> There is a <a href=\"doc-files\/net-properties.html#Ipv4IPv6\">couple of\n+ * System Properties<\/a> affecting how IPv4 and IPv6 addresses are used.\n@@ -134,1 +151,5 @@\n- * <h3> Host Name Resolution <\/h3>\n+ * <h2 id=\"host-name-resolution\"> Host Name Resolution <\/h2>\n+ *\n+ * <p> The InetAddress class provides methods to resolve host names to\n+ * their IP addresses and vice versa. The actual resolution is delegated to an\n+ * {@linkplain InetAddressResolver InetAddress resolver}.\n@@ -136,6 +157,2 @@\n- * Host name-to-IP address <i>resolution<\/i> is accomplished through\n- * the use of a combination of local machine configuration information\n- * and network naming services such as the Domain Name System (DNS)\n- * and Network Information Service(NIS). The particular naming\n- * services(s) being used is by default the local machine configured\n- * one. For any host name, its corresponding IP address is returned.\n+ * <p> <i>Host name-to-IP address resolution<\/i> maps a host name to an IP address.\n+ * For any host name, its corresponding IP address is returned.\n@@ -146,2 +163,12 @@\n- * <p> The InetAddress class provides methods to resolve host names to\n- * their IP addresses and vice versa.\n+ * <p id=\"built-in-resolver\"> The built-in InetAddress resolver implementation does\n+ * host name-to-IP address resolution and vice versa through the use of\n+ * a combination of local machine configuration information and network\n+ * naming services such as the Domain Name System (DNS) and the Lightweight Directory\n+ * Access Protocol (LDAP).\n+ * The particular naming services that the built-in resolver uses by default\n+ * depends on the configuration of the local machine.\n+ *\n+ * <p> {@code InetAddress} has a service provider mechanism for InetAddress resolvers\n+ * that allows a custom InetAddress resolver to be used instead of the built-in implementation.\n+ * {@link InetAddressResolverProvider} is the service provider class. Its API docs provide all the\n+ * details on this mechanism.\n@@ -149,1 +176,1 @@\n- * <h3> InetAddress Caching <\/h3>\n+ * <h2> InetAddress Caching <\/h2>\n@@ -201,4 +228,0 @@\n-    @Native static final int PREFER_IPV4_VALUE = 0;\n-    @Native static final int PREFER_IPV6_VALUE = 1;\n-    @Native static final int PREFER_SYSTEM_VALUE = 2;\n-\n@@ -217,3 +240,0 @@\n-    \/* Specify address family preference *\/\n-    static final transient int preferIPv6Address;\n-\n@@ -291,2 +311,5 @@\n-    \/* Used to store the name service provider *\/\n-    private static transient NameService nameService;\n+    \/* Used to store the system-wide resolver *\/\n+    @Stable\n+    private static volatile InetAddressResolver resolver;\n+\n+    private static final InetAddressResolver BUILTIN_RESOLVER;\n@@ -304,0 +327,9 @@\n+    \/\/ \"java.net.preferIPv4Stack\" system property value\n+    private static final String PREFER_IPV4_STACK_VALUE;\n+\n+    \/\/ \"java.net.preferIPv6Addresses\" system property value\n+    private static final String PREFER_IPV6_ADDRESSES_VALUE;\n+\n+    \/\/ \"jdk.net.hosts.file\" system property value\n+    private static final String HOSTS_FILE_NAME;\n+\n@@ -308,12 +340,6 @@\n-        String str = GetPropertyAction.privilegedGetProperty(\"java.net.preferIPv6Addresses\");\n-        if (str == null) {\n-            preferIPv6Address = PREFER_IPV4_VALUE;\n-        } else if (str.equalsIgnoreCase(\"true\")) {\n-            preferIPv6Address = PREFER_IPV6_VALUE;\n-        } else if (str.equalsIgnoreCase(\"false\")) {\n-            preferIPv6Address = PREFER_IPV4_VALUE;\n-        } else if (str.equalsIgnoreCase(\"system\")) {\n-            preferIPv6Address = PREFER_SYSTEM_VALUE;\n-        } else {\n-            preferIPv6Address = PREFER_IPV4_VALUE;\n-        }\n+        PREFER_IPV4_STACK_VALUE =\n+                GetPropertyAction.privilegedGetProperty(\"java.net.preferIPv4Stack\");\n+        PREFER_IPV6_ADDRESSES_VALUE =\n+                GetPropertyAction.privilegedGetProperty(\"java.net.preferIPv6Addresses\");\n+        HOSTS_FILE_NAME =\n+                GetPropertyAction.privilegedGetProperty(\"jdk.net.hosts.file\");\n@@ -327,7 +353,0 @@\n-                    public InetAddress getByName(String hostName,\n-                                                 InetAddress hostAddress)\n-                        throws UnknownHostException\n-                    {\n-                        return InetAddress.getByName(hostName, hostAddress);\n-                    }\n-\n@@ -346,0 +365,125 @@\n+    \/**\n+     * Creates an address lookup policy from {@code \"java.net.preferIPv4Stack\"},\n+     * {@code \"java.net.preferIPv6Addresses\"} system property values, and O\/S configuration.\n+     *\/\n+    private static final LookupPolicy initializePlatformLookupPolicy() {\n+        \/\/ Calculate AddressFamily value first\n+        boolean ipv4Available = isIPv4Available();\n+        if (\"true\".equals(PREFER_IPV4_STACK_VALUE) && ipv4Available) {\n+            return LookupPolicy.of(IPV4);\n+        }\n+        \/\/ Check if IPv6 is not supported\n+        if (InetAddress.impl instanceof Inet4AddressImpl) {\n+            return LookupPolicy.of(IPV4);\n+        }\n+        \/\/ Check if system supports IPv4, if not use IPv6\n+        if (!ipv4Available) {\n+            return LookupPolicy.of(IPV6);\n+        }\n+        \/\/ If both address families are needed - check preferIPv6Addresses value\n+        if (PREFER_IPV6_ADDRESSES_VALUE != null) {\n+            if (PREFER_IPV6_ADDRESSES_VALUE.equalsIgnoreCase(\"true\")) {\n+                return LookupPolicy.of(IPV4 | IPV6 | IPV6_FIRST);\n+            }\n+            if (PREFER_IPV6_ADDRESSES_VALUE.equalsIgnoreCase(\"false\")) {\n+                return LookupPolicy.of(IPV4 | IPV6 | IPV4_FIRST);\n+            }\n+            if (PREFER_IPV6_ADDRESSES_VALUE.equalsIgnoreCase(\"system\")) {\n+                return LookupPolicy.of(IPV4 | IPV6);\n+            }\n+        }\n+        \/\/ Default value with both address families needed - IPv4 addresses come first\n+        return LookupPolicy.of(IPV4 | IPV6 | IPV4_FIRST);\n+    }\n+\n+    static boolean systemAddressesOrder(int lookupCharacteristics) {\n+        return (lookupCharacteristics & (IPV4_FIRST | IPV6_FIRST)) == 0;\n+    }\n+\n+    static boolean ipv4AddressesFirst(int lookupCharacteristics) {\n+        return (lookupCharacteristics & IPV4_FIRST) != 0;\n+    }\n+\n+    static boolean ipv6AddressesFirst(int lookupCharacteristics) {\n+        return (lookupCharacteristics & IPV6_FIRST) != 0;\n+    }\n+\n+    \/\/ Native method to check if IPv4 is available\n+    private static native boolean isIPv4Available();\n+\n+    \/**\n+     * The {@code RuntimePermission(\"inetAddressResolverProvider\")} is\n+     * necessary to subclass and instantiate the {@code InetAddressResolverProvider}\n+     * class, as well as to obtain resolver from an instance of that class,\n+     * and it is also required to obtain the operating system name resolution configurations.\n+     *\/\n+    private static final RuntimePermission INET_ADDRESS_RESOLVER_PERMISSION =\n+            new RuntimePermission(\"inetAddressResolverProvider\");\n+\n+    private static final ReentrantLock RESOLVER_LOCK = new ReentrantLock();\n+    private static volatile InetAddressResolver bootstrapResolver;\n+\n+    @SuppressWarnings(\"removal\")\n+    private static InetAddressResolver resolver() {\n+        InetAddressResolver cns = resolver;\n+        if (cns != null) {\n+            return cns;\n+        }\n+        if (VM.isBooted()) {\n+            RESOLVER_LOCK.lock();\n+            boolean bootstrapSet = false;\n+            try {\n+                cns = resolver;\n+                if (cns != null) {\n+                    return cns;\n+                }\n+                \/\/ Protection against provider calling InetAddress APIs during initialization\n+                if (bootstrapResolver != null) {\n+                    return bootstrapResolver;\n+                }\n+                bootstrapResolver = BUILTIN_RESOLVER;\n+                bootstrapSet = true;\n+\n+                if (HOSTS_FILE_NAME != null) {\n+                    \/\/ The default resolver service is already host file resolver\n+                    cns = BUILTIN_RESOLVER;\n+                } else if (System.getSecurityManager() != null) {\n+                    PrivilegedAction<InetAddressResolver> pa = InetAddress::loadResolver;\n+                    cns = AccessController.doPrivileged(\n+                            pa, null, INET_ADDRESS_RESOLVER_PERMISSION);\n+                } else {\n+                    cns = loadResolver();\n+                }\n+\n+                InetAddress.resolver = cns;\n+                return cns;\n+            } finally {\n+                \/\/ We want to clear bootstrap resolver reference only after an attempt to\n+                \/\/ instantiate a resolver has been completed.\n+                if (bootstrapSet) {\n+                    bootstrapResolver = null;\n+                }\n+                RESOLVER_LOCK.unlock();\n+            }\n+        } else {\n+            return BUILTIN_RESOLVER;\n+        }\n+    }\n+\n+    private static InetAddressResolver loadResolver() {\n+        return ServiceLoader.load(InetAddressResolverProvider.class)\n+                .findFirst()\n+                .map(nsp -> nsp.get(builtinConfiguration()))\n+                .orElse(BUILTIN_RESOLVER);\n+    }\n+\n+    private static InetAddressResolverProvider.Configuration builtinConfiguration() {\n+        return new ResolverProviderConfiguration(BUILTIN_RESOLVER, () -> {\n+            try {\n+                return impl.getLocalHostName();\n+            } catch (UnknownHostException unknownHostException) {\n+                return \"localhost\";\n+            }\n+        });\n+    }\n+\n@@ -558,1 +702,1 @@\n-     * configured name lookup service. If a lookup of the name service\n+     * configured resolver. If a lookup of the name service\n@@ -659,1 +803,2 @@\n-        String host = null;\n+        String host;\n+        var resolver = resolver();\n@@ -662,1 +807,1 @@\n-            host = nameService.getHostByAddr(addr.getAddress());\n+            host = resolver.lookupByAddress(addr.getAddress());\n@@ -694,1 +839,5 @@\n-        } catch (SecurityException e) {\n+        } catch (RuntimeException | UnknownHostException e) {\n+            \/\/ 'resolver.lookupByAddress' and 'InetAddress.getAllByName0' delegate to\n+            \/\/ the system-wide resolver, which could be a custom one. At that point we\n+            \/\/ treat any unexpected RuntimeException thrown by the resolver as we would\n+            \/\/ treat an UnknownHostException or an unmatched host name.\n@@ -696,3 +845,0 @@\n-        } catch (UnknownHostException e) {\n-            host = addr.getHostAddress();\n-            \/\/ let next provider resolve the hostname\n@@ -758,2 +904,3 @@\n-     * If the host name is unresolved, no reverse name service lookup\n-     * is performed. The hostname part will be represented by an empty string.\n+     * If the host name is unresolved, no reverse lookup\n+     * is performed. The hostname part will be represented\n+     * by an empty string.\n@@ -824,1 +971,0 @@\n-        private final InetAddress reqAddr;\n@@ -826,1 +972,1 @@\n-        NameServiceAddresses(String host, InetAddress reqAddr) {\n+        NameServiceAddresses(String host) {\n@@ -828,1 +974,0 @@\n-            this.reqAddr = reqAddr;\n@@ -852,1 +997,1 @@\n-                        inetAddresses = getAddressesFromNameService(host, reqAddr);\n+                        inetAddresses = getAddressesFromNameService(host);\n@@ -878,1 +1023,1 @@\n-                    if (inetAddresses == null) {\n+                    if (inetAddresses == null || inetAddresses.length == 0) {\n@@ -892,32 +1037,1 @@\n-     * NameService provides host and address lookup service\n-     *\n-     * @since 9\n-     *\/\n-    private interface NameService {\n-\n-        \/**\n-         * Lookup a host mapping by name. Retrieve the IP addresses\n-         * associated with a host\n-         *\n-         * @param host the specified hostname\n-         * @return array of IP addresses for the requested host\n-         * @throws UnknownHostException\n-         *             if no IP address for the {@code host} could be found\n-         *\/\n-        InetAddress[] lookupAllHostAddr(String host)\n-                throws UnknownHostException;\n-\n-        \/**\n-         * Lookup the host corresponding to the IP address provided\n-         *\n-         * @param addr byte array representing an IP address\n-         * @return {@code String} representing the host name mapping\n-         * @throws UnknownHostException\n-         *             if no host found for the specified IP address\n-         *\/\n-        String getHostByAddr(byte[] addr) throws UnknownHostException;\n-\n-    }\n-\n-    \/**\n-     * The default NameService implementation, which delegates to the underlying\n+     * The default InetAddressResolver implementation, which delegates to the underlying\n@@ -928,1 +1042,1 @@\n-    private static final class PlatformNameService implements NameService {\n+    private static final class PlatformResolver implements InetAddressResolver {\n@@ -930,4 +1044,5 @@\n-        public InetAddress[] lookupAllHostAddr(String host)\n-            throws UnknownHostException\n-        {\n-            return impl.lookupAllHostAddr(host);\n+        public Stream<InetAddress> lookupByName(String host, LookupPolicy policy)\n+                throws UnknownHostException {\n+            Objects.requireNonNull(host);\n+            Objects.requireNonNull(policy);\n+            return Arrays.stream(impl.lookupAllHostAddr(host, policy));\n@@ -936,3 +1051,6 @@\n-        public String getHostByAddr(byte[] addr)\n-            throws UnknownHostException\n-        {\n+        public String lookupByAddress(byte[] addr)\n+                throws UnknownHostException {\n+            Objects.requireNonNull(addr);\n+            if (addr.length != Inet4Address.INADDRSZ && addr.length != Inet6Address.INADDRSZ) {\n+                throw new IllegalArgumentException(\"Invalid address length\");\n+            }\n@@ -944,1 +1062,1 @@\n-     * The HostsFileNameService provides host address mapping\n+     * The HostsFileResolver provides host address mapping\n@@ -951,1 +1069,1 @@\n-     * <p>When the file lookup is enabled it replaces the default NameService\n+     * <p>When the file lookup is enabled it replaces the default InetAddressResolver\n@@ -956,7 +1074,1 @@\n-    private static final class HostsFileNameService implements NameService {\n-\n-        private static final InetAddress[] EMPTY_ARRAY = new InetAddress[0];\n-\n-        \/\/ Specify if only IPv4 addresses should be returned by HostsFileService implementation\n-        private static final boolean preferIPv4Stack = Boolean.parseBoolean(\n-                GetPropertyAction.privilegedGetProperty(\"java.net.preferIPv4Stack\"));\n+    private static final class HostsFileResolver implements InetAddressResolver {\n@@ -966,1 +1078,1 @@\n-        public HostsFileNameService(String hostsFileName) {\n+        public HostsFileResolver(String hostsFileName) {\n@@ -977,2 +1089,3 @@\n-         * @throws UnknownHostException\n-         *             if no host found for the specified IP address\n+         * @throws UnknownHostException if no host found for the specified IP address\n+         * @throws IllegalArgumentException if IP address is of illegal length\n+         * @throws NullPointerException     if addr is {@code null}\n@@ -981,1 +1094,1 @@\n-        public String getHostByAddr(byte[] addr) throws UnknownHostException {\n+        public String lookupByAddress(byte[] addr) throws UnknownHostException {\n@@ -984,0 +1097,5 @@\n+            Objects.requireNonNull(addr);\n+            \/\/ Check the length of the address array\n+            if (addr.length != Inet4Address.INADDRSZ && addr.length != Inet6Address.INADDRSZ) {\n+                throw new IllegalArgumentException(\"Invalid address length\");\n+            }\n@@ -986,2 +1104,1 @@\n-                                                        UTF_8.INSTANCE))\n-            {\n+                                                        UTF_8.INSTANCE)) {\n@@ -1023,1 +1140,4 @@\n-         * @return array of IP addresses for the requested host\n+         * @param lookupPolicy IP addresses lookup policy which specifies addresses\n+         *                     family and their order\n+         * @return stream of IP addresses for the requested host\n+         * @throws NullPointerException if either parameter is {@code null}\n@@ -1027,1 +1147,1 @@\n-        public InetAddress[] lookupAllHostAddr(String host)\n+        public Stream<InetAddress> lookupByName(String host, LookupPolicy lookupPolicy)\n@@ -1032,0 +1152,3 @@\n+\n+            Objects.requireNonNull(host);\n+            Objects.requireNonNull(lookupPolicy);\n@@ -1035,0 +1158,3 @@\n+            int flags = lookupPolicy.characteristics();\n+            boolean needIPv4 = (flags & IPv4) != 0;\n+            boolean needIPv6 = (flags & IPv6) != 0;\n@@ -1038,1 +1164,1 @@\n-                                                        UTF_8.INSTANCE)) {\n+                    UTF_8.INSTANCE)) {\n@@ -1050,1 +1176,1 @@\n-                                    if (address instanceof Inet4Address) {\n+                                    if (address instanceof Inet4Address && needIPv4) {\n@@ -1053,1 +1179,1 @@\n-                                    if (address instanceof Inet6Address) {\n+                                    if (address instanceof Inet6Address && needIPv6) {\n@@ -1065,13 +1191,4 @@\n-\n-            List<InetAddress> res;\n-            \/\/ If \"preferIPv4Stack\" system property is set to \"true\" then return\n-            \/\/ only IPv4 addresses\n-            if (preferIPv4Stack) {\n-                res = inet4Addresses;\n-            } else {\n-                \/\/ Otherwise, analyse \"preferIPv6Addresses\" value\n-                res = switch (preferIPv6Address) {\n-                    case PREFER_IPV4_VALUE -> concatAddresses(inet4Addresses, inet6Addresses);\n-                    case PREFER_IPV6_VALUE -> concatAddresses(inet6Addresses, inet4Addresses);\n-                    default -> inetAddresses;\n-                };\n+            \/\/ Check if only IPv4 addresses are requested\n+            if (needIPv4 && !needIPv6) {\n+                checkResultsList(inet4Addresses, host);\n+                return inet4Addresses.stream();\n@@ -1079,4 +1196,4 @@\n-\n-            if (res.isEmpty()) {\n-                throw new UnknownHostException(\"Unable to resolve host \" + host\n-                        + \" in hosts file \" + hostsFile);\n+            \/\/ Check if only IPv6 addresses are requested\n+            if (!needIPv4 && needIPv6) {\n+                checkResultsList(inet6Addresses, host);\n+                return inet6Addresses.stream();\n@@ -1084,1 +1201,12 @@\n-            return res.toArray(EMPTY_ARRAY);\n+            \/\/ If both type of addresses are requested:\n+            \/\/ First, check if there is any results. Then arrange\n+            \/\/ addresses according to LookupPolicy value.\n+            checkResultsList(inetAddresses, host);\n+            if (ipv6AddressesFirst(flags)) {\n+                return Stream.concat(inet6Addresses.stream(), inet4Addresses.stream());\n+            } else if (ipv4AddressesFirst(flags)) {\n+                return Stream.concat(inet4Addresses.stream(), inet6Addresses.stream());\n+            }\n+            \/\/ Only \"system\" addresses order is possible at this stage\n+            assert systemAddressesOrder(flags);\n+            return inetAddresses.stream();\n@@ -1087,5 +1215,8 @@\n-        private static List<InetAddress> concatAddresses(List<InetAddress> firstPart,\n-                                                         List<InetAddress> secondPart) {\n-            List<InetAddress> result = new ArrayList<>(firstPart);\n-            result.addAll(secondPart);\n-            return result;\n+        \/\/ Checks if result list with addresses is not empty.\n+        \/\/ If it is empty throw an UnknownHostException.\n+        private void checkResultsList(List<InetAddress> addressesList, String hostName)\n+                throws UnknownHostException {\n+            if (addressesList.isEmpty()) {\n+                throw new UnknownHostException(\"Unable to resolve host \" + hostName\n+                        + \" in hosts file \" + hostsFile);\n+            }\n@@ -1133,0 +1264,6 @@\n+    \/**\n+     * Platform-wide {@code LookupPolicy} initialized from {@code \"java.net.preferIPv4Stack\"},\n+     * {@code \"java.net.preferIPv6Addresses\"} system properties.\n+     *\/\n+    static final LookupPolicy PLATFORM_LOOKUP_POLICY;\n+\n@@ -1137,2 +1274,5 @@\n-        \/\/ create name service\n-        nameService = createNameService();\n+        \/\/ impl must be initialized before calling this method\n+        PLATFORM_LOOKUP_POLICY = initializePlatformLookupPolicy();\n+\n+        \/\/ create built-in resolver\n+        BUILTIN_RESOLVER = createBuiltinInetAddressResolver();\n@@ -1142,1 +1282,1 @@\n-     * Create an instance of the NameService interface based on\n+     * Create an instance of the InetAddressResolver interface based on\n@@ -1145,1 +1285,1 @@\n-     * <p>The default NameService is the PlatformNameService, which typically\n+     * <p>The default InetAddressResolver is the PlatformResolver, which typically\n@@ -1149,1 +1289,1 @@\n-     * <p> A HostsFileNameService is created if the {@code jdk.net.hosts.file}\n+     * <p> A HostsFileResolver is created if the {@code jdk.net.hosts.file}\n@@ -1154,1 +1294,1 @@\n-     * @return a NameService\n+     * @return an InetAddressResolver\n@@ -1156,7 +1296,4 @@\n-    private static NameService createNameService() {\n-\n-        String hostsFileName =\n-                GetPropertyAction.privilegedGetProperty(\"jdk.net.hosts.file\");\n-        NameService theNameService;\n-        if (hostsFileName != null) {\n-            theNameService = new HostsFileNameService(hostsFileName);\n+    private static InetAddressResolver createBuiltinInetAddressResolver() {\n+        InetAddressResolver theResolver;\n+        if (HOSTS_FILE_NAME != null) {\n+            theResolver = new HostsFileResolver(HOSTS_FILE_NAME);\n@@ -1164,1 +1301,1 @@\n-            theNameService = new PlatformNameService();\n+            theResolver = new PlatformResolver();\n@@ -1166,1 +1303,1 @@\n-        return theNameService;\n+        return theResolver;\n@@ -1171,1 +1308,2 @@\n-     * No name service is checked for the validity of the address.\n+     * The system-wide {@linkplain InetAddressResolver resolver} is not used to check\n+     * the validity of the address.\n@@ -1254,6 +1392,0 @@\n-    \/\/ called from deployment cache manager\n-    private static InetAddress getByName(String host, InetAddress reqAddr)\n-        throws UnknownHostException {\n-        return InetAddress.getAllByName(host, reqAddr)[0];\n-    }\n-\n@@ -1262,1 +1394,1 @@\n-     * based on the configured name service on the system.\n+     * based on the configured system {@linkplain InetAddressResolver resolver}.\n@@ -1301,5 +1433,0 @@\n-        return getAllByName(host, null);\n-    }\n-\n-    private static InetAddress[] getAllByName(String host, InetAddress reqAddr)\n-        throws UnknownHostException {\n@@ -1367,1 +1494,1 @@\n-        return getAllByName0(host, reqAddr, true, true);\n+        return getAllByName0(host, true, true);\n@@ -1417,6 +1544,0 @@\n-    private static InetAddress[] getAllByName0 (String host)\n-        throws UnknownHostException\n-    {\n-        return getAllByName0(host, true);\n-    }\n-\n@@ -1428,1 +1549,1 @@\n-        return getAllByName0 (host, null, check, true);\n+        return getAllByName0(host, check, true);\n@@ -1435,1 +1556,0 @@\n-     * @param reqAddr requested address to be the 1st in returned array\n@@ -1443,1 +1563,0 @@\n-                                               InetAddress reqAddr,\n@@ -1501,1 +1620,1 @@\n-                addrs = new NameServiceAddresses(host, reqAddr)\n+                    addrs = new NameServiceAddresses(host)\n@@ -1512,1 +1631,1 @@\n-    static InetAddress[] getAddressesFromNameService(String host, InetAddress reqAddr)\n+    static InetAddress[] getAddressesFromNameService(String host)\n@@ -1514,1 +1633,1 @@\n-        InetAddress[] addresses = null;\n+        Stream<InetAddress> addresses = null;\n@@ -1517,0 +1636,1 @@\n+        var resolver = resolver();\n@@ -1518,2 +1638,2 @@\n-            addresses = nameService.lookupAllHostAddr(host);\n-        } catch (UnknownHostException uhe) {\n+            addresses = resolver.lookupByName(host, PLATFORM_LOOKUP_POLICY);\n+        } catch (RuntimeException | UnknownHostException x) {\n@@ -1521,2 +1641,2 @@\n-                addresses = new InetAddress[]{impl.loopbackAddress()};\n-            } else {\n+                addresses = Stream.of(impl.loopbackAddress());\n+            } else if (x instanceof UnknownHostException uhe) {\n@@ -1524,0 +1644,3 @@\n+            } else {\n+                ex = new UnknownHostException();\n+                ex.initCause(x);\n@@ -1526,2 +1649,3 @@\n-\n-        if (addresses == null) {\n+        InetAddress[] result = addresses == null ? null\n+                : addresses.toArray(InetAddress[]::new);\n+        if (result == null || result.length == 0) {\n@@ -1530,23 +1654,1 @@\n-\n-        \/\/ More to do?\n-        if (reqAddr != null && addresses.length > 1 && !addresses[0].equals(reqAddr)) {\n-            \/\/ Find it?\n-            int i = 1;\n-            for (; i < addresses.length; i++) {\n-                if (addresses[i].equals(reqAddr)) {\n-                    break;\n-                }\n-            }\n-            \/\/ Rotate\n-            if (i < addresses.length) {\n-                InetAddress tmp, tmp2 = reqAddr;\n-                for (int j = 0; j < i; j++) {\n-                    tmp = addresses[j];\n-                    addresses[j] = tmp2;\n-                    tmp2 = tmp;\n-                }\n-                addresses[i] = tmp2;\n-            }\n-        }\n-\n-        return addresses;\n+        return result;\n@@ -1560,2 +1662,1 @@\n-     * <p> This method doesn't block, i.e. no reverse name service lookup\n-     * is performed.\n+     * <p> This method doesn't block, i.e. no reverse lookup is performed.\n@@ -1640,1 +1741,1 @@\n-                    localAddr = getAllByName0(local, null, false, false)[0];\n+                    localAddr = getAllByName0(local, false, false)[0];\n","filename":"src\/java.base\/share\/classes\/java\/net\/InetAddress.java","additions":311,"deletions":210,"binary":false,"changes":521,"status":"modified"},{"patch":"@@ -27,0 +27,1 @@\n+\n@@ -28,0 +29,2 @@\n+import java.net.spi.InetAddressResolver.LookupPolicy;\n+\n@@ -41,1 +44,1 @@\n-        lookupAllHostAddr(String hostname) throws UnknownHostException;\n+        lookupAllHostAddr(String hostname, LookupPolicy lookupPolicy) throws UnknownHostException;\n","filename":"src\/java.base\/share\/classes\/java\/net\/InetAddressImpl.java","additions":4,"deletions":1,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -3,1 +3,1 @@\n- Copyright (c) 1998, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ Copyright (c) 1998, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -64,1 +64,1 @@\n-    returned by the operating system.<\/P>\n+    returned by the system-wide {@linkplain java.net.spi.InetAddressResolver resolver}.<\/P>\n","filename":"src\/java.base\/share\/classes\/java\/net\/doc-files\/net-properties.html","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -0,0 +1,201 @@\n+\/*\n+ * Copyright (c) 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.  Oracle designates this\n+ * particular file as subject to the \"Classpath\" exception as provided\n+ * by Oracle in the LICENSE file that accompanied this code.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+package java.net.spi;\n+\n+import java.lang.annotation.Native;\n+import java.net.InetAddress;\n+import java.net.UnknownHostException;\n+import java.util.stream.Stream;\n+\n+\/**\n+ * This interface defines operations for looking up host names and IP addresses.\n+ * {@link InetAddress} delegates all lookup operations to the <i>system-wide\n+ * resolver<\/i>.\n+ *\n+ * <p> The <i>system-wide resolver<\/i> can be customized by\n+ * <a href=\"InetAddressResolverProvider.html#system-wide-resolver\">\n+ * deploying an implementation<\/a> of {@link InetAddressResolverProvider}.\n+ *\n+ * @since 18\n+ *\/\n+public interface InetAddressResolver {\n+\n+    \/**\n+     * Given the name of a host, returns a stream of IP addresses of the requested\n+     * address family associated with a provided hostname.\n+     *\n+     * <p> {@code host} should be a machine name, such as \"{@code www.example.com}\",\n+     * not a textual representation of its IP address. No validation is performed on\n+     * the given {@code host} name: if a textual representation is supplied, the name\n+     * resolution is likely to fail and {@link UnknownHostException} may be thrown.\n+     *\n+     * <p> The address family type and addresses order are specified by the\n+     * {@code LookupPolicy} instance. Lookup operation characteristics could be\n+     * acquired with {@link LookupPolicy#characteristics()}.\n+     * If {@link InetAddressResolver.LookupPolicy#IPV4} and\n+     * {@link InetAddressResolver.LookupPolicy#IPV6} characteristics provided then this\n+     * method returns addresses of both IPV4 and IPV6 families.\n+     *\n+     * @param host         the specified hostname\n+     * @param lookupPolicy the address lookup policy\n+     * @return a stream of IP addresses for the requested host\n+     * @throws NullPointerException if either parameter is {@code null}\n+     * @throws UnknownHostException if no IP address for the {@code host} could be found\n+     * @see LookupPolicy\n+     *\/\n+    Stream<InetAddress> lookupByName(String host, LookupPolicy lookupPolicy) throws UnknownHostException;\n+\n+    \/**\n+     * Lookup the host name corresponding to the raw IP address provided.\n+     *\n+     * <p> {@code addr} argument is in network byte order: the highest order byte of the address\n+     * is in {@code addr[0]}.\n+     *\n+     * <p> IPv4 address byte array must be 4 bytes long and IPv6 byte array\n+     * must be 16 bytes long.\n+     *\n+     * @param addr byte array representing a raw IP address\n+     * @return {@code String} representing the host name mapping\n+     * @throws UnknownHostException     if no host name is found for the specified IP address\n+     * @throws IllegalArgumentException if the length of the provided byte array doesn't correspond\n+     *                                  to a valid IP address length\n+     * @throws NullPointerException     if addr is {@code null}\n+     *\/\n+    String lookupByAddress(byte[] addr) throws UnknownHostException;\n+\n+    \/**\n+     * A {@code LookupPolicy} object describes characteristics that can be applied to a lookup operation.\n+     * In particular, it is used to specify the ordering and which filtering should be performed when\n+     * {@linkplain InetAddressResolver#lookupByName(String, LookupPolicy) looking up host addresses}.\n+     *\n+     * <p> The default platform-wide lookup policy is constructed by consulting\n+     * <a href=\"doc-files\/net-properties.html#Ipv4IPv6\">System Properties<\/a> which affect\n+     * how IPv4 and IPv6 addresses are returned.\n+     *\n+     * @since 18\n+     *\/\n+    final class LookupPolicy {\n+\n+        \/**\n+         * Characteristic value signifying if IPv4 addresses need to be queried during lookup.\n+         *\/\n+        @Native\n+        public static final int IPV4 = 1 << 0;\n+\n+        \/**\n+         * Characteristic value signifying if IPv6 addresses need to be queried during lookup.\n+         *\/\n+        @Native\n+        public static final int IPV6 = 1 << 1;\n+\n+        \/**\n+         * Characteristic value signifying if IPv4 addresses should be returned\n+         * first by {@code InetAddressResolver}.\n+         *\/\n+        @Native\n+        public static final int IPV4_FIRST = 1 << 2;\n+\n+        \/**\n+         * Characteristic value signifying if IPv6 addresses should be returned\n+         * first by {@code InetAddressResolver}.\n+         *\/\n+        @Native\n+        public static final int IPV6_FIRST = 1 << 3;\n+\n+        private final int characteristics;\n+\n+        private LookupPolicy(int characteristics) {\n+            this.characteristics = characteristics;\n+        }\n+\n+        \/**\n+         * This factory method creates a {@link LookupPolicy LookupPolicy} instance with\n+         * the given {@code characteristics} value.\n+         *\n+         * <p> The {@code characteristics} value is an integer bit mask which defines\n+         * parameters of a forward lookup operation. These parameters define at least:\n+         * <ul>\n+         *     <li>the family type of the returned addresses<\/li>\n+         *     <li>the order in which a {@linkplain InetAddressResolver resolver}\n+         *         implementation should return its results<\/li>\n+         * <\/ul>\n+         *\n+         * <p> To request addresses of specific family types the following bit masks can be combined:\n+         * <ul>\n+         *     <li>{@link LookupPolicy#IPV4}: to request IPv4 addresses<\/li>\n+         *     <li>{@link LookupPolicy#IPV6}: to request IPv6 addresses<\/li>\n+         * <\/ul>\n+         * <br>It is an error if neither {@link LookupPolicy#IPV4} or {@link LookupPolicy#IPV6} are set.\n+         *\n+         * <p> To request a specific ordering of the results:\n+         * <ul>\n+         *     <li>{@link LookupPolicy#IPV4_FIRST}: return IPv4 addresses before any IPv6 address<\/li>\n+         *     <li>{@link LookupPolicy#IPV6_FIRST}: return IPv6 addresses before any IPv4 address<\/li>\n+         * <\/ul>\n+         * <br>If neither {@link LookupPolicy#IPV4_FIRST} or {@link LookupPolicy#IPV6_FIRST} are set it\n+         * implies <a href=\"{@docRoot}\/java.base\/java\/net\/doc-files\/net-properties.html#Ipv4IPv6\">\"system\"<\/a>\n+         * order of addresses.\n+         * It is an error to request both {@link LookupPolicy#IPV4_FIRST} and {@link LookupPolicy#IPV6_FIRST}.\n+         *\n+         * @param characteristics a value which represents the set of lookup characteristics\n+         * @return an instance of {@code InetAddressResolver.LookupPolicy}\n+         * @throws IllegalArgumentException if an illegal characteristics bit mask is provided\n+         * @see InetAddressResolver#lookupByName(String, LookupPolicy)\n+         *\/\n+        public static LookupPolicy of(int characteristics) {\n+            \/\/ At least one type of addresses should be requested\n+            if ((characteristics & IPV4) == 0 && (characteristics & IPV6) == 0) {\n+                throw new IllegalArgumentException(\"No address type specified\");\n+            }\n+\n+            \/\/ Requested order of addresses couldn't be determined\n+            if ((characteristics & IPV4_FIRST) != 0 && (characteristics & IPV6_FIRST) != 0) {\n+                throw new IllegalArgumentException(\"Addresses order cannot be determined\");\n+            }\n+\n+            \/\/ If IPv4 addresses requested to be returned first then they should be requested too\n+            if ((characteristics & IPV4_FIRST) != 0 && (characteristics & IPV4) == 0) {\n+                throw new IllegalArgumentException(\"Addresses order and type do not match\");\n+            }\n+\n+            \/\/ If IPv6 addresses requested to be returned first then they should be requested too\n+            if ((characteristics & IPV6_FIRST) != 0 && (characteristics & IPV6) == 0) {\n+                throw new IllegalArgumentException(\"Addresses order and type do not match\");\n+            }\n+            return new LookupPolicy(characteristics);\n+        }\n+\n+        \/**\n+         * Returns the set of characteristics of this lookup policy.\n+         *\n+         * @return a characteristics value\n+         * @see InetAddressResolver#lookupByName(String, LookupPolicy)\n+         *\/\n+        public int characteristics() {\n+            return characteristics;\n+        }\n+    }\n+}\n","filename":"src\/java.base\/share\/classes\/java\/net\/spi\/InetAddressResolver.java","additions":201,"deletions":0,"binary":false,"changes":201,"status":"added"},{"patch":"@@ -0,0 +1,160 @@\n+\/*\n+ * Copyright (c) 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.  Oracle designates this\n+ * particular file as subject to the \"Classpath\" exception as provided\n+ * by Oracle in the LICENSE file that accompanied this code.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+package java.net.spi;\n+\n+import sun.net.ResolverProviderConfiguration;\n+\n+import java.net.InetAddress;\n+import java.util.ServiceLoader;\n+\n+\/**\n+ * Service-provider class for {@linkplain InetAddressResolver InetAddress resolvers}.\n+ *\n+ * <p> A resolver provider is a factory for custom implementations of {@linkplain\n+ * InetAddressResolver InetAddress resolvers}. A resolver defines operations for\n+ * looking up (resolving) host names and IP addresses.\n+ * <p>A resolver provider is a concrete subclass of this class that has a\n+ * zero-argument constructor and implements the abstract methods specified below.\n+ *\n+ * <p> A given invocation of the Java virtual machine maintains a single\n+ * system-wide resolver instance, which is used by\n+ * <a href=\"{@docRoot}\/java.base\/java\/net\/InetAddress.html#host-name-resolution\">\n+ * InetAddress<\/a>. It is set after the VM is fully initialized and when an\n+ * invocation of a method in {@link InetAddress} class triggers the first lookup\n+ * operation.\n+ *\n+ * <p id=\"system-wide-resolver\"> A resolver provider is located and loaded by\n+ * {@link InetAddress} to create the system-wide resolver as follows:\n+ * <ol>\n+ *  <li>The {@link ServiceLoader} mechanism is used to locate an\n+ *      {@code InetAddressResolverProvider} using the\n+ *      system class loader. The order in which providers are located is\n+ *      {@linkplain ServiceLoader#load(java.lang.Class, java.lang.ClassLoader)\n+ *      implementation specific}.\n+ *      The first provider found will be used to instantiate the\n+ *      {@link InetAddressResolver InetAddressResolver} by invoking the\n+ *      {@link InetAddressResolverProvider#get(InetAddressResolverProvider.Configuration)}\n+ *      method. The returned {@code InetAddressResolver} will be set as the\n+ *      system-wide resolver.\n+ *  <li>If the previous step fails to find any resolver provider the\n+ *      <a href=\"{@docRoot}\/java.base\/java\/net\/InetAddress.html#built-in-resolver\">\n+ *      built-in resolver<\/a> will be set as the system-wide resolver.\n+ * <\/ol>\n+ *\n+ * <p> If instantiating a custom resolver from a provider discovered in\n+ * step 1 throws an error or exception, the system-wide resolver will not be\n+ * set and the error or exception will be propagated to the calling thread.\n+ * Otherwise, any lookup operation will be performed using the\n+ * <i>system-wide resolver<\/i>.\n+ *\n+ * @implNote {@link InetAddress} will use the <i>built-in resolver<\/i> for any lookup operation\n+ * that might occur before the VM is fully booted.\n+ *\n+ * @since 18\n+ *\/\n+public abstract class InetAddressResolverProvider {\n+\n+    \/**\n+     * Initialize and return an {@link InetAddressResolver} provided by\n+     * this provider. This method is called by {@link InetAddress} when\n+     * <a href=\"#system-wide-resolver\">installing<\/a>\n+     * the system-wide resolver implementation.\n+     *\n+     * <p> Any error or exception thrown by this method is considered as\n+     * a failure of {@code InetAddressResolver} instantiation and will be propagated to\n+     * the calling thread.\n+     * @param configuration a {@link Configuration} instance containing platform built-in address\n+     *                     resolution configuration.\n+     * @return the resolver provided by this provider\n+     *\/\n+    public abstract InetAddressResolver get(Configuration configuration);\n+\n+    \/**\n+     * {@return the name of this provider, or {@code null} if unnamed}\n+     *\/\n+    public abstract String name();\n+\n+    \/**\n+     * The {@code RuntimePermission(\"inetAddressResolverProvider\")} is\n+     * necessary to subclass and instantiate the {@code InetAddressResolverProvider} class,\n+     * as well as to obtain resolver from an instance of that class,\n+     * and it is also required to obtain the operating system name resolution configurations.\n+     *\/\n+    private static final RuntimePermission INET_ADDRESS_RESOLVER_PERMISSION =\n+            new RuntimePermission(\"inetAddressResolverProvider\");\n+\n+    \/**\n+     * Creates a new instance of {@code InetAddressResolverProvider}.\n+     *\n+     * @throws SecurityException if a security manager is present and its\n+     *                           {@code checkPermission} method doesn't allow the\n+     *                           {@code RuntimePermission(\"inetAddressResolverProvider\")}.\n+     * @implNote It is recommended that an {@code InetAddressResolverProvider} service\n+     * implementation initialization should be as simple as possible, in order to avoid\n+     * possible risks of deadlock or class loading cycles during the instantiation of the\n+     * service provider.\n+     *\/\n+    protected InetAddressResolverProvider() {\n+        this(checkPermission());\n+    }\n+\n+    private InetAddressResolverProvider(Void unused) {\n+    }\n+\n+    @SuppressWarnings(\"removal\")\n+    private static Void checkPermission() {\n+        final SecurityManager sm = System.getSecurityManager();\n+        if (sm != null) {\n+            sm.checkPermission(INET_ADDRESS_RESOLVER_PERMISSION);\n+        }\n+        return null;\n+    }\n+\n+    \/**\n+     * A {@code Configuration} object is supplied to the\n+     * {@link InetAddressResolverProvider#get(Configuration)} method when\n+     * setting the system-wide resolver.\n+     * A resolver implementation can then delegate to the built-in resolver\n+     * provided by this interface if it needs to.\n+     *\n+     * @since 18\n+     *\/\n+    public sealed interface Configuration permits ResolverProviderConfiguration {\n+        \/**\n+         * Returns the built-in {@linkplain InetAddressResolver resolver}.\n+         *\n+         * @return the JDK built-in resolver.\n+         *\/\n+        InetAddressResolver builtinResolver();\n+\n+        \/**\n+         * Reads the localhost name from the system configuration.\n+         *\n+         * @return the localhost name.\n+         *\/\n+        String lookupLocalHostName();\n+    }\n+}\n","filename":"src\/java.base\/share\/classes\/java\/net\/spi\/InetAddressResolverProvider.java","additions":160,"deletions":0,"binary":false,"changes":160,"status":"added"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2015, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2015, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -29,2 +29,2 @@\n- * <p> Only developers who are defining new URL stream handler providers\n- * should need to make direct use of this package.\n+ * <p> Only developers who are defining new URL stream handler providers or implementing\n+ * a custom resolver provider should need to make direct use of this package.\n","filename":"src\/java.base\/share\/classes\/java\/net\/spi\/package-info.java","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1996, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1996, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -1149,3 +1149,2 @@\n-                \/\/ Because 'algorithm' and 'provider' are private\n-                \/\/ members of our supertype, we must perform a cast to\n-                \/\/ access them.\n+                \/\/ Because 'algorithm' is private member of our supertype,\n+                \/\/ we must perform a cast to access it.\n@@ -1155,1 +1154,1 @@\n-                that.provider = ((Signature)this).provider;\n+                that.provider = this.provider;\n","filename":"src\/java.base\/share\/classes\/java\/security\/Signature.java","additions":4,"deletions":5,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -126,1 +126,1 @@\n-    private String type;\n+    private final transient String type;\n@@ -273,1 +273,2 @@\n-     * {@code CertPathRep} object.\n+     * {@link CertPathRep CertPathRep} object containing the\n+     * {@code Certificate} type and encoded bytes of the {@code CertPath}.\n@@ -275,1 +276,2 @@\n-     * @return the {@code CertPathRep} to be serialized\n+     * @return a {@code CertPathRep} containing the {@code Certificate} type\n+     *         and encoded bytes of the {@code CertPath}\n@@ -302,1 +304,1 @@\n-        \/** The Certificate type *\/\n+        \/** The type of {@code Certificate}s in the {@code CertPath}. *\/\n@@ -304,1 +306,1 @@\n-        \/** The encoded form of the cert path *\/\n+        \/** The encoded form of the {@code CertPath}. *\/\n@@ -311,1 +313,1 @@\n-         * @param type the standard name of a {@code CertPath} type\n+         * @param type the standard name of a {@code Certificate} type\n@@ -320,1 +322,2 @@\n-         * Returns a {@code CertPath} constructed from the type and data.\n+         * Returns a {@code CertPath} constructed from the type and data of\n+         * this {@code CertPathRep}.\n@@ -324,1 +327,1 @@\n-         * @throws ObjectStreamException if a {@code CertPath} could not\n+         * @throws ObjectStreamException if a {@code CertPath} object could not\n","filename":"src\/java.base\/share\/classes\/java\/security\/cert\/CertPath.java","additions":11,"deletions":8,"binary":false,"changes":19,"status":"modified"},{"patch":"@@ -69,1 +69,1 @@\n-    private final String type;\n+    private final transient String type;\n@@ -72,1 +72,1 @@\n-    private int hash = -1; \/\/ Default to -1\n+    private transient int hash = -1; \/\/ Default to -1\n@@ -239,1 +239,1 @@\n-     * Alternate Certificate class for serialization.\n+     * Alternate {@code Certificate} class for serialization.\n@@ -254,2 +254,2 @@\n-         * Construct the alternate Certificate class with the Certificate\n-         * type and Certificate encoding bytes.\n+         * Construct the alternate {@code Certificate} class with the\n+         * {@code Certificate} type and {@code Certificate} encoding bytes.\n@@ -257,1 +257,1 @@\n-         * @param type the standard name of the Certificate type.\n+         * @param type the standard name of the {@code Certificate} type.\n@@ -259,1 +259,1 @@\n-         * @param data the Certificate data.\n+         * @param data the {@code Certificate} data.\n@@ -267,1 +267,2 @@\n-         * Resolve the Certificate Object.\n+         * Returns a {@code Certificate} with the type and data of this\n+         * {@code CertificateRep}.\n@@ -269,1 +270,1 @@\n-         * @return the resolved Certificate Object\n+         * @return the resolved {@code Certificate} object\n@@ -271,1 +272,1 @@\n-         * @throws java.io.ObjectStreamException if the Certificate\n+         * @throws java.io.ObjectStreamException if the {@code Certificate}\n@@ -291,1 +292,3 @@\n-     * Replace the Certificate to be serialized.\n+     * Replace the {@code Certificate} to be serialized with a\n+     * {@link CertificateRep CertificateRep} object containing the type and\n+     * encoded bytes of the {@code Certificate}.\n@@ -293,1 +296,2 @@\n-     * @return the alternate Certificate object to be serialized\n+     * @return a {@code CertificateRep} object containing the type and encoded\n+     *         bytes of the {@code Certificate}\n@@ -295,2 +299,2 @@\n-     * @throws java.io.ObjectStreamException if a new object representing\n-     * this Certificate could not be created\n+     * @throws java.io.ObjectStreamException if a {@code CertificateRep} object\n+     *         representing this {@code Certificate} could not be created\n","filename":"src\/java.base\/share\/classes\/java\/security\/cert\/Certificate.java","additions":18,"deletions":14,"binary":false,"changes":32,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2012, 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2012, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -75,1 +75,0 @@\n-import java.util.HashMap;\n@@ -124,35 +123,0 @@\n-    \/**\n-     * Narrow names for eras.\n-     *\/\n-    private static final HashMap<String, String[]> ERA_NARROW_NAMES = new HashMap<>();\n-    \/**\n-     * Short names for eras.\n-     *\/\n-    private static final HashMap<String, String[]> ERA_SHORT_NAMES = new HashMap<>();\n-    \/**\n-     * Full names for eras.\n-     *\/\n-    private static final HashMap<String, String[]> ERA_FULL_NAMES = new HashMap<>();\n-    \/**\n-     * Fallback language for the era names.\n-     *\/\n-    private static final String FALLBACK_LANGUAGE = \"en\";\n-    \/**\n-     * Language that has the era names.\n-     *\/\n-    private static final String TARGET_LANGUAGE = \"th\";\n-    \/**\n-     * Name data.\n-     *\/\n-    static {\n-        ERA_NARROW_NAMES.put(FALLBACK_LANGUAGE, new String[]{\"BB\", \"BE\"});\n-        ERA_NARROW_NAMES.put(TARGET_LANGUAGE, new String[]{\"BB\", \"BE\"});\n-        ERA_SHORT_NAMES.put(FALLBACK_LANGUAGE, new String[]{\"B.B.\", \"B.E.\"});\n-        ERA_SHORT_NAMES.put(TARGET_LANGUAGE,\n-                new String[]{\"\\u0e1e.\\u0e28.\",\n-                \"\\u0e1b\\u0e35\\u0e01\\u0e48\\u0e2d\\u0e19\\u0e04\\u0e23\\u0e34\\u0e2a\\u0e15\\u0e4c\\u0e01\\u0e32\\u0e25\\u0e17\\u0e35\\u0e48\"});\n-        ERA_FULL_NAMES.put(FALLBACK_LANGUAGE, new String[]{\"Before Buddhist\", \"Budhhist Era\"});\n-        ERA_FULL_NAMES.put(TARGET_LANGUAGE,\n-                new String[]{\"\\u0e1e\\u0e38\\u0e17\\u0e18\\u0e28\\u0e31\\u0e01\\u0e23\\u0e32\\u0e0a\",\n-                \"\\u0e1b\\u0e35\\u0e01\\u0e48\\u0e2d\\u0e19\\u0e04\\u0e23\\u0e34\\u0e2a\\u0e15\\u0e4c\\u0e01\\u0e32\\u0e25\\u0e17\\u0e35\\u0e48\"});\n-    }\n","filename":"src\/java.base\/share\/classes\/java\/time\/chrono\/ThaiBuddhistChronology.java","additions":1,"deletions":37,"binary":false,"changes":38,"status":"modified"},{"patch":"@@ -691,2 +691,2 @@\n-     * If the field value in the date-time to be printed is invalid it\n-     * cannot be printed and an exception will be thrown.\n+     * If the field value in the date-time to be printed is outside the\n+     * range of valid values then an exception will be thrown.\n","filename":"src\/java.base\/share\/classes\/java\/time\/format\/DateTimeFormatterBuilder.java","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1996, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1996, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -627,1 +627,1 @@\n-        gdate = (BaseCalendar.Date) gcal.newCalendarDate(zone);\n+        gdate = gcal.newCalendarDate(zone);\n@@ -701,1 +701,1 @@\n-        gdate = (BaseCalendar.Date) gcal.newCalendarDate(getZone());\n+        gdate = gcal.newCalendarDate(getZone());\n@@ -739,1 +739,1 @@\n-        gdate = (BaseCalendar.Date) gcal.newCalendarDate(getZone());\n+        gdate = gcal.newCalendarDate(getZone());\n@@ -2373,1 +2373,1 @@\n-            cdate = (BaseCalendar.Date) jcal.newCalendarDate(getZone());\n+            cdate = jcal.newCalendarDate(getZone());\n@@ -3112,1 +3112,1 @@\n-            date = (BaseCalendar.Date) gcal.newCalendarDate(TimeZone.NO_TIMEZONE);\n+            date = gcal.newCalendarDate(TimeZone.NO_TIMEZONE);\n@@ -3201,1 +3201,1 @@\n-            gdate = (BaseCalendar.Date) gcal.newCalendarDate(getZone());\n+            gdate = gcal.newCalendarDate(getZone());\n","filename":"src\/java.base\/share\/classes\/java\/util\/GregorianCalendar.java","additions":7,"deletions":7,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -338,1 +338,1 @@\n-        map = (((HashSet<?>)this) instanceof LinkedHashSet ?\n+        map = (this instanceof LinkedHashSet ?\n","filename":"src\/java.base\/share\/classes\/java\/util\/HashSet.java","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -239,1 +239,1 @@\n-     * kinds of line, <i>natural lines<\/i> and <i>logical lines<\/i>.\n+     * kinds of lines, <i>natural lines<\/i> and <i>logical lines<\/i>.\n@@ -256,2 +256,2 @@\n-     * {@code '#'} or {@code '!'} as its first non-white\n-     * space character; comment lines are also ignored and do not\n+     * {@code '#'} or {@code '!'} as its first non-whitespace\n+     * character; comment lines are also ignored and do not\n","filename":"src\/java.base\/share\/classes\/java\/util\/Properties.java","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2012, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2012, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -615,1 +615,1 @@\n-        IntFunction rawGenerator = (IntFunction) generator;\n+        IntFunction rawGenerator = generator;\n","filename":"src\/java.base\/share\/classes\/java\/util\/stream\/ReferencePipeline.java","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -1634,5 +1634,2 @@\n-                        if ((entryLen == nameLen && entry.equals(name)) ||\n-                                (addSlash &&\n-                                nameLen + 1 == entryLen &&\n-                                entry.startsWith(name) &&\n-                                entry.charAt(entryLen - 1) == '\/')) {\n+                        if (entryLen == nameLen && entry.equals(name)) {\n+                            \/\/ Found our match\n@@ -1641,0 +1638,8 @@\n+                        \/\/ If addSlash is true we'll now test for name+\/ providing\n+                        if (addSlash && nameLen + 1 == entryLen\n+                                && entry.startsWith(name) &&\n+                                entry.charAt(entryLen - 1) == '\/') {\n+                            \/\/ Found the entry \"name+\/\", now find the CEN entry pos\n+                            int exactPos = getEntryPos(name, false);\n+                            return exactPos == -1 ? pos : exactPos;\n+                        }\n","filename":"src\/java.base\/share\/classes\/java\/util\/zip\/ZipFile.java","additions":10,"deletions":5,"binary":false,"changes":15,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2015, 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2015, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -31,1 +31,0 @@\n-import java.net.UnknownHostException;\n@@ -40,9 +39,0 @@\n-    \/**\n-     * Get the InetAddress of the provided host. If an InetAddress is provided\n-     * then it will be the default address returned for all calls to either\n-     * form of getByName. This is required to maintain consistency when\n-     * caching addresses and hostnames.\n-     *\/\n-    InetAddress getByName(String hostName, InetAddress hostAddress)\n-            throws UnknownHostException;\n-\n","filename":"src\/java.base\/share\/classes\/jdk\/internal\/access\/JavaNetInetAddressAccess.java","additions":1,"deletions":11,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -356,0 +356,3 @@\n+        static final long BYTE_BUFFER_IS_READ_ONLY\n+                = UNSAFE.objectFieldOffset(ByteBuffer.class, \"isReadOnly\");\n+\n@@ -376,0 +379,5 @@\n+    @ForceInline\n+    public static boolean isReadOnly(ByteBuffer bb) {\n+        return UNSAFE.getBoolean(bb, BufferAccess.BYTE_BUFFER_IS_READ_ONLY);\n+    }\n+\n@@ -382,1 +390,1 @@\n-                          VectorSupport.LoadOperation<ByteBuffer, V, E, S> defaultImpl) {\n+                          VectorSupport.LoadOperation<ByteBuffer, V, S> defaultImpl) {\n@@ -403,1 +411,1 @@\n-                          VectorSupport.LoadOperation<ByteBuffer, V, E, S> defaultImpl) {\n+                          VectorSupport.LoadOperation<ByteBuffer, V, S> defaultImpl) {\n@@ -409,0 +417,2 @@\n+            final byte[] base = (byte[]) BufferAccess.bufferBase(bb);\n+\n@@ -410,1 +420,44 @@\n-                    BufferAccess.bufferBase(bb), BufferAccess.bufferAddress(bb, offset),\n+                      base, BufferAccess.bufferAddress(bb, offset),\n+                      bb, offset, s,\n+                      defaultImpl);\n+        } finally {\n+            Reference.reachabilityFence(scope);\n+        }\n+    }\n+\n+    @ForceInline\n+    public static\n+    <V extends VectorSupport.Vector<E>, E, S extends VectorSupport.VectorSpecies<E>,\n+     M extends VectorSupport.VectorMask<E>>\n+    V loadFromByteBufferMasked(Class<? extends V> vmClass, Class<M> maskClass, Class<E> e,\n+                               int length, ByteBuffer bb, int offset, M m, S s,\n+                               VectorSupport.LoadVectorMaskedOperation<ByteBuffer, V, S, M> defaultImpl) {\n+        try {\n+            return loadFromByteBufferMaskedScoped(\n+                    BufferAccess.scope(bb),\n+                    vmClass, maskClass, e, length,\n+                    bb, offset, m,\n+                    s,\n+                    defaultImpl);\n+        } catch (ScopedMemoryAccess.Scope.ScopedAccessError ex) {\n+            throw new IllegalStateException(\"This segment is already closed\");\n+        }\n+    }\n+\n+    @Scoped\n+    @ForceInline\n+    private static\n+    <V extends VectorSupport.Vector<E>, E, S extends VectorSupport.VectorSpecies<E>,\n+     M extends VectorSupport.VectorMask<E>>\n+    V loadFromByteBufferMaskedScoped(ScopedMemoryAccess.Scope scope, Class<? extends V> vmClass,\n+                                     Class<M> maskClass, Class<E> e, int length,\n+                                     ByteBuffer bb, int offset, M m,\n+                                     S s,\n+                                     VectorSupport.LoadVectorMaskedOperation<ByteBuffer, V, S, M> defaultImpl) {\n+        try {\n+            if (scope != null) {\n+                scope.checkValidState();\n+            }\n+\n+            return VectorSupport.loadMasked(vmClass, maskClass, e, length,\n+                    BufferAccess.bufferBase(bb), BufferAccess.bufferAddress(bb, offset), m,\n@@ -451,0 +504,2 @@\n+            final byte[] base = (byte[]) BufferAccess.bufferBase(bb);\n+\n@@ -452,0 +507,43 @@\n+                                base, BufferAccess.bufferAddress(bb, offset),\n+                                v,\n+                                bb, offset,\n+                                defaultImpl);\n+        } finally {\n+            Reference.reachabilityFence(scope);\n+        }\n+    }\n+\n+    @ForceInline\n+    public static\n+    <V extends VectorSupport.Vector<E>, E, M extends VectorSupport.VectorMask<E>>\n+    void storeIntoByteBufferMasked(Class<? extends V> vmClass, Class<M> maskClass, Class<E> e,\n+                                   int length, V v, M m,\n+                                   ByteBuffer bb, int offset,\n+                                   VectorSupport.StoreVectorMaskedOperation<ByteBuffer, V, M> defaultImpl) {\n+        try {\n+            storeIntoByteBufferMaskedScoped(\n+                    BufferAccess.scope(bb),\n+                    vmClass, maskClass, e, length,\n+                    v, m,\n+                    bb, offset,\n+                    defaultImpl);\n+        } catch (ScopedMemoryAccess.Scope.ScopedAccessError ex) {\n+            throw new IllegalStateException(\"This segment is already closed\");\n+        }\n+    }\n+\n+    @Scoped\n+    @ForceInline\n+    private static\n+    <V extends VectorSupport.Vector<E>, E, M extends VectorSupport.VectorMask<E>>\n+    void storeIntoByteBufferMaskedScoped(ScopedMemoryAccess.Scope scope,\n+                                         Class<? extends V> vmClass, Class<M> maskClass,\n+                                         Class<E> e, int length, V v, M m,\n+                                         ByteBuffer bb, int offset,\n+                                         VectorSupport.StoreVectorMaskedOperation<ByteBuffer, V, M> defaultImpl) {\n+        try {\n+            if (scope != null) {\n+                scope.checkValidState();\n+            }\n+\n+            VectorSupport.storeMasked(vmClass, maskClass, e, length,\n@@ -453,1 +551,1 @@\n-                    v,\n+                    v, m,\n@@ -461,1 +559,0 @@\n-\n","filename":"src\/java.base\/share\/classes\/jdk\/internal\/misc\/X-ScopedMemoryAccess.java.template","additions":102,"deletions":5,"binary":false,"changes":107,"status":"modified"},{"patch":"@@ -72,0 +72,1 @@\n+    public static final int VECTOR_OP_MASK_TOLONG    = 22;\n@@ -74,2 +75,2 @@\n-    public static final int VECTOR_OP_LROTATE = 22;\n-    public static final int VECTOR_OP_RROTATE = 23;\n+    public static final int VECTOR_OP_LROTATE = 23;\n+    public static final int VECTOR_OP_RROTATE = 24;\n@@ -159,1 +160,2 @@\n-    public interface BroadcastOperation<VM, E, S extends VectorSpecies<E>> {\n+    public interface BroadcastOperation<VM extends VectorPayload,\n+                                        S extends VectorSpecies<?>> {\n@@ -165,4 +167,7 @@\n-    <VM, E, S extends VectorSpecies<E>>\n-    VM broadcastCoerced(Class<? extends VM> vmClass, Class<E> E, int length,\n-                                  long bits, S s,\n-                                  BroadcastOperation<VM, E, S> defaultImpl) {\n+    <VM extends VectorPayload,\n+     S extends VectorSpecies<E>,\n+     E>\n+    VM broadcastCoerced(Class<? extends VM> vmClass, Class<E> eClass,\n+                        int length,\n+                        long bits, S s,\n+                        BroadcastOperation<VM, S> defaultImpl) {\n@@ -174,2 +179,3 @@\n-    public interface ShuffleIotaOperation<E, S extends VectorSpecies<E>> {\n-        VectorShuffle<E> apply(int length, int start, int step, S s);\n+    public interface ShuffleIotaOperation<S extends VectorSpecies<?>,\n+                                          SH extends VectorShuffle<?>> {\n+        SH apply(int length, int start, int step, S s);\n@@ -180,3 +186,7 @@\n-    <E, S extends VectorSpecies<E>>\n-    VectorShuffle<E> shuffleIota(Class<?> E, Class<?> ShuffleClass, S s, int length,\n-                     int start, int step, int wrap, ShuffleIotaOperation<E, S> defaultImpl) {\n+    <E,\n+     S extends VectorSpecies<E>,\n+     SH extends VectorShuffle<E>>\n+    SH shuffleIota(Class<E> eClass, Class<? extends SH> shClass, S s,\n+                   int length,\n+                   int start, int step, int wrap,\n+                   ShuffleIotaOperation<S, SH> defaultImpl) {\n@@ -187,2 +197,3 @@\n-    public interface ShuffleToVectorOperation<VM, Sh, E> {\n-       VM apply(Sh s);\n+    public interface ShuffleToVectorOperation<V extends Vector<?>,\n+                                              SH extends VectorShuffle<?>> {\n+       V apply(SH sh);\n@@ -193,3 +204,6 @@\n-    <VM ,Sh extends VectorShuffle<E>, E>\n-    VM shuffleToVector(Class<?> VM, Class<?>E , Class<?> ShuffleClass, Sh s, int length,\n-                       ShuffleToVectorOperation<VM,Sh,E> defaultImpl) {\n+    <V extends Vector<E>,\n+     SH extends VectorShuffle<E>,\n+     E>\n+    V shuffleToVector(Class<? extends Vector<E>> vClass, Class<E> eClass, Class<? extends SH> shClass, SH sh,\n+                      int length,\n+                      ShuffleToVectorOperation<V, SH> defaultImpl) {\n@@ -197,1 +211,1 @@\n-      return defaultImpl.apply(s);\n+      return defaultImpl.apply(sh);\n@@ -201,1 +215,2 @@\n-    public interface IndexOperation<V extends Vector<E>, E, S extends VectorSpecies<E>> {\n+    public interface IndexOperation<V extends Vector<?>,\n+                                    S extends VectorSpecies<?>> {\n@@ -207,2 +222,5 @@\n-    <V extends Vector<E>, E, S extends VectorSpecies<E>>\n-    V indexVector(Class<? extends V> vClass, Class<E> E, int length,\n+    <V extends Vector<E>,\n+     E,\n+     S extends VectorSpecies<E>>\n+    V indexVector(Class<? extends V> vClass, Class<E> eClass,\n+                  int length,\n@@ -210,1 +228,1 @@\n-                  IndexOperation<V, E, S> defaultImpl) {\n+                  IndexOperation<V, S> defaultImpl) {\n@@ -217,0 +235,5 @@\n+    public interface ReductionOperation<V extends Vector<?>,\n+                                        M extends VectorMask<?>> {\n+        long apply(V v, M m);\n+    }\n+\n@@ -219,4 +242,8 @@\n-    <V extends Vector<?>>\n-    long reductionCoerced(int oprId, Class<?> vectorClass, Class<?> elementType, int length,\n-                          V v,\n-                          Function<V,Long> defaultImpl) {\n+    <V extends Vector<E>,\n+     M extends VectorMask<E>,\n+     E>\n+    long reductionCoerced(int oprId,\n+                          Class<? extends V> vClass, Class<? extends M> mClass, Class<E> eClass,\n+                          int length,\n+                          V v, M m,\n+                          ReductionOperation<V, M> defaultImpl) {\n@@ -224,1 +251,1 @@\n-        return defaultImpl.apply(v);\n+        return defaultImpl.apply(v, m);\n@@ -227,0 +254,1 @@\n+\n@@ -229,2 +257,2 @@\n-    public interface VecExtractOp<V> {\n-        long apply(V v1, int idx);\n+    public interface VecExtractOp<V extends Vector<?>> {\n+        long apply(V v, int i);\n@@ -235,3 +263,5 @@\n-    <V extends Vector<?>>\n-    long extract(Class<?> vectorClass, Class<?> elementType, int vlen,\n-                 V vec, int ix,\n+    <V extends Vector<E>,\n+     E>\n+    long extract(Class<? extends V> vClass, Class<E> eClass,\n+                 int length,\n+                 V v, int i,\n@@ -240,1 +270,1 @@\n-        return defaultImpl.apply(vec, ix);\n+        return defaultImpl.apply(v, i);\n@@ -245,2 +275,2 @@\n-    public interface VecInsertOp<V> {\n-        V apply(V v1, int idx, long val);\n+    public interface VecInsertOp<V extends Vector<?>> {\n+        V apply(V v, int i, long val);\n@@ -251,3 +281,5 @@\n-    <V extends Vector<?>>\n-    V insert(Class<? extends V> vectorClass, Class<?> elementType, int vlen,\n-             V vec, int ix, long val,\n+    <V extends Vector<E>,\n+     E>\n+    V insert(Class<? extends V> vClass, Class<E> eClass,\n+             int length,\n+             V v, int i, long val,\n@@ -256,1 +288,1 @@\n-        return defaultImpl.apply(vec, ix, val);\n+        return defaultImpl.apply(v, i, val);\n@@ -261,0 +293,5 @@\n+    public interface UnaryOperation<V extends Vector<?>,\n+                                    M extends VectorMask<?>> {\n+        V apply(V v, M m);\n+    }\n+\n@@ -263,4 +300,8 @@\n-    <VM>\n-    VM unaryOp(int oprId, Class<? extends VM> vmClass, Class<?> elementType, int length,\n-               VM vm,\n-               Function<VM, VM> defaultImpl) {\n+    <V extends Vector<E>,\n+     M extends VectorMask<E>,\n+     E>\n+    V unaryOp(int oprId,\n+              Class<? extends V> vClass, Class<? extends M> mClass, Class<E> eClass,\n+              int length,\n+              V v, M m,\n+              UnaryOperation<V, M> defaultImpl) {\n@@ -268,1 +309,1 @@\n-        return defaultImpl.apply(vm);\n+        return defaultImpl.apply(v, m);\n@@ -273,0 +314,5 @@\n+    public interface BinaryOperation<VM extends VectorPayload,\n+                                     M extends VectorMask<?>> {\n+        VM apply(VM v1, VM v2, M m);\n+    }\n+\n@@ -275,4 +321,8 @@\n-    <VM>\n-    VM binaryOp(int oprId, Class<? extends VM> vmClass, Class<?> elementType, int length,\n-                VM vm1, VM vm2,\n-                BiFunction<VM, VM, VM> defaultImpl) {\n+    <VM extends VectorPayload,\n+     M extends VectorMask<E>,\n+     E>\n+    VM binaryOp(int oprId,\n+                Class<? extends VM> vmClass, Class<? extends M> mClass, Class<E> eClass,\n+                int length,\n+                VM v1, VM v2, M m,\n+                BinaryOperation<VM, M> defaultImpl) {\n@@ -280,1 +330,1 @@\n-        return defaultImpl.apply(vm1, vm2);\n+        return defaultImpl.apply(v1, v2, m);\n@@ -285,2 +335,3 @@\n-    public interface TernaryOperation<V> {\n-        V apply(V v1, V v2, V v3);\n+    public interface TernaryOperation<V extends Vector<?>,\n+                                      M extends VectorMask<?>> {\n+        V apply(V v1, V v2, V v3, M m);\n@@ -291,4 +342,8 @@\n-    <VM>\n-    VM ternaryOp(int oprId, Class<? extends VM> vmClass, Class<?> elementType, int length,\n-                 VM vm1, VM vm2, VM vm3,\n-                 TernaryOperation<VM> defaultImpl) {\n+    <V extends Vector<E>,\n+     M extends VectorMask<E>,\n+     E>\n+    V ternaryOp(int oprId,\n+                Class<? extends V> vClass, Class<? extends M> mClass, Class<E> eClass,\n+                int length,\n+                V v1, V v2, V v3, M m,\n+                TernaryOperation<V, M> defaultImpl) {\n@@ -296,1 +351,1 @@\n-        return defaultImpl.apply(vm1, vm2, vm3);\n+        return defaultImpl.apply(v1, v2, v3, m);\n@@ -303,2 +358,4 @@\n-    public interface LoadOperation<C, V, E, S extends VectorSpecies<E>> {\n-        V load(C container, int index, S s);\n+    public interface LoadOperation<C,\n+                                   VM extends VectorPayload,\n+                                   S extends VectorSpecies<?>> {\n+        VM load(C container, int index, S s);\n@@ -309,5 +366,9 @@\n-    <C, VM, E, S extends VectorSpecies<E>>\n-    VM load(Class<? extends VM> vmClass, Class<E> E, int length,\n-           Object base, long offset,    \/\/ Unsafe addressing\n-           C container, int index, S s,     \/\/ Arguments for default implementation\n-           LoadOperation<C, VM, E, S> defaultImpl) {\n+    <C,\n+     VM extends VectorPayload,\n+     E,\n+     S extends VectorSpecies<E>>\n+    VM load(Class<? extends VM> vmClass, Class<E> eClass,\n+            int length,\n+            Object base, long offset,\n+            C container, int index, S s,\n+            LoadOperation<C, VM, S> defaultImpl) {\n@@ -320,2 +381,5 @@\n-    public interface LoadVectorOperationWithMap<C, V extends Vector<?>, E, S extends VectorSpecies<E>> {\n-        V loadWithMap(C container, int index, int[] indexMap, int indexM, S s);\n+    public interface LoadVectorMaskedOperation<C,\n+                                               V extends Vector<?>,\n+                                               S extends VectorSpecies<?>,\n+                                               M extends VectorMask<?>> {\n+        V load(C container, int index, S s, M m);\n@@ -326,3 +390,35 @@\n-    <C, V extends Vector<?>, W extends Vector<Integer>, E, S extends VectorSpecies<E>>\n-    V loadWithMap(Class<?> vectorClass, Class<E> E, int length, Class<?> vectorIndexClass,\n-                  Object base, long offset, \/\/ Unsafe addressing\n+    <C,\n+     V extends Vector<?>,\n+     E,\n+     S extends VectorSpecies<E>,\n+     M extends VectorMask<E>>\n+    V loadMasked(Class<? extends V> vClass, Class<M> mClass, Class<E> eClass,\n+                 int length,\n+                 Object base, long offset,\n+                 M m, C container, int index, S s,\n+                 LoadVectorMaskedOperation<C, V, S, M> defaultImpl) {\n+        assert isNonCapturingLambda(defaultImpl) : defaultImpl;\n+        return defaultImpl.load(container, index, s, m);\n+    }\n+\n+    \/* ============================================================================ *\/\n+\n+    public interface LoadVectorOperationWithMap<C,\n+                                                V extends Vector<?>,\n+                                                S extends VectorSpecies<?>,\n+                                                M extends VectorMask<?>> {\n+        V loadWithMap(C container, int index, int[] indexMap, int indexM, S s, M m);\n+    }\n+\n+    @IntrinsicCandidate\n+    public static\n+    <C,\n+     V extends Vector<?>,\n+     W extends Vector<Integer>,\n+     S extends VectorSpecies<E>,\n+     M extends VectorMask<E>,\n+     E>\n+    V loadWithMap(Class<? extends V> vClass, Class<M> mClass, Class<E> eClass,\n+                  int length,\n+                  Class<? extends Vector<Integer>> vectorIndexClass,\n+                  Object base, long offset,\n@@ -330,2 +426,2 @@\n-                  C container, int index, int[] indexMap, int indexM, S s, \/\/ Arguments for default implementation\n-                  LoadVectorOperationWithMap<C, V, E, S> defaultImpl) {\n+                  M m, C container, int index, int[] indexMap, int indexM, S s,\n+                  LoadVectorOperationWithMap<C, V, S, M> defaultImpl) {\n@@ -333,1 +429,1 @@\n-        return defaultImpl.loadWithMap(container, index, indexMap, indexM, s);\n+        return defaultImpl.loadWithMap(container, index, indexMap, indexM, s, m);\n@@ -338,1 +434,2 @@\n-    public interface StoreVectorOperation<C, V extends Vector<?>> {\n+    public interface StoreVectorOperation<C,\n+                                          V extends Vector<?>> {\n@@ -344,5 +441,6 @@\n-    <C, V extends Vector<?>>\n-    void store(Class<?> vectorClass, Class<?> elementType, int length,\n-               Object base, long offset,    \/\/ Unsafe addressing\n-               V v,\n-               C container, int index,      \/\/ Arguments for default implementation\n+    <C,\n+     V extends Vector<?>>\n+    void store(Class<?> vClass, Class<?> eClass,\n+               int length,\n+               Object base, long offset,\n+               V v, C container, int index,\n@@ -354,0 +452,21 @@\n+    public interface StoreVectorMaskedOperation<C,\n+                                                V extends Vector<?>,\n+                                                M extends VectorMask<?>> {\n+        void store(C container, int index, V v, M m);\n+    }\n+\n+    @IntrinsicCandidate\n+    public static\n+    <C,\n+     V extends Vector<E>,\n+     M extends VectorMask<E>,\n+     E>\n+    void storeMasked(Class<? extends V> vClass, Class<M> mClass, Class<E> eClass,\n+                     int length,\n+                     Object base, long offset,\n+                     V v, M m, C container, int index,\n+                     StoreVectorMaskedOperation<C, V, M> defaultImpl) {\n+        assert isNonCapturingLambda(defaultImpl) : defaultImpl;\n+        defaultImpl.store(container, index, v, m);\n+    }\n+\n@@ -356,2 +475,4 @@\n-    public interface StoreVectorOperationWithMap<C, V extends Vector<?>> {\n-        void storeWithMap(C container, int index, V v, int[] indexMap, int indexM);\n+    public interface StoreVectorOperationWithMap<C,\n+                                                 V extends Vector<?>,\n+                                                 M extends VectorMask<?>> {\n+        void storeWithMap(C container, int index, V v, int[] indexMap, int indexM, M m);\n@@ -362,6 +483,12 @@\n-    <C, V extends Vector<?>, W extends Vector<Integer>>\n-    void storeWithMap(Class<?> vectorClass, Class<?> elementType, int length, Class<?> vectorIndexClass,\n-                      Object base, long offset,    \/\/ Unsafe addressing\n-                      W index_vector, V v,\n-                      C container, int index, int[] indexMap, int indexM, \/\/ Arguments for default implementation\n-                      StoreVectorOperationWithMap<C, V> defaultImpl) {\n+    <C,\n+     V extends Vector<E>,\n+     W extends Vector<Integer>,\n+     M extends VectorMask<E>,\n+     E>\n+    void storeWithMap(Class<? extends V> vClass, Class<M> mClass, Class<E> eClass,\n+                      int length,\n+                      Class<? extends Vector<Integer>> vectorIndexClass,\n+                      Object base, long offset,\n+                      W index_vector,\n+                      V v, M m, C container, int index, int[] indexMap, int indexM,\n+                      StoreVectorOperationWithMap<C, V, M> defaultImpl) {\n@@ -369,1 +496,1 @@\n-        defaultImpl.storeWithMap(container, index, v, indexMap, indexM);\n+        defaultImpl.storeWithMap(container, index, v, indexMap, indexM, m);\n@@ -376,4 +503,7 @@\n-    <VM>\n-    boolean test(int cond, Class<?> vmClass, Class<?> elementType, int length,\n-                 VM vm1, VM vm2,\n-                 BiFunction<VM, VM, Boolean> defaultImpl) {\n+    <M extends VectorMask<E>,\n+     E>\n+    boolean test(int cond,\n+                 Class<?> mClass, Class<?> eClass,\n+                 int length,\n+                 M m1, M m2,\n+                 BiFunction<M, M, Boolean> defaultImpl) {\n@@ -381,1 +511,1 @@\n-        return defaultImpl.apply(vm1, vm2);\n+        return defaultImpl.apply(m1, m2);\n@@ -386,2 +516,3 @@\n-    public interface VectorCompareOp<V,M> {\n-        M apply(int cond, V v1, V v2);\n+    public interface VectorCompareOp<V extends Vector<?>,\n+                                     M extends VectorMask<?>> {\n+        M apply(int cond, V v1, V v2, M m);\n@@ -391,6 +522,9 @@\n-    public static <V extends Vector<E>,\n-                   M extends VectorMask<E>,\n-                   E>\n-    M compare(int cond, Class<? extends V> vectorClass, Class<M> maskClass, Class<?> elementType, int length,\n-              V v1, V v2,\n-              VectorCompareOp<V,M> defaultImpl) {\n+    public static\n+    <V extends Vector<E>,\n+     M extends VectorMask<E>,\n+     E>\n+    M compare(int cond,\n+              Class<? extends V> vectorClass, Class<M> mClass, Class<E> eClass,\n+              int length,\n+              V v1, V v2, M m,\n+              VectorCompareOp<V, M> defaultImpl) {\n@@ -398,1 +532,1 @@\n-        return defaultImpl.apply(cond, v1, v2);\n+        return defaultImpl.apply(cond, v1, v2, m);\n@@ -402,5 +536,4 @@\n-\n-    public interface VectorRearrangeOp<V extends Vector<E>,\n-            Sh extends VectorShuffle<E>,\n-            E> {\n-        V apply(V v1, Sh shuffle);\n+    public interface VectorRearrangeOp<V extends Vector<?>,\n+                                       SH extends VectorShuffle<?>,\n+                                       M extends VectorMask<?>> {\n+        V apply(V v, SH sh, M m);\n@@ -412,5 +545,7 @@\n-            Sh extends VectorShuffle<E>,\n-            E>\n-    V rearrangeOp(Class<? extends V> vectorClass, Class<Sh> shuffleClass, Class<?> elementType, int vlen,\n-                  V v1, Sh sh,\n-                  VectorRearrangeOp<V,Sh, E> defaultImpl) {\n+     SH extends VectorShuffle<E>,\n+     M  extends VectorMask<E>,\n+     E>\n+    V rearrangeOp(Class<? extends V> vClass, Class<SH> shClass, Class<M> mClass, Class<E> eClass,\n+                  int length,\n+                  V v, SH sh, M m,\n+                  VectorRearrangeOp<V, SH, M> defaultImpl) {\n@@ -418,1 +553,1 @@\n-        return defaultImpl.apply(v1, sh);\n+        return defaultImpl.apply(v, sh, m);\n@@ -423,4 +558,3 @@\n-    public interface VectorBlendOp<V extends Vector<E>,\n-            M extends VectorMask<E>,\n-            E> {\n-        V apply(V v1, V v2, M mask);\n+    public interface VectorBlendOp<V extends Vector<?>,\n+                                   M extends VectorMask<?>> {\n+        V apply(V v1, V v2, M m);\n@@ -434,1 +568,2 @@\n-    V blend(Class<? extends V> vectorClass, Class<M> maskClass, Class<?> elementType, int length,\n+    V blend(Class<? extends V> vClass, Class<M> mClass, Class<E> eClass,\n+            int length,\n@@ -436,1 +571,1 @@\n-            VectorBlendOp<V,M, E> defaultImpl) {\n+            VectorBlendOp<V, M> defaultImpl) {\n@@ -443,2 +578,3 @@\n-    public interface VectorBroadcastIntOp<V extends Vector<?>> {\n-        V apply(V v, int n);\n+    public interface VectorBroadcastIntOp<V extends Vector<?>,\n+                                          M extends VectorMask<?>> {\n+        V apply(V v, int n, M m);\n@@ -449,4 +585,8 @@\n-    <V extends Vector<?>>\n-    V broadcastInt(int opr, Class<? extends V> vectorClass, Class<?> elementType, int length,\n-                   V v, int n,\n-                   VectorBroadcastIntOp<V> defaultImpl) {\n+    <V extends Vector<E>,\n+     M extends VectorMask<E>,\n+     E>\n+    V broadcastInt(int opr,\n+                   Class<? extends V> vClass, Class<? extends M> mClass, Class<E> eClass,\n+                   int length,\n+                   V v, int n, M m,\n+                   VectorBroadcastIntOp<V, M> defaultImpl) {\n@@ -454,1 +594,1 @@\n-        return defaultImpl.apply(v, n);\n+        return defaultImpl.apply(v, n, m);\n@@ -459,2 +599,4 @@\n-    public interface VectorConvertOp<VOUT, VIN, S> {\n-        VOUT apply(VIN v, S species);\n+    public interface VectorConvertOp<VOUT extends VectorPayload,\n+                                     VIN extends VectorPayload,\n+                                     S extends VectorSpecies<?>> {\n+        VOUT apply(VIN v, S s);\n@@ -472,2 +614,2 @@\n-              Class<?> fromVectorClass, Class<?> fromElementType, int fromVLen,\n-              Class<?>   toVectorClass, Class<?>   toElementType, int   toVLen,\n+              Class<?> fromVectorClass, Class<?> fromeClass, int fromVLen,\n+              Class<?>   toVectorClass, Class<?>   toeClass, int   toVLen,\n@@ -483,1 +625,3 @@\n-    public static <V> V maybeRebox(V v) {\n+    public static\n+    <VP extends VectorPayload>\n+    VP maybeRebox(VP v) {\n@@ -491,2 +635,2 @@\n-    public interface VectorMaskOp<M> {\n-        int apply(M m);\n+    public interface VectorMaskOp<M extends VectorMask<?>> {\n+        long apply(M m);\n@@ -497,3 +641,7 @@\n-    <E, M>\n-    int maskReductionCoerced(int oper, Class<? extends M> maskClass, Class<?> elemClass, int length, M m,\n-               VectorMaskOp<M> defaultImpl) {\n+    <M extends VectorMask<E>,\n+     E>\n+    long maskReductionCoerced(int oper,\n+                              Class<? extends M> mClass, Class<?> eClass,\n+                              int length,\n+                              M m,\n+                              VectorMaskOp<M> defaultImpl) {\n","filename":"src\/java.base\/share\/classes\/jdk\/internal\/vm\/vector\/VectorSupport.java","additions":282,"deletions":134,"binary":false,"changes":416,"status":"modified"},{"patch":"@@ -372,0 +372,1 @@\n+    uses java.net.spi.InetAddressResolverProvider;\n","filename":"src\/java.base\/share\/classes\/module-info.java","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -0,0 +1,53 @@\n+\/*\n+ * Copyright (c) 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.  Oracle designates this\n+ * particular file as subject to the \"Classpath\" exception as provided\n+ * by Oracle in the LICENSE file that accompanied this code.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+package sun.net;\n+\n+import java.net.spi.InetAddressResolver;\n+import java.net.spi.InetAddressResolverProvider;\n+import java.util.function.Supplier;\n+\n+public final class ResolverProviderConfiguration implements\n+        InetAddressResolverProvider.Configuration {\n+\n+    private final InetAddressResolver builtinResolver;\n+    private final Supplier<String> localHostNameSupplier;\n+\n+    public ResolverProviderConfiguration(InetAddressResolver builtinResolver,\n+                                         Supplier<String> localHostNameSupplier) {\n+        this.builtinResolver = builtinResolver;\n+        this.localHostNameSupplier = localHostNameSupplier;\n+    }\n+\n+    @Override\n+    public InetAddressResolver builtinResolver() {\n+        return builtinResolver;\n+    }\n+\n+    @Override\n+    public String lookupLocalHostName() {\n+        return localHostNameSupplier.get();\n+    }\n+}\n","filename":"src\/java.base\/share\/classes\/sun\/net\/ResolverProviderConfiguration.java","additions":53,"deletions":0,"binary":false,"changes":53,"status":"added"},{"patch":"@@ -27,0 +27,1 @@\n+\n@@ -92,1 +93,1 @@\n-        return (FileNameMap)mt;\n+        return mt;\n","filename":"src\/java.base\/share\/classes\/sun\/net\/www\/MimeTable.java","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -80,1 +80,1 @@\n-            return (AuthenticationInfo)list.get (0);\n+            return list.get(0);\n","filename":"src\/java.base\/share\/classes\/sun\/net\/www\/protocol\/http\/AuthCacheImpl.java","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -278,1 +278,0 @@\n-        ObjectIdentifier oid;\n@@ -283,0 +282,3 @@\n+            if (PKCS9Attribute.PKCS9_OIDS[i] == null) {\n+                continue;\n+            }\n@@ -326,1 +328,0 @@\n-        ObjectIdentifier oid;\n@@ -331,0 +332,3 @@\n+            if (PKCS9Attribute.PKCS9_OIDS[i] == null) {\n+                continue;\n+            }\n@@ -341,1 +345,1 @@\n-            sb.append(value.toString());\n+            sb.append(value);\n","filename":"src\/java.base\/share\/classes\/sun\/security\/pkcs\/PKCS9Attributes.java","additions":7,"deletions":3,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1996, 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1996, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -125,1 +125,1 @@\n-                return (DSAParams)paramSpec;\n+                return paramSpec;\n","filename":"src\/java.base\/share\/classes\/sun\/security\/provider\/DSAPublicKey.java","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -294,1 +294,0 @@\n-                        (Collection<? extends Certificate>)\n","filename":"src\/java.base\/share\/classes\/sun\/security\/provider\/certpath\/PKIX.java","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -795,2 +795,1 @@\n-        RevocationStatus rs =\n-            (RevocationStatus)response.getSingleResponse(certId);\n+        RevocationStatus rs = response.getSingleResponse(certId);\n","filename":"src\/java.base\/share\/classes\/sun\/security\/provider\/certpath\/RevocationChecker.java","additions":1,"deletions":2,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -125,1 +125,1 @@\n-        readCipher.dispose();\n+        this.readCipher.dispose();\n","filename":"src\/java.base\/share\/classes\/sun\/security\/ssl\/InputRecord.java","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -144,0 +144,5 @@\n+    \/\/ SSLEngine and SSLSocket\n+    void disposeWriteCipher() {\n+        throw new UnsupportedOperationException();\n+    }\n+\n@@ -193,1 +198,1 @@\n-            writeCipher.dispose();\n+            disposeWriteCipher();\n@@ -222,1 +227,1 @@\n-            writeCipher.dispose();\n+            disposeWriteCipher();\n","filename":"src\/java.base\/share\/classes\/sun\/security\/ssl\/OutputRecord.java","additions":7,"deletions":2,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -154,0 +154,9 @@\n+    @Override\n+    void disposeWriteCipher() {\n+        if (fragmenter == null) {\n+            writeCipher.dispose();\n+        } else {\n+            fragmenter.queueUpCipherDispose();\n+        }\n+    }\n+\n@@ -364,0 +373,1 @@\n+        boolean         disposeCipher;\n@@ -425,0 +435,9 @@\n+        void queueUpCipherDispose() {\n+            RecordMemo lastMemo = handshakeMemos.peekLast();\n+            if (lastMemo != null) {\n+                lastMemo.disposeCipher = true;\n+            } else {\n+                writeCipher.dispose();\n+            }\n+        }\n+\n@@ -524,0 +543,3 @@\n+            if (memo.disposeCipher) {\n+                memo.encodeCipher.dispose();\n+            }\n","filename":"src\/java.base\/share\/classes\/sun\/security\/ssl\/SSLEngineOutputRecord.java","additions":22,"deletions":0,"binary":false,"changes":22,"status":"modified"},{"patch":"@@ -1039,1 +1039,1 @@\n-        return (java.security.cert.Certificate[])peerCerts.clone();\n+        return peerCerts.clone();\n@@ -1057,2 +1057,1 @@\n-        return (localCerts == null ? null :\n-            (java.security.cert.Certificate[])localCerts.clone());\n+        return (localCerts == null ? null : localCerts.clone());\n","filename":"src\/java.base\/share\/classes\/sun\/security\/ssl\/SSLSessionImpl.java","additions":2,"deletions":3,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -246,0 +246,5 @@\n+    @Override\n+    void disposeWriteCipher() {\n+        writeCipher.dispose();\n+    }\n+\n","filename":"src\/java.base\/share\/classes\/sun\/security\/ssl\/SSLSocketOutputRecord.java","additions":5,"deletions":0,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -108,0 +108,1 @@\n+    private static final int KU_NON_REPUDIATION = 1;\n@@ -359,1 +360,3 @@\n-        if (checkKeyUsage(cert, KU_SIGNATURE) == false) {\n+        \/\/ KU and EKU should be consistent\n+        if (!checkKeyUsage(cert, KU_SIGNATURE)\n+                && !checkKeyUsage(cert, KU_NON_REPUDIATION)) {\n@@ -361,1 +364,1 @@\n-                (\"KeyUsage does not allow digital signatures\",\n+                (\"KeyUsage does not allow digital signatures or non repudiation\",\n","filename":"src\/java.base\/share\/classes\/sun\/security\/validator\/EndEntityChecker.java","additions":5,"deletions":2,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2003, 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2003, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -83,1 +83,1 @@\n-        BaseCalendar.Date d = (BaseCalendar.Date) gcal.newCalendarDate(null);\n+        BaseCalendar.Date d = gcal.newCalendarDate(null);\n","filename":"src\/java.base\/share\/classes\/sun\/util\/calendar\/Era.java","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -642,15 +642,0 @@\n-static jboolean isJNUEncodingSupported = JNI_FALSE;\n-static jboolean jnuEncodingSupported(JNIEnv *env) {\n-    jboolean exe;\n-    if (isJNUEncodingSupported == JNI_TRUE) {\n-        return JNI_TRUE;\n-    }\n-    isJNUEncodingSupported = (jboolean) JNU_CallStaticMethodByName (\n-                                    env, &exe,\n-                                    \"java\/nio\/charset\/Charset\",\n-                                    \"isSupported\",\n-                                    \"(Ljava\/lang\/String;)Z\",\n-                                    jnuEncoding).z;\n-    return isJNUEncodingSupported;\n-}\n-\n@@ -674,16 +659,2 @@\n-        if (jnuEncodingSupported(env)) {\n-            result = (*env)->NewObject(env, strClazz,\n-                                       String_init_ID, bytes, jnuEncoding);\n-        } else {\n-            \/*If the encoding specified in sun.jnu.encoding is not endorsed\n-              by \"Charset.isSupported\" we have to fall back to use String(byte[])\n-              explicitly here without specifying the encoding name, in which the\n-              StringCoding class will pickup the iso-8859-1 as the fallback\n-              converter for us.\n-             *\/\n-            jmethodID mid = (*env)->GetMethodID(env, strClazz,\n-                                                \"<init>\", \"([B)V\");\n-            if (mid != NULL) {\n-                result = (*env)->NewObject(env, strClazz, mid, bytes);\n-            }\n-        }\n+        result = (*env)->NewObject(env, strClazz,\n+                                   String_init_ID, bytes, jnuEncoding);\n@@ -769,0 +740,1 @@\n+            jboolean exe;\n@@ -772,2 +744,20 @@\n-            fastEncoding = NO_FAST_ENCODING;\n-            jnuEncoding = (jstring)(*env)->NewGlobalRef(env, enc);\n+\n+            if ((jboolean) JNU_CallStaticMethodByName (\n+                                            env, &exe,\n+                                            \"java\/nio\/charset\/Charset\",\n+                                            \"isSupported\",\n+                                            \"(Ljava\/lang\/String;)Z\",\n+                                            enc).z == JNI_TRUE) {\n+                fastEncoding = NO_FAST_ENCODING;\n+                jnuEncoding = (jstring)(*env)->NewGlobalRef(env, enc);\n+            } else {\n+                \/\/ jnuEncoding falls back to UTF-8\n+                jstring utf8 = (*env)->NewStringUTF(env, \"UTF-8\");\n+                if (utf8 == NULL) {\n+                    (*env)->DeleteLocalRef(env, enc);\n+                    return;\n+                }\n+                fastEncoding = FAST_UTF_8;\n+                jnuEncoding = (jstring)(*env)->NewGlobalRef(env, utf8);\n+                (*env)->DeleteLocalRef(env, utf8);\n+            }\n@@ -825,10 +815,12 @@\n-    if (jnuEncodingSupported(env)) {\n-        hab = (*env)->CallObjectMethod(env, jstr, String_getBytes_ID, jnuEncoding);\n-    } else {\n-        jmethodID mid;\n-        jclass strClazz = JNU_ClassString(env);\n-        CHECK_NULL_RETURN(strClazz, 0);\n-        mid = (*env)->GetMethodID(env, strClazz,\n-                                       \"getBytes\", \"()[B\");\n-        if (mid != NULL) {\n-            hab = (*env)->CallObjectMethod(env, jstr, mid);\n+    hab = (*env)->CallObjectMethod(env, jstr, String_getBytes_ID, jnuEncoding);\n+    if (hab != 0) {\n+        if (!(*env)->ExceptionCheck(env)) {\n+            jint len = (*env)->GetArrayLength(env, hab);\n+            result = MALLOC_MIN4(len);\n+            if (result == 0) {\n+                JNU_ThrowOutOfMemoryError(env, 0);\n+                (*env)->DeleteLocalRef(env, hab);\n+                return 0;\n+            }\n+            (*env)->GetByteArrayRegion(env, hab, 0, len, (jbyte *)result);\n+            result[len] = 0; \/* NULL-terminate *\/\n@@ -836,1 +828,0 @@\n-    }\n@@ -838,10 +829,1 @@\n-    if (!(*env)->ExceptionCheck(env)) {\n-        jint len = (*env)->GetArrayLength(env, hab);\n-        result = MALLOC_MIN4(len);\n-        if (result == 0) {\n-            JNU_ThrowOutOfMemoryError(env, 0);\n-            (*env)->DeleteLocalRef(env, hab);\n-            return 0;\n-        }\n-        (*env)->GetByteArrayRegion(env, hab, 0, len, (jbyte *)result);\n-        result[len] = 0; \/* NULL-terminate *\/\n+        (*env)->DeleteLocalRef(env, hab);\n@@ -849,2 +831,0 @@\n-\n-    (*env)->DeleteLocalRef(env, hab);\n","filename":"src\/java.base\/share\/native\/libjava\/jni_util.c","additions":36,"deletions":56,"binary":false,"changes":92,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2016, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -42,1 +42,0 @@\n-jfieldID ia_preferIPv6AddressID;\n@@ -64,2 +63,0 @@\n-        ia_preferIPv6AddressID = (*env)->GetStaticFieldID(env, ia_class, \"preferIPv6Address\", \"I\");\n-        CHECK_NULL(ia_preferIPv6AddressID);\n@@ -78,0 +75,9 @@\n+\n+\/*\n+ * Class:     java_net_InetAddress\n+ * Method:    isIPv4Available\n+ *\/\n+JNIEXPORT jboolean JNICALL\n+Java_java_net_InetAddress_isIPv4Available(JNIEnv *env, jclass clazz) {\n+    return ipv4_available();\n+}\n","filename":"src\/java.base\/share\/native\/libnet\/InetAddress.c","additions":10,"deletions":4,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -29,0 +29,1 @@\n+#include \"java_net_spi_InetAddressResolver_LookupPolicy.h\"\n@@ -335,0 +336,20 @@\n+\n+int lookupCharacteristicsToAddressFamily(int characteristics) {\n+    int ipv4 = characteristics & java_net_spi_InetAddressResolver_LookupPolicy_IPV4;\n+    int ipv6 = characteristics & java_net_spi_InetAddressResolver_LookupPolicy_IPV6;\n+\n+    if (ipv4 != 0 && ipv6 == 0) {\n+        return AF_INET;\n+    }\n+\n+    if (ipv4 == 0 && ipv6 != 0) {\n+        return AF_INET6;\n+    }\n+    return AF_UNSPEC;\n+}\n+\n+int addressesInSystemOrder(int characteristics) {\n+    return (characteristics &\n+           (java_net_spi_InetAddressResolver_LookupPolicy_IPV4_FIRST |\n+            java_net_spi_InetAddressResolver_LookupPolicy_IPV6_FIRST)) == 0;\n+}\n","filename":"src\/java.base\/share\/native\/libnet\/net_util.c","additions":21,"deletions":0,"binary":false,"changes":21,"status":"modified"},{"patch":"@@ -54,1 +54,0 @@\n-extern jfieldID ia_preferIPv6AddressID;\n@@ -195,0 +194,4 @@\n+int lookupCharacteristicsToAddressFamily(int characteristics);\n+\n+int addressesInSystemOrder(int characteristics);\n+\n","filename":"src\/java.base\/share\/native\/libnet\/net_util.h","additions":4,"deletions":1,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2000, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2000, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -39,0 +39,1 @@\n+#include \"java_net_spi_InetAddressResolver_LookupPolicy.h\"\n@@ -41,1 +42,2 @@\n-extern jobjectArray lookupIfLocalhost(JNIEnv *env, const char *hostname, jboolean includeV6);\n+extern jobjectArray lookupIfLocalhost(JNIEnv *env, const char *hostname, jboolean includeV6,\n+                                      int addressesOrder);\n@@ -114,1 +116,3 @@\n-        ret = lookupIfLocalhost(env, hostname, JNI_FALSE);\n+        \/\/ java_net_spi_InetAddressResolver_LookupPolicy_IPV4_FIRST and no ordering is ok\n+        \/\/ here since only AF_INET addresses will be returned.\n+        ret = lookupIfLocalhost(env, hostname, JNI_FALSE, java_net_spi_InetAddressResolver_LookupPolicy_IPV4);\n","filename":"src\/java.base\/unix\/native\/libnet\/Inet4AddressImpl.c","additions":7,"deletions":3,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2000, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2000, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -44,0 +44,2 @@\n+#include \"java_net_spi_InetAddressResolver_LookupPolicy.h\"\n+\n@@ -77,1 +79,1 @@\n-lookupIfLocalhost(JNIEnv *env, const char *hostname, jboolean includeV6)\n+lookupIfLocalhost(JNIEnv *env, const char *hostname, jboolean includeV6, int characteristics)\n@@ -154,1 +156,1 @@\n-    if ((*env)->GetStaticBooleanField(env, ia_class, ia_preferIPv6AddressID)) {\n+    if ((characteristics & java_net_spi_InetAddressResolver_LookupPolicy_IPV6_FIRST) != 0) {\n@@ -207,1 +209,1 @@\n-                                                 jstring host) {\n+                                                 jstring host, jint characteristics) {\n@@ -227,1 +229,1 @@\n-    hints.ai_family = AF_UNSPEC;\n+    hints.ai_family = lookupCharacteristicsToAddressFamily(characteristics);\n@@ -234,1 +236,1 @@\n-        ret = lookupIfLocalhost(env, hostname, JNI_TRUE);\n+        ret = lookupIfLocalhost(env, hostname, JNI_TRUE, characteristics);\n@@ -245,2 +247,0 @@\n-        int addressPreference =\n-            (*env)->GetStaticIntField(env, ia_class, ia_preferIPv6AddressID);;\n@@ -325,1 +325,1 @@\n-        if (addressPreference == java_net_InetAddress_PREFER_IPV6_VALUE) {\n+        if ((characteristics & java_net_spi_InetAddressResolver_LookupPolicy_IPV6_FIRST) != 0) {\n@@ -328,1 +328,1 @@\n-        } else if (addressPreference == java_net_InetAddress_PREFER_IPV4_VALUE) {\n+        } else if ((characteristics & java_net_spi_InetAddressResolver_LookupPolicy_IPV4_FIRST) != 0) {\n@@ -331,1 +331,1 @@\n-        } else if (addressPreference == java_net_InetAddress_PREFER_SYSTEM_VALUE) {\n+        } else {\n@@ -374,1 +374,2 @@\n-            if (addressPreference == java_net_InetAddress_PREFER_SYSTEM_VALUE) {\n+            \/\/ Check if addresses are requested to be returned in SYSTEM order\n+            if (addressesInSystemOrder(characteristics)) {\n","filename":"src\/java.base\/unix\/native\/libnet\/Inet6AddressImpl.c","additions":13,"deletions":12,"binary":false,"changes":25,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2000, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2000, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -32,0 +32,1 @@\n+#include \"java_net_spi_InetAddressResolver_LookupPolicy.h\"\n@@ -59,1 +60,1 @@\n-                                                 jstring host) {\n+                                                 jstring host, jint characteristics) {\n@@ -79,1 +80,1 @@\n-    hints.ai_family = AF_UNSPEC;\n+    hints.ai_family = lookupCharacteristicsToAddressFamily(characteristics);\n@@ -91,2 +92,0 @@\n-        int addressPreference =\n-            (*env)->GetStaticIntField(env, ia_class, ia_preferIPv6AddressID);\n@@ -171,1 +170,1 @@\n-        if (addressPreference == java_net_InetAddress_PREFER_IPV6_VALUE) {\n+        if ((characteristics & java_net_spi_InetAddressResolver_LookupPolicy_IPV6_FIRST) != 0) {\n@@ -174,1 +173,1 @@\n-        } else if (addressPreference == java_net_InetAddress_PREFER_IPV4_VALUE) {\n+        } else if ((characteristics & java_net_spi_InetAddressResolver_LookupPolicy_IPV4_FIRST) != 0) {\n@@ -177,1 +176,1 @@\n-        } else if (addressPreference == java_net_InetAddress_PREFER_SYSTEM_VALUE) {\n+        } else {\n@@ -220,1 +219,2 @@\n-            if (addressPreference == java_net_InetAddress_PREFER_SYSTEM_VALUE) {\n+            \/\/ Check if addresses are requested to be returned in SYSTEM order\n+            if (addressesInSystemOrder(characteristics)) {\n","filename":"src\/java.base\/windows\/native\/libnet\/Inet6AddressImpl.c","additions":9,"deletions":9,"binary":false,"changes":18,"status":"modified"},{"patch":"@@ -315,2 +315,2 @@\n-     * This pattern matches regular identifiers, keywords, restricted\n-     * keywords, restricted identifiers and the literals {@code \"true\"},\n+     * This pattern matches regular identifiers, keywords, contextual\n+     * keywords, and the literals {@code \"true\"},\n@@ -362,2 +362,2 @@\n-     * This method returns {@code true} for <i>restricted\n-     * keywords<\/i> and <i>restricted identifiers<\/i>.\n+     * This method returns {@code true} for <i>contextual\n+     * keywords<\/i>.\n@@ -388,2 +388,2 @@\n-     * This method returns {@code true} for <i>restricted\n-     * keywords<\/i> and <i>restricted identifiers<\/i>.\n+     * This method returns {@code true} for <i>contextual\n+     * keywords<\/i>.\n@@ -412,2 +412,2 @@\n-     * This method returns {@code false} for <i>restricted\n-     * keywords<\/i> and <i>restricted identifiers<\/i>.\n+     * This method returns {@code false} for <i>contextual\n+     * keywords<\/i>.\n@@ -429,2 +429,2 @@\n-     * This method returns {@code false} for <i>restricted\n-     * keywords<\/i> and <i>restricted identifiers<\/i>.\n+     * This method returns {@code false} for <i>contextual\n+     * keywords<\/i>.\n","filename":"src\/java.compiler\/share\/classes\/javax\/lang\/model\/SourceVersion.java","additions":10,"deletions":10,"binary":false,"changes":20,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2011, 2016, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2011, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -284,1 +284,1 @@\n-    class _PreferencesDispatcher extends _AppEventDispatcher<PreferencesHandler> {\n+    static class _PreferencesDispatcher extends _AppEventDispatcher<PreferencesHandler> {\n@@ -297,1 +297,1 @@\n-    class _OpenAppDispatcher extends _QueuingAppEventDispatcher<com.apple.eawt._OpenAppHandler> {\n+    static class _OpenAppDispatcher extends _QueuingAppEventDispatcher<com.apple.eawt._OpenAppHandler> {\n@@ -303,1 +303,1 @@\n-    class _AppReOpenedDispatcher extends _AppEventMultiplexor<AppReopenedListener> {\n+    static class _AppReOpenedDispatcher extends _AppEventMultiplexor<AppReopenedListener> {\n@@ -310,1 +310,1 @@\n-    class _AppForegroundDispatcher extends _BooleanAppEventMultiplexor<AppForegroundListener, AppForegroundEvent> {\n+    static class _AppForegroundDispatcher extends _BooleanAppEventMultiplexor<AppForegroundListener, AppForegroundEvent> {\n@@ -322,1 +322,1 @@\n-    class _HiddenAppDispatcher extends _BooleanAppEventMultiplexor<AppHiddenListener, AppHiddenEvent> {\n+    static class _HiddenAppDispatcher extends _BooleanAppEventMultiplexor<AppHiddenListener, AppHiddenEvent> {\n@@ -334,1 +334,1 @@\n-    class _UserSessionDispatcher extends _BooleanAppEventMultiplexor<UserSessionListener, UserSessionEvent> {\n+    static class _UserSessionDispatcher extends _BooleanAppEventMultiplexor<UserSessionListener, UserSessionEvent> {\n@@ -352,1 +352,1 @@\n-    class _ScreenSleepDispatcher extends _BooleanAppEventMultiplexor<ScreenSleepListener, ScreenSleepEvent> {\n+    static class _ScreenSleepDispatcher extends _BooleanAppEventMultiplexor<ScreenSleepListener, ScreenSleepEvent> {\n@@ -368,1 +368,1 @@\n-    class _SystemSleepDispatcher extends _BooleanAppEventMultiplexor<SystemSleepListener, SystemSleepEvent> {\n+    static class _SystemSleepDispatcher extends _BooleanAppEventMultiplexor<SystemSleepListener, SystemSleepEvent> {\n@@ -384,1 +384,1 @@\n-    class _OpenFileDispatcher extends _QueuingAppEventDispatcher<OpenFilesHandler> {\n+    static class _OpenFileDispatcher extends _QueuingAppEventDispatcher<OpenFilesHandler> {\n@@ -397,1 +397,1 @@\n-    class _PrintFileDispatcher extends _QueuingAppEventDispatcher<PrintFilesHandler> {\n+    static class _PrintFileDispatcher extends _QueuingAppEventDispatcher<PrintFilesHandler> {\n@@ -409,1 +409,1 @@\n-    class _OpenURIDispatcher extends _QueuingAppEventDispatcher<OpenURIHandler> {\n+    static class _OpenURIDispatcher extends _QueuingAppEventDispatcher<OpenURIHandler> {\n@@ -453,1 +453,1 @@\n-    abstract class _AppEventMultiplexor<L> {\n+    abstract static class _AppEventMultiplexor<L> {\n@@ -506,1 +506,1 @@\n-    abstract class _BooleanAppEventMultiplexor<L, E> extends _AppEventMultiplexor<L> {\n+    abstract static class _BooleanAppEventMultiplexor<L, E> extends _AppEventMultiplexor<L> {\n@@ -533,1 +533,1 @@\n-    abstract class _AppEventDispatcher<H> {\n+    abstract static class _AppEventDispatcher<H> {\n@@ -578,1 +578,1 @@\n-    abstract class _QueuingAppEventDispatcher<H> extends _AppEventDispatcher<H> {\n+    abstract static class _QueuingAppEventDispatcher<H> extends _AppEventDispatcher<H> {\n","filename":"src\/java.desktop\/macosx\/classes\/com\/apple\/eawt\/_AppEventHandler.java","additions":16,"deletions":16,"binary":false,"changes":32,"status":"modified"},{"patch":"@@ -2448,1 +2448,1 @@\n-    class JTableExtension extends JTable {\n+    static class JTableExtension extends JTable {\n","filename":"src\/java.desktop\/macosx\/classes\/com\/apple\/laf\/AquaFileChooserUI.java","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -281,1 +281,1 @@\n-    abstract class QuickSort {\n+    abstract static class QuickSort {\n@@ -341,1 +341,1 @@\n-    class QuickSortNames extends QuickSort {\n+    static class QuickSortNames extends QuickSort {\n@@ -349,1 +349,1 @@\n-    class QuickSortDates extends QuickSort {\n+    static class QuickSortDates extends QuickSort {\n@@ -356,1 +356,1 @@\n-    class SortableFile \/* extends FileView *\/{\n+    static class SortableFile \/* extends FileView *\/{\n","filename":"src\/java.desktop\/macosx\/classes\/com\/apple\/laf\/AquaFileSystemModel.java","additions":4,"deletions":4,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2011, 2014, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2011, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -190,1 +190,1 @@\n-    class AquaDockingDesktopManager extends AquaInternalFrameManager {\n+    static class AquaDockingDesktopManager extends AquaInternalFrameManager {\n","filename":"src\/java.desktop\/macosx\/classes\/com\/apple\/laf\/AquaInternalFramePaneUI.java","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -3264,1 +3264,1 @@\n-    private class ScrollableTabButton extends javax.swing.plaf.basic.BasicArrowButton implements UIResource, SwingConstants {\n+    private static class ScrollableTabButton extends javax.swing.plaf.basic.BasicArrowButton implements UIResource, SwingConstants {\n","filename":"src\/java.desktop\/macosx\/classes\/com\/apple\/laf\/AquaTabbedPaneCopyFromBasicUI.java","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -113,1 +113,1 @@\n-    private class Tracer extends MTLRenderer {\n+    private static class Tracer extends MTLRenderer {\n","filename":"src\/java.desktop\/macosx\/classes\/sun\/java2d\/metal\/MTLRenderer.java","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -167,18 +167,12 @@\n-                    \/\/ At least for now don't handle combo box menu state changes.\n-                    \/\/ This may change when later fixing issues which currently\n-                    \/\/ exist for combo boxes, but for now the following is only\n-                    \/\/ for JPopupMenus, not for combobox menus.\n-                    if (parentRole != AccessibleRole.COMBO_BOX) {\n-                        if (thisRole == AccessibleRole.POPUP_MENU) {\n-                            if ( newValue != null &&\n-                                 ((AccessibleState)newValue) == AccessibleState.VISIBLE ) {\n-                                    menuOpened(ptr);\n-                            } else if ( oldValue != null &&\n-                                        ((AccessibleState)oldValue) == AccessibleState.VISIBLE ) {\n-                                menuClosed(ptr);\n-                            }\n-                        } else if (thisRole == AccessibleRole.MENU_ITEM) {\n-                            if ( newValue != null &&\n-                                 ((AccessibleState)newValue) == AccessibleState.FOCUSED ) {\n-                                menuItemSelected(ptr);\n-                            }\n+                    if (thisRole == AccessibleRole.POPUP_MENU) {\n+                        if ( newValue != null &&\n+                                ((AccessibleState)newValue) == AccessibleState.VISIBLE ) {\n+                            menuOpened(ptr);\n+                        } else if ( oldValue != null &&\n+                                ((AccessibleState)oldValue) == AccessibleState.VISIBLE ) {\n+                            menuClosed(ptr);\n+                        }\n+                    } else if (thisRole == AccessibleRole.MENU_ITEM) {\n+                        if ( newValue != null &&\n+                                ((AccessibleState)newValue) == AccessibleState.FOCUSED ) {\n+                            menuItemSelected(ptr);\n","filename":"src\/java.desktop\/macosx\/classes\/sun\/lwawt\/macosx\/CAccessible.java","additions":12,"deletions":18,"binary":false,"changes":30,"status":"modified"},{"patch":"@@ -429,2 +429,1 @@\n-    class OSXPlatformFont extends sun.awt.PlatformFont\n-    {\n+    static class OSXPlatformFont extends sun.awt.PlatformFont {\n","filename":"src\/java.desktop\/macosx\/classes\/sun\/lwawt\/macosx\/LWCToolkit.java","additions":1,"deletions":2,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -1519,48 +1519,0 @@\n-\/*\n- * Class:     sun_lwawt_macosx_CAccessible\n- * Method:    menuOpened\n- * Signature: (I)V\n- *\/\n-JNIEXPORT void JNICALL Java_sun_lwawt_macosx_CAccessible_menuOpened\n-(JNIEnv *env, jclass jklass, jlong element)\n-{\n-JNI_COCOA_ENTER(env);\n-    [ThreadUtilities performOnMainThread:@selector(postMenuOpened)\n-                     on:(JavaComponentAccessibility *)jlong_to_ptr(element)\n-                     withObject:nil\n-                     waitUntilDone:NO];\n-JNI_COCOA_EXIT(env);\n-}\n-\n-\/*\n- * Class:     sun_lwawt_macosx_CAccessible\n- * Method:    menuClosed\n- * Signature: (I)V\n- *\/\n-JNIEXPORT void JNICALL Java_sun_lwawt_macosx_CAccessible_menuClosed\n-(JNIEnv *env, jclass jklass, jlong element)\n-{\n-JNI_COCOA_ENTER(env);\n-    [ThreadUtilities performOnMainThread:@selector(postMenuClosed)\n-                     on:(JavaComponentAccessibility *)jlong_to_ptr(element)\n-                     withObject:nil\n-                     waitUntilDone:NO];\n-JNI_COCOA_EXIT(env);\n-}\n-\n-\/*\n- * Class:     sun_lwawt_macosx_CAccessible\n- * Method:    menuItemSelected\n- * Signature: (I)V\n- *\/\n-JNIEXPORT void JNICALL Java_sun_lwawt_macosx_CAccessible_menuItemSelected\n-(JNIEnv *env, jclass jklass, jlong element)\n-{\n-JNI_COCOA_ENTER(env);\n-    [ThreadUtilities performOnMainThread:@selector(postMenuItemSelected)\n-                     on:(JavaComponentAccessibility *)jlong_to_ptr(element)\n-                     withObject:nil\n-                     waitUntilDone:NO];\n-JNI_COCOA_EXIT(env);\n-}\n-\n","filename":"src\/java.desktop\/macosx\/native\/libawt_lwawt\/awt\/JavaComponentAccessibility.m","additions":0,"deletions":48,"binary":false,"changes":48,"status":"modified"},{"patch":"@@ -125,1 +125,1 @@\n-    rolesMap = [[NSMutableDictionary alloc] initWithCapacity:46];\n+    rolesMap = [[NSMutableDictionary alloc] initWithCapacity:50];\n@@ -155,0 +155,4 @@\n+    [rolesMap setObject:@\"MenuBarAccessibility\" forKey:@\"menubar\"];\n+    [rolesMap setObject:@\"MenuAccessibility\" forKey:@\"menu\"];\n+    [rolesMap setObject:@\"MenuItemAccessibility\" forKey:@\"menuitem\"];\n+    [rolesMap setObject:@\"MenuAccessibility\" forKey:@\"popupmenu\"];\n@@ -1256,0 +1260,48 @@\n+\n+\/*\n+ * Class:     sun_lwawt_macosx_CAccessible\n+ * Method:    menuOpened\n+ * Signature: (I)V\n+ *\/\n+JNIEXPORT void JNICALL Java_sun_lwawt_macosx_CAccessible_menuOpened\n+    (JNIEnv *env, jclass jklass, jlong element)\n+{\n+    JNI_COCOA_ENTER(env);\n+        [ThreadUtilities performOnMainThread:@selector(postMenuOpened)\n+                         on:(CommonComponentAccessibility *)jlong_to_ptr(element)\n+                         withObject:nil\n+                         waitUntilDone:NO];\n+    JNI_COCOA_EXIT(env);\n+}\n+\n+\/*\n+ * Class:     sun_lwawt_macosx_CAccessible\n+ * Method:    menuClosed\n+ * Signature: (I)V\n+ *\/\n+JNIEXPORT void JNICALL Java_sun_lwawt_macosx_CAccessible_menuClosed\n+    (JNIEnv *env, jclass jklass, jlong element)\n+{\n+    JNI_COCOA_ENTER(env);\n+        [ThreadUtilities performOnMainThread:@selector(postMenuClosed)\n+                         on:(CommonComponentAccessibility *)jlong_to_ptr(element)\n+                         withObject:nil\n+                         waitUntilDone:NO];\n+    JNI_COCOA_EXIT(env);\n+}\n+\n+\/*\n+ * Class:     sun_lwawt_macosx_CAccessible\n+ * Method:    menuItemSelected\n+ * Signature: (I)V\n+ *\/\n+JNIEXPORT void JNICALL Java_sun_lwawt_macosx_CAccessible_menuItemSelected\n+    (JNIEnv *env, jclass jklass, jlong element)\n+{\n+    JNI_COCOA_ENTER(env);\n+        [ThreadUtilities performOnMainThread:@selector(postMenuItemSelected)\n+                         on:(CommonComponentAccessibility *)jlong_to_ptr(element)\n+                         withObject:nil\n+                         waitUntilDone:NO];\n+    JNI_COCOA_EXIT(env);\n+}\n","filename":"src\/java.desktop\/macosx\/native\/libawt_lwawt\/awt\/a11y\/CommonComponentAccessibility.m","additions":53,"deletions":1,"binary":false,"changes":54,"status":"modified"},{"patch":"@@ -0,0 +1,35 @@\n+\/*\n+ * Copyright (c) 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.  Oracle designates this\n+ * particular file as subject to the \"Classpath\" exception as provided\n+ * by Oracle in the LICENSE file that accompanied this code.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#import \"CommonComponentAccessibility.h\"\n+#import \"GroupAccessibility.h\"\n+\n+#import <AppKit\/AppKit.h>\n+\n+@interface MenuAccessibility : GroupAccessibility {\n+\n+};\n+- (NSAccessibilityRole _Nonnull)accessibilityRole;\n+@end\n","filename":"src\/java.desktop\/macosx\/native\/libawt_lwawt\/awt\/a11y\/MenuAccessibility.h","additions":35,"deletions":0,"binary":false,"changes":35,"status":"added"},{"patch":"@@ -0,0 +1,45 @@\n+\/*\n+ * Copyright (c) 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.  Oracle designates this\n+ * particular file as subject to the \"Classpath\" exception as provided\n+ * by Oracle in the LICENSE file that accompanied this code.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#import \"MenuAccessibility.h\"\n+\n+\/*\n+ * Implementing a protocol that represents menus both as submenu and as a\n+ * MenuBar components\n+ *\/\n+@implementation MenuAccessibility\n+- (NSAccessibilityRole _Nonnull)accessibilityRole\n+{\n+        return [[[self parent] javaRole] isEqualToString:@\"combobox\"]\n+               ? NSAccessibilityPopUpButtonRole\n+               : NSAccessibilityMenuRole;\n+}\n+\n+- (BOOL)isAccessibilityElement\n+{\n+    return YES;\n+}\n+\n+@end\n","filename":"src\/java.desktop\/macosx\/native\/libawt_lwawt\/awt\/a11y\/MenuAccessibility.m","additions":45,"deletions":0,"binary":false,"changes":45,"status":"added"},{"patch":"@@ -0,0 +1,34 @@\n+\/*\n+ * Copyright (c) 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.  Oracle designates this\n+ * particular file as subject to the \"Classpath\" exception as provided\n+ * by Oracle in the LICENSE file that accompanied this code.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#import \"CommonComponentAccessibility.h\"\n+\n+#import <AppKit\/AppKit.h>\n+\n+@interface MenuBarAccessibility : CommonComponentAccessibility {\n+\n+};\n+- (NSAccessibilityRole _Nonnull)accessibilityRole;\n+@end\n","filename":"src\/java.desktop\/macosx\/native\/libawt_lwawt\/awt\/a11y\/MenuBarAccessibility.h","additions":34,"deletions":0,"binary":false,"changes":34,"status":"added"},{"patch":"@@ -0,0 +1,45 @@\n+\/*\n+ * Copyright (c) 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.  Oracle designates this\n+ * particular file as subject to the \"Classpath\" exception as provided\n+ * by Oracle in the LICENSE file that accompanied this code.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#import \"MenuBarAccessibility.h\"\n+#import \"JNIUtilities.h\"\n+#import \"ThreadUtilities.h\"\n+#import \"sun_lwawt_macosx_CAccessibility.h\"\n+\n+\/*\n+ * This is the protocol for the Menu Bar component\n+ *\/\n+@implementation MenuBarAccessibility\n+- (NSAccessibilityRole _Nonnull)accessibilityRole\n+{\n+    return NSAccessibilityMenuBarRole;\n+}\n+\n+- (BOOL)isAccessibilityElement\n+{\n+    return YES;\n+}\n+\n+@end\n","filename":"src\/java.desktop\/macosx\/native\/libawt_lwawt\/awt\/a11y\/MenuBarAccessibility.m","additions":45,"deletions":0,"binary":false,"changes":45,"status":"added"},{"patch":"@@ -0,0 +1,36 @@\n+\/*\n+ * Copyright (c) 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.  Oracle designates this\n+ * particular file as subject to the \"Classpath\" exception as provided\n+ * by Oracle in the LICENSE file that accompanied this code.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#import \"CommonComponentAccessibility.h\"\n+#import \"ButtonAccessibility.h\"\n+\n+#import <AppKit\/AppKit.h>\n+\n+@interface MenuItemAccessibility : ButtonAccessibility {\n+\n+};\n+- (NSAccessibilityRole _Nonnull)accessibilityRole;\n+- (void)handleAction:(NSMenuItem * _Nonnull)sender;\n+@end\n","filename":"src\/java.desktop\/macosx\/native\/libawt_lwawt\/awt\/a11y\/MenuItemAccessibility.h","additions":36,"deletions":0,"binary":false,"changes":36,"status":"added"},{"patch":"@@ -0,0 +1,62 @@\n+\/*\n+ * Copyright (c) 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.  Oracle designates this\n+ * particular file as subject to the \"Classpath\" exception as provided\n+ * by Oracle in the LICENSE file that accompanied this code.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#import \"MenuItemAccessibility.h\"\n+\n+\/*\n+ * This is the protocol for the MenuItem component.\n+ *\/\n+@implementation MenuItemAccessibility\n+- (NSAccessibilityRole _Nonnull)accessibilityRole\n+{\n+    return NSAccessibilityMenuItemRole;\n+}\n+\n+- (BOOL)isAccessibilityElement\n+{\n+    return YES;\n+}\n+\n+- (BOOL)accessibilityPerformPick\n+{\n+    return [self performAccessibleAction:0];\n+}\n+\n+- (BOOL)accessibilityPerformPress\n+{\n+    return [self performAccessibleAction:0];\n+}\n+\n+- (NSString * _Nullable)accessibilityLabel\n+{\n+    return [super accessibilityLabel];\n+}\n+\n+- (id _Nullable)accessibilityValue\n+{\n+    return [super accessibilityValue];\n+}\n+\n+@end\n","filename":"src\/java.desktop\/macosx\/native\/libawt_lwawt\/awt\/a11y\/MenuItemAccessibility.m","additions":62,"deletions":0,"binary":false,"changes":62,"status":"added"},{"patch":"@@ -141,1 +141,1 @@\n-    class Htable implements Cloneable {\n+    static class Htable implements Cloneable {\n","filename":"src\/java.desktop\/share\/classes\/com\/sun\/imageio\/plugins\/jpeg\/DHTMarkerSegment.java","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -174,1 +174,1 @@\n-    class Qtable implements Cloneable {\n+    static class Qtable implements Cloneable {\n","filename":"src\/java.desktop\/share\/classes\/com\/sun\/imageio\/plugins\/jpeg\/DQTMarkerSegment.java","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -555,1 +555,1 @@\n-    private class IllegalThumbException extends Exception {}\n+    private static class IllegalThumbException extends Exception {}\n@@ -797,1 +797,1 @@\n-    abstract class JFIFThumb implements Cloneable {\n+    abstract static class JFIFThumb implements Cloneable {\n@@ -1114,1 +1114,1 @@\n-    class JFIFThumbJPEG extends JFIFThumb {\n+    static class JFIFThumbJPEG extends JFIFThumb {\n@@ -1237,1 +1237,1 @@\n-        private class ThumbnailReadListener\n+        private static class ThumbnailReadListener\n","filename":"src\/java.desktop\/share\/classes\/com\/sun\/imageio\/plugins\/jpeg\/JFIFMarkerSegment.java","additions":4,"deletions":4,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2001, 2014, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2001, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -202,1 +202,1 @@\n-    class ComponentSpec implements Cloneable {\n+    static class ComponentSpec implements Cloneable {\n","filename":"src\/java.desktop\/share\/classes\/com\/sun\/imageio\/plugins\/jpeg\/SOFMarkerSegment.java","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2001, 2014, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2001, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -28,1 +28,0 @@\n-\/\/import javax.imageio.IIOException;\n@@ -184,1 +183,1 @@\n-    class ScanComponentSpec implements Cloneable {\n+    static class ScanComponentSpec implements Cloneable {\n","filename":"src\/java.desktop\/share\/classes\/com\/sun\/imageio\/plugins\/jpeg\/SOSMarkerSegment.java","additions":2,"deletions":3,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2002, 2014, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2002, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -1301,1 +1301,1 @@\n-    private class OpaqueLabel extends JLabel {\n+    private static class OpaqueLabel extends JLabel {\n","filename":"src\/java.desktop\/share\/classes\/com\/sun\/java\/swing\/plaf\/gtk\/GTKColorChooserPanel.java","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -77,1 +77,1 @@\n-    private class DragPane extends JComponent {\n+    private static class DragPane extends JComponent {\n@@ -88,1 +88,1 @@\n-    private class MotifDesktopManager extends DefaultDesktopManager implements Serializable, UIResource {\n+    private static class MotifDesktopManager extends DefaultDesktopManager implements Serializable, UIResource {\n","filename":"src\/java.desktop\/share\/classes\/com\/sun\/java\/swing\/plaf\/motif\/MotifDesktopPaneUI.java","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1998, 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1998, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -353,1 +353,1 @@\n-    private class EventInfo {\n+    private static class EventInfo {\n@@ -386,1 +386,1 @@\n-    private class ClipInfo {\n+    private static class ClipInfo {\n","filename":"src\/java.desktop\/share\/classes\/com\/sun\/media\/sound\/EventDispatcher.java","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2003, 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2003, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -1027,1 +1027,1 @@\n-    private class ControllerListElement {\n+    private static class ControllerListElement {\n","filename":"src\/java.desktop\/share\/classes\/com\/sun\/media\/sound\/RealTimeSequencer.java","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2007, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2007, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -51,2 +51,1 @@\n-    private class SoftChannelMixerContainer\n-    {\n+    private static class SoftChannelMixerContainer {\n","filename":"src\/java.desktop\/share\/classes\/com\/sun\/media\/sound\/SoftMainMixer.java","additions":2,"deletions":3,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1999, 2018, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1999, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -154,1 +154,1 @@\n-    final class NoCloseInputStream extends InputStream {\n+    static final class NoCloseInputStream extends InputStream {\n","filename":"src\/java.desktop\/share\/classes\/com\/sun\/media\/sound\/SunFileWriter.java","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -77,1 +77,1 @@\n-    class Card implements Serializable {\n+    static class Card implements Serializable {\n","filename":"src\/java.desktop\/share\/classes\/java\/awt\/CardLayout.java","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -3934,1 +3934,1 @@\n-    private class ProxyCapabilities extends ExtendedBufferCapabilities {\n+    private static class ProxyCapabilities extends ExtendedBufferCapabilities {\n","filename":"src\/java.desktop\/share\/classes\/java\/awt\/Component.java","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -578,1 +578,1 @@\n-    class PolygonPathIterator implements PathIterator {\n+    static class PolygonPathIterator implements PathIterator {\n","filename":"src\/java.desktop\/share\/classes\/java\/awt\/Polygon.java","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -743,2 +743,1 @@\n-    class PeerFixer implements AdjustmentListener, java.io.Serializable\n-    {\n+    static class PeerFixer implements AdjustmentListener, java.io.Serializable {\n","filename":"src\/java.desktop\/share\/classes\/java\/awt\/ScrollPane.java","additions":1,"deletions":2,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -2093,1 +2093,1 @@\n-    private class SelectiveAWTEventListener implements AWTEventListener {\n+    private static class SelectiveAWTEventListener implements AWTEventListener {\n","filename":"src\/java.desktop\/share\/classes\/java\/awt\/Toolkit.java","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -122,1 +122,1 @@\n- *     <td>{@link NumericShaper#ARABIC NumericShaper.ARABIC}\n+ *     <th scope=\"row\">{@link NumericShaper#ARABIC NumericShaper.ARABIC}\n@@ -126,1 +126,0 @@\n- *   <\/tr>\n@@ -128,1 +127,1 @@\n- *     <td>{@link NumericShaper.Range#ARABIC}\n+ *     <th scope=\"row\">{@link NumericShaper.Range#ARABIC}\n@@ -135,2 +134,2 @@\n- *     <th scope=\"row\">Tai Tham\n- *     <td>{@link NumericShaper.Range#TAI_THAM_HORA}\n+ *     <th scope=\"rowgroup\">Tai Tham\n+ *     <th scope=\"row\">{@link NumericShaper.Range#TAI_THAM_HORA}\n","filename":"src\/java.desktop\/share\/classes\/java\/awt\/font\/NumericShaper.java","additions":4,"deletions":5,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2014, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -177,1 +177,1 @@\n-    private class BookPage {\n+    private static class BookPage {\n","filename":"src\/java.desktop\/share\/classes\/java\/awt\/print\/Book.java","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2000, 2015, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2000, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -221,1 +221,1 @@\n-    private class ValueData {\n+    private static class ValueData {\n","filename":"src\/java.desktop\/share\/classes\/java\/beans\/XMLEncoder.java","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2000, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2000, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -29,1 +29,0 @@\n-import java.util.Collection;\n@@ -31,1 +30,0 @@\n-import java.util.Iterator;\n@@ -95,1 +93,1 @@\n-    class Element {\n+    static class Element {\n@@ -116,1 +114,1 @@\n-    class Attribute {\n+    static class Attribute {\n@@ -136,1 +134,1 @@\n-    class ObjectValue<T> {\n+    static class ObjectValue<T> {\n","filename":"src\/java.desktop\/share\/classes\/javax\/imageio\/metadata\/IIOMetadataFormatImpl.java","additions":4,"deletions":6,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -230,1 +230,1 @@\n-    private class PrinterStateReasonSet\n+    private static class PrinterStateReasonSet\n@@ -257,1 +257,1 @@\n-    private class PrinterStateReasonSetIterator implements Iterator<PrinterStateReason> {\n+    private static class PrinterStateReasonSetIterator implements Iterator<PrinterStateReason> {\n","filename":"src\/java.desktop\/share\/classes\/javax\/print\/attribute\/standard\/PrinterStateReasons.java","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1999, 2017, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1999, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -477,1 +477,1 @@\n-    private class TargetDataLineInputStream extends InputStream {\n+    private static class TargetDataLineInputStream extends InputStream {\n","filename":"src\/java.desktop\/share\/classes\/javax\/sound\/sampled\/AudioInputStream.java","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -1270,1 +1270,1 @@\n-    private abstract class Spring {\n+    private abstract static class Spring {\n@@ -3170,1 +3170,1 @@\n-    private class GapSpring extends Spring {\n+    private static class GapSpring extends Spring {\n","filename":"src\/java.desktop\/share\/classes\/javax\/swing\/GroupLayout.java","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -1558,1 +1558,1 @@\n-    class DefaultKeySelectionManager implements KeySelectionManager, Serializable {\n+    static class DefaultKeySelectionManager implements KeySelectionManager, Serializable {\n","filename":"src\/java.desktop\/share\/classes\/javax\/swing\/JComboBox.java","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -3451,1 +3451,1 @@\n-    final class ActionStandin implements Action {\n+    static final class ActionStandin implements Action {\n@@ -5504,2 +5504,1 @@\n-    private class ReadObjectCallback implements ObjectInputValidation\n-    {\n+    private static class ReadObjectCallback implements ObjectInputValidation {\n","filename":"src\/java.desktop\/share\/classes\/javax\/swing\/JComponent.java","additions":2,"deletions":3,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -6582,1 +6582,1 @@\n-    private class ThreadSafePrintable implements Printable {\n+    private static class ThreadSafePrintable implements Printable {\n","filename":"src\/java.desktop\/share\/classes\/javax\/swing\/JTable.java","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -32,2 +32,0 @@\n-import java.beans.*;\n-import javax.swing.event.*;\n@@ -374,1 +372,1 @@\n-    class ComponentKeyStrokePair {\n+    static class ComponentKeyStrokePair {\n","filename":"src\/java.desktop\/share\/classes\/javax\/swing\/KeyboardManager.java","additions":1,"deletions":3,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -1815,1 +1815,1 @@\n-    private class DoubleBufferInfo {\n+    private static class DoubleBufferInfo {\n","filename":"src\/java.desktop\/share\/classes\/javax\/swing\/RepaintManager.java","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2015, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -34,0 +34,2 @@\n+import java.awt.Component;\n+import java.awt.Container;\n@@ -36,0 +38,1 @@\n+import java.awt.FocusTraversalPolicy;\n@@ -38,1 +41,1 @@\n-import java.awt.*;\n+import java.awt.Point;\n@@ -350,1 +353,1 @@\n-    private class BasicDesktopManager extends DefaultDesktopManager\n+    private static class BasicDesktopManager extends DefaultDesktopManager\n","filename":"src\/java.desktop\/share\/classes\/javax\/swing\/plaf\/basic\/BasicDesktopPaneUI.java","additions":6,"deletions":3,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1998, 2015, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1998, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -43,1 +43,0 @@\n-import sun.swing.SwingUtilities2;\n@@ -1207,1 +1206,1 @@\n-    class GlobFilter extends FileFilter {\n+    static class GlobFilter extends FileFilter {\n","filename":"src\/java.desktop\/share\/classes\/javax\/swing\/plaf\/basic\/BasicFileChooserUI.java","additions":2,"deletions":3,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2014, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -971,1 +971,1 @@\n-    private class NoFocusButton extends JButton {\n+    private static class NoFocusButton extends JButton {\n","filename":"src\/java.desktop\/share\/classes\/javax\/swing\/plaf\/basic\/BasicInternalFrameTitlePane.java","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -31,2 +31,0 @@\n-import javax.swing.plaf.basic.*;\n-import javax.swing.border.*;\n@@ -37,2 +35,0 @@\n-import java.awt.Container;\n-import java.awt.Dimension;\n@@ -45,3 +41,0 @@\n-import java.beans.PropertyChangeListener;\n-import java.beans.PropertyChangeEvent;\n-\n@@ -50,1 +43,0 @@\n-import sun.swing.DefaultLookup;\n@@ -297,1 +289,1 @@\n-    private class BasicPopupMenuListener implements PopupMenuListener {\n+    private static class BasicPopupMenuListener implements PopupMenuListener {\n","filename":"src\/java.desktop\/share\/classes\/javax\/swing\/plaf\/basic\/BasicPopupMenuUI.java","additions":1,"deletions":9,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2018, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -3948,2 +3948,2 @@\n-    private class ScrollableTabButton extends BasicArrowButton implements UIResource,\n-                                                                            SwingConstants {\n+    private static class ScrollableTabButton extends BasicArrowButton\n+            implements UIResource, SwingConstants {\n","filename":"src\/java.desktop\/share\/classes\/javax\/swing\/plaf\/basic\/BasicTabbedPaneUI.java","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1998, 2014, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1998, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -30,1 +30,0 @@\n-import java.io.Serializable;\n@@ -32,1 +31,0 @@\n-import java.awt.event.*;\n@@ -92,1 +90,1 @@\n-    class EditorBorder extends AbstractBorder {\n+    static class EditorBorder extends AbstractBorder {\n","filename":"src\/java.desktop\/share\/classes\/javax\/swing\/plaf\/metal\/MetalComboBoxEditor.java","additions":2,"deletions":4,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -41,2 +41,0 @@\n-import java.security.AccessController;\n-import java.security.PrivilegedAction;\n@@ -961,1 +959,1 @@\n-    class IndentIcon implements Icon {\n+    static class IndentIcon implements Icon {\n@@ -1364,1 +1362,1 @@\n-    private class AlignedLabel extends JLabel {\n+    private static class AlignedLabel extends JLabel {\n","filename":"src\/java.desktop\/share\/classes\/javax\/swing\/plaf\/metal\/MetalFileChooserUI.java","additions":2,"deletions":4,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1998, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1998, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -1624,1 +1624,1 @@\n-        class ImageGcPair {\n+        static class ImageGcPair {\n","filename":"src\/java.desktop\/share\/classes\/javax\/swing\/plaf\/metal\/MetalIconFactory.java","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -498,1 +498,1 @@\n-    private class LinkProperty implements UIDefaults.ActiveValue, UIResource{\n+    private static class LinkProperty implements UIDefaults.ActiveValue, UIResource{\n","filename":"src\/java.desktop\/share\/classes\/javax\/swing\/plaf\/nimbus\/NimbusLookAndFeel.java","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -1022,1 +1022,1 @@\n-    private final class RuntimeState implements Cloneable {\n+    private static final class RuntimeState implements Cloneable {\n","filename":"src\/java.desktop\/share\/classes\/javax\/swing\/plaf\/nimbus\/NimbusStyle.java","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2002, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2002, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -936,1 +936,1 @@\n-    private class SynthScrollableTabButton extends SynthArrowButton implements\n+    private static class SynthScrollableTabButton extends SynthArrowButton implements\n","filename":"src\/java.desktop\/share\/classes\/javax\/swing\/plaf\/synth\/SynthTabbedPaneUI.java","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2002, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2002, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -716,1 +716,1 @@\n-    private class SynthBooleanTableCellRenderer extends JCheckBox implements\n+    private static class SynthBooleanTableCellRenderer extends JCheckBox implements\n","filename":"src\/java.desktop\/share\/classes\/javax\/swing\/plaf\/synth\/SynthTableUI.java","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -512,1 +512,1 @@\n-    class HighlightInfo implements Highlighter.Highlight {\n+    static class HighlightInfo implements Highlighter.Highlight {\n@@ -536,1 +536,1 @@\n-    class LayeredHighlightInfo extends HighlightInfo {\n+    static class LayeredHighlightInfo extends HighlightInfo {\n","filename":"src\/java.desktop\/share\/classes\/javax\/swing\/text\/DefaultHighlighter.java","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1998, 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1998, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -81,1 +81,1 @@\n-    private class StackItem implements Cloneable {\n+    private static class StackItem implements Cloneable {\n","filename":"src\/java.desktop\/share\/classes\/javax\/swing\/text\/ElementIterator.java","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -55,3 +55,0 @@\n-import java.awt.print.Printable;\n-import java.awt.print.PrinterException;\n-\n@@ -59,1 +56,0 @@\n-import javax.print.attribute.PrintRequestAttributeSet;\n@@ -5112,1 +5108,1 @@\n-    private class DoSetCaretPosition implements Runnable {\n+    private static class DoSetCaretPosition implements Runnable {\n","filename":"src\/java.desktop\/share\/classes\/javax\/swing\/text\/JTextComponent.java","additions":1,"deletions":5,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2017, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -30,1 +30,0 @@\n-import javax.swing.SwingUtilities;\n@@ -327,1 +326,1 @@\n-    final class PosRec {\n+    static final class PosRec {\n@@ -372,1 +371,1 @@\n-    final class UndoPosRef {\n+    static final class UndoPosRef {\n","filename":"src\/java.desktop\/share\/classes\/javax\/swing\/text\/StringContent.java","additions":3,"deletions":4,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -1058,1 +1058,1 @@\n-    class KeyEnumeration implements Enumeration<Object> {\n+    static class KeyEnumeration implements Enumeration<Object> {\n@@ -1100,1 +1100,1 @@\n-    class KeyBuilder {\n+    static class KeyBuilder {\n","filename":"src\/java.desktop\/share\/classes\/javax\/swing\/text\/StyleContext.java","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -1059,1 +1059,1 @@\n-    private class ImageLabelView extends InlineView {\n+    private static class ImageLabelView extends InlineView {\n","filename":"src\/java.desktop\/share\/classes\/javax\/swing\/text\/html\/ImageView.java","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1998, 2018, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1998, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -1694,1 +1694,1 @@\n-    class CellView extends BlockView {\n+    static class CellView extends BlockView {\n","filename":"src\/java.desktop\/share\/classes\/javax\/swing\/text\/html\/TableView.java","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -672,2 +672,1 @@\n-class DiscardingDestination implements Destination\n-{\n+static class DiscardingDestination implements Destination {\n@@ -1074,1 +1073,1 @@\n-class InfoDestination\n+static class InfoDestination\n","filename":"src\/java.desktop\/share\/classes\/javax\/swing\/text\/rtf\/RTFReader.java","additions":2,"deletions":3,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -1333,1 +1333,1 @@\n-    private final class PreorderEnumeration implements Enumeration<TreeNode> {\n+    private static final class PreorderEnumeration implements Enumeration<TreeNode> {\n@@ -1365,1 +1365,1 @@\n-    final class PostorderEnumeration implements Enumeration<TreeNode> {\n+    static final class PostorderEnumeration implements Enumeration<TreeNode> {\n@@ -1401,1 +1401,1 @@\n-    final class BreadthFirstEnumeration implements Enumeration<TreeNode> {\n+    static final class BreadthFirstEnumeration implements Enumeration<TreeNode> {\n@@ -1433,1 +1433,1 @@\n-        final class Queue {\n+        static final class Queue {\n@@ -1437,1 +1437,1 @@\n-            final class QNode {\n+            static final class QNode {\n@@ -1489,1 +1489,1 @@\n-    final class PathBetweenNodesEnumeration implements Enumeration<TreeNode> {\n+    static final class PathBetweenNodesEnumeration implements Enumeration<TreeNode> {\n","filename":"src\/java.desktop\/share\/classes\/javax\/swing\/tree\/DefaultMutableTreeNode.java","additions":6,"deletions":6,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -431,2 +431,1 @@\n-    class PlatformFontCache\n-    {\n+    static class PlatformFontCache {\n","filename":"src\/java.desktop\/share\/classes\/sun\/awt\/PlatformFont.java","additions":1,"deletions":2,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1999, 2018, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1999, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -29,1 +29,1 @@\n-    final class Entry {\n+    static final class Entry {\n","filename":"src\/java.desktop\/share\/classes\/sun\/java2d\/loops\/RenderCache.java","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2003, 2011, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2003, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -111,1 +111,1 @@\n-    private class Tracer extends OGLRenderer {\n+    private static class Tracer extends OGLRenderer {\n","filename":"src\/java.desktop\/share\/classes\/sun\/java2d\/opengl\/OGLRenderer.java","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2002, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -46,1 +46,1 @@\n-    class TileContext {\n+    static class TileContext {\n","filename":"src\/java.desktop\/share\/classes\/sun\/java2d\/pipe\/GeneralCompositePipe.java","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1998, 2018, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1998, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -54,1 +54,1 @@\n-    class SCRcontext {\n+    static class SCRcontext {\n","filename":"src\/java.desktop\/share\/classes\/sun\/java2d\/pipe\/SpanClipRenderer.java","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -57,1 +57,0 @@\n-import javax.print.attribute.ResolutionSyntax;\n@@ -67,1 +66,0 @@\n-import javax.print.attribute.standard.PrinterResolution;\n@@ -75,2 +73,0 @@\n-import sun.print.SunPageSelection;\n-import sun.print.SunMinMaxPage;\n@@ -1036,1 +1032,1 @@\n-    private class MessageQ {\n+    private static class MessageQ {\n","filename":"src\/java.desktop\/share\/classes\/sun\/print\/PrintJob2D.java","additions":1,"deletions":5,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -31,2 +31,0 @@\n-import java.awt.Dialog;\n-import java.awt.Frame;\n@@ -307,1 +305,1 @@\n-    private class GraphicsState {\n+    private static class GraphicsState {\n","filename":"src\/java.desktop\/share\/classes\/sun\/print\/RasterPrinterJob.java","additions":1,"deletions":3,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -29,1 +29,0 @@\n-import java.awt.Color;\n@@ -38,1 +37,0 @@\n-import java.awt.GridLayout;\n@@ -40,1 +38,0 @@\n-import java.awt.Toolkit;\n@@ -64,1 +61,0 @@\n-import javax.swing.border.Border;\n@@ -66,1 +62,0 @@\n-import javax.swing.border.TitledBorder;\n@@ -69,2 +64,0 @@\n-import javax.swing.event.DocumentEvent;\n-import javax.swing.event.DocumentListener;\n@@ -2838,1 +2831,1 @@\n-    private class IconRadioButton extends JPanel {\n+    private static class IconRadioButton extends JPanel {\n@@ -2892,1 +2885,1 @@\n-    private class ValidatingFileChooser extends JFileChooser {\n+    private static class ValidatingFileChooser extends JFileChooser {\n","filename":"src\/java.desktop\/share\/classes\/sun\/print\/ServiceDialog.java","additions":2,"deletions":9,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2002, 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2002, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -410,1 +410,1 @@\n-    class GlobFilter extends FileFilter {\n+    static class GlobFilter extends FileFilter {\n","filename":"src\/java.desktop\/share\/classes\/sun\/swing\/plaf\/synth\/SynthFileChooserUI.java","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -32,2 +32,0 @@\n-import java.security.AccessController;\n-import java.security.PrivilegedAction;\n@@ -700,1 +698,1 @@\n-    class IndentIcon implements Icon {\n+    static class IndentIcon implements Icon {\n@@ -1071,1 +1069,1 @@\n-    private class AlignedLabel extends JLabel {\n+    private static class AlignedLabel extends JLabel {\n","filename":"src\/java.desktop\/share\/classes\/sun\/swing\/plaf\/synth\/SynthFileChooserUIImpl.java","additions":2,"deletions":4,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2005, 2014, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2005, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -192,1 +192,1 @@\n-    private class EmptyIcon implements Icon, Serializable {\n+    private static class EmptyIcon implements Icon, Serializable {\n","filename":"src\/java.desktop\/share\/classes\/sun\/swing\/table\/DefaultTableCellHeaderRenderer.java","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2003, 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2003, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -29,1 +29,0 @@\n-import java.awt.peer.ComponentPeer;\n@@ -529,1 +528,1 @@\n-    final class AWTTextAreaUI extends MotifTextAreaUI {\n+    static final class AWTTextAreaUI extends MotifTextAreaUI {\n@@ -636,1 +635,1 @@\n-    final class XAWTScrollBarButton extends BasicArrowButton {\n+    static final class XAWTScrollBarButton extends BasicArrowButton {\n@@ -807,1 +806,1 @@\n-    final class XAWTScrollBarUI extends BasicScrollBarUI {\n+    static final class XAWTScrollBarUI extends BasicScrollBarUI {\n@@ -897,1 +896,1 @@\n-    final class AWTTextArea extends JTextArea implements DocumentListener {\n+    static final class AWTTextArea extends JTextArea implements DocumentListener {\n@@ -986,1 +985,1 @@\n-    final class XAWTScrollPaneUI extends BasicScrollPaneUI {\n+    static final class XAWTScrollPaneUI extends BasicScrollPaneUI {\n@@ -1096,1 +1095,1 @@\n-    private class AWTTextPane extends JScrollPane implements FocusListener {\n+    private static class AWTTextPane extends JScrollPane implements FocusListener {\n","filename":"src\/java.desktop\/unix\/classes\/sun\/awt\/X11\/XTextAreaPeer.java","additions":7,"deletions":8,"binary":false,"changes":15,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2003, 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2003, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -31,3 +31,0 @@\n-import java.awt.event.ActionEvent;\n-import java.awt.event.ActionListener;\n-import java.awt.event.TextEvent;\n@@ -43,4 +40,0 @@\n-import java.awt.event.MouseEvent;\n-import java.awt.event.FocusEvent;\n-import java.awt.event.KeyEvent;\n-\n@@ -448,1 +441,1 @@\n-    final class AWTTextFieldUI extends MotifPasswordFieldUI {\n+    static final class AWTTextFieldUI extends MotifPasswordFieldUI {\n@@ -544,1 +537,1 @@\n-    final class XAWTTextField extends JPasswordField\n+    static final class XAWTTextField extends JPasswordField\n","filename":"src\/java.desktop\/unix\/classes\/sun\/awt\/X11\/XTextFieldPeer.java","additions":3,"deletions":10,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1996, 2005, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1996, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -49,1 +49,1 @@\n-    private class Encoder extends CharsetEncoder {\n+    private static class Encoder extends CharsetEncoder {\n@@ -101,1 +101,1 @@\n-    private class Decoder extends  CharsetDecoder {\n+    private static class Decoder extends  CharsetDecoder {\n","filename":"src\/java.desktop\/unix\/classes\/sun\/font\/X11GB2312.java","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1999, 2005, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1999, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -47,1 +47,1 @@\n-    private class Encoder extends DoubleByte.Encoder {\n+    private static class Encoder extends DoubleByte.Encoder {\n","filename":"src\/java.desktop\/unix\/classes\/sun\/font\/X11GBK.java","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1996, 2005, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1996, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -49,1 +49,1 @@\n-    private class Encoder extends CharsetEncoder {\n+    private static class Encoder extends CharsetEncoder {\n@@ -100,1 +100,1 @@\n-    private class Decoder extends  CharsetDecoder {\n+    private static class Decoder extends  CharsetDecoder {\n","filename":"src\/java.desktop\/unix\/classes\/sun\/font\/X11KSC5601.java","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -835,1 +835,1 @@\n-    private class ExtFinishing extends Finishings {\n+    private static class ExtFinishing extends Finishings {\n","filename":"src\/java.desktop\/unix\/classes\/sun\/print\/IPPPrintService.java","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -596,1 +596,1 @@\n-        private class WindowsComboBoxDashedBorder extends DashedBorder {\n+        private static class WindowsComboBoxDashedBorder extends DashedBorder {\n","filename":"src\/java.desktop\/windows\/classes\/com\/sun\/java\/swing\/plaf\/windows\/WindowsComboBoxUI.java","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -1057,1 +1057,1 @@\n-    class IndentIcon implements Icon {\n+    static class IndentIcon implements Icon {\n","filename":"src\/java.desktop\/windows\/classes\/com\/sun\/java\/swing\/plaf\/windows\/WindowsFileChooserUI.java","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -2116,1 +2116,1 @@\n-    private class ActiveWindowsIcon implements UIDefaults.ActiveValue {\n+    private static class ActiveWindowsIcon implements UIDefaults.ActiveValue {\n@@ -2422,1 +2422,1 @@\n-    private class TriggerDesktopProperty extends WindowsDesktopProperty {\n+    private static class TriggerDesktopProperty extends WindowsDesktopProperty {\n@@ -2439,1 +2439,1 @@\n-    private class FontDesktopProperty extends TriggerDesktopProperty {\n+    private static class FontDesktopProperty extends TriggerDesktopProperty {\n","filename":"src\/java.desktop\/windows\/classes\/com\/sun\/java\/swing\/plaf\/windows\/WindowsLookAndFeel.java","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -304,1 +304,1 @@\n-    private class XPFillBorder extends LineBorder implements UIResource {\n+    private static class XPFillBorder extends LineBorder implements UIResource {\n@@ -401,1 +401,1 @@\n-    private class XPEmptyBorder extends EmptyBorder implements UIResource {\n+    private static class XPEmptyBorder extends EmptyBorder implements UIResource {\n","filename":"src\/java.desktop\/windows\/classes\/com\/sun\/java\/swing\/plaf\/windows\/XPStyle.java","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -2300,1 +2300,1 @@\n-class PrintToFileErrorDialog extends Dialog implements ActionListener{\n+static class PrintToFileErrorDialog extends Dialog implements ActionListener {\n","filename":"src\/java.desktop\/windows\/classes\/sun\/awt\/windows\/WPrinterJob.java","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1996, 2018, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1996, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -165,1 +165,1 @@\n-    class ScrollEvent extends PeerEvent {\n+    static class ScrollEvent extends PeerEvent {\n","filename":"src\/java.desktop\/windows\/classes\/sun\/awt\/windows\/WScrollPanePeer.java","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2007, 2011, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2007, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -33,1 +33,0 @@\n-import sun.java2d.pipe.BufferedPaints;\n@@ -111,1 +110,1 @@\n-    private class Tracer extends D3DRenderer {\n+    private static class Tracer extends D3DRenderer {\n","filename":"src\/java.desktop\/windows\/classes\/sun\/java2d\/d3d\/D3DRenderer.java","additions":2,"deletions":3,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1996, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1996, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -50,0 +50,1 @@\n+#include \"math.h\"\n@@ -2237,2 +2238,2 @@\n-    DWORD i;\n-    for (i = 0; i < rgndata->rdh.nCount; i++, r++) {\n+        DWORD i;\n+        for (i = 0; i < rgndata->rdh.nCount; i++, r++) {\n@@ -2250,0 +2251,9 @@\n+        \/\/ The Windows may request to update the small region of pixels that\n+        \/\/ cannot be represented in the user's space, in this case, we will\n+        \/\/ request to repaint the smallest non-empty bounding box in the user's\n+        \/\/ space\n+        int screen = GetScreenImOn();\n+        Devices::InstanceAccess devices;\n+        AwtWin32GraphicsDevice* device = devices->GetDevice(screen);\n+        float scaleX = (device == NULL) ? 1 : device->GetScaleX();\n+        float scaleY = (device == NULL) ? 1 : device->GetScaleY();\n@@ -2252,5 +2262,5 @@\n-                DoCallback(\"handleExpose\", \"(IIII)V\",\n-                           ScaleDownX(un[i]->left),\n-                           ScaleDownY(un[i]->top),\n-                           ScaleDownX(un[i]->right - un[i]->left),\n-                           ScaleDownY(un[i]->bottom - un[i]->top));\n+                int x1 = floor(un[i]->left \/ scaleX);\n+                int y1 = floor(un[i]->top \/ scaleY);\n+                int x2 = ceil(un[i]->right \/ scaleX);\n+                int y2 = ceil(un[i]->bottom  \/ scaleY);\n+                DoCallback(\"handleExpose\", \"(IIII)V\", x1, y1, x2 - x1, y2 - y1);\n","filename":"src\/java.desktop\/windows\/native\/libawt\/windows\/awt_Component.cpp","additions":18,"deletions":8,"binary":false,"changes":26,"status":"modified"},{"patch":"@@ -1651,2 +1651,1 @@\n-        final IOException x = new IOException(message);\n-        return EnvHelp.initCause(x,cause);\n+        return new IOException(message, cause);\n","filename":"src\/java.management.rmi\/share\/classes\/javax\/management\/remote\/rmi\/RMIConnectionImpl.java","additions":1,"deletions":2,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -370,1 +370,1 @@\n-            throw EnvHelp.initCause(new IOException(msg),e);\n+            throw new IOException(msg, e);\n@@ -546,3 +546,1 @@\n-            final IOException x =\n-                    new IOException(\"Failed to close: \" + closeException);\n-            throw EnvHelp.initCause(x,closeException);\n+            throw new IOException(\"Failed to close: \" + closeException, closeException);\n","filename":"src\/java.management.rmi\/share\/classes\/javax\/management\/remote\/rmi\/RMIConnector.java","additions":2,"deletions":4,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2002, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2002, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -423,2 +423,1 @@\n-                    throw EnvHelp.initCause(\n-                        new IllegalArgumentException(e.getMessage()), e);\n+                    throw new IllegalArgumentException(e.getMessage(), e);\n@@ -437,3 +436,1 @@\n-            IllegalArgumentException x = new\n-                IllegalArgumentException(\"ClassLoader not found: \"+infc);\n-            throw EnvHelp.initCause(x,infc);\n+            throw new IllegalArgumentException(\"ClassLoader not found: \" + infc, infc);\n@@ -834,2 +831,1 @@\n-        final IOException x = new IOException(message);\n-        return EnvHelp.initCause(x,cause);\n+        return new IOException(message, cause);\n","filename":"src\/java.management.rmi\/share\/classes\/javax\/management\/remote\/rmi\/RMIConnectorServer.java","additions":4,"deletions":8,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -41,1 +41,0 @@\n-import java.io.ObjectInputStream;\n@@ -83,1 +82,0 @@\n-import javax.management.OperationsException;\n@@ -211,2 +209,1 @@\n-            throw EnvHelp.initCause(\n-                new IllegalArgumentException(\"Unexpected exception: \" + e), e);\n+            throw new IllegalArgumentException(\"Unexpected exception: \" + e, e);\n","filename":"src\/java.management\/share\/classes\/com\/sun\/jmx\/interceptor\/DefaultMBeanServerInterceptor.java","additions":1,"deletions":4,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -613,2 +613,0 @@\n-            RuntimeException re = new IllegalArgumentException(msg + e);\n-            EnvHelp.initCause(re, e);\n@@ -617,1 +615,1 @@\n-            throw re;\n+            throw new IllegalArgumentException(msg + e, e);\n","filename":"src\/java.management\/share\/classes\/com\/sun\/jmx\/remote\/internal\/ArrayNotificationBuffer.java","additions":1,"deletions":3,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -54,1 +54,0 @@\n-import java.lang.reflect.UndeclaredThrowableException;\n@@ -343,3 +342,1 @@\n-                IOException ioe = new IOException(ire.toString());\n-                EnvHelp.initCause(ioe, ire);\n-                throw ioe;\n+                throw new IOException(ire.toString(), ire);\n@@ -384,3 +381,1 @@\n-                      IOException ioe = new IOException(ire.toString());\n-                      EnvHelp.initCause(ioe, ire);\n-                      throw ioe;\n+                      throw new IOException(ire.toString(), ire);\n@@ -824,4 +819,1 @@\n-                    IOException ioe = new IOException(ire.toString());\n-                    EnvHelp.initCause(ioe, ire);\n-\n-                    throw ioe;\n+                    throw new IOException(ire.toString(), ire);\n@@ -904,4 +896,1 @@\n-                IOException ioe = new IOException(ire.toString());\n-                EnvHelp.initCause(ioe, ire);\n-\n-                throw ioe;\n+                throw new IOException(ire.toString(), ire);\n","filename":"src\/java.management\/share\/classes\/com\/sun\/jmx\/remote\/internal\/ClientNotifForwarder.java","additions":4,"deletions":15,"binary":false,"changes":19,"status":"modified"},{"patch":"@@ -124,3 +124,1 @@\n-                IOException ioe = new IOException(mfoe.getMessage());\n-                ioe.initCause(mfoe);\n-                throw ioe;\n+                throw new IOException(mfoe.getMessage(), mfoe);\n","filename":"src\/java.management\/share\/classes\/com\/sun\/jmx\/remote\/internal\/ServerNotifForwarder.java","additions":1,"deletions":3,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -47,1 +47,0 @@\n-import com.sun.jmx.remote.util.EnvHelp;\n@@ -231,3 +230,1 @@\n-            final SecurityException e = new SecurityException(msg);\n-            EnvHelp.initCause(e, exception);\n-            se = e;\n+            se = new SecurityException(msg, exception);\n","filename":"src\/java.management\/share\/classes\/com\/sun\/jmx\/remote\/security\/JMXPluggableAuthenticator.java","additions":1,"deletions":4,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1999, 2008, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1999, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -170,0 +170,1 @@\n+            int cmp = sval1.compareTo(sval2);\n@@ -172,1 +173,1 @@\n-                return sval1.compareTo(sval2) > 0;\n+                return cmp > 0;\n@@ -174,1 +175,1 @@\n-                return sval1.compareTo(sval2) < 0;\n+                return cmp < 0;\n@@ -176,1 +177,1 @@\n-                return sval1.compareTo(sval2) >= 0;\n+                return cmp >= 0;\n@@ -178,1 +179,1 @@\n-                return sval1.compareTo(sval2) <= 0;\n+                return cmp <= 0;\n@@ -180,1 +181,1 @@\n-                return sval1.compareTo(sval2) == 0;\n+                return cmp == 0;\n","filename":"src\/java.management\/share\/classes\/javax\/management\/BinaryRelQueryExp.java","additions":7,"deletions":6,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -28,3 +28,0 @@\n-\/\/ Java import\n-import com.sun.jmx.defaults.JmxProperties;\n-\n@@ -37,1 +34,0 @@\n-import java.io.FileOutputStream;\n@@ -74,1 +70,0 @@\n-import com.sun.jmx.defaults.ServiceName;\n@@ -1273,18 +1268,10 @@\n-        if (type.compareTo(\"java.lang.Boolean\") == 0)\n-             return Boolean.valueOf(param);\n-        if (type.compareTo(\"java.lang.Byte\") == 0)\n-             return Byte.valueOf(param);\n-        if (type.compareTo(\"java.lang.Short\") == 0)\n-             return Short.valueOf(param);\n-        if (type.compareTo(\"java.lang.Long\") == 0)\n-             return Long.valueOf(param);\n-        if (type.compareTo(\"java.lang.Integer\") == 0)\n-             return Integer.valueOf(param);\n-        if (type.compareTo(\"java.lang.Float\") == 0)\n-             return Float.valueOf(param);\n-        if (type.compareTo(\"java.lang.Double\") == 0)\n-             return Double.valueOf(param);\n-        if (type.compareTo(\"java.lang.String\") == 0)\n-             return param;\n-\n-        return param;\n+         return switch (type) {\n+             case \"java.lang.Boolean\" -> Boolean.valueOf(param);\n+             case \"java.lang.Byte\" -> Byte.valueOf(param);\n+             case \"java.lang.Short\" -> Short.valueOf(param);\n+             case \"java.lang.Long\" -> Long.valueOf(param);\n+             case \"java.lang.Integer\" -> Integer.valueOf(param);\n+             case \"java.lang.Float\" -> Float.valueOf(param);\n+             case \"java.lang.Double\" -> Double.valueOf(param);\n+             default -> param;\n+         };\n","filename":"src\/java.management\/share\/classes\/javax\/management\/loading\/MLet.java","additions":10,"deletions":23,"binary":false,"changes":33,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2000, 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2000, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -30,2 +30,0 @@\n-\/\/ java import\n-\/\/\n@@ -47,1 +45,0 @@\n-import com.sun.jmx.remote.util.EnvHelp;\n@@ -629,1 +626,1 @@\n-            throw EnvHelp.initCause(new IllegalArgumentException(msg), e);\n+            throw new IllegalArgumentException(msg, e);\n","filename":"src\/java.management\/share\/classes\/javax\/management\/openmbean\/OpenMBeanAttributeInfoSupport.java","additions":2,"deletions":5,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -622,2 +622,1 @@\n-                                exception = EnvHelp.initCause(\n-                                    new IOException(e.getMessage()), e);\n+                                exception = new IOException(e.getMessage(), e);\n","filename":"src\/java.management\/share\/classes\/javax\/management\/remote\/JMXConnectorFactory.java","additions":1,"deletions":2,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2015, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2015, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -259,0 +259,13 @@\n+        \/**\n+         * Sets the request method of this builder to HEAD.\n+         *\n+         * @implSpec The default implementation is expected to have the same behaviour as:\n+         * {@code return method(\"HEAD\", BodyPublishers.noBody());}\n+         *\n+         * @return this builder\n+         * @since 18\n+         *\/\n+        default Builder HEAD() {\n+            return method(\"HEAD\", BodyPublishers.noBody());\n+        }\n+\n@@ -363,1 +376,1 @@\n-                \/\/ otherwise, the body is absent, special case for GET\/DELETE,\n+                \/\/ otherwise, the body is absent, special case for GET\/DELETE\/HEAD,\n@@ -369,0 +382,1 @@\n+                        case \"HEAD\" -> builder.HEAD();\n","filename":"src\/java.net.http\/share\/classes\/java\/net\/http\/HttpRequest.java","additions":16,"deletions":2,"binary":false,"changes":18,"status":"modified"},{"patch":"@@ -185,0 +185,5 @@\n+    @Override\n+    public HttpRequest.Builder HEAD() {\n+        return method0(\"HEAD\", null);\n+    }\n+\n","filename":"src\/java.net.http\/share\/classes\/jdk\/internal\/net\/http\/HttpRequestBuilderImpl.java","additions":5,"deletions":0,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -5101,0 +5101,8 @@\n+    @Override\n+    public void visitModifiers(JCModifiers tree) {\n+        \/\/error recovery only:\n+        Assert.check(resultInfo.pkind == KindSelector.ERR);\n+\n+        attribAnnotationTypes(tree.annotations, env);\n+    }\n+\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/tools\/javac\/comp\/Attr.java","additions":8,"deletions":0,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -49,1 +49,0 @@\n-    currentMileageField = new CIntField(type.getCIntegerField(\"_current_mileage\"), 0);\n@@ -55,1 +54,0 @@\n-    currentMileageField = new CIntField(type.getCIntegerField(\"_current_mileage\"), 0);\n@@ -66,1 +64,0 @@\n-  private static CIntField currentMileageField;\n@@ -144,1 +141,1 @@\n-    return (int)currentMileageField.getValue(getAddress());\n+    return 0;\n","filename":"src\/jdk.hotspot.agent\/share\/classes\/sun\/jvm\/hotspot\/ci\/ciMethodData.java","additions":1,"deletions":4,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2004, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2004, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -58,4 +58,2 @@\n-        Iterator i = threadTable.entrySet().iterator();\n-        while (i.hasNext()) {\n-            Entry e = (Entry)i.next();\n-            if (dfn(e) >= 0) {\n+        for (Entry<JavaThread, Integer> e : threadTable.entrySet()) {\n+            if (e.getValue() >= 0) {\n@@ -67,1 +65,1 @@\n-            JavaThread thread = (JavaThread)e.getKey();\n+            JavaThread thread = e.getKey();\n@@ -121,1 +119,1 @@\n-                waitingToLockMonitor = (ObjectMonitor)currentThread.getCurrentPendingMonitor();\n+                waitingToLockMonitor = currentThread.getCurrentPendingMonitor();\n@@ -165,4 +163,0 @@\n-    private static int dfn(Entry e) {\n-        return ((Integer)e.getValue()).intValue();\n-    }\n-\n","filename":"src\/jdk.hotspot.agent\/share\/classes\/sun\/jvm\/hotspot\/runtime\/DeadlockDetector.java","additions":5,"deletions":11,"binary":false,"changes":16,"status":"modified"},{"patch":"@@ -55,0 +55,1 @@\n+  private static AddressField activeHandlesField;\n@@ -98,0 +99,1 @@\n+    activeHandlesField = type.getAddressField(\"_active_handles\");\n@@ -412,0 +414,8 @@\n+  public JNIHandleBlock activeHandles() {\n+    Address a = activeHandlesField.getAddress(addr);\n+    if (a == null) {\n+      return null;\n+    }\n+    return new JNIHandleBlock(a);\n+  }\n+\n","filename":"src\/jdk.hotspot.agent\/share\/classes\/sun\/jvm\/hotspot\/runtime\/JavaThread.java","additions":10,"deletions":0,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -40,1 +40,0 @@\n-  private static AddressField activeHandlesField;\n@@ -62,1 +61,0 @@\n-    activeHandlesField = typeThread.getAddressField(\"_active_handles\");\n@@ -84,8 +82,0 @@\n-  public JNIHandleBlock activeHandles() {\n-    Address a = activeHandlesField.getAddress(addr);\n-    if (a == null) {\n-      return null;\n-    }\n-    return new JNIHandleBlock(a);\n-  }\n-\n","filename":"src\/jdk.hotspot.agent\/share\/classes\/sun\/jvm\/hotspot\/runtime\/Thread.java","additions":0,"deletions":10,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2002, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2002, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -28,1 +28,0 @@\n-import java.io.*;\n@@ -196,3 +195,2 @@\n-      Iterator i = updates.iterator();\n-      while (i.hasNext()) {\n-        textArea.append((String)i.next());\n+      for (String update : updates) {\n+        textArea.append(update);\n","filename":"src\/jdk.hotspot.agent\/share\/classes\/sun\/jvm\/hotspot\/ui\/FindInHeapPanel.java","additions":3,"deletions":5,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -44,2 +44,2 @@\n-    final static char CR = '\\r';\n-    final static char LF = '\\n';\n+    static final char CR = '\\r';\n+    static final char LF = '\\n';\n@@ -49,1 +49,1 @@\n-    private final static int MAX_CHUNK_HEADER_SIZE = 2050;\n+    private static final int MAX_CHUNK_HEADER_SIZE = 2050;\n","filename":"src\/jdk.httpserver\/share\/classes\/sun\/net\/httpserver\/ChunkedInputStream.java","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -54,1 +54,1 @@\n-    final static int CHUNK_SIZE = 4096;\n+    static final int CHUNK_SIZE = 4096;\n@@ -56,1 +56,1 @@\n-    final static int OFFSET = 6; \/* initial <=4 bytes for len + CRLF *\/\n+    static final int OFFSET = 6; \/* initial <=4 bytes for len + CRLF *\/\n","filename":"src\/jdk.httpserver\/share\/classes\/sun\/net\/httpserver\/ChunkedOutputStream.java","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -39,3 +39,3 @@\n-    final static int BUF_LEN = 2048;\n-    final static byte CR = 13;\n-    final static byte LF = 10;\n+    static final int BUF_LEN = 2048;\n+    static final byte CR = 13;\n+    static final byte LF = 10;\n@@ -233,1 +233,1 @@\n-        final static int BUFSIZE = 8 * 1024;\n+        static final int BUFSIZE = 8 * 1024;\n","filename":"src\/jdk.httpserver\/share\/classes\/sun\/net\/httpserver\/Request.java","additions":4,"deletions":4,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -78,7 +78,7 @@\n-    final static int CLOCK_TICK = ServerConfig.getClockTick();\n-    final static long IDLE_INTERVAL = ServerConfig.getIdleInterval();\n-    final static int MAX_IDLE_CONNECTIONS = ServerConfig.getMaxIdleConnections();\n-    final static long TIMER_MILLIS = ServerConfig.getTimerMillis ();\n-    final static long MAX_REQ_TIME=getTimeMillis(ServerConfig.getMaxReqTime());\n-    final static long MAX_RSP_TIME=getTimeMillis(ServerConfig.getMaxRspTime());\n-    final static boolean timer1Enabled = MAX_REQ_TIME != -1 || MAX_RSP_TIME != -1;\n+    static final int CLOCK_TICK = ServerConfig.getClockTick();\n+    static final long IDLE_INTERVAL = ServerConfig.getIdleInterval();\n+    static final int MAX_IDLE_CONNECTIONS = ServerConfig.getMaxIdleConnections();\n+    static final long TIMER_MILLIS = ServerConfig.getTimerMillis ();\n+    static final long MAX_REQ_TIME=getTimeMillis(ServerConfig.getMaxReqTime());\n+    static final long MAX_RSP_TIME=getTimeMillis(ServerConfig.getMaxRspTime());\n+    static final boolean timer1Enabled = MAX_REQ_TIME != -1 || MAX_RSP_TIME != -1;\n","filename":"src\/jdk.httpserver\/share\/classes\/sun\/net\/httpserver\/ServerImpl.java","additions":7,"deletions":7,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -147,1 +147,1 @@\n-    private final static class Out {\n+    private static final class Out {\n","filename":"src\/jdk.httpserver\/share\/classes\/sun\/net\/httpserver\/simpleserver\/SimpleFileServerImpl.java","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -27,0 +27,2 @@\n+import java.util.Objects;\n+\n@@ -65,0 +67,1 @@\n+    @ForceInline\n@@ -66,15 +69,6 @@\n-        return getBits()[i];\n-    }\n-\n-    @Override\n-    public long toLong() {\n-        \/\/ FIXME: This should be an intrinsic.\n-        if (length() > Long.SIZE) {\n-            throw new UnsupportedOperationException(\"too many lanes for one long\");\n-        }\n-        long res = 0;\n-        long set = 1;\n-        boolean[] bits = getBits();\n-        for (int i = 0; i < bits.length; i++) {\n-            res = bits[i] ? res | set : res;\n-            set = set << 1;\n+        int length = length();\n+        Objects.checkIndex(i, length);\n+        if (length <= Long.SIZE) {\n+            return ((toLong() >>> i) & 1L) == 1;\n+        } else {\n+            return getBits()[i];\n@@ -82,1 +76,0 @@\n-        return res;\n@@ -117,0 +110,17 @@\n+    @Override\n+    @ForceInline\n+    @SuppressWarnings(\"unchecked\")\n+    <F> VectorMask<F> check(Class<? extends VectorMask<F>> maskClass, Vector<F> vector) {\n+        if (!sameSpecies(maskClass, vector)) {\n+            throw AbstractSpecies.checkFailed(this, vector);\n+        }\n+        return (VectorMask<F>) this;\n+    }\n+\n+    @ForceInline\n+    private <F> boolean sameSpecies(Class<? extends VectorMask<F>> maskClass, Vector<F> vector) {\n+        boolean same = getClass() == maskClass;\n+        assert (same == (vectorSpecies() == vector.species())) : same;\n+        return same;\n+    }\n+\n@@ -165,0 +175,11 @@\n+    \/*package-private*\/\n+    static long toLongHelper(boolean[] bits) {\n+        long res = 0;\n+        long set = 1;\n+        for (int i = 0; i < bits.length; i++) {\n+            res = bits[i] ? res | set : res;\n+            set = set << 1;\n+        }\n+        return res;\n+    }\n+\n@@ -218,1 +239,0 @@\n-            \/\/ This requires a split test.\n@@ -220,6 +240,3 @@\n-            int elemCount = Math.min(vlength, (alength - clipOffset) \/ esize);\n-            badMask = checkIndex0(0, elemCount, iota, vlength);\n-            clipOffset &= (esize - 1);  \/\/ power of two, so OK\n-            VectorMask<E> badMask2 = checkIndex0(clipOffset \/ esize, vlength,\n-                                                 iota, vlength);\n-            badMask = badMask.or(badMask2);\n+            badMask = checkIndex0(clipOffset, alength,\n+                                  iota.lanewise(VectorOperators.MUL, esize),\n+                                  vlength * esize);\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/AbstractMask.java","additions":40,"deletions":23,"binary":false,"changes":63,"status":"modified"},{"patch":"@@ -239,2 +239,2 @@\n-    byte rOp(byte v, FBinOp f) {\n-        return super.rOpTemplate(v, f);  \/\/ specialize\n+    byte rOp(byte v, VectorMask<Byte> m, FBinOp f) {\n+        return super.rOpTemplate(v, m, f);  \/\/ specialize\n@@ -276,0 +276,6 @@\n+    @Override\n+    @ForceInline\n+    public Byte128Vector lanewise(Unary op, VectorMask<Byte> m) {\n+        return (Byte128Vector) super.lanewiseTemplate(op, Byte128Mask.class, (Byte128Mask) m);  \/\/ specialize\n+    }\n+\n@@ -282,0 +288,6 @@\n+    @Override\n+    @ForceInline\n+    public Byte128Vector lanewise(Binary op, Vector<Byte> v, VectorMask<Byte> m) {\n+        return (Byte128Vector) super.lanewiseTemplate(op, Byte128Mask.class, v, (Byte128Mask) m);  \/\/ specialize\n+    }\n+\n@@ -289,0 +301,7 @@\n+    \/*package-private*\/\n+    @Override\n+    @ForceInline Byte128Vector\n+    lanewiseShift(VectorOperators.Binary op, int e, VectorMask<Byte> m) {\n+        return (Byte128Vector) super.lanewiseShiftTemplate(op, Byte128Mask.class, e, (Byte128Mask) m);  \/\/ specialize\n+    }\n+\n@@ -294,1 +313,1 @@\n-    lanewise(VectorOperators.Ternary op, Vector<Byte> v1, Vector<Byte> v2) {\n+    lanewise(Ternary op, Vector<Byte> v1, Vector<Byte> v2) {\n@@ -298,0 +317,8 @@\n+    @Override\n+    @ForceInline\n+    public final\n+    Byte128Vector\n+    lanewise(Ternary op, Vector<Byte> v1, Vector<Byte> v2, VectorMask<Byte> m) {\n+        return (Byte128Vector) super.lanewiseTemplate(op, Byte128Mask.class, v1, v2, (Byte128Mask) m);  \/\/ specialize\n+    }\n+\n@@ -317,1 +344,1 @@\n-        return super.reduceLanesTemplate(op, m);  \/\/ specialized\n+        return super.reduceLanesTemplate(op, Byte128Mask.class, (Byte128Mask) m);  \/\/ specialized\n@@ -330,1 +357,1 @@\n-        return (long) super.reduceLanesTemplate(op, m);  \/\/ specialized\n+        return (long) super.reduceLanesTemplate(op, Byte128Mask.class, (Byte128Mask) m);  \/\/ specialized\n@@ -366,0 +393,7 @@\n+    @Override\n+    @ForceInline\n+    public final Byte128Mask compare(Comparison op, Vector<Byte> v, VectorMask<Byte> m) {\n+        return super.compareTemplate(Byte128Mask.class, op, v, (Byte128Mask) m);\n+    }\n+\n+\n@@ -422,0 +456,1 @@\n+                                    Byte128Mask.class,\n@@ -615,10 +650,6 @@\n-            if (VSIZE == species.vectorBitSize()) {\n-                Class<?> dtype = species.elementType();\n-                Class<?> dmtype = species.maskType();\n-                return VectorSupport.convert(VectorSupport.VECTOR_OP_REINTERPRET,\n-                    this.getClass(), ETYPE, VLENGTH,\n-                    dmtype, dtype, VLENGTH,\n-                    this, species,\n-                    Byte128Mask::defaultMaskCast);\n-            }\n-            return this.defaultMaskCast(species);\n+\n+            return VectorSupport.convert(VectorSupport.VECTOR_OP_CAST,\n+                this.getClass(), ETYPE, VLENGTH,\n+                species.maskType(), species.elementType(), VLENGTH,\n+                this, species,\n+                (m, s) -> s.maskFactory(m.toArray()).check(s));\n@@ -650,3 +681,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_AND, Byte128Mask.class, byte.class, VLENGTH,\n-                                             this, m,\n-                                             (m1, m2) -> m1.bOp(m2, (i, a, b) -> a & b));\n+            return VectorSupport.binaryOp(VECTOR_OP_AND, Byte128Mask.class, null, byte.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a & b));\n@@ -660,3 +691,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_OR, Byte128Mask.class, byte.class, VLENGTH,\n-                                             this, m,\n-                                             (m1, m2) -> m1.bOp(m2, (i, a, b) -> a | b));\n+            return VectorSupport.binaryOp(VECTOR_OP_OR, Byte128Mask.class, null, byte.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a | b));\n@@ -670,3 +701,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_XOR, Byte128Mask.class, byte.class, VLENGTH,\n-                                          this, m,\n-                                          (m1, m2) -> m1.bOp(m2, (i, a, b) -> a ^ b));\n+            return VectorSupport.binaryOp(VECTOR_OP_XOR, Byte128Mask.class, null, byte.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a ^ b));\n@@ -680,2 +711,2 @@\n-            return VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_TRUECOUNT, Byte128Mask.class, byte.class, VLENGTH, this,\n-                                                      (m) -> trueCountHelper(((Byte128Mask)m).getBits()));\n+            return (int) VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_TRUECOUNT, Byte128Mask.class, byte.class, VLENGTH, this,\n+                                                      (m) -> trueCountHelper(m.getBits()));\n@@ -687,2 +718,2 @@\n-            return VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_FIRSTTRUE, Byte128Mask.class, byte.class, VLENGTH, this,\n-                                                      (m) -> firstTrueHelper(((Byte128Mask)m).getBits()));\n+            return (int) VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_FIRSTTRUE, Byte128Mask.class, byte.class, VLENGTH, this,\n+                                                      (m) -> firstTrueHelper(m.getBits()));\n@@ -694,2 +725,12 @@\n-            return VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_LASTTRUE, Byte128Mask.class, byte.class, VLENGTH, this,\n-                                                      (m) -> lastTrueHelper(((Byte128Mask)m).getBits()));\n+            return (int) VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_LASTTRUE, Byte128Mask.class, byte.class, VLENGTH, this,\n+                                                      (m) -> lastTrueHelper(m.getBits()));\n+        }\n+\n+        @Override\n+        @ForceInline\n+        public long toLong() {\n+            if (length() > Long.SIZE) {\n+                throw new UnsupportedOperationException(\"too many lanes for one long\");\n+            }\n+            return VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_TOLONG, Byte128Mask.class, byte.class, VLENGTH, this,\n+                                                      (m) -> toLongHelper(m.getBits()));\n@@ -806,0 +847,8 @@\n+    @ForceInline\n+    @Override\n+    final\n+    ByteVector fromArray0(byte[] a, int offset, VectorMask<Byte> m) {\n+        return super.fromArray0Template(Byte128Mask.class, a, offset, (Byte128Mask) m);  \/\/ specialize\n+    }\n+\n+\n@@ -814,0 +863,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    ByteVector fromBooleanArray0(boolean[] a, int offset, VectorMask<Byte> m) {\n+        return super.fromBooleanArray0Template(Byte128Mask.class, a, offset, (Byte128Mask) m);  \/\/ specialize\n+    }\n+\n@@ -821,0 +877,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    ByteVector fromByteArray0(byte[] a, int offset, VectorMask<Byte> m) {\n+        return super.fromByteArray0Template(Byte128Mask.class, a, offset, (Byte128Mask) m);  \/\/ specialize\n+    }\n+\n@@ -828,0 +891,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    ByteVector fromByteBuffer0(ByteBuffer bb, int offset, VectorMask<Byte> m) {\n+        return super.fromByteBuffer0Template(Byte128Mask.class, bb, offset, (Byte128Mask) m);  \/\/ specialize\n+    }\n+\n@@ -835,0 +905,15 @@\n+    @ForceInline\n+    @Override\n+    final\n+    void intoArray0(byte[] a, int offset, VectorMask<Byte> m) {\n+        super.intoArray0Template(Byte128Mask.class, a, offset, (Byte128Mask) m);\n+    }\n+\n+\n+    @ForceInline\n+    @Override\n+    final\n+    void intoBooleanArray0(boolean[] a, int offset, VectorMask<Byte> m) {\n+        super.intoBooleanArray0Template(Byte128Mask.class, a, offset, (Byte128Mask) m);\n+    }\n+\n@@ -842,0 +927,15 @@\n+    @ForceInline\n+    @Override\n+    final\n+    void intoByteArray0(byte[] a, int offset, VectorMask<Byte> m) {\n+        super.intoByteArray0Template(Byte128Mask.class, a, offset, (Byte128Mask) m);  \/\/ specialize\n+    }\n+\n+    @ForceInline\n+    @Override\n+    final\n+    void intoByteBuffer0(ByteBuffer bb, int offset, VectorMask<Byte> m) {\n+        super.intoByteBuffer0Template(Byte128Mask.class, bb, offset, (Byte128Mask) m);\n+    }\n+\n+\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/Byte128Vector.java","additions":130,"deletions":30,"binary":false,"changes":160,"status":"modified"},{"patch":"@@ -239,2 +239,2 @@\n-    byte rOp(byte v, FBinOp f) {\n-        return super.rOpTemplate(v, f);  \/\/ specialize\n+    byte rOp(byte v, VectorMask<Byte> m, FBinOp f) {\n+        return super.rOpTemplate(v, m, f);  \/\/ specialize\n@@ -276,0 +276,6 @@\n+    @Override\n+    @ForceInline\n+    public Byte256Vector lanewise(Unary op, VectorMask<Byte> m) {\n+        return (Byte256Vector) super.lanewiseTemplate(op, Byte256Mask.class, (Byte256Mask) m);  \/\/ specialize\n+    }\n+\n@@ -282,0 +288,6 @@\n+    @Override\n+    @ForceInline\n+    public Byte256Vector lanewise(Binary op, Vector<Byte> v, VectorMask<Byte> m) {\n+        return (Byte256Vector) super.lanewiseTemplate(op, Byte256Mask.class, v, (Byte256Mask) m);  \/\/ specialize\n+    }\n+\n@@ -289,0 +301,7 @@\n+    \/*package-private*\/\n+    @Override\n+    @ForceInline Byte256Vector\n+    lanewiseShift(VectorOperators.Binary op, int e, VectorMask<Byte> m) {\n+        return (Byte256Vector) super.lanewiseShiftTemplate(op, Byte256Mask.class, e, (Byte256Mask) m);  \/\/ specialize\n+    }\n+\n@@ -294,1 +313,1 @@\n-    lanewise(VectorOperators.Ternary op, Vector<Byte> v1, Vector<Byte> v2) {\n+    lanewise(Ternary op, Vector<Byte> v1, Vector<Byte> v2) {\n@@ -298,0 +317,8 @@\n+    @Override\n+    @ForceInline\n+    public final\n+    Byte256Vector\n+    lanewise(Ternary op, Vector<Byte> v1, Vector<Byte> v2, VectorMask<Byte> m) {\n+        return (Byte256Vector) super.lanewiseTemplate(op, Byte256Mask.class, v1, v2, (Byte256Mask) m);  \/\/ specialize\n+    }\n+\n@@ -317,1 +344,1 @@\n-        return super.reduceLanesTemplate(op, m);  \/\/ specialized\n+        return super.reduceLanesTemplate(op, Byte256Mask.class, (Byte256Mask) m);  \/\/ specialized\n@@ -330,1 +357,1 @@\n-        return (long) super.reduceLanesTemplate(op, m);  \/\/ specialized\n+        return (long) super.reduceLanesTemplate(op, Byte256Mask.class, (Byte256Mask) m);  \/\/ specialized\n@@ -366,0 +393,7 @@\n+    @Override\n+    @ForceInline\n+    public final Byte256Mask compare(Comparison op, Vector<Byte> v, VectorMask<Byte> m) {\n+        return super.compareTemplate(Byte256Mask.class, op, v, (Byte256Mask) m);\n+    }\n+\n+\n@@ -422,0 +456,1 @@\n+                                    Byte256Mask.class,\n@@ -647,10 +682,6 @@\n-            if (VSIZE == species.vectorBitSize()) {\n-                Class<?> dtype = species.elementType();\n-                Class<?> dmtype = species.maskType();\n-                return VectorSupport.convert(VectorSupport.VECTOR_OP_REINTERPRET,\n-                    this.getClass(), ETYPE, VLENGTH,\n-                    dmtype, dtype, VLENGTH,\n-                    this, species,\n-                    Byte256Mask::defaultMaskCast);\n-            }\n-            return this.defaultMaskCast(species);\n+\n+            return VectorSupport.convert(VectorSupport.VECTOR_OP_CAST,\n+                this.getClass(), ETYPE, VLENGTH,\n+                species.maskType(), species.elementType(), VLENGTH,\n+                this, species,\n+                (m, s) -> s.maskFactory(m.toArray()).check(s));\n@@ -682,3 +713,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_AND, Byte256Mask.class, byte.class, VLENGTH,\n-                                             this, m,\n-                                             (m1, m2) -> m1.bOp(m2, (i, a, b) -> a & b));\n+            return VectorSupport.binaryOp(VECTOR_OP_AND, Byte256Mask.class, null, byte.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a & b));\n@@ -692,3 +723,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_OR, Byte256Mask.class, byte.class, VLENGTH,\n-                                             this, m,\n-                                             (m1, m2) -> m1.bOp(m2, (i, a, b) -> a | b));\n+            return VectorSupport.binaryOp(VECTOR_OP_OR, Byte256Mask.class, null, byte.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a | b));\n@@ -702,3 +733,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_XOR, Byte256Mask.class, byte.class, VLENGTH,\n-                                          this, m,\n-                                          (m1, m2) -> m1.bOp(m2, (i, a, b) -> a ^ b));\n+            return VectorSupport.binaryOp(VECTOR_OP_XOR, Byte256Mask.class, null, byte.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a ^ b));\n@@ -712,2 +743,2 @@\n-            return VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_TRUECOUNT, Byte256Mask.class, byte.class, VLENGTH, this,\n-                                                      (m) -> trueCountHelper(((Byte256Mask)m).getBits()));\n+            return (int) VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_TRUECOUNT, Byte256Mask.class, byte.class, VLENGTH, this,\n+                                                      (m) -> trueCountHelper(m.getBits()));\n@@ -719,2 +750,2 @@\n-            return VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_FIRSTTRUE, Byte256Mask.class, byte.class, VLENGTH, this,\n-                                                      (m) -> firstTrueHelper(((Byte256Mask)m).getBits()));\n+            return (int) VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_FIRSTTRUE, Byte256Mask.class, byte.class, VLENGTH, this,\n+                                                      (m) -> firstTrueHelper(m.getBits()));\n@@ -726,2 +757,12 @@\n-            return VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_LASTTRUE, Byte256Mask.class, byte.class, VLENGTH, this,\n-                                                      (m) -> lastTrueHelper(((Byte256Mask)m).getBits()));\n+            return (int) VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_LASTTRUE, Byte256Mask.class, byte.class, VLENGTH, this,\n+                                                      (m) -> lastTrueHelper(m.getBits()));\n+        }\n+\n+        @Override\n+        @ForceInline\n+        public long toLong() {\n+            if (length() > Long.SIZE) {\n+                throw new UnsupportedOperationException(\"too many lanes for one long\");\n+            }\n+            return VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_TOLONG, Byte256Mask.class, byte.class, VLENGTH, this,\n+                                                      (m) -> toLongHelper(m.getBits()));\n@@ -838,0 +879,8 @@\n+    @ForceInline\n+    @Override\n+    final\n+    ByteVector fromArray0(byte[] a, int offset, VectorMask<Byte> m) {\n+        return super.fromArray0Template(Byte256Mask.class, a, offset, (Byte256Mask) m);  \/\/ specialize\n+    }\n+\n+\n@@ -846,0 +895,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    ByteVector fromBooleanArray0(boolean[] a, int offset, VectorMask<Byte> m) {\n+        return super.fromBooleanArray0Template(Byte256Mask.class, a, offset, (Byte256Mask) m);  \/\/ specialize\n+    }\n+\n@@ -853,0 +909,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    ByteVector fromByteArray0(byte[] a, int offset, VectorMask<Byte> m) {\n+        return super.fromByteArray0Template(Byte256Mask.class, a, offset, (Byte256Mask) m);  \/\/ specialize\n+    }\n+\n@@ -860,0 +923,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    ByteVector fromByteBuffer0(ByteBuffer bb, int offset, VectorMask<Byte> m) {\n+        return super.fromByteBuffer0Template(Byte256Mask.class, bb, offset, (Byte256Mask) m);  \/\/ specialize\n+    }\n+\n@@ -867,0 +937,15 @@\n+    @ForceInline\n+    @Override\n+    final\n+    void intoArray0(byte[] a, int offset, VectorMask<Byte> m) {\n+        super.intoArray0Template(Byte256Mask.class, a, offset, (Byte256Mask) m);\n+    }\n+\n+\n+    @ForceInline\n+    @Override\n+    final\n+    void intoBooleanArray0(boolean[] a, int offset, VectorMask<Byte> m) {\n+        super.intoBooleanArray0Template(Byte256Mask.class, a, offset, (Byte256Mask) m);\n+    }\n+\n@@ -874,0 +959,15 @@\n+    @ForceInline\n+    @Override\n+    final\n+    void intoByteArray0(byte[] a, int offset, VectorMask<Byte> m) {\n+        super.intoByteArray0Template(Byte256Mask.class, a, offset, (Byte256Mask) m);  \/\/ specialize\n+    }\n+\n+    @ForceInline\n+    @Override\n+    final\n+    void intoByteBuffer0(ByteBuffer bb, int offset, VectorMask<Byte> m) {\n+        super.intoByteBuffer0Template(Byte256Mask.class, bb, offset, (Byte256Mask) m);\n+    }\n+\n+\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/Byte256Vector.java","additions":130,"deletions":30,"binary":false,"changes":160,"status":"modified"},{"patch":"@@ -239,2 +239,2 @@\n-    byte rOp(byte v, FBinOp f) {\n-        return super.rOpTemplate(v, f);  \/\/ specialize\n+    byte rOp(byte v, VectorMask<Byte> m, FBinOp f) {\n+        return super.rOpTemplate(v, m, f);  \/\/ specialize\n@@ -276,0 +276,6 @@\n+    @Override\n+    @ForceInline\n+    public Byte512Vector lanewise(Unary op, VectorMask<Byte> m) {\n+        return (Byte512Vector) super.lanewiseTemplate(op, Byte512Mask.class, (Byte512Mask) m);  \/\/ specialize\n+    }\n+\n@@ -282,0 +288,6 @@\n+    @Override\n+    @ForceInline\n+    public Byte512Vector lanewise(Binary op, Vector<Byte> v, VectorMask<Byte> m) {\n+        return (Byte512Vector) super.lanewiseTemplate(op, Byte512Mask.class, v, (Byte512Mask) m);  \/\/ specialize\n+    }\n+\n@@ -289,0 +301,7 @@\n+    \/*package-private*\/\n+    @Override\n+    @ForceInline Byte512Vector\n+    lanewiseShift(VectorOperators.Binary op, int e, VectorMask<Byte> m) {\n+        return (Byte512Vector) super.lanewiseShiftTemplate(op, Byte512Mask.class, e, (Byte512Mask) m);  \/\/ specialize\n+    }\n+\n@@ -294,1 +313,1 @@\n-    lanewise(VectorOperators.Ternary op, Vector<Byte> v1, Vector<Byte> v2) {\n+    lanewise(Ternary op, Vector<Byte> v1, Vector<Byte> v2) {\n@@ -298,0 +317,8 @@\n+    @Override\n+    @ForceInline\n+    public final\n+    Byte512Vector\n+    lanewise(Ternary op, Vector<Byte> v1, Vector<Byte> v2, VectorMask<Byte> m) {\n+        return (Byte512Vector) super.lanewiseTemplate(op, Byte512Mask.class, v1, v2, (Byte512Mask) m);  \/\/ specialize\n+    }\n+\n@@ -317,1 +344,1 @@\n-        return super.reduceLanesTemplate(op, m);  \/\/ specialized\n+        return super.reduceLanesTemplate(op, Byte512Mask.class, (Byte512Mask) m);  \/\/ specialized\n@@ -330,1 +357,1 @@\n-        return (long) super.reduceLanesTemplate(op, m);  \/\/ specialized\n+        return (long) super.reduceLanesTemplate(op, Byte512Mask.class, (Byte512Mask) m);  \/\/ specialized\n@@ -366,0 +393,7 @@\n+    @Override\n+    @ForceInline\n+    public final Byte512Mask compare(Comparison op, Vector<Byte> v, VectorMask<Byte> m) {\n+        return super.compareTemplate(Byte512Mask.class, op, v, (Byte512Mask) m);\n+    }\n+\n+\n@@ -422,0 +456,1 @@\n+                                    Byte512Mask.class,\n@@ -711,10 +746,6 @@\n-            if (VSIZE == species.vectorBitSize()) {\n-                Class<?> dtype = species.elementType();\n-                Class<?> dmtype = species.maskType();\n-                return VectorSupport.convert(VectorSupport.VECTOR_OP_REINTERPRET,\n-                    this.getClass(), ETYPE, VLENGTH,\n-                    dmtype, dtype, VLENGTH,\n-                    this, species,\n-                    Byte512Mask::defaultMaskCast);\n-            }\n-            return this.defaultMaskCast(species);\n+\n+            return VectorSupport.convert(VectorSupport.VECTOR_OP_CAST,\n+                this.getClass(), ETYPE, VLENGTH,\n+                species.maskType(), species.elementType(), VLENGTH,\n+                this, species,\n+                (m, s) -> s.maskFactory(m.toArray()).check(s));\n@@ -746,3 +777,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_AND, Byte512Mask.class, byte.class, VLENGTH,\n-                                             this, m,\n-                                             (m1, m2) -> m1.bOp(m2, (i, a, b) -> a & b));\n+            return VectorSupport.binaryOp(VECTOR_OP_AND, Byte512Mask.class, null, byte.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a & b));\n@@ -756,3 +787,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_OR, Byte512Mask.class, byte.class, VLENGTH,\n-                                             this, m,\n-                                             (m1, m2) -> m1.bOp(m2, (i, a, b) -> a | b));\n+            return VectorSupport.binaryOp(VECTOR_OP_OR, Byte512Mask.class, null, byte.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a | b));\n@@ -766,3 +797,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_XOR, Byte512Mask.class, byte.class, VLENGTH,\n-                                          this, m,\n-                                          (m1, m2) -> m1.bOp(m2, (i, a, b) -> a ^ b));\n+            return VectorSupport.binaryOp(VECTOR_OP_XOR, Byte512Mask.class, null, byte.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a ^ b));\n@@ -776,2 +807,2 @@\n-            return VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_TRUECOUNT, Byte512Mask.class, byte.class, VLENGTH, this,\n-                                                      (m) -> trueCountHelper(((Byte512Mask)m).getBits()));\n+            return (int) VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_TRUECOUNT, Byte512Mask.class, byte.class, VLENGTH, this,\n+                                                      (m) -> trueCountHelper(m.getBits()));\n@@ -783,2 +814,2 @@\n-            return VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_FIRSTTRUE, Byte512Mask.class, byte.class, VLENGTH, this,\n-                                                      (m) -> firstTrueHelper(((Byte512Mask)m).getBits()));\n+            return (int) VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_FIRSTTRUE, Byte512Mask.class, byte.class, VLENGTH, this,\n+                                                      (m) -> firstTrueHelper(m.getBits()));\n@@ -790,2 +821,12 @@\n-            return VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_LASTTRUE, Byte512Mask.class, byte.class, VLENGTH, this,\n-                                                      (m) -> lastTrueHelper(((Byte512Mask)m).getBits()));\n+            return (int) VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_LASTTRUE, Byte512Mask.class, byte.class, VLENGTH, this,\n+                                                      (m) -> lastTrueHelper(m.getBits()));\n+        }\n+\n+        @Override\n+        @ForceInline\n+        public long toLong() {\n+            if (length() > Long.SIZE) {\n+                throw new UnsupportedOperationException(\"too many lanes for one long\");\n+            }\n+            return VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_TOLONG, Byte512Mask.class, byte.class, VLENGTH, this,\n+                                                      (m) -> toLongHelper(m.getBits()));\n@@ -902,0 +943,8 @@\n+    @ForceInline\n+    @Override\n+    final\n+    ByteVector fromArray0(byte[] a, int offset, VectorMask<Byte> m) {\n+        return super.fromArray0Template(Byte512Mask.class, a, offset, (Byte512Mask) m);  \/\/ specialize\n+    }\n+\n+\n@@ -910,0 +959,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    ByteVector fromBooleanArray0(boolean[] a, int offset, VectorMask<Byte> m) {\n+        return super.fromBooleanArray0Template(Byte512Mask.class, a, offset, (Byte512Mask) m);  \/\/ specialize\n+    }\n+\n@@ -917,0 +973,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    ByteVector fromByteArray0(byte[] a, int offset, VectorMask<Byte> m) {\n+        return super.fromByteArray0Template(Byte512Mask.class, a, offset, (Byte512Mask) m);  \/\/ specialize\n+    }\n+\n@@ -924,0 +987,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    ByteVector fromByteBuffer0(ByteBuffer bb, int offset, VectorMask<Byte> m) {\n+        return super.fromByteBuffer0Template(Byte512Mask.class, bb, offset, (Byte512Mask) m);  \/\/ specialize\n+    }\n+\n@@ -931,0 +1001,15 @@\n+    @ForceInline\n+    @Override\n+    final\n+    void intoArray0(byte[] a, int offset, VectorMask<Byte> m) {\n+        super.intoArray0Template(Byte512Mask.class, a, offset, (Byte512Mask) m);\n+    }\n+\n+\n+    @ForceInline\n+    @Override\n+    final\n+    void intoBooleanArray0(boolean[] a, int offset, VectorMask<Byte> m) {\n+        super.intoBooleanArray0Template(Byte512Mask.class, a, offset, (Byte512Mask) m);\n+    }\n+\n@@ -938,0 +1023,15 @@\n+    @ForceInline\n+    @Override\n+    final\n+    void intoByteArray0(byte[] a, int offset, VectorMask<Byte> m) {\n+        super.intoByteArray0Template(Byte512Mask.class, a, offset, (Byte512Mask) m);  \/\/ specialize\n+    }\n+\n+    @ForceInline\n+    @Override\n+    final\n+    void intoByteBuffer0(ByteBuffer bb, int offset, VectorMask<Byte> m) {\n+        super.intoByteBuffer0Template(Byte512Mask.class, bb, offset, (Byte512Mask) m);\n+    }\n+\n+\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/Byte512Vector.java","additions":130,"deletions":30,"binary":false,"changes":160,"status":"modified"},{"patch":"@@ -239,2 +239,2 @@\n-    byte rOp(byte v, FBinOp f) {\n-        return super.rOpTemplate(v, f);  \/\/ specialize\n+    byte rOp(byte v, VectorMask<Byte> m, FBinOp f) {\n+        return super.rOpTemplate(v, m, f);  \/\/ specialize\n@@ -276,0 +276,6 @@\n+    @Override\n+    @ForceInline\n+    public Byte64Vector lanewise(Unary op, VectorMask<Byte> m) {\n+        return (Byte64Vector) super.lanewiseTemplate(op, Byte64Mask.class, (Byte64Mask) m);  \/\/ specialize\n+    }\n+\n@@ -282,0 +288,6 @@\n+    @Override\n+    @ForceInline\n+    public Byte64Vector lanewise(Binary op, Vector<Byte> v, VectorMask<Byte> m) {\n+        return (Byte64Vector) super.lanewiseTemplate(op, Byte64Mask.class, v, (Byte64Mask) m);  \/\/ specialize\n+    }\n+\n@@ -289,0 +301,7 @@\n+    \/*package-private*\/\n+    @Override\n+    @ForceInline Byte64Vector\n+    lanewiseShift(VectorOperators.Binary op, int e, VectorMask<Byte> m) {\n+        return (Byte64Vector) super.lanewiseShiftTemplate(op, Byte64Mask.class, e, (Byte64Mask) m);  \/\/ specialize\n+    }\n+\n@@ -294,1 +313,1 @@\n-    lanewise(VectorOperators.Ternary op, Vector<Byte> v1, Vector<Byte> v2) {\n+    lanewise(Ternary op, Vector<Byte> v1, Vector<Byte> v2) {\n@@ -298,0 +317,8 @@\n+    @Override\n+    @ForceInline\n+    public final\n+    Byte64Vector\n+    lanewise(Ternary op, Vector<Byte> v1, Vector<Byte> v2, VectorMask<Byte> m) {\n+        return (Byte64Vector) super.lanewiseTemplate(op, Byte64Mask.class, v1, v2, (Byte64Mask) m);  \/\/ specialize\n+    }\n+\n@@ -317,1 +344,1 @@\n-        return super.reduceLanesTemplate(op, m);  \/\/ specialized\n+        return super.reduceLanesTemplate(op, Byte64Mask.class, (Byte64Mask) m);  \/\/ specialized\n@@ -330,1 +357,1 @@\n-        return (long) super.reduceLanesTemplate(op, m);  \/\/ specialized\n+        return (long) super.reduceLanesTemplate(op, Byte64Mask.class, (Byte64Mask) m);  \/\/ specialized\n@@ -366,0 +393,7 @@\n+    @Override\n+    @ForceInline\n+    public final Byte64Mask compare(Comparison op, Vector<Byte> v, VectorMask<Byte> m) {\n+        return super.compareTemplate(Byte64Mask.class, op, v, (Byte64Mask) m);\n+    }\n+\n+\n@@ -422,0 +456,1 @@\n+                                    Byte64Mask.class,\n@@ -599,10 +634,6 @@\n-            if (VSIZE == species.vectorBitSize()) {\n-                Class<?> dtype = species.elementType();\n-                Class<?> dmtype = species.maskType();\n-                return VectorSupport.convert(VectorSupport.VECTOR_OP_REINTERPRET,\n-                    this.getClass(), ETYPE, VLENGTH,\n-                    dmtype, dtype, VLENGTH,\n-                    this, species,\n-                    Byte64Mask::defaultMaskCast);\n-            }\n-            return this.defaultMaskCast(species);\n+\n+            return VectorSupport.convert(VectorSupport.VECTOR_OP_CAST,\n+                this.getClass(), ETYPE, VLENGTH,\n+                species.maskType(), species.elementType(), VLENGTH,\n+                this, species,\n+                (m, s) -> s.maskFactory(m.toArray()).check(s));\n@@ -634,3 +665,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_AND, Byte64Mask.class, byte.class, VLENGTH,\n-                                             this, m,\n-                                             (m1, m2) -> m1.bOp(m2, (i, a, b) -> a & b));\n+            return VectorSupport.binaryOp(VECTOR_OP_AND, Byte64Mask.class, null, byte.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a & b));\n@@ -644,3 +675,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_OR, Byte64Mask.class, byte.class, VLENGTH,\n-                                             this, m,\n-                                             (m1, m2) -> m1.bOp(m2, (i, a, b) -> a | b));\n+            return VectorSupport.binaryOp(VECTOR_OP_OR, Byte64Mask.class, null, byte.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a | b));\n@@ -654,3 +685,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_XOR, Byte64Mask.class, byte.class, VLENGTH,\n-                                          this, m,\n-                                          (m1, m2) -> m1.bOp(m2, (i, a, b) -> a ^ b));\n+            return VectorSupport.binaryOp(VECTOR_OP_XOR, Byte64Mask.class, null, byte.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a ^ b));\n@@ -664,2 +695,2 @@\n-            return VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_TRUECOUNT, Byte64Mask.class, byte.class, VLENGTH, this,\n-                                                      (m) -> trueCountHelper(((Byte64Mask)m).getBits()));\n+            return (int) VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_TRUECOUNT, Byte64Mask.class, byte.class, VLENGTH, this,\n+                                                      (m) -> trueCountHelper(m.getBits()));\n@@ -671,2 +702,2 @@\n-            return VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_FIRSTTRUE, Byte64Mask.class, byte.class, VLENGTH, this,\n-                                                      (m) -> firstTrueHelper(((Byte64Mask)m).getBits()));\n+            return (int) VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_FIRSTTRUE, Byte64Mask.class, byte.class, VLENGTH, this,\n+                                                      (m) -> firstTrueHelper(m.getBits()));\n@@ -678,2 +709,12 @@\n-            return VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_LASTTRUE, Byte64Mask.class, byte.class, VLENGTH, this,\n-                                                      (m) -> lastTrueHelper(((Byte64Mask)m).getBits()));\n+            return (int) VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_LASTTRUE, Byte64Mask.class, byte.class, VLENGTH, this,\n+                                                      (m) -> lastTrueHelper(m.getBits()));\n+        }\n+\n+        @Override\n+        @ForceInline\n+        public long toLong() {\n+            if (length() > Long.SIZE) {\n+                throw new UnsupportedOperationException(\"too many lanes for one long\");\n+            }\n+            return VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_TOLONG, Byte64Mask.class, byte.class, VLENGTH, this,\n+                                                      (m) -> toLongHelper(m.getBits()));\n@@ -790,0 +831,8 @@\n+    @ForceInline\n+    @Override\n+    final\n+    ByteVector fromArray0(byte[] a, int offset, VectorMask<Byte> m) {\n+        return super.fromArray0Template(Byte64Mask.class, a, offset, (Byte64Mask) m);  \/\/ specialize\n+    }\n+\n+\n@@ -798,0 +847,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    ByteVector fromBooleanArray0(boolean[] a, int offset, VectorMask<Byte> m) {\n+        return super.fromBooleanArray0Template(Byte64Mask.class, a, offset, (Byte64Mask) m);  \/\/ specialize\n+    }\n+\n@@ -805,0 +861,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    ByteVector fromByteArray0(byte[] a, int offset, VectorMask<Byte> m) {\n+        return super.fromByteArray0Template(Byte64Mask.class, a, offset, (Byte64Mask) m);  \/\/ specialize\n+    }\n+\n@@ -812,0 +875,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    ByteVector fromByteBuffer0(ByteBuffer bb, int offset, VectorMask<Byte> m) {\n+        return super.fromByteBuffer0Template(Byte64Mask.class, bb, offset, (Byte64Mask) m);  \/\/ specialize\n+    }\n+\n@@ -819,0 +889,15 @@\n+    @ForceInline\n+    @Override\n+    final\n+    void intoArray0(byte[] a, int offset, VectorMask<Byte> m) {\n+        super.intoArray0Template(Byte64Mask.class, a, offset, (Byte64Mask) m);\n+    }\n+\n+\n+    @ForceInline\n+    @Override\n+    final\n+    void intoBooleanArray0(boolean[] a, int offset, VectorMask<Byte> m) {\n+        super.intoBooleanArray0Template(Byte64Mask.class, a, offset, (Byte64Mask) m);\n+    }\n+\n@@ -826,0 +911,15 @@\n+    @ForceInline\n+    @Override\n+    final\n+    void intoByteArray0(byte[] a, int offset, VectorMask<Byte> m) {\n+        super.intoByteArray0Template(Byte64Mask.class, a, offset, (Byte64Mask) m);  \/\/ specialize\n+    }\n+\n+    @ForceInline\n+    @Override\n+    final\n+    void intoByteBuffer0(ByteBuffer bb, int offset, VectorMask<Byte> m) {\n+        super.intoByteBuffer0Template(Byte64Mask.class, bb, offset, (Byte64Mask) m);\n+    }\n+\n+\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/Byte64Vector.java","additions":130,"deletions":30,"binary":false,"changes":160,"status":"modified"},{"patch":"@@ -239,2 +239,2 @@\n-    byte rOp(byte v, FBinOp f) {\n-        return super.rOpTemplate(v, f);  \/\/ specialize\n+    byte rOp(byte v, VectorMask<Byte> m, FBinOp f) {\n+        return super.rOpTemplate(v, m, f);  \/\/ specialize\n@@ -276,0 +276,6 @@\n+    @Override\n+    @ForceInline\n+    public ByteMaxVector lanewise(Unary op, VectorMask<Byte> m) {\n+        return (ByteMaxVector) super.lanewiseTemplate(op, ByteMaxMask.class, (ByteMaxMask) m);  \/\/ specialize\n+    }\n+\n@@ -282,0 +288,6 @@\n+    @Override\n+    @ForceInline\n+    public ByteMaxVector lanewise(Binary op, Vector<Byte> v, VectorMask<Byte> m) {\n+        return (ByteMaxVector) super.lanewiseTemplate(op, ByteMaxMask.class, v, (ByteMaxMask) m);  \/\/ specialize\n+    }\n+\n@@ -289,0 +301,7 @@\n+    \/*package-private*\/\n+    @Override\n+    @ForceInline ByteMaxVector\n+    lanewiseShift(VectorOperators.Binary op, int e, VectorMask<Byte> m) {\n+        return (ByteMaxVector) super.lanewiseShiftTemplate(op, ByteMaxMask.class, e, (ByteMaxMask) m);  \/\/ specialize\n+    }\n+\n@@ -294,1 +313,1 @@\n-    lanewise(VectorOperators.Ternary op, Vector<Byte> v1, Vector<Byte> v2) {\n+    lanewise(Ternary op, Vector<Byte> v1, Vector<Byte> v2) {\n@@ -298,0 +317,8 @@\n+    @Override\n+    @ForceInline\n+    public final\n+    ByteMaxVector\n+    lanewise(Ternary op, Vector<Byte> v1, Vector<Byte> v2, VectorMask<Byte> m) {\n+        return (ByteMaxVector) super.lanewiseTemplate(op, ByteMaxMask.class, v1, v2, (ByteMaxMask) m);  \/\/ specialize\n+    }\n+\n@@ -317,1 +344,1 @@\n-        return super.reduceLanesTemplate(op, m);  \/\/ specialized\n+        return super.reduceLanesTemplate(op, ByteMaxMask.class, (ByteMaxMask) m);  \/\/ specialized\n@@ -330,1 +357,1 @@\n-        return (long) super.reduceLanesTemplate(op, m);  \/\/ specialized\n+        return (long) super.reduceLanesTemplate(op, ByteMaxMask.class, (ByteMaxMask) m);  \/\/ specialized\n@@ -366,0 +393,7 @@\n+    @Override\n+    @ForceInline\n+    public final ByteMaxMask compare(Comparison op, Vector<Byte> v, VectorMask<Byte> m) {\n+        return super.compareTemplate(ByteMaxMask.class, op, v, (ByteMaxMask) m);\n+    }\n+\n+\n@@ -422,0 +456,1 @@\n+                                    ByteMaxMask.class,\n@@ -585,10 +620,6 @@\n-            if (VSIZE == species.vectorBitSize()) {\n-                Class<?> dtype = species.elementType();\n-                Class<?> dmtype = species.maskType();\n-                return VectorSupport.convert(VectorSupport.VECTOR_OP_REINTERPRET,\n-                    this.getClass(), ETYPE, VLENGTH,\n-                    dmtype, dtype, VLENGTH,\n-                    this, species,\n-                    ByteMaxMask::defaultMaskCast);\n-            }\n-            return this.defaultMaskCast(species);\n+\n+            return VectorSupport.convert(VectorSupport.VECTOR_OP_CAST,\n+                this.getClass(), ETYPE, VLENGTH,\n+                species.maskType(), species.elementType(), VLENGTH,\n+                this, species,\n+                (m, s) -> s.maskFactory(m.toArray()).check(s));\n@@ -620,3 +651,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_AND, ByteMaxMask.class, byte.class, VLENGTH,\n-                                             this, m,\n-                                             (m1, m2) -> m1.bOp(m2, (i, a, b) -> a & b));\n+            return VectorSupport.binaryOp(VECTOR_OP_AND, ByteMaxMask.class, null, byte.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a & b));\n@@ -630,3 +661,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_OR, ByteMaxMask.class, byte.class, VLENGTH,\n-                                             this, m,\n-                                             (m1, m2) -> m1.bOp(m2, (i, a, b) -> a | b));\n+            return VectorSupport.binaryOp(VECTOR_OP_OR, ByteMaxMask.class, null, byte.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a | b));\n@@ -640,3 +671,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_XOR, ByteMaxMask.class, byte.class, VLENGTH,\n-                                          this, m,\n-                                          (m1, m2) -> m1.bOp(m2, (i, a, b) -> a ^ b));\n+            return VectorSupport.binaryOp(VECTOR_OP_XOR, ByteMaxMask.class, null, byte.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a ^ b));\n@@ -650,2 +681,2 @@\n-            return VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_TRUECOUNT, ByteMaxMask.class, byte.class, VLENGTH, this,\n-                                                      (m) -> trueCountHelper(((ByteMaxMask)m).getBits()));\n+            return (int) VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_TRUECOUNT, ByteMaxMask.class, byte.class, VLENGTH, this,\n+                                                      (m) -> trueCountHelper(m.getBits()));\n@@ -657,2 +688,2 @@\n-            return VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_FIRSTTRUE, ByteMaxMask.class, byte.class, VLENGTH, this,\n-                                                      (m) -> firstTrueHelper(((ByteMaxMask)m).getBits()));\n+            return (int) VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_FIRSTTRUE, ByteMaxMask.class, byte.class, VLENGTH, this,\n+                                                      (m) -> firstTrueHelper(m.getBits()));\n@@ -664,2 +695,12 @@\n-            return VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_LASTTRUE, ByteMaxMask.class, byte.class, VLENGTH, this,\n-                                                      (m) -> lastTrueHelper(((ByteMaxMask)m).getBits()));\n+            return (int) VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_LASTTRUE, ByteMaxMask.class, byte.class, VLENGTH, this,\n+                                                      (m) -> lastTrueHelper(m.getBits()));\n+        }\n+\n+        @Override\n+        @ForceInline\n+        public long toLong() {\n+            if (length() > Long.SIZE) {\n+                throw new UnsupportedOperationException(\"too many lanes for one long\");\n+            }\n+            return VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_TOLONG, ByteMaxMask.class, byte.class, VLENGTH, this,\n+                                                      (m) -> toLongHelper(m.getBits()));\n@@ -776,0 +817,8 @@\n+    @ForceInline\n+    @Override\n+    final\n+    ByteVector fromArray0(byte[] a, int offset, VectorMask<Byte> m) {\n+        return super.fromArray0Template(ByteMaxMask.class, a, offset, (ByteMaxMask) m);  \/\/ specialize\n+    }\n+\n+\n@@ -784,0 +833,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    ByteVector fromBooleanArray0(boolean[] a, int offset, VectorMask<Byte> m) {\n+        return super.fromBooleanArray0Template(ByteMaxMask.class, a, offset, (ByteMaxMask) m);  \/\/ specialize\n+    }\n+\n@@ -791,0 +847,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    ByteVector fromByteArray0(byte[] a, int offset, VectorMask<Byte> m) {\n+        return super.fromByteArray0Template(ByteMaxMask.class, a, offset, (ByteMaxMask) m);  \/\/ specialize\n+    }\n+\n@@ -798,0 +861,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    ByteVector fromByteBuffer0(ByteBuffer bb, int offset, VectorMask<Byte> m) {\n+        return super.fromByteBuffer0Template(ByteMaxMask.class, bb, offset, (ByteMaxMask) m);  \/\/ specialize\n+    }\n+\n@@ -805,0 +875,15 @@\n+    @ForceInline\n+    @Override\n+    final\n+    void intoArray0(byte[] a, int offset, VectorMask<Byte> m) {\n+        super.intoArray0Template(ByteMaxMask.class, a, offset, (ByteMaxMask) m);\n+    }\n+\n+\n+    @ForceInline\n+    @Override\n+    final\n+    void intoBooleanArray0(boolean[] a, int offset, VectorMask<Byte> m) {\n+        super.intoBooleanArray0Template(ByteMaxMask.class, a, offset, (ByteMaxMask) m);\n+    }\n+\n@@ -812,0 +897,15 @@\n+    @ForceInline\n+    @Override\n+    final\n+    void intoByteArray0(byte[] a, int offset, VectorMask<Byte> m) {\n+        super.intoByteArray0Template(ByteMaxMask.class, a, offset, (ByteMaxMask) m);  \/\/ specialize\n+    }\n+\n+    @ForceInline\n+    @Override\n+    final\n+    void intoByteBuffer0(ByteBuffer bb, int offset, VectorMask<Byte> m) {\n+        super.intoByteBuffer0Template(ByteMaxMask.class, bb, offset, (ByteMaxMask) m);\n+    }\n+\n+\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/ByteMaxVector.java","additions":130,"deletions":30,"binary":false,"changes":160,"status":"modified"},{"patch":"@@ -32,1 +32,0 @@\n-import java.util.function.BinaryOperator;\n@@ -176,0 +175,3 @@\n+        if (m == null) {\n+            return uOpTemplate(f);\n+        }\n@@ -219,0 +221,3 @@\n+        if (m == null) {\n+            return bOpTemplate(o, f);\n+        }\n@@ -268,0 +273,3 @@\n+        if (m == null) {\n+            return tOpTemplate(o1, o2, f);\n+        }\n@@ -283,1 +291,16 @@\n-    byte rOp(byte v, FBinOp f);\n+    byte rOp(byte v, VectorMask<Byte> m, FBinOp f);\n+\n+    @ForceInline\n+    final\n+    byte rOpTemplate(byte v, VectorMask<Byte> m, FBinOp f) {\n+        if (m == null) {\n+            return rOpTemplate(v, f);\n+        }\n+        byte[] vec = vec();\n+        boolean[] mbits = ((AbstractMask<Byte>)m).getBits();\n+        for (int i = 0; i < vec.length; i++) {\n+            v = mbits[i] ? f.apply(i, v, vec[i]) : v;\n+        }\n+        return v;\n+    }\n+\n@@ -552,1 +575,1 @@\n-                return broadcast(-1).lanewiseTemplate(XOR, this);\n+                return broadcast(-1).lanewise(XOR, this);\n@@ -555,1 +578,1 @@\n-                return broadcast(0).lanewiseTemplate(SUB, this);\n+                return broadcast(0).lanewise(SUB, this);\n@@ -560,10 +583,3 @@\n-            opc, getClass(), byte.class, length(),\n-            this,\n-            UN_IMPL.find(op, opc, (opc_) -> {\n-              switch (opc_) {\n-                case VECTOR_OP_NEG: return v0 ->\n-                        v0.uOp((i, a) -> (byte) -a);\n-                case VECTOR_OP_ABS: return v0 ->\n-                        v0.uOp((i, a) -> (byte) Math.abs(a));\n-                default: return null;\n-              }}));\n+            opc, getClass(), null, byte.class, length(),\n+            this, null,\n+            UN_IMPL.find(op, opc, ByteVector::unaryOperations));\n@@ -571,3 +587,0 @@\n-    private static final\n-    ImplCache<Unary,UnaryOperator<ByteVector>> UN_IMPL\n-        = new ImplCache<>(Unary.class, ByteVector.class);\n@@ -578,2 +591,2 @@\n-    @ForceInline\n-    public final\n+    @Override\n+    public abstract\n@@ -581,2 +594,36 @@\n-                                  VectorMask<Byte> m) {\n-        return blend(lanewise(op), m);\n+                                  VectorMask<Byte> m);\n+    @ForceInline\n+    final\n+    ByteVector lanewiseTemplate(VectorOperators.Unary op,\n+                                          Class<? extends VectorMask<Byte>> maskClass,\n+                                          VectorMask<Byte> m) {\n+        m.check(maskClass, this);\n+        if (opKind(op, VO_SPECIAL)) {\n+            if (op == ZOMO) {\n+                return blend(broadcast(-1), compare(NE, 0, m));\n+            }\n+            if (op == NOT) {\n+                return lanewise(XOR, broadcast(-1), m);\n+            } else if (op == NEG) {\n+                return lanewise(NOT, m).lanewise(ADD, broadcast(1), m);\n+            }\n+        }\n+        int opc = opCode(op);\n+        return VectorSupport.unaryOp(\n+            opc, getClass(), maskClass, byte.class, length(),\n+            this, m,\n+            UN_IMPL.find(op, opc, ByteVector::unaryOperations));\n+    }\n+\n+    private static final\n+    ImplCache<Unary, UnaryOperation<ByteVector, VectorMask<Byte>>>\n+        UN_IMPL = new ImplCache<>(Unary.class, ByteVector.class);\n+\n+    private static UnaryOperation<ByteVector, VectorMask<Byte>> unaryOperations(int opc_) {\n+        switch (opc_) {\n+            case VECTOR_OP_NEG: return (v0, m) ->\n+                    v0.uOp(m, (i, a) -> (byte) -a);\n+            case VECTOR_OP_ABS: return (v0, m) ->\n+                    v0.uOp(m, (i, a) -> (byte) Math.abs(a));\n+            default: return null;\n+        }\n@@ -602,0 +649,1 @@\n+\n@@ -620,1 +668,1 @@\n-                VectorMask<Byte> eqz = that.eq((byte)0);\n+                VectorMask<Byte> eqz = that.eq((byte) 0);\n@@ -626,0 +674,1 @@\n+\n@@ -628,34 +677,3 @@\n-            opc, getClass(), byte.class, length(),\n-            this, that,\n-            BIN_IMPL.find(op, opc, (opc_) -> {\n-              switch (opc_) {\n-                case VECTOR_OP_ADD: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (byte)(a + b));\n-                case VECTOR_OP_SUB: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (byte)(a - b));\n-                case VECTOR_OP_MUL: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (byte)(a * b));\n-                case VECTOR_OP_DIV: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (byte)(a \/ b));\n-                case VECTOR_OP_MAX: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (byte)Math.max(a, b));\n-                case VECTOR_OP_MIN: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (byte)Math.min(a, b));\n-                case VECTOR_OP_AND: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (byte)(a & b));\n-                case VECTOR_OP_OR: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (byte)(a | b));\n-                case VECTOR_OP_XOR: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (byte)(a ^ b));\n-                case VECTOR_OP_LSHIFT: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, n) -> (byte)(a << n));\n-                case VECTOR_OP_RSHIFT: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, n) -> (byte)(a >> n));\n-                case VECTOR_OP_URSHIFT: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, n) -> (byte)((a & LSHR_SETUP_MASK) >>> n));\n-                case VECTOR_OP_LROTATE: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, n) -> rotateLeft(a, (int)n));\n-                case VECTOR_OP_RROTATE: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, n) -> rotateRight(a, (int)n));\n-                default: return null;\n-                }}));\n+            opc, getClass(), null, byte.class, length(),\n+            this, that, null,\n+            BIN_IMPL.find(op, opc, ByteVector::binaryOperations));\n@@ -663,3 +681,0 @@\n-    private static final\n-    ImplCache<Binary,BinaryOperator<ByteVector>> BIN_IMPL\n-        = new ImplCache<>(Binary.class, ByteVector.class);\n@@ -671,2 +686,2 @@\n-    @ForceInline\n-    public final\n+    @Override\n+    public abstract\n@@ -675,1 +690,6 @@\n-                                  VectorMask<Byte> m) {\n+                                  VectorMask<Byte> m);\n+    @ForceInline\n+    final\n+    ByteVector lanewiseTemplate(VectorOperators.Binary op,\n+                                          Class<? extends VectorMask<Byte>> maskClass,\n+                                          Vector<Byte> v, VectorMask<Byte> m) {\n@@ -677,4 +697,27 @@\n-        if (op == DIV) {\n-            VectorMask<Byte> eqz = that.eq((byte)0);\n-            if (eqz.and(m).anyTrue()) {\n-                throw that.divZeroException();\n+        that.check(this);\n+        m.check(maskClass, this);\n+\n+        if (opKind(op, VO_SPECIAL  | VO_SHIFT)) {\n+            if (op == FIRST_NONZERO) {\n+                \/\/ FIXME: Support this in the JIT.\n+                VectorMask<Byte> thisNZ\n+                    = this.viewAsIntegralLanes().compare(NE, (byte) 0);\n+                that = that.blend((byte) 0, thisNZ.cast(vspecies()));\n+                op = OR_UNCHECKED;\n+            }\n+            if (opKind(op, VO_SHIFT)) {\n+                \/\/ As per shift specification for Java, mask the shift count.\n+                \/\/ This allows the JIT to ignore some ISA details.\n+                that = that.lanewise(AND, SHIFT_MASK);\n+            }\n+            if (op == AND_NOT) {\n+                \/\/ FIXME: Support this in the JIT.\n+                that = that.lanewise(NOT);\n+                op = AND;\n+            } else if (op == DIV) {\n+                VectorMask<Byte> eqz = that.eq((byte)0);\n+                if (eqz.and(m).anyTrue()) {\n+                    throw that.divZeroException();\n+                }\n+                \/\/ suppress div\/0 exceptions in unset lanes\n+                that = that.lanewise(NOT, eqz);\n@@ -682,3 +725,0 @@\n-            \/\/ suppress div\/0 exceptions in unset lanes\n-            that = that.lanewise(NOT, eqz);\n-            return blend(lanewise(DIV, that), m);\n@@ -686,1 +726,44 @@\n-        return blend(lanewise(op, v), m);\n+\n+        int opc = opCode(op);\n+        return VectorSupport.binaryOp(\n+            opc, getClass(), maskClass, byte.class, length(),\n+            this, that, m,\n+            BIN_IMPL.find(op, opc, ByteVector::binaryOperations));\n+    }\n+\n+    private static final\n+    ImplCache<Binary, BinaryOperation<ByteVector, VectorMask<Byte>>>\n+        BIN_IMPL = new ImplCache<>(Binary.class, ByteVector.class);\n+\n+    private static BinaryOperation<ByteVector, VectorMask<Byte>> binaryOperations(int opc_) {\n+        switch (opc_) {\n+            case VECTOR_OP_ADD: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> (byte)(a + b));\n+            case VECTOR_OP_SUB: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> (byte)(a - b));\n+            case VECTOR_OP_MUL: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> (byte)(a * b));\n+            case VECTOR_OP_DIV: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> (byte)(a \/ b));\n+            case VECTOR_OP_MAX: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> (byte)Math.max(a, b));\n+            case VECTOR_OP_MIN: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> (byte)Math.min(a, b));\n+            case VECTOR_OP_AND: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> (byte)(a & b));\n+            case VECTOR_OP_OR: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> (byte)(a | b));\n+            case VECTOR_OP_XOR: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> (byte)(a ^ b));\n+            case VECTOR_OP_LSHIFT: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, n) -> (byte)(a << n));\n+            case VECTOR_OP_RSHIFT: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, n) -> (byte)(a >> n));\n+            case VECTOR_OP_URSHIFT: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, n) -> (byte)((a & LSHR_SETUP_MASK) >>> n));\n+            case VECTOR_OP_LROTATE: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, n) -> rotateLeft(a, (int)n));\n+            case VECTOR_OP_RROTATE: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, n) -> rotateRight(a, (int)n));\n+            default: return null;\n+        }\n@@ -688,0 +771,1 @@\n+\n@@ -750,1 +834,7 @@\n-        return blend(lanewise(op, e), m);\n+        if (opKind(op, VO_SHIFT) && (byte)(int)e == e) {\n+            return lanewiseShift(op, (int) e, m);\n+        }\n+        if (op == AND_NOT) {\n+            op = AND; e = (byte) ~e;\n+        }\n+        return lanewise(op, broadcast(e), m);\n@@ -770,2 +860,1 @@\n-            && !(opKind(op, VO_SHIFT) && (int)e1 == e)\n-            ) {\n+            && !(opKind(op, VO_SHIFT) && (int)e1 == e)) {\n@@ -791,1 +880,7 @@\n-        return blend(lanewise(op, e), m);\n+        byte e1 = (byte) e;\n+        if ((long)e1 != e\n+            \/\/ allow shift ops to clip down their int parameters\n+            && !(opKind(op, VO_SHIFT) && (int)e1 == e)) {\n+            vspecies().checkValue(e);  \/\/ for exception\n+        }\n+        return lanewise(op, e1, m);\n@@ -808,16 +903,3 @@\n-            opc, getClass(), byte.class, length(),\n-            this, e,\n-            BIN_INT_IMPL.find(op, opc, (opc_) -> {\n-              switch (opc_) {\n-                case VECTOR_OP_LSHIFT: return (v, n) ->\n-                        v.uOp((i, a) -> (byte)(a << n));\n-                case VECTOR_OP_RSHIFT: return (v, n) ->\n-                        v.uOp((i, a) -> (byte)(a >> n));\n-                case VECTOR_OP_URSHIFT: return (v, n) ->\n-                        v.uOp((i, a) -> (byte)((a & LSHR_SETUP_MASK) >>> n));\n-                case VECTOR_OP_LROTATE: return (v, n) ->\n-                        v.uOp((i, a) -> rotateLeft(a, (int)n));\n-                case VECTOR_OP_RROTATE: return (v, n) ->\n-                        v.uOp((i, a) -> rotateRight(a, (int)n));\n-                default: return null;\n-                }}));\n+            opc, getClass(), null, byte.class, length(),\n+            this, e, null,\n+            BIN_INT_IMPL.find(op, opc, ByteVector::broadcastIntOperations));\n@@ -825,0 +907,22 @@\n+\n+    \/*package-private*\/\n+    abstract ByteVector\n+    lanewiseShift(VectorOperators.Binary op, int e, VectorMask<Byte> m);\n+\n+    \/*package-private*\/\n+    @ForceInline\n+    final ByteVector\n+    lanewiseShiftTemplate(VectorOperators.Binary op,\n+                          Class<? extends VectorMask<Byte>> maskClass,\n+                          int e, VectorMask<Byte> m) {\n+        m.check(maskClass, this);\n+        assert(opKind(op, VO_SHIFT));\n+        \/\/ As per shift specification for Java, mask the shift count.\n+        e &= SHIFT_MASK;\n+        int opc = opCode(op);\n+        return VectorSupport.broadcastInt(\n+            opc, getClass(), maskClass, byte.class, length(),\n+            this, e, m,\n+            BIN_INT_IMPL.find(op, opc, ByteVector::broadcastIntOperations));\n+    }\n+\n@@ -826,1 +930,1 @@\n-    ImplCache<Binary,VectorBroadcastIntOp<ByteVector>> BIN_INT_IMPL\n+    ImplCache<Binary,VectorBroadcastIntOp<ByteVector, VectorMask<Byte>>> BIN_INT_IMPL\n@@ -829,0 +933,16 @@\n+    private static VectorBroadcastIntOp<ByteVector, VectorMask<Byte>> broadcastIntOperations(int opc_) {\n+        switch (opc_) {\n+            case VECTOR_OP_LSHIFT: return (v, n, m) ->\n+                    v.uOp(m, (i, a) -> (byte)(a << n));\n+            case VECTOR_OP_RSHIFT: return (v, n, m) ->\n+                    v.uOp(m, (i, a) -> (byte)(a >> n));\n+            case VECTOR_OP_URSHIFT: return (v, n, m) ->\n+                    v.uOp(m, (i, a) -> (byte)((a & LSHR_SETUP_MASK) >>> n));\n+            case VECTOR_OP_LROTATE: return (v, n, m) ->\n+                    v.uOp(m, (i, a) -> rotateLeft(a, (int)n));\n+            case VECTOR_OP_RROTATE: return (v, n, m) ->\n+                    v.uOp(m, (i, a) -> rotateRight(a, (int)n));\n+            default: return null;\n+        }\n+    }\n+\n@@ -881,6 +1001,3 @@\n-            opc, getClass(), byte.class, length(),\n-            this, that, tother,\n-            TERN_IMPL.find(op, opc, (opc_) -> {\n-              switch (opc_) {\n-                default: return null;\n-                }}));\n+            opc, getClass(), null, byte.class, length(),\n+            this, that, tother, null,\n+            TERN_IMPL.find(op, opc, ByteVector::ternaryOperations));\n@@ -888,3 +1005,0 @@\n-    private static final\n-    ImplCache<Ternary,TernaryOperation<ByteVector>> TERN_IMPL\n-        = new ImplCache<>(Ternary.class, ByteVector.class);\n@@ -898,2 +1012,2 @@\n-    @ForceInline\n-    public final\n+    @Override\n+    public abstract\n@@ -903,2 +1017,37 @@\n-                                  VectorMask<Byte> m) {\n-        return blend(lanewise(op, v1, v2), m);\n+                                  VectorMask<Byte> m);\n+    @ForceInline\n+    final\n+    ByteVector lanewiseTemplate(VectorOperators.Ternary op,\n+                                          Class<? extends VectorMask<Byte>> maskClass,\n+                                          Vector<Byte> v1,\n+                                          Vector<Byte> v2,\n+                                          VectorMask<Byte> m) {\n+        ByteVector that = (ByteVector) v1;\n+        ByteVector tother = (ByteVector) v2;\n+        \/\/ It's a word: https:\/\/www.dictionary.com\/browse\/tother\n+        \/\/ See also Chapter 11 of Dickens, Our Mutual Friend:\n+        \/\/ \"Totherest Governor,\" replied Mr Riderhood...\n+        that.check(this);\n+        tother.check(this);\n+        m.check(maskClass, this);\n+\n+        if (op == BITWISE_BLEND) {\n+            \/\/ FIXME: Support this in the JIT.\n+            that = this.lanewise(XOR, that).lanewise(AND, tother);\n+            return this.lanewise(XOR, that, m);\n+        }\n+        int opc = opCode(op);\n+        return VectorSupport.ternaryOp(\n+            opc, getClass(), maskClass, byte.class, length(),\n+            this, that, tother, m,\n+            TERN_IMPL.find(op, opc, ByteVector::ternaryOperations));\n+    }\n+\n+    private static final\n+    ImplCache<Ternary, TernaryOperation<ByteVector, VectorMask<Byte>>>\n+        TERN_IMPL = new ImplCache<>(Ternary.class, ByteVector.class);\n+\n+    private static TernaryOperation<ByteVector, VectorMask<Byte>> ternaryOperations(int opc_) {\n+        switch (opc_) {\n+            default: return null;\n+        }\n@@ -961,1 +1110,1 @@\n-        return blend(lanewise(op, e1, e2), m);\n+        return lanewise(op, broadcast(e1), broadcast(e2), m);\n@@ -1019,1 +1168,1 @@\n-        return blend(lanewise(op, v1, e2), m);\n+        return lanewise(op, v1, broadcast(e2), m);\n@@ -1076,1 +1225,1 @@\n-        return blend(lanewise(op, e1, v2), m);\n+        return lanewise(op, broadcast(e1), v2, m);\n@@ -1748,2 +1897,0 @@\n-        Objects.requireNonNull(v);\n-        ByteSpecies vsp = vspecies();\n@@ -1755,2 +1902,2 @@\n-            this, that,\n-            (cond, v0, v1) -> {\n+            this, that, null,\n+            (cond, v0, v1, m1) -> {\n@@ -1766,0 +1913,22 @@\n+    \/*package-private*\/\n+    @ForceInline\n+    final\n+    <M extends VectorMask<Byte>>\n+    M compareTemplate(Class<M> maskType, Comparison op, Vector<Byte> v, M m) {\n+        ByteVector that = (ByteVector) v;\n+        that.check(this);\n+        m.check(maskType, this);\n+        int opc = opCode(op);\n+        return VectorSupport.compare(\n+            opc, getClass(), maskType, byte.class, length(),\n+            this, that, m,\n+            (cond, v0, v1, m1) -> {\n+                AbstractMask<Byte> cmpM\n+                    = v0.bTest(cond, v1, (cond_, i, a, b)\n+                               -> compareWithOp(cond, a, b));\n+                @SuppressWarnings(\"unchecked\")\n+                M m2 = (M) cmpM.and(m1);\n+                return m2;\n+            });\n+    }\n+\n@@ -1783,12 +1952,0 @@\n-    \/**\n-     * {@inheritDoc} <!--workaround-->\n-     *\/\n-    @Override\n-    @ForceInline\n-    public final\n-    VectorMask<Byte> compare(VectorOperators.Comparison op,\n-                                  Vector<Byte> v,\n-                                  VectorMask<Byte> m) {\n-        return compare(op, v).and(m);\n-    }\n-\n@@ -1853,1 +2010,1 @@\n-        return compare(op, e).and(m);\n+        return compare(op, broadcast(e), m);\n@@ -2104,3 +2261,3 @@\n-            getClass(), shuffletype, byte.class, length(),\n-            this, shuffle,\n-            (v1, s_) -> v1.uOp((i, a) -> {\n+            getClass(), shuffletype, null, byte.class, length(),\n+            this, shuffle, null,\n+            (v1, s_, m_) -> v1.uOp((i, a) -> {\n@@ -2123,1 +2280,1 @@\n-    <S extends VectorShuffle<Byte>>\n+    <S extends VectorShuffle<Byte>, M extends VectorMask<Byte>>\n@@ -2125,0 +2282,1 @@\n+                                           Class<M> masktype,\n@@ -2126,9 +2284,3 @@\n-                                           VectorMask<Byte> m) {\n-        ByteVector unmasked =\n-            VectorSupport.rearrangeOp(\n-                getClass(), shuffletype, byte.class, length(),\n-                this, shuffle,\n-                (v1, s_) -> v1.uOp((i, a) -> {\n-                    int ei = s_.laneSource(i);\n-                    return ei < 0 ? 0 : v1.lane(ei);\n-                }));\n+                                           M m) {\n+\n+        m.check(masktype, this);\n@@ -2140,1 +2292,7 @@\n-        return broadcast((byte)0).blend(unmasked, m);\n+        return VectorSupport.rearrangeOp(\n+                   getClass(), shuffletype, masktype, byte.class, length(),\n+                   this, shuffle, m,\n+                   (v1, s_, m_) -> v1.uOp((i, a) -> {\n+                        int ei = s_.laneSource(i);\n+                        return ei < 0  || !m_.laneIsSet(i) ? 0 : v1.lane(ei);\n+                   }));\n@@ -2163,3 +2321,3 @@\n-                getClass(), shuffletype, byte.class, length(),\n-                this, ws,\n-                (v0, s_) -> v0.uOp((i, a) -> {\n+                getClass(), shuffletype, null, byte.class, length(),\n+                this, ws, null,\n+                (v0, s_, m_) -> v0.uOp((i, a) -> {\n@@ -2171,3 +2329,3 @@\n-                getClass(), shuffletype, byte.class, length(),\n-                v, ws,\n-                (v1, s_) -> v1.uOp((i, a) -> {\n+                getClass(), shuffletype, null, byte.class, length(),\n+                v, ws, null,\n+                (v1, s_, m_) -> v1.uOp((i, a) -> {\n@@ -2436,0 +2594,1 @@\n+                               Class<? extends VectorMask<Byte>> maskClass,\n@@ -2437,2 +2596,10 @@\n-        ByteVector v = reduceIdentityVector(op).blend(this, m);\n-        return v.reduceLanesTemplate(op);\n+        m.check(maskClass, this);\n+        if (op == FIRST_NONZERO) {\n+            ByteVector v = reduceIdentityVector(op).blend(this, m);\n+            return v.reduceLanesTemplate(op);\n+        }\n+        int opc = opCode(op);\n+        return fromBits(VectorSupport.reductionCoerced(\n+            opc, getClass(), maskClass, byte.class, length(),\n+            this, m,\n+            REDUCE_IMPL.find(op, opc, ByteVector::reductionOperations)));\n@@ -2453,20 +2620,3 @@\n-            opc, getClass(), byte.class, length(),\n-            this,\n-            REDUCE_IMPL.find(op, opc, (opc_) -> {\n-              switch (opc_) {\n-              case VECTOR_OP_ADD: return v ->\n-                      toBits(v.rOp((byte)0, (i, a, b) -> (byte)(a + b)));\n-              case VECTOR_OP_MUL: return v ->\n-                      toBits(v.rOp((byte)1, (i, a, b) -> (byte)(a * b)));\n-              case VECTOR_OP_MIN: return v ->\n-                      toBits(v.rOp(MAX_OR_INF, (i, a, b) -> (byte) Math.min(a, b)));\n-              case VECTOR_OP_MAX: return v ->\n-                      toBits(v.rOp(MIN_OR_INF, (i, a, b) -> (byte) Math.max(a, b)));\n-              case VECTOR_OP_AND: return v ->\n-                      toBits(v.rOp((byte)-1, (i, a, b) -> (byte)(a & b)));\n-              case VECTOR_OP_OR: return v ->\n-                      toBits(v.rOp((byte)0, (i, a, b) -> (byte)(a | b)));\n-              case VECTOR_OP_XOR: return v ->\n-                      toBits(v.rOp((byte)0, (i, a, b) -> (byte)(a ^ b)));\n-              default: return null;\n-              }})));\n+            opc, getClass(), null, byte.class, length(),\n+            this, null,\n+            REDUCE_IMPL.find(op, opc, ByteVector::reductionOperations)));\n@@ -2474,0 +2624,1 @@\n+\n@@ -2475,2 +2626,22 @@\n-    ImplCache<Associative,Function<ByteVector,Long>> REDUCE_IMPL\n-        = new ImplCache<>(Associative.class, ByteVector.class);\n+    ImplCache<Associative, ReductionOperation<ByteVector, VectorMask<Byte>>>\n+        REDUCE_IMPL = new ImplCache<>(Associative.class, ByteVector.class);\n+\n+    private static ReductionOperation<ByteVector, VectorMask<Byte>> reductionOperations(int opc_) {\n+        switch (opc_) {\n+            case VECTOR_OP_ADD: return (v, m) ->\n+                    toBits(v.rOp((byte)0, m, (i, a, b) -> (byte)(a + b)));\n+            case VECTOR_OP_MUL: return (v, m) ->\n+                    toBits(v.rOp((byte)1, m, (i, a, b) -> (byte)(a * b)));\n+            case VECTOR_OP_MIN: return (v, m) ->\n+                    toBits(v.rOp(MAX_OR_INF, m, (i, a, b) -> (byte) Math.min(a, b)));\n+            case VECTOR_OP_MAX: return (v, m) ->\n+                    toBits(v.rOp(MIN_OR_INF, m, (i, a, b) -> (byte) Math.max(a, b)));\n+            case VECTOR_OP_AND: return (v, m) ->\n+                    toBits(v.rOp((byte)-1, m, (i, a, b) -> (byte)(a & b)));\n+            case VECTOR_OP_OR: return (v, m) ->\n+                    toBits(v.rOp((byte)0, m, (i, a, b) -> (byte)(a | b)));\n+            case VECTOR_OP_XOR: return (v, m) ->\n+                    toBits(v.rOp((byte)0, m, (i, a, b) -> (byte)(a ^ b)));\n+            default: return null;\n+        }\n+    }\n@@ -2702,3 +2873,1 @@\n-            ByteVector zero = vsp.zero();\n-            ByteVector v = zero.fromByteArray0(a, offset);\n-            return zero.blend(v.maybeSwap(bo), m);\n+            return vsp.dummyVector().fromByteArray0(a, offset, m).maybeSwap(bo);\n@@ -2766,2 +2935,1 @@\n-            ByteVector zero = vsp.zero();\n-            return zero.blend(zero.fromArray0(a, offset), m);\n+            return vsp.dummyVector().fromArray0(a, offset, m);\n@@ -2924,1 +3092,1 @@\n-            return zero.blend(zero.fromBooleanArray0(a, offset), m);\n+            return vsp.dummyVector().fromBooleanArray0(a, offset, m);\n@@ -3102,3 +3270,1 @@\n-            ByteVector zero = vsp.zero();\n-            ByteVector v = zero.fromByteBuffer0(bb, offset);\n-            return zero.blend(v.maybeSwap(bo), m);\n+            return vsp.dummyVector().fromByteBuffer0(bb, offset, m).maybeSwap(bo);\n@@ -3176,1 +3342,0 @@\n-            \/\/ FIXME: optimize\n@@ -3179,1 +3344,1 @@\n-            stOp(a, offset, m, (arr, off, i, v) -> arr[off+i] = v);\n+            intoArray0(a, offset, m);\n@@ -3332,1 +3497,0 @@\n-            \/\/ FIXME: optimize\n@@ -3335,1 +3499,1 @@\n-            stOp(a, offset, m, (arr, off, i, e) -> arr[off+i] = (e & 1) != 0);\n+            intoBooleanArray0(a, offset, m);\n@@ -3454,1 +3618,0 @@\n-            \/\/ FIXME: optimize\n@@ -3457,3 +3620,1 @@\n-            ByteBuffer wb = wrapper(a, bo);\n-            this.stOp(wb, offset, m,\n-                    (wb_, o, i, e) -> wb_.put(o + i * 1, e));\n+            maybeSwap(bo).intoByteArray0(a, offset, m);\n@@ -3471,1 +3632,1 @@\n-        if (bb.isReadOnly()) {\n+        if (ScopedMemoryAccess.isReadOnly(bb)) {\n@@ -3490,1 +3651,0 @@\n-            \/\/ FIXME: optimize\n@@ -3496,3 +3656,1 @@\n-            ByteBuffer wb = wrapper(bb, bo);\n-            this.stOp(wb, offset, m,\n-                    (wb_, o, i, e) -> wb_.put(o + i * 1, e));\n+            maybeSwap(bo).intoByteBuffer0(bb, offset, m);\n@@ -3536,0 +3694,18 @@\n+    \/*package-private*\/\n+    abstract\n+    ByteVector fromArray0(byte[] a, int offset, VectorMask<Byte> m);\n+    @ForceInline\n+    final\n+    <M extends VectorMask<Byte>>\n+    ByteVector fromArray0Template(Class<M> maskClass, byte[] a, int offset, M m) {\n+        m.check(species());\n+        ByteSpecies vsp = vspecies();\n+        return VectorSupport.loadMasked(\n+            vsp.vectorType(), maskClass, vsp.elementType(), vsp.laneCount(),\n+            a, arrayAddress(a, offset), m,\n+            a, offset, vsp,\n+            (arr, off, s, vm) -> s.ldOp(arr, off, vm,\n+                                        (arr_, off_, i) -> arr_[off_ + i]));\n+    }\n+\n+\n@@ -3552,0 +3728,17 @@\n+    \/*package-private*\/\n+    abstract\n+    ByteVector fromBooleanArray0(boolean[] a, int offset, VectorMask<Byte> m);\n+    @ForceInline\n+    final\n+    <M extends VectorMask<Byte>>\n+    ByteVector fromBooleanArray0Template(Class<M> maskClass, boolean[] a, int offset, M m) {\n+        m.check(species());\n+        ByteSpecies vsp = vspecies();\n+        return VectorSupport.loadMasked(\n+            vsp.vectorType(), maskClass, vsp.elementType(), vsp.laneCount(),\n+            a, booleanArrayAddress(a, offset), m,\n+            a, offset, vsp,\n+            (arr, off, s, vm) -> s.ldOp(arr, off, vm,\n+                                        (arr_, off_, i) -> (byte) (arr_[off_ + i] ? 1 : 0)));\n+    }\n+\n@@ -3570,0 +3763,19 @@\n+    abstract\n+    ByteVector fromByteArray0(byte[] a, int offset, VectorMask<Byte> m);\n+    @ForceInline\n+    final\n+    <M extends VectorMask<Byte>>\n+    ByteVector fromByteArray0Template(Class<M> maskClass, byte[] a, int offset, M m) {\n+        ByteSpecies vsp = vspecies();\n+        m.check(vsp);\n+        return VectorSupport.loadMasked(\n+            vsp.vectorType(), maskClass, vsp.elementType(), vsp.laneCount(),\n+            a, byteArrayAddress(a, offset), m,\n+            a, offset, vsp,\n+            (arr, off, s, vm) -> {\n+                ByteBuffer wb = wrapper(arr, NATIVE_ENDIAN);\n+                return s.ldOp(wb, off, vm,\n+                        (wb_, o, i) -> wb_.get(o + i * 1));\n+            });\n+    }\n+\n@@ -3586,0 +3798,18 @@\n+    abstract\n+    ByteVector fromByteBuffer0(ByteBuffer bb, int offset, VectorMask<Byte> m);\n+    @ForceInline\n+    final\n+    <M extends VectorMask<Byte>>\n+    ByteVector fromByteBuffer0Template(Class<M> maskClass, ByteBuffer bb, int offset, M m) {\n+        ByteSpecies vsp = vspecies();\n+        m.check(vsp);\n+        return ScopedMemoryAccess.loadFromByteBufferMasked(\n+                vsp.vectorType(), maskClass, vsp.elementType(), vsp.laneCount(),\n+                bb, offset, m, vsp,\n+                (buf, off, s, vm) -> {\n+                    ByteBuffer wb = wrapper(buf, NATIVE_ENDIAN);\n+                    return s.ldOp(wb, off, vm,\n+                            (wb_, o, i) -> wb_.get(o + i * 1));\n+                });\n+    }\n+\n@@ -3605,0 +3835,36 @@\n+    abstract\n+    void intoArray0(byte[] a, int offset, VectorMask<Byte> m);\n+    @ForceInline\n+    final\n+    <M extends VectorMask<Byte>>\n+    void intoArray0Template(Class<M> maskClass, byte[] a, int offset, M m) {\n+        m.check(species());\n+        ByteSpecies vsp = vspecies();\n+        VectorSupport.storeMasked(\n+            vsp.vectorType(), maskClass, vsp.elementType(), vsp.laneCount(),\n+            a, arrayAddress(a, offset),\n+            this, m, a, offset,\n+            (arr, off, v, vm)\n+            -> v.stOp(arr, off, vm,\n+                      (arr_, off_, i, e) -> arr_[off_ + i] = e));\n+    }\n+\n+\n+    abstract\n+    void intoBooleanArray0(boolean[] a, int offset, VectorMask<Byte> m);\n+    @ForceInline\n+    final\n+    <M extends VectorMask<Byte>>\n+    void intoBooleanArray0Template(Class<M> maskClass, boolean[] a, int offset, M m) {\n+        m.check(species());\n+        ByteSpecies vsp = vspecies();\n+        ByteVector normalized = this.and((byte) 1);\n+        VectorSupport.storeMasked(\n+            vsp.vectorType(), maskClass, vsp.elementType(), vsp.laneCount(),\n+            a, booleanArrayAddress(a, offset),\n+            normalized, m, a, offset,\n+            (arr, off, v, vm)\n+            -> v.stOp(arr, off, vm,\n+                      (arr_, off_, i, e) -> arr_[off_ + i] = (e & 1) != 0));\n+    }\n+\n@@ -3622,0 +3888,19 @@\n+    abstract\n+    void intoByteArray0(byte[] a, int offset, VectorMask<Byte> m);\n+    @ForceInline\n+    final\n+    <M extends VectorMask<Byte>>\n+    void intoByteArray0Template(Class<M> maskClass, byte[] a, int offset, M m) {\n+        ByteSpecies vsp = vspecies();\n+        m.check(vsp);\n+        VectorSupport.storeMasked(\n+            vsp.vectorType(), maskClass, vsp.elementType(), vsp.laneCount(),\n+            a, byteArrayAddress(a, offset),\n+            this, m, a, offset,\n+            (arr, off, v, vm) -> {\n+                ByteBuffer wb = wrapper(arr, NATIVE_ENDIAN);\n+                v.stOp(wb, off, vm,\n+                        (tb_, o, i, e) -> tb_.put(o + i * 1, e));\n+            });\n+    }\n+\n@@ -3636,0 +3921,19 @@\n+    abstract\n+    void intoByteBuffer0(ByteBuffer bb, int offset, VectorMask<Byte> m);\n+    @ForceInline\n+    final\n+    <M extends VectorMask<Byte>>\n+    void intoByteBuffer0Template(Class<M> maskClass, ByteBuffer bb, int offset, M m) {\n+        ByteSpecies vsp = vspecies();\n+        m.check(vsp);\n+        ScopedMemoryAccess.storeIntoByteBufferMasked(\n+                vsp.vectorType(), maskClass, vsp.elementType(), vsp.laneCount(),\n+                this, m, bb, offset,\n+                (buf, off, v, vm) -> {\n+                    ByteBuffer wb = wrapper(buf, NATIVE_ENDIAN);\n+                    v.stOp(wb, off, vm,\n+                            (wb_, o, i, e) -> wb_.put(o + i * 1, e));\n+                });\n+    }\n+\n+\n@@ -3962,1 +4266,1 @@\n-                                      AbstractMask<Byte> m,\n+                                      VectorMask<Byte> m,\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/ByteVector.java","additions":495,"deletions":191,"binary":false,"changes":686,"status":"modified"},{"patch":"@@ -239,2 +239,2 @@\n-    double rOp(double v, FBinOp f) {\n-        return super.rOpTemplate(v, f);  \/\/ specialize\n+    double rOp(double v, VectorMask<Double> m, FBinOp f) {\n+        return super.rOpTemplate(v, m, f);  \/\/ specialize\n@@ -276,0 +276,6 @@\n+    @Override\n+    @ForceInline\n+    public Double128Vector lanewise(Unary op, VectorMask<Double> m) {\n+        return (Double128Vector) super.lanewiseTemplate(op, Double128Mask.class, (Double128Mask) m);  \/\/ specialize\n+    }\n+\n@@ -282,0 +288,6 @@\n+    @Override\n+    @ForceInline\n+    public Double128Vector lanewise(Binary op, Vector<Double> v, VectorMask<Double> m) {\n+        return (Double128Vector) super.lanewiseTemplate(op, Double128Mask.class, v, (Double128Mask) m);  \/\/ specialize\n+    }\n+\n@@ -288,1 +300,1 @@\n-    lanewise(VectorOperators.Ternary op, Vector<Double> v1, Vector<Double> v2) {\n+    lanewise(Ternary op, Vector<Double> v1, Vector<Double> v2) {\n@@ -292,0 +304,8 @@\n+    @Override\n+    @ForceInline\n+    public final\n+    Double128Vector\n+    lanewise(Ternary op, Vector<Double> v1, Vector<Double> v2, VectorMask<Double> m) {\n+        return (Double128Vector) super.lanewiseTemplate(op, Double128Mask.class, v1, v2, (Double128Mask) m);  \/\/ specialize\n+    }\n+\n@@ -311,1 +331,1 @@\n-        return super.reduceLanesTemplate(op, m);  \/\/ specialized\n+        return super.reduceLanesTemplate(op, Double128Mask.class, (Double128Mask) m);  \/\/ specialized\n@@ -324,1 +344,1 @@\n-        return (long) super.reduceLanesTemplate(op, m);  \/\/ specialized\n+        return (long) super.reduceLanesTemplate(op, Double128Mask.class, (Double128Mask) m);  \/\/ specialized\n@@ -360,0 +380,7 @@\n+    @Override\n+    @ForceInline\n+    public final Double128Mask compare(Comparison op, Vector<Double> v, VectorMask<Double> m) {\n+        return super.compareTemplate(Double128Mask.class, op, v, (Double128Mask) m);\n+    }\n+\n+\n@@ -416,0 +443,1 @@\n+                                    Double128Mask.class,\n@@ -583,10 +611,6 @@\n-            if (VSIZE == species.vectorBitSize()) {\n-                Class<?> dtype = species.elementType();\n-                Class<?> dmtype = species.maskType();\n-                return VectorSupport.convert(VectorSupport.VECTOR_OP_REINTERPRET,\n-                    this.getClass(), ETYPE, VLENGTH,\n-                    dmtype, dtype, VLENGTH,\n-                    this, species,\n-                    Double128Mask::defaultMaskCast);\n-            }\n-            return this.defaultMaskCast(species);\n+\n+            return VectorSupport.convert(VectorSupport.VECTOR_OP_CAST,\n+                this.getClass(), ETYPE, VLENGTH,\n+                species.maskType(), species.elementType(), VLENGTH,\n+                this, species,\n+                (m, s) -> s.maskFactory(m.toArray()).check(s));\n@@ -618,3 +642,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_AND, Double128Mask.class, long.class, VLENGTH,\n-                                             this, m,\n-                                             (m1, m2) -> m1.bOp(m2, (i, a, b) -> a & b));\n+            return VectorSupport.binaryOp(VECTOR_OP_AND, Double128Mask.class, null, long.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a & b));\n@@ -628,3 +652,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_OR, Double128Mask.class, long.class, VLENGTH,\n-                                             this, m,\n-                                             (m1, m2) -> m1.bOp(m2, (i, a, b) -> a | b));\n+            return VectorSupport.binaryOp(VECTOR_OP_OR, Double128Mask.class, null, long.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a | b));\n@@ -638,3 +662,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_XOR, Double128Mask.class, long.class, VLENGTH,\n-                                          this, m,\n-                                          (m1, m2) -> m1.bOp(m2, (i, a, b) -> a ^ b));\n+            return VectorSupport.binaryOp(VECTOR_OP_XOR, Double128Mask.class, null, long.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a ^ b));\n@@ -648,2 +672,2 @@\n-            return VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_TRUECOUNT, Double128Mask.class, long.class, VLENGTH, this,\n-                                                      (m) -> trueCountHelper(((Double128Mask)m).getBits()));\n+            return (int) VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_TRUECOUNT, Double128Mask.class, long.class, VLENGTH, this,\n+                                                      (m) -> trueCountHelper(m.getBits()));\n@@ -655,2 +679,2 @@\n-            return VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_FIRSTTRUE, Double128Mask.class, long.class, VLENGTH, this,\n-                                                      (m) -> firstTrueHelper(((Double128Mask)m).getBits()));\n+            return (int) VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_FIRSTTRUE, Double128Mask.class, long.class, VLENGTH, this,\n+                                                      (m) -> firstTrueHelper(m.getBits()));\n@@ -662,2 +686,12 @@\n-            return VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_LASTTRUE, Double128Mask.class, long.class, VLENGTH, this,\n-                                                      (m) -> lastTrueHelper(((Double128Mask)m).getBits()));\n+            return (int) VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_LASTTRUE, Double128Mask.class, long.class, VLENGTH, this,\n+                                                      (m) -> lastTrueHelper(m.getBits()));\n+        }\n+\n+        @Override\n+        @ForceInline\n+        public long toLong() {\n+            if (length() > Long.SIZE) {\n+                throw new UnsupportedOperationException(\"too many lanes for one long\");\n+            }\n+            return VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_TOLONG, Double128Mask.class, long.class, VLENGTH, this,\n+                                                      (m) -> toLongHelper(m.getBits()));\n@@ -774,0 +808,14 @@\n+    @ForceInline\n+    @Override\n+    final\n+    DoubleVector fromArray0(double[] a, int offset, VectorMask<Double> m) {\n+        return super.fromArray0Template(Double128Mask.class, a, offset, (Double128Mask) m);  \/\/ specialize\n+    }\n+\n+    @ForceInline\n+    @Override\n+    final\n+    DoubleVector fromArray0(double[] a, int offset, int[] indexMap, int mapOffset, VectorMask<Double> m) {\n+        return super.fromArray0Template(Double128Mask.class, a, offset, indexMap, mapOffset, (Double128Mask) m);\n+    }\n+\n@@ -783,0 +831,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    DoubleVector fromByteArray0(byte[] a, int offset, VectorMask<Double> m) {\n+        return super.fromByteArray0Template(Double128Mask.class, a, offset, (Double128Mask) m);  \/\/ specialize\n+    }\n+\n@@ -790,0 +845,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    DoubleVector fromByteBuffer0(ByteBuffer bb, int offset, VectorMask<Double> m) {\n+        return super.fromByteBuffer0Template(Double128Mask.class, bb, offset, (Double128Mask) m);  \/\/ specialize\n+    }\n+\n@@ -797,0 +859,15 @@\n+    @ForceInline\n+    @Override\n+    final\n+    void intoArray0(double[] a, int offset, VectorMask<Double> m) {\n+        super.intoArray0Template(Double128Mask.class, a, offset, (Double128Mask) m);\n+    }\n+\n+    @ForceInline\n+    @Override\n+    final\n+    void intoArray0(double[] a, int offset, int[] indexMap, int mapOffset, VectorMask<Double> m) {\n+        super.intoArray0Template(Double128Mask.class, a, offset, indexMap, mapOffset, (Double128Mask) m);\n+    }\n+\n+\n@@ -804,0 +881,15 @@\n+    @ForceInline\n+    @Override\n+    final\n+    void intoByteArray0(byte[] a, int offset, VectorMask<Double> m) {\n+        super.intoByteArray0Template(Double128Mask.class, a, offset, (Double128Mask) m);  \/\/ specialize\n+    }\n+\n+    @ForceInline\n+    @Override\n+    final\n+    void intoByteBuffer0(ByteBuffer bb, int offset, VectorMask<Double> m) {\n+        super.intoByteBuffer0Template(Double128Mask.class, bb, offset, (Double128Mask) m);\n+    }\n+\n+\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/Double128Vector.java","additions":122,"deletions":30,"binary":false,"changes":152,"status":"modified"},{"patch":"@@ -239,2 +239,2 @@\n-    double rOp(double v, FBinOp f) {\n-        return super.rOpTemplate(v, f);  \/\/ specialize\n+    double rOp(double v, VectorMask<Double> m, FBinOp f) {\n+        return super.rOpTemplate(v, m, f);  \/\/ specialize\n@@ -276,0 +276,6 @@\n+    @Override\n+    @ForceInline\n+    public Double256Vector lanewise(Unary op, VectorMask<Double> m) {\n+        return (Double256Vector) super.lanewiseTemplate(op, Double256Mask.class, (Double256Mask) m);  \/\/ specialize\n+    }\n+\n@@ -282,0 +288,6 @@\n+    @Override\n+    @ForceInline\n+    public Double256Vector lanewise(Binary op, Vector<Double> v, VectorMask<Double> m) {\n+        return (Double256Vector) super.lanewiseTemplate(op, Double256Mask.class, v, (Double256Mask) m);  \/\/ specialize\n+    }\n+\n@@ -288,1 +300,1 @@\n-    lanewise(VectorOperators.Ternary op, Vector<Double> v1, Vector<Double> v2) {\n+    lanewise(Ternary op, Vector<Double> v1, Vector<Double> v2) {\n@@ -292,0 +304,8 @@\n+    @Override\n+    @ForceInline\n+    public final\n+    Double256Vector\n+    lanewise(Ternary op, Vector<Double> v1, Vector<Double> v2, VectorMask<Double> m) {\n+        return (Double256Vector) super.lanewiseTemplate(op, Double256Mask.class, v1, v2, (Double256Mask) m);  \/\/ specialize\n+    }\n+\n@@ -311,1 +331,1 @@\n-        return super.reduceLanesTemplate(op, m);  \/\/ specialized\n+        return super.reduceLanesTemplate(op, Double256Mask.class, (Double256Mask) m);  \/\/ specialized\n@@ -324,1 +344,1 @@\n-        return (long) super.reduceLanesTemplate(op, m);  \/\/ specialized\n+        return (long) super.reduceLanesTemplate(op, Double256Mask.class, (Double256Mask) m);  \/\/ specialized\n@@ -360,0 +380,7 @@\n+    @Override\n+    @ForceInline\n+    public final Double256Mask compare(Comparison op, Vector<Double> v, VectorMask<Double> m) {\n+        return super.compareTemplate(Double256Mask.class, op, v, (Double256Mask) m);\n+    }\n+\n+\n@@ -416,0 +443,1 @@\n+                                    Double256Mask.class,\n@@ -587,10 +615,6 @@\n-            if (VSIZE == species.vectorBitSize()) {\n-                Class<?> dtype = species.elementType();\n-                Class<?> dmtype = species.maskType();\n-                return VectorSupport.convert(VectorSupport.VECTOR_OP_REINTERPRET,\n-                    this.getClass(), ETYPE, VLENGTH,\n-                    dmtype, dtype, VLENGTH,\n-                    this, species,\n-                    Double256Mask::defaultMaskCast);\n-            }\n-            return this.defaultMaskCast(species);\n+\n+            return VectorSupport.convert(VectorSupport.VECTOR_OP_CAST,\n+                this.getClass(), ETYPE, VLENGTH,\n+                species.maskType(), species.elementType(), VLENGTH,\n+                this, species,\n+                (m, s) -> s.maskFactory(m.toArray()).check(s));\n@@ -622,3 +646,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_AND, Double256Mask.class, long.class, VLENGTH,\n-                                             this, m,\n-                                             (m1, m2) -> m1.bOp(m2, (i, a, b) -> a & b));\n+            return VectorSupport.binaryOp(VECTOR_OP_AND, Double256Mask.class, null, long.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a & b));\n@@ -632,3 +656,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_OR, Double256Mask.class, long.class, VLENGTH,\n-                                             this, m,\n-                                             (m1, m2) -> m1.bOp(m2, (i, a, b) -> a | b));\n+            return VectorSupport.binaryOp(VECTOR_OP_OR, Double256Mask.class, null, long.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a | b));\n@@ -642,3 +666,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_XOR, Double256Mask.class, long.class, VLENGTH,\n-                                          this, m,\n-                                          (m1, m2) -> m1.bOp(m2, (i, a, b) -> a ^ b));\n+            return VectorSupport.binaryOp(VECTOR_OP_XOR, Double256Mask.class, null, long.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a ^ b));\n@@ -652,2 +676,2 @@\n-            return VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_TRUECOUNT, Double256Mask.class, long.class, VLENGTH, this,\n-                                                      (m) -> trueCountHelper(((Double256Mask)m).getBits()));\n+            return (int) VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_TRUECOUNT, Double256Mask.class, long.class, VLENGTH, this,\n+                                                      (m) -> trueCountHelper(m.getBits()));\n@@ -659,2 +683,2 @@\n-            return VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_FIRSTTRUE, Double256Mask.class, long.class, VLENGTH, this,\n-                                                      (m) -> firstTrueHelper(((Double256Mask)m).getBits()));\n+            return (int) VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_FIRSTTRUE, Double256Mask.class, long.class, VLENGTH, this,\n+                                                      (m) -> firstTrueHelper(m.getBits()));\n@@ -666,2 +690,12 @@\n-            return VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_LASTTRUE, Double256Mask.class, long.class, VLENGTH, this,\n-                                                      (m) -> lastTrueHelper(((Double256Mask)m).getBits()));\n+            return (int) VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_LASTTRUE, Double256Mask.class, long.class, VLENGTH, this,\n+                                                      (m) -> lastTrueHelper(m.getBits()));\n+        }\n+\n+        @Override\n+        @ForceInline\n+        public long toLong() {\n+            if (length() > Long.SIZE) {\n+                throw new UnsupportedOperationException(\"too many lanes for one long\");\n+            }\n+            return VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_TOLONG, Double256Mask.class, long.class, VLENGTH, this,\n+                                                      (m) -> toLongHelper(m.getBits()));\n@@ -778,0 +812,14 @@\n+    @ForceInline\n+    @Override\n+    final\n+    DoubleVector fromArray0(double[] a, int offset, VectorMask<Double> m) {\n+        return super.fromArray0Template(Double256Mask.class, a, offset, (Double256Mask) m);  \/\/ specialize\n+    }\n+\n+    @ForceInline\n+    @Override\n+    final\n+    DoubleVector fromArray0(double[] a, int offset, int[] indexMap, int mapOffset, VectorMask<Double> m) {\n+        return super.fromArray0Template(Double256Mask.class, a, offset, indexMap, mapOffset, (Double256Mask) m);\n+    }\n+\n@@ -787,0 +835,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    DoubleVector fromByteArray0(byte[] a, int offset, VectorMask<Double> m) {\n+        return super.fromByteArray0Template(Double256Mask.class, a, offset, (Double256Mask) m);  \/\/ specialize\n+    }\n+\n@@ -794,0 +849,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    DoubleVector fromByteBuffer0(ByteBuffer bb, int offset, VectorMask<Double> m) {\n+        return super.fromByteBuffer0Template(Double256Mask.class, bb, offset, (Double256Mask) m);  \/\/ specialize\n+    }\n+\n@@ -801,0 +863,15 @@\n+    @ForceInline\n+    @Override\n+    final\n+    void intoArray0(double[] a, int offset, VectorMask<Double> m) {\n+        super.intoArray0Template(Double256Mask.class, a, offset, (Double256Mask) m);\n+    }\n+\n+    @ForceInline\n+    @Override\n+    final\n+    void intoArray0(double[] a, int offset, int[] indexMap, int mapOffset, VectorMask<Double> m) {\n+        super.intoArray0Template(Double256Mask.class, a, offset, indexMap, mapOffset, (Double256Mask) m);\n+    }\n+\n+\n@@ -808,0 +885,15 @@\n+    @ForceInline\n+    @Override\n+    final\n+    void intoByteArray0(byte[] a, int offset, VectorMask<Double> m) {\n+        super.intoByteArray0Template(Double256Mask.class, a, offset, (Double256Mask) m);  \/\/ specialize\n+    }\n+\n+    @ForceInline\n+    @Override\n+    final\n+    void intoByteBuffer0(ByteBuffer bb, int offset, VectorMask<Double> m) {\n+        super.intoByteBuffer0Template(Double256Mask.class, bb, offset, (Double256Mask) m);\n+    }\n+\n+\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/Double256Vector.java","additions":122,"deletions":30,"binary":false,"changes":152,"status":"modified"},{"patch":"@@ -239,2 +239,2 @@\n-    double rOp(double v, FBinOp f) {\n-        return super.rOpTemplate(v, f);  \/\/ specialize\n+    double rOp(double v, VectorMask<Double> m, FBinOp f) {\n+        return super.rOpTemplate(v, m, f);  \/\/ specialize\n@@ -276,0 +276,6 @@\n+    @Override\n+    @ForceInline\n+    public Double512Vector lanewise(Unary op, VectorMask<Double> m) {\n+        return (Double512Vector) super.lanewiseTemplate(op, Double512Mask.class, (Double512Mask) m);  \/\/ specialize\n+    }\n+\n@@ -282,0 +288,6 @@\n+    @Override\n+    @ForceInline\n+    public Double512Vector lanewise(Binary op, Vector<Double> v, VectorMask<Double> m) {\n+        return (Double512Vector) super.lanewiseTemplate(op, Double512Mask.class, v, (Double512Mask) m);  \/\/ specialize\n+    }\n+\n@@ -288,1 +300,1 @@\n-    lanewise(VectorOperators.Ternary op, Vector<Double> v1, Vector<Double> v2) {\n+    lanewise(Ternary op, Vector<Double> v1, Vector<Double> v2) {\n@@ -292,0 +304,8 @@\n+    @Override\n+    @ForceInline\n+    public final\n+    Double512Vector\n+    lanewise(Ternary op, Vector<Double> v1, Vector<Double> v2, VectorMask<Double> m) {\n+        return (Double512Vector) super.lanewiseTemplate(op, Double512Mask.class, v1, v2, (Double512Mask) m);  \/\/ specialize\n+    }\n+\n@@ -311,1 +331,1 @@\n-        return super.reduceLanesTemplate(op, m);  \/\/ specialized\n+        return super.reduceLanesTemplate(op, Double512Mask.class, (Double512Mask) m);  \/\/ specialized\n@@ -324,1 +344,1 @@\n-        return (long) super.reduceLanesTemplate(op, m);  \/\/ specialized\n+        return (long) super.reduceLanesTemplate(op, Double512Mask.class, (Double512Mask) m);  \/\/ specialized\n@@ -360,0 +380,7 @@\n+    @Override\n+    @ForceInline\n+    public final Double512Mask compare(Comparison op, Vector<Double> v, VectorMask<Double> m) {\n+        return super.compareTemplate(Double512Mask.class, op, v, (Double512Mask) m);\n+    }\n+\n+\n@@ -416,0 +443,1 @@\n+                                    Double512Mask.class,\n@@ -595,10 +623,6 @@\n-            if (VSIZE == species.vectorBitSize()) {\n-                Class<?> dtype = species.elementType();\n-                Class<?> dmtype = species.maskType();\n-                return VectorSupport.convert(VectorSupport.VECTOR_OP_REINTERPRET,\n-                    this.getClass(), ETYPE, VLENGTH,\n-                    dmtype, dtype, VLENGTH,\n-                    this, species,\n-                    Double512Mask::defaultMaskCast);\n-            }\n-            return this.defaultMaskCast(species);\n+\n+            return VectorSupport.convert(VectorSupport.VECTOR_OP_CAST,\n+                this.getClass(), ETYPE, VLENGTH,\n+                species.maskType(), species.elementType(), VLENGTH,\n+                this, species,\n+                (m, s) -> s.maskFactory(m.toArray()).check(s));\n@@ -630,3 +654,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_AND, Double512Mask.class, long.class, VLENGTH,\n-                                             this, m,\n-                                             (m1, m2) -> m1.bOp(m2, (i, a, b) -> a & b));\n+            return VectorSupport.binaryOp(VECTOR_OP_AND, Double512Mask.class, null, long.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a & b));\n@@ -640,3 +664,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_OR, Double512Mask.class, long.class, VLENGTH,\n-                                             this, m,\n-                                             (m1, m2) -> m1.bOp(m2, (i, a, b) -> a | b));\n+            return VectorSupport.binaryOp(VECTOR_OP_OR, Double512Mask.class, null, long.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a | b));\n@@ -650,3 +674,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_XOR, Double512Mask.class, long.class, VLENGTH,\n-                                          this, m,\n-                                          (m1, m2) -> m1.bOp(m2, (i, a, b) -> a ^ b));\n+            return VectorSupport.binaryOp(VECTOR_OP_XOR, Double512Mask.class, null, long.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a ^ b));\n@@ -660,2 +684,2 @@\n-            return VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_TRUECOUNT, Double512Mask.class, long.class, VLENGTH, this,\n-                                                      (m) -> trueCountHelper(((Double512Mask)m).getBits()));\n+            return (int) VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_TRUECOUNT, Double512Mask.class, long.class, VLENGTH, this,\n+                                                      (m) -> trueCountHelper(m.getBits()));\n@@ -667,2 +691,2 @@\n-            return VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_FIRSTTRUE, Double512Mask.class, long.class, VLENGTH, this,\n-                                                      (m) -> firstTrueHelper(((Double512Mask)m).getBits()));\n+            return (int) VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_FIRSTTRUE, Double512Mask.class, long.class, VLENGTH, this,\n+                                                      (m) -> firstTrueHelper(m.getBits()));\n@@ -674,2 +698,12 @@\n-            return VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_LASTTRUE, Double512Mask.class, long.class, VLENGTH, this,\n-                                                      (m) -> lastTrueHelper(((Double512Mask)m).getBits()));\n+            return (int) VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_LASTTRUE, Double512Mask.class, long.class, VLENGTH, this,\n+                                                      (m) -> lastTrueHelper(m.getBits()));\n+        }\n+\n+        @Override\n+        @ForceInline\n+        public long toLong() {\n+            if (length() > Long.SIZE) {\n+                throw new UnsupportedOperationException(\"too many lanes for one long\");\n+            }\n+            return VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_TOLONG, Double512Mask.class, long.class, VLENGTH, this,\n+                                                      (m) -> toLongHelper(m.getBits()));\n@@ -786,0 +820,14 @@\n+    @ForceInline\n+    @Override\n+    final\n+    DoubleVector fromArray0(double[] a, int offset, VectorMask<Double> m) {\n+        return super.fromArray0Template(Double512Mask.class, a, offset, (Double512Mask) m);  \/\/ specialize\n+    }\n+\n+    @ForceInline\n+    @Override\n+    final\n+    DoubleVector fromArray0(double[] a, int offset, int[] indexMap, int mapOffset, VectorMask<Double> m) {\n+        return super.fromArray0Template(Double512Mask.class, a, offset, indexMap, mapOffset, (Double512Mask) m);\n+    }\n+\n@@ -795,0 +843,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    DoubleVector fromByteArray0(byte[] a, int offset, VectorMask<Double> m) {\n+        return super.fromByteArray0Template(Double512Mask.class, a, offset, (Double512Mask) m);  \/\/ specialize\n+    }\n+\n@@ -802,0 +857,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    DoubleVector fromByteBuffer0(ByteBuffer bb, int offset, VectorMask<Double> m) {\n+        return super.fromByteBuffer0Template(Double512Mask.class, bb, offset, (Double512Mask) m);  \/\/ specialize\n+    }\n+\n@@ -809,0 +871,15 @@\n+    @ForceInline\n+    @Override\n+    final\n+    void intoArray0(double[] a, int offset, VectorMask<Double> m) {\n+        super.intoArray0Template(Double512Mask.class, a, offset, (Double512Mask) m);\n+    }\n+\n+    @ForceInline\n+    @Override\n+    final\n+    void intoArray0(double[] a, int offset, int[] indexMap, int mapOffset, VectorMask<Double> m) {\n+        super.intoArray0Template(Double512Mask.class, a, offset, indexMap, mapOffset, (Double512Mask) m);\n+    }\n+\n+\n@@ -816,0 +893,15 @@\n+    @ForceInline\n+    @Override\n+    final\n+    void intoByteArray0(byte[] a, int offset, VectorMask<Double> m) {\n+        super.intoByteArray0Template(Double512Mask.class, a, offset, (Double512Mask) m);  \/\/ specialize\n+    }\n+\n+    @ForceInline\n+    @Override\n+    final\n+    void intoByteBuffer0(ByteBuffer bb, int offset, VectorMask<Double> m) {\n+        super.intoByteBuffer0Template(Double512Mask.class, bb, offset, (Double512Mask) m);\n+    }\n+\n+\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/Double512Vector.java","additions":122,"deletions":30,"binary":false,"changes":152,"status":"modified"},{"patch":"@@ -239,2 +239,2 @@\n-    double rOp(double v, FBinOp f) {\n-        return super.rOpTemplate(v, f);  \/\/ specialize\n+    double rOp(double v, VectorMask<Double> m, FBinOp f) {\n+        return super.rOpTemplate(v, m, f);  \/\/ specialize\n@@ -276,0 +276,6 @@\n+    @Override\n+    @ForceInline\n+    public Double64Vector lanewise(Unary op, VectorMask<Double> m) {\n+        return (Double64Vector) super.lanewiseTemplate(op, Double64Mask.class, (Double64Mask) m);  \/\/ specialize\n+    }\n+\n@@ -282,0 +288,6 @@\n+    @Override\n+    @ForceInline\n+    public Double64Vector lanewise(Binary op, Vector<Double> v, VectorMask<Double> m) {\n+        return (Double64Vector) super.lanewiseTemplate(op, Double64Mask.class, v, (Double64Mask) m);  \/\/ specialize\n+    }\n+\n@@ -288,1 +300,1 @@\n-    lanewise(VectorOperators.Ternary op, Vector<Double> v1, Vector<Double> v2) {\n+    lanewise(Ternary op, Vector<Double> v1, Vector<Double> v2) {\n@@ -292,0 +304,8 @@\n+    @Override\n+    @ForceInline\n+    public final\n+    Double64Vector\n+    lanewise(Ternary op, Vector<Double> v1, Vector<Double> v2, VectorMask<Double> m) {\n+        return (Double64Vector) super.lanewiseTemplate(op, Double64Mask.class, v1, v2, (Double64Mask) m);  \/\/ specialize\n+    }\n+\n@@ -311,1 +331,1 @@\n-        return super.reduceLanesTemplate(op, m);  \/\/ specialized\n+        return super.reduceLanesTemplate(op, Double64Mask.class, (Double64Mask) m);  \/\/ specialized\n@@ -324,1 +344,1 @@\n-        return (long) super.reduceLanesTemplate(op, m);  \/\/ specialized\n+        return (long) super.reduceLanesTemplate(op, Double64Mask.class, (Double64Mask) m);  \/\/ specialized\n@@ -360,0 +380,7 @@\n+    @Override\n+    @ForceInline\n+    public final Double64Mask compare(Comparison op, Vector<Double> v, VectorMask<Double> m) {\n+        return super.compareTemplate(Double64Mask.class, op, v, (Double64Mask) m);\n+    }\n+\n+\n@@ -416,0 +443,1 @@\n+                                    Double64Mask.class,\n@@ -581,10 +609,6 @@\n-            if (VSIZE == species.vectorBitSize()) {\n-                Class<?> dtype = species.elementType();\n-                Class<?> dmtype = species.maskType();\n-                return VectorSupport.convert(VectorSupport.VECTOR_OP_REINTERPRET,\n-                    this.getClass(), ETYPE, VLENGTH,\n-                    dmtype, dtype, VLENGTH,\n-                    this, species,\n-                    Double64Mask::defaultMaskCast);\n-            }\n-            return this.defaultMaskCast(species);\n+\n+            return VectorSupport.convert(VectorSupport.VECTOR_OP_CAST,\n+                this.getClass(), ETYPE, VLENGTH,\n+                species.maskType(), species.elementType(), VLENGTH,\n+                this, species,\n+                (m, s) -> s.maskFactory(m.toArray()).check(s));\n@@ -616,3 +640,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_AND, Double64Mask.class, long.class, VLENGTH,\n-                                             this, m,\n-                                             (m1, m2) -> m1.bOp(m2, (i, a, b) -> a & b));\n+            return VectorSupport.binaryOp(VECTOR_OP_AND, Double64Mask.class, null, long.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a & b));\n@@ -626,3 +650,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_OR, Double64Mask.class, long.class, VLENGTH,\n-                                             this, m,\n-                                             (m1, m2) -> m1.bOp(m2, (i, a, b) -> a | b));\n+            return VectorSupport.binaryOp(VECTOR_OP_OR, Double64Mask.class, null, long.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a | b));\n@@ -636,3 +660,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_XOR, Double64Mask.class, long.class, VLENGTH,\n-                                          this, m,\n-                                          (m1, m2) -> m1.bOp(m2, (i, a, b) -> a ^ b));\n+            return VectorSupport.binaryOp(VECTOR_OP_XOR, Double64Mask.class, null, long.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a ^ b));\n@@ -646,2 +670,2 @@\n-            return VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_TRUECOUNT, Double64Mask.class, long.class, VLENGTH, this,\n-                                                      (m) -> trueCountHelper(((Double64Mask)m).getBits()));\n+            return (int) VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_TRUECOUNT, Double64Mask.class, long.class, VLENGTH, this,\n+                                                      (m) -> trueCountHelper(m.getBits()));\n@@ -653,2 +677,2 @@\n-            return VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_FIRSTTRUE, Double64Mask.class, long.class, VLENGTH, this,\n-                                                      (m) -> firstTrueHelper(((Double64Mask)m).getBits()));\n+            return (int) VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_FIRSTTRUE, Double64Mask.class, long.class, VLENGTH, this,\n+                                                      (m) -> firstTrueHelper(m.getBits()));\n@@ -660,2 +684,12 @@\n-            return VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_LASTTRUE, Double64Mask.class, long.class, VLENGTH, this,\n-                                                      (m) -> lastTrueHelper(((Double64Mask)m).getBits()));\n+            return (int) VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_LASTTRUE, Double64Mask.class, long.class, VLENGTH, this,\n+                                                      (m) -> lastTrueHelper(m.getBits()));\n+        }\n+\n+        @Override\n+        @ForceInline\n+        public long toLong() {\n+            if (length() > Long.SIZE) {\n+                throw new UnsupportedOperationException(\"too many lanes for one long\");\n+            }\n+            return VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_TOLONG, Double64Mask.class, long.class, VLENGTH, this,\n+                                                      (m) -> toLongHelper(m.getBits()));\n@@ -772,0 +806,14 @@\n+    @ForceInline\n+    @Override\n+    final\n+    DoubleVector fromArray0(double[] a, int offset, VectorMask<Double> m) {\n+        return super.fromArray0Template(Double64Mask.class, a, offset, (Double64Mask) m);  \/\/ specialize\n+    }\n+\n+    @ForceInline\n+    @Override\n+    final\n+    DoubleVector fromArray0(double[] a, int offset, int[] indexMap, int mapOffset, VectorMask<Double> m) {\n+        return super.fromArray0Template(Double64Mask.class, a, offset, indexMap, mapOffset, (Double64Mask) m);\n+    }\n+\n@@ -781,0 +829,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    DoubleVector fromByteArray0(byte[] a, int offset, VectorMask<Double> m) {\n+        return super.fromByteArray0Template(Double64Mask.class, a, offset, (Double64Mask) m);  \/\/ specialize\n+    }\n+\n@@ -788,0 +843,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    DoubleVector fromByteBuffer0(ByteBuffer bb, int offset, VectorMask<Double> m) {\n+        return super.fromByteBuffer0Template(Double64Mask.class, bb, offset, (Double64Mask) m);  \/\/ specialize\n+    }\n+\n@@ -795,0 +857,15 @@\n+    @ForceInline\n+    @Override\n+    final\n+    void intoArray0(double[] a, int offset, VectorMask<Double> m) {\n+        super.intoArray0Template(Double64Mask.class, a, offset, (Double64Mask) m);\n+    }\n+\n+    @ForceInline\n+    @Override\n+    final\n+    void intoArray0(double[] a, int offset, int[] indexMap, int mapOffset, VectorMask<Double> m) {\n+        super.intoArray0Template(Double64Mask.class, a, offset, indexMap, mapOffset, (Double64Mask) m);\n+    }\n+\n+\n@@ -802,0 +879,15 @@\n+    @ForceInline\n+    @Override\n+    final\n+    void intoByteArray0(byte[] a, int offset, VectorMask<Double> m) {\n+        super.intoByteArray0Template(Double64Mask.class, a, offset, (Double64Mask) m);  \/\/ specialize\n+    }\n+\n+    @ForceInline\n+    @Override\n+    final\n+    void intoByteBuffer0(ByteBuffer bb, int offset, VectorMask<Double> m) {\n+        super.intoByteBuffer0Template(Double64Mask.class, bb, offset, (Double64Mask) m);\n+    }\n+\n+\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/Double64Vector.java","additions":122,"deletions":30,"binary":false,"changes":152,"status":"modified"},{"patch":"@@ -239,2 +239,2 @@\n-    double rOp(double v, FBinOp f) {\n-        return super.rOpTemplate(v, f);  \/\/ specialize\n+    double rOp(double v, VectorMask<Double> m, FBinOp f) {\n+        return super.rOpTemplate(v, m, f);  \/\/ specialize\n@@ -276,0 +276,6 @@\n+    @Override\n+    @ForceInline\n+    public DoubleMaxVector lanewise(Unary op, VectorMask<Double> m) {\n+        return (DoubleMaxVector) super.lanewiseTemplate(op, DoubleMaxMask.class, (DoubleMaxMask) m);  \/\/ specialize\n+    }\n+\n@@ -282,0 +288,6 @@\n+    @Override\n+    @ForceInline\n+    public DoubleMaxVector lanewise(Binary op, Vector<Double> v, VectorMask<Double> m) {\n+        return (DoubleMaxVector) super.lanewiseTemplate(op, DoubleMaxMask.class, v, (DoubleMaxMask) m);  \/\/ specialize\n+    }\n+\n@@ -288,1 +300,1 @@\n-    lanewise(VectorOperators.Ternary op, Vector<Double> v1, Vector<Double> v2) {\n+    lanewise(Ternary op, Vector<Double> v1, Vector<Double> v2) {\n@@ -292,0 +304,8 @@\n+    @Override\n+    @ForceInline\n+    public final\n+    DoubleMaxVector\n+    lanewise(Ternary op, Vector<Double> v1, Vector<Double> v2, VectorMask<Double> m) {\n+        return (DoubleMaxVector) super.lanewiseTemplate(op, DoubleMaxMask.class, v1, v2, (DoubleMaxMask) m);  \/\/ specialize\n+    }\n+\n@@ -311,1 +331,1 @@\n-        return super.reduceLanesTemplate(op, m);  \/\/ specialized\n+        return super.reduceLanesTemplate(op, DoubleMaxMask.class, (DoubleMaxMask) m);  \/\/ specialized\n@@ -324,1 +344,1 @@\n-        return (long) super.reduceLanesTemplate(op, m);  \/\/ specialized\n+        return (long) super.reduceLanesTemplate(op, DoubleMaxMask.class, (DoubleMaxMask) m);  \/\/ specialized\n@@ -360,0 +380,7 @@\n+    @Override\n+    @ForceInline\n+    public final DoubleMaxMask compare(Comparison op, Vector<Double> v, VectorMask<Double> m) {\n+        return super.compareTemplate(DoubleMaxMask.class, op, v, (DoubleMaxMask) m);\n+    }\n+\n+\n@@ -416,0 +443,1 @@\n+                                    DoubleMaxMask.class,\n@@ -580,10 +608,6 @@\n-            if (VSIZE == species.vectorBitSize()) {\n-                Class<?> dtype = species.elementType();\n-                Class<?> dmtype = species.maskType();\n-                return VectorSupport.convert(VectorSupport.VECTOR_OP_REINTERPRET,\n-                    this.getClass(), ETYPE, VLENGTH,\n-                    dmtype, dtype, VLENGTH,\n-                    this, species,\n-                    DoubleMaxMask::defaultMaskCast);\n-            }\n-            return this.defaultMaskCast(species);\n+\n+            return VectorSupport.convert(VectorSupport.VECTOR_OP_CAST,\n+                this.getClass(), ETYPE, VLENGTH,\n+                species.maskType(), species.elementType(), VLENGTH,\n+                this, species,\n+                (m, s) -> s.maskFactory(m.toArray()).check(s));\n@@ -615,3 +639,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_AND, DoubleMaxMask.class, long.class, VLENGTH,\n-                                             this, m,\n-                                             (m1, m2) -> m1.bOp(m2, (i, a, b) -> a & b));\n+            return VectorSupport.binaryOp(VECTOR_OP_AND, DoubleMaxMask.class, null, long.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a & b));\n@@ -625,3 +649,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_OR, DoubleMaxMask.class, long.class, VLENGTH,\n-                                             this, m,\n-                                             (m1, m2) -> m1.bOp(m2, (i, a, b) -> a | b));\n+            return VectorSupport.binaryOp(VECTOR_OP_OR, DoubleMaxMask.class, null, long.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a | b));\n@@ -635,3 +659,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_XOR, DoubleMaxMask.class, long.class, VLENGTH,\n-                                          this, m,\n-                                          (m1, m2) -> m1.bOp(m2, (i, a, b) -> a ^ b));\n+            return VectorSupport.binaryOp(VECTOR_OP_XOR, DoubleMaxMask.class, null, long.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a ^ b));\n@@ -645,2 +669,2 @@\n-            return VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_TRUECOUNT, DoubleMaxMask.class, long.class, VLENGTH, this,\n-                                                      (m) -> trueCountHelper(((DoubleMaxMask)m).getBits()));\n+            return (int) VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_TRUECOUNT, DoubleMaxMask.class, long.class, VLENGTH, this,\n+                                                      (m) -> trueCountHelper(m.getBits()));\n@@ -652,2 +676,2 @@\n-            return VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_FIRSTTRUE, DoubleMaxMask.class, long.class, VLENGTH, this,\n-                                                      (m) -> firstTrueHelper(((DoubleMaxMask)m).getBits()));\n+            return (int) VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_FIRSTTRUE, DoubleMaxMask.class, long.class, VLENGTH, this,\n+                                                      (m) -> firstTrueHelper(m.getBits()));\n@@ -659,2 +683,12 @@\n-            return VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_LASTTRUE, DoubleMaxMask.class, long.class, VLENGTH, this,\n-                                                      (m) -> lastTrueHelper(((DoubleMaxMask)m).getBits()));\n+            return (int) VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_LASTTRUE, DoubleMaxMask.class, long.class, VLENGTH, this,\n+                                                      (m) -> lastTrueHelper(m.getBits()));\n+        }\n+\n+        @Override\n+        @ForceInline\n+        public long toLong() {\n+            if (length() > Long.SIZE) {\n+                throw new UnsupportedOperationException(\"too many lanes for one long\");\n+            }\n+            return VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_TOLONG, DoubleMaxMask.class, long.class, VLENGTH, this,\n+                                                      (m) -> toLongHelper(m.getBits()));\n@@ -771,0 +805,14 @@\n+    @ForceInline\n+    @Override\n+    final\n+    DoubleVector fromArray0(double[] a, int offset, VectorMask<Double> m) {\n+        return super.fromArray0Template(DoubleMaxMask.class, a, offset, (DoubleMaxMask) m);  \/\/ specialize\n+    }\n+\n+    @ForceInline\n+    @Override\n+    final\n+    DoubleVector fromArray0(double[] a, int offset, int[] indexMap, int mapOffset, VectorMask<Double> m) {\n+        return super.fromArray0Template(DoubleMaxMask.class, a, offset, indexMap, mapOffset, (DoubleMaxMask) m);\n+    }\n+\n@@ -780,0 +828,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    DoubleVector fromByteArray0(byte[] a, int offset, VectorMask<Double> m) {\n+        return super.fromByteArray0Template(DoubleMaxMask.class, a, offset, (DoubleMaxMask) m);  \/\/ specialize\n+    }\n+\n@@ -787,0 +842,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    DoubleVector fromByteBuffer0(ByteBuffer bb, int offset, VectorMask<Double> m) {\n+        return super.fromByteBuffer0Template(DoubleMaxMask.class, bb, offset, (DoubleMaxMask) m);  \/\/ specialize\n+    }\n+\n@@ -794,0 +856,15 @@\n+    @ForceInline\n+    @Override\n+    final\n+    void intoArray0(double[] a, int offset, VectorMask<Double> m) {\n+        super.intoArray0Template(DoubleMaxMask.class, a, offset, (DoubleMaxMask) m);\n+    }\n+\n+    @ForceInline\n+    @Override\n+    final\n+    void intoArray0(double[] a, int offset, int[] indexMap, int mapOffset, VectorMask<Double> m) {\n+        super.intoArray0Template(DoubleMaxMask.class, a, offset, indexMap, mapOffset, (DoubleMaxMask) m);\n+    }\n+\n+\n@@ -801,0 +878,15 @@\n+    @ForceInline\n+    @Override\n+    final\n+    void intoByteArray0(byte[] a, int offset, VectorMask<Double> m) {\n+        super.intoByteArray0Template(DoubleMaxMask.class, a, offset, (DoubleMaxMask) m);  \/\/ specialize\n+    }\n+\n+    @ForceInline\n+    @Override\n+    final\n+    void intoByteBuffer0(ByteBuffer bb, int offset, VectorMask<Double> m) {\n+        super.intoByteBuffer0Template(DoubleMaxMask.class, bb, offset, (DoubleMaxMask) m);\n+    }\n+\n+\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/DoubleMaxVector.java","additions":122,"deletions":30,"binary":false,"changes":152,"status":"modified"},{"patch":"@@ -32,1 +32,0 @@\n-import java.util.function.BinaryOperator;\n@@ -176,0 +175,3 @@\n+        if (m == null) {\n+            return uOpTemplate(f);\n+        }\n@@ -219,0 +221,3 @@\n+        if (m == null) {\n+            return bOpTemplate(o, f);\n+        }\n@@ -268,0 +273,3 @@\n+        if (m == null) {\n+            return tOpTemplate(o1, o2, f);\n+        }\n@@ -283,1 +291,16 @@\n-    double rOp(double v, FBinOp f);\n+    double rOp(double v, VectorMask<Double> m, FBinOp f);\n+\n+    @ForceInline\n+    final\n+    double rOpTemplate(double v, VectorMask<Double> m, FBinOp f) {\n+        if (m == null) {\n+            return rOpTemplate(v, f);\n+        }\n+        double[] vec = vec();\n+        boolean[] mbits = ((AbstractMask<Double>)m).getBits();\n+        for (int i = 0; i < vec.length; i++) {\n+            v = mbits[i] ? f.apply(i, v, vec[i]) : v;\n+        }\n+        return v;\n+    }\n+\n@@ -543,42 +566,3 @@\n-            opc, getClass(), double.class, length(),\n-            this,\n-            UN_IMPL.find(op, opc, (opc_) -> {\n-              switch (opc_) {\n-                case VECTOR_OP_NEG: return v0 ->\n-                        v0.uOp((i, a) -> (double) -a);\n-                case VECTOR_OP_ABS: return v0 ->\n-                        v0.uOp((i, a) -> (double) Math.abs(a));\n-                case VECTOR_OP_SIN: return v0 ->\n-                        v0.uOp((i, a) -> (double) Math.sin(a));\n-                case VECTOR_OP_COS: return v0 ->\n-                        v0.uOp((i, a) -> (double) Math.cos(a));\n-                case VECTOR_OP_TAN: return v0 ->\n-                        v0.uOp((i, a) -> (double) Math.tan(a));\n-                case VECTOR_OP_ASIN: return v0 ->\n-                        v0.uOp((i, a) -> (double) Math.asin(a));\n-                case VECTOR_OP_ACOS: return v0 ->\n-                        v0.uOp((i, a) -> (double) Math.acos(a));\n-                case VECTOR_OP_ATAN: return v0 ->\n-                        v0.uOp((i, a) -> (double) Math.atan(a));\n-                case VECTOR_OP_EXP: return v0 ->\n-                        v0.uOp((i, a) -> (double) Math.exp(a));\n-                case VECTOR_OP_LOG: return v0 ->\n-                        v0.uOp((i, a) -> (double) Math.log(a));\n-                case VECTOR_OP_LOG10: return v0 ->\n-                        v0.uOp((i, a) -> (double) Math.log10(a));\n-                case VECTOR_OP_SQRT: return v0 ->\n-                        v0.uOp((i, a) -> (double) Math.sqrt(a));\n-                case VECTOR_OP_CBRT: return v0 ->\n-                        v0.uOp((i, a) -> (double) Math.cbrt(a));\n-                case VECTOR_OP_SINH: return v0 ->\n-                        v0.uOp((i, a) -> (double) Math.sinh(a));\n-                case VECTOR_OP_COSH: return v0 ->\n-                        v0.uOp((i, a) -> (double) Math.cosh(a));\n-                case VECTOR_OP_TANH: return v0 ->\n-                        v0.uOp((i, a) -> (double) Math.tanh(a));\n-                case VECTOR_OP_EXPM1: return v0 ->\n-                        v0.uOp((i, a) -> (double) Math.expm1(a));\n-                case VECTOR_OP_LOG1P: return v0 ->\n-                        v0.uOp((i, a) -> (double) Math.log1p(a));\n-                default: return null;\n-              }}));\n+            opc, getClass(), null, double.class, length(),\n+            this, null,\n+            UN_IMPL.find(op, opc, DoubleVector::unaryOperations));\n@@ -586,3 +570,0 @@\n-    private static final\n-    ImplCache<Unary,UnaryOperator<DoubleVector>> UN_IMPL\n-        = new ImplCache<>(Unary.class, DoubleVector.class);\n@@ -593,2 +574,2 @@\n-    @ForceInline\n-    public final\n+    @Override\n+    public abstract\n@@ -596,2 +577,63 @@\n-                                  VectorMask<Double> m) {\n-        return blend(lanewise(op), m);\n+                                  VectorMask<Double> m);\n+    @ForceInline\n+    final\n+    DoubleVector lanewiseTemplate(VectorOperators.Unary op,\n+                                          Class<? extends VectorMask<Double>> maskClass,\n+                                          VectorMask<Double> m) {\n+        m.check(maskClass, this);\n+        if (opKind(op, VO_SPECIAL)) {\n+            if (op == ZOMO) {\n+                return blend(broadcast(-1), compare(NE, 0, m));\n+            }\n+        }\n+        int opc = opCode(op);\n+        return VectorSupport.unaryOp(\n+            opc, getClass(), maskClass, double.class, length(),\n+            this, m,\n+            UN_IMPL.find(op, opc, DoubleVector::unaryOperations));\n+    }\n+\n+    private static final\n+    ImplCache<Unary, UnaryOperation<DoubleVector, VectorMask<Double>>>\n+        UN_IMPL = new ImplCache<>(Unary.class, DoubleVector.class);\n+\n+    private static UnaryOperation<DoubleVector, VectorMask<Double>> unaryOperations(int opc_) {\n+        switch (opc_) {\n+            case VECTOR_OP_NEG: return (v0, m) ->\n+                    v0.uOp(m, (i, a) -> (double) -a);\n+            case VECTOR_OP_ABS: return (v0, m) ->\n+                    v0.uOp(m, (i, a) -> (double) Math.abs(a));\n+            case VECTOR_OP_SIN: return (v0, m) ->\n+                    v0.uOp(m, (i, a) -> (double) Math.sin(a));\n+            case VECTOR_OP_COS: return (v0, m) ->\n+                    v0.uOp(m, (i, a) -> (double) Math.cos(a));\n+            case VECTOR_OP_TAN: return (v0, m) ->\n+                    v0.uOp(m, (i, a) -> (double) Math.tan(a));\n+            case VECTOR_OP_ASIN: return (v0, m) ->\n+                    v0.uOp(m, (i, a) -> (double) Math.asin(a));\n+            case VECTOR_OP_ACOS: return (v0, m) ->\n+                    v0.uOp(m, (i, a) -> (double) Math.acos(a));\n+            case VECTOR_OP_ATAN: return (v0, m) ->\n+                    v0.uOp(m, (i, a) -> (double) Math.atan(a));\n+            case VECTOR_OP_EXP: return (v0, m) ->\n+                    v0.uOp(m, (i, a) -> (double) Math.exp(a));\n+            case VECTOR_OP_LOG: return (v0, m) ->\n+                    v0.uOp(m, (i, a) -> (double) Math.log(a));\n+            case VECTOR_OP_LOG10: return (v0, m) ->\n+                    v0.uOp(m, (i, a) -> (double) Math.log10(a));\n+            case VECTOR_OP_SQRT: return (v0, m) ->\n+                    v0.uOp(m, (i, a) -> (double) Math.sqrt(a));\n+            case VECTOR_OP_CBRT: return (v0, m) ->\n+                    v0.uOp(m, (i, a) -> (double) Math.cbrt(a));\n+            case VECTOR_OP_SINH: return (v0, m) ->\n+                    v0.uOp(m, (i, a) -> (double) Math.sinh(a));\n+            case VECTOR_OP_COSH: return (v0, m) ->\n+                    v0.uOp(m, (i, a) -> (double) Math.cosh(a));\n+            case VECTOR_OP_TANH: return (v0, m) ->\n+                    v0.uOp(m, (i, a) -> (double) Math.tanh(a));\n+            case VECTOR_OP_EXPM1: return (v0, m) ->\n+                    v0.uOp(m, (i, a) -> (double) Math.expm1(a));\n+            case VECTOR_OP_LOG1P: return (v0, m) ->\n+                    v0.uOp(m, (i, a) -> (double) Math.log1p(a));\n+            default: return null;\n+        }\n@@ -617,0 +659,1 @@\n+\n@@ -630,0 +673,1 @@\n+\n@@ -632,24 +676,3 @@\n-            opc, getClass(), double.class, length(),\n-            this, that,\n-            BIN_IMPL.find(op, opc, (opc_) -> {\n-              switch (opc_) {\n-                case VECTOR_OP_ADD: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (double)(a + b));\n-                case VECTOR_OP_SUB: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (double)(a - b));\n-                case VECTOR_OP_MUL: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (double)(a * b));\n-                case VECTOR_OP_DIV: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (double)(a \/ b));\n-                case VECTOR_OP_MAX: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (double)Math.max(a, b));\n-                case VECTOR_OP_MIN: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (double)Math.min(a, b));\n-                case VECTOR_OP_ATAN2: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (double) Math.atan2(a, b));\n-                case VECTOR_OP_POW: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (double) Math.pow(a, b));\n-                case VECTOR_OP_HYPOT: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (double) Math.hypot(a, b));\n-                default: return null;\n-                }}));\n+            opc, getClass(), null, double.class, length(),\n+            this, that, null,\n+            BIN_IMPL.find(op, opc, DoubleVector::binaryOperations));\n@@ -657,3 +680,0 @@\n-    private static final\n-    ImplCache<Binary,BinaryOperator<DoubleVector>> BIN_IMPL\n-        = new ImplCache<>(Binary.class, DoubleVector.class);\n@@ -665,2 +685,2 @@\n-    @ForceInline\n-    public final\n+    @Override\n+    public abstract\n@@ -669,2 +689,21 @@\n-                                  VectorMask<Double> m) {\n-        return blend(lanewise(op, v), m);\n+                                  VectorMask<Double> m);\n+    @ForceInline\n+    final\n+    DoubleVector lanewiseTemplate(VectorOperators.Binary op,\n+                                          Class<? extends VectorMask<Double>> maskClass,\n+                                          Vector<Double> v, VectorMask<Double> m) {\n+        DoubleVector that = (DoubleVector) v;\n+        that.check(this);\n+        m.check(maskClass, this);\n+\n+        if (opKind(op, VO_SPECIAL )) {\n+            if (op == FIRST_NONZERO) {\n+                return blend(lanewise(op, v), m);\n+            }\n+        }\n+\n+        int opc = opCode(op);\n+        return VectorSupport.binaryOp(\n+            opc, getClass(), maskClass, double.class, length(),\n+            this, that, m,\n+            BIN_IMPL.find(op, opc, DoubleVector::binaryOperations));\n@@ -672,0 +711,31 @@\n+\n+    private static final\n+    ImplCache<Binary, BinaryOperation<DoubleVector, VectorMask<Double>>>\n+        BIN_IMPL = new ImplCache<>(Binary.class, DoubleVector.class);\n+\n+    private static BinaryOperation<DoubleVector, VectorMask<Double>> binaryOperations(int opc_) {\n+        switch (opc_) {\n+            case VECTOR_OP_ADD: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> (double)(a + b));\n+            case VECTOR_OP_SUB: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> (double)(a - b));\n+            case VECTOR_OP_MUL: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> (double)(a * b));\n+            case VECTOR_OP_DIV: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> (double)(a \/ b));\n+            case VECTOR_OP_MAX: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> (double)Math.max(a, b));\n+            case VECTOR_OP_MIN: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> (double)Math.min(a, b));\n+            case VECTOR_OP_OR: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> fromBits(toBits(a) | toBits(b)));\n+            case VECTOR_OP_ATAN2: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> (double) Math.atan2(a, b));\n+            case VECTOR_OP_POW: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> (double) Math.pow(a, b));\n+            case VECTOR_OP_HYPOT: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> (double) Math.hypot(a, b));\n+            default: return null;\n+        }\n+    }\n+\n@@ -728,1 +798,1 @@\n-        return blend(lanewise(op, e), m);\n+        return lanewise(op, broadcast(e), m);\n@@ -746,2 +816,1 @@\n-        if ((long)e1 != e\n-            ) {\n+        if ((long)e1 != e) {\n@@ -767,1 +836,5 @@\n-        return blend(lanewise(op, e), m);\n+        double e1 = (double) e;\n+        if ((long)e1 != e) {\n+            vspecies().checkValue(e);  \/\/ for exception\n+        }\n+        return lanewise(op, e1, m);\n@@ -809,8 +882,3 @@\n-            opc, getClass(), double.class, length(),\n-            this, that, tother,\n-            TERN_IMPL.find(op, opc, (opc_) -> {\n-              switch (opc_) {\n-                case VECTOR_OP_FMA: return (v0, v1_, v2_) ->\n-                        v0.tOp(v1_, v2_, (i, a, b, c) -> Math.fma(a, b, c));\n-                default: return null;\n-                }}));\n+            opc, getClass(), null, double.class, length(),\n+            this, that, tother, null,\n+            TERN_IMPL.find(op, opc, DoubleVector::ternaryOperations));\n@@ -818,3 +886,0 @@\n-    private static final\n-    ImplCache<Ternary,TernaryOperation<DoubleVector>> TERN_IMPL\n-        = new ImplCache<>(Ternary.class, DoubleVector.class);\n@@ -828,2 +893,2 @@\n-    @ForceInline\n-    public final\n+    @Override\n+    public abstract\n@@ -833,2 +898,34 @@\n-                                  VectorMask<Double> m) {\n-        return blend(lanewise(op, v1, v2), m);\n+                                  VectorMask<Double> m);\n+    @ForceInline\n+    final\n+    DoubleVector lanewiseTemplate(VectorOperators.Ternary op,\n+                                          Class<? extends VectorMask<Double>> maskClass,\n+                                          Vector<Double> v1,\n+                                          Vector<Double> v2,\n+                                          VectorMask<Double> m) {\n+        DoubleVector that = (DoubleVector) v1;\n+        DoubleVector tother = (DoubleVector) v2;\n+        \/\/ It's a word: https:\/\/www.dictionary.com\/browse\/tother\n+        \/\/ See also Chapter 11 of Dickens, Our Mutual Friend:\n+        \/\/ \"Totherest Governor,\" replied Mr Riderhood...\n+        that.check(this);\n+        tother.check(this);\n+        m.check(maskClass, this);\n+\n+        int opc = opCode(op);\n+        return VectorSupport.ternaryOp(\n+            opc, getClass(), maskClass, double.class, length(),\n+            this, that, tother, m,\n+            TERN_IMPL.find(op, opc, DoubleVector::ternaryOperations));\n+    }\n+\n+    private static final\n+    ImplCache<Ternary, TernaryOperation<DoubleVector, VectorMask<Double>>>\n+        TERN_IMPL = new ImplCache<>(Ternary.class, DoubleVector.class);\n+\n+    private static TernaryOperation<DoubleVector, VectorMask<Double>> ternaryOperations(int opc_) {\n+        switch (opc_) {\n+            case VECTOR_OP_FMA: return (v0, v1_, v2_, m) ->\n+                    v0.tOp(v1_, v2_, m, (i, a, b, c) -> Math.fma(a, b, c));\n+            default: return null;\n+        }\n@@ -891,1 +988,1 @@\n-        return blend(lanewise(op, e1, e2), m);\n+        return lanewise(op, broadcast(e1), broadcast(e2), m);\n@@ -949,1 +1046,1 @@\n-        return blend(lanewise(op, v1, e2), m);\n+        return lanewise(op, v1, broadcast(e2), m);\n@@ -1006,1 +1103,1 @@\n-        return blend(lanewise(op, e1, v2), m);\n+        return lanewise(op, broadcast(e1), v2, m);\n@@ -1650,2 +1747,0 @@\n-        Objects.requireNonNull(v);\n-        DoubleSpecies vsp = vspecies();\n@@ -1657,2 +1752,2 @@\n-            this, that,\n-            (cond, v0, v1) -> {\n+            this, that, null,\n+            (cond, v0, v1, m1) -> {\n@@ -1668,0 +1763,22 @@\n+    \/*package-private*\/\n+    @ForceInline\n+    final\n+    <M extends VectorMask<Double>>\n+    M compareTemplate(Class<M> maskType, Comparison op, Vector<Double> v, M m) {\n+        DoubleVector that = (DoubleVector) v;\n+        that.check(this);\n+        m.check(maskType, this);\n+        int opc = opCode(op);\n+        return VectorSupport.compare(\n+            opc, getClass(), maskType, double.class, length(),\n+            this, that, m,\n+            (cond, v0, v1, m1) -> {\n+                AbstractMask<Double> cmpM\n+                    = v0.bTest(cond, v1, (cond_, i, a, b)\n+                               -> compareWithOp(cond, a, b));\n+                @SuppressWarnings(\"unchecked\")\n+                M m2 = (M) cmpM.and(m1);\n+                return m2;\n+            });\n+    }\n+\n@@ -1681,12 +1798,0 @@\n-    \/**\n-     * {@inheritDoc} <!--workaround-->\n-     *\/\n-    @Override\n-    @ForceInline\n-    public final\n-    VectorMask<Double> compare(VectorOperators.Comparison op,\n-                                  Vector<Double> v,\n-                                  VectorMask<Double> m) {\n-        return compare(op, v).and(m);\n-    }\n-\n@@ -1751,1 +1856,1 @@\n-        return compare(op, e).and(m);\n+        return compare(op, broadcast(e), m);\n@@ -2002,3 +2107,3 @@\n-            getClass(), shuffletype, double.class, length(),\n-            this, shuffle,\n-            (v1, s_) -> v1.uOp((i, a) -> {\n+            getClass(), shuffletype, null, double.class, length(),\n+            this, shuffle, null,\n+            (v1, s_, m_) -> v1.uOp((i, a) -> {\n@@ -2021,1 +2126,1 @@\n-    <S extends VectorShuffle<Double>>\n+    <S extends VectorShuffle<Double>, M extends VectorMask<Double>>\n@@ -2023,0 +2128,1 @@\n+                                           Class<M> masktype,\n@@ -2024,9 +2130,3 @@\n-                                           VectorMask<Double> m) {\n-        DoubleVector unmasked =\n-            VectorSupport.rearrangeOp(\n-                getClass(), shuffletype, double.class, length(),\n-                this, shuffle,\n-                (v1, s_) -> v1.uOp((i, a) -> {\n-                    int ei = s_.laneSource(i);\n-                    return ei < 0 ? 0 : v1.lane(ei);\n-                }));\n+                                           M m) {\n+\n+        m.check(masktype, this);\n@@ -2038,1 +2138,7 @@\n-        return broadcast((double)0).blend(unmasked, m);\n+        return VectorSupport.rearrangeOp(\n+                   getClass(), shuffletype, masktype, double.class, length(),\n+                   this, shuffle, m,\n+                   (v1, s_, m_) -> v1.uOp((i, a) -> {\n+                        int ei = s_.laneSource(i);\n+                        return ei < 0  || !m_.laneIsSet(i) ? 0 : v1.lane(ei);\n+                   }));\n@@ -2061,3 +2167,3 @@\n-                getClass(), shuffletype, double.class, length(),\n-                this, ws,\n-                (v0, s_) -> v0.uOp((i, a) -> {\n+                getClass(), shuffletype, null, double.class, length(),\n+                this, ws, null,\n+                (v0, s_, m_) -> v0.uOp((i, a) -> {\n@@ -2069,3 +2175,3 @@\n-                getClass(), shuffletype, double.class, length(),\n-                v, ws,\n-                (v1, s_) -> v1.uOp((i, a) -> {\n+                getClass(), shuffletype, null, double.class, length(),\n+                v, ws, null,\n+                (v1, s_, m_) -> v1.uOp((i, a) -> {\n@@ -2312,0 +2418,1 @@\n+                               Class<? extends VectorMask<Double>> maskClass,\n@@ -2313,2 +2420,10 @@\n-        DoubleVector v = reduceIdentityVector(op).blend(this, m);\n-        return v.reduceLanesTemplate(op);\n+        m.check(maskClass, this);\n+        if (op == FIRST_NONZERO) {\n+            DoubleVector v = reduceIdentityVector(op).blend(this, m);\n+            return v.reduceLanesTemplate(op);\n+        }\n+        int opc = opCode(op);\n+        return fromBits(VectorSupport.reductionCoerced(\n+            opc, getClass(), maskClass, double.class, length(),\n+            this, m,\n+            REDUCE_IMPL.find(op, opc, DoubleVector::reductionOperations)));\n@@ -2329,14 +2444,3 @@\n-            opc, getClass(), double.class, length(),\n-            this,\n-            REDUCE_IMPL.find(op, opc, (opc_) -> {\n-              switch (opc_) {\n-              case VECTOR_OP_ADD: return v ->\n-                      toBits(v.rOp((double)0, (i, a, b) -> (double)(a + b)));\n-              case VECTOR_OP_MUL: return v ->\n-                      toBits(v.rOp((double)1, (i, a, b) -> (double)(a * b)));\n-              case VECTOR_OP_MIN: return v ->\n-                      toBits(v.rOp(MAX_OR_INF, (i, a, b) -> (double) Math.min(a, b)));\n-              case VECTOR_OP_MAX: return v ->\n-                      toBits(v.rOp(MIN_OR_INF, (i, a, b) -> (double) Math.max(a, b)));\n-              default: return null;\n-              }})));\n+            opc, getClass(), null, double.class, length(),\n+            this, null,\n+            REDUCE_IMPL.find(op, opc, DoubleVector::reductionOperations)));\n@@ -2344,0 +2448,1 @@\n+\n@@ -2345,2 +2450,16 @@\n-    ImplCache<Associative,Function<DoubleVector,Long>> REDUCE_IMPL\n-        = new ImplCache<>(Associative.class, DoubleVector.class);\n+    ImplCache<Associative, ReductionOperation<DoubleVector, VectorMask<Double>>>\n+        REDUCE_IMPL = new ImplCache<>(Associative.class, DoubleVector.class);\n+\n+    private static ReductionOperation<DoubleVector, VectorMask<Double>> reductionOperations(int opc_) {\n+        switch (opc_) {\n+            case VECTOR_OP_ADD: return (v, m) ->\n+                    toBits(v.rOp((double)0, m, (i, a, b) -> (double)(a + b)));\n+            case VECTOR_OP_MUL: return (v, m) ->\n+                    toBits(v.rOp((double)1, m, (i, a, b) -> (double)(a * b)));\n+            case VECTOR_OP_MIN: return (v, m) ->\n+                    toBits(v.rOp(MAX_OR_INF, m, (i, a, b) -> (double) Math.min(a, b)));\n+            case VECTOR_OP_MAX: return (v, m) ->\n+                    toBits(v.rOp(MIN_OR_INF, m, (i, a, b) -> (double) Math.max(a, b)));\n+            default: return null;\n+        }\n+    }\n@@ -2552,3 +2671,1 @@\n-            DoubleVector zero = vsp.zero();\n-            DoubleVector v = zero.fromByteArray0(a, offset);\n-            return zero.blend(v.maybeSwap(bo), m);\n+            return vsp.dummyVector().fromByteArray0(a, offset, m).maybeSwap(bo);\n@@ -2616,2 +2733,1 @@\n-            DoubleVector zero = vsp.zero();\n-            return zero.blend(zero.fromArray0(a, offset), m);\n+            return vsp.dummyVector().fromArray0(a, offset, m);\n@@ -2693,3 +2809,3 @@\n-            vectorType, double.class, vsp.laneCount(),\n-            IntVector.species(vsp.indexShape()).vectorType(),\n-            a, ARRAY_BASE, vix,\n+            vectorType, null, double.class, vsp.laneCount(),\n+            isp.vectorType(),\n+            a, ARRAY_BASE, vix, null,\n@@ -2697,1 +2813,1 @@\n-            (double[] c, int idx, int[] iMap, int idy, DoubleSpecies s) ->\n+            (c, idx, iMap, idy, s, vm) ->\n@@ -2699,1 +2815,1 @@\n-        }\n+    }\n@@ -2747,1 +2863,0 @@\n-            \/\/ FIXME: Cannot vectorize yet, if there's a mask.\n@@ -2749,1 +2864,1 @@\n-            return vsp.vOp(m, n -> a[offset + indexMap[mapOffset + n]]);\n+            return vsp.dummyVector().fromArray0(a, offset, indexMap, mapOffset, m);\n@@ -2843,3 +2958,1 @@\n-            DoubleVector zero = vsp.zero();\n-            DoubleVector v = zero.fromByteBuffer0(bb, offset);\n-            return zero.blend(v.maybeSwap(bo), m);\n+            return vsp.dummyVector().fromByteBuffer0(bb, offset, m).maybeSwap(bo);\n@@ -2917,1 +3030,0 @@\n-            \/\/ FIXME: optimize\n@@ -2920,1 +3032,1 @@\n-            stOp(a, offset, m, (arr, off, i, v) -> arr[off+i] = v);\n+            intoArray0(a, offset, m);\n@@ -2983,1 +3095,1 @@\n-            vsp.vectorType(), vsp.elementType(), vsp.laneCount(),\n+            vsp.vectorType(), null, vsp.elementType(), vsp.laneCount(),\n@@ -2986,1 +3098,1 @@\n-            this,\n+            this, null,\n@@ -2988,1 +3100,1 @@\n-            (arr, off, v, map, mo)\n+            (arr, off, v, map, mo, vm)\n@@ -3035,6 +3147,1 @@\n-            \/\/ FIXME: Cannot vectorize yet, if there's a mask.\n-            stOp(a, offset, m,\n-                 (arr, off, i, e) -> {\n-                     int j = indexMap[mapOffset + i];\n-                     arr[off + j] = e;\n-                 });\n+            intoArray0(a, offset, indexMap, mapOffset, m);\n@@ -3070,1 +3177,0 @@\n-            \/\/ FIXME: optimize\n@@ -3073,3 +3179,1 @@\n-            ByteBuffer wb = wrapper(a, bo);\n-            this.stOp(wb, offset, m,\n-                    (wb_, o, i, e) -> wb_.putDouble(o + i * 8, e));\n+            maybeSwap(bo).intoByteArray0(a, offset, m);\n@@ -3087,1 +3191,1 @@\n-        if (bb.isReadOnly()) {\n+        if (ScopedMemoryAccess.isReadOnly(bb)) {\n@@ -3106,1 +3210,0 @@\n-            \/\/ FIXME: optimize\n@@ -3112,3 +3215,1 @@\n-            ByteBuffer wb = wrapper(bb, bo);\n-            this.stOp(wb, offset, m,\n-                    (wb_, o, i, e) -> wb_.putDouble(o + i * 8, e));\n+            maybeSwap(bo).intoByteBuffer0(bb, offset, m);\n@@ -3152,0 +3253,69 @@\n+    \/*package-private*\/\n+    abstract\n+    DoubleVector fromArray0(double[] a, int offset, VectorMask<Double> m);\n+    @ForceInline\n+    final\n+    <M extends VectorMask<Double>>\n+    DoubleVector fromArray0Template(Class<M> maskClass, double[] a, int offset, M m) {\n+        m.check(species());\n+        DoubleSpecies vsp = vspecies();\n+        return VectorSupport.loadMasked(\n+            vsp.vectorType(), maskClass, vsp.elementType(), vsp.laneCount(),\n+            a, arrayAddress(a, offset), m,\n+            a, offset, vsp,\n+            (arr, off, s, vm) -> s.ldOp(arr, off, vm,\n+                                        (arr_, off_, i) -> arr_[off_ + i]));\n+    }\n+\n+    \/*package-private*\/\n+    abstract\n+    DoubleVector fromArray0(double[] a, int offset,\n+                                    int[] indexMap, int mapOffset,\n+                                    VectorMask<Double> m);\n+    @ForceInline\n+    final\n+    <M extends VectorMask<Double>>\n+    DoubleVector fromArray0Template(Class<M> maskClass, double[] a, int offset,\n+                                            int[] indexMap, int mapOffset, M m) {\n+        DoubleSpecies vsp = vspecies();\n+        IntVector.IntSpecies isp = IntVector.species(vsp.indexShape());\n+        Objects.requireNonNull(a);\n+        Objects.requireNonNull(indexMap);\n+        m.check(vsp);\n+        Class<? extends DoubleVector> vectorType = vsp.vectorType();\n+\n+        if (vsp.laneCount() == 1) {\n+          return DoubleVector.fromArray(vsp, a, offset + indexMap[mapOffset], m);\n+        }\n+\n+        \/\/ Index vector: vix[0:n] = k -> offset + indexMap[mapOffset + k]\n+        IntVector vix;\n+        if (isp.laneCount() != vsp.laneCount()) {\n+            \/\/ For DoubleMaxVector,  if vector length is non-power-of-two or\n+            \/\/ 2048 bits, indexShape of Double species is S_MAX_BIT.\n+            \/\/ Assume that vector length is 2048, then the lane count of Double\n+            \/\/ vector is 32. When converting Double species to int species,\n+            \/\/ indexShape is still S_MAX_BIT, but the lane count of int vector\n+            \/\/ is 64. So when loading index vector (IntVector), only lower half\n+            \/\/ of index data is needed.\n+            vix = IntVector\n+                .fromArray(isp, indexMap, mapOffset, IntMaxVector.IntMaxMask.LOWER_HALF_TRUE_MASK)\n+                .add(offset);\n+        } else {\n+            vix = IntVector\n+                .fromArray(isp, indexMap, mapOffset)\n+                .add(offset);\n+        }\n+\n+        \/\/ FIXME: Check index under mask controlling.\n+        vix = VectorIntrinsics.checkIndex(vix, a.length);\n+\n+        return VectorSupport.loadWithMap(\n+            vectorType, maskClass, double.class, vsp.laneCount(),\n+            isp.vectorType(),\n+            a, ARRAY_BASE, vix, m,\n+            a, offset, indexMap, mapOffset, vsp,\n+            (c, idx, iMap, idy, s, vm) ->\n+            s.vOp(vm, n -> c[idx + iMap[idy+n]]));\n+    }\n+\n@@ -3172,0 +3342,19 @@\n+    abstract\n+    DoubleVector fromByteArray0(byte[] a, int offset, VectorMask<Double> m);\n+    @ForceInline\n+    final\n+    <M extends VectorMask<Double>>\n+    DoubleVector fromByteArray0Template(Class<M> maskClass, byte[] a, int offset, M m) {\n+        DoubleSpecies vsp = vspecies();\n+        m.check(vsp);\n+        return VectorSupport.loadMasked(\n+            vsp.vectorType(), maskClass, vsp.elementType(), vsp.laneCount(),\n+            a, byteArrayAddress(a, offset), m,\n+            a, offset, vsp,\n+            (arr, off, s, vm) -> {\n+                ByteBuffer wb = wrapper(arr, NATIVE_ENDIAN);\n+                return s.ldOp(wb, off, vm,\n+                        (wb_, o, i) -> wb_.getDouble(o + i * 8));\n+            });\n+    }\n+\n@@ -3188,0 +3377,18 @@\n+    abstract\n+    DoubleVector fromByteBuffer0(ByteBuffer bb, int offset, VectorMask<Double> m);\n+    @ForceInline\n+    final\n+    <M extends VectorMask<Double>>\n+    DoubleVector fromByteBuffer0Template(Class<M> maskClass, ByteBuffer bb, int offset, M m) {\n+        DoubleSpecies vsp = vspecies();\n+        m.check(vsp);\n+        return ScopedMemoryAccess.loadFromByteBufferMasked(\n+                vsp.vectorType(), maskClass, vsp.elementType(), vsp.laneCount(),\n+                bb, offset, m, vsp,\n+                (buf, off, s, vm) -> {\n+                    ByteBuffer wb = wrapper(buf, NATIVE_ENDIAN);\n+                    return s.ldOp(wb, off, vm,\n+                            (wb_, o, i) -> wb_.getDouble(o + i * 8));\n+                });\n+    }\n+\n@@ -3207,0 +3414,71 @@\n+    abstract\n+    void intoArray0(double[] a, int offset, VectorMask<Double> m);\n+    @ForceInline\n+    final\n+    <M extends VectorMask<Double>>\n+    void intoArray0Template(Class<M> maskClass, double[] a, int offset, M m) {\n+        m.check(species());\n+        DoubleSpecies vsp = vspecies();\n+        VectorSupport.storeMasked(\n+            vsp.vectorType(), maskClass, vsp.elementType(), vsp.laneCount(),\n+            a, arrayAddress(a, offset),\n+            this, m, a, offset,\n+            (arr, off, v, vm)\n+            -> v.stOp(arr, off, vm,\n+                      (arr_, off_, i, e) -> arr_[off_ + i] = e));\n+    }\n+\n+    abstract\n+    void intoArray0(double[] a, int offset,\n+                    int[] indexMap, int mapOffset,\n+                    VectorMask<Double> m);\n+    @ForceInline\n+    final\n+    <M extends VectorMask<Double>>\n+    void intoArray0Template(Class<M> maskClass, double[] a, int offset,\n+                            int[] indexMap, int mapOffset, M m) {\n+        m.check(species());\n+        DoubleSpecies vsp = vspecies();\n+        IntVector.IntSpecies isp = IntVector.species(vsp.indexShape());\n+        if (vsp.laneCount() == 1) {\n+            intoArray(a, offset + indexMap[mapOffset], m);\n+            return;\n+        }\n+\n+        \/\/ Index vector: vix[0:n] = i -> offset + indexMap[mo + i]\n+        IntVector vix;\n+        if (isp.laneCount() != vsp.laneCount()) {\n+            \/\/ For DoubleMaxVector,  if vector length  is 2048 bits, indexShape\n+            \/\/ of Double species is S_MAX_BIT. and the lane count of Double\n+            \/\/ vector is 32. When converting Double species to int species,\n+            \/\/ indexShape is still S_MAX_BIT, but the lane count of int vector\n+            \/\/ is 64. So when loading index vector (IntVector), only lower half\n+            \/\/ of index data is needed.\n+            vix = IntVector\n+                .fromArray(isp, indexMap, mapOffset, IntMaxVector.IntMaxMask.LOWER_HALF_TRUE_MASK)\n+                .add(offset);\n+        } else {\n+            vix = IntVector\n+                .fromArray(isp, indexMap, mapOffset)\n+                .add(offset);\n+        }\n+\n+\n+        \/\/ FIXME: Check index under mask controlling.\n+        vix = VectorIntrinsics.checkIndex(vix, a.length);\n+\n+        VectorSupport.storeWithMap(\n+            vsp.vectorType(), maskClass, vsp.elementType(), vsp.laneCount(),\n+            isp.vectorType(),\n+            a, arrayAddress(a, 0), vix,\n+            this, m,\n+            a, offset, indexMap, mapOffset,\n+            (arr, off, v, map, mo, vm)\n+            -> v.stOp(arr, off, vm,\n+                      (arr_, off_, i, e) -> {\n+                          int j = map[mo + i];\n+                          arr[off + j] = e;\n+                      }));\n+    }\n+\n+\n@@ -3224,0 +3502,19 @@\n+    abstract\n+    void intoByteArray0(byte[] a, int offset, VectorMask<Double> m);\n+    @ForceInline\n+    final\n+    <M extends VectorMask<Double>>\n+    void intoByteArray0Template(Class<M> maskClass, byte[] a, int offset, M m) {\n+        DoubleSpecies vsp = vspecies();\n+        m.check(vsp);\n+        VectorSupport.storeMasked(\n+            vsp.vectorType(), maskClass, vsp.elementType(), vsp.laneCount(),\n+            a, byteArrayAddress(a, offset),\n+            this, m, a, offset,\n+            (arr, off, v, vm) -> {\n+                ByteBuffer wb = wrapper(arr, NATIVE_ENDIAN);\n+                v.stOp(wb, off, vm,\n+                        (tb_, o, i, e) -> tb_.putDouble(o + i * 8, e));\n+            });\n+    }\n+\n@@ -3238,0 +3535,19 @@\n+    abstract\n+    void intoByteBuffer0(ByteBuffer bb, int offset, VectorMask<Double> m);\n+    @ForceInline\n+    final\n+    <M extends VectorMask<Double>>\n+    void intoByteBuffer0Template(Class<M> maskClass, ByteBuffer bb, int offset, M m) {\n+        DoubleSpecies vsp = vspecies();\n+        m.check(vsp);\n+        ScopedMemoryAccess.storeIntoByteBufferMasked(\n+                vsp.vectorType(), maskClass, vsp.elementType(), vsp.laneCount(),\n+                this, m, bb, offset,\n+                (buf, off, v, vm) -> {\n+                    ByteBuffer wb = wrapper(buf, NATIVE_ENDIAN);\n+                    v.stOp(wb, off, vm,\n+                            (wb_, o, i, e) -> wb_.putDouble(o + i * 8, e));\n+                });\n+    }\n+\n+\n@@ -3555,1 +3871,1 @@\n-                                      AbstractMask<Double> m,\n+                                      VectorMask<Double> m,\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/DoubleVector.java","additions":511,"deletions":195,"binary":false,"changes":706,"status":"modified"},{"patch":"@@ -239,2 +239,2 @@\n-    float rOp(float v, FBinOp f) {\n-        return super.rOpTemplate(v, f);  \/\/ specialize\n+    float rOp(float v, VectorMask<Float> m, FBinOp f) {\n+        return super.rOpTemplate(v, m, f);  \/\/ specialize\n@@ -276,0 +276,6 @@\n+    @Override\n+    @ForceInline\n+    public Float128Vector lanewise(Unary op, VectorMask<Float> m) {\n+        return (Float128Vector) super.lanewiseTemplate(op, Float128Mask.class, (Float128Mask) m);  \/\/ specialize\n+    }\n+\n@@ -282,0 +288,6 @@\n+    @Override\n+    @ForceInline\n+    public Float128Vector lanewise(Binary op, Vector<Float> v, VectorMask<Float> m) {\n+        return (Float128Vector) super.lanewiseTemplate(op, Float128Mask.class, v, (Float128Mask) m);  \/\/ specialize\n+    }\n+\n@@ -288,1 +300,1 @@\n-    lanewise(VectorOperators.Ternary op, Vector<Float> v1, Vector<Float> v2) {\n+    lanewise(Ternary op, Vector<Float> v1, Vector<Float> v2) {\n@@ -292,0 +304,8 @@\n+    @Override\n+    @ForceInline\n+    public final\n+    Float128Vector\n+    lanewise(Ternary op, Vector<Float> v1, Vector<Float> v2, VectorMask<Float> m) {\n+        return (Float128Vector) super.lanewiseTemplate(op, Float128Mask.class, v1, v2, (Float128Mask) m);  \/\/ specialize\n+    }\n+\n@@ -311,1 +331,1 @@\n-        return super.reduceLanesTemplate(op, m);  \/\/ specialized\n+        return super.reduceLanesTemplate(op, Float128Mask.class, (Float128Mask) m);  \/\/ specialized\n@@ -324,1 +344,1 @@\n-        return (long) super.reduceLanesTemplate(op, m);  \/\/ specialized\n+        return (long) super.reduceLanesTemplate(op, Float128Mask.class, (Float128Mask) m);  \/\/ specialized\n@@ -360,0 +380,7 @@\n+    @Override\n+    @ForceInline\n+    public final Float128Mask compare(Comparison op, Vector<Float> v, VectorMask<Float> m) {\n+        return super.compareTemplate(Float128Mask.class, op, v, (Float128Mask) m);\n+    }\n+\n+\n@@ -416,0 +443,1 @@\n+                                    Float128Mask.class,\n@@ -587,10 +615,6 @@\n-            if (VSIZE == species.vectorBitSize()) {\n-                Class<?> dtype = species.elementType();\n-                Class<?> dmtype = species.maskType();\n-                return VectorSupport.convert(VectorSupport.VECTOR_OP_REINTERPRET,\n-                    this.getClass(), ETYPE, VLENGTH,\n-                    dmtype, dtype, VLENGTH,\n-                    this, species,\n-                    Float128Mask::defaultMaskCast);\n-            }\n-            return this.defaultMaskCast(species);\n+\n+            return VectorSupport.convert(VectorSupport.VECTOR_OP_CAST,\n+                this.getClass(), ETYPE, VLENGTH,\n+                species.maskType(), species.elementType(), VLENGTH,\n+                this, species,\n+                (m, s) -> s.maskFactory(m.toArray()).check(s));\n@@ -622,3 +646,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_AND, Float128Mask.class, int.class, VLENGTH,\n-                                             this, m,\n-                                             (m1, m2) -> m1.bOp(m2, (i, a, b) -> a & b));\n+            return VectorSupport.binaryOp(VECTOR_OP_AND, Float128Mask.class, null, int.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a & b));\n@@ -632,3 +656,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_OR, Float128Mask.class, int.class, VLENGTH,\n-                                             this, m,\n-                                             (m1, m2) -> m1.bOp(m2, (i, a, b) -> a | b));\n+            return VectorSupport.binaryOp(VECTOR_OP_OR, Float128Mask.class, null, int.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a | b));\n@@ -642,3 +666,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_XOR, Float128Mask.class, int.class, VLENGTH,\n-                                          this, m,\n-                                          (m1, m2) -> m1.bOp(m2, (i, a, b) -> a ^ b));\n+            return VectorSupport.binaryOp(VECTOR_OP_XOR, Float128Mask.class, null, int.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a ^ b));\n@@ -652,2 +676,2 @@\n-            return VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_TRUECOUNT, Float128Mask.class, int.class, VLENGTH, this,\n-                                                      (m) -> trueCountHelper(((Float128Mask)m).getBits()));\n+            return (int) VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_TRUECOUNT, Float128Mask.class, int.class, VLENGTH, this,\n+                                                      (m) -> trueCountHelper(m.getBits()));\n@@ -659,2 +683,2 @@\n-            return VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_FIRSTTRUE, Float128Mask.class, int.class, VLENGTH, this,\n-                                                      (m) -> firstTrueHelper(((Float128Mask)m).getBits()));\n+            return (int) VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_FIRSTTRUE, Float128Mask.class, int.class, VLENGTH, this,\n+                                                      (m) -> firstTrueHelper(m.getBits()));\n@@ -666,2 +690,12 @@\n-            return VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_LASTTRUE, Float128Mask.class, int.class, VLENGTH, this,\n-                                                      (m) -> lastTrueHelper(((Float128Mask)m).getBits()));\n+            return (int) VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_LASTTRUE, Float128Mask.class, int.class, VLENGTH, this,\n+                                                      (m) -> lastTrueHelper(m.getBits()));\n+        }\n+\n+        @Override\n+        @ForceInline\n+        public long toLong() {\n+            if (length() > Long.SIZE) {\n+                throw new UnsupportedOperationException(\"too many lanes for one long\");\n+            }\n+            return VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_TOLONG, Float128Mask.class, int.class, VLENGTH, this,\n+                                                      (m) -> toLongHelper(m.getBits()));\n@@ -778,0 +812,14 @@\n+    @ForceInline\n+    @Override\n+    final\n+    FloatVector fromArray0(float[] a, int offset, VectorMask<Float> m) {\n+        return super.fromArray0Template(Float128Mask.class, a, offset, (Float128Mask) m);  \/\/ specialize\n+    }\n+\n+    @ForceInline\n+    @Override\n+    final\n+    FloatVector fromArray0(float[] a, int offset, int[] indexMap, int mapOffset, VectorMask<Float> m) {\n+        return super.fromArray0Template(Float128Mask.class, a, offset, indexMap, mapOffset, (Float128Mask) m);\n+    }\n+\n@@ -787,0 +835,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    FloatVector fromByteArray0(byte[] a, int offset, VectorMask<Float> m) {\n+        return super.fromByteArray0Template(Float128Mask.class, a, offset, (Float128Mask) m);  \/\/ specialize\n+    }\n+\n@@ -794,0 +849,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    FloatVector fromByteBuffer0(ByteBuffer bb, int offset, VectorMask<Float> m) {\n+        return super.fromByteBuffer0Template(Float128Mask.class, bb, offset, (Float128Mask) m);  \/\/ specialize\n+    }\n+\n@@ -801,0 +863,15 @@\n+    @ForceInline\n+    @Override\n+    final\n+    void intoArray0(float[] a, int offset, VectorMask<Float> m) {\n+        super.intoArray0Template(Float128Mask.class, a, offset, (Float128Mask) m);\n+    }\n+\n+    @ForceInline\n+    @Override\n+    final\n+    void intoArray0(float[] a, int offset, int[] indexMap, int mapOffset, VectorMask<Float> m) {\n+        super.intoArray0Template(Float128Mask.class, a, offset, indexMap, mapOffset, (Float128Mask) m);\n+    }\n+\n+\n@@ -808,0 +885,15 @@\n+    @ForceInline\n+    @Override\n+    final\n+    void intoByteArray0(byte[] a, int offset, VectorMask<Float> m) {\n+        super.intoByteArray0Template(Float128Mask.class, a, offset, (Float128Mask) m);  \/\/ specialize\n+    }\n+\n+    @ForceInline\n+    @Override\n+    final\n+    void intoByteBuffer0(ByteBuffer bb, int offset, VectorMask<Float> m) {\n+        super.intoByteBuffer0Template(Float128Mask.class, bb, offset, (Float128Mask) m);\n+    }\n+\n+\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/Float128Vector.java","additions":122,"deletions":30,"binary":false,"changes":152,"status":"modified"},{"patch":"@@ -239,2 +239,2 @@\n-    float rOp(float v, FBinOp f) {\n-        return super.rOpTemplate(v, f);  \/\/ specialize\n+    float rOp(float v, VectorMask<Float> m, FBinOp f) {\n+        return super.rOpTemplate(v, m, f);  \/\/ specialize\n@@ -276,0 +276,6 @@\n+    @Override\n+    @ForceInline\n+    public Float256Vector lanewise(Unary op, VectorMask<Float> m) {\n+        return (Float256Vector) super.lanewiseTemplate(op, Float256Mask.class, (Float256Mask) m);  \/\/ specialize\n+    }\n+\n@@ -282,0 +288,6 @@\n+    @Override\n+    @ForceInline\n+    public Float256Vector lanewise(Binary op, Vector<Float> v, VectorMask<Float> m) {\n+        return (Float256Vector) super.lanewiseTemplate(op, Float256Mask.class, v, (Float256Mask) m);  \/\/ specialize\n+    }\n+\n@@ -288,1 +300,1 @@\n-    lanewise(VectorOperators.Ternary op, Vector<Float> v1, Vector<Float> v2) {\n+    lanewise(Ternary op, Vector<Float> v1, Vector<Float> v2) {\n@@ -292,0 +304,8 @@\n+    @Override\n+    @ForceInline\n+    public final\n+    Float256Vector\n+    lanewise(Ternary op, Vector<Float> v1, Vector<Float> v2, VectorMask<Float> m) {\n+        return (Float256Vector) super.lanewiseTemplate(op, Float256Mask.class, v1, v2, (Float256Mask) m);  \/\/ specialize\n+    }\n+\n@@ -311,1 +331,1 @@\n-        return super.reduceLanesTemplate(op, m);  \/\/ specialized\n+        return super.reduceLanesTemplate(op, Float256Mask.class, (Float256Mask) m);  \/\/ specialized\n@@ -324,1 +344,1 @@\n-        return (long) super.reduceLanesTemplate(op, m);  \/\/ specialized\n+        return (long) super.reduceLanesTemplate(op, Float256Mask.class, (Float256Mask) m);  \/\/ specialized\n@@ -360,0 +380,7 @@\n+    @Override\n+    @ForceInline\n+    public final Float256Mask compare(Comparison op, Vector<Float> v, VectorMask<Float> m) {\n+        return super.compareTemplate(Float256Mask.class, op, v, (Float256Mask) m);\n+    }\n+\n+\n@@ -416,0 +443,1 @@\n+                                    Float256Mask.class,\n@@ -595,10 +623,6 @@\n-            if (VSIZE == species.vectorBitSize()) {\n-                Class<?> dtype = species.elementType();\n-                Class<?> dmtype = species.maskType();\n-                return VectorSupport.convert(VectorSupport.VECTOR_OP_REINTERPRET,\n-                    this.getClass(), ETYPE, VLENGTH,\n-                    dmtype, dtype, VLENGTH,\n-                    this, species,\n-                    Float256Mask::defaultMaskCast);\n-            }\n-            return this.defaultMaskCast(species);\n+\n+            return VectorSupport.convert(VectorSupport.VECTOR_OP_CAST,\n+                this.getClass(), ETYPE, VLENGTH,\n+                species.maskType(), species.elementType(), VLENGTH,\n+                this, species,\n+                (m, s) -> s.maskFactory(m.toArray()).check(s));\n@@ -630,3 +654,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_AND, Float256Mask.class, int.class, VLENGTH,\n-                                             this, m,\n-                                             (m1, m2) -> m1.bOp(m2, (i, a, b) -> a & b));\n+            return VectorSupport.binaryOp(VECTOR_OP_AND, Float256Mask.class, null, int.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a & b));\n@@ -640,3 +664,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_OR, Float256Mask.class, int.class, VLENGTH,\n-                                             this, m,\n-                                             (m1, m2) -> m1.bOp(m2, (i, a, b) -> a | b));\n+            return VectorSupport.binaryOp(VECTOR_OP_OR, Float256Mask.class, null, int.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a | b));\n@@ -650,3 +674,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_XOR, Float256Mask.class, int.class, VLENGTH,\n-                                          this, m,\n-                                          (m1, m2) -> m1.bOp(m2, (i, a, b) -> a ^ b));\n+            return VectorSupport.binaryOp(VECTOR_OP_XOR, Float256Mask.class, null, int.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a ^ b));\n@@ -660,2 +684,2 @@\n-            return VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_TRUECOUNT, Float256Mask.class, int.class, VLENGTH, this,\n-                                                      (m) -> trueCountHelper(((Float256Mask)m).getBits()));\n+            return (int) VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_TRUECOUNT, Float256Mask.class, int.class, VLENGTH, this,\n+                                                      (m) -> trueCountHelper(m.getBits()));\n@@ -667,2 +691,2 @@\n-            return VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_FIRSTTRUE, Float256Mask.class, int.class, VLENGTH, this,\n-                                                      (m) -> firstTrueHelper(((Float256Mask)m).getBits()));\n+            return (int) VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_FIRSTTRUE, Float256Mask.class, int.class, VLENGTH, this,\n+                                                      (m) -> firstTrueHelper(m.getBits()));\n@@ -674,2 +698,12 @@\n-            return VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_LASTTRUE, Float256Mask.class, int.class, VLENGTH, this,\n-                                                      (m) -> lastTrueHelper(((Float256Mask)m).getBits()));\n+            return (int) VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_LASTTRUE, Float256Mask.class, int.class, VLENGTH, this,\n+                                                      (m) -> lastTrueHelper(m.getBits()));\n+        }\n+\n+        @Override\n+        @ForceInline\n+        public long toLong() {\n+            if (length() > Long.SIZE) {\n+                throw new UnsupportedOperationException(\"too many lanes for one long\");\n+            }\n+            return VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_TOLONG, Float256Mask.class, int.class, VLENGTH, this,\n+                                                      (m) -> toLongHelper(m.getBits()));\n@@ -786,0 +820,14 @@\n+    @ForceInline\n+    @Override\n+    final\n+    FloatVector fromArray0(float[] a, int offset, VectorMask<Float> m) {\n+        return super.fromArray0Template(Float256Mask.class, a, offset, (Float256Mask) m);  \/\/ specialize\n+    }\n+\n+    @ForceInline\n+    @Override\n+    final\n+    FloatVector fromArray0(float[] a, int offset, int[] indexMap, int mapOffset, VectorMask<Float> m) {\n+        return super.fromArray0Template(Float256Mask.class, a, offset, indexMap, mapOffset, (Float256Mask) m);\n+    }\n+\n@@ -795,0 +843,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    FloatVector fromByteArray0(byte[] a, int offset, VectorMask<Float> m) {\n+        return super.fromByteArray0Template(Float256Mask.class, a, offset, (Float256Mask) m);  \/\/ specialize\n+    }\n+\n@@ -802,0 +857,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    FloatVector fromByteBuffer0(ByteBuffer bb, int offset, VectorMask<Float> m) {\n+        return super.fromByteBuffer0Template(Float256Mask.class, bb, offset, (Float256Mask) m);  \/\/ specialize\n+    }\n+\n@@ -809,0 +871,15 @@\n+    @ForceInline\n+    @Override\n+    final\n+    void intoArray0(float[] a, int offset, VectorMask<Float> m) {\n+        super.intoArray0Template(Float256Mask.class, a, offset, (Float256Mask) m);\n+    }\n+\n+    @ForceInline\n+    @Override\n+    final\n+    void intoArray0(float[] a, int offset, int[] indexMap, int mapOffset, VectorMask<Float> m) {\n+        super.intoArray0Template(Float256Mask.class, a, offset, indexMap, mapOffset, (Float256Mask) m);\n+    }\n+\n+\n@@ -816,0 +893,15 @@\n+    @ForceInline\n+    @Override\n+    final\n+    void intoByteArray0(byte[] a, int offset, VectorMask<Float> m) {\n+        super.intoByteArray0Template(Float256Mask.class, a, offset, (Float256Mask) m);  \/\/ specialize\n+    }\n+\n+    @ForceInline\n+    @Override\n+    final\n+    void intoByteBuffer0(ByteBuffer bb, int offset, VectorMask<Float> m) {\n+        super.intoByteBuffer0Template(Float256Mask.class, bb, offset, (Float256Mask) m);\n+    }\n+\n+\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/Float256Vector.java","additions":122,"deletions":30,"binary":false,"changes":152,"status":"modified"},{"patch":"@@ -239,2 +239,2 @@\n-    float rOp(float v, FBinOp f) {\n-        return super.rOpTemplate(v, f);  \/\/ specialize\n+    float rOp(float v, VectorMask<Float> m, FBinOp f) {\n+        return super.rOpTemplate(v, m, f);  \/\/ specialize\n@@ -276,0 +276,6 @@\n+    @Override\n+    @ForceInline\n+    public Float512Vector lanewise(Unary op, VectorMask<Float> m) {\n+        return (Float512Vector) super.lanewiseTemplate(op, Float512Mask.class, (Float512Mask) m);  \/\/ specialize\n+    }\n+\n@@ -282,0 +288,6 @@\n+    @Override\n+    @ForceInline\n+    public Float512Vector lanewise(Binary op, Vector<Float> v, VectorMask<Float> m) {\n+        return (Float512Vector) super.lanewiseTemplate(op, Float512Mask.class, v, (Float512Mask) m);  \/\/ specialize\n+    }\n+\n@@ -288,1 +300,1 @@\n-    lanewise(VectorOperators.Ternary op, Vector<Float> v1, Vector<Float> v2) {\n+    lanewise(Ternary op, Vector<Float> v1, Vector<Float> v2) {\n@@ -292,0 +304,8 @@\n+    @Override\n+    @ForceInline\n+    public final\n+    Float512Vector\n+    lanewise(Ternary op, Vector<Float> v1, Vector<Float> v2, VectorMask<Float> m) {\n+        return (Float512Vector) super.lanewiseTemplate(op, Float512Mask.class, v1, v2, (Float512Mask) m);  \/\/ specialize\n+    }\n+\n@@ -311,1 +331,1 @@\n-        return super.reduceLanesTemplate(op, m);  \/\/ specialized\n+        return super.reduceLanesTemplate(op, Float512Mask.class, (Float512Mask) m);  \/\/ specialized\n@@ -324,1 +344,1 @@\n-        return (long) super.reduceLanesTemplate(op, m);  \/\/ specialized\n+        return (long) super.reduceLanesTemplate(op, Float512Mask.class, (Float512Mask) m);  \/\/ specialized\n@@ -360,0 +380,7 @@\n+    @Override\n+    @ForceInline\n+    public final Float512Mask compare(Comparison op, Vector<Float> v, VectorMask<Float> m) {\n+        return super.compareTemplate(Float512Mask.class, op, v, (Float512Mask) m);\n+    }\n+\n+\n@@ -416,0 +443,1 @@\n+                                    Float512Mask.class,\n@@ -611,10 +639,6 @@\n-            if (VSIZE == species.vectorBitSize()) {\n-                Class<?> dtype = species.elementType();\n-                Class<?> dmtype = species.maskType();\n-                return VectorSupport.convert(VectorSupport.VECTOR_OP_REINTERPRET,\n-                    this.getClass(), ETYPE, VLENGTH,\n-                    dmtype, dtype, VLENGTH,\n-                    this, species,\n-                    Float512Mask::defaultMaskCast);\n-            }\n-            return this.defaultMaskCast(species);\n+\n+            return VectorSupport.convert(VectorSupport.VECTOR_OP_CAST,\n+                this.getClass(), ETYPE, VLENGTH,\n+                species.maskType(), species.elementType(), VLENGTH,\n+                this, species,\n+                (m, s) -> s.maskFactory(m.toArray()).check(s));\n@@ -646,3 +670,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_AND, Float512Mask.class, int.class, VLENGTH,\n-                                             this, m,\n-                                             (m1, m2) -> m1.bOp(m2, (i, a, b) -> a & b));\n+            return VectorSupport.binaryOp(VECTOR_OP_AND, Float512Mask.class, null, int.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a & b));\n@@ -656,3 +680,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_OR, Float512Mask.class, int.class, VLENGTH,\n-                                             this, m,\n-                                             (m1, m2) -> m1.bOp(m2, (i, a, b) -> a | b));\n+            return VectorSupport.binaryOp(VECTOR_OP_OR, Float512Mask.class, null, int.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a | b));\n@@ -666,3 +690,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_XOR, Float512Mask.class, int.class, VLENGTH,\n-                                          this, m,\n-                                          (m1, m2) -> m1.bOp(m2, (i, a, b) -> a ^ b));\n+            return VectorSupport.binaryOp(VECTOR_OP_XOR, Float512Mask.class, null, int.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a ^ b));\n@@ -676,2 +700,2 @@\n-            return VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_TRUECOUNT, Float512Mask.class, int.class, VLENGTH, this,\n-                                                      (m) -> trueCountHelper(((Float512Mask)m).getBits()));\n+            return (int) VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_TRUECOUNT, Float512Mask.class, int.class, VLENGTH, this,\n+                                                      (m) -> trueCountHelper(m.getBits()));\n@@ -683,2 +707,2 @@\n-            return VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_FIRSTTRUE, Float512Mask.class, int.class, VLENGTH, this,\n-                                                      (m) -> firstTrueHelper(((Float512Mask)m).getBits()));\n+            return (int) VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_FIRSTTRUE, Float512Mask.class, int.class, VLENGTH, this,\n+                                                      (m) -> firstTrueHelper(m.getBits()));\n@@ -690,2 +714,12 @@\n-            return VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_LASTTRUE, Float512Mask.class, int.class, VLENGTH, this,\n-                                                      (m) -> lastTrueHelper(((Float512Mask)m).getBits()));\n+            return (int) VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_LASTTRUE, Float512Mask.class, int.class, VLENGTH, this,\n+                                                      (m) -> lastTrueHelper(m.getBits()));\n+        }\n+\n+        @Override\n+        @ForceInline\n+        public long toLong() {\n+            if (length() > Long.SIZE) {\n+                throw new UnsupportedOperationException(\"too many lanes for one long\");\n+            }\n+            return VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_TOLONG, Float512Mask.class, int.class, VLENGTH, this,\n+                                                      (m) -> toLongHelper(m.getBits()));\n@@ -802,0 +836,14 @@\n+    @ForceInline\n+    @Override\n+    final\n+    FloatVector fromArray0(float[] a, int offset, VectorMask<Float> m) {\n+        return super.fromArray0Template(Float512Mask.class, a, offset, (Float512Mask) m);  \/\/ specialize\n+    }\n+\n+    @ForceInline\n+    @Override\n+    final\n+    FloatVector fromArray0(float[] a, int offset, int[] indexMap, int mapOffset, VectorMask<Float> m) {\n+        return super.fromArray0Template(Float512Mask.class, a, offset, indexMap, mapOffset, (Float512Mask) m);\n+    }\n+\n@@ -811,0 +859,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    FloatVector fromByteArray0(byte[] a, int offset, VectorMask<Float> m) {\n+        return super.fromByteArray0Template(Float512Mask.class, a, offset, (Float512Mask) m);  \/\/ specialize\n+    }\n+\n@@ -818,0 +873,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    FloatVector fromByteBuffer0(ByteBuffer bb, int offset, VectorMask<Float> m) {\n+        return super.fromByteBuffer0Template(Float512Mask.class, bb, offset, (Float512Mask) m);  \/\/ specialize\n+    }\n+\n@@ -825,0 +887,15 @@\n+    @ForceInline\n+    @Override\n+    final\n+    void intoArray0(float[] a, int offset, VectorMask<Float> m) {\n+        super.intoArray0Template(Float512Mask.class, a, offset, (Float512Mask) m);\n+    }\n+\n+    @ForceInline\n+    @Override\n+    final\n+    void intoArray0(float[] a, int offset, int[] indexMap, int mapOffset, VectorMask<Float> m) {\n+        super.intoArray0Template(Float512Mask.class, a, offset, indexMap, mapOffset, (Float512Mask) m);\n+    }\n+\n+\n@@ -832,0 +909,15 @@\n+    @ForceInline\n+    @Override\n+    final\n+    void intoByteArray0(byte[] a, int offset, VectorMask<Float> m) {\n+        super.intoByteArray0Template(Float512Mask.class, a, offset, (Float512Mask) m);  \/\/ specialize\n+    }\n+\n+    @ForceInline\n+    @Override\n+    final\n+    void intoByteBuffer0(ByteBuffer bb, int offset, VectorMask<Float> m) {\n+        super.intoByteBuffer0Template(Float512Mask.class, bb, offset, (Float512Mask) m);\n+    }\n+\n+\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/Float512Vector.java","additions":122,"deletions":30,"binary":false,"changes":152,"status":"modified"},{"patch":"@@ -239,2 +239,2 @@\n-    float rOp(float v, FBinOp f) {\n-        return super.rOpTemplate(v, f);  \/\/ specialize\n+    float rOp(float v, VectorMask<Float> m, FBinOp f) {\n+        return super.rOpTemplate(v, m, f);  \/\/ specialize\n@@ -276,0 +276,6 @@\n+    @Override\n+    @ForceInline\n+    public Float64Vector lanewise(Unary op, VectorMask<Float> m) {\n+        return (Float64Vector) super.lanewiseTemplate(op, Float64Mask.class, (Float64Mask) m);  \/\/ specialize\n+    }\n+\n@@ -282,0 +288,6 @@\n+    @Override\n+    @ForceInline\n+    public Float64Vector lanewise(Binary op, Vector<Float> v, VectorMask<Float> m) {\n+        return (Float64Vector) super.lanewiseTemplate(op, Float64Mask.class, v, (Float64Mask) m);  \/\/ specialize\n+    }\n+\n@@ -288,1 +300,1 @@\n-    lanewise(VectorOperators.Ternary op, Vector<Float> v1, Vector<Float> v2) {\n+    lanewise(Ternary op, Vector<Float> v1, Vector<Float> v2) {\n@@ -292,0 +304,8 @@\n+    @Override\n+    @ForceInline\n+    public final\n+    Float64Vector\n+    lanewise(Ternary op, Vector<Float> v1, Vector<Float> v2, VectorMask<Float> m) {\n+        return (Float64Vector) super.lanewiseTemplate(op, Float64Mask.class, v1, v2, (Float64Mask) m);  \/\/ specialize\n+    }\n+\n@@ -311,1 +331,1 @@\n-        return super.reduceLanesTemplate(op, m);  \/\/ specialized\n+        return super.reduceLanesTemplate(op, Float64Mask.class, (Float64Mask) m);  \/\/ specialized\n@@ -324,1 +344,1 @@\n-        return (long) super.reduceLanesTemplate(op, m);  \/\/ specialized\n+        return (long) super.reduceLanesTemplate(op, Float64Mask.class, (Float64Mask) m);  \/\/ specialized\n@@ -360,0 +380,7 @@\n+    @Override\n+    @ForceInline\n+    public final Float64Mask compare(Comparison op, Vector<Float> v, VectorMask<Float> m) {\n+        return super.compareTemplate(Float64Mask.class, op, v, (Float64Mask) m);\n+    }\n+\n+\n@@ -416,0 +443,1 @@\n+                                    Float64Mask.class,\n@@ -583,10 +611,6 @@\n-            if (VSIZE == species.vectorBitSize()) {\n-                Class<?> dtype = species.elementType();\n-                Class<?> dmtype = species.maskType();\n-                return VectorSupport.convert(VectorSupport.VECTOR_OP_REINTERPRET,\n-                    this.getClass(), ETYPE, VLENGTH,\n-                    dmtype, dtype, VLENGTH,\n-                    this, species,\n-                    Float64Mask::defaultMaskCast);\n-            }\n-            return this.defaultMaskCast(species);\n+\n+            return VectorSupport.convert(VectorSupport.VECTOR_OP_CAST,\n+                this.getClass(), ETYPE, VLENGTH,\n+                species.maskType(), species.elementType(), VLENGTH,\n+                this, species,\n+                (m, s) -> s.maskFactory(m.toArray()).check(s));\n@@ -618,3 +642,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_AND, Float64Mask.class, int.class, VLENGTH,\n-                                             this, m,\n-                                             (m1, m2) -> m1.bOp(m2, (i, a, b) -> a & b));\n+            return VectorSupport.binaryOp(VECTOR_OP_AND, Float64Mask.class, null, int.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a & b));\n@@ -628,3 +652,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_OR, Float64Mask.class, int.class, VLENGTH,\n-                                             this, m,\n-                                             (m1, m2) -> m1.bOp(m2, (i, a, b) -> a | b));\n+            return VectorSupport.binaryOp(VECTOR_OP_OR, Float64Mask.class, null, int.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a | b));\n@@ -638,3 +662,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_XOR, Float64Mask.class, int.class, VLENGTH,\n-                                          this, m,\n-                                          (m1, m2) -> m1.bOp(m2, (i, a, b) -> a ^ b));\n+            return VectorSupport.binaryOp(VECTOR_OP_XOR, Float64Mask.class, null, int.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a ^ b));\n@@ -648,2 +672,2 @@\n-            return VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_TRUECOUNT, Float64Mask.class, int.class, VLENGTH, this,\n-                                                      (m) -> trueCountHelper(((Float64Mask)m).getBits()));\n+            return (int) VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_TRUECOUNT, Float64Mask.class, int.class, VLENGTH, this,\n+                                                      (m) -> trueCountHelper(m.getBits()));\n@@ -655,2 +679,2 @@\n-            return VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_FIRSTTRUE, Float64Mask.class, int.class, VLENGTH, this,\n-                                                      (m) -> firstTrueHelper(((Float64Mask)m).getBits()));\n+            return (int) VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_FIRSTTRUE, Float64Mask.class, int.class, VLENGTH, this,\n+                                                      (m) -> firstTrueHelper(m.getBits()));\n@@ -662,2 +686,12 @@\n-            return VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_LASTTRUE, Float64Mask.class, int.class, VLENGTH, this,\n-                                                      (m) -> lastTrueHelper(((Float64Mask)m).getBits()));\n+            return (int) VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_LASTTRUE, Float64Mask.class, int.class, VLENGTH, this,\n+                                                      (m) -> lastTrueHelper(m.getBits()));\n+        }\n+\n+        @Override\n+        @ForceInline\n+        public long toLong() {\n+            if (length() > Long.SIZE) {\n+                throw new UnsupportedOperationException(\"too many lanes for one long\");\n+            }\n+            return VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_TOLONG, Float64Mask.class, int.class, VLENGTH, this,\n+                                                      (m) -> toLongHelper(m.getBits()));\n@@ -774,0 +808,14 @@\n+    @ForceInline\n+    @Override\n+    final\n+    FloatVector fromArray0(float[] a, int offset, VectorMask<Float> m) {\n+        return super.fromArray0Template(Float64Mask.class, a, offset, (Float64Mask) m);  \/\/ specialize\n+    }\n+\n+    @ForceInline\n+    @Override\n+    final\n+    FloatVector fromArray0(float[] a, int offset, int[] indexMap, int mapOffset, VectorMask<Float> m) {\n+        return super.fromArray0Template(Float64Mask.class, a, offset, indexMap, mapOffset, (Float64Mask) m);\n+    }\n+\n@@ -783,0 +831,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    FloatVector fromByteArray0(byte[] a, int offset, VectorMask<Float> m) {\n+        return super.fromByteArray0Template(Float64Mask.class, a, offset, (Float64Mask) m);  \/\/ specialize\n+    }\n+\n@@ -790,0 +845,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    FloatVector fromByteBuffer0(ByteBuffer bb, int offset, VectorMask<Float> m) {\n+        return super.fromByteBuffer0Template(Float64Mask.class, bb, offset, (Float64Mask) m);  \/\/ specialize\n+    }\n+\n@@ -797,0 +859,15 @@\n+    @ForceInline\n+    @Override\n+    final\n+    void intoArray0(float[] a, int offset, VectorMask<Float> m) {\n+        super.intoArray0Template(Float64Mask.class, a, offset, (Float64Mask) m);\n+    }\n+\n+    @ForceInline\n+    @Override\n+    final\n+    void intoArray0(float[] a, int offset, int[] indexMap, int mapOffset, VectorMask<Float> m) {\n+        super.intoArray0Template(Float64Mask.class, a, offset, indexMap, mapOffset, (Float64Mask) m);\n+    }\n+\n+\n@@ -804,0 +881,15 @@\n+    @ForceInline\n+    @Override\n+    final\n+    void intoByteArray0(byte[] a, int offset, VectorMask<Float> m) {\n+        super.intoByteArray0Template(Float64Mask.class, a, offset, (Float64Mask) m);  \/\/ specialize\n+    }\n+\n+    @ForceInline\n+    @Override\n+    final\n+    void intoByteBuffer0(ByteBuffer bb, int offset, VectorMask<Float> m) {\n+        super.intoByteBuffer0Template(Float64Mask.class, bb, offset, (Float64Mask) m);\n+    }\n+\n+\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/Float64Vector.java","additions":122,"deletions":30,"binary":false,"changes":152,"status":"modified"},{"patch":"@@ -239,2 +239,2 @@\n-    float rOp(float v, FBinOp f) {\n-        return super.rOpTemplate(v, f);  \/\/ specialize\n+    float rOp(float v, VectorMask<Float> m, FBinOp f) {\n+        return super.rOpTemplate(v, m, f);  \/\/ specialize\n@@ -276,0 +276,6 @@\n+    @Override\n+    @ForceInline\n+    public FloatMaxVector lanewise(Unary op, VectorMask<Float> m) {\n+        return (FloatMaxVector) super.lanewiseTemplate(op, FloatMaxMask.class, (FloatMaxMask) m);  \/\/ specialize\n+    }\n+\n@@ -282,0 +288,6 @@\n+    @Override\n+    @ForceInline\n+    public FloatMaxVector lanewise(Binary op, Vector<Float> v, VectorMask<Float> m) {\n+        return (FloatMaxVector) super.lanewiseTemplate(op, FloatMaxMask.class, v, (FloatMaxMask) m);  \/\/ specialize\n+    }\n+\n@@ -288,1 +300,1 @@\n-    lanewise(VectorOperators.Ternary op, Vector<Float> v1, Vector<Float> v2) {\n+    lanewise(Ternary op, Vector<Float> v1, Vector<Float> v2) {\n@@ -292,0 +304,8 @@\n+    @Override\n+    @ForceInline\n+    public final\n+    FloatMaxVector\n+    lanewise(Ternary op, Vector<Float> v1, Vector<Float> v2, VectorMask<Float> m) {\n+        return (FloatMaxVector) super.lanewiseTemplate(op, FloatMaxMask.class, v1, v2, (FloatMaxMask) m);  \/\/ specialize\n+    }\n+\n@@ -311,1 +331,1 @@\n-        return super.reduceLanesTemplate(op, m);  \/\/ specialized\n+        return super.reduceLanesTemplate(op, FloatMaxMask.class, (FloatMaxMask) m);  \/\/ specialized\n@@ -324,1 +344,1 @@\n-        return (long) super.reduceLanesTemplate(op, m);  \/\/ specialized\n+        return (long) super.reduceLanesTemplate(op, FloatMaxMask.class, (FloatMaxMask) m);  \/\/ specialized\n@@ -360,0 +380,7 @@\n+    @Override\n+    @ForceInline\n+    public final FloatMaxMask compare(Comparison op, Vector<Float> v, VectorMask<Float> m) {\n+        return super.compareTemplate(FloatMaxMask.class, op, v, (FloatMaxMask) m);\n+    }\n+\n+\n@@ -416,0 +443,1 @@\n+                                    FloatMaxMask.class,\n@@ -580,10 +608,6 @@\n-            if (VSIZE == species.vectorBitSize()) {\n-                Class<?> dtype = species.elementType();\n-                Class<?> dmtype = species.maskType();\n-                return VectorSupport.convert(VectorSupport.VECTOR_OP_REINTERPRET,\n-                    this.getClass(), ETYPE, VLENGTH,\n-                    dmtype, dtype, VLENGTH,\n-                    this, species,\n-                    FloatMaxMask::defaultMaskCast);\n-            }\n-            return this.defaultMaskCast(species);\n+\n+            return VectorSupport.convert(VectorSupport.VECTOR_OP_CAST,\n+                this.getClass(), ETYPE, VLENGTH,\n+                species.maskType(), species.elementType(), VLENGTH,\n+                this, species,\n+                (m, s) -> s.maskFactory(m.toArray()).check(s));\n@@ -615,3 +639,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_AND, FloatMaxMask.class, int.class, VLENGTH,\n-                                             this, m,\n-                                             (m1, m2) -> m1.bOp(m2, (i, a, b) -> a & b));\n+            return VectorSupport.binaryOp(VECTOR_OP_AND, FloatMaxMask.class, null, int.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a & b));\n@@ -625,3 +649,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_OR, FloatMaxMask.class, int.class, VLENGTH,\n-                                             this, m,\n-                                             (m1, m2) -> m1.bOp(m2, (i, a, b) -> a | b));\n+            return VectorSupport.binaryOp(VECTOR_OP_OR, FloatMaxMask.class, null, int.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a | b));\n@@ -635,3 +659,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_XOR, FloatMaxMask.class, int.class, VLENGTH,\n-                                          this, m,\n-                                          (m1, m2) -> m1.bOp(m2, (i, a, b) -> a ^ b));\n+            return VectorSupport.binaryOp(VECTOR_OP_XOR, FloatMaxMask.class, null, int.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a ^ b));\n@@ -645,2 +669,2 @@\n-            return VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_TRUECOUNT, FloatMaxMask.class, int.class, VLENGTH, this,\n-                                                      (m) -> trueCountHelper(((FloatMaxMask)m).getBits()));\n+            return (int) VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_TRUECOUNT, FloatMaxMask.class, int.class, VLENGTH, this,\n+                                                      (m) -> trueCountHelper(m.getBits()));\n@@ -652,2 +676,2 @@\n-            return VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_FIRSTTRUE, FloatMaxMask.class, int.class, VLENGTH, this,\n-                                                      (m) -> firstTrueHelper(((FloatMaxMask)m).getBits()));\n+            return (int) VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_FIRSTTRUE, FloatMaxMask.class, int.class, VLENGTH, this,\n+                                                      (m) -> firstTrueHelper(m.getBits()));\n@@ -659,2 +683,12 @@\n-            return VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_LASTTRUE, FloatMaxMask.class, int.class, VLENGTH, this,\n-                                                      (m) -> lastTrueHelper(((FloatMaxMask)m).getBits()));\n+            return (int) VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_LASTTRUE, FloatMaxMask.class, int.class, VLENGTH, this,\n+                                                      (m) -> lastTrueHelper(m.getBits()));\n+        }\n+\n+        @Override\n+        @ForceInline\n+        public long toLong() {\n+            if (length() > Long.SIZE) {\n+                throw new UnsupportedOperationException(\"too many lanes for one long\");\n+            }\n+            return VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_TOLONG, FloatMaxMask.class, int.class, VLENGTH, this,\n+                                                      (m) -> toLongHelper(m.getBits()));\n@@ -771,0 +805,14 @@\n+    @ForceInline\n+    @Override\n+    final\n+    FloatVector fromArray0(float[] a, int offset, VectorMask<Float> m) {\n+        return super.fromArray0Template(FloatMaxMask.class, a, offset, (FloatMaxMask) m);  \/\/ specialize\n+    }\n+\n+    @ForceInline\n+    @Override\n+    final\n+    FloatVector fromArray0(float[] a, int offset, int[] indexMap, int mapOffset, VectorMask<Float> m) {\n+        return super.fromArray0Template(FloatMaxMask.class, a, offset, indexMap, mapOffset, (FloatMaxMask) m);\n+    }\n+\n@@ -780,0 +828,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    FloatVector fromByteArray0(byte[] a, int offset, VectorMask<Float> m) {\n+        return super.fromByteArray0Template(FloatMaxMask.class, a, offset, (FloatMaxMask) m);  \/\/ specialize\n+    }\n+\n@@ -787,0 +842,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    FloatVector fromByteBuffer0(ByteBuffer bb, int offset, VectorMask<Float> m) {\n+        return super.fromByteBuffer0Template(FloatMaxMask.class, bb, offset, (FloatMaxMask) m);  \/\/ specialize\n+    }\n+\n@@ -794,0 +856,15 @@\n+    @ForceInline\n+    @Override\n+    final\n+    void intoArray0(float[] a, int offset, VectorMask<Float> m) {\n+        super.intoArray0Template(FloatMaxMask.class, a, offset, (FloatMaxMask) m);\n+    }\n+\n+    @ForceInline\n+    @Override\n+    final\n+    void intoArray0(float[] a, int offset, int[] indexMap, int mapOffset, VectorMask<Float> m) {\n+        super.intoArray0Template(FloatMaxMask.class, a, offset, indexMap, mapOffset, (FloatMaxMask) m);\n+    }\n+\n+\n@@ -801,0 +878,15 @@\n+    @ForceInline\n+    @Override\n+    final\n+    void intoByteArray0(byte[] a, int offset, VectorMask<Float> m) {\n+        super.intoByteArray0Template(FloatMaxMask.class, a, offset, (FloatMaxMask) m);  \/\/ specialize\n+    }\n+\n+    @ForceInline\n+    @Override\n+    final\n+    void intoByteBuffer0(ByteBuffer bb, int offset, VectorMask<Float> m) {\n+        super.intoByteBuffer0Template(FloatMaxMask.class, bb, offset, (FloatMaxMask) m);\n+    }\n+\n+\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/FloatMaxVector.java","additions":122,"deletions":30,"binary":false,"changes":152,"status":"modified"},{"patch":"@@ -32,1 +32,0 @@\n-import java.util.function.BinaryOperator;\n@@ -176,0 +175,3 @@\n+        if (m == null) {\n+            return uOpTemplate(f);\n+        }\n@@ -219,0 +221,3 @@\n+        if (m == null) {\n+            return bOpTemplate(o, f);\n+        }\n@@ -268,0 +273,3 @@\n+        if (m == null) {\n+            return tOpTemplate(o1, o2, f);\n+        }\n@@ -283,1 +291,16 @@\n-    float rOp(float v, FBinOp f);\n+    float rOp(float v, VectorMask<Float> m, FBinOp f);\n+\n+    @ForceInline\n+    final\n+    float rOpTemplate(float v, VectorMask<Float> m, FBinOp f) {\n+        if (m == null) {\n+            return rOpTemplate(v, f);\n+        }\n+        float[] vec = vec();\n+        boolean[] mbits = ((AbstractMask<Float>)m).getBits();\n+        for (int i = 0; i < vec.length; i++) {\n+            v = mbits[i] ? f.apply(i, v, vec[i]) : v;\n+        }\n+        return v;\n+    }\n+\n@@ -543,42 +566,3 @@\n-            opc, getClass(), float.class, length(),\n-            this,\n-            UN_IMPL.find(op, opc, (opc_) -> {\n-              switch (opc_) {\n-                case VECTOR_OP_NEG: return v0 ->\n-                        v0.uOp((i, a) -> (float) -a);\n-                case VECTOR_OP_ABS: return v0 ->\n-                        v0.uOp((i, a) -> (float) Math.abs(a));\n-                case VECTOR_OP_SIN: return v0 ->\n-                        v0.uOp((i, a) -> (float) Math.sin(a));\n-                case VECTOR_OP_COS: return v0 ->\n-                        v0.uOp((i, a) -> (float) Math.cos(a));\n-                case VECTOR_OP_TAN: return v0 ->\n-                        v0.uOp((i, a) -> (float) Math.tan(a));\n-                case VECTOR_OP_ASIN: return v0 ->\n-                        v0.uOp((i, a) -> (float) Math.asin(a));\n-                case VECTOR_OP_ACOS: return v0 ->\n-                        v0.uOp((i, a) -> (float) Math.acos(a));\n-                case VECTOR_OP_ATAN: return v0 ->\n-                        v0.uOp((i, a) -> (float) Math.atan(a));\n-                case VECTOR_OP_EXP: return v0 ->\n-                        v0.uOp((i, a) -> (float) Math.exp(a));\n-                case VECTOR_OP_LOG: return v0 ->\n-                        v0.uOp((i, a) -> (float) Math.log(a));\n-                case VECTOR_OP_LOG10: return v0 ->\n-                        v0.uOp((i, a) -> (float) Math.log10(a));\n-                case VECTOR_OP_SQRT: return v0 ->\n-                        v0.uOp((i, a) -> (float) Math.sqrt(a));\n-                case VECTOR_OP_CBRT: return v0 ->\n-                        v0.uOp((i, a) -> (float) Math.cbrt(a));\n-                case VECTOR_OP_SINH: return v0 ->\n-                        v0.uOp((i, a) -> (float) Math.sinh(a));\n-                case VECTOR_OP_COSH: return v0 ->\n-                        v0.uOp((i, a) -> (float) Math.cosh(a));\n-                case VECTOR_OP_TANH: return v0 ->\n-                        v0.uOp((i, a) -> (float) Math.tanh(a));\n-                case VECTOR_OP_EXPM1: return v0 ->\n-                        v0.uOp((i, a) -> (float) Math.expm1(a));\n-                case VECTOR_OP_LOG1P: return v0 ->\n-                        v0.uOp((i, a) -> (float) Math.log1p(a));\n-                default: return null;\n-              }}));\n+            opc, getClass(), null, float.class, length(),\n+            this, null,\n+            UN_IMPL.find(op, opc, FloatVector::unaryOperations));\n@@ -586,3 +570,0 @@\n-    private static final\n-    ImplCache<Unary,UnaryOperator<FloatVector>> UN_IMPL\n-        = new ImplCache<>(Unary.class, FloatVector.class);\n@@ -593,2 +574,2 @@\n-    @ForceInline\n-    public final\n+    @Override\n+    public abstract\n@@ -596,2 +577,63 @@\n-                                  VectorMask<Float> m) {\n-        return blend(lanewise(op), m);\n+                                  VectorMask<Float> m);\n+    @ForceInline\n+    final\n+    FloatVector lanewiseTemplate(VectorOperators.Unary op,\n+                                          Class<? extends VectorMask<Float>> maskClass,\n+                                          VectorMask<Float> m) {\n+        m.check(maskClass, this);\n+        if (opKind(op, VO_SPECIAL)) {\n+            if (op == ZOMO) {\n+                return blend(broadcast(-1), compare(NE, 0, m));\n+            }\n+        }\n+        int opc = opCode(op);\n+        return VectorSupport.unaryOp(\n+            opc, getClass(), maskClass, float.class, length(),\n+            this, m,\n+            UN_IMPL.find(op, opc, FloatVector::unaryOperations));\n+    }\n+\n+    private static final\n+    ImplCache<Unary, UnaryOperation<FloatVector, VectorMask<Float>>>\n+        UN_IMPL = new ImplCache<>(Unary.class, FloatVector.class);\n+\n+    private static UnaryOperation<FloatVector, VectorMask<Float>> unaryOperations(int opc_) {\n+        switch (opc_) {\n+            case VECTOR_OP_NEG: return (v0, m) ->\n+                    v0.uOp(m, (i, a) -> (float) -a);\n+            case VECTOR_OP_ABS: return (v0, m) ->\n+                    v0.uOp(m, (i, a) -> (float) Math.abs(a));\n+            case VECTOR_OP_SIN: return (v0, m) ->\n+                    v0.uOp(m, (i, a) -> (float) Math.sin(a));\n+            case VECTOR_OP_COS: return (v0, m) ->\n+                    v0.uOp(m, (i, a) -> (float) Math.cos(a));\n+            case VECTOR_OP_TAN: return (v0, m) ->\n+                    v0.uOp(m, (i, a) -> (float) Math.tan(a));\n+            case VECTOR_OP_ASIN: return (v0, m) ->\n+                    v0.uOp(m, (i, a) -> (float) Math.asin(a));\n+            case VECTOR_OP_ACOS: return (v0, m) ->\n+                    v0.uOp(m, (i, a) -> (float) Math.acos(a));\n+            case VECTOR_OP_ATAN: return (v0, m) ->\n+                    v0.uOp(m, (i, a) -> (float) Math.atan(a));\n+            case VECTOR_OP_EXP: return (v0, m) ->\n+                    v0.uOp(m, (i, a) -> (float) Math.exp(a));\n+            case VECTOR_OP_LOG: return (v0, m) ->\n+                    v0.uOp(m, (i, a) -> (float) Math.log(a));\n+            case VECTOR_OP_LOG10: return (v0, m) ->\n+                    v0.uOp(m, (i, a) -> (float) Math.log10(a));\n+            case VECTOR_OP_SQRT: return (v0, m) ->\n+                    v0.uOp(m, (i, a) -> (float) Math.sqrt(a));\n+            case VECTOR_OP_CBRT: return (v0, m) ->\n+                    v0.uOp(m, (i, a) -> (float) Math.cbrt(a));\n+            case VECTOR_OP_SINH: return (v0, m) ->\n+                    v0.uOp(m, (i, a) -> (float) Math.sinh(a));\n+            case VECTOR_OP_COSH: return (v0, m) ->\n+                    v0.uOp(m, (i, a) -> (float) Math.cosh(a));\n+            case VECTOR_OP_TANH: return (v0, m) ->\n+                    v0.uOp(m, (i, a) -> (float) Math.tanh(a));\n+            case VECTOR_OP_EXPM1: return (v0, m) ->\n+                    v0.uOp(m, (i, a) -> (float) Math.expm1(a));\n+            case VECTOR_OP_LOG1P: return (v0, m) ->\n+                    v0.uOp(m, (i, a) -> (float) Math.log1p(a));\n+            default: return null;\n+        }\n@@ -617,0 +659,1 @@\n+\n@@ -630,0 +673,1 @@\n+\n@@ -632,24 +676,3 @@\n-            opc, getClass(), float.class, length(),\n-            this, that,\n-            BIN_IMPL.find(op, opc, (opc_) -> {\n-              switch (opc_) {\n-                case VECTOR_OP_ADD: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (float)(a + b));\n-                case VECTOR_OP_SUB: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (float)(a - b));\n-                case VECTOR_OP_MUL: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (float)(a * b));\n-                case VECTOR_OP_DIV: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (float)(a \/ b));\n-                case VECTOR_OP_MAX: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (float)Math.max(a, b));\n-                case VECTOR_OP_MIN: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (float)Math.min(a, b));\n-                case VECTOR_OP_ATAN2: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (float) Math.atan2(a, b));\n-                case VECTOR_OP_POW: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (float) Math.pow(a, b));\n-                case VECTOR_OP_HYPOT: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (float) Math.hypot(a, b));\n-                default: return null;\n-                }}));\n+            opc, getClass(), null, float.class, length(),\n+            this, that, null,\n+            BIN_IMPL.find(op, opc, FloatVector::binaryOperations));\n@@ -657,3 +680,0 @@\n-    private static final\n-    ImplCache<Binary,BinaryOperator<FloatVector>> BIN_IMPL\n-        = new ImplCache<>(Binary.class, FloatVector.class);\n@@ -665,2 +685,2 @@\n-    @ForceInline\n-    public final\n+    @Override\n+    public abstract\n@@ -669,2 +689,21 @@\n-                                  VectorMask<Float> m) {\n-        return blend(lanewise(op, v), m);\n+                                  VectorMask<Float> m);\n+    @ForceInline\n+    final\n+    FloatVector lanewiseTemplate(VectorOperators.Binary op,\n+                                          Class<? extends VectorMask<Float>> maskClass,\n+                                          Vector<Float> v, VectorMask<Float> m) {\n+        FloatVector that = (FloatVector) v;\n+        that.check(this);\n+        m.check(maskClass, this);\n+\n+        if (opKind(op, VO_SPECIAL )) {\n+            if (op == FIRST_NONZERO) {\n+                return blend(lanewise(op, v), m);\n+            }\n+        }\n+\n+        int opc = opCode(op);\n+        return VectorSupport.binaryOp(\n+            opc, getClass(), maskClass, float.class, length(),\n+            this, that, m,\n+            BIN_IMPL.find(op, opc, FloatVector::binaryOperations));\n@@ -672,0 +711,31 @@\n+\n+    private static final\n+    ImplCache<Binary, BinaryOperation<FloatVector, VectorMask<Float>>>\n+        BIN_IMPL = new ImplCache<>(Binary.class, FloatVector.class);\n+\n+    private static BinaryOperation<FloatVector, VectorMask<Float>> binaryOperations(int opc_) {\n+        switch (opc_) {\n+            case VECTOR_OP_ADD: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> (float)(a + b));\n+            case VECTOR_OP_SUB: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> (float)(a - b));\n+            case VECTOR_OP_MUL: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> (float)(a * b));\n+            case VECTOR_OP_DIV: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> (float)(a \/ b));\n+            case VECTOR_OP_MAX: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> (float)Math.max(a, b));\n+            case VECTOR_OP_MIN: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> (float)Math.min(a, b));\n+            case VECTOR_OP_OR: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> fromBits(toBits(a) | toBits(b)));\n+            case VECTOR_OP_ATAN2: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> (float) Math.atan2(a, b));\n+            case VECTOR_OP_POW: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> (float) Math.pow(a, b));\n+            case VECTOR_OP_HYPOT: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> (float) Math.hypot(a, b));\n+            default: return null;\n+        }\n+    }\n+\n@@ -728,1 +798,1 @@\n-        return blend(lanewise(op, e), m);\n+        return lanewise(op, broadcast(e), m);\n@@ -746,2 +816,1 @@\n-        if ((long)e1 != e\n-            ) {\n+        if ((long)e1 != e) {\n@@ -767,1 +836,5 @@\n-        return blend(lanewise(op, e), m);\n+        float e1 = (float) e;\n+        if ((long)e1 != e) {\n+            vspecies().checkValue(e);  \/\/ for exception\n+        }\n+        return lanewise(op, e1, m);\n@@ -809,8 +882,3 @@\n-            opc, getClass(), float.class, length(),\n-            this, that, tother,\n-            TERN_IMPL.find(op, opc, (opc_) -> {\n-              switch (opc_) {\n-                case VECTOR_OP_FMA: return (v0, v1_, v2_) ->\n-                        v0.tOp(v1_, v2_, (i, a, b, c) -> Math.fma(a, b, c));\n-                default: return null;\n-                }}));\n+            opc, getClass(), null, float.class, length(),\n+            this, that, tother, null,\n+            TERN_IMPL.find(op, opc, FloatVector::ternaryOperations));\n@@ -818,3 +886,0 @@\n-    private static final\n-    ImplCache<Ternary,TernaryOperation<FloatVector>> TERN_IMPL\n-        = new ImplCache<>(Ternary.class, FloatVector.class);\n@@ -828,2 +893,2 @@\n-    @ForceInline\n-    public final\n+    @Override\n+    public abstract\n@@ -833,2 +898,34 @@\n-                                  VectorMask<Float> m) {\n-        return blend(lanewise(op, v1, v2), m);\n+                                  VectorMask<Float> m);\n+    @ForceInline\n+    final\n+    FloatVector lanewiseTemplate(VectorOperators.Ternary op,\n+                                          Class<? extends VectorMask<Float>> maskClass,\n+                                          Vector<Float> v1,\n+                                          Vector<Float> v2,\n+                                          VectorMask<Float> m) {\n+        FloatVector that = (FloatVector) v1;\n+        FloatVector tother = (FloatVector) v2;\n+        \/\/ It's a word: https:\/\/www.dictionary.com\/browse\/tother\n+        \/\/ See also Chapter 11 of Dickens, Our Mutual Friend:\n+        \/\/ \"Totherest Governor,\" replied Mr Riderhood...\n+        that.check(this);\n+        tother.check(this);\n+        m.check(maskClass, this);\n+\n+        int opc = opCode(op);\n+        return VectorSupport.ternaryOp(\n+            opc, getClass(), maskClass, float.class, length(),\n+            this, that, tother, m,\n+            TERN_IMPL.find(op, opc, FloatVector::ternaryOperations));\n+    }\n+\n+    private static final\n+    ImplCache<Ternary, TernaryOperation<FloatVector, VectorMask<Float>>>\n+        TERN_IMPL = new ImplCache<>(Ternary.class, FloatVector.class);\n+\n+    private static TernaryOperation<FloatVector, VectorMask<Float>> ternaryOperations(int opc_) {\n+        switch (opc_) {\n+            case VECTOR_OP_FMA: return (v0, v1_, v2_, m) ->\n+                    v0.tOp(v1_, v2_, m, (i, a, b, c) -> Math.fma(a, b, c));\n+            default: return null;\n+        }\n@@ -891,1 +988,1 @@\n-        return blend(lanewise(op, e1, e2), m);\n+        return lanewise(op, broadcast(e1), broadcast(e2), m);\n@@ -949,1 +1046,1 @@\n-        return blend(lanewise(op, v1, e2), m);\n+        return lanewise(op, v1, broadcast(e2), m);\n@@ -1006,1 +1103,1 @@\n-        return blend(lanewise(op, e1, v2), m);\n+        return lanewise(op, broadcast(e1), v2, m);\n@@ -1662,2 +1759,0 @@\n-        Objects.requireNonNull(v);\n-        FloatSpecies vsp = vspecies();\n@@ -1669,2 +1764,2 @@\n-            this, that,\n-            (cond, v0, v1) -> {\n+            this, that, null,\n+            (cond, v0, v1, m1) -> {\n@@ -1680,0 +1775,22 @@\n+    \/*package-private*\/\n+    @ForceInline\n+    final\n+    <M extends VectorMask<Float>>\n+    M compareTemplate(Class<M> maskType, Comparison op, Vector<Float> v, M m) {\n+        FloatVector that = (FloatVector) v;\n+        that.check(this);\n+        m.check(maskType, this);\n+        int opc = opCode(op);\n+        return VectorSupport.compare(\n+            opc, getClass(), maskType, float.class, length(),\n+            this, that, m,\n+            (cond, v0, v1, m1) -> {\n+                AbstractMask<Float> cmpM\n+                    = v0.bTest(cond, v1, (cond_, i, a, b)\n+                               -> compareWithOp(cond, a, b));\n+                @SuppressWarnings(\"unchecked\")\n+                M m2 = (M) cmpM.and(m1);\n+                return m2;\n+            });\n+    }\n+\n@@ -1693,12 +1810,0 @@\n-    \/**\n-     * {@inheritDoc} <!--workaround-->\n-     *\/\n-    @Override\n-    @ForceInline\n-    public final\n-    VectorMask<Float> compare(VectorOperators.Comparison op,\n-                                  Vector<Float> v,\n-                                  VectorMask<Float> m) {\n-        return compare(op, v).and(m);\n-    }\n-\n@@ -1763,1 +1868,1 @@\n-        return compare(op, e).and(m);\n+        return compare(op, broadcast(e), m);\n@@ -2014,3 +2119,3 @@\n-            getClass(), shuffletype, float.class, length(),\n-            this, shuffle,\n-            (v1, s_) -> v1.uOp((i, a) -> {\n+            getClass(), shuffletype, null, float.class, length(),\n+            this, shuffle, null,\n+            (v1, s_, m_) -> v1.uOp((i, a) -> {\n@@ -2033,1 +2138,1 @@\n-    <S extends VectorShuffle<Float>>\n+    <S extends VectorShuffle<Float>, M extends VectorMask<Float>>\n@@ -2035,0 +2140,1 @@\n+                                           Class<M> masktype,\n@@ -2036,9 +2142,3 @@\n-                                           VectorMask<Float> m) {\n-        FloatVector unmasked =\n-            VectorSupport.rearrangeOp(\n-                getClass(), shuffletype, float.class, length(),\n-                this, shuffle,\n-                (v1, s_) -> v1.uOp((i, a) -> {\n-                    int ei = s_.laneSource(i);\n-                    return ei < 0 ? 0 : v1.lane(ei);\n-                }));\n+                                           M m) {\n+\n+        m.check(masktype, this);\n@@ -2050,1 +2150,7 @@\n-        return broadcast((float)0).blend(unmasked, m);\n+        return VectorSupport.rearrangeOp(\n+                   getClass(), shuffletype, masktype, float.class, length(),\n+                   this, shuffle, m,\n+                   (v1, s_, m_) -> v1.uOp((i, a) -> {\n+                        int ei = s_.laneSource(i);\n+                        return ei < 0  || !m_.laneIsSet(i) ? 0 : v1.lane(ei);\n+                   }));\n@@ -2073,3 +2179,3 @@\n-                getClass(), shuffletype, float.class, length(),\n-                this, ws,\n-                (v0, s_) -> v0.uOp((i, a) -> {\n+                getClass(), shuffletype, null, float.class, length(),\n+                this, ws, null,\n+                (v0, s_, m_) -> v0.uOp((i, a) -> {\n@@ -2081,3 +2187,3 @@\n-                getClass(), shuffletype, float.class, length(),\n-                v, ws,\n-                (v1, s_) -> v1.uOp((i, a) -> {\n+                getClass(), shuffletype, null, float.class, length(),\n+                v, ws, null,\n+                (v1, s_, m_) -> v1.uOp((i, a) -> {\n@@ -2332,0 +2438,1 @@\n+                               Class<? extends VectorMask<Float>> maskClass,\n@@ -2333,2 +2440,10 @@\n-        FloatVector v = reduceIdentityVector(op).blend(this, m);\n-        return v.reduceLanesTemplate(op);\n+        m.check(maskClass, this);\n+        if (op == FIRST_NONZERO) {\n+            FloatVector v = reduceIdentityVector(op).blend(this, m);\n+            return v.reduceLanesTemplate(op);\n+        }\n+        int opc = opCode(op);\n+        return fromBits(VectorSupport.reductionCoerced(\n+            opc, getClass(), maskClass, float.class, length(),\n+            this, m,\n+            REDUCE_IMPL.find(op, opc, FloatVector::reductionOperations)));\n@@ -2349,14 +2464,3 @@\n-            opc, getClass(), float.class, length(),\n-            this,\n-            REDUCE_IMPL.find(op, opc, (opc_) -> {\n-              switch (opc_) {\n-              case VECTOR_OP_ADD: return v ->\n-                      toBits(v.rOp((float)0, (i, a, b) -> (float)(a + b)));\n-              case VECTOR_OP_MUL: return v ->\n-                      toBits(v.rOp((float)1, (i, a, b) -> (float)(a * b)));\n-              case VECTOR_OP_MIN: return v ->\n-                      toBits(v.rOp(MAX_OR_INF, (i, a, b) -> (float) Math.min(a, b)));\n-              case VECTOR_OP_MAX: return v ->\n-                      toBits(v.rOp(MIN_OR_INF, (i, a, b) -> (float) Math.max(a, b)));\n-              default: return null;\n-              }})));\n+            opc, getClass(), null, float.class, length(),\n+            this, null,\n+            REDUCE_IMPL.find(op, opc, FloatVector::reductionOperations)));\n@@ -2364,0 +2468,1 @@\n+\n@@ -2365,2 +2470,16 @@\n-    ImplCache<Associative,Function<FloatVector,Long>> REDUCE_IMPL\n-        = new ImplCache<>(Associative.class, FloatVector.class);\n+    ImplCache<Associative, ReductionOperation<FloatVector, VectorMask<Float>>>\n+        REDUCE_IMPL = new ImplCache<>(Associative.class, FloatVector.class);\n+\n+    private static ReductionOperation<FloatVector, VectorMask<Float>> reductionOperations(int opc_) {\n+        switch (opc_) {\n+            case VECTOR_OP_ADD: return (v, m) ->\n+                    toBits(v.rOp((float)0, m, (i, a, b) -> (float)(a + b)));\n+            case VECTOR_OP_MUL: return (v, m) ->\n+                    toBits(v.rOp((float)1, m, (i, a, b) -> (float)(a * b)));\n+            case VECTOR_OP_MIN: return (v, m) ->\n+                    toBits(v.rOp(MAX_OR_INF, m, (i, a, b) -> (float) Math.min(a, b)));\n+            case VECTOR_OP_MAX: return (v, m) ->\n+                    toBits(v.rOp(MIN_OR_INF, m, (i, a, b) -> (float) Math.max(a, b)));\n+            default: return null;\n+        }\n+    }\n@@ -2576,3 +2695,1 @@\n-            FloatVector zero = vsp.zero();\n-            FloatVector v = zero.fromByteArray0(a, offset);\n-            return zero.blend(v.maybeSwap(bo), m);\n+            return vsp.dummyVector().fromByteArray0(a, offset, m).maybeSwap(bo);\n@@ -2640,2 +2757,1 @@\n-            FloatVector zero = vsp.zero();\n-            return zero.blend(zero.fromArray0(a, offset), m);\n+            return vsp.dummyVector().fromArray0(a, offset, m);\n@@ -2699,3 +2815,3 @@\n-            vectorType, float.class, vsp.laneCount(),\n-            IntVector.species(vsp.indexShape()).vectorType(),\n-            a, ARRAY_BASE, vix,\n+            vectorType, null, float.class, vsp.laneCount(),\n+            isp.vectorType(),\n+            a, ARRAY_BASE, vix, null,\n@@ -2703,1 +2819,1 @@\n-            (float[] c, int idx, int[] iMap, int idy, FloatSpecies s) ->\n+            (c, idx, iMap, idy, s, vm) ->\n@@ -2705,1 +2821,1 @@\n-        }\n+    }\n@@ -2753,1 +2869,0 @@\n-            \/\/ FIXME: Cannot vectorize yet, if there's a mask.\n@@ -2755,1 +2870,1 @@\n-            return vsp.vOp(m, n -> a[offset + indexMap[mapOffset + n]]);\n+            return vsp.dummyVector().fromArray0(a, offset, indexMap, mapOffset, m);\n@@ -2849,3 +2964,1 @@\n-            FloatVector zero = vsp.zero();\n-            FloatVector v = zero.fromByteBuffer0(bb, offset);\n-            return zero.blend(v.maybeSwap(bo), m);\n+            return vsp.dummyVector().fromByteBuffer0(bb, offset, m).maybeSwap(bo);\n@@ -2923,1 +3036,0 @@\n-            \/\/ FIXME: optimize\n@@ -2926,1 +3038,1 @@\n-            stOp(a, offset, m, (arr, off, i, v) -> arr[off+i] = v);\n+            intoArray0(a, offset, m);\n@@ -2970,1 +3082,1 @@\n-            vsp.vectorType(), vsp.elementType(), vsp.laneCount(),\n+            vsp.vectorType(), null, vsp.elementType(), vsp.laneCount(),\n@@ -2973,1 +3085,1 @@\n-            this,\n+            this, null,\n@@ -2975,1 +3087,1 @@\n-            (arr, off, v, map, mo)\n+            (arr, off, v, map, mo, vm)\n@@ -3022,6 +3134,1 @@\n-            \/\/ FIXME: Cannot vectorize yet, if there's a mask.\n-            stOp(a, offset, m,\n-                 (arr, off, i, e) -> {\n-                     int j = indexMap[mapOffset + i];\n-                     arr[off + j] = e;\n-                 });\n+            intoArray0(a, offset, indexMap, mapOffset, m);\n@@ -3057,1 +3164,0 @@\n-            \/\/ FIXME: optimize\n@@ -3060,3 +3166,1 @@\n-            ByteBuffer wb = wrapper(a, bo);\n-            this.stOp(wb, offset, m,\n-                    (wb_, o, i, e) -> wb_.putFloat(o + i * 4, e));\n+            maybeSwap(bo).intoByteArray0(a, offset, m);\n@@ -3074,1 +3178,1 @@\n-        if (bb.isReadOnly()) {\n+        if (ScopedMemoryAccess.isReadOnly(bb)) {\n@@ -3093,1 +3197,0 @@\n-            \/\/ FIXME: optimize\n@@ -3099,3 +3202,1 @@\n-            ByteBuffer wb = wrapper(bb, bo);\n-            this.stOp(wb, offset, m,\n-                    (wb_, o, i, e) -> wb_.putFloat(o + i * 4, e));\n+            maybeSwap(bo).intoByteBuffer0(bb, offset, m);\n@@ -3139,0 +3240,51 @@\n+    \/*package-private*\/\n+    abstract\n+    FloatVector fromArray0(float[] a, int offset, VectorMask<Float> m);\n+    @ForceInline\n+    final\n+    <M extends VectorMask<Float>>\n+    FloatVector fromArray0Template(Class<M> maskClass, float[] a, int offset, M m) {\n+        m.check(species());\n+        FloatSpecies vsp = vspecies();\n+        return VectorSupport.loadMasked(\n+            vsp.vectorType(), maskClass, vsp.elementType(), vsp.laneCount(),\n+            a, arrayAddress(a, offset), m,\n+            a, offset, vsp,\n+            (arr, off, s, vm) -> s.ldOp(arr, off, vm,\n+                                        (arr_, off_, i) -> arr_[off_ + i]));\n+    }\n+\n+    \/*package-private*\/\n+    abstract\n+    FloatVector fromArray0(float[] a, int offset,\n+                                    int[] indexMap, int mapOffset,\n+                                    VectorMask<Float> m);\n+    @ForceInline\n+    final\n+    <M extends VectorMask<Float>>\n+    FloatVector fromArray0Template(Class<M> maskClass, float[] a, int offset,\n+                                            int[] indexMap, int mapOffset, M m) {\n+        FloatSpecies vsp = vspecies();\n+        IntVector.IntSpecies isp = IntVector.species(vsp.indexShape());\n+        Objects.requireNonNull(a);\n+        Objects.requireNonNull(indexMap);\n+        m.check(vsp);\n+        Class<? extends FloatVector> vectorType = vsp.vectorType();\n+\n+        \/\/ Index vector: vix[0:n] = k -> offset + indexMap[mapOffset + k]\n+        IntVector vix = IntVector\n+            .fromArray(isp, indexMap, mapOffset)\n+            .add(offset);\n+\n+        \/\/ FIXME: Check index under mask controlling.\n+        vix = VectorIntrinsics.checkIndex(vix, a.length);\n+\n+        return VectorSupport.loadWithMap(\n+            vectorType, maskClass, float.class, vsp.laneCount(),\n+            isp.vectorType(),\n+            a, ARRAY_BASE, vix, m,\n+            a, offset, indexMap, mapOffset, vsp,\n+            (c, idx, iMap, idy, s, vm) ->\n+            s.vOp(vm, n -> c[idx + iMap[idy+n]]));\n+    }\n+\n@@ -3159,0 +3311,19 @@\n+    abstract\n+    FloatVector fromByteArray0(byte[] a, int offset, VectorMask<Float> m);\n+    @ForceInline\n+    final\n+    <M extends VectorMask<Float>>\n+    FloatVector fromByteArray0Template(Class<M> maskClass, byte[] a, int offset, M m) {\n+        FloatSpecies vsp = vspecies();\n+        m.check(vsp);\n+        return VectorSupport.loadMasked(\n+            vsp.vectorType(), maskClass, vsp.elementType(), vsp.laneCount(),\n+            a, byteArrayAddress(a, offset), m,\n+            a, offset, vsp,\n+            (arr, off, s, vm) -> {\n+                ByteBuffer wb = wrapper(arr, NATIVE_ENDIAN);\n+                return s.ldOp(wb, off, vm,\n+                        (wb_, o, i) -> wb_.getFloat(o + i * 4));\n+            });\n+    }\n+\n@@ -3175,0 +3346,18 @@\n+    abstract\n+    FloatVector fromByteBuffer0(ByteBuffer bb, int offset, VectorMask<Float> m);\n+    @ForceInline\n+    final\n+    <M extends VectorMask<Float>>\n+    FloatVector fromByteBuffer0Template(Class<M> maskClass, ByteBuffer bb, int offset, M m) {\n+        FloatSpecies vsp = vspecies();\n+        m.check(vsp);\n+        return ScopedMemoryAccess.loadFromByteBufferMasked(\n+                vsp.vectorType(), maskClass, vsp.elementType(), vsp.laneCount(),\n+                bb, offset, m, vsp,\n+                (buf, off, s, vm) -> {\n+                    ByteBuffer wb = wrapper(buf, NATIVE_ENDIAN);\n+                    return s.ldOp(wb, off, vm,\n+                            (wb_, o, i) -> wb_.getFloat(o + i * 4));\n+                });\n+    }\n+\n@@ -3194,0 +3383,52 @@\n+    abstract\n+    void intoArray0(float[] a, int offset, VectorMask<Float> m);\n+    @ForceInline\n+    final\n+    <M extends VectorMask<Float>>\n+    void intoArray0Template(Class<M> maskClass, float[] a, int offset, M m) {\n+        m.check(species());\n+        FloatSpecies vsp = vspecies();\n+        VectorSupport.storeMasked(\n+            vsp.vectorType(), maskClass, vsp.elementType(), vsp.laneCount(),\n+            a, arrayAddress(a, offset),\n+            this, m, a, offset,\n+            (arr, off, v, vm)\n+            -> v.stOp(arr, off, vm,\n+                      (arr_, off_, i, e) -> arr_[off_ + i] = e));\n+    }\n+\n+    abstract\n+    void intoArray0(float[] a, int offset,\n+                    int[] indexMap, int mapOffset,\n+                    VectorMask<Float> m);\n+    @ForceInline\n+    final\n+    <M extends VectorMask<Float>>\n+    void intoArray0Template(Class<M> maskClass, float[] a, int offset,\n+                            int[] indexMap, int mapOffset, M m) {\n+        m.check(species());\n+        FloatSpecies vsp = vspecies();\n+        IntVector.IntSpecies isp = IntVector.species(vsp.indexShape());\n+        \/\/ Index vector: vix[0:n] = i -> offset + indexMap[mo + i]\n+        IntVector vix = IntVector\n+            .fromArray(isp, indexMap, mapOffset)\n+            .add(offset);\n+\n+        \/\/ FIXME: Check index under mask controlling.\n+        vix = VectorIntrinsics.checkIndex(vix, a.length);\n+\n+        VectorSupport.storeWithMap(\n+            vsp.vectorType(), maskClass, vsp.elementType(), vsp.laneCount(),\n+            isp.vectorType(),\n+            a, arrayAddress(a, 0), vix,\n+            this, m,\n+            a, offset, indexMap, mapOffset,\n+            (arr, off, v, map, mo, vm)\n+            -> v.stOp(arr, off, vm,\n+                      (arr_, off_, i, e) -> {\n+                          int j = map[mo + i];\n+                          arr[off + j] = e;\n+                      }));\n+    }\n+\n+\n@@ -3211,0 +3452,19 @@\n+    abstract\n+    void intoByteArray0(byte[] a, int offset, VectorMask<Float> m);\n+    @ForceInline\n+    final\n+    <M extends VectorMask<Float>>\n+    void intoByteArray0Template(Class<M> maskClass, byte[] a, int offset, M m) {\n+        FloatSpecies vsp = vspecies();\n+        m.check(vsp);\n+        VectorSupport.storeMasked(\n+            vsp.vectorType(), maskClass, vsp.elementType(), vsp.laneCount(),\n+            a, byteArrayAddress(a, offset),\n+            this, m, a, offset,\n+            (arr, off, v, vm) -> {\n+                ByteBuffer wb = wrapper(arr, NATIVE_ENDIAN);\n+                v.stOp(wb, off, vm,\n+                        (tb_, o, i, e) -> tb_.putFloat(o + i * 4, e));\n+            });\n+    }\n+\n@@ -3225,0 +3485,19 @@\n+    abstract\n+    void intoByteBuffer0(ByteBuffer bb, int offset, VectorMask<Float> m);\n+    @ForceInline\n+    final\n+    <M extends VectorMask<Float>>\n+    void intoByteBuffer0Template(Class<M> maskClass, ByteBuffer bb, int offset, M m) {\n+        FloatSpecies vsp = vspecies();\n+        m.check(vsp);\n+        ScopedMemoryAccess.storeIntoByteBufferMasked(\n+                vsp.vectorType(), maskClass, vsp.elementType(), vsp.laneCount(),\n+                this, m, bb, offset,\n+                (buf, off, v, vm) -> {\n+                    ByteBuffer wb = wrapper(buf, NATIVE_ENDIAN);\n+                    v.stOp(wb, off, vm,\n+                            (wb_, o, i, e) -> wb_.putFloat(o + i * 4, e));\n+                });\n+    }\n+\n+\n@@ -3542,1 +3821,1 @@\n-                                      AbstractMask<Float> m,\n+                                      VectorMask<Float> m,\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/FloatVector.java","additions":474,"deletions":195,"binary":false,"changes":669,"status":"modified"},{"patch":"@@ -239,2 +239,2 @@\n-    int rOp(int v, FBinOp f) {\n-        return super.rOpTemplate(v, f);  \/\/ specialize\n+    int rOp(int v, VectorMask<Integer> m, FBinOp f) {\n+        return super.rOpTemplate(v, m, f);  \/\/ specialize\n@@ -276,0 +276,6 @@\n+    @Override\n+    @ForceInline\n+    public Int128Vector lanewise(Unary op, VectorMask<Integer> m) {\n+        return (Int128Vector) super.lanewiseTemplate(op, Int128Mask.class, (Int128Mask) m);  \/\/ specialize\n+    }\n+\n@@ -282,0 +288,6 @@\n+    @Override\n+    @ForceInline\n+    public Int128Vector lanewise(Binary op, Vector<Integer> v, VectorMask<Integer> m) {\n+        return (Int128Vector) super.lanewiseTemplate(op, Int128Mask.class, v, (Int128Mask) m);  \/\/ specialize\n+    }\n+\n@@ -289,0 +301,7 @@\n+    \/*package-private*\/\n+    @Override\n+    @ForceInline Int128Vector\n+    lanewiseShift(VectorOperators.Binary op, int e, VectorMask<Integer> m) {\n+        return (Int128Vector) super.lanewiseShiftTemplate(op, Int128Mask.class, e, (Int128Mask) m);  \/\/ specialize\n+    }\n+\n@@ -294,1 +313,1 @@\n-    lanewise(VectorOperators.Ternary op, Vector<Integer> v1, Vector<Integer> v2) {\n+    lanewise(Ternary op, Vector<Integer> v1, Vector<Integer> v2) {\n@@ -298,0 +317,8 @@\n+    @Override\n+    @ForceInline\n+    public final\n+    Int128Vector\n+    lanewise(Ternary op, Vector<Integer> v1, Vector<Integer> v2, VectorMask<Integer> m) {\n+        return (Int128Vector) super.lanewiseTemplate(op, Int128Mask.class, v1, v2, (Int128Mask) m);  \/\/ specialize\n+    }\n+\n@@ -317,1 +344,1 @@\n-        return super.reduceLanesTemplate(op, m);  \/\/ specialized\n+        return super.reduceLanesTemplate(op, Int128Mask.class, (Int128Mask) m);  \/\/ specialized\n@@ -330,1 +357,1 @@\n-        return (long) super.reduceLanesTemplate(op, m);  \/\/ specialized\n+        return (long) super.reduceLanesTemplate(op, Int128Mask.class, (Int128Mask) m);  \/\/ specialized\n@@ -366,0 +393,7 @@\n+    @Override\n+    @ForceInline\n+    public final Int128Mask compare(Comparison op, Vector<Integer> v, VectorMask<Integer> m) {\n+        return super.compareTemplate(Int128Mask.class, op, v, (Int128Mask) m);\n+    }\n+\n+\n@@ -422,0 +456,1 @@\n+                                    Int128Mask.class,\n@@ -591,10 +626,6 @@\n-            if (VSIZE == species.vectorBitSize()) {\n-                Class<?> dtype = species.elementType();\n-                Class<?> dmtype = species.maskType();\n-                return VectorSupport.convert(VectorSupport.VECTOR_OP_REINTERPRET,\n-                    this.getClass(), ETYPE, VLENGTH,\n-                    dmtype, dtype, VLENGTH,\n-                    this, species,\n-                    Int128Mask::defaultMaskCast);\n-            }\n-            return this.defaultMaskCast(species);\n+\n+            return VectorSupport.convert(VectorSupport.VECTOR_OP_CAST,\n+                this.getClass(), ETYPE, VLENGTH,\n+                species.maskType(), species.elementType(), VLENGTH,\n+                this, species,\n+                (m, s) -> s.maskFactory(m.toArray()).check(s));\n@@ -626,3 +657,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_AND, Int128Mask.class, int.class, VLENGTH,\n-                                             this, m,\n-                                             (m1, m2) -> m1.bOp(m2, (i, a, b) -> a & b));\n+            return VectorSupport.binaryOp(VECTOR_OP_AND, Int128Mask.class, null, int.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a & b));\n@@ -636,3 +667,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_OR, Int128Mask.class, int.class, VLENGTH,\n-                                             this, m,\n-                                             (m1, m2) -> m1.bOp(m2, (i, a, b) -> a | b));\n+            return VectorSupport.binaryOp(VECTOR_OP_OR, Int128Mask.class, null, int.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a | b));\n@@ -646,3 +677,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_XOR, Int128Mask.class, int.class, VLENGTH,\n-                                          this, m,\n-                                          (m1, m2) -> m1.bOp(m2, (i, a, b) -> a ^ b));\n+            return VectorSupport.binaryOp(VECTOR_OP_XOR, Int128Mask.class, null, int.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a ^ b));\n@@ -656,2 +687,2 @@\n-            return VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_TRUECOUNT, Int128Mask.class, int.class, VLENGTH, this,\n-                                                      (m) -> trueCountHelper(((Int128Mask)m).getBits()));\n+            return (int) VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_TRUECOUNT, Int128Mask.class, int.class, VLENGTH, this,\n+                                                      (m) -> trueCountHelper(m.getBits()));\n@@ -663,2 +694,2 @@\n-            return VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_FIRSTTRUE, Int128Mask.class, int.class, VLENGTH, this,\n-                                                      (m) -> firstTrueHelper(((Int128Mask)m).getBits()));\n+            return (int) VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_FIRSTTRUE, Int128Mask.class, int.class, VLENGTH, this,\n+                                                      (m) -> firstTrueHelper(m.getBits()));\n@@ -670,2 +701,12 @@\n-            return VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_LASTTRUE, Int128Mask.class, int.class, VLENGTH, this,\n-                                                      (m) -> lastTrueHelper(((Int128Mask)m).getBits()));\n+            return (int) VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_LASTTRUE, Int128Mask.class, int.class, VLENGTH, this,\n+                                                      (m) -> lastTrueHelper(m.getBits()));\n+        }\n+\n+        @Override\n+        @ForceInline\n+        public long toLong() {\n+            if (length() > Long.SIZE) {\n+                throw new UnsupportedOperationException(\"too many lanes for one long\");\n+            }\n+            return VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_TOLONG, Int128Mask.class, int.class, VLENGTH, this,\n+                                                      (m) -> toLongHelper(m.getBits()));\n@@ -782,0 +823,14 @@\n+    @ForceInline\n+    @Override\n+    final\n+    IntVector fromArray0(int[] a, int offset, VectorMask<Integer> m) {\n+        return super.fromArray0Template(Int128Mask.class, a, offset, (Int128Mask) m);  \/\/ specialize\n+    }\n+\n+    @ForceInline\n+    @Override\n+    final\n+    IntVector fromArray0(int[] a, int offset, int[] indexMap, int mapOffset, VectorMask<Integer> m) {\n+        return super.fromArray0Template(Int128Mask.class, a, offset, indexMap, mapOffset, (Int128Mask) m);\n+    }\n+\n@@ -791,0 +846,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    IntVector fromByteArray0(byte[] a, int offset, VectorMask<Integer> m) {\n+        return super.fromByteArray0Template(Int128Mask.class, a, offset, (Int128Mask) m);  \/\/ specialize\n+    }\n+\n@@ -798,0 +860,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    IntVector fromByteBuffer0(ByteBuffer bb, int offset, VectorMask<Integer> m) {\n+        return super.fromByteBuffer0Template(Int128Mask.class, bb, offset, (Int128Mask) m);  \/\/ specialize\n+    }\n+\n@@ -805,0 +874,15 @@\n+    @ForceInline\n+    @Override\n+    final\n+    void intoArray0(int[] a, int offset, VectorMask<Integer> m) {\n+        super.intoArray0Template(Int128Mask.class, a, offset, (Int128Mask) m);\n+    }\n+\n+    @ForceInline\n+    @Override\n+    final\n+    void intoArray0(int[] a, int offset, int[] indexMap, int mapOffset, VectorMask<Integer> m) {\n+        super.intoArray0Template(Int128Mask.class, a, offset, indexMap, mapOffset, (Int128Mask) m);\n+    }\n+\n+\n@@ -812,0 +896,15 @@\n+    @ForceInline\n+    @Override\n+    final\n+    void intoByteArray0(byte[] a, int offset, VectorMask<Integer> m) {\n+        super.intoByteArray0Template(Int128Mask.class, a, offset, (Int128Mask) m);  \/\/ specialize\n+    }\n+\n+    @ForceInline\n+    @Override\n+    final\n+    void intoByteBuffer0(ByteBuffer bb, int offset, VectorMask<Integer> m) {\n+        super.intoByteBuffer0Template(Int128Mask.class, bb, offset, (Int128Mask) m);\n+    }\n+\n+\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/Int128Vector.java","additions":129,"deletions":30,"binary":false,"changes":159,"status":"modified"},{"patch":"@@ -239,2 +239,2 @@\n-    int rOp(int v, FBinOp f) {\n-        return super.rOpTemplate(v, f);  \/\/ specialize\n+    int rOp(int v, VectorMask<Integer> m, FBinOp f) {\n+        return super.rOpTemplate(v, m, f);  \/\/ specialize\n@@ -276,0 +276,6 @@\n+    @Override\n+    @ForceInline\n+    public Int256Vector lanewise(Unary op, VectorMask<Integer> m) {\n+        return (Int256Vector) super.lanewiseTemplate(op, Int256Mask.class, (Int256Mask) m);  \/\/ specialize\n+    }\n+\n@@ -282,0 +288,6 @@\n+    @Override\n+    @ForceInline\n+    public Int256Vector lanewise(Binary op, Vector<Integer> v, VectorMask<Integer> m) {\n+        return (Int256Vector) super.lanewiseTemplate(op, Int256Mask.class, v, (Int256Mask) m);  \/\/ specialize\n+    }\n+\n@@ -289,0 +301,7 @@\n+    \/*package-private*\/\n+    @Override\n+    @ForceInline Int256Vector\n+    lanewiseShift(VectorOperators.Binary op, int e, VectorMask<Integer> m) {\n+        return (Int256Vector) super.lanewiseShiftTemplate(op, Int256Mask.class, e, (Int256Mask) m);  \/\/ specialize\n+    }\n+\n@@ -294,1 +313,1 @@\n-    lanewise(VectorOperators.Ternary op, Vector<Integer> v1, Vector<Integer> v2) {\n+    lanewise(Ternary op, Vector<Integer> v1, Vector<Integer> v2) {\n@@ -298,0 +317,8 @@\n+    @Override\n+    @ForceInline\n+    public final\n+    Int256Vector\n+    lanewise(Ternary op, Vector<Integer> v1, Vector<Integer> v2, VectorMask<Integer> m) {\n+        return (Int256Vector) super.lanewiseTemplate(op, Int256Mask.class, v1, v2, (Int256Mask) m);  \/\/ specialize\n+    }\n+\n@@ -317,1 +344,1 @@\n-        return super.reduceLanesTemplate(op, m);  \/\/ specialized\n+        return super.reduceLanesTemplate(op, Int256Mask.class, (Int256Mask) m);  \/\/ specialized\n@@ -330,1 +357,1 @@\n-        return (long) super.reduceLanesTemplate(op, m);  \/\/ specialized\n+        return (long) super.reduceLanesTemplate(op, Int256Mask.class, (Int256Mask) m);  \/\/ specialized\n@@ -366,0 +393,7 @@\n+    @Override\n+    @ForceInline\n+    public final Int256Mask compare(Comparison op, Vector<Integer> v, VectorMask<Integer> m) {\n+        return super.compareTemplate(Int256Mask.class, op, v, (Int256Mask) m);\n+    }\n+\n+\n@@ -422,0 +456,1 @@\n+                                    Int256Mask.class,\n@@ -599,10 +634,6 @@\n-            if (VSIZE == species.vectorBitSize()) {\n-                Class<?> dtype = species.elementType();\n-                Class<?> dmtype = species.maskType();\n-                return VectorSupport.convert(VectorSupport.VECTOR_OP_REINTERPRET,\n-                    this.getClass(), ETYPE, VLENGTH,\n-                    dmtype, dtype, VLENGTH,\n-                    this, species,\n-                    Int256Mask::defaultMaskCast);\n-            }\n-            return this.defaultMaskCast(species);\n+\n+            return VectorSupport.convert(VectorSupport.VECTOR_OP_CAST,\n+                this.getClass(), ETYPE, VLENGTH,\n+                species.maskType(), species.elementType(), VLENGTH,\n+                this, species,\n+                (m, s) -> s.maskFactory(m.toArray()).check(s));\n@@ -634,3 +665,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_AND, Int256Mask.class, int.class, VLENGTH,\n-                                             this, m,\n-                                             (m1, m2) -> m1.bOp(m2, (i, a, b) -> a & b));\n+            return VectorSupport.binaryOp(VECTOR_OP_AND, Int256Mask.class, null, int.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a & b));\n@@ -644,3 +675,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_OR, Int256Mask.class, int.class, VLENGTH,\n-                                             this, m,\n-                                             (m1, m2) -> m1.bOp(m2, (i, a, b) -> a | b));\n+            return VectorSupport.binaryOp(VECTOR_OP_OR, Int256Mask.class, null, int.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a | b));\n@@ -654,3 +685,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_XOR, Int256Mask.class, int.class, VLENGTH,\n-                                          this, m,\n-                                          (m1, m2) -> m1.bOp(m2, (i, a, b) -> a ^ b));\n+            return VectorSupport.binaryOp(VECTOR_OP_XOR, Int256Mask.class, null, int.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a ^ b));\n@@ -664,2 +695,2 @@\n-            return VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_TRUECOUNT, Int256Mask.class, int.class, VLENGTH, this,\n-                                                      (m) -> trueCountHelper(((Int256Mask)m).getBits()));\n+            return (int) VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_TRUECOUNT, Int256Mask.class, int.class, VLENGTH, this,\n+                                                      (m) -> trueCountHelper(m.getBits()));\n@@ -671,2 +702,2 @@\n-            return VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_FIRSTTRUE, Int256Mask.class, int.class, VLENGTH, this,\n-                                                      (m) -> firstTrueHelper(((Int256Mask)m).getBits()));\n+            return (int) VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_FIRSTTRUE, Int256Mask.class, int.class, VLENGTH, this,\n+                                                      (m) -> firstTrueHelper(m.getBits()));\n@@ -678,2 +709,12 @@\n-            return VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_LASTTRUE, Int256Mask.class, int.class, VLENGTH, this,\n-                                                      (m) -> lastTrueHelper(((Int256Mask)m).getBits()));\n+            return (int) VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_LASTTRUE, Int256Mask.class, int.class, VLENGTH, this,\n+                                                      (m) -> lastTrueHelper(m.getBits()));\n+        }\n+\n+        @Override\n+        @ForceInline\n+        public long toLong() {\n+            if (length() > Long.SIZE) {\n+                throw new UnsupportedOperationException(\"too many lanes for one long\");\n+            }\n+            return VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_TOLONG, Int256Mask.class, int.class, VLENGTH, this,\n+                                                      (m) -> toLongHelper(m.getBits()));\n@@ -790,0 +831,14 @@\n+    @ForceInline\n+    @Override\n+    final\n+    IntVector fromArray0(int[] a, int offset, VectorMask<Integer> m) {\n+        return super.fromArray0Template(Int256Mask.class, a, offset, (Int256Mask) m);  \/\/ specialize\n+    }\n+\n+    @ForceInline\n+    @Override\n+    final\n+    IntVector fromArray0(int[] a, int offset, int[] indexMap, int mapOffset, VectorMask<Integer> m) {\n+        return super.fromArray0Template(Int256Mask.class, a, offset, indexMap, mapOffset, (Int256Mask) m);\n+    }\n+\n@@ -799,0 +854,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    IntVector fromByteArray0(byte[] a, int offset, VectorMask<Integer> m) {\n+        return super.fromByteArray0Template(Int256Mask.class, a, offset, (Int256Mask) m);  \/\/ specialize\n+    }\n+\n@@ -806,0 +868,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    IntVector fromByteBuffer0(ByteBuffer bb, int offset, VectorMask<Integer> m) {\n+        return super.fromByteBuffer0Template(Int256Mask.class, bb, offset, (Int256Mask) m);  \/\/ specialize\n+    }\n+\n@@ -813,0 +882,15 @@\n+    @ForceInline\n+    @Override\n+    final\n+    void intoArray0(int[] a, int offset, VectorMask<Integer> m) {\n+        super.intoArray0Template(Int256Mask.class, a, offset, (Int256Mask) m);\n+    }\n+\n+    @ForceInline\n+    @Override\n+    final\n+    void intoArray0(int[] a, int offset, int[] indexMap, int mapOffset, VectorMask<Integer> m) {\n+        super.intoArray0Template(Int256Mask.class, a, offset, indexMap, mapOffset, (Int256Mask) m);\n+    }\n+\n+\n@@ -820,0 +904,15 @@\n+    @ForceInline\n+    @Override\n+    final\n+    void intoByteArray0(byte[] a, int offset, VectorMask<Integer> m) {\n+        super.intoByteArray0Template(Int256Mask.class, a, offset, (Int256Mask) m);  \/\/ specialize\n+    }\n+\n+    @ForceInline\n+    @Override\n+    final\n+    void intoByteBuffer0(ByteBuffer bb, int offset, VectorMask<Integer> m) {\n+        super.intoByteBuffer0Template(Int256Mask.class, bb, offset, (Int256Mask) m);\n+    }\n+\n+\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/Int256Vector.java","additions":129,"deletions":30,"binary":false,"changes":159,"status":"modified"},{"patch":"@@ -239,2 +239,2 @@\n-    int rOp(int v, FBinOp f) {\n-        return super.rOpTemplate(v, f);  \/\/ specialize\n+    int rOp(int v, VectorMask<Integer> m, FBinOp f) {\n+        return super.rOpTemplate(v, m, f);  \/\/ specialize\n@@ -276,0 +276,6 @@\n+    @Override\n+    @ForceInline\n+    public Int512Vector lanewise(Unary op, VectorMask<Integer> m) {\n+        return (Int512Vector) super.lanewiseTemplate(op, Int512Mask.class, (Int512Mask) m);  \/\/ specialize\n+    }\n+\n@@ -282,0 +288,6 @@\n+    @Override\n+    @ForceInline\n+    public Int512Vector lanewise(Binary op, Vector<Integer> v, VectorMask<Integer> m) {\n+        return (Int512Vector) super.lanewiseTemplate(op, Int512Mask.class, v, (Int512Mask) m);  \/\/ specialize\n+    }\n+\n@@ -289,0 +301,7 @@\n+    \/*package-private*\/\n+    @Override\n+    @ForceInline Int512Vector\n+    lanewiseShift(VectorOperators.Binary op, int e, VectorMask<Integer> m) {\n+        return (Int512Vector) super.lanewiseShiftTemplate(op, Int512Mask.class, e, (Int512Mask) m);  \/\/ specialize\n+    }\n+\n@@ -294,1 +313,1 @@\n-    lanewise(VectorOperators.Ternary op, Vector<Integer> v1, Vector<Integer> v2) {\n+    lanewise(Ternary op, Vector<Integer> v1, Vector<Integer> v2) {\n@@ -298,0 +317,8 @@\n+    @Override\n+    @ForceInline\n+    public final\n+    Int512Vector\n+    lanewise(Ternary op, Vector<Integer> v1, Vector<Integer> v2, VectorMask<Integer> m) {\n+        return (Int512Vector) super.lanewiseTemplate(op, Int512Mask.class, v1, v2, (Int512Mask) m);  \/\/ specialize\n+    }\n+\n@@ -317,1 +344,1 @@\n-        return super.reduceLanesTemplate(op, m);  \/\/ specialized\n+        return super.reduceLanesTemplate(op, Int512Mask.class, (Int512Mask) m);  \/\/ specialized\n@@ -330,1 +357,1 @@\n-        return (long) super.reduceLanesTemplate(op, m);  \/\/ specialized\n+        return (long) super.reduceLanesTemplate(op, Int512Mask.class, (Int512Mask) m);  \/\/ specialized\n@@ -366,0 +393,7 @@\n+    @Override\n+    @ForceInline\n+    public final Int512Mask compare(Comparison op, Vector<Integer> v, VectorMask<Integer> m) {\n+        return super.compareTemplate(Int512Mask.class, op, v, (Int512Mask) m);\n+    }\n+\n+\n@@ -422,0 +456,1 @@\n+                                    Int512Mask.class,\n@@ -615,10 +650,6 @@\n-            if (VSIZE == species.vectorBitSize()) {\n-                Class<?> dtype = species.elementType();\n-                Class<?> dmtype = species.maskType();\n-                return VectorSupport.convert(VectorSupport.VECTOR_OP_REINTERPRET,\n-                    this.getClass(), ETYPE, VLENGTH,\n-                    dmtype, dtype, VLENGTH,\n-                    this, species,\n-                    Int512Mask::defaultMaskCast);\n-            }\n-            return this.defaultMaskCast(species);\n+\n+            return VectorSupport.convert(VectorSupport.VECTOR_OP_CAST,\n+                this.getClass(), ETYPE, VLENGTH,\n+                species.maskType(), species.elementType(), VLENGTH,\n+                this, species,\n+                (m, s) -> s.maskFactory(m.toArray()).check(s));\n@@ -650,3 +681,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_AND, Int512Mask.class, int.class, VLENGTH,\n-                                             this, m,\n-                                             (m1, m2) -> m1.bOp(m2, (i, a, b) -> a & b));\n+            return VectorSupport.binaryOp(VECTOR_OP_AND, Int512Mask.class, null, int.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a & b));\n@@ -660,3 +691,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_OR, Int512Mask.class, int.class, VLENGTH,\n-                                             this, m,\n-                                             (m1, m2) -> m1.bOp(m2, (i, a, b) -> a | b));\n+            return VectorSupport.binaryOp(VECTOR_OP_OR, Int512Mask.class, null, int.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a | b));\n@@ -670,3 +701,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_XOR, Int512Mask.class, int.class, VLENGTH,\n-                                          this, m,\n-                                          (m1, m2) -> m1.bOp(m2, (i, a, b) -> a ^ b));\n+            return VectorSupport.binaryOp(VECTOR_OP_XOR, Int512Mask.class, null, int.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a ^ b));\n@@ -680,2 +711,2 @@\n-            return VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_TRUECOUNT, Int512Mask.class, int.class, VLENGTH, this,\n-                                                      (m) -> trueCountHelper(((Int512Mask)m).getBits()));\n+            return (int) VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_TRUECOUNT, Int512Mask.class, int.class, VLENGTH, this,\n+                                                      (m) -> trueCountHelper(m.getBits()));\n@@ -687,2 +718,2 @@\n-            return VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_FIRSTTRUE, Int512Mask.class, int.class, VLENGTH, this,\n-                                                      (m) -> firstTrueHelper(((Int512Mask)m).getBits()));\n+            return (int) VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_FIRSTTRUE, Int512Mask.class, int.class, VLENGTH, this,\n+                                                      (m) -> firstTrueHelper(m.getBits()));\n@@ -694,2 +725,12 @@\n-            return VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_LASTTRUE, Int512Mask.class, int.class, VLENGTH, this,\n-                                                      (m) -> lastTrueHelper(((Int512Mask)m).getBits()));\n+            return (int) VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_LASTTRUE, Int512Mask.class, int.class, VLENGTH, this,\n+                                                      (m) -> lastTrueHelper(m.getBits()));\n+        }\n+\n+        @Override\n+        @ForceInline\n+        public long toLong() {\n+            if (length() > Long.SIZE) {\n+                throw new UnsupportedOperationException(\"too many lanes for one long\");\n+            }\n+            return VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_TOLONG, Int512Mask.class, int.class, VLENGTH, this,\n+                                                      (m) -> toLongHelper(m.getBits()));\n@@ -806,0 +847,14 @@\n+    @ForceInline\n+    @Override\n+    final\n+    IntVector fromArray0(int[] a, int offset, VectorMask<Integer> m) {\n+        return super.fromArray0Template(Int512Mask.class, a, offset, (Int512Mask) m);  \/\/ specialize\n+    }\n+\n+    @ForceInline\n+    @Override\n+    final\n+    IntVector fromArray0(int[] a, int offset, int[] indexMap, int mapOffset, VectorMask<Integer> m) {\n+        return super.fromArray0Template(Int512Mask.class, a, offset, indexMap, mapOffset, (Int512Mask) m);\n+    }\n+\n@@ -815,0 +870,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    IntVector fromByteArray0(byte[] a, int offset, VectorMask<Integer> m) {\n+        return super.fromByteArray0Template(Int512Mask.class, a, offset, (Int512Mask) m);  \/\/ specialize\n+    }\n+\n@@ -822,0 +884,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    IntVector fromByteBuffer0(ByteBuffer bb, int offset, VectorMask<Integer> m) {\n+        return super.fromByteBuffer0Template(Int512Mask.class, bb, offset, (Int512Mask) m);  \/\/ specialize\n+    }\n+\n@@ -829,0 +898,15 @@\n+    @ForceInline\n+    @Override\n+    final\n+    void intoArray0(int[] a, int offset, VectorMask<Integer> m) {\n+        super.intoArray0Template(Int512Mask.class, a, offset, (Int512Mask) m);\n+    }\n+\n+    @ForceInline\n+    @Override\n+    final\n+    void intoArray0(int[] a, int offset, int[] indexMap, int mapOffset, VectorMask<Integer> m) {\n+        super.intoArray0Template(Int512Mask.class, a, offset, indexMap, mapOffset, (Int512Mask) m);\n+    }\n+\n+\n@@ -836,0 +920,15 @@\n+    @ForceInline\n+    @Override\n+    final\n+    void intoByteArray0(byte[] a, int offset, VectorMask<Integer> m) {\n+        super.intoByteArray0Template(Int512Mask.class, a, offset, (Int512Mask) m);  \/\/ specialize\n+    }\n+\n+    @ForceInline\n+    @Override\n+    final\n+    void intoByteBuffer0(ByteBuffer bb, int offset, VectorMask<Integer> m) {\n+        super.intoByteBuffer0Template(Int512Mask.class, bb, offset, (Int512Mask) m);\n+    }\n+\n+\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/Int512Vector.java","additions":129,"deletions":30,"binary":false,"changes":159,"status":"modified"},{"patch":"@@ -239,2 +239,2 @@\n-    int rOp(int v, FBinOp f) {\n-        return super.rOpTemplate(v, f);  \/\/ specialize\n+    int rOp(int v, VectorMask<Integer> m, FBinOp f) {\n+        return super.rOpTemplate(v, m, f);  \/\/ specialize\n@@ -276,0 +276,6 @@\n+    @Override\n+    @ForceInline\n+    public Int64Vector lanewise(Unary op, VectorMask<Integer> m) {\n+        return (Int64Vector) super.lanewiseTemplate(op, Int64Mask.class, (Int64Mask) m);  \/\/ specialize\n+    }\n+\n@@ -282,0 +288,6 @@\n+    @Override\n+    @ForceInline\n+    public Int64Vector lanewise(Binary op, Vector<Integer> v, VectorMask<Integer> m) {\n+        return (Int64Vector) super.lanewiseTemplate(op, Int64Mask.class, v, (Int64Mask) m);  \/\/ specialize\n+    }\n+\n@@ -289,0 +301,7 @@\n+    \/*package-private*\/\n+    @Override\n+    @ForceInline Int64Vector\n+    lanewiseShift(VectorOperators.Binary op, int e, VectorMask<Integer> m) {\n+        return (Int64Vector) super.lanewiseShiftTemplate(op, Int64Mask.class, e, (Int64Mask) m);  \/\/ specialize\n+    }\n+\n@@ -294,1 +313,1 @@\n-    lanewise(VectorOperators.Ternary op, Vector<Integer> v1, Vector<Integer> v2) {\n+    lanewise(Ternary op, Vector<Integer> v1, Vector<Integer> v2) {\n@@ -298,0 +317,8 @@\n+    @Override\n+    @ForceInline\n+    public final\n+    Int64Vector\n+    lanewise(Ternary op, Vector<Integer> v1, Vector<Integer> v2, VectorMask<Integer> m) {\n+        return (Int64Vector) super.lanewiseTemplate(op, Int64Mask.class, v1, v2, (Int64Mask) m);  \/\/ specialize\n+    }\n+\n@@ -317,1 +344,1 @@\n-        return super.reduceLanesTemplate(op, m);  \/\/ specialized\n+        return super.reduceLanesTemplate(op, Int64Mask.class, (Int64Mask) m);  \/\/ specialized\n@@ -330,1 +357,1 @@\n-        return (long) super.reduceLanesTemplate(op, m);  \/\/ specialized\n+        return (long) super.reduceLanesTemplate(op, Int64Mask.class, (Int64Mask) m);  \/\/ specialized\n@@ -366,0 +393,7 @@\n+    @Override\n+    @ForceInline\n+    public final Int64Mask compare(Comparison op, Vector<Integer> v, VectorMask<Integer> m) {\n+        return super.compareTemplate(Int64Mask.class, op, v, (Int64Mask) m);\n+    }\n+\n+\n@@ -422,0 +456,1 @@\n+                                    Int64Mask.class,\n@@ -587,10 +622,6 @@\n-            if (VSIZE == species.vectorBitSize()) {\n-                Class<?> dtype = species.elementType();\n-                Class<?> dmtype = species.maskType();\n-                return VectorSupport.convert(VectorSupport.VECTOR_OP_REINTERPRET,\n-                    this.getClass(), ETYPE, VLENGTH,\n-                    dmtype, dtype, VLENGTH,\n-                    this, species,\n-                    Int64Mask::defaultMaskCast);\n-            }\n-            return this.defaultMaskCast(species);\n+\n+            return VectorSupport.convert(VectorSupport.VECTOR_OP_CAST,\n+                this.getClass(), ETYPE, VLENGTH,\n+                species.maskType(), species.elementType(), VLENGTH,\n+                this, species,\n+                (m, s) -> s.maskFactory(m.toArray()).check(s));\n@@ -622,3 +653,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_AND, Int64Mask.class, int.class, VLENGTH,\n-                                             this, m,\n-                                             (m1, m2) -> m1.bOp(m2, (i, a, b) -> a & b));\n+            return VectorSupport.binaryOp(VECTOR_OP_AND, Int64Mask.class, null, int.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a & b));\n@@ -632,3 +663,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_OR, Int64Mask.class, int.class, VLENGTH,\n-                                             this, m,\n-                                             (m1, m2) -> m1.bOp(m2, (i, a, b) -> a | b));\n+            return VectorSupport.binaryOp(VECTOR_OP_OR, Int64Mask.class, null, int.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a | b));\n@@ -642,3 +673,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_XOR, Int64Mask.class, int.class, VLENGTH,\n-                                          this, m,\n-                                          (m1, m2) -> m1.bOp(m2, (i, a, b) -> a ^ b));\n+            return VectorSupport.binaryOp(VECTOR_OP_XOR, Int64Mask.class, null, int.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a ^ b));\n@@ -652,2 +683,2 @@\n-            return VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_TRUECOUNT, Int64Mask.class, int.class, VLENGTH, this,\n-                                                      (m) -> trueCountHelper(((Int64Mask)m).getBits()));\n+            return (int) VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_TRUECOUNT, Int64Mask.class, int.class, VLENGTH, this,\n+                                                      (m) -> trueCountHelper(m.getBits()));\n@@ -659,2 +690,2 @@\n-            return VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_FIRSTTRUE, Int64Mask.class, int.class, VLENGTH, this,\n-                                                      (m) -> firstTrueHelper(((Int64Mask)m).getBits()));\n+            return (int) VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_FIRSTTRUE, Int64Mask.class, int.class, VLENGTH, this,\n+                                                      (m) -> firstTrueHelper(m.getBits()));\n@@ -666,2 +697,12 @@\n-            return VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_LASTTRUE, Int64Mask.class, int.class, VLENGTH, this,\n-                                                      (m) -> lastTrueHelper(((Int64Mask)m).getBits()));\n+            return (int) VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_LASTTRUE, Int64Mask.class, int.class, VLENGTH, this,\n+                                                      (m) -> lastTrueHelper(m.getBits()));\n+        }\n+\n+        @Override\n+        @ForceInline\n+        public long toLong() {\n+            if (length() > Long.SIZE) {\n+                throw new UnsupportedOperationException(\"too many lanes for one long\");\n+            }\n+            return VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_TOLONG, Int64Mask.class, int.class, VLENGTH, this,\n+                                                      (m) -> toLongHelper(m.getBits()));\n@@ -778,0 +819,14 @@\n+    @ForceInline\n+    @Override\n+    final\n+    IntVector fromArray0(int[] a, int offset, VectorMask<Integer> m) {\n+        return super.fromArray0Template(Int64Mask.class, a, offset, (Int64Mask) m);  \/\/ specialize\n+    }\n+\n+    @ForceInline\n+    @Override\n+    final\n+    IntVector fromArray0(int[] a, int offset, int[] indexMap, int mapOffset, VectorMask<Integer> m) {\n+        return super.fromArray0Template(Int64Mask.class, a, offset, indexMap, mapOffset, (Int64Mask) m);\n+    }\n+\n@@ -787,0 +842,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    IntVector fromByteArray0(byte[] a, int offset, VectorMask<Integer> m) {\n+        return super.fromByteArray0Template(Int64Mask.class, a, offset, (Int64Mask) m);  \/\/ specialize\n+    }\n+\n@@ -794,0 +856,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    IntVector fromByteBuffer0(ByteBuffer bb, int offset, VectorMask<Integer> m) {\n+        return super.fromByteBuffer0Template(Int64Mask.class, bb, offset, (Int64Mask) m);  \/\/ specialize\n+    }\n+\n@@ -801,0 +870,15 @@\n+    @ForceInline\n+    @Override\n+    final\n+    void intoArray0(int[] a, int offset, VectorMask<Integer> m) {\n+        super.intoArray0Template(Int64Mask.class, a, offset, (Int64Mask) m);\n+    }\n+\n+    @ForceInline\n+    @Override\n+    final\n+    void intoArray0(int[] a, int offset, int[] indexMap, int mapOffset, VectorMask<Integer> m) {\n+        super.intoArray0Template(Int64Mask.class, a, offset, indexMap, mapOffset, (Int64Mask) m);\n+    }\n+\n+\n@@ -808,0 +892,15 @@\n+    @ForceInline\n+    @Override\n+    final\n+    void intoByteArray0(byte[] a, int offset, VectorMask<Integer> m) {\n+        super.intoByteArray0Template(Int64Mask.class, a, offset, (Int64Mask) m);  \/\/ specialize\n+    }\n+\n+    @ForceInline\n+    @Override\n+    final\n+    void intoByteBuffer0(ByteBuffer bb, int offset, VectorMask<Integer> m) {\n+        super.intoByteBuffer0Template(Int64Mask.class, bb, offset, (Int64Mask) m);\n+    }\n+\n+\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/Int64Vector.java","additions":129,"deletions":30,"binary":false,"changes":159,"status":"modified"},{"patch":"@@ -239,2 +239,2 @@\n-    int rOp(int v, FBinOp f) {\n-        return super.rOpTemplate(v, f);  \/\/ specialize\n+    int rOp(int v, VectorMask<Integer> m, FBinOp f) {\n+        return super.rOpTemplate(v, m, f);  \/\/ specialize\n@@ -276,0 +276,6 @@\n+    @Override\n+    @ForceInline\n+    public IntMaxVector lanewise(Unary op, VectorMask<Integer> m) {\n+        return (IntMaxVector) super.lanewiseTemplate(op, IntMaxMask.class, (IntMaxMask) m);  \/\/ specialize\n+    }\n+\n@@ -282,0 +288,6 @@\n+    @Override\n+    @ForceInline\n+    public IntMaxVector lanewise(Binary op, Vector<Integer> v, VectorMask<Integer> m) {\n+        return (IntMaxVector) super.lanewiseTemplate(op, IntMaxMask.class, v, (IntMaxMask) m);  \/\/ specialize\n+    }\n+\n@@ -289,0 +301,7 @@\n+    \/*package-private*\/\n+    @Override\n+    @ForceInline IntMaxVector\n+    lanewiseShift(VectorOperators.Binary op, int e, VectorMask<Integer> m) {\n+        return (IntMaxVector) super.lanewiseShiftTemplate(op, IntMaxMask.class, e, (IntMaxMask) m);  \/\/ specialize\n+    }\n+\n@@ -294,1 +313,1 @@\n-    lanewise(VectorOperators.Ternary op, Vector<Integer> v1, Vector<Integer> v2) {\n+    lanewise(Ternary op, Vector<Integer> v1, Vector<Integer> v2) {\n@@ -298,0 +317,8 @@\n+    @Override\n+    @ForceInline\n+    public final\n+    IntMaxVector\n+    lanewise(Ternary op, Vector<Integer> v1, Vector<Integer> v2, VectorMask<Integer> m) {\n+        return (IntMaxVector) super.lanewiseTemplate(op, IntMaxMask.class, v1, v2, (IntMaxMask) m);  \/\/ specialize\n+    }\n+\n@@ -317,1 +344,1 @@\n-        return super.reduceLanesTemplate(op, m);  \/\/ specialized\n+        return super.reduceLanesTemplate(op, IntMaxMask.class, (IntMaxMask) m);  \/\/ specialized\n@@ -330,1 +357,1 @@\n-        return (long) super.reduceLanesTemplate(op, m);  \/\/ specialized\n+        return (long) super.reduceLanesTemplate(op, IntMaxMask.class, (IntMaxMask) m);  \/\/ specialized\n@@ -366,0 +393,7 @@\n+    @Override\n+    @ForceInline\n+    public final IntMaxMask compare(Comparison op, Vector<Integer> v, VectorMask<Integer> m) {\n+        return super.compareTemplate(IntMaxMask.class, op, v, (IntMaxMask) m);\n+    }\n+\n+\n@@ -422,0 +456,1 @@\n+                                    IntMaxMask.class,\n@@ -585,10 +620,6 @@\n-            if (VSIZE == species.vectorBitSize()) {\n-                Class<?> dtype = species.elementType();\n-                Class<?> dmtype = species.maskType();\n-                return VectorSupport.convert(VectorSupport.VECTOR_OP_REINTERPRET,\n-                    this.getClass(), ETYPE, VLENGTH,\n-                    dmtype, dtype, VLENGTH,\n-                    this, species,\n-                    IntMaxMask::defaultMaskCast);\n-            }\n-            return this.defaultMaskCast(species);\n+\n+            return VectorSupport.convert(VectorSupport.VECTOR_OP_CAST,\n+                this.getClass(), ETYPE, VLENGTH,\n+                species.maskType(), species.elementType(), VLENGTH,\n+                this, species,\n+                (m, s) -> s.maskFactory(m.toArray()).check(s));\n@@ -620,3 +651,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_AND, IntMaxMask.class, int.class, VLENGTH,\n-                                             this, m,\n-                                             (m1, m2) -> m1.bOp(m2, (i, a, b) -> a & b));\n+            return VectorSupport.binaryOp(VECTOR_OP_AND, IntMaxMask.class, null, int.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a & b));\n@@ -630,3 +661,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_OR, IntMaxMask.class, int.class, VLENGTH,\n-                                             this, m,\n-                                             (m1, m2) -> m1.bOp(m2, (i, a, b) -> a | b));\n+            return VectorSupport.binaryOp(VECTOR_OP_OR, IntMaxMask.class, null, int.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a | b));\n@@ -640,3 +671,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_XOR, IntMaxMask.class, int.class, VLENGTH,\n-                                          this, m,\n-                                          (m1, m2) -> m1.bOp(m2, (i, a, b) -> a ^ b));\n+            return VectorSupport.binaryOp(VECTOR_OP_XOR, IntMaxMask.class, null, int.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a ^ b));\n@@ -650,2 +681,2 @@\n-            return VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_TRUECOUNT, IntMaxMask.class, int.class, VLENGTH, this,\n-                                                      (m) -> trueCountHelper(((IntMaxMask)m).getBits()));\n+            return (int) VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_TRUECOUNT, IntMaxMask.class, int.class, VLENGTH, this,\n+                                                      (m) -> trueCountHelper(m.getBits()));\n@@ -657,2 +688,2 @@\n-            return VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_FIRSTTRUE, IntMaxMask.class, int.class, VLENGTH, this,\n-                                                      (m) -> firstTrueHelper(((IntMaxMask)m).getBits()));\n+            return (int) VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_FIRSTTRUE, IntMaxMask.class, int.class, VLENGTH, this,\n+                                                      (m) -> firstTrueHelper(m.getBits()));\n@@ -664,2 +695,12 @@\n-            return VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_LASTTRUE, IntMaxMask.class, int.class, VLENGTH, this,\n-                                                      (m) -> lastTrueHelper(((IntMaxMask)m).getBits()));\n+            return (int) VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_LASTTRUE, IntMaxMask.class, int.class, VLENGTH, this,\n+                                                      (m) -> lastTrueHelper(m.getBits()));\n+        }\n+\n+        @Override\n+        @ForceInline\n+        public long toLong() {\n+            if (length() > Long.SIZE) {\n+                throw new UnsupportedOperationException(\"too many lanes for one long\");\n+            }\n+            return VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_TOLONG, IntMaxMask.class, int.class, VLENGTH, this,\n+                                                      (m) -> toLongHelper(m.getBits()));\n@@ -787,0 +828,14 @@\n+    @ForceInline\n+    @Override\n+    final\n+    IntVector fromArray0(int[] a, int offset, VectorMask<Integer> m) {\n+        return super.fromArray0Template(IntMaxMask.class, a, offset, (IntMaxMask) m);  \/\/ specialize\n+    }\n+\n+    @ForceInline\n+    @Override\n+    final\n+    IntVector fromArray0(int[] a, int offset, int[] indexMap, int mapOffset, VectorMask<Integer> m) {\n+        return super.fromArray0Template(IntMaxMask.class, a, offset, indexMap, mapOffset, (IntMaxMask) m);\n+    }\n+\n@@ -796,0 +851,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    IntVector fromByteArray0(byte[] a, int offset, VectorMask<Integer> m) {\n+        return super.fromByteArray0Template(IntMaxMask.class, a, offset, (IntMaxMask) m);  \/\/ specialize\n+    }\n+\n@@ -803,0 +865,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    IntVector fromByteBuffer0(ByteBuffer bb, int offset, VectorMask<Integer> m) {\n+        return super.fromByteBuffer0Template(IntMaxMask.class, bb, offset, (IntMaxMask) m);  \/\/ specialize\n+    }\n+\n@@ -810,0 +879,15 @@\n+    @ForceInline\n+    @Override\n+    final\n+    void intoArray0(int[] a, int offset, VectorMask<Integer> m) {\n+        super.intoArray0Template(IntMaxMask.class, a, offset, (IntMaxMask) m);\n+    }\n+\n+    @ForceInline\n+    @Override\n+    final\n+    void intoArray0(int[] a, int offset, int[] indexMap, int mapOffset, VectorMask<Integer> m) {\n+        super.intoArray0Template(IntMaxMask.class, a, offset, indexMap, mapOffset, (IntMaxMask) m);\n+    }\n+\n+\n@@ -817,0 +901,15 @@\n+    @ForceInline\n+    @Override\n+    final\n+    void intoByteArray0(byte[] a, int offset, VectorMask<Integer> m) {\n+        super.intoByteArray0Template(IntMaxMask.class, a, offset, (IntMaxMask) m);  \/\/ specialize\n+    }\n+\n+    @ForceInline\n+    @Override\n+    final\n+    void intoByteBuffer0(ByteBuffer bb, int offset, VectorMask<Integer> m) {\n+        super.intoByteBuffer0Template(IntMaxMask.class, bb, offset, (IntMaxMask) m);\n+    }\n+\n+\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/IntMaxVector.java","additions":129,"deletions":30,"binary":false,"changes":159,"status":"modified"},{"patch":"@@ -32,1 +32,0 @@\n-import java.util.function.BinaryOperator;\n@@ -176,0 +175,3 @@\n+        if (m == null) {\n+            return uOpTemplate(f);\n+        }\n@@ -219,0 +221,3 @@\n+        if (m == null) {\n+            return bOpTemplate(o, f);\n+        }\n@@ -268,0 +273,3 @@\n+        if (m == null) {\n+            return tOpTemplate(o1, o2, f);\n+        }\n@@ -283,1 +291,16 @@\n-    int rOp(int v, FBinOp f);\n+    int rOp(int v, VectorMask<Integer> m, FBinOp f);\n+\n+    @ForceInline\n+    final\n+    int rOpTemplate(int v, VectorMask<Integer> m, FBinOp f) {\n+        if (m == null) {\n+            return rOpTemplate(v, f);\n+        }\n+        int[] vec = vec();\n+        boolean[] mbits = ((AbstractMask<Integer>)m).getBits();\n+        for (int i = 0; i < vec.length; i++) {\n+            v = mbits[i] ? f.apply(i, v, vec[i]) : v;\n+        }\n+        return v;\n+    }\n+\n@@ -552,1 +575,1 @@\n-                return broadcast(-1).lanewiseTemplate(XOR, this);\n+                return broadcast(-1).lanewise(XOR, this);\n@@ -555,1 +578,1 @@\n-                return broadcast(0).lanewiseTemplate(SUB, this);\n+                return broadcast(0).lanewise(SUB, this);\n@@ -560,10 +583,3 @@\n-            opc, getClass(), int.class, length(),\n-            this,\n-            UN_IMPL.find(op, opc, (opc_) -> {\n-              switch (opc_) {\n-                case VECTOR_OP_NEG: return v0 ->\n-                        v0.uOp((i, a) -> (int) -a);\n-                case VECTOR_OP_ABS: return v0 ->\n-                        v0.uOp((i, a) -> (int) Math.abs(a));\n-                default: return null;\n-              }}));\n+            opc, getClass(), null, int.class, length(),\n+            this, null,\n+            UN_IMPL.find(op, opc, IntVector::unaryOperations));\n@@ -571,3 +587,0 @@\n-    private static final\n-    ImplCache<Unary,UnaryOperator<IntVector>> UN_IMPL\n-        = new ImplCache<>(Unary.class, IntVector.class);\n@@ -578,2 +591,2 @@\n-    @ForceInline\n-    public final\n+    @Override\n+    public abstract\n@@ -581,2 +594,36 @@\n-                                  VectorMask<Integer> m) {\n-        return blend(lanewise(op), m);\n+                                  VectorMask<Integer> m);\n+    @ForceInline\n+    final\n+    IntVector lanewiseTemplate(VectorOperators.Unary op,\n+                                          Class<? extends VectorMask<Integer>> maskClass,\n+                                          VectorMask<Integer> m) {\n+        m.check(maskClass, this);\n+        if (opKind(op, VO_SPECIAL)) {\n+            if (op == ZOMO) {\n+                return blend(broadcast(-1), compare(NE, 0, m));\n+            }\n+            if (op == NOT) {\n+                return lanewise(XOR, broadcast(-1), m);\n+            } else if (op == NEG) {\n+                return lanewise(NOT, m).lanewise(ADD, broadcast(1), m);\n+            }\n+        }\n+        int opc = opCode(op);\n+        return VectorSupport.unaryOp(\n+            opc, getClass(), maskClass, int.class, length(),\n+            this, m,\n+            UN_IMPL.find(op, opc, IntVector::unaryOperations));\n+    }\n+\n+    private static final\n+    ImplCache<Unary, UnaryOperation<IntVector, VectorMask<Integer>>>\n+        UN_IMPL = new ImplCache<>(Unary.class, IntVector.class);\n+\n+    private static UnaryOperation<IntVector, VectorMask<Integer>> unaryOperations(int opc_) {\n+        switch (opc_) {\n+            case VECTOR_OP_NEG: return (v0, m) ->\n+                    v0.uOp(m, (i, a) -> (int) -a);\n+            case VECTOR_OP_ABS: return (v0, m) ->\n+                    v0.uOp(m, (i, a) -> (int) Math.abs(a));\n+            default: return null;\n+        }\n@@ -602,0 +649,1 @@\n+\n@@ -620,1 +668,1 @@\n-                VectorMask<Integer> eqz = that.eq((int)0);\n+                VectorMask<Integer> eqz = that.eq((int) 0);\n@@ -626,0 +674,1 @@\n+\n@@ -628,34 +677,3 @@\n-            opc, getClass(), int.class, length(),\n-            this, that,\n-            BIN_IMPL.find(op, opc, (opc_) -> {\n-              switch (opc_) {\n-                case VECTOR_OP_ADD: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (int)(a + b));\n-                case VECTOR_OP_SUB: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (int)(a - b));\n-                case VECTOR_OP_MUL: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (int)(a * b));\n-                case VECTOR_OP_DIV: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (int)(a \/ b));\n-                case VECTOR_OP_MAX: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (int)Math.max(a, b));\n-                case VECTOR_OP_MIN: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (int)Math.min(a, b));\n-                case VECTOR_OP_AND: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (int)(a & b));\n-                case VECTOR_OP_OR: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (int)(a | b));\n-                case VECTOR_OP_XOR: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (int)(a ^ b));\n-                case VECTOR_OP_LSHIFT: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, n) -> (int)(a << n));\n-                case VECTOR_OP_RSHIFT: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, n) -> (int)(a >> n));\n-                case VECTOR_OP_URSHIFT: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, n) -> (int)((a & LSHR_SETUP_MASK) >>> n));\n-                case VECTOR_OP_LROTATE: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, n) -> rotateLeft(a, (int)n));\n-                case VECTOR_OP_RROTATE: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, n) -> rotateRight(a, (int)n));\n-                default: return null;\n-                }}));\n+            opc, getClass(), null, int.class, length(),\n+            this, that, null,\n+            BIN_IMPL.find(op, opc, IntVector::binaryOperations));\n@@ -663,3 +681,0 @@\n-    private static final\n-    ImplCache<Binary,BinaryOperator<IntVector>> BIN_IMPL\n-        = new ImplCache<>(Binary.class, IntVector.class);\n@@ -671,2 +686,2 @@\n-    @ForceInline\n-    public final\n+    @Override\n+    public abstract\n@@ -675,1 +690,6 @@\n-                                  VectorMask<Integer> m) {\n+                                  VectorMask<Integer> m);\n+    @ForceInline\n+    final\n+    IntVector lanewiseTemplate(VectorOperators.Binary op,\n+                                          Class<? extends VectorMask<Integer>> maskClass,\n+                                          Vector<Integer> v, VectorMask<Integer> m) {\n@@ -677,4 +697,27 @@\n-        if (op == DIV) {\n-            VectorMask<Integer> eqz = that.eq((int)0);\n-            if (eqz.and(m).anyTrue()) {\n-                throw that.divZeroException();\n+        that.check(this);\n+        m.check(maskClass, this);\n+\n+        if (opKind(op, VO_SPECIAL  | VO_SHIFT)) {\n+            if (op == FIRST_NONZERO) {\n+                \/\/ FIXME: Support this in the JIT.\n+                VectorMask<Integer> thisNZ\n+                    = this.viewAsIntegralLanes().compare(NE, (int) 0);\n+                that = that.blend((int) 0, thisNZ.cast(vspecies()));\n+                op = OR_UNCHECKED;\n+            }\n+            if (opKind(op, VO_SHIFT)) {\n+                \/\/ As per shift specification for Java, mask the shift count.\n+                \/\/ This allows the JIT to ignore some ISA details.\n+                that = that.lanewise(AND, SHIFT_MASK);\n+            }\n+            if (op == AND_NOT) {\n+                \/\/ FIXME: Support this in the JIT.\n+                that = that.lanewise(NOT);\n+                op = AND;\n+            } else if (op == DIV) {\n+                VectorMask<Integer> eqz = that.eq((int)0);\n+                if (eqz.and(m).anyTrue()) {\n+                    throw that.divZeroException();\n+                }\n+                \/\/ suppress div\/0 exceptions in unset lanes\n+                that = that.lanewise(NOT, eqz);\n@@ -682,3 +725,0 @@\n-            \/\/ suppress div\/0 exceptions in unset lanes\n-            that = that.lanewise(NOT, eqz);\n-            return blend(lanewise(DIV, that), m);\n@@ -686,1 +726,44 @@\n-        return blend(lanewise(op, v), m);\n+\n+        int opc = opCode(op);\n+        return VectorSupport.binaryOp(\n+            opc, getClass(), maskClass, int.class, length(),\n+            this, that, m,\n+            BIN_IMPL.find(op, opc, IntVector::binaryOperations));\n+    }\n+\n+    private static final\n+    ImplCache<Binary, BinaryOperation<IntVector, VectorMask<Integer>>>\n+        BIN_IMPL = new ImplCache<>(Binary.class, IntVector.class);\n+\n+    private static BinaryOperation<IntVector, VectorMask<Integer>> binaryOperations(int opc_) {\n+        switch (opc_) {\n+            case VECTOR_OP_ADD: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> (int)(a + b));\n+            case VECTOR_OP_SUB: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> (int)(a - b));\n+            case VECTOR_OP_MUL: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> (int)(a * b));\n+            case VECTOR_OP_DIV: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> (int)(a \/ b));\n+            case VECTOR_OP_MAX: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> (int)Math.max(a, b));\n+            case VECTOR_OP_MIN: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> (int)Math.min(a, b));\n+            case VECTOR_OP_AND: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> (int)(a & b));\n+            case VECTOR_OP_OR: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> (int)(a | b));\n+            case VECTOR_OP_XOR: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> (int)(a ^ b));\n+            case VECTOR_OP_LSHIFT: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, n) -> (int)(a << n));\n+            case VECTOR_OP_RSHIFT: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, n) -> (int)(a >> n));\n+            case VECTOR_OP_URSHIFT: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, n) -> (int)((a & LSHR_SETUP_MASK) >>> n));\n+            case VECTOR_OP_LROTATE: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, n) -> rotateLeft(a, (int)n));\n+            case VECTOR_OP_RROTATE: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, n) -> rotateRight(a, (int)n));\n+            default: return null;\n+        }\n@@ -688,0 +771,1 @@\n+\n@@ -750,1 +834,7 @@\n-        return blend(lanewise(op, e), m);\n+        if (opKind(op, VO_SHIFT) && (int)(int)e == e) {\n+            return lanewiseShift(op, (int) e, m);\n+        }\n+        if (op == AND_NOT) {\n+            op = AND; e = (int) ~e;\n+        }\n+        return lanewise(op, broadcast(e), m);\n@@ -770,2 +860,1 @@\n-            && !(opKind(op, VO_SHIFT) && (int)e1 == e)\n-            ) {\n+            && !(opKind(op, VO_SHIFT) && (int)e1 == e)) {\n@@ -791,1 +880,7 @@\n-        return blend(lanewise(op, e), m);\n+        int e1 = (int) e;\n+        if ((long)e1 != e\n+            \/\/ allow shift ops to clip down their int parameters\n+            && !(opKind(op, VO_SHIFT) && (int)e1 == e)) {\n+            vspecies().checkValue(e);  \/\/ for exception\n+        }\n+        return lanewise(op, e1, m);\n@@ -808,16 +903,3 @@\n-            opc, getClass(), int.class, length(),\n-            this, e,\n-            BIN_INT_IMPL.find(op, opc, (opc_) -> {\n-              switch (opc_) {\n-                case VECTOR_OP_LSHIFT: return (v, n) ->\n-                        v.uOp((i, a) -> (int)(a << n));\n-                case VECTOR_OP_RSHIFT: return (v, n) ->\n-                        v.uOp((i, a) -> (int)(a >> n));\n-                case VECTOR_OP_URSHIFT: return (v, n) ->\n-                        v.uOp((i, a) -> (int)((a & LSHR_SETUP_MASK) >>> n));\n-                case VECTOR_OP_LROTATE: return (v, n) ->\n-                        v.uOp((i, a) -> rotateLeft(a, (int)n));\n-                case VECTOR_OP_RROTATE: return (v, n) ->\n-                        v.uOp((i, a) -> rotateRight(a, (int)n));\n-                default: return null;\n-                }}));\n+            opc, getClass(), null, int.class, length(),\n+            this, e, null,\n+            BIN_INT_IMPL.find(op, opc, IntVector::broadcastIntOperations));\n@@ -825,0 +907,22 @@\n+\n+    \/*package-private*\/\n+    abstract IntVector\n+    lanewiseShift(VectorOperators.Binary op, int e, VectorMask<Integer> m);\n+\n+    \/*package-private*\/\n+    @ForceInline\n+    final IntVector\n+    lanewiseShiftTemplate(VectorOperators.Binary op,\n+                          Class<? extends VectorMask<Integer>> maskClass,\n+                          int e, VectorMask<Integer> m) {\n+        m.check(maskClass, this);\n+        assert(opKind(op, VO_SHIFT));\n+        \/\/ As per shift specification for Java, mask the shift count.\n+        e &= SHIFT_MASK;\n+        int opc = opCode(op);\n+        return VectorSupport.broadcastInt(\n+            opc, getClass(), maskClass, int.class, length(),\n+            this, e, m,\n+            BIN_INT_IMPL.find(op, opc, IntVector::broadcastIntOperations));\n+    }\n+\n@@ -826,1 +930,1 @@\n-    ImplCache<Binary,VectorBroadcastIntOp<IntVector>> BIN_INT_IMPL\n+    ImplCache<Binary,VectorBroadcastIntOp<IntVector, VectorMask<Integer>>> BIN_INT_IMPL\n@@ -829,0 +933,16 @@\n+    private static VectorBroadcastIntOp<IntVector, VectorMask<Integer>> broadcastIntOperations(int opc_) {\n+        switch (opc_) {\n+            case VECTOR_OP_LSHIFT: return (v, n, m) ->\n+                    v.uOp(m, (i, a) -> (int)(a << n));\n+            case VECTOR_OP_RSHIFT: return (v, n, m) ->\n+                    v.uOp(m, (i, a) -> (int)(a >> n));\n+            case VECTOR_OP_URSHIFT: return (v, n, m) ->\n+                    v.uOp(m, (i, a) -> (int)((a & LSHR_SETUP_MASK) >>> n));\n+            case VECTOR_OP_LROTATE: return (v, n, m) ->\n+                    v.uOp(m, (i, a) -> rotateLeft(a, (int)n));\n+            case VECTOR_OP_RROTATE: return (v, n, m) ->\n+                    v.uOp(m, (i, a) -> rotateRight(a, (int)n));\n+            default: return null;\n+        }\n+    }\n+\n@@ -880,6 +1000,3 @@\n-            opc, getClass(), int.class, length(),\n-            this, that, tother,\n-            TERN_IMPL.find(op, opc, (opc_) -> {\n-              switch (opc_) {\n-                default: return null;\n-                }}));\n+            opc, getClass(), null, int.class, length(),\n+            this, that, tother, null,\n+            TERN_IMPL.find(op, opc, IntVector::ternaryOperations));\n@@ -887,3 +1004,0 @@\n-    private static final\n-    ImplCache<Ternary,TernaryOperation<IntVector>> TERN_IMPL\n-        = new ImplCache<>(Ternary.class, IntVector.class);\n@@ -897,2 +1011,2 @@\n-    @ForceInline\n-    public final\n+    @Override\n+    public abstract\n@@ -902,2 +1016,37 @@\n-                                  VectorMask<Integer> m) {\n-        return blend(lanewise(op, v1, v2), m);\n+                                  VectorMask<Integer> m);\n+    @ForceInline\n+    final\n+    IntVector lanewiseTemplate(VectorOperators.Ternary op,\n+                                          Class<? extends VectorMask<Integer>> maskClass,\n+                                          Vector<Integer> v1,\n+                                          Vector<Integer> v2,\n+                                          VectorMask<Integer> m) {\n+        IntVector that = (IntVector) v1;\n+        IntVector tother = (IntVector) v2;\n+        \/\/ It's a word: https:\/\/www.dictionary.com\/browse\/tother\n+        \/\/ See also Chapter 11 of Dickens, Our Mutual Friend:\n+        \/\/ \"Totherest Governor,\" replied Mr Riderhood...\n+        that.check(this);\n+        tother.check(this);\n+        m.check(maskClass, this);\n+\n+        if (op == BITWISE_BLEND) {\n+            \/\/ FIXME: Support this in the JIT.\n+            that = this.lanewise(XOR, that).lanewise(AND, tother);\n+            return this.lanewise(XOR, that, m);\n+        }\n+        int opc = opCode(op);\n+        return VectorSupport.ternaryOp(\n+            opc, getClass(), maskClass, int.class, length(),\n+            this, that, tother, m,\n+            TERN_IMPL.find(op, opc, IntVector::ternaryOperations));\n+    }\n+\n+    private static final\n+    ImplCache<Ternary, TernaryOperation<IntVector, VectorMask<Integer>>>\n+        TERN_IMPL = new ImplCache<>(Ternary.class, IntVector.class);\n+\n+    private static TernaryOperation<IntVector, VectorMask<Integer>> ternaryOperations(int opc_) {\n+        switch (opc_) {\n+            default: return null;\n+        }\n@@ -960,1 +1109,1 @@\n-        return blend(lanewise(op, e1, e2), m);\n+        return lanewise(op, broadcast(e1), broadcast(e2), m);\n@@ -1018,1 +1167,1 @@\n-        return blend(lanewise(op, v1, e2), m);\n+        return lanewise(op, v1, broadcast(e2), m);\n@@ -1075,1 +1224,1 @@\n-        return blend(lanewise(op, e1, v2), m);\n+        return lanewise(op, broadcast(e1), v2, m);\n@@ -1747,2 +1896,0 @@\n-        Objects.requireNonNull(v);\n-        IntSpecies vsp = vspecies();\n@@ -1754,2 +1901,2 @@\n-            this, that,\n-            (cond, v0, v1) -> {\n+            this, that, null,\n+            (cond, v0, v1, m1) -> {\n@@ -1765,0 +1912,22 @@\n+    \/*package-private*\/\n+    @ForceInline\n+    final\n+    <M extends VectorMask<Integer>>\n+    M compareTemplate(Class<M> maskType, Comparison op, Vector<Integer> v, M m) {\n+        IntVector that = (IntVector) v;\n+        that.check(this);\n+        m.check(maskType, this);\n+        int opc = opCode(op);\n+        return VectorSupport.compare(\n+            opc, getClass(), maskType, int.class, length(),\n+            this, that, m,\n+            (cond, v0, v1, m1) -> {\n+                AbstractMask<Integer> cmpM\n+                    = v0.bTest(cond, v1, (cond_, i, a, b)\n+                               -> compareWithOp(cond, a, b));\n+                @SuppressWarnings(\"unchecked\")\n+                M m2 = (M) cmpM.and(m1);\n+                return m2;\n+            });\n+    }\n+\n@@ -1782,12 +1951,0 @@\n-    \/**\n-     * {@inheritDoc} <!--workaround-->\n-     *\/\n-    @Override\n-    @ForceInline\n-    public final\n-    VectorMask<Integer> compare(VectorOperators.Comparison op,\n-                                  Vector<Integer> v,\n-                                  VectorMask<Integer> m) {\n-        return compare(op, v).and(m);\n-    }\n-\n@@ -1852,1 +2009,1 @@\n-        return compare(op, e).and(m);\n+        return compare(op, broadcast(e), m);\n@@ -2103,3 +2260,3 @@\n-            getClass(), shuffletype, int.class, length(),\n-            this, shuffle,\n-            (v1, s_) -> v1.uOp((i, a) -> {\n+            getClass(), shuffletype, null, int.class, length(),\n+            this, shuffle, null,\n+            (v1, s_, m_) -> v1.uOp((i, a) -> {\n@@ -2122,1 +2279,1 @@\n-    <S extends VectorShuffle<Integer>>\n+    <S extends VectorShuffle<Integer>, M extends VectorMask<Integer>>\n@@ -2124,0 +2281,1 @@\n+                                           Class<M> masktype,\n@@ -2125,9 +2283,3 @@\n-                                           VectorMask<Integer> m) {\n-        IntVector unmasked =\n-            VectorSupport.rearrangeOp(\n-                getClass(), shuffletype, int.class, length(),\n-                this, shuffle,\n-                (v1, s_) -> v1.uOp((i, a) -> {\n-                    int ei = s_.laneSource(i);\n-                    return ei < 0 ? 0 : v1.lane(ei);\n-                }));\n+                                           M m) {\n+\n+        m.check(masktype, this);\n@@ -2139,1 +2291,7 @@\n-        return broadcast((int)0).blend(unmasked, m);\n+        return VectorSupport.rearrangeOp(\n+                   getClass(), shuffletype, masktype, int.class, length(),\n+                   this, shuffle, m,\n+                   (v1, s_, m_) -> v1.uOp((i, a) -> {\n+                        int ei = s_.laneSource(i);\n+                        return ei < 0  || !m_.laneIsSet(i) ? 0 : v1.lane(ei);\n+                   }));\n@@ -2162,3 +2320,3 @@\n-                getClass(), shuffletype, int.class, length(),\n-                this, ws,\n-                (v0, s_) -> v0.uOp((i, a) -> {\n+                getClass(), shuffletype, null, int.class, length(),\n+                this, ws, null,\n+                (v0, s_, m_) -> v0.uOp((i, a) -> {\n@@ -2170,3 +2328,3 @@\n-                getClass(), shuffletype, int.class, length(),\n-                v, ws,\n-                (v1, s_) -> v1.uOp((i, a) -> {\n+                getClass(), shuffletype, null, int.class, length(),\n+                v, ws, null,\n+                (v1, s_, m_) -> v1.uOp((i, a) -> {\n@@ -2435,0 +2593,1 @@\n+                               Class<? extends VectorMask<Integer>> maskClass,\n@@ -2436,2 +2595,10 @@\n-        IntVector v = reduceIdentityVector(op).blend(this, m);\n-        return v.reduceLanesTemplate(op);\n+        m.check(maskClass, this);\n+        if (op == FIRST_NONZERO) {\n+            IntVector v = reduceIdentityVector(op).blend(this, m);\n+            return v.reduceLanesTemplate(op);\n+        }\n+        int opc = opCode(op);\n+        return fromBits(VectorSupport.reductionCoerced(\n+            opc, getClass(), maskClass, int.class, length(),\n+            this, m,\n+            REDUCE_IMPL.find(op, opc, IntVector::reductionOperations)));\n@@ -2452,20 +2619,3 @@\n-            opc, getClass(), int.class, length(),\n-            this,\n-            REDUCE_IMPL.find(op, opc, (opc_) -> {\n-              switch (opc_) {\n-              case VECTOR_OP_ADD: return v ->\n-                      toBits(v.rOp((int)0, (i, a, b) -> (int)(a + b)));\n-              case VECTOR_OP_MUL: return v ->\n-                      toBits(v.rOp((int)1, (i, a, b) -> (int)(a * b)));\n-              case VECTOR_OP_MIN: return v ->\n-                      toBits(v.rOp(MAX_OR_INF, (i, a, b) -> (int) Math.min(a, b)));\n-              case VECTOR_OP_MAX: return v ->\n-                      toBits(v.rOp(MIN_OR_INF, (i, a, b) -> (int) Math.max(a, b)));\n-              case VECTOR_OP_AND: return v ->\n-                      toBits(v.rOp((int)-1, (i, a, b) -> (int)(a & b)));\n-              case VECTOR_OP_OR: return v ->\n-                      toBits(v.rOp((int)0, (i, a, b) -> (int)(a | b)));\n-              case VECTOR_OP_XOR: return v ->\n-                      toBits(v.rOp((int)0, (i, a, b) -> (int)(a ^ b)));\n-              default: return null;\n-              }})));\n+            opc, getClass(), null, int.class, length(),\n+            this, null,\n+            REDUCE_IMPL.find(op, opc, IntVector::reductionOperations)));\n@@ -2473,0 +2623,1 @@\n+\n@@ -2474,2 +2625,22 @@\n-    ImplCache<Associative,Function<IntVector,Long>> REDUCE_IMPL\n-        = new ImplCache<>(Associative.class, IntVector.class);\n+    ImplCache<Associative, ReductionOperation<IntVector, VectorMask<Integer>>>\n+        REDUCE_IMPL = new ImplCache<>(Associative.class, IntVector.class);\n+\n+    private static ReductionOperation<IntVector, VectorMask<Integer>> reductionOperations(int opc_) {\n+        switch (opc_) {\n+            case VECTOR_OP_ADD: return (v, m) ->\n+                    toBits(v.rOp((int)0, m, (i, a, b) -> (int)(a + b)));\n+            case VECTOR_OP_MUL: return (v, m) ->\n+                    toBits(v.rOp((int)1, m, (i, a, b) -> (int)(a * b)));\n+            case VECTOR_OP_MIN: return (v, m) ->\n+                    toBits(v.rOp(MAX_OR_INF, m, (i, a, b) -> (int) Math.min(a, b)));\n+            case VECTOR_OP_MAX: return (v, m) ->\n+                    toBits(v.rOp(MIN_OR_INF, m, (i, a, b) -> (int) Math.max(a, b)));\n+            case VECTOR_OP_AND: return (v, m) ->\n+                    toBits(v.rOp((int)-1, m, (i, a, b) -> (int)(a & b)));\n+            case VECTOR_OP_OR: return (v, m) ->\n+                    toBits(v.rOp((int)0, m, (i, a, b) -> (int)(a | b)));\n+            case VECTOR_OP_XOR: return (v, m) ->\n+                    toBits(v.rOp((int)0, m, (i, a, b) -> (int)(a ^ b)));\n+            default: return null;\n+        }\n+    }\n@@ -2694,3 +2865,1 @@\n-            IntVector zero = vsp.zero();\n-            IntVector v = zero.fromByteArray0(a, offset);\n-            return zero.blend(v.maybeSwap(bo), m);\n+            return vsp.dummyVector().fromByteArray0(a, offset, m).maybeSwap(bo);\n@@ -2758,2 +2927,1 @@\n-            IntVector zero = vsp.zero();\n-            return zero.blend(zero.fromArray0(a, offset), m);\n+            return vsp.dummyVector().fromArray0(a, offset, m);\n@@ -2817,3 +2985,3 @@\n-            vectorType, int.class, vsp.laneCount(),\n-            IntVector.species(vsp.indexShape()).vectorType(),\n-            a, ARRAY_BASE, vix,\n+            vectorType, null, int.class, vsp.laneCount(),\n+            isp.vectorType(),\n+            a, ARRAY_BASE, vix, null,\n@@ -2821,1 +2989,1 @@\n-            (int[] c, int idx, int[] iMap, int idy, IntSpecies s) ->\n+            (c, idx, iMap, idy, s, vm) ->\n@@ -2823,1 +2991,1 @@\n-        }\n+    }\n@@ -2871,1 +3039,0 @@\n-            \/\/ FIXME: Cannot vectorize yet, if there's a mask.\n@@ -2873,1 +3040,1 @@\n-            return vsp.vOp(m, n -> a[offset + indexMap[mapOffset + n]]);\n+            return vsp.dummyVector().fromArray0(a, offset, indexMap, mapOffset, m);\n@@ -2967,3 +3134,1 @@\n-            IntVector zero = vsp.zero();\n-            IntVector v = zero.fromByteBuffer0(bb, offset);\n-            return zero.blend(v.maybeSwap(bo), m);\n+            return vsp.dummyVector().fromByteBuffer0(bb, offset, m).maybeSwap(bo);\n@@ -3041,1 +3206,0 @@\n-            \/\/ FIXME: optimize\n@@ -3044,1 +3208,1 @@\n-            stOp(a, offset, m, (arr, off, i, v) -> arr[off+i] = v);\n+            intoArray0(a, offset, m);\n@@ -3088,1 +3252,1 @@\n-            vsp.vectorType(), vsp.elementType(), vsp.laneCount(),\n+            vsp.vectorType(), null, vsp.elementType(), vsp.laneCount(),\n@@ -3091,1 +3255,1 @@\n-            this,\n+            this, null,\n@@ -3093,1 +3257,1 @@\n-            (arr, off, v, map, mo)\n+            (arr, off, v, map, mo, vm)\n@@ -3140,6 +3304,1 @@\n-            \/\/ FIXME: Cannot vectorize yet, if there's a mask.\n-            stOp(a, offset, m,\n-                 (arr, off, i, e) -> {\n-                     int j = indexMap[mapOffset + i];\n-                     arr[off + j] = e;\n-                 });\n+            intoArray0(a, offset, indexMap, mapOffset, m);\n@@ -3175,1 +3334,0 @@\n-            \/\/ FIXME: optimize\n@@ -3178,3 +3336,1 @@\n-            ByteBuffer wb = wrapper(a, bo);\n-            this.stOp(wb, offset, m,\n-                    (wb_, o, i, e) -> wb_.putInt(o + i * 4, e));\n+            maybeSwap(bo).intoByteArray0(a, offset, m);\n@@ -3192,1 +3348,1 @@\n-        if (bb.isReadOnly()) {\n+        if (ScopedMemoryAccess.isReadOnly(bb)) {\n@@ -3211,1 +3367,0 @@\n-            \/\/ FIXME: optimize\n@@ -3217,3 +3372,1 @@\n-            ByteBuffer wb = wrapper(bb, bo);\n-            this.stOp(wb, offset, m,\n-                    (wb_, o, i, e) -> wb_.putInt(o + i * 4, e));\n+            maybeSwap(bo).intoByteBuffer0(bb, offset, m);\n@@ -3257,0 +3410,51 @@\n+    \/*package-private*\/\n+    abstract\n+    IntVector fromArray0(int[] a, int offset, VectorMask<Integer> m);\n+    @ForceInline\n+    final\n+    <M extends VectorMask<Integer>>\n+    IntVector fromArray0Template(Class<M> maskClass, int[] a, int offset, M m) {\n+        m.check(species());\n+        IntSpecies vsp = vspecies();\n+        return VectorSupport.loadMasked(\n+            vsp.vectorType(), maskClass, vsp.elementType(), vsp.laneCount(),\n+            a, arrayAddress(a, offset), m,\n+            a, offset, vsp,\n+            (arr, off, s, vm) -> s.ldOp(arr, off, vm,\n+                                        (arr_, off_, i) -> arr_[off_ + i]));\n+    }\n+\n+    \/*package-private*\/\n+    abstract\n+    IntVector fromArray0(int[] a, int offset,\n+                                    int[] indexMap, int mapOffset,\n+                                    VectorMask<Integer> m);\n+    @ForceInline\n+    final\n+    <M extends VectorMask<Integer>>\n+    IntVector fromArray0Template(Class<M> maskClass, int[] a, int offset,\n+                                            int[] indexMap, int mapOffset, M m) {\n+        IntSpecies vsp = vspecies();\n+        IntVector.IntSpecies isp = IntVector.species(vsp.indexShape());\n+        Objects.requireNonNull(a);\n+        Objects.requireNonNull(indexMap);\n+        m.check(vsp);\n+        Class<? extends IntVector> vectorType = vsp.vectorType();\n+\n+        \/\/ Index vector: vix[0:n] = k -> offset + indexMap[mapOffset + k]\n+        IntVector vix = IntVector\n+            .fromArray(isp, indexMap, mapOffset)\n+            .add(offset);\n+\n+        \/\/ FIXME: Check index under mask controlling.\n+        vix = VectorIntrinsics.checkIndex(vix, a.length);\n+\n+        return VectorSupport.loadWithMap(\n+            vectorType, maskClass, int.class, vsp.laneCount(),\n+            isp.vectorType(),\n+            a, ARRAY_BASE, vix, m,\n+            a, offset, indexMap, mapOffset, vsp,\n+            (c, idx, iMap, idy, s, vm) ->\n+            s.vOp(vm, n -> c[idx + iMap[idy+n]]));\n+    }\n+\n@@ -3277,0 +3481,19 @@\n+    abstract\n+    IntVector fromByteArray0(byte[] a, int offset, VectorMask<Integer> m);\n+    @ForceInline\n+    final\n+    <M extends VectorMask<Integer>>\n+    IntVector fromByteArray0Template(Class<M> maskClass, byte[] a, int offset, M m) {\n+        IntSpecies vsp = vspecies();\n+        m.check(vsp);\n+        return VectorSupport.loadMasked(\n+            vsp.vectorType(), maskClass, vsp.elementType(), vsp.laneCount(),\n+            a, byteArrayAddress(a, offset), m,\n+            a, offset, vsp,\n+            (arr, off, s, vm) -> {\n+                ByteBuffer wb = wrapper(arr, NATIVE_ENDIAN);\n+                return s.ldOp(wb, off, vm,\n+                        (wb_, o, i) -> wb_.getInt(o + i * 4));\n+            });\n+    }\n+\n@@ -3293,0 +3516,18 @@\n+    abstract\n+    IntVector fromByteBuffer0(ByteBuffer bb, int offset, VectorMask<Integer> m);\n+    @ForceInline\n+    final\n+    <M extends VectorMask<Integer>>\n+    IntVector fromByteBuffer0Template(Class<M> maskClass, ByteBuffer bb, int offset, M m) {\n+        IntSpecies vsp = vspecies();\n+        m.check(vsp);\n+        return ScopedMemoryAccess.loadFromByteBufferMasked(\n+                vsp.vectorType(), maskClass, vsp.elementType(), vsp.laneCount(),\n+                bb, offset, m, vsp,\n+                (buf, off, s, vm) -> {\n+                    ByteBuffer wb = wrapper(buf, NATIVE_ENDIAN);\n+                    return s.ldOp(wb, off, vm,\n+                            (wb_, o, i) -> wb_.getInt(o + i * 4));\n+                });\n+    }\n+\n@@ -3312,0 +3553,52 @@\n+    abstract\n+    void intoArray0(int[] a, int offset, VectorMask<Integer> m);\n+    @ForceInline\n+    final\n+    <M extends VectorMask<Integer>>\n+    void intoArray0Template(Class<M> maskClass, int[] a, int offset, M m) {\n+        m.check(species());\n+        IntSpecies vsp = vspecies();\n+        VectorSupport.storeMasked(\n+            vsp.vectorType(), maskClass, vsp.elementType(), vsp.laneCount(),\n+            a, arrayAddress(a, offset),\n+            this, m, a, offset,\n+            (arr, off, v, vm)\n+            -> v.stOp(arr, off, vm,\n+                      (arr_, off_, i, e) -> arr_[off_ + i] = e));\n+    }\n+\n+    abstract\n+    void intoArray0(int[] a, int offset,\n+                    int[] indexMap, int mapOffset,\n+                    VectorMask<Integer> m);\n+    @ForceInline\n+    final\n+    <M extends VectorMask<Integer>>\n+    void intoArray0Template(Class<M> maskClass, int[] a, int offset,\n+                            int[] indexMap, int mapOffset, M m) {\n+        m.check(species());\n+        IntSpecies vsp = vspecies();\n+        IntVector.IntSpecies isp = IntVector.species(vsp.indexShape());\n+        \/\/ Index vector: vix[0:n] = i -> offset + indexMap[mo + i]\n+        IntVector vix = IntVector\n+            .fromArray(isp, indexMap, mapOffset)\n+            .add(offset);\n+\n+        \/\/ FIXME: Check index under mask controlling.\n+        vix = VectorIntrinsics.checkIndex(vix, a.length);\n+\n+        VectorSupport.storeWithMap(\n+            vsp.vectorType(), maskClass, vsp.elementType(), vsp.laneCount(),\n+            isp.vectorType(),\n+            a, arrayAddress(a, 0), vix,\n+            this, m,\n+            a, offset, indexMap, mapOffset,\n+            (arr, off, v, map, mo, vm)\n+            -> v.stOp(arr, off, vm,\n+                      (arr_, off_, i, e) -> {\n+                          int j = map[mo + i];\n+                          arr[off + j] = e;\n+                      }));\n+    }\n+\n+\n@@ -3329,0 +3622,19 @@\n+    abstract\n+    void intoByteArray0(byte[] a, int offset, VectorMask<Integer> m);\n+    @ForceInline\n+    final\n+    <M extends VectorMask<Integer>>\n+    void intoByteArray0Template(Class<M> maskClass, byte[] a, int offset, M m) {\n+        IntSpecies vsp = vspecies();\n+        m.check(vsp);\n+        VectorSupport.storeMasked(\n+            vsp.vectorType(), maskClass, vsp.elementType(), vsp.laneCount(),\n+            a, byteArrayAddress(a, offset),\n+            this, m, a, offset,\n+            (arr, off, v, vm) -> {\n+                ByteBuffer wb = wrapper(arr, NATIVE_ENDIAN);\n+                v.stOp(wb, off, vm,\n+                        (tb_, o, i, e) -> tb_.putInt(o + i * 4, e));\n+            });\n+    }\n+\n@@ -3343,0 +3655,19 @@\n+    abstract\n+    void intoByteBuffer0(ByteBuffer bb, int offset, VectorMask<Integer> m);\n+    @ForceInline\n+    final\n+    <M extends VectorMask<Integer>>\n+    void intoByteBuffer0Template(Class<M> maskClass, ByteBuffer bb, int offset, M m) {\n+        IntSpecies vsp = vspecies();\n+        m.check(vsp);\n+        ScopedMemoryAccess.storeIntoByteBufferMasked(\n+                vsp.vectorType(), maskClass, vsp.elementType(), vsp.laneCount(),\n+                this, m, bb, offset,\n+                (buf, off, v, vm) -> {\n+                    ByteBuffer wb = wrapper(buf, NATIVE_ENDIAN);\n+                    v.stOp(wb, off, vm,\n+                            (wb_, o, i, e) -> wb_.putInt(o + i * 4, e));\n+                });\n+    }\n+\n+\n@@ -3660,1 +3991,1 @@\n-                                      AbstractMask<Integer> m,\n+                                      VectorMask<Integer> m,\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/IntVector.java","additions":535,"deletions":204,"binary":false,"changes":739,"status":"modified"},{"patch":"@@ -234,2 +234,2 @@\n-    long rOp(long v, FBinOp f) {\n-        return super.rOpTemplate(v, f);  \/\/ specialize\n+    long rOp(long v, VectorMask<Long> m, FBinOp f) {\n+        return super.rOpTemplate(v, m, f);  \/\/ specialize\n@@ -271,0 +271,6 @@\n+    @Override\n+    @ForceInline\n+    public Long128Vector lanewise(Unary op, VectorMask<Long> m) {\n+        return (Long128Vector) super.lanewiseTemplate(op, Long128Mask.class, (Long128Mask) m);  \/\/ specialize\n+    }\n+\n@@ -277,0 +283,6 @@\n+    @Override\n+    @ForceInline\n+    public Long128Vector lanewise(Binary op, Vector<Long> v, VectorMask<Long> m) {\n+        return (Long128Vector) super.lanewiseTemplate(op, Long128Mask.class, v, (Long128Mask) m);  \/\/ specialize\n+    }\n+\n@@ -284,0 +296,7 @@\n+    \/*package-private*\/\n+    @Override\n+    @ForceInline Long128Vector\n+    lanewiseShift(VectorOperators.Binary op, int e, VectorMask<Long> m) {\n+        return (Long128Vector) super.lanewiseShiftTemplate(op, Long128Mask.class, e, (Long128Mask) m);  \/\/ specialize\n+    }\n+\n@@ -289,1 +308,1 @@\n-    lanewise(VectorOperators.Ternary op, Vector<Long> v1, Vector<Long> v2) {\n+    lanewise(Ternary op, Vector<Long> v1, Vector<Long> v2) {\n@@ -293,0 +312,8 @@\n+    @Override\n+    @ForceInline\n+    public final\n+    Long128Vector\n+    lanewise(Ternary op, Vector<Long> v1, Vector<Long> v2, VectorMask<Long> m) {\n+        return (Long128Vector) super.lanewiseTemplate(op, Long128Mask.class, v1, v2, (Long128Mask) m);  \/\/ specialize\n+    }\n+\n@@ -312,1 +339,1 @@\n-        return super.reduceLanesTemplate(op, m);  \/\/ specialized\n+        return super.reduceLanesTemplate(op, Long128Mask.class, (Long128Mask) m);  \/\/ specialized\n@@ -325,1 +352,1 @@\n-        return (long) super.reduceLanesTemplate(op, m);  \/\/ specialized\n+        return (long) super.reduceLanesTemplate(op, Long128Mask.class, (Long128Mask) m);  \/\/ specialized\n@@ -356,0 +383,7 @@\n+    @Override\n+    @ForceInline\n+    public final Long128Mask compare(Comparison op, Vector<Long> v, VectorMask<Long> m) {\n+        return super.compareTemplate(Long128Mask.class, op, v, (Long128Mask) m);\n+    }\n+\n+\n@@ -412,0 +446,1 @@\n+                                    Long128Mask.class,\n@@ -577,10 +612,6 @@\n-            if (VSIZE == species.vectorBitSize()) {\n-                Class<?> dtype = species.elementType();\n-                Class<?> dmtype = species.maskType();\n-                return VectorSupport.convert(VectorSupport.VECTOR_OP_REINTERPRET,\n-                    this.getClass(), ETYPE, VLENGTH,\n-                    dmtype, dtype, VLENGTH,\n-                    this, species,\n-                    Long128Mask::defaultMaskCast);\n-            }\n-            return this.defaultMaskCast(species);\n+\n+            return VectorSupport.convert(VectorSupport.VECTOR_OP_CAST,\n+                this.getClass(), ETYPE, VLENGTH,\n+                species.maskType(), species.elementType(), VLENGTH,\n+                this, species,\n+                (m, s) -> s.maskFactory(m.toArray()).check(s));\n@@ -612,3 +643,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_AND, Long128Mask.class, long.class, VLENGTH,\n-                                             this, m,\n-                                             (m1, m2) -> m1.bOp(m2, (i, a, b) -> a & b));\n+            return VectorSupport.binaryOp(VECTOR_OP_AND, Long128Mask.class, null, long.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a & b));\n@@ -622,3 +653,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_OR, Long128Mask.class, long.class, VLENGTH,\n-                                             this, m,\n-                                             (m1, m2) -> m1.bOp(m2, (i, a, b) -> a | b));\n+            return VectorSupport.binaryOp(VECTOR_OP_OR, Long128Mask.class, null, long.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a | b));\n@@ -632,3 +663,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_XOR, Long128Mask.class, long.class, VLENGTH,\n-                                          this, m,\n-                                          (m1, m2) -> m1.bOp(m2, (i, a, b) -> a ^ b));\n+            return VectorSupport.binaryOp(VECTOR_OP_XOR, Long128Mask.class, null, long.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a ^ b));\n@@ -642,2 +673,2 @@\n-            return VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_TRUECOUNT, Long128Mask.class, long.class, VLENGTH, this,\n-                                                      (m) -> trueCountHelper(((Long128Mask)m).getBits()));\n+            return (int) VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_TRUECOUNT, Long128Mask.class, long.class, VLENGTH, this,\n+                                                      (m) -> trueCountHelper(m.getBits()));\n@@ -649,2 +680,2 @@\n-            return VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_FIRSTTRUE, Long128Mask.class, long.class, VLENGTH, this,\n-                                                      (m) -> firstTrueHelper(((Long128Mask)m).getBits()));\n+            return (int) VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_FIRSTTRUE, Long128Mask.class, long.class, VLENGTH, this,\n+                                                      (m) -> firstTrueHelper(m.getBits()));\n@@ -656,2 +687,12 @@\n-            return VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_LASTTRUE, Long128Mask.class, long.class, VLENGTH, this,\n-                                                      (m) -> lastTrueHelper(((Long128Mask)m).getBits()));\n+            return (int) VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_LASTTRUE, Long128Mask.class, long.class, VLENGTH, this,\n+                                                      (m) -> lastTrueHelper(m.getBits()));\n+        }\n+\n+        @Override\n+        @ForceInline\n+        public long toLong() {\n+            if (length() > Long.SIZE) {\n+                throw new UnsupportedOperationException(\"too many lanes for one long\");\n+            }\n+            return VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_TOLONG, Long128Mask.class, long.class, VLENGTH, this,\n+                                                      (m) -> toLongHelper(m.getBits()));\n@@ -768,0 +809,14 @@\n+    @ForceInline\n+    @Override\n+    final\n+    LongVector fromArray0(long[] a, int offset, VectorMask<Long> m) {\n+        return super.fromArray0Template(Long128Mask.class, a, offset, (Long128Mask) m);  \/\/ specialize\n+    }\n+\n+    @ForceInline\n+    @Override\n+    final\n+    LongVector fromArray0(long[] a, int offset, int[] indexMap, int mapOffset, VectorMask<Long> m) {\n+        return super.fromArray0Template(Long128Mask.class, a, offset, indexMap, mapOffset, (Long128Mask) m);\n+    }\n+\n@@ -777,0 +832,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    LongVector fromByteArray0(byte[] a, int offset, VectorMask<Long> m) {\n+        return super.fromByteArray0Template(Long128Mask.class, a, offset, (Long128Mask) m);  \/\/ specialize\n+    }\n+\n@@ -784,0 +846,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    LongVector fromByteBuffer0(ByteBuffer bb, int offset, VectorMask<Long> m) {\n+        return super.fromByteBuffer0Template(Long128Mask.class, bb, offset, (Long128Mask) m);  \/\/ specialize\n+    }\n+\n@@ -791,0 +860,15 @@\n+    @ForceInline\n+    @Override\n+    final\n+    void intoArray0(long[] a, int offset, VectorMask<Long> m) {\n+        super.intoArray0Template(Long128Mask.class, a, offset, (Long128Mask) m);\n+    }\n+\n+    @ForceInline\n+    @Override\n+    final\n+    void intoArray0(long[] a, int offset, int[] indexMap, int mapOffset, VectorMask<Long> m) {\n+        super.intoArray0Template(Long128Mask.class, a, offset, indexMap, mapOffset, (Long128Mask) m);\n+    }\n+\n+\n@@ -798,0 +882,15 @@\n+    @ForceInline\n+    @Override\n+    final\n+    void intoByteArray0(byte[] a, int offset, VectorMask<Long> m) {\n+        super.intoByteArray0Template(Long128Mask.class, a, offset, (Long128Mask) m);  \/\/ specialize\n+    }\n+\n+    @ForceInline\n+    @Override\n+    final\n+    void intoByteBuffer0(ByteBuffer bb, int offset, VectorMask<Long> m) {\n+        super.intoByteBuffer0Template(Long128Mask.class, bb, offset, (Long128Mask) m);\n+    }\n+\n+\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/Long128Vector.java","additions":129,"deletions":30,"binary":false,"changes":159,"status":"modified"},{"patch":"@@ -234,2 +234,2 @@\n-    long rOp(long v, FBinOp f) {\n-        return super.rOpTemplate(v, f);  \/\/ specialize\n+    long rOp(long v, VectorMask<Long> m, FBinOp f) {\n+        return super.rOpTemplate(v, m, f);  \/\/ specialize\n@@ -271,0 +271,6 @@\n+    @Override\n+    @ForceInline\n+    public Long256Vector lanewise(Unary op, VectorMask<Long> m) {\n+        return (Long256Vector) super.lanewiseTemplate(op, Long256Mask.class, (Long256Mask) m);  \/\/ specialize\n+    }\n+\n@@ -277,0 +283,6 @@\n+    @Override\n+    @ForceInline\n+    public Long256Vector lanewise(Binary op, Vector<Long> v, VectorMask<Long> m) {\n+        return (Long256Vector) super.lanewiseTemplate(op, Long256Mask.class, v, (Long256Mask) m);  \/\/ specialize\n+    }\n+\n@@ -284,0 +296,7 @@\n+    \/*package-private*\/\n+    @Override\n+    @ForceInline Long256Vector\n+    lanewiseShift(VectorOperators.Binary op, int e, VectorMask<Long> m) {\n+        return (Long256Vector) super.lanewiseShiftTemplate(op, Long256Mask.class, e, (Long256Mask) m);  \/\/ specialize\n+    }\n+\n@@ -289,1 +308,1 @@\n-    lanewise(VectorOperators.Ternary op, Vector<Long> v1, Vector<Long> v2) {\n+    lanewise(Ternary op, Vector<Long> v1, Vector<Long> v2) {\n@@ -293,0 +312,8 @@\n+    @Override\n+    @ForceInline\n+    public final\n+    Long256Vector\n+    lanewise(Ternary op, Vector<Long> v1, Vector<Long> v2, VectorMask<Long> m) {\n+        return (Long256Vector) super.lanewiseTemplate(op, Long256Mask.class, v1, v2, (Long256Mask) m);  \/\/ specialize\n+    }\n+\n@@ -312,1 +339,1 @@\n-        return super.reduceLanesTemplate(op, m);  \/\/ specialized\n+        return super.reduceLanesTemplate(op, Long256Mask.class, (Long256Mask) m);  \/\/ specialized\n@@ -325,1 +352,1 @@\n-        return (long) super.reduceLanesTemplate(op, m);  \/\/ specialized\n+        return (long) super.reduceLanesTemplate(op, Long256Mask.class, (Long256Mask) m);  \/\/ specialized\n@@ -356,0 +383,7 @@\n+    @Override\n+    @ForceInline\n+    public final Long256Mask compare(Comparison op, Vector<Long> v, VectorMask<Long> m) {\n+        return super.compareTemplate(Long256Mask.class, op, v, (Long256Mask) m);\n+    }\n+\n+\n@@ -412,0 +446,1 @@\n+                                    Long256Mask.class,\n@@ -581,10 +616,6 @@\n-            if (VSIZE == species.vectorBitSize()) {\n-                Class<?> dtype = species.elementType();\n-                Class<?> dmtype = species.maskType();\n-                return VectorSupport.convert(VectorSupport.VECTOR_OP_REINTERPRET,\n-                    this.getClass(), ETYPE, VLENGTH,\n-                    dmtype, dtype, VLENGTH,\n-                    this, species,\n-                    Long256Mask::defaultMaskCast);\n-            }\n-            return this.defaultMaskCast(species);\n+\n+            return VectorSupport.convert(VectorSupport.VECTOR_OP_CAST,\n+                this.getClass(), ETYPE, VLENGTH,\n+                species.maskType(), species.elementType(), VLENGTH,\n+                this, species,\n+                (m, s) -> s.maskFactory(m.toArray()).check(s));\n@@ -616,3 +647,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_AND, Long256Mask.class, long.class, VLENGTH,\n-                                             this, m,\n-                                             (m1, m2) -> m1.bOp(m2, (i, a, b) -> a & b));\n+            return VectorSupport.binaryOp(VECTOR_OP_AND, Long256Mask.class, null, long.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a & b));\n@@ -626,3 +657,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_OR, Long256Mask.class, long.class, VLENGTH,\n-                                             this, m,\n-                                             (m1, m2) -> m1.bOp(m2, (i, a, b) -> a | b));\n+            return VectorSupport.binaryOp(VECTOR_OP_OR, Long256Mask.class, null, long.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a | b));\n@@ -636,3 +667,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_XOR, Long256Mask.class, long.class, VLENGTH,\n-                                          this, m,\n-                                          (m1, m2) -> m1.bOp(m2, (i, a, b) -> a ^ b));\n+            return VectorSupport.binaryOp(VECTOR_OP_XOR, Long256Mask.class, null, long.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a ^ b));\n@@ -646,2 +677,2 @@\n-            return VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_TRUECOUNT, Long256Mask.class, long.class, VLENGTH, this,\n-                                                      (m) -> trueCountHelper(((Long256Mask)m).getBits()));\n+            return (int) VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_TRUECOUNT, Long256Mask.class, long.class, VLENGTH, this,\n+                                                      (m) -> trueCountHelper(m.getBits()));\n@@ -653,2 +684,2 @@\n-            return VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_FIRSTTRUE, Long256Mask.class, long.class, VLENGTH, this,\n-                                                      (m) -> firstTrueHelper(((Long256Mask)m).getBits()));\n+            return (int) VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_FIRSTTRUE, Long256Mask.class, long.class, VLENGTH, this,\n+                                                      (m) -> firstTrueHelper(m.getBits()));\n@@ -660,2 +691,12 @@\n-            return VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_LASTTRUE, Long256Mask.class, long.class, VLENGTH, this,\n-                                                      (m) -> lastTrueHelper(((Long256Mask)m).getBits()));\n+            return (int) VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_LASTTRUE, Long256Mask.class, long.class, VLENGTH, this,\n+                                                      (m) -> lastTrueHelper(m.getBits()));\n+        }\n+\n+        @Override\n+        @ForceInline\n+        public long toLong() {\n+            if (length() > Long.SIZE) {\n+                throw new UnsupportedOperationException(\"too many lanes for one long\");\n+            }\n+            return VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_TOLONG, Long256Mask.class, long.class, VLENGTH, this,\n+                                                      (m) -> toLongHelper(m.getBits()));\n@@ -772,0 +813,14 @@\n+    @ForceInline\n+    @Override\n+    final\n+    LongVector fromArray0(long[] a, int offset, VectorMask<Long> m) {\n+        return super.fromArray0Template(Long256Mask.class, a, offset, (Long256Mask) m);  \/\/ specialize\n+    }\n+\n+    @ForceInline\n+    @Override\n+    final\n+    LongVector fromArray0(long[] a, int offset, int[] indexMap, int mapOffset, VectorMask<Long> m) {\n+        return super.fromArray0Template(Long256Mask.class, a, offset, indexMap, mapOffset, (Long256Mask) m);\n+    }\n+\n@@ -781,0 +836,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    LongVector fromByteArray0(byte[] a, int offset, VectorMask<Long> m) {\n+        return super.fromByteArray0Template(Long256Mask.class, a, offset, (Long256Mask) m);  \/\/ specialize\n+    }\n+\n@@ -788,0 +850,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    LongVector fromByteBuffer0(ByteBuffer bb, int offset, VectorMask<Long> m) {\n+        return super.fromByteBuffer0Template(Long256Mask.class, bb, offset, (Long256Mask) m);  \/\/ specialize\n+    }\n+\n@@ -795,0 +864,15 @@\n+    @ForceInline\n+    @Override\n+    final\n+    void intoArray0(long[] a, int offset, VectorMask<Long> m) {\n+        super.intoArray0Template(Long256Mask.class, a, offset, (Long256Mask) m);\n+    }\n+\n+    @ForceInline\n+    @Override\n+    final\n+    void intoArray0(long[] a, int offset, int[] indexMap, int mapOffset, VectorMask<Long> m) {\n+        super.intoArray0Template(Long256Mask.class, a, offset, indexMap, mapOffset, (Long256Mask) m);\n+    }\n+\n+\n@@ -802,0 +886,15 @@\n+    @ForceInline\n+    @Override\n+    final\n+    void intoByteArray0(byte[] a, int offset, VectorMask<Long> m) {\n+        super.intoByteArray0Template(Long256Mask.class, a, offset, (Long256Mask) m);  \/\/ specialize\n+    }\n+\n+    @ForceInline\n+    @Override\n+    final\n+    void intoByteBuffer0(ByteBuffer bb, int offset, VectorMask<Long> m) {\n+        super.intoByteBuffer0Template(Long256Mask.class, bb, offset, (Long256Mask) m);\n+    }\n+\n+\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/Long256Vector.java","additions":129,"deletions":30,"binary":false,"changes":159,"status":"modified"},{"patch":"@@ -234,2 +234,2 @@\n-    long rOp(long v, FBinOp f) {\n-        return super.rOpTemplate(v, f);  \/\/ specialize\n+    long rOp(long v, VectorMask<Long> m, FBinOp f) {\n+        return super.rOpTemplate(v, m, f);  \/\/ specialize\n@@ -271,0 +271,6 @@\n+    @Override\n+    @ForceInline\n+    public Long512Vector lanewise(Unary op, VectorMask<Long> m) {\n+        return (Long512Vector) super.lanewiseTemplate(op, Long512Mask.class, (Long512Mask) m);  \/\/ specialize\n+    }\n+\n@@ -277,0 +283,6 @@\n+    @Override\n+    @ForceInline\n+    public Long512Vector lanewise(Binary op, Vector<Long> v, VectorMask<Long> m) {\n+        return (Long512Vector) super.lanewiseTemplate(op, Long512Mask.class, v, (Long512Mask) m);  \/\/ specialize\n+    }\n+\n@@ -284,0 +296,7 @@\n+    \/*package-private*\/\n+    @Override\n+    @ForceInline Long512Vector\n+    lanewiseShift(VectorOperators.Binary op, int e, VectorMask<Long> m) {\n+        return (Long512Vector) super.lanewiseShiftTemplate(op, Long512Mask.class, e, (Long512Mask) m);  \/\/ specialize\n+    }\n+\n@@ -289,1 +308,1 @@\n-    lanewise(VectorOperators.Ternary op, Vector<Long> v1, Vector<Long> v2) {\n+    lanewise(Ternary op, Vector<Long> v1, Vector<Long> v2) {\n@@ -293,0 +312,8 @@\n+    @Override\n+    @ForceInline\n+    public final\n+    Long512Vector\n+    lanewise(Ternary op, Vector<Long> v1, Vector<Long> v2, VectorMask<Long> m) {\n+        return (Long512Vector) super.lanewiseTemplate(op, Long512Mask.class, v1, v2, (Long512Mask) m);  \/\/ specialize\n+    }\n+\n@@ -312,1 +339,1 @@\n-        return super.reduceLanesTemplate(op, m);  \/\/ specialized\n+        return super.reduceLanesTemplate(op, Long512Mask.class, (Long512Mask) m);  \/\/ specialized\n@@ -325,1 +352,1 @@\n-        return (long) super.reduceLanesTemplate(op, m);  \/\/ specialized\n+        return (long) super.reduceLanesTemplate(op, Long512Mask.class, (Long512Mask) m);  \/\/ specialized\n@@ -356,0 +383,7 @@\n+    @Override\n+    @ForceInline\n+    public final Long512Mask compare(Comparison op, Vector<Long> v, VectorMask<Long> m) {\n+        return super.compareTemplate(Long512Mask.class, op, v, (Long512Mask) m);\n+    }\n+\n+\n@@ -412,0 +446,1 @@\n+                                    Long512Mask.class,\n@@ -589,10 +624,6 @@\n-            if (VSIZE == species.vectorBitSize()) {\n-                Class<?> dtype = species.elementType();\n-                Class<?> dmtype = species.maskType();\n-                return VectorSupport.convert(VectorSupport.VECTOR_OP_REINTERPRET,\n-                    this.getClass(), ETYPE, VLENGTH,\n-                    dmtype, dtype, VLENGTH,\n-                    this, species,\n-                    Long512Mask::defaultMaskCast);\n-            }\n-            return this.defaultMaskCast(species);\n+\n+            return VectorSupport.convert(VectorSupport.VECTOR_OP_CAST,\n+                this.getClass(), ETYPE, VLENGTH,\n+                species.maskType(), species.elementType(), VLENGTH,\n+                this, species,\n+                (m, s) -> s.maskFactory(m.toArray()).check(s));\n@@ -624,3 +655,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_AND, Long512Mask.class, long.class, VLENGTH,\n-                                             this, m,\n-                                             (m1, m2) -> m1.bOp(m2, (i, a, b) -> a & b));\n+            return VectorSupport.binaryOp(VECTOR_OP_AND, Long512Mask.class, null, long.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a & b));\n@@ -634,3 +665,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_OR, Long512Mask.class, long.class, VLENGTH,\n-                                             this, m,\n-                                             (m1, m2) -> m1.bOp(m2, (i, a, b) -> a | b));\n+            return VectorSupport.binaryOp(VECTOR_OP_OR, Long512Mask.class, null, long.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a | b));\n@@ -644,3 +675,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_XOR, Long512Mask.class, long.class, VLENGTH,\n-                                          this, m,\n-                                          (m1, m2) -> m1.bOp(m2, (i, a, b) -> a ^ b));\n+            return VectorSupport.binaryOp(VECTOR_OP_XOR, Long512Mask.class, null, long.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a ^ b));\n@@ -654,2 +685,2 @@\n-            return VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_TRUECOUNT, Long512Mask.class, long.class, VLENGTH, this,\n-                                                      (m) -> trueCountHelper(((Long512Mask)m).getBits()));\n+            return (int) VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_TRUECOUNT, Long512Mask.class, long.class, VLENGTH, this,\n+                                                      (m) -> trueCountHelper(m.getBits()));\n@@ -661,2 +692,2 @@\n-            return VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_FIRSTTRUE, Long512Mask.class, long.class, VLENGTH, this,\n-                                                      (m) -> firstTrueHelper(((Long512Mask)m).getBits()));\n+            return (int) VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_FIRSTTRUE, Long512Mask.class, long.class, VLENGTH, this,\n+                                                      (m) -> firstTrueHelper(m.getBits()));\n@@ -668,2 +699,12 @@\n-            return VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_LASTTRUE, Long512Mask.class, long.class, VLENGTH, this,\n-                                                      (m) -> lastTrueHelper(((Long512Mask)m).getBits()));\n+            return (int) VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_LASTTRUE, Long512Mask.class, long.class, VLENGTH, this,\n+                                                      (m) -> lastTrueHelper(m.getBits()));\n+        }\n+\n+        @Override\n+        @ForceInline\n+        public long toLong() {\n+            if (length() > Long.SIZE) {\n+                throw new UnsupportedOperationException(\"too many lanes for one long\");\n+            }\n+            return VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_TOLONG, Long512Mask.class, long.class, VLENGTH, this,\n+                                                      (m) -> toLongHelper(m.getBits()));\n@@ -780,0 +821,14 @@\n+    @ForceInline\n+    @Override\n+    final\n+    LongVector fromArray0(long[] a, int offset, VectorMask<Long> m) {\n+        return super.fromArray0Template(Long512Mask.class, a, offset, (Long512Mask) m);  \/\/ specialize\n+    }\n+\n+    @ForceInline\n+    @Override\n+    final\n+    LongVector fromArray0(long[] a, int offset, int[] indexMap, int mapOffset, VectorMask<Long> m) {\n+        return super.fromArray0Template(Long512Mask.class, a, offset, indexMap, mapOffset, (Long512Mask) m);\n+    }\n+\n@@ -789,0 +844,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    LongVector fromByteArray0(byte[] a, int offset, VectorMask<Long> m) {\n+        return super.fromByteArray0Template(Long512Mask.class, a, offset, (Long512Mask) m);  \/\/ specialize\n+    }\n+\n@@ -796,0 +858,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    LongVector fromByteBuffer0(ByteBuffer bb, int offset, VectorMask<Long> m) {\n+        return super.fromByteBuffer0Template(Long512Mask.class, bb, offset, (Long512Mask) m);  \/\/ specialize\n+    }\n+\n@@ -803,0 +872,15 @@\n+    @ForceInline\n+    @Override\n+    final\n+    void intoArray0(long[] a, int offset, VectorMask<Long> m) {\n+        super.intoArray0Template(Long512Mask.class, a, offset, (Long512Mask) m);\n+    }\n+\n+    @ForceInline\n+    @Override\n+    final\n+    void intoArray0(long[] a, int offset, int[] indexMap, int mapOffset, VectorMask<Long> m) {\n+        super.intoArray0Template(Long512Mask.class, a, offset, indexMap, mapOffset, (Long512Mask) m);\n+    }\n+\n+\n@@ -810,0 +894,15 @@\n+    @ForceInline\n+    @Override\n+    final\n+    void intoByteArray0(byte[] a, int offset, VectorMask<Long> m) {\n+        super.intoByteArray0Template(Long512Mask.class, a, offset, (Long512Mask) m);  \/\/ specialize\n+    }\n+\n+    @ForceInline\n+    @Override\n+    final\n+    void intoByteBuffer0(ByteBuffer bb, int offset, VectorMask<Long> m) {\n+        super.intoByteBuffer0Template(Long512Mask.class, bb, offset, (Long512Mask) m);\n+    }\n+\n+\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/Long512Vector.java","additions":129,"deletions":30,"binary":false,"changes":159,"status":"modified"},{"patch":"@@ -234,2 +234,2 @@\n-    long rOp(long v, FBinOp f) {\n-        return super.rOpTemplate(v, f);  \/\/ specialize\n+    long rOp(long v, VectorMask<Long> m, FBinOp f) {\n+        return super.rOpTemplate(v, m, f);  \/\/ specialize\n@@ -271,0 +271,6 @@\n+    @Override\n+    @ForceInline\n+    public Long64Vector lanewise(Unary op, VectorMask<Long> m) {\n+        return (Long64Vector) super.lanewiseTemplate(op, Long64Mask.class, (Long64Mask) m);  \/\/ specialize\n+    }\n+\n@@ -277,0 +283,6 @@\n+    @Override\n+    @ForceInline\n+    public Long64Vector lanewise(Binary op, Vector<Long> v, VectorMask<Long> m) {\n+        return (Long64Vector) super.lanewiseTemplate(op, Long64Mask.class, v, (Long64Mask) m);  \/\/ specialize\n+    }\n+\n@@ -284,0 +296,7 @@\n+    \/*package-private*\/\n+    @Override\n+    @ForceInline Long64Vector\n+    lanewiseShift(VectorOperators.Binary op, int e, VectorMask<Long> m) {\n+        return (Long64Vector) super.lanewiseShiftTemplate(op, Long64Mask.class, e, (Long64Mask) m);  \/\/ specialize\n+    }\n+\n@@ -289,1 +308,1 @@\n-    lanewise(VectorOperators.Ternary op, Vector<Long> v1, Vector<Long> v2) {\n+    lanewise(Ternary op, Vector<Long> v1, Vector<Long> v2) {\n@@ -293,0 +312,8 @@\n+    @Override\n+    @ForceInline\n+    public final\n+    Long64Vector\n+    lanewise(Ternary op, Vector<Long> v1, Vector<Long> v2, VectorMask<Long> m) {\n+        return (Long64Vector) super.lanewiseTemplate(op, Long64Mask.class, v1, v2, (Long64Mask) m);  \/\/ specialize\n+    }\n+\n@@ -312,1 +339,1 @@\n-        return super.reduceLanesTemplate(op, m);  \/\/ specialized\n+        return super.reduceLanesTemplate(op, Long64Mask.class, (Long64Mask) m);  \/\/ specialized\n@@ -325,1 +352,1 @@\n-        return (long) super.reduceLanesTemplate(op, m);  \/\/ specialized\n+        return (long) super.reduceLanesTemplate(op, Long64Mask.class, (Long64Mask) m);  \/\/ specialized\n@@ -356,0 +383,7 @@\n+    @Override\n+    @ForceInline\n+    public final Long64Mask compare(Comparison op, Vector<Long> v, VectorMask<Long> m) {\n+        return super.compareTemplate(Long64Mask.class, op, v, (Long64Mask) m);\n+    }\n+\n+\n@@ -412,0 +446,1 @@\n+                                    Long64Mask.class,\n@@ -575,10 +610,6 @@\n-            if (VSIZE == species.vectorBitSize()) {\n-                Class<?> dtype = species.elementType();\n-                Class<?> dmtype = species.maskType();\n-                return VectorSupport.convert(VectorSupport.VECTOR_OP_REINTERPRET,\n-                    this.getClass(), ETYPE, VLENGTH,\n-                    dmtype, dtype, VLENGTH,\n-                    this, species,\n-                    Long64Mask::defaultMaskCast);\n-            }\n-            return this.defaultMaskCast(species);\n+\n+            return VectorSupport.convert(VectorSupport.VECTOR_OP_CAST,\n+                this.getClass(), ETYPE, VLENGTH,\n+                species.maskType(), species.elementType(), VLENGTH,\n+                this, species,\n+                (m, s) -> s.maskFactory(m.toArray()).check(s));\n@@ -610,3 +641,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_AND, Long64Mask.class, long.class, VLENGTH,\n-                                             this, m,\n-                                             (m1, m2) -> m1.bOp(m2, (i, a, b) -> a & b));\n+            return VectorSupport.binaryOp(VECTOR_OP_AND, Long64Mask.class, null, long.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a & b));\n@@ -620,3 +651,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_OR, Long64Mask.class, long.class, VLENGTH,\n-                                             this, m,\n-                                             (m1, m2) -> m1.bOp(m2, (i, a, b) -> a | b));\n+            return VectorSupport.binaryOp(VECTOR_OP_OR, Long64Mask.class, null, long.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a | b));\n@@ -630,3 +661,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_XOR, Long64Mask.class, long.class, VLENGTH,\n-                                          this, m,\n-                                          (m1, m2) -> m1.bOp(m2, (i, a, b) -> a ^ b));\n+            return VectorSupport.binaryOp(VECTOR_OP_XOR, Long64Mask.class, null, long.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a ^ b));\n@@ -640,2 +671,2 @@\n-            return VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_TRUECOUNT, Long64Mask.class, long.class, VLENGTH, this,\n-                                                      (m) -> trueCountHelper(((Long64Mask)m).getBits()));\n+            return (int) VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_TRUECOUNT, Long64Mask.class, long.class, VLENGTH, this,\n+                                                      (m) -> trueCountHelper(m.getBits()));\n@@ -647,2 +678,2 @@\n-            return VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_FIRSTTRUE, Long64Mask.class, long.class, VLENGTH, this,\n-                                                      (m) -> firstTrueHelper(((Long64Mask)m).getBits()));\n+            return (int) VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_FIRSTTRUE, Long64Mask.class, long.class, VLENGTH, this,\n+                                                      (m) -> firstTrueHelper(m.getBits()));\n@@ -654,2 +685,12 @@\n-            return VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_LASTTRUE, Long64Mask.class, long.class, VLENGTH, this,\n-                                                      (m) -> lastTrueHelper(((Long64Mask)m).getBits()));\n+            return (int) VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_LASTTRUE, Long64Mask.class, long.class, VLENGTH, this,\n+                                                      (m) -> lastTrueHelper(m.getBits()));\n+        }\n+\n+        @Override\n+        @ForceInline\n+        public long toLong() {\n+            if (length() > Long.SIZE) {\n+                throw new UnsupportedOperationException(\"too many lanes for one long\");\n+            }\n+            return VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_TOLONG, Long64Mask.class, long.class, VLENGTH, this,\n+                                                      (m) -> toLongHelper(m.getBits()));\n@@ -766,0 +807,14 @@\n+    @ForceInline\n+    @Override\n+    final\n+    LongVector fromArray0(long[] a, int offset, VectorMask<Long> m) {\n+        return super.fromArray0Template(Long64Mask.class, a, offset, (Long64Mask) m);  \/\/ specialize\n+    }\n+\n+    @ForceInline\n+    @Override\n+    final\n+    LongVector fromArray0(long[] a, int offset, int[] indexMap, int mapOffset, VectorMask<Long> m) {\n+        return super.fromArray0Template(Long64Mask.class, a, offset, indexMap, mapOffset, (Long64Mask) m);\n+    }\n+\n@@ -775,0 +830,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    LongVector fromByteArray0(byte[] a, int offset, VectorMask<Long> m) {\n+        return super.fromByteArray0Template(Long64Mask.class, a, offset, (Long64Mask) m);  \/\/ specialize\n+    }\n+\n@@ -782,0 +844,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    LongVector fromByteBuffer0(ByteBuffer bb, int offset, VectorMask<Long> m) {\n+        return super.fromByteBuffer0Template(Long64Mask.class, bb, offset, (Long64Mask) m);  \/\/ specialize\n+    }\n+\n@@ -789,0 +858,15 @@\n+    @ForceInline\n+    @Override\n+    final\n+    void intoArray0(long[] a, int offset, VectorMask<Long> m) {\n+        super.intoArray0Template(Long64Mask.class, a, offset, (Long64Mask) m);\n+    }\n+\n+    @ForceInline\n+    @Override\n+    final\n+    void intoArray0(long[] a, int offset, int[] indexMap, int mapOffset, VectorMask<Long> m) {\n+        super.intoArray0Template(Long64Mask.class, a, offset, indexMap, mapOffset, (Long64Mask) m);\n+    }\n+\n+\n@@ -796,0 +880,15 @@\n+    @ForceInline\n+    @Override\n+    final\n+    void intoByteArray0(byte[] a, int offset, VectorMask<Long> m) {\n+        super.intoByteArray0Template(Long64Mask.class, a, offset, (Long64Mask) m);  \/\/ specialize\n+    }\n+\n+    @ForceInline\n+    @Override\n+    final\n+    void intoByteBuffer0(ByteBuffer bb, int offset, VectorMask<Long> m) {\n+        super.intoByteBuffer0Template(Long64Mask.class, bb, offset, (Long64Mask) m);\n+    }\n+\n+\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/Long64Vector.java","additions":129,"deletions":30,"binary":false,"changes":159,"status":"modified"},{"patch":"@@ -234,2 +234,2 @@\n-    long rOp(long v, FBinOp f) {\n-        return super.rOpTemplate(v, f);  \/\/ specialize\n+    long rOp(long v, VectorMask<Long> m, FBinOp f) {\n+        return super.rOpTemplate(v, m, f);  \/\/ specialize\n@@ -271,0 +271,6 @@\n+    @Override\n+    @ForceInline\n+    public LongMaxVector lanewise(Unary op, VectorMask<Long> m) {\n+        return (LongMaxVector) super.lanewiseTemplate(op, LongMaxMask.class, (LongMaxMask) m);  \/\/ specialize\n+    }\n+\n@@ -277,0 +283,6 @@\n+    @Override\n+    @ForceInline\n+    public LongMaxVector lanewise(Binary op, Vector<Long> v, VectorMask<Long> m) {\n+        return (LongMaxVector) super.lanewiseTemplate(op, LongMaxMask.class, v, (LongMaxMask) m);  \/\/ specialize\n+    }\n+\n@@ -284,0 +296,7 @@\n+    \/*package-private*\/\n+    @Override\n+    @ForceInline LongMaxVector\n+    lanewiseShift(VectorOperators.Binary op, int e, VectorMask<Long> m) {\n+        return (LongMaxVector) super.lanewiseShiftTemplate(op, LongMaxMask.class, e, (LongMaxMask) m);  \/\/ specialize\n+    }\n+\n@@ -289,1 +308,1 @@\n-    lanewise(VectorOperators.Ternary op, Vector<Long> v1, Vector<Long> v2) {\n+    lanewise(Ternary op, Vector<Long> v1, Vector<Long> v2) {\n@@ -293,0 +312,8 @@\n+    @Override\n+    @ForceInline\n+    public final\n+    LongMaxVector\n+    lanewise(Ternary op, Vector<Long> v1, Vector<Long> v2, VectorMask<Long> m) {\n+        return (LongMaxVector) super.lanewiseTemplate(op, LongMaxMask.class, v1, v2, (LongMaxMask) m);  \/\/ specialize\n+    }\n+\n@@ -312,1 +339,1 @@\n-        return super.reduceLanesTemplate(op, m);  \/\/ specialized\n+        return super.reduceLanesTemplate(op, LongMaxMask.class, (LongMaxMask) m);  \/\/ specialized\n@@ -325,1 +352,1 @@\n-        return (long) super.reduceLanesTemplate(op, m);  \/\/ specialized\n+        return (long) super.reduceLanesTemplate(op, LongMaxMask.class, (LongMaxMask) m);  \/\/ specialized\n@@ -356,0 +383,7 @@\n+    @Override\n+    @ForceInline\n+    public final LongMaxMask compare(Comparison op, Vector<Long> v, VectorMask<Long> m) {\n+        return super.compareTemplate(LongMaxMask.class, op, v, (LongMaxMask) m);\n+    }\n+\n+\n@@ -412,0 +446,1 @@\n+                                    LongMaxMask.class,\n@@ -575,10 +610,6 @@\n-            if (VSIZE == species.vectorBitSize()) {\n-                Class<?> dtype = species.elementType();\n-                Class<?> dmtype = species.maskType();\n-                return VectorSupport.convert(VectorSupport.VECTOR_OP_REINTERPRET,\n-                    this.getClass(), ETYPE, VLENGTH,\n-                    dmtype, dtype, VLENGTH,\n-                    this, species,\n-                    LongMaxMask::defaultMaskCast);\n-            }\n-            return this.defaultMaskCast(species);\n+\n+            return VectorSupport.convert(VectorSupport.VECTOR_OP_CAST,\n+                this.getClass(), ETYPE, VLENGTH,\n+                species.maskType(), species.elementType(), VLENGTH,\n+                this, species,\n+                (m, s) -> s.maskFactory(m.toArray()).check(s));\n@@ -610,3 +641,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_AND, LongMaxMask.class, long.class, VLENGTH,\n-                                             this, m,\n-                                             (m1, m2) -> m1.bOp(m2, (i, a, b) -> a & b));\n+            return VectorSupport.binaryOp(VECTOR_OP_AND, LongMaxMask.class, null, long.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a & b));\n@@ -620,3 +651,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_OR, LongMaxMask.class, long.class, VLENGTH,\n-                                             this, m,\n-                                             (m1, m2) -> m1.bOp(m2, (i, a, b) -> a | b));\n+            return VectorSupport.binaryOp(VECTOR_OP_OR, LongMaxMask.class, null, long.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a | b));\n@@ -630,3 +661,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_XOR, LongMaxMask.class, long.class, VLENGTH,\n-                                          this, m,\n-                                          (m1, m2) -> m1.bOp(m2, (i, a, b) -> a ^ b));\n+            return VectorSupport.binaryOp(VECTOR_OP_XOR, LongMaxMask.class, null, long.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a ^ b));\n@@ -640,2 +671,2 @@\n-            return VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_TRUECOUNT, LongMaxMask.class, long.class, VLENGTH, this,\n-                                                      (m) -> trueCountHelper(((LongMaxMask)m).getBits()));\n+            return (int) VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_TRUECOUNT, LongMaxMask.class, long.class, VLENGTH, this,\n+                                                      (m) -> trueCountHelper(m.getBits()));\n@@ -647,2 +678,2 @@\n-            return VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_FIRSTTRUE, LongMaxMask.class, long.class, VLENGTH, this,\n-                                                      (m) -> firstTrueHelper(((LongMaxMask)m).getBits()));\n+            return (int) VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_FIRSTTRUE, LongMaxMask.class, long.class, VLENGTH, this,\n+                                                      (m) -> firstTrueHelper(m.getBits()));\n@@ -654,2 +685,12 @@\n-            return VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_LASTTRUE, LongMaxMask.class, long.class, VLENGTH, this,\n-                                                      (m) -> lastTrueHelper(((LongMaxMask)m).getBits()));\n+            return (int) VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_LASTTRUE, LongMaxMask.class, long.class, VLENGTH, this,\n+                                                      (m) -> lastTrueHelper(m.getBits()));\n+        }\n+\n+        @Override\n+        @ForceInline\n+        public long toLong() {\n+            if (length() > Long.SIZE) {\n+                throw new UnsupportedOperationException(\"too many lanes for one long\");\n+            }\n+            return VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_TOLONG, LongMaxMask.class, long.class, VLENGTH, this,\n+                                                      (m) -> toLongHelper(m.getBits()));\n@@ -766,0 +807,14 @@\n+    @ForceInline\n+    @Override\n+    final\n+    LongVector fromArray0(long[] a, int offset, VectorMask<Long> m) {\n+        return super.fromArray0Template(LongMaxMask.class, a, offset, (LongMaxMask) m);  \/\/ specialize\n+    }\n+\n+    @ForceInline\n+    @Override\n+    final\n+    LongVector fromArray0(long[] a, int offset, int[] indexMap, int mapOffset, VectorMask<Long> m) {\n+        return super.fromArray0Template(LongMaxMask.class, a, offset, indexMap, mapOffset, (LongMaxMask) m);\n+    }\n+\n@@ -775,0 +830,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    LongVector fromByteArray0(byte[] a, int offset, VectorMask<Long> m) {\n+        return super.fromByteArray0Template(LongMaxMask.class, a, offset, (LongMaxMask) m);  \/\/ specialize\n+    }\n+\n@@ -782,0 +844,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    LongVector fromByteBuffer0(ByteBuffer bb, int offset, VectorMask<Long> m) {\n+        return super.fromByteBuffer0Template(LongMaxMask.class, bb, offset, (LongMaxMask) m);  \/\/ specialize\n+    }\n+\n@@ -789,0 +858,15 @@\n+    @ForceInline\n+    @Override\n+    final\n+    void intoArray0(long[] a, int offset, VectorMask<Long> m) {\n+        super.intoArray0Template(LongMaxMask.class, a, offset, (LongMaxMask) m);\n+    }\n+\n+    @ForceInline\n+    @Override\n+    final\n+    void intoArray0(long[] a, int offset, int[] indexMap, int mapOffset, VectorMask<Long> m) {\n+        super.intoArray0Template(LongMaxMask.class, a, offset, indexMap, mapOffset, (LongMaxMask) m);\n+    }\n+\n+\n@@ -796,0 +880,15 @@\n+    @ForceInline\n+    @Override\n+    final\n+    void intoByteArray0(byte[] a, int offset, VectorMask<Long> m) {\n+        super.intoByteArray0Template(LongMaxMask.class, a, offset, (LongMaxMask) m);  \/\/ specialize\n+    }\n+\n+    @ForceInline\n+    @Override\n+    final\n+    void intoByteBuffer0(ByteBuffer bb, int offset, VectorMask<Long> m) {\n+        super.intoByteBuffer0Template(LongMaxMask.class, bb, offset, (LongMaxMask) m);\n+    }\n+\n+\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/LongMaxVector.java","additions":129,"deletions":30,"binary":false,"changes":159,"status":"modified"},{"patch":"@@ -32,1 +32,0 @@\n-import java.util.function.BinaryOperator;\n@@ -176,0 +175,3 @@\n+        if (m == null) {\n+            return uOpTemplate(f);\n+        }\n@@ -219,0 +221,3 @@\n+        if (m == null) {\n+            return bOpTemplate(o, f);\n+        }\n@@ -268,0 +273,3 @@\n+        if (m == null) {\n+            return tOpTemplate(o1, o2, f);\n+        }\n@@ -283,1 +291,16 @@\n-    long rOp(long v, FBinOp f);\n+    long rOp(long v, VectorMask<Long> m, FBinOp f);\n+\n+    @ForceInline\n+    final\n+    long rOpTemplate(long v, VectorMask<Long> m, FBinOp f) {\n+        if (m == null) {\n+            return rOpTemplate(v, f);\n+        }\n+        long[] vec = vec();\n+        boolean[] mbits = ((AbstractMask<Long>)m).getBits();\n+        for (int i = 0; i < vec.length; i++) {\n+            v = mbits[i] ? f.apply(i, v, vec[i]) : v;\n+        }\n+        return v;\n+    }\n+\n@@ -510,1 +533,1 @@\n-                return broadcast(-1).lanewiseTemplate(XOR, this);\n+                return broadcast(-1).lanewise(XOR, this);\n@@ -513,1 +536,1 @@\n-                return broadcast(0).lanewiseTemplate(SUB, this);\n+                return broadcast(0).lanewise(SUB, this);\n@@ -518,10 +541,3 @@\n-            opc, getClass(), long.class, length(),\n-            this,\n-            UN_IMPL.find(op, opc, (opc_) -> {\n-              switch (opc_) {\n-                case VECTOR_OP_NEG: return v0 ->\n-                        v0.uOp((i, a) -> (long) -a);\n-                case VECTOR_OP_ABS: return v0 ->\n-                        v0.uOp((i, a) -> (long) Math.abs(a));\n-                default: return null;\n-              }}));\n+            opc, getClass(), null, long.class, length(),\n+            this, null,\n+            UN_IMPL.find(op, opc, LongVector::unaryOperations));\n@@ -529,3 +545,0 @@\n-    private static final\n-    ImplCache<Unary,UnaryOperator<LongVector>> UN_IMPL\n-        = new ImplCache<>(Unary.class, LongVector.class);\n@@ -536,2 +549,2 @@\n-    @ForceInline\n-    public final\n+    @Override\n+    public abstract\n@@ -539,2 +552,36 @@\n-                                  VectorMask<Long> m) {\n-        return blend(lanewise(op), m);\n+                                  VectorMask<Long> m);\n+    @ForceInline\n+    final\n+    LongVector lanewiseTemplate(VectorOperators.Unary op,\n+                                          Class<? extends VectorMask<Long>> maskClass,\n+                                          VectorMask<Long> m) {\n+        m.check(maskClass, this);\n+        if (opKind(op, VO_SPECIAL)) {\n+            if (op == ZOMO) {\n+                return blend(broadcast(-1), compare(NE, 0, m));\n+            }\n+            if (op == NOT) {\n+                return lanewise(XOR, broadcast(-1), m);\n+            } else if (op == NEG) {\n+                return lanewise(NOT, m).lanewise(ADD, broadcast(1), m);\n+            }\n+        }\n+        int opc = opCode(op);\n+        return VectorSupport.unaryOp(\n+            opc, getClass(), maskClass, long.class, length(),\n+            this, m,\n+            UN_IMPL.find(op, opc, LongVector::unaryOperations));\n+    }\n+\n+    private static final\n+    ImplCache<Unary, UnaryOperation<LongVector, VectorMask<Long>>>\n+        UN_IMPL = new ImplCache<>(Unary.class, LongVector.class);\n+\n+    private static UnaryOperation<LongVector, VectorMask<Long>> unaryOperations(int opc_) {\n+        switch (opc_) {\n+            case VECTOR_OP_NEG: return (v0, m) ->\n+                    v0.uOp(m, (i, a) -> (long) -a);\n+            case VECTOR_OP_ABS: return (v0, m) ->\n+                    v0.uOp(m, (i, a) -> (long) Math.abs(a));\n+            default: return null;\n+        }\n@@ -560,0 +607,1 @@\n+\n@@ -578,1 +626,1 @@\n-                VectorMask<Long> eqz = that.eq((long)0);\n+                VectorMask<Long> eqz = that.eq((long) 0);\n@@ -584,0 +632,1 @@\n+\n@@ -586,34 +635,3 @@\n-            opc, getClass(), long.class, length(),\n-            this, that,\n-            BIN_IMPL.find(op, opc, (opc_) -> {\n-              switch (opc_) {\n-                case VECTOR_OP_ADD: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (long)(a + b));\n-                case VECTOR_OP_SUB: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (long)(a - b));\n-                case VECTOR_OP_MUL: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (long)(a * b));\n-                case VECTOR_OP_DIV: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (long)(a \/ b));\n-                case VECTOR_OP_MAX: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (long)Math.max(a, b));\n-                case VECTOR_OP_MIN: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (long)Math.min(a, b));\n-                case VECTOR_OP_AND: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (long)(a & b));\n-                case VECTOR_OP_OR: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (long)(a | b));\n-                case VECTOR_OP_XOR: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (long)(a ^ b));\n-                case VECTOR_OP_LSHIFT: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, n) -> (long)(a << n));\n-                case VECTOR_OP_RSHIFT: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, n) -> (long)(a >> n));\n-                case VECTOR_OP_URSHIFT: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, n) -> (long)((a & LSHR_SETUP_MASK) >>> n));\n-                case VECTOR_OP_LROTATE: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, n) -> rotateLeft(a, (int)n));\n-                case VECTOR_OP_RROTATE: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, n) -> rotateRight(a, (int)n));\n-                default: return null;\n-                }}));\n+            opc, getClass(), null, long.class, length(),\n+            this, that, null,\n+            BIN_IMPL.find(op, opc, LongVector::binaryOperations));\n@@ -621,3 +639,0 @@\n-    private static final\n-    ImplCache<Binary,BinaryOperator<LongVector>> BIN_IMPL\n-        = new ImplCache<>(Binary.class, LongVector.class);\n@@ -629,2 +644,2 @@\n-    @ForceInline\n-    public final\n+    @Override\n+    public abstract\n@@ -633,1 +648,6 @@\n-                                  VectorMask<Long> m) {\n+                                  VectorMask<Long> m);\n+    @ForceInline\n+    final\n+    LongVector lanewiseTemplate(VectorOperators.Binary op,\n+                                          Class<? extends VectorMask<Long>> maskClass,\n+                                          Vector<Long> v, VectorMask<Long> m) {\n@@ -635,4 +655,10 @@\n-        if (op == DIV) {\n-            VectorMask<Long> eqz = that.eq((long)0);\n-            if (eqz.and(m).anyTrue()) {\n-                throw that.divZeroException();\n+        that.check(this);\n+        m.check(maskClass, this);\n+\n+        if (opKind(op, VO_SPECIAL  | VO_SHIFT)) {\n+            if (op == FIRST_NONZERO) {\n+                \/\/ FIXME: Support this in the JIT.\n+                VectorMask<Long> thisNZ\n+                    = this.viewAsIntegralLanes().compare(NE, (long) 0);\n+                that = that.blend((long) 0, thisNZ.cast(vspecies()));\n+                op = OR_UNCHECKED;\n@@ -640,3 +666,61 @@\n-            \/\/ suppress div\/0 exceptions in unset lanes\n-            that = that.lanewise(NOT, eqz);\n-            return blend(lanewise(DIV, that), m);\n+            if (opKind(op, VO_SHIFT)) {\n+                \/\/ As per shift specification for Java, mask the shift count.\n+                \/\/ This allows the JIT to ignore some ISA details.\n+                that = that.lanewise(AND, SHIFT_MASK);\n+            }\n+            if (op == AND_NOT) {\n+                \/\/ FIXME: Support this in the JIT.\n+                that = that.lanewise(NOT);\n+                op = AND;\n+            } else if (op == DIV) {\n+                VectorMask<Long> eqz = that.eq((long)0);\n+                if (eqz.and(m).anyTrue()) {\n+                    throw that.divZeroException();\n+                }\n+                \/\/ suppress div\/0 exceptions in unset lanes\n+                that = that.lanewise(NOT, eqz);\n+            }\n+        }\n+\n+        int opc = opCode(op);\n+        return VectorSupport.binaryOp(\n+            opc, getClass(), maskClass, long.class, length(),\n+            this, that, m,\n+            BIN_IMPL.find(op, opc, LongVector::binaryOperations));\n+    }\n+\n+    private static final\n+    ImplCache<Binary, BinaryOperation<LongVector, VectorMask<Long>>>\n+        BIN_IMPL = new ImplCache<>(Binary.class, LongVector.class);\n+\n+    private static BinaryOperation<LongVector, VectorMask<Long>> binaryOperations(int opc_) {\n+        switch (opc_) {\n+            case VECTOR_OP_ADD: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> (long)(a + b));\n+            case VECTOR_OP_SUB: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> (long)(a - b));\n+            case VECTOR_OP_MUL: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> (long)(a * b));\n+            case VECTOR_OP_DIV: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> (long)(a \/ b));\n+            case VECTOR_OP_MAX: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> (long)Math.max(a, b));\n+            case VECTOR_OP_MIN: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> (long)Math.min(a, b));\n+            case VECTOR_OP_AND: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> (long)(a & b));\n+            case VECTOR_OP_OR: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> (long)(a | b));\n+            case VECTOR_OP_XOR: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> (long)(a ^ b));\n+            case VECTOR_OP_LSHIFT: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, n) -> (long)(a << n));\n+            case VECTOR_OP_RSHIFT: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, n) -> (long)(a >> n));\n+            case VECTOR_OP_URSHIFT: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, n) -> (long)((a & LSHR_SETUP_MASK) >>> n));\n+            case VECTOR_OP_LROTATE: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, n) -> rotateLeft(a, (int)n));\n+            case VECTOR_OP_RROTATE: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, n) -> rotateRight(a, (int)n));\n+            default: return null;\n@@ -644,1 +728,0 @@\n-        return blend(lanewise(op, v), m);\n@@ -646,0 +729,1 @@\n+\n@@ -708,1 +792,7 @@\n-        return blend(lanewise(op, e), m);\n+        if (opKind(op, VO_SHIFT) && (long)(int)e == e) {\n+            return lanewiseShift(op, (int) e, m);\n+        }\n+        if (op == AND_NOT) {\n+            op = AND; e = (long) ~e;\n+        }\n+        return lanewise(op, broadcast(e), m);\n@@ -726,16 +816,3 @@\n-            opc, getClass(), long.class, length(),\n-            this, e,\n-            BIN_INT_IMPL.find(op, opc, (opc_) -> {\n-              switch (opc_) {\n-                case VECTOR_OP_LSHIFT: return (v, n) ->\n-                        v.uOp((i, a) -> (long)(a << n));\n-                case VECTOR_OP_RSHIFT: return (v, n) ->\n-                        v.uOp((i, a) -> (long)(a >> n));\n-                case VECTOR_OP_URSHIFT: return (v, n) ->\n-                        v.uOp((i, a) -> (long)((a & LSHR_SETUP_MASK) >>> n));\n-                case VECTOR_OP_LROTATE: return (v, n) ->\n-                        v.uOp((i, a) -> rotateLeft(a, (int)n));\n-                case VECTOR_OP_RROTATE: return (v, n) ->\n-                        v.uOp((i, a) -> rotateRight(a, (int)n));\n-                default: return null;\n-                }}));\n+            opc, getClass(), null, long.class, length(),\n+            this, e, null,\n+            BIN_INT_IMPL.find(op, opc, LongVector::broadcastIntOperations));\n@@ -743,0 +820,22 @@\n+\n+    \/*package-private*\/\n+    abstract LongVector\n+    lanewiseShift(VectorOperators.Binary op, int e, VectorMask<Long> m);\n+\n+    \/*package-private*\/\n+    @ForceInline\n+    final LongVector\n+    lanewiseShiftTemplate(VectorOperators.Binary op,\n+                          Class<? extends VectorMask<Long>> maskClass,\n+                          int e, VectorMask<Long> m) {\n+        m.check(maskClass, this);\n+        assert(opKind(op, VO_SHIFT));\n+        \/\/ As per shift specification for Java, mask the shift count.\n+        e &= SHIFT_MASK;\n+        int opc = opCode(op);\n+        return VectorSupport.broadcastInt(\n+            opc, getClass(), maskClass, long.class, length(),\n+            this, e, m,\n+            BIN_INT_IMPL.find(op, opc, LongVector::broadcastIntOperations));\n+    }\n+\n@@ -744,1 +843,1 @@\n-    ImplCache<Binary,VectorBroadcastIntOp<LongVector>> BIN_INT_IMPL\n+    ImplCache<Binary,VectorBroadcastIntOp<LongVector, VectorMask<Long>>> BIN_INT_IMPL\n@@ -747,0 +846,16 @@\n+    private static VectorBroadcastIntOp<LongVector, VectorMask<Long>> broadcastIntOperations(int opc_) {\n+        switch (opc_) {\n+            case VECTOR_OP_LSHIFT: return (v, n, m) ->\n+                    v.uOp(m, (i, a) -> (long)(a << n));\n+            case VECTOR_OP_RSHIFT: return (v, n, m) ->\n+                    v.uOp(m, (i, a) -> (long)(a >> n));\n+            case VECTOR_OP_URSHIFT: return (v, n, m) ->\n+                    v.uOp(m, (i, a) -> (long)((a & LSHR_SETUP_MASK) >>> n));\n+            case VECTOR_OP_LROTATE: return (v, n, m) ->\n+                    v.uOp(m, (i, a) -> rotateLeft(a, (int)n));\n+            case VECTOR_OP_RROTATE: return (v, n, m) ->\n+                    v.uOp(m, (i, a) -> rotateRight(a, (int)n));\n+            default: return null;\n+        }\n+    }\n+\n@@ -798,6 +913,3 @@\n-            opc, getClass(), long.class, length(),\n-            this, that, tother,\n-            TERN_IMPL.find(op, opc, (opc_) -> {\n-              switch (opc_) {\n-                default: return null;\n-                }}));\n+            opc, getClass(), null, long.class, length(),\n+            this, that, tother, null,\n+            TERN_IMPL.find(op, opc, LongVector::ternaryOperations));\n@@ -805,3 +917,0 @@\n-    private static final\n-    ImplCache<Ternary,TernaryOperation<LongVector>> TERN_IMPL\n-        = new ImplCache<>(Ternary.class, LongVector.class);\n@@ -815,2 +924,2 @@\n-    @ForceInline\n-    public final\n+    @Override\n+    public abstract\n@@ -820,2 +929,37 @@\n-                                  VectorMask<Long> m) {\n-        return blend(lanewise(op, v1, v2), m);\n+                                  VectorMask<Long> m);\n+    @ForceInline\n+    final\n+    LongVector lanewiseTemplate(VectorOperators.Ternary op,\n+                                          Class<? extends VectorMask<Long>> maskClass,\n+                                          Vector<Long> v1,\n+                                          Vector<Long> v2,\n+                                          VectorMask<Long> m) {\n+        LongVector that = (LongVector) v1;\n+        LongVector tother = (LongVector) v2;\n+        \/\/ It's a word: https:\/\/www.dictionary.com\/browse\/tother\n+        \/\/ See also Chapter 11 of Dickens, Our Mutual Friend:\n+        \/\/ \"Totherest Governor,\" replied Mr Riderhood...\n+        that.check(this);\n+        tother.check(this);\n+        m.check(maskClass, this);\n+\n+        if (op == BITWISE_BLEND) {\n+            \/\/ FIXME: Support this in the JIT.\n+            that = this.lanewise(XOR, that).lanewise(AND, tother);\n+            return this.lanewise(XOR, that, m);\n+        }\n+        int opc = opCode(op);\n+        return VectorSupport.ternaryOp(\n+            opc, getClass(), maskClass, long.class, length(),\n+            this, that, tother, m,\n+            TERN_IMPL.find(op, opc, LongVector::ternaryOperations));\n+    }\n+\n+    private static final\n+    ImplCache<Ternary, TernaryOperation<LongVector, VectorMask<Long>>>\n+        TERN_IMPL = new ImplCache<>(Ternary.class, LongVector.class);\n+\n+    private static TernaryOperation<LongVector, VectorMask<Long>> ternaryOperations(int opc_) {\n+        switch (opc_) {\n+            default: return null;\n+        }\n@@ -878,1 +1022,1 @@\n-        return blend(lanewise(op, e1, e2), m);\n+        return lanewise(op, broadcast(e1), broadcast(e2), m);\n@@ -936,1 +1080,1 @@\n-        return blend(lanewise(op, v1, e2), m);\n+        return lanewise(op, v1, broadcast(e2), m);\n@@ -993,1 +1137,1 @@\n-        return blend(lanewise(op, e1, v2), m);\n+        return lanewise(op, broadcast(e1), v2, m);\n@@ -1665,2 +1809,0 @@\n-        Objects.requireNonNull(v);\n-        LongSpecies vsp = vspecies();\n@@ -1672,2 +1814,2 @@\n-            this, that,\n-            (cond, v0, v1) -> {\n+            this, that, null,\n+            (cond, v0, v1, m1) -> {\n@@ -1683,0 +1825,22 @@\n+    \/*package-private*\/\n+    @ForceInline\n+    final\n+    <M extends VectorMask<Long>>\n+    M compareTemplate(Class<M> maskType, Comparison op, Vector<Long> v, M m) {\n+        LongVector that = (LongVector) v;\n+        that.check(this);\n+        m.check(maskType, this);\n+        int opc = opCode(op);\n+        return VectorSupport.compare(\n+            opc, getClass(), maskType, long.class, length(),\n+            this, that, m,\n+            (cond, v0, v1, m1) -> {\n+                AbstractMask<Long> cmpM\n+                    = v0.bTest(cond, v1, (cond_, i, a, b)\n+                               -> compareWithOp(cond, a, b));\n+                @SuppressWarnings(\"unchecked\")\n+                M m2 = (M) cmpM.and(m1);\n+                return m2;\n+            });\n+    }\n+\n@@ -1700,12 +1864,0 @@\n-    \/**\n-     * {@inheritDoc} <!--workaround-->\n-     *\/\n-    @Override\n-    @ForceInline\n-    public final\n-    VectorMask<Long> compare(VectorOperators.Comparison op,\n-                                  Vector<Long> v,\n-                                  VectorMask<Long> m) {\n-        return compare(op, v).and(m);\n-    }\n-\n@@ -1770,1 +1922,1 @@\n-        return compare(op, e).and(m);\n+        return compare(op, broadcast(e), m);\n@@ -1974,3 +2126,3 @@\n-            getClass(), shuffletype, long.class, length(),\n-            this, shuffle,\n-            (v1, s_) -> v1.uOp((i, a) -> {\n+            getClass(), shuffletype, null, long.class, length(),\n+            this, shuffle, null,\n+            (v1, s_, m_) -> v1.uOp((i, a) -> {\n@@ -1993,1 +2145,1 @@\n-    <S extends VectorShuffle<Long>>\n+    <S extends VectorShuffle<Long>, M extends VectorMask<Long>>\n@@ -1995,0 +2147,1 @@\n+                                           Class<M> masktype,\n@@ -1996,9 +2149,3 @@\n-                                           VectorMask<Long> m) {\n-        LongVector unmasked =\n-            VectorSupport.rearrangeOp(\n-                getClass(), shuffletype, long.class, length(),\n-                this, shuffle,\n-                (v1, s_) -> v1.uOp((i, a) -> {\n-                    int ei = s_.laneSource(i);\n-                    return ei < 0 ? 0 : v1.lane(ei);\n-                }));\n+                                           M m) {\n+\n+        m.check(masktype, this);\n@@ -2010,1 +2157,7 @@\n-        return broadcast((long)0).blend(unmasked, m);\n+        return VectorSupport.rearrangeOp(\n+                   getClass(), shuffletype, masktype, long.class, length(),\n+                   this, shuffle, m,\n+                   (v1, s_, m_) -> v1.uOp((i, a) -> {\n+                        int ei = s_.laneSource(i);\n+                        return ei < 0  || !m_.laneIsSet(i) ? 0 : v1.lane(ei);\n+                   }));\n@@ -2033,3 +2186,3 @@\n-                getClass(), shuffletype, long.class, length(),\n-                this, ws,\n-                (v0, s_) -> v0.uOp((i, a) -> {\n+                getClass(), shuffletype, null, long.class, length(),\n+                this, ws, null,\n+                (v0, s_, m_) -> v0.uOp((i, a) -> {\n@@ -2041,3 +2194,3 @@\n-                getClass(), shuffletype, long.class, length(),\n-                v, ws,\n-                (v1, s_) -> v1.uOp((i, a) -> {\n+                getClass(), shuffletype, null, long.class, length(),\n+                v, ws, null,\n+                (v1, s_, m_) -> v1.uOp((i, a) -> {\n@@ -2306,0 +2459,1 @@\n+                               Class<? extends VectorMask<Long>> maskClass,\n@@ -2307,2 +2461,10 @@\n-        LongVector v = reduceIdentityVector(op).blend(this, m);\n-        return v.reduceLanesTemplate(op);\n+        m.check(maskClass, this);\n+        if (op == FIRST_NONZERO) {\n+            LongVector v = reduceIdentityVector(op).blend(this, m);\n+            return v.reduceLanesTemplate(op);\n+        }\n+        int opc = opCode(op);\n+        return fromBits(VectorSupport.reductionCoerced(\n+            opc, getClass(), maskClass, long.class, length(),\n+            this, m,\n+            REDUCE_IMPL.find(op, opc, LongVector::reductionOperations)));\n@@ -2323,20 +2485,3 @@\n-            opc, getClass(), long.class, length(),\n-            this,\n-            REDUCE_IMPL.find(op, opc, (opc_) -> {\n-              switch (opc_) {\n-              case VECTOR_OP_ADD: return v ->\n-                      toBits(v.rOp((long)0, (i, a, b) -> (long)(a + b)));\n-              case VECTOR_OP_MUL: return v ->\n-                      toBits(v.rOp((long)1, (i, a, b) -> (long)(a * b)));\n-              case VECTOR_OP_MIN: return v ->\n-                      toBits(v.rOp(MAX_OR_INF, (i, a, b) -> (long) Math.min(a, b)));\n-              case VECTOR_OP_MAX: return v ->\n-                      toBits(v.rOp(MIN_OR_INF, (i, a, b) -> (long) Math.max(a, b)));\n-              case VECTOR_OP_AND: return v ->\n-                      toBits(v.rOp((long)-1, (i, a, b) -> (long)(a & b)));\n-              case VECTOR_OP_OR: return v ->\n-                      toBits(v.rOp((long)0, (i, a, b) -> (long)(a | b)));\n-              case VECTOR_OP_XOR: return v ->\n-                      toBits(v.rOp((long)0, (i, a, b) -> (long)(a ^ b)));\n-              default: return null;\n-              }})));\n+            opc, getClass(), null, long.class, length(),\n+            this, null,\n+            REDUCE_IMPL.find(op, opc, LongVector::reductionOperations)));\n@@ -2344,0 +2489,1 @@\n+\n@@ -2345,2 +2491,22 @@\n-    ImplCache<Associative,Function<LongVector,Long>> REDUCE_IMPL\n-        = new ImplCache<>(Associative.class, LongVector.class);\n+    ImplCache<Associative, ReductionOperation<LongVector, VectorMask<Long>>>\n+        REDUCE_IMPL = new ImplCache<>(Associative.class, LongVector.class);\n+\n+    private static ReductionOperation<LongVector, VectorMask<Long>> reductionOperations(int opc_) {\n+        switch (opc_) {\n+            case VECTOR_OP_ADD: return (v, m) ->\n+                    toBits(v.rOp((long)0, m, (i, a, b) -> (long)(a + b)));\n+            case VECTOR_OP_MUL: return (v, m) ->\n+                    toBits(v.rOp((long)1, m, (i, a, b) -> (long)(a * b)));\n+            case VECTOR_OP_MIN: return (v, m) ->\n+                    toBits(v.rOp(MAX_OR_INF, m, (i, a, b) -> (long) Math.min(a, b)));\n+            case VECTOR_OP_MAX: return (v, m) ->\n+                    toBits(v.rOp(MIN_OR_INF, m, (i, a, b) -> (long) Math.max(a, b)));\n+            case VECTOR_OP_AND: return (v, m) ->\n+                    toBits(v.rOp((long)-1, m, (i, a, b) -> (long)(a & b)));\n+            case VECTOR_OP_OR: return (v, m) ->\n+                    toBits(v.rOp((long)0, m, (i, a, b) -> (long)(a | b)));\n+            case VECTOR_OP_XOR: return (v, m) ->\n+                    toBits(v.rOp((long)0, m, (i, a, b) -> (long)(a ^ b)));\n+            default: return null;\n+        }\n+    }\n@@ -2560,3 +2726,1 @@\n-            LongVector zero = vsp.zero();\n-            LongVector v = zero.fromByteArray0(a, offset);\n-            return zero.blend(v.maybeSwap(bo), m);\n+            return vsp.dummyVector().fromByteArray0(a, offset, m).maybeSwap(bo);\n@@ -2624,2 +2788,1 @@\n-            LongVector zero = vsp.zero();\n-            return zero.blend(zero.fromArray0(a, offset), m);\n+            return vsp.dummyVector().fromArray0(a, offset, m);\n@@ -2701,3 +2864,3 @@\n-            vectorType, long.class, vsp.laneCount(),\n-            IntVector.species(vsp.indexShape()).vectorType(),\n-            a, ARRAY_BASE, vix,\n+            vectorType, null, long.class, vsp.laneCount(),\n+            isp.vectorType(),\n+            a, ARRAY_BASE, vix, null,\n@@ -2705,1 +2868,1 @@\n-            (long[] c, int idx, int[] iMap, int idy, LongSpecies s) ->\n+            (c, idx, iMap, idy, s, vm) ->\n@@ -2707,1 +2870,1 @@\n-        }\n+    }\n@@ -2755,1 +2918,0 @@\n-            \/\/ FIXME: Cannot vectorize yet, if there's a mask.\n@@ -2757,1 +2919,1 @@\n-            return vsp.vOp(m, n -> a[offset + indexMap[mapOffset + n]]);\n+            return vsp.dummyVector().fromArray0(a, offset, indexMap, mapOffset, m);\n@@ -2851,3 +3013,1 @@\n-            LongVector zero = vsp.zero();\n-            LongVector v = zero.fromByteBuffer0(bb, offset);\n-            return zero.blend(v.maybeSwap(bo), m);\n+            return vsp.dummyVector().fromByteBuffer0(bb, offset, m).maybeSwap(bo);\n@@ -2925,1 +3085,0 @@\n-            \/\/ FIXME: optimize\n@@ -2928,1 +3087,1 @@\n-            stOp(a, offset, m, (arr, off, i, v) -> arr[off+i] = v);\n+            intoArray0(a, offset, m);\n@@ -2991,1 +3150,1 @@\n-            vsp.vectorType(), vsp.elementType(), vsp.laneCount(),\n+            vsp.vectorType(), null, vsp.elementType(), vsp.laneCount(),\n@@ -2994,1 +3153,1 @@\n-            this,\n+            this, null,\n@@ -2996,1 +3155,1 @@\n-            (arr, off, v, map, mo)\n+            (arr, off, v, map, mo, vm)\n@@ -3043,6 +3202,1 @@\n-            \/\/ FIXME: Cannot vectorize yet, if there's a mask.\n-            stOp(a, offset, m,\n-                 (arr, off, i, e) -> {\n-                     int j = indexMap[mapOffset + i];\n-                     arr[off + j] = e;\n-                 });\n+            intoArray0(a, offset, indexMap, mapOffset, m);\n@@ -3078,1 +3232,0 @@\n-            \/\/ FIXME: optimize\n@@ -3081,3 +3234,1 @@\n-            ByteBuffer wb = wrapper(a, bo);\n-            this.stOp(wb, offset, m,\n-                    (wb_, o, i, e) -> wb_.putLong(o + i * 8, e));\n+            maybeSwap(bo).intoByteArray0(a, offset, m);\n@@ -3095,1 +3246,1 @@\n-        if (bb.isReadOnly()) {\n+        if (ScopedMemoryAccess.isReadOnly(bb)) {\n@@ -3114,1 +3265,0 @@\n-            \/\/ FIXME: optimize\n@@ -3120,3 +3270,1 @@\n-            ByteBuffer wb = wrapper(bb, bo);\n-            this.stOp(wb, offset, m,\n-                    (wb_, o, i, e) -> wb_.putLong(o + i * 8, e));\n+            maybeSwap(bo).intoByteBuffer0(bb, offset, m);\n@@ -3160,0 +3308,69 @@\n+    \/*package-private*\/\n+    abstract\n+    LongVector fromArray0(long[] a, int offset, VectorMask<Long> m);\n+    @ForceInline\n+    final\n+    <M extends VectorMask<Long>>\n+    LongVector fromArray0Template(Class<M> maskClass, long[] a, int offset, M m) {\n+        m.check(species());\n+        LongSpecies vsp = vspecies();\n+        return VectorSupport.loadMasked(\n+            vsp.vectorType(), maskClass, vsp.elementType(), vsp.laneCount(),\n+            a, arrayAddress(a, offset), m,\n+            a, offset, vsp,\n+            (arr, off, s, vm) -> s.ldOp(arr, off, vm,\n+                                        (arr_, off_, i) -> arr_[off_ + i]));\n+    }\n+\n+    \/*package-private*\/\n+    abstract\n+    LongVector fromArray0(long[] a, int offset,\n+                                    int[] indexMap, int mapOffset,\n+                                    VectorMask<Long> m);\n+    @ForceInline\n+    final\n+    <M extends VectorMask<Long>>\n+    LongVector fromArray0Template(Class<M> maskClass, long[] a, int offset,\n+                                            int[] indexMap, int mapOffset, M m) {\n+        LongSpecies vsp = vspecies();\n+        IntVector.IntSpecies isp = IntVector.species(vsp.indexShape());\n+        Objects.requireNonNull(a);\n+        Objects.requireNonNull(indexMap);\n+        m.check(vsp);\n+        Class<? extends LongVector> vectorType = vsp.vectorType();\n+\n+        if (vsp.laneCount() == 1) {\n+          return LongVector.fromArray(vsp, a, offset + indexMap[mapOffset], m);\n+        }\n+\n+        \/\/ Index vector: vix[0:n] = k -> offset + indexMap[mapOffset + k]\n+        IntVector vix;\n+        if (isp.laneCount() != vsp.laneCount()) {\n+            \/\/ For LongMaxVector,  if vector length is non-power-of-two or\n+            \/\/ 2048 bits, indexShape of Long species is S_MAX_BIT.\n+            \/\/ Assume that vector length is 2048, then the lane count of Long\n+            \/\/ vector is 32. When converting Long species to int species,\n+            \/\/ indexShape is still S_MAX_BIT, but the lane count of int vector\n+            \/\/ is 64. So when loading index vector (IntVector), only lower half\n+            \/\/ of index data is needed.\n+            vix = IntVector\n+                .fromArray(isp, indexMap, mapOffset, IntMaxVector.IntMaxMask.LOWER_HALF_TRUE_MASK)\n+                .add(offset);\n+        } else {\n+            vix = IntVector\n+                .fromArray(isp, indexMap, mapOffset)\n+                .add(offset);\n+        }\n+\n+        \/\/ FIXME: Check index under mask controlling.\n+        vix = VectorIntrinsics.checkIndex(vix, a.length);\n+\n+        return VectorSupport.loadWithMap(\n+            vectorType, maskClass, long.class, vsp.laneCount(),\n+            isp.vectorType(),\n+            a, ARRAY_BASE, vix, m,\n+            a, offset, indexMap, mapOffset, vsp,\n+            (c, idx, iMap, idy, s, vm) ->\n+            s.vOp(vm, n -> c[idx + iMap[idy+n]]));\n+    }\n+\n@@ -3180,0 +3397,19 @@\n+    abstract\n+    LongVector fromByteArray0(byte[] a, int offset, VectorMask<Long> m);\n+    @ForceInline\n+    final\n+    <M extends VectorMask<Long>>\n+    LongVector fromByteArray0Template(Class<M> maskClass, byte[] a, int offset, M m) {\n+        LongSpecies vsp = vspecies();\n+        m.check(vsp);\n+        return VectorSupport.loadMasked(\n+            vsp.vectorType(), maskClass, vsp.elementType(), vsp.laneCount(),\n+            a, byteArrayAddress(a, offset), m,\n+            a, offset, vsp,\n+            (arr, off, s, vm) -> {\n+                ByteBuffer wb = wrapper(arr, NATIVE_ENDIAN);\n+                return s.ldOp(wb, off, vm,\n+                        (wb_, o, i) -> wb_.getLong(o + i * 8));\n+            });\n+    }\n+\n@@ -3196,0 +3432,18 @@\n+    abstract\n+    LongVector fromByteBuffer0(ByteBuffer bb, int offset, VectorMask<Long> m);\n+    @ForceInline\n+    final\n+    <M extends VectorMask<Long>>\n+    LongVector fromByteBuffer0Template(Class<M> maskClass, ByteBuffer bb, int offset, M m) {\n+        LongSpecies vsp = vspecies();\n+        m.check(vsp);\n+        return ScopedMemoryAccess.loadFromByteBufferMasked(\n+                vsp.vectorType(), maskClass, vsp.elementType(), vsp.laneCount(),\n+                bb, offset, m, vsp,\n+                (buf, off, s, vm) -> {\n+                    ByteBuffer wb = wrapper(buf, NATIVE_ENDIAN);\n+                    return s.ldOp(wb, off, vm,\n+                            (wb_, o, i) -> wb_.getLong(o + i * 8));\n+                });\n+    }\n+\n@@ -3215,0 +3469,71 @@\n+    abstract\n+    void intoArray0(long[] a, int offset, VectorMask<Long> m);\n+    @ForceInline\n+    final\n+    <M extends VectorMask<Long>>\n+    void intoArray0Template(Class<M> maskClass, long[] a, int offset, M m) {\n+        m.check(species());\n+        LongSpecies vsp = vspecies();\n+        VectorSupport.storeMasked(\n+            vsp.vectorType(), maskClass, vsp.elementType(), vsp.laneCount(),\n+            a, arrayAddress(a, offset),\n+            this, m, a, offset,\n+            (arr, off, v, vm)\n+            -> v.stOp(arr, off, vm,\n+                      (arr_, off_, i, e) -> arr_[off_ + i] = e));\n+    }\n+\n+    abstract\n+    void intoArray0(long[] a, int offset,\n+                    int[] indexMap, int mapOffset,\n+                    VectorMask<Long> m);\n+    @ForceInline\n+    final\n+    <M extends VectorMask<Long>>\n+    void intoArray0Template(Class<M> maskClass, long[] a, int offset,\n+                            int[] indexMap, int mapOffset, M m) {\n+        m.check(species());\n+        LongSpecies vsp = vspecies();\n+        IntVector.IntSpecies isp = IntVector.species(vsp.indexShape());\n+        if (vsp.laneCount() == 1) {\n+            intoArray(a, offset + indexMap[mapOffset], m);\n+            return;\n+        }\n+\n+        \/\/ Index vector: vix[0:n] = i -> offset + indexMap[mo + i]\n+        IntVector vix;\n+        if (isp.laneCount() != vsp.laneCount()) {\n+            \/\/ For LongMaxVector,  if vector length  is 2048 bits, indexShape\n+            \/\/ of Long species is S_MAX_BIT. and the lane count of Long\n+            \/\/ vector is 32. When converting Long species to int species,\n+            \/\/ indexShape is still S_MAX_BIT, but the lane count of int vector\n+            \/\/ is 64. So when loading index vector (IntVector), only lower half\n+            \/\/ of index data is needed.\n+            vix = IntVector\n+                .fromArray(isp, indexMap, mapOffset, IntMaxVector.IntMaxMask.LOWER_HALF_TRUE_MASK)\n+                .add(offset);\n+        } else {\n+            vix = IntVector\n+                .fromArray(isp, indexMap, mapOffset)\n+                .add(offset);\n+        }\n+\n+\n+        \/\/ FIXME: Check index under mask controlling.\n+        vix = VectorIntrinsics.checkIndex(vix, a.length);\n+\n+        VectorSupport.storeWithMap(\n+            vsp.vectorType(), maskClass, vsp.elementType(), vsp.laneCount(),\n+            isp.vectorType(),\n+            a, arrayAddress(a, 0), vix,\n+            this, m,\n+            a, offset, indexMap, mapOffset,\n+            (arr, off, v, map, mo, vm)\n+            -> v.stOp(arr, off, vm,\n+                      (arr_, off_, i, e) -> {\n+                          int j = map[mo + i];\n+                          arr[off + j] = e;\n+                      }));\n+    }\n+\n+\n@@ -3232,0 +3557,19 @@\n+    abstract\n+    void intoByteArray0(byte[] a, int offset, VectorMask<Long> m);\n+    @ForceInline\n+    final\n+    <M extends VectorMask<Long>>\n+    void intoByteArray0Template(Class<M> maskClass, byte[] a, int offset, M m) {\n+        LongSpecies vsp = vspecies();\n+        m.check(vsp);\n+        VectorSupport.storeMasked(\n+            vsp.vectorType(), maskClass, vsp.elementType(), vsp.laneCount(),\n+            a, byteArrayAddress(a, offset),\n+            this, m, a, offset,\n+            (arr, off, v, vm) -> {\n+                ByteBuffer wb = wrapper(arr, NATIVE_ENDIAN);\n+                v.stOp(wb, off, vm,\n+                        (tb_, o, i, e) -> tb_.putLong(o + i * 8, e));\n+            });\n+    }\n+\n@@ -3246,0 +3590,19 @@\n+    abstract\n+    void intoByteBuffer0(ByteBuffer bb, int offset, VectorMask<Long> m);\n+    @ForceInline\n+    final\n+    <M extends VectorMask<Long>>\n+    void intoByteBuffer0Template(Class<M> maskClass, ByteBuffer bb, int offset, M m) {\n+        LongSpecies vsp = vspecies();\n+        m.check(vsp);\n+        ScopedMemoryAccess.storeIntoByteBufferMasked(\n+                vsp.vectorType(), maskClass, vsp.elementType(), vsp.laneCount(),\n+                this, m, bb, offset,\n+                (buf, off, v, vm) -> {\n+                    ByteBuffer wb = wrapper(buf, NATIVE_ENDIAN);\n+                    v.stOp(wb, off, vm,\n+                            (wb_, o, i, e) -> wb_.putLong(o + i * 8, e));\n+                });\n+    }\n+\n+\n@@ -3554,1 +3917,1 @@\n-                                      AbstractMask<Long> m,\n+                                      VectorMask<Long> m,\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/LongVector.java","additions":564,"deletions":201,"binary":false,"changes":765,"status":"modified"},{"patch":"@@ -239,2 +239,2 @@\n-    short rOp(short v, FBinOp f) {\n-        return super.rOpTemplate(v, f);  \/\/ specialize\n+    short rOp(short v, VectorMask<Short> m, FBinOp f) {\n+        return super.rOpTemplate(v, m, f);  \/\/ specialize\n@@ -276,0 +276,6 @@\n+    @Override\n+    @ForceInline\n+    public Short128Vector lanewise(Unary op, VectorMask<Short> m) {\n+        return (Short128Vector) super.lanewiseTemplate(op, Short128Mask.class, (Short128Mask) m);  \/\/ specialize\n+    }\n+\n@@ -282,0 +288,6 @@\n+    @Override\n+    @ForceInline\n+    public Short128Vector lanewise(Binary op, Vector<Short> v, VectorMask<Short> m) {\n+        return (Short128Vector) super.lanewiseTemplate(op, Short128Mask.class, v, (Short128Mask) m);  \/\/ specialize\n+    }\n+\n@@ -289,0 +301,7 @@\n+    \/*package-private*\/\n+    @Override\n+    @ForceInline Short128Vector\n+    lanewiseShift(VectorOperators.Binary op, int e, VectorMask<Short> m) {\n+        return (Short128Vector) super.lanewiseShiftTemplate(op, Short128Mask.class, e, (Short128Mask) m);  \/\/ specialize\n+    }\n+\n@@ -294,1 +313,1 @@\n-    lanewise(VectorOperators.Ternary op, Vector<Short> v1, Vector<Short> v2) {\n+    lanewise(Ternary op, Vector<Short> v1, Vector<Short> v2) {\n@@ -298,0 +317,8 @@\n+    @Override\n+    @ForceInline\n+    public final\n+    Short128Vector\n+    lanewise(Ternary op, Vector<Short> v1, Vector<Short> v2, VectorMask<Short> m) {\n+        return (Short128Vector) super.lanewiseTemplate(op, Short128Mask.class, v1, v2, (Short128Mask) m);  \/\/ specialize\n+    }\n+\n@@ -317,1 +344,1 @@\n-        return super.reduceLanesTemplate(op, m);  \/\/ specialized\n+        return super.reduceLanesTemplate(op, Short128Mask.class, (Short128Mask) m);  \/\/ specialized\n@@ -330,1 +357,1 @@\n-        return (long) super.reduceLanesTemplate(op, m);  \/\/ specialized\n+        return (long) super.reduceLanesTemplate(op, Short128Mask.class, (Short128Mask) m);  \/\/ specialized\n@@ -366,0 +393,7 @@\n+    @Override\n+    @ForceInline\n+    public final Short128Mask compare(Comparison op, Vector<Short> v, VectorMask<Short> m) {\n+        return super.compareTemplate(Short128Mask.class, op, v, (Short128Mask) m);\n+    }\n+\n+\n@@ -422,0 +456,1 @@\n+                                    Short128Mask.class,\n@@ -599,10 +634,6 @@\n-            if (VSIZE == species.vectorBitSize()) {\n-                Class<?> dtype = species.elementType();\n-                Class<?> dmtype = species.maskType();\n-                return VectorSupport.convert(VectorSupport.VECTOR_OP_REINTERPRET,\n-                    this.getClass(), ETYPE, VLENGTH,\n-                    dmtype, dtype, VLENGTH,\n-                    this, species,\n-                    Short128Mask::defaultMaskCast);\n-            }\n-            return this.defaultMaskCast(species);\n+\n+            return VectorSupport.convert(VectorSupport.VECTOR_OP_CAST,\n+                this.getClass(), ETYPE, VLENGTH,\n+                species.maskType(), species.elementType(), VLENGTH,\n+                this, species,\n+                (m, s) -> s.maskFactory(m.toArray()).check(s));\n@@ -634,3 +665,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_AND, Short128Mask.class, short.class, VLENGTH,\n-                                             this, m,\n-                                             (m1, m2) -> m1.bOp(m2, (i, a, b) -> a & b));\n+            return VectorSupport.binaryOp(VECTOR_OP_AND, Short128Mask.class, null, short.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a & b));\n@@ -644,3 +675,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_OR, Short128Mask.class, short.class, VLENGTH,\n-                                             this, m,\n-                                             (m1, m2) -> m1.bOp(m2, (i, a, b) -> a | b));\n+            return VectorSupport.binaryOp(VECTOR_OP_OR, Short128Mask.class, null, short.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a | b));\n@@ -654,3 +685,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_XOR, Short128Mask.class, short.class, VLENGTH,\n-                                          this, m,\n-                                          (m1, m2) -> m1.bOp(m2, (i, a, b) -> a ^ b));\n+            return VectorSupport.binaryOp(VECTOR_OP_XOR, Short128Mask.class, null, short.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a ^ b));\n@@ -664,2 +695,2 @@\n-            return VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_TRUECOUNT, Short128Mask.class, short.class, VLENGTH, this,\n-                                                      (m) -> trueCountHelper(((Short128Mask)m).getBits()));\n+            return (int) VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_TRUECOUNT, Short128Mask.class, short.class, VLENGTH, this,\n+                                                      (m) -> trueCountHelper(m.getBits()));\n@@ -671,2 +702,2 @@\n-            return VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_FIRSTTRUE, Short128Mask.class, short.class, VLENGTH, this,\n-                                                      (m) -> firstTrueHelper(((Short128Mask)m).getBits()));\n+            return (int) VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_FIRSTTRUE, Short128Mask.class, short.class, VLENGTH, this,\n+                                                      (m) -> firstTrueHelper(m.getBits()));\n@@ -678,2 +709,12 @@\n-            return VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_LASTTRUE, Short128Mask.class, short.class, VLENGTH, this,\n-                                                      (m) -> lastTrueHelper(((Short128Mask)m).getBits()));\n+            return (int) VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_LASTTRUE, Short128Mask.class, short.class, VLENGTH, this,\n+                                                      (m) -> lastTrueHelper(m.getBits()));\n+        }\n+\n+        @Override\n+        @ForceInline\n+        public long toLong() {\n+            if (length() > Long.SIZE) {\n+                throw new UnsupportedOperationException(\"too many lanes for one long\");\n+            }\n+            return VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_TOLONG, Short128Mask.class, short.class, VLENGTH, this,\n+                                                      (m) -> toLongHelper(m.getBits()));\n@@ -790,0 +831,8 @@\n+    @ForceInline\n+    @Override\n+    final\n+    ShortVector fromArray0(short[] a, int offset, VectorMask<Short> m) {\n+        return super.fromArray0Template(Short128Mask.class, a, offset, (Short128Mask) m);  \/\/ specialize\n+    }\n+\n+\n@@ -797,0 +846,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    ShortVector fromCharArray0(char[] a, int offset, VectorMask<Short> m) {\n+        return super.fromCharArray0Template(Short128Mask.class, a, offset, (Short128Mask) m);  \/\/ specialize\n+    }\n+\n@@ -805,0 +861,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    ShortVector fromByteArray0(byte[] a, int offset, VectorMask<Short> m) {\n+        return super.fromByteArray0Template(Short128Mask.class, a, offset, (Short128Mask) m);  \/\/ specialize\n+    }\n+\n@@ -812,0 +875,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    ShortVector fromByteBuffer0(ByteBuffer bb, int offset, VectorMask<Short> m) {\n+        return super.fromByteBuffer0Template(Short128Mask.class, bb, offset, (Short128Mask) m);  \/\/ specialize\n+    }\n+\n@@ -819,0 +889,9 @@\n+    @ForceInline\n+    @Override\n+    final\n+    void intoArray0(short[] a, int offset, VectorMask<Short> m) {\n+        super.intoArray0Template(Short128Mask.class, a, offset, (Short128Mask) m);\n+    }\n+\n+\n+\n@@ -826,0 +905,21 @@\n+    @ForceInline\n+    @Override\n+    final\n+    void intoByteArray0(byte[] a, int offset, VectorMask<Short> m) {\n+        super.intoByteArray0Template(Short128Mask.class, a, offset, (Short128Mask) m);  \/\/ specialize\n+    }\n+\n+    @ForceInline\n+    @Override\n+    final\n+    void intoByteBuffer0(ByteBuffer bb, int offset, VectorMask<Short> m) {\n+        super.intoByteBuffer0Template(Short128Mask.class, bb, offset, (Short128Mask) m);\n+    }\n+\n+    @ForceInline\n+    @Override\n+    final\n+    void intoCharArray0(char[] a, int offset, VectorMask<Short> m) {\n+        super.intoCharArray0Template(Short128Mask.class, a, offset, (Short128Mask) m);\n+    }\n+\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/Short128Vector.java","additions":130,"deletions":30,"binary":false,"changes":160,"status":"modified"},{"patch":"@@ -239,2 +239,2 @@\n-    short rOp(short v, FBinOp f) {\n-        return super.rOpTemplate(v, f);  \/\/ specialize\n+    short rOp(short v, VectorMask<Short> m, FBinOp f) {\n+        return super.rOpTemplate(v, m, f);  \/\/ specialize\n@@ -276,0 +276,6 @@\n+    @Override\n+    @ForceInline\n+    public Short256Vector lanewise(Unary op, VectorMask<Short> m) {\n+        return (Short256Vector) super.lanewiseTemplate(op, Short256Mask.class, (Short256Mask) m);  \/\/ specialize\n+    }\n+\n@@ -282,0 +288,6 @@\n+    @Override\n+    @ForceInline\n+    public Short256Vector lanewise(Binary op, Vector<Short> v, VectorMask<Short> m) {\n+        return (Short256Vector) super.lanewiseTemplate(op, Short256Mask.class, v, (Short256Mask) m);  \/\/ specialize\n+    }\n+\n@@ -289,0 +301,7 @@\n+    \/*package-private*\/\n+    @Override\n+    @ForceInline Short256Vector\n+    lanewiseShift(VectorOperators.Binary op, int e, VectorMask<Short> m) {\n+        return (Short256Vector) super.lanewiseShiftTemplate(op, Short256Mask.class, e, (Short256Mask) m);  \/\/ specialize\n+    }\n+\n@@ -294,1 +313,1 @@\n-    lanewise(VectorOperators.Ternary op, Vector<Short> v1, Vector<Short> v2) {\n+    lanewise(Ternary op, Vector<Short> v1, Vector<Short> v2) {\n@@ -298,0 +317,8 @@\n+    @Override\n+    @ForceInline\n+    public final\n+    Short256Vector\n+    lanewise(Ternary op, Vector<Short> v1, Vector<Short> v2, VectorMask<Short> m) {\n+        return (Short256Vector) super.lanewiseTemplate(op, Short256Mask.class, v1, v2, (Short256Mask) m);  \/\/ specialize\n+    }\n+\n@@ -317,1 +344,1 @@\n-        return super.reduceLanesTemplate(op, m);  \/\/ specialized\n+        return super.reduceLanesTemplate(op, Short256Mask.class, (Short256Mask) m);  \/\/ specialized\n@@ -330,1 +357,1 @@\n-        return (long) super.reduceLanesTemplate(op, m);  \/\/ specialized\n+        return (long) super.reduceLanesTemplate(op, Short256Mask.class, (Short256Mask) m);  \/\/ specialized\n@@ -366,0 +393,7 @@\n+    @Override\n+    @ForceInline\n+    public final Short256Mask compare(Comparison op, Vector<Short> v, VectorMask<Short> m) {\n+        return super.compareTemplate(Short256Mask.class, op, v, (Short256Mask) m);\n+    }\n+\n+\n@@ -422,0 +456,1 @@\n+                                    Short256Mask.class,\n@@ -615,10 +650,6 @@\n-            if (VSIZE == species.vectorBitSize()) {\n-                Class<?> dtype = species.elementType();\n-                Class<?> dmtype = species.maskType();\n-                return VectorSupport.convert(VectorSupport.VECTOR_OP_REINTERPRET,\n-                    this.getClass(), ETYPE, VLENGTH,\n-                    dmtype, dtype, VLENGTH,\n-                    this, species,\n-                    Short256Mask::defaultMaskCast);\n-            }\n-            return this.defaultMaskCast(species);\n+\n+            return VectorSupport.convert(VectorSupport.VECTOR_OP_CAST,\n+                this.getClass(), ETYPE, VLENGTH,\n+                species.maskType(), species.elementType(), VLENGTH,\n+                this, species,\n+                (m, s) -> s.maskFactory(m.toArray()).check(s));\n@@ -650,3 +681,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_AND, Short256Mask.class, short.class, VLENGTH,\n-                                             this, m,\n-                                             (m1, m2) -> m1.bOp(m2, (i, a, b) -> a & b));\n+            return VectorSupport.binaryOp(VECTOR_OP_AND, Short256Mask.class, null, short.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a & b));\n@@ -660,3 +691,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_OR, Short256Mask.class, short.class, VLENGTH,\n-                                             this, m,\n-                                             (m1, m2) -> m1.bOp(m2, (i, a, b) -> a | b));\n+            return VectorSupport.binaryOp(VECTOR_OP_OR, Short256Mask.class, null, short.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a | b));\n@@ -670,3 +701,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_XOR, Short256Mask.class, short.class, VLENGTH,\n-                                          this, m,\n-                                          (m1, m2) -> m1.bOp(m2, (i, a, b) -> a ^ b));\n+            return VectorSupport.binaryOp(VECTOR_OP_XOR, Short256Mask.class, null, short.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a ^ b));\n@@ -680,2 +711,2 @@\n-            return VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_TRUECOUNT, Short256Mask.class, short.class, VLENGTH, this,\n-                                                      (m) -> trueCountHelper(((Short256Mask)m).getBits()));\n+            return (int) VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_TRUECOUNT, Short256Mask.class, short.class, VLENGTH, this,\n+                                                      (m) -> trueCountHelper(m.getBits()));\n@@ -687,2 +718,2 @@\n-            return VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_FIRSTTRUE, Short256Mask.class, short.class, VLENGTH, this,\n-                                                      (m) -> firstTrueHelper(((Short256Mask)m).getBits()));\n+            return (int) VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_FIRSTTRUE, Short256Mask.class, short.class, VLENGTH, this,\n+                                                      (m) -> firstTrueHelper(m.getBits()));\n@@ -694,2 +725,12 @@\n-            return VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_LASTTRUE, Short256Mask.class, short.class, VLENGTH, this,\n-                                                      (m) -> lastTrueHelper(((Short256Mask)m).getBits()));\n+            return (int) VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_LASTTRUE, Short256Mask.class, short.class, VLENGTH, this,\n+                                                      (m) -> lastTrueHelper(m.getBits()));\n+        }\n+\n+        @Override\n+        @ForceInline\n+        public long toLong() {\n+            if (length() > Long.SIZE) {\n+                throw new UnsupportedOperationException(\"too many lanes for one long\");\n+            }\n+            return VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_TOLONG, Short256Mask.class, short.class, VLENGTH, this,\n+                                                      (m) -> toLongHelper(m.getBits()));\n@@ -806,0 +847,8 @@\n+    @ForceInline\n+    @Override\n+    final\n+    ShortVector fromArray0(short[] a, int offset, VectorMask<Short> m) {\n+        return super.fromArray0Template(Short256Mask.class, a, offset, (Short256Mask) m);  \/\/ specialize\n+    }\n+\n+\n@@ -813,0 +862,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    ShortVector fromCharArray0(char[] a, int offset, VectorMask<Short> m) {\n+        return super.fromCharArray0Template(Short256Mask.class, a, offset, (Short256Mask) m);  \/\/ specialize\n+    }\n+\n@@ -821,0 +877,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    ShortVector fromByteArray0(byte[] a, int offset, VectorMask<Short> m) {\n+        return super.fromByteArray0Template(Short256Mask.class, a, offset, (Short256Mask) m);  \/\/ specialize\n+    }\n+\n@@ -828,0 +891,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    ShortVector fromByteBuffer0(ByteBuffer bb, int offset, VectorMask<Short> m) {\n+        return super.fromByteBuffer0Template(Short256Mask.class, bb, offset, (Short256Mask) m);  \/\/ specialize\n+    }\n+\n@@ -835,0 +905,9 @@\n+    @ForceInline\n+    @Override\n+    final\n+    void intoArray0(short[] a, int offset, VectorMask<Short> m) {\n+        super.intoArray0Template(Short256Mask.class, a, offset, (Short256Mask) m);\n+    }\n+\n+\n+\n@@ -842,0 +921,21 @@\n+    @ForceInline\n+    @Override\n+    final\n+    void intoByteArray0(byte[] a, int offset, VectorMask<Short> m) {\n+        super.intoByteArray0Template(Short256Mask.class, a, offset, (Short256Mask) m);  \/\/ specialize\n+    }\n+\n+    @ForceInline\n+    @Override\n+    final\n+    void intoByteBuffer0(ByteBuffer bb, int offset, VectorMask<Short> m) {\n+        super.intoByteBuffer0Template(Short256Mask.class, bb, offset, (Short256Mask) m);\n+    }\n+\n+    @ForceInline\n+    @Override\n+    final\n+    void intoCharArray0(char[] a, int offset, VectorMask<Short> m) {\n+        super.intoCharArray0Template(Short256Mask.class, a, offset, (Short256Mask) m);\n+    }\n+\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/Short256Vector.java","additions":130,"deletions":30,"binary":false,"changes":160,"status":"modified"},{"patch":"@@ -239,2 +239,2 @@\n-    short rOp(short v, FBinOp f) {\n-        return super.rOpTemplate(v, f);  \/\/ specialize\n+    short rOp(short v, VectorMask<Short> m, FBinOp f) {\n+        return super.rOpTemplate(v, m, f);  \/\/ specialize\n@@ -276,0 +276,6 @@\n+    @Override\n+    @ForceInline\n+    public Short512Vector lanewise(Unary op, VectorMask<Short> m) {\n+        return (Short512Vector) super.lanewiseTemplate(op, Short512Mask.class, (Short512Mask) m);  \/\/ specialize\n+    }\n+\n@@ -282,0 +288,6 @@\n+    @Override\n+    @ForceInline\n+    public Short512Vector lanewise(Binary op, Vector<Short> v, VectorMask<Short> m) {\n+        return (Short512Vector) super.lanewiseTemplate(op, Short512Mask.class, v, (Short512Mask) m);  \/\/ specialize\n+    }\n+\n@@ -289,0 +301,7 @@\n+    \/*package-private*\/\n+    @Override\n+    @ForceInline Short512Vector\n+    lanewiseShift(VectorOperators.Binary op, int e, VectorMask<Short> m) {\n+        return (Short512Vector) super.lanewiseShiftTemplate(op, Short512Mask.class, e, (Short512Mask) m);  \/\/ specialize\n+    }\n+\n@@ -294,1 +313,1 @@\n-    lanewise(VectorOperators.Ternary op, Vector<Short> v1, Vector<Short> v2) {\n+    lanewise(Ternary op, Vector<Short> v1, Vector<Short> v2) {\n@@ -298,0 +317,8 @@\n+    @Override\n+    @ForceInline\n+    public final\n+    Short512Vector\n+    lanewise(Ternary op, Vector<Short> v1, Vector<Short> v2, VectorMask<Short> m) {\n+        return (Short512Vector) super.lanewiseTemplate(op, Short512Mask.class, v1, v2, (Short512Mask) m);  \/\/ specialize\n+    }\n+\n@@ -317,1 +344,1 @@\n-        return super.reduceLanesTemplate(op, m);  \/\/ specialized\n+        return super.reduceLanesTemplate(op, Short512Mask.class, (Short512Mask) m);  \/\/ specialized\n@@ -330,1 +357,1 @@\n-        return (long) super.reduceLanesTemplate(op, m);  \/\/ specialized\n+        return (long) super.reduceLanesTemplate(op, Short512Mask.class, (Short512Mask) m);  \/\/ specialized\n@@ -366,0 +393,7 @@\n+    @Override\n+    @ForceInline\n+    public final Short512Mask compare(Comparison op, Vector<Short> v, VectorMask<Short> m) {\n+        return super.compareTemplate(Short512Mask.class, op, v, (Short512Mask) m);\n+    }\n+\n+\n@@ -422,0 +456,1 @@\n+                                    Short512Mask.class,\n@@ -647,10 +682,6 @@\n-            if (VSIZE == species.vectorBitSize()) {\n-                Class<?> dtype = species.elementType();\n-                Class<?> dmtype = species.maskType();\n-                return VectorSupport.convert(VectorSupport.VECTOR_OP_REINTERPRET,\n-                    this.getClass(), ETYPE, VLENGTH,\n-                    dmtype, dtype, VLENGTH,\n-                    this, species,\n-                    Short512Mask::defaultMaskCast);\n-            }\n-            return this.defaultMaskCast(species);\n+\n+            return VectorSupport.convert(VectorSupport.VECTOR_OP_CAST,\n+                this.getClass(), ETYPE, VLENGTH,\n+                species.maskType(), species.elementType(), VLENGTH,\n+                this, species,\n+                (m, s) -> s.maskFactory(m.toArray()).check(s));\n@@ -682,3 +713,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_AND, Short512Mask.class, short.class, VLENGTH,\n-                                             this, m,\n-                                             (m1, m2) -> m1.bOp(m2, (i, a, b) -> a & b));\n+            return VectorSupport.binaryOp(VECTOR_OP_AND, Short512Mask.class, null, short.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a & b));\n@@ -692,3 +723,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_OR, Short512Mask.class, short.class, VLENGTH,\n-                                             this, m,\n-                                             (m1, m2) -> m1.bOp(m2, (i, a, b) -> a | b));\n+            return VectorSupport.binaryOp(VECTOR_OP_OR, Short512Mask.class, null, short.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a | b));\n@@ -702,3 +733,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_XOR, Short512Mask.class, short.class, VLENGTH,\n-                                          this, m,\n-                                          (m1, m2) -> m1.bOp(m2, (i, a, b) -> a ^ b));\n+            return VectorSupport.binaryOp(VECTOR_OP_XOR, Short512Mask.class, null, short.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a ^ b));\n@@ -712,2 +743,2 @@\n-            return VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_TRUECOUNT, Short512Mask.class, short.class, VLENGTH, this,\n-                                                      (m) -> trueCountHelper(((Short512Mask)m).getBits()));\n+            return (int) VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_TRUECOUNT, Short512Mask.class, short.class, VLENGTH, this,\n+                                                      (m) -> trueCountHelper(m.getBits()));\n@@ -719,2 +750,2 @@\n-            return VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_FIRSTTRUE, Short512Mask.class, short.class, VLENGTH, this,\n-                                                      (m) -> firstTrueHelper(((Short512Mask)m).getBits()));\n+            return (int) VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_FIRSTTRUE, Short512Mask.class, short.class, VLENGTH, this,\n+                                                      (m) -> firstTrueHelper(m.getBits()));\n@@ -726,2 +757,12 @@\n-            return VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_LASTTRUE, Short512Mask.class, short.class, VLENGTH, this,\n-                                                      (m) -> lastTrueHelper(((Short512Mask)m).getBits()));\n+            return (int) VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_LASTTRUE, Short512Mask.class, short.class, VLENGTH, this,\n+                                                      (m) -> lastTrueHelper(m.getBits()));\n+        }\n+\n+        @Override\n+        @ForceInline\n+        public long toLong() {\n+            if (length() > Long.SIZE) {\n+                throw new UnsupportedOperationException(\"too many lanes for one long\");\n+            }\n+            return VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_TOLONG, Short512Mask.class, short.class, VLENGTH, this,\n+                                                      (m) -> toLongHelper(m.getBits()));\n@@ -838,0 +879,8 @@\n+    @ForceInline\n+    @Override\n+    final\n+    ShortVector fromArray0(short[] a, int offset, VectorMask<Short> m) {\n+        return super.fromArray0Template(Short512Mask.class, a, offset, (Short512Mask) m);  \/\/ specialize\n+    }\n+\n+\n@@ -845,0 +894,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    ShortVector fromCharArray0(char[] a, int offset, VectorMask<Short> m) {\n+        return super.fromCharArray0Template(Short512Mask.class, a, offset, (Short512Mask) m);  \/\/ specialize\n+    }\n+\n@@ -853,0 +909,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    ShortVector fromByteArray0(byte[] a, int offset, VectorMask<Short> m) {\n+        return super.fromByteArray0Template(Short512Mask.class, a, offset, (Short512Mask) m);  \/\/ specialize\n+    }\n+\n@@ -860,0 +923,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    ShortVector fromByteBuffer0(ByteBuffer bb, int offset, VectorMask<Short> m) {\n+        return super.fromByteBuffer0Template(Short512Mask.class, bb, offset, (Short512Mask) m);  \/\/ specialize\n+    }\n+\n@@ -867,0 +937,9 @@\n+    @ForceInline\n+    @Override\n+    final\n+    void intoArray0(short[] a, int offset, VectorMask<Short> m) {\n+        super.intoArray0Template(Short512Mask.class, a, offset, (Short512Mask) m);\n+    }\n+\n+\n+\n@@ -874,0 +953,21 @@\n+    @ForceInline\n+    @Override\n+    final\n+    void intoByteArray0(byte[] a, int offset, VectorMask<Short> m) {\n+        super.intoByteArray0Template(Short512Mask.class, a, offset, (Short512Mask) m);  \/\/ specialize\n+    }\n+\n+    @ForceInline\n+    @Override\n+    final\n+    void intoByteBuffer0(ByteBuffer bb, int offset, VectorMask<Short> m) {\n+        super.intoByteBuffer0Template(Short512Mask.class, bb, offset, (Short512Mask) m);\n+    }\n+\n+    @ForceInline\n+    @Override\n+    final\n+    void intoCharArray0(char[] a, int offset, VectorMask<Short> m) {\n+        super.intoCharArray0Template(Short512Mask.class, a, offset, (Short512Mask) m);\n+    }\n+\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/Short512Vector.java","additions":130,"deletions":30,"binary":false,"changes":160,"status":"modified"},{"patch":"@@ -239,2 +239,2 @@\n-    short rOp(short v, FBinOp f) {\n-        return super.rOpTemplate(v, f);  \/\/ specialize\n+    short rOp(short v, VectorMask<Short> m, FBinOp f) {\n+        return super.rOpTemplate(v, m, f);  \/\/ specialize\n@@ -276,0 +276,6 @@\n+    @Override\n+    @ForceInline\n+    public Short64Vector lanewise(Unary op, VectorMask<Short> m) {\n+        return (Short64Vector) super.lanewiseTemplate(op, Short64Mask.class, (Short64Mask) m);  \/\/ specialize\n+    }\n+\n@@ -282,0 +288,6 @@\n+    @Override\n+    @ForceInline\n+    public Short64Vector lanewise(Binary op, Vector<Short> v, VectorMask<Short> m) {\n+        return (Short64Vector) super.lanewiseTemplate(op, Short64Mask.class, v, (Short64Mask) m);  \/\/ specialize\n+    }\n+\n@@ -289,0 +301,7 @@\n+    \/*package-private*\/\n+    @Override\n+    @ForceInline Short64Vector\n+    lanewiseShift(VectorOperators.Binary op, int e, VectorMask<Short> m) {\n+        return (Short64Vector) super.lanewiseShiftTemplate(op, Short64Mask.class, e, (Short64Mask) m);  \/\/ specialize\n+    }\n+\n@@ -294,1 +313,1 @@\n-    lanewise(VectorOperators.Ternary op, Vector<Short> v1, Vector<Short> v2) {\n+    lanewise(Ternary op, Vector<Short> v1, Vector<Short> v2) {\n@@ -298,0 +317,8 @@\n+    @Override\n+    @ForceInline\n+    public final\n+    Short64Vector\n+    lanewise(Ternary op, Vector<Short> v1, Vector<Short> v2, VectorMask<Short> m) {\n+        return (Short64Vector) super.lanewiseTemplate(op, Short64Mask.class, v1, v2, (Short64Mask) m);  \/\/ specialize\n+    }\n+\n@@ -317,1 +344,1 @@\n-        return super.reduceLanesTemplate(op, m);  \/\/ specialized\n+        return super.reduceLanesTemplate(op, Short64Mask.class, (Short64Mask) m);  \/\/ specialized\n@@ -330,1 +357,1 @@\n-        return (long) super.reduceLanesTemplate(op, m);  \/\/ specialized\n+        return (long) super.reduceLanesTemplate(op, Short64Mask.class, (Short64Mask) m);  \/\/ specialized\n@@ -366,0 +393,7 @@\n+    @Override\n+    @ForceInline\n+    public final Short64Mask compare(Comparison op, Vector<Short> v, VectorMask<Short> m) {\n+        return super.compareTemplate(Short64Mask.class, op, v, (Short64Mask) m);\n+    }\n+\n+\n@@ -422,0 +456,1 @@\n+                                    Short64Mask.class,\n@@ -591,10 +626,6 @@\n-            if (VSIZE == species.vectorBitSize()) {\n-                Class<?> dtype = species.elementType();\n-                Class<?> dmtype = species.maskType();\n-                return VectorSupport.convert(VectorSupport.VECTOR_OP_REINTERPRET,\n-                    this.getClass(), ETYPE, VLENGTH,\n-                    dmtype, dtype, VLENGTH,\n-                    this, species,\n-                    Short64Mask::defaultMaskCast);\n-            }\n-            return this.defaultMaskCast(species);\n+\n+            return VectorSupport.convert(VectorSupport.VECTOR_OP_CAST,\n+                this.getClass(), ETYPE, VLENGTH,\n+                species.maskType(), species.elementType(), VLENGTH,\n+                this, species,\n+                (m, s) -> s.maskFactory(m.toArray()).check(s));\n@@ -626,3 +657,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_AND, Short64Mask.class, short.class, VLENGTH,\n-                                             this, m,\n-                                             (m1, m2) -> m1.bOp(m2, (i, a, b) -> a & b));\n+            return VectorSupport.binaryOp(VECTOR_OP_AND, Short64Mask.class, null, short.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a & b));\n@@ -636,3 +667,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_OR, Short64Mask.class, short.class, VLENGTH,\n-                                             this, m,\n-                                             (m1, m2) -> m1.bOp(m2, (i, a, b) -> a | b));\n+            return VectorSupport.binaryOp(VECTOR_OP_OR, Short64Mask.class, null, short.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a | b));\n@@ -646,3 +677,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_XOR, Short64Mask.class, short.class, VLENGTH,\n-                                          this, m,\n-                                          (m1, m2) -> m1.bOp(m2, (i, a, b) -> a ^ b));\n+            return VectorSupport.binaryOp(VECTOR_OP_XOR, Short64Mask.class, null, short.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a ^ b));\n@@ -656,2 +687,2 @@\n-            return VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_TRUECOUNT, Short64Mask.class, short.class, VLENGTH, this,\n-                                                      (m) -> trueCountHelper(((Short64Mask)m).getBits()));\n+            return (int) VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_TRUECOUNT, Short64Mask.class, short.class, VLENGTH, this,\n+                                                      (m) -> trueCountHelper(m.getBits()));\n@@ -663,2 +694,2 @@\n-            return VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_FIRSTTRUE, Short64Mask.class, short.class, VLENGTH, this,\n-                                                      (m) -> firstTrueHelper(((Short64Mask)m).getBits()));\n+            return (int) VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_FIRSTTRUE, Short64Mask.class, short.class, VLENGTH, this,\n+                                                      (m) -> firstTrueHelper(m.getBits()));\n@@ -670,2 +701,12 @@\n-            return VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_LASTTRUE, Short64Mask.class, short.class, VLENGTH, this,\n-                                                      (m) -> lastTrueHelper(((Short64Mask)m).getBits()));\n+            return (int) VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_LASTTRUE, Short64Mask.class, short.class, VLENGTH, this,\n+                                                      (m) -> lastTrueHelper(m.getBits()));\n+        }\n+\n+        @Override\n+        @ForceInline\n+        public long toLong() {\n+            if (length() > Long.SIZE) {\n+                throw new UnsupportedOperationException(\"too many lanes for one long\");\n+            }\n+            return VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_TOLONG, Short64Mask.class, short.class, VLENGTH, this,\n+                                                      (m) -> toLongHelper(m.getBits()));\n@@ -782,0 +823,8 @@\n+    @ForceInline\n+    @Override\n+    final\n+    ShortVector fromArray0(short[] a, int offset, VectorMask<Short> m) {\n+        return super.fromArray0Template(Short64Mask.class, a, offset, (Short64Mask) m);  \/\/ specialize\n+    }\n+\n+\n@@ -789,0 +838,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    ShortVector fromCharArray0(char[] a, int offset, VectorMask<Short> m) {\n+        return super.fromCharArray0Template(Short64Mask.class, a, offset, (Short64Mask) m);  \/\/ specialize\n+    }\n+\n@@ -797,0 +853,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    ShortVector fromByteArray0(byte[] a, int offset, VectorMask<Short> m) {\n+        return super.fromByteArray0Template(Short64Mask.class, a, offset, (Short64Mask) m);  \/\/ specialize\n+    }\n+\n@@ -804,0 +867,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    ShortVector fromByteBuffer0(ByteBuffer bb, int offset, VectorMask<Short> m) {\n+        return super.fromByteBuffer0Template(Short64Mask.class, bb, offset, (Short64Mask) m);  \/\/ specialize\n+    }\n+\n@@ -811,0 +881,9 @@\n+    @ForceInline\n+    @Override\n+    final\n+    void intoArray0(short[] a, int offset, VectorMask<Short> m) {\n+        super.intoArray0Template(Short64Mask.class, a, offset, (Short64Mask) m);\n+    }\n+\n+\n+\n@@ -818,0 +897,21 @@\n+    @ForceInline\n+    @Override\n+    final\n+    void intoByteArray0(byte[] a, int offset, VectorMask<Short> m) {\n+        super.intoByteArray0Template(Short64Mask.class, a, offset, (Short64Mask) m);  \/\/ specialize\n+    }\n+\n+    @ForceInline\n+    @Override\n+    final\n+    void intoByteBuffer0(ByteBuffer bb, int offset, VectorMask<Short> m) {\n+        super.intoByteBuffer0Template(Short64Mask.class, bb, offset, (Short64Mask) m);\n+    }\n+\n+    @ForceInline\n+    @Override\n+    final\n+    void intoCharArray0(char[] a, int offset, VectorMask<Short> m) {\n+        super.intoCharArray0Template(Short64Mask.class, a, offset, (Short64Mask) m);\n+    }\n+\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/Short64Vector.java","additions":130,"deletions":30,"binary":false,"changes":160,"status":"modified"},{"patch":"@@ -239,2 +239,2 @@\n-    short rOp(short v, FBinOp f) {\n-        return super.rOpTemplate(v, f);  \/\/ specialize\n+    short rOp(short v, VectorMask<Short> m, FBinOp f) {\n+        return super.rOpTemplate(v, m, f);  \/\/ specialize\n@@ -276,0 +276,6 @@\n+    @Override\n+    @ForceInline\n+    public ShortMaxVector lanewise(Unary op, VectorMask<Short> m) {\n+        return (ShortMaxVector) super.lanewiseTemplate(op, ShortMaxMask.class, (ShortMaxMask) m);  \/\/ specialize\n+    }\n+\n@@ -282,0 +288,6 @@\n+    @Override\n+    @ForceInline\n+    public ShortMaxVector lanewise(Binary op, Vector<Short> v, VectorMask<Short> m) {\n+        return (ShortMaxVector) super.lanewiseTemplate(op, ShortMaxMask.class, v, (ShortMaxMask) m);  \/\/ specialize\n+    }\n+\n@@ -289,0 +301,7 @@\n+    \/*package-private*\/\n+    @Override\n+    @ForceInline ShortMaxVector\n+    lanewiseShift(VectorOperators.Binary op, int e, VectorMask<Short> m) {\n+        return (ShortMaxVector) super.lanewiseShiftTemplate(op, ShortMaxMask.class, e, (ShortMaxMask) m);  \/\/ specialize\n+    }\n+\n@@ -294,1 +313,1 @@\n-    lanewise(VectorOperators.Ternary op, Vector<Short> v1, Vector<Short> v2) {\n+    lanewise(Ternary op, Vector<Short> v1, Vector<Short> v2) {\n@@ -298,0 +317,8 @@\n+    @Override\n+    @ForceInline\n+    public final\n+    ShortMaxVector\n+    lanewise(Ternary op, Vector<Short> v1, Vector<Short> v2, VectorMask<Short> m) {\n+        return (ShortMaxVector) super.lanewiseTemplate(op, ShortMaxMask.class, v1, v2, (ShortMaxMask) m);  \/\/ specialize\n+    }\n+\n@@ -317,1 +344,1 @@\n-        return super.reduceLanesTemplate(op, m);  \/\/ specialized\n+        return super.reduceLanesTemplate(op, ShortMaxMask.class, (ShortMaxMask) m);  \/\/ specialized\n@@ -330,1 +357,1 @@\n-        return (long) super.reduceLanesTemplate(op, m);  \/\/ specialized\n+        return (long) super.reduceLanesTemplate(op, ShortMaxMask.class, (ShortMaxMask) m);  \/\/ specialized\n@@ -366,0 +393,7 @@\n+    @Override\n+    @ForceInline\n+    public final ShortMaxMask compare(Comparison op, Vector<Short> v, VectorMask<Short> m) {\n+        return super.compareTemplate(ShortMaxMask.class, op, v, (ShortMaxMask) m);\n+    }\n+\n+\n@@ -422,0 +456,1 @@\n+                                    ShortMaxMask.class,\n@@ -585,10 +620,6 @@\n-            if (VSIZE == species.vectorBitSize()) {\n-                Class<?> dtype = species.elementType();\n-                Class<?> dmtype = species.maskType();\n-                return VectorSupport.convert(VectorSupport.VECTOR_OP_REINTERPRET,\n-                    this.getClass(), ETYPE, VLENGTH,\n-                    dmtype, dtype, VLENGTH,\n-                    this, species,\n-                    ShortMaxMask::defaultMaskCast);\n-            }\n-            return this.defaultMaskCast(species);\n+\n+            return VectorSupport.convert(VectorSupport.VECTOR_OP_CAST,\n+                this.getClass(), ETYPE, VLENGTH,\n+                species.maskType(), species.elementType(), VLENGTH,\n+                this, species,\n+                (m, s) -> s.maskFactory(m.toArray()).check(s));\n@@ -620,3 +651,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_AND, ShortMaxMask.class, short.class, VLENGTH,\n-                                             this, m,\n-                                             (m1, m2) -> m1.bOp(m2, (i, a, b) -> a & b));\n+            return VectorSupport.binaryOp(VECTOR_OP_AND, ShortMaxMask.class, null, short.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a & b));\n@@ -630,3 +661,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_OR, ShortMaxMask.class, short.class, VLENGTH,\n-                                             this, m,\n-                                             (m1, m2) -> m1.bOp(m2, (i, a, b) -> a | b));\n+            return VectorSupport.binaryOp(VECTOR_OP_OR, ShortMaxMask.class, null, short.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a | b));\n@@ -640,3 +671,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_XOR, ShortMaxMask.class, short.class, VLENGTH,\n-                                          this, m,\n-                                          (m1, m2) -> m1.bOp(m2, (i, a, b) -> a ^ b));\n+            return VectorSupport.binaryOp(VECTOR_OP_XOR, ShortMaxMask.class, null, short.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a ^ b));\n@@ -650,2 +681,2 @@\n-            return VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_TRUECOUNT, ShortMaxMask.class, short.class, VLENGTH, this,\n-                                                      (m) -> trueCountHelper(((ShortMaxMask)m).getBits()));\n+            return (int) VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_TRUECOUNT, ShortMaxMask.class, short.class, VLENGTH, this,\n+                                                      (m) -> trueCountHelper(m.getBits()));\n@@ -657,2 +688,2 @@\n-            return VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_FIRSTTRUE, ShortMaxMask.class, short.class, VLENGTH, this,\n-                                                      (m) -> firstTrueHelper(((ShortMaxMask)m).getBits()));\n+            return (int) VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_FIRSTTRUE, ShortMaxMask.class, short.class, VLENGTH, this,\n+                                                      (m) -> firstTrueHelper(m.getBits()));\n@@ -664,2 +695,12 @@\n-            return VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_LASTTRUE, ShortMaxMask.class, short.class, VLENGTH, this,\n-                                                      (m) -> lastTrueHelper(((ShortMaxMask)m).getBits()));\n+            return (int) VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_LASTTRUE, ShortMaxMask.class, short.class, VLENGTH, this,\n+                                                      (m) -> lastTrueHelper(m.getBits()));\n+        }\n+\n+        @Override\n+        @ForceInline\n+        public long toLong() {\n+            if (length() > Long.SIZE) {\n+                throw new UnsupportedOperationException(\"too many lanes for one long\");\n+            }\n+            return VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_TOLONG, ShortMaxMask.class, short.class, VLENGTH, this,\n+                                                      (m) -> toLongHelper(m.getBits()));\n@@ -776,0 +817,8 @@\n+    @ForceInline\n+    @Override\n+    final\n+    ShortVector fromArray0(short[] a, int offset, VectorMask<Short> m) {\n+        return super.fromArray0Template(ShortMaxMask.class, a, offset, (ShortMaxMask) m);  \/\/ specialize\n+    }\n+\n+\n@@ -783,0 +832,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    ShortVector fromCharArray0(char[] a, int offset, VectorMask<Short> m) {\n+        return super.fromCharArray0Template(ShortMaxMask.class, a, offset, (ShortMaxMask) m);  \/\/ specialize\n+    }\n+\n@@ -791,0 +847,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    ShortVector fromByteArray0(byte[] a, int offset, VectorMask<Short> m) {\n+        return super.fromByteArray0Template(ShortMaxMask.class, a, offset, (ShortMaxMask) m);  \/\/ specialize\n+    }\n+\n@@ -798,0 +861,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    ShortVector fromByteBuffer0(ByteBuffer bb, int offset, VectorMask<Short> m) {\n+        return super.fromByteBuffer0Template(ShortMaxMask.class, bb, offset, (ShortMaxMask) m);  \/\/ specialize\n+    }\n+\n@@ -805,0 +875,9 @@\n+    @ForceInline\n+    @Override\n+    final\n+    void intoArray0(short[] a, int offset, VectorMask<Short> m) {\n+        super.intoArray0Template(ShortMaxMask.class, a, offset, (ShortMaxMask) m);\n+    }\n+\n+\n+\n@@ -812,0 +891,21 @@\n+    @ForceInline\n+    @Override\n+    final\n+    void intoByteArray0(byte[] a, int offset, VectorMask<Short> m) {\n+        super.intoByteArray0Template(ShortMaxMask.class, a, offset, (ShortMaxMask) m);  \/\/ specialize\n+    }\n+\n+    @ForceInline\n+    @Override\n+    final\n+    void intoByteBuffer0(ByteBuffer bb, int offset, VectorMask<Short> m) {\n+        super.intoByteBuffer0Template(ShortMaxMask.class, bb, offset, (ShortMaxMask) m);\n+    }\n+\n+    @ForceInline\n+    @Override\n+    final\n+    void intoCharArray0(char[] a, int offset, VectorMask<Short> m) {\n+        super.intoCharArray0Template(ShortMaxMask.class, a, offset, (ShortMaxMask) m);\n+    }\n+\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/ShortMaxVector.java","additions":130,"deletions":30,"binary":false,"changes":160,"status":"modified"},{"patch":"@@ -32,1 +32,0 @@\n-import java.util.function.BinaryOperator;\n@@ -176,0 +175,3 @@\n+        if (m == null) {\n+            return uOpTemplate(f);\n+        }\n@@ -219,0 +221,3 @@\n+        if (m == null) {\n+            return bOpTemplate(o, f);\n+        }\n@@ -268,0 +273,3 @@\n+        if (m == null) {\n+            return tOpTemplate(o1, o2, f);\n+        }\n@@ -283,1 +291,16 @@\n-    short rOp(short v, FBinOp f);\n+    short rOp(short v, VectorMask<Short> m, FBinOp f);\n+\n+    @ForceInline\n+    final\n+    short rOpTemplate(short v, VectorMask<Short> m, FBinOp f) {\n+        if (m == null) {\n+            return rOpTemplate(v, f);\n+        }\n+        short[] vec = vec();\n+        boolean[] mbits = ((AbstractMask<Short>)m).getBits();\n+        for (int i = 0; i < vec.length; i++) {\n+            v = mbits[i] ? f.apply(i, v, vec[i]) : v;\n+        }\n+        return v;\n+    }\n+\n@@ -552,1 +575,1 @@\n-                return broadcast(-1).lanewiseTemplate(XOR, this);\n+                return broadcast(-1).lanewise(XOR, this);\n@@ -555,1 +578,1 @@\n-                return broadcast(0).lanewiseTemplate(SUB, this);\n+                return broadcast(0).lanewise(SUB, this);\n@@ -560,10 +583,3 @@\n-            opc, getClass(), short.class, length(),\n-            this,\n-            UN_IMPL.find(op, opc, (opc_) -> {\n-              switch (opc_) {\n-                case VECTOR_OP_NEG: return v0 ->\n-                        v0.uOp((i, a) -> (short) -a);\n-                case VECTOR_OP_ABS: return v0 ->\n-                        v0.uOp((i, a) -> (short) Math.abs(a));\n-                default: return null;\n-              }}));\n+            opc, getClass(), null, short.class, length(),\n+            this, null,\n+            UN_IMPL.find(op, opc, ShortVector::unaryOperations));\n@@ -571,3 +587,0 @@\n-    private static final\n-    ImplCache<Unary,UnaryOperator<ShortVector>> UN_IMPL\n-        = new ImplCache<>(Unary.class, ShortVector.class);\n@@ -578,2 +591,2 @@\n-    @ForceInline\n-    public final\n+    @Override\n+    public abstract\n@@ -581,2 +594,36 @@\n-                                  VectorMask<Short> m) {\n-        return blend(lanewise(op), m);\n+                                  VectorMask<Short> m);\n+    @ForceInline\n+    final\n+    ShortVector lanewiseTemplate(VectorOperators.Unary op,\n+                                          Class<? extends VectorMask<Short>> maskClass,\n+                                          VectorMask<Short> m) {\n+        m.check(maskClass, this);\n+        if (opKind(op, VO_SPECIAL)) {\n+            if (op == ZOMO) {\n+                return blend(broadcast(-1), compare(NE, 0, m));\n+            }\n+            if (op == NOT) {\n+                return lanewise(XOR, broadcast(-1), m);\n+            } else if (op == NEG) {\n+                return lanewise(NOT, m).lanewise(ADD, broadcast(1), m);\n+            }\n+        }\n+        int opc = opCode(op);\n+        return VectorSupport.unaryOp(\n+            opc, getClass(), maskClass, short.class, length(),\n+            this, m,\n+            UN_IMPL.find(op, opc, ShortVector::unaryOperations));\n+    }\n+\n+    private static final\n+    ImplCache<Unary, UnaryOperation<ShortVector, VectorMask<Short>>>\n+        UN_IMPL = new ImplCache<>(Unary.class, ShortVector.class);\n+\n+    private static UnaryOperation<ShortVector, VectorMask<Short>> unaryOperations(int opc_) {\n+        switch (opc_) {\n+            case VECTOR_OP_NEG: return (v0, m) ->\n+                    v0.uOp(m, (i, a) -> (short) -a);\n+            case VECTOR_OP_ABS: return (v0, m) ->\n+                    v0.uOp(m, (i, a) -> (short) Math.abs(a));\n+            default: return null;\n+        }\n@@ -602,0 +649,1 @@\n+\n@@ -620,1 +668,1 @@\n-                VectorMask<Short> eqz = that.eq((short)0);\n+                VectorMask<Short> eqz = that.eq((short) 0);\n@@ -626,0 +674,1 @@\n+\n@@ -628,34 +677,3 @@\n-            opc, getClass(), short.class, length(),\n-            this, that,\n-            BIN_IMPL.find(op, opc, (opc_) -> {\n-              switch (opc_) {\n-                case VECTOR_OP_ADD: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (short)(a + b));\n-                case VECTOR_OP_SUB: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (short)(a - b));\n-                case VECTOR_OP_MUL: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (short)(a * b));\n-                case VECTOR_OP_DIV: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (short)(a \/ b));\n-                case VECTOR_OP_MAX: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (short)Math.max(a, b));\n-                case VECTOR_OP_MIN: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (short)Math.min(a, b));\n-                case VECTOR_OP_AND: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (short)(a & b));\n-                case VECTOR_OP_OR: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (short)(a | b));\n-                case VECTOR_OP_XOR: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> (short)(a ^ b));\n-                case VECTOR_OP_LSHIFT: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, n) -> (short)(a << n));\n-                case VECTOR_OP_RSHIFT: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, n) -> (short)(a >> n));\n-                case VECTOR_OP_URSHIFT: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, n) -> (short)((a & LSHR_SETUP_MASK) >>> n));\n-                case VECTOR_OP_LROTATE: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, n) -> rotateLeft(a, (int)n));\n-                case VECTOR_OP_RROTATE: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, n) -> rotateRight(a, (int)n));\n-                default: return null;\n-                }}));\n+            opc, getClass(), null, short.class, length(),\n+            this, that, null,\n+            BIN_IMPL.find(op, opc, ShortVector::binaryOperations));\n@@ -663,3 +681,0 @@\n-    private static final\n-    ImplCache<Binary,BinaryOperator<ShortVector>> BIN_IMPL\n-        = new ImplCache<>(Binary.class, ShortVector.class);\n@@ -671,2 +686,2 @@\n-    @ForceInline\n-    public final\n+    @Override\n+    public abstract\n@@ -675,1 +690,6 @@\n-                                  VectorMask<Short> m) {\n+                                  VectorMask<Short> m);\n+    @ForceInline\n+    final\n+    ShortVector lanewiseTemplate(VectorOperators.Binary op,\n+                                          Class<? extends VectorMask<Short>> maskClass,\n+                                          Vector<Short> v, VectorMask<Short> m) {\n@@ -677,4 +697,27 @@\n-        if (op == DIV) {\n-            VectorMask<Short> eqz = that.eq((short)0);\n-            if (eqz.and(m).anyTrue()) {\n-                throw that.divZeroException();\n+        that.check(this);\n+        m.check(maskClass, this);\n+\n+        if (opKind(op, VO_SPECIAL  | VO_SHIFT)) {\n+            if (op == FIRST_NONZERO) {\n+                \/\/ FIXME: Support this in the JIT.\n+                VectorMask<Short> thisNZ\n+                    = this.viewAsIntegralLanes().compare(NE, (short) 0);\n+                that = that.blend((short) 0, thisNZ.cast(vspecies()));\n+                op = OR_UNCHECKED;\n+            }\n+            if (opKind(op, VO_SHIFT)) {\n+                \/\/ As per shift specification for Java, mask the shift count.\n+                \/\/ This allows the JIT to ignore some ISA details.\n+                that = that.lanewise(AND, SHIFT_MASK);\n+            }\n+            if (op == AND_NOT) {\n+                \/\/ FIXME: Support this in the JIT.\n+                that = that.lanewise(NOT);\n+                op = AND;\n+            } else if (op == DIV) {\n+                VectorMask<Short> eqz = that.eq((short)0);\n+                if (eqz.and(m).anyTrue()) {\n+                    throw that.divZeroException();\n+                }\n+                \/\/ suppress div\/0 exceptions in unset lanes\n+                that = that.lanewise(NOT, eqz);\n@@ -682,3 +725,0 @@\n-            \/\/ suppress div\/0 exceptions in unset lanes\n-            that = that.lanewise(NOT, eqz);\n-            return blend(lanewise(DIV, that), m);\n@@ -686,1 +726,44 @@\n-        return blend(lanewise(op, v), m);\n+\n+        int opc = opCode(op);\n+        return VectorSupport.binaryOp(\n+            opc, getClass(), maskClass, short.class, length(),\n+            this, that, m,\n+            BIN_IMPL.find(op, opc, ShortVector::binaryOperations));\n+    }\n+\n+    private static final\n+    ImplCache<Binary, BinaryOperation<ShortVector, VectorMask<Short>>>\n+        BIN_IMPL = new ImplCache<>(Binary.class, ShortVector.class);\n+\n+    private static BinaryOperation<ShortVector, VectorMask<Short>> binaryOperations(int opc_) {\n+        switch (opc_) {\n+            case VECTOR_OP_ADD: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> (short)(a + b));\n+            case VECTOR_OP_SUB: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> (short)(a - b));\n+            case VECTOR_OP_MUL: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> (short)(a * b));\n+            case VECTOR_OP_DIV: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> (short)(a \/ b));\n+            case VECTOR_OP_MAX: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> (short)Math.max(a, b));\n+            case VECTOR_OP_MIN: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> (short)Math.min(a, b));\n+            case VECTOR_OP_AND: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> (short)(a & b));\n+            case VECTOR_OP_OR: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> (short)(a | b));\n+            case VECTOR_OP_XOR: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> (short)(a ^ b));\n+            case VECTOR_OP_LSHIFT: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, n) -> (short)(a << n));\n+            case VECTOR_OP_RSHIFT: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, n) -> (short)(a >> n));\n+            case VECTOR_OP_URSHIFT: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, n) -> (short)((a & LSHR_SETUP_MASK) >>> n));\n+            case VECTOR_OP_LROTATE: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, n) -> rotateLeft(a, (int)n));\n+            case VECTOR_OP_RROTATE: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, n) -> rotateRight(a, (int)n));\n+            default: return null;\n+        }\n@@ -688,0 +771,1 @@\n+\n@@ -750,1 +834,7 @@\n-        return blend(lanewise(op, e), m);\n+        if (opKind(op, VO_SHIFT) && (short)(int)e == e) {\n+            return lanewiseShift(op, (int) e, m);\n+        }\n+        if (op == AND_NOT) {\n+            op = AND; e = (short) ~e;\n+        }\n+        return lanewise(op, broadcast(e), m);\n@@ -770,2 +860,1 @@\n-            && !(opKind(op, VO_SHIFT) && (int)e1 == e)\n-            ) {\n+            && !(opKind(op, VO_SHIFT) && (int)e1 == e)) {\n@@ -791,1 +880,7 @@\n-        return blend(lanewise(op, e), m);\n+        short e1 = (short) e;\n+        if ((long)e1 != e\n+            \/\/ allow shift ops to clip down their int parameters\n+            && !(opKind(op, VO_SHIFT) && (int)e1 == e)) {\n+            vspecies().checkValue(e);  \/\/ for exception\n+        }\n+        return lanewise(op, e1, m);\n@@ -808,16 +903,24 @@\n-            opc, getClass(), short.class, length(),\n-            this, e,\n-            BIN_INT_IMPL.find(op, opc, (opc_) -> {\n-              switch (opc_) {\n-                case VECTOR_OP_LSHIFT: return (v, n) ->\n-                        v.uOp((i, a) -> (short)(a << n));\n-                case VECTOR_OP_RSHIFT: return (v, n) ->\n-                        v.uOp((i, a) -> (short)(a >> n));\n-                case VECTOR_OP_URSHIFT: return (v, n) ->\n-                        v.uOp((i, a) -> (short)((a & LSHR_SETUP_MASK) >>> n));\n-                case VECTOR_OP_LROTATE: return (v, n) ->\n-                        v.uOp((i, a) -> rotateLeft(a, (int)n));\n-                case VECTOR_OP_RROTATE: return (v, n) ->\n-                        v.uOp((i, a) -> rotateRight(a, (int)n));\n-                default: return null;\n-                }}));\n+            opc, getClass(), null, short.class, length(),\n+            this, e, null,\n+            BIN_INT_IMPL.find(op, opc, ShortVector::broadcastIntOperations));\n+    }\n+\n+    \/*package-private*\/\n+    abstract ShortVector\n+    lanewiseShift(VectorOperators.Binary op, int e, VectorMask<Short> m);\n+\n+    \/*package-private*\/\n+    @ForceInline\n+    final ShortVector\n+    lanewiseShiftTemplate(VectorOperators.Binary op,\n+                          Class<? extends VectorMask<Short>> maskClass,\n+                          int e, VectorMask<Short> m) {\n+        m.check(maskClass, this);\n+        assert(opKind(op, VO_SHIFT));\n+        \/\/ As per shift specification for Java, mask the shift count.\n+        e &= SHIFT_MASK;\n+        int opc = opCode(op);\n+        return VectorSupport.broadcastInt(\n+            opc, getClass(), maskClass, short.class, length(),\n+            this, e, m,\n+            BIN_INT_IMPL.find(op, opc, ShortVector::broadcastIntOperations));\n@@ -825,0 +928,1 @@\n+\n@@ -826,1 +930,1 @@\n-    ImplCache<Binary,VectorBroadcastIntOp<ShortVector>> BIN_INT_IMPL\n+    ImplCache<Binary,VectorBroadcastIntOp<ShortVector, VectorMask<Short>>> BIN_INT_IMPL\n@@ -829,0 +933,16 @@\n+    private static VectorBroadcastIntOp<ShortVector, VectorMask<Short>> broadcastIntOperations(int opc_) {\n+        switch (opc_) {\n+            case VECTOR_OP_LSHIFT: return (v, n, m) ->\n+                    v.uOp(m, (i, a) -> (short)(a << n));\n+            case VECTOR_OP_RSHIFT: return (v, n, m) ->\n+                    v.uOp(m, (i, a) -> (short)(a >> n));\n+            case VECTOR_OP_URSHIFT: return (v, n, m) ->\n+                    v.uOp(m, (i, a) -> (short)((a & LSHR_SETUP_MASK) >>> n));\n+            case VECTOR_OP_LROTATE: return (v, n, m) ->\n+                    v.uOp(m, (i, a) -> rotateLeft(a, (int)n));\n+            case VECTOR_OP_RROTATE: return (v, n, m) ->\n+                    v.uOp(m, (i, a) -> rotateRight(a, (int)n));\n+            default: return null;\n+        }\n+    }\n+\n@@ -881,6 +1001,3 @@\n-            opc, getClass(), short.class, length(),\n-            this, that, tother,\n-            TERN_IMPL.find(op, opc, (opc_) -> {\n-              switch (opc_) {\n-                default: return null;\n-                }}));\n+            opc, getClass(), null, short.class, length(),\n+            this, that, tother, null,\n+            TERN_IMPL.find(op, opc, ShortVector::ternaryOperations));\n@@ -888,3 +1005,0 @@\n-    private static final\n-    ImplCache<Ternary,TernaryOperation<ShortVector>> TERN_IMPL\n-        = new ImplCache<>(Ternary.class, ShortVector.class);\n@@ -898,2 +1012,2 @@\n-    @ForceInline\n-    public final\n+    @Override\n+    public abstract\n@@ -903,2 +1017,37 @@\n-                                  VectorMask<Short> m) {\n-        return blend(lanewise(op, v1, v2), m);\n+                                  VectorMask<Short> m);\n+    @ForceInline\n+    final\n+    ShortVector lanewiseTemplate(VectorOperators.Ternary op,\n+                                          Class<? extends VectorMask<Short>> maskClass,\n+                                          Vector<Short> v1,\n+                                          Vector<Short> v2,\n+                                          VectorMask<Short> m) {\n+        ShortVector that = (ShortVector) v1;\n+        ShortVector tother = (ShortVector) v2;\n+        \/\/ It's a word: https:\/\/www.dictionary.com\/browse\/tother\n+        \/\/ See also Chapter 11 of Dickens, Our Mutual Friend:\n+        \/\/ \"Totherest Governor,\" replied Mr Riderhood...\n+        that.check(this);\n+        tother.check(this);\n+        m.check(maskClass, this);\n+\n+        if (op == BITWISE_BLEND) {\n+            \/\/ FIXME: Support this in the JIT.\n+            that = this.lanewise(XOR, that).lanewise(AND, tother);\n+            return this.lanewise(XOR, that, m);\n+        }\n+        int opc = opCode(op);\n+        return VectorSupport.ternaryOp(\n+            opc, getClass(), maskClass, short.class, length(),\n+            this, that, tother, m,\n+            TERN_IMPL.find(op, opc, ShortVector::ternaryOperations));\n+    }\n+\n+    private static final\n+    ImplCache<Ternary, TernaryOperation<ShortVector, VectorMask<Short>>>\n+        TERN_IMPL = new ImplCache<>(Ternary.class, ShortVector.class);\n+\n+    private static TernaryOperation<ShortVector, VectorMask<Short>> ternaryOperations(int opc_) {\n+        switch (opc_) {\n+            default: return null;\n+        }\n@@ -961,1 +1110,1 @@\n-        return blend(lanewise(op, e1, e2), m);\n+        return lanewise(op, broadcast(e1), broadcast(e2), m);\n@@ -1019,1 +1168,1 @@\n-        return blend(lanewise(op, v1, e2), m);\n+        return lanewise(op, v1, broadcast(e2), m);\n@@ -1076,1 +1225,1 @@\n-        return blend(lanewise(op, e1, v2), m);\n+        return lanewise(op, broadcast(e1), v2, m);\n@@ -1748,2 +1897,0 @@\n-        Objects.requireNonNull(v);\n-        ShortSpecies vsp = vspecies();\n@@ -1755,2 +1902,2 @@\n-            this, that,\n-            (cond, v0, v1) -> {\n+            this, that, null,\n+            (cond, v0, v1, m1) -> {\n@@ -1766,0 +1913,22 @@\n+    \/*package-private*\/\n+    @ForceInline\n+    final\n+    <M extends VectorMask<Short>>\n+    M compareTemplate(Class<M> maskType, Comparison op, Vector<Short> v, M m) {\n+        ShortVector that = (ShortVector) v;\n+        that.check(this);\n+        m.check(maskType, this);\n+        int opc = opCode(op);\n+        return VectorSupport.compare(\n+            opc, getClass(), maskType, short.class, length(),\n+            this, that, m,\n+            (cond, v0, v1, m1) -> {\n+                AbstractMask<Short> cmpM\n+                    = v0.bTest(cond, v1, (cond_, i, a, b)\n+                               -> compareWithOp(cond, a, b));\n+                @SuppressWarnings(\"unchecked\")\n+                M m2 = (M) cmpM.and(m1);\n+                return m2;\n+            });\n+    }\n+\n@@ -1783,12 +1952,0 @@\n-    \/**\n-     * {@inheritDoc} <!--workaround-->\n-     *\/\n-    @Override\n-    @ForceInline\n-    public final\n-    VectorMask<Short> compare(VectorOperators.Comparison op,\n-                                  Vector<Short> v,\n-                                  VectorMask<Short> m) {\n-        return compare(op, v).and(m);\n-    }\n-\n@@ -1853,1 +2010,1 @@\n-        return compare(op, e).and(m);\n+        return compare(op, broadcast(e), m);\n@@ -2104,3 +2261,3 @@\n-            getClass(), shuffletype, short.class, length(),\n-            this, shuffle,\n-            (v1, s_) -> v1.uOp((i, a) -> {\n+            getClass(), shuffletype, null, short.class, length(),\n+            this, shuffle, null,\n+            (v1, s_, m_) -> v1.uOp((i, a) -> {\n@@ -2123,1 +2280,1 @@\n-    <S extends VectorShuffle<Short>>\n+    <S extends VectorShuffle<Short>, M extends VectorMask<Short>>\n@@ -2125,0 +2282,1 @@\n+                                           Class<M> masktype,\n@@ -2126,9 +2284,3 @@\n-                                           VectorMask<Short> m) {\n-        ShortVector unmasked =\n-            VectorSupport.rearrangeOp(\n-                getClass(), shuffletype, short.class, length(),\n-                this, shuffle,\n-                (v1, s_) -> v1.uOp((i, a) -> {\n-                    int ei = s_.laneSource(i);\n-                    return ei < 0 ? 0 : v1.lane(ei);\n-                }));\n+                                           M m) {\n+\n+        m.check(masktype, this);\n@@ -2140,1 +2292,7 @@\n-        return broadcast((short)0).blend(unmasked, m);\n+        return VectorSupport.rearrangeOp(\n+                   getClass(), shuffletype, masktype, short.class, length(),\n+                   this, shuffle, m,\n+                   (v1, s_, m_) -> v1.uOp((i, a) -> {\n+                        int ei = s_.laneSource(i);\n+                        return ei < 0  || !m_.laneIsSet(i) ? 0 : v1.lane(ei);\n+                   }));\n@@ -2163,3 +2321,3 @@\n-                getClass(), shuffletype, short.class, length(),\n-                this, ws,\n-                (v0, s_) -> v0.uOp((i, a) -> {\n+                getClass(), shuffletype, null, short.class, length(),\n+                this, ws, null,\n+                (v0, s_, m_) -> v0.uOp((i, a) -> {\n@@ -2171,3 +2329,3 @@\n-                getClass(), shuffletype, short.class, length(),\n-                v, ws,\n-                (v1, s_) -> v1.uOp((i, a) -> {\n+                getClass(), shuffletype, null, short.class, length(),\n+                v, ws, null,\n+                (v1, s_, m_) -> v1.uOp((i, a) -> {\n@@ -2436,0 +2594,1 @@\n+                               Class<? extends VectorMask<Short>> maskClass,\n@@ -2437,2 +2596,10 @@\n-        ShortVector v = reduceIdentityVector(op).blend(this, m);\n-        return v.reduceLanesTemplate(op);\n+        m.check(maskClass, this);\n+        if (op == FIRST_NONZERO) {\n+            ShortVector v = reduceIdentityVector(op).blend(this, m);\n+            return v.reduceLanesTemplate(op);\n+        }\n+        int opc = opCode(op);\n+        return fromBits(VectorSupport.reductionCoerced(\n+            opc, getClass(), maskClass, short.class, length(),\n+            this, m,\n+            REDUCE_IMPL.find(op, opc, ShortVector::reductionOperations)));\n@@ -2453,20 +2620,3 @@\n-            opc, getClass(), short.class, length(),\n-            this,\n-            REDUCE_IMPL.find(op, opc, (opc_) -> {\n-              switch (opc_) {\n-              case VECTOR_OP_ADD: return v ->\n-                      toBits(v.rOp((short)0, (i, a, b) -> (short)(a + b)));\n-              case VECTOR_OP_MUL: return v ->\n-                      toBits(v.rOp((short)1, (i, a, b) -> (short)(a * b)));\n-              case VECTOR_OP_MIN: return v ->\n-                      toBits(v.rOp(MAX_OR_INF, (i, a, b) -> (short) Math.min(a, b)));\n-              case VECTOR_OP_MAX: return v ->\n-                      toBits(v.rOp(MIN_OR_INF, (i, a, b) -> (short) Math.max(a, b)));\n-              case VECTOR_OP_AND: return v ->\n-                      toBits(v.rOp((short)-1, (i, a, b) -> (short)(a & b)));\n-              case VECTOR_OP_OR: return v ->\n-                      toBits(v.rOp((short)0, (i, a, b) -> (short)(a | b)));\n-              case VECTOR_OP_XOR: return v ->\n-                      toBits(v.rOp((short)0, (i, a, b) -> (short)(a ^ b)));\n-              default: return null;\n-              }})));\n+            opc, getClass(), null, short.class, length(),\n+            this, null,\n+            REDUCE_IMPL.find(op, opc, ShortVector::reductionOperations)));\n@@ -2474,0 +2624,1 @@\n+\n@@ -2475,2 +2626,22 @@\n-    ImplCache<Associative,Function<ShortVector,Long>> REDUCE_IMPL\n-        = new ImplCache<>(Associative.class, ShortVector.class);\n+    ImplCache<Associative, ReductionOperation<ShortVector, VectorMask<Short>>>\n+        REDUCE_IMPL = new ImplCache<>(Associative.class, ShortVector.class);\n+\n+    private static ReductionOperation<ShortVector, VectorMask<Short>> reductionOperations(int opc_) {\n+        switch (opc_) {\n+            case VECTOR_OP_ADD: return (v, m) ->\n+                    toBits(v.rOp((short)0, m, (i, a, b) -> (short)(a + b)));\n+            case VECTOR_OP_MUL: return (v, m) ->\n+                    toBits(v.rOp((short)1, m, (i, a, b) -> (short)(a * b)));\n+            case VECTOR_OP_MIN: return (v, m) ->\n+                    toBits(v.rOp(MAX_OR_INF, m, (i, a, b) -> (short) Math.min(a, b)));\n+            case VECTOR_OP_MAX: return (v, m) ->\n+                    toBits(v.rOp(MIN_OR_INF, m, (i, a, b) -> (short) Math.max(a, b)));\n+            case VECTOR_OP_AND: return (v, m) ->\n+                    toBits(v.rOp((short)-1, m, (i, a, b) -> (short)(a & b)));\n+            case VECTOR_OP_OR: return (v, m) ->\n+                    toBits(v.rOp((short)0, m, (i, a, b) -> (short)(a | b)));\n+            case VECTOR_OP_XOR: return (v, m) ->\n+                    toBits(v.rOp((short)0, m, (i, a, b) -> (short)(a ^ b)));\n+            default: return null;\n+        }\n+    }\n@@ -2702,3 +2873,1 @@\n-            ShortVector zero = vsp.zero();\n-            ShortVector v = zero.fromByteArray0(a, offset);\n-            return zero.blend(v.maybeSwap(bo), m);\n+            return vsp.dummyVector().fromByteArray0(a, offset, m).maybeSwap(bo);\n@@ -2766,2 +2935,1 @@\n-            ShortVector zero = vsp.zero();\n-            return zero.blend(zero.fromArray0(a, offset), m);\n+            return vsp.dummyVector().fromArray0(a, offset, m);\n@@ -2916,2 +3084,1 @@\n-            ShortVector zero = vsp.zero();\n-            return zero.blend(zero.fromCharArray0(a, offset), m);\n+            return vsp.dummyVector().fromCharArray0(a, offset, m);\n@@ -3102,3 +3269,1 @@\n-            ShortVector zero = vsp.zero();\n-            ShortVector v = zero.fromByteBuffer0(bb, offset);\n-            return zero.blend(v.maybeSwap(bo), m);\n+            return vsp.dummyVector().fromByteBuffer0(bb, offset, m).maybeSwap(bo);\n@@ -3176,1 +3341,0 @@\n-            \/\/ FIXME: optimize\n@@ -3179,1 +3343,1 @@\n-            stOp(a, offset, m, (arr, off, i, v) -> arr[off+i] = v);\n+            intoArray0(a, offset, m);\n@@ -3324,1 +3488,0 @@\n-            \/\/ FIXME: optimize\n@@ -3327,1 +3490,1 @@\n-            stOp(a, offset, m, (arr, off, i, v) -> arr[off+i] = (char) v);\n+            intoCharArray0(a, offset, m);\n@@ -3441,1 +3604,0 @@\n-            \/\/ FIXME: optimize\n@@ -3444,3 +3606,1 @@\n-            ByteBuffer wb = wrapper(a, bo);\n-            this.stOp(wb, offset, m,\n-                    (wb_, o, i, e) -> wb_.putShort(o + i * 2, e));\n+            maybeSwap(bo).intoByteArray0(a, offset, m);\n@@ -3458,1 +3618,1 @@\n-        if (bb.isReadOnly()) {\n+        if (ScopedMemoryAccess.isReadOnly(bb)) {\n@@ -3477,1 +3637,0 @@\n-            \/\/ FIXME: optimize\n@@ -3483,3 +3642,1 @@\n-            ByteBuffer wb = wrapper(bb, bo);\n-            this.stOp(wb, offset, m,\n-                    (wb_, o, i, e) -> wb_.putShort(o + i * 2, e));\n+            maybeSwap(bo).intoByteBuffer0(bb, offset, m);\n@@ -3523,0 +3680,18 @@\n+    \/*package-private*\/\n+    abstract\n+    ShortVector fromArray0(short[] a, int offset, VectorMask<Short> m);\n+    @ForceInline\n+    final\n+    <M extends VectorMask<Short>>\n+    ShortVector fromArray0Template(Class<M> maskClass, short[] a, int offset, M m) {\n+        m.check(species());\n+        ShortSpecies vsp = vspecies();\n+        return VectorSupport.loadMasked(\n+            vsp.vectorType(), maskClass, vsp.elementType(), vsp.laneCount(),\n+            a, arrayAddress(a, offset), m,\n+            a, offset, vsp,\n+            (arr, off, s, vm) -> s.ldOp(arr, off, vm,\n+                                        (arr_, off_, i) -> arr_[off_ + i]));\n+    }\n+\n+\n@@ -3538,0 +3713,17 @@\n+    \/*package-private*\/\n+    abstract\n+    ShortVector fromCharArray0(char[] a, int offset, VectorMask<Short> m);\n+    @ForceInline\n+    final\n+    <M extends VectorMask<Short>>\n+    ShortVector fromCharArray0Template(Class<M> maskClass, char[] a, int offset, M m) {\n+        m.check(species());\n+        ShortSpecies vsp = vspecies();\n+        return VectorSupport.loadMasked(\n+                vsp.vectorType(), maskClass, vsp.elementType(), vsp.laneCount(),\n+                a, charArrayAddress(a, offset), m,\n+                a, offset, vsp,\n+                (arr, off, s, vm) -> s.ldOp(arr, off, vm,\n+                                            (arr_, off_, i) -> (short) arr_[off_ + i]));\n+    }\n+\n@@ -3557,0 +3749,19 @@\n+    abstract\n+    ShortVector fromByteArray0(byte[] a, int offset, VectorMask<Short> m);\n+    @ForceInline\n+    final\n+    <M extends VectorMask<Short>>\n+    ShortVector fromByteArray0Template(Class<M> maskClass, byte[] a, int offset, M m) {\n+        ShortSpecies vsp = vspecies();\n+        m.check(vsp);\n+        return VectorSupport.loadMasked(\n+            vsp.vectorType(), maskClass, vsp.elementType(), vsp.laneCount(),\n+            a, byteArrayAddress(a, offset), m,\n+            a, offset, vsp,\n+            (arr, off, s, vm) -> {\n+                ByteBuffer wb = wrapper(arr, NATIVE_ENDIAN);\n+                return s.ldOp(wb, off, vm,\n+                        (wb_, o, i) -> wb_.getShort(o + i * 2));\n+            });\n+    }\n+\n@@ -3573,0 +3784,18 @@\n+    abstract\n+    ShortVector fromByteBuffer0(ByteBuffer bb, int offset, VectorMask<Short> m);\n+    @ForceInline\n+    final\n+    <M extends VectorMask<Short>>\n+    ShortVector fromByteBuffer0Template(Class<M> maskClass, ByteBuffer bb, int offset, M m) {\n+        ShortSpecies vsp = vspecies();\n+        m.check(vsp);\n+        return ScopedMemoryAccess.loadFromByteBufferMasked(\n+                vsp.vectorType(), maskClass, vsp.elementType(), vsp.laneCount(),\n+                bb, offset, m, vsp,\n+                (buf, off, s, vm) -> {\n+                    ByteBuffer wb = wrapper(buf, NATIVE_ENDIAN);\n+                    return s.ldOp(wb, off, vm,\n+                            (wb_, o, i) -> wb_.getShort(o + i * 2));\n+                });\n+    }\n+\n@@ -3592,0 +3821,19 @@\n+    abstract\n+    void intoArray0(short[] a, int offset, VectorMask<Short> m);\n+    @ForceInline\n+    final\n+    <M extends VectorMask<Short>>\n+    void intoArray0Template(Class<M> maskClass, short[] a, int offset, M m) {\n+        m.check(species());\n+        ShortSpecies vsp = vspecies();\n+        VectorSupport.storeMasked(\n+            vsp.vectorType(), maskClass, vsp.elementType(), vsp.laneCount(),\n+            a, arrayAddress(a, offset),\n+            this, m, a, offset,\n+            (arr, off, v, vm)\n+            -> v.stOp(arr, off, vm,\n+                      (arr_, off_, i, e) -> arr_[off_ + i] = e));\n+    }\n+\n+\n+\n@@ -3609,0 +3857,19 @@\n+    abstract\n+    void intoByteArray0(byte[] a, int offset, VectorMask<Short> m);\n+    @ForceInline\n+    final\n+    <M extends VectorMask<Short>>\n+    void intoByteArray0Template(Class<M> maskClass, byte[] a, int offset, M m) {\n+        ShortSpecies vsp = vspecies();\n+        m.check(vsp);\n+        VectorSupport.storeMasked(\n+            vsp.vectorType(), maskClass, vsp.elementType(), vsp.laneCount(),\n+            a, byteArrayAddress(a, offset),\n+            this, m, a, offset,\n+            (arr, off, v, vm) -> {\n+                ByteBuffer wb = wrapper(arr, NATIVE_ENDIAN);\n+                v.stOp(wb, off, vm,\n+                        (tb_, o, i, e) -> tb_.putShort(o + i * 2, e));\n+            });\n+    }\n+\n@@ -3623,0 +3890,36 @@\n+    abstract\n+    void intoByteBuffer0(ByteBuffer bb, int offset, VectorMask<Short> m);\n+    @ForceInline\n+    final\n+    <M extends VectorMask<Short>>\n+    void intoByteBuffer0Template(Class<M> maskClass, ByteBuffer bb, int offset, M m) {\n+        ShortSpecies vsp = vspecies();\n+        m.check(vsp);\n+        ScopedMemoryAccess.storeIntoByteBufferMasked(\n+                vsp.vectorType(), maskClass, vsp.elementType(), vsp.laneCount(),\n+                this, m, bb, offset,\n+                (buf, off, v, vm) -> {\n+                    ByteBuffer wb = wrapper(buf, NATIVE_ENDIAN);\n+                    v.stOp(wb, off, vm,\n+                            (wb_, o, i, e) -> wb_.putShort(o + i * 2, e));\n+                });\n+    }\n+\n+    \/*package-private*\/\n+    abstract\n+    void intoCharArray0(char[] a, int offset, VectorMask<Short> m);\n+    @ForceInline\n+    final\n+    <M extends VectorMask<Short>>\n+    void intoCharArray0Template(Class<M> maskClass, char[] a, int offset, M m) {\n+        m.check(species());\n+        ShortSpecies vsp = vspecies();\n+        VectorSupport.storeMasked(\n+            vsp.vectorType(), maskClass, vsp.elementType(), vsp.laneCount(),\n+            a, charArrayAddress(a, offset),\n+            this, m, a, offset,\n+            (arr, off, v, vm)\n+            -> v.stOp(arr, off, vm,\n+                      (arr_, off_, i, e) -> arr_[off_ + i] = (char) e));\n+    }\n+\n@@ -3957,1 +4260,1 @@\n-                                      AbstractMask<Short> m,\n+                                      VectorMask<Short> m,\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/ShortVector.java","additions":495,"deletions":192,"binary":false,"changes":687,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2017, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2017, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -519,0 +519,2 @@\n+     * @throws IndexOutOfBoundsException if the index is out of range\n+     * ({@code < 0 || >= length()})\n@@ -556,0 +558,18 @@\n+    \/**\n+     * Checks that this mask has the same class with the given mask class,\n+     * and it has the same species with given vector's species,\n+     * and returns this mask unchanged.\n+     * The effect is similar to this pseudocode:\n+     * {@code getClass() == maskClass &&\n+     *        vectorSpecies() == vector.species()\n+     *        ? this\n+     *        : throw new ClassCastException()}.\n+     *\n+     * @param maskClass the class required for this mask\n+     * @param vector its species required for this mask\n+     * @param <F> the boxed element type of the required species\n+     * @return the same mask\n+     * @throws ClassCastException if the species is wrong\n+     *\/\n+    abstract <F> VectorMask<F> check(Class<? extends VectorMask<F>> maskClass, Vector<F> vector);\n+\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/VectorMask.java","additions":21,"deletions":1,"binary":false,"changes":22,"status":"modified"},{"patch":"@@ -32,1 +32,0 @@\n-import java.util.function.BinaryOperator;\n@@ -180,0 +179,3 @@\n+        if (m == null) {\n+            return uOpTemplate(f);\n+        }\n@@ -223,0 +225,3 @@\n+        if (m == null) {\n+            return bOpTemplate(o, f);\n+        }\n@@ -272,0 +277,3 @@\n+        if (m == null) {\n+            return tOpTemplate(o1, o2, f);\n+        }\n@@ -287,1 +295,16 @@\n-    $type$ rOp($type$ v, FBinOp f);\n+    $type$ rOp($type$ v, VectorMask<$Boxtype$> m, FBinOp f);\n+\n+    @ForceInline\n+    final\n+    $type$ rOpTemplate($type$ v, VectorMask<$Boxtype$> m, FBinOp f) {\n+        if (m == null) {\n+            return rOpTemplate(v, f);\n+        }\n+        $type$[] vec = vec();\n+        boolean[] mbits = ((AbstractMask<$Boxtype$>)m).getBits();\n+        for (int i = 0; i < vec.length; i++) {\n+            v = mbits[i] ? f.apply(i, v, vec[i]) : v;\n+        }\n+        return v;\n+    }\n+\n@@ -575,1 +598,1 @@\n-                return broadcast(-1).lanewiseTemplate(XOR, this);\n+                return broadcast(-1).lanewise(XOR, this);\n@@ -578,1 +601,1 @@\n-                return broadcast(0).lanewiseTemplate(SUB, this);\n+                return broadcast(0).lanewise(SUB, this);\n@@ -584,44 +607,3 @@\n-            opc, getClass(), $type$.class, length(),\n-            this,\n-            UN_IMPL.find(op, opc, (opc_) -> {\n-              switch (opc_) {\n-                case VECTOR_OP_NEG: return v0 ->\n-                        v0.uOp((i, a) -> ($type$) -a);\n-                case VECTOR_OP_ABS: return v0 ->\n-                        v0.uOp((i, a) -> ($type$) Math.abs(a));\n-#if[FP]\n-                case VECTOR_OP_SIN: return v0 ->\n-                        v0.uOp((i, a) -> ($type$) Math.sin(a));\n-                case VECTOR_OP_COS: return v0 ->\n-                        v0.uOp((i, a) -> ($type$) Math.cos(a));\n-                case VECTOR_OP_TAN: return v0 ->\n-                        v0.uOp((i, a) -> ($type$) Math.tan(a));\n-                case VECTOR_OP_ASIN: return v0 ->\n-                        v0.uOp((i, a) -> ($type$) Math.asin(a));\n-                case VECTOR_OP_ACOS: return v0 ->\n-                        v0.uOp((i, a) -> ($type$) Math.acos(a));\n-                case VECTOR_OP_ATAN: return v0 ->\n-                        v0.uOp((i, a) -> ($type$) Math.atan(a));\n-                case VECTOR_OP_EXP: return v0 ->\n-                        v0.uOp((i, a) -> ($type$) Math.exp(a));\n-                case VECTOR_OP_LOG: return v0 ->\n-                        v0.uOp((i, a) -> ($type$) Math.log(a));\n-                case VECTOR_OP_LOG10: return v0 ->\n-                        v0.uOp((i, a) -> ($type$) Math.log10(a));\n-                case VECTOR_OP_SQRT: return v0 ->\n-                        v0.uOp((i, a) -> ($type$) Math.sqrt(a));\n-                case VECTOR_OP_CBRT: return v0 ->\n-                        v0.uOp((i, a) -> ($type$) Math.cbrt(a));\n-                case VECTOR_OP_SINH: return v0 ->\n-                        v0.uOp((i, a) -> ($type$) Math.sinh(a));\n-                case VECTOR_OP_COSH: return v0 ->\n-                        v0.uOp((i, a) -> ($type$) Math.cosh(a));\n-                case VECTOR_OP_TANH: return v0 ->\n-                        v0.uOp((i, a) -> ($type$) Math.tanh(a));\n-                case VECTOR_OP_EXPM1: return v0 ->\n-                        v0.uOp((i, a) -> ($type$) Math.expm1(a));\n-                case VECTOR_OP_LOG1P: return v0 ->\n-                        v0.uOp((i, a) -> ($type$) Math.log1p(a));\n-#end[FP]\n-                default: return null;\n-              }}));\n+            opc, getClass(), null, $type$.class, length(),\n+            this, null,\n+            UN_IMPL.find(op, opc, $abstractvectortype$::unaryOperations));\n@@ -629,3 +611,0 @@\n-    private static final\n-    ImplCache<Unary,UnaryOperator<$abstractvectortype$>> UN_IMPL\n-        = new ImplCache<>(Unary.class, $Type$Vector.class);\n@@ -636,2 +615,2 @@\n-    @ForceInline\n-    public final\n+    @Override\n+    public abstract\n@@ -639,2 +618,72 @@\n-                                  VectorMask<$Boxtype$> m) {\n-        return blend(lanewise(op), m);\n+                                  VectorMask<$Boxtype$> m);\n+    @ForceInline\n+    final\n+    $abstractvectortype$ lanewiseTemplate(VectorOperators.Unary op,\n+                                          Class<? extends VectorMask<$Boxtype$>> maskClass,\n+                                          VectorMask<$Boxtype$> m) {\n+        m.check(maskClass, this);\n+        if (opKind(op, VO_SPECIAL)) {\n+            if (op == ZOMO) {\n+                return blend(broadcast(-1), compare(NE, 0, m));\n+            }\n+#if[BITWISE]\n+            if (op == NOT) {\n+                return lanewise(XOR, broadcast(-1), m);\n+            } else if (op == NEG) {\n+                return lanewise(NOT, m).lanewise(ADD, broadcast(1), m);\n+            }\n+#end[BITWISE]\n+        }\n+        int opc = opCode(op);\n+        return VectorSupport.unaryOp(\n+            opc, getClass(), maskClass, $type$.class, length(),\n+            this, m,\n+            UN_IMPL.find(op, opc, $abstractvectortype$::unaryOperations));\n+    }\n+\n+    private static final\n+    ImplCache<Unary, UnaryOperation<$abstractvectortype$, VectorMask<$Boxtype$>>>\n+        UN_IMPL = new ImplCache<>(Unary.class, $Type$Vector.class);\n+\n+    private static UnaryOperation<$abstractvectortype$, VectorMask<$Boxtype$>> unaryOperations(int opc_) {\n+        switch (opc_) {\n+            case VECTOR_OP_NEG: return (v0, m) ->\n+                    v0.uOp(m, (i, a) -> ($type$) -a);\n+            case VECTOR_OP_ABS: return (v0, m) ->\n+                    v0.uOp(m, (i, a) -> ($type$) Math.abs(a));\n+#if[FP]\n+            case VECTOR_OP_SIN: return (v0, m) ->\n+                    v0.uOp(m, (i, a) -> ($type$) Math.sin(a));\n+            case VECTOR_OP_COS: return (v0, m) ->\n+                    v0.uOp(m, (i, a) -> ($type$) Math.cos(a));\n+            case VECTOR_OP_TAN: return (v0, m) ->\n+                    v0.uOp(m, (i, a) -> ($type$) Math.tan(a));\n+            case VECTOR_OP_ASIN: return (v0, m) ->\n+                    v0.uOp(m, (i, a) -> ($type$) Math.asin(a));\n+            case VECTOR_OP_ACOS: return (v0, m) ->\n+                    v0.uOp(m, (i, a) -> ($type$) Math.acos(a));\n+            case VECTOR_OP_ATAN: return (v0, m) ->\n+                    v0.uOp(m, (i, a) -> ($type$) Math.atan(a));\n+            case VECTOR_OP_EXP: return (v0, m) ->\n+                    v0.uOp(m, (i, a) -> ($type$) Math.exp(a));\n+            case VECTOR_OP_LOG: return (v0, m) ->\n+                    v0.uOp(m, (i, a) -> ($type$) Math.log(a));\n+            case VECTOR_OP_LOG10: return (v0, m) ->\n+                    v0.uOp(m, (i, a) -> ($type$) Math.log10(a));\n+            case VECTOR_OP_SQRT: return (v0, m) ->\n+                    v0.uOp(m, (i, a) -> ($type$) Math.sqrt(a));\n+            case VECTOR_OP_CBRT: return (v0, m) ->\n+                    v0.uOp(m, (i, a) -> ($type$) Math.cbrt(a));\n+            case VECTOR_OP_SINH: return (v0, m) ->\n+                    v0.uOp(m, (i, a) -> ($type$) Math.sinh(a));\n+            case VECTOR_OP_COSH: return (v0, m) ->\n+                    v0.uOp(m, (i, a) -> ($type$) Math.cosh(a));\n+            case VECTOR_OP_TANH: return (v0, m) ->\n+                    v0.uOp(m, (i, a) -> ($type$) Math.tanh(a));\n+            case VECTOR_OP_EXPM1: return (v0, m) ->\n+                    v0.uOp(m, (i, a) -> ($type$) Math.expm1(a));\n+            case VECTOR_OP_LOG1P: return (v0, m) ->\n+                    v0.uOp(m, (i, a) -> ($type$) Math.log1p(a));\n+#end[FP]\n+            default: return null;\n+        }\n@@ -660,0 +709,1 @@\n+\n@@ -687,1 +737,1 @@\n-                VectorMask<$Boxtype$> eqz = that.eq(($type$)0);\n+                VectorMask<$Boxtype$> eqz = that.eq(($type$) 0);\n@@ -694,0 +744,1 @@\n+\n@@ -696,44 +747,3 @@\n-            opc, getClass(), $type$.class, length(),\n-            this, that,\n-            BIN_IMPL.find(op, opc, (opc_) -> {\n-              switch (opc_) {\n-                case VECTOR_OP_ADD: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> ($type$)(a + b));\n-                case VECTOR_OP_SUB: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> ($type$)(a - b));\n-                case VECTOR_OP_MUL: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> ($type$)(a * b));\n-                case VECTOR_OP_DIV: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> ($type$)(a \/ b));\n-                case VECTOR_OP_MAX: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> ($type$)Math.max(a, b));\n-                case VECTOR_OP_MIN: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> ($type$)Math.min(a, b));\n-#if[BITWISE]\n-                case VECTOR_OP_AND: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> ($type$)(a & b));\n-                case VECTOR_OP_OR: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> ($type$)(a | b));\n-                case VECTOR_OP_XOR: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> ($type$)(a ^ b));\n-                case VECTOR_OP_LSHIFT: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, n) -> ($type$)(a << n));\n-                case VECTOR_OP_RSHIFT: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, n) -> ($type$)(a >> n));\n-                case VECTOR_OP_URSHIFT: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, n) -> ($type$)((a & LSHR_SETUP_MASK) >>> n));\n-                case VECTOR_OP_LROTATE: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, n) -> rotateLeft(a, (int)n));\n-                case VECTOR_OP_RROTATE: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, n) -> rotateRight(a, (int)n));\n-#end[BITWISE]\n-#if[FP]\n-                case VECTOR_OP_ATAN2: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> ($type$) Math.atan2(a, b));\n-                case VECTOR_OP_POW: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> ($type$) Math.pow(a, b));\n-                case VECTOR_OP_HYPOT: return (v0, v1) ->\n-                        v0.bOp(v1, (i, a, b) -> ($type$) Math.hypot(a, b));\n-#end[FP]\n-                default: return null;\n-                }}));\n+            opc, getClass(), null, $type$.class, length(),\n+            this, that, null,\n+            BIN_IMPL.find(op, opc, $abstractvectortype$::binaryOperations));\n@@ -741,3 +751,0 @@\n-    private static final\n-    ImplCache<Binary,BinaryOperator<$abstractvectortype$>> BIN_IMPL\n-        = new ImplCache<>(Binary.class, $Type$Vector.class);\n@@ -749,2 +756,2 @@\n-    @ForceInline\n-    public final\n+    @Override\n+    public abstract\n@@ -753,2 +760,6 @@\n-                                  VectorMask<$Boxtype$> m) {\n-#if[BITWISE]\n+                                  VectorMask<$Boxtype$> m);\n+    @ForceInline\n+    final\n+    $abstractvectortype$ lanewiseTemplate(VectorOperators.Binary op,\n+                                          Class<? extends VectorMask<$Boxtype$>> maskClass,\n+                                          Vector<$Boxtype$> v, VectorMask<$Boxtype$> m) {\n@@ -756,4 +767,34 @@\n-        if (op == DIV) {\n-            VectorMask<$Boxtype$> eqz = that.eq(($type$)0);\n-            if (eqz.and(m).anyTrue()) {\n-                throw that.divZeroException();\n+        that.check(this);\n+        m.check(maskClass, this);\n+\n+        if (opKind(op, VO_SPECIAL {#if[!FP]? | VO_SHIFT})) {\n+            if (op == FIRST_NONZERO) {\n+#if[FP]\n+                return blend(lanewise(op, v), m);\n+#else[FP]\n+                \/\/ FIXME: Support this in the JIT.\n+                VectorMask<$Boxbitstype$> thisNZ\n+                    = this.viewAsIntegralLanes().compare(NE, ($bitstype$) 0);\n+                that = that.blend(($type$) 0, thisNZ.cast(vspecies()));\n+                op = OR_UNCHECKED;\n+#end[FP]\n+            }\n+#if[BITWISE]\n+#if[!FP]\n+            if (opKind(op, VO_SHIFT)) {\n+                \/\/ As per shift specification for Java, mask the shift count.\n+                \/\/ This allows the JIT to ignore some ISA details.\n+                that = that.lanewise(AND, SHIFT_MASK);\n+            }\n+#end[!FP]\n+            if (op == AND_NOT) {\n+                \/\/ FIXME: Support this in the JIT.\n+                that = that.lanewise(NOT);\n+                op = AND;\n+            } else if (op == DIV) {\n+                VectorMask<$Boxtype$> eqz = that.eq(($type$)0);\n+                if (eqz.and(m).anyTrue()) {\n+                    throw that.divZeroException();\n+                }\n+                \/\/ suppress div\/0 exceptions in unset lanes\n+                that = that.lanewise(NOT, eqz);\n@@ -761,3 +802,1 @@\n-            \/\/ suppress div\/0 exceptions in unset lanes\n-            that = that.lanewise(NOT, eqz);\n-            return blend(lanewise(DIV, that), m);\n+#end[BITWISE]\n@@ -765,0 +804,43 @@\n+\n+        int opc = opCode(op);\n+        return VectorSupport.binaryOp(\n+            opc, getClass(), maskClass, $type$.class, length(),\n+            this, that, m,\n+            BIN_IMPL.find(op, opc, $abstractvectortype$::binaryOperations));\n+    }\n+\n+    private static final\n+    ImplCache<Binary, BinaryOperation<$abstractvectortype$, VectorMask<$Boxtype$>>>\n+        BIN_IMPL = new ImplCache<>(Binary.class, $Type$Vector.class);\n+\n+    private static BinaryOperation<$abstractvectortype$, VectorMask<$Boxtype$>> binaryOperations(int opc_) {\n+        switch (opc_) {\n+            case VECTOR_OP_ADD: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> ($type$)(a + b));\n+            case VECTOR_OP_SUB: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> ($type$)(a - b));\n+            case VECTOR_OP_MUL: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> ($type$)(a * b));\n+            case VECTOR_OP_DIV: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> ($type$)(a \/ b));\n+            case VECTOR_OP_MAX: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> ($type$)Math.max(a, b));\n+            case VECTOR_OP_MIN: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> ($type$)Math.min(a, b));\n+#if[BITWISE]\n+            case VECTOR_OP_AND: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> ($type$)(a & b));\n+            case VECTOR_OP_OR: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> ($type$)(a | b));\n+            case VECTOR_OP_XOR: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> ($type$)(a ^ b));\n+            case VECTOR_OP_LSHIFT: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, n) -> ($type$)(a << n));\n+            case VECTOR_OP_RSHIFT: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, n) -> ($type$)(a >> n));\n+            case VECTOR_OP_URSHIFT: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, n) -> ($type$)((a & LSHR_SETUP_MASK) >>> n));\n+            case VECTOR_OP_LROTATE: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, n) -> rotateLeft(a, (int)n));\n+            case VECTOR_OP_RROTATE: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, n) -> rotateRight(a, (int)n));\n@@ -766,1 +848,12 @@\n-        return blend(lanewise(op, v), m);\n+#if[FP]\n+            case VECTOR_OP_OR: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> fromBits(toBits(a) | toBits(b)));\n+            case VECTOR_OP_ATAN2: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> ($type$) Math.atan2(a, b));\n+            case VECTOR_OP_POW: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> ($type$) Math.pow(a, b));\n+            case VECTOR_OP_HYPOT: return (v0, v1, vm) ->\n+                    v0.bOp(v1, vm, (i, a, b) -> ($type$) Math.hypot(a, b));\n+#end[FP]\n+            default: return null;\n+        }\n@@ -768,0 +861,1 @@\n+\n@@ -832,1 +926,9 @@\n-        return blend(lanewise(op, e), m);\n+#if[BITWISE]\n+        if (opKind(op, VO_SHIFT) && ($type$)(int)e == e) {\n+            return lanewiseShift(op, (int) e, m);\n+        }\n+        if (op == AND_NOT) {\n+            op = AND; e = ($type$) ~e;\n+        }\n+#end[BITWISE]\n+        return lanewise(op, broadcast(e), m);\n@@ -851,1 +953,0 @@\n-        if ((long)e1 != e\n@@ -853,0 +954,1 @@\n+        if ((long)e1 != e\n@@ -854,1 +956,3 @@\n-            && !(opKind(op, VO_SHIFT) && (int)e1 == e)\n+            && !(opKind(op, VO_SHIFT) && (int)e1 == e)) {\n+#else[BITWISE]\n+        if ((long)e1 != e) {\n@@ -856,1 +960,0 @@\n-            ) {\n@@ -876,1 +979,11 @@\n-        return blend(lanewise(op, e), m);\n+        $type$ e1 = ($type$) e;\n+#if[BITWISE]\n+        if ((long)e1 != e\n+            \/\/ allow shift ops to clip down their int parameters\n+            && !(opKind(op, VO_SHIFT) && (int)e1 == e)) {\n+#else[BITWISE]\n+        if ((long)e1 != e) {\n+#end[BITWISE]\n+            vspecies().checkValue(e);  \/\/ for exception\n+        }\n+        return lanewise(op, e1, m);\n@@ -895,16 +1008,24 @@\n-            opc, getClass(), $type$.class, length(),\n-            this, e,\n-            BIN_INT_IMPL.find(op, opc, (opc_) -> {\n-              switch (opc_) {\n-                case VECTOR_OP_LSHIFT: return (v, n) ->\n-                        v.uOp((i, a) -> ($type$)(a << n));\n-                case VECTOR_OP_RSHIFT: return (v, n) ->\n-                        v.uOp((i, a) -> ($type$)(a >> n));\n-                case VECTOR_OP_URSHIFT: return (v, n) ->\n-                        v.uOp((i, a) -> ($type$)((a & LSHR_SETUP_MASK) >>> n));\n-                case VECTOR_OP_LROTATE: return (v, n) ->\n-                        v.uOp((i, a) -> rotateLeft(a, (int)n));\n-                case VECTOR_OP_RROTATE: return (v, n) ->\n-                        v.uOp((i, a) -> rotateRight(a, (int)n));\n-                default: return null;\n-                }}));\n+            opc, getClass(), null, $type$.class, length(),\n+            this, e, null,\n+            BIN_INT_IMPL.find(op, opc, $abstractvectortype$::broadcastIntOperations));\n+    }\n+\n+    \/*package-private*\/\n+    abstract $abstractvectortype$\n+    lanewiseShift(VectorOperators.Binary op, int e, VectorMask<$Boxtype$> m);\n+\n+    \/*package-private*\/\n+    @ForceInline\n+    final $abstractvectortype$\n+    lanewiseShiftTemplate(VectorOperators.Binary op,\n+                          Class<? extends VectorMask<$Boxtype$>> maskClass,\n+                          int e, VectorMask<$Boxtype$> m) {\n+        m.check(maskClass, this);\n+        assert(opKind(op, VO_SHIFT));\n+        \/\/ As per shift specification for Java, mask the shift count.\n+        e &= SHIFT_MASK;\n+        int opc = opCode(op);\n+        return VectorSupport.broadcastInt(\n+            opc, getClass(), maskClass, $type$.class, length(),\n+            this, e, m,\n+            BIN_INT_IMPL.find(op, opc, $abstractvectortype$::broadcastIntOperations));\n@@ -912,0 +1033,1 @@\n+\n@@ -913,1 +1035,1 @@\n-    ImplCache<Binary,VectorBroadcastIntOp<$abstractvectortype$>> BIN_INT_IMPL\n+    ImplCache<Binary,VectorBroadcastIntOp<$abstractvectortype$, VectorMask<$Boxtype$>>> BIN_INT_IMPL\n@@ -916,0 +1038,16 @@\n+    private static VectorBroadcastIntOp<$abstractvectortype$, VectorMask<$Boxtype$>> broadcastIntOperations(int opc_) {\n+        switch (opc_) {\n+            case VECTOR_OP_LSHIFT: return (v, n, m) ->\n+                    v.uOp(m, (i, a) -> ($type$)(a << n));\n+            case VECTOR_OP_RSHIFT: return (v, n, m) ->\n+                    v.uOp(m, (i, a) -> ($type$)(a >> n));\n+            case VECTOR_OP_URSHIFT: return (v, n, m) ->\n+                    v.uOp(m, (i, a) -> ($type$)((a & LSHR_SETUP_MASK) >>> n));\n+            case VECTOR_OP_LROTATE: return (v, n, m) ->\n+                    v.uOp(m, (i, a) -> rotateLeft(a, (int)n));\n+            case VECTOR_OP_RROTATE: return (v, n, m) ->\n+                    v.uOp(m, (i, a) -> rotateRight(a, (int)n));\n+            default: return null;\n+        }\n+    }\n+\n@@ -975,10 +1113,3 @@\n-            opc, getClass(), $type$.class, length(),\n-            this, that, tother,\n-            TERN_IMPL.find(op, opc, (opc_) -> {\n-              switch (opc_) {\n-#if[FP]\n-                case VECTOR_OP_FMA: return (v0, v1_, v2_) ->\n-                        v0.tOp(v1_, v2_, (i, a, b, c) -> Math.fma(a, b, c));\n-#end[FP]\n-                default: return null;\n-                }}));\n+            opc, getClass(), null, $type$.class, length(),\n+            this, that, tother, null,\n+            TERN_IMPL.find(op, opc, $abstractvectortype$::ternaryOperations));\n@@ -986,3 +1117,0 @@\n-    private static final\n-    ImplCache<Ternary,TernaryOperation<$abstractvectortype$>> TERN_IMPL\n-        = new ImplCache<>(Ternary.class, $Type$Vector.class);\n@@ -996,2 +1124,2 @@\n-    @ForceInline\n-    public final\n+    @Override\n+    public abstract\n@@ -1001,2 +1129,43 @@\n-                                  VectorMask<$Boxtype$> m) {\n-        return blend(lanewise(op, v1, v2), m);\n+                                  VectorMask<$Boxtype$> m);\n+    @ForceInline\n+    final\n+    $abstractvectortype$ lanewiseTemplate(VectorOperators.Ternary op,\n+                                          Class<? extends VectorMask<$Boxtype$>> maskClass,\n+                                          Vector<$Boxtype$> v1,\n+                                          Vector<$Boxtype$> v2,\n+                                          VectorMask<$Boxtype$> m) {\n+        $abstractvectortype$ that = ($abstractvectortype$) v1;\n+        $abstractvectortype$ tother = ($abstractvectortype$) v2;\n+        \/\/ It's a word: https:\/\/www.dictionary.com\/browse\/tother\n+        \/\/ See also Chapter 11 of Dickens, Our Mutual Friend:\n+        \/\/ \"Totherest Governor,\" replied Mr Riderhood...\n+        that.check(this);\n+        tother.check(this);\n+        m.check(maskClass, this);\n+\n+#if[BITWISE]\n+        if (op == BITWISE_BLEND) {\n+            \/\/ FIXME: Support this in the JIT.\n+            that = this.lanewise(XOR, that).lanewise(AND, tother);\n+            return this.lanewise(XOR, that, m);\n+        }\n+#end[BITWISE]\n+        int opc = opCode(op);\n+        return VectorSupport.ternaryOp(\n+            opc, getClass(), maskClass, $type$.class, length(),\n+            this, that, tother, m,\n+            TERN_IMPL.find(op, opc, $abstractvectortype$::ternaryOperations));\n+    }\n+\n+    private static final\n+    ImplCache<Ternary, TernaryOperation<$abstractvectortype$, VectorMask<$Boxtype$>>>\n+        TERN_IMPL = new ImplCache<>(Ternary.class, $Type$Vector.class);\n+\n+    private static TernaryOperation<$abstractvectortype$, VectorMask<$Boxtype$>> ternaryOperations(int opc_) {\n+        switch (opc_) {\n+#if[FP]\n+            case VECTOR_OP_FMA: return (v0, v1_, v2_, m) ->\n+                    v0.tOp(v1_, v2_, m, (i, a, b, c) -> Math.fma(a, b, c));\n+#end[FP]\n+            default: return null;\n+        }\n@@ -1059,1 +1228,1 @@\n-        return blend(lanewise(op, e1, e2), m);\n+        return lanewise(op, broadcast(e1), broadcast(e2), m);\n@@ -1117,1 +1286,1 @@\n-        return blend(lanewise(op, v1, e2), m);\n+        return lanewise(op, v1, broadcast(e2), m);\n@@ -1174,1 +1343,1 @@\n-        return blend(lanewise(op, e1, v2), m);\n+        return lanewise(op, broadcast(e1), v2, m);\n@@ -2019,2 +2188,0 @@\n-        Objects.requireNonNull(v);\n-        $Type$Species vsp = vspecies();\n@@ -2026,2 +2193,2 @@\n-            this, that,\n-            (cond, v0, v1) -> {\n+            this, that, null,\n+            (cond, v0, v1, m1) -> {\n@@ -2037,0 +2204,22 @@\n+    \/*package-private*\/\n+    @ForceInline\n+    final\n+    <M extends VectorMask<$Boxtype$>>\n+    M compareTemplate(Class<M> maskType, Comparison op, Vector<$Boxtype$> v, M m) {\n+        $abstractvectortype$ that = ($abstractvectortype$) v;\n+        that.check(this);\n+        m.check(maskType, this);\n+        int opc = opCode(op);\n+        return VectorSupport.compare(\n+            opc, getClass(), maskType, $type$.class, length(),\n+            this, that, m,\n+            (cond, v0, v1, m1) -> {\n+                AbstractMask<$Boxtype$> cmpM\n+                    = v0.bTest(cond, v1, (cond_, i, a, b)\n+                               -> compareWithOp(cond, a, b));\n+                @SuppressWarnings(\"unchecked\")\n+                M m2 = (M) cmpM.and(m1);\n+                return m2;\n+            });\n+    }\n+\n@@ -2056,12 +2245,0 @@\n-    \/**\n-     * {@inheritDoc} <!--workaround-->\n-     *\/\n-    @Override\n-    @ForceInline\n-    public final\n-    VectorMask<$Boxtype$> compare(VectorOperators.Comparison op,\n-                                  Vector<$Boxtype$> v,\n-                                  VectorMask<$Boxtype$> m) {\n-        return compare(op, v).and(m);\n-    }\n-\n@@ -2126,1 +2303,1 @@\n-        return compare(op, e).and(m);\n+        return compare(op, broadcast(e), m);\n@@ -2381,3 +2558,3 @@\n-            getClass(), shuffletype, $type$.class, length(),\n-            this, shuffle,\n-            (v1, s_) -> v1.uOp((i, a) -> {\n+            getClass(), shuffletype, null, $type$.class, length(),\n+            this, shuffle, null,\n+            (v1, s_, m_) -> v1.uOp((i, a) -> {\n@@ -2400,1 +2577,1 @@\n-    <S extends VectorShuffle<$Boxtype$>>\n+    <S extends VectorShuffle<$Boxtype$>, M extends VectorMask<$Boxtype$>>\n@@ -2402,0 +2579,1 @@\n+                                           Class<M> masktype,\n@@ -2403,9 +2581,3 @@\n-                                           VectorMask<$Boxtype$> m) {\n-        $abstractvectortype$ unmasked =\n-            VectorSupport.rearrangeOp(\n-                getClass(), shuffletype, $type$.class, length(),\n-                this, shuffle,\n-                (v1, s_) -> v1.uOp((i, a) -> {\n-                    int ei = s_.laneSource(i);\n-                    return ei < 0 ? 0 : v1.lane(ei);\n-                }));\n+                                           M m) {\n+\n+        m.check(masktype, this);\n@@ -2417,1 +2589,7 @@\n-        return broadcast(($type$)0).blend(unmasked, m);\n+        return VectorSupport.rearrangeOp(\n+                   getClass(), shuffletype, masktype, $type$.class, length(),\n+                   this, shuffle, m,\n+                   (v1, s_, m_) -> v1.uOp((i, a) -> {\n+                        int ei = s_.laneSource(i);\n+                        return ei < 0  || !m_.laneIsSet(i) ? 0 : v1.lane(ei);\n+                   }));\n@@ -2440,3 +2618,3 @@\n-                getClass(), shuffletype, $type$.class, length(),\n-                this, ws,\n-                (v0, s_) -> v0.uOp((i, a) -> {\n+                getClass(), shuffletype, null, $type$.class, length(),\n+                this, ws, null,\n+                (v0, s_, m_) -> v0.uOp((i, a) -> {\n@@ -2448,3 +2626,3 @@\n-                getClass(), shuffletype, $type$.class, length(),\n-                v, ws,\n-                (v1, s_) -> v1.uOp((i, a) -> {\n+                getClass(), shuffletype, null, $type$.class, length(),\n+                v, ws, null,\n+                (v1, s_, m_) -> v1.uOp((i, a) -> {\n@@ -2842,0 +3020,1 @@\n+                               Class<? extends VectorMask<$Boxtype$>> maskClass,\n@@ -2843,2 +3022,10 @@\n-        $abstractvectortype$ v = reduceIdentityVector(op).blend(this, m);\n-        return v.reduceLanesTemplate(op);\n+        m.check(maskClass, this);\n+        if (op == FIRST_NONZERO) {\n+            $abstractvectortype$ v = reduceIdentityVector(op).blend(this, m);\n+            return v.reduceLanesTemplate(op);\n+        }\n+        int opc = opCode(op);\n+        return fromBits(VectorSupport.reductionCoerced(\n+            opc, getClass(), maskClass, $type$.class, length(),\n+            this, m,\n+            REDUCE_IMPL.find(op, opc, $abstractvectortype$::reductionOperations)));\n@@ -2859,12 +3046,19 @@\n-            opc, getClass(), $type$.class, length(),\n-            this,\n-            REDUCE_IMPL.find(op, opc, (opc_) -> {\n-              switch (opc_) {\n-              case VECTOR_OP_ADD: return v ->\n-                      toBits(v.rOp(($type$)0, (i, a, b) -> ($type$)(a + b)));\n-              case VECTOR_OP_MUL: return v ->\n-                      toBits(v.rOp(($type$)1, (i, a, b) -> ($type$)(a * b)));\n-              case VECTOR_OP_MIN: return v ->\n-                      toBits(v.rOp(MAX_OR_INF, (i, a, b) -> ($type$) Math.min(a, b)));\n-              case VECTOR_OP_MAX: return v ->\n-                      toBits(v.rOp(MIN_OR_INF, (i, a, b) -> ($type$) Math.max(a, b)));\n+            opc, getClass(), null, $type$.class, length(),\n+            this, null,\n+            REDUCE_IMPL.find(op, opc, $abstractvectortype$::reductionOperations)));\n+    }\n+\n+    private static final\n+    ImplCache<Associative, ReductionOperation<$abstractvectortype$, VectorMask<$Boxtype$>>>\n+        REDUCE_IMPL = new ImplCache<>(Associative.class, $Type$Vector.class);\n+\n+    private static ReductionOperation<$abstractvectortype$, VectorMask<$Boxtype$>> reductionOperations(int opc_) {\n+        switch (opc_) {\n+            case VECTOR_OP_ADD: return (v, m) ->\n+                    toBits(v.rOp(($type$)0, m, (i, a, b) -> ($type$)(a + b)));\n+            case VECTOR_OP_MUL: return (v, m) ->\n+                    toBits(v.rOp(($type$)1, m, (i, a, b) -> ($type$)(a * b)));\n+            case VECTOR_OP_MIN: return (v, m) ->\n+                    toBits(v.rOp(MAX_OR_INF, m, (i, a, b) -> ($type$) Math.min(a, b)));\n+            case VECTOR_OP_MAX: return (v, m) ->\n+                    toBits(v.rOp(MIN_OR_INF, m, (i, a, b) -> ($type$) Math.max(a, b)));\n@@ -2872,6 +3066,6 @@\n-              case VECTOR_OP_AND: return v ->\n-                      toBits(v.rOp(($type$)-1, (i, a, b) -> ($type$)(a & b)));\n-              case VECTOR_OP_OR: return v ->\n-                      toBits(v.rOp(($type$)0, (i, a, b) -> ($type$)(a | b)));\n-              case VECTOR_OP_XOR: return v ->\n-                      toBits(v.rOp(($type$)0, (i, a, b) -> ($type$)(a ^ b)));\n+            case VECTOR_OP_AND: return (v, m) ->\n+                    toBits(v.rOp(($type$)-1, m, (i, a, b) -> ($type$)(a & b)));\n+            case VECTOR_OP_OR: return (v, m) ->\n+                    toBits(v.rOp(($type$)0, m, (i, a, b) -> ($type$)(a | b)));\n+            case VECTOR_OP_XOR: return (v, m) ->\n+                    toBits(v.rOp(($type$)0, m, (i, a, b) -> ($type$)(a ^ b)));\n@@ -2879,2 +3073,2 @@\n-              default: return null;\n-              }})));\n+            default: return null;\n+        }\n@@ -2882,3 +3076,0 @@\n-    private static final\n-    ImplCache<Associative,Function<$abstractvectortype$,Long>> REDUCE_IMPL\n-        = new ImplCache<>(Associative.class, $Type$Vector.class);\n@@ -3178,3 +3369,1 @@\n-            $abstractvectortype$ zero = vsp.zero();\n-            $abstractvectortype$ v = zero.fromByteArray0(a, offset);\n-            return zero.blend(v.maybeSwap(bo), m);\n+            return vsp.dummyVector().fromByteArray0(a, offset, m).maybeSwap(bo);\n@@ -3242,2 +3431,1 @@\n-            $abstractvectortype$ zero = vsp.zero();\n-            return zero.blend(zero.fromArray0(a, offset), m);\n+            return vsp.dummyVector().fromArray0(a, offset, m);\n@@ -3336,3 +3524,3 @@\n-            vectorType, $type$.class, vsp.laneCount(),\n-            IntVector.species(vsp.indexShape()).vectorType(),\n-            a, ARRAY_BASE, vix,\n+            vectorType, null, $type$.class, vsp.laneCount(),\n+            isp.vectorType(),\n+            a, ARRAY_BASE, vix, null,\n@@ -3340,1 +3528,1 @@\n-            ($type$[] c, int idx, int[] iMap, int idy, $Type$Species s) ->\n+            (c, idx, iMap, idy, s, vm) ->\n@@ -3342,1 +3530,1 @@\n-        }\n+    }\n@@ -3402,1 +3590,0 @@\n-            \/\/ FIXME: Cannot vectorize yet, if there's a mask.\n@@ -3404,1 +3591,1 @@\n-            return vsp.vOp(m, n -> a[offset + indexMap[mapOffset + n]]);\n+            return vsp.dummyVector().fromArray0(a, offset, indexMap, mapOffset, m);\n@@ -3465,2 +3652,1 @@\n-            $abstractvectortype$ zero = vsp.zero();\n-            return zero.blend(zero.fromCharArray0(a, offset), m);\n+            return vsp.dummyVector().fromCharArray0(a, offset, m);\n@@ -3626,1 +3812,1 @@\n-            return zero.blend(zero.fromBooleanArray0(a, offset), m);\n+            return vsp.dummyVector().fromBooleanArray0(a, offset, m);\n@@ -3817,3 +4003,1 @@\n-            $abstractvectortype$ zero = vsp.zero();\n-            $abstractvectortype$ v = zero.fromByteBuffer0(bb, offset);\n-            return zero.blend(v.maybeSwap(bo), m);\n+            return vsp.dummyVector().fromByteBuffer0(bb, offset, m).maybeSwap(bo);\n@@ -3891,1 +4075,0 @@\n-            \/\/ FIXME: optimize\n@@ -3894,1 +4077,1 @@\n-            stOp(a, offset, m, (arr, off, i, v) -> arr[off+i] = v);\n+            intoArray0(a, offset, m);\n@@ -3976,1 +4159,1 @@\n-            vsp.vectorType(), vsp.elementType(), vsp.laneCount(),\n+            vsp.vectorType(), null, vsp.elementType(), vsp.laneCount(),\n@@ -3979,1 +4162,1 @@\n-            this,\n+            this, null,\n@@ -3981,1 +4164,1 @@\n-            (arr, off, v, map, mo)\n+            (arr, off, v, map, mo, vm)\n@@ -4042,6 +4225,1 @@\n-            \/\/ FIXME: Cannot vectorize yet, if there's a mask.\n-            stOp(a, offset, m,\n-                 (arr, off, i, e) -> {\n-                     int j = indexMap[mapOffset + i];\n-                     arr[off + j] = e;\n-                 });\n+            intoArray0(a, offset, indexMap, mapOffset, m);\n@@ -4115,1 +4293,0 @@\n-            \/\/ FIXME: optimize\n@@ -4118,1 +4295,1 @@\n-            stOp(a, offset, m, (arr, off, i, v) -> arr[off+i] = (char) v);\n+            intoCharArray0(a, offset, m);\n@@ -4278,1 +4455,0 @@\n-            \/\/ FIXME: optimize\n@@ -4281,1 +4457,1 @@\n-            stOp(a, offset, m, (arr, off, i, e) -> arr[off+i] = (e & 1) != 0);\n+            intoBooleanArray0(a, offset, m);\n@@ -4401,1 +4577,0 @@\n-            \/\/ FIXME: optimize\n@@ -4404,3 +4579,1 @@\n-            ByteBuffer wb = wrapper(a, bo);\n-            this.stOp(wb, offset, m,\n-                    (wb_, o, i, e) -> wb_.put{#if[byte]?(:$Type$(}o + i * $sizeInBytes$, e));\n+            maybeSwap(bo).intoByteArray0(a, offset, m);\n@@ -4418,1 +4591,1 @@\n-        if (bb.isReadOnly()) {\n+        if (ScopedMemoryAccess.isReadOnly(bb)) {\n@@ -4437,1 +4610,0 @@\n-            \/\/ FIXME: optimize\n@@ -4443,3 +4615,1 @@\n-            ByteBuffer wb = wrapper(bb, bo);\n-            this.stOp(wb, offset, m,\n-                    (wb_, o, i, e) -> wb_.put{#if[byte]?(:$Type$(}o + i * $sizeInBytes$, e));\n+            maybeSwap(bo).intoByteBuffer0(bb, offset, m);\n@@ -4483,0 +4653,78 @@\n+    \/*package-private*\/\n+    abstract\n+    $abstractvectortype$ fromArray0($type$[] a, int offset, VectorMask<$Boxtype$> m);\n+    @ForceInline\n+    final\n+    <M extends VectorMask<$Boxtype$>>\n+    $abstractvectortype$ fromArray0Template(Class<M> maskClass, $type$[] a, int offset, M m) {\n+        m.check(species());\n+        $Type$Species vsp = vspecies();\n+        return VectorSupport.loadMasked(\n+            vsp.vectorType(), maskClass, vsp.elementType(), vsp.laneCount(),\n+            a, arrayAddress(a, offset), m,\n+            a, offset, vsp,\n+            (arr, off, s, vm) -> s.ldOp(arr, off, vm,\n+                                        (arr_, off_, i) -> arr_[off_ + i]));\n+    }\n+\n+#if[!byteOrShort]\n+    \/*package-private*\/\n+    abstract\n+    $abstractvectortype$ fromArray0($type$[] a, int offset,\n+                                    int[] indexMap, int mapOffset,\n+                                    VectorMask<$Boxtype$> m);\n+    @ForceInline\n+    final\n+    <M extends VectorMask<$Boxtype$>>\n+    $abstractvectortype$ fromArray0Template(Class<M> maskClass, $type$[] a, int offset,\n+                                            int[] indexMap, int mapOffset, M m) {\n+        $Type$Species vsp = vspecies();\n+        IntVector.IntSpecies isp = IntVector.species(vsp.indexShape());\n+        Objects.requireNonNull(a);\n+        Objects.requireNonNull(indexMap);\n+        m.check(vsp);\n+        Class<? extends $abstractvectortype$> vectorType = vsp.vectorType();\n+\n+#if[longOrDouble]\n+        if (vsp.laneCount() == 1) {\n+          return $abstractvectortype$.fromArray(vsp, a, offset + indexMap[mapOffset], m);\n+        }\n+\n+        \/\/ Index vector: vix[0:n] = k -> offset + indexMap[mapOffset + k]\n+        IntVector vix;\n+        if (isp.laneCount() != vsp.laneCount()) {\n+            \/\/ For $Type$MaxVector,  if vector length is non-power-of-two or\n+            \/\/ 2048 bits, indexShape of $Type$ species is S_MAX_BIT.\n+            \/\/ Assume that vector length is 2048, then the lane count of $Type$\n+            \/\/ vector is 32. When converting $Type$ species to int species,\n+            \/\/ indexShape is still S_MAX_BIT, but the lane count of int vector\n+            \/\/ is 64. So when loading index vector (IntVector), only lower half\n+            \/\/ of index data is needed.\n+            vix = IntVector\n+                .fromArray(isp, indexMap, mapOffset, IntMaxVector.IntMaxMask.LOWER_HALF_TRUE_MASK)\n+                .add(offset);\n+        } else {\n+            vix = IntVector\n+                .fromArray(isp, indexMap, mapOffset)\n+                .add(offset);\n+        }\n+#else[longOrDouble]\n+        \/\/ Index vector: vix[0:n] = k -> offset + indexMap[mapOffset + k]\n+        IntVector vix = IntVector\n+            .fromArray(isp, indexMap, mapOffset)\n+            .add(offset);\n+#end[longOrDouble]\n+\n+        \/\/ FIXME: Check index under mask controlling.\n+        vix = VectorIntrinsics.checkIndex(vix, a.length);\n+\n+        return VectorSupport.loadWithMap(\n+            vectorType, maskClass, $type$.class, vsp.laneCount(),\n+            isp.vectorType(),\n+            a, ARRAY_BASE, vix, m,\n+            a, offset, indexMap, mapOffset, vsp,\n+            (c, idx, iMap, idy, s, vm) ->\n+            s.vOp(vm, n -> c[idx + iMap[idy+n]]));\n+    }\n+#end[!byteOrShort]\n+\n@@ -4498,0 +4746,17 @@\n+\n+    \/*package-private*\/\n+    abstract\n+    $abstractvectortype$ fromCharArray0(char[] a, int offset, VectorMask<$Boxtype$> m);\n+    @ForceInline\n+    final\n+    <M extends VectorMask<$Boxtype$>>\n+    $abstractvectortype$ fromCharArray0Template(Class<M> maskClass, char[] a, int offset, M m) {\n+        m.check(species());\n+        $Type$Species vsp = vspecies();\n+        return VectorSupport.loadMasked(\n+                vsp.vectorType(), maskClass, vsp.elementType(), vsp.laneCount(),\n+                a, charArrayAddress(a, offset), m,\n+                a, offset, vsp,\n+                (arr, off, s, vm) -> s.ldOp(arr, off, vm,\n+                                            (arr_, off_, i) -> (short) arr_[off_ + i]));\n+    }\n@@ -4515,0 +4780,17 @@\n+\n+    \/*package-private*\/\n+    abstract\n+    $abstractvectortype$ fromBooleanArray0(boolean[] a, int offset, VectorMask<$Boxtype$> m);\n+    @ForceInline\n+    final\n+    <M extends VectorMask<$Boxtype$>>\n+    $abstractvectortype$ fromBooleanArray0Template(Class<M> maskClass, boolean[] a, int offset, M m) {\n+        m.check(species());\n+        $Type$Species vsp = vspecies();\n+        return VectorSupport.loadMasked(\n+            vsp.vectorType(), maskClass, vsp.elementType(), vsp.laneCount(),\n+            a, booleanArrayAddress(a, offset), m,\n+            a, offset, vsp,\n+            (arr, off, s, vm) -> s.ldOp(arr, off, vm,\n+                                        (arr_, off_, i) -> (byte) (arr_[off_ + i] ? 1 : 0)));\n+    }\n@@ -4535,0 +4817,19 @@\n+    abstract\n+    $abstractvectortype$ fromByteArray0(byte[] a, int offset, VectorMask<$Boxtype$> m);\n+    @ForceInline\n+    final\n+    <M extends VectorMask<$Boxtype$>>\n+    $abstractvectortype$ fromByteArray0Template(Class<M> maskClass, byte[] a, int offset, M m) {\n+        $Type$Species vsp = vspecies();\n+        m.check(vsp);\n+        return VectorSupport.loadMasked(\n+            vsp.vectorType(), maskClass, vsp.elementType(), vsp.laneCount(),\n+            a, byteArrayAddress(a, offset), m,\n+            a, offset, vsp,\n+            (arr, off, s, vm) -> {\n+                ByteBuffer wb = wrapper(arr, NATIVE_ENDIAN);\n+                return s.ldOp(wb, off, vm,\n+                        (wb_, o, i) -> wb_.get{#if[byte]?(:$Type$(}o + i * $sizeInBytes$));\n+            });\n+    }\n+\n@@ -4551,0 +4852,18 @@\n+    abstract\n+    $abstractvectortype$ fromByteBuffer0(ByteBuffer bb, int offset, VectorMask<$Boxtype$> m);\n+    @ForceInline\n+    final\n+    <M extends VectorMask<$Boxtype$>>\n+    $abstractvectortype$ fromByteBuffer0Template(Class<M> maskClass, ByteBuffer bb, int offset, M m) {\n+        $Type$Species vsp = vspecies();\n+        m.check(vsp);\n+        return ScopedMemoryAccess.loadFromByteBufferMasked(\n+                vsp.vectorType(), maskClass, vsp.elementType(), vsp.laneCount(),\n+                bb, offset, m, vsp,\n+                (buf, off, s, vm) -> {\n+                    ByteBuffer wb = wrapper(buf, NATIVE_ENDIAN);\n+                    return s.ldOp(wb, off, vm,\n+                            (wb_, o, i) -> wb_.get{#if[byte]?(:$Type$(}o + i * $sizeInBytes$));\n+                });\n+    }\n+\n@@ -4570,0 +4889,99 @@\n+    abstract\n+    void intoArray0($type$[] a, int offset, VectorMask<$Boxtype$> m);\n+    @ForceInline\n+    final\n+    <M extends VectorMask<$Boxtype$>>\n+    void intoArray0Template(Class<M> maskClass, $type$[] a, int offset, M m) {\n+        m.check(species());\n+        $Type$Species vsp = vspecies();\n+        VectorSupport.storeMasked(\n+            vsp.vectorType(), maskClass, vsp.elementType(), vsp.laneCount(),\n+            a, arrayAddress(a, offset),\n+            this, m, a, offset,\n+            (arr, off, v, vm)\n+            -> v.stOp(arr, off, vm,\n+                      (arr_, off_, i, e) -> arr_[off_ + i] = e));\n+    }\n+\n+#if[!byteOrShort]\n+    abstract\n+    void intoArray0($type$[] a, int offset,\n+                    int[] indexMap, int mapOffset,\n+                    VectorMask<$Boxtype$> m);\n+    @ForceInline\n+    final\n+    <M extends VectorMask<$Boxtype$>>\n+    void intoArray0Template(Class<M> maskClass, $type$[] a, int offset,\n+                            int[] indexMap, int mapOffset, M m) {\n+        m.check(species());\n+        $Type$Species vsp = vspecies();\n+        IntVector.IntSpecies isp = IntVector.species(vsp.indexShape());\n+#if[longOrDouble]\n+        if (vsp.laneCount() == 1) {\n+            intoArray(a, offset + indexMap[mapOffset], m);\n+            return;\n+        }\n+\n+        \/\/ Index vector: vix[0:n] = i -> offset + indexMap[mo + i]\n+        IntVector vix;\n+        if (isp.laneCount() != vsp.laneCount()) {\n+            \/\/ For $Type$MaxVector,  if vector length  is 2048 bits, indexShape\n+            \/\/ of $Type$ species is S_MAX_BIT. and the lane count of $Type$\n+            \/\/ vector is 32. When converting $Type$ species to int species,\n+            \/\/ indexShape is still S_MAX_BIT, but the lane count of int vector\n+            \/\/ is 64. So when loading index vector (IntVector), only lower half\n+            \/\/ of index data is needed.\n+            vix = IntVector\n+                .fromArray(isp, indexMap, mapOffset, IntMaxVector.IntMaxMask.LOWER_HALF_TRUE_MASK)\n+                .add(offset);\n+        } else {\n+            vix = IntVector\n+                .fromArray(isp, indexMap, mapOffset)\n+                .add(offset);\n+        }\n+\n+#else[longOrDouble]\n+        \/\/ Index vector: vix[0:n] = i -> offset + indexMap[mo + i]\n+        IntVector vix = IntVector\n+            .fromArray(isp, indexMap, mapOffset)\n+            .add(offset);\n+#end[longOrDouble]\n+\n+        \/\/ FIXME: Check index under mask controlling.\n+        vix = VectorIntrinsics.checkIndex(vix, a.length);\n+\n+        VectorSupport.storeWithMap(\n+            vsp.vectorType(), maskClass, vsp.elementType(), vsp.laneCount(),\n+            isp.vectorType(),\n+            a, arrayAddress(a, 0), vix,\n+            this, m,\n+            a, offset, indexMap, mapOffset,\n+            (arr, off, v, map, mo, vm)\n+            -> v.stOp(arr, off, vm,\n+                      (arr_, off_, i, e) -> {\n+                          int j = map[mo + i];\n+                          arr[off + j] = e;\n+                      }));\n+    }\n+#end[!byteOrShort]\n+\n+#if[byte]\n+    abstract\n+    void intoBooleanArray0(boolean[] a, int offset, VectorMask<$Boxtype$> m);\n+    @ForceInline\n+    final\n+    <M extends VectorMask<$Boxtype$>>\n+    void intoBooleanArray0Template(Class<M> maskClass, boolean[] a, int offset, M m) {\n+        m.check(species());\n+        $Type$Species vsp = vspecies();\n+        ByteVector normalized = this.and((byte) 1);\n+        VectorSupport.storeMasked(\n+            vsp.vectorType(), maskClass, vsp.elementType(), vsp.laneCount(),\n+            a, booleanArrayAddress(a, offset),\n+            normalized, m, a, offset,\n+            (arr, off, v, vm)\n+            -> v.stOp(arr, off, vm,\n+                      (arr_, off_, i, e) -> arr_[off_ + i] = (e & 1) != 0));\n+    }\n+#end[byte]\n+\n@@ -4587,0 +5005,19 @@\n+    abstract\n+    void intoByteArray0(byte[] a, int offset, VectorMask<$Boxtype$> m);\n+    @ForceInline\n+    final\n+    <M extends VectorMask<$Boxtype$>>\n+    void intoByteArray0Template(Class<M> maskClass, byte[] a, int offset, M m) {\n+        $Type$Species vsp = vspecies();\n+        m.check(vsp);\n+        VectorSupport.storeMasked(\n+            vsp.vectorType(), maskClass, vsp.elementType(), vsp.laneCount(),\n+            a, byteArrayAddress(a, offset),\n+            this, m, a, offset,\n+            (arr, off, v, vm) -> {\n+                ByteBuffer wb = wrapper(arr, NATIVE_ENDIAN);\n+                v.stOp(wb, off, vm,\n+                        (tb_, o, i, e) -> tb_.put{#if[byte]?(:$Type$(}o + i * $sizeInBytes$, e));\n+            });\n+    }\n+\n@@ -4601,0 +5038,38 @@\n+    abstract\n+    void intoByteBuffer0(ByteBuffer bb, int offset, VectorMask<$Boxtype$> m);\n+    @ForceInline\n+    final\n+    <M extends VectorMask<$Boxtype$>>\n+    void intoByteBuffer0Template(Class<M> maskClass, ByteBuffer bb, int offset, M m) {\n+        $Type$Species vsp = vspecies();\n+        m.check(vsp);\n+        ScopedMemoryAccess.storeIntoByteBufferMasked(\n+                vsp.vectorType(), maskClass, vsp.elementType(), vsp.laneCount(),\n+                this, m, bb, offset,\n+                (buf, off, v, vm) -> {\n+                    ByteBuffer wb = wrapper(buf, NATIVE_ENDIAN);\n+                    v.stOp(wb, off, vm,\n+                            (wb_, o, i, e) -> wb_.put{#if[byte]?(:$Type$(}o + i * $sizeInBytes$, e));\n+                });\n+    }\n+\n+#if[short]\n+    \/*package-private*\/\n+    abstract\n+    void intoCharArray0(char[] a, int offset, VectorMask<$Boxtype$> m);\n+    @ForceInline\n+    final\n+    <M extends VectorMask<$Boxtype$>>\n+    void intoCharArray0Template(Class<M> maskClass, char[] a, int offset, M m) {\n+        m.check(species());\n+        $Type$Species vsp = vspecies();\n+        VectorSupport.storeMasked(\n+            vsp.vectorType(), maskClass, vsp.elementType(), vsp.laneCount(),\n+            a, charArrayAddress(a, offset),\n+            this, m, a, offset,\n+            (arr, off, v, vm)\n+            -> v.stOp(arr, off, vm,\n+                      (arr_, off_, i, e) -> arr_[off_ + i] = (char) e));\n+    }\n+#end[short]\n+\n@@ -4976,1 +5451,1 @@\n-                                      AbstractMask<$Boxtype$> m,\n+                                      VectorMask<$Boxtype$> m,\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/X-Vector.java.template","additions":737,"deletions":262,"binary":false,"changes":999,"status":"modified"},{"patch":"@@ -241,2 +241,2 @@\n-    $type$ rOp($type$ v, FBinOp f) {\n-        return super.rOpTemplate(v, f);  \/\/ specialize\n+    $type$ rOp($type$ v, VectorMask<$Boxtype$> m, FBinOp f) {\n+        return super.rOpTemplate(v, m, f);  \/\/ specialize\n@@ -278,0 +278,6 @@\n+    @Override\n+    @ForceInline\n+    public $vectortype$ lanewise(Unary op, VectorMask<$Boxtype$> m) {\n+        return ($vectortype$) super.lanewiseTemplate(op, $masktype$.class, ($masktype$) m);  \/\/ specialize\n+    }\n+\n@@ -284,0 +290,6 @@\n+    @Override\n+    @ForceInline\n+    public $vectortype$ lanewise(Binary op, Vector<$Boxtype$> v, VectorMask<$Boxtype$> m) {\n+        return ($vectortype$) super.lanewiseTemplate(op, $masktype$.class, v, ($masktype$) m);  \/\/ specialize\n+    }\n+\n@@ -291,0 +303,7 @@\n+\n+    \/*package-private*\/\n+    @Override\n+    @ForceInline $vectortype$\n+    lanewiseShift(VectorOperators.Binary op, int e, VectorMask<$Boxtype$> m) {\n+        return ($vectortype$) super.lanewiseShiftTemplate(op, $masktype$.class, e, ($masktype$) m);  \/\/ specialize\n+    }\n@@ -298,1 +317,1 @@\n-    lanewise(VectorOperators.Ternary op, Vector<$Boxtype$> v1, Vector<$Boxtype$> v2) {\n+    lanewise(Ternary op, Vector<$Boxtype$> v1, Vector<$Boxtype$> v2) {\n@@ -302,0 +321,8 @@\n+    @Override\n+    @ForceInline\n+    public final\n+    $vectortype$\n+    lanewise(Ternary op, Vector<$Boxtype$> v1, Vector<$Boxtype$> v2, VectorMask<$Boxtype$> m) {\n+        return ($vectortype$) super.lanewiseTemplate(op, $masktype$.class, v1, v2, ($masktype$) m);  \/\/ specialize\n+    }\n+\n@@ -321,1 +348,1 @@\n-        return super.reduceLanesTemplate(op, m);  \/\/ specialized\n+        return super.reduceLanesTemplate(op, $masktype$.class, ($masktype$) m);  \/\/ specialized\n@@ -334,1 +361,1 @@\n-        return (long) super.reduceLanesTemplate(op, m);  \/\/ specialized\n+        return (long) super.reduceLanesTemplate(op, $masktype$.class, ($masktype$) m);  \/\/ specialized\n@@ -372,0 +399,7 @@\n+    @Override\n+    @ForceInline\n+    public final $masktype$ compare(Comparison op, Vector<$Boxtype$> v, VectorMask<$Boxtype$> m) {\n+        return super.compareTemplate($masktype$.class, op, v, ($masktype$) m);\n+    }\n+\n+\n@@ -428,0 +462,1 @@\n+                                    $masktype$.class,\n@@ -858,10 +893,6 @@\n-            if (VSIZE == species.vectorBitSize()) {\n-                Class<?> dtype = species.elementType();\n-                Class<?> dmtype = species.maskType();\n-                return VectorSupport.convert(VectorSupport.VECTOR_OP_REINTERPRET,\n-                    this.getClass(), ETYPE, VLENGTH,\n-                    dmtype, dtype, VLENGTH,\n-                    this, species,\n-                    $Type$$bits$Mask::defaultMaskCast);\n-            }\n-            return this.defaultMaskCast(species);\n+\n+            return VectorSupport.convert(VectorSupport.VECTOR_OP_CAST,\n+                this.getClass(), ETYPE, VLENGTH,\n+                species.maskType(), species.elementType(), VLENGTH,\n+                this, species,\n+                (m, s) -> s.maskFactory(m.toArray()).check(s));\n@@ -893,3 +924,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_AND, $masktype$.class, $bitstype$.class, VLENGTH,\n-                                             this, m,\n-                                             (m1, m2) -> m1.bOp(m2, (i, a, b) -> a & b));\n+            return VectorSupport.binaryOp(VECTOR_OP_AND, $masktype$.class, null, $bitstype$.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a & b));\n@@ -903,3 +934,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_OR, $masktype$.class, $bitstype$.class, VLENGTH,\n-                                             this, m,\n-                                             (m1, m2) -> m1.bOp(m2, (i, a, b) -> a | b));\n+            return VectorSupport.binaryOp(VECTOR_OP_OR, $masktype$.class, null, $bitstype$.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a | b));\n@@ -913,3 +944,3 @@\n-            return VectorSupport.binaryOp(VECTOR_OP_XOR, $masktype$.class, $bitstype$.class, VLENGTH,\n-                                          this, m,\n-                                          (m1, m2) -> m1.bOp(m2, (i, a, b) -> a ^ b));\n+            return VectorSupport.binaryOp(VECTOR_OP_XOR, $masktype$.class, null, $bitstype$.class, VLENGTH,\n+                                          this, m, null,\n+                                          (m1, m2, vm) -> m1.bOp(m2, (i, a, b) -> a ^ b));\n@@ -923,2 +954,2 @@\n-            return VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_TRUECOUNT, $masktype$.class, $bitstype$.class, VLENGTH, this,\n-                                                      (m) -> trueCountHelper((($masktype$)m).getBits()));\n+            return (int) VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_TRUECOUNT, $masktype$.class, $bitstype$.class, VLENGTH, this,\n+                                                      (m) -> trueCountHelper(m.getBits()));\n@@ -930,2 +961,2 @@\n-            return VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_FIRSTTRUE, $masktype$.class, $bitstype$.class, VLENGTH, this,\n-                                                      (m) -> firstTrueHelper((($masktype$)m).getBits()));\n+            return (int) VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_FIRSTTRUE, $masktype$.class, $bitstype$.class, VLENGTH, this,\n+                                                      (m) -> firstTrueHelper(m.getBits()));\n@@ -937,2 +968,12 @@\n-            return VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_LASTTRUE, $masktype$.class, $bitstype$.class, VLENGTH, this,\n-                                                      (m) -> lastTrueHelper((($masktype$)m).getBits()));\n+            return (int) VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_LASTTRUE, $masktype$.class, $bitstype$.class, VLENGTH, this,\n+                                                      (m) -> lastTrueHelper(m.getBits()));\n+        }\n+\n+        @Override\n+        @ForceInline\n+        public long toLong() {\n+            if (length() > Long.SIZE) {\n+                throw new UnsupportedOperationException(\"too many lanes for one long\");\n+            }\n+            return VectorSupport.maskReductionCoerced(VECTOR_OP_MASK_TOLONG, $masktype$.class, $bitstype$.class, VLENGTH, this,\n+                                                      (m) -> toLongHelper(m.getBits()));\n@@ -1064,0 +1105,16 @@\n+    @ForceInline\n+    @Override\n+    final\n+    $abstractvectortype$ fromArray0($type$[] a, int offset, VectorMask<$Boxtype$> m) {\n+        return super.fromArray0Template($masktype$.class, a, offset, ($masktype$) m);  \/\/ specialize\n+    }\n+\n+#if[!byteOrShort]\n+    @ForceInline\n+    @Override\n+    final\n+    $abstractvectortype$ fromArray0($type$[] a, int offset, int[] indexMap, int mapOffset, VectorMask<$Boxtype$> m) {\n+        return super.fromArray0Template($masktype$.class, a, offset, indexMap, mapOffset, ($masktype$) m);\n+    }\n+#end[!byteOrShort]\n+\n@@ -1071,0 +1128,7 @@\n+\n+    @ForceInline\n+    @Override\n+    final\n+    $abstractvectortype$ fromCharArray0(char[] a, int offset, VectorMask<$Boxtype$> m) {\n+        return super.fromCharArray0Template($masktype$.class, a, offset, ($masktype$) m);  \/\/ specialize\n+    }\n@@ -1080,0 +1144,7 @@\n+\n+    @ForceInline\n+    @Override\n+    final\n+    $abstractvectortype$ fromBooleanArray0(boolean[] a, int offset, VectorMask<$Boxtype$> m) {\n+        return super.fromBooleanArray0Template($masktype$.class, a, offset, ($masktype$) m);  \/\/ specialize\n+    }\n@@ -1089,0 +1160,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    $abstractvectortype$ fromByteArray0(byte[] a, int offset, VectorMask<$Boxtype$> m) {\n+        return super.fromByteArray0Template($masktype$.class, a, offset, ($masktype$) m);  \/\/ specialize\n+    }\n+\n@@ -1096,0 +1174,7 @@\n+    @ForceInline\n+    @Override\n+    final\n+    $abstractvectortype$ fromByteBuffer0(ByteBuffer bb, int offset, VectorMask<$Boxtype$> m) {\n+        return super.fromByteBuffer0Template($masktype$.class, bb, offset, ($masktype$) m);  \/\/ specialize\n+    }\n+\n@@ -1103,0 +1188,25 @@\n+    @ForceInline\n+    @Override\n+    final\n+    void intoArray0($type$[] a, int offset, VectorMask<$Boxtype$> m) {\n+        super.intoArray0Template($masktype$.class, a, offset, ($masktype$) m);\n+    }\n+\n+#if[!byteOrShort]\n+    @ForceInline\n+    @Override\n+    final\n+    void intoArray0($type$[] a, int offset, int[] indexMap, int mapOffset, VectorMask<$Boxtype$> m) {\n+        super.intoArray0Template($masktype$.class, a, offset, indexMap, mapOffset, ($masktype$) m);\n+    }\n+#end[!byteOrShort]\n+\n+#if[byte]\n+    @ForceInline\n+    @Override\n+    final\n+    void intoBooleanArray0(boolean[] a, int offset, VectorMask<$Boxtype$> m) {\n+        super.intoBooleanArray0Template($masktype$.class, a, offset, ($masktype$) m);\n+    }\n+#end[byte]\n+\n@@ -1110,0 +1220,23 @@\n+    @ForceInline\n+    @Override\n+    final\n+    void intoByteArray0(byte[] a, int offset, VectorMask<$Boxtype$> m) {\n+        super.intoByteArray0Template($masktype$.class, a, offset, ($masktype$) m);  \/\/ specialize\n+    }\n+\n+    @ForceInline\n+    @Override\n+    final\n+    void intoByteBuffer0(ByteBuffer bb, int offset, VectorMask<$Boxtype$> m) {\n+        super.intoByteBuffer0Template($masktype$.class, bb, offset, ($masktype$) m);\n+    }\n+\n+#if[short]\n+    @ForceInline\n+    @Override\n+    final\n+    void intoCharArray0(char[] a, int offset, VectorMask<$Boxtype$> m) {\n+        super.intoCharArray0Template($masktype$.class, a, offset, ($masktype$) m);\n+    }\n+#end[short]\n+\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/X-VectorBits.java.template","additions":163,"deletions":30,"binary":false,"changes":193,"status":"modified"},{"patch":"@@ -22,3 +22,0 @@\n-; This file contains duplicate entries as globalDefinitions_vecApi.hpp\n-; It is intended for inclusion in .s files compiled with masm\n-\n","filename":"src\/jdk.incubator.vector\/windows\/native\/libjsvml\/globals_vectorApiSupport_windows.S.inc","additions":0,"deletions":3,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -109,1 +109,1 @@\n-        if ((uriString == null) || (uriString.compareTo(\"localhost\") == 0)) {\n+        if (uriString == null || uriString.equals(\"localhost\")) {\n@@ -250,1 +250,1 @@\n-        if ((scheme != null) && (scheme.compareTo(\"file\") == 0)) {\n+        if (\"file\".equals(scheme)) {\n@@ -346,1 +346,1 @@\n-        if ((scheme != null) && (scheme.compareTo(\"file\") == 0)) {\n+        if (\"file\".equals(scheme)) {\n","filename":"src\/jdk.internal.jvmstat\/share\/classes\/sun\/jvmstat\/monitor\/HostIdentifier.java","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2004, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2004, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -206,1 +206,1 @@\n-            if (hostname.compareTo(\"localhost\") == 0) {\n+            if (hostname.equals(\"localhost\")) {\n","filename":"src\/jdk.internal.jvmstat\/share\/classes\/sun\/jvmstat\/monitor\/MonitoredHost.java","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2004, 2011, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2004, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -85,1 +85,1 @@\n-        } else if (commandLine.compareTo(\"Unknown\") == 0) {\n+        } else if (commandLine.equals(\"Unknown\")) {\n","filename":"src\/jdk.internal.jvmstat\/share\/classes\/sun\/jvmstat\/monitor\/MonitoredVmUtil.java","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -177,1 +177,1 @@\n-        if ((s != null) && (s.compareTo(\"file\") == 0)) {\n+        if (\"file\".equals(s)) {\n","filename":"src\/jdk.internal.jvmstat\/share\/classes\/sun\/jvmstat\/monitor\/VmIdentifier.java","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2004, 2014, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2004, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -31,1 +31,0 @@\n-import java.util.regex.*;\n@@ -130,1 +129,1 @@\n-                    || (currentToken.sval.compareTo(ALIAS) != 0)) {\n+                    || !currentToken.sval.equals(ALIAS)) {\n@@ -146,1 +145,1 @@\n-                     && (currentToken.sval.compareTo(ALIAS) != 0));\n+                     && !currentToken.sval.equals(ALIAS));\n","filename":"src\/jdk.internal.jvmstat\/share\/classes\/sun\/jvmstat\/perfdata\/monitor\/AliasFileParser.java","additions":3,"deletions":4,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2004, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2004, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -31,1 +31,0 @@\n-import java.net.URI;\n@@ -63,1 +62,1 @@\n-            if (mode.compareTo(\"r\") == 0) {\n+            if (mode.equals(\"r\")) {\n@@ -65,1 +64,1 @@\n-            } else if (mode.compareTo(\"rw\") == 0) {\n+            } else if (mode.equals(\"rw\")) {\n","filename":"src\/jdk.internal.jvmstat\/share\/classes\/sun\/jvmstat\/perfdata\/monitor\/protocol\/file\/PerfDataBuffer.java","additions":3,"deletions":4,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -31,1 +31,0 @@\n-import java.util.regex.*;\n@@ -363,1 +362,1 @@\n-        if (collector.stringValue().compareTo(\"PSScavenge\") == 0) {\n+        if (collector.stringValue().equals(\"PSScavenge\")) {\n","filename":"src\/jdk.internal.jvmstat\/share\/classes\/sun\/jvmstat\/perfdata\/monitor\/v1_0\/PerfDataBuffer.java","additions":1,"deletions":2,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -382,0 +382,7 @@\n+    public List<DocPath> getAdditionalScripts() {\n+        return options.additionalScripts().stream()\n+                .map(sf -> DocFile.createFileForInput(this, sf))\n+                .map(file -> DocPath.create(file.getName()))\n+                .collect(Collectors.toCollection(ArrayList::new));\n+    }\n+\n","filename":"src\/jdk.javadoc\/share\/classes\/jdk\/javadoc\/internal\/doclets\/formats\/html\/HtmlConfiguration.java","additions":7,"deletions":0,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -29,2 +29,0 @@\n-import java.io.OutputStream;\n-import java.io.Writer;\n@@ -218,1 +216,1 @@\n-            SourceToHTMLConverter.convertRoot(configuration,DocPaths.SOURCE_OUTPUT);\n+            SourceToHTMLConverter.convertRoot(configuration, DocPaths.SOURCE_OUTPUT);\n@@ -228,2 +226,2 @@\n-        performCopy(options.helpFile());\n-        performCopy(options.stylesheetFile());\n+        performCopy(options.helpFile(), DocPath.empty);\n+        performCopy(options.stylesheetFile(), DocPath.empty);\n@@ -231,1 +229,4 @@\n-            performCopy(stylesheet);\n+            performCopy(stylesheet, DocPath.empty);\n+        }\n+        for (String script : options.additionalScripts()) {\n+            performCopy(script, DocPaths.SCRIPT_DIR);\n@@ -332,1 +333,1 @@\n-            DocPath filePath = DocPaths.JQUERY_FILES.resolve(file);\n+            DocPath filePath = DocPaths.SCRIPT_DIR.resolve(file);\n@@ -431,2 +432,2 @@\n-    private void performCopy(String filename) throws DocFileIOException {\n-        if (filename.isEmpty())\n+    private void performCopy(String filename, DocPath targetPath) throws DocFileIOException {\n+        if (filename.isEmpty()) {\n@@ -434,0 +435,1 @@\n+        }\n@@ -436,1 +438,1 @@\n-        DocPath path = DocPath.create(fromfile.getName());\n+        DocPath path = targetPath.resolve(fromfile.getName());\n@@ -438,1 +440,1 @@\n-        if (toFile.isSameFile(fromfile))\n+        if (toFile.isSameFile(fromfile)) {\n@@ -440,0 +442,1 @@\n+        }\n@@ -442,1 +445,1 @@\n-                fromfile.toString(), path.getPath());\n+                fromfile.getPath(), path.getPath());\n","filename":"src\/jdk.javadoc\/share\/classes\/jdk\/javadoc\/internal\/doclets\/formats\/html\/HtmlDoclet.java","additions":15,"deletions":12,"binary":false,"changes":27,"status":"modified"},{"patch":"@@ -462,0 +462,1 @@\n+                .setAdditionalScripts(configuration.getAdditionalScripts())\n","filename":"src\/jdk.javadoc\/share\/classes\/jdk\/javadoc\/internal\/doclets\/formats\/html\/HtmlDocletWriter.java","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1998, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1998, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -62,0 +62,5 @@\n+    \/**\n+     * Argument for command-line option {@code --add-script}.\n+     *\/\n+    private List<String> additionalScripts = new ArrayList<>();\n+\n@@ -202,0 +207,8 @@\n+                new Option(resources, \"--add-script\", 1) {\n+                    @Override\n+                    public boolean process(String opt, List<String> args) {\n+                        additionalScripts.add(args.get(0));\n+                        return true;\n+                    }\n+                },\n+\n@@ -503,1 +516,8 @@\n-\n+        \/\/ check if additional scripts exists\n+        for (String script : additionalScripts) {\n+            DocFile sfile = DocFile.createFileForInput(config, script);\n+            if (!sfile.exists()) {\n+                messages.error(\"doclet.File_not_found\", script);\n+                return false;\n+            }\n+        }\n@@ -517,0 +537,7 @@\n+    \/**\n+     * Argument for command-line option {@code --add-script}.\n+     *\/\n+    List<String> additionalScripts() {\n+        return additionalScripts;\n+    }\n+\n","filename":"src\/jdk.javadoc\/share\/classes\/jdk\/javadoc\/internal\/doclets\/formats\/html\/HtmlOptions.java","additions":29,"deletions":2,"binary":false,"changes":31,"status":"modified"},{"patch":"@@ -766,3 +766,5 @@\n-                desc.add((description != null && !description.isEmpty())\n-                        ? HtmlTree.DIV(HtmlStyle.block, description)\n-                        : Entity.NO_BREAK_SPACE);\n+                if (description != null && !description.isEmpty()) {\n+                    desc.add(HtmlTree.DIV(HtmlStyle.block, description));\n+                } else {\n+                    addSummaryComment(srv, desc);\n+                }\n@@ -771,1 +773,1 @@\n-                }\n+            }\n","filename":"src\/jdk.javadoc\/share\/classes\/jdk\/javadoc\/internal\/doclets\/formats\/html\/ModuleWriterImpl.java","additions":6,"deletions":4,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2003, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2003, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -66,0 +66,2 @@\n+    \/\/ Scripts added via --add-script option\n+    private List<DocPath> additionalScripts = Collections.emptyList();\n@@ -171,0 +173,13 @@\n+    \/**\n+     * Sets the list of additional script files to be added to the HEAD element.\n+     * The path for the script files must be relative to the root of the generated\n+     * documentation hierarchy.\n+     *\n+     * @param scripts the list of additional script files\n+     * @return this object\n+     *\/\n+    public Head setAdditionalScripts(List<DocPath> scripts) {\n+        this.additionalScripts = scripts;\n+        return this;\n+    }\n+\n@@ -318,1 +333,1 @@\n-            addStylesheet(tree, DocPaths.JQUERY_FILES.resolve(DocPaths.JQUERY_UI_CSS));\n+            addStylesheet(tree, DocPaths.SCRIPT_DIR.resolve(DocPaths.JQUERY_UI_CSS));\n@@ -340,2 +355,5 @@\n-            addJQueryFile(tree, DocPaths.JQUERY_JS);\n-            addJQueryFile(tree, DocPaths.JQUERY_UI_JS);\n+            addScriptElement(tree, DocPaths.JQUERY_JS);\n+            addScriptElement(tree, DocPaths.JQUERY_UI_JS);\n+        }\n+        for (DocPath path : additionalScripts) {\n+            addScriptElement(tree, path);\n@@ -348,3 +366,3 @@\n-    private void addJQueryFile(HtmlTree tree, DocPath filePath) {\n-        DocPath jqueryFile = pathToRoot.resolve(DocPaths.JQUERY_FILES.resolve(filePath));\n-        tree.add(HtmlTree.SCRIPT(jqueryFile.getPath()));\n+    private void addScriptElement(HtmlTree tree, DocPath filePath) {\n+        DocPath scriptFile = pathToRoot.resolve(DocPaths.SCRIPT_DIR).resolve(filePath);\n+        tree.add(HtmlTree.SCRIPT(scriptFile.getPath()));\n","filename":"src\/jdk.javadoc\/share\/classes\/jdk\/javadoc\/internal\/doclets\/formats\/html\/markup\/Head.java","additions":25,"deletions":7,"binary":false,"changes":32,"status":"modified"},{"patch":"@@ -384,0 +384,4 @@\n+doclet.usage.add-script.parameters=\\\n+    <file>\n+doclet.usage.add-script.description=\\\n+    Add a script file to the generated documentation\n@@ -387,1 +391,1 @@\n-    Additional stylesheet file for the generated documentation\n+    Add a stylesheet file to the generated documentation\n","filename":"src\/jdk.javadoc\/share\/classes\/jdk\/javadoc\/internal\/doclets\/formats\/html\/resources\/standard.properties","additions":5,"deletions":1,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -106,3 +106,0 @@\n-    \/** The name of the directory for the jQuery files. *\/\n-    public static final DocPath JQUERY_FILES = DocPath.create(\"script-dir\");\n-\n@@ -127,0 +124,3 @@\n+    \/** The name of the file for new elements. *\/\n+    public static final DocPath NEW_LIST = DocPath.create(\"new-list.html\");\n+\n@@ -151,2 +151,2 @@\n-    \/** The name of the file for new elements. *\/\n-    public static final DocPath NEW_LIST = DocPath.create(\"new-list.html\");\n+    \/** The name of the directory for the script files. *\/\n+    public static final DocPath SCRIPT_DIR = DocPath.create(\"script-dir\");\n","filename":"src\/jdk.javadoc\/share\/classes\/jdk\/javadoc\/internal\/doclets\/toolkit\/util\/DocPaths.java","additions":5,"deletions":5,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -34,1 +34,0 @@\n-import com.sun.tools.attach.VirtualMachineDescriptor;\n@@ -174,2 +173,1 @@\n-        for (int i = 0; i < subopts.length; i++) {\n-            String subopt = subopts[i];\n+        for (String subopt : subopts) {\n@@ -187,2 +185,2 @@\n-               parallel = subopt.substring(\"parallel=\".length());\n-               if (parallel == null) {\n+                parallel = subopt.substring(\"parallel=\".length());\n+                if (parallel == null) {\n@@ -191,1 +189,1 @@\n-               }\n+                }\n@@ -212,2 +210,1 @@\n-        for (int i = 0; i < subopts.length; i++) {\n-            String subopt = subopts[i];\n+        for (String subopt : subopts) {\n","filename":"src\/jdk.jcmd\/share\/classes\/sun\/tools\/jmap\/JMap.java","additions":5,"deletions":8,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2004, 2018, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2004, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -67,3 +67,3 @@\n-            if ((args[0].compareTo(\"-?\") == 0)\n-                || (args[0].compareTo(\"-h\")== 0)\n-                || (args[0].compareTo(\"--help\")== 0)\n+            if ((args[0].equals(\"-?\"))\n+                || (args[0].equals(\"-h\"))\n+                || (args[0].equals(\"--help\"))\n@@ -71,1 +71,1 @@\n-                || (args[0].compareTo(\"-help\")== 0)) {\n+                || (args[0].equals(\"-help\"))) {\n@@ -81,1 +81,1 @@\n-            if (arg.compareTo(\"-q\") == 0) {\n+            if (arg.equals(\"-q\")) {\n","filename":"src\/jdk.jcmd\/share\/classes\/sun\/tools\/jps\/Arguments.java","additions":6,"deletions":6,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2004, 2018, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2004, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -118,2 +118,2 @@\n-        for (int i = 0; i < unitStrings.length; i++) {\n-            int index = s.indexOf(unitStrings[i]);\n+        for (String unit : unitStrings) {\n+            int index = s.indexOf(unit);\n@@ -130,1 +130,1 @@\n-            if (unitString == null || unitString.compareTo(\"ms\") == 0) {\n+            if (unitString == null || unitString.equals(\"ms\")) {\n@@ -132,1 +132,1 @@\n-            } else if (unitString.compareTo(\"s\") == 0) {\n+            } else if (unitString.equals(\"s\")) {\n@@ -152,3 +152,3 @@\n-        if ((args[0].compareTo(\"-?\") == 0)\n-                || (args[0].compareTo(\"-h\") == 0)\n-                || (args[0].compareTo(\"--help\") == 0)\n+        if ((args[0].equals(\"-?\"))\n+                || (args[0].equals(\"-h\"))\n+                || (args[0].equals(\"--help\"))\n@@ -156,1 +156,1 @@\n-                || (args[0].compareTo(\"-help\") == 0)) {\n+                || (args[0].equals(\"-help\"))) {\n@@ -159,1 +159,1 @@\n-        } else if (args[0].compareTo(\"-options\") == 0) {\n+        } else if (args[0].equals(\"-options\")) {\n@@ -162,1 +162,1 @@\n-        } else if (args[0].compareTo(\"-list\") == 0) {\n+        } else if (args[0].equals(\"-list\")) {\n@@ -174,1 +174,1 @@\n-            if (arg.compareTo(\"-a\") == 0) {\n+            if (arg.equals(\"-a\")) {\n@@ -176,1 +176,1 @@\n-            } else if (arg.compareTo(\"-d\") == 0) {\n+            } else if (arg.equals(\"-d\")) {\n@@ -178,1 +178,1 @@\n-            } else if (arg.compareTo(\"-t\") == 0) {\n+            } else if (arg.equals(\"-t\")) {\n@@ -180,1 +180,1 @@\n-            } else if (arg.compareTo(\"-v\") == 0) {\n+            } else if (arg.equals(\"-v\")) {\n@@ -182,2 +182,2 @@\n-            } else if ((arg.compareTo(\"-constants\") == 0)\n-                       || (arg.compareTo(\"-c\") == 0)) {\n+            } else if ((arg.equals(\"-constants\"))\n+                       || (arg.equals(\"-c\"))) {\n@@ -185,2 +185,2 @@\n-            } else if ((arg.compareTo(\"-strings\") == 0)\n-                       || (arg.compareTo(\"-s\") == 0)) {\n+            } else if ((arg.equals(\"-strings\"))\n+                       || (arg.equals(\"-s\"))) {\n@@ -190,1 +190,1 @@\n-                if (arg.compareTo(\"-h\") != 0) {\n+                if (!arg.equals(\"-h\")) {\n@@ -248,1 +248,1 @@\n-                    if ((argc == 0) && (args[argc].compareTo(\"-snap\") == 0)) {\n+                    if ((argc == 0) && (args[argc].equals(\"-snap\"))) {\n","filename":"src\/jdk.jcmd\/share\/classes\/sun\/tools\/jstat\/Arguments.java","additions":21,"deletions":21,"binary":false,"changes":42,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2004, 2018, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2004, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -28,2 +28,0 @@\n-import java.util.*;\n-\n@@ -159,3 +157,2 @@\n-        for (Iterator<OptionFormat> i = children.iterator();  i.hasNext(); \/* empty *\/) {\n-            OptionFormat of = i.next();\n-            of.printFormat(indentLevel+1);\n+        for (OptionFormat of : children) {\n+            of.printFormat(indentLevel + 1);\n","filename":"src\/jdk.jcmd\/share\/classes\/sun\/tools\/jstat\/ColumnFormat.java","additions":3,"deletions":6,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2004, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2004, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -80,4 +80,4 @@\n-      for (Iterator<OptionFormat> i = children.iterator(); i.hasNext(); \/* empty *\/) {\n-          OptionFormat o = i.next();\n-          c.visit(o, i.hasNext());\n-      }\n+        for (Iterator<OptionFormat> i = children.iterator(); i.hasNext(); \/* empty *\/) {\n+            OptionFormat o = i.next();\n+            c.visit(o, i.hasNext());\n+        }\n@@ -85,4 +85,3 @@\n-      for (Iterator <OptionFormat>i = children.iterator(); i.hasNext(); \/* empty *\/) {\n-          OptionFormat o = i.next();\n-          o.apply(c);\n-      }\n+        for (OptionFormat o : children) {\n+            o.apply(c);\n+        }\n","filename":"src\/jdk.jcmd\/share\/classes\/sun\/tools\/jstat\/OptionFormat.java","additions":8,"deletions":9,"binary":false,"changes":17,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2004, 2010, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2004, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -76,1 +76,1 @@\n-            if (of.getName().compareTo(\"timestamp\") == 0) {\n+            if (of.getName().equals(\"timestamp\")) {\n","filename":"src\/jdk.jcmd\/share\/classes\/sun\/tools\/jstat\/OptionLister.java","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2004, 2018, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2004, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -83,1 +83,1 @@\n-    private static Set<String> reservedWords;\n+    private static final Set<String> reservedWords = Set.of(otherKeyWords);\n@@ -106,3 +106,2 @@\n-        reservedWords = new HashSet<String>();\n-        for (int i = 0; i < otherKeyWords.length; i++) {\n-            reservedWords.add(otherKeyWords[i]);\n+        for (char delimiter : delimiters) {\n+            st.ordinaryChar(delimiter);\n@@ -111,6 +110,2 @@\n-        for (int i = 0; i < delimiters.length; i++ ) {\n-            st.ordinaryChar(delimiters[i]);\n-        }\n-\n-        for (int i = 0; i < infixOps.length; i++ ) {\n-            st.ordinaryChar(infixOps[i]);\n+        for (char infixOp : infixOps) {\n+            st.ordinaryChar(infixOp);\n@@ -234,2 +229,2 @@\n-        for (int i = 0; i < infixOps.length; i++) {\n-            if (op == infixOps[i]) {\n+        for (char infixOp : infixOps) {\n+            if (op == infixOp) {\n@@ -476,1 +471,1 @@\n-            if (lookahead.sval.compareTo(DATA) == 0) {\n+            if (lookahead.sval.equals(DATA)) {\n@@ -478,1 +473,1 @@\n-            } else if (lookahead.sval.compareTo(HEADER) == 0) {\n+            } else if (lookahead.sval.equals(HEADER)) {\n@@ -480,1 +475,1 @@\n-            } else if (lookahead.sval.compareTo(WIDTH) == 0) {\n+            } else if (lookahead.sval.equals(WIDTH)) {\n@@ -482,1 +477,1 @@\n-            } else if (lookahead.sval.compareTo(FORMAT) == 0) {\n+            } else if (lookahead.sval.equals(FORMAT)) {\n@@ -484,1 +479,1 @@\n-            } else if (lookahead.sval.compareTo(ALIGN) == 0) {\n+            } else if (lookahead.sval.equals(ALIGN)) {\n@@ -486,1 +481,1 @@\n-            } else if (lookahead.sval.compareTo(SCALE) == 0) {\n+            } else if (lookahead.sval.equals(SCALE)) {\n@@ -488,1 +483,1 @@\n-            } else if (lookahead.sval.compareTo(REQUIRED) == 0) {\n+            } else if (lookahead.sval.equals(REQUIRED)) {\n@@ -547,1 +542,1 @@\n-                    || (lookahead.sval.compareTo(START) != 0)) {\n+                    || (!lookahead.sval.equals(START))) {\n@@ -577,1 +572,1 @@\n-                    || (lookahead.sval.compareTo(START) != 0)) {\n+                    || (!lookahead.sval.equals(START))) {\n","filename":"src\/jdk.jcmd\/share\/classes\/sun\/tools\/jstat\/Parser.java","additions":17,"deletions":22,"binary":false,"changes":39,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2004, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2004, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -51,2 +51,1 @@\n-            for (Iterator<Monitor> i = logged.iterator(); i.hasNext(); \/* empty *\/ ) {\n-                Monitor m = i.next();\n+            for (Monitor m : logged) {\n@@ -63,2 +62,1 @@\n-        for (Iterator<Monitor> i = logged.iterator(); i.hasNext(); \/* empty *\/ ) {\n-            Monitor m = i.next();\n+        for (Monitor m : logged) {\n","filename":"src\/jdk.jcmd\/share\/classes\/sun\/tools\/jstat\/RawOutputFormatter.java","additions":3,"deletions":5,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -1,1 +1,1 @@\n-.\\\" Copyright (c) 2012, 2020, Oracle and\/or its affiliates. All rights reserved.\n+.\\\" Copyright (c) 2012, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -182,0 +182,9 @@\n+.B \\f[CB]Compiler.perfmap\\f[R] (Linux only)\n+Write map file for Linux perf tool.\n+.RS\n+.PP\n+Impact: Low\n+.PP\n+Permission: \\f[CB]java.lang.management.ManagementPermission(monitor)\\f[R]\n+.RE\n+.TP\n@@ -249,0 +258,8 @@\n+.IP \\[bu] 2\n+\\f[CB]\\-parallel\\f[R]: (Optional) Number of parallel threads to use for\n+heap inspection.\n+0 (the default) means let the VM determine the number of threads to use.\n+1 means use one thread (disable parallelism).\n+For any other value the VM will try to use the specified number of\n+threads, but might use fewer.\n+(INT, 0)\n@@ -278,0 +295,8 @@\n+.IP \\[bu] 2\n+\\f[CB]\\-gz\\f[R]: (Optional) If specified, the heap dump is written in\n+gzipped format using the given compression level.\n+1 (recommended) is the fastest, 9 the strongest compression.\n+(INT, 1)\n+.IP \\[bu] 2\n+\\f[CB]\\-overwrite\\f[R]: (Optional) If specified, the dump file will be\n+overwritten if it exists (BOOLEAN, false)\n@@ -444,2 +469,0 @@\n-If \\f[CB]%p\\f[R] and\/or \\f[CB]%t\\f[R] is specified in the filename, it\n-expands to the JVM\\[aq]s PID and the current timestamp, respectively.\n@@ -514,2 +537,0 @@\n-If \\f[CB]%p\\f[R] and\/or \\f[CB]%t\\f[R] is specified in the filename, it\n-expands to the JVM\\[aq]s PID and the current timestamp, respectively.\n@@ -607,2 +628,0 @@\n-If \\f[CB]%p\\f[R] and\/or \\f[CB]%t\\f[R] is specified in the filename, it\n-expands to the JVM\\[aq]s PID and the current timestamp, respectively.\n@@ -754,0 +773,9 @@\n+.B \\f[CB]System.trim_native_heap\\f[R] (Linux only)\n+Attempts to free up memory by trimming the C\\-heap.\n+.RS\n+.PP\n+Impact: Low\n+.PP\n+Permission: \\f[CB]java.lang.management.ManagementPermission(control)\\f[R]\n+.RE\n+.TP\n@@ -769,0 +797,3 @@\n+\\f[CB]\\-e\\f[R]: (Optional) Print extended thread information (BOOLEAN,\n+false)\n+.IP \\[bu] 2\n@@ -773,0 +804,42 @@\n+.B \\f[CB]VM.cds\\f[R] [\\f[I]arguments\\f[R]]\n+Dump a static or dynamic shared archive including all shareable classes.\n+.RS\n+.PP\n+Impact: Medium \\-\\-\\- pause time depends on number of loaded classes\n+.PP\n+Permission: \\f[CB]java.lang.management.ManagementPermission(monitor)\\f[R]\n+.PP\n+\\f[I]arguments\\f[R]:\n+.IP \\[bu] 2\n+\\f[CB]subcmd\\f[R]: \\f[CB]static_dump\\f[R] | \\f[CB]dynamic_dump\\f[R] (STRING,\n+no default value)\n+.IP \\[bu] 2\n+\\f[CB]filename\\f[R]: (Optional) Name of the shared archive to be dumped\n+(STRING, no default value)\n+.RE\n+.TP\n+.B \\f[CB]VM.classloaders\\f[R] [\\f[I]options\\f[R]]\n+Prints classloader hierarchy.\n+.RS\n+.PP\n+Impact: Medium \\-\\-\\- Depends on number of class loaders and classes\n+loaded.\n+.PP\n+Permission: \\f[CB]java.lang.management.ManagementPermission(monitor)\\f[R]\n+.PP\n+The following \\f[I]options\\f[R] must be specified using either\n+\\f[I]key\\f[R] or \\f[I]key\\f[R]\\f[CB]=\\f[R]\\f[I]value\\f[R] syntax.\n+.PP\n+\\f[I]options\\f[R]:\n+.IP \\[bu] 2\n+\\f[CB]show\\-classes\\f[R]: (Optional) Print loaded classes.\n+(BOOLEAN, false)\n+.IP \\[bu] 2\n+\\f[CB]verbose\\f[R]: (Optional) Print detailed information.\n+(BOOLEAN, false)\n+.IP \\[bu] 2\n+\\f[CB]fold\\f[R]: (Optional) Show loaders of the same name and class as\n+one.\n+(BOOLEAN, true)\n+.RE\n+.TP\n@@ -834,0 +907,25 @@\n+.B \\f[CB]VM.events\\f[R] [\\f[I]options\\f[R]]\n+Print VM event logs\n+.RS\n+.PP\n+Impact: Low \\-\\-\\- Depends on event log size.\n+.PP\n+Permission: \\f[CB]java.lang.management.ManagementPermission(monitor)\\f[R]\n+.PP\n+\\f[I]options\\f[R]:\n+.PP\n+\\f[B]Note:\\f[R]\n+.PP\n+The following \\f[I]options\\f[R] must be specified using either\n+\\f[I]key\\f[R] or \\f[I]key\\f[R]\\f[CB]=\\f[R]\\f[I]value\\f[R] syntax.\n+.IP \\[bu] 2\n+\\f[CB]log\\f[R]: (Optional) Name of log to be printed.\n+If omitted, all logs are printed.\n+(STRING, no default value)\n+.IP \\[bu] 2\n+\\f[CB]max\\f[R]: (Optional) Maximum number of events to be printed (newest\n+first).\n+If omitted, all events are printed.\n+(STRING, no default value)\n+.RE\n+.TP\n@@ -903,0 +1001,42 @@\n+.B \\f[CB]VM.metaspace\\f[R] [\\f[I]options\\f[R]]\n+Prints the statistics for the metaspace\n+.RS\n+.PP\n+Impact: Medium \\-\\-\\- Depends on number of classes loaded.\n+.PP\n+Permission: \\f[CB]java.lang.management.ManagementPermission(monitor)\\f[R]\n+.PP\n+\\f[B]Note:\\f[R]\n+.PP\n+The following \\f[I]options\\f[R] must be specified using either\n+\\f[I]key\\f[R] or \\f[I]key\\f[R]\\f[CB]=\\f[R]\\f[I]value\\f[R] syntax.\n+.PP\n+\\f[I]options\\f[R]:\n+.IP \\[bu] 2\n+\\f[CB]basic\\f[R]: (Optional) Prints a basic summary (does not need a\n+safepoint).\n+(BOOLEAN, false)\n+.IP \\[bu] 2\n+\\f[CB]show\\-loaders\\f[R]: (Optional) Shows usage by class loader.\n+(BOOLEAN, false)\n+.IP \\[bu] 2\n+\\f[CB]show\\-classes\\f[R]: (Optional) If show\\-loaders is set, shows loaded\n+classes for each loader.\n+(BOOLEAN, false)\n+.IP \\[bu] 2\n+\\f[CB]by\\-chunktype\\f[R]: (Optional) Break down numbers by chunk type.\n+(BOOLEAN, false)\n+.IP \\[bu] 2\n+\\f[CB]by\\-spacetype\\f[R]: (Optional) Break down numbers by loader type.\n+(BOOLEAN, false)\n+.IP \\[bu] 2\n+\\f[CB]vslist\\f[R]: (Optional) Shows details about the underlying virtual\n+space.\n+(BOOLEAN, false)\n+.IP \\[bu] 2\n+\\f[CB]scale\\f[R]: (Optional) Memory usage in which to scale.\n+Valid values are: 1, KB, MB or GB (fixed scale) or \"dynamic\" for a\n+dynamically chosen scale.\n+(STRING, dynamic)\n+.RE\n+.TP\n","filename":"src\/jdk.jcmd\/share\/man\/jcmd.1","additions":147,"deletions":7,"binary":false,"changes":154,"status":"modified"},{"patch":"@@ -53,1 +53,1 @@\n-    abstract class AsyncExecution {\n+    abstract static class AsyncExecution {\n@@ -711,1 +711,1 @@\n-        if ((val != null) && (val instanceof ObjectReference)) {\n+        if (val instanceof ObjectReference object) {\n@@ -713,1 +713,1 @@\n-                thread.stop((ObjectReference)val);\n+                thread.stop(object);\n@@ -1807,2 +1807,1 @@\n-            if ((val != null) && (val instanceof ObjectReference)) {\n-                ObjectReference object = (ObjectReference)val;\n+            if (val instanceof ObjectReference object) {\n@@ -1903,2 +1902,1 @@\n-        if ((val != null) && (val instanceof ObjectReference)) {\n-            ObjectReference object = (ObjectReference)val;\n+        if (val instanceof ObjectReference object) {\n@@ -1932,2 +1930,1 @@\n-        if ((val != null) && (val instanceof ObjectReference)) {\n-            ObjectReference object = (ObjectReference)val;\n+        if (val instanceof ObjectReference object) {\n","filename":"src\/jdk.jdi\/share\/classes\/com\/sun\/tools\/example\/debug\/tty\/Commands.java","additions":6,"deletions":9,"binary":false,"changes":15,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1998, 2017, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1998, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -43,2 +43,2 @@\n-        if ((obj != null) && (obj instanceof BooleanValue)) {\n-            return (value == ((BooleanValue)obj).value()) &&\n+        if (obj instanceof BooleanValue other) {\n+            return (value == other.value()) &&\n","filename":"src\/jdk.jdi\/share\/classes\/com\/sun\/tools\/jdi\/BooleanValueImpl.java","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1998, 2017, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1998, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -44,2 +44,2 @@\n-        if ((obj != null) && (obj instanceof ByteValue)) {\n-            return (value == ((ByteValue)obj).value())\n+        if (obj instanceof ByteValue other) {\n+            return (value == other.value())\n","filename":"src\/jdk.jdi\/share\/classes\/com\/sun\/tools\/jdi\/ByteValueImpl.java","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1998, 2017, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1998, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -44,2 +44,2 @@\n-        if ((obj != null) && (obj instanceof CharValue)) {\n-            return (value == ((CharValue)obj).value()) &&\n+        if (obj instanceof CharValue other) {\n+            return (value == other.value()) &&\n","filename":"src\/jdk.jdi\/share\/classes\/com\/sun\/tools\/jdi\/CharValueImpl.java","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -149,1 +149,1 @@\n-    abstract class ArgumentImpl implements Connector.Argument, Cloneable {\n+    abstract static class ArgumentImpl implements Connector.Argument, Cloneable {\n@@ -195,2 +195,1 @@\n-            if ((obj != null) && (obj instanceof Connector.Argument)) {\n-                Connector.Argument other = (Connector.Argument)obj;\n+            if (obj instanceof Argument other) {\n@@ -279,1 +278,1 @@\n-    class IntegerArgumentImpl extends ConnectorImpl.ArgumentImpl\n+    static class IntegerArgumentImpl extends ConnectorImpl.ArgumentImpl\n@@ -380,1 +379,1 @@\n-    class StringArgumentImpl extends ConnectorImpl.ArgumentImpl\n+    static class StringArgumentImpl extends ConnectorImpl.ArgumentImpl\n@@ -397,1 +396,1 @@\n-    class SelectedArgumentImpl extends ConnectorImpl.ArgumentImpl\n+    static class SelectedArgumentImpl extends ConnectorImpl.ArgumentImpl\n","filename":"src\/jdk.jdi\/share\/classes\/com\/sun\/tools\/jdi\/ConnectorImpl.java","additions":5,"deletions":6,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1998, 2017, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1998, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -43,2 +43,2 @@\n-        if ((obj != null) && (obj instanceof DoubleValue)) {\n-            return (value == ((DoubleValue)obj).value()) &&\n+        if (obj instanceof DoubleValue other) {\n+            return (value == other.value()) &&\n","filename":"src\/jdk.jdi\/share\/classes\/com\/sun\/tools\/jdi\/DoubleValueImpl.java","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -44,2 +44,1 @@\n-        if ((obj != null) && (obj instanceof FieldImpl)) {\n-            FieldImpl other = (FieldImpl)obj;\n+        if (obj instanceof FieldImpl other) {\n","filename":"src\/jdk.jdi\/share\/classes\/com\/sun\/tools\/jdi\/FieldImpl.java","additions":1,"deletions":2,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1998, 2017, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1998, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -43,2 +43,2 @@\n-        if ((obj != null) && (obj instanceof FloatValue)) {\n-            return (value == ((FloatValue)obj).value()) &&\n+        if (obj instanceof FloatValue other) {\n+            return (value == other.value()) &&\n","filename":"src\/jdk.jdi\/share\/classes\/com\/sun\/tools\/jdi\/FloatValueImpl.java","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1998, 2011, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1998, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -43,2 +43,2 @@\n-        if ((obj != null) && (obj instanceof IntegerValue)) {\n-            return (value == ((IntegerValue)obj).value()) &&\n+        if (obj instanceof IntegerValue other) {\n+            return (value == other.value()) &&\n","filename":"src\/jdk.jdi\/share\/classes\/com\/sun\/tools\/jdi\/IntegerValueImpl.java","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1998, 2017, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1998, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -69,2 +69,1 @@\n-        if ((obj != null) && (obj instanceof LocalVariableImpl)) {\n-            LocalVariableImpl other = (LocalVariableImpl)obj;\n+        if (obj instanceof LocalVariableImpl other) {\n","filename":"src\/jdk.jdi\/share\/classes\/com\/sun\/tools\/jdi\/LocalVariableImpl.java","additions":2,"deletions":3,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1998, 2018, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1998, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -65,2 +65,1 @@\n-        if ((obj != null) && (obj instanceof Location)) {\n-            Location other = (Location)obj;\n+        if (obj instanceof Location other) {\n","filename":"src\/jdk.jdi\/share\/classes\/com\/sun\/tools\/jdi\/LocationImpl.java","additions":2,"deletions":3,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1998, 2017, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1998, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -43,2 +43,2 @@\n-        if ((obj != null) && (obj instanceof LongValue)) {\n-            return (value == ((LongValue)obj).value()) &&\n+        if (obj instanceof LongValue other) {\n+            return (value == other.value()) &&\n","filename":"src\/jdk.jdi\/share\/classes\/com\/sun\/tools\/jdi\/LongValueImpl.java","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -88,2 +88,1 @@\n-        if ((obj != null) && (obj instanceof MethodImpl)) {\n-            MethodImpl other = (MethodImpl)obj;\n+        if (obj instanceof MethodImpl other) {\n","filename":"src\/jdk.jdi\/share\/classes\/com\/sun\/tools\/jdi\/MethodImpl.java","additions":1,"deletions":2,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1998, 2018, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1998, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -52,2 +52,1 @@\n-        if ((obj != null) && (obj instanceof Mirror)) {\n-            Mirror other = (Mirror)obj;\n+        if (obj instanceof Mirror other) {\n","filename":"src\/jdk.jdi\/share\/classes\/com\/sun\/tools\/jdi\/MirrorImpl.java","additions":2,"deletions":3,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -149,2 +149,1 @@\n-        if ((obj != null) && (obj instanceof ObjectReferenceImpl)) {\n-            ObjectReferenceImpl other = (ObjectReferenceImpl)obj;\n+        if (obj instanceof ObjectReferenceImpl other) {\n","filename":"src\/jdk.jdi\/share\/classes\/com\/sun\/tools\/jdi\/ObjectReferenceImpl.java","additions":1,"deletions":2,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -144,2 +144,1 @@\n-        if ((obj != null) && (obj instanceof ReferenceTypeImpl)) {\n-            ReferenceTypeImpl other = (ReferenceTypeImpl)obj;\n+        if (obj instanceof ReferenceTypeImpl other) {\n","filename":"src\/jdk.jdi\/share\/classes\/com\/sun\/tools\/jdi\/ReferenceTypeImpl.java","additions":1,"deletions":2,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2001, 2017, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2001, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -42,1 +42,1 @@\n-    private class FileTableRecord {\n+    private static class FileTableRecord {\n@@ -75,1 +75,1 @@\n-    private class LineTableRecord {\n+    private static class LineTableRecord {\n@@ -85,1 +85,1 @@\n-    private class StratumTableRecord {\n+    private static class StratumTableRecord {\n","filename":"src\/jdk.jdi\/share\/classes\/com\/sun\/tools\/jdi\/SDE.java","additions":4,"deletions":4,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1998, 2017, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1998, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -43,2 +43,2 @@\n-        if ((obj != null) && (obj instanceof ShortValue)) {\n-            return (value == ((ShortValue)obj).value()) &&\n+        if (obj instanceof ShortValue other) {\n+            return (value == other.value()) &&\n","filename":"src\/jdk.jdi\/share\/classes\/com\/sun\/tools\/jdi\/ShortValueImpl.java","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1998, 2017, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1998, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -114,2 +114,1 @@\n-        if ((obj != null) && (obj instanceof StackFrameImpl)) {\n-            StackFrameImpl other = (StackFrameImpl)obj;\n+        if (obj instanceof StackFrameImpl other) {\n","filename":"src\/jdk.jdi\/share\/classes\/com\/sun\/tools\/jdi\/StackFrameImpl.java","additions":2,"deletions":3,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1998, 2017, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1998, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -49,2 +49,1 @@\n-        if ((obj != null) && (obj instanceof Type)) {\n-            Type other = (Type)obj;\n+        if (obj instanceof Type other) {\n","filename":"src\/jdk.jdi\/share\/classes\/com\/sun\/tools\/jdi\/TypeImpl.java","additions":2,"deletions":3,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1998, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1998, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -40,1 +40,1 @@\n-        return (obj != null) && (obj instanceof VoidValue) && super.equals(obj);\n+        return (obj instanceof VoidValue) && super.equals(obj);\n","filename":"src\/jdk.jdi\/share\/classes\/com\/sun\/tools\/jdi\/VoidValueImpl.java","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -76,0 +76,1 @@\n+    unsigned int handlingAppResume : 1;\n@@ -682,1 +683,4 @@\n-                if (!(state & JVMTI_THREAD_STATE_SUSPENDED)) {\n+                \/* !node->handlingAppResume && resumeFrameDepth > 0\n+                 * means the thread has entered Thread.resume() *\/\n+                if (!(state & JVMTI_THREAD_STATE_SUSPENDED) &&\n+                    !node->handlingAppResume) {\n@@ -755,0 +759,5 @@\n+\/*\n+ * The caller is expected to hold threadLock and handlerLock.\n+ * eventHandler_createInternalThreadOnly() can deadlock because of\n+ * wrong lock ordering if the caller does not hold handlerLock.\n+ *\/\n@@ -803,1 +812,0 @@\n-    jthread resumee = getResumee(resumer);\n@@ -806,7 +814,0 @@\n-    if (resumee != NULL) {\n-        \/*\n-         * Hold up any attempt to resume as long as the debugger\n-         * has suspended the resumee.\n-         *\/\n-        blockOnDebuggerSuspend(resumee);\n-    }\n@@ -814,0 +815,5 @@\n+    \/*\n+     * Actual handling has to be deferred. We cannot block right here if the\n+     * target of the resume call is suspended by the debugger since we are\n+     * holding handlerLock which must not be released. See doPendingTasks().\n+     *\/\n@@ -815,10 +821,4 @@\n-        \/*\n-         * Track the resuming thread by marking it as being within\n-         * a resume and by setting up for notification on\n-         * a frame pop or exception. We won't allow the debugger\n-         * to suspend threads while any thread is within a\n-         * call to resume. This (along with the block above)\n-         * ensures that when the debugger\n-         * suspends a thread it will remain suspended.\n-         *\/\n-        trackAppResume(resumer);\n+        ThreadNode* node = findThread(&runningThreads, resumer);\n+        if (node != NULL) {\n+            node->handlingAppResume = JNI_TRUE;\n+        }\n@@ -2182,0 +2182,53 @@\n+    \/* Deferred breakpoint handling for Thread.resume() *\/\n+    if (node->handlingAppResume) {\n+        jthread resumer = node->thread;\n+        jthread resumee = getResumee(resumer);\n+\n+        if (resumer != NULL) {\n+            \/*\n+             * trackAppResume indirectly aquires handlerLock. For proper lock\n+             * ordering handlerLock has to be acquired before threadLock.\n+             *\/\n+            debugMonitorExit(threadLock);\n+            eventHandler_lock();\n+            debugMonitorEnter(threadLock);\n+\n+            \/*\n+             * Track the resuming thread by marking it as being within\n+             * a resume and by setting up for notification on\n+             * a frame pop or exception. We won't allow the debugger\n+             * to suspend threads while any thread is within a\n+             * call to resume. This (along with the block below)\n+             * ensures that when the debugger\n+             * suspends a thread it will remain suspended.\n+             *\/\n+            trackAppResume(resumer);\n+\n+            \/*\n+             * handlerLock is not needed anymore. We must release it before calling\n+             * blockOnDebuggerSuspend() because it is required for resumes done by\n+             * the debugger. If resumee is currently suspended by the debugger, then\n+             * blockOnDebuggerSuspend() will block until a debugger resume is done.\n+             * If it blocks while holding the handlerLock, then the resume will deadlock.\n+             *\/\n+            eventHandler_unlock();\n+        }\n+\n+        if (resumee != NULL) {\n+            \/*\n+             * Hold up any attempt to resume as long as the debugger\n+             * has suspended the resumee.\n+             *\/\n+            blockOnDebuggerSuspend(resumee);\n+        }\n+\n+        node->handlingAppResume = JNI_FALSE;\n+\n+        \/*\n+         * The blocks exit condition: resumee's suspendCount == 0.\n+         *\n+         * Debugger suspends are blocked if any thread is executing\n+         * Thread.resume(), i.e. !handlingAppResume && frameDepth > 0.\n+         *\/\n+    }\n+\n@@ -2482,0 +2535,2 @@\n+    \/* Threads could be waiting in blockOnDebuggerSuspend *\/\n+    debugMonitorNotifyAll(threadLock);\n","filename":"src\/jdk.jdwp.agent\/share\/native\/libjdwp\/threadControl.c","additions":74,"deletions":19,"binary":false,"changes":93,"status":"modified"},{"patch":"@@ -111,0 +111,5 @@\n+    private static final String releaseSuffix(Map<String, ? super Object> params) {\n+        return Optional.ofNullable(RELEASE.fetchFrom(params, false)).map(\n+                rel -> \"-\" + rel).orElse(\"\");\n+    }\n+\n@@ -116,1 +121,1 @@\n-                            + \"-\" + RELEASE.fetchFrom(params)\n+                            + releaseSuffix(params)\n@@ -278,3 +283,3 @@\n-                new PackageProperty(\"Version\", String.format(\"%s-%s\",\n-                        VERSION.fetchFrom(params), RELEASE.fetchFrom(params)),\n-                        \"APPLICATION_VERSION-APPLICATION_RELEASE\",\n+                new PackageProperty(\"Version\", String.format(\"%s%s\",\n+                        VERSION.fetchFrom(params), releaseSuffix(params)),\n+                        \"APPLICATION_VERSION_WITH_RELEASE\",\n@@ -445,0 +450,2 @@\n+        data.put(\"APPLICATION_VERSION_WITH_RELEASE\", String.format(\"%s%s\",\n+                VERSION.fetchFrom(params), releaseSuffix(params)));\n","filename":"src\/jdk.jpackage\/linux\/classes\/jdk\/jpackage\/internal\/LinuxDebBundler.java","additions":11,"deletions":4,"binary":false,"changes":15,"status":"modified"},{"patch":"@@ -223,1 +223,0 @@\n-        data.put(\"APPLICATION_RELEASE\", RELEASE.fetchFrom(params));\n","filename":"src\/jdk.jpackage\/linux\/classes\/jdk\/jpackage\/internal\/LinuxPackageBundler.java","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -180,0 +180,1 @@\n+        data.put(\"APPLICATION_RELEASE\", RELEASE.fetchFrom(params));\n","filename":"src\/jdk.jpackage\/linux\/classes\/jdk\/jpackage\/internal\/LinuxRpmBundler.java","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n-Version: APPLICATION_VERSION-APPLICATION_RELEASE\n+Version: APPLICATION_VERSION_WITH_RELEASE\n","filename":"src\/jdk.jpackage\/linux\/classes\/jdk\/jpackage\/internal\/resources\/template.control","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -32,0 +32,1 @@\n+#include <sys\/wait.h>\n@@ -33,0 +34,1 @@\n+#include <stddef.h>\n@@ -46,1 +48,1 @@\n-static JvmlLauncherData* initJvmlLauncherData(void) {\n+static JvmlLauncherData* initJvmlLauncherData(int* size) {\n@@ -89,1 +91,1 @@\n-    result = jvmLauncherCreateJvmlLauncherData(api, jvmLauncherHandle);\n+    result = jvmLauncherCreateJvmlLauncherData(api, jvmLauncherHandle, size);\n@@ -134,0 +136,32 @@\n+static void closePipeEnd(int* pipefd, int idx) {\n+    if (pipefd[idx] >= 0) {\n+        close(pipefd[idx]);\n+        pipefd[idx] = -1;\n+    }\n+}\n+\n+\n+static void initJvmlLauncherDataPointers(void* baseAddress,\n+                                        JvmlLauncherData* jvmLauncherData) {\n+    ptrdiff_t offset = (char*)jvmLauncherData - (char*)baseAddress;\n+    int i;\n+\n+    jvmLauncherData->jliLibPath += offset;\n+    jvmLauncherData->jliLaunchArgv =\n+            (char**)((char*)jvmLauncherData->jliLaunchArgv + offset);\n+    jvmLauncherData->envVarNames =\n+            (char**)((char*)jvmLauncherData->envVarNames + offset);\n+    jvmLauncherData->envVarValues =\n+            (char**)((char*)jvmLauncherData->envVarValues + offset);\n+\n+    for (i = 0; i != jvmLauncherData->jliLaunchArgc; i++) {\n+        jvmLauncherData->jliLaunchArgv[i] += offset;\n+    }\n+\n+    for (i = 0; i != jvmLauncherData->envVarCount; i++) {\n+        jvmLauncherData->envVarNames[i] += offset;\n+        jvmLauncherData->envVarValues[i] += offset;\n+    }\n+}\n+\n+\n@@ -135,0 +169,2 @@\n+    int pipefd[2];\n+    pid_t cpid;\n@@ -136,1 +172,2 @@\n-    JvmlLauncherData* jvmLauncherData;\n+    JvmlLauncherData* jvmLauncherData = 0;\n+    int jvmLauncherDataBufferSize = 0;\n@@ -141,2 +178,75 @@\n-    jvmLauncherData = initJvmlLauncherData();\n-    if (jvmLauncherData) {\n+    if (pipe(pipefd) == -1) {\n+        JP_LOG_ERRNO;\n+        return exitCode;\n+    }\n+\n+    cpid = fork();\n+    if (cpid == -1) {\n+        JP_LOG_ERRNO;\n+    } else if (cpid == 0) \/* Child process *\/ {\n+        \/* Close unused read end *\/\n+        closePipeEnd(pipefd, 0);\n+\n+        jvmLauncherData = initJvmlLauncherData(&jvmLauncherDataBufferSize);\n+        if (jvmLauncherData) {\n+            \/* Buffer size *\/\n+            if (write(pipefd[1], &jvmLauncherDataBufferSize,\n+                                    sizeof(jvmLauncherDataBufferSize)) == -1) {\n+                JP_LOG_ERRNO;\n+                goto cleanup;\n+            }\n+            if (jvmLauncherDataBufferSize) {\n+                \/* Buffer address *\/\n+                if (write(pipefd[1], &jvmLauncherData,\n+                                            sizeof(jvmLauncherData)) == -1) {\n+                    JP_LOG_ERRNO;\n+                    goto cleanup;\n+                }\n+                \/* Buffer data *\/\n+                if (write(pipefd[1], jvmLauncherData,\n+                                            jvmLauncherDataBufferSize) == -1) {\n+                    JP_LOG_ERRNO;\n+                    goto cleanup;\n+                }\n+            }\n+        }\n+\n+        exitCode = 0;\n+    } else if (cpid > 0) {\n+        void* baseAddress = 0;\n+\n+        \/* Close unused write end *\/\n+        closePipeEnd(pipefd, 1);\n+\n+        if (read(pipefd[0], &jvmLauncherDataBufferSize,\n+                                sizeof(jvmLauncherDataBufferSize)) == -1) {\n+            JP_LOG_ERRNO;\n+            goto cleanup;\n+        }\n+\n+        if (jvmLauncherDataBufferSize == 0) {\n+            JP_LOG_ERRNO;\n+            goto cleanup;\n+        }\n+\n+        if (read(pipefd[0], &baseAddress, sizeof(baseAddress)) == -1) {\n+            JP_LOG_ERRNO;\n+            goto cleanup;\n+        }\n+\n+        jvmLauncherData = malloc(jvmLauncherDataBufferSize);\n+        if (!jvmLauncherData) {\n+            JP_LOG_ERRNO;\n+            goto cleanup;\n+        }\n+\n+        if (read(pipefd[0], jvmLauncherData,\n+                                        jvmLauncherDataBufferSize) == -1) {\n+            JP_LOG_ERRNO;\n+            goto cleanup;\n+        }\n+\n+        closePipeEnd(pipefd, 0);\n+        wait(NULL); \/* Wait child process to terminate *\/\n+\n+        initJvmlLauncherDataPointers(baseAddress, jvmLauncherData);\n@@ -144,1 +254,0 @@\n-        free(jvmLauncherData);\n@@ -147,0 +256,4 @@\n+cleanup:\n+    closePipeEnd(pipefd, 0);\n+    closePipeEnd(pipefd, 1);\n+    free(jvmLauncherData);\n","filename":"src\/jdk.jpackage\/linux\/native\/applauncher\/LinuxLauncher.c","additions":119,"deletions":6,"binary":false,"changes":125,"status":"modified"},{"patch":"@@ -116,10 +116,0 @@\n-    JP_TRY;\n-    if (0 != setenv(_JPACKAGE_LAUNCHER.c_str(), launchInfo.c_str(), 1)) {\n-        JP_THROW(tstrings::any() << \"setenv(\" << _JPACKAGE_LAUNCHER\n-                << \", \" << launchInfo << \") failed. Error: \" << lastCRTError());\n-    } else {\n-        LOG_TRACE(tstrings::any() << \"Set \"\n-                << _JPACKAGE_LAUNCHER << \"=[\" << launchInfo << \"]\");\n-    }\n-    JP_CATCH_ALL;\n-\n@@ -127,0 +117,2 @@\n+\n+    jvmLauncher->addEnvVariable(_JPACKAGE_LAUNCHER, launchInfo);\n","filename":"src\/jdk.jpackage\/linux\/native\/libapplauncher\/LinuxLauncherLib.cpp","additions":2,"deletions":10,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -133,0 +133,2 @@\n+    std::unique_ptr<Jvm> jvm(new Jvm());\n+\n@@ -134,1 +136,1 @@\n-        SysInfo::setEnvVariable(libEnvVarName, SysInfo::getEnvVariable(\n+        (*jvm).addEnvVariable(libEnvVarName, SysInfo::getEnvVariable(\n@@ -140,2 +142,0 @@\n-    std::unique_ptr<Jvm> jvm(new Jvm());\n-\n","filename":"src\/jdk.jpackage\/share\/native\/applauncher\/AppLauncher.cpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -204,1 +204,1 @@\n-                                                            jlh.release()));\n+                                                        jlh.release(), 0));\n@@ -218,0 +218,7 @@\n+void Jvm::setEnvVariables() {\n+    for (size_t i = 0; i != envVarNames.size(); i++) {\n+        SysInfo::setEnvVariable(envVarNames.at(i), envVarValues.at(i));\n+    }\n+}\n+\n+\n@@ -223,0 +230,2 @@\n+    tstring_array envVarNames;\n+    tstring_array envVarValues;\n@@ -233,0 +242,23 @@\n+    template <class T>\n+    static char* copyStrings(const std::vector<T>& src,\n+                JvmlLauncherData* ptr, const size_t offset, char* curPtr) {\n+        char** strArray = 0;\n+        if (ptr) {\n+            strArray = *reinterpret_cast<char***>(\n+                                    reinterpret_cast<char*>(ptr) + offset);\n+        }\n+\n+        for (size_t i = 0; i != src.size(); i++) {\n+            const size_t count = (src[i].size() + 1 \/* trailing zero *\/)\n+                                            * sizeof(typename T::value_type);\n+            if (ptr) {\n+                std::memcpy(curPtr, src[i].c_str(), count);\n+                strArray[i] = curPtr;\n+            }\n+\n+            curPtr += count;\n+        };\n+\n+        return curPtr;\n+    }\n+\n@@ -246,1 +278,1 @@\n-        \/\/ Next write array of char* pointing to JLI lib arg strings.\n+        \/\/ Write array of char* pointing to JLI lib arg strings.\n@@ -253,2 +285,1 @@\n-\n-        \/\/ Skip memory occupied by char* array.\n+        \/\/ Skip memory occupied by JvmlLauncherData::jliLaunchArgv array.\n@@ -256,0 +287,3 @@\n+        \/\/ Store JLI lib arg strings.\n+        curPtr = copyStrings(args, ptr,\n+                            offsetof(JvmlLauncherData, jliLaunchArgv), curPtr);\n@@ -257,9 +291,20 @@\n-        \/\/ Store array of strings.\n-        for (size_t i = 0; i != args.size(); i++) {\n-            const size_t count = (args[i].size() + 1 \/* trailing zero *\/);\n-            if (ptr) {\n-                std::memcpy(curPtr, args[i].c_str(), count);\n-                ptr->jliLaunchArgv[i] = curPtr;\n-            }\n-            curPtr += count;\n-        };\n+        \/\/ Write array of char* pointing to env variable name strings.\n+        if (ptr) {\n+            ptr->envVarNames = reinterpret_cast<TCHAR**>(curPtr);\n+            ptr->envVarCount = (int)envVarNames.size();\n+        }\n+        \/\/ Skip memory occupied by JvmlLauncherData::envVarNames array.\n+        curPtr += sizeof(TCHAR*) * envVarNames.size();\n+        \/\/ Store env variable names.\n+        curPtr = copyStrings(envVarNames, ptr,\n+                            offsetof(JvmlLauncherData, envVarNames), curPtr);\n+\n+        \/\/ Write array of char* pointing to env variable value strings.\n+        if (ptr) {\n+            ptr->envVarValues = reinterpret_cast<TCHAR**>(curPtr);\n+        }\n+        \/\/ Skip memory occupied by JvmlLauncherData::envVarValues array.\n+        curPtr += sizeof(TCHAR*) * envVarValues.size();\n+        \/\/ Store env variable values.\n+        curPtr = copyStrings(envVarValues, ptr,\n+                            offsetof(JvmlLauncherData, envVarValues), curPtr);\n@@ -279,7 +324,1 @@\n-} \/\/ namespace\n-\n-JvmlLauncherHandle Jvm::exportLauncher() const {\n-    std::unique_ptr<JliLaunchData> result(new JliLaunchData());\n-\n-    result->jliLibPath = tstrings::toUtf8(jvmPath);\n-\n+void copyStringArray(const tstring_array& src, std::vector<std::string>& dst) {\n@@ -288,2 +327,2 @@\n-        tstring_array::const_iterator it = args.begin();\n-        const tstring_array::const_iterator end = args.end();\n+        tstring_array::const_iterator it = src.begin();\n+        const tstring_array::const_iterator end = src.end();\n@@ -291,1 +330,1 @@\n-            result->args.push_back(tstrings::toACP(*it));\n+            dst.push_back(tstrings::toACP(*it));\n@@ -295,1 +334,1 @@\n-    result->args = args;\n+    dst = src;\n@@ -297,0 +336,12 @@\n+}\n+\n+} \/\/ namespace\n+\n+JvmlLauncherHandle Jvm::exportLauncher() const {\n+    std::unique_ptr<JliLaunchData> result(new JliLaunchData());\n+\n+    result->jliLibPath = tstrings::toUtf8(jvmPath);\n+\n+    copyStringArray(args, result->args);\n+    result->envVarNames = envVarNames;\n+    result->envVarValues = envVarValues;\n","filename":"src\/jdk.jpackage\/share\/native\/applauncher\/JvmLauncher.cpp","additions":75,"deletions":24,"binary":false,"changes":99,"status":"modified"},{"patch":"@@ -32,0 +32,4 @@\n+#ifdef _WIN32\n+#include <windows.h>\n+#include <tchar.h>\n+#endif\n@@ -38,0 +42,4 @@\n+#ifndef _WIN32\n+typedef char TCHAR;\n+#endif\n+\n@@ -41,0 +49,1 @@\n+    int envVarCount;\n@@ -42,0 +51,2 @@\n+    TCHAR** envVarNames;\n+    TCHAR** envVarValues;\n@@ -74,1 +85,2 @@\n-JvmlLauncherData* jvmLauncherCreateJvmlLauncherData(JvmlLauncherAPI* api, JvmlLauncherHandle h);\n+JvmlLauncherData* jvmLauncherCreateJvmlLauncherData(JvmlLauncherAPI* api,\n+                                            JvmlLauncherHandle h, int* size);\n@@ -108,0 +120,6 @@\n+    Jvm& addEnvVariable(const tstring& name, const tstring& value) {\n+        envVarNames.push_back(name);\n+        envVarValues.push_back(value);\n+        return *this;\n+    }\n+\n@@ -121,0 +139,2 @@\n+    void setEnvVariables();\n+\n@@ -126,0 +146,2 @@\n+    tstring_array envVarNames;\n+    tstring_array envVarValues;\n","filename":"src\/jdk.jpackage\/share\/native\/applauncher\/JvmLauncher.h","additions":23,"deletions":1,"binary":false,"changes":24,"status":"modified"},{"patch":"@@ -29,0 +29,3 @@\n+#ifdef LINUX\n+#include <unistd.h>\n+#endif\n@@ -47,1 +50,1 @@\n-                                JvmlLauncherAPI* api, JvmlLauncherHandle h) {\n+                    JvmlLauncherAPI* api, JvmlLauncherHandle h, int* size) {\n@@ -72,0 +75,3 @@\n+        if (size) {\n+            *size = jvmLauncherDataBufferSize;\n+        }\n@@ -88,0 +94,4 @@\n+    for (i = 0; i < jvmArgs->envVarCount; ++i) {\n+        JP_LOG_TRACE(\"env var[%d]: %s=[%s]\", i, jvmArgs->envVarNames[i],\n+                                                jvmArgs->envVarValues[i]);\n+    }\n@@ -92,0 +102,1 @@\n+    int i;\n@@ -95,0 +106,15 @@\n+\n+    for (i = 0; i < jvmArgs->envVarCount; ++i) {\n+#ifdef _WIN32\n+        if (!SetEnvironmentVariable(jvmArgs->envVarNames[i],\n+                                                jvmArgs->envVarValues[i])) {\n+            JP_LOG_TRACE(\"SetEnvironmentVariable(%d) failed\", i);\n+        }\n+#else\n+        if (setenv(jvmArgs->envVarNames[i],\n+                                        jvmArgs->envVarValues[i], 1) != 0) {\n+            JP_LOG_TRACE(\"setenv(%d) failed\", i);\n+        }\n+#endif\n+    }\n+\n@@ -124,0 +150,3 @@\n+#endif\n+#ifdef LINUX\n+    fprintf(stderr, \"[%d]: \", getpid());\n","filename":"src\/jdk.jpackage\/share\/native\/applauncher\/JvmLauncherLib.c","additions":30,"deletions":1,"binary":false,"changes":31,"status":"modified"},{"patch":"@@ -59,1 +59,4 @@\n-    ::setenv(name.c_str(), value.c_str(), 1);\n+    if (::setenv(name.c_str(), value.c_str(), 1) != 0) {\n+        JP_THROW(tstrings::any() << \"setenv(\" << name << \", \" << value\n+                                    << \") failed. Error: \" << lastCRTError());\n+    }\n","filename":"src\/jdk.jpackage\/unix\/native\/common\/UnixSysInfo.cpp","additions":4,"deletions":1,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -155,0 +155,2 @@\n+        jvm->setEnvVariables();\n+\n","filename":"src\/jdk.jpackage\/windows\/native\/applauncher\/WinLauncher.cpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -82,1 +82,1 @@\n-                                                   dcmd_arg_info_array);\n+                                                   dcmd_arg_info_array, num_arg);\n","filename":"src\/jdk.management\/share\/native\/libmanagement_ext\/DiagnosticCommandImpl.c","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -191,1 +191,1 @@\n-    final static class ResolvableHelper implements Serializable {\n+    static final class ResolvableHelper implements Serializable {\n","filename":"src\/utils\/IdealGraphVisualizer\/Bytecodes\/src\/main\/java\/com\/sun\/hotspot\/igv\/bytecodes\/BytecodeViewTopComponent.java","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -168,1 +168,1 @@\n-    final static class ResolvableHelper implements Serializable {\n+    static final class ResolvableHelper implements Serializable {\n","filename":"src\/utils\/IdealGraphVisualizer\/ControlFlow\/src\/main\/java\/com\/sun\/hotspot\/igv\/controlflow\/ControlFlowTopComponent.java","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -94,1 +94,1 @@\n-    private static abstract class Member implements LengthToString {\n+    private abstract static class Member implements LengthToString {\n","filename":"src\/utils\/IdealGraphVisualizer\/Data\/src\/main\/java\/com\/sun\/hotspot\/igv\/data\/serialization\/BinaryParser.java","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -662,1 +662,1 @@\n-    final static class ResolvableHelper implements Serializable {\n+    static final class ResolvableHelper implements Serializable {\n","filename":"src\/utils\/IdealGraphVisualizer\/FilterWindow\/src\/main\/java\/com\/sun\/hotspot\/igv\/filterwindow\/FilterTopComponent.java","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -35,12 +35,12 @@\n-    public final static String NODE_TEXT = \"nodeText\";\n-    public final static String NODE_TEXT_DEFAULT = \"[idx] [name]\";\n-    public final static String NODE_SHORT_TEXT = \"nodeShortText\";\n-    public final static String NODE_SHORT_TEXT_DEFAULT = \"[idx] [name]\";\n-    public final static String NODE_WIDTH = \"nodeWidth\";\n-    public final static String NODE_WIDTH_DEFAULT = \"100\";\n-    public final static String PORT = \"port\";\n-    public final static String PORT_BINARY = \"portBinary\";\n-    public final static String PORT_DEFAULT = \"4444\";\n-    public final static String PORT_BINARY_DEFAULT = \"4445\";\n-    public final static String DIRECTORY = \"directory\";\n-    public final static String DIRECTORY_DEFAULT = System.getProperty(\"user.dir\");\n+    public static final String NODE_TEXT = \"nodeText\";\n+    public static final String NODE_TEXT_DEFAULT = \"[idx] [name]\";\n+    public static final String NODE_SHORT_TEXT = \"nodeShortText\";\n+    public static final String NODE_SHORT_TEXT_DEFAULT = \"[idx] [name]\";\n+    public static final String NODE_WIDTH = \"nodeWidth\";\n+    public static final String NODE_WIDTH_DEFAULT = \"100\";\n+    public static final String PORT = \"port\";\n+    public static final String PORT_BINARY = \"portBinary\";\n+    public static final String PORT_DEFAULT = \"4444\";\n+    public static final String PORT_BINARY_DEFAULT = \"4445\";\n+    public static final String DIRECTORY = \"directory\";\n+    public static final String DIRECTORY_DEFAULT = System.getProperty(\"user.dir\");\n","filename":"src\/utils\/IdealGraphVisualizer\/Settings\/src\/main\/java\/com\/sun\/hotspot\/igv\/settings\/Settings.java","additions":12,"deletions":12,"binary":false,"changes":24,"status":"modified"},{"patch":"@@ -93,1 +93,1 @@\n-    abstract public void print(PrintStream stream, boolean printID);\n+    public abstract void print(PrintStream stream, boolean printID);\n","filename":"src\/utils\/LogCompilation\/src\/main\/java\/com\/sun\/hotspot\/tools\/compiler\/BasicLogEvent.java","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -57,3 +57,3 @@\n-    static final private Matcher duplicateCompileID = Pattern.compile(\".+ compile_id='[0-9]+'.*( compile_id='[0-9]+)\").matcher(\"\");\n-    static final private Matcher compilerName = Pattern.compile(\"' (C[12]) compile_id=\").matcher(\"\");\n-    static final private Matcher destroyVM = Pattern.compile(\"'(destroy_vm)\/\").matcher(\"\");\n+    private static final Matcher duplicateCompileID = Pattern.compile(\".+ compile_id='[0-9]+'.*( compile_id='[0-9]+)\").matcher(\"\");\n+    private static final Matcher compilerName = Pattern.compile(\"' (C[12]) compile_id=\").matcher(\"\");\n+    private static final Matcher destroyVM = Pattern.compile(\"'(destroy_vm)\/\").matcher(\"\");\n","filename":"src\/utils\/LogCompilation\/src\/main\/java\/com\/sun\/hotspot\/tools\/compiler\/LogCleanupReader.java","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -442,3 +442,3 @@\n-        final public Method method;\n-        final public int bci;\n-        final public String toString() {\n+        public final Method method;\n+        public final int bci;\n+        public final String toString() {\n","filename":"src\/utils\/LogCompilation\/src\/main\/java\/com\/sun\/hotspot\/tools\/compiler\/LogParser.java","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -70,0 +70,4 @@\n+If you want to build hsdis with binutils provided by system\n+(e.g. binutils-devel from Fedora, binutils-dev from Ubuntu), you can pass\n+\"--with-binutils=system\". \"system\" is available on Linux only.\n+\n","filename":"src\/utils\/hsdis\/README","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2008, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2008, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -52,0 +52,1 @@\n+#ifndef SYSTEM_BINUTILS\n@@ -53,0 +54,2 @@\n+#endif\n+\n@@ -59,1 +62,0 @@\n-#include <bfdver.h>\n@@ -568,1 +570,1 @@\n-#if BFD_VERSION >= 234000000\n+#ifdef SEC_ELF_OCTETS\n","filename":"src\/utils\/hsdis\/hsdis.c","additions":5,"deletions":3,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -60,1 +60,1 @@\n-    final static boolean verbose = false;\n+    static final boolean verbose = false;\n","filename":"src\/utils\/src\/build\/tools\/commentchecker\/CommentChecker.java","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -32,1 +32,1 @@\n-    private final static String FILE_SEPARATOR = System.getProperty(\"file.separator\");\n+    private static final String FILE_SEPARATOR = System.getProperty(\"file.separator\");\n","filename":"src\/utils\/src\/build\/tools\/dirdiff\/DirDiff.java","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -10,0 +10,49 @@\n+# These tables are legal immediate logical operands\n+immediates8 \\\n+     = [0x1, 0x0c, 0x3e, 0x60, 0x7c, 0x80, 0x83,\n+        0xe1, 0xbf, 0xef, 0xf3, 0xfe]\n+\n+immediates16 \\\n+     = [0x1, 0x38, 0x7e, 0xff, 0x1fc, 0x1ff, 0x3f0,\n+        0x7e0, 0xfc0, 0x1f80, 0x3ff0, 0x7e00, 0x7e00,\n+        0x8000, 0x81ff, 0xc1ff, 0xc003, 0xc7ff, 0xdfff,\n+        0xe03f, 0xe10f, 0xe1ff, 0xf801, 0xfc00, 0xfc07,\n+        0xff03, 0xfffe]\n+\n+immediates32 \\\n+     = [0x1, 0x3f, 0x1f0, 0x7e0,\n+        0x1c00, 0x3ff0, 0x8000, 0x1e000,\n+        0x3e000, 0x78000, 0xe0000, 0x100000,\n+        0x1fffe0, 0x3fe000, 0x780000, 0x7ffff8,\n+        0xff8000, 0x1800180, 0x1fffc00, 0x3c003c0,\n+        0x3ffff00, 0x7c00000, 0x7fffe00, 0xf000f00,\n+        0xfffe000, 0x18181818, 0x1ffc0000, 0x1ffffffe,\n+        0x3f003f00, 0x3fffe000, 0x60006000, 0x7f807f80,\n+        0x7ffffc00, 0x800001ff, 0x803fffff, 0x9f9f9f9f,\n+        0xc0000fff, 0xc0c0c0c0, 0xe0000000, 0xe003e003,\n+        0xe3ffffff, 0xf0000fff, 0xf0f0f0f0, 0xf80000ff,\n+        0xf83ff83f, 0xfc00007f, 0xfc1fffff, 0xfe0001ff,\n+        0xfe3fffff, 0xff003fff, 0xff800003, 0xff87ff87,\n+        0xffc00fff, 0xffe0000f, 0xffefffef, 0xfff1fff1,\n+        0xfff83fff, 0xfffc0fff, 0xfffe0fff, 0xffff3fff,\n+        0xffffc007, 0xffffe1ff, 0xfffff80f, 0xfffffe07,\n+        0xffffffbf, 0xfffffffd]\n+\n+immediates64 \\\n+     = [0x1, 0x1f80, 0x3fff0, 0x3ffffc,\n+        0x3fe0000, 0x1ffc0000, 0xf8000000, 0x3ffffc000,\n+        0xffffffe00, 0x3ffffff800, 0xffffc00000, 0x3f000000000,\n+        0x7fffffff800, 0x1fe000001fe0, 0x3ffffff80000, 0xc00000000000,\n+        0x1ffc000000000, 0x3ffff0003ffff, 0x7ffffffe00000, 0xfffffffffc000,\n+        0x1ffffffffffc00, 0x3fffffffffff00, 0x7ffffffffffc00, 0xffffffffff8000,\n+        0x1ffffffff800000, 0x3fffffc03fffffc, 0x7fffc0000000000, 0xff80ff80ff80ff8,\n+        0x1c00000000000000, 0x1fffffffffff0000, 0x3fffff803fffff80, 0x7fc000007fc00000,\n+        0x8000000000000000, 0x803fffff803fffff, 0xc000007fc000007f, 0xe00000000000ffff,\n+        0xe3ffffffffffffff, 0xf007f007f007f007, 0xf80003ffffffffff, 0xfc000003fc000003,\n+        0xfe000000007fffff, 0xff00000000007fff, 0xff800000000003ff, 0xffc00000000000ff,\n+        0xffe00000000003ff, 0xfff0000000003fff, 0xfff80000001fffff, 0xfffc0000fffc0000,\n+        0xfffe003fffffffff, 0xffff3fffffffffff, 0xffffc0000007ffff, 0xffffe01fffffe01f,\n+        0xfffff800000007ff, 0xfffffc0fffffffff, 0xffffff00003fffff, 0xffffffc0000007ff,\n+        0xfffffff0000001ff, 0xfffffffc00003fff, 0xffffffff07ffffff, 0xffffffffe003ffff,\n+        0xfffffffffc01ffff, 0xffffffffffc00003, 0xfffffffffffc000f, 0xffffffffffffe07f]\n+\n@@ -354,39 +403,0 @@\n-\n-     # These tables are legal immediate logical operands\n-     immediates32 \\\n-         = [0x1, 0x3f, 0x1f0, 0x7e0,\n-            0x1c00, 0x3ff0, 0x8000, 0x1e000,\n-            0x3e000, 0x78000, 0xe0000, 0x100000,\n-            0x1fffe0, 0x3fe000, 0x780000, 0x7ffff8,\n-            0xff8000, 0x1800180, 0x1fffc00, 0x3c003c0,\n-            0x3ffff00, 0x7c00000, 0x7fffe00, 0xf000f00,\n-            0xfffe000, 0x18181818, 0x1ffc0000, 0x1ffffffe,\n-            0x3f003f00, 0x3fffe000, 0x60006000, 0x7f807f80,\n-            0x7ffffc00, 0x800001ff, 0x803fffff, 0x9f9f9f9f,\n-            0xc0000fff, 0xc0c0c0c0, 0xe0000000, 0xe003e003,\n-            0xe3ffffff, 0xf0000fff, 0xf0f0f0f0, 0xf80000ff,\n-            0xf83ff83f, 0xfc00007f, 0xfc1fffff, 0xfe0001ff,\n-            0xfe3fffff, 0xff003fff, 0xff800003, 0xff87ff87,\n-            0xffc00fff, 0xffe0000f, 0xffefffef, 0xfff1fff1,\n-            0xfff83fff, 0xfffc0fff, 0xfffe0fff, 0xffff3fff,\n-            0xffffc007, 0xffffe1ff, 0xfffff80f, 0xfffffe07,\n-            0xffffffbf, 0xfffffffd]\n-\n-     immediates \\\n-         = [0x1, 0x1f80, 0x3fff0, 0x3ffffc,\n-            0x3fe0000, 0x1ffc0000, 0xf8000000, 0x3ffffc000,\n-            0xffffffe00, 0x3ffffff800, 0xffffc00000, 0x3f000000000,\n-            0x7fffffff800, 0x1fe000001fe0, 0x3ffffff80000, 0xc00000000000,\n-            0x1ffc000000000, 0x3ffff0003ffff, 0x7ffffffe00000, 0xfffffffffc000,\n-            0x1ffffffffffc00, 0x3fffffffffff00, 0x7ffffffffffc00, 0xffffffffff8000,\n-            0x1ffffffff800000, 0x3fffffc03fffffc, 0x7fffc0000000000, 0xff80ff80ff80ff8,\n-            0x1c00000000000000, 0x1fffffffffff0000, 0x3fffff803fffff80, 0x7fc000007fc00000,\n-            0x8000000000000000, 0x803fffff803fffff, 0xc000007fc000007f, 0xe00000000000ffff,\n-            0xe3ffffffffffffff, 0xf007f007f007f007, 0xf80003ffffffffff, 0xfc000003fc000003,\n-            0xfe000000007fffff, 0xff00000000007fff, 0xff800000000003ff, 0xffc00000000000ff,\n-            0xffe00000000003ff, 0xfff0000000003fff, 0xfff80000001fffff, 0xfffc0000fffc0000,\n-            0xfffe003fffffffff, 0xffff3fffffffffff, 0xffffc0000007ffff, 0xffffe01fffffe01f,\n-            0xfffff800000007ff, 0xfffffc0fffffffff, 0xffffff00003fffff, 0xffffffc0000007ff,\n-            0xfffffff0000001ff, 0xfffffffc00003fff, 0xffffffff07ffffff, 0xffffffffe003ffff,\n-            0xfffffffffc01ffff, 0xffffffffffc00003, 0xfffffffffffc000f, 0xffffffffffffe07f]\n-\n@@ -396,1 +406,1 @@\n-              self.immediates32[random.randint(0, len(self.immediates32)-1)] \\\n+              immediates32[random.randint(0, len(immediates32)-1)] \\\n@@ -398,1 +408,1 @@\n-              self.immediates[random.randint(0, len(self.immediates)-1)]\n+              immediates64[random.randint(0, len(immediates64)-1)]\n@@ -409,0 +419,38 @@\n+class SVEBinaryImmOp(Instruction):\n+    def __init__(self, name):\n+        reg = SVEVectorRegister().generate()\n+        self.reg = [reg, reg]\n+        self.numRegs = len(self.reg)\n+        self._width = RegVariant(0, 3)\n+        self._isLogical = False\n+        if name in [\"and\", \"eor\", \"orr\"]:\n+            self._isLogical = True\n+        Instruction.__init__(self, name)\n+\n+    def generate(self):\n+        Instruction.generate(self)\n+        self.immed = random.randint(0, (1<<8)-1)\n+        if self._isLogical:\n+            vectype = self._width.cstr()\n+            if vectype == \"__ B\":\n+                self.immed = immediates8[random.randint(0, len(immediates8)-1)]\n+            elif vectype == \"__ H\":\n+                self.immed = immediates16[random.randint(0, len(immediates16)-1)]\n+            elif vectype == \"__ S\":\n+                self.immed = immediates32[random.randint(0, len(immediates32)-1)]\n+            elif vectype == \"__ D\":\n+                self.immed = immediates64[random.randint(0, len(immediates64)-1)]\n+        return self\n+\n+    def cstr(self):\n+        formatStr = \"%s%s, %s, %su);\"\n+        return (formatStr\n+                % tuple([\"__ sve_\" + self._name + \"(\"] +\n+                        [str(self.reg[0]), self._width.cstr(), self.immed]))\n+\n+    def astr(self):\n+        formatStr = \"%s%s, %s, #0x%x\"\n+        Regs = [str(self.reg[i]) + self._width.astr() for i in range(0, self.numRegs)]\n+        return (formatStr\n+                % tuple([Instruction.astr(self)] + Regs + [self.immed]))\n+\n@@ -1558,0 +1606,17 @@\n+                        [\"lsl\",     \"__ sve_lsl(z0, __ B, p0, 0);\",                       \"lsl\\tz0.b, p0\/m, z0.b, #0\"],\n+                        [\"lsl\",     \"__ sve_lsl(z0, __ B, p0, 5);\",                       \"lsl\\tz0.b, p0\/m, z0.b, #5\"],\n+                        [\"lsl\",     \"__ sve_lsl(z1, __ H, p1, 15);\",                      \"lsl\\tz1.h, p1\/m, z1.h, #15\"],\n+                        [\"lsl\",     \"__ sve_lsl(z2, __ S, p2, 31);\",                      \"lsl\\tz2.s, p2\/m, z2.s, #31\"],\n+                        [\"lsl\",     \"__ sve_lsl(z3, __ D, p3, 63);\",                      \"lsl\\tz3.d, p3\/m, z3.d, #63\"],\n+                        [\"lsr\",     \"__ sve_lsr(z0, __ B, p0, 1);\",                       \"lsr\\tz0.b, p0\/m, z0.b, #1\"],\n+                        [\"lsr\",     \"__ sve_lsr(z0, __ B, p0, 8);\",                       \"lsr\\tz0.b, p0\/m, z0.b, #8\"],\n+                        [\"lsr\",     \"__ sve_lsr(z1, __ H, p1, 15);\",                      \"lsr\\tz1.h, p1\/m, z1.h, #15\"],\n+                        [\"lsr\",     \"__ sve_lsr(z2, __ S, p2, 7);\",                       \"lsr\\tz2.s, p2\/m, z2.s, #7\"],\n+                        [\"lsr\",     \"__ sve_lsr(z2, __ S, p2, 31);\",                      \"lsr\\tz2.s, p2\/m, z2.s, #31\"],\n+                        [\"lsr\",     \"__ sve_lsr(z3, __ D, p3, 63);\",                      \"lsr\\tz3.d, p3\/m, z3.d, #63\"],\n+                        [\"asr\",     \"__ sve_asr(z0, __ B, p0, 1);\",                       \"asr\\tz0.b, p0\/m, z0.b, #1\"],\n+                        [\"asr\",     \"__ sve_asr(z0, __ B, p0, 7);\",                       \"asr\\tz0.b, p0\/m, z0.b, #7\"],\n+                        [\"asr\",     \"__ sve_asr(z1, __ H, p1, 5);\",                       \"asr\\tz1.h, p1\/m, z1.h, #5\"],\n+                        [\"asr\",     \"__ sve_asr(z1, __ H, p1, 15);\",                      \"asr\\tz1.h, p1\/m, z1.h, #15\"],\n+                        [\"asr\",     \"__ sve_asr(z2, __ S, p2, 31);\",                      \"asr\\tz2.s, p2\/m, z2.s, #31\"],\n+                        [\"asr\",     \"__ sve_asr(z3, __ D, p3, 63);\",                      \"asr\\tz3.d, p3\/m, z3.d, #63\"],\n@@ -1652,0 +1717,23 @@\n+                        [\"and\",     \"__ sve_and(p0, p1, p2, p3);\",                        \"and\\tp0.b, p1\/z, p2.b, p3.b\"],\n+                        [\"ands\",    \"__ sve_ands(p4, p5, p6, p0);\",                       \"ands\\tp4.b, p5\/z, p6.b, p0.b\"],\n+                        [\"eor\",     \"__ sve_eor(p0, p1, p2, p3);\",                        \"eor\\tp0.b, p1\/z, p2.b, p3.b\"],\n+                        [\"eors\",    \"__ sve_eors(p5, p6, p0, p1);\",                       \"eors\\tp5.b, p6\/z, p0.b, p1.b\"],\n+                        [\"orr\",     \"__ sve_orr(p0, p1, p2, p3);\",                        \"orr\\tp0.b, p1\/z, p2.b, p3.b\"],\n+                        [\"orrs\",    \"__ sve_orrs(p9, p1, p4, p5);\",                       \"orrs\\tp9.b, p1\/z, p4.b, p5.b\"],\n+                        [\"bic\",     \"__ sve_bic(p10, p7, p9, p11);\",                      \"bic\\tp10.b, p7\/z, p9.b, p11.b\"],\n+                        [\"ptest\",   \"__ sve_ptest(p7, p1);\",                              \"ptest\\tp7, p1.b\"],\n+                        [\"ptrue\",   \"__ sve_ptrue(p1, __ B);\",                            \"ptrue\\tp1.b\"],\n+                        [\"ptrue\",   \"__ sve_ptrue(p2, __ H);\",                            \"ptrue\\tp2.h\"],\n+                        [\"ptrue\",   \"__ sve_ptrue(p3, __ S);\",                            \"ptrue\\tp3.s\"],\n+                        [\"ptrue\",   \"__ sve_ptrue(p4, __ D);\",                            \"ptrue\\tp4.d\"],\n+                        [\"pfalse\",  \"__ sve_pfalse(p7);\",                                 \"pfalse\\tp7.b\"],\n+                        [\"uzp1\",    \"__ sve_uzp1(p0, __ B, p0, p1);\",                     \"uzp1\\tp0.b, p0.b, p1.b\"],\n+                        [\"uzp1\",    \"__ sve_uzp1(p0, __ H, p0, p1);\",                     \"uzp1\\tp0.h, p0.h, p1.h\"],\n+                        [\"uzp1\",    \"__ sve_uzp1(p0, __ S, p0, p1);\",                     \"uzp1\\tp0.s, p0.s, p1.s\"],\n+                        [\"uzp1\",    \"__ sve_uzp1(p0, __ D, p0, p1);\",                     \"uzp1\\tp0.d, p0.d, p1.d\"],\n+                        [\"uzp2\",    \"__ sve_uzp2(p0, __ B, p0, p1);\",                     \"uzp2\\tp0.b, p0.b, p1.b\"],\n+                        [\"uzp2\",    \"__ sve_uzp2(p0, __ H, p0, p1);\",                     \"uzp2\\tp0.h, p0.h, p1.h\"],\n+                        [\"uzp2\",    \"__ sve_uzp2(p0, __ S, p0, p1);\",                     \"uzp2\\tp0.s, p0.s, p1.s\"],\n+                        [\"uzp2\",    \"__ sve_uzp2(p0, __ D, p0, p1);\",                     \"uzp2\\tp0.d, p0.d, p1.d\"],\n+                        [\"punpklo\", \"__ sve_punpklo(p1, p0);\",                            \"punpklo\\tp1.h, p0.b\"],\n+                        [\"punpkhi\", \"__ sve_punpkhi(p1, p0);\",                            \"punpkhi\\tp1.h, p0.b\"],\n@@ -1682,0 +1770,3 @@\n+for i in range(6):\n+    generate(SVEBinaryImmOp, [\"add\", \"sub\", \"and\", \"eor\", \"orr\"])\n+\n@@ -1689,0 +1780,1 @@\n+                       [\"and\", \"ZPZ\", \"m\", \"dn\"],\n@@ -1691,0 +1783,1 @@\n+                       [\"eor\", \"ZPZ\", \"m\", \"dn\"],\n@@ -1696,0 +1789,1 @@\n+                       [\"orr\", \"ZPZ\", \"m\", \"dn\"],\n@@ -1711,0 +1805,1 @@\n+                       [\"fmad\", \"ZPZZ\", \"m\"],\n","filename":"test\/hotspot\/gtest\/aarch64\/aarch64-asmtest.py","additions":136,"deletions":41,"binary":false,"changes":177,"status":"modified"},{"patch":"@@ -747,0 +747,17 @@\n+    __ sve_lsl(z0, __ B, p0, 0);                       \/\/       lsl     z0.b, p0\/m, z0.b, #0\n+    __ sve_lsl(z0, __ B, p0, 5);                       \/\/       lsl     z0.b, p0\/m, z0.b, #5\n+    __ sve_lsl(z1, __ H, p1, 15);                      \/\/       lsl     z1.h, p1\/m, z1.h, #15\n+    __ sve_lsl(z2, __ S, p2, 31);                      \/\/       lsl     z2.s, p2\/m, z2.s, #31\n+    __ sve_lsl(z3, __ D, p3, 63);                      \/\/       lsl     z3.d, p3\/m, z3.d, #63\n+    __ sve_lsr(z0, __ B, p0, 1);                       \/\/       lsr     z0.b, p0\/m, z0.b, #1\n+    __ sve_lsr(z0, __ B, p0, 8);                       \/\/       lsr     z0.b, p0\/m, z0.b, #8\n+    __ sve_lsr(z1, __ H, p1, 15);                      \/\/       lsr     z1.h, p1\/m, z1.h, #15\n+    __ sve_lsr(z2, __ S, p2, 7);                       \/\/       lsr     z2.s, p2\/m, z2.s, #7\n+    __ sve_lsr(z2, __ S, p2, 31);                      \/\/       lsr     z2.s, p2\/m, z2.s, #31\n+    __ sve_lsr(z3, __ D, p3, 63);                      \/\/       lsr     z3.d, p3\/m, z3.d, #63\n+    __ sve_asr(z0, __ B, p0, 1);                       \/\/       asr     z0.b, p0\/m, z0.b, #1\n+    __ sve_asr(z0, __ B, p0, 7);                       \/\/       asr     z0.b, p0\/m, z0.b, #7\n+    __ sve_asr(z1, __ H, p1, 5);                       \/\/       asr     z1.h, p1\/m, z1.h, #5\n+    __ sve_asr(z1, __ H, p1, 15);                      \/\/       asr     z1.h, p1\/m, z1.h, #15\n+    __ sve_asr(z2, __ S, p2, 31);                      \/\/       asr     z2.s, p2\/m, z2.s, #31\n+    __ sve_asr(z3, __ D, p3, 63);                      \/\/       asr     z3.d, p3\/m, z3.d, #63\n@@ -841,0 +858,23 @@\n+    __ sve_and(p0, p1, p2, p3);                        \/\/       and     p0.b, p1\/z, p2.b, p3.b\n+    __ sve_ands(p4, p5, p6, p0);                       \/\/       ands    p4.b, p5\/z, p6.b, p0.b\n+    __ sve_eor(p0, p1, p2, p3);                        \/\/       eor     p0.b, p1\/z, p2.b, p3.b\n+    __ sve_eors(p5, p6, p0, p1);                       \/\/       eors    p5.b, p6\/z, p0.b, p1.b\n+    __ sve_orr(p0, p1, p2, p3);                        \/\/       orr     p0.b, p1\/z, p2.b, p3.b\n+    __ sve_orrs(p9, p1, p4, p5);                       \/\/       orrs    p9.b, p1\/z, p4.b, p5.b\n+    __ sve_bic(p10, p7, p9, p11);                      \/\/       bic     p10.b, p7\/z, p9.b, p11.b\n+    __ sve_ptest(p7, p1);                              \/\/       ptest   p7, p1.b\n+    __ sve_ptrue(p1, __ B);                            \/\/       ptrue   p1.b\n+    __ sve_ptrue(p2, __ H);                            \/\/       ptrue   p2.h\n+    __ sve_ptrue(p3, __ S);                            \/\/       ptrue   p3.s\n+    __ sve_ptrue(p4, __ D);                            \/\/       ptrue   p4.d\n+    __ sve_pfalse(p7);                                 \/\/       pfalse  p7.b\n+    __ sve_uzp1(p0, __ B, p0, p1);                     \/\/       uzp1    p0.b, p0.b, p1.b\n+    __ sve_uzp1(p0, __ H, p0, p1);                     \/\/       uzp1    p0.h, p0.h, p1.h\n+    __ sve_uzp1(p0, __ S, p0, p1);                     \/\/       uzp1    p0.s, p0.s, p1.s\n+    __ sve_uzp1(p0, __ D, p0, p1);                     \/\/       uzp1    p0.d, p0.d, p1.d\n+    __ sve_uzp2(p0, __ B, p0, p1);                     \/\/       uzp2    p0.b, p0.b, p1.b\n+    __ sve_uzp2(p0, __ H, p0, p1);                     \/\/       uzp2    p0.h, p0.h, p1.h\n+    __ sve_uzp2(p0, __ S, p0, p1);                     \/\/       uzp2    p0.s, p0.s, p1.s\n+    __ sve_uzp2(p0, __ D, p0, p1);                     \/\/       uzp2    p0.d, p0.d, p1.d\n+    __ sve_punpklo(p1, p0);                            \/\/       punpklo p1.h, p0.b\n+    __ sve_punpkhi(p1, p0);                            \/\/       punpkhi p1.h, p0.b\n@@ -976,0 +1016,42 @@\n+\/\/ SVEBinaryImmOp\n+    __ sve_add(z4, __ B, 147u);                        \/\/       add     z4.b, z4.b, #0x93\n+    __ sve_sub(z0, __ B, 124u);                        \/\/       sub     z0.b, z0.b, #0x7c\n+    __ sve_and(z1, __ H, 508u);                        \/\/       and     z1.h, z1.h, #0x1fc\n+    __ sve_eor(z9, __ D, 18374686479671656447u);       \/\/       eor     z9.d, z9.d, #0xff00000000007fff\n+    __ sve_orr(z22, __ S, 251662080u);                 \/\/       orr     z22.s, z22.s, #0xf000f00\n+\n+\/\/ SVEBinaryImmOp\n+    __ sve_add(z8, __ S, 248u);                        \/\/       add     z8.s, z8.s, #0xf8\n+    __ sve_sub(z6, __ S, 16u);                         \/\/       sub     z6.s, z6.s, #0x10\n+    __ sve_and(z11, __ D, 4160749568u);                \/\/       and     z11.d, z11.d, #0xf8000000\n+    __ sve_eor(z26, __ S, 1610637312u);                \/\/       eor     z26.s, z26.s, #0x60006000\n+    __ sve_orr(z13, __ D, 18446181398634037247u);      \/\/       orr     z13.d, z13.d, #0xfffe003fffffffff\n+\n+\/\/ SVEBinaryImmOp\n+    __ sve_add(z5, __ B, 112u);                        \/\/       add     z5.b, z5.b, #0x70\n+    __ sve_sub(z10, __ S, 88u);                        \/\/       sub     z10.s, z10.s, #0x58\n+    __ sve_and(z26, __ S, 253952u);                    \/\/       and     z26.s, z26.s, #0x3e000\n+    __ sve_eor(z22, __ S, 496u);                       \/\/       eor     z22.s, z22.s, #0x1f0\n+    __ sve_orr(z19, __ S, 536870910u);                 \/\/       orr     z19.s, z19.s, #0x1ffffffe\n+\n+\/\/ SVEBinaryImmOp\n+    __ sve_add(z14, __ H, 22u);                        \/\/       add     z14.h, z14.h, #0x16\n+    __ sve_sub(z16, __ B, 172u);                       \/\/       sub     z16.b, z16.b, #0xac\n+    __ sve_and(z23, __ B, 62u);                        \/\/       and     z23.b, z23.b, #0x3e\n+    __ sve_eor(z17, __ H, 33279u);                     \/\/       eor     z17.h, z17.h, #0x81ff\n+    __ sve_orr(z16, __ B, 254u);                       \/\/       orr     z16.b, z16.b, #0xfe\n+\n+\/\/ SVEBinaryImmOp\n+    __ sve_add(z3, __ B, 49u);                         \/\/       add     z3.b, z3.b, #0x31\n+    __ sve_sub(z17, __ S, 110u);                       \/\/       sub     z17.s, z17.s, #0x6e\n+    __ sve_and(z12, __ S, 4290777087u);                \/\/       and     z12.s, z12.s, #0xffc00fff\n+    __ sve_eor(z19, __ S, 134217216u);                 \/\/       eor     z19.s, z19.s, #0x7fffe00\n+    __ sve_orr(z23, __ B, 254u);                       \/\/       orr     z23.b, z23.b, #0xfe\n+\n+\/\/ SVEBinaryImmOp\n+    __ sve_add(z13, __ S, 54u);                        \/\/       add     z13.s, z13.s, #0x36\n+    __ sve_sub(z0, __ B, 120u);                        \/\/       sub     z0.b, z0.b, #0x78\n+    __ sve_and(z17, __ D, 18014398509481728u);         \/\/       and     z17.d, z17.d, #0x3fffffffffff00\n+    __ sve_eor(z22, __ S, 4294709247u);                \/\/       eor     z22.s, z22.s, #0xfffc0fff\n+    __ sve_orr(z2, __ B, 225u);                        \/\/       orr     z2.b, z2.b, #0xe1\n+\n@@ -977,41 +1059,45 @@\n-    __ sve_add(z4, __ B, z6, z17);                     \/\/       add     z4.b, z6.b, z17.b\n-    __ sve_sub(z3, __ H, z15, z1);                     \/\/       sub     z3.h, z15.h, z1.h\n-    __ sve_fadd(z6, __ D, z5, z9);                     \/\/       fadd    z6.d, z5.d, z9.d\n-    __ sve_fmul(z7, __ D, z20, z22);                   \/\/       fmul    z7.d, z20.d, z22.d\n-    __ sve_fsub(z5, __ D, z10, z8);                    \/\/       fsub    z5.d, z10.d, z8.d\n-    __ sve_abs(z30, __ B, p1, z17);                    \/\/       abs     z30.b, p1\/m, z17.b\n-    __ sve_add(z11, __ B, p7, z28);                    \/\/       add     z11.b, p7\/m, z11.b, z28.b\n-    __ sve_asr(z26, __ H, p5, z28);                    \/\/       asr     z26.h, p5\/m, z26.h, z28.h\n-    __ sve_cnt(z13, __ D, p7, z16);                    \/\/       cnt     z13.d, p7\/m, z16.d\n-    __ sve_lsl(z5, __ H, p0, z13);                     \/\/       lsl     z5.h, p0\/m, z5.h, z13.h\n-    __ sve_lsr(z15, __ S, p2, z26);                    \/\/       lsr     z15.s, p2\/m, z15.s, z26.s\n-    __ sve_mul(z11, __ S, p1, z22);                    \/\/       mul     z11.s, p1\/m, z11.s, z22.s\n-    __ sve_neg(z4, __ S, p0, z19);                     \/\/       neg     z4.s, p0\/m, z19.s\n-    __ sve_not(z17, __ H, p3, z14);                    \/\/       not     z17.h, p3\/m, z14.h\n-    __ sve_smax(z2, __ S, p4, z3);                     \/\/       smax    z2.s, p4\/m, z2.s, z3.s\n-    __ sve_smin(z23, __ B, p1, z6);                    \/\/       smin    z23.b, p1\/m, z23.b, z6.b\n-    __ sve_sub(z17, __ S, p3, z27);                    \/\/       sub     z17.s, p3\/m, z17.s, z27.s\n-    __ sve_fabs(z16, __ D, p1, z2);                    \/\/       fabs    z16.d, p1\/m, z2.d\n-    __ sve_fadd(z3, __ D, p1, z6);                     \/\/       fadd    z3.d, p1\/m, z3.d, z6.d\n-    __ sve_fdiv(z19, __ D, p3, z12);                   \/\/       fdiv    z19.d, p3\/m, z19.d, z12.d\n-    __ sve_fmax(z8, __ D, p6, z19);                    \/\/       fmax    z8.d, p6\/m, z8.d, z19.d\n-    __ sve_fmin(z0, __ S, p2, z23);                    \/\/       fmin    z0.s, p2\/m, z0.s, z23.s\n-    __ sve_fmul(z19, __ D, p7, z13);                   \/\/       fmul    z19.d, p7\/m, z19.d, z13.d\n-    __ sve_fneg(z6, __ S, p0, z7);                     \/\/       fneg    z6.s, p0\/m, z7.s\n-    __ sve_frintm(z17, __ S, p6, z8);                  \/\/       frintm  z17.s, p6\/m, z8.s\n-    __ sve_frintn(z22, __ D, p5, z22);                 \/\/       frintn  z22.d, p5\/m, z22.d\n-    __ sve_frintp(z2, __ D, p0, z15);                  \/\/       frintp  z2.d, p0\/m, z15.d\n-    __ sve_fsqrt(z20, __ D, p1, z4);                   \/\/       fsqrt   z20.d, p1\/m, z4.d\n-    __ sve_fsub(z7, __ D, p0, z8);                     \/\/       fsub    z7.d, p0\/m, z7.d, z8.d\n-    __ sve_fmla(z19, __ S, p5, z4, z15);               \/\/       fmla    z19.s, p5\/m, z4.s, z15.s\n-    __ sve_fmls(z22, __ D, p2, z25, z5);               \/\/       fmls    z22.d, p2\/m, z25.d, z5.d\n-    __ sve_fnmla(z16, __ S, p3, z22, z11);             \/\/       fnmla   z16.s, p3\/m, z22.s, z11.s\n-    __ sve_fnmls(z13, __ D, p2, z20, z16);             \/\/       fnmls   z13.d, p2\/m, z20.d, z16.d\n-    __ sve_mla(z15, __ H, p1, z4, z17);                \/\/       mla     z15.h, p1\/m, z4.h, z17.h\n-    __ sve_mls(z6, __ S, p7, z4, z28);                 \/\/       mls     z6.s, p7\/m, z4.s, z28.s\n-    __ sve_and(z29, z26, z9);                          \/\/       and     z29.d, z26.d, z9.d\n-    __ sve_eor(z2, z11, z28);                          \/\/       eor     z2.d, z11.d, z28.d\n-    __ sve_orr(z7, z1, z26);                           \/\/       orr     z7.d, z1.d, z26.d\n-    __ sve_bic(z17, z14, z8);                          \/\/       bic     z17.d, z14.d, z8.d\n-    __ sve_uzp1(z21, __ S, z24, z5);                   \/\/       uzp1    z21.s, z24.s, z5.s\n-    __ sve_uzp2(z21, __ S, z17, z22);                  \/\/       uzp2    z21.s, z17.s, z22.s\n+    __ sve_add(z20, __ D, z7, z4);                     \/\/       add     z20.d, z7.d, z4.d\n+    __ sve_sub(z7, __ S, z0, z8);                      \/\/       sub     z7.s, z0.s, z8.s\n+    __ sve_fadd(z19, __ D, z22, z4);                   \/\/       fadd    z19.d, z22.d, z4.d\n+    __ sve_fmul(z9, __ D, z22, z11);                   \/\/       fmul    z9.d, z22.d, z11.d\n+    __ sve_fsub(z5, __ S, z30, z16);                   \/\/       fsub    z5.s, z30.s, z16.s\n+    __ sve_abs(z22, __ H, p3, z1);                     \/\/       abs     z22.h, p3\/m, z1.h\n+    __ sve_add(z8, __ D, p5, z16);                     \/\/       add     z8.d, p5\/m, z8.d, z16.d\n+    __ sve_and(z15, __ S, p1, z4);                     \/\/       and     z15.s, p1\/m, z15.s, z4.s\n+    __ sve_asr(z8, __ B, p1, z29);                     \/\/       asr     z8.b, p1\/m, z8.b, z29.b\n+    __ sve_cnt(z28, __ D, p4, z29);                    \/\/       cnt     z28.d, p4\/m, z29.d\n+    __ sve_eor(z9, __ H, p3, z2);                      \/\/       eor     z9.h, p3\/m, z9.h, z2.h\n+    __ sve_lsl(z28, __ B, p0, z7);                     \/\/       lsl     z28.b, p0\/m, z28.b, z7.b\n+    __ sve_lsr(z26, __ H, p5, z17);                    \/\/       lsr     z26.h, p5\/m, z26.h, z17.h\n+    __ sve_mul(z8, __ D, p4, z21);                     \/\/       mul     z8.d, p4\/m, z8.d, z21.d\n+    __ sve_neg(z5, __ S, p5, z21);                     \/\/       neg     z5.s, p5\/m, z21.s\n+    __ sve_not(z22, __ S, p4, z29);                    \/\/       not     z22.s, p4\/m, z29.s\n+    __ sve_orr(z19, __ S, p0, z4);                     \/\/       orr     z19.s, p0\/m, z19.s, z4.s\n+    __ sve_smax(z23, __ B, p1, z19);                   \/\/       smax    z23.b, p1\/m, z23.b, z19.b\n+    __ sve_smin(z23, __ B, p6, z19);                   \/\/       smin    z23.b, p6\/m, z23.b, z19.b\n+    __ sve_sub(z8, __ D, p2, z14);                     \/\/       sub     z8.d, p2\/m, z8.d, z14.d\n+    __ sve_fabs(z17, __ S, p7, z21);                   \/\/       fabs    z17.s, p7\/m, z21.s\n+    __ sve_fadd(z30, __ D, p0, z10);                   \/\/       fadd    z30.d, p0\/m, z30.d, z10.d\n+    __ sve_fdiv(z12, __ S, p0, z9);                    \/\/       fdiv    z12.s, p0\/m, z12.s, z9.s\n+    __ sve_fmax(z24, __ D, p4, z4);                    \/\/       fmax    z24.d, p4\/m, z24.d, z4.d\n+    __ sve_fmin(z6, __ D, p2, z27);                    \/\/       fmin    z6.d, p2\/m, z6.d, z27.d\n+    __ sve_fmul(z13, __ D, p4, z30);                   \/\/       fmul    z13.d, p4\/m, z13.d, z30.d\n+    __ sve_fneg(z22, __ D, p5, z30);                   \/\/       fneg    z22.d, p5\/m, z30.d\n+    __ sve_frintm(z9, __ S, p3, z19);                  \/\/       frintm  z9.s, p3\/m, z19.s\n+    __ sve_frintn(z20, __ S, p7, z9);                  \/\/       frintn  z20.s, p7\/m, z9.s\n+    __ sve_frintp(z13, __ S, p3, z19);                 \/\/       frintp  z13.s, p3\/m, z19.s\n+    __ sve_fsqrt(z24, __ S, p2, z19);                  \/\/       fsqrt   z24.s, p2\/m, z19.s\n+    __ sve_fsub(z17, __ S, p4, z16);                   \/\/       fsub    z17.s, p4\/m, z17.s, z16.s\n+    __ sve_fmad(z0, __ S, p0, z11, z7);                \/\/       fmad    z0.s, p0\/m, z11.s, z7.s\n+    __ sve_fmla(z14, __ D, p4, z4, z15);               \/\/       fmla    z14.d, p4\/m, z4.d, z15.d\n+    __ sve_fmls(z5, __ D, p0, z10, z21);               \/\/       fmls    z5.d, p0\/m, z10.d, z21.d\n+    __ sve_fnmla(z3, __ D, p0, z9, z19);               \/\/       fnmla   z3.d, p0\/m, z9.d, z19.d\n+    __ sve_fnmls(z10, __ S, p6, z3, z19);              \/\/       fnmls   z10.s, p6\/m, z3.s, z19.s\n+    __ sve_mla(z23, __ H, p7, z13, z21);               \/\/       mla     z23.h, p7\/m, z13.h, z21.h\n+    __ sve_mls(z26, __ S, p3, z17, z30);               \/\/       mls     z26.s, p3\/m, z17.s, z30.s\n+    __ sve_and(z14, z2, z29);                          \/\/       and     z14.d, z2.d, z29.d\n+    __ sve_eor(z21, z20, z7);                          \/\/       eor     z21.d, z20.d, z7.d\n+    __ sve_orr(z2, z1, z26);                           \/\/       orr     z2.d, z1.d, z26.d\n+    __ sve_bic(z9, z16, z17);                          \/\/       bic     z9.d, z16.d, z17.d\n+    __ sve_uzp1(z0, __ D, z4, z2);                     \/\/       uzp1    z0.d, z4.d, z2.d\n+    __ sve_uzp2(z14, __ S, z6, z11);                   \/\/       uzp2    z14.s, z6.s, z11.s\n@@ -1020,9 +1106,9 @@\n-    __ sve_andv(v29, __ B, p5, z19);                   \/\/       andv b29, p5, z19.b\n-    __ sve_orv(v4, __ B, p4, z23);                     \/\/       orv b4, p4, z23.b\n-    __ sve_eorv(v19, __ D, p1, z23);                   \/\/       eorv d19, p1, z23.d\n-    __ sve_smaxv(v19, __ H, p0, z8);                   \/\/       smaxv h19, p0, z8.h\n-    __ sve_sminv(v14, __ D, p6, z17);                  \/\/       sminv d14, p6, z17.d\n-    __ sve_fminv(v21, __ S, p1, z30);                  \/\/       fminv s21, p1, z30.s\n-    __ sve_fmaxv(v10, __ S, p5, z12);                  \/\/       fmaxv s10, p5, z12.s\n-    __ sve_fadda(v9, __ D, p1, z24);                   \/\/       fadda d9, p1, d9, z24.d\n-    __ sve_uaddv(v4, __ H, p6, z6);                    \/\/       uaddv d4, p6, z6.h\n+    __ sve_andv(v14, __ H, p4, z29);                   \/\/       andv h14, p4, z29.h\n+    __ sve_orv(v3, __ H, p0, z22);                     \/\/       orv h3, p0, z22.h\n+    __ sve_eorv(v3, __ B, p6, z27);                    \/\/       eorv b3, p6, z27.b\n+    __ sve_smaxv(v19, __ D, p5, z7);                   \/\/       smaxv d19, p5, z7.d\n+    __ sve_sminv(v21, __ H, p3, z5);                   \/\/       sminv h21, p3, z5.h\n+    __ sve_fminv(v25, __ D, p1, z21);                  \/\/       fminv d25, p1, z21.d\n+    __ sve_fmaxv(v17, __ S, p0, z3);                   \/\/       fmaxv s17, p0, z3.s\n+    __ sve_fadda(v19, __ S, p3, z7);                   \/\/       fadda s19, p3, s19, z7.s\n+    __ sve_uaddv(v14, __ H, p4, z17);                  \/\/       uaddv d14, p4, z17.h\n@@ -1047,7 +1133,7 @@\n-    0x14000000,     0x17ffffd7,     0x1400034e,     0x94000000,\n-    0x97ffffd4,     0x9400034b,     0x3400000a,     0x34fffa2a,\n-    0x3400690a,     0x35000008,     0x35fff9c8,     0x350068a8,\n-    0xb400000b,     0xb4fff96b,     0xb400684b,     0xb500001d,\n-    0xb5fff91d,     0xb50067fd,     0x10000013,     0x10fff8b3,\n-    0x10006793,     0x90000013,     0x36300016,     0x3637f836,\n-    0x36306716,     0x3758000c,     0x375ff7cc,     0x375866ac,\n+    0x14000000,     0x17ffffd7,     0x14000398,     0x94000000,\n+    0x97ffffd4,     0x94000395,     0x3400000a,     0x34fffa2a,\n+    0x3400724a,     0x35000008,     0x35fff9c8,     0x350071e8,\n+    0xb400000b,     0xb4fff96b,     0xb400718b,     0xb500001d,\n+    0xb5fff91d,     0xb500713d,     0x10000013,     0x10fff8b3,\n+    0x100070d3,     0x90000013,     0x36300016,     0x3637f836,\n+    0x36307056,     0x3758000c,     0x375ff7cc,     0x37586fec,\n@@ -1058,13 +1144,13 @@\n-    0x54006480,     0x54000001,     0x54fff541,     0x54006421,\n-    0x54000002,     0x54fff4e2,     0x540063c2,     0x54000002,\n-    0x54fff482,     0x54006362,     0x54000003,     0x54fff423,\n-    0x54006303,     0x54000003,     0x54fff3c3,     0x540062a3,\n-    0x54000004,     0x54fff364,     0x54006244,     0x54000005,\n-    0x54fff305,     0x540061e5,     0x54000006,     0x54fff2a6,\n-    0x54006186,     0x54000007,     0x54fff247,     0x54006127,\n-    0x54000008,     0x54fff1e8,     0x540060c8,     0x54000009,\n-    0x54fff189,     0x54006069,     0x5400000a,     0x54fff12a,\n-    0x5400600a,     0x5400000b,     0x54fff0cb,     0x54005fab,\n-    0x5400000c,     0x54fff06c,     0x54005f4c,     0x5400000d,\n-    0x54fff00d,     0x54005eed,     0x5400000e,     0x54ffefae,\n-    0x54005e8e,     0x5400000f,     0x54ffef4f,     0x54005e2f,\n+    0x54006dc0,     0x54000001,     0x54fff541,     0x54006d61,\n+    0x54000002,     0x54fff4e2,     0x54006d02,     0x54000002,\n+    0x54fff482,     0x54006ca2,     0x54000003,     0x54fff423,\n+    0x54006c43,     0x54000003,     0x54fff3c3,     0x54006be3,\n+    0x54000004,     0x54fff364,     0x54006b84,     0x54000005,\n+    0x54fff305,     0x54006b25,     0x54000006,     0x54fff2a6,\n+    0x54006ac6,     0x54000007,     0x54fff247,     0x54006a67,\n+    0x54000008,     0x54fff1e8,     0x54006a08,     0x54000009,\n+    0x54fff189,     0x540069a9,     0x5400000a,     0x54fff12a,\n+    0x5400694a,     0x5400000b,     0x54fff0cb,     0x540068eb,\n+    0x5400000c,     0x54fff06c,     0x5400688c,     0x5400000d,\n+    0x54fff00d,     0x5400682d,     0x5400000e,     0x54ffefae,\n+    0x540067ce,     0x5400000f,     0x54ffef4f,     0x5400676f,\n@@ -1102,1 +1188,1 @@\n-    0xbd1b1869,     0x58004e7b,     0x1800000b,     0xf8945060,\n+    0xbd1b1869,     0x580057bb,     0x1800000b,     0xf8945060,\n@@ -1195,24 +1281,34 @@\n-    0x042053ff,     0x047f5401,     0x25208028,     0x2538cfe0,\n-    0x2578d001,     0x25b8efe2,     0x25f8f007,     0x2538dfea,\n-    0x25b8dfeb,     0xa400a3e0,     0xa420a7e0,     0xa4484be0,\n-    0xa467afe0,     0xa4a8a7ea,     0xa547a814,     0xa4084ffe,\n-    0xa55c53e0,     0xa5e1540b,     0xe400fbf6,     0xe408ffff,\n-    0xe420e7e0,     0xe4484be0,     0xe460efe0,     0xe547e400,\n-    0xe4014be0,     0xe4a84fe0,     0xe5f15000,     0x858043e0,\n-    0x85a043ff,     0xe59f5d08,     0x0420e3e9,     0x0460e3ea,\n-    0x04a0e3eb,     0x04e0e3ec,     0x25104042,     0x25104871,\n-    0x25904861,     0x25904c92,     0x05344020,     0x05744041,\n-    0x05b44062,     0x05f44083,     0x252c8840,     0x253c1420,\n-    0x25681572,     0x25a21ce3,     0x25ea1e34,     0x0522c020,\n-    0x05e6c0a4,     0x2401a001,     0x2443a051,     0x24858881,\n-    0x24c78cd1,     0x24850891,     0x24c70cc1,     0x250f9001,\n-    0x25508051,     0x25802491,     0x25df28c1,     0x25850c81,\n-    0x251e10d1,     0x65816001,     0x65c36051,     0x65854891,\n-    0x65c74cc1,     0x05733820,     0x05b238a4,     0x05f138e6,\n-    0x0570396a,     0x65d0a001,     0x65d6a443,     0x65d4a826,\n-    0x6594ac26,     0x6554ac26,     0x6556ac26,     0x6552ac26,\n-    0x65cbac85,     0x65caac01,     0x65dea833,     0x659ca509,\n-    0x65d8a801,     0x65dcac01,     0x655cb241,     0x0520a1e0,\n-    0x0521a601,     0x052281e0,     0x05238601,     0x04a14026,\n-    0x0568aca7,     0x05b23230,     0x853040af,     0xc5b040af,\n-    0xe57080af,     0xe5b080af,     0x1e601000,     0x1e603000,\n+    0x04038100,     0x040381a0,     0x040387e1,     0x04438be2,\n+    0x04c38fe3,     0x040181e0,     0x04018100,     0x04018621,\n+    0x04418b22,     0x04418822,     0x04818c23,     0x040081e0,\n+    0x04008120,     0x04008761,     0x04008621,     0x04408822,\n+    0x04808c23,     0x042053ff,     0x047f5401,     0x25208028,\n+    0x2538cfe0,     0x2578d001,     0x25b8efe2,     0x25f8f007,\n+    0x2538dfea,     0x25b8dfeb,     0xa400a3e0,     0xa420a7e0,\n+    0xa4484be0,     0xa467afe0,     0xa4a8a7ea,     0xa547a814,\n+    0xa4084ffe,     0xa55c53e0,     0xa5e1540b,     0xe400fbf6,\n+    0xe408ffff,     0xe420e7e0,     0xe4484be0,     0xe460efe0,\n+    0xe547e400,     0xe4014be0,     0xe4a84fe0,     0xe5f15000,\n+    0x858043e0,     0x85a043ff,     0xe59f5d08,     0x0420e3e9,\n+    0x0460e3ea,     0x04a0e3eb,     0x04e0e3ec,     0x25104042,\n+    0x25104871,     0x25904861,     0x25904c92,     0x05344020,\n+    0x05744041,     0x05b44062,     0x05f44083,     0x252c8840,\n+    0x253c1420,     0x25681572,     0x25a21ce3,     0x25ea1e34,\n+    0x0522c020,     0x05e6c0a4,     0x2401a001,     0x2443a051,\n+    0x24858881,     0x24c78cd1,     0x24850891,     0x24c70cc1,\n+    0x250f9001,     0x25508051,     0x25802491,     0x25df28c1,\n+    0x25850c81,     0x251e10d1,     0x65816001,     0x65c36051,\n+    0x65854891,     0x65c74cc1,     0x05733820,     0x05b238a4,\n+    0x05f138e6,     0x0570396a,     0x65d0a001,     0x65d6a443,\n+    0x65d4a826,     0x6594ac26,     0x6554ac26,     0x6556ac26,\n+    0x6552ac26,     0x65cbac85,     0x65caac01,     0x65dea833,\n+    0x659ca509,     0x65d8a801,     0x65dcac01,     0x655cb241,\n+    0x0520a1e0,     0x0521a601,     0x052281e0,     0x05238601,\n+    0x04a14026,     0x0568aca7,     0x05b23230,     0x853040af,\n+    0xc5b040af,     0xe57080af,     0xe5b080af,     0x25034440,\n+    0x254054c4,     0x25034640,     0x25415a05,     0x25834440,\n+    0x25c54489,     0x250b5d3a,     0x2550dc20,     0x2518e3e1,\n+    0x2558e3e2,     0x2598e3e3,     0x25d8e3e4,     0x2518e407,\n+    0x05214800,     0x05614800,     0x05a14800,     0x05e14800,\n+    0x05214c00,     0x05614c00,     0x05a14c00,     0x05e14c00,\n+    0x05304001,     0x05314001,     0x1e601000,     0x1e603000,\n@@ -1246,14 +1342,22 @@\n-    0xcec0835a,     0xce608ad8,     0x043100c4,     0x046105e3,\n-    0x65c900a6,     0x65d60a87,     0x65c80545,     0x0416a63e,\n-    0x04001f8b,     0x0450979a,     0x04dabe0d,     0x045381a5,\n-    0x04918b4f,     0x049006cb,     0x0497a264,     0x045eadd1,\n-    0x04881062,     0x040a04d7,     0x04810f71,     0x04dca450,\n-    0x65c084c3,     0x65cd8d93,     0x65c69a68,     0x65878ae0,\n-    0x65c29db3,     0x049da0e6,     0x6582b911,     0x65c0b6d6,\n-    0x65c1a1e2,     0x65cda494,     0x65c18107,     0x65af1493,\n-    0x65e52b36,     0x65ab4ed0,     0x65f06a8d,     0x0451448f,\n-    0x049c7c86,     0x0429335d,     0x04bc3162,     0x047a3027,\n-    0x04e831d1,     0x05a56b15,     0x05b66e35,     0x041a367d,\n-    0x041832e4,     0x04d926f3,     0x04482113,     0x04ca3a2e,\n-    0x658727d5,     0x6586358a,     0x65d82709,     0x044138c4,\n-\n+    0xcec0835a,     0xce608ad8,     0x2520d264,     0x2521cf80,\n+    0x058074c1,     0x054242c9,     0x05004476,     0x25a0df08,\n+    0x25a1c206,     0x0583288b,     0x05401c3a,     0x05027e8d,\n+    0x2520ce05,     0x25a1cb0a,     0x0580989a,     0x0540e096,\n+    0x0500fb73,     0x2560c2ce,     0x2521d590,     0x05803e97,\n+    0x05400d31,     0x05003ed0,     0x2520c623,     0x25a1cdd1,\n+    0x058052ac,     0x0540ba33,     0x05003ed7,     0x25a0c6cd,\n+    0x2521cf00,     0x0583c5b1,     0x05407336,     0x05001e62,\n+    0x04e400f4,     0x04a80407,     0x65c402d3,     0x65cb0ac9,\n+    0x659007c5,     0x0456ac36,     0x04c01608,     0x049a048f,\n+    0x041087a8,     0x04dab3bc,     0x04590c49,     0x041380fc,\n+    0x0451963a,     0x04d012a8,     0x0497b6a5,     0x049eb3b6,\n+    0x04980093,     0x04080677,     0x040a1a77,     0x04c109c8,\n+    0x049cbeb1,     0x65c0815e,     0x658d812c,     0x65c69098,\n+    0x65c78b66,     0x65c293cd,     0x04ddb7d6,     0x6582ae69,\n+    0x6580bd34,     0x6581ae6d,     0x658daa78,     0x65819211,\n+    0x65a78160,     0x65ef108e,     0x65f52145,     0x65f34123,\n+    0x65b3786a,     0x04555db7,     0x049e6e3a,     0x043d304e,\n+    0x04a73295,     0x047a3022,     0x04f13209,     0x05e26880,\n+    0x05ab6cce,     0x045a33ae,     0x045822c3,     0x04193b63,\n+    0x04c834f3,     0x044a2cb5,     0x65c726b9,     0x65862071,\n+    0x65982cf3,     0x0441322e,\n","filename":"test\/hotspot\/gtest\/aarch64\/asmtest.out.h","additions":213,"deletions":109,"binary":false,"changes":322,"status":"modified"},{"patch":"@@ -37,1 +37,1 @@\n-  class G1CountCardsClosure : public G1CardSet::G1CardSetCardIterator {\n+  class G1CountCardsClosure : public G1CardSet::CardClosure {\n@@ -85,1 +85,1 @@\n-  static void iterate_cards(G1CardSet* card_set, G1CardSet::G1CardSetCardIterator* cl);\n+  static void iterate_cards(G1CardSet* card_set, G1CardSet::CardClosure* cl);\n@@ -104,1 +104,1 @@\n-class G1CheckCardClosure : public G1CardSet::G1CardSetCardIterator {\n+class G1CheckCardClosure : public G1CardSet::CardClosure {\n@@ -166,1 +166,1 @@\n-class G1CountCardsOccupied : public G1CardSet::G1CardSetPtrIterator {\n+class G1CountCardsOccupied : public G1CardSet::CardSetPtrClosure {\n@@ -181,1 +181,1 @@\n-  class CheckIterator : public G1CardSet::G1CardSetCardIterator {\n+  class CheckIterator : public G1CardSet::CardClosure {\n@@ -213,1 +213,2 @@\n-                                CardsPerRegion);\n+                                CardsPerRegion,\n+                                0);\n@@ -431,1 +432,2 @@\n-                                CardsPerRegion);\n+                                CardsPerRegion,\n+                                0);\n","filename":"test\/hotspot\/gtest\/gc\/g1\/test_g1CardSet.cpp","additions":9,"deletions":7,"binary":false,"changes":16,"status":"modified"},{"patch":"@@ -40,0 +40,2 @@\n+\n+\n@@ -134,2 +136,5 @@\n-  ASSERT_TRUE(false); \/\/ we should not be here; we should have crashed\n-  os::free(p2);\n+\n+  \/\/ Still here?\n+  tty->print_cr(\"NMT did not detect corruption on os::realloc?\");\n+  \/\/ Note: don't use ASSERT here, that does not work as expected in death tests. Just\n+  \/\/ let the test run its course, it should notice something is amiss.\n@@ -137,1 +142,1 @@\n-static void test_corruption_on_realloc_growing()    { test_corruption_on_realloc(0x101, 0x102); }\n+static void test_corruption_on_realloc_growing()    { test_corruption_on_realloc(0x10, 0x11); }\n@@ -139,1 +144,1 @@\n-static void test_corruption_on_realloc_shrinking()  { test_corruption_on_realloc(0x102, 0x101); }\n+static void test_corruption_on_realloc_shrinking()  { test_corruption_on_realloc(0x11, 0x10); }\n","filename":"test\/hotspot\/gtest\/nmt\/test_nmt_buffer_overflow_detection.cpp","additions":9,"deletions":4,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -108,0 +108,1 @@\n+runtime\/jni\/checked\/TestPrimitiveArrayCriticalWithBadParam.java 8277350 macosx-x64\n@@ -121,4 +122,7 @@\n-serviceability\/sa\/ClhsdbCDSCore.java 8269982 macosx-aarch64\n-serviceability\/sa\/ClhsdbFindPC.java#xcomp-core 8269982 macosx-aarch64\n-serviceability\/sa\/ClhsdbFindPC.java#no-xcomp-core 8269982 macosx-aarch64\n-serviceability\/sa\/ClhsdbPstack.java#core 8269982 macosx-aarch64\n+serviceability\/sa\/ClhsdbCDSCore.java 8269982,8267433 macosx-aarch64,macosx-x64\n+serviceability\/sa\/ClhsdbFindPC.java#xcomp-core 8269982,8267433 macosx-aarch64,macosx-x64\n+serviceability\/sa\/ClhsdbFindPC.java#no-xcomp-core 8269982,8267433 macosx-aarch64,macosx-x64\n+serviceability\/sa\/ClhsdbPmap.java#core 8267433 macosx-x64\n+serviceability\/sa\/ClhsdbPstack.java#core 8269982,8267433 macosx-aarch64,macosx-x64\n+serviceability\/sa\/TestJmapCore.java 8267433 macosx-x64\n+serviceability\/sa\/TestJmapCoreMetaspace.java 8267433 macosx-x64\n@@ -161,1 +165,0 @@\n-vmTestbase\/nsk\/jvmti\/RedefineClasses\/StressRedefineWithoutBytecodeCorruption\/TestDescription.java 8272800 generic-all\n","filename":"test\/hotspot\/jtreg\/ProblemList.txt","additions":8,"deletions":5,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -214,1 +214,0 @@\n-  -gc\/CriticalNativeArgs.java \\\n@@ -229,3 +228,1 @@\n-tier2_gc_epsilon = \\\n-  gc\/CriticalNativeArgs.java \\\n-  gc\/stress\/CriticalNativeStress.java\n+tier2_gc_epsilon =\n@@ -270,2 +267,0 @@\n-  gc\/CriticalNativeArgs.java \\\n-  gc\/stress\/CriticalNativeStress.java \\\n","filename":"test\/hotspot\/jtreg\/TEST.groups","additions":1,"deletions":6,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -0,0 +1,345 @@\n+\/*\n+ * Copyright (c) 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+package compiler.c2.irTests;\n+\n+import jdk.test.lib.Asserts;\n+import compiler.lib.ir_framework.*;\n+\n+\/*\n+ * @test\n+ * @bug 8276162\n+ * @summary Test that unsigned comparison transformation works as intended.\n+ * @library \/test\/lib \/\n+ * @run driver compiler.c2.irTests.TestUnsignedComparison\n+ *\/\n+public class TestUnsignedComparison {\n+    private static final String CMP_REGEX = \"(\\\\d+(\\\\s){2}(\" + \"Cmp(I|L)\" + \".*)+(\\\\s){2}===.*)\";\n+    private static final String CMPU_REGEX = \"(\\\\d+(\\\\s){2}(\" + \"Cmp(U|UL)\" + \".*)+(\\\\s){2}===.*)\";\n+    private static final String ADD_REGEX = \"(\\\\d+(\\\\s){2}(\" + \"Add(I|L)\" + \".*)+(\\\\s){2}===.*)\";\n+\n+    private static final int INT_MIN = Integer.MIN_VALUE;\n+    private static final long LONG_MIN = Long.MIN_VALUE;\n+\n+    \/\/ Integers are sorted in unsignedly increasing order\n+    private static final int[] INT_DATA = {\n+        0,\n+        1,\n+        2,\n+        3,\n+        0x8000_0000,\n+        0x8000_0001,\n+        0x8000_0002,\n+        0x8000_0003,\n+        0xFFFF_FFFE,\n+        0xFFFF_FFFF,\n+    };\n+\n+    \/\/ Longs are sorted in unsignedly increasing order\n+    private static final long[] LONG_DATA = {\n+        0L,\n+        1L,\n+        2L,\n+        3L,\n+        0x00000000_80000000L,\n+        0x00000000_FFFFFFFFL,\n+        0x00000001_00000000L,\n+        0x80000000_00000000L,\n+        0x80000000_00000001L,\n+        0x80000000_00000002L,\n+        0x80000000_00000003L,\n+        0x80000000_80000000L,\n+        0xFFFFFFFF_FFFFFFFEL,\n+        0xFFFFFFFF_FFFFFFFFL,\n+    };\n+\n+    \/\/ Constants to compare against, add MIN_VALUE beforehand for convenience\n+    private static final int CONST_INDEX = 6;\n+    private static final int INT_CONST = INT_DATA[CONST_INDEX] + INT_MIN;\n+    private static final long LONG_CONST = LONG_DATA[CONST_INDEX] + LONG_MIN;\n+\n+    public static void main(String[] args) {\n+        TestFramework framework = new TestFramework();\n+        framework.start();\n+    }\n+\n+    @Test\n+    @IR(failOn = {CMP_REGEX, ADD_REGEX})\n+    @IR(counts = {CMPU_REGEX, \"1\"})\n+    public boolean testIntVarEQ(int x, int y) {\n+        return x + INT_MIN == y + INT_MIN;\n+    }\n+\n+    @Test\n+    @IR(failOn = {CMP_REGEX, ADD_REGEX})\n+    @IR(counts = {CMPU_REGEX, \"1\"})\n+    public boolean testIntVarNE(int x, int y) {\n+        return x + INT_MIN != y + INT_MIN;\n+    }\n+\n+    @Test\n+    @IR(failOn = {CMP_REGEX, ADD_REGEX})\n+    @IR(counts = {CMPU_REGEX, \"1\"})\n+    public boolean testIntVarLT(int x, int y) {\n+        return x + INT_MIN < y + INT_MIN;\n+    }\n+\n+    @Test\n+    @IR(failOn = {CMP_REGEX, ADD_REGEX})\n+    @IR(counts = {CMPU_REGEX, \"1\"})\n+    public boolean testIntVarLE(int x, int y) {\n+        return x + INT_MIN <= y + INT_MIN;\n+    }\n+\n+    @Test\n+    @IR(failOn = {CMP_REGEX, ADD_REGEX})\n+    @IR(counts = {CMPU_REGEX, \"1\"})\n+    public boolean testIntVarGT(int x, int y) {\n+        return x + INT_MIN > y + INT_MIN;\n+    }\n+\n+    @Test\n+    @IR(failOn = {CMP_REGEX, ADD_REGEX})\n+    @IR(counts = {CMPU_REGEX, \"1\"})\n+    public boolean testIntVarGE(int x, int y) {\n+        return x + INT_MIN >= y + INT_MIN;\n+    }\n+\n+    @Run(test = {\"testIntVarEQ\", \"testIntVarNE\",\n+                 \"testIntVarLT\", \"testIntVarLE\",\n+                 \"testIntVarGT\", \"testIntVarGE\"})\n+    public void checkTestIntVar() {\n+        \/\/ Verify the transformation \"cmp (add X min_jint) (add Y min_jint)\"\n+        \/\/ to \"cmpu X Y\"\n+        for (int i = 0; i < INT_DATA.length; i++) {\n+            for (int j = 0; j < INT_DATA.length; j++) {\n+                Asserts.assertEquals(testIntVarEQ(INT_DATA[i], INT_DATA[j]),\n+                                     i == j);\n+                Asserts.assertEquals(testIntVarNE(INT_DATA[i], INT_DATA[j]),\n+                                     i != j);\n+                Asserts.assertEquals(testIntVarLT(INT_DATA[i], INT_DATA[j]),\n+                                     i <  j);\n+                Asserts.assertEquals(testIntVarLE(INT_DATA[i], INT_DATA[j]),\n+                                     i <= j);\n+                Asserts.assertEquals(testIntVarGT(INT_DATA[i], INT_DATA[j]),\n+                                     i >  j);\n+                Asserts.assertEquals(testIntVarGE(INT_DATA[i], INT_DATA[j]),\n+                                     i >= j);\n+            }\n+        }\n+    }\n+\n+    @Test\n+    @IR(failOn = {CMP_REGEX, ADD_REGEX})\n+    @IR(counts = {CMPU_REGEX, \"1\"})\n+    public boolean testIntConEQ(int x) {\n+        return x + INT_MIN == INT_CONST;\n+    }\n+\n+    @Test\n+    @IR(failOn = {CMP_REGEX, ADD_REGEX})\n+    @IR(counts = {CMPU_REGEX, \"1\"})\n+    public boolean testIntConNE(int x) {\n+        return x + INT_MIN != INT_CONST;\n+    }\n+\n+    @Test\n+    @IR(failOn = {CMP_REGEX, ADD_REGEX})\n+    @IR(counts = {CMPU_REGEX, \"1\"})\n+    public boolean testIntConLT(int x) {\n+        return x + INT_MIN < INT_CONST;\n+    }\n+\n+    @Test\n+    @IR(failOn = {CMP_REGEX, ADD_REGEX})\n+    @IR(counts = {CMPU_REGEX, \"1\"})\n+    public boolean testIntConLE(int x) {\n+        return x + INT_MIN <= INT_CONST;\n+    }\n+\n+    @Test\n+    @IR(failOn = {CMP_REGEX, ADD_REGEX})\n+    @IR(counts = {CMPU_REGEX, \"1\"})\n+    public boolean testIntConGT(int x) {\n+        return x + INT_MIN > INT_CONST;\n+    }\n+\n+    @Test\n+    @IR(failOn = {CMP_REGEX, ADD_REGEX})\n+    @IR(counts = {CMPU_REGEX, \"1\"})\n+    public boolean testIntConGE(int x) {\n+        return x + INT_MIN >= INT_CONST;\n+    }\n+\n+    @Run(test = {\"testIntConEQ\", \"testIntConNE\",\n+                 \"testIntConLT\", \"testIntConLE\",\n+                 \"testIntConGT\", \"testIntConGE\"})\n+    public void checkTestIntCon() {\n+        \/\/ Verify the transformation \"cmp (add X min_jint) c\"\n+        \/\/ to \"cmpu X (c + min_jint)\"\n+        for (int i = 0; i < INT_DATA.length; i++) {\n+            Asserts.assertEquals(testIntConEQ(INT_DATA[i]),\n+                                 i == CONST_INDEX);\n+            Asserts.assertEquals(testIntConNE(INT_DATA[i]),\n+                                 i != CONST_INDEX);\n+            Asserts.assertEquals(testIntConLT(INT_DATA[i]),\n+                                 i <  CONST_INDEX);\n+            Asserts.assertEquals(testIntConLE(INT_DATA[i]),\n+                                 i <= CONST_INDEX);\n+            Asserts.assertEquals(testIntConGT(INT_DATA[i]),\n+                                 i >  CONST_INDEX);\n+            Asserts.assertEquals(testIntConGE(INT_DATA[i]),\n+                                 i >= CONST_INDEX);\n+        }\n+    }\n+\n+    @Test\n+    @IR(failOn = {CMP_REGEX, ADD_REGEX})\n+    @IR(counts = {CMPU_REGEX, \"1\"})\n+    public boolean testLongVarEQ(long x, long y) {\n+        return x + LONG_MIN == y + LONG_MIN;\n+    }\n+\n+    @Test\n+    @IR(failOn = {CMP_REGEX, ADD_REGEX})\n+    @IR(counts = {CMPU_REGEX, \"1\"})\n+    public boolean testLongVarNE(long x, long y) {\n+        return x + LONG_MIN != y + LONG_MIN;\n+    }\n+\n+    @Test\n+    @IR(failOn = {CMP_REGEX, ADD_REGEX})\n+    @IR(counts = {CMPU_REGEX, \"1\"})\n+    public boolean testLongVarLT(long x, long y) {\n+        return x + LONG_MIN < y + LONG_MIN;\n+    }\n+\n+    @Test\n+    @IR(failOn = {CMP_REGEX, ADD_REGEX})\n+    @IR(counts = {CMPU_REGEX, \"1\"})\n+    public boolean testLongVarLE(long x, long y) {\n+        return x + LONG_MIN <= y + LONG_MIN;\n+    }\n+\n+    @Test\n+    @IR(failOn = {CMP_REGEX, ADD_REGEX})\n+    @IR(counts = {CMPU_REGEX, \"1\"})\n+    public boolean testLongVarGT(long x, long y) {\n+        return x + LONG_MIN > y + LONG_MIN;\n+    }\n+\n+    @Test\n+    @IR(failOn = {CMP_REGEX, ADD_REGEX})\n+    @IR(counts = {CMPU_REGEX, \"1\"})\n+    public boolean testLongVarGE(long x, long y) {\n+        return x + LONG_MIN >= y + LONG_MIN;\n+    }\n+\n+    @Run(test = {\"testLongVarEQ\", \"testLongVarNE\",\n+                 \"testLongVarLT\", \"testLongVarLE\",\n+                 \"testLongVarGT\", \"testLongVarGE\"})\n+    public void checkTestLongVar() {\n+        \/\/ Verify the transformation \"cmp (add X min_jlong) (add Y min_jlong)\"\n+        \/\/ to \"cmpu X Y\"\n+        for (int i = 0; i < LONG_DATA.length; i++) {\n+            for (int j = 0; j < LONG_DATA.length; j++) {\n+                Asserts.assertEquals(testLongVarEQ(LONG_DATA[i], LONG_DATA[j]),\n+                                     i == j);\n+                Asserts.assertEquals(testLongVarNE(LONG_DATA[i], LONG_DATA[j]),\n+                                     i != j);\n+                Asserts.assertEquals(testLongVarLT(LONG_DATA[i], LONG_DATA[j]),\n+                                     i <  j);\n+                Asserts.assertEquals(testLongVarLE(LONG_DATA[i], LONG_DATA[j]),\n+                                     i <= j);\n+                Asserts.assertEquals(testLongVarGT(LONG_DATA[i], LONG_DATA[j]),\n+                                     i >  j);\n+                Asserts.assertEquals(testLongVarGE(LONG_DATA[i], LONG_DATA[j]),\n+                                     i >= j);\n+            }\n+        }\n+    }\n+\n+    @Test\n+    @IR(failOn = {CMP_REGEX, ADD_REGEX})\n+    @IR(counts = {CMPU_REGEX, \"1\"})\n+    public boolean testLongConEQ(long x) {\n+        return x + LONG_MIN == LONG_CONST;\n+    }\n+\n+    @Test\n+    @IR(failOn = {CMP_REGEX, ADD_REGEX})\n+    @IR(counts = {CMPU_REGEX, \"1\"})\n+    public boolean testLongConNE(long x) {\n+        return x + LONG_MIN != LONG_CONST;\n+    }\n+\n+    @Test\n+    @IR(failOn = {CMP_REGEX, ADD_REGEX})\n+    @IR(counts = {CMPU_REGEX, \"1\"})\n+    public boolean testLongConLT(long x) {\n+        return x + LONG_MIN < LONG_CONST;\n+    }\n+\n+    @Test\n+    @IR(failOn = {CMP_REGEX, ADD_REGEX})\n+    @IR(counts = {CMPU_REGEX, \"1\"})\n+    public boolean testLongConLE(long x) {\n+        return x + LONG_MIN <= LONG_CONST;\n+    }\n+\n+    @Test\n+    @IR(failOn = {CMP_REGEX, ADD_REGEX})\n+    @IR(counts = {CMPU_REGEX, \"1\"})\n+    public boolean testLongConGT(long x) {\n+        return x + LONG_MIN > LONG_CONST;\n+    }\n+\n+    @Test\n+    @IR(failOn = {CMP_REGEX, ADD_REGEX})\n+    @IR(counts = {CMPU_REGEX, \"1\"})\n+    public boolean testLongConGE(long x) {\n+        return x + LONG_MIN >= LONG_CONST;\n+    }\n+\n+    @Run(test = {\"testLongConEQ\", \"testLongConNE\",\n+                 \"testLongConLT\", \"testLongConLE\",\n+                 \"testLongConGT\", \"testLongConGE\"})\n+    public void checkTestLongConGE() {\n+        \/\/ Verify the transformation \"cmp (add X min_jlong) c\"\n+        \/\/ to \"cmpu X (c + min_jlong)\"\n+        for (int i = 0; i < LONG_DATA.length; i++) {\n+            Asserts.assertEquals(testLongConEQ(LONG_DATA[i]),\n+                                 i == CONST_INDEX);\n+            Asserts.assertEquals(testLongConNE(LONG_DATA[i]),\n+                                 i != CONST_INDEX);\n+            Asserts.assertEquals(testLongConLT(LONG_DATA[i]),\n+                                 i <  CONST_INDEX);\n+            Asserts.assertEquals(testLongConLE(LONG_DATA[i]),\n+                                 i <= CONST_INDEX);\n+            Asserts.assertEquals(testLongConGT(LONG_DATA[i]),\n+                                 i >  CONST_INDEX);\n+            Asserts.assertEquals(testLongConGE(LONG_DATA[i]),\n+                                 i >= CONST_INDEX);\n+        }\n+    }\n+}\n","filename":"test\/hotspot\/jtreg\/compiler\/c2\/irTests\/TestUnsignedComparison.java","additions":345,"deletions":0,"binary":false,"changes":345,"status":"added"},{"patch":"@@ -28,1 +28,1 @@\n- * @summary testing of ciReplay with inlining\n+ * @summary testing of ciReplay with nested BoundMethodHandles\n","filename":"test\/hotspot\/jtreg\/compiler\/ciReplay\/TestLambdas.java","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2012, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2012, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -57,0 +57,1 @@\n+    byte[] a3 = new byte[ARRLEN];\n@@ -97,0 +98,10 @@\n+      test_addImm127(a1, a2);\n+      test_addImm(a1, a2, a3);\n+      test_addImm256(a1, a2);\n+      test_addImmNeg128(a1, a2);\n+      test_addImmNeg129(a1, a2);\n+      test_subImm(a1, a2, a3);\n+      test_andImm21(a1, a2);\n+      test_andImm7(a1, a2);\n+      test_orImm(a1, a2);\n+      test_xorImm(a1, a2, a3);\n@@ -490,0 +501,71 @@\n+      byte base = (byte) 10;\n+      for (int i = 0; i < ARRLEN; i++) {\n+        a1[i] = (byte) 10;\n+      }\n+      byte golden = (byte)(base + 127);\n+      test_addImm127(a1, a2);\n+      for (int i=0; i<ARRLEN; i++) {\n+        errn += verify(\"test_addImm127: a2\", i, a2[i], golden);\n+      }\n+      test_addImm(a1, a2, a3);\n+      golden = (byte)(base + 8);\n+      for (int i=0; i<ARRLEN; i++) {\n+        errn += verify(\"test_addImm: a2\", i, a2[i], golden);\n+      }\n+      golden = (byte) (base + 255);\n+      for (int i=0; i<ARRLEN; i++) {\n+        errn += verify(\"test_addImm: a3\", i, a3[i], golden);\n+      }\n+      test_addImm256(a1, a2);\n+      golden = (byte)(base + 256);\n+      for (int i=0; i<ARRLEN; i++) {\n+        errn += verify(\"test_addImm256: a3\", i, a2[i], golden);\n+      }\n+      test_addImmNeg128(a1, a2);\n+      golden = (byte)(base + (-128));\n+      for (int i=0; i<ARRLEN; i++) {\n+        errn += verify(\"test_addImmNeg128: a2\", i, a2[i], golden);\n+      }\n+      test_addImmNeg129(a1, a2);\n+      golden = (byte)(base + (-129));\n+      for (int i=0; i<ARRLEN; i++) {\n+        errn += verify(\"test_addImmNeg129: a2\", i, a2[i], golden);\n+      }\n+      \/\/ Reset for sub test\n+      base = (byte) 120;\n+      for (int i = 0; i < ARRLEN; i++) {\n+        a1[i] = (byte) 120;\n+      }\n+      test_subImm(a1, a2, a3);\n+      golden = (byte) (base - 56);\n+      for (int i=0; i<ARRLEN; i++) {\n+        errn += verify(\"test_subImm: a2\", i, a2[i], golden);\n+      }\n+      golden = (byte) (base - 256);\n+      for (int i=0; i<ARRLEN; i++) {\n+        errn += verify(\"test_subImm: a3\", i, a3[i], golden);\n+      }\n+      test_andImm21(a1, a2);\n+      golden = (byte) (base & 21);\n+      for (int i=0; i<ARRLEN; i++) {\n+        errn += verify(\"test_andImm21: a2\", i, a2[i], golden);\n+      }\n+      test_andImm7(a1, a2);\n+      golden = (byte) (base & 7);\n+      for (int i=0; i<ARRLEN; i++) {\n+        errn += verify(\"test_andImm7: a2\", i, a2[i], golden);\n+      }\n+      test_orImm(a1, a2);\n+      golden = (byte) (base | 3);\n+      for (int i=0; i<ARRLEN; i++) {\n+        errn += verify(\"test_orImm: a2\", i, a2[i], golden);\n+      }\n+      test_xorImm(a1, a2, a3);\n+      golden = (byte) (base ^ 127);\n+      for (int i=0; i<ARRLEN; i++) {\n+        errn += verify(\"test_xorImm: a2\", i, a2[i], golden);\n+      }\n+      golden = (byte) (base ^ 255);\n+      for (int i=0; i<ARRLEN; i++) {\n+        errn += verify(\"test_xorImm: a3\", i, a3[i], golden);\n+      }\n@@ -733,0 +815,53 @@\n+    start = System.currentTimeMillis();\n+    for (int i=0; i<ITERS; i++) {\n+      test_addImm127(a1, a2);\n+    }\n+    end = System.currentTimeMillis();\n+    System.out.println(\"test_addImm127: \" + (end - start));\n+    start = System.currentTimeMillis();\n+    for (int i=0; i<ITERS; i++) {\n+      test_addImm(a1, a2, a3);\n+    }\n+    end = System.currentTimeMillis();\n+    System.out.println(\"test_addImm: \" + (end - start));\n+    start = System.currentTimeMillis();\n+    for (int i=0; i<ITERS; i++) {\n+      test_addImm256(a1, a2);\n+    }\n+    end = System.currentTimeMillis();\n+    System.out.println(\"test_addImm256: \" + (end - start));\n+    start = System.currentTimeMillis();\n+    for (int i=0; i<ITERS; i++) {\n+      test_addImmNeg128(a1, a2);\n+    }\n+    end = System.currentTimeMillis();\n+    System.out.println(\"test_addImmNeg128: \" + (end - start));\n+    start = System.currentTimeMillis();\n+    for (int i=0; i<ITERS; i++) {\n+      test_addImmNeg129(a1, a2);\n+    }\n+    end = System.currentTimeMillis();\n+    System.out.println(\"test_addImmNeg129: \" + (end - start));\n+    start = System.currentTimeMillis();\n+    for (int i=0; i<ITERS; i++) {\n+      test_subImm(a1, a2, a3);\n+    }\n+    end = System.currentTimeMillis();\n+    System.out.println(\"test_subImm: \" + (end - start));\n+    start = System.currentTimeMillis();\n+    for (int i=0; i<ITERS; i++) {\n+      test_andImm7(a1, a2);\n+    }\n+    end = System.currentTimeMillis();\n+    System.out.println(\"test_andImm7: \" + (end - start));\n+    start = System.currentTimeMillis();\n+    for (int i=0; i<ITERS; i++) {\n+      test_orImm(a1, a2);\n+    }\n+    end = System.currentTimeMillis();\n+    System.out.println(\"test_orImm: \" + (end - start));\n+    start = System.currentTimeMillis();\n+    for (int i=0; i<ITERS; i++) {\n+      test_xorImm(a1, a2, a3);\n+    }\n+    end = System.currentTimeMillis();\n@@ -948,0 +1083,53 @@\n+  static void test_addImm127(byte[] a, byte[] b) {\n+    for (int i = 0; i < a.length; i++) {\n+      b[i] = (byte) (a[i] + 127);\n+    }\n+  }\n+  static void test_addImm(byte[] a, byte[] b, byte[] c) {\n+    for (int i = 0; i < a.length; i++) {\n+      b[i] = (byte) (a[i] + 8);\n+      c[i] = (byte) (a[i] + 255);\n+    }\n+  }\n+  static void test_addImm256(byte[] a, byte[] b) {\n+    for (int i = 0; i < a.length; i++) {\n+      b[i] = (byte) (a[i] + 256);\n+    }\n+  }\n+  static void test_addImmNeg128(byte[] a, byte[] b) {\n+    for (int i = 0; i < a.length; i++) {\n+      b[i] = (byte) (a[i] + (-128));\n+    }\n+  }\n+  static void test_addImmNeg129(byte[] a, byte[] b) {\n+    for (int i = 0; i < a.length; i++) {\n+      b[i] = (byte) (a[i] + (-129));\n+    }\n+  }\n+  static void test_subImm(byte[] a, byte[] b, byte[] c) {\n+    for (int i = 0; i < a.length; i++) {\n+      b[i] = (byte) (a[i] - 56);\n+      c[i] = (byte) (a[i] - 256);\n+    }\n+  }\n+  static void test_andImm21(byte[] a, byte[] b) {\n+    for (int i = 0; i < a.length; i++) {\n+      b[i] = (byte) (a[i] & 21);\n+    }\n+  }\n+  static void test_andImm7(byte[] a, byte[] b) {\n+    for (int i = 0; i < a.length; i++) {\n+      b[i] = (byte) (a[i] & 7);\n+    }\n+  }\n+  static void test_orImm(byte[] a, byte[] b) {\n+    for (int i = 0; i < a.length; i++) {\n+      b[i] = (byte) (a[i] | 3);\n+    }\n+  }\n+  static void test_xorImm(byte[] a, byte[] b, byte[] c) {\n+    for (int i = 0; i < a.length; i++) {\n+      b[i] = (byte) (a[i] ^ 127);\n+      c[i] = (byte) (a[i] ^ 255);\n+    }\n+  }\n","filename":"test\/hotspot\/jtreg\/compiler\/codegen\/TestByteVect.java","additions":189,"deletions":1,"binary":false,"changes":190,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2012, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2012, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -57,0 +57,1 @@\n+    char[] a3 = new char[ARRLEN];\n@@ -97,0 +98,7 @@\n+      test_addImm129(a1, a2);\n+      test_addImm(a1, a2, a3);\n+      test_subImm56(a1, a2);\n+      test_subImm256(a1, a2);\n+      test_andImm(a1, a2);\n+      test_orImm(a1, a2);\n+      test_xorImm(a1, a2);\n@@ -490,0 +498,50 @@\n+      \/\/ Reset for binary operation with immediate.\n+      char base = (char) 3;\n+      for (int i = 0; i < ARRLEN; i++) {\n+        a1[i] = (char) 3;\n+      }\n+      char golden = (char)(base + 129);\n+      test_addImm129(a1, a2);\n+      for (int i=0; i<ARRLEN; i++) {\n+        errn += verify(\"test_addImm129: a2\", i, a2[i], golden);\n+      }\n+      test_addImm(a1, a2, a3);\n+      golden = (char)(base + 129);\n+      for (int i=0; i<ARRLEN; i++) {\n+        errn += verify(\"test_addImm: a2\", i, a2[i], golden);\n+      }\n+      golden = (char) (base + 255);\n+      for (int i=0; i<ARRLEN; i++) {\n+        errn += verify(\"test_addImm: a3\", i, a3[i], golden);\n+      }\n+      \/\/ Reset for sub operation test.\n+      base = (char) 120;\n+      for (int i = 0; i < ARRLEN; i++) {\n+        a1[i] = (char) 120;\n+      }\n+      test_subImm56(a1, a2);\n+      golden = (char) (base - 56);\n+      for (int i=0; i<ARRLEN; i++) {\n+        errn += verify(\"test_subImm56: a2\", i, a2[i], golden);\n+      }\n+      test_subImm256(a1, a2);\n+      golden = (char) (base - 256);\n+      for (int i=0; i<ARRLEN; i++) {\n+        errn += verify(\"test_subImm256: a2\", i, a2[i], golden);\n+      }\n+      test_andImm(a1, a2);\n+      golden = (char) (base & 0xfe);\n+      for (int i=0; i<ARRLEN; i++) {\n+        errn += verify(\"test_andImm: a2\", i, a2[i], golden);\n+      }\n+      test_orImm(a1, a2);\n+      golden = (char) (base | 0xff);\n+      for (int i=0; i<ARRLEN; i++) {\n+        errn += verify(\"test_orImm: a2\", i, a2[i], golden);\n+      }\n+      test_xorImm(a1, a2);\n+      golden = (char) (base ^ 0xc7);\n+      for (int i=0; i<ARRLEN; i++) {\n+        errn += verify(\"test_xorImm: a2\", i, a2[i], golden);\n+      }\n+\n@@ -733,0 +791,43 @@\n+    start = System.currentTimeMillis();\n+    for (int i=0; i<ITERS; i++) {\n+      test_addImm129(a1, a2);\n+    }\n+    end = System.currentTimeMillis();\n+    System.out.println(\"test_addImm129: \" + (end - start));\n+    start = System.currentTimeMillis();\n+    for (int i=0; i<ITERS; i++) {\n+      test_addImm(a1, a2, a3);\n+    }\n+    end = System.currentTimeMillis();\n+    System.out.println(\"test_addImm: \" + (end - start));\n+    start = System.currentTimeMillis();\n+    for (int i=0; i<ITERS; i++) {\n+      test_subImm56(a1, a2);\n+    }\n+    end = System.currentTimeMillis();\n+    System.out.println(\"test_subImm56: \" + (end - start));\n+    start = System.currentTimeMillis();\n+    for (int i=0; i<ITERS; i++) {\n+      test_subImm256(a1, a2);\n+    }\n+    end = System.currentTimeMillis();\n+    System.out.println(\"test_subImm256: \" + (end - start));\n+    start = System.currentTimeMillis();\n+    for (int i=0; i<ITERS; i++) {\n+      test_andImm(a1, a2);\n+    }\n+    end = System.currentTimeMillis();\n+    System.out.println(\"test_andImm: \" + (end - start));\n+    start = System.currentTimeMillis();\n+    for (int i=0; i<ITERS; i++) {\n+      test_orImm(a1, a2);\n+    }\n+    end = System.currentTimeMillis();\n+    System.out.println(\"test_orImm: \" + (end - start));\n+    start = System.currentTimeMillis();\n+    for (int i=0; i<ITERS; i++) {\n+      test_xorImm(a1, a2);\n+    }\n+    end = System.currentTimeMillis();\n+    System.out.println(\"test_xorImm: \" + (end - start));\n+\n@@ -948,0 +1049,41 @@\n+  static void test_addImm129(char[] a, char[] b) {\n+    for (int i = 0; i < a.length; i++) {\n+      b[i] = (char) (a[i] + 129);\n+    }\n+  }\n+  static void test_addImm(char[] a, char[] b, char[] c) {\n+    for (int i = 0; i < a.length; i++) {\n+      b[i] = (char) (a[i] + 129);\n+      c[i] = (char) (a[i] + 255);\n+    }\n+  }\n+\n+  static void test_subImm56(char[] a, char[] b) {\n+    for (int i = 0; i < a.length; i++) {\n+      b[i] = (char) (a[i] - 56);\n+    }\n+  }\n+\n+  static void test_subImm256(char[] a, char[] b) {\n+    for (int i = 0; i < a.length; i++) {\n+      b[i] = (char) (a[i] - 256);\n+    }\n+  }\n+\n+  static void test_andImm(char[] a, char[] b) {\n+    for (int i = 0; i < a.length; i++) {\n+      b[i] = (char) (a[i] & 0xfe);\n+    }\n+  }\n+\n+  static void test_orImm(char[] a, char[] b) {\n+    for (int i = 0; i < a.length; i++) {\n+      b[i] = (char) (a[i] | 0xff);\n+    }\n+  }\n+\n+  static void test_xorImm(char[] a, char[] b) {\n+    for (int i = 0; i < a.length; i++) {\n+      b[i] = (char) (a[i] ^ 0xc7);\n+    }\n+  }\n","filename":"test\/hotspot\/jtreg\/compiler\/codegen\/TestCharVect.java","additions":143,"deletions":1,"binary":false,"changes":144,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2012, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2012, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -78,0 +78,2 @@\n+      test_conv_i2f(a1, b1);\n+      test_conv_f2i(a1, b1);\n@@ -341,0 +343,66 @@\n+      \/\/ Reset to test conversion from int to float.\n+      for (int i=0; i<ARRLEN; i++) {\n+        a1[i] = (int)i;\n+      }\n+      test_conv_i2f(a1, b1);\n+      for (int i=0; i<ARRLEN; i++) {\n+        errn += verify(\"test_conv_i2f: a1\", i, b1[i], (float)i);\n+      }\n+      \/\/ Reset to test conversion from float to int.\n+      for (int i=0; i<ARRLEN; i++) {\n+        b1[i] = (float)(i+1);\n+      }\n+      test_conv_f2i(a1, b1);\n+      for (int i=0; i<ARRLEN; i++) {\n+        errn += verify(\"test_conv_f2i: a1\", i, a1[i], (i+1));\n+      }\n+      \/\/ Reset to test NAN conversion from int to float.\n+      for (int i=0; i<ARRLEN; i++) {\n+        a1[i] = Integer.MIN_VALUE;\n+      }\n+      test_conv_i2f(a1, b1);\n+      for (int i=0; i<ARRLEN; i++) {\n+        errn += verify(\"test_conv_i2f: a1\", i, b1[i], (float)Integer.MIN_VALUE);\n+      }\n+      for (int i=0; i<ARRLEN; i++) {\n+        a1[i] = Integer.MAX_VALUE;\n+      }\n+      test_conv_i2f(a1, b1);\n+      for (int i=0; i<ARRLEN; i++) {\n+        errn += verify(\"test_conv_i2f: a1\", i, b1[i], (float)Integer.MAX_VALUE);\n+      }\n+      for (int i=0; i<ARRLEN; i++) {\n+        b1[i] = Float.NaN;\n+      }\n+      test_conv_f2i(a1, b1);\n+      for (int i=0; i<ARRLEN; i++) {\n+        errn += verify(\"test_conv_f2i: a1\", i, a1[i], (int)Float.NaN);\n+      }\n+      for (int i=0; i<ARRLEN; i++) {\n+        b1[i] = Float.POSITIVE_INFINITY;\n+      }\n+      test_conv_f2i(a1, b1);\n+      for (int i=0; i<ARRLEN; i++) {\n+        errn += verify(\"test_conv_f2i: a1\", i, a1[i], (int)Float.POSITIVE_INFINITY);\n+      }\n+      for (int i=0; i<ARRLEN; i++) {\n+        b1[i] = Float.NEGATIVE_INFINITY;\n+      }\n+      test_conv_f2i(a1, b1);\n+      for (int i=0; i<ARRLEN; i++) {\n+        errn += verify(\"test_conv_f2i: a1\", i, a1[i], (int)Float.NEGATIVE_INFINITY);\n+      }\n+      for (int i=0; i<ARRLEN; i++) {\n+        b1[i] = 0.0f;\n+      }\n+      test_conv_f2i(a1, b1);\n+      for (int i=0; i<ARRLEN; i++) {\n+        errn += verify(\"test_conv_f2i: a1\", i, a1[i], (int)0.0);\n+      }\n+      for (int i=0; i<ARRLEN; i++) {\n+        b1[i] = -0.0f;\n+      }\n+      test_conv_f2i(a1, b1);\n+      for (int i=0; i<ARRLEN; i++) {\n+        errn += verify(\"test_conv_f2i: a1\", i, a1[i], (int)(-0.0));\n+      }\n@@ -451,0 +519,12 @@\n+    start = System.currentTimeMillis();\n+    for (int i=0; i<ITERS; i++) {\n+      test_conv_i2f(a1, b1);\n+    }\n+    end = System.currentTimeMillis();\n+    System.out.println(\"test_conv_i2f: \" + (end - start));\n+    start = System.currentTimeMillis();\n+    for (int i=0; i<ITERS; i++) {\n+      test_conv_f2i(a1, b1);\n+    }\n+    end = System.currentTimeMillis();\n+    System.out.println(\"test_conv_f2i: \" + (end - start));\n@@ -559,0 +639,10 @@\n+  static void test_conv_i2f(int[] a, float[] b){\n+    for (int i = 0; i < a.length; i+=1) {\n+      b[i] = (float)a[i];\n+    }\n+  }\n+  static void test_conv_f2i(int[] a, float[] b){\n+    for (int i = 0; i < a.length; i+=1) {\n+      a[i] = (int)b[i];\n+    }\n+  }\n@@ -568,1 +658,1 @@\n-    if (elem != val) {\n+    if (elem != val && !(Float.isNaN(elem) && Float.isNaN(val))) {\n","filename":"test\/hotspot\/jtreg\/compiler\/codegen\/TestIntFloatVect.java","additions":92,"deletions":2,"binary":false,"changes":94,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2012, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2012, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -57,0 +57,1 @@\n+    int[] a3 = new int[ARRLEN];\n@@ -97,0 +98,7 @@\n+      test_addImm127(a1, a2);\n+      test_addImm(a1, a2, a3);\n+      test_addImm256(a1, a2);\n+      test_subImm(a1, a2, a3);\n+      test_andImm(a1, a2, a3);\n+      test_orImm(a1, a2);\n+      test_xorImm(a1, a2);\n@@ -490,0 +498,57 @@\n+      \/\/ Reset for binary operation with immediate\n+      int base = 10;\n+      for (int i = 0; i < ARRLEN; i++) {\n+          a1[i] = 10;\n+      }\n+      int golden = base + 127;\n+      test_addImm127(a1, a2);\n+      for (int i=0; i<ARRLEN; i++) {\n+        errn += verify(\"test_addImm127: a2\", i, a2[i], golden);\n+      }\n+      test_addImm(a1, a2, a3);\n+      golden = base + 127;\n+      for (int i=0; i<ARRLEN; i++) {\n+        errn += verify(\"test_addImm: a2\", i, a2[i], golden);\n+      }\n+      golden = base + 255;\n+      for (int i=0; i<ARRLEN; i++) {\n+        errn += verify(\"test_addImm: a3\", i, a3[i], golden);\n+      }\n+      test_addImm256(a1, a2);\n+      golden = base + 256;\n+      for (int i=0; i<ARRLEN; i++) {\n+        errn += verify(\"test_addImm256: a2\", i, a2[i], golden);\n+      }\n+      \/\/ Reset for sub test\n+      base = 10000;\n+      for (int i = 0; i < ARRLEN; i++) {\n+        a1[i] = 10000;\n+      }\n+      test_subImm(a1, a2, a3);\n+      golden = base - 2304;\n+      for (int i=0; i<ARRLEN; i++) {\n+        errn += verify(\"test_subImm: a2\", i, a2[i], golden);\n+      }\n+      golden = base - 65280;\n+      for (int i=0; i<ARRLEN; i++) {\n+        errn += verify(\"test_subImm: a3\", i, a3[i], golden);\n+      }\n+      test_andImm(a1, a2, a3);\n+      golden = base + 2560;\n+      for (int i=0; i<ARRLEN; i++) {\n+        errn += verify(\"test_andImm: a2\", i, a2[i], golden);\n+      }\n+      golden = base & 516096;\n+      for (int i=0; i<ARRLEN; i++) {\n+        errn += verify(\"test_andImm: a3\", i, a3[i], golden);\n+      }\n+      test_orImm(a1, a2);\n+      golden = base | 8257536;\n+      for (int i=0; i<ARRLEN; i++) {\n+        errn += verify(\"test_orImm: a2\", i, a2[i], golden);\n+      }\n+      test_xorImm(a1, a2);\n+      golden = base ^ 2032;\n+      for (int i=0; i<ARRLEN; i++) {\n+        errn += verify(\"test_xorImm: a2\", i, a2[i], golden);\n+      }\n@@ -733,0 +798,42 @@\n+    start = System.currentTimeMillis();\n+    for (int i=0; i<ITERS; i++) {\n+      test_addImm127(a1, a2);\n+    }\n+    end = System.currentTimeMillis();\n+    System.out.println(\"test_addImm127: \" + (end - start));\n+    start = System.currentTimeMillis();\n+    for (int i=0; i<ITERS; i++) {\n+      test_addImm(a1, a2, a3);\n+    }\n+    end = System.currentTimeMillis();\n+    System.out.println(\"test_addImm: \" + (end - start));\n+    start = System.currentTimeMillis();\n+    for (int i=0; i<ITERS; i++) {\n+      test_addImm256(a1, a2);\n+    }\n+    end = System.currentTimeMillis();\n+    System.out.println(\"test_addImm256: \" + (end - start));\n+    start = System.currentTimeMillis();\n+    for (int i=0; i<ITERS; i++) {\n+      test_subImm(a1, a2, a3);\n+    }\n+    end = System.currentTimeMillis();\n+    System.out.println(\"test_subImm: \" + (end - start));\n+    start = System.currentTimeMillis();\n+    for (int i=0; i<ITERS; i++) {\n+      test_andImm(a1, a2, a3);\n+    }\n+    end = System.currentTimeMillis();\n+    System.out.println(\"test_andImm: \" + (end - start));\n+    start = System.currentTimeMillis();\n+    for (int i=0; i<ITERS; i++) {\n+      test_orImm(a1, a2);\n+    }\n+    end = System.currentTimeMillis();\n+    System.out.println(\"test_orImm: \" + (end - start));\n+    start = System.currentTimeMillis();\n+    for (int i=0; i<ITERS; i++) {\n+      test_xorImm(a1, a2);\n+    }\n+    end = System.currentTimeMillis();\n+    System.out.println(\"test_xorImm: \" + (end - start));\n@@ -948,0 +1055,44 @@\n+  static void test_addImm127(int[] a, int[] b) {\n+    for (int i = 0; i < a.length; i++) {\n+      b[i] = a[i] + 127;\n+    }\n+  }\n+  static void test_addImm(int[] a, int[] b, int[] c) {\n+    for (int i = 0; i < a.length; i++) {\n+      b[i] = a[i] + 127;\n+      c[i] = a[i] + 255;\n+    }\n+  }\n+  static void test_addImm256(int[] a, int[] b) {\n+    for (int i = 0; i < a.length; i++) {\n+      b[i] = a[i] + 256;\n+    }\n+  }\n+  static void test_subImm(int[] a, int[] b, int[] c) {\n+    for (int i = 0; i < a.length; i++) {\n+      b[i] = a[i] - 2304;\n+      c[i] = a[i] - 65280;\n+    }\n+  }\n+  static void test_andImm21(int[] a, int[] b) {\n+    for (int i = 0; i < a.length; i++) {\n+      b[i] = a[i] & 21;\n+    }\n+  }\n+  static void test_andImm(int[] a, int[] b, int[] c) {\n+    for (int i = 0; i < a.length; i++) {\n+      b[i] = a[i] + 2560;\n+      c[i] = a[i] & 516096;\n+    }\n+  }\n+  static void test_orImm(int[] a, int[] b) {\n+    for (int i = 0; i < a.length; i++) {\n+      b[i] = a[i] | 8257536;\n+    }\n+  }\n+  static void test_xorImm(int[] a, int[] b) {\n+    for (int i = 0; i < a.length; i++) {\n+      b[i] = a[i] ^ 2032;\n+    }\n+  }\n+\n","filename":"test\/hotspot\/jtreg\/compiler\/codegen\/TestIntVect.java","additions":152,"deletions":1,"binary":false,"changes":153,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2012, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2012, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -78,0 +78,2 @@\n+      test_conv_l2d(a1, b1);\n+      test_conv_d2l(a1, b1);\n@@ -341,0 +343,66 @@\n+      \/\/ Reset to test conversion from int to float.\n+      for (int i=0; i<ARRLEN; i++) {\n+        a1[i] = (long)i;\n+      }\n+      test_conv_l2d(a1, b1);\n+      for (int i=0; i<ARRLEN; i++) {\n+        errn += verify(\"test_conv_l2d: a1\", i, b1[i], (double)i);\n+      }\n+      \/\/ Reset to test conversion from float to int.\n+      for (int i=0; i<ARRLEN; i++) {\n+        b1[i] = (double)(i+1);\n+      }\n+      test_conv_d2l(a1, b1);\n+      for (int i=0; i<ARRLEN; i++) {\n+        errn += verify(\"test_conv_d2l: a1\", i, a1[i], (long)(i+1));\n+      }\n+      \/\/ Reset to test special conversion from int to float.\n+      for (int i=0; i<ARRLEN; i++) {\n+        b1[i] = Double.NaN;\n+      }\n+      test_conv_d2l(a1, b1);\n+      for (int i=0; i<ARRLEN; i++) {\n+        errn += verify(\"test_conv_d2l: a1\", i, a1[i], (long)Double.NaN);\n+      }\n+      for (int i=0; i<ARRLEN; i++) {\n+        a1[i] = Long.MIN_VALUE;\n+      }\n+      test_conv_l2d(a1, b1);\n+      for (int i=0; i<ARRLEN; i++) {\n+        errn += verify(\"test_conv_l2d: a1\", i, b1[i], (double)Long.MIN_VALUE);\n+      }\n+      for (int i=0; i<ARRLEN; i++) {\n+        a1[i] = Long.MAX_VALUE;\n+      }\n+      test_conv_l2d(a1, b1);\n+      for (int i=0; i<ARRLEN; i++) {\n+        errn += verify(\"test_conv_l2d: a1\", i, b1[i], (double)Long.MAX_VALUE);\n+      }\n+      for (int i=0; i<ARRLEN; i++) {\n+        b1[i] = Double.POSITIVE_INFINITY;\n+      }\n+      test_conv_d2l(a1, b1);\n+      for (int i=0; i<ARRLEN; i++) {\n+        errn += verify(\"test_conv_d2l: a1\", i, a1[i], (long)Double.POSITIVE_INFINITY);\n+      }\n+      for (int i=0; i<ARRLEN; i++) {\n+        b1[i] = Double.NEGATIVE_INFINITY;\n+      }\n+      test_conv_d2l(a1, b1);\n+      for (int i=0; i<ARRLEN; i++) {\n+        errn += verify(\"test_conv_d2l: a1\", i, a1[i], (long)Double.NEGATIVE_INFINITY);\n+      }\n+      for (int i=0; i<ARRLEN; i++) {\n+        b1[i] = 0.0;\n+      }\n+      test_conv_d2l(a1, b1);\n+      for (int i=0; i<ARRLEN; i++) {\n+        errn += verify(\"test_conv_d2l: a1\", i, a1[i], (long)0.0);\n+      }\n+      for (int i=0; i<ARRLEN; i++) {\n+        b1[i] = -0.0;\n+      }\n+      test_conv_d2l(a1, b1);\n+      for (int i=0; i<ARRLEN; i++) {\n+        errn += verify(\"test_conv_d2l: a1\", i, a1[i], (long)(-0.0));\n+      }\n@@ -451,0 +519,12 @@\n+    start = System.currentTimeMillis();\n+    for (int i=0; i<ITERS; i++) {\n+      test_conv_l2d(a1, b1);\n+    }\n+    end = System.currentTimeMillis();\n+    System.out.println(\"test_conv_l2d: \" + (end - start));\n+    start = System.currentTimeMillis();\n+    for (int i=0; i<ITERS; i++) {\n+      test_conv_d2l(a1, b1);\n+    }\n+    end = System.currentTimeMillis();\n+    System.out.println(\"test_conv_d2l: \" + (end - start));\n@@ -559,0 +639,10 @@\n+  static void test_conv_l2d(long[] a, double[] b){\n+    for (int i = 0; i < a.length; i+=1) {\n+      b[i] = (double)a[i];\n+    }\n+  }\n+  static void test_conv_d2l(long[] a, double[] b){\n+    for (int i = 0; i < a.length; i+=1) {\n+      a[i] = (long)b[i];\n+    }\n+  }\n@@ -568,1 +658,1 @@\n-    if (elem != val) {\n+    if (elem != val && !(Double.isNaN(elem) && Double.isNaN(val))) {\n","filename":"test\/hotspot\/jtreg\/compiler\/codegen\/TestLongDoubleVect.java","additions":92,"deletions":2,"binary":false,"changes":94,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2012, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2012, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -57,0 +57,1 @@\n+    long[] a3 = new long[ARRLEN];\n@@ -97,0 +98,6 @@\n+      test_addImm(a1, a2, a3);\n+      test_subImm(a1, a2, a3);\n+      test_subImm256(a1, a2);\n+      test_andImm(a1, a2);\n+      test_orImm(a1, a2);\n+      test_xorImm(a1, a2);\n@@ -490,0 +497,47 @@\n+      \/\/ Reset for binary operations with immediate.\n+      for (int i=0; i<ARRLEN; i++) {\n+      a1[i] = 10;\n+      }\n+      long base = 10;\n+      test_addImm(a1, a2, a3);\n+      long golden = base & 516097;\n+      for (int i=0; i<ARRLEN; i++) {\n+        errn += verify(\"test_addImm: a2\", i, a2[i], golden);\n+      }\n+      golden = base + 65280;\n+      for (int i=0; i<ARRLEN; i++) {\n+        errn += verify(\"test_addImm: a3\", i, a3[i], golden);\n+      }\n+      base = 120;\n+      for (int i=0; i<ARRLEN; i++) {\n+        a1[i] = 120;\n+      }\n+      test_subImm(a1, a2, a3);\n+      golden = base + 65535;\n+      for (int i=0; i<ARRLEN; i++) {\n+        errn += verify(\"test_subImm: a2\", i, a2[i], golden);\n+      }\n+      golden = base - 2147483647;\n+      for (int i=0; i<ARRLEN; i++) {\n+        errn += verify(\"test_subImm: a3\", i, a3[i], golden);\n+      }\n+      test_subImm256(a1, a2);\n+      golden = base - 256;\n+      for (int i=0; i<ARRLEN; i++) {\n+        errn += verify(\"test_subImm256: a2\", i, a2[i], golden);\n+      }\n+      test_andImm(a1, a2);\n+      golden = base & 132120576;\n+      for (int i=0; i<ARRLEN; i++) {\n+        errn += verify(\"test_andImm: a2\", i, a2[i], golden);\n+      }\n+      test_orImm(a1, a2);\n+      golden = base | 2113929216;\n+      for (int i=0; i<ARRLEN; i++) {\n+        errn += verify(\"test_orImm: a2\", i, a2[i], golden);\n+      }\n+      test_xorImm(a1, a2);\n+      golden = base ^ 516096;\n+      for (int i=0; i<ARRLEN; i++) {\n+        errn += verify(\"test_xorImm: a2\", i, a2[i], golden);\n+      }\n@@ -733,0 +787,36 @@\n+    start = System.currentTimeMillis();\n+    for (int i=0; i<ITERS; i++) {\n+      test_addImm(a1, a2, a3);\n+    }\n+    end = System.currentTimeMillis();\n+    System.out.println(\"test_addImm: \" + (end - start));\n+    start = System.currentTimeMillis();\n+    for (int i=0; i<ITERS; i++) {\n+      test_subImm(a1, a2, a3);\n+    }\n+    end = System.currentTimeMillis();\n+    System.out.println(\"test_subImm: \" + (end - start));\n+    start = System.currentTimeMillis();\n+    for (int i=0; i<ITERS; i++) {\n+      test_subImm256(a1, a2);\n+    }\n+    end = System.currentTimeMillis();\n+    System.out.println(\"test_subImm256: \" + (end - start));\n+    start = System.currentTimeMillis();\n+    for (int i=0; i<ITERS; i++) {\n+      test_andImm(a1, a2);\n+    }\n+    end = System.currentTimeMillis();\n+    System.out.println(\"test_andImm: \" + (end - start));\n+    start = System.currentTimeMillis();\n+    for (int i=0; i<ITERS; i++) {\n+      test_orImm(a1, a2);\n+    }\n+    end = System.currentTimeMillis();\n+    System.out.println(\"test_orImm: \" + (end - start));\n+    start = System.currentTimeMillis();\n+    for (int i=0; i<ITERS; i++) {\n+      test_xorImm(a1, a2);\n+    }\n+    end = System.currentTimeMillis();\n+    System.out.println(\"test_xorImm: \" + (end - start));\n@@ -949,0 +1039,38 @@\n+  static void test_addImm(long[] a, long[] b, long[] c) {\n+    for (int i = 0; i < a.length; i++) {\n+      b[i] = a[i] & 516097;\n+      c[i] = a[i] + 65280;\n+    }\n+  }\n+\n+  static void test_subImm(long[] a, long[] b, long[] c) {\n+    for (int i = 0; i < a.length; i++) {\n+      b[i] = a[i] + 65535;\n+      c[i] = a[i] - 2147483647;\n+    }\n+  }\n+\n+  static void test_subImm256(long[] a, long[] b) {\n+    for (int i = 0; i < a.length; i++) {\n+      b[i] = a[i] - 256;\n+    }\n+  }\n+\n+  static void test_andImm(long[] a, long[] b) {\n+    for (int i = 0; i < a.length; i++) {\n+      b[i] = a[i] & 132120576;\n+    }\n+  }\n+\n+  static void test_orImm(long[] a, long[] b) {\n+    for (int i = 0; i < a.length; i++) {\n+      b[i] = a[i] | 2113929216;\n+    }\n+  }\n+\n+  static void test_xorImm(long[] a, long[] b) {\n+    for (int i = 0; i < a.length; i++) {\n+      b[i] = a[i] ^ 516096;\n+    }\n+  }\n+\n","filename":"test\/hotspot\/jtreg\/compiler\/codegen\/TestLongVect.java","additions":129,"deletions":1,"binary":false,"changes":130,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2012, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2012, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -57,0 +57,1 @@\n+    short[] a3 = new short[ARRLEN];\n@@ -97,0 +98,7 @@\n+      test_addImm129(a1, a2);\n+      test_addImm(a1, a2, a3);\n+      test_subImm56(a1, a2);\n+      test_subImm256(a1, a2);\n+      test_andImm(a1, a2);\n+      test_orImm(a1, a2);\n+      test_xorImm(a1, a2);\n@@ -490,0 +498,48 @@\n+      short base = (short) 3;\n+      for (int i = 0; i < ARRLEN; i++) {\n+          a1[i] = (short) 3;\n+      }\n+      short golden = (short)(base + 129);\n+      test_addImm129(a1, a2);\n+      for (int i=0; i<ARRLEN; i++) {\n+        errn += verify(\"test_addImm129: a2\", i, a2[i], golden);\n+      }\n+      test_addImm(a1, a2, a3);\n+      golden = (short)(base + 129);\n+      for (int i=0; i<ARRLEN; i++) {\n+        errn += verify(\"test_addImm: a2\", i, a2[i], golden);\n+      }\n+      golden = (short) (base + 255);\n+      for (int i=0; i<ARRLEN; i++) {\n+        errn += verify(\"test_addImm: a3\", i, a3[i], golden);\n+      }\n+      \/\/ Reset for sub test\n+      base = (short) 120;\n+      for (int i = 0; i < ARRLEN; i++) {\n+        a1[i] = (short) 120;\n+      }\n+      test_subImm56(a1, a2);\n+      golden = (short) (base - 56);\n+      for (int i=0; i<ARRLEN; i++) {\n+        errn += verify(\"test_subImm56: a2\", i, a2[i], golden);\n+      }\n+      test_subImm256(a1, a2);\n+      golden = (short) (base - 256);\n+      for (int i=0; i<ARRLEN; i++) {\n+        errn += verify(\"test_subImm256: a2\", i, a2[i], golden);\n+      }\n+      test_andImm(a1, a2);\n+      golden = (short) (base & 0xfe);\n+      for (int i=0; i<ARRLEN; i++) {\n+        errn += verify(\"test_andImm: a2\", i, a2[i], golden);\n+      }\n+      test_orImm(a1, a2);\n+      golden = (short) (base | 0xff);\n+      for (int i=0; i<ARRLEN; i++) {\n+        errn += verify(\"test_orImm: a2\", i, a2[i], golden);\n+      }\n+      test_xorImm(a1, a2);\n+      golden = (short) (base ^ 0xc7);\n+      for (int i=0; i<ARRLEN; i++) {\n+        errn += verify(\"test_xorImm: a2\", i, a2[i], golden);\n+      }\n@@ -733,0 +789,42 @@\n+    start = System.currentTimeMillis();\n+    for (int i=0; i<ITERS; i++) {\n+      test_addImm129(a1, a2);\n+    }\n+    end = System.currentTimeMillis();\n+    System.out.println(\"test_addImm129: \" + (end - start));\n+    start = System.currentTimeMillis();\n+    for (int i=0; i<ITERS; i++) {\n+      test_addImm(a1, a2, a3);\n+    }\n+    end = System.currentTimeMillis();\n+    System.out.println(\"test_addImm: \" + (end - start));\n+    start = System.currentTimeMillis();\n+    for (int i=0; i<ITERS; i++) {\n+      test_subImm56(a1, a2);\n+    }\n+    end = System.currentTimeMillis();\n+    System.out.println(\"test_subImm56: \" + (end - start));\n+    start = System.currentTimeMillis();\n+    for (int i=0; i<ITERS; i++) {\n+      test_subImm256(a1, a2);\n+    }\n+    end = System.currentTimeMillis();\n+    System.out.println(\"test_subImm256: \" + (end - start));\n+    start = System.currentTimeMillis();\n+    for (int i=0; i<ITERS; i++) {\n+      test_andImm(a1, a2);\n+    }\n+    end = System.currentTimeMillis();\n+    System.out.println(\"test_andImm: \" + (end - start));\n+    start = System.currentTimeMillis();\n+    for (int i=0; i<ITERS; i++) {\n+      test_orImm(a1, a2);\n+    }\n+    end = System.currentTimeMillis();\n+    System.out.println(\"test_orImm: \" + (end - start));\n+    start = System.currentTimeMillis();\n+    for (int i=0; i<ITERS; i++) {\n+      test_xorImm(a1, a2);\n+    }\n+    end = System.currentTimeMillis();\n+    System.out.println(\"test_xorImm: \" + (end - start));\n@@ -948,0 +1046,37 @@\n+  static void test_addImm129(short[] a, short[] b) {\n+    for (int i = 0; i < a.length; i++) {\n+      b[i] = (short) (a[i] + 129);\n+    }\n+  }\n+\n+  static void test_addImm(short[] a, short[] b, short[] c) {\n+    for (int i = 0; i < a.length; i++) {\n+      b[i] = (short) (a[i] + 129);\n+      c[i] = (short) (a[i] + 255);\n+    }\n+  }\n+  static void test_subImm56(short[] a, short[] b) {\n+    for (int i = 0; i < a.length; i++) {\n+      b[i] = (short) (a[i] - 56);\n+    }\n+  }\n+  static void test_subImm256(short[] a, short[] b) {\n+    for (int i = 0; i < a.length; i++) {\n+      b[i] = (short) (a[i] - 256);\n+    }\n+  }\n+  static void test_andImm(short[] a, short[] b) {\n+    for (int i = 0; i < a.length; i++) {\n+      b[i] = (short) (a[i] & 0xfe);\n+    }\n+  }\n+  static void test_orImm(short[] a, short[] b) {\n+    for (int i = 0; i < a.length; i++) {\n+      b[i] = (short) (a[i] | 0xff);\n+    }\n+  }\n+  static void test_xorImm(short[] a, short[] b) {\n+    for (int i = 0; i < a.length; i++) {\n+      b[i] = (short) (a[i] ^ 0xc7);\n+    }\n+  }\n","filename":"test\/hotspot\/jtreg\/compiler\/codegen\/TestShortVect.java","additions":136,"deletions":1,"binary":false,"changes":137,"status":"modified"},{"patch":"@@ -28,1 +28,1 @@\n- * @summary Verify that box object identity matches after deoptimization When it is eliminated.\n+ * @summary Verify that box object identity matches after deoptimization when it is eliminated.\n@@ -31,1 +31,1 @@\n- * @run main\/othervm -Xbatch compiler.c2.TestIdentityWithEliminateBoxInDebugInfo\n+ * @run main\/othervm -Xbatch compiler.eliminateAutobox.TestIdentityWithEliminateBoxInDebugInfo\n@@ -33,1 +33,2 @@\n-package compiler.c2;\n+\n+package compiler.eliminateAutobox;\n@@ -43,6 +44,6 @@\n-      \/\/ warmup\n-      for(int i = 0; i < 100000; i++) {\n-        f.apply(true);\n-      }\n-      \/\/ deoptimize\n-      f.apply(false);\n+        \/\/ warmup\n+        for (int i = 0; i < 100000; i++) {\n+            f.apply(true);\n+        }\n+        \/\/ deoptimize\n+        f.apply(false);\n","filename":"test\/hotspot\/jtreg\/compiler\/eliminateAutobox\/TestIdentityWithEliminateBoxInDebugInfo.java","additions":10,"deletions":9,"binary":false,"changes":19,"status":"modified"},{"patch":"@@ -0,0 +1,173 @@\n+\/*\n+ * Copyright (c) 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+\/*\n+ * @test\n+ * @key randomness\n+ * @bug 8276112\n+ * @summary Verify consistency of safepoint debug info when boxes are scalar\n+ *          replaced during incremental inlining.\n+ * @library \/test\/lib\n+ * @run main\/othervm -Xbatch -XX:+IgnoreUnrecognizedVMOptions -XX:+AlwaysIncrementalInline\n+ *                   -XX:CompileCommand=compileonly,compiler.eliminateAutobox.TestSafepointDebugInfo::test*\n+ *                   compiler.eliminateAutobox.TestSafepointDebugInfo\n+ *\/\n+\n+package compiler.eliminateAutobox;\n+\n+import java.util.Random;\n+\n+import jdk.test.lib.Asserts;\n+import jdk.test.lib.Utils;\n+\n+public class TestSafepointDebugInfo {\n+\n+    private static final Random random = Utils.getRandomInstance();\n+\n+    static Integer fBox;\n+\n+    public static Integer helper(int i) {\n+        return Integer.valueOf(i);\n+    }\n+\n+    \/\/ assert(local) failed: use _top instead of null\n+    public static int test1(int i) {\n+        Integer box = helper(i);\n+        fBox = Integer.valueOf(i);\n+        return box.intValue();\n+    }\n+\n+    \/\/ Wrong execution, same types\n+    public static int test2(int i1, int i2) {\n+        Integer box1 = helper(i1);\n+        Integer box2 = Integer.valueOf(i2);\n+        fBox = Integer.valueOf(i1);\n+        return box1.intValue() + box2.intValue();\n+    }\n+\n+    \/\/ Wrong execution, different types\n+    public static long test3(int i1, long i2) {\n+        Integer box1 = helper(i1);\n+        Long box2 = Long.valueOf(i2);\n+        fBox = Integer.valueOf(i1);\n+        return box1.intValue() + box2.longValue();\n+    }\n+\n+    \/\/ assert(i < _max) failed: oob: i=16, _max=16\n+    public static int test4(int i1, int i2) {\n+        Integer box1 = helper(i1);\n+        Integer box2 = helper(i2);\n+        fBox = Integer.valueOf(i1);\n+        return box1.intValue() + box2.intValue();\n+    }\n+\n+    public static Integer test5_helper(int i1, int i2) {\n+        Integer box1 = helper(i1);\n+        Integer box2 = helper(i2);\n+        fBox = Integer.valueOf(i1);\n+        return box1.intValue() + box2.intValue();\n+    }\n+\n+    \/\/ assert(local) failed: use _top instead of null\n+    \/\/ Variant with deeper inlining\n+    public static int test5(int i1, int i2) {\n+        return test5_helper(i1, i2);\n+    }\n+\n+    public static int test6_helper(int i1, int i2) {\n+        Integer box = helper(i1);\n+        fBox = Integer.valueOf(i2);\n+        return box.intValue();\n+    }\n+\n+    \/\/ Wrong execution, variant with more arguments\n+    public static int test6(int i1, int i2, int i3, int i4) {\n+        Integer box1 = helper(i1);\n+        Integer box2 = helper(i2);\n+        int res = test6_helper(i3, i4);\n+        res += box1.intValue() + box2.intValue();\n+        return res;\n+    }\n+\n+    public static void main(String[] args) {\n+        \/\/ Warmup\n+        for (int i = 0; i < 100_000; ++i) {\n+            int val = (i % 10);\n+            Asserts.assertEquals(test1(val), val);\n+            Asserts.assertEquals(fBox, val);\n+            Asserts.assertEquals(test2(val, val), 2*val);\n+            Asserts.assertEquals(fBox, val);\n+            Asserts.assertEquals(test3(val, val), 2L*val);\n+            Asserts.assertEquals(fBox, val);\n+            Asserts.assertEquals(test4(val, val), 2*val);\n+            Asserts.assertEquals(fBox, val);\n+            Asserts.assertEquals(test5(val, val), 2*val);\n+            Asserts.assertEquals(fBox, val);\n+            Asserts.assertEquals(test6(val, val, val, val), 3*val);\n+            Asserts.assertEquals(fBox, val);\n+        }\n+\n+        \/\/ Trigger deoptimization by choosing a value that does not\n+        \/\/ fit in the Integer cache and check the result.\n+        int val = 4000;\n+        Asserts.assertEquals(test1(val), val);\n+        switch (random.nextInt(3)) {\n+            case 0:\n+                Asserts.assertEquals(test2(val, 1), val + 1);\n+                Asserts.assertEquals(fBox, val);\n+                Asserts.assertEquals(test3(val, 1), (long)val + 1);\n+                Asserts.assertEquals(fBox, val);\n+                Asserts.assertEquals(test4(val, 1), val + 1);\n+                Asserts.assertEquals(fBox, val);\n+                Asserts.assertEquals(test5(val, 1), val + 1);\n+                Asserts.assertEquals(fBox, val);\n+                Asserts.assertEquals(test6(val, 1, 2, 3), val + 3);\n+                Asserts.assertEquals(fBox, 3);\n+                break;\n+           case 1:\n+                Asserts.assertEquals(test2(1, val), val + 1);\n+                Asserts.assertEquals(fBox, 1);\n+                Asserts.assertEquals(test3(1, val), (long)val + 1);\n+                Asserts.assertEquals(fBox, 1);\n+                Asserts.assertEquals(test4(1, val), val + 1);\n+                Asserts.assertEquals(fBox, 1);\n+                Asserts.assertEquals(test5(1, val), val + 1);\n+                Asserts.assertEquals(fBox, 1);\n+                Asserts.assertEquals(test6(1, val, 2, 3), val + 3);\n+                Asserts.assertEquals(fBox, 3);\n+                break;\n+           case 2:\n+                Asserts.assertEquals(test2(1, 2), 3);\n+                Asserts.assertEquals(fBox, 1);\n+                Asserts.assertEquals(test3(1, 2), 3L);\n+                Asserts.assertEquals(fBox, 1);\n+                Asserts.assertEquals(test4(1, 2), 3);\n+                Asserts.assertEquals(fBox, 1);\n+                Asserts.assertEquals(test5(1, 2), 3);\n+                Asserts.assertEquals(fBox, 1);\n+                Asserts.assertEquals(test6(1, 2, 3, val), 6);\n+                Asserts.assertEquals(fBox, val);\n+                break;\n+        }\n+    }\n+}\n","filename":"test\/hotspot\/jtreg\/compiler\/eliminateAutobox\/TestSafepointDebugInfo.java","additions":173,"deletions":0,"binary":false,"changes":173,"status":"added"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2016, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2016, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -33,1 +33,1 @@\n- * @run main\/othervm -ea -XX:-UseSharedSpaces -Diters=30000 -XX:TieredStopAtLevel=1\n+ * @run main\/othervm -ea -Xshare:off -Diters=30000 -XX:TieredStopAtLevel=1\n","filename":"test\/hotspot\/jtreg\/compiler\/intrinsics\/klass\/TestIsPrimitive.java","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -104,1 +104,9 @@\n-        Asserts.assertEQ(str2, str3,\n+        String[] str2Lines = str2.split(System.lineSeparator());\n+        String[] str3Lines = str3.split(System.lineSeparator());\n+        \/\/ skip the first two lines since it contains a timestamp that may vary from different invocations\n+        \/\/ <empty-line>\n+        \/\/ Compiled method (c2)     309  463       4       compiler.jvmci.compilerToVM.CompileCodeTestCase$Dummy::staticMethod (1 bytes)\n+        \/\/ <empty-line>\n+        \/\/ Compiled method (c2)     310  463       4       compiler.jvmci.compilerToVM.CompileCodeTestCase$Dummy::staticMethod (1 bytes)\n+        for (int i = 2; i < str2Lines.length; i++) {\n+            Asserts.assertEQ(str2Lines[i], str3Lines[i],\n@@ -106,0 +114,1 @@\n+        }\n","filename":"test\/hotspot\/jtreg\/compiler\/jvmci\/compilerToVM\/DisassembleCodeBlobTest.java","additions":10,"deletions":1,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -125,0 +125,1 @@\n+                    \"CompileThreshold\",\n","filename":"test\/hotspot\/jtreg\/compiler\/lib\/ir_framework\/TestFramework.java","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -40,0 +40,1 @@\n+import java.util.stream.Collectors;\n@@ -100,1 +101,2 @@\n-        String[] jtregVMFlags = Utils.getTestJavaOpts();\n+        \/\/ Ignore CompileCommand flags which have an impact on the profiling information.\n+        List<String> jtregVMFlags = Arrays.stream(Utils.getTestJavaOpts()).filter(s -> !s.contains(\"CompileThreshold\")).collect(Collectors.toList());\n@@ -102,1 +104,1 @@\n-            cmds.addAll(Arrays.asList(jtregVMFlags));\n+            cmds.addAll(jtregVMFlags);\n@@ -114,1 +116,1 @@\n-            cmds.addAll(Arrays.asList(jtregVMFlags));\n+            cmds.addAll(jtregVMFlags);\n","filename":"test\/hotspot\/jtreg\/compiler\/lib\/ir_framework\/driver\/TestVMProcess.java","additions":5,"deletions":3,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -0,0 +1,93 @@\n+\/*\n+ * Copyright (C) 2021 THL A29 Limited, a Tencent company. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+\/**\n+ * @test\n+ * @bug 8275854\n+ * @summary Crashes in PhaseIdealLoop::transform_long_counted_loop\n+ * @requires vm.compiler2.enabled\n+ *\n+ * @run main\/othervm -Xbatch -XX:CompileCommand=compileonly,TestLoopEndNodeEliminate::lMeth TestLoopEndNodeEliminate\n+ *\n+ *\/\n+\n+public class TestLoopEndNodeEliminate {\n+     public volatile boolean bFld=true;\n+     public volatile byte byFld=0;\n+     public volatile short sArrFld[]=new short[N];\n+     public int iArrFld[]=new int[N];\n+     public boolean bArrFld[]=new boolean[N];\n+\n+     public static int iFld=10;\n+     public static final int N = 400;\n+     public static long instanceCount=0L;\n+     public static long lMeth_check_sum = 0;\n+\n+     public long lMeth() {\n+         long l1=-33582180L;\n+         int i14=-5, i15=-14, i16=0, i17=25699, i18=97, i19=-3, i20=0, i21=0, i22=42, i23=0, i24=25699, i25=97;\n+\n+         for (l1 = 286; l1 > 16; l1 -= 3) {\n+             for (i15 = 17; i15 > l1; --i15) {\n+                 switch (((iArrFld[i15] >>> 1) % 7) + 101) {\n+                 case 101:\n+                 case 102:\n+                 case 103:\n+                 case 104:\n+                     for (i17 = (int)(l1); i17 < 1; i17++) {\n+                         bArrFld[i17] = bFld;\n+                     }\n+                     break;\n+                 case 105:\n+                 case 106:\n+                 case 107:\n+                 }\n+             }\n+             for (i19 = 1; i19 < 270; ++i19) {\n+                 TestLoopEndNodeEliminate.iFld += byFld;\n+                 i21 = 1;\n+                 while (++i21 < 2) {\n+                     bFld = true;\n+                 }\n+                 for (i22 = 1; 2 > i22; ++i22) {\n+                     bFld = true;\n+                 }\n+                 for (i24 = 1; 2 > i24; ++i24) {\n+                     bFld = true;\n+                 }\n+                 bArrFld[(int)(l1) % N] = bFld;\n+                 sArrFld[i19 - 1] ^= (short)(++TestLoopEndNodeEliminate.instanceCount);\n+             }\n+         }\n+         long meth_res = l1 + i14 + i15 + i16 + i17 + i18 + i19 + i20 + i21 + i22 + i23 + i24 + i25;\n+         lMeth_check_sum += meth_res;\n+         return (long)meth_res;\n+     }\n+\n+     public static void main(String[] strArr) {\n+        TestLoopEndNodeEliminate _instance = new TestLoopEndNodeEliminate();\n+        for (int i = 0; i < 10000; i++ ) {\n+            _instance.lMeth();\n+        }\n+     }\n+}\n","filename":"test\/hotspot\/jtreg\/compiler\/loopopts\/TestLoopEndNodeEliminate.java","additions":93,"deletions":0,"binary":false,"changes":93,"status":"added"},{"patch":"@@ -0,0 +1,66 @@\n+\/*\n+ * Copyright (C) 2021 THL A29 Limited, a Tencent company. All rights reserved.\n+ * Copyright (c) 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+\/**\n+ * @test\n+ * @bug 8273277\n+ * @summary Skeleton predicates sometimes need to be negated\n+ * @run main compiler.loopopts.TestSkeletonPredicateNegation\n+ *\n+ *\/\n+\n+package compiler.loopopts;\n+\n+public class TestSkeletonPredicateNegation {\n+    public static int in0 = 2;\n+\n+    public static void main(String[] args) {\n+        try {\n+            TestSkeletonPredicateNegation instance = new TestSkeletonPredicateNegation();\n+            for (int i = 0; i < 10000; ++i) {\n+                instance.mainTest(args);\n+            }\n+        } catch (Exception ex) {\n+            System.out.println(ex.getClass().getCanonicalName());\n+        } catch (OutOfMemoryError e) {\n+            System.out.println(\"OOM Error\");\n+        }\n+    }\n+\n+    public void mainTest (String[] args){\n+        long loa11[] = new long[1987];\n+\n+        for (long lo14 : loa11) {\n+            TestSkeletonPredicateNegation.in0 = -128;\n+            for (int i18 = 0; i18 < 52; i18++) {\n+                try {\n+                    loa11[TestSkeletonPredicateNegation.in0] %= 2275269548L;\n+                    Math.ceil(1374905370.2785515599);\n+                } catch (Exception a_e) {\n+                    TestSkeletonPredicateNegation.in0--;\n+                }\n+            }\n+        }\n+    }\n+}\n","filename":"test\/hotspot\/jtreg\/compiler\/loopopts\/TestSkeletonPredicateNegation.java","additions":66,"deletions":0,"binary":false,"changes":66,"status":"added"},{"patch":"@@ -0,0 +1,218 @@\n+\/*\n+ * Copyright (c) 2021, Amazon.com Inc. or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+\/**\n+ * @test TestOnSpinWaitAArch64\n+ * @summary Checks that java.lang.Thread.onSpinWait is intrinsified with instructions specified with '-XX:OnSpinWaitInst' and '-XX:OnSpinWaitInstCount'\n+ * @bug 8186670\n+ * @library \/test\/lib\n+ *\n+ * @requires vm.flagless\n+ * @requires os.arch==\"aarch64\"\n+ *\n+ * @run driver compiler.onSpinWait.TestOnSpinWaitAArch64 c2 nop 7\n+ * @run driver compiler.onSpinWait.TestOnSpinWaitAArch64 c2 isb 3\n+ * @run driver compiler.onSpinWait.TestOnSpinWaitAArch64 c2 yield 1\n+ * @run driver compiler.onSpinWait.TestOnSpinWaitAArch64 c1 nop 7\n+ * @run driver compiler.onSpinWait.TestOnSpinWaitAArch64 c1 isb 3\n+ * @run driver compiler.onSpinWait.TestOnSpinWaitAArch64 c1 yield\n+ *\/\n+\n+package compiler.onSpinWait;\n+\n+import java.util.ArrayList;\n+import java.util.Iterator;\n+import java.util.ListIterator;\n+import jdk.test.lib.process.OutputAnalyzer;\n+import jdk.test.lib.process.ProcessTools;\n+\n+public class TestOnSpinWaitAArch64 {\n+    public static void main(String[] args) throws Exception {\n+        String compiler = args[0];\n+        String spinWaitInst = args[1];\n+        String spinWaitInstCount = (args.length == 3) ? args[2] : \"1\";\n+        ArrayList<String> command = new ArrayList<String>();\n+        command.add(\"-XX:+IgnoreUnrecognizedVMOptions\");\n+        command.add(\"-showversion\");\n+        command.add(\"-XX:-BackgroundCompilation\");\n+        command.add(\"-XX:+UnlockDiagnosticVMOptions\");\n+        command.add(\"-XX:+PrintAssembly\");\n+        if (compiler.equals(\"c2\")) {\n+            command.add(\"-XX:-TieredCompilation\");\n+        } else if (compiler.equals(\"c1\")) {\n+            command.add(\"-XX:+TieredCompilation\");\n+            command.add(\"-XX:TieredStopAtLevel=1\");\n+        } else {\n+            throw new RuntimeException(\"Unknown compiler: \" + compiler);\n+        }\n+        command.add(\"-Xbatch\");\n+        command.add(\"-XX:OnSpinWaitInst=\" + spinWaitInst);\n+        command.add(\"-XX:OnSpinWaitInstCount=\" + spinWaitInstCount);\n+        command.add(\"-XX:CompileCommand=compileonly,\" + Launcher.class.getName() + \"::\" + \"test\");\n+        command.add(Launcher.class.getName());\n+\n+        ProcessBuilder pb = ProcessTools.createJavaProcessBuilder(command);\n+\n+        OutputAnalyzer analyzer = new OutputAnalyzer(pb.start());\n+\n+        analyzer.shouldHaveExitValue(0);\n+\n+        System.out.println(analyzer.getOutput());\n+\n+        checkOutput(analyzer, spinWaitInst, Integer.parseInt(spinWaitInstCount));\n+    }\n+\n+    private static String getSpinWaitInstHex(String spinWaitInst) {\n+      if (\"nop\".equals(spinWaitInst)) {\n+          return \"1f20 03d5\";\n+      } else if (\"isb\".equals(spinWaitInst)) {\n+          return \"df3f 03d5\";\n+      } else if (\"yield\".equals(spinWaitInst)) {\n+          return \"3f20 03d5\";\n+      } else {\n+          throw new RuntimeException(\"Unknown spin wait instruction: \" + spinWaitInst);\n+      }\n+    }\n+\n+    private static void addInstrs(String line, ArrayList<String> instrs) {\n+        for (String instr : line.split(\"\\\\|\")) {\n+            instrs.add(instr.trim());\n+        }\n+    }\n+\n+    \/\/ The expected output of PrintAssembly for example for a spin wait with three NOPs:\n+    \/\/\n+    \/\/ # {method} {0x0000ffff6ac00370} 'test' '()V' in 'compiler\/onSpinWait\/TestOnSpinWaitAArch64$Launcher'\n+    \/\/ #           [sp+0x40]  (sp of caller)\n+    \/\/ 0x0000ffff9d557680: 1f20 03d5 | e953 40d1 | 3f01 00f9 | ff03 01d1 | fd7b 03a9 | 1f20 03d5 | 1f20 03d5\n+    \/\/\n+    \/\/ 0x0000ffff9d5576ac: ;*invokestatic onSpinWait {reexecute=0 rethrow=0 return_oop=0}\n+    \/\/                     ; - compiler.onSpinWait.TestOnSpinWaitAArch64$Launcher::test@0 (line 161)\n+    \/\/ 0x0000ffff9d5576ac: 1f20 03d5 | fd7b 43a9 | ff03 0191\n+    \/\/\n+    \/\/ The checkOutput method adds hex instructions before 'invokestatic onSpinWait' and from the line after\n+    \/\/ it to a list. The list is traversed from the end to count spin wait instructions.\n+    \/\/\n+    \/\/ If JVM finds the hsdis library the output is like:\n+    \/\/\n+    \/\/ # {method} {0x0000ffff63000370} 'test' '()V' in 'compiler\/onSpinWait\/TestOnSpinWaitAArch64$Launcher'\n+    \/\/ #           [sp+0x20]  (sp of caller)\n+    \/\/ 0x0000ffffa409da80:   nop\n+    \/\/ 0x0000ffffa409da84:   sub sp, sp, #0x20\n+    \/\/ 0x0000ffffa409da88:   stp x29, x30, [sp, #16]         ;*synchronization entry\n+    \/\/                                                       ; - compiler.onSpinWait.TestOnSpinWaitAArch64$Launcher::test@-1 (line 187)\n+    \/\/ 0x0000ffffa409da8c:   nop\n+    \/\/ 0x0000ffffa409da90:   nop\n+    \/\/ 0x0000ffffa409da94:   nop\n+    \/\/ 0x0000ffffa409da98:   nop\n+    \/\/ 0x0000ffffa409da9c:   nop\n+    \/\/ 0x0000ffffa409daa0:   nop\n+    \/\/ 0x0000ffffa409daa4:   nop                                 ;*invokestatic onSpinWait {reexecute=0 rethrow=0 return_oop=0}\n+    \/\/                                                           ; - compiler.onSpinWait.TestOnSpinWaitAArch64$Launcher::test@0 (line 187)\n+    private static void checkOutput(OutputAnalyzer output, String spinWaitInst, int spinWaitInstCount) {\n+        Iterator<String> iter = output.asLines().listIterator();\n+\n+        String match = skipTo(iter, \"'test' '()V' in 'compiler\/onSpinWait\/TestOnSpinWaitAArch64$Launcher'\");\n+        if (match == null) {\n+            throw new RuntimeException(\"Missing compiler output for the method compiler.onSpinWait.TestOnSpinWaitAArch64$Launcher::test\");\n+        }\n+\n+        ArrayList<String> instrs = new ArrayList<String>();\n+        String line = null;\n+        boolean hasHexInstInOutput = false;\n+        while (iter.hasNext()) {\n+            line = iter.next();\n+            if (line.contains(\"*invokestatic onSpinWait\")) {\n+                break;\n+            }\n+            if (!hasHexInstInOutput) {\n+                hasHexInstInOutput = line.contains(\"|\");\n+            }\n+            if (line.contains(\"0x\") && !line.contains(\";\")) {\n+                addInstrs(line, instrs);\n+            }\n+        }\n+\n+        if (!iter.hasNext() || !iter.next().contains(\"- compiler.onSpinWait.TestOnSpinWaitAArch64$Launcher::test@0\") || !iter.hasNext()) {\n+            throw new RuntimeException(\"Missing compiler output for Thread.onSpinWait intrinsic\");\n+        }\n+\n+        String strToSearch = null;\n+        if (!hasHexInstInOutput) {\n+            instrs.add(line.split(\";\")[0].trim());\n+            strToSearch = spinWaitInst;\n+        } else {\n+            line = iter.next();\n+            if (!line.contains(\"0x\") || line.contains(\";\")) {\n+                throw new RuntimeException(\"Expected hex instructions\");\n+            }\n+\n+            addInstrs(line, instrs);\n+            strToSearch = getSpinWaitInstHex(spinWaitInst);\n+        }\n+\n+        int foundInstCount = 0;\n+\n+        ListIterator<String> instrReverseIter = instrs.listIterator(instrs.size());\n+        while (instrReverseIter.hasPrevious()) {\n+            if (instrReverseIter.previous().endsWith(strToSearch)) {\n+                foundInstCount = 1;\n+                break;\n+            }\n+        }\n+\n+        while (instrReverseIter.hasPrevious()) {\n+            if (!instrReverseIter.previous().endsWith(strToSearch)) {\n+                break;\n+            }\n+            ++foundInstCount;\n+        }\n+\n+        if (foundInstCount != spinWaitInstCount) {\n+            throw new RuntimeException(\"Wrong instruction \" + strToSearch + \" count \" + foundInstCount + \"!\\n  -- expecting \" + spinWaitInstCount);\n+        }\n+    }\n+\n+    private static String skipTo(Iterator<String> iter, String substring) {\n+        while (iter.hasNext()) {\n+            String nextLine = iter.next();\n+            if (nextLine.contains(substring)) {\n+                return nextLine;\n+            }\n+        }\n+        return null;\n+    }\n+\n+    static class Launcher {\n+        public static void main(final String[] args) throws Exception {\n+            int end = 20_000;\n+\n+            for (int i=0; i < end; i++) {\n+                test();\n+            }\n+        }\n+        static void test() {\n+            java.lang.Thread.onSpinWait();\n+        }\n+    }\n+}\n","filename":"test\/hotspot\/jtreg\/compiler\/onSpinWait\/TestOnSpinWaitAArch64.java","additions":218,"deletions":0,"binary":false,"changes":218,"status":"added"},{"patch":"@@ -0,0 +1,97 @@\n+\/*\n+ * Copyright Amazon.com Inc. or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+\/**\n+ * @test TestOnSpinWaitAArch64DefaultFlags\n+ * @summary Check default values of '-XX:OnSpinWaitInst' and '-XX:OnSpinWaitInstCount' for AArch64 implementations.\n+ * @bug 8277137\n+ * @library \/test\/lib \/\n+ *\n+ * @requires os.arch==\"aarch64\"\n+ *\n+ * @build sun.hotspot.WhiteBox\n+ * @run driver jdk.test.lib.helpers.ClassFileInstaller sun.hotspot.WhiteBox\n+ * @run main\/othervm -Xbootclasspath\/a:. -XX:+UnlockDiagnosticVMOptions\n+ *                   -XX:+WhiteBoxAPI\n+ *                   compiler.onSpinWait.TestOnSpinWaitAArch64DefaultFlags\n+ *\/\n+\n+package compiler.onSpinWait;\n+\n+import java.util.Iterator;\n+import java.util.List;\n+import jdk.test.lib.process.OutputAnalyzer;\n+import jdk.test.lib.process.ProcessTools;\n+import sun.hotspot.cpuinfo.CPUInfo;\n+\n+public class TestOnSpinWaitAArch64DefaultFlags {\n+    private static boolean isCPUModelNeoverseN1(String cpuModel) {\n+        return cpuModel.contains(\"0xd0c\");\n+    }\n+\n+    private static void checkFinalFlagsEqualTo(ProcessBuilder pb, String expectedOnSpinWaitInstValue, String expectedOnSpinWaitInstCountValue) throws Exception {\n+        OutputAnalyzer analyzer = new OutputAnalyzer(pb.start());\n+        analyzer.shouldHaveExitValue(0);\n+\n+        Iterator<String> iter = analyzer.asLines().listIterator();\n+        String line = null;\n+        boolean hasExpectedOnSpinWaitInstValue = false;\n+        boolean hasExpectedOnSpinWaitInstCountValue = false;\n+        while (iter.hasNext()) {\n+            line = iter.next();\n+            if (!hasExpectedOnSpinWaitInstValue && line.contains(\"ccstr OnSpinWaitInst\")) {\n+                hasExpectedOnSpinWaitInstValue = line.contains(\"= \" + expectedOnSpinWaitInstValue);\n+            }\n+\n+            if (!hasExpectedOnSpinWaitInstCountValue && line.contains(\"uint OnSpinWaitInstCount\")) {\n+                hasExpectedOnSpinWaitInstCountValue = line.contains(\"= \" + expectedOnSpinWaitInstCountValue);\n+            }\n+        }\n+        if (!hasExpectedOnSpinWaitInstValue) {\n+            System.out.println(analyzer.getOutput());\n+            throw new RuntimeException(\"OnSpinWaitInst with the expected value '\" + expectedOnSpinWaitInstValue + \"' not found.\");\n+        }\n+        if (!hasExpectedOnSpinWaitInstCountValue) {\n+            System.out.println(analyzer.getOutput());\n+            throw new RuntimeException(\"OnSpinWaitInstCount with the expected value '\" + expectedOnSpinWaitInstCountValue + \"' not found.\");\n+        }\n+    }\n+\n+    public static void main(String[] args) throws Exception {\n+        List<String> cpuFeatures = CPUInfo.getFeatures();\n+        if (cpuFeatures.isEmpty()) {\n+            System.out.println(\"Skip because no CPU features are available.\");\n+            return;\n+        }\n+\n+        final String cpuModel = cpuFeatures.get(0);\n+\n+        if (isCPUModelNeoverseN1(cpuModel)) {\n+            checkFinalFlagsEqualTo(ProcessTools.createJavaProcessBuilder(\"-XX:+PrintFlagsFinal\", \"-version\"), \"isb\", \"1\");\n+            checkFinalFlagsEqualTo(ProcessTools.createJavaProcessBuilder(\"-XX:+UnlockDiagnosticVMOptions\", \"-XX:OnSpinWaitInstCount=2\", \"-XX:+PrintFlagsFinal\", \"-version\"),\n+                \"isb\", \"2\");\n+        } else {\n+            System.out.println(\"Skip because no defaults for CPU model: \" + cpuModel);\n+        }\n+    }\n+}\n","filename":"test\/hotspot\/jtreg\/compiler\/onSpinWait\/TestOnSpinWaitAArch64DefaultFlags.java","additions":97,"deletions":0,"binary":false,"changes":97,"status":"added"},{"patch":"@@ -0,0 +1,82 @@\n+\/*\n+ * Copyright (c) 2021, Amazon.com Inc. or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+\/**\n+ * @test TestOnSpinWaitNoneAArch64\n+ * @summary Checks that java.lang.Thread.onSpinWait is not intrinsified when '-XX:OnSpinWaitInst=none' is used\n+ * @bug 8186670\n+ * @library \/test\/lib\n+ *\n+ * @requires vm.flagless\n+ * @requires os.arch==\"aarch64\"\n+ *\n+ * @run driver compiler.onSpinWait.TestOnSpinWaitNoneAArch64\n+ *\/\n+\n+package compiler.onSpinWait;\n+\n+import java.util.ArrayList;\n+import jdk.test.lib.process.OutputAnalyzer;\n+import jdk.test.lib.process.ProcessTools;\n+\n+public class TestOnSpinWaitNoneAArch64 {\n+\n+    public static void main(String[] args) throws Exception {\n+        ArrayList<String> command = new ArrayList<String>();\n+        command.add(\"-XX:+IgnoreUnrecognizedVMOptions\");\n+        command.add(\"-showversion\");\n+        command.add(\"-XX:-TieredCompilation\");\n+        command.add(\"-Xbatch\");\n+        command.add(\"-XX:+PrintCompilation\");\n+        command.add(\"-XX:+UnlockDiagnosticVMOptions\");\n+        command.add(\"-XX:+PrintInlining\");\n+        command.add(\"-XX:OnSpinWaitInst=none\");\n+        command.add(Launcher.class.getName());\n+\n+        \/\/ Test C2 compiler\n+        ProcessBuilder pb = ProcessTools.createJavaProcessBuilder(command);\n+\n+        OutputAnalyzer analyzer = new OutputAnalyzer(pb.start());\n+\n+        analyzer.shouldHaveExitValue(0);\n+\n+        \/\/ The test is applicable only to C2 (present in Server VM).\n+        if (analyzer.getStderr().contains(\"Server VM\")) {\n+            analyzer.shouldNotContain(\"java.lang.Thread::onSpinWait (1 bytes)   (intrinsic)\");\n+        }\n+    }\n+\n+    static class Launcher {\n+\n+        public static void main(final String[] args) throws Exception {\n+            int end = 20_000;\n+\n+            for (int i=0; i < end; i++) {\n+                test();\n+            }\n+        }\n+        static void test() {\n+            java.lang.Thread.onSpinWait();\n+        }\n+    }\n+}\n","filename":"test\/hotspot\/jtreg\/compiler\/onSpinWait\/TestOnSpinWaitNoneAArch64.java","additions":82,"deletions":0,"binary":false,"changes":82,"status":"added"},{"patch":"@@ -1,61 +0,0 @@\n-\/*\n- * Copyright (c) 2017, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-\n-\/* @test\n- * @bug 8167409\n- * @requires (os.arch != \"aarch64\") & (os.arch != \"arm\") & (vm.flavor != \"zero\")\n- * @run main\/othervm\/native -Xcomp -XX:+CriticalJNINatives compiler.runtime.criticalnatives.argumentcorruption.CheckLongArgs\n- *\/\n-package compiler.runtime.criticalnatives.argumentcorruption;\n-public class CheckLongArgs {\n-    static {\n-        System.loadLibrary(\"CNCheckLongArgs\");\n-    }\n-    static native void m1(long a1, long a2, long a3, long a4,  long a5, long a6, long a7, long a8, byte[] result);\n-    static native void m2(long a1, int[] a2, long a3, int[] a4, long a5, int[] a6, long a7, int[] a8, long a9, byte[] result);\n-    public static void main(String args[]) throws Exception {\n-        test();\n-    }\n-    private static void test() throws Exception {\n-        int[] l1 = { 1111, 2222, 3333 };\n-        int[] l2 = { 4444, 5555, 6666 };\n-        int[] l3 = { 7777, 8888, 9999 };\n-        int[] l4 = { 1010, 2020, 3030 };\n-        byte[] result = { -1 };\n-        m1(1111111122222222L, 3333333344444444L, 5555555566666666L, 7777777788888888L, 9999999900000000L, 1212121234343434L,\n-           5656565678787878L, 9090909012121212L, result);\n-        check(result[0]);\n-        result[0] = -1;\n-        m2(1111111122222222L, l1, 3333333344444444L, l2, 5555555566666666L, l3, 7777777788888888L, l4, 9999999900000000L, result);\n-        check(result[0]);\n-    }\n-    private static void check(byte result) throws Exception {\n-        if (result != 2) {\n-            if (result == 1) {\n-              throw new Exception(\"critical native arguments mismatch\");\n-            }\n-            throw new Exception(\"critical native lookup failed\");\n-        }\n-    }\n-}\n","filename":"test\/hotspot\/jtreg\/compiler\/runtime\/criticalnatives\/argumentcorruption\/CheckLongArgs.java","additions":0,"deletions":61,"binary":false,"changes":61,"status":"deleted"},{"patch":"@@ -1,30 +0,0 @@\n-#include \"jni.h\"\n-JNIEXPORT void JNICALL JavaCritical_compiler_runtime_criticalnatives_argumentcorruption_CheckLongArgs_m1\n-  (jlong a1, jlong a2, jlong a3, jlong a4, jlong a5, jlong a6, jlong a7, jlong a8,jint result_length,jbyte* result) {\n-\n-  if (a1 != 1111111122222222LL || a2 != 3333333344444444LL || a3 != 5555555566666666LL || a4 != 7777777788888888LL ||\n-      a5 != 9999999900000000LL || a6 != 1212121234343434LL || a7 != 5656565678787878LL || a8 != 9090909012121212LL ||\n-      result_length != 1 || result[0] != -1) {\n-    result[0] = 1;\n-  } else {\n-    result[0] = 2;\n-  }\n-}\n-\n-JNIEXPORT void JNICALL JavaCritical_compiler_runtime_criticalnatives_argumentcorruption_CheckLongArgs_m2\n-  (jlong a1, jint a2_length, jint* a2, jlong a3, jint a4_length, jint* a4, jlong a5, jint a6_length, jint* a6, jlong a7,\n-   jint a8_length, jint* a8, jlong a9, jint result_length, jbyte* result) {\n-  if (a1 != 1111111122222222LL || a2_length != 3 || a2[0] != 1111 || a3 != 3333333344444444LL || a4_length != 3 || a4[0] != 4444 ||\n-      a5 != 5555555566666666LL || a6_length != 3 || a6[0] != 7777 || a7 != 7777777788888888LL || a8_length != 3 || a8[0] != 1010 || a9 != 9999999900000000LL ||\n-      result_length != 1 || result[0] != -1) {\n-    result[0] = 1;\n-  } else {\n-    result[0] = 2;\n-  }\n-}\n-\n-JNIEXPORT void JNICALL Java_compiler_runtime_criticalnatives_argumentcorruption_CheckLongArgs_m1\n-  (JNIEnv * env, jclass jclazz, jlong a3, jlong a4, jlong a5, jlong a6, jlong a7, jlong a8, jlong a9, jlong a10, jbyteArray result) {}\n-\n-JNIEXPORT void JNICALL Java_compiler_runtime_criticalnatives_argumentcorruption_CheckLongArgs_m2\n-  (JNIEnv * env, jclass jclazz, jlong a3, jintArray a4, jlong a5, jintArray a6, jlong a7, jintArray a8, jlong a9, jintArray a10, jlong a11, jbyteArray result) {}\n","filename":"test\/hotspot\/jtreg\/compiler\/runtime\/criticalnatives\/argumentcorruption\/libCNCheckLongArgs.c","additions":0,"deletions":30,"binary":false,"changes":30,"status":"deleted"},{"patch":"@@ -1,60 +0,0 @@\n-\/*\n- * Copyright (c) 2017, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-\n-\/* @test\n- * @bug 8167408\n- * @requires (os.arch != \"aarch64\") & (os.arch != \"arm\") & (vm.flavor != \"zero\")\n- * @run main\/othervm\/native -Xcomp -XX:+CriticalJNINatives compiler.runtime.criticalnatives.lookup.LookUp\n- *\/\n-package compiler.runtime.criticalnatives.lookup;\n-public class LookUp {\n-    static {\n-        System.loadLibrary(\"CNLookUp\");\n-    }\n-    static native void m1(byte a1, long a2, char a3, int a4,  float a5, double a6, byte[] result);\n-    static native void m2(int a1, int[] a2, long a3, long[] a4, float a5,float[] a6, double a7, double[] a8, byte result[]);\n-    public static void main(String args[]) throws Exception {\n-        test();\n-    }\n-    private static void test() throws Exception {\n-        int[] l1 = { 1111, 2222, 3333 };\n-        long[] l2 = { 4444L, 5555L, 6666L };\n-        float[] l3 = { 7777.0F, 8888.0F, 9999.0F };\n-        double[] l4 = { 4545.0D, 5656.0D, 6767.0D };\n-        byte[] result = { -1 };\n-        m1((byte)0xA, 4444444455555555L, 'A', 12345678, 343434.0F, 6666666677777777.0D, result);\n-        check(result[0]);\n-        result[0] = -1;\n-        m2(12345678, l1, 4444444455555555L, l2, 343434.0F, l3, 6666666677777777.0D, l4, result);\n-        check(result[0]);\n-    }\n-    private static void check(byte result) throws Exception {\n-        if (result != 2) {\n-            if (result == 1) {\n-              throw new Exception(\"critical native arguments mismatch\");\n-            }\n-            throw new Exception(\"critical native lookup failed\");\n-        }\n-    }\n-}\n","filename":"test\/hotspot\/jtreg\/compiler\/runtime\/criticalnatives\/lookup\/LookUp.java","additions":0,"deletions":60,"binary":false,"changes":60,"status":"deleted"},{"patch":"@@ -1,35 +0,0 @@\n-#include \"jni.h\"\n-JNIEXPORT void JNICALL JavaCritical_compiler_runtime_criticalnatives_lookup_LookUp_m1\n-  (jbyte a1, jlong a2, jchar a3, jint a4, jfloat a5, jdouble a6, jint result_length, jbyte* result) {\n-  jint l1 = (jint) a5;\n-  jlong l2 = (jlong) a6;\n-\n-  if (a1 != 0xA || a2 != 4444444455555555LL || a3 != 0x41 || a4 != 12345678 || l1 != 343434 || l2 != 6666666677777777LL ||\n-      result_length != 1 || result[0] != -1) {\n-    result[0] = 1;\n-  } else {\n-    result[0] = 2;\n-  }\n-}\n-\n-JNIEXPORT void JNICALL JavaCritical_compiler_runtime_criticalnatives_lookup_LookUp_m2\n-  (jint a1, jint a2_length, jint* a2, jlong a3, jint a4_length, jlong* a4, jfloat a5, jint a6_length, jfloat* a6, jdouble a7,\n-   jint a8_length, jdouble* a8, jint result_length, jbyte* result) {\n-  jint l1 = (jint) a5;\n-  jlong l2 = (jlong) a7;\n-\n-  if (a1 != 12345678 || a2_length != 3 || a2[0] != 1111 || a3 != 4444444455555555LL || a4_length != 3 || a4[0] != 4444 ||\n-      l1 != 343434 ||  a6_length != 3 ||  7777 != (jint)a6[0] || l2 != 6666666677777777LL || a8_length != 3 || 4545 != (jlong)a8[0] ||\n-      result_length != 1 || result[0] != -1) {\n-    result[0] = 1;\n-  } else {\n-    result[0] = 2;\n-  }\n-}\n-\n-JNIEXPORT void JNICALL Java_compiler_runtime_criticalnatives_lookup_LookUp_m1\n-  (JNIEnv * env, jclass jclazz, jbyte a3, jlong a4, jchar a5, jint a6, jfloat a7, jdouble a8, jbyteArray result) {}\n-\n-JNIEXPORT void JNICALL Java_compiler_runtime_criticalnatives_lookup_LookUp_m2\n-  (JNIEnv * env, jclass jclazz, jint a3, jintArray a4, jlong a5, jlongArray a6, jfloat a7, jfloatArray a8, jdouble a9, jdoubleArray a10, jbyteArray result) {}\n-\n","filename":"test\/hotspot\/jtreg\/compiler\/runtime\/criticalnatives\/lookup\/libCNLookUp.c","additions":0,"deletions":35,"binary":false,"changes":35,"status":"deleted"},{"patch":"@@ -0,0 +1,50 @@\n+\/*\n+ * Copyright (c) 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+package compiler.vectorapi;\n+\n+import jdk.incubator.vector.LongVector;\n+\n+\/*\n+ * @test\n+ * @bug 8275643\n+ * @summary Test that LongVector.neg is properly handled by the _VectorUnaryOp C2 intrinsic\n+ * @modules jdk.incubator.vector\n+ * @run main\/othervm -XX:-TieredCompilation -XX:+AlwaysIncrementalInline -Xbatch\n+ *                   -XX:CompileCommand=dontinline,compiler.vectorapi.TestLongVectorNeg::test\n+ *                   compiler.vectorapi.TestLongVectorNeg\n+ *\/\n+public class TestLongVectorNeg {\n+\n+    static LongVector test(LongVector v) {\n+        return v.neg();\n+    }\n+\n+    public static void main(String[] args) {\n+        LongVector v = LongVector.zero(LongVector.SPECIES_128);\n+        for (int i = 0; i < 50_000; ++i) {\n+            v.abs(); \/\/ Warmup to make sure the !VO_SPECIAL code path is taken as well\n+            test(v);\n+        }\n+    }\n+}\n","filename":"test\/hotspot\/jtreg\/compiler\/vectorapi\/TestLongVectorNeg.java","additions":50,"deletions":0,"binary":false,"changes":50,"status":"added"},{"patch":"@@ -0,0 +1,468 @@\n+\/*\n+ * Copyright (c) 2021, Arm Limited. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+package compiler.vectorapi;\n+\n+import java.util.Random;\n+\n+import jdk.incubator.vector.ByteVector;\n+import jdk.incubator.vector.DoubleVector;\n+import jdk.incubator.vector.FloatVector;\n+import jdk.incubator.vector.IntVector;\n+import jdk.incubator.vector.LongVector;\n+import jdk.incubator.vector.ShortVector;\n+import jdk.incubator.vector.VectorMask;\n+import jdk.test.lib.Utils;\n+\n+import org.testng.Assert;\n+import org.testng.annotations.Test;\n+\n+\/**\n+ * @test\n+ * @bug 8273264\n+ * @key randomness\n+ * @library \/test\/lib\n+ * @summary AArch64: [vector] Add missing rules for VectorMaskCast\n+ * @modules jdk.incubator.vector\n+ *\n+ * @run testng\/othervm -XX:-TieredCompilation -XX:CompileThreshold=100 compiler.vectorapi.VectorMaskCastTest\n+ *\/\n+\n+\n+\/\/ Current vector mask cast test cases at test\/jdk\/jdk\/incubator\/vector\/*ConversionTests.java\n+\/\/ could not be intrinsfied, hence not able to verify compiler codegen, see [1]. As a\n+\/\/ supplement, we add more tests for vector mask cast operations, which could be intrinsified\n+\/\/ by c2 compiler to generate vector\/mask instructions on supported targets.\n+\/\/\n+\/\/ [1] https:\/\/bugs.openjdk.java.net\/browse\/JDK-8259610\n+\n+public class VectorMaskCastTest{\n+\n+    private static final int NUM_ITER = 5000;\n+    private static final Random rd = Utils.getRandomInstance();\n+\n+    public static boolean[] genMask() {\n+        boolean[] mask = new boolean[64];\n+        for (int i = 0; i < 64; i ++) {\n+            mask[i] = rd.nextBoolean();\n+        }\n+        return mask;\n+    }\n+\n+    \/\/ Byte\n+    private static void testByte64ToShort128(boolean[] mask_arr) {\n+        VectorMask<Byte> mByte64 = VectorMask.fromArray(ByteVector.SPECIES_64, mask_arr, 0);\n+        Assert.assertEquals(mByte64.cast(ShortVector.SPECIES_128).toString(), mByte64.toString());\n+    }\n+\n+    private static void testByte64ToInt256(boolean[] mask_arr) {\n+        VectorMask<Byte> mByte64 = VectorMask.fromArray(ByteVector.SPECIES_64, mask_arr, 0);\n+        Assert.assertEquals(mByte64.cast(IntVector.SPECIES_256).toString(), mByte64.toString());\n+    }\n+\n+    private static void testByte64ToFloat256(boolean[] mask_arr) {\n+        VectorMask<Byte> mByte64 = VectorMask.fromArray(ByteVector.SPECIES_64, mask_arr, 0);\n+        Assert.assertEquals(mByte64.cast(FloatVector.SPECIES_256).toString(), mByte64.toString());\n+    }\n+\n+    private static void testByte64ToLong512(boolean[] mask_arr) {\n+        VectorMask<Byte> mByte64 = VectorMask.fromArray(ByteVector.SPECIES_64, mask_arr, 0);\n+        Assert.assertEquals(mByte64.cast(LongVector.SPECIES_512).toString(), mByte64.toString());\n+    }\n+\n+    private static void testByte64ToDouble512(boolean[] mask_arr) {\n+        VectorMask<Byte> mByte64 = VectorMask.fromArray(ByteVector.SPECIES_64, mask_arr, 0);\n+        Assert.assertEquals(mByte64.cast(DoubleVector.SPECIES_512).toString(), mByte64.toString());\n+    }\n+\n+    private static void testByte128ToShort256(boolean[] mask_arr) {\n+        VectorMask<Byte> mByte128 = VectorMask.fromArray(ByteVector.SPECIES_128, mask_arr, 0);\n+        Assert.assertEquals(mByte128.cast(ShortVector.SPECIES_256).toString(), mByte128.toString());\n+    }\n+\n+    private static void testByte128ToInt512(boolean[] mask_arr) {\n+        VectorMask<Byte> mByte128 = VectorMask.fromArray(ByteVector.SPECIES_128, mask_arr, 0);\n+        Assert.assertEquals(mByte128.cast(IntVector.SPECIES_512).toString(), mByte128.toString());\n+    }\n+\n+    private static void testByte128ToFloat512(boolean[] mask_arr) {\n+        VectorMask<Byte> mByte128 = VectorMask.fromArray(ByteVector.SPECIES_128, mask_arr, 0);\n+        Assert.assertEquals(mByte128.cast(FloatVector.SPECIES_512).toString(), mByte128.toString());\n+    }\n+\n+    private static void testByte256ToShort512(boolean[] mask_arr) {\n+        VectorMask<Byte> mByte256 = VectorMask.fromArray(ByteVector.SPECIES_256, mask_arr, 0);\n+        Assert.assertEquals(mByte256.cast(ShortVector.SPECIES_512).toString(), mByte256.toString());\n+    }\n+\n+    \/\/ Short\n+    private static void testShort64ToInt128(boolean[] mask_arr) {\n+        VectorMask<Short> mShort64 = VectorMask.fromArray(ShortVector.SPECIES_64, mask_arr, 0);\n+        Assert.assertEquals(mShort64.cast(IntVector.SPECIES_128).toString(), mShort64.toString());\n+    }\n+\n+    private static void testShort64ToFloat128(boolean[] mask_arr) {\n+        VectorMask<Short> mShort64 = VectorMask.fromArray(ShortVector.SPECIES_64, mask_arr, 0);\n+        Assert.assertEquals(mShort64.cast(FloatVector.SPECIES_128).toString(), mShort64.toString());\n+    }\n+\n+    private static void testShort64ToLong256(boolean[] mask_arr) {\n+        VectorMask<Short> mShort64 = VectorMask.fromArray(ShortVector.SPECIES_64, mask_arr, 0);\n+        Assert.assertEquals(mShort64.cast(LongVector.SPECIES_256).toString(), mShort64.toString());\n+    }\n+\n+    private static void testShort64ToDouble256(boolean[] mask_arr) {\n+        VectorMask<Short> mShort64 = VectorMask.fromArray(ShortVector.SPECIES_64, mask_arr, 0);\n+        Assert.assertEquals(mShort64.cast(DoubleVector.SPECIES_256).toString(), mShort64.toString());\n+    }\n+\n+    private static void testShort128ToByte64(boolean[] mask_arr) {\n+        VectorMask<Short> mShort128 = VectorMask.fromArray(ShortVector.SPECIES_128, mask_arr, 0);\n+        Assert.assertEquals(mShort128.cast(ByteVector.SPECIES_64).toString(), mShort128.toString());\n+    }\n+\n+    private static void testShort128ToInt256(boolean[] mask_arr) {\n+        VectorMask<Short> mShort128 = VectorMask.fromArray(ShortVector.SPECIES_128, mask_arr, 0);\n+        Assert.assertEquals(mShort128.cast(IntVector.SPECIES_256).toString(), mShort128.toString());\n+    }\n+\n+    private static void testShort128ToFloat256(boolean[] mask_arr) {\n+        VectorMask<Short> mShort128 = VectorMask.fromArray(ShortVector.SPECIES_128, mask_arr, 0);\n+        Assert.assertEquals(mShort128.cast(FloatVector.SPECIES_256).toString(), mShort128.toString());\n+    }\n+\n+    private static void testShort128ToLong512(boolean[] mask_arr) {\n+        VectorMask<Short> mShort128 = VectorMask.fromArray(ShortVector.SPECIES_128, mask_arr, 0);\n+        Assert.assertEquals(mShort128.cast(LongVector.SPECIES_512).toString(), mShort128.toString());\n+    }\n+\n+    private static void testShort128ToDouble512(boolean[] mask_arr) {\n+        VectorMask<Short> mShort128 = VectorMask.fromArray(ShortVector.SPECIES_128, mask_arr, 0);\n+        Assert.assertEquals(mShort128.cast(DoubleVector.SPECIES_512).toString(), mShort128.toString());\n+    }\n+\n+    private static void testShort256ToByte128(boolean[] mask_arr) {\n+        VectorMask<Short> mShort256 = VectorMask.fromArray(ShortVector.SPECIES_256, mask_arr, 0);\n+        Assert.assertEquals(mShort256.cast(ByteVector.SPECIES_128).toString(), mShort256.toString());\n+    }\n+\n+    private static void testShort256ToInt512(boolean[] mask_arr) {\n+        VectorMask<Short> mShort256 = VectorMask.fromArray(ShortVector.SPECIES_256, mask_arr, 0);\n+        Assert.assertEquals(mShort256.cast(IntVector.SPECIES_512).toString(), mShort256.toString());\n+    }\n+\n+    private static void testShort256ToFloat512(boolean[] mask_arr) {\n+        VectorMask<Short> mShort256 = VectorMask.fromArray(ShortVector.SPECIES_256, mask_arr, 0);\n+        Assert.assertEquals(mShort256.cast(FloatVector.SPECIES_512).toString(), mShort256.toString());\n+    }\n+\n+    private static void testShort512ToByte256(boolean[] mask_arr) {\n+        VectorMask<Short> mShort512 = VectorMask.fromArray(ShortVector.SPECIES_512, mask_arr, 0);\n+        Assert.assertEquals(mShort512.cast(ByteVector.SPECIES_256).toString(), mShort512.toString());\n+    }\n+\n+    \/\/ Int\n+    private static void testInt64ToLong128(boolean[] mask_arr) {\n+        VectorMask<Integer> mInt64 = VectorMask.fromArray(IntVector.SPECIES_64, mask_arr, 0);\n+        Assert.assertEquals(mInt64.cast(LongVector.SPECIES_128).toString(), mInt64.toString());\n+    }\n+\n+    private static void testInt64ToDouble128(boolean[] mask_arr) {\n+        VectorMask<Integer> mInt64 = VectorMask.fromArray(IntVector.SPECIES_64, mask_arr, 0);\n+        Assert.assertEquals(mInt64.cast(DoubleVector.SPECIES_128).toString(), mInt64.toString());\n+    }\n+\n+    private static void testInt128ToShort64(boolean[] mask_arr) {\n+        VectorMask<Integer> mInt128 = VectorMask.fromArray(IntVector.SPECIES_128, mask_arr, 0);\n+        Assert.assertEquals(mInt128.cast(ShortVector.SPECIES_64).toString(), mInt128.toString());\n+    }\n+\n+    private static void testInt128ToLong256(boolean[] mask_arr) {\n+        VectorMask<Integer> mInt128 = VectorMask.fromArray(IntVector.SPECIES_128, mask_arr, 0);\n+        Assert.assertEquals(mInt128.cast(LongVector.SPECIES_256).toString(), mInt128.toString());\n+    }\n+\n+    private static void testInt128ToDouble256(boolean[] mask_arr) {\n+        VectorMask<Integer> mInt128 = VectorMask.fromArray(IntVector.SPECIES_128, mask_arr, 0);\n+        Assert.assertEquals(mInt128.cast(DoubleVector.SPECIES_256).toString(), mInt128.toString());\n+    }\n+\n+    private static void testInt256ToShort128(boolean[] mask_arr) {\n+        VectorMask<Integer> mInt256 = VectorMask.fromArray(IntVector.SPECIES_256, mask_arr, 0);\n+        Assert.assertEquals(mInt256.cast(ShortVector.SPECIES_128).toString(), mInt256.toString());\n+    }\n+\n+    private static void testInt256ToByte64(boolean[] mask_arr) {\n+        VectorMask<Integer> mInt256 = VectorMask.fromArray(IntVector.SPECIES_256, mask_arr, 0);\n+        Assert.assertEquals(mInt256.cast(ByteVector.SPECIES_64).toString(), mInt256.toString());\n+    }\n+\n+    private static void testInt256ToLong512(boolean[] mask_arr) {\n+        VectorMask<Integer> mInt256 = VectorMask.fromArray(IntVector.SPECIES_256, mask_arr, 0);\n+        Assert.assertEquals(mInt256.cast(LongVector.SPECIES_512).toString(), mInt256.toString());\n+    }\n+\n+    private static void testInt256ToDouble512(boolean[] mask_arr) {\n+        VectorMask<Integer> mInt256 = VectorMask.fromArray(IntVector.SPECIES_256, mask_arr, 0);\n+        Assert.assertEquals(mInt256.cast(DoubleVector.SPECIES_512).toString(), mInt256.toString());\n+    }\n+\n+    private static void testInt512ToShort256(boolean[] mask_arr) {\n+        VectorMask<Integer> mInt512 = VectorMask.fromArray(IntVector.SPECIES_512, mask_arr, 0);\n+        Assert.assertEquals(mInt512.cast(ShortVector.SPECIES_256).toString(), mInt512.toString());\n+    }\n+\n+    private static void testInt512ToByte128(boolean[] mask_arr) {\n+        VectorMask<Integer> mInt512 = VectorMask.fromArray(IntVector.SPECIES_512, mask_arr, 0);\n+        Assert.assertEquals(mInt512.cast(ByteVector.SPECIES_128).toString(), mInt512.toString());\n+    }\n+\n+    \/\/ Float\n+    private static void testFloat64ToLong128(boolean[] mask_arr) {\n+        VectorMask<Float> mFloat64 = VectorMask.fromArray(FloatVector.SPECIES_64, mask_arr, 0);\n+        Assert.assertEquals(mFloat64.cast(LongVector.SPECIES_128).toString(), mFloat64.toString());\n+    }\n+\n+    private static void testFloat64ToDouble128(boolean[] mask_arr) {\n+        VectorMask<Float> mFloat64 = VectorMask.fromArray(FloatVector.SPECIES_64, mask_arr, 0);\n+        Assert.assertEquals(mFloat64.cast(DoubleVector.SPECIES_128).toString(), mFloat64.toString());\n+    }\n+\n+    private static void testFloat128ToShort64(boolean[] mask_arr) {\n+        VectorMask<Float> mFloat128 = VectorMask.fromArray(FloatVector.SPECIES_128, mask_arr, 0);\n+        Assert.assertEquals(mFloat128.cast(ShortVector.SPECIES_64).toString(), mFloat128.toString());\n+    }\n+\n+    private static void testFloat128ToLong256(boolean[] mask_arr) {\n+        VectorMask<Float> mFloat128 = VectorMask.fromArray(FloatVector.SPECIES_128, mask_arr, 0);\n+        Assert.assertEquals(mFloat128.cast(LongVector.SPECIES_256).toString(), mFloat128.toString());\n+    }\n+\n+    private static void testFloat128ToDouble256(boolean[] mask_arr) {\n+        VectorMask<Float> mFloat128 = VectorMask.fromArray(FloatVector.SPECIES_128, mask_arr, 0);\n+        Assert.assertEquals(mFloat128.cast(DoubleVector.SPECIES_256).toString(), mFloat128.toString());\n+    }\n+\n+    private static void testFloat256ToShort128(boolean[] mask_arr) {\n+        VectorMask<Float> mFloat256 = VectorMask.fromArray(FloatVector.SPECIES_256, mask_arr, 0);\n+        Assert.assertEquals(mFloat256.cast(ShortVector.SPECIES_128).toString(), mFloat256.toString());\n+    }\n+\n+    private static void testFloat256ToByte64(boolean[] mask_arr) {\n+        VectorMask<Float> mFloat256 = VectorMask.fromArray(FloatVector.SPECIES_256, mask_arr, 0);\n+        Assert.assertEquals(mFloat256.cast(ByteVector.SPECIES_64).toString(), mFloat256.toString());\n+    }\n+\n+    private static void testFloat256ToLong512(boolean[] mask_arr) {\n+        VectorMask<Float> mFloat256 = VectorMask.fromArray(FloatVector.SPECIES_256, mask_arr, 0);\n+        Assert.assertEquals(mFloat256.cast(LongVector.SPECIES_512).toString(), mFloat256.toString());\n+    }\n+\n+    private static void testFloat256ToDouble512(boolean[] mask_arr) {\n+        VectorMask<Float> mFloat256 = VectorMask.fromArray(FloatVector.SPECIES_256, mask_arr, 0);\n+        Assert.assertEquals(mFloat256.cast(DoubleVector.SPECIES_512).toString(), mFloat256.toString());\n+    }\n+\n+    private static void testFloat512ToShort256(boolean[] mask_arr) {\n+        VectorMask<Float> mFloat512 = VectorMask.fromArray(FloatVector.SPECIES_512, mask_arr, 0);\n+        Assert.assertEquals(mFloat512.cast(ShortVector.SPECIES_256).toString(), mFloat512.toString());\n+    }\n+\n+    private static void testFloat512ToByte128(boolean[] mask_arr) {\n+        VectorMask<Float> mFloat512 = VectorMask.fromArray(FloatVector.SPECIES_512, mask_arr, 0);\n+        Assert.assertEquals(mFloat512.cast(ByteVector.SPECIES_128).toString(), mFloat512.toString());\n+    }\n+\n+    \/\/ Long\n+    private static void testLong128ToInt64(boolean[] mask_arr) {\n+        VectorMask<Long> mLong128 = VectorMask.fromArray(LongVector.SPECIES_128, mask_arr, 0);\n+        Assert.assertEquals(mLong128.cast(IntVector.SPECIES_64).toString(), mLong128.toString());\n+    }\n+\n+    private static void testLong128ToFloat64(boolean[] mask_arr) {\n+        VectorMask<Long> mLong128 = VectorMask.fromArray(LongVector.SPECIES_128, mask_arr, 0);\n+        Assert.assertEquals(mLong128.cast(FloatVector.SPECIES_64).toString(), mLong128.toString());\n+    }\n+\n+    private static void testLong256ToInt128(boolean[] mask_arr) {\n+        VectorMask<Long> mLong256 = VectorMask.fromArray(LongVector.SPECIES_256, mask_arr, 0);\n+        Assert.assertEquals(mLong256.cast(IntVector.SPECIES_128).toString(), mLong256.toString());\n+    }\n+\n+    private static void testLong256ToFloat128(boolean[] mask_arr) {\n+        VectorMask<Long> mLong256 = VectorMask.fromArray(LongVector.SPECIES_256, mask_arr, 0);\n+        Assert.assertEquals(mLong256.cast(FloatVector.SPECIES_128).toString(), mLong256.toString());\n+    }\n+\n+    private static void testLong256ToShort64(boolean[] mask_arr) {\n+        VectorMask<Long> mLong256 = VectorMask.fromArray(LongVector.SPECIES_256, mask_arr, 0);\n+        Assert.assertEquals(mLong256.cast(ShortVector.SPECIES_64).toString(), mLong256.toString());\n+    }\n+\n+    private static void testLong512ToInt256(boolean[] mask_arr) {\n+        VectorMask<Long> mLong512 = VectorMask.fromArray(LongVector.SPECIES_512, mask_arr, 0);\n+        Assert.assertEquals(mLong512.cast(IntVector.SPECIES_256).toString(), mLong512.toString());\n+    }\n+\n+    private static void testLong512ToFloat256(boolean[] mask_arr) {\n+        VectorMask<Long> mLong512 = VectorMask.fromArray(LongVector.SPECIES_512, mask_arr, 0);\n+        Assert.assertEquals(mLong512.cast(FloatVector.SPECIES_256).toString(), mLong512.toString());\n+    }\n+\n+    private static void testLong512ToShort128(boolean[] mask_arr) {\n+        VectorMask<Long> mLong512 = VectorMask.fromArray(LongVector.SPECIES_512, mask_arr, 0);\n+        Assert.assertEquals(mLong512.cast(ShortVector.SPECIES_128).toString(), mLong512.toString());\n+    }\n+\n+    private static void testLong512ToByte64(boolean[] mask_arr) {\n+        VectorMask<Long> mLong512 = VectorMask.fromArray(LongVector.SPECIES_512, mask_arr, 0);\n+        Assert.assertEquals(mLong512.cast(ByteVector.SPECIES_64).toString(), mLong512.toString());\n+    }\n+\n+    \/\/ Double\n+    private static void testDouble128ToInt64(boolean[] mask_arr) {\n+        VectorMask<Double> mDouble128 = VectorMask.fromArray(DoubleVector.SPECIES_128, mask_arr, 0);\n+        Assert.assertEquals(mDouble128.cast(IntVector.SPECIES_64).toString(), mDouble128.toString());\n+    }\n+\n+    private static void testDouble128ToFloat64(boolean[] mask_arr) {\n+        VectorMask<Double> mDouble128 = VectorMask.fromArray(DoubleVector.SPECIES_128, mask_arr, 0);\n+        Assert.assertEquals(mDouble128.cast(FloatVector.SPECIES_64).toString(), mDouble128.toString());\n+    }\n+\n+    private static void testDouble256ToInt128(boolean[] mask_arr) {\n+        VectorMask<Double> mDouble256 = VectorMask.fromArray(DoubleVector.SPECIES_256, mask_arr, 0);\n+        Assert.assertEquals(mDouble256.cast(IntVector.SPECIES_128).toString(), mDouble256.toString());\n+    }\n+\n+    private static void testDouble256ToFloat128(boolean[] mask_arr) {\n+        VectorMask<Double> mDouble256 = VectorMask.fromArray(DoubleVector.SPECIES_256, mask_arr, 0);\n+        Assert.assertEquals(mDouble256.cast(FloatVector.SPECIES_128).toString(), mDouble256.toString());\n+    }\n+\n+    private static void testDouble256ToShort64(boolean[] mask_arr) {\n+        VectorMask<Double> mDouble256 = VectorMask.fromArray(DoubleVector.SPECIES_256, mask_arr, 0);\n+        Assert.assertEquals(mDouble256.cast(ShortVector.SPECIES_64).toString(), mDouble256.toString());\n+    };\n+\n+    private static void testDouble512ToInt256(boolean[] mask_arr) {\n+        VectorMask<Double> mDouble512 = VectorMask.fromArray(DoubleVector.SPECIES_512, mask_arr, 0);\n+        Assert.assertEquals(mDouble512.cast(IntVector.SPECIES_256).toString(), mDouble512.toString());\n+    }\n+\n+    private static void testDouble512ToFloat256(boolean[] mask_arr) {\n+        VectorMask<Double> mDouble512 = VectorMask.fromArray(DoubleVector.SPECIES_512, mask_arr, 0);\n+        Assert.assertEquals(mDouble512.cast(FloatVector.SPECIES_256).toString(), mDouble512.toString());\n+    }\n+\n+    private static void testDouble512ToShort128(boolean[] mask_arr) {\n+        VectorMask<Double> mDouble512 = VectorMask.fromArray(DoubleVector.SPECIES_512, mask_arr, 0);\n+        Assert.assertEquals(mDouble512.cast(ShortVector.SPECIES_128).toString(), mDouble512.toString());\n+    }\n+\n+    private static void testDouble512ToByte64(boolean[] mask_arr) {\n+        VectorMask<Double> mDouble512 = VectorMask.fromArray(DoubleVector.SPECIES_512, mask_arr, 0);\n+        Assert.assertEquals(mDouble512.cast(ByteVector.SPECIES_64).toString(), mDouble512.toString());\n+    }\n+\n+\n+    @Test\n+    public static void testMaskCast() {\n+        for (int i = 0; i < NUM_ITER; i++) {\n+            boolean[] mask = genMask();\n+            \/\/ Byte\n+            testByte64ToShort128(mask);\n+            testByte64ToInt256(mask);\n+            testByte64ToFloat256(mask);\n+            testByte64ToLong512(mask);\n+            testByte64ToDouble512(mask);\n+            testByte128ToShort256(mask);\n+            testByte128ToInt512(mask);\n+            testByte128ToFloat512(mask);\n+            testByte256ToShort512(mask);\n+\n+            \/\/ Short\n+            testShort64ToInt128(mask);\n+            testShort64ToFloat128(mask);\n+            testShort64ToLong256(mask);\n+            testShort64ToDouble256(mask);\n+            testShort128ToByte64(mask);\n+            testShort128ToInt256(mask);\n+            testShort128ToFloat256(mask);\n+            testShort128ToLong512(mask);\n+            testShort128ToDouble512(mask);\n+            testShort256ToByte128(mask);\n+            testShort256ToInt512(mask);\n+            testShort256ToFloat512(mask);\n+            testShort512ToByte256(mask);\n+\n+            \/\/ Int\n+            testInt64ToLong128(mask);\n+            testInt64ToDouble128(mask);\n+            testInt128ToShort64(mask);\n+            testInt128ToLong256(mask);\n+            testInt128ToDouble256(mask);\n+            testInt256ToShort128(mask);\n+            testInt256ToByte64(mask);\n+            testInt256ToLong512(mask);\n+            testInt256ToDouble512(mask);\n+            testInt512ToShort256(mask);\n+            testInt512ToByte128(mask);\n+\n+            \/\/ Float\n+            testFloat64ToLong128(mask);\n+            testFloat64ToDouble128(mask);\n+            testFloat128ToShort64(mask);\n+            testFloat128ToLong256(mask);\n+            testFloat128ToDouble256(mask);\n+            testFloat256ToShort128(mask);\n+            testFloat256ToByte64(mask);\n+            testFloat256ToLong512(mask);\n+            testFloat256ToDouble512(mask);\n+            testFloat512ToShort256(mask);\n+            testFloat512ToByte128(mask);\n+\n+            \/\/ Long\n+            testLong128ToInt64(mask);\n+            testLong128ToFloat64(mask);\n+            testLong256ToInt128(mask);\n+            testLong256ToFloat128(mask);\n+            testLong256ToShort64(mask);\n+            testLong512ToInt256(mask);\n+            testLong512ToFloat256(mask);\n+            testLong512ToShort128(mask);\n+            testLong512ToByte64(mask);\n+\n+            \/\/ Double\n+            testDouble128ToInt64(mask);\n+            testDouble128ToFloat64(mask);\n+            testDouble256ToInt128(mask);\n+            testDouble256ToFloat128(mask);\n+            testDouble256ToShort64(mask);\n+            testDouble512ToInt256(mask);\n+            testDouble512ToFloat256(mask);\n+            testDouble512ToShort128(mask);\n+            testDouble512ToByte64(mask);\n+        }\n+    }\n+}\n","filename":"test\/hotspot\/jtreg\/compiler\/vectorapi\/VectorMaskCastTest.java","additions":468,"deletions":0,"binary":false,"changes":468,"status":"added"},{"patch":"@@ -0,0 +1,235 @@\n+\/*\n+ * Copyright (c) 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+package compiler.vectorapi;\n+\n+import java.util.Random;\n+\n+import jdk.incubator.vector.VectorSpecies;\n+import jdk.incubator.vector.ByteVector;\n+import jdk.incubator.vector.DoubleVector;\n+import jdk.incubator.vector.FloatVector;\n+import jdk.incubator.vector.IntVector;\n+import jdk.incubator.vector.LongVector;\n+import jdk.incubator.vector.ShortVector;\n+import jdk.incubator.vector.VectorMask;\n+import jdk.test.lib.Utils;\n+\n+import org.testng.Assert;\n+import org.testng.annotations.Test;\n+\n+\/**\n+ * @test\n+ * @bug 8274569\n+ * @key randomness\n+ * @library \/test\/lib\n+ * @summary Tests X86 backend related incorrectness issues in legacy storemask patterns\n+ * @modules jdk.incubator.vector\n+ *\n+ * @run testng\/othervm -XX:-TieredCompilation -XX:CompileThreshold=100 compiler.vectorapi.VectorMaskLoadStoreTest\n+ *\/\n+\n+\n+public class VectorMaskLoadStoreTest{\n+\n+    private static final int NUM_ITER = 5000;\n+    private static final Random rd = Utils.getRandomInstance();\n+\n+    public static void testByte64(long val) {\n+        VectorSpecies<Byte> SPECIES = ByteVector.SPECIES_64;\n+        VectorMask<Byte> mask = VectorMask.fromLong(SPECIES, val);\n+        Assert.assertEquals(mask.toLong(), val & 0xFFL);\n+    }\n+\n+    public static void testByte128(long val) {\n+        VectorSpecies<Byte> SPECIES = ByteVector.SPECIES_128;\n+        VectorMask<Byte> mask = VectorMask.fromLong(SPECIES, val);\n+        Assert.assertEquals(mask.toLong(), val & 0xFFFFL);\n+    }\n+\n+    public static void testByte256(long val) {\n+        VectorSpecies<Byte> SPECIES = ByteVector.SPECIES_256;\n+        VectorMask<Byte> mask = VectorMask.fromLong(SPECIES, val);\n+        Assert.assertEquals(mask.toLong(), val & 0xFFFFFFFFL);\n+    }\n+\n+    public static void testByte512(long val) {\n+        VectorSpecies<Byte> SPECIES = ByteVector.SPECIES_512;\n+        VectorMask<Byte> mask = VectorMask.fromLong(SPECIES, val);\n+        Assert.assertEquals(mask.toLong(), val & -1L);\n+    }\n+\n+    public static void testShort64(long val) {\n+        VectorSpecies<Short> SPECIES = ShortVector.SPECIES_64;\n+        VectorMask<Short> mask = VectorMask.fromLong(SPECIES, val);\n+        Assert.assertEquals(mask.toLong(), val & 0xFL);\n+    }\n+\n+    public static void testShort128(long val) {\n+        VectorSpecies<Short> SPECIES = ShortVector.SPECIES_128;\n+        VectorMask<Short> mask = VectorMask.fromLong(SPECIES, val);\n+        Assert.assertEquals(mask.toLong(), val & 0xFFL);\n+    }\n+\n+    public static void testShort256(long val) {\n+        VectorSpecies<Short> SPECIES = ShortVector.SPECIES_256;\n+        VectorMask<Short> mask = VectorMask.fromLong(SPECIES, val);\n+        Assert.assertEquals(mask.toLong(), val & 0xFFFFL);\n+    }\n+\n+    public static void testShort512(long val) {\n+        VectorSpecies<Short> SPECIES = ShortVector.SPECIES_512;\n+        VectorMask<Short> mask = VectorMask.fromLong(SPECIES, val);\n+        Assert.assertEquals(mask.toLong(), val & 0xFFFFFFFFL);\n+    }\n+\n+    public static void testInteger64(long val) {\n+        VectorSpecies<Integer> SPECIES = IntVector.SPECIES_64;\n+        VectorMask<Integer> mask = VectorMask.fromLong(SPECIES, val);\n+        Assert.assertEquals(mask.toLong(), val & 0x3L);\n+    }\n+\n+    public static void testInteger128(long val) {\n+        VectorSpecies<Integer> SPECIES = IntVector.SPECIES_128;\n+        VectorMask<Integer> mask = VectorMask.fromLong(SPECIES, val);\n+        Assert.assertEquals(mask.toLong(), val & 0xFL);\n+    }\n+\n+    public static void testInteger256(long val) {\n+        VectorSpecies<Integer> SPECIES = IntVector.SPECIES_256;\n+        VectorMask<Integer> mask = VectorMask.fromLong(SPECIES, val);\n+        Assert.assertEquals(mask.toLong(), val & 0xFFL);\n+    }\n+\n+    public static void testInteger512(long val) {\n+        VectorSpecies<Integer> SPECIES = IntVector.SPECIES_512;\n+        VectorMask<Integer> mask = VectorMask.fromLong(SPECIES, val);\n+        Assert.assertEquals(mask.toLong(), val & 0xFFFFL);\n+    }\n+\n+    public static void testLong64(long val) {\n+        VectorSpecies<Long> SPECIES = LongVector.SPECIES_64;\n+        VectorMask<Long> mask = VectorMask.fromLong(SPECIES, val);\n+        Assert.assertEquals(mask.toLong(), val & 0x1L);\n+    }\n+\n+    public static void testLong128(long val) {\n+        VectorSpecies<Long> SPECIES = LongVector.SPECIES_128;\n+        VectorMask<Long> mask = VectorMask.fromLong(SPECIES, val);\n+        Assert.assertEquals(mask.toLong(), val & 0x3L);\n+    }\n+\n+    public static void testLong256(long val) {\n+        VectorSpecies<Long> SPECIES = LongVector.SPECIES_256;\n+        VectorMask<Long> mask = VectorMask.fromLong(SPECIES, val);\n+        Assert.assertEquals(mask.toLong(), val & 0xFL);\n+    }\n+\n+    public static void testLong512(long val) {\n+        VectorSpecies<Long> SPECIES = LongVector.SPECIES_512;\n+        VectorMask<Long> mask = VectorMask.fromLong(SPECIES, val);\n+        Assert.assertEquals(mask.toLong(), val & 0xFFL);\n+    }\n+\n+    public static void testFloat64(long val) {\n+        VectorSpecies<Float> SPECIES = FloatVector.SPECIES_64;\n+        VectorMask<Float> mask = VectorMask.fromLong(SPECIES, val);\n+        Assert.assertEquals(mask.toLong(), val & 0x3L);\n+    }\n+\n+    public static void testFloat128(long val) {\n+        VectorSpecies<Float> SPECIES = FloatVector.SPECIES_128;\n+        VectorMask<Float> mask = VectorMask.fromLong(SPECIES, val);\n+        Assert.assertEquals(mask.toLong(), val & 0xFL);\n+    }\n+\n+    public static void testFloat256(long val) {\n+        VectorSpecies<Float> SPECIES = FloatVector.SPECIES_256;\n+        VectorMask<Float> mask = VectorMask.fromLong(SPECIES, val);\n+        Assert.assertEquals(mask.toLong(), val & 0xFFL);\n+    }\n+\n+    public static void testFloat512(long val) {\n+        VectorSpecies<Float> SPECIES = FloatVector.SPECIES_512;\n+        VectorMask<Float> mask = VectorMask.fromLong(SPECIES, val);\n+        Assert.assertEquals(mask.toLong(), val & 0xFFFFL);\n+    }\n+\n+    public static void testDouble64(long val) {\n+        VectorSpecies<Double> SPECIES = DoubleVector.SPECIES_64;\n+        VectorMask<Double> mask = VectorMask.fromLong(SPECIES, val);\n+        Assert.assertEquals(mask.toLong(), val & 0x1L);\n+    }\n+\n+    public static void testDouble128(long val) {\n+        VectorSpecies<Double> SPECIES = DoubleVector.SPECIES_128;\n+        VectorMask<Double> mask = VectorMask.fromLong(SPECIES, val);\n+        Assert.assertEquals(mask.toLong(), val & 0x3L);\n+    }\n+\n+    public static void testDouble256(long val) {\n+        VectorSpecies<Double> SPECIES = DoubleVector.SPECIES_256;\n+        VectorMask<Double> mask = VectorMask.fromLong(SPECIES, val);\n+        Assert.assertEquals(mask.toLong(), val & 0xFL);\n+    }\n+\n+    public static void testDouble512(long val) {\n+        VectorSpecies<Double> SPECIES = DoubleVector.SPECIES_512;\n+        VectorMask<Double> mask = VectorMask.fromLong(SPECIES, val);\n+        Assert.assertEquals(mask.toLong(), val & 0xFFL);\n+    }\n+\n+    @Test\n+    public static void testMaskCast() {\n+        long [] vals = {-1L, 0, rd.nextLong(), rd.nextLong()};\n+        for(int i = 0; i < vals.length; i++) {\n+            long val = vals[i];\n+            for (int ctr = 0; ctr < NUM_ITER; ctr++) {\n+                testByte64(val);\n+                testByte128(val);\n+                testByte256(val);\n+                testByte512(val);\n+                testShort64(val);\n+                testShort128(val);\n+                testShort256(val);\n+                testShort512(val);\n+                testInteger64(val);\n+                testInteger128(val);\n+                testInteger256(val);\n+                testInteger512(val);\n+                testLong64(val);\n+                testLong128(val);\n+                testLong256(val);\n+                testLong512(val);\n+                testFloat64(val);\n+                testFloat128(val);\n+                testFloat256(val);\n+                testFloat512(val);\n+                testDouble64(val);\n+                testDouble128(val);\n+                testDouble256(val);\n+                testDouble512(val);\n+            }\n+        }\n+    }\n+}\n","filename":"test\/hotspot\/jtreg\/compiler\/vectorapi\/VectorMaskLoadStoreTest.java","additions":235,"deletions":0,"binary":false,"changes":235,"status":"added"},{"patch":"@@ -0,0 +1,67 @@\n+\/*\n+ *  Copyright (c) 2021, Oracle and\/or its affiliates. All rights reserved.\n+ *  Copyright (c) 2021, Rado Smogura. All rights reserved.\n+ *\n+ *  DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ *  This code is free software; you can redistribute it and\/or modify it\n+ *  under the terms of the GNU General Public License version 2 only, as\n+ *  published by the Free Software Foundation.\n+ *\n+ *  This code is distributed in the hope that it will be useful, but WITHOUT\n+ *  ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ *  FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ *  version 2 for more details (a copy is included in the LICENSE file that\n+ *  accompanied this code).\n+ *\n+ *  You should have received a copy of the GNU General Public License version\n+ *  2 along with this work; if not, write to the Free Software Foundation,\n+ *  Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ *  Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ *  or visit www.oracle.com if you need additional information or have any\n+ *  questions.\n+ *\n+ *\/\n+\n+\/*\n+ * @test\n+ * @summary Test if memory ordering is preserved\n+ *\n+ * @run main\/othervm -XX:-TieredCompilation -XX:+UnlockDiagnosticVMOptions -XX:+AbortVMOnCompilationFailure\n+ *      -XX:CompileThreshold=100 -XX:CompileCommand=dontinline,compiler.vectorapi.VectorMemoryAlias::test\n+ *      compiler.vectorapi.VectorMemoryAlias\n+ * @modules jdk.incubator.vector\n+ *\/\n+\n+package compiler.vectorapi;\n+\n+import java.nio.ByteBuffer;\n+import java.nio.ByteOrder;\n+import jdk.incubator.vector.ByteVector;\n+import jdk.incubator.vector.VectorSpecies;\n+\n+public class VectorMemoryAlias {\n+  public static final VectorSpecies<Byte> SPECIES = VectorSpecies.ofLargestShape(byte.class);\n+  public static void main(String[] args) {\n+    for (int i=0; i < 30000; i++) {\n+      if (test() != 1) {\n+        throw new AssertionError();\n+      }\n+    }\n+  }\n+\n+  public static int test() {\n+    byte arr[] = new byte[256];\n+    final var bb = ByteBuffer.wrap(arr);\n+    final var ones = ByteVector.broadcast(SPECIES, 1);\n+    var res = ByteVector.zero(SPECIES);\n+\n+    int result = 0;\n+    result += arr[2];\n+    res.add(ones).intoByteBuffer(bb, 0, ByteOrder.nativeOrder());\n+    result += arr[2];\n+\n+    return result;\n+  }\n+}\n","filename":"test\/hotspot\/jtreg\/compiler\/vectorapi\/VectorMemoryAlias.java","additions":67,"deletions":0,"binary":false,"changes":67,"status":"added"},{"patch":"@@ -1,36 +0,0 @@\n-\/*\n- * Copyright (c) 2019, Red Hat, Inc. and\/or its affiliates. All rights reserved.\n- * Copyright (c) 2019, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-package gc;\n-\n-public class CriticalNative {\n-    static {\n-        System.loadLibrary(\"CriticalNative\");\n-    }\n-\n-    public static native boolean isNull(int[] a);\n-    public static native long sum1(long[] a);\n-    \/\/ More than 6 parameters\n-    public static native long sum2(long a1, int[] a2, int[] a3, long[] a4, int[] a5);\n-}\n","filename":"test\/hotspot\/jtreg\/gc\/CriticalNative.java","additions":0,"deletions":36,"binary":false,"changes":36,"status":"deleted"},{"patch":"@@ -1,109 +0,0 @@\n-\/*\n- * Copyright (c) 2018, 2019, Red Hat, Inc. and\/or its affiliates.\n- * Copyright (c) 2020, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\n- *\/\n-\n-package gc;\n-\n-\/*\n- * @test CriticalNativeStressEpsilon\n- * @bug 8199868\n- * @library \/\n- * @requires os.arch ==\"x86_64\" | os.arch == \"amd64\" | os.arch==\"x86\" | os.arch==\"i386\"\n- * @requires vm.gc.Epsilon\n- * @summary test argument unpacking nmethod wrapper of critical native method\n- * @run main\/othervm\/native -XX:+UnlockExperimentalVMOptions -XX:+UseEpsilonGC -Xcomp -Xmx256M\n- *                          -XX:-CriticalJNINatives\n- *                          gc.CriticalNativeArgs\n- * @run main\/othervm\/native -XX:+UnlockExperimentalVMOptions -XX:+UseEpsilonGC -Xcomp -Xmx256M\n- *                          -XX:+CriticalJNINatives\n- *                          gc.CriticalNativeArgs\n- *\/\n-\n-\/*\n- * @test CriticalNativeStressShenandoah\n- * @bug 8199868\n- * @library \/\n- * @requires os.arch ==\"x86_64\" | os.arch == \"amd64\" | os.arch==\"x86\" | os.arch==\"i386\"\n- * @requires vm.gc.Shenandoah\n- * @summary test argument unpacking nmethod wrapper of critical native method\n- *\n- * @run main\/othervm\/native -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions -Xcomp -Xmx512M\n- *                          -XX:+UseShenandoahGC\n- *                          -XX:-CriticalJNINatives\n- *                          gc.CriticalNativeArgs\n- * @run main\/othervm\/native -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions -Xcomp -Xmx512M\n- *                          -XX:+UseShenandoahGC\n- *                          -XX:+CriticalJNINatives\n- *                          gc.CriticalNativeArgs\n- *\n- * @run main\/othervm\/native -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions -Xcomp -Xmx512M\n- *                          -XX:+UseShenandoahGC -XX:ShenandoahGCMode=passive -XX:+ShenandoahDegeneratedGC\n- *                          -XX:+CriticalJNINatives\n- *                          gc.CriticalNativeArgs\n- * @run main\/othervm\/native -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions -Xcomp -Xmx512M\n- *                          -XX:+UseShenandoahGC -XX:ShenandoahGCMode=passive -XX:-ShenandoahDegeneratedGC\n- *                          -XX:+CriticalJNINatives\n- *                          gc.CriticalNativeArgs\n- *\n- * @run main\/othervm\/native -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions -Xcomp -Xmx512M\n- *                          -XX:+UseShenandoahGC -XX:ShenandoahGCHeuristics=aggressive\n- *                          -XX:+CriticalJNINatives\n- *                          gc.CriticalNativeArgs\n- *\n- * @run main\/othervm\/native -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions -Xcomp -Xmx512M\n- *                          -XX:+UseShenandoahGC -XX:ShenandoahGCMode=iu\n- *                          -XX:+CriticalJNINatives\n- *                          gc.CriticalNativeArgs\n- * @run main\/othervm\/native -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions -Xcomp -Xmx512M\n- *                          -XX:+UseShenandoahGC -XX:ShenandoahGCMode=iu -XX:ShenandoahGCHeuristics=aggressive\n- *                          -XX:+CriticalJNINatives\n- *                          gc.CriticalNativeArgs\n- *\/\n-\n-\/*\n- * @test CriticalNativeStress\n- * @bug 8199868 8233343\n- * @library \/\n- * @requires os.arch ==\"x86_64\" | os.arch == \"amd64\" | os.arch==\"x86\" | os.arch==\"i386\" | os.arch==\"ppc64\" | os.arch==\"ppc64le\" | os.arch==\"s390x\"\n- * @summary test argument unpacking nmethod wrapper of critical native method\n- * @run main\/othervm\/native -Xcomp -Xmx512M\n- *                          -XX:-CriticalJNINatives\n- *                          gc.CriticalNativeArgs\n- * @run main\/othervm\/native -Xcomp -Xmx512M\n- *                          -XX:+CriticalJNINatives\n- *                          gc.CriticalNativeArgs\n- *\/\n-public class CriticalNativeArgs {\n-    public static void main(String[] args) {\n-        int[] arr = new int[2];\n-\n-        if (CriticalNative.isNull(arr)) {\n-            throw new RuntimeException(\"Should not be null\");\n-        }\n-\n-        if (!CriticalNative.isNull(null)) {\n-            throw new RuntimeException(\"Should be null\");\n-        }\n-    }\n-}\n","filename":"test\/hotspot\/jtreg\/gc\/CriticalNativeArgs.java","additions":0,"deletions":109,"binary":false,"changes":109,"status":"deleted"},{"patch":"@@ -27,1 +27,1 @@\n- * @test TestSystemGCSerial\n+ * @test id=Serial\n@@ -31,0 +31,1 @@\n+ * @run main\/othervm -XX:+UseSerialGC -XX:+UseLargePages gc.TestSystemGC\n@@ -34,1 +35,1 @@\n- * @test TestSystemGCParallel\n+ * @test id=Parallel\n@@ -38,0 +39,1 @@\n+ * @run main\/othervm -XX:+UseParallelGC -XX:+UseLargePages gc.TestSystemGC\n@@ -41,1 +43,1 @@\n- * @test TestSystemGCG1\n+ * @test id=G1\n@@ -46,0 +48,1 @@\n+ * @run main\/othervm -XX:+UseG1GC -XX:+UseLargePages gc.TestSystemGC\n@@ -49,1 +52,1 @@\n- * @test TestSystemGCShenandoah\n+ * @test id=Shenandoah\n@@ -52,2 +55,3 @@\n- * @run main\/othervm -XX:+UnlockExperimentalVMOptions -XX:+UseShenandoahGC gc.TestSystemGC\n- * @run main\/othervm -XX:+UnlockExperimentalVMOptions -XX:+UseShenandoahGC -XX:+ExplicitGCInvokesConcurrent gc.TestSystemGC\n+ * @run main\/othervm -XX:+UseShenandoahGC gc.TestSystemGC\n+ * @run main\/othervm -XX:+UseShenandoahGC -XX:+ExplicitGCInvokesConcurrent gc.TestSystemGC\n+ * @run main\/othervm -XX:+UseShenandoahGC -XX:+UseLargePages gc.TestSystemGC\n@@ -57,1 +61,4 @@\n- * @test TestSystemGCLargePages\n+ * @test id=Z\n+ * @requires vm.gc.Z\n+ * @comment ZGC will not start when LargePages cannot be allocated, therefore\n+ *          we do not run such configuration.\n@@ -59,1 +66,1 @@\n- * @run main\/othervm -XX:+UseLargePages gc.TestSystemGC\n+ * @run main\/othervm -XX:+UseZGC gc.TestSystemGC\n","filename":"test\/hotspot\/jtreg\/gc\/TestSystemGC.java","additions":15,"deletions":8,"binary":false,"changes":23,"status":"modified"},{"patch":"@@ -45,0 +45,1 @@\n+import jdk.test.lib.Platform;\n@@ -85,1 +86,8 @@\n-    checkG1HeapRegionSize(new String[] { \"-Xmx256m\", \"-XX:G1HeapRegionSize=64m\" }, 32*M, 1);\n+    if (Platform.is64bit()) {\n+      checkG1HeapRegionSize(new String[] { \"-Xmx4096m\", \"-XX:G1HeapRegionSize=64m\" }, 64*M, 0);\n+      checkG1HeapRegionSize(new String[] { \"-Xmx4096m\", \"-XX:G1HeapRegionSize=512m\" }, 512*M, 0);\n+      checkG1HeapRegionSize(new String[] { \"-Xmx4096m\", \"-XX:G1HeapRegionSize=1024m\" }, 512*M, 1);\n+      checkG1HeapRegionSize(new String[] { \"-Xmx128g\" }, 32*M, 0);\n+    } else {\n+      checkG1HeapRegionSize(new String[] { \"-Xmx256m\", \"-XX:G1HeapRegionSize=64m\" }, 64*M, 1);\n+    }\n","filename":"test\/hotspot\/jtreg\/gc\/arguments\/TestG1HeapRegionSize.java","additions":9,"deletions":1,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -1,130 +0,0 @@\n-\/*\n- * Copyright (c) 2018, Red Hat, Inc. and\/or its affiliates.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\n- *\/\n-\n-#include \"jni.h\"\n-\n-JNIEXPORT jlong JNICALL JavaCritical_gc_CriticalNative_sum1\n-  (jint length, jlong* a) {\n-  jlong sum = 0;\n-  jint index;\n-  for (index = 0; index < length; index ++) {\n-    sum += a[index];\n-  }\n-\n-  return sum;\n-}\n-\n-JNIEXPORT jlong JNICALL  JavaCritical_gc_CriticalNative_sum2\n-  (jlong a1, jint a2_length, jint* a2, jint a4_length, jint* a4, jint a6_length, jlong* a6, jint a8_length, jint* a8) {\n-  jlong sum = a1;\n-  jint index;\n-  for (index = 0; index < a2_length; index ++) {\n-    sum += a2[index];\n-  }\n-\n-  for (index = 0; index < a4_length; index ++) {\n-    sum += a4[index];\n-  }\n-\n-  for (index = 0; index < a6_length; index ++) {\n-    sum += a6[index];\n-  }\n-\n-  for (index = 0; index < a8_length; index ++) {\n-    sum += a8[index];\n-  }\n-  return sum;\n-}\n-\n-JNIEXPORT jlong JNICALL Java_gc_CriticalNative_sum1\n-  (JNIEnv *env, jclass jclazz, jlongArray a) {\n-  jlong sum = 0;\n-  jsize len = (*env)->GetArrayLength(env, a);\n-  jsize index;\n-  jlong* arr = (jlong*)(*env)->GetPrimitiveArrayCritical(env, a, 0);\n-  for (index = 0; index < len; index ++) {\n-    sum += arr[index];\n-  }\n-\n-  (*env)->ReleasePrimitiveArrayCritical(env, a, arr, 0);\n-  return sum;\n-}\n-\n-JNIEXPORT jlong JNICALL Java_gc_CriticalNative_sum2\n-  (JNIEnv *env, jclass jclazz, jlong a1, jintArray a2, jintArray a3, jlongArray a4, jintArray a5) {\n-  jlong sum = a1;\n-  jsize index;\n-  jsize len;\n-  jint* a2_arr;\n-  jint* a3_arr;\n-  jlong* a4_arr;\n-  jint* a5_arr;\n-\n-  len = (*env)->GetArrayLength(env, a2);\n-  a2_arr = (jint*)(*env)->GetPrimitiveArrayCritical(env, a2, 0);\n-  for (index = 0; index < len; index ++) {\n-    sum += a2_arr[index];\n-  }\n-  (*env)->ReleasePrimitiveArrayCritical(env, a2, a2_arr, 0);\n-\n-  len = (*env)->GetArrayLength(env, a3);\n-  a3_arr = (jint*)(*env)->GetPrimitiveArrayCritical(env, a3, 0);\n-  for (index = 0; index < len; index ++) {\n-    sum += a3_arr[index];\n-  }\n-  (*env)->ReleasePrimitiveArrayCritical(env, a3, a3_arr, 0);\n-\n-  len = (*env)->GetArrayLength(env, a4);\n-  a4_arr = (jlong*)(*env)->GetPrimitiveArrayCritical(env, a4, 0);\n-  for (index = 0; index < len; index ++) {\n-    sum += a4_arr[index];\n-  }\n-  (*env)->ReleasePrimitiveArrayCritical(env, a4, a4_arr, 0);\n-\n-  len = (*env)->GetArrayLength(env, a5);\n-  a5_arr = (jint*)(*env)->GetPrimitiveArrayCritical(env, a5, 0);\n-  for (index = 0; index < len; index ++) {\n-    sum += a5_arr[index];\n-  }\n-  (*env)->ReleasePrimitiveArrayCritical(env, a5, a5_arr, 0);\n-\n-  return sum;\n-}\n-\n-\n-JNIEXPORT jboolean JNICALL JavaCritical_gc_CriticalNative_isNull\n-  (jint length, jint* a) {\n-  return (a == NULL) && (length == 0);\n-}\n-\n-JNIEXPORT jboolean JNICALL Java_gc_CriticalNative_isNull\n-  (JNIEnv *env, jclass jclazz, jintArray a) {\n-  if (a == NULL) return JNI_TRUE;\n-  jsize len = (*env)->GetArrayLength(env, a);\n-  jint* arr = (jint*)(*env)->GetPrimitiveArrayCritical(env, a, 0);\n-  jboolean is_null = (arr == NULL) && (len == 0);\n-  (*env)->ReleasePrimitiveArrayCritical(env, a, arr, 0);\n-  return is_null;\n-}\n-\n","filename":"test\/hotspot\/jtreg\/gc\/libCriticalNative.c","additions":0,"deletions":130,"binary":false,"changes":130,"status":"deleted"},{"patch":"@@ -1,226 +0,0 @@\n-\/*\n- * Copyright (c) 2018, 2019, Red Hat, Inc. and\/or its affiliates.\n- * Copyright (c) 2020, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\n- *\/\n-package gc.stress;\n-\n-import java.util.Random;\n-\n-import gc.CriticalNative;\n-import jdk.test.lib.Utils;\n-\n-\/*\n- * @test CriticalNativeStressEpsilon\n- * @key randomness\n- * @bug 8199868\n- * @library \/ \/test\/lib\n- * @requires os.arch ==\"x86_64\" | os.arch == \"amd64\" | os.arch==\"x86\" | os.arch==\"i386\"\n- * @requires vm.gc.Epsilon\n- * @summary test argument pinning by nmethod wrapper of critical native method\n- * @run main\/othervm\/native -XX:+UnlockExperimentalVMOptions -XX:+UseEpsilonGC -Xcomp -Xmx1G -XX:+CriticalJNINatives gc.stress.CriticalNativeStress\n- *\/\n-\n-\/*\n- * @test CriticalNativeStressShenandoah\n- * @key randomness\n- * @bug 8199868\n- * @library \/ \/test\/lib\n- * @requires os.arch ==\"x86_64\" | os.arch == \"amd64\" | os.arch==\"x86\" | os.arch==\"i386\"\n- * @requires vm.gc.Shenandoah\n- * @summary test argument pinning by nmethod wrapper of critical native method\n- * @run main\/othervm\/native -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions -XX:+UseShenandoahGC -XX:ShenandoahGCMode=passive    -XX:-ShenandoahDegeneratedGC -Xcomp -Xmx512M -XX:+CriticalJNINatives gc.stress.CriticalNativeStress\n- * @run main\/othervm\/native -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions -XX:+UseShenandoahGC -XX:ShenandoahGCMode=passive    -XX:+ShenandoahDegeneratedGC -Xcomp -Xmx512M -XX:+CriticalJNINatives gc.stress.CriticalNativeStress\n- *\n- * @run main\/othervm\/native -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions -XX:+UseShenandoahGC -XX:ShenandoahGCHeuristics=aggressive -Xcomp -Xmx512M -XX:+CriticalJNINatives gc.stress.CriticalNativeStress\n- * @run main\/othervm\/native -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions -XX:+UseShenandoahGC                                       -Xcomp -Xmx256M -XX:+CriticalJNINatives gc.stress.CriticalNativeStress\n- * @run main\/othervm\/native -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions -XX:+UseShenandoahGC -XX:ShenandoahGCMode=iu        -Xcomp -Xmx512M -XX:+CriticalJNINatives gc.stress.CriticalNativeStress\n- * @run main\/othervm\/native -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions -XX:+UseShenandoahGC -XX:ShenandoahGCMode=iu -XX:ShenandoahGCHeuristics=aggressive -Xcomp -Xmx512M -XX:+CriticalJNINatives gc.stress.CriticalNativeStress\n- *\/\n-\n-\/*\n- * @test CriticalNativeStress\n- * @key randomness\n- * @bug 8199868 8233343\n- * @library \/ \/test\/lib\n- * @requires os.arch ==\"x86_64\" | os.arch == \"amd64\" | os.arch==\"x86\" | os.arch==\"i386\" | os.arch==\"ppc64\" | os.arch==\"ppc64le\" | os.arch==\"s390x\"\n- * @summary test argument unpacking nmethod wrapper of critical native method\n- * @run main\/othervm\/native -Xcomp -Xmx512M -XX:+CriticalJNINatives gc.stress.CriticalNativeStress\n- *\/\n-\n-public class CriticalNativeStress {\n-    \/\/ CYCLES and THREAD_PER_CASE are used to tune the tests for different GC settings,\n-    \/\/ so that they can execrise enough GC cycles and not OOM\n-    private static int CYCLES = Integer.getInteger(\"cycles\", 3);\n-    private static int THREAD_PER_CASE = Integer.getInteger(\"threadPerCase\", 1);\n-\n-    static long sum(long[] a) {\n-        long sum = 0;\n-        for (int index = 0; index < a.length; index ++) {\n-            sum += a[index];\n-        }\n-        return sum;\n-    }\n-\n-    static long sum(int[] a) {\n-        long sum = 0;\n-        for (int index = 0; index < a.length; index ++) {\n-            sum += a[index];\n-        }\n-        return sum;\n-    }\n-\n-    private static volatile String garbage_array[];\n-\n-    \/\/ GC potentially moves arrays passed to critical native methods\n-    \/\/ if they are not pinned correctly.\n-    \/\/ Create enough garbages to exercise GC cycles, verify\n-    \/\/ the arrays are pinned correctly.\n-    static void create_garbage(int len) {\n-        len = Math.max(len, 1024);\n-        String array[] = new String[len];\n-        for (int index = 0; index < len; index ++) {\n-            array[index] = \"String \" + index;\n-        }\n-        garbage_array = array;\n-    }\n-\n-    \/\/ Two test cases with different method signatures:\n-    \/\/ Tests generate arbitrary length of arrays with\n-    \/\/ arbitrary values, then calcuate sum of the array\n-    \/\/ elements with critical native JNI methods and java\n-    \/\/ methods, and compare the results for correctness.\n-    static void run_test_case1(Random rand) {\n-        \/\/ Create testing arary with arbitrary length and\n-        \/\/ values\n-        int length = rand.nextInt(50) + 1;\n-        long[] arr = new long[length];\n-        for (int index = 0; index < length; index ++) {\n-            arr[index] = rand.nextLong() % 1002;\n-        }\n-\n-        \/\/ Generate garbages to trigger GCs\n-        for (int index = 0; index < length; index ++) {\n-            create_garbage(index);\n-        }\n-\n-        \/\/ Compare results for correctness.\n-        long native_sum = CriticalNative.sum1(arr);\n-        long java_sum = sum(arr);\n-        if (native_sum != java_sum) {\n-            StringBuffer sb = new StringBuffer(\"Sums do not match: native = \")\n-                .append(native_sum).append(\" java = \").append(java_sum);\n-\n-            throw new RuntimeException(sb.toString());\n-        }\n-    }\n-\n-    static void run_test_case2(Random rand) {\n-        \/\/ Create testing arary with arbitrary length and\n-        \/\/ values\n-        int index;\n-        long a1 = rand.nextLong() % 1025;\n-\n-        int a2_length = rand.nextInt(50) + 1;\n-        int[] a2 = new int[a2_length];\n-        for (index = 0; index < a2_length; index ++) {\n-            a2[index] = rand.nextInt(106);\n-        }\n-\n-        int a3_length = rand.nextInt(150) + 1;\n-        int[] a3 = new int[a3_length];\n-        for (index = 0; index < a3_length; index ++) {\n-            a3[index] = rand.nextInt(3333);\n-        }\n-\n-        int a4_length = rand.nextInt(200) + 1;\n-        long[] a4 = new long[a4_length];\n-        for (index = 0; index < a4_length; index ++) {\n-            a4[index] = rand.nextLong() % 122;\n-        }\n-\n-        int a5_length = rand.nextInt(350) + 1;\n-        int[] a5 = new int[a5_length];\n-        for (index = 0; index < a5_length; index ++) {\n-            a5[index] = rand.nextInt(333);\n-        }\n-\n-        \/\/ Generate garbages to trigger GCs\n-        for (index = 0; index < a1; index ++) {\n-            create_garbage(index);\n-        }\n-\n-        \/\/ Compare results for correctness.\n-        long native_sum = CriticalNative.sum2(a1, a2, a3, a4, a5);\n-        long java_sum = a1 + sum(a2) + sum(a3) + sum(a4) + sum(a5);\n-        if (native_sum != java_sum) {\n-            StringBuffer sb = new StringBuffer(\"Sums do not match: native = \")\n-                .append(native_sum).append(\" java = \").append(java_sum);\n-\n-            throw new RuntimeException(sb.toString());\n-        }\n-    }\n-\n-    static class Case1Runner extends Thread {\n-        private final Random rand;\n-        public Case1Runner() {\n-            rand = new Random(Utils.getRandomInstance().nextLong());\n-            start();\n-        }\n-\n-        public void run() {\n-            for (int index = 0; index < CYCLES; index ++) {\n-                run_test_case1(rand);\n-            }\n-        }\n-    }\n-\n-    static class Case2Runner extends Thread {\n-        private final Random rand;\n-        public Case2Runner() {\n-            rand = new Random(Utils.getRandomInstance().nextLong());\n-            start();\n-        }\n-\n-        public void run() {\n-            for (int index = 0; index < CYCLES; index ++) {\n-                run_test_case2(rand);\n-            }\n-        }\n-    }\n-\n-    public static void main(String[] args) {\n-        Thread[] thrs = new Thread[THREAD_PER_CASE * 2];\n-        for (int index = 0; index < thrs.length; index = index + 2) {\n-            thrs[index] = new Case1Runner();\n-            thrs[index + 1] = new Case2Runner();\n-        }\n-\n-        for (int index = 0; index < thrs.length; index ++) {\n-            try {\n-                thrs[index].join();\n-            } catch (Exception e) {\n-                e.printStackTrace();\n-            }\n-        }\n-    }\n-}\n","filename":"test\/hotspot\/jtreg\/gc\/stress\/CriticalNativeStress.java","additions":0,"deletions":226,"binary":false,"changes":226,"status":"deleted"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2015, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2015, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -50,0 +50,4 @@\n+        {\"UseSharedSpaces\",           \"false\"},\n+        {\"RequireSharedSpaces\",       \"false\"},\n+        {\"DumpSharedSpaces\",          \"false\"},\n+        {\"DynamicDumpSharedSpaces\",   \"false\"},\n","filename":"test\/hotspot\/jtreg\/runtime\/CommandLine\/VMDeprecatedOptions.java","additions":5,"deletions":1,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -27,1 +27,1 @@\n- * @bug 8274944\n+ * @bug 8274944 8276184\n@@ -52,2 +52,1 @@\n-        \/\/ dump class list\n-        CDSTestUtils.dumpClassList(classList, \"-cp\", appJar, mainClass);\n+        String[] mainArgs = { \"dummy\", \"addLambda\" };\n@@ -55,10 +54,3 @@\n-        \/\/ create archive with the class list\n-        CDSOptions opts = (new CDSOptions())\n-            .addPrefix(\"-XX:ExtraSharedClassListFile=\" + classList,\n-                       \"-cp\", appJar,\n-                       \"-Xlog:class+load,cds\")\n-            .setArchiveName(archiveName);\n-        OutputAnalyzer output = CDSTestUtils.createArchiveAndCheck(opts);\n-        TestCommon.checkExecReturn(output, 0, true,\n-                                   \"Skipping OldProvider: Old class has been linked\");\n-        output.shouldMatch(\"Skipping.LambdaContainsOldInfApp[$][$]Lambda[$].*0x.*:.*Old.class.has.been.linked\");\n+        for (String mainArg : mainArgs) {\n+            \/\/ dump class list\n+            CDSTestUtils.dumpClassList(classList, \"-cp\", appJar, mainClass, mainArg);\n@@ -66,11 +58,24 @@\n-        \/\/ run with archive\n-        CDSOptions runOpts = (new CDSOptions())\n-            .addPrefix(\"-cp\", appJar, \"-Xlog:class+load,cds=debug\")\n-            .setArchiveName(archiveName)\n-            .setUseVersion(false)\n-            .addSuffix(mainClass);\n-        output = CDSTestUtils.runWithArchive(runOpts);\n-        TestCommon.checkExecReturn(output, 0, true,\n-            \"[class,load] LambdaContainsOldInfApp source: shared objects file\");\n-        output.shouldMatch(\".class.load. OldProvider.source:.*lambdacontainsoldinf.jar\")\n-              .shouldMatch(\".class.load. LambdaContainsOldInfApp[$][$]Lambda[$].*\/0x.*source:.*LambdaContainsOldInf\");\n+            \/\/ create archive with the class list\n+            CDSOptions opts = (new CDSOptions())\n+                .addPrefix(\"-XX:ExtraSharedClassListFile=\" + classList,\n+                           \"-cp\", appJar,\n+                           \"-Xlog:class+load,cds\")\n+                .setArchiveName(archiveName);\n+            OutputAnalyzer output = CDSTestUtils.createArchiveAndCheck(opts);\n+            TestCommon.checkExecReturn(output, 0, true,\n+                                       \"Skipping OldProvider: Old class has been linked\");\n+            output.shouldMatch(\"Skipping.LambdaContainsOldInfApp[$][$]Lambda[$].*0x.*:.*Old.class.has.been.linked\");\n+\n+            \/\/ run with archive\n+            CDSOptions runOpts = (new CDSOptions())\n+                .addPrefix(\"-cp\", appJar, \"-Xlog:class+load,cds=debug\")\n+                .setArchiveName(archiveName)\n+                .setUseVersion(false)\n+                .addSuffix(mainClass)\n+                .addSuffix(mainArg);\n+            output = CDSTestUtils.runWithArchive(runOpts);\n+            TestCommon.checkExecReturn(output, 0, true,\n+                \"[class,load] LambdaContainsOldInfApp source: shared objects file\");\n+            output.shouldMatch(\".class.load. OldProvider.source:.*lambdacontainsoldinf.jar\")\n+                  .shouldMatch(\".class.load. LambdaContainsOldInfApp[$][$]Lambda[$].*\/0x.*source:.*LambdaContainsOldInf\");\n+       }\n","filename":"test\/hotspot\/jtreg\/runtime\/cds\/appcds\/LambdaContainsOldInf.java","additions":29,"deletions":24,"binary":false,"changes":53,"status":"modified"},{"patch":"@@ -27,0 +27,1 @@\n+ * @bug 8276184\n@@ -45,2 +46,0 @@\n-        output = TestCommon.dump(signedJar, TestCommon.list(\"Hello\"));\n-        TestCommon.checkDump(output, \"Skipping Hello: Signed JAR\");\n@@ -48,5 +47,3 @@\n-        \/\/ At runtime, the Hello class should be loaded from the jar file\n-        \/\/ instead of from the shared archive since a class from a signed\n-        \/\/ jar shouldn't be dumped into the archive.\n-        output = TestCommon.exec(signedJar, \"-verbose:class\", \"Hello\");\n-        String expectedOutput = \".class,load. Hello source: file:.*signed_hello.jar\";\n+        \/\/ \"testlambda\" is for testing JDK-8276184\n+        String[] mainArgs = { \"dummy\", \"testlambda\" };\n+        String mainClass = \"Hello\";\n@@ -54,5 +51,15 @@\n-        try {\n-            output.shouldMatch(expectedOutput);\n-        } catch (Exception e) {\n-            TestCommon.checkCommonExecExceptions(output, e);\n-        }\n+        String skipMsg = \"Skipping Hello: Signed JAR\";\n+        String lambdaInArchive = \"klasses.*=.*app.*Hello[$][$]Lambda[$].*hidden\";\n+        String loadFromJar = \".class,load. Hello source: file:.*signed_hello.jar\";\n+        String lambdaLoadFromHello = \".class.load. Hello[$][$]Lambda[$].*\/0x.*source.*Hello\";\n+\n+        for (String mainArg : mainArgs) {\n+            output = TestCommon.dump(signedJar, TestCommon.list(mainClass),\n+                                     \"-Xlog:cds+class=debug\", mainClass, mainArg);\n+            TestCommon.checkDump(output, skipMsg);\n+            output.shouldNotContain(lambdaInArchive);\n+\n+            \/\/ At runtime, the Hello class should be loaded from the jar file\n+            \/\/ instead of from the shared archive since a class from a signed\n+            \/\/ jar shouldn't be dumped into the archive.\n+            output = TestCommon.exec(signedJar, \"-verbose:class\", mainClass, mainArg);\n@@ -60,4 +67,16 @@\n-        \/\/ Test class exists in both signed JAR and unsigned JAR\n-        String jars = signedJar + System.getProperty(\"path.separator\") + unsignedJar;\n-        output = TestCommon.dump(jars, TestCommon.list(\"Hello\"));\n-        TestCommon.checkDump(output, \"Skipping Hello: Signed JAR\");\n+            try {\n+                output.shouldMatch(loadFromJar);\n+                if (mainArg.equals(\"testlambda\")) {\n+                    output.shouldMatch(lambdaLoadFromHello);\n+                }\n+            } catch (Exception e) {\n+                TestCommon.checkCommonExecExceptions(output, e);\n+            }\n+\n+            \/\/ Test class exists in both signed JAR and unsigned JAR\n+            String jars = signedJar + System.getProperty(\"path.separator\") + unsignedJar;\n+            output = TestCommon.dump(jars, TestCommon.list(mainClass),\n+                                     \"-Xlog:cds+class=debug\", mainClass, mainArg);\n+            TestCommon.checkDump(output, skipMsg);\n+            output.shouldNotContain(lambdaInArchive);\n+        }\n","filename":"test\/hotspot\/jtreg\/runtime\/cds\/appcds\/SignedJar.java","additions":35,"deletions":16,"binary":false,"changes":51,"status":"modified"},{"patch":"@@ -294,1 +294,1 @@\n-    private static boolean isUseSharedSpacesDisabled() {\n+    public static boolean isUseSharedSpacesDisabled() {\n","filename":"test\/hotspot\/jtreg\/runtime\/cds\/appcds\/dynamicArchive\/DynamicArchiveTestBase.java","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -27,1 +27,1 @@\n- * @bug 8274944\n+ * @bug 8274944 8276184\n@@ -56,10 +56,14 @@\n-        dump(topArchiveName,\n-            \"-XX:+UnlockDiagnosticVMOptions\",\n-            \"-XX:+WhiteBoxAPI\",\n-            \"-Xlog:class+load=debug,cds=debug,cds+dynamic=info\",\n-            use_whitebox_jar,\n-            \"-cp\", appJar, mainClass)\n-            .assertNormalExit(output -> {\n-                output.shouldContain(\"Skipping OldProvider: Old class has been linked\")\n-                      .shouldMatch(\"Skipping.LambdaContainsOldInfApp[$][$]Lambda[$].*0x.*:.*Old.class.has.been.linked\")\n-                      .shouldHaveExitValue(0);\n+        String[] mainArgs = { \"dummy\", \"addLambda\" };\n+\n+        for (String mainArg : mainArgs) {\n+\n+            dump(topArchiveName,\n+                \"-XX:+UnlockDiagnosticVMOptions\",\n+                \"-XX:+WhiteBoxAPI\",\n+                \"-Xlog:class+load=debug,cds=debug,cds+dynamic=info\",\n+                use_whitebox_jar,\n+                \"-cp\", appJar, mainClass, mainArg)\n+                .assertNormalExit(output -> {\n+                    output.shouldContain(\"Skipping OldProvider: Old class has been linked\")\n+                          .shouldMatch(\"Skipping.LambdaContainsOldInfApp[$][$]Lambda[$].*0x.*:.*Old.class.has.been.linked\")\n+                          .shouldHaveExitValue(0);\n@@ -68,11 +72,11 @@\n-        run(topArchiveName,\n-            \"-XX:+UnlockDiagnosticVMOptions\",\n-            \"-XX:+WhiteBoxAPI\",\n-            use_whitebox_jar,\n-            \"-Xlog:class+load=debug\",\n-            \"-cp\", appJar, mainClass)\n-            .assertNormalExit(output -> {\n-                output.shouldContain(\"[class,load] LambdaContainsOldInfApp source: shared objects file (top)\")\n-                      .shouldMatch(\".class.load. OldProvider.source:.*lambda_contains_old_inf.jar\")\n-                      .shouldMatch(\".class.load. LambdaContainsOldInfApp[$][$]Lambda[$].*\/0x.*source:.*LambdaContainsOldInf\")\n-                      .shouldHaveExitValue(0);\n+            run(topArchiveName,\n+                \"-XX:+UnlockDiagnosticVMOptions\",\n+                \"-XX:+WhiteBoxAPI\",\n+                use_whitebox_jar,\n+                \"-Xlog:class+load=debug\",\n+                \"-cp\", appJar, mainClass, mainArg)\n+                .assertNormalExit(output -> {\n+                    output.shouldContain(\"[class,load] LambdaContainsOldInfApp source: shared objects file (top)\")\n+                          .shouldMatch(\".class.load. OldProvider.source:.*lambda_contains_old_inf.jar\")\n+                          .shouldMatch(\".class.load. LambdaContainsOldInfApp[$][$]Lambda[$].*\/0x.*source:.*LambdaContainsOldInf\")\n+                          .shouldHaveExitValue(0);\n@@ -80,0 +84,1 @@\n+        }\n","filename":"test\/hotspot\/jtreg\/runtime\/cds\/appcds\/dynamicArchive\/LambdaContainsOldInf.java","additions":27,"deletions":22,"binary":false,"changes":49,"status":"modified"},{"patch":"@@ -0,0 +1,97 @@\n+\/*\n+ * Copyright (c) 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+\/*\n+ * @test\n+ * @bug 8276184\n+ * @summary Archive an old interface in the base archive and an app class which\n+ *          uses the old interface via a lambda expression in the dynamic archive.\n+ *          The lambda proxy class of the app class should be in the dynamic archive.\n+ * @requires vm.cds\n+ * @library \/test\/lib \/test\/hotspot\/jtreg\/runtime\/cds\/appcds \/test\/hotspot\/jtreg\/runtime\/cds\/appcds\/test-classes\n+ * @build LambdaContainsOldInfApp sun.hotspot.WhiteBox OldProvider\n+ * @run driver jdk.test.lib.helpers.ClassFileInstaller -jar old-inf-base-archive.jar LambdaContainsOldInfApp OldProvider\n+ * @run driver jdk.test.lib.helpers.ClassFileInstaller sun.hotspot.WhiteBox\n+ * @run main\/othervm -XX:+UnlockDiagnosticVMOptions -XX:+WhiteBoxAPI -Xbootclasspath\/a:. LambdaForOldInfInBaseArchive\n+ *\/\n+\n+import java.io.File;\n+import jdk.test.lib.cds.CDSOptions;\n+import jdk.test.lib.cds.CDSTestUtils;\n+import jdk.test.lib.process.OutputAnalyzer;\n+import jdk.test.lib.helpers.ClassFileInstaller;\n+\n+public class LambdaForOldInfInBaseArchive extends DynamicArchiveTestBase {\n+    static final String classList = CDSTestUtils.getOutputFileName(\"classlist\");\n+    static final String appClass = \"LambdaContainsOldInfApp\";\n+    static final String baseArchiveClass = \"OldProvider\";\n+\n+    public static void main(String[] args) throws Exception {\n+        runTest(LambdaForOldInfInBaseArchive::testCustomBase);\n+    }\n+\n+    static void testCustomBase() throws Exception {\n+        String topArchiveName = getNewArchiveName(\"top\");\n+        doTestCustomBase(topArchiveName);\n+    }\n+\n+    private static void doTestCustomBase(String topArchiveName) throws Exception {\n+        String appJar = ClassFileInstaller.getJarPath(\"old-inf-base-archive.jar\");\n+\n+        \/\/ create a custom base archive containing and old interface\n+        OutputAnalyzer output = TestCommon.dump(appJar,\n+            TestCommon.list(\"OldProvider\"), \"-Xlog:class+load,cds+class=debug\");\n+        TestCommon.checkDump(output);\n+        \/\/ Check that the OldProvider is being dumped into the base archive.\n+        output.shouldMatch(\".cds,class.*klass.*0x.*app.*OldProvider.*unlinked\");\n+\n+        String baseArchiveName = TestCommon.getCurrentArchiveName();\n+\n+        \/\/ create a dynamic archive with the custom base archive.\n+        \/\/ The old interface is in the base archive and will be\n+        \/\/ accessed using a lambda expression of LambdaContainsOldInfApp.\n+        \/\/ The lambda proxy class and the app class will be archived in the dynamic archive.\n+        dump2(baseArchiveName, topArchiveName,\n+              \"-Xlog:cds,cds+dynamic,class+load,cds+class=debug\",\n+              \"-cp\", appJar,\n+              appClass)\n+            .assertNormalExit(out -> {\n+                    out.shouldContain(\"OldProvider source: shared objects file\")\n+                       .shouldMatch(\"Archiving hidden LambdaContainsOldInfApp[$][$]Lambda[$][\\\\d+]*\");\n+                });\n+\n+        \/\/ Run with both base and dynamic archives. The OldProvider class\n+        \/\/ should be loaded from the base archive. The LambdaContainsOldInfApp\n+        \/\/ and its lambda proxy class should be loaded from the dynamic archive.\n+        run2(baseArchiveName, topArchiveName,\n+              \"-Xlog:cds,cds+dynamic,class+load\",\n+              \"-cp\", appJar,\n+              appClass)\n+            .assertNormalExit(out -> {\n+                    out.shouldContain(\"OldProvider source: shared objects file\")\n+                       .shouldContain(\"LambdaContainsOldInfApp source: shared objects file (top)\")\n+                       .shouldMatch(\".class.load. LambdaContainsOldInfApp[$][$]Lambda[$].*\/0x.*source:.*shared.*objects.*file.*(top)\");\n+                });\n+    }\n+}\n","filename":"test\/hotspot\/jtreg\/runtime\/cds\/appcds\/dynamicArchive\/LambdaForOldInfInBaseArchive.java","additions":97,"deletions":0,"binary":false,"changes":97,"status":"added"},{"patch":"@@ -0,0 +1,103 @@\n+\/*\n+ * Copyright (c) 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+\/*\n+ * @test\n+ * @bug 8276184\n+ * @summary Archive an old class in the base archive and an app class which\n+ *          uses the old class in the dynamic archive.\n+ *          The old class should be loaded from the base archive. The app class\n+ *          should be loaded from the dynamic archive.\n+ * @requires vm.cds\n+ * @library \/test\/lib \/test\/hotspot\/jtreg\/runtime\/cds\/appcds \/test\/hotspot\/jtreg\/runtime\/cds\/appcds\/test-classes\n+ * @build OldSuperApp sun.hotspot.WhiteBox OldSuper ChildOldSuper GChild\n+ * @run driver jdk.test.lib.helpers.ClassFileInstaller -jar old-class-base-archive.jar OldSuperApp OldSuper ChildOldSuper GChild\n+ * @run driver jdk.test.lib.helpers.ClassFileInstaller sun.hotspot.WhiteBox\n+ * @run main\/othervm -XX:+UnlockDiagnosticVMOptions -XX:+WhiteBoxAPI -Xbootclasspath\/a:. OldClassInBaseArchive\n+ *\/\n+\n+import java.io.File;\n+import jdk.test.lib.cds.CDSOptions;\n+import jdk.test.lib.cds.CDSTestUtils;\n+import jdk.test.lib.process.OutputAnalyzer;\n+import jdk.test.lib.helpers.ClassFileInstaller;\n+\n+public class OldClassInBaseArchive extends DynamicArchiveTestBase {\n+    static final String classList = CDSTestUtils.getOutputFileName(\"classlist\");\n+    static final String appClass = \"OldSuperApp\";\n+    static final String baseArchiveClass = \"OldSuper\";\n+\n+    public static void main(String[] args) throws Exception {\n+        runTest(OldClassInBaseArchive::testCustomBase);\n+    }\n+\n+    static void testCustomBase() throws Exception {\n+        String topArchiveName = getNewArchiveName(\"top\");\n+        doTestCustomBase(topArchiveName);\n+    }\n+\n+    private static void doTestCustomBase(String topArchiveName) throws Exception {\n+        String appJar = ClassFileInstaller.getJarPath(\"old-class-base-archive.jar\");\n+\n+        \/\/ create a custom base archive containing and old class\n+        OutputAnalyzer output = TestCommon.dump(appJar,\n+            TestCommon.list(\"OldSuper\"), \"-Xlog:class+load,cds+class=debug\");\n+        TestCommon.checkDump(output);\n+        \/\/ Check the OldSuper is being dumped into the base archive.\n+        output.shouldMatch(\".cds.class.*klass.*0x.*app.*OldSuper.*unlinked\");\n+\n+        String baseArchiveName = TestCommon.getCurrentArchiveName();\n+\n+        \/\/ create a dynamic archive with the custom base archive.\n+        \/\/ The old class is in the base archive and will be\n+        \/\/ accessed from OldSuperApp.\n+        \/\/ The OldSuperApp, ChildOldSuper, and GChild classes will be archived\n+        \/\/ in the dynamic archive.\n+        dump2(baseArchiveName, topArchiveName,\n+              \"-Xlog:cds,cds+dynamic,class+load,cds+class=debug\",\n+              \"-cp\", appJar,\n+              appClass)\n+            .assertNormalExit(out -> {\n+                    out.shouldContain(\"OldSuper source: shared objects file\")\n+                       \/\/ Check the following classes are being dumped into the dynamic archive.\n+                       .shouldMatch(\".cds,class.*klass.*0x.*app.*OldSuperApp\")\n+                       .shouldMatch(\".cds,class.*klass.*0x.*app.*ChildOldSuper\")\n+                       .shouldMatch(\".cds,class.*klass.*0x.*app.*GChild\");\n+                });\n+\n+        \/\/ Run with both base and dynamic archives. The OldSuper class\n+        \/\/ should be loaded from the base archive. The OldSuperApp\n+        \/\/ and related classes should be loaded from the dynamic archive.\n+        run2(baseArchiveName, topArchiveName,\n+              \"-Xlog:cds,cds+dynamic,class+load\",\n+              \"-cp\", appJar,\n+              appClass)\n+            .assertNormalExit(out -> {\n+                    out.shouldContain(\"OldSuper source: shared objects file\")\n+                       .shouldContain(\"OldSuperApp source: shared objects file (top)\")\n+                       .shouldContain(\"ChildOldSuper source: shared objects file (top)\")\n+                       .shouldContain(\"GChild source: shared objects file (top)\");\n+                });\n+    }\n+}\n","filename":"test\/hotspot\/jtreg\/runtime\/cds\/appcds\/dynamicArchive\/OldClassInBaseArchive.java","additions":103,"deletions":0,"binary":false,"changes":103,"status":"added"},{"patch":"@@ -0,0 +1,107 @@\n+\/*\n+ * Copyright (c) 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+\/*\n+ * @test\n+ * @bug 8276184\n+ * @summary If the caller class is redefined during dump time, the caller class\n+ *          and its lambda proxy class should not be archived.\n+ * @requires vm.cds\n+ * @requires vm.jvmti\n+ * @library \/test\/lib \/test\/hotspot\/jtreg\/runtime\/cds\/appcds\n+ *          \/test\/hotspot\/jtreg\/runtime\/cds\/appcds\/test-classes\n+ *          \/test\/hotspot\/jtreg\/runtime\/cds\/appcds\/dynamicArchive\/test-classes\n+ * @build sun.hotspot.WhiteBox OldProvider\n+ * @run driver jdk.test.lib.helpers.ClassFileInstaller sun.hotspot.WhiteBox\n+ * @run driver RedefineClassHelper\n+ * @run main\/othervm -XX:+UnlockDiagnosticVMOptions -XX:+WhiteBoxAPI -Xbootclasspath\/a:. RedefineCallerClassTest\n+ *\/\n+\n+import jdk.test.lib.helpers.ClassFileInstaller;\n+\n+public class RedefineCallerClassTest extends DynamicArchiveTestBase {\n+    static String mainClass = RedefineCallerClass.class.getName();\n+\n+    static String providerClass = OldProvider.class.getName();\n+\n+    static String sharedClasses[] = {\n+        mainClass,\n+        \"SimpleLambda\", \/\/ caller class will be redefined in RedefineCallerClass\n+        providerClass,  \/\/ inteface with class file major version < 50\n+        \"jdk\/test\/lib\/compiler\/InMemoryJavaCompiler\",\n+        \"jdk\/test\/lib\/compiler\/InMemoryJavaCompiler$FileManagerWrapper\",\n+        \"jdk\/test\/lib\/compiler\/InMemoryJavaCompiler$FileManagerWrapper$1\",\n+        \"jdk\/test\/lib\/compiler\/InMemoryJavaCompiler$MemoryJavaFileObject\"\n+    };\n+\n+    public static void main(String[] args) throws Exception {\n+        runTest(RedefineCallerClassTest::test);\n+    }\n+\n+    static void test() throws Exception {\n+        String topArchiveName = getNewArchiveName();\n+        String appJar = ClassFileInstaller.writeJar(\"redefine_caller_class.jar\", sharedClasses);\n+\n+        String[] mainArgs = {\n+            \"redefineCaller\", \/\/ redefine caller class only\n+            \"useOldInf\",      \/\/ use old interface only\n+            \"both\"            \/\/ both of the above\n+        };\n+\n+        for (String mainArg : mainArgs) {\n+            String[] options = {\n+                \"-Xlog:class+load,cds\",\n+                \"-XX:+UnlockDiagnosticVMOptions\",\n+                \"-XX:+AllowArchivingWithJavaAgent\",\n+                \"-javaagent:redefineagent.jar\",\n+                \"-cp\", appJar, mainClass, mainArg\n+            };\n+\n+            dump(topArchiveName, options)\n+                .assertNormalExit(output -> {\n+                    output.shouldHaveExitValue(0);\n+                    if (mainArg.equals(\"both\") || mainArg.equals(\"useOldInf\")) {\n+                        output.shouldContain(\"Skipping OldProvider: Old class has been linked\")\n+                              .shouldMatch(\"Skipping.SimpleLambda[$][$]Lambda[$].*0x.*:.*Old.class.has.been.linked\");\n+                    }\n+                    if (mainArg.equals(\"both\") || mainArg.equals(\"redefineCaller\")) {\n+                        output.shouldContain(\"Skipping SimpleLambda: Has been redefined\");\n+                    }\n+                });\n+\n+            run(topArchiveName, options)\n+                .assertNormalExit(output -> {\n+                    output.shouldHaveExitValue(0)\n+                          .shouldContain(\"RedefineCallerClass source: shared objects file (top)\")\n+                          .shouldMatch(\".class.load. SimpleLambda[$][$]Lambda[$].*\/0x.*source:.*SimpleLambda\");\n+                    if (mainArg.equals(\"both\") || mainArg.equals(\"useOldInf\")) {\n+                        output.shouldMatch(\".class.load. OldProvider.source:.*redefine_caller_class.jar\");\n+                    }\n+                    if (mainArg.equals(\"both\") || mainArg.equals(\"redefineCaller\")) {\n+                        output.shouldMatch(\".class.load. SimpleLambda.source:.*redefine_caller_class.jar\");\n+                    }\n+                });\n+        }\n+    }\n+}\n","filename":"test\/hotspot\/jtreg\/runtime\/cds\/appcds\/dynamicArchive\/RedefineCallerClassTest.java","additions":107,"deletions":0,"binary":false,"changes":107,"status":"added"},{"patch":"@@ -27,0 +27,1 @@\n+ * @bug 8276787\n@@ -37,2 +38,0 @@\n-import jdk.test.lib.helpers.ClassFileInstaller;\n-\n@@ -40,0 +39,3 @@\n+import jdk.test.lib.Asserts;\n+import jdk.test.lib.helpers.ClassFileInstaller;\n+import jtreg.SkippedException;\n@@ -56,0 +58,20 @@\n+    private static int testnum = 0;\n+    private static void testcase(String s) {\n+        System.out.println(\"\\n\\nTest #\" + (++testnum) + \" \" + s);\n+    }\n+\n+    private interface MyRunnable {\n+        public void run() throws Exception;\n+    }\n+\n+    private static void mustSkipWith(String expectedMsg, MyRunnable r) throws Exception {\n+        try {\n+            r.run();\n+        } catch (SkippedException e) {\n+            System.out.println(\"Got SkippedException: \" + e);\n+            Asserts.assertTrue(e.getMessage().contains(expectedMsg), \"SkippedException must have message \" + expectedMsg);\n+            return;\n+        }\n+        Asserts.fail(\"SkippedException should have been thrown\");\n+    }\n+\n@@ -61,1 +83,1 @@\n-        \/\/ -Xshare:dump specified with -XX:ArchiveClassesAtExit\n+        testcase(\"-Xshare:dump specified with -XX:ArchiveClassesAtExit\");\n@@ -71,2 +93,1 @@\n-        \/\/ more than 1 archive file specified in -XX:SharedArchiveFile during\n-        \/\/ dynamic dumpgin\n+        testcase(\"more than 1 archive file specified in -XX:SharedArchiveFile during dynamic dump\");\n@@ -82,1 +103,1 @@\n-        \/\/ normal dynamic archive dumping\n+        testcase(\"normal dynamic archive dumping\");\n@@ -91,1 +112,1 @@\n-        \/\/ same archive file specified for -XX:SharedArchiveFile and -XX:ArchiveClassesAtExit\n+        testcase(\"same archive file specified for -XX:SharedArchiveFile and -XX:ArchiveClassesAtExit\");\n@@ -102,1 +123,1 @@\n-        \/\/ a top archive specified in the base archive position\n+        testcase(\"a top archive specified in the base archive position\");\n@@ -111,1 +132,1 @@\n-        \/\/ a base archive specified in the top archive position\n+        testcase(\"a base archive specified in the top archive position\");\n@@ -120,1 +141,1 @@\n-        \/\/ more than 2 archives specified in the -XX:ShareArchiveFile option\n+        testcase(\"more than 2 archives specified in the -XX:ShareArchiveFile option\");\n@@ -131,1 +152,1 @@\n-        \/\/ base archive not specified\n+        testcase(\"base archive not specified\");\n@@ -142,1 +163,1 @@\n-        \/\/ top archive not specified\n+        testcase(\"top archive not specified\");\n@@ -152,0 +173,95 @@\n+\n+\n+        testcase(\"A dynamic archive is already loaded when -XX:SharedArchiveFile is specified\");\n+        dump2(baseArchiveName \/*this is overridden by -XX:SharedArchiveFile= below*\/,\n+              topArchiveName,\n+              \"-XX:SharedArchiveFile=\" + topArchiveName,\n+              \"-cp\", appJar, mainClass)\n+            .assertAbnormalExit(\"-XX:ArchiveClassesAtExit is unsupported when a dynamic CDS archive is specified in -XX:SharedArchiveFile:\");\n+\n+        testcase(\"A dynamic archive is already loaded when -XX:+RecordDynamicDumpInfo is specified\");\n+        if (isUseSharedSpacesDisabled()) {\n+            System.out.println(\"This test is not applicable when JTREG tests are executed with -Xshare:off, or if the JDK doesn't have a default archive.\");\n+        } else {\n+          run2(null, topArchiveName,\n+               \"-XX:+RecordDynamicDumpInfo\",\n+               \"-cp\", appJar, mainClass)\n+              .assertAbnormalExit(\"-XX:+RecordDynamicDumpInfo is unsupported when a dynamic CDS archive is specified in -XX:SharedArchiveFile:\");\n+        }\n+\n+        testcase(\"-XX:+RecordDynamicDumpInfo cannot be used with -XX:ArchiveClassesAtExit\");\n+        dump2(baseArchiveName,\n+              topArchiveName,\n+              \"-XX:+RecordDynamicDumpInfo\",\n+              \"-cp\", appJar, mainClass)\n+            .assertAbnormalExit(\"-XX:+RecordDynamicDumpInfo cannot be used with -XX:ArchiveClassesAtExit\");\n+\n+        testcase(\"Specifying -XX:+RecordDynamicDumpInfo should not cause dynamic dump\");\n+        run2(baseArchiveName, null,\n+             \"-XX:+RecordDynamicDumpInfo\",\n+             \"-Xlog:cds+dynamic=debug\",\n+             \"-cp\", appJar, mainClass)\n+            .assertNormalExit(output -> {\n+                    output.shouldNotMatch(\"\\\\[cds,dynamic\");\n+                });\n+\n+        {\n+            String ERROR = \"-XX:ArchiveClassesAtExit is unsupported when base CDS archive is not loaded\";\n+\n+            testcase(\"-XX:ArchiveClassesAtExit with CDS disabled (-Xshare:off)\");\n+            mustSkipWith(ERROR, () -> {\n+                    dump2(baseArchiveName,\n+                          topArchiveName,\n+                          \"-Xshare:off\",\n+                          \"-cp\", appJar, mainClass);\n+                });\n+\n+            testcase(\"-XX:ArchiveClassesAtExit with CDS disabled (Base archive cannot be mapped -- doesn't exist\");\n+            mustSkipWith(ERROR, () -> {\n+                    dump2(baseArchiveName + \".notExist\",\n+                          topArchiveName,\n+                          \"-Xlog:cds\",\n+                          \"-Xshare:auto\",\n+                          \"-cp\", appJar, mainClass);\n+                });\n+\n+            testcase(\"-XX:ArchiveClassesAtExit with CDS disabled (incompatible VM options)\");\n+            dump2(baseArchiveName,\n+                  topArchiveName,\n+                  \"--patch-module\",\n+                  \"foo.bar=xxx\",\n+                  \"-Xshare:auto\",\n+                  \"-Xlog:cds\",\n+                  \"-cp\", appJar, mainClass)\n+                .assertAbnormalExit(\"Cannot use the following option when dumping the shared archive: --patch-module\");\n+        }\n+\n+        {\n+            String ERROR = \"-XX:+RecordDynamicDumpInfo is unsupported when base CDS archive is not loaded\";\n+\n+            testcase(\"-XX:+RecordDynamicDumpInfo with CDS disabled (-Xshare:off)\");\n+            run2(baseArchiveName, null,\n+                 \"-XX:+RecordDynamicDumpInfo\",\n+                 \"-Xshare:off\",\n+                 \"-cp\", appJar, mainClass)\n+                .assertAbnormalExit(ERROR);\n+\n+            testcase(\"-XX:+RecordDynamicDumpInfo with CDS disabled (Base archive cannot be mapped -- doesn't exist\");\n+            run2(baseArchiveName + \".notExist\", null,\n+                 \"-XX:+RecordDynamicDumpInfo\",\n+                 \"-Xshare:auto\",\n+                 \"-Xlog:cds\",\n+                 \"-cp\", appJar, mainClass)\n+                .assertAbnormalExit(ERROR);\n+\n+            testcase(\"-XX:+RecordDynamicDumpInfo with CDS disabled (incompatible VM options)\");\n+            run2(baseArchiveName + \".notExist\", null,\n+                 \"-XX:+RecordDynamicDumpInfo\",\n+                 \"--patch-module\",\n+                 \"foo.bar=xxx\",\n+                 \"-Xshare:auto\",\n+                 \"-Xlog:cds\",\n+                 \"-cp\", appJar, mainClass)\n+                .assertAbnormalExit(\"CDS is disabled when the --patch-module option is specified\",\n+                                    ERROR);\n+        }\n","filename":"test\/hotspot\/jtreg\/runtime\/cds\/appcds\/dynamicArchive\/SharedArchiveFileOption.java","additions":128,"deletions":12,"binary":false,"changes":140,"status":"modified"},{"patch":"@@ -0,0 +1,65 @@\n+\/*\n+ * Copyright (c) 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+class SimpleLambda {\n+    public Runnable getRunnable() {\n+        return () -> {};\n+    }\n+    public OldProvider getProvider() {\n+        return () -> {\n+            return null;\n+        };\n+    }\n+}\n+\n+public class RedefineCallerClass {\n+\n+    public static String newClass =\n+        \" class SimpleLambda { \" +\n+        \"     public Runnable getRunnable() { \" +\n+        \"         return () -> {}; \" +\n+        \"     } \" +\n+        \"     public OldProvider getProvider() { \" +\n+        \"         return () -> { \" +\n+        \"             return null; \" +\n+        \"         }; \" +\n+        \"     } \" +\n+        \" } \";\n+\n+    public static void main(String args[]) throws Exception {\n+      String mode = \"both\";\n+      if (args.length == 1) {\n+          mode = args[0];\n+      }\n+      SimpleLambda s = new SimpleLambda();\n+      if (mode.equals(\"both\") || mode.equals(\"useOldInf\")) {\n+          System.out.println(s.getProvider());\n+      } else {\n+          System.out.println(s.getRunnable());\n+      }\n+      if (mode.equals(\"both\") || mode.equals(\"redefineCaller\")) {\n+          RedefineClassHelper.redefineClass(s.getClass(), newClass);\n+      }\n+    }\n+}\n","filename":"test\/hotspot\/jtreg\/runtime\/cds\/appcds\/dynamicArchive\/test-classes\/RedefineCallerClass.java","additions":65,"deletions":0,"binary":false,"changes":65,"status":"added"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2014, 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2014, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -28,0 +28,7 @@\n+    if (args.length > 0 && args[0].equals(\"testlambda\")) {\n+        System.out.println(getRunnable());\n+    }\n+  }\n+\n+  public static Runnable getRunnable() {\n+    return () -> {};\n","filename":"test\/hotspot\/jtreg\/runtime\/cds\/appcds\/test-classes\/Hello.java","additions":8,"deletions":1,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -27,0 +27,3 @@\n+        if (args.length == 1 && args[0].equals(\"addLambda\")) {\n+            getProvider2();\n+        }\n@@ -34,0 +37,6 @@\n+\n+    public static OldProvider getProvider2() {\n+        return () -> {\n+            return null;\n+        };\n+    }\n","filename":"test\/hotspot\/jtreg\/runtime\/cds\/appcds\/test-classes\/LambdaContainsOldInfApp.java","additions":9,"deletions":0,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -0,0 +1,103 @@\n+\/*\n+ * Copyright (c) 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+package ir_framework.tests;\n+\n+import compiler.lib.ir_framework.*;\n+import compiler.lib.ir_framework.driver.IRViolationException;\n+import jdk.test.lib.Asserts;\n+\n+\/*\n+ * @test\n+ * @bug 8276546\n+ * @requires vm.debug == true & vm.compMode != \"Xint\" & vm.compMode != \"Xcomp\" & vm.compiler1.enabled & vm.compiler2.enabled & vm.flagless\n+ * @summary Test that CompileThreshold flag is ignored when passed as Java\/VM option to the framework.\n+ *          Normally, the framework should be called with driver.\n+ * @library \/test\/lib \/testlibrary_tests \/\n+ * @run main\/othervm -XX:CompileThreshold=12 -XX:+UseG1GC ir_framework.tests.TestCompileThreshold\n+ *\/\n+\n+public class TestCompileThreshold {\n+    public int iFld = 0;\n+\n+    public static void main(String[] args) throws Exception {\n+        try {\n+            \/\/ CompileThreshold=12 passed to the JTreg test is ignored even though we prefer command line flags.\n+            \/\/ CompileThreshold=10 is user defined and passed directly to the framework and thus not ignored.\n+            \/\/ InterpreterProfilePercentage=0 ensures that we compile exactly after 10 invocations.\n+            TestFramework.runWithFlags(\"-XX:CompileThreshold=10\", \"-XX:InterpreterProfilePercentage=0\",\n+                                       \"-XX:-TieredCompilation\", \"-DTest=testWithCompileThreshold\",\n+                                       \"-DPreferCommandLineFlags=true\");\n+        } catch (IRViolationException e) {\n+            Asserts.assertTrue(e.getExceptionInfo().contains(\"Failed IR Rules (1)\"), \"exactly one rule failed\");\n+            Asserts.assertTrue(e.getExceptionInfo().contains(\"testWithCompileThreshold()\"),\n+                               \"testWithCompileThreshold() failed\");\n+        }\n+\n+        try {\n+            TestFramework.runWithFlags(\"-XX:InterpreterProfilePercentage=0\", \"-XX:-TieredCompilation\",\n+                                       \"-DTest=testWithoutCompileThreshold\");\n+        } catch (IRViolationException e) {\n+            Asserts.assertTrue(e.getExceptionInfo().contains(\"Failed IR Rules (1)\"), \"exactly one rule failed\");\n+            Asserts.assertTrue(e.getExceptionInfo().contains(\"testWithoutCompileThreshold()\"),\n+                               \"testWithoutCompileThreshold() failed\");\n+        }\n+    }\n+\n+    @Test\n+    @IR(counts = {IRNode.CALL, \"1\"}) \/\/ fails\n+    public void testWithCompileThreshold() {\n+        iFld++;\n+    }\n+\n+    @Run(test = \"testWithCompileThreshold\")\n+    @Warmup(20)\n+    public void runTestWithCompileThreshold(RunInfo info) {\n+        if (iFld == 10) {\n+            TestFramework.assertNotCompiled(info.getTest());\n+        } else if (iFld == 11) {\n+            \/\/ CompileThreshold=10 is passed directly as a flag to the framework.\n+            \/\/ Therefore, testWithCompileThreshold() must be compiled by now.\n+            TestFramework.assertCompiled(info.getTest());\n+        }\n+        testWithCompileThreshold();\n+    }\n+\n+\n+    @Test\n+    @IR(counts = {IRNode.CALL, \"1\"}) \/\/ fails\n+    public void testWithoutCompileThreshold() {\n+        iFld++;\n+    }\n+\n+    @Run(test = \"testWithoutCompileThreshold\")\n+    @Warmup(20)\n+    public void runTestWithoutCompileThreshold(RunInfo info) {\n+        testWithCompileThreshold();\n+        if (info.isWarmUp()) {\n+            \/\/ CompileThreshold=12 is passed to the JTreg test but not directly to the framework.\n+            \/\/ Therefore, it is ignored and we do not trigger a compilation until the framework does.\n+            TestFramework.assertNotCompiled(info.getTest());\n+        }\n+    }\n+}\n","filename":"test\/hotspot\/jtreg\/testlibrary_tests\/ir_framework\/tests\/TestCompileThreshold.java","additions":103,"deletions":0,"binary":false,"changes":103,"status":"added"},{"patch":"@@ -26,0 +26,1 @@\n+ * @bug 8273277\n@@ -33,0 +34,1 @@\n+ * @run main\/othervm -XX:-OmitStackTraceInFastThrow -Xbatch -XX:Tier0BackedgeNotifyFreqLog=0 -XX:Tier2BackedgeNotifyFreqLog=0 -XX:Tier3BackedgeNotifyFreqLog=0 -XX:Tier2BackEdgeThreshold=1 -XX:Tier3BackEdgeThreshold=1 -XX:Tier4BackEdgeThreshold=1 jit.t.t105.t105\n@@ -34,1 +36,1 @@\n- * This test must be run with ProfileTraps disabled to avoid preallocated\n+ * This test must be run with OmitStackTraceInFastThrow disabled to avoid preallocated\n","filename":"test\/hotspot\/jtreg\/vmTestbase\/jit\/t\/t105\/t105.java","additions":3,"deletions":1,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2017, 2018, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2017, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -74,0 +74,1 @@\n+                                               .addToolArg(\"-J-Xmx1G\")\n","filename":"test\/hotspot\/jtreg\/vmTestbase\/nsk\/sysdict\/share\/GenClassesBuilder.java","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -151,2 +151,2 @@\n-                } catch (OutOfMemoryError | ClassNotFoundException e) {\n-                    \/\/ just ignore\n+                } catch (OutOfMemoryError | ClassNotFoundException | NoClassDefFoundError e) {\n+                    \/\/ just ignore, note that CNFE and NCDFE can be caused by OOM exceptions.\n","filename":"test\/hotspot\/jtreg\/vmTestbase\/nsk\/sysdict\/share\/SysDictTest.java","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -193,1 +193,1 @@\n-java\/awt\/FullScreen\/FullScreenInsets\/FullScreenInsets.java 7019055,8266245 windows-all,linux-all,macosx-aarch64\n+java\/awt\/FullScreen\/FullScreenInsets\/FullScreenInsets.java 7019055,8266245 windows-all,linux-all,macosx-all\n@@ -261,1 +261,1 @@\n-java\/awt\/Frame\/MiscUndecorated\/RepaintTest.java 8079267,8266244 windows-all,linux-all,macosx-aarch64\n+java\/awt\/Frame\/MiscUndecorated\/RepaintTest.java 8266244 macosx-aarch64\n@@ -492,1 +492,0 @@\n-java\/awt\/FullScreen\/NoResizeEventOnDMChangeTest\/NoResizeEventOnDMChangeTest.java 8169468 macosx-all\n","filename":"test\/jdk\/ProblemList.txt","additions":2,"deletions":3,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -0,0 +1,245 @@\n+\/*\n+ * Copyright (c) 2021 SAP SE. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+\/**\n+ * @test\n+ * @bug 8274687\n+ * @summary Test the special handling in the JDWP agent of threads that call\n+ *          j.l.Thread.resume().\n+ *\n+ *          This is the sequence of actions by the debugger and the threads\n+ *          \"main\" and \"resumee\" in the target vm.\n+ *\n+ *          \"resumee\": Reaches breakpoint in methodWithBreakpoint() and is\n+ *                     suspended then.\n+ *\n+ *          \"main\": Calls j.l.Thread.resume() for \"resumee\". There is an internal\n+ *                  breakpoint in Thread.resume() so the JDWP agent receives a\n+ *                  breakpoint event. It finds that \"resumee\" is suspended because\n+ *                  of JDWP actions. The resume() call would interfere with the\n+ *                  debugger therefore \"main\" is blocked.\n+ *\n+ *          Debugger: Tests if the suspended \"resumee\" can be suspended a second\n+ *                    time and resumes it again.\n+ *\n+ *          Debugger: Resumes \"resumee\" by calling ThreadReference.resume().\n+ *                    The JDWP agent notifies \"main\" about it.\n+ *\n+ *          \"resumee\": Continues execution.\n+ *\n+ *          \"main\": Receives the notification, finds that \"resumee\" is not\n+ *                  suspended anymore and continues execution.\n+ *\n+ *          Debugger: Verifies that \"main\" is no longer blocked.\n+ *\n+ * @author Richard Reingruber richard DOT reingruber AT sap DOT com\n+ *\n+ * @library \/test\/lib\n+ *\n+ * @run build TestScaffold VMConnection TargetListener TargetAdapter\n+ * @run compile -g ResumeAfterThreadResumeCallTest.java\n+ * @run driver ResumeAfterThreadResumeCallTest\n+ *\/\n+import com.sun.jdi.*;\n+import com.sun.jdi.event.*;\n+import jdk.test.lib.Asserts;\n+\n+import java.util.*;\n+\n+\/\/ Target program for the debugger\n+class ResumeAfterThreadResumeCallTarg extends Thread {\n+\n+    public boolean reachedBreakpoint;\n+    public boolean mainThreadReturnedFromResumeCall;\n+    public boolean testFinished;\n+\n+    public ResumeAfterThreadResumeCallTarg(String name) {\n+        super(name);\n+    }\n+\n+    public static void log(String m) {\n+        String threadName = Thread.currentThread().getName();\n+        System.out.println(\"###(Target,\"+ threadName +\") \" + m);\n+    }\n+\n+    public static void main(String[] args) {\n+        log(\"Entered main()\");\n+\n+        \/\/ Start \"resumee\" thread.\n+        ResumeAfterThreadResumeCallTarg resumee = new ResumeAfterThreadResumeCallTarg(\"resumee\");\n+        resumee.start();\n+\n+        \/\/ Wait for \"resumee\" to reach the breakpoint in methodWithBreakpoint().\n+        while (!resumee.reachedBreakpoint) {\n+            try {\n+                Thread.sleep(100);\n+            } catch (InterruptedException e) { \/* ignored *\/ }\n+        }\n+\n+        \/\/ \"resumee\" is suspended now because of the breakpoint\n+        \/\/ Calling Thread.resume() will block this thread.\n+        log(\"Calling Thread.resume()\");\n+        resumee.resume();\n+        resumee.mainThreadReturnedFromResumeCall = true;\n+        log(\"Thread.resume() returned\");\n+\n+        \/\/ Wait for debugger\n+        while (!resumee.testFinished) {\n+            try {\n+                Thread.sleep(100);\n+            } catch (InterruptedException e) { \/* ignored *\/ }\n+        }\n+    }\n+\n+    public void run() {\n+        log(\"up and running.\");\n+        methodWithBreakpoint();\n+    }\n+\n+    public void methodWithBreakpoint() {\n+        log(\"Entered methodWithBreakpoint()\");\n+    }\n+}\n+\n+\n+\/\/ Debugger program\n+\n+public class ResumeAfterThreadResumeCallTest extends TestScaffold {\n+    public static final String TARGET_CLS_NAME = ResumeAfterThreadResumeCallTarg.class.getName();\n+    public static final long UNBLOCK_TIMEOUT = 10000;\n+\n+    ResumeAfterThreadResumeCallTest (String args[]) {\n+        super(args);\n+    }\n+\n+    public static void main(String[] args)      throws Exception {\n+        new ResumeAfterThreadResumeCallTest(args).startTests();\n+    }\n+\n+    \/**\n+     * Set a breakpoint in the given method and resume all threads. The\n+     * breakpoint is configured to suspend just the thread that reaches it\n+     * instead of all threads.\n+     *\/\n+    public BreakpointEvent resumeTo(String clsName, String methodName, String signature) {\n+        boolean suspendThreadOnly = true;\n+        return resumeTo(clsName, methodName, signature, suspendThreadOnly);\n+    }\n+\n+    protected void runTests() throws Exception {\n+        BreakpointEvent bpe = startToMain(TARGET_CLS_NAME);\n+        mainThread = bpe.thread();\n+\n+        log(\"Resuming to methodWithBreakpoint()\");\n+        bpe = resumeTo(TARGET_CLS_NAME, \"methodWithBreakpoint\", \"()V\");\n+\n+        log(\"Thread \\\"resumee\\\" has reached the breakpoint and is suspended now.\");\n+        ThreadReference resumee = bpe.thread();\n+        ObjectReference resumeeThreadObj = resumee.frame(1).thisObject();\n+        printStack(resumee);\n+        mainThread.suspend();\n+        printStack(mainThread);\n+        mainThread.resume();\n+        log(\"resumee.isSuspended() -> \" + resumee.isSuspended());\n+        log(\"mainThread.isSuspended() -> \" + mainThread.isSuspended());\n+        log(\"Notify target main thread to continue by setting reachedBreakpoint = true.\");\n+        setField(resumeeThreadObj, \"reachedBreakpoint\", vm().mirrorOf(true));\n+\n+        log(\"Sleeping 500ms so that the main thread is blocked calling Thread.resume() on \\\"resumee\\\" Thread.\");\n+        Thread.sleep(500);\n+        log(\"After sleep.\");\n+        mainThread.suspend();\n+        printStack(mainThread);\n+        mainThread.resume();\n+\n+        boolean mainThreadReturnedFromResumeCall = false;\n+        boolean resumedResumee = false;\n+        for (long sleepTime = 50; sleepTime < UNBLOCK_TIMEOUT && !mainThreadReturnedFromResumeCall; sleepTime <<= 1) {\n+            log(\"mainThread.isSuspended() -> \" + mainThread.isSuspended());\n+            Value v = getField(resumeeThreadObj, \"mainThreadReturnedFromResumeCall\");\n+            mainThreadReturnedFromResumeCall = ((PrimitiveValue) v).booleanValue();\n+            if (!resumedResumee) {\n+                \/\/ main thread should still be blocked.\n+                Asserts.assertFalse(mainThreadReturnedFromResumeCall, \"main Thread was not blocked\");\n+\n+                \/\/ Test suspending the already suspended resumee thread.\n+                Asserts.assertTrue(resumee.isSuspended(), \"\\\"resumee\\\" is not suspended.\");\n+                log(\"Check if suspended \\\"resumee\\\" can be suspended a 2nd time.\");\n+                resumee.suspend();\n+                log(\"resumee.isSuspended() -> \" + resumee.isSuspended());\n+                log(\"Resuming \\\"resumee\\\"\");\n+                resumee.resume();\n+                Asserts.assertTrue(resumee.isSuspended(), \"\\\"resumee\\\" is not suspended.\");\n+\n+                \/\/ Really resume the resumee thread.\n+                log(\"Resuming \\\"resumee\\\" a 2nd time will unblock the main thread.\");\n+                resumee.resume();\n+                Asserts.assertFalse(resumee.isSuspended(), \"\\\"resumee\\\" is still suspended.\");\n+                resumedResumee = true;\n+            }\n+            log(\"Sleeping \" + sleepTime + \"ms\");\n+            Thread.sleep(sleepTime);\n+        }\n+        Asserts.assertTrue(mainThreadReturnedFromResumeCall, \"main Thread was not unblocked\");\n+\n+        setField(resumeeThreadObj, \"testFinished\", vm().mirrorOf(true));\n+\n+        \/\/ Resume the target listening for events\n+        listenUntilVMDisconnect();\n+    }\n+\n+    public void printStack(ThreadReference thread) throws Exception {\n+        log(\"Stack of thread '\" + thread.name() + \"':\");\n+        List<StackFrame> stack_frames = thread.frames();\n+        int i = 0;\n+        for (StackFrame ff : stack_frames) {\n+            Location loc = ff.location();\n+            String locString = \"bci:\" + loc.codeIndex();\n+            try {\n+                locString = loc.sourceName() + \":\" + loc.lineNumber() + \",\" + locString;\n+            } catch (AbsentInformationException e) {\/* empty *\/};\n+            log(\"  frame[\" + i++ +\"]: \" + ff.location().method() + \" (\" + locString + \")\");\n+        }\n+    }\n+\n+    public void setField(ObjectReference obj, String fName, Value val) throws Exception {\n+        log(\"set field \" + fName + \" = \" + val);\n+        ReferenceType rt = obj.referenceType();\n+        Field fld = rt.fieldByName(fName);\n+        obj.setValue(fld, val);\n+        log(\"ok\");\n+    }\n+\n+    public Value getField(ObjectReference obj, String fName) throws Exception {\n+        log(\"get field \" + fName);\n+        ReferenceType rt = obj.referenceType();\n+        Field fld = rt.fieldByName(fName);\n+        Value val = obj.getValue(fld);\n+        log(\"result : \" + val);\n+        return val;\n+    }\n+\n+    public void log(String m) {\n+        System.out.println(\"###(Debugger) \" + m);\n+    }\n+}\n","filename":"test\/jdk\/com\/sun\/jdi\/ResumeAfterThreadResumeCallTest.java","additions":245,"deletions":0,"binary":false,"changes":245,"status":"added"},{"patch":"@@ -0,0 +1,153 @@\n+\/*\n+ * Copyright (c) 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+\/*\n+ * @test\n+ * @bug 8276848\n+ * @summary Tests the command-line tool with port not specified\n+ * @modules jdk.httpserver\n+ * @library \/test\/lib\n+ * @run testng\/othervm\/manual CommandLinePortNotSpecifiedTest\n+ *\/\n+\n+import java.io.IOException;\n+import java.net.InetAddress;\n+import java.nio.file.Files;\n+import java.nio.file.Path;\n+import java.util.concurrent.TimeUnit;\n+import jdk.test.lib.Platform;\n+import jdk.test.lib.process.OutputAnalyzer;\n+import jdk.test.lib.process.ProcessTools;\n+import jdk.test.lib.util.FileUtils;\n+import org.testng.annotations.AfterTest;\n+import org.testng.annotations.BeforeTest;\n+import org.testng.annotations.Test;\n+import static java.lang.System.out;\n+\n+public class CommandLinePortNotSpecifiedTest {\n+\n+    static final Path JAVA_HOME = Path.of(System.getProperty(\"java.home\"));\n+    static final String JAVA = getJava(JAVA_HOME);\n+    static final Path CWD = Path.of(\".\").toAbsolutePath().normalize();\n+    static final Path TEST_DIR = CWD.resolve(\"CommandLinePortNotSpecifiedTest\");\n+    static final Path TEST_FILE = TEST_DIR.resolve(\"file.txt\");\n+    static final String TEST_DIR_STR = TEST_DIR.toString();\n+    static final String LOOPBACK_ADDR = InetAddress.getLoopbackAddress().getHostAddress();\n+\n+    @BeforeTest\n+    public void setup() throws IOException {\n+        if (Files.exists(TEST_DIR)) {\n+            FileUtils.deleteFileTreeWithRetry(TEST_DIR);\n+        }\n+        Files.createDirectories(TEST_DIR);\n+        Files.createFile(TEST_FILE);\n+    }\n+\n+    static final int SIGTERM = 15;\n+    static final int NORMAL_EXIT_CODE = normalExitCode();\n+\n+    static int normalExitCode() {\n+        if (Platform.isWindows()) {\n+            return 1; \/\/ expected process destroy exit code\n+        } else {\n+            \/\/ signal terminated exit code on Unix is 128 + signal value\n+            return 128 + SIGTERM;\n+        }\n+    }\n+\n+    \/**\n+     * This is a manual test to confirm the command-line tool starts successfully\n+     * in the case where the port is not specified. In this case the server uses\n+     * the default port 8000. The test is manual to avoid a BindException in the\n+     * unlikely but not impossible case that the port is already in use.\n+     *\/\n+    @Test\n+    public void testPortNotSpecified() throws Throwable {\n+        out.println(\"\\n--- testPortNotSpecified\");\n+        simpleserver(JAVA, \"-m\", \"jdk.httpserver\")\n+                .shouldHaveExitValue(NORMAL_EXIT_CODE)\n+                .shouldContain(\"Binding to loopback by default. For all interfaces use \\\"-b 0.0.0.0\\\" or \\\"-b ::\\\".\")\n+                .shouldContain(\"Serving \" + TEST_DIR_STR + \" and subdirectories on \" + LOOPBACK_ADDR + \" port\")\n+                .shouldContain(\"URL http:\/\/\" + LOOPBACK_ADDR);\n+    }\n+\n+    @AfterTest\n+    public void teardown() throws IOException {\n+        if (Files.exists(TEST_DIR)) {\n+            FileUtils.deleteFileTreeWithRetry(TEST_DIR);\n+        }\n+    }\n+\n+    \/\/ --- infra ---\n+\n+    static String getJava(Path image) {\n+        boolean isWindows = System.getProperty(\"os.name\").startsWith(\"Windows\");\n+        Path java = image.resolve(\"bin\").resolve(isWindows ? \"java.exe\" : \"java\");\n+        if (Files.notExists(java))\n+            throw new RuntimeException(java + \" not found\");\n+        return java.toAbsolutePath().toString();\n+    }\n+\n+    static final String REGULAR_STARTUP_LINE1_STRING = \"Serving\";\n+    static final String REGULAR_STARTUP_LINE2_STRING = \"URL http:\/\/\";\n+\n+    static final String OPTIONS_TEXT = \"\"\"\n+            Options:\n+            -b, --bind-address    - Address to bind to. Default: %s (loopback).\n+                                    For all interfaces use \"-b 0.0.0.0\" or \"-b ::\".\n+            -d, --directory       - Directory to serve. Default: current directory.\n+            -o, --output          - Output format. none|info|verbose. Default: info.\n+            -p, --port            - Port to listen on. Default: 8000.\n+            -h, -?, --help        - Print this help message.\n+            To stop the server, press Ctrl + C.\"\"\".formatted(LOOPBACK_ADDR);\n+\n+    \/\/ The stdout\/stderr output line to wait for when starting the simpleserver\n+    enum WaitForLine {\n+        REGULAR_STARTUP_LINE (REGULAR_STARTUP_LINE2_STRING) ,\n+        HELP_STARTUP_LINE (OPTIONS_TEXT.lines().reduce((first, second) -> second).orElseThrow());\n+\n+        final String value;\n+        WaitForLine(String value) { this.value = value; }\n+    }\n+\n+    static OutputAnalyzer simpleserver(String... args) throws Throwable {\n+        return simpleserver(WaitForLine.REGULAR_STARTUP_LINE, true, args);\n+    }\n+\n+    static OutputAnalyzer simpleserver(WaitForLine waitForLine, boolean destroy, String... args) throws Throwable {\n+        StringBuffer sb = new StringBuffer();  \/\/ stdout & stderr\n+        \/\/ start the process and await the waitForLine before returning\n+        var p = ProcessTools.startProcess(\"simpleserver\",\n+                new ProcessBuilder(args).directory(TEST_DIR.toFile()),\n+                line -> sb.append(line + \"\\n\"),\n+                line -> line.startsWith(waitForLine.value),\n+                30,  \/\/ suitably high default timeout, not expected to timeout\n+                TimeUnit.SECONDS);\n+        if (destroy) {\n+            p.destroy();  \/\/ SIGTERM on Unix\n+        }\n+        int ec = p.waitFor();\n+        var outputAnalyser = new OutputAnalyzer(sb.toString(), \"\", ec);\n+        return outputAnalyser;\n+    }\n+}\n","filename":"test\/jdk\/com\/sun\/net\/httpserver\/simpleserver\/CommandLinePortNotSpecifiedTest.java","additions":153,"deletions":0,"binary":false,"changes":153,"status":"added"},{"patch":"@@ -137,2 +137,2 @@\n-        out.println(\"\\n--- testPort, opt=\\\"%s\\\" \".formatted(opt));\n-        simpleserver(JAVA, \"-m\", \"jdk.httpserver\", opt, \"0.0.0.0\")\n+        out.println(\"\\n--- testBindAllInterfaces, opt=\\\"%s\\\" \".formatted(opt));\n+        simpleserver(JAVA, \"-m\", \"jdk.httpserver\", \"-p\", \"0\", opt, \"0.0.0.0\")\n","filename":"test\/jdk\/com\/sun\/net\/httpserver\/simpleserver\/CommandLinePositiveTest.java","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -49,0 +49,4 @@\n+    \/\/ move away from cursor\n+    private final static int OFFSET_X = -20;\n+    private final static int OFFSET_Y = -20;\n+\n@@ -104,1 +108,1 @@\n-        Color c = r.getPixelColor(p.x + f.getWidth() \/ 2, p.y + f.getHeight() \/ 2);\n+        Color c = r.getPixelColor(p.x + f.getWidth() \/ 2 - OFFSET_X, p.y + f.getHeight() \/ 2 - OFFSET_Y);\n","filename":"test\/jdk\/java\/awt\/Dialog\/MakeWindowAlwaysOnTop\/MakeWindowAlwaysOnTop.java","additions":5,"deletions":1,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1999, 2016, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1999, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -36,1 +36,11 @@\n-import java.awt.*;\n+import java.awt.BorderLayout;\n+import java.awt.Button;\n+import java.awt.Color;\n+import java.awt.Component;\n+import java.awt.Container;\n+import java.awt.Frame;\n+import java.awt.Panel;\n+import java.awt.Point;\n+import java.awt.Rectangle;\n+import java.awt.TextField;\n+import java.awt.Toolkit;\n@@ -42,2 +52,3 @@\n-import java.io.*;\n-import java.awt.image.*;\n+import java.awt.image.BufferedImage;\n+import java.awt.image.PixelGrabber;\n+import java.io.File;\n@@ -46,1 +57,1 @@\n-    private static int delay = 150;\n+    private static final int delay = 150;\n@@ -53,1 +64,1 @@\n-    private Object buttonLock = new Object();\n+    private final Object buttonLock = new Object();\n@@ -56,1 +67,1 @@\n-    private int MAX_TOLERANCE_LEVEL = 10;\n+    private final int MAX_TOLERANCE_LEVEL = 10;\n@@ -84,19 +95,0 @@\n-    \/**\n-     * Do screen capture and save it as image\n-     *\/\n-    private static void captureScreenAndSave() {\n-\n-        try {\n-            Robot robot = new Robot();\n-            Dimension screenSize = Toolkit.getDefaultToolkit().getScreenSize();\n-            Rectangle rectangle = new Rectangle(0, 0, screenSize.width, screenSize.height);\n-            System.out.println(\"About to screen capture - \" + rectangle);\n-            BufferedImage image = robot.createScreenCapture(rectangle);\n-            javax.imageio.ImageIO.write(image, \"jpg\", new File(\"ScreenImage.jpg\"));\n-            robot.delay(3000);\n-        } catch (Throwable t) {\n-            System.out.println(\"WARNING: Exception thrown while screen capture!\");\n-            t.printStackTrace();\n-        }\n-    }\n-\n@@ -118,0 +110,1 @@\n+        frame.setLocationRelativeTo(null);\n@@ -121,1 +114,0 @@\n-        frame.toFront();\n@@ -139,1 +131,1 @@\n-        }else {\n+        } else {\n@@ -178,0 +170,2 @@\n+\n+        robot.click();\n@@ -179,3 +173,0 @@\n-        robot.mousePress(InputEvent.BUTTON1_MASK);\n-        robot.waitForIdle(delay);\n-        robot.mouseRelease(InputEvent.BUTTON1_MASK);\n@@ -233,1 +224,0 @@\n-                    frame.toFront();\n","filename":"test\/jdk\/java\/awt\/Frame\/MiscUndecorated\/RepaintTest.java","additions":22,"deletions":32,"binary":false,"changes":54,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2007, 2016, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2007, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -29,1 +29,0 @@\n- * @author Dmitri.Trembovetski@sun.com: area=Graphics\n@@ -154,0 +153,2 @@\n+                                Frame f = fsWin instanceof Frame ? (Frame) fsWin : (Frame) fsWin.getOwner();\n+                                DisplayMode oldMode = f.getGraphicsConfiguration().getDevice().getDisplayMode();\n@@ -155,2 +156,13 @@\n-                                r1.incDmChanges();\n-                                r2.incDmChanges();\n+                                sleep(2000);\n+                                \/\/ Check if setting new display mode actually results in frame being\n+                                \/\/ placed onto display with different resolution.\n+                                DisplayMode newMode = f.getGraphicsConfiguration().getDevice().getDisplayMode();\n+                                if (oldMode.getWidth() != newMode.getWidth()\n+                                        || oldMode.getHeight() != newMode.getHeight()) {\n+                                    r1.incDmChanges();\n+                                    r2.incDmChanges();\n+                                } else {\n+                                    System.out.println(\"Skipping this iteration. Details:\");\n+                                    System.out.println(\"Requested device = \" + gd);\n+                                    System.out.println(\"Actual device = \" + f.getGraphicsConfiguration().getDevice());\n+                                }\n@@ -169,0 +181,1 @@\n+\n@@ -194,3 +207,6 @@\n-        try {\n-            Thread.sleep(ms);\n-        } catch (InterruptedException ex) {}\n+        long targetTime = System.currentTimeMillis() + ms;\n+        do {\n+            try {\n+                Thread.sleep(targetTime - System.currentTimeMillis());\n+            } catch (InterruptedException ex) {}\n+        } while (System.currentTimeMillis() < targetTime);\n@@ -198,0 +214,1 @@\n+\n","filename":"test\/jdk\/java\/awt\/FullScreen\/NoResizeEventOnDMChangeTest\/NoResizeEventOnDMChangeTest.java","additions":24,"deletions":7,"binary":false,"changes":31,"status":"modified"},{"patch":"@@ -56,1 +56,1 @@\n- * @bug 8147440 8147016\n+ * @bug 8147440 8147016 8270874\n","filename":"test\/jdk\/java\/awt\/Window\/WindowResizingOnDPIChanging\/WindowResizingOnMovingToAnotherDisplay.java","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -0,0 +1,107 @@\n+\/*\n+ * Copyright (c) 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+import org.testng.annotations.DataProvider;\n+import org.testng.annotations.Test;\n+import static org.testng.Assert.assertEquals;\n+\n+import java.io.ByteArrayOutputStream;\n+import java.io.IOException;\n+import java.io.OutputStreamWriter;\n+import java.io.PrintStream;\n+import java.io.PrintWriter;\n+import java.nio.charset.Charset;\n+import java.nio.charset.StandardCharsets;\n+\n+\/**\n+ * @test\n+ * @bug 8276970\n+ * @summary Test to verify the charset in PrintStream is inherited\n+ *      in the OutputStreamWriter\/PrintWriter\n+ * @run testng InheritEncodingTest\n+ *\/\n+@Test\n+public class InheritEncodingTest {\n+\n+    private static final String testString = \"\\u00e9\\u3042\"; \/\/ \"\"\n+\n+    @DataProvider\n+    public Object[][] encodings() {\n+        return new Object[][]{\n+                {StandardCharsets.ISO_8859_1},\n+                {StandardCharsets.US_ASCII},\n+                {StandardCharsets.UTF_8},\n+                {StandardCharsets.UTF_16},\n+                {StandardCharsets.UTF_16BE},\n+                {StandardCharsets.UTF_16LE},\n+        };\n+    }\n+\n+    @Test (dataProvider = \"encodings\")\n+    public void testOutputStreamWriter(Charset stdCharset) throws IOException {\n+        var ba = new ByteArrayOutputStream();\n+        var ps = new PrintStream(ba, true, stdCharset);\n+        var expected = new String(testString.getBytes(stdCharset), stdCharset);\n+\n+        \/\/ tests OutputStreamWriter's encoding explicitly\n+        var osw = new OutputStreamWriter(ps);\n+        assertEquals(Charset.forName(osw.getEncoding()), stdCharset);\n+\n+        \/\/ tests roundtrip result\n+        osw.write(testString);\n+        osw.flush();\n+        var result = ba.toString(stdCharset);\n+        assertEquals(result, expected);\n+    }\n+\n+    @Test (dataProvider = \"encodings\")\n+    public void testPrintWriter(Charset stdCharset) throws IOException {\n+        var ba = new ByteArrayOutputStream();\n+        var ps = new PrintStream(ba, true, stdCharset);\n+        var expected = new String(testString.getBytes(stdCharset), stdCharset);\n+\n+        \/\/ tests roundtrip result\n+        var pw = new PrintWriter(ps);\n+        pw.write(testString);\n+        pw.flush();\n+        var result = ba.toString(stdCharset);\n+        assertEquals(result, expected);\n+    }\n+\n+    @Test (dataProvider = \"encodings\")\n+    public void testPrintStream(Charset stdCharset) throws IOException {\n+        var ba = new ByteArrayOutputStream();\n+        var ps = new PrintStream(ba, true, stdCharset);\n+        var expected = new String(testString.getBytes(stdCharset), stdCharset);\n+\n+        \/\/ tests PrintStream's charset explicitly\n+        var psWrapper = new PrintStream(ps);\n+        assertEquals(psWrapper.charset(), stdCharset);\n+\n+        \/\/ tests roundtrip result\n+        psWrapper.print(testString);\n+        psWrapper.flush();\n+        var result = ba.toString(stdCharset);\n+        assertEquals(result, expected);\n+    }\n+}\n","filename":"test\/jdk\/java\/io\/PrintStream\/InheritEncodingTest.java","additions":107,"deletions":0,"binary":false,"changes":107,"status":"added"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2018, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2018, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -26,1 +26,1 @@\n- * @bug 8203433\n+ * @bug 8203433 8276559\n@@ -51,1 +51,0 @@\n-import javax.net.ServerSocketFactory;\n@@ -55,4 +54,0 @@\n-import java.io.OutputStream;\n-import java.io.OutputStreamWriter;\n-import java.io.PrintWriter;\n-import java.io.Writer;\n@@ -61,2 +56,0 @@\n-import java.net.ServerSocket;\n-import java.net.Socket;\n@@ -69,13 +62,0 @@\n-import java.util.ArrayList;\n-import java.util.Arrays;\n-import java.util.Collections;\n-import java.util.HashMap;\n-import java.util.List;\n-import java.util.Locale;\n-import java.util.Map;\n-import java.util.StringTokenizer;\n-import java.util.concurrent.ConcurrentHashMap;\n-import java.util.concurrent.ConcurrentLinkedQueue;\n-import java.util.concurrent.atomic.AtomicLong;\n-import java.util.stream.Collectors;\n-import java.util.stream.Stream;\n@@ -84,2 +64,0 @@\n-import static java.nio.charset.StandardCharsets.UTF_8;\n-import static java.net.HttpURLConnection.HTTP_OK;\n@@ -87,1 +65,0 @@\n-import static org.testng.Assert.assertTrue;\n@@ -101,2 +78,0 @@\n-    static final String MESSAGE = \"Basic HeadTest message body\";\n-    static final int ITERATIONS = 3;\n@@ -136,2 +111,0 @@\n-    static final AtomicLong requestCounter = new AtomicLong();\n-\n@@ -142,5 +115,0 @@\n-        HttpClient client = HttpClient.newBuilder()\n-                .followRedirects(Redirect.ALWAYS)\n-                .sslContext(sslContext)\n-                .build();\n-\n@@ -148,1 +116,0 @@\n-\n@@ -152,1 +119,0 @@\n-\n@@ -156,1 +122,21 @@\n-        HttpRequest request = requestBuilder.build();\n+        doTest(requestBuilder.build(), expResp);\n+        \/\/ repeat the test this time by building the request using convenience\n+        \/\/ GET and HEAD methods\n+        requestBuilder = HttpRequest.newBuilder(uri);\n+        if (version != null) {\n+            requestBuilder.version(version);\n+        }\n+        switch (method) {\n+            case \"GET\" -> requestBuilder.GET();\n+            case \"HEAD\" -> requestBuilder.HEAD();\n+            default -> throw new IllegalArgumentException(\"Unexpected method \" + method);\n+        }\n+        doTest(requestBuilder.build(), expResp);\n+    }\n+\n+    \/\/ issue a request with no body and verify the response code is the expected response code\n+    private void doTest(HttpRequest request, int expResp) throws Exception {\n+        HttpClient client = HttpClient.newBuilder()\n+                .followRedirects(Redirect.ALWAYS)\n+                .sslContext(sslContext)\n+                .build();\n","filename":"test\/jdk\/java\/net\/httpclient\/HeadTest.java","additions":23,"deletions":37,"binary":false,"changes":60,"status":"modified"},{"patch":"@@ -39,1 +39,1 @@\n- * @bug 8170064\n+ * @bug 8170064 8276559\n@@ -159,0 +159,1 @@\n+        test0(\"HEAD\", () -> HttpRequest.newBuilder(TEST_URI).HEAD().build(), null);\n@@ -257,1 +258,3 @@\n-\n+        method(\"newBuilder(TEST_URI).HEAD().build().method() == HEAD\",\n+                () -> HttpRequest.newBuilder(TEST_URI).HEAD(),\n+                \"HEAD\");\n","filename":"test\/jdk\/java\/net\/httpclient\/HttpRequestBuilderTest.java","additions":5,"deletions":2,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n-* Copyright (c) 2020, Oracle and\/or its affiliates. All rights reserved.\n+* Copyright (c) 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -54,1 +54,1 @@\n-* @bug 8252304\n+* @bug 8252304 8276559\n@@ -123,0 +123,1 @@\n+                { HttpRequest.newBuilder(URI.create(\"https:\/\/method-0\/\")).HEAD().build() },\n","filename":"test\/jdk\/java\/net\/httpclient\/HttpRequestNewBuilderTest.java","additions":3,"deletions":2,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2017, 2018, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2017, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -26,0 +26,1 @@\n+ * @bug 8276559\n@@ -163,0 +164,4 @@\n+        request = newBuilder(uri).HEAD().build();\n+        assertEquals(request.method(), \"HEAD\");\n+        assertFalse(request.bodyPublisher().isPresent());\n+\n","filename":"test\/jdk\/java\/net\/httpclient\/RequestBuilderTest.java","additions":6,"deletions":1,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -0,0 +1,104 @@\n+\/*\n+ * Copyright (c) 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+import java.net.InetAddress;\n+import java.net.UnknownHostException;\n+\n+import org.testng.Assert;\n+import org.testng.annotations.Test;\n+\n+import impl.SimpleResolverProviderImpl;\n+\n+\n+\/*\n+ * @test\n+ * @summary Test that InetAddress caching security properties work as expected\n+ *  when a custom resolver is installed.\n+ * @library lib providers\/simple\n+ * @build test.library\/testlib.ResolutionRegistry\n+ *  simple.provider\/impl.SimpleResolverProviderImpl AddressesCachingTest\n+ * @run testng\/othervm -Djava.security.properties=${test.src}\/NeverCache.props\n+ *  -Dtest.cachingDisabled=true AddressesCachingTest\n+ * @run testng\/othervm -Djava.security.properties=${test.src}\/ForeverCache.props\n+ *  -Dtest.cachingDisabled=false AddressesCachingTest\n+ *\/\n+public class AddressesCachingTest {\n+\n+    @Test\n+    public void testPositiveCaching() {\n+        boolean observedTwoLookups = performLookups(false);\n+        if (CACHING_DISABLED) {\n+            Assert.assertTrue(observedTwoLookups,\n+                    \"Two positive lookups are expected with caching disabled\");\n+        } else {\n+            Assert.assertFalse(observedTwoLookups,\n+                    \"Only one positive lookup is expected with caching enabled\");\n+        }\n+    }\n+\n+    @Test\n+    public void testNegativeCaching() {\n+        boolean observedTwoLookups = performLookups(true);\n+        if (CACHING_DISABLED) {\n+            Assert.assertTrue(observedTwoLookups,\n+                    \"Two negative lookups are expected with caching disabled\");\n+        } else {\n+            Assert.assertFalse(observedTwoLookups,\n+                    \"Only one negative lookup is expected with caching enabled\");\n+        }\n+    }\n+\n+    \/*\n+     * Performs two subsequent positive or negative lookups.\n+     * Returns true if the timestamp of this lookups differs,\n+     * false otherwise.\n+     *\/\n+    private static boolean performLookups(boolean performNegativeLookup) {\n+        doLookup(performNegativeLookup);\n+        long firstTimestamp = SimpleResolverProviderImpl.getLastLookupTimestamp();\n+        doLookup(performNegativeLookup);\n+        long secondTimestamp = SimpleResolverProviderImpl.getLastLookupTimestamp();\n+        return firstTimestamp != secondTimestamp;\n+    }\n+\n+    \/\/ Performs negative or positive lookup.\n+    \/\/ It is a test error if UnknownHostException is thrown during positive lookup.\n+    \/\/ It is a test error if UnknownHostException is NOT thrown during negative lookup.\n+    private static void doLookup(boolean performNegativeLookup) {\n+        String hostName = performNegativeLookup ? \"notKnowHost.org\" : \"javaTest.org\";\n+        try {\n+            InetAddress.getByName(hostName);\n+            if (performNegativeLookup) {\n+                Assert.fail(\"Host name is expected to get unresolved\");\n+            }\n+        } catch (UnknownHostException uhe) {\n+            if (!performNegativeLookup) {\n+                Assert.fail(\"Host name is expected to get resolved\");\n+            }\n+        }\n+    }\n+\n+    \/\/ Helper system property that signals to the test if both negative and positive\n+    \/\/ caches are disabled.\n+    private static final boolean CACHING_DISABLED = Boolean.getBoolean(\"test.cachingDisabled\");\n+}\n","filename":"test\/jdk\/java\/net\/spi\/InetAddressResolverProvider\/AddressesCachingTest.java","additions":104,"deletions":0,"binary":false,"changes":104,"status":"added"},{"patch":"@@ -0,0 +1,49 @@\n+\/*\n+ * Copyright (c) 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+import java.net.InetAddress;\n+\n+import org.testng.Assert;\n+import org.testng.annotations.Test;\n+\n+import static impl.WithBootstrapResolverUsageProvider.numberOfGetCalls;\n+\n+\/**\n+ * @test\n+ * @summary Test that InetAddress class properly avoids stack-overflow by\n+ * correctly tracking the bootstrap resolver instance when\n+ * InetAddressResolverProvider.get method uses InetAddress lookup API.\n+ * @library providers\/bootstrapUsage\n+ * @build bootstrap.usage.provider\/impl.WithBootstrapResolverUsageProvider\n+ * @run testng\/othervm BootstrapResolverUsageTest\n+ *\/\n+\n+public class BootstrapResolverUsageTest {\n+\n+    @Test\n+    public void testSuccessfulProviderInstantiationTest() throws Exception {\n+        System.err.println(InetAddress.getAllByName(InetAddress.getLocalHost().getHostName()));\n+        Assert.assertEquals(numberOfGetCalls, 1,\n+                \"InetAddressResolverProvider.get was called more than once\");\n+    }\n+}\n","filename":"test\/jdk\/java\/net\/spi\/InetAddressResolverProvider\/BootstrapResolverUsageTest.java","additions":49,"deletions":0,"binary":false,"changes":49,"status":"added"},{"patch":"@@ -0,0 +1,79 @@\n+\/*\n+ * Copyright (c) 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+import org.testng.annotations.BeforeTest;\n+import org.testng.annotations.Test;\n+\n+import java.lang.reflect.Field;\n+import java.net.InetAddress;\n+import java.net.UnknownHostException;\n+import java.net.spi.InetAddressResolver;\n+\n+import static org.testng.Assert.*;\n+\n+\/*\n+ * @test\n+ * @summary white-box test to check that the built-in resolver\n+ *  is used by default.\n+ * @modules java.base\/java.net:open\n+ * @run testng\/othervm BuiltInResolverTest\n+ *\/\n+\n+public class BuiltInResolverTest {\n+\n+    private Field builtInResolverField, resolverField;\n+\n+    @BeforeTest\n+    public void beforeTest() throws NoSuchFieldException {\n+        Class<InetAddress> inetAddressClass = InetAddress.class;\n+        \/\/ Needs to happen for InetAddress.resolver to be initialized\n+        try {\n+            InetAddress.getByName(\"test\");\n+        } catch (UnknownHostException e) {\n+            \/\/ Do nothing, only want to assign resolver\n+        }\n+        builtInResolverField = inetAddressClass.getDeclaredField(\"BUILTIN_RESOLVER\");\n+        builtInResolverField.setAccessible(true);\n+        resolverField = inetAddressClass.getDeclaredField(\"resolver\");\n+        resolverField.setAccessible(true);\n+    }\n+\n+    @Test\n+    public void testDefaultNSContext() throws IllegalAccessException {\n+        \/\/ Test that the resolver used by default is the BUILTIN_RESOLVER\n+        Object defaultResolverObject = builtInResolverField.get(InetAddressResolver.class);\n+        Object usedResolverObject = resolverField.get(InetAddressResolver.class);\n+\n+        assertTrue(defaultResolverObject == usedResolverObject);\n+\n+        String defaultClassName = defaultResolverObject.getClass().getCanonicalName();\n+        String currentClassName = usedResolverObject.getClass().getCanonicalName();\n+\n+        assertNotNull(defaultClassName, \"defaultClassName not set\");\n+        assertNotNull(currentClassName, \"currentClassName name not set\");\n+\n+        assertEquals(currentClassName, defaultClassName,\n+                \"BUILTIN_RESOLVER resolver was not used.\");\n+        System.err.println(\"Resolver used by default is the built-in resolver\");\n+    }\n+}\n","filename":"test\/jdk\/java\/net\/spi\/InetAddressResolverProvider\/BuiltInResolverTest.java","additions":79,"deletions":0,"binary":false,"changes":79,"status":"added"},{"patch":"@@ -0,0 +1,52 @@\n+\/*\n+ * Copyright (c) 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+import org.testng.Assert;\n+import org.testng.annotations.Test;\n+\n+import java.net.InetAddress;\n+import java.net.UnknownHostException;\n+import java.util.Arrays;\n+\n+\/*\n+ * @test\n+ * @summary checks that InetAddress forward lookup API throw UnknownHostException\n+ *   when resolver returns empty address stream.\n+ * @library providers\/empty\n+ * @build empty.results.provider\/impl.EmptyResultsProviderImpl\n+ * @run testng\/othervm EmptyResultsStreamTest\n+ *\/\n+public class EmptyResultsStreamTest {\n+\n+    @Test(expectedExceptions = UnknownHostException.class)\n+    public void getAllByNameTest() throws UnknownHostException {\n+        System.err.println(\"getAllByName unexpectedly completed: \" +\n+                Arrays.deepToString(InetAddress.getAllByName(\"test1.org\")));\n+    }\n+\n+    @Test(expectedExceptions = UnknownHostException.class)\n+    public void getByNameTest() throws UnknownHostException {\n+        System.err.println(\"getByName unexpectedly completed: \" +\n+                InetAddress.getByName(\"test2.org\"));\n+    }\n+}\n","filename":"test\/jdk\/java\/net\/spi\/InetAddressResolverProvider\/EmptyResultsStreamTest.java","additions":52,"deletions":0,"binary":false,"changes":52,"status":"added"},{"patch":"@@ -0,0 +1,2 @@\n+networkaddress.cache.ttl=-1\n+networkaddress.cache.negative.ttl=-1\n","filename":"test\/jdk\/java\/net\/spi\/InetAddressResolverProvider\/ForeverCache.props","additions":2,"deletions":0,"binary":false,"changes":2,"status":"added"},{"patch":"@@ -0,0 +1,44 @@\n+\/*\n+ * Copyright (c) 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+import org.testng.Assert;\n+import org.testng.annotations.Test;\n+\n+import java.net.InetAddress;\n+\n+\/**\n+ * @test\n+ * @summary Test that provider which uses InetAddress APIs during its initialization\n+ * wouldn't cause stack overflow and will be successfully installed.\n+ * @library providers\/recursive\n+ * @build recursive.init.provider\/impl.InetAddressUsageInGetProviderImpl\n+ * @run testng\/othervm InetAddressUsageInGetProviderTest\n+ *\/\n+\n+public class InetAddressUsageInGetProviderTest {\n+\n+    @Test\n+    public void testSuccessfulProviderInstantiationTest() throws Exception {\n+        System.err.println(InetAddress.getAllByName(InetAddress.getLocalHost().getHostName()));\n+    }\n+}\n","filename":"test\/jdk\/java\/net\/spi\/InetAddressResolverProvider\/InetAddressUsageInGetProviderTest.java","additions":44,"deletions":0,"binary":false,"changes":44,"status":"added"},{"patch":"@@ -0,0 +1,172 @@\n+\/*\n+ * Copyright (c) 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+import java.net.InetAddress;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+\n+import static java.net.spi.InetAddressResolver.LookupPolicy.IPV4;\n+import static java.net.spi.InetAddressResolver.LookupPolicy.IPV4_FIRST;\n+import static java.net.spi.InetAddressResolver.LookupPolicy.IPV6;\n+import static java.net.spi.InetAddressResolver.LookupPolicy.IPV6_FIRST;\n+\n+import jdk.test.lib.net.IPSupport;\n+import jdk.test.lib.NetworkConfiguration;\n+import org.testng.annotations.Test;\n+import org.testng.Assert;\n+import org.testng.SkipException;\n+\n+\/*\n+ * @test\n+ * @summary Test that platform lookup characteristic value is correctly initialized from\n+ *  system properties affecting order and type of queried addresses.\n+ * @library lib providers\/simple \/test\/lib\n+ * @build test.library\/testlib.ResolutionRegistry simple.provider\/impl.SimpleResolverProviderImpl\n+ *        jdk.test.lib.net.IPSupport LookupPolicyMappingTest\n+ * @run testng\/othervm LookupPolicyMappingTest\n+ * @run testng\/othervm -Djava.net.preferIPv4Stack=true -Djava.net.preferIPv6Addresses=true LookupPolicyMappingTest\n+ * @run testng\/othervm -Djava.net.preferIPv4Stack=true -Djava.net.preferIPv6Addresses=false LookupPolicyMappingTest\n+ * @run testng\/othervm -Djava.net.preferIPv4Stack=true -Djava.net.preferIPv6Addresses=system LookupPolicyMappingTest\n+ * @run testng\/othervm -Djava.net.preferIPv4Stack=true -Djava.net.preferIPv6Addresses LookupPolicyMappingTest\n+ * @run testng\/othervm -Djava.net.preferIPv4Stack=true LookupPolicyMappingTest\n+ * @run testng\/othervm -Djava.net.preferIPv4Stack=false -Djava.net.preferIPv6Addresses=true LookupPolicyMappingTest\n+ * @run testng\/othervm -Djava.net.preferIPv4Stack=false -Djava.net.preferIPv6Addresses=false LookupPolicyMappingTest\n+ * @run testng\/othervm -Djava.net.preferIPv4Stack=false -Djava.net.preferIPv6Addresses=system LookupPolicyMappingTest\n+ * @run testng\/othervm -Djava.net.preferIPv4Stack=false -Djava.net.preferIPv6Addresses LookupPolicyMappingTest\n+ * @run testng\/othervm -Djava.net.preferIPv4Stack=false LookupPolicyMappingTest\n+ * @run testng\/othervm -Djava.net.preferIPv4Stack -Djava.net.preferIPv6Addresses=true LookupPolicyMappingTest\n+ * @run testng\/othervm -Djava.net.preferIPv4Stack -Djava.net.preferIPv6Addresses=false LookupPolicyMappingTest\n+ * @run testng\/othervm -Djava.net.preferIPv4Stack -Djava.net.preferIPv6Addresses=system LookupPolicyMappingTest\n+ * @run testng\/othervm -Djava.net.preferIPv4Stack -Djava.net.preferIPv6Addresses LookupPolicyMappingTest\n+ * @run testng\/othervm -Djava.net.preferIPv4Stack LookupPolicyMappingTest\n+ * @run testng\/othervm -Djava.net.preferIPv6Addresses=true LookupPolicyMappingTest\n+ * @run testng\/othervm -Djava.net.preferIPv6Addresses=false LookupPolicyMappingTest\n+ * @run testng\/othervm -Djava.net.preferIPv6Addresses=system LookupPolicyMappingTest\n+ * @run testng\/othervm -Djava.net.preferIPv6Addresses LookupPolicyMappingTest\n+ *\/\n+\n+public class LookupPolicyMappingTest {\n+\n+    @Test\n+    public void testSystemProperties() throws Exception {\n+\n+        \/\/ Check if platform network configuration matches the test requirements,\n+        \/\/ if not throw a SkipException\n+        checkPlatformNetworkConfiguration();\n+\n+        System.err.println(\"javaTest.org resolved to:\" + Arrays.deepToString(\n+                InetAddress.getAllByName(\"javaTest.org\")));\n+\n+        \/\/ Acquire runtime characteristics from the test NSP\n+        int runtimeCharacteristics = impl.SimpleResolverProviderImpl.lastLookupPolicy().characteristics();\n+\n+        \/\/ Calculate expected lookup policy characteristic\n+        String preferIPv4Stack = System.getProperty(\"java.net.preferIPv4Stack\");\n+        String preferIPv6Addresses = System.getProperty(\"java.net.preferIPv6Addresses\");\n+        String expectedResultsKey = calculateMapKey(preferIPv4Stack, preferIPv6Addresses);\n+        int expectedCharacteristics = EXPECTED_RESULTS_MAP.get(expectedResultsKey);\n+\n+        Assert.assertTrue(characteristicsMatch(\n+                runtimeCharacteristics, expectedCharacteristics), \"Unexpected LookupPolicy observed\");\n+    }\n+\n+    \/\/ Throws SkipException if platform doesn't support required IP address types\n+    static void checkPlatformNetworkConfiguration() {\n+        IPSupport.throwSkippedExceptionIfNonOperational();\n+        IPSupport.printPlatformSupport(System.err);\n+        NetworkConfiguration.printSystemConfiguration(System.err);\n+        \/\/ If preferIPv4=true and no IPv4 - skip\n+        if (IPSupport.preferIPv4Stack()) {\n+            if (!IPSupport.hasIPv4()) {\n+                throw new SkipException(\"Skip tests - IPv4 support required\");\n+            }\n+            return;\n+        }\n+    }\n+\n+    record ExpectedResult(String ipv4stack, String ipv6addresses, int characteristics) {\n+        ExpectedResult {\n+            if (!IPSupport.hasIPv4()) {\n+                characteristics = IPV6;\n+            } else if (!IPSupport.hasIPv6()) {\n+                characteristics = IPV4;\n+            }\n+        }\n+\n+        public String key() {\n+            return calculateMapKey(ipv4stack, ipv6addresses);\n+        }\n+    }\n+\n+    \/*\n+     *  Each row describes a combination of 'preferIPv4Stack', 'preferIPv6Addresses'\n+     *  values and the expected characteristic value\n+     *\/\n+    private static List<ExpectedResult> EXPECTED_RESULTS_TABLE = List.of(\n+            new ExpectedResult(\"true\", \"true\", IPV4),\n+            new ExpectedResult(\"true\", \"false\", IPV4),\n+            new ExpectedResult(\"true\", \"system\", IPV4),\n+            new ExpectedResult(\"true\", \"\", IPV4),\n+            new ExpectedResult(\"true\", null, IPV4),\n+\n+            new ExpectedResult(\"false\", \"true\", IPV4 | IPV6 | IPV6_FIRST),\n+            new ExpectedResult(\"false\", \"false\", IPV4 | IPV6 | IPV4_FIRST),\n+            new ExpectedResult(\"false\", \"system\", IPV4 | IPV6),\n+            new ExpectedResult(\"false\", \"\", IPV4 | IPV6 | IPV4_FIRST),\n+            new ExpectedResult(\"false\", null, IPV4 | IPV6 | IPV4_FIRST),\n+\n+            new ExpectedResult(\"\", \"true\", IPV4 | IPV6 | IPV6_FIRST),\n+            new ExpectedResult(\"\", \"false\", IPV4 | IPV6 | IPV4_FIRST),\n+            new ExpectedResult(\"\", \"system\", IPV4 | IPV6),\n+            new ExpectedResult(\"\", \"\", IPV4 | IPV6 | IPV4_FIRST),\n+            new ExpectedResult(\"\", null, IPV4 | IPV6 | IPV4_FIRST),\n+\n+            new ExpectedResult(null, \"true\", IPV4 | IPV6 | IPV6_FIRST),\n+            new ExpectedResult(null, \"false\", IPV4 | IPV6 | IPV4_FIRST),\n+            new ExpectedResult(null, \"system\", IPV4 | IPV6),\n+            new ExpectedResult(null, \"\", IPV4 | IPV6 | IPV4_FIRST),\n+            new ExpectedResult(null, null, IPV4 | IPV6 | IPV4_FIRST));\n+\n+    private static final Map<String, Integer> EXPECTED_RESULTS_MAP = calculateExpectedCharacteristics();\n+\n+    private static Map<String, Integer> calculateExpectedCharacteristics() {\n+        return EXPECTED_RESULTS_TABLE.stream()\n+                .collect(Collectors.toUnmodifiableMap(\n+                        ExpectedResult::key,\n+                        ExpectedResult::characteristics)\n+                );\n+    }\n+\n+    private static String calculateMapKey(String ipv4stack, String ipv6addresses) {\n+        return ipv4stack + \"_\" + ipv6addresses;\n+    }\n+\n+    private static boolean characteristicsMatch(int actual, int expected) {\n+        System.err.printf(\"Comparing characteristics:%n\\tActual: %s%n\\tExpected: %s%n\",\n+                Integer.toBinaryString(actual),\n+                Integer.toBinaryString(expected));\n+        return (actual & (IPV4 | IPV6 | IPV4_FIRST | IPV6_FIRST)) == expected;\n+    }\n+}\n","filename":"test\/jdk\/java\/net\/spi\/InetAddressResolverProvider\/LookupPolicyMappingTest.java","additions":172,"deletions":0,"binary":false,"changes":172,"status":"added"},{"patch":"@@ -0,0 +1,88 @@\n+\/*\n+ * Copyright (c) 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+\/*\n+ * @test\n+ * @summary check if LookupPolicy.of correctly handles valid and illegal\n+ * combinations of characteristics bit mask flags.\n+ * @run testng LookupPolicyOfTest\n+ *\/\n+\n+import org.testng.annotations.DataProvider;\n+import org.testng.annotations.Test;\n+\n+import java.net.spi.InetAddressResolver.LookupPolicy;\n+import java.util.List;\n+\n+import static java.net.spi.InetAddressResolver.LookupPolicy.IPV4;\n+import static java.net.spi.InetAddressResolver.LookupPolicy.IPV4_FIRST;\n+import static java.net.spi.InetAddressResolver.LookupPolicy.IPV6;\n+import static java.net.spi.InetAddressResolver.LookupPolicy.IPV6_FIRST;\n+\n+public class LookupPolicyOfTest {\n+\n+    @Test(dataProvider = \"validCharacteristics\")\n+    public void testValidCharacteristicCombinations(List<Integer> validCombination) {\n+        LookupPolicy.of(bitFlagsToCharacteristicsValue(validCombination));\n+    }\n+\n+    @Test(dataProvider = \"invalidCharacteristics\", expectedExceptions = IllegalArgumentException.class)\n+    public void testInvalidCharacteristicCombinations(List<Integer> invalidCombination) {\n+        LookupPolicy.of(bitFlagsToCharacteristicsValue(invalidCombination));\n+    }\n+\n+    @DataProvider(name = \"validCharacteristics\")\n+    public Object[][] validCharacteristicValue() {\n+        return new Object[][]{\n+                {List.of(IPV4)},\n+                {List.of(IPV4, IPV4_FIRST)},\n+                {List.of(IPV6)},\n+                {List.of(IPV6, IPV6_FIRST)},\n+                {List.of(IPV4, IPV6)},\n+                {List.of(IPV4, IPV6, IPV4_FIRST)},\n+                {List.of(IPV4, IPV6, IPV6_FIRST)},\n+                \/\/ Custom flag values alongside to address type flags\n+                \/\/ that could be used by custom providers\n+                {List.of(IPV4, IPV6, 0x10)},\n+                {List.of(IPV4, IPV6, 0x20)},\n+        };\n+    }\n+\n+    @DataProvider(name = \"invalidCharacteristics\")\n+    public Object[][] illegalCharacteristicValue() {\n+        return new Object[][]{\n+                {List.of()},\n+                {List.of(IPV4_FIRST)},\n+                {List.of(IPV6_FIRST)},\n+                {List.of(IPV4_FIRST, IPV6_FIRST)},\n+                {List.of(IPV4, IPV6_FIRST)},\n+                {List.of(IPV6, IPV4_FIRST)},\n+                {List.of(IPV4, IPV6, IPV4_FIRST, IPV6_FIRST)},\n+        };\n+    }\n+\n+    private static int bitFlagsToCharacteristicsValue(List<Integer> bitFlagsList) {\n+        return bitFlagsList.stream()\n+                .reduce(0, (flag1, flag2) -> flag1 | flag2);\n+    }\n+}\n","filename":"test\/jdk\/java\/net\/spi\/InetAddressResolverProvider\/LookupPolicyOfTest.java","additions":88,"deletions":0,"binary":false,"changes":88,"status":"added"},{"patch":"@@ -0,0 +1,2 @@\n+networkaddress.cache.ttl=0\n+networkaddress.cache.negative.ttl=0\n","filename":"test\/jdk\/java\/net\/spi\/InetAddressResolverProvider\/NeverCache.props","additions":2,"deletions":0,"binary":false,"changes":2,"status":"added"},{"patch":"@@ -0,0 +1,63 @@\n+\/*\n+ * Copyright (c) 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+import java.net.InetAddress;\n+import java.util.Arrays;\n+\n+import org.testng.Assert;\n+import org.testng.annotations.Test;\n+\n+import static impl.FaultyResolverProviderGetImpl.EXCEPTION_MESSAGE;\n+\n+\/*\n+ * @test\n+ * @summary Test that InetAddress fast-fails if custom provider fails to\n+ *  instantiate a resolver.\n+ * @library providers\/faulty\n+ * @build faulty.provider\/impl.FaultyResolverProviderGetImpl\n+ * @run testng\/othervm ProviderGetExceptionTest\n+ *\/\n+\n+public class ProviderGetExceptionTest {\n+\n+    @Test\n+    public void getByNameExceptionTest() {\n+        String hostName = \"test.host\";\n+        System.out.println(\"Looking up address for the following host name:\" + hostName);\n+        callInetAddressAndCheckException(() -> InetAddress.getByName(hostName));\n+    }\n+\n+    @Test\n+    public void getByAddressExceptionTest() {\n+        byte[] address = new byte[]{1, 2, 3, 4};\n+        System.out.println(\"Looking up host name for the following address:\" + Arrays.toString(address));\n+        callInetAddressAndCheckException(() -> InetAddress.getByAddress(address).getHostName());\n+    }\n+\n+    private void callInetAddressAndCheckException(Assert.ThrowingRunnable apiCall) {\n+        IllegalArgumentException iae = Assert.expectThrows(IllegalArgumentException.class, apiCall);\n+        System.out.println(\"Got exception of expected type:\" + iae);\n+        Assert.assertNull(iae.getCause(), \"cause is not null\");\n+        Assert.assertEquals(iae.getMessage(), EXCEPTION_MESSAGE);\n+    }\n+}\n","filename":"test\/jdk\/java\/net\/spi\/InetAddressResolverProvider\/ProviderGetExceptionTest.java","additions":63,"deletions":0,"binary":false,"changes":63,"status":"added"},{"patch":"@@ -0,0 +1,94 @@\n+\/*\n+ * Copyright (c) 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+import java.net.InetAddress;\n+import java.net.UnknownHostException;\n+\n+import impl.ThrowingLookupsProviderImpl;\n+\n+import static impl.ThrowingLookupsProviderImpl.RUNTIME_EXCEPTION_MESSAGE;\n+\n+import org.testng.Assert;\n+import org.testng.annotations.Test;\n+\n+\/*\n+ * @test\n+ * @summary Test that only UnknownHostException is thrown if resolver\n+ * implementation throws RuntimeException during forward or reverse lookup.\n+ * @library providers\/throwing\n+ * @build throwing.lookups.provider\/impl.ThrowingLookupsProviderImpl\n+ * @run testng\/othervm ResolutionWithExceptionTest\n+ *\/\n+\n+public class ResolutionWithExceptionTest {\n+\n+    @Test\n+    public void getByNameUnknownHostException() {\n+        ThrowingLookupsProviderImpl.throwRuntimeException = false;\n+        runGetByNameTest();\n+    }\n+\n+    @Test\n+    public void getByNameRuntimeException() {\n+        ThrowingLookupsProviderImpl.throwRuntimeException = true;\n+        runGetByNameTest();\n+    }\n+\n+    @Test\n+    public void getByAddressUnknownHostException() throws UnknownHostException {\n+        ThrowingLookupsProviderImpl.throwRuntimeException = false;\n+        runGetByAddressTest();\n+    }\n+\n+    @Test\n+    public void getByAddressRuntimeException() throws UnknownHostException {\n+        ThrowingLookupsProviderImpl.throwRuntimeException = true;\n+        runGetByAddressTest();\n+    }\n+\n+    private void runGetByNameTest() {\n+        \/\/ InetAddress.getByName() is expected to throw UnknownHostException in all cases\n+        UnknownHostException uhe = Assert.expectThrows(UnknownHostException.class,\n+                () -> InetAddress.getByName(\"doesnt.matter.com\"));\n+        \/\/ If provider is expected to throw RuntimeException - check that UnknownHostException\n+        \/\/ is set as its cause\n+        if (ThrowingLookupsProviderImpl.throwRuntimeException) {\n+            Throwable cause = uhe.getCause();\n+            if (cause instanceof RuntimeException re) {\n+                \/\/ Check RuntimeException message\n+                Assert.assertEquals(re.getMessage(), RUNTIME_EXCEPTION_MESSAGE,\n+                        \"incorrect exception message\");\n+            } else {\n+                Assert.fail(\"UnknownHostException cause is not RuntimeException\");\n+            }\n+        }\n+    }\n+\n+    private void runGetByAddressTest() throws UnknownHostException {\n+        \/\/ getCanonicalHostName is not expected to throw an exception:\n+        \/\/ if there is an error during reverse lookup operation the literal IP\n+        \/\/ address String will be returned.\n+        String literalIP = InetAddress.getByAddress(new byte[]{1, 2, 3, 4}).getCanonicalHostName();\n+        Assert.assertEquals(literalIP, \"1.2.3.4\");\n+    }\n+}\n","filename":"test\/jdk\/java\/net\/spi\/InetAddressResolverProvider\/ResolutionWithExceptionTest.java","additions":94,"deletions":0,"binary":false,"changes":94,"status":"added"},{"patch":"@@ -0,0 +1,95 @@\n+\/*\n+ * Copyright (c) 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+import java.net.InetAddress;\n+import java.net.SocketPermission;\n+import java.net.UnknownHostException;\n+import java.security.Permission;\n+import java.util.logging.Logger;\n+\n+import org.testng.Assert;\n+import org.testng.annotations.Test;\n+\n+\/*\n+ * @test\n+ * @summary Test that resolution of host name requires SocketPermission(\"resolve\", <host name>)\n+ * permission when running with security manager and custom resolver provider installed.\n+ * @library lib providers\/simple\n+ * @build test.library\/testlib.ResolutionRegistry simple.provider\/impl.SimpleResolverProviderImpl\n+ *        ResolvePermissionTest\n+ * @run testng\/othervm -Dtest.dataFileName=nonExistentFile -Djava.security.manager=allow\n+ *                      ResolvePermissionTest\n+ *\/\n+\n+public class ResolvePermissionTest {\n+\n+    @Test\n+    public void withResolvePermission() throws Exception {\n+        testResolvePermission(true);\n+    }\n+\n+    @Test\n+    public void noResolvePermission() throws Exception {\n+        testResolvePermission(false);\n+    }\n+\n+    @SuppressWarnings(\"removal\")\n+    private void testResolvePermission(boolean grantResolvePermission) throws Exception {\n+        \/\/ Set security manager which grants or denies permission to resolve 'javaTest.org' host\n+        var securityManager = new ResolvePermissionTest.TestSecurityManager(grantResolvePermission);\n+        try {\n+            System.setSecurityManager(securityManager);\n+            Class expectedExceptionClass = grantResolvePermission ?\n+                    UnknownHostException.class : SecurityException.class;\n+            var exception = Assert.expectThrows(expectedExceptionClass, () -> InetAddress.getByName(\"javaTest.org\"));\n+            LOGGER.info(\"Got expected exception: \" + exception);\n+        } finally {\n+            System.setSecurityManager(null);\n+        }\n+    }\n+\n+    static class TestSecurityManager extends SecurityManager {\n+        final boolean allowJavaTestOrgResolve;\n+\n+        public TestSecurityManager(boolean allowJavaTestOrgResolve) {\n+            this.allowJavaTestOrgResolve = allowJavaTestOrgResolve;\n+        }\n+\n+        @Override\n+        public void checkPermission(Permission permission) {\n+            if (permission instanceof java.net.SocketPermission) {\n+                SocketPermission sockPerm = (SocketPermission) permission;\n+                if (\"resolve\".equals(sockPerm.getActions())) {\n+                    String host = sockPerm.getName();\n+                    LOGGER.info(\"Checking 'resolve' SocketPermission: \" + permission);\n+                    if (\"javaTest.org\".equals(host) && !allowJavaTestOrgResolve) {\n+                        LOGGER.info(\"Denying 'resolve' permission for 'javaTest.org'\");\n+                        throw new SecurityException(\"Access Denied\");\n+                    }\n+                }\n+            }\n+        }\n+    }\n+\n+    private static final Logger LOGGER = Logger.getLogger(ResolvePermissionTest.class.getName());\n+}\n","filename":"test\/jdk\/java\/net\/spi\/InetAddressResolverProvider\/ResolvePermissionTest.java","additions":95,"deletions":0,"binary":false,"changes":95,"status":"added"},{"patch":"@@ -0,0 +1,62 @@\n+\/*\n+ * Copyright (c) 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+import impl.DelegatingProviderImpl;\n+import org.testng.Assert;\n+import org.testng.annotations.Test;\n+\n+import java.net.InetAddress;\n+import java.net.UnknownHostException;\n+\n+import static impl.DelegatingProviderImpl.changeReverseLookupAddress;\n+import static impl.DelegatingProviderImpl.lastReverseLookupThrowable;\n+\n+\/*\n+ * @test\n+ * @summary checks delegation of illegal reverse lookup request to the built-in\n+ *  InetAddressResolver.\n+ * @library providers\/delegating\n+ * @build delegating.provider\/impl.DelegatingProviderImpl\n+ * @run testng\/othervm ReverseLookupDelegationTest\n+ *\/\n+public class ReverseLookupDelegationTest {\n+\n+    @Test\n+    public void delegateHostNameLookupWithWrongByteArray() throws UnknownHostException {\n+        \/\/ The underlying resolver implementation will ignore the supplied\n+        \/\/ byte array and will replace it with byte array of incorrect size.\n+        changeReverseLookupAddress = true;\n+        String canonicalHostName = InetAddress.getByAddress(new byte[]{1, 2, 3, 4}).getCanonicalHostName();\n+        \/\/ Output canonical host name and the exception thrown by the built-in resolver\n+        System.err.println(\"Canonical host name:\" + canonicalHostName);\n+        System.err.println(\"Exception thrown by the built-in resolver:\" + lastReverseLookupThrowable);\n+\n+        \/\/ Check that originally supplied byte array was used to construct canonical host name after\n+        \/\/ failed reverse lookup.\n+        Assert.assertEquals(\"1.2.3.4\", canonicalHostName, \"unexpected canonical hostname\");\n+\n+        \/\/ Check that on a provider side the IllegalArgumentException has been thrown by the built-in resolver\n+        Assert.assertTrue(lastReverseLookupThrowable instanceof IllegalArgumentException,\n+                \"wrong exception type is thrown by the built-in resolver\");\n+    }\n+}\n","filename":"test\/jdk\/java\/net\/spi\/InetAddressResolverProvider\/ReverseLookupDelegationTest.java","additions":62,"deletions":0,"binary":false,"changes":62,"status":"added"},{"patch":"@@ -0,0 +1,100 @@\n+\/*\n+ * Copyright (c) 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+import org.testng.Assert;\n+import org.testng.annotations.Test;\n+\n+import java.net.InetAddress;\n+import java.security.Permission;\n+import java.util.ServiceConfigurationError;\n+import java.util.logging.Logger;\n+\n+\/*\n+ * @test\n+ * @summary Test that instantiation of InetAddressResolverProvider requires \"inetAddressResolverProvider\"\n+ *          RuntimePermission when running with security manager.\n+ * @library lib providers\/simple\n+ * @build test.library\/testlib.ResolutionRegistry simple.provider\/impl.SimpleResolverProviderImpl\n+ *        RuntimePermissionTest\n+ * @run testng\/othervm -Djava.security.manager=allow RuntimePermissionTest\n+ *\/\n+\n+public class RuntimePermissionTest {\n+\n+    @Test\n+    public void withRuntimePermission() throws Exception {\n+        testRuntimePermission(true);\n+    }\n+\n+    @Test\n+    public void noRuntimePermission() throws Exception {\n+        testRuntimePermission(false);\n+    }\n+\n+    @SuppressWarnings(\"removal\")\n+    private void testRuntimePermission(boolean permitInetAddressResolver) throws Exception {\n+        \/\/ Set security manager which grants all permissions + RuntimePermission(\"inetAddressResolverProvider\")\n+        var securityManager = new TestSecurityManager(permitInetAddressResolver);\n+        try {\n+            System.setSecurityManager(securityManager);\n+            if (permitInetAddressResolver) {\n+                InetAddress.getByName(\"javaTest.org\");\n+            } else {\n+                ServiceConfigurationError sce =\n+                        Assert.expectThrows(ServiceConfigurationError.class,\n+                                            () -> InetAddress.getByName(\"javaTest.org\"));\n+                LOGGER.info(\"Got ServiceConfigurationError: \" + sce);\n+                Throwable cause = sce.getCause();\n+                Assert.assertTrue(cause instanceof SecurityException);\n+                Assert.assertTrue(cause.getMessage().contains(RUNTIME_PERMISSION_NAME));\n+            }\n+        } finally {\n+            System.setSecurityManager(null);\n+        }\n+    }\n+\n+    static class TestSecurityManager extends SecurityManager {\n+        final boolean permitInetAddressResolver;\n+\n+        public TestSecurityManager(boolean permitInetAddressResolver) {\n+            this.permitInetAddressResolver = permitInetAddressResolver;\n+            LOGGER.info(\"inetAddressResolverProvider permission is \" +\n+                        (permitInetAddressResolver ? \"granted\" : \"not granted\"));\n+        }\n+\n+        @Override\n+        public void checkPermission(Permission permission) {\n+            if (permission instanceof RuntimePermission) {\n+                LOGGER.info(\"Checking RuntimePermission: \" + permission);\n+                if (RUNTIME_PERMISSION_NAME.equals(permission.getName()) && !permitInetAddressResolver) {\n+                    LOGGER.info(\"Denying '\" + RUNTIME_PERMISSION_NAME + \"' permission\");\n+                    throw new SecurityException(\"Access Denied: \" + RUNTIME_PERMISSION_NAME);\n+                }\n+            }\n+        }\n+    }\n+\n+    private static final String RUNTIME_PERMISSION_NAME = \"inetAddressResolverProvider\";\n+    private static final Logger LOGGER = Logger.getLogger(RuntimePermissionTest.class.getName());\n+\n+}\n","filename":"test\/jdk\/java\/net\/spi\/InetAddressResolverProvider\/RuntimePermissionTest.java","additions":100,"deletions":0,"binary":false,"changes":100,"status":"added"},{"patch":"@@ -0,0 +1,7 @@\n+# Test data file for InetAddressResolverProvider SPI tests\n+# Format: <IP address> <Host Name>\n+# If multiple IP addresses are required for host:\n+#    multiple lines could be added\n+\n+1.2.3.4    javaTest.org\n+[ca:fe:ba:be::1] javaTest.org\n","filename":"test\/jdk\/java\/net\/spi\/InetAddressResolverProvider\/addresses.txt","additions":7,"deletions":0,"binary":false,"changes":7,"status":"added"},{"patch":"@@ -0,0 +1,27 @@\n+\/*\n+ * Copyright (c) 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+module test.library {\n+    exports testlib;\n+    requires java.logging;\n+}\n","filename":"test\/jdk\/java\/net\/spi\/InetAddressResolverProvider\/lib\/test.library\/module-info.java","additions":27,"deletions":0,"binary":false,"changes":27,"status":"added"},{"patch":"@@ -0,0 +1,239 @@\n+\/*\n+ * Copyright (c) 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.  Oracle designates this\n+ * particular file as subject to the \"Classpath\" exception as provided\n+ * by Oracle in the LICENSE file that accompanied this code.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+package testlib;\n+\n+import java.io.IOException;\n+import java.net.InetAddress;\n+import java.net.UnknownHostException;\n+import java.net.spi.InetAddressResolver.LookupPolicy;\n+import java.nio.file.Files;\n+import java.nio.file.Path;\n+import java.nio.file.Paths;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.function.Predicate;\n+import java.util.logging.Level;\n+import java.util.logging.Logger;\n+import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n+import java.util.Comparator;\n+\n+import static java.net.spi.InetAddressResolver.LookupPolicy.*;\n+\n+public class ResolutionRegistry {\n+\n+    \/\/ Map to store hostName -> InetAddress mappings\n+    private final Map<String, List<byte[]>> registry;\n+    private static final int IPV4_RAW_LEN = 4;\n+    private static final int IPV6_RAW_LEN = 16;\n+\n+    private static final Logger LOGGER = Logger.getLogger(ResolutionRegistry.class.getName());\n+\n+    public ResolutionRegistry() {\n+\n+        \/\/ Populate registry from test data file\n+        String fileName = System.getProperty(\"test.dataFileName\", \"addresses.txt\");\n+        Path addressesFile = Paths.get(System.getProperty(\"test.src\", \".\")).resolve(fileName);\n+        LOGGER.info(\"Creating ResolutionRegistry instance from file:\" + addressesFile);\n+        registry = parseDataFile(addressesFile);\n+    }\n+\n+    private Map<String, List<byte[]>> parseDataFile(Path addressesFile) {\n+        try {\n+            if (addressesFile.toFile().isFile()) {\n+                Map<String, List<byte[]>> resReg = new ConcurrentHashMap<>();\n+                \/\/ Prepare list of hostname\/address entries\n+                List<String[]> entriesList = Files.readAllLines(addressesFile).stream()\n+                        .map(String::trim)\n+                        .filter(Predicate.not(String::isBlank))\n+                        .filter(s -> !s.startsWith(\"#\"))\n+                        .map(s -> s.split(\"\\\\s+\"))\n+                        .filter(sarray -> sarray.length == 2)\n+                        .filter(ResolutionRegistry::hasLiteralAddress)\n+                        .filter(Objects::nonNull)\n+                        .collect(Collectors.toList());\n+                \/\/ Convert list of entries into registry Map\n+                for (var entry : entriesList) {\n+                    String ipAddress = entry[0].trim();\n+                    String hostName = entry[1].trim();\n+                    byte[] addrBytes = toByteArray(ipAddress);\n+                    if (addrBytes != null) {\n+                        var list = resReg.containsKey(hostName) ? resReg.get(hostName) : new ArrayList();\n+                        list.add(addrBytes);\n+                        if (!resReg.containsKey(hostName)) {\n+                            resReg.put(hostName, list);\n+                        }\n+                    }\n+                }\n+                resReg.replaceAll((k, v) -> Collections.unmodifiableList(v));\n+                \/\/ Print constructed registry\n+                StringBuilder sb = new StringBuilder(\"Constructed addresses registry:\" + System.lineSeparator());\n+                for (var entry : resReg.entrySet()) {\n+                    sb.append(\"\\t\" + entry.getKey() + \": \");\n+                    for (byte[] addr : entry.getValue()) {\n+                        sb.append(addressBytesToString(addr) + \" \");\n+                    }\n+                    sb.append(System.lineSeparator());\n+                }\n+                LOGGER.info(sb.toString());\n+                return resReg;\n+            } else {\n+                \/\/ If file doesn't exist - return empty map\n+                return Collections.emptyMap();\n+            }\n+        } catch (IOException ioException) {\n+            \/\/ If any problems parsing the file - log a warning and return an empty map\n+            LOGGER.log(Level.WARNING, \"Error reading data file\", ioException);\n+            return Collections.emptyMap();\n+        }\n+    }\n+\n+    \/\/ Line is not a blank and not a comment\n+    private static boolean hasLiteralAddress(String[] lineFields) {\n+        String addressString = lineFields[0].trim();\n+        return addressString.charAt(0) == '[' ||\n+                Character.digit(addressString.charAt(0), 16) != -1 ||\n+                (addressString.charAt(0) == ':');\n+    }\n+\n+    \/\/ Line is not blank and not comment\n+    private static byte[] toByteArray(String addressString) {\n+        InetAddress address;\n+        \/\/ Will reuse InetAddress functionality to parse literal IP address\n+        \/\/ strings. This call is guarded by 'hasLiteralAddress' method.\n+        try {\n+            address = InetAddress.getByName(addressString);\n+        } catch (UnknownHostException unknownHostException) {\n+            LOGGER.warning(\"Can't parse address string:'\" + addressString + \"'\");\n+            return null;\n+        }\n+        return address.getAddress();\n+    }\n+\n+    public Stream<InetAddress> lookupHost(String host, LookupPolicy lookupPolicy)\n+            throws UnknownHostException {\n+        LOGGER.info(\"Looking-up '\" + host + \"' address\");\n+        if (!registry.containsKey(host)) {\n+            LOGGER.info(\"Registry doesn't contain addresses for '\" + host + \"'\");\n+            throw new UnknownHostException(host);\n+        }\n+\n+        int characteristics = lookupPolicy.characteristics();\n+        \/\/ Filter IPV4 or IPV6 as needed. Then sort with\n+        \/\/ comparator for IPV4_FIRST or IPV6_FIRST.\n+        return registry.get(host)\n+                .stream()\n+                .filter(ba -> filterAddressByLookupPolicy(ba, characteristics))\n+                .sorted(new AddressOrderPref(characteristics))\n+                .map(ba -> constructInetAddress(host, ba))\n+                .filter(Objects::nonNull);\n+    }\n+\n+    private static boolean filterAddressByLookupPolicy(byte[] ba, int ch) {\n+        \/\/ If 0011, return both. If 0001, IPv4. If 0010, IPv6\n+        boolean ipv4Flag = (ch & IPV4) == IPV4;\n+        boolean ipv6Flag = (ch & IPV6) == IPV6;\n+\n+        if (ipv4Flag && ipv6Flag)\n+            return true; \/\/ Return regardless of length\n+        else if (ipv4Flag)\n+            return (ba.length == IPV4_RAW_LEN);\n+        else if (ipv6Flag)\n+            return (ba.length == IPV6_RAW_LEN);\n+\n+        throw new RuntimeException(\"Lookup policy characteristics were improperly set. \" +\n+                \"Characteristics: \" + Integer.toString(ch, 2));\n+    }\n+\n+    private static InetAddress constructInetAddress(String host, byte[] address) {\n+        try {\n+            return InetAddress.getByAddress(host, address);\n+        } catch (UnknownHostException unknownHostException) {\n+            return null;\n+        }\n+    }\n+\n+    public String lookupAddress(byte[] addressBytes) {\n+        for (var entry : registry.entrySet()) {\n+            if (entry.getValue()\n+                    .stream()\n+                    .filter(ba -> Arrays.equals(ba, addressBytes))\n+                    .findAny()\n+                    .isPresent()) {\n+                return entry.getKey();\n+            }\n+        }\n+        try {\n+            return InetAddress.getByAddress(addressBytes).getHostAddress();\n+        } catch (UnknownHostException unknownHostException) {\n+            throw new IllegalArgumentException();\n+        }\n+    }\n+\n+    public boolean containsAddressMapping(InetAddress address) {\n+        String hostName = address.getHostName();\n+        if (registry.containsKey(hostName)) {\n+            var mappedBytes = registry.get(address.getHostName());\n+            for (byte[] mappedAddr : mappedBytes) {\n+                if (Arrays.equals(mappedAddr, address.getAddress())) {\n+                    return true;\n+                }\n+            }\n+        }\n+        return false;\n+    }\n+\n+    public static String addressBytesToString(byte[] bytes) {\n+        try {\n+            return InetAddress.getByAddress(bytes).toString();\n+        } catch (UnknownHostException unknownHostException) {\n+            return Arrays.toString(bytes);\n+        }\n+    }\n+\n+    private class AddressOrderPref implements Comparator<byte[]> {\n+\n+        private final int ch;\n+\n+        AddressOrderPref(int ch) {\n+            this.ch = ch;\n+        }\n+\n+        @Override\n+        public int compare(byte[] o1, byte[] o2) {\n+            \/\/ Compares based on address length, 4 bytes for IPv4,\n+            \/\/ 16 bytes for IPv6.\n+            return ((ch & IPV4_FIRST) == IPV4_FIRST) ?\n+                    Integer.compare(o1.length, o2.length) :\n+                    Integer.compare(o2.length, o1.length);\n+        }\n+    }\n+}\n","filename":"test\/jdk\/java\/net\/spi\/InetAddressResolverProvider\/lib\/test.library\/testlib\/ResolutionRegistry.java","additions":239,"deletions":0,"binary":false,"changes":239,"status":"added"},{"patch":"@@ -0,0 +1,76 @@\n+\/*\n+ * Copyright (c) 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+package impl;\n+\n+import java.net.InetAddress;\n+import java.net.UnknownHostException;\n+import java.net.spi.InetAddressResolver;\n+import java.net.spi.InetAddressResolverProvider;\n+import java.util.stream.Stream;\n+\n+public class WithBootstrapResolverUsageProvider extends InetAddressResolverProvider {\n+\n+    public static volatile long numberOfGetCalls;\n+\n+    @Override\n+    public InetAddressResolver get(Configuration configuration) {\n+        numberOfGetCalls++;\n+        System.out.println(\"The following provider will be used by current test:\" +\n+                this.getClass().getCanonicalName());\n+        System.out.println(\"InetAddressResolverProvider::get() called \" + numberOfGetCalls + \" times\");\n+\n+        \/\/ We use different names to avoid InetAddress-level caching\n+        doLookup(\"foo\" + numberOfGetCalls + \".A.org\");\n+\n+        \/\/ We need second call to test how InetAddress internals maintain reference to a bootstrap resolver\n+        doLookup(\"foo\" + numberOfGetCalls + \".B.org\");\n+\n+        return new InetAddressResolver() {\n+            @Override\n+            public Stream<InetAddress> lookupByName(String host, LookupPolicy lookupPolicy)\n+                    throws UnknownHostException {\n+                return Stream.of(InetAddress.getByAddress(host, new byte[]{127, 0, 2, 1}));\n+            }\n+\n+            @Override\n+            public String lookupByAddress(byte[] addr) throws UnknownHostException {\n+                return configuration.builtinResolver().lookupByAddress(addr);\n+            }\n+        };\n+    }\n+\n+    \/\/ Perform an InetAddress resolution lookup operation\n+    private static void doLookup(String hostName) {\n+        try {\n+            InetAddress.getByName(hostName);\n+        } catch (UnknownHostException e) {\n+            \/\/ Ignore UHE since the bootstrap resolver is used here\n+        }\n+    }\n+\n+    @Override\n+    public String name() {\n+        return \"WithBootstrapResolverUsageProvider\";\n+    }\n+}\n","filename":"test\/jdk\/java\/net\/spi\/InetAddressResolverProvider\/providers\/bootstrapUsage\/bootstrap.usage.provider\/impl\/WithBootstrapResolverUsageProvider.java","additions":76,"deletions":0,"binary":false,"changes":76,"status":"added"},{"patch":"@@ -0,0 +1,30 @@\n+\/*\n+ * Copyright (c) 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+import java.net.spi.InetAddressResolverProvider;\n+\n+module bootstrap.usage.provider {\n+    exports impl;\n+    requires java.logging;\n+    provides InetAddressResolverProvider with impl.WithBootstrapResolverUsageProvider;\n+}\n","filename":"test\/jdk\/java\/net\/spi\/InetAddressResolverProvider\/providers\/bootstrapUsage\/bootstrap.usage.provider\/module-info.java","additions":30,"deletions":0,"binary":false,"changes":30,"status":"added"},{"patch":"@@ -0,0 +1,68 @@\n+\/*\n+ * Copyright (c) 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+package impl;\n+\n+import java.net.InetAddress;\n+import java.net.UnknownHostException;\n+import java.net.spi.InetAddressResolver;\n+import java.net.spi.InetAddressResolverProvider;\n+import java.util.stream.Stream;\n+\n+public class DelegatingProviderImpl extends InetAddressResolverProvider {\n+\n+    public static volatile boolean changeReverseLookupAddress;\n+    public static volatile Throwable lastReverseLookupThrowable;\n+\n+    @Override\n+    public InetAddressResolver get(Configuration configuration) {\n+        System.out.println(\"The following provider will be used by current test:\" +\n+                this.getClass().getCanonicalName());\n+        return new InetAddressResolver() {\n+            @Override\n+            public Stream<InetAddress> lookupByName(String host, LookupPolicy lookupPolicy) throws UnknownHostException {\n+                return configuration.builtinResolver().lookupByName(host, lookupPolicy);\n+            }\n+\n+            @Override\n+            public String lookupByAddress(byte[] addr) throws UnknownHostException {\n+                try {\n+                    if (!changeReverseLookupAddress) {\n+                        return configuration.builtinResolver().lookupByAddress(addr);\n+                    } else {\n+                        \/\/ Deliberately supply address bytes array with wrong size\n+                        return configuration.builtinResolver().lookupByAddress(new byte[]{1, 2, 3});\n+                    }\n+                } catch (Throwable t) {\n+                    lastReverseLookupThrowable = t;\n+                    throw t;\n+                }\n+            }\n+        };\n+    }\n+\n+    @Override\n+    public String name() {\n+        return \"DelegatingProvider\";\n+    }\n+}\n","filename":"test\/jdk\/java\/net\/spi\/InetAddressResolverProvider\/providers\/delegating\/delegating.provider\/impl\/DelegatingProviderImpl.java","additions":68,"deletions":0,"binary":false,"changes":68,"status":"added"},{"patch":"@@ -0,0 +1,29 @@\n+\/*\n+ * Copyright (c) 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+*\/\n+\n+import java.net.spi.InetAddressResolverProvider;\n+\n+module delegating.provider {\n+    exports impl;\n+    provides InetAddressResolverProvider with impl.DelegatingProviderImpl;\n+}\n","filename":"test\/jdk\/java\/net\/spi\/InetAddressResolverProvider\/providers\/delegating\/delegating.provider\/module-info.java","additions":29,"deletions":0,"binary":false,"changes":29,"status":"added"},{"patch":"@@ -0,0 +1,56 @@\n+\/*\n+ * Copyright (c) 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+package impl;\n+\n+import java.net.InetAddress;\n+import java.net.UnknownHostException;\n+import java.net.spi.InetAddressResolver;\n+import java.net.spi.InetAddressResolverProvider;\n+import java.util.stream.Stream;\n+\n+public class EmptyResultsProviderImpl extends InetAddressResolverProvider {\n+    @Override\n+    public InetAddressResolver get(Configuration configuration) {\n+        System.out.println(\"The following provider will be used by current test:\" +\n+                this.getClass().getCanonicalName());\n+\n+        return new InetAddressResolver() {\n+            @Override\n+            public Stream<InetAddress> lookupByName(String host, LookupPolicy lookupPolicy)\n+                    throws UnknownHostException {\n+                return Stream.empty();\n+            }\n+\n+            @Override\n+            public String lookupByAddress(byte[] addr) throws UnknownHostException {\n+                return configuration.builtinResolver().lookupByAddress(addr);\n+            }\n+        };\n+    }\n+\n+    @Override\n+    public String name() {\n+        return \"EmptyForwardLookupResultsProvider\";\n+    }\n+}\n","filename":"test\/jdk\/java\/net\/spi\/InetAddressResolverProvider\/providers\/empty\/empty.results.provider\/impl\/EmptyResultsProviderImpl.java","additions":56,"deletions":0,"binary":false,"changes":56,"status":"added"},{"patch":"@@ -0,0 +1,29 @@\n+\/*\n+ * Copyright (c) 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+*\/\n+\n+import java.net.spi.InetAddressResolverProvider;\n+\n+module empty.results.provider {\n+    exports impl;\n+    provides InetAddressResolverProvider with impl.EmptyResultsProviderImpl;\n+}\n","filename":"test\/jdk\/java\/net\/spi\/InetAddressResolverProvider\/providers\/empty\/empty.results.provider\/module-info.java","additions":29,"deletions":0,"binary":false,"changes":29,"status":"added"},{"patch":"@@ -0,0 +1,42 @@\n+\/*\n+ * Copyright (c) 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+package impl;\n+\n+import java.net.spi.InetAddressResolverProvider;\n+import java.net.spi.InetAddressResolver;\n+\n+public class FaultyResolverProviderGetImpl extends InetAddressResolverProvider {\n+    public static final String EXCEPTION_MESSAGE = \"This provider provides nothing\";\n+\n+    @Override\n+    public InetAddressResolver get(Configuration configuration) {\n+        System.out.println(\"The following provider will be used by current test:\" + this.getClass().getCanonicalName());\n+        throw new IllegalArgumentException(EXCEPTION_MESSAGE);\n+    }\n+\n+    @Override\n+    public String name() {\n+        return \"faultyInetAddressResolverGet\";\n+    }\n+}\n","filename":"test\/jdk\/java\/net\/spi\/InetAddressResolverProvider\/providers\/faulty\/faulty.provider\/impl\/FaultyResolverProviderGetImpl.java","additions":42,"deletions":0,"binary":false,"changes":42,"status":"added"},{"patch":"@@ -0,0 +1,30 @@\n+\/*\n+ * Copyright (c) 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+*\/\n+\n+import java.net.spi.InetAddressResolverProvider;\n+\n+module faulty.provider {\n+    exports impl;\n+    requires java.logging;\n+    provides InetAddressResolverProvider with impl.FaultyResolverProviderGetImpl;\n+}\n","filename":"test\/jdk\/java\/net\/spi\/InetAddressResolverProvider\/providers\/faulty\/faulty.provider\/module-info.java","additions":30,"deletions":0,"binary":false,"changes":30,"status":"added"},{"patch":"@@ -0,0 +1,63 @@\n+\/*\n+ * Copyright (c) 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+package impl;\n+\n+import java.net.InetAddress;\n+import java.net.UnknownHostException;\n+import java.net.spi.InetAddressResolver;\n+import java.net.spi.InetAddressResolverProvider;\n+import java.util.stream.Stream;\n+\n+public class InetAddressUsageInGetProviderImpl extends InetAddressResolverProvider {\n+    @Override\n+    public InetAddressResolver get(Configuration configuration) {\n+        System.out.println(\"The following provider will be used by current test:\" + this.getClass().getCanonicalName());\n+        String localHostName;\n+        try {\n+            localHostName = InetAddress.getLocalHost().getHostName();\n+        } catch (UnknownHostException e) {\n+            throw new RuntimeException(\"Provider failed to initialize\");\n+        }\n+        return new InetAddressResolver() {\n+            @Override\n+            public Stream<InetAddress> lookupByName(String host, LookupPolicy lookupPolicy) throws UnknownHostException {\n+                if (host.equals(localHostName)) {\n+                    return configuration.builtinResolver().lookupByName(host, lookupPolicy);\n+                } else {\n+                    throw new UnknownHostException(host);\n+                }\n+            }\n+\n+            @Override\n+            public String lookupByAddress(byte[] addr) throws UnknownHostException {\n+                return configuration.builtinResolver().lookupByAddress(addr);\n+            }\n+        };\n+    }\n+\n+    @Override\n+    public String name() {\n+        return \"ProviderWithInetAddressUsageInGet\";\n+    }\n+}\n","filename":"test\/jdk\/java\/net\/spi\/InetAddressResolverProvider\/providers\/recursive\/recursive.init.provider\/impl\/InetAddressUsageInGetProviderImpl.java","additions":63,"deletions":0,"binary":false,"changes":63,"status":"added"},{"patch":"@@ -0,0 +1,30 @@\n+\/*\n+ * Copyright (c) 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+*\/\n+\n+import java.net.spi.InetAddressResolverProvider;\n+\n+module recursive.init.provider {\n+    exports impl;\n+    requires java.logging;\n+    provides InetAddressResolverProvider with impl.InetAddressUsageInGetProviderImpl;\n+}\n","filename":"test\/jdk\/java\/net\/spi\/InetAddressResolverProvider\/providers\/recursive\/recursive.init.provider\/module-info.java","additions":30,"deletions":0,"binary":false,"changes":30,"status":"added"},{"patch":"@@ -0,0 +1,91 @@\n+\/*\n+ * Copyright (c) 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+package impl;\n+\n+import java.net.InetAddress;\n+import java.net.UnknownHostException;\n+import java.net.spi.InetAddressResolver;\n+import java.net.spi.InetAddressResolver.LookupPolicy;\n+import java.net.spi.InetAddressResolverProvider;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.logging.Logger;\n+import java.util.stream.Stream;\n+\n+import testlib.ResolutionRegistry;\n+\n+public class SimpleResolverProviderImpl extends InetAddressResolverProvider {\n+\n+    public static ResolutionRegistry registry = new ResolutionRegistry();\n+    private static List<LookupPolicy> LOOKUP_HISTORY = Collections.synchronizedList(new ArrayList<>());\n+    private static volatile long LAST_LOOKUP_TIMESTAMP;\n+    private static Logger LOGGER = Logger.getLogger(SimpleResolverProviderImpl.class.getName());\n+\n+    @Override\n+    public InetAddressResolver get(Configuration configuration) {\n+        System.out.println(\"The following provider will be used by current test:\" + this.getClass().getCanonicalName());\n+        return new InetAddressResolver() {\n+            @Override\n+            public Stream<InetAddress> lookupByName(String host, LookupPolicy lookupPolicy) throws UnknownHostException {\n+                LOGGER.info(\"Looking-up addresses for '\" + host + \"'. Lookup characteristics:\" +\n+                        Integer.toString(lookupPolicy.characteristics(), 2));\n+                LOOKUP_HISTORY.add(lookupPolicy);\n+                LAST_LOOKUP_TIMESTAMP = System.nanoTime();\n+                return registry.lookupHost(host, lookupPolicy);\n+            }\n+\n+            @Override\n+            public String lookupByAddress(byte[] addr) throws UnknownHostException {\n+                LOGGER.info(\"Looking host name for the following address:\" + ResolutionRegistry.addressBytesToString(addr));\n+                return registry.lookupAddress(addr);\n+            }\n+        };\n+    }\n+\n+    \/\/ Utility methods\n+    public static LookupPolicy lastLookupPolicy() {\n+        return lookupPolicyHistory(0);\n+    }\n+\n+    public static long getLastLookupTimestamp() {\n+        return LAST_LOOKUP_TIMESTAMP;\n+    }\n+\n+    public static LookupPolicy lookupPolicyHistory(int position) {\n+        if (LOOKUP_HISTORY.isEmpty()) {\n+            throw new RuntimeException(\"No registered lookup policies\");\n+        }\n+        if (position >= LOOKUP_HISTORY.size()) {\n+            throw new IllegalArgumentException(\"No element available with provided position\");\n+        }\n+        return LOOKUP_HISTORY.get(LOOKUP_HISTORY.size() - position - 1);\n+    }\n+\n+\n+    @Override\n+    public String name() {\n+        return \"simpleInetAddressResolver\";\n+    }\n+}\n","filename":"test\/jdk\/java\/net\/spi\/InetAddressResolverProvider\/providers\/simple\/simple.provider\/impl\/SimpleResolverProviderImpl.java","additions":91,"deletions":0,"binary":false,"changes":91,"status":"added"},{"patch":"@@ -0,0 +1,31 @@\n+\/*\n+ * Copyright (c) 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+*\/\n+\n+import java.net.spi.InetAddressResolverProvider;\n+\n+module simple.provider {\n+    exports impl;\n+    requires java.logging;\n+    requires test.library;\n+    provides InetAddressResolverProvider with impl.SimpleResolverProviderImpl;\n+}\n","filename":"test\/jdk\/java\/net\/spi\/InetAddressResolverProvider\/providers\/simple\/simple.provider\/module-info.java","additions":31,"deletions":0,"binary":false,"changes":31,"status":"added"},{"patch":"@@ -0,0 +1,73 @@\n+\/*\n+ * Copyright (c) 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+package impl;\n+\n+import java.net.InetAddress;\n+import java.net.UnknownHostException;\n+import java.net.spi.InetAddressResolver;\n+import java.net.spi.InetAddressResolverProvider;\n+import java.util.stream.Stream;\n+\n+public class ThrowingLookupsProviderImpl extends InetAddressResolverProvider {\n+    @Override\n+    public InetAddressResolver get(Configuration configuration) {\n+        System.out.println(\"The following provider will be used by current test:\" +\n+                this.getClass().getCanonicalName());\n+\n+        return new InetAddressResolver() {\n+            @Override\n+            public Stream<InetAddress> lookupByName(String host, LookupPolicy lookupPolicy)\n+                    throws UnknownHostException {\n+                if (throwRuntimeException) {\n+                    System.err.println(name()+\" forward lookup: throwing RuntimeException\");\n+                    throw new RuntimeException(RUNTIME_EXCEPTION_MESSAGE);\n+                } else {\n+                    System.err.println(name()+\" forward lookup: throwing UnknownHostException\");\n+                    throw new UnknownHostException();\n+                }\n+            }\n+\n+            @Override\n+            public String lookupByAddress(byte[] addr) throws UnknownHostException {\n+                if (throwRuntimeException) {\n+                    System.err.println(name()+\" reverse lookup: throwing RuntimeException\");\n+                    throw new RuntimeException(RUNTIME_EXCEPTION_MESSAGE);\n+                } else {\n+                    System.err.println(name()+\" reverse lookup: throwing UnknownHostException\");\n+                    throw new UnknownHostException();\n+                }\n+            }\n+        };\n+    }\n+\n+    @Override\n+    public String name() {\n+        return \"ThrowingLookupsProvider\";\n+    }\n+\n+    \/\/ Indicates if provider need to throw RuntimeException for forward and reverse lookup operations.\n+    \/\/ If it is set to 'false' then UnknownHostException will thrown for each operation.\n+    public static volatile boolean throwRuntimeException;\n+    public static final String RUNTIME_EXCEPTION_MESSAGE = \"This provider only throws exceptions\";\n+}\n","filename":"test\/jdk\/java\/net\/spi\/InetAddressResolverProvider\/providers\/throwing\/throwing.lookups.provider\/impl\/ThrowingLookupsProviderImpl.java","additions":73,"deletions":0,"binary":false,"changes":73,"status":"added"},{"patch":"@@ -0,0 +1,29 @@\n+\/*\n+ * Copyright (c) 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+*\/\n+\n+import java.net.spi.InetAddressResolverProvider;\n+\n+module throwing.lookups.provider {\n+    exports impl;\n+    provides InetAddressResolverProvider with impl.ThrowingLookupsProviderImpl;\n+}\n","filename":"test\/jdk\/java\/net\/spi\/InetAddressResolverProvider\/providers\/throwing\/throwing.lookups.provider\/module-info.java","additions":29,"deletions":0,"binary":false,"changes":29,"status":"added"},{"patch":"@@ -0,0 +1,49 @@\n+\/*\n+ * Copyright (c) 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+import java.net.InetAddress;\n+\n+import org.testng.annotations.Test;\n+\n+import static org.testng.Assert.assertThrows;\n+\n+\/*\n+ * @test\n+ * @summary Test that InetAddressResolverProvider implementation can be installed to a class path.\n+ * @library ..\/..\/lib\n+ * @build test.library\/testlib.ResolutionRegistry ClasspathResolverProviderImpl\n+ * @run testng\/othervm ClasspathProviderTest\n+ *\/\n+\n+public class ClasspathProviderTest {\n+\n+    @Test\n+    public void testResolution() throws Exception {\n+        InetAddress inetAddress = InetAddress.getByName(\"classpath-provider-test.org\");\n+        System.err.println(\"Resolved address:\" + inetAddress);\n+\n+        if (!ClasspathResolverProviderImpl.registry.containsAddressMapping(inetAddress)) {\n+            throw new RuntimeException(\"InetAddressResolverProvider was not properly installed\");\n+        }\n+    }\n+}\n","filename":"test\/jdk\/java\/net\/spi\/InetAddressResolverProvider\/serviceProviderOriginType\/classpath\/ClasspathProviderTest.java","additions":49,"deletions":0,"binary":false,"changes":49,"status":"added"},{"patch":"@@ -0,0 +1,67 @@\n+\/*\n+ * Copyright (c) 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+import java.net.InetAddress;\n+import java.net.UnknownHostException;\n+import java.net.spi.InetAddressResolverProvider;\n+import java.net.spi.InetAddressResolver;\n+import java.net.spi.InetAddressResolver.LookupPolicy;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.logging.Logger;\n+import java.util.stream.Stream;\n+\n+import testlib.ResolutionRegistry;\n+\n+public class ClasspathResolverProviderImpl extends InetAddressResolverProvider {\n+\n+    public static ResolutionRegistry registry = new ResolutionRegistry();\n+    private static List<LookupPolicy> LOOKUP_HISTORY = Collections.synchronizedList(new ArrayList<>());\n+    private static Logger LOGGER = Logger.getLogger(ClasspathResolverProviderImpl.class.getName());\n+\n+    @Override\n+    public InetAddressResolver get(Configuration configuration) {\n+        System.out.println(\"The following provider will be used by current test:\" + this.getClass().getCanonicalName());\n+        return new InetAddressResolver() {\n+            @Override\n+            public Stream<InetAddress> lookupByName(String host, LookupPolicy lookupPolicy) throws UnknownHostException {\n+                LOGGER.info(\"Looking-up addresses for '\" + host + \"'. Lookup characteristics:\" +\n+                        Integer.toString(lookupPolicy.characteristics(), 2));\n+                LOOKUP_HISTORY.add(lookupPolicy);\n+                return registry.lookupHost(host, lookupPolicy);\n+            }\n+\n+            @Override\n+            public String lookupByAddress(byte[] addr) throws UnknownHostException {\n+                LOGGER.info(\"Looking host name for the following address:\" + ResolutionRegistry.addressBytesToString(addr));\n+                return registry.lookupAddress(addr);\n+            }\n+        };\n+    }\n+\n+    @Override\n+    public String name() {\n+        return \"classpathINSP\";\n+    }\n+}\n","filename":"test\/jdk\/java\/net\/spi\/InetAddressResolverProvider\/serviceProviderOriginType\/classpath\/ClasspathResolverProviderImpl.java","additions":67,"deletions":0,"binary":false,"changes":67,"status":"added"},{"patch":"@@ -0,0 +1,1 @@\n+ClasspathResolverProviderImpl\n","filename":"test\/jdk\/java\/net\/spi\/InetAddressResolverProvider\/serviceProviderOriginType\/classpath\/META-INF\/services\/java.net.spi.InetAddressResolverProvider","additions":1,"deletions":0,"binary":false,"changes":1,"status":"added"},{"patch":"@@ -0,0 +1,7 @@\n+# Test data file for classpath origin type tests.\n+# Format: <IP address> <Host Name>\n+# If multiple IP addresses are required for host:\n+#    multiple lines could be added\n+\n+1.2.3.4    classpath-provider-test.org\n+[ca:fe:ba:be::1] classpath-provider-test.org\n","filename":"test\/jdk\/java\/net\/spi\/InetAddressResolverProvider\/serviceProviderOriginType\/classpath\/addresses.txt","additions":7,"deletions":0,"binary":false,"changes":7,"status":"added"},{"patch":"@@ -0,0 +1,49 @@\n+\/*\n+ * Copyright (c) 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+import java.net.InetAddress;\n+\n+import org.testng.annotations.Test;\n+\n+\/*\n+ * @test\n+ * @summary Test that implementation of InetAddressResolverProvider can be installed to a module path.\n+ * @library ..\/..\/lib ..\/..\/providers\/simple\n+ * @build test.library\/testlib.ResolutionRegistry simple.provider\/impl.SimpleResolverProviderImpl\n+ *        ModularProviderTest\n+ * @run testng\/othervm ModularProviderTest\n+ *\/\n+\n+\n+public class ModularProviderTest {\n+\n+    @Test\n+    public void testResolution() throws Exception {\n+        InetAddress inetAddress = InetAddress.getByName(\"modular-provider-test.org\");\n+        System.err.println(\"Resolved address:\" + inetAddress);\n+\n+        if (!impl.SimpleResolverProviderImpl.registry.containsAddressMapping(inetAddress)) {\n+            throw new RuntimeException(\"InetAddressResolverProvider was not properly installed\");\n+        }\n+    }\n+}\n","filename":"test\/jdk\/java\/net\/spi\/InetAddressResolverProvider\/serviceProviderOriginType\/module\/ModularProviderTest.java","additions":49,"deletions":0,"binary":false,"changes":49,"status":"added"},{"patch":"@@ -0,0 +1,7 @@\n+# Test data file for tests in modularTests directory\n+# Format: <IP address> <Host Name>\n+# If multiple IP addresses are required for host:\n+#    multiple lines could be added\n+\n+1.2.3.4    modular-provider-test.org\n+[ca:fe:ba:be::1] modular-provider-test.org\n","filename":"test\/jdk\/java\/net\/spi\/InetAddressResolverProvider\/serviceProviderOriginType\/module\/addresses.txt","additions":7,"deletions":0,"binary":false,"changes":7,"status":"added"},{"patch":"@@ -141,14 +141,28 @@\n-        \/\/ preparing two temporary files which will be compared at the end of the test\n-        Path sourceFile = Files.createTempFile(null, null);\n-        Path targetFile = Files.createTempFile(null, null);\n-\n-        \/\/ writing 3 GB of random bytes into source file\n-        for (int i = 0; i < NUM_WRITES; i++)\n-            Files.write(sourceFile, createRandomBytes(BYTES_PER_WRITE, 0), StandardOpenOption.APPEND);\n-\n-        \/\/ performing actual transfer, effectively by multiple invocations of Filechannel.transferTo(FileChannel)\n-        long count;\n-        try (InputStream inputStream = Channels.newInputStream(FileChannel.open(sourceFile));\n-                OutputStream outputStream = Channels\n-                        .newOutputStream(FileChannel.open(targetFile, StandardOpenOption.WRITE))) {\n-            count = inputStream.transferTo(outputStream);\n+        Path sourceFile = Files.createTempFile(\"test2GBSource\", null);\n+        try {\n+            \/\/ preparing two temporary files which will be compared at the end of the test\n+            Path targetFile = Files.createTempFile(\"test2GBtarget\", null);\n+            try {\n+                \/\/ writing 3 GB of random bytes into source file\n+                for (int i = 0; i < NUM_WRITES; i++)\n+                    Files.write(sourceFile, createRandomBytes(BYTES_PER_WRITE, 0), StandardOpenOption.APPEND);\n+\n+                \/\/ performing actual transfer, effectively by multiple invocations of Filechannel.transferTo(FileChannel)\n+                long count;\n+                try (InputStream inputStream = Channels.newInputStream(FileChannel.open(sourceFile));\n+                     OutputStream outputStream = Channels\n+                             .newOutputStream(FileChannel.open(targetFile, StandardOpenOption.WRITE))) {\n+                    count = inputStream.transferTo(outputStream);\n+                }\n+\n+                \/\/ comparing reported transferred bytes, must be 3 GB\n+                assertEquals(count, BYTES_WRITTEN);\n+\n+                \/\/ comparing content of both files, failing in case of any difference\n+                assertEquals(Files.mismatch(sourceFile, targetFile), -1);\n+\n+            } finally {\n+                 Files.delete(targetFile);\n+            }\n+        } finally {\n+            Files.delete(sourceFile);\n@@ -156,6 +170,0 @@\n-\n-        \/\/ comparing reported transferred bytes, must be 3 GB\n-        assertEquals(count, BYTES_WRITTEN);\n-\n-        \/\/ comparing content of both files, failing in case of any difference\n-        assertEquals(Files.mismatch(sourceFile, targetFile), -1);\n","filename":"test\/jdk\/java\/nio\/channels\/Channels\/TransferTo.java","additions":28,"deletions":20,"binary":false,"changes":48,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2019, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -161,1 +161,1 @@\n-            s.setSoTimeout(1000);\n+            s.setSoTimeout(60_000);\n@@ -174,1 +174,1 @@\n-            s.setSoTimeout(5000);\n+            s.setSoTimeout(60_000);\n@@ -186,1 +186,1 @@\n-            s.setSoTimeout(1000);\n+            s.setSoTimeout(500);\n@@ -199,1 +199,1 @@\n-            s.setSoTimeout(60*1000);\n+            s.setSoTimeout(60_000);\n@@ -213,1 +213,1 @@\n-                s.setSoTimeout(60*1000);\n+                s.setSoTimeout(60_000);\n@@ -231,1 +231,1 @@\n-                s.setSoTimeout(60*1000);\n+                s.setSoTimeout(60_000);\n@@ -399,1 +399,1 @@\n-            s.setSoTimeout(60*1000);\n+            s.setSoTimeout(60_000);\n@@ -424,1 +424,1 @@\n-            s.setSoTimeout(60*1000);\n+            s.setSoTimeout(60_000);\n@@ -439,1 +439,1 @@\n-                s.setSoTimeout(60*1000);\n+                s.setSoTimeout(60_000);\n","filename":"test\/jdk\/java\/nio\/channels\/SocketChannel\/AdaptorStreams.java","additions":10,"deletions":10,"binary":false,"changes":20,"status":"modified"},{"patch":"@@ -148,0 +148,10 @@\n+                } catch (FileSystemException fse) {\n+                    \/\/ On Linux, ignore the FSE if the path is one of the\n+                    \/\/ \/run\/user\/$UID mounts created by pam_systemd(8) as it\n+                    \/\/ might be mounted as a fuse.portal filesystem and\n+                    \/\/ its access attempt might fail with EPERM\n+                    if (!Platform.isLinux() || store.toString().indexOf(\"\/run\/user\") == -1) {\n+                        throw new RuntimeException(fse);\n+                    } else {\n+                        System.err.format(\"%s error: %s\\n\", store, fse);\n+                    }\n","filename":"test\/jdk\/java\/nio\/file\/FileStore\/Basic.java","additions":10,"deletions":0,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -0,0 +1,581 @@\n+\/*\n+ * Copyright (c) 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+import org.testng.annotations.AfterTest;\n+import org.testng.annotations.BeforeTest;\n+import org.testng.annotations.DataProvider;\n+import org.testng.annotations.Test;\n+\n+import java.io.ByteArrayOutputStream;\n+import java.io.FileInputStream;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.net.JarURLConnection;\n+import java.net.URL;\n+import java.nio.charset.StandardCharsets;\n+import java.nio.file.Files;\n+import java.nio.file.Path;\n+import java.nio.file.Paths;\n+import java.util.Arrays;\n+import java.util.Formatter;\n+import java.util.List;\n+import java.util.jar.JarEntry;\n+import java.util.jar.JarFile;\n+import java.util.jar.JarInputStream;\n+import java.util.jar.JarOutputStream;\n+import java.util.stream.Collectors;\n+import java.util.zip.ZipEntry;\n+import java.util.zip.ZipFile;\n+import java.util.zip.ZipInputStream;\n+import java.util.zip.ZipOutputStream;\n+\n+import static org.testng.Assert.*;\n+\n+\/**\n+ * @test\n+ * @bug 8276123\n+ * @summary ZipFile::getEntry will not return a file entry when there is a\n+ * directory entry of the same name within a Zip File\n+ * @run testng\/othervm ZipFileDuplicateEntryTest\n+ *\/\n+public class ZipFileDuplicateEntryTest {\n+\n+    \/**\n+     * Name to use for creating Zip entries\n+     *\/\n+    private static final String ENTRY_NAME = \"entry\";\n+\n+    \/**\n+     * Zip and Jar files to be created\n+     *\/\n+    private static final Path ZIP_FILE = Paths.get(\"fileDirEntry.zip\");\n+    private static final Path ZIP_FILE2 = Paths.get(\"OnlyDirEntry.zip\");\n+    private static final Path DUPLICATE_FILE_ENTRY_FILE = Paths.get(\"DupFIleEntry.zip\");\n+    private static final Path TEST_JAR = Paths.get(\"fileDirEntry.jar\");\n+\n+    \/**\n+     * Directory entry added to the Zip File.\n+     *\/\n+    private static final Entry DIR_ENTRY =\n+            Entry.of(ENTRY_NAME + \"\/\", ZipEntry.DEFLATED,\n+                    \"I am a Directory\");\n+\n+    \/**\n+     * File entry added to the Zip File.\n+     *\/\n+    private static final Entry FILE_ENTRY =\n+            Entry.of(ENTRY_NAME, ZipEntry.DEFLATED, \"I am a File\");\n+\n+    \/**\n+     * Duplicate File entry added to the Zip file. This is the 2nd entry added\n+     * to the Zip file and is expected to be returned.\n+     *\/\n+    private static final Entry DUPLICATE_FILE_ENTRY =\n+            Entry.of(ENTRY_NAME, ZipEntry.DEFLATED, \"Yet another File\");\n+    \/**\n+     * Entries expected to be returned via ZipFile::stream\n+     *\/\n+    private static final List<String> EXPECTED_ENTRIES =\n+            Arrays.asList(FILE_ENTRY.name, DIR_ENTRY.name);\n+\n+    \/**\n+     * Max buffer size for readAllBytes method which can be used when\n+     * InputStream::readAllBytes is not available\n+     *\/\n+    private static final int MAX_BUFFER_SIZE = 1024;\n+\n+    \/**\n+     * Flag to enable test output\n+     *\/\n+    private static final boolean DEBUG = false;\n+\n+    \/**\n+     * Array representing a Jar File with the entries:\n+     * Name: entry, contents: \"I am a File\"\n+     * Name: entry, contents: \"Yet another File\"\n+     * See createByteArray()\n+     *\/\n+    private static final byte[] DUPLICATE_ENTRY_JAR_BYTES = {\n+            (byte) 0x50, (byte) 0x4b, (byte) 0x3, (byte) 0x4, (byte) 0x14, (byte) 0x0, (byte) 0x0, (byte) 0x8,\n+            (byte) 0x8, (byte) 0x0, (byte) 0x60, (byte) 0x59, (byte) 0x55, (byte) 0x53, (byte) 0x8e, (byte) 0x39,\n+            (byte) 0x14, (byte) 0x49, (byte) 0xd, (byte) 0x0, (byte) 0x0, (byte) 0x0, (byte) 0xb, (byte) 0x0,\n+            (byte) 0x0, (byte) 0x0, (byte) 0x5, (byte) 0x0, (byte) 0x14, (byte) 0x0, (byte) 0x65, (byte) 0x6e,\n+            (byte) 0x74, (byte) 0x72, (byte) 0x79, (byte) 0x1, (byte) 0x0, (byte) 0x10, (byte) 0x0, (byte) 0xb,\n+            (byte) 0x0, (byte) 0x0, (byte) 0x0, (byte) 0x0, (byte) 0x0, (byte) 0x0, (byte) 0x0, (byte) 0xd,\n+            (byte) 0x0, (byte) 0x0, (byte) 0x0, (byte) 0x0, (byte) 0x0, (byte) 0x0, (byte) 0x0, (byte) 0xf3,\n+            (byte) 0x54, (byte) 0x48, (byte) 0xcc, (byte) 0x55, (byte) 0x48, (byte) 0x54, (byte) 0x70, (byte) 0xcb,\n+            (byte) 0xcc, (byte) 0x49, (byte) 0x5, (byte) 0x0, (byte) 0x50, (byte) 0x4b, (byte) 0x3, (byte) 0x4,\n+            (byte) 0x14, (byte) 0x0, (byte) 0x0, (byte) 0x8, (byte) 0x8, (byte) 0x0, (byte) 0x60, (byte) 0x59,\n+            (byte) 0x55, (byte) 0x53, (byte) 0xe1, (byte) 0x4c, (byte) 0x29, (byte) 0xa4, (byte) 0x12, (byte) 0x0,\n+            (byte) 0x0, (byte) 0x0, (byte) 0x10, (byte) 0x0, (byte) 0x0, (byte) 0x0, (byte) 0x5, (byte) 0x0,\n+            (byte) 0x14, (byte) 0x0, (byte) 0x65, (byte) 0x6e, (byte) 0x74, (byte) 0x72, (byte) 0x79, (byte) 0x1,\n+            (byte) 0x0, (byte) 0x10, (byte) 0x0, (byte) 0x10, (byte) 0x0, (byte) 0x0, (byte) 0x0, (byte) 0x0,\n+            (byte) 0x0, (byte) 0x0, (byte) 0x0, (byte) 0x12, (byte) 0x0, (byte) 0x0, (byte) 0x0, (byte) 0x0,\n+            (byte) 0x0, (byte) 0x0, (byte) 0x0, (byte) 0x8b, (byte) 0x4c, (byte) 0x2d, (byte) 0x51, (byte) 0x48,\n+            (byte) 0xcc, (byte) 0xcb, (byte) 0x2f, (byte) 0xc9, (byte) 0x48, (byte) 0x2d, (byte) 0x52, (byte) 0x70,\n+            (byte) 0xcb, (byte) 0xcc, (byte) 0x49, (byte) 0x5, (byte) 0x0, (byte) 0x50, (byte) 0x4b, (byte) 0x1,\n+            (byte) 0x2, (byte) 0x14, (byte) 0x0, (byte) 0x14, (byte) 0x0, (byte) 0x0, (byte) 0x8, (byte) 0x8,\n+            (byte) 0x0, (byte) 0x60, (byte) 0x59, (byte) 0x55, (byte) 0x53, (byte) 0x8e, (byte) 0x39, (byte) 0x14,\n+            (byte) 0x49, (byte) 0xd, (byte) 0x0, (byte) 0x0, (byte) 0x0, (byte) 0xb, (byte) 0x0, (byte) 0x0,\n+            (byte) 0x0, (byte) 0x5, (byte) 0x0, (byte) 0x0, (byte) 0x0, (byte) 0x0, (byte) 0x0, (byte) 0x0,\n+            (byte) 0x0, (byte) 0x0, (byte) 0x0, (byte) 0x0, (byte) 0x0, (byte) 0x0, (byte) 0x0, (byte) 0x0,\n+            (byte) 0x0, (byte) 0x0, (byte) 0x0, (byte) 0x65, (byte) 0x6e, (byte) 0x74, (byte) 0x72, (byte) 0x79,\n+            (byte) 0x50, (byte) 0x4b, (byte) 0x1, (byte) 0x2, (byte) 0x14, (byte) 0x0, (byte) 0x14, (byte) 0x0,\n+            (byte) 0x0, (byte) 0x8, (byte) 0x8, (byte) 0x0, (byte) 0x60, (byte) 0x59, (byte) 0x55, (byte) 0x53,\n+            (byte) 0xe1, (byte) 0x4c, (byte) 0x29, (byte) 0xa4, (byte) 0x12, (byte) 0x0, (byte) 0x0, (byte) 0x0,\n+            (byte) 0x10, (byte) 0x0, (byte) 0x0, (byte) 0x0, (byte) 0x5, (byte) 0x0, (byte) 0x0, (byte) 0x0,\n+            (byte) 0x0, (byte) 0x0, (byte) 0x0, (byte) 0x0, (byte) 0x0, (byte) 0x0, (byte) 0x0, (byte) 0x0,\n+            (byte) 0x0, (byte) 0x0, (byte) 0x44, (byte) 0x0, (byte) 0x0, (byte) 0x0, (byte) 0x65, (byte) 0x6e,\n+            (byte) 0x74, (byte) 0x72, (byte) 0x79, (byte) 0x50, (byte) 0x4b, (byte) 0x5, (byte) 0x6, (byte) 0x0,\n+            (byte) 0x0, (byte) 0x0, (byte) 0x0, (byte) 0x2, (byte) 0x0, (byte) 0x2, (byte) 0x0, (byte) 0x66,\n+            (byte) 0x0, (byte) 0x0, (byte) 0x0, (byte) 0x8d, (byte) 0x0, (byte) 0x0, (byte) 0x0, (byte) 0x0,\n+            (byte) 0x0,\n+    };\n+\n+    \/**\n+     * Create Zip files used by the tests.\n+     *\n+     * @throws IOException If an error occurs\n+     *\/\n+    @BeforeTest\n+    public static void setup() throws IOException {\n+\n+        \/**\n+         *  Zip contains two entries named \"entry\" and \"entry\/\"\n+         *\/\n+        Files.deleteIfExists(ZIP_FILE);\n+        try (ZipOutputStream zos = new ZipOutputStream(Files.newOutputStream(ZIP_FILE))) {\n+            zos.putNextEntry(new ZipEntry(FILE_ENTRY.name));\n+            zos.write(FILE_ENTRY.bytes);\n+            zos.closeEntry();\n+            zos.putNextEntry(new ZipEntry(DIR_ENTRY.name));\n+            zos.write(DIR_ENTRY.bytes);\n+            zos.closeEntry();\n+        }\n+\n+        \/**\n+         *  Jar contains two entries named \"entry\" and \"entry\/\"\n+         *\/\n+        Files.deleteIfExists(TEST_JAR);\n+        try (JarOutputStream jos = new JarOutputStream(Files.newOutputStream(TEST_JAR))) {\n+            jos.putNextEntry(new JarEntry(FILE_ENTRY.name));\n+            jos.write(FILE_ENTRY.bytes);\n+            jos.closeEntry();\n+            jos.putNextEntry(new JarEntry(DIR_ENTRY.name));\n+            jos.write(DIR_ENTRY.bytes);\n+            jos.closeEntry();\n+        }\n+\n+        \/**\n+         *  Zip contains the entry \"entry\/\"\n+         *\/\n+        Files.deleteIfExists(ZIP_FILE2);\n+        try (ZipOutputStream zos = new ZipOutputStream(Files.newOutputStream(ZIP_FILE2))) {\n+            zos.putNextEntry(new ZipEntry(DIR_ENTRY.name));\n+            zos.write(DIR_ENTRY.bytes);\n+            zos.closeEntry();\n+        }\n+\n+        \/**\n+         *  Create a Jar that contains two entries named \"entry\"\n+         *\/\n+        Files.deleteIfExists(DUPLICATE_FILE_ENTRY_FILE);\n+        Files.write(DUPLICATE_FILE_ENTRY_FILE, DUPLICATE_ENTRY_JAR_BYTES);\n+    }\n+\n+    \/**\n+     * Clean up after the test run\n+     *\n+     * @throws IOException If an error occurs\n+     *\/\n+    @AfterTest\n+    public static void cleanup() throws IOException {\n+        Files.deleteIfExists(ZIP_FILE);\n+        Files.deleteIfExists(ZIP_FILE2);\n+        Files.deleteIfExists(DUPLICATE_FILE_ENTRY_FILE);\n+        Files.deleteIfExists(TEST_JAR);\n+    }\n+\n+    \/**\n+     * DataProvider used to specify the Zip entries to use\n+     *\n+     * @return The Entry to use within the test\n+     *\/\n+    @DataProvider\n+    public Object[][] entries() {\n+        return new Object[][]{\n+                {FILE_ENTRY},\n+                {DIR_ENTRY}\n+        };\n+    }\n+\n+    \/**\n+     * Test whether ZipFile::getEntry can find a directory entry within a Zip\n+     * file specifying \"name\" vs \"name\/\"\n+     *\n+     * @throws IOException If an error occurs\n+     *\/\n+    @Test\n+    public void readDirWithoutSlash() throws IOException {\n+        System.out.printf(\"%n%n**** readDirWithoutSlash ***%n\");\n+        try (ZipFile zip = new ZipFile(ZIP_FILE2.toString())) {\n+            ZipEntry ze = zip.getEntry(ENTRY_NAME);\n+            if (DEBUG) {\n+                System.out.printf(\"    Entry:%s, found:%s%n\", ENTRY_NAME, ze != null);\n+            }\n+            assertNotNull(ze);\n+            assertTrue(ze.isDirectory());\n+            try (InputStream in = zip.getInputStream(ze)) {\n+                byte[] bytes = in.readAllBytes();\n+                if (DEBUG) {\n+                    System.out.printf(\"name: %s, isDirectory: %s, payload= %s%n\",\n+                            ze.getName(), ze.isDirectory(), new String(bytes));\n+                }\n+                assertEquals(bytes, DIR_ENTRY.bytes,\n+                        String.format(\"Expected payload: %s\",\n+                                new String(DIR_ENTRY.bytes)));\n+            }\n+        }\n+    }\n+\n+    \/**\n+     * Validate that ZipFile::getEntry will return the correct entry when a file\n+     * and directory have the same name\n+     *\n+     * @param entry The entry to search for\n+     * @throws IOException If an error occurs\n+     *\/\n+    @Test(dataProvider = \"entries\")\n+    public void testSameFileDirEntryName(Entry entry) throws IOException {\n+        System.out.printf(\"%n%n**** testSameFileDirEntryName ***%n\");\n+\n+        try (ZipFile zip = new ZipFile(ZIP_FILE.toString())) {\n+            ZipEntry ze = zip.getEntry(entry.name);\n+            if (DEBUG) {\n+                System.out.printf(\"    Entry:%s, found:%s%n\", entry.name, ze != null);\n+            }\n+            assertNotNull(ze);\n+            try (InputStream in = zip.getInputStream(ze)) {\n+                byte[] bytes = in.readAllBytes();\n+                if (DEBUG) {\n+                    System.out.printf(\"name: %s, isDirectory: %s, payload= %s%n\",\n+                            ze.getName(), ze.isDirectory(), new String(bytes));\n+                }\n+                assertEquals(entry.bytes, bytes,\n+                        String.format(\"Expected payload: %s\", new String(entry.bytes)));\n+            }\n+        }\n+    }\n+\n+    \/**\n+     * Validate that ZipFile::getEntry will return the correct entry, which\n+     * is the second entry, when there are duplicate entries within the Zip file.\n+     *\n+     * @throws IOException If an error occurs\n+     *\/\n+    @Test\n+    public void DupFileEntryTest() throws IOException {\n+        System.out.printf(\"%n%n**** DupFileEntryTest ***%n\");\n+        try (ZipFile zip =\n+                     new ZipFile(DUPLICATE_FILE_ENTRY_FILE.toString())) {\n+            ZipEntry ze = zip.getEntry(ENTRY_NAME);\n+            if (DEBUG) {\n+                System.out.printf(\"    Entry:%s, found:%s%n\", ENTRY_NAME, ze != null);\n+            }\n+            assertNotNull(ze);\n+            try (InputStream in = zip.getInputStream(ze)) {\n+                byte[] bytes = in.readAllBytes();\n+                if (DEBUG) {\n+                    System.out.printf(\"name: %s, isDirectory: %s, payload= %s%n\",\n+                            ze.getName(), ze.isDirectory(), new String(bytes));\n+                }\n+                assertEquals(bytes, DUPLICATE_FILE_ENTRY.bytes,\n+                        String.format(\"Expected payload: %s\", new String(DUPLICATE_FILE_ENTRY.bytes)));\n+            }\n+        }\n+    }\n+\n+    \/**\n+     * Verify that ZipInputStream can be used to read all Zip entries including\n+     * a file and directory entry with the same name\n+     *\n+     * @throws IOException If an error occurs\n+     *\/\n+    @Test\n+    public void ZipInputStreamTest() throws IOException {\n+        System.out.printf(\"%n%n**** ZipInputStreamTest ***%n\");\n+        try (ZipInputStream zis = new ZipInputStream(\n+                new FileInputStream(ZIP_FILE.toFile()))) {\n+            ZipEntry zipEntry = zis.getNextEntry();\n+            assertNotNull(zipEntry);\n+            while (zipEntry != null) {\n+                Entry e;\n+                if (zipEntry.getName().equals(FILE_ENTRY.name)) {\n+                    e = FILE_ENTRY;\n+                } else if (zipEntry.getName().equals(DIR_ENTRY.name)) {\n+                    e = DIR_ENTRY;\n+                } else {\n+                    throw new RuntimeException(\n+                            String.format(\"Invalid Zip entry: %s\", zipEntry.getName()));\n+                }\n+                assertEquals(zipEntry.getMethod(), e.method);\n+                assertEquals(zis.readAllBytes(), e.bytes,\n+                        String.format(\"Expected payload: %s\", new String(e.bytes)));\n+                zipEntry = zis.getNextEntry();\n+            }\n+        }\n+    }\n+\n+    \/**\n+     * Verify that ZipFile::stream returns all Zip entries including\n+     * a file and directory entry with the same name\n+     *\n+     * @throws IOException If an error occurs\n+     *\/\n+    @Test\n+    public void ZipFileStreamTest() throws IOException {\n+        System.out.printf(\"%n%n**** ZipFileStreamTest ***%n\");\n+        try (ZipFile zf = new ZipFile(ZIP_FILE.toFile())) {\n+            List<? extends ZipEntry> entries = zf.stream().collect(Collectors.toList());\n+            assertEquals(EXPECTED_ENTRIES.size(), entries.size());\n+            for (ZipEntry e : entries) {\n+                assertTrue(EXPECTED_ENTRIES.contains(e.getName()));\n+            }\n+        }\n+    }\n+\n+    \/**\n+     * Verify that JarFile can be used to read all the entries including\n+     * a file and directory entry with the same name\n+     *\n+     * @param entry The entry to validate\n+     * @throws IOException If an error occurs\n+     *\/\n+    @Test(dataProvider = \"entries\")\n+    public static void JarFileInputStreamTest(Entry entry) throws IOException {\n+        System.out.printf(\"%n%n**** JarFileInputStreamTest ***%n\");\n+        try (JarFile jarFile = new JarFile(TEST_JAR.toFile())) {\n+            JarEntry je = jarFile.getJarEntry(entry.name);\n+            assertNotNull(je);\n+            if (DEBUG) {\n+                System.out.printf(\"Entry Name: %s, method: %s, Expected Method: %s%n\",\n+                        entry.name, je.getMethod(), entry.method);\n+            }\n+            assertEquals(entry.method, je.getMethod());\n+            try (InputStream in = jarFile.getInputStream(je)) {\n+                byte[] bytes = in.readAllBytes();\n+                if (DEBUG) {\n+                    System.out.printf(\"bytes= %s, expected=%s%n\",\n+                            new String(bytes), new String(entry.bytes));\n+                }\n+                assertEquals(bytes, entry.bytes,\n+                        String.format(\"Expected payload: %s\", new String(entry.bytes)));\n+            }\n+        }\n+    }\n+\n+    \/**\n+     * Verify that JarInputStream can be used to read all entries including\n+     * a file and directory entry with the same name\n+     *\n+     * @throws IOException If an error occurs\n+     *\/\n+    @Test\n+    public void JarInputStreamTest() throws IOException {\n+        System.out.printf(\"%n%n**** JarInputStreamTest ***%n\");\n+        try (JarInputStream jis = new JarInputStream(\n+                new FileInputStream(TEST_JAR.toFile()))) {\n+            JarEntry jarEntry = jis.getNextJarEntry();\n+            assertNotNull(jarEntry);\n+            while (jarEntry != null) {\n+                Entry e;\n+                if (jarEntry.getName().equals(FILE_ENTRY.name)) {\n+                    e = FILE_ENTRY;\n+                } else if (jarEntry.getName().equals(DIR_ENTRY.name)) {\n+                    e = DIR_ENTRY;\n+                } else {\n+                    throw new RuntimeException(\n+                            String.format(\"Invalid Jar entry: %s\", jarEntry.getName()));\n+                }\n+                assertEquals(jarEntry.getMethod(), e.method);\n+                assertEquals(jis.readAllBytes(), e.bytes,\n+                        String.format(\"Expected payload: %s\", new String(e.bytes)));\n+                jarEntry = jis.getNextJarEntry();\n+            }\n+        }\n+    }\n+\n+    \/**\n+     * Verify that JarURLConnection can be used to access all the entries including\n+     * a file and directory entry with the same name within a jar file\n+     *\n+     * @param entry The entry to validate\n+     * @throws IOException If an error occurs\n+     *\/\n+    @Test(dataProvider = \"entries\")\n+    public void JarURLConnectionTest(Entry entry) throws Exception {\n+        System.out.printf(\"%n%n**** JarURLConnectionTest ***%n\");\n+        URL url = new URL(\"jar:\" + TEST_JAR.toUri().toURL() + \"!\/\" + entry.name);\n+        if (DEBUG) {\n+            System.out.printf(\"URL=%s%n\", url);\n+        }\n+        JarURLConnection con = (JarURLConnection) url.openConnection();\n+        con.connect();\n+        JarEntry je = con.getJarEntry();\n+        try (JarFile jarFile = con.getJarFile()) {\n+            assertNotNull(je);\n+            assertNotNull(jarFile);\n+            assertNull(con.getAttributes());\n+            assertNull(con.getMainAttributes());\n+            assertNull(con.getManifest());\n+            assertEquals(je.getName(), entry.name);\n+            assertEquals(con.getEntryName(), entry.name);\n+            assertEquals(je.getMethod(), entry.method);\n+            assertEquals(con.getJarFileURL(), TEST_JAR.toUri().toURL());\n+            if (DEBUG) {\n+                System.out.printf(\"   getEntryName: %s,  getJarFileURL:%s%n\",\n+                        con.getEntryName(), con.getJarFileURL());\n+                System.out.printf(\"   Jar Entry= %s, size= %s%n\", je.getName(), je.getSize());\n+            }\n+\n+            try (InputStream is = jarFile.getInputStream(je)) {\n+                byte[] bytes = is.readAllBytes();\n+                if (DEBUG) {\n+                    System.out.printf(\"   Bytes read:%s%n\", new String(bytes));\n+                }\n+                assertEquals(bytes, entry.bytes,\n+                        String.format(\"Expected payload: %s\", new String(entry.bytes)));\n+            }\n+        }\n+    }\n+\n+    \/**\n+     * Verify that JarFile::stream returns all entries including\n+     * a file and directory entry with the same name\n+     *\n+     * @throws IOException If an error occurs\n+     *\/\n+    @Test\n+    public void JarFileStreamTest() throws IOException {\n+        System.out.printf(\"%n%n**** JarFileStreamTest ***%n\");\n+        try (JarFile jf = new JarFile(TEST_JAR.toFile())) {\n+            List<? extends JarEntry> entries = jf.stream().collect(Collectors.toList());\n+            assertEquals(EXPECTED_ENTRIES.size(), jf.size());\n+            for (JarEntry e : entries) {\n+                assertTrue(EXPECTED_ENTRIES.contains(e.getName()));\n+            }\n+        }\n+    }\n+\n+    \/**\n+     * Method used to read  the bytes from an InputStream.  This method is\n+     * here so that the test could be backported to JDK 8 if needed as\n+     * InputStream::readAllBytes() does not exist\n+     *\n+     * @param is The InputStream to read from\n+     * @return The byte array representing bytes read from the InputStream\n+     * @throws IOException If an error occurs\n+     *\/\n+    public static byte[] readAllBytes(InputStream is) throws IOException {\n+        byte[] data = new byte[MAX_BUFFER_SIZE];\n+        ByteArrayOutputStream buffer = new ByteArrayOutputStream();\n+        int len;\n+        while ((len = is.read(data, 0, data.length)) != -1) {\n+            buffer.write(data, 0, len);\n+        }\n+        buffer.flush();\n+        return buffer.toByteArray();\n+    }\n+\n+    \/**\n+     * Method used to create a byte[] representing a Jar file with\n+     * duplicate file entries.  This uses ZipArchiveOutputStream as ZipOutputStream\n+     * will fail with a \"java.util.zip.ZipException: duplicate entry\".\n+     *\/\n+\/\/    public static void  createJarWithDuplicateFileEntries() throws IOException {\n+\/\/    Files.deleteIfExists(DUPFILE_ENTRY_FILE);\n+\/\/    try (ZipArchiveOutputStream zos =\n+\/\/                     new ZipArchiveOutputStream(DUPFILE_ENTRY_FILE.toFile())) {\n+\/\/            zos.putArchiveEntry(new ZipArchiveEntry(FILE_ENTRY.name));\n+\/\/            zos.write(FILE_ENTRY.bytes);\n+\/\/            zos.putArchiveEntry(new ZipArchiveEntry(FILE_ENTRY.name));\n+\/\/            zos.write(\"Yet another File\".getBytes(StandardCharsets.UTF_8));\n+\/\/            zos.closeArchiveEntry();\n+\/\/        } catch (IOException e) {\n+\/\/            e.printStackTrace();\n+\/\/        }\n+\/\/        byte[] jarBytes = Files.readAllBytes(DUPFILE_ENTRY_FILE);\n+\/\/        String result = createByteArray(jarBytes, \"DUPLICATE_ENTRY_JAR_BYTES\");\n+\/\/        System.out.println(result);\n+\/\/    }\n+\n+    \/**\n+     * Utility method which takes a byte array and converts to byte array\n+     * declaration.  For example:\n+     * <pre>\n+     *     {@code\n+     *        var fooJar = Files.readAllBytes(Path.of(\"foo.jar\"));\n+     *        var result = createByteArray(fooJar, \"FOOBYTES\");\n+     *      }\n+     * <\/pre>\n+     *\n+     * @param bytes A byte array used to create a byte array declaration\n+     * @param name  Name to be used in the byte array declaration\n+     * @return The formatted byte array declaration\n+     *\/\n+    public static String createByteArray(byte[] bytes, String name) {\n+        StringBuilder sb = new StringBuilder(bytes.length * 5);\n+        Formatter fmt = new Formatter(sb);\n+        fmt.format(\"    public static byte[] %s = {\", name);\n+        final int linelen = 8;\n+        for (int i = 0; i < bytes.length; i++) {\n+            if (i % linelen == 0) {\n+                fmt.format(\"%n        \");\n+            }\n+            fmt.format(\" (byte) 0x%x,\", bytes[i] & 0xff);\n+        }\n+        fmt.format(\"%n    };%n\");\n+        return sb.toString();\n+    }\n+\n+    \/**\n+     * Represents an entry in a Zip file. An entry encapsulates a name, a\n+     * compression method, and its contents\/data.\n+     *\/\n+    public static class Entry {\n+        public final String name;\n+        public final int method;\n+        public final byte[] bytes;\n+\n+        public Entry(String name, int method, String contents) {\n+            this.name = name;\n+            this.method = method;\n+            this.bytes = contents.getBytes(StandardCharsets.UTF_8);\n+        }\n+\n+        public static Entry of(String name, int method, String contents) {\n+            return new Entry(name, method, contents);\n+        }\n+    }\n+}\n","filename":"test\/jdk\/java\/util\/zip\/ZipFile\/ZipFileDuplicateEntryTest.java","additions":581,"deletions":0,"binary":false,"changes":581,"status":"added"},{"patch":"@@ -29,0 +29,1 @@\n+import java.awt.Rectangle;\n@@ -30,0 +31,1 @@\n+import java.awt.Toolkit;\n@@ -59,0 +61,4 @@\n+    private static volatile Point location;\n+    \/\/ move away from cursor\n+    private final static int OFFSET_X = -20;\n+    private final static int OFFSET_Y = -20;\n@@ -62,1 +68,1 @@\n-        robot.setAutoDelay(50);\n+        robot.setAutoDelay(100);\n@@ -66,0 +72,1 @@\n+        robot.delay(1000);\n@@ -70,1 +77,1 @@\n-            Point location = frame.getLocation();\n+            location = frame.getLocation();\n@@ -77,0 +84,2 @@\n+        System.out.println(\"scale \" + scale);\n+\n@@ -78,1 +87,0 @@\n-        robot.mousePress(InputEvent.BUTTON1_MASK);\n@@ -80,3 +88,11 @@\n-        Thread.sleep(100);\n-        Color color = robot.getPixelColor(centerX, centerY);\n-        robot.mouseRelease(InputEvent.BUTTON1_MASK);\n+        robot.mousePress(InputEvent.BUTTON1_DOWN_MASK);\n+        robot.waitForIdle();\n+        Color color = robot.getPixelColor(centerX - OFFSET_X, centerY - OFFSET_Y);\n+\n+        Dimension screenSize = Toolkit.getDefaultToolkit().getScreenSize();\n+        Rectangle screen = new Rectangle(0, 0, (int) screenSize.getWidth(), (int) screenSize.getHeight());\n+        BufferedImage img = robot.createScreenCapture(screen);\n+        javax.imageio.ImageIO.write(img, \"png\", new java.io.File(\"image.png\"));\n+\n+        robot.mouseRelease(InputEvent.BUTTON1_DOWN_MASK);\n+\n@@ -86,3 +102,7 @@\n-        if ((scale == 1 && !similar(color, COLOR_1X))\n-                || (scale == 2 && !similar(color, COLOR_2X))) {\n-            throw new RuntimeException(\"Colors are different!\");\n+        if (scale == 1 && !similar(color, COLOR_1X)) {\n+            System.out.println(\"color \" + color + \" COLOR_1X \" + COLOR_1X);\n+            throw new RuntimeException(\"Colors is different for scale=1!\");\n+        }\n+        if (scale == 2 && !similar(color, COLOR_2X)) {\n+            System.out.println(\"color \" + color + \" COLOR_2X \" + COLOR_2X);\n+            throw new RuntimeException(\"Colors is different for scale=2!\");\n@@ -90,0 +110,1 @@\n+        System.out.println(\"Test Passed\");\n@@ -111,0 +132,2 @@\n+        frame.setUndecorated(true);\n+        frame.setLocationRelativeTo(null);\n","filename":"test\/jdk\/javax\/swing\/JButton\/8151303\/PressedIconTest.java","additions":32,"deletions":9,"binary":false,"changes":41,"status":"modified"},{"patch":"@@ -26,0 +26,1 @@\n+import java.awt.Dimension;\n@@ -29,0 +30,2 @@\n+import java.awt.Toolkit;\n+import java.awt.image.BufferedImage;\n@@ -55,0 +58,4 @@\n+     \/\/ move away from cursor\n+    private final static int OFFSET_X = -20;\n+    private final static int OFFSET_Y = -20;\n+\n@@ -69,1 +76,1 @@\n-            robot.setAutoDelay(50);\n+            robot.setAutoDelay(100);\n@@ -82,1 +89,1 @@\n-            robot.mousePress(InputEvent.BUTTON1_MASK);\n+            robot.mousePress(InputEvent.BUTTON1_DOWN_MASK);\n@@ -84,1 +91,1 @@\n-            robot.mouseRelease(InputEvent.BUTTON1_MASK);\n+            robot.mouseRelease(InputEvent.BUTTON1_DOWN_MASK);\n@@ -91,1 +98,13 @@\n-            if (!FRAME_COLOR.equals(robot.getPixelColor(cx, cy))) {\n+            robot.waitForIdle();\n+            Color color = robot.getPixelColor(cx - OFFSET_X, cy - OFFSET_Y);\n+\n+            if (!FRAME_COLOR.equals(color)) {\n+                System.out.println(\"cx \" + cx + \" cy \" + cy);\n+                System.err.println(\"FRAME_COLOR Red: \" + FRAME_COLOR.getRed() + \"; Green: \" + FRAME_COLOR.getGreen() + \"; Blue: \" + FRAME_COLOR.getBlue());\n+                System.err.println(\"Pixel color Red: \" + color.getRed() + \"; Green: \" + color.getGreen() + \"; Blue: \" + color.getBlue());\n+\n+                Dimension screenSize = Toolkit.getDefaultToolkit().getScreenSize();\n+                Rectangle screen = new Rectangle(0, 0, (int) screenSize.getWidth(), (int) screenSize.getHeight());\n+                BufferedImage img = robot.createScreenCapture(screen);\n+                javax.imageio.ImageIO.write(img, \"png\", new java.io.File(\"image.png\"));\n+\n@@ -95,3 +114,5 @@\n-            if (frame != null) {\n-                frame.dispose();\n-            }\n+            SwingUtilities.invokeAndWait(() -> {\n+                if (frame != null) {\n+                    frame.dispose();\n+                }\n+            });\n@@ -99,0 +120,1 @@\n+        System.out.println(\"Test Passed\");\n@@ -103,0 +125,1 @@\n+        System.out.println(\"d3d \" + d3d);\n@@ -141,0 +164,1 @@\n+        frame.setLocationRelativeTo(null);\n","filename":"test\/jdk\/javax\/swing\/JInternalFrame\/8069348\/bug8069348.java","additions":31,"deletions":7,"binary":false,"changes":38,"status":"modified"},{"patch":"@@ -40,1 +40,1 @@\n-public class bug6276188 extends JFrame {\n+public class bug6276188 {\n@@ -44,0 +44,5 @@\n+    private static JFrame testFrame;\n+\n+     \/\/ move away from cursor\n+    private final static int OFFSET_X = -20;\n+    private final static int OFFSET_Y = -20;\n@@ -46,2 +51,3 @@\n-        SynthLookAndFeel lookAndFeel = new SynthLookAndFeel();\n-        lookAndFeel.load(bug6276188.class.getResourceAsStream(\"bug6276188.xml\"), bug6276188.class);\n+        try {\n+            Robot robot = new Robot();\n+            robot.setAutoDelay(100);\n@@ -49,7 +55,3 @@\n-        UIManager.setLookAndFeel(lookAndFeel);\n-        SwingUtilities.invokeAndWait(new Runnable() {\n-            public void run() {\n-                JFrame testFrame = new JFrame();\n-                testFrame.setLayout(new BorderLayout());\n-                testFrame.setDefaultCloseOperation(JFrame.EXIT_ON_CLOSE);\n-                testFrame.add(BorderLayout.CENTER, button = new JButton());\n+            SynthLookAndFeel lookAndFeel = new SynthLookAndFeel();\n+            lookAndFeel.load(bug6276188.class.getResourceAsStream(\"bug6276188.xml\"), bug6276188.class);\n+            UIManager.setLookAndFeel(lookAndFeel);\n@@ -57,4 +59,6 @@\n-                testFrame.setSize(new Dimension(320, 200));\n-                testFrame.setVisible(true);\n-            }\n-        });\n+            SwingUtilities.invokeAndWait(new Runnable() {\n+                public void run() {\n+                    testFrame = new JFrame();\n+                    testFrame.setLayout(new BorderLayout());\n+                    testFrame.setDefaultCloseOperation(JFrame.EXIT_ON_CLOSE);\n+                    testFrame.add(BorderLayout.CENTER, button = new JButton());\n@@ -62,4 +66,5 @@\n-        Robot robot = new Robot();\n-        robot.setAutoDelay(50);\n-        robot.waitForIdle();\n-        robot.delay(200);\n+                    testFrame.setSize(new Dimension(320, 200));\n+                    testFrame.setLocationRelativeTo(null);\n+                    testFrame.setVisible(true);\n+                }\n+            });\n@@ -67,1 +72,2 @@\n-        p = Util.getCenterPoint(button);\n+            robot.waitForIdle();\n+            robot.delay(1000);\n@@ -69,4 +75,2 @@\n-        robot.mouseMove(p.x , p.y);\n-        robot.mousePress(InputEvent.BUTTON1_MASK);\n-        robot.waitForIdle();\n-        robot.delay(1000);\n+            p = Util.getCenterPoint(button);\n+            System.out.println(\"Button center point: \" + p);\n@@ -74,6 +78,23 @@\n-        Color color = robot.getPixelColor(p.x, p.y);\n-        robot.mouseRelease(InputEvent.BUTTON1_MASK);\n-        boolean red = color.getRed() > 0 && color.getGreen() == 0 && color.getBlue() == 0;\n-        if (!red) {\n-            System.err.println(\"Red: \" + color.getRed() + \"; Green: \" + color.getGreen() + \"; Blue: \" + color.getBlue());\n-            throw new RuntimeException(\"Synth ButtonUI does not handle PRESSED & MOUSE_OVER state\");\n+            robot.mouseMove(p.x , p.y);\n+            robot.waitForIdle();\n+            robot.mousePress(InputEvent.BUTTON1_DOWN_MASK);\n+            robot.waitForIdle();\n+\n+            Color color = robot.getPixelColor(p.x - OFFSET_X, p.y - OFFSET_Y);\n+            robot.mouseRelease(InputEvent.BUTTON1_DOWN_MASK);\n+            robot.waitForIdle();\n+            boolean red = color.getRed() > 0 && color.getGreen() == 0 && color.getBlue() == 0;\n+            if (!red) {\n+                System.err.println(\"Red: \" + color.getRed() + \"; Green: \" + color.getGreen() + \"; Blue: \" + color.getBlue());\n+                Dimension screenSize = Toolkit.getDefaultToolkit().getScreenSize();\n+                Rectangle screen = new Rectangle(0, 0, (int) screenSize.getWidth(), (int) screenSize.getHeight());\n+                BufferedImage img = robot.createScreenCapture(screen);\n+                javax.imageio.ImageIO.write(img, \"png\", new java.io.File(\"image.png\"));\n+                throw new RuntimeException(\"Synth ButtonUI does not handle PRESSED & MOUSE_OVER state\");\n+            }\n+        } finally {\n+            SwingUtilities.invokeAndWait(() -> {\n+                if (testFrame != null) {\n+                    testFrame.dispose();\n+                }\n+            });\n","filename":"test\/jdk\/javax\/swing\/plaf\/synth\/SynthButtonUI\/6276188\/bug6276188.java","additions":50,"deletions":29,"binary":false,"changes":79,"status":"modified"},{"patch":"@@ -26,1 +26,1 @@\n- * @bug 4990825\n+ * @bug 4990825 8251155\n","filename":"test\/jdk\/sun\/jvmstat\/monitor\/HostIdentifier\/HostIdentifierCreate.java","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -221,0 +221,14 @@\n+<testcase id=\"31\" HostIdentifierInput=\"12345\">\n+<description>\n+Purely numeric\n+<\/description>\n+<HostIdentifier> \/\/12345 <\/HostIdentifier>\n+<\/testcase>\n+\n+<testcase id=\"32\" HostIdentifierInput=\"12345:123\">\n+<description>\n+Purely numeric\n+<\/description>\n+<HostIdentifier> \/\/12345:123 <\/HostIdentifier>\n+<\/testcase>\n+\n","filename":"test\/jdk\/sun\/jvmstat\/monitor\/HostIdentifier\/testcases","additions":14,"deletions":0,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -848,1 +848,1 @@\n-        gencert(\"ts\", \"-ext eku:critical=ts -validity 500\");\n+        gencert(\"ts\", \"-ext eku:critical=ts -ext ku=nonrep -validity 500\");\n","filename":"test\/jdk\/sun\/security\/tools\/jarsigner\/TimestampCheck.java","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2012, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2012, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -26,1 +26,1 @@\n- * @bug 7180907\n+ * @bug 7180907 8277224\n@@ -63,0 +63,1 @@\n+            new PKCS9Attribute(PKCS9Attribute.SIGNATURE_TIMESTAMP_TOKEN_OID, \"test\".getBytes())\n@@ -65,0 +66,4 @@\n+        \/\/ test PKCS9Attributes.toString(), PKCS9Attributes.getAttributes()\n+        System.out.println(authed);\n+        authed.getAttributes();\n+\n","filename":"test\/jdk\/sun\/security\/x509\/AlgorithmId\/NonStandardNames.java","additions":7,"deletions":2,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -1,46 +0,0 @@\n-\/*\n- * Copyright (C) 2020 THL A29 Limited, a Tencent company. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-import jdk.test.lib.process.OutputAnalyzer;\n-\n-\/*\n- * @test\n- * @bug 8251155\n- * @summary Test host names starting with digits\n- * @library \/test\/lib\n- * @build JpsHelper\n- * @run driver TestJpsHostName\n- *\/\n-public class TestJpsHostName {\n-\n-    public static void main(String[] args) throws Throwable {\n-        testJpsHostName(\"12345\");\n-        testJpsHostName(\"12345:37266\");\n-    }\n-\n-    private static void testJpsHostName(String hostname) throws Exception {\n-        OutputAnalyzer output = JpsHelper.jps(hostname);\n-        output.shouldNotContain(\"Malformed Host Identifier: \" + hostname);\n-    }\n-\n-}\n","filename":"test\/jdk\/sun\/tools\/jps\/TestJpsHostName.java","additions":0,"deletions":46,"binary":false,"changes":46,"status":"deleted"},{"patch":"@@ -47,2 +47,16 @@\n-    private static String getRelease(JPackageCommand cmd) {\n-        return cmd.getArgumentValue(\"--linux-app-release\", () -> \"1\");\n+    private static String getReleaseSuffix(JPackageCommand cmd) {\n+        String value = null;\n+        final PackageType packageType = cmd.packageType();\n+        switch (packageType) {\n+            case LINUX_DEB:\n+                value = Optional.ofNullable(cmd.getArgumentValue(\n+                        \"--linux-app-release\", () -> null)).map(v -> \"-\" + v).orElse(\n+                        \"\");\n+                break;\n+\n+            case LINUX_RPM:\n+                value = \"-\" + cmd.getArgumentValue(\"--linux-app-release\",\n+                        () -> \"1\");\n+                break;\n+        }\n+        return value;\n@@ -77,1 +91,1 @@\n-                format = \"%s_%s-%s_%s\";\n+                format = \"%s_%s%s_%s\";\n@@ -81,1 +95,1 @@\n-                format = \"%s-%s-%s.%s\";\n+                format = \"%s-%s%s.%s\";\n@@ -85,1 +99,1 @@\n-        final String release = getRelease(cmd);\n+        final String releaseSuffix = getReleaseSuffix(cmd);\n@@ -88,1 +102,1 @@\n-        return String.format(format, getPackageName(cmd), version, release,\n+        return String.format(format, getPackageName(cmd), version, releaseSuffix,\n","filename":"test\/jdk\/tools\/jpackage\/helpers\/jdk\/jpackage\/test\/LinuxHelper.java","additions":20,"deletions":6,"binary":false,"changes":26,"status":"modified"},{"patch":"@@ -84,1 +84,1 @@\n-                    \"Expected value of \\\"Version\\\" property is [1.0-1]. Actual value in output package is [1.2.3-R2]\")\n+                    \"Expected value of \\\"Version\\\" property is [1.0]. Actual value in output package is [1.2.3-R2]\")\n","filename":"test\/jdk\/tools\/jpackage\/linux\/LinuxResourceTest.java","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -27,0 +27,1 @@\n+import jdk.jpackage.test.JPackageCommand;\n@@ -50,0 +51,1 @@\n+ * @requires (jpackage.test.SQETest == null)\n@@ -54,0 +56,15 @@\n+\n+\/*\n+ * @test\n+ * @summary jpackage with --linux-app-release\n+ * @library ..\/helpers\n+ * @key jpackagePlatformPackage\n+ * @build jdk.jpackage.test.*\n+ * @build ReleaseTest\n+ * @requires (os.family == \"linux\")\n+ * @requires (jpackage.test.SQETest != null)\n+ * @modules jdk.jpackage\/jdk.jpackage.internal\n+ * @run main\/othervm\/timeout=360 -Xmx512m jdk.jpackage.test.Main\n+ *  --jpt-run=ReleaseTest.test\n+ *\/\n+\n@@ -74,0 +91,13 @@\n+\n+    @Test\n+    public static void testNoExplitRelease() {\n+        new PackageTest()\n+                .forTypes(PackageType.LINUX)\n+                .configureHelloApp()\n+                .addInitializer(JPackageCommand::setFakeRuntime)\n+                .forTypes(PackageType.LINUX_RPM)\n+                .addBundlePropertyVerifier(\"Release\", \"1\")\n+                .forTypes(PackageType.LINUX_DEB)\n+                .addBundlePropertyVerifier(\"Version\", \"1.0\")\n+                .run();\n+    }\n","filename":"test\/jdk\/tools\/jpackage\/linux\/ReleaseTest.java","additions":30,"deletions":0,"binary":false,"changes":30,"status":"modified"},{"patch":"@@ -168,0 +168,1 @@\n+                \"--add-script\",\n","filename":"test\/langtools\/jdk\/javadoc\/doclet\/testHelpOption\/TestHelpOption.java","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2017, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2017, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -26,1 +26,1 @@\n- * @bug 8178067 8192007 8182765 8184205\n+ * @bug 8178067 8192007 8182765 8184205 8277028\n@@ -96,0 +96,20 @@\n+        mb = new ModuleBuilder(tb, \"moduleServiceProviderNoDescription\")\n+                .comment(\"\"\"\n+                    This is another module that provides an implementation of a service.\n+                    @provides pkgService.Service\"\"\")\n+                .requires(\"moduleService\")\n+                .provides(\"pkgService.Service\", \"pkgServiceProviderNoDesc.ServiceProviderNoDescription\")\n+                .classes(\"\/**A Package that has a service provider.*\/ package pkgServiceProviderNoDesc;\")\n+                .classes(\"\"\"\n+                    package pkgServiceProviderNoDesc;\n+                    public class ServiceProviderNoDescription implements pkgService.Service {\n+                        \/**\n+                         * {@inheritDoc}\n+                         *\/\n+                        public void testMethod1() {}\n+                        \/**\n+                         * This is an internal implementation so the documentation will not be seen.\n+                         *\/\n+                        public void testMethod2() {}\n+                    }\"\"\");\n+        mb.write(src);\n@@ -131,1 +151,2 @@\n-                \"--module\", \"moduleService,moduleServiceProvider,moduleServiceUser,moduleServiceUserNoDescription\",\n+                \"--module\", \"moduleService,moduleServiceProvider,moduleServiceProviderNoDescription\",\n+                \"--module\", \"moduleServiceUser,moduleServiceUserNoDescription\",\n@@ -142,0 +163,6 @@\n+        checkOutput(\"moduleServiceProviderNoDescription\/module-summary.html\", true,\n+                \"\"\"\n+                    <div class=\"col-first even-row-color\"><a href=\"..\/moduleService\/pkgService\/Service.ht\\\n+                    ml\" title=\"interface in pkgService\">Service<\/a><\/div>\n+                    <div class=\"col-last even-row-color\">\n+                    <div class=\"block\">A service Interface for service providers.<\/div>\"\"\");\n","filename":"test\/langtools\/jdk\/javadoc\/doclet\/testModules\/TestModuleServices.java","additions":30,"deletions":3,"binary":false,"changes":33,"status":"modified"},{"patch":"@@ -27,2 +27,4 @@\n- * @summary  Test the output for -header, -footer, -nooverview, -nodeprecatedlist, -nonavbar, -notree,\n- *           -stylesheetfile, --main-stylesheet, --add-stylesheet options.\n+ *           8275786\n+ * @summary  Test the output for -header, -footer, -nooverview, -nodeprecatedlist,\n+ *           -nonavbar, -notree, -stylesheetfile, --main-stylesheet, --add-stylesheet,\n+ *           --add-script options.\n@@ -181,0 +183,31 @@\n+    @Test\n+    public void testAdditionalScriptFile() {\n+        javadoc(\"-d\", \"out-additional-script\",\n+                \"--add-script\", new File(testSrc, \"additional-script-1.js\").getAbsolutePath(),\n+                \"--add-script\", new File(testSrc, \"additional-script-2.js\").getAbsolutePath(),\n+                \"-sourcepath\", testSrc,\n+                \"pkg\");\n+        checkExit(Exit.OK);\n+\n+        checkOutput(\"script-dir\/additional-script-1.js\", true, \"Additional script file 1\");\n+        checkOutput(\"script-dir\/additional-script-2.js\", true, \"Additional script file 2\");\n+        checkOutput(\"pkg\/Foo.html\", true,\n+                \"\"\"\n+                    <script type=\"text\/javascript\" src=\"..\/script-dir\/additional-script-1.js\"><\/script>\n+                    <script type=\"text\/javascript\" src=\"..\/script-dir\/additional-script-2.js\"><\/script>\n+                    \"\"\");\n+    }\n+\n+    @Test\n+    public void testInvalidAdditionalScriptFile() {\n+        javadoc(\"-d\", \"out-invalid-additional-script\",\n+                \"--add-script\", new File(testSrc, \"additional-script-3.js\").getAbsolutePath(),\n+                \"-sourcepath\", testSrc,\n+                \"pkg\");\n+        checkExit(Exit.ERROR);\n+\n+        checkOutput(Output.OUT, true,\n+                \"error: File not found:\",\n+                \"additional-script-3.js\");\n+    }\n+\n","filename":"test\/langtools\/jdk\/javadoc\/doclet\/testOptions\/TestOptions.java","additions":35,"deletions":2,"binary":false,"changes":37,"status":"modified"},{"patch":"@@ -0,0 +1,3 @@\n+\/\/ Additional script file 1\n+\n+console.log(\"hello world\");\n","filename":"test\/langtools\/jdk\/javadoc\/doclet\/testOptions\/additional-script-1.js","additions":3,"deletions":0,"binary":false,"changes":3,"status":"added"},{"patch":"@@ -0,0 +1,3 @@\n+\/\/ Additional script file 2\n+\n+console.log(\"goodbye\");\n","filename":"test\/langtools\/jdk\/javadoc\/doclet\/testOptions\/additional-script-2.js","additions":3,"deletions":0,"binary":false,"changes":3,"status":"added"},{"patch":"@@ -65,0 +65,1 @@\n+            \"--add-script\",\n","filename":"test\/langtools\/jdk\/javadoc\/tool\/CheckManPageOptions.java","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -26,1 +26,1 @@\n- * @bug 8270139\n+ * @bug 8270139 8273039\n@@ -40,0 +40,1 @@\n+import static jdk.jshell.Snippet.Status.REJECTED;\n@@ -52,0 +53,7 @@\n+\n+    public void testBrokenName() {\n+        assertEval(\"int strictfp = 0;\",\n+                   DiagCheck.DIAG_ERROR,\n+                   DiagCheck.DIAG_IGNORE,\n+                   ste(MAIN_SNIPPET, NONEXISTENT, REJECTED, false, null));\n+    }\n","filename":"test\/langtools\/jdk\/jshell\/ErrorRecoveryTest.java","additions":9,"deletions":1,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2015, 2018, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2015, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -556,1 +556,1 @@\n-        test(\n+        test(new String[]{\"-R\", \"-Duser.language=en\", \"-R\", \"-Duser.country=US\"},\n","filename":"test\/langtools\/jdk\/jshell\/ToolBasicTest.java","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2016, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2016, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -747,1 +747,2 @@\n-        test(new String[]{\"--startup\", \"DEFAULT\", \"--startup\", \"PRINTING\"},\n+        test(new String[]{\"-R\", \"-Duser.language=en\", \"-R\", \"-Duser.country=US\",\n+                          \"--startup\", \"DEFAULT\", \"--startup\", \"PRINTING\"},\n","filename":"test\/langtools\/jdk\/jshell\/ToolSimpleTest.java","additions":3,"deletions":2,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -0,0 +1,165 @@\n+\/*\n+ * Copyright (c) 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+\/**\n+ * @test\n+ * @bug 8273039\n+ * @summary Verify error recovery in Attr\n+ * @library \/tools\/lib\n+ * @modules jdk.compiler\/com.sun.tools.javac.api\n+ *          jdk.compiler\/com.sun.tools.javac.main\n+ *          jdk.compiler\/com.sun.tools.javac.util\n+ * @build toolbox.ToolBox toolbox.JavacTask\n+ * @run main AttrRecoveryTest\n+*\/\n+\n+import com.sun.source.tree.AnnotationTree;\n+import com.sun.source.tree.CompilationUnitTree;\n+import com.sun.source.tree.ErroneousTree;\n+import com.sun.source.util.TaskEvent;\n+import com.sun.source.util.TaskListener;\n+import com.sun.source.util.TreePath;\n+import com.sun.source.util.TreePathScanner;\n+import com.sun.source.util.Trees;\n+import java.nio.file.Files;\n+import java.nio.file.Path;\n+import java.nio.file.Paths;\n+import java.util.List;\n+import javax.lang.model.element.Element;\n+\n+import toolbox.TestRunner;\n+import toolbox.JavacTask;\n+import toolbox.Task;\n+import toolbox.ToolBox;\n+\n+public class AttrRecoveryTest extends TestRunner {\n+\n+    ToolBox tb;\n+\n+    public static void main(String... args) throws Exception {\n+        new AttrRecoveryTest().runTests();\n+    }\n+\n+    AttrRecoveryTest() {\n+        super(System.err);\n+        tb = new ToolBox();\n+    }\n+\n+    public void runTests() throws Exception {\n+        runTests(m -> new Object[] { Paths.get(m.getName()) });\n+    }\n+\n+    @Test\n+    public void testModifiers(Path base) throws Exception {\n+        record TestCase(String name, String source, String expectedAnnotation, String... errors) {}\n+        TestCase[] tests = new TestCase[] {\n+            new TestCase(\"a\",\n+                         \"\"\"\n+                         public class Test {\n+                             Object i () { return int strictfp @Deprecated = 0; }\n+                         }\n+                         \"\"\",\n+                         \"java.lang.Deprecated\",\n+                         \"Test.java:2:30: compiler.err.dot.class.expected\",\n+                         \"Test.java:2:51: compiler.err.expected4: class, interface, enum, record\",\n+                         \"Test.java:2:26: compiler.err.unexpected.type: kindname.value, kindname.class\",\n+                         \"3 errors\"),\n+            new TestCase(\"b\",\n+                         \"\"\"\n+                         public class Test {\n+                             Object i () { return int strictfp = 0; }\n+                         }\n+                         \"\"\",\n+                         null,\n+                         \"Test.java:2:30: compiler.err.dot.class.expected\",\n+                         \"Test.java:2:39: compiler.err.expected4: class, interface, enum, record\",\n+                         \"Test.java:2:26: compiler.err.unexpected.type: kindname.value, kindname.class\",\n+                         \"3 errors\")\n+        };\n+        for (TestCase test : tests) {\n+            Path current = base.resolve(\"\" + test.name);\n+            Path src = current.resolve(\"src\");\n+            Path classes = current.resolve(\"classes\");\n+            tb.writeJavaFiles(src,\n+                              test.source);\n+\n+            Files.createDirectories(classes);\n+\n+            var log =\n+                    new JavacTask(tb)\n+                        .options(\"-XDrawDiagnostics\",\n+                                 \"-XDshould-stop.at=FLOW\",\n+                                 \"-Xlint:-preview\")\n+                        .outdir(classes)\n+                        .files(tb.findJavaFiles(src))\n+                        .callback(t -> {\n+                            t.addTaskListener(new TaskListener() {\n+                                CompilationUnitTree parsed;\n+                                @Override\n+                                public void finished(TaskEvent e) {\n+                                    switch (e.getKind()) {\n+                                        case PARSE -> parsed = e.getCompilationUnit();\n+                                        case ANALYZE ->\n+                                            checkAnnotationsValid(t, parsed, test.expectedAnnotation);\n+                                    }\n+                                }\n+                            });\n+                        })\n+                        .run(Task.Expect.FAIL, 1)\n+                        .writeAll()\n+                        .getOutputLines(Task.OutputKind.DIRECT);\n+            if (!List.of(test.errors).equals(log)) {\n+                throw new AssertionError(\"Incorrect errors, expected: \" + List.of(test.errors) +\n+                                          \", actual: \" + log);\n+            }\n+        }\n+    }\n+\n+    private void checkAnnotationsValid(com.sun.source.util.JavacTask task,\n+                                       CompilationUnitTree cut,\n+                                       String expected) {\n+        boolean[] foundAnnotation = new boolean[1];\n+        Trees trees = Trees.instance(task);\n+\n+        new TreePathScanner<Void, Void>() {\n+            @Override\n+            public Void visitAnnotation(AnnotationTree node, Void p) {\n+                TreePath typePath = new TreePath(getCurrentPath(), node.getAnnotationType());\n+                Element el = trees.getElement(typePath);\n+                if (el == null || !el.equals(task.getElements().getTypeElement(expected))) {\n+                    throw new AssertionError();\n+                }\n+                foundAnnotation[0] = true;\n+                return super.visitAnnotation(node, p);\n+            }\n+\n+            @Override\n+            public Void visitErroneous(ErroneousTree node, Void p) {\n+                return scan(node.getErrorTrees(), p);\n+            }\n+        }.scan(cut, null);\n+        if (foundAnnotation[0] ^ (expected != null)) {\n+            throw new AssertionError();\n+        }\n+    }\n+}\n","filename":"test\/langtools\/tools\/javac\/attr\/AttrRecoveryTest.java","additions":165,"deletions":0,"binary":false,"changes":165,"status":"added"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2012, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2012, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -28,1 +28,1 @@\n- * @run testng LambdaTranslationTest1\n+ * @run testng\/othervm -Duser.language=en -Duser.country=US LambdaTranslationTest1\n","filename":"test\/langtools\/tools\/javac\/lambda\/lambdaExecution\/LambdaTranslationTest1.java","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2012, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2012, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -28,1 +28,1 @@\n- * @run testng LambdaTranslationTest2\n+ * @run testng\/othervm -Duser.language=en -Duser.country=US LambdaTranslationTest2\n","filename":"test\/langtools\/tools\/javac\/lambda\/lambdaExecution\/LambdaTranslationTest2.java","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -44,1 +44,1 @@\n-        \"DynamicDumpSharedSpaces is unsupported when base CDS archive is not loaded\";\n+        \"-XX:ArchiveClassesAtExit is unsupported when base CDS archive is not loaded\";\n@@ -329,3 +329,1 @@\n-        if (isUnableToMap(output)) {\n-            throw new SkippedException(UnableToMapMsg);\n-        }\n+        checkMappingFailure(output);\n@@ -354,1 +352,1 @@\n-    public static boolean isUnableToMap(OutputAnalyzer output) {\n+    private static String hasUnableToMapMessage(OutputAnalyzer output) {\n@@ -356,3 +354,7 @@\n-        if ((output.getExitValue() == 1) &&\n-            (outStr.contains(MSG_RANGE_NOT_WITHIN_HEAP) || outStr.contains(MSG_DYNAMIC_NOT_SUPPORTED))) {\n-            return true;\n+        if ((output.getExitValue() == 1)) {\n+            if (outStr.contains(MSG_RANGE_NOT_WITHIN_HEAP)) {\n+                return MSG_RANGE_NOT_WITHIN_HEAP;\n+            }\n+            if (outStr.contains(MSG_DYNAMIC_NOT_SUPPORTED)) {\n+                return MSG_DYNAMIC_NOT_SUPPORTED;\n+            }\n@@ -361,1 +363,5 @@\n-        return false;\n+        return null;\n+    }\n+\n+    public static boolean isUnableToMap(OutputAnalyzer output) {\n+        return hasUnableToMapMessage(output) != null;\n@@ -365,2 +371,3 @@\n-        if (isUnableToMap(out)) {\n-            throw new SkippedException(UnableToMapMsg);\n+        String match = hasUnableToMapMessage(out);\n+        if (match != null) {\n+            throw new SkippedException(UnableToMapMsg + \": \" + match);\n@@ -475,4 +482,1 @@\n-        if (isUnableToMap(output)) {\n-            throw new SkippedException(UnableToMapMsg);\n-        }\n-\n+        checkMappingFailure(output);\n","filename":"test\/lib\/jdk\/test\/lib\/cds\/CDSTestUtils.java","additions":19,"deletions":15,"binary":false,"changes":34,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2019, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2019, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -30,0 +30,2 @@\n+import java.net.Inet4Address;\n+import java.net.Inet6Address;\n@@ -31,4 +33,3 @@\n-import java.net.InetSocketAddress;\n-import java.net.Socket;\n-import java.net.SocketException;\n-import java.net.UnknownHostException;\n+import java.net.ProtocolFamily;\n+import java.net.StandardProtocolFamily;\n+import java.nio.channels.SocketChannel;\n@@ -52,13 +53,2 @@\n-        try {\n-            InetAddress loopbackIPv4 = InetAddress.getByAddress(\n-                    new byte[] {0x7F, 0x00, 0x00, 0x01});\n-\n-            InetAddress loopbackIPv6 = InetAddress.getByAddress(\n-                    new byte[] {0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,\n-                                0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x01});\n-\n-            hasIPv4 = runPrivilegedAction(() -> hasAddress(loopbackIPv4));\n-            hasIPv6 = runPrivilegedAction(() -> hasAddress(loopbackIPv6));\n-        } catch (UnknownHostException e) {\n-            throw new AssertionError(e);\n-        }\n+        hasIPv4 = runPrivilegedAction(() -> isSupported(Inet4Address.class));\n+        hasIPv6 = runPrivilegedAction(() -> isSupported(Inet6Address.class));\n@@ -74,3 +64,4 @@\n-    private static boolean hasAddress(InetAddress address) {\n-        try (Socket socket = new Socket()) {\n-            socket.bind(new InetSocketAddress(address, 0));\n+    private static boolean isSupported(Class<? extends InetAddress> addressType) {\n+        ProtocolFamily family = addressType == Inet4Address.class ?\n+                StandardProtocolFamily.INET : StandardProtocolFamily.INET6;\n+        try (var sc = SocketChannel.open(family)) {\n@@ -78,1 +69,1 @@\n-        } catch (SocketException se) {\n+        } catch (IOException | UnsupportedOperationException ex) {\n@@ -80,2 +71,0 @@\n-        } catch (IOException e) {\n-            throw new UncheckedIOException(e);\n","filename":"test\/lib\/jdk\/test\/lib\/net\/IPSupport.java","additions":13,"deletions":24,"binary":false,"changes":37,"status":"modified"},{"patch":"@@ -0,0 +1,52 @@\n+\/*\n+ * Copyright (c) 2021, Amazon.com Inc. or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+package org.openjdk.bench.java.lang;\n+\n+import org.openjdk.jmh.annotations.Benchmark;\n+import org.openjdk.jmh.annotations.BenchmarkMode;\n+import org.openjdk.jmh.annotations.Mode;\n+import org.openjdk.jmh.annotations.OutputTimeUnit;\n+import org.openjdk.jmh.annotations.Threads;\n+\n+import java.util.concurrent.TimeUnit;\n+\n+@BenchmarkMode(Mode.AverageTime)\n+@OutputTimeUnit(TimeUnit.NANOSECONDS)\n+public class ThreadOnSpinWait {\n+    @Benchmark\n+    @Threads(1)\n+    public void testOnSpinWait() {\n+        Thread.onSpinWait();\n+    }\n+\n+    @Benchmark\n+    @Threads(1)\n+    public void testSleep0() throws InterruptedException {\n+        Thread.sleep(0);\n+    }\n+\n+    @Benchmark\n+    @Threads(1)\n+    public void testEmpty() {\n+    }\n+}\n","filename":"test\/micro\/org\/openjdk\/bench\/java\/lang\/ThreadOnSpinWait.java","additions":52,"deletions":0,"binary":false,"changes":52,"status":"added"},{"patch":"@@ -0,0 +1,204 @@\n+\/*\n+ * Copyright (c) 2021, Amazon.com Inc. or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+package org.openjdk.bench.java.lang;\n+\n+import org.openjdk.jmh.annotations.Benchmark;\n+import org.openjdk.jmh.annotations.BenchmarkMode;\n+import org.openjdk.jmh.annotations.Level;\n+import org.openjdk.jmh.annotations.Mode;\n+import org.openjdk.jmh.annotations.OutputTimeUnit;\n+import org.openjdk.jmh.annotations.Param;\n+import org.openjdk.jmh.annotations.Scope;\n+import org.openjdk.jmh.annotations.Setup;\n+import org.openjdk.jmh.annotations.State;\n+import org.openjdk.jmh.annotations.Threads;\n+\n+import org.openjdk.jmh.infra.Blackhole;\n+\n+import java.math.BigInteger;\n+import java.util.Random;\n+import java.util.concurrent.TimeUnit;\n+import java.util.function.BooleanSupplier;\n+\n+\/**\n+ * This microbenchmark models producer-consumer.\n+ *\n+ * The microbenchmark uses two thread: 1 for a producer, 1 for a consumer.\n+ * The microbenchmark uses BigInteger to have latencies of producing\/consuming\n+ * data comparable with synchronization operations.\n+ *\n+ * Thread.onSpinWait is used in a spin loop which is used to avoid heavy locks.\n+ * In the spin loop volatile fields are checked. To reduce overhead accessing them\n+ * they are only checked after a number of iterations.\n+ *\/\n+@BenchmarkMode(Mode.AverageTime)\n+@OutputTimeUnit(TimeUnit.MICROSECONDS)\n+@State(Scope.Benchmark)\n+@Threads(1)\n+public class ThreadOnSpinWaitProducerConsumer {\n+    @Param({\"100\"})\n+    public int maxNum;\n+\n+    @Param({\"125\"})\n+    public int spinNum;\n+\n+    @Param({\"10\"})\n+    public int checkSpinCondAfterIters;\n+\n+    @Param({\"256\"})\n+    public int dataBitLength;\n+\n+    private Thread threadProducer;\n+    private Thread threadConsumer;\n+    private Object monitor;\n+\n+    private BigInteger a;\n+    private BigInteger b;\n+    private Blackhole bh;\n+\n+    private volatile int dataId;\n+    private volatile int seenDataId;\n+\n+    private int producedDataCount;\n+    private int consumedDataCount;\n+\n+    private void produceData() {\n+        if (!isDataSeen()) {\n+            return;\n+        }\n+\n+        b = a.not();\n+        ++dataId;\n+        ++producedDataCount;\n+    }\n+\n+    private void consumeData() {\n+        if (isDataSeen()) {\n+            return;\n+        }\n+        bh.consume(a.equals(b.not()));\n+        seenDataId = dataId;\n+        ++consumedDataCount;\n+    }\n+\n+    private boolean isDataSeen() {\n+        return seenDataId == dataId;\n+    }\n+\n+    private boolean isNewData() {\n+        return seenDataId != dataId;\n+    }\n+\n+    private boolean spinWaitForCondition(int spinNum, BooleanSupplier cond) {\n+        for (int i = 0; i < spinNum; ++i) {\n+            if ((i % checkSpinCondAfterIters) == 0 && cond.getAsBoolean()) {\n+                return true;\n+            }\n+            Thread.onSpinWait();\n+        }\n+        return cond.getAsBoolean();\n+    }\n+\n+    void produce() {\n+        try {\n+            while (dataId < maxNum) {\n+                if (spinWaitForCondition(this.spinNum, this::isDataSeen)) {\n+                    synchronized (monitor) {\n+                        produceData();\n+                        monitor.notify();\n+                    }\n+                } else {\n+                    synchronized (monitor) {\n+                        while (!isDataSeen()) {\n+                            monitor.wait();\n+                        }\n+\n+                        produceData();\n+                        monitor.notify();\n+                    }\n+                }\n+            }\n+        } catch (InterruptedException e) {}\n+    }\n+\n+    void consume() {\n+        try {\n+            for (;;) {\n+                if (spinWaitForCondition(this.spinNum, this::isNewData)) {\n+                    synchronized (monitor) {\n+                         consumeData();\n+                         monitor.notify();\n+                    }\n+                } else {\n+                    synchronized (monitor) {\n+                        while (isDataSeen()) {\n+                            monitor.wait();\n+                        }\n+\n+                        consumeData();\n+                        monitor.notify();\n+                    }\n+                }\n+            }\n+        } catch (InterruptedException e) {}\n+    }\n+\n+    @Setup(Level.Trial)\n+    public void setup01() {\n+        Random rnd = new Random(111);\n+        a = BigInteger.probablePrime(dataBitLength, rnd);\n+        monitor = new Object();\n+    }\n+\n+    @Setup(Level.Invocation)\n+    public void setup02() {\n+        threadProducer = new Thread(this::produce);\n+        threadConsumer = new Thread(this::consume);\n+    }\n+\n+    @Benchmark\n+    public void trial(Blackhole bh) throws Exception {\n+        this.bh = bh;\n+        producedDataCount = 0;\n+        consumedDataCount = 0;\n+        dataId = 0;\n+        seenDataId = 0;\n+        threadProducer.start();\n+        threadConsumer.start();\n+        threadProducer.join();\n+\n+        synchronized (monitor) {\n+            while (!isDataSeen()) {\n+                monitor.wait();\n+            }\n+        }\n+        threadConsumer.interrupt();\n+\n+        if (producedDataCount != maxNum) {\n+            throw new RuntimeException(\"Produced: \" + producedDataCount + \". Expected: \" + maxNum);\n+        }\n+        if (producedDataCount != consumedDataCount) {\n+            throw new RuntimeException(\"produced != consumed: \" + producedDataCount + \" != \" + consumedDataCount);\n+        }\n+    }\n+}\n","filename":"test\/micro\/org\/openjdk\/bench\/java\/lang\/ThreadOnSpinWaitProducerConsumer.java","additions":204,"deletions":0,"binary":false,"changes":204,"status":"added"},{"patch":"@@ -0,0 +1,88 @@\n+\/*\n+ * Copyright (c) 2021, Red Hat Inc. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+package org.openjdk.bench.java.lang;\n+\n+import org.openjdk.jmh.annotations.Benchmark;\n+import org.openjdk.jmh.annotations.BenchmarkMode;\n+import org.openjdk.jmh.annotations.Level;\n+import org.openjdk.jmh.annotations.Mode;\n+import org.openjdk.jmh.annotations.OutputTimeUnit;\n+import org.openjdk.jmh.annotations.Param;\n+import org.openjdk.jmh.annotations.Scope;\n+import org.openjdk.jmh.annotations.Setup;\n+import org.openjdk.jmh.annotations.State;\n+\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.atomic.AtomicInteger;\n+\n+@BenchmarkMode(Mode.AverageTime)\n+@OutputTimeUnit(TimeUnit.MILLISECONDS)\n+@State(Scope.Benchmark)\n+public class ThreadOnSpinWaitSharedCounter {\n+    @Param({\"1000000\"})\n+    public int maxNum;\n+\n+    @Param({\"4\"})\n+    public int threadCount;\n+\n+    AtomicInteger theCounter;\n+\n+    Thread threads[];\n+\n+    void work() {\n+        for (;;) {\n+            int prev = theCounter.get();\n+            if (prev >= maxNum) {\n+                break;\n+            }\n+            if (theCounter.compareAndExchange(prev, prev + 1) != prev) {\n+                Thread.onSpinWait();\n+            }\n+        }\n+    }\n+\n+    @Setup(Level.Trial)\n+    public void foo() {\n+        theCounter = new AtomicInteger();\n+    }\n+\n+    @Setup(Level.Invocation)\n+    public void setup() {\n+        theCounter.set(0);\n+        threads = new Thread[threadCount];\n+\n+        for (int i = 0; i < threads.length; i++) {\n+            threads[i] = new Thread(this::work);\n+        }\n+    }\n+\n+    @Benchmark\n+    public void trial() throws Exception {\n+        for (int i = 0; i < threads.length; i++) {\n+            threads[i].start();\n+        }\n+        for (int i = 0; i < threads.length; i++) {\n+            threads[i].join();\n+        }\n+    }\n+}\n","filename":"test\/micro\/org\/openjdk\/bench\/java\/lang\/ThreadOnSpinWaitSharedCounter.java","additions":88,"deletions":0,"binary":false,"changes":88,"status":"added"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2019, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -198,0 +198,28 @@\n+    @Benchmark\n+    public void convert_i2f() {\n+        for (int i = 0; i < COUNT; i++) {\n+            resF[i] = (float) ints[i];\n+        }\n+    }\n+\n+    @Benchmark\n+    public void convert_f2i() {\n+        for (int i = 0; i < COUNT; i++) {\n+            resI[i] = (int) floats[i];\n+        }\n+    }\n+\n+    @Benchmark\n+    public void convert_l2d() {\n+        for (int i = 0; i < COUNT; i++) {\n+            resD[i] = (double) longs[i];\n+        }\n+    }\n+\n+    @Benchmark\n+    public void convert_d2l() {\n+        for (int i = 0; i < COUNT; i++) {\n+            resL[i] = (long) doubles[i];\n+        }\n+    }\n+\n","filename":"test\/micro\/org\/openjdk\/bench\/vm\/compiler\/TypeVectorOperations.java","additions":29,"deletions":1,"binary":false,"changes":30,"status":"modified"},{"patch":"@@ -0,0 +1,138 @@\n+\/*\n+ * Copyright (c) 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+package org.openjdk.bench.vm.compiler;\n+\n+import org.openjdk.jmh.annotations.Benchmark;\n+import org.openjdk.jmh.annotations.BenchmarkMode;\n+import org.openjdk.jmh.annotations.Fork;\n+import org.openjdk.jmh.annotations.Level;\n+import org.openjdk.jmh.annotations.Mode;\n+import org.openjdk.jmh.annotations.OutputTimeUnit;\n+import org.openjdk.jmh.annotations.Scope;\n+import org.openjdk.jmh.annotations.Setup;\n+import org.openjdk.jmh.annotations.State;\n+import org.openjdk.jmh.infra.Blackhole;\n+import java.util.concurrent.TimeUnit;\n+\n+@BenchmarkMode(Mode.AverageTime)\n+@OutputTimeUnit(TimeUnit.NANOSECONDS)\n+@Fork(2)\n+@State(Scope.Thread)\n+public class UnsignedComparison {\n+    private static final int ITERATIONS = 1000;\n+\n+    private static final int CONST_OPERAND = 4;\n+    private static final int INT_MIN = Integer.MIN_VALUE;\n+    private static final long LONG_MIN = Long.MIN_VALUE;\n+\n+    int arg0 = 0, arg1 = 4;\n+\n+    @Setup(Level.Invocation)\n+    public void toggle() {\n+        arg0 = (arg0 + 1) & 7;\n+    }\n+\n+    @Benchmark\n+    public void intVarDirect(Blackhole bh) {\n+        for (int i = 0; i < ITERATIONS; i++) {\n+            bh.consume(arg0 + INT_MIN < arg1 + INT_MIN);\n+        }\n+    }\n+\n+    @Benchmark\n+    public void intVarLibLT(Blackhole bh) {\n+        for (int i = 0; i < ITERATIONS; i++) {\n+            bh.consume(Integer.compareUnsigned(arg0, arg1) < 0);\n+        }\n+    }\n+\n+    @Benchmark\n+    public void intVarLibGT(Blackhole bh) {\n+        for (int i = 0; i < ITERATIONS; i++) {\n+            bh.consume(Integer.compareUnsigned(arg0, arg1) > 0);\n+        }\n+    }\n+\n+    @Benchmark\n+    public void intConDirect(Blackhole bh) {\n+        for (int i = 0; i < ITERATIONS; i++) {\n+            bh.consume(arg0 + INT_MIN < CONST_OPERAND + INT_MIN);\n+        }\n+    }\n+\n+    @Benchmark\n+    public void intConLibLT(Blackhole bh) {\n+        for (int i = 0; i < ITERATIONS; i++) {\n+            bh.consume(Integer.compareUnsigned(arg0, CONST_OPERAND) < 0);\n+        }\n+    }\n+\n+    @Benchmark\n+    public void intConLibGT(Blackhole bh) {\n+        for (int i = 0; i < ITERATIONS; i++) {\n+            bh.consume(Integer.compareUnsigned(arg0, CONST_OPERAND) > 0);\n+        }\n+    }\n+\n+    @Benchmark\n+    public void longVarDirect(Blackhole bh) {\n+        for (int i = 0; i < ITERATIONS; i++) {\n+            bh.consume(arg0 + LONG_MIN < arg1 + LONG_MIN);\n+        }\n+    }\n+\n+    @Benchmark\n+    public void longVarLibLT(Blackhole bh) {\n+        for (int i = 0; i < ITERATIONS; i++) {\n+            bh.consume(Long.compareUnsigned(arg0, arg1) < 0);\n+        }\n+    }\n+\n+    @Benchmark\n+    public void longVarLibGT(Blackhole bh) {\n+        for (int i = 0; i < ITERATIONS; i++) {\n+            bh.consume(Long.compareUnsigned(arg0, arg1) > 0);\n+        }\n+    }\n+\n+    @Benchmark\n+    public void longConDirect(Blackhole bh) {\n+        for (int i = 0; i < ITERATIONS; i++) {\n+            bh.consume(arg0 + LONG_MIN < CONST_OPERAND + LONG_MIN);\n+        }\n+    }\n+\n+    @Benchmark\n+    public void longConLibLT(Blackhole bh) {\n+        for (int i = 0; i < ITERATIONS; i++) {\n+            bh.consume(Long.compareUnsigned(arg0, CONST_OPERAND) < 0);\n+        }\n+    }\n+\n+    @Benchmark\n+    public void longConLibGT(Blackhole bh) {\n+        for (int i = 0; i < ITERATIONS; i++) {\n+            bh.consume(Long.compareUnsigned(arg0, CONST_OPERAND) > 0);\n+        }\n+    }\n+}\n","filename":"test\/micro\/org\/openjdk\/bench\/vm\/compiler\/UnsignedComparison.java","additions":138,"deletions":0,"binary":false,"changes":138,"status":"added"}]}