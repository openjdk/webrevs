{"files":[{"patch":"@@ -190,0 +190,1 @@\n+SIMDSORT_BASE_DIR := $(TOPDIR)\/src\/java.base\/linux\/native\/libsimdsort\n@@ -199,0 +200,1 @@\n+      SRC := $(SIMDSORT_BASE_DIR)\/x86, \\\n@@ -207,0 +209,17 @@\n+ifeq ($(call isTargetOs, linux)+$(call isTargetCpu, aarch64)+$(INCLUDE_COMPILER2)+$(filter $(TOOLCHAIN_TYPE), gcc), true+true+true+gcc)\n+  $(eval $(call SetupJdkLibrary, BUILD_LIBSIMD_SORT, \\\n+      NAME := simdsort, \\\n+      TOOLCHAIN := TOOLCHAIN_LINK_CXX, \\\n+      OPTIMIZATION := HIGH, \\\n+      SRC := $(SIMDSORT_BASE_DIR)\/aarch64, \\\n+      CFLAGS := $(CFLAGS_JDKLIB) -march=armv8.2-a+sve, \\\n+      CXXFLAGS := $(CXXFLAGS_JDKLIB) -march=armv8.2-a+sve -std=c++17, \\\n+      LDFLAGS := $(LDFLAGS_JDKLIB) \\\n+          $(call SET_SHARED_LIBRARY_ORIGIN), \\\n+      LIBS := $(LIBCXX), \\\n+      DISABLED_WARNINGS_gcc := unused-variable, \\\n+      LIBS_linux := -lc -lm -ldl, \\\n+  ))\n+\n+  TARGETS += $(BUILD_LIBSIMD_SORT)\n+endif\n","filename":"make\/modules\/java.base\/Lib.gmk","additions":19,"deletions":0,"binary":false,"changes":19,"status":"modified"},{"patch":"@@ -130,1 +130,3 @@\n-\n+  product(bool, UseSVELibSimdSortForFP, false, EXPERIMENTAL,            \\\n+          \"Use SVE-based LibSimdSort for float type on SVE supporting \" \\\n+          \"machines\")                                                   \\\n","filename":"src\/hotspot\/cpu\/aarch64\/globals_aarch64.hpp","additions":3,"deletions":1,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -200,0 +200,7 @@\n+    \/\/ SIMD sort is supported only on SVE machines\n+    if (VM_Version::supports_sve()) {\n+      \/\/ Currently, only T_INT and T_FLOAT types are supported.\n+      \/\/ However, T_FLOAT is supported only if the experimental\n+      \/\/ flag - UseSVELibSimdSortForFP is enabled.\n+      return (bt == T_INT || (bt == T_FLOAT && UseSVELibSimdSortForFP));\n+    }\n","filename":"src\/hotspot\/cpu\/aarch64\/matcher_aarch64.hpp","additions":7,"deletions":0,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -11876,0 +11876,4 @@\n+    \/\/ Load sve_sort library on supported hardware to enable SIMD sort and partition intrinsics\n+    if (VM_Version::supports_sve()) {\n+      (void)StubRoutines::try_load_simdsort(\"sve_sort\", \"sve_partition\");\n+    }\n","filename":"src\/hotspot\/cpu\/aarch64\/stubGenerator_aarch64.cpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -4306,16 +4306,4 @@\n-    void *libsimdsort = nullptr;\n-    char ebuf_[1024];\n-    char dll_name_simd_sort[JVM_MAXPATHLEN];\n-    if (os::dll_locate_lib(dll_name_simd_sort, sizeof(dll_name_simd_sort), Arguments::get_dll_dir(), \"simdsort\")) {\n-      libsimdsort = os::dll_load(dll_name_simd_sort, ebuf_, sizeof ebuf_);\n-    }\n-    \/\/ Get addresses for SIMD sort and partition routines\n-    if (libsimdsort != nullptr) {\n-      log_info(library)(\"Loaded library %s, handle \" INTPTR_FORMAT, JNI_LIB_PREFIX \"simdsort\" JNI_LIB_SUFFIX, p2i(libsimdsort));\n-\n-      os::snprintf_checked(ebuf_, sizeof(ebuf_), VM_Version::supports_avx512_simd_sort() ? \"avx512_sort\" : \"avx2_sort\");\n-      StubRoutines::_array_sort = (address)os::dll_lookup(libsimdsort, ebuf_);\n-\n-      os::snprintf_checked(ebuf_, sizeof(ebuf_), VM_Version::supports_avx512_simd_sort() ? \"avx512_partition\" : \"avx2_partition\");\n-      StubRoutines::_array_partition = (address)os::dll_lookup(libsimdsort, ebuf_);\n-    }\n+    const bool use_avx512 = VM_Version::supports_avx512_simd_sort();\n+    const char* sort_sym      = use_avx512 ? \"avx512_sort\"      : \"avx2_sort\";\n+    const char* partition_sym = use_avx512 ? \"avx512_partition\" : \"avx2_partition\";\n+    (void)StubRoutines::try_load_simdsort(sort_sym, partition_sym);\n","filename":"src\/hotspot\/cpu\/x86\/stubGenerator_x86_64.cpp","additions":4,"deletions":16,"binary":false,"changes":20,"status":"modified"},{"patch":"@@ -472,0 +472,32 @@\n+bool StubRoutines::try_load_simdsort(const char* sort_sym, const char* partition_sym) {\n+  void* libsimdsort = nullptr;\n+  char ebuf_[1024];\n+  char dll_name_simd_sort[JVM_MAXPATHLEN];\n+\n+  if (os::dll_locate_lib(dll_name_simd_sort, sizeof(dll_name_simd_sort),\n+                         Arguments::get_dll_dir(), \"simdsort\")) {\n+    libsimdsort = os::dll_load(dll_name_simd_sort, ebuf_, sizeof ebuf_);\n+  }\n+\n+  if (libsimdsort == nullptr) {\n+    return false;\n+  }\n+\n+  \/\/ Get addresses for SIMD sort and partition routines\n+  log_info(library)(\"Loaded library %s, handle \" INTPTR_FORMAT,\n+                    JNI_LIB_PREFIX \"simdsort\" JNI_LIB_SUFFIX, p2i(libsimdsort));\n+  address sort_addr      = (address)os::dll_lookup(libsimdsort, sort_sym);\n+  address partition_addr = (address)os::dll_lookup(libsimdsort, partition_sym);\n+\n+  if (sort_addr == nullptr || partition_addr == nullptr) {\n+    log_warning(library)(\"libsimdsort missing symbols: %s=\" INTPTR_FORMAT \", %s=\" INTPTR_FORMAT,\n+                sort_sym, p2i(sort_addr), partition_sym, p2i(partition_addr));\n+    \/\/ If either of the addresses are null, return false.\n+    return false;\n+  }\n+\n+  StubRoutines::_array_sort = sort_addr;\n+  StubRoutines::_array_partition = partition_addr;\n+  return true;\n+}\n+\n","filename":"src\/hotspot\/share\/runtime\/stubRoutines.cpp","additions":32,"deletions":0,"binary":false,"changes":32,"status":"modified"},{"patch":"@@ -31,0 +31,1 @@\n+#include \"runtime\/arguments.hpp\"\n@@ -365,0 +366,3 @@\n+  \/\/ SIMD sort support. This method resolves the symbols - sort_sym, partition_sym\n+  \/\/ and on success sets the StubRoutines::_array_sort\/_array_partition and returns true.\n+  static bool try_load_simdsort(const char* sort_sym, const char* partition_sym);\n","filename":"src\/hotspot\/share\/runtime\/stubRoutines.hpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -0,0 +1,54 @@\n+\/*\n+ * Copyright (c) 2021, 2023, Intel Corporation. All rights reserved.\n+ * Copyright (c) 2021 Serge Sans Paille. All rights reserved.\n+ * Copyright 2025 Arm Limited and\/or its affiliates.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef AARCH64_SVE_PIVOT_SELECTION_HPP\n+#define AARCH64_SVE_PIVOT_SELECTION_HPP\n+\n+#include <algorithm>\n+#include \"sve-config.hpp\"\n+\n+\/* <TODO> The current pivot selection method follows median-of-three method.\n+ * Possible improvements could be the usage of sorting network (Compare and exchange sorting)\n+ * for larger arrays.\n+ *\/\n+\n+template <typename vtype, typename type_t>\n+static inline type_t get_pivot_blocks(type_t* arr, const arrsize_t left, const arrsize_t right) {\n+  const arrsize_t len = right - left;\n+  if (len < 64) return arr[left];\n+\n+  const arrsize_t mid = left + (len \/ 2);\n+  const type_t a = arr[left];\n+  const type_t b = arr[mid];\n+  const type_t c = arr[right - 1];\n+\n+  const type_t min_ab = std::min(a, b);\n+  const type_t max_ab = std::max(a, b);\n+\n+  return std::min(max_ab, std::max(min_ab, c));\n+}\n+\n+#endif \/\/ AARCH64_SVE_PIVOT_SELECTION_HPP\n","filename":"src\/java.base\/linux\/native\/libsimdsort\/aarch64\/pivot-selection.hpp","additions":54,"deletions":0,"binary":false,"changes":54,"status":"added"},{"patch":"@@ -0,0 +1,40 @@\n+\/*\n+ * Copyright (c) 2023 Intel Corporation. All rights reserved.\n+ * Copyright 2025 Arm Limited and\/or its affiliates.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef SIMDSORT_SUPPORT_HPP\n+#define SIMDSORT_SUPPORT_HPP\n+#include <stdio.h>\n+#include <stdlib.h>\n+\n+#undef assert\n+#define assert(cond, msg) { if (!(cond)) { fprintf(stderr, \"assert fails %s %d: %s\\n\", __FILE__, __LINE__, msg); abort(); }}\n+\n+\/\/ GCC >= 10.1 is required for a full support of ARM SVE ACLE intrinsics (which also includes the header file - arm_sve.h)\n+#if defined(__aarch64__) && defined(_LP64) && defined(__GNUC__) && \\\n+    ((__GNUC__ > 10) || (__GNUC__ == 10 && __GNUC_MINOR__ >= 1))\n+#define __SIMDSORT_SUPPORTED_LINUX\n+#endif\n+\n+#endif \/\/SIMDSORT_SUPPORT_HPP\n","filename":"src\/java.base\/linux\/native\/libsimdsort\/aarch64\/simdsort-support.hpp","additions":40,"deletions":0,"binary":false,"changes":40,"status":"added"},{"patch":"@@ -0,0 +1,518 @@\n+\/*\n+ * Copyright (c) 2021, 2023, Intel Corporation. All rights reserved.\n+ * Copyright (c) 2021 Serge Sans Paille. All rights reserved.\n+ * Copyright 2025 Arm Limited and\/or its affiliates.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef AARCH64_SVE_COMMON_QSORT_HPP\n+#define AARCH64_SVE_COMMON_QSORT_HPP\n+#include <algorithm>\n+#include <cmath>\n+#include <cstring>\n+#include <utility>\n+\n+#include \"sve-config.hpp\"\n+#include \"classfile_constants.h\"\n+#include \"simdsort-support.hpp\"\n+#include \"sve-qsort.hpp\"\n+#include \"pivot-selection.hpp\"\n+#include \"sve-oet-sort.hpp\"\n+\n+template <typename vtype, typename T = typename vtype::type_t>\n+bool sve_comparison_func_ge(const T &a, const T &b) {\n+    return a < b;\n+}\n+\n+template <typename vtype, typename T = typename vtype::type_t>\n+bool sve_comparison_func_gt(const T &a, const T &b) {\n+    return a <= b;\n+}\n+\n+\/*\n+ * Partitions a single SIMD vector based on a pivot and returns the number\n+ * of lanes greater than or equal to the pivot.\n+ *\/\n+template <typename vtype, typename type_t,\n+          typename reg_t = typename vtype::reg_t>\n+SVE_SORT_INLINE arrsize_t partition_vec(type_t *l_store, type_t *r_store,\n+                                        const reg_t curr_vec,\n+                                        const reg_t pivot_vec,\n+                                        reg_t &smallest_vec,\n+                                        reg_t &biggest_vec, bool use_gt) {\n+    typename vtype::opmask_t mask;\n+    if (use_gt) {\n+        mask = vtype::gt(curr_vec, pivot_vec);\n+    } else {\n+        mask = vtype::ge(curr_vec, pivot_vec);\n+    }\n+\n+    int amount_ge_pivot = vtype::double_compressstore(l_store, r_store, mask, curr_vec);\n+\n+    smallest_vec = vtype::min(curr_vec, smallest_vec);\n+    biggest_vec  = vtype::max(curr_vec, biggest_vec);\n+\n+    return amount_ge_pivot;\n+}\n+\n+\/*\n+ * Partition an array based on the pivot and returns the index of the\n+ * first element that is greater than or equal to the pivot.\n+ *\/\n+template <typename vtype, typename type_t>\n+SVE_SORT_INLINE arrsize_t sve_vect_partition_(type_t *arr, arrsize_t left,\n+                                              arrsize_t right, type_t pivot,\n+                                              type_t *smallest,\n+                                              type_t *biggest,\n+                                              bool use_gt) {\n+    auto comparison_func = use_gt ? sve_comparison_func_gt<vtype> : sve_comparison_func_ge<vtype>;\n+\n+    \/\/ Store the number of lanes in a local variable\n+    const arrsize_t num_lanes = vtype::numlanes();\n+\n+    \/* make array length divisible by num_lanes, shortening the array *\/\n+    for (int32_t i = (right - left) % num_lanes; i > 0; --i) {\n+        *smallest = std::min(*smallest, arr[left], comparison_func);\n+        *biggest = std::max(*biggest, arr[left], comparison_func);\n+\n+        if (!comparison_func(arr[left], pivot)) {\n+            std::swap(arr[left], arr[--right]);\n+        } else {\n+            ++left;\n+        }\n+    }\n+\n+    if (left == right)\n+        return left; \/* less than num_lanes elements in the array *\/\n+\n+    using reg_t = typename vtype::reg_t;\n+\n+    reg_t pivot_vec = vtype::set1(pivot);\n+    reg_t min_vec = vtype::set1(*smallest);\n+    reg_t max_vec = vtype::set1(*biggest);\n+\n+    \/\/ If there is only num_lanes worth of elements to be sorted\n+    if (right - left == num_lanes) {\n+        reg_t vec = vtype::loadu(arr + left);\n+        arrsize_t l_store = left;\n+        arrsize_t r_store = l_store;\n+\n+        arrsize_t amount_ge_pivot = partition_vec<vtype>(arr + l_store,\n+                                                         arr + r_store,\n+                                                         vec, pivot_vec, min_vec, max_vec, use_gt);\n+\n+        l_store  += (num_lanes - amount_ge_pivot);\n+        *smallest = vtype::reducemin(min_vec);\n+        *biggest  = vtype::reducemax(max_vec);\n+\n+        return l_store;\n+    }\n+\n+    \/\/ first and last num_lanes values are partitioned at the end\n+    reg_t vec_left = vtype::loadu(arr + left);\n+    reg_t vec_right = vtype::loadu(arr + (right - num_lanes));\n+\n+    \/\/ store points of the vectors\n+    arrsize_t l_store = left;\n+    arrsize_t r_store = right - num_lanes;\n+\n+    \/\/ indices for loading the elements\n+    left  += num_lanes;\n+    right -= num_lanes;\n+\n+    while (right - left != 0) {\n+        reg_t curr_vec;\n+        \/*\n+         * if fewer elements are stored on the right side of the array,\n+         * then next elements are loaded from the right side,\n+         * otherwise from the left side\n+         *\/\n+        if ((r_store + num_lanes) - right < left - l_store) {\n+            right -= num_lanes;\n+            curr_vec = vtype::loadu(arr + right);\n+        } else {\n+            curr_vec = vtype::loadu(arr + left);\n+            left += num_lanes;\n+        }\n+        \/\/ partition the current vector and save it on both sides of the array\n+        arrsize_t amount_ge_pivot = partition_vec<vtype>(arr + l_store,\n+                                                         arr + r_store,\n+                                                         curr_vec, pivot_vec, min_vec, max_vec, use_gt);\n+        l_store += (num_lanes - amount_ge_pivot);\n+        r_store -= amount_ge_pivot;\n+    }\n+\n+    \/* partition and save vec_left and vec_right *\/\n+    arrsize_t amount_ge_pivot = partition_vec<vtype>(arr + l_store,\n+                                                     arr + r_store,\n+                                                     vec_left, pivot_vec, min_vec, max_vec, use_gt);\n+    l_store += (num_lanes - amount_ge_pivot);\n+    r_store -= amount_ge_pivot;\n+\n+\n+    amount_ge_pivot = partition_vec<vtype>(arr + l_store,\n+                                           arr + r_store,\n+                                           vec_right, pivot_vec, min_vec, max_vec, use_gt);\n+    l_store += (num_lanes - amount_ge_pivot);\n+    r_store -= amount_ge_pivot;\n+\n+    *smallest = vtype::reducemin(min_vec);\n+    *biggest  = vtype::reducemax(max_vec);\n+\n+    return l_store;\n+}\n+\n+\/\/ Process a single vector for partitioning\n+template <typename vtype, typename type_t>\n+SVE_SORT_INLINE void sve_partition_single_vec(type_t* arr,\n+                                              arrsize_t& l_store,\n+                                              arrsize_t& r_store,\n+                                              typename vtype::reg_t v,\n+                                              typename vtype::reg_t pivot_vec,\n+                                              typename vtype::reg_t& min_vec,\n+                                              typename vtype::reg_t& max_vec,\n+                                              bool use_gt, arrsize_t num_lanes) {\n+    arrsize_t amount_ge_pivot = partition_vec<vtype>(arr + l_store,\n+                                                     arr + r_store,\n+                                                     v, pivot_vec, min_vec, max_vec, use_gt);\n+\n+    l_store += num_lanes - amount_ge_pivot;\n+    r_store -= amount_ge_pivot;\n+}\n+\n+\/\/ Unrolled version of sve_vect_partition_() with an UNROLL_FACTOR of either 2 or 4\n+\/\/ The UNROLL_FACTOR is 2 if the vector length <= 16B and it is 4 if the vector length > 16B\n+template <typename vtype, typename type_t, int UNROLL_FACTOR>\n+SVE_SORT_INLINE arrsize_t\n+sve_partition_unrolled(type_t* arr, arrsize_t left, arrsize_t right,\n+                       type_t pivot, type_t* smallest, type_t* biggest, bool use_gt) {\n+    static_assert(UNROLL_FACTOR == 2 || UNROLL_FACTOR == 4, \"unsupported unroll factor\");\n+\n+    const arrsize_t num_lanes = vtype::numlanes();\n+\n+    if constexpr (UNROLL_FACTOR == 0) {\n+        return sve_vect_partition_<vtype, type_t>(arr, left, right, pivot, smallest, biggest, use_gt);\n+    }\n+\n+    \/\/ use regular partition routine for small arrays\n+    if (right - left < 3 * UNROLL_FACTOR * num_lanes) {\n+        return sve_vect_partition_<vtype, type_t>(arr, left, right, pivot, smallest, biggest, use_gt);\n+    }\n+\n+    auto comparison_func = use_gt ? sve_comparison_func_gt<vtype>\n+                                  : sve_comparison_func_ge<vtype>;\n+\n+    \/\/ make array length divisible by num_lanes, shortening the array\n+    for (int32_t i = (right - left) % num_lanes; i > 0; --i) {\n+        *smallest = std::min(*smallest, arr[left], comparison_func);\n+        *biggest  = std::max(*biggest,  arr[left], comparison_func);\n+        if (!comparison_func(arr[left], pivot)) {\n+            std::swap(arr[left], arr[--right]);\n+        } else {\n+            ++left;\n+        }\n+    }\n+\n+    arrsize_t l_store = left;\n+    arrsize_t r_store = right - num_lanes;\n+\n+    using reg_t = typename vtype::reg_t;\n+    reg_t pivot_vec = vtype::set1(pivot);\n+    reg_t min_vec   = vtype::set1(*smallest);\n+    reg_t max_vec   = vtype::set1(*biggest);\n+\n+    \/* Calculate and load more registers to make the rest of the array a\n+     * multiple of num_unroll. These registers will be partitioned at the very\n+     * end. *\/\n+    int vecsToPartition = ((right - left) \/ num_lanes) % UNROLL_FACTOR;\n+\n+#define SVE_UNROLL_APPLY(OP)                                                     \\\n+    do {                                                                         \\\n+        if constexpr (UNROLL_FACTOR >= 1) { OP(0); }                             \\\n+        if constexpr (UNROLL_FACTOR >= 2) { OP(1); }                             \\\n+        if constexpr (UNROLL_FACTOR >= 3) { OP(2); }                             \\\n+        if constexpr (UNROLL_FACTOR >= 4) { OP(3); }                             \\\n+    } while (false)\n+\n+#define SVE_DECLARE_REG_SET(NAME, INIT)                                          \\\n+    [[maybe_unused]] reg_t NAME##0 = (INIT);                                     \\\n+    [[maybe_unused]] reg_t NAME##1 = NAME##0;                                    \\\n+    [[maybe_unused]] reg_t NAME##2 = NAME##0;                                    \\\n+    [[maybe_unused]] reg_t NAME##3 = NAME##0\n+\n+#define SVE_DECLARE_REG_SET_UNINIT(NAME)                                         \\\n+    reg_t NAME##0;                                                               \\\n+    reg_t NAME##1;                                                               \\\n+    reg_t NAME##2;                                                               \\\n+    reg_t NAME##3\n+\n+#define SVE_REG(NAME, IDX) NAME##IDX\n+\n+#define SVE_PARTITION_ONE(REG)                                                   \\\n+    sve_partition_single_vec<vtype, type_t>(arr, l_store, r_store,               \\\n+                                            REG, pivot_vec, min_vec, max_vec,    \\\n+                                            use_gt, num_lanes)\n+\n+#define SVE_LOAD_BLOCK_FROM(BASE_PTR, NAME, I)                                   \\\n+    SVE_REG(NAME, I) = vtype::loadu((BASE_PTR) + (I) * num_lanes)\n+\n+#define SVE_LOAD_TAIL(I)                                                         \\\n+    do {                                                                         \\\n+        if (vecsToPartition > (I)) {                                             \\\n+            SVE_LOAD_BLOCK_FROM(arr + left, align_vec, I);                       \\\n+        }                                                                        \\\n+    } while(false)\n+\n+#define SVE_LOAD_LEFT(I)                                                         \\\n+    SVE_LOAD_BLOCK_FROM(arr + left, left_vec, I)\n+\n+#define SVE_LOAD_RIGHT(I)                                                        \\\n+    SVE_LOAD_BLOCK_FROM(arr + right_load_start, right_vec, I)\n+\n+#define SVE_LOAD_BATCH_FROM_RIGHT(I)                                             \\\n+    SVE_LOAD_BLOCK_FROM(arr + right, curr_vec, I)\n+\n+#define SVE_LOAD_BATCH_FROM_LEFT(I)                                              \\\n+    SVE_LOAD_BLOCK_FROM(arr + left, curr_vec, I)\n+\n+#define SVE_PARTITION_BATCH(I)                                                   \\\n+    SVE_PARTITION_ONE(SVE_REG(curr_vec, I))\n+#define SVE_PARTITION_LEFT(I) SVE_PARTITION_ONE(SVE_REG(left_vec, I))\n+#define SVE_PARTITION_RIGHT(I) SVE_PARTITION_ONE(SVE_REG(right_vec, I))\n+#define SVE_PARTITION_TAIL(I)                                                    \\\n+    do {                                                                         \\\n+        if (vecsToPartition > (I)) {                                             \\\n+            SVE_PARTITION_ONE(SVE_REG(align_vec, I));                            \\\n+        }                                                                        \\\n+    } while(false)\n+\n+    \/\/ Initialize the vectors to something arbitrary which will be overwritten when\n+    \/\/ the appropriate array elements are loaded in them\n+    SVE_DECLARE_REG_SET(align_vec, vtype::set1(pivot));\n+\n+    \/\/ Load the align_vec vectors depending on the vecsToPartition value\n+    SVE_UNROLL_APPLY(SVE_LOAD_TAIL);\n+\n+    \/\/ Initialize the vectors to something arbitrary which will be overwritten when\n+    \/\/ the appropriate array elements are loaded in them\n+    left += vecsToPartition * num_lanes;\n+\n+    \/* Load left and right vtype::numlanes*num_unroll values into\n+     * registers to make space for in-place parition. The vec_left and\n+     * vec_right registers are partitioned at the end.\n+     * Similar to the align_vec<x> vectors, the left<x> and right<x> vectors\n+     * are also initialized to an arbitrary value which will eventually be\n+     * overwritten by array loads. *\/\n+\n+    SVE_DECLARE_REG_SET(left_vec, vtype::set1(pivot));\n+    SVE_DECLARE_REG_SET(right_vec, vtype::set1(pivot));\n+\n+    const arrsize_t right_load_start = right - UNROLL_FACTOR * num_lanes;\n+\n+    SVE_UNROLL_APPLY(SVE_LOAD_LEFT);\n+    SVE_UNROLL_APPLY(SVE_LOAD_RIGHT);\n+\n+    \/* indices for loading the elements *\/\n+    left  += UNROLL_FACTOR * num_lanes;\n+    right -= UNROLL_FACTOR * num_lanes;\n+\n+    while ((right - left) != 0) {\n+        if ((r_store + num_lanes) - right < left - l_store) {\n+            \/\/ Load from the right side if there are fewer elements on the right\n+            \/\/ and partition the vectors\n+            \/\/ TODO: Explore if prefetching the next set of vectors would be beneficial here\n+            right -= (UNROLL_FACTOR * num_lanes);\n+            SVE_DECLARE_REG_SET_UNINIT(curr_vec);\n+            SVE_UNROLL_APPLY(SVE_LOAD_BATCH_FROM_RIGHT);\n+            SVE_UNROLL_APPLY(SVE_PARTITION_BATCH);\n+        } else {\n+            \/\/ Load from the left side if there are fewer elements on the left\n+            \/\/ and partition the vectors\n+            SVE_DECLARE_REG_SET_UNINIT(curr_vec);\n+            SVE_UNROLL_APPLY(SVE_LOAD_BATCH_FROM_LEFT);\n+            left += UNROLL_FACTOR * num_lanes;\n+            SVE_UNROLL_APPLY(SVE_PARTITION_BATCH);\n+        }\n+    }\n+\n+    \/\/ Partition the left and right vectors\n+    SVE_UNROLL_APPLY(SVE_PARTITION_LEFT);\n+    SVE_UNROLL_APPLY(SVE_PARTITION_RIGHT);\n+\n+    \/\/ Partition the align_vec<x> vectors\n+    SVE_UNROLL_APPLY(SVE_PARTITION_TAIL);\n+\n+#undef SVE_LOAD_TAIL\n+#undef SVE_LOAD_LEFT\n+#undef SVE_LOAD_RIGHT\n+#undef SVE_PARTITION_LEFT\n+#undef SVE_PARTITION_RIGHT\n+#undef SVE_PARTITION_TAIL\n+#undef SVE_PARTITION_BATCH\n+#undef SVE_LOAD_BATCH_FROM_LEFT\n+#undef SVE_LOAD_BATCH_FROM_RIGHT\n+#undef SVE_PARTITION_ONE\n+#undef SVE_REG\n+#undef SVE_DECLARE_REG_SET\n+#undef SVE_DECLARE_REG_SET_UNINIT\n+#undef SVE_UNROLL_APPLY\n+\n+    *smallest = vtype::reducemin(min_vec);\n+    *biggest  = vtype::reducemax(max_vec);\n+    return l_store;\n+}\n+\n+template <typename vtype, typename type_t>\n+SVE_SORT_INLINE arrsize_t sve_partition_select(type_t *arr, arrsize_t left, arrsize_t right, type_t pivot,\n+                                               type_t *smallest, type_t *biggest, bool use_gt) {\n+    if (vtype::partition_unroll_factor() == 4) {\n+        return sve_partition_unrolled<vtype, type_t, 4>(arr, left, right, pivot, smallest, biggest, use_gt);\n+    } else {\n+        return sve_partition_unrolled<vtype, type_t, 2>(arr, left, right, pivot, smallest, biggest, use_gt);\n+    }\n+}\n+\n+template <typename vtype, typename type_t>\n+SVE_SORT_INLINE void sve_qsort(type_t* arr, arrsize_t left, arrsize_t right,\n+                               arrsize_t max_iters) {\n+    if ((right - left) <= OET_SORT_THRESHOLD)\n+        return;\n+\n+    if (max_iters <= 0) {\n+        std::sort(arr + left, arr + right, sve_comparison_func_ge<vtype>);\n+        return;\n+    }\n+\n+    type_t pivot = get_pivot_blocks<vtype, type_t>(arr, left, right);\n+\n+    type_t smallest = vtype::type_max();\n+    type_t biggest = vtype::type_min();\n+\n+    arrsize_t pivot_index = sve_partition_select<vtype, type_t>(arr, left, right,\n+                                                                pivot, &smallest,\n+                                                                &biggest, false);\n+\n+    if (pivot != smallest) {\n+        sve_qsort<vtype>(arr, left, pivot_index, max_iters - 1);\n+    }\n+    if (pivot != biggest) {\n+        sve_qsort<vtype>(arr, pivot_index, right, max_iters - 1);\n+    }\n+}\n+\n+template <typename vtype, typename type_t>\n+SVE_SORT_INLINE int64_t sve_vect_partition(type_t* arr, int64_t from_index, int64_t to_index, type_t pivot, bool use_gt) {\n+    type_t smallest = vtype::type_max();\n+    type_t biggest = vtype::type_min();\n+    int64_t pivot_index = sve_partition_select<vtype, type_t>(arr, from_index, to_index,\n+                                                              pivot, &smallest, &biggest, use_gt);\n+    return pivot_index;\n+}\n+\n+template <typename vtype, typename T>\n+SVE_SORT_INLINE void sve_dual_pivot_partition(T* arr, int64_t from_index, int64_t to_index,\n+                                              int32_t *pivot_indices, int64_t index_pivot1, int64_t index_pivot2){\n+    const T pivot1 = arr[index_pivot1];\n+    const T pivot2 = arr[index_pivot2];\n+\n+    const int64_t low = from_index;\n+    const int64_t high = to_index;\n+    const int64_t start = low + 1;\n+    const int64_t end = high - 1;\n+\n+    std::swap(arr[index_pivot1], arr[low]);\n+    std::swap(arr[index_pivot2], arr[end]);\n+\n+    const int64_t pivot_index2 = sve_vect_partition<vtype, T>(arr, start, end, pivot2, true); \/\/ use_gt = true\n+    std::swap(arr[end], arr[pivot_index2]);\n+    int64_t upper = pivot_index2;\n+\n+    \/\/ if all other elements are greater than pivot2 (and pivot1), no need to do further partitioning\n+    if (upper == start) {\n+        pivot_indices[0] = low;\n+        pivot_indices[1] = upper;\n+        return;\n+    }\n+\n+    const int64_t pivot_index1 = sve_vect_partition<vtype, T>(arr, start, upper, pivot1, false); \/\/ use_ge (use_gt = false)\n+    int64_t lower = pivot_index1 - 1;\n+    std::swap(arr[low], arr[lower]);\n+\n+    pivot_indices[0] = lower;\n+    pivot_indices[1] = upper;\n+}\n+\n+template <typename vtype, typename T>\n+SVE_SORT_INLINE void sve_single_pivot_partition(T* arr, int64_t from_index, int64_t to_index,\n+                                                int32_t *pivot_indices, int64_t index_pivot) {\n+    const T pivot = arr[index_pivot];\n+\n+    const int64_t low = from_index;\n+    const int64_t high = to_index;\n+    const int64_t end = high - 1;\n+\n+\n+    const int64_t pivot_index1 = sve_vect_partition<vtype, T>(arr, low, high, pivot, false); \/\/ use_gt = false (use_ge)\n+    int64_t lower = pivot_index1;\n+\n+    const int64_t pivot_index2 = sve_vect_partition<vtype, T>(arr, pivot_index1, high, pivot, true); \/\/ use_gt = true\n+    int64_t upper = pivot_index2;\n+\n+    pivot_indices[0] = lower;\n+    pivot_indices[1] = upper;\n+}\n+\n+template <typename T>\n+SVE_SORT_INLINE void insertion_sort(T* arr, int32_t from_index, int32_t to_index) {\n+    for (int i, k = from_index; ++k < to_index; ) {\n+        T ai = arr[i = k];\n+        if (ai < arr[i - 1]) {\n+            while (--i >= from_index && ai < arr[i]) {\n+                arr[i + 1] = arr[i];\n+            }\n+            arr[i + 1] = ai;\n+        }\n+    }\n+}\n+\n+template <typename T>\n+SVE_SORT_INLINE void sve_fast_sort(T* arr, arrsize_t from_index, arrsize_t to_index, const arrsize_t INS_SORT_THRESHOLD) {\n+    arrsize_t arrsize = to_index - from_index;\n+\n+    if (arrsize <= INS_SORT_THRESHOLD) {\n+        insertion_sort<T>(arr, from_index, to_index);\n+    } else {\n+        sve_qsort<sve_vector<T>, T>(arr, from_index, to_index, 2 * (arrsize_t) (63 - __builtin_clzll((unsigned long long) arrsize)));\n+        sve_oet_sort<sve_vector<T>, T>(arr, from_index, to_index);\n+    }\n+}\n+\n+template <typename T>\n+SVE_SORT_INLINE void sve_fast_partition(T* arr, int64_t from_index, int64_t to_index, int32_t *pivot_indices, int64_t index_pivot1, int64_t index_pivot2) {\n+    if (index_pivot1 != index_pivot2) {\n+        sve_dual_pivot_partition<sve_vector<T>, T>(arr, from_index, to_index, pivot_indices, index_pivot1, index_pivot2);\n+    }\n+    else {\n+        sve_single_pivot_partition<sve_vector<T>, T>(arr, from_index, to_index, pivot_indices, index_pivot1);\n+    }\n+}\n+#endif \/\/ AARCH64_SVE_COMMON_QSORT_HPP\n","filename":"src\/java.base\/linux\/native\/libsimdsort\/aarch64\/sve-common-qsort.hpp","additions":518,"deletions":0,"binary":false,"changes":518,"status":"added"},{"patch":"@@ -0,0 +1,55 @@\n+\/*\n+ * Copyright 2025 Arm Limited and\/or its affiliates.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef AARCH64_SVE_CONFIG_HPP\n+#define AARCH64_SVE_CONFIG_HPP\n+\n+#include <cstddef>\n+#include <cstdint>\n+#include <limits>\n+#include \"simdsort-support.hpp\"\n+\n+#define SIMD_SORT_INFINITYF std::numeric_limits<float>::infinity()\n+#define SIMD_SORT_MAX_INT32 std::numeric_limits<int32_t>::max()\n+#define SIMD_SORT_MIN_INT32 std::numeric_limits<int32_t>::min()\n+\n+#if defined(__GNUC__)\n+  #define SVE_SORT_INLINE  static inline\n+  #define SVE_SORT_FINLINE static inline __attribute__((always_inline))\n+#else\n+  #define SVE_SORT_INLINE  static\n+  #define SVE_SORT_FINLINE static\n+#endif\n+\n+#ifndef DLL_PUBLIC\n+  #define DLL_PUBLIC __attribute__((visibility(\"default\")))\n+#endif\n+\n+using arrsize_t = std::size_t;\n+\n+#ifndef OET_SORT_THRESHOLD\n+  #define OET_SORT_THRESHOLD 8\n+#endif\n+\n+#endif \/\/ AARCH64_SVE_CONFIG_HPP\n","filename":"src\/java.base\/linux\/native\/libsimdsort\/aarch64\/sve-config.hpp","additions":55,"deletions":0,"binary":false,"changes":55,"status":"added"},{"patch":"@@ -0,0 +1,63 @@\n+\/*\n+ * Copyright (c) 2023 Intel Corporation. All rights reserved.\n+ * Copyright 2025 Arm Limited and\/or its affiliates.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#include \"sve-config.hpp\"\n+#include \"sve-common-qsort.hpp\"\n+#include \"classfile_constants.h\"\n+#include \"simdsort-support.hpp\"\n+#include <cstdint>\n+\n+extern \"C\" {\n+\n+    DLL_PUBLIC void sve_sort(void *array, int elem_type, int32_t from_index, int32_t to_index) {\n+        switch(elem_type) {\n+            case JVM_T_INT:\n+                sve_fast_sort((int32_t*)array, from_index, to_index, 64);\n+                break;\n+            case JVM_T_FLOAT:\n+                sve_fast_sort((float*)array, from_index, to_index, 64);\n+                break;\n+            case JVM_T_LONG:\n+            case JVM_T_DOUBLE:\n+            default:\n+                assert(false, \"Unexpected type\");\n+        }\n+    }\n+\n+    DLL_PUBLIC void sve_partition(void *array, int elem_type, int32_t from_index, int32_t to_index, int32_t *pivot_indices, int32_t index_pivot1, int32_t index_pivot2) {\n+        switch(elem_type) {\n+            case JVM_T_INT:\n+                sve_fast_partition((int32_t*)array, from_index, to_index, pivot_indices, index_pivot1, index_pivot2);\n+                break;\n+            case JVM_T_FLOAT:\n+                sve_fast_partition((float*)array, from_index, to_index, pivot_indices, index_pivot1, index_pivot2);\n+                break;\n+            case JVM_T_LONG:\n+            case JVM_T_DOUBLE:\n+            default:\n+                assert(false, \"Unexpected type\");\n+        }\n+    }\n+}\n","filename":"src\/java.base\/linux\/native\/libsimdsort\/aarch64\/sve-linux-qsort.cpp","additions":63,"deletions":0,"binary":false,"changes":63,"status":"added"},{"patch":"@@ -0,0 +1,52 @@\n+\/*\n+ * Copyright 2025 Arm Limited and\/or its affiliates.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef AARCH64_SVE_OET_SORT_HPP\n+#define AARCH64_SVE_OET_SORT_HPP\n+\n+#include \"sve-config.hpp\"\n+#include \"sve-qsort.hpp\"\n+\n+template <typename vtype, typename type_t>\n+SVE_SORT_INLINE void sve_oet_sort(type_t* arr, arrsize_t from_index, arrsize_t to_index) {\n+    arrsize_t arr_num = to_index - from_index;\n+    const uint8_t numLanes = vtype::numlanes();\n+\n+    for (int32_t i = 0; i < OET_SORT_THRESHOLD; i++) {\n+        \/\/ Odd-even pass: even i -> j starts at from_index\n+        \/\/                odd i  -> j starts at from_index + 1\n+        int32_t j = from_index + i % 2;\n+        int32_t remaining = arr_num - (i % 2);\n+\n+        while (remaining >= 2) {\n+            const int32_t vals_per_iteration = (remaining < (2 * numLanes)) ? remaining : 2 * numLanes;\n+            const int32_t num = vals_per_iteration \/ 2;\n+            vtype::oet_sort(&arr[j], num);\n+\n+            j += vals_per_iteration;\n+            remaining -= vals_per_iteration;\n+        }\n+    }\n+}\n+#endif \/\/ AARCH64_SVE_OET_SORT_HPP\n","filename":"src\/java.base\/linux\/native\/libsimdsort\/aarch64\/sve-oet-sort.hpp","additions":52,"deletions":0,"binary":false,"changes":52,"status":"added"},{"patch":"@@ -0,0 +1,242 @@\n+\/*\n+ * Copyright (c) 2021, 2023, Intel Corporation. All rights reserved.\n+ * Copyright (c) 2021 Serge Sans Paille. All rights reserved.\n+ * Copyright 2025 Arm Limited and\/or its affiliates.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef SVE_QSORT_VECTOR\n+#define SVE_QSORT_VECTOR\n+\n+#include <arm_sve.h>\n+#include <cfloat>\n+#include <limits.h>\n+\n+template <typename type>\n+struct sve_vector;\n+\n+template <>\n+struct sve_vector<int32_t> {\n+    using type_t = int32_t;\n+    using reg_t = svint32_t;       \/\/ SVE 32-bit integer vector\n+    using opmask_t = svbool_t;     \/\/ predicate register\n+    \/* TODO: Prefer avoiding a runtime svcntw() call when the vector length\n+     * is known at compile time. One option is to add a template parameter to\n+     * this struct for common cases - 128\/256 bits with a fallback to svcntw()\n+     * if the vector width is unknown at compile time.\n+     *\/\n+    static inline uint8_t numlanes() {\n+        return static_cast<uint8_t>(svcntw());\n+    }\n+\n+    static inline int partition_unroll_factor() {\n+        return (svcntw() * sizeof(type_t)) > 16 ? 4 : 2;\n+    }\n+\n+    static type_t type_max() { return SIMD_SORT_MAX_INT32; }\n+    static type_t type_min() { return SIMD_SORT_MIN_INT32; }\n+\n+    static opmask_t knot_opmask(opmask_t x) {\n+        return svnot_b_z(svptrue_b32(), x);\n+    }\n+\n+    static opmask_t ge(reg_t x, reg_t y) {\n+        return svcmpge_s32(svptrue_b32(),x, y);\n+    }\n+\n+    static opmask_t gt(reg_t x, reg_t y) {\n+        return svcmpgt_s32(svptrue_b32(),x, y);\n+    }\n+\n+    static reg_t loadu(void const *mem) {\n+        return svld1_s32(svptrue_b32(), (const int32_t*)mem);\n+    }\n+\n+    static type_t reducemax(reg_t v) {\n+        return svmaxv_s32(svptrue_b32(), v);\n+    }\n+\n+    static type_t reducemin(reg_t v) {\n+        return svminv_s32(svptrue_b32(), v);\n+    }\n+\n+    static reg_t set1(type_t v) {\n+        return svdup_n_s32(v);\n+    }\n+\n+    static void storeu(void *mem, reg_t x) {\n+        return svst1_s32(svptrue_b32(), (int32_t*)mem, x);\n+    }\n+\n+    static reg_t min(reg_t x, reg_t y) {\n+        return svmin_s32_z(svptrue_b32(), x, y);\n+    }\n+\n+    static reg_t max(reg_t x, reg_t y) {\n+        return svmax_s32_z(svptrue_b32(), x, y);\n+    }\n+\n+    static int double_compressstore(type_t *left_addr, type_t *right_addr,\n+                                    opmask_t k, reg_t reg) {\n+        \/\/ fast path if all vector elements are less than pivot\n+        svbool_t pg = svptrue_b32();\n+        if (!svptest_any(pg, k)) {\n+            svst1_s32(pg, (int32_t*)left_addr, reg);\n+            return 0;\n+        }\n+\n+        \/\/ fast path if all vector elements are greater than pivot\n+        if (!svptest_any(pg, svnot_b_z(pg, k))) {\n+            svst1_s32(pg, (int32_t*)right_addr, reg);\n+            return numlanes();\n+        }\n+\n+        uint64_t amount_ge_pivot = svcntp_b32(svptrue_b32(), k);\n+        uint64_t amount_nge_pivot = numlanes() - amount_ge_pivot;\n+\n+        svint32_t compressed_1 = svcompact_s32(knot_opmask(k), reg);\n+        svint32_t compressed_2 = svcompact_s32(k, reg);\n+\n+        svbool_t store_mask_1 = svwhilelt_b32_u64(0, amount_nge_pivot);\n+        svbool_t store_mask_2 = svwhilelt_b32_u64(0, amount_ge_pivot);\n+\n+        svst1_s32(store_mask_1, (int32_t*)left_addr, compressed_1);\n+        svst1_s32(store_mask_2, (int32_t*)(right_addr + amount_nge_pivot), compressed_2);\n+\n+        return amount_ge_pivot;\n+    }\n+\n+    static void oet_sort(type_t *arr, arrsize_t num) {\n+        svbool_t p1 = svwhilelt_b32_u64(0, num);\n+        const svint32x2_t z0_z1 = svld2_s32(p1, arr);\n+        const svbool_t p2 = svcmplt_s32(p1, svget2_s32(z0_z1, 0), svget2_s32(z0_z1, 1));\n+\n+        const svint32_t z4 = svsel_s32(p2, svget2_s32(z0_z1, 0), svget2_s32(z0_z1, 1)); \/\/ z4 <- smaller values\n+        const svint32_t z5 = svsel_s32(p2, svget2_s32(z0_z1, 1), svget2_s32(z0_z1, 0)); \/\/ z5 <- larger values\n+\n+        svst2_s32(p1, arr, svcreate2_s32(z4, z5));\n+    }\n+};\n+\n+template <>\n+struct sve_vector<float> {\n+    using type_t = float;\n+    using reg_t = svfloat32_t;     \/\/ SVE 32-bit float vector\n+    using opmask_t = svbool_t;     \/\/ predicate register\n+    \/* TODO: Prefer avoiding a runtime svcntw() call when the vector length\n+     * is known at compile time. One option is to add a template parameter to\n+     * this struct for common cases - 128\/256 bits with a fallback to svcntw()\n+     * if the vector width is unknown at compile time.\n+     *\/\n+    static inline uint8_t numlanes() {\n+        return static_cast<uint8_t>(svcntw());\n+    }\n+\n+    static inline int partition_unroll_factor() {\n+        return (svcntw() * sizeof(type_t)) > 16 ? 4 : 2;\n+    }\n+\n+    static type_t type_max() { return SIMD_SORT_INFINITYF; }\n+    static type_t type_min() { return -SIMD_SORT_INFINITYF; }\n+\n+    static opmask_t knot_opmask(opmask_t x) {\n+        return svnot_b_z(svptrue_b32(), x);\n+    }\n+\n+    static opmask_t ge(reg_t x, reg_t y) {\n+        return svcmpge_f32(svptrue_b32(),x, y);\n+    }\n+\n+    static opmask_t gt(reg_t x, reg_t y) {\n+        return svcmpgt_f32(svptrue_b32(),x, y);\n+    }\n+\n+    static reg_t loadu(void const *mem) {\n+        return svld1_f32(svptrue_b32(), (const float*)mem);\n+    }\n+\n+    static type_t reducemax(reg_t v) {\n+        return svmaxv_f32(svptrue_b32(), v);\n+    }\n+\n+    static type_t reducemin(reg_t v) {\n+        return svminv_f32(svptrue_b32(), v);\n+    }\n+\n+    static reg_t set1(type_t v) {\n+        return svdup_n_f32(v);\n+    }\n+\n+    static void storeu(void *mem, reg_t x) {\n+        return svst1_f32(svptrue_b32(), (float32_t*)mem, x);\n+    }\n+\n+    static reg_t min(reg_t x, reg_t y) {\n+        return svmin_f32_z(svptrue_b32(), x, y);\n+    }\n+\n+    static reg_t max(reg_t x, reg_t y) {\n+        return svmax_f32_z(svptrue_b32(), x, y);\n+    }\n+\n+    static int double_compressstore(type_t *left_addr, type_t *right_addr,\n+                                    opmask_t k, reg_t reg) {\n+        \/\/ fast path if all vector elements are less than pivot\n+        svbool_t pg = svptrue_b32();\n+        if (!svptest_any(pg, k)) {\n+            svst1_f32(pg, (float32_t*)left_addr, reg);\n+            return 0;\n+        }\n+\n+        \/\/ fast path if all vector elements are greater than pivot\n+        if (!svptest_any(pg, svnot_b_z(pg, k))) {\n+            svst1_f32(pg, (float32_t*)right_addr, reg);\n+            return numlanes();\n+        }\n+\n+        uint64_t amount_ge_pivot = svcntp_b32(svptrue_b32(), k);\n+        uint64_t amount_nge_pivot = numlanes() - amount_ge_pivot;\n+\n+        svfloat32_t compressed_1 = svcompact_f32(knot_opmask(k), reg);\n+        svfloat32_t compressed_2 = svcompact_f32(k, reg);\n+\n+        svbool_t store_mask_1 = svwhilelt_b32_u64(0, amount_nge_pivot);\n+        svbool_t store_mask_2 = svwhilelt_b32_u64(0, amount_ge_pivot);\n+\n+        svst1_f32(store_mask_1, (float32_t*)left_addr, compressed_1);\n+        svst1_f32(store_mask_2, (float32_t*)(right_addr + amount_nge_pivot), compressed_2);\n+\n+        return amount_ge_pivot;\n+    }\n+\n+    static void oet_sort(type_t *arr, arrsize_t num) {\n+        svbool_t p1 = svwhilelt_b32_u64(0, num);\n+        const svfloat32x2_t z0_z1 = svld2_f32(p1, arr);\n+        const svbool_t p2 = svcmplt_f32(p1, svget2_f32(z0_z1, 0), svget2_f32(z0_z1, 1));\n+\n+        const svfloat32_t z4 = svsel_f32(p2, svget2_f32(z0_z1, 0), svget2_f32(z0_z1, 1)); \/\/ z4 <- smaller values\n+        const svfloat32_t z5 = svsel_f32(p2, svget2_f32(z0_z1, 1), svget2_f32(z0_z1, 0)); \/\/ z5 <- larger values\n+\n+        svst2_f32(p1, arr, svcreate2_f32(z4, z5));\n+    }\n+};\n+#endif  \/\/ SVE_QSORT_VECTOR\n","filename":"src\/java.base\/linux\/native\/libsimdsort\/aarch64\/sve-qsort.hpp","additions":242,"deletions":0,"binary":false,"changes":242,"status":"added"},{"patch":"","filename":"src\/java.base\/linux\/native\/libsimdsort\/x86\/avx2-32bit-qsort.hpp","additions":0,"deletions":0,"binary":false,"changes":0,"previous_filename":"src\/java.base\/linux\/native\/libsimdsort\/avx2-32bit-qsort.hpp","status":"renamed"},{"patch":"","filename":"src\/java.base\/linux\/native\/libsimdsort\/x86\/avx2-emu-funcs.hpp","additions":0,"deletions":0,"binary":false,"changes":0,"previous_filename":"src\/java.base\/linux\/native\/libsimdsort\/avx2-emu-funcs.hpp","status":"renamed"},{"patch":"","filename":"src\/java.base\/linux\/native\/libsimdsort\/x86\/avx2-linux-qsort.cpp","additions":0,"deletions":0,"binary":false,"changes":0,"previous_filename":"src\/java.base\/linux\/native\/libsimdsort\/avx2-linux-qsort.cpp","status":"renamed"},{"patch":"","filename":"src\/java.base\/linux\/native\/libsimdsort\/x86\/avx512-32bit-qsort.hpp","additions":0,"deletions":0,"binary":false,"changes":0,"previous_filename":"src\/java.base\/linux\/native\/libsimdsort\/avx512-32bit-qsort.hpp","status":"renamed"},{"patch":"","filename":"src\/java.base\/linux\/native\/libsimdsort\/x86\/avx512-64bit-qsort.hpp","additions":0,"deletions":0,"binary":false,"changes":0,"previous_filename":"src\/java.base\/linux\/native\/libsimdsort\/avx512-64bit-qsort.hpp","status":"renamed"},{"patch":"","filename":"src\/java.base\/linux\/native\/libsimdsort\/x86\/avx512-linux-qsort.cpp","additions":0,"deletions":0,"binary":false,"changes":0,"previous_filename":"src\/java.base\/linux\/native\/libsimdsort\/avx512-linux-qsort.cpp","status":"renamed"},{"patch":"","filename":"src\/java.base\/linux\/native\/libsimdsort\/x86\/simdsort-support.hpp","additions":0,"deletions":0,"binary":false,"changes":0,"previous_filename":"src\/java.base\/linux\/native\/libsimdsort\/simdsort-support.hpp","status":"renamed"},{"patch":"","filename":"src\/java.base\/linux\/native\/libsimdsort\/x86\/xss-common-includes.h","additions":0,"deletions":0,"binary":false,"changes":0,"previous_filename":"src\/java.base\/linux\/native\/libsimdsort\/xss-common-includes.h","status":"renamed"},{"patch":"","filename":"src\/java.base\/linux\/native\/libsimdsort\/x86\/xss-common-qsort.h","additions":0,"deletions":0,"binary":false,"changes":0,"previous_filename":"src\/java.base\/linux\/native\/libsimdsort\/xss-common-qsort.h","status":"renamed"},{"patch":"","filename":"src\/java.base\/linux\/native\/libsimdsort\/x86\/xss-network-qsort.hpp","additions":0,"deletions":0,"binary":false,"changes":0,"previous_filename":"src\/java.base\/linux\/native\/libsimdsort\/xss-network-qsort.hpp","status":"renamed"},{"patch":"","filename":"src\/java.base\/linux\/native\/libsimdsort\/x86\/xss-optimal-networks.hpp","additions":0,"deletions":0,"binary":false,"changes":0,"previous_filename":"src\/java.base\/linux\/native\/libsimdsort\/xss-optimal-networks.hpp","status":"renamed"},{"patch":"","filename":"src\/java.base\/linux\/native\/libsimdsort\/x86\/xss-pivot-selection.hpp","additions":0,"deletions":0,"binary":false,"changes":0,"previous_filename":"src\/java.base\/linux\/native\/libsimdsort\/xss-pivot-selection.hpp","status":"renamed"}]}