{"files":[{"patch":"@@ -135,1 +135,1 @@\n-# Param2 - _nocoops, or empty\n+# Param2 - _nocoops, _coh, _nocoops_coh, or empty\n@@ -137,2 +137,8 @@\n-  $1_$2_DUMP_EXTRA_ARG := $(if $(filter _nocoops, $2), -XX:-UseCompressedOops, )\n-  $1_$2_DUMP_TYPE      := $(if $(filter _nocoops, $2), -NOCOOPS, )\n+  $1_$2_COOPS_OPTION := $(if $(findstring _nocoops, $2),-XX:-UseCompressedOops)\n+  # enable and also explicitly disable coh as needed.\n+  ifeq ($(call isTargetCpuBits, 64), true)\n+    $1_$2_COH_OPTION := -XX:+UnlockExperimentalVMOptions \\\n+                        $(if $(findstring _coh, $2),-XX:+UseCompactObjectHeaders,-XX:-UseCompactObjectHeaders)\n+  endif\n+  $1_$2_DUMP_EXTRA_ARG := $$($1_$2_COOPS_OPTION) $$($1_$2_COH_OPTION)\n+  $1_$2_DUMP_TYPE      := $(if $(findstring _nocoops, $2),-NOCOOPS,)$(if $(findstring _coh, $2),-COH,)\n@@ -193,0 +199,8 @@\n+    ifeq ($(BUILD_CDS_ARCHIVE_COH), true)\n+      $(foreach v, $(JVM_VARIANTS), \\\n+        $(eval $(call CreateCDSArchive,$v,_coh)) \\\n+      )\n+      $(foreach v, $(JVM_VARIANTS), \\\n+        $(eval $(call CreateCDSArchive,$v,_nocoops_coh)) \\\n+      )\n+    endif\n","filename":"make\/Images.gmk","additions":17,"deletions":3,"binary":false,"changes":20,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n-# Copyright (c) 2011, 2023, Oracle and\/or its affiliates. All rights reserved.\n+# Copyright (c) 2011, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -263,0 +263,1 @@\n+JDKOPT_ENABLE_DISABLE_CDS_ARCHIVE_COH\n","filename":"make\/autoconf\/configure.ac","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -676,0 +676,30 @@\n+################################################################################\n+#\n+# Enable or disable the default CDS archive generation for Compact Object Headers\n+#\n+AC_DEFUN([JDKOPT_ENABLE_DISABLE_CDS_ARCHIVE_COH],\n+[\n+  UTIL_ARG_ENABLE(NAME: cds-archive-coh, DEFAULT: auto, RESULT: BUILD_CDS_ARCHIVE_COH,\n+      DESC: [enable generation of default CDS archives for compact object headers (requires --enable-cds-archive)],\n+      DEFAULT_DESC: [auto],\n+      CHECKING_MSG: [if default CDS archives for compact object headers should be generated],\n+      CHECK_AVAILABLE: [\n+        AC_MSG_CHECKING([if CDS archive with compact object headers is available])\n+        if test \"x$BUILD_CDS_ARCHIVE\" = \"xfalse\"; then\n+          AC_MSG_RESULT([no (CDS default archive generation is disabled)])\n+          AVAILABLE=false\n+        elif test \"x$OPENJDK_TARGET_CPU\" != \"xx86_64\" &&\n+             test \"x$OPENJDK_TARGET_CPU\" != \"xaarch64\" &&\n+             test \"x$OPENJDK_TARGET_CPU\" != \"xppc64\" &&\n+             test \"x$OPENJDK_TARGET_CPU\" != \"xppc64le\" &&\n+             test \"x$OPENJDK_TARGET_CPU\" != \"xriscv64\"; then\n+          AC_MSG_RESULT([no (compact object headers not supported for this platform)])\n+          AVAILABLE=false\n+        else\n+          AC_MSG_RESULT([yes])\n+          AVAILABLE=true\n+        fi\n+      ])\n+  AC_SUBST(BUILD_CDS_ARCHIVE_COH)\n+])\n+\n","filename":"make\/autoconf\/jdk-options.m4","additions":30,"deletions":0,"binary":false,"changes":30,"status":"modified"},{"patch":"@@ -373,0 +373,1 @@\n+BUILD_CDS_ARCHIVE_COH := @BUILD_CDS_ARCHIVE_COH@\n","filename":"make\/autoconf\/spec.gmk.template","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -5741,0 +5741,4 @@\n+opclass memory_noindex(indirect,\n+                       indOffI1, indOffL1,indOffI2, indOffL2, indOffI4, indOffL4, indOffI8, indOffL8,\n+                       indirectN, indOffIN, indOffLN, indirectX2P, indOffX2P);\n+\n@@ -6667,1 +6671,1 @@\n-  predicate(!needs_acquiring_load(n));\n+  predicate(!needs_acquiring_load(n) && !UseCompactObjectHeaders);\n@@ -6677,0 +6681,14 @@\n+instruct loadNKlassCompactHeaders(iRegNNoSp dst, memory_noindex mem)\n+%{\n+  match(Set dst (LoadNKlass mem));\n+  predicate(!needs_acquiring_load(n) && UseCompactObjectHeaders);\n+\n+  ins_cost(4 * INSN_COST);\n+  format %{ \"load_narrow_klass_compact  $dst, $mem\\t# compressed class ptr\" %}\n+  ins_encode %{\n+    assert($mem$$index$$Register == noreg, \"must not have indexed address\");\n+    __ load_narrow_klass_compact_c2($dst$$Register, $mem$$base$$Register, $mem$$disp);\n+  %}\n+  ins_pipe(iload_reg_mem);\n+%}\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/aarch64.ad","additions":19,"deletions":1,"binary":false,"changes":20,"status":"modified"},{"patch":"@@ -2249,2 +2249,0 @@\n-  Address src_klass_addr = Address(src, oopDesc::klass_offset_in_bytes());\n-  Address dst_klass_addr = Address(dst, oopDesc::klass_offset_in_bytes());\n@@ -2311,9 +2309,1 @@\n-      if (UseCompressedClassPointers) {\n-        __ ldrw(tmp, src_klass_addr);\n-        __ ldrw(rscratch1, dst_klass_addr);\n-        __ cmpw(tmp, rscratch1);\n-      } else {\n-        __ ldr(tmp, src_klass_addr);\n-        __ ldr(rscratch1, dst_klass_addr);\n-        __ cmp(tmp, rscratch1);\n-      }\n+      __ cmp_klasses_from_objects(src, dst, tmp, rscratch1);\n@@ -2441,3 +2431,0 @@\n-    if (UseCompressedClassPointers) {\n-      __ encode_klass_not_null(tmp);\n-    }\n@@ -2446,8 +2433,1 @@\n-\n-      if (UseCompressedClassPointers) {\n-        __ ldrw(rscratch1, dst_klass_addr);\n-        __ cmpw(tmp, rscratch1);\n-      } else {\n-        __ ldr(rscratch1, dst_klass_addr);\n-        __ cmp(tmp, rscratch1);\n-      }\n+      __ cmp_klass(dst, tmp, rscratch1);\n@@ -2455,7 +2435,1 @@\n-      if (UseCompressedClassPointers) {\n-        __ ldrw(rscratch1, src_klass_addr);\n-        __ cmpw(tmp, rscratch1);\n-      } else {\n-        __ ldr(rscratch1, src_klass_addr);\n-        __ cmp(tmp, rscratch1);\n-      }\n+      __ cmp_klass(src, tmp, rscratch1);\n@@ -2464,7 +2438,1 @@\n-      if (UseCompressedClassPointers) {\n-        __ ldrw(rscratch1, dst_klass_addr);\n-        __ cmpw(tmp, rscratch1);\n-      } else {\n-        __ ldr(rscratch1, dst_klass_addr);\n-        __ cmp(tmp, rscratch1);\n-      }\n+      __ cmp_klass(dst, tmp, rscratch1);\n@@ -2553,6 +2521,1 @@\n-  if (UseCompressedClassPointers) {\n-    __ ldrw(result, Address (obj, oopDesc::klass_offset_in_bytes()));\n-    __ decode_klass_not_null(result);\n-  } else {\n-    __ ldr(result, Address (obj, oopDesc::klass_offset_in_bytes()));\n-  }\n+  __ load_klass(result, obj);\n","filename":"src\/hotspot\/cpu\/aarch64\/c1_LIRAssembler_aarch64.cpp","additions":5,"deletions":42,"binary":false,"changes":47,"status":"modified"},{"patch":"@@ -178,3 +178,0 @@\n-  \/\/ This assumes that all prototype bits fit in an int32_t\n-  mov(t1, (int32_t)(intptr_t)markWord::prototype().value());\n-  str(t1, Address(obj, oopDesc::mark_offset_in_bytes()));\n@@ -182,3 +179,3 @@\n-  if (UseCompressedClassPointers) { \/\/ Take care not to kill klass\n-    encode_klass_not_null(t1, klass);\n-    strw(t1, Address(obj, oopDesc::klass_offset_in_bytes()));\n+  if (UseCompactObjectHeaders) {\n+    ldr(t1, Address(klass, Klass::prototype_header_offset()));\n+    str(t1, Address(obj, oopDesc::mark_offset_in_bytes()));\n@@ -186,1 +183,8 @@\n-    str(klass, Address(obj, oopDesc::klass_offset_in_bytes()));\n+    mov(t1, checked_cast<int32_t>(markWord::prototype().value()));\n+    str(t1, Address(obj, oopDesc::mark_offset_in_bytes()));\n+    if (UseCompressedClassPointers) { \/\/ Take care not to kill klass\n+      encode_klass_not_null(t1, klass);\n+      strw(t1, Address(obj, oopDesc::klass_offset_in_bytes()));\n+    } else {\n+      str(klass, Address(obj, oopDesc::klass_offset_in_bytes()));\n+    }\n@@ -197,1 +201,1 @@\n-  } else if (UseCompressedClassPointers) {\n+  } else if (UseCompressedClassPointers && !UseCompactObjectHeaders) {\n","filename":"src\/hotspot\/cpu\/aarch64\/c1_MacroAssembler_aarch64.cpp","additions":12,"deletions":8,"binary":false,"changes":20,"status":"modified"},{"patch":"@@ -2596,0 +2596,9 @@\n+\n+void C2_MacroAssembler::load_narrow_klass_compact_c2(Register dst, Register obj, int disp) {\n+  \/\/ Note: Don't clobber obj anywhere in that method!\n+\n+  \/\/ The incoming address is pointing into obj-start + klass_offset_in_bytes. We need to extract\n+  \/\/ obj-start, so that we can load from the object's mark-word instead.\n+  ldr(dst, Address(obj, disp - oopDesc::klass_offset_in_bytes()));\n+  lsr(dst, dst, markWord::klass_shift);\n+}\n","filename":"src\/hotspot\/cpu\/aarch64\/c2_MacroAssembler_aarch64.cpp","additions":9,"deletions":0,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -182,0 +182,2 @@\n+  void load_narrow_klass_compact_c2(Register dst, Register obj, int disp);\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/c2_MacroAssembler_aarch64.hpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -120,16 +120,0 @@\n-\n-void CompressedKlassPointers::initialize(address addr, size_t len) {\n-  constexpr uintptr_t unscaled_max = nth_bit(32);\n-  assert(len <= unscaled_max, \"Klass range larger than 32 bits?\");\n-\n-  \/\/ Shift is always 0 on aarch64.\n-  _shift = 0;\n-\n-  \/\/ On aarch64, we don't bother with zero-based encoding (base=0 shift>0).\n-  address const end = addr + len;\n-  _base = (end <= (address)unscaled_max) ? nullptr : addr;\n-\n-  \/\/ Remember the Klass range:\n-  _klass_range_start = addr;\n-  _klass_range_end = addr + len;\n-}\n","filename":"src\/hotspot\/cpu\/aarch64\/compressedKlass_aarch64.cpp","additions":0,"deletions":16,"binary":false,"changes":16,"status":"modified"},{"patch":"@@ -1005,0 +1005,1 @@\n+  int extra_instructions = UseCompactObjectHeaders ? 1 : 0;\n@@ -1006,1 +1007,1 @@\n-    return NativeInstruction::instruction_size * 7;\n+    return NativeInstruction::instruction_size * (7 + extra_instructions);\n@@ -1008,1 +1009,1 @@\n-    return NativeInstruction::instruction_size * 5;\n+    return NativeInstruction::instruction_size * (5 + extra_instructions);\n@@ -1026,1 +1027,5 @@\n-  if (UseCompressedClassPointers) {\n+  if (UseCompactObjectHeaders) {\n+    load_narrow_klass_compact(tmp1, receiver);\n+    ldrw(tmp2, Address(data, CompiledICData::speculated_klass_offset()));\n+    cmpw(tmp1, tmp2);\n+  } else if (UseCompressedClassPointers) {\n@@ -4842,0 +4847,11 @@\n+\/\/ Loads the obj's Klass* into dst.\n+\/\/ Preserves all registers (incl src, rscratch1 and rscratch2).\n+\/\/ Input:\n+\/\/ src - the oop we want to load the klass from.\n+\/\/ dst - output narrow klass.\n+void MacroAssembler::load_narrow_klass_compact(Register dst, Register src) {\n+  assert(UseCompactObjectHeaders, \"expects UseCompactObjectHeaders\");\n+  ldr(dst, Address(src, oopDesc::mark_offset_in_bytes()));\n+  lsr(dst, dst, markWord::klass_shift);\n+}\n+\n@@ -4843,1 +4859,4 @@\n-  if (UseCompressedClassPointers) {\n+  if (UseCompactObjectHeaders) {\n+    load_narrow_klass_compact(dst, src);\n+    decode_klass_not_null(dst);\n+  } else if (UseCompressedClassPointers) {\n@@ -4898,1 +4917,2 @@\n-void MacroAssembler::cmp_klass(Register oop, Register trial_klass, Register tmp) {\n+void MacroAssembler::cmp_klass(Register obj, Register klass, Register tmp) {\n+  assert_different_registers(obj, klass, tmp);\n@@ -4900,1 +4920,5 @@\n-    ldrw(tmp, Address(oop, oopDesc::klass_offset_in_bytes()));\n+    if (UseCompactObjectHeaders) {\n+      load_narrow_klass_compact(tmp, obj);\n+    } else {\n+      ldrw(tmp, Address(obj, oopDesc::klass_offset_in_bytes()));\n+    }\n@@ -4902,1 +4926,1 @@\n-      cmp(trial_klass, tmp, LSL, CompressedKlassPointers::shift());\n+      cmp(klass, tmp, LSL, CompressedKlassPointers::shift());\n@@ -4907,1 +4931,1 @@\n-      cmpw(trial_klass, tmp);\n+      cmpw(klass, tmp);\n@@ -4912,1 +4936,18 @@\n-    ldr(tmp, Address(oop, oopDesc::klass_offset_in_bytes()));\n+    ldr(tmp, Address(obj, oopDesc::klass_offset_in_bytes()));\n+  }\n+  cmp(klass, tmp);\n+}\n+\n+void MacroAssembler::cmp_klasses_from_objects(Register obj1, Register obj2, Register tmp1, Register tmp2) {\n+  if (UseCompactObjectHeaders) {\n+    load_narrow_klass_compact(tmp1, obj1);\n+    load_narrow_klass_compact(tmp2,  obj2);\n+    cmpw(tmp1, tmp2);\n+  } else if (UseCompressedClassPointers) {\n+    ldrw(tmp1, Address(obj1, oopDesc::klass_offset_in_bytes()));\n+    ldrw(tmp2, Address(obj2, oopDesc::klass_offset_in_bytes()));\n+    cmpw(tmp1, tmp2);\n+  } else {\n+    ldr(tmp1, Address(obj1, oopDesc::klass_offset_in_bytes()));\n+    ldr(tmp2, Address(obj2, oopDesc::klass_offset_in_bytes()));\n+    cmp(tmp1, tmp2);\n@@ -4914,1 +4955,0 @@\n-  cmp(trial_klass, tmp);\n@@ -4920,0 +4960,1 @@\n+  assert(!UseCompactObjectHeaders, \"not with compact headers\");\n@@ -4929,0 +4970,1 @@\n+  assert(!UseCompactObjectHeaders, \"not with compact headers\");\n@@ -5077,3 +5119,0 @@\n-  assert(LogKlassAlignmentInBytes == CompressedKlassPointers::shift()\n-         || 0 == CompressedKlassPointers::shift(), \"decode alg wrong\");\n-\n@@ -5105,1 +5144,1 @@\n-      lsr(dst, src, LogKlassAlignmentInBytes);\n+      lsr(dst, src, CompressedKlassPointers::shift());\n@@ -5114,1 +5153,1 @@\n-      lsr(dst, dst, LogKlassAlignmentInBytes);\n+      lsr(dst, dst, CompressedKlassPointers::shift());\n@@ -5122,1 +5161,1 @@\n-      ubfx(dst, src, LogKlassAlignmentInBytes, 32);\n+      ubfx(dst, src, CompressedKlassPointers::shift(), 32);\n@@ -5144,1 +5183,1 @@\n-      lsl(dst, src, LogKlassAlignmentInBytes);\n+      lsl(dst, src, CompressedKlassPointers::shift());\n@@ -5152,1 +5191,1 @@\n-      lsl(dst, src, LogKlassAlignmentInBytes);\n+      lsl(dst, src, CompressedKlassPointers::shift());\n@@ -5167,1 +5206,1 @@\n-      lsl(dst, dst, LogKlassAlignmentInBytes);\n+      lsl(dst, dst, CompressedKlassPointers::shift());\n","filename":"src\/hotspot\/cpu\/aarch64\/macroAssembler_aarch64.cpp","additions":58,"deletions":19,"binary":false,"changes":77,"status":"modified"},{"patch":"@@ -878,0 +878,1 @@\n+  void load_narrow_klass_compact(Register dst, Register src);\n@@ -880,1 +881,2 @@\n-  void cmp_klass(Register oop, Register trial_klass, Register tmp);\n+  void cmp_klass(Register obj, Register klass, Register tmp);\n+  void cmp_klasses_from_objects(Register obj1, Register obj2, Register tmp1, Register tmp2);\n","filename":"src\/hotspot\/cpu\/aarch64\/macroAssembler_aarch64.hpp","additions":3,"deletions":1,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -3632,1 +3632,3 @@\n-    __ sub(r3, r3, sizeof(oopDesc));\n+    int header_size = oopDesc::header_size() * HeapWordSize;\n+    assert(is_aligned(header_size, BytesPerLong), \"oop header size must be 8-byte-aligned\");\n+    __ sub(r3, r3, header_size);\n@@ -3637,1 +3639,1 @@\n-      __ add(r2, r0, sizeof(oopDesc));\n+      __ add(r2, r0, header_size);\n@@ -3647,4 +3649,9 @@\n-    __ mov(rscratch1, (intptr_t)markWord::prototype().value());\n-    __ str(rscratch1, Address(r0, oopDesc::mark_offset_in_bytes()));\n-    __ store_klass_gap(r0, zr);  \/\/ zero klass gap for compressed oops\n-    __ store_klass(r0, r4);      \/\/ store klass last\n+    if (UseCompactObjectHeaders) {\n+      __ ldr(rscratch1, Address(r4, Klass::prototype_header_offset()));\n+      __ str(rscratch1, Address(r0, oopDesc::mark_offset_in_bytes()));\n+    } else {\n+      __ mov(rscratch1, (intptr_t)markWord::prototype().value());\n+      __ str(rscratch1, Address(r0, oopDesc::mark_offset_in_bytes()));\n+      __ store_klass_gap(r0, zr);  \/\/ zero klass gap for compressed oops\n+      __ store_klass(r0, r4);      \/\/ store klass last\n+    }\n","filename":"src\/hotspot\/cpu\/aarch64\/templateTable_aarch64.cpp","additions":13,"deletions":6,"binary":false,"changes":19,"status":"modified"},{"patch":"@@ -1979,10 +1979,1 @@\n-      if (UseCompressedClassPointers) {\n-        \/\/ We don't need decode because we just need to compare.\n-        __ lwz(tmp, oopDesc::klass_offset_in_bytes(), src);\n-        __ lwz(tmp2, oopDesc::klass_offset_in_bytes(), dst);\n-        __ cmpw(CCR0, tmp, tmp2);\n-      } else {\n-        __ ld(tmp, oopDesc::klass_offset_in_bytes(), src);\n-        __ ld(tmp2, oopDesc::klass_offset_in_bytes(), dst);\n-        __ cmpd(CCR0, tmp, tmp2);\n-      }\n+      __ cmp_klasses_from_objects(CCR0, src, dst, tmp, tmp2);\n@@ -2111,19 +2102,5 @@\n-    if (UseCompressedClassPointers) {\n-      \/\/ Tmp holds the default type. It currently comes uncompressed after the\n-      \/\/ load of a constant, so encode it.\n-      __ encode_klass_not_null(tmp);\n-      \/\/ Load the raw value of the dst klass, since we will be comparing\n-      \/\/ uncompressed values directly.\n-      __ lwz(tmp2, oopDesc::klass_offset_in_bytes(), dst);\n-      __ cmpw(CCR0, tmp, tmp2);\n-      if (basic_type != T_OBJECT) {\n-        __ bne(CCR0, halt);\n-        \/\/ Load the raw value of the src klass.\n-        __ lwz(tmp2, oopDesc::klass_offset_in_bytes(), src);\n-        __ cmpw(CCR0, tmp, tmp2);\n-        __ beq(CCR0, known_ok);\n-      } else {\n-        __ beq(CCR0, known_ok);\n-        __ cmpw(CCR0, src, dst);\n-        __ beq(CCR0, known_ok);\n-      }\n+    __ cmp_klass(CCR0, dst, tmp, R11_scratch1, R12_scratch2);\n+    if (basic_type != T_OBJECT) {\n+      __ bne(CCR0, halt);\n+      __ cmp_klass(CCR0, src, tmp, R11_scratch1, R12_scratch2);\n+      __ beq(CCR0, known_ok);\n@@ -2131,13 +2108,3 @@\n-      __ ld(tmp2, oopDesc::klass_offset_in_bytes(), dst);\n-      __ cmpd(CCR0, tmp, tmp2);\n-      if (basic_type != T_OBJECT) {\n-        __ bne(CCR0, halt);\n-        \/\/ Load the raw value of the src klass.\n-        __ ld(tmp2, oopDesc::klass_offset_in_bytes(), src);\n-        __ cmpd(CCR0, tmp, tmp2);\n-        __ beq(CCR0, known_ok);\n-      } else {\n-        __ beq(CCR0, known_ok);\n-        __ cmpd(CCR0, src, dst);\n-        __ beq(CCR0, known_ok);\n-      }\n+      __ beq(CCR0, known_ok);\n+      __ cmpw(CCR0, src, dst);\n+      __ beq(CCR0, known_ok);\n@@ -2721,6 +2688,1 @@\n-  if (UseCompressedClassPointers) {\n-    __ lwz(result, oopDesc::klass_offset_in_bytes(), obj);\n-    __ decode_klass_not_null(result);\n-  } else {\n-    __ ld(result, oopDesc::klass_offset_in_bytes(), obj);\n-  }\n+  __ load_klass(result, obj);\n","filename":"src\/hotspot\/cpu\/ppc\/c1_LIRAssembler_ppc.cpp","additions":10,"deletions":48,"binary":false,"changes":58,"status":"modified"},{"patch":"@@ -200,3 +200,10 @@\n-  load_const_optimized(t1, (intx)markWord::prototype().value());\n-  std(t1, oopDesc::mark_offset_in_bytes(), obj);\n-  store_klass(obj, klass);\n+\n+  if (UseCompactObjectHeaders) {\n+    ld(t1, in_bytes(Klass::prototype_header_offset()), klass);\n+    std(t1, oopDesc::mark_offset_in_bytes(), obj);\n+  } else {\n+    load_const_optimized(t1, (intx)markWord::prototype().value());\n+    std(t1, oopDesc::mark_offset_in_bytes(), obj);\n+    store_klass(obj, klass);\n+  }\n+\n@@ -205,1 +212,1 @@\n-  } else if (UseCompressedClassPointers) {\n+  } else if (UseCompressedClassPointers && !UseCompactObjectHeaders) {\n","filename":"src\/hotspot\/cpu\/ppc\/c1_MacroAssembler_ppc.cpp","additions":11,"deletions":4,"binary":false,"changes":15,"status":"modified"},{"patch":"@@ -50,0 +50,9 @@\n+void C2_MacroAssembler::load_narrow_klass_compact_c2(Register dst, Register obj, int disp) {\n+  \/\/ Note: Don't clobber obj anywhere in that method!\n+\n+  \/\/ The incoming address is pointing into obj-start + klass_offset_in_bytes. We need to extract\n+  \/\/ obj-start, so that we can load from the object's mark-word instead.\n+  ld(dst, disp - oopDesc::klass_offset_in_bytes(), obj);\n+  srdi(dst, dst, markWord::klass_shift);\n+}\n+\n","filename":"src\/hotspot\/cpu\/ppc\/c2_MacroAssembler_ppc.cpp","additions":9,"deletions":0,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -37,0 +37,2 @@\n+  void load_narrow_klass_compact_c2(Register dst, Register obj, int disp);\n+\n","filename":"src\/hotspot\/cpu\/ppc\/c2_MacroAssembler_ppc.hpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -1221,0 +1221,3 @@\n+\n+  if (UseCompactObjectHeaders) num_ins++;\n+\n@@ -1248,1 +1251,3 @@\n-    if (UseCompressedClassPointers) {\n+    if (UseCompactObjectHeaders) {\n+      load_narrow_klass_compact(tmp1, receiver);\n+    } else if (UseCompressedClassPointers) {\n@@ -3246,0 +3251,1 @@\n+  assert(!UseCompactObjectHeaders, \"not with compact headers\");\n@@ -3255,0 +3261,1 @@\n+  assert(!UseCompactObjectHeaders, \"not with compact headers\");\n@@ -3260,1 +3267,1 @@\n-    stw(val, oopDesc::klass_gap_offset_in_bytes(), dst_oop); \/\/ klass gap if compressed\n+    stw(val, oopDesc::klass_gap_offset_in_bytes(), dst_oop);\n@@ -3301,1 +3308,4 @@\n-  if (UseCompressedClassPointers) {\n+  if (UseCompactObjectHeaders) {\n+    load_narrow_klass_compact(dst, src);\n+    decode_klass_not_null(dst);\n+  } else if (UseCompressedClassPointers) {\n@@ -3303,2 +3313,1 @@\n-    \/\/ Attention: no null check here!\n-    decode_klass_not_null(dst, dst);\n+    decode_klass_not_null(dst);\n@@ -3310,0 +3319,43 @@\n+\/\/ Loads the obj's Klass* into dst.\n+\/\/ Preserves all registers (incl src, rscratch1 and rscratch2).\n+\/\/ Input:\n+\/\/ src - the oop we want to load the klass from.\n+\/\/ dst - output nklass.\n+void MacroAssembler::load_narrow_klass_compact(Register dst, Register src) {\n+  assert(UseCompactObjectHeaders, \"expects UseCompactObjectHeaders\");\n+  ld(dst, oopDesc::mark_offset_in_bytes(), src);\n+  srdi(dst, dst, markWord::klass_shift);\n+}\n+\n+void MacroAssembler::cmp_klass(ConditionRegister dst, Register obj, Register klass, Register tmp, Register tmp2) {\n+  assert_different_registers(obj, klass, tmp);\n+  if (UseCompressedClassPointers) {\n+    if (UseCompactObjectHeaders) {\n+      load_narrow_klass_compact(tmp, obj);\n+    } else {\n+      lwz(tmp, oopDesc::klass_offset_in_bytes(), obj);\n+    }\n+    Register encoded_klass = encode_klass_not_null(tmp2, klass);\n+    cmpw(dst, tmp, encoded_klass);\n+  } else {\n+    ld(tmp, oopDesc::klass_offset_in_bytes(), obj);\n+    cmpd(dst, tmp, klass);\n+  }\n+}\n+\n+void MacroAssembler::cmp_klasses_from_objects(ConditionRegister dst, Register obj1, Register obj2, Register tmp1, Register tmp2) {\n+  if (UseCompactObjectHeaders) {\n+    load_narrow_klass_compact(tmp1, obj1);\n+    load_narrow_klass_compact(tmp2, obj2);\n+    cmpw(dst, tmp1, tmp2);\n+  } else if (UseCompressedClassPointers) {\n+    lwz(tmp1, oopDesc::klass_offset_in_bytes(), obj1);\n+    lwz(tmp2, oopDesc::klass_offset_in_bytes(), obj2);\n+    cmpw(dst, tmp1, tmp2);\n+  } else {\n+    ld(tmp1, oopDesc::klass_offset_in_bytes(), obj1);\n+    ld(tmp2, oopDesc::klass_offset_in_bytes(), obj2);\n+    cmpd(dst, tmp1, tmp2);\n+  }\n+}\n+\n","filename":"src\/hotspot\/cpu\/ppc\/macroAssembler_ppc.cpp","additions":57,"deletions":5,"binary":false,"changes":62,"status":"modified"},{"patch":"@@ -760,0 +760,3 @@\n+  void load_narrow_klass_compact(Register dst, Register src);\n+  void cmp_klass(ConditionRegister dst, Register obj, Register klass, Register tmp, Register tmp2);\n+  void cmp_klasses_from_objects(ConditionRegister dst, Register obj1, Register obj2, Register tmp1, Register tmp2);\n","filename":"src\/hotspot\/cpu\/ppc\/macroAssembler_ppc.hpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -5503,0 +5503,1 @@\n+  predicate(!UseCompactObjectHeaders);\n@@ -5511,0 +5512,14 @@\n+instruct loadNKlassCompactHeaders(iRegNdst dst, memory mem) %{\n+  match(Set dst (LoadNKlass mem));\n+  predicate(UseCompactObjectHeaders);\n+  ins_cost(MEMORY_REF_COST);\n+\n+  format %{ \"load_narrow_klass_compact $dst, $mem \\t\/\/ compressed class ptr\" %}\n+  size(8);\n+  ins_encode %{\n+    assert($mem$$index$$Register == R0, \"must not have indexed address: %s[%s]\", $mem$$base$$Register.name(), $mem$$index$$Register.name());\n+    __ load_narrow_klass_compact_c2($dst$$Register, $mem$$base$$Register, $mem$$disp);\n+  %}\n+  ins_pipe(pipe_class_memory);\n+%}\n+\n","filename":"src\/hotspot\/cpu\/ppc\/ppc.ad","additions":15,"deletions":0,"binary":false,"changes":15,"status":"modified"},{"patch":"@@ -3843,2 +3843,3 @@\n-      __ addi(Rinstance_size, Rinstance_size, 7 - (int)sizeof(oopDesc));\n-      __ addi(Rbase, RallocatedObject, sizeof(oopDesc));\n+      int header_size = oopDesc::header_size() * HeapWordSize;\n+      __ addi(Rinstance_size, Rinstance_size, 7 - header_size);\n+      __ addi(Rbase, RallocatedObject, header_size);\n@@ -3854,6 +3855,9 @@\n-    __ load_const_optimized(Rscratch, markWord::prototype().value(), R0);\n-    __ std(Rscratch, oopDesc::mark_offset_in_bytes(), RallocatedObject);\n-\n-    \/\/ Init klass.\n-    __ store_klass_gap(RallocatedObject);\n-    __ store_klass(RallocatedObject, RinstanceKlass, Rscratch); \/\/ klass (last for cms)\n+    if (UseCompactObjectHeaders) {\n+      __ ld(Rscratch, in_bytes(Klass::prototype_header_offset()), RinstanceKlass);\n+      __ std(Rscratch, oopDesc::mark_offset_in_bytes(), RallocatedObject);\n+    } else {\n+      __ load_const_optimized(Rscratch, markWord::prototype().value(), R0);\n+      __ std(Rscratch, oopDesc::mark_offset_in_bytes(), RallocatedObject);\n+      __ store_klass_gap(RallocatedObject);\n+      __ store_klass(RallocatedObject, RinstanceKlass, Rscratch);\n+    }\n","filename":"src\/hotspot\/cpu\/ppc\/templateTable_ppc_64.cpp","additions":12,"deletions":8,"binary":false,"changes":20,"status":"modified"},{"patch":"@@ -197,1 +197,4 @@\n-    if (UseCompressedClassPointers) {\n+    if (UseCompactObjectHeaders) {\n+      __ load_narrow_klass_compact(tmp, src);\n+      __ load_narrow_klass_compact(t0, dst);\n+    } else if (UseCompressedClassPointers) {\n@@ -247,1 +250,0 @@\n-\n@@ -264,12 +266,2 @@\n-      if (UseCompressedClassPointers) {\n-        __ lwu(t0, Address(dst, oopDesc::klass_offset_in_bytes()));\n-      } else {\n-        __ ld(t0, Address(dst, oopDesc::klass_offset_in_bytes()));\n-      }\n-      __ bne(tmp, t0, halt);\n-      if (UseCompressedClassPointers) {\n-        __ lwu(t0, Address(src, oopDesc::klass_offset_in_bytes()));\n-      } else {\n-        __ ld(t0, Address(src, oopDesc::klass_offset_in_bytes()));\n-      }\n-      __ beq(tmp, t0, known_ok);\n+      __ cmp_klass_compressed(dst, tmp, t0, halt, false);\n+      __ cmp_klass_compressed(src, tmp, t0, known_ok, true);\n@@ -277,6 +269,1 @@\n-      if (UseCompressedClassPointers) {\n-        __ lwu(t0, Address(dst, oopDesc::klass_offset_in_bytes()));\n-      } else {\n-        __ ld(t0, Address(dst, oopDesc::klass_offset_in_bytes()));\n-      }\n-      __ beq(tmp, t0, known_ok);\n+      __ cmp_klass_compressed(dst, tmp, t0, known_ok, true);\n","filename":"src\/hotspot\/cpu\/riscv\/c1_LIRAssembler_arraycopy_riscv.cpp","additions":7,"deletions":20,"binary":false,"changes":27,"status":"modified"},{"patch":"@@ -1528,6 +1528,1 @@\n-  if (UseCompressedClassPointers) {\n-    __ lwu(result, Address(obj, oopDesc::klass_offset_in_bytes()));\n-    __ decode_klass_not_null(result);\n-  } else {\n-    __ ld(result, Address(obj, oopDesc::klass_offset_in_bytes()));\n-  }\n+  __ load_klass(result, obj);\n","filename":"src\/hotspot\/cpu\/riscv\/c1_LIRAssembler_riscv.cpp","additions":1,"deletions":6,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -167,7 +167,3 @@\n-  \/\/ This assumes that all prototype bits fitr in an int32_t\n-  mv(tmp1, (int32_t)(intptr_t)markWord::prototype().value());\n-  sd(tmp1, Address(obj, oopDesc::mark_offset_in_bytes()));\n-\n-  if (UseCompressedClassPointers) { \/\/ Take care not to kill klass\n-    encode_klass_not_null(tmp1, klass, tmp2);\n-    sw(tmp1, Address(obj, oopDesc::klass_offset_in_bytes()));\n+  if (UseCompactObjectHeaders) {\n+    ld(tmp1, Address(klass, Klass::prototype_header_offset()));\n+    sd(tmp1, Address(obj, oopDesc::mark_offset_in_bytes()));\n@@ -175,1 +171,9 @@\n-    sd(klass, Address(obj, oopDesc::klass_offset_in_bytes()));\n+    \/\/ This assumes that all prototype bits fitr in an int32_t\n+    mv(tmp1, checked_cast<int32_t>(markWord::prototype().value()));\n+    sd(tmp1, Address(obj, oopDesc::mark_offset_in_bytes()));\n+    if (UseCompressedClassPointers) { \/\/ Take care not to kill klass\n+      encode_klass_not_null(tmp1, klass, tmp2);\n+      sw(tmp1, Address(obj, oopDesc::klass_offset_in_bytes()));\n+    } else {\n+      sd(klass, Address(obj, oopDesc::klass_offset_in_bytes()));\n+    }\n@@ -186,1 +190,1 @@\n-  } else if (UseCompressedClassPointers) {\n+  } else if (UseCompressedClassPointers && !UseCompactObjectHeaders) {\n","filename":"src\/hotspot\/cpu\/riscv\/c1_MacroAssembler_riscv.cpp","additions":13,"deletions":9,"binary":false,"changes":22,"status":"modified"},{"patch":"@@ -3122,0 +3122,10 @@\n+\n+void C2_MacroAssembler::load_narrow_klass_compact_c2(Register dst, Address src) {\n+  \/\/ The incoming address is pointing into obj-start + klass_offset_in_bytes. We need to extract\n+  \/\/ obj-start, so that we can load from the object's mark-word instead. Usually the address\n+  \/\/ comes as obj-start in obj and klass_offset_in_bytes in disp.\n+  assert(UseCompactObjectHeaders, \"must\");\n+  int offset = oopDesc::mark_offset_in_bytes() - oopDesc::klass_offset_in_bytes();\n+  ld(dst, Address(src.base(), src.offset() + offset));\n+  srli(dst, dst, markWord::klass_shift);\n+}\n","filename":"src\/hotspot\/cpu\/riscv\/c2_MacroAssembler_riscv.cpp","additions":10,"deletions":0,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -280,0 +280,2 @@\n+  void load_narrow_klass_compact_c2(Register dst, Address src);\n+\n","filename":"src\/hotspot\/cpu\/riscv\/c2_MacroAssembler_riscv.hpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -59,1 +59,1 @@\n-  \/\/ Failing that, optimize for case (3) - a base with only bits set between [33-44)\n+  \/\/ Failing that, optimize for case (3) - a base with only bits set between [32-44)\n@@ -61,1 +61,1 @@\n-    const uintptr_t from = nth_bit(32 + (optimize_for_zero_base ? LogKlassAlignmentInBytes : 0));\n+    const uintptr_t from = nth_bit(32);\n","filename":"src\/hotspot\/cpu\/riscv\/compressedKlass_riscv.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -2487,10 +2487,5 @@\n-void MacroAssembler::cmp_klass(Register oop, Register trial_klass, Register tmp1, Register tmp2, Label &L) {\n-  assert_different_registers(oop, trial_klass, tmp1, tmp2);\n-  if (UseCompressedClassPointers) {\n-    lwu(tmp1, Address(oop, oopDesc::klass_offset_in_bytes()));\n-    if (CompressedKlassPointers::base() == nullptr) {\n-      slli(tmp1, tmp1, CompressedKlassPointers::shift());\n-      beq(trial_klass, tmp1, L);\n-      return;\n-    }\n-    decode_klass_not_null(tmp1, tmp2);\n+void MacroAssembler::cmp_klass_compressed(Register oop, Register trial_klass, Register tmp, Label &L, bool equal) {\n+  if (UseCompactObjectHeaders) {\n+    load_narrow_klass_compact(tmp, oop);\n+  } else if (UseCompressedClassPointers) {\n+    lwu(tmp, Address(oop, oopDesc::klass_offset_in_bytes()));\n@@ -2498,1 +2493,6 @@\n-    ld(tmp1, Address(oop, oopDesc::klass_offset_in_bytes()));\n+    ld(tmp, Address(oop, oopDesc::klass_offset_in_bytes()));\n+  }\n+  if (equal) {\n+    beq(trial_klass, tmp, L);\n+  } else {\n+    bne(trial_klass, tmp, L);\n@@ -2500,1 +2500,0 @@\n-  beq(trial_klass, tmp1, L);\n@@ -2727,0 +2726,6 @@\n+void MacroAssembler::load_narrow_klass_compact(Register dst, Register src) {\n+  assert(UseCompactObjectHeaders, \"expects UseCompactObjectHeaders\");\n+  ld(dst, Address(src, oopDesc::mark_offset_in_bytes()));\n+  srli(dst, dst, markWord::klass_shift);\n+}\n+\n@@ -2730,1 +2735,4 @@\n-  if (UseCompressedClassPointers) {\n+  if (UseCompactObjectHeaders) {\n+    load_narrow_klass_compact(dst, src);\n+    decode_klass_not_null(dst, tmp);\n+  } else if (UseCompressedClassPointers) {\n@@ -2741,0 +2749,1 @@\n+  assert(!UseCompactObjectHeaders, \"not with compact headers\");\n@@ -2750,0 +2759,1 @@\n+  assert(!UseCompactObjectHeaders, \"not with compact headers\");\n@@ -2766,2 +2776,1 @@\n-      assert(LogKlassAlignmentInBytes == CompressedKlassPointers::shift(), \"decode alg wrong\");\n-      slli(dst, src, LogKlassAlignmentInBytes);\n+      slli(dst, src, CompressedKlassPointers::shift());\n@@ -2783,3 +2792,3 @@\n-    assert(LogKlassAlignmentInBytes == CompressedKlassPointers::shift(), \"decode alg wrong\");\n-    assert_different_registers(t0, xbase);\n-    shadd(dst, src, xbase, t0, LogKlassAlignmentInBytes);\n+    Register t = src == dst ? dst : t0;\n+    assert_different_registers(t, xbase);\n+    shadd(dst, src, xbase, t, CompressedKlassPointers::shift());\n@@ -2801,2 +2810,1 @@\n-      assert(LogKlassAlignmentInBytes == CompressedKlassPointers::shift(), \"decode alg wrong\");\n-      srli(dst, src, LogKlassAlignmentInBytes);\n+      srli(dst, src, CompressedKlassPointers::shift());\n@@ -2824,2 +2832,1 @@\n-    assert(LogKlassAlignmentInBytes == CompressedKlassPointers::shift(), \"decode alg wrong\");\n-    srli(dst, dst, LogKlassAlignmentInBytes);\n+    srli(dst, dst, CompressedKlassPointers::shift());\n@@ -4320,1 +4327,1 @@\n-          far_branch_size();\n+          far_branch_size() + (UseCompactObjectHeaders ? MacroAssembler::instruction_size * 1 : 0);\n@@ -4340,1 +4347,4 @@\n-  if (UseCompressedClassPointers) {\n+  if (UseCompactObjectHeaders) {\n+    load_narrow_klass_compact(tmp1, receiver);\n+    lwu(tmp2, Address(data, CompiledICData::speculated_klass_offset()));\n+  } else if (UseCompressedClassPointers) {\n","filename":"src\/hotspot\/cpu\/riscv\/macroAssembler_riscv.cpp","additions":34,"deletions":24,"binary":false,"changes":58,"status":"modified"},{"patch":"@@ -198,0 +198,1 @@\n+  void load_narrow_klass_compact(Register dst, Register src);\n@@ -199,1 +200,1 @@\n-  void cmp_klass(Register oop, Register trial_klass, Register tmp1, Register tmp2, Label &L);\n+  void cmp_klass_compressed(Register oop, Register trial_klass, Register tmp, Label &L, bool equal);\n","filename":"src\/hotspot\/cpu\/riscv\/macroAssembler_riscv.hpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -4824,0 +4824,1 @@\n+  predicate(!UseCompactObjectHeaders);\n@@ -4836,0 +4837,15 @@\n+instruct loadNKlassCompactHeaders(iRegNNoSp dst, memory mem)\n+%{\n+  predicate(UseCompactObjectHeaders);\n+  match(Set dst (LoadNKlass mem));\n+\n+  ins_cost(LOAD_COST);\n+  format %{ \"lwu  $dst, $mem\\t# loadNKlass, compressed class ptr, #@loadNKlass\" %}\n+\n+  ins_encode %{\n+    __ load_narrow_klass_compact_c2(as_Register($dst$$reg), Address(as_Register($mem$$base), $mem$$disp));\n+  %}\n+\n+  ins_pipe(iload_reg_mem);\n+%}\n+\n","filename":"src\/hotspot\/cpu\/riscv\/riscv.ad","additions":16,"deletions":0,"binary":false,"changes":16,"status":"modified"},{"patch":"@@ -3568,1 +3568,6 @@\n-    __ sub(x13, x13, sizeof(oopDesc));\n+    if (UseCompactObjectHeaders) {\n+      assert(is_aligned(oopDesc::base_offset_in_bytes(), BytesPerLong), \"oop base offset must be 8-byte-aligned\");\n+      __ sub(x13, x13, oopDesc::base_offset_in_bytes());\n+    } else {\n+      __ sub(x13, x13, sizeof(oopDesc));\n+    }\n@@ -3573,1 +3578,6 @@\n-      __ add(x12, x10, sizeof(oopDesc));\n+      if (UseCompactObjectHeaders) {\n+        assert(is_aligned(oopDesc::base_offset_in_bytes(), BytesPerLong), \"oop base offset must be 8-byte-aligned\");\n+        __ add(x12, x10, oopDesc::base_offset_in_bytes());\n+      } else {\n+        __ add(x12, x10, sizeof(oopDesc));\n+      }\n@@ -3584,4 +3594,9 @@\n-    __ mv(t0, (intptr_t)markWord::prototype().value());\n-    __ sd(t0, Address(x10, oopDesc::mark_offset_in_bytes()));\n-    __ store_klass_gap(x10, zr);   \/\/ zero klass gap for compressed oops\n-    __ store_klass(x10, x14);      \/\/ store klass last\n+    if (UseCompactObjectHeaders) {\n+      __ ld(t0, Address(x14, Klass::prototype_header_offset()));\n+      __ sd(t0, Address(x10, oopDesc::mark_offset_in_bytes()));\n+    } else {\n+      __ mv(t0, (intptr_t)markWord::prototype().value());\n+      __ sd(t0, Address(x10, oopDesc::mark_offset_in_bytes()));\n+      __ store_klass_gap(x10, zr);   \/\/ zero klass gap for compressed oops\n+      __ store_klass(x10, x14);      \/\/ store klass last\n+    }\n","filename":"src\/hotspot\/cpu\/riscv\/templateTable_riscv.cpp","additions":21,"deletions":6,"binary":false,"changes":27,"status":"modified"},{"patch":"@@ -3855,1 +3855,1 @@\n-  z_tmll(current, KlassAlignmentInBytes-1); \/\/ Check alignment.\n+  z_tmll(current, CompressedKlassPointers::klass_alignment_in_bytes() - 1); \/\/ Check alignment.\n@@ -3869,1 +3869,0 @@\n-    assert (LogKlassAlignmentInBytes == shift, \"decode alg wrong\");\n@@ -3999,1 +3998,1 @@\n-  z_tmll(dst, KlassAlignmentInBytes-1); \/\/ Check alignment.\n+  z_tmll(dst, CompressedKlassPointers::klass_alignment_in_bytes() - 1); \/\/ Check alignment.\n@@ -4046,1 +4045,1 @@\n-  z_tmll(dst, KlassAlignmentInBytes-1); \/\/ Check alignment.\n+  z_tmll(dst, CompressedKlassPointers::klass_alignment_in_bytes() - 1); \/\/ Check alignment.\n@@ -4115,1 +4114,5 @@\n-    assert((shift == 0) || (shift == LogKlassAlignmentInBytes), \"cKlass encoder detected bad shift\");\n+    if (UseCompactObjectHeaders) {\n+      assert(shift >= 3, \"cKlass encoder detected bad shift\");\n+    } else {\n+      assert((shift == 0) || (shift == 3), \"cKlass encoder detected bad shift\");\n+    }\n","filename":"src\/hotspot\/cpu\/s390\/macroAssembler_s390.cpp","additions":8,"deletions":5,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -3052,0 +3052,1 @@\n+  Register tmp2 = UseCompactObjectHeaders ? rscratch2 : noreg;\n@@ -3176,2 +3177,0 @@\n-  Address src_klass_addr = Address(src, oopDesc::klass_offset_in_bytes());\n-  Address dst_klass_addr = Address(dst, oopDesc::klass_offset_in_bytes());\n@@ -3243,7 +3242,1 @@\n-      if (UseCompressedClassPointers) {\n-        __ movl(tmp, src_klass_addr);\n-        __ cmpl(tmp, dst_klass_addr);\n-      } else {\n-        __ movptr(tmp, src_klass_addr);\n-        __ cmpptr(tmp, dst_klass_addr);\n-      }\n+      __ cmp_klasses_from_objects(src, dst, tmp, tmp2);\n@@ -3308,0 +3301,1 @@\n+       Address dst_klass_addr = Address(dst, oopDesc::klass_offset_in_bytes());\n@@ -3411,3 +3405,1 @@\n-\n-      if (UseCompressedClassPointers)          __ cmpl(tmp, dst_klass_addr);\n-      else                   __ cmpptr(tmp, dst_klass_addr);\n+      __ cmp_klass(tmp, dst, tmp2);\n@@ -3415,2 +3407,1 @@\n-      if (UseCompressedClassPointers)          __ cmpl(tmp, src_klass_addr);\n-      else                   __ cmpptr(tmp, src_klass_addr);\n+      __ cmp_klass(tmp, src, tmp2);\n@@ -3419,2 +3410,1 @@\n-      if (UseCompressedClassPointers)          __ cmpl(tmp, dst_klass_addr);\n-      else                   __ cmpptr(tmp, dst_klass_addr);\n+      __ cmp_klass(tmp, dst, tmp2);\n@@ -3517,7 +3507,1 @@\n-#ifdef _LP64\n-  if (UseCompressedClassPointers) {\n-    __ movl(result, Address(obj, oopDesc::klass_offset_in_bytes()));\n-    __ decode_klass_not_null(result, rscratch1);\n-  } else\n-#endif\n-    __ movptr(result, Address(obj, oopDesc::klass_offset_in_bytes()));\n+  __ load_klass(result, obj, rscratch1);\n","filename":"src\/hotspot\/cpu\/x86\/c1_LIRAssembler_x86.cpp","additions":7,"deletions":23,"binary":false,"changes":30,"status":"modified"},{"patch":"@@ -173,2 +173,1 @@\n-  assert_different_registers(obj, klass, len);\n-  movptr(Address(obj, oopDesc::mark_offset_in_bytes()), checked_cast<int32_t>(markWord::prototype().value()));\n+  assert_different_registers(obj, klass, len, t1, t2);\n@@ -176,1 +175,5 @@\n-  if (UseCompressedClassPointers) { \/\/ Take care not to kill klass\n+  if (UseCompactObjectHeaders) {\n+    movptr(t1, Address(klass, Klass::prototype_header_offset()));\n+    movptr(Address(obj, oopDesc::mark_offset_in_bytes()), t1);\n+  } else if (UseCompressedClassPointers) { \/\/ Take care not to kill klass\n+    movptr(Address(obj, oopDesc::mark_offset_in_bytes()), checked_cast<int32_t>(markWord::prototype().value()));\n@@ -183,0 +186,1 @@\n+    movptr(Address(obj, oopDesc::mark_offset_in_bytes()), checked_cast<int32_t>(markWord::prototype().value()));\n@@ -199,1 +203,1 @@\n-  else if (UseCompressedClassPointers) {\n+  else if (UseCompressedClassPointers && !UseCompactObjectHeaders) {\n@@ -233,1 +237,3 @@\n-\n+  if (UseCompactObjectHeaders) {\n+    assert(hdr_size_in_bytes == 8, \"check object headers size\");\n+  }\n","filename":"src\/hotspot\/cpu\/x86\/c1_MacroAssembler_x86.cpp","additions":11,"deletions":5,"binary":false,"changes":16,"status":"modified"},{"patch":"@@ -6478,0 +6478,10 @@\n+\n+#ifdef _LP64\n+void C2_MacroAssembler::load_narrow_klass_compact_c2(Register dst, Address src) {\n+  \/\/ The incoming address is pointing into obj-start + klass_offset_in_bytes. We need to extract\n+  \/\/ obj-start, so that we can load from the object's mark-word instead. Usually the address\n+  \/\/ comes as obj-start in obj and klass_offset_in_bytes in disp.\n+  movq(dst, src.plus_disp(-oopDesc::klass_offset_in_bytes()));\n+  shrq(dst, markWord::klass_shift);\n+}\n+#endif\n","filename":"src\/hotspot\/cpu\/x86\/c2_MacroAssembler_x86.cpp","additions":10,"deletions":0,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -508,0 +508,2 @@\n+  void load_narrow_klass_compact_c2(Register dst, Address src);\n+\n","filename":"src\/hotspot\/cpu\/x86\/c2_MacroAssembler_x86.hpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -29,0 +29,1 @@\n+#include \"oops\/arrayOop.hpp\"\n@@ -163,0 +164,3 @@\n+static void copy_to_stack(Register haystack, Register haystack_len, bool isU, Register tmp,\n+                          XMMRegister xtmp, MacroAssembler *_masm);\n+\n@@ -398,1 +402,1 @@\n-  __ ja_b(L_bigSwitchTop);\n+  __ ja(L_bigSwitchTop);\n@@ -404,1 +408,1 @@\n-  \/\/       at least 16 bytes of header preceeding the haystack pointer.\n+  \/\/       at least 8 bytes of header preceeding the haystack pointer.\n@@ -406,1 +410,1 @@\n-  \/\/ This means that we're copying up to 15 bytes of the header onto the stack along\n+  \/\/ This means that we're copying up to 7 bytes of the header onto the stack along\n@@ -410,3 +414,1 @@\n-    Label L_moreThan16, L_adjustHaystack;\n-\n-    const Register index = rax;\n+    const Register tmp = rax;\n@@ -414,19 +416,1 @@\n-\n-    \/\/ Only a single vector load\/store of either 16 or 32 bytes\n-    __ cmpq(haystack_len, 0x10);\n-    __ ja_b(L_moreThan16);\n-\n-    __ movq(index, COPIED_HAYSTACK_STACK_OFFSET + 0x10);\n-    __ movdqu(XMM_TMP1, Address(haystack, haystack_len, Address::times_1, -0x10));\n-    __ movdqu(Address(rsp, COPIED_HAYSTACK_STACK_OFFSET), XMM_TMP1);\n-    __ jmpb(L_adjustHaystack);\n-\n-    __ bind(L_moreThan16);\n-    __ movq(index, COPIED_HAYSTACK_STACK_OFFSET + 0x20);\n-    __ vmovdqu(XMM_TMP1, Address(haystack, haystack_len, Address::times_1, -0x20));\n-    __ vmovdqu(Address(rsp, COPIED_HAYSTACK_STACK_OFFSET), XMM_TMP1);\n-\n-    \/\/ Point the haystack at the correct location of the first byte of the \"real\" haystack on the stack\n-    __ bind(L_adjustHaystack);\n-    __ subq(index, haystack_len);\n-    __ leaq(haystack, Address(rsp, index, Address::times_1));\n+    copy_to_stack(haystack, haystack_len, false, tmp, XMM_TMP1, _masm);\n@@ -763,2 +747,2 @@\n-    \/\/ Reads of existing needle are 16-byte chunks\n-    \/\/ Writes to copied needle are 32-byte chunks\n+    \/\/ Reads of existing needle are 8-byte chunks\n+    \/\/ Writes to copied needle are 16-byte chunks\n@@ -767,3 +751,3 @@\n-    \/\/ Start first read at [((ndlLen % 16) - 16) & 0xf]\n-    \/\/ outndx += 32\n-    \/\/ inndx += 16\n+    \/\/ Start first read at [((ndlLen % 8) - 8) & 0x7]\n+    \/\/ outndx += 16\n+    \/\/ inndx += 8\n@@ -773,1 +757,1 @@\n-    \/\/ Final index of start of needle at ((16 - (ndlLen %16)) & 0xf) << 1\n+    \/\/ Final index of start of needle at ((8 - (ndlLen % 8)) & 0x7) << 1\n@@ -775,2 +759,2 @@\n-    \/\/ Starting read for needle at -(16 - (nLen % 16))\n-    \/\/ Offset of needle in stack should be (16 - (nLen % 16)) * 2\n+    \/\/ Starting read for needle at -(8 - (nLen % 8))\n+    \/\/ Offset of needle in stack should be (8 - (nLen % 8)) * 2\n@@ -779,3 +763,3 @@\n-    __ andq(index, 0xf);  \/\/ nLen % 16\n-    __ movq(offset, 0x10);\n-    __ subq(offset, index);  \/\/ 16 - (nLen % 16)\n+    __ andq(index, 0x7);  \/\/ nLen % 8\n+    __ movq(offset, 0x8);\n+    __ subq(offset, index);  \/\/ 8 - (nLen % 8)\n@@ -784,1 +768,1 @@\n-    __ negq(index);      \/\/ -(16 - (nLen % 16))\n+    __ negq(index);      \/\/ -(8 - (nLen % 8))\n@@ -789,1 +773,1 @@\n-    __ vpmovzxbw(xmm0, Address(needle, index, Address::times_1), Assembler::AVX_256bit);\n+    __ vpmovzxbw(xmm0, Address(needle, index, Address::times_1), Assembler::AVX_128bit);\n@@ -791,2 +775,2 @@\n-    __ vmovdqu(Address(rsp, wr_index, Address::times_1, EXPANDED_NEEDLE_STACK_OFFSET), xmm0);\n-    __ addq(index, 0x10);\n+    __ movdqu(Address(rsp, wr_index, Address::times_1, EXPANDED_NEEDLE_STACK_OFFSET), xmm0);\n+    __ addq(index, 0x8);\n@@ -795,1 +779,1 @@\n-    __ addq(wr_index, 32);\n+    __ addq(wr_index, 16);\n@@ -1585,29 +1569,3 @@\n-  \/\/ Copy incoming haystack onto stack\n-  {\n-    Label L_adjustHaystack, L_moreThan16;\n-\n-    \/\/ Copy haystack to stack (haystack <= 32 bytes)\n-    __ subptr(rsp, COPIED_HAYSTACK_STACK_SIZE);\n-    __ cmpq(haystack_len, isU ? 0x8 : 0x10);\n-    __ ja_b(L_moreThan16);\n-\n-    __ movq(tmp, COPIED_HAYSTACK_STACK_OFFSET + 0x10);\n-    __ movdqu(XMM0, Address(haystack, haystack_len, isU ? Address::times_2 : Address::times_1, -0x10));\n-    __ movdqu(Address(rsp, COPIED_HAYSTACK_STACK_OFFSET), XMM0);\n-    __ jmpb(L_adjustHaystack);\n-\n-    __ bind(L_moreThan16);\n-    __ movq(tmp, COPIED_HAYSTACK_STACK_OFFSET + 0x20);\n-    __ vmovdqu(XMM0, Address(haystack, haystack_len, isU ? Address::times_2 : Address::times_1, -0x20));\n-    __ vmovdqu(Address(rsp, COPIED_HAYSTACK_STACK_OFFSET), XMM0);\n-\n-    __ bind(L_adjustHaystack);\n-    __ subptr(tmp, haystack_len);\n-\n-    if (isU) {\n-      \/\/ For UTF-16, lengths are half\n-      __ subptr(tmp, haystack_len);\n-    }\n-    \/\/ Point the haystack to the stack\n-    __ leaq(haystack, Address(rsp, tmp, Address::times_1));\n-  }\n+  \/\/ Copy incoming haystack onto stack (haystack <= 32 bytes)\n+  __ subptr(rsp, COPIED_HAYSTACK_STACK_SIZE);\n+  copy_to_stack(haystack, haystack_len, isU, tmp, XMM0, _masm);\n@@ -1675,0 +1633,80 @@\n+\n+\n+\/\/ Copy the small (<= 32 byte) haystack to the stack.  Allows for vector reads without page fault\n+\/\/ Only done for small haystacks\n+\/\/ NOTE: This code assumes that the haystack points to a java array type AND there are\n+\/\/       at least 8 bytes of header preceeding the haystack pointer.\n+\/\/ We're copying up to 7 bytes of the header onto the stack along with the haystack bytes.\n+\/\/ After the copy completes, we adjust the haystack pointer\n+\/\/ to the valid haystack bytes on the stack.\n+\/\/\n+\/\/ Copy haystack array elements to stack at region\n+\/\/ (COPIED_HAYSTACK_STACK_OFFSET - COPIED_HAYSTACK_STACK_OFFSET+63) with the following conditions:\n+\/\/   It may copy up to 7 bytes that precede the array\n+\/\/   It doesn't read beyond the end of the array\n+\/\/   There are atleast 31 bytes of stack region beyond the end of array\n+\/\/ Inputs:\n+\/\/   haystack - Address of haystack\n+\/\/   haystack_len - Number of elements in haystack\n+\/\/   isU - Boolean indicating if each element is Latin1 or UTF16\n+\/\/   tmp, xtmp - Scratch registers\n+\/\/ Output:\n+\/\/   haystack - Address of copied string on stack\n+\n+static void copy_to_stack(Register haystack, Register haystack_len, bool isU,\n+                          Register tmp, XMMRegister xtmp, MacroAssembler *_masm) {\n+  Label L_moreThan8, L_moreThan16, L_moreThan24, L_adjustHaystack;\n+\n+  assert(arrayOopDesc::base_offset_in_bytes(isU ? T_CHAR : T_BYTE) >= 8,\n+         \"Needs at least 8 bytes preceding the array body\");\n+\n+  \/\/ Copy haystack to stack (haystack <= 32 bytes)\n+  int scale = isU ? 2 : 1; \/\/ bytes per char\n+  Address::ScaleFactor addrScale = isU ? Address::times_2 : Address::times_1;\n+\n+  __ cmpq(haystack_len, 16\/scale);\n+  __ ja_b(L_moreThan16);\n+\n+  __ cmpq(haystack_len, 8\/scale);\n+  __ ja_b(L_moreThan8);\n+  \/\/ haystack length <= 8 bytes, copy 8 bytes upto haystack end reading at most 7 bytes into the header\n+  __ movq(tmp, COPIED_HAYSTACK_STACK_OFFSET + 8);\n+  __ movq(xtmp, Address(haystack, haystack_len, addrScale, -8));\n+  __ movq(Address(rsp, COPIED_HAYSTACK_STACK_OFFSET), xtmp);\n+  __ jmpb(L_adjustHaystack);\n+\n+  __ bind(L_moreThan8);\n+  \/\/ haystack length > 8 and <=16 bytes, copy 16 bytes upto haystack end reading at most 7 bytes into the header\n+  __ movq(tmp, COPIED_HAYSTACK_STACK_OFFSET + 16);\n+  __ movdqu(xtmp, Address(haystack, haystack_len, addrScale, -16));\n+  __ movdqu(Address(rsp, COPIED_HAYSTACK_STACK_OFFSET), xtmp);\n+  __ jmpb(L_adjustHaystack);\n+\n+  __ bind(L_moreThan16);\n+  __ cmpq(haystack_len, 24\/scale);\n+  __ ja_b(L_moreThan24);\n+  \/\/ haystack length > 16 and <=24 bytes, copy 24 bytes upto haystack end reading at most 7 bytes into the header\n+  __ movq(tmp, COPIED_HAYSTACK_STACK_OFFSET + 24);\n+  __ movdqu(xtmp, Address(haystack, haystack_len, addrScale, -24));\n+  __ movdqu(Address(rsp, COPIED_HAYSTACK_STACK_OFFSET), xtmp);\n+  __ movq(xtmp, Address(haystack, haystack_len, addrScale, -8));\n+  __ movq(Address(rsp, COPIED_HAYSTACK_STACK_OFFSET + 16), xtmp);\n+  __ jmpb(L_adjustHaystack);\n+\n+  __ bind(L_moreThan24);\n+  \/\/ haystack length > 24 and < 32 bytes, copy 32 bytes upto haystack end reading at most 7 bytes into the header\n+  __ movq(tmp, COPIED_HAYSTACK_STACK_OFFSET + 32);\n+  __ vmovdqu(xtmp, Address(haystack, haystack_len, addrScale, -32));\n+  __ vmovdqu(Address(rsp, COPIED_HAYSTACK_STACK_OFFSET), xtmp);\n+\n+  __ bind(L_adjustHaystack);\n+  __ subptr(tmp, haystack_len);\n+\n+  if (isU) {\n+    __ subptr(tmp, haystack_len);\n+  }\n+\n+  \/\/ Point the haystack to the stack\n+  __ leaq(haystack, Address(rsp, tmp, Address::times_1));\n+}\n+\n","filename":"src\/hotspot\/cpu\/x86\/c2_stubGenerator_x86_64_string.cpp","additions":108,"deletions":70,"binary":false,"changes":178,"status":"modified"},{"patch":"@@ -1353,1 +1353,2 @@\n-  return LP64_ONLY(14) NOT_LP64(12);\n+  return\n+      LP64_ONLY(UseCompactObjectHeaders ? 17 : 14) NOT_LP64(12);\n@@ -1369,0 +1370,6 @@\n+#ifdef _LP64\n+  if (UseCompactObjectHeaders) {\n+    load_narrow_klass_compact(temp, receiver);\n+    cmpl(temp, Address(data, CompiledICData::speculated_klass_offset()));\n+  } else\n+#endif\n@@ -1379,1 +1386,1 @@\n-  assert((offset() % end_alignment) == 0, \"Misaligned verified entry point\");\n+  assert((offset() % end_alignment) == 0, \"Misaligned verified entry point (%d, %d, %d)\", uep_offset, offset(), end_alignment);\n@@ -5677,0 +5684,8 @@\n+#ifdef _LP64\n+void MacroAssembler::load_narrow_klass_compact(Register dst, Register src) {\n+  assert(UseCompactObjectHeaders, \"expect compact object headers\");\n+  movq(dst, Address(src, oopDesc::mark_offset_in_bytes()));\n+  shrq(dst, markWord::klass_shift);\n+}\n+#endif\n+\n@@ -5681,1 +5696,4 @@\n-  if (UseCompressedClassPointers) {\n+  if (UseCompactObjectHeaders) {\n+    load_narrow_klass_compact(dst, src);\n+    decode_klass_not_null(dst, tmp);\n+  } else if (UseCompressedClassPointers) {\n@@ -5686,0 +5704,1 @@\n+  {\n@@ -5687,0 +5706,1 @@\n+  }\n@@ -5690,0 +5710,1 @@\n+  assert(!UseCompactObjectHeaders, \"not with compact headers\");\n@@ -5701,0 +5722,35 @@\n+void MacroAssembler::cmp_klass(Register klass, Register obj, Register tmp) {\n+#ifdef _LP64\n+  if (UseCompactObjectHeaders) {\n+    assert(tmp != noreg, \"need tmp\");\n+    assert_different_registers(klass, obj, tmp);\n+    load_narrow_klass_compact(tmp, obj);\n+    cmpl(klass, tmp);\n+  } else if (UseCompressedClassPointers) {\n+    cmpl(klass, Address(obj, oopDesc::klass_offset_in_bytes()));\n+  } else\n+#endif\n+  {\n+    cmpptr(klass, Address(obj, oopDesc::klass_offset_in_bytes()));\n+  }\n+}\n+\n+void MacroAssembler::cmp_klasses_from_objects(Register obj1, Register obj2, Register tmp1, Register tmp2) {\n+#ifdef _LP64\n+  if (UseCompactObjectHeaders) {\n+    assert(tmp2 != noreg, \"need tmp2\");\n+    assert_different_registers(obj1, obj2, tmp1, tmp2);\n+    load_narrow_klass_compact(tmp1, obj1);\n+    load_narrow_klass_compact(tmp2, obj2);\n+    cmpl(tmp1, tmp2);\n+  } else if (UseCompressedClassPointers) {\n+    movl(tmp1, Address(obj1, oopDesc::klass_offset_in_bytes()));\n+    cmpl(tmp1, Address(obj2, oopDesc::klass_offset_in_bytes()));\n+  } else\n+#endif\n+  {\n+    movptr(tmp1, Address(obj1, oopDesc::klass_offset_in_bytes()));\n+    cmpptr(tmp1, Address(obj2, oopDesc::klass_offset_in_bytes()));\n+  }\n+}\n+\n@@ -5748,0 +5804,1 @@\n+  assert(!UseCompactObjectHeaders, \"Don't use with compact headers\");\n@@ -5912,2 +5969,1 @@\n-    assert (LogKlassAlignmentInBytes == CompressedKlassPointers::shift(), \"decode alg wrong\");\n-    shrq(r, LogKlassAlignmentInBytes);\n+    shrq(r, CompressedKlassPointers::shift());\n@@ -5926,2 +5982,1 @@\n-    assert (LogKlassAlignmentInBytes == CompressedKlassPointers::shift(), \"decode alg wrong\");\n-    shrq(dst, LogKlassAlignmentInBytes);\n+    shrq(dst, CompressedKlassPointers::shift());\n@@ -5939,2 +5994,1 @@\n-    assert(LogKlassAlignmentInBytes == CompressedKlassPointers::shift(), \"decode alg wrong\");\n-    shlq(r, LogKlassAlignmentInBytes);\n+    shlq(r, CompressedKlassPointers::shift());\n@@ -5962,9 +6016,12 @@\n-    if (CompressedKlassPointers::base() != nullptr) {\n-      mov64(dst, (int64_t)CompressedKlassPointers::base());\n-    } else {\n-      xorq(dst, dst);\n-    }\n-    if (CompressedKlassPointers::shift() != 0) {\n-      assert(LogKlassAlignmentInBytes == CompressedKlassPointers::shift(), \"decode alg wrong\");\n-      assert(LogKlassAlignmentInBytes == Address::times_8, \"klass not aligned on 64bits?\");\n-      leaq(dst, Address(dst, src, Address::times_8, 0));\n+    if (CompressedKlassPointers::shift() <= Address::times_8) {\n+      if (CompressedKlassPointers::base() != nullptr) {\n+        mov64(dst, (int64_t)CompressedKlassPointers::base());\n+      } else {\n+        xorq(dst, dst);\n+      }\n+      if (CompressedKlassPointers::shift() != 0) {\n+        assert(CompressedKlassPointers::shift() == Address::times_8, \"klass not aligned on 64bits?\");\n+        leaq(dst, Address(dst, src, Address::times_8, 0));\n+      } else {\n+        addq(dst, src);\n+      }\n@@ -5972,0 +6029,7 @@\n+      if (CompressedKlassPointers::base() != nullptr) {\n+        const uint64_t base_right_shifted =\n+            (uint64_t)CompressedKlassPointers::base() >> CompressedKlassPointers::shift();\n+        mov64(dst, base_right_shifted);\n+      } else {\n+        xorq(dst, dst);\n+      }\n@@ -5973,0 +6037,1 @@\n+      shlq(dst, CompressedKlassPointers::shift());\n","filename":"src\/hotspot\/cpu\/x86\/macroAssembler_x86.cpp","additions":83,"deletions":18,"binary":false,"changes":101,"status":"modified"},{"patch":"@@ -366,0 +366,3 @@\n+#ifdef _LP64\n+  void load_narrow_klass_compact(Register dst, Register src);\n+#endif\n@@ -369,0 +372,8 @@\n+  \/\/ Compares the Klass pointer of an object to a given Klass (which might be narrow,\n+  \/\/ depending on UseCompressedClassPointers).\n+  void cmp_klass(Register klass, Register obj, Register tmp);\n+\n+  \/\/ Compares the Klass pointer of two objects obj1 and obj2. Result is in the condition flags.\n+  \/\/ Uses tmp1 and tmp2 as temporary registers.\n+  void cmp_klasses_from_objects(Register obj1, Register obj2, Register tmp1, Register tmp2);\n+\n","filename":"src\/hotspot\/cpu\/x86\/macroAssembler_x86.hpp","additions":11,"deletions":0,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -96,1 +96,1 @@\n-    return (LogKlassAlignmentInBytes <= 3);\n+    return (CompressedKlassPointers::shift() <= 3);\n","filename":"src\/hotspot\/cpu\/x86\/matcher_x86.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -40,1 +40,1 @@\n-  _compiler_stubs_code_size     = 20000 LP64_ONLY(+46000) WINDOWS_ONLY(+2000),\n+  _compiler_stubs_code_size     = 20000 LP64_ONLY(+47000) WINDOWS_ONLY(+2000),\n","filename":"src\/hotspot\/cpu\/x86\/stubRoutines_x86.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -4087,1 +4087,6 @@\n-    __ decrement(rdx, sizeof(oopDesc));\n+    if (UseCompactObjectHeaders) {\n+      assert(is_aligned(oopDesc::base_offset_in_bytes(), BytesPerLong), \"oop base offset must be 8-byte-aligned\");\n+      __ decrement(rdx, oopDesc::base_offset_in_bytes());\n+    } else {\n+      __ decrement(rdx, sizeof(oopDesc));\n+    }\n@@ -4109,2 +4114,4 @@\n-    __ movptr(Address(rax, rdx, Address::times_8, sizeof(oopDesc) - 1*oopSize), rcx);\n-    NOT_LP64(__ movptr(Address(rax, rdx, Address::times_8, sizeof(oopDesc) - 2*oopSize), rcx));\n+    int header_size_bytes = oopDesc::header_size() * HeapWordSize;\n+    assert(is_aligned(header_size_bytes, BytesPerLong), \"oop header size must be 8-byte-aligned\");\n+    __ movptr(Address(rax, rdx, Address::times_8, header_size_bytes - 1*oopSize), rcx);\n+    NOT_LP64(__ movptr(Address(rax, rdx, Address::times_8, header_size_bytes - 2*oopSize), rcx));\n@@ -4117,3 +4124,8 @@\n-    __ movptr(Address(rax, oopDesc::mark_offset_in_bytes()),\n-              (intptr_t)markWord::prototype().value()); \/\/ header\n-    __ pop(rcx);   \/\/ get saved klass back in the register.\n+    if (UseCompactObjectHeaders) {\n+      __ pop(rcx);   \/\/ get saved klass back in the register.\n+      __ movptr(rbx, Address(rcx, Klass::prototype_header_offset()));\n+      __ movptr(Address(rax, oopDesc::mark_offset_in_bytes()), rbx);\n+    } else {\n+      __ movptr(Address(rax, oopDesc::mark_offset_in_bytes()),\n+                (intptr_t)markWord::prototype().value()); \/\/ header\n+      __ pop(rcx);   \/\/ get saved klass back in the register.\n@@ -4121,2 +4133,2 @@\n-    __ xorl(rsi, rsi); \/\/ use zero reg to clear memory (shorter code)\n-    __ store_klass_gap(rax, rsi);  \/\/ zero klass gap for compressed oops\n+      __ xorl(rsi, rsi); \/\/ use zero reg to clear memory (shorter code)\n+      __ store_klass_gap(rax, rsi);  \/\/ zero klass gap for compressed oops\n@@ -4124,1 +4136,2 @@\n-    __ store_klass(rax, rcx, rscratch1);  \/\/ klass\n+      __ store_klass(rax, rcx, rscratch1);  \/\/ klass\n+    }\n","filename":"src\/hotspot\/cpu\/x86\/templateTable_x86.cpp","additions":22,"deletions":9,"binary":false,"changes":31,"status":"modified"},{"patch":"@@ -4372,0 +4372,1 @@\n+  predicate(!UseCompactObjectHeaders);\n@@ -4382,0 +4383,13 @@\n+instruct loadNKlassCompactHeaders(rRegN dst, memory mem, rFlagsReg cr)\n+%{\n+  predicate(UseCompactObjectHeaders);\n+  match(Set dst (LoadNKlass mem));\n+  effect(KILL cr);\n+  ins_cost(125); \/\/ XXX\n+  format %{ \"load_narrow_klass_compact    $dst, $mem\\t# compressed klass ptr\" %}\n+  ins_encode %{\n+    __ load_narrow_klass_compact_c2($dst$$Register, $mem$$Address);\n+  %}\n+  ins_pipe(pipe_slow); \/\/ XXX\n+%}\n+\n@@ -11712,0 +11726,1 @@\n+  predicate(!UseCompactObjectHeaders);\n","filename":"src\/hotspot\/cpu\/x86\/x86_64.ad","additions":15,"deletions":0,"binary":false,"changes":15,"status":"modified"},{"patch":"@@ -230,2 +230,4 @@\n-    \/\/ See RunTimeClassInfo::get_for()\n-    _estimated_metaspaceobj_bytes += align_up(BytesPerWord, SharedSpaceObjectAlignment);\n+    \/\/ See RunTimeClassInfo::get_for(): make sure we have enough space for both maximum\n+    \/\/ Klass alignment as well as the RuntimeInfo* pointer we will embed in front of a Klass.\n+    _estimated_metaspaceobj_bytes += align_up(BytesPerWord, CompressedKlassPointers::klass_alignment_in_bytes()) +\n+        align_up(sizeof(void*), SharedSpaceObjectAlignment);\n@@ -664,1 +666,1 @@\n-    \/\/ Save a pointer immediate in front of an InstanceKlass, so\n+    \/\/ Allocate space for a pointer directly in front of the future InstanceKlass, so\n@@ -673,0 +675,12 @@\n+    \/\/ Allocate space for the future InstanceKlass with proper alignment\n+    const size_t alignment =\n+#ifdef _LP64\n+      UseCompressedClassPointers ?\n+        nth_bit(ArchiveBuilder::precomputed_narrow_klass_shift()) :\n+        SharedSpaceObjectAlignment;\n+#else\n+      SharedSpaceObjectAlignment;\n+#endif\n+    dest = dump_region->allocate(bytes, alignment);\n+  } else {\n+    dest = dump_region->allocate(bytes);\n@@ -674,1 +688,0 @@\n-  dest = dump_region->allocate(bytes);\n@@ -705,0 +718,2 @@\n+\n+  DEBUG_ONLY(_alloc_stats.verify((int)dump_region->used(), src_info->read_only()));\n@@ -783,0 +798,9 @@\n+#ifdef _LP64\n+    if (UseCompactObjectHeaders) {\n+      Klass* requested_k = to_requested(k);\n+      address narrow_klass_base = _requested_static_archive_bottom; \/\/ runtime encoding base == runtime mapping start\n+      const int narrow_klass_shift = precomputed_narrow_klass_shift();\n+      narrowKlass nk = CompressedKlassPointers::encode_not_null_without_asserts(requested_k, narrow_klass_base, narrow_klass_shift);\n+      k->set_prototype_header(markWord::prototype().set_narrow_klass(nk));\n+    }\n+#endif \/\/_LP64\n@@ -880,0 +904,5 @@\n+  const int narrow_klass_shift = ArchiveBuilder::precomputed_narrow_klass_shift();\n+#ifdef ASSERT\n+  const size_t klass_alignment = MAX2(SharedSpaceObjectAlignment, (size_t)nth_bit(narrow_klass_shift));\n+  assert(is_aligned(k, klass_alignment), \"Klass \" PTR_FORMAT \" misaligned.\", p2i(k));\n+#endif\n@@ -881,2 +910,3 @@\n-  const int narrow_klass_shift = ArchiveHeapWriter::precomputed_narrow_klass_shift;\n-  return CompressedKlassPointers::encode_not_null(requested_k, narrow_klass_base, narrow_klass_shift);\n+  \/\/ Note: use the \"raw\" version of encode that takes explicit narrow klass base and shift. Don't use any\n+  \/\/ of the variants that do sanity checks, nor any of those that use the current - dump - JVM's encoding setting.\n+  return CompressedKlassPointers::encode_not_null_without_asserts(requested_k, narrow_klass_base, narrow_klass_shift);\n@@ -962,0 +992,14 @@\n+#ifdef _LP64\n+int ArchiveBuilder::precomputed_narrow_klass_shift() {\n+  \/\/ Legacy Mode:\n+  \/\/    We use 32 bits for narrowKlass, which should cover the full 4G Klass range. Shift can be 0.\n+  \/\/ CompactObjectHeader Mode:\n+  \/\/    narrowKlass is much smaller, and we use the highest possible shift value to later get the maximum\n+  \/\/    Klass encoding range.\n+  \/\/\n+  \/\/ Note that all of this may change in the future, if we decide to correct the pre-calculated\n+  \/\/ narrow Klass IDs at archive load time.\n+  assert(UseCompressedClassPointers, \"Only needed for compressed class pointers\");\n+  return UseCompactObjectHeaders ?  CompressedKlassPointers::max_shift() : 0;\n+}\n+#endif \/\/ _LP64\n","filename":"src\/hotspot\/share\/cds\/archiveBuilder.cpp","additions":50,"deletions":6,"binary":false,"changes":56,"status":"modified"},{"patch":"@@ -30,0 +30,1 @@\n+#include \"memory\/metaspace.hpp\"\n@@ -46,3 +47,3 @@\n-\/\/ Metaspace::allocate() requires that all blocks must be aligned with KlassAlignmentInBytes.\n-\/\/ We enforce the same alignment rule in blocks allocated from the shared space.\n-const int SharedSpaceObjectAlignment = KlassAlignmentInBytes;\n+\/\/ The minimum alignment for non-Klass objects inside the CDS archive. Klass objects need\n+\/\/ to follow CompressedKlassPointers::klass_alignment_in_bytes().\n+constexpr size_t SharedSpaceObjectAlignment = Metaspace::min_allocation_alignment_bytes;\n@@ -463,0 +464,23 @@\n+\n+#ifdef _LP64\n+  \/\/ The CDS archive contains pre-computed narrow Klass IDs. It carries them in the headers of\n+  \/\/ archived heap objects. With +UseCompactObjectHeaders, it also carries them in prototypes\n+  \/\/ in Klass.\n+  \/\/ When generating the archive, these narrow Klass IDs are computed using the following scheme:\n+  \/\/ 1) The future encoding base is assumed to point to the first address of the generated mapping.\n+  \/\/    That means that at runtime, the narrow Klass encoding must be set up with base pointing to\n+  \/\/    the start address of the mapped CDS metadata archive (wherever that may be). This precludes\n+  \/\/    zero-based encoding.\n+  \/\/ 2) The shift must be large enough to result in an encoding range that covers the future assumed\n+  \/\/    runtime Klass range. That future Klass range will contain both the CDS metadata archive and\n+  \/\/    the future runtime class space. Since we do not know the size of the future class space, we\n+  \/\/    need to chose an encoding base\/shift combination that will result in a \"large enough\" size.\n+  \/\/    The details depend on whether we use compact object headers or legacy object headers.\n+  \/\/  In Legacy Mode, a narrow Klass ID is 32 bit. This gives us an encoding range size of 4G even\n+  \/\/    with shift = 0, which is all we need. Therefore, we use a shift=0 for pre-calculating the\n+  \/\/    narrow Klass IDs.\n+  \/\/ TinyClassPointer Mode:\n+  \/\/    We use the highest possible shift value to maximize the encoding range size.\n+  static int precomputed_narrow_klass_shift();\n+#endif \/\/ _LP64\n+\n","filename":"src\/hotspot\/share\/cds\/archiveBuilder.hpp","additions":27,"deletions":3,"binary":false,"changes":30,"status":"modified"},{"patch":"@@ -189,2 +189,6 @@\n-  oopDesc::set_mark(mem, markWord::prototype());\n-  oopDesc::release_set_klass(mem, Universe::objectArrayKlass());\n+  if (UseCompactObjectHeaders) {\n+    oopDesc::release_set_mark(mem, Universe::objectArrayKlass()->prototype_header());\n+  } else {\n+    oopDesc::set_mark(mem, markWord::prototype());\n+    oopDesc::release_set_klass(mem, Universe::objectArrayKlass());\n+  }\n@@ -353,1 +357,0 @@\n-  oopDesc::set_mark(mem, markWord::prototype());\n@@ -355,1 +358,6 @@\n-  cast_to_oop(mem)->set_narrow_klass(nk);\n+  if (UseCompactObjectHeaders) {\n+    oopDesc::release_set_mark(mem, markWord::prototype().set_narrow_klass(nk));\n+  } else {\n+    oopDesc::set_mark(mem, markWord::prototype());\n+    cast_to_oop(mem)->set_narrow_klass(nk);\n+  }\n@@ -559,1 +567,5 @@\n-  fake_oop->set_narrow_klass(nk);\n+  if (UseCompactObjectHeaders) {\n+    fake_oop->set_mark(markWord::prototype().set_narrow_klass(nk));\n+  } else {\n+    fake_oop->set_narrow_klass(nk);\n+  }\n@@ -565,1 +577,5 @@\n-    fake_oop->set_mark(markWord::prototype().copy_set_hash(src_hash));\n+    if (UseCompactObjectHeaders) {\n+      fake_oop->set_mark(markWord::prototype().set_narrow_klass(nk).copy_set_hash(src_hash));\n+    } else {\n+      fake_oop->set_mark(markWord::prototype().copy_set_hash(src_hash));\n+    }\n","filename":"src\/hotspot\/share\/cds\/archiveHeapWriter.cpp","additions":22,"deletions":6,"binary":false,"changes":28,"status":"modified"},{"patch":"@@ -243,11 +243,0 @@\n-  \/\/ Archived heap object headers carry pre-computed narrow Klass ids calculated with the\n-  \/\/ following scheme:\n-  \/\/ 1) the encoding base must be the mapping start address.\n-  \/\/ 2) shift must be large enough to result in an encoding range that covers the runtime Klass range.\n-  \/\/    That Klass range is defined by CDS archive size and runtime class space size. Luckily, the maximum\n-  \/\/    size can be predicted: archive size is assumed to be <1G, class space size capped at 3G, and at\n-  \/\/    runtime we put both regions adjacent to each other. Therefore, runtime Klass range size < 4G.\n-  \/\/    Since nKlass itself is 32 bit, our encoding range len is 4G, and since we set the base directly\n-  \/\/    at mapping start, these 4G are enough. Therefore, we don't need to shift at all (shift=0).\n-  static constexpr int precomputed_narrow_klass_shift = 0;\n-\n","filename":"src\/hotspot\/share\/cds\/archiveHeapWriter.hpp","additions":0,"deletions":11,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -244,3 +244,4 @@\n-\n-char* DumpRegion::allocate(size_t num_bytes) {\n-  char* p = (char*)align_up(_top, (size_t)SharedSpaceObjectAlignment);\n+char* DumpRegion::allocate(size_t num_bytes, size_t alignment) {\n+  \/\/ Always align to at least minimum alignment\n+  alignment = MAX2(SharedSpaceObjectAlignment, alignment);\n+  char* p = (char*)align_up(_top, alignment);\n@@ -347,1 +348,1 @@\n-  assert(tag == old_tag, \"old tag doesn't match\");\n+  assert(tag == old_tag, \"tag doesn't match (%d, expected %d)\", old_tag, tag);\n","filename":"src\/hotspot\/share\/cds\/archiveUtils.cpp","additions":5,"deletions":4,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -159,1 +159,2 @@\n-      _max_delta(max_delta), _is_packed(false) {}\n+      _max_delta(max_delta), _is_packed(false),\n+      _rs(NULL), _vs(NULL) {}\n@@ -162,1 +163,1 @@\n-  char* allocate(size_t num_bytes);\n+  char* allocate(size_t num_bytes, size_t alignment = 0);\n","filename":"src\/hotspot\/share\/cds\/archiveUtils.hpp","additions":3,"deletions":2,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -85,7 +85,14 @@\n-    size_t jvm_path_len = strlen(jvm_path);\n-    size_t file_sep_len = strlen(os::file_separator());\n-    const size_t len = jvm_path_len + file_sep_len + 20;\n-    _default_archive_path = NEW_C_HEAP_ARRAY(char, len, mtArguments);\n-    jio_snprintf(_default_archive_path, len,\n-                LP64_ONLY(!UseCompressedOops ? \"%s%sclasses_nocoops.jsa\":) \"%s%sclasses.jsa\",\n-                jvm_path, os::file_separator());\n+    stringStream tmp;\n+    tmp.print(\"%s%sclasses\", jvm_path, os::file_separator());\n+#ifdef _LP64\n+    if (!UseCompressedOops) {\n+      tmp.print_raw(\"_nocoops\");\n+    }\n+    if (UseCompactObjectHeaders) {\n+      \/\/ Note that generation of xxx_coh.jsa variants require\n+      \/\/ --enable-cds-archive-coh at build time\n+      tmp.print_raw(\"_coh\");\n+    }\n+#endif\n+    tmp.print_raw(\".jsa\");\n+    _default_archive_path = os::strdup(tmp.base());\n","filename":"src\/hotspot\/share\/cds\/cdsConfig.cpp","additions":14,"deletions":7,"binary":false,"changes":21,"status":"modified"},{"patch":"@@ -118,0 +118,12 @@\n+\n+#ifdef ASSERT\n+void DumpAllocStats::verify(int expected_byte_size, bool read_only) const {\n+  int bytes = 0;\n+  const int what = (int)(read_only ? RO : RW);\n+  for (int type = 0; type < int(_number_of_types); type ++) {\n+    bytes += _bytes[what][type];\n+  }\n+  assert(bytes == expected_byte_size, \"counter mismatch (%s: %d vs %d)\",\n+         (read_only ? \"RO\" : \"RW\"), bytes, expected_byte_size);\n+}\n+#endif \/\/ ASSERT\n","filename":"src\/hotspot\/share\/cds\/dumpAllocStats.cpp","additions":12,"deletions":0,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -138,0 +138,3 @@\n+\n+  DEBUG_ONLY(void verify(int expected_byte_size, bool read_only) const);\n+\n","filename":"src\/hotspot\/share\/cds\/dumpAllocStats.hpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -58,0 +58,1 @@\n+#include \"oops\/compressedKlass.hpp\"\n@@ -207,0 +208,1 @@\n+  _compact_headers = UseCompactObjectHeaders;\n@@ -215,0 +217,8 @@\n+  if (UseCompressedClassPointers) {\n+#ifdef _LP64\n+    _narrow_klass_pointer_bits = CompressedKlassPointers::narrow_klass_pointer_bits();\n+    _narrow_klass_shift = ArchiveBuilder::precomputed_narrow_klass_shift();\n+#endif\n+  } else {\n+    _narrow_klass_pointer_bits = _narrow_klass_shift = -1;\n+  }\n@@ -273,0 +283,1 @@\n+  st->print_cr(\"- compact_headers:                %d\", _compact_headers);\n@@ -278,0 +289,2 @@\n+  st->print_cr(\"- narrow_klass_pointer_bits:      %d\", _narrow_klass_pointer_bits);\n+  st->print_cr(\"- narrow_klass_shift:             %d\", _narrow_klass_shift);\n@@ -2088,1 +2101,1 @@\n-  \/\/ ArchiveHeapWriter::precomputed_narrow_klass_shift. We enforce this encoding at runtime (see\n+  \/\/ ArchiveBuilder::precomputed_narrow_klass_shift. We enforce this encoding at runtime (see\n@@ -2092,1 +2105,2 @@\n-  const int archive_narrow_klass_shift = ArchiveHeapWriter::precomputed_narrow_klass_shift;\n+  const int archive_narrow_klass_pointer_bits = header()->narrow_klass_pointer_bits();\n+  const int archive_narrow_klass_shift = header()->narrow_klass_shift();\n@@ -2096,2 +2110,2 @@\n-  log_info(cds)(\"    narrow_klass_base at mapping start address, narrow_klass_shift = %d\",\n-                archive_narrow_klass_shift);\n+  log_info(cds)(\"    narrow_klass_base at mapping start address, narrow_klass_pointer_bits = %d, narrow_klass_shift = %d\",\n+                archive_narrow_klass_pointer_bits, archive_narrow_klass_shift);\n@@ -2102,2 +2116,2 @@\n-  log_info(cds)(\"    narrow_klass_base = \" PTR_FORMAT \", narrow_klass_shift = %d\",\n-                p2i(CompressedKlassPointers::base()), CompressedKlassPointers::shift());\n+  log_info(cds)(\"    narrow_klass_base = \" PTR_FORMAT \", arrow_klass_pointer_bits = %d, narrow_klass_shift = %d\",\n+                p2i(CompressedKlassPointers::base()), CompressedKlassPointers::narrow_klass_pointer_bits(), CompressedKlassPointers::shift());\n@@ -2112,4 +2126,29 @@\n-  assert(archive_narrow_klass_base == CompressedKlassPointers::base(), \"Unexpected encoding base encountered \"\n-         \"(\" PTR_FORMAT \", expected \" PTR_FORMAT \")\", p2i(CompressedKlassPointers::base()), p2i(archive_narrow_klass_base));\n-  assert(archive_narrow_klass_shift == CompressedKlassPointers::shift(), \"Unexpected encoding shift encountered \"\n-         \"(%d, expected %d)\", CompressedKlassPointers::shift(), archive_narrow_klass_shift);\n+  int err = 0;\n+  if ( archive_narrow_klass_base != CompressedKlassPointers::base() ||\n+       (err = 1, archive_narrow_klass_pointer_bits != CompressedKlassPointers::narrow_klass_pointer_bits()) ||\n+       (err = 2, archive_narrow_klass_shift != CompressedKlassPointers::shift()) ) {\n+    stringStream ss;\n+    switch (err) {\n+    case 0:\n+      ss.print(\"Unexpected encoding base encountered (\" PTR_FORMAT \", expected \" PTR_FORMAT \")\",\n+               p2i(CompressedKlassPointers::base()), p2i(archive_narrow_klass_base));\n+      break;\n+    case 1:\n+      ss.print(\"Unexpected narrow Klass bit length encountered (%d, expected %d)\",\n+               CompressedKlassPointers::narrow_klass_pointer_bits(), archive_narrow_klass_pointer_bits);\n+      break;\n+    case 2:\n+      ss.print(\"Unexpected narrow Klass shift encountered (%d, expected %d)\",\n+               CompressedKlassPointers::shift(), archive_narrow_klass_shift);\n+      break;\n+    default:\n+      ShouldNotReachHere();\n+    };\n+    LogTarget(Info, cds) lt;\n+    if (lt.is_enabled()) {\n+      LogStream ls(lt);\n+      ls.print_raw(ss.base());\n+      header()->print(&ls);\n+    }\n+    assert(false, \"%s\", ss.base());\n+  }\n@@ -2487,2 +2526,2 @@\n-  log_info(cds)(\"Archive was created with UseCompressedOops = %d, UseCompressedClassPointers = %d\",\n-                          compressed_oops(), compressed_class_pointers());\n+  log_info(cds)(\"Archive was created with UseCompressedOops = %d, UseCompressedClassPointers = %d, UseCompactObjectHeaders = %d\",\n+                          compressed_oops(), compressed_class_pointers(), compact_headers());\n@@ -2490,1 +2529,1 @@\n-    log_info(cds)(\"Unable to use shared archive.\\nThe saved state of UseCompressedOops and UseCompressedClassPointers is \"\n+    log_warning(cds)(\"Unable to use shared archive.\\nThe saved state of UseCompressedOops and UseCompressedClassPointers is \"\n@@ -2500,0 +2539,8 @@\n+  if (compact_headers() != UseCompactObjectHeaders) {\n+    log_warning(cds)(\"Unable to use shared archive.\\nThe shared archive file's UseCompactObjectHeaders setting (%s)\"\n+                     \" does not equal the current UseCompactObjectHeaders setting (%s).\",\n+                     _compact_headers          ? \"enabled\" : \"disabled\",\n+                     UseCompactObjectHeaders   ? \"enabled\" : \"disabled\");\n+    return false;\n+  }\n+\n","filename":"src\/hotspot\/share\/cds\/filemap.cpp","additions":60,"deletions":13,"binary":false,"changes":73,"status":"modified"},{"patch":"@@ -191,0 +191,1 @@\n+  bool   _compact_headers;                        \/\/ value of UseCompactObjectHeaders\n@@ -196,0 +197,2 @@\n+  int     _narrow_klass_pointer_bits;             \/\/ save number of bits in narrowKlass\n+  int     _narrow_klass_shift;                    \/\/ save shift width used to pre-compute narrowKlass IDs in archived heap objects\n@@ -262,0 +265,1 @@\n+  bool compact_headers()                   const { return _compact_headers; }\n@@ -273,0 +277,2 @@\n+  int narrow_klass_pointer_bits()          const { return _narrow_klass_pointer_bits; }\n+  int narrow_klass_shift()                 const { return _narrow_klass_shift; }\n","filename":"src\/hotspot\/share\/cds\/filemap.hpp","additions":6,"deletions":0,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -90,0 +90,1 @@\n+#include \"utilities\/macros.hpp\"\n@@ -1200,11 +1201,12 @@\n-#if INCLUDE_CDS_JAVA_HEAP\n-          \/\/ We archived objects with pre-computed narrow Klass id. Set up encoding such that these Ids stay valid.\n-          address precomputed_narrow_klass_base = cds_base;\n-          const int precomputed_narrow_klass_shift = ArchiveHeapWriter::precomputed_narrow_klass_shift;\n-          CompressedKlassPointers::initialize_for_given_encoding(\n-            cds_base, ccs_end - cds_base, \/\/ Klass range\n-            precomputed_narrow_klass_base, precomputed_narrow_klass_shift \/\/ precomputed encoding, see ArchiveHeapWriter\n-            );\n-#else\n-          CompressedKlassPointers::initialize (\n-            cds_base, ccs_end - cds_base \/\/ Klass range\n+          if (INCLUDE_CDS_JAVA_HEAP || UseCompactObjectHeaders) {\n+            \/\/ The CDS archive may contain narrow Klass IDs that were precomputed at archive generation time:\n+            \/\/ - every archived java object header (only if INCLUDE_CDS_JAVA_HEAP)\n+            \/\/ - every archived Klass' prototype   (only if +UseCompactObjectHeaders)\n+            \/\/\n+            \/\/ In order for those IDs to still be valid, we need to dictate base and shift: base should be the\n+            \/\/ mapping start, shift the shift used at archive generation time.\n+            address precomputed_narrow_klass_base = cds_base;\n+            const int precomputed_narrow_klass_shift = ArchiveBuilder::precomputed_narrow_klass_shift();\n+            CompressedKlassPointers::initialize_for_given_encoding(\n+              cds_base, ccs_end - cds_base, \/\/ Klass range\n+              precomputed_narrow_klass_base, precomputed_narrow_klass_shift \/\/ precomputed encoding, see ArchiveBuilder\n@@ -1212,1 +1214,6 @@\n-#endif \/\/ INCLUDE_CDS_JAVA_HEAP\n+          } else {\n+            \/\/ Let JVM freely chose encoding base and shift\n+            CompressedKlassPointers::initialize (\n+              cds_base, ccs_end - cds_base \/\/ Klass range\n+              );\n+          }\n@@ -1267,1 +1274,1 @@\n-\/\/  encoding, the range [Base, End) not surpass KlassEncodingMetaspaceMax.\n+\/\/  encoding, the range [Base, End) and not surpass the max. range for that encoding.\n@@ -1388,1 +1395,1 @@\n-      \/\/ encoding base since the archived heap objects contain nKlass IDs pre-calculated toward the start\n+      \/\/ encoding base since the archived heap objects contain narrow Klass IDs pre-calculated toward the start\n","filename":"src\/hotspot\/share\/cds\/metaspaceShared.cpp","additions":21,"deletions":14,"binary":false,"changes":35,"status":"modified"},{"patch":"@@ -261,0 +261,20 @@\n+\n+\/\/ ------------------------------------------------------------------\n+\/\/ ciKlass::prototype_header_offset\n+juint ciKlass::prototype_header_offset() {\n+  assert(is_loaded(), \"must be loaded\");\n+\n+  VM_ENTRY_MARK;\n+  Klass* this_klass = get_Klass();\n+  return in_bytes(this_klass->prototype_header_offset());\n+}\n+\n+\/\/ ------------------------------------------------------------------\n+\/\/ ciKlass::prototype_header\n+uintptr_t ciKlass::prototype_header() {\n+  assert(is_loaded(), \"must be loaded\");\n+\n+  VM_ENTRY_MARK;\n+  Klass* this_klass = get_Klass();\n+  return (uintptr_t)this_klass->prototype_header().to_pointer();\n+}\n","filename":"src\/hotspot\/share\/ci\/ciKlass.cpp","additions":20,"deletions":0,"binary":false,"changes":20,"status":"modified"},{"patch":"@@ -142,0 +142,3 @@\n+\n+  juint prototype_header_offset();\n+  uintptr_t prototype_header();\n","filename":"src\/hotspot\/share\/ci\/ciKlass.hpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -5868,0 +5868,9 @@\n+\/\/ Returns true if the future Klass will need to be addressable with a narrow Klass ID.\n+bool ClassFileParser::klass_needs_narrow_id() const {\n+  \/\/ Classes that are never instantiated need no narrow Klass Id, since the\n+  \/\/ only point of having a narrow id is to put it into an object header. Keeping\n+  \/\/ never instantiated classes out of class space lessens the class space pressure.\n+  \/\/ For more details, see JDK-8338526.\n+  return !is_interface() && !is_abstract();\n+}\n+\n","filename":"src\/hotspot\/share\/classfile\/classFileParser.cpp","additions":9,"deletions":0,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -552,0 +552,4 @@\n+  \/\/ Returns true if the Klass to be generated will need to be addressable\n+  \/\/ with a narrow Klass ID.\n+  bool klass_needs_narrow_id() const;\n+\n","filename":"src\/hotspot\/share\/classfile\/classFileParser.hpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -62,0 +62,1 @@\n+#include \"oops\/compressedKlass.inline.hpp\"\n@@ -86,0 +87,10 @@\n+#ifdef ASSERT\n+static void check_klass_after_loading(const Klass* k) {\n+#ifdef _LP64\n+  if (k != nullptr && UseCompressedClassPointers && k->needs_narrow_id()) {\n+    CompressedKlassPointers::check_encodable(k);\n+  }\n+#endif\n+}\n+#endif\n+\n@@ -431,0 +442,3 @@\n+\n+  DEBUG_ONLY(check_klass_after_loading(k);)\n+\n@@ -1340,1 +1354,1 @@\n-    assert(check_alignment(record->_klass), \"Address not aligned\");\n+    DEBUG_ONLY(check_klass_after_loading(record->_klass);)\n","filename":"src\/hotspot\/share\/classfile\/systemDictionaryShared.cpp","additions":15,"deletions":1,"binary":false,"changes":16,"status":"modified"},{"patch":"@@ -37,0 +37,1 @@\n+#include \"gc\/shared\/fullGCForwarding.hpp\"\n@@ -250,0 +251,1 @@\n+  FullGCForwarding::initialize_flags(heap_reserved_size_bytes());\n","filename":"src\/hotspot\/share\/gc\/g1\/g1Arguments.cpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -80,0 +80,1 @@\n+#include \"gc\/shared\/fullGCForwarding.hpp\"\n@@ -88,1 +89,0 @@\n-#include \"gc\/shared\/preservedMarks.inline.hpp\"\n@@ -1438,0 +1438,2 @@\n+  FullGCForwarding::initialize(heap_rs.region());\n+\n","filename":"src\/hotspot\/share\/gc\/g1\/g1CollectedHeap.cpp","additions":3,"deletions":1,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -32,0 +32,1 @@\n+#include \"gc\/shared\/fullGCForwarding.inline.hpp\"\n@@ -44,1 +45,1 @@\n-  if (obj->is_forwarded()) {\n+  if (FullGCForwarding::is_forwarded(obj)) {\n@@ -55,2 +56,2 @@\n-  assert(obj->is_forwarded(), \"Sanity!\");\n-  assert(obj->forwardee() != obj, \"Object must have a new location\");\n+  assert(FullGCForwarding::is_forwarded(obj), \"Sanity!\");\n+  assert(FullGCForwarding::forwardee(obj) != obj, \"Object must have a new location\");\n@@ -61,1 +62,1 @@\n-  HeapWord* destination = cast_from_oop<HeapWord*>(obj->forwardee());\n+  HeapWord* destination = cast_from_oop<HeapWord*>(FullGCForwarding::forwardee(obj));\n@@ -124,1 +125,1 @@\n-  HeapWord* destination = cast_from_oop<HeapWord*>(obj->forwardee());\n+  HeapWord* destination = cast_from_oop<HeapWord*>(FullGCForwarding::forwardee(obj));\n","filename":"src\/hotspot\/share\/gc\/g1\/g1FullGCCompactTask.cpp","additions":6,"deletions":5,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -29,0 +29,1 @@\n+#include \"gc\/shared\/fullGCForwarding.inline.hpp\"\n@@ -109,2 +110,2 @@\n-    object->forward_to(cast_to_oop(_compaction_top));\n-    assert(object->is_forwarded(), \"must be forwarded\");\n+    FullGCForwarding::forward_to(object, cast_to_oop(_compaction_top));\n+    assert(FullGCForwarding::is_forwarded(object), \"must be forwarded\");\n@@ -112,1 +113,1 @@\n-    assert(!object->is_forwarded(), \"must not be forwarded\");\n+    assert(!FullGCForwarding::is_forwarded(object), \"must not be forwarded\");\n@@ -175,2 +176,2 @@\n-  obj->forward_to(cast_to_oop(dest_hr->bottom()));\n-  assert(obj->is_forwarded(), \"Object must be forwarded!\");\n+  FullGCForwarding::forward_to(obj, cast_to_oop(dest_hr->bottom()));\n+  assert(FullGCForwarding::is_forwarded(obj), \"Object must be forwarded!\");\n","filename":"src\/hotspot\/share\/gc\/g1\/g1FullGCCompactionPoint.cpp","additions":6,"deletions":5,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -35,0 +35,1 @@\n+#include \"gc\/shared\/fullGCForwarding.inline.hpp\"\n@@ -68,2 +69,2 @@\n-  if (obj->is_forwarded()) {\n-    oop forwardee = obj->forwardee();\n+  if (FullGCForwarding::is_forwarded(obj)) {\n+    oop forwardee = FullGCForwarding::forwardee(obj);\n","filename":"src\/hotspot\/share\/gc\/g1\/g1FullGCOopClosures.inline.hpp","additions":3,"deletions":2,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -35,0 +35,1 @@\n+#include \"gc\/shared\/fullGCForwarding.inline.hpp\"\n@@ -117,1 +118,1 @@\n-  if (obj->is_forwarded()) {\n+  if (FullGCForwarding::is_forwarded(obj)) {\n@@ -120,1 +121,1 @@\n-    if (cast_from_oop<HeapWord*>(obj->forwardee()) < _dense_prefix_top) {\n+    if (cast_from_oop<HeapWord*>(FullGCForwarding::forwardee(obj)) < _dense_prefix_top) {\n","filename":"src\/hotspot\/share\/gc\/g1\/g1FullGCPrepareTask.inline.hpp","additions":3,"deletions":2,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -108,1 +108,0 @@\n-  _gc_par_phases[RestorePreservedMarks] = new WorkerDataArray<double>(\"RestorePreservedMarks\", \"Restore Preserved Marks (ms):\", max_gc_threads);\n@@ -515,1 +514,0 @@\n-    debug_phase(_gc_par_phases[RestorePreservedMarks], 1);\n","filename":"src\/hotspot\/share\/gc\/g1\/g1GCPhaseTimes.cpp","additions":0,"deletions":2,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -90,1 +90,0 @@\n-    RestorePreservedMarks,\n","filename":"src\/hotspot\/share\/gc\/g1\/g1GCPhaseTimes.hpp","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -231,1 +231,1 @@\n-      forwardee = m.forwardee();\n+      forwardee = obj->forwardee(m);\n","filename":"src\/hotspot\/share\/gc\/g1\/g1OopClosures.inline.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -40,1 +40,0 @@\n-#include \"gc\/shared\/preservedMarks.inline.hpp\"\n@@ -62,1 +61,0 @@\n-                                           PreservedMarks* preserved_marks,\n@@ -93,1 +91,0 @@\n-    _preserved_marks(preserved_marks),\n@@ -219,1 +216,1 @@\n-    obj = m.forwardee();\n+    obj = obj->forwardee(m);\n@@ -235,1 +232,0 @@\n-  assert(from_obj->is_objArray(), \"must be obj array\");\n@@ -268,1 +264,0 @@\n-  assert(from_obj->is_objArray(), \"precondition\");\n@@ -404,1 +399,1 @@\n-                                                  oop const old, size_t word_sz, uint age,\n+                                                  Klass* klass, size_t word_sz, uint age,\n@@ -408,1 +403,1 @@\n-    _g1h->gc_tracer_stw()->report_promotion_in_new_plab_event(old->klass(), word_sz * HeapWordSize, age,\n+    _g1h->gc_tracer_stw()->report_promotion_in_new_plab_event(klass, word_sz * HeapWordSize, age,\n@@ -412,1 +407,1 @@\n-    _g1h->gc_tracer_stw()->report_promotion_outside_plab_event(old->klass(), word_sz * HeapWordSize, age,\n+    _g1h->gc_tracer_stw()->report_promotion_outside_plab_event(klass, word_sz * HeapWordSize, age,\n@@ -419,1 +414,1 @@\n-                                                   oop old,\n+                                                   Klass* klass,\n@@ -442,1 +437,1 @@\n-      report_promotion_event(*dest_attr, old, word_sz, age, obj_ptr, node_index);\n+      report_promotion_event(*dest_attr, klass, word_sz, age, obj_ptr, node_index);\n@@ -477,3 +472,11 @@\n-  \/\/ Get the klass once.  We'll need it again later, and this avoids\n-  \/\/ re-decoding when it's compressed.\n-  Klass* klass = old->klass();\n+  \/\/ NOTE: With compact headers, it is not safe to load the Klass* from old, because\n+  \/\/ that would access the mark-word, that might change at any time by concurrent\n+  \/\/ workers.\n+  \/\/ This mark word would refer to a forwardee, which may not yet have completed\n+  \/\/ copying. Therefore we must load the Klass* from the mark-word that we already\n+  \/\/ loaded. This is safe, because we only enter here if not yet forwarded.\n+  assert(!old_mark.is_forwarded(), \"precondition\");\n+  Klass* klass = UseCompactObjectHeaders\n+      ? old_mark.klass()\n+      : old->klass();\n+\n@@ -497,1 +500,1 @@\n-    obj_ptr = allocate_copy_slow(&dest_attr, old, word_sz, age, node_index);\n+    obj_ptr = allocate_copy_slow(&dest_attr, klass, word_sz, age, node_index);\n@@ -598,1 +601,0 @@\n-                               _preserved_marks_set.get(worker_id),\n@@ -658,1 +660,1 @@\n-  oop forward_ptr = old->forward_to_atomic(old, m, memory_order_relaxed);\n+  oop forward_ptr = old->forward_to_self_atomic(m, memory_order_relaxed);\n@@ -671,2 +673,0 @@\n-    _preserved_marks->push_if_necessary(old, m);\n-\n@@ -730,1 +730,0 @@\n-    _preserved_marks_set(true \/* in_c_heap *\/),\n@@ -739,1 +738,0 @@\n-  _preserved_marks_set.init(num_workers);\n@@ -752,1 +750,0 @@\n-  _preserved_marks_set.reclaim();\n","filename":"src\/hotspot\/share\/gc\/g1\/g1ParScanThreadState.cpp","additions":19,"deletions":22,"binary":false,"changes":41,"status":"modified"},{"patch":"@@ -37,1 +37,0 @@\n-#include \"gc\/shared\/preservedMarks.hpp\"\n@@ -51,2 +50,0 @@\n-class PreservedMarks;\n-class PreservedMarksSet;\n@@ -109,1 +106,0 @@\n-  PreservedMarks* _preserved_marks;\n@@ -128,1 +124,0 @@\n-                       PreservedMarks* preserved_marks,\n@@ -177,1 +172,1 @@\n-                               oop old,\n+                               Klass* klass,\n@@ -212,1 +207,1 @@\n-                              oop const old, size_t word_sz, uint age,\n+                              Klass* klass, size_t word_sz, uint age,\n@@ -249,1 +244,0 @@\n-  PreservedMarksSet _preserved_marks_set;\n@@ -267,1 +261,0 @@\n-  PreservedMarksSet* preserved_marks_set() { return &_preserved_marks_set; }\n","filename":"src\/hotspot\/share\/gc\/g1\/g1ParScanThreadState.hpp","additions":2,"deletions":9,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -56,1 +56,0 @@\n-#include \"gc\/shared\/preservedMarks.hpp\"\n","filename":"src\/hotspot\/share\/gc\/g1\/g1YoungCollector.cpp","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -45,1 +45,0 @@\n-#include \"gc\/shared\/preservedMarks.inline.hpp\"\n@@ -254,2 +253,2 @@\n-        assert(obj->is_forwarded() && obj->forwardee() == obj, \"must be self-forwarded\");\n-        obj->init_mark();\n+        assert(obj->is_self_forwarded(), \"must be self-forwarded\");\n+        obj->unset_self_forwarded();\n@@ -480,21 +479,0 @@\n-class G1PostEvacuateCollectionSetCleanupTask2::RestorePreservedMarksTask : public G1AbstractSubTask {\n-  PreservedMarksSet* _preserved_marks;\n-  WorkerTask* _task;\n-\n-public:\n-  RestorePreservedMarksTask(PreservedMarksSet* preserved_marks) :\n-    G1AbstractSubTask(G1GCPhaseTimes::RestorePreservedMarks),\n-    _preserved_marks(preserved_marks),\n-    _task(preserved_marks->create_task()) { }\n-\n-  virtual ~RestorePreservedMarksTask() {\n-    delete _task;\n-  }\n-\n-  double worker_cost() const override {\n-    return _preserved_marks->num();\n-  }\n-\n-  void do_work(uint worker_id) override { _task->work(worker_id); }\n-};\n-\n@@ -982,1 +960,0 @@\n-    add_parallel_task(new RestorePreservedMarksTask(per_thread_states->preserved_marks_set()));\n","filename":"src\/hotspot\/share\/gc\/g1\/g1YoungGCPostEvacuateTasks.cpp","additions":2,"deletions":25,"binary":false,"changes":27,"status":"modified"},{"patch":"@@ -59,1 +59,0 @@\n-\/\/ - Restore Preserved Marks (on evacuation failure)\n@@ -70,1 +69,0 @@\n-  class RestorePreservedMarksTask;\n","filename":"src\/hotspot\/share\/gc\/g1\/g1YoungGCPostEvacuateTasks.hpp","additions":0,"deletions":2,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -220,1 +220,7 @@\n-    if (!obj->is_forwarded()) {\n+    if (obj->is_forwarded()) {\n+      assert(!obj->is_self_forwarded(), \"must not be self-forwarded\");\n+      \/\/ It is safe to use the forwardee here. Parallel GC only uses\n+      \/\/ header-based forwarding during promotion. Full GC doesn't\n+      \/\/ use the object header for forwarding at all.\n+      p += obj->forwardee()->size();\n+    } else {\n@@ -222,0 +228,1 @@\n+      p += obj->size();\n@@ -223,6 +230,0 @@\n-#ifdef ASSERT\n-    else {\n-      assert(obj->forwardee() != obj, \"must not be self-forwarded\");\n-    }\n-#endif\n-    p += obj->size();\n","filename":"src\/hotspot\/share\/gc\/parallel\/mutableSpace.cpp","additions":8,"deletions":7,"binary":false,"changes":15,"status":"modified"},{"patch":"@@ -30,0 +30,1 @@\n+#include \"gc\/shared\/fullGCForwarding.hpp\"\n@@ -130,0 +131,1 @@\n+  FullGCForwarding::initialize_flags(heap_reserved_size_bytes());\n","filename":"src\/hotspot\/share\/gc\/parallel\/parallelArguments.cpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -36,0 +36,1 @@\n+#include \"gc\/shared\/fullGCForwarding.inline.hpp\"\n@@ -132,0 +133,2 @@\n+  FullGCForwarding::initialize(heap_rs.region());\n+\n","filename":"src\/hotspot\/share\/gc\/parallel\/parallelScavengeHeap.cpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -47,0 +47,1 @@\n+#include \"gc\/shared\/fullGCForwarding.inline.hpp\"\n@@ -775,0 +776,2 @@\n+  \/\/ With +UseCompactObjectHeaders, the minimum filler size is only one word,\n+  \/\/ because the Klass* gets encoded in the mark-word.\n@@ -782,8 +785,1 @@\n-  \/\/ Note: If min-fill-size decreases to 1, this whole method becomes redundant.\n-  assert(CollectedHeap::min_fill_size() >= 2, \"inv\");\n-#ifndef _LP64\n-  \/\/ In 32-bit system, each heap word is 4 bytes, so MinObjAlignment == 2.\n-  \/\/ The gap is always equal to min-fill-size, so nothing to do.\n-  return;\n-#endif\n-  if (MinObjAlignment > 1) {\n+  if (MinObjAlignment >= checked_cast<int>(CollectedHeap::min_fill_size())) {\n@@ -792,0 +788,2 @@\n+\n+  assert(!UseCompactObjectHeaders, \"Compact headers can allocate small objects\");\n@@ -1595,1 +1593,1 @@\n-              obj->forward_to(cast_to_oop(new_addr));\n+              FullGCForwarding::forward_to(obj, cast_to_oop(new_addr));\n@@ -1638,1 +1636,1 @@\n-        assert(obj->forwardee() == cast_to_oop(bump_ptr), \"inv\");\n+        assert(FullGCForwarding::forwardee(obj) == cast_to_oop(bump_ptr), \"inv\");\n@@ -2401,2 +2399,2 @@\n-    assert(cast_to_oop(source())->is_forwarded(), \"inv\");\n-    assert(cast_to_oop(source())->forwardee() == cast_to_oop(destination()), \"inv\");\n+    assert(FullGCForwarding::is_forwarded(cast_to_oop(source())), \"inv\");\n+    assert(FullGCForwarding::forwardee(cast_to_oop(source())) == cast_to_oop(destination()), \"inv\");\n","filename":"src\/hotspot\/share\/gc\/parallel\/psParallelCompact.cpp","additions":10,"deletions":12,"binary":false,"changes":22,"status":"modified"},{"patch":"@@ -34,0 +34,1 @@\n+#include \"gc\/shared\/fullGCForwarding.inline.hpp\"\n@@ -82,1 +83,1 @@\n-    oop new_obj = obj->forwardee();\n+    oop new_obj = FullGCForwarding::forwardee(obj);\n","filename":"src\/hotspot\/share\/gc\/parallel\/psParallelCompact.inline.hpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -324,1 +324,0 @@\n-  assert(old_obj->is_objArray(), \"precondition\");\n@@ -360,1 +359,1 @@\n-  if (obj->forward_to_atomic(obj, obj_mark) == nullptr) {\n+  if (obj->forward_to_self_atomic(obj_mark) == nullptr) {\n","filename":"src\/hotspot\/share\/gc\/parallel\/psPromotionManager.cpp","additions":1,"deletions":2,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -114,1 +114,1 @@\n-  inline void promotion_trace_event(oop new_obj, oop old_obj, size_t obj_size,\n+  inline void promotion_trace_event(oop new_obj, Klass* klass, size_t obj_size,\n","filename":"src\/hotspot\/share\/gc\/parallel\/psPromotionManager.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -69,1 +69,1 @@\n-inline void PSPromotionManager::promotion_trace_event(oop new_obj, oop old_obj,\n+inline void PSPromotionManager::promotion_trace_event(oop new_obj, Klass* klass,\n@@ -82,1 +82,1 @@\n-        gc_tracer->report_promotion_in_new_plab_event(old_obj->klass(), obj_bytes,\n+        gc_tracer->report_promotion_in_new_plab_event(klass, obj_bytes,\n@@ -89,1 +89,1 @@\n-        gc_tracer->report_promotion_outside_plab_event(old_obj->klass(), obj_bytes,\n+        gc_tracer->report_promotion_outside_plab_event(klass, obj_bytes,\n@@ -152,1 +152,1 @@\n-    return m.forwardee();\n+    return o->forwardee(m);\n@@ -168,1 +168,13 @@\n-  size_t new_obj_size = o->size();\n+\n+  \/\/ NOTE: With compact headers, it is not safe to load the Klass* from old, because\n+  \/\/ that would access the mark-word, that might change at any time by concurrent\n+  \/\/ workers.\n+  \/\/ This mark word would refer to a forwardee, which may not yet have completed\n+  \/\/ copying. Therefore we must load the Klass* from the mark-word that we already\n+  \/\/ loaded. This is safe, because we only enter here if not yet forwarded.\n+  assert(!test_mark.is_forwarded(), \"precondition\");\n+  Klass* klass = UseCompactObjectHeaders\n+      ? test_mark.klass()\n+      : o->klass();\n+\n+  size_t new_obj_size = o->size_given_klass(klass);\n@@ -183,1 +195,1 @@\n-          promotion_trace_event(new_obj, o, new_obj_size, age, false, nullptr);\n+          promotion_trace_event(new_obj, klass, new_obj_size, age, false, nullptr);\n@@ -193,1 +205,1 @@\n-            promotion_trace_event(new_obj, o, new_obj_size, age, false, &_young_lab);\n+            promotion_trace_event(new_obj, klass, new_obj_size, age, false, &_young_lab);\n@@ -219,1 +231,1 @@\n-          promotion_trace_event(new_obj, o, new_obj_size, age, true, nullptr);\n+          promotion_trace_event(new_obj, klass, new_obj_size, age, true, nullptr);\n@@ -229,1 +241,1 @@\n-            promotion_trace_event(new_obj, o, new_obj_size, age, true, &_old_lab);\n+            promotion_trace_event(new_obj, klass, new_obj_size, age, true, &_old_lab);\n","filename":"src\/hotspot\/share\/gc\/parallel\/psPromotionManager.inline.hpp","additions":21,"deletions":9,"binary":false,"changes":30,"status":"modified"},{"patch":"@@ -42,1 +42,0 @@\n-#include \"gc\/shared\/preservedMarks.inline.hpp\"\n@@ -230,1 +229,0 @@\n-    _preserved_marks_set(false \/* in_c_heap *\/),\n@@ -612,2 +610,0 @@\n-  \/\/ The preserved marks should be empty at the start of the GC.\n-  _preserved_marks_set.init(1);\n@@ -684,2 +680,0 @@\n-  \/\/ We should have processed and cleared all the preserved marks.\n-  _preserved_marks_set.reclaim();\n@@ -709,2 +703,6 @@\n-      if (obj->is_forwarded()) {\n-        obj->init_mark();\n+      if (obj->is_self_forwarded()) {\n+        obj->unset_self_forwarded();\n+      } else if (obj->is_forwarded()) {\n+        \/\/ To restore the klass-bits in the header.\n+        \/\/ Needed for object iteration to work properly.\n+        obj->set_mark(obj->forwardee()->prototype_mark());\n@@ -716,6 +714,0 @@\n-\n-  restore_preserved_marks();\n-}\n-\n-void DefNewGeneration::restore_preserved_marks() {\n-  _preserved_marks_set.restore(nullptr);\n@@ -729,1 +721,0 @@\n-  _preserved_marks_set.get()->push_if_necessary(old, old->mark());\n@@ -734,1 +725,1 @@\n-  old->forward_to(old);\n+  old->forward_to_self();\n","filename":"src\/hotspot\/share\/gc\/serial\/defNewGeneration.cpp","additions":7,"deletions":16,"binary":false,"changes":23,"status":"modified"},{"patch":"@@ -35,1 +35,0 @@\n-#include \"gc\/shared\/preservedMarks.hpp\"\n@@ -102,5 +101,0 @@\n-  virtual void restore_preserved_marks();\n-\n-  \/\/ Preserved marks\n-  PreservedMarksSet _preserved_marks_set;\n-\n","filename":"src\/hotspot\/share\/gc\/serial\/defNewGeneration.hpp","additions":0,"deletions":6,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -26,0 +26,1 @@\n+#include \"gc\/shared\/fullGCForwarding.hpp\"\n@@ -30,0 +31,5 @@\n+void SerialArguments::initialize_heap_flags_and_sizes() {\n+  GenArguments::initialize_heap_flags_and_sizes();\n+  FullGCForwarding::initialize_flags(MaxHeapSize);\n+}\n+\n","filename":"src\/hotspot\/share\/gc\/serial\/serialArguments.cpp","additions":6,"deletions":0,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -35,0 +35,1 @@\n+  virtual void initialize_heap_flags_and_sizes();\n","filename":"src\/hotspot\/share\/gc\/serial\/serialArguments.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -46,0 +46,1 @@\n+#include \"gc\/shared\/fullGCForwarding.inline.hpp\"\n@@ -233,1 +234,1 @@\n-      obj->forward_to(cast_to_oop(new_addr));\n+      FullGCForwarding::forward_to(obj, cast_to_oop(new_addr));\n@@ -258,1 +259,1 @@\n-    oop new_obj = obj->forwardee();\n+    oop new_obj = FullGCForwarding::forwardee(obj);\n@@ -355,1 +356,1 @@\n-      if (!cast_to_oop(cur_addr)->is_forwarded()) {\n+      if (!FullGCForwarding::is_forwarded(cast_to_oop(cur_addr))) {\n@@ -361,1 +362,1 @@\n-        if (!cast_to_oop(cur_addr)->is_forwarded()) {\n+        if (!FullGCForwarding::is_forwarded(cast_to_oop(cur_addr))) {\n@@ -596,1 +597,1 @@\n-  obj->set_mark(markWord::prototype().set_marked());\n+  obj->set_mark(obj->prototype_mark().set_marked());\n@@ -627,2 +628,2 @@\n-    if (obj->is_forwarded()) {\n-      oop new_obj = obj->forwardee();\n+    if (FullGCForwarding::is_forwarded(obj)) {\n+      oop new_obj = FullGCForwarding::forwardee(obj);\n","filename":"src\/hotspot\/share\/gc\/serial\/serialFullGC.cpp","additions":8,"deletions":7,"binary":false,"changes":15,"status":"modified"},{"patch":"@@ -43,0 +43,1 @@\n+#include \"gc\/shared\/fullGCForwarding.hpp\"\n@@ -203,0 +204,2 @@\n+  FullGCForwarding::initialize(_reserved);\n+\n","filename":"src\/hotspot\/share\/gc\/serial\/serialHeap.cpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -713,1 +713,1 @@\n-  \/\/ 8  - 32-bit VM\n+  \/\/ 8  - 32-bit VM or 64-bit VM, compact headers\n@@ -718,0 +718,1 @@\n+    assert(!UseCompactObjectHeaders, \"\");\n","filename":"src\/hotspot\/share\/gc\/shared\/c2\/barrierSetC2.cpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -223,0 +223,20 @@\n+static bool klass_is_sane(oop object) {\n+  if (UseCompactObjectHeaders) {\n+    \/\/ With compact headers, we can't safely access the Klass* when\n+    \/\/ the object has been forwarded, because non-full-GC-forwarding\n+    \/\/ temporarily overwrites the mark-word, and thus the Klass*, with\n+    \/\/ the forwarding pointer, and here we have no way to make a\n+    \/\/ distinction between Full-GC and regular GC forwarding.\n+    markWord mark = object->mark();\n+    if (mark.is_forwarded()) {\n+      \/\/ We can't access the Klass*. We optimistically assume that\n+      \/\/ it is ok. This happens very rarely.\n+      return true;\n+    }\n+\n+    return Metaspace::contains(mark.klass_without_asserts());\n+  }\n+\n+  return Metaspace::contains(object->klass_without_asserts());\n+}\n+\n@@ -232,1 +252,1 @@\n-  if (!Metaspace::contains(object->klass_without_asserts())) {\n+  if (!klass_is_sane(object)) {\n","filename":"src\/hotspot\/share\/gc\/shared\/collectedHeap.cpp","additions":21,"deletions":1,"binary":false,"changes":22,"status":"modified"},{"patch":"@@ -309,1 +309,1 @@\n-  static constexpr size_t min_dummy_object_size() {\n+  static size_t min_dummy_object_size() {\n","filename":"src\/hotspot\/share\/gc\/shared\/collectedHeap.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -0,0 +1,57 @@\n+\/*\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"gc\/shared\/fullGCForwarding.hpp\"\n+#include \"memory\/memRegion.hpp\"\n+#include \"runtime\/globals_extension.hpp\"\n+\n+HeapWord* FullGCForwarding::_heap_base = nullptr;\n+int FullGCForwarding::_num_low_bits = 0;\n+\n+void FullGCForwarding::initialize_flags(size_t max_heap_size) {\n+#ifdef _LP64\n+  size_t max_narrow_heap_size = right_n_bits(NumLowBitsNarrow - Shift);\n+  if (UseCompactObjectHeaders && max_heap_size > max_narrow_heap_size * HeapWordSize) {\n+    warning(\"Compact object headers require a java heap size smaller than \" SIZE_FORMAT\n+            \"%s (given: \" SIZE_FORMAT \"%s). Disabling compact object headers.\",\n+            byte_size_in_proper_unit(max_narrow_heap_size * HeapWordSize),\n+            proper_unit_for_byte_size(max_narrow_heap_size * HeapWordSize),\n+            byte_size_in_proper_unit(max_heap_size),\n+            proper_unit_for_byte_size(max_heap_size));\n+    FLAG_SET_ERGO(UseCompactObjectHeaders, false);\n+  }\n+#endif\n+}\n+\n+void FullGCForwarding::initialize(MemRegion heap) {\n+#ifdef _LP64\n+  _heap_base = heap.start();\n+  if (UseCompactObjectHeaders) {\n+    _num_low_bits = NumLowBitsNarrow;\n+  } else {\n+    _num_low_bits = NumLowBitsWide;\n+  }\n+#endif\n+}\n","filename":"src\/hotspot\/share\/gc\/shared\/fullGCForwarding.cpp","additions":57,"deletions":0,"binary":false,"changes":57,"status":"added"},{"patch":"@@ -0,0 +1,59 @@\n+\/*\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef SHARE_GC_SHARED_FULLGCFORWARDING_HPP\n+#define SHARE_GC_SHARED_FULLGCFORWARDING_HPP\n+\n+#include \"memory\/allStatic.hpp\"\n+#include \"memory\/memRegion.hpp\"\n+#include \"oops\/markWord.hpp\"\n+#include \"oops\/oopsHierarchy.hpp\"\n+\n+\/*\n+ * Implements forwarding for the Full GCs of Serial, Parallel, G1 and Shenandoah in\n+ * a way that preserves upper N bits of object mark-words, which contain crucial\n+ * Klass* information when running with compact headers. The encoding is similar to\n+ * compressed-oops encoding: it basically subtracts the forwardee address from the\n+ * heap-base, shifts that difference into the right place, and sets the lowest two\n+ * bits (to indicate 'forwarded' state as usual).\n+ * With compact-headers, we have 40 bits to encode forwarding pointers. This is\n+ * enough to address 8TB of heap. If the heap size exceeds that limit, we turn off\n+ * compact headers.\n+ *\/\n+class FullGCForwarding : public AllStatic {\n+  static const int NumLowBitsNarrow = LP64_ONLY(markWord::klass_shift) NOT_LP64(0 \/*unused*\/);\n+  static const int NumLowBitsWide   = BitsPerWord;\n+  static const int Shift            = markWord::lock_bits + markWord::lock_shift;\n+\n+  static HeapWord* _heap_base;\n+  static int _num_low_bits;\n+public:\n+  static void initialize_flags(size_t max_heap_size);\n+  static void initialize(MemRegion heap);\n+  static inline void forward_to(oop from, oop to);\n+  static inline oop forwardee(oop from);\n+  static inline bool is_forwarded(oop obj);\n+};\n+\n+#endif \/\/ SHARE_GC_SHARED_FULLGCFORWARDING_HPP\n","filename":"src\/hotspot\/share\/gc\/shared\/fullGCForwarding.hpp","additions":59,"deletions":0,"binary":false,"changes":59,"status":"added"},{"patch":"@@ -0,0 +1,60 @@\n+\/*\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef GC_SHARED_FULLGCFORWARDING_INLINE_HPP\n+#define GC_SHARED_FULLGCFORWARDING_INLINE_HPP\n+\n+#include \"gc\/shared\/fullGCForwarding.hpp\"\n+\n+#include \"oops\/oop.inline.hpp\"\n+#include \"utilities\/globalDefinitions.hpp\"\n+\n+void FullGCForwarding::forward_to(oop from, oop to) {\n+#ifdef _LP64\n+  uintptr_t encoded = pointer_delta(cast_from_oop<HeapWord*>(to), _heap_base) << Shift;\n+  assert(encoded <= static_cast<uintptr_t>(right_n_bits(_num_low_bits)), \"encoded forwardee must fit\");\n+  uintptr_t mark = from->mark().value();\n+  mark &= ~right_n_bits(_num_low_bits);\n+  mark |= (encoded | markWord::marked_value);\n+  from->set_mark(markWord(mark));\n+#else\n+  from->forward_to(to);\n+#endif\n+}\n+\n+oop FullGCForwarding::forwardee(oop from) {\n+#ifdef _LP64\n+  uintptr_t mark = from->mark().value();\n+  HeapWord* decoded = _heap_base + ((mark & right_n_bits(_num_low_bits)) >> Shift);\n+  return cast_to_oop(decoded);\n+#else\n+  return from->forwardee();\n+#endif\n+}\n+\n+bool FullGCForwarding::is_forwarded(oop obj) {\n+  return obj->mark().is_forwarded();\n+}\n+\n+#endif \/\/ GC_SHARED_FULLGCFORWARDING_INLINE_HPP\n","filename":"src\/hotspot\/share\/gc\/shared\/fullGCForwarding.inline.hpp","additions":60,"deletions":0,"binary":false,"changes":60,"status":"added"},{"patch":"@@ -364,1 +364,3 @@\n-  oopDesc::set_klass_gap(mem, 0);\n+  if (oopDesc::has_klass_gap()) {\n+    oopDesc::set_klass_gap(mem, 0);\n+  }\n@@ -370,2 +372,0 @@\n-  \/\/ May be bootstrapping\n-  oopDesc::set_mark(mem, markWord::prototype());\n@@ -375,1 +375,6 @@\n-  oopDesc::release_set_klass(mem, _klass);\n+  if (UseCompactObjectHeaders) {\n+    oopDesc::release_set_mark(mem, _klass->prototype_header());\n+  } else {\n+    oopDesc::set_mark(mem, markWord::prototype());\n+    oopDesc::release_set_klass(mem, _klass);\n+  }\n","filename":"src\/hotspot\/share\/gc\/shared\/memAllocator.cpp","additions":9,"deletions":4,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -26,0 +26,1 @@\n+#include \"gc\/shared\/fullGCForwarding.inline.hpp\"\n@@ -45,2 +46,2 @@\n-  if (obj->is_forwarded()) {\n-    elem->set_oop(obj->forwardee());\n+  if (FullGCForwarding::is_forwarded(obj)) {\n+    elem->set_oop(FullGCForwarding::forwardee(obj));\n","filename":"src\/hotspot\/share\/gc\/shared\/preservedMarks.cpp","additions":3,"deletions":2,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -26,0 +26,1 @@\n+#include \"gc\/shared\/fullGCForwarding.hpp\"\n@@ -201,0 +202,5 @@\n+void ShenandoahArguments::initialize_heap_flags_and_sizes() {\n+  GCArguments::initialize_heap_flags_and_sizes();\n+  FullGCForwarding::initialize_flags(MaxHeapSize);\n+}\n+\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahArguments.cpp","additions":6,"deletions":0,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -38,0 +38,1 @@\n+  virtual void initialize_heap_flags_and_sizes();\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahArguments.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -200,1 +200,1 @@\n-  Klass* obj_klass = obj->klass_or_null();\n+  Klass* obj_klass = ShenandoahForwarding::klass(obj);\n@@ -238,1 +238,1 @@\n-    if (obj_klass != fwd->klass()) {\n+    if (obj_klass != ShenandoahForwarding::klass(fwd)) {\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahAsserts.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -65,0 +65,2 @@\n+  static inline size_t size(oop obj);\n+  static inline Klass* klass(oop obj);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahForwarding.hpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -93,0 +93,17 @@\n+inline Klass* ShenandoahForwarding::klass(oop obj) {\n+  if (UseCompactObjectHeaders) {\n+    markWord mark = obj->mark();\n+    if (mark.is_marked()) {\n+      oop fwd = cast_to_oop(mark.clear_lock_bits().to_pointer());\n+      mark = fwd->mark();\n+    }\n+    return mark.klass();\n+  } else {\n+    return obj->klass();\n+  }\n+}\n+\n+inline size_t ShenandoahForwarding::size(oop obj) {\n+  return obj->size_given_klass(klass(obj));\n+}\n+\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahForwarding.inline.hpp","additions":17,"deletions":0,"binary":false,"changes":17,"status":"modified"},{"patch":"@@ -29,0 +29,1 @@\n+#include \"gc\/shared\/fullGCForwarding.inline.hpp\"\n@@ -372,1 +373,1 @@\n-      p->forward_to(cast_to_oop(_compact_point));\n+      FullGCForwarding::forward_to(p, cast_to_oop(_compact_point));\n@@ -495,1 +496,1 @@\n-        old_obj->forward_to(cast_to_oop(heap->get_region(start)->bottom()));\n+        FullGCForwarding::forward_to(old_obj, cast_to_oop(heap->get_region(start)->bottom()));\n@@ -755,2 +756,2 @@\n-      if (obj->is_forwarded()) {\n-        oop forw = obj->forwardee();\n+      if (FullGCForwarding::is_forwarded(obj)) {\n+        oop forw = FullGCForwarding::forwardee(obj);\n@@ -866,1 +867,1 @@\n-    if (p->is_forwarded()) {\n+    if (FullGCForwarding::is_forwarded(p)) {\n@@ -868,1 +869,1 @@\n-      HeapWord* compact_to = cast_from_oop<HeapWord*>(p->forwardee());\n+      HeapWord* compact_to = cast_from_oop<HeapWord*>(FullGCForwarding::forwardee(p));\n@@ -973,1 +974,1 @@\n-      if (!old_obj->is_forwarded()) {\n+      if (!FullGCForwarding::is_forwarded(old_obj)) {\n@@ -982,1 +983,1 @@\n-      size_t new_start = heap->heap_region_index_containing(old_obj->forwardee());\n+      size_t new_start = heap->heap_region_index_containing(FullGCForwarding::forwardee(old_obj));\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahFullGC.cpp","additions":9,"deletions":8,"binary":false,"changes":17,"status":"modified"},{"patch":"@@ -31,0 +31,1 @@\n+#include \"gc\/shared\/fullGCForwarding.hpp\"\n@@ -425,0 +426,2 @@\n+  FullGCForwarding::initialize(_heap_region);\n+\n@@ -1133,1 +1136,1 @@\n-  size_t size = p->size();\n+  size_t size = ShenandoahForwarding::size(p);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeap.cpp","additions":4,"deletions":1,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -437,1 +437,1 @@\n-    size_t size = obj->size();\n+    size_t size = ShenandoahForwarding::size(obj);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeap.inline.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -105,1 +105,1 @@\n-      if (is_instance_ref_klass(obj->klass())) {\n+      if (is_instance_ref_klass(ShenandoahForwarding::klass(obj))) {\n@@ -132,1 +132,1 @@\n-    Klass* obj_klass = obj->klass_or_null();\n+    Klass* obj_klass = ShenandoahForwarding::klass(obj);\n@@ -147,1 +147,1 @@\n-        check(ShenandoahAsserts::_safe_unknown, obj, (obj_addr + obj->size()) <= obj_reg->top(),\n+        check(ShenandoahAsserts::_safe_unknown, obj, (obj_addr + ShenandoahForwarding::size(obj)) <= obj_reg->top(),\n@@ -151,1 +151,1 @@\n-        size_t humongous_end = humongous_start + (obj->size() >> ShenandoahHeapRegion::region_size_words_shift());\n+        size_t humongous_end = humongous_start + (ShenandoahForwarding::size(obj) >> ShenandoahHeapRegion::region_size_words_shift());\n@@ -168,1 +168,1 @@\n-          Atomic::add(&_ld[obj_reg->index()], (uint) obj->size(), memory_order_relaxed);\n+          Atomic::add(&_ld[obj_reg->index()], (uint) ShenandoahForwarding::size(obj), memory_order_relaxed);\n@@ -212,1 +212,1 @@\n-      check(ShenandoahAsserts::_safe_oop, obj, (fwd_addr + fwd->size()) <= fwd_reg->top(),\n+      check(ShenandoahAsserts::_safe_oop, obj, (fwd_addr + ShenandoahForwarding::size(fwd)) <= fwd_reg->top(),\n@@ -330,1 +330,5 @@\n-    obj->oop_iterate(this);\n+    \/\/ oop_iterate() can not deal with forwarded objects, because\n+    \/\/ it needs to load klass(), which may be overridden by the\n+    \/\/ forwarding pointer.\n+    oop fwd = ShenandoahForwarding::get_forwardee_raw(obj);\n+    fwd->oop_iterate(this);\n@@ -594,1 +598,1 @@\n-        addr += cast_to_oop(addr)->size();\n+        addr += ShenandoahForwarding::size(cast_to_oop(addr));\n@@ -610,1 +614,1 @@\n-    if (!is_instance_ref_klass(obj->klass())) {\n+    if (!is_instance_ref_klass(ShenandoahForwarding::klass(obj))) {\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahVerifier.cpp","additions":13,"deletions":9,"binary":false,"changes":22,"status":"modified"},{"patch":"@@ -301,1 +301,1 @@\n-        assert(!UseCompressedClassPointers, \"should only happen without compressed class pointers\");\n+        assert(!UseCompressedClassPointers || UseCompactObjectHeaders, \"should only happen without compressed class pointers\");\n","filename":"src\/hotspot\/share\/gc\/x\/c2\/xBarrierSetC2.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -76,2 +76,6 @@\n-  arrayOopDesc::set_mark(mem, markWord::prototype());\n-  arrayOopDesc::release_set_klass(mem, _klass);\n+  if (UseCompactObjectHeaders) {\n+    arrayOopDesc::release_set_mark(mem, _klass->prototype_header());\n+  } else {\n+    arrayOopDesc::set_mark(mem, markWord::prototype());\n+    arrayOopDesc::release_set_klass(mem, _klass);\n+  }\n","filename":"src\/hotspot\/share\/gc\/x\/xObjArrayAllocator.cpp","additions":6,"deletions":2,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -442,1 +442,1 @@\n-        assert(!UseCompressedClassPointers, \"should only happen without compressed class pointers\");\n+        assert(!UseCompressedClassPointers || UseCompactObjectHeaders, \"should only happen without compressed class pointers\");\n","filename":"src\/hotspot\/share\/gc\/z\/c2\/zBarrierSetC2.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -66,2 +66,6 @@\n-  arrayOopDesc::set_mark(mem, markWord::prototype().set_marked());\n-  arrayOopDesc::release_set_klass(mem, _klass);\n+  if (UseCompactObjectHeaders) {\n+    oopDesc::release_set_mark(mem, _klass->prototype_header().set_marked());\n+  } else {\n+    arrayOopDesc::set_mark(mem, markWord::prototype().set_marked());\n+    arrayOopDesc::release_set_klass(mem, _klass);\n+  }\n@@ -155,1 +159,5 @@\n-  oopDesc::release_set_mark(mem, markWord::prototype());\n+  if (UseCompactObjectHeaders) {\n+    oopDesc::release_set_mark(mem, _klass->prototype_header());\n+  } else {\n+    oopDesc::release_set_mark(mem, markWord::prototype());\n+  }\n","filename":"src\/hotspot\/share\/gc\/z\/zObjArrayAllocator.cpp","additions":11,"deletions":3,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -2024,4 +2024,7 @@\n-              oopDesc::set_mark(result, markWord::prototype());\n-              oopDesc::set_klass_gap(result, 0);\n-              oopDesc::release_set_klass(result, ik);\n-\n+              if (UseCompactObjectHeaders) {\n+                oopDesc::release_set_mark(result, ik->prototype_header());\n+              } else {\n+                oopDesc::set_mark(result, markWord::prototype());\n+                oopDesc::set_klass_gap(result, 0);\n+                oopDesc::release_set_klass(result, ik);\n+              }\n","filename":"src\/hotspot\/share\/interpreter\/zero\/bytecodeInterpreter.cpp","additions":7,"deletions":4,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -73,1 +73,1 @@\n-    obj->set_mark(markWord::prototype().set_marked());\n+    obj->set_mark(obj->prototype_mark().set_marked());\n","filename":"src\/hotspot\/share\/jfr\/leakprofiler\/chains\/objectSampleMarker.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -42,0 +42,3 @@\n+    static int oopDesc_klass_offset_in_bytes;\n+    static int arrayOopDesc_length_offset_in_bytes;\n+\n","filename":"src\/hotspot\/share\/jvmci\/jvmciCompilerToVM.hpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -59,0 +59,2 @@\n+int CompilerToVM::Data::oopDesc_klass_offset_in_bytes;\n+int CompilerToVM::Data::arrayOopDesc_length_offset_in_bytes;\n@@ -152,0 +154,3 @@\n+  oopDesc_klass_offset_in_bytes = oopDesc::klass_offset_in_bytes();\n+  arrayOopDesc_length_offset_in_bytes = arrayOopDesc::length_offset_in_bytes();\n+\n","filename":"src\/hotspot\/share\/jvmci\/jvmciCompilerToVMInit.cpp","additions":5,"deletions":0,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -60,0 +60,3 @@\n+  static_field(CompilerToVM::Data,             oopDesc_klass_offset_in_bytes,          int)                                          \\\n+  static_field(CompilerToVM::Data,             arrayOopDesc_length_offset_in_bytes,    int)                                          \\\n+                                                                                                                                     \\\n@@ -280,0 +283,1 @@\n+  nonstatic_field(Klass,                       _prototype_header,                             markWord)                              \\\n@@ -484,1 +488,0 @@\n-  declare_constant(LogKlassAlignmentInBytes)                              \\\n@@ -800,0 +803,1 @@\n+  declare_constant(markWord::klass_shift)                                 \\\n","filename":"src\/hotspot\/share\/jvmci\/vmStructs_jvmci.cpp","additions":5,"deletions":1,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -33,0 +33,1 @@\n+#include \"memory\/metaspace\/metablock.hpp\"\n@@ -35,0 +36,2 @@\n+#include \"memory\/metaspace\/metaspaceCommon.hpp\"\n+#include \"memory\/metaspace\/metaspaceContext.hpp\"\n@@ -39,0 +42,1 @@\n+#include \"oops\/klass.hpp\"\n@@ -43,0 +47,1 @@\n+using metaspace::MetaBlock;\n@@ -44,0 +49,1 @@\n+using metaspace::MetaspaceContext;\n@@ -52,0 +58,10 @@\n+    ClassLoaderMetaspace(lock, space_type,\n+                         MetaspaceContext::context_nonclass(),\n+                         MetaspaceContext::context_class(),\n+                         CompressedKlassPointers::klass_alignment_in_words())\n+{}\n+\n+ClassLoaderMetaspace::ClassLoaderMetaspace(Mutex* lock, Metaspace::MetaspaceType space_type,\n+                                           MetaspaceContext* non_class_context,\n+                                           MetaspaceContext* class_context,\n+                                           size_t klass_alignment_words) :\n@@ -57,3 +73,0 @@\n-  ChunkManager* const non_class_cm =\n-          ChunkManager::chunkmanager_nonclass();\n-\n@@ -62,1 +75,1 @@\n-      non_class_cm,\n+      non_class_context,\n@@ -64,2 +77,2 @@\n-      RunningCounters::used_nonclass_counter(),\n-      \"non-class sm\");\n+      Metaspace::min_allocation_alignment_words,\n+      \"non-class arena\");\n@@ -68,3 +81,1 @@\n-  if (Metaspace::using_class_space()) {\n-    ChunkManager* const class_cm =\n-            ChunkManager::chunkmanager_class();\n+  if (class_context != nullptr) {\n@@ -72,1 +83,1 @@\n-        class_cm,\n+        class_context,\n@@ -74,2 +85,2 @@\n-        RunningCounters::used_class_counter(),\n-        \"class sm\");\n+        klass_alignment_words,\n+        \"class arena\");\n@@ -92,0 +103,1 @@\n+  word_size = align_up(word_size, Metaspace::min_allocation_word_size);\n@@ -93,2 +105,5 @@\n-  if (Metaspace::is_class_space_allocation(mdType)) {\n-    return class_space_arena()->allocate(word_size);\n+  MetaBlock result, wastage;\n+  const bool is_class = have_class_space_arena() && mdType == Metaspace::ClassType;\n+  if (is_class) {\n+    assert(word_size >= (sizeof(Klass)\/BytesPerWord), \"weird size for klass: %zu\", word_size);\n+    result = class_space_arena()->allocate(word_size, wastage);\n@@ -96,1 +111,1 @@\n-    return non_class_space_arena()->allocate(word_size);\n+    result = non_class_space_arena()->allocate(word_size, wastage);\n@@ -98,0 +113,12 @@\n+  if (wastage.is_nonempty()) {\n+    non_class_space_arena()->deallocate(wastage);\n+  }\n+#ifdef ASSERT\n+  if (result.is_nonempty()) {\n+    const bool in_class_arena = class_space_arena() != nullptr ? class_space_arena()->contains(result) : false;\n+    const bool in_nonclass_arena = non_class_space_arena()->contains(result);\n+    assert((is_class && in_class_arena) || (!is_class && in_class_arena != in_nonclass_arena),\n+           \"block from neither arena \" METABLOCKFORMAT \"?\", METABLOCKFORMATARGS(result));\n+  }\n+#endif\n+  return result.base();\n@@ -135,5 +162,7 @@\n-  const bool is_class = Metaspace::using_class_space() && Metaspace::is_in_class_space(ptr);\n-  if (is_class) {\n-    class_space_arena()->deallocate(ptr, word_size);\n-  } else {\n-    non_class_space_arena()->deallocate(ptr, word_size);\n+  NOT_LP64(word_size = align_down(word_size, Metaspace::min_allocation_word_size);)\n+  MetaBlock bl(ptr, word_size);\n+  \/\/ Add to class arena only if block is usable for encodable Klass storage.\n+  MetaspaceArena* receiving_arena = non_class_space_arena();\n+  if (Metaspace::using_class_space() && Metaspace::is_in_class_space(ptr) &&\n+      is_aligned(ptr, class_space_arena()->allocation_alignment_bytes())) {\n+    receiving_arena = class_space_arena();\n@@ -141,0 +170,1 @@\n+  receiving_arena->deallocate(bl);\n@@ -183,1 +213,1 @@\n-    if (Metaspace::using_class_space()) {\n+    if (have_class_space_arena()) {\n","filename":"src\/hotspot\/share\/memory\/classLoaderMetaspace.cpp","additions":51,"deletions":21,"binary":false,"changes":72,"status":"modified"},{"patch":"@@ -36,0 +36,1 @@\n+  class ClmsTester;\n@@ -37,0 +38,1 @@\n+  class MetaspaceContext;\n@@ -60,0 +62,1 @@\n+  friend class metaspace::ClmsTester; \/\/ for gtests\n@@ -78,1 +81,6 @@\n-public:\n+  bool have_class_space_arena() const { return _class_space_arena != nullptr; }\n+\n+  ClassLoaderMetaspace(Mutex* lock, Metaspace::MetaspaceType space_type,\n+                       metaspace::MetaspaceContext* non_class_context,\n+                       metaspace::MetaspaceContext* class_context,\n+                       size_t klass_alignment_words);\n@@ -80,0 +88,1 @@\n+public:\n","filename":"src\/hotspot\/share\/memory\/classLoaderMetaspace.hpp","additions":10,"deletions":1,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -654,1 +654,1 @@\n-    \/\/ Let CCS size not be larger than 80% of MaxMetaspaceSize. Note that is\n+    \/\/ Let Class Space not be larger than 80% of MaxMetaspaceSize. Note that is\n@@ -659,2 +659,9 @@\n-    size_t max_ccs_size = 8 * (MaxMetaspaceSize \/ 10);\n-    size_t adjusted_ccs_size = MIN2(CompressedClassSpaceSize, max_ccs_size);\n+    const size_t max_ccs_size = 8 * (MaxMetaspaceSize \/ 10);\n+\n+    \/\/ Sanity check.\n+    const size_t max_klass_range = CompressedKlassPointers::max_klass_range_size();\n+    assert(max_klass_range >= reserve_alignment(),\n+           \"Klass range (%zu) must cover at least a full root chunk (%zu)\",\n+           max_klass_range, reserve_alignment());\n+\n+    size_t adjusted_ccs_size = MIN3(CompressedClassSpaceSize, max_ccs_size, max_klass_range);\n@@ -667,0 +674,12 @@\n+    \/\/ Print a warning if the adjusted size differs from the users input\n+    if (CompressedClassSpaceSize != adjusted_ccs_size) {\n+      #define X \"CompressedClassSpaceSize adjusted from user input \" \\\n+                \"%zu bytes to %zu bytes\", CompressedClassSpaceSize, adjusted_ccs_size\n+      if (FLAG_IS_CMDLINE(CompressedClassSpaceSize)) {\n+        log_warning(metaspace)(X);\n+      } else {\n+        log_info(metaspace)(X);\n+      }\n+      #undef X\n+    }\n+\n@@ -670,1 +689,0 @@\n-\n@@ -781,0 +799,1 @@\n+    \/\/ In CDS=off mode, we give the JVM some leeway to choose a favorable base\/shift combination.\n@@ -849,0 +868,9 @@\n+#ifdef ASSERT\n+    if (using_class_space() && mdtype == ClassType) {\n+      assert(is_in_class_space(result) &&\n+             is_aligned(result, CompressedKlassPointers::klass_alignment_in_bytes()), \"Sanity\");\n+    } else {\n+      assert((is_in_class_space(result) || is_in_nonclass_metaspace(result)) &&\n+             is_aligned(result, Metaspace::min_allocation_alignment_bytes), \"Sanity\");\n+    }\n+#endif\n@@ -851,1 +879,0 @@\n-\n","filename":"src\/hotspot\/share\/memory\/metaspace.cpp","additions":32,"deletions":5,"binary":false,"changes":37,"status":"modified"},{"patch":"@@ -29,0 +29,1 @@\n+#include \"memory\/virtualspace.hpp\"\n@@ -38,1 +39,0 @@\n-class ReservedSpace;\n@@ -112,0 +112,11 @@\n+  \/\/ Minimum allocation alignment, in bytes. All MetaData shall be aligned correctly\n+  \/\/ to be able to hold 64-bit data types. Unlike malloc, we don't care for larger\n+  \/\/ data types.\n+  static constexpr size_t min_allocation_alignment_bytes = sizeof(uint64_t);\n+\n+  \/\/ Minimum allocation alignment, in words, Metaspace observes.\n+  static constexpr size_t min_allocation_alignment_words = min_allocation_alignment_bytes \/ BytesPerWord;\n+\n+  \/\/ Every allocation will get rounded up to the minimum word size.\n+  static constexpr size_t min_allocation_word_size = min_allocation_alignment_words;\n+\n","filename":"src\/hotspot\/share\/memory\/metaspace.hpp","additions":12,"deletions":1,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -31,0 +31,1 @@\n+#include \"memory\/metaspace\/metablock.hpp\"\n@@ -39,2 +40,1 @@\n-\/\/ (only a few words). It is used to manage deallocated blocks - see\n-\/\/ class FreeBlocks.\n+\/\/ (only a few words). It is used to manage deallocated small blocks.\n@@ -146,1 +146,4 @@\n-  void add_block(MetaWord* p, size_t word_size) {\n+  void add_block(MetaBlock mb) {\n+    assert(!mb.is_empty(), \"Don't add empty blocks\");\n+    const size_t word_size = mb.word_size();\n+    MetaWord* const p = mb.base();\n@@ -158,2 +161,2 @@\n-  \/\/ Block may be larger. Real block size is returned in *p_real_word_size.\n-  MetaWord* remove_block(size_t word_size, size_t* p_real_word_size) {\n+  \/\/ Block may be larger.\n+  MetaBlock remove_block(size_t word_size) {\n@@ -162,0 +165,1 @@\n+    MetaBlock result;\n@@ -172,5 +176,1 @@\n-      *p_real_word_size = real_word_size;\n-      return (MetaWord*)b;\n-    } else {\n-      *p_real_word_size = 0;\n-      return nullptr;\n+      result = MetaBlock((MetaWord*)b, real_word_size);\n@@ -178,0 +178,1 @@\n+    return result;\n@@ -194,0 +195,1 @@\n+      Block* b_last = nullptr; \/\/ catch simple circularities\n@@ -196,0 +198,1 @@\n+        assert(b != b_last, \"Circle\");\n@@ -197,0 +200,1 @@\n+        b_last = b;\n","filename":"src\/hotspot\/share\/memory\/metaspace\/binList.hpp","additions":14,"deletions":10,"binary":false,"changes":24,"status":"modified"},{"patch":"@@ -183,2 +183,2 @@\n-void BlockTree::zap_range(MetaWord* p, size_t word_size) {\n-  memset(p, 0xF3, word_size * sizeof(MetaWord));\n+void BlockTree::zap_block(MetaBlock bl) {\n+  memset(bl.base(), 0xF3, bl.word_size() * sizeof(MetaWord));\n@@ -227,0 +227,6 @@\n+      \/\/ Handle simple circularities\n+      if (n == n->_right || n == n->_left || n == n->_next) {\n+        st->print_cr(\"@\" PTR_FORMAT \": circularity detected.\", p2i(n));\n+        return; \/\/ stop printing\n+      }\n+\n","filename":"src\/hotspot\/share\/memory\/metaspace\/blockTree.cpp","additions":8,"deletions":2,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -32,0 +32,1 @@\n+#include \"memory\/metaspace\/metablock.hpp\"\n@@ -38,1 +39,1 @@\n-\/\/  manage small to medium free memory blocks (see class FreeBlocks).\n+\/\/  manage medium to large free memory blocks.\n@@ -83,2 +84,1 @@\n-    \/\/  The space for that is there (these nodes are only used to manage larger blocks,\n-    \/\/  see FreeBlocks::MaxSmallBlocksWordSize).\n+    \/\/  The space for that is there (these nodes are only used to manage larger blocks).\n@@ -338,1 +338,1 @@\n-  void zap_range(MetaWord* p, size_t word_size);\n+  void zap_block(MetaBlock block);\n@@ -348,2 +348,3 @@\n-  void add_block(MetaWord* p, size_t word_size) {\n-    DEBUG_ONLY(zap_range(p, word_size));\n+  void add_block(MetaBlock block) {\n+    DEBUG_ONLY(zap_block(block);)\n+    const size_t word_size = block.word_size();\n@@ -351,1 +352,1 @@\n-    Node* n = new(p) Node(word_size);\n+    Node* n = new(block.base()) Node(word_size);\n@@ -361,3 +362,2 @@\n-  \/\/  larger than that size. Upon return, *p_real_word_size contains the actual\n-  \/\/  block size.\n-  MetaWord* remove_block(size_t word_size, size_t* p_real_word_size) {\n+  \/\/  larger than that size.\n+  MetaBlock remove_block(size_t word_size) {\n@@ -366,0 +366,1 @@\n+    MetaBlock result;\n@@ -382,2 +383,1 @@\n-      MetaWord* p = (MetaWord*)n;\n-      *p_real_word_size = n->_word_size;\n+      result = MetaBlock((MetaWord*)n, n->_word_size);\n@@ -387,2 +387,1 @@\n-      DEBUG_ONLY(zap_range(p, n->_word_size));\n-      return p;\n+      DEBUG_ONLY(zap_block(result);)\n@@ -390,1 +389,1 @@\n-    return nullptr;\n+    return result;\n","filename":"src\/hotspot\/share\/memory\/metaspace\/blockTree.hpp","additions":14,"deletions":15,"binary":false,"changes":29,"status":"modified"},{"patch":"@@ -33,3 +33,3 @@\n-void FreeBlocks::add_block(MetaWord* p, size_t word_size) {\n-  if (word_size > MaxSmallBlocksWordSize) {\n-    _tree.add_block(p, word_size);\n+void FreeBlocks::add_block(MetaBlock bl) {\n+  if (bl.word_size() > _small_blocks.MaxWordSize) {\n+    _tree.add_block(bl);\n@@ -37,1 +37,1 @@\n-    _small_blocks.add_block(p, word_size);\n+    _small_blocks.add_block(bl);\n@@ -41,1 +41,1 @@\n-MetaWord* FreeBlocks::remove_block(size_t requested_word_size) {\n+MetaBlock FreeBlocks::remove_block(size_t requested_word_size) {\n@@ -43,3 +43,3 @@\n-  MetaWord* p = nullptr;\n-  if (requested_word_size > MaxSmallBlocksWordSize) {\n-    p = _tree.remove_block(requested_word_size, &real_size);\n+  MetaBlock bl;\n+  if (requested_word_size > _small_blocks.MaxWordSize) {\n+    bl = _tree.remove_block(requested_word_size);\n@@ -47,1 +47,1 @@\n-    p = _small_blocks.remove_block(requested_word_size, &real_size);\n+    bl = _small_blocks.remove_block(requested_word_size);\n@@ -49,9 +49,1 @@\n-  if (p != nullptr) {\n-    \/\/ Blocks which are larger than a certain threshold are split and\n-    \/\/  the remainder is handed back to the manager.\n-    const size_t waste = real_size - requested_word_size;\n-    if (waste >= MinWordSize) {\n-      add_block(p + requested_word_size, waste);\n-    }\n-  }\n-  return p;\n+  return bl;\n@@ -61,1 +53,0 @@\n-\n","filename":"src\/hotspot\/share\/memory\/metaspace\/freeBlocks.cpp","additions":10,"deletions":19,"binary":false,"changes":29,"status":"modified"},{"patch":"@@ -76,4 +76,0 @@\n-  \/\/ Cutoff point: blocks larger than this size are kept in the\n-  \/\/ tree, blocks smaller than or equal to this size in the bin list.\n-  const size_t MaxSmallBlocksWordSize = BinList32::MaxWordSize;\n-\n@@ -86,1 +82,1 @@\n-  void add_block(MetaWord* p, size_t word_size);\n+  void add_block(MetaBlock bl);\n@@ -88,2 +84,2 @@\n-  \/\/ Retrieve a block of at least requested_word_size.\n-  MetaWord* remove_block(size_t requested_word_size);\n+  \/\/ Retrieve a block of at least requested_word_size. May be larger.\n+  MetaBlock remove_block(size_t requested_word_size);\n","filename":"src\/hotspot\/share\/memory\/metaspace\/freeBlocks.hpp","additions":3,"deletions":7,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -0,0 +1,76 @@\n+\/*\n+ * Copyright (c) 2023 Red Hat, Inc. All rights reserved.\n+ * Copyright (c) 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef SHARE_MEMORY_METASPACE_METABLOCK_HPP\n+#define SHARE_MEMORY_METASPACE_METABLOCK_HPP\n+\n+#include \"utilities\/globalDefinitions.hpp\"\n+\n+class outputStream;\n+\n+namespace metaspace {\n+\n+\/\/ Tiny structure to be passed by value\n+class MetaBlock {\n+\n+  MetaWord* _base;\n+  size_t _word_size;\n+\n+public:\n+\n+  MetaBlock(MetaWord* p, size_t word_size) :\n+    _base(word_size == 0 ? nullptr : p), _word_size(word_size) {}\n+  MetaBlock() : MetaBlock(nullptr, 0) {}\n+\n+  MetaWord* base() const { return _base; }\n+  const MetaWord* end() const { return _base + _word_size; }\n+  size_t word_size() const { return _word_size; }\n+  bool is_empty() const { return _base == nullptr; }\n+  bool is_nonempty() const { return _base != nullptr; }\n+  void reset() { _base = nullptr; _word_size = 0; }\n+\n+  bool operator==(const MetaBlock& rhs) const {\n+    return base() == rhs.base() &&\n+           word_size() == rhs.word_size();\n+  }\n+\n+  \/\/ Split off tail block.\n+  inline MetaBlock split_off_tail(size_t tailsize);\n+\n+  DEBUG_ONLY(inline void verify() const;)\n+\n+  \/\/ Convenience functions\n+  inline bool is_aligned_base(size_t alignment_words) const;\n+  inline bool is_aligned_size(size_t alignment_words) const;\n+\n+  void print_on(outputStream* st) const;\n+};\n+\n+#define METABLOCKFORMAT                 \"block (@\" PTR_FORMAT \" word size \" SIZE_FORMAT \")\"\n+#define METABLOCKFORMATARGS(__block__)  p2i((__block__).base()), (__block__).word_size()\n+\n+} \/\/ namespace metaspace\n+\n+#endif \/\/ SHARE_MEMORY_METASPACE_METABLOCK_HPP\n","filename":"src\/hotspot\/share\/memory\/metaspace\/metablock.hpp","additions":76,"deletions":0,"binary":false,"changes":76,"status":"added"},{"patch":"@@ -0,0 +1,87 @@\n+\/*\n+ * Copyright (c) 2023 Red Hat, Inc. All rights reserved.\n+ * Copyright (c) 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef SHARE_MEMORY_METASPACE_METABLOCK_INLINE_HPP\n+#define SHARE_MEMORY_METASPACE_METABLOCK_INLINE_HPP\n+\n+#include \"memory\/metaspace\/metablock.hpp\"\n+#include \"utilities\/globalDefinitions.hpp\"\n+#include \"utilities\/align.hpp\"\n+#include \"utilities\/ostream.hpp\"\n+#include \"utilities\/debug.hpp\"\n+\n+class outputStream;\n+\n+namespace metaspace {\n+\n+inline MetaBlock MetaBlock::split_off_tail(size_t tailsize) {\n+  if (is_empty() || tailsize == 0) {\n+    return MetaBlock();\n+  }\n+  assert(tailsize <= _word_size, \"invalid split point for block \"\n+         METABLOCKFORMAT \": %zu\", METABLOCKFORMATARGS(*this), tailsize);\n+  const size_t new_size = _word_size - tailsize;\n+  MetaBlock tail(_base + new_size, tailsize);\n+  _word_size = new_size;\n+  if (_word_size == 0) {\n+    _base = nullptr;\n+  }\n+  return tail;\n+}\n+\n+inline void MetaBlock::print_on(outputStream* st) const {\n+  st->print(METABLOCKFORMAT, METABLOCKFORMATARGS(*this));\n+}\n+\n+\/\/ Convenience functions\n+inline bool MetaBlock::is_aligned_base(size_t alignment_words) const {\n+  return is_aligned(_base, alignment_words * BytesPerWord);\n+}\n+\n+inline bool MetaBlock::is_aligned_size(size_t alignment_words) const {\n+  return is_aligned(_word_size, alignment_words);\n+}\n+\n+\/\/ some convenience asserts\n+#define assert_block_base_aligned(block, alignment_words) \\\n+  assert(block.is_aligned_base(alignment_words), \"Block wrong base alignment \" METABLOCKFORMAT, METABLOCKFORMATARGS(block));\n+\n+#define assert_block_size_aligned(block, alignment_words) \\\n+  assert(block.is_aligned_size(alignment_words), \"Block wrong size alignment \" METABLOCKFORMAT, METABLOCKFORMATARGS(block));\n+\n+#define assert_block_larger_or_equal(block, x) \\\n+  assert(block.word_size() >= x, \"Block too small \" METABLOCKFORMAT, METABLOCKFORMATARGS(block));\n+\n+#ifdef ASSERT\n+inline void MetaBlock::verify() const {\n+  assert( (_base == nullptr && _word_size == 0) ||\n+          (_base != nullptr && _word_size > 0),\n+          \"block invalid \" METABLOCKFORMAT, METABLOCKFORMATARGS(*this));\n+}\n+#endif\n+\n+} \/\/ namespace metaspace\n+\n+#endif \/\/ SHARE_MEMORY_METASPACE_METABLOCK_INLINE_HPP\n","filename":"src\/hotspot\/share\/memory\/metaspace\/metablock.inline.hpp","additions":87,"deletions":0,"binary":false,"changes":87,"status":"added"},{"patch":"@@ -33,0 +33,1 @@\n+#include \"memory\/metaspace\/metablock.inline.hpp\"\n@@ -37,0 +38,1 @@\n+#include \"memory\/metaspace\/metaspaceContext.hpp\"\n@@ -59,3 +61,4 @@\n-\/\/ Given a chunk, add its remaining free committed space to the free block list.\n-void MetaspaceArena::salvage_chunk(Metachunk* c) {\n-  size_t remaining_words = c->free_below_committed_words();\n+\/\/ Given a chunk, return the committed remainder of this chunk.\n+MetaBlock MetaspaceArena::salvage_chunk(Metachunk* c) {\n+  MetaBlock result;\n+  const size_t remaining_words = c->free_below_committed_words();\n@@ -68,1 +71,0 @@\n-    _total_used_words_counter->increment_by(remaining_words);\n@@ -70,1 +72,1 @@\n-    add_allocation_to_fbl(ptr, remaining_words);\n+    result = MetaBlock(ptr, remaining_words);\n@@ -77,0 +79,1 @@\n+  return result;\n@@ -100,5 +103,4 @@\n-void MetaspaceArena::add_allocation_to_fbl(MetaWord* p, size_t word_size) {\n-  assert(p != nullptr, \"p is null\");\n-  assert_is_aligned_metaspace_pointer(p);\n-  assert(word_size > 0, \"zero sized\");\n-\n+void MetaspaceArena::add_allocation_to_fbl(MetaBlock bl) {\n+  assert(bl.is_nonempty(), \"Sanity\");\n+  assert_block_base_aligned(bl, allocation_alignment_words());\n+  assert_block_size_aligned(bl, Metaspace::min_allocation_alignment_words);\n@@ -108,1 +110,1 @@\n-  _fbl->add_block(p, word_size);\n+  _fbl->add_block(bl);\n@@ -111,4 +113,6 @@\n-MetaspaceArena::MetaspaceArena(ChunkManager* chunk_manager, const ArenaGrowthPolicy* growth_policy,\n-                               SizeAtomicCounter* total_used_words_counter,\n-                               const char* name) :\n-  _chunk_manager(chunk_manager),\n+MetaspaceArena::MetaspaceArena(MetaspaceContext* context,\n+               const ArenaGrowthPolicy* growth_policy,\n+               size_t allocation_alignment_words,\n+               const char* name) :\n+  _allocation_alignment_words(allocation_alignment_words),\n+  _chunk_manager(context->cm()),\n@@ -118,1 +122,1 @@\n-  _total_used_words_counter(total_used_words_counter),\n+  _total_used_words_counter(context->used_words_counter()),\n@@ -121,1 +125,7 @@\n-  UL(debug, \": born.\");\n+  \/\/ Check arena allocation alignment\n+  assert(is_power_of_2(_allocation_alignment_words) &&\n+         _allocation_alignment_words >= Metaspace::min_allocation_alignment_words &&\n+         _allocation_alignment_words <= chunklevel::MIN_CHUNK_WORD_SIZE,\n+         \"Invalid alignment: %zu\", _allocation_alignment_words);\n+\n+  UL(debug, \"born.\");\n@@ -143,1 +153,1 @@\n-  UL2(info, \"returned %d chunks, total capacity \" SIZE_FORMAT \" words.\",\n+  UL2(debug, \"returned %d chunks, total capacity \" SIZE_FORMAT \" words.\",\n@@ -208,1 +218,1 @@\n-MetaWord* MetaspaceArena::allocate(size_t requested_word_size) {\n+MetaBlock MetaspaceArena::allocate(size_t requested_word_size, MetaBlock& wastage) {\n@@ -211,1 +221,0 @@\n-  MetaWord* p = nullptr;\n@@ -214,0 +223,3 @@\n+  MetaBlock result;\n+  bool taken_from_fbl = false;\n+\n@@ -216,2 +228,8 @@\n-    p = _fbl->remove_block(aligned_word_size);\n-    if (p != nullptr) {\n+    result = _fbl->remove_block(aligned_word_size);\n+    if (result.is_nonempty()) {\n+      assert_block_larger_or_equal(result, aligned_word_size);\n+      assert_block_base_aligned(result, allocation_alignment_words());\n+      assert_block_size_aligned(result, Metaspace::min_allocation_alignment_words);\n+      \/\/ Split off wastage\n+      wastage = result.split_off_tail(result.word_size() - aligned_word_size);\n+      \/\/ Stats, logging\n@@ -219,3 +237,2 @@\n-      UL2(trace, \"returning \" PTR_FORMAT \" - taken from fbl (now: %d, \" SIZE_FORMAT \").\",\n-          p2i(p), _fbl->count(), _fbl->total_size());\n-      assert_is_aligned_metaspace_pointer(p);\n+      UL2(trace, \"returning \" METABLOCKFORMAT \" with wastage \" METABLOCKFORMAT \" - taken from fbl (now: %d, \" SIZE_FORMAT \").\",\n+          METABLOCKFORMATARGS(result), METABLOCKFORMATARGS(wastage), _fbl->count(), _fbl->total_size());\n@@ -223,3 +240,2 @@\n-      \/\/ therefore we have no need to adjust any usage counters (see epilogue of allocate_inner())\n-      \/\/ and can just return here.\n-      return p;\n+      \/\/ therefore we don't need to adjust any usage counters (see epilogue of allocate_inner()).\n+      taken_from_fbl = true;\n@@ -229,2 +245,4 @@\n-  \/\/ Primary allocation\n-  p = allocate_inner(aligned_word_size);\n+  if (result.is_empty()) {\n+    \/\/ Free-block allocation failed; we allocate from the arena.\n+    result = allocate_inner(aligned_word_size, wastage);\n+  }\n@@ -232,1 +250,34 @@\n-  return p;\n+  \/\/ Logging\n+  if (result.is_nonempty()) {\n+    LogTarget(Trace, metaspace) lt;\n+    if (lt.is_enabled()) {\n+      LogStream ls(lt);\n+      ls.print(LOGFMT \": returning \" METABLOCKFORMAT \" taken from %s, \", LOGFMT_ARGS,\n+               METABLOCKFORMATARGS(result), (taken_from_fbl ? \"fbl\" : \"arena\"));\n+      if (wastage.is_empty()) {\n+        ls.print(\"no wastage\");\n+      } else {\n+        ls.print(\"wastage \" METABLOCKFORMAT, METABLOCKFORMATARGS(wastage));\n+      }\n+    }\n+  } else {\n+    UL(info, \"allocation failed, returned null.\");\n+  }\n+\n+  \/\/ Final sanity checks\n+#ifdef ASSERT\n+   result.verify();\n+   wastage.verify();\n+   if (result.is_nonempty()) {\n+     assert(result.word_size() == aligned_word_size &&\n+            is_aligned(result.base(), _allocation_alignment_words * BytesPerWord),\n+            \"result bad or unaligned: \" METABLOCKFORMAT \".\", METABLOCKFORMATARGS(result));\n+   }\n+   if (wastage.is_nonempty()) {\n+     assert(wastage.is_empty() ||\n+            (wastage.is_aligned_base(Metaspace::min_allocation_alignment_words) &&\n+             wastage.is_aligned_size(Metaspace::min_allocation_alignment_words)),\n+            \"Misaligned wastage: \" METABLOCKFORMAT\".\", METABLOCKFORMATARGS(wastage));\n+   }\n+#endif \/\/ ASSERT\n+   return result;\n@@ -236,2 +287,1 @@\n-MetaWord* MetaspaceArena::allocate_inner(size_t word_size) {\n-  assert_is_aligned(word_size, metaspace::AllocationAlignmentWordSize);\n+MetaBlock MetaspaceArena::allocate_inner(size_t word_size, MetaBlock& wastage) {\n@@ -239,1 +289,1 @@\n-  MetaWord* p = nullptr;\n+  MetaBlock result;\n@@ -242,0 +292,1 @@\n+  size_t alignment_gap_size = 0;\n@@ -244,1 +295,0 @@\n-\n@@ -247,0 +297,4 @@\n+    const MetaWord* const chunk_top = current_chunk()->top();\n+    alignment_gap_size = align_up(chunk_top, _allocation_alignment_words * BytesPerWord) - chunk_top;\n+    const size_t word_size_plus_alignment = word_size + alignment_gap_size;\n+\n@@ -249,2 +303,2 @@\n-    if (current_chunk()->free_words() < word_size) {\n-      if (!attempt_enlarge_current_chunk(word_size)) {\n+    if (current_chunk()->free_words() < word_size_plus_alignment) {\n+      if (!attempt_enlarge_current_chunk(word_size_plus_alignment)) {\n@@ -262,2 +316,2 @@\n-      if (!current_chunk()->ensure_committed_additional(word_size)) {\n-        UL2(info, \"commit failure (requested size: \" SIZE_FORMAT \")\", word_size);\n+      if (!current_chunk()->ensure_committed_additional(word_size_plus_alignment)) {\n+        UL2(info, \"commit failure (requested size: \" SIZE_FORMAT \")\", word_size_plus_alignment);\n@@ -270,2 +324,8 @@\n-      p = current_chunk()->allocate(word_size);\n-      assert(p != nullptr, \"Allocation from chunk failed.\");\n+      MetaWord* const p_gap = current_chunk()->allocate(word_size_plus_alignment);\n+      assert(p_gap != nullptr, \"Allocation from chunk failed.\");\n+      MetaWord* const p_user_allocation = p_gap + alignment_gap_size;\n+      result = MetaBlock(p_user_allocation, word_size);\n+      if (alignment_gap_size > 0) {\n+        NOT_LP64(assert(alignment_gap_size >= AllocationAlignmentWordSize, \"Sanity\"));\n+        wastage = MetaBlock(p_gap, alignment_gap_size);\n+      }\n@@ -275,1 +335,1 @@\n-  if (p == nullptr) {\n+  if (result.is_empty()) {\n@@ -289,1 +349,1 @@\n-        salvage_chunk(current_chunk());\n+        wastage = salvage_chunk(current_chunk());\n@@ -295,2 +355,4 @@\n-      \/\/ Now, allocate from that chunk. That should work.\n-      p = current_chunk()->allocate(word_size);\n+      \/\/ Now, allocate from that chunk. That should work. Note that the resulting allocation\n+      \/\/ is guaranteed to be aligned to arena alignment, since arena alignment cannot be larger\n+      \/\/ than smallest chunk size, and chunk starts are aligned by their size (buddy allocation).\n+      MetaWord* const p = current_chunk()->allocate(word_size);\n@@ -298,0 +360,1 @@\n+      result = MetaBlock(p, word_size);\n@@ -303,1 +366,1 @@\n-  if (p == nullptr) {\n+  if (result.is_empty()) {\n@@ -307,1 +370,1 @@\n-    _total_used_words_counter->increment_by(word_size);\n+    _total_used_words_counter->increment_by(word_size + wastage.word_size());\n@@ -312,3 +375,1 @@\n-  if (p == nullptr) {\n-    UL(info, \"allocation failed, returned null.\");\n-  } else {\n+  if (result.is_nonempty()) {\n@@ -317,1 +378,0 @@\n-    UL2(trace, \"returning \" PTR_FORMAT \".\", p2i(p));\n@@ -320,1 +380,12 @@\n-  assert_is_aligned_metaspace_pointer(p);\n+#ifdef ASSERT\n+  if (wastage.is_nonempty()) {\n+    \/\/ Wastage from arena allocations only occurs if either or both are true:\n+    \/\/ - it is too small to hold the requested allocation words\n+    \/\/ - it is misaligned\n+    assert(!wastage.is_aligned_base(allocation_alignment_words()) ||\n+           wastage.word_size() < word_size,\n+           \"Unexpected wastage: \" METABLOCKFORMAT \", arena alignment: %zu, allocation word size: %zu\",\n+           METABLOCKFORMATARGS(wastage), allocation_alignment_words(), word_size);\n+    wastage.verify();\n+  }\n+#endif \/\/ ASSERT\n@@ -322,1 +393,1 @@\n-  return p;\n+  return result;\n@@ -327,19 +398,14 @@\n-void MetaspaceArena::deallocate(MetaWord* p, size_t word_size) {\n-  \/\/ At this point a current chunk must exist since we only deallocate if we did allocate before.\n-  assert(current_chunk() != nullptr, \"stray deallocation?\");\n-  assert(is_valid_area(p, word_size),\n-         \"Pointer range not part of this Arena and cannot be deallocated: (\" PTR_FORMAT \"..\" PTR_FORMAT \").\",\n-         p2i(p), p2i(p + word_size));\n-\n-  UL2(trace, \"deallocating \" PTR_FORMAT \", word size: \" SIZE_FORMAT \".\",\n-      p2i(p), word_size);\n-\n-  \/\/ Only blocks that had been allocated via MetaspaceArena::allocate(size) must be handed in\n-  \/\/ to MetaspaceArena::deallocate(), and only with the same size that had been original used for allocation.\n-  \/\/ Therefore the pointer must be aligned correctly, and size can be alignment-adjusted (the latter\n-  \/\/ only matters on 32-bit):\n-  assert_is_aligned_metaspace_pointer(p);\n-  size_t raw_word_size = get_raw_word_size_for_requested_word_size(word_size);\n-\n-  add_allocation_to_fbl(p, raw_word_size);\n-\n+void MetaspaceArena::deallocate(MetaBlock block) {\n+  \/\/ Note that we may receive blocks that don't originate from this\n+  \/\/ arena, and that is okay.\n+  DEBUG_ONLY(block.verify();)\n+  \/\/ This only matters on 32-bit:\n+  \/\/ Since we always align up allocations from arena, we align up here, too.\n+#ifndef _LP64\n+  MetaBlock raw_block(block.base(), get_raw_word_size_for_requested_word_size(block.word_size()));\n+  add_allocation_to_fbl(raw_block);\n+#else\n+  add_allocation_to_fbl(block);\n+#endif\n+  UL2(trace, \"added to fbl: \" METABLOCKFORMAT \", (now: %d, \" SIZE_FORMAT \").\",\n+      METABLOCKFORMATARGS(block), _fbl->count(), _fbl->total_size());\n@@ -403,4 +469,4 @@\n-\/\/ Returns true if the area indicated by pointer and size have actually been allocated\n-\/\/ from this arena.\n-bool MetaspaceArena::is_valid_area(MetaWord* p, size_t word_size) const {\n-  assert(p != nullptr && word_size > 0, \"Sanity\");\n+\/\/ Returns true if the given block is contained in this arena\n+bool MetaspaceArena::contains(MetaBlock bl) const {\n+  DEBUG_ONLY(bl.verify();)\n+  assert(bl.is_nonempty(), \"Sanity\");\n@@ -409,3 +475,3 @@\n-    assert(c->is_valid_committed_pointer(p) ==\n-           c->is_valid_committed_pointer(p + word_size - 1), \"range intersects\");\n-    found = c->is_valid_committed_pointer(p);\n+    assert(c->is_valid_committed_pointer(bl.base()) ==\n+           c->is_valid_committed_pointer(bl.end() - 1), \"range intersects\");\n+    found = c->is_valid_committed_pointer(bl.base());\n","filename":"src\/hotspot\/share\/memory\/metaspace\/metaspaceArena.cpp","additions":145,"deletions":79,"binary":false,"changes":224,"status":"modified"},{"patch":"@@ -32,0 +32,1 @@\n+#include \"memory\/metaspace\/metablock.hpp\"\n@@ -40,0 +41,1 @@\n+struct ArenaStats;\n@@ -41,1 +43,0 @@\n-class Metachunk;\n@@ -43,0 +44,2 @@\n+class Metachunk;\n+class MetaspaceContext;\n@@ -44,1 +47,0 @@\n-struct ArenaStats;\n@@ -77,0 +79,1 @@\n+  friend class MetaspaceArenaTestFriend;\n@@ -81,0 +84,3 @@\n+  \/\/ Allocation alignment specific to this arena\n+  const size_t _allocation_alignment_words;\n+\n@@ -107,1 +113,1 @@\n-  void add_allocation_to_fbl(MetaWord* p, size_t word_size);\n+  void add_allocation_to_fbl(MetaBlock bl);\n@@ -109,2 +115,2 @@\n-  \/\/ Given a chunk, add its remaining free committed space to the free block list.\n-  void salvage_chunk(Metachunk* c);\n+  \/\/ Given a chunk, return the committed remainder of this chunk.\n+  MetaBlock salvage_chunk(Metachunk* c);\n@@ -125,4 +131,0 @@\n-  \/\/ Returns true if the area indicated by pointer and size have actually been allocated\n-  \/\/ from this arena.\n-  DEBUG_ONLY(bool is_valid_area(MetaWord* p, size_t word_size) const;)\n-\n@@ -130,1 +132,1 @@\n-  MetaWord* allocate_inner(size_t word_size);\n+  MetaBlock allocate_inner(size_t word_size, MetaBlock& wastage);\n@@ -134,2 +136,3 @@\n-  MetaspaceArena(ChunkManager* chunk_manager, const ArenaGrowthPolicy* growth_policy,\n-                 SizeAtomicCounter* total_used_words_counter,\n+  MetaspaceArena(MetaspaceContext* context,\n+                 const ArenaGrowthPolicy* growth_policy,\n+                 size_t allocation_alignment_words,\n@@ -140,0 +143,3 @@\n+  size_t allocation_alignment_words() const { return _allocation_alignment_words; }\n+  size_t allocation_alignment_bytes() const { return allocation_alignment_words() * BytesPerWord; }\n+\n@@ -141,6 +147,5 @@\n-  \/\/ 1) Attempt to allocate from the dictionary of deallocated blocks.\n-  \/\/ 2) Attempt to allocate from the current chunk.\n-  \/\/ 3) Attempt to enlarge the current chunk in place if it is too small.\n-  \/\/ 4) Attempt to get a new chunk and allocate from that chunk.\n-  \/\/ At any point, if we hit a commit limit, we return null.\n-  MetaWord* allocate(size_t word_size);\n+  \/\/ On success, returns non-empty block of the specified word size, and\n+  \/\/ possibly a wastage block that is the result of alignment operations.\n+  \/\/ On failure, returns an empty block. Failure may happen if we hit a\n+  \/\/ commit limit.\n+  MetaBlock allocate(size_t word_size, MetaBlock& wastage);\n@@ -150,1 +155,1 @@\n-  void deallocate(MetaWord* p, size_t word_size);\n+  void deallocate(MetaBlock bl);\n@@ -164,0 +169,2 @@\n+  \/\/ Returns true if the given block is contained in this arena\n+  DEBUG_ONLY(bool contains(MetaBlock bl) const;)\n","filename":"src\/hotspot\/share\/memory\/metaspace\/metaspaceArena.hpp","additions":26,"deletions":19,"binary":false,"changes":45,"status":"modified"},{"patch":"@@ -45,2 +45,1 @@\n-\/\/ Klass* structures need to be aligned to KlassAlignmentInBytes, but since that is\n-\/\/ 64-bit, we don't need special handling for allocating Klass*.\n+\/\/ Klass* structures need to be aligned to Klass* alignment,\n@@ -51,1 +50,0 @@\n-STATIC_ASSERT(AllocationAlignmentByteSize == (size_t)KlassAlignmentInBytes);\n","filename":"src\/hotspot\/share\/memory\/metaspace\/metaspaceCommon.hpp","additions":1,"deletions":3,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -78,0 +78,12 @@\n+size_t MetaspaceContext::used_words() const {\n+  return _used_words_counter.get();\n+}\n+\n+size_t MetaspaceContext::committed_words() const {\n+  return _vslist->committed_words();\n+}\n+\n+size_t MetaspaceContext::reserved_words() const {\n+  return _vslist->reserved_words();\n+}\n+\n","filename":"src\/hotspot\/share\/memory\/metaspace\/metaspaceContext.cpp","additions":12,"deletions":0,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -30,0 +30,1 @@\n+#include \"memory\/metaspace\/counters.hpp\"\n@@ -64,0 +65,1 @@\n+  SizeAtomicCounter _used_words_counter;\n@@ -81,2 +83,3 @@\n-  VirtualSpaceList* vslist() { return _vslist; }\n-  ChunkManager* cm() { return _cm; }\n+  VirtualSpaceList* vslist()                    { return _vslist; }\n+  ChunkManager* cm()                            { return _cm; }\n+  SizeAtomicCounter* used_words_counter()       { return &_used_words_counter; }\n@@ -106,0 +109,3 @@\n+  size_t used_words() const;\n+  size_t committed_words() const;\n+  size_t reserved_words() const;\n","filename":"src\/hotspot\/share\/memory\/metaspace\/metaspaceContext.hpp","additions":8,"deletions":2,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -42,0 +42,1 @@\n+#include \"oops\/compressedKlass.hpp\"\n@@ -121,0 +122,3 @@\n+#ifdef _LP64\n+  CompressedKlassPointers::print_mode(out);\n+#endif\n@@ -328,1 +332,1 @@\n-  out->print(\"(percentages refer to total committed size \");\n+  out->print(\" (percentages refer to total committed size \");\n","filename":"src\/hotspot\/share\/memory\/metaspace\/metaspaceReporter.cpp","additions":5,"deletions":1,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -189,1 +189,0 @@\n-\n@@ -196,3 +195,0 @@\n-  \/\/ Deallocated allocations still count as used\n-  assert(total_used >= _free_blocks_word_size,\n-         \"Sanity\");\n","filename":"src\/hotspot\/share\/memory\/metaspace\/metaspaceStatistics.cpp","additions":0,"deletions":4,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -28,1 +28,1 @@\n-#include \"memory\/metaspace\/counters.hpp\"\n+#include \"memory\/metaspace\/metaspaceContext.hpp\"\n@@ -34,3 +34,0 @@\n-SizeAtomicCounter RunningCounters::_used_class_counter;\n-SizeAtomicCounter RunningCounters::_used_nonclass_counter;\n-\n@@ -75,1 +72,2 @@\n-  return _used_class_counter.get();\n+  const MetaspaceContext* context = MetaspaceContext::context_class();\n+  return context != nullptr ? context->used_words() : 0;\n@@ -79,1 +77,1 @@\n-  return _used_nonclass_counter.get();\n+  return MetaspaceContext::context_nonclass()->used_words();\n","filename":"src\/hotspot\/share\/memory\/metaspace\/runningCounters.cpp","additions":4,"deletions":6,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -30,1 +30,0 @@\n-#include \"memory\/metaspace\/counters.hpp\"\n@@ -35,6 +34,1 @@\n-class RunningCounters : public AllStatic {\n-\n-  static SizeAtomicCounter _used_class_counter;\n-  static SizeAtomicCounter _used_nonclass_counter;\n-\n-public:\n+struct RunningCounters : public AllStatic {\n@@ -68,4 +62,0 @@\n-  \/\/ Direct access to the counters.\n-  static SizeAtomicCounter* used_nonclass_counter()     { return &_used_nonclass_counter; }\n-  static SizeAtomicCounter* used_class_counter()        { return &_used_class_counter; }\n-\n","filename":"src\/hotspot\/share\/memory\/metaspace\/runningCounters.hpp","additions":1,"deletions":11,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -57,1 +57,6 @@\n-  return _arena->allocate(word_size);\n+  MetaBlock result, wastage;\n+  result = _arena->allocate(word_size, wastage);\n+  if (wastage.is_nonempty()) {\n+    _arena->deallocate(wastage);\n+  }\n+  return result.base();\n@@ -62,1 +67,1 @@\n-  return _arena->deallocate(p, word_size);\n+  _arena->deallocate(MetaBlock(p, word_size));\n@@ -73,1 +78,0 @@\n-  _used_words_counter(),\n@@ -106,1 +110,1 @@\n-    arena = new MetaspaceArena(_context->cm(), growth_policy, &_used_words_counter, _name);\n+    arena = new MetaspaceArena(_context, growth_policy, Metaspace::min_allocation_alignment_words, _name);\n@@ -127,0 +131,13 @@\n+size_t MetaspaceTestContext::used_words() const {\n+  return _context->used_words_counter()->get();\n+}\n+\n+size_t MetaspaceTestContext::committed_words() const {\n+  assert(_commit_limiter.committed_words() == _context->committed_words(), \"Sanity\");\n+  return _context->committed_words();\n+}\n+\n+size_t MetaspaceTestContext::reserved_words() const {\n+  return _context->reserved_words();\n+}\n+\n","filename":"src\/hotspot\/share\/memory\/metaspace\/testHelpers.cpp","additions":21,"deletions":4,"binary":false,"changes":25,"status":"modified"},{"patch":"@@ -78,1 +78,0 @@\n-  SizeAtomicCounter _used_words_counter;\n@@ -101,0 +100,1 @@\n+  MetaspaceContext* context() const           { return _context; }\n@@ -107,3 +107,3 @@\n-  \/\/ Convenience function to retrieve total committed\/used words\n-  size_t used_words() const       { return _used_words_counter.get(); }\n-  size_t committed_words() const  { return _commit_limiter.committed_words(); }\n+  size_t used_words() const;\n+  size_t committed_words() const;\n+  size_t reserved_words() const;\n","filename":"src\/hotspot\/share\/memory\/metaspace\/testHelpers.hpp","additions":4,"deletions":4,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -400,2 +400,7 @@\n-  assert(oopDesc::klass_offset_in_bytes() < static_cast<intptr_t>(os::vm_page_size()),\n-         \"Klass offset is expected to be less than the page size\");\n+  if (UseCompactObjectHeaders) {\n+    assert(oopDesc::mark_offset_in_bytes() < static_cast<intptr_t>(os::vm_page_size()),\n+           \"Mark offset is expected to be less than the page size\");\n+  } else {\n+    assert(oopDesc::klass_offset_in_bytes() < static_cast<intptr_t>(os::vm_page_size()),\n+           \"Klass offset is expected to be less than the page size\");\n+  }\n","filename":"src\/hotspot\/share\/memory\/universe.cpp","additions":7,"deletions":2,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -82,2 +82,3 @@\n-  \/\/ declared nonstatic fields in arrayOopDesc if not compressed, otherwise\n-  \/\/ it occupies the second half of the _klass field in oopDesc.\n+  \/\/ mark-word when using compact headers (+UseCompactObjectHeaders), otherwise\n+  \/\/ after the compressed Klass* when running with compressed class-pointers\n+  \/\/ (+UseCompressedClassPointers), or else after the full Klass*.\n@@ -85,2 +86,1 @@\n-    return UseCompressedClassPointers ? klass_gap_offset_in_bytes() :\n-                               (int)sizeof(arrayOopDesc);\n+    return oopDesc::base_offset_in_bytes();\n","filename":"src\/hotspot\/share\/oops\/arrayOop.hpp","additions":4,"deletions":4,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -30,0 +30,1 @@\n+#include \"runtime\/java.hpp\"\n@@ -35,2 +36,5 @@\n-address CompressedKlassPointers::_base = nullptr;\n-int CompressedKlassPointers::_shift = 0;\n+int CompressedKlassPointers::_narrow_klass_pointer_bits = -1;\n+int CompressedKlassPointers::_max_shift = -1;\n+\n+address CompressedKlassPointers::_base = (address)-1;\n+int CompressedKlassPointers::_shift = -1;\n@@ -39,0 +43,2 @@\n+narrowKlass CompressedKlassPointers::_lowest_valid_narrow_klass_id = (narrowKlass)-1;\n+narrowKlass CompressedKlassPointers::_highest_valid_narrow_klass_id = (narrowKlass)-1;\n@@ -42,4 +48,17 @@\n-#ifdef ASSERT\n-void CompressedKlassPointers::assert_is_valid_encoding(address addr, size_t len, address base, int shift) {\n-  assert(base + nth_bit(32 + shift) >= addr + len, \"Encoding (base=\" PTR_FORMAT \", shift=%d) does not \"\n-         \"fully cover the class range \" PTR_FORMAT \"-\" PTR_FORMAT, p2i(base), shift, p2i(addr), p2i(addr + len));\n+size_t CompressedKlassPointers::max_klass_range_size() {\n+  \/\/ We disallow klass range sizes larger than 4GB even if the encoding\n+  \/\/ range would allow for a larger Klass range (e.g. Base=zero, shift=3 -> 32GB).\n+  \/\/ That is because many CPU-specific compiler decodings do not want the\n+  \/\/ shifted narrow Klass to spill over into the third quadrant of the 64-bit target\n+  \/\/ address, e.g. to use a 16-bit move for a simplified base addition.\n+  return MIN2(4 * G, max_encoding_range_size());\n+}\n+\n+void CompressedKlassPointers::pre_initialize() {\n+  if (UseCompactObjectHeaders) {\n+    _narrow_klass_pointer_bits = narrow_klass_pointer_bits_coh;\n+    _max_shift = max_shift_coh;\n+  } else {\n+    _narrow_klass_pointer_bits = narrow_klass_pointer_bits_noncoh;\n+    _max_shift = max_shift_noncoh;\n+  }\n@@ -47,0 +66,65 @@\n+\n+#ifdef ASSERT\n+void CompressedKlassPointers::sanity_check_after_initialization() {\n+  \/\/ In expectation of an assert, prepare condensed info to be printed with the assert.\n+  char tmp[256];\n+  os::snprintf(tmp, sizeof(tmp), \"klass range: \" RANGE2FMT \",\"\n+      \" base \" PTR_FORMAT \", shift %d, lowest\/highest valid narrowKlass %u\/%u\",\n+      RANGE2FMTARGS(_klass_range_start, _klass_range_end),\n+      p2i(_base), _shift, _lowest_valid_narrow_klass_id, _highest_valid_narrow_klass_id);\n+#define ASSERT_HERE(cond) assert(cond, \" (%s)\", tmp);\n+#define ASSERT_HERE_2(cond, msg) assert(cond, msg \" (%s)\", tmp);\n+\n+  \/\/ All values must be inited\n+  ASSERT_HERE(_max_shift != -1);\n+  ASSERT_HERE(_klass_range_start != (address)-1);\n+  ASSERT_HERE(_klass_range_end != (address)-1);\n+  ASSERT_HERE(_lowest_valid_narrow_klass_id != (narrowKlass)-1);\n+  ASSERT_HERE(_base != (address)-1);\n+  ASSERT_HERE(_shift != -1);\n+\n+  const size_t klass_align = klass_alignment_in_bytes();\n+\n+  \/\/ must be aligned enough hold 64-bit data\n+  ASSERT_HERE(is_aligned(klass_align, sizeof(uint64_t)));\n+\n+  \/\/ should be smaller than the minimum metaspace chunk size (soft requirement)\n+  ASSERT_HERE(klass_align <= K);\n+\n+  ASSERT_HERE(_klass_range_end > _klass_range_start);\n+\n+  \/\/ Check that Klass range is fully engulfed in the encoding range\n+  const address encoding_start = _base;\n+  const address encoding_end = _base + nth_bit(narrow_klass_pointer_bits() + _shift);\n+  ASSERT_HERE_2(_klass_range_start >= _base && _klass_range_end <= encoding_end,\n+                \"Resulting encoding range does not fully cover the class range\");\n+\n+  \/\/ Check that Klass range is aligned to Klass alignment. Note that this should never be\n+  \/\/ an issue since the Klass range is handed in by either CDS- or Metaspace-initialization, and\n+  \/\/ it should be the result of an mmap operation that operates on page sizes. So as long as\n+  \/\/ the Klass alignment is <= page size, we are fine.\n+  ASSERT_HERE_2(is_aligned(_klass_range_start, klass_align) &&\n+                is_aligned(_klass_range_end, klass_align),\n+                \"Klass range must start and end at a properly aligned address\");\n+\n+  \/\/ Check _lowest_valid_narrow_klass_id and _highest_valid_narrow_klass_id\n+  ASSERT_HERE_2(_lowest_valid_narrow_klass_id > 0, \"Null is not a valid narrowKlass\");\n+  ASSERT_HERE(_highest_valid_narrow_klass_id > _lowest_valid_narrow_klass_id);\n+\n+  Klass* const k1 = decode_not_null_without_asserts(_lowest_valid_narrow_klass_id, _base, _shift);\n+  if (encoding_start == _klass_range_start) {\n+    ASSERT_HERE_2((address)k1 == _klass_range_start + klass_align, \"Not lowest\");\n+  } else {\n+    ASSERT_HERE_2((address)k1 == _klass_range_start, \"Not lowest\");\n+  }\n+  narrowKlass nk1 = encode_not_null_without_asserts(k1, _base, _shift);\n+  ASSERT_HERE_2(nk1 == _lowest_valid_narrow_klass_id, \"not reversible\");\n+\n+  Klass* const k2 = decode_not_null_without_asserts(_highest_valid_narrow_klass_id, _base, _shift);\n+  ASSERT_HERE((address)k2 == _klass_range_end - klass_align);\n+  narrowKlass nk2 = encode_not_null_without_asserts(k2, _base, _shift);\n+  ASSERT_HERE_2(nk2 == _highest_valid_narrow_klass_id, \"not reversible\");\n+\n+#ifdef AARCH64\n+  \/\/ On aarch64, we never expect a shift value > 0 in standard (non-coh) mode\n+  ASSERT_HERE_2(UseCompactObjectHeaders || _shift == 0, \"Shift > 0 in non-coh mode?\");\n@@ -48,0 +132,20 @@\n+#undef ASSERT_HERE\n+#undef ASSERT_HERE_2\n+}\n+#endif \/\/ ASSERT\n+\n+\/\/ Helper function: given current Klass Range, Base and Shift, calculate the lowest and highest values\n+\/\/ of narrowKlass we can expect.\n+void CompressedKlassPointers::calc_lowest_highest_narrow_klass_id() {\n+  address lowest_possible_klass_location = _klass_range_start;\n+\n+  \/\/ A Klass will never be placed at the Encoding range start, since that would translate to a narrowKlass=0, which\n+  \/\/ is disallowed. Note that both Metaspace and CDS prvent allocation at the first address for this reason.\n+  if (lowest_possible_klass_location == _base) {\n+    lowest_possible_klass_location += klass_alignment_in_bytes();\n+  }\n+  _lowest_valid_narrow_klass_id = (narrowKlass) ((uintptr_t)(lowest_possible_klass_location - _base) >> _shift);\n+\n+  address highest_possible_klass_location = _klass_range_end - klass_alignment_in_bytes();\n+  _highest_valid_narrow_klass_id = (narrowKlass) ((uintptr_t)(highest_possible_klass_location - _base) >> _shift);\n+}\n@@ -55,3 +159,7 @@\n-  const int narrow_klasspointer_bits = sizeof(narrowKlass) * 8;\n-  const size_t encoding_range_size = nth_bit(narrow_klasspointer_bits + requested_shift);\n-  address encoding_range_end = requested_base + encoding_range_size;\n+  if (len > max_klass_range_size()) {\n+    stringStream ss;\n+    ss.print(\"Class space size and CDS archive size combined (%zu) \"\n+             \"exceed the maximum possible size (%zu)\",\n+             len, max_klass_range_size());\n+    vm_exit_during_initialization(ss.base());\n+  }\n@@ -59,2 +167,3 @@\n-  \/\/ Note: it would be technically valid for the encoding base to precede the start of the Klass range. But we only call\n-  \/\/ this function from CDS, and therefore know this to be true.\n+  \/\/ Note: While it would be technically valid for the encoding base to precede the start of the Klass range,\n+  \/\/ we never do this here. This is used at CDS runtime to re-instate the scheme used to precompute the\n+  \/\/ narrow Klass IDs in the archive, and the requested base should point to the start of the Klass range.\n@@ -62,1 +171,0 @@\n-  assert(encoding_range_end >= end, \"Encoding does not cover the full Klass range\");\n@@ -71,1 +179,3 @@\n-  DEBUG_ONLY(assert_is_valid_encoding(addr, len, _base, _shift);)\n+  calc_lowest_highest_narrow_klass_id();\n+\n+  DEBUG_ONLY(sanity_check_after_initialization();)\n@@ -80,1 +190,2 @@\n-  return reserve_address_space_X(0, nth_bit(32), size, Metaspace::reserve_alignment(), aslr);\n+  const size_t unscaled_max = nth_bit(narrow_klass_pointer_bits());\n+  return reserve_address_space_X(0, unscaled_max, size, Metaspace::reserve_alignment(), aslr);\n@@ -84,1 +195,3 @@\n-  return reserve_address_space_X(nth_bit(32), nth_bit(32 + LogKlassAlignmentInBytes), size, Metaspace::reserve_alignment(), aslr);\n+  const size_t unscaled_max = nth_bit(narrow_klass_pointer_bits());\n+  const size_t zerobased_max = nth_bit(narrow_klass_pointer_bits() + max_shift());\n+  return reserve_address_space_X(unscaled_max, zerobased_max, size, Metaspace::reserve_alignment(), aslr);\n@@ -91,2 +204,0 @@\n-#if !defined(AARCH64) || defined(ZERO)\n-\/\/ On aarch64 we have an own version; all other platforms use the default version\n@@ -95,0 +206,7 @@\n+  if (len > max_klass_range_size()) {\n+    stringStream ss;\n+    ss.print(\"Class space size (%zu) exceeds the maximum possible size (%zu)\",\n+              len, max_klass_range_size());\n+    vm_exit_during_initialization(ss.base());\n+  }\n+\n@@ -99,8 +217,1 @@\n-  \/\/ The default version of this code tries, in order of preference:\n-  \/\/ -unscaled    (base=0 shift=0)\n-  \/\/ -zero-based  (base=0 shift>0)\n-  \/\/ -nonzero-base (base>0 shift=0)\n-  \/\/ Note that base>0 shift>0 should never be needed, since the klass range will\n-  \/\/ never exceed 4GB.\n-  constexpr uintptr_t unscaled_max = nth_bit(32);\n-  assert(len <= unscaled_max, \"Klass range larger than 32 bits?\");\n+  \/\/ Calculate Base and Shift:\n@@ -108,1 +219,15 @@\n-  constexpr uintptr_t zerobased_max = nth_bit(32 + LogKlassAlignmentInBytes);\n+  if (UseCompactObjectHeaders) {\n+\n+    \/\/ In compact object header mode, with 22-bit narrowKlass, we don't attempt for\n+    \/\/ zero-based mode. Instead, we set the base to the start of the klass range and\n+    \/\/ then try for the smallest shift possible that still covers the whole range.\n+    \/\/ The reason is that we want to avoid, if possible, shifts larger than\n+    \/\/ a cacheline size.\n+    _base = addr;\n+\n+    const int log_cacheline = exact_log2(DEFAULT_CACHE_LINE_SIZE);\n+    int s = max_shift();\n+    while (s > log_cacheline && ((size_t)nth_bit(narrow_klass_pointer_bits() + s - 1) > len)) {\n+      s--;\n+    }\n+    _shift = s;\n@@ -110,4 +235,0 @@\n-  address const end = addr + len;\n-  if (end <= (address)unscaled_max) {\n-    _base = nullptr;\n-    _shift = 0;\n@@ -115,1 +236,20 @@\n-    if (end <= (address)zerobased_max) {\n+\n+    \/\/ Traditional (non-compact) header mode\n+    const uintptr_t unscaled_max = nth_bit(narrow_klass_pointer_bits());\n+    const uintptr_t zerobased_max = nth_bit(narrow_klass_pointer_bits() + max_shift());\n+\n+#ifdef AARCH64\n+    \/\/ Aarch64 avoids zero-base shifted mode (_base=0 _shift>0), instead prefers\n+    \/\/ non-zero-based mode with a zero shift.\n+    _shift = 0;\n+    address const end = addr + len;\n+    _base = (end <= (address)unscaled_max) ? nullptr : addr;\n+#else\n+    \/\/ We try, in order of preference:\n+    \/\/ -unscaled    (base=0 shift=0)\n+    \/\/ -zero-based  (base=0 shift>0)\n+    \/\/ -nonzero-base (base>0 shift=0)\n+    \/\/ Note that base>0 shift>0 should never be needed, since the klass range will\n+    \/\/ never exceed 4GB.\n+    address const end = addr + len;\n+    if (end <= (address)unscaled_max) {\n@@ -117,3 +257,0 @@\n-      _shift = LogKlassAlignmentInBytes;\n-    } else {\n-      _base = addr;\n@@ -121,0 +258,8 @@\n+    } else {\n+      if (end <= (address)zerobased_max) {\n+        _base = nullptr;\n+        _shift = max_shift();\n+      } else {\n+        _base = addr;\n+        _shift = 0;\n+      }\n@@ -122,0 +267,1 @@\n+#endif \/\/ AARCH64\n@@ -124,1 +270,5 @@\n-  DEBUG_ONLY(assert_is_valid_encoding(addr, len, _base, _shift);)\n+  calc_lowest_highest_narrow_klass_id();\n+\n+#ifdef ASSERT\n+  sanity_check_after_initialization();\n+#endif\n@@ -126,1 +276,0 @@\n-#endif \/\/ !AARCH64 || ZERO\n@@ -129,0 +278,2 @@\n+  st->print_cr(\"UseCompressedClassPointers %d, UseCompactObjectHeaders %d\",\n+               UseCompressedClassPointers, UseCompactObjectHeaders);\n@@ -130,0 +281,2 @@\n+    st->print_cr(\"Narrow klass pointer bits %d, Max shift %d\",\n+                 _narrow_klass_pointer_bits, _max_shift);\n@@ -134,0 +287,2 @@\n+    st->print_cr(\"Klass ID Range:  [%u - %u) (%u)\", _lowest_valid_narrow_klass_id, _highest_valid_narrow_klass_id + 1,\n+                 _highest_valid_narrow_klass_id + 1 - _lowest_valid_narrow_klass_id);\n","filename":"src\/hotspot\/share\/oops\/compressedKlass.cpp","additions":191,"deletions":36,"binary":false,"changes":227,"status":"modified"},{"patch":"@@ -29,0 +29,1 @@\n+#include \"utilities\/align.hpp\"\n@@ -100,8 +101,0 @@\n-const int LogKlassAlignmentInBytes = 3;\n-const int KlassAlignmentInBytes    = 1 << LogKlassAlignmentInBytes;\n-\n-\/\/ Maximal size of compressed class space. Above this limit compression is not possible.\n-\/\/ Also upper bound for placement of zero based class space. (Class space is further limited\n-\/\/ to be < 3G, see arguments.cpp.)\n-const  uint64_t KlassEncodingMetaspaceMax = (uint64_t(max_juint) + 1) << LogKlassAlignmentInBytes;\n-\n@@ -113,0 +106,18 @@\n+  \/\/ We use a different narrow Klass pointer geometry depending on\n+  \/\/ whether we run in standard mode or in compact-object-header-mode.\n+\n+  \/\/ Narrow klass pointer bits for an unshifted narrow Klass pointer.\n+  static constexpr int narrow_klass_pointer_bits_noncoh = 32;\n+  static constexpr int narrow_klass_pointer_bits_coh = 22;\n+\n+  \/\/ Bit size of a narrowKlass\n+  static int _narrow_klass_pointer_bits;\n+\n+  \/\/ The maximum shift values we can use depending on UseCompactObjectHeaders\n+  static constexpr int max_shift_noncoh = 3;\n+  static constexpr int max_shift_coh = 10;\n+\n+  \/\/ Maximum shift usable\n+  static int _max_shift;\n+\n+  \/\/ Encoding Base, Encoding Shift\n@@ -117,1 +128,1 @@\n-  \/\/ Note: guaranteed to be aligned to KlassAlignmentInBytes\n+  \/\/ Note: guaranteed to be aligned to 1<<shift (klass_alignment_in_bytes)\n@@ -121,0 +132,6 @@\n+  \/\/ Values for the lowest (inclusive) and highest (inclusive) narrow Klass ID, given the\n+  \/\/ current Klass Range and encoding settings.\n+  static narrowKlass _lowest_valid_narrow_klass_id;\n+  static narrowKlass _highest_valid_narrow_klass_id;\n+\n+\n@@ -126,0 +143,1 @@\n+  static void calc_lowest_highest_narrow_klass_id();\n@@ -127,1 +145,3 @@\n-  DEBUG_ONLY(static void assert_is_valid_encoding(address addr, size_t len, address base, int shift);)\n+#ifdef ASSERT\n+  static void sanity_check_after_initialization();\n+#endif \/\/ ASSERT\n@@ -129,2 +149,4 @@\n-  static inline Klass* decode_not_null_without_asserts(narrowKlass v, address base, int shift);\n-  static inline Klass* decode_not_null(narrowKlass v, address base, int shift);\n+  template <typename T>\n+  static inline void check_init(T var) {\n+    assert(var != (T)-1, \"Not yet initialized\");\n+  }\n@@ -132,1 +154,1 @@\n-  static inline narrowKlass encode_not_null(Klass* v, address base, int shift);\n+  static inline Klass* decode_not_null_without_asserts(narrowKlass v, address base, int shift);\n@@ -136,0 +158,35 @@\n+  \/\/ Initialization sequence:\n+  \/\/ 1) Parse arguments. The following arguments take a role:\n+  \/\/      - UseCompressedClassPointers\n+  \/\/      - UseCompactObjectHeaders\n+  \/\/      - Xshare on off dump\n+  \/\/      - CompressedClassSpaceSize\n+  \/\/ 2) call pre_initialize(): depending on UseCompactObjectHeaders, defines the limits of narrow Klass pointer\n+  \/\/    geometry (how many bits, the max. possible shift)\n+  \/\/ 3) .. from here on, narrow_klass_pointer_bits() and max_shift() can be used\n+  \/\/ 4) call reserve_address_space_for_compressed_classes() either from CDS initialization or, if CDS is off,\n+  \/\/    from metaspace initialization. Reserves space for class space + CDS, attempts to reserve such that\n+  \/\/    we later can use a \"good\" encoding scheme. Reservation is highly CPU-specific.\n+  \/\/ 5) Initialize the narrow Klass encoding scheme by determining encoding base and shift:\n+  \/\/   5a) if CDS=on: Calls initialize_for_given_encoding() with the reservation base from step (4) and the\n+  \/\/       CDS-intrinsic setting for shift; here, we don't have any freedom to deviate from the base.\n+  \/\/   5b) if CDS=off: Calls initialize() - here, we have more freedom and, if we want, can choose an encoding\n+  \/\/       base that differs from the reservation base from step (4). That allows us, e.g., to later use\n+  \/\/       zero-based encoding.\n+  \/\/ 6) ... from now on, we can use base() and shift().\n+\n+  \/\/ Called right after argument parsing; defines narrow klass pointer geometry limits\n+  static void pre_initialize();\n+\n+  \/\/ The number of bits a narrow Klass pointer has;\n+  static int narrow_klass_pointer_bits() { check_init(_narrow_klass_pointer_bits); return _narrow_klass_pointer_bits; }\n+\n+  \/\/ The maximum possible shift; the actual shift employed later can be smaller (see initialize())\n+  static int max_shift()                 { check_init(_max_shift); return _max_shift; }\n+\n+  \/\/ Returns the maximum encoding range, given the current geometry (narrow klass bit size and shift)\n+  static size_t max_encoding_range_size() { return nth_bit(narrow_klass_pointer_bits() + max_shift()); }\n+\n+  \/\/ Returns the maximum allowed klass range size.\n+  static size_t max_klass_range_size();\n+\n@@ -155,2 +212,3 @@\n-  static address  base()               { return  _base; }\n-  static int      shift()              { return  _shift; }\n+  \/\/ Can only be used after initialization\n+  static address  base()             { check_init(_base); return  _base; }\n+  static int      shift()            { check_init(_shift); return  _shift; }\n@@ -163,0 +221,9 @@\n+  \/\/ Returns the alignment a Klass* is guaranteed to have.\n+  \/\/ Note: *Not* the same as 1 << shift ! Klass are always guaranteed to be at least 64-bit aligned,\n+  \/\/ so this will return 8 even if shift is 0.\n+  static int klass_alignment_in_bytes() { return nth_bit(MAX2(3, _shift)); }\n+  static int klass_alignment_in_words() { return klass_alignment_in_bytes() \/ BytesPerWord; }\n+\n+  \/\/ Returns the highest possible narrowKlass value given the current Klass range\n+  static narrowKlass highest_valid_narrow_klass_id() { return _highest_valid_narrow_klass_id; }\n+\n@@ -167,1 +234,0 @@\n-  static inline Klass* decode_not_null_without_asserts(narrowKlass v);\n@@ -169,1 +235,0 @@\n-\n@@ -173,0 +238,1 @@\n+  static inline narrowKlass encode_not_null_without_asserts(Klass* k, address narrow_base, int shift);\n@@ -176,0 +242,7 @@\n+#ifdef ASSERT\n+  \/\/ Given an address, check that it can be encoded with the current encoding\n+  inline static void check_encodable(const void* addr);\n+  \/\/ Given a narrow Klass ID, check that it is valid according to current encoding\n+  inline static void check_valid_narrow_klass_id(narrowKlass nk);\n+#endif\n+\n@@ -178,3 +251,8 @@\n-  static inline bool is_encodable(const void* p) {\n-    return (address) p >= _klass_range_start &&\n-           (address) p < _klass_range_end;\n+  static inline bool is_encodable(const void* addr) {\n+    \/\/ An address can only be encoded if:\n+    \/\/\n+    \/\/ 1) the address lies within the klass range.\n+    \/\/ 2) It is suitably aligned to 2^encoding_shift. This only really matters for\n+    \/\/    +UseCompactObjectHeaders, since the encoding shift can be large (max 10 bits -> 1KB).\n+    return (address)addr >= _klass_range_start && (address)addr < _klass_range_end &&\n+        is_aligned(addr, klass_alignment_in_bytes());\n@@ -182,0 +260,1 @@\n+\n","filename":"src\/hotspot\/share\/oops\/compressedKlass.hpp","additions":99,"deletions":20,"binary":false,"changes":119,"status":"modified"},{"patch":"@@ -35,2 +35,2 @@\n-static inline bool check_alignment(Klass* v) {\n-  return (intptr_t)v % KlassAlignmentInBytes == 0;\n+inline Klass* CompressedKlassPointers::decode_not_null_without_asserts(narrowKlass v, address narrow_base_base, int shift) {\n+  return (Klass*)((uintptr_t)narrow_base_base +((uintptr_t)v << shift));\n@@ -39,24 +39,2 @@\n-inline Klass* CompressedKlassPointers::decode_not_null_without_asserts(narrowKlass v, address narrow_base, int shift) {\n-  return (Klass*)((uintptr_t)narrow_base +((uintptr_t)v << shift));\n-}\n-\n-inline Klass* CompressedKlassPointers::decode_not_null(narrowKlass v, address narrow_base, int shift) {\n-  assert(!is_null(v), \"narrow klass value can never be zero\");\n-  Klass* result = decode_not_null_without_asserts(v, narrow_base, shift);\n-  assert(check_alignment(result), \"address not aligned: \" PTR_FORMAT, p2i(result));\n-  return result;\n-}\n-\n-inline narrowKlass CompressedKlassPointers::encode_not_null(Klass* v, address narrow_base, int shift) {\n-  assert(!is_null(v), \"klass value can never be zero\");\n-  assert(check_alignment(v), \"Address not aligned\");\n-  uint64_t pd = (uint64_t)(pointer_delta(v, narrow_base, 1));\n-  assert(KlassEncodingMetaspaceMax > pd, \"change encoding max if new encoding (Klass \" PTR_FORMAT \", Base \" PTR_FORMAT \")\", p2i(v), p2i(narrow_base));\n-  uint64_t result = pd >> shift;\n-  assert((result & CONST64(0xffffffff00000000)) == 0, \"narrow klass pointer overflow\");\n-  assert(decode_not_null((narrowKlass)result, narrow_base, shift) == v, \"reversibility\");\n-  return (narrowKlass)result;\n-}\n-\n-inline Klass* CompressedKlassPointers::decode_not_null_without_asserts(narrowKlass v) {\n-  return decode_not_null_without_asserts(v, base(), shift());\n+inline narrowKlass CompressedKlassPointers::encode_not_null_without_asserts(Klass* k, address narrow_base, int shift) {\n+  return (narrowKlass)(pointer_delta(k, narrow_base, 1) >> shift);\n@@ -66,1 +44,1 @@\n-  return is_null(v) ? nullptr : decode_not_null_without_asserts(v);\n+  return is_null(v) ? nullptr : decode_not_null_without_asserts(v, base(), shift());\n@@ -70,1 +48,5 @@\n-  return decode_not_null(v, base(), shift());\n+  assert(!is_null(v), \"narrow klass value can never be zero\");\n+  DEBUG_ONLY(check_valid_narrow_klass_id(v);)\n+  Klass* const k = decode_not_null_without_asserts(v, base(), shift());\n+  DEBUG_ONLY(check_encodable(k));\n+  return k;\n@@ -78,1 +60,6 @@\n-  return encode_not_null(v, base(), shift());\n+  assert(!is_null(v), \"klass value can never be zero\");\n+  DEBUG_ONLY(check_encodable(v);)\n+  const narrowKlass nk = encode_not_null_without_asserts(v, base(), shift());\n+  assert(decode_not_null_without_asserts(nk, base(), shift()) == v, \"reversibility\");\n+  DEBUG_ONLY(check_valid_narrow_klass_id(nk);)\n+  return nk;\n@@ -85,0 +72,20 @@\n+#ifdef ASSERT\n+inline void CompressedKlassPointers::check_encodable(const void* addr) {\n+  assert(UseCompressedClassPointers, \"Only call for +UseCCP\");\n+  assert(addr != nullptr, \"Null Klass?\");\n+  assert(is_encodable(addr),\n+         \"Address \" PTR_FORMAT \" is not encodable (Klass range: \" RANGEFMT \", klass alignment: %d)\",\n+         p2i(addr), RANGE2FMTARGS(_klass_range_start, _klass_range_end), klass_alignment_in_bytes());\n+}\n+\n+inline void CompressedKlassPointers::check_valid_narrow_klass_id(narrowKlass nk) {\n+  check_init(_base);\n+  assert(UseCompressedClassPointers, \"Only call for +UseCCP\");\n+  assert(nk > 0, \"narrow Klass ID is 0\");\n+  const uint64_t nk_mask = ~right_n_bits(narrow_klass_pointer_bits());\n+  assert(((uint64_t)nk & nk_mask) == 0, \"narrow klass id bit spillover (%u)\", nk);\n+  assert(nk >= _lowest_valid_narrow_klass_id &&\n+         nk <= _highest_valid_narrow_klass_id, \"narrowKlass ID out of range (%u)\", nk);\n+}\n+#endif \/\/ ASSERT\n+\n@@ -86,1 +93,1 @@\n-  const int max_bits = (sizeof(narrowKlass) * BitsPerByte) + _shift; \/\/ narrowKlass are 32 bit\n+  const int max_bits = narrow_klass_pointer_bits() + _shift;\n","filename":"src\/hotspot\/share\/oops\/compressedKlass.inline.hpp","additions":37,"deletions":30,"binary":false,"changes":67,"status":"modified"},{"patch":"@@ -457,1 +457,1 @@\n-  const bool use_class_space = !parser.is_interface() && !parser.is_abstract();\n+  const bool use_class_space = parser.klass_needs_narrow_id();\n@@ -477,0 +477,5 @@\n+  if (ik != nullptr && UseCompressedClassPointers && use_class_space) {\n+    assert(CompressedKlassPointers::is_encodable(ik),\n+           \"Klass \" PTR_FORMAT \"needs a narrow Klass ID, but is not encodable\", p2i(ik));\n+  }\n+\n","filename":"src\/hotspot\/share\/oops\/instanceKlass.cpp","additions":6,"deletions":1,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -35,11 +35,0 @@\n- public:\n-  \/\/ aligned header size.\n-  static int header_size() { return sizeof(instanceOopDesc)\/HeapWordSize; }\n-\n-  \/\/ If compressed, the offset of the fields of the instance may not be aligned.\n-  static int base_offset_in_bytes() {\n-    return (UseCompressedClassPointers) ?\n-            klass_gap_offset_in_bytes() :\n-            sizeof(instanceOopDesc);\n-\n-  }\n","filename":"src\/hotspot\/share\/oops\/instanceOop.hpp","additions":0,"deletions":11,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -46,0 +46,1 @@\n+#include \"oops\/compressedKlass.inline.hpp\"\n@@ -250,0 +251,17 @@\n+static markWord make_prototype(const Klass* kls) {\n+  markWord prototype = markWord::prototype();\n+#ifdef _LP64\n+  if (UseCompactObjectHeaders) {\n+    \/\/ With compact object headers, the narrow Klass ID is part of the mark word.\n+    \/\/ We therfore seed the mark word with the narrow Klass ID.\n+    \/\/ Note that only those Klass that can be instantiated have a narrow Klass ID.\n+    \/\/ For those who don't, we leave the klass bits empty and assert if someone\n+    \/\/ tries to use those.\n+    const narrowKlass nk = CompressedKlassPointers::is_encodable(kls) ?\n+        CompressedKlassPointers::encode(const_cast<Klass*>(kls)) : 0;\n+    prototype = prototype.set_narrow_klass(nk);\n+  }\n+#endif\n+  return prototype;\n+}\n+\n@@ -259,0 +277,1 @@\n+                           _prototype_header(make_prototype(this)),\n@@ -969,0 +988,4 @@\n+     if (UseCompactObjectHeaders) {\n+       st->print(BULLET\"prototype_header: \" INTPTR_FORMAT, _prototype_header.value());\n+       st->cr();\n+     }\n@@ -991,1 +1014,8 @@\n-  assert(Metaspace::contains((address)this), \"Should be\");\n+#ifdef ASSERT\n+  if (UseCompressedClassPointers && needs_narrow_id()) {\n+    \/\/ Stricter checks for both correct alignment and placement\n+    CompressedKlassPointers::check_encodable(this);\n+  } else {\n+    assert(Metaspace::contains((address)this), \"Should be\");\n+  }\n+#endif \/\/ ASSERT\n@@ -1019,0 +1049,2 @@\n+\/\/ Note: this function is called with an address that may or may not be a Klass.\n+\/\/ The point is not to assert it is but to check if it could be.\n","filename":"src\/hotspot\/share\/oops\/klass.cpp","additions":33,"deletions":1,"binary":false,"changes":34,"status":"modified"},{"patch":"@@ -163,0 +163,1 @@\n+  markWord _prototype_header;   \/\/ Used to initialize objects' header\n@@ -713,0 +714,4 @@\n+  inline markWord prototype_header() const;\n+  inline void set_prototype_header(markWord header);\n+  static ByteSize prototype_header_offset() { return in_ByteSize(offset_of(Klass, _prototype_header)); }\n+\n@@ -767,0 +772,4 @@\n+\n+  \/\/ Returns true if this Klass needs to be addressable via narrow Klass ID.\n+  inline bool needs_narrow_id() const;\n+\n","filename":"src\/hotspot\/share\/oops\/klass.hpp","additions":9,"deletions":0,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -54,0 +54,18 @@\n+inline markWord Klass::prototype_header() const {\n+  assert(UseCompactObjectHeaders, \"only use with compact object headers\");\n+#ifdef _LP64\n+  \/\/ You only need prototypes for allocating objects. If the class is not instantiable, it won't live in\n+  \/\/ class space and have no narrow Klass ID. But in that case we should not need the prototype.\n+  assert(_prototype_header.narrow_klass() > 0, \"Klass \" PTR_FORMAT \": invalid prototype (\" PTR_FORMAT \")\",\n+         p2i(this), _prototype_header.value());\n+#endif\n+  return _prototype_header;\n+}\n+\n+\/\/ This is only used when dumping the archive. In other cases,\n+\/\/ the _prototype_header is already initialized to the right thing.\n+inline void Klass::set_prototype_header(markWord header) {\n+  assert(UseCompactObjectHeaders, \"only with compact headers\");\n+  _prototype_header = header;\n+}\n+\n@@ -78,0 +96,10 @@\n+\/\/ Returns true if this Klass needs to be addressable via narrow Klass ID.\n+inline bool Klass::needs_narrow_id() const {\n+  \/\/ Classes that are never instantiated need no narrow Klass Id, since the\n+  \/\/ only point of having a narrow id is to put it into an object header. Keeping\n+  \/\/ never instantiated classes out of class space lessens the class space pressure.\n+  \/\/ For more details, see JDK-8338526.\n+  \/\/ Note: don't call this function before access flags are initialized.\n+  return !is_abstract() && !is_interface();\n+}\n+\n","filename":"src\/hotspot\/share\/oops\/klass.inline.hpp","additions":28,"deletions":0,"binary":false,"changes":28,"status":"modified"},{"patch":"@@ -32,0 +32,6 @@\n+#ifdef _LP64\n+STATIC_ASSERT(markWord::klass_shift + markWord::klass_bits == 64);\n+\/\/ The hash (preceding klass bits) shall be a direct neighbor but not interleave\n+STATIC_ASSERT(markWord::klass_shift == markWord::hash_bits + markWord::hash_shift);\n+#endif\n+\n","filename":"src\/hotspot\/share\/oops\/markWord.cpp","additions":6,"deletions":0,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -29,0 +29,1 @@\n+#include \"oops\/compressedKlass.hpp\"\n@@ -40,1 +41,1 @@\n-\/\/             hash:25 ------------>| age:4  unused_gap:1  lock:2 (normal object)\n+\/\/             hash:25 ------------>| age:4  self-fwd:1  lock:2 (normal object)\n@@ -44,1 +45,5 @@\n-\/\/  unused:25 hash:31 -->| unused_gap:1  age:4  unused_gap:1  lock:2 (normal object)\n+\/\/  unused:22 hash:31 -->| unused_gap:4  age:4  self-fwd:1  lock:2 (normal object)\n+\/\/\n+\/\/  64 bits (with compact headers):\n+\/\/  -------------------------------\n+\/\/  klass:22  hash:31 -->| unused_gap:4  age:4  self-fwd:1  lock:2 (normal object)\n@@ -107,2 +112,2 @@\n-  static const int first_unused_gap_bits          = 1;\n-  static const int max_hash_bits                  = BitsPerWord - age_bits - lock_bits - first_unused_gap_bits;\n+  static const int self_fwd_bits                  = 1;\n+  static const int max_hash_bits                  = BitsPerWord - age_bits - lock_bits - self_fwd_bits;\n@@ -110,1 +115,1 @@\n-  static const int second_unused_gap_bits         = LP64_ONLY(1) NOT_LP64(0);\n+  static const int unused_gap_bits                = LP64_ONLY(4) NOT_LP64(0); \/\/ Reserved for Valhalla.\n@@ -113,2 +118,3 @@\n-  static const int age_shift                      = lock_bits + first_unused_gap_bits;\n-  static const int hash_shift                     = age_shift + age_bits + second_unused_gap_bits;\n+  static const int self_fwd_shift                 = lock_shift + lock_bits;\n+  static const int age_shift                      = self_fwd_shift + self_fwd_bits;\n+  static const int hash_shift                     = age_shift + age_bits + unused_gap_bits;\n@@ -118,0 +124,2 @@\n+  static const uintptr_t self_fwd_mask            = right_n_bits(self_fwd_bits);\n+  static const uintptr_t self_fwd_mask_in_place   = self_fwd_mask << self_fwd_shift;\n@@ -123,0 +131,12 @@\n+#ifdef _LP64\n+  \/\/ Used only with compact headers:\n+  \/\/ We store the (narrow) Klass* in the bits 43 to 64.\n+\n+  \/\/ These are for bit-precise extraction of the narrow Klass* from the 64-bit Markword\n+  static constexpr int klass_shift                = hash_shift + hash_bits;\n+  static constexpr int klass_bits                 = 22;\n+  static constexpr uintptr_t klass_mask           = right_n_bits(klass_bits);\n+  static constexpr uintptr_t klass_mask_in_place  = klass_mask << klass_shift;\n+#endif\n+\n+\n@@ -147,2 +167,3 @@\n-  bool is_forwarded()   const {\n-    return (mask_bits(value(), lock_mask_in_place) == marked_value);\n+  bool is_forwarded() const {\n+    \/\/ Returns true for normal forwarded (0b011) and self-forwarded (0b1xx).\n+    return mask_bits(value(), lock_mask_in_place | self_fwd_mask_in_place) >= static_cast<intptr_t>(marked_value);\n@@ -263,0 +284,6 @@\n+  inline Klass* klass() const;\n+  inline Klass* klass_or_null() const;\n+  inline Klass* klass_without_asserts() const;\n+  inline narrowKlass narrow_klass() const;\n+  inline markWord set_narrow_klass(narrowKlass narrow_klass) const;\n+\n@@ -277,0 +304,15 @@\n+  inline bool is_self_forwarded() const {\n+    NOT_LP64(assert(LockingMode != LM_LEGACY, \"incorrect with LM_LEGACY on 32 bit\");)\n+    return mask_bits(value(), self_fwd_mask_in_place) != 0;\n+  }\n+\n+  inline markWord set_self_forwarded() const {\n+    NOT_LP64(assert(LockingMode != LM_LEGACY, \"incorrect with LM_LEGACY on 32 bit\");)\n+    return markWord(value() | self_fwd_mask_in_place);\n+  }\n+\n+  inline markWord unset_self_forwarded() const {\n+    NOT_LP64(assert(LockingMode != LM_LEGACY, \"incorrect with LM_LEGACY on 32 bit\");)\n+    return markWord(value() & ~self_fwd_mask_in_place);\n+  }\n+\n","filename":"src\/hotspot\/share\/oops\/markWord.hpp","additions":51,"deletions":9,"binary":false,"changes":60,"status":"modified"},{"patch":"@@ -0,0 +1,81 @@\n+\/*\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef SHARE_OOPS_MARKWORD_INLINE_HPP\n+#define SHARE_OOPS_MARKWORD_INLINE_HPP\n+\n+#include \"oops\/compressedOops.inline.hpp\"\n+#include \"oops\/markWord.hpp\"\n+\n+narrowKlass markWord::narrow_klass() const {\n+#ifdef _LP64\n+  assert(UseCompactObjectHeaders, \"only used with compact object headers\");\n+  return narrowKlass(value() >> klass_shift);\n+#else\n+  ShouldNotReachHere();\n+  return 0;\n+#endif\n+}\n+\n+markWord markWord::set_narrow_klass(narrowKlass narrow_klass) const {\n+#ifdef _LP64\n+  assert(UseCompactObjectHeaders, \"only used with compact object headers\");\n+  return markWord((value() & ~klass_mask_in_place) | ((uintptr_t) narrow_klass << klass_shift));\n+#else\n+  ShouldNotReachHere();\n+  return markWord(0);\n+#endif\n+}\n+\n+Klass* markWord::klass() const {\n+#ifdef _LP64\n+  assert(UseCompactObjectHeaders, \"only used with compact object headers\");\n+  return CompressedKlassPointers::decode_not_null(narrow_klass());\n+#else\n+  ShouldNotReachHere();\n+  return nullptr;\n+#endif\n+}\n+\n+Klass* markWord::klass_or_null() const {\n+#ifdef _LP64\n+  assert(UseCompactObjectHeaders, \"only used with compact object headers\");\n+  return CompressedKlassPointers::decode(narrow_klass());\n+#else\n+  ShouldNotReachHere();\n+  return nullptr;\n+#endif\n+}\n+\n+Klass* markWord::klass_without_asserts() const {\n+#ifdef _LP64\n+  assert(UseCompactObjectHeaders, \"only used with compact object headers\");\n+  return CompressedKlassPointers::decode_without_asserts(narrow_klass());\n+#else\n+  ShouldNotReachHere();\n+  return nullptr;\n+#endif\n+}\n+\n+#endif \/\/ SHARE_OOPS_MARKWORD_INLINE_HPP\n","filename":"src\/hotspot\/share\/oops\/markWord.inline.hpp","additions":81,"deletions":0,"binary":false,"changes":81,"status":"added"},{"patch":"@@ -146,1 +146,4 @@\n-  assert(obj->is_objArray(), \"must be object array\");\n+  \/\/ In this assert, we cannot safely access the Klass* with compact headers,\n+  \/\/ because size_given_klass() calls oop_size() on objects that might be\n+  \/\/ concurrently forwarded, which would overwrite the Klass*.\n+  assert(UseCompactObjectHeaders || obj->is_objArray(), \"must be object array\");\n","filename":"src\/hotspot\/share\/oops\/objArrayKlass.cpp","additions":4,"deletions":1,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -73,1 +73,1 @@\n-  assert (obj->is_array(), \"obj must be array\");\n+  assert(obj->is_array(), \"obj must be array\");\n","filename":"src\/hotspot\/share\/oops\/objArrayKlass.inline.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -156,2 +156,1 @@\n-  \/\/ Only has a klass gap when compressed class pointers are used.\n-  return UseCompressedClassPointers;\n+  return UseCompressedClassPointers && !UseCompactObjectHeaders;\n","filename":"src\/hotspot\/share\/oops\/oop.cpp","additions":1,"deletions":2,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -65,0 +65,2 @@\n+  inline oop cas_set_forwardee(markWord new_mark, markWord old_mark, atomic_memory_order order);\n+\n@@ -81,0 +83,3 @@\n+  \/\/ Returns the prototype mark that should be used for this object.\n+  inline markWord prototype_mark() const;\n+\n@@ -98,2 +103,8 @@\n-  \/\/ size of object header, aligned to platform wordSize\n-  static constexpr int header_size() { return sizeof(oopDesc)\/HeapWordSize; }\n+  \/\/ Size of object header, aligned to platform wordSize\n+  static int header_size() {\n+    if (UseCompactObjectHeaders) {\n+      return sizeof(markWord) \/ HeapWordSize;\n+    } else {\n+      return sizeof(oopDesc)  \/ HeapWordSize;\n+    }\n+  }\n@@ -261,0 +272,1 @@\n+  inline bool is_self_forwarded() const;\n@@ -263,0 +275,1 @@\n+  inline void forward_to_self();\n@@ -269,0 +282,1 @@\n+  inline oop forward_to_self_atomic(markWord compare, atomic_memory_order order = memory_order_conservative);\n@@ -271,0 +285,3 @@\n+  inline oop forwardee(markWord header) const;\n+\n+  inline void unset_self_forwarded();\n@@ -314,1 +331,16 @@\n-  static int klass_offset_in_bytes()     { return (int)offset_of(oopDesc, _metadata._klass); }\n+  static int klass_offset_in_bytes()     {\n+#ifdef _LP64\n+    if (UseCompactObjectHeaders) {\n+      \/\/ NOTE: The only places where this is used with compact headers are the C2\n+      \/\/ compiler and JVMCI, and even there we don't use it to access the (narrow)Klass*\n+      \/\/ directly. It is used only as a placeholder to identify the special memory slice\n+      \/\/ containing Klass* info. This value could be any value that is not a valid\n+      \/\/ field offset. Use an offset halfway into the markWord, as the markWord is never\n+      \/\/ partially loaded from C2 and JVMCI.\n+      return mark_offset_in_bytes() + 4;\n+    } else\n+#endif\n+    {\n+      return (int)offset_of(oopDesc, _metadata._klass);\n+    }\n+  }\n@@ -320,0 +352,12 @@\n+  static int base_offset_in_bytes() {\n+    if (UseCompactObjectHeaders) {\n+      \/\/ With compact headers, the Klass* field is not used for the Klass*\n+      \/\/ and is used for the object fields instead.\n+      return sizeof(markWord);\n+    } else if (UseCompressedClassPointers) {\n+      return sizeof(markWord) + sizeof(narrowKlass);\n+    } else {\n+      return sizeof(markWord) + sizeof(Klass*);\n+    }\n+  }\n+\n","filename":"src\/hotspot\/share\/oops\/oop.hpp","additions":47,"deletions":3,"binary":false,"changes":50,"status":"modified"},{"patch":"@@ -37,1 +37,1 @@\n-#include \"oops\/markWord.hpp\"\n+#include \"oops\/markWord.inline.hpp\"\n@@ -85,0 +85,8 @@\n+markWord oopDesc::prototype_mark() const {\n+  if (UseCompactObjectHeaders) {\n+    return klass()->prototype_header();\n+  } else {\n+    return markWord::prototype();\n+  }\n+}\n+\n@@ -86,1 +94,1 @@\n-  set_mark(markWord::prototype());\n+  set_mark(prototype_mark());\n@@ -90,2 +98,4 @@\n-  if (UseCompressedClassPointers) {\n-    return CompressedKlassPointers::decode_not_null(_metadata._compressed_klass);\n+  if (UseCompactObjectHeaders) {\n+    return mark().klass();\n+  } else if (UseCompressedClassPointers) {\n+     return CompressedKlassPointers::decode_not_null(_metadata._compressed_klass);\n@@ -98,1 +108,3 @@\n-  if (UseCompressedClassPointers) {\n+  if (UseCompactObjectHeaders) {\n+    return mark().klass_or_null();\n+  } else if (UseCompressedClassPointers) {\n@@ -106,3 +118,5 @@\n-  if (UseCompressedClassPointers) {\n-    narrowKlass nklass = Atomic::load_acquire(&_metadata._compressed_klass);\n-    return CompressedKlassPointers::decode(nklass);\n+  if (UseCompactObjectHeaders) {\n+    return mark_acquire().klass();\n+  } else if (UseCompressedClassPointers) {\n+    narrowKlass narrow_klass = Atomic::load_acquire(&_metadata._compressed_klass);\n+    return CompressedKlassPointers::decode(narrow_klass);\n@@ -115,1 +129,3 @@\n-  if (UseCompressedClassPointers) {\n+  if (UseCompactObjectHeaders) {\n+    return mark().klass_without_asserts();\n+  } else if (UseCompressedClassPointers) {\n@@ -124,0 +140,1 @@\n+  assert(!UseCompactObjectHeaders, \"don't set Klass* with compact headers\");\n@@ -133,0 +150,1 @@\n+  assert(!UseCompactObjectHeaders, \"don't set Klass* with compact headers\");\n@@ -143,3 +161,2 @@\n-  if (UseCompressedClassPointers) {\n-    *(int*)(((char*)mem) + klass_gap_offset_in_bytes()) = v;\n-  }\n+  assert(has_klass_gap(), \"precondition\");\n+  *(int*)(((char*)mem) + klass_gap_offset_in_bytes()) = v;\n@@ -270,0 +287,4 @@\n+bool oopDesc::is_self_forwarded() const {\n+  return mark().is_self_forwarded();\n+}\n+\n@@ -272,0 +293,2 @@\n+  assert(cast_from_oop<oopDesc*>(p) != this,\n+         \"must not be used for self-forwarding, use forward_to_self() instead\");\n@@ -277,4 +300,6 @@\n-oop oopDesc::forward_to_atomic(oop p, markWord compare, atomic_memory_order order) {\n-  markWord m = markWord::encode_pointer_as_mark(p);\n-  assert(m.decode_pointer() == p, \"encoding must be reversible\");\n-  markWord old_mark = cas_set_mark(m, compare, order);\n+void oopDesc::forward_to_self() {\n+  set_mark(mark().set_self_forwarded());\n+}\n+\n+oop oopDesc::cas_set_forwardee(markWord new_mark, markWord compare, atomic_memory_order order) {\n+  markWord old_mark = cas_set_mark(new_mark, compare, order);\n@@ -284,1 +309,25 @@\n-    return cast_to_oop(old_mark.decode_pointer());\n+    assert(old_mark.is_forwarded(), \"must be forwarded here\");\n+    return forwardee(old_mark);\n+  }\n+}\n+\n+oop oopDesc::forward_to_atomic(oop p, markWord compare, atomic_memory_order order) {\n+  assert(cast_from_oop<oopDesc*>(p) != this,\n+         \"must not be used for self-forwarding, use forward_to_self_atomic() instead\");\n+  markWord m = markWord::encode_pointer_as_mark(p);\n+  assert(forwardee(m) == p, \"encoding must be reversible\");\n+  return cas_set_forwardee(m, compare, order);\n+}\n+\n+oop oopDesc::forward_to_self_atomic(markWord old_mark, atomic_memory_order order) {\n+  markWord new_mark = old_mark.set_self_forwarded();\n+  assert(forwardee(new_mark) == cast_to_oop(this), \"encoding must be reversible\");\n+  return cas_set_forwardee(new_mark, old_mark, order);\n+}\n+\n+oop oopDesc::forwardee(markWord mark) const {\n+  assert(mark.is_forwarded(), \"only decode when actually forwarded\");\n+  if (mark.is_self_forwarded()) {\n+    return cast_to_oop(this);\n+  } else {\n+    return mark.forwardee();\n@@ -292,1 +341,5 @@\n-  return mark().forwardee();\n+  return forwardee(mark());\n+}\n+\n+void oopDesc::unset_self_forwarded() {\n+  set_mark(mark().unset_self_forwarded());\n@@ -349,0 +402,1 @@\n+  \/\/ In this assert, we cannot safely access the Klass* with compact headers.\n","filename":"src\/hotspot\/share\/oops\/oop.inline.hpp","additions":72,"deletions":18,"binary":false,"changes":90,"status":"modified"},{"patch":"@@ -174,1 +174,2 @@\n-  assert(obj->is_typeArray(),\"must be a type array\");\n+  \/\/ In this assert, we cannot safely access the Klass* with compact headers.\n+  assert(UseCompactObjectHeaders || obj->is_typeArray(),\"must be a type array\");\n","filename":"src\/hotspot\/share\/oops\/typeArrayKlass.cpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -1618,2 +1618,8 @@\n-  \/\/ For now only enable fast locking for non-array types\n-  mark_node = phase->MakeConX(markWord::prototype().value());\n+  if (UseCompactObjectHeaders) {\n+    Node* klass_node = in(AllocateNode::KlassNode);\n+    Node* proto_adr = phase->transform(new AddPNode(klass_node, klass_node, phase->MakeConX(in_bytes(Klass::prototype_header_offset()))));\n+    mark_node = LoadNode::make(*phase, control, mem, proto_adr, TypeRawPtr::BOTTOM, TypeX_X, TypeX_X->basic_type(), MemNode::unordered);\n+  } else {\n+    \/\/ For now only enable fast locking for non-array types\n+    mark_node = phase->MakeConX(markWord::prototype().value());\n+  }\n","filename":"src\/hotspot\/share\/opto\/callnode.cpp","additions":8,"deletions":2,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -1704,0 +1704,4 @@\n+      if (UseCompactObjectHeaders) {\n+        if (flat->offset() == in_bytes(Klass::prototype_header_offset()))\n+          alias_type(idx)->set_rewritable(false);\n+      }\n","filename":"src\/hotspot\/share\/opto\/compile.cpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -279,2 +279,2 @@\n-        if ((UseCompressedOops || UseCompressedClassPointers) &&\n-            (CompressedOops::shift() == 0 || CompressedKlassPointers::shift() == 0)) {\n+        if ((UseCompressedOops && CompressedOops::shift() == 0) ||\n+            (UseCompressedClassPointers && CompressedKlassPointers::shift() == 0)) {\n","filename":"src\/hotspot\/share\/opto\/lcm.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -1288,1 +1288,0 @@\n-    assert(arrayOopDesc::base_offset_in_bytes(T_BYTE) >= 16, \"Needed for indexOf\");\n","filename":"src\/hotspot\/share\/opto\/library_call.cpp","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -1709,1 +1709,3 @@\n-  rawmem = make_store(control, rawmem, object, oopDesc::klass_offset_in_bytes(), klass_node, T_METADATA);\n+  if (!UseCompactObjectHeaders) {\n+    rawmem = make_store(control, rawmem, object, oopDesc::klass_offset_in_bytes(), klass_node, T_METADATA);\n+  }\n","filename":"src\/hotspot\/share\/opto\/macro.cpp","additions":3,"deletions":1,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -1972,0 +1972,2 @@\n+  assert(!UseCompactObjectHeaders || tkls->offset() != in_bytes(Klass::prototype_header_offset()),\n+         \"must not happen\");\n@@ -2150,0 +2152,7 @@\n+      if (UseCompactObjectHeaders) {\n+        if (tkls->offset() == in_bytes(Klass::prototype_header_offset())) {\n+          \/\/ The field is Klass::_prototype_header. Return its (constant) value.\n+          assert(this->Opcode() == Op_LoadX, \"must load a proper type from _prototype_header\");\n+          return TypeX::make(klass->prototype_header());\n+        }\n+      }\n@@ -2239,3 +2248,5 @@\n-  Node* alloc = is_new_object_mark_load();\n-  if (alloc != nullptr) {\n-    return TypeX::make(markWord::prototype().value());\n+  if (!UseCompactObjectHeaders) {\n+    Node* alloc = is_new_object_mark_load();\n+    if (alloc != nullptr) {\n+      return TypeX::make(markWord::prototype().value());\n+    }\n","filename":"src\/hotspot\/share\/opto\/memnode.cpp","additions":14,"deletions":3,"binary":false,"changes":17,"status":"modified"},{"patch":"@@ -1428,7 +1428,0 @@\n-void Arguments::set_use_compressed_klass_ptrs() {\n-#ifdef _LP64\n-  assert(!UseCompressedClassPointers || CompressedClassSpaceSize <= KlassEncodingMetaspaceMax,\n-         \"CompressedClassSpaceSize is too large for UseCompressedClassPointers\");\n-#endif \/\/ _LP64\n-}\n-\n@@ -1453,1 +1446,0 @@\n-  set_use_compressed_klass_ptrs();\n@@ -1825,0 +1817,9 @@\n+#ifndef _LP64\n+  if (LockingMode == LM_LEGACY) {\n+    FLAG_SET_CMDLINE(LockingMode, LM_LIGHTWEIGHT);\n+    \/\/ Self-forwarding in bit 3 of the mark-word conflicts\n+    \/\/ with 4-byte-aligned stack-locks.\n+    warning(\"Legacy locking not supported on this platform\");\n+  }\n+#endif\n+\n@@ -3648,0 +3649,26 @@\n+#ifdef _LP64\n+  if (UseCompactObjectHeaders && FLAG_IS_CMDLINE(UseCompressedClassPointers) && !UseCompressedClassPointers) {\n+    warning(\"Compact object headers require compressed class pointers. Disabling compact object headers.\");\n+    FLAG_SET_DEFAULT(UseCompactObjectHeaders, false);\n+  }\n+  if (UseCompactObjectHeaders && LockingMode != LM_LIGHTWEIGHT) {\n+    FLAG_SET_DEFAULT(LockingMode, LM_LIGHTWEIGHT);\n+  }\n+  if (UseCompactObjectHeaders && !UseObjectMonitorTable) {\n+    \/\/ If UseCompactObjectHeaders is on the command line, turn on UseObjectMonitorTable.\n+    if (FLAG_IS_CMDLINE(UseCompactObjectHeaders)) {\n+      FLAG_SET_DEFAULT(UseObjectMonitorTable, true);\n+\n+    \/\/ If UseObjectMonitorTable is on the command line, turn off UseCompactObjectHeaders.\n+    } else if (FLAG_IS_CMDLINE(UseObjectMonitorTable)) {\n+      FLAG_SET_DEFAULT(UseCompactObjectHeaders, false);\n+    \/\/ If neither on the command line, the defaults are incompatible, but turn on UseObjectMonitorTable.\n+    } else {\n+      FLAG_SET_DEFAULT(UseObjectMonitorTable, true);\n+    }\n+  }\n+  if (UseCompactObjectHeaders && !UseCompressedClassPointers) {\n+    FLAG_SET_DEFAULT(UseCompressedClassPointers, true);\n+  }\n+#endif\n+\n@@ -3661,0 +3688,4 @@\n+  if (UseCompressedClassPointers) {\n+    CompressedKlassPointers::pre_initialize();\n+  }\n+\n","filename":"src\/hotspot\/share\/runtime\/arguments.cpp","additions":39,"deletions":8,"binary":false,"changes":47,"status":"modified"},{"patch":"@@ -264,1 +264,0 @@\n-  static void set_use_compressed_klass_ptrs();\n","filename":"src\/hotspot\/share\/runtime\/arguments.hpp","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -131,0 +131,3 @@\n+  product(bool, UseCompactObjectHeaders, false, EXPERIMENTAL,               \\\n+          \"Use compact 64-bit object headers in 64-bit VM\")                 \\\n+                                                                            \\\n@@ -147,0 +150,1 @@\n+const bool UseCompactObjectHeaders = false;\n","filename":"src\/hotspot\/share\/runtime\/globals.hpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -2008,2 +2008,0 @@\n-  declare_constant(LogKlassAlignmentInBytes)                              \\\n-                                                                          \\\n@@ -2514,0 +2512,1 @@\n+  LP64_ONLY(declare_constant(markWord::klass_shift))                      \\\n","filename":"src\/hotspot\/share\/runtime\/vmStructs.cpp","additions":1,"deletions":2,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -27,0 +27,3 @@\n+import sun.jvm.hotspot.oops.Mark;\n+import sun.jvm.hotspot.runtime.VM;\n+\n@@ -397,1 +400,9 @@\n-    long value = readCInteger(address, getKlassPtrSize(), true);\n+    long value;\n+    if (VM.getVM().isCompactObjectHeadersEnabled()) {\n+      \/\/ With compact headers, the compressed Klass* is currently read from the mark\n+      \/\/ word. We need to load the whole mark, and shift the upper parts.\n+      value = readCInteger(address, machDesc.getAddressSize(), true);\n+      value = value >>> Mark.getKlassShift();\n+    } else {\n+      value = readCInteger(address, getKlassPtrSize(), true);\n+    }\n","filename":"src\/jdk.hotspot.agent\/share\/classes\/sun\/jvm\/hotspot\/debugger\/DebuggerBase.java","additions":12,"deletions":1,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -84,1 +84,3 @@\n-    if (VM.getVM().isCompressedKlassPointersEnabled()) {\n+    if (VM.getVM().isCompactObjectHeadersEnabled()) {\n+      lengthOffsetInBytes = Oop.getHeaderSize();\n+    } else if (VM.getVM().isCompressedKlassPointersEnabled()) {\n","filename":"src\/jdk.hotspot.agent\/share\/classes\/sun\/jvm\/hotspot\/oops\/Array.java","additions":3,"deletions":1,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -58,1 +58,3 @@\n-    if (VM.getVM().isCompressedKlassPointersEnabled()) {\n+    if (VM.getVM().isCompactObjectHeadersEnabled()) {\n+      return Oop.getHeaderSize();\n+    } else if (VM.getVM().isCompressedKlassPointersEnabled()) {\n","filename":"src\/jdk.hotspot.agent\/share\/classes\/sun\/jvm\/hotspot\/oops\/Instance.java","additions":3,"deletions":1,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -57,0 +57,3 @@\n+    if (VM.getVM().isLP64()) {\n+      klassShift          = db.lookupLongConstant(\"markWord::klass_shift\").longValue();\n+    }\n@@ -85,0 +88,1 @@\n+  private static long klassShift;\n@@ -105,0 +109,4 @@\n+  public static long getKlassShift() {\n+    return klassShift;\n+  }\n+\n@@ -194,0 +202,5 @@\n+  public Klass getKlass() {\n+    assert(VM.getVM().isCompactObjectHeadersEnabled());\n+    return (Klass)Metadata.instantiateWrapperFor(addr.getCompKlassAddressAt(0));\n+  }\n+\n","filename":"src\/jdk.hotspot.agent\/share\/classes\/sun\/jvm\/hotspot\/oops\/Mark.java","additions":13,"deletions":0,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -49,3 +49,8 @@\n-    klass      = new MetadataField(type.getAddressField(\"_metadata._klass\"), 0);\n-    compressedKlass  = new NarrowKlassField(type.getAddressField(\"_metadata._compressed_klass\"), 0);\n-    headerSize = type.getSize();\n+    if (VM.getVM().isCompactObjectHeadersEnabled()) {\n+      Type markType = db.lookupType(\"markWord\");\n+      headerSize = markType.getSize();\n+    } else {\n+      headerSize = type.getSize();\n+      klass      = new MetadataField(type.getAddressField(\"_metadata._klass\"), 0);\n+      compressedKlass  = new NarrowKlassField(type.getAddressField(\"_metadata._compressed_klass\"), 0);\n+    }\n@@ -78,0 +83,1 @@\n+\n@@ -79,1 +85,4 @@\n-    if (VM.getVM().isCompressedKlassPointersEnabled()) {\n+    if (VM.getVM().isCompactObjectHeadersEnabled()) {\n+      assert(VM.getVM().isCompressedKlassPointersEnabled());\n+      return getMark().getKlass();\n+    } else if (VM.getVM().isCompressedKlassPointersEnabled()) {\n@@ -150,4 +159,6 @@\n-      if (VM.getVM().isCompressedKlassPointersEnabled()) {\n-        visitor.doMetadata(compressedKlass, true);\n-      } else {\n-        visitor.doMetadata(klass, true);\n+      if (!VM.getVM().isCompactObjectHeadersEnabled()) {\n+        if (VM.getVM().isCompressedKlassPointersEnabled()) {\n+          visitor.doMetadata(compressedKlass, true);\n+        } else {\n+          visitor.doMetadata(klass, true);\n+        }\n@@ -209,1 +220,4 @@\n-    if (VM.getVM().isCompressedKlassPointersEnabled()) {\n+    if (VM.getVM().isCompactObjectHeadersEnabled()) {\n+      Mark mark = new Mark(handle);\n+      return mark.getKlass();\n+    } else if (VM.getVM().isCompressedKlassPointersEnabled()) {\n","filename":"src\/jdk.hotspot.agent\/share\/classes\/sun\/jvm\/hotspot\/oops\/Oop.java","additions":23,"deletions":9,"binary":false,"changes":32,"status":"modified"},{"patch":"@@ -151,0 +151,1 @@\n+  private Boolean compactObjectHeadersEnabled;\n@@ -963,0 +964,9 @@\n+  public boolean isCompactObjectHeadersEnabled() {\n+    if (compactObjectHeadersEnabled == null) {\n+        Flag flag = getCommandLineFlag(\"UseCompactObjectHeaders\");\n+        compactObjectHeadersEnabled = (flag == null) ? Boolean.FALSE:\n+             (flag.getBool()? Boolean.TRUE: Boolean.FALSE);\n+    }\n+    return compactObjectHeadersEnabled.booleanValue();\n+  }\n+\n","filename":"src\/jdk.hotspot.agent\/share\/classes\/sun\/jvm\/hotspot\/runtime\/VM.java","additions":10,"deletions":0,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -29,0 +29,1 @@\n+import sun.jvm.hotspot.oops.Oop;\n@@ -40,20 +41,0 @@\n-  private static AddressField klassField;\n-\n-  static {\n-    VM.registerVMInitializedObserver(new Observer() {\n-        public void update(Observable o, Object data) {\n-          initialize(VM.getVM().getTypeDataBase());\n-        }\n-      });\n-  }\n-\n-  private static void initialize(TypeDataBase db) {\n-    Type type = db.lookupType(\"oopDesc\");\n-\n-    if (VM.getVM().isCompressedKlassPointersEnabled()) {\n-      klassField = type.getAddressField(\"_metadata._compressed_klass\");\n-    } else {\n-      klassField = type.getAddressField(\"_metadata._klass\");\n-    }\n-  }\n-\n@@ -69,5 +50,1 @@\n-      if (VM.getVM().isCompressedKlassPointersEnabled()) {\n-        Metadata.instantiateWrapperFor(oop.getCompKlassAddressAt(klassField.getOffset()));\n-      } else {\n-        Metadata.instantiateWrapperFor(klassField.getValue(oop));\n-      }\n+      Oop.getKlassForOopHandle(oop);\n","filename":"src\/jdk.hotspot.agent\/share\/classes\/sun\/jvm\/hotspot\/utilities\/RobustOopDeterminator.java","additions":2,"deletions":25,"binary":false,"changes":27,"status":"modified"},{"patch":"@@ -72,1 +72,1 @@\n-    final int hubOffset = getFieldOffset(\"oopDesc::_metadata._klass\", Integer.class, \"Klass*\");\n+    final int klassOffsetInBytes = getFieldValue(\"CompilerToVM::Data::oopDesc_klass_offset_in_bytes\", Integer.class, \"int\");\n","filename":"src\/jdk.internal.vm.ci\/share\/classes\/jdk\/vm\/ci\/hotspot\/HotSpotVMConfig.java","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -194,1 +194,5 @@\n-        return runtime().compilerToVm.getResolvedJavaType(object, runtime().getConfig().hubOffset, false);\n+        \/\/ HotSpot tests if the offset is oopDesc::klass_offset_in_bytes() and returns\n+        \/\/ the object type accordingly. When UseCompactClassPointers is enabled,\n+        \/\/ oopDesc::klass_offset_in_bytes() will return an offset halfway into the\n+        \/\/ object's markWord as a placeholder referring to the klass pointer.\n+        return runtime().compilerToVm.getResolvedJavaType(object, runtime().getConfig().klassOffsetInBytes, false);\n","filename":"src\/jdk.internal.vm.ci\/share\/classes\/jdk\/vm\/ci\/hotspot\/SharedLibraryJVMCIReflection.java","additions":5,"deletions":1,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -26,0 +26,1 @@\n+#include \"gc\/shared\/fullGCForwarding.inline.hpp\"\n@@ -29,19 +30,2 @@\n-\/\/ Class to create a \"fake\" oop with a mark that will\n-\/\/ return true for calls to must_be_preserved().\n-class FakeOop {\n-  oopDesc _oop;\n-\n-public:\n-  FakeOop() : _oop() { _oop.set_mark(originalMark()); }\n-\n-  oop get_oop() { return &_oop; }\n-  markWord mark() { return _oop.mark(); }\n-  void set_mark(markWord m) { _oop.set_mark(m); }\n-  void forward_to(oop obj) {\n-    markWord m = markWord::encode_pointer_as_mark(obj);\n-    _oop.set_mark(m);\n-  }\n-\n-  static markWord originalMark() { return markWord(markWord::lock_mask_in_place); }\n-  static markWord changedMark()  { return markWord(0x4711); }\n-};\n+static markWord originalMark() { return markWord(markWord::lock_mask_in_place); }\n+static markWord changedMark()  { return markWord(0x4711); }\n@@ -53,4 +37,9 @@\n-  FakeOop o1;\n-  FakeOop o2;\n-  FakeOop o3;\n-  FakeOop o4;\n+\n+  HeapWord fakeheap[32] = { nullptr };\n+  HeapWord* heap = align_up(fakeheap, 8 * sizeof(HeapWord));\n+  FullGCForwarding::initialize(MemRegion(&heap[0], &heap[16]));\n+\n+  oop o1 = cast_to_oop(&heap[0]); o1->set_mark(originalMark());\n+  oop o2 = cast_to_oop(&heap[2]); o2->set_mark(originalMark());\n+  oop o3 = cast_to_oop(&heap[4]); o3->set_mark(originalMark());\n+  oop o4 = cast_to_oop(&heap[6]); o4->set_mark(originalMark());\n@@ -59,4 +48,4 @@\n-  ASSERT_MARK_WORD_EQ(o1.mark(), FakeOop::originalMark());\n-  ASSERT_MARK_WORD_EQ(o2.mark(), FakeOop::originalMark());\n-  ASSERT_MARK_WORD_EQ(o3.mark(), FakeOop::originalMark());\n-  ASSERT_MARK_WORD_EQ(o4.mark(), FakeOop::originalMark());\n+  ASSERT_MARK_WORD_EQ(o1->mark(), originalMark());\n+  ASSERT_MARK_WORD_EQ(o2->mark(), originalMark());\n+  ASSERT_MARK_WORD_EQ(o3->mark(), originalMark());\n+  ASSERT_MARK_WORD_EQ(o4->mark(), originalMark());\n@@ -65,4 +54,4 @@\n-  o1.set_mark(FakeOop::changedMark());\n-  o2.set_mark(FakeOop::changedMark());\n-  ASSERT_MARK_WORD_EQ(o1.mark(), FakeOop::changedMark());\n-  ASSERT_MARK_WORD_EQ(o2.mark(), FakeOop::changedMark());\n+  o1->set_mark(changedMark());\n+  o2->set_mark(changedMark());\n+  ASSERT_MARK_WORD_EQ(o1->mark(), changedMark());\n+  ASSERT_MARK_WORD_EQ(o2->mark(), changedMark());\n@@ -71,2 +60,2 @@\n-  pm.push_if_necessary(o1.get_oop(), o1.mark());\n-  pm.push_if_necessary(o2.get_oop(), o2.mark());\n+  pm.push_if_necessary(o1, o1->mark());\n+  pm.push_if_necessary(o2, o2->mark());\n@@ -75,4 +64,4 @@\n-  o1.forward_to(o3.get_oop());\n-  o2.forward_to(o4.get_oop());\n-  ASSERT_EQ(o1.get_oop()->forwardee(), o3.get_oop());\n-  ASSERT_EQ(o2.get_oop()->forwardee(), o4.get_oop());\n+  FullGCForwarding::forward_to(o1, o3);\n+  FullGCForwarding::forward_to(o2, o4);\n+  ASSERT_EQ(FullGCForwarding::forwardee(o1), o3);\n+  ASSERT_EQ(FullGCForwarding::forwardee(o2), o4);\n@@ -86,2 +75,2 @@\n-  ASSERT_MARK_WORD_EQ(o3.mark(), FakeOop::changedMark());\n-  ASSERT_MARK_WORD_EQ(o4.mark(), FakeOop::changedMark());\n+  ASSERT_MARK_WORD_EQ(o3->mark(), changedMark());\n+  ASSERT_MARK_WORD_EQ(o4->mark(), changedMark());\n","filename":"test\/hotspot\/gtest\/gc\/shared\/test_preservedMarks.cpp","additions":28,"deletions":39,"binary":false,"changes":67,"status":"modified"},{"patch":"@@ -40,0 +40,1 @@\n+  int _num_arenas_created;\n@@ -42,2 +43,5 @@\n-    metaspace::MetaspaceTestContext(\"gtest-metaspace-context\", commit_limit, reserve_limit)\n-  {}\n+    metaspace::MetaspaceTestContext(\"gtest-metaspace-context\", commit_limit, reserve_limit),\n+    _num_arenas_created(0) {}\n+\n+  int num_arenas_created() const { return _num_arenas_created; }\n+  void inc_num_arenas_created() { _num_arenas_created ++; }\n","filename":"test\/hotspot\/gtest\/metaspace\/metaspaceGtestContexts.hpp","additions":6,"deletions":2,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -29,0 +29,1 @@\n+#include \"memory\/metaspace\/metablock.hpp\"\n@@ -35,0 +36,1 @@\n+using metaspace::MetaBlock;\n@@ -46,0 +48,13 @@\n+template <int num_lists>\n+struct TestedBinList : public BinListImpl<num_lists> {\n+  typedef BinListImpl<num_lists> ListType;\n+  void add_block(MetaWord* p, size_t word_size) {\n+    ListType::add_block(MetaBlock(p, word_size));\n+  }\n+  MetaWord* remove_block(size_t requested_size, size_t* real_size) {\n+    MetaBlock result = ListType::remove_block(requested_size);\n+    (*real_size) = result.word_size();\n+    return result.base();\n+  }\n+};\n+\n@@ -209,3 +224,3 @@\n-TEST_VM(metaspace, BinList_basic_1)     { BinListBasicTest< BinListImpl<1> >::basic_test(); }\n-TEST_VM(metaspace, BinList_basic_8)     { BinListBasicTest< BinListImpl<8> >::basic_test(); }\n-TEST_VM(metaspace, BinList_basic_32)    { BinListBasicTest<BinList32>::basic_test(); }\n+TEST_VM(metaspace, BinList_basic_1)     { BinListBasicTest< TestedBinList<1> >::basic_test(); }\n+TEST_VM(metaspace, BinList_basic_8)     { BinListBasicTest< TestedBinList<8> >::basic_test(); }\n+TEST_VM(metaspace, BinList_basic_32)    { BinListBasicTest< TestedBinList<32> >::basic_test(); }\n@@ -213,3 +228,3 @@\n-TEST_VM(metaspace, BinList_basic_2_1)     { BinListBasicTest< BinListImpl<1> >::basic_test_2(); }\n-TEST_VM(metaspace, BinList_basic_2_8)     { BinListBasicTest< BinListImpl<8> >::basic_test_2(); }\n-TEST_VM(metaspace, BinList_basic_2_32)    { BinListBasicTest<BinList32>::basic_test_2(); }\n+TEST_VM(metaspace, BinList_basic_2_1)     { BinListBasicTest< TestedBinList<1> >::basic_test_2(); }\n+TEST_VM(metaspace, BinList_basic_2_8)     { BinListBasicTest< TestedBinList<8> >::basic_test_2(); }\n+TEST_VM(metaspace, BinList_basic_2_32)    { BinListBasicTest< TestedBinList<32> >::basic_test_2(); }\n@@ -217,3 +232,3 @@\n-TEST_VM(metaspace, BinList_basic_rand_1)     { BinListBasicTest< BinListImpl<1> >::random_test(); }\n-TEST_VM(metaspace, BinList_basic_rand_8)     { BinListBasicTest< BinListImpl<8> >::random_test(); }\n-TEST_VM(metaspace, BinList_basic_rand_32)    { BinListBasicTest<BinList32>::random_test(); }\n+TEST_VM(metaspace, BinList_basic_rand_1)     { BinListBasicTest< TestedBinList<1> >::random_test(); }\n+TEST_VM(metaspace, BinList_basic_rand_8)     { BinListBasicTest< TestedBinList<8> >::random_test(); }\n+TEST_VM(metaspace, BinList_basic_rand_32)    { BinListBasicTest< TestedBinList<32> >::random_test(); }\n","filename":"test\/hotspot\/gtest\/metaspace\/test_binlist.cpp","additions":24,"deletions":9,"binary":false,"changes":33,"status":"modified"},{"patch":"@@ -29,0 +29,1 @@\n+#include \"memory\/metaspace\/metablock.hpp\"\n@@ -35,0 +36,12 @@\n+using metaspace::MetaBlock;\n+\n+struct TestedBlockTree : public BlockTree {\n+  void add_block(MetaWord* p, size_t word_size) {\n+    BlockTree::add_block(MetaBlock(p, word_size));\n+  }\n+  MetaWord* remove_block(size_t requested_size, size_t* real_size) {\n+    MetaBlock result = BlockTree::remove_block(requested_size);\n+    (*real_size) = result.word_size();\n+    return result.base();\n+  }\n+};\n@@ -38,1 +51,1 @@\n-static void create_nodes(const size_t sizes[], FeederBuffer& fb, BlockTree& bt) {\n+static void create_nodes(const size_t sizes[], FeederBuffer& fb, TestedBlockTree& bt) {\n@@ -58,1 +71,1 @@\n-  BlockTree bt;\n+  TestedBlockTree bt;\n@@ -115,1 +128,1 @@\n-  BlockTree bt;\n+  TestedBlockTree bt;\n@@ -158,1 +171,1 @@\n-  BlockTree bt;\n+  TestedBlockTree bt;\n@@ -173,1 +186,1 @@\n-  BlockTree bt;\n+  TestedBlockTree bt;\n@@ -207,1 +220,1 @@\n-  BlockTree bt;\n+  TestedBlockTree bt;\n@@ -225,1 +238,1 @@\n-  BlockTree bt;\n+  TestedBlockTree bt;\n@@ -252,1 +265,1 @@\n-  BlockTree _bt[2];\n+  TestedBlockTree _bt[2];\n@@ -359,1 +372,1 @@\n-      BlockTree* bt = _bt + which;\n+      TestedBlockTree* bt = _bt + which;\n","filename":"test\/hotspot\/gtest\/metaspace\/test_blocktree.cpp","additions":22,"deletions":9,"binary":false,"changes":31,"status":"modified"},{"patch":"@@ -0,0 +1,409 @@\n+\/*\n+ * Copyright (c) 2024 Red Hat, Inc. All rights reserved.\n+ * Copyright (c) 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"memory\/classLoaderMetaspace.hpp\"\n+#include \"memory\/metaspace\/freeBlocks.hpp\"\n+#include \"memory\/metaspace\/metablock.inline.hpp\"\n+#include \"memory\/metaspace\/metaspaceArena.hpp\"\n+#include \"memory\/metaspace\/metaspaceSettings.hpp\"\n+#include \"memory\/metaspace\/metaspaceStatistics.hpp\"\n+#include \"memory\/metaspace.hpp\"\n+#include \"oops\/klass.hpp\"\n+#include \"runtime\/mutex.hpp\"\n+#include \"utilities\/debug.hpp\"\n+#include \"utilities\/align.hpp\"\n+#include \"utilities\/globalDefinitions.hpp\"\n+\n+#ifdef _LP64\n+\n+#define LOG_PLEASE\n+#include \"metaspaceGtestCommon.hpp\"\n+#include \"metaspaceGtestContexts.hpp\"\n+#include \"metaspaceGtestRangeHelpers.hpp\"\n+#include \"metaspaceGtestSparseArray.hpp\"\n+\n+#define HANDLE_FAILURE \\\n+  if (testing::Test::HasFailure()) { \\\n+    return; \\\n+  }\n+\n+namespace metaspace {\n+\n+class ClmsTester {\n+\n+  Mutex _lock;\n+  MetaspaceContext* _class_context;\n+  MetaspaceContext* _nonclass_context;\n+  ClassLoaderMetaspace* _clms;\n+  const size_t _klass_arena_alignment_words;\n+  unsigned _num_allocations;\n+\n+  struct Deltas {\n+    int num_chunks_delta;\n+    ssize_t used_words_delta;\n+    int num_freeblocks_delta;\n+    ssize_t freeblocks_words_delta;\n+  };\n+\n+  Deltas calc_deltas(const ArenaStats& before, const ArenaStats& after) {\n+    Deltas d;\n+    d.num_chunks_delta = after.totals()._num - before.totals()._num;\n+    d.used_words_delta = after.totals()._used_words - before.totals()._used_words;\n+    d.num_freeblocks_delta = (int)after._free_blocks_num - (int)before._free_blocks_num;\n+    d.freeblocks_words_delta = after._free_blocks_word_size - before._free_blocks_word_size;\n+    return d;\n+  }\n+\n+public:\n+\n+  ClmsTester(size_t klass_alignment_words, Metaspace::MetaspaceType space_type,\n+             MetaspaceContext* class_context, MetaspaceContext* nonclass_context)\n+  : _lock(Monitor::nosafepoint, \"CLMSTest_lock\"),\n+    _class_context(class_context), _nonclass_context(nonclass_context),\n+    _clms(nullptr), _klass_arena_alignment_words(klass_alignment_words), _num_allocations(0) {\n+    _clms = new ClassLoaderMetaspace(&_lock, space_type, nonclass_context, class_context, klass_alignment_words);\n+  }\n+\n+  ~ClmsTester() {\n+    delete _clms;\n+    EXPECT_EQ(_class_context->used_words(), (size_t)0);\n+    EXPECT_EQ(_nonclass_context->used_words(), (size_t)0);\n+  }\n+\n+  MetaBlock allocate_and_check(size_t word_size, bool is_class) {\n+\n+    \/\/ take stats before allocation\n+    ClmsStats stats_before;\n+    _clms->add_to_statistics(&stats_before);\n+\n+    \/\/ allocate\n+    MetaWord* p = _clms->allocate(word_size, is_class ? Metaspace::ClassType : Metaspace::NonClassType);\n+    _num_allocations ++;\n+\n+    \/\/ take stats after allocation\n+    ClmsStats stats_after;\n+    _clms->add_to_statistics(&stats_after);\n+\n+    \/\/ for less verbose testing:\n+    const ArenaStats& ca_before = stats_before._arena_stats_class;\n+    const ArenaStats& ca_after = stats_after._arena_stats_class;\n+    const ArenaStats& nca_before = stats_before._arena_stats_nonclass;\n+    const ArenaStats& nca_after = stats_after._arena_stats_nonclass;\n+\n+    \/\/ deltas\n+    const Deltas d_ca = calc_deltas(ca_before, ca_after);\n+    const Deltas d_nca = calc_deltas(nca_before, nca_after);\n+\n+#define EXPECT_FREEBLOCKS_UNCHANGED(arena_prefix) \\\n+    EXPECT_EQ(d_##arena_prefix.num_freeblocks_delta, 0);  \\\n+    EXPECT_EQ(d_##arena_prefix.freeblocks_words_delta, (ssize_t)0);\n+\n+#define EXPECT_ARENA_UNCHANGED(arena_prefix) \\\n+    EXPECT_EQ(d_##arena_prefix.num_chunks_delta, 0);  \\\n+    EXPECT_EQ(d_##arena_prefix.used_words_delta, (ssize_t)0);\n+\n+    if (p != nullptr) {\n+\n+      MetaBlock bl(p, word_size);\n+\n+      if (is_class) {\n+\n+        EXPECT_TRUE(bl.is_aligned_base(_klass_arena_alignment_words));\n+\n+        if (_num_allocations == 1) {\n+          \/\/ first allocation: nonclass arena unchanged, class arena grows by 1 chunk and wordsize,\n+          \/\/ class arena freeblocks unchanged\n+          EXPECT_ARENA_UNCHANGED(nca);\n+          EXPECT_FREEBLOCKS_UNCHANGED(nca);\n+          EXPECT_EQ(d_ca.num_chunks_delta, 1);\n+          EXPECT_EQ((size_t)d_ca.used_words_delta, word_size);\n+          EXPECT_FREEBLOCKS_UNCHANGED(ca);\n+          return bl;\n+        }\n+\n+        \/\/ Had this been taken from class arena freeblocks?\n+        if (d_ca.num_freeblocks_delta == -1) {\n+          \/\/ the class arena freeblocks should have gone down, and the non-class arena freeblocks may have gone\n+          \/\/ up in case the block was larger than required\n+          const size_t wordsize_block_taken = (size_t)(-d_ca.freeblocks_words_delta);\n+          EXPECT_GE(wordsize_block_taken, word_size); \/\/ the block we took must be at least allocation size\n+          const size_t expected_freeblock_remainder = wordsize_block_taken - word_size;\n+          if (expected_freeblock_remainder > 0) {\n+            \/\/ the remainder, if it existed, should have been added to nonclass freeblocks\n+            EXPECT_EQ(d_nca.num_freeblocks_delta, 1);\n+            EXPECT_EQ((size_t)d_nca.freeblocks_words_delta, expected_freeblock_remainder);\n+          }\n+          \/\/ finally, nothing should have happened in the arenas proper.\n+          EXPECT_ARENA_UNCHANGED(ca);\n+          EXPECT_ARENA_UNCHANGED(nca);\n+          return bl;\n+        }\n+\n+        \/\/ block was taken from class arena proper\n+\n+        \/\/ We expect allocation waste due to alignment, should have been added to the freeblocks\n+        \/\/ of nonclass arena. Allocation waste can be 0. If no chunk turnover happened, it must be\n+        \/\/ smaller than klass alignment, otherwise it can get as large as a commit granule.\n+        const size_t max_expected_allocation_waste =\n+            d_ca.num_chunks_delta == 0 ? (_klass_arena_alignment_words - 1) : Settings::commit_granule_words();\n+        EXPECT_GE(d_ca.num_chunks_delta, 0);\n+        EXPECT_LE(d_ca.num_chunks_delta, 1);\n+        EXPECT_GE((size_t)d_ca.used_words_delta, word_size);\n+        EXPECT_LE((size_t)d_ca.used_words_delta, word_size + max_expected_allocation_waste);\n+        EXPECT_FREEBLOCKS_UNCHANGED(ca);\n+        EXPECT_ARENA_UNCHANGED(nca);\n+        if (max_expected_allocation_waste > 0) {\n+          EXPECT_GE(d_nca.num_freeblocks_delta, 0);\n+          EXPECT_LE(d_nca.num_freeblocks_delta, 1);\n+          EXPECT_GE(d_nca.freeblocks_words_delta, 0);\n+          EXPECT_LE((size_t)d_nca.freeblocks_words_delta, max_expected_allocation_waste);\n+        } else {\n+          EXPECT_FREEBLOCKS_UNCHANGED(nca);\n+        }\n+        return bl;\n+        \/\/ end: is_class\n+      } else {\n+        \/\/ Nonclass arena allocation.\n+        \/\/ Allocation waste can happen:\n+        \/\/ - if we allocate from nonclass freeblocks, the block remainder\n+        \/\/ - if we allocate from arena proper, by chunk turnover\n+\n+        if (d_nca.freeblocks_words_delta < 0) {\n+          \/\/ We allocated a block from the nonclass arena freeblocks.\n+          const size_t wordsize_block_taken = (size_t)(-d_nca.freeblocks_words_delta);\n+          EXPECT_EQ(wordsize_block_taken, word_size);\n+          \/\/ The number of blocks may or may not have decreased (depending on whether there\n+          \/\/ was a wastage block)\n+          EXPECT_GE(d_nca.num_chunks_delta, -1);\n+          EXPECT_LE(d_nca.num_chunks_delta, 0);\n+          EXPECT_ARENA_UNCHANGED(nca);\n+          EXPECT_ARENA_UNCHANGED(ca);\n+          EXPECT_FREEBLOCKS_UNCHANGED(ca);\n+          return bl;\n+        }\n+\n+        \/\/ We don't expect alignment waste. Only wastage happens at chunk turnover.\n+        const size_t max_expected_allocation_waste =\n+            d_nca.num_chunks_delta == 0 ? 0 : Settings::commit_granule_words();\n+        EXPECT_ARENA_UNCHANGED(ca);\n+        EXPECT_FREEBLOCKS_UNCHANGED(ca);\n+        EXPECT_GE(d_nca.num_chunks_delta, 0);\n+        EXPECT_LE(d_nca.num_chunks_delta, 1);\n+        EXPECT_GE((size_t)d_nca.used_words_delta, word_size);\n+        EXPECT_LE((size_t)d_nca.used_words_delta, word_size + max_expected_allocation_waste);\n+        if (max_expected_allocation_waste == 0) {\n+          EXPECT_FREEBLOCKS_UNCHANGED(nca);\n+        }\n+      }\n+      return bl;\n+\n+    } \/\/ end: allocation successful\n+\n+    \/\/ allocation failed.\n+    EXPECT_ARENA_UNCHANGED(ca);\n+    EXPECT_FREEBLOCKS_UNCHANGED(ca);\n+    EXPECT_ARENA_UNCHANGED(nca);\n+    EXPECT_FREEBLOCKS_UNCHANGED(nca);\n+\n+    return MetaBlock();\n+  }\n+\n+  MetaBlock allocate_expect_success(size_t word_size, bool is_class) {\n+    MetaBlock bl = allocate_and_check(word_size, is_class);\n+    EXPECT_TRUE(bl.is_nonempty());\n+    return bl;\n+  }\n+\n+  MetaBlock allocate_expect_failure(size_t word_size, bool is_class) {\n+    MetaBlock bl = allocate_and_check(word_size, is_class);\n+    EXPECT_TRUE(bl.is_empty());\n+    return bl;\n+  }\n+\n+  void deallocate_and_check(MetaBlock bl) {\n+\n+    \/\/ take stats before deallocation\n+    ClmsStats stats_before;\n+    _clms->add_to_statistics(&stats_before);\n+\n+    \/\/ allocate\n+    _clms->deallocate(bl.base(), bl.word_size());\n+\n+    \/\/ take stats after deallocation\n+    ClmsStats stats_after;\n+    _clms->add_to_statistics(&stats_after);\n+\n+    \/\/ for less verbose testing:\n+    const ArenaStats& ca_before = stats_before._arena_stats_class;\n+    const ArenaStats& ca_after = stats_after._arena_stats_class;\n+    const ArenaStats& nca_before = stats_before._arena_stats_nonclass;\n+    const ArenaStats& nca_after = stats_after._arena_stats_nonclass;\n+\n+    \/\/ deltas\n+    \/\/ deltas\n+    const Deltas d_ca = calc_deltas(ca_before, ca_after);\n+    const Deltas d_nca = calc_deltas(nca_before, nca_after);\n+\n+    EXPECT_ARENA_UNCHANGED(ca);\n+    EXPECT_ARENA_UNCHANGED(nca);\n+    \/\/ Depending on whether the returned block was suitable for Klass,\n+    \/\/ it may have gone to either the non-class freelist or the class freelist\n+    if (d_ca.num_freeblocks_delta == 1) {\n+      EXPECT_EQ(d_ca.num_freeblocks_delta, 1);\n+      EXPECT_EQ((size_t)d_ca.freeblocks_words_delta, bl.word_size());\n+      EXPECT_FREEBLOCKS_UNCHANGED(nca);\n+    } else {\n+      EXPECT_EQ(d_nca.num_freeblocks_delta, 1);\n+      EXPECT_EQ((size_t)d_nca.freeblocks_words_delta, bl.word_size());\n+      EXPECT_FREEBLOCKS_UNCHANGED(ca);\n+    }\n+  }\n+};\n+\n+static constexpr size_t klass_size = sizeof(Klass) \/ BytesPerWord;\n+\n+static void basic_test(size_t klass_arena_alignment) {\n+  MetaspaceGtestContext class_context, nonclass_context;\n+  {\n+    ClmsTester tester(klass_arena_alignment, Metaspace::StandardMetaspaceType, class_context.context(), nonclass_context.context());\n+\n+    MetaBlock bl1 = tester.allocate_expect_success(klass_size, true);\n+    HANDLE_FAILURE;\n+\n+    MetaBlock bl2 = tester.allocate_expect_success(klass_size, true);\n+    HANDLE_FAILURE;\n+\n+    tester.deallocate_and_check(bl1);\n+    HANDLE_FAILURE;\n+\n+    MetaBlock bl3 = tester.allocate_expect_success(klass_size, true);\n+    HANDLE_FAILURE;\n+\n+    MetaBlock bl4 = tester.allocate_expect_success(Metaspace::min_allocation_word_size, false);\n+    HANDLE_FAILURE;\n+\n+    MetaBlock bl5 = tester.allocate_expect_success(K, false);\n+    HANDLE_FAILURE;\n+\n+    tester.deallocate_and_check(bl5);\n+    HANDLE_FAILURE;\n+\n+    MetaBlock bl6 = tester.allocate_expect_success(K, false);\n+    HANDLE_FAILURE;\n+\n+    EXPECT_EQ(bl5, bl6); \/\/ should have gotten the same block back from freelist\n+  }\n+  EXPECT_EQ(class_context.used_words(), (size_t)0);\n+  EXPECT_EQ(nonclass_context.used_words(), (size_t)0);\n+  \/\/ we should have used exactly one commit granule (64K), not more, for each context\n+  EXPECT_EQ(class_context.committed_words(), Settings::commit_granule_words());\n+  EXPECT_EQ(nonclass_context.committed_words(), Settings::commit_granule_words());\n+}\n+\n+#define TEST_BASIC_N(n)               \\\n+TEST_VM(metaspace, CLMS_basics_##n) { \\\n+  basic_test(n);            \\\n+}\n+\n+TEST_BASIC_N(1)\n+TEST_BASIC_N(4)\n+TEST_BASIC_N(16)\n+TEST_BASIC_N(32)\n+TEST_BASIC_N(128)\n+\n+static void test_random(size_t klass_arena_alignment) {\n+  MetaspaceGtestContext class_context, nonclass_context;\n+  constexpr int max_allocations = 1024;\n+  const SizeRange nonclass_alloc_range(Metaspace::min_allocation_alignment_words, 1024);\n+  const SizeRange class_alloc_range(klass_size, 1024);\n+  const IntRange one_out_of_ten(0, 10);\n+  for (int runs = 9; runs >= 0; runs--) {\n+    {\n+      ClmsTester tester(64, Metaspace::StandardMetaspaceType, class_context.context(), nonclass_context.context());\n+      struct LifeBlock {\n+        MetaBlock bl;\n+        bool is_class;\n+      };\n+      LifeBlock life_allocations[max_allocations];\n+      for (int i = 0; i < max_allocations; i++) {\n+        life_allocations[i].bl.reset();\n+      }\n+\n+      unsigned num_class_allocs = 0, num_nonclass_allocs = 0, num_class_deallocs = 0, num_nonclass_deallocs = 0;\n+      for (int i = 0; i < 5000; i ++) {\n+        const int slot = IntRange(0, max_allocations).random_value();\n+        if (life_allocations[slot].bl.is_empty()) {\n+          const bool is_class = one_out_of_ten.random_value() == 0;\n+          const size_t word_size =\n+              is_class ? class_alloc_range.random_value() : nonclass_alloc_range.random_value();\n+          MetaBlock bl = tester.allocate_expect_success(word_size, is_class);\n+          HANDLE_FAILURE;\n+          life_allocations[slot].bl = bl;\n+          life_allocations[slot].is_class = is_class;\n+          if (is_class) {\n+            num_class_allocs ++;\n+          } else {\n+            num_nonclass_allocs ++;\n+          }\n+        } else {\n+          tester.deallocate_and_check(life_allocations[slot].bl);\n+          HANDLE_FAILURE;\n+          life_allocations[slot].bl.reset();\n+          if (life_allocations[slot].is_class) {\n+            num_class_deallocs ++;\n+          } else {\n+            num_nonclass_deallocs ++;\n+          }\n+        }\n+      }\n+      LOG(\"num class allocs: %u, num nonclass allocs: %u, num class deallocs: %u, num nonclass deallocs: %u\",\n+          num_class_allocs, num_nonclass_allocs, num_class_deallocs, num_nonclass_deallocs);\n+    }\n+    EXPECT_EQ(class_context.used_words(), (size_t)0);\n+    EXPECT_EQ(nonclass_context.used_words(), (size_t)0);\n+    constexpr float fragmentation_factor = 3.0f;\n+    const size_t max_expected_nonclass_committed = max_allocations * nonclass_alloc_range.highest() * fragmentation_factor;\n+    const size_t max_expected_class_committed = max_allocations * class_alloc_range.highest() * fragmentation_factor;\n+    \/\/ we should have used exactly one commit granule (64K), not more, for each context\n+    EXPECT_LT(class_context.committed_words(), max_expected_class_committed);\n+    EXPECT_LT(nonclass_context.committed_words(), max_expected_nonclass_committed);\n+  }\n+}\n+\n+#define TEST_RANDOM_N(n)               \\\n+TEST_VM(metaspace, CLMS_random_##n) {  \\\n+  test_random(n);                      \\\n+}\n+\n+TEST_RANDOM_N(1)\n+TEST_RANDOM_N(4)\n+TEST_RANDOM_N(16)\n+TEST_RANDOM_N(32)\n+TEST_RANDOM_N(128)\n+\n+} \/\/ namespace metaspace\n+\n+#endif \/\/ _LP64\n","filename":"test\/hotspot\/gtest\/metaspace\/test_clms.cpp","additions":409,"deletions":0,"binary":false,"changes":409,"status":"added"},{"patch":"@@ -29,0 +29,2 @@\n+#include \"memory\/metaspace\/metablock.hpp\"\n+\n@@ -33,0 +35,1 @@\n+using metaspace::MetaBlock;\n@@ -46,166 +49,0 @@\n-class FreeBlocksTest {\n-\n-  FeederBuffer _fb;\n-  FreeBlocks _freeblocks;\n-\n-  \/\/ random generator for block feeding\n-  RandSizeGenerator _rgen_feeding;\n-\n-  \/\/ random generator for allocations (and, hence, deallocations)\n-  RandSizeGenerator _rgen_allocations;\n-\n-  SizeCounter _allocated_words;\n-\n-  struct allocation_t {\n-    allocation_t* next;\n-    size_t word_size;\n-    MetaWord* p;\n-  };\n-\n-  \/\/ Array of the same size as the pool max capacity; holds the allocated elements.\n-  allocation_t* _allocations;\n-\n-  int _num_allocs;\n-  int _num_deallocs;\n-  int _num_feeds;\n-\n-  bool feed_some() {\n-    size_t word_size = _rgen_feeding.get();\n-    MetaWord* p = _fb.get(word_size);\n-    if (p != nullptr) {\n-      _freeblocks.add_block(p, word_size);\n-      return true;\n-    }\n-    return false;\n-  }\n-\n-  bool deallocate_top() {\n-\n-    allocation_t* a = _allocations;\n-    if (a != nullptr) {\n-      _allocations = a->next;\n-      check_marked_range(a->p, a->word_size);\n-      _freeblocks.add_block(a->p, a->word_size);\n-      delete a;\n-      DEBUG_ONLY(_freeblocks.verify();)\n-      return true;\n-    }\n-    return false;\n-  }\n-\n-  void deallocate_all() {\n-    while (deallocate_top());\n-  }\n-\n-  bool allocate() {\n-\n-    size_t word_size = MAX2(_rgen_allocations.get(), _freeblocks.MinWordSize);\n-    MetaWord* p = _freeblocks.remove_block(word_size);\n-    if (p != nullptr) {\n-      _allocated_words.increment_by(word_size);\n-      allocation_t* a = new allocation_t;\n-      a->p = p; a->word_size = word_size;\n-      a->next = _allocations;\n-      _allocations = a;\n-      DEBUG_ONLY(_freeblocks.verify();)\n-      mark_range(p, word_size);\n-      return true;\n-    }\n-    return false;\n-  }\n-\n-  void test_all_marked_ranges() {\n-    for (allocation_t* a = _allocations; a != nullptr; a = a->next) {\n-      check_marked_range(a->p, a->word_size);\n-    }\n-  }\n-\n-  void test_loop() {\n-    \/\/ We loop and in each iteration execute one of three operations:\n-    \/\/ - allocation from fbl\n-    \/\/ - deallocation to fbl of a previously allocated block\n-    \/\/ - feeding a new larger block into the fbl (mimicks chunk retiring)\n-    \/\/ When we have fed all large blocks into the fbl (feedbuffer empty), we\n-    \/\/  switch to draining the fbl completely (only allocs)\n-    bool forcefeed = false;\n-    bool draining = false;\n-    bool stop = false;\n-    int iter = 25000; \/\/ safety stop\n-    while (!stop && iter > 0) {\n-      iter --;\n-      int surprise = (int)os::random() % 10;\n-      if (!draining && (surprise >= 7 || forcefeed)) {\n-        forcefeed = false;\n-        if (feed_some()) {\n-          _num_feeds++;\n-        } else {\n-          \/\/ We fed all input memory into the fbl. Now lets proceed until the fbl is drained.\n-          draining = true;\n-        }\n-      } else if (!draining && surprise < 1) {\n-        deallocate_top();\n-        _num_deallocs++;\n-      } else {\n-        if (allocate()) {\n-          _num_allocs++;\n-        } else {\n-          if (draining) {\n-            stop = _freeblocks.total_size() < 512;\n-          } else {\n-            forcefeed = true;\n-          }\n-        }\n-      }\n-      if ((iter % 1000) == 0) {\n-        DEBUG_ONLY(_freeblocks.verify();)\n-        test_all_marked_ranges();\n-        LOG(\"a %d (\" SIZE_FORMAT \"), d %d, f %d\", _num_allocs, _allocated_words.get(), _num_deallocs, _num_feeds);\n-#ifdef LOG_PLEASE\n-        _freeblocks.print(tty, true);\n-        tty->cr();\n-#endif\n-      }\n-    }\n-\n-    \/\/ Drain\n-\n-  }\n-\n-public:\n-\n-  FreeBlocksTest(size_t avg_alloc_size) :\n-    _fb(512 * K), _freeblocks(),\n-    _rgen_feeding(128, 4096),\n-    _rgen_allocations(avg_alloc_size \/ 4, avg_alloc_size * 2, 0.01f, avg_alloc_size \/ 3, avg_alloc_size * 30),\n-    _allocations(nullptr),\n-    _num_allocs(0),\n-    _num_deallocs(0),\n-    _num_feeds(0)\n-  {\n-    CHECK_CONTENT(_freeblocks, 0, 0);\n-    \/\/ some initial feeding\n-    _freeblocks.add_block(_fb.get(1024), 1024);\n-    CHECK_CONTENT(_freeblocks, 1, 1024);\n-  }\n-\n-  ~FreeBlocksTest() {\n-    deallocate_all();\n-  }\n-\n-  static void test_small_allocations() {\n-    FreeBlocksTest test(10);\n-    test.test_loop();\n-  }\n-\n-  static void test_medium_allocations() {\n-    FreeBlocksTest test(30);\n-    test.test_loop();\n-  }\n-\n-  static void test_large_allocations() {\n-    FreeBlocksTest test(150);\n-    test.test_loop();\n-  }\n-\n-};\n-\n@@ -218,1 +55,2 @@\n-  fbl.add_block(tmp, 1024);\n+  MetaBlock bl(tmp, 1024);\n+  fbl.add_block(bl);\n@@ -223,2 +61,2 @@\n-  MetaWord* p = fbl.remove_block(1024);\n-  EXPECT_EQ(p, tmp);\n+  MetaBlock bl2 = fbl.remove_block(1024);\n+  ASSERT_EQ(bl, bl2);\n@@ -229,13 +67,0 @@\n-\n-TEST_VM(metaspace, freeblocks_small) {\n-  FreeBlocksTest::test_small_allocations();\n-}\n-\n-TEST_VM(metaspace, freeblocks_medium) {\n-  FreeBlocksTest::test_medium_allocations();\n-}\n-\n-TEST_VM(metaspace, freeblocks_large) {\n-  FreeBlocksTest::test_large_allocations();\n-}\n-\n","filename":"test\/hotspot\/gtest\/metaspace\/test_freeblocks.cpp","additions":7,"deletions":182,"binary":false,"changes":189,"status":"modified"},{"patch":"@@ -0,0 +1,97 @@\n+\/*\n+ * Copyright (c) 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2020 SAP SE. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"memory\/metaspace\/metablock.inline.hpp\"\n+\/\/#define LOG_PLEASE\n+#include \"metaspaceGtestCommon.hpp\"\n+\n+using metaspace::MetaBlock;\n+\n+\n+#define CHECK_BLOCK_EMPTY(block) { \\\n+  EXPECT_TRUE(block.is_empty()); \\\n+  DEBUG_ONLY(block.verify()); \\\n+}\n+\n+#define CHECK_BLOCK(block, expected_base, expected_size) { \\\n+    EXPECT_EQ(block.base(), (MetaWord*)expected_base); \\\n+    EXPECT_EQ((size_t)expected_size, block.word_size()); \\\n+    EXPECT_EQ(block.end(), expected_base + expected_size); \\\n+    DEBUG_ONLY(block.verify()); \\\n+}\n+\n+static constexpr uintptr_t large_pointer = NOT_LP64(0x99999990) LP64_ONLY(0x9999999999999990ULL);\n+\n+TEST(metaspace, MetaBlock_1) {\n+  MetaBlock bl;\n+  CHECK_BLOCK_EMPTY(bl);\n+}\n+\n+TEST(metaspace, MetaBlock_2) {\n+  MetaWord* const p = (MetaWord*)large_pointer;\n+  constexpr size_t s = G;\n+  MetaBlock bl(p, s);\n+  CHECK_BLOCK(bl, p, s);\n+}\n+\n+TEST(metaspace, MetaBlock_3) {\n+  MetaWord* const p = (MetaWord*)large_pointer;\n+  MetaBlock bl(p, 0);\n+  CHECK_BLOCK_EMPTY(bl);\n+}\n+\n+TEST_VM(metaspace, MetaBlock_4) {\n+  MetaWord* const p = (MetaWord*)large_pointer;\n+  MetaBlock bl(p, G);\n+  CHECK_BLOCK(bl, p, G);\n+\n+  MetaBlock bl_copy = bl, bl2;\n+\n+  bl2 = bl.split_off_tail(M);\n+  CHECK_BLOCK(bl, p, G - M);\n+  CHECK_BLOCK(bl2, p + G - M, M);\n+\n+  bl = bl_copy;\n+\n+bl.print_on(tty);\n+bl2.print_on(tty);\n+  bl2 = bl.split_off_tail(G);\n+  bl.print_on(tty);\n+  bl2.print_on(tty);\n+\n+  ASSERT_EQ(bl2, bl_copy);\n+  ASSERT_TRUE(bl.is_empty());\n+\n+  bl = bl_copy;\n+\n+  bl2 = bl.split_off_tail(0);\n+  ASSERT_EQ(bl, bl_copy);\n+  ASSERT_TRUE(bl2.is_empty());\n+\n+  MetaBlock empty;\n+  bl = empty.split_off_tail(0);\n+  ASSERT_TRUE(bl.is_empty());\n+}\n","filename":"test\/hotspot\/gtest\/metaspace\/test_metablock.cpp","additions":97,"deletions":0,"binary":false,"changes":97,"status":"added"},{"patch":"@@ -31,1 +31,1 @@\n-TEST_VM(MetaspaceUtils, reserved) {\n+TEST_VM(metaspace, MetaspaceUtils_reserved) {\n@@ -40,1 +40,1 @@\n-TEST_VM(MetaspaceUtils, reserved_compressed_class_pointers) {\n+TEST_VM(metaspace, MetaspaceUtils_reserved_compressed_class_pointers) {\n@@ -52,1 +52,1 @@\n-TEST_VM(MetaspaceUtils, committed) {\n+TEST_VM(metaspace, MetaspaceUtils_committed) {\n@@ -64,1 +64,1 @@\n-TEST_VM(MetaspaceUtils, committed_compressed_class_pointers) {\n+TEST_VM(metaspace, MetaspaceUtils_committed_compressed_class_pointers) {\n@@ -76,1 +76,1 @@\n-TEST_VM(MetaspaceUtils, non_compressed_class_pointers) {\n+TEST_VM(metaspace, MetaspaceUtils_non_compressed_class_pointers) {\n@@ -102,1 +102,1 @@\n-TEST_VM(MetaspaceUtils, get_statistics) {\n+TEST_VM(MetaspaceUtils, MetaspaceUtils_get_statistics) {\n","filename":"test\/hotspot\/gtest\/metaspace\/test_metaspaceUtils.cpp","additions":6,"deletions":6,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -4,0 +4,1 @@\n+ * Copyright (c) 2023 Red Hat, Inc. All rights reserved.\n@@ -31,0 +32,2 @@\n+#include \"memory\/metaspace\/freeBlocks.hpp\"\n+#include \"memory\/metaspace\/metablock.inline.hpp\"\n@@ -33,0 +36,1 @@\n+#include \"memory\/metaspace\/metachunkList.hpp\"\n@@ -36,0 +40,1 @@\n+#include \"memory\/metaspace.hpp\"\n@@ -39,1 +44,1 @@\n-\/\/#define LOG_PLEASE\n+#define LOG_PLEASE\n@@ -44,9 +49,14 @@\n-using metaspace::AllocationAlignmentByteSize;\n-using metaspace::ArenaGrowthPolicy;\n-using metaspace::CommitLimiter;\n-using metaspace::InternalStats;\n-using metaspace::MemRangeCounter;\n-using metaspace::MetaspaceArena;\n-using metaspace::SizeAtomicCounter;\n-using metaspace::Settings;\n-using metaspace::ArenaStats;\n+#define HANDLE_FAILURE \\\n+  if (testing::Test::HasFailure()) { \\\n+    return; \\\n+  }\n+\n+namespace metaspace {\n+\n+class MetaspaceArenaTestFriend {\n+  const MetaspaceArena* const _arena;\n+public:\n+  MetaspaceArenaTestFriend(const MetaspaceArena* arena) : _arena(arena) {}\n+  const MetachunkList& chunks() const { return _arena->_chunks; }\n+  const FreeBlocks* fbl() const { return _arena->_fbl; }\n+};\n@@ -57,0 +67,1 @@\n+  const ArenaGrowthPolicy* const _growth_policy;\n@@ -58,2 +69,0 @@\n-  const ArenaGrowthPolicy* _growth_policy;\n-  SizeAtomicCounter _used_words_counter;\n@@ -62,6 +71,0 @@\n-  void initialize(const ArenaGrowthPolicy* growth_policy, const char* name = \"gtest-MetaspaceArena\") {\n-    _growth_policy = growth_policy;\n-    _arena = new MetaspaceArena(&_context.cm(), _growth_policy, &_used_words_counter, name);\n-    DEBUG_ONLY(_arena->verify());\n-  }\n-\n@@ -70,9 +73,0 @@\n-  \/\/ Create a helper; growth policy for arena is determined by the given spacetype|class tupel\n-  MetaspaceArenaTestHelper(MetaspaceGtestContext& helper,\n-                            Metaspace::MetaspaceType space_type, bool is_class,\n-                            const char* name = \"gtest-MetaspaceArena\") :\n-    _context(helper)\n-  {\n-    initialize(ArenaGrowthPolicy::policy_for_space_type(space_type, is_class), name);\n-  }\n-\n@@ -81,2 +75,2 @@\n-                           const char* name = \"gtest-MetaspaceArena\") :\n-    _context(helper)\n+                           size_t allocation_alignment_words = Metaspace::min_allocation_alignment_words) :\n+    _context(helper), _growth_policy(growth_policy), _arena(nullptr)\n@@ -84,1 +78,3 @@\n-    initialize(growth_policy, name);\n+    _arena = new MetaspaceArena(_context.context(), _growth_policy, allocation_alignment_words, \"gtest-MetaspaceArena\");\n+    DEBUG_ONLY(_arena->verify());\n+    _context.inc_num_arenas_created();\n@@ -87,0 +83,8 @@\n+\n+  \/\/ Create a helper; growth policy for arena is determined by the given spacetype|class tupel\n+  MetaspaceArenaTestHelper(MetaspaceGtestContext& helper,\n+                           Metaspace::MetaspaceType space_type, bool is_class,\n+                           size_t allocation_alignment_words = Metaspace::min_allocation_alignment_words) :\n+    MetaspaceArenaTestHelper(helper, ArenaGrowthPolicy::policy_for_space_type(space_type, is_class), allocation_alignment_words)\n+  {}\n+\n@@ -91,1 +95,0 @@\n-  const CommitLimiter& limiter() const { return _context.commit_limiter(); }\n@@ -93,1 +96,0 @@\n-  SizeAtomicCounter& used_words_counter() { return _used_words_counter; }\n@@ -100,2 +102,2 @@\n-      size_t used_words_before = _used_words_counter.get();\n-      size_t committed_words_before = limiter().committed_words();\n+      size_t used_words_before = _context.used_words();\n+      size_t committed_words_before = _context.committed_words();\n@@ -105,3 +107,8 @@\n-      size_t used_words_after = _used_words_counter.get();\n-      size_t committed_words_after = limiter().committed_words();\n-      ASSERT_0(used_words_after);\n+      size_t used_words_after = _context.used_words();\n+      size_t committed_words_after = _context.committed_words();\n+      assert(_context.num_arenas_created() >= 1, \"Sanity\");\n+      if (_context.num_arenas_created() == 1) {\n+        ASSERT_0(used_words_after);\n+      } else {\n+        ASSERT_LE(used_words_after, used_words_before);\n+      }\n@@ -113,7 +120,17 @@\n-    _arena->usage_numbers(p_used, p_committed, p_capacity);\n-    if (p_used != nullptr) {\n-      if (p_committed != nullptr) {\n-        ASSERT_GE(*p_committed, *p_used);\n-      }\n-      \/\/ Since we own the used words counter, it should reflect our usage number 1:1\n-      ASSERT_EQ(_used_words_counter.get(), *p_used);\n+    size_t arena_used = 0, arena_committed = 0, arena_reserved = 0;\n+    _arena->usage_numbers(&arena_used, &arena_committed, &arena_reserved);\n+    EXPECT_GE(arena_committed, arena_used);\n+    EXPECT_GE(arena_reserved, arena_committed);\n+\n+    size_t context_used = _context.used_words();\n+    size_t context_committed = _context.committed_words();\n+    size_t context_reserved = _context.reserved_words();\n+    EXPECT_GE(context_committed, context_used);\n+    EXPECT_GE(context_reserved, context_committed);\n+\n+    \/\/ If only one arena uses the context, usage numbers must match.\n+    if (_context.num_arenas_created() == 1) {\n+      EXPECT_EQ(context_used, arena_used);\n+    } else {\n+      assert(_context.num_arenas_created() > 1, \"Sanity\");\n+      EXPECT_GE(context_used, arena_used);\n@@ -121,2 +138,13 @@\n-    if (p_committed != nullptr && p_capacity != nullptr) {\n-      ASSERT_GE(*p_capacity, *p_committed);\n+\n+    \/\/ commit, reserve numbers don't have to match since free chunks may exist\n+    EXPECT_GE(context_committed, arena_committed);\n+    EXPECT_GE(context_reserved, arena_reserved);\n+\n+    if (p_used) {\n+      *p_used = arena_used;\n+    }\n+    if (p_committed) {\n+      *p_committed = arena_committed;\n+    }\n+    if (p_capacity) {\n+      *p_capacity = arena_reserved;\n@@ -145,1 +173,0 @@\n-  \/\/ Allocate; it may or may not work; return value in *p_return_value\n@@ -147,0 +174,11 @@\n+    MetaBlock result, wastage;\n+    allocate_from_arena_with_tests(word_size, result, wastage);\n+    if (wastage.is_nonempty()) {\n+      _arena->deallocate(wastage);\n+      wastage.reset();\n+    }\n+    (*p_return_value) = result.base();\n+  }\n+\n+  \/\/ Allocate; it may or may not work; return value in *p_return_value\n+  void allocate_from_arena_with_tests(size_t word_size, MetaBlock& result, MetaBlock& wastage) {\n@@ -152,1 +190,1 @@\n-    size_t possible_expansion = limiter().possible_expansion_words();\n+    size_t possible_expansion = _context.commit_limiter().possible_expansion_words();\n@@ -154,1 +192,1 @@\n-    MetaWord* p = _arena->allocate(word_size);\n+    result = _arena->allocate(word_size, wastage);\n@@ -161,1 +199,1 @@\n-    if (p == nullptr) {\n+    if (result.is_empty()) {\n@@ -169,1 +207,2 @@\n-      ASSERT_TRUE(is_aligned(p, AllocationAlignmentByteSize));\n+      ASSERT_TRUE(result.is_aligned_base(_arena->allocation_alignment_words()));\n+\n@@ -178,2 +217,0 @@\n-\n-    *p_return_value = p;\n@@ -192,1 +229,1 @@\n-    _arena->deallocate(p, word_size);\n+    _arena->deallocate(MetaBlock(p, word_size));\n@@ -212,0 +249,4 @@\n+  MetaspaceArenaTestFriend internal_access() const {\n+    return MetaspaceArenaTestFriend (_arena);\n+  }\n+\n@@ -214,1 +255,1 @@\n-    return get_arena_statistics().totals()._num;\n+    return internal_access().chunks().count();\n@@ -393,0 +434,1 @@\n+    ASSERT_FALSE(HasFailure());\n@@ -396,0 +438,1 @@\n+    ASSERT_FALSE(HasFailure());\n@@ -402,0 +445,1 @@\n+    ASSERT_FALSE(HasFailure());\n@@ -407,0 +451,1 @@\n+    ASSERT_FALSE(HasFailure());\n@@ -410,0 +455,1 @@\n+    ASSERT_FALSE(HasFailure());\n@@ -452,0 +498,1 @@\n+    HANDLE_FAILURE\n@@ -496,1 +543,1 @@\n-  MetaspaceArenaTestHelper smhelper(context, type, is_class, \"Grower\");\n+  MetaspaceArenaTestHelper smhelper(context, type, is_class);\n@@ -498,1 +545,1 @@\n-  MetaspaceArenaTestHelper smhelper_harrasser(context, Metaspace::ReflectionMetaspaceType, true, \"Harasser\");\n+  MetaspaceArenaTestHelper smhelper_harrasser(context, Metaspace::ReflectionMetaspaceType, true);\n@@ -561,0 +608,1 @@\n+    HANDLE_FAILURE\n@@ -567,0 +615,1 @@\n+    HANDLE_FAILURE\n@@ -608,0 +657,4 @@\n+  \/\/ No FBL should exist, we did not deallocate\n+  ASSERT_EQ(smhelper.internal_access().fbl(), (FreeBlocks*)nullptr);\n+  ASSERT_EQ(smhelper_harrasser.internal_access().fbl(), (FreeBlocks*)nullptr);\n+\n@@ -711,1 +764,2 @@\n-  for (size_t blocksize = Metaspace::max_allocation_word_size(); blocksize >= 1; blocksize \/= 2) {\n+  for (size_t blocksize = Metaspace::max_allocation_word_size();\n+       blocksize >= Metaspace::min_allocation_word_size; blocksize \/= 2) {\n@@ -723,0 +777,1 @@\n+      HANDLE_FAILURE\n@@ -732,0 +787,1 @@\n+      HANDLE_FAILURE\n@@ -750,0 +806,117 @@\n+\n+static void test_random_aligned_allocation(size_t arena_alignment_words, SizeRange range) {\n+  \/\/ We let the arena use 4K chunks, unless the alloc size is larger.\n+  chunklevel_t level = CHUNK_LEVEL_4K;\n+  const ArenaGrowthPolicy policy (&level, 1);\n+  const size_t chunk_word_size = word_size_for_level(level);\n+\n+  size_t expected_used = 0;\n+\n+  MetaspaceGtestContext context;\n+  MetaspaceArenaTestHelper helper(context, &policy, arena_alignment_words);\n+\n+  size_t last_alloc_size = 0;\n+  unsigned num_allocations = 0;\n+\n+  const size_t max_used = MIN2(MAX2(chunk_word_size * 10, (range.highest() * 100)),\n+                               LP64_ONLY(64) NOT_LP64(16) * M); \/\/ word size!\n+  while (expected_used < max_used) {\n+\n+    const int chunks_before = helper.get_number_of_chunks();\n+\n+    MetaBlock result, wastage;\n+    size_t alloc_words = range.random_value();\n+    NOT_LP64(alloc_words = align_up(alloc_words, Metaspace::min_allocation_alignment_words));\n+    helper.allocate_from_arena_with_tests(alloc_words, result, wastage);\n+\n+    ASSERT_TRUE(result.is_nonempty());\n+    ASSERT_TRUE(result.is_aligned_base(arena_alignment_words));\n+    ASSERT_EQ(result.word_size(), alloc_words);\n+\n+    expected_used += alloc_words + wastage.word_size();\n+    const int chunks_now = helper.get_number_of_chunks();\n+    ASSERT_GE(chunks_now, chunks_before);\n+    ASSERT_LE(chunks_now, chunks_before + 1);\n+\n+    \/\/ Estimate wastage:\n+    \/\/ Guessing at wastage is somewhat simple since we don't expect to ever use the fbl (we\n+    \/\/ don't deallocate). Therefore, wastage can only be caused by alignment gap or by\n+    \/\/ salvaging an old chunk before a new chunk is added.\n+    const bool expect_alignment_gap = !is_aligned(last_alloc_size, arena_alignment_words);\n+    const bool new_chunk_added = chunks_now > chunks_before;\n+\n+    if (num_allocations == 0) {\n+      \/\/ expect no wastage if its the first allocation in the arena\n+      ASSERT_TRUE(wastage.is_empty());\n+    } else {\n+      if (expect_alignment_gap) {\n+        \/\/ expect wastage if the alignment requires it\n+        ASSERT_TRUE(wastage.is_nonempty());\n+      }\n+    }\n+\n+    if (wastage.is_nonempty()) {\n+      \/\/ If we have wastage, we expect it to be either too small or unaligned. That would not be true\n+      \/\/ for wastage from the fbl, which could have any size; however, in this test we don't deallocate,\n+      \/\/ so we don't expect wastage from the fbl.\n+      if (wastage.is_aligned_base(arena_alignment_words)) {\n+        ASSERT_LT(wastage.word_size(), alloc_words);\n+      }\n+      if (new_chunk_added) {\n+        \/\/ chunk turnover: no more wastage than size of a commit granule, since we salvage the\n+        \/\/ committed remainder of the old chunk.\n+        ASSERT_LT(wastage.word_size(), Settings::commit_granule_words());\n+      } else {\n+        \/\/ No chunk turnover: no more wastage than what alignment requires.\n+        ASSERT_LT(wastage.word_size(), arena_alignment_words);\n+      }\n+    }\n+\n+    \/\/ Check stats too\n+    size_t used, committed, reserved;\n+    helper.usage_numbers_with_test(&used, &committed, &reserved);\n+    ASSERT_EQ(used, expected_used);\n+\n+    \/\/ No FBL should exist, we did not deallocate\n+    ASSERT_EQ(helper.internal_access().fbl(), (FreeBlocks*)nullptr);\n+\n+    HANDLE_FAILURE\n+\n+    last_alloc_size = alloc_words;\n+    num_allocations ++;\n+  }\n+  LOG(\"allocs: %u\", num_allocations);\n+}\n+\n+#define TEST_ARENA_WITH_ALIGNMENT_SMALL_RANGE(al)                              \\\n+TEST_VM(metaspace, MetaspaceArena_test_random_small_aligned_allocation_##al) { \\\n+  static const SizeRange range(Metaspace::min_allocation_word_size, 128);      \\\n+  test_random_aligned_allocation(al, range);                                   \\\n+}\n+\n+#ifdef _LP64\n+TEST_ARENA_WITH_ALIGNMENT_SMALL_RANGE(1);\n+#endif\n+TEST_ARENA_WITH_ALIGNMENT_SMALL_RANGE(2);\n+TEST_ARENA_WITH_ALIGNMENT_SMALL_RANGE(8);\n+TEST_ARENA_WITH_ALIGNMENT_SMALL_RANGE(32);\n+TEST_ARENA_WITH_ALIGNMENT_SMALL_RANGE(128);\n+TEST_ARENA_WITH_ALIGNMENT_SMALL_RANGE(MIN_CHUNK_WORD_SIZE);\n+\n+#define TEST_ARENA_WITH_ALIGNMENT_LARGE_RANGE(al)                              \\\n+TEST_VM(metaspace, MetaspaceArena_test_random_large_aligned_allocation_##al) { \\\n+  static const SizeRange range(Metaspace::max_allocation_word_size() \/ 2,      \\\n+                                   Metaspace::max_allocation_word_size());     \\\n+  test_random_aligned_allocation(al, range);                                   \\\n+}\n+\n+#ifdef _LP64\n+TEST_ARENA_WITH_ALIGNMENT_LARGE_RANGE(1);\n+#endif\n+TEST_ARENA_WITH_ALIGNMENT_LARGE_RANGE(2);\n+TEST_ARENA_WITH_ALIGNMENT_LARGE_RANGE(8);\n+TEST_ARENA_WITH_ALIGNMENT_LARGE_RANGE(32);\n+TEST_ARENA_WITH_ALIGNMENT_LARGE_RANGE(128);\n+TEST_ARENA_WITH_ALIGNMENT_LARGE_RANGE(MIN_CHUNK_WORD_SIZE);\n+\n+} \/\/ namespace metaspace\n","filename":"test\/hotspot\/gtest\/metaspace\/test_metaspacearena.cpp","additions":231,"deletions":58,"binary":false,"changes":289,"status":"modified"},{"patch":"@@ -29,0 +29,1 @@\n+#include \"memory\/metaspace\/metablock.hpp\"\n@@ -31,0 +32,1 @@\n+#include \"memory\/metaspace\/metaspaceContext.hpp\"\n@@ -46,0 +48,1 @@\n+using metaspace::MetaBlock;\n@@ -47,0 +50,1 @@\n+using metaspace::MetaspaceContext;\n@@ -127,8 +131,6 @@\n-  MetaspaceArenaTestBed(ChunkManager* cm, const ArenaGrowthPolicy* alloc_sequence,\n-                        SizeAtomicCounter* used_words_counter, SizeRange allocation_range) :\n-    _arena(nullptr),\n-    _allocation_range(allocation_range),\n-    _size_of_last_failed_allocation(0),\n-    _allocations(nullptr),\n-    _alloc_count(),\n-    _dealloc_count()\n+  MetaspaceArenaTestBed(MetaspaceContext* context, const ArenaGrowthPolicy* growth_policy,\n+                        size_t allocation_alignment_words, SizeRange allocation_range)\n+    : _arena(nullptr)\n+    , _allocation_range(allocation_range)\n+    , _size_of_last_failed_allocation(0)\n+    , _allocations(nullptr)\n@@ -136,1 +138,1 @@\n-    _arena = new MetaspaceArena(cm, alloc_sequence, used_words_counter, \"gtest-MetaspaceArenaTestBed-sm\");\n+    _arena = new MetaspaceArena(context, growth_policy, Metaspace::min_allocation_alignment_words, \"gtest-MetaspaceArenaTestBed-sm\");\n@@ -166,3 +168,10 @@\n-    MetaWord* p = _arena->allocate(word_size);\n-    if (p != nullptr) {\n-      EXPECT_TRUE(is_aligned(p, AllocationAlignmentByteSize));\n+    MetaBlock wastage;\n+    MetaBlock bl = _arena->allocate(word_size, wastage);\n+    \/\/ We only expect wastage if either alignment was not met or the chunk remainder\n+    \/\/ was not large enough.\n+    if (wastage.is_nonempty()) {\n+      _arena->deallocate(wastage);\n+      wastage.reset();\n+    }\n+    if (bl.is_nonempty()) {\n+      EXPECT_TRUE(is_aligned(bl.base(), AllocationAlignmentByteSize));\n@@ -172,1 +181,1 @@\n-      a->p = p;\n+      a->p = bl.base();\n@@ -196,1 +205,1 @@\n-      _arena->deallocate(a->p, a->word_size);\n+      _arena->deallocate(MetaBlock(a->p, a->word_size));\n@@ -221,2 +230,2 @@\n-    MetaspaceArenaTestBed* bed = new MetaspaceArenaTestBed(&_context.cm(), growth_policy,\n-                                                       &_used_words_counter, allocation_range);\n+    MetaspaceArenaTestBed* bed = new MetaspaceArenaTestBed(_context.context(), growth_policy,\n+        Metaspace::min_allocation_alignment_words, allocation_range);\n","filename":"test\/hotspot\/gtest\/metaspace\/test_metaspacearena_stress.cpp","additions":25,"deletions":16,"binary":false,"changes":41,"status":"modified"},{"patch":"@@ -85,1 +85,17 @@\n-  if (UseCompressedClassPointers) {\n+  if (UseCompactObjectHeaders) {\n+    EXPECT_EQ(arrayOopDesc::base_offset_in_bytes(T_BOOLEAN), 12);\n+    EXPECT_EQ(arrayOopDesc::base_offset_in_bytes(T_BYTE),    12);\n+    EXPECT_EQ(arrayOopDesc::base_offset_in_bytes(T_SHORT),   12);\n+    EXPECT_EQ(arrayOopDesc::base_offset_in_bytes(T_CHAR),    12);\n+    EXPECT_EQ(arrayOopDesc::base_offset_in_bytes(T_INT),     12);\n+    EXPECT_EQ(arrayOopDesc::base_offset_in_bytes(T_FLOAT),   12);\n+    EXPECT_EQ(arrayOopDesc::base_offset_in_bytes(T_LONG),    16);\n+    EXPECT_EQ(arrayOopDesc::base_offset_in_bytes(T_DOUBLE),  16);\n+    if (UseCompressedOops) {\n+      EXPECT_EQ(arrayOopDesc::base_offset_in_bytes(T_OBJECT), 12);\n+      EXPECT_EQ(arrayOopDesc::base_offset_in_bytes(T_ARRAY),  12);\n+    } else {\n+      EXPECT_EQ(arrayOopDesc::base_offset_in_bytes(T_OBJECT), 16);\n+      EXPECT_EQ(arrayOopDesc::base_offset_in_bytes(T_ARRAY),  16);\n+    }\n+  } else if (UseCompressedClassPointers) {\n","filename":"test\/hotspot\/gtest\/oops\/test_arrayOop.cpp","additions":17,"deletions":1,"binary":false,"changes":18,"status":"modified"},{"patch":"@@ -39,0 +39,1 @@\n+\n@@ -47,1 +48,2 @@\n-    ShouldNotReachHere();\n+    const size_t expected_size = nth_bit(CompressedKlassPointers::narrow_klass_pointer_bits() + CompressedKlassPointers::shift());\n+    ASSERT_EQ(CompressedKlassPointers::encoding_range_end() - CompressedKlassPointers::base(), (ptrdiff_t)expected_size);\n@@ -51,0 +53,12 @@\n+TEST_VM(CompressedKlass, ccp_off) {\n+  if (UseCompressedClassPointers) {\n+    return;\n+  }\n+  ASSERT_EQ(CompressedKlassPointers::klass_range_start(), (address)nullptr);\n+  ASSERT_EQ(CompressedKlassPointers::klass_range_end(), (address)nullptr);\n+  \/\/ We should be able to call CompressedKlassPointers::is_encodable, and it should\n+  \/\/ always return false\n+  ASSERT_FALSE(CompressedKlassPointers::is_encodable((address)0x12345));\n+}\n+\n+\n@@ -71,0 +85,16 @@\n+TEST_VM(CompressedKlass, test_unaligned_address) {\n+  if (!UseCompressedClassPointers) {\n+    return;\n+  }\n+  const size_t alignment = CompressedKlassPointers::klass_alignment_in_bytes();\n+  address addr = CompressedKlassPointers::klass_range_start() + alignment - 1;\n+  ASSERT_FALSE(CompressedKlassPointers::is_encodable(addr));\n+  \/\/ Try word-aligned, but not sufficiently aligned\n+  if (alignment > BytesPerWord) {\n+    addr = CompressedKlassPointers::klass_range_start() + BytesPerWord;\n+    ASSERT_FALSE(CompressedKlassPointers::is_encodable(addr));\n+  }\n+  addr = CompressedKlassPointers::klass_range_end() - 1;\n+  ASSERT_FALSE(CompressedKlassPointers::is_encodable(addr));\n+}\n+\n@@ -75,0 +105,1 @@\n+  const size_t alignment = CompressedKlassPointers::klass_alignment_in_bytes();\n@@ -77,1 +108,1 @@\n-  addr = CompressedKlassPointers::klass_range_end() - 1;\n+  addr = CompressedKlassPointers::klass_range_end() - alignment;\n","filename":"test\/hotspot\/gtest\/oops\/test_compressedKlass.cpp","additions":33,"deletions":2,"binary":false,"changes":35,"status":"modified"},{"patch":"@@ -31,1 +31,1 @@\n-    int objal; bool ccp; bool coops; int result;\n+    int objal; bool ccp; bool coops; bool coh; int result;\n@@ -33,1 +33,1 @@\n-\/\/    ObjAligInB, UseCCP, UseCoops, object size in heap words\n+\/\/    ObjAligInB, UseCCP, UseCoops, UseCOH, object size in heap words\n@@ -35,12 +35,18 @@\n-    { 8,          false,  false,    4 },  \/\/ 20 byte header, 8 byte oops\n-    { 8,          false,  true,     3 },  \/\/ 20 byte header, 4 byte oops\n-    { 8,          true,   false,    3 },  \/\/ 16 byte header, 8 byte oops\n-    { 8,          true,   true,     3 },  \/\/ 16 byte header, 4 byte oops\n-    { 16,         false,  false,    4 },  \/\/ 20 byte header, 8 byte oops, 16-byte align\n-    { 16,         false,  true,     4 },  \/\/ 20 byte header, 4 byte oops, 16-byte align\n-    { 16,         true,   false,    4 },  \/\/ 16 byte header, 8 byte oops, 16-byte align\n-    { 16,         true,   true,     4 },  \/\/ 16 byte header, 4 byte oops, 16-byte align\n-    { 256,        false,  false,    32 }, \/\/ 20 byte header, 8 byte oops, 256-byte align\n-    { 256,        false,  true,     32 }, \/\/ 20 byte header, 4 byte oops, 256-byte align\n-    { 256,        true,   false,    32 }, \/\/ 16 byte header, 8 byte oops, 256-byte align\n-    { 256,        true,   true,     32 }, \/\/ 16 byte header, 4 byte oops, 256-byte align\n+    { 8,          false,  false, false,   4 },  \/\/ 20 byte header, 8 byte oops\n+    { 8,          false,  true,  false,   3 },  \/\/ 20 byte header, 4 byte oops\n+    { 8,          true,   false, false,   3 },  \/\/ 16 byte header, 8 byte oops\n+    { 8,          true,   true,  false,   3 },  \/\/ 16 byte header, 4 byte oops\n+    { 8,          true,   false, true,    3 },  \/\/ 12 byte header, 8 byte oops\n+    { 8,          true,   true,  true,    2 },  \/\/ 12 byte header, 4 byte oops\n+    { 16,         false,  false, false,   4 },  \/\/ 20 byte header, 8 byte oops, 16-byte align\n+    { 16,         false,  true,  false,   4 },  \/\/ 20 byte header, 4 byte oops, 16-byte align\n+    { 16,         true,   false, false,   4 },  \/\/ 16 byte header, 8 byte oops, 16-byte align\n+    { 16,         true,   true,  false,   4 },  \/\/ 16 byte header, 4 byte oops, 16-byte align\n+    { 16,         true,   false, true,    4 },  \/\/ 12 byte header, 8 byte oops, 16-byte align\n+    { 16,         true,   true,  true,    2 },  \/\/ 12 byte header, 4 byte oops, 16-byte align\n+    { 256,        false,  false, false,  32 }, \/\/ 20 byte header, 8 byte oops, 256-byte align\n+    { 256,        false,  true,  false,  32 }, \/\/ 20 byte header, 4 byte oops, 256-byte align\n+    { 256,        true,   false, false,  32 }, \/\/ 16 byte header, 8 byte oops, 256-byte align\n+    { 256,        true,   true,  false,  32 }, \/\/ 16 byte header, 4 byte oops, 256-byte align\n+    { 256,        true,   false, true,   32 }, \/\/ 12 byte header, 8 byte oops, 256-byte align\n+    { 256,        true,   true,  true,   32 }, \/\/ 12 byte header, 4 byte oops, 256-byte align\n@@ -48,1 +54,1 @@\n-    { 8,          false,  false,    4 }, \/\/ 12 byte header, 4 byte oops, wordsize 4\n+    { 8,          false,  false, false,   4 }, \/\/ 12 byte header, 4 byte oops, wordsize 4\n@@ -50,1 +56,1 @@\n-    { -1,         false,  false,   -1 }\n+    { -1,         false,  false, false,  -1 }\n@@ -53,1 +59,2 @@\n-    if (x[i].objal == (int)ObjectAlignmentInBytes && x[i].ccp == UseCompressedClassPointers && x[i].coops == UseCompressedOops) {\n+    if (x[i].objal == (int)ObjectAlignmentInBytes && x[i].ccp == UseCompressedClassPointers && x[i].coops == UseCompressedOops &&\n+        x[i].coh == UseCompactObjectHeaders) {\n","filename":"test\/hotspot\/gtest\/oops\/test_objArrayOop.cpp","additions":24,"deletions":17,"binary":false,"changes":41,"status":"modified"},{"patch":"@@ -39,1 +39,5 @@\n-  o->set_klass(Universe::boolArrayKlass());\n+  if (UseCompactObjectHeaders) {\n+    o->set_mark(Universe::boolArrayKlass()->prototype_header());\n+  } else {\n+    o->set_klass(Universe::boolArrayKlass());\n+  }\n","filename":"test\/hotspot\/gtest\/oops\/test_typeArrayOop.cpp","additions":5,"deletions":1,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -121,0 +121,4 @@\n+# Fails with +UseCompactObjectHeaders on aarch64\n+runtime\/cds\/appcds\/SharedBaseAddress.java 8340212 linux-aarch64,macosx-aarch64\n+runtime\/cds\/SharedBaseAddress.java 8340212 linux-aarch64,macosx-aarch64\n+\n","filename":"test\/hotspot\/jtreg\/ProblemList.txt","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -51,1 +51,1 @@\n-    @IR(counts = { IRNode.NOP, \"1\" })\n+    @IR(counts = { IRNode.NOP, \"<= 1\" })\n","filename":"test\/hotspot\/jtreg\/compiler\/c2\/irTests\/TestPadding.java","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -150,1 +150,3 @@\n-    @IR(counts = { IRNode.LOAD_VECTOR_L, \">=1\", IRNode.STORE_VECTOR, \">=1\" })\n+    @IR(counts = { IRNode.LOAD_VECTOR_L, \">=1\", IRNode.STORE_VECTOR, \">=1\" },\n+        \/\/ This test fails with compact headers, but only with UseSSE<=3.\n+        applyIf = { \"UseCompactObjectHeaders\", \"false\" })\n@@ -163,1 +165,3 @@\n-    @IR(counts = { IRNode.LOAD_VECTOR_L, \">=1\", IRNode.STORE_VECTOR, \">=1\" })\n+    @IR(counts = { IRNode.LOAD_VECTOR_L, \">=1\", IRNode.STORE_VECTOR, \">=1\" },\n+        \/\/ This test fails with compact headers, but only with UseSSE<=3.\n+        applyIf = { \"UseCompactObjectHeaders\", \"false\" })\n@@ -176,1 +180,3 @@\n-    @IR(counts = { IRNode.LOAD_VECTOR_L, \">=1\", IRNode.STORE_VECTOR, \">=1\" })\n+    @IR(counts = { IRNode.LOAD_VECTOR_L, \">=1\", IRNode.STORE_VECTOR, \">=1\" },\n+        \/\/ This test fails with compact headers, but only with UseSSE<=3.\n+        applyIf = { \"UseCompactObjectHeaders\", \"false\" })\n@@ -205,1 +211,3 @@\n-    @IR(counts = { IRNode.LOAD_VECTOR_L, \">=1\", IRNode.STORE_VECTOR, \">=1\" })\n+    @IR(counts = { IRNode.LOAD_VECTOR_L, \">=1\", IRNode.STORE_VECTOR, \">=1\" },\n+        \/\/ This test fails with compact headers, but only with UseSSE<=3.\n+        applyIf = { \"UseCompactObjectHeaders\", \"false\" })\n@@ -219,1 +227,3 @@\n-    @IR(counts = { IRNode.LOAD_VECTOR_L, \">=1\", IRNode.STORE_VECTOR, \">=1\" })\n+    @IR(counts = { IRNode.LOAD_VECTOR_L, \">=1\", IRNode.STORE_VECTOR, \">=1\" },\n+        \/\/ This test fails with compact headers, but only with UseSSE<=3.\n+        applyIf = { \"UseCompactObjectHeaders\", \"false\" })\n@@ -232,1 +242,3 @@\n-    @IR(counts = { IRNode.LOAD_VECTOR_L, \">=1\", IRNode.STORE_VECTOR, \">=1\" })\n+    @IR(counts = { IRNode.LOAD_VECTOR_L, \">=1\", IRNode.STORE_VECTOR, \">=1\" },\n+        \/\/ This test fails with compact headers, but only with UseSSE<=3.\n+        applyIf = { \"UseCompactObjectHeaders\", \"false\" })\n","filename":"test\/hotspot\/jtreg\/compiler\/c2\/irTests\/TestVectorizationMismatchedAccess.java","additions":18,"deletions":6,"binary":false,"changes":24,"status":"modified"},{"patch":"@@ -54,1 +54,3 @@\n-    @IR(counts = { IRNode.LOAD_VECTOR_L, \">=1\", IRNode.STORE_VECTOR, \">=1\" })\n+    @IR(counts = { IRNode.LOAD_VECTOR_L, \">=1\", IRNode.STORE_VECTOR, \">=1\" },\n+        \/\/ This test fails with compact headers, but only with UseSSE<=3.\n+        applyIf = { \"UseCompactObjectHeaders\", \"false\" })\n","filename":"test\/hotspot\/jtreg\/compiler\/c2\/irTests\/TestVectorizationNotRun.java","additions":3,"deletions":1,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2014, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2014, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -125,0 +125,4 @@\n+        \/\/ Hide timestamps from warnings (e.g. due to potential CDS\n+        \/\/ saved\/runtime state mismatch), to avoid false positives when\n+        \/\/ comparing output across runs.\n+        vmOpts.add(\"-Xlog:all=warning:stdout:level,tags\");\n","filename":"test\/hotspot\/jtreg\/compiler\/intrinsics\/bmi\/BMITestRunner.java","additions":5,"deletions":1,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -145,1 +145,2 @@\n-                    \"LogCompilation\"\n+                    \"LogCompilation\",\n+                    \"UseCompactObjectHeaders\"\n","filename":"test\/hotspot\/jtreg\/compiler\/lib\/ir_framework\/TestFramework.java","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -401,0 +401,1 @@\n+        applyIf = {\"UseCompactObjectHeaders\", \"false\"},\n@@ -709,1 +710,1 @@\n-        applyIf = {\"MaxVectorSize\", \">=16\"},\n+        applyIfAnd = {\"MaxVectorSize\", \">=16\", \"UseCompactObjectHeaders\", \"false\"},\n@@ -1004,0 +1005,1 @@\n+        applyIf = {\"UseCompactObjectHeaders\", \"false\"},\n@@ -1020,0 +1022,1 @@\n+        applyIf = {\"UseCompactObjectHeaders\", \"false\"},\n@@ -1040,0 +1043,1 @@\n+        applyIf = {\"UseCompactObjectHeaders\", \"false\"},\n@@ -1075,0 +1079,1 @@\n+        applyIf = {\"UseCompactObjectHeaders\", \"false\"},\n@@ -1091,0 +1096,1 @@\n+        applyIf = {\"UseCompactObjectHeaders\", \"false\"},\n@@ -1111,0 +1117,1 @@\n+        applyIf = {\"UseCompactObjectHeaders\", \"false\"},\n","filename":"test\/hotspot\/jtreg\/compiler\/loopopts\/superword\/TestAlignVector.java","additions":8,"deletions":1,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -166,0 +166,1 @@\n+        applyIf = { \"UseCompactObjectHeaders\", \"false\" },\n@@ -168,1 +169,1 @@\n-        applyIf = {\"MaxVectorSize\", \"16\"}, \/\/ AD file requires vector_length = 16\n+        applyIfAnd = {\"MaxVectorSize\", \"16\", \"UseCompactObjectHeaders\", \"false\"}, \/\/ AD file requires vector_length = 16\n@@ -171,0 +172,1 @@\n+        applyIf = { \"UseCompactObjectHeaders\", \"false\" },\n@@ -184,0 +186,1 @@\n+        applyIf = { \"UseCompactObjectHeaders\", \"false\" },\n@@ -186,1 +189,1 @@\n-        applyIf = {\"MaxVectorSize\", \"16\"}, \/\/ AD file requires vector_length = 16\n+        applyIfAnd = {\"MaxVectorSize\", \"16\", \"UseCompactObjectHeaders\", \"false\" }, \/\/ AD file requires vector_length = 16\n@@ -189,0 +192,1 @@\n+        applyIf = { \"UseCompactObjectHeaders\", \"false\" },\n@@ -202,0 +206,1 @@\n+        applyIf = { \"UseCompactObjectHeaders\", \"false\" },\n@@ -204,1 +209,1 @@\n-        applyIf = {\"MaxVectorSize\", \"16\"}, \/\/ AD file requires vector_length = 16\n+        applyIfAnd = {\"MaxVectorSize\", \"16\", \"UseCompactObjectHeaders\", \"false\" }, \/\/ AD file requires vector_length = 16\n@@ -207,0 +212,1 @@\n+        applyIf = { \"UseCompactObjectHeaders\", \"false\" },\n@@ -220,0 +226,1 @@\n+        applyIf = { \"UseCompactObjectHeaders\", \"false\" },\n@@ -222,1 +229,1 @@\n-        applyIf = {\"MaxVectorSize\", \"16\"}, \/\/ AD file requires vector_length = 16\n+        applyIfAnd = {\"MaxVectorSize\", \"16\", \"UseCompactObjectHeaders\", \"false\" }, \/\/ AD file requires vector_length = 16\n@@ -225,0 +232,1 @@\n+        applyIf = { \"UseCompactObjectHeaders\", \"false\" },\n@@ -238,0 +246,1 @@\n+        applyIf = { \"UseCompactObjectHeaders\", \"false\" },\n@@ -240,1 +249,1 @@\n-        applyIf = {\"MaxVectorSize\", \"16\"}, \/\/ AD file requires vector_length = 16\n+        applyIfAnd = {\"MaxVectorSize\", \"16\", \"UseCompactObjectHeaders\", \"false\" }, \/\/ AD file requires vector_length = 16\n@@ -243,0 +252,1 @@\n+        applyIf = { \"UseCompactObjectHeaders\", \"false\" },\n","filename":"test\/hotspot\/jtreg\/compiler\/loopopts\/superword\/TestMulAddS2I.java","additions":15,"deletions":5,"binary":false,"changes":20,"status":"modified"},{"patch":"@@ -226,0 +226,2 @@\n+        \/\/ This test fails with compact headers, but only with UseSSE<=3.\n+        applyIf = { \"UseCompactObjectHeaders\", \"false\" },\n@@ -239,0 +241,2 @@\n+        \/\/ This test fails with compact headers, but only with UseSSE<=3.\n+        applyIf = { \"UseCompactObjectHeaders\", \"false\" },\n","filename":"test\/hotspot\/jtreg\/compiler\/vectorization\/runner\/LoopCombinedOpTest.java","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -262,1 +262,0 @@\n-        new LogMessageWithLevel(\"Restore Preserved Marks \\\\(ms\\\\):\", Level.DEBUG),\n","filename":"test\/hotspot\/jtreg\/gc\/g1\/TestGCLogMessages.java","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -35,1 +35,1 @@\n- * @run main\/timeout=240 gc.g1.plab.TestPLABPromotion\n+ * @run main\/othervm\/timeout=240 -Xbootclasspath\/a:. -XX:+UnlockDiagnosticVMOptions -XX:+WhiteBoxAPI gc.g1.plab.TestPLABPromotion\n@@ -51,0 +51,1 @@\n+import jdk.test.whitebox.WhiteBox;\n@@ -57,0 +58,2 @@\n+    private static final boolean COMPACT_HEADERS = Platform.is64bit() && WhiteBox.getWhiteBox().getBooleanVMFlag(\"UseCompactObjectHeaders\");\n+\n@@ -77,1 +80,1 @@\n-    private static final int OBJECT_SIZE_HIGH   = 3072 * HEAP_WORD_SIZE;\n+    private static final int OBJECT_SIZE_HIGH   = (COMPACT_HEADERS ? 3266 : 3250) * HEAP_WORD_SIZE;\n","filename":"test\/hotspot\/jtreg\/gc\/g1\/plab\/TestPLABPromotion.java","additions":5,"deletions":2,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -38,1 +38,22 @@\n- * @run main\/native GTestWrapper --gtest_filter=CompressedKlass* -Xlog:metaspace* -Xmx6g -Xms128m -Xshare:off -XX:CompressedClassSpaceSize=128m\n+ * @run main\/native GTestWrapper --gtest_filter=CompressedKlass* -XX:+UnlockExperimentalVMOptions -XX:-UseCompactObjectHeaders -Xlog:metaspace* -Xmx6g -Xms128m -Xshare:off -XX:CompressedClassSpaceSize=128m\n+ *\/\n+\n+\/* @test id=ccp_off\n+ * @library \/test\/lib\n+ * @modules java.base\/jdk.internal.misc\n+ *          java.xml\n+ * @run main\/native GTestWrapper --gtest_filter=CompressedKlass* -XX:-UseCompressedClassPointers -Xlog:metaspace* -Xmx6g -Xms128m\n+ *\/\n+\n+\/* @test id=use-zero-based-encoding-coh\n+ * @library \/test\/lib\n+ * @modules java.base\/jdk.internal.misc\n+ *          java.xml\n+ * @run main\/native GTestWrapper --gtest_filter=CompressedKlass* -XX:+UnlockExperimentalVMOptions -XX:+UseCompactObjectHeaders -Xlog:metaspace* -Xmx6g -Xms128m -Xshare:off -XX:CompressedClassSpaceSize=128m\n+ *\/\n+\n+\/* @test id=use-zero-based-encoding-coh-large-class-space\n+ * @library \/test\/lib\n+ * @modules java.base\/jdk.internal.misc\n+ *          java.xml\n+ * @run main\/native GTestWrapper --gtest_filter=CompressedKlass* -XX:+UnlockExperimentalVMOptions -XX:+UseCompactObjectHeaders -Xlog:metaspace* -Xmx6g -Xms128m -Xshare:off -XX:CompressedClassSpaceSize=4g\n","filename":"test\/hotspot\/jtreg\/gtest\/CompressedKlassGtest.java","additions":22,"deletions":1,"binary":false,"changes":23,"status":"modified"},{"patch":"@@ -52,0 +52,11 @@\n+\n+\/* @test id=UseCompactObjectHeaders\n+ * @summary Run metaspace-related gtests with tiny classpointers\n+ * @library \/test\/lib\n+ * @modules java.base\/jdk.internal.misc\n+ *          java.xml\n+ * @requires vm.bits == 64\n+ * @requires vm.flagless\n+ * @requires vm.debug\n+ * @run main\/native GTestWrapper --gtest_filter=metaspace* -XX:+UnlockExperimentalVMOptions -XX:+UseCompactObjectHeaders\n+ *\/\n","filename":"test\/hotspot\/jtreg\/gtest\/MetaspaceGtests.java","additions":11,"deletions":0,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -1,40 +0,0 @@\n-\/*\n- * Copyright (C) 2021 THL A29 Limited, a Tencent company. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\n- *\/\n-\n-\/*\n- * Note: This runs the metaspace utils related parts of gtest in configurations which\n- *  are not tested explicitly in the standard gtests.\n- *\n- *\/\n-\n-\/* @test\n- * @bug 8264008\n- * @summary Run metaspace utils related gtests with compressed class pointers off\n- * @requires vm.bits == 64\n- * @library \/test\/lib\n- * @modules java.base\/jdk.internal.misc\n- *          java.xml\n- * @requires vm.flagless\n- * @run main\/native GTestWrapper --gtest_filter=MetaspaceUtils* -XX:-UseCompressedClassPointers\n- *\/\n","filename":"test\/hotspot\/jtreg\/gtest\/MetaspaceUtilsGtests.java","additions":0,"deletions":40,"binary":false,"changes":40,"status":"modified"},{"patch":"@@ -59,0 +59,1 @@\n+                \"-XX:+UnlockExperimentalVMOptions\", \"-XX:-UseCompactObjectHeaders\",\n","filename":"test\/hotspot\/jtreg\/runtime\/CompressedOops\/CompressedCPUSpecificClassSpaceReservation.java","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -49,0 +49,1 @@\n+    static final String usesCompactObjectHeadersPat = \"UseCompactObjectHeaders 1\";\n@@ -60,0 +61,5 @@\n+    \/\/ Returns true if the output indicates that the VM uses compact object headers\n+    static boolean usesCompactObjectHeaders(OutputAnalyzer output) {\n+        return output.getOutput().contains(usesCompactObjectHeadersPat);\n+    }\n+\n@@ -224,1 +230,1 @@\n-        if (!isCCSReservedAnywhere(output)) {\n+        if (!isCCSReservedAnywhere(output) && !usesCompactObjectHeaders(output)) {\n@@ -242,1 +248,1 @@\n-        if (!isCCSReservedAnywhere(output)) {\n+        if (!isCCSReservedAnywhere(output) && !usesCompactObjectHeaders(output)) {\n@@ -245,1 +251,1 @@\n-        if (!Platform.isAArch64() && !Platform.isPPC()) {\n+        if (!Platform.isAArch64()  && !usesCompactObjectHeaders(output) && !Platform.isPPC()) {\n@@ -264,1 +270,1 @@\n-        if (!isCCSReservedAnywhere(output)) {\n+        if (!isCCSReservedAnywhere(output) && !usesCompactObjectHeaders(output)) {\n@@ -267,1 +273,1 @@\n-        if (!Platform.isAArch64() && !Platform.isPPC()) {\n+        if (!Platform.isAArch64()  && !usesCompactObjectHeaders(output) && !Platform.isPPC()) {\n","filename":"test\/hotspot\/jtreg\/runtime\/CompressedOops\/CompressedClassPointers.java","additions":11,"deletions":5,"binary":false,"changes":16,"status":"modified"},{"patch":"@@ -45,1 +45,1 @@\n-    private static void test(long forceAddress, long classSpaceSize, long expectedEncodingBase, int expectedEncodingShift) throws IOException {\n+    private static void test(long forceAddress, boolean COH, long classSpaceSize, long expectedEncodingBase, int expectedEncodingShift) throws IOException {\n@@ -52,0 +52,3 @@\n+                \"-XX:+UnlockExperimentalVMOptions\",\n+                \"-XX:\" + (COH ? \"+\" : \"-\") + \"UseCompactObjectHeaders\",\n+                \"-XX:\" + (COH ? \"+\" : \"-\") + \"UseObjectMonitorTable\",\n@@ -63,1 +66,2 @@\n-            throw new SkippedException(\"Skipping because we cannot force ccs to \" + forceAddressString);\n+            System.out.println(\"Skipping because we cannot force ccs to \" + forceAddressString);\n+            return;\n@@ -76,1 +80,1 @@\n-        test(4 * G - 128 * M, 128 * M, 0, 0);\n+        test(4 * G - 128 * M, false, 128 * M, 0, 0);\n@@ -78,0 +82,19 @@\n+        \/\/ Test ccs nestling right at the end of the 32G range\n+        \/\/ Expecting:\n+        \/\/ - non-aarch64: base=0, shift=3\n+        \/\/ - aarch64: base to start of class range, shift 0\n+        if (Platform.isAArch64()) {\n+            \/\/ The best we can do on aarch64 is to be *near* the end of the 32g range, since a valid encoding base\n+            \/\/ on aarch64 must be 4G aligned, and the max. class space size is 3G.\n+            long forceAddress = 0x7_0000_0000L; \/\/ 28g, and also a valid EOR immediate\n+            test(forceAddress, false, 3 * G, forceAddress, 0);\n+        } else {\n+            test(32 * G - 128 * M, false, 128 * M, 0, 3);\n+        }\n+\n+        \/\/ Test ccs starting *below* 4G, but extending upwards beyond 4G. All platforms except aarch64 should pick\n+        \/\/ zero based encoding. On aarch64, this test is excluded since the only valid mode would be XOR, but bit\n+        \/\/ pattern for base and bit pattern would overlap.\n+        if (!Platform.isAArch64()) {\n+            test(4 * G - 128 * M, false, 2 * 128 * M, 0, 3);\n+        }\n@@ -80,0 +103,21 @@\n+        \/\/ Compact Object Header Mode:\n+        \/\/ On all platforms we expect the VM to chose the smallest possible shift value needed to cover\n+        \/\/ the encoding range. We expect the encoding Base to start at the class space start - but to enforce that,\n+        \/\/ we choose a high address.\n+        long forceAddress = 32 * G;\n+\n+        long ccsSize = 128 * M;\n+        int expectedShift = 6;\n+        test(forceAddress, true, ccsSize, forceAddress, expectedShift);\n+\n+        ccsSize = 512 * M;\n+        expectedShift = 8;\n+        test(forceAddress, true, ccsSize, forceAddress, expectedShift);\n+\n+        ccsSize = G;\n+        expectedShift = 9;\n+        test(forceAddress, true, ccsSize, forceAddress, expectedShift);\n+\n+        ccsSize = 3 * G;\n+        expectedShift = 10;\n+        test(forceAddress, true, ccsSize, forceAddress, expectedShift);\n","filename":"test\/hotspot\/jtreg\/runtime\/CompressedOops\/CompressedClassPointersEncodingScheme.java","additions":47,"deletions":3,"binary":false,"changes":50,"status":"modified"},{"patch":"@@ -1,113 +0,0 @@\n-\/*\n- * Copyright Amazon.com Inc. or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-\/*\n- * @test id=with-coops-no-ccp\n- * @library \/test\/lib\n- * @requires vm.bits == \"64\"\n- * @modules java.base\/jdk.internal.misc\n- * @run main\/othervm -XX:+UseCompressedOops -XX:-UseCompressedClassPointers ArrayBaseOffsets\n- *\/\n-\/*\n- * @test id=with-coops-with-ccp\n- * @library \/test\/lib\n- * @requires vm.bits == \"64\"\n- * @requires vm.opt.UseCompressedClassPointers != false\n- * @modules java.base\/jdk.internal.misc\n- * @run main\/othervm -XX:+UseCompressedOops -XX:+UseCompressedClassPointers ArrayBaseOffsets\n- *\/\n-\/*\n- * @test id=no-coops-no-ccp\n- * @library \/test\/lib\n- * @requires vm.bits == \"64\"\n- * @modules java.base\/jdk.internal.misc\n- * @run main\/othervm -XX:-UseCompressedOops -XX:-UseCompressedClassPointers ArrayBaseOffsets\n- *\/\n-\/*\n- * @test id=no-coops-with-ccp\n- * @library \/test\/lib\n- * @requires vm.bits == \"64\"\n- * @requires vm.opt.UseCompressedClassPointers != false\n- * @modules java.base\/jdk.internal.misc\n- * @run main\/othervm -XX:-UseCompressedOops -XX:+UseCompressedClassPointers ArrayBaseOffsets\n- *\/\n-\/*\n- * @test id=32bit\n- * @library \/test\/lib\n- * @requires vm.bits == \"32\"\n- * @modules java.base\/jdk.internal.misc\n- * @run main\/othervm ArrayBaseOffsets\n- *\/\n-\n-import jdk.internal.misc.Unsafe;\n-\n-import java.lang.management.ManagementFactory;\n-import java.lang.management.RuntimeMXBean;\n-import java.util.List;\n-\n-import jdk.test.lib.Asserts;\n-import jdk.test.lib.Platform;\n-\n-public class ArrayBaseOffsets {\n-\n-    private static final boolean COOP;\n-    private static final boolean CCP;\n-\n-    static {\n-        if (Platform.is64bit()) {\n-            RuntimeMXBean runtime = ManagementFactory.getRuntimeMXBean();\n-            List<String> vmargs = runtime.getInputArguments();\n-            CCP = !vmargs.contains(\"-XX:-UseCompressedClassPointers\");\n-            COOP = System.getProperty(\"java.vm.compressedOopsMode\") != null;\n-        } else {\n-            COOP = CCP = false;\n-        }\n-    }\n-\n-    static public void main(String[] args) {\n-        Unsafe unsafe = Unsafe.getUnsafe();\n-        int intOffset, longOffset;\n-        if (Platform.is64bit()) {\n-            if (CCP) {\n-                intOffset = 16;\n-                longOffset = 16;\n-            } else {\n-                intOffset = 20;\n-                longOffset = 24;\n-            }\n-        } else {\n-            intOffset = 12;\n-            longOffset = 16;\n-        }\n-        Asserts.assertEquals(unsafe.arrayBaseOffset(boolean[].class), intOffset,  \"Misplaced boolean array base\");\n-        Asserts.assertEquals(unsafe.arrayBaseOffset(byte[].class),    intOffset,  \"Misplaced byte    array base\");\n-        Asserts.assertEquals(unsafe.arrayBaseOffset(char[].class),    intOffset,  \"Misplaced char    array base\");\n-        Asserts.assertEquals(unsafe.arrayBaseOffset(short[].class),   intOffset,  \"Misplaced short   array base\");\n-        Asserts.assertEquals(unsafe.arrayBaseOffset(int[].class),     intOffset,  \"Misplaced int     array base\");\n-        Asserts.assertEquals(unsafe.arrayBaseOffset(long[].class),    longOffset, \"Misplaced long    array base\");\n-        Asserts.assertEquals(unsafe.arrayBaseOffset(float[].class),   intOffset,  \"Misplaced float   array base\");\n-        Asserts.assertEquals(unsafe.arrayBaseOffset(double[].class),  longOffset, \"Misplaced double  array base\");\n-        int expectedObjArrayOffset = (COOP || !Platform.is64bit()) ? intOffset : longOffset;\n-        Asserts.assertEquals(unsafe.arrayBaseOffset(Object[].class),  expectedObjArrayOffset, \"Misplaced object  array base\");\n-    }\n-}\n","filename":"test\/hotspot\/jtreg\/runtime\/FieldLayout\/ArrayBaseOffsets.java","additions":0,"deletions":113,"binary":false,"changes":113,"status":"modified"},{"patch":"@@ -0,0 +1,157 @@\n+\/*\n+ * Copyright (c) 2022, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+\/*\n+ * @test id=with-coops-with-ccp\n+ * @library \/test\/lib \/\n+ * @requires vm.bits == \"64\"\n+ * @modules java.base\/jdk.internal.misc\n+ *          java.management\n+ * @build jdk.test.whitebox.WhiteBox\n+ * @run driver jdk.test.lib.helpers.ClassFileInstaller jdk.test.whitebox.WhiteBox\n+ * @run main\/othervm -Xbootclasspath\/a:. -XX:+UnlockDiagnosticVMOptions -XX:+WhiteBoxAPI -XX:+UnlockExperimentalVMOptions -XX:+UseCompressedOops -XX:+UseCompressedClassPointers -XX:-UseCompactObjectHeaders BaseOffsets\n+ *\/\n+\/*\n+ * @test id=no-coops-with-ccp\n+ * @library \/test\/lib \/\n+ * @requires vm.bits == \"64\"\n+ * @modules java.base\/jdk.internal.misc\n+ *          java.management\n+ * @build jdk.test.whitebox.WhiteBox\n+ * @run driver jdk.test.lib.helpers.ClassFileInstaller jdk.test.whitebox.WhiteBox\n+ * @run main\/othervm -Xbootclasspath\/a:. -XX:+UnlockDiagnosticVMOptions -XX:+WhiteBoxAPI -XX:+UnlockExperimentalVMOptions -XX:-UseCompressedOops -XX:+UseCompressedClassPointers -XX:-UseCompactObjectHeaders BaseOffsets\n+ *\/\n+\/*\n+ * @test id=with-coops-no-ccp\n+ * @library \/test\/lib \/\n+ * @requires vm.bits == \"64\"\n+ * @modules java.base\/jdk.internal.misc\n+ *          java.management\n+ * @build jdk.test.whitebox.WhiteBox\n+ * @run driver jdk.test.lib.helpers.ClassFileInstaller jdk.test.whitebox.WhiteBox\n+ * @run main\/othervm -Xbootclasspath\/a:. -XX:+UnlockDiagnosticVMOptions -XX:+WhiteBoxAPI -XX:+UnlockExperimentalVMOptions -XX:+UseCompressedOops -XX:-UseCompressedClassPointers -XX:-UseCompactObjectHeaders BaseOffsets\n+ *\/\n+\/*\n+ * @test id=no-coops-no-ccp\n+ * @library \/test\/lib \/\n+ * @requires vm.bits == \"64\"\n+ * @modules java.base\/jdk.internal.misc\n+ *          java.management\n+ * @build jdk.test.whitebox.WhiteBox\n+ * @run driver jdk.test.lib.helpers.ClassFileInstaller jdk.test.whitebox.WhiteBox\n+ * @run main\/othervm -Xbootclasspath\/a:. -XX:+UnlockDiagnosticVMOptions -XX:+WhiteBoxAPI -XX:+UnlockExperimentalVMOptions -XX:-UseCompressedOops -XX:-UseCompressedClassPointers -XX:-UseCompactObjectHeaders BaseOffsets\n+ *\/\n+\/*\n+ * @test id=with-coop--with-coh\n+ * @library \/test\/lib \/\n+ * @requires vm.bits == \"64\"\n+ * @modules java.base\/jdk.internal.misc\n+ *          java.management\n+ * @build jdk.test.whitebox.WhiteBox\n+ * @run driver jdk.test.lib.helpers.ClassFileInstaller jdk.test.whitebox.WhiteBox\n+ * @run main\/othervm -Xbootclasspath\/a:. -XX:+UnlockDiagnosticVMOptions -XX:+WhiteBoxAPI -XX:+UnlockExperimentalVMOptions -XX:+UseCompressedOops -XX:+UseCompactObjectHeaders BaseOffsets\n+ *\/\n+\/*\n+ * @test id=no-coops-with-coh\n+ * @library \/test\/lib \/\n+ * @requires vm.bits == \"64\"\n+ * @modules java.base\/jdk.internal.misc\n+ *          java.management\n+ * @build jdk.test.whitebox.WhiteBox\n+ * @run driver jdk.test.lib.helpers.ClassFileInstaller jdk.test.whitebox.WhiteBox\n+ * @run main\/othervm -Xbootclasspath\/a:. -XX:+UnlockDiagnosticVMOptions -XX:+WhiteBoxAPI -XX:+UnlockExperimentalVMOptions -XX:-UseCompressedOops -XX:+UseCompactObjectHeaders BaseOffsets\n+ *\/\n+\/*\n+ * @test id=32bit\n+ * @library \/test\/lib \/\n+ * @requires vm.bits == \"32\"\n+ * @modules java.base\/jdk.internal.misc\n+ *          java.management\n+ * @build jdk.test.whitebox.WhiteBox\n+ * @run driver jdk.test.lib.helpers.ClassFileInstaller jdk.test.whitebox.WhiteBox\n+ * @run main\/othervm -Xbootclasspath\/a:. -XX:+UnlockDiagnosticVMOptions -XX:+WhiteBoxAPI BaseOffsets\n+ *\/\n+\n+import java.lang.reflect.Field;\n+import java.util.Arrays;\n+import java.util.Comparator;\n+import jdk.internal.misc.Unsafe;\n+\n+import jdk.test.lib.Asserts;\n+import jdk.test.lib.Platform;\n+import jdk.test.whitebox.WhiteBox;\n+\n+public class BaseOffsets {\n+\n+    static class LIClass {\n+        public int i;\n+    }\n+\n+    public static final WhiteBox WB = WhiteBox.getWhiteBox();\n+\n+    static final long INT_OFFSET;\n+    static final int  INT_ARRAY_OFFSET;\n+    static final int  LONG_ARRAY_OFFSET;\n+    static {\n+        if (!Platform.is64bit() || WB.getBooleanVMFlag(\"UseCompactObjectHeaders\")) {\n+            INT_OFFSET = 8;\n+            INT_ARRAY_OFFSET = 12;\n+            LONG_ARRAY_OFFSET = 16;\n+        } else if (WB.getBooleanVMFlag(\"UseCompressedClassPointers\")) {\n+            INT_OFFSET = 12;\n+            INT_ARRAY_OFFSET = 16;\n+            LONG_ARRAY_OFFSET = 16;\n+        } else {\n+            INT_OFFSET = 16;\n+            INT_ARRAY_OFFSET = 20;\n+            LONG_ARRAY_OFFSET = 24;\n+        }\n+    }\n+\n+    static public void main(String[] args) {\n+        Unsafe unsafe = Unsafe.getUnsafe();\n+        Class c = LIClass.class;\n+        Field[] fields = c.getFields();\n+        for (int i = 0; i < fields.length; i++) {\n+            long offset = unsafe.objectFieldOffset(fields[i]);\n+            if (fields[i].getType() == int.class) {\n+                Asserts.assertEquals(offset, INT_OFFSET, \"Misplaced int field\");\n+            } else {\n+                Asserts.fail(\"Unexpected field type\");\n+            }\n+        }\n+\n+        Asserts.assertEquals(unsafe.arrayBaseOffset(boolean[].class), INT_ARRAY_OFFSET,  \"Misplaced boolean array base\");\n+        Asserts.assertEquals(unsafe.arrayBaseOffset(byte[].class),    INT_ARRAY_OFFSET,  \"Misplaced byte    array base\");\n+        Asserts.assertEquals(unsafe.arrayBaseOffset(char[].class),    INT_ARRAY_OFFSET,  \"Misplaced char    array base\");\n+        Asserts.assertEquals(unsafe.arrayBaseOffset(short[].class),   INT_ARRAY_OFFSET,  \"Misplaced short   array base\");\n+        Asserts.assertEquals(unsafe.arrayBaseOffset(int[].class),     INT_ARRAY_OFFSET,  \"Misplaced int     array base\");\n+        Asserts.assertEquals(unsafe.arrayBaseOffset(long[].class),    LONG_ARRAY_OFFSET, \"Misplaced long    array base\");\n+        Asserts.assertEquals(unsafe.arrayBaseOffset(float[].class),   INT_ARRAY_OFFSET,  \"Misplaced float   array base\");\n+        Asserts.assertEquals(unsafe.arrayBaseOffset(double[].class),  LONG_ARRAY_OFFSET, \"Misplaced double  array base\");\n+        boolean narrowOops = System.getProperty(\"java.vm.compressedOopsMode\") != null ||\n+                             !Platform.is64bit();\n+        int expected_objary_offset = narrowOops ? INT_ARRAY_OFFSET : LONG_ARRAY_OFFSET;\n+        Asserts.assertEquals(unsafe.arrayBaseOffset(Object[].class),  expected_objary_offset, \"Misplaced object  array base\");\n+    }\n+}\n","filename":"test\/hotspot\/jtreg\/runtime\/FieldLayout\/BaseOffsets.java","additions":157,"deletions":0,"binary":false,"changes":157,"status":"added"},{"patch":"@@ -0,0 +1,122 @@\n+\/*\n+ * Copyright (c) 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2024, Red Hat, Inc. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+\/**\n+ * @test id=nocoops_nocoh\n+ * @summary Test Loading of default archives in all configurations\n+ * @requires vm.cds\n+ * @requires vm.bits == 64\n+ * @library \/test\/lib\n+ * @modules java.base\/jdk.internal.misc\n+ *          java.management\n+ * @run driver TestDefaultArchiveLoading nocoops_nocoh\n+ *\/\n+\n+\/**\n+ * @test id=nocoops_coh\n+ * @summary Test Loading of default archives in all configurations (requires --enable-cds-archive-coh)\n+ * @requires vm.cds\n+ * @requires vm.bits == 64\n+ * @requires !vm.gc.ZGenerational\n+ * @library \/test\/lib\n+ * @modules java.base\/jdk.internal.misc\n+ *          java.management\n+ * @run driver TestDefaultArchiveLoading nocoops_coh\n+ *\/\n+\n+\/**\n+ * @test id=coops_nocoh\n+ * @summary Test Loading of default archives in all configurations\n+ * @requires vm.cds\n+ * @requires vm.bits == 64\n+ * @library \/test\/lib\n+ * @modules java.base\/jdk.internal.misc\n+ *          java.management\n+ * @run driver TestDefaultArchiveLoading coops_nocoh\n+ *\/\n+\n+\/**\n+ * @test id=coops_coh\n+ * @summary Test Loading of default archives in all configurations (requires --enable-cds-archive-coh)\n+ * @requires vm.cds\n+ * @requires vm.bits == 64\n+ * @requires !vm.gc.ZGenerational\n+ * @library \/test\/lib\n+ * @modules java.base\/jdk.internal.misc\n+ *          java.management\n+ * @run driver TestDefaultArchiveLoading coops_coh\n+ *\/\n+\n+import jdk.test.lib.Platform;\n+import jdk.test.lib.process.OutputAnalyzer;\n+import jdk.test.lib.process.ProcessTools;\n+import jtreg.SkippedException;\n+\n+public class TestDefaultArchiveLoading {\n+    public static void main(String[] args) throws Exception {\n+\n+        if (args.length != 1) {\n+            throw new RuntimeException(\"Expected argument\");\n+        }\n+\n+        String archiveSuffix;\n+        char coh, coops;\n+\n+        switch (args[0]) {\n+            case \"nocoops_nocoh\":\n+                coh = coops = '-';\n+                archiveSuffix = \"_nocoops\";\n+                break;\n+            case \"nocoops_coh\":\n+                coops = '-';\n+                coh = '+';\n+                archiveSuffix = \"_nocoops_coh\";\n+                break;\n+            case \"coops_nocoh\":\n+                coops = '+';\n+                coh = '-';\n+                archiveSuffix = \"\";\n+                break;\n+            case \"coops_coh\":\n+                coh = coops = '+';\n+                archiveSuffix = \"_coh\";\n+                break;\n+            default: throw new RuntimeException(\"Invalid argument \" + args[0]);\n+        }\n+\n+        ProcessBuilder pb = ProcessTools.createLimitedTestJavaProcessBuilder(\n+                \"-XX:+UnlockExperimentalVMOptions\",\n+                \"-XX:\" + coh + \"UseCompactObjectHeaders\",\n+                \"-XX:\" + coops + \"UseCompressedOops\",\n+                \"-Xlog:cds\",\n+                \"-Xshare:on\", \/\/ fail if we cannot load archive\n+                \"-version\");\n+\n+        OutputAnalyzer output = new OutputAnalyzer(pb.start());\n+        output.shouldHaveExitValue(0);\n+\n+        output.shouldContain(\"classes\" + archiveSuffix + \".jsa\");\n+\n+    }\n+}\n","filename":"test\/hotspot\/jtreg\/runtime\/cds\/TestDefaultArchiveLoading.java","additions":122,"deletions":0,"binary":false,"changes":122,"status":"added"},{"patch":"@@ -59,0 +59,1 @@\n+         String compactHeaders = \"-XX:\" + (zGenerational.equals(\"-XX:+ZGenerational\") ? \"+\" : \"-\") + \"UseCompactObjectHeaders\";\n@@ -66,0 +67,2 @@\n+                                        \"-XX:+UnlockExperimentalVMOptions\",\n+                                        compactHeaders,\n@@ -75,0 +78,2 @@\n+                         \"-XX:+UnlockExperimentalVMOptions\",\n+                         compactHeaders,\n@@ -86,0 +91,2 @@\n+                         \"-XX:+UnlockExperimentalVMOptions\",\n+                         compactHeaders,\n@@ -98,0 +105,2 @@\n+                         \"-XX:+UnlockExperimentalVMOptions\",\n+                         compactHeaders,\n@@ -110,0 +119,2 @@\n+                         \"-XX:+UnlockExperimentalVMOptions\",\n+                         compactHeaders,\n@@ -121,0 +132,2 @@\n+                         \"-XX:+UnlockExperimentalVMOptions\",\n+                         compactHeaders,\n@@ -133,0 +146,2 @@\n+                         \"-XX:+UnlockExperimentalVMOptions\",\n+                         compactHeaders,\n@@ -146,0 +161,2 @@\n+                         \"-XX:+UnlockExperimentalVMOptions\",\n+                         compactHeaders,\n@@ -155,0 +172,2 @@\n+                         \"-XX:+UnlockExperimentalVMOptions\",\n+                         compactHeaders,\n","filename":"test\/hotspot\/jtreg\/runtime\/cds\/appcds\/TestZGCWithCDS.java","additions":19,"deletions":0,"binary":false,"changes":19,"status":"modified"},{"patch":"@@ -147,0 +147,1 @@\n+        removeDefaultArchive(java_home_dst, variant, \"_coh\");\n","filename":"test\/hotspot\/jtreg\/runtime\/cds\/appcds\/dynamicArchive\/TestAutoCreateSharedArchiveNoDefaultArchive.java","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -96,1 +96,1 @@\n-        \/\/ longConstant markWord::hash_mask_in_place 549755813632\n+        \/\/ longConstant markWord::hash_mask_in_place 4398046509056\n@@ -101,1 +101,1 @@\n-                       Platform.is64bit() ? 549755813632L: 4294967168L);\n+                       Platform.is64bit() ? 4398046509056L: 4294967168L);\n","filename":"test\/hotspot\/jtreg\/serviceability\/sa\/ClhsdbLongConstant.java","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -304,0 +304,1 @@\n+    private static final boolean COMPACT_HEADERS = Platform.is64bit() && WhiteBox.getWhiteBox().getBooleanVMFlag(\"UseCompactObjectHeaders\");\n@@ -377,0 +378,10 @@\n+    private static long expectedSmallObjSize() {\n+        long size;\n+        if (!Platform.is64bit() || COMPACT_HEADERS) {\n+            size = 8;\n+        } else {\n+            size = 16;\n+        }\n+        return roundUp(size, OBJ_ALIGN);\n+    }\n+\n@@ -378,1 +389,1 @@\n-        long expected = roundUp(Platform.is64bit() ? 16 : 8, OBJ_ALIGN);\n+        long expected = expectedSmallObjSize();\n@@ -385,1 +396,1 @@\n-        long expected = roundUp(Platform.is64bit() ? 16 : 8, OBJ_ALIGN);\n+        long expected = expectedSmallObjSize();\n@@ -395,1 +406,1 @@\n-        long expected = roundUp(Platform.is64bit() ? 16 : 8, OBJ_ALIGN);\n+        long expected = expectedSmallObjSize();\n","filename":"test\/jdk\/java\/lang\/instrument\/GetObjectSizeIntrinsicsTest.java","additions":14,"deletions":3,"binary":false,"changes":17,"status":"modified"},{"patch":"@@ -29,0 +29,1 @@\n+import jdk.test.whitebox.WhiteBox;\n@@ -47,1 +48,3 @@\n- * @run main CDSPluginTest\n+ * @build jdk.test.whitebox.WhiteBox\n+ * @run driver jdk.test.lib.helpers.ClassFileInstaller jdk.test.whitebox.WhiteBox\n+ * @run main\/othervm -XX:+UnlockDiagnosticVMOptions -XX:+WhiteBoxAPI -Xbootclasspath\/a:. CDSPluginTest\n@@ -51,1 +54,0 @@\n-\n@@ -78,0 +80,7 @@\n+        WhiteBox wb = WhiteBox.getWhiteBox();\n+        boolean COMPACT_HEADERS = Platform.is64bit() &&\n+                                  wb.getBooleanVMFlag(\"UseCompactObjectHeaders\") &&\n+                                  wb.isDefaultVMFlag(\"UseCompactObjectHeaders\");\n+\n+        String suffix = COMPACT_HEADERS ? \"_coh.jsa\" : \".jsa\";\n+\n@@ -80,1 +89,1 @@\n-                      new String[] { subDir + \"classes.jsa\", subDir + \"classes_nocoops.jsa\" });\n+                      new String[] { subDir + \"classes\" + suffix, subDir + \"classes_nocoops\" + suffix });\n@@ -83,1 +92,1 @@\n-                      new String[] { subDir + \"classes.jsa\" });\n+                      new String[] { subDir + \"classes\" + suffix });\n","filename":"test\/jdk\/tools\/jlink\/plugins\/CDSPluginTest.java","additions":13,"deletions":4,"binary":false,"changes":17,"status":"modified"}]}