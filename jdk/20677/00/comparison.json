{"files":[{"patch":"@@ -135,1 +135,1 @@\n-# Param2 - _nocoops, or empty\n+# Param2 - _nocoops, _coh, _nocoops_coh, or empty\n@@ -137,2 +137,8 @@\n-  $1_$2_DUMP_EXTRA_ARG := $(if $(filter _nocoops, $2),-XX:-UseCompressedOops,)\n-  $1_$2_DUMP_TYPE      := $(if $(filter _nocoops, $2),-NOCOOPS,)\n+  $1_$2_COOPS_OPTION := $(if $(findstring _nocoops, $2),-XX:-UseCompressedOops)\n+  # enable and also explicitly disable coh as needed.\n+  ifeq ($(call isTargetCpuBits, 64), true)\n+    $1_$2_COH_OPTION := -XX:+UnlockExperimentalVMOptions \\\n+                        $(if $(findstring _coh, $2),-XX:+UseCompactObjectHeaders,-XX:-UseCompactObjectHeaders)\n+  endif\n+  $1_$2_DUMP_EXTRA_ARG := $$($1_$2_COOPS_OPTION) $$($1_$2_COH_OPTION)\n+  $1_$2_DUMP_TYPE      := $(if $(findstring _nocoops, $2),-NOCOOPS,)$(if $(findstring _coh, $2),-COH,)\n@@ -193,0 +199,8 @@\n+    ifeq ($(BUILD_CDS_ARCHIVE_COH), true)\n+      $(foreach v, $(JVM_VARIANTS), \\\n+        $(eval $(call CreateCDSArchive,$v,_coh)) \\\n+      )\n+      $(foreach v, $(JVM_VARIANTS), \\\n+        $(eval $(call CreateCDSArchive,$v,_nocoops_coh)) \\\n+      )\n+    endif\n","filename":"make\/Images.gmk","additions":17,"deletions":3,"binary":false,"changes":20,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n-# Copyright (c) 2011, 2023, Oracle and\/or its affiliates. All rights reserved.\n+# Copyright (c) 2011, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -263,0 +263,1 @@\n+JDKOPT_ENABLE_DISABLE_CDS_ARCHIVE_COH\n","filename":"make\/autoconf\/configure.ac","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -676,0 +676,27 @@\n+################################################################################\n+#\n+# Enable or disable the default CDS archive generation for Compact Object Headers\n+#\n+AC_DEFUN([JDKOPT_ENABLE_DISABLE_CDS_ARCHIVE_COH],\n+[\n+  UTIL_ARG_ENABLE(NAME: cds-archive-coh, DEFAULT: auto, RESULT: BUILD_CDS_ARCHIVE_COH,\n+      DESC: [enable generation of default CDS archives for compact object headers (requires --enable-cds-archive)],\n+      DEFAULT_DESC: [auto],\n+      CHECKING_MSG: [if default CDS archives for compact object headers should be generated],\n+      CHECK_AVAILABLE: [\n+        AC_MSG_CHECKING([if CDS archive with compact object headers is available])\n+        if test \"x$BUILD_CDS_ARCHIVE\" = \"xfalse\"; then\n+          AC_MSG_RESULT([no (CDS default archive generation is disabled)])\n+          AVAILABLE=false\n+        elif test \"x$OPENJDK_TARGET_CPU\" != \"xx86_64\" &&\n+             test \"x$OPENJDK_TARGET_CPU\" != \"xaarch64\"; then\n+          AC_MSG_RESULT([no (compact object headers not supported for this platform)])\n+          AVAILABLE=false\n+        else\n+          AC_MSG_RESULT([yes])\n+          AVAILABLE=true\n+        fi\n+      ])\n+  AC_SUBST(BUILD_CDS_ARCHIVE_COH)\n+])\n+\n","filename":"make\/autoconf\/jdk-options.m4","additions":27,"deletions":0,"binary":false,"changes":27,"status":"modified"},{"patch":"@@ -373,0 +373,1 @@\n+BUILD_CDS_ARCHIVE_COH := @BUILD_CDS_ARCHIVE_COH@\n","filename":"make\/autoconf\/spec.gmk.template","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -6441,1 +6441,1 @@\n-  predicate(!needs_acquiring_load(n));\n+  predicate(!needs_acquiring_load(n) && !UseCompactObjectHeaders);\n@@ -6451,0 +6451,14 @@\n+instruct loadNKlassCompactHeaders(iRegNNoSp dst, memory mem, rFlagsReg cr)\n+%{\n+  match(Set dst (LoadNKlass mem));\n+  effect(KILL cr);\n+  predicate(!needs_acquiring_load(n) && UseCompactObjectHeaders);\n+\n+  ins_cost(4 * INSN_COST);\n+  format %{ \"ldrw  $dst, $mem\\t# compressed class ptr\" %}\n+  ins_encode %{\n+    __ load_nklass_compact($dst$$Register, $mem$$base$$Register, $mem$$index$$Register, $mem$$scale, $mem$$disp);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/aarch64.ad","additions":15,"deletions":1,"binary":false,"changes":16,"status":"modified"},{"patch":"@@ -2249,2 +2249,0 @@\n-  Address src_klass_addr = Address(src, oopDesc::klass_offset_in_bytes());\n-  Address dst_klass_addr = Address(dst, oopDesc::klass_offset_in_bytes());\n@@ -2311,9 +2309,1 @@\n-      if (UseCompressedClassPointers) {\n-        __ ldrw(tmp, src_klass_addr);\n-        __ ldrw(rscratch1, dst_klass_addr);\n-        __ cmpw(tmp, rscratch1);\n-      } else {\n-        __ ldr(tmp, src_klass_addr);\n-        __ ldr(rscratch1, dst_klass_addr);\n-        __ cmp(tmp, rscratch1);\n-      }\n+      __ cmp_klass(src, dst, tmp, rscratch1);\n@@ -2441,3 +2431,0 @@\n-    if (UseCompressedClassPointers) {\n-      __ encode_klass_not_null(tmp);\n-    }\n@@ -2446,8 +2433,1 @@\n-\n-      if (UseCompressedClassPointers) {\n-        __ ldrw(rscratch1, dst_klass_addr);\n-        __ cmpw(tmp, rscratch1);\n-      } else {\n-        __ ldr(rscratch1, dst_klass_addr);\n-        __ cmp(tmp, rscratch1);\n-      }\n+      __ cmp_klass(dst, tmp, rscratch1);\n@@ -2455,7 +2435,1 @@\n-      if (UseCompressedClassPointers) {\n-        __ ldrw(rscratch1, src_klass_addr);\n-        __ cmpw(tmp, rscratch1);\n-      } else {\n-        __ ldr(rscratch1, src_klass_addr);\n-        __ cmp(tmp, rscratch1);\n-      }\n+      __ cmp_klass(src, tmp, rscratch1);\n@@ -2464,7 +2438,1 @@\n-      if (UseCompressedClassPointers) {\n-        __ ldrw(rscratch1, dst_klass_addr);\n-        __ cmpw(tmp, rscratch1);\n-      } else {\n-        __ ldr(rscratch1, dst_klass_addr);\n-        __ cmp(tmp, rscratch1);\n-      }\n+      __ cmp_klass(dst, tmp, rscratch1);\n@@ -2554,1 +2522,6 @@\n-    __ ldrw(result, Address (obj, oopDesc::klass_offset_in_bytes()));\n+    if (UseCompactObjectHeaders) {\n+      __ ldr(result, Address(obj, oopDesc::mark_offset_in_bytes()));\n+      __ lsr(result, result, markWord::klass_shift);\n+    } else {\n+      __ ldrw(result, Address (obj, oopDesc::klass_offset_in_bytes()));\n+    }\n","filename":"src\/hotspot\/cpu\/aarch64\/c1_LIRAssembler_aarch64.cpp","additions":10,"deletions":37,"binary":false,"changes":47,"status":"modified"},{"patch":"@@ -178,3 +178,0 @@\n-  \/\/ This assumes that all prototype bits fit in an int32_t\n-  mov(t1, (int32_t)(intptr_t)markWord::prototype().value());\n-  str(t1, Address(obj, oopDesc::mark_offset_in_bytes()));\n@@ -182,3 +179,3 @@\n-  if (UseCompressedClassPointers) { \/\/ Take care not to kill klass\n-    encode_klass_not_null(t1, klass);\n-    strw(t1, Address(obj, oopDesc::klass_offset_in_bytes()));\n+  if (UseCompactObjectHeaders) {\n+    ldr(t1, Address(klass, Klass::prototype_header_offset()));\n+    str(t1, Address(obj, oopDesc::mark_offset_in_bytes()));\n@@ -186,1 +183,8 @@\n-    str(klass, Address(obj, oopDesc::klass_offset_in_bytes()));\n+    mov(t1, checked_cast<int32_t>(markWord::prototype().value()));\n+    str(t1, Address(obj, oopDesc::mark_offset_in_bytes()));\n+    if (UseCompressedClassPointers) { \/\/ Take care not to kill klass\n+      encode_klass_not_null(t1, klass);\n+      strw(t1, Address(obj, oopDesc::klass_offset_in_bytes()));\n+    } else {\n+      str(klass, Address(obj, oopDesc::klass_offset_in_bytes()));\n+    }\n@@ -197,1 +201,1 @@\n-  } else if (UseCompressedClassPointers) {\n+  } else if (UseCompressedClassPointers && !UseCompactObjectHeaders) {\n","filename":"src\/hotspot\/cpu\/aarch64\/c1_MacroAssembler_aarch64.cpp","additions":12,"deletions":8,"binary":false,"changes":20,"status":"modified"},{"patch":"@@ -2560,0 +2560,19 @@\n+\n+void C2_MacroAssembler::load_nklass_compact(Register dst, Register obj, Register index, int scale, int disp) {\n+  \/\/ Note: Don't clobber obj anywhere in that method!\n+\n+  \/\/ The incoming address is pointing into obj-start + klass_offset_in_bytes. We need to extract\n+  \/\/ obj-start, so that we can load from the object's mark-word instead. Usually the address\n+  \/\/ comes as obj-start in obj and klass_offset_in_bytes in disp. However, sometimes C2\n+  \/\/ emits code that pre-computes obj-start + klass_offset_in_bytes into a register, and\n+  \/\/ then passes that register as obj and 0 in disp. The following code extracts the base\n+  \/\/ and offset to load the mark-word.\n+  int offset = oopDesc::mark_offset_in_bytes() + disp - oopDesc::klass_offset_in_bytes();\n+  if (index == noreg) {\n+    ldr(dst, Address(obj, offset));\n+  } else {\n+    lea(dst, Address(obj, index, Address::lsl(scale)));\n+    ldr(dst, Address(dst, offset));\n+  }\n+  lsr(dst, dst, markWord::klass_shift);\n+}\n","filename":"src\/hotspot\/cpu\/aarch64\/c2_MacroAssembler_aarch64.cpp","additions":19,"deletions":0,"binary":false,"changes":19,"status":"modified"},{"patch":"@@ -182,0 +182,2 @@\n+  void load_nklass_compact(Register dst, Register obj, Register index, int scale, int disp);\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/c2_MacroAssembler_aarch64.hpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -121,1 +121,9 @@\n-void CompressedKlassPointers::initialize(address addr, size_t len) {\n+bool CompressedKlassPointers::pd_initialize(address addr, size_t len) {\n+\n+  if (tiny_classpointer_mode()) {\n+    \/\/ In tiny-classpointer mode, we do what all other platforms do.\n+    return false;\n+  }\n+\n+  \/\/ Aarch64 uses an own initialization logic that avoids zero-base shifted mode\n+  \/\/ (_base=0 _shift>0), instead preferring non-zero-based mode with shift=0\n@@ -125,1 +133,0 @@\n-  \/\/ Shift is always 0 on aarch64.\n@@ -128,1 +135,0 @@\n-  \/\/ On aarch64, we don't bother with zero-based encoding (base=0 shift>0).\n@@ -133,0 +139,9 @@\n+\n+#ifdef ASSERT\n+  _klass_range_start = addr;\n+  _klass_range_end = addr + len;\n+  calc_lowest_highest_narrow_klass_id();\n+  sanity_check_after_initialization();\n+#endif\n+\n+  return true;\n","filename":"src\/hotspot\/cpu\/aarch64\/compressedKlass_aarch64.cpp","additions":18,"deletions":3,"binary":false,"changes":21,"status":"modified"},{"patch":"@@ -1005,0 +1005,1 @@\n+  int extra_instructions = UseCompactObjectHeaders ? 1 : 0;\n@@ -1006,1 +1007,1 @@\n-    return NativeInstruction::instruction_size * 7;\n+    return NativeInstruction::instruction_size * (7 + extra_instructions);\n@@ -1008,1 +1009,1 @@\n-    return NativeInstruction::instruction_size * 5;\n+    return NativeInstruction::instruction_size * (5 + extra_instructions);\n@@ -1026,1 +1027,5 @@\n-  if (UseCompressedClassPointers) {\n+  if (UseCompactObjectHeaders) {\n+    load_nklass_compact(tmp1, receiver);\n+    ldrw(tmp2, Address(data, CompiledICData::speculated_klass_offset()));\n+    cmpw(tmp1, tmp2);\n+  } else if (UseCompressedClassPointers) {\n@@ -4841,0 +4846,11 @@\n+\/\/ Loads the obj's Klass* into dst.\n+\/\/ Preserves all registers (incl src, rscratch1 and rscratch2).\n+\/\/ Input:\n+\/\/ src - the oop we want to load the klass from.\n+\/\/ dst - output nklass.\n+void MacroAssembler::load_nklass_compact(Register dst, Register src) {\n+  assert(UseCompactObjectHeaders, \"expects UseCompactObjectHeaders\");\n+  ldr(dst, Address(src, oopDesc::mark_offset_in_bytes()));\n+  lsr(dst, dst, markWord::klass_shift);\n+}\n+\n@@ -4842,1 +4858,4 @@\n-  if (UseCompressedClassPointers) {\n+  if (UseCompactObjectHeaders) {\n+    load_nklass_compact(dst, src);\n+    decode_klass_not_null(dst);\n+  } else if (UseCompressedClassPointers) {\n@@ -4898,0 +4917,1 @@\n+  assert_different_registers(oop, trial_klass, tmp);\n@@ -4899,1 +4919,5 @@\n-    ldrw(tmp, Address(oop, oopDesc::klass_offset_in_bytes()));\n+    if (UseCompactObjectHeaders) {\n+      load_nklass_compact(tmp, oop);\n+    } else {\n+      ldrw(tmp, Address(oop, oopDesc::klass_offset_in_bytes()));\n+    }\n@@ -4916,0 +4940,16 @@\n+void MacroAssembler::cmp_klass(Register src, Register dst, Register tmp1, Register tmp2) {\n+  if (UseCompactObjectHeaders) {\n+    load_nklass_compact(tmp1, src);\n+    load_nklass_compact(tmp2, dst);\n+    cmpw(tmp1, tmp2);\n+  } else if (UseCompressedClassPointers) {\n+    ldrw(tmp1, Address(src, oopDesc::klass_offset_in_bytes()));\n+    ldrw(tmp2, Address(dst, oopDesc::klass_offset_in_bytes()));\n+    cmpw(tmp1, tmp2);\n+  } else {\n+    ldr(tmp1, Address(src, oopDesc::klass_offset_in_bytes()));\n+    ldr(tmp2, Address(dst, oopDesc::klass_offset_in_bytes()));\n+    cmp(tmp1, tmp2);\n+  }\n+}\n+\n@@ -4919,0 +4959,1 @@\n+  assert(!UseCompactObjectHeaders, \"not with compact headers\");\n@@ -4928,0 +4969,1 @@\n+  assert(!UseCompactObjectHeaders, \"not with compact headers\");\n@@ -5076,3 +5118,0 @@\n-  assert(LogKlassAlignmentInBytes == CompressedKlassPointers::shift()\n-         || 0 == CompressedKlassPointers::shift(), \"decode alg wrong\");\n-\n@@ -5104,1 +5143,1 @@\n-      lsr(dst, src, LogKlassAlignmentInBytes);\n+      lsr(dst, src, CompressedKlassPointers::shift());\n@@ -5113,1 +5152,1 @@\n-      lsr(dst, dst, LogKlassAlignmentInBytes);\n+      lsr(dst, dst, CompressedKlassPointers::shift());\n@@ -5121,1 +5160,1 @@\n-      ubfx(dst, src, LogKlassAlignmentInBytes, 32);\n+      ubfx(dst, src, CompressedKlassPointers::shift(), 32);\n@@ -5143,1 +5182,1 @@\n-      lsl(dst, src, LogKlassAlignmentInBytes);\n+      lsl(dst, src, CompressedKlassPointers::shift());\n@@ -5151,1 +5190,1 @@\n-      lsl(dst, src, LogKlassAlignmentInBytes);\n+      lsl(dst, src, CompressedKlassPointers::shift());\n@@ -5166,1 +5205,1 @@\n-      lsl(dst, dst, LogKlassAlignmentInBytes);\n+      lsl(dst, dst, CompressedKlassPointers::shift());\n","filename":"src\/hotspot\/cpu\/aarch64\/macroAssembler_aarch64.cpp","additions":53,"deletions":14,"binary":false,"changes":67,"status":"modified"},{"patch":"@@ -878,0 +878,1 @@\n+  void load_nklass_compact(Register dst, Register src);\n@@ -881,0 +882,1 @@\n+  void cmp_klass(Register src, Register dst, Register tmp1, Register tmp2);\n","filename":"src\/hotspot\/cpu\/aarch64\/macroAssembler_aarch64.hpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -3631,1 +3631,6 @@\n-    __ sub(r3, r3, sizeof(oopDesc));\n+    if (UseCompactObjectHeaders) {\n+      assert(is_aligned(oopDesc::base_offset_in_bytes(), BytesPerLong), \"oop base offset must be 8-byte-aligned\");\n+      __ sub(r3, r3, oopDesc::base_offset_in_bytes());\n+    } else {\n+      __ sub(r3, r3, sizeof(oopDesc));\n+    }\n@@ -3636,1 +3641,6 @@\n-      __ add(r2, r0, sizeof(oopDesc));\n+      if (UseCompactObjectHeaders) {\n+        assert(is_aligned(oopDesc::base_offset_in_bytes(), BytesPerLong), \"oop base offset must be 8-byte-aligned\");\n+        __ add(r2, r0, oopDesc::base_offset_in_bytes());\n+      } else {\n+        __ add(r2, r0, sizeof(oopDesc));\n+      }\n@@ -3646,4 +3656,9 @@\n-    __ mov(rscratch1, (intptr_t)markWord::prototype().value());\n-    __ str(rscratch1, Address(r0, oopDesc::mark_offset_in_bytes()));\n-    __ store_klass_gap(r0, zr);  \/\/ zero klass gap for compressed oops\n-    __ store_klass(r0, r4);      \/\/ store klass last\n+    if (UseCompactObjectHeaders) {\n+      __ ldr(rscratch1, Address(r4, Klass::prototype_header_offset()));\n+      __ str(rscratch1, Address(r0, oopDesc::mark_offset_in_bytes()));\n+    } else {\n+      __ mov(rscratch1, (intptr_t)markWord::prototype().value());\n+      __ str(rscratch1, Address(r0, oopDesc::mark_offset_in_bytes()));\n+      __ store_klass_gap(r0, zr);  \/\/ zero klass gap for compressed oops\n+      __ store_klass(r0, r4);      \/\/ store klass last\n+    }\n","filename":"src\/hotspot\/cpu\/aarch64\/templateTable_aarch64.cpp","additions":21,"deletions":6,"binary":false,"changes":27,"status":"modified"},{"patch":"@@ -49,0 +49,2 @@\n+\n+bool CompressedKlassPointers::pd_initialize(address addr, size_t len) { return false; }\n","filename":"src\/hotspot\/cpu\/ppc\/compressedKlass_ppc.cpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -170,1 +170,1 @@\n-\n+\/\/ Todo UseCompactObjectHeaders\n","filename":"src\/hotspot\/cpu\/riscv\/c1_MacroAssembler_riscv.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -59,1 +59,1 @@\n-  \/\/ Failing that, optimize for case (3) - a base with only bits set between [33-44)\n+  \/\/ Failing that, optimize for case (3) - a base with only bits set between [32-44)\n@@ -61,1 +61,1 @@\n-    const uintptr_t from = nth_bit(32 + (optimize_for_zero_base ? LogKlassAlignmentInBytes : 0));\n+    const uintptr_t from = nth_bit(32);\n@@ -77,0 +77,2 @@\n+\n+bool CompressedKlassPointers::pd_initialize(address addr, size_t len) { return false; }\n","filename":"src\/hotspot\/cpu\/riscv\/compressedKlass_riscv.cpp","additions":4,"deletions":2,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -2737,2 +2737,1 @@\n-      assert(LogKlassAlignmentInBytes == CompressedKlassPointers::shift(), \"decode alg wrong\");\n-      slli(dst, src, LogKlassAlignmentInBytes);\n+      slli(dst, src, CompressedKlassPointers::shift());\n@@ -2754,1 +2753,0 @@\n-    assert(LogKlassAlignmentInBytes == CompressedKlassPointers::shift(), \"decode alg wrong\");\n@@ -2756,1 +2754,1 @@\n-    shadd(dst, src, xbase, t0, LogKlassAlignmentInBytes);\n+    shadd(dst, src, xbase, t0, CompressedKlassPointers::shift());\n@@ -2772,2 +2770,1 @@\n-      assert(LogKlassAlignmentInBytes == CompressedKlassPointers::shift(), \"decode alg wrong\");\n-      srli(dst, src, LogKlassAlignmentInBytes);\n+      srli(dst, src, CompressedKlassPointers::shift());\n@@ -2795,2 +2792,1 @@\n-    assert(LogKlassAlignmentInBytes == CompressedKlassPointers::shift(), \"decode alg wrong\");\n-    srli(dst, dst, LogKlassAlignmentInBytes);\n+    srli(dst, dst, CompressedKlassPointers::shift());\n","filename":"src\/hotspot\/cpu\/riscv\/macroAssembler_riscv.cpp","additions":4,"deletions":8,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -53,0 +53,2 @@\n+\n+bool CompressedKlassPointers::pd_initialize(address addr, size_t len) { return false; }\n","filename":"src\/hotspot\/cpu\/s390\/compressedKlass_s390.cpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -3824,1 +3824,1 @@\n-  z_tmll(current, KlassAlignmentInBytes-1); \/\/ Check alignment.\n+  z_tmll(current, CompressedKlassPointers::klass_alignment_in_bytes() - 1); \/\/ Check alignment.\n@@ -3838,1 +3838,0 @@\n-    assert (LogKlassAlignmentInBytes == shift, \"decode alg wrong\");\n@@ -3968,1 +3967,1 @@\n-  z_tmll(dst, KlassAlignmentInBytes-1); \/\/ Check alignment.\n+  z_tmll(dst, CompressedKlassPointers::klass_alignment_in_bytes() - 1); \/\/ Check alignment.\n@@ -4015,1 +4014,1 @@\n-  z_tmll(dst, KlassAlignmentInBytes-1); \/\/ Check alignment.\n+  z_tmll(dst, CompressedKlassPointers::klass_alignment_in_bytes() - 1); \/\/ Check alignment.\n@@ -4084,1 +4083,5 @@\n-    assert((shift == 0) || (shift == LogKlassAlignmentInBytes), \"cKlass encoder detected bad shift\");\n+    if (CompressedKlassPointers::tiny_classpointer_mode()) {\n+      assert(shift >= 3, \"cKlass encoder detected bad shift\");\n+    } else {\n+      assert((shift == 0) || (shift == 3), \"cKlass encoder detected bad shift\");\n+    }\n","filename":"src\/hotspot\/cpu\/s390\/macroAssembler_s390.cpp","additions":8,"deletions":5,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -3051,0 +3051,1 @@\n+  Register tmp2 = UseCompactObjectHeaders ? rscratch2 : noreg;\n@@ -3175,2 +3176,0 @@\n-  Address src_klass_addr = Address(src, oopDesc::klass_offset_in_bytes());\n-  Address dst_klass_addr = Address(dst, oopDesc::klass_offset_in_bytes());\n@@ -3242,7 +3241,1 @@\n-      if (UseCompressedClassPointers) {\n-        __ movl(tmp, src_klass_addr);\n-        __ cmpl(tmp, dst_klass_addr);\n-      } else {\n-        __ movptr(tmp, src_klass_addr);\n-        __ cmpptr(tmp, dst_klass_addr);\n-      }\n+      __ cmp_klass(src, dst, tmp, tmp2);\n@@ -3307,0 +3300,1 @@\n+       Address dst_klass_addr = Address(dst, oopDesc::klass_offset_in_bytes());\n@@ -3410,3 +3404,1 @@\n-\n-      if (UseCompressedClassPointers)          __ cmpl(tmp, dst_klass_addr);\n-      else                   __ cmpptr(tmp, dst_klass_addr);\n+      __ cmp_klass(tmp, dst, tmp2);\n@@ -3414,2 +3406,1 @@\n-      if (UseCompressedClassPointers)          __ cmpl(tmp, src_klass_addr);\n-      else                   __ cmpptr(tmp, src_klass_addr);\n+      __ cmp_klass(tmp, src, tmp2);\n@@ -3418,2 +3409,1 @@\n-      if (UseCompressedClassPointers)          __ cmpl(tmp, dst_klass_addr);\n-      else                   __ cmpptr(tmp, dst_klass_addr);\n+      __ cmp_klass(tmp, dst, tmp2);\n@@ -3517,1 +3507,9 @@\n-  if (UseCompressedClassPointers) {\n+  if (UseCompactObjectHeaders) {\n+    Register tmp = rscratch1;\n+    assert_different_registers(tmp, obj);\n+    assert_different_registers(tmp, result);\n+\n+    __ movq(result, Address(obj, oopDesc::mark_offset_in_bytes()));\n+    __ shrq(result, markWord::klass_shift);\n+    __ decode_klass_not_null(result, tmp);\n+  } else if (UseCompressedClassPointers) {\n@@ -3522,0 +3520,1 @@\n+  {\n@@ -3523,0 +3522,1 @@\n+  }\n","filename":"src\/hotspot\/cpu\/x86\/c1_LIRAssembler_x86.cpp","additions":17,"deletions":17,"binary":false,"changes":34,"status":"modified"},{"patch":"@@ -174,2 +174,1 @@\n-  assert_different_registers(obj, klass, len);\n-  movptr(Address(obj, oopDesc::mark_offset_in_bytes()), checked_cast<int32_t>(markWord::prototype().value()));\n+  assert_different_registers(obj, klass, len, t1, t2);\n@@ -177,1 +176,5 @@\n-  if (UseCompressedClassPointers) { \/\/ Take care not to kill klass\n+  if (UseCompactObjectHeaders) {\n+    movptr(t1, Address(klass, Klass::prototype_header_offset()));\n+    movptr(Address(obj, oopDesc::mark_offset_in_bytes()), t1);\n+  } else if (UseCompressedClassPointers) { \/\/ Take care not to kill klass\n+    movptr(Address(obj, oopDesc::mark_offset_in_bytes()), checked_cast<int32_t>(markWord::prototype().value()));\n@@ -184,0 +187,1 @@\n+    movptr(Address(obj, oopDesc::mark_offset_in_bytes()), checked_cast<int32_t>(markWord::prototype().value()));\n@@ -200,1 +204,1 @@\n-  else if (UseCompressedClassPointers) {\n+  else if (UseCompressedClassPointers && !UseCompactObjectHeaders) {\n@@ -234,1 +238,3 @@\n-\n+  if (UseCompactObjectHeaders) {\n+    assert(hdr_size_in_bytes == 8, \"check object headers size\");\n+  }\n","filename":"src\/hotspot\/cpu\/x86\/c1_MacroAssembler_x86.cpp","additions":11,"deletions":5,"binary":false,"changes":16,"status":"modified"},{"patch":"@@ -6513,0 +6513,16 @@\n+\n+#ifdef _LP64\n+void C2_MacroAssembler::load_nklass_compact_c2(Register dst, Register obj, Register index, Address::ScaleFactor scale, int disp) {\n+  \/\/ Note: Don't clobber obj anywhere in that method!\n+\n+  \/\/ The incoming address is pointing into obj-start + klass_offset_in_bytes. We need to extract\n+  \/\/ obj-start, so that we can load from the object's mark-word instead. Usually the address\n+  \/\/ comes as obj-start in obj and klass_offset_in_bytes in disp. However, sometimes C2\n+  \/\/ emits code that pre-computes obj-start + klass_offset_in_bytes into a register, and\n+  \/\/ then passes that register as obj and 0 in disp. The following code extracts the base\n+  \/\/ and offset to load the mark-word.\n+  int offset = oopDesc::mark_offset_in_bytes() + disp - oopDesc::klass_offset_in_bytes();\n+  movq(dst, Address(obj, index, scale, offset));\n+  shrq(dst, markWord::klass_shift);\n+}\n+#endif\n","filename":"src\/hotspot\/cpu\/x86\/c2_MacroAssembler_x86.cpp","additions":16,"deletions":0,"binary":false,"changes":16,"status":"modified"},{"patch":"@@ -508,0 +508,2 @@\n+  void load_nklass_compact_c2(Register dst, Register obj, Register index, Address::ScaleFactor scale, int disp);\n+\n","filename":"src\/hotspot\/cpu\/x86\/c2_MacroAssembler_x86.hpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -49,0 +49,2 @@\n+bool CompressedKlassPointers::pd_initialize(address addr, size_t len) { return false; }\n+\n","filename":"src\/hotspot\/cpu\/x86\/compressedKlass_x86.cpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -1353,1 +1353,2 @@\n-  return LP64_ONLY(14) NOT_LP64(12);\n+  return\n+      LP64_ONLY(UseCompactObjectHeaders ? 17 : 14) NOT_LP64(12);\n@@ -1369,0 +1370,6 @@\n+#ifdef _LP64\n+  if (UseCompactObjectHeaders) {\n+    load_nklass_compact(temp, receiver);\n+    cmpl(temp, Address(data, CompiledICData::speculated_klass_offset()));\n+  } else\n+#endif\n@@ -1379,1 +1386,1 @@\n-  assert((offset() % end_alignment) == 0, \"Misaligned verified entry point\");\n+  assert((offset() % end_alignment) == 0, \"Misaligned verified entry point (%d, %d, %d)\", uep_offset, offset(), end_alignment);\n@@ -5676,0 +5683,8 @@\n+#ifdef _LP64\n+void MacroAssembler::load_nklass_compact(Register dst, Register src) {\n+  assert(UseCompactObjectHeaders, \"expect compact object headers\");\n+  movq(dst, Address(src, oopDesc::mark_offset_in_bytes()));\n+  shrq(dst, markWord::klass_shift);\n+}\n+#endif\n+\n@@ -5677,0 +5692,1 @@\n+  BLOCK_COMMENT(\"load_klass\");\n@@ -5680,1 +5696,4 @@\n-  if (UseCompressedClassPointers) {\n+  if (UseCompactObjectHeaders) {\n+    load_nklass_compact(dst, src);\n+    decode_klass_not_null(dst, tmp);\n+  } else if (UseCompressedClassPointers) {\n@@ -5685,0 +5704,1 @@\n+  {\n@@ -5686,0 +5706,1 @@\n+  }\n@@ -5689,0 +5710,1 @@\n+  assert(!UseCompactObjectHeaders, \"not with compact headers\");\n@@ -5700,0 +5722,35 @@\n+void MacroAssembler::cmp_klass(Register klass, Register obj, Register tmp) {\n+  BLOCK_COMMENT(\"cmp_klass 1\");\n+#ifdef _LP64\n+  if (UseCompactObjectHeaders) {\n+    load_nklass_compact(tmp, obj);\n+    cmpl(klass, tmp);\n+  } else if (UseCompressedClassPointers) {\n+    cmpl(klass, Address(obj, oopDesc::klass_offset_in_bytes()));\n+  } else\n+#endif\n+  {\n+    cmpptr(klass, Address(obj, oopDesc::klass_offset_in_bytes()));\n+  }\n+}\n+\n+void MacroAssembler::cmp_klass(Register src, Register dst, Register tmp1, Register tmp2) {\n+  BLOCK_COMMENT(\"cmp_klass 2\");\n+#ifdef _LP64\n+  if (UseCompactObjectHeaders) {\n+    assert(tmp2 != noreg, \"need tmp2\");\n+    assert_different_registers(src, dst, tmp1, tmp2);\n+    load_nklass_compact(tmp1, src);\n+    load_nklass_compact(tmp2, dst);\n+    cmpl(tmp1, tmp2);\n+  } else if (UseCompressedClassPointers) {\n+    movl(tmp1, Address(src, oopDesc::klass_offset_in_bytes()));\n+    cmpl(tmp1, Address(dst, oopDesc::klass_offset_in_bytes()));\n+  } else\n+#endif\n+  {\n+    movptr(tmp1, Address(src, oopDesc::klass_offset_in_bytes()));\n+    cmpptr(tmp1, Address(dst, oopDesc::klass_offset_in_bytes()));\n+  }\n+}\n+\n@@ -5747,0 +5804,1 @@\n+  assert(!UseCompactObjectHeaders, \"Don't use with compact headers\");\n@@ -5911,2 +5969,1 @@\n-    assert (LogKlassAlignmentInBytes == CompressedKlassPointers::shift(), \"decode alg wrong\");\n-    shrq(r, LogKlassAlignmentInBytes);\n+    shrq(r, CompressedKlassPointers::shift());\n@@ -5925,2 +5982,1 @@\n-    assert (LogKlassAlignmentInBytes == CompressedKlassPointers::shift(), \"decode alg wrong\");\n-    shrq(dst, LogKlassAlignmentInBytes);\n+    shrq(dst, CompressedKlassPointers::shift());\n@@ -5938,2 +5994,1 @@\n-    assert(LogKlassAlignmentInBytes == CompressedKlassPointers::shift(), \"decode alg wrong\");\n-    shlq(r, LogKlassAlignmentInBytes);\n+    shlq(r, CompressedKlassPointers::shift());\n@@ -5961,9 +6016,12 @@\n-    if (CompressedKlassPointers::base() != nullptr) {\n-      mov64(dst, (int64_t)CompressedKlassPointers::base());\n-    } else {\n-      xorq(dst, dst);\n-    }\n-    if (CompressedKlassPointers::shift() != 0) {\n-      assert(LogKlassAlignmentInBytes == CompressedKlassPointers::shift(), \"decode alg wrong\");\n-      assert(LogKlassAlignmentInBytes == Address::times_8, \"klass not aligned on 64bits?\");\n-      leaq(dst, Address(dst, src, Address::times_8, 0));\n+    if (CompressedKlassPointers::shift() <= Address::times_8) {\n+      if (CompressedKlassPointers::base() != nullptr) {\n+        mov64(dst, (int64_t)CompressedKlassPointers::base());\n+      } else {\n+        xorq(dst, dst);\n+      }\n+      if (CompressedKlassPointers::shift() != 0) {\n+        assert(CompressedKlassPointers::shift() == Address::times_8, \"klass not aligned on 64bits?\");\n+        leaq(dst, Address(dst, src, Address::times_8, 0));\n+      } else {\n+        addq(dst, src);\n+      }\n@@ -5971,0 +6029,7 @@\n+      if (CompressedKlassPointers::base() != nullptr) {\n+        const uint64_t base_right_shifted =\n+            (uint64_t)CompressedKlassPointers::base() >> CompressedKlassPointers::shift();\n+        mov64(dst, base_right_shifted);\n+      } else {\n+        xorq(dst, dst);\n+      }\n@@ -5972,0 +6037,1 @@\n+      shlq(dst, CompressedKlassPointers::shift());\n","filename":"src\/hotspot\/cpu\/x86\/macroAssembler_x86.cpp","additions":84,"deletions":18,"binary":false,"changes":102,"status":"modified"},{"patch":"@@ -366,0 +366,3 @@\n+#ifdef _LP64\n+  void load_nklass_compact(Register dst, Register src);\n+#endif\n@@ -369,0 +372,8 @@\n+  \/\/ Compares the Klass pointer of an object to a given Klass (which might be narrow,\n+  \/\/ depending on UseCompressedClassPointers).\n+  void cmp_klass(Register klass, Register dst, Register tmp);\n+\n+  \/\/ Compares the Klass pointer of two objects o1 and o2. Result is in the condition flags.\n+  \/\/ Uses tmp1 and tmp2 as temporary registers.\n+  void cmp_klass(Register src, Register dst, Register tmp1, Register tmp2);\n+\n","filename":"src\/hotspot\/cpu\/x86\/macroAssembler_x86.hpp","additions":11,"deletions":0,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -96,1 +96,1 @@\n-    return (LogKlassAlignmentInBytes <= 3);\n+    return (CompressedKlassPointers::shift() <= 3);\n","filename":"src\/hotspot\/cpu\/x86\/matcher_x86.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -4007,1 +4007,1 @@\n-  if ((UseAVX == 2) && EnableX86ECoreOpts) {\n+  if ((UseAVX == 2) && EnableX86ECoreOpts && !UseCompactObjectHeaders) {\n","filename":"src\/hotspot\/cpu\/x86\/stubGenerator_x86_64.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -4087,1 +4087,6 @@\n-    __ decrement(rdx, sizeof(oopDesc));\n+    if (UseCompactObjectHeaders) {\n+      assert(is_aligned(oopDesc::base_offset_in_bytes(), BytesPerLong), \"oop base offset must be 8-byte-aligned\");\n+      __ decrement(rdx, oopDesc::base_offset_in_bytes());\n+    } else {\n+      __ decrement(rdx, sizeof(oopDesc));\n+    }\n@@ -4109,2 +4114,9 @@\n-    __ movptr(Address(rax, rdx, Address::times_8, sizeof(oopDesc) - 1*oopSize), rcx);\n-    NOT_LP64(__ movptr(Address(rax, rdx, Address::times_8, sizeof(oopDesc) - 2*oopSize), rcx));\n+    if (UseCompactObjectHeaders) {\n+      assert(is_aligned(oopDesc::base_offset_in_bytes(), BytesPerLong), \"oop base offset must be 8-byte-aligned\");\n+      int header_size = oopDesc::base_offset_in_bytes();\n+      __ movptr(Address(rax, rdx, Address::times_8, header_size - 1*oopSize), rcx);\n+      NOT_LP64(__ movptr(Address(rax, rdx, Address::times_8, header_size - 2*oopSize), rcx));\n+    } else {\n+      __ movptr(Address(rax, rdx, Address::times_8, sizeof(oopDesc) - 1*oopSize), rcx);\n+      NOT_LP64(__ movptr(Address(rax, rdx, Address::times_8, sizeof(oopDesc) - 2*oopSize), rcx));\n+    }\n@@ -4117,3 +4129,8 @@\n-    __ movptr(Address(rax, oopDesc::mark_offset_in_bytes()),\n-              (intptr_t)markWord::prototype().value()); \/\/ header\n-    __ pop(rcx);   \/\/ get saved klass back in the register.\n+    if (UseCompactObjectHeaders) {\n+      __ pop(rcx);   \/\/ get saved klass back in the register.\n+      __ movptr(rbx, Address(rcx, Klass::prototype_header_offset()));\n+      __ movptr(Address(rax, oopDesc::mark_offset_in_bytes()), rbx);\n+    } else {\n+      __ movptr(Address(rax, oopDesc::mark_offset_in_bytes()),\n+                (intptr_t)markWord::prototype().value()); \/\/ header\n+      __ pop(rcx);   \/\/ get saved klass back in the register.\n@@ -4121,2 +4138,2 @@\n-    __ xorl(rsi, rsi); \/\/ use zero reg to clear memory (shorter code)\n-    __ store_klass_gap(rax, rsi);  \/\/ zero klass gap for compressed oops\n+      __ xorl(rsi, rsi); \/\/ use zero reg to clear memory (shorter code)\n+      __ store_klass_gap(rax, rsi);  \/\/ zero klass gap for compressed oops\n@@ -4124,1 +4141,2 @@\n-    __ store_klass(rax, rcx, rscratch1);  \/\/ klass\n+      __ store_klass(rax, rcx, rscratch1);  \/\/ klass\n+    }\n","filename":"src\/hotspot\/cpu\/x86\/templateTable_x86.cpp","additions":27,"deletions":9,"binary":false,"changes":36,"status":"modified"},{"patch":"@@ -4372,0 +4372,1 @@\n+  predicate(!UseCompactObjectHeaders);\n@@ -4382,0 +4383,15 @@\n+instruct loadNKlassCompactHeaders(rRegN dst, memory mem, rFlagsReg cr)\n+%{\n+  predicate(UseCompactObjectHeaders);\n+  match(Set dst (LoadNKlass mem));\n+  effect(KILL cr);\n+  ins_cost(125); \/\/ XXX\n+  format %{ \"movl    $dst, $mem\\t# compressed klass ptr\" %}\n+  ins_encode %{\n+    Register index = $mem$$index != 4 ? $mem$$index$$Register : noreg;\n+    Address::ScaleFactor sf = (index != noreg) ? static_cast<Address::ScaleFactor>($mem$$scale) : Address::no_scale;\n+    __ load_nklass_compact_c2($dst$$Register, $mem$$base$$Register, index, sf, $mem$$disp);\n+  %}\n+  ins_pipe(pipe_slow); \/\/ XXX\n+%}\n+\n@@ -11719,0 +11735,1 @@\n+  predicate(!UseCompactObjectHeaders);\n","filename":"src\/hotspot\/cpu\/x86\/x86_64.ad","additions":17,"deletions":0,"binary":false,"changes":17,"status":"modified"},{"patch":"@@ -230,2 +230,4 @@\n-    \/\/ See RunTimeClassInfo::get_for()\n-    _estimated_metaspaceobj_bytes += align_up(BytesPerWord, SharedSpaceObjectAlignment);\n+    \/\/ See RunTimeClassInfo::get_for(): make sure we have enough space for both maximum\n+    \/\/ Klass alignment as well as the RuntimeInfo* pointer we will embed in front of a Klass.\n+    _estimated_metaspaceobj_bytes += align_up(BytesPerWord, CompressedKlassPointers::klass_alignment_in_bytes()) +\n+        align_up(sizeof(void*), SharedSpaceObjectAlignment);\n@@ -664,1 +666,1 @@\n-    \/\/ Save a pointer immediate in front of an InstanceKlass, so\n+    \/\/ Allocate space for a pointer directly in front of the future InstanceKlass, so\n@@ -673,0 +675,12 @@\n+    \/\/ Allocate space for the future InstanceKlass with proper alignment\n+    const size_t alignment =\n+#ifdef _LP64\n+      UseCompressedClassPointers ?\n+        nth_bit(ArchiveBuilder::precomputed_narrow_klass_shift()) :\n+        SharedSpaceObjectAlignment;\n+#else\n+    SharedSpaceObjectAlignment;\n+#endif\n+    dest = dump_region->allocate(bytes, alignment);\n+  } else {\n+    dest = dump_region->allocate(bytes);\n@@ -674,1 +688,0 @@\n-  dest = dump_region->allocate(bytes);\n@@ -705,0 +718,2 @@\n+\n+  DEBUG_ONLY(_alloc_stats.verify((int)dump_region->used(), src_info->read_only()));\n@@ -783,0 +798,9 @@\n+#ifdef _LP64\n+    if (UseCompactObjectHeaders) {\n+      Klass* requested_k = to_requested(k);\n+      address narrow_klass_base = _requested_static_archive_bottom; \/\/ runtime encoding base == runtime mapping start\n+      const int narrow_klass_shift = precomputed_narrow_klass_shift();\n+      narrowKlass nk = CompressedKlassPointers::encode_not_null_without_asserts(requested_k, narrow_klass_base, narrow_klass_shift);\n+      k->set_prototype_header(markWord::prototype().set_narrow_klass(nk));\n+    }\n+#endif \/\/_LP64\n@@ -880,0 +904,5 @@\n+  const int narrow_klass_shift = ArchiveBuilder::precomputed_narrow_klass_shift();\n+#ifdef ASSERT\n+  const size_t klass_alignment = MAX2(SharedSpaceObjectAlignment, (size_t)nth_bit(narrow_klass_shift));\n+  assert(is_aligned(k, klass_alignment), \"Klass \" PTR_FORMAT \" misaligned.\", p2i(k));\n+#endif\n@@ -881,2 +910,3 @@\n-  const int narrow_klass_shift = ArchiveHeapWriter::precomputed_narrow_klass_shift;\n-  return CompressedKlassPointers::encode_not_null(requested_k, narrow_klass_base, narrow_klass_shift);\n+  \/\/ Note: use the \"raw\" version of encode that takes explicit narrow klass base and shift. Don't use any\n+  \/\/ of the variants that do sanity checks, nor any of those that use the current - dump - JVM's encoding setting.\n+  return CompressedKlassPointers::encode_not_null_without_asserts(requested_k, narrow_klass_base, narrow_klass_shift);\n@@ -962,0 +992,14 @@\n+#ifdef _LP64\n+int ArchiveBuilder::precomputed_narrow_klass_shift() {\n+  \/\/ Legacy Mode:\n+  \/\/    We use 32 bits for narrowKlass, which should cover the full 4G Klass range. Shift can be 0.\n+  \/\/ CompactObjectHeader Mode:\n+  \/\/    narrowKlass is much smaller, and we use the highest possible shift value to later get the maximum\n+  \/\/    Klass encoding range.\n+  \/\/\n+  \/\/ Note that all of this may change in the future, if we decide to correct the pre-calculated\n+  \/\/ narrow Klass IDs at archive load time.\n+  assert(UseCompressedClassPointers, \"Only needed for compressed class pointers\");\n+  return CompressedKlassPointers::tiny_classpointer_mode() ?  CompressedKlassPointers::max_shift() : 0;\n+}\n+#endif \/\/ _LP64\n","filename":"src\/hotspot\/share\/cds\/archiveBuilder.cpp","additions":50,"deletions":6,"binary":false,"changes":56,"status":"modified"},{"patch":"@@ -30,0 +30,1 @@\n+#include \"memory\/metaspace.hpp\"\n@@ -46,3 +47,3 @@\n-\/\/ Metaspace::allocate() requires that all blocks must be aligned with KlassAlignmentInBytes.\n-\/\/ We enforce the same alignment rule in blocks allocated from the shared space.\n-const int SharedSpaceObjectAlignment = KlassAlignmentInBytes;\n+\/\/ The minimum alignment for non-Klass objects inside the CDS archive. Klass objects need\n+\/\/ to follow CompressedKlassPointers::klass_alignment_in_bytes().\n+constexpr size_t SharedSpaceObjectAlignment = Metaspace::min_allocation_alignment_bytes;\n@@ -463,0 +464,19 @@\n+\n+#ifdef _LP64\n+  \/\/ Archived heap object headers (and soon, with Lilliput, markword prototypes) carry pre-computed\n+  \/\/ narrow Klass ids calculated with the following scheme:\n+  \/\/ 1) the encoding base must be the mapping start address.\n+  \/\/ 2) shift must be large enough to result in an encoding range that covers the runtime Klass range.\n+  \/\/    That Klass range is defined by CDS archive size and runtime class space size. Luckily, the maximum\n+  \/\/    size can be predicted: archive size is assumed to be <1G, class space size capped at 3G, and at\n+  \/\/    runtime we put both regions adjacent to each other. Therefore, runtime Klass range size < 4G.\n+  \/\/ The value of this precomputed shift depends on the class pointer mode at dump time.\n+  \/\/ Legacy Mode:\n+  \/\/    Since nKlass itself is 32 bit, our encoding range len is 4G, and since we set the base directly\n+  \/\/    at mapping start, these 4G are enough. Therefore, we don't need to shift at all (shift=0).\n+  \/\/ TinyClassPointer Mode:\n+  \/\/    To cover the 4G, we need the highest possible shift value. That may change in the future, if\n+  \/\/    we decide to correct the pre-calculated narrow Klass IDs at load time.\n+  static int precomputed_narrow_klass_shift();\n+#endif \/\/ _LP64\n+\n","filename":"src\/hotspot\/share\/cds\/archiveBuilder.hpp","additions":23,"deletions":3,"binary":false,"changes":26,"status":"modified"},{"patch":"@@ -208,2 +208,7 @@\n-    oopDesc::set_mark(mem, markWord::prototype());\n-    oopDesc::release_set_klass(mem, k);\n+    if (UseCompactObjectHeaders) {\n+      narrowKlass nk = ArchiveBuilder::current()->get_requested_narrow_klass(k);\n+      oopDesc::release_set_mark(mem, markWord::prototype().set_narrow_klass(nk));\n+    } else {\n+      oopDesc::set_mark(mem, markWord::prototype());\n+      oopDesc::release_set_klass(mem, k);\n+    }\n@@ -330,1 +335,0 @@\n-  oopDesc::set_mark(mem, markWord::prototype());\n@@ -332,1 +336,6 @@\n-  cast_to_oop(mem)->set_narrow_klass(nk);\n+  if (UseCompactObjectHeaders) {\n+    oopDesc::release_set_mark(mem, markWord::prototype().set_narrow_klass(nk));\n+  } else {\n+    oopDesc::set_mark(mem, markWord::prototype());\n+    cast_to_oop(mem)->set_narrow_klass(nk);\n+  }\n@@ -532,1 +541,5 @@\n-  fake_oop->set_narrow_klass(nk);\n+  if (UseCompactObjectHeaders) {\n+    fake_oop->set_mark(markWord::prototype().set_narrow_klass(nk));\n+  } else {\n+    fake_oop->set_narrow_klass(nk);\n+  }\n@@ -538,1 +551,5 @@\n-    fake_oop->set_mark(markWord::prototype().copy_set_hash(src_hash));\n+    if (UseCompactObjectHeaders) {\n+      fake_oop->set_mark(markWord::prototype().set_narrow_klass(nk).copy_set_hash(src_hash));\n+    } else {\n+      fake_oop->set_mark(markWord::prototype().copy_set_hash(src_hash));\n+    }\n","filename":"src\/hotspot\/share\/cds\/archiveHeapWriter.cpp","additions":23,"deletions":6,"binary":false,"changes":29,"status":"modified"},{"patch":"@@ -251,11 +251,0 @@\n-  \/\/ Archived heap object headers carry pre-computed narrow Klass ids calculated with the\n-  \/\/ following scheme:\n-  \/\/ 1) the encoding base must be the mapping start address.\n-  \/\/ 2) shift must be large enough to result in an encoding range that covers the runtime Klass range.\n-  \/\/    That Klass range is defined by CDS archive size and runtime class space size. Luckily, the maximum\n-  \/\/    size can be predicted: archive size is assumed to be <1G, class space size capped at 3G, and at\n-  \/\/    runtime we put both regions adjacent to each other. Therefore, runtime Klass range size < 4G.\n-  \/\/    Since nKlass itself is 32 bit, our encoding range len is 4G, and since we set the base directly\n-  \/\/    at mapping start, these 4G are enough. Therefore, we don't need to shift at all (shift=0).\n-  static constexpr int precomputed_narrow_klass_shift = 0;\n-\n","filename":"src\/hotspot\/share\/cds\/archiveHeapWriter.hpp","additions":0,"deletions":11,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -244,3 +244,4 @@\n-\n-char* DumpRegion::allocate(size_t num_bytes) {\n-  char* p = (char*)align_up(_top, (size_t)SharedSpaceObjectAlignment);\n+char* DumpRegion::allocate(size_t num_bytes, size_t alignment) {\n+  \/\/ Always align to at least minimum alignment\n+  alignment = MAX2(SharedSpaceObjectAlignment, alignment);\n+  char* p = (char*)align_up(_top, alignment);\n@@ -347,1 +348,1 @@\n-  assert(tag == old_tag, \"old tag doesn't match\");\n+  assert(tag == old_tag, \"tag doesn't match (%d, expected %d)\", old_tag, tag);\n","filename":"src\/hotspot\/share\/cds\/archiveUtils.cpp","additions":5,"deletions":4,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -159,1 +159,2 @@\n-      _max_delta(max_delta), _is_packed(false) {}\n+      _max_delta(max_delta), _is_packed(false),\n+      _rs(NULL), _vs(NULL) {}\n@@ -162,1 +163,1 @@\n-  char* allocate(size_t num_bytes);\n+  char* allocate(size_t num_bytes, size_t alignment = 0);\n","filename":"src\/hotspot\/share\/cds\/archiveUtils.hpp","additions":3,"deletions":2,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -85,7 +85,14 @@\n-    size_t jvm_path_len = strlen(jvm_path);\n-    size_t file_sep_len = strlen(os::file_separator());\n-    const size_t len = jvm_path_len + file_sep_len + 20;\n-    _default_archive_path = NEW_C_HEAP_ARRAY(char, len, mtArguments);\n-    jio_snprintf(_default_archive_path, len,\n-                LP64_ONLY(!UseCompressedOops ? \"%s%sclasses_nocoops.jsa\":) \"%s%sclasses.jsa\",\n-                jvm_path, os::file_separator());\n+    stringStream tmp;\n+    tmp.print(\"%s%sclasses\", jvm_path, os::file_separator());\n+#ifdef _LP64\n+    if (!UseCompressedOops) {\n+      tmp.print_raw(\"_nocoops\");\n+    }\n+    if (UseCompactObjectHeaders) {\n+      \/\/ Note that generation of xxx_coh.jsa variants require\n+      \/\/ --enable-cds-archive-coh at build time\n+      tmp.print_raw(\"_coh\");\n+    }\n+#endif\n+    tmp.print_raw(\".jsa\");\n+    _default_archive_path = os::strdup(tmp.base());\n","filename":"src\/hotspot\/share\/cds\/cdsConfig.cpp","additions":14,"deletions":7,"binary":false,"changes":21,"status":"modified"},{"patch":"@@ -118,0 +118,12 @@\n+\n+#ifdef ASSERT\n+void DumpAllocStats::verify(int expected_byte_size, bool read_only) const {\n+  int bytes = 0;\n+  const int what = (int)(read_only ? RO : RW);\n+  for (int type = 0; type < int(_number_of_types); type ++) {\n+    bytes += _bytes[what][type];\n+  }\n+  assert(bytes == expected_byte_size, \"counter mismatch (%s: %d vs %d)\",\n+         (read_only ? \"RO\" : \"RW\"), bytes, expected_byte_size);\n+}\n+#endif \/\/ ASSERT\n","filename":"src\/hotspot\/share\/cds\/dumpAllocStats.cpp","additions":12,"deletions":0,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -138,0 +138,3 @@\n+\n+  DEBUG_ONLY(void verify(int expected_byte_size, bool read_only) const);\n+\n","filename":"src\/hotspot\/share\/cds\/dumpAllocStats.hpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -58,0 +58,1 @@\n+#include \"oops\/compressedKlass.hpp\"\n@@ -207,0 +208,1 @@\n+  _compact_headers = UseCompactObjectHeaders;\n@@ -215,0 +217,8 @@\n+  if (UseCompressedClassPointers) {\n+#ifdef _LP64\n+    _narrow_klass_pointer_bits = CompressedKlassPointers::narrow_klass_pointer_bits();\n+    _narrow_klass_shift = ArchiveBuilder::precomputed_narrow_klass_shift();\n+#endif\n+  } else {\n+    _narrow_klass_pointer_bits = _narrow_klass_shift = -1;\n+  }\n@@ -274,0 +284,1 @@\n+  st->print_cr(\"- compact_headers:                %d\", _compact_headers);\n@@ -279,0 +290,2 @@\n+  st->print_cr(\"- narrow_klass_pointer_bits:      %d\", _narrow_klass_pointer_bits);\n+  st->print_cr(\"- narrow_klass_shift:             %d\", _narrow_klass_shift);\n@@ -2056,1 +2069,1 @@\n-  \/\/ ArchiveHeapWriter::precomputed_narrow_klass_shift. We enforce this encoding at runtime (see\n+  \/\/ ArchiveBuilder::precomputed_narrow_klass_shift. We enforce this encoding at runtime (see\n@@ -2060,1 +2073,2 @@\n-  const int archive_narrow_klass_shift = ArchiveHeapWriter::precomputed_narrow_klass_shift;\n+  const int archive_narrow_klass_pointer_bits = header()->narrow_klass_pointer_bits();\n+  const int archive_narrow_klass_shift = header()->narrow_klass_shift();\n@@ -2064,2 +2078,2 @@\n-  log_info(cds)(\"    narrow_klass_base at mapping start address, narrow_klass_shift = %d\",\n-                archive_narrow_klass_shift);\n+  log_info(cds)(\"    narrow_klass_base at mapping start address, narrow_klass_pointer_bits = %d, narrow_klass_shift = %d\",\n+                archive_narrow_klass_pointer_bits, archive_narrow_klass_shift);\n@@ -2070,2 +2084,2 @@\n-  log_info(cds)(\"    narrow_klass_base = \" PTR_FORMAT \", narrow_klass_shift = %d\",\n-                p2i(CompressedKlassPointers::base()), CompressedKlassPointers::shift());\n+  log_info(cds)(\"    narrow_klass_base = \" PTR_FORMAT \", arrow_klass_pointer_bits = %d, narrow_klass_shift = %d\",\n+                p2i(CompressedKlassPointers::base()), CompressedKlassPointers::narrow_klass_pointer_bits(), CompressedKlassPointers::shift());\n@@ -2080,4 +2094,29 @@\n-  assert(archive_narrow_klass_base == CompressedKlassPointers::base(), \"Unexpected encoding base encountered \"\n-         \"(\" PTR_FORMAT \", expected \" PTR_FORMAT \")\", p2i(CompressedKlassPointers::base()), p2i(archive_narrow_klass_base));\n-  assert(archive_narrow_klass_shift == CompressedKlassPointers::shift(), \"Unexpected encoding shift encountered \"\n-         \"(%d, expected %d)\", CompressedKlassPointers::shift(), archive_narrow_klass_shift);\n+  int err = 0;\n+  if ( archive_narrow_klass_base != CompressedKlassPointers::base() ||\n+       (err = 1, archive_narrow_klass_pointer_bits != CompressedKlassPointers::narrow_klass_pointer_bits()) ||\n+       (err = 2, archive_narrow_klass_shift != CompressedKlassPointers::shift()) ) {\n+    stringStream ss;\n+    switch (err) {\n+    case 0:\n+      ss.print(\"Unexpected encoding base encountered (\" PTR_FORMAT \", expected \" PTR_FORMAT \")\",\n+               p2i(CompressedKlassPointers::base()), p2i(archive_narrow_klass_base));\n+      break;\n+    case 1:\n+      ss.print(\"Unexpected narrow Klass bit length encountered (%d, expected %d)\",\n+               CompressedKlassPointers::narrow_klass_pointer_bits(), archive_narrow_klass_pointer_bits);\n+      break;\n+    case 2:\n+      ss.print(\"Unexpected narrow Klass shift encountered (%d, expected %d)\",\n+               CompressedKlassPointers::shift(), archive_narrow_klass_shift);\n+      break;\n+    default:\n+      ShouldNotReachHere();\n+    };\n+    LogTarget(Info, cds) lt;\n+    if (lt.is_enabled()) {\n+      LogStream ls(lt);\n+      ls.print_raw(ss.base());\n+      header()->print(&ls);\n+    }\n+    assert(false, \"%s\", ss.base());\n+  }\n@@ -2468,0 +2507,8 @@\n+  if (compact_headers() != UseCompactObjectHeaders) {\n+    log_info(cds)(\"The shared archive file's UseCompactObjectHeaders setting (%s)\"\n+                  \" does not equal the current UseCompactObjectHeaders setting (%s).\",\n+                  _compact_headers          ? \"enabled\" : \"disabled\",\n+                  UseCompactObjectHeaders   ? \"enabled\" : \"disabled\");\n+    return false;\n+  }\n+\n","filename":"src\/hotspot\/share\/cds\/filemap.cpp","additions":57,"deletions":10,"binary":false,"changes":67,"status":"modified"},{"patch":"@@ -190,0 +190,1 @@\n+  bool   _compact_headers;                        \/\/ value of UseCompactObjectHeaders\n@@ -195,0 +196,2 @@\n+  int     _narrow_klass_pointer_bits;             \/\/ save number of bits in narrowKlass\n+  int     _narrow_klass_shift;                    \/\/ save shift width used to pre-compute narrowKlass IDs in archived heap objects\n@@ -262,0 +265,1 @@\n+  bool compact_headers()                   const { return _compact_headers; }\n@@ -273,0 +277,2 @@\n+  int narrow_klass_pointer_bits()          const { return _narrow_klass_pointer_bits; }\n+  int narrow_klass_shift()                 const { return _narrow_klass_shift; }\n","filename":"src\/hotspot\/share\/cds\/filemap.hpp","additions":6,"deletions":0,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -89,0 +89,1 @@\n+#include \"utilities\/macros.hpp\"\n@@ -1185,11 +1186,12 @@\n-#if INCLUDE_CDS_JAVA_HEAP\n-          \/\/ We archived objects with pre-computed narrow Klass id. Set up encoding such that these Ids stay valid.\n-          address precomputed_narrow_klass_base = cds_base;\n-          const int precomputed_narrow_klass_shift = ArchiveHeapWriter::precomputed_narrow_klass_shift;\n-          CompressedKlassPointers::initialize_for_given_encoding(\n-            cds_base, ccs_end - cds_base, \/\/ Klass range\n-            precomputed_narrow_klass_base, precomputed_narrow_klass_shift \/\/ precomputed encoding, see ArchiveHeapWriter\n-            );\n-#else\n-          CompressedKlassPointers::initialize (\n-            cds_base, ccs_end - cds_base \/\/ Klass range\n+          if (INCLUDE_CDS_JAVA_HEAP || UseCompactObjectHeaders) {\n+            \/\/ The CDS archive may contain narrow Klass IDs that were precomputed at archive generation time:\n+            \/\/ - every archived java object header (only if INCLUDE_CDS_JAVA_HEAP)\n+            \/\/ - every archived Klass' prototype   (only if +UseCompactObjectHeaders)\n+            \/\/\n+            \/\/ In order for those IDs to still be valid, we need to dictate base and shift: base should be the\n+            \/\/ mapping start, shift the shift used at archive generation time.\n+            address precomputed_narrow_klass_base = cds_base;\n+            const int precomputed_narrow_klass_shift = ArchiveBuilder::precomputed_narrow_klass_shift();\n+            CompressedKlassPointers::initialize_for_given_encoding(\n+              cds_base, ccs_end - cds_base, \/\/ Klass range\n+              precomputed_narrow_klass_base, precomputed_narrow_klass_shift \/\/ precomputed encoding, see ArchiveBuilder\n@@ -1197,1 +1199,6 @@\n-#endif \/\/ INCLUDE_CDS_JAVA_HEAP\n+          } else {\n+            \/\/ Let JVM freely chose encoding base and shift\n+            CompressedKlassPointers::initialize (\n+              cds_base, ccs_end - cds_base \/\/ Klass range\n+              );\n+          }\n@@ -1252,1 +1259,1 @@\n-\/\/  encoding, the range [Base, End) not surpass KlassEncodingMetaspaceMax.\n+\/\/  encoding, the range [Base, End) and not surpass the max. range for that encoding.\n","filename":"src\/hotspot\/share\/cds\/metaspaceShared.cpp","additions":20,"deletions":13,"binary":false,"changes":33,"status":"modified"},{"patch":"@@ -252,0 +252,20 @@\n+\n+\/\/ ------------------------------------------------------------------\n+\/\/ ciKlass::prototype_header_offset\n+juint ciKlass::prototype_header_offset() {\n+  assert(is_loaded(), \"must be loaded\");\n+\n+  VM_ENTRY_MARK;\n+  Klass* this_klass = get_Klass();\n+  return in_bytes(this_klass->prototype_header_offset());\n+}\n+\n+\/\/ ------------------------------------------------------------------\n+\/\/ ciKlass::prototype_header\n+uintptr_t ciKlass::prototype_header() {\n+  assert(is_loaded(), \"must be loaded\");\n+\n+  VM_ENTRY_MARK;\n+  Klass* this_klass = get_Klass();\n+  return (uintptr_t)this_klass->prototype_header().to_pointer();\n+}\n","filename":"src\/hotspot\/share\/ci\/ciKlass.cpp","additions":20,"deletions":0,"binary":false,"changes":20,"status":"modified"},{"patch":"@@ -132,0 +132,3 @@\n+\n+  juint prototype_header_offset();\n+  uintptr_t prototype_header();\n","filename":"src\/hotspot\/share\/ci\/ciKlass.hpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -62,0 +62,1 @@\n+#include \"oops\/compressedKlass.inline.hpp\"\n@@ -424,0 +425,7 @@\n+\n+#ifdef ASSERT\n+    if (UseCompressedClassPointers && k != nullptr) {\n+      CompressedKlassPointers::check_valid_klass(k);\n+    }\n+#endif\n+\n@@ -1333,1 +1341,5 @@\n-    assert(check_alignment(record->_klass), \"Address not aligned\");\n+#ifdef _LP64\n+    if (UseCompressedClassPointers) {\n+      DEBUG_ONLY(CompressedKlassPointers::check_valid_klass(record->_klass);)\n+    }\n+#endif\n","filename":"src\/hotspot\/share\/classfile\/systemDictionaryShared.cpp","additions":13,"deletions":1,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -38,0 +38,1 @@\n+#include \"gc\/shared\/gcForwarding.hpp\"\n@@ -250,0 +251,1 @@\n+  GCForwarding::initialize_flags(heap_reserved_size_bytes());\n","filename":"src\/hotspot\/share\/gc\/g1\/g1Arguments.cpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -81,0 +81,1 @@\n+#include \"gc\/shared\/gcForwarding.hpp\"\n@@ -88,1 +89,0 @@\n-#include \"gc\/shared\/preservedMarks.inline.hpp\"\n@@ -1438,0 +1438,2 @@\n+  GCForwarding::initialize(heap_rs.region());\n+\n","filename":"src\/hotspot\/share\/gc\/g1\/g1CollectedHeap.cpp","additions":3,"deletions":1,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -32,0 +32,1 @@\n+#include \"gc\/shared\/gcForwarding.inline.hpp\"\n@@ -44,1 +45,1 @@\n-  if (obj->is_forwarded()) {\n+  if (GCForwarding::is_forwarded(obj)) {\n@@ -55,2 +56,2 @@\n-  assert(obj->is_forwarded(), \"Sanity!\");\n-  assert(obj->forwardee() != obj, \"Object must have a new location\");\n+  assert(GCForwarding::is_forwarded(obj), \"Sanity!\");\n+  assert(GCForwarding::forwardee(obj) != obj, \"Object must have a new location\");\n@@ -61,1 +62,1 @@\n-  HeapWord* destination = cast_from_oop<HeapWord*>(obj->forwardee());\n+  HeapWord* destination = cast_from_oop<HeapWord*>(GCForwarding::forwardee(obj));\n@@ -124,1 +125,1 @@\n-  HeapWord* destination = cast_from_oop<HeapWord*>(obj->forwardee());\n+  HeapWord* destination = cast_from_oop<HeapWord*>(GCForwarding::forwardee(obj));\n","filename":"src\/hotspot\/share\/gc\/g1\/g1FullGCCompactTask.cpp","additions":6,"deletions":5,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -29,0 +29,1 @@\n+#include \"gc\/shared\/gcForwarding.inline.hpp\"\n@@ -109,2 +110,2 @@\n-    object->forward_to(cast_to_oop(_compaction_top));\n-    assert(object->is_forwarded(), \"must be forwarded\");\n+    GCForwarding::forward_to(object, cast_to_oop(_compaction_top));\n+    assert(GCForwarding::is_forwarded(object), \"must be forwarded\");\n@@ -112,1 +113,1 @@\n-    assert(!object->is_forwarded(), \"must not be forwarded\");\n+    assert(!GCForwarding::is_forwarded(object), \"must not be forwarded\");\n@@ -175,2 +176,2 @@\n-  obj->forward_to(cast_to_oop(dest_hr->bottom()));\n-  assert(obj->is_forwarded(), \"Object must be forwarded!\");\n+  GCForwarding::forward_to(obj, cast_to_oop(dest_hr->bottom()));\n+  assert(GCForwarding::is_forwarded(obj), \"Object must be forwarded!\");\n","filename":"src\/hotspot\/share\/gc\/g1\/g1FullGCCompactionPoint.cpp","additions":6,"deletions":5,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -35,0 +35,1 @@\n+#include \"gc\/shared\/gcForwarding.inline.hpp\"\n@@ -68,2 +69,2 @@\n-  if (obj->is_forwarded()) {\n-    oop forwardee = obj->forwardee();\n+  if (GCForwarding::is_forwarded(obj)) {\n+    oop forwardee = GCForwarding::forwardee(obj);\n","filename":"src\/hotspot\/share\/gc\/g1\/g1FullGCOopClosures.inline.hpp","additions":3,"deletions":2,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -35,0 +35,1 @@\n+#include \"gc\/shared\/gcForwarding.inline.hpp\"\n@@ -117,1 +118,1 @@\n-  if (obj->is_forwarded()) {\n+  if (GCForwarding::is_forwarded(obj)) {\n@@ -120,1 +121,1 @@\n-    if (cast_from_oop<HeapWord*>(obj->forwardee()) < _dense_prefix_top) {\n+    if (cast_from_oop<HeapWord*>(GCForwarding::forwardee(obj)) < _dense_prefix_top) {\n","filename":"src\/hotspot\/share\/gc\/g1\/g1FullGCPrepareTask.inline.hpp","additions":3,"deletions":2,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -108,1 +108,0 @@\n-  _gc_par_phases[RestorePreservedMarks] = new WorkerDataArray<double>(\"RestorePreservedMarks\", \"Restore Preserved Marks (ms):\", max_gc_threads);\n@@ -515,1 +514,0 @@\n-    debug_phase(_gc_par_phases[RestorePreservedMarks], 1);\n","filename":"src\/hotspot\/share\/gc\/g1\/g1GCPhaseTimes.cpp","additions":0,"deletions":2,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -90,1 +90,0 @@\n-    RestorePreservedMarks,\n","filename":"src\/hotspot\/share\/gc\/g1\/g1GCPhaseTimes.hpp","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -231,1 +231,1 @@\n-      forwardee = m.forwardee();\n+      forwardee = obj->forwardee(m);\n","filename":"src\/hotspot\/share\/gc\/g1\/g1OopClosures.inline.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -40,1 +40,0 @@\n-#include \"gc\/shared\/preservedMarks.inline.hpp\"\n@@ -62,1 +61,0 @@\n-                                           PreservedMarks* preserved_marks,\n@@ -93,1 +91,0 @@\n-    _preserved_marks(preserved_marks),\n@@ -219,1 +216,1 @@\n-    obj = m.forwardee();\n+    obj = obj->forwardee(m);\n@@ -235,1 +232,1 @@\n-  assert(from_obj->is_objArray(), \"must be obj array\");\n+  assert(from_obj->forward_safe_klass()->is_objArray_klass(), \"must be obj array\");\n@@ -268,1 +265,1 @@\n-  assert(from_obj->is_objArray(), \"precondition\");\n+  assert(from_obj->forward_safe_klass()->is_objArray_klass(), \"precondition\");\n@@ -404,1 +401,1 @@\n-                                                  oop const old, size_t word_sz, uint age,\n+                                                  Klass* klass, size_t word_sz, uint age,\n@@ -408,1 +405,1 @@\n-    _g1h->gc_tracer_stw()->report_promotion_in_new_plab_event(old->klass(), word_sz * HeapWordSize, age,\n+    _g1h->gc_tracer_stw()->report_promotion_in_new_plab_event(klass, word_sz * HeapWordSize, age,\n@@ -412,1 +409,1 @@\n-    _g1h->gc_tracer_stw()->report_promotion_outside_plab_event(old->klass(), word_sz * HeapWordSize, age,\n+    _g1h->gc_tracer_stw()->report_promotion_outside_plab_event(klass, word_sz * HeapWordSize, age,\n@@ -419,1 +416,1 @@\n-                                                   oop old,\n+                                                   Klass* klass,\n@@ -442,1 +439,1 @@\n-      report_promotion_event(*dest_attr, old, word_sz, age, obj_ptr, node_index);\n+      report_promotion_event(*dest_attr, klass, word_sz, age, obj_ptr, node_index);\n@@ -479,1 +476,7 @@\n-  Klass* klass = old->klass();\n+  \/\/ NOTE: With compact headers, it is not safe to load the Klass* from o, because\n+  \/\/ that would access the mark-word, and the mark-word might change at any time by\n+  \/\/ concurrent promotion. The promoted mark-word would point to the forwardee, which\n+  \/\/ may not yet have completed copying. Therefore we must load the Klass* from\n+  \/\/ the mark-word that we have already loaded. This is safe, because we have checked\n+  \/\/ that this is not yet forwarded in the caller.\n+  Klass* klass = old->forward_safe_klass(old_mark);\n@@ -497,1 +500,1 @@\n-    obj_ptr = allocate_copy_slow(&dest_attr, old, word_sz, age, node_index);\n+    obj_ptr = allocate_copy_slow(&dest_attr, klass, word_sz, age, node_index);\n@@ -598,1 +601,0 @@\n-                               _preserved_marks_set.get(worker_id),\n@@ -658,1 +660,1 @@\n-  oop forward_ptr = old->forward_to_atomic(old, m, memory_order_relaxed);\n+  oop forward_ptr = old->forward_to_self_atomic(m, memory_order_relaxed);\n@@ -671,2 +673,0 @@\n-    _preserved_marks->push_if_necessary(old, m);\n-\n@@ -730,1 +730,0 @@\n-    _preserved_marks_set(true \/* in_c_heap *\/),\n@@ -739,1 +738,0 @@\n-  _preserved_marks_set.init(num_workers);\n@@ -752,1 +750,0 @@\n-  _preserved_marks_set.reclaim();\n","filename":"src\/hotspot\/share\/gc\/g1\/g1ParScanThreadState.cpp","additions":17,"deletions":20,"binary":false,"changes":37,"status":"modified"},{"patch":"@@ -37,1 +37,0 @@\n-#include \"gc\/shared\/preservedMarks.hpp\"\n@@ -51,2 +50,0 @@\n-class PreservedMarks;\n-class PreservedMarksSet;\n@@ -111,1 +108,0 @@\n-  PreservedMarks* _preserved_marks;\n@@ -130,1 +126,0 @@\n-                       PreservedMarks* preserved_marks,\n@@ -179,1 +174,1 @@\n-                               oop old,\n+                               Klass* klass,\n@@ -214,1 +209,1 @@\n-                              oop const old, size_t word_sz, uint age,\n+                              Klass* klass, size_t word_sz, uint age,\n@@ -251,1 +246,0 @@\n-  PreservedMarksSet _preserved_marks_set;\n@@ -269,1 +263,0 @@\n-  PreservedMarksSet* preserved_marks_set() { return &_preserved_marks_set; }\n","filename":"src\/hotspot\/share\/gc\/g1\/g1ParScanThreadState.hpp","additions":2,"deletions":9,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -56,1 +56,0 @@\n-#include \"gc\/shared\/preservedMarks.hpp\"\n","filename":"src\/hotspot\/share\/gc\/g1\/g1YoungCollector.cpp","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -45,1 +45,0 @@\n-#include \"gc\/shared\/preservedMarks.inline.hpp\"\n@@ -255,1 +254,1 @@\n-        obj->init_mark();\n+        obj->unset_self_forwarded();\n@@ -480,21 +479,0 @@\n-class G1PostEvacuateCollectionSetCleanupTask2::RestorePreservedMarksTask : public G1AbstractSubTask {\n-  PreservedMarksSet* _preserved_marks;\n-  WorkerTask* _task;\n-\n-public:\n-  RestorePreservedMarksTask(PreservedMarksSet* preserved_marks) :\n-    G1AbstractSubTask(G1GCPhaseTimes::RestorePreservedMarks),\n-    _preserved_marks(preserved_marks),\n-    _task(preserved_marks->create_task()) { }\n-\n-  virtual ~RestorePreservedMarksTask() {\n-    delete _task;\n-  }\n-\n-  double worker_cost() const override {\n-    return _preserved_marks->num();\n-  }\n-\n-  void do_work(uint worker_id) override { _task->work(worker_id); }\n-};\n-\n@@ -982,1 +960,0 @@\n-    add_parallel_task(new RestorePreservedMarksTask(per_thread_states->preserved_marks_set()));\n","filename":"src\/hotspot\/share\/gc\/g1\/g1YoungGCPostEvacuateTasks.cpp","additions":1,"deletions":24,"binary":false,"changes":25,"status":"modified"},{"patch":"@@ -59,1 +59,0 @@\n-\/\/ - Restore Preserved Marks (on evacuation failure)\n@@ -70,1 +69,0 @@\n-  class RestorePreservedMarksTask;\n","filename":"src\/hotspot\/share\/gc\/g1\/g1YoungGCPostEvacuateTasks.hpp","additions":0,"deletions":2,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -213,1 +213,2 @@\n-void MutableSpace::object_iterate(ObjectClosure* cl) {\n+template<bool COMPACT_HEADERS>\n+void MutableSpace::object_iterate_impl(ObjectClosure* cl) {\n@@ -222,3 +223,2 @@\n-    }\n-#ifdef ASSERT\n-    else {\n+      p += obj->size();\n+    } else {\n@@ -226,0 +226,8 @@\n+      if (COMPACT_HEADERS) {\n+        \/\/ It is safe to use the forwardee here. Parallel GC only uses\n+        \/\/ header-based forwarding during promotion. Full GC doesn't\n+        \/\/ use the object header for forwarding at all.\n+        p += obj->forwardee()->size();\n+      } else {\n+        p += obj->size();\n+      }\n@@ -227,2 +235,8 @@\n-#endif\n-    p += obj->size();\n+  }\n+}\n+\n+void MutableSpace::object_iterate(ObjectClosure* cl) {\n+  if (UseCompactObjectHeaders) {\n+    object_iterate_impl<true>(cl);\n+  } else {\n+    object_iterate_impl<false>(cl);\n","filename":"src\/hotspot\/share\/gc\/parallel\/mutableSpace.cpp","additions":20,"deletions":6,"binary":false,"changes":26,"status":"modified"},{"patch":"@@ -64,0 +64,3 @@\n+  template<bool COMPACT_HEADERS>\n+  void object_iterate_impl(ObjectClosure* cl);\n+\n","filename":"src\/hotspot\/share\/gc\/parallel\/mutableSpace.hpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -31,0 +31,1 @@\n+#include \"gc\/shared\/gcForwarding.hpp\"\n@@ -130,0 +131,1 @@\n+  GCForwarding::initialize_flags(heap_reserved_size_bytes());\n","filename":"src\/hotspot\/share\/gc\/parallel\/parallelArguments.cpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -36,0 +36,1 @@\n+#include \"gc\/shared\/gcForwarding.inline.hpp\"\n@@ -132,0 +133,2 @@\n+  GCForwarding::initialize(heap_rs.region());\n+\n","filename":"src\/hotspot\/share\/gc\/parallel\/parallelScavengeHeap.cpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -48,0 +48,1 @@\n+#include \"gc\/shared\/gcForwarding.inline.hpp\"\n@@ -783,0 +784,4 @@\n+  if (UseCompactObjectHeaders) {\n+    \/\/ The gap is always equal to min-fill-size, so nothing to do.\n+    return;\n+  }\n@@ -1595,1 +1600,1 @@\n-              obj->forward_to(cast_to_oop(new_addr));\n+              GCForwarding::forward_to(obj, cast_to_oop(new_addr));\n@@ -1638,1 +1643,1 @@\n-        assert(obj->forwardee() == cast_to_oop(bump_ptr), \"inv\");\n+        assert(GCForwarding::forwardee(obj) == cast_to_oop(bump_ptr), \"inv\");\n@@ -2401,2 +2406,2 @@\n-    assert(cast_to_oop(source())->is_forwarded(), \"inv\");\n-    assert(cast_to_oop(source())->forwardee() == cast_to_oop(destination()), \"inv\");\n+    assert(GCForwarding::is_forwarded(cast_to_oop(source())), \"inv\");\n+    assert(GCForwarding::forwardee(cast_to_oop(source())) == cast_to_oop(destination()), \"inv\");\n","filename":"src\/hotspot\/share\/gc\/parallel\/psParallelCompact.cpp","additions":9,"deletions":4,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -34,0 +34,1 @@\n+#include \"gc\/shared\/gcForwarding.inline.hpp\"\n@@ -82,1 +83,1 @@\n-    oop new_obj = obj->forwardee();\n+    oop new_obj = GCForwarding::forwardee(obj);\n","filename":"src\/hotspot\/share\/gc\/parallel\/psParallelCompact.inline.hpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -284,1 +284,1 @@\n-  assert(old->is_objArray(), \"invariant\");\n+  assert(old->forward_safe_klass()->is_objArray_klass(), \"invariant\");\n@@ -322,1 +322,1 @@\n-  if (obj->forward_to_atomic(obj, obj_mark) == nullptr) {\n+  if (obj->forward_to_self_atomic(obj_mark) == nullptr) {\n","filename":"src\/hotspot\/share\/gc\/parallel\/psPromotionManager.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -108,1 +108,1 @@\n-  inline void promotion_trace_event(oop new_obj, oop old_obj, size_t obj_size,\n+  inline void promotion_trace_event(oop new_obj, Klass* klass, size_t obj_size,\n","filename":"src\/hotspot\/share\/gc\/parallel\/psPromotionManager.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -69,1 +69,1 @@\n-inline void PSPromotionManager::promotion_trace_event(oop new_obj, oop old_obj,\n+inline void PSPromotionManager::promotion_trace_event(oop new_obj, Klass* klass,\n@@ -82,1 +82,1 @@\n-        gc_tracer->report_promotion_in_new_plab_event(old_obj->klass(), obj_bytes,\n+        gc_tracer->report_promotion_in_new_plab_event(klass, obj_bytes,\n@@ -89,1 +89,1 @@\n-        gc_tracer->report_promotion_outside_plab_event(old_obj->klass(), obj_bytes,\n+        gc_tracer->report_promotion_outside_plab_event(klass, obj_bytes,\n@@ -152,1 +152,1 @@\n-    return m.forwardee();\n+    return o->forwardee(m);\n@@ -168,1 +168,8 @@\n-  size_t new_obj_size = o->size();\n+  \/\/ NOTE: With compact headers, it is not safe to load the Klass* from o, because\n+  \/\/ that would access the mark-word, and the mark-word might change at any time by\n+  \/\/ concurrent promotion. The promoted mark-word would point to the forwardee, which\n+  \/\/ may not yet have completed copying. Therefore we must load the Klass* from\n+  \/\/ the mark-word that we have already loaded. This is safe, because we have checked\n+  \/\/ that this is not yet forwarded in the caller.\n+  Klass* klass = o->forward_safe_klass(test_mark);\n+  size_t new_obj_size = o->size_given_klass(klass);\n@@ -183,1 +190,1 @@\n-          promotion_trace_event(new_obj, o, new_obj_size, age, false, nullptr);\n+          promotion_trace_event(new_obj, klass, new_obj_size, age, false, nullptr);\n@@ -193,1 +200,1 @@\n-            promotion_trace_event(new_obj, o, new_obj_size, age, false, &_young_lab);\n+            promotion_trace_event(new_obj, klass, new_obj_size, age, false, &_young_lab);\n@@ -219,1 +226,1 @@\n-          promotion_trace_event(new_obj, o, new_obj_size, age, true, nullptr);\n+          promotion_trace_event(new_obj, klass, new_obj_size, age, true, nullptr);\n@@ -229,1 +236,1 @@\n-            promotion_trace_event(new_obj, o, new_obj_size, age, true, &_old_lab);\n+            promotion_trace_event(new_obj, klass, new_obj_size, age, true, &_old_lab);\n","filename":"src\/hotspot\/share\/gc\/parallel\/psPromotionManager.inline.hpp","additions":16,"deletions":9,"binary":false,"changes":25,"status":"modified"},{"patch":"@@ -42,1 +42,0 @@\n-#include \"gc\/shared\/preservedMarks.inline.hpp\"\n@@ -230,1 +229,0 @@\n-    _preserved_marks_set(false \/* in_c_heap *\/),\n@@ -612,2 +610,0 @@\n-  \/\/ The preserved marks should be empty at the start of the GC.\n-  _preserved_marks_set.init(1);\n@@ -684,2 +680,0 @@\n-  \/\/ We should have processed and cleared all the preserved marks.\n-  _preserved_marks_set.reclaim();\n@@ -709,2 +703,5 @@\n-      if (obj->is_forwarded()) {\n-        obj->init_mark();\n+      if (obj->is_self_forwarded()) {\n+        obj->unset_self_forwarded();\n+      } else if (obj->is_forwarded()) {\n+        \/\/ To restore the klass-bits in the header.\n+        obj->forward_safe_init_mark();\n@@ -716,6 +713,0 @@\n-\n-  restore_preserved_marks();\n-}\n-\n-void DefNewGeneration::restore_preserved_marks() {\n-  _preserved_marks_set.restore(nullptr);\n@@ -729,1 +720,0 @@\n-  _preserved_marks_set.get()->push_if_necessary(old, old->mark());\n@@ -734,1 +724,1 @@\n-  old->forward_to(old);\n+  old->forward_to_self();\n","filename":"src\/hotspot\/share\/gc\/serial\/defNewGeneration.cpp","additions":6,"deletions":16,"binary":false,"changes":22,"status":"modified"},{"patch":"@@ -35,1 +35,0 @@\n-#include \"gc\/shared\/preservedMarks.hpp\"\n@@ -102,5 +101,0 @@\n-  virtual void restore_preserved_marks();\n-\n-  \/\/ Preserved marks\n-  PreservedMarksSet _preserved_marks_set;\n-\n","filename":"src\/hotspot\/share\/gc\/serial\/defNewGeneration.hpp","additions":0,"deletions":6,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -26,0 +26,1 @@\n+#include \"gc\/shared\/gcForwarding.hpp\"\n@@ -30,0 +31,5 @@\n+void SerialArguments::initialize_heap_flags_and_sizes() {\n+  GenArguments::initialize_heap_flags_and_sizes();\n+  GCForwarding::initialize_flags(MaxNewSize + MaxOldSize);\n+}\n+\n","filename":"src\/hotspot\/share\/gc\/serial\/serialArguments.cpp","additions":6,"deletions":0,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -35,0 +35,1 @@\n+  void initialize_heap_flags_and_sizes();\n","filename":"src\/hotspot\/share\/gc\/serial\/serialArguments.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -46,0 +46,1 @@\n+#include \"gc\/shared\/gcForwarding.inline.hpp\"\n@@ -233,1 +234,1 @@\n-      obj->forward_to(cast_to_oop(new_addr));\n+      GCForwarding::forward_to(obj, cast_to_oop(new_addr));\n@@ -258,1 +259,1 @@\n-    oop new_obj = obj->forwardee();\n+    oop new_obj = GCForwarding::forwardee(obj);\n@@ -355,1 +356,1 @@\n-      if (!cast_to_oop(cur_addr)->is_forwarded()) {\n+      if (!GCForwarding::is_forwarded(cast_to_oop(cur_addr))) {\n@@ -361,1 +362,1 @@\n-        if (!cast_to_oop(cur_addr)->is_forwarded()) {\n+        if (!GCForwarding::is_forwarded(cast_to_oop(cur_addr))) {\n@@ -596,1 +597,1 @@\n-  obj->set_mark(markWord::prototype().set_marked());\n+  obj->set_mark(obj->prototype_mark().set_marked());\n@@ -627,2 +628,2 @@\n-    if (obj->is_forwarded()) {\n-      oop new_obj = obj->forwardee();\n+    if (GCForwarding::is_forwarded(obj)) {\n+      oop new_obj = GCForwarding::forwardee(obj);\n","filename":"src\/hotspot\/share\/gc\/serial\/serialFullGC.cpp","additions":8,"deletions":7,"binary":false,"changes":15,"status":"modified"},{"patch":"@@ -43,0 +43,1 @@\n+#include \"gc\/shared\/gcForwarding.hpp\"\n@@ -203,0 +204,2 @@\n+  GCForwarding::initialize(_reserved);\n+\n","filename":"src\/hotspot\/share\/gc\/serial\/serialHeap.cpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -714,0 +714,1 @@\n+    assert(!UseCompactObjectHeaders, \"\");\n","filename":"src\/hotspot\/share\/gc\/shared\/c2\/barrierSetC2.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -232,1 +232,3 @@\n-  if (!Metaspace::contains(object->klass_without_asserts())) {\n+  \/\/ With compact headers, we can't safely access the class, due\n+  \/\/ to possibly forwarded objects.\n+  if (!UseCompactObjectHeaders && !Metaspace::contains(object->klass_without_asserts())) {\n","filename":"src\/hotspot\/share\/gc\/shared\/collectedHeap.cpp","additions":3,"deletions":1,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -309,1 +309,1 @@\n-  static constexpr size_t min_dummy_object_size() {\n+  static size_t min_dummy_object_size() {\n","filename":"src\/hotspot\/share\/gc\/shared\/collectedHeap.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -0,0 +1,52 @@\n+\/*\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"gc\/shared\/gcForwarding.hpp\"\n+#include \"memory\/memRegion.hpp\"\n+#include \"runtime\/globals_extension.hpp\"\n+\n+HeapWord* GCForwarding::_heap_base = nullptr;\n+int GCForwarding::_num_low_bits = 0;\n+\n+void GCForwarding::initialize_flags(size_t max_heap_size) {\n+#ifdef _LP64\n+  size_t max_narrow_heap_size = right_n_bits(NumLowBitsNarrow - Shift);\n+  if (UseCompactObjectHeaders && max_heap_size > max_narrow_heap_size * HeapWordSize) {\n+    FLAG_SET_DEFAULT(UseCompactObjectHeaders, false);\n+  }\n+#endif\n+}\n+\n+void GCForwarding::initialize(MemRegion heap) {\n+#ifdef _LP64\n+  _heap_base = heap.start();\n+  if (heap.word_size() <= right_n_bits(NumLowBitsNarrow - Shift)) {\n+    _num_low_bits = NumLowBitsNarrow;\n+  } else {\n+    assert(!UseCompactObjectHeaders, \"Compact object headers should be turned off for large heaps\");\n+    _num_low_bits = NumLowBitsWide;\n+  }\n+#endif\n+}\n","filename":"src\/hotspot\/share\/gc\/shared\/gcForwarding.cpp","additions":52,"deletions":0,"binary":false,"changes":52,"status":"added"},{"patch":"@@ -0,0 +1,57 @@\n+\/*\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef SHARE_GC_SHARED_GCFORWARDING_HPP\n+#define SHARE_GC_SHARED_GCFORWARDING_HPP\n+\n+#include \"memory\/allStatic.hpp\"\n+#include \"memory\/memRegion.hpp\"\n+#include \"oops\/markWord.hpp\"\n+#include \"oops\/oopsHierarchy.hpp\"\n+\n+\/*\n+ * Implements forwarding for the full-GCs of Serial, Parallel, G1 and Shenandoah in\n+ * a way that preserves upper N bits of object mark-words, which contain crucial\n+ * Klass* information when running with compact headers. The encoding is similar to\n+ * compressed-oops encoding: it basically subtracts the forwardee address from the\n+ * heap-base, shifts that difference into the right place, and sets the lowest two\n+ * bits (to indicate 'forwarded' state as usual).\n+ *\/\n+class GCForwarding : public AllStatic {\n+  static const int NumKlassBits     = LP64_ONLY(markWord::klass_shift) NOT_LP64(0 \/*unused*\/);\n+  static const int NumLowBitsNarrow = BitsPerWord - NumKlassBits;\n+  static const int NumLowBitsWide   = BitsPerWord;\n+  static const int Shift            = markWord::lock_bits + markWord::lock_shift;\n+\n+  static HeapWord* _heap_base;\n+  static int _num_low_bits;\n+public:\n+  static void initialize_flags(size_t max_heap_size);\n+  static void initialize(MemRegion heap);\n+  static inline void forward_to(oop from, oop to);\n+  static inline oop forwardee(oop from);\n+  static inline bool is_forwarded(oop obj);\n+};\n+\n+#endif \/\/ SHARE_GC_SHARED_GCFORWARDING_HPP\n","filename":"src\/hotspot\/share\/gc\/shared\/gcForwarding.hpp","additions":57,"deletions":0,"binary":false,"changes":57,"status":"added"},{"patch":"@@ -0,0 +1,59 @@\n+\/*\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef GC_SHARED_GCFORWARDING_INLINE_HPP\n+#define GC_SHARED_GCFORWARDING_INLINE_HPP\n+\n+#include \"gc\/shared\/gcForwarding.hpp\"\n+#include \"oops\/oop.inline.hpp\"\n+#include \"utilities\/globalDefinitions.hpp\"\n+\n+void GCForwarding::forward_to(oop from, oop to) {\n+#ifdef _LP64\n+  uintptr_t encoded = pointer_delta(cast_from_oop<HeapWord*>(to), _heap_base) << Shift;\n+  assert(encoded <= static_cast<uintptr_t>(right_n_bits(_num_low_bits)), \"encoded forwardee must fit\");\n+  uintptr_t mark = from->mark().value();\n+  mark &= ~right_n_bits(_num_low_bits);\n+  mark |= (encoded | markWord::marked_value);\n+  from->set_mark(markWord(mark));\n+#else\n+  from->forward_to(to);\n+#endif\n+}\n+\n+oop GCForwarding::forwardee(oop from) {\n+#ifdef _LP64\n+  uintptr_t mark = from->mark().value();\n+  HeapWord* decoded = _heap_base + ((mark & right_n_bits(_num_low_bits)) >> Shift);\n+  return cast_to_oop(decoded);\n+#else\n+  return from->forwardee();\n+#endif\n+}\n+\n+bool GCForwarding::is_forwarded(oop obj) {\n+  return obj->mark().is_forwarded();\n+}\n+\n+#endif \/\/ GC_SHARED_GCFORWARDING_INLINE_HPP\n","filename":"src\/hotspot\/share\/gc\/shared\/gcForwarding.inline.hpp","additions":59,"deletions":0,"binary":false,"changes":59,"status":"added"},{"patch":"@@ -364,1 +364,3 @@\n-  oopDesc::set_klass_gap(mem, 0);\n+  if (!UseCompactObjectHeaders) {\n+    oopDesc::set_klass_gap(mem, 0);\n+  }\n@@ -370,2 +372,0 @@\n-  \/\/ May be bootstrapping\n-  oopDesc::set_mark(mem, markWord::prototype());\n@@ -375,1 +375,6 @@\n-  oopDesc::release_set_klass(mem, _klass);\n+  if (UseCompactObjectHeaders) {\n+    oopDesc::release_set_mark(mem, _klass->prototype_header());\n+  } else {\n+    oopDesc::set_mark(mem, markWord::prototype());\n+    oopDesc::release_set_klass(mem, _klass);\n+  }\n","filename":"src\/hotspot\/share\/gc\/shared\/memAllocator.cpp","additions":9,"deletions":4,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -26,0 +26,1 @@\n+#include \"gc\/shared\/gcForwarding.inline.hpp\"\n@@ -45,2 +46,2 @@\n-  if (obj->is_forwarded()) {\n-    elem->set_oop(obj->forwardee());\n+  if (GCForwarding::is_forwarded(obj)) {\n+    elem->set_oop(GCForwarding::forwardee(obj));\n","filename":"src\/hotspot\/share\/gc\/shared\/preservedMarks.cpp","additions":3,"deletions":2,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -27,0 +27,1 @@\n+#include \"gc\/shared\/gcForwarding.hpp\"\n@@ -201,0 +202,5 @@\n+void ShenandoahArguments::initialize_heap_flags_and_sizes() {\n+  GCArguments::initialize_heap_flags_and_sizes();\n+  GCForwarding::initialize_flags(MaxHeapSize);\n+}\n+\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahArguments.cpp","additions":6,"deletions":0,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -38,0 +38,1 @@\n+  virtual void initialize_heap_flags_and_sizes();\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahArguments.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -200,1 +200,1 @@\n-  Klass* obj_klass = obj->klass_or_null();\n+  Klass* obj_klass = obj->forward_safe_klass();\n@@ -238,1 +238,1 @@\n-    if (obj_klass != fwd->klass()) {\n+    if (obj_klass != fwd->forward_safe_klass()) {\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahAsserts.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -29,0 +29,1 @@\n+#include \"gc\/shared\/gcForwarding.inline.hpp\"\n@@ -372,1 +373,1 @@\n-      p->forward_to(cast_to_oop(_compact_point));\n+      GCForwarding::forward_to(p, cast_to_oop(_compact_point));\n@@ -495,1 +496,1 @@\n-        old_obj->forward_to(cast_to_oop(heap->get_region(start)->bottom()));\n+        GCForwarding::forward_to(old_obj, cast_to_oop(heap->get_region(start)->bottom()));\n@@ -755,2 +756,2 @@\n-      if (obj->is_forwarded()) {\n-        oop forw = obj->forwardee();\n+      if (GCForwarding::is_forwarded(obj)) {\n+        oop forw = GCForwarding::forwardee(obj);\n@@ -866,1 +867,1 @@\n-    if (p->is_forwarded()) {\n+    if (GCForwarding::is_forwarded(p)) {\n@@ -868,1 +869,1 @@\n-      HeapWord* compact_to = cast_from_oop<HeapWord*>(p->forwardee());\n+      HeapWord* compact_to = cast_from_oop<HeapWord*>(GCForwarding::forwardee(p));\n@@ -973,1 +974,1 @@\n-      if (!old_obj->is_forwarded()) {\n+      if (!GCForwarding::is_forwarded(old_obj)) {\n@@ -982,1 +983,1 @@\n-      size_t new_start = heap->heap_region_index_containing(old_obj->forwardee());\n+      size_t new_start = heap->heap_region_index_containing(GCForwarding::forwardee(old_obj));\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahFullGC.cpp","additions":9,"deletions":8,"binary":false,"changes":17,"status":"modified"},{"patch":"@@ -32,0 +32,1 @@\n+#include \"gc\/shared\/gcForwarding.hpp\"\n@@ -425,0 +426,2 @@\n+  GCForwarding::initialize(_heap_region);\n+\n@@ -1133,1 +1136,1 @@\n-  size_t size = p->size();\n+  size_t size = p->forward_safe_size();\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeap.cpp","additions":4,"deletions":1,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -437,1 +437,1 @@\n-    size_t size = obj->size();\n+    size_t size = obj->forward_safe_size();\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeap.inline.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -105,1 +105,1 @@\n-      if (is_instance_ref_klass(obj->klass())) {\n+      if (is_instance_ref_klass(obj->forward_safe_klass())) {\n@@ -132,1 +132,1 @@\n-    Klass* obj_klass = obj->klass_or_null();\n+    Klass* obj_klass = obj->forward_safe_klass();\n@@ -147,1 +147,1 @@\n-        check(ShenandoahAsserts::_safe_unknown, obj, (obj_addr + obj->size()) <= obj_reg->top(),\n+        check(ShenandoahAsserts::_safe_unknown, obj, (obj_addr + obj->forward_safe_size()) <= obj_reg->top(),\n@@ -151,1 +151,1 @@\n-        size_t humongous_end = humongous_start + (obj->size() >> ShenandoahHeapRegion::region_size_words_shift());\n+        size_t humongous_end = humongous_start + (obj->forward_safe_size() >> ShenandoahHeapRegion::region_size_words_shift());\n@@ -168,1 +168,1 @@\n-          Atomic::add(&_ld[obj_reg->index()], (uint) obj->size(), memory_order_relaxed);\n+          Atomic::add(&_ld[obj_reg->index()], (uint) obj->forward_safe_size(), memory_order_relaxed);\n@@ -212,1 +212,1 @@\n-      check(ShenandoahAsserts::_safe_oop, obj, (fwd_addr + fwd->size()) <= fwd_reg->top(),\n+      check(ShenandoahAsserts::_safe_oop, obj, (fwd_addr + fwd->forward_safe_size()) <= fwd_reg->top(),\n@@ -330,1 +330,2 @@\n-    obj->oop_iterate(this);\n+    Klass* klass = obj->forward_safe_klass();\n+    obj->oop_iterate_backwards(this, klass);\n@@ -594,1 +595,1 @@\n-        addr += cast_to_oop(addr)->size();\n+        addr += cast_to_oop(addr)->forward_safe_size();\n@@ -610,1 +611,1 @@\n-    if (!is_instance_ref_klass(obj->klass())) {\n+    if (!is_instance_ref_klass(obj->forward_safe_klass())) {\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahVerifier.cpp","additions":10,"deletions":9,"binary":false,"changes":19,"status":"modified"},{"patch":"@@ -301,1 +301,1 @@\n-        assert(!UseCompressedClassPointers, \"should only happen without compressed class pointers\");\n+        assert(!UseCompressedClassPointers || UseCompactObjectHeaders, \"should only happen without compressed class pointers\");\n","filename":"src\/hotspot\/share\/gc\/x\/c2\/xBarrierSetC2.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -76,2 +76,6 @@\n-  arrayOopDesc::set_mark(mem, markWord::prototype());\n-  arrayOopDesc::release_set_klass(mem, _klass);\n+  if (UseCompactObjectHeaders) {\n+    arrayOopDesc::release_set_mark(mem, _klass->prototype_header());\n+  } else {\n+    arrayOopDesc::set_mark(mem, markWord::prototype());\n+    arrayOopDesc::release_set_klass(mem, _klass);\n+  }\n","filename":"src\/hotspot\/share\/gc\/x\/xObjArrayAllocator.cpp","additions":6,"deletions":2,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -442,1 +442,1 @@\n-        assert(!UseCompressedClassPointers, \"should only happen without compressed class pointers\");\n+        assert(!UseCompressedClassPointers || UseCompactObjectHeaders, \"should only happen without compressed class pointers\");\n","filename":"src\/hotspot\/share\/gc\/z\/c2\/zBarrierSetC2.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -66,2 +66,6 @@\n-  arrayOopDesc::set_mark(mem, markWord::prototype().set_marked());\n-  arrayOopDesc::release_set_klass(mem, _klass);\n+  if (UseCompactObjectHeaders) {\n+    oopDesc::release_set_mark(mem, _klass->prototype_header().set_marked());\n+  } else {\n+    arrayOopDesc::set_mark(mem, markWord::prototype().set_marked());\n+    arrayOopDesc::release_set_klass(mem, _klass);\n+  }\n@@ -155,1 +159,5 @@\n-  oopDesc::release_set_mark(mem, markWord::prototype());\n+  if (UseCompactObjectHeaders) {\n+    oopDesc::release_set_mark(mem, _klass->prototype_header());\n+  } else {\n+    oopDesc::release_set_mark(mem, markWord::prototype());\n+  }\n","filename":"src\/hotspot\/share\/gc\/z\/zObjArrayAllocator.cpp","additions":11,"deletions":3,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -594,1 +594,0 @@\n-    const size_t size = ZUtils::object_size(from_addr);\n@@ -602,0 +601,1 @@\n+        const size_t size = ZUtils::object_size(to_addr);\n@@ -608,0 +608,1 @@\n+    const size_t size = ZUtils::object_size(from_addr);\n","filename":"src\/hotspot\/share\/gc\/z\/zRelocate.cpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -2024,4 +2024,7 @@\n-              oopDesc::set_mark(result, markWord::prototype());\n-              oopDesc::set_klass_gap(result, 0);\n-              oopDesc::release_set_klass(result, ik);\n-\n+              if (UseCompactObjectHeaders) {\n+                oopDesc::release_set_mark(result, ik->prototype_header());\n+              } else {\n+                oopDesc::set_mark(result, markWord::prototype());\n+                oopDesc::set_klass_gap(result, 0);\n+                oopDesc::release_set_klass(result, ik);\n+              }\n","filename":"src\/hotspot\/share\/interpreter\/zero\/bytecodeInterpreter.cpp","additions":7,"deletions":4,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -73,1 +73,1 @@\n-    obj->set_mark(markWord::prototype().set_marked());\n+    obj->set_mark(obj->prototype_mark().set_marked());\n","filename":"src\/hotspot\/share\/jfr\/leakprofiler\/chains\/objectSampleMarker.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -478,1 +478,0 @@\n-  declare_constant(LogKlassAlignmentInBytes)                              \\\n","filename":"src\/hotspot\/share\/jvmci\/vmStructs_jvmci.cpp","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -33,0 +33,1 @@\n+#include \"memory\/metaspace\/metablock.hpp\"\n@@ -35,0 +36,2 @@\n+#include \"memory\/metaspace\/metaspaceCommon.hpp\"\n+#include \"memory\/metaspace\/metaspaceContext.hpp\"\n@@ -39,0 +42,1 @@\n+#include \"oops\/klass.hpp\"\n@@ -43,0 +47,1 @@\n+using metaspace::MetaBlock;\n@@ -44,0 +49,1 @@\n+using metaspace::MetaspaceContext;\n@@ -52,0 +58,10 @@\n+    ClassLoaderMetaspace(lock, space_type,\n+                         MetaspaceContext::context_nonclass(),\n+                         MetaspaceContext::context_class(),\n+                         CompressedKlassPointers::klass_alignment_in_words())\n+{}\n+\n+ClassLoaderMetaspace::ClassLoaderMetaspace(Mutex* lock, Metaspace::MetaspaceType space_type,\n+                                           MetaspaceContext* non_class_context,\n+                                           MetaspaceContext* class_context,\n+                                           size_t klass_alignment_words) :\n@@ -57,3 +73,0 @@\n-  ChunkManager* const non_class_cm =\n-          ChunkManager::chunkmanager_nonclass();\n-\n@@ -62,1 +75,1 @@\n-      non_class_cm,\n+      non_class_context,\n@@ -64,2 +77,2 @@\n-      RunningCounters::used_nonclass_counter(),\n-      \"non-class sm\");\n+      Metaspace::min_allocation_alignment_words,\n+      \"non-class arena\");\n@@ -68,3 +81,1 @@\n-  if (Metaspace::using_class_space()) {\n-    ChunkManager* const class_cm =\n-            ChunkManager::chunkmanager_class();\n+  if (class_context != nullptr) {\n@@ -72,1 +83,1 @@\n-        class_cm,\n+        class_context,\n@@ -74,2 +85,2 @@\n-        RunningCounters::used_class_counter(),\n-        \"class sm\");\n+        klass_alignment_words,\n+        \"class arena\");\n@@ -92,0 +103,1 @@\n+  word_size = align_up(word_size, Metaspace::min_allocation_word_size);\n@@ -93,2 +105,5 @@\n-  if (Metaspace::is_class_space_allocation(mdType)) {\n-    return class_space_arena()->allocate(word_size);\n+  MetaBlock result, wastage;\n+  const bool is_class = have_class_space_arena() && mdType == Metaspace::ClassType;\n+  if (is_class) {\n+    assert(word_size >= (sizeof(Klass)\/BytesPerWord), \"weird size for klass: %zu\", word_size);\n+    result = class_space_arena()->allocate(word_size, wastage);\n@@ -96,1 +111,11 @@\n-    return non_class_space_arena()->allocate(word_size);\n+    result = non_class_space_arena()->allocate(word_size, wastage);\n+  }\n+  if (wastage.is_nonempty()) {\n+    non_class_space_arena()->deallocate(wastage);\n+  }\n+#ifdef ASSERT\n+  if (result.is_nonempty()) {\n+    const bool in_class_arena = class_space_arena() != nullptr ? class_space_arena()->contains(result) : false;\n+    const bool in_nonclass_arena = non_class_space_arena()->contains(result);\n+    assert((is_class && in_class_arena) || (!is_class && in_class_arena != in_nonclass_arena),\n+           \"block from neither arena \" METABLOCKFORMAT \"?\", METABLOCKFORMATARGS(result));\n@@ -98,0 +123,2 @@\n+#endif\n+  return result.base();\n@@ -134,5 +161,12 @@\n-  MutexLocker fcl(lock(), Mutex::_no_safepoint_check_flag);\n-  if (Metaspace::using_class_space() && is_class) {\n-    class_space_arena()->deallocate(ptr, word_size);\n-  } else {\n-    non_class_space_arena()->deallocate(ptr, word_size);\n+  NOT_LP64(word_size = align_down(word_size, Metaspace::min_allocation_word_size);)\n+  MetaBlock bl(ptr, word_size);\n+  if (word_size >= Metaspace::min_allocation_word_size) {\n+    MutexLocker fcl(lock(), Mutex::_no_safepoint_check_flag);\n+    if (have_class_space_arena() && is_class) {\n+      assert(class_space_arena()->contains(bl),\n+             \"Not from class arena \" METABLOCKFORMAT \"?\", METABLOCKFORMATARGS(bl));\n+      class_space_arena()->deallocate(MetaBlock(ptr, word_size));\n+    } else {\n+      non_class_space_arena()->deallocate(MetaBlock(ptr, word_size));\n+    }\n+    DEBUG_ONLY(InternalStats::inc_num_deallocs();)\n@@ -140,1 +174,0 @@\n-  DEBUG_ONLY(InternalStats::inc_num_deallocs();)\n@@ -182,1 +215,1 @@\n-    if (Metaspace::using_class_space()) {\n+    if (have_class_space_arena()) {\n","filename":"src\/hotspot\/share\/memory\/classLoaderMetaspace.cpp","additions":55,"deletions":22,"binary":false,"changes":77,"status":"modified"},{"patch":"@@ -36,0 +36,1 @@\n+  class ClmsTester;\n@@ -37,0 +38,1 @@\n+  class MetaspaceContext;\n@@ -60,0 +62,1 @@\n+  friend class metaspace::ClmsTester; \/\/ for gtests\n@@ -78,1 +81,6 @@\n-public:\n+  bool have_class_space_arena() const { return _class_space_arena != nullptr; }\n+\n+  ClassLoaderMetaspace(Mutex* lock, Metaspace::MetaspaceType space_type,\n+                       metaspace::MetaspaceContext* non_class_context,\n+                       metaspace::MetaspaceContext* class_context,\n+                       size_t klass_alignment_words);\n@@ -80,0 +88,1 @@\n+public:\n","filename":"src\/hotspot\/share\/memory\/classLoaderMetaspace.hpp","additions":10,"deletions":1,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -541,0 +541,2 @@\n+const void* Metaspace::_class_space_start = nullptr;\n+const void* Metaspace::_class_space_end = nullptr;\n@@ -573,0 +575,2 @@\n+  _class_space_start = rs.base();\n+  _class_space_end = rs.end();\n@@ -650,0 +654,4 @@\n+    \/\/ Adjust size of the compressed class space.\n+\n+    const size_t res_align = reserve_alignment();\n+\n@@ -655,2 +663,9 @@\n-    size_t max_ccs_size = 8 * (MaxMetaspaceSize \/ 10);\n-    size_t adjusted_ccs_size = MIN2(CompressedClassSpaceSize, max_ccs_size);\n+    const size_t max_ccs_size = 8 * (MaxMetaspaceSize \/ 10);\n+\n+    \/\/ CCS is also limited by the max. possible Klass encoding range size\n+    const size_t max_encoding_range = CompressedKlassPointers::max_encoding_range_size();\n+    assert(max_encoding_range >= res_align,\n+           \"Encoding range (%zu) must cover at least a full root chunk (%zu)\",\n+           max_encoding_range, res_align);\n+\n+    size_t adjusted_ccs_size = MIN3(CompressedClassSpaceSize, max_ccs_size, max_encoding_range);\n@@ -659,3 +674,14 @@\n-    \/\/  root chunk.\n-    adjusted_ccs_size = align_up(adjusted_ccs_size, reserve_alignment());\n-    adjusted_ccs_size = MAX2(adjusted_ccs_size, reserve_alignment());\n+    \/\/  root chunk. But impose a miminum size of 1 root chunk (16MB).\n+    adjusted_ccs_size = MAX2(align_down(adjusted_ccs_size, res_align), res_align);\n+\n+    \/\/ Print a warning if the adjusted size differs from the users input\n+    if (CompressedClassSpaceSize != adjusted_ccs_size) {\n+      #define X \"CompressedClassSpaceSize adjusted from user input \" \\\n+                \"%zu bytes to %zu bytes\", CompressedClassSpaceSize, adjusted_ccs_size\n+      if (FLAG_IS_CMDLINE(CompressedClassSpaceSize)) {\n+        log_warning(metaspace)(X);\n+      } else {\n+        log_info(metaspace)(X);\n+      }\n+      #undef X\n+    }\n@@ -666,1 +692,0 @@\n-\n@@ -777,0 +802,1 @@\n+    \/\/ In CDS=off mode, we give the JVM some leeway to choose a favorable base\/shift combination.\n@@ -845,0 +871,9 @@\n+#ifdef ASSERT\n+    if (using_class_space() && mdtype == ClassType) {\n+      assert(is_in_class_space(result) &&\n+             is_aligned(result, CompressedKlassPointers::klass_alignment_in_bytes()), \"Sanity\");\n+    } else {\n+      assert((is_in_class_space(result) || is_in_nonclass_metaspace(result)) &&\n+             is_aligned(result, Metaspace::min_allocation_alignment_bytes), \"Sanity\");\n+    }\n+#endif\n@@ -847,1 +882,0 @@\n-\n@@ -982,5 +1016,4 @@\n-bool Metaspace::contains(const void* ptr) {\n-  if (MetaspaceShared::is_in_shared_metaspace(ptr)) {\n-    return true;\n-  }\n-  return contains_non_shared(ptr);\n+\/\/ Returns true if pointer points into one of the metaspace regions, or\n+\/\/ into the class space.\n+bool Metaspace::is_in_shared_metaspace(const void* ptr) {\n+  return MetaspaceShared::is_in_shared_metaspace(ptr);\n@@ -989,5 +1022,2 @@\n-bool Metaspace::contains_non_shared(const void* ptr) {\n-  if (using_class_space() && VirtualSpaceList::vslist_class()->contains((MetaWord*)ptr)) {\n-     return true;\n-  }\n-\n+\/\/ Returns true if pointer points into one of the non-class-space metaspace regions.\n+bool Metaspace::is_in_nonclass_metaspace(const void* ptr) {\n","filename":"src\/hotspot\/share\/memory\/metaspace.cpp","additions":47,"deletions":17,"binary":false,"changes":64,"status":"modified"},{"patch":"@@ -29,0 +29,1 @@\n+#include \"memory\/virtualspace.hpp\"\n@@ -38,1 +39,0 @@\n-class ReservedSpace;\n@@ -67,1 +67,3 @@\n-  static bool _initialized;\n+  \/\/ For quick pointer testing: extent of class space; nullptr if no class space.\n+  static const void* _class_space_start;\n+  static const void* _class_space_end;\n@@ -108,0 +110,11 @@\n+  \/\/ Minimum allocation alignment, in bytes. All MetaData shall be aligned correclty\n+  \/\/ to be able to hold 64-bit data types. Unlike malloc, we don't care for larger\n+  \/\/ data types.\n+  static constexpr size_t min_allocation_alignment_bytes = sizeof(uint64_t);\n+\n+  \/\/ Minimum allocation alignment, in words, Metaspace observes.\n+  static constexpr size_t min_allocation_alignment_words = min_allocation_alignment_bytes \/ BytesPerWord;\n+\n+  \/\/ Every allocation will get rounded up to the minimum word size.\n+  static constexpr size_t min_allocation_word_size = min_allocation_alignment_words;\n+\n@@ -116,2 +129,24 @@\n-  static bool contains(const void* ptr);\n-  static bool contains_non_shared(const void* ptr);\n+  static bool contains(const void* ptr) {\n+    return is_in_shared_metaspace(ptr) || \/\/ in cds\n+           is_in_class_space(ptr) ||      \/\/ in class space\n+           is_in_nonclass_metaspace(ptr); \/\/ in one of the non-class regions?\n+  }\n+\n+  \/\/ kept for now for backward compat reasons, but lets test if callers really need this\n+  static bool contains_non_shared(const void* ptr) {\n+    return is_in_class_space(ptr) ||      \/\/ in class space\n+           is_in_nonclass_metaspace(ptr); \/\/ in one of the non-class regions?\n+  }\n+\n+  \/\/ Returns true if pointer points into one of the metaspace regions, or\n+  \/\/ into the class space.\n+  static bool is_in_shared_metaspace(const void* ptr);\n+\n+  \/\/ Returns true if pointer points into one of the non-class-space metaspace regions.\n+  static bool is_in_nonclass_metaspace(const void* ptr);\n+\n+  \/\/ Returns true if ptr points into class space, false if it doesn't or if\n+  \/\/ there is no class space.\n+  static inline bool is_in_class_space(const void* ptr) {\n+    return ptr < _class_space_end && ptr >= _class_space_start;\n+  }\n","filename":"src\/hotspot\/share\/memory\/metaspace.hpp","additions":39,"deletions":4,"binary":false,"changes":43,"status":"modified"},{"patch":"@@ -31,0 +31,1 @@\n+#include \"memory\/metaspace\/metablock.hpp\"\n@@ -39,2 +40,1 @@\n-\/\/ (only a few words). It is used to manage deallocated blocks - see\n-\/\/ class FreeBlocks.\n+\/\/ (only a few words). It is used to manage deallocated small blocks.\n@@ -146,1 +146,4 @@\n-  void add_block(MetaWord* p, size_t word_size) {\n+  void add_block(MetaBlock mb) {\n+    assert(!mb.is_empty(), \"Don't add empty blocks\");\n+    const size_t word_size = mb.word_size();\n+    MetaWord* const p = mb.base();\n@@ -158,2 +161,2 @@\n-  \/\/ Block may be larger. Real block size is returned in *p_real_word_size.\n-  MetaWord* remove_block(size_t word_size, size_t* p_real_word_size) {\n+  \/\/ Block may be larger.\n+  MetaBlock remove_block(size_t word_size) {\n@@ -162,0 +165,1 @@\n+    MetaBlock result;\n@@ -172,5 +176,1 @@\n-      *p_real_word_size = real_word_size;\n-      return (MetaWord*)b;\n-    } else {\n-      *p_real_word_size = 0;\n-      return nullptr;\n+      result = MetaBlock((MetaWord*)b, real_word_size);\n@@ -178,0 +178,1 @@\n+    return result;\n@@ -194,0 +195,1 @@\n+      Block* b_last = nullptr; \/\/ catch simple circularities\n@@ -196,0 +198,1 @@\n+        assert(b != b_last, \"Circle\");\n@@ -197,0 +200,1 @@\n+        b_last = b;\n@@ -198,0 +202,1 @@\n+      if (UseNewCode)printf(\"\\n\");\n","filename":"src\/hotspot\/share\/memory\/metaspace\/binList.hpp","additions":15,"deletions":10,"binary":false,"changes":25,"status":"modified"},{"patch":"@@ -183,2 +183,2 @@\n-void BlockTree::zap_range(MetaWord* p, size_t word_size) {\n-  memset(p, 0xF3, word_size * sizeof(MetaWord));\n+void BlockTree::zap_block(MetaBlock bl) {\n+  memset(bl.base(), 0xF3, bl.word_size() * sizeof(MetaWord));\n@@ -227,0 +227,6 @@\n+      \/\/ Handle simple circularities\n+      if (n == n->_right || n == n->_left || n == n->_next) {\n+        st->print_cr(\"@\" PTR_FORMAT \": circularity detected.\", p2i(n));\n+        return; \/\/ stop printing\n+      }\n+\n","filename":"src\/hotspot\/share\/memory\/metaspace\/blockTree.cpp","additions":8,"deletions":2,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -32,0 +32,1 @@\n+#include \"memory\/metaspace\/metablock.hpp\"\n@@ -38,1 +39,1 @@\n-\/\/  manage small to medium free memory blocks (see class FreeBlocks).\n+\/\/  manage medium to large free memory blocks.\n@@ -83,2 +84,1 @@\n-    \/\/  The space for that is there (these nodes are only used to manage larger blocks,\n-    \/\/  see FreeBlocks::MaxSmallBlocksWordSize).\n+    \/\/  The space for that is there (these nodes are only used to manage larger blocks).\n@@ -338,1 +338,1 @@\n-  void zap_range(MetaWord* p, size_t word_size);\n+  void zap_block(MetaBlock block);\n@@ -348,2 +348,3 @@\n-  void add_block(MetaWord* p, size_t word_size) {\n-    DEBUG_ONLY(zap_range(p, word_size));\n+  void add_block(MetaBlock block) {\n+    DEBUG_ONLY(zap_block(block);)\n+    const size_t word_size = block.word_size();\n@@ -351,1 +352,1 @@\n-    Node* n = new(p) Node(word_size);\n+    Node* n = new(block.base()) Node(word_size);\n@@ -361,3 +362,2 @@\n-  \/\/  larger than that size. Upon return, *p_real_word_size contains the actual\n-  \/\/  block size.\n-  MetaWord* remove_block(size_t word_size, size_t* p_real_word_size) {\n+  \/\/  larger than that size.\n+  MetaBlock remove_block(size_t word_size) {\n@@ -366,0 +366,1 @@\n+    MetaBlock result;\n@@ -382,2 +383,1 @@\n-      MetaWord* p = (MetaWord*)n;\n-      *p_real_word_size = n->_word_size;\n+      result = MetaBlock((MetaWord*)n, n->_word_size);\n@@ -387,2 +387,1 @@\n-      DEBUG_ONLY(zap_range(p, n->_word_size));\n-      return p;\n+      DEBUG_ONLY(zap_block(result);)\n@@ -390,1 +389,1 @@\n-    return nullptr;\n+    return result;\n","filename":"src\/hotspot\/share\/memory\/metaspace\/blockTree.hpp","additions":14,"deletions":15,"binary":false,"changes":29,"status":"modified"},{"patch":"@@ -33,3 +33,3 @@\n-void FreeBlocks::add_block(MetaWord* p, size_t word_size) {\n-  if (word_size > MaxSmallBlocksWordSize) {\n-    _tree.add_block(p, word_size);\n+void FreeBlocks::add_block(MetaBlock bl) {\n+  if (bl.word_size() > _small_blocks.MaxWordSize) {\n+    _tree.add_block(bl);\n@@ -37,1 +37,1 @@\n-    _small_blocks.add_block(p, word_size);\n+    _small_blocks.add_block(bl);\n@@ -41,1 +41,1 @@\n-MetaWord* FreeBlocks::remove_block(size_t requested_word_size) {\n+MetaBlock FreeBlocks::remove_block(size_t requested_word_size) {\n@@ -43,3 +43,3 @@\n-  MetaWord* p = nullptr;\n-  if (requested_word_size > MaxSmallBlocksWordSize) {\n-    p = _tree.remove_block(requested_word_size, &real_size);\n+  MetaBlock bl;\n+  if (requested_word_size > _small_blocks.MaxWordSize) {\n+    bl = _tree.remove_block(requested_word_size);\n@@ -47,1 +47,1 @@\n-    p = _small_blocks.remove_block(requested_word_size, &real_size);\n+    bl = _small_blocks.remove_block(requested_word_size);\n@@ -49,9 +49,1 @@\n-  if (p != nullptr) {\n-    \/\/ Blocks which are larger than a certain threshold are split and\n-    \/\/  the remainder is handed back to the manager.\n-    const size_t waste = real_size - requested_word_size;\n-    if (waste >= MinWordSize) {\n-      add_block(p + requested_word_size, waste);\n-    }\n-  }\n-  return p;\n+  return bl;\n@@ -61,1 +53,0 @@\n-\n","filename":"src\/hotspot\/share\/memory\/metaspace\/freeBlocks.cpp","additions":10,"deletions":19,"binary":false,"changes":29,"status":"modified"},{"patch":"@@ -76,4 +76,0 @@\n-  \/\/ Cutoff point: blocks larger than this size are kept in the\n-  \/\/ tree, blocks smaller than or equal to this size in the bin list.\n-  const size_t MaxSmallBlocksWordSize = BinList32::MaxWordSize;\n-\n@@ -86,1 +82,1 @@\n-  void add_block(MetaWord* p, size_t word_size);\n+  void add_block(MetaBlock bl);\n@@ -88,2 +84,2 @@\n-  \/\/ Retrieve a block of at least requested_word_size.\n-  MetaWord* remove_block(size_t requested_word_size);\n+  \/\/ Retrieve a block of at least requested_word_size. May be larger.\n+  MetaBlock remove_block(size_t requested_word_size);\n","filename":"src\/hotspot\/share\/memory\/metaspace\/freeBlocks.hpp","additions":3,"deletions":7,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -0,0 +1,76 @@\n+\/*\n+ * Copyright (c) 2023 Red Hat, Inc. All rights reserved.\n+ * Copyright (c) 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef SHARE_MEMORY_METASPACE_METABLOCK_HPP\n+#define SHARE_MEMORY_METASPACE_METABLOCK_HPP\n+\n+#include \"utilities\/globalDefinitions.hpp\"\n+\n+class outputStream;\n+\n+namespace metaspace {\n+\n+\/\/ Tiny structure to be passed by value\n+class MetaBlock {\n+\n+  MetaWord* _base;\n+  size_t _word_size;\n+\n+public:\n+\n+  MetaBlock(MetaWord* p, size_t word_size) :\n+    _base(word_size == 0 ? nullptr : p), _word_size(word_size) {}\n+  MetaBlock() : MetaBlock(nullptr, 0) {}\n+\n+  MetaWord* base() const { return _base; }\n+  const MetaWord* end() const { return _base + _word_size; }\n+  size_t word_size() const { return _word_size; }\n+  bool is_empty() const { return _base == nullptr; }\n+  bool is_nonempty() const { return _base != nullptr; }\n+  void reset() { _base = nullptr; _word_size = 0; }\n+\n+  bool operator==(const MetaBlock& rhs) const {\n+    return base() == rhs.base() &&\n+           word_size() == rhs.word_size();\n+  }\n+\n+  \/\/ Split off tail block.\n+  inline MetaBlock split_off_tail(size_t tailsize);\n+\n+  DEBUG_ONLY(inline void verify() const;)\n+\n+  \/\/ Convenience functions\n+  inline bool is_aligned_base(size_t alignment_words) const;\n+  inline bool is_aligned_size(size_t alignment_words) const;\n+\n+  void print_on(outputStream* st) const;\n+};\n+\n+#define METABLOCKFORMAT                 \"block (@\" PTR_FORMAT \" word size \" SIZE_FORMAT \")\"\n+#define METABLOCKFORMATARGS(__block__)  p2i((__block__).base()), (__block__).word_size()\n+\n+} \/\/ namespace metaspace\n+\n+#endif \/\/ SHARE_MEMORY_METASPACE_METABLOCK_HPP\n","filename":"src\/hotspot\/share\/memory\/metaspace\/metablock.hpp","additions":76,"deletions":0,"binary":false,"changes":76,"status":"added"},{"patch":"@@ -0,0 +1,87 @@\n+\/*\n+ * Copyright (c) 2023 Red Hat, Inc. All rights reserved.\n+ * Copyright (c) 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef SHARE_MEMORY_METASPACE_METABLOCK_INLINE_HPP\n+#define SHARE_MEMORY_METASPACE_METABLOCK_INLINE_HPP\n+\n+#include \"memory\/metaspace\/metablock.hpp\"\n+#include \"utilities\/globalDefinitions.hpp\"\n+#include \"utilities\/align.hpp\"\n+#include \"utilities\/ostream.hpp\"\n+#include \"utilities\/debug.hpp\"\n+\n+class outputStream;\n+\n+namespace metaspace {\n+\n+inline MetaBlock MetaBlock::split_off_tail(size_t tailsize) {\n+  if (is_empty() || tailsize == 0) {\n+    return MetaBlock();\n+  }\n+  assert(tailsize <= _word_size, \"invalid split point for block \"\n+         METABLOCKFORMAT \": %zu\", METABLOCKFORMATARGS(*this), tailsize);\n+  const size_t new_size = _word_size - tailsize;\n+  MetaBlock tail(_base + new_size, tailsize);\n+  _word_size = new_size;\n+  if (_word_size == 0) {\n+    _base = nullptr;\n+  }\n+  return tail;\n+}\n+\n+inline void MetaBlock::print_on(outputStream* st) const {\n+  st->print(METABLOCKFORMAT, METABLOCKFORMATARGS(*this));\n+}\n+\n+\/\/ Convenience functions\n+inline bool MetaBlock::is_aligned_base(size_t alignment_words) const {\n+  return is_aligned(_base, alignment_words * BytesPerWord);\n+}\n+\n+inline bool MetaBlock::is_aligned_size(size_t alignment_words) const {\n+  return is_aligned(_word_size, alignment_words);\n+}\n+\n+\/\/ some convenience asserts\n+#define assert_block_base_aligned(block, alignment_words) \\\n+  assert(block.is_aligned_base(alignment_words), \"Block wrong base alignment \" METABLOCKFORMAT, METABLOCKFORMATARGS(block));\n+\n+#define assert_block_size_aligned(block, alignment_words) \\\n+  assert(block.is_aligned_size(alignment_words), \"Block wrong size alignment \" METABLOCKFORMAT, METABLOCKFORMATARGS(block));\n+\n+#define assert_block_larger_or_equal(block, x) \\\n+  assert(block.word_size() >= x, \"Block too small \" METABLOCKFORMAT, METABLOCKFORMATARGS(block));\n+\n+#ifdef ASSERT\n+inline void MetaBlock::verify() const {\n+  assert( (_base == nullptr && _word_size == 0) ||\n+          (_base != nullptr && _word_size > 0),\n+          \"block invalid \" METABLOCKFORMAT, METABLOCKFORMATARGS(*this));\n+}\n+#endif\n+\n+} \/\/ namespace metaspace\n+\n+#endif \/\/ SHARE_MEMORY_METASPACE_METABLOCK_INLINE_HPP\n","filename":"src\/hotspot\/share\/memory\/metaspace\/metablock.inline.hpp","additions":87,"deletions":0,"binary":false,"changes":87,"status":"added"},{"patch":"@@ -33,0 +33,1 @@\n+#include \"memory\/metaspace\/metablock.inline.hpp\"\n@@ -37,0 +38,1 @@\n+#include \"memory\/metaspace\/metaspaceContext.hpp\"\n@@ -59,3 +61,4 @@\n-\/\/ Given a chunk, add its remaining free committed space to the free block list.\n-void MetaspaceArena::salvage_chunk(Metachunk* c) {\n-  size_t remaining_words = c->free_below_committed_words();\n+\/\/ Given a chunk, return the committed remainder of this chunk.\n+MetaBlock MetaspaceArena::salvage_chunk(Metachunk* c) {\n+  MetaBlock result;\n+  const size_t remaining_words = c->free_below_committed_words();\n@@ -68,1 +71,0 @@\n-    _total_used_words_counter->increment_by(remaining_words);\n@@ -70,1 +72,1 @@\n-    add_allocation_to_fbl(ptr, remaining_words);\n+    result = MetaBlock(ptr, remaining_words);\n@@ -77,0 +79,1 @@\n+  return result;\n@@ -100,5 +103,4 @@\n-void MetaspaceArena::add_allocation_to_fbl(MetaWord* p, size_t word_size) {\n-  assert(p != nullptr, \"p is null\");\n-  assert_is_aligned_metaspace_pointer(p);\n-  assert(word_size > 0, \"zero sized\");\n-\n+void MetaspaceArena::add_allocation_to_fbl(MetaBlock bl) {\n+  assert(bl.is_nonempty(), \"Sanity\");\n+  assert_block_base_aligned(bl, allocation_alignment_words());\n+  assert_block_size_aligned(bl, Metaspace::min_allocation_alignment_words);\n@@ -108,1 +110,1 @@\n-  _fbl->add_block(p, word_size);\n+  _fbl->add_block(bl);\n@@ -111,4 +113,6 @@\n-MetaspaceArena::MetaspaceArena(ChunkManager* chunk_manager, const ArenaGrowthPolicy* growth_policy,\n-                               SizeAtomicCounter* total_used_words_counter,\n-                               const char* name) :\n-  _chunk_manager(chunk_manager),\n+MetaspaceArena::MetaspaceArena(MetaspaceContext* context,\n+               const ArenaGrowthPolicy* growth_policy,\n+               size_t allocation_alignment_words,\n+               const char* name) :\n+  _allocation_alignment_words(allocation_alignment_words),\n+  _chunk_manager(context->cm()),\n@@ -118,1 +122,1 @@\n-  _total_used_words_counter(total_used_words_counter),\n+  _total_used_words_counter(context->used_words_counter()),\n@@ -121,1 +125,7 @@\n-  UL(debug, \": born.\");\n+  \/\/ Check arena allocation alignment\n+  assert(is_power_of_2(_allocation_alignment_words) &&\n+         _allocation_alignment_words >= Metaspace::min_allocation_alignment_words &&\n+         _allocation_alignment_words <= chunklevel::MIN_CHUNK_WORD_SIZE,\n+         \"Invalid alignment: %zu\", _allocation_alignment_words);\n+\n+  UL(debug, \"born.\");\n@@ -143,1 +153,1 @@\n-  UL2(info, \"returned %d chunks, total capacity \" SIZE_FORMAT \" words.\",\n+  UL2(debug, \"returned %d chunks, total capacity \" SIZE_FORMAT \" words.\",\n@@ -208,1 +218,1 @@\n-MetaWord* MetaspaceArena::allocate(size_t requested_word_size) {\n+MetaBlock MetaspaceArena::allocate(size_t requested_word_size, MetaBlock& wastage) {\n@@ -211,1 +221,0 @@\n-  MetaWord* p = nullptr;\n@@ -214,0 +223,3 @@\n+  MetaBlock result;\n+  bool taken_from_fbl = false;\n+\n@@ -216,2 +228,8 @@\n-    p = _fbl->remove_block(aligned_word_size);\n-    if (p != nullptr) {\n+    result = _fbl->remove_block(aligned_word_size);\n+    if (result.is_nonempty()) {\n+      assert_block_larger_or_equal(result, aligned_word_size);\n+      assert_block_base_aligned(result, allocation_alignment_words());\n+      assert_block_size_aligned(result, Metaspace::min_allocation_alignment_words);\n+      \/\/ Split off wastage\n+      wastage = result.split_off_tail(result.word_size() - aligned_word_size);\n+      \/\/ Stats, logging\n@@ -219,3 +237,2 @@\n-      UL2(trace, \"returning \" PTR_FORMAT \" - taken from fbl (now: %d, \" SIZE_FORMAT \").\",\n-          p2i(p), _fbl->count(), _fbl->total_size());\n-      assert_is_aligned_metaspace_pointer(p);\n+      UL2(trace, \"returning \" METABLOCKFORMAT \" with wastage \" METABLOCKFORMAT \" - taken from fbl (now: %d, \" SIZE_FORMAT \").\",\n+          METABLOCKFORMATARGS(result), METABLOCKFORMATARGS(wastage), _fbl->count(), _fbl->total_size());\n@@ -223,3 +240,2 @@\n-      \/\/ therefore we have no need to adjust any usage counters (see epilogue of allocate_inner())\n-      \/\/ and can just return here.\n-      return p;\n+      \/\/ therefore we don't need to adjust any usage counters (see epilogue of allocate_inner()).\n+      taken_from_fbl = true;\n@@ -229,2 +245,4 @@\n-  \/\/ Primary allocation\n-  p = allocate_inner(aligned_word_size);\n+  if (result.is_empty()) {\n+    \/\/ Free-block allocation failed; we allocate from the arena.\n+    result = allocate_inner(aligned_word_size, wastage);\n+  }\n@@ -232,1 +250,34 @@\n-  return p;\n+  \/\/ Logging\n+  if (result.is_nonempty()) {\n+    LogTarget(Trace, metaspace) lt;\n+    if (lt.is_enabled()) {\n+      LogStream ls(lt);\n+      ls.print(LOGFMT \": returning \" METABLOCKFORMAT \" taken from %s, \", LOGFMT_ARGS,\n+               METABLOCKFORMATARGS(result), (taken_from_fbl ? \"fbl\" : \"arena\"));\n+      if (wastage.is_empty()) {\n+        ls.print(\"no wastage\");\n+      } else {\n+        ls.print(\"wastage \" METABLOCKFORMAT, METABLOCKFORMATARGS(wastage));\n+      }\n+    }\n+  } else {\n+    UL(info, \"allocation failed, returned null.\");\n+  }\n+\n+  \/\/ Final sanity checks\n+#ifdef ASSERT\n+   result.verify();\n+   wastage.verify();\n+   if (result.is_nonempty()) {\n+     assert(result.word_size() == aligned_word_size &&\n+            is_aligned(result.base(), _allocation_alignment_words * BytesPerWord),\n+            \"result bad or unaligned: \" METABLOCKFORMAT \".\", METABLOCKFORMATARGS(result));\n+   }\n+   if (wastage.is_nonempty()) {\n+     assert(wastage.is_empty() ||\n+            (wastage.is_aligned_base(Metaspace::min_allocation_alignment_words) &&\n+             wastage.is_aligned_size(Metaspace::min_allocation_alignment_words)),\n+            \"Misaligned wastage: \" METABLOCKFORMAT\".\", METABLOCKFORMATARGS(wastage));\n+   }\n+#endif \/\/ ASSERT\n+   return result;\n@@ -236,2 +287,1 @@\n-MetaWord* MetaspaceArena::allocate_inner(size_t word_size) {\n-  assert_is_aligned(word_size, metaspace::AllocationAlignmentWordSize);\n+MetaBlock MetaspaceArena::allocate_inner(size_t word_size, MetaBlock& wastage) {\n@@ -239,1 +289,1 @@\n-  MetaWord* p = nullptr;\n+  MetaBlock result;\n@@ -242,0 +292,1 @@\n+  size_t alignment_gap_size = 0;\n@@ -244,1 +295,0 @@\n-\n@@ -247,0 +297,4 @@\n+    const MetaWord* const chunk_top = current_chunk()->top();\n+    alignment_gap_size = align_up(chunk_top, _allocation_alignment_words * BytesPerWord) - chunk_top;\n+    const size_t word_size_plus_alignment = word_size + alignment_gap_size;\n+\n@@ -249,2 +303,2 @@\n-    if (current_chunk()->free_words() < word_size) {\n-      if (!attempt_enlarge_current_chunk(word_size)) {\n+    if (current_chunk()->free_words() < word_size_plus_alignment) {\n+      if (!attempt_enlarge_current_chunk(word_size_plus_alignment)) {\n@@ -262,2 +316,2 @@\n-      if (!current_chunk()->ensure_committed_additional(word_size)) {\n-        UL2(info, \"commit failure (requested size: \" SIZE_FORMAT \")\", word_size);\n+      if (!current_chunk()->ensure_committed_additional(word_size_plus_alignment)) {\n+        UL2(info, \"commit failure (requested size: \" SIZE_FORMAT \")\", word_size_plus_alignment);\n@@ -270,2 +324,8 @@\n-      p = current_chunk()->allocate(word_size);\n-      assert(p != nullptr, \"Allocation from chunk failed.\");\n+      MetaWord* const p_gap = current_chunk()->allocate(word_size_plus_alignment);\n+      assert(p_gap != nullptr, \"Allocation from chunk failed.\");\n+      MetaWord* const p_user_allocation = p_gap + alignment_gap_size;\n+      result = MetaBlock(p_user_allocation, word_size);\n+      if (alignment_gap_size > 0) {\n+        NOT_LP64(assert(alignment_gap_size >= AllocationAlignmentWordSize, \"Sanity\"));\n+        wastage = MetaBlock(p_gap, alignment_gap_size);\n+      }\n@@ -275,1 +335,1 @@\n-  if (p == nullptr) {\n+  if (result.is_empty()) {\n@@ -289,1 +349,1 @@\n-        salvage_chunk(current_chunk());\n+        wastage = salvage_chunk(current_chunk());\n@@ -295,2 +355,4 @@\n-      \/\/ Now, allocate from that chunk. That should work.\n-      p = current_chunk()->allocate(word_size);\n+      \/\/ Now, allocate from that chunk. That should work. Note that the resulting allocation\n+      \/\/ is guaranteed to be aligned to arena alignment, since arena alignment cannot be larger\n+      \/\/ than smallest chunk size, and chunk starts are aligned by their size (buddy allocation).\n+      MetaWord* const p = current_chunk()->allocate(word_size);\n@@ -298,0 +360,1 @@\n+      result = MetaBlock(p, word_size);\n@@ -303,1 +366,1 @@\n-  if (p == nullptr) {\n+  if (result.is_empty()) {\n@@ -307,1 +370,1 @@\n-    _total_used_words_counter->increment_by(word_size);\n+    _total_used_words_counter->increment_by(word_size + wastage.word_size());\n@@ -312,3 +375,1 @@\n-  if (p == nullptr) {\n-    UL(info, \"allocation failed, returned null.\");\n-  } else {\n+  if (result.is_nonempty()) {\n@@ -317,1 +378,0 @@\n-    UL2(trace, \"returning \" PTR_FORMAT \".\", p2i(p));\n@@ -320,1 +380,12 @@\n-  assert_is_aligned_metaspace_pointer(p);\n+#ifdef ASSERT\n+  if (wastage.is_nonempty()) {\n+    \/\/ Wastage from arena allocations only occurs if either or both are true:\n+    \/\/ - it is too small to hold the requested allocation words\n+    \/\/ - it is misaligned\n+    assert(!wastage.is_aligned_base(allocation_alignment_words()) ||\n+           wastage.word_size() < word_size,\n+           \"Unexpected wastage: \" METABLOCKFORMAT \", arena alignment: %zu, allocation word size: %zu\",\n+           METABLOCKFORMATARGS(wastage), allocation_alignment_words(), word_size);\n+    wastage.verify();\n+  }\n+#endif \/\/ ASSERT\n@@ -322,1 +393,1 @@\n-  return p;\n+  return result;\n@@ -327,19 +398,12 @@\n-void MetaspaceArena::deallocate(MetaWord* p, size_t word_size) {\n-  \/\/ At this point a current chunk must exist since we only deallocate if we did allocate before.\n-  assert(current_chunk() != nullptr, \"stray deallocation?\");\n-  assert(is_valid_area(p, word_size),\n-         \"Pointer range not part of this Arena and cannot be deallocated: (\" PTR_FORMAT \"..\" PTR_FORMAT \").\",\n-         p2i(p), p2i(p + word_size));\n-\n-  UL2(trace, \"deallocating \" PTR_FORMAT \", word size: \" SIZE_FORMAT \".\",\n-      p2i(p), word_size);\n-\n-  \/\/ Only blocks that had been allocated via MetaspaceArena::allocate(size) must be handed in\n-  \/\/ to MetaspaceArena::deallocate(), and only with the same size that had been original used for allocation.\n-  \/\/ Therefore the pointer must be aligned correctly, and size can be alignment-adjusted (the latter\n-  \/\/ only matters on 32-bit):\n-  assert_is_aligned_metaspace_pointer(p);\n-  size_t raw_word_size = get_raw_word_size_for_requested_word_size(word_size);\n-\n-  add_allocation_to_fbl(p, raw_word_size);\n-\n+void MetaspaceArena::deallocate(MetaBlock block) {\n+  DEBUG_ONLY(block.verify();)\n+  \/\/ This only matters on 32-bit:\n+  \/\/ Since we always align up allocations from arena, we align up here, too.\n+#ifndef _LP64\n+  MetaBlock raw_block(block.base(), get_raw_word_size_for_requested_word_size(block.word_size()));\n+  add_allocation_to_fbl(raw_block);\n+#else\n+  add_allocation_to_fbl(block);\n+#endif\n+  UL2(trace, \"added to fbl: \" METABLOCKFORMAT \", (now: %d, \" SIZE_FORMAT \").\",\n+      METABLOCKFORMATARGS(block), _fbl->count(), _fbl->total_size());\n@@ -403,4 +467,5 @@\n-\/\/ Returns true if the area indicated by pointer and size have actually been allocated\n-\/\/ from this arena.\n-bool MetaspaceArena::is_valid_area(MetaWord* p, size_t word_size) const {\n-  assert(p != nullptr && word_size > 0, \"Sanity\");\n+\/\/ Returns true if the given block is contained in this arena\n+\/\/ Returns true if the given block is contained in this arena\n+bool MetaspaceArena::contains(MetaBlock bl) const {\n+  DEBUG_ONLY(bl.verify();)\n+  assert(bl.is_nonempty(), \"Sanity\");\n@@ -409,3 +474,3 @@\n-    assert(c->is_valid_committed_pointer(p) ==\n-           c->is_valid_committed_pointer(p + word_size - 1), \"range intersects\");\n-    found = c->is_valid_committed_pointer(p);\n+    assert(c->is_valid_committed_pointer(bl.base()) ==\n+           c->is_valid_committed_pointer(bl.end() - 1), \"range intersects\");\n+    found = c->is_valid_committed_pointer(bl.base());\n","filename":"src\/hotspot\/share\/memory\/metaspace\/metaspaceArena.cpp","additions":144,"deletions":79,"binary":false,"changes":223,"status":"modified"},{"patch":"@@ -32,0 +32,1 @@\n+#include \"memory\/metaspace\/metablock.hpp\"\n@@ -40,0 +41,2 @@\n+struct ArenaStats;\n+class MetaspaceContext;\n@@ -41,1 +44,0 @@\n-class Metachunk;\n@@ -43,0 +45,1 @@\n+class Metachunk;\n@@ -44,1 +47,0 @@\n-struct ArenaStats;\n@@ -77,0 +79,1 @@\n+  friend class MetaspaceArenaTestFriend;\n@@ -81,0 +84,2 @@\n+  const size_t _allocation_alignment_words;\n+\n@@ -107,1 +112,1 @@\n-  void add_allocation_to_fbl(MetaWord* p, size_t word_size);\n+  void add_allocation_to_fbl(MetaBlock bl);\n@@ -109,2 +114,2 @@\n-  \/\/ Given a chunk, add its remaining free committed space to the free block list.\n-  void salvage_chunk(Metachunk* c);\n+  \/\/ Given a chunk, return the committed remainder of this chunk.\n+  MetaBlock salvage_chunk(Metachunk* c);\n@@ -125,4 +130,0 @@\n-  \/\/ Returns true if the area indicated by pointer and size have actually been allocated\n-  \/\/ from this arena.\n-  DEBUG_ONLY(bool is_valid_area(MetaWord* p, size_t word_size) const;)\n-\n@@ -130,1 +131,1 @@\n-  MetaWord* allocate_inner(size_t word_size);\n+  MetaBlock allocate_inner(size_t word_size, MetaBlock& wastage);\n@@ -134,2 +135,3 @@\n-  MetaspaceArena(ChunkManager* chunk_manager, const ArenaGrowthPolicy* growth_policy,\n-                 SizeAtomicCounter* total_used_words_counter,\n+  MetaspaceArena(MetaspaceContext* context,\n+                 const ArenaGrowthPolicy* growth_policy,\n+                 size_t allocation_alignment_words,\n@@ -140,0 +142,3 @@\n+  size_t allocation_alignment_words() const { return _allocation_alignment_words; }\n+  size_t allocation_alignment_bytes() const { return allocation_alignment_words() * BytesPerWord; }\n+\n@@ -141,6 +146,5 @@\n-  \/\/ 1) Attempt to allocate from the dictionary of deallocated blocks.\n-  \/\/ 2) Attempt to allocate from the current chunk.\n-  \/\/ 3) Attempt to enlarge the current chunk in place if it is too small.\n-  \/\/ 4) Attempt to get a new chunk and allocate from that chunk.\n-  \/\/ At any point, if we hit a commit limit, we return null.\n-  MetaWord* allocate(size_t word_size);\n+  \/\/ On success, returns non-empty block of the specified word size, and\n+  \/\/ possibly a wastage block that is the result of alignment operations.\n+  \/\/ On failure, returns an empty block. Failure may happen if we hit a\n+  \/\/ commit limit.\n+  MetaBlock allocate(size_t word_size, MetaBlock& wastage);\n@@ -150,1 +154,1 @@\n-  void deallocate(MetaWord* p, size_t word_size);\n+  void deallocate(MetaBlock bl);\n@@ -164,0 +168,2 @@\n+  \/\/ Returns true if the given block is contained in this arena\n+  DEBUG_ONLY(bool contains(MetaBlock bl) const;)\n","filename":"src\/hotspot\/share\/memory\/metaspace\/metaspaceArena.hpp","additions":25,"deletions":19,"binary":false,"changes":44,"status":"modified"},{"patch":"@@ -45,2 +45,1 @@\n-\/\/ Klass* structures need to be aligned to KlassAlignmentInBytes, but since that is\n-\/\/ 64-bit, we don't need special handling for allocating Klass*.\n+\/\/ Klass* structures need to be aligned to Klass* alignment,\n@@ -51,1 +50,0 @@\n-STATIC_ASSERT(AllocationAlignmentByteSize == (size_t)KlassAlignmentInBytes);\n","filename":"src\/hotspot\/share\/memory\/metaspace\/metaspaceCommon.hpp","additions":1,"deletions":3,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -78,0 +78,12 @@\n+size_t MetaspaceContext::used_words() const {\n+  return _used_words_counter.get();\n+}\n+\n+size_t MetaspaceContext::committed_words() const {\n+  return _vslist->committed_words();\n+}\n+\n+size_t MetaspaceContext::reserved_words() const {\n+  return _vslist->reserved_words();\n+}\n+\n","filename":"src\/hotspot\/share\/memory\/metaspace\/metaspaceContext.cpp","additions":12,"deletions":0,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -30,0 +30,1 @@\n+#include \"memory\/metaspace\/counters.hpp\"\n@@ -64,0 +65,1 @@\n+  SizeAtomicCounter _used_words_counter;\n@@ -81,2 +83,3 @@\n-  VirtualSpaceList* vslist() { return _vslist; }\n-  ChunkManager* cm() { return _cm; }\n+  VirtualSpaceList* vslist()                    { return _vslist; }\n+  ChunkManager* cm()                            { return _cm; }\n+  SizeAtomicCounter* used_words_counter()       { return &_used_words_counter; }\n@@ -106,0 +109,3 @@\n+  size_t used_words() const;\n+  size_t committed_words() const;\n+  size_t reserved_words() const;\n","filename":"src\/hotspot\/share\/memory\/metaspace\/metaspaceContext.hpp","additions":8,"deletions":2,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -42,0 +42,1 @@\n+#include \"oops\/compressedKlass.hpp\"\n@@ -121,0 +122,3 @@\n+#ifdef _LP64\n+  CompressedKlassPointers::print_mode(out);\n+#endif\n@@ -328,1 +332,1 @@\n-  out->print(\"(percentages refer to total committed size \");\n+  out->print(\" (percentages refer to total committed size \");\n","filename":"src\/hotspot\/share\/memory\/metaspace\/metaspaceReporter.cpp","additions":5,"deletions":1,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -189,1 +189,0 @@\n-\n@@ -196,3 +195,0 @@\n-  \/\/ Deallocated allocations still count as used\n-  assert(total_used >= _free_blocks_word_size,\n-         \"Sanity\");\n","filename":"src\/hotspot\/share\/memory\/metaspace\/metaspaceStatistics.cpp","additions":0,"deletions":4,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -28,1 +28,1 @@\n-#include \"memory\/metaspace\/counters.hpp\"\n+#include \"memory\/metaspace\/metaspaceContext.hpp\"\n@@ -34,3 +34,0 @@\n-SizeAtomicCounter RunningCounters::_used_class_counter;\n-SizeAtomicCounter RunningCounters::_used_nonclass_counter;\n-\n@@ -75,1 +72,2 @@\n-  return _used_class_counter.get();\n+  const MetaspaceContext* context = MetaspaceContext::context_class();\n+  return context != nullptr ? context->used_words() : 0;\n@@ -79,1 +77,1 @@\n-  return _used_nonclass_counter.get();\n+  return MetaspaceContext::context_nonclass()->used_words();\n","filename":"src\/hotspot\/share\/memory\/metaspace\/runningCounters.cpp","additions":4,"deletions":6,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -30,1 +30,0 @@\n-#include \"memory\/metaspace\/counters.hpp\"\n@@ -35,6 +34,1 @@\n-class RunningCounters : public AllStatic {\n-\n-  static SizeAtomicCounter _used_class_counter;\n-  static SizeAtomicCounter _used_nonclass_counter;\n-\n-public:\n+struct RunningCounters : public AllStatic {\n@@ -68,4 +62,0 @@\n-  \/\/ Direct access to the counters.\n-  static SizeAtomicCounter* used_nonclass_counter()     { return &_used_nonclass_counter; }\n-  static SizeAtomicCounter* used_class_counter()        { return &_used_class_counter; }\n-\n","filename":"src\/hotspot\/share\/memory\/metaspace\/runningCounters.hpp","additions":1,"deletions":11,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -57,1 +57,6 @@\n-  return _arena->allocate(word_size);\n+  MetaBlock result, wastage;\n+  result = _arena->allocate(word_size, wastage);\n+  if (wastage.is_nonempty()) {\n+    _arena->deallocate(wastage);\n+  }\n+  return result.base();\n@@ -62,1 +67,1 @@\n-  return _arena->deallocate(p, word_size);\n+  _arena->deallocate(MetaBlock(p, word_size));\n@@ -73,1 +78,0 @@\n-  _used_words_counter(),\n@@ -106,1 +110,1 @@\n-    arena = new MetaspaceArena(_context->cm(), growth_policy, &_used_words_counter, _name);\n+    arena = new MetaspaceArena(_context, growth_policy, Metaspace::min_allocation_alignment_words, _name);\n@@ -127,0 +131,13 @@\n+size_t MetaspaceTestContext::used_words() const {\n+  return _context->used_words_counter()->get();\n+}\n+\n+size_t MetaspaceTestContext::committed_words() const {\n+  assert(_commit_limiter.committed_words() == _context->committed_words(), \"Sanity\");\n+  return _context->committed_words();\n+}\n+\n+size_t MetaspaceTestContext::reserved_words() const {\n+  return _context->reserved_words();\n+}\n+\n","filename":"src\/hotspot\/share\/memory\/metaspace\/testHelpers.cpp","additions":21,"deletions":4,"binary":false,"changes":25,"status":"modified"},{"patch":"@@ -78,1 +78,0 @@\n-  SizeAtomicCounter _used_words_counter;\n@@ -101,0 +100,1 @@\n+  MetaspaceContext* context() const           { return _context; }\n@@ -107,3 +107,3 @@\n-  \/\/ Convenience function to retrieve total committed\/used words\n-  size_t used_words() const       { return _used_words_counter.get(); }\n-  size_t committed_words() const  { return _commit_limiter.committed_words(); }\n+  size_t used_words() const;\n+  size_t committed_words() const;\n+  size_t reserved_words() const;\n","filename":"src\/hotspot\/share\/memory\/metaspace\/testHelpers.hpp","additions":4,"deletions":4,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -385,2 +385,7 @@\n-  assert(oopDesc::klass_offset_in_bytes() < static_cast<intptr_t>(os::vm_page_size()),\n-         \"Klass offset is expected to be less than the page size\");\n+  if (UseCompactObjectHeaders) {\n+    assert(oopDesc::mark_offset_in_bytes() < static_cast<intptr_t>(os::vm_page_size()),\n+           \"Mark offset is expected to be less than the page size\");\n+  } else {\n+    assert(oopDesc::klass_offset_in_bytes() < static_cast<intptr_t>(os::vm_page_size()),\n+           \"Klass offset is expected to be less than the page size\");\n+  }\n","filename":"src\/hotspot\/share\/memory\/universe.cpp","additions":7,"deletions":2,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -85,2 +85,7 @@\n-    return UseCompressedClassPointers ? klass_gap_offset_in_bytes() :\n-                               (int)sizeof(arrayOopDesc);\n+    if (UseCompactObjectHeaders) {\n+      return oopDesc::base_offset_in_bytes();\n+    } else if (UseCompressedClassPointers) {\n+      return klass_gap_offset_in_bytes();\n+    } else {\n+      return (int)sizeof(arrayOopDesc);\n+    }\n","filename":"src\/hotspot\/share\/oops\/arrayOop.hpp","additions":7,"deletions":2,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -28,1 +28,2 @@\n-#include \"oops\/compressedKlass.hpp\"\n+#include \"oops\/klass.hpp\"\n+#include \"oops\/compressedKlass.inline.hpp\"\n@@ -30,0 +31,1 @@\n+#include \"runtime\/java.hpp\"\n@@ -35,3 +37,13 @@\n-address CompressedKlassPointers::_base = nullptr;\n-int CompressedKlassPointers::_shift = 0;\n-size_t CompressedKlassPointers::_range = 0;\n+int CompressedKlassPointers::_tiny_cp = -1;\n+int CompressedKlassPointers::_narrow_klass_pointer_bits = -1;\n+int CompressedKlassPointers::_max_shift = -1;\n+#ifdef ASSERT\n+address CompressedKlassPointers::_klass_range_start = (address)-1;\n+address CompressedKlassPointers::_klass_range_end = (address)-1;\n+narrowKlass CompressedKlassPointers::_lowest_valid_narrow_klass_id = (narrowKlass)-1;\n+narrowKlass CompressedKlassPointers::_highest_valid_narrow_klass_id = (narrowKlass)-1;\n+#endif\n+\n+address CompressedKlassPointers::_base = (address)-1;\n+int CompressedKlassPointers::_shift = -1;\n+size_t CompressedKlassPointers::_range = (size_t)-1;\n@@ -41,4 +53,22 @@\n-#ifdef ASSERT\n-void CompressedKlassPointers::assert_is_valid_encoding(address addr, size_t len, address base, int shift) {\n-  assert(base + nth_bit(32 + shift) >= addr + len, \"Encoding (base=\" PTR_FORMAT \", shift=%d) does not \"\n-         \"fully cover the class range \" PTR_FORMAT \"-\" PTR_FORMAT, p2i(base), shift, p2i(addr), p2i(addr + len));\n+\/\/ Returns the maximum encoding range that can be covered with the currently\n+\/\/ chosen nKlassID geometry (nKlass bit size, max shift)\n+size_t CompressedKlassPointers::max_encoding_range_size() {\n+  \/\/ Whatever the nKlass geometry is, we don't support cases where the offset\n+  \/\/ into the Klass encoding range (the shifted nKlass) exceeds 32 bits. That\n+  \/\/ is because many CPU-specific decoding functions use e.g. 16-bit moves to\n+  \/\/ combine base and offset.\n+  constexpr int max_preshifted_nklass_bits = 32;\n+  return nth_bit(MIN2(max_preshifted_nklass_bits,\n+                      narrow_klass_pointer_bits() + max_shift()));\n+}\n+\n+void CompressedKlassPointers::pre_initialize() {\n+  if (UseCompactObjectHeaders) {\n+    _tiny_cp = 1;\n+    _narrow_klass_pointer_bits = narrow_klass_pointer_bits_tinycp;\n+    _max_shift = max_shift_tinycp;\n+  } else {\n+    _tiny_cp = 0;\n+    _narrow_klass_pointer_bits = narrow_klass_pointer_bits_legacy;\n+    _max_shift = max_shift_legacy;\n+  }\n@@ -46,0 +76,60 @@\n+\n+#ifdef ASSERT\n+void CompressedKlassPointers::sanity_check_after_initialization() {\n+  \/\/ In expectation of an assert, prepare condensed info to be printed with the assert.\n+  char tmp[256];\n+  os::snprintf(tmp, sizeof(tmp), PTR_FORMAT \" \" PTR_FORMAT \" \" PTR_FORMAT \" %d \" SIZE_FORMAT \" %u %u\",\n+      p2i(_klass_range_start), p2i(_klass_range_end), p2i(_base), _shift, _range,\n+      _lowest_valid_narrow_klass_id, _highest_valid_narrow_klass_id);\n+#define ASSERT_HERE(cond) assert(cond, \" (%s)\", tmp);\n+#define ASSERT_HERE_2(cond, msg) assert(cond, msg \" (%s)\", tmp);\n+\n+  \/\/ All values must be inited\n+  ASSERT_HERE(_max_shift != -1);\n+  ASSERT_HERE(_klass_range_start != (address)-1);\n+  ASSERT_HERE(_klass_range_end != (address)-1);\n+  ASSERT_HERE(_lowest_valid_narrow_klass_id != (narrowKlass)-1);\n+  ASSERT_HERE(_base != (address)-1);\n+  ASSERT_HERE(_shift != -1);\n+  ASSERT_HERE(_range != (size_t)-1);\n+\n+  const size_t klab = klass_alignment_in_bytes();\n+  \/\/ must be aligned enough hold 64-bit data\n+  ASSERT_HERE(is_aligned(klab, sizeof(uint64_t)));\n+\n+  \/\/ should be smaller than the minimum metaspace chunk size (soft requirement)\n+  ASSERT_HERE(klab <= K);\n+\n+  \/\/ Check that Klass range is fully engulfed in the encoding range\n+  ASSERT_HERE(_klass_range_end > _klass_range_start);\n+\n+  const address encoding_end = _base + nth_bit(narrow_klass_pointer_bits() + _shift);\n+  ASSERT_HERE_2(_klass_range_start >= _base && _klass_range_end <= encoding_end,\n+                \"Resulting encoding range does not fully cover the class range\");\n+\n+  \/\/ Check that Klass range is aligned to Klass alignment. That should never be an issue since we mmap the\n+  \/\/ relevant regions and klass alignment - tied to smallest metachunk size of 1K - will always be smaller\n+  \/\/ than smallest page size of 4K.\n+  ASSERT_HERE_2(is_aligned(_klass_range_start, klab) && is_aligned(_klass_range_end, klab),\n+                \"Klass range must start and end at a properly aligned address\");\n+\n+  \/\/ Check that lowest and highest possible narrowKlass values make sense\n+  ASSERT_HERE_2(_lowest_valid_narrow_klass_id > 0, \"Null is not a valid narrowKlass\");\n+  ASSERT_HERE(_highest_valid_narrow_klass_id > _lowest_valid_narrow_klass_id);\n+\n+  Klass* k1 = decode_not_null_without_asserts(_lowest_valid_narrow_klass_id, _base, _shift);\n+  ASSERT_HERE_2((address)k1 == _klass_range_start + klab, \"Not lowest\");\n+  narrowKlass nk1 = encode_not_null_without_asserts(k1, _base, _shift);\n+  ASSERT_HERE_2(nk1 == _lowest_valid_narrow_klass_id, \"not reversible\");\n+\n+  Klass* k2 = decode_not_null_without_asserts(_highest_valid_narrow_klass_id, _base, _shift);\n+  \/\/ _highest_valid_narrow_klass_id must be decoded to the highest theoretically possible\n+  \/\/ valid Klass* position in range, if we assume minimal Klass size\n+  ASSERT_HERE((address)k2 < _klass_range_end);\n+  ASSERT_HERE_2(align_up(((address)k2 + sizeof(Klass)), klab) >= _klass_range_end, \"Not highest\");\n+  narrowKlass nk2 = encode_not_null_without_asserts(k2, _base, _shift);\n+  ASSERT_HERE_2(nk2 == _highest_valid_narrow_klass_id, \"not reversible\");\n+\n+#ifdef AARCH64\n+  \/\/ On aarch64, we never expect a shift value > 0 in legacy mode\n+  ASSERT_HERE_2(tiny_classpointer_mode() || _shift == 0, \"Shift > 0 in legacy mode?\");\n@@ -47,0 +137,15 @@\n+#undef ASSERT_HERE\n+#undef ASSERT_HERE_2\n+}\n+\n+void CompressedKlassPointers::calc_lowest_highest_narrow_klass_id() {\n+  \/\/ Given a Klass range, calculate lowest and highest narrowKlass.\n+  const size_t klab = klass_alignment_in_bytes();\n+  \/\/ Note that 0 is not a valid narrowKlass, and Metaspace prevents us for that reason from allocating at\n+  \/\/ the very start of class space. So the very first valid Klass position is start-of-range + klab.\n+  _lowest_valid_narrow_klass_id =\n+      (narrowKlass) (((uintptr_t)(_klass_range_start - _base) + klab) >> _shift);\n+  address highest_possible_klass = align_down(_klass_range_end - sizeof(Klass), klab);\n+  _highest_valid_narrow_klass_id = (narrowKlass) ((uintptr_t)(highest_possible_klass - _base) >> _shift);\n+}\n+#endif \/\/ ASSERT\n@@ -54,2 +159,9 @@\n-  const int narrow_klasspointer_bits = sizeof(narrowKlass) * 8;\n-  const size_t encoding_range_size = nth_bit(narrow_klasspointer_bits + requested_shift);\n+  if (len > max_encoding_range_size()) {\n+    stringStream ss;\n+    ss.print(\"Class space size and CDS archive size combined (%zu) \"\n+             \"exceed the maximum possible size (%zu)\",\n+             len, max_encoding_range_size());\n+    vm_exit_during_initialization(ss.base());\n+  }\n+\n+  const size_t encoding_range_size = nth_bit(narrow_klass_pointer_bits() + requested_shift);\n@@ -61,1 +173,0 @@\n-  assert(encoding_range_end >= end, \"Encoding does not cover the full Klass range\");\n@@ -67,1 +178,8 @@\n-  DEBUG_ONLY(assert_is_valid_encoding(addr, len, _base, _shift);)\n+#ifdef ASSERT\n+  _klass_range_start = addr;\n+  _klass_range_end = addr + len;\n+  calc_lowest_highest_narrow_klass_id();\n+  sanity_check_after_initialization();\n+#endif\n+\n+  DEBUG_ONLY(sanity_check_after_initialization();)\n@@ -76,1 +194,2 @@\n-  return reserve_address_space_X(0, nth_bit(32), size, Metaspace::reserve_alignment(), aslr);\n+  const size_t unscaled_max = nth_bit(narrow_klass_pointer_bits());\n+  return reserve_address_space_X(0, unscaled_max, size, Metaspace::reserve_alignment(), aslr);\n@@ -80,1 +199,3 @@\n-  return reserve_address_space_X(nth_bit(32), nth_bit(32 + LogKlassAlignmentInBytes), size, Metaspace::reserve_alignment(), aslr);\n+  const size_t unscaled_max = nth_bit(narrow_klass_pointer_bits());\n+  const size_t zerobased_max = nth_bit(narrow_klass_pointer_bits() + max_shift());\n+  return reserve_address_space_X(unscaled_max, zerobased_max, size, Metaspace::reserve_alignment(), aslr);\n@@ -87,2 +208,0 @@\n-#if !defined(AARCH64) || defined(ZERO)\n-\/\/ On aarch64 we have an own version; all other platforms use the default version\n@@ -90,8 +209,0 @@\n-  \/\/ The default version of this code tries, in order of preference:\n-  \/\/ -unscaled    (base=0 shift=0)\n-  \/\/ -zero-based  (base=0 shift>0)\n-  \/\/ -nonzero-base (base>0 shift=0)\n-  \/\/ Note that base>0 shift>0 should never be needed, since the klass range will\n-  \/\/ never exceed 4GB.\n-  constexpr uintptr_t unscaled_max = nth_bit(32);\n-  assert(len <= unscaled_max, \"Klass range larger than 32 bits?\");\n@@ -99,1 +210,30 @@\n-  constexpr uintptr_t zerobased_max = nth_bit(32 + LogKlassAlignmentInBytes);\n+  if (len > max_encoding_range_size()) {\n+    stringStream ss;\n+    ss.print(\"Class space size (%zu) exceeds the maximum possible size (%zu)\",\n+              len, max_encoding_range_size());\n+    vm_exit_during_initialization(ss.base());\n+  }\n+\n+  \/\/ Give CPU a shot at a specialized init sequence\n+#ifndef ZERO\n+  if (pd_initialize(addr, len)) {\n+    return;\n+  }\n+#endif\n+\n+  if (tiny_classpointer_mode()) {\n+\n+    \/\/ In tiny classpointer mode, we don't attempt for zero-based mode.\n+    \/\/ Instead, we set the base to the start of the klass range and then try\n+    \/\/ for the smallest shift possible that still covers the whole range.\n+    \/\/ The reason is that we want to avoid, if possible, shifts larger than\n+    \/\/ a cacheline size.\n+    _base = addr;\n+    _range = len;\n+\n+    constexpr int log_cacheline = 6;\n+    int s = max_shift();\n+    while (s > log_cacheline && ((size_t)nth_bit(narrow_klass_pointer_bits() + s - 1) > len)) {\n+      s--;\n+    }\n+    _shift = s;\n@@ -101,4 +241,0 @@\n-  address const end = addr + len;\n-  if (end <= (address)unscaled_max) {\n-    _base = nullptr;\n-    _shift = 0;\n@@ -106,1 +242,12 @@\n-    if (end <= (address)zerobased_max) {\n+\n+    \/\/ In legacy mode, we try, in order of preference:\n+    \/\/ -unscaled    (base=0 shift=0)\n+    \/\/ -zero-based  (base=0 shift>0)\n+    \/\/ -nonzero-base (base>0 shift=0)\n+    \/\/ Note that base>0 shift>0 should never be needed, since the klass range will\n+    \/\/ never exceed 4GB.\n+    const uintptr_t unscaled_max = nth_bit(narrow_klass_pointer_bits());\n+    const uintptr_t zerobased_max = nth_bit(narrow_klass_pointer_bits() + max_shift());\n+\n+    address const end = addr + len;\n+    if (end <= (address)unscaled_max) {\n@@ -108,3 +255,0 @@\n-      _shift = LogKlassAlignmentInBytes;\n-    } else {\n-      _base = addr;\n@@ -112,0 +256,8 @@\n+    } else {\n+      if (end <= (address)zerobased_max) {\n+        _base = nullptr;\n+        _shift = max_shift();\n+      } else {\n+        _base = addr;\n+        _shift = 0;\n+      }\n@@ -113,0 +265,2 @@\n+    _range = end - _base;\n+\n@@ -114,1 +268,0 @@\n-  _range = end - _base;\n@@ -116,1 +269,6 @@\n-  DEBUG_ONLY(assert_is_valid_encoding(addr, len, _base, _shift);)\n+#ifdef ASSERT\n+  _klass_range_start = addr;\n+  _klass_range_end = addr + len;\n+  calc_lowest_highest_narrow_klass_id();\n+  sanity_check_after_initialization();\n+#endif\n@@ -118,1 +276,0 @@\n-#endif \/\/ !AARCH64 || ZERO\n@@ -121,0 +278,8 @@\n+  st->print_cr(\"UseCompressedClassPointers %d, UseCompactObjectHeaders %d, \"\n+               \"narrow klass pointer bits %d, max shift %d\",\n+               UseCompressedClassPointers, UseCompactObjectHeaders,\n+               _narrow_klass_pointer_bits, _max_shift);\n+  if (_base == (address)-1) {\n+    st->print_cr(\"Narrow klass encoding not initialized\");\n+    return;\n+  }\n@@ -124,0 +289,6 @@\n+#ifdef ASSERT\n+  st->print_cr(\"Klass range: [\" PTR_FORMAT \",\" PTR_FORMAT \")\",\n+               p2i(_klass_range_start), p2i(_klass_range_end));\n+  st->print_cr(\"Lowest valid nklass id: %u Highest valid nklass id: %u\",\n+               _lowest_valid_narrow_klass_id, _highest_valid_narrow_klass_id);\n+#endif\n","filename":"src\/hotspot\/share\/oops\/compressedKlass.cpp","additions":207,"deletions":36,"binary":false,"changes":243,"status":"modified"},{"patch":"@@ -37,8 +37,0 @@\n-const int LogKlassAlignmentInBytes = 3;\n-const int KlassAlignmentInBytes    = 1 << LogKlassAlignmentInBytes;\n-\n-\/\/ Maximal size of compressed class space. Above this limit compression is not possible.\n-\/\/ Also upper bound for placement of zero based class space. (Class space is further limited\n-\/\/ to be < 3G, see arguments.cpp.)\n-const  uint64_t KlassEncodingMetaspaceMax = (uint64_t(max_juint) + 1) << LogKlassAlignmentInBytes;\n-\n@@ -50,0 +42,19 @@\n+  \/\/ Tiny-class-pointer mode\n+  static int _tiny_cp; \/\/ -1, 0=true, 1=false\n+\n+  \/\/ We use a different narrow Klass pointer geometry depending on\n+  \/\/ whether we run in standard mode or in compact-object-header-mode (Lilliput):\n+  \/\/ In Lilliput, we use smaller-than-32-bit class pointers (\"tiny classpointer mode\")\n+\n+  \/\/ Narrow klass pointer bits for an unshifted narrow Klass pointer.\n+  static constexpr int narrow_klass_pointer_bits_legacy = 32;\n+  static constexpr int narrow_klass_pointer_bits_tinycp = 22;\n+\n+  static int _narrow_klass_pointer_bits;\n+\n+  \/\/ The maximum shift we can use for standard mode and for TinyCP mode\n+  static constexpr int max_shift_legacy = 3;\n+  static constexpr int max_shift_tinycp = 10;\n+\n+  static int _max_shift;\n+\n@@ -66,1 +77,20 @@\n-  DEBUG_ONLY(static void assert_is_valid_encoding(address addr, size_t len, address base, int shift);)\n+  \/\/ Returns the highest address expressable with an unshifted narrow Klass pointer\n+  inline static uintptr_t highest_unscaled_address();\n+\n+  static bool pd_initialize(address addr, size_t len);\n+\n+#ifdef ASSERT\n+  \/\/ For sanity checks: Klass range\n+  static address _klass_range_start;\n+  static address _klass_range_end;\n+  \/\/ For sanity checks: lowest, highest valid narrow klass ids != null\n+  static narrowKlass _lowest_valid_narrow_klass_id;\n+  static narrowKlass _highest_valid_narrow_klass_id;\n+  static void calc_lowest_highest_narrow_klass_id();\n+  static void sanity_check_after_initialization();\n+#endif \/\/ ASSERT\n+\n+  template <typename T>\n+  static inline void check_init(T var) {\n+    assert(var != (T)-1, \"Not yet initialized\");\n+  }\n@@ -75,0 +105,35 @@\n+  \/\/ Initialization sequence:\n+  \/\/ 1) Parse arguments. The following arguments take a role:\n+  \/\/      - UseCompressedClassPointers\n+  \/\/      - UseCompactObjectHeaders\n+  \/\/      - Xshare on off dump\n+  \/\/      - CompressedClassSpaceSize\n+  \/\/ 2) call pre_initialize(): depending on UseCompactObjectHeaders, defines the limits of narrow Klass pointer\n+  \/\/    geometry (how many bits, the max. possible shift)\n+  \/\/ 3) .. from here on, narrow_klass_pointer_bits() and max_shift() can be used\n+  \/\/ 4) call reserve_address_space_for_compressed_classes() either from CDS initialization or, if CDS is off,\n+  \/\/    from metaspace initialization. Reserves space for class space + CDS, attempts to reserve such that\n+  \/\/    we later can use a \"good\" encoding scheme. Reservation is highly CPU-specific.\n+  \/\/ 5) Initialize the narrow Klass encoding scheme by determining encoding base and shift:\n+  \/\/   5a) if CDS=on: Calls initialize_for_given_encoding() with the reservation base from step (4) and the\n+  \/\/       CDS-intrinsic setting for shift; here, we don't have any freedom to deviate from the base.\n+  \/\/   5b) if CDS=off: Calls initialize() - here, we have more freedom and, if we want, can choose an encoding\n+  \/\/       base that differs from the reservation base from step (4). That allows us, e.g., to later use\n+  \/\/       zero-based encoding.\n+  \/\/ 6) ... from now on, we can use base() and shift().\n+\n+  \/\/ Called right after argument parsing; defines narrow klass pointer geometry limits\n+  static void pre_initialize();\n+\n+  static bool tiny_classpointer_mode()   { check_init(_tiny_cp); return (_tiny_cp == 1); }\n+\n+  \/\/ The number of bits a narrow Klass pointer has;\n+  static int narrow_klass_pointer_bits() { check_init(_narrow_klass_pointer_bits); return _narrow_klass_pointer_bits; }\n+\n+  \/\/ The maximum possible shift; the actual shift employed later can be smaller (see initialize())\n+  static int max_shift()                 { check_init(_max_shift); return _max_shift; }\n+\n+  \/\/ Returns the maximum encoding range that can be covered with the currently\n+  \/\/ choosen nKlassID geometry (nKlass bit size, max shift)\n+  static size_t max_encoding_range_size();\n+\n@@ -94,3 +159,10 @@\n-  static address  base()               { return  _base; }\n-  static size_t   range()              { return  _range; }\n-  static int      shift()              { return  _shift; }\n+  \/\/ Can only be used after initialization\n+  static address  base()             { check_init(_base); return  _base; }\n+  static size_t   range()            { check_init(_range); return  _range; }\n+  static int      shift()            { check_init(_shift); return  _shift; }\n+\n+  \/\/ Returns the alignment a Klass* is guaranteed to have.\n+  \/\/ Note: *Not* the same as 1 << shift ! Klass are always guaranteed to be at least 64-bit aligned,\n+  \/\/ so this will return 8 even if shift is 0.\n+  static int klass_alignment_in_bytes() { return nth_bit(MAX2(3, _shift)); }\n+  static int klass_alignment_in_words() { return klass_alignment_in_bytes() \/ BytesPerWord; }\n@@ -108,0 +180,1 @@\n+  static inline narrowKlass encode_not_null_without_asserts(Klass* k, address narrow_base, int shift);\n@@ -110,0 +183,10 @@\n+\n+#ifdef ASSERT\n+  \/\/ Given a Klass* k and an encoding (base, shift), check that k can be encoded\n+  inline static void check_valid_klass(const Klass* k, address base, int shift);\n+  \/\/ Given a Klass* k, check that k can be encoded with the current encoding\n+  inline static void check_valid_klass(const Klass* k);\n+  \/\/ Given a narrow Klass ID, check that it is valid according to current encoding\n+  inline static void check_valid_narrow_klass_id(narrowKlass nk);\n+#endif\n+\n","filename":"src\/hotspot\/share\/oops\/compressedKlass.hpp","additions":95,"deletions":12,"binary":false,"changes":107,"status":"modified"},{"patch":"@@ -35,6 +35,2 @@\n-static inline bool check_alignment(Klass* v) {\n-  return (intptr_t)v % KlassAlignmentInBytes == 0;\n-}\n-\n-inline Klass* CompressedKlassPointers::decode_not_null_without_asserts(narrowKlass v, address narrow_base, int shift) {\n-  return (Klass*)((uintptr_t)narrow_base +((uintptr_t)v << shift));\n+inline Klass* CompressedKlassPointers::decode_not_null_without_asserts(narrowKlass v, address narrow_base_base, int shift) {\n+  return (Klass*)((uintptr_t)narrow_base_base +((uintptr_t)v << shift));\n@@ -46,1 +42,1 @@\n-  assert(check_alignment(result), \"address not aligned: \" PTR_FORMAT, p2i(result));\n+  DEBUG_ONLY(check_valid_klass(result, narrow_base, shift));\n@@ -50,0 +46,4 @@\n+inline narrowKlass CompressedKlassPointers::encode_not_null_without_asserts(Klass* k, address narrow_base, int shift) {\n+  return (narrowKlass)(pointer_delta(k, narrow_base, 1) >> shift);\n+}\n+\n@@ -52,5 +52,2 @@\n-  assert(check_alignment(v), \"Address not aligned\");\n-  uint64_t pd = (uint64_t)(pointer_delta(v, narrow_base, 1));\n-  assert(KlassEncodingMetaspaceMax > pd, \"change encoding max if new encoding\");\n-  uint64_t result = pd >> shift;\n-  assert((result & CONST64(0xffffffff00000000)) == 0, \"narrow klass pointer overflow\");\n+  DEBUG_ONLY(check_valid_klass(v);)\n+  narrowKlass result = encode_not_null_without_asserts(v, narrow_base, shift);\n@@ -58,1 +55,1 @@\n-  return (narrowKlass)result;\n+  return result;\n@@ -70,0 +67,1 @@\n+  DEBUG_ONLY(check_valid_narrow_klass_id(v);)\n@@ -78,1 +76,3 @@\n-  return encode_not_null(v, base(), shift());\n+  narrowKlass nk = encode_not_null(v, base(), shift());\n+  DEBUG_ONLY(check_valid_narrow_klass_id(nk);)\n+  return nk;\n@@ -85,0 +85,31 @@\n+#ifdef ASSERT\n+inline void CompressedKlassPointers::check_valid_klass(const Klass* k, address base, int shift) {\n+  const int log_alignment = MAX2(3, shift); \/\/ always at least 64-bit aligned\n+  assert(is_aligned(k, nth_bit(log_alignment)), \"Klass (\" PTR_FORMAT \") not properly aligned to %zu\",\n+         p2i(k), nth_bit(shift));\n+  const address encoding_end = base + nth_bit(narrow_klass_pointer_bits() + shift);\n+  assert((address)k >= base && (address)k < encoding_end,\n+         \"Klass (\" PTR_FORMAT \") falls outside of the valid encoding range [\" PTR_FORMAT \"-\" PTR_FORMAT \")\",\n+         p2i(k), p2i(base), p2i(encoding_end));\n+}\n+\n+inline void CompressedKlassPointers::check_valid_klass(const Klass* k) {\n+  assert(UseCompressedClassPointers, \"Only call for +UseCCP\");\n+  check_valid_klass(k, base(), shift());\n+  \/\/ Also assert that k falls into what we know is the valid Klass range. This is usually smaller\n+  \/\/ than the encoding range (e.g. encoding range covers 4G, but we only have 1G class space and a\n+  \/\/ tiny bit of CDS => 1.1G)\n+  const address klassrange_end = base() + range();\n+  assert((address)k < klassrange_end,\n+      \"Klass (\" PTR_FORMAT \") falls outside of the valid klass range [\" PTR_FORMAT \"-\" PTR_FORMAT \")\",\n+      p2i(k), p2i(base()), p2i(klassrange_end));\n+}\n+inline void CompressedKlassPointers::check_valid_narrow_klass_id(narrowKlass nk) {\n+  assert(UseCompressedClassPointers, \"Only call for +UseCCP\");\n+  const uint64_t nk_mask = ~right_n_bits(narrow_klass_pointer_bits());\n+  assert(((uint64_t)nk & nk_mask) == 0, \"narrow klass id bit spillover (%u)\", nk);\n+  assert(nk >= _lowest_valid_narrow_klass_id &&\n+         nk <= _highest_valid_narrow_klass_id, \"narrowKlass ID out of range (%u)\", nk);\n+}\n+#endif \/\/ ASSERT\n+\n","filename":"src\/hotspot\/share\/oops\/compressedKlass.inline.hpp","additions":45,"deletions":14,"binary":false,"changes":59,"status":"modified"},{"patch":"@@ -36,3 +36,0 @@\n-  \/\/ aligned header size.\n-  static int header_size() { return sizeof(instanceOopDesc)\/HeapWordSize; }\n-\n@@ -41,4 +38,7 @@\n-    return (UseCompressedClassPointers) ?\n-            klass_gap_offset_in_bytes() :\n-            sizeof(instanceOopDesc);\n-\n+    if (UseCompactObjectHeaders) {\n+      return oopDesc::base_offset_in_bytes();\n+    } else if (UseCompressedClassPointers) {\n+      return klass_gap_offset_in_bytes();\n+    } else {\n+      return sizeof(instanceOopDesc);\n+    }\n","filename":"src\/hotspot\/share\/oops\/instanceOop.hpp","additions":7,"deletions":7,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -254,0 +254,10 @@\n+static markWord make_prototype(Klass* kls) {\n+  markWord prototype = markWord::prototype();\n+#ifdef _LP64\n+  if (UseCompactObjectHeaders) {\n+    prototype = prototype.set_klass(kls);\n+  }\n+#endif\n+  return prototype;\n+}\n+\n@@ -263,0 +273,1 @@\n+                           _prototype_header(make_prototype(this)),\n@@ -973,0 +984,4 @@\n+     if (UseCompactObjectHeaders) {\n+       st->print(BULLET\"prototype_header: \" INTPTR_FORMAT, _prototype_header.value());\n+       st->cr();\n+     }\n","filename":"src\/hotspot\/share\/oops\/klass.cpp","additions":15,"deletions":0,"binary":false,"changes":15,"status":"modified"},{"patch":"@@ -169,0 +169,2 @@\n+  markWord _prototype_header;   \/\/ Used to initialize objects' header\n+\n@@ -707,0 +709,7 @@\n+  markWord prototype_header() const {\n+    assert(UseCompactObjectHeaders, \"only use with compact object headers\");\n+    return _prototype_header;\n+  }\n+  inline void set_prototype_header(markWord header);\n+  static ByteSize prototype_header_offset() { return in_ByteSize(offset_of(Klass, _prototype_header)); }\n+\n","filename":"src\/hotspot\/share\/oops\/klass.hpp","additions":9,"deletions":0,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -55,0 +55,5 @@\n+inline void Klass::set_prototype_header(markWord header) {\n+  assert(UseCompactObjectHeaders, \"only with compact headers\");\n+  _prototype_header = header;\n+}\n+\n","filename":"src\/hotspot\/share\/oops\/klass.inline.hpp","additions":5,"deletions":0,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -32,0 +32,8 @@\n+#ifdef _LP64\n+STATIC_ASSERT((markWord::klass_shadow_mask_inplace & markWord::klass_mask_in_place) == 0);\n+STATIC_ASSERT((markWord::klass_load_shift + markWord::klass_shadow_bits) == markWord::klass_shift);\n+STATIC_ASSERT(markWord::klass_shift + markWord::klass_bits == 64);\n+\/\/ The hash (preceding nKlass) shall be a direct neighbor but not interleave\n+STATIC_ASSERT(markWord::klass_shift == markWord::hash_bits + markWord::hash_shift);\n+#endif\n+\n","filename":"src\/hotspot\/share\/oops\/markWord.cpp","additions":8,"deletions":0,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -29,0 +29,1 @@\n+#include \"oops\/compressedKlass.hpp\"\n@@ -40,1 +41,1 @@\n-\/\/             hash:25 ------------>| age:4  unused_gap:1  lock:2 (normal object)\n+\/\/             hash:25 ------------>| age:4  self-fwd:1  lock:2 (normal object)\n@@ -44,1 +45,5 @@\n-\/\/  unused:25 hash:31 -->| unused_gap:1  age:4  unused_gap:1  lock:2 (normal object)\n+\/\/  unused:26 hash:31 -->| unused_gap:4  age:4  self-fwd:1  lock:2 (normal object)\n+\/\/\n+\/\/  64 bits (with compact headers):\n+\/\/  -------------------------------\n+\/\/  nklass:22 hash:31 -->| unused_gap:4  age:4  self-fwd:1  lock:2 (normal object)\n@@ -107,2 +112,2 @@\n-  static const int first_unused_gap_bits          = 1;\n-  static const int max_hash_bits                  = BitsPerWord - age_bits - lock_bits - first_unused_gap_bits;\n+  static const int self_fwd_bits                  = 1;\n+  static const int max_hash_bits                  = BitsPerWord - age_bits - lock_bits - self_fwd_bits;\n@@ -110,1 +115,1 @@\n-  static const int second_unused_gap_bits         = LP64_ONLY(1) NOT_LP64(0);\n+  static const int unused_gap_bits                = 4; \/\/ Reserved for Valhalla.\n@@ -113,2 +118,3 @@\n-  static const int age_shift                      = lock_bits + first_unused_gap_bits;\n-  static const int hash_shift                     = age_shift + age_bits + second_unused_gap_bits;\n+  static const int self_fwd_shift                 = lock_shift + lock_bits;\n+  static const int age_shift                      = self_fwd_shift + self_fwd_bits;\n+  static const int hash_shift                     = age_shift + age_bits + unused_gap_bits;\n@@ -118,0 +124,2 @@\n+  static const uintptr_t self_fwd_mask            = right_n_bits(self_fwd_bits);\n+  static const uintptr_t self_fwd_mask_in_place   = self_fwd_mask << self_fwd_shift;\n@@ -123,0 +131,21 @@\n+#ifdef _LP64\n+  \/\/ Used only with compact headers:\n+  \/\/ We store nKlass in the bits 38 to 60 (leaving 4 bits for later usage). When extracting,\n+  \/\/ we need to read the upper 32 bits and rightshift by the lower 6 foreign bits.\n+\n+  \/\/ These are for loading the nKlass with a 32-bit load and subsequent masking of the lower\n+  \/\/ shadow bits\n+  static constexpr int klass_load_shift           = 32;\n+  static constexpr int klass_load_bits            = 32;\n+  static constexpr int klass_shadow_bits          = 10;\n+  static constexpr uintptr_t klass_shadow_mask    = right_n_bits(klass_shadow_bits);\n+  static constexpr uintptr_t klass_shadow_mask_inplace  = klass_shadow_mask << klass_load_shift;\n+\n+  \/\/ These are for bit-precise extraction of the nKlass from the 64-bit Markword\n+  static constexpr int klass_shift                = hash_shift + hash_bits;\n+  static constexpr int klass_bits                 = 22;\n+  static constexpr uintptr_t klass_mask           = right_n_bits(klass_bits);\n+  static constexpr uintptr_t klass_mask_in_place  = klass_mask << klass_shift;\n+#endif\n+\n+\n@@ -147,2 +176,3 @@\n-  bool is_forwarded()   const {\n-    return (mask_bits(value(), lock_mask_in_place) == marked_value);\n+  bool is_forwarded() const {\n+    \/\/ Returns true for normal forwarded (0b011) and self-forwarded (0b1xx).\n+    return mask_bits(value(), lock_mask_in_place | self_fwd_mask_in_place) >= static_cast<intptr_t>(marked_value);\n@@ -150,0 +180,1 @@\n+\n@@ -263,0 +294,7 @@\n+  inline Klass* klass() const;\n+  inline Klass* klass_or_null() const;\n+  inline Klass* klass_without_asserts() const;\n+  inline narrowKlass narrow_klass() const;\n+  inline markWord set_narrow_klass(narrowKlass nklass) const;\n+  inline markWord set_klass(Klass* klass) const;\n+\n@@ -277,0 +315,15 @@\n+  inline bool is_self_forwarded() const {\n+    NOT_LP64(assert(LockingMode != LM_LEGACY, \"incorrect with LM_LEGACY on 32 bit\");)\n+    return mask_bits(value(), self_fwd_mask_in_place) != 0;\n+  }\n+\n+  inline markWord set_self_forwarded() const {\n+    NOT_LP64(assert(LockingMode != LM_LEGACY, \"incorrect with LM_LEGACY on 32 bit\");)\n+    return markWord(value() | self_fwd_mask_in_place);\n+  }\n+\n+  inline markWord unset_self_forwarded() const {\n+    NOT_LP64(assert(LockingMode != LM_LEGACY, \"incorrect with LM_LEGACY on 32 bit\");)\n+    return markWord(value() & ~self_fwd_mask_in_place);\n+  }\n+\n","filename":"src\/hotspot\/share\/oops\/markWord.hpp","additions":62,"deletions":9,"binary":false,"changes":71,"status":"modified"},{"patch":"@@ -0,0 +1,93 @@\n+\/*\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef SHARE_OOPS_MARKWORD_INLINE_HPP\n+#define SHARE_OOPS_MARKWORD_INLINE_HPP\n+\n+#include \"oops\/markWord.hpp\"\n+#include \"oops\/compressedOops.inline.hpp\"\n+\n+narrowKlass markWord::narrow_klass() const {\n+#ifdef _LP64\n+  assert(UseCompactObjectHeaders, \"only used with compact object headers\");\n+  return narrowKlass(value() >> klass_shift);\n+#else\n+  ShouldNotReachHere();\n+  return 0;\n+#endif\n+}\n+\n+markWord markWord::set_narrow_klass(narrowKlass nklass) const {\n+#ifdef _LP64\n+  assert(UseCompactObjectHeaders, \"only used with compact object headers\");\n+  return markWord((value() & ~klass_mask_in_place) | ((uintptr_t) nklass << klass_shift));\n+#else\n+  ShouldNotReachHere();\n+  return markWord(0);\n+#endif\n+}\n+\n+Klass* markWord::klass() const {\n+#ifdef _LP64\n+  assert(UseCompactObjectHeaders, \"only used with compact object headers\");\n+  return CompressedKlassPointers::decode_not_null(narrow_klass());\n+#else\n+  ShouldNotReachHere();\n+  return nullptr;\n+#endif\n+}\n+\n+Klass* markWord::klass_or_null() const {\n+#ifdef _LP64\n+  assert(UseCompactObjectHeaders, \"only used with compact object headers\");\n+  return CompressedKlassPointers::decode(narrow_klass());\n+#else\n+  ShouldNotReachHere();\n+  return nullptr;\n+#endif\n+}\n+\n+Klass* markWord::klass_without_asserts() const {\n+#ifdef _LP64\n+  assert(UseCompactObjectHeaders, \"only used with compact object headers\");\n+  return CompressedKlassPointers::decode_without_asserts(narrow_klass());\n+#else\n+  ShouldNotReachHere();\n+  return nullptr;\n+#endif\n+}\n+\n+markWord markWord::set_klass(Klass* klass) const {\n+#ifdef _LP64\n+  assert(UseCompactObjectHeaders, \"only used with compact object headers\");\n+  assert(UseCompressedClassPointers, \"expect compressed klass pointers\");\n+  narrowKlass nklass = CompressedKlassPointers::encode(const_cast<Klass*>(klass));\n+  return set_narrow_klass(nklass);\n+#else\n+  ShouldNotReachHere();\n+  return markWord();\n+#endif\n+}\n+\n+#endif \/\/ SHARE_OOPS_MARKWORD_INLINE_HPP\n","filename":"src\/hotspot\/share\/oops\/markWord.inline.hpp","additions":93,"deletions":0,"binary":false,"changes":93,"status":"added"},{"patch":"@@ -146,1 +146,2 @@\n-  assert(obj->is_objArray(), \"must be object array\");\n+  \/\/ In this assert, we cannot safely access the Klass* with compact headers.\n+  assert(UseCompactObjectHeaders || obj->is_objArray(), \"must be object array\");\n","filename":"src\/hotspot\/share\/oops\/objArrayKlass.cpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -73,1 +73,2 @@\n-  assert (obj->is_array(), \"obj must be array\");\n+  \/\/ In this assert, we cannot safely access the Klass* with compact headers.\n+  assert (UseCompactObjectHeaders || obj->is_array(), \"obj must be array\");\n","filename":"src\/hotspot\/share\/oops\/objArrayKlass.inline.hpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -157,1 +157,2 @@\n-  return UseCompressedClassPointers;\n+  \/\/ Except when using compact headers.\n+  return UseCompressedClassPointers && !UseCompactObjectHeaders;\n@@ -223,1 +224,1 @@\n-bool oopDesc::size_might_change() {\n+bool oopDesc::size_might_change(Klass* klass) {\n@@ -229,1 +230,1 @@\n-  return Universe::heap()->is_stw_gc_active() && is_objArray() && is_forwarded() && (UseParallelGC || UseG1GC);\n+  return Universe::heap()->is_stw_gc_active() && klass->is_objArray_klass() && is_forwarded() && (UseParallelGC || UseG1GC);\n","filename":"src\/hotspot\/share\/oops\/oop.cpp","additions":4,"deletions":3,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -65,0 +65,2 @@\n+  inline oop cas_set_forwardee(markWord new_mark, markWord old_mark, atomic_memory_order order);\n+\n@@ -81,0 +83,3 @@\n+  \/\/ Returns the prototype mark that should be used for this object.\n+  inline markWord prototype_mark() const;\n+\n@@ -99,1 +104,7 @@\n-  static constexpr int header_size() { return sizeof(oopDesc)\/HeapWordSize; }\n+  static int header_size() {\n+    if (UseCompactObjectHeaders) {\n+      return sizeof(markWord) \/ HeapWordSize;\n+    } else {\n+      return sizeof(oopDesc)\/HeapWordSize;\n+    }\n+  }\n@@ -111,0 +122,14 @@\n+  \/\/ The following set of methods is used to access the mark-word and related\n+  \/\/ properties when the object may be forwarded. Be careful where and when\n+  \/\/ using this method. It assumes that the forwardee is installed in\n+  \/\/ the header as a plain pointer (or self-forwarded). In particular,\n+  \/\/ those methods can not deal with the encoded forwarding that is used\n+  \/\/ in Serial, Parallel, G1 and Shenandoah full-GCs.\n+private:\n+  inline Klass*   forward_safe_klass_impl(markWord m) const;\n+public:\n+  inline Klass*   forward_safe_klass() const;\n+  inline Klass*   forward_safe_klass(markWord m) const;\n+  inline size_t   forward_safe_size();\n+  inline void     forward_safe_init_mark();\n+\n@@ -261,0 +286,1 @@\n+  inline bool is_self_forwarded() const;\n@@ -263,0 +289,1 @@\n+  inline void forward_to_self();\n@@ -269,0 +296,1 @@\n+  inline oop forward_to_self_atomic(markWord compare, atomic_memory_order order = memory_order_conservative);\n@@ -271,0 +299,3 @@\n+  inline oop forwardee(markWord header) const;\n+\n+  inline void unset_self_forwarded();\n@@ -314,1 +345,18 @@\n-  static int klass_offset_in_bytes()     { return (int)offset_of(oopDesc, _metadata._klass); }\n+  static int klass_offset_in_bytes()     {\n+#ifdef _LP64\n+    if (UseCompactObjectHeaders) {\n+      \/\/ NOTE: The only place where this is used with compact headers is\n+      \/\/ the C2 compiler, and even there we don't use it to access the (narrow)Klass*\n+      \/\/ directly. It is used only as a placeholder to identify the special memory slice\n+      \/\/ of LoadNKlass instructions. This value could be any value that is not a valid\n+      \/\/ field offset. Also, if it weren't for C2, we could\n+      \/\/ assert(!UseCompactObjectHeaders) here.\n+      constexpr int load_shift = markWord::klass_load_shift;\n+      STATIC_ASSERT(load_shift % 8 == 0);\n+      return mark_offset_in_bytes() + load_shift \/ 8;\n+    } else\n+#endif\n+    {\n+      return (int)offset_of(oopDesc, _metadata._klass);\n+    }\n+  }\n@@ -317,0 +365,1 @@\n+    assert(!UseCompactObjectHeaders, \"don't use klass_gap_offset_in_bytes() with compact headers\");\n@@ -320,0 +369,16 @@\n+  static int base_offset_in_bytes() {\n+#ifdef _LP64\n+    if (UseCompactObjectHeaders) {\n+      \/\/ With compact headers, the Klass* field is not used for the Klass*\n+      \/\/ and is used for the object fields instead.\n+      STATIC_ASSERT(sizeof(markWord) == 8);\n+      return sizeof(markWord);\n+    } else if (UseCompressedClassPointers) {\n+      return sizeof(markWord) + sizeof(narrowKlass);\n+    } else\n+#endif\n+    {\n+      return sizeof(oopDesc);\n+    }\n+  }\n+\n@@ -323,1 +388,1 @@\n-  DEBUG_ONLY(bool size_might_change();)\n+  DEBUG_ONLY(bool size_might_change(Klass* klass);)\n","filename":"src\/hotspot\/share\/oops\/oop.hpp","additions":68,"deletions":3,"binary":false,"changes":71,"status":"modified"},{"patch":"@@ -37,1 +37,1 @@\n-#include \"oops\/markWord.hpp\"\n+#include \"oops\/markWord.inline.hpp\"\n@@ -85,0 +85,8 @@\n+markWord oopDesc::prototype_mark() const {\n+  if (UseCompactObjectHeaders) {\n+    return klass()->prototype_header();\n+  } else {\n+    return markWord::prototype();\n+  }\n+}\n+\n@@ -86,1 +94,5 @@\n-  set_mark(markWord::prototype());\n+  if (UseCompactObjectHeaders) {\n+    set_mark(prototype_mark());\n+  } else {\n+    set_mark(markWord::prototype());\n+  }\n@@ -90,2 +102,4 @@\n-  if (UseCompressedClassPointers) {\n-    return CompressedKlassPointers::decode_not_null(_metadata._compressed_klass);\n+  if (UseCompactObjectHeaders) {\n+    return mark().klass();\n+  } else if (UseCompressedClassPointers) {\n+     return CompressedKlassPointers::decode_not_null(_metadata._compressed_klass);\n@@ -98,1 +112,3 @@\n-  if (UseCompressedClassPointers) {\n+  if (UseCompactObjectHeaders) {\n+    return mark().klass_or_null();\n+  } else if (UseCompressedClassPointers) {\n@@ -106,1 +122,3 @@\n-  if (UseCompressedClassPointers) {\n+  if (UseCompactObjectHeaders) {\n+    return mark_acquire().klass();\n+  } else if (UseCompressedClassPointers) {\n@@ -115,1 +133,3 @@\n-  if (UseCompressedClassPointers) {\n+  if (UseCompactObjectHeaders) {\n+    return mark().klass_without_asserts();\n+  } else if (UseCompressedClassPointers) {\n@@ -124,0 +144,1 @@\n+  assert(!UseCompactObjectHeaders, \"don't set Klass* with compact headers\");\n@@ -133,0 +154,1 @@\n+  assert(!UseCompactObjectHeaders, \"don't set Klass* with compact headers\");\n@@ -143,0 +165,1 @@\n+  assert(!UseCompactObjectHeaders, \"don't set Klass* gap with compact headers\");\n@@ -193,1 +216,1 @@\n-      assert(s == klass->oop_size(this) || size_might_change(), \"wrong array object size\");\n+      assert(s == klass->oop_size(this) || size_might_change(klass), \"wrong array object size\");\n@@ -205,0 +228,47 @@\n+#ifdef _LP64\n+Klass* oopDesc::forward_safe_klass_impl(markWord m) const {\n+  assert(UseCompactObjectHeaders, \"Only get here with compact headers\");\n+  if (m.is_marked()) {\n+    oop fwd = forwardee(m);\n+    markWord m2 = fwd->mark();\n+    assert(!m2.is_marked() || m2.is_self_forwarded(), \"no double forwarding: this: \" PTR_FORMAT \" (\" INTPTR_FORMAT \"), fwd: \" PTR_FORMAT \" (\" INTPTR_FORMAT \")\", p2i(this), m.value(), p2i(fwd), m2.value());\n+    m = m2;\n+  }\n+  return m.klass();\n+}\n+#endif\n+\n+Klass* oopDesc::forward_safe_klass(markWord m) const {\n+#ifdef _LP64\n+  if (UseCompactObjectHeaders) {\n+    return forward_safe_klass_impl(m);\n+  } else\n+#endif\n+  {\n+    return klass();\n+  }\n+}\n+\n+Klass* oopDesc::forward_safe_klass() const {\n+#ifdef _LP64\n+  if (UseCompactObjectHeaders) {\n+    return forward_safe_klass_impl(mark());\n+  } else\n+#endif\n+  {\n+    return klass();\n+  }\n+}\n+\n+size_t oopDesc::forward_safe_size() {\n+  return size_given_klass(forward_safe_klass());\n+}\n+\n+void oopDesc::forward_safe_init_mark() {\n+  if (UseCompactObjectHeaders) {\n+    set_mark(forward_safe_klass()->prototype_header());\n+  } else {\n+    set_mark(markWord::prototype());\n+  }\n+}\n+\n@@ -270,0 +340,4 @@\n+bool oopDesc::is_self_forwarded() const {\n+  return mark().is_self_forwarded();\n+}\n+\n@@ -277,4 +351,6 @@\n-oop oopDesc::forward_to_atomic(oop p, markWord compare, atomic_memory_order order) {\n-  markWord m = markWord::encode_pointer_as_mark(p);\n-  assert(m.decode_pointer() == p, \"encoding must be reversible\");\n-  markWord old_mark = cas_set_mark(m, compare, order);\n+void oopDesc::forward_to_self() {\n+  set_mark(mark().set_self_forwarded());\n+}\n+\n+oop oopDesc::cas_set_forwardee(markWord new_mark, markWord compare, atomic_memory_order order) {\n+  markWord old_mark = cas_set_mark(new_mark, compare, order);\n@@ -284,1 +360,23 @@\n-    return cast_to_oop(old_mark.decode_pointer());\n+    assert(old_mark.is_forwarded(), \"must be forwarded here\");\n+    return forwardee(old_mark);\n+  }\n+}\n+\n+oop oopDesc::forward_to_atomic(oop p, markWord compare, atomic_memory_order order) {\n+  markWord m = markWord::encode_pointer_as_mark(p);\n+  assert(forwardee(m) == p, \"encoding must be reversible\");\n+  return cas_set_forwardee(m, compare, order);\n+}\n+\n+oop oopDesc::forward_to_self_atomic(markWord old_mark, atomic_memory_order order) {\n+  markWord new_mark = old_mark.set_self_forwarded();\n+  assert(forwardee(new_mark) == cast_to_oop(this), \"encoding must be reversible\");\n+  return cas_set_forwardee(new_mark, old_mark, order);\n+}\n+\n+oop oopDesc::forwardee(markWord mark) const {\n+  assert(mark.is_forwarded(), \"only decode when actually forwarded\");\n+  if (mark.is_self_forwarded()) {\n+    return cast_to_oop(this);\n+  } else {\n+    return mark.forwardee();\n@@ -292,1 +390,5 @@\n-  return mark().forwardee();\n+  return forwardee(mark());\n+}\n+\n+void oopDesc::unset_self_forwarded() {\n+  set_mark(mark().unset_self_forwarded());\n@@ -349,1 +451,2 @@\n-  assert(k == klass(), \"wrong klass\");\n+  \/\/ In this assert, we cannot safely access the Klass* with compact headers.\n+  assert(UseCompactObjectHeaders || k == klass(), \"wrong klass\");\n","filename":"src\/hotspot\/share\/oops\/oop.inline.hpp","additions":118,"deletions":15,"binary":false,"changes":133,"status":"modified"},{"patch":"@@ -174,1 +174,2 @@\n-  assert(obj->is_typeArray(),\"must be a type array\");\n+  \/\/ In this assert, we cannot safely access the Klass* with compact headers.\n+  assert(UseCompactObjectHeaders || obj->is_typeArray(),\"must be a type array\");\n","filename":"src\/hotspot\/share\/oops\/typeArrayKlass.cpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -38,1 +38,2 @@\n-  assert(obj->is_typeArray(),\"must be a type array\");\n+  \/\/ In this assert, we cannot safely access the Klass* with compact headers.\n+  assert(UseCompactObjectHeaders || obj->is_typeArray(),\"must be a type array\");\n","filename":"src\/hotspot\/share\/oops\/typeArrayKlass.inline.hpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -1618,2 +1618,8 @@\n-  \/\/ For now only enable fast locking for non-array types\n-  mark_node = phase->MakeConX(markWord::prototype().value());\n+  if (UseCompactObjectHeaders) {\n+    Node* klass_node = in(AllocateNode::KlassNode);\n+    Node* proto_adr = phase->transform(new AddPNode(klass_node, klass_node, phase->MakeConX(in_bytes(Klass::prototype_header_offset()))));\n+    mark_node = LoadNode::make(*phase, control, mem, proto_adr, TypeRawPtr::BOTTOM, TypeX_X, TypeX_X->basic_type(), MemNode::unordered);\n+  } else {\n+    \/\/ For now only enable fast locking for non-array types\n+    mark_node = phase->MakeConX(markWord::prototype().value());\n+  }\n","filename":"src\/hotspot\/share\/opto\/callnode.cpp","additions":8,"deletions":2,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -1698,0 +1698,4 @@\n+      if (UseCompactObjectHeaders) {\n+        if (flat->offset() == in_bytes(Klass::prototype_header_offset()))\n+          alias_type(idx)->set_rewritable(false);\n+      }\n","filename":"src\/hotspot\/share\/opto\/compile.cpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -271,2 +271,2 @@\n-        if ((UseCompressedOops || UseCompressedClassPointers) &&\n-            (CompressedOops::shift() == 0 || CompressedKlassPointers::shift() == 0)) {\n+        if ((UseCompressedOops && CompressedOops::shift() == 0) ||\n+            (UseCompressedClassPointers && CompressedKlassPointers::shift() == 0)) {\n","filename":"src\/hotspot\/share\/opto\/lcm.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -390,1 +390,1 @@\n-  if (t->isa_narrowklass() && CompressedKlassPointers::shift() == 0) {\n+  if (t->isa_narrowklass() && UseCompressedClassPointers && CompressedKlassPointers::shift() == 0) {\n","filename":"src\/hotspot\/share\/opto\/machnode.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -1709,1 +1709,3 @@\n-  rawmem = make_store(control, rawmem, object, oopDesc::klass_offset_in_bytes(), klass_node, T_METADATA);\n+  if (!UseCompactObjectHeaders) {\n+    rawmem = make_store(control, rawmem, object, oopDesc::klass_offset_in_bytes(), klass_node, T_METADATA);\n+  }\n","filename":"src\/hotspot\/share\/opto\/macro.cpp","additions":3,"deletions":1,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -1937,0 +1937,7 @@\n+  if (UseCompactObjectHeaders) {\n+    if (tkls->offset() == in_bytes(Klass::prototype_header_offset())) {\n+      \/\/ The field is Klass::_prototype_header.  Return its (constant) value.\n+      assert(this->Opcode() == Op_LoadX, \"must load a proper type from _prototype_header\");\n+      return TypeX::make(klass->prototype_header());\n+    }\n+  }\n@@ -2109,0 +2116,7 @@\n+      if (UseCompactObjectHeaders) {\n+        if (tkls->offset() == in_bytes(Klass::prototype_header_offset())) {\n+          \/\/ The field is Klass::_prototype_header. Return its (constant) value.\n+          assert(this->Opcode() == Op_LoadX, \"must load a proper type from _prototype_header\");\n+          return TypeX::make(klass->prototype_header());\n+        }\n+      }\n@@ -2199,1 +2213,1 @@\n-  if (alloc != nullptr) {\n+  if (!UseCompactObjectHeaders && alloc != nullptr) {\n","filename":"src\/hotspot\/share\/opto\/memnode.cpp","additions":15,"deletions":1,"binary":false,"changes":16,"status":"modified"},{"patch":"@@ -1417,7 +1417,0 @@\n-void Arguments::set_use_compressed_klass_ptrs() {\n-#ifdef _LP64\n-  assert(!UseCompressedClassPointers || CompressedClassSpaceSize <= KlassEncodingMetaspaceMax,\n-         \"CompressedClassSpaceSize is too large for UseCompressedClassPointers\");\n-#endif \/\/ _LP64\n-}\n-\n@@ -1442,1 +1435,0 @@\n-  set_use_compressed_klass_ptrs();\n@@ -3644,0 +3636,26 @@\n+#ifdef _LP64\n+  if (UseCompactObjectHeaders && FLAG_IS_CMDLINE(UseCompressedClassPointers) && !UseCompressedClassPointers) {\n+    warning(\"Compact object headers require compressed class pointers. Disabling compact object headers.\");\n+    FLAG_SET_DEFAULT(UseCompactObjectHeaders, false);\n+  }\n+  if (UseCompactObjectHeaders && LockingMode != LM_LIGHTWEIGHT) {\n+    FLAG_SET_DEFAULT(LockingMode, LM_LIGHTWEIGHT);\n+  }\n+  if (UseCompactObjectHeaders && !UseObjectMonitorTable) {\n+    \/\/ If UseCompactObjectHeaders is on the command line, turn on UseObjectMonitorTable.\n+    if (FLAG_IS_CMDLINE(UseCompactObjectHeaders)) {\n+      FLAG_SET_DEFAULT(UseObjectMonitorTable, true);\n+\n+    \/\/ If UseObjectMonitorTable is on the command line, turn off UseCompactObjectHeaders.\n+    } else if (FLAG_IS_CMDLINE(UseObjectMonitorTable)) {\n+      FLAG_SET_DEFAULT(UseCompactObjectHeaders, false);\n+    \/\/ If neither on the command line, the defaults are incompatible, but turn on UseObjectMonitorTable.\n+    } else {\n+      FLAG_SET_DEFAULT(UseObjectMonitorTable, true);\n+    }\n+  }\n+  if (UseCompactObjectHeaders && !UseCompressedClassPointers) {\n+    FLAG_SET_DEFAULT(UseCompressedClassPointers, true);\n+  }\n+#endif\n+\n@@ -3657,0 +3675,4 @@\n+  if (UseCompressedClassPointers) {\n+    CompressedKlassPointers::pre_initialize();\n+  }\n+\n","filename":"src\/hotspot\/share\/runtime\/arguments.cpp","additions":30,"deletions":8,"binary":false,"changes":38,"status":"modified"},{"patch":"@@ -264,1 +264,0 @@\n-  static void set_use_compressed_klass_ptrs();\n","filename":"src\/hotspot\/share\/runtime\/arguments.hpp","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -131,0 +131,3 @@\n+  product(bool, UseCompactObjectHeaders, false, EXPERIMENTAL,               \\\n+          \"Use compact 64-bit object headers in 64-bit VM\")                 \\\n+                                                                            \\\n@@ -147,0 +150,1 @@\n+const bool UseCompactObjectHeaders = false;\n","filename":"src\/hotspot\/share\/runtime\/globals.hpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -1997,2 +1997,0 @@\n-  declare_constant(LogKlassAlignmentInBytes)                              \\\n-                                                                          \\\n@@ -2505,0 +2503,1 @@\n+  LP64_ONLY(declare_constant(markWord::klass_shift))                      \\\n","filename":"src\/hotspot\/share\/runtime\/vmStructs.cpp","additions":1,"deletions":2,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -27,0 +27,3 @@\n+import sun.jvm.hotspot.oops.Mark;\n+import sun.jvm.hotspot.runtime.VM;\n+\n@@ -397,1 +400,9 @@\n-    long value = readCInteger(address, getKlassPtrSize(), true);\n+    long value;\n+    if (VM.getVM().isCompactObjectHeadersEnabled()) {\n+      \/\/ With compact headers, the compressed Klass* is currently read from the mark\n+      \/\/ word. We need to load the whole mark, and shift the upper parts.\n+      value = readCInteger(address, machDesc.getAddressSize(), true);\n+      value = value >>> Mark.getKlassShift();\n+    } else {\n+      value = readCInteger(address, getKlassPtrSize(), true);\n+    }\n","filename":"src\/jdk.hotspot.agent\/share\/classes\/sun\/jvm\/hotspot\/debugger\/DebuggerBase.java","additions":12,"deletions":1,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -84,1 +84,3 @@\n-    if (VM.getVM().isCompressedKlassPointersEnabled()) {\n+    if (VM.getVM().isCompactObjectHeadersEnabled()) {\n+      lengthOffsetInBytes = Oop.getHeaderSize();\n+    } else if (VM.getVM().isCompressedKlassPointersEnabled()) {\n","filename":"src\/jdk.hotspot.agent\/share\/classes\/sun\/jvm\/hotspot\/oops\/Array.java","additions":3,"deletions":1,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -58,1 +58,3 @@\n-    if (VM.getVM().isCompressedKlassPointersEnabled()) {\n+    if (VM.getVM().isCompactObjectHeadersEnabled()) {\n+      return Oop.getHeaderSize();\n+    } else if (VM.getVM().isCompressedKlassPointersEnabled()) {\n","filename":"src\/jdk.hotspot.agent\/share\/classes\/sun\/jvm\/hotspot\/oops\/Instance.java","additions":3,"deletions":1,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -57,0 +57,3 @@\n+    if (VM.getVM().isLP64()) {\n+      klassShift          = db.lookupLongConstant(\"markWord::klass_shift\").longValue();\n+    }\n@@ -63,0 +66,1 @@\n+    hashMaskCompactInPlace = db.lookupLongConstant(\"markWord::hash_mask_compact_in_place\").longValue();\n@@ -85,0 +89,1 @@\n+  private static long klassShift;\n@@ -92,0 +97,1 @@\n+  private static long hashMaskCompactInPlace;\n@@ -105,0 +111,4 @@\n+  public static long getKlassShift() {\n+    return klassShift;\n+  }\n+\n@@ -194,0 +204,5 @@\n+  public Klass getKlass() {\n+    assert(VM.getVM().isCompactObjectHeadersEnabled());\n+    return (Klass)Metadata.instantiateWrapperFor(addr.getCompKlassAddressAt(0));\n+  }\n+\n","filename":"src\/jdk.hotspot.agent\/share\/classes\/sun\/jvm\/hotspot\/oops\/Mark.java","additions":15,"deletions":0,"binary":false,"changes":15,"status":"modified"},{"patch":"@@ -49,3 +49,8 @@\n-    klass      = new MetadataField(type.getAddressField(\"_metadata._klass\"), 0);\n-    compressedKlass  = new NarrowKlassField(type.getAddressField(\"_metadata._compressed_klass\"), 0);\n-    headerSize = type.getSize();\n+    if (VM.getVM().isCompactObjectHeadersEnabled()) {\n+      Type markType = db.lookupType(\"markWord\");\n+      headerSize = markType.getSize();\n+    } else {\n+      headerSize = type.getSize();\n+      klass      = new MetadataField(type.getAddressField(\"_metadata._klass\"), 0);\n+      compressedKlass  = new NarrowKlassField(type.getAddressField(\"_metadata._compressed_klass\"), 0);\n+    }\n@@ -78,0 +83,6 @@\n+\n+  private static Klass getKlass(Mark mark) {\n+    assert(VM.getVM().isCompactObjectHeadersEnabled());\n+    return mark.getKlass();\n+  }\n+\n@@ -79,1 +90,4 @@\n-    if (VM.getVM().isCompressedKlassPointersEnabled()) {\n+    if (VM.getVM().isCompactObjectHeadersEnabled()) {\n+      assert(VM.getVM().isCompressedKlassPointersEnabled());\n+      return getKlass(getMark());\n+    } else if (VM.getVM().isCompressedKlassPointersEnabled()) {\n@@ -150,4 +164,6 @@\n-      if (VM.getVM().isCompressedKlassPointersEnabled()) {\n-        visitor.doMetadata(compressedKlass, true);\n-      } else {\n-        visitor.doMetadata(klass, true);\n+      if (!VM.getVM().isCompactObjectHeadersEnabled()) {\n+        if (VM.getVM().isCompressedKlassPointersEnabled()) {\n+          visitor.doMetadata(compressedKlass, true);\n+        } else {\n+          visitor.doMetadata(klass, true);\n+        }\n@@ -209,1 +225,4 @@\n-    if (VM.getVM().isCompressedKlassPointersEnabled()) {\n+    if (VM.getVM().isCompactObjectHeadersEnabled()) {\n+      Mark mark = new Mark(handle);\n+      return getKlass(mark);\n+    } else if (VM.getVM().isCompressedKlassPointersEnabled()) {\n","filename":"src\/jdk.hotspot.agent\/share\/classes\/sun\/jvm\/hotspot\/oops\/Oop.java","additions":28,"deletions":9,"binary":false,"changes":37,"status":"modified"},{"patch":"@@ -151,0 +151,1 @@\n+  private Boolean compactObjectHeadersEnabled;\n@@ -963,0 +964,9 @@\n+  public boolean isCompactObjectHeadersEnabled() {\n+    if (compactObjectHeadersEnabled == null) {\n+        Flag flag = getCommandLineFlag(\"UseCompactObjectHeaders\");\n+        compactObjectHeadersEnabled = (flag == null) ? Boolean.FALSE:\n+             (flag.getBool()? Boolean.TRUE: Boolean.FALSE);\n+    }\n+    return compactObjectHeadersEnabled.booleanValue();\n+  }\n+\n","filename":"src\/jdk.hotspot.agent\/share\/classes\/sun\/jvm\/hotspot\/runtime\/VM.java","additions":10,"deletions":0,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -29,0 +29,1 @@\n+import sun.jvm.hotspot.oops.Oop;\n@@ -40,20 +41,0 @@\n-  private static AddressField klassField;\n-\n-  static {\n-    VM.registerVMInitializedObserver(new Observer() {\n-        public void update(Observable o, Object data) {\n-          initialize(VM.getVM().getTypeDataBase());\n-        }\n-      });\n-  }\n-\n-  private static void initialize(TypeDataBase db) {\n-    Type type = db.lookupType(\"oopDesc\");\n-\n-    if (VM.getVM().isCompressedKlassPointersEnabled()) {\n-      klassField = type.getAddressField(\"_metadata._compressed_klass\");\n-    } else {\n-      klassField = type.getAddressField(\"_metadata._klass\");\n-    }\n-  }\n-\n@@ -69,5 +50,1 @@\n-      if (VM.getVM().isCompressedKlassPointersEnabled()) {\n-        Metadata.instantiateWrapperFor(oop.getCompKlassAddressAt(klassField.getOffset()));\n-      } else {\n-        Metadata.instantiateWrapperFor(klassField.getValue(oop));\n-      }\n+      Oop.getKlassForOopHandle(oop);\n","filename":"src\/jdk.hotspot.agent\/share\/classes\/sun\/jvm\/hotspot\/utilities\/RobustOopDeterminator.java","additions":2,"deletions":25,"binary":false,"changes":27,"status":"modified"},{"patch":"@@ -26,0 +26,1 @@\n+#include \"gc\/shared\/gcForwarding.inline.hpp\"\n@@ -29,19 +30,2 @@\n-\/\/ Class to create a \"fake\" oop with a mark that will\n-\/\/ return true for calls to must_be_preserved().\n-class FakeOop {\n-  oopDesc _oop;\n-\n-public:\n-  FakeOop() : _oop() { _oop.set_mark(originalMark()); }\n-\n-  oop get_oop() { return &_oop; }\n-  markWord mark() { return _oop.mark(); }\n-  void set_mark(markWord m) { _oop.set_mark(m); }\n-  void forward_to(oop obj) {\n-    markWord m = markWord::encode_pointer_as_mark(obj);\n-    _oop.set_mark(m);\n-  }\n-\n-  static markWord originalMark() { return markWord(markWord::lock_mask_in_place); }\n-  static markWord changedMark()  { return markWord(0x4711); }\n-};\n+static markWord originalMark() { return markWord(markWord::lock_mask_in_place); }\n+static markWord changedMark()  { return markWord(0x4711); }\n@@ -53,4 +37,9 @@\n-  FakeOop o1;\n-  FakeOop o2;\n-  FakeOop o3;\n-  FakeOop o4;\n+\n+  HeapWord fakeheap[32] = { nullptr };\n+  HeapWord* heap = align_up(fakeheap, 8 * sizeof(HeapWord));\n+  GCForwarding::initialize(MemRegion(&heap[0], &heap[16]));\n+\n+  oop o1 = cast_to_oop(&heap[0]); o1->set_mark(originalMark());\n+  oop o2 = cast_to_oop(&heap[2]); o2->set_mark(originalMark());\n+  oop o3 = cast_to_oop(&heap[4]); o3->set_mark(originalMark());\n+  oop o4 = cast_to_oop(&heap[6]); o4->set_mark(originalMark());\n@@ -59,4 +48,4 @@\n-  ASSERT_MARK_WORD_EQ(o1.mark(), FakeOop::originalMark());\n-  ASSERT_MARK_WORD_EQ(o2.mark(), FakeOop::originalMark());\n-  ASSERT_MARK_WORD_EQ(o3.mark(), FakeOop::originalMark());\n-  ASSERT_MARK_WORD_EQ(o4.mark(), FakeOop::originalMark());\n+  ASSERT_MARK_WORD_EQ(o1->mark(), originalMark());\n+  ASSERT_MARK_WORD_EQ(o2->mark(), originalMark());\n+  ASSERT_MARK_WORD_EQ(o3->mark(), originalMark());\n+  ASSERT_MARK_WORD_EQ(o4->mark(), originalMark());\n@@ -65,4 +54,4 @@\n-  o1.set_mark(FakeOop::changedMark());\n-  o2.set_mark(FakeOop::changedMark());\n-  ASSERT_MARK_WORD_EQ(o1.mark(), FakeOop::changedMark());\n-  ASSERT_MARK_WORD_EQ(o2.mark(), FakeOop::changedMark());\n+  o1->set_mark(changedMark());\n+  o2->set_mark(changedMark());\n+  ASSERT_MARK_WORD_EQ(o1->mark(), changedMark());\n+  ASSERT_MARK_WORD_EQ(o2->mark(), changedMark());\n@@ -71,2 +60,2 @@\n-  pm.push_if_necessary(o1.get_oop(), o1.mark());\n-  pm.push_if_necessary(o2.get_oop(), o2.mark());\n+  pm.push_if_necessary(o1, o1->mark());\n+  pm.push_if_necessary(o2, o2->mark());\n@@ -75,4 +64,4 @@\n-  o1.forward_to(o3.get_oop());\n-  o2.forward_to(o4.get_oop());\n-  ASSERT_EQ(o1.get_oop()->forwardee(), o3.get_oop());\n-  ASSERT_EQ(o2.get_oop()->forwardee(), o4.get_oop());\n+  GCForwarding::forward_to(o1, o3);\n+  GCForwarding::forward_to(o2, o4);\n+  ASSERT_EQ(GCForwarding::forwardee(o1), o3);\n+  ASSERT_EQ(GCForwarding::forwardee(o2), o4);\n@@ -86,2 +75,2 @@\n-  ASSERT_MARK_WORD_EQ(o3.mark(), FakeOop::changedMark());\n-  ASSERT_MARK_WORD_EQ(o4.mark(), FakeOop::changedMark());\n+  ASSERT_MARK_WORD_EQ(o3->mark(), changedMark());\n+  ASSERT_MARK_WORD_EQ(o4->mark(), changedMark());\n","filename":"test\/hotspot\/gtest\/gc\/shared\/test_preservedMarks.cpp","additions":28,"deletions":39,"binary":false,"changes":67,"status":"modified"},{"patch":"@@ -40,0 +40,1 @@\n+  int _num_arenas_created;\n@@ -42,2 +43,5 @@\n-    metaspace::MetaspaceTestContext(\"gtest-metaspace-context\", commit_limit, reserve_limit)\n-  {}\n+    metaspace::MetaspaceTestContext(\"gtest-metaspace-context\", commit_limit, reserve_limit),\n+    _num_arenas_created(0) {}\n+\n+  int num_arenas_created() const { return _num_arenas_created; }\n+  void inc_num_arenas_created() { _num_arenas_created ++; }\n","filename":"test\/hotspot\/gtest\/metaspace\/metaspaceGtestContexts.hpp","additions":6,"deletions":2,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -29,0 +29,1 @@\n+#include \"memory\/metaspace\/metablock.hpp\"\n@@ -35,0 +36,1 @@\n+using metaspace::MetaBlock;\n@@ -46,0 +48,13 @@\n+template <int num_lists>\n+struct TestedBinList : public BinListImpl<num_lists> {\n+  typedef BinListImpl<num_lists> ListType;\n+  void add_block(MetaWord* p, size_t word_size) {\n+    ListType::add_block(MetaBlock(p, word_size));\n+  }\n+  MetaWord* remove_block(size_t requested_size, size_t* real_size) {\n+    MetaBlock result = ListType::remove_block(requested_size);\n+    (*real_size) = result.word_size();\n+    return result.base();\n+  }\n+};\n+\n@@ -209,3 +224,3 @@\n-TEST_VM(metaspace, BinList_basic_1)     { BinListBasicTest< BinListImpl<1> >::basic_test(); }\n-TEST_VM(metaspace, BinList_basic_8)     { BinListBasicTest< BinListImpl<8> >::basic_test(); }\n-TEST_VM(metaspace, BinList_basic_32)    { BinListBasicTest<BinList32>::basic_test(); }\n+TEST_VM(metaspace, BinList_basic_1)     { BinListBasicTest< TestedBinList<1> >::basic_test(); }\n+TEST_VM(metaspace, BinList_basic_8)     { BinListBasicTest< TestedBinList<8> >::basic_test(); }\n+TEST_VM(metaspace, BinList_basic_32)    { BinListBasicTest< TestedBinList<32> >::basic_test(); }\n@@ -213,3 +228,3 @@\n-TEST_VM(metaspace, BinList_basic_2_1)     { BinListBasicTest< BinListImpl<1> >::basic_test_2(); }\n-TEST_VM(metaspace, BinList_basic_2_8)     { BinListBasicTest< BinListImpl<8> >::basic_test_2(); }\n-TEST_VM(metaspace, BinList_basic_2_32)    { BinListBasicTest<BinList32>::basic_test_2(); }\n+TEST_VM(metaspace, BinList_basic_2_1)     { BinListBasicTest< TestedBinList<1> >::basic_test_2(); }\n+TEST_VM(metaspace, BinList_basic_2_8)     { BinListBasicTest< TestedBinList<8> >::basic_test_2(); }\n+TEST_VM(metaspace, BinList_basic_2_32)    { BinListBasicTest< TestedBinList<32> >::basic_test_2(); }\n@@ -217,3 +232,3 @@\n-TEST_VM(metaspace, BinList_basic_rand_1)     { BinListBasicTest< BinListImpl<1> >::random_test(); }\n-TEST_VM(metaspace, BinList_basic_rand_8)     { BinListBasicTest< BinListImpl<8> >::random_test(); }\n-TEST_VM(metaspace, BinList_basic_rand_32)    { BinListBasicTest<BinList32>::random_test(); }\n+TEST_VM(metaspace, BinList_basic_rand_1)     { BinListBasicTest< TestedBinList<1> >::random_test(); }\n+TEST_VM(metaspace, BinList_basic_rand_8)     { BinListBasicTest< TestedBinList<8> >::random_test(); }\n+TEST_VM(metaspace, BinList_basic_rand_32)    { BinListBasicTest< TestedBinList<32> >::random_test(); }\n","filename":"test\/hotspot\/gtest\/metaspace\/test_binlist.cpp","additions":24,"deletions":9,"binary":false,"changes":33,"status":"modified"},{"patch":"@@ -29,0 +29,1 @@\n+#include \"memory\/metaspace\/metablock.hpp\"\n@@ -35,0 +36,12 @@\n+using metaspace::MetaBlock;\n+\n+struct TestedBlockTree : public BlockTree {\n+  void add_block(MetaWord* p, size_t word_size) {\n+    BlockTree::add_block(MetaBlock(p, word_size));\n+  }\n+  MetaWord* remove_block(size_t requested_size, size_t* real_size) {\n+    MetaBlock result = BlockTree::remove_block(requested_size);\n+    (*real_size) = result.word_size();\n+    return result.base();\n+  }\n+};\n@@ -38,1 +51,1 @@\n-static void create_nodes(const size_t sizes[], FeederBuffer& fb, BlockTree& bt) {\n+static void create_nodes(const size_t sizes[], FeederBuffer& fb, TestedBlockTree& bt) {\n@@ -58,1 +71,1 @@\n-  BlockTree bt;\n+  TestedBlockTree bt;\n@@ -115,1 +128,1 @@\n-  BlockTree bt;\n+  TestedBlockTree bt;\n@@ -158,1 +171,1 @@\n-  BlockTree bt;\n+  TestedBlockTree bt;\n@@ -173,1 +186,1 @@\n-  BlockTree bt;\n+  TestedBlockTree bt;\n@@ -207,1 +220,1 @@\n-  BlockTree bt;\n+  TestedBlockTree bt;\n@@ -225,1 +238,1 @@\n-  BlockTree bt;\n+  TestedBlockTree bt;\n@@ -252,1 +265,1 @@\n-  BlockTree _bt[2];\n+  TestedBlockTree _bt[2];\n@@ -359,1 +372,1 @@\n-      BlockTree* bt = _bt + which;\n+      TestedBlockTree* bt = _bt + which;\n","filename":"test\/hotspot\/gtest\/metaspace\/test_blocktree.cpp","additions":22,"deletions":9,"binary":false,"changes":31,"status":"modified"},{"patch":"@@ -0,0 +1,412 @@\n+\/*\n+ * Copyright (c) 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2023 Red Hat, Inc. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"memory\/classLoaderMetaspace.hpp\"\n+#include \"memory\/metaspace\/freeBlocks.hpp\"\n+#include \"memory\/metaspace\/metablock.inline.hpp\"\n+#include \"memory\/metaspace\/metaspaceArena.hpp\"\n+#include \"memory\/metaspace\/metaspaceSettings.hpp\"\n+#include \"memory\/metaspace\/metaspaceStatistics.hpp\"\n+#include \"memory\/metaspace.hpp\"\n+#include \"oops\/klass.hpp\"\n+#include \"runtime\/mutex.hpp\"\n+#include \"utilities\/debug.hpp\"\n+#include \"utilities\/align.hpp\"\n+#include \"utilities\/globalDefinitions.hpp\"\n+\n+#ifdef _LP64\n+\n+#define LOG_PLEASE\n+#include \"metaspaceGtestCommon.hpp\"\n+#include \"metaspaceGtestContexts.hpp\"\n+#include \"metaspaceGtestRangeHelpers.hpp\"\n+#include \"metaspaceGtestSparseArray.hpp\"\n+\n+#define HANDLE_FAILURE \\\n+  if (testing::Test::HasFailure()) { \\\n+    return; \\\n+  }\n+\n+namespace metaspace {\n+\n+class ClmsTester {\n+\n+  Mutex _lock;\n+  MetaspaceContext* _class_context;\n+  MetaspaceContext* _nonclass_context;\n+  ClassLoaderMetaspace* _clms;\n+  const size_t _klass_arena_alignment_words;\n+  unsigned _num_allocations;\n+\n+  struct Deltas {\n+    int num_chunks_delta;\n+    ssize_t used_words_delta;\n+    int num_freeblocks_delta;\n+    ssize_t freeblocks_words_delta;\n+  };\n+\n+  Deltas calc_deltas(const ArenaStats& before, const ArenaStats& after) {\n+    Deltas d;\n+    d.num_chunks_delta = after.totals()._num - before.totals()._num;\n+    d.used_words_delta = after.totals()._used_words - before.totals()._used_words;\n+    d.num_freeblocks_delta = (int)after._free_blocks_num - (int)before._free_blocks_num;\n+    d.freeblocks_words_delta = after._free_blocks_word_size - before._free_blocks_word_size;\n+    return d;\n+  }\n+\n+public:\n+\n+  ClmsTester(size_t klass_alignment_words, Metaspace::MetaspaceType space_type,\n+             MetaspaceContext* class_context, MetaspaceContext* nonclass_context)\n+  : _lock(Monitor::nosafepoint, \"CLMSTest_lock\"),\n+    _class_context(class_context), _nonclass_context(nonclass_context),\n+    _clms(nullptr), _klass_arena_alignment_words(klass_alignment_words), _num_allocations(0) {\n+    _clms = new ClassLoaderMetaspace(&_lock, space_type, nonclass_context, class_context, klass_alignment_words);\n+  }\n+\n+  ~ClmsTester() {\n+    delete _clms;\n+    EXPECT_EQ(_class_context->used_words(), (size_t)0);\n+    EXPECT_EQ(_nonclass_context->used_words(), (size_t)0);\n+  }\n+\n+  MetaBlock allocate_and_check(size_t word_size, bool is_class) {\n+\n+    \/\/ take stats before allocation\n+    ClmsStats stats_before;\n+    _clms->add_to_statistics(&stats_before);\n+\n+    \/\/ allocate\n+    MetaWord* p = _clms->allocate(word_size, is_class ? Metaspace::ClassType : Metaspace::NonClassType);\n+    _num_allocations ++;\n+\n+    \/\/ take stats after allocation\n+    ClmsStats stats_after;\n+    _clms->add_to_statistics(&stats_after);\n+\n+    \/\/ for less verbose testing:\n+    const ArenaStats& ca_before = stats_before._arena_stats_class;\n+    const ArenaStats& ca_after = stats_after._arena_stats_class;\n+    const ArenaStats& nca_before = stats_before._arena_stats_nonclass;\n+    const ArenaStats& nca_after = stats_after._arena_stats_nonclass;\n+\n+    \/\/ deltas\n+    const Deltas d_ca = calc_deltas(ca_before, ca_after);\n+    const Deltas d_nca = calc_deltas(nca_before, nca_after);\n+\n+#define EXPECT_FREEBLOCKS_UNCHANGED(arena_prefix) \\\n+    EXPECT_EQ(d_##arena_prefix.num_freeblocks_delta, 0);  \\\n+    EXPECT_EQ(d_##arena_prefix.freeblocks_words_delta, (ssize_t)0);\n+\n+#define EXPECT_ARENA_UNCHANGED(arena_prefix) \\\n+    EXPECT_EQ(d_##arena_prefix.num_chunks_delta, 0);  \\\n+    EXPECT_EQ(d_##arena_prefix.used_words_delta, (ssize_t)0);\n+\n+    if (p != nullptr) {\n+\n+      MetaBlock bl(p, word_size);\n+\n+      if (is_class) {\n+\n+        EXPECT_TRUE(bl.is_aligned_base(_klass_arena_alignment_words));\n+\n+        if (_num_allocations == 1) {\n+          \/\/ first allocation: nonclass arena unchanged, class arena grows by 1 chunk and wordsize,\n+          \/\/ class arena freeblocks unchanged\n+          EXPECT_ARENA_UNCHANGED(nca);\n+          EXPECT_FREEBLOCKS_UNCHANGED(nca);\n+          EXPECT_EQ(d_ca.num_chunks_delta, 1);\n+          EXPECT_EQ((size_t)d_ca.used_words_delta, word_size);\n+          EXPECT_FREEBLOCKS_UNCHANGED(ca);\n+          return bl;\n+        }\n+\n+        \/\/ Had this been taken from class arena freeblocks?\n+        if (d_ca.num_freeblocks_delta == -1) {\n+          \/\/ the class arena freeblocks should have gone down, and the non-class arena freeblocks may have gone\n+          \/\/ up in case the block was larger than required\n+          const size_t wordsize_block_taken = (size_t)(-d_ca.freeblocks_words_delta);\n+          EXPECT_GE(wordsize_block_taken, word_size); \/\/ the block we took must be at least allocation size\n+          const size_t expected_freeblock_remainder = wordsize_block_taken - word_size;\n+          if (expected_freeblock_remainder > 0) {\n+            \/\/ the remainder, if it existed, should have been added to nonclass freeblocks\n+            EXPECT_EQ(d_nca.num_freeblocks_delta, 1);\n+            EXPECT_EQ((size_t)d_nca.freeblocks_words_delta, expected_freeblock_remainder);\n+          }\n+          \/\/ finally, nothing should have happened in the arenas proper.\n+          EXPECT_ARENA_UNCHANGED(ca);\n+          EXPECT_ARENA_UNCHANGED(nca);\n+          return bl;\n+        }\n+\n+        \/\/ block was taken from class arena proper\n+\n+        \/\/ We expect allocation waste due to alignment, should have been added to the freeblocks\n+        \/\/ of nonclass arena. Allocation waste can be 0. If no chunk turnover happened, it must be\n+        \/\/ smaller than klass alignment, otherwise it can get as large as a commit granule.\n+        const size_t max_expected_allocation_waste =\n+            d_ca.num_chunks_delta == 0 ? (_klass_arena_alignment_words - 1) : Settings::commit_granule_words();\n+        EXPECT_GE(d_ca.num_chunks_delta, 0);\n+        EXPECT_LE(d_ca.num_chunks_delta, 1);\n+        EXPECT_GE((size_t)d_ca.used_words_delta, word_size);\n+        EXPECT_LE((size_t)d_ca.used_words_delta, word_size + max_expected_allocation_waste);\n+        EXPECT_FREEBLOCKS_UNCHANGED(ca);\n+        EXPECT_ARENA_UNCHANGED(nca);\n+        if (max_expected_allocation_waste > 0) {\n+          EXPECT_GE(d_nca.num_freeblocks_delta, 0);\n+          EXPECT_LE(d_nca.num_freeblocks_delta, 1);\n+          EXPECT_GE(d_nca.freeblocks_words_delta, 0);\n+          EXPECT_LE((size_t)d_nca.freeblocks_words_delta, max_expected_allocation_waste);\n+        } else {\n+          EXPECT_FREEBLOCKS_UNCHANGED(nca);\n+        }\n+        return bl;\n+\n+      } \/\/ end: is_class\n+\n+      else\n+\n+      {\n+        \/\/ Nonclass arena allocation.\n+        \/\/ Allocation waste can happen:\n+        \/\/ - if we allocate from nonclass freeblocks, the block remainder\n+        \/\/ - if we allocate from arena proper, by chunk turnover\n+\n+        if (d_nca.freeblocks_words_delta < 0) {\n+          \/\/ We allocated a block from the nonclass arena freeblocks.\n+          const size_t wordsize_block_taken = (size_t)(-d_nca.freeblocks_words_delta);\n+          EXPECT_EQ(wordsize_block_taken, word_size);\n+          \/\/ The number of blocks may or may not have decreased (depending on whether there\n+          \/\/ was a wastage block)\n+          EXPECT_GE(d_nca.num_chunks_delta, -1);\n+          EXPECT_LE(d_nca.num_chunks_delta, 0);\n+          EXPECT_ARENA_UNCHANGED(nca);\n+          EXPECT_ARENA_UNCHANGED(ca);\n+          EXPECT_FREEBLOCKS_UNCHANGED(ca);\n+          return bl;\n+        }\n+\n+        \/\/ We don't expect alignment waste. Only wastage happens at chunk turnover.\n+        const size_t max_expected_allocation_waste =\n+            d_nca.num_chunks_delta == 0 ? 0 : Settings::commit_granule_words();\n+        EXPECT_ARENA_UNCHANGED(ca);\n+        EXPECT_FREEBLOCKS_UNCHANGED(ca);\n+        EXPECT_GE(d_nca.num_chunks_delta, 0);\n+        EXPECT_LE(d_nca.num_chunks_delta, 1);\n+        EXPECT_GE((size_t)d_nca.used_words_delta, word_size);\n+        EXPECT_LE((size_t)d_nca.used_words_delta, word_size + max_expected_allocation_waste);\n+        if (max_expected_allocation_waste == 0) {\n+          EXPECT_FREEBLOCKS_UNCHANGED(nca);\n+        }\n+      }\n+      return bl;\n+\n+    } \/\/ end: allocation successful\n+\n+    \/\/ allocation failed.\n+    EXPECT_ARENA_UNCHANGED(ca);\n+    EXPECT_FREEBLOCKS_UNCHANGED(ca);\n+    EXPECT_ARENA_UNCHANGED(nca);\n+    EXPECT_FREEBLOCKS_UNCHANGED(nca);\n+\n+    return MetaBlock();\n+  }\n+\n+  MetaBlock allocate_expect_success(size_t word_size, bool is_class) {\n+    MetaBlock bl = allocate_and_check(word_size, is_class);\n+    EXPECT_TRUE(bl.is_nonempty());\n+    return bl;\n+  }\n+\n+  MetaBlock allocate_expect_failure(size_t word_size, bool is_class) {\n+    MetaBlock bl = allocate_and_check(word_size, is_class);\n+    EXPECT_TRUE(bl.is_empty());\n+    return bl;\n+  }\n+\n+  void deallocate_and_check(MetaBlock bl, bool is_class) {\n+\n+    \/\/ take stats before deallocation\n+    ClmsStats stats_before;\n+    _clms->add_to_statistics(&stats_before);\n+\n+    \/\/ allocate\n+    _clms->deallocate(bl.base(), bl.word_size(), is_class);\n+\n+    \/\/ take stats after deallocation\n+    ClmsStats stats_after;\n+    _clms->add_to_statistics(&stats_after);\n+\n+    \/\/ for less verbose testing:\n+    const ArenaStats& ca_before = stats_before._arena_stats_class;\n+    const ArenaStats& ca_after = stats_after._arena_stats_class;\n+    const ArenaStats& nca_before = stats_before._arena_stats_nonclass;\n+    const ArenaStats& nca_after = stats_after._arena_stats_nonclass;\n+\n+    \/\/ deltas\n+    \/\/ deltas\n+    const Deltas d_ca = calc_deltas(ca_before, ca_after);\n+    const Deltas d_nca = calc_deltas(nca_before, nca_after);\n+\n+    EXPECT_ARENA_UNCHANGED(ca);\n+    EXPECT_ARENA_UNCHANGED(nca);\n+    if (is_class) {\n+      EXPECT_EQ(d_ca.num_freeblocks_delta, 1);\n+      EXPECT_EQ((size_t)d_ca.freeblocks_words_delta, bl.word_size());\n+      EXPECT_FREEBLOCKS_UNCHANGED(nca);\n+    } else {\n+      EXPECT_EQ(d_nca.num_freeblocks_delta, 1);\n+      EXPECT_EQ((size_t)d_nca.freeblocks_words_delta, bl.word_size());\n+      EXPECT_FREEBLOCKS_UNCHANGED(ca);\n+    }\n+  }\n+};\n+\n+static constexpr size_t klass_size = sizeof(Klass) \/ BytesPerWord;\n+\n+static void basic_test(size_t klass_arena_alignment) {\n+  MetaspaceGtestContext class_context, nonclass_context;\n+  {\n+    ClmsTester tester(klass_arena_alignment, Metaspace::StandardMetaspaceType, class_context.context(), nonclass_context.context());\n+\n+    MetaBlock bl1 = tester.allocate_expect_success(klass_size, true);\n+    HANDLE_FAILURE;\n+\n+    MetaBlock bl2 = tester.allocate_expect_success(klass_size, true);\n+    HANDLE_FAILURE;\n+\n+    tester.deallocate_and_check(bl1, true);\n+    HANDLE_FAILURE;\n+\n+    MetaBlock bl3 = tester.allocate_expect_success(klass_size, true);\n+    HANDLE_FAILURE;\n+    EXPECT_EQ(bl3, bl1); \/\/ should have gotten the same block back from freelist\n+\n+    MetaBlock bl4 = tester.allocate_expect_success(Metaspace::min_allocation_word_size, false);\n+    HANDLE_FAILURE;\n+\n+    MetaBlock bl5 = tester.allocate_expect_success(K, false);\n+    HANDLE_FAILURE;\n+\n+    tester.deallocate_and_check(bl5, false);\n+    HANDLE_FAILURE;\n+\n+    MetaBlock bl6 = tester.allocate_expect_success(K, false);\n+    HANDLE_FAILURE;\n+\n+    EXPECT_EQ(bl5, bl6); \/\/ should have gotten the same block back from freelist\n+  }\n+  EXPECT_EQ(class_context.used_words(), (size_t)0);\n+  EXPECT_EQ(nonclass_context.used_words(), (size_t)0);\n+  \/\/ we should have used exactly one commit granule (64K), not more, for each context\n+  EXPECT_EQ(class_context.committed_words(), Settings::commit_granule_words());\n+  EXPECT_EQ(nonclass_context.committed_words(), Settings::commit_granule_words());\n+}\n+\n+#define TEST_BASIC_N(n)               \\\n+TEST_VM(metaspace, CLMS_basics_##n) { \\\n+  basic_test(n);            \\\n+}\n+\n+TEST_BASIC_N(1)\n+TEST_BASIC_N(4)\n+TEST_BASIC_N(16)\n+TEST_BASIC_N(32)\n+TEST_BASIC_N(128)\n+\n+static void test_random(size_t klass_arena_alignment) {\n+  MetaspaceGtestContext class_context, nonclass_context;\n+  constexpr int max_allocations = 1024;\n+  const SizeRange nonclass_alloc_range(Metaspace::min_allocation_alignment_words, 1024);\n+  const SizeRange class_alloc_range(klass_size, 1024);\n+  const IntRange one_out_of_ten(0, 10);\n+  for (int runs = 9; runs >= 0; runs--) {\n+    {\n+      ClmsTester tester(64, Metaspace::StandardMetaspaceType, class_context.context(), nonclass_context.context());\n+      struct LifeBlock {\n+        MetaBlock bl;\n+        bool is_class;\n+      };\n+      LifeBlock life_allocations[max_allocations];\n+      for (int i = 0; i < max_allocations; i++) {\n+        life_allocations[i].bl.reset();\n+      }\n+\n+      unsigned num_class_allocs = 0, num_nonclass_allocs = 0, num_class_deallocs = 0, num_nonclass_deallocs = 0;\n+      for (int i = 0; i < 5000; i ++) {\n+        const int slot = IntRange(0, max_allocations).random_value();\n+        if (life_allocations[slot].bl.is_empty()) {\n+          const bool is_class = one_out_of_ten.random_value() == 0;\n+          const size_t word_size =\n+              is_class ? class_alloc_range.random_value() : nonclass_alloc_range.random_value();\n+          MetaBlock bl = tester.allocate_expect_success(word_size, is_class);\n+          HANDLE_FAILURE;\n+          life_allocations[slot].bl = bl;\n+          life_allocations[slot].is_class = is_class;\n+          if (is_class) {\n+            num_class_allocs ++;\n+          } else {\n+            num_nonclass_allocs ++;\n+          }\n+        } else {\n+          tester.deallocate_and_check(life_allocations[slot].bl, life_allocations[slot].is_class);\n+          HANDLE_FAILURE;\n+          life_allocations[slot].bl.reset();\n+          if (life_allocations[slot].is_class) {\n+            num_class_deallocs ++;\n+          } else {\n+            num_nonclass_deallocs ++;\n+          }\n+        }\n+      }\n+      LOG(\"num class allocs: %u, num nonclass allocs: %u, num class deallocs: %u, num nonclass deallocs: %u\",\n+          num_class_allocs, num_nonclass_allocs, num_class_deallocs, num_nonclass_deallocs);\n+    }\n+    EXPECT_EQ(class_context.used_words(), (size_t)0);\n+    EXPECT_EQ(nonclass_context.used_words(), (size_t)0);\n+    constexpr float fragmentation_factor = 3.0f;\n+    const size_t max_expected_nonclass_committed = max_allocations * nonclass_alloc_range.highest() * fragmentation_factor;\n+    const size_t max_expected_class_committed = max_allocations * class_alloc_range.highest() * fragmentation_factor;\n+    \/\/ we should have used exactly one commit granule (64K), not more, for each context\n+    EXPECT_LT(class_context.committed_words(), max_expected_class_committed);\n+    EXPECT_LT(nonclass_context.committed_words(), max_expected_nonclass_committed);\n+  }\n+}\n+\n+#define TEST_RANDOM_N(n)               \\\n+TEST_VM(metaspace, CLMS_random_##n) {  \\\n+  test_random(n);                      \\\n+}\n+\n+TEST_RANDOM_N(1)\n+TEST_RANDOM_N(4)\n+TEST_RANDOM_N(16)\n+TEST_RANDOM_N(32)\n+TEST_RANDOM_N(128)\n+\n+} \/\/ namespace metaspace\n+\n+#endif \/\/ _LP64\n","filename":"test\/hotspot\/gtest\/metaspace\/test_clms.cpp","additions":412,"deletions":0,"binary":false,"changes":412,"status":"added"},{"patch":"@@ -29,0 +29,2 @@\n+#include \"memory\/metaspace\/metablock.hpp\"\n+\n@@ -33,0 +35,1 @@\n+using metaspace::MetaBlock;\n@@ -46,166 +49,0 @@\n-class FreeBlocksTest {\n-\n-  FeederBuffer _fb;\n-  FreeBlocks _freeblocks;\n-\n-  \/\/ random generator for block feeding\n-  RandSizeGenerator _rgen_feeding;\n-\n-  \/\/ random generator for allocations (and, hence, deallocations)\n-  RandSizeGenerator _rgen_allocations;\n-\n-  SizeCounter _allocated_words;\n-\n-  struct allocation_t {\n-    allocation_t* next;\n-    size_t word_size;\n-    MetaWord* p;\n-  };\n-\n-  \/\/ Array of the same size as the pool max capacity; holds the allocated elements.\n-  allocation_t* _allocations;\n-\n-  int _num_allocs;\n-  int _num_deallocs;\n-  int _num_feeds;\n-\n-  bool feed_some() {\n-    size_t word_size = _rgen_feeding.get();\n-    MetaWord* p = _fb.get(word_size);\n-    if (p != nullptr) {\n-      _freeblocks.add_block(p, word_size);\n-      return true;\n-    }\n-    return false;\n-  }\n-\n-  bool deallocate_top() {\n-\n-    allocation_t* a = _allocations;\n-    if (a != nullptr) {\n-      _allocations = a->next;\n-      check_marked_range(a->p, a->word_size);\n-      _freeblocks.add_block(a->p, a->word_size);\n-      delete a;\n-      DEBUG_ONLY(_freeblocks.verify();)\n-      return true;\n-    }\n-    return false;\n-  }\n-\n-  void deallocate_all() {\n-    while (deallocate_top());\n-  }\n-\n-  bool allocate() {\n-\n-    size_t word_size = MAX2(_rgen_allocations.get(), _freeblocks.MinWordSize);\n-    MetaWord* p = _freeblocks.remove_block(word_size);\n-    if (p != nullptr) {\n-      _allocated_words.increment_by(word_size);\n-      allocation_t* a = new allocation_t;\n-      a->p = p; a->word_size = word_size;\n-      a->next = _allocations;\n-      _allocations = a;\n-      DEBUG_ONLY(_freeblocks.verify();)\n-      mark_range(p, word_size);\n-      return true;\n-    }\n-    return false;\n-  }\n-\n-  void test_all_marked_ranges() {\n-    for (allocation_t* a = _allocations; a != nullptr; a = a->next) {\n-      check_marked_range(a->p, a->word_size);\n-    }\n-  }\n-\n-  void test_loop() {\n-    \/\/ We loop and in each iteration execute one of three operations:\n-    \/\/ - allocation from fbl\n-    \/\/ - deallocation to fbl of a previously allocated block\n-    \/\/ - feeding a new larger block into the fbl (mimicks chunk retiring)\n-    \/\/ When we have fed all large blocks into the fbl (feedbuffer empty), we\n-    \/\/  switch to draining the fbl completely (only allocs)\n-    bool forcefeed = false;\n-    bool draining = false;\n-    bool stop = false;\n-    int iter = 25000; \/\/ safety stop\n-    while (!stop && iter > 0) {\n-      iter --;\n-      int surprise = (int)os::random() % 10;\n-      if (!draining && (surprise >= 7 || forcefeed)) {\n-        forcefeed = false;\n-        if (feed_some()) {\n-          _num_feeds++;\n-        } else {\n-          \/\/ We fed all input memory into the fbl. Now lets proceed until the fbl is drained.\n-          draining = true;\n-        }\n-      } else if (!draining && surprise < 1) {\n-        deallocate_top();\n-        _num_deallocs++;\n-      } else {\n-        if (allocate()) {\n-          _num_allocs++;\n-        } else {\n-          if (draining) {\n-            stop = _freeblocks.total_size() < 512;\n-          } else {\n-            forcefeed = true;\n-          }\n-        }\n-      }\n-      if ((iter % 1000) == 0) {\n-        DEBUG_ONLY(_freeblocks.verify();)\n-        test_all_marked_ranges();\n-        LOG(\"a %d (\" SIZE_FORMAT \"), d %d, f %d\", _num_allocs, _allocated_words.get(), _num_deallocs, _num_feeds);\n-#ifdef LOG_PLEASE\n-        _freeblocks.print(tty, true);\n-        tty->cr();\n-#endif\n-      }\n-    }\n-\n-    \/\/ Drain\n-\n-  }\n-\n-public:\n-\n-  FreeBlocksTest(size_t avg_alloc_size) :\n-    _fb(512 * K), _freeblocks(),\n-    _rgen_feeding(128, 4096),\n-    _rgen_allocations(avg_alloc_size \/ 4, avg_alloc_size * 2, 0.01f, avg_alloc_size \/ 3, avg_alloc_size * 30),\n-    _allocations(nullptr),\n-    _num_allocs(0),\n-    _num_deallocs(0),\n-    _num_feeds(0)\n-  {\n-    CHECK_CONTENT(_freeblocks, 0, 0);\n-    \/\/ some initial feeding\n-    _freeblocks.add_block(_fb.get(1024), 1024);\n-    CHECK_CONTENT(_freeblocks, 1, 1024);\n-  }\n-\n-  ~FreeBlocksTest() {\n-    deallocate_all();\n-  }\n-\n-  static void test_small_allocations() {\n-    FreeBlocksTest test(10);\n-    test.test_loop();\n-  }\n-\n-  static void test_medium_allocations() {\n-    FreeBlocksTest test(30);\n-    test.test_loop();\n-  }\n-\n-  static void test_large_allocations() {\n-    FreeBlocksTest test(150);\n-    test.test_loop();\n-  }\n-\n-};\n-\n@@ -218,1 +55,2 @@\n-  fbl.add_block(tmp, 1024);\n+  MetaBlock bl(tmp, 1024);\n+  fbl.add_block(bl);\n@@ -223,2 +61,2 @@\n-  MetaWord* p = fbl.remove_block(1024);\n-  EXPECT_EQ(p, tmp);\n+  MetaBlock bl2 = fbl.remove_block(1024);\n+  ASSERT_EQ(bl, bl2);\n@@ -229,13 +67,0 @@\n-\n-TEST_VM(metaspace, freeblocks_small) {\n-  FreeBlocksTest::test_small_allocations();\n-}\n-\n-TEST_VM(metaspace, freeblocks_medium) {\n-  FreeBlocksTest::test_medium_allocations();\n-}\n-\n-TEST_VM(metaspace, freeblocks_large) {\n-  FreeBlocksTest::test_large_allocations();\n-}\n-\n","filename":"test\/hotspot\/gtest\/metaspace\/test_freeblocks.cpp","additions":7,"deletions":182,"binary":false,"changes":189,"status":"modified"},{"patch":"@@ -0,0 +1,97 @@\n+\/*\n+ * Copyright (c) 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2020 SAP SE. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"memory\/metaspace\/metablock.inline.hpp\"\n+\/\/#define LOG_PLEASE\n+#include \"metaspaceGtestCommon.hpp\"\n+\n+using metaspace::MetaBlock;\n+\n+\n+#define CHECK_BLOCK_EMPTY(block) { \\\n+  EXPECT_TRUE(block.is_empty()); \\\n+  DEBUG_ONLY(block.verify()); \\\n+}\n+\n+#define CHECK_BLOCK(block, expected_base, expected_size) { \\\n+    EXPECT_EQ(block.base(), (MetaWord*)expected_base); \\\n+    EXPECT_EQ((size_t)expected_size, block.word_size()); \\\n+    EXPECT_EQ(block.end(), expected_base + expected_size); \\\n+    DEBUG_ONLY(block.verify()); \\\n+}\n+\n+static constexpr uintptr_t large_pointer = NOT_LP64(0x99999990) LP64_ONLY(0x9999999999999990ULL);\n+\n+TEST(metaspace, MetaBlock_1) {\n+  MetaBlock bl;\n+  CHECK_BLOCK_EMPTY(bl);\n+}\n+\n+TEST(metaspace, MetaBlock_2) {\n+  MetaWord* const p = (MetaWord*)large_pointer;\n+  constexpr size_t s = G;\n+  MetaBlock bl(p, s);\n+  CHECK_BLOCK(bl, p, s);\n+}\n+\n+TEST(metaspace, MetaBlock_3) {\n+  MetaWord* const p = (MetaWord*)large_pointer;\n+  MetaBlock bl(p, 0);\n+  CHECK_BLOCK_EMPTY(bl);\n+}\n+\n+TEST_VM(metaspace, MetaBlock_4) {\n+  MetaWord* const p = (MetaWord*)large_pointer;\n+  MetaBlock bl(p, G);\n+  CHECK_BLOCK(bl, p, G);\n+\n+  MetaBlock bl_copy = bl, bl2;\n+\n+  bl2 = bl.split_off_tail(M);\n+  CHECK_BLOCK(bl, p, G - M);\n+  CHECK_BLOCK(bl2, p + G - M, M);\n+\n+  bl = bl_copy;\n+\n+bl.print_on(tty);\n+bl2.print_on(tty);\n+  bl2 = bl.split_off_tail(G);\n+  bl.print_on(tty);\n+  bl2.print_on(tty);\n+\n+  ASSERT_EQ(bl2, bl_copy);\n+  ASSERT_TRUE(bl.is_empty());\n+\n+  bl = bl_copy;\n+\n+  bl2 = bl.split_off_tail(0);\n+  ASSERT_EQ(bl, bl_copy);\n+  ASSERT_TRUE(bl2.is_empty());\n+\n+  MetaBlock empty;\n+  bl = empty.split_off_tail(0);\n+  ASSERT_TRUE(bl.is_empty());\n+}\n","filename":"test\/hotspot\/gtest\/metaspace\/test_metablock.cpp","additions":97,"deletions":0,"binary":false,"changes":97,"status":"added"},{"patch":"@@ -31,1 +31,1 @@\n-TEST_VM(MetaspaceUtils, reserved) {\n+TEST_VM(metaspace, MetaspaceUtils_reserved) {\n@@ -40,1 +40,1 @@\n-TEST_VM(MetaspaceUtils, reserved_compressed_class_pointers) {\n+TEST_VM(metaspace, MetaspaceUtils_reserved_compressed_class_pointers) {\n@@ -52,1 +52,1 @@\n-TEST_VM(MetaspaceUtils, committed) {\n+TEST_VM(metaspace, MetaspaceUtils_committed) {\n@@ -64,1 +64,1 @@\n-TEST_VM(MetaspaceUtils, committed_compressed_class_pointers) {\n+TEST_VM(metaspace, MetaspaceUtils_committed_compressed_class_pointers) {\n@@ -76,1 +76,1 @@\n-TEST_VM(MetaspaceUtils, non_compressed_class_pointers) {\n+TEST_VM(metaspace, MetaspaceUtils_non_compressed_class_pointers) {\n@@ -102,1 +102,1 @@\n-TEST_VM(MetaspaceUtils, get_statistics) {\n+TEST_VM(MetaspaceUtils, MetaspaceUtils_get_statistics) {\n","filename":"test\/hotspot\/gtest\/metaspace\/test_metaspaceUtils.cpp","additions":6,"deletions":6,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -4,0 +4,1 @@\n+ * Copyright (c) 2023 Red Hat, Inc. All rights reserved.\n@@ -31,0 +32,2 @@\n+#include \"memory\/metaspace\/freeBlocks.hpp\"\n+#include \"memory\/metaspace\/metablock.inline.hpp\"\n@@ -33,0 +36,1 @@\n+#include \"memory\/metaspace\/metachunkList.hpp\"\n@@ -36,0 +40,1 @@\n+#include \"memory\/metaspace.hpp\"\n@@ -39,1 +44,1 @@\n-\/\/#define LOG_PLEASE\n+#define LOG_PLEASE\n@@ -44,9 +49,14 @@\n-using metaspace::AllocationAlignmentByteSize;\n-using metaspace::ArenaGrowthPolicy;\n-using metaspace::CommitLimiter;\n-using metaspace::InternalStats;\n-using metaspace::MemRangeCounter;\n-using metaspace::MetaspaceArena;\n-using metaspace::SizeAtomicCounter;\n-using metaspace::Settings;\n-using metaspace::ArenaStats;\n+#define HANDLE_FAILURE \\\n+  if (testing::Test::HasFailure()) { \\\n+    return; \\\n+  }\n+\n+namespace metaspace {\n+\n+class MetaspaceArenaTestFriend {\n+  const MetaspaceArena* const _arena;\n+public:\n+  MetaspaceArenaTestFriend(const MetaspaceArena* arena) : _arena(arena) {}\n+  const MetachunkList& chunks() const { return _arena->_chunks; }\n+  const FreeBlocks* fbl() const { return _arena->_fbl; }\n+};\n@@ -57,0 +67,1 @@\n+  const ArenaGrowthPolicy* const _growth_policy;\n@@ -58,2 +69,0 @@\n-  const ArenaGrowthPolicy* _growth_policy;\n-  SizeAtomicCounter _used_words_counter;\n@@ -62,6 +71,0 @@\n-  void initialize(const ArenaGrowthPolicy* growth_policy, const char* name = \"gtest-MetaspaceArena\") {\n-    _growth_policy = growth_policy;\n-    _arena = new MetaspaceArena(&_context.cm(), _growth_policy, &_used_words_counter, name);\n-    DEBUG_ONLY(_arena->verify());\n-  }\n-\n@@ -70,9 +73,0 @@\n-  \/\/ Create a helper; growth policy for arena is determined by the given spacetype|class tupel\n-  MetaspaceArenaTestHelper(MetaspaceGtestContext& helper,\n-                            Metaspace::MetaspaceType space_type, bool is_class,\n-                            const char* name = \"gtest-MetaspaceArena\") :\n-    _context(helper)\n-  {\n-    initialize(ArenaGrowthPolicy::policy_for_space_type(space_type, is_class), name);\n-  }\n-\n@@ -81,2 +75,2 @@\n-                           const char* name = \"gtest-MetaspaceArena\") :\n-    _context(helper)\n+                           size_t allocation_alignment_words = Metaspace::min_allocation_alignment_words) :\n+    _context(helper), _growth_policy(growth_policy), _arena(nullptr)\n@@ -84,1 +78,3 @@\n-    initialize(growth_policy, name);\n+    _arena = new MetaspaceArena(_context.context(), _growth_policy, allocation_alignment_words, \"gtest-MetaspaceArena\");\n+    DEBUG_ONLY(_arena->verify());\n+    _context.inc_num_arenas_created();\n@@ -87,0 +83,8 @@\n+\n+  \/\/ Create a helper; growth policy for arena is determined by the given spacetype|class tupel\n+  MetaspaceArenaTestHelper(MetaspaceGtestContext& helper,\n+                           Metaspace::MetaspaceType space_type, bool is_class,\n+                           size_t allocation_alignment_words = Metaspace::min_allocation_alignment_words) :\n+    MetaspaceArenaTestHelper(helper, ArenaGrowthPolicy::policy_for_space_type(space_type, is_class), allocation_alignment_words)\n+  {}\n+\n@@ -91,1 +95,0 @@\n-  const CommitLimiter& limiter() const { return _context.commit_limiter(); }\n@@ -93,1 +96,0 @@\n-  SizeAtomicCounter& used_words_counter() { return _used_words_counter; }\n@@ -100,2 +102,2 @@\n-      size_t used_words_before = _used_words_counter.get();\n-      size_t committed_words_before = limiter().committed_words();\n+      size_t used_words_before = _context.used_words();\n+      size_t committed_words_before = _context.committed_words();\n@@ -105,3 +107,8 @@\n-      size_t used_words_after = _used_words_counter.get();\n-      size_t committed_words_after = limiter().committed_words();\n-      ASSERT_0(used_words_after);\n+      size_t used_words_after = _context.used_words();\n+      size_t committed_words_after = _context.committed_words();\n+      assert(_context.num_arenas_created() >= 1, \"Sanity\");\n+      if (_context.num_arenas_created() == 1) {\n+        ASSERT_0(used_words_after);\n+      } else {\n+        ASSERT_LE(used_words_after, used_words_before);\n+      }\n@@ -113,7 +120,17 @@\n-    _arena->usage_numbers(p_used, p_committed, p_capacity);\n-    if (p_used != nullptr) {\n-      if (p_committed != nullptr) {\n-        ASSERT_GE(*p_committed, *p_used);\n-      }\n-      \/\/ Since we own the used words counter, it should reflect our usage number 1:1\n-      ASSERT_EQ(_used_words_counter.get(), *p_used);\n+    size_t arena_used = 0, arena_committed = 0, arena_reserved = 0;\n+    _arena->usage_numbers(&arena_used, &arena_committed, &arena_reserved);\n+    EXPECT_GE(arena_committed, arena_used);\n+    EXPECT_GE(arena_reserved, arena_committed);\n+\n+    size_t context_used = _context.used_words();\n+    size_t context_committed = _context.committed_words();\n+    size_t context_reserved = _context.reserved_words();\n+    EXPECT_GE(context_committed, context_used);\n+    EXPECT_GE(context_reserved, context_committed);\n+\n+    \/\/ If only one arena uses the context, usage numbers must match.\n+    if (_context.num_arenas_created() == 1) {\n+      EXPECT_EQ(context_used, arena_used);\n+    } else {\n+      assert(_context.num_arenas_created() > 1, \"Sanity\");\n+      EXPECT_GE(context_used, arena_used);\n@@ -121,2 +138,13 @@\n-    if (p_committed != nullptr && p_capacity != nullptr) {\n-      ASSERT_GE(*p_capacity, *p_committed);\n+\n+    \/\/ commit, reserve numbers don't have to match since free chunks may exist\n+    EXPECT_GE(context_committed, arena_committed);\n+    EXPECT_GE(context_reserved, arena_reserved);\n+\n+    if (p_used) {\n+      *p_used = arena_used;\n+    }\n+    if (p_committed) {\n+      *p_committed = arena_committed;\n+    }\n+    if (p_capacity) {\n+      *p_capacity = arena_reserved;\n@@ -145,1 +173,0 @@\n-  \/\/ Allocate; it may or may not work; return value in *p_return_value\n@@ -147,0 +174,11 @@\n+    MetaBlock result, wastage;\n+    allocate_from_arena_with_tests(word_size, result, wastage);\n+    if (wastage.is_nonempty()) {\n+      _arena->deallocate(wastage);\n+      wastage.reset();\n+    }\n+    (*p_return_value) = result.base();\n+  }\n+\n+  \/\/ Allocate; it may or may not work; return value in *p_return_value\n+  void allocate_from_arena_with_tests(size_t word_size, MetaBlock& result, MetaBlock& wastage) {\n@@ -152,1 +190,1 @@\n-    size_t possible_expansion = limiter().possible_expansion_words();\n+    size_t possible_expansion = _context.commit_limiter().possible_expansion_words();\n@@ -154,1 +192,1 @@\n-    MetaWord* p = _arena->allocate(word_size);\n+    result = _arena->allocate(word_size, wastage);\n@@ -161,1 +199,1 @@\n-    if (p == nullptr) {\n+    if (result.is_empty()) {\n@@ -169,1 +207,2 @@\n-      ASSERT_TRUE(is_aligned(p, AllocationAlignmentByteSize));\n+      ASSERT_TRUE(result.is_aligned_base(_arena->allocation_alignment_words()));\n+\n@@ -178,2 +217,0 @@\n-\n-    *p_return_value = p;\n@@ -192,1 +229,1 @@\n-    _arena->deallocate(p, word_size);\n+    _arena->deallocate(MetaBlock(p, word_size));\n@@ -212,0 +249,4 @@\n+  MetaspaceArenaTestFriend internal_access() const {\n+    return MetaspaceArenaTestFriend (_arena);\n+  }\n+\n@@ -214,1 +255,1 @@\n-    return get_arena_statistics().totals()._num;\n+    return internal_access().chunks().count();\n@@ -393,0 +434,1 @@\n+    ASSERT_FALSE(HasFailure());\n@@ -396,0 +438,1 @@\n+    ASSERT_FALSE(HasFailure());\n@@ -402,0 +445,1 @@\n+    ASSERT_FALSE(HasFailure());\n@@ -407,0 +451,1 @@\n+    ASSERT_FALSE(HasFailure());\n@@ -410,0 +455,1 @@\n+    ASSERT_FALSE(HasFailure());\n@@ -452,0 +498,1 @@\n+    HANDLE_FAILURE\n@@ -496,1 +543,1 @@\n-  MetaspaceArenaTestHelper smhelper(context, type, is_class, \"Grower\");\n+  MetaspaceArenaTestHelper smhelper(context, type, is_class);\n@@ -498,1 +545,1 @@\n-  MetaspaceArenaTestHelper smhelper_harrasser(context, Metaspace::ReflectionMetaspaceType, true, \"Harasser\");\n+  MetaspaceArenaTestHelper smhelper_harrasser(context, Metaspace::ReflectionMetaspaceType, true);\n@@ -561,0 +608,1 @@\n+    HANDLE_FAILURE\n@@ -567,0 +615,1 @@\n+    HANDLE_FAILURE\n@@ -608,0 +657,4 @@\n+  \/\/ No FBL should exist, we did not deallocate\n+  ASSERT_EQ(smhelper.internal_access().fbl(), (FreeBlocks*)nullptr);\n+  ASSERT_EQ(smhelper_harrasser.internal_access().fbl(), (FreeBlocks*)nullptr);\n+\n@@ -711,1 +764,2 @@\n-  for (size_t blocksize = Metaspace::max_allocation_word_size(); blocksize >= 1; blocksize \/= 2) {\n+  for (size_t blocksize = Metaspace::max_allocation_word_size();\n+       blocksize >= Metaspace::min_allocation_word_size; blocksize \/= 2) {\n@@ -723,0 +777,1 @@\n+      HANDLE_FAILURE\n@@ -732,0 +787,1 @@\n+      HANDLE_FAILURE\n@@ -750,0 +806,117 @@\n+\n+static void test_random_aligned_allocation(size_t arena_alignment_words, SizeRange range) {\n+  \/\/ We let the arena use 4K chunks, unless the alloc size is larger.\n+  chunklevel_t level = CHUNK_LEVEL_4K;\n+  const ArenaGrowthPolicy policy (&level, 1);\n+  const size_t chunk_word_size = word_size_for_level(level);\n+\n+  size_t expected_used = 0;\n+\n+  MetaspaceGtestContext context;\n+  MetaspaceArenaTestHelper helper(context, &policy, arena_alignment_words);\n+\n+  size_t last_alloc_size = 0;\n+  unsigned num_allocations = 0;\n+\n+  const size_t max_used = MIN2(MAX2(chunk_word_size * 10, (range.highest() * 100)),\n+                               LP64_ONLY(64) NOT_LP64(16) * M); \/\/ word size!\n+  while (expected_used < max_used) {\n+\n+    const int chunks_before = helper.get_number_of_chunks();\n+\n+    MetaBlock result, wastage;\n+    size_t alloc_words = range.random_value();\n+    NOT_LP64(alloc_words = align_up(alloc_words, Metaspace::min_allocation_alignment_words));\n+    helper.allocate_from_arena_with_tests(alloc_words, result, wastage);\n+\n+    ASSERT_TRUE(result.is_nonempty());\n+    ASSERT_TRUE(result.is_aligned_base(arena_alignment_words));\n+    ASSERT_EQ(result.word_size(), alloc_words);\n+\n+    expected_used += alloc_words + wastage.word_size();\n+    const int chunks_now = helper.get_number_of_chunks();\n+    ASSERT_GE(chunks_now, chunks_before);\n+    ASSERT_LE(chunks_now, chunks_before + 1);\n+\n+    \/\/ Estimate wastage:\n+    \/\/ Guessing at wastage is somewhat simple since we don't expect to ever use the fbl (we\n+    \/\/ don't deallocate). Therefore, wastage can only be caused by alignment gap or by\n+    \/\/ salvaging an old chunk before a new chunk is added.\n+    const bool expect_alignment_gap = !is_aligned(last_alloc_size, arena_alignment_words);\n+    const bool new_chunk_added = chunks_now > chunks_before;\n+\n+    if (num_allocations == 0) {\n+      \/\/ expect no wastage if its the first allocation in the arena\n+      ASSERT_TRUE(wastage.is_empty());\n+    } else {\n+      if (expect_alignment_gap) {\n+        \/\/ expect wastage if the alignment requires it\n+        ASSERT_TRUE(wastage.is_nonempty());\n+      }\n+    }\n+\n+    if (wastage.is_nonempty()) {\n+      \/\/ If we have wastage, we expect it to be either too small or unaligned. That would not be true\n+      \/\/ for wastage from the fbl, which could have any size; however, in this test we don't deallocate,\n+      \/\/ so we don't expect wastage from the fbl.\n+      if (wastage.is_aligned_base(arena_alignment_words)) {\n+        ASSERT_LT(wastage.word_size(), alloc_words);\n+      }\n+      if (new_chunk_added) {\n+        \/\/ chunk turnover: no more wastage than size of a commit granule, since we salvage the\n+        \/\/ committed remainder of the old chunk.\n+        ASSERT_LT(wastage.word_size(), Settings::commit_granule_words());\n+      } else {\n+        \/\/ No chunk turnover: no more wastage than what alignment requires.\n+        ASSERT_LT(wastage.word_size(), arena_alignment_words);\n+      }\n+    }\n+\n+    \/\/ Check stats too\n+    size_t used, committed, reserved;\n+    helper.usage_numbers_with_test(&used, &committed, &reserved);\n+    ASSERT_EQ(used, expected_used);\n+\n+    \/\/ No FBL should exist, we did not deallocate\n+    ASSERT_EQ(helper.internal_access().fbl(), (FreeBlocks*)nullptr);\n+\n+    HANDLE_FAILURE\n+\n+    last_alloc_size = alloc_words;\n+    num_allocations ++;\n+  }\n+  LOG(\"allocs: %u\", num_allocations);\n+}\n+\n+#define TEST_ARENA_WITH_ALIGNMENT_SMALL_RANGE(al)                              \\\n+TEST_VM(metaspace, MetaspaceArena_test_random_small_aligned_allocation_##al) { \\\n+  static const SizeRange range(Metaspace::min_allocation_word_size, 128);      \\\n+  test_random_aligned_allocation(al, range);                                   \\\n+}\n+\n+#ifdef _LP64\n+TEST_ARENA_WITH_ALIGNMENT_SMALL_RANGE(1);\n+#endif\n+TEST_ARENA_WITH_ALIGNMENT_SMALL_RANGE(2);\n+TEST_ARENA_WITH_ALIGNMENT_SMALL_RANGE(8);\n+TEST_ARENA_WITH_ALIGNMENT_SMALL_RANGE(32);\n+TEST_ARENA_WITH_ALIGNMENT_SMALL_RANGE(128);\n+TEST_ARENA_WITH_ALIGNMENT_SMALL_RANGE(MIN_CHUNK_WORD_SIZE);\n+\n+#define TEST_ARENA_WITH_ALIGNMENT_LARGE_RANGE(al)                              \\\n+TEST_VM(metaspace, MetaspaceArena_test_random_large_aligned_allocation_##al) { \\\n+  static const SizeRange range(Metaspace::max_allocation_word_size() \/ 2,      \\\n+                                   Metaspace::max_allocation_word_size());     \\\n+  test_random_aligned_allocation(al, range);                                   \\\n+}\n+\n+#ifdef _LP64\n+TEST_ARENA_WITH_ALIGNMENT_LARGE_RANGE(1);\n+#endif\n+TEST_ARENA_WITH_ALIGNMENT_LARGE_RANGE(2);\n+TEST_ARENA_WITH_ALIGNMENT_LARGE_RANGE(8);\n+TEST_ARENA_WITH_ALIGNMENT_LARGE_RANGE(32);\n+TEST_ARENA_WITH_ALIGNMENT_LARGE_RANGE(128);\n+TEST_ARENA_WITH_ALIGNMENT_LARGE_RANGE(MIN_CHUNK_WORD_SIZE);\n+\n+} \/\/ namespace metaspace\n","filename":"test\/hotspot\/gtest\/metaspace\/test_metaspacearena.cpp","additions":231,"deletions":58,"binary":false,"changes":289,"status":"modified"},{"patch":"@@ -29,0 +29,1 @@\n+#include \"memory\/metaspace\/metablock.hpp\"\n@@ -31,0 +32,1 @@\n+#include \"memory\/metaspace\/metaspaceContext.hpp\"\n@@ -46,0 +48,1 @@\n+using metaspace::MetaBlock;\n@@ -47,0 +50,1 @@\n+using metaspace::MetaspaceContext;\n@@ -127,8 +131,6 @@\n-  MetaspaceArenaTestBed(ChunkManager* cm, const ArenaGrowthPolicy* alloc_sequence,\n-                        SizeAtomicCounter* used_words_counter, SizeRange allocation_range) :\n-    _arena(nullptr),\n-    _allocation_range(allocation_range),\n-    _size_of_last_failed_allocation(0),\n-    _allocations(nullptr),\n-    _alloc_count(),\n-    _dealloc_count()\n+  MetaspaceArenaTestBed(MetaspaceContext* context, const ArenaGrowthPolicy* growth_policy,\n+                        size_t allocation_alignment_words, SizeRange allocation_range)\n+    : _arena(nullptr)\n+    , _allocation_range(allocation_range)\n+    , _size_of_last_failed_allocation(0)\n+    , _allocations(nullptr)\n@@ -136,1 +138,1 @@\n-    _arena = new MetaspaceArena(cm, alloc_sequence, used_words_counter, \"gtest-MetaspaceArenaTestBed-sm\");\n+    _arena = new MetaspaceArena(context, growth_policy, Metaspace::min_allocation_alignment_words, \"gtest-MetaspaceArenaTestBed-sm\");\n@@ -166,3 +168,10 @@\n-    MetaWord* p = _arena->allocate(word_size);\n-    if (p != nullptr) {\n-      EXPECT_TRUE(is_aligned(p, AllocationAlignmentByteSize));\n+    MetaBlock wastage;\n+    MetaBlock bl = _arena->allocate(word_size, wastage);\n+    \/\/ We only expect wastage if either alignment was not met or the chunk remainder\n+    \/\/ was not large enough.\n+    if (wastage.is_nonempty()) {\n+      _arena->deallocate(wastage);\n+      wastage.reset();\n+    }\n+    if (bl.is_nonempty()) {\n+      EXPECT_TRUE(is_aligned(bl.base(), AllocationAlignmentByteSize));\n@@ -172,1 +181,1 @@\n-      a->p = p;\n+      a->p = bl.base();\n@@ -196,1 +205,1 @@\n-      _arena->deallocate(a->p, a->word_size);\n+      _arena->deallocate(MetaBlock(a->p, a->word_size));\n@@ -221,2 +230,2 @@\n-    MetaspaceArenaTestBed* bed = new MetaspaceArenaTestBed(&_context.cm(), growth_policy,\n-                                                       &_used_words_counter, allocation_range);\n+    MetaspaceArenaTestBed* bed = new MetaspaceArenaTestBed(_context.context(), growth_policy,\n+        Metaspace::min_allocation_alignment_words, allocation_range);\n","filename":"test\/hotspot\/gtest\/metaspace\/test_metaspacearena_stress.cpp","additions":25,"deletions":16,"binary":false,"changes":41,"status":"modified"},{"patch":"@@ -85,1 +85,17 @@\n-  if (UseCompressedClassPointers) {\n+  if (UseCompactObjectHeaders) {\n+    EXPECT_EQ(arrayOopDesc::base_offset_in_bytes(T_BOOLEAN), 12);\n+    EXPECT_EQ(arrayOopDesc::base_offset_in_bytes(T_BYTE),    12);\n+    EXPECT_EQ(arrayOopDesc::base_offset_in_bytes(T_SHORT),   12);\n+    EXPECT_EQ(arrayOopDesc::base_offset_in_bytes(T_CHAR),    12);\n+    EXPECT_EQ(arrayOopDesc::base_offset_in_bytes(T_INT),     12);\n+    EXPECT_EQ(arrayOopDesc::base_offset_in_bytes(T_FLOAT),   12);\n+    EXPECT_EQ(arrayOopDesc::base_offset_in_bytes(T_LONG),    16);\n+    EXPECT_EQ(arrayOopDesc::base_offset_in_bytes(T_DOUBLE),  16);\n+    if (UseCompressedOops) {\n+      EXPECT_EQ(arrayOopDesc::base_offset_in_bytes(T_OBJECT), 12);\n+      EXPECT_EQ(arrayOopDesc::base_offset_in_bytes(T_ARRAY),  12);\n+    } else {\n+      EXPECT_EQ(arrayOopDesc::base_offset_in_bytes(T_OBJECT), 16);\n+      EXPECT_EQ(arrayOopDesc::base_offset_in_bytes(T_ARRAY),  16);\n+    }\n+  } else if (UseCompressedClassPointers) {\n","filename":"test\/hotspot\/gtest\/oops\/test_arrayOop.cpp","additions":17,"deletions":1,"binary":false,"changes":18,"status":"modified"},{"patch":"@@ -31,1 +31,1 @@\n-    int objal; bool ccp; bool coops; int result;\n+    int objal; bool ccp; bool coops; bool coh; int result;\n@@ -33,1 +33,1 @@\n-\/\/    ObjAligInB, UseCCP, UseCoops, object size in heap words\n+\/\/    ObjAligInB, UseCCP, UseCoops, UseCOH, object size in heap words\n@@ -35,12 +35,18 @@\n-    { 8,          false,  false,    4 },  \/\/ 20 byte header, 8 byte oops\n-    { 8,          false,  true,     3 },  \/\/ 20 byte header, 4 byte oops\n-    { 8,          true,   false,    3 },  \/\/ 16 byte header, 8 byte oops\n-    { 8,          true,   true,     3 },  \/\/ 16 byte header, 4 byte oops\n-    { 16,         false,  false,    4 },  \/\/ 20 byte header, 8 byte oops, 16-byte align\n-    { 16,         false,  true,     4 },  \/\/ 20 byte header, 4 byte oops, 16-byte align\n-    { 16,         true,   false,    4 },  \/\/ 16 byte header, 8 byte oops, 16-byte align\n-    { 16,         true,   true,     4 },  \/\/ 16 byte header, 4 byte oops, 16-byte align\n-    { 256,        false,  false,    32 }, \/\/ 20 byte header, 8 byte oops, 256-byte align\n-    { 256,        false,  true,     32 }, \/\/ 20 byte header, 4 byte oops, 256-byte align\n-    { 256,        true,   false,    32 }, \/\/ 16 byte header, 8 byte oops, 256-byte align\n-    { 256,        true,   true,     32 }, \/\/ 16 byte header, 4 byte oops, 256-byte align\n+    { 8,          false,  false, false,   4 },  \/\/ 20 byte header, 8 byte oops\n+    { 8,          false,  true,  false,   3 },  \/\/ 20 byte header, 4 byte oops\n+    { 8,          true,   false, false,   3 },  \/\/ 16 byte header, 8 byte oops\n+    { 8,          true,   true,  false,   3 },  \/\/ 16 byte header, 4 byte oops\n+    { 8,          true,   false, true,    3 },  \/\/ 12 byte header, 8 byte oops\n+    { 8,          true,   true,  true,    2 },  \/\/ 12 byte header, 4 byte oops\n+    { 16,         false,  false, false,   4 },  \/\/ 20 byte header, 8 byte oops, 16-byte align\n+    { 16,         false,  true,  false,   4 },  \/\/ 20 byte header, 4 byte oops, 16-byte align\n+    { 16,         true,   false, false,   4 },  \/\/ 16 byte header, 8 byte oops, 16-byte align\n+    { 16,         true,   true,  false,   4 },  \/\/ 16 byte header, 4 byte oops, 16-byte align\n+    { 16,         true,   false, true,    4 },  \/\/ 12 byte header, 8 byte oops, 16-byte align\n+    { 16,         true,   true,  true,    2 },  \/\/ 12 byte header, 4 byte oops, 16-byte align\n+    { 256,        false,  false, false,  32 }, \/\/ 20 byte header, 8 byte oops, 256-byte align\n+    { 256,        false,  true,  false,  32 }, \/\/ 20 byte header, 4 byte oops, 256-byte align\n+    { 256,        true,   false, false,  32 }, \/\/ 16 byte header, 8 byte oops, 256-byte align\n+    { 256,        true,   true,  false,  32 }, \/\/ 16 byte header, 4 byte oops, 256-byte align\n+    { 256,        true,   false, true,   32 }, \/\/ 12 byte header, 8 byte oops, 256-byte align\n+    { 256,        true,   true,  true,   32 }, \/\/ 12 byte header, 4 byte oops, 256-byte align\n@@ -48,1 +54,1 @@\n-    { 8,          false,  false,    4 }, \/\/ 12 byte header, 4 byte oops, wordsize 4\n+    { 8,          false,  false, false,   4 }, \/\/ 12 byte header, 4 byte oops, wordsize 4\n@@ -50,1 +56,1 @@\n-    { -1,         false,  false,   -1 }\n+    { -1,         false,  false, false,  -1 }\n@@ -53,1 +59,2 @@\n-    if (x[i].objal == (int)ObjectAlignmentInBytes && x[i].ccp == UseCompressedClassPointers && x[i].coops == UseCompressedOops) {\n+    if (x[i].objal == (int)ObjectAlignmentInBytes && x[i].ccp == UseCompressedClassPointers && x[i].coops == UseCompressedOops &&\n+        x[i].coh == UseCompactObjectHeaders) {\n","filename":"test\/hotspot\/gtest\/oops\/test_objArrayOop.cpp","additions":24,"deletions":17,"binary":false,"changes":41,"status":"modified"},{"patch":"@@ -39,1 +39,5 @@\n-  o->set_klass(Universe::boolArrayKlass());\n+  if (UseCompactObjectHeaders) {\n+    o->set_mark(Universe::boolArrayKlass()->prototype_header());\n+  } else {\n+    o->set_klass(Universe::boolArrayKlass());\n+  }\n","filename":"test\/hotspot\/gtest\/oops\/test_typeArrayOop.cpp","additions":5,"deletions":1,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -150,1 +150,2 @@\n-    @IR(counts = { IRNode.LOAD_VECTOR_L, \">=1\", IRNode.STORE_VECTOR, \">=1\" })\n+    @IR(applyIf = {\"UseCompactObjectHeaders\", \"false\"},\n+        counts = { IRNode.LOAD_VECTOR_L, \">=1\", IRNode.STORE_VECTOR, \">=1\" })\n@@ -163,1 +164,2 @@\n-    @IR(counts = { IRNode.LOAD_VECTOR_L, \">=1\", IRNode.STORE_VECTOR, \">=1\" })\n+    @IR(applyIf = {\"UseCompactObjectHeaders\", \"false\"},\n+        counts = { IRNode.LOAD_VECTOR_L, \">=1\", IRNode.STORE_VECTOR, \">=1\" })\n@@ -176,1 +178,2 @@\n-    @IR(counts = { IRNode.LOAD_VECTOR_L, \">=1\", IRNode.STORE_VECTOR, \">=1\" })\n+    @IR(applyIf = {\"UseCompactObjectHeaders\", \"false\"},\n+        counts = { IRNode.LOAD_VECTOR_L, \">=1\", IRNode.STORE_VECTOR, \">=1\" })\n@@ -205,1 +208,2 @@\n-    @IR(counts = { IRNode.LOAD_VECTOR_L, \">=1\", IRNode.STORE_VECTOR, \">=1\" })\n+    @IR(applyIf = {\"UseCompactObjectHeaders\", \"false\"},\n+        counts = { IRNode.LOAD_VECTOR_L, \">=1\", IRNode.STORE_VECTOR, \">=1\" })\n@@ -219,1 +223,2 @@\n-    @IR(counts = { IRNode.LOAD_VECTOR_L, \">=1\", IRNode.STORE_VECTOR, \">=1\" })\n+    @IR(applyIf = {\"UseCompactObjectHeaders\", \"false\"},\n+        counts = { IRNode.LOAD_VECTOR_L, \">=1\", IRNode.STORE_VECTOR, \">=1\" })\n@@ -232,1 +237,2 @@\n-    @IR(counts = { IRNode.LOAD_VECTOR_L, \">=1\", IRNode.STORE_VECTOR, \">=1\" })\n+    @IR(applyIf = {\"UseCompactObjectHeaders\", \"false\"},\n+        counts = { IRNode.LOAD_VECTOR_L, \">=1\", IRNode.STORE_VECTOR, \">=1\" })\n","filename":"test\/hotspot\/jtreg\/compiler\/c2\/irTests\/TestVectorizationMismatchedAccess.java","additions":12,"deletions":6,"binary":false,"changes":18,"status":"modified"},{"patch":"@@ -54,1 +54,2 @@\n-    @IR(counts = { IRNode.LOAD_VECTOR_L, \">=1\", IRNode.STORE_VECTOR, \">=1\" })\n+    @IR(applyIf = {\"UseCompactObjectHeaders\", \"false\"},\n+        counts = { IRNode.LOAD_VECTOR_L, \">=1\", IRNode.STORE_VECTOR, \">=1\" })\n","filename":"test\/hotspot\/jtreg\/compiler\/c2\/irTests\/TestVectorizationNotRun.java","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -145,1 +145,2 @@\n-                    \"LogCompilation\"\n+                    \"LogCompilation\",\n+                    \"UseCompactObjectHeaders\"\n","filename":"test\/hotspot\/jtreg\/compiler\/lib\/ir_framework\/TestFramework.java","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -401,0 +401,1 @@\n+        applyIf = {\"UseCompactObjectHeaders\", \"false\"},\n@@ -709,1 +710,1 @@\n-        applyIf = {\"MaxVectorSize\", \">=16\"},\n+        applyIfAnd = {\"MaxVectorSize\", \">=16\", \"UseCompactObjectHeaders\", \"false\"},\n@@ -1004,0 +1005,1 @@\n+        applyIf = {\"UseCompactObjectHeaders\", \"false\"},\n@@ -1020,0 +1022,1 @@\n+        applyIf = {\"UseCompactObjectHeaders\", \"false\"},\n@@ -1040,0 +1043,1 @@\n+        applyIf = {\"UseCompactObjectHeaders\", \"false\"},\n@@ -1075,0 +1079,1 @@\n+        applyIf = {\"UseCompactObjectHeaders\", \"false\"},\n@@ -1091,0 +1096,1 @@\n+        applyIf = {\"UseCompactObjectHeaders\", \"false\"},\n@@ -1111,0 +1117,1 @@\n+        applyIf = {\"UseCompactObjectHeaders\", \"false\"},\n","filename":"test\/hotspot\/jtreg\/compiler\/loopopts\/superword\/TestAlignVector.java","additions":8,"deletions":1,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -271,1 +271,2 @@\n-    @IR(counts = {IRNode.ADD_VI, \"> 0\", IRNode.MUL_VI, \"> 0\", IRNode.ADD_VF, \"> 0\"},\n+    @IR(applyIf = {\"UseCompactObjectHeaders\", \"false\"},\n+        counts = {IRNode.ADD_VI, \"> 0\", IRNode.MUL_VI, \"> 0\", IRNode.ADD_VF, \"> 0\"},\n","filename":"test\/hotspot\/jtreg\/compiler\/loopopts\/superword\/TestIndependentPacksWithCyclicDependency.java","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -166,0 +166,1 @@\n+        applyIf = { \"UseCompactObjectHeaders\", \"false\" },\n@@ -168,1 +169,1 @@\n-        applyIf = {\"MaxVectorSize\", \"16\"}, \/\/ AD file requires vector_length = 16\n+        applyIfAnd = {\"MaxVectorSize\", \"16\", \"UseCompactObjectHeaders\", \"false\" }, \/\/ AD file requires vector_length = 16\n@@ -171,0 +172,1 @@\n+        applyIf = { \"UseCompactObjectHeaders\", \"false\" },\n@@ -184,0 +186,1 @@\n+        applyIf = { \"UseCompactObjectHeaders\", \"false\" },\n@@ -186,1 +189,1 @@\n-        applyIf = {\"MaxVectorSize\", \"16\"}, \/\/ AD file requires vector_length = 16\n+        applyIfAnd = {\"MaxVectorSize\", \"16\", \"UseCompactObjectHeaders\", \"false\" }, \/\/ AD file requires vector_length = 16\n@@ -189,0 +192,1 @@\n+        applyIf = { \"UseCompactObjectHeaders\", \"false\" },\n@@ -202,0 +206,1 @@\n+        applyIf = { \"UseCompactObjectHeaders\", \"false\" },\n@@ -204,1 +209,1 @@\n-        applyIf = {\"MaxVectorSize\", \"16\"}, \/\/ AD file requires vector_length = 16\n+        applyIfAnd = {\"MaxVectorSize\", \"16\", \"UseCompactObjectHeaders\", \"false\" }, \/\/ AD file requires vector_length = 16\n@@ -207,0 +212,1 @@\n+        applyIf = { \"UseCompactObjectHeaders\", \"false\" },\n@@ -220,0 +226,1 @@\n+        applyIf = { \"UseCompactObjectHeaders\", \"false\" },\n@@ -222,1 +229,1 @@\n-        applyIf = {\"MaxVectorSize\", \"16\"}, \/\/ AD file requires vector_length = 16\n+        applyIfAnd = {\"MaxVectorSize\", \"16\", \"UseCompactObjectHeaders\", \"false\" }, \/\/ AD file requires vector_length = 16\n@@ -225,0 +232,1 @@\n+        applyIf = { \"UseCompactObjectHeaders\", \"false\" },\n@@ -238,0 +246,1 @@\n+        applyIf = { \"UseCompactObjectHeaders\", \"false\" },\n@@ -240,1 +249,1 @@\n-        applyIf = {\"MaxVectorSize\", \"16\"}, \/\/ AD file requires vector_length = 16\n+        applyIfAnd = {\"MaxVectorSize\", \"16\", \"UseCompactObjectHeaders\", \"false\" }, \/\/ AD file requires vector_length = 16\n@@ -243,0 +252,1 @@\n+        applyIf = { \"UseCompactObjectHeaders\", \"false\" },\n","filename":"test\/hotspot\/jtreg\/compiler\/loopopts\/superword\/TestMulAddS2I.java","additions":15,"deletions":5,"binary":false,"changes":20,"status":"modified"},{"patch":"@@ -262,1 +262,0 @@\n-        new LogMessageWithLevel(\"Restore Preserved Marks \\\\(ms\\\\):\", Level.DEBUG),\n","filename":"test\/hotspot\/jtreg\/gc\/g1\/TestGCLogMessages.java","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -35,1 +35,1 @@\n- * @run main\/timeout=240 gc.g1.plab.TestPLABPromotion\n+ * @run main\/othervm\/timeout=240 -Xbootclasspath\/a:. -XX:+UnlockDiagnosticVMOptions -XX:+WhiteBoxAPI gc.g1.plab.TestPLABPromotion\n@@ -51,0 +51,1 @@\n+import jdk.test.whitebox.WhiteBox;\n@@ -57,0 +58,2 @@\n+    private static final boolean COMPACT_HEADERS = Platform.is64bit() && WhiteBox.getWhiteBox().getBooleanVMFlag(\"UseCompactObjectHeaders\");\n+\n@@ -77,1 +80,1 @@\n-    private static final int OBJECT_SIZE_HIGH   = 3072 * HEAP_WORD_SIZE;\n+    private static final int OBJECT_SIZE_HIGH   = (COMPACT_HEADERS ? 3266 : 3250) * HEAP_WORD_SIZE;\n","filename":"test\/hotspot\/jtreg\/gc\/g1\/plab\/TestPLABPromotion.java","additions":5,"deletions":2,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -52,0 +52,11 @@\n+\n+\/* @test id=UseCompactObjectHeaders\n+ * @summary Run metaspace-related gtests with tiny classpointers\n+ * @library \/test\/lib\n+ * @modules java.base\/jdk.internal.misc\n+ *          java.xml\n+ * @requires vm.bits == 64\n+ * @requires vm.flagless\n+ * @requires vm.debug\n+ * @run main\/native GTestWrapper --gtest_filter=metaspace* -XX:+UnlockExperimentalVMOptions -XX:+UseCompactObjectHeaders\n+ *\/\n","filename":"test\/hotspot\/jtreg\/gtest\/MetaspaceGtests.java","additions":11,"deletions":0,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -1,40 +0,0 @@\n-\/*\n- * Copyright (C) 2021 THL A29 Limited, a Tencent company. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\n- *\/\n-\n-\/*\n- * Note: This runs the metaspace utils related parts of gtest in configurations which\n- *  are not tested explicitly in the standard gtests.\n- *\n- *\/\n-\n-\/* @test\n- * @bug 8264008\n- * @summary Run metaspace utils related gtests with compressed class pointers off\n- * @requires vm.bits == 64\n- * @library \/test\/lib\n- * @modules java.base\/jdk.internal.misc\n- *          java.xml\n- * @requires vm.flagless\n- * @run main\/native GTestWrapper --gtest_filter=MetaspaceUtils* -XX:-UseCompressedClassPointers\n- *\/\n","filename":"test\/hotspot\/jtreg\/gtest\/MetaspaceUtilsGtests.java","additions":0,"deletions":40,"binary":false,"changes":40,"status":"modified"},{"patch":"@@ -59,0 +59,1 @@\n+                \"-XX:+UnlockExperimentalVMOptions\", \"-XX:-UseCompactObjectHeaders\",\n","filename":"test\/hotspot\/jtreg\/runtime\/CompressedOops\/CompressedCPUSpecificClassSpaceReservation.java","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -49,0 +49,1 @@\n+    static final String usesCompactObjectHeadersPat = \"UseCompactObjectHeaders 1\";\n@@ -60,0 +61,5 @@\n+    \/\/ Returns true if the output indicates that the VM uses compact object headers\n+    static boolean usesCompactObjectHeaders(OutputAnalyzer output) {\n+        return output.getOutput().contains(usesCompactObjectHeadersPat);\n+    }\n+\n@@ -224,1 +230,1 @@\n-        if (!isCCSReservedAnywhere(output)) {\n+        if (!isCCSReservedAnywhere(output) && !usesCompactObjectHeaders(output)) {\n@@ -242,1 +248,1 @@\n-        if (!isCCSReservedAnywhere(output)) {\n+        if (!isCCSReservedAnywhere(output) && !usesCompactObjectHeaders(output)) {\n@@ -245,1 +251,1 @@\n-        if (!Platform.isAArch64() && !Platform.isPPC()) {\n+        if (!Platform.isAArch64()  && !usesCompactObjectHeaders(output) && !Platform.isPPC()) {\n@@ -264,1 +270,1 @@\n-        if (!isCCSReservedAnywhere(output)) {\n+        if (!isCCSReservedAnywhere(output) && !usesCompactObjectHeaders(output)) {\n@@ -267,1 +273,1 @@\n-        if (!Platform.isAArch64() && !Platform.isPPC()) {\n+        if (!Platform.isAArch64()  && !usesCompactObjectHeaders(output) && !Platform.isPPC()) {\n","filename":"test\/hotspot\/jtreg\/runtime\/CompressedOops\/CompressedClassPointers.java","additions":11,"deletions":5,"binary":false,"changes":16,"status":"modified"},{"patch":"@@ -45,1 +45,1 @@\n-    private static void test(long forceAddress, long classSpaceSize, long expectedEncodingBase, int expectedEncodingShift) throws IOException {\n+    private static void test(long forceAddress, boolean COH, long classSpaceSize, long expectedEncodingBase, int expectedEncodingShift) throws IOException {\n@@ -52,0 +52,3 @@\n+                \"-XX:+UnlockExperimentalVMOptions\",\n+                \"-XX:\" + (COH ? \"+\" : \"-\") + \"UseCompactObjectHeaders\",\n+                \"-XX:\" + (COH ? \"+\" : \"-\") + \"UseObjectMonitorTable\",\n@@ -63,1 +66,2 @@\n-            throw new SkippedException(\"Skipping because we cannot force ccs to \" + forceAddressString);\n+            System.out.println(\"Skipping because we cannot force ccs to \" + forceAddressString);\n+            return;\n@@ -76,1 +80,1 @@\n-        test(4 * G - 128 * M, 128 * M, 0, 0);\n+        test(4 * G - 128 * M, false, 128 * M, 0, 0);\n@@ -78,0 +82,21 @@\n+        \/\/ Test ccs nestling right at the end of the 32G range\n+        \/\/ Expecting:\n+        \/\/ - non-aarch64: base=0, shift=3\n+        \/\/ - aarch64: base to start of class range, shift 0\n+        if (Platform.isAArch64()) {\n+            \/\/ The best we can do on aarch64 is to be *near* the end of the 32g range, since a valid encoding base\n+            \/\/ on aarch64 must be 4G aligned, and the max. class space size is 3G.\n+            long forceAddress = 0x7_0000_0000L; \/\/ 28g, and also a valid EOR immediate\n+            test(forceAddress, false, 3 * G, forceAddress, 0);\n+        } else {\n+            test(32 * G - 128 * M, false, 128 * M, 0, 3);\n+        }\n+\n+        \/\/ Test ccs starting *below* 4G, but extending upwards beyond 4G. All platforms except aarch64 should pick\n+        \/\/ zero based encoding.\n+        if (Platform.isAArch64()) {\n+            long forceAddress = 0xC000_0000L; \/\/ make sure we have a valid EOR immediate\n+            test(forceAddress, false, G + (128 * M), forceAddress, 0);\n+        } else {\n+            test(4 * G - 128 * M, false, 2 * 128 * M, 0, 3);\n+        }\n@@ -80,0 +105,19 @@\n+        \/\/ Compact Object Header Mode with tiny classpointers\n+        \/\/ On all platforms we expect the VM to chose the smallest possible shift value needed to cover the encoding range\n+        long forceAddress = 30 * G;\n+\n+        long ccsSize = 128 * M;\n+        int expectedShift = 6;\n+        test(forceAddress, true, ccsSize, forceAddress, expectedShift);\n+\n+        ccsSize = 512 * M;\n+        expectedShift = 8;\n+        test(forceAddress, true, ccsSize, forceAddress, expectedShift);\n+\n+        ccsSize = G;\n+        expectedShift = 9;\n+        test(forceAddress, true, ccsSize, forceAddress, expectedShift);\n+\n+        ccsSize = 3 * G;\n+        expectedShift = 10;\n+        test(forceAddress, true, ccsSize, forceAddress, expectedShift);\n","filename":"test\/hotspot\/jtreg\/runtime\/CompressedOops\/CompressedClassPointersEncodingScheme.java","additions":47,"deletions":3,"binary":false,"changes":50,"status":"modified"},{"patch":"@@ -1,113 +0,0 @@\n-\/*\n- * Copyright Amazon.com Inc. or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-\/*\n- * @test id=with-coops-no-ccp\n- * @library \/test\/lib\n- * @requires vm.bits == \"64\"\n- * @modules java.base\/jdk.internal.misc\n- * @run main\/othervm -XX:+UseCompressedOops -XX:-UseCompressedClassPointers ArrayBaseOffsets\n- *\/\n-\/*\n- * @test id=with-coops-with-ccp\n- * @library \/test\/lib\n- * @requires vm.bits == \"64\"\n- * @requires vm.opt.UseCompressedClassPointers != false\n- * @modules java.base\/jdk.internal.misc\n- * @run main\/othervm -XX:+UseCompressedOops -XX:+UseCompressedClassPointers ArrayBaseOffsets\n- *\/\n-\/*\n- * @test id=no-coops-no-ccp\n- * @library \/test\/lib\n- * @requires vm.bits == \"64\"\n- * @modules java.base\/jdk.internal.misc\n- * @run main\/othervm -XX:-UseCompressedOops -XX:-UseCompressedClassPointers ArrayBaseOffsets\n- *\/\n-\/*\n- * @test id=no-coops-with-ccp\n- * @library \/test\/lib\n- * @requires vm.bits == \"64\"\n- * @requires vm.opt.UseCompressedClassPointers != false\n- * @modules java.base\/jdk.internal.misc\n- * @run main\/othervm -XX:-UseCompressedOops -XX:+UseCompressedClassPointers ArrayBaseOffsets\n- *\/\n-\/*\n- * @test id=32bit\n- * @library \/test\/lib\n- * @requires vm.bits == \"32\"\n- * @modules java.base\/jdk.internal.misc\n- * @run main\/othervm ArrayBaseOffsets\n- *\/\n-\n-import jdk.internal.misc.Unsafe;\n-\n-import java.lang.management.ManagementFactory;\n-import java.lang.management.RuntimeMXBean;\n-import java.util.List;\n-\n-import jdk.test.lib.Asserts;\n-import jdk.test.lib.Platform;\n-\n-public class ArrayBaseOffsets {\n-\n-    private static final boolean COOP;\n-    private static final boolean CCP;\n-\n-    static {\n-        if (Platform.is64bit()) {\n-            RuntimeMXBean runtime = ManagementFactory.getRuntimeMXBean();\n-            List<String> vmargs = runtime.getInputArguments();\n-            CCP = !vmargs.contains(\"-XX:-UseCompressedClassPointers\");\n-            COOP = System.getProperty(\"java.vm.compressedOopsMode\") != null;\n-        } else {\n-            COOP = CCP = false;\n-        }\n-    }\n-\n-    static public void main(String[] args) {\n-        Unsafe unsafe = Unsafe.getUnsafe();\n-        int intOffset, longOffset;\n-        if (Platform.is64bit()) {\n-            if (CCP) {\n-                intOffset = 16;\n-                longOffset = 16;\n-            } else {\n-                intOffset = 20;\n-                longOffset = 24;\n-            }\n-        } else {\n-            intOffset = 12;\n-            longOffset = 16;\n-        }\n-        Asserts.assertEquals(unsafe.arrayBaseOffset(boolean[].class), intOffset,  \"Misplaced boolean array base\");\n-        Asserts.assertEquals(unsafe.arrayBaseOffset(byte[].class),    intOffset,  \"Misplaced byte    array base\");\n-        Asserts.assertEquals(unsafe.arrayBaseOffset(char[].class),    intOffset,  \"Misplaced char    array base\");\n-        Asserts.assertEquals(unsafe.arrayBaseOffset(short[].class),   intOffset,  \"Misplaced short   array base\");\n-        Asserts.assertEquals(unsafe.arrayBaseOffset(int[].class),     intOffset,  \"Misplaced int     array base\");\n-        Asserts.assertEquals(unsafe.arrayBaseOffset(long[].class),    longOffset, \"Misplaced long    array base\");\n-        Asserts.assertEquals(unsafe.arrayBaseOffset(float[].class),   intOffset,  \"Misplaced float   array base\");\n-        Asserts.assertEquals(unsafe.arrayBaseOffset(double[].class),  longOffset, \"Misplaced double  array base\");\n-        int expectedObjArrayOffset = (COOP || !Platform.is64bit()) ? intOffset : longOffset;\n-        Asserts.assertEquals(unsafe.arrayBaseOffset(Object[].class),  expectedObjArrayOffset, \"Misplaced object  array base\");\n-    }\n-}\n","filename":"test\/hotspot\/jtreg\/runtime\/FieldLayout\/ArrayBaseOffsets.java","additions":0,"deletions":113,"binary":false,"changes":113,"status":"modified"},{"patch":"@@ -0,0 +1,157 @@\n+\/*\n+ * Copyright (c) 2022, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+\/*\n+ * @test id=with-coops-with-ccp\n+ * @library \/test\/lib \/\n+ * @requires vm.bits == \"64\"\n+ * @modules java.base\/jdk.internal.misc\n+ *          java.management\n+ * @build jdk.test.whitebox.WhiteBox\n+ * @run driver jdk.test.lib.helpers.ClassFileInstaller jdk.test.whitebox.WhiteBox\n+ * @run main\/othervm -Xbootclasspath\/a:. -XX:+UnlockDiagnosticVMOptions -XX:+WhiteBoxAPI -XX:+UnlockExperimentalVMOptions -XX:+UseCompressedOops -XX:+UseCompressedClassPointers -XX:-UseCompactObjectHeaders BaseOffsets\n+ *\/\n+\/*\n+ * @test id=no-coops-with-ccp\n+ * @library \/test\/lib \/\n+ * @requires vm.bits == \"64\"\n+ * @modules java.base\/jdk.internal.misc\n+ *          java.management\n+ * @build jdk.test.whitebox.WhiteBox\n+ * @run driver jdk.test.lib.helpers.ClassFileInstaller jdk.test.whitebox.WhiteBox\n+ * @run main\/othervm -Xbootclasspath\/a:. -XX:+UnlockDiagnosticVMOptions -XX:+WhiteBoxAPI -XX:+UnlockExperimentalVMOptions -XX:-UseCompressedOops -XX:+UseCompressedClassPointers -XX:-UseCompactObjectHeaders BaseOffsets\n+ *\/\n+\/*\n+ * @test id=with-coops-no-ccp\n+ * @library \/test\/lib \/\n+ * @requires vm.bits == \"64\"\n+ * @modules java.base\/jdk.internal.misc\n+ *          java.management\n+ * @build jdk.test.whitebox.WhiteBox\n+ * @run driver jdk.test.lib.helpers.ClassFileInstaller jdk.test.whitebox.WhiteBox\n+ * @run main\/othervm -Xbootclasspath\/a:. -XX:+UnlockDiagnosticVMOptions -XX:+WhiteBoxAPI -XX:+UnlockExperimentalVMOptions -XX:+UseCompressedOops -XX:-UseCompressedClassPointers -XX:-UseCompactObjectHeaders BaseOffsets\n+ *\/\n+\/*\n+ * @test id=no-coops-no-ccp\n+ * @library \/test\/lib \/\n+ * @requires vm.bits == \"64\"\n+ * @modules java.base\/jdk.internal.misc\n+ *          java.management\n+ * @build jdk.test.whitebox.WhiteBox\n+ * @run driver jdk.test.lib.helpers.ClassFileInstaller jdk.test.whitebox.WhiteBox\n+ * @run main\/othervm -Xbootclasspath\/a:. -XX:+UnlockDiagnosticVMOptions -XX:+WhiteBoxAPI -XX:+UnlockExperimentalVMOptions -XX:-UseCompressedOops -XX:-UseCompressedClassPointers -XX:-UseCompactObjectHeaders BaseOffsets\n+ *\/\n+\/*\n+ * @test id=with-coop--with-coh\n+ * @library \/test\/lib \/\n+ * @requires vm.bits == \"64\"\n+ * @modules java.base\/jdk.internal.misc\n+ *          java.management\n+ * @build jdk.test.whitebox.WhiteBox\n+ * @run driver jdk.test.lib.helpers.ClassFileInstaller jdk.test.whitebox.WhiteBox\n+ * @run main\/othervm -Xbootclasspath\/a:. -XX:+UnlockDiagnosticVMOptions -XX:+WhiteBoxAPI -XX:+UnlockExperimentalVMOptions -XX:+UseCompressedOops -XX:+UseCompactObjectHeaders BaseOffsets\n+ *\/\n+\/*\n+ * @test id=no-coops-with-coh\n+ * @library \/test\/lib \/\n+ * @requires vm.bits == \"64\"\n+ * @modules java.base\/jdk.internal.misc\n+ *          java.management\n+ * @build jdk.test.whitebox.WhiteBox\n+ * @run driver jdk.test.lib.helpers.ClassFileInstaller jdk.test.whitebox.WhiteBox\n+ * @run main\/othervm -Xbootclasspath\/a:. -XX:+UnlockDiagnosticVMOptions -XX:+WhiteBoxAPI -XX:+UnlockExperimentalVMOptions -XX:-UseCompressedOops -XX:+UseCompactObjectHeaders BaseOffsets\n+ *\/\n+\/*\n+ * @test id=32bit\n+ * @library \/test\/lib \/\n+ * @requires vm.bits == \"32\"\n+ * @modules java.base\/jdk.internal.misc\n+ *          java.management\n+ * @build jdk.test.whitebox.WhiteBox\n+ * @run driver jdk.test.lib.helpers.ClassFileInstaller jdk.test.whitebox.WhiteBox\n+ * @run main\/othervm -Xbootclasspath\/a:. -XX:+UnlockDiagnosticVMOptions -XX:+WhiteBoxAPI BaseOffsets\n+ *\/\n+\n+import java.lang.reflect.Field;\n+import java.util.Arrays;\n+import java.util.Comparator;\n+import jdk.internal.misc.Unsafe;\n+\n+import jdk.test.lib.Asserts;\n+import jdk.test.lib.Platform;\n+import jdk.test.whitebox.WhiteBox;\n+\n+public class BaseOffsets {\n+\n+    static class LIClass {\n+        public int i;\n+    }\n+\n+    public static final WhiteBox WB = WhiteBox.getWhiteBox();\n+\n+    static final long INT_OFFSET;\n+    static final int  INT_ARRAY_OFFSET;\n+    static final int  LONG_ARRAY_OFFSET;\n+    static {\n+        if (!Platform.is64bit() || WB.getBooleanVMFlag(\"UseCompactObjectHeaders\")) {\n+            INT_OFFSET = 8;\n+            INT_ARRAY_OFFSET = 12;\n+            LONG_ARRAY_OFFSET = 16;\n+        } else if (WB.getBooleanVMFlag(\"UseCompressedClassPointers\")) {\n+            INT_OFFSET = 12;\n+            INT_ARRAY_OFFSET = 16;\n+            LONG_ARRAY_OFFSET = 16;\n+        } else {\n+            INT_OFFSET = 16;\n+            INT_ARRAY_OFFSET = 20;\n+            LONG_ARRAY_OFFSET = 24;\n+        }\n+    }\n+\n+    static public void main(String[] args) {\n+        Unsafe unsafe = Unsafe.getUnsafe();\n+        Class c = LIClass.class;\n+        Field[] fields = c.getFields();\n+        for (int i = 0; i < fields.length; i++) {\n+            long offset = unsafe.objectFieldOffset(fields[i]);\n+            if (fields[i].getType() == int.class) {\n+                Asserts.assertEquals(offset, INT_OFFSET, \"Misplaced int field\");\n+            } else {\n+                Asserts.fail(\"Unexpected field type\");\n+            }\n+        }\n+\n+        Asserts.assertEquals(unsafe.arrayBaseOffset(boolean[].class), INT_ARRAY_OFFSET,  \"Misplaced boolean array base\");\n+        Asserts.assertEquals(unsafe.arrayBaseOffset(byte[].class),    INT_ARRAY_OFFSET,  \"Misplaced byte    array base\");\n+        Asserts.assertEquals(unsafe.arrayBaseOffset(char[].class),    INT_ARRAY_OFFSET,  \"Misplaced char    array base\");\n+        Asserts.assertEquals(unsafe.arrayBaseOffset(short[].class),   INT_ARRAY_OFFSET,  \"Misplaced short   array base\");\n+        Asserts.assertEquals(unsafe.arrayBaseOffset(int[].class),     INT_ARRAY_OFFSET,  \"Misplaced int     array base\");\n+        Asserts.assertEquals(unsafe.arrayBaseOffset(long[].class),    LONG_ARRAY_OFFSET, \"Misplaced long    array base\");\n+        Asserts.assertEquals(unsafe.arrayBaseOffset(float[].class),   INT_ARRAY_OFFSET,  \"Misplaced float   array base\");\n+        Asserts.assertEquals(unsafe.arrayBaseOffset(double[].class),  LONG_ARRAY_OFFSET, \"Misplaced double  array base\");\n+        boolean narrowOops = System.getProperty(\"java.vm.compressedOopsMode\") != null ||\n+                             !Platform.is64bit();\n+        int expected_objary_offset = narrowOops ? INT_ARRAY_OFFSET : LONG_ARRAY_OFFSET;\n+        Asserts.assertEquals(unsafe.arrayBaseOffset(Object[].class),  expected_objary_offset, \"Misplaced object  array base\");\n+    }\n+}\n","filename":"test\/hotspot\/jtreg\/runtime\/FieldLayout\/BaseOffsets.java","additions":157,"deletions":0,"binary":false,"changes":157,"status":"added"},{"patch":"@@ -0,0 +1,122 @@\n+\/*\n+ * Copyright (c) 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2024, Red Hat, Inc. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+\/**\n+ * @test id=nocoops_nocoh\n+ * @summary Test Loading of default archives in all configurations\n+ * @requires vm.cds\n+ * @requires vm.bits == 64\n+ * @library \/test\/lib\n+ * @modules java.base\/jdk.internal.misc\n+ *          java.management\n+ * @run driver TestDefaultArchiveLoading nocoops_nocoh\n+ *\/\n+\n+\/**\n+ * @test id=nocoops_coh\n+ * @summary Test Loading of default archives in all configurations (requires --enable-cds-archive-coh)\n+ * @requires vm.cds\n+ * @requires vm.bits == 64\n+ * @requires !vm.gc.ZGenerational\n+ * @library \/test\/lib\n+ * @modules java.base\/jdk.internal.misc\n+ *          java.management\n+ * @run driver TestDefaultArchiveLoading nocoops_coh\n+ *\/\n+\n+\/**\n+ * @test id=coops_nocoh\n+ * @summary Test Loading of default archives in all configurations\n+ * @requires vm.cds\n+ * @requires vm.bits == 64\n+ * @library \/test\/lib\n+ * @modules java.base\/jdk.internal.misc\n+ *          java.management\n+ * @run driver TestDefaultArchiveLoading coops_nocoh\n+ *\/\n+\n+\/**\n+ * @test id=coops_coh\n+ * @summary Test Loading of default archives in all configurations (requires --enable-cds-archive-coh)\n+ * @requires vm.cds\n+ * @requires vm.bits == 64\n+ * @requires !vm.gc.ZGenerational\n+ * @library \/test\/lib\n+ * @modules java.base\/jdk.internal.misc\n+ *          java.management\n+ * @run driver TestDefaultArchiveLoading coops_coh\n+ *\/\n+\n+import jdk.test.lib.Platform;\n+import jdk.test.lib.process.OutputAnalyzer;\n+import jdk.test.lib.process.ProcessTools;\n+import jtreg.SkippedException;\n+\n+public class TestDefaultArchiveLoading {\n+    public static void main(String[] args) throws Exception {\n+\n+        if (args.length != 1) {\n+            throw new RuntimeException(\"Expected argument\");\n+        }\n+\n+        String archiveSuffix;\n+        char coh, coops;\n+\n+        switch (args[0]) {\n+            case \"nocoops_nocoh\":\n+                coh = coops = '-';\n+                archiveSuffix = \"_nocoops\";\n+                break;\n+            case \"nocoops_coh\":\n+                coops = '-';\n+                coh = '+';\n+                archiveSuffix = \"_nocoops_coh\";\n+                break;\n+            case \"coops_nocoh\":\n+                coops = '+';\n+                coh = '-';\n+                archiveSuffix = \"\";\n+                break;\n+            case \"coops_coh\":\n+                coh = coops = '+';\n+                archiveSuffix = \"_coh\";\n+                break;\n+            default: throw new RuntimeException(\"Invalid argument \" + args[0]);\n+        }\n+\n+        ProcessBuilder pb = ProcessTools.createLimitedTestJavaProcessBuilder(\n+                \"-XX:+UnlockExperimentalVMOptions\",\n+                \"-XX:\" + coh + \"UseCompactObjectHeaders\",\n+                \"-XX:\" + coops + \"UseCompressedOops\",\n+                \"-Xlog:cds\",\n+                \"-Xshare:on\", \/\/ fail if we cannot load archive\n+                \"-version\");\n+\n+        OutputAnalyzer output = new OutputAnalyzer(pb.start());\n+        output.shouldHaveExitValue(0);\n+\n+        output.shouldContain(\"classes\" + archiveSuffix + \".jsa\");\n+\n+    }\n+}\n","filename":"test\/hotspot\/jtreg\/runtime\/cds\/TestDefaultArchiveLoading.java","additions":122,"deletions":0,"binary":false,"changes":122,"status":"added"},{"patch":"@@ -59,0 +59,1 @@\n+         String compactHeaders = \"-XX:\" + (zGenerational.equals(\"-XX:+ZGenerational\") ? \"+\" : \"-\") + \"UseCompactObjectHeaders\";\n@@ -66,0 +67,2 @@\n+                                        \"-XX:+UnlockExperimentalVMOptions\",\n+                                        compactHeaders,\n@@ -75,0 +78,2 @@\n+                         \"-XX:+UnlockExperimentalVMOptions\",\n+                         compactHeaders,\n@@ -86,0 +91,2 @@\n+                         \"-XX:+UnlockExperimentalVMOptions\",\n+                         compactHeaders,\n@@ -98,0 +105,2 @@\n+                         \"-XX:+UnlockExperimentalVMOptions\",\n+                         compactHeaders,\n@@ -110,0 +119,2 @@\n+                         \"-XX:+UnlockExperimentalVMOptions\",\n+                         compactHeaders,\n@@ -121,0 +132,2 @@\n+                         \"-XX:+UnlockExperimentalVMOptions\",\n+                         compactHeaders,\n@@ -133,0 +146,2 @@\n+                         \"-XX:+UnlockExperimentalVMOptions\",\n+                         compactHeaders,\n@@ -146,0 +161,2 @@\n+                         \"-XX:+UnlockExperimentalVMOptions\",\n+                         compactHeaders,\n@@ -155,0 +172,2 @@\n+                         \"-XX:+UnlockExperimentalVMOptions\",\n+                         compactHeaders,\n","filename":"test\/hotspot\/jtreg\/runtime\/cds\/appcds\/TestZGCWithCDS.java","additions":19,"deletions":0,"binary":false,"changes":19,"status":"modified"},{"patch":"@@ -170,1 +170,1 @@\n-                                                \"-XX:+UseZGC\", zGenerational, \"-XX:ZCollectionInterval=0.01\",\n+                                                \"-XX:+UseZGC\", zGenerational, \"-XX:ZCollectionInterval=0.01\", \"-XX:+UnlockExperimentalVMOptions\", \"-XX:-UseCompactObjectHeaders\",\n","filename":"test\/hotspot\/jtreg\/runtime\/cds\/appcds\/loaderConstraints\/DynamicLoaderConstraintsTest.java","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -304,0 +304,1 @@\n+    private static final boolean COMPACT_HEADERS = Platform.is64bit() && WhiteBox.getWhiteBox().getBooleanVMFlag(\"UseCompactObjectHeaders\");\n@@ -377,0 +378,10 @@\n+    private static long expectedSmallObjSize() {\n+        long size;\n+        if (!Platform.is64bit() || COMPACT_HEADERS) {\n+            size = 8;\n+        } else {\n+            size = 16;\n+        }\n+        return roundUp(size, OBJ_ALIGN);\n+    }\n+\n@@ -378,1 +389,1 @@\n-        long expected = roundUp(Platform.is64bit() ? 16 : 8, OBJ_ALIGN);\n+        long expected = expectedSmallObjSize();\n@@ -385,1 +396,1 @@\n-        long expected = roundUp(Platform.is64bit() ? 16 : 8, OBJ_ALIGN);\n+        long expected = expectedSmallObjSize();\n@@ -395,1 +406,1 @@\n-        long expected = roundUp(Platform.is64bit() ? 16 : 8, OBJ_ALIGN);\n+        long expected = expectedSmallObjSize();\n","filename":"test\/jdk\/java\/lang\/instrument\/GetObjectSizeIntrinsicsTest.java","additions":14,"deletions":3,"binary":false,"changes":17,"status":"modified"}]}