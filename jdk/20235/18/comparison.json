{"files":[{"patch":"@@ -549,0 +549,155 @@\n+\/\/ Generate stub for ghash process blocks.\n+\/\/\n+\/\/ Arguments for generated stub:\n+\/\/      state:    R3_ARG1 (long[] state)\n+\/\/      subkeyH:  R4_ARG2 (long[] subH)\n+\/\/      data:     R5_ARG3 (byte[] data)\n+\/\/      blocks:   R6_ARG4 (number of 16-byte blocks to process)\n+\/\/\n+\/\/ The polynomials are processed in bit-reflected order for efficiency reasons.\n+\/\/ This optimization leverages the structure of the Galois field arithmetic\n+\/\/ to minimize the number of bit manipulations required during multiplication.\n+\/\/ For an explanation of how this works, refer :\n+\/\/ Vinodh Gopal, Erdinc Ozturk, Wajdi Feghali, Jim Guilford, Gil Wolrich,\n+\/\/ Martin Dixon. \"Optimized Galois-Counter-Mode Implementation on Intel®\n+\/\/ Architecture Processor\"\n+\/\/ http:\/\/web.archive.org\/web\/20130609111954\/http:\/\/www.intel.com\/content\/dam\/www\/public\/us\/en\/documents\/white-papers\/communications-ia-galois-counter-mode-paper.pdf\n+\/\/\n+\/\/\n+address generate_ghash_processBlocks() {\n+  StubCodeMark mark(this, \"StubRoutines\", \"ghash\");\n+  address start = __ function_entry();\n+\n+  \/\/ Registers for parameters\n+  Register state = R3_ARG1;                     \/\/ long[] state\n+  Register subkeyH = R4_ARG2;                   \/\/ long[] subH\n+  Register data = R5_ARG3;                      \/\/ byte[] data\n+  Register blocks = R6_ARG4;\n+  Register temp1 = R8;\n+  Register temp2 = R9;\n+  Register temp3 = R10;\n+  Register temp4 = R11;\n+  Register align = data;\n+  Register load = R12;\n+  \/\/ Vector Registers\n+  VectorRegister vZero = VR0;\n+  VectorRegister vH = VR1;\n+  VectorRegister vLowerH = VR2;\n+  VectorRegister vHigherH = VR3;\n+  VectorRegister vTmp4 = VR4;\n+  VectorRegister vTmp5 = VR5;\n+  VectorRegister vTmp6 = VR6;\n+  VectorRegister vTmp7 = VR7;\n+  VectorRegister vTmp8 = VR8;\n+  VectorRegister vTmp9 = VR9;\n+  VectorRegister vTmp10 = VR10;\n+  VectorRegister vTmp11 = VR11;\n+  VectorRegister vTmp12 = VR12;\n+  VectorRegister loadOrder = VR13;\n+  VectorRegister vHigh = VR14;\n+  VectorRegister vLow = VR15;\n+  VectorRegister vState = VR16;\n+  VectorRegister vPerm = VR17;\n+  VectorRegister vConstC2 = VR19;\n+  Label L_end, L_aligned, L_error;\n+\n+  __ li(temp1, 0xc2);\n+  __ sldi(temp1, temp1, 56);\n+  __ vspltisb(vZero, 0);\n+  __ mtvrd(vConstC2, temp1);\n+  __ lxvd2x(vH->to_vsr(), subkeyH);\n+  __ lxvd2x(vState->to_vsr(), state);\n+  \/\/ Operations to obtain lower and higher bytes of subkey H.\n+  __ vspltisb(vTmp7, 1);\n+  __ vspltisb(vTmp10, 7);\n+  __ vsldoi(vTmp8, vZero, vTmp7, 1);            \/\/ 0x1\n+  __ vor(vTmp8, vConstC2, vTmp8);               \/\/ 0xC2...1\n+  __ vsplt(vTmp9, 0, vH);                       \/\/ MSB of H\n+  __ vsl(vH, vH, vTmp7);                        \/\/ Carry = H<<7\n+  __ vsrab(vTmp9, vTmp9, vTmp10);\n+  __ vand(vTmp9, vTmp9, vTmp8);                 \/\/ Carry\n+  __ vxor(vTmp10, vH, vTmp9);\n+  __ vsldoi(vConstC2, vZero, vConstC2, 8);\n+  __ vsldoi(vTmp11, vTmp10, vTmp10, 8);         \/\/ swap Lower and Higher Halves of subkey H\n+  __ vsldoi(vLowerH, vZero, vTmp11, 8);         \/\/ H.L\n+  __ vsldoi(vHigherH, vTmp11, vZero, 8);        \/\/ H.H\n+  #ifdef ASSERT\n+    __ cmpwi(CR0, blocks, 0);\n+    __ beq(CR0, L_error);\n+  #endif\n+  __ clrldi(blocks, blocks, 32);\n+  __ mtctr(blocks);\n+  __ li(temp1, 0);\n+  __ lvsl(loadOrder, temp1);\n+  #ifdef VM_LITTLE_ENDIAN\n+    __ vspltisb(vTmp12, 0xf);\n+    __ vxor(loadOrder, loadOrder, vTmp12);\n+  #endif\n+  \/\/ This code performs Karatsuba multiplication in Galois fields to compute the GHASH operation.\n+  \/\/\n+  \/\/ The Karatsuba method breaks the multiplication of two 128-bit numbers into smaller parts,\n+  \/\/ performing three 128-bit multiplications and combining the results efficiently.\n+  \/\/\n+  \/\/ (C1:C0) = A1*B1, (D1:D0) = A0*B0, (E1:E0) = (A0+A1)(B0+B1)\n+  \/\/ (A1:A0)(B1:B0) = C1:(C0+C1+D1+E1):(D1+C0+D0+E0):D0\n+  \/\/\n+  \/\/ Inputs:\n+  \/\/ - vH:       The data vector (state), containing both B0 (lower half) and B1 (higher half).\n+  \/\/ - vLowerH:  Lower half of the subkey H (A0).\n+  \/\/ - vHigherH: Higher half of the subkey H (A1).\n+  \/\/ - vConstC2: Constant used for reduction (for final processing).\n+  \/\/\n+  \/\/ References:\n+  \/\/ Shay Gueron, Michael E. Kounavis.\n+  \/\/ \"Intel® Carry-Less Multiplication Instruction and its Usage for Computing the GCM Mode\"\n+  \/\/ https:\/\/web.archive.org\/web\/20110609115824\/https:\/\/software.intel.com\/file\/24918\n+  \/\/\n+  Label loop;\n+  __ bind(loop);\n+    __ vspltisb(vZero, 0);\n+    __ li(temp1, 0);\n+    __ andi(temp1, data, 15);\n+    __ cmpwi(CR0, temp1, 0);\n+    __ beq(CR0, L_aligned);                    \/\/ Check if address is aligned (mask lower 4 bits)\n+    __ li(temp1, 0);\n+    __ lvx(vHigh, temp1, data);\n+    __ lvsl(vPerm, temp1, data);\n+    __ addi(data, data, 16);\n+    __ lvx(vLow, temp1, data);\n+    __ vec_perm(vHigh, vHigh, vHigh, loadOrder);\n+    __ vec_perm(vLow, vLow, vLow, loadOrder);\n+    __ vec_perm(vH, vLow, vHigh, vPerm);\n+    __ subi(data, data, 16);\n+    __ b(L_end);\n+    __ bind(L_aligned);\n+    __ li(temp1, 0);\n+    __ lvx(vH, temp1, data);\n+    __ vec_perm(vH, vH, vH, loadOrder);\n+    __ bind(L_end);\n+    __ vxor(vH, vH, vState);\n+    \/\/ Perform GCM multiplication\n+    __ vpmsumd(vTmp4, vLowerH, vH);             \/\/ L : Lower Half of subkey H\n+    __ vpmsumd(vTmp5, vTmp11, vH);              \/\/ M : Combined halves of subkey H\n+    __ vpmsumd(vTmp6, vHigherH, vH);            \/\/ H :  Higher Half of subkeyH\n+    __ vpmsumd(vTmp7, vTmp4, vConstC2);         \/\/ reduction\n+    __ vsldoi(vTmp8, vTmp5, vZero, 8);          \/\/ mL : Extract the lower 64 bits of M\n+    __ vsldoi(vTmp9, vZero, vTmp5, 8);          \/\/ mH : Extract the higher 64 bits of M\n+    __ vxor(vTmp4, vTmp4, vTmp8);               \/\/ LL + LL : Combine L and mL (partial result for lower half)\n+    __ vxor(vTmp6, vTmp6, vTmp9);               \/\/ HH + HH : Combine H and mH (partial result for upper half)\n+    __ vsldoi(vTmp4, vTmp4, vTmp4, 8);          \/\/ swap\n+    __ vxor(vTmp4, vTmp4, vTmp7);               \/\/ reduction using  the reduction constant\n+    __ vsldoi(vTmp10, vTmp4, vTmp4, 8);         \/\/ swap\n+    __ vpmsumd(vTmp4, vTmp4, vConstC2);         \/\/ reduction using the reduction constant\n+    __ vxor(vTmp10, vTmp10, vTmp6);             \/\/ Combine the reduced Low and High products\n+    __ vxor(vZero, vTmp4, vTmp10);\n+    __ vmr(vState, vZero);\n+    __ addi(data, data, 16);\n+    __ bdnz(loop);\n+  __ stxvd2x(vZero->to_vsr(), state);\n+  __ blr();\n+  #ifdef ASSERT\n+    __ bind(L_error);\n+    __ stop(\"ghash_processBlocks : number of blocks must be positive\");\n+  #endif\n+  return start;\n+}\n@@ -4931,0 +5086,3 @@\n+    if (UseGHASHIntrinsics) {\n+      StubRoutines::_ghash_processBlocks = generate_ghash_processBlocks();\n+    }\n","filename":"src\/hotspot\/cpu\/ppc\/stubGenerator_ppc.cpp","additions":158,"deletions":0,"binary":false,"changes":158,"status":"modified"},{"patch":"@@ -3,1 +3,1 @@\n- * Copyright (c) 2012, 2024 SAP SE. All rights reserved.\n+ * Copyright (c) 2012, 2025 SAP SE. All rights reserved.\n@@ -284,1 +284,1 @@\n-    if (FLAG_IS_DEFAULT(UseAES)) {\n+   if (FLAG_IS_DEFAULT(UseAES)) {\n@@ -308,2 +308,7 @@\n-  if (UseGHASHIntrinsics) {\n-    warning(\"GHASH intrinsics are not available on this CPU\");\n+  if (VM_Version::has_vsx()) {\n+    if (FLAG_IS_DEFAULT(UseGHASHIntrinsics)) {\n+      UseGHASHIntrinsics = true;\n+    }\n+  } else if (UseGHASHIntrinsics) {\n+    if (!FLAG_IS_DEFAULT(UseGHASHIntrinsics))\n+      warning(\"GHASH intrinsics are not available on this CPU\");\n","filename":"src\/hotspot\/cpu\/ppc\/vm_version_ppc.cpp","additions":9,"deletions":4,"binary":false,"changes":13,"status":"modified"}]}