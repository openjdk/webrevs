{"files":[{"patch":"@@ -549,0 +549,25 @@\n+static void computeGCMProduct(MacroAssembler* masm, \n+                              VectorRegister vLowerH, VectorRegister vH, VectorRegister vHigherH,\n+                              VectorRegister vConstC2, VectorRegister vZero, VectorRegister vState,\n+                              VectorRegister vTmp4, VectorRegister vTmp5, VectorRegister vTmp6,\n+                              VectorRegister vTmp7, VectorRegister vTmp8, VectorRegister vTmp9,\n+                              VectorRegister vTmp10, VectorRegister vTmp11, Register data) {\n+    assert(masm != nullptr, \"MacroAssembler pointer is null\");\n+    masm->vxor(vH, vH, vState);\n+    masm->vpmsumd(vTmp4, vLowerH, vH);     \/\/ L : Lower Half of subkey H\n+    masm->vpmsumd(vTmp5, vTmp11, vH);      \/\/ M : Combined halves of subkey H\n+    masm->vpmsumd(vTmp6, vHigherH, vH);    \/\/ H : Higher Half of subkey H\n+    masm->vpmsumd(vTmp7, vTmp4, vConstC2); \/\/ Reduction\n+    masm->vsldoi(vTmp8, vTmp5, vZero, 8);  \/\/ mL : Extract the lower 64 bits of M\n+    masm->vsldoi(vTmp9, vZero, vTmp5, 8);  \/\/ mH : Extract the higher 64 bits of M\n+    masm->vxor(vTmp4, vTmp4, vTmp8);       \/\/ LL + LL : Partial result for lower half\n+    masm->vxor(vTmp6, vTmp6, vTmp9);       \/\/ HH + HH : Partial result for upper half\n+    masm->vsldoi(vTmp4, vTmp4, vTmp4, 8);  \/\/ Swap\n+    masm->vxor(vTmp4, vTmp4, vTmp7);       \/\/ Reduction using constant\n+    masm->vsldoi(vTmp10, vTmp4, vTmp4, 8); \/\/ Swap\n+    masm->vpmsumd(vTmp4, vTmp4, vConstC2); \/\/ Reduction\n+    masm->vxor(vTmp10, vTmp10, vTmp6);     \/\/ Combine reduced Low & High products\n+    masm->vxor(vState, vTmp4, vTmp10);\n+    masm->addi(data, data, 16);\n+}\n+\n@@ -602,1 +627,1 @@\n-  Label L_end, L_aligned, L_error;\n+  Label L_end, L_aligned, L_error, L_trigger_assert, L_skip_assert;\n@@ -625,2 +650,5 @@\n-    __ cmpwi(CR0, blocks, 0);\n-    __ beq(CR0, L_error);\n+      __ cmpwi(CR0, blocks, 0);                 \/\/ Compare 'blocks' (R6_ARG4) with zero\n+      __ beq(CR0, L_trigger_assert);\n+      __ b(L_skip_assert);                      \/\/ Skip assertion if 'blocks' is nonzero\n+      __ bind(L_trigger_assert);\n+      __ asm_assert_eq(\"blocks should NOT be zero\");\n@@ -628,0 +656,1 @@\n+  __ bind(L_skip_assert);\n@@ -655,2 +684,16 @@\n-  Label loop;\n-  __ bind(loop);\n+  Label L_aligned_loop, L_store, L_unaligned_loop;\n+  __ andi(temp1, data, 15);\n+  __ cmpwi(CR0, temp1, 0);\n+  __ beq(CR0, L_aligned_loop);\n+  __ li(temp1,0);\n+  __ lvsl(vPerm, temp1, data);\n+  __ b(L_unaligned_loop);\n+  __ bind(L_aligned_loop);\n+    __ vspltisb(vZero, 0);\n+    __ lvx(vH, temp1, data);\n+    __ vec_perm(vH, vH, vH, loadOrder);\n+    computeGCMProduct(_masm, vLowerH, vH, vHigherH, vConstC2, vZero, vState,\n+                  vTmp4, vTmp5, vTmp6, vTmp7, vTmp8, vTmp9, vTmp10, vTmp11, data);\n+    __ bdnz(L_aligned_loop);\n+  __ b(L_store);\n+  __ bind(L_unaligned_loop);\n@@ -658,5 +701,0 @@\n-    __ li(temp1, 0);\n-    __ andi(temp1, data, 15);\n-    __ cmpwi(CR0, temp1, 0);\n-    __ beq(CR0, L_aligned);                    \/\/ Check if address is aligned (mask lower 4 bits)\n-    __ li(temp1, 0);\n@@ -664,1 +702,0 @@\n-    __ lvsl(vPerm, temp1, data);\n@@ -671,26 +708,5 @@\n-    __ b(L_end);\n-    __ bind(L_aligned);\n-    __ li(temp1, 0);\n-    __ lvx(vH, temp1, data);\n-    __ vec_perm(vH, vH, vH, loadOrder);\n-    __ bind(L_end);\n-    __ vxor(vH, vH, vState);\n-    \/\/ Perform GCM multiplication\n-    __ vpmsumd(vTmp4, vLowerH, vH);             \/\/ L : Lower Half of subkey H\n-    __ vpmsumd(vTmp5, vTmp11, vH);              \/\/ M : Combined halves of subkey H\n-    __ vpmsumd(vTmp6, vHigherH, vH);            \/\/ H :  Higher Half of subkeyH\n-    __ vpmsumd(vTmp7, vTmp4, vConstC2);         \/\/ reduction\n-    __ vsldoi(vTmp8, vTmp5, vZero, 8);          \/\/ mL : Extract the lower 64 bits of M\n-    __ vsldoi(vTmp9, vZero, vTmp5, 8);          \/\/ mH : Extract the higher 64 bits of M\n-    __ vxor(vTmp4, vTmp4, vTmp8);               \/\/ LL + LL : Combine L and mL (partial result for lower half)\n-    __ vxor(vTmp6, vTmp6, vTmp9);               \/\/ HH + HH : Combine H and mH (partial result for upper half)\n-    __ vsldoi(vTmp4, vTmp4, vTmp4, 8);          \/\/ swap\n-    __ vxor(vTmp4, vTmp4, vTmp7);               \/\/ reduction using  the reduction constant\n-    __ vsldoi(vTmp10, vTmp4, vTmp4, 8);         \/\/ swap\n-    __ vpmsumd(vTmp4, vTmp4, vConstC2);         \/\/ reduction using the reduction constant\n-    __ vxor(vTmp10, vTmp10, vTmp6);             \/\/ Combine the reduced Low and High products\n-    __ vxor(vZero, vTmp4, vTmp10);\n-    __ vmr(vState, vZero);\n-    __ addi(data, data, 16);\n-    __ bdnz(loop);\n-  __ stxvd2x(vZero->to_vsr(), state);\n+    computeGCMProduct(_masm, vLowerH, vH, vHigherH, vConstC2, vZero, vState,\n+                  vTmp4, vTmp5, vTmp6, vTmp7, vTmp8, vTmp9, vTmp10, vTmp11, data);\n+    __ bdnz(L_unaligned_loop);\n+  __ bind(L_store);\n+  __ stxvd2x(vState->to_vsr(), state);\n@@ -698,4 +714,0 @@\n-  #ifdef ASSERT\n-    __ bind(L_error);\n-    __ stop(\"ghash_processBlocks : number of blocks must be positive\");\n-  #endif\n@@ -5086,0 +5098,1 @@\n+\n","filename":"src\/hotspot\/cpu\/ppc\/stubGenerator_ppc.cpp","additions":54,"deletions":41,"binary":false,"changes":95,"status":"modified"},{"patch":"@@ -284,1 +284,1 @@\n-   if (FLAG_IS_DEFAULT(UseAES)) {\n+    if (FLAG_IS_DEFAULT(UseAES)) {\n@@ -308,1 +308,0 @@\n-  if (VM_Version::has_vsx()) {\n@@ -312,5 +311,0 @@\n-  } else if (UseGHASHIntrinsics) {\n-    if (!FLAG_IS_DEFAULT(UseGHASHIntrinsics))\n-      warning(\"GHASH intrinsics are not available on this CPU\");\n-    FLAG_SET_DEFAULT(UseGHASHIntrinsics, false);\n-  }\n","filename":"src\/hotspot\/cpu\/ppc\/vm_version_ppc.cpp","additions":1,"deletions":7,"binary":false,"changes":8,"status":"modified"}]}