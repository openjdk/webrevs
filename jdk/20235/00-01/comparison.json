{"files":[{"patch":"@@ -634,1 +634,1 @@\n-\/\/ Generate stub for ghash process  blocks.\n+\/\/ Generate stub for ghash process blocks.\n@@ -640,1 +640,2 @@\n-\/\/\n+\/\/      blocks: R6_ARG4\n+\/\/     \n@@ -655,0 +656,1 @@\n+  Register load = R12;\n@@ -656,28 +658,18 @@\n-  VectorRegister vH = VR0;\n-  VectorSRegister vHS = VSR32;\n-  VectorRegister vX = VR1;\n-  VectorRegister vH_shift = VR2;\n-  VectorRegister vTmp1 = VR3;\n-  VectorRegister vTmp2 = VR4;\n-  VectorRegister vSwappedH = VR5;\n-  VectorRegister vTmp4 = VR6;\n-  VectorRegister loadOrder = VR7;\n-  VectorRegister vMSB = VR8;\n-  VectorRegister vLowerH = VR9;\n-  VectorRegister vHigherH = VR10;\n-  VectorRegister vZero = VR11;\n-  VectorRegister vConst1 = VR12;\n-  VectorRegister vConst7 = VR13;\n-  VectorRegister vConstC2 = VR14;\n-  VectorRegister vTmp3 = VR16;\n-  VectorRegister vTmp5 = VR17;\n-  VectorRegister vTmp6 = VR18;\n-  VectorRegister vTmp7 = VR19;\n-  VectorRegister vHigh = VR20;\n-  VectorRegister vLow = VR21;\n-  VectorRegister vPerm = VR22;\n-  VectorRegister vZero_Stored = VR23;\n-  VectorSRegister vZero_StoredS = VSR55;\n-  VectorRegister vMask = VR24;\n-  VectorRegister vS = VR25;\n-  VectorSRegister vXS = VSR33;\n+  VectorRegister vZero = VR0;\n+  VectorRegister vH = VR1;\n+  VectorRegister vLowerH = VR2;\n+  VectorRegister vHigherH = VR3;\n+  VectorRegister vTmp4 = VR4;\n+  VectorRegister vTmp5 = VR5;\n+  VectorRegister vTmp6 = VR6;\n+  VectorRegister vTmp7 = VR7;\n+  VectorRegister vTmp8 = VR8;\n+  VectorRegister vTmp9 = VR9;\n+  VectorRegister vTmp10 = VR10;\n+  VectorRegister vTmp11 = VR11;\n+  VectorRegister vTmp12 = VR12;\n+  VectorRegister loadOrder = VR13;\n+  VectorRegister vHigh = VR14;\n+  VectorRegister vLow = VR15;\n+  VectorRegister vState = VR16;\n+  VectorRegister vConstC2 = VR19;\n@@ -686,2 +678,0 @@\n-  static const unsigned char perm_pattern[16] __attribute__((aligned(16))) = {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15};\n-\n@@ -690,0 +680,1 @@\n+  __ vxor(vZero, vZero, vZero);\n@@ -691,1 +682,0 @@\n-  __ vxor(vConstC2, vConstC2, vConstC2);\n@@ -693,3 +683,2 @@\n-  __ vxor(vZero, vZero, vZero);\n-  __ lxvd2x(vHS, subkeyH);\n-  __ lxvd2x(vZero_StoredS, state);\n+  __ lxvd2x(vH->to_vsr(), subkeyH);\n+  __ lxvd2x(vState->to_vsr(), state);\n@@ -697,10 +686,9 @@\n-  __ vspltisb(vConst1, 1);\n-  __ vspltisb(vConst7, 7);\n-  __ vsldoi(vTmp4, vZero, vConst1, 1);          \/\/ 0x1\n-  __ vor(vTmp4, vConstC2, vTmp4);               \/\/ 0xC2...1\n-  __ vsplt(vMSB, 0, vH);                        \/\/ MSB of H\n-  __ vxor(vH_shift, vH_shift, vH_shift);\n-  __ vsl(vH_shift, vH, vConst1);                \/\/ Carry = H<<7\n-  __ vsrab(vMSB, vMSB, vConst7);\n-  __ vand(vMSB, vMSB, vTmp4);                   \/\/ Carry\n-  __ vxor(vTmp2, vH_shift, vMSB);\n+  __ vspltisb(vTmp7, 1);\n+  __ vspltisb(vTmp10, 7);\n+  __ vsldoi(vTmp8, vZero, vTmp7, 1);          \/\/ 0x1\n+  __ vor(vTmp8, vConstC2, vTmp8);               \/\/ 0xC2...1\n+  __ vsplt(vTmp9, 0, vH);                        \/\/ MSB of H\n+  __ vsl(vH, vH, vTmp7);                \/\/ Carry = H<<7\n+  __ vsrab(vTmp9, vTmp9, vTmp10);\n+  __ vand(vTmp9, vTmp9, vTmp8);                   \/\/ Carry\n+  __ vxor(vTmp10, vH, vTmp9);\n@@ -708,5 +696,3 @@\n-  __ vsldoi(vSwappedH, vTmp2, vTmp2, 8);        \/\/ swap Lower and Higher Halves of subkey H\n-  __ vsldoi(vLowerH, vZero, vSwappedH, 8);      \/\/ H.L\n-  __ vsldoi(vHigherH, vSwappedH, vZero, 8);     \/\/ H.H\n-  __ vxor(vTmp1, vTmp1, vTmp1);\n-  __ vxor(vZero, vZero, vZero);\n+  __ vsldoi(vTmp11, vTmp10, vTmp10, 8);        \/\/ swap Lower and Higher Halves of subkey H\n+  __ vsldoi(vLowerH, vZero, vTmp11, 8);      \/\/ H.L\n+  __ vsldoi(vHigherH, vTmp11, vZero, 8);     \/\/ H.H\n@@ -714,3 +700,0 @@\n-  __ li(temp1, 0);\n-  __ load_const_optimized(temp2, (uintptr_t)&perm_pattern);\n-  __ lvx(loadOrder, temp2);\n@@ -728,1 +711,1 @@\n-    __ lvsl(vPerm, temp1, data);\n+    __ lvsl(loadOrder, temp1, data);\n@@ -731,1 +714,1 @@\n-    __ vec_perm(vX, vHigh, vLow, vPerm);\n+    __ vec_perm(vH, vHigh, vLow, loadOrder);\n@@ -736,1 +719,1 @@\n-    __ lvx(vX, temp1, data);\n+    __ lvx(vH, temp1, data);\n@@ -738,2 +721,8 @@\n-    __ vec_perm(vX, vX, vX, loadOrder);\n-    __ vxor(vX, vX, vZero_Stored);\n+    __ li(temp1, 0);\n+    __ lvsl(loadOrder, temp1);\n+    #ifdef VM_LITTLE_ENDIAN\n+      __ vspltisb(vTmp12, 0xf);\n+      __ vxor(loadOrder, loadOrder, vTmp12);\n+    #endif\n+    __ vec_perm(vH, vH, vH, loadOrder);\n+    __ vxor(vH, vH, vState);\n@@ -741,15 +730,15 @@\n-    __ vpmsumd(vTmp1, vLowerH, vX);             \/\/ L : Lower Half of subkey H\n-    __ vpmsumd(vTmp2, vSwappedH, vX);           \/\/ M : Combined halves of subkey H\n-    __ vpmsumd(vTmp3, vHigherH, vX);            \/\/ H :  Higher Half of subkeyH\n-    __ vpmsumd(vTmp4, vTmp1, vConstC2);         \/\/ reduction\n-    __ vsldoi(vTmp5, vTmp2, vZero, 8);          \/\/ mL : Extract the lower 64 bits of M\n-    __ vsldoi(vTmp6, vZero, vTmp2, 8);          \/\/ mH : Extract the higher 64 bits of M\n-    __ vxor(vTmp1, vTmp1, vTmp5);               \/\/ LL + LL : Combine L and mL (partial result for lower half)\n-    __ vxor(vTmp3, vTmp3, vTmp6);               \/\/ HH + HH : Combine H and mH (partial result for upper half)\n-    __ vsldoi(vTmp1, vTmp1, vTmp1, 8);          \/\/ swap\n-    __ vxor(vTmp1, vTmp1, vTmp4);               \/\/ reduction using  the reduction constant\n-    __ vsldoi(vTmp7, vTmp1, vTmp1, 8);          \/\/ swap\n-    __ vpmsumd(vTmp1, vTmp1, vConstC2);         \/\/ reduction using the reduction constant\n-    __ vxor(vTmp7, vTmp7, vTmp3);               \/\/ Combine the reduced Low and High products\n-    __ vxor(vZero, vTmp1, vTmp7);\n-    __ vmr(vZero_Stored, vZero);\n+    __ vpmsumd(vTmp4, vLowerH, vH);             \/\/ L : Lower Half of subkey H\n+    __ vpmsumd(vTmp5, vTmp11, vH);              \/\/ M : Combined halves of subkey H\n+    __ vpmsumd(vTmp6, vHigherH, vH);            \/\/ H :  Higher Half of subkeyH\n+    __ vpmsumd(vTmp7, vTmp4, vConstC2);         \/\/ reduction\n+    __ vsldoi(vTmp8, vTmp5, vZero, 8);          \/\/ mL : Extract the lower 64 bits of M\n+    __ vsldoi(vTmp9, vZero, vTmp5, 8);          \/\/ mH : Extract the higher 64 bits of M\n+    __ vxor(vTmp4, vTmp4, vTmp8);               \/\/ LL + LL : Combine L and mL (partial result for lower half)\n+    __ vxor(vTmp6, vTmp6, vTmp9);               \/\/ HH + HH : Combine H and mH (partial result for upper half)\n+    __ vsldoi(vTmp4, vTmp4, vTmp4, 8);          \/\/ swap\n+    __ vxor(vTmp4, vTmp4, vTmp7);               \/\/ reduction using  the reduction constant\n+    __ vsldoi(vTmp10, vTmp4, vTmp4, 8);          \/\/ swap\n+    __ vpmsumd(vTmp4, vTmp4, vConstC2);         \/\/ reduction using the reduction constant\n+    __ vxor(vTmp10, vTmp10, vTmp6);               \/\/ Combine the reduced Low and High products\n+    __ vxor(vZero, vTmp4, vTmp10);\n+    __ vmr(vState, vZero);\n@@ -759,2 +748,2 @@\n-  __ blr();                                     \/\/ Return from function\n-\n+  \n+  __ blr();\n","filename":"src\/hotspot\/cpu\/ppc\/stubGenerator_ppc.cpp","additions":65,"deletions":76,"binary":false,"changes":141,"status":"modified"}]}