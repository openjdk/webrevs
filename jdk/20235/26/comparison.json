{"files":[{"patch":"@@ -549,0 +549,171 @@\n+  \/\/ Computes the Galois\/Counter Mode (GCM) product and reduction.\n+  \/\/\n+  \/\/ This function performs polynomial multiplication of the subkey H with\n+  \/\/ the current GHASH state using vectorized polynomial multiplication (`vpmsumd`).\n+  \/\/ The subkey H is divided into lower, middle, and higher halves.\n+  \/\/ The multiplication results are reduced using `vConstC2` to stay within GF(2^128).\n+  \/\/ The final computed value is stored back into `vState`.\n+  static void computeGCMProduct(MacroAssembler* masm,\n+                              VectorRegister vLowerH, VectorRegister vH, VectorRegister vHigherH,\n+                              VectorRegister vConstC2, VectorRegister vZero, VectorRegister vState,\n+                              VectorRegister vTmp4, VectorRegister vTmp5, VectorRegister vTmp6,\n+                              VectorRegister vTmp7, VectorRegister vTmp8, VectorRegister vTmp9,\n+                              VectorRegister vTmp10, VectorRegister vTmp11) {\n+    assert(masm != nullptr, \"MacroAssembler pointer is null\");\n+    masm->vxor(vH, vH, vState);\n+    masm->vpmsumd(vTmp4, vLowerH, vH);            \/\/ L : Lower Half of subkey H\n+    masm->vpmsumd(vTmp5, vTmp11, vH);             \/\/ M : Combined halves of subkey H\n+    masm->vpmsumd(vTmp6, vHigherH, vH);           \/\/ H : Higher Half of subkey H\n+    masm->vpmsumd(vTmp7, vTmp4, vConstC2);        \/\/ Reduction\n+    masm->vsldoi(vTmp8, vTmp5, vZero, 8);         \/\/ mL : Extract the lower 64 bits of M\n+    masm->vsldoi(vTmp9, vZero, vTmp5, 8);         \/\/ mH : Extract the higher 64 bits of M\n+    masm->vxor(vTmp4, vTmp4, vTmp8);              \/\/ LL + LL : Partial result for lower half\n+    masm->vxor(vTmp6, vTmp6, vTmp9);              \/\/ HH + HH : Partial result for upper half\n+    masm->vsldoi(vTmp4, vTmp4, vTmp4, 8);         \/\/ Swap\n+    masm->vxor(vTmp4, vTmp4, vTmp7);              \/\/ Reduction using constant\n+    masm->vsldoi(vTmp10, vTmp4, vTmp4, 8);        \/\/ Swap\n+    masm->vpmsumd(vTmp4, vTmp4, vConstC2);        \/\/ Reduction\n+    masm->vxor(vTmp10, vTmp10, vTmp6);            \/\/ Combine reduced Low & High products\n+    masm->vxor(vState, vTmp4, vTmp10);\n+  }\n+\n+  \/\/ Generate stub for ghash process blocks.\n+  \/\/\n+  \/\/ Arguments for generated stub:\n+  \/\/      state:    R3_ARG1 (long[] state)\n+  \/\/      subkeyH:  R4_ARG2 (long[] subH)\n+  \/\/      data:     R5_ARG3 (byte[] data)\n+  \/\/      blocks:   R6_ARG4 (number of 16-byte blocks to process)\n+  \/\/\n+  \/\/ The polynomials are processed in bit-reflected order for efficiency reasons.\n+  \/\/ This optimization leverages the structure of the Galois field arithmetic\n+  \/\/ to minimize the number of bit manipulations required during multiplication.\n+  \/\/ For an explanation of how this works, refer :\n+  \/\/ Vinodh Gopal, Erdinc Ozturk, Wajdi Feghali, Jim Guilford, Gil Wolrich,\n+  \/\/ Martin Dixon. \"Optimized Galois-Counter-Mode Implementation on Intel®\n+  \/\/ Architecture Processor\"\n+  \/\/ http:\/\/web.archive.org\/web\/20130609111954\/http:\/\/www.intel.com\/content\/dam\/www\/public\/us\/en\/documents\/white-papers\/communications-ia-galois-counter-mode-paper.pdf\n+  \/\/\n+  \/\/\n+  address generate_ghash_processBlocks() {\n+    StubCodeMark mark(this, \"StubRoutines\", \"ghash\");\n+    address start = __ function_entry();\n+\n+    \/\/ Registers for parameters\n+    Register state = R3_ARG1;                     \/\/ long[] state\n+    Register subkeyH = R4_ARG2;                   \/\/ long[] subH\n+    Register data = R5_ARG3;                      \/\/ byte[] data\n+    Register blocks = R6_ARG4;\n+    Register temp1 = R8;\n+    \/\/ Vector Registers\n+    VectorRegister vZero = VR0;\n+    VectorRegister vH = VR1;\n+    VectorRegister vLowerH = VR2;\n+    VectorRegister vHigherH = VR3;\n+    VectorRegister vTmp4 = VR4;\n+    VectorRegister vTmp5 = VR5;\n+    VectorRegister vTmp6 = VR6;\n+    VectorRegister vTmp7 = VR7;\n+    VectorRegister vTmp8 = VR8;\n+    VectorRegister vTmp9 = VR9;\n+    VectorRegister vTmp10 = VR10;\n+    VectorRegister vTmp11 = VR11;\n+    VectorRegister vTmp12 = VR12;\n+    VectorRegister loadOrder = VR13;\n+    VectorRegister vHigh = VR14;\n+    VectorRegister vLow = VR15;\n+    VectorRegister vState = VR16;\n+    VectorRegister vPerm = VR17;\n+    VectorRegister vConstC2 = VR19;\n+\n+    __ li(temp1, 0xc2);\n+    __ sldi(temp1, temp1, 56);\n+    __ vspltisb(vZero, 0);\n+    __ mtvrd(vConstC2, temp1);\n+    __ lxvd2x(vH->to_vsr(), subkeyH);\n+    __ lxvd2x(vState->to_vsr(), state);\n+    \/\/ Operations to obtain lower and higher bytes of subkey H.\n+    __ vspltisb(vTmp7, 1);\n+    __ vspltisb(vTmp10, 7);\n+    __ vsldoi(vTmp8, vZero, vTmp7, 1);            \/\/ 0x1\n+    __ vor(vTmp8, vConstC2, vTmp8);               \/\/ 0xC2...1\n+    __ vsplt(vTmp9, 0, vH);                       \/\/ MSB of H\n+    __ vsl(vH, vH, vTmp7);                        \/\/ Carry = H<<7\n+    __ vsrab(vTmp9, vTmp9, vTmp10);\n+    __ vand(vTmp9, vTmp9, vTmp8);                 \/\/ Carry\n+    __ vxor(vTmp10, vH, vTmp9);\n+    __ vsldoi(vConstC2, vZero, vConstC2, 8);\n+    __ vsldoi(vTmp11, vTmp10, vTmp10, 8);         \/\/ swap Lower and Higher Halves of subkey H\n+    __ vsldoi(vLowerH, vZero, vTmp11, 8);         \/\/ H.L\n+    __ vsldoi(vHigherH, vTmp11, vZero, 8);        \/\/ H.H\n+#ifdef ASSERT\n+    __ cmpwi(CR0, blocks, 0);                     \/\/ Compare 'blocks' (R6_ARG4) with zero\n+    __ asm_assert_ne(\"blocks should NOT be zero\");\n+#endif\n+    __ clrldi(blocks, blocks, 32);\n+    __ mtctr(blocks);\n+    __ lvsl(loadOrder, temp1);\n+#ifdef VM_LITTLE_ENDIAN\n+    __ vspltisb(vTmp12, 0xf);\n+    __ vxor(loadOrder, loadOrder, vTmp12);\n+#define LE_swap_bytes(x) __ vec_perm(x, x, x, loadOrder)\n+#else\n+#define LE_swap_bytes(x)\n+#endif\n+\n+    \/\/ This code performs Karatsuba multiplication in Galois fields to compute the GHASH operation.\n+    \/\/\n+    \/\/ The Karatsuba method breaks the multiplication of two 128-bit numbers into smaller parts,\n+    \/\/ performing three 128-bit multiplications and combining the results efficiently.\n+    \/\/\n+    \/\/ (C1:C0) = A1*B1, (D1:D0) = A0*B0, (E1:E0) = (A0+A1)(B0+B1)\n+    \/\/ (A1:A0)(B1:B0) = C1:(C0+C1+D1+E1):(D1+C0+D0+E0):D0\n+    \/\/\n+    \/\/ Inputs:\n+    \/\/ - vH:       The data vector (state), containing both B0 (lower half) and B1 (higher half).\n+    \/\/ - vLowerH:  Lower half of the subkey H (A0).\n+    \/\/ - vHigherH: Higher half of the subkey H (A1).\n+    \/\/ - vConstC2: Constant used for reduction (for final processing).\n+    \/\/\n+    \/\/ References:\n+    \/\/ Shay Gueron, Michael E. Kounavis.\n+    \/\/ \"Intel® Carry-Less Multiplication Instruction and its Usage for Computing the GCM Mode\"\n+    \/\/ https:\/\/web.archive.org\/web\/20110609115824\/https:\/\/software.intel.com\/file\/24918\n+    \/\/\n+    Label L_aligned_loop, L_store, L_unaligned_loop, L_initialize_unaligned_loop;\n+    __ andi(temp1, data, 15);\n+    __ cmpwi(CR0, temp1, 0);\n+    __ bne(CR0, L_initialize_unaligned_loop);\n+\n+    __ bind(L_aligned_loop);\n+      __ lvx(vH, temp1, data);\n+      LE_swap_bytes(vH);\n+      computeGCMProduct(_masm, vLowerH, vH, vHigherH, vConstC2, vZero, vState,\n+                    vTmp4, vTmp5, vTmp6, vTmp7, vTmp8, vTmp9, vTmp10, vTmp11);\n+      __ addi(data, data, 16);\n+    __ bdnz(L_aligned_loop);\n+    __ b(L_store);\n+\n+    __ bind(L_initialize_unaligned_loop);\n+    __ li(temp1,0);\n+    __ lvsl(vPerm, temp1, data);\n+    __ lvx(vHigh, temp1, data);\n+#ifdef VM_LITTLE_ENDIAN\n+    __ xxspltib(vTmp12->to_vsr(), 31);\n+    __ vxor(vPerm, vPerm, vTmp12);\n+#endif\n+    __ bind(L_unaligned_loop);\n+      __ addi(data, data, 16);\n+      __ lvx(vLow, temp1, data);\n+      __ vec_perm(vH, vHigh, vLow, vPerm);\n+      computeGCMProduct(_masm, vLowerH, vH, vHigherH, vConstC2, vZero, vState,\n+                    vTmp4, vTmp5, vTmp6, vTmp7, vTmp8, vTmp9, vTmp10, vTmp11);\n+      __ vmr(vHigh, vLow);\n+    __ bdnz(L_unaligned_loop);\n+\n+    __ bind(L_store);\n+    __ stxvd2x(vState->to_vsr(), state);\n+    __ blr();\n+\n+    return start;\n+  }\n@@ -4932,0 +5103,4 @@\n+    if (UseGHASHIntrinsics) {\n+      StubRoutines::_ghash_processBlocks = generate_ghash_processBlocks();\n+    }\n+\n","filename":"src\/hotspot\/cpu\/ppc\/stubGenerator_ppc.cpp","additions":175,"deletions":0,"binary":false,"changes":175,"status":"modified"},{"patch":"@@ -3,1 +3,1 @@\n- * Copyright (c) 2012, 2024 SAP SE. All rights reserved.\n+ * Copyright (c) 2012, 2025 SAP SE. All rights reserved.\n@@ -308,2 +308,8 @@\n-  if (UseGHASHIntrinsics) {\n-    warning(\"GHASH intrinsics are not available on this CPU\");\n+  if (VM_Version::has_vsx()) {\n+    if (FLAG_IS_DEFAULT(UseGHASHIntrinsics)) {\n+      UseGHASHIntrinsics = true;\n+    }\n+  } else if (UseGHASHIntrinsics) {\n+    if (!FLAG_IS_DEFAULT(UseGHASHIntrinsics)) {\n+      warning(\"GHASH intrinsics are not available on this CPU\");\n+    }\n","filename":"src\/hotspot\/cpu\/ppc\/vm_version_ppc.cpp","additions":9,"deletions":3,"binary":false,"changes":12,"status":"modified"}]}