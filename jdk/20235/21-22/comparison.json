{"files":[{"patch":"@@ -549,1 +549,9 @@\n-static void computeGCMProduct(MacroAssembler* masm,\n+  \/\/ Computes the Galois\/Counter Mode (GCM) product and reduction.\n+  \/\/\n+  \/\/ This function performs polynomial multiplication of the subkey H with\n+  \/\/ the current GHASH state using vectorized polynomial multiplication (`vpmsumd`).\n+  \/\/ The subkey H is divided into lower, middle, and higher halves.\n+  \/\/ The multiplication results are reduced using `vConstC2` to stay within GF(2^128).\n+  \/\/ The final computed value is stored back into `vState`.\n+\n+  static void computeGCMProduct(MacroAssembler* masm,\n@@ -557,13 +565,13 @@\n-    masm->vpmsumd(vTmp4, vLowerH, vH);     \/\/ L : Lower Half of subkey H\n-    masm->vpmsumd(vTmp5, vTmp11, vH);      \/\/ M : Combined halves of subkey H\n-    masm->vpmsumd(vTmp6, vHigherH, vH);    \/\/ H : Higher Half of subkey H\n-    masm->vpmsumd(vTmp7, vTmp4, vConstC2); \/\/ Reduction\n-    masm->vsldoi(vTmp8, vTmp5, vZero, 8);  \/\/ mL : Extract the lower 64 bits of M\n-    masm->vsldoi(vTmp9, vZero, vTmp5, 8);  \/\/ mH : Extract the higher 64 bits of M\n-    masm->vxor(vTmp4, vTmp4, vTmp8);       \/\/ LL + LL : Partial result for lower half\n-    masm->vxor(vTmp6, vTmp6, vTmp9);       \/\/ HH + HH : Partial result for upper half\n-    masm->vsldoi(vTmp4, vTmp4, vTmp4, 8);  \/\/ Swap\n-    masm->vxor(vTmp4, vTmp4, vTmp7);       \/\/ Reduction using constant\n-    masm->vsldoi(vTmp10, vTmp4, vTmp4, 8); \/\/ Swap\n-    masm->vpmsumd(vTmp4, vTmp4, vConstC2); \/\/ Reduction\n-    masm->vxor(vTmp10, vTmp10, vTmp6);     \/\/ Combine reduced Low & High products\n+    masm->vpmsumd(vTmp4, vLowerH, vH);            \/\/ L : Lower Half of subkey H\n+    masm->vpmsumd(vTmp5, vTmp11, vH);             \/\/ M : Combined halves of subkey H\n+    masm->vpmsumd(vTmp6, vHigherH, vH);           \/\/ H : Higher Half of subkey H\n+    masm->vpmsumd(vTmp7, vTmp4, vConstC2);        \/\/ Reduction\n+    masm->vsldoi(vTmp8, vTmp5, vZero, 8);         \/\/ mL : Extract the lower 64 bits of M\n+    masm->vsldoi(vTmp9, vZero, vTmp5, 8);         \/\/ mH : Extract the higher 64 bits of M\n+    masm->vxor(vTmp4, vTmp4, vTmp8);              \/\/ LL + LL : Partial result for lower half\n+    masm->vxor(vTmp6, vTmp6, vTmp9);              \/\/ HH + HH : Partial result for upper half\n+    masm->vsldoi(vTmp4, vTmp4, vTmp4, 8);         \/\/ Swap\n+    masm->vxor(vTmp4, vTmp4, vTmp7);              \/\/ Reduction using constant\n+    masm->vsldoi(vTmp10, vTmp4, vTmp4, 8);        \/\/ Swap\n+    masm->vpmsumd(vTmp4, vTmp4, vConstC2);        \/\/ Reduction\n+    masm->vxor(vTmp10, vTmp10, vTmp6);            \/\/ Combine reduced Low & High products\n@@ -571,98 +579,3 @@\n-    masm->addi(data, data, 16);\n-}\n-\n-\/\/ Generate stub for ghash process blocks.\n-\/\/\n-\/\/ Arguments for generated stub:\n-\/\/      state:    R3_ARG1 (long[] state)\n-\/\/      subkeyH:  R4_ARG2 (long[] subH)\n-\/\/      data:     R5_ARG3 (byte[] data)\n-\/\/      blocks:   R6_ARG4 (number of 16-byte blocks to process)\n-\/\/\n-\/\/ The polynomials are processed in bit-reflected order for efficiency reasons.\n-\/\/ This optimization leverages the structure of the Galois field arithmetic\n-\/\/ to minimize the number of bit manipulations required during multiplication.\n-\/\/ For an explanation of how this works, refer :\n-\/\/ Vinodh Gopal, Erdinc Ozturk, Wajdi Feghali, Jim Guilford, Gil Wolrich,\n-\/\/ Martin Dixon. \"Optimized Galois-Counter-Mode Implementation on Intel®\n-\/\/ Architecture Processor\"\n-\/\/ http:\/\/web.archive.org\/web\/20130609111954\/http:\/\/www.intel.com\/content\/dam\/www\/public\/us\/en\/documents\/white-papers\/communications-ia-galois-counter-mode-paper.pdf\n-\/\/\n-\/\/\n-address generate_ghash_processBlocks() {\n-  StubCodeMark mark(this, \"StubRoutines\", \"ghash\");\n-  address start = __ function_entry();\n-\n-  \/\/ Registers for parameters\n-  Register state = R3_ARG1;                     \/\/ long[] state\n-  Register subkeyH = R4_ARG2;                   \/\/ long[] subH\n-  Register data = R5_ARG3;                      \/\/ byte[] data\n-  Register blocks = R6_ARG4;\n-  Register temp1 = R8;\n-  Register temp2 = R9;\n-  Register temp3 = R10;\n-  Register temp4 = R11;\n-  Register align = data;\n-  Register load = R12;\n-  \/\/ Vector Registers\n-  VectorRegister vZero = VR0;\n-  VectorRegister vH = VR1;\n-  VectorRegister vLowerH = VR2;\n-  VectorRegister vHigherH = VR3;\n-  VectorRegister vTmp4 = VR4;\n-  VectorRegister vTmp5 = VR5;\n-  VectorRegister vTmp6 = VR6;\n-  VectorRegister vTmp7 = VR7;\n-  VectorRegister vTmp8 = VR8;\n-  VectorRegister vTmp9 = VR9;\n-  VectorRegister vTmp10 = VR10;\n-  VectorRegister vTmp11 = VR11;\n-  VectorRegister vTmp12 = VR12;\n-  VectorRegister loadOrder = VR13;\n-  VectorRegister vHigh = VR14;\n-  VectorRegister vLow = VR15;\n-  VectorRegister vState = VR16;\n-  VectorRegister vPerm = VR17;\n-  VectorRegister vConstC2 = VR19;\n-  Label L_end, L_aligned, L_error, L_trigger_assert, L_skip_assert;\n-\n-  __ li(temp1, 0xc2);\n-  __ sldi(temp1, temp1, 56);\n-  __ vspltisb(vZero, 0);\n-  __ mtvrd(vConstC2, temp1);\n-  __ lxvd2x(vH->to_vsr(), subkeyH);\n-  __ lxvd2x(vState->to_vsr(), state);\n-  \/\/ Operations to obtain lower and higher bytes of subkey H.\n-  __ vspltisb(vTmp7, 1);\n-  __ vspltisb(vTmp10, 7);\n-  __ vsldoi(vTmp8, vZero, vTmp7, 1);            \/\/ 0x1\n-  __ vor(vTmp8, vConstC2, vTmp8);               \/\/ 0xC2...1\n-  __ vsplt(vTmp9, 0, vH);                       \/\/ MSB of H\n-  __ vsl(vH, vH, vTmp7);                        \/\/ Carry = H<<7\n-  __ vsrab(vTmp9, vTmp9, vTmp10);\n-  __ vand(vTmp9, vTmp9, vTmp8);                 \/\/ Carry\n-  __ vxor(vTmp10, vH, vTmp9);\n-  __ vsldoi(vConstC2, vZero, vConstC2, 8);\n-  __ vsldoi(vTmp11, vTmp10, vTmp10, 8);         \/\/ swap Lower and Higher Halves of subkey H\n-  __ vsldoi(vLowerH, vZero, vTmp11, 8);         \/\/ H.L\n-  __ vsldoi(vHigherH, vTmp11, vZero, 8);        \/\/ H.H\n-  #ifdef ASSERT\n-      __ cmpwi(CR0, blocks, 0);                 \/\/ Compare 'blocks' (R6_ARG4) with zero\n-      __ beq(CR0, L_trigger_assert);\n-      __ b(L_skip_assert);                      \/\/ Skip assertion if 'blocks' is nonzero\n-      __ bind(L_trigger_assert);\n-      __ asm_assert_eq(\"blocks should NOT be zero\");\n-  #endif\n-  __ bind(L_skip_assert);\n-  __ clrldi(blocks, blocks, 32);\n-  __ mtctr(blocks);\n-  __ li(temp1, 0);\n-  __ lvsl(loadOrder, temp1);\n-  #ifdef VM_LITTLE_ENDIAN\n-    __ vspltisb(vTmp12, 0xf);\n-    __ vxor(loadOrder, loadOrder, vTmp12);\n-  #endif\n-  \/\/ This code performs Karatsuba multiplication in Galois fields to compute the GHASH operation.\n-  \/\/\n-  \/\/ The Karatsuba method breaks the multiplication of two 128-bit numbers into smaller parts,\n-  \/\/ performing three 128-bit multiplications and combining the results efficiently.\n+  }\n+\n+  \/\/ Generate stub for ghash process blocks.\n@@ -670,2 +583,5 @@\n-  \/\/ (C1:C0) = A1*B1, (D1:D0) = A0*B0, (E1:E0) = (A0+A1)(B0+B1)\n-  \/\/ (A1:A0)(B1:B0) = C1:(C0+C1+D1+E1):(D1+C0+D0+E0):D0\n+  \/\/ Arguments for generated stub:\n+  \/\/      state:    R3_ARG1 (long[] state)\n+  \/\/      subkeyH:  R4_ARG2 (long[] subH)\n+  \/\/      data:     R5_ARG3 (byte[] data)\n+  \/\/      blocks:   R6_ARG4 (number of 16-byte blocks to process)\n@@ -673,5 +589,8 @@\n-  \/\/ Inputs:\n-  \/\/ - vH:       The data vector (state), containing both B0 (lower half) and B1 (higher half).\n-  \/\/ - vLowerH:  Lower half of the subkey H (A0).\n-  \/\/ - vHigherH: Higher half of the subkey H (A1).\n-  \/\/ - vConstC2: Constant used for reduction (for final processing).\n+  \/\/ The polynomials are processed in bit-reflected order for efficiency reasons.\n+  \/\/ This optimization leverages the structure of the Galois field arithmetic\n+  \/\/ to minimize the number of bit manipulations required during multiplication.\n+  \/\/ For an explanation of how this works, refer :\n+  \/\/ Vinodh Gopal, Erdinc Ozturk, Wajdi Feghali, Jim Guilford, Gil Wolrich,\n+  \/\/ Martin Dixon. \"Optimized Galois-Counter-Mode Implementation on Intel®\n+  \/\/ Architecture Processor\"\n+  \/\/ http:\/\/web.archive.org\/web\/20130609111954\/http:\/\/www.intel.com\/content\/dam\/www\/public\/us\/en\/documents\/white-papers\/communications-ia-galois-counter-mode-paper.pdf\n@@ -679,4 +598,0 @@\n-  \/\/ References:\n-  \/\/ Shay Gueron, Michael E. Kounavis.\n-  \/\/ \"Intel® Carry-Less Multiplication Instruction and its Usage for Computing the GCM Mode\"\n-  \/\/ https:\/\/web.archive.org\/web\/20110609115824\/https:\/\/software.intel.com\/file\/24918\n@@ -684,8 +599,39 @@\n-  Label L_aligned_loop, L_store, L_unaligned_loop;\n-  __ andi(temp1, data, 15);\n-  __ cmpwi(CR0, temp1, 0);\n-  __ beq(CR0, L_aligned_loop);\n-  __ li(temp1,0);\n-  __ lvsl(vPerm, temp1, data);\n-  __ b(L_unaligned_loop);\n-  __ bind(L_aligned_loop);\n+  address generate_ghash_processBlocks() {\n+    StubCodeMark mark(this, \"StubRoutines\", \"ghash\");\n+    address start = __ function_entry();\n+\n+    \/\/ Registers for parameters\n+    Register state = R3_ARG1;                     \/\/ long[] state\n+    Register subkeyH = R4_ARG2;                   \/\/ long[] subH\n+    Register data = R5_ARG3;                      \/\/ byte[] data\n+    Register blocks = R6_ARG4;\n+    Register temp1 = R8;\n+    Register temp2 = R9;\n+    Register temp3 = R10;\n+    Register temp4 = R11;\n+    Register align = data;\n+    Register load = R12;\n+    \/\/ Vector Registers\n+    VectorRegister vZero = VR0;\n+    VectorRegister vH = VR1;\n+    VectorRegister vLowerH = VR2;\n+    VectorRegister vHigherH = VR3;\n+    VectorRegister vTmp4 = VR4;\n+    VectorRegister vTmp5 = VR5;\n+    VectorRegister vTmp6 = VR6;\n+    VectorRegister vTmp7 = VR7;\n+    VectorRegister vTmp8 = VR8;\n+    VectorRegister vTmp9 = VR9;\n+    VectorRegister vTmp10 = VR10;\n+    VectorRegister vTmp11 = VR11;\n+    VectorRegister vTmp12 = VR12;\n+    VectorRegister loadOrder = VR13;\n+    VectorRegister vHigh = VR14;\n+    VectorRegister vLow = VR15;\n+    VectorRegister vState = VR16;\n+    VectorRegister vPerm = VR17;\n+    VectorRegister vConstC2 = VR19;\n+    Label L_end, L_aligned, L_error, L_trigger_assert, L_skip_assert;\n+\n+    __ li(temp1, 0xc2);\n+    __ sldi(temp1, temp1, 56);\n@@ -693,7 +639,53 @@\n-    __ lvx(vH, temp1, data);\n-    __ vec_perm(vH, vH, vH, loadOrder);\n-    computeGCMProduct(_masm, vLowerH, vH, vHigherH, vConstC2, vZero, vState,\n-                  vTmp4, vTmp5, vTmp6, vTmp7, vTmp8, vTmp9, vTmp10, vTmp11, data);\n-    __ bdnz(L_aligned_loop);\n-  __ b(L_store);\n-  __ bind(L_unaligned_loop);\n+    __ mtvrd(vConstC2, temp1);\n+    __ lxvd2x(vH->to_vsr(), subkeyH);\n+    __ lxvd2x(vState->to_vsr(), state);\n+    \/\/ Operations to obtain lower and higher bytes of subkey H.\n+    __ vspltisb(vTmp7, 1);\n+    __ vspltisb(vTmp10, 7);\n+    __ vsldoi(vTmp8, vZero, vTmp7, 1);            \/\/ 0x1\n+    __ vor(vTmp8, vConstC2, vTmp8);               \/\/ 0xC2...1\n+    __ vsplt(vTmp9, 0, vH);                       \/\/ MSB of H\n+    __ vsl(vH, vH, vTmp7);                        \/\/ Carry = H<<7\n+    __ vsrab(vTmp9, vTmp9, vTmp10);\n+    __ vand(vTmp9, vTmp9, vTmp8);                 \/\/ Carry\n+    __ vxor(vTmp10, vH, vTmp9);\n+    __ vsldoi(vConstC2, vZero, vConstC2, 8);\n+    __ vsldoi(vTmp11, vTmp10, vTmp10, 8);         \/\/ swap Lower and Higher Halves of subkey H\n+    __ vsldoi(vLowerH, vZero, vTmp11, 8);         \/\/ H.L\n+    __ vsldoi(vHigherH, vTmp11, vZero, 8);        \/\/ H.H\n+#ifdef ASSERT\n+    __ cmpwi(CR0, blocks, 0);                     \/\/ Compare 'blocks' (R6_ARG4) with zero\n+    __ beq(CR0, L_trigger_assert);\n+    __ b(L_skip_assert);                          \/\/ Skip assertion if 'blocks' is nonzero\n+    __ bind(L_trigger_assert);\n+    __ asm_assert_eq(\"blocks should NOT be zero\");\n+#endif\n+    __ bind(L_skip_assert);\n+    __ clrldi(blocks, blocks, 32);\n+    __ mtctr(blocks);\n+    __ li(temp1, 0);\n+    __ lvsl(loadOrder, temp1);\n+#ifdef VM_LITTLE_ENDIAN\n+    __ vspltisb(vTmp12, 0xf);\n+    __ vxor(loadOrder, loadOrder, vTmp12);\n+#endif\n+    \/\/ This code performs Karatsuba multiplication in Galois fields to compute the GHASH operation.\n+    \/\/\n+    \/\/ The Karatsuba method breaks the multiplication of two 128-bit numbers into smaller parts,\n+    \/\/ performing three 128-bit multiplications and combining the results efficiently.\n+    \/\/\n+    \/\/ (C1:C0) = A1*B1, (D1:D0) = A0*B0, (E1:E0) = (A0+A1)(B0+B1)\n+    \/\/ (A1:A0)(B1:B0) = C1:(C0+C1+D1+E1):(D1+C0+D0+E0):D0\n+    \/\/\n+    \/\/ Inputs:\n+    \/\/ - vH:       The data vector (state), containing both B0 (lower half) and B1 (higher half).\n+    \/\/ - vLowerH:  Lower half of the subkey H (A0).\n+    \/\/ - vHigherH: Higher half of the subkey H (A1).\n+    \/\/ - vConstC2: Constant used for reduction (for final processing).\n+    \/\/\n+    \/\/ References:\n+    \/\/ Shay Gueron, Michael E. Kounavis.\n+    \/\/ \"Intel® Carry-Less Multiplication Instruction and its Usage for Computing the GCM Mode\"\n+    \/\/ https:\/\/web.archive.org\/web\/20110609115824\/https:\/\/software.intel.com\/file\/24918\n+    \/\/\n+    Label L_aligned_loop, L_store, L_unaligned_loop;\n@@ -701,0 +693,5 @@\n+    __ andi(temp1, data, 15);\n+    __ cmpwi(CR0, temp1, 0);\n+    __ beq(CR0, L_aligned_loop);\n+    __ li(temp1,0);\n+    __ lvsl(vPerm, temp1, data);\n@@ -702,14 +699,26 @@\n-    __ addi(data, data, 16);\n-    __ lvx(vLow, temp1, data);\n-    __ vec_perm(vHigh, vHigh, vHigh, loadOrder);\n-    __ vec_perm(vLow, vLow, vLow, loadOrder);\n-    __ vec_perm(vH, vLow, vHigh, vPerm);\n-    __ subi(data, data, 16);\n-    computeGCMProduct(_masm, vLowerH, vH, vHigherH, vConstC2, vZero, vState,\n-                  vTmp4, vTmp5, vTmp6, vTmp7, vTmp8, vTmp9, vTmp10, vTmp11, data);\n-    __ bdnz(L_unaligned_loop);\n-  __ bind(L_store);\n-  __ stxvd2x(vState->to_vsr(), state);\n-  __ blr();\n-  return start;\n-}\n+    __ b(L_unaligned_loop);\n+    __ bind(L_aligned_loop);\n+      __ lvx(vH, temp1, data);\n+      __ vec_perm(vH, vH, vH, loadOrder);\n+      computeGCMProduct(_masm, vLowerH, vH, vHigherH, vConstC2, vZero, vState,\n+                    vTmp4, vTmp5, vTmp6, vTmp7, vTmp8, vTmp9, vTmp10, vTmp11, data);\n+      __ addi(data, data, 16);\n+      __ bdnz(L_aligned_loop);\n+    __ b(L_store);\n+    __ bind(L_unaligned_loop);\n+      __ addi(data, data, 16);\n+      __ lvx(vLow, temp1, data);\n+      __ vec_perm(vTmp4, vHigh, vHigh, loadOrder);\n+      __ vec_perm(vTmp5, vLow, vLow, loadOrder);\n+      __ vec_perm(vH, vTmp5, vTmp4, vPerm);\n+      __ subi(data, data, 16);\n+      computeGCMProduct(_masm, vLowerH, vH, vHigherH, vConstC2, vZero, vState,\n+                    vTmp4, vTmp5, vTmp6, vTmp7, vTmp8, vTmp9, vTmp10, vTmp11, data);\n+      __ vmr(vHigh, vLow);\n+      __ addi(data, data, 16);\n+      __ bdnz(L_unaligned_loop);\n+    __ bind(L_store);\n+    __ stxvd2x(vState->to_vsr(), state);\n+    __ blr();\n+    return start;\n+  }\n","filename":"src\/hotspot\/cpu\/ppc\/stubGenerator_ppc.cpp","additions":161,"deletions":152,"binary":false,"changes":313,"status":"modified"}]}