{"files":[{"patch":"@@ -634,0 +634,118 @@\n+\/\/ Generate stub for ghash process blocks.\n+\/\/\n+\/\/ Arguments for generated stub:\n+\/\/      state:  R3_ARG1\n+\/\/      subkeyH:    R4_ARG2\n+\/\/      data: R5_ARG3\n+\/\/      blocks: R6_ARG4\n+\/\/     \n+address generate_ghash_processBlocks() {\n+  StubCodeMark mark(this, \"StubRoutines\", \"ghash\");\n+  address start = __ function_entry();\n+\n+  \/\/ Registers for parameters\n+  Register state = R3_ARG1;                     \/\/ long[] state\n+  Register subkeyH = R4_ARG2;                   \/\/ long[] subH\n+  Register data = R5_ARG3;                      \/\/ byte[] data\n+  Register blocks = R6_ARG4;\n+  Register temp1 = R8;\n+  Register temp2 = R9;\n+  Register temp3 = R10;\n+  Register temp4 = R11;\n+  Register align = data;\n+  Register load = R12;\n+  \/\/ Vector Registers\n+  VectorRegister vZero = VR0;\n+  VectorRegister vH = VR1;\n+  VectorRegister vLowerH = VR2;\n+  VectorRegister vHigherH = VR3;\n+  VectorRegister vTmp4 = VR4;\n+  VectorRegister vTmp5 = VR5;\n+  VectorRegister vTmp6 = VR6;\n+  VectorRegister vTmp7 = VR7;\n+  VectorRegister vTmp8 = VR8;\n+  VectorRegister vTmp9 = VR9;\n+  VectorRegister vTmp10 = VR10;\n+  VectorRegister vTmp11 = VR11;\n+  VectorRegister vTmp12 = VR12;\n+  VectorRegister loadOrder = VR13;\n+  VectorRegister vHigh = VR14;\n+  VectorRegister vLow = VR15;\n+  VectorRegister vState = VR16;\n+  VectorRegister vConstC2 = VR19;\n+  Label L_end, L_aligned;\n+\n+  __ li(temp1, 0xc2);\n+  __ sldi(temp1, temp1, 56);\n+  __ vxor(vZero, vZero, vZero);\n+  \/\/ Load the vector from memory into vConstC2\n+  __ mtvrd(vConstC2, temp1);\n+  __ lxvd2x(vH->to_vsr(), subkeyH);\n+  __ lxvd2x(vState->to_vsr(), state);\n+  \/\/ Operations to obtain lower and higher bytes of subkey H.\n+  __ vspltisb(vTmp7, 1);\n+  __ vspltisb(vTmp10, 7);\n+  __ vsldoi(vTmp8, vZero, vTmp7, 1);          \/\/ 0x1\n+  __ vor(vTmp8, vConstC2, vTmp8);               \/\/ 0xC2...1\n+  __ vsplt(vTmp9, 0, vH);                        \/\/ MSB of H\n+  __ vsl(vH, vH, vTmp7);                \/\/ Carry = H<<7\n+  __ vsrab(vTmp9, vTmp9, vTmp10);\n+  __ vand(vTmp9, vTmp9, vTmp8);                   \/\/ Carry\n+  __ vxor(vTmp10, vH, vTmp9);\n+  __ vsldoi(vConstC2, vZero, vConstC2, 8);\n+  __ vsldoi(vTmp11, vTmp10, vTmp10, 8);        \/\/ swap Lower and Higher Halves of subkey H\n+  __ vsldoi(vLowerH, vZero, vTmp11, 8);      \/\/ H.L\n+  __ vsldoi(vHigherH, vTmp11, vZero, 8);     \/\/ H.H\n+  __ mtctr(blocks);\n+  \/\/ Performing Karatsuba multiplication in Galois fields\n+  Label loop;\n+  __ bind(loop);\n+    \/\/ Load immediate value 0 into temp\n+    __ vxor(vZero, vZero, vZero);\n+    __ li(temp1, 0);\n+    __ andi(temp1, data, 15);\n+    __ cmpwi(CCR0, temp1, 0);\n+    __ beq(CCR0, L_aligned);                      \/\/ Check if address is aligned (mask lower 4 bits)\n+    __ li(temp1, 0);\n+    __ lvx(vHigh, temp1, data);\n+    __ lvsl(loadOrder, temp1, data);\n+    __ addi(data, data, 16);\n+    __ lvx(vLow, temp1, data);\n+    __ vec_perm(vH, vHigh, vLow, loadOrder);\n+    __ subi(data, data, 16);\n+    __ b(L_end);\n+    __ bind(L_aligned);\n+    __ li(temp1, 0);\n+    __ lvx(vH, temp1, data);\n+    __ bind(L_end);\n+    __ li(temp1, 0);\n+    __ lvsl(loadOrder, temp1);\n+    #ifdef VM_LITTLE_ENDIAN\n+      __ vspltisb(vTmp12, 0xf);\n+      __ vxor(loadOrder, loadOrder, vTmp12);\n+    #endif\n+    __ vec_perm(vH, vH, vH, loadOrder);\n+    __ vxor(vH, vH, vState);\n+      \/\/ Perform GCM multiplication\n+    __ vpmsumd(vTmp4, vLowerH, vH);             \/\/ L : Lower Half of subkey H\n+    __ vpmsumd(vTmp5, vTmp11, vH);              \/\/ M : Combined halves of subkey H\n+    __ vpmsumd(vTmp6, vHigherH, vH);            \/\/ H :  Higher Half of subkeyH\n+    __ vpmsumd(vTmp7, vTmp4, vConstC2);         \/\/ reduction\n+    __ vsldoi(vTmp8, vTmp5, vZero, 8);          \/\/ mL : Extract the lower 64 bits of M\n+    __ vsldoi(vTmp9, vZero, vTmp5, 8);          \/\/ mH : Extract the higher 64 bits of M\n+    __ vxor(vTmp4, vTmp4, vTmp8);               \/\/ LL + LL : Combine L and mL (partial result for lower half)\n+    __ vxor(vTmp6, vTmp6, vTmp9);               \/\/ HH + HH : Combine H and mH (partial result for upper half)\n+    __ vsldoi(vTmp4, vTmp4, vTmp4, 8);          \/\/ swap\n+    __ vxor(vTmp4, vTmp4, vTmp7);               \/\/ reduction using  the reduction constant\n+    __ vsldoi(vTmp10, vTmp4, vTmp4, 8);          \/\/ swap\n+    __ vpmsumd(vTmp4, vTmp4, vConstC2);         \/\/ reduction using the reduction constant\n+    __ vxor(vTmp10, vTmp10, vTmp6);               \/\/ Combine the reduced Low and High products\n+    __ vxor(vZero, vTmp4, vTmp10);\n+    __ vmr(vState, vZero);\n+    __ addi(data, data, 16);\n+    __ bdnz(loop);\n+  __ stxvd2x(vZero->to_vsr(), state);\n+  \n+  __ blr();\n+  return start;\n+}\n@@ -4772,0 +4890,3 @@\n+    if (UseGHASHIntrinsics) {\n+      StubRoutines::_ghash_processBlocks = generate_ghash_processBlocks();\n+    }\n","filename":"src\/hotspot\/cpu\/ppc\/stubGenerator_ppc.cpp","additions":121,"deletions":0,"binary":false,"changes":121,"status":"modified"},{"patch":"@@ -2,2 +2,2 @@\n- * Copyright (c) 1997, 2023, Oracle and\/or its affiliates. All rights reserved.\n- * Copyright (c) 2012, 2023 SAP SE. All rights reserved.\n+ * Copyright (c) 1997, 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2012, 2024 SAP SE. All rights reserved.\n@@ -309,3 +309,2 @@\n-  if (UseGHASHIntrinsics) {\n-    warning(\"GHASH intrinsics are not available on this CPU\");\n-    FLAG_SET_DEFAULT(UseGHASHIntrinsics, false);\n+  if (FLAG_IS_DEFAULT(UseGHASHIntrinsics)) {\n+    FLAG_SET_DEFAULT(UseGHASHIntrinsics, true);\n","filename":"src\/hotspot\/cpu\/ppc\/vm_version_ppc.cpp","additions":4,"deletions":5,"binary":false,"changes":9,"status":"modified"}]}