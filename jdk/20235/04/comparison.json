{"files":[{"patch":"@@ -1,4821 +0,0 @@\n-\/*\n- * Copyright (c) 1997, 2024, Oracle and\/or its affiliates. All rights reserved.\n- * Copyright (c) 2012, 2024 SAP SE. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\n- *\/\n-\n-#include \"precompiled.hpp\"\n-#include \"asm\/macroAssembler.inline.hpp\"\n-#include \"compiler\/oopMap.hpp\"\n-#include \"gc\/shared\/barrierSet.hpp\"\n-#include \"gc\/shared\/barrierSetAssembler.hpp\"\n-#include \"gc\/shared\/barrierSetNMethod.hpp\"\n-#include \"interpreter\/interpreter.hpp\"\n-#include \"nativeInst_ppc.hpp\"\n-#include \"oops\/instanceOop.hpp\"\n-#include \"oops\/method.hpp\"\n-#include \"oops\/objArrayKlass.hpp\"\n-#include \"oops\/oop.inline.hpp\"\n-#include \"prims\/methodHandles.hpp\"\n-#include \"prims\/upcallLinker.hpp\"\n-#include \"runtime\/continuation.hpp\"\n-#include \"runtime\/continuationEntry.inline.hpp\"\n-#include \"runtime\/frame.inline.hpp\"\n-#include \"runtime\/handles.inline.hpp\"\n-#include \"runtime\/javaThread.hpp\"\n-#include \"runtime\/sharedRuntime.hpp\"\n-#include \"runtime\/stubCodeGenerator.hpp\"\n-#include \"runtime\/stubRoutines.hpp\"\n-#include \"runtime\/vm_version.hpp\"\n-#include \"utilities\/align.hpp\"\n-#include \"utilities\/powerOfTwo.hpp\"\n-#if INCLUDE_ZGC\n-#include \"gc\/z\/zBarrierSetAssembler.hpp\"\n-#endif\n-\n-\/\/ Declaration and definition of StubGenerator (no .hpp file).\n-\/\/ For a more detailed description of the stub routine structure\n-\/\/ see the comment in stubRoutines.hpp.\n-\n-#define __ _masm->\n-\n-#ifdef PRODUCT\n-#define BLOCK_COMMENT(str) \/\/ nothing\n-#else\n-#define BLOCK_COMMENT(str) __ block_comment(str)\n-#endif\n-\n-#if defined(ABI_ELFv2)\n-#define STUB_ENTRY(name) StubRoutines::name\n-#else\n-#define STUB_ENTRY(name) ((FunctionDescriptor*)StubRoutines::name)->entry()\n-#endif\n-\n-class StubGenerator: public StubCodeGenerator {\n- private:\n-\n-  \/\/ Call stubs are used to call Java from C\n-  \/\/\n-  \/\/ Arguments:\n-  \/\/\n-  \/\/   R3  - call wrapper address     : address\n-  \/\/   R4  - result                   : intptr_t*\n-  \/\/   R5  - result type              : BasicType\n-  \/\/   R6  - method                   : Method\n-  \/\/   R7  - frame mgr entry point    : address\n-  \/\/   R8  - parameter block          : intptr_t*\n-  \/\/   R9  - parameter count in words : int\n-  \/\/   R10 - thread                   : Thread*\n-  \/\/\n-  address generate_call_stub(address& return_address) {\n-    \/\/ Setup a new c frame, copy java arguments, call frame manager or\n-    \/\/ native_entry, and process result.\n-\n-    StubCodeMark mark(this, \"StubRoutines\", \"call_stub\");\n-\n-    address start = __ function_entry();\n-\n-    \/\/ some sanity checks\n-    assert((sizeof(frame::native_abi_minframe) % 16) == 0,    \"unaligned\");\n-    assert((sizeof(frame::native_abi_reg_args) % 16) == 0,    \"unaligned\");\n-    assert((sizeof(frame::spill_nonvolatiles) % 16) == 0,     \"unaligned\");\n-    assert((sizeof(frame::parent_ijava_frame_abi) % 16) == 0, \"unaligned\");\n-    assert((sizeof(frame::entry_frame_locals) % 16) == 0,     \"unaligned\");\n-\n-    Register r_arg_call_wrapper_addr        = R3;\n-    Register r_arg_result_addr              = R4;\n-    Register r_arg_result_type              = R5;\n-    Register r_arg_method                   = R6;\n-    Register r_arg_entry                    = R7;\n-    Register r_arg_thread                   = R10;\n-\n-    Register r_temp                         = R24;\n-    Register r_top_of_arguments_addr        = R25;\n-    Register r_entryframe_fp                = R26;\n-\n-    {\n-      \/\/ Stack on entry to call_stub:\n-      \/\/\n-      \/\/      F1      [C_FRAME]\n-      \/\/              ...\n-\n-      Register r_arg_argument_addr          = R8;\n-      Register r_arg_argument_count         = R9;\n-      Register r_frame_alignment_in_bytes   = R27;\n-      Register r_argument_addr              = R28;\n-      Register r_argumentcopy_addr          = R29;\n-      Register r_argument_size_in_bytes     = R30;\n-      Register r_frame_size                 = R23;\n-\n-      Label arguments_copied;\n-\n-      \/\/ Save LR\/CR to caller's C_FRAME.\n-      __ save_LR_CR(R0);\n-\n-      \/\/ Zero extend arg_argument_count.\n-      __ clrldi(r_arg_argument_count, r_arg_argument_count, 32);\n-\n-      \/\/ Save non-volatiles GPRs to ENTRY_FRAME (not yet pushed, but it's safe).\n-      __ save_nonvolatile_gprs(R1_SP, _spill_nonvolatiles_neg(r14));\n-\n-      \/\/ Keep copy of our frame pointer (caller's SP).\n-      __ mr(r_entryframe_fp, R1_SP);\n-\n-      BLOCK_COMMENT(\"Push ENTRY_FRAME including arguments\");\n-      \/\/ Push ENTRY_FRAME including arguments:\n-      \/\/\n-      \/\/      F0      [TOP_IJAVA_FRAME_ABI]\n-      \/\/              alignment (optional)\n-      \/\/              [outgoing Java arguments]\n-      \/\/              [ENTRY_FRAME_LOCALS]\n-      \/\/      F1      [C_FRAME]\n-      \/\/              ...\n-\n-      \/\/ calculate frame size\n-\n-      \/\/ unaligned size of arguments\n-      __ sldi(r_argument_size_in_bytes,\n-                  r_arg_argument_count, Interpreter::logStackElementSize);\n-      \/\/ arguments alignment (max 1 slot)\n-      \/\/ FIXME: use round_to() here\n-      __ andi_(r_frame_alignment_in_bytes, r_arg_argument_count, 1);\n-      __ sldi(r_frame_alignment_in_bytes,\n-              r_frame_alignment_in_bytes, Interpreter::logStackElementSize);\n-\n-      \/\/ size = unaligned size of arguments + top abi's size\n-      __ addi(r_frame_size, r_argument_size_in_bytes,\n-              frame::top_ijava_frame_abi_size);\n-      \/\/ size += arguments alignment\n-      __ add(r_frame_size,\n-             r_frame_size, r_frame_alignment_in_bytes);\n-      \/\/ size += size of call_stub locals\n-      __ addi(r_frame_size,\n-              r_frame_size, frame::entry_frame_locals_size);\n-\n-      \/\/ push ENTRY_FRAME\n-      __ push_frame(r_frame_size, r_temp);\n-\n-      \/\/ initialize call_stub locals (step 1)\n-      __ std(r_arg_call_wrapper_addr,\n-             _entry_frame_locals_neg(call_wrapper_address), r_entryframe_fp);\n-      __ std(r_arg_result_addr,\n-             _entry_frame_locals_neg(result_address), r_entryframe_fp);\n-      __ std(r_arg_result_type,\n-             _entry_frame_locals_neg(result_type), r_entryframe_fp);\n-      \/\/ we will save arguments_tos_address later\n-\n-\n-      BLOCK_COMMENT(\"Copy Java arguments\");\n-      \/\/ copy Java arguments\n-\n-      \/\/ Calculate top_of_arguments_addr which will be R17_tos (not prepushed) later.\n-      \/\/ FIXME: why not simply use SP+frame::top_ijava_frame_size?\n-      __ addi(r_top_of_arguments_addr,\n-              R1_SP, frame::top_ijava_frame_abi_size);\n-      __ add(r_top_of_arguments_addr,\n-             r_top_of_arguments_addr, r_frame_alignment_in_bytes);\n-\n-      \/\/ any arguments to copy?\n-      __ cmpdi(CCR0, r_arg_argument_count, 0);\n-      __ beq(CCR0, arguments_copied);\n-\n-      \/\/ prepare loop and copy arguments in reverse order\n-      {\n-        \/\/ init CTR with arg_argument_count\n-        __ mtctr(r_arg_argument_count);\n-\n-        \/\/ let r_argumentcopy_addr point to last outgoing Java arguments P\n-        __ mr(r_argumentcopy_addr, r_top_of_arguments_addr);\n-\n-        \/\/ let r_argument_addr point to last incoming java argument\n-        __ add(r_argument_addr,\n-                   r_arg_argument_addr, r_argument_size_in_bytes);\n-        __ addi(r_argument_addr, r_argument_addr, -BytesPerWord);\n-\n-        \/\/ now loop while CTR > 0 and copy arguments\n-        {\n-          Label next_argument;\n-          __ bind(next_argument);\n-\n-          __ ld(r_temp, 0, r_argument_addr);\n-          \/\/ argument_addr--;\n-          __ addi(r_argument_addr, r_argument_addr, -BytesPerWord);\n-          __ std(r_temp, 0, r_argumentcopy_addr);\n-          \/\/ argumentcopy_addr++;\n-          __ addi(r_argumentcopy_addr, r_argumentcopy_addr, BytesPerWord);\n-\n-          __ bdnz(next_argument);\n-        }\n-      }\n-\n-      \/\/ Arguments copied, continue.\n-      __ bind(arguments_copied);\n-    }\n-\n-    {\n-      BLOCK_COMMENT(\"Call frame manager or native entry.\");\n-      \/\/ Call frame manager or native entry.\n-      Register r_new_arg_entry = R14;\n-      assert_different_registers(r_new_arg_entry, r_top_of_arguments_addr,\n-                                 r_arg_method, r_arg_thread);\n-\n-      __ mr(r_new_arg_entry, r_arg_entry);\n-\n-      \/\/ Register state on entry to frame manager \/ native entry:\n-      \/\/\n-      \/\/   tos         -  intptr_t*    sender tos (prepushed) Lesp = (SP) + copied_arguments_offset - 8\n-      \/\/   R19_method  -  Method\n-      \/\/   R16_thread  -  JavaThread*\n-\n-      \/\/ Tos must point to last argument - element_size.\n-      const Register tos = R15_esp;\n-\n-      __ addi(tos, r_top_of_arguments_addr, -Interpreter::stackElementSize);\n-\n-      \/\/ initialize call_stub locals (step 2)\n-      \/\/ now save tos as arguments_tos_address\n-      __ std(tos, _entry_frame_locals_neg(arguments_tos_address), r_entryframe_fp);\n-\n-      \/\/ load argument registers for call\n-      __ mr(R19_method, r_arg_method);\n-      __ mr(R16_thread, r_arg_thread);\n-      assert(tos != r_arg_method, \"trashed r_arg_method\");\n-      assert(tos != r_arg_thread && R19_method != r_arg_thread, \"trashed r_arg_thread\");\n-\n-      \/\/ Set R15_prev_state to 0 for simplifying checks in callee.\n-      __ load_const_optimized(R25_templateTableBase, (address)Interpreter::dispatch_table((TosState)0), R11_scratch1);\n-      \/\/ Stack on entry to frame manager \/ native entry:\n-      \/\/\n-      \/\/      F0      [TOP_IJAVA_FRAME_ABI]\n-      \/\/              alignment (optional)\n-      \/\/              [outgoing Java arguments]\n-      \/\/              [ENTRY_FRAME_LOCALS]\n-      \/\/      F1      [C_FRAME]\n-      \/\/              ...\n-      \/\/\n-\n-      \/\/ global toc register\n-      __ load_const_optimized(R29_TOC, MacroAssembler::global_toc(), R11_scratch1);\n-      \/\/ Remember the senderSP so we interpreter can pop c2i arguments off of the stack\n-      \/\/ when called via a c2i.\n-\n-      \/\/ Pass initial_caller_sp to framemanager.\n-      __ mr(R21_sender_SP, R1_SP);\n-\n-      \/\/ Do a light-weight C-call here, r_new_arg_entry holds the address\n-      \/\/ of the interpreter entry point (frame manager or native entry)\n-      \/\/ and save runtime-value of LR in return_address.\n-      assert(r_new_arg_entry != tos && r_new_arg_entry != R19_method && r_new_arg_entry != R16_thread,\n-             \"trashed r_new_arg_entry\");\n-      return_address = __ call_stub(r_new_arg_entry);\n-    }\n-\n-    {\n-      BLOCK_COMMENT(\"Returned from frame manager or native entry.\");\n-      \/\/ Returned from frame manager or native entry.\n-      \/\/ Now pop frame, process result, and return to caller.\n-\n-      \/\/ Stack on exit from frame manager \/ native entry:\n-      \/\/\n-      \/\/      F0      [ABI]\n-      \/\/              ...\n-      \/\/              [ENTRY_FRAME_LOCALS]\n-      \/\/      F1      [C_FRAME]\n-      \/\/              ...\n-      \/\/\n-      \/\/ Just pop the topmost frame ...\n-      \/\/\n-\n-      Label ret_is_object;\n-      Label ret_is_long;\n-      Label ret_is_float;\n-      Label ret_is_double;\n-\n-      Register r_entryframe_fp = R30;\n-      Register r_lr            = R7_ARG5;\n-      Register r_cr            = R8_ARG6;\n-\n-      \/\/ Reload some volatile registers which we've spilled before the call\n-      \/\/ to frame manager \/ native entry.\n-      \/\/ Access all locals via frame pointer, because we know nothing about\n-      \/\/ the topmost frame's size.\n-      __ ld(r_entryframe_fp, _abi0(callers_sp), R1_SP);\n-      assert_different_registers(r_entryframe_fp, R3_RET, r_arg_result_addr, r_arg_result_type, r_cr, r_lr);\n-      __ ld(r_arg_result_addr,\n-            _entry_frame_locals_neg(result_address), r_entryframe_fp);\n-      __ ld(r_arg_result_type,\n-            _entry_frame_locals_neg(result_type), r_entryframe_fp);\n-      __ ld(r_cr, _abi0(cr), r_entryframe_fp);\n-      __ ld(r_lr, _abi0(lr), r_entryframe_fp);\n-\n-      \/\/ pop frame and restore non-volatiles, LR and CR\n-      __ mr(R1_SP, r_entryframe_fp);\n-      __ pop_cont_fastpath();\n-      __ mtcr(r_cr);\n-      __ mtlr(r_lr);\n-\n-      \/\/ Store result depending on type. Everything that is not\n-      \/\/ T_OBJECT, T_LONG, T_FLOAT, or T_DOUBLE is treated as T_INT.\n-      __ cmpwi(CCR0, r_arg_result_type, T_OBJECT);\n-      __ cmpwi(CCR1, r_arg_result_type, T_LONG);\n-      __ cmpwi(CCR5, r_arg_result_type, T_FLOAT);\n-      __ cmpwi(CCR6, r_arg_result_type, T_DOUBLE);\n-\n-      \/\/ restore non-volatile registers\n-      __ restore_nonvolatile_gprs(R1_SP, _spill_nonvolatiles_neg(r14));\n-\n-\n-      \/\/ Stack on exit from call_stub:\n-      \/\/\n-      \/\/      0       [C_FRAME]\n-      \/\/              ...\n-      \/\/\n-      \/\/  no call_stub frames left.\n-\n-      \/\/ All non-volatiles have been restored at this point!!\n-      assert(R3_RET == R3, \"R3_RET should be R3\");\n-\n-      __ beq(CCR0, ret_is_object);\n-      __ beq(CCR1, ret_is_long);\n-      __ beq(CCR5, ret_is_float);\n-      __ beq(CCR6, ret_is_double);\n-\n-      \/\/ default:\n-      __ stw(R3_RET, 0, r_arg_result_addr);\n-      __ blr(); \/\/ return to caller\n-\n-      \/\/ case T_OBJECT:\n-      __ bind(ret_is_object);\n-      __ std(R3_RET, 0, r_arg_result_addr);\n-      __ blr(); \/\/ return to caller\n-\n-      \/\/ case T_LONG:\n-      __ bind(ret_is_long);\n-      __ std(R3_RET, 0, r_arg_result_addr);\n-      __ blr(); \/\/ return to caller\n-\n-      \/\/ case T_FLOAT:\n-      __ bind(ret_is_float);\n-      __ stfs(F1_RET, 0, r_arg_result_addr);\n-      __ blr(); \/\/ return to caller\n-\n-      \/\/ case T_DOUBLE:\n-      __ bind(ret_is_double);\n-      __ stfd(F1_RET, 0, r_arg_result_addr);\n-      __ blr(); \/\/ return to caller\n-    }\n-\n-    return start;\n-  }\n-\n-  \/\/ Return point for a Java call if there's an exception thrown in\n-  \/\/ Java code.  The exception is caught and transformed into a\n-  \/\/ pending exception stored in JavaThread that can be tested from\n-  \/\/ within the VM.\n-  \/\/\n-  address generate_catch_exception() {\n-    StubCodeMark mark(this, \"StubRoutines\", \"catch_exception\");\n-\n-    address start = __ pc();\n-\n-    \/\/ Registers alive\n-    \/\/\n-    \/\/  R16_thread\n-    \/\/  R3_ARG1 - address of pending exception\n-    \/\/  R4_ARG2 - return address in call stub\n-\n-    const Register exception_file = R21_tmp1;\n-    const Register exception_line = R22_tmp2;\n-\n-    __ load_const(exception_file, (void*)__FILE__);\n-    __ load_const(exception_line, (void*)__LINE__);\n-\n-    __ std(R3_ARG1, in_bytes(JavaThread::pending_exception_offset()), R16_thread);\n-    \/\/ store into `char *'\n-    __ std(exception_file, in_bytes(JavaThread::exception_file_offset()), R16_thread);\n-    \/\/ store into `int'\n-    __ stw(exception_line, in_bytes(JavaThread::exception_line_offset()), R16_thread);\n-\n-    \/\/ complete return to VM\n-    assert(StubRoutines::_call_stub_return_address != nullptr, \"must have been generated before\");\n-\n-    __ mtlr(R4_ARG2);\n-    \/\/ continue in call stub\n-    __ blr();\n-\n-    return start;\n-  }\n-\n-  \/\/ Continuation point for runtime calls returning with a pending\n-  \/\/ exception.  The pending exception check happened in the runtime\n-  \/\/ or native call stub.  The pending exception in Thread is\n-  \/\/ converted into a Java-level exception.\n-  \/\/\n-  \/\/ Read:\n-  \/\/\n-  \/\/   LR:     The pc the runtime library callee wants to return to.\n-  \/\/           Since the exception occurred in the callee, the return pc\n-  \/\/           from the point of view of Java is the exception pc.\n-  \/\/   thread: Needed for method handles.\n-  \/\/\n-  \/\/ Invalidate:\n-  \/\/\n-  \/\/   volatile registers (except below).\n-  \/\/\n-  \/\/ Update:\n-  \/\/\n-  \/\/   R4_ARG2: exception\n-  \/\/\n-  \/\/ (LR is unchanged and is live out).\n-  \/\/\n-  address generate_forward_exception() {\n-    StubCodeMark mark(this, \"StubRoutines\", \"forward_exception\");\n-    address start = __ pc();\n-\n-    if (VerifyOops) {\n-      \/\/ Get pending exception oop.\n-      __ ld(R3_ARG1,\n-                in_bytes(Thread::pending_exception_offset()),\n-                R16_thread);\n-      \/\/ Make sure that this code is only executed if there is a pending exception.\n-      {\n-        Label L;\n-        __ cmpdi(CCR0, R3_ARG1, 0);\n-        __ bne(CCR0, L);\n-        __ stop(\"StubRoutines::forward exception: no pending exception (1)\");\n-        __ bind(L);\n-      }\n-      __ verify_oop(R3_ARG1, \"StubRoutines::forward exception: not an oop\");\n-    }\n-\n-    \/\/ Save LR\/CR and copy exception pc (LR) into R4_ARG2.\n-    __ save_LR(R4_ARG2);\n-    __ push_frame_reg_args(0, R0);\n-    \/\/ Find exception handler.\n-    __ call_VM_leaf(CAST_FROM_FN_PTR(address,\n-                     SharedRuntime::exception_handler_for_return_address),\n-                    R16_thread,\n-                    R4_ARG2);\n-    \/\/ Copy handler's address.\n-    __ mtctr(R3_RET);\n-    __ pop_frame();\n-    __ restore_LR(R0);\n-\n-    \/\/ Set up the arguments for the exception handler:\n-    \/\/  - R3_ARG1: exception oop\n-    \/\/  - R4_ARG2: exception pc.\n-\n-    \/\/ Load pending exception oop.\n-    __ ld(R3_ARG1,\n-              in_bytes(Thread::pending_exception_offset()),\n-              R16_thread);\n-\n-    \/\/ The exception pc is the return address in the caller.\n-    \/\/ Must load it into R4_ARG2.\n-    __ mflr(R4_ARG2);\n-\n-#ifdef ASSERT\n-    \/\/ Make sure exception is set.\n-    {\n-      Label L;\n-      __ cmpdi(CCR0, R3_ARG1, 0);\n-      __ bne(CCR0, L);\n-      __ stop(\"StubRoutines::forward exception: no pending exception (2)\");\n-      __ bind(L);\n-    }\n-#endif\n-\n-    \/\/ Clear the pending exception.\n-    __ li(R0, 0);\n-    __ std(R0,\n-               in_bytes(Thread::pending_exception_offset()),\n-               R16_thread);\n-    \/\/ Jump to exception handler.\n-    __ bctr();\n-\n-    return start;\n-  }\n-\n-#undef __\n-#define __ _masm->\n-\n-\n-  \/\/ Support for void zero_words_aligned8(HeapWord* to, size_t count)\n-  \/\/\n-  \/\/ Arguments:\n-  \/\/   to:\n-  \/\/   count:\n-  \/\/\n-  \/\/ Destroys:\n-  \/\/\n-  address generate_zero_words_aligned8() {\n-    StubCodeMark mark(this, \"StubRoutines\", \"zero_words_aligned8\");\n-\n-    \/\/ Implemented as in ClearArray.\n-    address start = __ function_entry();\n-\n-    Register base_ptr_reg   = R3_ARG1; \/\/ tohw (needs to be 8b aligned)\n-    Register cnt_dwords_reg = R4_ARG2; \/\/ count (in dwords)\n-    Register tmp1_reg       = R5_ARG3;\n-    Register tmp2_reg       = R6_ARG4;\n-    Register zero_reg       = R7_ARG5;\n-\n-    \/\/ Procedure for large arrays (uses data cache block zero instruction).\n-    Label dwloop, fast, fastloop, restloop, lastdword, done;\n-    int cl_size = VM_Version::L1_data_cache_line_size();\n-    int cl_dwords = cl_size >> 3;\n-    int cl_dwordaddr_bits = exact_log2(cl_dwords);\n-    int min_dcbz = 2; \/\/ Needs to be positive, apply dcbz only to at least min_dcbz cache lines.\n-\n-    \/\/ Clear up to 128byte boundary if long enough, dword_cnt=(16-(base>>3))%16.\n-    __ dcbtst(base_ptr_reg);                    \/\/ Indicate write access to first cache line ...\n-    __ andi(tmp2_reg, cnt_dwords_reg, 1);       \/\/ to check if number of dwords is even.\n-    __ srdi_(tmp1_reg, cnt_dwords_reg, 1);      \/\/ number of double dwords\n-    __ load_const_optimized(zero_reg, 0L);      \/\/ Use as zero register.\n-\n-    __ cmpdi(CCR1, tmp2_reg, 0);                \/\/ cnt_dwords even?\n-    __ beq(CCR0, lastdword);                    \/\/ size <= 1\n-    __ mtctr(tmp1_reg);                         \/\/ Speculatively preload counter for rest loop (>0).\n-    __ cmpdi(CCR0, cnt_dwords_reg, (min_dcbz+1)*cl_dwords-1); \/\/ Big enough to ensure >=min_dcbz cache lines are included?\n-    __ neg(tmp1_reg, base_ptr_reg);             \/\/ bit 0..58: bogus, bit 57..60: (16-(base>>3))%16, bit 61..63: 000\n-\n-    __ blt(CCR0, restloop);                     \/\/ Too small. (<31=(2*cl_dwords)-1 is sufficient, but bigger performs better.)\n-    __ rldicl_(tmp1_reg, tmp1_reg, 64-3, 64-cl_dwordaddr_bits); \/\/ Extract number of dwords to 128byte boundary=(16-(base>>3))%16.\n-\n-    __ beq(CCR0, fast);                         \/\/ already 128byte aligned\n-    __ mtctr(tmp1_reg);                         \/\/ Set ctr to hit 128byte boundary (0<ctr<cnt).\n-    __ subf(cnt_dwords_reg, tmp1_reg, cnt_dwords_reg); \/\/ rest (>0 since size>=256-8)\n-\n-    \/\/ Clear in first cache line dword-by-dword if not already 128byte aligned.\n-    __ bind(dwloop);\n-      __ std(zero_reg, 0, base_ptr_reg);        \/\/ Clear 8byte aligned block.\n-      __ addi(base_ptr_reg, base_ptr_reg, 8);\n-    __ bdnz(dwloop);\n-\n-    \/\/ clear 128byte blocks\n-    __ bind(fast);\n-    __ srdi(tmp1_reg, cnt_dwords_reg, cl_dwordaddr_bits); \/\/ loop count for 128byte loop (>0 since size>=256-8)\n-    __ andi(tmp2_reg, cnt_dwords_reg, 1);       \/\/ to check if rest even\n-\n-    __ mtctr(tmp1_reg);                         \/\/ load counter\n-    __ cmpdi(CCR1, tmp2_reg, 0);                \/\/ rest even?\n-    __ rldicl_(tmp1_reg, cnt_dwords_reg, 63, 65-cl_dwordaddr_bits); \/\/ rest in double dwords\n-\n-    __ bind(fastloop);\n-      __ dcbz(base_ptr_reg);                    \/\/ Clear 128byte aligned block.\n-      __ addi(base_ptr_reg, base_ptr_reg, cl_size);\n-    __ bdnz(fastloop);\n-\n-    \/\/__ dcbtst(base_ptr_reg);                  \/\/ Indicate write access to last cache line.\n-    __ beq(CCR0, lastdword);                    \/\/ rest<=1\n-    __ mtctr(tmp1_reg);                         \/\/ load counter\n-\n-    \/\/ Clear rest.\n-    __ bind(restloop);\n-      __ std(zero_reg, 0, base_ptr_reg);        \/\/ Clear 8byte aligned block.\n-      __ std(zero_reg, 8, base_ptr_reg);        \/\/ Clear 8byte aligned block.\n-      __ addi(base_ptr_reg, base_ptr_reg, 16);\n-    __ bdnz(restloop);\n-\n-    __ bind(lastdword);\n-    __ beq(CCR1, done);\n-    __ std(zero_reg, 0, base_ptr_reg);\n-    __ bind(done);\n-    __ blr();                                   \/\/ return\n-\n-    return start;\n-  }\n-\n-#if !defined(PRODUCT)\n-  \/\/ Wrapper which calls oopDesc::is_oop_or_null()\n-  \/\/ Only called by MacroAssembler::verify_oop\n-  static void verify_oop_helper(const char* message, oopDesc* o) {\n-    if (!oopDesc::is_oop_or_null(o)) {\n-      fatal(\"%s. oop: \" PTR_FORMAT, message, p2i(o));\n-    }\n-    ++ StubRoutines::_verify_oop_count;\n-  }\n-#endif\n-\n-  \/\/ Return address of code to be called from code generated by\n-  \/\/ MacroAssembler::verify_oop.\n-  \/\/\n-  \/\/ Don't generate, rather use C++ code.\n-  address generate_verify_oop() {\n-    \/\/ this is actually a `FunctionDescriptor*'.\n-    address start = 0;\n-\n-#if !defined(PRODUCT)\n-    start = CAST_FROM_FN_PTR(address, verify_oop_helper);\n-#endif\n-\n-    return start;\n-  }\n-\n-  \/\/ -XX:+OptimizeFill : convert fill\/copy loops into intrinsic\n-  \/\/\n-  \/\/ The code is implemented(ported from sparc) as we believe it benefits JVM98, however\n-  \/\/ tracing(-XX:+TraceOptimizeFill) shows the intrinsic replacement doesn't happen at all!\n-  \/\/\n-  \/\/ Source code in function is_range_check_if() shows that OptimizeFill relaxed the condition\n-  \/\/ for turning on loop predication optimization, and hence the behavior of \"array range check\"\n-  \/\/ and \"loop invariant check\" could be influenced, which potentially boosted JVM98.\n-  \/\/\n-  \/\/ Generate stub for disjoint short fill. If \"aligned\" is true, the\n-  \/\/ \"to\" address is assumed to be heapword aligned.\n-  \/\/\n-  \/\/ Arguments for generated stub:\n-  \/\/   to:    R3_ARG1\n-  \/\/   value: R4_ARG2\n-  \/\/   count: R5_ARG3 treated as signed\n-  \/\/\n-  address generate_fill(BasicType t, bool aligned, const char* name) {\n-    StubCodeMark mark(this, \"StubRoutines\", name);\n-    address start = __ function_entry();\n-\n-    const Register to    = R3_ARG1;   \/\/ source array address\n-    const Register value = R4_ARG2;   \/\/ fill value\n-    const Register count = R5_ARG3;   \/\/ elements count\n-    const Register temp  = R6_ARG4;   \/\/ temp register\n-\n-    \/\/assert_clean_int(count, O3);    \/\/ Make sure 'count' is clean int.\n-\n-    Label L_exit, L_skip_align1, L_skip_align2, L_fill_byte;\n-    Label L_fill_2_bytes, L_fill_4_bytes, L_fill_elements, L_fill_32_bytes;\n-\n-    int shift = -1;\n-    switch (t) {\n-       case T_BYTE:\n-        shift = 2;\n-        \/\/ Clone bytes (zero extend not needed because store instructions below ignore high order bytes).\n-        __ rldimi(value, value, 8, 48);     \/\/ 8 bit -> 16 bit\n-        __ cmpdi(CCR0, count, 2<<shift);    \/\/ Short arrays (< 8 bytes) fill by element.\n-        __ blt(CCR0, L_fill_elements);\n-        __ rldimi(value, value, 16, 32);    \/\/ 16 bit -> 32 bit\n-        break;\n-       case T_SHORT:\n-        shift = 1;\n-        \/\/ Clone bytes (zero extend not needed because store instructions below ignore high order bytes).\n-        __ rldimi(value, value, 16, 32);    \/\/ 16 bit -> 32 bit\n-        __ cmpdi(CCR0, count, 2<<shift);    \/\/ Short arrays (< 8 bytes) fill by element.\n-        __ blt(CCR0, L_fill_elements);\n-        break;\n-      case T_INT:\n-        shift = 0;\n-        __ cmpdi(CCR0, count, 2<<shift);    \/\/ Short arrays (< 8 bytes) fill by element.\n-        __ blt(CCR0, L_fill_4_bytes);\n-        break;\n-      default: ShouldNotReachHere();\n-    }\n-\n-    if (!aligned && (t == T_BYTE || t == T_SHORT)) {\n-      \/\/ Align source address at 4 bytes address boundary.\n-      if (t == T_BYTE) {\n-        \/\/ One byte misalignment happens only for byte arrays.\n-        __ andi_(temp, to, 1);\n-        __ beq(CCR0, L_skip_align1);\n-        __ stb(value, 0, to);\n-        __ addi(to, to, 1);\n-        __ addi(count, count, -1);\n-        __ bind(L_skip_align1);\n-      }\n-      \/\/ Two bytes misalignment happens only for byte and short (char) arrays.\n-      __ andi_(temp, to, 2);\n-      __ beq(CCR0, L_skip_align2);\n-      __ sth(value, 0, to);\n-      __ addi(to, to, 2);\n-      __ addi(count, count, -(1 << (shift - 1)));\n-      __ bind(L_skip_align2);\n-    }\n-\n-    if (!aligned) {\n-      \/\/ Align to 8 bytes, we know we are 4 byte aligned to start.\n-      __ andi_(temp, to, 7);\n-      __ beq(CCR0, L_fill_32_bytes);\n-      __ stw(value, 0, to);\n-      __ addi(to, to, 4);\n-      __ addi(count, count, -(1 << shift));\n-      __ bind(L_fill_32_bytes);\n-    }\n-\n-    __ li(temp, 8<<shift);                  \/\/ Prepare for 32 byte loop.\n-    \/\/ Clone bytes int->long as above.\n-    __ rldimi(value, value, 32, 0);         \/\/ 32 bit -> 64 bit\n-\n-    Label L_check_fill_8_bytes;\n-    \/\/ Fill 32-byte chunks.\n-    __ subf_(count, temp, count);\n-    __ blt(CCR0, L_check_fill_8_bytes);\n-\n-    Label L_fill_32_bytes_loop;\n-    __ align(32);\n-    __ bind(L_fill_32_bytes_loop);\n-\n-    __ std(value, 0, to);\n-    __ std(value, 8, to);\n-    __ subf_(count, temp, count);           \/\/ Update count.\n-    __ std(value, 16, to);\n-    __ std(value, 24, to);\n-\n-    __ addi(to, to, 32);\n-    __ bge(CCR0, L_fill_32_bytes_loop);\n-\n-    __ bind(L_check_fill_8_bytes);\n-    __ add_(count, temp, count);\n-    __ beq(CCR0, L_exit);\n-    __ addic_(count, count, -(2 << shift));\n-    __ blt(CCR0, L_fill_4_bytes);\n-\n-    \/\/\n-    \/\/ Length is too short, just fill 8 bytes at a time.\n-    \/\/\n-    Label L_fill_8_bytes_loop;\n-    __ bind(L_fill_8_bytes_loop);\n-    __ std(value, 0, to);\n-    __ addic_(count, count, -(2 << shift));\n-    __ addi(to, to, 8);\n-    __ bge(CCR0, L_fill_8_bytes_loop);\n-\n-    \/\/ Fill trailing 4 bytes.\n-    __ bind(L_fill_4_bytes);\n-    __ andi_(temp, count, 1<<shift);\n-    __ beq(CCR0, L_fill_2_bytes);\n-\n-    __ stw(value, 0, to);\n-    if (t == T_BYTE || t == T_SHORT) {\n-      __ addi(to, to, 4);\n-      \/\/ Fill trailing 2 bytes.\n-      __ bind(L_fill_2_bytes);\n-      __ andi_(temp, count, 1<<(shift-1));\n-      __ beq(CCR0, L_fill_byte);\n-      __ sth(value, 0, to);\n-      if (t == T_BYTE) {\n-        __ addi(to, to, 2);\n-        \/\/ Fill trailing byte.\n-        __ bind(L_fill_byte);\n-        __ andi_(count, count, 1);\n-        __ beq(CCR0, L_exit);\n-        __ stb(value, 0, to);\n-      } else {\n-        __ bind(L_fill_byte);\n-      }\n-    } else {\n-      __ bind(L_fill_2_bytes);\n-    }\n-    __ bind(L_exit);\n-    __ blr();\n-\n-    \/\/ Handle copies less than 8 bytes. Int is handled elsewhere.\n-    if (t == T_BYTE) {\n-      __ bind(L_fill_elements);\n-      Label L_fill_2, L_fill_4;\n-      __ andi_(temp, count, 1);\n-      __ beq(CCR0, L_fill_2);\n-      __ stb(value, 0, to);\n-      __ addi(to, to, 1);\n-      __ bind(L_fill_2);\n-      __ andi_(temp, count, 2);\n-      __ beq(CCR0, L_fill_4);\n-      __ stb(value, 0, to);\n-      __ stb(value, 0, to);\n-      __ addi(to, to, 2);\n-      __ bind(L_fill_4);\n-      __ andi_(temp, count, 4);\n-      __ beq(CCR0, L_exit);\n-      __ stb(value, 0, to);\n-      __ stb(value, 1, to);\n-      __ stb(value, 2, to);\n-      __ stb(value, 3, to);\n-      __ blr();\n-    }\n-\n-    if (t == T_SHORT) {\n-      Label L_fill_2;\n-      __ bind(L_fill_elements);\n-      __ andi_(temp, count, 1);\n-      __ beq(CCR0, L_fill_2);\n-      __ sth(value, 0, to);\n-      __ addi(to, to, 2);\n-      __ bind(L_fill_2);\n-      __ andi_(temp, count, 2);\n-      __ beq(CCR0, L_exit);\n-      __ sth(value, 0, to);\n-      __ sth(value, 2, to);\n-      __ blr();\n-    }\n-    return start;\n-  }\n-\n-  inline void assert_positive_int(Register count) {\n-#ifdef ASSERT\n-    __ srdi_(R0, count, 31);\n-    __ asm_assert_eq(\"missing zero extend\");\n-#endif\n-  }\n-\n-  \/\/ Generate overlap test for array copy stubs.\n-  \/\/\n-  \/\/ Input:\n-  \/\/   R3_ARG1    -  from\n-  \/\/   R4_ARG2    -  to\n-  \/\/   R5_ARG3    -  element count\n-  \/\/\n-  void array_overlap_test(address no_overlap_target, int log2_elem_size) {\n-    Register tmp1 = R6_ARG4;\n-    Register tmp2 = R7_ARG5;\n-\n-    assert_positive_int(R5_ARG3);\n-\n-    __ subf(tmp1, R3_ARG1, R4_ARG2); \/\/ distance in bytes\n-    __ sldi(tmp2, R5_ARG3, log2_elem_size); \/\/ size in bytes\n-    __ cmpld(CCR0, R3_ARG1, R4_ARG2); \/\/ Use unsigned comparison!\n-    __ cmpld(CCR1, tmp1, tmp2);\n-    __ crnand(CCR0, Assembler::less, CCR1, Assembler::less);\n-    \/\/ Overlaps if Src before dst and distance smaller than size.\n-    \/\/ Branch to forward copy routine otherwise (within range of 32kB).\n-    __ bc(Assembler::bcondCRbiIs1, Assembler::bi0(CCR0, Assembler::less), no_overlap_target);\n-\n-    \/\/ need to copy backwards\n-  }\n-\n-  \/\/ This is common errorexit stub for UnsafeMemoryAccess.\n-  address generate_unsafecopy_common_error_exit() {\n-    address start_pc = __ pc();\n-    Register tmp1 = R6_ARG4;\n-    \/\/ probably copy stub would have changed value reset it.\n-    if (VM_Version::has_mfdscr()) {\n-      __ load_const_optimized(tmp1, VM_Version::_dscr_val);\n-      __ mtdscr(tmp1);\n-    }\n-    __ li(R3_RET, 0); \/\/ return 0\n-    __ blr();\n-    return start_pc;\n-  }\n-\n-  \/\/ The guideline in the implementations of generate_disjoint_xxx_copy\n-  \/\/ (xxx=byte,short,int,long,oop) is to copy as many elements as possible with\n-  \/\/ single instructions, but to avoid alignment interrupts (see subsequent\n-  \/\/ comment). Furthermore, we try to minimize misaligned access, even\n-  \/\/ though they cause no alignment interrupt.\n-  \/\/\n-  \/\/ In Big-Endian mode, the PowerPC architecture requires implementations to\n-  \/\/ handle automatically misaligned integer halfword and word accesses,\n-  \/\/ word-aligned integer doubleword accesses, and word-aligned floating-point\n-  \/\/ accesses. Other accesses may or may not generate an Alignment interrupt\n-  \/\/ depending on the implementation.\n-  \/\/ Alignment interrupt handling may require on the order of hundreds of cycles,\n-  \/\/ so every effort should be made to avoid misaligned memory values.\n-  \/\/\n-  \/\/\n-  \/\/ Generate stub for disjoint byte copy.  If \"aligned\" is true, the\n-  \/\/ \"from\" and \"to\" addresses are assumed to be heapword aligned.\n-  \/\/\n-  \/\/ Arguments for generated stub:\n-  \/\/      from:  R3_ARG1\n-  \/\/      to:    R4_ARG2\n-  \/\/      count: R5_ARG3 treated as signed\n-  \/\/\n-  address generate_disjoint_byte_copy(bool aligned, const char * name) {\n-    StubCodeMark mark(this, \"StubRoutines\", name);\n-    address start = __ function_entry();\n-    assert_positive_int(R5_ARG3);\n-\n-    Register tmp1 = R6_ARG4;\n-    Register tmp2 = R7_ARG5;\n-    Register tmp3 = R8_ARG6;\n-    Register tmp4 = R9_ARG7;\n-\n-    VectorSRegister tmp_vsr1  = VSR1;\n-    VectorSRegister tmp_vsr2  = VSR2;\n-\n-    Label l_1, l_2, l_3, l_4, l_5, l_6, l_7, l_8, l_9, l_10;\n-    {\n-      \/\/ UnsafeMemoryAccess page error: continue at UnsafeMemoryAccess common_error_exit\n-      UnsafeMemoryAccessMark umam(this, !aligned, false);\n-\n-      \/\/ Don't try anything fancy if arrays don't have many elements.\n-      __ li(tmp3, 0);\n-      __ cmpwi(CCR0, R5_ARG3, 17);\n-      __ ble(CCR0, l_6); \/\/ copy 4 at a time\n-\n-      if (!aligned) {\n-        __ xorr(tmp1, R3_ARG1, R4_ARG2);\n-        __ andi_(tmp1, tmp1, 3);\n-        __ bne(CCR0, l_6); \/\/ If arrays don't have the same alignment mod 4, do 4 element copy.\n-\n-        \/\/ Copy elements if necessary to align to 4 bytes.\n-        __ neg(tmp1, R3_ARG1); \/\/ Compute distance to alignment boundary.\n-        __ andi_(tmp1, tmp1, 3);\n-        __ beq(CCR0, l_2);\n-\n-        __ subf(R5_ARG3, tmp1, R5_ARG3);\n-        __ bind(l_9);\n-        __ lbz(tmp2, 0, R3_ARG1);\n-        __ addic_(tmp1, tmp1, -1);\n-        __ stb(tmp2, 0, R4_ARG2);\n-        __ addi(R3_ARG1, R3_ARG1, 1);\n-        __ addi(R4_ARG2, R4_ARG2, 1);\n-        __ bne(CCR0, l_9);\n-\n-        __ bind(l_2);\n-      }\n-\n-      \/\/ copy 8 elements at a time\n-      __ xorr(tmp2, R3_ARG1, R4_ARG2); \/\/ skip if src & dest have differing alignment mod 8\n-      __ andi_(tmp1, tmp2, 7);\n-      __ bne(CCR0, l_7); \/\/ not same alignment -> to or from is aligned -> copy 8\n-\n-      \/\/ copy a 2-element word if necessary to align to 8 bytes\n-      __ andi_(R0, R3_ARG1, 7);\n-      __ beq(CCR0, l_7);\n-\n-      __ lwzx(tmp2, R3_ARG1, tmp3);\n-      __ addi(R5_ARG3, R5_ARG3, -4);\n-      __ stwx(tmp2, R4_ARG2, tmp3);\n-      { \/\/ FasterArrayCopy\n-        __ addi(R3_ARG1, R3_ARG1, 4);\n-        __ addi(R4_ARG2, R4_ARG2, 4);\n-      }\n-      __ bind(l_7);\n-\n-      { \/\/ FasterArrayCopy\n-        __ cmpwi(CCR0, R5_ARG3, 31);\n-        __ ble(CCR0, l_6); \/\/ copy 2 at a time if less than 32 elements remain\n-\n-        __ srdi(tmp1, R5_ARG3, 5);\n-        __ andi_(R5_ARG3, R5_ARG3, 31);\n-        __ mtctr(tmp1);\n-\n-       if (!VM_Version::has_vsx()) {\n-\n-        __ bind(l_8);\n-        \/\/ Use unrolled version for mass copying (copy 32 elements a time)\n-        \/\/ Load feeding store gets zero latency on Power6, however not on Power5.\n-        \/\/ Therefore, the following sequence is made for the good of both.\n-        __ ld(tmp1, 0, R3_ARG1);\n-        __ ld(tmp2, 8, R3_ARG1);\n-        __ ld(tmp3, 16, R3_ARG1);\n-        __ ld(tmp4, 24, R3_ARG1);\n-        __ std(tmp1, 0, R4_ARG2);\n-        __ std(tmp2, 8, R4_ARG2);\n-        __ std(tmp3, 16, R4_ARG2);\n-        __ std(tmp4, 24, R4_ARG2);\n-        __ addi(R3_ARG1, R3_ARG1, 32);\n-        __ addi(R4_ARG2, R4_ARG2, 32);\n-        __ bdnz(l_8);\n-\n-      } else { \/\/ Processor supports VSX, so use it to mass copy.\n-\n-        \/\/ Prefetch the data into the L2 cache.\n-        __ dcbt(R3_ARG1, 0);\n-\n-        \/\/ If supported set DSCR pre-fetch to deepest.\n-        if (VM_Version::has_mfdscr()) {\n-          __ load_const_optimized(tmp2, VM_Version::_dscr_val | 7);\n-          __ mtdscr(tmp2);\n-        }\n-\n-        __ li(tmp1, 16);\n-\n-        \/\/ Backbranch target aligned to 32-byte. Not 16-byte align as\n-        \/\/ loop contains < 8 instructions that fit inside a single\n-        \/\/ i-cache sector.\n-        __ align(32);\n-\n-        __ bind(l_10);\n-        \/\/ Use loop with VSX load\/store instructions to\n-        \/\/ copy 32 elements a time.\n-        __ lxvd2x(tmp_vsr1, R3_ARG1);        \/\/ Load src\n-        __ stxvd2x(tmp_vsr1, R4_ARG2);       \/\/ Store to dst\n-        __ lxvd2x(tmp_vsr2, tmp1, R3_ARG1);  \/\/ Load src + 16\n-        __ stxvd2x(tmp_vsr2, tmp1, R4_ARG2); \/\/ Store to dst + 16\n-        __ addi(R3_ARG1, R3_ARG1, 32);       \/\/ Update src+=32\n-        __ addi(R4_ARG2, R4_ARG2, 32);       \/\/ Update dsc+=32\n-        __ bdnz(l_10);                       \/\/ Dec CTR and loop if not zero.\n-\n-        \/\/ Restore DSCR pre-fetch value.\n-        if (VM_Version::has_mfdscr()) {\n-          __ load_const_optimized(tmp2, VM_Version::_dscr_val);\n-          __ mtdscr(tmp2);\n-        }\n-\n-      } \/\/ VSX\n-     } \/\/ FasterArrayCopy\n-\n-      __ bind(l_6);\n-\n-      \/\/ copy 4 elements at a time\n-      __ cmpwi(CCR0, R5_ARG3, 4);\n-      __ blt(CCR0, l_1);\n-      __ srdi(tmp1, R5_ARG3, 2);\n-      __ mtctr(tmp1); \/\/ is > 0\n-      __ andi_(R5_ARG3, R5_ARG3, 3);\n-\n-      { \/\/ FasterArrayCopy\n-        __ addi(R3_ARG1, R3_ARG1, -4);\n-        __ addi(R4_ARG2, R4_ARG2, -4);\n-        __ bind(l_3);\n-        __ lwzu(tmp2, 4, R3_ARG1);\n-        __ stwu(tmp2, 4, R4_ARG2);\n-        __ bdnz(l_3);\n-        __ addi(R3_ARG1, R3_ARG1, 4);\n-        __ addi(R4_ARG2, R4_ARG2, 4);\n-      }\n-\n-      \/\/ do single element copy\n-      __ bind(l_1);\n-      __ cmpwi(CCR0, R5_ARG3, 0);\n-      __ beq(CCR0, l_4);\n-\n-      { \/\/ FasterArrayCopy\n-        __ mtctr(R5_ARG3);\n-        __ addi(R3_ARG1, R3_ARG1, -1);\n-        __ addi(R4_ARG2, R4_ARG2, -1);\n-\n-        __ bind(l_5);\n-        __ lbzu(tmp2, 1, R3_ARG1);\n-        __ stbu(tmp2, 1, R4_ARG2);\n-        __ bdnz(l_5);\n-      }\n-    }\n-\n-    __ bind(l_4);\n-    __ li(R3_RET, 0); \/\/ return 0\n-    __ blr();\n-\n-    return start;\n-  }\n-\n-  \/\/ Generate stub for conjoint byte copy.  If \"aligned\" is true, the\n-  \/\/ \"from\" and \"to\" addresses are assumed to be heapword aligned.\n-  \/\/\n-  \/\/ Arguments for generated stub:\n-  \/\/      from:  R3_ARG1\n-  \/\/      to:    R4_ARG2\n-  \/\/      count: R5_ARG3 treated as signed\n-  \/\/\n-  address generate_conjoint_byte_copy(bool aligned, const char * name) {\n-    StubCodeMark mark(this, \"StubRoutines\", name);\n-    address start = __ function_entry();\n-    assert_positive_int(R5_ARG3);\n-\n-    Register tmp1 = R6_ARG4;\n-    Register tmp2 = R7_ARG5;\n-    Register tmp3 = R8_ARG6;\n-\n-    address nooverlap_target = aligned ?\n-      STUB_ENTRY(arrayof_jbyte_disjoint_arraycopy()) :\n-      STUB_ENTRY(jbyte_disjoint_arraycopy());\n-\n-    array_overlap_test(nooverlap_target, 0);\n-    \/\/ Do reverse copy. We assume the case of actual overlap is rare enough\n-    \/\/ that we don't have to optimize it.\n-    Label l_1, l_2;\n-    {\n-      \/\/ UnsafeMemoryAccess page error: continue at UnsafeMemoryAccess common_error_exit\n-      UnsafeMemoryAccessMark umam(this, !aligned, false);\n-      __ b(l_2);\n-      __ bind(l_1);\n-      __ stbx(tmp1, R4_ARG2, R5_ARG3);\n-      __ bind(l_2);\n-      __ addic_(R5_ARG3, R5_ARG3, -1);\n-      __ lbzx(tmp1, R3_ARG1, R5_ARG3);\n-      __ bge(CCR0, l_1);\n-    }\n-    __ li(R3_RET, 0); \/\/ return 0\n-    __ blr();\n-\n-    return start;\n-  }\n-\n-  \/\/ Generate stub for disjoint short copy.  If \"aligned\" is true, the\n-  \/\/ \"from\" and \"to\" addresses are assumed to be heapword aligned.\n-  \/\/\n-  \/\/ Arguments for generated stub:\n-  \/\/      from:  R3_ARG1\n-  \/\/      to:    R4_ARG2\n-  \/\/  elm.count: R5_ARG3 treated as signed\n-  \/\/\n-  \/\/ Strategy for aligned==true:\n-  \/\/\n-  \/\/  If length <= 9:\n-  \/\/     1. copy 2 elements at a time (l_6)\n-  \/\/     2. copy last element if original element count was odd (l_1)\n-  \/\/\n-  \/\/  If length > 9:\n-  \/\/     1. copy 4 elements at a time until less than 4 elements are left (l_7)\n-  \/\/     2. copy 2 elements at a time until less than 2 elements are left (l_6)\n-  \/\/     3. copy last element if one was left in step 2. (l_1)\n-  \/\/\n-  \/\/\n-  \/\/ Strategy for aligned==false:\n-  \/\/\n-  \/\/  If length <= 9: same as aligned==true case, but NOTE: load\/stores\n-  \/\/                  can be unaligned (see comment below)\n-  \/\/\n-  \/\/  If length > 9:\n-  \/\/     1. continue with step 6. if the alignment of from and to mod 4\n-  \/\/        is different.\n-  \/\/     2. align from and to to 4 bytes by copying 1 element if necessary\n-  \/\/     3. at l_2 from and to are 4 byte aligned; continue with\n-  \/\/        5. if they cannot be aligned to 8 bytes because they have\n-  \/\/        got different alignment mod 8.\n-  \/\/     4. at this point we know that both, from and to, have the same\n-  \/\/        alignment mod 8, now copy one element if necessary to get\n-  \/\/        8 byte alignment of from and to.\n-  \/\/     5. copy 4 elements at a time until less than 4 elements are\n-  \/\/        left; depending on step 3. all load\/stores are aligned or\n-  \/\/        either all loads or all stores are unaligned.\n-  \/\/     6. copy 2 elements at a time until less than 2 elements are\n-  \/\/        left (l_6); arriving here from step 1., there is a chance\n-  \/\/        that all accesses are unaligned.\n-  \/\/     7. copy last element if one was left in step 6. (l_1)\n-  \/\/\n-  \/\/  There are unaligned data accesses using integer load\/store\n-  \/\/  instructions in this stub. POWER allows such accesses.\n-  \/\/\n-  \/\/  According to the manuals (PowerISA_V2.06_PUBLIC, Book II,\n-  \/\/  Chapter 2: Effect of Operand Placement on Performance) unaligned\n-  \/\/  integer load\/stores have good performance. Only unaligned\n-  \/\/  floating point load\/stores can have poor performance.\n-  \/\/\n-  \/\/  TODO:\n-  \/\/\n-  \/\/  1. check if aligning the backbranch target of loops is beneficial\n-  \/\/\n-  address generate_disjoint_short_copy(bool aligned, const char * name) {\n-    StubCodeMark mark(this, \"StubRoutines\", name);\n-\n-    Register tmp1 = R6_ARG4;\n-    Register tmp2 = R7_ARG5;\n-    Register tmp3 = R8_ARG6;\n-    Register tmp4 = R9_ARG7;\n-\n-    VectorSRegister tmp_vsr1  = VSR1;\n-    VectorSRegister tmp_vsr2  = VSR2;\n-\n-    address start = __ function_entry();\n-    assert_positive_int(R5_ARG3);\n-\n-    Label l_1, l_2, l_3, l_4, l_5, l_6, l_7, l_8, l_9;\n-    {\n-      \/\/ UnsafeMemoryAccess page error: continue at UnsafeMemoryAccess common_error_exit\n-      UnsafeMemoryAccessMark umam(this, !aligned, false);\n-      \/\/ don't try anything fancy if arrays don't have many elements\n-      __ li(tmp3, 0);\n-      __ cmpwi(CCR0, R5_ARG3, 9);\n-      __ ble(CCR0, l_6); \/\/ copy 2 at a time\n-\n-      if (!aligned) {\n-        __ xorr(tmp1, R3_ARG1, R4_ARG2);\n-        __ andi_(tmp1, tmp1, 3);\n-        __ bne(CCR0, l_6); \/\/ if arrays don't have the same alignment mod 4, do 2 element copy\n-\n-        \/\/ At this point it is guaranteed that both, from and to have the same alignment mod 4.\n-\n-        \/\/ Copy 1 element if necessary to align to 4 bytes.\n-        __ andi_(tmp1, R3_ARG1, 3);\n-        __ beq(CCR0, l_2);\n-\n-        __ lhz(tmp2, 0, R3_ARG1);\n-        __ addi(R3_ARG1, R3_ARG1, 2);\n-        __ sth(tmp2, 0, R4_ARG2);\n-        __ addi(R4_ARG2, R4_ARG2, 2);\n-        __ addi(R5_ARG3, R5_ARG3, -1);\n-        __ bind(l_2);\n-\n-        \/\/ At this point the positions of both, from and to, are at least 4 byte aligned.\n-\n-        \/\/ Copy 4 elements at a time.\n-        \/\/ Align to 8 bytes, but only if both, from and to, have same alignment mod 8.\n-        __ xorr(tmp2, R3_ARG1, R4_ARG2);\n-        __ andi_(tmp1, tmp2, 7);\n-        __ bne(CCR0, l_7); \/\/ not same alignment mod 8 -> copy 4, either from or to will be unaligned\n-\n-        \/\/ Copy a 2-element word if necessary to align to 8 bytes.\n-        __ andi_(R0, R3_ARG1, 7);\n-        __ beq(CCR0, l_7);\n-\n-        __ lwzx(tmp2, R3_ARG1, tmp3);\n-        __ addi(R5_ARG3, R5_ARG3, -2);\n-        __ stwx(tmp2, R4_ARG2, tmp3);\n-        { \/\/ FasterArrayCopy\n-          __ addi(R3_ARG1, R3_ARG1, 4);\n-          __ addi(R4_ARG2, R4_ARG2, 4);\n-        }\n-      }\n-\n-      __ bind(l_7);\n-\n-      \/\/ Copy 4 elements at a time; either the loads or the stores can\n-      \/\/ be unaligned if aligned == false.\n-\n-      { \/\/ FasterArrayCopy\n-        __ cmpwi(CCR0, R5_ARG3, 15);\n-        __ ble(CCR0, l_6); \/\/ copy 2 at a time if less than 16 elements remain\n-\n-        __ srdi(tmp1, R5_ARG3, 4);\n-        __ andi_(R5_ARG3, R5_ARG3, 15);\n-        __ mtctr(tmp1);\n-\n-        if (!VM_Version::has_vsx()) {\n-\n-          __ bind(l_8);\n-          \/\/ Use unrolled version for mass copying (copy 16 elements a time).\n-          \/\/ Load feeding store gets zero latency on Power6, however not on Power5.\n-          \/\/ Therefore, the following sequence is made for the good of both.\n-          __ ld(tmp1, 0, R3_ARG1);\n-          __ ld(tmp2, 8, R3_ARG1);\n-          __ ld(tmp3, 16, R3_ARG1);\n-          __ ld(tmp4, 24, R3_ARG1);\n-          __ std(tmp1, 0, R4_ARG2);\n-          __ std(tmp2, 8, R4_ARG2);\n-          __ std(tmp3, 16, R4_ARG2);\n-          __ std(tmp4, 24, R4_ARG2);\n-          __ addi(R3_ARG1, R3_ARG1, 32);\n-          __ addi(R4_ARG2, R4_ARG2, 32);\n-          __ bdnz(l_8);\n-\n-        } else { \/\/ Processor supports VSX, so use it to mass copy.\n-\n-          \/\/ Prefetch src data into L2 cache.\n-          __ dcbt(R3_ARG1, 0);\n-\n-          \/\/ If supported set DSCR pre-fetch to deepest.\n-          if (VM_Version::has_mfdscr()) {\n-            __ load_const_optimized(tmp2, VM_Version::_dscr_val | 7);\n-            __ mtdscr(tmp2);\n-          }\n-          __ li(tmp1, 16);\n-\n-          \/\/ Backbranch target aligned to 32-byte. It's not aligned 16-byte\n-          \/\/ as loop contains < 8 instructions that fit inside a single\n-          \/\/ i-cache sector.\n-          __ align(32);\n-\n-          __ bind(l_9);\n-          \/\/ Use loop with VSX load\/store instructions to\n-          \/\/ copy 16 elements a time.\n-          __ lxvd2x(tmp_vsr1, R3_ARG1);        \/\/ Load from src.\n-          __ stxvd2x(tmp_vsr1, R4_ARG2);       \/\/ Store to dst.\n-          __ lxvd2x(tmp_vsr2, R3_ARG1, tmp1);  \/\/ Load from src + 16.\n-          __ stxvd2x(tmp_vsr2, R4_ARG2, tmp1); \/\/ Store to dst + 16.\n-          __ addi(R3_ARG1, R3_ARG1, 32);       \/\/ Update src+=32.\n-          __ addi(R4_ARG2, R4_ARG2, 32);       \/\/ Update dsc+=32.\n-          __ bdnz(l_9);                        \/\/ Dec CTR and loop if not zero.\n-\n-          \/\/ Restore DSCR pre-fetch value.\n-          if (VM_Version::has_mfdscr()) {\n-            __ load_const_optimized(tmp2, VM_Version::_dscr_val);\n-            __ mtdscr(tmp2);\n-          }\n-\n-        }\n-      } \/\/ FasterArrayCopy\n-      __ bind(l_6);\n-\n-      \/\/ copy 2 elements at a time\n-      { \/\/ FasterArrayCopy\n-        __ cmpwi(CCR0, R5_ARG3, 2);\n-        __ blt(CCR0, l_1);\n-        __ srdi(tmp1, R5_ARG3, 1);\n-        __ andi_(R5_ARG3, R5_ARG3, 1);\n-\n-        __ addi(R3_ARG1, R3_ARG1, -4);\n-        __ addi(R4_ARG2, R4_ARG2, -4);\n-        __ mtctr(tmp1);\n-\n-        __ bind(l_3);\n-        __ lwzu(tmp2, 4, R3_ARG1);\n-        __ stwu(tmp2, 4, R4_ARG2);\n-        __ bdnz(l_3);\n-\n-        __ addi(R3_ARG1, R3_ARG1, 4);\n-        __ addi(R4_ARG2, R4_ARG2, 4);\n-      }\n-\n-      \/\/ do single element copy\n-      __ bind(l_1);\n-      __ cmpwi(CCR0, R5_ARG3, 0);\n-      __ beq(CCR0, l_4);\n-\n-      { \/\/ FasterArrayCopy\n-        __ mtctr(R5_ARG3);\n-        __ addi(R3_ARG1, R3_ARG1, -2);\n-        __ addi(R4_ARG2, R4_ARG2, -2);\n-\n-        __ bind(l_5);\n-        __ lhzu(tmp2, 2, R3_ARG1);\n-        __ sthu(tmp2, 2, R4_ARG2);\n-        __ bdnz(l_5);\n-      }\n-    }\n-\n-    __ bind(l_4);\n-    __ li(R3_RET, 0); \/\/ return 0\n-    __ blr();\n-\n-    return start;\n-  }\n-\n-  \/\/ Generate stub for conjoint short copy.  If \"aligned\" is true, the\n-  \/\/ \"from\" and \"to\" addresses are assumed to be heapword aligned.\n-  \/\/\n-  \/\/ Arguments for generated stub:\n-  \/\/      from:  R3_ARG1\n-  \/\/      to:    R4_ARG2\n-  \/\/      count: R5_ARG3 treated as signed\n-  \/\/\n-  address generate_conjoint_short_copy(bool aligned, const char * name) {\n-    StubCodeMark mark(this, \"StubRoutines\", name);\n-    address start = __ function_entry();\n-    assert_positive_int(R5_ARG3);\n-\n-    Register tmp1 = R6_ARG4;\n-    Register tmp2 = R7_ARG5;\n-    Register tmp3 = R8_ARG6;\n-\n-    address nooverlap_target = aligned ?\n-      STUB_ENTRY(arrayof_jshort_disjoint_arraycopy()) :\n-      STUB_ENTRY(jshort_disjoint_arraycopy());\n-\n-    array_overlap_test(nooverlap_target, 1);\n-\n-    Label l_1, l_2;\n-    {\n-      \/\/ UnsafeMemoryAccess page error: continue at UnsafeMemoryAccess common_error_exit\n-      UnsafeMemoryAccessMark umam(this, !aligned, false);\n-      __ sldi(tmp1, R5_ARG3, 1);\n-      __ b(l_2);\n-      __ bind(l_1);\n-      __ sthx(tmp2, R4_ARG2, tmp1);\n-      __ bind(l_2);\n-      __ addic_(tmp1, tmp1, -2);\n-      __ lhzx(tmp2, R3_ARG1, tmp1);\n-      __ bge(CCR0, l_1);\n-    }\n-    __ li(R3_RET, 0); \/\/ return 0\n-    __ blr();\n-\n-    return start;\n-  }\n-\n-  \/\/ Generate core code for disjoint int copy (and oop copy on 32-bit).  If \"aligned\"\n-  \/\/ is true, the \"from\" and \"to\" addresses are assumed to be heapword aligned.\n-  \/\/\n-  \/\/ Arguments:\n-  \/\/      from:  R3_ARG1\n-  \/\/      to:    R4_ARG2\n-  \/\/      count: R5_ARG3 treated as signed\n-  \/\/\n-  void generate_disjoint_int_copy_core(bool aligned) {\n-    Register tmp1 = R6_ARG4;\n-    Register tmp2 = R7_ARG5;\n-    Register tmp3 = R8_ARG6;\n-    Register tmp4 = R0;\n-\n-    VectorSRegister tmp_vsr1  = VSR1;\n-    VectorSRegister tmp_vsr2  = VSR2;\n-\n-    Label l_1, l_2, l_3, l_4, l_5, l_6, l_7;\n-\n-    \/\/ for short arrays, just do single element copy\n-    __ li(tmp3, 0);\n-    __ cmpwi(CCR0, R5_ARG3, 5);\n-    __ ble(CCR0, l_2);\n-\n-    if (!aligned) {\n-        \/\/ check if arrays have same alignment mod 8.\n-        __ xorr(tmp1, R3_ARG1, R4_ARG2);\n-        __ andi_(R0, tmp1, 7);\n-        \/\/ Not the same alignment, but ld and std just need to be 4 byte aligned.\n-        __ bne(CCR0, l_4); \/\/ to OR from is 8 byte aligned -> copy 2 at a time\n-\n-        \/\/ copy 1 element to align to and from on an 8 byte boundary\n-        __ andi_(R0, R3_ARG1, 7);\n-        __ beq(CCR0, l_4);\n-\n-        __ lwzx(tmp2, R3_ARG1, tmp3);\n-        __ addi(R5_ARG3, R5_ARG3, -1);\n-        __ stwx(tmp2, R4_ARG2, tmp3);\n-        { \/\/ FasterArrayCopy\n-          __ addi(R3_ARG1, R3_ARG1, 4);\n-          __ addi(R4_ARG2, R4_ARG2, 4);\n-        }\n-        __ bind(l_4);\n-      }\n-\n-    { \/\/ FasterArrayCopy\n-      __ cmpwi(CCR0, R5_ARG3, 7);\n-      __ ble(CCR0, l_2); \/\/ copy 1 at a time if less than 8 elements remain\n-\n-      __ srdi(tmp1, R5_ARG3, 3);\n-      __ andi_(R5_ARG3, R5_ARG3, 7);\n-      __ mtctr(tmp1);\n-\n-     if (!VM_Version::has_vsx()) {\n-\n-      __ bind(l_6);\n-      \/\/ Use unrolled version for mass copying (copy 8 elements a time).\n-      \/\/ Load feeding store gets zero latency on power6, however not on power 5.\n-      \/\/ Therefore, the following sequence is made for the good of both.\n-      __ ld(tmp1, 0, R3_ARG1);\n-      __ ld(tmp2, 8, R3_ARG1);\n-      __ ld(tmp3, 16, R3_ARG1);\n-      __ ld(tmp4, 24, R3_ARG1);\n-      __ std(tmp1, 0, R4_ARG2);\n-      __ std(tmp2, 8, R4_ARG2);\n-      __ std(tmp3, 16, R4_ARG2);\n-      __ std(tmp4, 24, R4_ARG2);\n-      __ addi(R3_ARG1, R3_ARG1, 32);\n-      __ addi(R4_ARG2, R4_ARG2, 32);\n-      __ bdnz(l_6);\n-\n-    } else { \/\/ Processor supports VSX, so use it to mass copy.\n-\n-      \/\/ Prefetch the data into the L2 cache.\n-      __ dcbt(R3_ARG1, 0);\n-\n-      \/\/ If supported set DSCR pre-fetch to deepest.\n-      if (VM_Version::has_mfdscr()) {\n-        __ load_const_optimized(tmp2, VM_Version::_dscr_val | 7);\n-        __ mtdscr(tmp2);\n-      }\n-\n-      __ li(tmp1, 16);\n-\n-      \/\/ Backbranch target aligned to 32-byte. Not 16-byte align as\n-      \/\/ loop contains < 8 instructions that fit inside a single\n-      \/\/ i-cache sector.\n-      __ align(32);\n-\n-      __ bind(l_7);\n-      \/\/ Use loop with VSX load\/store instructions to\n-      \/\/ copy 8 elements a time.\n-      __ lxvd2x(tmp_vsr1, R3_ARG1);        \/\/ Load src\n-      __ stxvd2x(tmp_vsr1, R4_ARG2);       \/\/ Store to dst\n-      __ lxvd2x(tmp_vsr2, tmp1, R3_ARG1);  \/\/ Load src + 16\n-      __ stxvd2x(tmp_vsr2, tmp1, R4_ARG2); \/\/ Store to dst + 16\n-      __ addi(R3_ARG1, R3_ARG1, 32);       \/\/ Update src+=32\n-      __ addi(R4_ARG2, R4_ARG2, 32);       \/\/ Update dsc+=32\n-      __ bdnz(l_7);                        \/\/ Dec CTR and loop if not zero.\n-\n-      \/\/ Restore DSCR pre-fetch value.\n-      if (VM_Version::has_mfdscr()) {\n-        __ load_const_optimized(tmp2, VM_Version::_dscr_val);\n-        __ mtdscr(tmp2);\n-      }\n-\n-    } \/\/ VSX\n-   } \/\/ FasterArrayCopy\n-\n-    \/\/ copy 1 element at a time\n-    __ bind(l_2);\n-    __ cmpwi(CCR0, R5_ARG3, 0);\n-    __ beq(CCR0, l_1);\n-\n-    { \/\/ FasterArrayCopy\n-      __ mtctr(R5_ARG3);\n-      __ addi(R3_ARG1, R3_ARG1, -4);\n-      __ addi(R4_ARG2, R4_ARG2, -4);\n-\n-      __ bind(l_3);\n-      __ lwzu(tmp2, 4, R3_ARG1);\n-      __ stwu(tmp2, 4, R4_ARG2);\n-      __ bdnz(l_3);\n-    }\n-\n-    __ bind(l_1);\n-    return;\n-  }\n-\n-  \/\/ Generate stub for disjoint int copy.  If \"aligned\" is true, the\n-  \/\/ \"from\" and \"to\" addresses are assumed to be heapword aligned.\n-  \/\/\n-  \/\/ Arguments for generated stub:\n-  \/\/      from:  R3_ARG1\n-  \/\/      to:    R4_ARG2\n-  \/\/      count: R5_ARG3 treated as signed\n-  \/\/\n-  address generate_disjoint_int_copy(bool aligned, const char * name) {\n-    StubCodeMark mark(this, \"StubRoutines\", name);\n-    address start = __ function_entry();\n-    assert_positive_int(R5_ARG3);\n-    {\n-      \/\/ UnsafeMemoryAccess page error: continue at UnsafeMemoryAccess common_error_exit\n-      UnsafeMemoryAccessMark umam(this, !aligned, false);\n-      generate_disjoint_int_copy_core(aligned);\n-    }\n-    __ li(R3_RET, 0); \/\/ return 0\n-    __ blr();\n-    return start;\n-  }\n-\n-  \/\/ Generate core code for conjoint int copy (and oop copy on\n-  \/\/ 32-bit).  If \"aligned\" is true, the \"from\" and \"to\" addresses\n-  \/\/ are assumed to be heapword aligned.\n-  \/\/\n-  \/\/ Arguments:\n-  \/\/      from:  R3_ARG1\n-  \/\/      to:    R4_ARG2\n-  \/\/      count: R5_ARG3 treated as signed\n-  \/\/\n-  void generate_conjoint_int_copy_core(bool aligned) {\n-    \/\/ Do reverse copy.  We assume the case of actual overlap is rare enough\n-    \/\/ that we don't have to optimize it.\n-\n-    Label l_1, l_2, l_3, l_4, l_5, l_6, l_7;\n-\n-    Register tmp1 = R6_ARG4;\n-    Register tmp2 = R7_ARG5;\n-    Register tmp3 = R8_ARG6;\n-    Register tmp4 = R0;\n-\n-    VectorSRegister tmp_vsr1  = VSR1;\n-    VectorSRegister tmp_vsr2  = VSR2;\n-\n-    { \/\/ FasterArrayCopy\n-      __ cmpwi(CCR0, R5_ARG3, 0);\n-      __ beq(CCR0, l_6);\n-\n-      __ sldi(R5_ARG3, R5_ARG3, 2);\n-      __ add(R3_ARG1, R3_ARG1, R5_ARG3);\n-      __ add(R4_ARG2, R4_ARG2, R5_ARG3);\n-      __ srdi(R5_ARG3, R5_ARG3, 2);\n-\n-      if (!aligned) {\n-        \/\/ check if arrays have same alignment mod 8.\n-        __ xorr(tmp1, R3_ARG1, R4_ARG2);\n-        __ andi_(R0, tmp1, 7);\n-        \/\/ Not the same alignment, but ld and std just need to be 4 byte aligned.\n-        __ bne(CCR0, l_7); \/\/ to OR from is 8 byte aligned -> copy 2 at a time\n-\n-        \/\/ copy 1 element to align to and from on an 8 byte boundary\n-        __ andi_(R0, R3_ARG1, 7);\n-        __ beq(CCR0, l_7);\n-\n-        __ addi(R3_ARG1, R3_ARG1, -4);\n-        __ addi(R4_ARG2, R4_ARG2, -4);\n-        __ addi(R5_ARG3, R5_ARG3, -1);\n-        __ lwzx(tmp2, R3_ARG1);\n-        __ stwx(tmp2, R4_ARG2);\n-        __ bind(l_7);\n-      }\n-\n-      __ cmpwi(CCR0, R5_ARG3, 7);\n-      __ ble(CCR0, l_5); \/\/ copy 1 at a time if less than 8 elements remain\n-\n-      __ srdi(tmp1, R5_ARG3, 3);\n-      __ andi(R5_ARG3, R5_ARG3, 7);\n-      __ mtctr(tmp1);\n-\n-     if (!VM_Version::has_vsx()) {\n-      __ bind(l_4);\n-      \/\/ Use unrolled version for mass copying (copy 4 elements a time).\n-      \/\/ Load feeding store gets zero latency on Power6, however not on Power5.\n-      \/\/ Therefore, the following sequence is made for the good of both.\n-      __ addi(R3_ARG1, R3_ARG1, -32);\n-      __ addi(R4_ARG2, R4_ARG2, -32);\n-      __ ld(tmp4, 24, R3_ARG1);\n-      __ ld(tmp3, 16, R3_ARG1);\n-      __ ld(tmp2, 8, R3_ARG1);\n-      __ ld(tmp1, 0, R3_ARG1);\n-      __ std(tmp4, 24, R4_ARG2);\n-      __ std(tmp3, 16, R4_ARG2);\n-      __ std(tmp2, 8, R4_ARG2);\n-      __ std(tmp1, 0, R4_ARG2);\n-      __ bdnz(l_4);\n-     } else {  \/\/ Processor supports VSX, so use it to mass copy.\n-      \/\/ Prefetch the data into the L2 cache.\n-      __ dcbt(R3_ARG1, 0);\n-\n-      \/\/ If supported set DSCR pre-fetch to deepest.\n-      if (VM_Version::has_mfdscr()) {\n-        __ load_const_optimized(tmp2, VM_Version::_dscr_val | 7);\n-        __ mtdscr(tmp2);\n-      }\n-\n-      __ li(tmp1, 16);\n-\n-      \/\/ Backbranch target aligned to 32-byte. Not 16-byte align as\n-      \/\/ loop contains < 8 instructions that fit inside a single\n-      \/\/ i-cache sector.\n-      __ align(32);\n-\n-      __ bind(l_4);\n-      \/\/ Use loop with VSX load\/store instructions to\n-      \/\/ copy 8 elements a time.\n-      __ addi(R3_ARG1, R3_ARG1, -32);      \/\/ Update src-=32\n-      __ addi(R4_ARG2, R4_ARG2, -32);      \/\/ Update dsc-=32\n-      __ lxvd2x(tmp_vsr2, tmp1, R3_ARG1);  \/\/ Load src+16\n-      __ lxvd2x(tmp_vsr1, R3_ARG1);        \/\/ Load src\n-      __ stxvd2x(tmp_vsr2, tmp1, R4_ARG2); \/\/ Store to dst+16\n-      __ stxvd2x(tmp_vsr1, R4_ARG2);       \/\/ Store to dst\n-      __ bdnz(l_4);\n-\n-      \/\/ Restore DSCR pre-fetch value.\n-      if (VM_Version::has_mfdscr()) {\n-        __ load_const_optimized(tmp2, VM_Version::_dscr_val);\n-        __ mtdscr(tmp2);\n-      }\n-     }\n-\n-      __ cmpwi(CCR0, R5_ARG3, 0);\n-      __ beq(CCR0, l_6);\n-\n-      __ bind(l_5);\n-      __ mtctr(R5_ARG3);\n-      __ bind(l_3);\n-      __ lwz(R0, -4, R3_ARG1);\n-      __ stw(R0, -4, R4_ARG2);\n-      __ addi(R3_ARG1, R3_ARG1, -4);\n-      __ addi(R4_ARG2, R4_ARG2, -4);\n-      __ bdnz(l_3);\n-\n-      __ bind(l_6);\n-    }\n-  }\n-\n-  \/\/ Generate stub for conjoint int copy.  If \"aligned\" is true, the\n-  \/\/ \"from\" and \"to\" addresses are assumed to be heapword aligned.\n-  \/\/\n-  \/\/ Arguments for generated stub:\n-  \/\/      from:  R3_ARG1\n-  \/\/      to:    R4_ARG2\n-  \/\/      count: R5_ARG3 treated as signed\n-  \/\/\n-  address generate_conjoint_int_copy(bool aligned, const char * name) {\n-    StubCodeMark mark(this, \"StubRoutines\", name);\n-    address start = __ function_entry();\n-    assert_positive_int(R5_ARG3);\n-    address nooverlap_target = aligned ?\n-      STUB_ENTRY(arrayof_jint_disjoint_arraycopy()) :\n-      STUB_ENTRY(jint_disjoint_arraycopy());\n-\n-    array_overlap_test(nooverlap_target, 2);\n-    {\n-      \/\/ UnsafeMemoryAccess page error: continue at UnsafeMemoryAccess common_error_exit\n-      UnsafeMemoryAccessMark umam(this, !aligned, false);\n-      generate_conjoint_int_copy_core(aligned);\n-    }\n-\n-    __ li(R3_RET, 0); \/\/ return 0\n-    __ blr();\n-\n-    return start;\n-  }\n-\n-  \/\/ Generate core code for disjoint long copy (and oop copy on\n-  \/\/ 64-bit).  If \"aligned\" is true, the \"from\" and \"to\" addresses\n-  \/\/ are assumed to be heapword aligned.\n-  \/\/\n-  \/\/ Arguments:\n-  \/\/      from:  R3_ARG1\n-  \/\/      to:    R4_ARG2\n-  \/\/      count: R5_ARG3 treated as signed\n-  \/\/\n-  void generate_disjoint_long_copy_core(bool aligned) {\n-    Register tmp1 = R6_ARG4;\n-    Register tmp2 = R7_ARG5;\n-    Register tmp3 = R8_ARG6;\n-    Register tmp4 = R0;\n-\n-    Label l_1, l_2, l_3, l_4, l_5;\n-\n-    VectorSRegister tmp_vsr1  = VSR1;\n-    VectorSRegister tmp_vsr2  = VSR2;\n-\n-    { \/\/ FasterArrayCopy\n-      __ cmpwi(CCR0, R5_ARG3, 3);\n-      __ ble(CCR0, l_3); \/\/ copy 1 at a time if less than 4 elements remain\n-\n-      __ srdi(tmp1, R5_ARG3, 2);\n-      __ andi_(R5_ARG3, R5_ARG3, 3);\n-      __ mtctr(tmp1);\n-\n-    if (!VM_Version::has_vsx()) {\n-      __ bind(l_4);\n-      \/\/ Use unrolled version for mass copying (copy 4 elements a time).\n-      \/\/ Load feeding store gets zero latency on Power6, however not on Power5.\n-      \/\/ Therefore, the following sequence is made for the good of both.\n-      __ ld(tmp1, 0, R3_ARG1);\n-      __ ld(tmp2, 8, R3_ARG1);\n-      __ ld(tmp3, 16, R3_ARG1);\n-      __ ld(tmp4, 24, R3_ARG1);\n-      __ std(tmp1, 0, R4_ARG2);\n-      __ std(tmp2, 8, R4_ARG2);\n-      __ std(tmp3, 16, R4_ARG2);\n-      __ std(tmp4, 24, R4_ARG2);\n-      __ addi(R3_ARG1, R3_ARG1, 32);\n-      __ addi(R4_ARG2, R4_ARG2, 32);\n-      __ bdnz(l_4);\n-\n-    } else { \/\/ Processor supports VSX, so use it to mass copy.\n-\n-      \/\/ Prefetch the data into the L2 cache.\n-      __ dcbt(R3_ARG1, 0);\n-\n-      \/\/ If supported set DSCR pre-fetch to deepest.\n-      if (VM_Version::has_mfdscr()) {\n-        __ load_const_optimized(tmp2, VM_Version::_dscr_val | 7);\n-        __ mtdscr(tmp2);\n-      }\n-\n-      __ li(tmp1, 16);\n-\n-      \/\/ Backbranch target aligned to 32-byte. Not 16-byte align as\n-      \/\/ loop contains < 8 instructions that fit inside a single\n-      \/\/ i-cache sector.\n-      __ align(32);\n-\n-      __ bind(l_5);\n-      \/\/ Use loop with VSX load\/store instructions to\n-      \/\/ copy 4 elements a time.\n-      __ lxvd2x(tmp_vsr1, R3_ARG1);        \/\/ Load src\n-      __ stxvd2x(tmp_vsr1, R4_ARG2);       \/\/ Store to dst\n-      __ lxvd2x(tmp_vsr2, tmp1, R3_ARG1);  \/\/ Load src + 16\n-      __ stxvd2x(tmp_vsr2, tmp1, R4_ARG2); \/\/ Store to dst + 16\n-      __ addi(R3_ARG1, R3_ARG1, 32);       \/\/ Update src+=32\n-      __ addi(R4_ARG2, R4_ARG2, 32);       \/\/ Update dsc+=32\n-      __ bdnz(l_5);                        \/\/ Dec CTR and loop if not zero.\n-\n-      \/\/ Restore DSCR pre-fetch value.\n-      if (VM_Version::has_mfdscr()) {\n-        __ load_const_optimized(tmp2, VM_Version::_dscr_val);\n-        __ mtdscr(tmp2);\n-      }\n-\n-    } \/\/ VSX\n-   } \/\/ FasterArrayCopy\n-\n-    \/\/ copy 1 element at a time\n-    __ bind(l_3);\n-    __ cmpwi(CCR0, R5_ARG3, 0);\n-    __ beq(CCR0, l_1);\n-\n-    { \/\/ FasterArrayCopy\n-      __ mtctr(R5_ARG3);\n-      __ addi(R3_ARG1, R3_ARG1, -8);\n-      __ addi(R4_ARG2, R4_ARG2, -8);\n-\n-      __ bind(l_2);\n-      __ ldu(R0, 8, R3_ARG1);\n-      __ stdu(R0, 8, R4_ARG2);\n-      __ bdnz(l_2);\n-\n-    }\n-    __ bind(l_1);\n-  }\n-\n-  \/\/ Generate stub for disjoint long copy.  If \"aligned\" is true, the\n-  \/\/ \"from\" and \"to\" addresses are assumed to be heapword aligned.\n-  \/\/\n-  \/\/ Arguments for generated stub:\n-  \/\/      from:  R3_ARG1\n-  \/\/      to:    R4_ARG2\n-  \/\/      count: R5_ARG3 treated as signed\n-  \/\/\n-  address generate_disjoint_long_copy(bool aligned, const char * name) {\n-    StubCodeMark mark(this, \"StubRoutines\", name);\n-    address start = __ function_entry();\n-    assert_positive_int(R5_ARG3);\n-    {\n-      \/\/ UnsafeMemoryAccess page error: continue at UnsafeMemoryAccess common_error_exit\n-      UnsafeMemoryAccessMark umam(this, !aligned, false);\n-      generate_disjoint_long_copy_core(aligned);\n-    }\n-    __ li(R3_RET, 0); \/\/ return 0\n-    __ blr();\n-\n-  return start;\n-  }\n-\n-  \/\/ Generate core code for conjoint long copy (and oop copy on\n-  \/\/ 64-bit).  If \"aligned\" is true, the \"from\" and \"to\" addresses\n-  \/\/ are assumed to be heapword aligned.\n-  \/\/\n-  \/\/ Arguments:\n-  \/\/      from:  R3_ARG1\n-  \/\/      to:    R4_ARG2\n-  \/\/      count: R5_ARG3 treated as signed\n-  \/\/\n-  void generate_conjoint_long_copy_core(bool aligned) {\n-    Register tmp1 = R6_ARG4;\n-    Register tmp2 = R7_ARG5;\n-    Register tmp3 = R8_ARG6;\n-    Register tmp4 = R0;\n-\n-    VectorSRegister tmp_vsr1  = VSR1;\n-    VectorSRegister tmp_vsr2  = VSR2;\n-\n-    Label l_1, l_2, l_3, l_4, l_5;\n-\n-    __ cmpwi(CCR0, R5_ARG3, 0);\n-    __ beq(CCR0, l_1);\n-\n-    { \/\/ FasterArrayCopy\n-      __ sldi(R5_ARG3, R5_ARG3, 3);\n-      __ add(R3_ARG1, R3_ARG1, R5_ARG3);\n-      __ add(R4_ARG2, R4_ARG2, R5_ARG3);\n-      __ srdi(R5_ARG3, R5_ARG3, 3);\n-\n-      __ cmpwi(CCR0, R5_ARG3, 3);\n-      __ ble(CCR0, l_5); \/\/ copy 1 at a time if less than 4 elements remain\n-\n-      __ srdi(tmp1, R5_ARG3, 2);\n-      __ andi(R5_ARG3, R5_ARG3, 3);\n-      __ mtctr(tmp1);\n-\n-     if (!VM_Version::has_vsx()) {\n-      __ bind(l_4);\n-      \/\/ Use unrolled version for mass copying (copy 4 elements a time).\n-      \/\/ Load feeding store gets zero latency on Power6, however not on Power5.\n-      \/\/ Therefore, the following sequence is made for the good of both.\n-      __ addi(R3_ARG1, R3_ARG1, -32);\n-      __ addi(R4_ARG2, R4_ARG2, -32);\n-      __ ld(tmp4, 24, R3_ARG1);\n-      __ ld(tmp3, 16, R3_ARG1);\n-      __ ld(tmp2, 8, R3_ARG1);\n-      __ ld(tmp1, 0, R3_ARG1);\n-      __ std(tmp4, 24, R4_ARG2);\n-      __ std(tmp3, 16, R4_ARG2);\n-      __ std(tmp2, 8, R4_ARG2);\n-      __ std(tmp1, 0, R4_ARG2);\n-      __ bdnz(l_4);\n-     } else { \/\/ Processor supports VSX, so use it to mass copy.\n-      \/\/ Prefetch the data into the L2 cache.\n-      __ dcbt(R3_ARG1, 0);\n-\n-      \/\/ If supported set DSCR pre-fetch to deepest.\n-      if (VM_Version::has_mfdscr()) {\n-        __ load_const_optimized(tmp2, VM_Version::_dscr_val | 7);\n-        __ mtdscr(tmp2);\n-      }\n-\n-      __ li(tmp1, 16);\n-\n-      \/\/ Backbranch target aligned to 32-byte. Not 16-byte align as\n-      \/\/ loop contains < 8 instructions that fit inside a single\n-      \/\/ i-cache sector.\n-      __ align(32);\n-\n-      __ bind(l_4);\n-      \/\/ Use loop with VSX load\/store instructions to\n-      \/\/ copy 4 elements a time.\n-      __ addi(R3_ARG1, R3_ARG1, -32);      \/\/ Update src-=32\n-      __ addi(R4_ARG2, R4_ARG2, -32);      \/\/ Update dsc-=32\n-      __ lxvd2x(tmp_vsr2, tmp1, R3_ARG1);  \/\/ Load src+16\n-      __ lxvd2x(tmp_vsr1, R3_ARG1);        \/\/ Load src\n-      __ stxvd2x(tmp_vsr2, tmp1, R4_ARG2); \/\/ Store to dst+16\n-      __ stxvd2x(tmp_vsr1, R4_ARG2);       \/\/ Store to dst\n-      __ bdnz(l_4);\n-\n-      \/\/ Restore DSCR pre-fetch value.\n-      if (VM_Version::has_mfdscr()) {\n-        __ load_const_optimized(tmp2, VM_Version::_dscr_val);\n-        __ mtdscr(tmp2);\n-      }\n-     }\n-\n-      __ cmpwi(CCR0, R5_ARG3, 0);\n-      __ beq(CCR0, l_1);\n-\n-      __ bind(l_5);\n-      __ mtctr(R5_ARG3);\n-      __ bind(l_3);\n-      __ ld(R0, -8, R3_ARG1);\n-      __ std(R0, -8, R4_ARG2);\n-      __ addi(R3_ARG1, R3_ARG1, -8);\n-      __ addi(R4_ARG2, R4_ARG2, -8);\n-      __ bdnz(l_3);\n-\n-    }\n-    __ bind(l_1);\n-  }\n-\n-  \/\/ Generate stub for conjoint long copy.  If \"aligned\" is true, the\n-  \/\/ \"from\" and \"to\" addresses are assumed to be heapword aligned.\n-  \/\/\n-  \/\/ Arguments for generated stub:\n-  \/\/      from:  R3_ARG1\n-  \/\/      to:    R4_ARG2\n-  \/\/      count: R5_ARG3 treated as signed\n-  \/\/\n-  address generate_conjoint_long_copy(bool aligned, const char * name) {\n-    StubCodeMark mark(this, \"StubRoutines\", name);\n-    address start = __ function_entry();\n-    assert_positive_int(R5_ARG3);\n-    address nooverlap_target = aligned ?\n-      STUB_ENTRY(arrayof_jlong_disjoint_arraycopy()) :\n-      STUB_ENTRY(jlong_disjoint_arraycopy());\n-\n-    array_overlap_test(nooverlap_target, 3);\n-    {\n-      \/\/ UnsafeMemoryAccess page error: continue at UnsafeMemoryAccess common_error_exit\n-      UnsafeMemoryAccessMark umam(this, !aligned, false);\n-      generate_conjoint_long_copy_core(aligned);\n-    }\n-    __ li(R3_RET, 0); \/\/ return 0\n-    __ blr();\n-\n-    return start;\n-  }\n-\n-  \/\/ Generate stub for conjoint oop copy.  If \"aligned\" is true, the\n-  \/\/ \"from\" and \"to\" addresses are assumed to be heapword aligned.\n-  \/\/\n-  \/\/ Arguments for generated stub:\n-  \/\/      from:  R3_ARG1\n-  \/\/      to:    R4_ARG2\n-  \/\/      count: R5_ARG3 treated as signed\n-  \/\/      dest_uninitialized: G1 support\n-  \/\/\n-  address generate_conjoint_oop_copy(bool aligned, const char * name, bool dest_uninitialized) {\n-    StubCodeMark mark(this, \"StubRoutines\", name);\n-\n-    address start = __ function_entry();\n-    assert_positive_int(R5_ARG3);\n-    address nooverlap_target = aligned ?\n-      STUB_ENTRY(arrayof_oop_disjoint_arraycopy(dest_uninitialized)) :\n-      STUB_ENTRY(oop_disjoint_arraycopy(dest_uninitialized));\n-\n-    array_overlap_test(nooverlap_target, UseCompressedOops ? 2 : 3);\n-\n-    DecoratorSet decorators = IN_HEAP | IS_ARRAY;\n-    if (dest_uninitialized) {\n-      decorators |= IS_DEST_UNINITIALIZED;\n-    }\n-    if (aligned) {\n-      decorators |= ARRAYCOPY_ALIGNED;\n-    }\n-\n-    BarrierSetAssembler *bs = BarrierSet::barrier_set()->barrier_set_assembler();\n-    bs->arraycopy_prologue(_masm, decorators, T_OBJECT, R3_ARG1, R4_ARG2, R5_ARG3, noreg, noreg);\n-\n-    if (UseCompressedOops) {\n-      generate_conjoint_int_copy_core(aligned);\n-    } else {\n-#if INCLUDE_ZGC\n-      if (UseZGC) {\n-        ZBarrierSetAssembler *zbs = (ZBarrierSetAssembler*)bs;\n-        zbs->generate_conjoint_oop_copy(_masm, dest_uninitialized);\n-      } else\n-#endif\n-      generate_conjoint_long_copy_core(aligned);\n-    }\n-\n-    bs->arraycopy_epilogue(_masm, decorators, T_OBJECT, R4_ARG2, R5_ARG3, noreg);\n-    __ li(R3_RET, 0); \/\/ return 0\n-    __ blr();\n-    return start;\n-  }\n-\n-  \/\/ Generate stub for disjoint oop copy.  If \"aligned\" is true, the\n-  \/\/ \"from\" and \"to\" addresses are assumed to be heapword aligned.\n-  \/\/\n-  \/\/ Arguments for generated stub:\n-  \/\/      from:  R3_ARG1\n-  \/\/      to:    R4_ARG2\n-  \/\/      count: R5_ARG3 treated as signed\n-  \/\/      dest_uninitialized: G1 support\n-  \/\/\n-  address generate_disjoint_oop_copy(bool aligned, const char * name, bool dest_uninitialized) {\n-    StubCodeMark mark(this, \"StubRoutines\", name);\n-    address start = __ function_entry();\n-    assert_positive_int(R5_ARG3);\n-\n-    DecoratorSet decorators = IN_HEAP | IS_ARRAY | ARRAYCOPY_DISJOINT;\n-    if (dest_uninitialized) {\n-      decorators |= IS_DEST_UNINITIALIZED;\n-    }\n-    if (aligned) {\n-      decorators |= ARRAYCOPY_ALIGNED;\n-    }\n-\n-    BarrierSetAssembler *bs = BarrierSet::barrier_set()->barrier_set_assembler();\n-    bs->arraycopy_prologue(_masm, decorators, T_OBJECT, R3_ARG1, R4_ARG2, R5_ARG3, noreg, noreg);\n-\n-    if (UseCompressedOops) {\n-      generate_disjoint_int_copy_core(aligned);\n-    } else {\n-#if INCLUDE_ZGC\n-      if (UseZGC) {\n-        ZBarrierSetAssembler *zbs = (ZBarrierSetAssembler*)bs;\n-        zbs->generate_disjoint_oop_copy(_masm, dest_uninitialized);\n-      } else\n-#endif\n-      generate_disjoint_long_copy_core(aligned);\n-    }\n-\n-    bs->arraycopy_epilogue(_masm, decorators, T_OBJECT, R4_ARG2, R5_ARG3, noreg);\n-    __ li(R3_RET, 0); \/\/ return 0\n-    __ blr();\n-\n-    return start;\n-  }\n-\n-\n-  \/\/ Helper for generating a dynamic type check.\n-  \/\/ Smashes only the given temp registers.\n-  void generate_type_check(Register sub_klass,\n-                           Register super_check_offset,\n-                           Register super_klass,\n-                           Register temp,\n-                           Label& L_success) {\n-    assert_different_registers(sub_klass, super_check_offset, super_klass);\n-\n-    BLOCK_COMMENT(\"type_check:\");\n-\n-    Label L_miss;\n-\n-    __ check_klass_subtype_fast_path(sub_klass, super_klass, temp, R0, &L_success, &L_miss, nullptr,\n-                                     super_check_offset);\n-    __ check_klass_subtype_slow_path(sub_klass, super_klass, temp, R0, &L_success);\n-\n-    \/\/ Fall through on failure!\n-    __ bind(L_miss);\n-  }\n-\n-\n-  \/\/  Generate stub for checked oop copy.\n-  \/\/\n-  \/\/ Arguments for generated stub:\n-  \/\/      from:  R3\n-  \/\/      to:    R4\n-  \/\/      count: R5 treated as signed\n-  \/\/      ckoff: R6 (super_check_offset)\n-  \/\/      ckval: R7 (super_klass)\n-  \/\/      ret:   R3 zero for success; (-1^K) where K is partial transfer count\n-  \/\/\n-  address generate_checkcast_copy(const char *name, bool dest_uninitialized) {\n-\n-    const Register R3_from   = R3_ARG1;      \/\/ source array address\n-    const Register R4_to     = R4_ARG2;      \/\/ destination array address\n-    const Register R5_count  = R5_ARG3;      \/\/ elements count\n-    const Register R6_ckoff  = R6_ARG4;      \/\/ super_check_offset\n-    const Register R7_ckval  = R7_ARG5;      \/\/ super_klass\n-\n-    const Register R8_offset = R8_ARG6;      \/\/ loop var, with stride wordSize\n-    const Register R9_remain = R9_ARG7;      \/\/ loop var, with stride -1\n-    const Register R10_oop   = R10_ARG8;     \/\/ actual oop copied\n-    const Register R11_klass = R11_scratch1; \/\/ oop._klass\n-    const Register R12_tmp   = R12_scratch2;\n-\n-    const Register R2_minus1 = R2;\n-\n-    \/\/__ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", name);\n-    address start = __ function_entry();\n-\n-    \/\/ Assert that int is 64 bit sign extended and arrays are not conjoint.\n-#ifdef ASSERT\n-    {\n-    assert_positive_int(R5_ARG3);\n-    const Register tmp1 = R11_scratch1, tmp2 = R12_scratch2;\n-    Label no_overlap;\n-    __ subf(tmp1, R3_ARG1, R4_ARG2); \/\/ distance in bytes\n-    __ sldi(tmp2, R5_ARG3, LogBytesPerHeapOop); \/\/ size in bytes\n-    __ cmpld(CCR0, R3_ARG1, R4_ARG2); \/\/ Use unsigned comparison!\n-    __ cmpld(CCR1, tmp1, tmp2);\n-    __ crnand(CCR0, Assembler::less, CCR1, Assembler::less);\n-    \/\/ Overlaps if Src before dst and distance smaller than size.\n-    \/\/ Branch to forward copy routine otherwise.\n-    __ blt(CCR0, no_overlap);\n-    __ stop(\"overlap in checkcast_copy\");\n-    __ bind(no_overlap);\n-    }\n-#endif\n-\n-    DecoratorSet decorators = IN_HEAP | IS_ARRAY | ARRAYCOPY_CHECKCAST;\n-    if (dest_uninitialized) {\n-      decorators |= IS_DEST_UNINITIALIZED;\n-    }\n-\n-    BarrierSetAssembler *bs = BarrierSet::barrier_set()->barrier_set_assembler();\n-    bs->arraycopy_prologue(_masm, decorators, T_OBJECT, R3_from, R4_to, R5_count, \/* preserve: *\/ R6_ckoff, R7_ckval);\n-\n-    \/\/inc_counter_np(SharedRuntime::_checkcast_array_copy_ctr, R12_tmp, R3_RET);\n-\n-    Label load_element, store_element, store_null, success, do_epilogue;\n-    __ or_(R9_remain, R5_count, R5_count); \/\/ Initialize loop index, and test it.\n-    __ li(R8_offset, 0);                   \/\/ Offset from start of arrays.\n-    __ li(R2_minus1, -1);\n-    __ bne(CCR0, load_element);\n-\n-    \/\/ Empty array: Nothing to do.\n-    __ li(R3_RET, 0);           \/\/ Return 0 on (trivial) success.\n-    __ blr();\n-\n-    \/\/ ======== begin loop ========\n-    \/\/ (Entry is load_element.)\n-    __ align(OptoLoopAlignment);\n-    __ bind(store_element);\n-    if (UseCompressedOops) {\n-      __ encode_heap_oop_not_null(R10_oop);\n-      __ bind(store_null);\n-      __ stw(R10_oop, R8_offset, R4_to);\n-    } else {\n-      __ bind(store_null);\n-#if INCLUDE_ZGC\n-      if (UseZGC) {\n-        __ store_heap_oop(R10_oop, R8_offset, R4_to, R11_scratch1, R12_tmp, noreg,\n-                          MacroAssembler::PRESERVATION_FRAME_LR_GP_REGS,\n-                          dest_uninitialized ? IS_DEST_UNINITIALIZED : 0);\n-      } else\n-#endif\n-      __ std(R10_oop, R8_offset, R4_to);\n-    }\n-\n-    __ addi(R8_offset, R8_offset, heapOopSize);   \/\/ Step to next offset.\n-    __ add_(R9_remain, R2_minus1, R9_remain);     \/\/ Decrement the count.\n-    __ beq(CCR0, success);\n-\n-    \/\/ ======== loop entry is here ========\n-    __ bind(load_element);\n-#if INCLUDE_ZGC\n-    if (UseZGC) {\n-      __ load_heap_oop(R10_oop, R8_offset, R3_from,\n-                       R11_scratch1, R12_tmp,\n-                       MacroAssembler::PRESERVATION_FRAME_LR_GP_REGS,\n-                       0, &store_null);\n-    } else\n-#endif\n-    __ load_heap_oop(R10_oop, R8_offset, R3_from,\n-                     R11_scratch1, R12_tmp,\n-                     MacroAssembler::PRESERVATION_FRAME_LR_GP_REGS,\n-                     AS_RAW, &store_null);\n-\n-    __ load_klass(R11_klass, R10_oop); \/\/ Query the object klass.\n-\n-    generate_type_check(R11_klass, R6_ckoff, R7_ckval, R12_tmp,\n-                        \/\/ Branch to this on success:\n-                        store_element);\n-    \/\/ ======== end loop ========\n-\n-    \/\/ It was a real error; we must depend on the caller to finish the job.\n-    \/\/ Register R9_remain has number of *remaining* oops, R5_count number of *total* oops.\n-    \/\/ Emit GC store barriers for the oops we have copied (R5_count minus R9_remain),\n-    \/\/ and report their number to the caller.\n-    __ subf_(R5_count, R9_remain, R5_count);\n-    __ nand(R3_RET, R5_count, R5_count);   \/\/ report (-1^K) to caller\n-    __ bne(CCR0, do_epilogue);\n-    __ blr();\n-\n-    __ bind(success);\n-    __ li(R3_RET, 0);\n-\n-    __ bind(do_epilogue);\n-    bs->arraycopy_epilogue(_masm, decorators, T_OBJECT, R4_to, R5_count, \/* preserve *\/ R3_RET);\n-\n-    __ blr();\n-    return start;\n-  }\n-\n-\n-  \/\/  Generate 'unsafe' array copy stub.\n-  \/\/  Though just as safe as the other stubs, it takes an unscaled\n-  \/\/  size_t argument instead of an element count.\n-  \/\/\n-  \/\/ Arguments for generated stub:\n-  \/\/      from:  R3\n-  \/\/      to:    R4\n-  \/\/      count: R5 byte count, treated as ssize_t, can be zero\n-  \/\/\n-  \/\/ Examines the alignment of the operands and dispatches\n-  \/\/ to a long, int, short, or byte copy loop.\n-  \/\/\n-  address generate_unsafe_copy(const char* name,\n-                               address byte_copy_entry,\n-                               address short_copy_entry,\n-                               address int_copy_entry,\n-                               address long_copy_entry) {\n-\n-    const Register R3_from   = R3_ARG1;      \/\/ source array address\n-    const Register R4_to     = R4_ARG2;      \/\/ destination array address\n-    const Register R5_count  = R5_ARG3;      \/\/ elements count (as long on PPC64)\n-\n-    const Register R6_bits   = R6_ARG4;      \/\/ test copy of low bits\n-    const Register R7_tmp    = R7_ARG5;\n-\n-    \/\/__ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", name);\n-    address start = __ function_entry();\n-\n-    \/\/ Bump this on entry, not on exit:\n-    \/\/inc_counter_np(SharedRuntime::_unsafe_array_copy_ctr, R6_bits, R7_tmp);\n-\n-    Label short_copy, int_copy, long_copy;\n-\n-    __ orr(R6_bits, R3_from, R4_to);\n-    __ orr(R6_bits, R6_bits, R5_count);\n-    __ andi_(R0, R6_bits, (BytesPerLong-1));\n-    __ beq(CCR0, long_copy);\n-\n-    __ andi_(R0, R6_bits, (BytesPerInt-1));\n-    __ beq(CCR0, int_copy);\n-\n-    __ andi_(R0, R6_bits, (BytesPerShort-1));\n-    __ beq(CCR0, short_copy);\n-\n-    \/\/ byte_copy:\n-    __ b(byte_copy_entry);\n-\n-    __ bind(short_copy);\n-    __ srwi(R5_count, R5_count, LogBytesPerShort);\n-    __ b(short_copy_entry);\n-\n-    __ bind(int_copy);\n-    __ srwi(R5_count, R5_count, LogBytesPerInt);\n-    __ b(int_copy_entry);\n-\n-    __ bind(long_copy);\n-    __ srwi(R5_count, R5_count, LogBytesPerLong);\n-    __ b(long_copy_entry);\n-\n-    return start;\n-  }\n-\n-\n-  \/\/ Perform range checks on the proposed arraycopy.\n-  \/\/ Kills the two temps, but nothing else.\n-  \/\/ Also, clean the sign bits of src_pos and dst_pos.\n-  void arraycopy_range_checks(Register src,     \/\/ source array oop\n-                              Register src_pos, \/\/ source position\n-                              Register dst,     \/\/ destination array oop\n-                              Register dst_pos, \/\/ destination position\n-                              Register length,  \/\/ length of copy\n-                              Register temp1, Register temp2,\n-                              Label& L_failed) {\n-    BLOCK_COMMENT(\"arraycopy_range_checks:\");\n-\n-    const Register array_length = temp1;  \/\/ scratch\n-    const Register end_pos      = temp2;  \/\/ scratch\n-\n-    \/\/  if (src_pos + length > arrayOop(src)->length() ) FAIL;\n-    __ lwa(array_length, arrayOopDesc::length_offset_in_bytes(), src);\n-    __ add(end_pos, src_pos, length);  \/\/ src_pos + length\n-    __ cmpd(CCR0, end_pos, array_length);\n-    __ bgt(CCR0, L_failed);\n-\n-    \/\/  if (dst_pos + length > arrayOop(dst)->length() ) FAIL;\n-    __ lwa(array_length, arrayOopDesc::length_offset_in_bytes(), dst);\n-    __ add(end_pos, dst_pos, length);  \/\/ src_pos + length\n-    __ cmpd(CCR0, end_pos, array_length);\n-    __ bgt(CCR0, L_failed);\n-\n-    BLOCK_COMMENT(\"arraycopy_range_checks done\");\n-  }\n-\n-\n-  \/\/\n-  \/\/  Generate generic array copy stubs\n-  \/\/\n-  \/\/  Input:\n-  \/\/    R3    -  src oop\n-  \/\/    R4    -  src_pos\n-  \/\/    R5    -  dst oop\n-  \/\/    R6    -  dst_pos\n-  \/\/    R7    -  element count\n-  \/\/\n-  \/\/  Output:\n-  \/\/    R3 ==  0  -  success\n-  \/\/    R3 == -1  -  need to call System.arraycopy\n-  \/\/\n-  address generate_generic_copy(const char *name,\n-                                address entry_jbyte_arraycopy,\n-                                address entry_jshort_arraycopy,\n-                                address entry_jint_arraycopy,\n-                                address entry_oop_arraycopy,\n-                                address entry_disjoint_oop_arraycopy,\n-                                address entry_jlong_arraycopy,\n-                                address entry_checkcast_arraycopy) {\n-    Label L_failed, L_objArray;\n-\n-    \/\/ Input registers\n-    const Register src       = R3_ARG1;  \/\/ source array oop\n-    const Register src_pos   = R4_ARG2;  \/\/ source position\n-    const Register dst       = R5_ARG3;  \/\/ destination array oop\n-    const Register dst_pos   = R6_ARG4;  \/\/ destination position\n-    const Register length    = R7_ARG5;  \/\/ elements count\n-\n-    \/\/ registers used as temp\n-    const Register src_klass = R8_ARG6;  \/\/ source array klass\n-    const Register dst_klass = R9_ARG7;  \/\/ destination array klass\n-    const Register lh        = R10_ARG8; \/\/ layout handler\n-    const Register temp      = R2;\n-\n-    \/\/__ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", name);\n-    address start = __ function_entry();\n-\n-    \/\/ Bump this on entry, not on exit:\n-    \/\/inc_counter_np(SharedRuntime::_generic_array_copy_ctr, lh, temp);\n-\n-    \/\/ In principle, the int arguments could be dirty.\n-\n-    \/\/-----------------------------------------------------------------------\n-    \/\/ Assembler stubs will be used for this call to arraycopy\n-    \/\/ if the following conditions are met:\n-    \/\/\n-    \/\/ (1) src and dst must not be null.\n-    \/\/ (2) src_pos must not be negative.\n-    \/\/ (3) dst_pos must not be negative.\n-    \/\/ (4) length  must not be negative.\n-    \/\/ (5) src klass and dst klass should be the same and not null.\n-    \/\/ (6) src and dst should be arrays.\n-    \/\/ (7) src_pos + length must not exceed length of src.\n-    \/\/ (8) dst_pos + length must not exceed length of dst.\n-    BLOCK_COMMENT(\"arraycopy initial argument checks\");\n-\n-    __ cmpdi(CCR1, src, 0);      \/\/ if (src == nullptr) return -1;\n-    __ extsw_(src_pos, src_pos); \/\/ if (src_pos < 0) return -1;\n-    __ cmpdi(CCR5, dst, 0);      \/\/ if (dst == nullptr) return -1;\n-    __ cror(CCR1, Assembler::equal, CCR0, Assembler::less);\n-    __ extsw_(dst_pos, dst_pos); \/\/ if (src_pos < 0) return -1;\n-    __ cror(CCR5, Assembler::equal, CCR0, Assembler::less);\n-    __ extsw_(length, length);   \/\/ if (length < 0) return -1;\n-    __ cror(CCR1, Assembler::equal, CCR5, Assembler::equal);\n-    __ cror(CCR1, Assembler::equal, CCR0, Assembler::less);\n-    __ beq(CCR1, L_failed);\n-\n-    BLOCK_COMMENT(\"arraycopy argument klass checks\");\n-    __ load_klass(src_klass, src);\n-    __ load_klass(dst_klass, dst);\n-\n-    \/\/ Load layout helper\n-    \/\/\n-    \/\/  |array_tag|     | header_size | element_type |     |log2_element_size|\n-    \/\/ 32        30    24            16              8     2                 0\n-    \/\/\n-    \/\/   array_tag: typeArray = 0x3, objArray = 0x2, non-array = 0x0\n-    \/\/\n-\n-    int lh_offset = in_bytes(Klass::layout_helper_offset());\n-\n-    \/\/ Load 32-bits signed value. Use br() instruction with it to check icc.\n-    __ lwz(lh, lh_offset, src_klass);\n-\n-    \/\/ Handle objArrays completely differently...\n-    jint objArray_lh = Klass::array_layout_helper(T_OBJECT);\n-    __ load_const_optimized(temp, objArray_lh, R0);\n-    __ cmpw(CCR0, lh, temp);\n-    __ beq(CCR0, L_objArray);\n-\n-    __ cmpd(CCR5, src_klass, dst_klass);          \/\/ if (src->klass() != dst->klass()) return -1;\n-    __ cmpwi(CCR6, lh, Klass::_lh_neutral_value); \/\/ if (!src->is_Array()) return -1;\n-\n-    __ crnand(CCR5, Assembler::equal, CCR6, Assembler::less);\n-    __ beq(CCR5, L_failed);\n-\n-    \/\/ At this point, it is known to be a typeArray (array_tag 0x3).\n-#ifdef ASSERT\n-    { Label L;\n-      jint lh_prim_tag_in_place = (Klass::_lh_array_tag_type_value << Klass::_lh_array_tag_shift);\n-      __ load_const_optimized(temp, lh_prim_tag_in_place, R0);\n-      __ cmpw(CCR0, lh, temp);\n-      __ bge(CCR0, L);\n-      __ stop(\"must be a primitive array\");\n-      __ bind(L);\n-    }\n-#endif\n-\n-    arraycopy_range_checks(src, src_pos, dst, dst_pos, length,\n-                           temp, dst_klass, L_failed);\n-\n-    \/\/ TypeArrayKlass\n-    \/\/\n-    \/\/ src_addr = (src + array_header_in_bytes()) + (src_pos << log2elemsize);\n-    \/\/ dst_addr = (dst + array_header_in_bytes()) + (dst_pos << log2elemsize);\n-    \/\/\n-\n-    const Register offset = dst_klass;    \/\/ array offset\n-    const Register elsize = src_klass;    \/\/ log2 element size\n-\n-    __ rldicl(offset, lh, 64 - Klass::_lh_header_size_shift, 64 - exact_log2(Klass::_lh_header_size_mask + 1));\n-    __ andi(elsize, lh, Klass::_lh_log2_element_size_mask);\n-    __ add(src, offset, src);       \/\/ src array offset\n-    __ add(dst, offset, dst);       \/\/ dst array offset\n-\n-    \/\/ Next registers should be set before the jump to corresponding stub.\n-    const Register from     = R3_ARG1;  \/\/ source array address\n-    const Register to       = R4_ARG2;  \/\/ destination array address\n-    const Register count    = R5_ARG3;  \/\/ elements count\n-\n-    \/\/ 'from', 'to', 'count' registers should be set in this order\n-    \/\/ since they are the same as 'src', 'src_pos', 'dst'.\n-\n-    BLOCK_COMMENT(\"scale indexes to element size\");\n-    __ sld(src_pos, src_pos, elsize);\n-    __ sld(dst_pos, dst_pos, elsize);\n-    __ add(from, src_pos, src);  \/\/ src_addr\n-    __ add(to, dst_pos, dst);    \/\/ dst_addr\n-    __ mr(count, length);        \/\/ length\n-\n-    BLOCK_COMMENT(\"choose copy loop based on element size\");\n-    \/\/ Using conditional branches with range 32kB.\n-    const int bo = Assembler::bcondCRbiIs1, bi = Assembler::bi0(CCR0, Assembler::equal);\n-    __ cmpwi(CCR0, elsize, 0);\n-    __ bc(bo, bi, entry_jbyte_arraycopy);\n-    __ cmpwi(CCR0, elsize, LogBytesPerShort);\n-    __ bc(bo, bi, entry_jshort_arraycopy);\n-    __ cmpwi(CCR0, elsize, LogBytesPerInt);\n-    __ bc(bo, bi, entry_jint_arraycopy);\n-#ifdef ASSERT\n-    { Label L;\n-      __ cmpwi(CCR0, elsize, LogBytesPerLong);\n-      __ beq(CCR0, L);\n-      __ stop(\"must be long copy, but elsize is wrong\");\n-      __ bind(L);\n-    }\n-#endif\n-    __ b(entry_jlong_arraycopy);\n-\n-    \/\/ ObjArrayKlass\n-  __ bind(L_objArray);\n-    \/\/ live at this point:  src_klass, dst_klass, src[_pos], dst[_pos], length\n-\n-    Label L_disjoint_plain_copy, L_checkcast_copy;\n-    \/\/  test array classes for subtyping\n-    __ cmpd(CCR0, src_klass, dst_klass);         \/\/ usual case is exact equality\n-    __ bne(CCR0, L_checkcast_copy);\n-\n-    \/\/ Identically typed arrays can be copied without element-wise checks.\n-    arraycopy_range_checks(src, src_pos, dst, dst_pos, length,\n-                           temp, lh, L_failed);\n-\n-    __ addi(src, src, arrayOopDesc::base_offset_in_bytes(T_OBJECT)); \/\/src offset\n-    __ addi(dst, dst, arrayOopDesc::base_offset_in_bytes(T_OBJECT)); \/\/dst offset\n-    __ sldi(src_pos, src_pos, LogBytesPerHeapOop);\n-    __ sldi(dst_pos, dst_pos, LogBytesPerHeapOop);\n-    __ add(from, src_pos, src);  \/\/ src_addr\n-    __ add(to, dst_pos, dst);    \/\/ dst_addr\n-    __ mr(count, length);        \/\/ length\n-    __ b(entry_oop_arraycopy);\n-\n-  __ bind(L_checkcast_copy);\n-    \/\/ live at this point:  src_klass, dst_klass\n-    {\n-      \/\/ Before looking at dst.length, make sure dst is also an objArray.\n-      __ lwz(temp, lh_offset, dst_klass);\n-      __ cmpw(CCR0, lh, temp);\n-      __ bne(CCR0, L_failed);\n-\n-      \/\/ It is safe to examine both src.length and dst.length.\n-      arraycopy_range_checks(src, src_pos, dst, dst_pos, length,\n-                             temp, lh, L_failed);\n-\n-      \/\/ Marshal the base address arguments now, freeing registers.\n-      __ addi(src, src, arrayOopDesc::base_offset_in_bytes(T_OBJECT)); \/\/src offset\n-      __ addi(dst, dst, arrayOopDesc::base_offset_in_bytes(T_OBJECT)); \/\/dst offset\n-      __ sldi(src_pos, src_pos, LogBytesPerHeapOop);\n-      __ sldi(dst_pos, dst_pos, LogBytesPerHeapOop);\n-      __ add(from, src_pos, src);  \/\/ src_addr\n-      __ add(to, dst_pos, dst);    \/\/ dst_addr\n-      __ mr(count, length);        \/\/ length\n-\n-      Register sco_temp = R6_ARG4;             \/\/ This register is free now.\n-      assert_different_registers(from, to, count, sco_temp,\n-                                 dst_klass, src_klass);\n-\n-      \/\/ Generate the type check.\n-      int sco_offset = in_bytes(Klass::super_check_offset_offset());\n-      __ lwz(sco_temp, sco_offset, dst_klass);\n-      generate_type_check(src_klass, sco_temp, dst_klass,\n-                          temp, L_disjoint_plain_copy);\n-\n-      \/\/ Fetch destination element klass from the ObjArrayKlass header.\n-      int ek_offset = in_bytes(ObjArrayKlass::element_klass_offset());\n-\n-      \/\/ The checkcast_copy loop needs two extra arguments:\n-      __ ld(R7_ARG5, ek_offset, dst_klass);   \/\/ dest elem klass\n-      __ lwz(R6_ARG4, sco_offset, R7_ARG5);   \/\/ sco of elem klass\n-      __ b(entry_checkcast_arraycopy);\n-    }\n-\n-    __ bind(L_disjoint_plain_copy);\n-    __ b(entry_disjoint_oop_arraycopy);\n-\n-  __ bind(L_failed);\n-    __ li(R3_RET, -1); \/\/ return -1\n-    __ blr();\n-    return start;\n-  }\n-\n-  \/\/ Arguments for generated stub:\n-  \/\/   R3_ARG1   - source byte array address\n-  \/\/   R4_ARG2   - destination byte array address\n-  \/\/   R5_ARG3   - round key array\n-  address generate_aescrypt_encryptBlock() {\n-    assert(UseAES, \"need AES instructions and misaligned SSE support\");\n-    StubCodeMark mark(this, \"StubRoutines\", \"aescrypt_encryptBlock\");\n-\n-    address start = __ function_entry();\n-\n-    Label L_doLast, L_error;\n-\n-    Register from           = R3_ARG1;  \/\/ source array address\n-    Register to             = R4_ARG2;  \/\/ destination array address\n-    Register key            = R5_ARG3;  \/\/ round key array\n-\n-    Register keylen         = R8;\n-    Register temp           = R9;\n-    Register keypos         = R10;\n-    Register fifteen        = R12;\n-\n-    VectorRegister vRet     = VR0;\n-\n-    VectorRegister vKey1    = VR1;\n-    VectorRegister vKey2    = VR2;\n-    VectorRegister vKey3    = VR3;\n-    VectorRegister vKey4    = VR4;\n-\n-    VectorRegister fromPerm = VR5;\n-    VectorRegister keyPerm  = VR6;\n-    VectorRegister toPerm   = VR7;\n-    VectorRegister fSplt    = VR8;\n-\n-    VectorRegister vTmp1    = VR9;\n-    VectorRegister vTmp2    = VR10;\n-    VectorRegister vTmp3    = VR11;\n-    VectorRegister vTmp4    = VR12;\n-\n-    __ li              (fifteen, 15);\n-\n-    \/\/ load unaligned from[0-15] to vRet\n-    __ lvx             (vRet, from);\n-    __ lvx             (vTmp1, fifteen, from);\n-    __ lvsl            (fromPerm, from);\n-#ifdef VM_LITTLE_ENDIAN\n-    __ vspltisb        (fSplt, 0x0f);\n-    __ vxor            (fromPerm, fromPerm, fSplt);\n-#endif\n-    __ vperm           (vRet, vRet, vTmp1, fromPerm);\n-\n-    \/\/ load keylen (44 or 52 or 60)\n-    __ lwz             (keylen, arrayOopDesc::length_offset_in_bytes() - arrayOopDesc::base_offset_in_bytes(T_INT), key);\n-\n-    \/\/ to load keys\n-    __ load_perm       (keyPerm, key);\n-#ifdef VM_LITTLE_ENDIAN\n-    __ vspltisb        (vTmp2, -16);\n-    __ vrld            (keyPerm, keyPerm, vTmp2);\n-    __ vrld            (keyPerm, keyPerm, vTmp2);\n-    __ vsldoi          (keyPerm, keyPerm, keyPerm, 8);\n-#endif\n-\n-    \/\/ load the 1st round key to vTmp1\n-    __ lvx             (vTmp1, key);\n-    __ li              (keypos, 16);\n-    __ lvx             (vKey1, keypos, key);\n-    __ vec_perm        (vTmp1, vKey1, keyPerm);\n-\n-    \/\/ 1st round\n-    __ vxor            (vRet, vRet, vTmp1);\n-\n-    \/\/ load the 2nd round key to vKey1\n-    __ li              (keypos, 32);\n-    __ lvx             (vKey2, keypos, key);\n-    __ vec_perm        (vKey1, vKey2, keyPerm);\n-\n-    \/\/ load the 3rd round key to vKey2\n-    __ li              (keypos, 48);\n-    __ lvx             (vKey3, keypos, key);\n-    __ vec_perm        (vKey2, vKey3, keyPerm);\n-\n-    \/\/ load the 4th round key to vKey3\n-    __ li              (keypos, 64);\n-    __ lvx             (vKey4, keypos, key);\n-    __ vec_perm        (vKey3, vKey4, keyPerm);\n-\n-    \/\/ load the 5th round key to vKey4\n-    __ li              (keypos, 80);\n-    __ lvx             (vTmp1, keypos, key);\n-    __ vec_perm        (vKey4, vTmp1, keyPerm);\n-\n-    \/\/ 2nd - 5th rounds\n-    __ vcipher         (vRet, vRet, vKey1);\n-    __ vcipher         (vRet, vRet, vKey2);\n-    __ vcipher         (vRet, vRet, vKey3);\n-    __ vcipher         (vRet, vRet, vKey4);\n-\n-    \/\/ load the 6th round key to vKey1\n-    __ li              (keypos, 96);\n-    __ lvx             (vKey2, keypos, key);\n-    __ vec_perm        (vKey1, vTmp1, vKey2, keyPerm);\n-\n-    \/\/ load the 7th round key to vKey2\n-    __ li              (keypos, 112);\n-    __ lvx             (vKey3, keypos, key);\n-    __ vec_perm        (vKey2, vKey3, keyPerm);\n-\n-    \/\/ load the 8th round key to vKey3\n-    __ li              (keypos, 128);\n-    __ lvx             (vKey4, keypos, key);\n-    __ vec_perm        (vKey3, vKey4, keyPerm);\n-\n-    \/\/ load the 9th round key to vKey4\n-    __ li              (keypos, 144);\n-    __ lvx             (vTmp1, keypos, key);\n-    __ vec_perm        (vKey4, vTmp1, keyPerm);\n-\n-    \/\/ 6th - 9th rounds\n-    __ vcipher         (vRet, vRet, vKey1);\n-    __ vcipher         (vRet, vRet, vKey2);\n-    __ vcipher         (vRet, vRet, vKey3);\n-    __ vcipher         (vRet, vRet, vKey4);\n-\n-    \/\/ load the 10th round key to vKey1\n-    __ li              (keypos, 160);\n-    __ lvx             (vKey2, keypos, key);\n-    __ vec_perm        (vKey1, vTmp1, vKey2, keyPerm);\n-\n-    \/\/ load the 11th round key to vKey2\n-    __ li              (keypos, 176);\n-    __ lvx             (vTmp1, keypos, key);\n-    __ vec_perm        (vKey2, vTmp1, keyPerm);\n-\n-    \/\/ if all round keys are loaded, skip next 4 rounds\n-    __ cmpwi           (CCR0, keylen, 44);\n-    __ beq             (CCR0, L_doLast);\n-\n-    \/\/ 10th - 11th rounds\n-    __ vcipher         (vRet, vRet, vKey1);\n-    __ vcipher         (vRet, vRet, vKey2);\n-\n-    \/\/ load the 12th round key to vKey1\n-    __ li              (keypos, 192);\n-    __ lvx             (vKey2, keypos, key);\n-    __ vec_perm        (vKey1, vTmp1, vKey2, keyPerm);\n-\n-    \/\/ load the 13th round key to vKey2\n-    __ li              (keypos, 208);\n-    __ lvx             (vTmp1, keypos, key);\n-    __ vec_perm        (vKey2, vTmp1, keyPerm);\n-\n-    \/\/ if all round keys are loaded, skip next 2 rounds\n-    __ cmpwi           (CCR0, keylen, 52);\n-    __ beq             (CCR0, L_doLast);\n-\n-#ifdef ASSERT\n-    __ cmpwi           (CCR0, keylen, 60);\n-    __ bne             (CCR0, L_error);\n-#endif\n-\n-    \/\/ 12th - 13th rounds\n-    __ vcipher         (vRet, vRet, vKey1);\n-    __ vcipher         (vRet, vRet, vKey2);\n-\n-    \/\/ load the 14th round key to vKey1\n-    __ li              (keypos, 224);\n-    __ lvx             (vKey2, keypos, key);\n-    __ vec_perm        (vKey1, vTmp1, vKey2, keyPerm);\n-\n-    \/\/ load the 15th round key to vKey2\n-    __ li              (keypos, 240);\n-    __ lvx             (vTmp1, keypos, key);\n-    __ vec_perm        (vKey2, vTmp1, keyPerm);\n-\n-    __ bind(L_doLast);\n-\n-    \/\/ last two rounds\n-    __ vcipher         (vRet, vRet, vKey1);\n-    __ vcipherlast     (vRet, vRet, vKey2);\n-\n-#ifdef VM_LITTLE_ENDIAN\n-    \/\/ toPerm = 0x0F0E0D0C0B0A09080706050403020100\n-    __ lvsl            (toPerm, keypos); \/\/ keypos is a multiple of 16\n-    __ vxor            (toPerm, toPerm, fSplt);\n-\n-    \/\/ Swap Bytes\n-    __ vperm           (vRet, vRet, vRet, toPerm);\n-#endif\n-\n-    \/\/ store result (unaligned)\n-    \/\/ Note: We can't use a read-modify-write sequence which touches additional Bytes.\n-    Register lo = temp, hi = fifteen; \/\/ Reuse\n-    __ vsldoi          (vTmp1, vRet, vRet, 8);\n-    __ mfvrd           (hi, vRet);\n-    __ mfvrd           (lo, vTmp1);\n-    __ std             (hi, 0 LITTLE_ENDIAN_ONLY(+ 8), to);\n-    __ std             (lo, 0 BIG_ENDIAN_ONLY(+ 8), to);\n-\n-    __ blr();\n-\n-#ifdef ASSERT\n-    __ bind(L_error);\n-    __ stop(\"aescrypt_encryptBlock: invalid key length\");\n-#endif\n-     return start;\n-  }\n-\n-  \/\/ Arguments for generated stub:\n-  \/\/   R3_ARG1   - source byte array address\n-  \/\/   R4_ARG2   - destination byte array address\n-  \/\/   R5_ARG3   - K (key) in little endian int array\n-  address generate_aescrypt_decryptBlock() {\n-    assert(UseAES, \"need AES instructions and misaligned SSE support\");\n-    StubCodeMark mark(this, \"StubRoutines\", \"aescrypt_decryptBlock\");\n-\n-    address start = __ function_entry();\n-\n-    Label L_doLast, L_do44, L_do52, L_error;\n-\n-    Register from           = R3_ARG1;  \/\/ source array address\n-    Register to             = R4_ARG2;  \/\/ destination array address\n-    Register key            = R5_ARG3;  \/\/ round key array\n-\n-    Register keylen         = R8;\n-    Register temp           = R9;\n-    Register keypos         = R10;\n-    Register fifteen        = R12;\n-\n-    VectorRegister vRet     = VR0;\n-\n-    VectorRegister vKey1    = VR1;\n-    VectorRegister vKey2    = VR2;\n-    VectorRegister vKey3    = VR3;\n-    VectorRegister vKey4    = VR4;\n-    VectorRegister vKey5    = VR5;\n-\n-    VectorRegister fromPerm = VR6;\n-    VectorRegister keyPerm  = VR7;\n-    VectorRegister toPerm   = VR8;\n-    VectorRegister fSplt    = VR9;\n-\n-    VectorRegister vTmp1    = VR10;\n-    VectorRegister vTmp2    = VR11;\n-    VectorRegister vTmp3    = VR12;\n-    VectorRegister vTmp4    = VR13;\n-\n-    __ li              (fifteen, 15);\n-\n-    \/\/ load unaligned from[0-15] to vRet\n-    __ lvx             (vRet, from);\n-    __ lvx             (vTmp1, fifteen, from);\n-    __ lvsl            (fromPerm, from);\n-#ifdef VM_LITTLE_ENDIAN\n-    __ vspltisb        (fSplt, 0x0f);\n-    __ vxor            (fromPerm, fromPerm, fSplt);\n-#endif\n-    __ vperm           (vRet, vRet, vTmp1, fromPerm); \/\/ align [and byte swap in LE]\n-\n-    \/\/ load keylen (44 or 52 or 60)\n-    __ lwz             (keylen, arrayOopDesc::length_offset_in_bytes() - arrayOopDesc::base_offset_in_bytes(T_INT), key);\n-\n-    \/\/ to load keys\n-    __ load_perm       (keyPerm, key);\n-#ifdef VM_LITTLE_ENDIAN\n-    __ vxor            (vTmp2, vTmp2, vTmp2);\n-    __ vspltisb        (vTmp2, -16);\n-    __ vrld            (keyPerm, keyPerm, vTmp2);\n-    __ vrld            (keyPerm, keyPerm, vTmp2);\n-    __ vsldoi          (keyPerm, keyPerm, keyPerm, 8);\n-#endif\n-\n-    __ cmpwi           (CCR0, keylen, 44);\n-    __ beq             (CCR0, L_do44);\n-\n-    __ cmpwi           (CCR0, keylen, 52);\n-    __ beq             (CCR0, L_do52);\n-\n-#ifdef ASSERT\n-    __ cmpwi           (CCR0, keylen, 60);\n-    __ bne             (CCR0, L_error);\n-#endif\n-\n-    \/\/ load the 15th round key to vKey1\n-    __ li              (keypos, 240);\n-    __ lvx             (vKey1, keypos, key);\n-    __ li              (keypos, 224);\n-    __ lvx             (vKey2, keypos, key);\n-    __ vec_perm        (vKey1, vKey2, vKey1, keyPerm);\n-\n-    \/\/ load the 14th round key to vKey2\n-    __ li              (keypos, 208);\n-    __ lvx             (vKey3, keypos, key);\n-    __ vec_perm        (vKey2, vKey3, vKey2, keyPerm);\n-\n-    \/\/ load the 13th round key to vKey3\n-    __ li              (keypos, 192);\n-    __ lvx             (vKey4, keypos, key);\n-    __ vec_perm        (vKey3, vKey4, vKey3, keyPerm);\n-\n-    \/\/ load the 12th round key to vKey4\n-    __ li              (keypos, 176);\n-    __ lvx             (vKey5, keypos, key);\n-    __ vec_perm        (vKey4, vKey5, vKey4, keyPerm);\n-\n-    \/\/ load the 11th round key to vKey5\n-    __ li              (keypos, 160);\n-    __ lvx             (vTmp1, keypos, key);\n-    __ vec_perm        (vKey5, vTmp1, vKey5, keyPerm);\n-\n-    \/\/ 1st - 5th rounds\n-    __ vxor            (vRet, vRet, vKey1);\n-    __ vncipher        (vRet, vRet, vKey2);\n-    __ vncipher        (vRet, vRet, vKey3);\n-    __ vncipher        (vRet, vRet, vKey4);\n-    __ vncipher        (vRet, vRet, vKey5);\n-\n-    __ b               (L_doLast);\n-\n-    __ align(32);\n-    __ bind            (L_do52);\n-\n-    \/\/ load the 13th round key to vKey1\n-    __ li              (keypos, 208);\n-    __ lvx             (vKey1, keypos, key);\n-    __ li              (keypos, 192);\n-    __ lvx             (vKey2, keypos, key);\n-    __ vec_perm        (vKey1, vKey2, vKey1, keyPerm);\n-\n-    \/\/ load the 12th round key to vKey2\n-    __ li              (keypos, 176);\n-    __ lvx             (vKey3, keypos, key);\n-    __ vec_perm        (vKey2, vKey3, vKey2, keyPerm);\n-\n-    \/\/ load the 11th round key to vKey3\n-    __ li              (keypos, 160);\n-    __ lvx             (vTmp1, keypos, key);\n-    __ vec_perm        (vKey3, vTmp1, vKey3, keyPerm);\n-\n-    \/\/ 1st - 3rd rounds\n-    __ vxor            (vRet, vRet, vKey1);\n-    __ vncipher        (vRet, vRet, vKey2);\n-    __ vncipher        (vRet, vRet, vKey3);\n-\n-    __ b               (L_doLast);\n-\n-    __ align(32);\n-    __ bind            (L_do44);\n-\n-    \/\/ load the 11th round key to vKey1\n-    __ li              (keypos, 176);\n-    __ lvx             (vKey1, keypos, key);\n-    __ li              (keypos, 160);\n-    __ lvx             (vTmp1, keypos, key);\n-    __ vec_perm        (vKey1, vTmp1, vKey1, keyPerm);\n-\n-    \/\/ 1st round\n-    __ vxor            (vRet, vRet, vKey1);\n-\n-    __ bind            (L_doLast);\n-\n-    \/\/ load the 10th round key to vKey1\n-    __ li              (keypos, 144);\n-    __ lvx             (vKey2, keypos, key);\n-    __ vec_perm        (vKey1, vKey2, vTmp1, keyPerm);\n-\n-    \/\/ load the 9th round key to vKey2\n-    __ li              (keypos, 128);\n-    __ lvx             (vKey3, keypos, key);\n-    __ vec_perm        (vKey2, vKey3, vKey2, keyPerm);\n-\n-    \/\/ load the 8th round key to vKey3\n-    __ li              (keypos, 112);\n-    __ lvx             (vKey4, keypos, key);\n-    __ vec_perm        (vKey3, vKey4, vKey3, keyPerm);\n-\n-    \/\/ load the 7th round key to vKey4\n-    __ li              (keypos, 96);\n-    __ lvx             (vKey5, keypos, key);\n-    __ vec_perm        (vKey4, vKey5, vKey4, keyPerm);\n-\n-    \/\/ load the 6th round key to vKey5\n-    __ li              (keypos, 80);\n-    __ lvx             (vTmp1, keypos, key);\n-    __ vec_perm        (vKey5, vTmp1, vKey5, keyPerm);\n-\n-    \/\/ last 10th - 6th rounds\n-    __ vncipher        (vRet, vRet, vKey1);\n-    __ vncipher        (vRet, vRet, vKey2);\n-    __ vncipher        (vRet, vRet, vKey3);\n-    __ vncipher        (vRet, vRet, vKey4);\n-    __ vncipher        (vRet, vRet, vKey5);\n-\n-    \/\/ load the 5th round key to vKey1\n-    __ li              (keypos, 64);\n-    __ lvx             (vKey2, keypos, key);\n-    __ vec_perm        (vKey1, vKey2, vTmp1, keyPerm);\n-\n-    \/\/ load the 4th round key to vKey2\n-    __ li              (keypos, 48);\n-    __ lvx             (vKey3, keypos, key);\n-    __ vec_perm        (vKey2, vKey3, vKey2, keyPerm);\n-\n-    \/\/ load the 3rd round key to vKey3\n-    __ li              (keypos, 32);\n-    __ lvx             (vKey4, keypos, key);\n-    __ vec_perm        (vKey3, vKey4, vKey3, keyPerm);\n-\n-    \/\/ load the 2nd round key to vKey4\n-    __ li              (keypos, 16);\n-    __ lvx             (vKey5, keypos, key);\n-    __ vec_perm        (vKey4, vKey5, vKey4, keyPerm);\n-\n-    \/\/ load the 1st round key to vKey5\n-    __ lvx             (vTmp1, key);\n-    __ vec_perm        (vKey5, vTmp1, vKey5, keyPerm);\n-\n-    \/\/ last 5th - 1th rounds\n-    __ vncipher        (vRet, vRet, vKey1);\n-    __ vncipher        (vRet, vRet, vKey2);\n-    __ vncipher        (vRet, vRet, vKey3);\n-    __ vncipher        (vRet, vRet, vKey4);\n-    __ vncipherlast    (vRet, vRet, vKey5);\n-\n-#ifdef VM_LITTLE_ENDIAN\n-    \/\/ toPerm = 0x0F0E0D0C0B0A09080706050403020100\n-    __ lvsl            (toPerm, keypos); \/\/ keypos is a multiple of 16\n-    __ vxor            (toPerm, toPerm, fSplt);\n-\n-    \/\/ Swap Bytes\n-    __ vperm           (vRet, vRet, vRet, toPerm);\n-#endif\n-\n-    \/\/ store result (unaligned)\n-    \/\/ Note: We can't use a read-modify-write sequence which touches additional Bytes.\n-    Register lo = temp, hi = fifteen; \/\/ Reuse\n-    __ vsldoi          (vTmp1, vRet, vRet, 8);\n-    __ mfvrd           (hi, vRet);\n-    __ mfvrd           (lo, vTmp1);\n-    __ std             (hi, 0 LITTLE_ENDIAN_ONLY(+ 8), to);\n-    __ std             (lo, 0 BIG_ENDIAN_ONLY(+ 8), to);\n-\n-    __ blr();\n-\n-#ifdef ASSERT\n-    __ bind(L_error);\n-    __ stop(\"aescrypt_decryptBlock: invalid key length\");\n-#endif\n-     return start;\n-  }\n-\n-  address generate_sha256_implCompress(bool multi_block, const char *name) {\n-    assert(UseSHA, \"need SHA instructions\");\n-    StubCodeMark mark(this, \"StubRoutines\", name);\n-    address start = __ function_entry();\n-\n-    __ sha256 (multi_block);\n-    __ blr();\n-\n-    return start;\n-  }\n-\n-  address generate_sha512_implCompress(bool multi_block, const char *name) {\n-    assert(UseSHA, \"need SHA instructions\");\n-    StubCodeMark mark(this, \"StubRoutines\", name);\n-    address start = __ function_entry();\n-\n-    __ sha512 (multi_block);\n-    __ blr();\n-\n-    return start;\n-  }\n-\n-  address generate_data_cache_writeback() {\n-    const Register cacheline = R3_ARG1;\n-    StubCodeMark mark(this, \"StubRoutines\", \"_data_cache_writeback\");\n-    address start = __ pc();\n-\n-    __ cache_wb(Address(cacheline));\n-    __ blr();\n-\n-    return start;\n-  }\n-\n-  address generate_data_cache_writeback_sync() {\n-    const Register is_presync = R3_ARG1;\n-    Register temp = R4;\n-    Label SKIP;\n-\n-    StubCodeMark mark(this, \"StubRoutines\", \"_data_cache_writeback_sync\");\n-    address start = __ pc();\n-\n-    __ andi_(temp, is_presync, 1);\n-    __ bne(CCR0, SKIP);\n-    __ cache_wbsync(false); \/\/ post sync => emit 'sync'\n-    __ bind(SKIP);          \/\/ pre sync => emit nothing\n-    __ blr();\n-\n-    return start;\n-  }\n-\n-  void generate_arraycopy_stubs() {\n-    \/\/ Note: the disjoint stubs must be generated first, some of\n-    \/\/ the conjoint stubs use them.\n-\n-    address ucm_common_error_exit       =  generate_unsafecopy_common_error_exit();\n-    UnsafeMemoryAccess::set_common_exit_stub_pc(ucm_common_error_exit);\n-\n-    \/\/ non-aligned disjoint versions\n-    StubRoutines::_jbyte_disjoint_arraycopy       = generate_disjoint_byte_copy(false, \"jbyte_disjoint_arraycopy\");\n-    StubRoutines::_jshort_disjoint_arraycopy      = generate_disjoint_short_copy(false, \"jshort_disjoint_arraycopy\");\n-    StubRoutines::_jint_disjoint_arraycopy        = generate_disjoint_int_copy(false, \"jint_disjoint_arraycopy\");\n-    StubRoutines::_jlong_disjoint_arraycopy       = generate_disjoint_long_copy(false, \"jlong_disjoint_arraycopy\");\n-    StubRoutines::_oop_disjoint_arraycopy         = generate_disjoint_oop_copy(false, \"oop_disjoint_arraycopy\", false);\n-    StubRoutines::_oop_disjoint_arraycopy_uninit  = generate_disjoint_oop_copy(false, \"oop_disjoint_arraycopy_uninit\", true);\n-\n-    \/\/ aligned disjoint versions\n-    StubRoutines::_arrayof_jbyte_disjoint_arraycopy      = generate_disjoint_byte_copy(true, \"arrayof_jbyte_disjoint_arraycopy\");\n-    StubRoutines::_arrayof_jshort_disjoint_arraycopy     = generate_disjoint_short_copy(true, \"arrayof_jshort_disjoint_arraycopy\");\n-    StubRoutines::_arrayof_jint_disjoint_arraycopy       = generate_disjoint_int_copy(true, \"arrayof_jint_disjoint_arraycopy\");\n-    StubRoutines::_arrayof_jlong_disjoint_arraycopy      = generate_disjoint_long_copy(true, \"arrayof_jlong_disjoint_arraycopy\");\n-    StubRoutines::_arrayof_oop_disjoint_arraycopy        = generate_disjoint_oop_copy(true, \"arrayof_oop_disjoint_arraycopy\", false);\n-    StubRoutines::_arrayof_oop_disjoint_arraycopy_uninit = generate_disjoint_oop_copy(true, \"oop_disjoint_arraycopy_uninit\", true);\n-\n-    \/\/ non-aligned conjoint versions\n-    StubRoutines::_jbyte_arraycopy      = generate_conjoint_byte_copy(false, \"jbyte_arraycopy\");\n-    StubRoutines::_jshort_arraycopy     = generate_conjoint_short_copy(false, \"jshort_arraycopy\");\n-    StubRoutines::_jint_arraycopy       = generate_conjoint_int_copy(false, \"jint_arraycopy\");\n-    StubRoutines::_jlong_arraycopy      = generate_conjoint_long_copy(false, \"jlong_arraycopy\");\n-    StubRoutines::_oop_arraycopy        = generate_conjoint_oop_copy(false, \"oop_arraycopy\", false);\n-    StubRoutines::_oop_arraycopy_uninit = generate_conjoint_oop_copy(false, \"oop_arraycopy_uninit\", true);\n-\n-    \/\/ aligned conjoint versions\n-    StubRoutines::_arrayof_jbyte_arraycopy      = generate_conjoint_byte_copy(true, \"arrayof_jbyte_arraycopy\");\n-    StubRoutines::_arrayof_jshort_arraycopy     = generate_conjoint_short_copy(true, \"arrayof_jshort_arraycopy\");\n-    StubRoutines::_arrayof_jint_arraycopy       = generate_conjoint_int_copy(true, \"arrayof_jint_arraycopy\");\n-    StubRoutines::_arrayof_jlong_arraycopy      = generate_conjoint_long_copy(true, \"arrayof_jlong_arraycopy\");\n-    StubRoutines::_arrayof_oop_arraycopy        = generate_conjoint_oop_copy(true, \"arrayof_oop_arraycopy\", false);\n-    StubRoutines::_arrayof_oop_arraycopy_uninit = generate_conjoint_oop_copy(true, \"arrayof_oop_arraycopy\", true);\n-\n-    \/\/ special\/generic versions\n-    StubRoutines::_checkcast_arraycopy        = generate_checkcast_copy(\"checkcast_arraycopy\", false);\n-    StubRoutines::_checkcast_arraycopy_uninit = generate_checkcast_copy(\"checkcast_arraycopy_uninit\", true);\n-\n-    StubRoutines::_unsafe_arraycopy  = generate_unsafe_copy(\"unsafe_arraycopy\",\n-                                                            STUB_ENTRY(jbyte_arraycopy()),\n-                                                            STUB_ENTRY(jshort_arraycopy()),\n-                                                            STUB_ENTRY(jint_arraycopy()),\n-                                                            STUB_ENTRY(jlong_arraycopy()));\n-    StubRoutines::_generic_arraycopy = generate_generic_copy(\"generic_arraycopy\",\n-                                                             STUB_ENTRY(jbyte_arraycopy()),\n-                                                             STUB_ENTRY(jshort_arraycopy()),\n-                                                             STUB_ENTRY(jint_arraycopy()),\n-                                                             STUB_ENTRY(oop_arraycopy()),\n-                                                             STUB_ENTRY(oop_disjoint_arraycopy()),\n-                                                             STUB_ENTRY(jlong_arraycopy()),\n-                                                             STUB_ENTRY(checkcast_arraycopy()));\n-\n-    \/\/ fill routines\n-#ifdef COMPILER2\n-    if (OptimizeFill) {\n-      StubRoutines::_jbyte_fill          = generate_fill(T_BYTE,  false, \"jbyte_fill\");\n-      StubRoutines::_jshort_fill         = generate_fill(T_SHORT, false, \"jshort_fill\");\n-      StubRoutines::_jint_fill           = generate_fill(T_INT,   false, \"jint_fill\");\n-      StubRoutines::_arrayof_jbyte_fill  = generate_fill(T_BYTE,  true, \"arrayof_jbyte_fill\");\n-      StubRoutines::_arrayof_jshort_fill = generate_fill(T_SHORT, true, \"arrayof_jshort_fill\");\n-      StubRoutines::_arrayof_jint_fill   = generate_fill(T_INT,   true, \"arrayof_jint_fill\");\n-    }\n-#endif\n-  }\n-\n-  \/\/ Stub for BigInteger::multiplyToLen()\n-  \/\/\n-  \/\/  Arguments:\n-  \/\/\n-  \/\/  Input:\n-  \/\/    R3 - x address\n-  \/\/    R4 - x length\n-  \/\/    R5 - y address\n-  \/\/    R6 - y length\n-  \/\/    R7 - z address\n-  \/\/\n-  address generate_multiplyToLen() {\n-\n-    StubCodeMark mark(this, \"StubRoutines\", \"multiplyToLen\");\n-\n-    address start = __ function_entry();\n-\n-    const Register x     = R3;\n-    const Register xlen  = R4;\n-    const Register y     = R5;\n-    const Register ylen  = R6;\n-    const Register z     = R7;\n-\n-    const Register tmp1  = R2; \/\/ TOC not used.\n-    const Register tmp2  = R9;\n-    const Register tmp3  = R10;\n-    const Register tmp4  = R11;\n-    const Register tmp5  = R12;\n-\n-    \/\/ non-volatile regs\n-    const Register tmp6  = R31;\n-    const Register tmp7  = R30;\n-    const Register tmp8  = R29;\n-    const Register tmp9  = R28;\n-    const Register tmp10 = R27;\n-    const Register tmp11 = R26;\n-    const Register tmp12 = R25;\n-    const Register tmp13 = R24;\n-\n-    BLOCK_COMMENT(\"Entry:\");\n-\n-    \/\/ C2 does not respect int to long conversion for stub calls.\n-    __ clrldi(xlen, xlen, 32);\n-    __ clrldi(ylen, ylen, 32);\n-\n-    \/\/ Save non-volatile regs (frameless).\n-    int current_offs = 8;\n-    __ std(R24, -current_offs, R1_SP); current_offs += 8;\n-    __ std(R25, -current_offs, R1_SP); current_offs += 8;\n-    __ std(R26, -current_offs, R1_SP); current_offs += 8;\n-    __ std(R27, -current_offs, R1_SP); current_offs += 8;\n-    __ std(R28, -current_offs, R1_SP); current_offs += 8;\n-    __ std(R29, -current_offs, R1_SP); current_offs += 8;\n-    __ std(R30, -current_offs, R1_SP); current_offs += 8;\n-    __ std(R31, -current_offs, R1_SP);\n-\n-    __ multiply_to_len(x, xlen, y, ylen, z, tmp1, tmp2, tmp3, tmp4, tmp5,\n-                       tmp6, tmp7, tmp8, tmp9, tmp10, tmp11, tmp12, tmp13);\n-\n-    \/\/ Restore non-volatile regs.\n-    current_offs = 8;\n-    __ ld(R24, -current_offs, R1_SP); current_offs += 8;\n-    __ ld(R25, -current_offs, R1_SP); current_offs += 8;\n-    __ ld(R26, -current_offs, R1_SP); current_offs += 8;\n-    __ ld(R27, -current_offs, R1_SP); current_offs += 8;\n-    __ ld(R28, -current_offs, R1_SP); current_offs += 8;\n-    __ ld(R29, -current_offs, R1_SP); current_offs += 8;\n-    __ ld(R30, -current_offs, R1_SP); current_offs += 8;\n-    __ ld(R31, -current_offs, R1_SP);\n-\n-    __ blr();  \/\/ Return to caller.\n-\n-    return start;\n-  }\n-\n-  \/**\n-  *  Arguments:\n-  *\n-  *  Input:\n-  *   R3_ARG1    - out address\n-  *   R4_ARG2    - in address\n-  *   R5_ARG3    - offset\n-  *   R6_ARG4    - len\n-  *   R7_ARG5    - k\n-  *  Output:\n-  *   R3_RET     - carry\n-  *\/\n-  address generate_mulAdd() {\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", \"mulAdd\");\n-\n-    address start = __ function_entry();\n-\n-    \/\/ C2 does not sign extend signed parameters to full 64 bits registers:\n-    __ rldic (R5_ARG3, R5_ARG3, 2, 32);  \/\/ always positive\n-    __ clrldi(R6_ARG4, R6_ARG4, 32);     \/\/ force zero bits on higher word\n-    __ clrldi(R7_ARG5, R7_ARG5, 32);     \/\/ force zero bits on higher word\n-\n-    __ muladd(R3_ARG1, R4_ARG2, R5_ARG3, R6_ARG4, R7_ARG5, R8, R9, R10);\n-\n-    \/\/ Moves output carry to return register\n-    __ mr    (R3_RET,  R10);\n-\n-    __ blr();\n-\n-    return start;\n-  }\n-\n-  \/**\n-  *  Arguments:\n-  *\n-  *  Input:\n-  *   R3_ARG1    - in address\n-  *   R4_ARG2    - in length\n-  *   R5_ARG3    - out address\n-  *   R6_ARG4    - out length\n-  *\/\n-  address generate_squareToLen() {\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", \"squareToLen\");\n-\n-    address start = __ function_entry();\n-\n-    \/\/ args - higher word is cleaned (unsignedly) due to int to long casting\n-    const Register in        = R3_ARG1;\n-    const Register in_len    = R4_ARG2;\n-    __ clrldi(in_len, in_len, 32);\n-    const Register out       = R5_ARG3;\n-    const Register out_len   = R6_ARG4;\n-    __ clrldi(out_len, out_len, 32);\n-\n-    \/\/ output\n-    const Register ret       = R3_RET;\n-\n-    \/\/ temporaries\n-    const Register lplw_s    = R7;\n-    const Register in_aux    = R8;\n-    const Register out_aux   = R9;\n-    const Register piece     = R10;\n-    const Register product   = R14;\n-    const Register lplw      = R15;\n-    const Register i_minus1  = R16;\n-    const Register carry     = R17;\n-    const Register offset    = R18;\n-    const Register off_aux   = R19;\n-    const Register t         = R20;\n-    const Register mlen      = R21;\n-    const Register len       = R22;\n-    const Register a         = R23;\n-    const Register b         = R24;\n-    const Register i         = R25;\n-    const Register c         = R26;\n-    const Register cs        = R27;\n-\n-    \/\/ Labels\n-    Label SKIP_LSHIFT, SKIP_DIAGONAL_SUM, SKIP_ADDONE, SKIP_LOOP_SQUARE;\n-    Label LOOP_LSHIFT, LOOP_DIAGONAL_SUM, LOOP_ADDONE, LOOP_SQUARE;\n-\n-    \/\/ Save non-volatile regs (frameless).\n-    int current_offs = -8;\n-    __ std(R28, current_offs, R1_SP); current_offs -= 8;\n-    __ std(R27, current_offs, R1_SP); current_offs -= 8;\n-    __ std(R26, current_offs, R1_SP); current_offs -= 8;\n-    __ std(R25, current_offs, R1_SP); current_offs -= 8;\n-    __ std(R24, current_offs, R1_SP); current_offs -= 8;\n-    __ std(R23, current_offs, R1_SP); current_offs -= 8;\n-    __ std(R22, current_offs, R1_SP); current_offs -= 8;\n-    __ std(R21, current_offs, R1_SP); current_offs -= 8;\n-    __ std(R20, current_offs, R1_SP); current_offs -= 8;\n-    __ std(R19, current_offs, R1_SP); current_offs -= 8;\n-    __ std(R18, current_offs, R1_SP); current_offs -= 8;\n-    __ std(R17, current_offs, R1_SP); current_offs -= 8;\n-    __ std(R16, current_offs, R1_SP); current_offs -= 8;\n-    __ std(R15, current_offs, R1_SP); current_offs -= 8;\n-    __ std(R14, current_offs, R1_SP);\n-\n-    \/\/ Store the squares, right shifted one bit (i.e., divided by 2)\n-    __ subi   (out_aux,   out,       8);\n-    __ subi   (in_aux,    in,        4);\n-    __ cmpwi  (CCR0,      in_len,    0);\n-    \/\/ Initialize lplw outside of the loop\n-    __ xorr   (lplw,      lplw,      lplw);\n-    __ ble    (CCR0,      SKIP_LOOP_SQUARE);    \/\/ in_len <= 0\n-    __ mtctr  (in_len);\n-\n-    __ bind(LOOP_SQUARE);\n-    __ lwzu   (piece,     4,         in_aux);\n-    __ mulld  (product,   piece,     piece);\n-    \/\/ shift left 63 bits and only keep the MSB\n-    __ rldic  (lplw_s,    lplw,      63, 0);\n-    __ mr     (lplw,      product);\n-    \/\/ shift right 1 bit without sign extension\n-    __ srdi   (product,   product,   1);\n-    \/\/ join them to the same register and store it\n-    __ orr    (product,   lplw_s,    product);\n-#ifdef VM_LITTLE_ENDIAN\n-    \/\/ Swap low and high words for little endian\n-    __ rldicl (product,   product,   32, 0);\n-#endif\n-    __ stdu   (product,   8,         out_aux);\n-    __ bdnz   (LOOP_SQUARE);\n-\n-    __ bind(SKIP_LOOP_SQUARE);\n-\n-    \/\/ Add in off-diagonal sums\n-    __ cmpwi  (CCR0,      in_len,    0);\n-    __ ble    (CCR0,      SKIP_DIAGONAL_SUM);\n-    \/\/ Avoid CTR usage here in order to use it at mulAdd\n-    __ subi   (i_minus1,  in_len,    1);\n-    __ li     (offset,    4);\n-\n-    __ bind(LOOP_DIAGONAL_SUM);\n-\n-    __ sldi   (off_aux,   out_len,   2);\n-    __ sub    (off_aux,   off_aux,   offset);\n-\n-    __ mr     (len,       i_minus1);\n-    __ sldi   (mlen,      i_minus1,  2);\n-    __ lwzx   (t,         in,        mlen);\n-\n-    __ muladd (out, in, off_aux, len, t, a, b, carry);\n-\n-    \/\/ begin<addOne>\n-    \/\/ off_aux = out_len*4 - 4 - mlen - offset*4 - 4;\n-    __ addi   (mlen,      mlen,      4);\n-    __ sldi   (a,         out_len,   2);\n-    __ subi   (a,         a,         4);\n-    __ sub    (a,         a,         mlen);\n-    __ subi   (off_aux,   offset,    4);\n-    __ sub    (off_aux,   a,         off_aux);\n-\n-    __ lwzx   (b,         off_aux,   out);\n-    __ add    (b,         b,         carry);\n-    __ stwx   (b,         off_aux,   out);\n-\n-    \/\/ if (((uint64_t)s >> 32) != 0) {\n-    __ srdi_  (a,         b,         32);\n-    __ beq    (CCR0,      SKIP_ADDONE);\n-\n-    \/\/ while (--mlen >= 0) {\n-    __ bind(LOOP_ADDONE);\n-    __ subi   (mlen,      mlen,      4);\n-    __ cmpwi  (CCR0,      mlen,      0);\n-    __ beq    (CCR0,      SKIP_ADDONE);\n-\n-    \/\/ if (--offset_aux < 0) { \/\/ Carry out of number\n-    __ subi   (off_aux,   off_aux,   4);\n-    __ cmpwi  (CCR0,      off_aux,   0);\n-    __ blt    (CCR0,      SKIP_ADDONE);\n-\n-    \/\/ } else {\n-    __ lwzx   (b,         off_aux,   out);\n-    __ addi   (b,         b,         1);\n-    __ stwx   (b,         off_aux,   out);\n-    __ cmpwi  (CCR0,      b,         0);\n-    __ bne    (CCR0,      SKIP_ADDONE);\n-    __ b      (LOOP_ADDONE);\n-\n-    __ bind(SKIP_ADDONE);\n-    \/\/ } } } end<addOne>\n-\n-    __ addi   (offset,    offset,    8);\n-    __ subi   (i_minus1,  i_minus1,  1);\n-    __ cmpwi  (CCR0,      i_minus1,  0);\n-    __ bge    (CCR0,      LOOP_DIAGONAL_SUM);\n-\n-    __ bind(SKIP_DIAGONAL_SUM);\n-\n-    \/\/ Shift back up and set low bit\n-    \/\/ Shifts 1 bit left up to len positions. Assumes no leading zeros\n-    \/\/ begin<primitiveLeftShift>\n-    __ cmpwi  (CCR0,      out_len,   0);\n-    __ ble    (CCR0,      SKIP_LSHIFT);\n-    __ li     (i,         0);\n-    __ lwz    (c,         0,         out);\n-    __ subi   (b,         out_len,   1);\n-    __ mtctr  (b);\n-\n-    __ bind(LOOP_LSHIFT);\n-    __ mr     (b,         c);\n-    __ addi   (cs,        i,         4);\n-    __ lwzx   (c,         out,       cs);\n-\n-    __ sldi   (b,         b,         1);\n-    __ srwi   (cs,        c,         31);\n-    __ orr    (b,         b,         cs);\n-    __ stwx   (b,         i,         out);\n-\n-    __ addi   (i,         i,         4);\n-    __ bdnz   (LOOP_LSHIFT);\n-\n-    __ sldi   (c,         out_len,   2);\n-    __ subi   (c,         c,         4);\n-    __ lwzx   (b,         out,       c);\n-    __ sldi   (b,         b,         1);\n-    __ stwx   (b,         out,       c);\n-\n-    __ bind(SKIP_LSHIFT);\n-    \/\/ end<primitiveLeftShift>\n-\n-    \/\/ Set low bit\n-    __ sldi   (i,         in_len,    2);\n-    __ subi   (i,         i,         4);\n-    __ lwzx   (i,         in,        i);\n-    __ sldi   (c,         out_len,   2);\n-    __ subi   (c,         c,         4);\n-    __ lwzx   (b,         out,       c);\n-\n-    __ andi   (i,         i,         1);\n-    __ orr    (i,         b,         i);\n-\n-    __ stwx   (i,         out,       c);\n-\n-    \/\/ Restore non-volatile regs.\n-    current_offs = -8;\n-    __ ld(R28, current_offs, R1_SP); current_offs -= 8;\n-    __ ld(R27, current_offs, R1_SP); current_offs -= 8;\n-    __ ld(R26, current_offs, R1_SP); current_offs -= 8;\n-    __ ld(R25, current_offs, R1_SP); current_offs -= 8;\n-    __ ld(R24, current_offs, R1_SP); current_offs -= 8;\n-    __ ld(R23, current_offs, R1_SP); current_offs -= 8;\n-    __ ld(R22, current_offs, R1_SP); current_offs -= 8;\n-    __ ld(R21, current_offs, R1_SP); current_offs -= 8;\n-    __ ld(R20, current_offs, R1_SP); current_offs -= 8;\n-    __ ld(R19, current_offs, R1_SP); current_offs -= 8;\n-    __ ld(R18, current_offs, R1_SP); current_offs -= 8;\n-    __ ld(R17, current_offs, R1_SP); current_offs -= 8;\n-    __ ld(R16, current_offs, R1_SP); current_offs -= 8;\n-    __ ld(R15, current_offs, R1_SP); current_offs -= 8;\n-    __ ld(R14, current_offs, R1_SP);\n-\n-    __ mr(ret, out);\n-    __ blr();\n-\n-    return start;\n-  }\n-\n-  \/**\n-   * Arguments:\n-   *\n-   * Inputs:\n-   *   R3_ARG1    - int   crc\n-   *   R4_ARG2    - byte* buf\n-   *   R5_ARG3    - int   length (of buffer)\n-   *\n-   * scratch:\n-   *   R2, R6-R12\n-   *\n-   * Output:\n-   *   R3_RET     - int   crc result\n-   *\/\n-  \/\/ Compute CRC32 function.\n-  address generate_CRC32_updateBytes(bool is_crc32c) {\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", is_crc32c ? \"CRC32C_updateBytes\" : \"CRC32_updateBytes\");\n-    address start = __ function_entry();  \/\/ Remember stub start address (is rtn value).\n-    __ crc32(R3_ARG1, R4_ARG2, R5_ARG3, R2, R6, R7, R8, R9, R10, R11, R12, is_crc32c);\n-    __ blr();\n-    return start;\n-  }\n-\n-  address generate_floatToFloat16() {\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", \"floatToFloat16\");\n-    address start = __ function_entry();\n-    __ f2hf(R3_RET, F1_ARG1, F0);\n-    __ blr();\n-    return start;\n-  }\n-\n-  address generate_float16ToFloat() {\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", \"float16ToFloat\");\n-    address start = __ function_entry();\n-    __ hf2f(F1_RET, R3_ARG1);\n-    __ blr();\n-    return start;\n-  }\n-\n-  address generate_method_entry_barrier() {\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", \"nmethod_entry_barrier\");\n-\n-    address stub_address = __ pc();\n-\n-    int nbytes_save = MacroAssembler::num_volatile_regs * BytesPerWord;\n-    __ save_volatile_gprs(R1_SP, -nbytes_save, true);\n-\n-    \/\/ Link register points to instruction in prologue of the guarded nmethod.\n-    \/\/ As the stub requires one layer of indirection (argument is of type address* and not address),\n-    \/\/ passing the link register's value directly doesn't work.\n-    \/\/ Since we have to save the link register on the stack anyway, we calculate the corresponding stack address\n-    \/\/ and pass that one instead.\n-    __ addi(R3_ARG1, R1_SP, _abi0(lr));\n-\n-    __ save_LR(R0);\n-    __ push_frame_reg_args(nbytes_save, R0);\n-\n-    __ call_VM_leaf(CAST_FROM_FN_PTR(address, BarrierSetNMethod::nmethod_stub_entry_barrier));\n-    __ mr(R0, R3_RET);\n-\n-    __ pop_frame();\n-    __ restore_LR(R3_RET \/* used as tmp register *\/);\n-    __ restore_volatile_gprs(R1_SP, -nbytes_save, true);\n-\n-    __ cmpdi(CCR0, R0, 0);\n-\n-    \/\/ Return to prologue if no deoptimization is required (bnelr)\n-    __ bclr(Assembler::bcondCRbiIs1, Assembler::bi0(CCR0, Assembler::equal), Assembler::bhintIsTaken);\n-\n-    \/\/ Deoptimization required.\n-    \/\/ For actually handling the deoptimization, the 'wrong method stub' is invoked.\n-    __ load_const_optimized(R0, SharedRuntime::get_handle_wrong_method_stub());\n-    __ mtctr(R0);\n-\n-    \/\/ Pop the frame built in the prologue.\n-    __ pop_frame();\n-\n-    \/\/ Restore link register.  Required as the 'wrong method stub' needs the caller's frame\n-    \/\/ to properly deoptimize this method (e.g. by re-resolving the call site for compiled methods).\n-    \/\/ This method's prologue is aborted.\n-    __ restore_LR(R0);\n-\n-    __ bctr();\n-    return stub_address;\n-  }\n-\n-#ifdef VM_LITTLE_ENDIAN\n-\/\/ The following Base64 decode intrinsic is based on an algorithm outlined\n-\/\/ in here:\n-\/\/ http:\/\/0x80.pl\/notesen\/2016-01-17-sse-base64-decoding.html\n-\/\/ in the section titled \"Vector lookup (pshufb with bitmask)\"\n-\/\/\n-\/\/ This implementation differs in the following ways:\n-\/\/  * Instead of Intel SSE instructions, Power AltiVec VMX and VSX instructions\n-\/\/    are used instead.  It turns out that some of the vector operations\n-\/\/    needed in the algorithm require fewer AltiVec instructions.\n-\/\/  * The algorithm in the above mentioned paper doesn't handle the\n-\/\/    Base64-URL variant in RFC 4648.  Adjustments to both the code and to two\n-\/\/    lookup tables are needed for this.\n-\/\/  * The \"Pack\" section of the code is a complete rewrite for Power because we\n-\/\/    can utilize better instructions for this step.\n-\/\/\n-\n-\/\/ Offsets per group of Base64 characters\n-\/\/ Uppercase\n-#define UC  (signed char)((-'A' + 0) & 0xff)\n-\/\/ Lowercase\n-#define LC  (signed char)((-'a' + 26) & 0xff)\n-\/\/ Digits\n-#define DIG (signed char)((-'0' + 52) & 0xff)\n-\/\/ Plus sign (URL = 0)\n-#define PLS (signed char)((-'+' + 62) & 0xff)\n-\/\/ Hyphen (URL = 1)\n-#define HYP (signed char)((-'-' + 62) & 0xff)\n-\/\/ Slash (URL = 0)\n-#define SLS (signed char)((-'\/' + 63) & 0xff)\n-\/\/ Underscore (URL = 1)\n-#define US  (signed char)((-'_' + 63) & 0xff)\n-\n-\/\/ For P10 (or later) only\n-#define VALID_B64 0x80\n-#define VB64(x) (VALID_B64 | x)\n-\n-#define BLK_OFFSETOF(x) (offsetof(constant_block, x))\n-\n-\/\/ In little-endian mode, the lxv instruction loads the element at EA into\n-\/\/ element 15 of the vector register, EA+1 goes into element 14, and so\n-\/\/ on.\n-\/\/\n-\/\/ To make a look-up table easier to read, ARRAY_TO_LXV_ORDER reverses the\n-\/\/ order of the elements in a vector initialization.\n-#define ARRAY_TO_LXV_ORDER(e0, e1, e2, e3, e4, e5, e6, e7, e8, e9, e10, e11, e12, e13, e14, e15) e15, e14, e13, e12, e11, e10, e9, e8, e7, e6, e5, e4, e3, e2, e1, e0\n-\n-  \/\/\n-  \/\/ Base64 decodeBlock intrinsic\n-  address generate_base64_decodeBlock() {\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", \"base64_decodeBlock\");\n-    address start   = __ function_entry();\n-\n-    typedef struct {\n-      signed char offsetLUT_val[16];\n-      signed char offsetLUT_URL_val[16];\n-      unsigned char maskLUT_val[16];\n-      unsigned char maskLUT_URL_val[16];\n-      unsigned char bitposLUT_val[16];\n-      unsigned char table_32_47_val[16];\n-      unsigned char table_32_47_URL_val[16];\n-      unsigned char table_48_63_val[16];\n-      unsigned char table_64_79_val[16];\n-      unsigned char table_80_95_val[16];\n-      unsigned char table_80_95_URL_val[16];\n-      unsigned char table_96_111_val[16];\n-      unsigned char table_112_127_val[16];\n-      unsigned char pack_lshift_val[16];\n-      unsigned char pack_rshift_val[16];\n-      unsigned char pack_permute_val[16];\n-    } constant_block;\n-\n-    alignas(16) static const constant_block const_block = {\n-\n-      .offsetLUT_val = {\n-        ARRAY_TO_LXV_ORDER(\n-        0,   0, PLS, DIG,  UC,  UC,  LC,  LC,\n-        0,   0,   0,   0,   0,   0,   0,   0 ) },\n-\n-      .offsetLUT_URL_val = {\n-        ARRAY_TO_LXV_ORDER(\n-        0,   0, HYP, DIG,  UC,  UC,  LC,  LC,\n-        0,   0,   0,   0,   0,   0,   0,   0 ) },\n-\n-      .maskLUT_val = {\n-        ARRAY_TO_LXV_ORDER(\n-        \/* 0        *\/ (unsigned char)0b10101000,\n-        \/* 1 .. 9   *\/ (unsigned char)0b11111000, (unsigned char)0b11111000, (unsigned char)0b11111000, (unsigned char)0b11111000,\n-                       (unsigned char)0b11111000, (unsigned char)0b11111000, (unsigned char)0b11111000, (unsigned char)0b11111000,\n-                       (unsigned char)0b11111000,\n-        \/* 10       *\/ (unsigned char)0b11110000,\n-        \/* 11       *\/ (unsigned char)0b01010100,\n-        \/* 12 .. 14 *\/ (unsigned char)0b01010000, (unsigned char)0b01010000, (unsigned char)0b01010000,\n-        \/* 15       *\/ (unsigned char)0b01010100 ) },\n-\n-      .maskLUT_URL_val = {\n-        ARRAY_TO_LXV_ORDER(\n-        \/* 0        *\/ (unsigned char)0b10101000,\n-        \/* 1 .. 9   *\/ (unsigned char)0b11111000, (unsigned char)0b11111000, (unsigned char)0b11111000, (unsigned char)0b11111000,\n-                       (unsigned char)0b11111000, (unsigned char)0b11111000, (unsigned char)0b11111000, (unsigned char)0b11111000,\n-                       (unsigned char)0b11111000,\n-        \/* 10       *\/ (unsigned char)0b11110000,\n-        \/* 11 .. 12 *\/ (unsigned char)0b01010000, (unsigned char)0b01010000,\n-        \/* 13       *\/ (unsigned char)0b01010100,\n-        \/* 14       *\/ (unsigned char)0b01010000,\n-        \/* 15       *\/ (unsigned char)0b01110000 ) },\n-\n-      .bitposLUT_val = {\n-        ARRAY_TO_LXV_ORDER(\n-        0x01, 0x02, 0x04, 0x08, 0x10, 0x20, 0x40, (unsigned char)0x80,\n-        0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00 ) },\n-\n-      \/\/ In the following table_*_val constants, a 0 value means the\n-      \/\/ character is not in the Base64 character set\n-      .table_32_47_val = {\n-        ARRAY_TO_LXV_ORDER (\n-         \/* space .. '*' = 0 *\/ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \/* '+' = 62 *\/ VB64(62), \/* ',' .. '.' = 0 *\/ 0, 0, 0, \/* '\/' = 63 *\/ VB64(63) ) },\n-\n-      .table_32_47_URL_val = {\n-        ARRAY_TO_LXV_ORDER(\n-         \/* space .. ',' = 0 *\/ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \/* '-' = 62 *\/ VB64(62), \/* '.' .. '\/' *\/ 0, 0 ) },\n-\n-      .table_48_63_val = {\n-        ARRAY_TO_LXV_ORDER(\n-         \/* '0' .. '9' = 52 .. 61 *\/ VB64(52), VB64(53), VB64(54), VB64(55), VB64(56), VB64(57), VB64(58), VB64(59), VB64(60), VB64(61),\n-         \/* ':' .. '?' = 0 *\/ 0, 0, 0, 0, 0, 0 ) },\n-\n-      .table_64_79_val = {\n-        ARRAY_TO_LXV_ORDER(\n-         \/* '@' = 0 *\/ 0, \/* 'A' .. 'O' = 0 .. 14 *\/ VB64(0), VB64(1), VB64(2), VB64(3), VB64(4), VB64(5), VB64(6), VB64(7), VB64(8),\n-         VB64(9), VB64(10), VB64(11), VB64(12), VB64(13), VB64(14) ) },\n-\n-      .table_80_95_val = {\n-        ARRAY_TO_LXV_ORDER(\/* 'P' .. 'Z' = 15 .. 25 *\/ VB64(15), VB64(16), VB64(17), VB64(18), VB64(19), VB64(20), VB64(21), VB64(22),\n-        VB64(23), VB64(24), VB64(25), \/* '[' .. '_' = 0 *\/ 0, 0, 0, 0, 0 ) },\n-\n-      .table_80_95_URL_val = {\n-        ARRAY_TO_LXV_ORDER(\/* 'P' .. 'Z' = 15 .. 25 *\/ VB64(15), VB64(16), VB64(17), VB64(18), VB64(19), VB64(20), VB64(21), VB64(22),\n-        VB64(23), VB64(24), VB64(25), \/* '[' .. '^' = 0 *\/ 0, 0, 0, 0, \/* '_' = 63 *\/ VB64(63) ) },\n-\n-      .table_96_111_val = {\n-        ARRAY_TO_LXV_ORDER(\/* '`' = 0 *\/ 0, \/* 'a' .. 'o' = 26 .. 40 *\/ VB64(26), VB64(27), VB64(28), VB64(29), VB64(30), VB64(31),\n-        VB64(32), VB64(33), VB64(34), VB64(35), VB64(36), VB64(37), VB64(38), VB64(39), VB64(40) ) },\n-\n-      .table_112_127_val = {\n-        ARRAY_TO_LXV_ORDER(\/* 'p' .. 'z' = 41 .. 51 *\/ VB64(41), VB64(42), VB64(43), VB64(44), VB64(45), VB64(46), VB64(47), VB64(48),\n-        VB64(49), VB64(50), VB64(51), \/* '{' .. DEL = 0 *\/ 0, 0, 0, 0, 0 ) },\n-\n-      .pack_lshift_val = {\n-        ARRAY_TO_LXV_ORDER(\n-        0, 6, 4, 2, 0, 6, 4, 2, 0, 6, 4, 2, 0, 6, 4, 2 ) },\n-\n-      .pack_rshift_val = {\n-        ARRAY_TO_LXV_ORDER(\n-        0, 2, 4, 0, 0, 2, 4, 0, 0, 2, 4, 0, 0, 2, 4, 0 ) },\n-\n-      \/\/ The first 4 index values are \"don't care\" because\n-      \/\/ we only use the first 12 bytes of the vector,\n-      \/\/ which are decoded from 16 bytes of Base64 characters.\n-      .pack_permute_val = {\n-        ARRAY_TO_LXV_ORDER(\n-         0, 0, 0, 0,\n-         0,  1,  2,\n-         4,  5,  6,\n-         8,  9, 10,\n-        12, 13, 14 ) }\n-    };\n-\n-    const unsigned block_size = 16;  \/\/ number of bytes to process in each pass through the loop\n-    const unsigned block_size_shift = 4;\n-\n-    \/\/ According to the ELF V2 ABI, registers r3-r12 are volatile and available for use without save\/restore\n-    Register s      = R3_ARG1; \/\/ source starting address of Base64 characters\n-    Register sp     = R4_ARG2; \/\/ source offset\n-    Register sl     = R5_ARG3; \/\/ source length = # of Base64 characters to be processed\n-    Register d      = R6_ARG4; \/\/ destination address\n-    Register dp     = R7_ARG5; \/\/ destination offset\n-    Register isURL  = R8_ARG6; \/\/ boolean, if non-zero indicates use of RFC 4648 base64url encoding\n-    Register isMIME = R9_ARG7; \/\/ boolean, if non-zero indicates use of RFC 2045 MIME encoding - not used\n-\n-    \/\/ Local variables\n-    Register const_ptr     = R9;  \/\/ used for loading constants\n-    Register tmp_reg       = R10; \/\/ used for speeding up load_constant_optimized()\n-\n-    \/\/ Re-use R9 and R10 to avoid using non-volatile registers (requires save\/restore)\n-    Register out           = R9;  \/\/ moving out (destination) pointer\n-    Register in            = R10; \/\/ moving in (source) pointer\n-\n-    \/\/ Volatile VSRS are 0..13, 32..51 (VR0..VR13)\n-    \/\/ VR Constants\n-    VectorRegister  vec_0s                  = VR0;\n-    VectorRegister  vec_4s                  = VR1;\n-    VectorRegister  vec_8s                  = VR2;\n-    VectorRegister  vec_special_case_char   = VR3;\n-    VectorRegister  pack_rshift             = VR4;\n-    VectorRegister  pack_lshift             = VR5;\n-\n-    \/\/ VSR Constants\n-    VectorSRegister offsetLUT               = VSR0;\n-    VectorSRegister maskLUT                 = VSR1;\n-    VectorSRegister bitposLUT               = VSR2;\n-    VectorSRegister vec_0xfs                = VSR3;\n-    VectorSRegister vec_special_case_offset = VSR4;\n-    VectorSRegister pack_permute            = VSR5;\n-\n-    \/\/ P10 (or later) VSR lookup constants\n-    VectorSRegister table_32_47             = VSR0;\n-    VectorSRegister table_48_63             = VSR1;\n-    VectorSRegister table_64_79             = VSR2;\n-    VectorSRegister table_80_95             = VSR3;\n-    VectorSRegister table_96_111            = VSR4;\n-    VectorSRegister table_112_127           = VSR6;\n-\n-    \/\/ Data read in and later converted\n-    VectorRegister  input                   = VR6;\n-    \/\/ Variable for testing Base64 validity\n-    VectorRegister  non_match               = VR10;\n-\n-    \/\/ P9 VR Variables for lookup\n-    VectorRegister  higher_nibble           = VR7;\n-    VectorRegister  eq_special_case_char    = VR8;\n-    VectorRegister  offsets                 = VR9;\n-\n-    \/\/ P9 VSR lookup variables\n-    VectorSRegister bit                     = VSR6;\n-    VectorSRegister lower_nibble            = VSR7;\n-    VectorSRegister M                       = VSR8;\n-\n-    \/\/ P10 (or later) VSR lookup variables\n-    VectorSRegister  xlate_a                = VSR7;\n-    VectorSRegister  xlate_b                = VSR8;\n-\n-    \/\/ Variables for pack\n-    \/\/ VR\n-    VectorRegister  l                       = VR7;  \/\/ reuse higher_nibble's register\n-    VectorRegister  r                       = VR8;  \/\/ reuse eq_special_case_char's register\n-    VectorRegister  gathered                = VR10; \/\/ reuse non_match's register\n-\n-    Label not_URL, calculate_size, loop_start, loop_exit, return_zero;\n-\n-    \/\/ The upper 32 bits of the non-pointer parameter registers are not\n-    \/\/ guaranteed to be zero, so mask off those upper bits.\n-    __ clrldi(sp, sp, 32);\n-    __ clrldi(sl, sl, 32);\n-\n-    \/\/ Don't handle the last 4 characters of the source, because this\n-    \/\/ VSX-based algorithm doesn't handle padding characters.  Also the\n-    \/\/ vector code will always write 16 bytes of decoded data on each pass,\n-    \/\/ but only the first 12 of those 16 bytes are valid data (16 base64\n-    \/\/ characters become 12 bytes of binary data), so for this reason we\n-    \/\/ need to subtract an additional 8 bytes from the source length, in\n-    \/\/ order not to write past the end of the destination buffer.  The\n-    \/\/ result of this subtraction implies that a Java function in the\n-    \/\/ Base64 class will be used to process the last 12 characters.\n-    __ sub(sl, sl, sp);\n-    __ subi(sl, sl, 12);\n-\n-    \/\/ Load CTR with the number of passes through the loop\n-    \/\/ = sl >> block_size_shift.  After the shift, if sl <= 0, there's too\n-    \/\/ little data to be processed by this intrinsic.\n-    __ srawi_(sl, sl, block_size_shift);\n-    __ ble(CCR0, return_zero);\n-    __ mtctr(sl);\n-\n-    \/\/ Clear the other two parameter registers upper 32 bits.\n-    __ clrldi(isURL, isURL, 32);\n-    __ clrldi(dp, dp, 32);\n-\n-    \/\/ Load constant vec registers that need to be loaded from memory\n-    __ load_const_optimized(const_ptr, (address)&const_block, tmp_reg);\n-    __ lxv(bitposLUT, BLK_OFFSETOF(bitposLUT_val), const_ptr);\n-    __ lxv(pack_rshift->to_vsr(), BLK_OFFSETOF(pack_rshift_val), const_ptr);\n-    __ lxv(pack_lshift->to_vsr(), BLK_OFFSETOF(pack_lshift_val), const_ptr);\n-    __ lxv(pack_permute, BLK_OFFSETOF(pack_permute_val), const_ptr);\n-\n-    \/\/ Splat the constants that can use xxspltib\n-    __ xxspltib(vec_0s->to_vsr(), 0);\n-    __ xxspltib(vec_8s->to_vsr(), 8);\n-    if (PowerArchitecturePPC64 >= 10) {\n-      \/\/ Using VALID_B64 for the offsets effectively strips the upper bit\n-      \/\/ of each byte that was selected from the table.  Setting the upper\n-      \/\/ bit gives us a way to distinguish between the 6-bit value of 0\n-      \/\/ from an error code of 0, which will happen if the character is\n-      \/\/ outside the range of the lookup, or is an illegal Base64\n-      \/\/ character, such as %.\n-      __ xxspltib(offsets->to_vsr(), VALID_B64);\n-\n-      __ lxv(table_48_63, BLK_OFFSETOF(table_48_63_val), const_ptr);\n-      __ lxv(table_64_79, BLK_OFFSETOF(table_64_79_val), const_ptr);\n-      __ lxv(table_80_95, BLK_OFFSETOF(table_80_95_val), const_ptr);\n-      __ lxv(table_96_111, BLK_OFFSETOF(table_96_111_val), const_ptr);\n-      __ lxv(table_112_127, BLK_OFFSETOF(table_112_127_val), const_ptr);\n-    } else {\n-      __ xxspltib(vec_4s->to_vsr(), 4);\n-      __ xxspltib(vec_0xfs, 0xf);\n-      __ lxv(bitposLUT, BLK_OFFSETOF(bitposLUT_val), const_ptr);\n-    }\n-\n-    \/\/ The rest of the constants use different values depending on the\n-    \/\/ setting of isURL\n-    __ cmpwi(CCR0, isURL, 0);\n-    __ beq(CCR0, not_URL);\n-\n-    \/\/ isURL != 0 (true)\n-    if (PowerArchitecturePPC64 >= 10) {\n-      __ lxv(table_32_47, BLK_OFFSETOF(table_32_47_URL_val), const_ptr);\n-      __ lxv(table_80_95, BLK_OFFSETOF(table_80_95_URL_val), const_ptr);\n-    } else {\n-      __ lxv(offsetLUT, BLK_OFFSETOF(offsetLUT_URL_val), const_ptr);\n-      __ lxv(maskLUT, BLK_OFFSETOF(maskLUT_URL_val), const_ptr);\n-      __ xxspltib(vec_special_case_char->to_vsr(), '_');\n-      __ xxspltib(vec_special_case_offset, (unsigned char)US);\n-    }\n-    __ b(calculate_size);\n-\n-    \/\/ isURL = 0 (false)\n-    __ bind(not_URL);\n-    if (PowerArchitecturePPC64 >= 10) {\n-      __ lxv(table_32_47, BLK_OFFSETOF(table_32_47_val), const_ptr);\n-      __ lxv(table_80_95, BLK_OFFSETOF(table_80_95_val), const_ptr);\n-    } else {\n-      __ lxv(offsetLUT, BLK_OFFSETOF(offsetLUT_val), const_ptr);\n-      __ lxv(maskLUT, BLK_OFFSETOF(maskLUT_val), const_ptr);\n-      __ xxspltib(vec_special_case_char->to_vsr(), '\/');\n-      __ xxspltib(vec_special_case_offset, (unsigned char)SLS);\n-    }\n-\n-    __ bind(calculate_size);\n-\n-    \/\/ out starts at d + dp\n-    __ add(out, d, dp);\n-\n-    \/\/ in starts at s + sp\n-    __ add(in, s, sp);\n-\n-    __ align(32);\n-    __ bind(loop_start);\n-    __ lxv(input->to_vsr(), 0, in); \/\/ offset=0\n-\n-    \/\/\n-    \/\/ Lookup\n-    \/\/\n-    if (PowerArchitecturePPC64 >= 10) {\n-      \/\/ Use xxpermx to do a lookup of each Base64 character in the\n-      \/\/ input vector and translate it to a 6-bit value + 0x80.\n-      \/\/ Characters which are not valid Base64 characters will result\n-      \/\/ in a zero in the corresponding byte.\n-      \/\/\n-      \/\/ Note that due to align(32) call above, the xxpermx instructions do\n-      \/\/ not require align_prefix() calls, since the final xxpermx\n-      \/\/ prefix+opcode is at byte 24.\n-      __ xxpermx(xlate_a, table_32_47, table_48_63, input->to_vsr(), 1);    \/\/ offset=4\n-      __ xxpermx(xlate_b, table_64_79, table_80_95, input->to_vsr(), 2);    \/\/ offset=12\n-      __ xxlor(xlate_b, xlate_a, xlate_b);                                  \/\/ offset=20\n-      __ xxpermx(xlate_a, table_96_111, table_112_127, input->to_vsr(), 3); \/\/ offset=24\n-      __ xxlor(input->to_vsr(), xlate_a, xlate_b);\n-      \/\/ Check for non-Base64 characters by comparing each byte to zero.\n-      __ vcmpequb_(non_match, input, vec_0s);\n-    } else {\n-      \/\/ Isolate the upper 4 bits of each character by shifting it right 4 bits\n-      __ vsrb(higher_nibble, input, vec_4s);\n-      \/\/ Isolate the lower 4 bits by masking\n-      __ xxland(lower_nibble, input->to_vsr(), vec_0xfs);\n-\n-      \/\/ Get the offset (the value to subtract from the byte) by using\n-      \/\/ a lookup table indexed by the upper 4 bits of the character\n-      __ xxperm(offsets->to_vsr(), offsetLUT, higher_nibble->to_vsr());\n-\n-      \/\/ Find out which elements are the special case character (isURL ? '\/' : '-')\n-      __ vcmpequb(eq_special_case_char, input, vec_special_case_char);\n-\n-      \/\/ For each character in the input which is a special case\n-      \/\/ character, replace its offset with one that is special for that\n-      \/\/ character.\n-      __ xxsel(offsets->to_vsr(), offsets->to_vsr(), vec_special_case_offset, eq_special_case_char->to_vsr());\n-\n-      \/\/ Use the lower_nibble to select a mask \"M\" from the lookup table.\n-      __ xxperm(M, maskLUT, lower_nibble);\n-\n-      \/\/ \"bit\" is used to isolate which of the bits in M is relevant.\n-      __ xxperm(bit, bitposLUT, higher_nibble->to_vsr());\n-\n-      \/\/ Each element of non_match correspond to one each of the 16 input\n-      \/\/ characters.  Those elements that become 0x00 after the xxland\n-      \/\/ instruction are invalid Base64 characters.\n-      __ xxland(non_match->to_vsr(), M, bit);\n-\n-      \/\/ Compare each element to zero\n-      \/\/\n-      __ vcmpequb_(non_match, non_match, vec_0s);\n-    }\n-    \/\/ vmcmpequb_ sets the EQ bit of CCR6 if no elements compare equal.\n-    \/\/ Any element comparing equal to zero means there is an error in\n-    \/\/ that element.  Note that the comparison result register\n-    \/\/ non_match is not referenced again.  Only CCR6-EQ matters.\n-    __ bne_predict_not_taken(CCR6, loop_exit);\n-\n-    \/\/ The Base64 characters had no errors, so add the offsets, which in\n-    \/\/ the case of Power10 is a constant vector of all 0x80's (see earlier\n-    \/\/ comment where the offsets register is loaded).\n-    __ vaddubm(input, input, offsets);\n-\n-    \/\/ Pack\n-    \/\/\n-    \/\/ In the tables below, b0, b1, .. b15 are the bytes of decoded\n-    \/\/ binary data, the first line of each of the cells (except for\n-    \/\/ the constants) uses the bit-field nomenclature from the\n-    \/\/ above-linked paper, whereas the second line is more specific\n-    \/\/ about which exact bits are present, and is constructed using the\n-    \/\/ Power ISA 3.x document style, where:\n-    \/\/\n-    \/\/ * The specifier after the colon depicts which bits are there.\n-    \/\/ * The bit numbering is big endian style (bit 0 is the most\n-    \/\/   significant).\n-    \/\/ * || is a concatenate operator.\n-    \/\/ * Strings of 0's are a field of zeros with the shown length, and\n-    \/\/   likewise for strings of 1's.\n-\n-    \/\/ Note that only e12..e15 are shown here because the shifting\n-    \/\/ and OR'ing pattern replicates for e8..e11, e4..7, and\n-    \/\/ e0..e3.\n-    \/\/\n-    \/\/ +======================+=================+======================+======================+=============+\n-    \/\/ |        Vector        |       e12       |         e13          |         e14          |     e15     |\n-    \/\/ |       Element        |                 |                      |                      |             |\n-    \/\/ +======================+=================+======================+======================+=============+\n-    \/\/ |    after vaddubm     |    00dddddd     |       00cccccc       |       00bbbbbb       |  00aaaaaa   |\n-    \/\/ |                      |   00||b2:2..7   | 00||b1:4..7||b2:0..1 | 00||b0:6..7||b1:0..3 | 00||b0:0..5 |\n-    \/\/ +----------------------+-----------------+----------------------+----------------------+-------------+\n-    \/\/ |     pack_lshift      |                 |         << 6         |         << 4         |    << 2     |\n-    \/\/ +----------------------+-----------------+----------------------+----------------------+-------------+\n-    \/\/ |     l after vslb     |    00dddddd     |       cc000000       |       bbbb0000       |  aaaaaa00   |\n-    \/\/ |                      |   00||b2:2..7   |   b2:0..1||000000    |    b1:0..3||0000     | b0:0..5||00 |\n-    \/\/ +----------------------+-----------------+----------------------+----------------------+-------------+\n-    \/\/ |     l after vslo     |    cc000000     |       bbbb0000       |       aaaaaa00       |  00000000   |\n-    \/\/ |                      | b2:0..1||000000 |    b1:0..3||0000     |     b0:0..5||00      |  00000000   |\n-    \/\/ +----------------------+-----------------+----------------------+----------------------+-------------+\n-    \/\/ |     pack_rshift      |                 |         >> 2         |         >> 4         |             |\n-    \/\/ +----------------------+-----------------+----------------------+----------------------+-------------+\n-    \/\/ |     r after vsrb     |    00dddddd     |       0000cccc       |       000000bb       |  00aaaaaa   |\n-    \/\/ |                      |   00||b2:2..7   |    0000||b1:4..7     |   000000||b0:6..7    | 00||b0:0..5 |\n-    \/\/ +----------------------+-----------------+----------------------+----------------------+-------------+\n-    \/\/ | gathered after xxlor |    ccdddddd     |       bbbbcccc       |       aaaaaabb       |  00aaaaaa   |\n-    \/\/ |                      |     b2:0..7     |       b1:0..7        |       b0:0..7        | 00||b0:0..5 |\n-    \/\/ +======================+=================+======================+======================+=============+\n-    \/\/\n-    \/\/ Note: there is a typo in the above-linked paper that shows the result of the gathering process is:\n-    \/\/ [ddddddcc|bbbbcccc|aaaaaabb]\n-    \/\/ but should be:\n-    \/\/ [ccdddddd|bbbbcccc|aaaaaabb]\n-    \/\/\n-    __ vslb(l, input, pack_lshift);\n-    \/\/ vslo of vec_8s shifts the vector by one octet toward lower\n-    \/\/ element numbers, discarding element 0.  This means it actually\n-    \/\/ shifts to the right (not left) according to the order of the\n-    \/\/ table above.\n-    __ vslo(l, l, vec_8s);\n-    __ vsrb(r, input, pack_rshift);\n-    __ xxlor(gathered->to_vsr(), l->to_vsr(), r->to_vsr());\n-\n-    \/\/ Final rearrangement of bytes into their correct positions.\n-    \/\/ +==============+======+======+======+======+=====+=====+====+====+====+====+=====+=====+=====+=====+=====+=====+\n-    \/\/ |    Vector    |  e0  |  e1  |  e2  |  e3  | e4  | e5  | e6 | e7 | e8 | e9 | e10 | e11 | e12 | e13 | e14 | e15 |\n-    \/\/ |   Elements   |      |      |      |      |     |     |    |    |    |    |     |     |     |     |     |     |\n-    \/\/ +==============+======+======+======+======+=====+=====+====+====+====+====+=====+=====+=====+=====+=====+=====+\n-    \/\/ | after xxlor  | b11  | b10  |  b9  |  xx  | b8  | b7  | b6 | xx | b5 | b4 | b3  | xx  | b2  | b1  | b0  | xx  |\n-    \/\/ +--------------+------+------+------+------+-----+-----+----+----+----+----+-----+-----+-----+-----+-----+-----+\n-    \/\/ | pack_permute |  0   |  0   |  0   |  0   |  0  |  1  | 2  | 4  | 5  | 6  |  8  |  9  | 10  | 12  | 13  | 14  |\n-    \/\/ +--------------+------+------+------+------+-----+-----+----+----+----+----+-----+-----+-----+-----+-----+-----+\n-    \/\/ | after xxperm | b11* | b11* | b11* | b11* | b11 | b10 | b9 | b8 | b7 | b6 | b5  | b4  | b3  | b2  | b1  | b0  |\n-    \/\/ +==============+======+======+======+======+=====+=====+====+====+====+====+=====+=====+=====+=====+=====+=====+\n-    \/\/ xx bytes are not used to form the final data\n-    \/\/ b0..b15 are the decoded and reassembled 8-bit bytes of data\n-    \/\/ b11 with asterisk is a \"don't care\", because these bytes will be\n-    \/\/ overwritten on the next iteration.\n-    __ xxperm(gathered->to_vsr(), gathered->to_vsr(), pack_permute);\n-\n-    \/\/ We cannot use a static displacement on the store, since it's a\n-    \/\/ multiple of 12, not 16.  Note that this stxv instruction actually\n-    \/\/ writes 16 bytes, even though only the first 12 are valid data.\n-    __ stxv(gathered->to_vsr(), 0, out);\n-    __ addi(out, out, 12);\n-    __ addi(in, in, 16);\n-    __ bdnz(loop_start);\n-\n-    __ bind(loop_exit);\n-\n-    \/\/ Return the number of out bytes produced, which is (out - (d + dp)) == out - d - dp;\n-    __ sub(R3_RET, out, d);\n-    __ sub(R3_RET, R3_RET, dp);\n-\n-    __ blr();\n-\n-    __ bind(return_zero);\n-    __ li(R3_RET, 0);\n-    __ blr();\n-\n-    return start;\n-  }\n-\n-#undef UC\n-#undef LC\n-#undef DIG\n-#undef PLS\n-#undef HYP\n-#undef SLS\n-#undef US\n-\n-\/\/ This algorithm is based on the methods described in this paper:\n-\/\/ http:\/\/0x80.pl\/notesen\/2016-01-12-sse-base64-encoding.html\n-\/\/\n-\/\/ The details of this implementation vary from the paper due to the\n-\/\/ difference in the ISA between SSE and AltiVec, especially in the\n-\/\/ splitting bytes section where there is no need on Power to mask after\n-\/\/ the shift because the shift is byte-wise rather than an entire an entire\n-\/\/ 128-bit word.\n-\/\/\n-\/\/ For the lookup part of the algorithm, different logic is used than\n-\/\/ described in the paper because of the availability of vperm, which can\n-\/\/ do a 64-byte table lookup in four instructions, while preserving the\n-\/\/ branchless nature.\n-\/\/\n-\/\/ Description of the ENCODE_CORE macro\n-\/\/\n-\/\/ Expand first 12 x 8-bit data bytes into 16 x 6-bit bytes (upper 2\n-\/\/ bits of each byte are zeros)\n-\/\/\n-\/\/ (Note: e7..e0 are not shown because they follow the same pattern as\n-\/\/ e8..e15)\n-\/\/\n-\/\/ In the table below, b0, b1, .. b15 are the bytes of unencoded\n-\/\/ binary data, the first line of each of the cells (except for\n-\/\/ the constants) uses the bit-field nomenclature from the\n-\/\/ above-linked paper, whereas the second line is more specific\n-\/\/ about which exact bits are present, and is constructed using the\n-\/\/ Power ISA 3.x document style, where:\n-\/\/\n-\/\/ * The specifier after the colon depicts which bits are there.\n-\/\/ * The bit numbering is big endian style (bit 0 is the most\n-\/\/   significant).\n-\/\/ * || is a concatenate operator.\n-\/\/ * Strings of 0's are a field of zeros with the shown length, and\n-\/\/   likewise for strings of 1's.\n-\/\/\n-\/\/ +==========================+=============+======================+======================+=============+=============+======================+======================+=============+\n-\/\/ |          Vector          |     e8      |          e9          |         e10          |     e11     |     e12     |         e13          |         e14          |     e15     |\n-\/\/ |         Element          |             |                      |                      |             |             |                      |                      |             |\n-\/\/ +==========================+=============+======================+======================+=============+=============+======================+======================+=============+\n-\/\/ |        after lxv         |  jjjjkkkk   |       iiiiiijj       |       gghhhhhh       |  ffffgggg   |  eeeeeeff   |       ccdddddd       |       bbbbcccc       |  aaaaaabb   |\n-\/\/ |                          |     b7      |          b6          |          b5          |     b4      |     b3      |          b2          |          b1          |     b0      |\n-\/\/ +--------------------------+-------------+----------------------+----------------------+-------------+-------------+----------------------+----------------------+-------------+\n-\/\/ |      xxperm indexes      |      0      |          10          |          11          |     12      |      0      |          13          |          14          |     15      |\n-\/\/ +--------------------------+-------------+----------------------+----------------------+-------------+-------------+----------------------+----------------------+-------------+\n-\/\/ |     (1) after xxperm     |             |       gghhhhhh       |       ffffgggg       |  eeeeeeff   |             |       ccdddddd       |       bbbbcccc       |  aaaaaabb   |\n-\/\/ |                          |    (b15)    |          b5          |          b4          |     b3      |    (b15)    |          b2          |          b1          |     b0      |\n-\/\/ +--------------------------+-------------+----------------------+----------------------+-------------+-------------+----------------------+----------------------+-------------+\n-\/\/ |      rshift_amount       |      0      |          6           |          4           |      2      |      0      |          6           |          4           |      2      |\n-\/\/ +--------------------------+-------------+----------------------+----------------------+-------------+-------------+----------------------+----------------------+-------------+\n-\/\/ |        after vsrb        |             |       000000gg       |       0000ffff       |  00eeeeee   |             |       000000cc       |       0000bbbb       |  00aaaaaa   |\n-\/\/ |                          |    (b15)    |   000000||b5:0..1    |    0000||b4:0..3     | 00||b3:0..5 |    (b15)    |   000000||b2:0..1    |    0000||b1:0..3     | 00||b0:0..5 |\n-\/\/ +--------------------------+-------------+----------------------+----------------------+-------------+-------------+----------------------+----------------------+-------------+\n-\/\/ |       rshift_mask        |  00000000   |      000000||11      |      0000||1111      | 00||111111  |  00000000   |      000000||11      |      0000||1111      | 00||111111  |\n-\/\/ +--------------------------+-------------+----------------------+----------------------+-------------+-------------+----------------------+----------------------+-------------+\n-\/\/ |    rshift after vand     |  00000000   |       000000gg       |       0000ffff       |  00eeeeee   |  00000000   |       000000cc       |       0000bbbb       |  00aaaaaa   |\n-\/\/ |                          |  00000000   |   000000||b5:0..1    |    0000||b4:0..3     | 00||b3:0..5 |  00000000   |   000000||b2:0..1    |    0000||b1:0..3     | 00||b0:0..5 |\n-\/\/ +--------------------------+-------------+----------------------+----------------------+-------------+-------------+----------------------+----------------------+-------------+\n-\/\/ |    1 octet lshift (1)    |  gghhhhhh   |       ffffgggg       |       eeeeeeff       |             |  ccdddddd   |       bbbbcccc       |       aaaaaabb       |  00000000   |\n-\/\/ |                          |     b5      |          b4          |          b3          |    (b15)    |     b2      |          b1          |          b0          |  00000000   |\n-\/\/ +--------------------------+-------------+----------------------+----------------------+-------------+-------------+----------------------+----------------------+-------------+\n-\/\/ |      lshift_amount       |      0      |          2           |          4           |      0      |      0      |          2           |          4           |      0      |\n-\/\/ +--------------------------+-------------+----------------------+----------------------+-------------+-------------+----------------------+----------------------+-------------+\n-\/\/ |        after vslb        |  gghhhhhh   |       ffgggg00       |       eeff0000       |             |  ccdddddd   |       bbcccc00       |       aabb0000       |  00000000   |\n-\/\/ |                          |     b5      |     b4:2..7||00      |    b3:4..7||0000     |    (b15)    |   b2:0..7   |     b1:2..7||00      |    b0:4..7||0000     |  00000000   |\n-\/\/ +--------------------------+-------------+----------------------+----------------------+-------------+-------------+----------------------+----------------------+-------------+\n-\/\/ |       lshift_mask        | 00||111111  |     00||1111||00     |     00||11||0000     |  00000000   | 00||111111  |     00||1111||00     |     00||11||0000     |  00000000   |\n-\/\/ +--------------------------+-------------+----------------------+----------------------+-------------+-------------+----------------------+----------------------+-------------+\n-\/\/ |    lshift after vand     |  00hhhhhh   |       00gggg00       |       00ff0000       |  00000000   |  00dddddd   |       00cccc00       |       00bb0000       |  00000000   |\n-\/\/ |                          | 00||b5:2..7 |   00||b4:4..7||00    |  00||b3:6..7||0000   |  00000000   | 00||b2:2..7 |   00||b1:4..7||00    |  00||b0:6..7||0000   |  00000000   |\n-\/\/ +--------------------------+-------------+----------------------+----------------------+-------------+-------------+----------------------+----------------------+-------------+\n-\/\/ | after vor lshift, rshift |  00hhhhhh   |       00gggggg       |       00ffffff       |  00eeeeee   |  00dddddd   |       00cccccc       |       00bbbbbb       |  00aaaaaa   |\n-\/\/ |                          | 00||b5:2..7 | 00||b4:4..7||b5:0..1 | 00||b3:6..7||b4:0..3 | 00||b3:0..5 | 00||b2:2..7 | 00||b1:4..7||b2:0..1 | 00||b0:6..7||b1:0..3 | 00||b0:0..5 |\n-\/\/ +==========================+=============+======================+======================+=============+=============+======================+======================+=============+\n-\/\/\n-\/\/ Expand the first 12 bytes into 16 bytes, leaving every 4th byte\n-\/\/ blank for now.\n-\/\/ __ xxperm(input->to_vsr(), input->to_vsr(), expand_permute);\n-\/\/\n-\/\/ Generate two bit-shifted pieces - rshift and lshift - that will\n-\/\/ later be OR'd together.\n-\/\/\n-\/\/ First the right-shifted piece\n-\/\/ __ vsrb(rshift, input, expand_rshift);\n-\/\/ __ vand(rshift, rshift, expand_rshift_mask);\n-\/\/\n-\/\/ Now the left-shifted piece, which is done by octet shifting\n-\/\/ the input one byte to the left, then doing a variable shift,\n-\/\/ followed by a mask operation.\n-\/\/\n-\/\/ __ vslo(lshift, input, vec_8s);\n-\/\/ __ vslb(lshift, lshift, expand_lshift);\n-\/\/ __ vand(lshift, lshift, expand_lshift_mask);\n-\/\/\n-\/\/ Combine the two pieces by OR'ing\n-\/\/ __ vor(expanded, rshift, lshift);\n-\/\/\n-\/\/ At this point, expanded is a vector containing a 6-bit value in each\n-\/\/ byte.  These values are used as indexes into a 64-byte lookup table that\n-\/\/ is contained in four vector registers.  The lookup operation is done\n-\/\/ using vperm instructions with the same indexes for the lower 32 and\n-\/\/ upper 32 bytes.  To figure out which of the two looked-up bytes to use\n-\/\/ at each location, all values in expanded are compared to 31.  Using\n-\/\/ vsel, values higher than 31 use the results from the upper 32 bytes of\n-\/\/ the lookup operation, while values less than or equal to 31 use the\n-\/\/ lower 32 bytes of the lookup operation.\n-\/\/\n-\/\/ Note: it's tempting to use a xxpermx,xxpermx,vor sequence here on\n-\/\/ Power10 (or later), but experiments doing so on Power10 yielded a slight\n-\/\/ performance drop, perhaps due to the need for xxpermx instruction\n-\/\/ prefixes.\n-\n-#define ENCODE_CORE                                                        \\\n-    __ xxperm(input->to_vsr(), input->to_vsr(), expand_permute);           \\\n-    __ vsrb(rshift, input, expand_rshift);                                 \\\n-    __ vand(rshift, rshift, expand_rshift_mask);                           \\\n-    __ vslo(lshift, input, vec_8s);                                        \\\n-    __ vslb(lshift, lshift, expand_lshift);                                \\\n-    __ vand(lshift, lshift, expand_lshift_mask);                           \\\n-    __ vor(expanded, rshift, lshift);                                      \\\n-    __ vperm(encoded_00_31, vec_base64_00_15, vec_base64_16_31, expanded); \\\n-    __ vperm(encoded_32_63, vec_base64_32_47, vec_base64_48_63, expanded); \\\n-    __ vcmpgtub(gt_31, expanded, vec_31s);                                 \\\n-    __ vsel(expanded, encoded_00_31, encoded_32_63, gt_31);\n-\n-\/\/ Intrinsic function prototype in Base64.java:\n-\/\/ private void encodeBlock(byte[] src, int sp, int sl, byte[] dst, int dp, boolean isURL) {\n-\n-  address generate_base64_encodeBlock() {\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", \"base64_encodeBlock\");\n-    address start   = __ function_entry();\n-\n-    typedef struct {\n-      unsigned char expand_permute_val[16];\n-      unsigned char expand_rshift_val[16];\n-      unsigned char expand_rshift_mask_val[16];\n-      unsigned char expand_lshift_val[16];\n-      unsigned char expand_lshift_mask_val[16];\n-      unsigned char base64_00_15_val[16];\n-      unsigned char base64_16_31_val[16];\n-      unsigned char base64_32_47_val[16];\n-      unsigned char base64_48_63_val[16];\n-      unsigned char base64_48_63_URL_val[16];\n-    } constant_block;\n-\n-    alignas(16) static const constant_block const_block = {\n-      .expand_permute_val = {\n-        ARRAY_TO_LXV_ORDER(\n-        0,  4,  5,  6,\n-        0,  7,  8,  9,\n-        0, 10, 11, 12,\n-        0, 13, 14, 15 ) },\n-\n-      .expand_rshift_val = {\n-        ARRAY_TO_LXV_ORDER(\n-        0, 6, 4, 2,\n-        0, 6, 4, 2,\n-        0, 6, 4, 2,\n-        0, 6, 4, 2 ) },\n-\n-      .expand_rshift_mask_val = {\n-        ARRAY_TO_LXV_ORDER(\n-        0b00000000, 0b00000011, 0b00001111, 0b00111111,\n-        0b00000000, 0b00000011, 0b00001111, 0b00111111,\n-        0b00000000, 0b00000011, 0b00001111, 0b00111111,\n-        0b00000000, 0b00000011, 0b00001111, 0b00111111 ) },\n-\n-      .expand_lshift_val = {\n-        ARRAY_TO_LXV_ORDER(\n-        0, 2, 4, 0,\n-        0, 2, 4, 0,\n-        0, 2, 4, 0,\n-        0, 2, 4, 0 ) },\n-\n-      .expand_lshift_mask_val = {\n-        ARRAY_TO_LXV_ORDER(\n-        0b00111111, 0b00111100, 0b00110000, 0b00000000,\n-        0b00111111, 0b00111100, 0b00110000, 0b00000000,\n-        0b00111111, 0b00111100, 0b00110000, 0b00000000,\n-        0b00111111, 0b00111100, 0b00110000, 0b00000000 ) },\n-\n-      .base64_00_15_val = {\n-        ARRAY_TO_LXV_ORDER(\n-        'A','B','C','D','E','F','G','H','I','J','K','L','M','N','O','P' ) },\n-\n-      .base64_16_31_val = {\n-        ARRAY_TO_LXV_ORDER(\n-        'Q','R','S','T','U','V','W','X','Y','Z','a','b','c','d','e','f' ) },\n-\n-      .base64_32_47_val = {\n-        ARRAY_TO_LXV_ORDER(\n-        'g','h','i','j','k','l','m','n','o','p','q','r','s','t','u','v' ) },\n-\n-      .base64_48_63_val = {\n-        ARRAY_TO_LXV_ORDER(\n-        'w','x','y','z','0','1','2','3','4','5','6','7','8','9','+','\/' ) },\n-\n-      .base64_48_63_URL_val = {\n-        ARRAY_TO_LXV_ORDER(\n-        'w','x','y','z','0','1','2','3','4','5','6','7','8','9','-','_' ) }\n-    };\n-\n-    \/\/ Number of bytes to process in each pass through the main loop.\n-    \/\/ 12 of the 16 bytes from each lxv are encoded to 16 Base64 bytes.\n-    const unsigned block_size = 12;\n-\n-    \/\/ According to the ELF V2 ABI, registers r3-r12 are volatile and available for use without save\/restore\n-    Register src       = R3_ARG1; \/\/ source starting address of Base64 characters\n-    Register sp        = R4_ARG2; \/\/ source starting position\n-    Register sl        = R5_ARG3; \/\/ total source length of the Base64 characters to be processed\n-    Register dst       = R6_ARG4; \/\/ destination address\n-    Register dp        = R7_ARG5; \/\/ destination starting position\n-    Register isURL     = R8_ARG6; \/\/ boolean, if non-zero indicates use of RFC 4648 base64url encoding\n-\n-    \/\/ Local variables\n-    Register const_ptr     = R12; \/\/ used for loading constants (reuses isURL's register)\n-    Register tmp_reg       = R9;  \/\/ used for speeding up load_constant()\n-\n-    Register size           = R9;  \/\/ number of bytes to process (reuses tmp_reg's register)\n-    Register blocked_size   = R10; \/\/ number of bytes to process a block at a time\n-    Register block_modulo   = R12; \/\/ == block_size (reuse const_ptr)\n-    Register remaining      = R12; \/\/ bytes remaining to process after the blocks are completed (reuse block_modulo's reg)\n-    Register in             = R4;  \/\/ current input (source) pointer (reuse sp's register)\n-    Register num_blocks     = R11; \/\/ number of blocks to be processed by the loop\n-    Register out            = R8;  \/\/ current output (destination) pointer (reuse const_ptr's register)\n-    Register three          = R9;  \/\/ constant divisor (reuse size's register)\n-    Register bytes_to_write = R10; \/\/ number of bytes to write with the stxvl instr (reused blocked_size's register)\n-    Register tmp1           = R7;  \/\/ temp register for lxvl length (reuse dp's register)\n-    Register modulo_chars   = R7;  \/\/ number of bytes written during the final write % 4 (reuse tmp1's register)\n-    Register pad_char       = R6;  \/\/ literal '=' (reuse dst's register)\n-\n-    \/\/ Volatile VSRS are 0..13, 32..51 (VR0..VR13)\n-    \/\/ VR Constants\n-    VectorRegister  vec_8s             = VR0;\n-    VectorRegister  vec_31s            = VR1;\n-    VectorRegister  vec_base64_00_15   = VR2;\n-    VectorRegister  vec_base64_16_31   = VR3;\n-    VectorRegister  vec_base64_32_47   = VR4;\n-    VectorRegister  vec_base64_48_63   = VR5;\n-    VectorRegister  expand_rshift      = VR6;\n-    VectorRegister  expand_rshift_mask = VR7;\n-    VectorRegister  expand_lshift      = VR8;\n-    VectorRegister  expand_lshift_mask = VR9;\n-\n-    \/\/ VR variables for expand\n-    VectorRegister  input              = VR10;\n-    VectorRegister  rshift             = VR11;\n-    VectorRegister  lshift             = VR12;\n-    VectorRegister  expanded           = VR13;\n-\n-    \/\/ VR variables for lookup\n-    VectorRegister  encoded_00_31      = VR10; \/\/ (reuse input)\n-    VectorRegister  encoded_32_63      = VR11; \/\/ (reuse rshift)\n-    VectorRegister  gt_31              = VR12; \/\/ (reuse lshift)\n-\n-    \/\/ VSR Constants\n-    VectorSRegister expand_permute     = VSR0;\n-\n-    Label not_URL, calculate_size, calculate_blocked_size, skip_loop;\n-    Label loop_start, le_16_to_write, no_pad, one_pad_char;\n-\n-    \/\/ The upper 32 bits of the non-pointer parameter registers are not\n-    \/\/ guaranteed to be zero, so mask off those upper bits.\n-    __ clrldi(sp, sp, 32);\n-    __ clrldi(sl, sl, 32);\n-    __ clrldi(dp, dp, 32);\n-    __ clrldi(isURL, isURL, 32);\n-\n-    \/\/ load up the constants\n-    __ load_const_optimized(const_ptr, (address)&const_block, tmp_reg);\n-    __ lxv(expand_permute,               BLK_OFFSETOF(expand_permute_val),     const_ptr);\n-    __ lxv(expand_rshift->to_vsr(),      BLK_OFFSETOF(expand_rshift_val),      const_ptr);\n-    __ lxv(expand_rshift_mask->to_vsr(), BLK_OFFSETOF(expand_rshift_mask_val), const_ptr);\n-    __ lxv(expand_lshift->to_vsr(),      BLK_OFFSETOF(expand_lshift_val),      const_ptr);\n-    __ lxv(expand_lshift_mask->to_vsr(), BLK_OFFSETOF(expand_lshift_mask_val), const_ptr);\n-    __ lxv(vec_base64_00_15->to_vsr(),   BLK_OFFSETOF(base64_00_15_val),       const_ptr);\n-    __ lxv(vec_base64_16_31->to_vsr(),   BLK_OFFSETOF(base64_16_31_val),       const_ptr);\n-    __ lxv(vec_base64_32_47->to_vsr(),   BLK_OFFSETOF(base64_32_47_val),       const_ptr);\n-\n-    \/\/ Splat the constants that can use xxspltib\n-    __ xxspltib(vec_8s->to_vsr(), 8);\n-    __ xxspltib(vec_31s->to_vsr(), 31);\n-\n-\n-    \/\/ Use a different translation lookup table depending on the\n-    \/\/ setting of isURL\n-    __ cmpdi(CCR0, isURL, 0);\n-    __ beq(CCR0, not_URL);\n-    __ lxv(vec_base64_48_63->to_vsr(), BLK_OFFSETOF(base64_48_63_URL_val), const_ptr);\n-    __ b(calculate_size);\n-\n-    __ bind(not_URL);\n-    __ lxv(vec_base64_48_63->to_vsr(), BLK_OFFSETOF(base64_48_63_val), const_ptr);\n-\n-    __ bind(calculate_size);\n-\n-    \/\/ size = sl - sp - 4 (*)\n-    \/\/ (*) Don't process the last four bytes in the main loop because\n-    \/\/ we don't want the lxv instruction to read past the end of the src\n-    \/\/ data, in case those four bytes are on the start of an unmapped or\n-    \/\/ otherwise inaccessible page.\n-    \/\/\n-    __ sub(size, sl, sp);\n-    __ subi(size, size, 4);\n-    __ cmpdi(CCR7, size, block_size);\n-    __ bgt(CCR7, calculate_blocked_size);\n-    __ mr(remaining, size);\n-    \/\/ Add the 4 back into remaining again\n-    __ addi(remaining, remaining, 4);\n-    \/\/ make \"in\" point to the beginning of the source data: in = src + sp\n-    __ add(in, src, sp);\n-    \/\/ out = dst + dp\n-    __ add(out, dst, dp);\n-    __ b(skip_loop);\n-\n-    __ bind(calculate_blocked_size);\n-    __ li(block_modulo, block_size);\n-    \/\/ num_blocks = size \/ block_modulo\n-    __ divwu(num_blocks, size, block_modulo);\n-    \/\/ blocked_size = num_blocks * size\n-    __ mullw(blocked_size, num_blocks, block_modulo);\n-    \/\/ remaining = size - blocked_size\n-    __ sub(remaining, size, blocked_size);\n-    __ mtctr(num_blocks);\n-\n-    \/\/ Add the 4 back in to remaining again\n-    __ addi(remaining, remaining, 4);\n-\n-    \/\/ make \"in\" point to the beginning of the source data: in = src + sp\n-    __ add(in, src, sp);\n-\n-    \/\/ out = dst + dp\n-    __ add(out, dst, dp);\n-\n-    __ align(32);\n-    __ bind(loop_start);\n-\n-    __ lxv(input->to_vsr(), 0, in);\n-\n-    ENCODE_CORE\n-\n-    __ stxv(expanded->to_vsr(), 0, out);\n-    __ addi(in, in, 12);\n-    __ addi(out, out, 16);\n-    __ bdnz(loop_start);\n-\n-    __ bind(skip_loop);\n-\n-    \/\/ When there are less than 16 bytes left, we need to be careful not to\n-    \/\/ read beyond the end of the src buffer, which might be in an unmapped\n-    \/\/ page.\n-    \/\/ Load the remaining bytes using lxvl.\n-    __ rldicr(tmp1, remaining, 56, 7);\n-    __ lxvl(input->to_vsr(), in, tmp1);\n-\n-    ENCODE_CORE\n-\n-    \/\/ bytes_to_write = ((remaining * 4) + 2) \/ 3\n-    __ li(three, 3);\n-    __ rlwinm(bytes_to_write, remaining, 2, 0, 29); \/\/ remaining * 4\n-    __ addi(bytes_to_write, bytes_to_write, 2);\n-    __ divwu(bytes_to_write, bytes_to_write, three);\n-\n-    __ cmpwi(CCR7, bytes_to_write, 16);\n-    __ ble_predict_taken(CCR7, le_16_to_write);\n-    __ stxv(expanded->to_vsr(), 0, out);\n-\n-    \/\/ We've processed 12 of the 13-15 data bytes, so advance the pointers,\n-    \/\/ and do one final pass for the remaining 1-3 bytes.\n-    __ addi(in, in, 12);\n-    __ addi(out, out, 16);\n-    __ subi(remaining, remaining, 12);\n-    __ subi(bytes_to_write, bytes_to_write, 16);\n-    __ rldicr(tmp1, bytes_to_write, 56, 7);\n-    __ lxvl(input->to_vsr(), in, tmp1);\n-\n-    ENCODE_CORE\n-\n-    __ bind(le_16_to_write);\n-    \/\/ shift bytes_to_write into the upper 8 bits of t1 for use by stxvl\n-    __ rldicr(tmp1, bytes_to_write, 56, 7);\n-    __ stxvl(expanded->to_vsr(), out, tmp1);\n-    __ add(out, out, bytes_to_write);\n-\n-    __ li(pad_char, '=');\n-    __ rlwinm_(modulo_chars, bytes_to_write, 0, 30, 31); \/\/ bytes_to_write % 4, set CCR0\n-    \/\/ Examples:\n-    \/\/    remaining  bytes_to_write  modulo_chars  num pad chars\n-    \/\/        0            0               0            0\n-    \/\/        1            2               2            2\n-    \/\/        2            3               3            1\n-    \/\/        3            4               0            0\n-    \/\/        4            6               2            2\n-    \/\/        5            7               3            1\n-    \/\/        ...\n-    \/\/       12           16               0            0\n-    \/\/       13           18               2            2\n-    \/\/       14           19               3            1\n-    \/\/       15           20               0            0\n-    __ beq(CCR0, no_pad);\n-    __ cmpwi(CCR7, modulo_chars, 3);\n-    __ beq(CCR7, one_pad_char);\n-\n-    \/\/ two pad chars\n-    __ stb(pad_char, out);\n-    __ addi(out, out, 1);\n-\n-    __ bind(one_pad_char);\n-    __ stb(pad_char, out);\n-\n-    __ bind(no_pad);\n-\n-    __ blr();\n-    return start;\n-  }\n-\n-#endif \/\/ VM_LITTLE_ENDIAN\n-\n-address generate_lookup_secondary_supers_table_stub(u1 super_klass_index) {\n-    StubCodeMark mark(this, \"StubRoutines\", \"lookup_secondary_supers_table\");\n-\n-    address start = __ pc();\n-    const Register\n-      r_super_klass  = R4_ARG2,\n-      r_array_base   = R3_ARG1,\n-      r_array_length = R7_ARG5,\n-      r_array_index  = R6_ARG4,\n-      r_sub_klass    = R5_ARG3,\n-      r_bitmap       = R11_scratch1,\n-      result         = R8_ARG6;\n-\n-    __ lookup_secondary_supers_table(r_sub_klass, r_super_klass,\n-                                     r_array_base, r_array_length, r_array_index,\n-                                     r_bitmap, result, super_klass_index);\n-    __ blr();\n-\n-    return start;\n-  }\n-\n-  \/\/ Slow path implementation for UseSecondarySupersTable.\n-  address generate_lookup_secondary_supers_table_slow_path_stub() {\n-    StubCodeMark mark(this, \"StubRoutines\", \"lookup_secondary_supers_table_slow_path\");\n-\n-    address start = __ pc();\n-    const Register\n-      r_super_klass  = R4_ARG2,\n-      r_array_base   = R3_ARG1,\n-      temp1          = R7_ARG5,\n-      r_array_index  = R6_ARG4,\n-      r_bitmap       = R11_scratch1,\n-      result         = R8_ARG6;\n-\n-    __ lookup_secondary_supers_table_slow_path(r_super_klass, r_array_base, r_array_index, r_bitmap, result, temp1);\n-    __ blr();\n-\n-    return start;\n-  }\n-\n-  address generate_cont_thaw(const char* label, Continuation::thaw_kind kind) {\n-    if (!Continuations::enabled()) return nullptr;\n-\n-    bool return_barrier = Continuation::is_thaw_return_barrier(kind);\n-    bool return_barrier_exception = Continuation::is_thaw_return_barrier_exception(kind);\n-\n-    StubCodeMark mark(this, \"StubRoutines\", label);\n-\n-    Register tmp1 = R10_ARG8;\n-    Register tmp2 = R9_ARG7;\n-    Register tmp3 = R8_ARG6;\n-    Register nvtmp = R15_esp;   \/\/ nonvolatile tmp register\n-    FloatRegister nvftmp = F20; \/\/ nonvolatile fp tmp register\n-\n-    address start = __ pc();\n-\n-    if (kind == Continuation::thaw_top) {\n-      __ clobber_nonvolatile_registers(); \/\/ Except R16_thread and R29_TOC\n-    }\n-\n-    if (return_barrier) {\n-      __ mr(nvtmp, R3_RET); __ fmr(nvftmp, F1_RET); \/\/ preserve possible return value from a method returning to the return barrier\n-      DEBUG_ONLY(__ ld_ptr(tmp1, _abi0(callers_sp), R1_SP);)\n-      __ ld_ptr(R1_SP, JavaThread::cont_entry_offset(), R16_thread);\n-#ifdef ASSERT\n-      __ ld_ptr(tmp2, _abi0(callers_sp), R1_SP);\n-      __ cmpd(CCR0, tmp1, tmp2);\n-      __ asm_assert_eq(FILE_AND_LINE \": callers sp is corrupt\");\n-#endif\n-    }\n-#ifdef ASSERT\n-    __ ld_ptr(tmp1, JavaThread::cont_entry_offset(), R16_thread);\n-    __ cmpd(CCR0, R1_SP, tmp1);\n-    __ asm_assert_eq(FILE_AND_LINE \": incorrect R1_SP\");\n-#endif\n-\n-    __ li(R4_ARG2, return_barrier ? 1 : 0);\n-    __ call_VM_leaf(CAST_FROM_FN_PTR(address, Continuation::prepare_thaw), R16_thread, R4_ARG2);\n-\n-#ifdef ASSERT\n-    DEBUG_ONLY(__ ld_ptr(tmp1, JavaThread::cont_entry_offset(), R16_thread));\n-    DEBUG_ONLY(__ cmpd(CCR0, R1_SP, tmp1));\n-    __ asm_assert_eq(FILE_AND_LINE \": incorrect R1_SP\");\n-#endif\n-\n-    \/\/ R3_RET contains the size of the frames to thaw, 0 if overflow or no more frames\n-    Label thaw_success;\n-    __ cmpdi(CCR0, R3_RET, 0);\n-    __ bne(CCR0, thaw_success);\n-    __ load_const_optimized(tmp1, (SharedRuntime::throw_StackOverflowError_entry()), R0);\n-    __ mtctr(tmp1); __ bctr();\n-    __ bind(thaw_success);\n-\n-    __ addi(R3_RET, R3_RET, frame::native_abi_reg_args_size); \/\/ Large abi required for C++ calls.\n-    __ neg(R3_RET, R3_RET);\n-    \/\/ align down resulting in a smaller negative offset\n-    __ clrrdi(R3_RET, R3_RET, exact_log2(frame::alignment_in_bytes));\n-    DEBUG_ONLY(__ mr(tmp1, R1_SP);)\n-    __ resize_frame(R3_RET, tmp2);  \/\/ make room for the thawed frames\n-\n-    __ li(R4_ARG2, kind);\n-    __ call_VM_leaf(Continuation::thaw_entry(), R16_thread, R4_ARG2);\n-    __ mr(R1_SP, R3_RET); \/\/ R3_RET contains the SP of the thawed top frame\n-\n-    if (return_barrier) {\n-      \/\/ we're now in the caller of the frame that returned to the barrier\n-      __ mr(R3_RET, nvtmp); __ fmr(F1_RET, nvftmp); \/\/ restore return value (no safepoint in the call to thaw, so even an oop return value should be OK)\n-    } else {\n-      \/\/ we're now on the yield frame (which is in an address above us b\/c rsp has been pushed down)\n-      __ li(R3_RET, 0); \/\/ return 0 (success) from doYield\n-    }\n-\n-    if (return_barrier_exception) {\n-      Register ex_pc = R17_tos;   \/\/ nonvolatile register\n-      __ ld(ex_pc, _abi0(lr), R1_SP); \/\/ LR\n-      __ mr(nvtmp, R3_RET); \/\/ save return value containing the exception oop\n-      \/\/ The thawed top frame has got a frame::java_abi. This is not sufficient for the runtime call.\n-      __ push_frame_reg_args(0, tmp1);\n-      __ call_VM_leaf(CAST_FROM_FN_PTR(address, SharedRuntime::exception_handler_for_return_address), R16_thread, ex_pc);\n-      __ mtlr(R3_RET); \/\/ the exception handler\n-      __ pop_frame();\n-      \/\/ See OptoRuntime::generate_exception_blob for register arguments\n-      __ mr(R3_ARG1, nvtmp); \/\/ exception oop\n-      __ mr(R4_ARG2, ex_pc); \/\/ exception pc\n-    } else {\n-      \/\/ We're \"returning\" into the topmost thawed frame; see Thaw::push_return_frame\n-      __ ld(R0, _abi0(lr), R1_SP); \/\/ LR\n-      __ mtlr(R0);\n-    }\n-    __ blr();\n-\n-    return start;\n-  }\n-\n-  address generate_cont_thaw() {\n-    return generate_cont_thaw(\"Cont thaw\", Continuation::thaw_top);\n-  }\n-\n-  \/\/ TODO: will probably need multiple return barriers depending on return type\n-\n-  address generate_cont_returnBarrier() {\n-    return generate_cont_thaw(\"Cont thaw return barrier\", Continuation::thaw_return_barrier);\n-  }\n-\n-  address generate_cont_returnBarrier_exception() {\n-    return generate_cont_thaw(\"Cont thaw return barrier exception\", Continuation::thaw_return_barrier_exception);\n-  }\n-\n-  address generate_cont_preempt_stub() {\n-    if (!Continuations::enabled()) return nullptr;\n-    StubCodeMark mark(this, \"StubRoutines\",\"Continuation preempt stub\");\n-    address start = __ pc();\n-\n-    __ clobber_nonvolatile_registers(); \/\/ Except R16_thread and R29_TOC\n-\n-    __ reset_last_Java_frame(false \/*check_last_java_sp*\/);\n-\n-    \/\/ Set sp to enterSpecial frame, i.e. remove all frames copied into the heap.\n-    __ ld_ptr(R1_SP, JavaThread::cont_entry_offset(), R16_thread);\n-\n-    Label preemption_cancelled;\n-    __ lbz(R11_scratch1, in_bytes(JavaThread::preemption_cancelled_offset()), R16_thread);\n-    __ cmpwi(CCR0, R11_scratch1, 0);\n-    __ bne(CCR0, preemption_cancelled);\n-\n-    \/\/ Remove enterSpecial frame from the stack and return to Continuation.run() to unmount.\n-    SharedRuntime::continuation_enter_cleanup(_masm);\n-    __ pop_frame();\n-    __ restore_LR(R11_scratch1);\n-    __ blr();\n-\n-    \/\/ We acquired the monitor after freezing the frames so call thaw to continue execution.\n-    __ bind(preemption_cancelled);\n-    __ li(R11_scratch1, 0); \/\/ false\n-    __ stb(R11_scratch1, in_bytes(JavaThread::preemption_cancelled_offset()), R16_thread);\n-    int simm16_offs = __ load_const_optimized(R11_scratch1, ContinuationEntry::thaw_call_pc_address(), R0, true);\n-    __ ld(R11_scratch1, simm16_offs, R11_scratch1);\n-    __ mtctr(R11_scratch1);\n-    __ bctr();\n-\n-    return start;\n-  }\n-\n-  \/\/ exception handler for upcall stubs\n-  address generate_upcall_stub_exception_handler() {\n-    StubCodeMark mark(this, \"StubRoutines\", \"upcall stub exception handler\");\n-    address start = __ pc();\n-\n-    \/\/ Native caller has no idea how to handle exceptions,\n-    \/\/ so we just crash here. Up to callee to catch exceptions.\n-    __ verify_oop(R3_ARG1);\n-    __ load_const_optimized(R12_scratch2, CAST_FROM_FN_PTR(uint64_t, UpcallLinker::handle_uncaught_exception), R0);\n-    __ call_c(R12_scratch2);\n-    __ should_not_reach_here();\n-\n-    return start;\n-  }\n-\n-  \/\/ load Method* target of MethodHandle\n-  \/\/ R3_ARG1 = jobject receiver\n-  \/\/ R19_method = result Method*\n-  address generate_upcall_stub_load_target() {\n-\n-    StubCodeMark mark(this, \"StubRoutines\", \"upcall_stub_load_target\");\n-    address start = __ pc();\n-\n-    __ resolve_global_jobject(R3_ARG1, R22_tmp2, R23_tmp3, MacroAssembler::PRESERVATION_FRAME_LR_GP_FP_REGS);\n-    \/\/ Load target method from receiver\n-    __ load_heap_oop(R19_method, java_lang_invoke_MethodHandle::form_offset(), R3_ARG1,\n-                     R22_tmp2, R23_tmp3, MacroAssembler::PRESERVATION_FRAME_LR_GP_FP_REGS, IS_NOT_NULL);\n-    __ load_heap_oop(R19_method, java_lang_invoke_LambdaForm::vmentry_offset(), R19_method,\n-                     R22_tmp2, R23_tmp3, MacroAssembler::PRESERVATION_FRAME_LR_GP_FP_REGS, IS_NOT_NULL);\n-    __ load_heap_oop(R19_method, java_lang_invoke_MemberName::method_offset(), R19_method,\n-                     R22_tmp2, R23_tmp3, MacroAssembler::PRESERVATION_FRAME_LR_GP_FP_REGS, IS_NOT_NULL);\n-    __ ld(R19_method, java_lang_invoke_ResolvedMethodName::vmtarget_offset(), R19_method);\n-    __ std(R19_method, in_bytes(JavaThread::callee_target_offset()), R16_thread); \/\/ just in case callee is deoptimized\n-\n-    __ blr();\n-\n-    return start;\n-  }\n-\n-  \/\/ Initialization\n-  void generate_initial_stubs() {\n-    \/\/ Generates all stubs and initializes the entry points\n-\n-    \/\/ Entry points that exist in all platforms.\n-    \/\/ Note: This is code that could be shared among different platforms - however the\n-    \/\/ benefit seems to be smaller than the disadvantage of having a\n-    \/\/ much more complicated generator structure. See also comment in\n-    \/\/ stubRoutines.hpp.\n-\n-    StubRoutines::_forward_exception_entry          = generate_forward_exception();\n-    StubRoutines::_call_stub_entry                  = generate_call_stub(StubRoutines::_call_stub_return_address);\n-    StubRoutines::_catch_exception_entry            = generate_catch_exception();\n-\n-    if (UnsafeMemoryAccess::_table == nullptr) {\n-      UnsafeMemoryAccess::create_table(8 + 4); \/\/ 8 for copyMemory; 4 for setMemory\n-    }\n-\n-    \/\/ CRC32 Intrinsics.\n-    if (UseCRC32Intrinsics) {\n-      StubRoutines::_crc_table_adr = StubRoutines::ppc::generate_crc_constants(REVERSE_CRC32_POLY);\n-      StubRoutines::_updateBytesCRC32 = generate_CRC32_updateBytes(false);\n-    }\n-\n-    \/\/ CRC32C Intrinsics.\n-    if (UseCRC32CIntrinsics) {\n-      StubRoutines::_crc32c_table_addr = StubRoutines::ppc::generate_crc_constants(REVERSE_CRC32C_POLY);\n-      StubRoutines::_updateBytesCRC32C = generate_CRC32_updateBytes(true);\n-    }\n-\n-    if (VM_Version::supports_float16()) {\n-      \/\/ For results consistency both intrinsics should be enabled.\n-      StubRoutines::_hf2f = generate_float16ToFloat();\n-      StubRoutines::_f2hf = generate_floatToFloat16();\n-    }\n-  }\n-\n-  void generate_continuation_stubs() {\n-    \/\/ Continuation stubs:\n-    StubRoutines::_cont_thaw          = generate_cont_thaw();\n-    StubRoutines::_cont_returnBarrier = generate_cont_returnBarrier();\n-    StubRoutines::_cont_returnBarrierExc = generate_cont_returnBarrier_exception();\n-    StubRoutines::_cont_preempt_stub  = generate_cont_preempt_stub();\n-  }\n-\n-  void generate_final_stubs() {\n-    \/\/ Generates all stubs and initializes the entry points\n-\n-    \/\/ support for verify_oop (must happen after universe_init)\n-    StubRoutines::_verify_oop_subroutine_entry             = generate_verify_oop();\n-\n-    \/\/ nmethod entry barriers for concurrent class unloading\n-    BarrierSetNMethod* bs_nm = BarrierSet::barrier_set()->barrier_set_nmethod();\n-    if (bs_nm != nullptr) {\n-      StubRoutines::_method_entry_barrier            = generate_method_entry_barrier();\n-    }\n-\n-    \/\/ arraycopy stubs used by compilers\n-    generate_arraycopy_stubs();\n-\n-    if (UseSecondarySupersTable) {\n-      StubRoutines::_lookup_secondary_supers_table_slow_path_stub = generate_lookup_secondary_supers_table_slow_path_stub();\n-      if (!InlineSecondarySupersTest) {\n-        for (int slot = 0; slot < Klass::SECONDARY_SUPERS_TABLE_SIZE; slot++) {\n-          StubRoutines::_lookup_secondary_supers_table_stubs[slot]\n-            = generate_lookup_secondary_supers_table_stub(slot);\n-        }\n-      }\n-    }\n-\n-    StubRoutines::_upcall_stub_exception_handler = generate_upcall_stub_exception_handler();\n-    StubRoutines::_upcall_stub_load_target = generate_upcall_stub_load_target();\n-  }\n-\n-  void generate_compiler_stubs() {\n-#if COMPILER2_OR_JVMCI\n-\n-#ifdef COMPILER2\n-    if (UseMultiplyToLenIntrinsic) {\n-      StubRoutines::_multiplyToLen = generate_multiplyToLen();\n-    }\n-    if (UseSquareToLenIntrinsic) {\n-      StubRoutines::_squareToLen = generate_squareToLen();\n-    }\n-    if (UseMulAddIntrinsic) {\n-      StubRoutines::_mulAdd = generate_mulAdd();\n-    }\n-    if (UseMontgomeryMultiplyIntrinsic) {\n-      StubRoutines::_montgomeryMultiply\n-        = CAST_FROM_FN_PTR(address, SharedRuntime::montgomery_multiply);\n-    }\n-    if (UseMontgomerySquareIntrinsic) {\n-      StubRoutines::_montgomerySquare\n-        = CAST_FROM_FN_PTR(address, SharedRuntime::montgomery_square);\n-    }\n-#endif\n-\n-    \/\/ data cache line writeback\n-    if (VM_Version::supports_data_cache_line_flush()) {\n-      StubRoutines::_data_cache_writeback = generate_data_cache_writeback();\n-      StubRoutines::_data_cache_writeback_sync = generate_data_cache_writeback_sync();\n-    }\n-\n-    if (UseAESIntrinsics) {\n-      StubRoutines::_aescrypt_encryptBlock = generate_aescrypt_encryptBlock();\n-      StubRoutines::_aescrypt_decryptBlock = generate_aescrypt_decryptBlock();\n-    }\n-\n-    if (UseSHA256Intrinsics) {\n-      StubRoutines::_sha256_implCompress   = generate_sha256_implCompress(false, \"sha256_implCompress\");\n-      StubRoutines::_sha256_implCompressMB = generate_sha256_implCompress(true,  \"sha256_implCompressMB\");\n-    }\n-    if (UseSHA512Intrinsics) {\n-      StubRoutines::_sha512_implCompress   = generate_sha512_implCompress(false, \"sha512_implCompress\");\n-      StubRoutines::_sha512_implCompressMB = generate_sha512_implCompress(true, \"sha512_implCompressMB\");\n-    }\n-\n-#ifdef VM_LITTLE_ENDIAN\n-    \/\/ Currently supported on PPC64LE only\n-    if (UseBASE64Intrinsics) {\n-      StubRoutines::_base64_decodeBlock = generate_base64_decodeBlock();\n-      StubRoutines::_base64_encodeBlock = generate_base64_encodeBlock();\n-    }\n-#endif\n-#endif \/\/ COMPILER2_OR_JVMCI\n-  }\n-\n- public:\n-  StubGenerator(CodeBuffer* code, StubsKind kind) : StubCodeGenerator(code) {\n-    switch(kind) {\n-    case Initial_stubs:\n-      generate_initial_stubs();\n-      break;\n-     case Continuation_stubs:\n-      generate_continuation_stubs();\n-      break;\n-    case Compiler_stubs:\n-      generate_compiler_stubs();\n-      break;\n-    case Final_stubs:\n-      generate_final_stubs();\n-      break;\n-    default:\n-      fatal(\"unexpected stubs kind: %d\", kind);\n-      break;\n-    };\n-  }\n-};\n-\n-void StubGenerator_generate(CodeBuffer* code, StubCodeGenerator::StubsKind kind) {\n-  StubGenerator g(code, kind);\n-}\n","filename":"src\/hotspot\/cpu\/ppc\/stubGenerator_ppc.cpp","additions":0,"deletions":4821,"binary":false,"changes":4821,"status":"modified"},{"patch":"@@ -2,2 +2,2 @@\n- * Copyright (c) 1997, 2023, Oracle and\/or its affiliates. All rights reserved.\n- * Copyright (c) 2012, 2023 SAP SE. All rights reserved.\n+ * Copyright (c) 1997, 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2012, 2024 SAP SE. All rights reserved.\n@@ -309,3 +309,2 @@\n-  if (UseGHASHIntrinsics) {\n-    warning(\"GHASH intrinsics are not available on this CPU\");\n-    FLAG_SET_DEFAULT(UseGHASHIntrinsics, false);\n+  if (FLAG_IS_DEFAULT(UseGHASHIntrinsics) && VM_Version::has_vsx()) {\n+    FLAG_SET_DEFAULT(UseGHASHIntrinsics, true);\n","filename":"src\/hotspot\/cpu\/ppc\/vm_version_ppc.cpp","additions":4,"deletions":5,"binary":false,"changes":9,"status":"modified"}]}