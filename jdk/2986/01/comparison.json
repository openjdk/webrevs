{"files":[{"patch":"@@ -48,0 +48,1 @@\n+#include \"utilities\/lockFreeQueue.inline.hpp\"\n@@ -49,0 +50,1 @@\n+#include \"utilities\/pair.hpp\"\n@@ -119,109 +121,0 @@\n-#ifdef ASSERT\n-G1DirtyCardQueueSet::Queue::~Queue() {\n-  assert(_head == NULL, \"precondition\");\n-  assert(_tail == NULL, \"precondition\");\n-}\n-#endif \/\/ ASSERT\n-\n-BufferNode* G1DirtyCardQueueSet::Queue::top() const {\n-  return Atomic::load(&_head);\n-}\n-\n-\/\/ An append operation atomically exchanges the new tail with the queue tail.\n-\/\/ It then sets the \"next\" value of the old tail to the head of the list being\n-\/\/ appended; it is an invariant that the old tail's \"next\" value is NULL.\n-\/\/ But if the old tail is NULL then the queue was empty.  In this case the\n-\/\/ head of the list being appended is instead stored in the queue head; it is\n-\/\/ an invariant that the queue head is NULL in this case.\n-\/\/\n-\/\/ This means there is a period between the exchange and the old tail update\n-\/\/ where the queue sequence is split into two parts, the list from the queue\n-\/\/ head to the old tail, and the list being appended.  If there are concurrent\n-\/\/ push\/append operations, each may introduce another such segment.  But they\n-\/\/ all eventually get resolved by their respective updates of their old tail's\n-\/\/ \"next\" value.  This also means that pop operations must handle a buffer\n-\/\/ with a NULL \"next\" value specially.\n-\/\/\n-\/\/ A push operation is just a degenerate append, where the buffer being pushed\n-\/\/ is both the head and the tail of the list being appended.\n-void G1DirtyCardQueueSet::Queue::append(BufferNode& first, BufferNode& last) {\n-  assert(last.next() == NULL, \"precondition\");\n-  BufferNode* old_tail = Atomic::xchg(&_tail, &last);\n-  if (old_tail == NULL) {       \/\/ Was empty.\n-    Atomic::store(&_head, &first);\n-  } else {\n-    assert(old_tail->next() == NULL, \"invariant\");\n-    old_tail->set_next(&first);\n-  }\n-}\n-\n-BufferNode* G1DirtyCardQueueSet::Queue::pop() {\n-  Thread* current_thread = Thread::current();\n-  while (true) {\n-    \/\/ Use a critical section per iteration, rather than over the whole\n-    \/\/ operation.  We're not guaranteed to make progress.  Lingering in one\n-    \/\/ CS could lead to excessive allocation of buffers, because the CS\n-    \/\/ blocks return of released buffers to the free list for reuse.\n-    GlobalCounter::CriticalSection cs(current_thread);\n-\n-    BufferNode* result = Atomic::load_acquire(&_head);\n-    if (result == NULL) return NULL; \/\/ Queue is empty.\n-\n-    BufferNode* next = Atomic::load_acquire(BufferNode::next_ptr(*result));\n-    if (next != NULL) {\n-      \/\/ The \"usual\" lock-free pop from the head of a singly linked list.\n-      if (result == Atomic::cmpxchg(&_head, result, next)) {\n-        \/\/ Former head successfully taken; it is not the last.\n-        assert(Atomic::load(&_tail) != result, \"invariant\");\n-        assert(result->next() != NULL, \"invariant\");\n-        result->set_next(NULL);\n-        return result;\n-      }\n-      \/\/ Lost the race; try again.\n-      continue;\n-    }\n-\n-    \/\/ next is NULL.  This case is handled differently from the \"usual\"\n-    \/\/ lock-free pop from the head of a singly linked list.\n-\n-    \/\/ If _tail == result then result is the only element in the list. We can\n-    \/\/ remove it from the list by first setting _tail to NULL and then setting\n-    \/\/ _head to NULL, the order being important.  We set _tail with cmpxchg in\n-    \/\/ case of a concurrent push\/append\/pop also changing _tail.  If we win\n-    \/\/ then we've claimed result.\n-    if (Atomic::cmpxchg(&_tail, result, (BufferNode*)NULL) == result) {\n-      assert(result->next() == NULL, \"invariant\");\n-      \/\/ Now that we've claimed result, also set _head to NULL.  But we must\n-      \/\/ be careful of a concurrent push\/append after we NULLed _tail, since\n-      \/\/ it may have already performed its list-was-empty update of _head,\n-      \/\/ which we must not overwrite.\n-      Atomic::cmpxchg(&_head, result, (BufferNode*)NULL);\n-      return result;\n-    }\n-\n-    \/\/ If _head != result then we lost the race to take result; try again.\n-    if (result != Atomic::load_acquire(&_head)) {\n-      continue;\n-    }\n-\n-    \/\/ An in-progress concurrent operation interfered with taking the head\n-    \/\/ element when it was the only element.  A concurrent pop may have won\n-    \/\/ the race to clear the tail but not yet cleared the head. Alternatively,\n-    \/\/ a concurrent push\/append may have changed the tail but not yet linked\n-    \/\/ result->next().  We cannot take result in either case.  We don't just\n-    \/\/ try again, because we could spin for a long time waiting for that\n-    \/\/ concurrent operation to finish.  In the first case, returning NULL is\n-    \/\/ fine; we lost the race for the only element to another thread.  We\n-    \/\/ also return NULL for the second case, and let the caller cope.\n-    return NULL;\n-  }\n-}\n-\n-G1DirtyCardQueueSet::HeadTail G1DirtyCardQueueSet::Queue::take_all() {\n-  assert_at_safepoint();\n-  HeadTail result(Atomic::load(&_head), Atomic::load(&_tail));\n-  Atomic::store(&_head, (BufferNode*)NULL);\n-  Atomic::store(&_tail, (BufferNode*)NULL);\n-  return result;\n-}\n-\n@@ -428,1 +321,1 @@\n-  HeadTail buffers = _completed.take_all();\n+  Pair<BufferNode*, BufferNode*> pair = _completed.take_all();\n@@ -431,1 +324,1 @@\n-  return G1BufferNodeList(buffers._head, buffers._tail, num_cards);\n+  return G1BufferNodeList(pair.first, pair.second, num_cards);\n","filename":"src\/hotspot\/share\/gc\/g1\/g1DirtyCardQueue.cpp","additions":4,"deletions":111,"binary":false,"changes":115,"status":"modified"},{"patch":"@@ -35,0 +35,1 @@\n+#include \"utilities\/lockFreeQueue.hpp\"\n@@ -79,11 +80,1 @@\n-  \/\/ A lock-free FIFO of BufferNodes, linked through their next() fields.\n-  \/\/ This class has a restriction that pop() may return NULL when there are\n-  \/\/ buffers in the queue if there is a concurrent push\/append operation.\n-  class Queue {\n-    BufferNode* volatile _head;\n-    DEFINE_PAD_MINUS_SIZE(1, DEFAULT_CACHE_LINE_SIZE, sizeof(BufferNode*));\n-    BufferNode* volatile _tail;\n-    DEFINE_PAD_MINUS_SIZE(2, DEFAULT_CACHE_LINE_SIZE, sizeof(BufferNode*));\n-\n-    NONCOPYABLE(Queue);\n-\n+  class CompletedQueue: public LockFreeQueue<BufferNode, &BufferNode::next_ptr> {\n@@ -91,23 +82,6 @@\n-    Queue() : _head(NULL), _tail(NULL) {}\n-    DEBUG_ONLY(~Queue();)\n-\n-    \/\/ Return the first buffer in the queue.\n-    \/\/ Thread-safe, but the result may change immediately.\n-    BufferNode* top() const;\n-\n-    \/\/ Thread-safe add the buffer to the end of the queue.\n-    void push(BufferNode& node) { append(node, node); }\n-\n-    \/\/ Thread-safe add the buffers from first to last to the end of the queue.\n-    void append(BufferNode& first, BufferNode& last);\n-\n-    \/\/ Thread-safe attempt to remove and return the first buffer in the queue.\n-    \/\/ Returns NULL if the queue is empty, or if a concurrent push\/append\n-    \/\/ interferes.  Uses GlobalCounter critical sections to address the ABA\n-    \/\/ problem; this works with the buffer allocator's use of GlobalCounter\n-    \/\/ synchronization.\n-    BufferNode* pop();\n-\n-    \/\/ Take all the buffers from the queue, leaving the queue empty.\n-    \/\/ Not thread-safe.\n-    HeadTail take_all();\n+    BufferNode* pop() {\n+      \/\/ Use GlobalCounter critical section to avoid ABA problem.\n+      \/\/ The release of a BufferNode to its allocator's free list uses\n+      \/\/ GlobalCounter::write_synchronize() to coordinate with the pop().\n+      return LockFreeQueue<BufferNode, &BufferNode::next_ptr>::pop<true \/*use_rcu*\/>();\n+    }\n@@ -203,1 +177,1 @@\n-  Queue _completed;           \/\/ Has inner padding, including trailer.\n+  CompletedQueue _completed;  \/\/ Has inner padding, including trailer.\n","filename":"src\/hotspot\/share\/gc\/g1\/g1DirtyCardQueue.hpp","additions":9,"deletions":35,"binary":false,"changes":44,"status":"modified"},{"patch":"@@ -88,0 +88,3 @@\n+\n+  \/\/ Similar to CriticalSection, but only enabled if \"enable\" is true.\n+  template<bool enable> class ConditionalCriticalSection;\n","filename":"src\/hotspot\/share\/utilities\/globalCounter.hpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -56,1 +56,0 @@\n- private:\n@@ -59,1 +58,1 @@\n- public:\n+public:\n@@ -70,0 +69,17 @@\n+template<bool enable> class GlobalCounter::ConditionalCriticalSection {\n+  Thread* _thread;\n+  CSContext _context;\n+public:\n+  inline ConditionalCriticalSection(Thread* thread) {\n+    if (enable) {\n+      _thread = thread;\n+      _context = GlobalCounter::critical_section_begin(thread);\n+    }\n+  }\n+  inline ~ConditionalCriticalSection() {\n+    if (enable) {\n+      GlobalCounter::critical_section_end(_thread, _context);\n+    }\n+  }\n+};\n+\n","filename":"src\/hotspot\/share\/utilities\/globalCounter.inline.hpp","additions":18,"deletions":2,"binary":false,"changes":20,"status":"modified"},{"patch":"@@ -0,0 +1,100 @@\n+\/*\n+ * Copyright (c) 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef SHARE_UTILITIES_LOCKFREEQUEUE_HPP\n+#define SHARE_UTILITIES_LOCKFREEQUEUE_HPP\n+\n+#include \"memory\/padded.hpp\"\n+#include \"utilities\/globalDefinitions.hpp\"\n+#include \"utilities\/pair.hpp\"\n+\n+\/\/ The LockFreeQueue template provides a lock-free FIFO. Its structure\n+\/\/ and usage is similar to LockFreeStack. It has inner paddings, and\n+\/\/ optionally use GlobalCounter critical section in pop() to address\n+\/\/ the ABA problem. This class has a restriction that pop() may return\n+\/\/ NULL when there are objects in the queue if there is a concurrent\n+\/\/ push\/append operation.\n+\/\/\n+\/\/ \\tparam T is the class of the elements in the queue.\n+\/\/\n+\/\/ \\tparam next_ptr is a function pointer.  Applying this function to\n+\/\/ an object of type T must return a pointer to the list entry member\n+\/\/ of the object associated with the LockFreeQueue type.\n+template<typename T, T* volatile* (*next_ptr)(T&)>\n+class LockFreeQueue {\n+  T* volatile _head;\n+  DEFINE_PAD_MINUS_SIZE(1, DEFAULT_CACHE_LINE_SIZE, sizeof(T*));\n+  T* volatile _tail;\n+  DEFINE_PAD_MINUS_SIZE(2, DEFAULT_CACHE_LINE_SIZE, sizeof(T*));\n+\n+  NONCOPYABLE(LockFreeQueue);\n+\n+  \/\/ Return the entry following node in the list used by the\n+  \/\/ specialized LockFreeQueue class.\n+  static T* next(const T& node);\n+\n+  \/\/ Set the entry following node to new_next in the list used by the\n+  \/\/ specialized LockFreeQueue class. Not thread-safe, as it cannot\n+  \/\/ concurrently run with push or pop operations that modify this\n+  \/\/ node.\n+  static void set_next(T& node, T* new_next);\n+\n+public:\n+  LockFreeQueue();\n+  DEBUG_ONLY(~LockFreeQueue();)\n+\n+  \/\/ Return the first object in the queue.\n+  \/\/ Thread-safe, but the result may change immediately.\n+  T* top() const;\n+\n+  \/\/ Return true if the queue is empty.\n+  bool empty() const { return top() == NULL; }\n+\n+  \/\/ Return the number of objects in the queue.\n+  \/\/ Not thread-safe. There must be no concurrent modification\n+  \/\/ while the length is being determined.\n+  size_t length() const;\n+\n+  \/\/ Thread-safe add the object to the end of the queue.\n+  void push(T& node) { append(node, node); }\n+\n+  \/\/ Thread-safe add the objects from first to last to the end of the queue.\n+  void append(T& first, T& last);\n+\n+  \/\/ Thread-safe attempt to remove and return the first object in the queue.\n+  \/\/ Returns NULL if the queue is empty, or if a concurrent push\/append\n+  \/\/ interferes.\n+  \/\/ If use_rcu is true, it applies GlobalCounter critical sections to\n+  \/\/ address the ABA problem. This requires the object's\n+  \/\/ allocator use GlobalCounter synchronization to defer reusing object.\n+  template<bool use_rcu> T* pop();\n+\n+  \/\/ Take all the objects from the queue, leaving the queue empty.\n+  \/\/ Not thread-safe. It should only be used when there is no concurrent\n+  \/\/ push\/append\/pop operation.\n+  \/\/ Returns a pair of <head, tail> pointers to the current queue.\n+  Pair<T*, T*> take_all();\n+};\n+\n+#endif \/\/ SHARE_UTILITIES_LOCKFREEQUEUE_HPP\n","filename":"src\/hotspot\/share\/utilities\/lockFreeQueue.hpp","additions":100,"deletions":0,"binary":false,"changes":100,"status":"added"},{"patch":"@@ -0,0 +1,177 @@\n+\/*\n+ * Copyright (c) 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef SHARE_UTILITIES_LOCKFREEQUEUE_INLINE_HPP\n+#define SHARE_UTILITIES_LOCKFREEQUEUE_INLINE_HPP\n+\n+#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/thread.inline.hpp\"\n+#include \"utilities\/globalCounter.inline.hpp\"\n+#include \"utilities\/lockFreeQueue.hpp\"\n+#include \"logging\/log.hpp\"\n+\n+template<typename T, T* volatile* (*next_ptr)(T&)>\n+T* LockFreeQueue<T, next_ptr>::next(const T& node) {\n+  return Atomic::load(next_ptr(const_cast<T&>(node)));\n+}\n+\n+template<typename T, T* volatile* (*next_ptr)(T&)>\n+void LockFreeQueue<T, next_ptr>::set_next(T& node, T* new_next) {\n+    Atomic::store(next_ptr(node), new_next);\n+}\n+\n+template<typename T, T* volatile* (*next_ptr)(T&)>\n+LockFreeQueue<T, next_ptr>::LockFreeQueue() : _head(NULL), _tail(NULL) {}\n+\n+#ifdef ASSERT\n+template<typename T, T* volatile* (*next_ptr)(T&)>\n+LockFreeQueue<T, next_ptr>::~LockFreeQueue() {\n+  assert(_head == NULL, \"precondition\");\n+  assert(_tail == NULL, \"precondition\");\n+}\n+#endif\n+\n+template<typename T, T* volatile* (*next_ptr)(T&)>\n+T* LockFreeQueue<T, next_ptr>::top() const {\n+  return Atomic::load(&_head);\n+}\n+\n+template<typename T, T* volatile* (*next_ptr)(T&)>\n+size_t LockFreeQueue<T, next_ptr>::length() const {\n+  size_t result = 0;\n+  for (const T* current = top(); current != NULL; current = next(*current)) {\n+    ++result;\n+  }\n+  return result;\n+}\n+\n+\/\/ An append operation atomically exchanges the new tail with the queue tail.\n+\/\/ It then sets the \"next\" value of the old tail to the head of the list being\n+\/\/ appended; it is an invariant that the old tail's \"next\" value is NULL.\n+\/\/ But if the old tail is NULL then the queue was empty.  In this case the\n+\/\/ head of the list being appended is instead stored in the queue head; it is\n+\/\/ an invariant that the queue head is NULL in this case.\n+\/\/\n+\/\/ This means there is a period between the exchange and the old tail update\n+\/\/ where the queue sequence is split into two parts, the list from the queue\n+\/\/ head to the old tail, and the list being appended.  If there are concurrent\n+\/\/ push\/append operations, each may introduce another such segment.  But they\n+\/\/ all eventually get resolved by their respective updates of their old tail's\n+\/\/ \"next\" value.  This also means that pop operations must handle an object\n+\/\/ with a NULL \"next\" value specially.\n+\/\/\n+\/\/ A push operation is just a degenerate append, where the object being pushed\n+\/\/ is both the head and the tail of the list being appended.\n+template<typename T, T* volatile* (*next_ptr)(T&)>\n+void LockFreeQueue<T, next_ptr>::append(T& first, T& last) {\n+  assert(next(last) == NULL, \"precondition\");\n+  T* old_tail = Atomic::xchg(&_tail, &last);\n+  if (old_tail == NULL) {       \/\/ Was empty.\n+    Atomic::store(&_head, &first);\n+  } else {\n+    assert(next(*old_tail) == NULL, \"invariant\");\n+    set_next(*old_tail, &first);\n+  }\n+}\n+\n+template<typename T, T* volatile* (*next_ptr)(T&)>\n+template<bool use_rcu>\n+T* LockFreeQueue<T, next_ptr>::pop() {\n+  while (true) {\n+    \/\/ Use a critical section per iteration, rather than over the whole\n+    \/\/ operation. We're not guaranteed to make progress. Lingering in one\n+    \/\/ CS could defer the write-side operation of RCU synchronization\n+    \/\/ too long, leading to unwanted effect. E.g., if the write-side\n+    \/\/ returns released objects to a free list for reuse, it could cause\n+    \/\/ excessive allocations.\n+    GlobalCounter::ConditionalCriticalSection<use_rcu> cs(use_rcu ?\n+                                                          Thread::current():\n+                                                          NULL);\n+\n+    \/\/ We only need memory_order_consume. Upgrade it to \"load_acquire\"\n+    \/\/ as the memory_order_consume API is not ready for use yet.\n+    T* result = Atomic::load_acquire(&_head);\n+    if (result == NULL) return NULL; \/\/ Queue is empty.\n+\n+    \/\/ This relaxed load is always followed by a cmpxchg(), thus it\n+    \/\/ is OK as the reader-side of the release-acquire ordering.\n+    T* next_node = Atomic::load(next_ptr(*result));\n+    if (next_node != NULL) {\n+      \/\/ The \"usual\" lock-free pop from the head of a singly linked list.\n+      if (result == Atomic::cmpxchg(&_head, result, next_node)) {\n+        \/\/ Former head successfully taken; it is not the last.\n+        assert(Atomic::load(&_tail) != result, \"invariant\");\n+        assert(next(*result) != NULL, \"invariant\");\n+        set_next(*result, NULL);\n+        return result;\n+      }\n+      \/\/ Lost the race; try again.\n+      continue;\n+    }\n+\n+    \/\/ next is NULL.  This case is handled differently from the \"usual\"\n+    \/\/ lock-free pop from the head of a singly linked list.\n+\n+    \/\/ If _tail == result then result is the only element in the list. We can\n+    \/\/ remove it from the list by first setting _tail to NULL and then setting\n+    \/\/ _head to NULL, the order being important.  We set _tail with cmpxchg in\n+    \/\/ case of a concurrent push\/append\/pop also changing _tail.  If we win\n+    \/\/ then we've claimed result.\n+    if (Atomic::cmpxchg(&_tail, result, (T*)NULL) == result) {\n+      assert(next(*result) == NULL, \"invariant\");\n+      \/\/ Now that we've claimed result, also set _head to NULL.  But we must\n+      \/\/ be careful of a concurrent push\/append after we NULLed _tail, since\n+      \/\/ it may have already performed its list-was-empty update of _head,\n+      \/\/ which we must not overwrite.\n+      Atomic::cmpxchg(&_head, result, (T*)NULL);\n+      return result;\n+    }\n+\n+    \/\/ If _head != result then we lost the race to take result; try again.\n+    if (result != Atomic::load_acquire(&_head)) {\n+      continue;\n+    }\n+\n+    \/\/ An in-progress concurrent operation interfered with taking the head\n+    \/\/ element when it was the only element.  A concurrent pop may have won\n+    \/\/ the race to clear the tail but not yet cleared the head. Alternatively,\n+    \/\/ a concurrent push\/append may have changed the tail but not yet linked\n+    \/\/ result->next().  We cannot take result in either case.  We don't just\n+    \/\/ try again, because we could spin for a long time waiting for that\n+    \/\/ concurrent operation to finish.  In the first case, returning NULL is\n+    \/\/ fine; we lost the race for the only element to another thread.  We\n+    \/\/ also return NULL for the second case, and let the caller cope.\n+    return NULL;\n+  }\n+}\n+\n+template<typename T, T* volatile* (*next_ptr)(T&)>\n+Pair<T*, T*> LockFreeQueue<T, next_ptr>::take_all() {\n+  Pair<T*, T*> result(Atomic::load(&_head), Atomic::load(&_tail));\n+  Atomic::store(&_head, (T*)NULL);\n+  Atomic::store(&_tail, (T*)NULL);\n+  return result;\n+}\n+\n+#endif \/\/ SHARE_UTILITIES_LOCKFREEQUEUE_INLINE_HPP\n","filename":"src\/hotspot\/share\/utilities\/lockFreeQueue.inline.hpp","additions":177,"deletions":0,"binary":false,"changes":177,"status":"added"},{"patch":"@@ -0,0 +1,310 @@\n+\/*\n+ * Copyright (c) 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"memory\/allocation.inline.hpp\"\n+#include \"runtime\/atomic.hpp\"\n+#include \"utilities\/globalDefinitions.hpp\"\n+#include \"utilities\/lockFreeQueue.inline.hpp\"\n+#include \"utilities\/pair.hpp\"\n+#include \"threadHelper.inline.hpp\"\n+#include \"unittest.hpp\"\n+#include <new>\n+\n+class LockFreeQueueTestElement {\n+  typedef LockFreeQueueTestElement Element;\n+\n+  Element* volatile _entry;\n+  Element* volatile _entry1;\n+  size_t _id;\n+\n+  static Element* volatile* entry_ptr(Element& e) { return &e._entry; }\n+  static Element* volatile* entry1_ptr(Element& e) { return &e._entry1; }\n+\n+public:\n+  class TestQueue: public LockFreeQueue<Element, &entry_ptr> {\n+  public:\n+    Element* pop() {\n+      return LockFreeQueue<Element, &entry_ptr>::pop<false>();\n+    }\n+    Element* pop_rcu() {\n+      return LockFreeQueue<Element, &entry_ptr>::pop<true>();\n+    }\n+  };\n+  class TestQueue1: public LockFreeQueue<Element, &entry1_ptr> {\n+  public:\n+    Element* pop() {\n+      return LockFreeQueue<Element, &entry1_ptr>::pop<false>();\n+    }\n+  };\n+\n+  LockFreeQueueTestElement(size_t id = 0) : _entry(), _entry1(), _id(id) {}\n+  size_t id() const { return _id; }\n+  void set_id(size_t value) { _id = value; }\n+  Element* next() { return _entry; }\n+  Element* next1() { return _entry1; }\n+};\n+\n+typedef LockFreeQueueTestElement Element;\n+typedef Element::TestQueue TestQueue;\n+typedef Element::TestQueue1 TestQueue1;\n+\n+static void initialize(Element* elements, size_t size, TestQueue* queue) {\n+  for (size_t i = 0; i < size; ++i) {\n+    elements[i].set_id(i);\n+  }\n+  ASSERT_TRUE(queue->empty());\n+  ASSERT_EQ(0u, queue->length());\n+  ASSERT_TRUE(queue->pop() == NULL);\n+  ASSERT_TRUE(queue->top() == NULL);\n+\n+  for (size_t id = 0; id < size; ++id) {\n+    ASSERT_EQ(id, queue->length());\n+    Element* e = &elements[id];\n+    ASSERT_EQ(id, e->id());\n+    queue->push(*e);\n+    ASSERT_FALSE(queue->empty());\n+    \/\/ top() is always the oldest element.\n+    ASSERT_EQ(&elements[0], queue->top());\n+  }\n+}\n+\n+class LockFreeQueueTestBasics : public ::testing::Test {\n+public:\n+  LockFreeQueueTestBasics();\n+\n+  static const size_t nelements = 10;\n+  Element elements[nelements];\n+  TestQueue queue;\n+};\n+\n+const size_t LockFreeQueueTestBasics::nelements;\n+\n+LockFreeQueueTestBasics::LockFreeQueueTestBasics() : queue() {\n+  initialize(elements, nelements, &queue);\n+}\n+\n+TEST_F(LockFreeQueueTestBasics, pop) {\n+  for (size_t i = 0; i < nelements; ++i) {\n+    ASSERT_FALSE(queue.empty());\n+    ASSERT_EQ(nelements - i, queue.length());\n+    Element* e = queue.pop();\n+    ASSERT_TRUE(e != NULL);\n+    ASSERT_EQ(&elements[i], e);\n+    ASSERT_EQ(i, e->id());\n+  }\n+  ASSERT_TRUE(queue.empty());\n+  ASSERT_EQ(0u, queue.length());\n+  ASSERT_TRUE(queue.pop() == NULL);\n+}\n+\n+TEST_VM(LockFreeQueueTestPopRCU, pop_rcu) {\n+  \/\/ We have to run this test in a JVM, so that Thread::current() can work.\n+  const size_t nelements = 10;\n+  Element elements[nelements];\n+  TestQueue queue;\n+  initialize(elements, nelements, &queue);\n+  for (size_t i = 0; i < nelements; ++i) {\n+    ASSERT_FALSE(queue.empty());\n+    ASSERT_EQ(nelements - i, queue.length());\n+    Element* e = queue.pop_rcu();\n+    ASSERT_TRUE(e != NULL);\n+    ASSERT_EQ(&elements[i], e);\n+    ASSERT_EQ(i, e->id());\n+  }\n+  ASSERT_TRUE(queue.empty());\n+  ASSERT_EQ(0u, queue.length());\n+  ASSERT_TRUE(queue.pop() == NULL);\n+}\n+\n+TEST_F(LockFreeQueueTestBasics, append) {\n+  TestQueue other_queue;\n+  ASSERT_TRUE(other_queue.empty());\n+  ASSERT_EQ(0u, other_queue.length());\n+  ASSERT_TRUE(other_queue.top() == NULL);\n+  ASSERT_TRUE(other_queue.pop() == NULL);\n+\n+  Pair<Element*, Element*> pair = queue.take_all();\n+  other_queue.append(*pair.first, *pair.second);\n+  ASSERT_EQ(nelements, other_queue.length());\n+  ASSERT_TRUE(queue.empty());\n+  ASSERT_EQ(0u, queue.length());\n+  ASSERT_TRUE(queue.pop() == NULL);\n+  ASSERT_TRUE(queue.top() == NULL);\n+\n+  for (size_t i = 0; i < nelements; ++i) {\n+    ASSERT_EQ(nelements - i, other_queue.length());\n+    Element* e = other_queue.pop();\n+    ASSERT_TRUE(e != NULL);\n+    ASSERT_EQ(&elements[i], e);\n+    ASSERT_EQ(i, e->id());\n+  }\n+  ASSERT_EQ(0u, other_queue.length());\n+  ASSERT_TRUE(other_queue.pop() == NULL);\n+}\n+\n+TEST_F(LockFreeQueueTestBasics, two_queues) {\n+  TestQueue1 queue1;\n+  ASSERT_TRUE(queue1.pop() == NULL);\n+\n+  for (size_t id = 0; id < nelements; ++id) {\n+    queue1.push(elements[id]);\n+  }\n+  ASSERT_EQ(nelements, queue1.length());\n+  Element* e0 = queue.top();\n+  Element* e1 = queue1.top();\n+  while (true) {\n+    ASSERT_EQ(e0, e1);\n+    if (e0 == NULL) break;\n+    e0 = e0->next();\n+    e1 = e1->next1();\n+  }\n+\n+  for (size_t i = 0; i < nelements; ++i) {\n+    ASSERT_EQ(nelements - i, queue.length());\n+    ASSERT_EQ(nelements - i, queue1.length());\n+\n+    Element* e = queue.pop();\n+    ASSERT_TRUE(e != NULL);\n+    ASSERT_EQ(&elements[i], e);\n+    ASSERT_EQ(i, e->id());\n+\n+    Element* e1 = queue1.pop();\n+    ASSERT_TRUE(e1 != NULL);\n+    ASSERT_EQ(&elements[i], e1);\n+    ASSERT_EQ(i, e1->id());\n+\n+    ASSERT_EQ(e, e1);\n+  }\n+  ASSERT_EQ(0u, queue.length());\n+  ASSERT_EQ(0u, queue1.length());\n+  ASSERT_TRUE(queue.pop() == NULL);\n+  ASSERT_TRUE(queue1.pop() == NULL);\n+}\n+\n+class LockFreeQueueTestThread : public JavaTestThread {\n+  uint _id;\n+  TestQueue* _from;\n+  TestQueue* _to;\n+  volatile size_t* _processed;\n+  size_t _process_limit;\n+  size_t _local_processed;\n+  volatile bool _ready;\n+\n+public:\n+  LockFreeQueueTestThread(Semaphore* post,\n+                          uint id,\n+                          TestQueue* from,\n+                          TestQueue* to,\n+                          volatile size_t* processed,\n+                          size_t process_limit) :\n+    JavaTestThread(post),\n+    _id(id),\n+    _from(from),\n+    _to(to),\n+    _processed(processed),\n+    _process_limit(process_limit),\n+    _local_processed(0),\n+    _ready(false)\n+  {}\n+\n+  virtual void main_run() {\n+    Atomic::release_store_fence(&_ready, true);\n+    while (true) {\n+      Element* e = _from->pop();\n+      if (e != NULL) {\n+        _to->push(*e);\n+        Atomic::inc(_processed);\n+        ++_local_processed;\n+      } else if (Atomic::load_acquire(_processed) == _process_limit) {\n+        tty->print_cr(\"thread %u processed \" SIZE_FORMAT, _id, _local_processed);\n+        return;\n+      }\n+    }\n+  }\n+\n+  bool ready() const { return Atomic::load_acquire(&_ready); }\n+};\n+\n+TEST_VM(LockFreeQueueTest, stress) {\n+  Semaphore post;\n+  TestQueue initial_queue;\n+  TestQueue start_queue;\n+  TestQueue middle_queue;\n+  TestQueue final_queue;\n+  volatile size_t stage1_processed = 0;\n+  volatile size_t stage2_processed = 0;\n+\n+  const size_t nelements = 10000;\n+  Element* elements = NEW_C_HEAP_ARRAY(Element, nelements, mtOther);\n+  for (size_t id = 0; id < nelements; ++id) {\n+    ::new (&elements[id]) Element(id);\n+    initial_queue.push(elements[id]);\n+  }\n+  ASSERT_EQ(nelements, initial_queue.length());\n+\n+  \/\/ - stage1 threads pop from start_queue and push to middle_queue.\n+  \/\/ - stage2 threads pop from middle_queue and push to final_queue.\n+  \/\/ - all threads in a stage count the number of elements processed in\n+  \/\/   their corresponding stageN_processed counter.\n+\n+  const uint stage1_threads = 2;\n+  const uint stage2_threads = 2;\n+  const uint nthreads = stage1_threads + stage2_threads;\n+  LockFreeQueueTestThread* threads[nthreads] = {};\n+\n+  for (uint i = 0; i < ARRAY_SIZE(threads); ++i) {\n+    TestQueue* from = &start_queue;\n+    TestQueue* to = &middle_queue;\n+    volatile size_t* processed = &stage1_processed;\n+    if (i >= stage1_threads) {\n+      from = &middle_queue;\n+      to = &final_queue;\n+      processed = &stage2_processed;\n+    }\n+    threads[i] =\n+      new LockFreeQueueTestThread(&post, i, from, to, processed, nelements);\n+    threads[i]->doit();\n+    while (!threads[i]->ready()) {} \/\/ Wait until ready to start test.\n+  }\n+\n+  \/\/ Transfer elements to start_queue to start test.\n+  Pair<Element*, Element*> pair = initial_queue.take_all();\n+  start_queue.append(*pair.first, *pair.second);\n+\n+  \/\/ Wait for all threads to complete.\n+  for (uint i = 0; i < nthreads; ++i) {\n+    post.wait();\n+  }\n+\n+  \/\/ Verify expected state.\n+  ASSERT_EQ(nelements, stage1_processed);\n+  ASSERT_EQ(nelements, stage2_processed);\n+  ASSERT_EQ(0u, initial_queue.length());\n+  ASSERT_EQ(0u, start_queue.length());\n+  ASSERT_EQ(0u, middle_queue.length());\n+  ASSERT_EQ(nelements, final_queue.length());\n+  while (final_queue.pop() != NULL) {}\n+\n+  FREE_C_HEAP_ARRAY(Element, elements);\n+}\n","filename":"test\/hotspot\/gtest\/utilities\/test_lockFreeQueue.cpp","additions":310,"deletions":0,"binary":false,"changes":310,"status":"added"}]}