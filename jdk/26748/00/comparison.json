{"files":[{"patch":"@@ -4,0 +4,1 @@\n+ * Copyright 2025 Arm Limited and\/or its affiliates.\n@@ -911,2 +912,12 @@\n-\n-void LIR_Assembler::mem2reg(LIR_Opr src, LIR_Opr dest, BasicType type, LIR_PatchCode patch_code, CodeEmitInfo* info, bool wide) {\n+\/\/ Specialised mem2reg function which is used for volatile loads since 8365147\n+\/\/ Uses LDAR to ensure memory ordering.\n+void LIR_Assembler::mem2reg_volatile(LIR_Opr src, LIR_Opr dest, BasicType type,\n+                                     LIR_PatchCode patch_code, CodeEmitInfo* info) {\n+  if (is_floating_point_type(type)) {\n+    \/\/ Use LDAR instead of DMB+LD+DMB, except for floats\/doubles (no LDAR equivalent).\n+    if (!CompilerConfig::is_c1_only_no_jvmci()) {\n+      __ membar(__ AnyAny);\n+    }\n+    mem2reg(src, dest, type, patch_code, info, false);\n+    return;\n+  }\n@@ -924,0 +935,54 @@\n+  if (info != nullptr) {\n+    add_debug_info_for_null_check_here(info);\n+  }\n+  int null_check_here = code_offset();\n+\n+  __ lea(rscratch1, as_Address(from_addr));\n+  switch (type) {\n+    case T_BOOLEAN:\n+      __ ldarb(dest->as_register(), rscratch1);\n+      break;\n+    case T_BYTE: \/\/ LDAR is unsigned so need to sign-extend for byte\n+      __ ldarb(dest->as_register(), rscratch1);\n+      __ sxtb(dest->as_register(), dest->as_register());\n+      break;\n+    case T_CHAR:\n+      __ ldarh(dest->as_register(), rscratch1);\n+      break;\n+    case T_SHORT: \/\/ LDAR is unsigned so need to sign-extend for short\n+      __ ldarh(dest->as_register(), rscratch1);\n+      __ sxth(dest->as_register(), dest->as_register());\n+      break;\n+    case T_INT:\n+      __ ldarw(dest->as_register(), rscratch1);\n+      break;\n+    case T_ADDRESS:\n+      __ ldar(dest->as_register(), rscratch1);\n+      break;\n+    case T_LONG:\n+      __ ldar(dest->as_register_lo(), rscratch1);\n+      break;\n+    case T_ARRAY:\n+    case T_OBJECT:\n+      if (UseCompressedOops) {\n+        __ ldarw(dest->as_register(), rscratch1);\n+      } else {\n+        __ ldar(dest->as_register(), rscratch1);\n+      }\n+      break;\n+    case T_METADATA:\n+      \/\/ We get here to store a method pointer to the stack to pass to\n+      \/\/ a dtrace runtime call. This can't work on 64 bit with\n+      \/\/ compressed klass ptrs: T_METADATA can be a compressed klass\n+      \/\/ ptr or a 64 bit method pointer.\n+    default:\n+      ShouldNotReachHere();\n+  }\n+\n+  if (is_reference_type(type)) {\n+    if (UseCompressedOops) {\n+      __ decode_heap_oop(dest->as_register());\n+    }\n+    __ verify_oop(dest->as_register());\n+  }\n+}\n@@ -925,0 +990,10 @@\n+void LIR_Assembler::mem2reg(LIR_Opr src, LIR_Opr dest, BasicType type, LIR_PatchCode patch_code, CodeEmitInfo* info, bool wide) {\n+  LIR_Address* addr = src->as_address_ptr();\n+  LIR_Address* from_addr = src->as_address_ptr();\n+  if (addr->base()->type() == T_OBJECT) {\n+    __ verify_oop(addr->base()->as_pointer_register());\n+  }\n+  if (patch_code != lir_patch_none) {\n+    deoptimize_trap(info);\n+    return;\n+  }\n@@ -2848,1 +2923,3 @@\n-  if (dest->is_address() || src->is_address()) {\n+  if (src->is_address()) {\n+    mem2reg_volatile(src, dest, type, lir_patch_none, info);\n+  } else if (dest->is_address()) {\n","filename":"src\/hotspot\/cpu\/aarch64\/c1_LIRAssembler_aarch64.cpp","additions":80,"deletions":3,"binary":false,"changes":83,"status":"modified"},{"patch":"@@ -4,0 +4,1 @@\n+ * Copyright 2025 Arm Limited and\/or its affiliates.\n@@ -1403,1 +1404,1 @@\n-void LIRGenerator::volatile_field_load(LIR_Address* address, LIR_Opr result,\n+bool LIRGenerator::volatile_field_load(LIR_Address* address, LIR_Opr result,\n@@ -1405,8 +1406,11 @@\n-  \/\/ 8179954: We need to make sure that the code generated for\n-  \/\/ volatile accesses forms a sequentially-consistent set of\n-  \/\/ operations when combined with STLR and LDAR.  Without a leading\n-  \/\/ membar it's possible for a simple Dekker test to fail if loads\n-  \/\/ use LD;DMB but stores use STLR.  This can happen if C2 compiles\n-  \/\/ the stores in one method and C1 compiles the loads in another.\n-  if (!CompilerConfig::is_c1_only_no_jvmci()) {\n-    __ membar();\n+\n+  \/\/ AArch64 uses LDAR for volatile field loads by default. However, if all accesses are forced to\n+  \/\/ be atomic - which includes unaligned ones - use the generic DMB + LD sequence, as LDAR might\n+  \/\/ fault for unaligned accesses.\n+  if (AlwaysAtomicAccesses) {\n+    if (!CompilerConfig::is_c1_only_no_jvmci()) {\n+      __ membar();\n+    }\n+    __ load(address, result, info);\n+  } else {\n+    __ volatile_load_mem_reg(address, result, info);\n@@ -1414,1 +1418,1 @@\n-  __ volatile_load_mem_reg(address, result, info);\n+  return AlwaysAtomicAccesses || is_floating_point_type(address->type());\n","filename":"src\/hotspot\/cpu\/aarch64\/c1_LIRGenerator_aarch64.cpp","additions":14,"deletions":10,"binary":false,"changes":24,"status":"modified"},{"patch":"@@ -1321,1 +1321,1 @@\n-void LIRGenerator::volatile_field_load(LIR_Address* address, LIR_Opr result,\n+bool LIRGenerator::volatile_field_load(LIR_Address* address, LIR_Opr result,\n@@ -1335,1 +1335,2 @@\n-    return;\n+  } else {\n+    __ load(address, result, info, lir_patch_none);\n@@ -1337,1 +1338,1 @@\n-  __ load(address, result, info, lir_patch_none);\n+  return true;\n","filename":"src\/hotspot\/cpu\/arm\/c1_LIRGenerator_arm.cpp","additions":4,"deletions":3,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -1138,1 +1138,1 @@\n-void LIRGenerator::volatile_field_load(LIR_Address* address, LIR_Opr result,\n+bool LIRGenerator::volatile_field_load(LIR_Address* address, LIR_Opr result,\n@@ -1146,0 +1146,1 @@\n+  return true;\n","filename":"src\/hotspot\/cpu\/ppc\/c1_LIRGenerator_ppc.cpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -1173,1 +1173,1 @@\n-void LIRGenerator::volatile_field_load(LIR_Address* address, LIR_Opr result,\n+bool LIRGenerator::volatile_field_load(LIR_Address* address, LIR_Opr result,\n@@ -1176,0 +1176,1 @@\n+  return true;\n","filename":"src\/hotspot\/cpu\/riscv\/c1_LIRGenerator_riscv.cpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -1046,1 +1046,1 @@\n-void LIRGenerator::volatile_field_load(LIR_Address* address, LIR_Opr result,\n+bool LIRGenerator::volatile_field_load(LIR_Address* address, LIR_Opr result,\n@@ -1049,0 +1049,1 @@\n+  return true;\n","filename":"src\/hotspot\/cpu\/s390\/c1_LIRGenerator_s390.cpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -1417,1 +1417,1 @@\n-void LIRGenerator::volatile_field_load(LIR_Address* address, LIR_Opr result,\n+bool LIRGenerator::volatile_field_load(LIR_Address* address, LIR_Opr result,\n@@ -1433,0 +1433,1 @@\n+  return true;\n","filename":"src\/hotspot\/cpu\/x86\/c1_LIRGenerator_x86.cpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -3,0 +3,1 @@\n+ * Copyright 2025 Arm Limited and\/or its affiliates.\n@@ -179,0 +180,2 @@\n+  void mem2reg_volatile(LIR_Opr src, LIR_Opr dest, BasicType type,\n+                        LIR_PatchCode patch_code, CodeEmitInfo* info);\n","filename":"src\/hotspot\/share\/c1\/c1_LIRAssembler.hpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -333,1 +333,1 @@\n-  \/\/ it always known as well.\n+  \/\/ is always known as well.\n@@ -335,1 +335,3 @@\n-  void volatile_field_load(LIR_Address* address, LIR_Opr result, CodeEmitInfo* info);\n+\n+  \/\/ returns false if it provides trailing membar semantics. Else returns true.\n+  bool volatile_field_load(LIR_Address* address, LIR_Opr result, CodeEmitInfo* info);\n","filename":"src\/hotspot\/share\/c1\/c1_LIRGenerator.hpp","additions":4,"deletions":2,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -3,0 +3,1 @@\n+ * Copyright 2025 Arm Limited and\/or its affiliates.\n@@ -176,0 +177,1 @@\n+  bool needs_trailing_dmb = is_volatile;\n@@ -185,1 +187,4 @@\n-    gen->volatile_field_load(access.resolved_addr()->as_address_ptr(), result, access.access_emit_info());\n+    \/\/ volatile_field_load returns false if it itself provides trailing membar semantics.\n+    \/\/  Hence trailing DMB is no longer needed.\n+    needs_trailing_dmb &= gen->volatile_field_load(access.resolved_addr()->as_address_ptr(),\n+                                                   result, access.access_emit_info());\n@@ -190,1 +195,1 @@\n-  if (is_volatile) {\n+  if (needs_trailing_dmb) {\n","filename":"src\/hotspot\/share\/gc\/shared\/c1\/barrierSetC1.cpp","additions":7,"deletions":2,"binary":false,"changes":9,"status":"modified"}]}