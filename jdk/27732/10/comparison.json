{"files":[{"patch":"@@ -151,1 +151,0 @@\n-  # Only G1 supports dumping the shared heap, so explicitly use G1 if the JVM supports it.\n","filename":"make\/Images.gmk","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -44,4 +44,0 @@\n-  do_stub(initial, get_previous_sp)                                     \\\n-  do_arch_entry(x86, initial, get_previous_sp,                          \\\n-                get_previous_sp_entry,                                  \\\n-                get_previous_sp_entry)                                  \\\n","filename":"src\/hotspot\/cpu\/x86\/stubDeclarations_x86.hpp","additions":0,"deletions":4,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -544,16 +544,0 @@\n-\/\/ Support for intptr_t get_previous_sp()\n-\/\/\n-\/\/ This routine is used to find the previous stack pointer for the\n-\/\/ caller.\n-address StubGenerator::generate_get_previous_sp() {\n-  StubId stub_id = StubId::stubgen_get_previous_sp_id;\n-  StubCodeMark mark(this, stub_id);\n-  address start = __ pc();\n-\n-  __ movptr(rax, rsp);\n-  __ addptr(rax, 8); \/\/ return address is at the top of the stack.\n-  __ ret(0);\n-\n-  return start;\n-}\n-\n@@ -4086,2 +4070,0 @@\n-  StubRoutines::x86::_get_previous_sp_entry = generate_get_previous_sp();\n-\n","filename":"src\/hotspot\/cpu\/x86\/stubGenerator_x86_64.cpp","additions":0,"deletions":18,"binary":false,"changes":18,"status":"modified"},{"patch":"@@ -71,6 +71,0 @@\n-  \/\/ Support for intptr_t get_previous_sp()\n-  \/\/\n-  \/\/ This routine is used to find the previous stack pointer for the\n-  \/\/ caller.\n-  address generate_get_previous_sp();\n-\n","filename":"src\/hotspot\/cpu\/x86\/stubGenerator_x86_64.hpp","additions":0,"deletions":6,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -54,0 +54,2 @@\n+#include <intrin.h>\n+\n@@ -250,0 +252,1 @@\n+__declspec(noinline)\n@@ -251,4 +254,1 @@\n-  typedef address get_sp_func();\n-  get_sp_func* func = CAST_TO_FN_PTR(get_sp_func*,\n-                                     StubRoutines::x86::get_previous_sp_entry());\n-  return (*func)();\n+  return ((address)_AddressOfReturnAddress()) + sizeof(void*);\n","filename":"src\/hotspot\/os_cpu\/windows_x86\/os_windows_x86.cpp","additions":4,"deletions":4,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -26,1 +26,4 @@\n-#include \"cds\/archiveHeapWriter.hpp\"\n+#include \"cds\/aotMappedHeapLoader.hpp\"\n+#include \"cds\/aotMappedHeapWriter.hpp\"\n+#include \"cds\/aotStreamedHeapLoader.hpp\"\n+#include \"cds\/aotStreamedHeapWriter.hpp\"\n@@ -48,2 +51,0 @@\n-size_t AOTMapLogger::_num_root_segments;\n-size_t AOTMapLogger::_num_obj_arrays_logged;\n@@ -51,1 +52,0 @@\n-ArchiveHeapInfo* AOTMapLogger::_dumptime_heap_info;\n@@ -89,1 +89,1 @@\n-                                ArchiveHeapInfo* heap_info,\n+                                ArchiveMappedHeapInfo* mapped_heap_info, ArchiveStreamedHeapInfo* streamed_heap_info,\n@@ -93,2 +93,0 @@\n-  _num_root_segments = mapinfo->heap_root_segments().count();\n-  _dumptime_heap_info = heap_info;\n@@ -109,2 +107,5 @@\n-  if (heap_info->is_used()) {\n-    dumptime_log_heap_region(heap_info);\n+  if (mapped_heap_info != nullptr && mapped_heap_info->is_used()) {\n+    dumptime_log_mapped_heap_region(mapped_heap_info);\n+  }\n+  if (streamed_heap_info != nullptr && streamed_heap_info->is_used()) {\n+    dumptime_log_streamed_heap_region(streamed_heap_info);\n@@ -195,1 +196,0 @@\n-    _num_root_segments = mapinfo->heap_root_segments().count();\n@@ -504,0 +504,3 @@\n+\/\/\n+\/\/ Each AOT heap reader and writer has its own oop_iterator() API that retrieves all the data required to build\n+\/\/ fake oops for logging.\n@@ -505,5 +508,2 @@\n-  static int _requested_shift;\n-  static intx _buffer_to_requested_delta;\n-  static address _buffer_start;\n-  static address _buffer_end;\n-  static uint64_t _buffer_start_narrow_oop; \/\/ The encoded narrow oop for the objects at _buffer_start\n+  OopDataIterator* _iter;\n+  OopData _data;\n@@ -511,4 +511,2 @@\n-  address _buffer_addr;\n-\n-  static void assert_range(address buffer_addr) {\n-    assert(_buffer_start <= buffer_addr && buffer_addr < _buffer_end, \"range check\");\n+  address* buffered_field_addr(int field_offset) {\n+    return (address*)(buffered_addr() + field_offset);\n@@ -517,2 +515,3 @@\n-  address* field_addr(int field_offset) {\n-    return (address*)(_buffer_addr + field_offset);\n+public:\n+  RequestedMetadataAddr metadata_field(int field_offset) {\n+    return RequestedMetadataAddr(*(address*)(buffered_field_addr(field_offset)));\n@@ -521,3 +520,2 @@\n-protected:\n-  RequestedMetadataAddr metadata_field(int field_offset) {\n-    return RequestedMetadataAddr(*(address*)(field_addr(field_offset)));\n+  address buffered_addr() {\n+    return _data._buffered_addr;\n@@ -529,18 +527,1 @@\n-  oop raw_oop() { return cast_to_oop(_buffer_addr); }\n-\n-public:\n-  static void init_globals(address requested_base, address requested_start, int requested_shift,\n-                           address buffer_start, address buffer_end) {\n-    _requested_shift = requested_shift;\n-    _buffer_to_requested_delta = requested_start - buffer_start;\n-    _buffer_start = buffer_start;\n-    _buffer_end = buffer_end;\n-\n-    precond(requested_start >= requested_base);\n-    if (UseCompressedOops) {\n-      _buffer_start_narrow_oop = (uint64_t)(pointer_delta(requested_start, requested_base, 1)) >> _requested_shift;\n-      assert(_buffer_start_narrow_oop < 0xffffffff, \"sanity\");\n-    } else {\n-      _buffer_start_narrow_oop = 0xdeadbeed;\n-    }\n-  }\n+  oop raw_oop() { return _data._raw_oop; }\n@@ -548,1 +529,2 @@\n-  FakeOop() : _buffer_addr(nullptr) {}\n+  FakeOop() : _data() {}\n+  FakeOop(OopDataIterator* iter, OopData data) : _iter(iter), _data(data) {}\n@@ -550,10 +532,4 @@\n-  FakeOop(address buffer_addr) : _buffer_addr(buffer_addr) {\n-    if (_buffer_addr != nullptr) {\n-      assert_range(_buffer_addr);\n-    }\n-  }\n-\n-  FakeMirror& as_mirror();\n-  FakeObjArray& as_obj_array();\n-  FakeString& as_string();\n-  FakeTypeArray& as_type_array();\n+  FakeMirror as_mirror();\n+  FakeObjArray as_obj_array();\n+  FakeString as_string();\n+  FakeTypeArray as_type_array();\n@@ -573,5 +549,1 @@\n-    if (_is_runtime_logging) {\n-      return raw_oop()->klass();\n-    } else {\n-      return ArchiveHeapWriter::real_klass_of_buffered_oop(_buffer_addr);\n-    }\n+    return _data._klass;\n@@ -582,5 +554,5 @@\n-    if (_is_runtime_logging) {\n-      return raw_oop()->size_given_klass(real_klass());\n-    } else {\n-      return ArchiveHeapWriter::size_of_buffered_oop(_buffer_addr);\n-    }\n+    return _data._size;\n+  }\n+\n+  bool is_root_segment() {\n+    return _data._is_root_segment;\n@@ -590,1 +562,1 @@\n-  bool is_null() { return _buffer_addr == nullptr; }\n+  bool is_null() { return buffered_addr() == nullptr; }\n@@ -597,0 +569,4 @@\n+  intptr_t target_location() {\n+    return _data._target_location;\n+  }\n+\n@@ -598,1 +574,1 @@\n-    return _buffer_addr + _buffer_to_requested_delta;\n+    return _data._requested_addr;\n@@ -603,5 +579,1 @@\n-    if (_buffer_addr == nullptr) {\n-      return 0;\n-    }\n-    uint64_t pd = (uint64_t)(pointer_delta(_buffer_addr, _buffer_start, 1));\n-    return checked_cast<uint32_t>(_buffer_start_narrow_oop + (pd >> _requested_shift));\n+    return _data._narrow_location;\n@@ -611,8 +583,1 @@\n-    uint64_t n = (uint64_t)(*addr);\n-    if (n == 0) {\n-      return FakeOop(nullptr);\n-    } else {\n-      precond(n >= _buffer_start_narrow_oop);\n-      address value = _buffer_start + ((n - _buffer_start_narrow_oop) << _requested_shift);\n-      return FakeOop(value);\n-    }\n+    return FakeOop(_iter, _iter->obj_at(addr));\n@@ -622,6 +587,1 @@\n-    address requested_value = cast_from_oop<address>(*addr);\n-    if (requested_value == nullptr) {\n-      return FakeOop(nullptr);\n-    } else {\n-      return FakeOop(requested_value - _buffer_to_requested_delta);\n-    }\n+    return FakeOop(_iter, _iter->obj_at(addr));\n@@ -647,0 +607,2 @@\n+  FakeMirror(OopDataIterator* iter, OopData data) : FakeOop(iter, data) {}\n+\n@@ -665,0 +627,2 @@\n+  FakeObjArray(OopDataIterator* iter, OopData data) : FakeOop(iter, data) {}\n+\n@@ -679,0 +643,2 @@\n+  FakeString(OopDataIterator* iter, OopData data) : FakeOop(iter, data) {}\n+\n@@ -697,0 +663,2 @@\n+  FakeTypeArray(OopDataIterator* iter, OopData data) : FakeOop(iter, data) {}\n+\n@@ -706,1 +674,1 @@\n-AOTMapLogger::FakeMirror& AOTMapLogger::FakeOop::as_mirror() {\n+AOTMapLogger::FakeMirror AOTMapLogger::FakeOop::as_mirror() {\n@@ -708,1 +676,1 @@\n-  return (FakeMirror&)*this;\n+  return FakeMirror(_iter, _data);\n@@ -711,1 +679,1 @@\n-AOTMapLogger::FakeObjArray& AOTMapLogger::FakeOop::as_obj_array() {\n+AOTMapLogger::FakeObjArray AOTMapLogger::FakeOop::as_obj_array() {\n@@ -713,1 +681,1 @@\n-  return (FakeObjArray&)*this;\n+  return FakeObjArray(_iter, _data);\n@@ -716,1 +684,1 @@\n-AOTMapLogger::FakeTypeArray& AOTMapLogger::FakeOop::as_type_array() {\n+AOTMapLogger::FakeTypeArray AOTMapLogger::FakeOop::as_type_array() {\n@@ -718,1 +686,1 @@\n-  return (FakeTypeArray&)*this;\n+  return FakeTypeArray(_iter, _data);\n@@ -721,1 +689,1 @@\n-AOTMapLogger::FakeString& AOTMapLogger::FakeOop::as_string() {\n+AOTMapLogger::FakeString AOTMapLogger::FakeOop::as_string() {\n@@ -723,1 +691,1 @@\n-  return (FakeString&)*this;\n+  return FakeString(_iter, _data);\n@@ -826,7 +794,1 @@\n-int AOTMapLogger::FakeOop::_requested_shift;\n-intx AOTMapLogger::FakeOop::_buffer_to_requested_delta;\n-address AOTMapLogger::FakeOop::_buffer_start;\n-address AOTMapLogger::FakeOop::_buffer_end;\n-uint64_t AOTMapLogger::FakeOop::_buffer_start_narrow_oop;\n-\n-void AOTMapLogger::dumptime_log_heap_region(ArchiveHeapInfo* heap_info) {\n+void AOTMapLogger::dumptime_log_mapped_heap_region(ArchiveMappedHeapInfo* heap_info) {\n@@ -837,5 +799,2 @@\n-  address requested_base = UseCompressedOops ? (address)CompressedOops::base() : (address)ArchiveHeapWriter::NOCOOPS_REQUESTED_BASE;\n-  address requested_start = UseCompressedOops ? ArchiveHeapWriter::buffered_addr_to_requested_addr(buffer_start) : requested_base;\n-  int requested_shift =  CompressedOops::shift();\n-\n-  FakeOop::init_globals(requested_base, requested_start, requested_shift, buffer_start, buffer_end);\n+  address requested_base = UseCompressedOops ? (address)CompressedOops::base() : (address)AOTMappedHeapWriter::NOCOOPS_REQUESTED_BASE;\n+  address requested_start = UseCompressedOops ? AOTMappedHeapWriter::buffered_addr_to_requested_addr(buffer_start) : requested_base;\n@@ -844,1 +803,10 @@\n-  log_oops(buffer_start, buffer_end);\n+  log_archived_objects(AOTMappedHeapWriter::oop_iterator(heap_info));\n+}\n+\n+void AOTMapLogger::dumptime_log_streamed_heap_region(ArchiveStreamedHeapInfo* heap_info) {\n+  MemRegion r = heap_info->buffer_region();\n+  address buffer_start = address(r.start()); \/\/ start of the current oop inside the buffer\n+  address buffer_end = address(r.end());\n+\n+  log_region_range(\"heap\", buffer_start, buffer_end, nullptr);\n+  log_archived_objects(AOTStreamedHeapWriter::oop_iterator(heap_info));\n@@ -849,0 +817,1 @@\n+\n@@ -851,15 +820,1 @@\n-  size_t alignment = ObjectAlignmentInBytes;\n-\n-  \/\/ Allocate a buffer and read the image of the archived heap region. This buffer is outside\n-  \/\/ of the real Java heap, so we must use FakeOop to access the contents of the archived heap objects.\n-  char* buffer = resource_allocate_bytes(r->used() + alignment);\n-  address buffer_start = (address)align_up(buffer, alignment);\n-  address buffer_end = buffer_start + r->used();\n-  if (!mapinfo->read_region(heap_region_index, (char*)buffer_start, r->used(), \/* do_commit = *\/ false)) {\n-    log_error(aot)(\"Cannot read heap region; AOT map logging of heap objects failed\");\n-    return;\n-  }\n-\n-  address requested_base = UseCompressedOops ? (address)mapinfo->narrow_oop_base() : mapinfo->heap_region_requested_address();\n-  address requested_start = requested_base + r->mapping_offset();\n-  int requested_shift = mapinfo->narrow_oop_shift();\n+  size_t alignment = (size_t)ObjectAlignmentInBytes;\n@@ -867,1 +822,15 @@\n-  FakeOop::init_globals(requested_base, requested_start, requested_shift, buffer_start, buffer_end);\n+  if (mapinfo->object_streaming_mode()) {\n+    address buffer_start = (address)r->mapped_base();\n+    address buffer_end = buffer_start + r->used();\n+    log_region_range(\"heap\", buffer_start, buffer_end, nullptr);\n+    log_archived_objects(AOTStreamedHeapLoader::oop_iterator(mapinfo, buffer_start, buffer_end));\n+  } else {\n+    \/\/ Allocate a buffer and read the image of the archived heap region. This buffer is outside\n+    \/\/ of the real Java heap, so we must use FakeOop to access the contents of the archived heap objects.\n+    char* buffer = resource_allocate_bytes(r->used() + alignment);\n+    address buffer_start = (address)align_up(buffer, alignment);\n+    address buffer_end = buffer_start + r->used();\n+    if (!mapinfo->read_region(heap_region_index, (char*)buffer_start, r->used(), \/* do_commit = *\/ false)) {\n+      log_error(aot)(\"Cannot read heap region; AOT map logging of heap objects failed\");\n+      return;\n+    }\n@@ -869,2 +838,5 @@\n-  log_region_range(\"heap\", buffer_start, buffer_end, requested_start);\n-  log_oops(buffer_start, buffer_end);\n+    address requested_base = UseCompressedOops ? (address)mapinfo->narrow_oop_base() : AOTMappedHeapLoader::heap_region_requested_address(mapinfo);\n+    address requested_start = requested_base + r->mapping_offset();\n+    log_region_range(\"heap\", buffer_start, buffer_end, requested_start);\n+    log_archived_objects(AOTMappedHeapLoader::oop_iterator(mapinfo, buffer_start, buffer_end));\n+  }\n@@ -873,1 +845,1 @@\n-void AOTMapLogger::log_oops(address buffer_start, address buffer_end) {\n+void AOTMapLogger::log_archived_objects(OopDataIterator* iter) {\n@@ -880,1 +852,0 @@\n-  _num_obj_arrays_logged = 0;\n@@ -882,4 +853,14 @@\n-  for (address fop = buffer_start; fop < buffer_end; ) {\n-    FakeOop fake_oop(fop);\n-    st.print(PTR_FORMAT \": @@ Object \", p2i(fake_oop.requested_addr()));\n-    print_oop_info_cr(&st, fake_oop, \/*print_requested_addr=*\/false);\n+  \/\/ Roots that are not segmented\n+  GrowableArrayCHeap<OopData, mtClass>* normal_roots = iter->roots();\n+  for (int i = 0; i < normal_roots->length(); ++i) {\n+    OopData data = normal_roots->at(i);\n+    FakeOop fop(iter, data);\n+    _roots->append(fop);\n+    st.print(\" root[%4d]: \", i);\n+    print_oop_info_cr(&st, fop);\n+  }\n+\n+  while (iter->has_next()) {\n+    FakeOop fake_oop(iter, iter->next());\n+    st.print(PTR_FORMAT \": @@ Object \", fake_oop.target_location());\n+    print_oop_info_cr(&st, fake_oop, \/*print_location=*\/false);\n@@ -892,4 +873,3 @@\n-    address next_fop = fop + fake_oop.size() * BytesPerWord;\n-    log_as_hex(fop, next_fop, fake_oop.requested_addr(), \/*is_heap=*\/true);\n-\n-    fop = next_fop;\n+    address fop = fake_oop.buffered_addr();\n+    address end_fop = fop + fake_oop.size() * BytesPerWord;\n+    log_as_hex(fop, end_fop, fake_oop.requested_addr(), \/*is_heap=*\/true);\n@@ -899,0 +879,2 @@\n+  delete iter;\n+  delete normal_roots;\n@@ -901,1 +883,1 @@\n-void AOTMapLogger::print_oop_info_cr(outputStream* st, FakeOop fake_oop, bool print_requested_addr) {\n+void AOTMapLogger::print_oop_info_cr(outputStream* st, FakeOop fake_oop, bool print_location) {\n@@ -907,3 +889,3 @@\n-    address requested_addr = fake_oop.requested_addr();\n-    if (print_requested_addr) {\n-      st->print(PTR_FORMAT \" \", p2i(requested_addr));\n+    intptr_t target_location = fake_oop.target_location();\n+    if (print_location) {\n+      st->print(PTR_FORMAT \" \", target_location);\n@@ -922,1 +904,2 @@\n-        fake_oop.as_string().print_on(st);\n+        FakeString fake_str = fake_oop.as_string();\n+        fake_str.print_on(st);\n@@ -945,1 +928,1 @@\n-    bool is_logging_root_segment = _num_obj_arrays_logged < _num_root_segments;\n+    bool is_logging_root_segment = fake_oop.is_root_segment();\n@@ -957,1 +940,0 @@\n-    _num_obj_arrays_logged ++;\n","filename":"src\/hotspot\/share\/cds\/aotMapLogger.cpp","additions":122,"deletions":140,"binary":false,"changes":262,"status":"modified"},{"patch":"@@ -35,1 +35,2 @@\n-class ArchiveHeapInfo;\n+class ArchiveMappedHeapInfo;\n+class ArchiveStreamedHeapInfo;\n@@ -67,0 +68,1 @@\n+public:\n@@ -74,0 +76,36 @@\n+#if INCLUDE_CDS_JAVA_HEAP\n+  struct OopData {\n+    address _buffered_addr;\n+    address _requested_addr;\n+    intptr_t _target_location;\n+    uint32_t _narrow_location;\n+    oopDesc* _raw_oop;\n+    Klass* _klass;\n+    size_t _size;\n+    bool _is_root_segment;\n+  };\n+\n+  class OopDataIterator : public CHeapObj<mtClassShared> {\n+  protected:\n+    OopData null_data() {\n+      return { nullptr,\n+               nullptr,\n+               0,\n+               0,\n+               nullptr,\n+               nullptr,\n+               0,\n+               false };\n+    }\n+\n+  public:\n+    virtual bool has_next() = 0;\n+    virtual OopData next() = 0;\n+    virtual OopData obj_at(narrowOop* p) = 0;\n+    virtual OopData obj_at(oop* p) = 0;\n+    virtual GrowableArrayCHeap<OopData, mtClass>* roots() = 0;\n+    virtual ~OopDataIterator() {}\n+  };\n+#endif\n+\n+private:\n@@ -79,2 +117,0 @@\n-  static size_t _num_root_segments;\n-  static size_t _num_obj_arrays_logged;\n@@ -82,1 +118,0 @@\n-  static ArchiveHeapInfo* _dumptime_heap_info;\n@@ -117,1 +152,2 @@\n-  static void dumptime_log_heap_region(ArchiveHeapInfo* heap_info);\n+  static void dumptime_log_mapped_heap_region(ArchiveMappedHeapInfo* mapped_heap_info);\n+  static void dumptime_log_streamed_heap_region(ArchiveStreamedHeapInfo* streamed_heap_info);\n@@ -120,1 +156,1 @@\n-  static void print_oop_info_cr(outputStream* st, FakeOop fake_oop, bool print_requested_addr = true);\n+  static void print_oop_info_cr(outputStream* st, FakeOop fake_oop, bool print_location = true);\n@@ -122,1 +158,2 @@\n-  static void log_oops(address buf_start, address buf_end);\n+  static void log_mapped_oops(address buf_start, address buf_end);\n+  static void log_archived_objects(OopDataIterator* iter);\n@@ -131,1 +168,1 @@\n-                           ArchiveHeapInfo* heap_info,\n+                           ArchiveMappedHeapInfo* mapped_heap_info, ArchiveStreamedHeapInfo* streamed_heap_info,\n","filename":"src\/hotspot\/share\/cds\/aotMapLogger.hpp","additions":45,"deletions":8,"binary":false,"changes":53,"status":"modified"},{"patch":"@@ -0,0 +1,847 @@\n+\/*\n+ * Copyright (c) 2018, 2025, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#include \"cds\/aotLogging.hpp\"\n+#include \"cds\/aotMappedHeapLoader.inline.hpp\"\n+#include \"cds\/aotMappedHeapWriter.hpp\"\n+#include \"cds\/aotMetaspace.hpp\"\n+#include \"cds\/cdsConfig.hpp\"\n+#include \"cds\/heapShared.inline.hpp\"\n+#include \"classfile\/classLoaderDataShared.hpp\"\n+#include \"classfile\/stringTable.hpp\"\n+#include \"classfile\/systemDictionaryShared.hpp\"\n+#include \"gc\/shared\/collectedHeap.hpp\"\n+#include \"logging\/log.hpp\"\n+#include \"logging\/logMessage.hpp\"\n+#include \"logging\/logStream.hpp\"\n+#include \"logging\/logTag.hpp\"\n+#include \"memory\/allocation.inline.hpp\"\n+#include \"memory\/iterator.inline.hpp\"\n+#include \"memory\/resourceArea.hpp\"\n+#include \"memory\/universe.hpp\"\n+#include \"sanitizers\/ub.hpp\"\n+#include \"utilities\/bitMap.inline.hpp\"\n+#include \"utilities\/copy.hpp\"\n+#if INCLUDE_G1GC\n+#include \"gc\/g1\/g1CollectedHeap.hpp\"\n+#include \"gc\/g1\/g1HeapRegion.hpp\"\n+#endif\n+\n+#if INCLUDE_CDS_JAVA_HEAP\n+\n+bool AOTMappedHeapLoader::_is_mapped = false;\n+bool AOTMappedHeapLoader::_is_loaded = false;\n+\n+bool    AOTMappedHeapLoader::_narrow_oop_base_initialized = false;\n+address AOTMappedHeapLoader::_narrow_oop_base;\n+int     AOTMappedHeapLoader::_narrow_oop_shift;\n+\n+\/\/ Support for loaded heap.\n+uintptr_t AOTMappedHeapLoader::_loaded_heap_bottom = 0;\n+uintptr_t AOTMappedHeapLoader::_loaded_heap_top = 0;\n+uintptr_t AOTMappedHeapLoader::_dumptime_base = UINTPTR_MAX;\n+uintptr_t AOTMappedHeapLoader::_dumptime_top = 0;\n+intx AOTMappedHeapLoader::_runtime_offset = 0;\n+bool AOTMappedHeapLoader::_loading_failed = false;\n+\n+\/\/ Support for mapped heap.\n+uintptr_t AOTMappedHeapLoader::_mapped_heap_bottom = 0;\n+bool      AOTMappedHeapLoader::_mapped_heap_relocation_initialized = false;\n+ptrdiff_t AOTMappedHeapLoader::_mapped_heap_delta = 0;\n+\n+\/\/ Heap roots\n+GrowableArrayCHeap<OopHandle, mtClassShared>* AOTMappedHeapLoader::_root_segments = nullptr;\n+int AOTMappedHeapLoader::_root_segment_max_size_elems;\n+\n+MemRegion AOTMappedHeapLoader::_mapped_heap_memregion;\n+bool AOTMappedHeapLoader::_heap_pointers_need_patching;\n+\n+\/\/ Every mapped region is offset by _mapped_heap_delta from its requested address.\n+\/\/ See FileMapInfo::heap_region_requested_address().\n+ATTRIBUTE_NO_UBSAN\n+void AOTMappedHeapLoader::init_mapped_heap_info(address mapped_heap_bottom, ptrdiff_t delta, int dumptime_oop_shift) {\n+  assert(!_mapped_heap_relocation_initialized, \"only once\");\n+  if (!UseCompressedOops) {\n+    assert(dumptime_oop_shift == 0, \"sanity\");\n+  }\n+  assert(can_map(), \"sanity\");\n+  init_narrow_oop_decoding(CompressedOops::base() + delta, dumptime_oop_shift);\n+  _mapped_heap_bottom = (intptr_t)mapped_heap_bottom;\n+  _mapped_heap_delta = delta;\n+  _mapped_heap_relocation_initialized = true;\n+}\n+\n+void AOTMappedHeapLoader::init_narrow_oop_decoding(address base, int shift) {\n+  assert(!_narrow_oop_base_initialized, \"only once\");\n+  _narrow_oop_base_initialized = true;\n+  _narrow_oop_base = base;\n+  _narrow_oop_shift = shift;\n+}\n+\n+void AOTMappedHeapLoader::fixup_region() {\n+  FileMapInfo* mapinfo = FileMapInfo::current_info();\n+  if (is_mapped()) {\n+    fixup_mapped_heap_region(mapinfo);\n+  } else if (_loading_failed) {\n+    fill_failed_loaded_heap();\n+  }\n+}\n+\n+\/\/ ------------------ Support for Region MAPPING -----------------------------------------\n+\n+\/\/ Patch all the embedded oop pointers inside an archived heap region,\n+\/\/ to be consistent with the runtime oop encoding.\n+class PatchCompressedEmbeddedPointers: public BitMapClosure {\n+  narrowOop* _start;\n+\n+ public:\n+  PatchCompressedEmbeddedPointers(narrowOop* start) : _start(start) {}\n+\n+  bool do_bit(size_t offset) {\n+    narrowOop* p = _start + offset;\n+    narrowOop v = *p;\n+    assert(!CompressedOops::is_null(v), \"null oops should have been filtered out at dump time\");\n+    oop o = AOTMappedHeapLoader::decode_from_mapped_archive(v);\n+    RawAccess<IS_NOT_NULL>::oop_store(p, o);\n+    return true;\n+  }\n+};\n+\n+class PatchCompressedEmbeddedPointersQuick: public BitMapClosure {\n+  narrowOop* _start;\n+  uint32_t _delta;\n+\n+ public:\n+  PatchCompressedEmbeddedPointersQuick(narrowOop* start, uint32_t delta) : _start(start), _delta(delta) {}\n+\n+  bool do_bit(size_t offset) {\n+    narrowOop* p = _start + offset;\n+    narrowOop v = *p;\n+    assert(!CompressedOops::is_null(v), \"null oops should have been filtered out at dump time\");\n+    narrowOop new_v = CompressedOops::narrow_oop_cast(CompressedOops::narrow_oop_value(v) + _delta);\n+    assert(!CompressedOops::is_null(new_v), \"should never relocate to narrowOop(0)\");\n+#ifdef ASSERT\n+    oop o1 = AOTMappedHeapLoader::decode_from_mapped_archive(v);\n+    oop o2 = CompressedOops::decode_not_null(new_v);\n+    assert(o1 == o2, \"quick delta must work\");\n+#endif\n+    RawAccess<IS_NOT_NULL>::oop_store(p, new_v);\n+    return true;\n+  }\n+};\n+\n+class PatchUncompressedEmbeddedPointers: public BitMapClosure {\n+  oop* _start;\n+  intptr_t _delta;\n+\n+ public:\n+  PatchUncompressedEmbeddedPointers(oop* start, intx runtime_offset) :\n+    _start(start),\n+    _delta(runtime_offset) {}\n+\n+  PatchUncompressedEmbeddedPointers(oop* start) :\n+    _start(start),\n+    _delta(AOTMappedHeapLoader::mapped_heap_delta()) {}\n+\n+  bool do_bit(size_t offset) {\n+    oop* p = _start + offset;\n+    intptr_t dumptime_oop = (intptr_t)((void*)*p);\n+    assert(dumptime_oop != 0, \"null oops should have been filtered out at dump time\");\n+    intptr_t runtime_oop = dumptime_oop + _delta;\n+    RawAccess<IS_NOT_NULL>::oop_store(p, cast_to_oop(runtime_oop));\n+    return true;\n+  }\n+};\n+\n+void AOTMappedHeapLoader::patch_compressed_embedded_pointers(BitMapView bm,\n+                                                             FileMapInfo* info,\n+                                                             MemRegion region) {\n+  narrowOop dt_encoded_bottom = encoded_heap_region_dumptime_address(info);\n+  narrowOop rt_encoded_bottom = CompressedOops::encode_not_null(cast_to_oop(region.start()));\n+  log_info(aot)(\"patching heap embedded pointers: narrowOop 0x%8x -> 0x%8x\",\n+                  (uint)dt_encoded_bottom, (uint)rt_encoded_bottom);\n+\n+  \/\/ Optimization: if dumptime shift is the same as runtime shift, we can perform a\n+  \/\/ quick conversion from \"dumptime narrowOop\" -> \"runtime narrowOop\".\n+  narrowOop* patching_start = (narrowOop*)region.start() + FileMapInfo::current_info()->mapped_heap()->oopmap_start_pos();\n+  if (_narrow_oop_shift == CompressedOops::shift()) {\n+    uint32_t quick_delta = (uint32_t)rt_encoded_bottom - (uint32_t)dt_encoded_bottom;\n+    log_info(aot)(\"heap data relocation quick delta = 0x%x\", quick_delta);\n+    if (quick_delta == 0) {\n+      log_info(aot)(\"heap data relocation unnecessary, quick_delta = 0\");\n+    } else {\n+      PatchCompressedEmbeddedPointersQuick patcher(patching_start, quick_delta);\n+      bm.iterate(&patcher);\n+    }\n+  } else {\n+    log_info(aot)(\"heap data quick relocation not possible\");\n+    PatchCompressedEmbeddedPointers patcher(patching_start);\n+    bm.iterate(&patcher);\n+  }\n+}\n+\n+\/\/ Patch all the non-null pointers that are embedded in the archived heap objects\n+\/\/ in this (mapped) region\n+void AOTMappedHeapLoader::patch_embedded_pointers(FileMapInfo* info,\n+                                                  MemRegion region, address oopmap,\n+                                                  size_t oopmap_size_in_bits) {\n+  BitMapView bm((BitMap::bm_word_t*)oopmap, oopmap_size_in_bits);\n+  if (UseCompressedOops) {\n+    patch_compressed_embedded_pointers(bm, info, region);\n+  } else {\n+    PatchUncompressedEmbeddedPointers patcher((oop*)region.start() + FileMapInfo::current_info()->mapped_heap()->oopmap_start_pos());\n+    bm.iterate(&patcher);\n+  }\n+}\n+\n+\/\/ ------------------ Support for Region LOADING -----------------------------------------\n+\n+\/\/ The CDS archive remembers each heap object by its address at dump time, but\n+\/\/ the heap object may be loaded at a different address at run time. This structure is used\n+\/\/ to translate the dump time addresses for all objects in FileMapInfo::space_at(region_index)\n+\/\/ to their runtime addresses.\n+struct LoadedArchiveHeapRegion {\n+  int       _region_index;   \/\/ index for FileMapInfo::space_at(index)\n+  size_t    _region_size;    \/\/ number of bytes in this region\n+  uintptr_t _dumptime_base;  \/\/ The dump-time (decoded) address of the first object in this region\n+  intx      _runtime_offset; \/\/ If an object's dump time address P is within in this region, its\n+                             \/\/ runtime address is P + _runtime_offset\n+  uintptr_t top() {\n+    return _dumptime_base + _region_size;\n+  }\n+};\n+\n+void AOTMappedHeapLoader::init_loaded_heap_relocation(LoadedArchiveHeapRegion* loaded_region) {\n+  _dumptime_base = loaded_region->_dumptime_base;\n+  _dumptime_top = loaded_region->top();\n+  _runtime_offset = loaded_region->_runtime_offset;\n+}\n+\n+bool AOTMappedHeapLoader::can_load() {\n+  return Universe::heap()->can_load_archived_objects();\n+}\n+\n+class AOTMappedHeapLoader::PatchLoadedRegionPointers: public BitMapClosure {\n+  narrowOop* _start;\n+  intx _offset;\n+  uintptr_t _base;\n+  uintptr_t _top;\n+\n+ public:\n+  PatchLoadedRegionPointers(narrowOop* start, LoadedArchiveHeapRegion* loaded_region)\n+    : _start(start),\n+      _offset(loaded_region->_runtime_offset),\n+      _base(loaded_region->_dumptime_base),\n+      _top(loaded_region->top()) {}\n+\n+  bool do_bit(size_t offset) {\n+    assert(UseCompressedOops, \"PatchLoadedRegionPointers for uncompressed oops is unimplemented\");\n+    narrowOop* p = _start + offset;\n+    narrowOop v = *p;\n+    assert(!CompressedOops::is_null(v), \"null oops should have been filtered out at dump time\");\n+    uintptr_t o = cast_from_oop<uintptr_t>(AOTMappedHeapLoader::decode_from_archive(v));\n+    assert(_base <= o && o < _top, \"must be\");\n+\n+    o += _offset;\n+    AOTMappedHeapLoader::assert_in_loaded_heap(o);\n+    RawAccess<IS_NOT_NULL>::oop_store(p, cast_to_oop(o));\n+    return true;\n+  }\n+};\n+\n+bool AOTMappedHeapLoader::init_loaded_region(FileMapInfo* mapinfo, LoadedArchiveHeapRegion* loaded_region,\n+                                             MemRegion& archive_space) {\n+  size_t total_bytes = 0;\n+  FileMapRegion* r = mapinfo->region_at(AOTMetaspace::hp);\n+  r->assert_is_heap_region();\n+  if (r->used() == 0) {\n+    return false;\n+  }\n+\n+  assert(is_aligned(r->used(), HeapWordSize), \"must be\");\n+  total_bytes += r->used();\n+  loaded_region->_region_index = AOTMetaspace::hp;\n+  loaded_region->_region_size = r->used();\n+  loaded_region->_dumptime_base = (uintptr_t)heap_region_dumptime_address(mapinfo);\n+\n+  assert(is_aligned(total_bytes, HeapWordSize), \"must be\");\n+  size_t word_size = total_bytes \/ HeapWordSize;\n+  HeapWord* buffer = Universe::heap()->allocate_loaded_archive_space(word_size);\n+  if (buffer == nullptr) {\n+    return false;\n+  }\n+\n+  archive_space = MemRegion(buffer, word_size);\n+  _loaded_heap_bottom = (uintptr_t)archive_space.start();\n+  _loaded_heap_top    = _loaded_heap_bottom + total_bytes;\n+\n+  loaded_region->_runtime_offset = _loaded_heap_bottom - loaded_region->_dumptime_base;\n+\n+  return true;\n+}\n+\n+bool AOTMappedHeapLoader::load_heap_region_impl(FileMapInfo* mapinfo, LoadedArchiveHeapRegion* loaded_region,\n+                                                uintptr_t load_address) {\n+  uintptr_t bitmap_base = (uintptr_t)mapinfo->map_bitmap_region();\n+  if (bitmap_base == 0) {\n+    _loading_failed = true;\n+    return false; \/\/ OOM or CRC error\n+  }\n+\n+  FileMapRegion* r = mapinfo->region_at(loaded_region->_region_index);\n+  if (!mapinfo->read_region(loaded_region->_region_index, (char*)load_address, r->used(), \/* do_commit = *\/ false)) {\n+    \/\/ There's no easy way to free the buffer, so we will fill it with zero later\n+    \/\/ in fill_failed_loaded_heap(), and it will eventually be GC'ed.\n+    log_warning(aot)(\"Loading of heap region %d has failed. Archived objects are disabled\", loaded_region->_region_index);\n+    _loading_failed = true;\n+    return false;\n+  }\n+  assert(r->mapped_base() == (char*)load_address, \"sanity\");\n+  log_info(aot)(\"Loaded heap    region #%d at base \" INTPTR_FORMAT \" top \" INTPTR_FORMAT\n+                \" size %6zu delta %zd\",\n+                loaded_region->_region_index, load_address, load_address + loaded_region->_region_size,\n+                loaded_region->_region_size, loaded_region->_runtime_offset);\n+\n+  uintptr_t oopmap = bitmap_base + r->oopmap_offset();\n+  BitMapView bm((BitMap::bm_word_t*)oopmap, r->oopmap_size_in_bits());\n+\n+  if (UseCompressedOops) {\n+    PatchLoadedRegionPointers patcher((narrowOop*)load_address + FileMapInfo::current_info()->mapped_heap()->oopmap_start_pos(), loaded_region);\n+    bm.iterate(&patcher);\n+  } else {\n+    PatchUncompressedEmbeddedPointers patcher((oop*)load_address + FileMapInfo::current_info()->mapped_heap()->oopmap_start_pos(), loaded_region->_runtime_offset);\n+    bm.iterate(&patcher);\n+  }\n+  return true;\n+}\n+\n+bool AOTMappedHeapLoader::load_heap_region(FileMapInfo* mapinfo) {\n+  assert(can_load(), \"loaded heap for must be supported\");\n+  init_narrow_oop_decoding(mapinfo->narrow_oop_base(), mapinfo->narrow_oop_shift());\n+\n+  LoadedArchiveHeapRegion loaded_region;\n+  memset(&loaded_region, 0, sizeof(loaded_region));\n+\n+  MemRegion archive_space;\n+  if (!init_loaded_region(mapinfo, &loaded_region, archive_space)) {\n+    return false;\n+  }\n+\n+  if (!load_heap_region_impl(mapinfo, &loaded_region, (uintptr_t)archive_space.start())) {\n+    assert(_loading_failed, \"must be\");\n+    return false;\n+  }\n+\n+  init_loaded_heap_relocation(&loaded_region);\n+  _is_loaded = true;\n+\n+  return true;\n+}\n+\n+objArrayOop AOTMappedHeapLoader::root_segment(int segment_idx) {\n+  if (CDSConfig::is_dumping_heap()) {\n+    assert(Thread::current() == (Thread*)VMThread::vm_thread(), \"should be in vm thread\");\n+  } else {\n+    assert(CDSConfig::is_using_archive(), \"must be\");\n+  }\n+\n+  objArrayOop segment = (objArrayOop)_root_segments->at(segment_idx).resolve();\n+  assert(segment != nullptr, \"should have been initialized\");\n+  return segment;\n+}\n+\n+void AOTMappedHeapLoader::get_segment_indexes(int idx, int& seg_idx, int& int_idx) {\n+  assert(_root_segment_max_size_elems > 0, \"sanity\");\n+\n+  \/\/ Try to avoid divisions for the common case.\n+  if (idx < _root_segment_max_size_elems) {\n+    seg_idx = 0;\n+    int_idx = idx;\n+  } else {\n+    seg_idx = idx \/ _root_segment_max_size_elems;\n+    int_idx = idx % _root_segment_max_size_elems;\n+  }\n+\n+  assert(idx == seg_idx * _root_segment_max_size_elems + int_idx,\n+         \"sanity: %d index maps to %d segment and %d internal\", idx, seg_idx, int_idx);\n+}\n+\n+void AOTMappedHeapLoader::add_root_segment(objArrayOop segment_oop) {\n+  assert(segment_oop != nullptr, \"must be\");\n+  assert(is_in_use(), \"must be\");\n+  if (_root_segments == nullptr) {\n+    _root_segments = new GrowableArrayCHeap<OopHandle, mtClassShared>(10);\n+  }\n+  _root_segments->push(OopHandle(Universe::vm_global(), segment_oop));\n+}\n+\n+void AOTMappedHeapLoader::init_root_segment_sizes(int max_size_elems) {\n+  _root_segment_max_size_elems = max_size_elems;\n+}\n+\n+oop AOTMappedHeapLoader::get_root(int index) {\n+  assert(!_root_segments->is_empty(), \"must have loaded shared heap\");\n+  int seg_idx, int_idx;\n+  get_segment_indexes(index, seg_idx, int_idx);\n+  objArrayOop result = objArrayOop(root_segment(seg_idx));\n+  return result->obj_at(int_idx);\n+}\n+\n+void AOTMappedHeapLoader::clear_root(int index) {\n+  int seg_idx, int_idx;\n+  get_segment_indexes(index, seg_idx, int_idx);\n+  root_segment(seg_idx)->obj_at_put(int_idx, nullptr);\n+}\n+\n+class VerifyLoadedHeapEmbeddedPointers: public BasicOopIterateClosure {\n+  HashTable<uintptr_t, bool>* _table;\n+\n+ public:\n+  VerifyLoadedHeapEmbeddedPointers(HashTable<uintptr_t, bool>* table) : _table(table) {}\n+\n+  virtual void do_oop(narrowOop* p) {\n+    \/\/ This should be called before the loaded region is modified, so all the embedded pointers\n+    \/\/ must be null, or must point to a valid object in the loaded region.\n+    narrowOop v = *p;\n+    if (!CompressedOops::is_null(v)) {\n+      oop o = CompressedOops::decode_not_null(v);\n+      uintptr_t u = cast_from_oop<uintptr_t>(o);\n+      AOTMappedHeapLoader::assert_in_loaded_heap(u);\n+      guarantee(_table->contains(u), \"must point to beginning of object in loaded archived region\");\n+    }\n+  }\n+  virtual void do_oop(oop* p) {\n+    oop v = *p;\n+    if(v != nullptr) {\n+      uintptr_t u = cast_from_oop<uintptr_t>(v);\n+      AOTMappedHeapLoader::assert_in_loaded_heap(u);\n+      guarantee(_table->contains(u), \"must point to beginning of object in loaded archived region\");\n+    }\n+  }\n+};\n+\n+void AOTMappedHeapLoader::finish_initialization(FileMapInfo* info) {\n+  patch_heap_embedded_pointers(info);\n+\n+  if (is_loaded()) {\n+    \/\/ These operations are needed only when the heap is loaded (not mapped).\n+    finish_loaded_heap();\n+    if (VerifyArchivedFields > 0) {\n+      verify_loaded_heap();\n+    }\n+  }\n+  if (is_in_use()) {\n+    patch_native_pointers();\n+    intptr_t bottom = is_loaded() ? _loaded_heap_bottom : _mapped_heap_bottom;\n+\n+    \/\/ The heap roots are stored in one or more segments that are laid out consecutively.\n+    \/\/ The size of each segment (except for the last one) is max_size_in_{elems,bytes}.\n+    HeapRootSegments segments = FileMapInfo::current_info()->mapped_heap()->root_segments();\n+    init_root_segment_sizes(segments.max_size_in_elems());\n+    intptr_t first_segment_addr = bottom + segments.base_offset();\n+    for (size_t c = 0; c < segments.count(); c++) {\n+      oop segment_oop = cast_to_oop(first_segment_addr + (c * segments.max_size_in_bytes()));\n+      assert(segment_oop->is_objArray(), \"Must be\");\n+      add_root_segment((objArrayOop)segment_oop);\n+    }\n+\n+    StringTable::load_shared_strings_array();\n+  }\n+}\n+\n+void AOTMappedHeapLoader::finish_loaded_heap() {\n+  HeapWord* bottom = (HeapWord*)_loaded_heap_bottom;\n+  HeapWord* top    = (HeapWord*)_loaded_heap_top;\n+\n+  MemRegion archive_space = MemRegion(bottom, top);\n+  Universe::heap()->complete_loaded_archive_space(archive_space);\n+}\n+\n+void AOTMappedHeapLoader::verify_loaded_heap() {\n+  log_info(aot, heap)(\"Verify all oops and pointers in loaded heap\");\n+\n+  ResourceMark rm;\n+  HashTable<uintptr_t, bool> table;\n+  VerifyLoadedHeapEmbeddedPointers verifier(&table);\n+  HeapWord* bottom = (HeapWord*)_loaded_heap_bottom;\n+  HeapWord* top    = (HeapWord*)_loaded_heap_top;\n+\n+  for (HeapWord* p = bottom; p < top; ) {\n+    oop o = cast_to_oop(p);\n+    table.put(cast_from_oop<uintptr_t>(o), true);\n+    p += o->size();\n+  }\n+\n+  for (HeapWord* p = bottom; p < top; ) {\n+    oop o = cast_to_oop(p);\n+    o->oop_iterate(&verifier);\n+    p += o->size();\n+  }\n+}\n+\n+void AOTMappedHeapLoader::fill_failed_loaded_heap() {\n+  assert(_loading_failed, \"must be\");\n+  if (_loaded_heap_bottom != 0) {\n+    assert(_loaded_heap_top != 0, \"must be\");\n+    HeapWord* bottom = (HeapWord*)_loaded_heap_bottom;\n+    HeapWord* top = (HeapWord*)_loaded_heap_top;\n+    Universe::heap()->fill_with_objects(bottom, top - bottom);\n+  }\n+}\n+\n+class PatchNativePointers: public BitMapClosure {\n+  Metadata** _start;\n+\n+ public:\n+  PatchNativePointers(Metadata** start) : _start(start) {}\n+\n+  bool do_bit(size_t offset) {\n+    Metadata** p = _start + offset;\n+    *p = (Metadata*)(address(*p) + AOTMetaspace::relocation_delta());\n+    return true;\n+  }\n+};\n+\n+void AOTMappedHeapLoader::patch_native_pointers() {\n+  if (AOTMetaspace::relocation_delta() == 0) {\n+    return;\n+  }\n+\n+  FileMapRegion* r = FileMapInfo::current_info()->region_at(AOTMetaspace::hp);\n+  if (r->mapped_base() != nullptr && r->has_ptrmap()) {\n+    log_info(aot, heap)(\"Patching native pointers in heap region\");\n+    BitMapView bm = FileMapInfo::current_info()->ptrmap_view(AOTMetaspace::hp);\n+    PatchNativePointers patcher((Metadata**)r->mapped_base() + FileMapInfo::current_info()->mapped_heap()->ptrmap_start_pos());\n+    bm.iterate(&patcher);\n+  }\n+}\n+\n+\/\/ The actual address of this region during dump time.\n+address AOTMappedHeapLoader::heap_region_dumptime_address(FileMapInfo* info) {\n+  FileMapRegion* r = info->region_at(AOTMetaspace::hp);\n+  assert(CDSConfig::is_using_archive(), \"runtime only\");\n+  assert(is_aligned(r->mapping_offset(), sizeof(HeapWord)), \"must be\");\n+  if (UseCompressedOops) {\n+    return \/*dumptime*\/ (address)((uintptr_t)info->narrow_oop_base() + r->mapping_offset());\n+  } else {\n+    return heap_region_requested_address(info);\n+  }\n+}\n+\n+\/\/ The address where this region can be mapped into the runtime heap without\n+\/\/ patching any of the pointers that are embedded in this region.\n+address AOTMappedHeapLoader::heap_region_requested_address(FileMapInfo* info) {\n+  assert(CDSConfig::is_using_archive(), \"runtime only\");\n+  FileMapRegion* r = info->region_at(AOTMetaspace::hp);\n+  assert(is_aligned(r->mapping_offset(), sizeof(HeapWord)), \"must be\");\n+  assert(can_use(), \"cannot be used by AOTMappedHeapLoader::can_load() mode\");\n+  if (UseCompressedOops) {\n+    \/\/ We can avoid relocation if each region's offset from the runtime CompressedOops::base()\n+    \/\/ is the same as its offset from the CompressedOops::base() during dumptime.\n+    \/\/ Note that CompressedOops::base() may be different between dumptime and runtime.\n+    \/\/\n+    \/\/ Example:\n+    \/\/ Dumptime base = 0x1000 and shift is 0. We have a region at address 0x2000. There's a\n+    \/\/ narrowOop P stored in this region that points to an object at address 0x2200.\n+    \/\/ P's encoded value is 0x1200.\n+    \/\/\n+    \/\/ Runtime base = 0x4000 and shift is also 0. If we map this region at 0x5000, then\n+    \/\/ the value P can remain 0x1200. The decoded address = (0x4000 + (0x1200 << 0)) = 0x5200,\n+    \/\/ which is the runtime location of the referenced object.\n+    return \/*runtime*\/ (address)((uintptr_t)CompressedOops::base() + r->mapping_offset());\n+  } else {\n+    \/\/ This was the hard-coded requested base address used at dump time. With uncompressed oops,\n+    \/\/ the heap range is assigned by the OS so we will most likely have to relocate anyway, no matter\n+    \/\/ what base address was picked at duump time.\n+    return (address)AOTMappedHeapWriter::NOCOOPS_REQUESTED_BASE;\n+  }\n+}\n+\n+bool AOTMappedHeapLoader::map_heap_region(FileMapInfo* info) {\n+  if (map_heap_region_impl(info)) {\n+#ifdef ASSERT\n+    \/\/ The \"old\" regions must be parsable -- we cannot have any unused space\n+    \/\/ at the start of the lowest G1 region that contains archived objects.\n+    assert(is_aligned(_mapped_heap_memregion.start(), G1HeapRegion::GrainBytes), \"must be\");\n+\n+    \/\/ Make sure we map at the very top of the heap - see comments in\n+    \/\/ init_heap_region_relocation().\n+    MemRegion heap_range = G1CollectedHeap::heap()->reserved();\n+    assert(heap_range.contains(_mapped_heap_memregion), \"must be\");\n+\n+    address heap_end = (address)heap_range.end();\n+    address mapped_heap_region_end = (address)_mapped_heap_memregion.end();\n+    assert(heap_end >= mapped_heap_region_end, \"must be\");\n+    assert(heap_end - mapped_heap_region_end < (intx)(G1HeapRegion::GrainBytes),\n+           \"must be at the top of the heap to avoid fragmentation\");\n+#endif\n+\n+    set_mapped();\n+    return true;\n+  } else {\n+    return false;\n+  }\n+}\n+\n+bool AOTMappedHeapLoader::map_heap_region_impl(FileMapInfo* info) {\n+  assert(UseG1GC, \"the following code assumes G1\");\n+\n+  FileMapRegion* r = info->region_at(AOTMetaspace::hp);\n+  size_t size = r->used();\n+  if (size == 0) {\n+    return false; \/\/ no archived java heap data\n+  }\n+\n+  size_t word_size = size \/ HeapWordSize;\n+  address requested_start = heap_region_requested_address(info);\n+\n+  aot_log_info(aot)(\"Preferred address to map heap data (to avoid relocation) is \" INTPTR_FORMAT, p2i(requested_start));\n+\n+  \/\/ allocate from java heap\n+  HeapWord* start = G1CollectedHeap::heap()->alloc_archive_region(word_size, (HeapWord*)requested_start);\n+  if (start == nullptr) {\n+    AOTMetaspace::report_loading_error(\"UseSharedSpaces: Unable to allocate java heap region for archive heap.\");\n+    return false;\n+  }\n+\n+  _mapped_heap_memregion = MemRegion(start, word_size);\n+\n+  \/\/ Map the archived heap data. No need to call MemTracker::record_virtual_memory_tag()\n+  \/\/ for mapped region as it is part of the reserved java heap, which is already recorded.\n+  char* addr = (char*)_mapped_heap_memregion.start();\n+  char* base;\n+\n+  if (AOTMetaspace::use_windows_memory_mapping() || UseLargePages) {\n+    \/\/ With UseLargePages, memory mapping may fail on some OSes if the size is not\n+    \/\/ large page aligned, so let's use read() instead. In this case, the memory region\n+    \/\/ is already commited by G1 so we don't need to commit it again.\n+    if (!info->read_region(AOTMetaspace::hp, addr,\n+                           align_up(_mapped_heap_memregion.byte_size(), os::vm_page_size()),\n+                           \/* do_commit = *\/ !UseLargePages)) {\n+      dealloc_heap_region(info);\n+      aot_log_error(aot)(\"Failed to read archived heap region into \" INTPTR_FORMAT, p2i(addr));\n+      return false;\n+    }\n+    \/\/ Checks for VerifySharedSpaces is already done inside read_region()\n+    base = addr;\n+  } else {\n+    base = info->map_heap_region(r, addr, _mapped_heap_memregion.byte_size());\n+    if (base == nullptr || base != addr) {\n+      dealloc_heap_region(info);\n+      AOTMetaspace::report_loading_error(\"UseSharedSpaces: Unable to map at required address in java heap. \"\n+                                            INTPTR_FORMAT \", size = %zu bytes\",\n+                                            p2i(addr), _mapped_heap_memregion.byte_size());\n+      return false;\n+    }\n+\n+    if (VerifySharedSpaces && !r->check_region_crc(base)) {\n+      dealloc_heap_region(info);\n+      AOTMetaspace::report_loading_error(\"UseSharedSpaces: mapped heap region is corrupt\");\n+      return false;\n+    }\n+  }\n+\n+  r->set_mapped_base(base);\n+\n+  \/\/ If the requested range is different from the range allocated by GC, then\n+  \/\/ the pointers need to be patched.\n+  address mapped_start = (address) _mapped_heap_memregion.start();\n+  ptrdiff_t delta = mapped_start - requested_start;\n+  if (UseCompressedOops &&\n+      (info->narrow_oop_mode() != CompressedOops::mode() ||\n+       info->narrow_oop_shift() != CompressedOops::shift())) {\n+    _heap_pointers_need_patching = true;\n+  }\n+  if (delta != 0) {\n+    _heap_pointers_need_patching = true;\n+  }\n+  init_mapped_heap_info(mapped_start, delta, info->narrow_oop_shift());\n+\n+  if (_heap_pointers_need_patching) {\n+    char* bitmap_base = info->map_bitmap_region();\n+    if (bitmap_base == nullptr) {\n+      AOTMetaspace::report_loading_error(\"CDS heap cannot be used because bitmap region cannot be mapped\");\n+      dealloc_heap_region(info);\n+      _heap_pointers_need_patching = false;\n+      return false;\n+    }\n+  }\n+  aot_log_info(aot)(\"Heap data mapped at \" INTPTR_FORMAT \", size = %8zu bytes\",\n+                p2i(mapped_start), _mapped_heap_memregion.byte_size());\n+  aot_log_info(aot)(\"CDS heap data relocation delta = %zd bytes\", delta);\n+  return true;\n+}\n+\n+narrowOop AOTMappedHeapLoader::encoded_heap_region_dumptime_address(FileMapInfo* info) {\n+  assert(CDSConfig::is_using_archive(), \"runtime only\");\n+  assert(UseCompressedOops, \"sanity\");\n+  FileMapRegion* r = info->region_at(AOTMetaspace::hp);\n+  return CompressedOops::narrow_oop_cast(r->mapping_offset() >> info->narrow_oop_shift());\n+}\n+\n+void AOTMappedHeapLoader::patch_heap_embedded_pointers(FileMapInfo* info) {\n+  if (!info->is_mapped() || !_heap_pointers_need_patching) {\n+    return;\n+  }\n+\n+  char* bitmap_base = info->map_bitmap_region();\n+  assert(bitmap_base != nullptr, \"must have already been mapped\");\n+\n+  FileMapRegion* r = info->region_at(AOTMetaspace::hp);\n+  patch_embedded_pointers(\n+      info, _mapped_heap_memregion,\n+      (address)(info->region_at(AOTMetaspace::bm)->mapped_base()) + r->oopmap_offset(),\n+      r->oopmap_size_in_bits());\n+}\n+\n+void AOTMappedHeapLoader::fixup_mapped_heap_region(FileMapInfo* info) {\n+  if (is_mapped()) {\n+    assert(!_mapped_heap_memregion.is_empty(), \"sanity\");\n+\n+    \/\/ Populate the archive regions' G1BlockOffsetTables. That ensures\n+    \/\/ fast G1BlockOffsetTable::block_start operations for any given address\n+    \/\/ within the archive regions when trying to find start of an object\n+    \/\/ (e.g. during card table scanning).\n+    G1CollectedHeap::heap()->populate_archive_regions_bot(_mapped_heap_memregion);\n+  }\n+}\n+\n+\/\/ dealloc the archive regions from java heap\n+void AOTMappedHeapLoader::dealloc_heap_region(FileMapInfo* info) {\n+  G1CollectedHeap::heap()->dealloc_archive_regions(_mapped_heap_memregion);\n+}\n+\n+AOTMapLogger::OopDataIterator* AOTMappedHeapLoader::oop_iterator(FileMapInfo* info, address buffer_start, address buffer_end) {\n+  class MappedLoaderOopIterator : public AOTMapLogger::OopDataIterator {\n+  private:\n+    address _current;\n+    address _next;\n+\n+    address _buffer_start;\n+    address _buffer_end;\n+    uint64_t _buffer_start_narrow_oop;\n+    intptr_t _buffer_to_requested_delta;\n+    int _requested_shift;\n+\n+    size_t _num_root_segments;\n+    size_t _num_obj_arrays_logged;\n+\n+  public:\n+    MappedLoaderOopIterator(address buffer_start,\n+                            address buffer_end,\n+                            uint64_t buffer_start_narrow_oop,\n+                            intptr_t buffer_to_requested_delta,\n+                            int requested_shift,\n+                            size_t num_root_segments)\n+      : _current(nullptr),\n+        _next(buffer_start),\n+        _buffer_start(buffer_start),\n+        _buffer_end(buffer_end),\n+        _buffer_start_narrow_oop(buffer_start_narrow_oop),\n+        _buffer_to_requested_delta(buffer_to_requested_delta),\n+        _requested_shift(requested_shift),\n+        _num_root_segments(num_root_segments),\n+        _num_obj_arrays_logged(0) {\n+    }\n+\n+\n+    AOTMapLogger::OopData capture(address buffered_addr) {\n+      oopDesc* raw_oop = (oopDesc*)buffered_addr;\n+      size_t size = raw_oop->size();\n+      address requested_addr = buffered_addr + _buffer_to_requested_delta;\n+      intptr_t target_location = intptr_t(requested_addr);\n+      uint64_t pd = (uint64_t)(pointer_delta(buffered_addr, _buffer_start, 1));\n+      uint32_t narrow_location = checked_cast<uint32_t>(_buffer_start_narrow_oop + (pd >> _requested_shift));\n+      Klass* klass = raw_oop->klass();\n+\n+      return { buffered_addr,\n+               requested_addr,\n+               target_location,\n+               narrow_location,\n+               raw_oop,\n+               klass,\n+               size,\n+               false };\n+    }\n+\n+    bool has_next() override {\n+      return _next < _buffer_end;\n+    }\n+\n+    AOTMapLogger::OopData next() override {\n+      _current = _next;\n+      AOTMapLogger::OopData result = capture(_current);\n+      if (result._klass->is_objArray_klass()) {\n+        result._is_root_segment = _num_obj_arrays_logged++ < _num_root_segments;\n+      }\n+      _next = _current + result._size * BytesPerWord;\n+      return result;\n+    }\n+\n+    AOTMapLogger::OopData obj_at(narrowOop* addr) override {\n+      uint64_t n = (uint64_t)(*addr);\n+      if (n == 0) {\n+        return null_data();\n+      } else {\n+        precond(n >= _buffer_start_narrow_oop);\n+        address buffer_addr = _buffer_start + ((n - _buffer_start_narrow_oop) << _requested_shift);\n+        return capture(buffer_addr);\n+      }\n+    }\n+\n+    AOTMapLogger::OopData obj_at(oop* addr) override {\n+      address requested_value = cast_from_oop<address>(*addr);\n+      if (requested_value == nullptr) {\n+        return null_data();\n+      } else {\n+        address buffer_addr = requested_value - _buffer_to_requested_delta;\n+        return capture(buffer_addr);\n+      }\n+    }\n+\n+    GrowableArrayCHeap<AOTMapLogger::OopData, mtClass>* roots() override {\n+      return new GrowableArrayCHeap<AOTMapLogger::OopData, mtClass>();\n+    }\n+  };\n+\n+  FileMapRegion* r = info->region_at(AOTMetaspace::hp);\n+  address requested_base = UseCompressedOops ? (address)info->narrow_oop_base() : heap_region_requested_address(info);\n+  address requested_start = requested_base + r->mapping_offset();\n+  int requested_shift = info->narrow_oop_shift();\n+  intptr_t buffer_to_requested_delta = requested_start - buffer_start;\n+  uint64_t buffer_start_narrow_oop = 0xdeadbeed;\n+  if (UseCompressedOops) {\n+    buffer_start_narrow_oop = (uint64_t)(pointer_delta(requested_start, requested_base, 1)) >> requested_shift;\n+    assert(buffer_start_narrow_oop < 0xffffffff, \"sanity\");\n+  }\n+\n+  return new MappedLoaderOopIterator(buffer_start,\n+                                     buffer_end,\n+                                     buffer_start_narrow_oop,\n+                                     buffer_to_requested_delta,\n+                                     requested_shift,\n+                                     info->mapped_heap()->root_segments().count());\n+}\n+\n+#endif \/\/ INCLUDE_CDS_JAVA_HEAP\n","filename":"src\/hotspot\/share\/cds\/aotMappedHeapLoader.cpp","additions":847,"deletions":0,"binary":false,"changes":847,"status":"added"},{"patch":"@@ -0,0 +1,193 @@\n+\/*\n+ * Copyright (c) 2018, 2025, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef SHARE_CDS_AOTMAPPEDHEAPLOADER_HPP\n+#define SHARE_CDS_AOTMAPPEDHEAPLOADER_HPP\n+\n+#include \"cds\/aotMapLogger.hpp\"\n+#include \"gc\/shared\/gc_globals.hpp\"\n+#include \"memory\/allocation.hpp\"\n+#include \"memory\/allStatic.hpp\"\n+#include \"memory\/memRegion.hpp\"\n+#include \"oops\/oopHandle.hpp\"\n+#include \"oops\/oopsHierarchy.hpp\"\n+#include \"runtime\/globals.hpp\"\n+#include \"utilities\/bitMap.hpp\"\n+#include \"utilities\/growableArray.hpp\"\n+#include \"utilities\/macros.hpp\"\n+\n+class  FileMapInfo;\n+struct LoadedArchiveHeapRegion;\n+\n+class AOTMappedHeapLoader : AllStatic {\n+  friend class AOTMapLogger;\n+\n+public:\n+  \/\/ At runtime, the heap region in the CDS archive can be used in two different ways,\n+  \/\/ depending on the GC type:\n+  \/\/ - Mapped: (G1 only) the region is directly mapped into the Java heap\n+  \/\/ - Loaded: At VM start-up, the objects in the heap region are copied into the\n+  \/\/           Java heap. This is easier to implement than mapping but\n+  \/\/           slightly less efficient, as the embedded pointers need to be relocated.\n+  static bool can_use() { return can_map() || can_load(); }\n+\n+  \/\/ Can this VM map archived heap region? Currently only G1+compressed{oops,cp}\n+  static bool can_map() {\n+    CDS_JAVA_HEAP_ONLY(return (UseG1GC && UseCompressedClassPointers);)\n+    NOT_CDS_JAVA_HEAP(return false;)\n+  }\n+\n+  \/\/ Can this VM load the objects from archived heap region into the heap at start-up?\n+  static bool can_load()  NOT_CDS_JAVA_HEAP_RETURN_(false);\n+  static bool is_loaded() {\n+    CDS_JAVA_HEAP_ONLY(return _is_loaded;)\n+    NOT_CDS_JAVA_HEAP(return false;)\n+  }\n+\n+  static bool is_in_use() {\n+    return is_loaded() || is_mapped();\n+  }\n+\n+  static ptrdiff_t mapped_heap_delta() {\n+    CDS_JAVA_HEAP_ONLY(assert(!is_loaded(), \"must be\"));\n+    CDS_JAVA_HEAP_ONLY(assert(_mapped_heap_relocation_initialized, \"must be\"));\n+    CDS_JAVA_HEAP_ONLY(return _mapped_heap_delta;)\n+    NOT_CDS_JAVA_HEAP_RETURN_(0L);\n+  }\n+\n+  static void set_mapped() {\n+    CDS_JAVA_HEAP_ONLY(_is_mapped = true;)\n+    NOT_CDS_JAVA_HEAP_RETURN;\n+  }\n+  static bool is_mapped() {\n+    CDS_JAVA_HEAP_ONLY(return _is_mapped;)\n+    NOT_CDS_JAVA_HEAP_RETURN_(false);\n+  }\n+\n+  static void finish_initialization(FileMapInfo* info) NOT_CDS_JAVA_HEAP_RETURN;\n+\n+  \/\/ NarrowOops stored in the CDS archive may use a different encoding scheme\n+  \/\/ than CompressedOops::{base,shift} -- see FileMapInfo::map_heap_region_impl.\n+  \/\/ To decode them, do not use CompressedOops::decode_not_null. Use this\n+  \/\/ function instead.\n+  inline static oop decode_from_archive(narrowOop v) NOT_CDS_JAVA_HEAP_RETURN_(nullptr);\n+\n+  \/\/ More efficient version, but works only when ArchiveHeap is mapped.\n+  inline static oop decode_from_mapped_archive(narrowOop v) NOT_CDS_JAVA_HEAP_RETURN_(nullptr);\n+\n+  static void patch_compressed_embedded_pointers(BitMapView bm,\n+                                                 FileMapInfo* info,\n+                                                 MemRegion region) NOT_CDS_JAVA_HEAP_RETURN;\n+\n+  static void patch_embedded_pointers(FileMapInfo* info,\n+                                      MemRegion region, address oopmap,\n+                                      size_t oopmap_size_in_bits) NOT_CDS_JAVA_HEAP_RETURN;\n+\n+  static void fixup_region() NOT_CDS_JAVA_HEAP_RETURN;\n+\n+#if INCLUDE_CDS_JAVA_HEAP\n+  static void init_mapped_heap_info(address mapped_heap_bottom, ptrdiff_t delta, int dumptime_oop_shift);\n+private:\n+  static bool _is_mapped;\n+  static bool _is_loaded;\n+\n+  \/\/ Support for loaded archived heap. These are cached values from\n+  \/\/ LoadedArchiveHeapRegion's.\n+  static uintptr_t _dumptime_base;\n+  static uintptr_t _dumptime_top;\n+  static intx _runtime_offset;\n+\n+  static uintptr_t _loaded_heap_bottom;\n+  static uintptr_t _loaded_heap_top;\n+  static bool _loading_failed;\n+\n+  \/\/ UseCompressedOops only: Used by decode_from_archive\n+  static bool    _narrow_oop_base_initialized;\n+  static address _narrow_oop_base;\n+  static int     _narrow_oop_shift;\n+\n+  \/\/ is_mapped() only: the mapped address of each region is offset by this amount from\n+  \/\/ their requested address.\n+  static uintptr_t _mapped_heap_bottom;\n+  static ptrdiff_t _mapped_heap_delta;\n+  static bool      _mapped_heap_relocation_initialized;\n+\n+  \/\/ Heap roots\n+  static GrowableArrayCHeap<OopHandle, mtClassShared>* _root_segments;\n+  static int _root_segment_max_size_elems;\n+\n+  static MemRegion _mapped_heap_memregion;\n+  static bool _heap_pointers_need_patching;\n+\n+  static void init_narrow_oop_decoding(address base, int shift);\n+  static bool init_loaded_region(FileMapInfo* mapinfo, LoadedArchiveHeapRegion* loaded_region,\n+                                 MemRegion& archive_space);\n+  static bool load_heap_region_impl(FileMapInfo* mapinfo, LoadedArchiveHeapRegion* loaded_region, uintptr_t buffer);\n+  static void init_loaded_heap_relocation(LoadedArchiveHeapRegion* reloc_info);\n+  static void patch_native_pointers();\n+  static void finish_loaded_heap();\n+  static void verify_loaded_heap();\n+  static void fill_failed_loaded_heap();\n+\n+  static bool is_in_loaded_heap(uintptr_t o) {\n+    return (_loaded_heap_bottom <= o && o < _loaded_heap_top);\n+  }\n+\n+  static objArrayOop root_segment(int segment_idx);\n+  static void get_segment_indexes(int idx, int& seg_idx, int& int_idx);\n+  static void add_root_segment(objArrayOop segment_oop);\n+  static void init_root_segment_sizes(int max_size_elems);\n+\n+  template<bool IS_MAPPED>\n+  inline static oop decode_from_archive_impl(narrowOop v) NOT_CDS_JAVA_HEAP_RETURN_(nullptr);\n+\n+  class PatchLoadedRegionPointers;\n+  class PatchUncompressedLoadedRegionPointers;\n+\n+  static address heap_region_dumptime_address(FileMapInfo* info);\n+  static address heap_region_requested_address(FileMapInfo* info);\n+  static bool map_heap_region_impl(FileMapInfo* info);\n+  static narrowOop encoded_heap_region_dumptime_address(FileMapInfo* info);\n+  static void patch_heap_embedded_pointers(FileMapInfo* info);\n+  static void fixup_mapped_heap_region(FileMapInfo* info);\n+  static void dealloc_heap_region(FileMapInfo* info);\n+\n+public:\n+\n+  static bool map_heap_region(FileMapInfo* info);\n+  static bool load_heap_region(FileMapInfo* mapinfo);\n+  static void assert_in_loaded_heap(uintptr_t o) {\n+    assert(is_in_loaded_heap(o), \"must be\");\n+  }\n+\n+  static oop get_root(int index);\n+  static void clear_root(int index);\n+\n+  static AOTMapLogger::OopDataIterator* oop_iterator(FileMapInfo* info, address buffer_start, address buffer_end);\n+\n+#endif \/\/ INCLUDE_CDS_JAVA_HEAP\n+\n+};\n+\n+#endif \/\/ SHARE_CDS_AOTMAPPEDHEAPLOADER_HPP\n","filename":"src\/hotspot\/share\/cds\/aotMappedHeapLoader.hpp","additions":193,"deletions":0,"binary":false,"changes":193,"status":"added"},{"patch":"@@ -0,0 +1,62 @@\n+\/*\n+ * Copyright (c) 2018, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef SHARE_CDS_AOTMAPPEDHEAPLOADER_INLINE_HPP\n+#define SHARE_CDS_AOTMAPPEDHEAPLOADER_INLINE_HPP\n+\n+#include \"cds\/aotMappedHeapLoader.hpp\"\n+\n+#include \"oops\/compressedOops.inline.hpp\"\n+#include \"utilities\/align.hpp\"\n+\n+#if INCLUDE_CDS_JAVA_HEAP\n+\n+template<bool IS_MAPPED>\n+inline oop AOTMappedHeapLoader::decode_from_archive_impl(narrowOop v) {\n+  assert(!CompressedOops::is_null(v), \"narrow oop value can never be zero\");\n+  assert(_narrow_oop_base_initialized, \"relocation information must have been initialized\");\n+  uintptr_t p = ((uintptr_t)_narrow_oop_base) + ((uintptr_t)v << _narrow_oop_shift);\n+  if (IS_MAPPED) {\n+    assert(_dumptime_base == UINTPTR_MAX, \"must be\");\n+  } else if (p >= _dumptime_base) {\n+    assert(p < _dumptime_top, \"must be\");\n+    p += _runtime_offset;\n+  }\n+\n+  oop result = cast_to_oop((uintptr_t)p);\n+  assert(is_object_aligned(result), \"address not aligned: \" INTPTR_FORMAT, p2i((void*) result));\n+  return result;\n+}\n+\n+inline oop AOTMappedHeapLoader::decode_from_archive(narrowOop v) {\n+  return decode_from_archive_impl<false>(v);\n+}\n+\n+inline oop AOTMappedHeapLoader::decode_from_mapped_archive(narrowOop v) {\n+  return decode_from_archive_impl<true>(v);\n+}\n+\n+#endif\n+\n+#endif \/\/ SHARE_CDS_AOTMAPPEDHEAPLOADER_INLINE_HPP\n","filename":"src\/hotspot\/share\/cds\/aotMappedHeapLoader.inline.hpp","additions":62,"deletions":0,"binary":false,"changes":62,"status":"added"},{"patch":"@@ -0,0 +1,939 @@\n+\/*\n+ * Copyright (c) 2023, 2025, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#include \"cds\/aotMappedHeapLoader.hpp\"\n+#include \"cds\/aotMappedHeapWriter.hpp\"\n+#include \"cds\/aotReferenceObjSupport.hpp\"\n+#include \"cds\/cdsConfig.hpp\"\n+#include \"cds\/filemap.hpp\"\n+#include \"cds\/heapShared.inline.hpp\"\n+#include \"cds\/regeneratedClasses.hpp\"\n+#include \"classfile\/javaClasses.hpp\"\n+#include \"classfile\/modules.hpp\"\n+#include \"classfile\/systemDictionary.hpp\"\n+#include \"gc\/shared\/collectedHeap.hpp\"\n+#include \"memory\/allocation.inline.hpp\"\n+#include \"memory\/iterator.inline.hpp\"\n+#include \"memory\/oopFactory.hpp\"\n+#include \"memory\/universe.hpp\"\n+#include \"oops\/compressedOops.hpp\"\n+#include \"oops\/objArrayOop.inline.hpp\"\n+#include \"oops\/oop.inline.hpp\"\n+#include \"oops\/oopHandle.inline.hpp\"\n+#include \"oops\/typeArrayKlass.hpp\"\n+#include \"oops\/typeArrayOop.hpp\"\n+#include \"runtime\/java.hpp\"\n+#include \"runtime\/mutexLocker.hpp\"\n+#include \"utilities\/bitMap.inline.hpp\"\n+#if INCLUDE_G1GC\n+#include \"gc\/g1\/g1CollectedHeap.hpp\"\n+#include \"gc\/g1\/g1HeapRegion.hpp\"\n+#endif\n+\n+#if INCLUDE_CDS_JAVA_HEAP\n+\n+GrowableArrayCHeap<u1, mtClassShared>* AOTMappedHeapWriter::_buffer = nullptr;\n+\n+\/\/ The following are offsets from buffer_bottom()\n+size_t AOTMappedHeapWriter::_buffer_used;\n+\n+\/\/ Heap root segments\n+HeapRootSegments AOTMappedHeapWriter::_heap_root_segments;\n+\n+address AOTMappedHeapWriter::_requested_bottom;\n+address AOTMappedHeapWriter::_requested_top;\n+\n+GrowableArrayCHeap<AOTMappedHeapWriter::NativePointerInfo, mtClassShared>* AOTMappedHeapWriter::_native_pointers;\n+GrowableArrayCHeap<oop, mtClassShared>* AOTMappedHeapWriter::_source_objs;\n+GrowableArrayCHeap<AOTMappedHeapWriter::HeapObjOrder, mtClassShared>* AOTMappedHeapWriter::_source_objs_order;\n+\n+AOTMappedHeapWriter::BufferOffsetToSourceObjectTable*\n+AOTMappedHeapWriter::_buffer_offset_to_source_obj_table = nullptr;\n+\n+DumpedInternedStrings *AOTMappedHeapWriter::_dumped_interned_strings = nullptr;\n+\n+typedef HashTable<\n+      size_t,    \/\/ offset of a filler from ArchiveHeapWriter::buffer_bottom()\n+      size_t,    \/\/ size of this filler (in bytes)\n+      127,       \/\/ prime number\n+      AnyObj::C_HEAP,\n+      mtClassShared> FillersTable;\n+static FillersTable* _fillers;\n+static int _num_native_ptrs = 0;\n+\n+void AOTMappedHeapWriter::init() {\n+  if (CDSConfig::is_dumping_heap()) {\n+    Universe::heap()->collect(GCCause::_java_lang_system_gc);\n+\n+    _buffer_offset_to_source_obj_table = new BufferOffsetToSourceObjectTable(\/*size (prime)*\/36137, \/*max size*\/1 * M);\n+    _dumped_interned_strings = new (mtClass)DumpedInternedStrings(INITIAL_TABLE_SIZE, MAX_TABLE_SIZE);\n+    _fillers = new FillersTable();\n+    _requested_bottom = nullptr;\n+    _requested_top = nullptr;\n+\n+    _native_pointers = new GrowableArrayCHeap<NativePointerInfo, mtClassShared>(2048);\n+    _source_objs = new GrowableArrayCHeap<oop, mtClassShared>(10000);\n+\n+    guarantee(MIN_GC_REGION_ALIGNMENT <= G1HeapRegion::min_region_size_in_words() * HeapWordSize, \"must be\");\n+  }\n+}\n+\n+void AOTMappedHeapWriter::delete_tables_with_raw_oops() {\n+  delete _source_objs;\n+  _source_objs = nullptr;\n+\n+  delete _dumped_interned_strings;\n+  _dumped_interned_strings = nullptr;\n+}\n+\n+void AOTMappedHeapWriter::add_source_obj(oop src_obj) {\n+  _source_objs->append(src_obj);\n+}\n+\n+void AOTMappedHeapWriter::write(GrowableArrayCHeap<oop, mtClassShared>* roots,\n+                                ArchiveMappedHeapInfo* heap_info) {\n+  assert(CDSConfig::is_dumping_heap(), \"sanity\");\n+  allocate_buffer();\n+  copy_source_objs_to_buffer(roots);\n+  set_requested_address(heap_info);\n+  relocate_embedded_oops(roots, heap_info);\n+}\n+\n+bool AOTMappedHeapWriter::is_too_large_to_archive(oop o) {\n+  return is_too_large_to_archive(o->size());\n+}\n+\n+bool AOTMappedHeapWriter::is_string_too_large_to_archive(oop string) {\n+  typeArrayOop value = java_lang_String::value_no_keepalive(string);\n+  return is_too_large_to_archive(value);\n+}\n+\n+bool AOTMappedHeapWriter::is_too_large_to_archive(size_t size) {\n+  assert(size > 0, \"no zero-size object\");\n+  assert(size * HeapWordSize > size, \"no overflow\");\n+  static_assert(MIN_GC_REGION_ALIGNMENT > 0, \"must be positive\");\n+\n+  size_t byte_size = size * HeapWordSize;\n+  if (byte_size > size_t(MIN_GC_REGION_ALIGNMENT)) {\n+    return true;\n+  } else {\n+    return false;\n+  }\n+}\n+\n+\/\/ Keep track of the contents of the archived interned string table. This table\n+\/\/ is used only by CDSHeapVerifier.\n+void AOTMappedHeapWriter::add_to_dumped_interned_strings(oop string) {\n+  assert_at_safepoint(); \/\/ DumpedInternedStrings uses raw oops\n+  assert(!is_string_too_large_to_archive(string), \"must be\");\n+  bool created;\n+  _dumped_interned_strings->put_if_absent(string, true, &created);\n+  if (created) {\n+    \/\/ Prevent string deduplication from changing the value field to\n+    \/\/ something not in the archive.\n+    java_lang_String::set_deduplication_forbidden(string);\n+    _dumped_interned_strings->maybe_grow();\n+  }\n+}\n+\n+bool AOTMappedHeapWriter::is_dumped_interned_string(oop o) {\n+  return _dumped_interned_strings->get(o) != nullptr;\n+}\n+\n+\/\/ Various lookup functions between source_obj, buffered_obj and requested_obj\n+bool AOTMappedHeapWriter::is_in_requested_range(oop o) {\n+  assert(_requested_bottom != nullptr, \"do not call before _requested_bottom is initialized\");\n+  address a = cast_from_oop<address>(o);\n+  return (_requested_bottom <= a && a < _requested_top);\n+}\n+\n+oop AOTMappedHeapWriter::requested_obj_from_buffer_offset(size_t offset) {\n+  oop req_obj = cast_to_oop(_requested_bottom + offset);\n+  assert(is_in_requested_range(req_obj), \"must be\");\n+  return req_obj;\n+}\n+\n+oop AOTMappedHeapWriter::source_obj_to_requested_obj(oop src_obj) {\n+  assert(CDSConfig::is_dumping_heap(), \"dump-time only\");\n+  HeapShared::CachedOopInfo* p = HeapShared::get_cached_oop_info(src_obj);\n+  if (p != nullptr) {\n+    return requested_obj_from_buffer_offset(p->buffer_offset());\n+  } else {\n+    return nullptr;\n+  }\n+}\n+\n+oop AOTMappedHeapWriter::buffered_addr_to_source_obj(address buffered_addr) {\n+  OopHandle* oh = _buffer_offset_to_source_obj_table->get(buffered_address_to_offset(buffered_addr));\n+  if (oh != nullptr) {\n+    return oh->resolve();\n+  } else {\n+    return nullptr;\n+  }\n+}\n+\n+Klass* AOTMappedHeapWriter::real_klass_of_buffered_oop(address buffered_addr) {\n+  oop p = buffered_addr_to_source_obj(buffered_addr);\n+  if (p != nullptr) {\n+    return p->klass();\n+  } else if (get_filler_size_at(buffered_addr) > 0) {\n+    return Universe::fillerArrayKlass();\n+  } else {\n+    \/\/ This is one of the root segments\n+    return Universe::objectArrayKlass();\n+  }\n+}\n+\n+size_t AOTMappedHeapWriter::size_of_buffered_oop(address buffered_addr) {\n+  oop p = buffered_addr_to_source_obj(buffered_addr);\n+  if (p != nullptr) {\n+    return p->size();\n+  }\n+\n+  size_t nbytes = get_filler_size_at(buffered_addr);\n+  if (nbytes > 0) {\n+    assert((nbytes % BytesPerWord) == 0, \"should be aligned\");\n+    return nbytes \/ BytesPerWord;\n+  }\n+\n+  address hrs = buffer_bottom();\n+  for (size_t seg_idx = 0; seg_idx < _heap_root_segments.count(); seg_idx++) {\n+    nbytes = _heap_root_segments.size_in_bytes(seg_idx);\n+    if (hrs == buffered_addr) {\n+      assert((nbytes % BytesPerWord) == 0, \"should be aligned\");\n+      return nbytes \/ BytesPerWord;\n+    }\n+    hrs += nbytes;\n+  }\n+\n+  ShouldNotReachHere();\n+  return 0;\n+}\n+\n+address AOTMappedHeapWriter::buffered_addr_to_requested_addr(address buffered_addr) {\n+  return _requested_bottom + buffered_address_to_offset(buffered_addr);\n+}\n+\n+address AOTMappedHeapWriter::requested_address() {\n+  assert(_buffer != nullptr, \"must be initialized\");\n+  return _requested_bottom;\n+}\n+\n+void AOTMappedHeapWriter::allocate_buffer() {\n+  int initial_buffer_size = 100000;\n+  _buffer = new GrowableArrayCHeap<u1, mtClassShared>(initial_buffer_size);\n+  _buffer_used = 0;\n+  ensure_buffer_space(1); \/\/ so that buffer_bottom() works\n+}\n+\n+void AOTMappedHeapWriter::ensure_buffer_space(size_t min_bytes) {\n+  \/\/ We usually have very small heaps. If we get a huge one it's probably caused by a bug.\n+  guarantee(min_bytes <= max_jint, \"we dont support archiving more than 2G of objects\");\n+  _buffer->at_grow(to_array_index(min_bytes));\n+}\n+\n+objArrayOop AOTMappedHeapWriter::allocate_root_segment(size_t offset, int element_count) {\n+  HeapWord* mem = offset_to_buffered_address<HeapWord *>(offset);\n+  memset(mem, 0, objArrayOopDesc::object_size(element_count));\n+\n+  \/\/ The initialization code is copied from MemAllocator::finish and ObjArrayAllocator::initialize.\n+  if (UseCompactObjectHeaders) {\n+    oopDesc::release_set_mark(mem, Universe::objectArrayKlass()->prototype_header());\n+  } else {\n+    oopDesc::set_mark(mem, markWord::prototype());\n+    oopDesc::release_set_klass(mem, Universe::objectArrayKlass());\n+  }\n+  arrayOopDesc::set_length(mem, element_count);\n+  return objArrayOop(cast_to_oop(mem));\n+}\n+\n+void AOTMappedHeapWriter::root_segment_at_put(objArrayOop segment, int index, oop root) {\n+  \/\/ Do not use arrayOop->obj_at_put(i, o) as arrayOop is outside the real heap!\n+  if (UseCompressedOops) {\n+    *segment->obj_at_addr<narrowOop>(index) = CompressedOops::encode(root);\n+  } else {\n+    *segment->obj_at_addr<oop>(index) = root;\n+  }\n+}\n+\n+void AOTMappedHeapWriter::copy_roots_to_buffer(GrowableArrayCHeap<oop, mtClassShared>* roots) {\n+  \/\/ Depending on the number of classes we are archiving, a single roots array may be\n+  \/\/ larger than MIN_GC_REGION_ALIGNMENT. Roots are allocated first in the buffer, which\n+  \/\/ allows us to chop the large array into a series of \"segments\". Current layout\n+  \/\/ starts with zero or more segments exactly fitting MIN_GC_REGION_ALIGNMENT, and end\n+  \/\/ with a single segment that may be smaller than MIN_GC_REGION_ALIGNMENT.\n+  \/\/ This is simple and efficient. We do not need filler objects anywhere between the segments,\n+  \/\/ or immediately after the last segment. This allows starting the object dump immediately\n+  \/\/ after the roots.\n+\n+  assert((_buffer_used % MIN_GC_REGION_ALIGNMENT) == 0,\n+         \"Pre-condition: Roots start at aligned boundary: %zu\", _buffer_used);\n+\n+  int max_elem_count = ((MIN_GC_REGION_ALIGNMENT - arrayOopDesc::header_size_in_bytes()) \/ heapOopSize);\n+  assert(objArrayOopDesc::object_size(max_elem_count)*HeapWordSize == MIN_GC_REGION_ALIGNMENT,\n+         \"Should match exactly\");\n+\n+  HeapRootSegments segments(_buffer_used,\n+                            roots->length(),\n+                            MIN_GC_REGION_ALIGNMENT,\n+                            max_elem_count);\n+\n+  int root_index = 0;\n+  for (size_t seg_idx = 0; seg_idx < segments.count(); seg_idx++) {\n+    int size_elems = segments.size_in_elems(seg_idx);\n+    size_t size_bytes = segments.size_in_bytes(seg_idx);\n+\n+    size_t oop_offset = _buffer_used;\n+    _buffer_used = oop_offset + size_bytes;\n+    ensure_buffer_space(_buffer_used);\n+\n+    assert((oop_offset % MIN_GC_REGION_ALIGNMENT) == 0,\n+           \"Roots segment %zu start is not aligned: %zu\",\n+           segments.count(), oop_offset);\n+\n+    objArrayOop seg_oop = allocate_root_segment(oop_offset, size_elems);\n+    for (int i = 0; i < size_elems; i++) {\n+      root_segment_at_put(seg_oop, i, roots->at(root_index++));\n+    }\n+\n+    log_info(aot, heap)(\"archived obj root segment [%d] = %zu bytes, obj = \" PTR_FORMAT,\n+                        size_elems, size_bytes, p2i(seg_oop));\n+  }\n+\n+  assert(root_index == roots->length(), \"Post-condition: All roots are handled\");\n+\n+  _heap_root_segments = segments;\n+}\n+\n+\/\/ The goal is to sort the objects in increasing order of:\n+\/\/ - objects that have only oop pointers\n+\/\/ - objects that have both native and oop pointers\n+\/\/ - objects that have only native pointers\n+\/\/ - objects that have no pointers\n+static int oop_sorting_rank(oop o) {\n+  bool has_oop_ptr, has_native_ptr;\n+  HeapShared::get_pointer_info(o, has_oop_ptr, has_native_ptr);\n+\n+  if (has_oop_ptr) {\n+    if (!has_native_ptr) {\n+      return 0;\n+    } else {\n+      return 1;\n+    }\n+  } else {\n+    if (has_native_ptr) {\n+      return 2;\n+    } else {\n+      return 3;\n+    }\n+  }\n+}\n+\n+int AOTMappedHeapWriter::compare_objs_by_oop_fields(HeapObjOrder* a, HeapObjOrder* b) {\n+  int rank_a = a->_rank;\n+  int rank_b = b->_rank;\n+\n+  if (rank_a != rank_b) {\n+    return rank_a - rank_b;\n+  } else {\n+    \/\/ If they are the same rank, sort them by their position in the _source_objs array\n+    return a->_index - b->_index;\n+  }\n+}\n+\n+void AOTMappedHeapWriter::sort_source_objs() {\n+  log_info(aot)(\"sorting heap objects\");\n+  int len = _source_objs->length();\n+  _source_objs_order = new GrowableArrayCHeap<HeapObjOrder, mtClassShared>(len);\n+\n+  for (int i = 0; i < len; i++) {\n+    oop o = _source_objs->at(i);\n+    int rank = oop_sorting_rank(o);\n+    HeapObjOrder os = {i, rank};\n+    _source_objs_order->append(os);\n+  }\n+  log_info(aot)(\"computed ranks\");\n+  _source_objs_order->sort(compare_objs_by_oop_fields);\n+  log_info(aot)(\"sorting heap objects done\");\n+}\n+\n+void AOTMappedHeapWriter::copy_source_objs_to_buffer(GrowableArrayCHeap<oop, mtClassShared>* roots) {\n+  \/\/ There could be multiple root segments, which we want to be aligned by region.\n+  \/\/ Putting them ahead of objects makes sure we waste no space.\n+  copy_roots_to_buffer(roots);\n+\n+  sort_source_objs();\n+  for (int i = 0; i < _source_objs_order->length(); i++) {\n+    int src_obj_index = _source_objs_order->at(i)._index;\n+    oop src_obj = _source_objs->at(src_obj_index);\n+    HeapShared::CachedOopInfo* info = HeapShared::get_cached_oop_info(src_obj);\n+    assert(info != nullptr, \"must be\");\n+    size_t buffer_offset = copy_one_source_obj_to_buffer(src_obj);\n+    info->set_buffer_offset(buffer_offset);\n+\n+    OopHandle handle(Universe::vm_global(), src_obj);\n+    _buffer_offset_to_source_obj_table->put_when_absent(buffer_offset, handle);\n+    _buffer_offset_to_source_obj_table->maybe_grow();\n+\n+    if (java_lang_Module::is_instance(src_obj)) {\n+      Modules::check_archived_module_oop(src_obj);\n+    }\n+  }\n+\n+  log_info(aot)(\"Size of heap region = %zu bytes, %d objects, %d roots, %d native ptrs\",\n+                _buffer_used, _source_objs->length() + 1, roots->length(), _num_native_ptrs);\n+}\n+\n+size_t AOTMappedHeapWriter::filler_array_byte_size(int length) {\n+  size_t byte_size = objArrayOopDesc::object_size(length) * HeapWordSize;\n+  return byte_size;\n+}\n+\n+int AOTMappedHeapWriter::filler_array_length(size_t fill_bytes) {\n+  assert(is_object_aligned(fill_bytes), \"must be\");\n+  size_t elemSize = (UseCompressedOops ? sizeof(narrowOop) : sizeof(oop));\n+\n+  int initial_length = to_array_length(fill_bytes \/ elemSize);\n+  for (int length = initial_length; length >= 0; length --) {\n+    size_t array_byte_size = filler_array_byte_size(length);\n+    if (array_byte_size == fill_bytes) {\n+      return length;\n+    }\n+  }\n+\n+  ShouldNotReachHere();\n+  return -1;\n+}\n+\n+HeapWord* AOTMappedHeapWriter::init_filler_array_at_buffer_top(int array_length, size_t fill_bytes) {\n+  assert(UseCompressedClassPointers, \"Archived heap only supported for compressed klasses\");\n+  Klass* oak = Universe::objectArrayKlass(); \/\/ already relocated to point to archived klass\n+  HeapWord* mem = offset_to_buffered_address<HeapWord*>(_buffer_used);\n+  memset(mem, 0, fill_bytes);\n+  narrowKlass nk = ArchiveBuilder::current()->get_requested_narrow_klass(oak);\n+  if (UseCompactObjectHeaders) {\n+    oopDesc::release_set_mark(mem, markWord::prototype().set_narrow_klass(nk));\n+  } else {\n+    oopDesc::set_mark(mem, markWord::prototype());\n+    cast_to_oop(mem)->set_narrow_klass(nk);\n+  }\n+  arrayOopDesc::set_length(mem, array_length);\n+  return mem;\n+}\n+\n+void AOTMappedHeapWriter::maybe_fill_gc_region_gap(size_t required_byte_size) {\n+  \/\/ We fill only with arrays (so we don't need to use a single HeapWord filler if the\n+  \/\/ leftover space is smaller than a zero-sized array object). Therefore, we need to\n+  \/\/ make sure there's enough space of min_filler_byte_size in the current region after\n+  \/\/ required_byte_size has been allocated. If not, fill the remainder of the current\n+  \/\/ region.\n+  size_t min_filler_byte_size = filler_array_byte_size(0);\n+  size_t new_used = _buffer_used + required_byte_size + min_filler_byte_size;\n+\n+  const size_t cur_min_region_bottom = align_down(_buffer_used, MIN_GC_REGION_ALIGNMENT);\n+  const size_t next_min_region_bottom = align_down(new_used, MIN_GC_REGION_ALIGNMENT);\n+\n+  if (cur_min_region_bottom != next_min_region_bottom) {\n+    \/\/ Make sure that no objects span across MIN_GC_REGION_ALIGNMENT. This way\n+    \/\/ we can map the region in any region-based collector.\n+    assert(next_min_region_bottom > cur_min_region_bottom, \"must be\");\n+    assert(next_min_region_bottom - cur_min_region_bottom == MIN_GC_REGION_ALIGNMENT,\n+           \"no buffered object can be larger than %d bytes\",  MIN_GC_REGION_ALIGNMENT);\n+\n+    const size_t filler_end = next_min_region_bottom;\n+    const size_t fill_bytes = filler_end - _buffer_used;\n+    assert(fill_bytes > 0, \"must be\");\n+    ensure_buffer_space(filler_end);\n+\n+    int array_length = filler_array_length(fill_bytes);\n+    log_info(aot, heap)(\"Inserting filler obj array of %d elements (%zu bytes total) @ buffer offset %zu\",\n+                        array_length, fill_bytes, _buffer_used);\n+    HeapWord* filler = init_filler_array_at_buffer_top(array_length, fill_bytes);\n+    _buffer_used = filler_end;\n+    _fillers->put(buffered_address_to_offset((address)filler), fill_bytes);\n+  }\n+}\n+\n+size_t AOTMappedHeapWriter::get_filler_size_at(address buffered_addr) {\n+  size_t* p = _fillers->get(buffered_address_to_offset(buffered_addr));\n+  if (p != nullptr) {\n+    assert(*p > 0, \"filler must be larger than zero bytes\");\n+    return *p;\n+  } else {\n+    return 0; \/\/ buffered_addr is not a filler\n+  }\n+}\n+\n+template <typename T>\n+void update_buffered_object_field(address buffered_obj, int field_offset, T value) {\n+  T* field_addr = cast_to_oop(buffered_obj)->field_addr<T>(field_offset);\n+  *field_addr = value;\n+}\n+\n+size_t AOTMappedHeapWriter::copy_one_source_obj_to_buffer(oop src_obj) {\n+  assert(!is_too_large_to_archive(src_obj), \"already checked\");\n+  size_t byte_size = src_obj->size() * HeapWordSize;\n+  assert(byte_size > 0, \"no zero-size objects\");\n+\n+  \/\/ For region-based collectors such as G1, the archive heap may be mapped into\n+  \/\/ multiple regions. We need to make sure that we don't have an object that can possible\n+  \/\/ span across two regions.\n+  maybe_fill_gc_region_gap(byte_size);\n+\n+  size_t new_used = _buffer_used + byte_size;\n+  assert(new_used > _buffer_used, \"no wrap around\");\n+\n+  size_t cur_min_region_bottom = align_down(_buffer_used, MIN_GC_REGION_ALIGNMENT);\n+  size_t next_min_region_bottom = align_down(new_used, MIN_GC_REGION_ALIGNMENT);\n+  assert(cur_min_region_bottom == next_min_region_bottom, \"no object should cross minimal GC region boundaries\");\n+\n+  ensure_buffer_space(new_used);\n+\n+  address from = cast_from_oop<address>(src_obj);\n+  address to = offset_to_buffered_address<address>(_buffer_used);\n+  assert(is_object_aligned(_buffer_used), \"sanity\");\n+  assert(is_object_aligned(byte_size), \"sanity\");\n+  memcpy(to, from, byte_size);\n+\n+  \/\/ These native pointers will be restored explicitly at run time.\n+  if (java_lang_Module::is_instance(src_obj)) {\n+    update_buffered_object_field<ModuleEntry*>(to, java_lang_Module::module_entry_offset(), nullptr);\n+  } else if (java_lang_ClassLoader::is_instance(src_obj)) {\n+#ifdef ASSERT\n+    \/\/ We only archive these loaders\n+    if (src_obj != SystemDictionary::java_platform_loader() &&\n+        src_obj != SystemDictionary::java_system_loader()) {\n+      assert(src_obj->klass()->name()->equals(\"jdk\/internal\/loader\/ClassLoaders$BootClassLoader\"), \"must be\");\n+    }\n+#endif\n+    update_buffered_object_field<ClassLoaderData*>(to, java_lang_ClassLoader::loader_data_offset(), nullptr);\n+  }\n+\n+  size_t buffered_obj_offset = _buffer_used;\n+  _buffer_used = new_used;\n+\n+  return buffered_obj_offset;\n+}\n+\n+void AOTMappedHeapWriter::set_requested_address(ArchiveMappedHeapInfo* info) {\n+  assert(!info->is_used(), \"only set once\");\n+\n+  size_t heap_region_byte_size = _buffer_used;\n+  assert(heap_region_byte_size > 0, \"must archived at least one object!\");\n+\n+  if (UseCompressedOops) {\n+    if (UseG1GC) {\n+      address heap_end = (address)G1CollectedHeap::heap()->reserved().end();\n+      log_info(aot, heap)(\"Heap end = %p\", heap_end);\n+      _requested_bottom = align_down(heap_end - heap_region_byte_size, G1HeapRegion::GrainBytes);\n+      _requested_bottom = align_down(_requested_bottom, MIN_GC_REGION_ALIGNMENT);\n+      assert(is_aligned(_requested_bottom, G1HeapRegion::GrainBytes), \"sanity\");\n+    } else {\n+      _requested_bottom = align_up(CompressedOops::begin(), MIN_GC_REGION_ALIGNMENT);\n+    }\n+  } else {\n+    \/\/ We always write the objects as if the heap started at this address. This\n+    \/\/ makes the contents of the archive heap deterministic.\n+    \/\/\n+    \/\/ Note that at runtime, the heap address is selected by the OS, so the archive\n+    \/\/ heap will not be mapped at 0x10000000, and the contents need to be patched.\n+    _requested_bottom = align_up((address)NOCOOPS_REQUESTED_BASE, MIN_GC_REGION_ALIGNMENT);\n+  }\n+\n+  assert(is_aligned(_requested_bottom, MIN_GC_REGION_ALIGNMENT), \"sanity\");\n+\n+  _requested_top = _requested_bottom + _buffer_used;\n+\n+  info->set_buffer_region(MemRegion(offset_to_buffered_address<HeapWord*>(0),\n+                                    offset_to_buffered_address<HeapWord*>(_buffer_used)));\n+  info->set_root_segments(_heap_root_segments);\n+}\n+\n+\/\/ Oop relocation\n+\n+template <typename T> T* AOTMappedHeapWriter::requested_addr_to_buffered_addr(T* p) {\n+  assert(is_in_requested_range(cast_to_oop(p)), \"must be\");\n+\n+  address addr = address(p);\n+  assert(addr >= _requested_bottom, \"must be\");\n+  size_t offset = addr - _requested_bottom;\n+  return offset_to_buffered_address<T*>(offset);\n+}\n+\n+template <typename T> oop AOTMappedHeapWriter::load_source_oop_from_buffer(T* buffered_addr) {\n+  oop o = load_oop_from_buffer(buffered_addr);\n+  assert(!in_buffer(cast_from_oop<address>(o)), \"must point to source oop\");\n+  return o;\n+}\n+\n+template <typename T> void AOTMappedHeapWriter::store_requested_oop_in_buffer(T* buffered_addr,\n+                                                                                   oop request_oop) {\n+  assert(request_oop == nullptr || is_in_requested_range(request_oop), \"must be\");\n+  store_oop_in_buffer(buffered_addr, request_oop);\n+}\n+\n+inline void AOTMappedHeapWriter::store_oop_in_buffer(oop* buffered_addr, oop requested_obj) {\n+  *buffered_addr = requested_obj;\n+}\n+\n+inline void AOTMappedHeapWriter::store_oop_in_buffer(narrowOop* buffered_addr, oop requested_obj) {\n+  narrowOop val = CompressedOops::encode(requested_obj);\n+  *buffered_addr = val;\n+}\n+\n+oop AOTMappedHeapWriter::load_oop_from_buffer(oop* buffered_addr) {\n+  return *buffered_addr;\n+}\n+\n+oop AOTMappedHeapWriter::load_oop_from_buffer(narrowOop* buffered_addr) {\n+  return CompressedOops::decode(*buffered_addr);\n+}\n+\n+template <typename T> void AOTMappedHeapWriter::relocate_field_in_buffer(T* field_addr_in_buffer, oop source_referent, CHeapBitMap* oopmap) {\n+  oop request_referent = source_obj_to_requested_obj(source_referent);\n+  store_requested_oop_in_buffer<T>(field_addr_in_buffer, request_referent);\n+  if (request_referent != nullptr) {\n+    mark_oop_pointer<T>(field_addr_in_buffer, oopmap);\n+  }\n+}\n+\n+template <typename T> void AOTMappedHeapWriter::mark_oop_pointer(T* buffered_addr, CHeapBitMap* oopmap) {\n+  T* request_p = (T*)(buffered_addr_to_requested_addr((address)buffered_addr));\n+  address requested_region_bottom;\n+\n+  assert(request_p >= (T*)_requested_bottom, \"sanity\");\n+  assert(request_p <  (T*)_requested_top, \"sanity\");\n+  requested_region_bottom = _requested_bottom;\n+\n+  \/\/ Mark the pointer in the oopmap\n+  T* region_bottom = (T*)requested_region_bottom;\n+  assert(request_p >= region_bottom, \"must be\");\n+  BitMap::idx_t idx = request_p - region_bottom;\n+  assert(idx < oopmap->size(), \"overflow\");\n+  oopmap->set_bit(idx);\n+}\n+\n+void AOTMappedHeapWriter::update_header_for_requested_obj(oop requested_obj, oop src_obj,  Klass* src_klass) {\n+  assert(UseCompressedClassPointers, \"Archived heap only supported for compressed klasses\");\n+  narrowKlass nk = ArchiveBuilder::current()->get_requested_narrow_klass(src_klass);\n+  address buffered_addr = requested_addr_to_buffered_addr(cast_from_oop<address>(requested_obj));\n+\n+  oop fake_oop = cast_to_oop(buffered_addr);\n+  if (UseCompactObjectHeaders) {\n+    fake_oop->set_mark(markWord::prototype().set_narrow_klass(nk));\n+  } else {\n+    fake_oop->set_narrow_klass(nk);\n+  }\n+\n+  if (src_obj == nullptr) {\n+    return;\n+  }\n+  \/\/ We need to retain the identity_hash, because it may have been used by some hashtables\n+  \/\/ in the shared heap.\n+  if (!src_obj->fast_no_hash_check()) {\n+    intptr_t src_hash = src_obj->identity_hash();\n+    if (UseCompactObjectHeaders) {\n+      fake_oop->set_mark(markWord::prototype().set_narrow_klass(nk).copy_set_hash(src_hash));\n+    } else {\n+      fake_oop->set_mark(markWord::prototype().copy_set_hash(src_hash));\n+    }\n+    assert(fake_oop->mark().is_unlocked(), \"sanity\");\n+\n+    DEBUG_ONLY(intptr_t archived_hash = fake_oop->identity_hash());\n+    assert(src_hash == archived_hash, \"Different hash codes: original \" INTPTR_FORMAT \", archived \" INTPTR_FORMAT, src_hash, archived_hash);\n+  }\n+  \/\/ Strip age bits.\n+  fake_oop->set_mark(fake_oop->mark().set_age(0));\n+}\n+\n+class AOTMappedHeapWriter::EmbeddedOopRelocator: public BasicOopIterateClosure {\n+  oop _src_obj;\n+  address _buffered_obj;\n+  CHeapBitMap* _oopmap;\n+  bool _is_java_lang_ref;\n+public:\n+  EmbeddedOopRelocator(oop src_obj, address buffered_obj, CHeapBitMap* oopmap) :\n+    _src_obj(src_obj), _buffered_obj(buffered_obj), _oopmap(oopmap)\n+  {\n+    _is_java_lang_ref = AOTReferenceObjSupport::check_if_ref_obj(src_obj);\n+  }\n+\n+  void do_oop(narrowOop *p) { EmbeddedOopRelocator::do_oop_work(p); }\n+  void do_oop(      oop *p) { EmbeddedOopRelocator::do_oop_work(p); }\n+\n+private:\n+  template <class T> void do_oop_work(T *p) {\n+    int field_offset = pointer_delta_as_int((char*)p, cast_from_oop<char*>(_src_obj));\n+    T* field_addr = (T*)(_buffered_obj + field_offset);\n+    oop referent = load_source_oop_from_buffer<T>(field_addr);\n+    referent = HeapShared::maybe_remap_referent(_is_java_lang_ref, field_offset, referent);\n+    AOTMappedHeapWriter::relocate_field_in_buffer<T>(field_addr, referent, _oopmap);\n+  }\n+};\n+\n+static void log_bitmap_usage(const char* which, BitMap* bitmap, size_t total_bits) {\n+  \/\/ The whole heap is covered by total_bits, but there are only non-zero bits within [start ... end).\n+  size_t start = bitmap->find_first_set_bit(0);\n+  size_t end = bitmap->size();\n+  log_info(aot)(\"%s = %7zu ... %7zu (%3zu%% ... %3zu%% = %3zu%%)\", which,\n+                start, end,\n+                start * 100 \/ total_bits,\n+                end * 100 \/ total_bits,\n+                (end - start) * 100 \/ total_bits);\n+}\n+\n+\/\/ Update all oop fields embedded in the buffered objects\n+void AOTMappedHeapWriter::relocate_embedded_oops(GrowableArrayCHeap<oop, mtClassShared>* roots,\n+                                                      ArchiveMappedHeapInfo* heap_info) {\n+  size_t oopmap_unit = (UseCompressedOops ? sizeof(narrowOop) : sizeof(oop));\n+  size_t heap_region_byte_size = _buffer_used;\n+  heap_info->oopmap()->resize(heap_region_byte_size   \/ oopmap_unit);\n+\n+  for (int i = 0; i < _source_objs_order->length(); i++) {\n+    int src_obj_index = _source_objs_order->at(i)._index;\n+    oop src_obj = _source_objs->at(src_obj_index);\n+    HeapShared::CachedOopInfo* info = HeapShared::get_cached_oop_info(src_obj);\n+    assert(info != nullptr, \"must be\");\n+    oop requested_obj = requested_obj_from_buffer_offset(info->buffer_offset());\n+    update_header_for_requested_obj(requested_obj, src_obj, src_obj->klass());\n+    address buffered_obj = offset_to_buffered_address<address>(info->buffer_offset());\n+    EmbeddedOopRelocator relocator(src_obj, buffered_obj, heap_info->oopmap());\n+    src_obj->oop_iterate(&relocator);\n+    mark_native_pointers(src_obj);\n+  };\n+\n+  \/\/ Relocate HeapShared::roots(), which is created in copy_roots_to_buffer() and\n+  \/\/ doesn't have a corresponding src_obj, so we can't use EmbeddedOopRelocator on it.\n+  for (size_t seg_idx = 0; seg_idx < _heap_root_segments.count(); seg_idx++) {\n+    size_t seg_offset = _heap_root_segments.segment_offset(seg_idx);\n+\n+    objArrayOop requested_obj = (objArrayOop)requested_obj_from_buffer_offset(seg_offset);\n+    update_header_for_requested_obj(requested_obj, nullptr, Universe::objectArrayKlass());\n+    address buffered_obj = offset_to_buffered_address<address>(seg_offset);\n+    int length = _heap_root_segments.size_in_elems(seg_idx);\n+\n+    size_t elem_size = UseCompressedOops ? sizeof(narrowOop) : sizeof(oop);\n+\n+    for (int i = 0; i < length; i++) {\n+      \/\/ There is no source object; these are native oops - load, translate and\n+      \/\/ write back\n+      size_t elem_offset = objArrayOopDesc::base_offset_in_bytes() + elem_size * i;\n+      HeapWord* elem_addr = (HeapWord*)(buffered_obj + elem_offset);\n+      oop obj = NativeAccess<>::oop_load(elem_addr);\n+      obj = HeapShared::maybe_remap_referent(false \/* is_reference_field *\/, elem_offset, obj);\n+      if (UseCompressedOops) {\n+        relocate_field_in_buffer<narrowOop>((narrowOop*)elem_addr, obj, heap_info->oopmap());\n+      } else {\n+        relocate_field_in_buffer<oop>((oop*)elem_addr, obj, heap_info->oopmap());\n+      }\n+    }\n+  }\n+\n+  compute_ptrmap(heap_info);\n+\n+  size_t total_bytes = (size_t)_buffer->length();\n+  log_bitmap_usage(\"oopmap\", heap_info->oopmap(), total_bytes \/ (UseCompressedOops ? sizeof(narrowOop) : sizeof(oop)));\n+  log_bitmap_usage(\"ptrmap\", heap_info->ptrmap(), total_bytes \/ sizeof(address));\n+}\n+\n+void AOTMappedHeapWriter::mark_native_pointer(oop src_obj, int field_offset) {\n+  Metadata* ptr = src_obj->metadata_field_acquire(field_offset);\n+  if (ptr != nullptr) {\n+    NativePointerInfo info;\n+    info._src_obj = src_obj;\n+    info._field_offset = field_offset;\n+    _native_pointers->append(info);\n+    HeapShared::set_has_native_pointers(src_obj);\n+    _num_native_ptrs ++;\n+  }\n+}\n+\n+void AOTMappedHeapWriter::mark_native_pointers(oop orig_obj) {\n+  HeapShared::do_metadata_offsets(orig_obj, [&](int offset) {\n+    mark_native_pointer(orig_obj, offset);\n+  });\n+}\n+\n+void AOTMappedHeapWriter::compute_ptrmap(ArchiveMappedHeapInfo* heap_info) {\n+  int num_non_null_ptrs = 0;\n+  Metadata** bottom = (Metadata**) _requested_bottom;\n+  Metadata** top = (Metadata**) _requested_top; \/\/ exclusive\n+  heap_info->ptrmap()->resize(top - bottom);\n+\n+  BitMap::idx_t max_idx = 32; \/\/ paranoid - don't make it too small\n+  for (int i = 0; i < _native_pointers->length(); i++) {\n+    NativePointerInfo info = _native_pointers->at(i);\n+    oop src_obj = info._src_obj;\n+    int field_offset = info._field_offset;\n+    HeapShared::CachedOopInfo* p = HeapShared::get_cached_oop_info(src_obj);\n+    \/\/ requested_field_addr = the address of this field in the requested space\n+    oop requested_obj = requested_obj_from_buffer_offset(p->buffer_offset());\n+    Metadata** requested_field_addr = (Metadata**)(cast_from_oop<address>(requested_obj) + field_offset);\n+    assert(bottom <= requested_field_addr && requested_field_addr < top, \"range check\");\n+\n+    \/\/ Mark this field in the bitmap\n+    BitMap::idx_t idx = requested_field_addr - bottom;\n+    heap_info->ptrmap()->set_bit(idx);\n+    num_non_null_ptrs ++;\n+    max_idx = MAX2(max_idx, idx);\n+\n+    \/\/ Set the native pointer to the requested address of the metadata (at runtime, the metadata will have\n+    \/\/ this address if the RO\/RW regions are mapped at the default location).\n+\n+    Metadata** buffered_field_addr = requested_addr_to_buffered_addr(requested_field_addr);\n+    Metadata* native_ptr = *buffered_field_addr;\n+    guarantee(native_ptr != nullptr, \"sanity\");\n+\n+    if (RegeneratedClasses::has_been_regenerated(native_ptr)) {\n+      native_ptr = RegeneratedClasses::get_regenerated_object(native_ptr);\n+    }\n+\n+    guarantee(ArchiveBuilder::current()->has_been_archived((address)native_ptr),\n+              \"Metadata %p should have been archived\", native_ptr);\n+\n+    address buffered_native_ptr = ArchiveBuilder::current()->get_buffered_addr((address)native_ptr);\n+    address requested_native_ptr = ArchiveBuilder::current()->to_requested(buffered_native_ptr);\n+    *buffered_field_addr = (Metadata*)requested_native_ptr;\n+  }\n+\n+  heap_info->ptrmap()->resize(max_idx + 1);\n+  log_info(aot, heap)(\"calculate_ptrmap: marked %d non-null native pointers for heap region (%zu bits)\",\n+                      num_non_null_ptrs, size_t(heap_info->ptrmap()->size()));\n+}\n+\n+AOTMapLogger::OopDataIterator* AOTMappedHeapWriter::oop_iterator(ArchiveMappedHeapInfo* heap_info) {\n+  class MappedWriterOopIterator : public AOTMapLogger::OopDataIterator {\n+  private:\n+    address _current;\n+    address _next;\n+\n+    address _buffer_start;\n+    address _buffer_end;\n+    uint64_t _buffer_start_narrow_oop;\n+    intptr_t _buffer_to_requested_delta;\n+    int _requested_shift;\n+\n+    size_t _num_root_segments;\n+    size_t _num_obj_arrays_logged;\n+\n+  public:\n+    MappedWriterOopIterator(address buffer_start,\n+                            address buffer_end,\n+                            uint64_t buffer_start_narrow_oop,\n+                            intptr_t buffer_to_requested_delta,\n+                            int requested_shift,\n+                            size_t num_root_segments)\n+      : _current(nullptr),\n+        _next(buffer_start),\n+        _buffer_start(buffer_start),\n+        _buffer_end(buffer_end),\n+        _buffer_start_narrow_oop(buffer_start_narrow_oop),\n+        _buffer_to_requested_delta(buffer_to_requested_delta),\n+        _requested_shift(requested_shift),\n+        _num_root_segments(num_root_segments),\n+        _num_obj_arrays_logged(0) {\n+    }\n+\n+    AOTMapLogger::OopData capture(address buffered_addr) {\n+      oopDesc* raw_oop = (oopDesc*)buffered_addr;\n+      size_t size = size_of_buffered_oop(buffered_addr);\n+      address requested_addr = buffered_addr_to_requested_addr(buffered_addr);\n+      intptr_t target_location = (intptr_t)requested_addr;\n+      uint64_t pd = (uint64_t)(pointer_delta(buffered_addr, _buffer_start, 1));\n+      uint32_t narrow_location = checked_cast<uint32_t>(_buffer_start_narrow_oop + (pd >> _requested_shift));\n+      Klass* klass = real_klass_of_buffered_oop(buffered_addr);\n+\n+      return { buffered_addr,\n+               requested_addr,\n+               target_location,\n+               narrow_location,\n+               raw_oop,\n+               klass,\n+               size,\n+               false };\n+    }\n+\n+    bool has_next() override {\n+      return _next < _buffer_end;\n+    }\n+\n+    AOTMapLogger::OopData next() override {\n+      _current = _next;\n+      AOTMapLogger::OopData result = capture(_current);\n+      if (result._klass->is_objArray_klass()) {\n+        result._is_root_segment = _num_obj_arrays_logged++ < _num_root_segments;\n+      }\n+      _next = _current + result._size * BytesPerWord;\n+      return result;\n+    }\n+\n+    AOTMapLogger::OopData obj_at(narrowOop* addr) override {\n+      uint64_t n = (uint64_t)(*addr);\n+      if (n == 0) {\n+        return null_data();\n+      } else {\n+        precond(n >= _buffer_start_narrow_oop);\n+        address buffer_addr = _buffer_start + ((n - _buffer_start_narrow_oop) << _requested_shift);\n+        return capture(buffer_addr);\n+      }\n+    }\n+\n+    AOTMapLogger::OopData obj_at(oop* addr) override {\n+      address requested_value = cast_from_oop<address>(*addr);\n+      if (requested_value == nullptr) {\n+        return null_data();\n+      } else {\n+        address buffer_addr = requested_value - _buffer_to_requested_delta;\n+        return capture(buffer_addr);\n+      }\n+    }\n+\n+    GrowableArrayCHeap<AOTMapLogger::OopData, mtClass>* roots() override {\n+      return new GrowableArrayCHeap<AOTMapLogger::OopData, mtClass>();\n+    }\n+  };\n+\n+  MemRegion r = heap_info->buffer_region();\n+  address buffer_start = address(r.start());\n+  address buffer_end = address(r.end());\n+\n+  address requested_base = UseCompressedOops ? (address)CompressedOops::base() : (address)AOTMappedHeapWriter::NOCOOPS_REQUESTED_BASE;\n+  address requested_start = UseCompressedOops ? buffered_addr_to_requested_addr(buffer_start) : requested_base;\n+  int requested_shift =  CompressedOops::shift();\n+  intptr_t buffer_to_requested_delta = requested_start - buffer_start;\n+  uint64_t buffer_start_narrow_oop = 0xdeadbeed;\n+  if (UseCompressedOops) {\n+    buffer_start_narrow_oop = (uint64_t)(pointer_delta(requested_start, requested_base, 1)) >> requested_shift;\n+    assert(buffer_start_narrow_oop < 0xffffffff, \"sanity\");\n+  }\n+\n+  return new MappedWriterOopIterator(buffer_start,\n+                                     buffer_end,\n+                                     buffer_start_narrow_oop,\n+                                     buffer_to_requested_delta,\n+                                     requested_shift,\n+                                     heap_info->root_segments().count());\n+}\n+\n+#endif \/\/ INCLUDE_CDS_JAVA_HEAP\n","filename":"src\/hotspot\/share\/cds\/aotMappedHeapWriter.cpp","additions":939,"deletions":0,"binary":false,"changes":939,"status":"added"},{"patch":"@@ -0,0 +1,250 @@\n+\/*\n+ * Copyright (c) 2024, 2025, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef SHARE_CDS_AOTMAPPEDHEAPWRITER_HPP\n+#define SHARE_CDS_AOTMAPPEDHEAPWRITER_HPP\n+\n+#include \"cds\/aotMapLogger.hpp\"\n+#include \"cds\/heapShared.hpp\"\n+#include \"memory\/allocation.hpp\"\n+#include \"memory\/allStatic.hpp\"\n+#include \"oops\/oopHandle.hpp\"\n+#include \"utilities\/bitMap.hpp\"\n+#include \"utilities\/exceptions.hpp\"\n+#include \"utilities\/growableArray.hpp\"\n+#include \"utilities\/hashTable.hpp\"\n+#include \"utilities\/macros.hpp\"\n+\n+class MemRegion;\n+\n+#if INCLUDE_CDS_JAVA_HEAP\n+class DumpedInternedStrings :\n+  public ResizeableHashTable<oop, bool,\n+                           AnyObj::C_HEAP,\n+                           mtClassShared,\n+                           HeapShared::string_oop_hash>\n+{\n+public:\n+  DumpedInternedStrings(unsigned size, unsigned max_size) :\n+    ResizeableHashTable<oop, bool,\n+                                AnyObj::C_HEAP,\n+                                mtClassShared,\n+                                HeapShared::string_oop_hash>(size, max_size) {}\n+};\n+\n+class AOTMappedHeapWriter : AllStatic {\n+  friend class HeapShared;\n+  friend class AOTMappedHeapLoader;\n+  \/\/ AOTMappedHeapWriter manipulates three types of addresses:\n+  \/\/\n+  \/\/     \"source\" vs \"buffered\" vs \"requested\"\n+  \/\/\n+  \/\/ (Note: the design and convention is the same as for the archiving of Metaspace objects.\n+  \/\/  See archiveBuilder.hpp.)\n+  \/\/\n+  \/\/ - \"source objects\" are regular Java objects allocated during the execution\n+  \/\/   of \"java -Xshare:dump\". They can be used as regular oops.\n+  \/\/\n+  \/\/   Between HeapShared::start_scanning_for_oops() and HeapShared::end_scanning_for_oops(),\n+  \/\/   we recursively search for the oops that need to be stored into the CDS archive.\n+  \/\/   These are entered into HeapShared::archived_object_cache().\n+  \/\/\n+  \/\/ - \"buffered objects\" are copies of the \"source objects\", and are stored in into\n+  \/\/   ArchiveHeapWriter::_buffer, which is a GrowableArray that sits outside of\n+  \/\/   the valid heap range. Therefore we avoid using the addresses of these copies\n+  \/\/   as oops. They are usually called \"buffered_addr\" in the code (of the type \"address\").\n+  \/\/\n+  \/\/   The buffered objects are stored contiguously, possibly with interleaving fillers\n+  \/\/   to make sure no objects span across boundaries of MIN_GC_REGION_ALIGNMENT.\n+  \/\/\n+  \/\/ - Each archived object has a \"requested address\" -- at run time, if the object\n+  \/\/   can be mapped at this address, we can avoid relocation.\n+  \/\/\n+  \/\/ The requested address is implemented differently depending on UseCompressedOops:\n+  \/\/\n+  \/\/ UseCompressedOops == true:\n+  \/\/   The archived objects are stored assuming that the runtime COOPS compression\n+  \/\/   scheme is exactly the same as in dump time (or else a more expensive runtime relocation\n+  \/\/   would be needed.)\n+  \/\/\n+  \/\/   At dump time, we assume that the runtime heap range is exactly the same as\n+  \/\/   in dump time. The requested addresses of the archived objects are chosen such that\n+  \/\/   they would occupy the top end of a G1 heap (TBD when dumping is supported by other\n+  \/\/   collectors. See JDK-8298614).\n+  \/\/\n+  \/\/ UseCompressedOops == false:\n+  \/\/   At runtime, the heap range is usually picked (randomly) by the OS, so we will almost always\n+  \/\/   need to perform relocation. Hence, the goal of the \"requested address\" is to ensure that\n+  \/\/   the contents of the archived objects are deterministic. I.e., the oop fields of archived\n+  \/\/   objects will always point to deterministic addresses.\n+  \/\/\n+  \/\/   For G1, the archived heap is written such that the lowest archived object is placed\n+  \/\/   at NOCOOPS_REQUESTED_BASE. (TBD after JDK-8298614).\n+  \/\/ ----------------------------------------------------------------------\n+\n+public:\n+  static const intptr_t NOCOOPS_REQUESTED_BASE = 0x10000000;\n+\n+  \/\/ The minimum region size of all collectors that are supported by CDS.\n+  \/\/ G1 heap region size can never be smaller than 1M.\n+  \/\/ Shenandoah heap region size can never be smaller than 256K.\n+  static constexpr int MIN_GC_REGION_ALIGNMENT = 256 * K;\n+\n+  static const int INITIAL_TABLE_SIZE = 15889; \/\/ prime number\n+  static const int MAX_TABLE_SIZE     = 1000000;\n+\n+private:\n+  class EmbeddedOopRelocator;\n+  struct NativePointerInfo {\n+    oop _src_obj;\n+    int _field_offset;\n+  };\n+\n+  static GrowableArrayCHeap<u1, mtClassShared>* _buffer;\n+\n+  \/\/ The number of bytes that have written into _buffer (may be smaller than _buffer->length()).\n+  static size_t _buffer_used;\n+\n+  \/\/ The heap root segments information.\n+  static HeapRootSegments _heap_root_segments;\n+\n+  \/\/ The address range of the requested location of the archived heap objects.\n+  static address _requested_bottom;\n+  static address _requested_top;\n+\n+  static GrowableArrayCHeap<NativePointerInfo, mtClassShared>* _native_pointers;\n+  static GrowableArrayCHeap<oop, mtClassShared>* _source_objs;\n+  static DumpedInternedStrings *_dumped_interned_strings;\n+\n+  \/\/ We sort _source_objs_order to minimize the number of bits in ptrmap and oopmap.\n+  \/\/ See comments near the body of ArchiveHeapWriter::compare_objs_by_oop_fields().\n+  \/\/ The objects will be written in the order of:\n+  \/\/_source_objs->at(_source_objs_order->at(0)._index)\n+  \/\/ source_objs->at(_source_objs_order->at(1)._index)\n+  \/\/ source_objs->at(_source_objs_order->at(2)._index)\n+  \/\/ ...\n+  struct HeapObjOrder {\n+    int _index;    \/\/ The location of this object in _source_objs\n+    int _rank;     \/\/ A lower rank means the object will be written at a lower location.\n+  };\n+  static GrowableArrayCHeap<HeapObjOrder, mtClassShared>* _source_objs_order;\n+\n+  typedef ResizeableHashTable<size_t, OopHandle,\n+      AnyObj::C_HEAP,\n+      mtClassShared> BufferOffsetToSourceObjectTable;\n+  static BufferOffsetToSourceObjectTable* _buffer_offset_to_source_obj_table;\n+\n+  static void allocate_buffer();\n+  static void ensure_buffer_space(size_t min_bytes);\n+\n+  \/\/ Both Java bytearray and GrowableArraty use int indices and lengths. Do a safe typecast with range check\n+  static int to_array_index(size_t i) {\n+    assert(i <= (size_t)max_jint, \"must be\");\n+    return (int)i;\n+  }\n+  static int to_array_length(size_t n) {\n+    return to_array_index(n);\n+  }\n+\n+  template <typename T> static T offset_to_buffered_address(size_t offset) {\n+    return (T)(_buffer->adr_at(to_array_index(offset)));\n+  }\n+\n+  static address buffer_bottom() {\n+    return offset_to_buffered_address<address>(0);\n+  }\n+\n+  \/\/ The exclusive end of the last object that was copied into the buffer.\n+  static address buffer_top() {\n+    return buffer_bottom() + _buffer_used;\n+  }\n+\n+  static bool in_buffer(address buffered_addr) {\n+    return (buffer_bottom() <= buffered_addr) && (buffered_addr < buffer_top());\n+  }\n+\n+  static size_t buffered_address_to_offset(address buffered_addr) {\n+    assert(in_buffer(buffered_addr), \"sanity\");\n+    return buffered_addr - buffer_bottom();\n+  }\n+\n+  static void root_segment_at_put(objArrayOop segment, int index, oop root);\n+  static objArrayOop allocate_root_segment(size_t offset, int element_count);\n+  static void copy_roots_to_buffer(GrowableArrayCHeap<oop, mtClassShared>* roots);\n+  static void copy_source_objs_to_buffer(GrowableArrayCHeap<oop, mtClassShared>* roots);\n+  static size_t copy_one_source_obj_to_buffer(oop src_obj);\n+\n+  static void maybe_fill_gc_region_gap(size_t required_byte_size);\n+  static size_t filler_array_byte_size(int length);\n+  static int filler_array_length(size_t fill_bytes);\n+  static HeapWord* init_filler_array_at_buffer_top(int array_length, size_t fill_bytes);\n+\n+  static void set_requested_address(ArchiveMappedHeapInfo* info);\n+  static void mark_native_pointers(oop orig_obj);\n+  static void relocate_embedded_oops(GrowableArrayCHeap<oop, mtClassShared>* roots, ArchiveMappedHeapInfo* info);\n+  static void compute_ptrmap(ArchiveMappedHeapInfo *info);\n+  static bool is_in_requested_range(oop o);\n+  static oop requested_obj_from_buffer_offset(size_t offset);\n+\n+  static oop load_oop_from_buffer(oop* buffered_addr);\n+  static oop load_oop_from_buffer(narrowOop* buffered_addr);\n+  inline static void store_oop_in_buffer(oop* buffered_addr, oop requested_obj);\n+  inline static void store_oop_in_buffer(narrowOop* buffered_addr, oop requested_obj);\n+\n+  template <typename T> static oop load_source_oop_from_buffer(T* buffered_addr);\n+  template <typename T> static void store_requested_oop_in_buffer(T* buffered_addr, oop request_oop);\n+\n+  template <typename T> static T* requested_addr_to_buffered_addr(T* p);\n+  template <typename T> static void relocate_field_in_buffer(T* field_addr_in_buffer, oop source_referent, CHeapBitMap* oopmap);\n+  template <typename T> static void mark_oop_pointer(T* buffered_addr, CHeapBitMap* oopmap);\n+\n+  static void update_header_for_requested_obj(oop requested_obj, oop src_obj, Klass* src_klass);\n+\n+  static int compare_objs_by_oop_fields(HeapObjOrder* a, HeapObjOrder* b);\n+  static void sort_source_objs();\n+\n+public:\n+  static void init() NOT_CDS_JAVA_HEAP_RETURN;\n+  static void delete_tables_with_raw_oops();\n+  static void add_source_obj(oop src_obj);\n+  static bool is_too_large_to_archive(size_t size);\n+  static bool is_too_large_to_archive(oop obj);\n+  static bool is_string_too_large_to_archive(oop string);\n+  static bool is_dumped_interned_string(oop o);\n+  static void add_to_dumped_interned_strings(oop string);\n+  static void write(GrowableArrayCHeap<oop, mtClassShared>*, ArchiveMappedHeapInfo* heap_info);\n+  static address requested_address();  \/\/ requested address of the lowest achived heap object\n+  static size_t get_filler_size_at(address buffered_addr);\n+\n+  static void mark_native_pointer(oop src_obj, int offset);\n+  static oop source_obj_to_requested_obj(oop src_obj);\n+  static oop buffered_addr_to_source_obj(address buffered_addr);\n+  static address buffered_addr_to_requested_addr(address buffered_addr);\n+  static Klass* real_klass_of_buffered_oop(address buffered_addr);\n+  static size_t size_of_buffered_oop(address buffered_addr);\n+\n+  static AOTMapLogger::OopDataIterator* oop_iterator(ArchiveMappedHeapInfo* heap_info);\n+};\n+#endif \/\/ INCLUDE_CDS_JAVA_HEAP\n+#endif \/\/ SHARE_CDS_AOTMAPPEDHEAPWRITER_HPP\n","filename":"src\/hotspot\/share\/cds\/aotMappedHeapWriter.hpp","additions":250,"deletions":0,"binary":false,"changes":250,"status":"added"},{"patch":"@@ -33,0 +33,1 @@\n+#include \"cds\/aotMappedHeapLoader.hpp\"\n@@ -36,2 +37,0 @@\n-#include \"cds\/archiveHeapLoader.hpp\"\n-#include \"cds\/archiveHeapWriter.hpp\"\n@@ -48,1 +47,1 @@\n-#include \"cds\/heapShared.hpp\"\n+#include \"cds\/heapShared.inline.hpp\"\n@@ -340,1 +339,4 @@\n-    static_mapinfo->unmap_region(AOTMetaspace::bm);\n+\n+    if (HeapShared::is_loading() && HeapShared::is_loading_mapping_mode()) {\n+      static_mapinfo->unmap_region(AOTMetaspace::bm);\n+    }\n@@ -396,1 +398,1 @@\n-        if (ArchiveHeapWriter::is_string_too_large_to_archive(str)) {\n+        if (HeapShared::is_string_too_large_to_archive(str)) {\n@@ -558,1 +560,2 @@\n-  ArchiveHeapInfo _heap_info;\n+  ArchiveMappedHeapInfo _mapped_heap_info;\n+  ArchiveStreamedHeapInfo _streamed_heap_info;\n@@ -573,1 +576,1 @@\n-    VM_Operation(), _heap_info(), _map_info(nullptr), _builder(b) {}\n+    VM_Operation(), _mapped_heap_info(), _streamed_heap_info(), _map_info(nullptr), _builder(b) {}\n@@ -578,1 +581,2 @@\n-  ArchiveHeapInfo* heap_info()  { return &_heap_info; }\n+  ArchiveMappedHeapInfo* mapped_heap_info()  { return &_mapped_heap_info; }\n+  ArchiveStreamedHeapInfo* streamed_heap_info()  { return &_streamed_heap_info; }\n@@ -1020,2 +1024,1 @@\n-    ArchiveHeapWriter::init();\n-\n+    HeapShared::init_heap_writer();\n@@ -1044,3 +1047,5 @@\n-    \/\/ Do this at the very end, when no Java code will be executed. Otherwise\n-    \/\/ some new strings may be added to the intern table.\n-    StringTable::allocate_shared_strings_array(CHECK);\n+    if (HeapShared::is_writing_mapping_mode()) {\n+      \/\/ Do this at the very end, when no Java code will be executed. Otherwise\n+      \/\/ some new strings may be added to the intern table.\n+      StringTable::allocate_shared_strings_array(CHECK);\n+    }\n@@ -1067,1 +1072,1 @@\n-  bool status = write_static_archive(&builder, op.map_info(), op.heap_info());\n+  bool status = write_static_archive(&builder, op.map_info(), op.mapped_heap_info(), op.streamed_heap_info());\n@@ -1081,1 +1086,4 @@\n-bool AOTMetaspace::write_static_archive(ArchiveBuilder* builder, FileMapInfo* map_info, ArchiveHeapInfo* heap_info) {\n+bool AOTMetaspace::write_static_archive(ArchiveBuilder* builder,\n+                                        FileMapInfo* map_info,\n+                                        ArchiveMappedHeapInfo* mapped_heap_info,\n+                                        ArchiveStreamedHeapInfo* streamed_heap_info) {\n@@ -1090,1 +1098,1 @@\n-  builder->write_archive(map_info, heap_info);\n+  builder->write_archive(map_info, mapped_heap_info, streamed_heap_info);\n@@ -1264,1 +1272,1 @@\n-    HeapShared::write_heap(&_heap_info);\n+    HeapShared::write_heap(&_mapped_heap_info, &_streamed_heap_info);\n@@ -1666,3 +1674,23 @@\n-      \/\/ map_or_load_heap_region() compares the current narrow oop and klass encodings\n-      \/\/ with the archived ones, so it must be done after all encodings are determined.\n-      static_mapinfo->map_or_load_heap_region();\n+      if (static_mapinfo->can_use_heap_region()) {\n+        if (static_mapinfo->object_streaming_mode()) {\n+          HeapShared::initialize_loading_mode(HeapArchiveMode::_streaming);\n+        } else {\n+          \/\/ map_or_load_heap_region() compares the current narrow oop and klass encodings\n+          \/\/ with the archived ones, so it must be done after all encodings are determined.\n+          static_mapinfo->map_or_load_heap_region();\n+          HeapShared::initialize_loading_mode(HeapArchiveMode::_mapping);\n+        }\n+      } else {\n+        FileMapRegion* r = static_mapinfo->region_at(AOTMetaspace::hp);\n+        if (r->used() > 0) {\n+          if (static_mapinfo->object_streaming_mode()) {\n+            AOTMetaspace::report_loading_error(\"Cannot use CDS heap data.\");\n+          } else {\n+            if (!UseCompressedOops && !AOTMappedHeapLoader::can_map()) {\n+              AOTMetaspace::report_loading_error(\"Cannot use CDS heap data. Selected GC not compatible -XX:-UseCompressedOops\");\n+            } else {\n+              AOTMetaspace::report_loading_error(\"Cannot use CDS heap data. UseEpsilonGC, UseG1GC, UseSerialGC, UseParallelGC, or UseShenandoahGC are required.\");\n+            }\n+          }\n+        }\n+      }\n@@ -2001,4 +2029,3 @@\n-  \/\/ Finish up archived heap initialization. These must be\n-  \/\/ done after ReadClosure.\n-  static_mapinfo->patch_heap_embedded_pointers();\n-  ArchiveHeapLoader::finish_initialization();\n+  \/\/ Finish initializing the heap dump mode used in the archive\n+  \/\/ Heap initialization can be done only after vtables are initialized by ReadClosure.\n+  HeapShared::finalize_initialization(static_mapinfo);\n@@ -2006,0 +2033,1 @@\n+\n@@ -2058,1 +2086,3 @@\n-    tty->print_cr(\"Number of shared strings: %zu\", StringTable::shared_entry_count());\n+    if (HeapShared::is_loading_mapping_mode()) {\n+      tty->print_cr(\"Number of shared strings: %zu\", StringTable::shared_entry_count());\n+    }\n","filename":"src\/hotspot\/share\/cds\/aotMetaspace.cpp","additions":55,"deletions":25,"binary":false,"changes":80,"status":"modified"},{"patch":"@@ -36,1 +36,2 @@\n-class ArchiveHeapInfo;\n+class ArchiveMappedHeapInfo;\n+class ArchiveStreamedHeapInfo;\n@@ -187,1 +188,4 @@\n-  static bool write_static_archive(ArchiveBuilder* builder, FileMapInfo* map_info, ArchiveHeapInfo* heap_info);\n+  static bool write_static_archive(ArchiveBuilder* builder,\n+                                   FileMapInfo* map_info,\n+                                   ArchiveMappedHeapInfo* mapped_heap_info,\n+                                   ArchiveStreamedHeapInfo* streamed_heap_info);\n","filename":"src\/hotspot\/share\/cds\/aotMetaspace.hpp","additions":6,"deletions":2,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -156,0 +156,3 @@\n+\n+    \/\/ Trigger a GC to prune eligible referents that were not kept alive\n+    Universe::heap()->collect(GCCause::_java_lang_system_gc);\n","filename":"src\/hotspot\/share\/cds\/aotReferenceObjSupport.cpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -0,0 +1,1183 @@\n+\/*\n+ * Copyright (c) 2018, 2025, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#include \"cds\/aotMetaspace.hpp\"\n+#include \"cds\/aotStreamedHeapLoader.hpp\"\n+#include \"cds\/aotThread.hpp\"\n+#include \"cds\/cdsConfig.hpp\"\n+#include \"cds\/filemap.hpp\"\n+#include \"cds\/heapShared.inline.hpp\"\n+#include \"classfile\/classLoaderDataShared.hpp\"\n+#include \"classfile\/javaClasses.inline.hpp\"\n+#include \"classfile\/stringTable.hpp\"\n+#include \"classfile\/vmClasses.hpp\"\n+#include \"gc\/shared\/collectedHeap.inline.hpp\"\n+#include \"gc\/shared\/oopStorage.inline.hpp\"\n+#include \"gc\/shared\/oopStorageSet.inline.hpp\"\n+#include \"logging\/log.hpp\"\n+#include \"memory\/iterator.inline.hpp\"\n+#include \"memory\/oopFactory.hpp\"\n+#include \"oops\/access.inline.hpp\"\n+#include \"oops\/objArrayOop.inline.hpp\"\n+#include \"oops\/oop.inline.hpp\"\n+#include \"runtime\/globals.hpp\"\n+#include \"runtime\/globals_extension.hpp\"\n+#include \"runtime\/handles.inline.hpp\"\n+#include \"runtime\/java.hpp\"\n+#include \"runtime\/mutex.hpp\"\n+#include \"runtime\/thread.hpp\"\n+#include \"utilities\/bitMap.inline.hpp\"\n+#include \"utilities\/exceptions.hpp\"\n+#include \"utilities\/globalDefinitions.hpp\"\n+#include \"utilities\/stack.inline.hpp\"\n+#include \"utilities\/ticks.hpp\"\n+\n+#include <type_traits>\n+\n+#if INCLUDE_CDS_JAVA_HEAP\n+\n+FileMapRegion* AOTStreamedHeapLoader::_heap_region;\n+FileMapRegion* AOTStreamedHeapLoader::_bitmap_region;\n+int* AOTStreamedHeapLoader::_roots_archive;\n+OopHandle AOTStreamedHeapLoader::_roots;\n+BitMapView AOTStreamedHeapLoader::_oopmap;\n+bool AOTStreamedHeapLoader::_is_in_use;\n+int AOTStreamedHeapLoader::_previous_batch_last_object_index;\n+int AOTStreamedHeapLoader::_current_batch_last_object_index;\n+int AOTStreamedHeapLoader::_current_root_index;\n+size_t AOTStreamedHeapLoader::_allocated_words;\n+bool AOTStreamedHeapLoader::_allow_gc;\n+bool AOTStreamedHeapLoader::_objects_are_handles;\n+size_t AOTStreamedHeapLoader::_num_archived_objects;\n+int AOTStreamedHeapLoader::_num_roots;\n+size_t AOTStreamedHeapLoader::_heap_region_used;\n+\n+size_t* AOTStreamedHeapLoader::_object_index_to_buffer_offset_table;\n+void** AOTStreamedHeapLoader::_object_index_to_heap_object_table;\n+int* AOTStreamedHeapLoader::_root_highest_object_index_table;\n+\n+bool AOTStreamedHeapLoader::_waiting_for_iterator;\n+bool AOTStreamedHeapLoader::_swapping_root_format;\n+\n+static uint64_t _early_materialization_time_ns = 0;\n+static uint64_t _late_materialization_time_ns = 0;\n+static uint64_t _final_materialization_time_ns = 0;\n+static uint64_t _cleanup_materialization_time_ns = 0;\n+static volatile uint64_t _accumulated_lazy_materialization_time_ns = 0;\n+static Ticks _materialization_start_ticks;\n+\n+int AOTStreamedHeapLoader::object_index_for_root_index(int root_index) {\n+  return _roots_archive[root_index];\n+}\n+\n+int AOTStreamedHeapLoader::highest_object_index_for_root_index(int root_index) {\n+  return _root_highest_object_index_table[root_index];\n+}\n+\n+size_t AOTStreamedHeapLoader::buffer_offset_for_object_index(int object_index) {\n+  return _object_index_to_buffer_offset_table[object_index];\n+}\n+\n+oopDesc* AOTStreamedHeapLoader::archive_object_for_object_index(int object_index) {\n+  size_t buffer_offset = buffer_offset_for_object_index(object_index);\n+  address bottom = (address)_heap_region->mapped_base();\n+  return (oopDesc*)(bottom + buffer_offset);\n+}\n+\n+size_t AOTStreamedHeapLoader::buffer_offset_for_archive_object(oopDesc* archive_object) {\n+  address bottom = (address)_heap_region->mapped_base();\n+  return size_t(archive_object) - size_t(bottom);\n+}\n+\n+template <bool use_coops>\n+BitMap::idx_t AOTStreamedHeapLoader::obj_bit_idx_for_buffer_offset(size_t buffer_offset) {\n+  if constexpr (use_coops) {\n+    return BitMap::idx_t(buffer_offset \/ sizeof(narrowOop));\n+  } else {\n+    return BitMap::idx_t(buffer_offset \/ sizeof(HeapWord));\n+  }\n+}\n+\n+oop AOTStreamedHeapLoader::heap_object_for_object_index(int object_index) {\n+  assert(object_index >= 0 && object_index <= (int)_num_archived_objects,\n+         \"Heap object reference out of index: %d\", object_index);\n+\n+  if (_objects_are_handles) {\n+    oop* handle = (oop*)_object_index_to_heap_object_table[object_index];\n+    if (handle == nullptr) {\n+      return nullptr;\n+    }\n+    return NativeAccess<>::oop_load(handle);\n+  } else {\n+    return cast_to_oop(_object_index_to_heap_object_table[object_index]);\n+  }\n+}\n+\n+void AOTStreamedHeapLoader::set_heap_object_for_object_index(int object_index, oop heap_object) {\n+  assert(heap_object_for_object_index(object_index) == nullptr, \"Should only set once with this API\");\n+  if (_objects_are_handles) {\n+    oop* handle = Universe::vm_global()->allocate();\n+    NativeAccess<>::oop_store(handle, heap_object);\n+    _object_index_to_heap_object_table[object_index] = (void*)handle;\n+  } else {\n+    _object_index_to_heap_object_table[object_index] = cast_from_oop<void*>(heap_object);\n+  }\n+}\n+\n+int AOTStreamedHeapLoader::archived_string_value_object_index(oopDesc* archive_object) {\n+    assert(archive_object->klass() == vmClasses::String_klass(), \"Must be an archived string\");\n+    address archive_string_value_addr = (address)archive_object + java_lang_String::value_offset();\n+    return UseCompressedOops ? *(int*)archive_string_value_addr : (int)*(int64_t*)archive_string_value_addr;\n+}\n+\n+static int archive_array_length(oopDesc* archive_array) {\n+  return *(int*)(address(archive_array) + arrayOopDesc::length_offset_in_bytes());\n+}\n+\n+static size_t archive_object_size(oopDesc* archive_object) {\n+  Klass* klass = archive_object->klass();\n+  int lh = klass->layout_helper();\n+\n+  if (Klass::layout_helper_is_instance(lh)) {\n+    \/\/ Instance\n+    if (Klass::layout_helper_needs_slow_path(lh)) {\n+      return ((size_t*)(archive_object))[-1];\n+    } else {\n+      return (size_t)Klass::layout_helper_size_in_bytes(lh) >> LogHeapWordSize;\n+    }\n+  } else if (Klass::layout_helper_is_array(lh)) {\n+    \/\/ Array\n+    size_t size_in_bytes;\n+    size_t array_length = (size_t)archive_array_length(archive_object);\n+    size_in_bytes = array_length << Klass::layout_helper_log2_element_size(lh);\n+    size_in_bytes += (size_t)Klass::layout_helper_header_size(lh);\n+\n+    return align_up(size_in_bytes, (size_t)MinObjAlignmentInBytes) \/ HeapWordSize;\n+  } else {\n+    \/\/ Other\n+    return ((size_t*)(archive_object))[-1];\n+  }\n+}\n+\n+oop AOTStreamedHeapLoader::allocate_object(oopDesc* archive_object, markWord mark, size_t size, TRAPS) {\n+  assert(!archive_object->is_stackChunk(), \"no such objects are archived\");\n+\n+  oop heap_object;\n+\n+  Klass* klass = archive_object->klass();\n+  if (klass->is_mirror_instance_klass()) {\n+    heap_object = Universe::heap()->class_allocate(klass, size, CHECK_NULL);\n+  } else if (klass->is_instance_klass()) {\n+    heap_object = Universe::heap()->obj_allocate(klass, size, CHECK_NULL);\n+  } else {\n+    assert(klass->is_array_klass(), \"must be\");\n+    int length = archive_array_length(archive_object);\n+    bool do_zero = klass->is_objArray_klass();\n+    heap_object = Universe::heap()->array_allocate(klass, size, length, do_zero, CHECK_NULL);\n+  }\n+\n+  heap_object->set_mark(mark);\n+\n+  return heap_object;\n+}\n+\n+void AOTStreamedHeapLoader::install_root(int root_index, oop heap_object) {\n+  objArrayOop roots = objArrayOop(_roots.resolve());\n+  OrderAccess::release(); \/\/ Once the store below publishes an object, it can be concurrently picked up by another thread without using the lock\n+  roots->obj_at_put(root_index, heap_object);\n+}\n+\n+void AOTStreamedHeapLoader::TracingObjectLoader::wait_for_iterator() {\n+  if (JavaThread::current()->is_active_Java_thread()) {\n+    \/\/ When the main thread has bootstrapped past the point of allowing safepoints,\n+    \/\/ we can and indeed have to use safepoint checking waiting.\n+    AOTHeapLoading_lock->wait();\n+  } else {\n+    \/\/ If we have no bootstrapped the main thread far enough, then we cannot and\n+    \/\/ indeed also don't need to perform safepoint checking waiting.\n+    AOTHeapLoading_lock->wait_without_safepoint_check();\n+  }\n+}\n+\n+\/\/ Link object after copying in-place\n+template <typename LinkerT>\n+class AOTStreamedHeapLoader::InPlaceLinkingOopClosure : public BasicOopIterateClosure {\n+private:\n+  oop _obj;\n+  LinkerT _linker;\n+\n+public:\n+  InPlaceLinkingOopClosure(oop obj, LinkerT linker)\n+    : _obj(obj),\n+      _linker(linker) {\n+  }\n+\n+  virtual void do_oop(oop* p) { do_oop_work(p, (int)*(intptr_t*)p); }\n+  virtual void do_oop(narrowOop* p) { do_oop_work(p, *(int*)p); }\n+\n+  template <typename T>\n+  void do_oop_work(T* p, int object_index) {\n+    int p_offset = pointer_delta_as_int((address)p, cast_from_oop<address>(_obj));\n+    oop pointee = _linker(p_offset, object_index);\n+    if (pointee != nullptr) {\n+      _obj->obj_field_put_access<IS_DEST_UNINITIALIZED>((int)p_offset, pointee);\n+    }\n+  }\n+};\n+\n+template <bool use_coops, typename LinkerT>\n+void AOTStreamedHeapLoader::copy_payload_carefully(oopDesc* archive_object,\n+                                                   oop heap_object,\n+                                                   BitMap::idx_t header_bit,\n+                                                   BitMap::idx_t start_bit,\n+                                                   BitMap::idx_t end_bit,\n+                                                   LinkerT linker) {\n+  using RawElementT = std::conditional_t<use_coops, int32_t, int64_t>;\n+  using OopElementT = std::conditional_t<use_coops, narrowOop, oop>;\n+\n+  BitMap::idx_t unfinished_bit = start_bit;\n+  BitMap::idx_t next_reference_bit = _oopmap.find_first_set_bit(unfinished_bit, end_bit);\n+\n+  \/\/ Fill in heap object bytes\n+  while (unfinished_bit < end_bit) {\n+    assert(unfinished_bit >= start_bit && unfinished_bit < end_bit, \"out of bounds copying\");\n+\n+    \/\/ This is the address of the pointee inside the input stream\n+    size_t payload_offset = unfinished_bit - header_bit;\n+    RawElementT* archive_payload_addr = ((RawElementT*)archive_object) + payload_offset;\n+    RawElementT* heap_payload_addr = cast_from_oop<RawElementT*>(heap_object) + payload_offset;\n+\n+    assert(heap_payload_addr >= cast_from_oop<RawElementT*>(heap_object) &&\n+           (HeapWord*)heap_payload_addr < cast_from_oop<HeapWord*>(heap_object) + heap_object->size(),\n+           \"Out of bounds copying\");\n+\n+    if (next_reference_bit > unfinished_bit) {\n+      \/\/ Primitive bytes available\n+      size_t primitive_elements = next_reference_bit - unfinished_bit;\n+      size_t primitive_bytes = primitive_elements * sizeof(RawElementT);\n+      ::memcpy(heap_payload_addr, archive_payload_addr, primitive_bytes);\n+\n+      unfinished_bit = next_reference_bit;\n+    } else {\n+      \/\/ Encountered reference\n+      RawElementT* archive_p = (RawElementT*)archive_payload_addr;\n+      OopElementT* heap_p = (OopElementT*)heap_payload_addr;\n+      int pointee_object_index = (int)*archive_p;\n+      int heap_p_offset = pointer_delta_as_int((address)heap_p, cast_from_oop<address>(heap_object));\n+\n+      \/\/ The object index is retrieved from the archive, not the heap object. This is\n+      \/\/ important after GC is enabled. Concurrent GC threads may scan references in the\n+      \/\/ heap for various reasons after this point. Therefore, it is not okay to first copy\n+      \/\/ the object index from a reference location in the archived object payload to a\n+      \/\/ corresponding location in the heap object payload, and then fix it up afterwards to\n+      \/\/ refer to a heap object. This is why this code iterates carefully over object references\n+      \/\/ in the archived object, linking them one by one, without clobbering the reference\n+      \/\/ locations in the heap objects with anything other than transitions from null to the\n+      \/\/ intended linked object.\n+      oop obj = linker(heap_p_offset, pointee_object_index);\n+      if (obj != nullptr) {\n+        heap_object->obj_field_put(heap_p_offset, obj);\n+      }\n+\n+      unfinished_bit++;\n+      next_reference_bit = _oopmap.find_first_set_bit(unfinished_bit, end_bit);\n+    }\n+  }\n+}\n+\n+template <bool use_coops, typename LinkerT>\n+void AOTStreamedHeapLoader::copy_object_impl(oopDesc* archive_object,\n+                                             oop heap_object,\n+                                             size_t size,\n+                                             LinkerT linker) {\n+  if (!_allow_gc) {\n+    \/\/ Without concurrent GC running, we can copy incorrect object references\n+    \/\/ and metadata references into the heap object and then fix them up in-place.\n+    size_t payload_size = size - 1;\n+    HeapWord* archive_start = ((HeapWord*)archive_object) + 1;\n+    HeapWord* heap_start = cast_from_oop<HeapWord*>(heap_object) + 1;\n+\n+    Copy::disjoint_words(archive_start, heap_start, payload_size);\n+\n+    \/\/ In-place linking fixes up object indices from references of the heap object,\n+    \/\/ and patches them up to refer to objects. This can be done because we just copied\n+    \/\/ the payload of the object from the archive to the heap object, including the\n+    \/\/ reference object indices. However, this is only okay to do before the GC can run.\n+    \/\/ A concurrent GC thread might racingly read the object payload after GC is enabled.\n+    InPlaceLinkingOopClosure cl(heap_object, linker);\n+    heap_object->oop_iterate(&cl);\n+    HeapShared::remap_loaded_metadata(heap_object);\n+    return;\n+  }\n+\n+  \/\/ When a concurrent GC may be running, we take care not to copy incorrect oops,\n+  \/\/ narrowOops or Metadata* into the heap objects. Transitions go from 0 to the\n+  \/\/ intended runtime linked values only.\n+  size_t word_scale = use_coops ? 2 : 1;\n+  using RawElementT = std::conditional_t<use_coops, int32_t, int64_t>;\n+\n+  \/\/ Skip the markWord; it is set at allocation time\n+  size_t header_size = word_scale;\n+\n+  size_t buffer_offset = buffer_offset_for_archive_object(archive_object);\n+  const BitMap::idx_t header_bit = obj_bit_idx_for_buffer_offset<use_coops>(buffer_offset);\n+  const BitMap::idx_t start_bit = header_bit + header_size;\n+  const BitMap::idx_t end_bit = header_bit + size * word_scale;\n+\n+  BitMap::idx_t curr_bit = start_bit;\n+\n+  \/\/ We are a bit paranoid about GC or other safepointing operations observing\n+  \/\/ shady metadata fields from the archive that do not point at real metadata.\n+  \/\/ We deal with this by explicitly reading the requested address from the\n+  \/\/ archive and fixing it to real Metadata before writing it into the heap object.\n+  HeapShared::do_metadata_offsets(heap_object, [&](int metadata_offset) {\n+    BitMap::idx_t metadata_field_idx = header_bit + (size_t)metadata_offset \/ sizeof(RawElementT);\n+    BitMap::idx_t skip = word_scale;\n+    assert(metadata_field_idx >= start_bit && metadata_field_idx + skip <= end_bit,\n+           \"Metadata field out of bounds\");\n+\n+    \/\/ Copy payload before metadata field\n+    copy_payload_carefully<use_coops>(archive_object,\n+                                      heap_object,\n+                                      header_bit,\n+                                      curr_bit,\n+                                      metadata_field_idx,\n+                                      linker);\n+\n+    \/\/ Copy metadata field\n+    Metadata* const archive_metadata = *(Metadata**)(uintptr_t(archive_object) + (size_t)metadata_offset);\n+    Metadata* const runtime_metadata = archive_metadata != nullptr\n+        ? (Metadata*)(address(archive_metadata) + AOTMetaspace::relocation_delta())\n+        : nullptr;\n+    assert(runtime_metadata == nullptr || AOTMetaspace::in_aot_cache(runtime_metadata), \"Invalid metadata pointer\");\n+    DEBUG_ONLY(Metadata* const previous_metadata = heap_object->metadata_field(metadata_offset);)\n+    assert(previous_metadata == nullptr || previous_metadata == runtime_metadata, \"Should not observe transient values\");\n+    heap_object->metadata_field_put(metadata_offset, runtime_metadata);\n+    curr_bit = metadata_field_idx + skip;\n+  });\n+\n+  \/\/ Copy trailing metadata after the last metadata word. This is usually doing\n+  \/\/ all the copying.\n+  copy_payload_carefully<use_coops>(archive_object,\n+                                    heap_object,\n+                                    header_bit,\n+                                    curr_bit,\n+                                    end_bit,\n+                                    linker);\n+}\n+\n+void AOTStreamedHeapLoader::copy_object_eager_linking(oopDesc* archive_object, oop heap_object, size_t size) {\n+  auto linker = [&](int p_offset, int pointee_object_index) {\n+    oop obj = AOTStreamedHeapLoader::heap_object_for_object_index(pointee_object_index);\n+    assert(pointee_object_index == 0 || obj != nullptr, \"Eager object loading should only encounter already allocated links\");\n+    return obj;\n+  };\n+  if (UseCompressedOops) {\n+    copy_object_impl<true>(archive_object, heap_object, size, linker);\n+  } else {\n+    copy_object_impl<false>(archive_object, heap_object, size, linker);\n+  }\n+}\n+\n+void AOTStreamedHeapLoader::TracingObjectLoader::copy_object_lazy_linking(int object_index,\n+                                                                          oopDesc* archive_object,\n+                                                                          oop heap_object,\n+                                                                          size_t size,\n+                                                                          Stack<AOTHeapTraversalEntry, mtClassShared>& dfs_stack) {\n+  auto linker = [&](int p_offset, int pointee_object_index) {\n+    dfs_stack.push({pointee_object_index, object_index, p_offset});\n+\n+    \/\/ The tracing linker is a bit lazy and mutates the reference fields in its traversal.\n+    \/\/ Returning null means don't link now.\n+    return oop(nullptr);\n+  };\n+  if (UseCompressedOops) {\n+    copy_object_impl<true>(archive_object, heap_object, size, linker);\n+  } else {\n+    copy_object_impl<false>(archive_object, heap_object, size, linker);\n+  }\n+}\n+\n+oop AOTStreamedHeapLoader::TracingObjectLoader::materialize_object_inner(int object_index, Stack<AOTHeapTraversalEntry, mtClassShared>& dfs_stack, TRAPS) {\n+  \/\/ Allocate object\n+  oopDesc* archive_object = archive_object_for_object_index(object_index);\n+  size_t size = archive_object_size(archive_object);\n+  markWord mark = archive_object->mark();\n+\n+  \/\/ The markWord is marked if the object is a String and it should be interned,\n+  \/\/ make sure to unmark it before allocating memory for the object.\n+  bool string_intern = mark.is_marked();\n+  mark = mark.set_unmarked();\n+\n+  oop heap_object;\n+\n+  if (string_intern) {\n+    int value_object_index = archived_string_value_object_index(archive_object);\n+\n+    \/\/ Materialize the value object.\n+    (void)materialize_object(value_object_index, dfs_stack, CHECK_NULL);\n+\n+    \/\/ Allocate and link the string.\n+    heap_object = allocate_object(archive_object, mark, size, CHECK_NULL);\n+    copy_object_eager_linking(archive_object, heap_object, size);\n+\n+    assert(java_lang_String::value(heap_object) == heap_object_for_object_index(value_object_index), \"Linker should have linked this correctly\");\n+\n+    \/\/ Replace the string with interned string\n+    heap_object = StringTable::intern(heap_object, CHECK_NULL);\n+  } else {\n+    heap_object = allocate_object(archive_object, mark, size, CHECK_NULL);\n+\n+    \/\/ Fill in object contents\n+    copy_object_lazy_linking(object_index, archive_object, heap_object, size, dfs_stack);\n+  }\n+\n+  \/\/ Install forwarding\n+  set_heap_object_for_object_index(object_index, heap_object);\n+\n+  return heap_object;\n+}\n+\n+oop AOTStreamedHeapLoader::TracingObjectLoader::materialize_object(int object_index, Stack<AOTHeapTraversalEntry, mtClassShared>& dfs_stack, TRAPS) {\n+  if (object_index <= _previous_batch_last_object_index) {\n+    \/\/ The transitive closure of this object has been materialized; no need to do anything\n+    return heap_object_for_object_index(object_index);\n+  }\n+\n+  if (object_index <= _current_batch_last_object_index) {\n+    \/\/ The AOTThread is currently materializing this object and its transitive closure; only need to wait for it to complete\n+    _waiting_for_iterator = true;\n+    while (object_index > _previous_batch_last_object_index) {\n+      wait_for_iterator();\n+    }\n+    _waiting_for_iterator = false;\n+\n+    \/\/ Notify the AOT thread if it is waiting for tracing to finish\n+    AOTHeapLoading_lock->notify_all();\n+    return heap_object_for_object_index(object_index);;\n+  }\n+\n+  oop heap_object = heap_object_for_object_index(object_index);\n+  if (heap_object != nullptr) {\n+    \/\/ Already materialized by mutator\n+    return heap_object;\n+  }\n+\n+  return materialize_object_inner(object_index, dfs_stack, THREAD);\n+}\n+\n+void AOTStreamedHeapLoader::TracingObjectLoader::drain_stack(Stack<AOTHeapTraversalEntry, mtClassShared>& dfs_stack, TRAPS) {\n+  while (!dfs_stack.is_empty()) {\n+    AOTHeapTraversalEntry entry = dfs_stack.pop();\n+    int pointee_object_index = entry._pointee_object_index;\n+    oop pointee_heap_object = materialize_object(pointee_object_index, dfs_stack, CHECK);\n+    oop heap_object = heap_object_for_object_index(entry._base_object_index);\n+    if (_allow_gc) {\n+      heap_object->obj_field_put(entry._heap_field_offset_bytes, pointee_heap_object);\n+    } else {\n+      heap_object->obj_field_put_access<IS_DEST_UNINITIALIZED>(entry._heap_field_offset_bytes, pointee_heap_object);\n+    }\n+  }\n+}\n+\n+oop AOTStreamedHeapLoader::TracingObjectLoader::materialize_object_transitive(int object_index, Stack<AOTHeapTraversalEntry, mtClassShared>& dfs_stack, TRAPS) {\n+  assert_locked_or_safepoint(AOTHeapLoading_lock);\n+  while (_waiting_for_iterator) {\n+    wait_for_iterator();\n+  }\n+\n+  auto handlized_materialize_object = [&](TRAPS) {\n+    oop obj = materialize_object(object_index, dfs_stack, CHECK_(Handle()));\n+    return Handle(THREAD, obj);\n+  };\n+\n+  Handle result = handlized_materialize_object(CHECK_NULL);\n+  drain_stack(dfs_stack, CHECK_NULL);\n+\n+  return result();\n+}\n+\n+oop AOTStreamedHeapLoader::TracingObjectLoader::materialize_root(int root_index, Stack<AOTHeapTraversalEntry, mtClassShared>& dfs_stack, TRAPS) {\n+  int root_object_index = object_index_for_root_index(root_index);\n+  oop root = materialize_object_transitive(root_object_index, dfs_stack, CHECK_NULL);\n+  install_root(root_index, root);\n+\n+  return root;\n+}\n+\n+int oop_handle_cmp(const void* left, const void* right) {\n+  oop* left_handle = *(oop**)left;\n+  oop* right_handle = *(oop**)right;\n+\n+  if (right_handle > left_handle) {\n+    return -1;\n+  } else if (left_handle > right_handle) {\n+    return 1;\n+  }\n+\n+  return 0;\n+}\n+\n+\/\/ The range is inclusive\n+void AOTStreamedHeapLoader::IterativeObjectLoader::initialize_range(int first_object_index, int last_object_index, TRAPS) {\n+  for (int i = first_object_index; i <= last_object_index; ++i) {\n+    oopDesc* archive_object = archive_object_for_object_index(i);\n+    markWord mark = archive_object->mark();\n+    bool string_intern = mark.is_marked();\n+    if (string_intern) {\n+      int value_object_index = archived_string_value_object_index(archive_object);\n+      if (value_object_index == i + 1) {\n+        \/\/ Interned strings are eagerly materialized in the allocation phase, so there is\n+        \/\/ nothing else to do for interned strings here for the string nor its value array.\n+        i++;\n+      }\n+      continue;\n+    }\n+    size_t size = archive_object_size(archive_object);\n+    oop heap_object = heap_object_for_object_index(i);\n+    copy_object_eager_linking(archive_object, heap_object, size);\n+  }\n+}\n+\n+\/\/ The range is inclusive\n+size_t AOTStreamedHeapLoader::IterativeObjectLoader::materialize_range(int first_object_index, int last_object_index, TRAPS) {\n+  GrowableArrayCHeap<int, mtClassShared> lazy_object_indices(0);\n+  size_t materialized_words = 0;\n+\n+  for (int i = first_object_index; i <= last_object_index; ++i) {\n+    oopDesc* archive_object = archive_object_for_object_index(i);\n+    markWord mark = archive_object->mark();\n+\n+    \/\/ The markWord is marked if the object is a String and it should be interned,\n+    \/\/ make sure to unmark it before allocating memory for the object.\n+    bool string_intern = mark.is_marked();\n+    mark = mark.set_unmarked();\n+\n+    size_t size = archive_object_size(archive_object);\n+    materialized_words += size;\n+    oop heap_object = heap_object_for_object_index(i);\n+    if (heap_object != nullptr) {\n+      \/\/ Lazy loading has already initialized the object; we must not mutate it\n+      lazy_object_indices.append(i);\n+      continue;\n+    }\n+\n+    if (!string_intern) {\n+     \/\/ The normal case; no lazy loading have loaded the object yet\n+      heap_object = allocate_object(archive_object, mark, size, CHECK_0);\n+      set_heap_object_for_object_index(i, heap_object);\n+      continue;\n+    }\n+\n+    \/\/ Eagerly materialize interned strings to ensure that objects earlier than the string\n+    \/\/ in a batch get linked to the intended interned string, and not a copy.\n+    int value_object_index = archived_string_value_object_index(archive_object);\n+\n+    bool is_normal_interned_string = value_object_index == i + 1;\n+\n+    if (value_object_index < first_object_index) {\n+      \/\/ If materialized in a previous batch, the value should already be allocated and initialized.\n+      assert(heap_object_for_object_index(value_object_index) != nullptr, \"should be materialized\");\n+    } else {\n+      \/\/ Materialize the value object.\n+      oopDesc* archive_value_object = archive_object_for_object_index(value_object_index);\n+      markWord value_mark = archive_value_object->mark();\n+      size_t value_size = archive_object_size(archive_value_object);\n+      oop value_heap_object;\n+\n+      if (is_normal_interned_string) {\n+        \/\/ The common case: the value is next to the string. This happens when only the interned\n+        \/\/ string points to its value character array.\n+        assert(value_object_index <= last_object_index, \"Must be within this batch: %d <= %d\", value_object_index, last_object_index);\n+        value_heap_object = allocate_object(archive_value_object, value_mark, value_size, CHECK_0);\n+        set_heap_object_for_object_index(value_object_index, value_heap_object);\n+      } else {\n+        \/\/ In the uncommon case, multiple strings point to the value of an interned string.\n+        \/\/ The string can then be earlier in the batch.\n+        assert(value_object_index < i, \"surprising index\");\n+        value_heap_object = heap_object_for_object_index(value_object_index);\n+      }\n+\n+      copy_object_eager_linking(archive_value_object, value_heap_object, value_size);\n+    }\n+    \/\/ Allocate and link the string.\n+    heap_object = allocate_object(archive_object, mark, size, CHECK_0);\n+    copy_object_eager_linking(archive_object, heap_object, size);\n+\n+    assert(java_lang_String::value(heap_object) == heap_object_for_object_index(value_object_index), \"Linker should have linked this correctly\");\n+\n+    \/\/ Replace the string with interned string\n+    heap_object = StringTable::intern(heap_object, CHECK_0);\n+    set_heap_object_for_object_index(i, heap_object);\n+\n+    if (is_normal_interned_string) {\n+      \/\/ Skip over the object value, already materialized\n+      i++;\n+    }\n+  }\n+\n+  if (lazy_object_indices.is_empty()) {\n+    \/\/ Normal case; no sprinkled lazy objects in the root subgraph\n+    initialize_range(first_object_index, last_object_index, CHECK_0);\n+  } else {\n+    \/\/ The user lazy initialized some objects that are already initialized; we have to initialize around them\n+    \/\/ to make sure they are not mutated.\n+    int previous_object_index = first_object_index - 1; \/\/ Exclusive start of initialization slice\n+    for (int i = 0; i < lazy_object_indices.length(); ++i) {\n+      int lazy_object_index = lazy_object_indices.at(i);\n+      int slice_start_object_index = previous_object_index;\n+      int slice_end_object_index = lazy_object_index;\n+\n+      if (slice_end_object_index - slice_start_object_index > 1) { \/\/ Both markers are exclusive\n+        initialize_range(slice_start_object_index + 1, slice_end_object_index - 1, CHECK_0);\n+      }\n+      previous_object_index = lazy_object_index;\n+    }\n+    \/\/ Process tail range\n+    if (last_object_index - previous_object_index > 0) {\n+      initialize_range(previous_object_index + 1, last_object_index, CHECK_0);\n+    }\n+  }\n+\n+  return materialized_words;\n+}\n+\n+bool AOTStreamedHeapLoader::IterativeObjectLoader::has_more() {\n+  return _current_root_index < _num_roots;\n+}\n+\n+void AOTStreamedHeapLoader::IterativeObjectLoader::materialize_next_batch(TRAPS) {\n+  assert(has_more(), \"only materialize if there is something to materialize\");\n+\n+  int min_batch_objects = 128;\n+  int from_root_index = _current_root_index;\n+  int max_to_root_index = _num_roots - 1;\n+  int until_root_index = from_root_index;\n+  int highest_object_index;\n+\n+  \/\/ Expand the batch size from one root, to N roots until we cross 128 objects in total\n+  for (;;) {\n+    highest_object_index = highest_object_index_for_root_index(until_root_index);\n+    if (highest_object_index - _previous_batch_last_object_index >= min_batch_objects) {\n+      break;\n+    }\n+    if (until_root_index == max_to_root_index) {\n+      break;\n+    }\n+    until_root_index++;\n+  }\n+\n+  oop root = nullptr;\n+\n+  \/\/ Materialize objects of necessary, representing the transitive closure of the root\n+  if (highest_object_index > _previous_batch_last_object_index) {\n+    while (_swapping_root_format) {\n+      \/\/ When the roots are being upgraded to use handles, it is not safe to racingly\n+      \/\/ iterate over the object; we must wait. Setting the current batch last object index\n+      \/\/ to something other than the previous batch last object index indicates to the\n+      \/\/ root swapping that there is current iteration ongoing.\n+      AOTHeapLoading_lock->wait();\n+    }\n+    int first_object_index = _previous_batch_last_object_index + 1;\n+    _current_batch_last_object_index = highest_object_index;\n+    size_t allocated_words;\n+    {\n+      MutexUnlocker ml(AOTHeapLoading_lock, Mutex::_safepoint_check_flag);\n+      allocated_words = materialize_range(first_object_index, highest_object_index, CHECK);\n+    }\n+    _allocated_words += allocated_words;\n+    _previous_batch_last_object_index = _current_batch_last_object_index;\n+    if (_waiting_for_iterator) {\n+      \/\/ If tracer is waiting, let it know at the next point of unlocking that the root\n+      \/\/ set it waited for has been processed now.\n+      AOTHeapLoading_lock->notify_all();\n+    }\n+  }\n+\n+  \/\/ Install the root\n+  for (int i = from_root_index; i <= until_root_index; ++i) {\n+    int root_object_index = object_index_for_root_index(i);\n+    root = heap_object_for_object_index(root_object_index);\n+    install_root(i, root);\n+    ++_current_root_index;\n+  }\n+}\n+\n+bool AOTStreamedHeapLoader::materialize_early(TRAPS) {\n+  Ticks start = Ticks::now();\n+\n+  \/\/ Only help with early materialization from the AOT thread if the heap archive can be allocated\n+  \/\/ without the need for a GC. Otherwise, do lazy loading until GC is enabled later in the bootstrapping.\n+  size_t bootstrap_max_memory = Universe::heap()->bootstrap_max_memory();\n+  size_t bootstrap_min_memory = MAX2(_heap_region_used, 2 * M);\n+\n+  size_t before_gc_materialize_budget_bytes = (bootstrap_max_memory > bootstrap_min_memory) ? bootstrap_max_memory - bootstrap_min_memory : 0;\n+  size_t before_gc_materialize_budget_words = before_gc_materialize_budget_bytes \/ HeapWordSize;\n+\n+  log_info(aot, heap)(\"Max bootstrapping memory: %zuM, min bootstrapping memory: %zuM, selected budget: %zuM\",\n+                      bootstrap_max_memory \/ M, bootstrap_min_memory \/ M, before_gc_materialize_budget_bytes \/ M);\n+\n+  while (IterativeObjectLoader::has_more()) {\n+    if (_allow_gc || _allocated_words > before_gc_materialize_budget_words) {\n+      log_info(aot, heap)(\"Early object materialization interrupted at root %d\", _current_root_index);\n+      break;\n+    }\n+\n+    IterativeObjectLoader::materialize_next_batch(CHECK_false);\n+  }\n+\n+  _early_materialization_time_ns = (Ticks::now() - start).nanoseconds();\n+\n+  bool finished_before_gc_allowed = !_allow_gc && !IterativeObjectLoader::has_more();\n+\n+  return finished_before_gc_allowed;\n+}\n+\n+void AOTStreamedHeapLoader::materialize_late(TRAPS) {\n+  Ticks start = Ticks::now();\n+\n+  \/\/ Continue materializing with GC allowed\n+\n+  while (IterativeObjectLoader::has_more()) {\n+    IterativeObjectLoader::materialize_next_batch(CHECK);\n+  }\n+\n+  _late_materialization_time_ns = (Ticks::now() - start).nanoseconds();\n+}\n+\n+void AOTStreamedHeapLoader::cleanup() {\n+  \/\/ First ensure there is no concurrent tracing going on\n+  while (_waiting_for_iterator) {\n+    AOTHeapLoading_lock->wait();\n+  }\n+\n+  Ticks start = Ticks::now();\n+\n+  \/\/ Remove OopStorage roots\n+  if (_objects_are_handles) {\n+    size_t num_handles = _num_archived_objects;\n+    \/\/ Skip the null entry\n+    oop** handles = ((oop**)_object_index_to_heap_object_table) + 1;\n+    \/\/ Sort the handles so that oop storage can release them faster\n+    qsort(handles, num_handles, sizeof(oop*), (int (*)(const void*, const void*))oop_handle_cmp);\n+    size_t num_null_handles = 0;\n+    for (size_t handles_remaining = num_handles; handles_remaining != 0; --handles_remaining) {\n+      oop* handle = handles[handles_remaining - 1];\n+      if (handle == nullptr) {\n+        num_null_handles = handles_remaining;\n+        break;\n+      }\n+      NativeAccess<>::oop_store(handle, nullptr);\n+    }\n+    Universe::vm_global()->release(&handles[num_null_handles], num_handles - num_null_handles);\n+  }\n+\n+  FREE_C_HEAP_ARRAY(void*, _object_index_to_heap_object_table);\n+\n+  \/\/ Unmap regions\n+  FileMapInfo::current_info()->unmap_region(AOTMetaspace::hp);\n+  FileMapInfo::current_info()->unmap_region(AOTMetaspace::bm);\n+\n+  _cleanup_materialization_time_ns = (Ticks::now() - start).nanoseconds();\n+\n+  log_statistics();\n+}\n+\n+void AOTStreamedHeapLoader::log_statistics() {\n+  uint64_t total_duration_us = (Ticks::now() - _materialization_start_ticks).microseconds();\n+  const bool is_async = CDSConfig::is_using_full_module_graph() && !AOTEagerlyLoadObjects;\n+  const char* const async_or_sync = is_async ? \"async\" : \"sync\";\n+  log_info(aot, heap)(\"start to finish materialization time: \" UINT64_FORMAT \"us\",\n+                      total_duration_us);\n+  log_info(aot, heap)(\"early object materialization time (%s): \" UINT64_FORMAT \"us\",\n+                      async_or_sync, _early_materialization_time_ns \/ 1000);\n+  log_info(aot, heap)(\"late object materialization time (%s): \" UINT64_FORMAT \"us\",\n+                      async_or_sync, _late_materialization_time_ns \/ 1000);\n+  log_info(aot, heap)(\"object materialization cleanup time (%s): \" UINT64_FORMAT \"us\",\n+                      async_or_sync, _cleanup_materialization_time_ns \/ 1000);\n+  log_info(aot, heap)(\"final object materialization time stall (sync): \" UINT64_FORMAT \"us\",\n+                      _final_materialization_time_ns \/ 1000);\n+  log_info(aot, heap)(\"bootstrapping lazy materialization time (sync): \" UINT64_FORMAT \"us\",\n+                      _accumulated_lazy_materialization_time_ns \/ 1000);\n+\n+  uint64_t sync_time = _final_materialization_time_ns + _accumulated_lazy_materialization_time_ns;\n+  uint64_t async_time = _early_materialization_time_ns + _late_materialization_time_ns + _cleanup_materialization_time_ns;\n+\n+  if (!is_async) {\n+    sync_time += async_time;\n+    async_time = 0;\n+  }\n+\n+  log_info(aot, heap)(\"sync materialization time: \" UINT64_FORMAT \"us\",\n+                      sync_time \/ 1000);\n+\n+  log_info(aot, heap)(\"async materialization time: \" UINT64_FORMAT \"us\",\n+                      async_time \/ 1000);\n+\n+  uint64_t iterative_time = (uint64_t)(is_async ? async_time : sync_time);\n+  uint64_t materialized_bytes = _allocated_words * HeapWordSize;\n+  log_info(aot, heap)(\"%s materialized \" UINT64_FORMAT \"K (\" UINT64_FORMAT \"M\/s)\", async_or_sync,\n+                      materialized_bytes \/ 1024, uint64_t(materialized_bytes * UCONST64(1'000'000'000) \/ M \/ iterative_time));\n+}\n+\n+void AOTStreamedHeapLoader::materialize_objects() {\n+  \/\/ We cannot handle any exception when materializing roots. Exits the VM.\n+  EXCEPTION_MARK\n+\n+  \/\/ Objects are laid out in DFS order; DFS traverse the roots by linearly walking all objects\n+  HandleMark hm(THREAD);\n+\n+  \/\/ Early materialization with a budget before GC is allowed\n+  MutexLocker ml(AOTHeapLoading_lock, Mutex::_safepoint_check_flag);\n+\n+  materialize_early(CHECK);\n+  await_gc_enabled();\n+  materialize_late(CHECK);\n+  \/\/ Notify materialization is done\n+  AOTHeapLoading_lock->notify_all();\n+  cleanup();\n+}\n+\n+void AOTStreamedHeapLoader::switch_object_index_to_handle(int object_index) {\n+  oop heap_object = cast_to_oop(_object_index_to_heap_object_table[object_index]);\n+  if (heap_object == nullptr) {\n+    return;\n+  }\n+\n+  oop* handle = Universe::vm_global()->allocate();\n+  NativeAccess<>::oop_store(handle, heap_object);\n+  _object_index_to_heap_object_table[object_index] = handle;\n+}\n+\n+void AOTStreamedHeapLoader::enable_gc() {\n+  if (AOTEagerlyLoadObjects && !IterativeObjectLoader::has_more()) {\n+    \/\/ Everything was loaded eagerly at early startup\n+    return;\n+  }\n+\n+  \/\/ We cannot handle any exception when materializing roots. Exits the VM.\n+  EXCEPTION_MARK\n+\n+  MutexLocker ml(AOTHeapLoading_lock, Mutex::_safepoint_check_flag);\n+\n+  \/\/ First wait until no tracing is active\n+  while (_waiting_for_iterator) {\n+    AOTHeapLoading_lock->wait();\n+  }\n+\n+  \/\/ Lock further tracing from starting\n+  _waiting_for_iterator = true;\n+\n+  \/\/ Record iterator progress\n+  int num_handles = (int)_num_archived_objects;\n+\n+  \/\/ Lock further iteration from starting\n+  _swapping_root_format = true;\n+\n+  \/\/ Then wait for the iterator to stop\n+  while (_previous_batch_last_object_index != _current_batch_last_object_index) {\n+    AOTHeapLoading_lock->wait();\n+  }\n+\n+  if (IterativeObjectLoader::has_more()) {\n+    \/\/ If there is more to be materialized, we have to upgrade the object index\n+    \/\/ to object mapping to use handles. If there isn't more to materialize, the\n+    \/\/ handle will no longer e used; they are only used to materialize objects.\n+\n+    for (int i = 1; i <= num_handles; ++i) {\n+      \/\/ Upgrade the roots to use handles\n+      switch_object_index_to_handle(i);\n+    }\n+\n+    \/\/ From now on, accessing the object table must be done through a handle.\n+    _objects_are_handles = true;\n+  }\n+\n+  \/\/ Unlock tracing\n+  _waiting_for_iterator = false;\n+\n+  \/\/ Unlock iteration\n+  _swapping_root_format = false;\n+\n+  _allow_gc = true;\n+\n+  AOTHeapLoading_lock->notify_all();\n+\n+  if (AOTEagerlyLoadObjects && IterativeObjectLoader::has_more()) {\n+    materialize_late(CHECK);\n+    cleanup();\n+  }\n+}\n+\n+void AOTStreamedHeapLoader::materialize_thread_object() {\n+  AOTThread::materialize_thread_object();\n+}\n+\n+void AOTStreamedHeapLoader::finish_materialize_objects() {\n+  Ticks start = Ticks::now();\n+\n+  if (CDSConfig::is_using_full_module_graph()) {\n+    MutexLocker ml(AOTHeapLoading_lock, Mutex::_safepoint_check_flag);\n+    \/\/ Wait for the AOT thread to finish\n+    while (IterativeObjectLoader::has_more()) {\n+      AOTHeapLoading_lock->wait();\n+    }\n+  } else {\n+    assert(!AOTEagerlyLoadObjects, \"sanity\");\n+    assert(_current_root_index == 0, \"sanity\");\n+    \/\/ Without the full module graph we have done only lazy tracing materialization.\n+    \/\/ Ensure all roots are processed here by triggering root loading on every root.\n+    for (int i = 0; i < _num_roots; ++i) {\n+      get_root(i);\n+    }\n+    cleanup();\n+  }\n+\n+  _final_materialization_time_ns = (Ticks::now() - start).nanoseconds();\n+}\n+\n+void account_lazy_materialization_time_ns(uint64_t time, const char* description, int index) {\n+  AtomicAccess::add(&_accumulated_lazy_materialization_time_ns, time);\n+  log_debug(aot, heap)(\"Lazy materialization of %s: %d end (\" UINT64_FORMAT \" us of \" UINT64_FORMAT \" us)\", description, index, time \/ 1000, _accumulated_lazy_materialization_time_ns \/ 1000);\n+}\n+\n+\/\/ Initialize an empty array of AOT heap roots; materialize them lazily\n+void AOTStreamedHeapLoader::initialize() {\n+  EXCEPTION_MARK\n+\n+  _materialization_start_ticks = Ticks::now();\n+\n+  FileMapInfo::current_info()->map_bitmap_region();\n+\n+  _heap_region = FileMapInfo::current_info()->region_at(AOTMetaspace::hp);\n+  _bitmap_region = FileMapInfo::current_info()->region_at(AOTMetaspace::bm);\n+\n+  assert(_heap_region->used() > 0, \"empty heap archive?\");\n+\n+  _is_in_use = true;\n+\n+  \/\/ archived roots are at this offset in the stream.\n+  size_t roots_offset = FileMapInfo::current_info()->streamed_heap()->roots_offset();\n+  size_t forwarding_offset = FileMapInfo::current_info()->streamed_heap()->forwarding_offset();\n+  size_t root_highest_object_index_table_offset = FileMapInfo::current_info()->streamed_heap()->root_highest_object_index_table_offset();\n+  _num_archived_objects = FileMapInfo::current_info()->streamed_heap()->num_archived_objects();\n+\n+  \/\/ The first int is the length of the array\n+  _roots_archive = ((int*)(((address)_heap_region->mapped_base()) + roots_offset)) + 1;\n+  _num_roots = _roots_archive[-1];\n+  _heap_region_used = _heap_region->used();\n+\n+  \/\/ We can't retire a TLAB until the filler klass is set; set it to the archived object klass.\n+  CollectedHeap::set_filler_object_klass(vmClasses::Object_klass());\n+\n+  objArrayOop roots = oopFactory::new_objectArray(_num_roots, CHECK);\n+  _roots = OopHandle(Universe::vm_global(), roots);\n+\n+  _object_index_to_buffer_offset_table = (size_t*)(((address)_heap_region->mapped_base()) + forwarding_offset);\n+  \/\/ We allocate the first entry for \"null\"\n+  _object_index_to_heap_object_table = NEW_C_HEAP_ARRAY(void*, _num_archived_objects + 1, mtClassShared);\n+  Copy::zero_to_bytes(_object_index_to_heap_object_table, (_num_archived_objects + 1) * sizeof(void*));\n+\n+  _root_highest_object_index_table = (int*)(((address)_heap_region->mapped_base()) + root_highest_object_index_table_offset);\n+\n+  address start = (address)(_bitmap_region->mapped_base()) + _heap_region->oopmap_offset();\n+  _oopmap = BitMapView((BitMap::bm_word_t*)start, _heap_region->oopmap_size_in_bits());\n+\n+\n+  if (FLAG_IS_DEFAULT(AOTEagerlyLoadObjects)) {\n+    \/\/ Concurrency will not help much if there are no extra cores available.\n+    FLAG_SET_ERGO(AOTEagerlyLoadObjects, os::initial_active_processor_count() <= 1);\n+  }\n+\n+  if (!CDSConfig::is_using_full_module_graph()) {\n+    \/\/ When not using FMG, fall back to tracing materialization\n+    FLAG_SET_ERGO(AOTEagerlyLoadObjects, false);\n+    return;\n+  }\n+\n+  if (AOTEagerlyLoadObjects) {\n+    \/\/ Objects are laid out in DFS order; DFS traverse the roots by linearly walking all objects\n+    HandleMark hm(THREAD);\n+\n+    \/\/ Early materialization with a budget before GC is allowed\n+    MutexLocker ml(AOTHeapLoading_lock, Mutex::_safepoint_check_flag);\n+\n+    bool finished_before_gc_allowed = materialize_early(CHECK);\n+    if (finished_before_gc_allowed) {\n+      cleanup();\n+    }\n+  } else {\n+    AOTThread::initialize();\n+  }\n+}\n+\n+oop AOTStreamedHeapLoader::materialize_root(int root_index) {\n+  Ticks start = Ticks::now();\n+  \/\/ We cannot handle any exception when materializing a root. Exits the VM.\n+  EXCEPTION_MARK\n+  Stack<AOTHeapTraversalEntry, mtClassShared> dfs_stack;\n+  HandleMark hm(THREAD);\n+\n+  oop result;\n+  {\n+    MutexLocker ml(AOTHeapLoading_lock, Mutex::_safepoint_check_flag);\n+\n+    oop root = objArrayOop(_roots.resolve())->obj_at(root_index);\n+\n+    if (root != nullptr) {\n+      \/\/ The root has already been materialized\n+      result = root;\n+    } else {\n+      \/\/ The root has not been materialized, start tracing materialization\n+      result = TracingObjectLoader::materialize_root(root_index, dfs_stack, CHECK_NULL);\n+    }\n+  }\n+\n+  uint64_t duration = (Ticks::now() - start).nanoseconds();\n+\n+  account_lazy_materialization_time_ns(duration, \"root\", root_index);\n+\n+  return result;\n+}\n+\n+oop AOTStreamedHeapLoader::get_root(int index) {\n+  oop result = objArrayOop(_roots.resolve())->obj_at(index);\n+  if (result == nullptr) {\n+    \/\/ Materialize root\n+    result = materialize_root(index);\n+  }\n+  if (result == _roots.resolve()) {\n+    \/\/ A self-reference to the roots array acts as a sentinel object for null,\n+    \/\/ indicating that the root has been cleared.\n+    result = nullptr;\n+  }\n+  \/\/ Acquire the root transitive object payload\n+  OrderAccess::acquire();\n+  return result;\n+}\n+\n+void AOTStreamedHeapLoader::clear_root(int index) {\n+  \/\/ Self-reference to the roots array acts as a sentinel object for null,\n+  \/\/ indicating that the root has been cleared.\n+  objArrayOop(_roots.resolve())->obj_at_put(index, _roots.resolve());\n+}\n+\n+void AOTStreamedHeapLoader::await_gc_enabled() {\n+  while (!_allow_gc) {\n+    AOTHeapLoading_lock->wait();\n+  }\n+}\n+\n+void AOTStreamedHeapLoader::finish_initialization(FileMapInfo* static_mapinfo) {\n+  static_mapinfo->stream_heap_region();\n+}\n+\n+AOTMapLogger::OopDataIterator* AOTStreamedHeapLoader::oop_iterator(FileMapInfo* info, address buffer_start, address buffer_end) {\n+  class StreamedLoaderOopIterator : public AOTMapLogger::OopDataIterator {\n+  private:\n+    int _current;\n+    int _next;\n+\n+    address _buffer_start;\n+\n+    int _num_archived_objects;\n+\n+  public:\n+    StreamedLoaderOopIterator(address buffer_start,\n+                              int num_archived_objects)\n+      : _current(0),\n+        _next(1),\n+        _buffer_start(buffer_start),\n+        _num_archived_objects(num_archived_objects) {\n+    }\n+\n+    AOTMapLogger::OopData capture(int dfs_index) {\n+      size_t buffered_offset = buffer_offset_for_object_index(dfs_index);\n+      address buffered_addr = _buffer_start + buffered_offset;\n+      oopDesc* raw_oop = (oopDesc*)buffered_addr;\n+      size_t size = archive_object_size(raw_oop);\n+\n+      intptr_t target_location = (intptr_t)buffered_offset;\n+      uint32_t narrow_location = checked_cast<uint32_t>(dfs_index);\n+      Klass* klass = raw_oop->klass();\n+\n+      address requested_addr = (address)buffered_offset;\n+\n+      return { buffered_addr,\n+               requested_addr,\n+               target_location,\n+               narrow_location,\n+               raw_oop,\n+               klass,\n+               size,\n+               false };\n+    }\n+\n+    bool has_next() override {\n+      return _next <= _num_archived_objects;\n+    }\n+\n+    AOTMapLogger::OopData next() override {\n+      _current = _next;\n+      AOTMapLogger::OopData result = capture(_current);\n+      _next = _current + 1;\n+      return result;\n+    }\n+\n+    AOTMapLogger::OopData obj_at(narrowOop* addr) override {\n+      int dfs_index = (int)(*addr);\n+      if (dfs_index == 0) {\n+        return null_data();\n+      } else {\n+        return capture(dfs_index);\n+      }\n+    }\n+\n+    AOTMapLogger::OopData obj_at(oop* addr) override {\n+      int dfs_index = (int)cast_from_oop<uintptr_t>(*addr);\n+      if (dfs_index == 0) {\n+        return null_data();\n+      } else {\n+        return capture(dfs_index);\n+      }\n+    }\n+\n+    GrowableArrayCHeap<AOTMapLogger::OopData, mtClass>* roots() override {\n+      GrowableArrayCHeap<AOTMapLogger::OopData, mtClass>* result = new GrowableArrayCHeap<AOTMapLogger::OopData, mtClass>();\n+\n+      for (int i = 0; i < _num_roots; ++i) {\n+        int object_index = object_index_for_root_index(i);\n+        result->append(capture(object_index));\n+      }\n+\n+      return result;\n+    }\n+  };\n+\n+  assert(_is_in_use, \"printing before initializing?\");\n+\n+  return new StreamedLoaderOopIterator(buffer_start, (int)info->streamed_heap()->num_archived_objects());\n+}\n+\n+#endif \/\/ INCLUDE_CDS_JAVA_HEAP\n","filename":"src\/hotspot\/share\/cds\/aotStreamedHeapLoader.cpp","additions":1183,"deletions":0,"binary":false,"changes":1183,"status":"added"},{"patch":"@@ -0,0 +1,244 @@\n+\/*\n+ * Copyright (c) 2018, 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef SHARE_CDS_AOTSTREAMEDHEAPLOADER_HPP\n+#define SHARE_CDS_AOTSTREAMEDHEAPLOADER_HPP\n+\n+#include \"cds\/aotMapLogger.hpp\"\n+#include \"cds\/filemap.hpp\"\n+#include \"memory\/allocation.hpp\"\n+#include \"memory\/allStatic.hpp\"\n+#include \"oops\/oopsHierarchy.hpp\"\n+#include \"utilities\/exceptions.hpp\"\n+#include \"utilities\/globalDefinitions.hpp\"\n+#include \"utilities\/growableArray.hpp\"\n+#include \"utilities\/macros.hpp\"\n+#include \"utilities\/stack.hpp\"\n+\n+#if INCLUDE_CDS_JAVA_HEAP\n+\n+\/\/ The streaming archive heap loader loads Java objects using normal allocations. It requires the objects\n+\/\/ to be ordered in DFS order already at dump time, given the set of roots into the archived heap.\n+\/\/ Since the objects are ordered in DFS order, that means that walking them linearly through the archive\n+\/\/ is equivalent to performing a DFS traversal, but without pushing and popping anything.\n+\/\/\n+\/\/ The advantage of this pre-ordering, other than the obvious locality improvement, is that we can have\n+\/\/ a separate thread, the AOTThread, perform this walk, in a way that allows us to split the archived\n+\/\/ heap into three separate zones. The first zone contains objects that have been transitively materialized,\n+\/\/ the second zone contains objects that are currently being materialized, and the last zone contains\n+\/\/ objects that have not and are not about to be touched by the AOT thread.\n+\/\/ Whenever a new root is traversed by the AOT thread, the zones are shifted atomically under a lock.\n+\/\/\n+\/\/ Visualization of the three zones:\n+\/\/\n+\/\/ +--------------------------------------+-------------------------+----------------------------------+\n+\/\/ |      transitively materialized       | currently materializing |        not yet materialized      |\n+\/\/ +--------------------------------------+-------------------------+----------------------------------+\n+\/\/\n+\/\/ Being able to split the memory into these three zones, allows the bootstrapping thread and potential\n+\/\/ other threads to be able to, under a lock, traverse a root, and know how to coordinate with the\n+\/\/ concurrent AOT thread. Whenever the traversal finds an object in the \"transitively materialized\"\n+\/\/ zone, then we know such objects don't need any processing at all. As for \"currently materializing\",\n+\/\/ we know that if we just stay out of the way and let the AOT thread finish its current root, then\n+\/\/ the transitive closure of such objects will be materialized. And the AOT thread can materialize faster\n+\/\/ then the rest as it doesn't need to perform any traversal. Finally, as for objects in the \"not yet\n+\/\/ materialized\" zone, we know that we can trace through it without stepping on the feed of the AOT thread\n+\/\/ which has published it won't be tracing anything in there.\n+\/\/\n+\/\/ What we get from this, is fast iterative traversal from the AOT thread (IterativeObjectLoader)\n+\/\/ while allowing lazyness and concurrency with the rest of the program (TracingObjectLoader).\n+\/\/ This way the AOT thread can remove the bulk of the work of materializing the Java objects from\n+\/\/ the critical bootstrapping thread.\n+\/\/\n+\/\/ When we start materializing objects, we have not yet come to the point in the bootstrapping where\n+\/\/ GC is allowed. This is a two edged sword. On the one hand side, we can materialize objects faster\n+\/\/ when we know there is no GC to coordinate with, but on the other hand side, if we need to perform\n+\/\/ a GC when allocating memory for archived objects, we will bring down the entire JVM. To deal with this,\n+\/\/ the AOT thread asks the GC for a budget of bytes it is allowed to allocate before GC is allowed.\n+\/\/ When we get to the point in the bootstrapping where GC is allowed, we resume materializing objects\n+\/\/ that didn't fit in the budget. Before we let the application run, we force materialization of any\n+\/\/ remaining objects that have not been materialized by the AOT thread yet, so that we don't get\n+\/\/ surprising OOMs due to object materialization while the program is running.\n+\/\/\n+\/\/ The object format of the archived heap is similar to a normal object. However, references are encoded\n+\/\/ as DFS indices, which in the end map to what index the object is in the buffer, as they are laid out\n+\/\/ in DFS order. The DFS indices start at 1 for the first object, and hence the number 0 represents\n+\/\/ null. The DFS index of objects is a core identifier of objects in this approach. From this index\n+\/\/ it is possible to find out what offset the archived object has into the buffer, as well as finding\n+\/\/ mappings to Java heap objects that have been materialized.\n+\/\/\n+\/\/ The table mapping DFS indices to Java heap objects is filled in when an object is allocated.\n+\/\/ Materializing objects involves allocating the object, initializing it, and linking it with other\n+\/\/ objects. Since linking the object requires whatever is being referenced to be at least allocated,\n+\/\/ the iterative traversal will first allocate all of the objects in its zone being worked on, and then\n+\/\/ perform initialization and linking in a second pass. What these passes have in common is that they\n+\/\/ are trivially parallelizable, should we ever need to do that. The tracing materialization links\n+\/\/ objects when going \"back\" in the DFS traversal.\n+\/\/\n+\/\/ The forwarding information for the mechanism contains raw oops before GC is allowed, and as we\n+\/\/ enable GC in the bootstrapping, all raw oops are handleified using OopStorage. All handles are\n+\/\/ handed back from the AOT thread when materialization has finished. The switch from raw oops to\n+\/\/ using OopStorage handles, happens under a lock while no iteration nor tracing is allowed.\n+\/\/\n+\/\/ The initialization code is also performed in a faster way when the GC is not allowed. In particular,\n+\/\/ before GC is allowed, we perform raw memcpy of the archived object into the Java heap. Then the\n+\/\/ object is initialized with IS_DEST_UNINITIALIZED stores. The assumption made here is that before\n+\/\/ any GC activity is allowed, we shouldn't have to worry about concurrent GC threads scanning the\n+\/\/ memory and getting tripped up by that. Once GC is enabled, we revert to a bit more careful approach\n+\/\/ that uses a pre-computed bitmap to find the holes where oops go, and carefully copy only the\n+\/\/ non-oop information with memcpy, while the oops are set separately with HeapAccess stores that\n+\/\/ should be able to cope well with concurrent activity.\n+\/\/\n+\/\/ The marked bit pattern of the mark word of archived heap objects is used for signalling which string\n+\/\/ objects should be interned. From the dump, some referenced strings were interned. This is\n+\/\/ really an identity property. We don't need to dump the entire string table as a way of communicating\n+\/\/ this identity property. Instead we intern strings on-the-fly, exploiting the dynamic object\n+\/\/ level linking that this approach has chosen to our advantage.\n+\n+class FileMapInfo;\n+class OopStorage;\n+class Thread;\n+struct AOTHeapTraversalEntry;\n+\n+struct alignas(AOTHeapTraversalEntry* \/* Requirement of Stack<AOTHeapTraversalEntry> *\/) AOTHeapTraversalEntry {\n+  int _pointee_object_index;\n+  int _base_object_index;\n+  int _heap_field_offset_bytes;\n+};\n+\n+class AOTStreamedHeapLoader {\n+  friend class InflateReferenceOopClosure;\n+private:\n+  static FileMapRegion* _heap_region;\n+  static FileMapRegion* _bitmap_region;\n+  static OopStorage* _oop_storage;\n+  static int* _roots_archive;\n+  static OopHandle _roots;\n+  static BitMapView _oopmap;\n+  static bool _is_in_use;\n+  static bool _allow_gc;\n+  static bool _objects_are_handles;\n+  static int _previous_batch_last_object_index;\n+  static int _current_batch_last_object_index;\n+  static size_t _allocated_words;\n+  static int _current_root_index;\n+  static size_t _num_archived_objects;\n+  static int _num_roots;\n+  static size_t _heap_region_used;\n+\n+  static size_t* _object_index_to_buffer_offset_table;\n+  static void** _object_index_to_heap_object_table;\n+  static int* _root_highest_object_index_table;\n+\n+  static bool _waiting_for_iterator;\n+  static bool _swapping_root_format;\n+\n+\n+  template <typename LinkerT>\n+  class InPlaceLinkingOopClosure;\n+\n+  static oop allocate_object(oopDesc* archive_object, markWord mark, size_t size, TRAPS);\n+  static int object_index_for_root_index(int root_index);\n+  static int highest_object_index_for_root_index(int root_index);\n+  static size_t buffer_offset_for_object_index(int object_index);\n+  static oopDesc* archive_object_for_object_index(int object_index);\n+  static size_t buffer_offset_for_archive_object(oopDesc* archive_object);\n+  template <bool use_coops>\n+  static BitMap::idx_t obj_bit_idx_for_buffer_offset(size_t buffer_offset);\n+\n+  template <bool use_coops, typename LinkerT>\n+  static void copy_payload_carefully(oopDesc* archive_object,\n+                                     oop heap_object,\n+                                     BitMap::idx_t header_bit,\n+                                     BitMap::idx_t start_bit,\n+                                     BitMap::idx_t end_bit,\n+                                     LinkerT linker);\n+\n+  template <bool use_coops, typename LinkerT>\n+  static void copy_object_impl(oopDesc* archive_object,\n+                               oop heap_object,\n+                               size_t size,\n+                               LinkerT linker);\n+\n+  static void copy_object_eager_linking(oopDesc* archive_object, oop heap_object, size_t size);\n+\n+  static void switch_object_index_to_handle(int object_index);\n+  static oop heap_object_for_object_index(int object_index);\n+  static void set_heap_object_for_object_index(int object_index, oop heap_object);\n+\n+  static int archived_string_value_object_index(oopDesc* archive_object);\n+\n+  static bool materialize_early(TRAPS);\n+  static void materialize_late(TRAPS);\n+  static void cleanup();\n+  static void log_statistics();\n+\n+  class TracingObjectLoader {\n+    static oop materialize_object(int object_index, Stack<AOTHeapTraversalEntry, mtClassShared>& dfs_stack, TRAPS);\n+    static oop materialize_object_inner(int object_index, Stack<AOTHeapTraversalEntry, mtClassShared>& dfs_stack, TRAPS);\n+    static void copy_object_lazy_linking(int object_index,\n+                                         oopDesc* archive_object,\n+                                         oop heap_object,\n+                                         size_t size,\n+                                         Stack<AOTHeapTraversalEntry, mtClassShared>& dfs_stack);\n+    static void drain_stack(Stack<AOTHeapTraversalEntry, mtClassShared>& dfs_stack, TRAPS);\n+    static oop materialize_object_transitive(int object_index, Stack<AOTHeapTraversalEntry, mtClassShared>& dfs_stack, TRAPS);\n+\n+    static void wait_for_iterator();\n+\n+  public:\n+    static oop materialize_root(int root_index, Stack<AOTHeapTraversalEntry, mtClassShared>& dfs_stack, TRAPS);\n+  };\n+\n+  class IterativeObjectLoader {\n+    static void initialize_range(int first_object_index, int last_object_index, TRAPS);\n+    static size_t materialize_range(int first_object_index, int last_object_index, TRAPS);\n+\n+  public:\n+    static bool has_more();\n+    static void materialize_next_batch(TRAPS);\n+  };\n+\n+  static void install_root(int root_index, oop heap_object);\n+\n+  static void await_gc_enabled();\n+  static void await_finished_processing();\n+\n+public:\n+  static void initialize();\n+  static void enable_gc();\n+  static void materialize_thread_object();\n+  static oop materialize_root(int root_index);\n+  static oop get_root(int root_index);\n+  static void clear_root(int index);\n+  static void materialize_objects();\n+  static void finish_materialize_objects();\n+  static bool is_in_use() { return _is_in_use; }\n+  static void finish_initialization(FileMapInfo* info);\n+\n+  static AOTMapLogger::OopDataIterator* oop_iterator(FileMapInfo* info, address buffer_start, address buffer_end);\n+};\n+\n+#endif \/\/ SHARE_CDS_AOTSTREAMEDHEAPLOADER_HPP\n+#endif \/\/ INCLUDE_CDS_JAVA_HEAP\n","filename":"src\/hotspot\/share\/cds\/aotStreamedHeapLoader.hpp","additions":244,"deletions":0,"binary":false,"changes":244,"status":"added"},{"patch":"@@ -0,0 +1,614 @@\n+\/*\n+ * Copyright (c) 2025, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#include \"cds\/aotReferenceObjSupport.hpp\"\n+#include \"cds\/aotStreamedHeapWriter.hpp\"\n+#include \"cds\/cdsConfig.hpp\"\n+#include \"cds\/filemap.hpp\"\n+#include \"cds\/heapShared.inline.hpp\"\n+#include \"cds\/regeneratedClasses.hpp\"\n+#include \"classfile\/modules.hpp\"\n+#include \"classfile\/stringTable.hpp\"\n+#include \"classfile\/systemDictionary.hpp\"\n+#include \"gc\/shared\/collectedHeap.hpp\"\n+#include \"memory\/iterator.inline.hpp\"\n+#include \"memory\/oopFactory.hpp\"\n+#include \"memory\/universe.hpp\"\n+#include \"oops\/compressedOops.hpp\"\n+#include \"oops\/objArrayOop.inline.hpp\"\n+#include \"oops\/oop.inline.hpp\"\n+#include \"oops\/oopHandle.inline.hpp\"\n+#include \"oops\/typeArrayKlass.hpp\"\n+#include \"oops\/typeArrayOop.hpp\"\n+#include \"runtime\/java.hpp\"\n+#include \"runtime\/mutexLocker.hpp\"\n+#include \"utilities\/bitMap.inline.hpp\"\n+#include \"utilities\/stack.inline.hpp\"\n+\n+#if INCLUDE_CDS_JAVA_HEAP\n+\n+GrowableArrayCHeap<u1, mtClassShared>* AOTStreamedHeapWriter::_buffer = nullptr;\n+\n+\/\/ The following are offsets from buffer_bottom()\n+size_t AOTStreamedHeapWriter::_buffer_used;\n+size_t AOTStreamedHeapWriter::_roots_offset;\n+size_t AOTStreamedHeapWriter::_forwarding_offset;\n+size_t AOTStreamedHeapWriter::_root_highest_object_index_table_offset;\n+\n+GrowableArrayCHeap<oop, mtClassShared>* AOTStreamedHeapWriter::_source_objs;\n+\n+AOTStreamedHeapWriter::BufferOffsetToSourceObjectTable* AOTStreamedHeapWriter::_buffer_offset_to_source_obj_table;\n+AOTStreamedHeapWriter::SourceObjectToDFSOrderTable* AOTStreamedHeapWriter::_dfs_order_table;\n+\n+int* AOTStreamedHeapWriter::_roots_highest_dfs;\n+size_t* AOTStreamedHeapWriter::_dfs_to_archive_object_table;\n+\n+static const int max_table_capacity = 0x3fffffff;\n+\n+void AOTStreamedHeapWriter::init() {\n+  if (CDSConfig::is_dumping_heap()) {\n+    _buffer_offset_to_source_obj_table = new (mtClassShared) BufferOffsetToSourceObjectTable(8, max_table_capacity);\n+\n+    int initial_source_objs_capacity = 10000;\n+    _source_objs = new GrowableArrayCHeap<oop, mtClassShared>(initial_source_objs_capacity);\n+  }\n+}\n+\n+void AOTStreamedHeapWriter::delete_tables_with_raw_oops() {\n+  delete _source_objs;\n+  _source_objs = nullptr;\n+\n+  delete _dfs_order_table;\n+  _dfs_order_table = nullptr;\n+}\n+\n+void AOTStreamedHeapWriter::add_source_obj(oop src_obj) {\n+  _source_objs->append(src_obj);\n+}\n+\n+class FollowOopIterateClosure: public BasicOopIterateClosure {\n+  Stack<oop, mtClassShared>* _dfs_stack;\n+  oop _src_obj;\n+  bool _is_java_lang_ref;\n+\n+public:\n+  FollowOopIterateClosure(Stack<oop, mtClassShared>* dfs_stack, oop src_obj, bool is_java_lang_ref) :\n+    _dfs_stack(dfs_stack),\n+    _src_obj(src_obj),\n+    _is_java_lang_ref(is_java_lang_ref) {}\n+\n+  void do_oop(narrowOop *p) { do_oop_work(p); }\n+  void do_oop(      oop *p) { do_oop_work(p); }\n+\n+private:\n+  template <class T> void do_oop_work(T *p) {\n+    size_t field_offset = pointer_delta(p, _src_obj, sizeof(char));\n+    oop obj = HeapShared::maybe_remap_referent(_is_java_lang_ref, field_offset, HeapAccess<>::oop_load(p));\n+    if (obj != nullptr) {\n+      _dfs_stack->push(obj);\n+    }\n+  }\n+};\n+\n+int AOTStreamedHeapWriter::cmp_dfs_order(oop* o1, oop* o2) {\n+  int* o1_dfs = _dfs_order_table->get(*o1);\n+  int* o2_dfs = _dfs_order_table->get(*o2);\n+  return *o1_dfs - *o2_dfs;\n+}\n+\n+void AOTStreamedHeapWriter::order_source_objs(GrowableArrayCHeap<oop, mtClassShared>* roots) {\n+  Stack<oop, mtClassShared> dfs_stack;\n+  _dfs_order_table = new (mtClassShared) SourceObjectToDFSOrderTable(8, max_table_capacity);\n+  _roots_highest_dfs = NEW_C_HEAP_ARRAY(int, (size_t)roots->length(), mtClassShared);\n+  _dfs_to_archive_object_table = NEW_C_HEAP_ARRAY(size_t, (size_t)_source_objs->length() + 1, mtClassShared);\n+\n+  for (int i = 0; i < _source_objs->length(); ++i) {\n+    oop obj = _source_objs->at(i);\n+    _dfs_order_table->put(cast_from_oop<void*>(obj), -1);\n+    _dfs_order_table->maybe_grow();\n+  }\n+\n+  int dfs_order = 0;\n+\n+  for (int i = 0; i < roots->length(); ++i) {\n+    oop root = roots->at(i);\n+\n+    if (root == nullptr) {\n+      log_info(aot, heap)(\"null root at %d\", i);\n+      continue;\n+    }\n+\n+    dfs_stack.push(root);\n+\n+    while (!dfs_stack.is_empty()) {\n+      oop obj = dfs_stack.pop();\n+      assert(obj != nullptr, \"null root\");\n+      int* dfs_number = _dfs_order_table->get(cast_from_oop<void*>(obj));\n+      if (*dfs_number != -1) {\n+        \/\/ Already visited in the traversal\n+        continue;\n+      }\n+      _dfs_order_table->put(cast_from_oop<void*>(obj), ++dfs_order);\n+      _dfs_order_table->maybe_grow();\n+\n+      FollowOopIterateClosure cl(&dfs_stack, obj, AOTReferenceObjSupport::check_if_ref_obj(obj));\n+      obj->oop_iterate(&cl);\n+    }\n+\n+    _roots_highest_dfs[i] = dfs_order;\n+  }\n+\n+  _source_objs->sort(cmp_dfs_order);\n+}\n+\n+void AOTStreamedHeapWriter::write(GrowableArrayCHeap<oop, mtClassShared>* roots,\n+                                  ArchiveStreamedHeapInfo* heap_info) {\n+  assert(CDSConfig::is_dumping_heap(), \"sanity\");\n+  allocate_buffer();\n+  order_source_objs(roots);\n+  copy_source_objs_to_buffer(roots);\n+  map_embedded_oops(heap_info);\n+  populate_archive_heap_info(heap_info);\n+}\n+\n+void AOTStreamedHeapWriter::allocate_buffer() {\n+  int initial_buffer_size = 100000;\n+  _buffer = new GrowableArrayCHeap<u1, mtClassShared>(initial_buffer_size);\n+  _buffer_used = 0;\n+  ensure_buffer_space(1); \/\/ so that buffer_bottom() works\n+}\n+\n+void AOTStreamedHeapWriter::ensure_buffer_space(size_t min_bytes) {\n+  \/\/ We usually have very small heaps. If we get a huge one it's probably caused by a bug.\n+  guarantee(min_bytes <= max_jint, \"we dont support archiving more than 2G of objects\");\n+  _buffer->at_grow(to_array_index(min_bytes));\n+}\n+\n+void AOTStreamedHeapWriter::copy_roots_to_buffer(GrowableArrayCHeap<oop, mtClassShared>* roots) {\n+  int length = roots->length();\n+  size_t byte_size = align_up(sizeof(int) + sizeof(int) * (size_t)length, (size_t)HeapWordSize);\n+\n+  size_t new_used = _buffer_used + byte_size;\n+  ensure_buffer_space(new_used);\n+\n+  int* mem = offset_to_buffered_address<int*>(_buffer_used);\n+  memset(mem, 0, byte_size);\n+  *mem = length;\n+\n+  for (int i = 0; i < length; i++) {\n+    \/\/ Do not use arrayOop->obj_at_put(i, o) as arrayOop is outside of the real heap!\n+    oop o = roots->at(i);\n+    int dfs_index = o == nullptr ? 0 : *_dfs_order_table->get(cast_from_oop<void*>(o));\n+    mem[i + 1] = dfs_index;\n+  }\n+  log_info(aot, heap)(\"archived obj roots[%d] = %zu bytes, mem = %p\", length, byte_size, mem);\n+\n+  _roots_offset = _buffer_used;\n+  _buffer_used = new_used;\n+}\n+\n+template <typename T>\n+void AOTStreamedHeapWriter::write(T value) {\n+  size_t new_used = _buffer_used + sizeof(T);\n+  ensure_buffer_space(new_used);\n+  T* mem = offset_to_buffered_address<T*>(_buffer_used);\n+  *mem = value;\n+  _buffer_used = new_used;\n+}\n+\n+void AOTStreamedHeapWriter::copy_forwarding_to_buffer() {\n+  _forwarding_offset = _buffer_used;\n+\n+  write<size_t>(0); \/\/ The first entry is the null entry\n+\n+  \/\/ Write a mapping from object index to buffer offset\n+  for (int i = 1; i <= _source_objs->length(); i++) {\n+    size_t buffer_offset = _dfs_to_archive_object_table[i];\n+    write(buffer_offset);\n+  }\n+}\n+\n+void AOTStreamedHeapWriter::copy_roots_max_dfs_to_buffer(int roots_length) {\n+  _root_highest_object_index_table_offset = _buffer_used;\n+\n+  for (int i = 0; i < roots_length; ++i) {\n+    int highest_dfs = _roots_highest_dfs[i];\n+    write(highest_dfs);\n+  }\n+\n+  if ((roots_length % 2) != 0) {\n+    write(-1); \/\/ Align up to a 64 bit word\n+  }\n+}\n+\n+static bool is_interned_string(oop obj) {\n+  if (!java_lang_String::is_instance(obj)) {\n+    return false;\n+  }\n+\n+  ResourceMark rm;\n+  int len;\n+  jchar* name = java_lang_String::as_unicode_string_or_null(obj, len);\n+  if (name == nullptr) {\n+    fatal(\"Insufficient memory for dumping\");\n+  }\n+  return StringTable::lookup(name, len) == obj;\n+}\n+\n+static BitMap::idx_t bit_idx_for_buffer_offset(size_t buffer_offset) {\n+  if (UseCompressedOops) {\n+    return BitMap::idx_t(buffer_offset \/ sizeof(narrowOop));\n+  } else {\n+    return BitMap::idx_t(buffer_offset \/ sizeof(HeapWord));\n+  }\n+}\n+\n+bool AOTStreamedHeapWriter::is_dumped_interned_string(oop obj) {\n+  return is_interned_string(obj) && HeapShared::get_cached_oop_info(obj) != nullptr;\n+}\n+\n+void AOTStreamedHeapWriter::copy_source_objs_to_buffer(GrowableArrayCHeap<oop, mtClassShared>* roots) {\n+  for (int i = 0; i < _source_objs->length(); i++) {\n+    oop src_obj = _source_objs->at(i);\n+    HeapShared::CachedOopInfo* info = HeapShared::get_cached_oop_info(src_obj);\n+    assert(info != nullptr, \"must be\");\n+    size_t buffer_offset = copy_one_source_obj_to_buffer(src_obj);\n+    info->set_buffer_offset(buffer_offset);\n+\n+    OopHandle handle(Universe::vm_global(), src_obj);\n+    _buffer_offset_to_source_obj_table->put_when_absent(buffer_offset, handle);\n+    _buffer_offset_to_source_obj_table->maybe_grow();\n+\n+    int dfs_order = i + 1;\n+    _dfs_to_archive_object_table[dfs_order] = buffer_offset;\n+  }\n+\n+  copy_roots_to_buffer(roots);\n+  copy_forwarding_to_buffer();\n+  copy_roots_max_dfs_to_buffer(roots->length());\n+\n+  log_info(aot)(\"Size of heap region = %zu bytes, %d objects, %d roots\",\n+                _buffer_used, _source_objs->length() + 1, roots->length());\n+}\n+\n+template <typename T>\n+void update_buffered_object_field(address buffered_obj, int field_offset, T value) {\n+  T* field_addr = cast_to_oop(buffered_obj)->field_addr<T>(field_offset);\n+  *field_addr = value;\n+}\n+\n+static bool needs_explicit_size(oop src_obj) {\n+  Klass* klass = src_obj->klass();\n+  int lh = klass->layout_helper();\n+\n+  \/\/ Simple instances or arrays don't need explicit size\n+  if (Klass::layout_helper_is_instance(lh)) {\n+    return Klass::layout_helper_needs_slow_path(lh);\n+  }\n+\n+  return !Klass::layout_helper_is_array(lh);\n+}\n+\n+size_t AOTStreamedHeapWriter::copy_one_source_obj_to_buffer(oop src_obj) {\n+  if (needs_explicit_size(src_obj)) {\n+    \/\/ Explicitly write object size for more complex objects, to avoid having to\n+    \/\/ pretend the buffer objects are objects when loading the objects, in order\n+    \/\/ to read the size. Most of the time, the layout helper of the class is enough.\n+    write<size_t>(src_obj->size());\n+  }\n+  size_t byte_size = src_obj->size() * HeapWordSize;\n+  assert(byte_size > 0, \"no zero-size objects\");\n+\n+  size_t new_used = _buffer_used + byte_size;\n+  assert(new_used > _buffer_used, \"no wrap around\");\n+\n+  ensure_buffer_space(new_used);\n+\n+  if (is_interned_string(src_obj)) {\n+    java_lang_String::hash_code(src_obj);                   \/\/ Sets the hash code field(s)\n+    java_lang_String::set_deduplication_forbidden(src_obj); \/\/ Allows faster interning at runtime\n+    assert(java_lang_String::hash_is_set(src_obj), \"hash must be set\");\n+  }\n+\n+  address from = cast_from_oop<address>(src_obj);\n+  address to = offset_to_buffered_address<address>(_buffer_used);\n+  assert(is_object_aligned(_buffer_used), \"sanity\");\n+  assert(is_object_aligned(byte_size), \"sanity\");\n+  memcpy(to, from, byte_size);\n+\n+  if (java_lang_Module::is_instance(src_obj)) {\n+    \/\/ These native pointers will be restored explicitly at run time.\n+    Modules::check_archived_module_oop(src_obj);\n+    update_buffered_object_field<ModuleEntry*>(to, java_lang_Module::module_entry_offset(), nullptr);\n+  } else if (java_lang_ClassLoader::is_instance(src_obj)) {\n+#ifdef ASSERT\n+    \/\/ We only archive these loaders\n+    if (src_obj != SystemDictionary::java_platform_loader() &&\n+        src_obj != SystemDictionary::java_system_loader()) {\n+      assert(src_obj->klass()->name()->equals(\"jdk\/internal\/loader\/ClassLoaders$BootClassLoader\"), \"must be\");\n+    }\n+#endif\n+    update_buffered_object_field<ClassLoaderData*>(to, java_lang_ClassLoader::loader_data_offset(), nullptr);\n+  }\n+\n+  size_t buffered_obj_offset = _buffer_used;\n+  _buffer_used = new_used;\n+\n+  return buffered_obj_offset;\n+}\n+\n+\/\/ Oop mapping\n+\n+inline void AOTStreamedHeapWriter::store_oop_in_buffer(oop* buffered_addr, int dfs_index) {\n+  *(ssize_t*)buffered_addr = dfs_index;\n+}\n+\n+inline void AOTStreamedHeapWriter::store_oop_in_buffer(narrowOop* buffered_addr, int dfs_index) {\n+  *(int32_t*)buffered_addr = (int32_t)dfs_index;\n+}\n+\n+template <typename T> void AOTStreamedHeapWriter::mark_oop_pointer(T* buffered_addr, CHeapBitMap* oopmap) {\n+  \/\/ Mark the pointer in the oopmap\n+  size_t buffered_offset = buffered_address_to_offset((address)buffered_addr);\n+  BitMap::idx_t idx = bit_idx_for_buffer_offset(buffered_offset);\n+  oopmap->set_bit(idx);\n+}\n+\n+template <typename T> void AOTStreamedHeapWriter::map_oop_field_in_buffer(oop obj, T* field_addr_in_buffer, CHeapBitMap* oopmap) {\n+  if (obj == nullptr) {\n+    store_oop_in_buffer(field_addr_in_buffer, 0);\n+  } else {\n+    int dfs_index = *_dfs_order_table->get(obj);\n+    store_oop_in_buffer(field_addr_in_buffer, dfs_index);\n+  }\n+\n+  mark_oop_pointer<T>(field_addr_in_buffer, oopmap);\n+}\n+\n+void AOTStreamedHeapWriter::update_header_for_buffered_addr(address buffered_addr, oop src_obj,  Klass* src_klass) {\n+  assert(UseCompressedClassPointers, \"Archived heap only supported for compressed klasses\");\n+  narrowKlass nk = ArchiveBuilder::current()->get_requested_narrow_klass(src_klass);\n+\n+  markWord mw = markWord::prototype();\n+  oopDesc* fake_oop = (oopDesc*)buffered_addr;\n+\n+  \/\/ We need to retain the identity_hash, because it may have been used by some hashtables\n+  \/\/ in the shared heap. This also has the side effect of pre-initializing the\n+  \/\/ identity_hash for all shared objects, so they are less likely to be written\n+  \/\/ into during run time, increasing the potential of memory sharing.\n+  if (src_obj != nullptr) {\n+    intptr_t src_hash = src_obj->identity_hash();\n+    mw = mw.copy_set_hash(src_hash);\n+  }\n+\n+  if (is_interned_string(src_obj)) {\n+    \/\/ Mark the mark word of interned string so the loader knows to link these to\n+    \/\/ the string table at runtime.\n+    mw = mw.set_marked();\n+  }\n+\n+  if (UseCompactObjectHeaders) {\n+    fake_oop->set_mark(mw.set_narrow_klass(nk));\n+  } else {\n+    fake_oop->set_mark(mw);\n+    fake_oop->set_narrow_klass(nk);\n+  }\n+}\n+\n+class AOTStreamedHeapWriter::EmbeddedOopMapper: public BasicOopIterateClosure {\n+  oop _src_obj;\n+  address _buffered_obj;\n+  CHeapBitMap* _oopmap;\n+  bool _is_java_lang_ref;\n+\n+public:\n+  EmbeddedOopMapper(oop src_obj, address buffered_obj, CHeapBitMap* oopmap)\n+    : _src_obj(src_obj),\n+      _buffered_obj(buffered_obj),\n+      _oopmap(oopmap),\n+      _is_java_lang_ref(AOTReferenceObjSupport::check_if_ref_obj(src_obj)) {}\n+\n+  void do_oop(narrowOop *p) { EmbeddedOopMapper::do_oop_work(p); }\n+  void do_oop(      oop *p) { EmbeddedOopMapper::do_oop_work(p); }\n+\n+private:\n+  template <typename T>\n+  void do_oop_work(T *p) {\n+    size_t field_offset = pointer_delta(p, _src_obj, sizeof(char));\n+    oop obj = HeapShared::maybe_remap_referent(_is_java_lang_ref, field_offset, HeapAccess<>::oop_load(p));\n+    AOTStreamedHeapWriter::map_oop_field_in_buffer<T>(obj, (T*)(_buffered_obj + field_offset), _oopmap);\n+  }\n+};\n+\n+static void log_bitmap_usage(const char* which, BitMap* bitmap, size_t total_bits) {\n+  \/\/ The whole heap is covered by total_bits, but there are only non-zero bits within [start ... end).\n+  size_t start = bitmap->find_first_set_bit(0);\n+  size_t end = bitmap->size();\n+  log_info(aot)(\"%s = %7zu ... %7zu (%3zu%% ... %3zu%% = %3zu%%)\", which,\n+                start, end,\n+                start * 100 \/ total_bits,\n+                end * 100 \/ total_bits,\n+                (end - start) * 100 \/ total_bits);\n+}\n+\n+\/\/ Update all oop fields embedded in the buffered objects\n+void AOTStreamedHeapWriter::map_embedded_oops(ArchiveStreamedHeapInfo* heap_info) {\n+  size_t oopmap_unit = (UseCompressedOops ? sizeof(narrowOop) : sizeof(oop));\n+  size_t heap_region_byte_size = _buffer_used;\n+  heap_info->oopmap()->resize(heap_region_byte_size \/ oopmap_unit);\n+\n+  for (int i = 0; i < _source_objs->length(); i++) {\n+    oop src_obj = _source_objs->at(i);\n+    HeapShared::CachedOopInfo* info = HeapShared::get_cached_oop_info(src_obj);\n+    assert(info != nullptr, \"must be\");\n+    address buffered_obj = offset_to_buffered_address<address>(info->buffer_offset());\n+\n+    update_header_for_buffered_addr(buffered_obj, src_obj, src_obj->klass());\n+\n+    EmbeddedOopMapper mapper(src_obj, buffered_obj, heap_info->oopmap());\n+    src_obj->oop_iterate(&mapper);\n+    HeapShared::remap_dumped_metadata(src_obj, buffered_obj);\n+  };\n+\n+  size_t total_bytes = (size_t)_buffer->length();\n+  log_bitmap_usage(\"oopmap\", heap_info->oopmap(), total_bytes \/ oopmap_unit);\n+}\n+\n+size_t AOTStreamedHeapWriter::source_obj_to_buffered_offset(oop src_obj) {\n+  HeapShared::CachedOopInfo* p = HeapShared::get_cached_oop_info(src_obj);\n+  return p->buffer_offset();\n+}\n+\n+address AOTStreamedHeapWriter::source_obj_to_buffered_addr(oop src_obj) {\n+  return offset_to_buffered_address<address>(source_obj_to_buffered_offset(src_obj));\n+}\n+\n+oop AOTStreamedHeapWriter::buffered_offset_to_source_obj(size_t buffered_offset) {\n+  OopHandle* oh = _buffer_offset_to_source_obj_table->get(buffered_offset);\n+  if (oh != nullptr) {\n+    return oh->resolve();\n+  } else {\n+    return nullptr;\n+  }\n+}\n+\n+oop AOTStreamedHeapWriter::buffered_addr_to_source_obj(address buffered_addr) {\n+  return buffered_offset_to_source_obj(buffered_address_to_offset(buffered_addr));\n+}\n+\n+void AOTStreamedHeapWriter::populate_archive_heap_info(ArchiveStreamedHeapInfo* info) {\n+  assert(!info->is_used(), \"only set once\");\n+\n+  size_t heap_region_byte_size = _buffer_used;\n+  assert(heap_region_byte_size > 0, \"must archived at least one object!\");\n+\n+  info->set_buffer_region(MemRegion(offset_to_buffered_address<HeapWord*>(0),\n+                                    offset_to_buffered_address<HeapWord*>(_buffer_used)));\n+  info->set_roots_offset(_roots_offset);\n+  info->set_num_roots((size_t)HeapShared::pending_roots()->length());\n+  info->set_forwarding_offset(_forwarding_offset);\n+  info->set_root_highest_object_index_table_offset(_root_highest_object_index_table_offset);\n+  info->set_num_archived_objects((size_t)_source_objs->length());\n+}\n+\n+AOTMapLogger::OopDataIterator* AOTStreamedHeapWriter::oop_iterator(ArchiveStreamedHeapInfo* heap_info) {\n+  class StreamedWriterOopIterator : public AOTMapLogger::OopDataIterator {\n+  private:\n+    int _current;\n+    int _next;\n+\n+    address _buffer_start;\n+\n+    int _num_archived_objects;\n+    int _num_archived_roots;\n+    int* _roots;\n+\n+  public:\n+    StreamedWriterOopIterator(address buffer_start,\n+                              int num_archived_objects,\n+                              int num_archived_roots,\n+                              int* roots)\n+      : _current(0),\n+        _next(1),\n+        _buffer_start(buffer_start),\n+        _num_archived_objects(num_archived_objects),\n+        _num_archived_roots(num_archived_roots),\n+        _roots(roots) {\n+    }\n+\n+    AOTMapLogger::OopData capture(int dfs_index) {\n+      size_t buffered_offset = _dfs_to_archive_object_table[dfs_index];\n+      address buffered_addr = _buffer_start + buffered_offset;\n+      oop src_obj = AOTStreamedHeapWriter::buffered_offset_to_source_obj(buffered_offset);\n+      assert(src_obj != nullptr, \"why is this null?\");\n+      oopDesc* raw_oop = (oopDesc*)buffered_addr;\n+      Klass* klass = src_obj->klass();\n+      size_t size = src_obj->size();\n+\n+      intptr_t target_location = (intptr_t)buffered_offset;\n+      uint32_t narrow_location = checked_cast<uint32_t>(dfs_index);\n+\n+      address requested_addr = (address)buffered_offset;\n+\n+      return { buffered_addr,\n+               requested_addr,\n+               target_location,\n+               narrow_location,\n+               raw_oop,\n+               klass,\n+               size,\n+               false };\n+    }\n+\n+    bool has_next() override {\n+      return _next <= _num_archived_objects;\n+    }\n+\n+    AOTMapLogger::OopData next() override {\n+      _current = _next;\n+      AOTMapLogger::OopData result = capture(_current);\n+      _next = _current + 1;\n+      return result;\n+    }\n+\n+    AOTMapLogger::OopData obj_at(narrowOop* addr) override {\n+      int dfs_index = (int)(*addr);\n+      if (dfs_index == 0) {\n+        return null_data();\n+      } else {\n+        return capture(dfs_index);\n+      }\n+    }\n+\n+    AOTMapLogger::OopData obj_at(oop* addr) override {\n+      int dfs_index = (int)cast_from_oop<uintptr_t>(*addr);\n+      if (dfs_index == 0) {\n+        return null_data();\n+      } else {\n+        return capture(dfs_index);\n+      }\n+    }\n+\n+    GrowableArrayCHeap<AOTMapLogger::OopData, mtClass>* roots() override {\n+      GrowableArrayCHeap<AOTMapLogger::OopData, mtClass>* result = new GrowableArrayCHeap<AOTMapLogger::OopData, mtClass>();\n+\n+      for (int i = 0; i < _num_archived_roots; ++i) {\n+        int object_index = _roots[i];\n+        result->append(capture(object_index));\n+      }\n+\n+      return result;\n+    }\n+  };\n+\n+  MemRegion r = heap_info->buffer_region();\n+  address buffer_start = address(r.start());\n+\n+  size_t roots_offset = heap_info->roots_offset();\n+  int* roots = ((int*)(buffer_start + roots_offset)) + 1;\n+\n+  return new StreamedWriterOopIterator(buffer_start, (int)heap_info->num_archived_objects(), (int)heap_info->num_roots(), roots);\n+}\n+\n+#endif \/\/ INCLUDE_CDS_JAVA_HEAP\n","filename":"src\/hotspot\/share\/cds\/aotStreamedHeapWriter.cpp","additions":614,"deletions":0,"binary":false,"changes":614,"status":"added"},{"patch":"@@ -0,0 +1,162 @@\n+\/*\n+ * Copyright (c) 2025, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef SHARE_CDS_AOTSTREAMEDHEAPWRITER_HPP\n+#define SHARE_CDS_AOTSTREAMEDHEAPWRITER_HPP\n+\n+#include \"cds\/aotMapLogger.hpp\"\n+#include \"cds\/heapShared.hpp\"\n+#include \"memory\/allocation.hpp\"\n+#include \"memory\/allStatic.hpp\"\n+#include \"oops\/oopHandle.hpp\"\n+#include \"utilities\/bitMap.hpp\"\n+#include \"utilities\/exceptions.hpp\"\n+#include \"utilities\/growableArray.hpp\"\n+#include \"utilities\/macros.hpp\"\n+#include \"utilities\/resizableHashTable.hpp\"\n+\n+class MemRegion;\n+\n+#if INCLUDE_CDS_JAVA_HEAP\n+class AOTStreamedHeapWriter : AllStatic {\n+  class EmbeddedOopMapper;\n+  static GrowableArrayCHeap<u1, mtClassShared>* _buffer;\n+\n+  \/\/ The number of bytes that have written into _buffer (may be smaller than _buffer->length()).\n+  static size_t _buffer_used;\n+\n+  \/\/ The bottom of the copy of Heap::roots() inside this->_buffer.\n+  static size_t _roots_offset;\n+\n+  \/\/ Offset to the forwarding information\n+  static size_t _forwarding_offset;\n+\n+  \/\/ Offset to dfs bounds information\n+  static size_t _root_highest_object_index_table_offset;\n+\n+  static GrowableArrayCHeap<oop, mtClassShared>* _source_objs;\n+\n+  typedef ResizeableHashTable<size_t, OopHandle,\n+                              AnyObj::C_HEAP,\n+                              mtClassShared> BufferOffsetToSourceObjectTable;\n+\n+  static BufferOffsetToSourceObjectTable* _buffer_offset_to_source_obj_table;\n+\n+  typedef ResizeableHashTable<void*, int,\n+                              AnyObj::C_HEAP,\n+                              mtClassShared> SourceObjectToDFSOrderTable;\n+  static SourceObjectToDFSOrderTable* _dfs_order_table;\n+\n+  static int* _roots_highest_dfs;\n+  static size_t* _dfs_to_archive_object_table;\n+\n+  static int cmp_dfs_order(oop* o1, oop* o2);\n+\n+  static void allocate_buffer();\n+  static void ensure_buffer_space(size_t min_bytes);\n+\n+  \/\/ Both Java bytearray and GrowableArraty use int indices and lengths. Do a safe typecast with range check\n+  static int to_array_index(size_t i) {\n+    assert(i <= (size_t)max_jint, \"must be\");\n+    return (int)i;\n+  }\n+  static int to_array_length(size_t n) {\n+    return to_array_index(n);\n+  }\n+\n+  template <typename T> static T offset_to_buffered_address(size_t offset) {\n+    return (T)(_buffer->adr_at(to_array_index(offset)));\n+  }\n+\n+  static address buffer_bottom() {\n+    return offset_to_buffered_address<address>(0);\n+  }\n+\n+  \/\/ The exclusive end of the last object that was copied into the buffer.\n+  static address buffer_top() {\n+    return buffer_bottom() + _buffer_used;\n+  }\n+\n+  static bool in_buffer(address buffered_addr) {\n+    return (buffer_bottom() <= buffered_addr) && (buffered_addr < buffer_top());\n+  }\n+\n+  static size_t buffered_address_to_offset(address buffered_addr) {\n+    assert(in_buffer(buffered_addr), \"sanity\");\n+    return buffered_addr - buffer_bottom();\n+  }\n+\n+  static void order_source_objs(GrowableArrayCHeap<oop, mtClassShared>* roots);\n+  static void copy_roots_to_buffer(GrowableArrayCHeap<oop, mtClassShared>* roots);\n+  static void copy_source_objs_to_buffer(GrowableArrayCHeap<oop, mtClassShared>* roots);\n+  static size_t copy_one_source_obj_to_buffer(oop src_obj);\n+\n+  template <typename T>\n+  static void write(T value);\n+  static void copy_forwarding_to_buffer();\n+  static void copy_roots_max_dfs_to_buffer(int roots_length);\n+\n+  static void map_embedded_oops(ArchiveStreamedHeapInfo* info);\n+  static bool is_in_requested_range(oop o);\n+  static oop requested_obj_from_buffer_offset(size_t offset);\n+\n+  static oop load_oop_from_buffer(oop* buffered_addr);\n+  static oop load_oop_from_buffer(narrowOop* buffered_addr);\n+  inline static void store_oop_in_buffer(oop* buffered_addr, int dfs_index);\n+  inline static void store_oop_in_buffer(narrowOop* buffered_addr, int dfs_index);\n+\n+  template <typename T> static void mark_oop_pointer(T* buffered_addr, CHeapBitMap* oopmap);\n+  template <typename T> static void map_oop_field_in_buffer(oop obj, T* field_addr_in_buffer, CHeapBitMap* oopmap);\n+\n+  static void update_header_for_buffered_addr(address buffered_addr, oop src_obj, Klass* src_klass);\n+\n+  static void populate_archive_heap_info(ArchiveStreamedHeapInfo* info);\n+\n+public:\n+  static void init() NOT_CDS_JAVA_HEAP_RETURN;\n+\n+  static void delete_tables_with_raw_oops();\n+  static void add_source_obj(oop src_obj);\n+  static void write(GrowableArrayCHeap<oop, mtClassShared>*, ArchiveStreamedHeapInfo* heap_info);\n+  static address buffered_heap_roots_addr() {\n+    return offset_to_buffered_address<address>(_roots_offset);\n+  }\n+\n+  static size_t buffered_addr_to_buffered_offset(address buffered_addr) {\n+    assert(buffered_addr != nullptr, \"should not be null\");\n+    return size_t(buffered_addr) - size_t(buffer_bottom());\n+  }\n+\n+  static bool is_dumped_interned_string(oop obj);\n+\n+  static size_t source_obj_to_buffered_offset(oop src_obj);\n+  static address source_obj_to_buffered_addr(oop src_obj);\n+\n+  static oop buffered_offset_to_source_obj(size_t buffered_offset);\n+  static oop buffered_addr_to_source_obj(address buffered_addr);\n+\n+  static AOTMapLogger::OopDataIterator* oop_iterator(ArchiveStreamedHeapInfo* heap_info);\n+};\n+#endif \/\/ INCLUDE_CDS_JAVA_HEAP\n+#endif \/\/ SHARE_CDS_AOTSTREAMEDHEAPWRITER_HPP\n","filename":"src\/hotspot\/share\/cds\/aotStreamedHeapWriter.hpp","additions":162,"deletions":0,"binary":false,"changes":162,"status":"added"},{"patch":"@@ -0,0 +1,114 @@\n+\/*\n+ * Copyright (c) 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#include \"cds\/aotStreamedHeapLoader.hpp\"\n+#include \"cds\/aotThread.hpp\"\n+#include \"cds\/heapShared.hpp\"\n+#include \"classfile\/javaClasses.hpp\"\n+#include \"classfile\/javaThreadStatus.hpp\"\n+#include \"classfile\/vmClasses.hpp\"\n+#include \"classfile\/vmSymbols.hpp\"\n+#include \"jfr\/jfr.hpp\"\n+#include \"runtime\/javaThread.inline.hpp\"\n+#include \"runtime\/mutexLocker.hpp\"\n+#include \"runtime\/osThread.hpp\"\n+#include \"runtime\/thread.hpp\"\n+#include \"runtime\/threads.hpp\"\n+#include \"utilities\/exceptions.hpp\"\n+\n+AOTThread* AOTThread::_aot_thread;\n+bool AOTThread::_started;\n+\n+\/\/ Starting the AOTThread is tricky. We wish to start it as early as possible, as\n+\/\/ that increases the amount of curling this thread can do for the application thread\n+\/\/ that is concurrently starting. But there are complications starting a thread this\n+\/\/ early. The java.lang.Thread class is not initialized and we may not execute any\n+\/\/ Java bytecodes yet. This is an internal thread, so we try to keep the bookkeeping\n+\/\/ minimal and use a logical ThreadIdentifier for JFR and monitor identity. The real\n+\/\/ thread object is created just after the main thread creates its Thread object, after\n+\/\/ the Thread class has been initialized.\n+void AOTThread::initialize() {\n+#if INCLUDE_CDS_JAVA_HEAP\n+  EXCEPTION_MARK;\n+\n+  \/\/ Spin up a thread without thread oop, because the java.lang classes\n+  \/\/ have not yet been initialized, and hence we can't allocate the Thread\n+  \/\/ object yet.\n+  AOTThread* thread = new AOTThread(&aot_thread_entry);\n+  _aot_thread = thread;\n+\n+#if INCLUDE_JVMTI\n+  \/\/ The line below hides JVMTI events from this thread (cf. should_hide_jvmti_events())\n+  \/\/ This is important because this thread runs before JVMTI monitors are set up appropriately.\n+  \/\/ Therefore, callbacks would not work as intended. JVMTI has no business peeking at how we\n+  \/\/ materialize primordial objects from the AOT cache.\n+  thread->toggle_is_disable_suspend();\n+#endif\n+\n+  JavaThread::vm_exit_on_osthread_failure(thread);\n+  _started = true;\n+\n+  \/\/ Note that the Thread class is not initialized yet at this point. We\n+  \/\/ can run a bit concurrently until the Thread class is initialized; then\n+  \/\/ materialize_thread_object is called to inflate the thread object.\n+\n+  \/\/ The thread needs an identifier. This thread is fine with a temporary ID\n+  \/\/ assignment; it will terminate soon anyway.\n+  int64_t tid = ThreadIdentifier::next();\n+  thread->set_monitor_owner_id(tid);\n+\n+  {\n+    MutexLocker mu(THREAD, Threads_lock);\n+    Threads::add(thread);\n+  }\n+\n+  JFR_ONLY(Jfr::on_java_thread_start(THREAD, thread);)\n+\n+  os::start_thread(thread);\n+#endif\n+}\n+\n+void AOTThread::materialize_thread_object() {\n+#if INCLUDE_CDS_JAVA_HEAP\n+  if (!_started) {\n+    \/\/ No thread object to materialize\n+    return;\n+  }\n+\n+  EXCEPTION_MARK;\n+\n+  HandleMark hm(THREAD);\n+  Handle thread_oop = JavaThread::create_system_thread_object(\"AOTThread\", CHECK);\n+\n+  java_lang_Thread::release_set_thread(thread_oop(), _aot_thread);\n+  _aot_thread->set_threadOopHandles(thread_oop());\n+#endif\n+}\n+\n+void AOTThread::aot_thread_entry(JavaThread* jt, TRAPS) {\n+#if INCLUDE_CDS_JAVA_HEAP\n+  AOTStreamedHeapLoader::materialize_objects();\n+  _aot_thread = nullptr; \/\/ AOT thread will get destroyed after this point\n+#endif\n+}\n","filename":"src\/hotspot\/share\/cds\/aotThread.cpp","additions":114,"deletions":0,"binary":false,"changes":114,"status":"added"},{"patch":"@@ -0,0 +1,52 @@\n+\/*\n+ * Copyright (c) 2025, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef SHARE_CDS_AOTTHREAD_HPP\n+#define SHARE_CDS_AOTTHREAD_HPP\n+\n+#include \"runtime\/javaThread.hpp\"\n+#include \"utilities\/macros.hpp\"\n+\n+\/\/ A hidden from external view JavaThread for materializing archived objects\n+\n+class AOTThread : public JavaThread {\n+private:\n+  static bool _started;\n+  static AOTThread* _aot_thread;\n+  static void aot_thread_entry(JavaThread* thread, TRAPS);\n+  AOTThread(ThreadFunction entry_point) : JavaThread(entry_point) {};\n+\n+public:\n+  static void initialize();\n+\n+  \/\/ Hide this thread from external view.\n+  virtual bool is_hidden_from_external_view() const { return true; }\n+\n+  static void materialize_thread_object();\n+\n+  static bool aot_thread_initialized() { return _started; };\n+  bool is_aot_thread() const { return true; };\n+};\n+\n+#endif \/\/ SHARE_CDS_AOTTHREAD_HPP\n","filename":"src\/hotspot\/share\/cds\/aotThread.hpp","additions":52,"deletions":0,"binary":false,"changes":52,"status":"added"},{"patch":"@@ -31,1 +31,0 @@\n-#include \"cds\/archiveHeapWriter.hpp\"\n@@ -1178,1 +1177,1 @@\n-void ArchiveBuilder::write_archive(FileMapInfo* mapinfo, ArchiveHeapInfo* heap_info) {\n+void ArchiveBuilder::write_archive(FileMapInfo* mapinfo, ArchiveMappedHeapInfo* mapped_heap_info, ArchiveStreamedHeapInfo* streamed_heap_info) {\n@@ -1183,0 +1182,2 @@\n+  ResourceMark rm;\n+\n@@ -1191,1 +1192,4 @@\n-  char* bitmap = mapinfo->write_bitmap_region(ArchivePtrMarker::rw_ptrmap(), ArchivePtrMarker::ro_ptrmap(), heap_info,\n+  char* bitmap = mapinfo->write_bitmap_region(ArchivePtrMarker::rw_ptrmap(),\n+                                              ArchivePtrMarker::ro_ptrmap(),\n+                                              mapped_heap_info,\n+                                              streamed_heap_info,\n@@ -1194,2 +1198,4 @@\n-  if (heap_info->is_used()) {\n-    _total_heap_region_size = mapinfo->write_heap_region(heap_info);\n+  if (mapped_heap_info != nullptr && mapped_heap_info->is_used()) {\n+    _total_heap_region_size = mapinfo->write_mapped_heap_region(mapped_heap_info);\n+  } else if (streamed_heap_info != nullptr && streamed_heap_info->is_used()) {\n+    _total_heap_region_size = mapinfo->write_streamed_heap_region(streamed_heap_info);\n@@ -1198,1 +1204,1 @@\n-  print_region_stats(mapinfo, heap_info);\n+  print_region_stats(mapinfo, mapped_heap_info, streamed_heap_info);\n@@ -1213,1 +1219,1 @@\n-    AOTMapLogger::dumptime_log(this, mapinfo, heap_info, bitmap, bitmap_size_in_bytes);\n+    AOTMapLogger::dumptime_log(this, mapinfo, mapped_heap_info, streamed_heap_info, bitmap, bitmap_size_in_bytes);\n@@ -1229,1 +1235,3 @@\n-void ArchiveBuilder::print_region_stats(FileMapInfo *mapinfo, ArchiveHeapInfo* heap_info) {\n+void ArchiveBuilder::print_region_stats(FileMapInfo *mapinfo,\n+                                        ArchiveMappedHeapInfo* mapped_heap_info,\n+                                        ArchiveStreamedHeapInfo* streamed_heap_info) {\n@@ -1247,2 +1255,4 @@\n-  if (heap_info->is_used()) {\n-    print_heap_region_stats(heap_info, total_reserved);\n+  if (mapped_heap_info != nullptr && mapped_heap_info->is_used()) {\n+    print_heap_region_stats(mapped_heap_info->buffer_start(), mapped_heap_info->buffer_byte_size(), total_reserved);\n+  } else if (streamed_heap_info != nullptr && streamed_heap_info->is_used()) {\n+    print_heap_region_stats(streamed_heap_info->buffer_start(), streamed_heap_info->buffer_byte_size(), total_reserved);\n@@ -1252,1 +1262,1 @@\n-                 total_bytes, total_reserved, total_u_perc);\n+                     total_bytes, total_reserved, total_u_perc);\n@@ -1257,1 +1267,1 @@\n-                 size, size\/double(total_size)*100.0, size);\n+                     size, size\/double(total_size)*100.0, size);\n@@ -1260,3 +1270,1 @@\n-void ArchiveBuilder::print_heap_region_stats(ArchiveHeapInfo *info, size_t total_size) {\n-  char* start = info->buffer_start();\n-  size_t size = info->buffer_byte_size();\n+void ArchiveBuilder::print_heap_region_stats(char* start, size_t size, size_t total_size) {\n","filename":"src\/hotspot\/share\/cds\/archiveBuilder.cpp","additions":23,"deletions":15,"binary":false,"changes":38,"status":"modified"},{"patch":"@@ -42,1 +42,2 @@\n-class ArchiveHeapInfo;\n+class ArchiveMappedHeapInfo;\n+class ArchiveStreamedHeapInfo;\n@@ -248,1 +249,3 @@\n-  void print_region_stats(FileMapInfo *map_info, ArchiveHeapInfo* heap_info);\n+  void print_region_stats(FileMapInfo *map_info,\n+                          ArchiveMappedHeapInfo* mapped_heap_info,\n+                          ArchiveStreamedHeapInfo* streamed_heap_info);\n@@ -250,1 +253,1 @@\n-  void print_heap_region_stats(ArchiveHeapInfo* heap_info, size_t total_size);\n+  void print_heap_region_stats(char* start, size_t size, size_t total_size);\n@@ -437,1 +440,3 @@\n-  void write_archive(FileMapInfo* mapinfo, ArchiveHeapInfo* heap_info);\n+  void write_archive(FileMapInfo* mapinfo,\n+                     ArchiveMappedHeapInfo* mapped_heap_info,\n+                     ArchiveStreamedHeapInfo* streamed_heap_info);\n@@ -505,0 +510,1 @@\n+  static void log_as_hex(address base, address top, address requested_base, bool is_heap = false);\n","filename":"src\/hotspot\/share\/cds\/archiveBuilder.hpp","additions":10,"deletions":4,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -1,468 +0,0 @@\n-\/*\n- * Copyright (c) 2018, 2025, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\n- *\/\n-\n-#include \"cds\/aotMetaspace.hpp\"\n-#include \"cds\/archiveHeapLoader.inline.hpp\"\n-#include \"cds\/cdsConfig.hpp\"\n-#include \"cds\/heapShared.hpp\"\n-#include \"classfile\/classLoaderDataShared.hpp\"\n-#include \"classfile\/systemDictionaryShared.hpp\"\n-#include \"gc\/shared\/collectedHeap.hpp\"\n-#include \"logging\/log.hpp\"\n-#include \"memory\/iterator.inline.hpp\"\n-#include \"memory\/resourceArea.hpp\"\n-#include \"memory\/universe.hpp\"\n-#include \"sanitizers\/ub.hpp\"\n-#include \"utilities\/bitMap.inline.hpp\"\n-#include \"utilities\/copy.hpp\"\n-\n-#if INCLUDE_CDS_JAVA_HEAP\n-\n-bool ArchiveHeapLoader::_is_mapped = false;\n-bool ArchiveHeapLoader::_is_loaded = false;\n-\n-bool    ArchiveHeapLoader::_narrow_oop_base_initialized = false;\n-address ArchiveHeapLoader::_narrow_oop_base;\n-int     ArchiveHeapLoader::_narrow_oop_shift;\n-\n-\/\/ Support for loaded heap.\n-uintptr_t ArchiveHeapLoader::_loaded_heap_bottom = 0;\n-uintptr_t ArchiveHeapLoader::_loaded_heap_top = 0;\n-uintptr_t ArchiveHeapLoader::_dumptime_base = UINTPTR_MAX;\n-uintptr_t ArchiveHeapLoader::_dumptime_top = 0;\n-intx ArchiveHeapLoader::_runtime_offset = 0;\n-bool ArchiveHeapLoader::_loading_failed = false;\n-\n-\/\/ Support for mapped heap.\n-uintptr_t ArchiveHeapLoader::_mapped_heap_bottom = 0;\n-bool      ArchiveHeapLoader::_mapped_heap_relocation_initialized = false;\n-ptrdiff_t ArchiveHeapLoader::_mapped_heap_delta = 0;\n-\n-\/\/ Every mapped region is offset by _mapped_heap_delta from its requested address.\n-\/\/ See FileMapInfo::heap_region_requested_address().\n-ATTRIBUTE_NO_UBSAN\n-void ArchiveHeapLoader::init_mapped_heap_info(address mapped_heap_bottom, ptrdiff_t delta, int dumptime_oop_shift) {\n-  assert(!_mapped_heap_relocation_initialized, \"only once\");\n-  if (!UseCompressedOops) {\n-    assert(dumptime_oop_shift == 0, \"sanity\");\n-  }\n-  assert(can_map(), \"sanity\");\n-  init_narrow_oop_decoding(CompressedOops::base() + delta, dumptime_oop_shift);\n-  _mapped_heap_bottom = (intptr_t)mapped_heap_bottom;\n-  _mapped_heap_delta = delta;\n-  _mapped_heap_relocation_initialized = true;\n-}\n-\n-void ArchiveHeapLoader::init_narrow_oop_decoding(address base, int shift) {\n-  assert(!_narrow_oop_base_initialized, \"only once\");\n-  _narrow_oop_base_initialized = true;\n-  _narrow_oop_base = base;\n-  _narrow_oop_shift = shift;\n-}\n-\n-void ArchiveHeapLoader::fixup_region() {\n-  FileMapInfo* mapinfo = FileMapInfo::current_info();\n-  if (is_mapped()) {\n-    mapinfo->fixup_mapped_heap_region();\n-  } else if (_loading_failed) {\n-    fill_failed_loaded_heap();\n-  }\n-  if (is_in_use()) {\n-    if (!CDSConfig::is_using_full_module_graph()) {\n-      \/\/ Need to remove all the archived java.lang.Module objects from HeapShared::roots().\n-      ClassLoaderDataShared::clear_archived_oops();\n-    }\n-  }\n-}\n-\n-\/\/ ------------------ Support for Region MAPPING -----------------------------------------\n-\n-\/\/ Patch all the embedded oop pointers inside an archived heap region,\n-\/\/ to be consistent with the runtime oop encoding.\n-class PatchCompressedEmbeddedPointers: public BitMapClosure {\n-  narrowOop* _start;\n-\n- public:\n-  PatchCompressedEmbeddedPointers(narrowOop* start) : _start(start) {}\n-\n-  bool do_bit(size_t offset) {\n-    narrowOop* p = _start + offset;\n-    narrowOop v = *p;\n-    assert(!CompressedOops::is_null(v), \"null oops should have been filtered out at dump time\");\n-    oop o = ArchiveHeapLoader::decode_from_mapped_archive(v);\n-    RawAccess<IS_NOT_NULL>::oop_store(p, o);\n-    return true;\n-  }\n-};\n-\n-class PatchCompressedEmbeddedPointersQuick: public BitMapClosure {\n-  narrowOop* _start;\n-  uint32_t _delta;\n-\n- public:\n-  PatchCompressedEmbeddedPointersQuick(narrowOop* start, uint32_t delta) : _start(start), _delta(delta) {}\n-\n-  bool do_bit(size_t offset) {\n-    narrowOop* p = _start + offset;\n-    narrowOop v = *p;\n-    assert(!CompressedOops::is_null(v), \"null oops should have been filtered out at dump time\");\n-    narrowOop new_v = CompressedOops::narrow_oop_cast(CompressedOops::narrow_oop_value(v) + _delta);\n-    assert(!CompressedOops::is_null(new_v), \"should never relocate to narrowOop(0)\");\n-#ifdef ASSERT\n-    oop o1 = ArchiveHeapLoader::decode_from_mapped_archive(v);\n-    oop o2 = CompressedOops::decode_not_null(new_v);\n-    assert(o1 == o2, \"quick delta must work\");\n-#endif\n-    RawAccess<IS_NOT_NULL>::oop_store(p, new_v);\n-    return true;\n-  }\n-};\n-\n-class PatchUncompressedEmbeddedPointers: public BitMapClosure {\n-  oop* _start;\n-  intptr_t _delta;\n-\n- public:\n-  PatchUncompressedEmbeddedPointers(oop* start, intx runtime_offset) :\n-    _start(start),\n-    _delta(runtime_offset) {}\n-\n-  PatchUncompressedEmbeddedPointers(oop* start) :\n-    _start(start),\n-    _delta(ArchiveHeapLoader::mapped_heap_delta()) {}\n-\n-  bool do_bit(size_t offset) {\n-    oop* p = _start + offset;\n-    intptr_t dumptime_oop = (intptr_t)((void*)*p);\n-    assert(dumptime_oop != 0, \"null oops should have been filtered out at dump time\");\n-    intptr_t runtime_oop = dumptime_oop + _delta;\n-    RawAccess<IS_NOT_NULL>::oop_store(p, cast_to_oop(runtime_oop));\n-    return true;\n-  }\n-};\n-\n-void ArchiveHeapLoader::patch_compressed_embedded_pointers(BitMapView bm,\n-                                                  FileMapInfo* info,\n-                                                  MemRegion region) {\n-  narrowOop dt_encoded_bottom = info->encoded_heap_region_dumptime_address();\n-  narrowOop rt_encoded_bottom = CompressedOops::encode_not_null(cast_to_oop(region.start()));\n-  log_info(aot)(\"patching heap embedded pointers: narrowOop 0x%8x -> 0x%8x\",\n-                  (uint)dt_encoded_bottom, (uint)rt_encoded_bottom);\n-\n-  \/\/ Optimization: if dumptime shift is the same as runtime shift, we can perform a\n-  \/\/ quick conversion from \"dumptime narrowOop\" -> \"runtime narrowOop\".\n-  narrowOop* patching_start = (narrowOop*)region.start() + FileMapInfo::current_info()->heap_oopmap_start_pos();\n-  if (_narrow_oop_shift == CompressedOops::shift()) {\n-    uint32_t quick_delta = (uint32_t)rt_encoded_bottom - (uint32_t)dt_encoded_bottom;\n-    log_info(aot)(\"heap data relocation quick delta = 0x%x\", quick_delta);\n-    if (quick_delta == 0) {\n-      log_info(aot)(\"heap data relocation unnecessary, quick_delta = 0\");\n-    } else {\n-      PatchCompressedEmbeddedPointersQuick patcher(patching_start, quick_delta);\n-      bm.iterate(&patcher);\n-    }\n-  } else {\n-    log_info(aot)(\"heap data quick relocation not possible\");\n-    PatchCompressedEmbeddedPointers patcher(patching_start);\n-    bm.iterate(&patcher);\n-  }\n-}\n-\n-\/\/ Patch all the non-null pointers that are embedded in the archived heap objects\n-\/\/ in this (mapped) region\n-void ArchiveHeapLoader::patch_embedded_pointers(FileMapInfo* info,\n-                                                MemRegion region, address oopmap,\n-                                                size_t oopmap_size_in_bits) {\n-  BitMapView bm((BitMap::bm_word_t*)oopmap, oopmap_size_in_bits);\n-  if (UseCompressedOops) {\n-    patch_compressed_embedded_pointers(bm, info, region);\n-  } else {\n-    PatchUncompressedEmbeddedPointers patcher((oop*)region.start() + FileMapInfo::current_info()->heap_oopmap_start_pos());\n-    bm.iterate(&patcher);\n-  }\n-}\n-\n-\/\/ ------------------ Support for Region LOADING -----------------------------------------\n-\n-\/\/ The CDS archive remembers each heap object by its address at dump time, but\n-\/\/ the heap object may be loaded at a different address at run time. This structure is used\n-\/\/ to translate the dump time addresses for all objects in FileMapInfo::space_at(region_index)\n-\/\/ to their runtime addresses.\n-struct LoadedArchiveHeapRegion {\n-  int       _region_index;   \/\/ index for FileMapInfo::space_at(index)\n-  size_t    _region_size;    \/\/ number of bytes in this region\n-  uintptr_t _dumptime_base;  \/\/ The dump-time (decoded) address of the first object in this region\n-  intx      _runtime_offset; \/\/ If an object's dump time address P is within in this region, its\n-                             \/\/ runtime address is P + _runtime_offset\n-  uintptr_t top() {\n-    return _dumptime_base + _region_size;\n-  }\n-};\n-\n-void ArchiveHeapLoader::init_loaded_heap_relocation(LoadedArchiveHeapRegion* loaded_region) {\n-  _dumptime_base = loaded_region->_dumptime_base;\n-  _dumptime_top = loaded_region->top();\n-  _runtime_offset = loaded_region->_runtime_offset;\n-}\n-\n-bool ArchiveHeapLoader::can_load() {\n-  return Universe::heap()->can_load_archived_objects();\n-}\n-\n-class ArchiveHeapLoader::PatchLoadedRegionPointers: public BitMapClosure {\n-  narrowOop* _start;\n-  intx _offset;\n-  uintptr_t _base;\n-  uintptr_t _top;\n-\n- public:\n-  PatchLoadedRegionPointers(narrowOop* start, LoadedArchiveHeapRegion* loaded_region)\n-    : _start(start),\n-      _offset(loaded_region->_runtime_offset),\n-      _base(loaded_region->_dumptime_base),\n-      _top(loaded_region->top()) {}\n-\n-  bool do_bit(size_t offset) {\n-    assert(UseCompressedOops, \"PatchLoadedRegionPointers for uncompressed oops is unimplemented\");\n-    narrowOop* p = _start + offset;\n-    narrowOop v = *p;\n-    assert(!CompressedOops::is_null(v), \"null oops should have been filtered out at dump time\");\n-    uintptr_t o = cast_from_oop<uintptr_t>(ArchiveHeapLoader::decode_from_archive(v));\n-    assert(_base <= o && o < _top, \"must be\");\n-\n-    o += _offset;\n-    ArchiveHeapLoader::assert_in_loaded_heap(o);\n-    RawAccess<IS_NOT_NULL>::oop_store(p, cast_to_oop(o));\n-    return true;\n-  }\n-};\n-\n-bool ArchiveHeapLoader::init_loaded_region(FileMapInfo* mapinfo, LoadedArchiveHeapRegion* loaded_region,\n-                                           MemRegion& archive_space) {\n-  size_t total_bytes = 0;\n-  FileMapRegion* r = mapinfo->region_at(AOTMetaspace::hp);\n-  r->assert_is_heap_region();\n-  if (r->used() == 0) {\n-    return false;\n-  }\n-\n-  assert(is_aligned(r->used(), HeapWordSize), \"must be\");\n-  total_bytes += r->used();\n-  loaded_region->_region_index = AOTMetaspace::hp;\n-  loaded_region->_region_size = r->used();\n-  loaded_region->_dumptime_base = (uintptr_t)mapinfo->heap_region_dumptime_address();\n-\n-  assert(is_aligned(total_bytes, HeapWordSize), \"must be\");\n-  size_t word_size = total_bytes \/ HeapWordSize;\n-  HeapWord* buffer = Universe::heap()->allocate_loaded_archive_space(word_size);\n-  if (buffer == nullptr) {\n-    return false;\n-  }\n-\n-  archive_space = MemRegion(buffer, word_size);\n-  _loaded_heap_bottom = (uintptr_t)archive_space.start();\n-  _loaded_heap_top    = _loaded_heap_bottom + total_bytes;\n-\n-  loaded_region->_runtime_offset = _loaded_heap_bottom - loaded_region->_dumptime_base;\n-\n-  return true;\n-}\n-\n-bool ArchiveHeapLoader::load_heap_region_impl(FileMapInfo* mapinfo, LoadedArchiveHeapRegion* loaded_region,\n-                                              uintptr_t load_address) {\n-  uintptr_t bitmap_base = (uintptr_t)mapinfo->map_bitmap_region();\n-  if (bitmap_base == 0) {\n-    _loading_failed = true;\n-    return false; \/\/ OOM or CRC error\n-  }\n-\n-  FileMapRegion* r = mapinfo->region_at(loaded_region->_region_index);\n-  if (!mapinfo->read_region(loaded_region->_region_index, (char*)load_address, r->used(), \/* do_commit = *\/ false)) {\n-    \/\/ There's no easy way to free the buffer, so we will fill it with zero later\n-    \/\/ in fill_failed_loaded_heap(), and it will eventually be GC'ed.\n-    log_warning(aot)(\"Loading of heap region %d has failed. Archived objects are disabled\", loaded_region->_region_index);\n-    _loading_failed = true;\n-    return false;\n-  }\n-  assert(r->mapped_base() == (char*)load_address, \"sanity\");\n-  log_info(aot)(\"Loaded heap    region #%d at base \" INTPTR_FORMAT \" top \" INTPTR_FORMAT\n-                \" size %6zu delta %zd\",\n-                loaded_region->_region_index, load_address, load_address + loaded_region->_region_size,\n-                loaded_region->_region_size, loaded_region->_runtime_offset);\n-\n-  uintptr_t oopmap = bitmap_base + r->oopmap_offset();\n-  BitMapView bm((BitMap::bm_word_t*)oopmap, r->oopmap_size_in_bits());\n-\n-  if (UseCompressedOops) {\n-    PatchLoadedRegionPointers patcher((narrowOop*)load_address + FileMapInfo::current_info()->heap_oopmap_start_pos(), loaded_region);\n-    bm.iterate(&patcher);\n-  } else {\n-    PatchUncompressedEmbeddedPointers patcher((oop*)load_address + FileMapInfo::current_info()->heap_oopmap_start_pos(), loaded_region->_runtime_offset);\n-    bm.iterate(&patcher);\n-  }\n-  return true;\n-}\n-\n-bool ArchiveHeapLoader::load_heap_region(FileMapInfo* mapinfo) {\n-  assert(can_load(), \"loaded heap for must be supported\");\n-  init_narrow_oop_decoding(mapinfo->narrow_oop_base(), mapinfo->narrow_oop_shift());\n-\n-  LoadedArchiveHeapRegion loaded_region;\n-  memset(&loaded_region, 0, sizeof(loaded_region));\n-\n-  MemRegion archive_space;\n-  if (!init_loaded_region(mapinfo, &loaded_region, archive_space)) {\n-    return false;\n-  }\n-\n-  if (!load_heap_region_impl(mapinfo, &loaded_region, (uintptr_t)archive_space.start())) {\n-    assert(_loading_failed, \"must be\");\n-    return false;\n-  }\n-\n-  init_loaded_heap_relocation(&loaded_region);\n-  _is_loaded = true;\n-\n-  return true;\n-}\n-\n-class VerifyLoadedHeapEmbeddedPointers: public BasicOopIterateClosure {\n-  HashTable<uintptr_t, bool>* _table;\n-\n- public:\n-  VerifyLoadedHeapEmbeddedPointers(HashTable<uintptr_t, bool>* table) : _table(table) {}\n-\n-  virtual void do_oop(narrowOop* p) {\n-    \/\/ This should be called before the loaded region is modified, so all the embedded pointers\n-    \/\/ must be null, or must point to a valid object in the loaded region.\n-    narrowOop v = *p;\n-    if (!CompressedOops::is_null(v)) {\n-      oop o = CompressedOops::decode_not_null(v);\n-      uintptr_t u = cast_from_oop<uintptr_t>(o);\n-      ArchiveHeapLoader::assert_in_loaded_heap(u);\n-      guarantee(_table->contains(u), \"must point to beginning of object in loaded archived region\");\n-    }\n-  }\n-  virtual void do_oop(oop* p) {\n-    oop v = *p;\n-    if(v != nullptr) {\n-      uintptr_t u = cast_from_oop<uintptr_t>(v);\n-      ArchiveHeapLoader::assert_in_loaded_heap(u);\n-      guarantee(_table->contains(u), \"must point to beginning of object in loaded archived region\");\n-    }\n-  }\n-};\n-\n-void ArchiveHeapLoader::finish_initialization() {\n-  if (is_loaded()) {\n-    \/\/ These operations are needed only when the heap is loaded (not mapped).\n-    finish_loaded_heap();\n-    if (VerifyArchivedFields > 0) {\n-      verify_loaded_heap();\n-    }\n-  }\n-  if (is_in_use()) {\n-    patch_native_pointers();\n-    intptr_t bottom = is_loaded() ? _loaded_heap_bottom : _mapped_heap_bottom;\n-\n-    \/\/ The heap roots are stored in one or more segments that are laid out consecutively.\n-    \/\/ The size of each segment (except for the last one) is max_size_in_{elems,bytes}.\n-    HeapRootSegments segments = FileMapInfo::current_info()->heap_root_segments();\n-    HeapShared::init_root_segment_sizes(segments.max_size_in_elems());\n-    intptr_t first_segment_addr = bottom + segments.base_offset();\n-    for (size_t c = 0; c < segments.count(); c++) {\n-      oop segment_oop = cast_to_oop(first_segment_addr + (c * segments.max_size_in_bytes()));\n-      assert(segment_oop->is_objArray(), \"Must be\");\n-      HeapShared::add_root_segment((objArrayOop)segment_oop);\n-    }\n-  }\n-}\n-\n-void ArchiveHeapLoader::finish_loaded_heap() {\n-  HeapWord* bottom = (HeapWord*)_loaded_heap_bottom;\n-  HeapWord* top    = (HeapWord*)_loaded_heap_top;\n-\n-  MemRegion archive_space = MemRegion(bottom, top);\n-  Universe::heap()->complete_loaded_archive_space(archive_space);\n-}\n-\n-void ArchiveHeapLoader::verify_loaded_heap() {\n-  log_info(aot, heap)(\"Verify all oops and pointers in loaded heap\");\n-\n-  ResourceMark rm;\n-  HashTable<uintptr_t, bool> table;\n-  VerifyLoadedHeapEmbeddedPointers verifier(&table);\n-  HeapWord* bottom = (HeapWord*)_loaded_heap_bottom;\n-  HeapWord* top    = (HeapWord*)_loaded_heap_top;\n-\n-  for (HeapWord* p = bottom; p < top; ) {\n-    oop o = cast_to_oop(p);\n-    table.put(cast_from_oop<uintptr_t>(o), true);\n-    p += o->size();\n-  }\n-\n-  for (HeapWord* p = bottom; p < top; ) {\n-    oop o = cast_to_oop(p);\n-    o->oop_iterate(&verifier);\n-    p += o->size();\n-  }\n-}\n-\n-void ArchiveHeapLoader::fill_failed_loaded_heap() {\n-  assert(_loading_failed, \"must be\");\n-  if (_loaded_heap_bottom != 0) {\n-    assert(_loaded_heap_top != 0, \"must be\");\n-    HeapWord* bottom = (HeapWord*)_loaded_heap_bottom;\n-    HeapWord* top = (HeapWord*)_loaded_heap_top;\n-    Universe::heap()->fill_with_objects(bottom, top - bottom);\n-  }\n-}\n-\n-class PatchNativePointers: public BitMapClosure {\n-  Metadata** _start;\n-\n- public:\n-  PatchNativePointers(Metadata** start) : _start(start) {}\n-\n-  bool do_bit(size_t offset) {\n-    Metadata** p = _start + offset;\n-    *p = (Metadata*)(address(*p) + AOTMetaspace::relocation_delta());\n-    return true;\n-  }\n-};\n-\n-void ArchiveHeapLoader::patch_native_pointers() {\n-  if (AOTMetaspace::relocation_delta() == 0) {\n-    return;\n-  }\n-\n-  FileMapRegion* r = FileMapInfo::current_info()->region_at(AOTMetaspace::hp);\n-  if (r->mapped_base() != nullptr && r->has_ptrmap()) {\n-    log_info(aot, heap)(\"Patching native pointers in heap region\");\n-    BitMapView bm = FileMapInfo::current_info()->ptrmap_view(AOTMetaspace::hp);\n-    PatchNativePointers patcher((Metadata**)r->mapped_base() + FileMapInfo::current_info()->heap_ptrmap_start_pos());\n-    bm.iterate(&patcher);\n-  }\n-}\n-#endif \/\/ INCLUDE_CDS_JAVA_HEAP\n","filename":"src\/hotspot\/share\/cds\/archiveHeapLoader.cpp","additions":0,"deletions":468,"binary":false,"changes":468,"status":"deleted"},{"patch":"@@ -1,160 +0,0 @@\n-\/*\n- * Copyright (c) 2018, 2025, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\n- *\/\n-\n-#ifndef SHARE_CDS_ARCHIVEHEAPLOADER_HPP\n-#define SHARE_CDS_ARCHIVEHEAPLOADER_HPP\n-\n-#include \"gc\/shared\/gc_globals.hpp\"\n-#include \"memory\/allocation.hpp\"\n-#include \"memory\/allStatic.hpp\"\n-#include \"memory\/memRegion.hpp\"\n-#include \"oops\/oopsHierarchy.hpp\"\n-#include \"runtime\/globals.hpp\"\n-#include \"utilities\/bitMap.hpp\"\n-#include \"utilities\/macros.hpp\"\n-\n-class  FileMapInfo;\n-struct LoadedArchiveHeapRegion;\n-\n-class ArchiveHeapLoader : AllStatic {\n-public:\n-  \/\/ At runtime, the heap region in the CDS archive can be used in two different ways,\n-  \/\/ depending on the GC type:\n-  \/\/ - Mapped: (G1 only) the region is directly mapped into the Java heap\n-  \/\/ - Loaded: At VM start-up, the objects in the heap region are copied into the\n-  \/\/           Java heap. This is easier to implement than mapping but\n-  \/\/           slightly less efficient, as the embedded pointers need to be relocated.\n-  static bool can_use() { return can_map() || can_load(); }\n-\n-  \/\/ Can this VM map archived heap region? Currently only G1+compressed{oops,cp}\n-  static bool can_map() {\n-    CDS_JAVA_HEAP_ONLY(return (UseG1GC && UseCompressedClassPointers);)\n-    NOT_CDS_JAVA_HEAP(return false;)\n-  }\n-\n-  \/\/ Can this VM load the objects from archived heap region into the heap at start-up?\n-  static bool can_load()  NOT_CDS_JAVA_HEAP_RETURN_(false);\n-  static void finish_initialization() NOT_CDS_JAVA_HEAP_RETURN;\n-  static bool is_loaded() {\n-    CDS_JAVA_HEAP_ONLY(return _is_loaded;)\n-    NOT_CDS_JAVA_HEAP(return false;)\n-  }\n-\n-  static bool is_in_use() {\n-    return is_loaded() || is_mapped();\n-  }\n-\n-  static ptrdiff_t mapped_heap_delta() {\n-    CDS_JAVA_HEAP_ONLY(assert(!is_loaded(), \"must be\"));\n-    CDS_JAVA_HEAP_ONLY(assert(_mapped_heap_relocation_initialized, \"must be\"));\n-    CDS_JAVA_HEAP_ONLY(return _mapped_heap_delta;)\n-    NOT_CDS_JAVA_HEAP_RETURN_(0L);\n-  }\n-\n-  static void set_mapped() {\n-    CDS_JAVA_HEAP_ONLY(_is_mapped = true;)\n-    NOT_CDS_JAVA_HEAP_RETURN;\n-  }\n-  static bool is_mapped() {\n-    CDS_JAVA_HEAP_ONLY(return _is_mapped;)\n-    NOT_CDS_JAVA_HEAP_RETURN_(false);\n-  }\n-\n-  \/\/ NarrowOops stored in the CDS archive may use a different encoding scheme\n-  \/\/ than CompressedOops::{base,shift} -- see FileMapInfo::map_heap_region_impl.\n-  \/\/ To decode them, do not use CompressedOops::decode_not_null. Use this\n-  \/\/ function instead.\n-  inline static oop decode_from_archive(narrowOop v) NOT_CDS_JAVA_HEAP_RETURN_(nullptr);\n-\n-  \/\/ More efficient version, but works only when ArchiveHeap is mapped.\n-  inline static oop decode_from_mapped_archive(narrowOop v) NOT_CDS_JAVA_HEAP_RETURN_(nullptr);\n-\n-  static void patch_compressed_embedded_pointers(BitMapView bm,\n-                                                 FileMapInfo* info,\n-                                                 MemRegion region) NOT_CDS_JAVA_HEAP_RETURN;\n-\n-  static void patch_embedded_pointers(FileMapInfo* info,\n-                                      MemRegion region, address oopmap,\n-                                      size_t oopmap_size_in_bits) NOT_CDS_JAVA_HEAP_RETURN;\n-\n-  static void fixup_region() NOT_CDS_JAVA_HEAP_RETURN;\n-\n-#if INCLUDE_CDS_JAVA_HEAP\n-  static void init_mapped_heap_info(address mapped_heap_bottom, ptrdiff_t delta, int dumptime_oop_shift);\n-private:\n-  static bool _is_mapped;\n-  static bool _is_loaded;\n-\n-  \/\/ Support for loaded archived heap. These are cached values from\n-  \/\/ LoadedArchiveHeapRegion's.\n-  static uintptr_t _dumptime_base;\n-  static uintptr_t _dumptime_top;\n-  static intx _runtime_offset;\n-\n-  static uintptr_t _loaded_heap_bottom;\n-  static uintptr_t _loaded_heap_top;\n-  static bool _loading_failed;\n-\n-  \/\/ UseCompressedOops only: Used by decode_from_archive\n-  static bool    _narrow_oop_base_initialized;\n-  static address _narrow_oop_base;\n-  static int     _narrow_oop_shift;\n-\n-  \/\/ is_mapped() only: the mapped address of each region is offset by this amount from\n-  \/\/ their requested address.\n-  static uintptr_t _mapped_heap_bottom;\n-  static ptrdiff_t _mapped_heap_delta;\n-  static bool      _mapped_heap_relocation_initialized;\n-\n-  static void init_narrow_oop_decoding(address base, int shift);\n-  static bool init_loaded_region(FileMapInfo* mapinfo, LoadedArchiveHeapRegion* loaded_region,\n-                                 MemRegion& archive_space);\n-  static bool load_heap_region_impl(FileMapInfo* mapinfo, LoadedArchiveHeapRegion* loaded_region, uintptr_t buffer);\n-  static void init_loaded_heap_relocation(LoadedArchiveHeapRegion* reloc_info);\n-  static void patch_native_pointers();\n-  static void finish_loaded_heap();\n-  static void verify_loaded_heap();\n-  static void fill_failed_loaded_heap();\n-\n-  static bool is_in_loaded_heap(uintptr_t o) {\n-    return (_loaded_heap_bottom <= o && o < _loaded_heap_top);\n-  }\n-\n-  template<bool IS_MAPPED>\n-  inline static oop decode_from_archive_impl(narrowOop v) NOT_CDS_JAVA_HEAP_RETURN_(nullptr);\n-\n-  class PatchLoadedRegionPointers;\n-  class PatchUncompressedLoadedRegionPointers;\n-\n-public:\n-\n-  static bool load_heap_region(FileMapInfo* mapinfo);\n-  static void assert_in_loaded_heap(uintptr_t o) {\n-    assert(is_in_loaded_heap(o), \"must be\");\n-  }\n-#endif \/\/ INCLUDE_CDS_JAVA_HEAP\n-\n-};\n-\n-#endif \/\/ SHARE_CDS_ARCHIVEHEAPLOADER_HPP\n","filename":"src\/hotspot\/share\/cds\/archiveHeapLoader.hpp","additions":0,"deletions":160,"binary":false,"changes":160,"status":"deleted"},{"patch":"@@ -1,62 +0,0 @@\n-\/*\n- * Copyright (c) 2018, 2023, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\n- *\/\n-\n-#ifndef SHARE_CDS_ARCHIVEHEAPLOADER_INLINE_HPP\n-#define SHARE_CDS_ARCHIVEHEAPLOADER_INLINE_HPP\n-\n-#include \"cds\/archiveHeapLoader.hpp\"\n-\n-#include \"oops\/compressedOops.inline.hpp\"\n-#include \"utilities\/align.hpp\"\n-\n-#if INCLUDE_CDS_JAVA_HEAP\n-\n-template<bool IS_MAPPED>\n-inline oop ArchiveHeapLoader::decode_from_archive_impl(narrowOop v) {\n-  assert(!CompressedOops::is_null(v), \"narrow oop value can never be zero\");\n-  assert(_narrow_oop_base_initialized, \"relocation information must have been initialized\");\n-  uintptr_t p = ((uintptr_t)_narrow_oop_base) + ((uintptr_t)v << _narrow_oop_shift);\n-  if (IS_MAPPED) {\n-    assert(_dumptime_base == UINTPTR_MAX, \"must be\");\n-  } else if (p >= _dumptime_base) {\n-    assert(p < _dumptime_top, \"must be\");\n-    p += _runtime_offset;\n-  }\n-\n-  oop result = cast_to_oop((uintptr_t)p);\n-  assert(is_object_aligned(result), \"address not aligned: \" INTPTR_FORMAT, p2i((void*) result));\n-  return result;\n-}\n-\n-inline oop ArchiveHeapLoader::decode_from_archive(narrowOop v) {\n-  return decode_from_archive_impl<false>(v);\n-}\n-\n-inline oop ArchiveHeapLoader::decode_from_mapped_archive(narrowOop v) {\n-  return decode_from_archive_impl<true>(v);\n-}\n-\n-#endif\n-\n-#endif \/\/ SHARE_CDS_ARCHIVEHEAPLOADER_INLINE_HPP\n","filename":"src\/hotspot\/share\/cds\/archiveHeapLoader.inline.hpp","additions":0,"deletions":62,"binary":false,"changes":62,"status":"deleted"},{"patch":"@@ -1,803 +0,0 @@\n-\/*\n- * Copyright (c) 2023, 2025, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\n- *\/\n-\n-#include \"cds\/aotReferenceObjSupport.hpp\"\n-#include \"cds\/archiveHeapWriter.hpp\"\n-#include \"cds\/cdsConfig.hpp\"\n-#include \"cds\/filemap.hpp\"\n-#include \"cds\/heapShared.hpp\"\n-#include \"cds\/regeneratedClasses.hpp\"\n-#include \"classfile\/javaClasses.hpp\"\n-#include \"classfile\/modules.hpp\"\n-#include \"classfile\/systemDictionary.hpp\"\n-#include \"gc\/shared\/collectedHeap.hpp\"\n-#include \"memory\/iterator.inline.hpp\"\n-#include \"memory\/oopFactory.hpp\"\n-#include \"memory\/universe.hpp\"\n-#include \"oops\/compressedOops.hpp\"\n-#include \"oops\/objArrayOop.inline.hpp\"\n-#include \"oops\/oop.inline.hpp\"\n-#include \"oops\/oopHandle.inline.hpp\"\n-#include \"oops\/typeArrayKlass.hpp\"\n-#include \"oops\/typeArrayOop.hpp\"\n-#include \"runtime\/java.hpp\"\n-#include \"runtime\/mutexLocker.hpp\"\n-#include \"utilities\/bitMap.inline.hpp\"\n-#if INCLUDE_G1GC\n-#include \"gc\/g1\/g1CollectedHeap.hpp\"\n-#include \"gc\/g1\/g1HeapRegion.hpp\"\n-#endif\n-\n-#if INCLUDE_CDS_JAVA_HEAP\n-\n-GrowableArrayCHeap<u1, mtClassShared>* ArchiveHeapWriter::_buffer = nullptr;\n-\n-\/\/ The following are offsets from buffer_bottom()\n-size_t ArchiveHeapWriter::_buffer_used;\n-\n-\/\/ Heap root segments\n-HeapRootSegments ArchiveHeapWriter::_heap_root_segments;\n-\n-address ArchiveHeapWriter::_requested_bottom;\n-address ArchiveHeapWriter::_requested_top;\n-\n-GrowableArrayCHeap<ArchiveHeapWriter::NativePointerInfo, mtClassShared>* ArchiveHeapWriter::_native_pointers;\n-GrowableArrayCHeap<oop, mtClassShared>* ArchiveHeapWriter::_source_objs;\n-GrowableArrayCHeap<ArchiveHeapWriter::HeapObjOrder, mtClassShared>* ArchiveHeapWriter::_source_objs_order;\n-\n-ArchiveHeapWriter::BufferOffsetToSourceObjectTable*\n-  ArchiveHeapWriter::_buffer_offset_to_source_obj_table = nullptr;\n-\n-\n-typedef HashTable<\n-      size_t,    \/\/ offset of a filler from ArchiveHeapWriter::buffer_bottom()\n-      size_t,    \/\/ size of this filler (in bytes)\n-      127,       \/\/ prime number\n-      AnyObj::C_HEAP,\n-      mtClassShared> FillersTable;\n-static FillersTable* _fillers;\n-static int _num_native_ptrs = 0;\n-\n-void ArchiveHeapWriter::init() {\n-  if (CDSConfig::is_dumping_heap()) {\n-    Universe::heap()->collect(GCCause::_java_lang_system_gc);\n-\n-    _buffer_offset_to_source_obj_table = new BufferOffsetToSourceObjectTable(\/*size (prime)*\/36137, \/*max size*\/1 * M);\n-    _fillers = new FillersTable();\n-    _requested_bottom = nullptr;\n-    _requested_top = nullptr;\n-\n-    _native_pointers = new GrowableArrayCHeap<NativePointerInfo, mtClassShared>(2048);\n-    _source_objs = new GrowableArrayCHeap<oop, mtClassShared>(10000);\n-\n-    guarantee(MIN_GC_REGION_ALIGNMENT <= G1HeapRegion::min_region_size_in_words() * HeapWordSize, \"must be\");\n-  }\n-}\n-\n-void ArchiveHeapWriter::delete_tables_with_raw_oops() {\n-  delete _source_objs;\n-  _source_objs = nullptr;\n-}\n-\n-void ArchiveHeapWriter::add_source_obj(oop src_obj) {\n-  _source_objs->append(src_obj);\n-}\n-\n-void ArchiveHeapWriter::write(GrowableArrayCHeap<oop, mtClassShared>* roots,\n-                              ArchiveHeapInfo* heap_info) {\n-  assert(CDSConfig::is_dumping_heap(), \"sanity\");\n-  allocate_buffer();\n-  copy_source_objs_to_buffer(roots);\n-  set_requested_address(heap_info);\n-  relocate_embedded_oops(roots, heap_info);\n-}\n-\n-bool ArchiveHeapWriter::is_too_large_to_archive(oop o) {\n-  return is_too_large_to_archive(o->size());\n-}\n-\n-bool ArchiveHeapWriter::is_string_too_large_to_archive(oop string) {\n-  typeArrayOop value = java_lang_String::value_no_keepalive(string);\n-  return is_too_large_to_archive(value);\n-}\n-\n-bool ArchiveHeapWriter::is_too_large_to_archive(size_t size) {\n-  assert(size > 0, \"no zero-size object\");\n-  assert(size * HeapWordSize > size, \"no overflow\");\n-  static_assert(MIN_GC_REGION_ALIGNMENT > 0, \"must be positive\");\n-\n-  size_t byte_size = size * HeapWordSize;\n-  if (byte_size > size_t(MIN_GC_REGION_ALIGNMENT)) {\n-    return true;\n-  } else {\n-    return false;\n-  }\n-}\n-\n-\/\/ Various lookup functions between source_obj, buffered_obj and requested_obj\n-bool ArchiveHeapWriter::is_in_requested_range(oop o) {\n-  assert(_requested_bottom != nullptr, \"do not call before _requested_bottom is initialized\");\n-  address a = cast_from_oop<address>(o);\n-  return (_requested_bottom <= a && a < _requested_top);\n-}\n-\n-oop ArchiveHeapWriter::requested_obj_from_buffer_offset(size_t offset) {\n-  oop req_obj = cast_to_oop(_requested_bottom + offset);\n-  assert(is_in_requested_range(req_obj), \"must be\");\n-  return req_obj;\n-}\n-\n-oop ArchiveHeapWriter::source_obj_to_requested_obj(oop src_obj) {\n-  assert(CDSConfig::is_dumping_heap(), \"dump-time only\");\n-  HeapShared::CachedOopInfo* p = HeapShared::get_cached_oop_info(src_obj);\n-  if (p != nullptr) {\n-    return requested_obj_from_buffer_offset(p->buffer_offset());\n-  } else {\n-    return nullptr;\n-  }\n-}\n-\n-oop ArchiveHeapWriter::buffered_addr_to_source_obj(address buffered_addr) {\n-  OopHandle* oh = _buffer_offset_to_source_obj_table->get(buffered_address_to_offset(buffered_addr));\n-  if (oh != nullptr) {\n-    return oh->resolve();\n-  } else {\n-    return nullptr;\n-  }\n-}\n-\n-Klass* ArchiveHeapWriter::real_klass_of_buffered_oop(address buffered_addr) {\n-  oop p = buffered_addr_to_source_obj(buffered_addr);\n-  if (p != nullptr) {\n-    return p->klass();\n-  } else if (get_filler_size_at(buffered_addr) > 0) {\n-    return Universe::fillerArrayKlass();\n-  } else {\n-    \/\/ This is one of the root segments\n-    return Universe::objectArrayKlass();\n-  }\n-}\n-\n-size_t ArchiveHeapWriter::size_of_buffered_oop(address buffered_addr) {\n-  oop p = buffered_addr_to_source_obj(buffered_addr);\n-  if (p != nullptr) {\n-    return p->size();\n-  }\n-\n-  size_t nbytes = get_filler_size_at(buffered_addr);\n-  if (nbytes > 0) {\n-    assert((nbytes % BytesPerWord) == 0, \"should be aligned\");\n-    return nbytes \/ BytesPerWord;\n-  }\n-\n-  address hrs = buffer_bottom();\n-  for (size_t seg_idx = 0; seg_idx < _heap_root_segments.count(); seg_idx++) {\n-    nbytes = _heap_root_segments.size_in_bytes(seg_idx);\n-    if (hrs == buffered_addr) {\n-      assert((nbytes % BytesPerWord) == 0, \"should be aligned\");\n-      return nbytes \/ BytesPerWord;\n-    }\n-    hrs += nbytes;\n-  }\n-\n-  ShouldNotReachHere();\n-  return 0;\n-}\n-\n-address ArchiveHeapWriter::buffered_addr_to_requested_addr(address buffered_addr) {\n-  return _requested_bottom + buffered_address_to_offset(buffered_addr);\n-}\n-\n-address ArchiveHeapWriter::requested_address() {\n-  assert(_buffer != nullptr, \"must be initialized\");\n-  return _requested_bottom;\n-}\n-\n-void ArchiveHeapWriter::allocate_buffer() {\n-  int initial_buffer_size = 100000;\n-  _buffer = new GrowableArrayCHeap<u1, mtClassShared>(initial_buffer_size);\n-  _buffer_used = 0;\n-  ensure_buffer_space(1); \/\/ so that buffer_bottom() works\n-}\n-\n-void ArchiveHeapWriter::ensure_buffer_space(size_t min_bytes) {\n-  \/\/ We usually have very small heaps. If we get a huge one it's probably caused by a bug.\n-  guarantee(min_bytes <= max_jint, \"we dont support archiving more than 2G of objects\");\n-  _buffer->at_grow(to_array_index(min_bytes));\n-}\n-\n-objArrayOop ArchiveHeapWriter::allocate_root_segment(size_t offset, int element_count) {\n-  HeapWord* mem = offset_to_buffered_address<HeapWord *>(offset);\n-  memset(mem, 0, objArrayOopDesc::object_size(element_count));\n-\n-  \/\/ The initialization code is copied from MemAllocator::finish and ObjArrayAllocator::initialize.\n-  if (UseCompactObjectHeaders) {\n-    oopDesc::release_set_mark(mem, Universe::objectArrayKlass()->prototype_header());\n-  } else {\n-    oopDesc::set_mark(mem, markWord::prototype());\n-    oopDesc::release_set_klass(mem, Universe::objectArrayKlass());\n-  }\n-  arrayOopDesc::set_length(mem, element_count);\n-  return objArrayOop(cast_to_oop(mem));\n-}\n-\n-void ArchiveHeapWriter::root_segment_at_put(objArrayOop segment, int index, oop root) {\n-  \/\/ Do not use arrayOop->obj_at_put(i, o) as arrayOop is outside the real heap!\n-  if (UseCompressedOops) {\n-    *segment->obj_at_addr<narrowOop>(index) = CompressedOops::encode(root);\n-  } else {\n-    *segment->obj_at_addr<oop>(index) = root;\n-  }\n-}\n-\n-void ArchiveHeapWriter::copy_roots_to_buffer(GrowableArrayCHeap<oop, mtClassShared>* roots) {\n-  \/\/ Depending on the number of classes we are archiving, a single roots array may be\n-  \/\/ larger than MIN_GC_REGION_ALIGNMENT. Roots are allocated first in the buffer, which\n-  \/\/ allows us to chop the large array into a series of \"segments\". Current layout\n-  \/\/ starts with zero or more segments exactly fitting MIN_GC_REGION_ALIGNMENT, and end\n-  \/\/ with a single segment that may be smaller than MIN_GC_REGION_ALIGNMENT.\n-  \/\/ This is simple and efficient. We do not need filler objects anywhere between the segments,\n-  \/\/ or immediately after the last segment. This allows starting the object dump immediately\n-  \/\/ after the roots.\n-\n-  assert((_buffer_used % MIN_GC_REGION_ALIGNMENT) == 0,\n-         \"Pre-condition: Roots start at aligned boundary: %zu\", _buffer_used);\n-\n-  int max_elem_count = ((MIN_GC_REGION_ALIGNMENT - arrayOopDesc::header_size_in_bytes()) \/ heapOopSize);\n-  assert(objArrayOopDesc::object_size(max_elem_count)*HeapWordSize == MIN_GC_REGION_ALIGNMENT,\n-         \"Should match exactly\");\n-\n-  HeapRootSegments segments(_buffer_used,\n-                            roots->length(),\n-                            MIN_GC_REGION_ALIGNMENT,\n-                            max_elem_count);\n-\n-  int root_index = 0;\n-  for (size_t seg_idx = 0; seg_idx < segments.count(); seg_idx++) {\n-    int size_elems = segments.size_in_elems(seg_idx);\n-    size_t size_bytes = segments.size_in_bytes(seg_idx);\n-\n-    size_t oop_offset = _buffer_used;\n-    _buffer_used = oop_offset + size_bytes;\n-    ensure_buffer_space(_buffer_used);\n-\n-    assert((oop_offset % MIN_GC_REGION_ALIGNMENT) == 0,\n-           \"Roots segment %zu start is not aligned: %zu\",\n-           segments.count(), oop_offset);\n-\n-    objArrayOop seg_oop = allocate_root_segment(oop_offset, size_elems);\n-    for (int i = 0; i < size_elems; i++) {\n-      root_segment_at_put(seg_oop, i, roots->at(root_index++));\n-    }\n-\n-    log_info(aot, heap)(\"archived obj root segment [%d] = %zu bytes, obj = \" PTR_FORMAT,\n-                        size_elems, size_bytes, p2i(seg_oop));\n-  }\n-\n-  assert(root_index == roots->length(), \"Post-condition: All roots are handled\");\n-\n-  _heap_root_segments = segments;\n-}\n-\n-\/\/ The goal is to sort the objects in increasing order of:\n-\/\/ - objects that have only oop pointers\n-\/\/ - objects that have both native and oop pointers\n-\/\/ - objects that have only native pointers\n-\/\/ - objects that have no pointers\n-static int oop_sorting_rank(oop o) {\n-  bool has_oop_ptr, has_native_ptr;\n-  HeapShared::get_pointer_info(o, has_oop_ptr, has_native_ptr);\n-\n-  if (has_oop_ptr) {\n-    if (!has_native_ptr) {\n-      return 0;\n-    } else {\n-      return 1;\n-    }\n-  } else {\n-    if (has_native_ptr) {\n-      return 2;\n-    } else {\n-      return 3;\n-    }\n-  }\n-}\n-\n-int ArchiveHeapWriter::compare_objs_by_oop_fields(HeapObjOrder* a, HeapObjOrder* b) {\n-  int rank_a = a->_rank;\n-  int rank_b = b->_rank;\n-\n-  if (rank_a != rank_b) {\n-    return rank_a - rank_b;\n-  } else {\n-    \/\/ If they are the same rank, sort them by their position in the _source_objs array\n-    return a->_index - b->_index;\n-  }\n-}\n-\n-void ArchiveHeapWriter::sort_source_objs() {\n-  log_info(aot)(\"sorting heap objects\");\n-  int len = _source_objs->length();\n-  _source_objs_order = new GrowableArrayCHeap<HeapObjOrder, mtClassShared>(len);\n-\n-  for (int i = 0; i < len; i++) {\n-    oop o = _source_objs->at(i);\n-    int rank = oop_sorting_rank(o);\n-    HeapObjOrder os = {i, rank};\n-    _source_objs_order->append(os);\n-  }\n-  log_info(aot)(\"computed ranks\");\n-  _source_objs_order->sort(compare_objs_by_oop_fields);\n-  log_info(aot)(\"sorting heap objects done\");\n-}\n-\n-void ArchiveHeapWriter::copy_source_objs_to_buffer(GrowableArrayCHeap<oop, mtClassShared>* roots) {\n-  \/\/ There could be multiple root segments, which we want to be aligned by region.\n-  \/\/ Putting them ahead of objects makes sure we waste no space.\n-  copy_roots_to_buffer(roots);\n-\n-  sort_source_objs();\n-  for (int i = 0; i < _source_objs_order->length(); i++) {\n-    int src_obj_index = _source_objs_order->at(i)._index;\n-    oop src_obj = _source_objs->at(src_obj_index);\n-    HeapShared::CachedOopInfo* info = HeapShared::get_cached_oop_info(src_obj);\n-    assert(info != nullptr, \"must be\");\n-    size_t buffer_offset = copy_one_source_obj_to_buffer(src_obj);\n-    info->set_buffer_offset(buffer_offset);\n-\n-    OopHandle handle(Universe::vm_global(), src_obj);\n-    _buffer_offset_to_source_obj_table->put_when_absent(buffer_offset, handle);\n-    _buffer_offset_to_source_obj_table->maybe_grow();\n-\n-    if (java_lang_Module::is_instance(src_obj)) {\n-      Modules::check_archived_module_oop(src_obj);\n-    }\n-  }\n-\n-  log_info(aot)(\"Size of heap region = %zu bytes, %d objects, %d roots, %d native ptrs\",\n-                _buffer_used, _source_objs->length() + 1, roots->length(), _num_native_ptrs);\n-}\n-\n-size_t ArchiveHeapWriter::filler_array_byte_size(int length) {\n-  size_t byte_size = objArrayOopDesc::object_size(length) * HeapWordSize;\n-  return byte_size;\n-}\n-\n-int ArchiveHeapWriter::filler_array_length(size_t fill_bytes) {\n-  assert(is_object_aligned(fill_bytes), \"must be\");\n-  size_t elemSize = (UseCompressedOops ? sizeof(narrowOop) : sizeof(oop));\n-\n-  int initial_length = to_array_length(fill_bytes \/ elemSize);\n-  for (int length = initial_length; length >= 0; length --) {\n-    size_t array_byte_size = filler_array_byte_size(length);\n-    if (array_byte_size == fill_bytes) {\n-      return length;\n-    }\n-  }\n-\n-  ShouldNotReachHere();\n-  return -1;\n-}\n-\n-HeapWord* ArchiveHeapWriter::init_filler_array_at_buffer_top(int array_length, size_t fill_bytes) {\n-  assert(UseCompressedClassPointers, \"Archived heap only supported for compressed klasses\");\n-  Klass* oak = Universe::objectArrayKlass(); \/\/ already relocated to point to archived klass\n-  HeapWord* mem = offset_to_buffered_address<HeapWord*>(_buffer_used);\n-  memset(mem, 0, fill_bytes);\n-  narrowKlass nk = ArchiveBuilder::current()->get_requested_narrow_klass(oak);\n-  if (UseCompactObjectHeaders) {\n-    oopDesc::release_set_mark(mem, markWord::prototype().set_narrow_klass(nk));\n-  } else {\n-    oopDesc::set_mark(mem, markWord::prototype());\n-    cast_to_oop(mem)->set_narrow_klass(nk);\n-  }\n-  arrayOopDesc::set_length(mem, array_length);\n-  return mem;\n-}\n-\n-void ArchiveHeapWriter::maybe_fill_gc_region_gap(size_t required_byte_size) {\n-  \/\/ We fill only with arrays (so we don't need to use a single HeapWord filler if the\n-  \/\/ leftover space is smaller than a zero-sized array object). Therefore, we need to\n-  \/\/ make sure there's enough space of min_filler_byte_size in the current region after\n-  \/\/ required_byte_size has been allocated. If not, fill the remainder of the current\n-  \/\/ region.\n-  size_t min_filler_byte_size = filler_array_byte_size(0);\n-  size_t new_used = _buffer_used + required_byte_size + min_filler_byte_size;\n-\n-  const size_t cur_min_region_bottom = align_down(_buffer_used, MIN_GC_REGION_ALIGNMENT);\n-  const size_t next_min_region_bottom = align_down(new_used, MIN_GC_REGION_ALIGNMENT);\n-\n-  if (cur_min_region_bottom != next_min_region_bottom) {\n-    \/\/ Make sure that no objects span across MIN_GC_REGION_ALIGNMENT. This way\n-    \/\/ we can map the region in any region-based collector.\n-    assert(next_min_region_bottom > cur_min_region_bottom, \"must be\");\n-    assert(next_min_region_bottom - cur_min_region_bottom == MIN_GC_REGION_ALIGNMENT,\n-           \"no buffered object can be larger than %d bytes\",  MIN_GC_REGION_ALIGNMENT);\n-\n-    const size_t filler_end = next_min_region_bottom;\n-    const size_t fill_bytes = filler_end - _buffer_used;\n-    assert(fill_bytes > 0, \"must be\");\n-    ensure_buffer_space(filler_end);\n-\n-    int array_length = filler_array_length(fill_bytes);\n-    log_info(aot, heap)(\"Inserting filler obj array of %d elements (%zu bytes total) @ buffer offset %zu\",\n-                        array_length, fill_bytes, _buffer_used);\n-    HeapWord* filler = init_filler_array_at_buffer_top(array_length, fill_bytes);\n-    _buffer_used = filler_end;\n-    _fillers->put(buffered_address_to_offset((address)filler), fill_bytes);\n-  }\n-}\n-\n-size_t ArchiveHeapWriter::get_filler_size_at(address buffered_addr) {\n-  size_t* p = _fillers->get(buffered_address_to_offset(buffered_addr));\n-  if (p != nullptr) {\n-    assert(*p > 0, \"filler must be larger than zero bytes\");\n-    return *p;\n-  } else {\n-    return 0; \/\/ buffered_addr is not a filler\n-  }\n-}\n-\n-template <typename T>\n-void update_buffered_object_field(address buffered_obj, int field_offset, T value) {\n-  T* field_addr = cast_to_oop(buffered_obj)->field_addr<T>(field_offset);\n-  *field_addr = value;\n-}\n-\n-size_t ArchiveHeapWriter::copy_one_source_obj_to_buffer(oop src_obj) {\n-  assert(!is_too_large_to_archive(src_obj), \"already checked\");\n-  size_t byte_size = src_obj->size() * HeapWordSize;\n-  assert(byte_size > 0, \"no zero-size objects\");\n-\n-  \/\/ For region-based collectors such as G1, the archive heap may be mapped into\n-  \/\/ multiple regions. We need to make sure that we don't have an object that can possible\n-  \/\/ span across two regions.\n-  maybe_fill_gc_region_gap(byte_size);\n-\n-  size_t new_used = _buffer_used + byte_size;\n-  assert(new_used > _buffer_used, \"no wrap around\");\n-\n-  size_t cur_min_region_bottom = align_down(_buffer_used, MIN_GC_REGION_ALIGNMENT);\n-  size_t next_min_region_bottom = align_down(new_used, MIN_GC_REGION_ALIGNMENT);\n-  assert(cur_min_region_bottom == next_min_region_bottom, \"no object should cross minimal GC region boundaries\");\n-\n-  ensure_buffer_space(new_used);\n-\n-  address from = cast_from_oop<address>(src_obj);\n-  address to = offset_to_buffered_address<address>(_buffer_used);\n-  assert(is_object_aligned(_buffer_used), \"sanity\");\n-  assert(is_object_aligned(byte_size), \"sanity\");\n-  memcpy(to, from, byte_size);\n-\n-  \/\/ These native pointers will be restored explicitly at run time.\n-  if (java_lang_Module::is_instance(src_obj)) {\n-    update_buffered_object_field<ModuleEntry*>(to, java_lang_Module::module_entry_offset(), nullptr);\n-  } else if (java_lang_ClassLoader::is_instance(src_obj)) {\n-#ifdef ASSERT\n-    \/\/ We only archive these loaders\n-    if (src_obj != SystemDictionary::java_platform_loader() &&\n-        src_obj != SystemDictionary::java_system_loader()) {\n-      assert(src_obj->klass()->name()->equals(\"jdk\/internal\/loader\/ClassLoaders$BootClassLoader\"), \"must be\");\n-    }\n-#endif\n-    update_buffered_object_field<ClassLoaderData*>(to, java_lang_ClassLoader::loader_data_offset(), nullptr);\n-  }\n-\n-  size_t buffered_obj_offset = _buffer_used;\n-  _buffer_used = new_used;\n-\n-  return buffered_obj_offset;\n-}\n-\n-void ArchiveHeapWriter::set_requested_address(ArchiveHeapInfo* info) {\n-  assert(!info->is_used(), \"only set once\");\n-\n-  size_t heap_region_byte_size = _buffer_used;\n-  assert(heap_region_byte_size > 0, \"must archived at least one object!\");\n-\n-  if (UseCompressedOops) {\n-    if (UseG1GC) {\n-      address heap_end = (address)G1CollectedHeap::heap()->reserved().end();\n-      log_info(aot, heap)(\"Heap end = %p\", heap_end);\n-      _requested_bottom = align_down(heap_end - heap_region_byte_size, G1HeapRegion::GrainBytes);\n-      _requested_bottom = align_down(_requested_bottom, MIN_GC_REGION_ALIGNMENT);\n-      assert(is_aligned(_requested_bottom, G1HeapRegion::GrainBytes), \"sanity\");\n-    } else {\n-      _requested_bottom = align_up(CompressedOops::begin(), MIN_GC_REGION_ALIGNMENT);\n-    }\n-  } else {\n-    \/\/ We always write the objects as if the heap started at this address. This\n-    \/\/ makes the contents of the archive heap deterministic.\n-    \/\/\n-    \/\/ Note that at runtime, the heap address is selected by the OS, so the archive\n-    \/\/ heap will not be mapped at 0x10000000, and the contents need to be patched.\n-    _requested_bottom = align_up((address)NOCOOPS_REQUESTED_BASE, MIN_GC_REGION_ALIGNMENT);\n-  }\n-\n-  assert(is_aligned(_requested_bottom, MIN_GC_REGION_ALIGNMENT), \"sanity\");\n-\n-  _requested_top = _requested_bottom + _buffer_used;\n-\n-  info->set_buffer_region(MemRegion(offset_to_buffered_address<HeapWord*>(0),\n-                                    offset_to_buffered_address<HeapWord*>(_buffer_used)));\n-  info->set_heap_root_segments(_heap_root_segments);\n-}\n-\n-\/\/ Oop relocation\n-\n-template <typename T> T* ArchiveHeapWriter::requested_addr_to_buffered_addr(T* p) {\n-  assert(is_in_requested_range(cast_to_oop(p)), \"must be\");\n-\n-  address addr = address(p);\n-  assert(addr >= _requested_bottom, \"must be\");\n-  size_t offset = addr - _requested_bottom;\n-  return offset_to_buffered_address<T*>(offset);\n-}\n-\n-template <typename T> oop ArchiveHeapWriter::load_source_oop_from_buffer(T* buffered_addr) {\n-  oop o = load_oop_from_buffer(buffered_addr);\n-  assert(!in_buffer(cast_from_oop<address>(o)), \"must point to source oop\");\n-  return o;\n-}\n-\n-template <typename T> void ArchiveHeapWriter::store_requested_oop_in_buffer(T* buffered_addr,\n-                                                                            oop request_oop) {\n-  assert(is_in_requested_range(request_oop), \"must be\");\n-  store_oop_in_buffer(buffered_addr, request_oop);\n-}\n-\n-inline void ArchiveHeapWriter::store_oop_in_buffer(oop* buffered_addr, oop requested_obj) {\n-  *buffered_addr = requested_obj;\n-}\n-\n-inline void ArchiveHeapWriter::store_oop_in_buffer(narrowOop* buffered_addr, oop requested_obj) {\n-  narrowOop val = CompressedOops::encode_not_null(requested_obj);\n-  *buffered_addr = val;\n-}\n-\n-oop ArchiveHeapWriter::load_oop_from_buffer(oop* buffered_addr) {\n-  return *buffered_addr;\n-}\n-\n-oop ArchiveHeapWriter::load_oop_from_buffer(narrowOop* buffered_addr) {\n-  return CompressedOops::decode(*buffered_addr);\n-}\n-\n-template <typename T> void ArchiveHeapWriter::relocate_field_in_buffer(T* field_addr_in_buffer, CHeapBitMap* oopmap) {\n-  oop source_referent = load_source_oop_from_buffer<T>(field_addr_in_buffer);\n-  if (source_referent != nullptr) {\n-    if (java_lang_Class::is_instance(source_referent)) {\n-      Klass* k = java_lang_Class::as_Klass(source_referent);\n-      if (RegeneratedClasses::has_been_regenerated(k)) {\n-        source_referent = RegeneratedClasses::get_regenerated_object(k)->java_mirror();\n-      }\n-      \/\/ When the source object points to a \"real\" mirror, the buffered object should point\n-      \/\/ to the \"scratch\" mirror, which has all unarchivable fields scrubbed (to be reinstated\n-      \/\/ at run time).\n-      source_referent = HeapShared::scratch_java_mirror(source_referent);\n-      assert(source_referent != nullptr, \"must be\");\n-    }\n-    oop request_referent = source_obj_to_requested_obj(source_referent);\n-    store_requested_oop_in_buffer<T>(field_addr_in_buffer, request_referent);\n-    mark_oop_pointer<T>(field_addr_in_buffer, oopmap);\n-  }\n-}\n-\n-template <typename T> void ArchiveHeapWriter::mark_oop_pointer(T* buffered_addr, CHeapBitMap* oopmap) {\n-  T* request_p = (T*)(buffered_addr_to_requested_addr((address)buffered_addr));\n-  address requested_region_bottom;\n-\n-  assert(request_p >= (T*)_requested_bottom, \"sanity\");\n-  assert(request_p <  (T*)_requested_top, \"sanity\");\n-  requested_region_bottom = _requested_bottom;\n-\n-  \/\/ Mark the pointer in the oopmap\n-  T* region_bottom = (T*)requested_region_bottom;\n-  assert(request_p >= region_bottom, \"must be\");\n-  BitMap::idx_t idx = request_p - region_bottom;\n-  assert(idx < oopmap->size(), \"overflow\");\n-  oopmap->set_bit(idx);\n-}\n-\n-void ArchiveHeapWriter::update_header_for_requested_obj(oop requested_obj, oop src_obj,  Klass* src_klass) {\n-  assert(UseCompressedClassPointers, \"Archived heap only supported for compressed klasses\");\n-  narrowKlass nk = ArchiveBuilder::current()->get_requested_narrow_klass(src_klass);\n-  address buffered_addr = requested_addr_to_buffered_addr(cast_from_oop<address>(requested_obj));\n-\n-  oop fake_oop = cast_to_oop(buffered_addr);\n-  if (UseCompactObjectHeaders) {\n-    fake_oop->set_mark(markWord::prototype().set_narrow_klass(nk));\n-  } else {\n-    fake_oop->set_narrow_klass(nk);\n-  }\n-\n-  if (src_obj == nullptr) {\n-    return;\n-  }\n-  \/\/ We need to retain the identity_hash, because it may have been used by some hashtables\n-  \/\/ in the shared heap.\n-  if (!src_obj->fast_no_hash_check()) {\n-    intptr_t src_hash = src_obj->identity_hash();\n-    if (UseCompactObjectHeaders) {\n-      fake_oop->set_mark(markWord::prototype().set_narrow_klass(nk).copy_set_hash(src_hash));\n-    } else {\n-      fake_oop->set_mark(markWord::prototype().copy_set_hash(src_hash));\n-    }\n-    assert(fake_oop->mark().is_unlocked(), \"sanity\");\n-\n-    DEBUG_ONLY(intptr_t archived_hash = fake_oop->identity_hash());\n-    assert(src_hash == archived_hash, \"Different hash codes: original \" INTPTR_FORMAT \", archived \" INTPTR_FORMAT, src_hash, archived_hash);\n-  }\n-  \/\/ Strip age bits.\n-  fake_oop->set_mark(fake_oop->mark().set_age(0));\n-}\n-\n-class ArchiveHeapWriter::EmbeddedOopRelocator: public BasicOopIterateClosure {\n-  oop _src_obj;\n-  address _buffered_obj;\n-  CHeapBitMap* _oopmap;\n-  bool _is_java_lang_ref;\n-public:\n-  EmbeddedOopRelocator(oop src_obj, address buffered_obj, CHeapBitMap* oopmap) :\n-    _src_obj(src_obj), _buffered_obj(buffered_obj), _oopmap(oopmap)\n-  {\n-    _is_java_lang_ref = AOTReferenceObjSupport::check_if_ref_obj(src_obj);\n-  }\n-\n-  void do_oop(narrowOop *p) { EmbeddedOopRelocator::do_oop_work(p); }\n-  void do_oop(      oop *p) { EmbeddedOopRelocator::do_oop_work(p); }\n-\n-private:\n-  template <class T> void do_oop_work(T *p) {\n-    int field_offset = pointer_delta_as_int((char*)p, cast_from_oop<char*>(_src_obj));\n-    T* field_addr = (T*)(_buffered_obj + field_offset);\n-    if (_is_java_lang_ref && AOTReferenceObjSupport::skip_field(field_offset)) {\n-      \/\/ Do not copy these fields. Set them to null\n-      *field_addr = (T)0x0;\n-    } else {\n-      ArchiveHeapWriter::relocate_field_in_buffer<T>(field_addr, _oopmap);\n-    }\n-  }\n-};\n-\n-static void log_bitmap_usage(const char* which, BitMap* bitmap, size_t total_bits) {\n-  \/\/ The whole heap is covered by total_bits, but there are only non-zero bits within [start ... end).\n-  size_t start = bitmap->find_first_set_bit(0);\n-  size_t end = bitmap->size();\n-  log_info(aot)(\"%s = %7zu ... %7zu (%3zu%% ... %3zu%% = %3zu%%)\", which,\n-                start, end,\n-                start * 100 \/ total_bits,\n-                end * 100 \/ total_bits,\n-                (end - start) * 100 \/ total_bits);\n-}\n-\n-\/\/ Update all oop fields embedded in the buffered objects\n-void ArchiveHeapWriter::relocate_embedded_oops(GrowableArrayCHeap<oop, mtClassShared>* roots,\n-                                               ArchiveHeapInfo* heap_info) {\n-  size_t oopmap_unit = (UseCompressedOops ? sizeof(narrowOop) : sizeof(oop));\n-  size_t heap_region_byte_size = _buffer_used;\n-  heap_info->oopmap()->resize(heap_region_byte_size   \/ oopmap_unit);\n-\n-  for (int i = 0; i < _source_objs_order->length(); i++) {\n-    int src_obj_index = _source_objs_order->at(i)._index;\n-    oop src_obj = _source_objs->at(src_obj_index);\n-    HeapShared::CachedOopInfo* info = HeapShared::get_cached_oop_info(src_obj);\n-    assert(info != nullptr, \"must be\");\n-    oop requested_obj = requested_obj_from_buffer_offset(info->buffer_offset());\n-    update_header_for_requested_obj(requested_obj, src_obj, src_obj->klass());\n-    address buffered_obj = offset_to_buffered_address<address>(info->buffer_offset());\n-    EmbeddedOopRelocator relocator(src_obj, buffered_obj, heap_info->oopmap());\n-    src_obj->oop_iterate(&relocator);\n-  };\n-\n-  \/\/ Relocate HeapShared::roots(), which is created in copy_roots_to_buffer() and\n-  \/\/ doesn't have a corresponding src_obj, so we can't use EmbeddedOopRelocator on it.\n-  for (size_t seg_idx = 0; seg_idx < _heap_root_segments.count(); seg_idx++) {\n-    size_t seg_offset = _heap_root_segments.segment_offset(seg_idx);\n-\n-    objArrayOop requested_obj = (objArrayOop)requested_obj_from_buffer_offset(seg_offset);\n-    update_header_for_requested_obj(requested_obj, nullptr, Universe::objectArrayKlass());\n-    address buffered_obj = offset_to_buffered_address<address>(seg_offset);\n-    int length = _heap_root_segments.size_in_elems(seg_idx);\n-\n-    if (UseCompressedOops) {\n-      for (int i = 0; i < length; i++) {\n-        narrowOop* addr = (narrowOop*)(buffered_obj + objArrayOopDesc::obj_at_offset<narrowOop>(i));\n-        relocate_field_in_buffer<narrowOop>(addr, heap_info->oopmap());\n-      }\n-    } else {\n-      for (int i = 0; i < length; i++) {\n-        oop* addr = (oop*)(buffered_obj + objArrayOopDesc::obj_at_offset<oop>(i));\n-        relocate_field_in_buffer<oop>(addr, heap_info->oopmap());\n-      }\n-    }\n-  }\n-\n-  compute_ptrmap(heap_info);\n-\n-  size_t total_bytes = (size_t)_buffer->length();\n-  log_bitmap_usage(\"oopmap\", heap_info->oopmap(), total_bytes \/ (UseCompressedOops ? sizeof(narrowOop) : sizeof(oop)));\n-  log_bitmap_usage(\"ptrmap\", heap_info->ptrmap(), total_bytes \/ sizeof(address));\n-}\n-\n-void ArchiveHeapWriter::mark_native_pointer(oop src_obj, int field_offset) {\n-  Metadata* ptr = src_obj->metadata_field_acquire(field_offset);\n-  if (ptr != nullptr) {\n-    NativePointerInfo info;\n-    info._src_obj = src_obj;\n-    info._field_offset = field_offset;\n-    _native_pointers->append(info);\n-    HeapShared::set_has_native_pointers(src_obj);\n-    _num_native_ptrs ++;\n-  }\n-}\n-\n-void ArchiveHeapWriter::compute_ptrmap(ArchiveHeapInfo* heap_info) {\n-  int num_non_null_ptrs = 0;\n-  Metadata** bottom = (Metadata**) _requested_bottom;\n-  Metadata** top = (Metadata**) _requested_top; \/\/ exclusive\n-  heap_info->ptrmap()->resize(top - bottom);\n-\n-  BitMap::idx_t max_idx = 32; \/\/ paranoid - don't make it too small\n-  for (int i = 0; i < _native_pointers->length(); i++) {\n-    NativePointerInfo info = _native_pointers->at(i);\n-    oop src_obj = info._src_obj;\n-    int field_offset = info._field_offset;\n-    HeapShared::CachedOopInfo* p = HeapShared::get_cached_oop_info(src_obj);\n-    \/\/ requested_field_addr = the address of this field in the requested space\n-    oop requested_obj = requested_obj_from_buffer_offset(p->buffer_offset());\n-    Metadata** requested_field_addr = (Metadata**)(cast_from_oop<address>(requested_obj) + field_offset);\n-    assert(bottom <= requested_field_addr && requested_field_addr < top, \"range check\");\n-\n-    \/\/ Mark this field in the bitmap\n-    BitMap::idx_t idx = requested_field_addr - bottom;\n-    heap_info->ptrmap()->set_bit(idx);\n-    num_non_null_ptrs ++;\n-    max_idx = MAX2(max_idx, idx);\n-\n-    \/\/ Set the native pointer to the requested address of the metadata (at runtime, the metadata will have\n-    \/\/ this address if the RO\/RW regions are mapped at the default location).\n-\n-    Metadata** buffered_field_addr = requested_addr_to_buffered_addr(requested_field_addr);\n-    Metadata* native_ptr = *buffered_field_addr;\n-    guarantee(native_ptr != nullptr, \"sanity\");\n-\n-    if (RegeneratedClasses::has_been_regenerated(native_ptr)) {\n-      native_ptr = RegeneratedClasses::get_regenerated_object(native_ptr);\n-    }\n-\n-    guarantee(ArchiveBuilder::current()->has_been_archived((address)native_ptr),\n-              \"Metadata %p should have been archived\", native_ptr);\n-\n-    address buffered_native_ptr = ArchiveBuilder::current()->get_buffered_addr((address)native_ptr);\n-    address requested_native_ptr = ArchiveBuilder::current()->to_requested(buffered_native_ptr);\n-    *buffered_field_addr = (Metadata*)requested_native_ptr;\n-  }\n-\n-  heap_info->ptrmap()->resize(max_idx + 1);\n-  log_info(aot, heap)(\"calculate_ptrmap: marked %d non-null native pointers for heap region (%zu bits)\",\n-                      num_non_null_ptrs, size_t(heap_info->ptrmap()->size()));\n-}\n-\n-#endif \/\/ INCLUDE_CDS_JAVA_HEAP\n","filename":"src\/hotspot\/share\/cds\/archiveHeapWriter.cpp","additions":0,"deletions":803,"binary":false,"changes":803,"status":"deleted"},{"patch":"@@ -1,247 +0,0 @@\n-\/*\n- * Copyright (c) 2024, 2025, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\n- *\/\n-\n-#ifndef SHARE_CDS_ARCHIVEHEAPWRITER_HPP\n-#define SHARE_CDS_ARCHIVEHEAPWRITER_HPP\n-\n-#include \"cds\/heapShared.hpp\"\n-#include \"memory\/allocation.hpp\"\n-#include \"memory\/allStatic.hpp\"\n-#include \"oops\/oopHandle.hpp\"\n-#include \"utilities\/bitMap.hpp\"\n-#include \"utilities\/exceptions.hpp\"\n-#include \"utilities\/growableArray.hpp\"\n-#include \"utilities\/hashTable.hpp\"\n-#include \"utilities\/macros.hpp\"\n-\n-class MemRegion;\n-\n-class ArchiveHeapInfo {\n-  MemRegion _buffer_region;             \/\/ Contains the archived objects to be written into the CDS archive.\n-  CHeapBitMap _oopmap;\n-  CHeapBitMap _ptrmap;\n-  HeapRootSegments _heap_root_segments;\n-\n-public:\n-  ArchiveHeapInfo() : _buffer_region(), _oopmap(128, mtClassShared), _ptrmap(128, mtClassShared) {}\n-  bool is_used() { return !_buffer_region.is_empty(); }\n-\n-  MemRegion buffer_region() { return _buffer_region; }\n-  void set_buffer_region(MemRegion r) { _buffer_region = r; }\n-\n-  char* buffer_start() { return (char*)_buffer_region.start(); }\n-  size_t buffer_byte_size() { return _buffer_region.byte_size();    }\n-\n-  CHeapBitMap* oopmap() { return &_oopmap; }\n-  CHeapBitMap* ptrmap() { return &_ptrmap; }\n-\n-  void set_heap_root_segments(HeapRootSegments segments) { _heap_root_segments = segments; };\n-  HeapRootSegments heap_root_segments() { return _heap_root_segments; }\n-};\n-\n-#if INCLUDE_CDS_JAVA_HEAP\n-class ArchiveHeapWriter : AllStatic {\n-  \/\/ ArchiveHeapWriter manipulates three types of addresses:\n-  \/\/\n-  \/\/     \"source\" vs \"buffered\" vs \"requested\"\n-  \/\/\n-  \/\/ (Note: the design and convention is the same as for the archiving of Metaspace objects.\n-  \/\/  See archiveBuilder.hpp.)\n-  \/\/\n-  \/\/ - \"source objects\" are regular Java objects allocated during the execution\n-  \/\/   of \"java -Xshare:dump\". They can be used as regular oops.\n-  \/\/\n-  \/\/   Between HeapShared::start_scanning_for_oops() and HeapShared::end_scanning_for_oops(),\n-  \/\/   we recursively search for the oops that need to be stored into the CDS archive.\n-  \/\/   These are entered into HeapShared::archived_object_cache().\n-  \/\/\n-  \/\/ - \"buffered objects\" are copies of the \"source objects\", and are stored in into\n-  \/\/   ArchiveHeapWriter::_buffer, which is a GrowableArray that sits outside of\n-  \/\/   the valid heap range. Therefore we avoid using the addresses of these copies\n-  \/\/   as oops. They are usually called \"buffered_addr\" in the code (of the type \"address\").\n-  \/\/\n-  \/\/   The buffered objects are stored contiguously, possibly with interleaving fillers\n-  \/\/   to make sure no objects span across boundaries of MIN_GC_REGION_ALIGNMENT.\n-  \/\/\n-  \/\/ - Each archived object has a \"requested address\" -- at run time, if the object\n-  \/\/   can be mapped at this address, we can avoid relocation.\n-  \/\/\n-  \/\/ The requested address is implemented differently depending on UseCompressedOops:\n-  \/\/\n-  \/\/ UseCompressedOops == true:\n-  \/\/   The archived objects are stored assuming that the runtime COOPS compression\n-  \/\/   scheme is exactly the same as in dump time (or else a more expensive runtime relocation\n-  \/\/   would be needed.)\n-  \/\/\n-  \/\/   At dump time, we assume that the runtime heap range is exactly the same as\n-  \/\/   in dump time. The requested addresses of the archived objects are chosen such that\n-  \/\/   they would occupy the top end of a G1 heap (TBD when dumping is supported by other\n-  \/\/   collectors. See JDK-8298614).\n-  \/\/\n-  \/\/ UseCompressedOops == false:\n-  \/\/   At runtime, the heap range is usually picked (randomly) by the OS, so we will almost always\n-  \/\/   need to perform relocation. Hence, the goal of the \"requested address\" is to ensure that\n-  \/\/   the contents of the archived objects are deterministic. I.e., the oop fields of archived\n-  \/\/   objects will always point to deterministic addresses.\n-  \/\/\n-  \/\/   For G1, the archived heap is written such that the lowest archived object is placed\n-  \/\/   at NOCOOPS_REQUESTED_BASE. (TBD after JDK-8298614).\n-  \/\/ ----------------------------------------------------------------------\n-\n-public:\n-  static const intptr_t NOCOOPS_REQUESTED_BASE = 0x10000000;\n-\n-  \/\/ The minimum region size of all collectors that are supported by CDS.\n-  \/\/ G1 heap region size can never be smaller than 1M.\n-  \/\/ Shenandoah heap region size can never be smaller than 256K.\n-  static constexpr int MIN_GC_REGION_ALIGNMENT = 256 * K;\n-\n-private:\n-  class EmbeddedOopRelocator;\n-  struct NativePointerInfo {\n-    oop _src_obj;\n-    int _field_offset;\n-  };\n-\n-  static GrowableArrayCHeap<u1, mtClassShared>* _buffer;\n-\n-  \/\/ The number of bytes that have written into _buffer (may be smaller than _buffer->length()).\n-  static size_t _buffer_used;\n-\n-  \/\/ The heap root segments information.\n-  static HeapRootSegments _heap_root_segments;\n-\n-  \/\/ The address range of the requested location of the archived heap objects.\n-  static address _requested_bottom;\n-  static address _requested_top;\n-\n-  static GrowableArrayCHeap<NativePointerInfo, mtClassShared>* _native_pointers;\n-  static GrowableArrayCHeap<oop, mtClassShared>* _source_objs;\n-\n-  \/\/ We sort _source_objs_order to minimize the number of bits in ptrmap and oopmap.\n-  \/\/ See comments near the body of ArchiveHeapWriter::compare_objs_by_oop_fields().\n-  \/\/ The objects will be written in the order of:\n-  \/\/_source_objs->at(_source_objs_order->at(0)._index)\n-  \/\/ source_objs->at(_source_objs_order->at(1)._index)\n-  \/\/ source_objs->at(_source_objs_order->at(2)._index)\n-  \/\/ ...\n-  struct HeapObjOrder {\n-    int _index;    \/\/ The location of this object in _source_objs\n-    int _rank;     \/\/ A lower rank means the object will be written at a lower location.\n-  };\n-  static GrowableArrayCHeap<HeapObjOrder, mtClassShared>* _source_objs_order;\n-\n-  typedef ResizeableHashTable<size_t, OopHandle,\n-      AnyObj::C_HEAP,\n-      mtClassShared> BufferOffsetToSourceObjectTable;\n-  static BufferOffsetToSourceObjectTable* _buffer_offset_to_source_obj_table;\n-\n-  static void allocate_buffer();\n-  static void ensure_buffer_space(size_t min_bytes);\n-\n-  \/\/ Both Java bytearray and GrowableArraty use int indices and lengths. Do a safe typecast with range check\n-  static int to_array_index(size_t i) {\n-    assert(i <= (size_t)max_jint, \"must be\");\n-    return (int)i;\n-  }\n-  static int to_array_length(size_t n) {\n-    return to_array_index(n);\n-  }\n-\n-  template <typename T> static T offset_to_buffered_address(size_t offset) {\n-    return (T)(_buffer->adr_at(to_array_index(offset)));\n-  }\n-\n-  static address buffer_bottom() {\n-    return offset_to_buffered_address<address>(0);\n-  }\n-\n-  \/\/ The exclusive end of the last object that was copied into the buffer.\n-  static address buffer_top() {\n-    return buffer_bottom() + _buffer_used;\n-  }\n-\n-  static bool in_buffer(address buffered_addr) {\n-    return (buffer_bottom() <= buffered_addr) && (buffered_addr < buffer_top());\n-  }\n-\n-  static size_t buffered_address_to_offset(address buffered_addr) {\n-    assert(in_buffer(buffered_addr), \"sanity\");\n-    return buffered_addr - buffer_bottom();\n-  }\n-\n-  static void root_segment_at_put(objArrayOop segment, int index, oop root);\n-  static objArrayOop allocate_root_segment(size_t offset, int element_count);\n-  static void copy_roots_to_buffer(GrowableArrayCHeap<oop, mtClassShared>* roots);\n-  static void copy_source_objs_to_buffer(GrowableArrayCHeap<oop, mtClassShared>* roots);\n-  static size_t copy_one_source_obj_to_buffer(oop src_obj);\n-\n-  static void maybe_fill_gc_region_gap(size_t required_byte_size);\n-  static size_t filler_array_byte_size(int length);\n-  static int filler_array_length(size_t fill_bytes);\n-  static HeapWord* init_filler_array_at_buffer_top(int array_length, size_t fill_bytes);\n-\n-  static void set_requested_address(ArchiveHeapInfo* info);\n-  static void relocate_embedded_oops(GrowableArrayCHeap<oop, mtClassShared>* roots, ArchiveHeapInfo* info);\n-  static void compute_ptrmap(ArchiveHeapInfo *info);\n-  static bool is_in_requested_range(oop o);\n-  static oop requested_obj_from_buffer_offset(size_t offset);\n-\n-  static oop load_oop_from_buffer(oop* buffered_addr);\n-  static oop load_oop_from_buffer(narrowOop* buffered_addr);\n-  inline static void store_oop_in_buffer(oop* buffered_addr, oop requested_obj);\n-  inline static void store_oop_in_buffer(narrowOop* buffered_addr, oop requested_obj);\n-\n-  template <typename T> static oop load_source_oop_from_buffer(T* buffered_addr);\n-  template <typename T> static void store_requested_oop_in_buffer(T* buffered_addr, oop request_oop);\n-\n-  template <typename T> static T* requested_addr_to_buffered_addr(T* p);\n-  template <typename T> static void relocate_field_in_buffer(T* field_addr_in_buffer, CHeapBitMap* oopmap);\n-  template <typename T> static void mark_oop_pointer(T* buffered_addr, CHeapBitMap* oopmap);\n-\n-  static void update_header_for_requested_obj(oop requested_obj, oop src_obj, Klass* src_klass);\n-\n-  static int compare_objs_by_oop_fields(HeapObjOrder* a, HeapObjOrder* b);\n-  static void sort_source_objs();\n-\n-public:\n-  static void init() NOT_CDS_JAVA_HEAP_RETURN;\n-  static void delete_tables_with_raw_oops();\n-  static void add_source_obj(oop src_obj);\n-  static bool is_too_large_to_archive(size_t size);\n-  static bool is_too_large_to_archive(oop obj);\n-  static bool is_string_too_large_to_archive(oop string);\n-  static void write(GrowableArrayCHeap<oop, mtClassShared>*, ArchiveHeapInfo* heap_info);\n-  static address requested_address();  \/\/ requested address of the lowest achived heap object\n-  static size_t get_filler_size_at(address buffered_addr);\n-\n-  static void mark_native_pointer(oop src_obj, int offset);\n-  static oop source_obj_to_requested_obj(oop src_obj);\n-  static oop buffered_addr_to_source_obj(address buffered_addr);\n-  static address buffered_addr_to_requested_addr(address buffered_addr);\n-  static Klass* real_klass_of_buffered_oop(address buffered_addr);\n-  static size_t size_of_buffered_oop(address buffered_addr);\n-};\n-#endif \/\/ INCLUDE_CDS_JAVA_HEAP\n-#endif \/\/ SHARE_CDS_ARCHIVEHEAPWRITER_HPP\n","filename":"src\/hotspot\/share\/cds\/archiveHeapWriter.hpp","additions":0,"deletions":247,"binary":false,"changes":247,"status":"deleted"},{"patch":"@@ -28,1 +28,0 @@\n-#include \"cds\/archiveHeapLoader.inline.hpp\"\n","filename":"src\/hotspot\/share\/cds\/archiveUtils.cpp","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -27,1 +27,0 @@\n-#include \"cds\/archiveHeapLoader.hpp\"\n@@ -31,1 +30,1 @@\n-#include \"cds\/heapShared.hpp\"\n+#include \"cds\/heapShared.inline.hpp\"\n@@ -896,5 +895,0 @@\n-  \/\/ Almost all GCs support heap region dump, except ZGC (so far).\n-  if (UseZGC) {\n-    return \"ZGC is not supported\";\n-  }\n-\n@@ -972,1 +966,1 @@\n-  return ArchiveHeapLoader::is_in_use();\n+  return HeapShared::is_archived_heap_in_use();\n@@ -984,1 +978,1 @@\n-  if (is_using_archive() && ArchiveHeapLoader::can_use()) {\n+  if (is_using_archive() && HeapShared::can_use_archived_heap()) {\n","filename":"src\/hotspot\/share\/cds\/cdsConfig.cpp","additions":3,"deletions":9,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -25,1 +25,0 @@\n-#include \"cds\/archiveHeapLoader.hpp\"\n@@ -27,1 +26,1 @@\n-#include \"cds\/heapShared.hpp\"\n+#include \"cds\/heapShared.inline.hpp\"\n@@ -112,1 +111,1 @@\n-  if (!ArchiveHeapLoader::is_in_use()) {\n+  if (!HeapShared::is_archived_heap_in_use()) {\n@@ -124,1 +123,0 @@\n-  oop mirror = k->java_mirror();\n@@ -131,1 +129,2 @@\n-      mirror->obj_field_put(fd.offset(), HeapShared::get_root(root_index, \/*clear=*\/true));\n+      oop root_object = HeapShared::get_root(root_index, \/*clear=*\/true);\n+      k->java_mirror()->obj_field_put(fd.offset(), root_object);\n","filename":"src\/hotspot\/share\/cds\/cdsEnumKlass.cpp","additions":4,"deletions":5,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -31,0 +31,1 @@\n+#include \"classfile\/stringTable.hpp\"\n@@ -294,1 +295,1 @@\n-    return true; \/* keep on iterating *\/\n+    return true;\n","filename":"src\/hotspot\/share\/cds\/cdsHeapVerifier.cpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -79,0 +79,6 @@\n+  product(bool, AOTStreamableObjects, false, DIAGNOSTIC,                    \\\n+          \"Archive the Java heap in a generic streamable object format\")    \\\n+                                                                            \\\n+  product(bool, AOTEagerlyLoadObjects, false, DIAGNOSTIC,                   \\\n+          \"Load streamable objects synchronously without concurrency\")      \\\n+                                                                            \\\n","filename":"src\/hotspot\/share\/cds\/cds_globals.hpp","additions":6,"deletions":0,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -31,1 +31,0 @@\n-#include \"cds\/archiveHeapWriter.hpp\"\n@@ -36,0 +35,1 @@\n+#include \"cds\/heapShared.hpp\"\n@@ -356,2 +356,1 @@\n-  ArchiveHeapInfo no_heap_for_dynamic_dump;\n-  ArchiveBuilder::write_archive(dynamic_info, &no_heap_for_dynamic_dump);\n+  ArchiveBuilder::write_archive(dynamic_info, nullptr, nullptr);\n","filename":"src\/hotspot\/share\/cds\/dynamicArchive.cpp","additions":2,"deletions":3,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -27,0 +27,2 @@\n+#include \"cds\/aotMappedHeapLoader.hpp\"\n+#include \"cds\/aotMappedHeapWriter.hpp\"\n@@ -29,2 +31,0 @@\n-#include \"cds\/archiveHeapLoader.inline.hpp\"\n-#include \"cds\/archiveHeapWriter.hpp\"\n@@ -36,1 +36,1 @@\n-#include \"cds\/heapShared.hpp\"\n+#include \"cds\/heapShared.inline.hpp\"\n@@ -221,0 +221,1 @@\n+    _object_streaming_mode = HeapShared::is_writing_streaming_mode();\n@@ -287,31 +288,39 @@\n-  st->print_cr(\"- core_region_alignment:          %zu\", _core_region_alignment);\n-  st->print_cr(\"- obj_alignment:                  %d\", _obj_alignment);\n-  st->print_cr(\"- narrow_oop_base:                \" INTPTR_FORMAT, p2i(_narrow_oop_base));\n-  st->print_cr(\"- narrow_oop_shift                %d\", _narrow_oop_shift);\n-  st->print_cr(\"- compact_strings:                %d\", _compact_strings);\n-  st->print_cr(\"- compact_headers:                %d\", _compact_headers);\n-  st->print_cr(\"- max_heap_size:                  %zu\", _max_heap_size);\n-  st->print_cr(\"- narrow_oop_mode:                %d\", _narrow_oop_mode);\n-  st->print_cr(\"- compressed_oops:                %d\", _compressed_oops);\n-  st->print_cr(\"- compressed_class_ptrs:          %d\", _compressed_class_ptrs);\n-  st->print_cr(\"- narrow_klass_pointer_bits:      %d\", _narrow_klass_pointer_bits);\n-  st->print_cr(\"- narrow_klass_shift:             %d\", _narrow_klass_shift);\n-  st->print_cr(\"- cloned_vtables_offset:          0x%zx\", _cloned_vtables_offset);\n-  st->print_cr(\"- early_serialized_data_offset:   0x%zx\", _early_serialized_data_offset);\n-  st->print_cr(\"- serialized_data_offset:         0x%zx\", _serialized_data_offset);\n-  st->print_cr(\"- jvm_ident:                      %s\", _jvm_ident);\n-  st->print_cr(\"- class_location_config_offset:   0x%zx\", _class_location_config_offset);\n-  st->print_cr(\"- verify_local:                   %d\", _verify_local);\n-  st->print_cr(\"- verify_remote:                  %d\", _verify_remote);\n-  st->print_cr(\"- has_platform_or_app_classes:    %d\", _has_platform_or_app_classes);\n-  st->print_cr(\"- requested_base_address:         \" INTPTR_FORMAT, p2i(_requested_base_address));\n-  st->print_cr(\"- mapped_base_address:            \" INTPTR_FORMAT, p2i(_mapped_base_address));\n-  st->print_cr(\"- heap_root_segments.roots_count: %d\" , _heap_root_segments.roots_count());\n-  st->print_cr(\"- heap_root_segments.base_offset: 0x%zx\", _heap_root_segments.base_offset());\n-  st->print_cr(\"- heap_root_segments.count:       %zu\", _heap_root_segments.count());\n-  st->print_cr(\"- heap_root_segments.max_size_elems: %d\", _heap_root_segments.max_size_in_elems());\n-  st->print_cr(\"- heap_root_segments.max_size_bytes: %d\", _heap_root_segments.max_size_in_bytes());\n-  st->print_cr(\"- _heap_oopmap_start_pos:         %zu\", _heap_oopmap_start_pos);\n-  st->print_cr(\"- _heap_ptrmap_start_pos:         %zu\", _heap_ptrmap_start_pos);\n-  st->print_cr(\"- _rw_ptrmap_start_pos:           %zu\", _rw_ptrmap_start_pos);\n-  st->print_cr(\"- _ro_ptrmap_start_pos:           %zu\", _ro_ptrmap_start_pos);\n+  st->print_cr(\"- core_region_alignment:                    %zu\", _core_region_alignment);\n+  st->print_cr(\"- obj_alignment:                            %d\", _obj_alignment);\n+  st->print_cr(\"- narrow_oop_base:                          \" INTPTR_FORMAT, p2i(_narrow_oop_base));\n+  st->print_cr(\"- narrow_oop_shift                          %d\", _narrow_oop_shift);\n+  st->print_cr(\"- compact_strings:                          %d\", _compact_strings);\n+  st->print_cr(\"- compact_headers:                          %d\", _compact_headers);\n+  st->print_cr(\"- max_heap_size:                            %zu\", _max_heap_size);\n+  st->print_cr(\"- narrow_oop_mode:                          %d\", _narrow_oop_mode);\n+  st->print_cr(\"- compressed_oops:                          %d\", _compressed_oops);\n+  st->print_cr(\"- compressed_class_ptrs:                    %d\", _compressed_class_ptrs);\n+  st->print_cr(\"- narrow_klass_pointer_bits:                %d\", _narrow_klass_pointer_bits);\n+  st->print_cr(\"- narrow_klass_shift:                       %d\", _narrow_klass_shift);\n+  st->print_cr(\"- cloned_vtables_offset:                    0x%zx\", _cloned_vtables_offset);\n+  st->print_cr(\"- early_serialized_data_offset:             0x%zx\", _early_serialized_data_offset);\n+  st->print_cr(\"- serialized_data_offset:                   0x%zx\", _serialized_data_offset);\n+  st->print_cr(\"- jvm_ident:                                %s\", _jvm_ident);\n+  st->print_cr(\"- class_location_config_offset:             0x%zx\", _class_location_config_offset);\n+  st->print_cr(\"- verify_local:                             %d\", _verify_local);\n+  st->print_cr(\"- verify_remote:                            %d\", _verify_remote);\n+  st->print_cr(\"- has_platform_or_app_classes:              %d\", _has_platform_or_app_classes);\n+  st->print_cr(\"- requested_base_address:                   \" INTPTR_FORMAT, p2i(_requested_base_address));\n+  st->print_cr(\"- mapped_base_address:                      \" INTPTR_FORMAT, p2i(_mapped_base_address));\n+\n+  st->print_cr(\"- object_streaming_mode:                    %d\", _object_streaming_mode);\n+  st->print_cr(\"- mapped_heap_header\");\n+  st->print_cr(\"  - root_segments\");\n+  st->print_cr(\"    - roots_count:                          %d\", _mapped_heap_header.root_segments().roots_count());\n+  st->print_cr(\"    - base_offset:                          0x%zx\", _mapped_heap_header.root_segments().base_offset());\n+  st->print_cr(\"    - count:                                %zu\", _mapped_heap_header.root_segments().count());\n+  st->print_cr(\"    - max_size_elems:                       %d\", _mapped_heap_header.root_segments().max_size_in_elems());\n+  st->print_cr(\"    - max_size_bytes:                       %d\", _mapped_heap_header.root_segments().max_size_in_bytes());\n+  st->print_cr(\"  - oopmap_start_pos:                       %zu\", _mapped_heap_header.oopmap_start_pos());\n+  st->print_cr(\"  - oopmap_ptrmap_pos:                      %zu\", _mapped_heap_header.ptrmap_start_pos());\n+  st->print_cr(\"- streamed_heap_header\");\n+  st->print_cr(\"  - forwarding_offset:                      %zu\", _streamed_heap_header.forwarding_offset());\n+  st->print_cr(\"  - roots_offset:                           %zu\", _streamed_heap_header.roots_offset());\n+  st->print_cr(\"  - num_roots:                              %zu\", _streamed_heap_header.num_roots());\n+  st->print_cr(\"  - root_highest_object_index_table_offset: %zu\", _streamed_heap_header.root_highest_object_index_table_offset());\n+  st->print_cr(\"  - num_archived_objects:                   %zu\", _streamed_heap_header.num_archived_objects());\n@@ -900,4 +909,6 @@\n-    requested_base = (char*)ArchiveHeapWriter::requested_address();\n-    if (UseCompressedOops) {\n-      mapping_offset = (size_t)((address)requested_base - CompressedOops::base());\n-      assert((mapping_offset >> CompressedOops::shift()) << CompressedOops::shift() == mapping_offset, \"must be\");\n+    if (HeapShared::is_writing_mapping_mode()) {\n+      requested_base = (char*)AOTMappedHeapWriter::requested_address();\n+      if (UseCompressedOops) {\n+        mapping_offset = (size_t)((address)requested_base - CompressedOops::base());\n+        assert((mapping_offset >> CompressedOops::shift()) << CompressedOops::shift() == mapping_offset, \"must be\");\n+      }\n@@ -905,1 +916,1 @@\n-      mapping_offset = 0; \/\/ not used with !UseCompressedOops\n+      requested_base = nullptr;\n@@ -958,1 +969,4 @@\n-char* FileMapInfo::write_bitmap_region(CHeapBitMap* rw_ptrmap, CHeapBitMap* ro_ptrmap, ArchiveHeapInfo* heap_info,\n+char* FileMapInfo::write_bitmap_region(CHeapBitMap* rw_ptrmap,\n+                                       CHeapBitMap* ro_ptrmap,\n+                                       ArchiveMappedHeapInfo* mapped_heap_info,\n+                                       ArchiveStreamedHeapInfo* streamed_heap_info,\n@@ -966,1 +980,1 @@\n-  if (heap_info->is_used()) {\n+  if (mapped_heap_info != nullptr && mapped_heap_info->is_used()) {\n@@ -968,4 +982,10 @@\n-    size_t removed_oop_leading_zeros = remove_bitmap_zeros(heap_info->oopmap());\n-    size_t removed_ptr_leading_zeros = remove_bitmap_zeros(heap_info->ptrmap());\n-    header()->set_heap_oopmap_start_pos(removed_oop_leading_zeros);\n-    header()->set_heap_ptrmap_start_pos(removed_ptr_leading_zeros);\n+    assert(HeapShared::is_writing_mapping_mode(), \"unexpected dumping mode\");\n+    size_t removed_oop_leading_zeros = remove_bitmap_zeros(mapped_heap_info->oopmap());\n+    size_t removed_ptr_leading_zeros = remove_bitmap_zeros(mapped_heap_info->ptrmap());\n+    mapped_heap_info->set_oopmap_start_pos(removed_oop_leading_zeros);\n+    mapped_heap_info->set_ptrmap_start_pos(removed_ptr_leading_zeros);\n+\n+    size_in_bytes += mapped_heap_info->oopmap()->size_in_bytes();\n+    size_in_bytes += mapped_heap_info->ptrmap()->size_in_bytes();\n+  } else if (streamed_heap_info != nullptr && streamed_heap_info->is_used()) {\n+    assert(HeapShared::is_writing_streaming_mode(), \"unexpected dumping mode\");\n@@ -973,2 +993,1 @@\n-    size_in_bytes += heap_info->oopmap()->size_in_bytes();\n-    size_in_bytes += heap_info->ptrmap()->size_in_bytes();\n+    size_in_bytes += streamed_heap_info->oopmap()->size_in_bytes();\n@@ -978,4 +997,4 @@\n-  \/\/ rw_ptrmap:           metaspace pointers inside the read-write region\n-  \/\/ ro_ptrmap:           metaspace pointers inside the read-only region\n-  \/\/ heap_info->oopmap(): Java oop pointers in the heap region\n-  \/\/ heap_info->ptrmap(): metaspace pointers in the heap region\n+  \/\/ rw_ptrmap:                  metaspace pointers inside the read-write region\n+  \/\/ ro_ptrmap:                  metaspace pointers inside the read-only region\n+  \/\/ *_heap_info->oopmap():      Java oop pointers in the heap region\n+  \/\/ mapped_heap_info->ptrmap(): metaspace pointers in the heap region\n@@ -991,1 +1010,2 @@\n-  if (heap_info->is_used()) {\n+  if (mapped_heap_info != nullptr && mapped_heap_info->is_used()) {\n+    assert(HeapShared::is_writing_mapping_mode(), \"unexpected dumping mode\");\n@@ -994,2 +1014,8 @@\n-    r->init_oopmap(written, heap_info->oopmap()->size());\n-    written = write_bitmap(heap_info->oopmap(), buffer, written);\n+    r->init_oopmap(written, mapped_heap_info->oopmap()->size());\n+    written = write_bitmap(mapped_heap_info->oopmap(), buffer, written);\n+\n+    r->init_ptrmap(written, mapped_heap_info->ptrmap()->size());\n+    written = write_bitmap(mapped_heap_info->ptrmap(), buffer, written);\n+  } else if (streamed_heap_info != nullptr && streamed_heap_info->is_used()) {\n+    assert(HeapShared::is_writing_streaming_mode(), \"unexpected dumping mode\");\n+    FileMapRegion* r = region_at(AOTMetaspace::hp);\n@@ -997,2 +1023,2 @@\n-    r->init_ptrmap(written, heap_info->ptrmap()->size());\n-    written = write_bitmap(heap_info->ptrmap(), buffer, written);\n+    r->init_oopmap(written, streamed_heap_info->oopmap()->size());\n+    written = write_bitmap(streamed_heap_info->oopmap(), buffer, written);\n@@ -1005,1 +1031,2 @@\n-size_t FileMapInfo::write_heap_region(ArchiveHeapInfo* heap_info) {\n+#if INCLUDE_CDS_JAVA_HEAP\n+size_t FileMapInfo::write_mapped_heap_region(ArchiveMappedHeapInfo* heap_info) {\n@@ -1009,1 +1036,9 @@\n-  header()->set_heap_root_segments(heap_info->heap_root_segments());\n+  header()->set_mapped_heap_header(heap_info->create_header());\n+  return buffer_size;\n+}\n+\n+size_t FileMapInfo::write_streamed_heap_region(ArchiveStreamedHeapInfo* heap_info) {\n+  char* buffer_start = heap_info->buffer_start();\n+  size_t buffer_size = heap_info->buffer_byte_size();\n+  write_region(AOTMetaspace::hp, buffer_start, buffer_size, true, false);\n+  header()->set_streamed_heap_header(heap_info->create_header());\n@@ -1012,0 +1047,1 @@\n+#endif \/\/ INCLUDE_CDS_JAVA_HEAP\n@@ -1080,1 +1116,1 @@\n-                        char *addr, size_t bytes, bool read_only,\n+                        char* addr, size_t bytes, bool read_only,\n@@ -1091,0 +1127,11 @@\n+char* FileMapInfo::map_heap_region(FileMapRegion* r, char* addr, size_t bytes) {\n+  return ::map_memory(_fd,\n+                    _full_path,\n+                    r->file_offset(),\n+                    addr,\n+                    bytes,\n+                    r->read_only(),\n+                    r->allow_exec(),\n+                    mtJavaHeap);\n+}\n+\n@@ -1258,2 +1305,2 @@\n-char* FileMapInfo::map_bitmap_region() {\n-  FileMapRegion* r = region_at(AOTMetaspace::bm);\n+char* FileMapInfo::map_auxiliary_region(int region_index, bool read_only) {\n+  FileMapRegion* r = region_at(region_index);\n@@ -1263,1 +1310,2 @@\n-  bool read_only = true, allow_exec = false;\n+  const char* region_name = shared_region_name[region_index];\n+  bool allow_exec = false;\n@@ -1265,1 +1313,1 @@\n-  char* bitmap_base = map_memory(_fd, _full_path, r->file_offset(),\n+  char* mapped_base = map_memory(_fd, _full_path, r->file_offset(),\n@@ -1267,2 +1315,2 @@\n-  if (bitmap_base == nullptr) {\n-    AOTMetaspace::report_loading_error(\"failed to map relocation bitmap\");\n+  if (mapped_base == nullptr) {\n+    AOTMetaspace::report_loading_error(\"failed to map %d region\", region_index);\n@@ -1272,4 +1320,4 @@\n-  if (VerifySharedSpaces && !r->check_region_crc(bitmap_base)) {\n-    aot_log_error(aot)(\"relocation bitmap CRC error\");\n-    if (!os::unmap_memory(bitmap_base, r->used_aligned())) {\n-      fatal(\"os::unmap_memory of relocation bitmap failed\");\n+  if (VerifySharedSpaces && !r->check_region_crc(mapped_base)) {\n+    aot_log_error(aot)(\"region %d CRC error\", region_index);\n+    if (!os::unmap_memory(mapped_base, r->used_aligned())) {\n+      fatal(\"os::unmap_memory of region %d failed\", region_index);\n@@ -1281,2 +1329,2 @@\n-  r->set_mapped_base(bitmap_base);\n-  aot_log_info(aot)(\"Mapped %s region #%d at base \" INTPTR_FORMAT \" top \" INTPTR_FORMAT \" (%s)\",\n+  r->set_mapped_base(mapped_base);\n+  aot_log_info(aot)(\"Mapped %s region #%d at base %zu top %zu (%s)\",\n@@ -1284,3 +1332,7 @@\n-                AOTMetaspace::bm, p2i(r->mapped_base()), p2i(r->mapped_end()),\n-                shared_region_name[AOTMetaspace::bm]);\n-  return bitmap_base;\n+                region_index, p2i(r->mapped_base()), p2i(r->mapped_end()),\n+                region_name);\n+  return mapped_base;\n+}\n+\n+char* FileMapInfo::map_bitmap_region() {\n+  return map_auxiliary_region(AOTMetaspace::bm, false);\n@@ -1433,1 +1485,0 @@\n-MemRegion FileMapInfo::_mapped_heap_memregion;\n@@ -1439,15 +1490,23 @@\n-\/\/ Returns the address range of the archived heap region computed using the\n-\/\/ current oop encoding mode. This range may be different than the one seen at\n-\/\/ dump time due to encoding mode differences. The result is used in determining\n-\/\/ if\/how these regions should be relocated at run time.\n-MemRegion FileMapInfo::get_heap_region_requested_range() {\n-  FileMapRegion* r = region_at(AOTMetaspace::hp);\n-  size_t size = r->used();\n-  assert(size > 0, \"must have non-empty heap region\");\n-\n-  address start = heap_region_requested_address();\n-  address end = start + size;\n-  aot_log_info(aot)(\"Requested heap region [\" INTPTR_FORMAT \" - \" INTPTR_FORMAT \"] = %8zu bytes\",\n-                p2i(start), p2i(end), size);\n-\n-  return MemRegion((HeapWord*)start, (HeapWord*)end);\n+static void on_heap_region_loading_error() {\n+  if (CDSConfig::is_using_aot_linked_classes()) {\n+    \/\/ It's too late to recover -- we have already committed to use the archived metaspace objects, but\n+    \/\/ the archived heap objects cannot be loaded, so we don't have the archived FMG to guarantee that\n+    \/\/ all AOT-linked classes are visible.\n+    \/\/\n+    \/\/ We get here because the heap is too small. The app will fail anyway. So let's quit.\n+    aot_log_error(aot)(\"%s has aot-linked classes but the archived \"\n+                       \"heap objects cannot be loaded. Try increasing your heap size.\",\n+                       CDSConfig::type_of_archive_being_loaded());\n+    AOTMetaspace::unrecoverable_loading_error();\n+  }\n+  CDSConfig::stop_using_full_module_graph();\n+}\n+\n+void FileMapInfo::stream_heap_region() {\n+  assert(object_streaming_mode(), \"This should only be done for the streaming approach\");\n+\n+  if (map_auxiliary_region(AOTMetaspace::hp, \/*readonly=*\/true) != nullptr) {\n+    HeapShared::initialize_streaming();\n+  } else {\n+    on_heap_region_loading_error();\n+  }\n@@ -1457,0 +1516,1 @@\n+  assert(!object_streaming_mode(), \"This should only be done for the mapping approach\");\n@@ -1459,12 +1519,4 @@\n-  if (can_use_heap_region()) {\n-    if (ArchiveHeapLoader::can_map()) {\n-      success = map_heap_region();\n-    } else if (ArchiveHeapLoader::can_load()) {\n-      success = ArchiveHeapLoader::load_heap_region(this);\n-    } else {\n-      if (!UseCompressedOops && !ArchiveHeapLoader::can_map()) {\n-        AOTMetaspace::report_loading_error(\"Cannot use CDS heap data. Selected GC not compatible -XX:-UseCompressedOops\");\n-      } else {\n-        AOTMetaspace::report_loading_error(\"Cannot use CDS heap data. UseEpsilonGC, UseG1GC, UseSerialGC, UseParallelGC, or UseShenandoahGC are required.\");\n-      }\n-    }\n+  if (AOTMappedHeapLoader::can_map()) {\n+    success = AOTMappedHeapLoader::map_heap_region(this);\n+  } else if (AOTMappedHeapLoader::can_load()) {\n+    success = AOTMappedHeapLoader::load_heap_region(this);\n@@ -1474,12 +1526,1 @@\n-    if (CDSConfig::is_using_aot_linked_classes()) {\n-      \/\/ It's too late to recover -- we have already committed to use the archived metaspace objects, but\n-      \/\/ the archived heap objects cannot be loaded, so we don't have the archived FMG to guarantee that\n-      \/\/ all AOT-linked classes are visible.\n-      \/\/\n-      \/\/ We get here because the heap is too small. The app will fail anyway. So let's quit.\n-      aot_log_error(aot)(\"%s has aot-linked classes but the archived \"\n-                     \"heap objects cannot be loaded. Try increasing your heap size.\",\n-                     CDSConfig::type_of_archive_being_loaded());\n-      AOTMetaspace::unrecoverable_loading_error();\n-    }\n-    CDSConfig::stop_using_full_module_graph(\"archive heap loading failed\");\n+    on_heap_region_loading_error();\n@@ -1493,0 +1534,4 @@\n+  if (!object_streaming_mode() && !Universe::heap()->can_load_archived_objects() && !UseG1GC) {\n+    \/\/ Incompatible object format\n+    return false;\n+  }\n@@ -1507,1 +1552,1 @@\n-  \/\/ ArchiveBuilder::precomputed_narrow_klass_shift. We enforce this encoding at runtime (see\n+  \/\/ HeapShared::precomputed_narrow_klass_shift. We enforce this encoding at runtime (see\n@@ -1516,0 +1561,1 @@\n+\n@@ -1518,2 +1564,4 @@\n-  aot_log_info(aot)(\"    narrow_oop_mode = %d, narrow_oop_base = \" PTR_FORMAT \", narrow_oop_shift = %d\",\n-                narrow_oop_mode(), p2i(narrow_oop_base()), narrow_oop_shift());\n+  if (UseCompressedOops) {\n+    aot_log_info(aot)(\"    narrow_oop_mode = %d, narrow_oop_base = \" PTR_FORMAT \", narrow_oop_shift = %d\",\n+                      narrow_oop_mode(), p2i(narrow_oop_base()), narrow_oop_shift());\n+  }\n@@ -1524,7 +1572,11 @@\n-  aot_log_info(aot)(\"    narrow_oop_mode = %d, narrow_oop_base = \" PTR_FORMAT \", narrow_oop_shift = %d\",\n-                CompressedOops::mode(), p2i(CompressedOops::base()), CompressedOops::shift());\n-  aot_log_info(aot)(\"    heap range = [\" PTR_FORMAT \" - \"  PTR_FORMAT \"]\",\n-                UseCompressedOops ? p2i(CompressedOops::begin()) :\n-                                    UseG1GC ? p2i((address)G1CollectedHeap::heap()->reserved().start()) : 0L,\n-                UseCompressedOops ? p2i(CompressedOops::end()) :\n-                                    UseG1GC ? p2i((address)G1CollectedHeap::heap()->reserved().end()) : 0L);\n+  if (UseCompressedOops) {\n+    aot_log_info(aot)(\"    narrow_oop_mode = %d, narrow_oop_base = \" PTR_FORMAT \", narrow_oop_shift = %d\",\n+                      CompressedOops::mode(), p2i(CompressedOops::base()), CompressedOops::shift());\n+  }\n+  if (!object_streaming_mode()) {\n+    aot_log_info(aot)(\"    heap range = [\" PTR_FORMAT \" - \"  PTR_FORMAT \"]\",\n+                      UseCompressedOops ? p2i(CompressedOops::begin()) :\n+                      UseG1GC ? p2i((address)G1CollectedHeap::heap()->reserved().start()) : 0L,\n+                      UseCompressedOops ? p2i(CompressedOops::end()) :\n+                      UseG1GC ? p2i((address)G1CollectedHeap::heap()->reserved().end()) : 0L);\n+  }\n@@ -1574,196 +1626,0 @@\n-\/\/ The actual address of this region during dump time.\n-address FileMapInfo::heap_region_dumptime_address() {\n-  FileMapRegion* r = region_at(AOTMetaspace::hp);\n-  assert(CDSConfig::is_using_archive(), \"runtime only\");\n-  assert(is_aligned(r->mapping_offset(), sizeof(HeapWord)), \"must be\");\n-  if (UseCompressedOops) {\n-    return \/*dumptime*\/ (address)((uintptr_t)narrow_oop_base() + r->mapping_offset());\n-  } else {\n-    return heap_region_requested_address();\n-  }\n-}\n-\n-\/\/ The address where this region can be mapped into the runtime heap without\n-\/\/ patching any of the pointers that are embedded in this region.\n-address FileMapInfo::heap_region_requested_address() {\n-  assert(CDSConfig::is_using_archive(), \"runtime only\");\n-  FileMapRegion* r = region_at(AOTMetaspace::hp);\n-  assert(is_aligned(r->mapping_offset(), sizeof(HeapWord)), \"must be\");\n-  assert(ArchiveHeapLoader::can_use(), \"GC must support mapping or loading\");\n-  if (UseCompressedOops) {\n-    \/\/ We can avoid relocation if each region's offset from the runtime CompressedOops::base()\n-    \/\/ is the same as its offset from the CompressedOops::base() during dumptime.\n-    \/\/ Note that CompressedOops::base() may be different between dumptime and runtime.\n-    \/\/\n-    \/\/ Example:\n-    \/\/ Dumptime base = 0x1000 and shift is 0. We have a region at address 0x2000. There's a\n-    \/\/ narrowOop P stored in this region that points to an object at address 0x2200.\n-    \/\/ P's encoded value is 0x1200.\n-    \/\/\n-    \/\/ Runtime base = 0x4000 and shift is also 0. If we map this region at 0x5000, then\n-    \/\/ the value P can remain 0x1200. The decoded address = (0x4000 + (0x1200 << 0)) = 0x5200,\n-    \/\/ which is the runtime location of the referenced object.\n-    return \/*runtime*\/ (address)((uintptr_t)CompressedOops::base() + r->mapping_offset());\n-  } else {\n-    \/\/ This was the hard-coded requested base address used at dump time. With uncompressed oops,\n-    \/\/ the heap range is assigned by the OS so we will most likely have to relocate anyway, no matter\n-    \/\/ what base address was picked at duump time.\n-    return (address)ArchiveHeapWriter::NOCOOPS_REQUESTED_BASE;\n-  }\n-}\n-\n-bool FileMapInfo::map_heap_region() {\n-  if (map_heap_region_impl()) {\n-#ifdef ASSERT\n-    \/\/ The \"old\" regions must be parsable -- we cannot have any unused space\n-    \/\/ at the start of the lowest G1 region that contains archived objects.\n-    assert(is_aligned(_mapped_heap_memregion.start(), G1HeapRegion::GrainBytes), \"must be\");\n-\n-    \/\/ Make sure we map at the very top of the heap - see comments in\n-    \/\/ init_heap_region_relocation().\n-    MemRegion heap_range = G1CollectedHeap::heap()->reserved();\n-    assert(heap_range.contains(_mapped_heap_memregion), \"must be\");\n-\n-    address heap_end = (address)heap_range.end();\n-    address mapped_heap_region_end = (address)_mapped_heap_memregion.end();\n-    assert(heap_end >= mapped_heap_region_end, \"must be\");\n-    assert(heap_end - mapped_heap_region_end < (intx)(G1HeapRegion::GrainBytes),\n-           \"must be at the top of the heap to avoid fragmentation\");\n-#endif\n-\n-    ArchiveHeapLoader::set_mapped();\n-    return true;\n-  } else {\n-    return false;\n-  }\n-}\n-\n-bool FileMapInfo::map_heap_region_impl() {\n-  assert(UseG1GC, \"the following code assumes G1\");\n-\n-  FileMapRegion* r = region_at(AOTMetaspace::hp);\n-  size_t size = r->used();\n-  if (size == 0) {\n-    return false; \/\/ no archived java heap data\n-  }\n-\n-  size_t word_size = size \/ HeapWordSize;\n-  address requested_start = heap_region_requested_address();\n-\n-  aot_log_info(aot)(\"Preferred address to map heap data (to avoid relocation) is \" INTPTR_FORMAT, p2i(requested_start));\n-\n-  \/\/ allocate from java heap\n-  HeapWord* start = G1CollectedHeap::heap()->alloc_archive_region(word_size, (HeapWord*)requested_start);\n-  if (start == nullptr) {\n-    AOTMetaspace::report_loading_error(\"UseSharedSpaces: Unable to allocate java heap region for archive heap.\");\n-    return false;\n-  }\n-\n-  _mapped_heap_memregion = MemRegion(start, word_size);\n-\n-  \/\/ Map the archived heap data. No need to call MemTracker::record_virtual_memory_tag()\n-  \/\/ for mapped region as it is part of the reserved java heap, which is already recorded.\n-  char* addr = (char*)_mapped_heap_memregion.start();\n-  char* base;\n-\n-  if (AOTMetaspace::use_windows_memory_mapping() || UseLargePages) {\n-    \/\/ With UseLargePages, memory mapping may fail on some OSes if the size is not\n-    \/\/ large page aligned, so let's use read() instead. In this case, the memory region\n-    \/\/ is already commited by G1 so we don't need to commit it again.\n-    if (!read_region(AOTMetaspace::hp, addr,\n-                     align_up(_mapped_heap_memregion.byte_size(), os::vm_page_size()),\n-                     \/* do_commit = *\/ !UseLargePages)) {\n-      dealloc_heap_region();\n-      aot_log_error(aot)(\"Failed to read archived heap region into \" INTPTR_FORMAT, p2i(addr));\n-      return false;\n-    }\n-    \/\/ Checks for VerifySharedSpaces is already done inside read_region()\n-    base = addr;\n-  } else {\n-    base = map_memory(_fd, _full_path, r->file_offset(),\n-                      addr, _mapped_heap_memregion.byte_size(), r->read_only(),\n-                      r->allow_exec(), mtJavaHeap);\n-    if (base == nullptr || base != addr) {\n-      dealloc_heap_region();\n-      AOTMetaspace::report_loading_error(\"UseSharedSpaces: Unable to map at required address in java heap. \"\n-                                            INTPTR_FORMAT \", size = %zu bytes\",\n-                                            p2i(addr), _mapped_heap_memregion.byte_size());\n-      return false;\n-    }\n-\n-    if (VerifySharedSpaces && !r->check_region_crc(base)) {\n-      dealloc_heap_region();\n-      AOTMetaspace::report_loading_error(\"UseSharedSpaces: mapped heap region is corrupt\");\n-      return false;\n-    }\n-  }\n-\n-  r->set_mapped_base(base);\n-\n-  \/\/ If the requested range is different from the range allocated by GC, then\n-  \/\/ the pointers need to be patched.\n-  address mapped_start = (address) _mapped_heap_memregion.start();\n-  ptrdiff_t delta = mapped_start - requested_start;\n-  if (UseCompressedOops &&\n-      (narrow_oop_mode() != CompressedOops::mode() ||\n-       narrow_oop_shift() != CompressedOops::shift())) {\n-    _heap_pointers_need_patching = true;\n-  }\n-  if (delta != 0) {\n-    _heap_pointers_need_patching = true;\n-  }\n-  ArchiveHeapLoader::init_mapped_heap_info(mapped_start, delta, narrow_oop_shift());\n-\n-  if (_heap_pointers_need_patching) {\n-    char* bitmap_base = map_bitmap_region();\n-    if (bitmap_base == nullptr) {\n-      AOTMetaspace::report_loading_error(\"CDS heap cannot be used because bitmap region cannot be mapped\");\n-      dealloc_heap_region();\n-      _heap_pointers_need_patching = false;\n-      return false;\n-    }\n-  }\n-  aot_log_info(aot)(\"Heap data mapped at \" INTPTR_FORMAT \", size = %8zu bytes\",\n-                p2i(mapped_start), _mapped_heap_memregion.byte_size());\n-  aot_log_info(aot)(\"CDS heap data relocation delta = %zd bytes\", delta);\n-  return true;\n-}\n-\n-narrowOop FileMapInfo::encoded_heap_region_dumptime_address() {\n-  assert(CDSConfig::is_using_archive(), \"runtime only\");\n-  assert(UseCompressedOops, \"sanity\");\n-  FileMapRegion* r = region_at(AOTMetaspace::hp);\n-  return CompressedOops::narrow_oop_cast(r->mapping_offset() >> narrow_oop_shift());\n-}\n-\n-void FileMapInfo::patch_heap_embedded_pointers() {\n-  if (!ArchiveHeapLoader::is_mapped() || !_heap_pointers_need_patching) {\n-    return;\n-  }\n-\n-  char* bitmap_base = map_bitmap_region();\n-  assert(bitmap_base != nullptr, \"must have already been mapped\");\n-\n-  FileMapRegion* r = region_at(AOTMetaspace::hp);\n-  ArchiveHeapLoader::patch_embedded_pointers(\n-      this, _mapped_heap_memregion,\n-      (address)(region_at(AOTMetaspace::bm)->mapped_base()) + r->oopmap_offset(),\n-      r->oopmap_size_in_bits());\n-}\n-\n-void FileMapInfo::fixup_mapped_heap_region() {\n-  if (ArchiveHeapLoader::is_mapped()) {\n-    assert(!_mapped_heap_memregion.is_empty(), \"sanity\");\n-\n-    \/\/ Populate the archive regions' G1BlockOffsetTables. That ensures\n-    \/\/ fast G1BlockOffsetTable::block_start operations for any given address\n-    \/\/ within the archive regions when trying to find start of an object\n-    \/\/ (e.g. during card table scanning).\n-    G1CollectedHeap::heap()->populate_archive_regions_bot(_mapped_heap_memregion);\n-  }\n-}\n-\n-\/\/ dealloc the archive regions from java heap\n-void FileMapInfo::dealloc_heap_region() {\n-  G1CollectedHeap::heap()->dealloc_archive_regions(_mapped_heap_memregion);\n-}\n@@ -1772,0 +1628,2 @@\n+\/\/ Unmap a memory region in the address space.\n+\n@@ -1779,2 +1637,0 @@\n-\/\/ Unmap a memory region in the address space.\n-\n@@ -1812,1 +1668,0 @@\n-bool FileMapInfo::_heap_pointers_need_patching = false;\n","filename":"src\/hotspot\/share\/cds\/filemap.cpp","additions":178,"deletions":323,"binary":false,"changes":501,"status":"modified"},{"patch":"@@ -30,0 +30,1 @@\n+#include \"cds\/heapShared.hpp\"\n@@ -36,0 +37,1 @@\n+#include \"utilities\/bitMap.hpp\"\n@@ -46,1 +48,0 @@\n-class ArchiveHeapInfo;\n@@ -117,0 +118,1 @@\n+  bool    _object_streaming_mode;                 \/\/ dump was created for object streaming\n@@ -142,3 +144,0 @@\n-  HeapRootSegments _heap_root_segments; \/\/ Heap root segments info\n-  size_t _heap_oopmap_start_pos;        \/\/ The first bit in the oopmap corresponds to this position in the heap.\n-  size_t _heap_ptrmap_start_pos;        \/\/ The first bit in the ptrmap corresponds to this position in the heap.\n@@ -148,0 +147,3 @@\n+  ArchiveMappedHeapHeader _mapped_heap_header;\n+  ArchiveStreamedHeapHeader _streamed_heap_header;\n+\n@@ -195,0 +197,1 @@\n+  bool object_streaming_mode()             const { return _object_streaming_mode; }\n@@ -204,1 +207,0 @@\n-  HeapRootSegments heap_root_segments()    const { return _heap_root_segments; }\n@@ -206,2 +208,0 @@\n-  size_t heap_oopmap_start_pos()           const { return _heap_oopmap_start_pos; }\n-  size_t heap_ptrmap_start_pos()           const { return _heap_ptrmap_start_pos; }\n@@ -211,0 +211,7 @@\n+  \/\/ Heap archiving\n+  const ArchiveMappedHeapHeader*   mapped_heap()   const { return &_mapped_heap_header; }\n+  const ArchiveStreamedHeapHeader* streamed_heap() const { return &_streamed_heap_header; }\n+\n+  void set_streamed_heap_header(ArchiveStreamedHeapHeader header) { _streamed_heap_header = header; }\n+  void set_mapped_heap_header(ArchiveMappedHeapHeader header) { _mapped_heap_header = header; }\n+\n@@ -216,3 +223,0 @@\n-  void set_heap_root_segments(HeapRootSegments segments) { _heap_root_segments = segments; }\n-  void set_heap_oopmap_start_pos(size_t n)       { _heap_oopmap_start_pos = n; }\n-  void set_heap_ptrmap_start_pos(size_t n)       { _heap_ptrmap_start_pos = n; }\n@@ -221,0 +225,1 @@\n+\n@@ -276,1 +281,0 @@\n-  static bool _heap_pointers_need_patching;\n@@ -306,1 +310,0 @@\n-  HeapRootSegments heap_root_segments() const { return header()->heap_root_segments(); }\n@@ -308,2 +311,0 @@\n-  size_t  heap_oopmap_start_pos() const { return header()->heap_oopmap_start_pos(); }\n-  size_t  heap_ptrmap_start_pos() const { return header()->heap_ptrmap_start_pos(); }\n@@ -311,0 +312,4 @@\n+  const ArchiveMappedHeapHeader*   mapped_heap()   const { return header()->mapped_heap(); }\n+  const ArchiveStreamedHeapHeader* streamed_heap() const { return header()->streamed_heap(); }\n+\n+  bool object_streaming_mode()                const { return header()->object_streaming_mode(); }\n@@ -327,0 +332,1 @@\n+  char* map_heap_region(FileMapRegion* r, char* addr, size_t bytes);\n@@ -366,1 +372,4 @@\n-  char* write_bitmap_region(CHeapBitMap* rw_ptrmap, CHeapBitMap* ro_ptrmap, ArchiveHeapInfo* heap_info,\n+  char* write_bitmap_region(CHeapBitMap* rw_ptrmap,\n+                            CHeapBitMap* ro_ptrmap,\n+                            ArchiveMappedHeapInfo* mapped_heap_info,\n+                            ArchiveStreamedHeapInfo* streamed_heap_info,\n@@ -368,1 +377,2 @@\n-  size_t write_heap_region(ArchiveHeapInfo* heap_info);\n+  size_t write_mapped_heap_region(ArchiveMappedHeapInfo* heap_info) NOT_CDS_JAVA_HEAP_RETURN_(0);\n+  size_t write_streamed_heap_region(ArchiveStreamedHeapInfo* heap_info) NOT_CDS_JAVA_HEAP_RETURN_(0);\n@@ -375,0 +385,3 @@\n+\n+  \/\/ Object loading support\n+  void  stream_heap_region() NOT_CDS_JAVA_HEAP_RETURN;\n@@ -376,2 +389,1 @@\n-  void  fixup_mapped_heap_region() NOT_CDS_JAVA_HEAP_RETURN;\n-  void  patch_heap_embedded_pointers() NOT_CDS_JAVA_HEAP_RETURN;\n+\n@@ -379,1 +391,0 @@\n-  MemRegion get_heap_region_requested_range() NOT_CDS_JAVA_HEAP_RETURN_(MemRegion());\n@@ -383,0 +394,1 @@\n+  char* map_forwarding_region();\n@@ -437,0 +449,1 @@\n+  bool  can_use_heap_region();\n@@ -441,6 +454,1 @@\n-  bool  map_heap_region_impl() NOT_CDS_JAVA_HEAP_RETURN_(false);\n-  void  dealloc_heap_region() NOT_CDS_JAVA_HEAP_RETURN;\n-  bool  can_use_heap_region();\n-  bool  load_heap_region() NOT_CDS_JAVA_HEAP_RETURN_(false);\n-  bool  map_heap_region() NOT_CDS_JAVA_HEAP_RETURN_(false);\n-  void  init_heap_region_relocation();\n+\n@@ -449,2 +457,1 @@\n-\n-  static MemRegion _mapped_heap_memregion;\n+  char* map_auxiliary_region(int region_index, bool read_only);\n@@ -453,3 +460,0 @@\n-  address heap_region_dumptime_address() NOT_CDS_JAVA_HEAP_RETURN_(nullptr);\n-  address heap_region_requested_address() NOT_CDS_JAVA_HEAP_RETURN_(nullptr);\n-  narrowOop encoded_heap_region_dumptime_address();\n","filename":"src\/hotspot\/share\/cds\/filemap.hpp","additions":34,"deletions":30,"binary":false,"changes":64,"status":"modified"},{"patch":"@@ -29,0 +29,2 @@\n+#include \"cds\/aotMappedHeapLoader.hpp\"\n+#include \"cds\/aotMappedHeapWriter.hpp\"\n@@ -32,0 +34,2 @@\n+#include \"cds\/aotStreamedHeapLoader.hpp\"\n+#include \"cds\/aotStreamedHeapWriter.hpp\"\n@@ -33,2 +37,0 @@\n-#include \"cds\/archiveHeapLoader.hpp\"\n-#include \"cds\/archiveHeapWriter.hpp\"\n@@ -36,0 +38,1 @@\n+#include \"cds\/cds_globals.hpp\"\n@@ -39,1 +42,1 @@\n-#include \"cds\/heapShared.hpp\"\n+#include \"cds\/heapShared.inline.hpp\"\n@@ -67,0 +70,1 @@\n+#include \"runtime\/globals_extension.hpp\"\n@@ -94,1 +98,51 @@\n-DumpedInternedStrings *HeapShared::_dumped_interned_strings = nullptr;\n+\/\/ Anything that goes in the header must be thoroughly purged from uninitialized memory\n+\/\/ as it will be written to disk. Therefore, the constructors memset the memory to 0.\n+\/\/ This is not the prettiest thing, but we need to know every byte is initialized,\n+\/\/ including potential padding between fields.\n+\n+ArchiveMappedHeapHeader::ArchiveMappedHeapHeader(size_t ptrmap_start_pos,\n+                                                 size_t oopmap_start_pos,\n+                                                 HeapRootSegments root_segments) {\n+  memset((char*)this, 0, sizeof(*this));\n+  _ptrmap_start_pos = ptrmap_start_pos;\n+  _oopmap_start_pos = oopmap_start_pos;\n+  _root_segments = root_segments;\n+}\n+\n+ArchiveMappedHeapHeader::ArchiveMappedHeapHeader() {\n+  memset((char*)this, 0, sizeof(*this));\n+}\n+\n+ArchiveMappedHeapHeader ArchiveMappedHeapInfo::create_header() {\n+  return ArchiveMappedHeapHeader{_ptrmap_start_pos,\n+                                 _oopmap_start_pos,\n+                                 _root_segments};\n+}\n+\n+ArchiveStreamedHeapHeader::ArchiveStreamedHeapHeader(size_t forwarding_offset,\n+                                                     size_t roots_offset,\n+                                                     size_t num_roots,\n+                                                     size_t root_highest_object_index_table_offset,\n+                                                     size_t num_archived_objects) {\n+  memset((char*)this, 0, sizeof(*this));\n+  _forwarding_offset = forwarding_offset;\n+  _roots_offset = roots_offset;\n+  _num_roots = num_roots;\n+  _root_highest_object_index_table_offset = root_highest_object_index_table_offset;\n+  _num_archived_objects = num_archived_objects;\n+}\n+\n+ArchiveStreamedHeapHeader::ArchiveStreamedHeapHeader() {\n+  memset((char*)this, 0, sizeof(*this));\n+}\n+\n+ArchiveStreamedHeapHeader ArchiveStreamedHeapInfo::create_header() {\n+  return ArchiveStreamedHeapHeader{_forwarding_offset,\n+                                   _roots_offset,\n+                                   _num_roots,\n+                                   _root_highest_object_index_table_offset,\n+                                   _num_archived_objects};\n+}\n+\n+HeapArchiveMode HeapShared::_heap_load_mode = HeapArchiveMode::_uninitialized;\n+HeapArchiveMode HeapShared::_heap_write_mode = HeapArchiveMode::_uninitialized;\n@@ -145,2 +199,0 @@\n-GrowableArrayCHeap<OopHandle, mtClassShared>* HeapShared::_root_segments = nullptr;\n-int HeapShared::_root_segment_max_size_elems;\n@@ -242,4 +294,10 @@\n-bool HeapShared::has_been_archived(oop obj) {\n-  assert(CDSConfig::is_dumping_heap(), \"dump-time only\");\n-  OopHandle oh(&obj);\n-  return archived_object_cache()->get(oh) != nullptr;\n+bool HeapShared::is_archived_heap_in_use() {\n+  if (HeapShared::is_loading()) {\n+    if (HeapShared::is_loading_streaming_mode()) {\n+      return AOTStreamedHeapLoader::is_in_use();\n+    } else {\n+      return AOTMappedHeapLoader::is_in_use();\n+    }\n+  }\n+\n+  return false;\n@@ -248,4 +306,13 @@\n-int HeapShared::append_root(oop obj) {\n-  assert(CDSConfig::is_dumping_heap(), \"dump-time only\");\n-  if (obj != nullptr) {\n-    assert(has_been_archived(obj), \"must be\");\n+bool HeapShared::can_use_archived_heap() {\n+  FileMapInfo* static_mapinfo = FileMapInfo::current_info();\n+  if (static_mapinfo == nullptr) {\n+    return false;\n+  }\n+  if (!static_mapinfo->has_heap_region()) {\n+    return false;\n+  }\n+  if (!static_mapinfo->object_streaming_mode() &&\n+      !Universe::heap()->can_load_archived_objects() &&\n+      !UseG1GC) {\n+    \/\/ Incompatible object format\n+    return false;\n@@ -253,2 +320,0 @@\n-  \/\/ No GC should happen since we aren't scanning _pending_roots.\n-  assert(Thread::current() == (Thread*)VMThread::vm_thread(), \"should be in vm thread\");\n@@ -256,1 +321,1 @@\n-  return _pending_roots->append(obj);\n+  return true;\n@@ -259,3 +324,11 @@\n-objArrayOop HeapShared::root_segment(int segment_idx) {\n-  if (CDSConfig::is_dumping_heap()) {\n-    assert(Thread::current() == (Thread*)VMThread::vm_thread(), \"should be in vm thread\");\n+bool HeapShared::is_too_large_to_archive(size_t size) {\n+  if (HeapShared::is_writing_streaming_mode()) {\n+    return false;\n+  } else {\n+    return AOTMappedHeapWriter::is_too_large_to_archive(size);\n+  }\n+}\n+\n+bool HeapShared::is_too_large_to_archive(oop obj) {\n+  if (HeapShared::is_writing_streaming_mode()) {\n+    return false;\n@@ -263,1 +336,1 @@\n-    assert(CDSConfig::is_using_archive(), \"must be\");\n+    return AOTMappedHeapWriter::is_too_large_to_archive(obj);\n@@ -265,0 +338,1 @@\n+}\n@@ -266,3 +340,3 @@\n-  objArrayOop segment = (objArrayOop)_root_segments->at(segment_idx).resolve();\n-  assert(segment != nullptr, \"should have been initialized\");\n-  return segment;\n+bool HeapShared::is_string_too_large_to_archive(oop string) {\n+  typeArrayOop value = java_lang_String::value_no_keepalive(string);\n+  return is_too_large_to_archive(value);\n@@ -271,2 +345,5 @@\n-void HeapShared::get_segment_indexes(int idx, int& seg_idx, int& int_idx) {\n-  assert(_root_segment_max_size_elems > 0, \"sanity\");\n+void HeapShared::initialize_loading_mode(HeapArchiveMode mode) {\n+  assert(_heap_load_mode == HeapArchiveMode::_uninitialized, \"already set?\");\n+  assert(mode != HeapArchiveMode::_uninitialized, \"sanity\");\n+  _heap_load_mode = mode;\n+};\n@@ -274,7 +351,90 @@\n-  \/\/ Try to avoid divisions for the common case.\n-  if (idx < _root_segment_max_size_elems) {\n-    seg_idx = 0;\n-    int_idx = idx;\n-  } else {\n-    seg_idx = idx \/ _root_segment_max_size_elems;\n-    int_idx = idx % _root_segment_max_size_elems;\n+void HeapShared::initialize_writing_mode() {\n+  assert(!FLAG_IS_ERGO(AOTStreamableObjects), \"Should not have been ergonomically set yet\");\n+\n+  if (!CDSConfig::is_dumping_archive()) {\n+    \/\/ We use FLAG_IS_CMDLINE below because we are specifically looking to warn\n+    \/\/ a user that explicitly sets the flag on the command line for a JVM that is\n+    \/\/ not dumping an archive.\n+    if (FLAG_IS_CMDLINE(AOTStreamableObjects)) {\n+      log_warning(cds)(\"-XX:%cAOTStreamableObjects was specified, \"\n+                       \"AOTStreamableObjects is only used for writing \"\n+                       \"the AOT cache.\",\n+                       AOTStreamableObjects ? '+' : '-');\n+    }\n+  }\n+\n+  \/\/ The below checks use !FLAG_IS_DEFAULT instead of FLAG_IS_CMDLINE\n+  \/\/ because the one step AOT cache creation transfers the AOTStreamableObjects\n+  \/\/ flag value from the training JVM to the assembly JVM using an environment\n+  \/\/ variable that sets the flag as ERGO in the assembly JVM.\n+  if (FLAG_IS_DEFAULT(AOTStreamableObjects)) {\n+    \/\/ By default, the value of AOTStreamableObjects should match !UseCompressedOops.\n+    FLAG_SET_DEFAULT(AOTStreamableObjects, !UseCompressedOops);\n+  } else if (!AOTStreamableObjects && UseZGC) {\n+    \/\/ Never write mapped heap with ZGC\n+    if (CDSConfig::is_dumping_archive()) {\n+      log_warning(cds)(\"Heap archiving without streaming not supported for -XX:+UseZGC\");\n+    }\n+    FLAG_SET_ERGO(AOTStreamableObjects, true);\n+  }\n+\n+  if (CDSConfig::is_dumping_archive()) {\n+    \/\/ Select default mode\n+    assert(_heap_write_mode == HeapArchiveMode::_uninitialized, \"already initialized?\");\n+    _heap_write_mode = AOTStreamableObjects ? HeapArchiveMode::_streaming : HeapArchiveMode::_mapping;\n+  }\n+}\n+\n+void HeapShared::initialize_streaming() {\n+  assert(is_loading_streaming_mode(), \"shouldn't call this\");\n+  if (can_use_archived_heap()) {\n+    AOTStreamedHeapLoader::initialize();\n+  }\n+}\n+\n+void HeapShared::enable_gc() {\n+  if (AOTStreamedHeapLoader::is_in_use()) {\n+    AOTStreamedHeapLoader::enable_gc();\n+  }\n+}\n+\n+void HeapShared::materialize_thread_object() {\n+  if (AOTStreamedHeapLoader::is_in_use()) {\n+    AOTStreamedHeapLoader::materialize_thread_object();\n+  }\n+}\n+\n+void HeapShared::add_to_dumped_interned_strings(oop string) {\n+  assert(HeapShared::is_writing_mapping_mode(), \"Only used by this mode\");\n+  AOTMappedHeapWriter::add_to_dumped_interned_strings(string);\n+}\n+\n+void HeapShared::finalize_initialization(FileMapInfo* static_mapinfo) {\n+  if (HeapShared::is_loading()) {\n+    if (HeapShared::is_loading_streaming_mode()) {\n+      \/\/ Heap initialization can be done only after vtables are initialized by ReadClosure.\n+      AOTStreamedHeapLoader::finish_initialization(static_mapinfo);\n+    } else {\n+      \/\/ Finish up archived heap initialization. These must be\n+      \/\/ done after ReadClosure.\n+      AOTMappedHeapLoader::finish_initialization(static_mapinfo);\n+    }\n+  }\n+}\n+\n+HeapShared::CachedOopInfo* HeapShared::get_cached_oop_info(oop obj) {\n+  OopHandle oh(Universe::vm_global(), obj);\n+  CachedOopInfo* result = _archived_object_cache->get(oh);\n+  oh.release(Universe::vm_global());\n+  return result;\n+}\n+\n+bool HeapShared::has_been_archived(oop obj) {\n+  assert(CDSConfig::is_dumping_heap(), \"dump-time only\");\n+  return get_cached_oop_info(obj) != nullptr;\n+}\n+\n+int HeapShared::append_root(oop obj) {\n+  assert(CDSConfig::is_dumping_heap(), \"dump-time only\");\n+  if (obj != nullptr) {\n+    assert(has_been_archived(obj), \"must be\");\n@@ -282,0 +442,2 @@\n+  \/\/ No GC should happen since we aren't scanning _pending_roots.\n+  assert(Thread::current() == (Thread*)VMThread::vm_thread(), \"should be in vm thread\");\n@@ -283,2 +445,1 @@\n-  assert(idx == seg_idx * _root_segment_max_size_elems + int_idx,\n-         \"sanity: %d index maps to %d segment and %d internal\", idx, seg_idx, int_idx);\n+  return _pending_roots->append(obj);\n@@ -287,1 +448,0 @@\n-\/\/ Returns an objArray that contains all the roots of the archived objects\n@@ -291,4 +451,10 @@\n-  assert(!_root_segments->is_empty(), \"must have loaded shared heap\");\n-  int seg_idx, int_idx;\n-  get_segment_indexes(index, seg_idx, int_idx);\n-  oop result = root_segment(seg_idx)->obj_at(int_idx);\n+  assert(is_archived_heap_in_use(), \"getting roots into heap that is not used\");\n+\n+  oop result;\n+  if (HeapShared::is_loading_streaming_mode()) {\n+    result = AOTStreamedHeapLoader::get_root(index);\n+  } else {\n+    assert(HeapShared::is_loading_mapping_mode(), \"must be\");\n+    result = AOTMappedHeapLoader::get_root(index);\n+  }\n+\n@@ -298,0 +464,1 @@\n+\n@@ -301,0 +468,6 @@\n+void HeapShared::finish_materialize_objects() {\n+  if (AOTStreamedHeapLoader::is_in_use()) {\n+    AOTStreamedHeapLoader::finish_materialize_objects();\n+  }\n+}\n+\n@@ -304,3 +477,1 @@\n-  if (ArchiveHeapLoader::is_in_use()) {\n-    int seg_idx, int_idx;\n-    get_segment_indexes(index, seg_idx, int_idx);\n+  if (is_archived_heap_in_use()) {\n@@ -308,2 +479,7 @@\n-      oop old = root_segment(seg_idx)->obj_at(int_idx);\n-      log_debug(aot, heap)(\"Clearing root %d: was \" PTR_FORMAT, index, p2i(old));\n+      log_debug(aot, heap)(\"Clearing root %d: was %zu\", index, p2i(get_root(index, false \/* clear *\/)));\n+    }\n+    if (HeapShared::is_loading_streaming_mode()) {\n+      AOTStreamedHeapLoader::clear_root(index);\n+    } else {\n+      assert(HeapShared::is_loading_mapping_mode(), \"must be\");\n+      AOTMappedHeapLoader::clear_root(index);\n@@ -311,1 +487,0 @@\n-    root_segment(seg_idx)->obj_at_put(int_idx, nullptr);\n@@ -323,1 +498,1 @@\n-  if (ArchiveHeapWriter::is_too_large_to_archive(obj->size())) {\n+  if (is_too_large_to_archive(obj)) {\n@@ -328,2 +503,1 @@\n-  } else {\n-    AOTOopChecker::check(obj); \/\/ Make sure contents of this oop are safe.\n+  }\n@@ -331,3 +505,2 @@\n-    count_allocation(obj->size());\n-    ArchiveHeapWriter::add_source_obj(obj);\n-    CachedOopInfo info = make_cached_oop_info(obj, referrer);\n+  AOTOopChecker::check(obj); \/\/ Make sure contents of this oop are safe.\n+  count_allocation(obj->size());\n@@ -335,4 +508,5 @@\n-    OopHandle oh(Universe::vm_global(), obj);\n-    archived_object_cache()->put_when_absent(oh, info);\n-    archived_object_cache()->maybe_grow();\n-    mark_native_pointers(obj);\n+  if (HeapShared::is_writing_streaming_mode()) {\n+    AOTStreamedHeapWriter::add_source_obj(obj);\n+  } else {\n+    AOTMappedHeapWriter::add_source_obj(obj);\n+  }\n@@ -340,21 +514,4 @@\n-    Klass* k = obj->klass();\n-    if (k->is_instance_klass()) {\n-      \/\/ Whenever we see a non-array Java object of type X, we mark X to be aot-initialized.\n-      \/\/ This ensures that during the production run, whenever Java code sees a cached object\n-      \/\/ of type X, we know that X is already initialized. (see TODO comment below ...)\n-\n-      if (InstanceKlass::cast(k)->is_enum_subclass()\n-          \/\/ We can't rerun <clinit> of enum classes (see cdsEnumKlass.cpp) so\n-          \/\/ we must store them as AOT-initialized.\n-          || (subgraph_info == _dump_time_special_subgraph))\n-          \/\/ TODO: we do this only for the special subgraph for now. Extending this to\n-          \/\/ other subgraphs would require more refactoring of the core library (such as\n-          \/\/ move some initialization logic into runtimeSetup()).\n-          \/\/\n-          \/\/ For the other subgraphs, we have a weaker mechanism to ensure that\n-          \/\/ all classes in a subgraph are initialized before the subgraph is programmatically\n-          \/\/ returned from jdk.internal.misc.CDS::initializeFromArchive().\n-          \/\/ See HeapShared::initialize_from_archived_subgraph().\n-      {\n-        AOTArtifactFinder::add_aot_inited_class(InstanceKlass::cast(k));\n-      }\n+  OopHandle oh(Universe::vm_global(), obj);\n+  CachedOopInfo info = make_cached_oop_info(obj, referrer);\n+  archived_object_cache()->put_when_absent(oh, info);\n+  archived_object_cache()->maybe_grow();\n@@ -362,13 +519,32 @@\n-      if (java_lang_Class::is_instance(obj)) {\n-        Klass* mirror_k = java_lang_Class::as_Klass(obj);\n-        if (mirror_k != nullptr) {\n-          AOTArtifactFinder::add_cached_class(mirror_k);\n-        }\n-      } else if (java_lang_invoke_ResolvedMethodName::is_instance(obj)) {\n-        Method* m = java_lang_invoke_ResolvedMethodName::vmtarget(obj);\n-        if (m != nullptr) {\n-          if (RegeneratedClasses::has_been_regenerated(m)) {\n-            m = RegeneratedClasses::get_regenerated_object(m);\n-          }\n-          InstanceKlass* method_holder = m->method_holder();\n-          AOTArtifactFinder::add_cached_class(method_holder);\n+  Klass* k = obj->klass();\n+  if (k->is_instance_klass()) {\n+    \/\/ Whenever we see a non-array Java object of type X, we mark X to be aot-initialized.\n+    \/\/ This ensures that during the production run, whenever Java code sees a cached object\n+    \/\/ of type X, we know that X is already initialized. (see TODO comment below ...)\n+\n+    if (InstanceKlass::cast(k)->is_enum_subclass()\n+        \/\/ We can't rerun <clinit> of enum classes (see cdsEnumKlass.cpp) so\n+        \/\/ we must store them as AOT-initialized.\n+        || (subgraph_info == _dump_time_special_subgraph))\n+        \/\/ TODO: we do this only for the special subgraph for now. Extending this to\n+        \/\/ other subgraphs would require more refactoring of the core library (such as\n+        \/\/ move some initialization logic into runtimeSetup()).\n+        \/\/\n+        \/\/ For the other subgraphs, we have a weaker mechanism to ensure that\n+        \/\/ all classes in a subgraph are initialized before the subgraph is programmatically\n+        \/\/ returned from jdk.internal.misc.CDS::initializeFromArchive().\n+        \/\/ See HeapShared::initialize_from_archived_subgraph().\n+    {\n+      AOTArtifactFinder::add_aot_inited_class(InstanceKlass::cast(k));\n+    }\n+\n+    if (java_lang_Class::is_instance(obj)) {\n+      Klass* mirror_k = java_lang_Class::as_Klass(obj);\n+      if (mirror_k != nullptr) {\n+        AOTArtifactFinder::add_cached_class(mirror_k);\n+      }\n+    } else if (java_lang_invoke_ResolvedMethodName::is_instance(obj)) {\n+      Method* m = java_lang_invoke_ResolvedMethodName::vmtarget(obj);\n+      if (m != nullptr) {\n+        if (RegeneratedClasses::has_been_regenerated(m)) {\n+          m = RegeneratedClasses::get_regenerated_object(m);\n@@ -376,0 +552,2 @@\n+        InstanceKlass* method_holder = m->method_holder();\n+        AOTArtifactFinder::add_cached_class(method_holder);\n@@ -378,0 +556,1 @@\n+  }\n@@ -379,13 +558,12 @@\n-    if (log_is_enabled(Debug, aot, heap)) {\n-      ResourceMark rm;\n-      LogTarget(Debug, aot, heap) log;\n-      LogStream out(log);\n-      out.print(\"Archived heap object \" PTR_FORMAT \" : %s \",\n-                p2i(obj), obj->klass()->external_name());\n-      if (java_lang_Class::is_instance(obj)) {\n-        Klass* k = java_lang_Class::as_Klass(obj);\n-        if (k != nullptr) {\n-          out.print(\"%s\", k->external_name());\n-        } else {\n-          out.print(\"primitive\");\n-        }\n+  if (log_is_enabled(Debug, aot, heap)) {\n+    ResourceMark rm;\n+    LogTarget(Debug, aot, heap) log;\n+    LogStream out(log);\n+    out.print(\"Archived heap object \" PTR_FORMAT \" : %s \",\n+              p2i(obj), obj->klass()->external_name());\n+    if (java_lang_Class::is_instance(obj)) {\n+      Klass* k = java_lang_Class::as_Klass(obj);\n+      if (k != nullptr) {\n+        out.print(\"%s\", k->external_name());\n+      } else {\n+        out.print(\"primitive\");\n@@ -393,1 +571,0 @@\n-      out.cr();\n@@ -395,2 +572,1 @@\n-\n-    return true;\n+    out.cr();\n@@ -398,0 +574,2 @@\n+\n+  return true;\n@@ -440,3 +618,3 @@\n-void HeapShared::init_dumping() {\n-  _scratch_objects_table = new (mtClass)MetaspaceObjToOopHandleTable();\n-  _pending_roots = new GrowableArrayCHeap<oop, mtClassShared>(500);\n+ void HeapShared::init_dumping() {\n+   _scratch_objects_table = new (mtClass)MetaspaceObjToOopHandleTable();\n+   _pending_roots = new GrowableArrayCHeap<oop, mtClassShared>(500);\n@@ -644,1 +822,1 @@\n-    if (rr != nullptr && !ArchiveHeapWriter::is_too_large_to_archive(rr)) {\n+    if (rr != nullptr && !HeapShared::is_too_large_to_archive(rr)) {\n@@ -652,0 +830,1 @@\n+  assert(HeapShared::is_writing_mapping_mode(), \"should not reach here\");\n@@ -664,9 +843,0 @@\n-void HeapShared::mark_native_pointers(oop orig_obj) {\n-  if (java_lang_Class::is_instance(orig_obj)) {\n-    ArchiveHeapWriter::mark_native_pointer(orig_obj, java_lang_Class::klass_offset());\n-    ArchiveHeapWriter::mark_native_pointer(orig_obj, java_lang_Class::array_klass_offset());\n-  } else if (java_lang_invoke_ResolvedMethodName::is_instance(orig_obj)) {\n-    ArchiveHeapWriter::mark_native_pointer(orig_obj, java_lang_invoke_ResolvedMethodName::vmtarget_offset());\n-  }\n-}\n-\n@@ -701,1 +871,1 @@\n-    if (UseCompressedOops || UseG1GC) {\n+    if (HeapShared::is_writing_mapping_mode() && (UseG1GC || UseCompressedOops)) {\n@@ -717,1 +887,3 @@\n-  archive_strings();\n+  if (is_writing_mapping_mode()) {\n+    archive_strings();\n+  }\n@@ -721,1 +893,1 @@\n-void HeapShared::write_heap(ArchiveHeapInfo *heap_info) {\n+void HeapShared::write_heap(ArchiveMappedHeapInfo* mapped_heap_info, ArchiveStreamedHeapInfo* streamed_heap_info) {\n@@ -728,2 +900,7 @@\n-  StringTable::write_shared_table();\n-  ArchiveHeapWriter::write(_pending_roots, heap_info);\n+  if (HeapShared::is_writing_mapping_mode()) {\n+    StringTable::write_shared_table();\n+    AOTMappedHeapWriter::write(_pending_roots, mapped_heap_info);\n+  } else {\n+    assert(HeapShared::is_writing_streaming_mode(), \"are there more modes?\");\n+    AOTStreamedHeapWriter::write(_pending_roots, streamed_heap_info);\n+  }\n@@ -1070,13 +1247,0 @@\n-void HeapShared::add_root_segment(objArrayOop segment_oop) {\n-  assert(segment_oop != nullptr, \"must be\");\n-  assert(ArchiveHeapLoader::is_in_use(), \"must be\");\n-  if (_root_segments == nullptr) {\n-    _root_segments = new GrowableArrayCHeap<OopHandle, mtClassShared>(10);\n-  }\n-  _root_segments->push(OopHandle(Universe::vm_global(), segment_oop));\n-}\n-\n-void HeapShared::init_root_segment_sizes(int max_size_elems) {\n-  _root_segment_max_size_elems = max_size_elems;\n-}\n-\n@@ -1103,4 +1267,4 @@\n-    VM_Verify verify_op;\n-    VMThread::execute(&verify_op);\n-\n-    if (VerifyArchivedFields > 1 && is_init_completed()) {\n+    if (VerifyArchivedFields == 1) {\n+      VM_Verify verify_op;\n+      VMThread::execute(&verify_op);\n+    } else if (VerifyArchivedFields == 2 && is_init_completed()) {\n@@ -1132,1 +1296,1 @@\n-  if (!ArchiveHeapLoader::is_in_use()) {\n+  if (!is_archived_heap_in_use()) {\n@@ -1191,1 +1355,1 @@\n-  if (!ArchiveHeapLoader::is_in_use()) {\n+  if (!is_archived_heap_in_use()) {\n@@ -1223,1 +1387,1 @@\n-  if (!ArchiveHeapLoader::is_in_use()) {\n+  if (!is_archived_heap_in_use()) {\n@@ -1359,3 +1523,0 @@\n-  \/\/ Load the subgraph entry fields from the record and store them back to\n-  \/\/ the corresponding fields within the mirror.\n-  oop m = k->java_mirror();\n@@ -1369,0 +1530,2 @@\n+      \/\/ Load the subgraph entry fields from the record and store them back to\n+      \/\/ the corresponding fields within the mirror.\n@@ -1370,0 +1533,1 @@\n+      oop m = k->java_mirror();\n@@ -1448,1 +1612,1 @@\n-    if (!CompressedOops::is_null(obj)) {\n+    if (obj != nullptr) {\n@@ -1497,1 +1661,1 @@\n-  if (ArchiveHeapLoader::is_in_use()) {\n+  if (is_archived_heap_in_use()) {\n@@ -1742,2 +1906,2 @@\n-    oop obj = RawAccess<>::oop_load(p);\n-    if (!CompressedOops::is_null(obj)) {\n+    oop obj = HeapAccess<>::oop_load(p);\n+    if (obj != nullptr) {\n@@ -2043,1 +2207,1 @@\n-  if (k != nullptr && ArchiveHeapLoader::is_in_use()) {\n+  if (k != nullptr && is_archived_heap_in_use()) {\n@@ -2064,1 +2228,0 @@\n-    _dumped_interned_strings = new (mtClass)DumpedInternedStrings(INITIAL_TABLE_SIZE, MAX_TABLE_SIZE);\n@@ -2069,0 +2232,8 @@\n+void HeapShared::init_heap_writer() {\n+  if (HeapShared::is_writing_streaming_mode()) {\n+    AOTStreamedHeapWriter::init();\n+  } else {\n+    AOTMappedHeapWriter::init();\n+  }\n+}\n+\n@@ -2119,15 +2290,0 @@\n-\/\/ Keep track of the contents of the archived interned string table. This table\n-\/\/ is used only by CDSHeapVerifier.\n-void HeapShared::add_to_dumped_interned_strings(oop string) {\n-  assert_at_safepoint(); \/\/ DumpedInternedStrings uses raw oops\n-  assert(!ArchiveHeapWriter::is_string_too_large_to_archive(string), \"must be\");\n-  bool created;\n-  _dumped_interned_strings->put_if_absent(string, true, &created);\n-  if (created) {\n-    \/\/ Prevent string deduplication from changing the value field to\n-    \/\/ something not in the archive.\n-    java_lang_String::set_deduplication_forbidden(string);\n-    _dumped_interned_strings->maybe_grow();\n-  }\n-}\n-\n@@ -2135,1 +2291,5 @@\n-  return _dumped_interned_strings->get(o) != nullptr;\n+  if (is_writing_mapping_mode()) {\n+    return AOTMappedHeapWriter::is_dumped_interned_string(o);\n+  } else {\n+    return AOTStreamedHeapWriter::is_dumped_interned_string(o);\n+  }\n@@ -2144,4 +2304,6 @@\n-  delete _dumped_interned_strings;\n-  _dumped_interned_strings = nullptr;\n-\n-  ArchiveHeapWriter::delete_tables_with_raw_oops();\n+  if (is_writing_mapping_mode()) {\n+    AOTMappedHeapWriter::delete_tables_with_raw_oops();\n+  } else {\n+    assert(is_writing_streaming_mode(), \"what other mode?\");\n+    AOTStreamedHeapWriter::delete_tables_with_raw_oops();\n+  }\n@@ -2244,0 +2406,29 @@\n+bool HeapShared::is_metadata_field(oop src_obj, int offset) {\n+  bool result = false;\n+  do_metadata_offsets(src_obj, [&](int metadata_offset) {\n+    if (metadata_offset == offset) {\n+      result = true;\n+    }\n+  });\n+  return result;\n+}\n+\n+void HeapShared::remap_dumped_metadata(oop src_obj, address archived_object) {\n+  do_metadata_offsets(src_obj, [&](int offset) {\n+    Metadata** buffered_field_addr = (Metadata**)(archived_object + offset);\n+    Metadata* native_ptr = *buffered_field_addr;\n+\n+    if (native_ptr == nullptr) {\n+      return;\n+    }\n+\n+    if (RegeneratedClasses::has_been_regenerated(native_ptr)) {\n+      native_ptr = RegeneratedClasses::get_regenerated_object(native_ptr);\n+    }\n+\n+    address buffered_native_ptr = ArchiveBuilder::current()->get_buffered_addr((address)native_ptr);\n+    address requested_native_ptr = ArchiveBuilder::current()->to_requested(buffered_native_ptr);\n+    *buffered_field_addr = (Metadata*)requested_native_ptr;\n+  });\n+}\n+\n","filename":"src\/hotspot\/share\/cds\/heapShared.cpp","additions":364,"deletions":173,"binary":false,"changes":537,"status":"modified"},{"patch":"@@ -50,1 +50,0 @@\n-class ArchiveHeapInfo;\n@@ -140,1 +139,128 @@\n-struct LoadedArchiveHeapRegion;\n+enum class HeapArchiveMode {\n+  _uninitialized,\n+  _mapping,\n+  _streaming\n+};\n+\n+class ArchiveMappedHeapHeader {\n+  size_t           _ptrmap_start_pos; \/\/ The first bit in the ptrmap corresponds to this position in the heap.\n+  size_t           _oopmap_start_pos; \/\/ The first bit in the oopmap corresponds to this position in the heap.\n+  HeapRootSegments _root_segments;    \/\/ Heap root segments info\n+\n+public:\n+  ArchiveMappedHeapHeader();\n+  ArchiveMappedHeapHeader(size_t ptrmap_start_pos,\n+                          size_t oopmap_start_pos,\n+                          HeapRootSegments root_segments);\n+\n+  size_t ptrmap_start_pos() const { return _ptrmap_start_pos; }\n+  size_t oopmap_start_pos() const { return _oopmap_start_pos; }\n+  HeapRootSegments root_segments() const { return _root_segments; }\n+\n+  \/\/ This class is trivially copyable and assignable.\n+  ArchiveMappedHeapHeader(const ArchiveMappedHeapHeader&) = default;\n+  ArchiveMappedHeapHeader& operator=(const ArchiveMappedHeapHeader&) = default;\n+};\n+\n+\n+class ArchiveStreamedHeapHeader {\n+  size_t _forwarding_offset;                      \/\/ Offset of forwarding information in the heap region.\n+  size_t _roots_offset;                           \/\/ Start position for the roots\n+  size_t _root_highest_object_index_table_offset; \/\/ Offset of root dfs depth information\n+  size_t _num_roots;                              \/\/ Number of embedded roots\n+  size_t _num_archived_objects;                   \/\/ The number of archived heap objects\n+\n+public:\n+  ArchiveStreamedHeapHeader();\n+  ArchiveStreamedHeapHeader(size_t forwarding_offset,\n+                            size_t roots_offset,\n+                            size_t num_roots,\n+                            size_t root_highest_object_index_table_offset,\n+                            size_t num_archived_objects);\n+\n+  size_t forwarding_offset() const { return _forwarding_offset; }\n+  size_t roots_offset() const { return _roots_offset; }\n+  size_t num_roots() const { return _num_roots; }\n+  size_t root_highest_object_index_table_offset() const { return _root_highest_object_index_table_offset; }\n+  size_t num_archived_objects() const { return _num_archived_objects; }\n+\n+  \/\/ This class is trivially copyable and assignable.\n+  ArchiveStreamedHeapHeader(const ArchiveStreamedHeapHeader&) = default;\n+  ArchiveStreamedHeapHeader& operator=(const ArchiveStreamedHeapHeader&) = default;\n+};\n+\n+class ArchiveMappedHeapInfo {\n+  MemRegion _buffer_region;             \/\/ Contains the archived objects to be written into the CDS archive.\n+  CHeapBitMap _oopmap;\n+  CHeapBitMap _ptrmap;\n+  HeapRootSegments _root_segments;\n+  size_t _oopmap_start_pos;             \/\/ How many zeros were removed from the beginning of the bit map?\n+  size_t _ptrmap_start_pos;             \/\/ How many zeros were removed from the beginning of the bit map?\n+\n+public:\n+  ArchiveMappedHeapInfo() :\n+    _buffer_region(),\n+    _oopmap(128, mtClassShared),\n+    _ptrmap(128, mtClassShared),\n+    _root_segments(),\n+    _oopmap_start_pos(),\n+    _ptrmap_start_pos() {}\n+  bool is_used() { return !_buffer_region.is_empty(); }\n+\n+  MemRegion buffer_region() { return _buffer_region; }\n+  void set_buffer_region(MemRegion r) { _buffer_region = r; }\n+\n+  char* buffer_start() { return (char*)_buffer_region.start(); }\n+  size_t buffer_byte_size() { return _buffer_region.byte_size();    }\n+\n+  CHeapBitMap* oopmap() { return &_oopmap; }\n+  CHeapBitMap* ptrmap() { return &_ptrmap; }\n+\n+  void set_oopmap_start_pos(size_t start_pos) { _oopmap_start_pos = start_pos; }\n+  void set_ptrmap_start_pos(size_t start_pos) { _ptrmap_start_pos = start_pos; }\n+\n+  void set_root_segments(HeapRootSegments segments) { _root_segments = segments; };\n+  HeapRootSegments root_segments() { return _root_segments; }\n+\n+  ArchiveMappedHeapHeader create_header();\n+};\n+\n+class ArchiveStreamedHeapInfo {\n+  MemRegion _buffer_region;             \/\/ Contains the archived objects to be written into the CDS archive.\n+  CHeapBitMap _oopmap;\n+  size_t _roots_offset;                 \/\/ Offset of the HeapShared::roots() object, from the bottom\n+                                        \/\/ of the archived heap objects, in bytes.\n+  size_t _num_roots;\n+\n+  size_t _forwarding_offset;            \/\/ Offset of forwarding information from the bottom\n+  size_t _root_highest_object_index_table_offset; \/\/ Offset to root dfs depth information\n+  size_t _num_archived_objects;         \/\/ The number of archived objects written into the CDS archive.\n+\n+public:\n+  ArchiveStreamedHeapInfo()\n+    : _buffer_region(),\n+      _oopmap(128, mtClassShared),\n+      _roots_offset(),\n+      _forwarding_offset(),\n+      _root_highest_object_index_table_offset(),\n+      _num_archived_objects() {}\n+\n+  bool is_used() { return !_buffer_region.is_empty(); }\n+\n+  void set_buffer_region(MemRegion r) { _buffer_region = r; }\n+  MemRegion buffer_region() { return _buffer_region; }\n+  char* buffer_start() { return (char*)_buffer_region.start(); }\n+  size_t buffer_byte_size() { return _buffer_region.byte_size();    }\n+\n+  CHeapBitMap* oopmap() { return &_oopmap; }\n+  void set_roots_offset(size_t n) { _roots_offset = n; }\n+  size_t roots_offset() { return _roots_offset; }\n+  void set_num_roots(size_t n) { _num_roots = n; }\n+  size_t num_roots() { return _num_roots; }\n+  void set_forwarding_offset(size_t n) { _forwarding_offset = n; }\n+  void set_root_highest_object_index_table_offset(size_t n) { _root_highest_object_index_table_offset = n; }\n+  void set_num_archived_objects(size_t n) { _num_archived_objects = n; }\n+  size_t num_archived_objects() { return _num_archived_objects; }\n+\n+  ArchiveStreamedHeapHeader create_header();\n+};\n@@ -146,0 +272,13 @@\n+  static void initialize_loading_mode(HeapArchiveMode mode) NOT_CDS_JAVA_HEAP_RETURN;\n+  static void initialize_writing_mode() NOT_CDS_JAVA_HEAP_RETURN;\n+\n+  inline static bool is_loading() NOT_CDS_JAVA_HEAP_RETURN_(false);\n+\n+  inline static bool is_loading_streaming_mode() NOT_CDS_JAVA_HEAP_RETURN_(false);\n+  inline static bool is_loading_mapping_mode() NOT_CDS_JAVA_HEAP_RETURN_(false);\n+\n+  inline static bool is_writing() NOT_CDS_JAVA_HEAP_RETURN_(false);\n+\n+  inline static bool is_writing_streaming_mode() NOT_CDS_JAVA_HEAP_RETURN_(false);\n+  inline static bool is_writing_mapping_mode() NOT_CDS_JAVA_HEAP_RETURN_(false);\n+\n@@ -154,0 +293,12 @@\n+  static bool is_archived_heap_in_use() NOT_CDS_JAVA_HEAP_RETURN_(false);\n+  static bool can_use_archived_heap() NOT_CDS_JAVA_HEAP_RETURN_(false);\n+  static bool is_too_large_to_archive(size_t size);\n+  static bool is_string_too_large_to_archive(oop string);\n+  static bool is_too_large_to_archive(oop obj);\n+\n+  static void initialize_streaming() NOT_CDS_JAVA_HEAP_RETURN;\n+  static void enable_gc() NOT_CDS_JAVA_HEAP_RETURN;\n+  static void materialize_thread_object() NOT_CDS_JAVA_HEAP_RETURN;\n+  static void add_to_dumped_interned_strings(oop string) NOT_CDS_JAVA_HEAP_RETURN;\n+  static void finalize_initialization(FileMapInfo* static_mapinfo) NOT_CDS_JAVA_HEAP_RETURN;\n+\n@@ -156,1 +307,2 @@\n-  static DumpedInternedStrings *_dumped_interned_strings;\n+  static HeapArchiveMode _heap_load_mode;\n+  static HeapArchiveMode _heap_write_mode;\n@@ -288,2 +440,0 @@\n-  static GrowableArrayCHeap<OopHandle, mtClassShared>* _root_segments;\n-  static int _root_segment_max_size_elems;\n@@ -332,10 +482,0 @@\n-  static int init_loaded_regions(FileMapInfo* mapinfo, LoadedArchiveHeapRegion* loaded_regions,\n-                                 MemRegion& archive_space);\n-  static void sort_loaded_regions(LoadedArchiveHeapRegion* loaded_regions, int num_loaded_regions,\n-                                  uintptr_t buffer);\n-  static bool load_regions(FileMapInfo* mapinfo, LoadedArchiveHeapRegion* loaded_regions,\n-                           int num_loaded_regions, uintptr_t buffer);\n-  static void init_loaded_heap_relocation(LoadedArchiveHeapRegion* reloc_info,\n-                                          int num_loaded_regions);\n-  static void fill_failed_loaded_region();\n-  static void mark_native_pointers(oop orig_obj);\n@@ -386,4 +526,1 @@\n-  static CachedOopInfo* get_cached_oop_info(oop orig_obj) {\n-    OopHandle oh(&orig_obj);\n-    return _archived_object_cache->get(oh);\n-  }\n+  static CachedOopInfo* get_cached_oop_info(oop orig_obj);\n@@ -397,1 +534,0 @@\n-  static void add_to_dumped_interned_strings(oop string);\n@@ -403,0 +539,5 @@\n+  static bool is_metadata_field(oop src_obj, int offset);\n+  template <typename T> static void do_metadata_offsets(oop src_obj, T callback);\n+  static void remap_dumped_metadata(oop src_obj, address archived_object);\n+  inline static void remap_loaded_metadata(oop obj);\n+  inline static oop maybe_remap_referent(bool is_java_lang_ref, size_t field_offset, oop referent);\n@@ -405,0 +546,1 @@\n+  static uintptr_t archive_location(oop src_obj);\n@@ -438,1 +580,3 @@\n-  static void write_heap(ArchiveHeapInfo* heap_info) NOT_CDS_JAVA_HEAP_RETURN;\n+  static void finish_materialize_objects() NOT_CDS_JAVA_HEAP_RETURN;\n+\n+  static void write_heap(ArchiveMappedHeapInfo* mapped_heap_info, ArchiveStreamedHeapInfo* streamed_heap_info) NOT_CDS_JAVA_HEAP_RETURN;\n@@ -454,0 +598,1 @@\n+  static void init_heap_writer() NOT_CDS_JAVA_HEAP_RETURN;\n@@ -455,2 +600,0 @@\n-  static void add_root_segment(objArrayOop segment_oop) NOT_CDS_JAVA_HEAP_RETURN;\n-  static void init_root_segment_sizes(int max_size_elems) NOT_CDS_JAVA_HEAP_RETURN;\n@@ -478,1 +621,0 @@\n-};\n@@ -480,13 +622,6 @@\n-#if INCLUDE_CDS_JAVA_HEAP\n-class DumpedInternedStrings :\n-  public ResizeableHashTable<oop, bool,\n-                           AnyObj::C_HEAP,\n-                           mtClassShared,\n-                           HeapShared::string_oop_hash>\n-{\n-public:\n-  DumpedInternedStrings(unsigned size, unsigned max_size) :\n-    ResizeableHashTable<oop, bool,\n-                                AnyObj::C_HEAP,\n-                                mtClassShared,\n-                                HeapShared::string_oop_hash>(size, max_size) {}\n+  static void log_heap_roots();\n+\n+  static intptr_t log_target_location(oop source_oop);\n+  static void log_oop_info(outputStream* st, oop source_oop, address archived_object_start, address archived_object_end);\n+  static void log_oop_info(outputStream* st, oop source_oop);\n+  static void log_oop_details(oop source_oop, address buffered_addr);\n@@ -494,1 +629,0 @@\n-#endif\n","filename":"src\/hotspot\/share\/cds\/heapShared.hpp","additions":172,"deletions":38,"binary":false,"changes":210,"status":"modified"},{"patch":"@@ -0,0 +1,114 @@\n+\/*\n+ * Copyright (c) 2025, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef SHARE_CDS_HEAPSHARED_INLINE_HPP\n+#define SHARE_CDS_HEAPSHARED_INLINE_HPP\n+\n+#include \"cds\/heapShared.hpp\"\n+\n+#include \"cds\/aotReferenceObjSupport.hpp\"\n+#include \"cds\/regeneratedClasses.hpp\"\n+#include \"classfile\/javaClasses.hpp\"\n+#include \"utilities\/macros.hpp\"\n+\n+#if INCLUDE_CDS_JAVA_HEAP\n+\n+inline bool HeapShared::is_loading() {\n+  return _heap_load_mode != HeapArchiveMode::_uninitialized;\n+}\n+\n+inline bool HeapShared::is_loading_streaming_mode() {\n+  assert(_heap_load_mode != HeapArchiveMode::_uninitialized, \"not initialized yet\");\n+  return _heap_load_mode == HeapArchiveMode::_streaming;\n+}\n+\n+inline bool HeapShared::is_loading_mapping_mode() {\n+  assert(_heap_load_mode != HeapArchiveMode::_uninitialized, \"not initialized yet\");\n+  return _heap_load_mode == HeapArchiveMode::_mapping;\n+}\n+\n+inline bool HeapShared::is_writing() {\n+  return _heap_write_mode != HeapArchiveMode::_uninitialized;\n+}\n+\n+inline bool HeapShared::is_writing_streaming_mode() {\n+  assert(_heap_write_mode != HeapArchiveMode::_uninitialized, \"not initialized yet\");\n+  return _heap_write_mode == HeapArchiveMode::_streaming;\n+}\n+\n+inline bool HeapShared::is_writing_mapping_mode() {\n+  assert(_heap_write_mode != HeapArchiveMode::_uninitialized, \"not initialized yet\");\n+  return _heap_write_mode == HeapArchiveMode::_mapping;\n+}\n+\n+\/\/ Keep the knowledge about which objects have what metadata in one single place\n+template <typename T>\n+void HeapShared::do_metadata_offsets(oop src_obj, T callback) {\n+  if (java_lang_Class::is_instance(src_obj)) {\n+    assert(java_lang_Class::klass_offset() < java_lang_Class::array_klass_offset(),\n+           \"metadata offsets must be sorted\");\n+    callback(java_lang_Class::klass_offset());\n+    callback(java_lang_Class::array_klass_offset());\n+  } else if (java_lang_invoke_ResolvedMethodName::is_instance(src_obj)) {\n+    callback(java_lang_invoke_ResolvedMethodName::vmtarget_offset());\n+  }\n+}\n+\n+inline void HeapShared::remap_loaded_metadata(oop src_obj) {\n+  do_metadata_offsets(src_obj, [&](int offset) {\n+    Metadata* metadata = src_obj->metadata_field(offset);\n+    if (metadata != nullptr) {\n+      metadata = (Metadata*)(address(metadata) + AOTMetaspace::relocation_delta());\n+      src_obj->metadata_field_put(offset, metadata);\n+    }\n+  });\n+}\n+\n+inline oop HeapShared::maybe_remap_referent(bool is_java_lang_ref, size_t field_offset, oop referent) {\n+  if (referent == nullptr) {\n+    return nullptr;\n+  }\n+\n+  if (is_java_lang_ref && AOTReferenceObjSupport::skip_field((int)field_offset)) {\n+    return nullptr;\n+  }\n+\n+  if (java_lang_Class::is_instance(referent)) {\n+    Klass* k = java_lang_Class::as_Klass(referent);\n+    if (RegeneratedClasses::has_been_regenerated(k)) {\n+      referent = RegeneratedClasses::get_regenerated_object(k)->java_mirror();\n+    }\n+    \/\/ When the source object points to a \"real\" mirror, the buffered object should point\n+    \/\/ to the \"scratch\" mirror, which has all unarchivable fields scrubbed (to be reinstated\n+    \/\/ at run time).\n+    referent = HeapShared::scratch_java_mirror(referent);\n+    assert(referent != nullptr, \"must be\");\n+  }\n+\n+  return referent;\n+}\n+\n+#endif \/\/ INCLUDE_CDS_JAVA_HEAP\n+\n+#endif \/\/ SHARE_CDS_HEAPSHARED_INLINE_HPP\n","filename":"src\/hotspot\/share\/cds\/heapShared.inline.hpp","additions":114,"deletions":0,"binary":false,"changes":114,"status":"added"},{"patch":"@@ -41,0 +41,1 @@\n+#include \"runtime\/init.hpp\"\n@@ -152,1 +153,2 @@\n-  NoSafepointVerifier no_safepoints;\n+  \/\/ Before is_init_completed(), GC is not allowed to run.\n+  NoSafepointVerifier no_safepoints(is_init_completed());\n","filename":"src\/hotspot\/share\/classfile\/classLoaderDataGraph.cpp","additions":3,"deletions":1,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -27,1 +27,1 @@\n-#include \"cds\/heapShared.hpp\"\n+#include \"cds\/heapShared.inline.hpp\"\n@@ -155,0 +155,29 @@\n+void ClassLoaderDataShared::load_archived_platform_and_system_class_loaders() {\n+#if INCLUDE_CDS_JAVA_HEAP\n+  \/\/ The streaming object loader prefers loading the class loader related objects before\n+  \/\/  the CLD constructor which has a NoSafepointVerifier.\n+  if (!HeapShared::is_loading_streaming_mode()) {\n+    return;\n+  }\n+\n+  \/\/ Ensure these class loaders are eagerly materialized before their CLDs are created.\n+  HeapShared::get_root(_platform_loader_root_index, false \/* clear *\/);\n+  HeapShared::get_root(_system_loader_root_index, false \/* clear *\/);\n+\n+  if (Universe::is_module_initialized() || !CDSConfig::is_using_full_module_graph()) {\n+    return;\n+  }\n+\n+  \/\/ When using the full module graph, we need to load unnamed modules too.\n+  ModuleEntry* platform_loader_module_entry = _archived_platform_loader_data.unnamed_module();\n+  if (platform_loader_module_entry != nullptr) {\n+    platform_loader_module_entry->preload_archived_oops();\n+  }\n+\n+  ModuleEntry* system_loader_module_entry = _archived_system_loader_data.unnamed_module();\n+  if (system_loader_module_entry != nullptr) {\n+    system_loader_module_entry->preload_archived_oops();\n+  }\n+#endif\n+}\n+\n","filename":"src\/hotspot\/share\/classfile\/classLoaderDataShared.cpp","additions":30,"deletions":1,"binary":false,"changes":31,"status":"modified"},{"patch":"@@ -41,0 +41,1 @@\n+  static void load_archived_platform_and_system_class_loaders() NOT_CDS_JAVA_HEAP_RETURN;\n","filename":"src\/hotspot\/share\/classfile\/classLoaderDataShared.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -28,1 +28,0 @@\n-#include \"cds\/archiveHeapLoader.hpp\"\n@@ -30,1 +29,1 @@\n-#include \"cds\/heapShared.hpp\"\n+#include \"cds\/heapShared.inline.hpp\"\n@@ -981,1 +980,1 @@\n-    if (ArchiveHeapLoader::is_in_use()) {\n+    if (HeapShared::is_archived_heap_in_use()) {\n","filename":"src\/hotspot\/share\/classfile\/javaClasses.cpp","additions":2,"deletions":3,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -549,0 +549,4 @@\n+void ModuleEntry::preload_archived_oops() {\n+  (void)HeapShared::get_root(_archived_module_index, false \/* clear *\/);\n+}\n+\n","filename":"src\/hotspot\/share\/classfile\/moduleEntry.cpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -209,0 +209,1 @@\n+  void preload_archived_oops();\n","filename":"src\/hotspot\/share\/classfile\/moduleEntry.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -742,0 +742,2 @@\n+  ClassLoaderDataShared::load_archived_platform_and_system_class_loaders();\n+\n","filename":"src\/hotspot\/share\/classfile\/modules.cpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -25,0 +25,1 @@\n+#include \"cds\/aotMappedHeapLoader.hpp\"\n@@ -26,2 +27,0 @@\n-#include \"cds\/archiveHeapLoader.inline.hpp\"\n-#include \"cds\/archiveHeapWriter.hpp\"\n@@ -29,1 +28,1 @@\n-#include \"cds\/heapShared.hpp\"\n+#include \"cds\/heapShared.inline.hpp\"\n@@ -83,1 +82,1 @@\n-  assert(ArchiveHeapLoader::is_in_use(), \"sanity\");\n+  assert(AOTMappedHeapLoader::is_in_use(), \"sanity\");\n@@ -319,0 +318,1 @@\n+}\n@@ -321,4 +321,2 @@\n-  if (ArchiveHeapLoader::is_in_use()) {\n-    _shared_strings_array = OopHandle(Universe::vm_global(), HeapShared::get_root(_shared_strings_array_root_index));\n-  }\n-#endif\n+void StringTable::load_shared_strings_array() {\n+  _shared_strings_array = OopHandle(Universe::vm_global(), HeapShared::get_root(_shared_strings_array_root_index));\n@@ -326,0 +324,1 @@\n+#endif\n@@ -935,0 +934,1 @@\n+  assert(HeapShared::is_loading_mapping_mode(), \"should not reach here\");\n@@ -939,0 +939,3 @@\n+  if (!AOTMappedHeapLoader::is_in_use()) {\n+    return nullptr;\n+  }\n@@ -946,0 +949,3 @@\n+  if (!AOTMappedHeapLoader::is_in_use()) {\n+    return nullptr;\n+  }\n@@ -958,0 +964,2 @@\n+  assert(HeapShared::is_writing_mapping_mode(), \"should not reach here\");\n+\n@@ -980,1 +988,1 @@\n-  if (!ArchiveHeapWriter::is_too_large_to_archive(single_array_size)) {\n+  if (!HeapShared::is_too_large_to_archive(single_array_size)) {\n@@ -991,1 +999,1 @@\n-    if (ArchiveHeapWriter::is_too_large_to_archive(secondary_array_size)) {\n+    if (HeapShared::is_too_large_to_archive(secondary_array_size)) {\n@@ -1017,1 +1025,1 @@\n-      assert(!ArchiveHeapWriter::is_too_large_to_archive(secondary), \"sanity\");\n+      assert(!HeapShared::is_too_large_to_archive(secondary), \"sanity\");\n@@ -1027,0 +1035,1 @@\n+  assert(HeapShared::is_writing_mapping_mode(), \"should not reach here\");\n@@ -1030,1 +1039,1 @@\n-    if (ArchiveHeapWriter::is_too_large_to_archive(next_size)) {\n+    if (HeapShared::is_too_large_to_archive(next_size)) {\n@@ -1053,0 +1062,1 @@\n+  assert(HeapShared::is_writing_mapping_mode(), \"should not reach here\");\n@@ -1060,1 +1070,1 @@\n-    if (string != nullptr && !ArchiveHeapWriter::is_string_too_large_to_archive(string)) {\n+    if (string != nullptr && !HeapShared::is_string_too_large_to_archive(string)) {\n@@ -1062,1 +1072,1 @@\n-      \/\/ - If there are no other refernences to it, it won't be stored into the archive,\n+      \/\/ - If there are no other references to it, it won't be stored into the archive,\n@@ -1064,1 +1074,1 @@\n-      \/\/ - If there's a referece to it, we will report an error inside HeapShared.cpp and\n+      \/\/ - If there's a reference to it, we will report an error inside HeapShared.cpp and\n@@ -1098,1 +1108,1 @@\n-    if (string != nullptr && !ArchiveHeapWriter::is_string_too_large_to_archive(string)) {\n+    if (string != nullptr && !HeapShared::is_string_too_large_to_archive(string)) {\n@@ -1112,0 +1122,1 @@\n+  assert(HeapShared::is_writing_mapping_mode(), \"should not reach here\");\n@@ -1121,1 +1132,1 @@\n-  } else if (!ArchiveHeapLoader::is_in_use()) {\n+  } else if (!AOTMappedHeapLoader::is_in_use()) {\n","filename":"src\/hotspot\/share\/classfile\/stringTable.cpp","additions":28,"deletions":17,"binary":false,"changes":45,"status":"modified"},{"patch":"@@ -131,1 +131,1 @@\n-  \/\/     ArchiveHeapWriter::is_too_large_to_archive(). In this case, the index is splited into two\n+  \/\/     AOTMappedHeapWriter::is_too_large_to_archive(). In this case, the index is splited into two\n@@ -150,0 +150,1 @@\n+  static void load_shared_strings_array() NOT_CDS_JAVA_HEAP_RETURN;\n","filename":"src\/hotspot\/share\/classfile\/stringTable.hpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -1188,0 +1188,1 @@\n+  assert(java_lang_Class::module(java_mirror) != nullptr, \"must have been archived\");\n@@ -1205,1 +1206,0 @@\n-  assert(java_lang_Class::module(java_mirror) != nullptr, \"must have been archived\");\n","filename":"src\/hotspot\/share\/classfile\/systemDictionary.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -26,1 +26,1 @@\n-#include \"cds\/archiveHeapLoader.hpp\"\n+#include \"cds\/aotMappedHeapLoader.hpp\"\n@@ -28,0 +28,1 @@\n+#include \"cds\/heapShared.inline.hpp\"\n@@ -30,0 +31,1 @@\n+#include \"classfile\/classLoaderDataShared.hpp\"\n@@ -135,15 +137,22 @@\n-    \/\/ It's unsafe to access the archived heap regions before they\n-    \/\/ are fixed up, so we must do the fixup as early as possible\n-    \/\/ before the archived java objects are accessed by functions\n-    \/\/ such as java_lang_Class::restore_archived_mirror and\n-    \/\/ ConstantPool::restore_unshareable_info (restores the archived\n-    \/\/ resolved_references array object).\n-    \/\/\n-    \/\/ ArchiveHeapLoader::fixup_regions fills the empty\n-    \/\/ spaces in the archived heap regions and may use\n-    \/\/ vmClasses::Object_klass(), so we can do this only after\n-    \/\/ Object_klass is resolved. See the above resolve_through()\n-    \/\/ call. No mirror objects are accessed\/restored in the above call.\n-    \/\/ Mirrors are restored after java.lang.Class is loaded.\n-    ArchiveHeapLoader::fixup_region();\n-\n+#if INCLUDE_CDS_JAVA_HEAP\n+    if (HeapShared::is_loading() && HeapShared::is_loading_mapping_mode()) {\n+      \/\/ It's unsafe to access the archived heap regions before they\n+      \/\/ are fixed up, so we must do the fixup as early as possible\n+      \/\/ before the archived java objects are accessed by functions\n+      \/\/ such as java_lang_Class::restore_archived_mirror and\n+      \/\/ ConstantPool::restore_unshareable_info (restores the archived\n+      \/\/ resolved_references array object).\n+      \/\/\n+      \/\/ AOTMappedHeapLoader::fixup_regions fills the empty\n+      \/\/ spaces in the archived heap regions and may use\n+      \/\/ vmClasses::Object_klass(), so we can do this only after\n+      \/\/ Object_klass is resolved. See the above resolve_through()\n+      \/\/ call. No mirror objects are accessed\/restored in the above call.\n+      \/\/ Mirrors are restored after java.lang.Class is loaded.\n+      AOTMappedHeapLoader::fixup_region();\n+    }\n+    if (HeapShared::is_archived_heap_in_use() && !CDSConfig::is_using_full_module_graph()) {\n+      \/\/ Need to remove all the archived java.lang.Module objects from HeapShared::roots().\n+      ClassLoaderDataShared::clear_archived_oops();\n+    }\n+#endif \/\/ INCLUDE_CDS_JAVA_HEAP\n","filename":"src\/hotspot\/share\/classfile\/vmClasses.cpp","additions":25,"deletions":16,"binary":false,"changes":41,"status":"modified"},{"patch":"@@ -638,0 +638,4 @@\n+size_t CollectedHeap::bootstrap_max_memory() const {\n+  return MaxNewSize;\n+}\n+\n","filename":"src\/hotspot\/share\/gc\/shared\/collectedHeap.cpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -507,0 +507,1 @@\n+  virtual size_t bootstrap_max_memory() const;\n","filename":"src\/hotspot\/share\/gc\/shared\/collectedHeap.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -25,0 +25,1 @@\n+#include \"cds\/cdsConfig.hpp\"\n","filename":"src\/hotspot\/share\/gc\/shared\/stringdedup\/stringDedupConfig.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -25,0 +25,2 @@\n+#include \"cds\/aotMappedHeapLoader.hpp\"\n+#include \"cds\/heapShared.inline.hpp\"\n@@ -517,0 +519,1 @@\n+  assert(!HeapShared::is_loading_streaming_mode(), \"should not reach here\");\n@@ -562,0 +565,1 @@\n+  assert(!HeapShared::is_loading_streaming_mode(), \"should not reach here\");\n@@ -612,1 +616,2 @@\n-  if ((StringTable::shared_entry_count() > 0) &&\n+  if (AOTMappedHeapLoader::is_in_use() &&\n+      (StringTable::shared_entry_count() > 0) &&\n","filename":"src\/hotspot\/share\/gc\/shared\/stringdedup\/stringDedupTable.cpp","additions":6,"deletions":1,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -28,1 +28,1 @@\n-#include \"cds\/archiveHeapWriter.hpp\"\n+#include \"cds\/aotMappedHeapWriter.hpp\"\n@@ -2814,1 +2814,1 @@\n-  guarantee(ShenandoahHeapRegion::region_size_bytes() >= ArchiveHeapWriter::MIN_GC_REGION_ALIGNMENT, \"Must be\");\n+  guarantee(ShenandoahHeapRegion::region_size_bytes() >= AOTMappedHeapWriter::MIN_GC_REGION_ALIGNMENT, \"Must be\");\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeap.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -211,0 +211,7 @@\n+  if (VerifyArchivedFields > 0) {\n+    \/\/ ZGC doesn't support verifying at arbitrary points as our normal state is that everything in the\n+    \/\/ heap looks completely insane. Only at some particular points does the heap look sort of sane.\n+    \/\/ So instead of verifying we trigger a GC that does its own verification when it's suitable.\n+    FLAG_SET_DEFAULT(VerifyArchivedFields, 2);\n+  }\n+\n","filename":"src\/hotspot\/share\/gc\/z\/zArguments.cpp","additions":7,"deletions":0,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -37,0 +37,1 @@\n+#include \"gc\/z\/zHeuristics.hpp\"\n@@ -301,0 +302,4 @@\n+size_t ZCollectedHeap::bootstrap_max_memory() const {\n+  return MaxHeapSize - ZHeuristics::significant_young_overhead();\n+}\n+\n","filename":"src\/hotspot\/share\/gc\/z\/zCollectedHeap.cpp","additions":5,"deletions":0,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -118,0 +118,2 @@\n+  size_t bootstrap_max_memory() const override;\n+\n","filename":"src\/hotspot\/share\/gc\/z\/zCollectedHeap.hpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -34,0 +34,1 @@\n+#include \"runtime\/init.hpp\"\n@@ -920,0 +921,6 @@\n+\n+    if (!is_init_completed()) {\n+      \/\/ Not allowed to start GCs yet\n+      continue;\n+    }\n+\n","filename":"src\/hotspot\/share\/gc\/z\/zDirector.cpp","additions":7,"deletions":0,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -480,1 +480,1 @@\n-static inline traceid load_java_thread_id(const Thread* t) {\n+static inline traceid load_java_thread_id(const JavaThread* t) {\n@@ -482,3 +482,2 @@\n-  assert(t->is_Java_thread(), \"invariant\");\n-  oop threadObj = JavaThread::cast(t)->threadObj();\n-  return threadObj != nullptr ? AccessThreadTraceId::id(threadObj) : 0;\n+  oop threadObj = t->threadObj();\n+  return threadObj != nullptr ? AccessThreadTraceId::id(threadObj) : static_cast<traceid>(t->monitor_owner_id());\n@@ -505,1 +504,1 @@\n-      tid = load_java_thread_id(t);\n+      tid = load_java_thread_id(JavaThread::cast(t));\n","filename":"src\/hotspot\/share\/jfr\/support\/jfrThreadLocal.cpp","additions":4,"deletions":5,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -29,0 +29,1 @@\n+#include \"cds\/heapShared.hpp\"\n","filename":"src\/hotspot\/share\/memory\/metaspace.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -26,1 +26,0 @@\n-#include \"cds\/archiveHeapLoader.hpp\"\n@@ -29,1 +28,1 @@\n-#include \"cds\/heapShared.hpp\"\n+#include \"cds\/heapShared.inline.hpp\"\n@@ -323,1 +322,1 @@\n-  if (ArchiveHeapLoader::is_in_use()) {\n+  if (HeapShared::is_archived_heap_in_use()) {\n@@ -557,1 +556,1 @@\n-        ArchiveHeapLoader::is_in_use() &&\n+        HeapShared::is_archived_heap_in_use() &&\n@@ -559,2 +558,0 @@\n-      assert(ArchiveHeapLoader::can_use(), \"Sanity\");\n-\n@@ -569,1 +566,1 @@\n-      \/\/ _basic_type_mirrors[T_INT], etc, are null if archived heap is not mapped.\n+      \/\/ _basic_type_mirrors[T_INT], etc, are null if not using an archived heap\n@@ -909,0 +906,15 @@\n+  \/\/ Add main_thread to threads list to finish barrier setup with\n+  \/\/ on_thread_attach.  Should be before starting to build Java objects in\n+  \/\/ the AOT heap loader, which invokes barriers.\n+  {\n+    JavaThread* main_thread = JavaThread::current();\n+    MutexLocker mu(Threads_lock);\n+    Threads::add(main_thread);\n+  }\n+\n+  HeapShared::initialize_writing_mode();\n+\n+  \/\/ Create the string table before the AOT object archive is loaded,\n+  \/\/ as it might need to access the string table.\n+  StringTable::create_table();\n+\n@@ -931,1 +943,0 @@\n-  StringTable::create_table();\n@@ -1175,0 +1186,1 @@\n+\n","filename":"src\/hotspot\/share\/memory\/universe.cpp","additions":20,"deletions":8,"binary":false,"changes":28,"status":"modified"},{"patch":"@@ -27,2 +27,0 @@\n-#include \"cds\/archiveHeapLoader.hpp\"\n-#include \"cds\/archiveHeapWriter.hpp\"\n@@ -30,1 +28,1 @@\n-#include \"cds\/heapShared.hpp\"\n+#include \"cds\/heapShared.inline.hpp\"\n@@ -361,1 +359,1 @@\n-            if (!ArchiveHeapWriter::is_string_too_large_to_archive(obj)) {\n+            if (!HeapShared::is_string_too_large_to_archive(obj)) {\n@@ -401,1 +399,1 @@\n-    if (ArchiveHeapLoader::is_in_use() &&\n+    if (HeapShared::is_archived_heap_in_use() &&\n","filename":"src\/hotspot\/share\/oops\/constantPool.cpp","additions":3,"deletions":5,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -25,1 +25,0 @@\n-#include \"cds\/archiveHeapLoader.hpp\"\n@@ -27,1 +26,1 @@\n-#include \"cds\/heapShared.hpp\"\n+#include \"cds\/heapShared.inline.hpp\"\n@@ -899,1 +898,1 @@\n-    if (ArchiveHeapLoader::is_in_use()) {\n+    if (HeapShared::is_archived_heap_in_use()) {\n","filename":"src\/hotspot\/share\/oops\/klass.cpp","additions":2,"deletions":3,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -39,1 +39,1 @@\n-  friend class ArchiveHeapWriter;\n+  friend class AOTMappedHeapWriter;\n","filename":"src\/hotspot\/share\/oops\/objArrayOop.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -36,1 +36,0 @@\n-  if (!Universe::is_fully_initialized()) return;\n@@ -46,1 +45,0 @@\n-  if (!Universe::is_fully_initialized()) return;\n","filename":"src\/hotspot\/share\/oops\/oopsHierarchy.cpp","additions":0,"deletions":2,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -3494,0 +3494,4 @@\n+bool is_vm_created() {\n+  return AtomicAccess::load(&vm_created) == COMPLETE;\n+}\n+\n","filename":"src\/hotspot\/share\/prims\/jni.cpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -25,0 +25,1 @@\n+#include \"cds\/aotThread.hpp\"\n@@ -1464,0 +1465,7 @@\n+  if (thread->is_aot_thread()) {\n+    \/\/ The AOT thread is hidden from view but has no thread oop when it starts due\n+    \/\/ to bootstrapping complexity, so we check for it before checking for bound\n+    \/\/ virtual threads. When exiting it is filtered out due to being hidden.\n+    return;\n+  }\n+\n","filename":"src\/hotspot\/share\/prims\/jvmtiExport.cpp","additions":8,"deletions":0,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -25,0 +25,1 @@\n+#include \"cds\/aotThread.hpp\"\n@@ -32,0 +33,1 @@\n+#include \"runtime\/threadSMR.hpp\"\n@@ -42,3 +44,0 @@\n-  assert((Threads::number_of_threads()==1),\n-         \"Java thread has not been created yet or more than one java thread \"\n-         \"is running. Raw monitor transition will not work\");\n@@ -46,0 +45,8 @@\n+\n+#ifdef ASSERT\n+  for (JavaThreadIteratorWithHandle jtiwh; JavaThread *thread = jtiwh.next(); ) {\n+    assert(thread == current_java_thread || thread->is_aot_thread(),\n+           \"Didn't expect concurrent application threads at this point\");\n+  }\n+#endif\n+\n","filename":"src\/hotspot\/share\/prims\/jvmtiRawMonitor.cpp","additions":10,"deletions":3,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -26,0 +26,1 @@\n+#include \"cds\/aotMappedHeapLoader.hpp\"\n@@ -27,1 +28,0 @@\n-#include \"cds\/archiveHeapLoader.hpp\"\n@@ -30,1 +30,1 @@\n-#include \"cds\/heapShared.hpp\"\n+#include \"cds\/heapShared.inline.hpp\"\n@@ -2201,0 +2201,3 @@\n+  if (!HeapShared::is_loading_mapping_mode()) {\n+    return false;\n+  }\n@@ -2213,1 +2216,1 @@\n-  return ArchiveHeapLoader::is_mapped();\n+  return AOTMappedHeapLoader::is_mapped();\n@@ -2226,1 +2229,1 @@\n-  return ArchiveHeapLoader::is_mapped();\n+  return AOTMappedHeapLoader::is_mapped();\n@@ -2256,1 +2259,1 @@\n-WB_ENTRY(jboolean, WB_CanWriteJavaHeapArchive(JNIEnv* env))\n+static bool canWriteJavaHeapArchive() {\n@@ -2258,0 +2261,8 @@\n+}\n+\n+WB_ENTRY(jboolean, WB_CanWriteJavaHeapArchive(JNIEnv* env))\n+  return canWriteJavaHeapArchive();\n+WB_END\n+\n+WB_ENTRY(jboolean, WB_CanWriteMappedJavaHeapArchive(JNIEnv* env))\n+  return canWriteJavaHeapArchive() && !AOTStreamableObjects;\n@@ -2260,0 +2271,3 @@\n+WB_ENTRY(jboolean, WB_CanWriteStreamedJavaHeapArchive(JNIEnv* env))\n+  return canWriteJavaHeapArchive() && AOTStreamableObjects;\n+WB_END\n@@ -3035,0 +3049,2 @@\n+  {CC\"canWriteMappedJavaHeapArchive\",     CC\"()Z\",    (void*)&WB_CanWriteMappedJavaHeapArchive },\n+  {CC\"canWriteStreamedJavaHeapArchive\",   CC\"()Z\",    (void*)&WB_CanWriteStreamedJavaHeapArchive },\n","filename":"src\/hotspot\/share\/prims\/whitebox.cpp","additions":21,"deletions":5,"binary":false,"changes":26,"status":"modified"},{"patch":"@@ -124,1 +124,1 @@\n-    {                                                                      \\\n+    if (!javathread->is_aot_thread()) {                                    \\\n@@ -761,1 +761,1 @@\n-  assert(_threadObj.peek() != nullptr, \"just checking\");\n+  assert(_threadObj.peek() != nullptr || is_aot_thread(), \"just checking\");\n","filename":"src\/hotspot\/share\/runtime\/javaThread.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -82,0 +82,1 @@\n+Monitor* AOTHeapLoading_lock          = nullptr;\n@@ -333,1 +334,3 @@\n-  MUTEX_DEFL(JNICritical_lock               , PaddedMonitor, AdapterHandlerLibrary_lock); \/\/ used for JNI critical regions\n+  MUTEX_DEFL(Module_lock                    , PaddedMutex  , AdapterHandlerLibrary_lock);\n+  MUTEX_DEFL(AOTHeapLoading_lock            , PaddedMonitor, Module_lock);\n+  MUTEX_DEFL(JNICritical_lock               , PaddedMonitor, AOTHeapLoading_lock); \/\/ used for JNI critical regions\n@@ -356,1 +359,0 @@\n-  MUTEX_DEFL(Module_lock                    , PaddedMutex  ,  ClassLoaderDataGraph_lock);\n","filename":"src\/hotspot\/share\/runtime\/mutexLocker.cpp","additions":4,"deletions":2,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -84,0 +84,1 @@\n+extern Monitor* AOTHeapLoading_lock;             \/\/ a lock used to guard materialization of AOT heap objects\n","filename":"src\/hotspot\/share\/runtime\/mutexLocker.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -33,1 +33,4 @@\n-NoSafepointVerifier::NoSafepointVerifier() : _thread(Thread::current()) {\n+NoSafepointVerifier::NoSafepointVerifier(bool active) : _thread(Thread::current()), _active(active) {\n+  if (!_active) {\n+    return;\n+  }\n@@ -40,0 +43,3 @@\n+  if (!_active) {\n+    return;\n+  }\n","filename":"src\/hotspot\/share\/runtime\/safepointVerifiers.cpp","additions":7,"deletions":1,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -41,0 +41,1 @@\n+  bool _active;\n@@ -42,1 +43,1 @@\n-  NoSafepointVerifier()  NOT_DEBUG_RETURN;\n+  explicit NoSafepointVerifier(bool active = true)  NOT_DEBUG_RETURN;\n","filename":"src\/hotspot\/share\/runtime\/safepointVerifiers.hpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -316,0 +316,1 @@\n+  virtual bool is_aot_thread() const                 { return false; }\n","filename":"src\/hotspot\/share\/runtime\/thread.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -30,1 +30,1 @@\n-#include \"cds\/heapShared.hpp\"\n+#include \"cds\/heapShared.inline.hpp\"\n@@ -105,0 +105,1 @@\n+#include \"utilities\/debug.hpp\"\n@@ -386,0 +387,2 @@\n+  HeapShared::materialize_thread_object();\n+\n@@ -579,1 +582,3 @@\n-  main_thread->set_monitor_owner_id(ThreadIdentifier::next());\n+  const int64_t main_thread_tid = ThreadIdentifier::next();\n+  guarantee(main_thread_tid == 3, \"Must equal the PRIMORDIAL_TID used in Threads.java\");\n+  main_thread->set_monitor_owner_id(main_thread_tid);\n@@ -615,8 +620,0 @@\n-  \/\/ Add main_thread to threads list to finish barrier setup with\n-  \/\/ on_thread_attach.  Should be before starting to build Java objects in\n-  \/\/ init_globals2, which invokes barriers.\n-  {\n-    MutexLocker mu(Threads_lock);\n-    Threads::add(main_thread);\n-  }\n-\n@@ -706,0 +703,3 @@\n+  \/\/ Prepare AOT heap loader for GC.\n+  HeapShared::enable_gc();\n+\n@@ -901,0 +901,3 @@\n+  \/\/ Finish materializing AOT objects\n+  HeapShared::finish_materialize_objects();\n+\n","filename":"src\/hotspot\/share\/runtime\/threads.cpp","additions":13,"deletions":10,"binary":false,"changes":23,"status":"modified"},{"patch":"@@ -553,0 +553,1 @@\n+extern bool is_vm_created();\n@@ -558,1 +559,1 @@\n-    if (is_init_completed()) {\n+    if (is_vm_created()) {\n","filename":"src\/hotspot\/share\/utilities\/exceptions.cpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -626,1 +626,1 @@\n-#if INCLUDE_CDS && INCLUDE_G1GC && defined(_LP64)\n+#if INCLUDE_CDS && defined(_LP64)\n","filename":"src\/hotspot\/share\/utilities\/macros.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -10,0 +10,1 @@\n+gc\/arguments\/TestVerifyBeforeAndAfterGCFlags.java     0000000 generic-all\n","filename":"test\/hotspot\/jtreg\/ProblemList-AotJdk.txt","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -88,0 +88,2 @@\n+    vm.cds.write.mapped.java.heap \\\n+    vm.cds.write.streamed.java.heap \\\n","filename":"test\/hotspot\/jtreg\/TEST.ROOT","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -27,1 +27,1 @@\n- * @test TestPLABAdaptToMinTLABSizeG1\n+ * @test id=G1\n@@ -38,1 +38,1 @@\n- * @test TestPLABAdaptToMinTLABSizeParallel\n+ * @test id=Parallel\n","filename":"test\/hotspot\/jtreg\/gc\/TestPLABAdaptToMinTLABSize.java","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -42,1 +42,2 @@\n-        doTest(false);\n+        doTest(false, false);\n+        doTest(false, true);\n@@ -46,1 +47,2 @@\n-            doTest(true);\n+            doTest(true, false);\n+            doTest(true, true);\n@@ -50,1 +52,1 @@\n-    public static void doTest(boolean compressed) throws Exception {\n+    public static void doTest(boolean compressed, boolean streamHeap) throws Exception {\n@@ -55,0 +57,6 @@\n+        vmArgs.add(\"-XX:+UnlockDiagnosticVMOptions\");\n+        if (streamHeap) {\n+            vmArgs.add(\"-XX:+AOTStreamableObjects\");\n+        } else {\n+            vmArgs.add(\"-XX:-AOTStreamableObjects\");\n+        }\n","filename":"test\/hotspot\/jtreg\/runtime\/cds\/AOTMapTest.java","additions":11,"deletions":3,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -28,1 +28,1 @@\n- * @requires vm.cds.write.archived.java.heap\n+ * @requires vm.cds.write.mapped.java.heap\n","filename":"test\/hotspot\/jtreg\/runtime\/cds\/SharedStrings.java","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -27,1 +27,1 @@\n- * @requires vm.cds.write.archived.java.heap\n+ * @requires vm.cds.write.mapped.java.heap\n","filename":"test\/hotspot\/jtreg\/runtime\/cds\/SharedStringsDedup.java","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -27,1 +27,1 @@\n- * @requires vm.cds.write.archived.java.heap\n+ * @requires vm.cds.write.mapped.java.heap\n","filename":"test\/hotspot\/jtreg\/runtime\/cds\/SharedStringsRunAuto.java","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -46,2 +46,3 @@\n-        String s = output.firstMatch(\"Average bucket size     : .*\");\n-        Float f = Float.parseFloat(s.substring(25));\n+        String regex = \"Average bucket size     :     ([0-9]+\\\\.[0-9]+).*\";\n+        String s = output.firstMatch(regex, 1);\n+        Float f = Float.parseFloat(s);\n","filename":"test\/hotspot\/jtreg\/runtime\/cds\/SharedSymbolTableBucketSize.java","additions":3,"deletions":2,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -58,0 +58,1 @@\n+ * @requires !vm.gc.Z\n@@ -71,0 +72,1 @@\n+ * @requires !vm.gc.Z\n","filename":"test\/hotspot\/jtreg\/runtime\/cds\/TestDefaultArchiveLoading.java","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -124,18 +124,17 @@\n-        if (!dumpWithParallel && execWithParallel) {\n-            \/\/ We dumped with G1, so we have an archived heap. At exec time, try to load them into\n-            \/\/ a small ParallelGC heap that may be too small.\n-            System.out.println(\"2. Exec with \" + execGC);\n-            out = TestCommon.exec(helloJar,\n-                                    execGC,\n-                                    small1,\n-                                    small2,\n-                                    \"-Xmx4m\",\n-                                    coops,\n-                                    \"-Xlog:cds\",\n-                                    \"Hello\");\n-            if (out.getExitValue() == 0) {\n-                out.shouldContain(HELLO);\n-                out.shouldNotContain(errMsg);\n-            } else {\n-                out.shouldNotHaveFatalError();\n-            }\n+        \/\/ Regardless of which GC dumped the heap, there will be an object archive, either\n+        \/\/ created with mapping if dumped with G1, or streaming if dumped with parallel GC.\n+        \/\/ At exec time, try to load them into a small ParallelGC heap that may be too small.\n+        System.out.println(\"2. Exec with \" + execGC);\n+        out = TestCommon.exec(helloJar,\n+                              execGC,\n+                              small1,\n+                              small2,\n+                              \"-Xmx4m\",\n+                              coops,\n+                              \"-Xlog:cds\",\n+                              \"Hello\");\n+        if (out.getExitValue() == 0) {\n+            out.shouldContain(HELLO);\n+            out.shouldNotContain(errMsg);\n+        } else {\n+            out.shouldNotHaveFatalError();\n","filename":"test\/hotspot\/jtreg\/runtime\/cds\/appcds\/TestParallelGCWithCDS.java","additions":17,"deletions":18,"binary":false,"changes":35,"status":"modified"},{"patch":"@@ -143,3 +143,2 @@\n-        if (dumpWithSerial == false && execWithSerial == true) {\n-            \/\/ We dumped with G1, so we have an archived heap. At exec time, try to load them into\n-            \/\/ a small SerialGC heap that may be too small.\n+        if (execWithSerial == true) {\n+            \/\/ At exec time, try to load archived objects into a small SerialGC heap that may be too small.\n","filename":"test\/hotspot\/jtreg\/runtime\/cds\/appcds\/TestSerialGCWithCDS.java","additions":2,"deletions":3,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -0,0 +1,159 @@\n+\/*\n+ * Copyright (c) 2022, 2025, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+\/*\n+ * @test Loading and writing AOT archived heap objects with ZGC\n+ * @requires vm.cds\n+ * @requires vm.gc.Z\n+ * @requires vm.gc.G1\n+ *\n+ * @comment don't run this test if any -XX::+Use???GC options are specified, since they will\n+ *          interfere with the test.\n+ * @requires vm.gc == null\n+ *\n+ * @library \/test\/lib \/test\/hotspot\/jtreg\/runtime\/cds\/appcds\n+ * @compile test-classes\/Hello.java\n+ * @run driver TestZGCWithAOTHeap\n+ *\/\n+\n+import jdk.test.lib.Platform;\n+import jdk.test.lib.process.OutputAnalyzer;\n+\n+public class TestZGCWithAOTHeap {\n+    public final static String HELLO = \"Hello World\";\n+    static String helloJar;\n+\n+    public static void main(String... args) throws Exception {\n+        helloJar = JarBuilder.build(\"hello\", \"Hello\");\n+\n+        \/\/ Check if we can use ZGC during dump time, or run time, or both.\n+        test(false, true, true, true);\n+        test(true,  false, true, true);\n+        test(true, true, true, true);\n+        test(false, true, false, true);\n+        test(false, true, true, false);\n+        test(true,  false, true, false);\n+        test(true, true, true, false);\n+        test(false, true, false, false);\n+    }\n+\n+    final static String G1 = \"-XX:+UseG1GC\";\n+    final static String Z = \"-XX:+UseZGC\";\n+\n+    static void test(boolean dumpWithZ, boolean execWithZ, boolean shouldStream, boolean shouldUseCOH) throws Exception {\n+        String unlockDiagnostic = \"-XX:+UnlockDiagnosticVMOptions\";\n+        String dumpGC = dumpWithZ ? Z : G1;\n+        String execGC = execWithZ ? Z : G1;\n+        String generalErrMsg = \"Cannot use CDS heap data.\";\n+        String coopsErrMsg = generalErrMsg + \" Selected GC not compatible -XX:-UseCompressedOops\";\n+        String coops = \"-XX:-UseCompressedOops\";\n+        String coh = shouldUseCOH ? \"-XX:+UseCompactObjectHeaders\" : \"-XX:-UseCompactObjectHeaders\";\n+        String stream = shouldStream ? \"-XX:+AOTStreamableObjects\" : \"-XX:-AOTStreamableObjects\";\n+        String eagerLoading = \"-XX:+AOTEagerlyLoadObjects\";\n+        OutputAnalyzer out;\n+\n+        System.out.println(\"0. Dump with \" + dumpGC + \", \" + coops + \", \" + coh + \", \" + stream);\n+        out = TestCommon.dump(helloJar,\n+                              new String[] {\"Hello\"},\n+                              dumpGC,\n+                              coops,\n+                              coh,\n+                              \"-XX:+UnlockDiagnosticVMOptions\",\n+                              stream,\n+                              \"-Xlog:cds,aot,aot+heap\");\n+        out.shouldContain(\"Dumping shared data to file:\");\n+        out.shouldHaveExitValue(0);\n+\n+        System.out.println(\"1. Exec with \" + execGC + \", \" + coops + \", \" + coh);\n+        out = TestCommon.exec(helloJar,\n+                              unlockDiagnostic,\n+                              execGC,\n+                              coops,\n+                              coh,\n+                              \"-Xlog:cds,aot,aot+heap\",\n+                              \"Hello\");\n+        if (!shouldStream && execWithZ) {\n+            \/\/ Only when dumping without streaming and executing with ZGC do we expect there\n+            \/\/ to be a problem. With -XX:+AOTClassLinking, the problem is worse.\n+            if (out.getExitValue() == 0) {\n+                out.shouldContain(HELLO);\n+                out.shouldContain(generalErrMsg);\n+            } else {\n+                out.shouldHaveExitValue(1);\n+            }\n+        } else {\n+            out.shouldContain(HELLO);\n+            out.shouldNotContain(generalErrMsg);\n+            out.shouldHaveExitValue(0);\n+        }\n+\n+        \/\/ Regardless of which GC dumped the heap, there will be an object archive, either\n+        \/\/ created with mapping if dumped with G1, or streaming if dumped with parallel GC.\n+        \/\/ At exec time, try to load them into a small ZGC heap that may be too small.\n+        System.out.println(\"2. Exec with \" + execGC + \", \" + coops + \", \" + coh);\n+        out = TestCommon.exec(helloJar,\n+                              unlockDiagnostic,\n+                              execGC,\n+                              \"-Xmx4m\",\n+                              coops,\n+                              coh,\n+                              \"-Xlog:cds,aot,aot+heap\",\n+                              \"Hello\");\n+        if (out.getExitValue() == 0) {\n+            if (!shouldStream && execWithZ) {\n+                out.shouldContain(coopsErrMsg);\n+            } else {\n+                out.shouldNotContain(generalErrMsg);\n+            }\n+        } else {\n+            out.shouldHaveExitValue(1);\n+        }\n+        out.shouldNotHaveFatalError();\n+\n+        if (shouldStream) {\n+            System.out.println(\"3. Exec with \" + execGC + \", \" + coops + \", \" + coh + \", \" + eagerLoading);\n+            out = TestCommon.exec(helloJar,\n+                                  unlockDiagnostic,\n+                                  execGC,\n+                                  coops,\n+                                  coh,\n+                                  eagerLoading,\n+                                  \"-Xlog:cds,aot,aot+heap\",\n+                                  \"Hello\");\n+            if (!shouldStream && execWithZ) {\n+                \/\/ Only when dumping without streaming and executing with ZGC do we expect there\n+                \/\/ to be a problem. With -XX:+AOTClassLinking, the problem is worse.\n+                if (out.getExitValue() == 0) {\n+                    out.shouldContain(HELLO);\n+                    out.shouldContain(generalErrMsg);\n+                } else {\n+                    out.shouldHaveExitValue(1);\n+                }\n+            } else {\n+                out.shouldContain(HELLO);\n+                out.shouldNotContain(generalErrMsg);\n+                out.shouldHaveExitValue(0);\n+            }\n+        }\n+    }\n+}\n","filename":"test\/hotspot\/jtreg\/runtime\/cds\/appcds\/TestZGCWithAOTHeap.java","additions":159,"deletions":0,"binary":false,"changes":159,"status":"added"},{"patch":"@@ -1,59 +0,0 @@\n-\/*\n- * Copyright (c) 2025, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\n- *\/\n-\n-\/**\n- * @test\n- * @summary -XX:AOTMode=create should be compatible with ZGC\n- * @bug 8352775\n- * @requires vm.cds\n- * @requires vm.gc.Z\n- * @library \/test\/lib\n- * @build AOTCacheWithZGC\n- * @run driver jdk.test.lib.helpers.ClassFileInstaller -jar app.jar AOTCacheWithZGCApp\n- * @run driver AOTCacheWithZGC\n- *\/\n-\n-import jdk.test.lib.cds.SimpleCDSAppTester;\n-import jdk.test.lib.process.OutputAnalyzer;\n-\n-public class AOTCacheWithZGC {\n-    public static void main(String... args) throws Exception {\n-        SimpleCDSAppTester.of(\"AOTCacheWithZGC\")\n-            .addVmArgs(\"-XX:+UseZGC\", \"-Xlog:cds\", \"-Xlog:aot\")\n-            .classpath(\"app.jar\")\n-            .appCommandLine(\"AOTCacheWithZGCApp\")\n-            .setProductionChecker((OutputAnalyzer out) -> {\n-                    \/\/ AOT-linked classes required cached Java heap objects, which is not\n-                    \/\/ yet supported by ZGC.\n-                    out.shouldContain(\"Using AOT-linked classes: false\");\n-                })\n-            .runAOTWorkflow();\n-    }\n-}\n-\n-class AOTCacheWithZGCApp {\n-    public static void main(String[] args) {\n-\n-    }\n-}\n","filename":"test\/hotspot\/jtreg\/runtime\/cds\/appcds\/aotClassLinking\/AOTCacheWithZGC.java","additions":0,"deletions":59,"binary":false,"changes":59,"status":"deleted"},{"patch":"@@ -30,0 +30,2 @@\n+ * @requires vm.bits == 64\n+ * @requires vm.opt.final.UseCompressedOops\n","filename":"test\/hotspot\/jtreg\/runtime\/cds\/appcds\/aotCode\/AOTCodeCompressedOopsTest.java","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -31,0 +31,2 @@\n+ * @build jdk.test.whitebox.WhiteBox\n+ * @run driver jdk.test.lib.helpers.ClassFileInstaller -jar WhiteBox.jar jdk.test.whitebox.WhiteBox\n@@ -33,1 +35,1 @@\n- * @run driver ArchivedIntegerCacheTest\n+ * @run main\/othervm -XX:+UnlockDiagnosticVMOptions -XX:+WhiteBoxAPI -Xbootclasspath\/a:.\/WhiteBox.jar ArchivedIntegerCacheTest\n@@ -42,0 +44,1 @@\n+import jdk.test.whitebox.WhiteBox;\n@@ -44,0 +47,1 @@\n+    private static WhiteBox WB = WhiteBox.getWhiteBox();\n@@ -136,2 +140,4 @@\n-        TestCommon.checkDump(output,\n-            \"Cannot archive the sub-graph referenced from [Ljava.lang.Integer; object\");\n+        if (WB.canWriteMappedJavaHeapArchive()) {\n+            \/\/ The mapping AOT heap archiving mechanism is unable to cache larger objects.\n+            TestCommon.checkDump(output, \"Cannot archive the sub-graph referenced from [Ljava.lang.Integer; object\");\n+        }\n","filename":"test\/hotspot\/jtreg\/runtime\/cds\/appcds\/cacheObject\/ArchivedIntegerCacheTest.java","additions":9,"deletions":3,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -40,1 +40,1 @@\n- * @run driver PrintSharedArchiveAndExit\n+ * @run main\/othervm -XX:+UnlockDiagnosticVMOptions -XX:+WhiteBoxAPI -Xbootclasspath\/a:.\/WhiteBox.jar PrintSharedArchiveAndExit\n@@ -48,0 +48,2 @@\n+    private static WhiteBox WB = WhiteBox.getWhiteBox();\n+\n@@ -85,1 +87,0 @@\n-              .shouldMatch(\"Number of shared strings: \\\\d+\")\n@@ -87,0 +88,5 @@\n+\n+        if (WB.canWriteMappedJavaHeapArchive()) {\n+            \/\/ With the mapping object dumper, the string table is dumped.\n+            output.shouldMatch(\"Number of shared strings: \\\\d+\");\n+        }\n","filename":"test\/hotspot\/jtreg\/runtime\/cds\/appcds\/customLoader\/PrintSharedArchiveAndExit.java","additions":8,"deletions":2,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -46,0 +46,2 @@\n+import jdk.test.whitebox.WhiteBox;\n+\n@@ -48,0 +50,1 @@\n+    private static final WhiteBox WB = WhiteBox.getWhiteBox();\n@@ -95,1 +98,0 @@\n-                      .shouldMatch(\"Number of shared strings: \\\\d+\")\n@@ -97,1 +99,5 @@\n-                });\n+                if (WB.canWriteMappedJavaHeapArchive()) {\n+                      \/\/ With the mapping object archiving mechanism, the string table is dumped\n+                      output.shouldMatch(\"Number of shared strings: \\\\d+\");\n+                }\n+            });\n","filename":"test\/hotspot\/jtreg\/runtime\/cds\/appcds\/dynamicArchive\/PrintSharedArchiveAndExit.java","additions":8,"deletions":2,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -28,2 +28,1 @@\n- * @requires vm.cds.write.archived.java.heap\n- * @requires vm.gc == null\n+ * @requires vm.cds.write.mapped.java.heap\n","filename":"test\/hotspot\/jtreg\/runtime\/cds\/appcds\/sharedStrings\/ExerciseGC.java","additions":1,"deletions":2,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -28,2 +28,1 @@\n- * @requires vm.cds.write.archived.java.heap & vm.hasJFR\n- * @requires vm.gc == null\n+ * @requires vm.cds.write.mapped.java.heap & vm.hasJFR\n@@ -39,2 +38,1 @@\n- * @requires vm.cds.write.archived.java.heap & !vm.hasJFR\n- * @requires vm.gc == null\n+ * @requires vm.cds.write.mapped.java.heap & !vm.hasJFR\n@@ -56,1 +54,1 @@\n-        SharedStringsUtils.runWithArchive(\"HelloString\", \"-XX:+UseG1GC\");\n+        SharedStringsUtils.runWithArchive(\"HelloString\");\n","filename":"test\/hotspot\/jtreg\/runtime\/cds\/appcds\/sharedStrings\/FlagCombo.java","additions":3,"deletions":5,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -103,0 +103,3 @@\n+        testExec(1, \"-XX:+UseG1GC\", \"-XX:-UseCompressedOops\", null, false);\n+\n+        \/\/ Try with ZGC\n@@ -104,1 +107,2 @@\n-            testDump(1, \"-XX:+UseZGC\", \"-XX:-UseCompressedOops\", null, false);\n+            testDump(2, \"-XX:+UseZGC\", \"-XX:-UseCompressedOops\", null, false);\n+            testExec(2, \"-XX:+UseZGC\", \"-XX:-UseCompressedOops\", null, false);\n@@ -115,3 +119,2 @@\n-        testDump(5, \"-XX:+UseG1GC\", \"-XX:+UseCompressedOops\", null, false);\n-        testExec(5, \"-XX:+UseG1GC\", \"-XX:-UseCompressedOops\",\n-                 COMPRESSED_OOPS_NOT_CONSISTENT, true);\n+        testDump(3, \"-XX:+UseG1GC\", \"-XX:+UseCompressedOops\", null, false);\n+        testExec(3, \"-XX:+UseG1GC\", \"-XX:-UseCompressedOops\", COMPRESSED_OOPS_NOT_CONSISTENT, true);\n@@ -121,2 +124,26 @@\n-        testExec(6, \"-XX:+UseParallelGC\", \"\", \"\", false);\n-        testExec(7, \"-XX:+UseSerialGC\", \"\", \"\", false);\n+        testExec(3, \"-XX:+UseParallelGC\", \"\", \"\", false);\n+        testExec(3, \"-XX:+UseSerialGC\", \"\", \"\", false);\n+\n+        \/\/ Explicitly archive with object streaming with one GC, run with other GCs.\n+        testDump(4, \"-XX:+UseG1GC\", \"-XX:+AOTStreamableObjects\", null, false);\n+        testExec(4, \"-XX:+UseParallelGC\", \"\", \"\", false);\n+        testExec(4, \"-XX:+UseSerialGC\", \"\", \"\", false);\n+\n+        if (GC.Z.isSupported()) {\n+            testExec(4, \"-XX:+UseZGC\", \"\", COMPRESSED_OOPS_NOT_CONSISTENT, true);\n+        }\n+\n+        \/\/ Explicitly archive with object streaming and COOPs with one GC, run with other GCs.\n+        testDump(4, \"-XX:-UseCompressedOops\", \"-XX:+AOTStreamableObjects\", null, false);\n+        testExec(4, \"-XX:+UseG1GC\", \"\", COMPRESSED_OOPS_NOT_CONSISTENT, true);\n+        testExec(4, \"-XX:+UseParallelGC\", \"\", COMPRESSED_OOPS_NOT_CONSISTENT, true);\n+        testExec(4, \"-XX:+UseSerialGC\", \"\", COMPRESSED_OOPS_NOT_CONSISTENT, true);\n+\n+        testExec(4, \"-XX:+UseParallelGC\", \"-XX:-UseCompressedOops\", \"\", false);\n+        testExec(4, \"-XX:+UseSerialGC\", \"-XX:-UseCompressedOops\", \"\", false);\n+        testExec(4, \"-XX:+UseG1GC\", \"-XX:-UseCompressedOops\", \"\", false);\n+\n+        if (GC.Z.isSupported()) {\n+            testExec(4, \"-XX:+UseZGC\", \"-XX:+ZGenerational\", \"\", false);\n+            testExec(4, \"-XX:+UseZGC\", \"-XX:-ZGenerational\", \"\", false);\n+        }\n@@ -125,3 +152,5 @@\n-        testDump(9, \"-XX:+UseG1GC\", \"-XX:ObjectAlignmentInBytes=8\", null, false);\n-        testExec(9, \"-XX:+UseG1GC\", \"-XX:ObjectAlignmentInBytes=16\",\n-                 OBJ_ALIGNMENT_MISMATCH, true);\n+        testDump(5, \"-XX:+UseG1GC\", \"-XX:ObjectAlignmentInBytes=8\", null, false);\n+        testExec(5, \"-XX:+UseG1GC\", \"-XX:ObjectAlignmentInBytes=16\", OBJ_ALIGNMENT_MISMATCH, true);\n+\n+        testDump(6, \"-XX:+AOTStreamableObjects\", \"-XX:ObjectAlignmentInBytes=8\", null, false);\n+        testExec(6, \"-XX:+AOTStreamableObjects\", \"-XX:ObjectAlignmentInBytes=16\", OBJ_ALIGNMENT_MISMATCH, true);\n@@ -134,2 +163,2 @@\n-        testDump(10, \"-XX:+UseG1GC\", \"-Xmx1g\", null, false);\n-        testExec(10, \"-XX:+UseG1GC\", \"-Xmx32g\", null, true);\n+        testDump(7, \"-XX:+UseG1GC\", \"-Xmx1g\", null, false);\n+        testExec(7, \"-XX:+UseG1GC\", \"-Xmx32g\", null, true);\n@@ -137,2 +166,2 @@\n-        testDump(11, \"-XX:+UseG1GC\", \"-XX:-UseCompressedOops\", null, false);\n-        testExec(11, \"-XX:+UseG1GC\", \"-XX:+UseCompressedOops\", null, true);\n+        testDump(8, \"-XX:+UseG1GC\", \"-XX:-UseCompressedOops\", null, false);\n+        testExec(8, \"-XX:+UseG1GC\", \"-XX:+UseCompressedOops\", null, true);\n@@ -140,2 +169,2 @@\n-        testDump(12, \"-XX:+UseG1GC\", \"-Xmx32G\", null, false);\n-        testExec(12, \"-XX:+UseG1GC\", \"-Xmx1G\", null, true);\n+        testDump(9, \"-XX:+UseG1GC\", \"-Xmx32G\", null, false);\n+        testExec(9, \"-XX:+UseG1GC\", \"-Xmx1G\", null, true);\n@@ -143,2 +172,5 @@\n-        testDump(13, \"-XX:+UseG1GC\", \"-XX:-CompactStrings\", null, false);\n-        testExec(13, \"-XX:+UseG1GC\", \"-XX:+CompactStrings\",\n+        testDump(10, \"-XX:+UseG1GC\", \"-XX:-CompactStrings\", null, false);\n+        testExec(10, \"-XX:+UseG1GC\", \"-XX:+CompactStrings\",\n+                 COMPACT_STRING_MISMATCH, true);\n+        testDump(11, \"-XX:+UseG1GC\", \"-XX:+CompactStrings\", null, false);\n+        testExec(11, \"-XX:+UseG1GC\", \"-XX:-CompactStrings\",\n@@ -146,2 +178,2 @@\n-        testDump(14, \"-XX:+UseG1GC\", \"-XX:+CompactStrings\", null, false);\n-        testExec(14, \"-XX:+UseG1GC\", \"-XX:-CompactStrings\",\n+        testDump(11, \"-XX:+AOTStreamableObjects\", \"-XX:+CompactStrings\", null, false);\n+        testExec(11, \"-XX:+AOTStreamableObjects\", \"-XX:-CompactStrings\",\n@@ -157,0 +189,1 @@\n+                \"-XX:+UnlockDiagnosticVMOptions\",\n@@ -184,0 +217,1 @@\n+                    \"-XX:+UnlockDiagnosticVMOptions\",\n@@ -189,0 +223,1 @@\n+                    \"-XX:+UnlockDiagnosticVMOptions\",\n","filename":"test\/hotspot\/jtreg\/runtime\/cds\/appcds\/sharedStrings\/IncompatibleOptions.java","additions":54,"deletions":19,"binary":false,"changes":73,"status":"modified"},{"patch":"@@ -28,1 +28,0 @@\n- * @requires vm.cds.write.archived.java.heap\n@@ -30,0 +29,1 @@\n+ * @requires vm.cds.write.mapped.java.heap\n@@ -37,0 +37,4 @@\n+\/\/ This test requires the vm.cds.write.mapped.java.heap specifically as it has expectations\n+\/\/ about using the mechanism for dumping the entire string table, which the streaming solution\n+\/\/ does not do.\n+\n","filename":"test\/hotspot\/jtreg\/runtime\/cds\/appcds\/sharedStrings\/InternSharedString.java","additions":5,"deletions":1,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -28,2 +28,1 @@\n- * @requires vm.cds.write.archived.java.heap\n- * @requires vm.gc == null\n+ * @requires vm.cds.write.mapped.java.heap\n","filename":"test\/hotspot\/jtreg\/runtime\/cds\/appcds\/sharedStrings\/LargePages.java","additions":1,"deletions":2,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -28,1 +28,1 @@\n- * @requires vm.cds.write.archived.java.heap\n+ * @requires vm.cds.write.mapped.java.heap\n@@ -36,0 +36,4 @@\n+\/\/ This test requires the vm.cds.write.mapped.java.heap specifically as it has expectations\n+\/\/ about using the mechanism for dumping the entire string table, which the streaming solution\n+\/\/ does not do.\n+\n","filename":"test\/hotspot\/jtreg\/runtime\/cds\/appcds\/sharedStrings\/SharedStringsBasic.java","additions":5,"deletions":1,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -28,2 +28,1 @@\n- * @requires vm.cds.write.archived.java.heap\n- * @requires vm.gc == null\n+ * @requires vm.cds.write.mapped.java.heap\n@@ -36,0 +35,4 @@\n+\/\/ This test requires the vm.cds.write.mapped.java.heap specifically as it has expectations\n+\/\/ about using the mechanism for dumping the entire string table, which the streaming solution\n+\/\/ does not do.\n+\n","filename":"test\/hotspot\/jtreg\/runtime\/cds\/appcds\/sharedStrings\/SharedStringsBasicPlus.java","additions":5,"deletions":2,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -29,1 +29,1 @@\n- * @requires vm.cds.write.archived.java.heap\n+ * @requires vm.cds.write.mapped.java.heap\n@@ -38,0 +38,5 @@\n+\n+\/\/ The problem with humongous strings, or humongous objects in general, does not\n+\/\/ exist with the streaming heap loader. Therefore, this test requres the mapping mode.\n+\/\/ Further more, humongous regions are a bit specific to G1, so G1 is needed.\n+\n","filename":"test\/hotspot\/jtreg\/runtime\/cds\/appcds\/sharedStrings\/SharedStringsHumongous.java","additions":6,"deletions":1,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -28,1 +28,1 @@\n- * @requires vm.cds.write.archived.java.heap\n+ * @requires vm.cds.write.mapped.java.heap\n@@ -33,0 +33,5 @@\n+\n+\/\/ This test requires the vm.cds.write.mapped.java.heap specifically as it has expectations\n+\/\/ about using the mechanism for dumping the entire string table, which the streaming solution\n+\/\/ does not do.\n+\n","filename":"test\/hotspot\/jtreg\/runtime\/cds\/appcds\/sharedStrings\/SharedStringsStress.java","additions":6,"deletions":1,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -99,1 +99,1 @@\n-            TestCommon.concat(extraOptions, \"-XX:+UseCompressedOops\",\n+            TestCommon.concat(extraOptions,\n@@ -127,1 +127,1 @@\n-            \"-cp\", appJar, \"-XX:+UseCompressedOops\", className);\n+            \"-cp\", appJar, className);\n@@ -145,2 +145,1 @@\n-        String[] args = TestCommon.concat(extraOptions,\n-            \"-XX:+UseCompressedOops\", className);\n+        String[] args = TestCommon.concat(extraOptions, className);\n","filename":"test\/hotspot\/jtreg\/runtime\/cds\/appcds\/sharedStrings\/SharedStringsUtils.java","additions":3,"deletions":4,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -28,2 +28,1 @@\n- * @requires vm.cds.write.archived.java.heap\n- * @requires vm.gc == null\n+ * @requires vm.cds.write.mapped.java.heap\n@@ -36,0 +35,4 @@\n+\/\/ This test requires the vm.cds.write.mapped.java.heap specifically as it has expectations\n+\/\/ about using the mechanism for dumping the entire string table, which the streaming solution\n+\/\/ does not do.\n+\n","filename":"test\/hotspot\/jtreg\/runtime\/cds\/appcds\/sharedStrings\/SharedStringsWbTest.java","additions":5,"deletions":2,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -37,1 +37,1 @@\n- * @run main\/othervm\/timeout=2400 -Xmx1g ClhsdbPrintAll\n+ * @run main\/othervm\/timeout=2400 -Xmx2g ClhsdbPrintAll\n","filename":"test\/hotspot\/jtreg\/serviceability\/sa\/ClhsdbPrintAll.java","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -103,0 +103,2 @@\n+    vm.cds.write.mapped.java.heap \\\n+    vm.cds.write.streamed.java.heap \\\n","filename":"test\/jdk\/TEST.ROOT","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -129,0 +129,2 @@\n+        map.put(\"vm.cds.write.mapped.java.heap\", this::vmCDSCanWriteMappedArchivedJavaHeap);\n+        map.put(\"vm.cds.write.streamed.java.heap\", this::vmCDSCanWriteStreamedArchivedJavaHeap);\n@@ -488,1 +490,1 @@\n-     *         if -XX:-UseCompressedClassPointers is specified,\n+     *         if -XX:-UseCompressedClassPointers is specified.\n@@ -491,2 +493,19 @@\n-        return \"\" + (\"true\".equals(vmCDS()) && WB.canWriteJavaHeapArchive()\n-                     && isCDSRuntimeOptionsCompatible());\n+        return \"\" + (\"true\".equals(vmCDS()) && WB.canWriteJavaHeapArchive());\n+    }\n+\n+    \/**\n+     * @return true if it's possible for \"java -Xshare:dump\" to write Java heap objects\n+     *         with the current set of jtreg VM options. For example, false will be returned\n+     *         if -XX:-UseCompressedClassPointers is specified.\n+     *\/\n+    protected String vmCDSCanWriteMappedArchivedJavaHeap() {\n+        return \"\" + (\"true\".equals(vmCDS()) && WB.canWriteMappedJavaHeapArchive());\n+    }\n+\n+    \/**\n+     * @return true if it's possible for \"java -Xshare:dump\" to write Java heap objects\n+     *         with the current set of jtreg VM options. For example, false will be returned\n+     *         if -XX:-UseCompressedClassPointers is specified.\n+     *\/\n+    protected String vmCDSCanWriteStreamedArchivedJavaHeap() {\n+        return \"\" + (\"true\".equals(vmCDS()) && WB.canWriteStreamedJavaHeapArchive());\n@@ -517,25 +536,0 @@\n-    \/**\n-     * @return true if the VM options specified via the \"test.cds.runtime.options\"\n-     * property is compatible with writing Java heap objects into the CDS archive\n-     *\/\n-    protected boolean isCDSRuntimeOptionsCompatible() {\n-        String jtropts = System.getProperty(\"test.cds.runtime.options\");\n-        if (jtropts == null) {\n-            return true;\n-        }\n-        String CCP_DISABLED = \"-XX:-UseCompressedClassPointers\";\n-        String G1GC_ENABLED = \"-XX:+UseG1GC\";\n-        String PARALLELGC_ENABLED = \"-XX:+UseParallelGC\";\n-        String SERIALGC_ENABLED = \"-XX:+UseSerialGC\";\n-        for (String opt : jtropts.split(\",\")) {\n-            if (opt.equals(CCP_DISABLED)) {\n-                return false;\n-            }\n-            if (opt.startsWith(GC_PREFIX) && opt.endsWith(GC_SUFFIX) &&\n-                !opt.equals(G1GC_ENABLED) && !opt.equals(PARALLELGC_ENABLED) && !opt.equals(SERIALGC_ENABLED)) {\n-                return false;\n-            }\n-        }\n-        return true;\n-    }\n-\n","filename":"test\/jtreg-ext\/requires\/VMProps.java","additions":22,"deletions":28,"binary":false,"changes":50,"status":"modified"},{"patch":"@@ -800,0 +800,2 @@\n+  public native boolean canWriteMappedJavaHeapArchive();\n+  public native boolean canWriteStreamedJavaHeapArchive();\n","filename":"test\/lib\/jdk\/test\/whitebox\/WhiteBox.java","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"}]}