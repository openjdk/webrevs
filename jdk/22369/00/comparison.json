{"files":[{"patch":"@@ -402,0 +402,135 @@\n+ArchiveWorkers::ArchiveWorkers() :\n+        _end_semaphore(0),\n+        _num_workers(max_workers()),\n+        _started_workers(0),\n+        _running_workers(0),\n+        _state(UNUSED),\n+        _task(nullptr) {}\n+\n+ArchiveWorkers::~ArchiveWorkers() {\n+  assert(Atomic::load(&_state) != WORKING, \"Should not be working\");\n+}\n+\n+int ArchiveWorkers::max_workers() {\n+  \/\/ The pool is used for short-lived bursty tasks. We do not want to spend\n+  \/\/ too much time creating and waking up threads unnecessarily. Plus, we do\n+  \/\/ not want to overwhelm large machines. This is why we want to be very\n+  \/\/ conservative about the number of workers actually needed.\n+  return MAX2(0, log2i_graceful(os::active_processor_count()));\n+}\n+\n+bool ArchiveWorkers::is_parallel() {\n+  return _num_workers > 0;\n+}\n+\n+void ArchiveWorkers::start_worker_if_needed() {\n+  while (true) {\n+    int cur = Atomic::load(&_started_workers);\n+    if (cur >= _num_workers) {\n+      return;\n+    }\n+    if (Atomic::cmpxchg(&_started_workers, cur, cur + 1, memory_order_relaxed) == cur) {\n+      new ArchiveWorkerThread(this);\n+      return;\n+    }\n+  }\n+}\n+\n+void ArchiveWorkers::run_task(ArchiveWorkerTask* task) {\n+  assert(Atomic::load(&_state) == UNUSED, \"Should be unused yet\");\n+  assert(Atomic::load(&_task) == nullptr, \"Should not have running tasks\");\n+  Atomic::store(&_state, WORKING);\n+\n+  if (is_parallel()) {\n+    run_task_multi(task);\n+  } else {\n+    run_task_single(task);\n+  }\n+\n+  assert(Atomic::load(&_state) == WORKING, \"Should be working\");\n+  Atomic::store(&_state, SHUTDOWN);\n+}\n+\n+void ArchiveWorkers::run_task_single(ArchiveWorkerTask* task) {\n+  \/\/ Single thread needs no chunking.\n+  task->configure_max_chunks(1);\n+\n+  \/\/ Execute the task ourselves, as there are no workers.\n+  task->work(0, 1);\n+}\n+\n+void ArchiveWorkers::run_task_multi(ArchiveWorkerTask* task) {\n+  \/\/ Multiple threads can work with multiple chunks.\n+  task->configure_max_chunks(_num_workers * CHUNKS_PER_WORKER);\n+\n+  \/\/ Set up the run and publish the task.\n+  Atomic::store(&_running_workers, _num_workers);\n+  Atomic::release_store(&_task, task);\n+\n+  \/\/ Kick off pool startup by starting a single worker, and proceed\n+  \/\/ immediately to executing the task locally.\n+  start_worker_if_needed();\n+\n+  \/\/ Execute the task ourselves, while workers are catching up.\n+  \/\/ This allows us to hide parts of task handoff latency.\n+  task->run();\n+\n+  \/\/ Done executing task locally, wait for any remaining workers to complete,\n+  \/\/ and then do the final housekeeping.\n+  _end_semaphore.wait();\n+  OrderAccess::fence();\n+\n+  assert(Atomic::load(&_running_workers) == 0, \"No workers are running\");\n+}\n+\n+void ArchiveWorkers::run_as_worker() {\n+  assert(is_parallel(), \"Should be in parallel mode\");\n+\n+  ArchiveWorkerTask* task = Atomic::load_acquire(&_task);\n+  task->run();\n+\n+  \/\/ All work done in threads should be visible to caller.\n+  OrderAccess::fence();\n+\n+  \/\/ Signal the pool the tasks are complete, if this was the last running worker.\n+  if (Atomic::sub(&_running_workers, 1, memory_order_relaxed) == 0) {\n+    _end_semaphore.signal();\n+  }\n+}\n+\n+void ArchiveWorkerTask::run() {\n+  while (true) {\n+    int chunk = Atomic::load(&_chunk);\n+    if (chunk >= _max_chunks) {\n+      return;\n+    }\n+    if (Atomic::cmpxchg(&_chunk, chunk, chunk + 1, memory_order_relaxed) == chunk) {\n+      assert(0 <= chunk && chunk < _max_chunks, \"Sanity\");\n+      work(chunk, _max_chunks);\n+    }\n+  }\n+}\n+\n+void ArchiveWorkerTask::configure_max_chunks(int max_chunks) {\n+  if (_max_chunks == 0) {\n+    _max_chunks = max_chunks;\n+  }\n+}\n+\n+ArchiveWorkerThread::ArchiveWorkerThread(ArchiveWorkers* pool) : NamedThread(), _pool(pool) {\n+  set_name(\"ArchiveWorkerThread\");\n+  os::create_thread(this, os::os_thread);\n+  os::start_thread(this);\n+}\n+\n+void ArchiveWorkerThread::run() {\n+  \/\/ Avalanche startup: each worker starts two others.\n+  _pool->start_worker_if_needed();\n+  _pool->start_worker_if_needed();\n+\n+  \/\/ Set ourselves up.\n+  os::set_priority(this, NearMaxPriority);\n+\n+  \/\/ Work.\n+  _pool->run_as_worker();\n+}\n","filename":"src\/hotspot\/share\/cds\/archiveUtils.cpp","additions":135,"deletions":0,"binary":false,"changes":135,"status":"modified"},{"patch":"@@ -36,0 +36,2 @@\n+#include \"runtime\/nonJavaThread.hpp\"\n+#include \"runtime\/semaphore.hpp\"\n@@ -322,0 +324,68 @@\n+class ArchiveWorkers;\n+\n+\/\/ A task to be worked on by worker threads\n+class ArchiveWorkerTask : public CHeapObj<mtInternal> {\n+  friend class ArchiveWorkers;\n+private:\n+  const char* _name;\n+  int _max_chunks;\n+  volatile int _chunk;\n+\n+  void run();\n+\n+  void configure_max_chunks(int max_chunks);\n+\n+public:\n+  ArchiveWorkerTask(const char* name) :\n+      _name(name), _max_chunks(0), _chunk(0) {}\n+  const char* name() const { return _name; }\n+  virtual void work(int chunk, int max_chunks) = 0;\n+};\n+\n+class ArchiveWorkerThread : public NamedThread {\n+  friend class ArchiveWorkers;\n+private:\n+  ArchiveWorkers* const _pool;\n+\n+public:\n+  ArchiveWorkerThread(ArchiveWorkers* pool);\n+  const char* type_name() const override { return \"Archive Worker Thread\"; }\n+  void run() override;\n+};\n+\n+\/\/ Special archive workers. The goal for this implementation is to startup fast,\n+\/\/ distribute spiky workloads efficiently, and shutdown immediately after use.\n+\/\/ This makes the implementation quite different from the normal GC worker pool.\n+class ArchiveWorkers : public StackObj {\n+  friend class ArchiveWorkerThread;\n+private:\n+  \/\/ Target number of chunks per worker. This should be large enough to even\n+  \/\/ out work imbalance, and small enough to keep bookkeeping overheads low.\n+  static constexpr int CHUNKS_PER_WORKER = 4;\n+  static int max_workers();\n+\n+  Semaphore _end_semaphore;\n+\n+  int _num_workers;\n+  int _started_workers;\n+  int _running_workers;\n+\n+  typedef enum { UNUSED, WORKING, SHUTDOWN } State;\n+  volatile State _state;\n+\n+  ArchiveWorkerTask* _task;\n+\n+  void run_as_worker();\n+  void start_worker_if_needed();\n+\n+  void run_task_single(ArchiveWorkerTask* task);\n+  void run_task_multi(ArchiveWorkerTask* task);\n+\n+  bool is_parallel();\n+\n+public:\n+  ArchiveWorkers();\n+  ~ArchiveWorkers();\n+  void run_task(ArchiveWorkerTask* task);\n+};\n+\n","filename":"src\/hotspot\/share\/cds\/archiveUtils.hpp","additions":70,"deletions":0,"binary":false,"changes":70,"status":"modified"},{"patch":"@@ -120,1 +120,4 @@\n-\n+                                                                            \\\n+  product(bool, AOTCacheParallelRelocation, true, DIAGNOSTIC,               \\\n+          \"Use parallel relocation code to speed up startup.\")              \\\n+                                                                            \\\n","filename":"src\/hotspot\/share\/cds\/cds_globals.hpp","additions":4,"deletions":1,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -1975,0 +1975,26 @@\n+class SharedDataRelocationTask : public ArchiveWorkerTask {\n+private:\n+  BitMapView* const _rw_bm;\n+  BitMapView* const _ro_bm;\n+  SharedDataRelocator* const _rw_reloc;\n+  SharedDataRelocator* const _ro_reloc;\n+\n+public:\n+  SharedDataRelocationTask(BitMapView* rw_bm, BitMapView* ro_bm, SharedDataRelocator* rw_reloc, SharedDataRelocator* ro_reloc) :\n+                           ArchiveWorkerTask(\"Shared Data Relocation\"),\n+                           _rw_bm(rw_bm), _ro_bm(ro_bm), _rw_reloc(rw_reloc), _ro_reloc(ro_reloc) {}\n+\n+  void work(int chunk, int max_chunks) override {\n+    work_on(chunk, max_chunks, _rw_bm, _rw_reloc);\n+    work_on(chunk, max_chunks, _ro_bm, _ro_reloc);\n+  }\n+\n+  void work_on(int chunk, int max_chunks, BitMapView* bm, SharedDataRelocator* reloc) {\n+    BitMap::idx_t size  = bm->size();\n+    BitMap::idx_t start = MIN2(size, size * chunk \/ max_chunks);\n+    BitMap::idx_t end   = MIN2(size, size * (chunk + 1) \/ max_chunks);\n+    assert(end > start, \"Sanity: no empty slices\");\n+    bm->iterate(reloc, start, end);\n+  }\n+};\n+\n@@ -2013,2 +2039,9 @@\n-    rw_ptrmap.iterate(&rw_patcher);\n-    ro_ptrmap.iterate(&ro_patcher);\n+\n+    if (AOTCacheParallelRelocation) {\n+      ArchiveWorkers workers;\n+      SharedDataRelocationTask task(&rw_ptrmap, &ro_ptrmap, &rw_patcher, &ro_patcher);\n+      workers.run_task(&task);\n+    } else {\n+      rw_ptrmap.iterate(&rw_patcher);\n+      ro_ptrmap.iterate(&ro_patcher);\n+    }\n","filename":"src\/hotspot\/share\/cds\/filemap.cpp","additions":35,"deletions":2,"binary":false,"changes":37,"status":"modified"},{"patch":"@@ -0,0 +1,69 @@\n+\/*\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"cds\/archiveUtils.hpp\"\n+#include \"unittest.hpp\"\n+\n+class TestArchiveWorkerTask : public ArchiveWorkerTask {\n+private:\n+  volatile int _sum;\n+  int _max;\n+public:\n+  TestArchiveWorkerTask() : ArchiveWorkerTask(\"Test\"), _sum(0), _max(0) {}\n+  void work(int chunk, int max_chunks) override {\n+    Atomic::add(&_sum, chunk);\n+    Atomic::store(&_max, max_chunks);\n+  }\n+  int sum() { return Atomic::load(&_sum); }\n+  int max() { return Atomic::load(&_max); }\n+};\n+\n+\/\/ Test a repeated cycle of pool start works.\n+TEST_VM(ArchiveWorkersTest, continuous_restart) {\n+  for (int c = 0; c < 1000; c++) {\n+    ArchiveWorkers workers;\n+  }\n+}\n+\n+\/\/ Test a repeated cycle of sample task works.\n+TEST_VM(ArchiveWorkersTest, single_task) {\n+  for (int c = 0; c < 1000; c++) {\n+    TestArchiveWorkerTask task;\n+    ArchiveWorkers workers;\n+    workers.run_task(&task);\n+    ASSERT_EQ(task.max() * (task.max() - 1) \/ 2, task.sum());\n+  }\n+}\n+\n+\/\/ Test that reusing the workers fails.\n+#ifdef ASSERT\n+TEST_VM_ASSERT_MSG(ArchiveWorkersTest, multiple_tasks, \".* Should be unused yet\") {\n+  TestArchiveWorkerTask task;\n+  ArchiveWorkers workers;\n+  workers.run_task(&task);\n+  workers.run_task(&task);\n+}\n+#endif \/\/ ASSERT\n+\n","filename":"test\/hotspot\/gtest\/cds\/test_archiveWorkers.cpp","additions":69,"deletions":0,"binary":false,"changes":69,"status":"added"}]}