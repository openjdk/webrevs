{"files":[{"patch":"@@ -3665,2 +3665,141 @@\n-#undef __\n-#define __ this->\n+  address generate_cont_thaw(Continuation::thaw_kind kind) {\n+    bool return_barrier = Continuation::is_thaw_return_barrier(kind);\n+    bool return_barrier_exception = Continuation::is_thaw_return_barrier_exception(kind);\n+\n+    address start = __ pc();\n+\n+    if (return_barrier) {\n+      __ ld(sp, Address(xthread, JavaThread::cont_entry_offset()));\n+    }\n+\n+#ifndef PRODUCT\n+    {\n+      Label OK;\n+      __ ld(t0, Address(xthread, JavaThread::cont_entry_offset()));\n+      __ beq(sp, t0, OK);\n+      __ stop(\"incorrect sp\");\n+      __ bind(OK);\n+    }\n+#endif\n+\n+    if (return_barrier) {\n+      \/\/ preserve possible return value from a method returning to the return barrier\n+      __ sub(sp, sp, 2 * wordSize);\n+      __ fsd(f10, Address(sp, 0 * wordSize));\n+      __ sd(x10, Address(sp, 1 * wordSize));\n+    }\n+\n+    __ mv(c_rarg1, (return_barrier ? 1 : 0));\n+    __ call_VM_leaf(CAST_FROM_FN_PTR(address, Continuation::prepare_thaw), xthread, c_rarg1);\n+    __ mv(t1, x10); \/\/ x10 contains the size of the frames to thaw, 0 if overflow or no more frames\n+\n+    if (return_barrier) {\n+      \/\/ restore return value (no safepoint in the call to thaw, so even an oop return value should be OK)\n+      __ ld(x10, Address(sp, 1 * wordSize));\n+      __ fld(f10, Address(sp, 0 * wordSize));\n+      __ add(sp, sp, 2 * wordSize);\n+    }\n+\n+#ifndef PRODUCT\n+    {\n+      Label OK;\n+      __ ld(t0, Address(xthread, JavaThread::cont_entry_offset()));\n+      __ beq(sp, t0, OK);\n+      __ stop(\"incorrect sp\");\n+      __ bind(OK);\n+    }\n+#endif\n+\n+    Label thaw_success;\n+    \/\/ t1 contains the size of the frames to thaw, 0 if overflow or no more frames\n+    __ bnez(t1, thaw_success);\n+    __ la(t0, ExternalAddress(StubRoutines::throw_StackOverflowError_entry()));\n+    __ jr(t0);\n+    __ bind(thaw_success);\n+\n+    \/\/ make room for the thawed frames\n+    __ sub(t0, sp, t1);\n+    __ andi(sp, t0, -16); \/\/ align\n+\n+    if (return_barrier) {\n+      \/\/ save original return value -- again\n+      __ sub(sp, sp, 2 * wordSize);\n+      __ fsd(f10, Address(sp, 0 * wordSize));\n+      __ sd(x10, Address(sp, 1 * wordSize));\n+    }\n+\n+    \/\/ If we want, we can templatize thaw by kind, and have three different entries\n+    __ mv(c_rarg1, kind);\n+\n+    __ call_VM_leaf(Continuation::thaw_entry(), xthread, c_rarg1);\n+    __ mv(t1, x10); \/\/ x10 is the sp of the yielding frame\n+\n+    if (return_barrier) {\n+      \/\/ restore return value (no safepoint in the call to thaw, so even an oop return value should be OK)\n+      __ ld(x10, Address(sp, 1 * wordSize));\n+      __ fld(f10, Address(sp, 0 * wordSize));\n+      __ add(sp, sp, 2 * wordSize);\n+    } else {\n+      __ mv(x10, zr); \/\/ return 0 (success) from doYield\n+    }\n+\n+    \/\/ we're now on the yield frame (which is in an address above us b\/c sp has been pushed down)\n+    __ mv(fp, t1);\n+    __ sub(sp, t1, 2 * wordSize); \/\/ now pointing to fp spill\n+\n+    if (return_barrier_exception) {\n+      __ ld(c_rarg1, Address(fp, -1 * wordSize)); \/\/ return address\n+      __ verify_oop(x10);\n+      __ mv(x9, x10); \/\/ save return value contaning the exception oop in callee-saved x9\n+\n+      __ call_VM_leaf(CAST_FROM_FN_PTR(address, SharedRuntime::exception_handler_for_return_address), xthread, c_rarg1);\n+\n+      \/\/ see OptoRuntime::generate_exception_blob: x10 -- exception oop, x13 -- exception pc\n+\n+      __ mv(x11, x10); \/\/ the exception handler\n+      __ mv(x10, x9); \/\/ restore return value contaning the exception oop\n+      __ verify_oop(x10);\n+\n+      __ leave();\n+      __ mv(x13, ra);\n+      __ jr(x11); \/\/ the exception handler\n+    } else {\n+      \/\/ We're \"returning\" into the topmost thawed frame; see Thaw::push_return_frame\n+      __ leave();\n+      __ ret();\n+    }\n+\n+    return start;\n+  }\n+\n+  address generate_cont_thaw() {\n+    if (!Continuations::enabled()) return nullptr;\n+\n+    StubCodeMark mark(this, \"StubRoutines\", \"Cont thaw\");\n+    address start = __ pc();\n+    generate_cont_thaw(Continuation::thaw_top);\n+    return start;\n+  }\n+\n+  address generate_cont_returnBarrier() {\n+    if (!Continuations::enabled()) return nullptr;\n+\n+    \/\/ TODO: will probably need multiple return barriers depending on return type\n+    StubCodeMark mark(this, \"StubRoutines\", \"cont return barrier\");\n+    address start = __ pc();\n+\n+    generate_cont_thaw(Continuation::thaw_return_barrier);\n+\n+    return start;\n+  }\n+\n+  address generate_cont_returnBarrier_exception() {\n+    if (!Continuations::enabled()) return nullptr;\n+\n+    StubCodeMark mark(this, \"StubRoutines\", \"cont return barrier exception handler\");\n+    address start = __ pc();\n+\n+    generate_cont_thaw(Continuation::thaw_return_barrier_exception);\n+\n+    return start;\n+  }\n@@ -3670,0 +3809,3 @@\n+#undef __\n+#define __ this->\n+\n@@ -3939,317 +4081,1 @@\n-      \/\/ x0 is not written, we known the number of vector elements.\n-\n-      if (vset_sew == Assembler::e64 && MaxVectorSize == 16) { \/\/ SHA512 and VLEN = 128\n-        __ vsetivli(x0, 4, vset_sew, Assembler::m2, Assembler::ma, Assembler::ta);\n-      } else {\n-        __ vsetivli(x0, 4, vset_sew, Assembler::m1, Assembler::ma, Assembler::ta);\n-      }\n-\n-      int64_t indexes = vset_sew == Assembler::e32 ? 0x00041014ul : 0x00082028ul;\n-      __ li(t0, indexes);\n-      __ vmv_v_x(vindex, t0);\n-\n-      \/\/ Step-over a,b, so we are pointing to c.\n-      \/\/ const_add is equal to 4x state variable, div by 2 is thus 2, a,b\n-      __ addi(state_c, state, const_add\/2);\n-\n-      \/\/ Use index-load to get {f,e,b,a},{h,g,d,c}\n-      __ vluxei8_v(vState0, state, vindex);\n-      __ vluxei8_v(vState1, state_c, vindex);\n-\n-      __ bind(multi_block_loop);\n-\n-      \/\/ Capture the initial H values in vHash0 and vHash1 to allow for computing\n-      \/\/ the resulting H', since H' = H+{a',b',c',...,h'}.\n-      __ vmv_v_v(vHash0, vState0);\n-      __ vmv_v_v(vHash1, vState1);\n-\n-      \/\/ Load the 512\/1024-bits of the message block in vW0-vW3 and perform\n-      \/\/ an endian swap on each 4\/8 bytes element.\n-      \/\/\n-      \/\/ If Zvkb is not implemented one can use vrgather\n-      \/\/ with an index sequence to byte-swap.\n-      \/\/  sequence = [3 2 1 0   7 6 5 4  11 10 9 8   15 14 13 12]\n-      \/\/   <https:\/\/oeis.org\/A004444> gives us \"N ^ 3\" as a nice formula to generate\n-      \/\/  this sequence. 'vid' gives us the N.\n-      __ vleXX_v(vset_sew, vW0, buf);\n-      __ vrev8_v(vW0, vW0);\n-      __ addi(buf, buf, const_add);\n-      __ vleXX_v(vset_sew, vW1, buf);\n-      __ vrev8_v(vW1, vW1);\n-      __ addi(buf, buf, const_add);\n-      __ vleXX_v(vset_sew, vW2, buf);\n-      __ vrev8_v(vW2, vW2);\n-      __ addi(buf, buf, const_add);\n-      __ vleXX_v(vset_sew, vW3, buf);\n-      __ vrev8_v(vW3, vW3);\n-      __ addi(buf, buf, const_add);\n-\n-      \/\/ Set v0 up for the vmerge that replaces the first word (idx==0)\n-      __ vid_v(v0);\n-      __ vmseq_vi(v0, v0, 0x0);  \/\/ v0.mask[i] = (i == 0 ? 1 : 0)\n-\n-      VectorRegister rotation_regs[] = {vW0, vW1, vW2, vW3};\n-      int rot_pos = 0;\n-      \/\/ Quad-round #0 (+0, vW0->vW1->vW2->vW3) ... #11 (+3, vW3->vW0->vW1->vW2)\n-      const int qr_end = vset_sew == Assembler::e32 ? 12 : 16;\n-      for (int i = 0; i < qr_end; i++) {\n-        sha2_quad_round(vset_sew,\n-                   rotation_regs[(rot_pos + 0) & 0x3],\n-                   rotation_regs[(rot_pos + 1) & 0x3],\n-                   rotation_regs[(rot_pos + 2) & 0x3],\n-                   rotation_regs[(rot_pos + 3) & 0x3],\n-                   consts,\n-                   vTmp1, vTmp0, vState0, vState1);\n-        ++rot_pos;\n-      }\n-      \/\/ Quad-round #12 (+0, vW0->vW1->vW2->vW3) ... #15 (+3, vW3->vW0->vW1->vW2)\n-      \/\/ Note that we stop generating new message schedule words (Wt, vW0-13)\n-      \/\/ as we already generated all the words we end up consuming (i.e., W[63:60]).\n-      const int qr_c_end = qr_end + 4;\n-      for (int i = qr_end; i < qr_c_end; i++) {\n-        sha2_quad_round(vset_sew,\n-                   rotation_regs[(rot_pos + 0) & 0x3],\n-                   rotation_regs[(rot_pos + 1) & 0x3],\n-                   rotation_regs[(rot_pos + 2) & 0x3],\n-                   rotation_regs[(rot_pos + 3) & 0x3],\n-                   consts,\n-                   vTmp1, vTmp0, vState0, vState1, false, i < (qr_c_end-1));\n-        ++rot_pos;\n-      }\n-\n-      \/\/--------------------------------------------------------------------------------\n-      \/\/ Compute the updated hash value H'\n-      \/\/   H' = H + {h',g',...,b',a'}\n-      \/\/      = {h,g,...,b,a} + {h',g',...,b',a'}\n-      \/\/      = {h+h',g+g',...,b+b',a+a'}\n-\n-      \/\/ H' = H+{a',b',c',...,h'}\n-      __ vadd_vv(vState0, vHash0, vState0);\n-      __ vadd_vv(vState1, vHash1, vState1);\n-\n-      if (multi_block) {\n-        int total_adds = vset_sew == Assembler::e32 ? 240 : 608;\n-        __ addi(consts, consts, -total_adds);\n-        __ add(ofs, ofs, vset_sew == Assembler::e32 ? 64 : 128);\n-        __ ble(ofs, limit, multi_block_loop);\n-        __ mv(c_rarg0, ofs); \/\/ return ofs\n-      }\n-\n-      \/\/ Store H[0..8] = {a,b,c,d,e,f,g,h} from\n-      \/\/  vState0 = {f,e,b,a}\n-      \/\/  vState1 = {h,g,d,c}\n-      __ vsuxei8_v(vState0, state,   vindex);\n-      __ vsuxei8_v(vState1, state_c, vindex);\n-\n-      __ leave();\n-      __ ret();\n-\n-      return start;\n-    }\n-  };\n-\n-#endif \/\/ COMPILER2_OR_JVMCI\n-\n-#undef __\n-#define __ masm->\n-\n-  \/\/ Continuation point for throwing of implicit exceptions that are\n-  \/\/ not handled in the current activation. Fabricates an exception\n-  \/\/ oop and initiates normal exception dispatching in this\n-  \/\/ frame. Since we need to preserve callee-saved values (currently\n-  \/\/ only for C2, but done for C1 as well) we need a callee-saved oop\n-  \/\/ map and therefore have to make these stubs into RuntimeStubs\n-  \/\/ rather than BufferBlobs.  If the compiler needs all registers to\n-  \/\/ be preserved between the fault point and the exception handler\n-  \/\/ then it must assume responsibility for that in\n-  \/\/ AbstractCompiler::continuation_for_implicit_null_exception or\n-  \/\/ continuation_for_implicit_division_by_zero_exception. All other\n-  \/\/ implicit exceptions (e.g., NullPointerException or\n-  \/\/ AbstractMethodError on entry) are either at call sites or\n-  \/\/ otherwise assume that stack unwinding will be initiated, so\n-  \/\/ caller saved registers were assumed volatile in the compiler.\n-\n-#undef __\n-#define __ masm->\n-\n-  address generate_throw_exception(const char* name,\n-                                   address runtime_entry,\n-                                   Register arg1 = noreg,\n-                                   Register arg2 = noreg) {\n-    \/\/ Information about frame layout at time of blocking runtime call.\n-    \/\/ Note that we only have to preserve callee-saved registers since\n-    \/\/ the compilers are responsible for supplying a continuation point\n-    \/\/ if they expect all registers to be preserved.\n-    \/\/ n.b. riscv asserts that frame::arg_reg_save_area_bytes == 0\n-    assert_cond(runtime_entry != nullptr);\n-    enum layout {\n-      fp_off = 0,\n-      fp_off2,\n-      return_off,\n-      return_off2,\n-      framesize \/\/ inclusive of return address\n-    };\n-\n-    const int insts_size = 1024;\n-    const int locs_size  = 64;\n-\n-    CodeBuffer code(name, insts_size, locs_size);\n-    OopMapSet* oop_maps  = new OopMapSet();\n-    MacroAssembler* masm = new MacroAssembler(&code);\n-    assert_cond(oop_maps != nullptr && masm != nullptr);\n-\n-    address start = __ pc();\n-\n-    \/\/ This is an inlined and slightly modified version of call_VM\n-    \/\/ which has the ability to fetch the return PC out of\n-    \/\/ thread-local storage and also sets up last_Java_sp slightly\n-    \/\/ differently than the real call_VM\n-\n-    __ enter(); \/\/ Save FP and RA before call\n-\n-    assert(is_even(framesize \/ 2), \"sp not 16-byte aligned\");\n-\n-    \/\/ ra and fp are already in place\n-    __ addi(sp, fp, 0 - ((unsigned)framesize << LogBytesPerInt)); \/\/ prolog\n-\n-    int frame_complete = __ pc() - start;\n-\n-    \/\/ Set up last_Java_sp and last_Java_fp\n-    address the_pc = __ pc();\n-    __ set_last_Java_frame(sp, fp, the_pc, t0);\n-\n-    \/\/ Call runtime\n-    if (arg1 != noreg) {\n-      assert(arg2 != c_rarg1, \"clobbered\");\n-      __ mv(c_rarg1, arg1);\n-    }\n-    if (arg2 != noreg) {\n-      __ mv(c_rarg2, arg2);\n-    }\n-    __ mv(c_rarg0, xthread);\n-    BLOCK_COMMENT(\"call runtime_entry\");\n-    __ call(runtime_entry);\n-\n-    \/\/ Generate oop map\n-    OopMap* map = new OopMap(framesize, 0);\n-    assert_cond(map != nullptr);\n-\n-    oop_maps->add_gc_map(the_pc - start, map);\n-\n-    __ reset_last_Java_frame(true);\n-\n-    __ leave();\n-\n-    \/\/ check for pending exceptions\n-#ifdef ASSERT\n-    Label L;\n-    __ ld(t0, Address(xthread, Thread::pending_exception_offset()));\n-    __ bnez(t0, L);\n-    __ should_not_reach_here();\n-    __ bind(L);\n-#endif \/\/ ASSERT\n-    __ far_jump(RuntimeAddress(StubRoutines::forward_exception_entry()));\n-\n-    \/\/ codeBlob framesize is in words (not VMRegImpl::slot_size)\n-    RuntimeStub* stub =\n-      RuntimeStub::new_runtime_stub(name,\n-                                    &code,\n-                                    frame_complete,\n-                                    (framesize >> (LogBytesPerWord - LogBytesPerInt)),\n-                                    oop_maps, false);\n-    assert(stub != nullptr, \"create runtime stub fail!\");\n-    return stub->entry_point();\n-  }\n-\n-#undef __\n-#define __ _masm->\n-\n-  address generate_cont_thaw(Continuation::thaw_kind kind) {\n-    bool return_barrier = Continuation::is_thaw_return_barrier(kind);\n-    bool return_barrier_exception = Continuation::is_thaw_return_barrier_exception(kind);\n-\n-    address start = __ pc();\n-\n-    if (return_barrier) {\n-      __ ld(sp, Address(xthread, JavaThread::cont_entry_offset()));\n-    }\n-\n-#ifndef PRODUCT\n-    {\n-      Label OK;\n-      __ ld(t0, Address(xthread, JavaThread::cont_entry_offset()));\n-      __ beq(sp, t0, OK);\n-      __ stop(\"incorrect sp\");\n-      __ bind(OK);\n-    }\n-#endif\n-\n-    if (return_barrier) {\n-      \/\/ preserve possible return value from a method returning to the return barrier\n-      __ sub(sp, sp, 2 * wordSize);\n-      __ fsd(f10, Address(sp, 0 * wordSize));\n-      __ sd(x10, Address(sp, 1 * wordSize));\n-    }\n-\n-    __ mv(c_rarg1, (return_barrier ? 1 : 0));\n-    __ call_VM_leaf(CAST_FROM_FN_PTR(address, Continuation::prepare_thaw), xthread, c_rarg1);\n-    __ mv(t1, x10); \/\/ x10 contains the size of the frames to thaw, 0 if overflow or no more frames\n-\n-    if (return_barrier) {\n-      \/\/ restore return value (no safepoint in the call to thaw, so even an oop return value should be OK)\n-      __ ld(x10, Address(sp, 1 * wordSize));\n-      __ fld(f10, Address(sp, 0 * wordSize));\n-      __ add(sp, sp, 2 * wordSize);\n-    }\n-\n-#ifndef PRODUCT\n-    {\n-      Label OK;\n-      __ ld(t0, Address(xthread, JavaThread::cont_entry_offset()));\n-      __ beq(sp, t0, OK);\n-      __ stop(\"incorrect sp\");\n-      __ bind(OK);\n-    }\n-#endif\n-\n-    Label thaw_success;\n-    \/\/ t1 contains the size of the frames to thaw, 0 if overflow or no more frames\n-    __ bnez(t1, thaw_success);\n-    __ la(t0, ExternalAddress(StubRoutines::throw_StackOverflowError_entry()));\n-    __ jr(t0);\n-    __ bind(thaw_success);\n-\n-    \/\/ make room for the thawed frames\n-    __ sub(t0, sp, t1);\n-    __ andi(sp, t0, -16); \/\/ align\n-\n-    if (return_barrier) {\n-      \/\/ save original return value -- again\n-      __ sub(sp, sp, 2 * wordSize);\n-      __ fsd(f10, Address(sp, 0 * wordSize));\n-      __ sd(x10, Address(sp, 1 * wordSize));\n-    }\n-\n-    \/\/ If we want, we can templatize thaw by kind, and have three different entries\n-    __ mv(c_rarg1, kind);\n-\n-    __ call_VM_leaf(Continuation::thaw_entry(), xthread, c_rarg1);\n-    __ mv(t1, x10); \/\/ x10 is the sp of the yielding frame\n-\n-    if (return_barrier) {\n-      \/\/ restore return value (no safepoint in the call to thaw, so even an oop return value should be OK)\n-      __ ld(x10, Address(sp, 1 * wordSize));\n-      __ fld(f10, Address(sp, 0 * wordSize));\n-      __ add(sp, sp, 2 * wordSize);\n-    } else {\n-      __ mv(x10, zr); \/\/ return 0 (success) from doYield\n-    }\n-\n-    \/\/ we're now on the yield frame (which is in an address above us b\/c sp has been pushed down)\n-    __ mv(fp, t1);\n-    __ sub(sp, t1, 2 * wordSize); \/\/ now pointing to fp spill\n-\n-    if (return_barrier_exception) {\n-      __ ld(c_rarg1, Address(fp, -1 * wordSize)); \/\/ return address\n-      __ verify_oop(x10);\n-      __ mv(x9, x10); \/\/ save return value contaning the exception oop in callee-saved x9\n+      \/\/ x0 is not written, we known the number of vector elements.\n@@ -4257,1 +4083,5 @@\n-      __ call_VM_leaf(CAST_FROM_FN_PTR(address, SharedRuntime::exception_handler_for_return_address), xthread, c_rarg1);\n+      if (vset_sew == Assembler::e64 && MaxVectorSize == 16) { \/\/ SHA512 and VLEN = 128\n+        __ vsetivli(x0, 4, vset_sew, Assembler::m2, Assembler::ma, Assembler::ta);\n+      } else {\n+        __ vsetivli(x0, 4, vset_sew, Assembler::m1, Assembler::ma, Assembler::ta);\n+      }\n@@ -4259,1 +4089,3 @@\n-      \/\/ see OptoRuntime::generate_exception_blob: x10 -- exception oop, x13 -- exception pc\n+      int64_t indexes = vset_sew == Assembler::e32 ? 0x00041014ul : 0x00082028ul;\n+      __ li(t0, indexes);\n+      __ vmv_v_x(vindex, t0);\n@@ -4261,3 +4093,3 @@\n-      __ mv(x11, x10); \/\/ the exception handler\n-      __ mv(x10, x9); \/\/ restore return value contaning the exception oop\n-      __ verify_oop(x10);\n+      \/\/ Step-over a,b, so we are pointing to c.\n+      \/\/ const_add is equal to 4x state variable, div by 2 is thus 2, a,b\n+      __ addi(state_c, state, const_add\/2);\n@@ -4265,8 +4097,3 @@\n-      __ leave();\n-      __ mv(x13, ra);\n-      __ jr(x11); \/\/ the exception handler\n-    } else {\n-      \/\/ We're \"returning\" into the topmost thawed frame; see Thaw::push_return_frame\n-      __ leave();\n-      __ ret();\n-    }\n+      \/\/ Use index-load to get {f,e,b,a},{h,g,d,c}\n+      __ vluxei8_v(vState0, state, vindex);\n+      __ vluxei8_v(vState1, state_c, vindex);\n@@ -4274,2 +4101,1 @@\n-    return start;\n-  }\n+      __ bind(multi_block_loop);\n@@ -4277,2 +4103,4 @@\n-  address generate_cont_thaw() {\n-    if (!Continuations::enabled()) return nullptr;\n+      \/\/ Capture the initial H values in vHash0 and vHash1 to allow for computing\n+      \/\/ the resulting H', since H' = H+{a',b',c',...,h'}.\n+      __ vmv_v_v(vHash0, vState0);\n+      __ vmv_v_v(vHash1, vState1);\n@@ -4280,5 +4108,20 @@\n-    StubCodeMark mark(this, \"StubRoutines\", \"Cont thaw\");\n-    address start = __ pc();\n-    generate_cont_thaw(Continuation::thaw_top);\n-    return start;\n-  }\n+      \/\/ Load the 512\/1024-bits of the message block in vW0-vW3 and perform\n+      \/\/ an endian swap on each 4\/8 bytes element.\n+      \/\/\n+      \/\/ If Zvkb is not implemented one can use vrgather\n+      \/\/ with an index sequence to byte-swap.\n+      \/\/  sequence = [3 2 1 0   7 6 5 4  11 10 9 8   15 14 13 12]\n+      \/\/   <https:\/\/oeis.org\/A004444> gives us \"N ^ 3\" as a nice formula to generate\n+      \/\/  this sequence. 'vid' gives us the N.\n+      __ vleXX_v(vset_sew, vW0, buf);\n+      __ vrev8_v(vW0, vW0);\n+      __ addi(buf, buf, const_add);\n+      __ vleXX_v(vset_sew, vW1, buf);\n+      __ vrev8_v(vW1, vW1);\n+      __ addi(buf, buf, const_add);\n+      __ vleXX_v(vset_sew, vW2, buf);\n+      __ vrev8_v(vW2, vW2);\n+      __ addi(buf, buf, const_add);\n+      __ vleXX_v(vset_sew, vW3, buf);\n+      __ vrev8_v(vW3, vW3);\n+      __ addi(buf, buf, const_add);\n@@ -4286,2 +4129,3 @@\n-  address generate_cont_returnBarrier() {\n-    if (!Continuations::enabled()) return nullptr;\n+      \/\/ Set v0 up for the vmerge that replaces the first word (idx==0)\n+      __ vid_v(v0);\n+      __ vmseq_vi(v0, v0, 0x0);  \/\/ v0.mask[i] = (i == 0 ? 1 : 0)\n@@ -4289,3 +4133,28 @@\n-    \/\/ TODO: will probably need multiple return barriers depending on return type\n-    StubCodeMark mark(this, \"StubRoutines\", \"cont return barrier\");\n-    address start = __ pc();\n+      VectorRegister rotation_regs[] = {vW0, vW1, vW2, vW3};\n+      int rot_pos = 0;\n+      \/\/ Quad-round #0 (+0, vW0->vW1->vW2->vW3) ... #11 (+3, vW3->vW0->vW1->vW2)\n+      const int qr_end = vset_sew == Assembler::e32 ? 12 : 16;\n+      for (int i = 0; i < qr_end; i++) {\n+        sha2_quad_round(vset_sew,\n+                   rotation_regs[(rot_pos + 0) & 0x3],\n+                   rotation_regs[(rot_pos + 1) & 0x3],\n+                   rotation_regs[(rot_pos + 2) & 0x3],\n+                   rotation_regs[(rot_pos + 3) & 0x3],\n+                   consts,\n+                   vTmp1, vTmp0, vState0, vState1);\n+        ++rot_pos;\n+      }\n+      \/\/ Quad-round #12 (+0, vW0->vW1->vW2->vW3) ... #15 (+3, vW3->vW0->vW1->vW2)\n+      \/\/ Note that we stop generating new message schedule words (Wt, vW0-13)\n+      \/\/ as we already generated all the words we end up consuming (i.e., W[63:60]).\n+      const int qr_c_end = qr_end + 4;\n+      for (int i = qr_end; i < qr_c_end; i++) {\n+        sha2_quad_round(vset_sew,\n+                   rotation_regs[(rot_pos + 0) & 0x3],\n+                   rotation_regs[(rot_pos + 1) & 0x3],\n+                   rotation_regs[(rot_pos + 2) & 0x3],\n+                   rotation_regs[(rot_pos + 3) & 0x3],\n+                   consts,\n+                   vTmp1, vTmp0, vState0, vState1, false, i < (qr_c_end-1));\n+        ++rot_pos;\n+      }\n@@ -4293,1 +4162,5 @@\n-    generate_cont_thaw(Continuation::thaw_return_barrier);\n+      \/\/--------------------------------------------------------------------------------\n+      \/\/ Compute the updated hash value H'\n+      \/\/   H' = H + {h',g',...,b',a'}\n+      \/\/      = {h,g,...,b,a} + {h',g',...,b',a'}\n+      \/\/      = {h+h',g+g',...,b+b',a+a'}\n@@ -4295,2 +4168,3 @@\n-    return start;\n-  }\n+      \/\/ H' = H+{a',b',c',...,h'}\n+      __ vadd_vv(vState0, vHash0, vState0);\n+      __ vadd_vv(vState1, vHash1, vState1);\n@@ -4298,2 +4172,7 @@\n-  address generate_cont_returnBarrier_exception() {\n-    if (!Continuations::enabled()) return nullptr;\n+      if (multi_block) {\n+        int total_adds = vset_sew == Assembler::e32 ? 240 : 608;\n+        __ addi(consts, consts, -total_adds);\n+        __ add(ofs, ofs, vset_sew == Assembler::e32 ? 64 : 128);\n+        __ ble(ofs, limit, multi_block_loop);\n+        __ mv(c_rarg0, ofs); \/\/ return ofs\n+      }\n@@ -4301,2 +4180,5 @@\n-    StubCodeMark mark(this, \"StubRoutines\", \"cont return barrier exception handler\");\n-    address start = __ pc();\n+      \/\/ Store H[0..8] = {a,b,c,d,e,f,g,h} from\n+      \/\/  vState0 = {f,e,b,a}\n+      \/\/  vState1 = {h,g,d,c}\n+      __ vsuxei8_v(vState0, state,   vindex);\n+      __ vsuxei8_v(vState1, state_c, vindex);\n@@ -4304,1 +4186,2 @@\n-    generate_cont_thaw(Continuation::thaw_return_barrier_exception);\n+      __ leave();\n+      __ ret();\n@@ -4306,2 +4189,6 @@\n-    return start;\n-  }\n+      return start;\n+    }\n+  };\n+\n+#undef __\n+#define __ _masm->\n@@ -4432,2 +4319,0 @@\n-#if COMPILER2_OR_JVMCI\n-\n@@ -4684,2 +4569,0 @@\n-#endif \/\/ COMPILER2_OR_JVMCI\n-\n@@ -4718,2 +4601,0 @@\n-#if COMPILER2_OR_JVMCI\n-\n@@ -4824,2 +4705,0 @@\n-#endif \/\/ COMPILER2_OR_JVMCI\n-\n@@ -5019,2 +4898,0 @@\n-#if COMPILER2_OR_JVMCI\n-\n@@ -5170,2 +5047,0 @@\n-\n-\n@@ -5485,0 +5360,108 @@\n+  \/\/ Continuation point for throwing of implicit exceptions that are\n+  \/\/ not handled in the current activation. Fabricates an exception\n+  \/\/ oop and initiates normal exception dispatching in this\n+  \/\/ frame. Since we need to preserve callee-saved values (currently\n+  \/\/ only for C2, but done for C1 as well) we need a callee-saved oop\n+  \/\/ map and therefore have to make these stubs into RuntimeStubs\n+  \/\/ rather than BufferBlobs.  If the compiler needs all registers to\n+  \/\/ be preserved between the fault point and the exception handler\n+  \/\/ then it must assume responsibility for that in\n+  \/\/ AbstractCompiler::continuation_for_implicit_null_exception or\n+  \/\/ continuation_for_implicit_division_by_zero_exception. All other\n+  \/\/ implicit exceptions (e.g., NullPointerException or\n+  \/\/ AbstractMethodError on entry) are either at call sites or\n+  \/\/ otherwise assume that stack unwinding will be initiated, so\n+  \/\/ caller saved registers were assumed volatile in the compiler.\n+\n+#undef __\n+#define __ masm->\n+\n+  address generate_throw_exception(const char* name,\n+                                   address runtime_entry,\n+                                   Register arg1 = noreg,\n+                                   Register arg2 = noreg) {\n+    \/\/ Information about frame layout at time of blocking runtime call.\n+    \/\/ Note that we only have to preserve callee-saved registers since\n+    \/\/ the compilers are responsible for supplying a continuation point\n+    \/\/ if they expect all registers to be preserved.\n+    \/\/ n.b. riscv asserts that frame::arg_reg_save_area_bytes == 0\n+    assert_cond(runtime_entry != nullptr);\n+    enum layout {\n+      fp_off = 0,\n+      fp_off2,\n+      return_off,\n+      return_off2,\n+      framesize \/\/ inclusive of return address\n+    };\n+\n+    const int insts_size = 1024;\n+    const int locs_size  = 64;\n+\n+    CodeBuffer code(name, insts_size, locs_size);\n+    OopMapSet* oop_maps  = new OopMapSet();\n+    MacroAssembler* masm = new MacroAssembler(&code);\n+    assert_cond(oop_maps != nullptr && masm != nullptr);\n+\n+    address start = __ pc();\n+\n+    \/\/ This is an inlined and slightly modified version of call_VM\n+    \/\/ which has the ability to fetch the return PC out of\n+    \/\/ thread-local storage and also sets up last_Java_sp slightly\n+    \/\/ differently than the real call_VM\n+\n+    __ enter(); \/\/ Save FP and RA before call\n+\n+    assert(is_even(framesize \/ 2), \"sp not 16-byte aligned\");\n+\n+    \/\/ ra and fp are already in place\n+    __ addi(sp, fp, 0 - ((unsigned)framesize << LogBytesPerInt)); \/\/ prolog\n+\n+    int frame_complete = __ pc() - start;\n+\n+    \/\/ Set up last_Java_sp and last_Java_fp\n+    address the_pc = __ pc();\n+    __ set_last_Java_frame(sp, fp, the_pc, t0);\n+\n+    \/\/ Call runtime\n+    if (arg1 != noreg) {\n+      assert(arg2 != c_rarg1, \"clobbered\");\n+      __ mv(c_rarg1, arg1);\n+    }\n+    if (arg2 != noreg) {\n+      __ mv(c_rarg2, arg2);\n+    }\n+    __ mv(c_rarg0, xthread);\n+    BLOCK_COMMENT(\"call runtime_entry\");\n+    __ call(runtime_entry);\n+\n+    \/\/ Generate oop map\n+    OopMap* map = new OopMap(framesize, 0);\n+    assert_cond(map != nullptr);\n+\n+    oop_maps->add_gc_map(the_pc - start, map);\n+\n+    __ reset_last_Java_frame(true);\n+\n+    __ leave();\n+\n+    \/\/ check for pending exceptions\n+#ifdef ASSERT\n+    Label L;\n+    __ ld(t0, Address(xthread, Thread::pending_exception_offset()));\n+    __ bnez(t0, L);\n+    __ should_not_reach_here();\n+    __ bind(L);\n+#endif \/\/ ASSERT\n+    __ far_jump(RuntimeAddress(StubRoutines::forward_exception_entry()));\n+\n+    \/\/ codeBlob framesize is in words (not VMRegImpl::slot_size)\n+    RuntimeStub* stub =\n+      RuntimeStub::new_runtime_stub(name,\n+                                    &code,\n+                                    frame_complete,\n+                                    (framesize >> (LogBytesPerWord - LogBytesPerInt)),\n+                                    oop_maps, false);\n+    assert(stub != nullptr, \"create runtime stub fail!\");\n+    return stub->entry_point();\n+  }\n+\n","filename":"src\/hotspot\/cpu\/riscv\/stubGenerator_riscv.cpp","additions":351,"deletions":368,"binary":false,"changes":719,"status":"modified"}]}