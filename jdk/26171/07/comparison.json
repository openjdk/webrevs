{"files":[{"patch":"@@ -40,0 +40,4 @@\n+size_t PLAB::min_size_bytes() {\n+  return min_size() * HeapWordSize;\n+}\n+\n","filename":"src\/hotspot\/share\/gc\/shared\/plab.cpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -81,0 +81,2 @@\n+  \/\/ Minimum PLAB size in bytes.\n+  static size_t min_size_bytes();\n","filename":"src\/hotspot\/share\/gc\/shared\/plab.hpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -29,0 +29,1 @@\n+#include \"gc\/shenandoah\/shenandoahFreeSet.hpp\"\n@@ -88,1 +89,1 @@\n-    } else if (region->is_regular()) {\n+    } else if (region->is_regular() && region->has_allocs()) {\n@@ -94,0 +95,3 @@\n+        if (region->reserved_for_direct_allocation()) {\n+          heap->free_set()->release_directly_allocatable_region(region);\n+        }\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/heuristics\/shenandoahGenerationalHeuristics.cpp","additions":5,"deletions":1,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -30,0 +30,1 @@\n+#include \"gc\/shenandoah\/shenandoahFreeSet.hpp\"\n@@ -109,1 +110,1 @@\n-    } else if (region->is_regular()) {\n+    } else if (region->is_regular() && region->has_allocs()) {\n@@ -115,0 +116,3 @@\n+        if (region->reserved_for_direct_allocation()) {\n+          heap->free_set()->release_directly_allocatable_region(region);\n+        }\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/heuristics\/shenandoahHeuristics.cpp","additions":5,"deletions":1,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -30,0 +30,1 @@\n+#include \"gc\/shenandoah\/shenandoahFreeSet.hpp\"\n@@ -105,0 +106,3 @@\n+    if (r->reserved_for_direct_allocation()) {\n+      _heap->free_set()->release_directly_allocatable_region(r);\n+    }\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahCollectionSet.cpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -35,0 +35,1 @@\n+#include \"gc\/shenandoah\/shenandoahUtils.hpp\"\n@@ -38,0 +39,1 @@\n+#include \"runtime\/interfaceSupport.inline.hpp\"\n@@ -406,1 +408,1 @@\n-void ShenandoahRegionPartitions::increase_used(ShenandoahFreeSetPartitionId which_partition, size_t bytes) {\n+inline void ShenandoahRegionPartitions::increase_used(ShenandoahFreeSetPartitionId which_partition, size_t bytes) {\n@@ -409,1 +411,0 @@\n-\n@@ -417,1 +418,1 @@\n-void ShenandoahRegionPartitions::decrease_used(ShenandoahFreeSetPartitionId which_partition, size_t bytes) {\n+inline void ShenandoahRegionPartitions::decrease_used(ShenandoahFreeSetPartitionId which_partition, size_t bytes) {\n@@ -420,1 +421,0 @@\n-  assert (_used[int(which_partition)] >= bytes, \"Must not use less than zero after decrease\");\n@@ -423,0 +423,3 @@\n+  assert (_used[int(which_partition)] <= _capacity[int(which_partition)],\n+          \"Must not use (%zu) more than capacity (%zu) after decrease by %zu\",\n+          _used[int(which_partition)], _capacity[int(which_partition)], bytes);\n@@ -660,1 +663,1 @@\n-  size_t waste_bytes = 0;\n+  size_t remnant_bytes = 0;\n@@ -668,3 +671,2 @@\n-    size_t fill_padding = _region_size_bytes - used_bytes;\n-    waste_bytes = fill_padding;\n-    increase_used(partition, fill_padding);\n+    remnant_bytes = _region_size_bytes - used_bytes;\n+    increase_used(partition, remnant_bytes);\n@@ -677,3 +679,2 @@\n-  \/\/ is retired and no more memory will be allocated from within it.\n-\n-  return waste_bytes;\n+  \/\/ is retired and no more memory will be allocated from within it\n+  return remnant_bytes;\n@@ -756,0 +757,4 @@\n+  if (orig_partition == ShenandoahFreeSetPartitionId::Mutator && r->reserved_for_direct_allocation()) {\n+    ShenandoahHeap::heap()->free_set()->release_directly_allocatable_region(r);\n+  }\n+\n@@ -936,0 +941,1 @@\n+    ShenandoahHeapRegion* r = ShenandoahHeap::heap()->get_region(i);\n@@ -940,2 +946,1 @@\n-        assert(!validate_totals || (capacity != _region_size_bytes), \"Should not be retired if empty\");\n-        ShenandoahHeapRegion* r = ShenandoahHeap::heap()->get_region(i);\n+        assert(!validate_totals || r->reserved_for_direct_allocation() || (capacity != _region_size_bytes), \"Should not be retired if empty\");\n@@ -957,1 +962,1 @@\n-          assert(r->is_cset() || (capacity < PLAB::min_size() * HeapWordSize),\n+          assert(r->is_cset() || r->reserved_for_direct_allocation() || (capacity < PLAB::min_size() * HeapWordSize),\n@@ -1190,0 +1195,43 @@\n+PaddedEnd<ShenandoahDirectlyAllocatableRegionAffinity::Affinity>* ShenandoahDirectlyAllocatableRegionAffinity::_affinity = nullptr;\n+THREAD_LOCAL Thread* ShenandoahDirectlyAllocatableRegionAffinity::_self = DIRECTLY_ALLOCATABLE_REGION_UNKNOWN_SELF;\n+THREAD_LOCAL uint ShenandoahDirectlyAllocatableRegionAffinity::_index = 0;\n+\n+uint ShenandoahDirectlyAllocatableRegionAffinity::index_slow() {\n+  \/\/ Set current thread\n+  if (_self == DIRECTLY_ALLOCATABLE_REGION_UNKNOWN_SELF) {\n+    _self = Thread::current();\n+  }\n+\n+  \/\/ Create a new random index where the thread will start allocation\n+  _index = static_cast<uint>(os::random()) % ShenandoahDirectlyAllocatableRegionCount;\n+\n+  \/\/ Update affinity table\n+  _affinity[_index]._thread = _self;\n+\n+  return _index;\n+}\n+\n+void ShenandoahDirectlyAllocatableRegionAffinity::initialize() {\n+  assert(_affinity == nullptr, \"Already initialized\");\n+  _affinity = PaddedArray<Affinity, mtGC>::create_unfreeable(ShenandoahDirectlyAllocatableRegionCount);\n+  for (uint32_t i = 0; i < ShenandoahDirectlyAllocatableRegionCount; i++) {\n+    _affinity[i]._thread = DIRECTLY_ALLOCATABLE_REGION_UNKNOWN_AFFINITY;\n+  }\n+}\n+\n+uint ShenandoahDirectlyAllocatableRegionAffinity::index() {\n+  assert(_affinity != nullptr, \"Not initialized\");\n+  \/\/ Fast path\n+  if (_affinity[_index]._thread == _self) {\n+    return _index;\n+  }\n+\n+  \/\/ Slow path\n+  return index_slow();\n+}\n+\n+void ShenandoahDirectlyAllocatableRegionAffinity::set_index(uint index) {\n+  _index = index;\n+  _affinity[_index]._thread = _self;\n+}\n+\n@@ -1208,0 +1256,5 @@\n+  _direct_allocation_regions = PaddedArray<ShenandoahDirectAllocationRegion, mtGC>::create_unfreeable(ShenandoahDirectlyAllocatableRegionCount);\n+  for (uint i = 0; i < ShenandoahDirectlyAllocatableRegionCount; i++) {\n+    _direct_allocation_regions[i]._address = nullptr;\n+  }\n+  ShenandoahDirectlyAllocatableRegionAffinity::initialize();\n@@ -1286,1 +1339,1 @@\n-    if (r->affiliation() == affiliation) {\n+    if (r->affiliation() == affiliation && !r->reserved_for_direct_allocation()) {\n@@ -1383,1 +1436,1 @@\n-    if (alloc_capacity(r) >= min_size * HeapWordSize) {\n+    if (!r->reserved_for_direct_allocation() && alloc_capacity(r) >= min_size * HeapWordSize) {\n@@ -1451,1 +1504,1 @@\n-    if (can_allocate_from(r)) {\n+    if (can_allocate_from(r) && !r->reserved_for_direct_allocation()) {\n@@ -1754,1 +1807,2 @@\n-    while (!can_allocate_from(_heap->get_region(end))) {\n+    ShenandoahHeapRegion* region = _heap->get_region(end);\n+    while (!can_allocate_from(region) || region->reserved_for_direct_allocation()) {\n@@ -1777,0 +1831,1 @@\n+      region = _heap->get_region(end);\n@@ -2201,13 +2256,6 @@\n-        if (region->is_trash() || !region->is_old()) {\n-          \/\/ Both young and old collected regions (trashed) are placed into the Mutator set\n-          _partitions.raw_assign_membership(idx, ShenandoahFreeSetPartitionId::Mutator);\n-          if (idx < mutator_leftmost) {\n-            mutator_leftmost = idx;\n-          }\n-          if (idx > mutator_rightmost) {\n-            mutator_rightmost = idx;\n-          }\n-          if (ac == region_size_bytes) {\n-            mutator_empty++;\n-            if (idx < mutator_leftmost_empty) {\n-              mutator_leftmost_empty = idx;\n+        if (!region->reserved_for_direct_allocation()) {\n+          if (region->is_trash() || !region->is_old()) {\n+            \/\/ Both young and old collected regions (trashed) are placed into the Mutator set\n+            _partitions.raw_assign_membership(idx, ShenandoahFreeSetPartitionId::Mutator);\n+            if (idx < mutator_leftmost) {\n+              mutator_leftmost = idx;\n@@ -2215,2 +2263,2 @@\n-            if (idx > mutator_rightmost_empty) {\n-              mutator_rightmost_empty = idx;\n+            if (idx > mutator_rightmost) {\n+              mutator_rightmost = idx;\n@@ -2218,0 +2266,14 @@\n+            if (ac == region_size_bytes) {\n+              mutator_empty++;\n+              if (idx < mutator_leftmost_empty) {\n+                mutator_leftmost_empty = idx;\n+              }\n+              if (idx > mutator_rightmost_empty) {\n+                mutator_rightmost_empty = idx;\n+              }\n+            } else {\n+              affiliated_mutator_regions++;\n+            }\n+            mutator_regions++;\n+            total_mutator_regions++;\n+            mutator_used += (region_size_bytes - ac);\n@@ -2219,1 +2281,13 @@\n-            affiliated_mutator_regions++;\n+            \/\/ !region->is_trash() && region is_old()\n+            _partitions.raw_assign_membership(idx, ShenandoahFreeSetPartitionId::OldCollector);\n+            if (idx < old_collector_leftmost) {\n+              old_collector_leftmost = idx;\n+            }\n+            if (idx > old_collector_rightmost) {\n+              old_collector_rightmost = idx;\n+            }\n+            assert(ac != region_size_bytes, \"Empty regions should be in mutator partition\");\n+            affiliated_old_collector_regions++;\n+            old_collector_regions++;\n+            total_old_collector_regions++;\n+            old_collector_used += region_size_bytes - ac;\n@@ -2221,3 +2295,0 @@\n-          mutator_regions++;\n-          total_mutator_regions++;\n-          mutator_used += (region_size_bytes - ac);\n@@ -2225,13 +2296,5 @@\n-          \/\/ !region->is_trash() && region is_old()\n-          _partitions.raw_assign_membership(idx, ShenandoahFreeSetPartitionId::OldCollector);\n-          if (idx < old_collector_leftmost) {\n-            old_collector_leftmost = idx;\n-          }\n-          if (idx > old_collector_rightmost) {\n-            old_collector_rightmost = idx;\n-          }\n-          assert(ac != region_size_bytes, \"Empty regions should be in mutator partition\");\n-          affiliated_old_collector_regions++;\n-          old_collector_regions++;\n-          total_old_collector_regions++;\n-          old_collector_used += region_size_bytes - ac;\n+          \/\/ region->reserved_for_direct_allocation(), a reserved region should count in mutator regions,\n+          \/\/ but it won't be assigned to Mutator in the partition table, the entire region is counted as used.\n+          total_mutator_regions++;\n+          mutator_used += region_size_bytes;\n+          affiliated_mutator_regions++;\n@@ -2934,1 +2997,1 @@\n-    } else {\n+    } else if (!r->reserved_for_direct_allocation()) {\n@@ -3315,0 +3378,312 @@\n+HeapWord* ShenandoahFreeSet::allocate_humongous(ShenandoahAllocRequest& req) {\n+  assert(ShenandoahHeapRegion::requires_humongous(req.size()), \"Must be humongous alloc\");\n+  ShenandoahHeapLocker locker(_heap->lock(), req.is_mutator_alloc());\n+  return allocate_contiguous(req, \/*is_humongous*\/true);\n+}\n+\n+HeapWord* ShenandoahFreeSet::allocate_contiguous_cds(ShenandoahAllocRequest& req) {\n+  assert(req.type() == ShenandoahAllocRequest::_alloc_cds, \"Must be CDS alloc.\");\n+  ShenandoahHeapLocker locker(_heap->lock(), req.is_mutator_alloc());\n+  return allocate_contiguous(req, \/*is_humongous*\/false);\n+}\n+\n+template<bool IS_TLAB>\n+HeapWord* ShenandoahFreeSet::cas_allocate_single_for_mutator(uint start_index, ShenandoahAllocRequest &req, bool &in_new_region) {\n+  HeapWord *obj = nullptr;\n+  uint i = 0u;\n+  while (i < ShenandoahDirectlyAllocatableRegionCount) {\n+    {\n+      \/\/ Yield to Safepoint\n+      ThreadBlockInVM tbivm(JavaThread::current());\n+    }\n+    uint idx = (start_index + i) % ShenandoahDirectlyAllocatableRegionCount;\n+    ShenandoahDirectAllocationRegion& shared_region = _direct_allocation_regions[idx];\n+    ShenandoahHeapRegion* r = nullptr;\n+    \/\/ Intentionally not using AtomicAccess::load, if a mutator see a stale region it will fail to allocate anyway.\n+    if ((r = shared_region._address) != nullptr && r->reserved_for_direct_allocation()) {\n+      obj = cas_allocate_in_for_mutator<IS_TLAB>(r, req, in_new_region);\n+      if (obj != nullptr) {\n+        return obj;\n+      }\n+    }\n+    i++;\n+  }\n+  return obj;\n+}\n+\n+template<bool IS_TLAB>\n+HeapWord* ShenandoahFreeSet::try_allocate_single_for_mutator(ShenandoahAllocRequest &req, bool &in_new_region) {\n+  shenandoah_assert_not_heaplocked();\n+  assert(req.is_mutator_alloc(), \"Must be mutator allocation\");\n+  assert(req.is_young(), \"Mutator allocations always come from young generation.\");\n+  assert(!ShenandoahHeapRegion::requires_humongous(req.size()), \"Must not\");\n+  assert(req.type() == ShenandoahAllocRequest::_alloc_tlab ||\n+         req.type() == ShenandoahAllocRequest::_alloc_shared ||\n+         req.type() == ShenandoahAllocRequest::_alloc_cds,  \"Must be\");\n+\n+  uint start_idx = ShenandoahDirectlyAllocatableRegionAffinity::index();\n+  for (;;) {\n+    HeapWord* obj = nullptr;\n+    obj = cas_allocate_single_for_mutator<IS_TLAB>(start_idx, req, in_new_region);\n+    if (obj != nullptr) {\n+      return obj;\n+    }\n+\n+    uint next_start_index = ShenandoahDirectlyAllocatableRegionCount;\n+    if (!try_allocate_directly_allocatable_regions(start_idx, req, obj, in_new_region, next_start_index)) {\n+      if (obj != nullptr) {\n+        return obj;\n+      }\n+\n+      \/\/ No new directly allocatable region, but an existing region with sufficient memory has been found.\n+      \/\/ This only happens when other thread wins the race to refresh the regions.\n+      if (next_start_index != ShenandoahDirectlyAllocatableRegionCount) {\n+        start_idx = next_start_index;\n+      } else {\n+        return nullptr;\n+      }\n+    } else {\n+      assert(obj != nullptr, \"Must be\");\n+      return obj;\n+    }\n+  }\n+}\n+\n+\/\/ Explicit specializations\n+template HeapWord* ShenandoahFreeSet::try_allocate_single_for_mutator<true>(ShenandoahAllocRequest &req, bool &in_new_region);\n+template HeapWord* ShenandoahFreeSet::try_allocate_single_for_mutator<false>(ShenandoahAllocRequest &req, bool &in_new_region);\n+\n+template<bool IS_TLAB>\n+HeapWord* ShenandoahFreeSet::cas_allocate_in_for_mutator(ShenandoahHeapRegion* region, ShenandoahAllocRequest &req, bool &in_new_region) {\n+  HeapWord* obj = nullptr;\n+  size_t actual_size = req.size();\n+  if (IS_TLAB) {\n+    obj = region->allocate_lab_atomic(req, actual_size);\n+  } else {\n+    obj = region->allocate_atomic(actual_size, req);\n+  }\n+  if (obj != nullptr) {\n+    assert(actual_size > 0, \"Must be\");\n+    req.set_actual_size(actual_size);\n+    if (pointer_delta(obj, region->bottom()) == actual_size) {\n+      \/\/ Set to true if it is the first object\/tlab allocated in the region.\n+      in_new_region = true;\n+    }\n+  }\n+  return obj;\n+}\n+\n+class DirectAllocatableRegionRefillClosure final : public ShenandoahHeapRegionIterationClosure {\n+  PaddedEnd<ShenandoahDirectAllocationRegion>* _direct_allocation_regions;\n+  \/\/ inclusive\n+  const uint _start_index;\n+  int _scanned_region;\n+  int _next_retire_eligible_region;\n+  ShenandoahFreeSet* const _free_set = ShenandoahHeap::heap()->free_set();\n+public:\n+  ShenandoahAllocRequest &_req;\n+  HeapWord* &_obj;\n+  bool &_in_new_region;\n+  bool _new_region_allocated = false;\n+  bool _affiliation_changed = false;\n+  uint _next_region_with_sufficient_mem;\n+  const size_t _min_req_byte_size;\n+\n+  DirectAllocatableRegionRefillClosure(PaddedEnd<ShenandoahDirectAllocationRegion>* direct_allocation_regions, uint start_index, ShenandoahAllocRequest &req, HeapWord* &obj, bool &in_new_region)\n+    : _direct_allocation_regions(direct_allocation_regions),\n+      _start_index(start_index),\n+      _scanned_region(0),\n+      _req(req),\n+      _obj(obj),\n+      _in_new_region(in_new_region),\n+      _next_region_with_sufficient_mem(ShenandoahDirectlyAllocatableRegionCount),\n+      _min_req_byte_size((req.type() == ShenandoahAllocRequest::_alloc_tlab ? req.min_size() : req.size()) * HeapWordSize) {\n+    _next_retire_eligible_region = find_next_retire_eligible_region();\n+  }\n+\n+  int find_next_retire_eligible_region() {\n+    while (_scanned_region < (int) ShenandoahDirectlyAllocatableRegionCount) {\n+      const uint idx = (_start_index + (size_t) _scanned_region) % ShenandoahDirectlyAllocatableRegionCount;\n+      _scanned_region++;\n+      ShenandoahDirectAllocationRegion& shared_region = _direct_allocation_regions[idx];\n+      ShenandoahHeapRegion* const r = shared_region._address;\n+      \/\/ Found the first eligible region\n+      if (r == nullptr) {\n+        return idx;\n+      }\n+      if (r->free() < PLAB::min_size_bytes()) {\n+        assert(r->reserved_for_direct_allocation(), \"Must be direct allocation reserved region.\");\n+        AtomicAccess::store(&shared_region._address, static_cast<ShenandoahHeapRegion*>(nullptr));\n+        while (r->direct_alloc_mutators() > 0) {\n+          if (os::is_MP()) SpinPause();\n+          else os::naked_yield();\n+        }\n+        r->release_from_direct_allocation();\n+        return idx;\n+      }\n+\n+      if (_obj == nullptr && _next_region_with_sufficient_mem == ShenandoahDirectlyAllocatableRegionCount && r->free() >= _min_req_byte_size) {\n+        _next_region_with_sufficient_mem = idx;\n+      }\n+    }\n+    return -1;\n+  }\n+\n+  bool heap_region_do(ShenandoahHeapRegion *r) {\n+    if (_next_retire_eligible_region == -1 && _obj != nullptr) return true;\n+    size_t ac = _free_set->alloc_capacity(r);\n+    if (ac < _min_req_byte_size) return false;\n+    if (r->reserved_for_direct_allocation()) return false;\n+    if (ShenandoahHeap::heap()->is_concurrent_weak_root_in_progress() && r->is_trash()) {\n+      return false;\n+    }\n+\n+    if (r->is_trash() || r->is_empty()) {\n+      r->try_recycle_under_lock();\n+      assert(r->affiliation() == FREE, \"Empty region must be free\");\n+      r->set_affiliation(YOUNG_GENERATION);\n+      r->make_regular_allocation(YOUNG_GENERATION);\n+      _free_set->partitions()->one_region_is_no_longer_empty(ShenandoahFreeSetPartitionId::Mutator);\n+      _affiliation_changed = true;\n+    }\n+    assert(r->is_affiliated(), \"Region %zu must be affiliated\", r->index());\n+    assert(r->is_young(), \"Region %zu must be young\", r->index());\n+\n+    if (_obj == nullptr) {\n+      _in_new_region = r->is_empty();\n+      size_t actual_size = _req.size();\n+      _obj = _req.is_lab_alloc() ? r->allocate_lab(_req, actual_size) : r->allocate(actual_size, _req);\n+      assert(_obj != nullptr, \"Must have successfully allocated the object.\");\n+      _req.set_actual_size(actual_size);\n+      _free_set->partitions()->increase_used(ShenandoahFreeSetPartitionId::Mutator, actual_size * HeapWordSize);\n+      _free_set->increase_bytes_allocated(actual_size * HeapWordSize);\n+      ac = _free_set->alloc_capacity(r);\n+    }\n+\n+    if (_next_retire_eligible_region != -1 && ac >= ShenandoahHeapRegion::max_tlab_size_bytes()) {\n+      \/\/ After satisfying object allocation, the region still has space to fit at least one tlab.\n+      ShenandoahDirectAllocationRegion& shared_region = _direct_allocation_regions[_next_retire_eligible_region];\n+      assert(AtomicAccess::load(&shared_region._address) == nullptr, \"Must have been released.\");\n+      r->reserve_for_direct_allocation();\n+      \/\/ Remove region from Mutator partition.\n+      size_t reserved_bytes = _free_set->partitions()->retire_from_partition(ShenandoahFreeSetPartitionId::Mutator, r->index(), r->used());\n+      _free_set->increase_bytes_allocated(reserved_bytes);\n+      AtomicAccess::store(&shared_region._address, r);\n+      if (!_new_region_allocated) {\n+        _new_region_allocated = true;\n+      }\n+      _next_retire_eligible_region = find_next_retire_eligible_region();\n+    } else if (ac < PLAB::min_size_bytes()) {\n+      \/\/ if the remaining memory is less than PLAB:min_size_bytes(), retire this region\n+      size_t wasted_bytes = _free_set->partitions()->retire_from_partition(ShenandoahFreeSetPartitionId::Mutator, r->index(), r->used());\n+      _free_set->increase_bytes_allocated(wasted_bytes);\n+    }\n+\n+    return _next_retire_eligible_region == -1 && _obj != nullptr;\n+  }\n+};\n+\n+bool ShenandoahFreeSet::try_allocate_directly_allocatable_regions(uint start_index,\n+                                                                  ShenandoahAllocRequest &req,\n+                                                                  HeapWord* &obj,\n+                                                                  bool& in_new_region,\n+                                                                  uint& new_start_index) {\n+  assert(Thread::current()->is_Java_thread(), \"Must be mutator\");\n+  assert(start_index < ShenandoahDirectlyAllocatableRegionCount, \"Must be\");\n+  shenandoah_assert_not_heaplocked();\n+\n+  ShenandoahHeapLocker locker(ShenandoahHeap::heap()->lock(), true);\n+  DirectAllocatableRegionRefillClosure cl(_direct_allocation_regions, start_index, req, obj, in_new_region);\n+  iterate_regions_for_alloc<true, false>(&cl, false);\n+  if (cl._next_region_with_sufficient_mem != ShenandoahDirectlyAllocatableRegionCount && obj == nullptr) {\n+    new_start_index = cl._next_region_with_sufficient_mem;\n+  }\n+  recompute_total_used<\/* UsedByMutatorChanged *\/ true,\n+                       \/* UsedByCollectorChanged *\/ false,\n+                       \/* UsedByOldCollectorChanged *\/ false>();\n+  if (cl._affiliation_changed) {\n+    recompute_total_affiliated<\/* MutatorEmptiesChanged *\/ true, \/* CollectorEmptiesChanged *\/ false,\n+                               \/* OldCollectorEmptiesChanged *\/ false, \/* MutatorSizeChanged *\/ false,\n+                               \/* CollectorSizeChanged *\/ false, \/* OldCollectorSizeChanged *\/ false,\n+                               \/* AffiliatedChangesAreYoungNeutral *\/ false, \/* AffiliatedChangesAreGlobalNeutral *\/ false,\n+                               \/* UnaffiliatedChangesAreYoungNeutral *\/ false>();\n+  }\n+  return cl._new_region_allocated;\n+}\n+\n+void ShenandoahFreeSet::release_all_directly_allocatable_regions() {\n+  assert_at_safepoint();\n+  shenandoah_assert_heaplocked();\n+  for (uint i = 0; i < ShenandoahDirectlyAllocatableRegionCount; i++) {\n+    ShenandoahDirectAllocationRegion& shared_region = _direct_allocation_regions[i];\n+    ShenandoahHeapRegion* r = AtomicAccess::load(&shared_region._address);\n+    if (r != nullptr) {\n+      assert(r->reserved_for_direct_allocation(), \"Must be\");\n+      AtomicAccess::store(&shared_region._address, static_cast<ShenandoahHeapRegion*>(nullptr));\n+      r->release_from_direct_allocation();\n+      size_t const free_bytes = r->free();\n+      if (free_bytes == ShenandoahHeapRegion::region_size_bytes()) {\n+        r->make_empty();\n+        r->set_affiliation(FREE);\n+      }\n+      if (free_bytes >= PLAB::min_size_bytes()) {\n+        _partitions.decrease_used(ShenandoahFreeSetPartitionId::Mutator, r->free());\n+        partitions()->unretire_to_partition(r, ShenandoahFreeSetPartitionId::Mutator);\n+      }\n+    }\n+  }\n+}\n+\n+void ShenandoahFreeSet::release_directly_allocatable_region(ShenandoahHeapRegion* region) {\n+  assert_at_safepoint();\n+  assert(region->free() != ShenandoahHeapRegion::region_size_bytes(), \"Must not\");\n+  shenandoah_assert_heaplocked();\n+  for (uint i = 0u; i < ShenandoahDirectlyAllocatableRegionCount; i++) {\n+    ShenandoahDirectAllocationRegion& shared_region = _direct_allocation_regions[i];\n+    if (AtomicAccess::load(&shared_region._address) == region) {\n+      AtomicAccess::store(&shared_region._address, static_cast<ShenandoahHeapRegion*>(nullptr));\n+      break;\n+    }\n+  }\n+  region->release_from_direct_allocation();\n+  if (region->free() >= PLAB::min_size_bytes()) {\n+    _partitions.decrease_used(ShenandoahFreeSetPartitionId::Mutator, region->free());\n+    partitions()->unretire_to_partition(region, ShenandoahFreeSetPartitionId::Mutator);\n+  }\n+}\n+\n+template<bool IS_MUTATOR, bool IS_OLD>\n+uint ShenandoahFreeSet::iterate_regions_for_alloc(ShenandoahHeapRegionIterationClosure* cl, bool use_empty) {\n+  assert((IS_MUTATOR && !IS_OLD) || !IS_MUTATOR, \"Sanity check\");\n+  ShenandoahFreeSetPartitionId partition = IS_MUTATOR ? ShenandoahFreeSetPartitionId::Mutator :\n+                                           (IS_OLD ? ShenandoahFreeSetPartitionId::OldCollector : ShenandoahFreeSetPartitionId::Mutator);\n+  if (_partitions.is_empty(partition)) {\n+    return 0u;\n+  }\n+\n+  if (IS_MUTATOR) {\n+    update_allocation_bias();\n+  }\n+\n+  if (_partitions.alloc_from_left_bias(partition)) {\n+    ShenandoahLeftRightIterator iterator(&_partitions, partition, use_empty);\n+    return iterate_regions_for_alloc(iterator, cl);\n+  } else {\n+    ShenandoahRightLeftIterator iterator(&_partitions, partition, use_empty);\n+    return iterate_regions_for_alloc(iterator, cl);\n+  }\n+}\n+\n+template<typename Iter>\n+uint ShenandoahFreeSet::iterate_regions_for_alloc(Iter& iterator, ShenandoahHeapRegionIterationClosure* cl) {\n+  uint regions_iterated = 0u;\n+  for (idx_t idx = iterator.current(); iterator.has_next(); idx = iterator.next()) {\n+    regions_iterated++;\n+    ShenandoahHeapRegion* r = _heap->get_region(idx);\n+    if (cl->heap_region_do(r)) {\n+      break;\n+    }\n+  }\n+  return regions_iterated;\n+}\n+\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahFreeSet.cpp","additions":426,"deletions":51,"binary":false,"changes":477,"status":"modified"},{"patch":"@@ -32,0 +32,1 @@\n+#include \"memory\/padded.inline.hpp\"\n@@ -51,0 +52,2 @@\n+friend class ShenandoahFreeSet;\n+friend class DirectAllocatableRegionRefillClosure;\n@@ -201,1 +204,1 @@\n-  \/\/ Return the number of waste bytes (if any).\n+  \/\/ Return the number of remnant bytes (if any).\n@@ -378,1 +381,1 @@\n-    return _available[int(which_partition)];\n+    return available_in(which_partition);\n@@ -388,0 +391,1 @@\n+    AtomicAccess::store(_used + int(which_partition), value);\n@@ -415,0 +419,23 @@\n+#define DIRECTLY_ALLOCATABLE_REGION_UNKNOWN_AFFINITY ((Thread*)-1)\n+#define DIRECTLY_ALLOCATABLE_REGION_UNKNOWN_SELF     ((Thread*)-2)\n+\/\/ When mutator threads allocate from directly allocatable regions, ideally the allocation should be evenly\n+\/\/ distributed to all the directly allocatable regions, random is the best portable option for this, but with random\n+\/\/ distribution it may worsen memory locality, e.g. two consecutive allocation from same thread are randomly\n+\/\/ distributed to different allocatable regions. ShenandoahDirectlyAllocatableRegionAffinity solves\/mitigates\n+\/\/ the memory locality issue.\n+\/\/ The idea and code is borrowed from ZGC's CPU affinity, but with random number instead of CPU id.\n+class ShenandoahDirectlyAllocatableRegionAffinity : public AllStatic {\n+  struct Affinity {\n+    Thread* _thread;\n+  };\n+\n+  static PaddedEnd<Affinity>* _affinity;\n+  static THREAD_LOCAL Thread* _self;\n+  static THREAD_LOCAL uint    _index;\n+  static uint index_slow();\n+public:\n+  static void initialize();\n+  static uint index();\n+  static void set_index(uint index);\n+};\n+\n@@ -439,0 +466,10 @@\n+struct ShenandoahDirectAllocationRegion {\n+  ShenandoahHeapRegion* volatile _address = nullptr;\n+};\n+\n+class ShenandoahHeapRegionIterationClosure : public StackObj {\n+public:\n+  \/\/ Return true to break the iteration loop.\n+  virtual bool heap_region_do(ShenandoahHeapRegion *r) { return false; };\n+};\n+\n@@ -440,0 +477,2 @@\n+  friend class DirectAllocatableRegionRefillClosure;\n+\n@@ -444,0 +483,1 @@\n+  PaddedEnd<ShenandoahDirectAllocationRegion>* _direct_allocation_regions;\n@@ -643,0 +683,28 @@\n+  template<bool IS_TLAB>\n+  HeapWord* cas_allocate_single_for_mutator(\n+    uint start_index, ShenandoahAllocRequest &req, bool &in_new_region);\n+\n+  template<bool IS_TLAB>\n+  HeapWord* cas_allocate_in_for_mutator(ShenandoahHeapRegion* region, ShenandoahAllocRequest &req, bool &in_new_region);\n+\n+  \/\/ Try to allocate and refresh directly allocatable regions with heap lock held.\n+  \/\/ It may allocate the object in the region before checking the free bytes of the region, so it may end up\n+  \/\/ allocating the object in region which has sufficient space for the alloc reqeust, but won't reserve the region for\n+  \/\/ CAS alloc because there is not more enough space left.\n+  \/\/ return true if any region is reserved for CAS alloc, which also implies the obj should have been allocated.\n+  \/\/ return false if no new region is reserved for CAS alloc, in this case there some specific scenarios we need to consider:\n+  \/\/  1. the obj may have been allocated in a region with only sufficient space for the alloc req.\n+  \/\/  2. there is no regions can be reserved for CAS alloc, but there is existing directly allocatable region with enough space, this happens\n+  \/\/     when there is other mutator thread contending the lock to refresh the regions.\n+  \/\/     In this scenario, the new_start_index will be set the index of the directly allocatable region.\n+  bool try_allocate_directly_allocatable_regions(uint start_index,\n+                                                 ShenandoahAllocRequest &req,\n+                                                 HeapWord* &obj,\n+                                                 bool &in_new_region,\n+                                                 uint& new_start_index);\n+  template<bool IS_MUTATOR, bool IS_OLD>\n+  uint iterate_regions_for_alloc(ShenandoahHeapRegionIterationClosure* cl, bool use_empty);\n+\n+  template<typename Iter>\n+  uint iterate_regions_for_alloc(Iter& iterator, ShenandoahHeapRegionIterationClosure* cl);\n+\n@@ -666,0 +734,3 @@\n+  ShenandoahRegionPartitions* partitions() {\n+    return &_partitions;\n+  }\n@@ -797,0 +868,11 @@\n+  HeapWord* allocate_humongous(ShenandoahAllocRequest &req);\n+\n+  HeapWord* allocate_contiguous_cds(ShenandoahAllocRequest &req);\n+\n+  void release_all_directly_allocatable_regions();\n+\n+  void release_directly_allocatable_region(ShenandoahHeapRegion *region);\n+\n+  template<bool IS_TLAB>\n+  HeapWord* try_allocate_single_for_mutator(ShenandoahAllocRequest &req, bool &in_new_region);\n+\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahFreeSet.hpp","additions":84,"deletions":2,"binary":false,"changes":86,"status":"modified"},{"patch":"@@ -220,0 +220,5 @@\n+  {\n+    ShenandoahHeapLocker locker(heap->lock());\n+    heap->free_set()->release_all_directly_allocatable_regions();\n+  }\n+\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahFullGC.cpp","additions":5,"deletions":0,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -577,1 +577,1 @@\n-        if (!heap->is_concurrent_old_mark_in_progress() && tams == original_top) {\n+        if (!heap->is_concurrent_old_mark_in_progress() && tams == original_top && !r->reserved_for_direct_allocation()) {\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahGeneration.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -921,1 +921,1 @@\n-      result = allocate_memory_under_lock(req, in_new_region);\n+      result = allocate_memory_for_mutator(req, in_new_region);\n@@ -953,1 +953,1 @@\n-        result = allocate_memory_under_lock(req, in_new_region);\n+        result = allocate_memory_for_mutator(req, in_new_region);\n@@ -994,0 +994,20 @@\n+HeapWord* ShenandoahHeap::allocate_memory_for_mutator(ShenandoahAllocRequest& req, bool& in_new_region) {\n+  assert(req.is_mutator_alloc(), \"Sanity\");\n+  assert(!req.is_old(), \"Sanity\");\n+  shenandoah_assert_not_heaplocked();\n+  ShenandoahFreeSet* free_set = ShenandoahHeap::free_set();\n+  if (ShenandoahHeapRegion::requires_humongous(req.size())) {\n+    in_new_region = true;\n+    if (req.type() == ShenandoahAllocRequest::_alloc_cds) {\n+      return free_set->allocate_contiguous_cds(req);\n+    } else {\n+      return free_set->allocate_humongous(req);\n+    }\n+  }\n+  if (req.is_lab_alloc()) {\n+    return free_set->try_allocate_single_for_mutator<true>(req, in_new_region);\n+  } else {\n+    return free_set->try_allocate_single_for_mutator<false>(req, in_new_region);\n+  }\n+}\n+\n@@ -2805,1 +2825,1 @@\n-    assert(r->is_regular(), \"Must be regular\");\n+    assert(r->is_regular(), \"Must be regular, state: %s\", r->region_state_to_string(r->state()));\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeap.cpp","additions":23,"deletions":3,"binary":false,"changes":26,"status":"modified"},{"patch":"@@ -681,0 +681,1 @@\n+  HeapWord* allocate_memory_for_mutator(ShenandoahAllocRequest& request, bool& in_new_region);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeap.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -92,0 +92,1 @@\n+  _direct_alloc_reserved.unset();\n@@ -327,0 +328,4 @@\n+    case _regular:\n+      if (free() != region_size_bytes()) {\n+        report_illegal_transition(\"emptying\");\n+      }\n@@ -363,3 +368,3 @@\n-  _tlab_allocs = 0;\n-  _gclab_allocs = 0;\n-  _plab_allocs = 0;\n+  AtomicAccess::store(&_tlab_allocs, size_t(0));\n+  AtomicAccess::store(&_gclab_allocs, size_t(0));\n+  AtomicAccess::store(&_plab_allocs, size_t(0));\n@@ -369,1 +374,1 @@\n-  return used() - (_tlab_allocs + _gclab_allocs + _plab_allocs) * HeapWordSize;\n+  return used() - (AtomicAccess::load(&_tlab_allocs) + AtomicAccess::load(&_gclab_allocs) + AtomicAccess::load(&_plab_allocs)) * HeapWordSize;\n@@ -373,1 +378,1 @@\n-  return _tlab_allocs * HeapWordSize;\n+  return AtomicAccess::load(&_tlab_allocs) * HeapWordSize;\n@@ -377,1 +382,1 @@\n-  return _gclab_allocs * HeapWordSize;\n+  return AtomicAccess::load(&_gclab_allocs) * HeapWordSize;\n@@ -381,1 +386,5 @@\n-  return _plab_allocs * HeapWordSize;\n+  return AtomicAccess::load(&_plab_allocs) * HeapWordSize;\n+}\n+\n+bool ShenandoahHeapRegion::has_allocs() const {\n+  return top() > bottom();\n@@ -853,0 +862,2 @@\n+  assert(new_affiliation != OLD_GENERATION || !reserved_for_direct_allocation(), \"Reserved region can't move to old\");\n+\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeapRegion.cpp","additions":18,"deletions":7,"binary":false,"changes":25,"status":"modified"},{"patch":"@@ -253,1 +253,1 @@\n-  HeapWord* _top;\n+  HeapWord* volatile _top;\n@@ -255,3 +255,3 @@\n-  size_t _tlab_allocs;\n-  size_t _gclab_allocs;\n-  size_t _plab_allocs;\n+  size_t volatile _tlab_allocs;\n+  size_t volatile _gclab_allocs;\n+  size_t volatile _plab_allocs;\n@@ -264,2 +264,2 @@\n-  uint _age;\n-  CENSUS_NOISE(uint _youth;)   \/\/ tracks epochs of retrograde ageing (rejuvenation)\n+  volatile uint _age;\n+  CENSUS_NOISE(volatile uint _youth;)   \/\/ tracks epochs of retrograde ageing (rejuvenation)\n@@ -271,0 +271,14 @@\n+  ShenandoahSharedFlag _direct_alloc_reserved; \/\/ Flag to indicate that whether the region is reserved for lock-free direct allocation\n+  volatile int _direct_alloc_mutators = 0;\n+\n+  class DirectAllocMutatorCounter {\n+  private:\n+    int volatile* _counter;\n+  public:\n+    DirectAllocMutatorCounter(int volatile* counter): _counter(counter) {\n+      AtomicAccess::inc(_counter);\n+    }\n+    ~DirectAllocMutatorCounter() {\n+      AtomicAccess::dec(_counter);\n+    }\n+  };\n@@ -372,0 +386,9 @@\n+  inline HeapWord* allocate_lab(const ShenandoahAllocRequest &req, size_t &actual_size);\n+\n+  \/\/ Atomic allocation using CAS, return nullptr if full or no enough space for the req\n+  inline HeapWord* allocate_atomic(size_t word_size, const ShenandoahAllocRequest &req);\n+\n+  inline HeapWord* allocate_lab_atomic(const ShenandoahAllocRequest &req, size_t &actual_size);\n+\n+  inline bool try_allocate(HeapWord* const obj, size_t const size);\n+\n@@ -431,2 +454,6 @@\n-  HeapWord* top() const         { return _top;     }\n-  void set_top(HeapWord* v)     { _top = v;        }\n+  HeapWord* top() const {\n+    return AtomicAccess::load(&_top);\n+  }\n+  void set_top(HeapWord* v) {\n+    AtomicAccess::store(&_top, v);\n+  }\n@@ -456,0 +483,1 @@\n+  bool has_allocs() const;\n@@ -467,2 +495,2 @@\n-  uint age() const { return _age; }\n-  CENSUS_NOISE(uint youth() const { return _youth; })\n+  uint age() const { return AtomicAccess::load(&_age); }\n+  CENSUS_NOISE(uint youth() const { return AtomicAccess::load(&_youth); })\n@@ -471,4 +499,5 @@\n-    const uint max_age = markWord::max_age;\n-    assert(_age <= max_age, \"Error\");\n-    if (_age++ >= max_age) {\n-      _age = max_age;   \/\/ clamp\n+    const uint current_age = age();\n+    assert(current_age <= markWord::max_age, \"Error\");\n+    if (current_age < markWord::max_age) {\n+      const uint old = AtomicAccess::cmpxchg(&_age, current_age, current_age + 1);\n+      assert(old == current_age || old == 0u, \"Only fail when any mutator reset the age.\");\n@@ -479,2 +508,8 @@\n-    CENSUS_NOISE(_youth += _age;)\n-    _age = 0;\n+    uint current = age();\n+    uint old;\n+    while ((old = current) != 0u &&\n+           (current = AtomicAccess::cmpxchg(&_age, old, 0u)) != old &&\n+           current != 0u) { }\n+    if (current != 0u) {\n+      CENSUS_NOISE(AtomicAccess::add(&_youth, current, memory_order_relaxed);)\n+    }\n@@ -483,1 +518,1 @@\n-  CENSUS_NOISE(void clear_youth() { _youth = 0; })\n+  CENSUS_NOISE(void clear_youth() { AtomicAccess::store(&_youth,  0u); })\n@@ -497,0 +532,18 @@\n+  inline void reserve_for_direct_allocation() {\n+    assert(_direct_alloc_reserved.is_unset(), \"Must be\");\n+    _direct_alloc_reserved.set();\n+  }\n+\n+  inline void release_from_direct_allocation() {\n+    assert(_direct_alloc_reserved.is_set(), \"Must be\");\n+    _direct_alloc_reserved.unset();\n+  }\n+\n+  inline bool reserved_for_direct_allocation() const {\n+    return _direct_alloc_reserved.is_set();\n+  }\n+\n+  inline int direct_alloc_mutators() const {\n+    return AtomicAccess::load(&_direct_alloc_mutators);\n+  }\n+\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeapRegion.hpp","additions":70,"deletions":17,"binary":false,"changes":87,"status":"modified"},{"patch":"@@ -128,0 +128,83 @@\n+HeapWord* ShenandoahHeapRegion::allocate_lab(const ShenandoahAllocRequest& req, size_t &actual_size) {\n+  shenandoah_assert_heaplocked_or_safepoint();\n+  assert(req.is_lab_alloc(), \"Only lab alloc\");\n+  assert(this->affiliation() == req.affiliation(), \"Region affiliation should already be established\");\n+\n+  size_t adjusted_size = req.size();\n+  HeapWord* obj = nullptr;\n+  HeapWord* old_top = top();\n+  size_t free_words = align_down(byte_size(old_top, end()) >> LogHeapWordSize, MinObjAlignment);\n+  if (adjusted_size > free_words) {\n+    adjusted_size = free_words;\n+  }\n+  if (adjusted_size >= req.min_size()) {\n+    obj = allocate(adjusted_size, req);\n+    actual_size = adjusted_size;\n+    assert(obj == old_top, \"Must be\");\n+  }\n+  return obj;\n+}\n+\n+HeapWord* ShenandoahHeapRegion::allocate_atomic(size_t size, const ShenandoahAllocRequest& req) {\n+  DirectAllocMutatorCounter c(&_direct_alloc_mutators);\n+  assert(is_object_aligned(size), \"alloc size breaks alignment: %zu\", size);\n+  assert(this->affiliation() == req.affiliation(), \"Region affiliation should already be established\");\n+  assert(this->is_regular() || this->is_regular_pinned(), \"must be a regular region\");\n+\n+  for (;;) {\n+    if (!reserved_for_direct_allocation()) {\n+      return nullptr;\n+    }\n+    HeapWord* obj = top();\n+    if (pointer_delta(end(), obj) >= size) {\n+      if (try_allocate(obj, size)) {\n+        reset_age();\n+        adjust_alloc_metadata(req.type(), size);\n+        return obj;\n+      }\n+    } else {\n+      return nullptr;\n+    }\n+  }\n+}\n+\n+HeapWord* ShenandoahHeapRegion::allocate_lab_atomic(const ShenandoahAllocRequest& req, size_t &actual_size) {\n+  DirectAllocMutatorCounter c(&_direct_alloc_mutators);\n+  assert(req.is_lab_alloc(), \"Only lab alloc\");\n+  assert(this->affiliation() == req.affiliation(), \"Region affiliation should already be established\");\n+  assert(this->is_regular() || this->is_regular_pinned(), \"must be a regular region\");\n+  size_t adjusted_size = req.size();\n+  for (;;) {\n+    if (!reserved_for_direct_allocation()) {\n+      return nullptr;\n+    }\n+    HeapWord* obj = top();\n+    size_t free_words = align_down(byte_size(obj, end()) >> LogHeapWordSize, MinObjAlignment);\n+    if (adjusted_size > free_words) {\n+      adjusted_size = free_words;\n+    }\n+    if (adjusted_size >= req.min_size()) {\n+      if (try_allocate(obj, adjusted_size)) {\n+        reset_age();\n+        actual_size = adjusted_size;\n+        adjust_alloc_metadata(req.type(), adjusted_size);\n+        return obj;\n+      }\n+    } else {\n+      log_trace(gc, free)(\"Failed to shrink TLAB or GCLAB request (%zu) in region %zu to %zu\"\n+                          \" because min_size() is %zu\", req.size(), index(), adjusted_size, req.min_size());\n+      return nullptr;\n+    }\n+  }\n+}\n+\n+bool ShenandoahHeapRegion::try_allocate(HeapWord* const obj, size_t const size) {\n+  HeapWord* new_top = obj + size;\n+  if (AtomicAccess::cmpxchg(&_top, obj, new_top) == obj) {\n+    assert(is_object_aligned(new_top), \"new top breaks alignment: \" PTR_FORMAT, p2i(new_top));\n+    assert(is_object_aligned(obj),     \"obj is not aligned: \"       PTR_FORMAT, p2i(obj));\n+    return true;\n+  }\n+  return false;\n+}\n+\n@@ -136,1 +219,1 @@\n-      _tlab_allocs += size;\n+      AtomicAccess::add(&_tlab_allocs, size);\n@@ -139,1 +222,1 @@\n-      _gclab_allocs += size;\n+      AtomicAccess::add(&_gclab_allocs, size);\n@@ -142,1 +225,1 @@\n-      _plab_allocs += size;\n+      AtomicAccess::add(&_plab_allocs, size);\n@@ -236,1 +319,1 @@\n-  _top_before_promoted = _top;\n+  _top_before_promoted = top();\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeapRegion.inline.hpp","additions":87,"deletions":4,"binary":false,"changes":91,"status":"modified"},{"patch":"@@ -380,2 +380,2 @@\n-    if (r->is_cset() || r->is_trash()) {\n-      \/\/ Count the entire cset or trashed (formerly cset) region as used\n+    if (r->is_cset() || r->is_trash() || r->reserved_for_direct_allocation()) {\n+      \/\/ Count the entire cset, trashed (formerly cset) or alloc reserved region as used\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahVerifier.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -552,0 +552,6 @@\n+                                                                            \\\n+  product(uintx, ShenandoahDirectlyAllocatableRegionCount, 13, EXPERIMENTAL,\\\n+         \"Number of regions Shenandoah will pre-allocate for \"              \\\n+         \"direct allocation with CAS, the values should less than \"         \\\n+         \"number of CPU cores. Ideally it should be a prime number.\")       \\\n+         range(1, 128)                                                      \\\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoah_globals.hpp","additions":6,"deletions":0,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -44,1 +44,1 @@\n-  nonstatic_field(ShenandoahHeapRegion, _top,                      HeapWord*)                         \\\n+  volatile_nonstatic_field(ShenandoahHeapRegion, _top,             HeapWord*)                         \\\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/vmStructs_shenandoah.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"}]}