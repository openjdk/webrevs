{"files":[{"patch":"@@ -40,0 +40,4 @@\n+size_t PLAB::min_size_bytes() {\n+  return min_size() * HeapWordSize;\n+}\n+\n","filename":"src\/hotspot\/share\/gc\/shared\/plab.cpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -81,0 +81,2 @@\n+  \/\/ Minimum PLAB size in bytes.\n+  static size_t min_size_bytes();\n","filename":"src\/hotspot\/share\/gc\/shared\/plab.hpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -80,0 +80,1 @@\n+    assert(!region->is_active_alloc_region(), \"There should be no active alloc regions when choosing collection set\");\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/heuristics\/shenandoahGenerationalHeuristics.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -102,0 +102,1 @@\n+    assert(!region->is_active_alloc_region(), \"There should be no active alloc regions when rebuilding free set\");\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/heuristics\/shenandoahHeuristics.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -28,0 +28,2 @@\n+#include \"utilities\/globalDefinitions.hpp\"\n+\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahAffiliation.hpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -0,0 +1,503 @@\n+\/*\n+* Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n+ * Copyright (c) 2025, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+#include \"gc\/shenandoah\/shenandoahAllocator.hpp\"\n+#include \"gc\/shenandoah\/shenandoahAllocRequest.hpp\"\n+#include \"gc\/shenandoah\/shenandoahFreeSet.hpp\"\n+#include \"gc\/shenandoah\/shenandoahHeap.hpp\"\n+#include \"gc\/shenandoah\/shenandoahHeapRegion.hpp\"\n+#include \"memory\/padded.inline.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n+#include \"runtime\/interfaceSupport.inline.hpp\"\n+#include \"utilities\/globalDefinitions.hpp\"\n+\n+template <ShenandoahFreeSetPartitionId ALLOC_PARTITION>\n+ShenandoahAllocator<ALLOC_PARTITION>::ShenandoahAllocator(uint const alloc_region_count, ShenandoahFreeSet* free_set):\n+  _alloc_region_count(alloc_region_count), _free_set(free_set), _alloc_partition_name(ShenandoahRegionPartitions::partition_name(ALLOC_PARTITION)) {\n+  if (alloc_region_count > 0) {\n+    _alloc_regions = PaddedArray<ShenandoahAllocRegion, mtGC>::create_unfreeable(alloc_region_count);\n+    for (uint i = 0; i < alloc_region_count; i++) {\n+      _alloc_regions[i].address = nullptr;\n+      _alloc_regions[i].alloc_region_index = i;\n+    }\n+  }\n+}\n+\n+class ShenandoahHeapAccountingUpdater : StackObj {\n+public:\n+  ShenandoahFreeSet* _free_set;\n+  ShenandoahFreeSetPartitionId _partition;\n+  bool _need_update = false;\n+\n+  ShenandoahHeapAccountingUpdater(ShenandoahFreeSet* free_set, ShenandoahFreeSetPartitionId partition): _free_set(free_set), _partition(partition) { }\n+\n+  ~ShenandoahHeapAccountingUpdater() {\n+    if (_need_update) {\n+      switch (_partition) {\n+        case ShenandoahFreeSetPartitionId::Mutator:\n+          _free_set->recompute_total_used<\/* UsedByMutatorChanged *\/ true,\n+                                          \/* UsedByCollectorChanged *\/ true, \/* UsedByOldCollectorChanged *\/ true>();\n+          _free_set->recompute_total_affiliated<\/* MutatorEmptiesChanged *\/ true, \/* CollectorEmptiesChanged *\/ false,\n+                                                \/* OldCollectorEmptiesChanged *\/ false, \/* MutatorSizeChanged *\/ false,\n+                                                \/* CollectorSizeChanged *\/ false, \/* OldCollectorSizeChanged *\/ false,\n+                                                \/* AffiliatedChangesAreYoungNeutral *\/ false, \/* AffiliatedChangesAreGlobalNeutral *\/ false,\n+                                                \/* UnaffiliatedChangesAreYoungNeutral *\/ false>();\n+\n+          break;\n+        case ShenandoahFreeSetPartitionId::Collector:\n+          _free_set->recompute_total_used<\/* UsedByMutatorChanged *\/ true,\n+                                          \/* UsedByCollectorChanged *\/ true, \/* UsedByOldCollectorChanged *\/ true>();\n+          _free_set->recompute_total_affiliated<\/* MutatorEmptiesChanged *\/ true, \/* CollectorEmptiesChanged *\/ true,\n+                                                \/* OldCollectorEmptiesChanged *\/ false, \/* MutatorSizeChanged *\/ true,\n+                                                \/* CollectorSizeChanged *\/ true, \/* OldCollectorSizeChanged *\/ false,\n+                                                \/* AffiliatedChangesAreYoungNeutral *\/ false, \/* AffiliatedChangesAreGlobalNeutral *\/ false,\n+                                                \/* UnaffiliatedChangesAreYoungNeutral *\/ false>();\n+          break;\n+        case ShenandoahFreeSetPartitionId::OldCollector:\n+          _free_set->recompute_total_used<\/* UsedByMutatorChanged *\/ true,\n+                                          \/* UsedByCollectorChanged *\/ true, \/* UsedByOldCollectorChanged *\/ true>();\n+          _free_set->recompute_total_affiliated<\/* MutatorEmptiesChanged *\/ true, \/* CollectorEmptiesChanged *\/ false,\n+                                                \/* OldCollectorEmptiesChanged *\/ true, \/* MutatorSizeChanged *\/ true,\n+                                                \/* CollectorSizeChanged *\/ false, \/* OldCollectorSizeChanged *\/ true,\n+                                                \/* AffiliatedChangesAreYoungNeutral *\/ false, \/* AffiliatedChangesAreGlobalNeutral *\/ false,\n+                                                \/* UnaffiliatedChangesAreYoungNeutral *\/ false>();\n+          break;\n+        case ShenandoahFreeSetPartitionId::NotFree:\n+        default:\n+          assert(false, \"won't happen\");\n+      }\n+    }\n+  }\n+};\n+\n+template <ShenandoahFreeSetPartitionId ALLOC_PARTITION>\n+uint ShenandoahAllocator<ALLOC_PARTITION>::alloc_start_index() {\n+  uint alloc_start_index = 0u;\n+  switch (ALLOC_PARTITION) {\n+    case ShenandoahFreeSetPartitionId::Mutator:\n+      alloc_start_index = ShenandoahThreadLocalData::mutator_allocator_start_index();\n+      break;\n+    case ShenandoahFreeSetPartitionId::Collector:\n+      alloc_start_index = ShenandoahThreadLocalData::collector_allocator_start_index();\n+      break;\n+    default:\n+      break;\n+  }\n+  if (alloc_start_index == UINT_MAX) {\n+    if (_alloc_region_count <= 1u) {\n+      alloc_start_index = 0u;\n+    } else {\n+      if (ALLOC_PARTITION == ShenandoahFreeSetPartitionId::Mutator) {\n+        alloc_start_index = abs(os::random()) % _alloc_region_count;\n+      } else {\n+        alloc_start_index = (Thread::current()->is_Worker_thread() ? WorkerThread::worker_id() : abs(os::random())) % _alloc_region_count;\n+      }\n+    }\n+    switch (ALLOC_PARTITION) {\n+      case ShenandoahFreeSetPartitionId::Mutator:\n+        ShenandoahThreadLocalData::set_mutator_allocator_start_index(alloc_start_index);\n+        break;\n+      case ShenandoahFreeSetPartitionId::Collector:\n+        ShenandoahThreadLocalData::set_collector_allocator_start_index(alloc_start_index);\n+        break;\n+      default:\n+        break;\n+    }\n+  }\n+  return alloc_start_index;\n+}\n+\n+template <ShenandoahFreeSetPartitionId ALLOC_PARTITION>\n+HeapWord* ShenandoahAllocator<ALLOC_PARTITION>::attempt_allocation(ShenandoahAllocRequest& req, bool& in_new_region) {\n+  \/\/ Always allocate under heap lock held when shared alloc region count is set to 0\n+  if (_alloc_region_count == 0u) {\n+    ShenandoahHeapLocker locker(ShenandoahHeap::heap()->lock(), _yield_to_safepoint);\n+    ShenandoahHeapAccountingUpdater accounting_updater(_free_set, ALLOC_PARTITION);\n+    HeapWord* obj = attempt_allocation_from_free_set(req, in_new_region);\n+    if (obj != nullptr) {\n+      accounting_updater._need_update = true;\n+    }\n+    return obj;\n+  }\n+\n+  uint regions_ready_for_refresh = 0u;\n+  uint32_t old_epoch_id = AtomicAccess::load(&_epoch_id);\n+  \/\/ Fast path: start the attempt to allocate in alloc regions right away\n+  HeapWord* obj = attempt_allocation_in_alloc_regions(req, in_new_region, alloc_start_index(), regions_ready_for_refresh);\n+  if (obj != nullptr && regions_ready_for_refresh < _alloc_region_count \/ 2) {\n+    return obj;\n+  }\n+  if (obj == nullptr) {\n+    \/\/ Slow path under heap lock\n+    obj = attempt_allocation_slow(req, in_new_region, regions_ready_for_refresh, old_epoch_id);\n+  } else {\n+    \/\/ Eagerly refresh alloc region if there are 1\/2 or more of alloc regions ready for retire\n+    ShenandoahHeapLocker locker(ShenandoahHeap::heap()->lock(), _yield_to_safepoint);\n+    if (_epoch_id == old_epoch_id) {\n+      refresh_alloc_regions(nullptr, nullptr, nullptr);\n+    }\n+  }\n+  return obj;\n+}\n+\n+template <ShenandoahFreeSetPartitionId ALLOC_PARTITION>\n+HeapWord* ShenandoahAllocator<ALLOC_PARTITION>::attempt_allocation_slow(ShenandoahAllocRequest& req, bool& in_new_region, uint regions_ready_for_refresh, uint32_t old_epoch_id) {\n+  ShenandoahHeapLocker locker(ShenandoahHeap::heap()->lock(), _yield_to_safepoint);\n+  HeapWord* obj = nullptr;\n+  if (old_epoch_id != _epoch_id) {\n+    \/\/ After taking heap lock, attempt to allocate in shared alloc regions again\n+    \/\/ if alloc regions have been refreshed by other thread while current thread waits to take heap lock.\n+    regions_ready_for_refresh = 0u; \/\/reset regions_ready_for_refresh to 0.\n+    obj = attempt_allocation_in_alloc_regions<true \/*holding heap lock*\/>(req, in_new_region, alloc_start_index(), regions_ready_for_refresh);\n+    if (obj != nullptr) {\n+      return obj;\n+    }\n+  }\n+\n+  ShenandoahHeapAccountingUpdater accounting_updater(_free_set, ALLOC_PARTITION);\n+\n+  if (regions_ready_for_refresh > 0u) {\n+    int refreshed = refresh_alloc_regions(&req, &in_new_region, &obj);\n+    if (refreshed > 0 || obj != nullptr) {\n+      accounting_updater._need_update = true;\n+    }\n+    if (obj != nullptr) {\n+      return obj;\n+    }\n+  }\n+\n+  obj = attempt_allocation_from_free_set(req, in_new_region);\n+  if (obj != nullptr) {\n+    accounting_updater._need_update = true;\n+    return obj;\n+  }\n+\n+  log_debug(gc, alloc)(\"%sAllocator: Failed to allocate satisfy the alloc request, reqeust size: %lu\",\n+    _alloc_partition_name, req.size());\n+  return nullptr;\n+}\n+\n+template <ShenandoahFreeSetPartitionId ALLOC_PARTITION>\n+HeapWord* ShenandoahAllocator<ALLOC_PARTITION>::attempt_allocation_from_free_set(ShenandoahAllocRequest& req, bool& in_new_region) {\n+  HeapWord* obj;\n+  size_t min_free_words = req.is_lab_alloc() ? req.min_size() : req.size();\n+  ShenandoahHeapRegion* r = _free_set->find_heap_region_for_allocation(ALLOC_PARTITION, min_free_words, req.is_lab_alloc(), in_new_region);\n+  \/\/ The region returned by find_heap_region_for_allocation must have sufficient free space for the allocation if it is not nullptr\n+  if (r != nullptr) {\n+    bool ready_for_retire = false;\n+    obj = allocate_in<false>(r, false, req, in_new_region, ready_for_retire);\n+    assert(obj != nullptr, \"Should always succeed.\");\n+\n+    _free_set->partitions()->increase_used(ALLOC_PARTITION, (req.actual_size() + req.waste()) * HeapWordSize);\n+    if (ALLOC_PARTITION == ShenandoahFreeSetPartitionId::Mutator) {\n+      _free_set->increase_bytes_allocated(req.actual_size() * HeapWordSize);\n+    }\n+\n+    if (ready_for_retire) {\n+      assert(r->free_words() < PLAB::min_size(), \"Must be\");\n+      size_t waste_bytes = _free_set->partitions()->retire_from_partition(ALLOC_PARTITION, r->index(), r->used());\n+      if (ALLOC_PARTITION == ShenandoahFreeSetPartitionId::Mutator && (waste_bytes > 0)) {\n+        _free_set->increase_bytes_allocated(waste_bytes);\n+      }\n+    }\n+    return obj;\n+  }\n+  log_debug(gc, alloc)(\"%sAllocator: Didn't find one region with at least %lu free words to satisfy the alloc request, request size: %lu\",\n+                       _alloc_partition_name, min_free_words, req.size());\n+  return nullptr;\n+}\n+\n+template <ShenandoahFreeSetPartitionId ALLOC_PARTITION>\n+template<bool HOLDING_HEAP_LOCK>\n+HeapWord* ShenandoahAllocator<ALLOC_PARTITION>::attempt_allocation_in_alloc_regions(ShenandoahAllocRequest &req,\n+                                                                                    bool &in_new_region,\n+                                                                                    uint const alloc_start_index,\n+                                                                                    uint &regions_ready_for_refresh) {\n+  assert(regions_ready_for_refresh == 0u && in_new_region == false && alloc_start_index < _alloc_region_count, \"Sanity check\");\n+  uint i = alloc_start_index;\n+  do {\n+    if (ShenandoahHeapRegion* r = nullptr; (r = HOLDING_HEAP_LOCK ? _alloc_regions[i].address : AtomicAccess::load(&_alloc_regions[i].address)) != nullptr) {\n+      bool ready_for_retire = false;\n+      HeapWord* obj = allocate_in<true>(r, true, req, in_new_region, ready_for_retire);\n+      if (ready_for_retire) {\n+        regions_ready_for_refresh++;\n+      }\n+      if (obj != nullptr) {\n+        return obj;\n+      }\n+    } else {\n+      \/\/ Empty shared alloc region slot is always ready for refresh\n+      regions_ready_for_refresh++;\n+    }\n+    if (++i == _alloc_region_count) {\n+      i = 0u;\n+    }\n+  } while (i != alloc_start_index);\n+\n+  return nullptr;\n+}\n+\n+template <ShenandoahFreeSetPartitionId ALLOC_PARTITION>\n+template <bool ATOMIC>\n+HeapWord* ShenandoahAllocator<ALLOC_PARTITION>::allocate_in(ShenandoahHeapRegion* region, bool const is_alloc_region, ShenandoahAllocRequest &req, bool &in_new_region, bool &ready_for_retire) {\n+  assert(ready_for_retire == false, \"Sanity check\");\n+  if (!ATOMIC) {\n+    shenandoah_assert_heaplocked();\n+  }\n+  HeapWord* obj = nullptr;\n+  size_t actual_size = req.size();\n+  if (req.is_lab_alloc()) {\n+    obj = ATOMIC ? region->allocate_lab_atomic(req, actual_size, ready_for_retire) : region->allocate_lab(req, actual_size);\n+  } else {\n+    obj = ATOMIC ? region->allocate_atomic(actual_size, req, ready_for_retire) : region->allocate(req.size(), req);\n+  }\n+  if (obj != nullptr) {\n+    assert(actual_size > 0, \"Must be\");\n+    log_debug(gc, alloc)(\"%sAllocator: Allocated %lu bytes from heap region %lu, request size: %lu, alloc region: %s, remnant: %lu\",\n+      _alloc_partition_name, actual_size * HeapWordSize, region->index(), req.size() * HeapWordSize, is_alloc_region ? \"true\" : \"false\", region->free());\n+    req.set_actual_size(actual_size);\n+    in_new_region = obj == region->bottom(); \/\/ is in new region when the allocated object is at the bottom of the region.\n+    if (req.is_gc_alloc()) {\n+      \/\/ For GC allocations, we advance update_watermark because the objects relocated into this memory during\n+      \/\/ evacuation are not updated during evacuation.  For both young and old regions r, it is essential that all\n+      \/\/ PLABs be made parsable at the end of evacuation.  This is enabled by retiring all plabs at end of evacuation.\n+      region->concurrent_set_update_watermark(region->top());\n+    }\n+\n+    if (!ATOMIC && region->free_words() < PLAB::min_size()) {\n+      ready_for_retire = true;\n+    }\n+  }\n+  return obj;\n+}\n+\n+template <ShenandoahFreeSetPartitionId ALLOC_PARTITION>\n+int ShenandoahAllocator<ALLOC_PARTITION>::refresh_alloc_regions(ShenandoahAllocRequest* req, bool* in_new_region, HeapWord** obj) {\n+  ResourceMark rm;\n+  shenandoah_assert_heaplocked();\n+  bool satisfy_alloc_req_first = (req != nullptr && obj != nullptr && *obj == nullptr);\n+  size_t min_req_size = 0;\n+  if (satisfy_alloc_req_first) {\n+    assert(in_new_region != nullptr && *in_new_region == false, \"Sanity check\");\n+    min_req_size = req->is_lab_alloc() ? req->min_size() : req->size();\n+  }\n+\n+  int refreshable_alloc_regions = 0;\n+  ShenandoahAllocRegion* refreshable[MAX_ALLOC_REGION_COUNT];\n+  \/\/ Step 1: find out the alloc regions which are ready to refresh.\n+  for (uint i = 0; i < _alloc_region_count; i++) {\n+    ShenandoahAllocRegion* alloc_region = &_alloc_regions[i];\n+    ShenandoahHeapRegion* region = alloc_region->address;\n+    const size_t free_bytes = region == nullptr ? 0 : region->free();\n+    if (region == nullptr || free_bytes \/ HeapWordSize < PLAB::min_size()) {\n+      if (region != nullptr) {\n+        region->unset_active_alloc_region();\n+        log_debug(gc, alloc)(\"%sAllocator: Removing heap region %li from alloc region %i.\",\n+          _alloc_partition_name, region->index(), alloc_region->alloc_region_index);\n+        AtomicAccess::store(&alloc_region->address, static_cast<ShenandoahHeapRegion*>(nullptr));\n+      }\n+      log_debug(gc, alloc)(\"%sAllocator: Adding alloc region %i to refreshable.\",\n+        _alloc_partition_name, alloc_region->alloc_region_index);\n+      refreshable[refreshable_alloc_regions++] = alloc_region;\n+    }\n+  }\n+\n+  if (refreshable_alloc_regions > 0) {\n+    \/\/ Step 2: allocate region from FreeSets to fill the alloc regions or satisfy the alloc request.\n+    ShenandoahHeapRegion* reserved[MAX_ALLOC_REGION_COUNT];\n+    int reserved_regions = _free_set->reserve_alloc_regions(ALLOC_PARTITION, refreshable_alloc_regions, PLAB::min_size(), reserved);\n+    assert(reserved_regions <= refreshable_alloc_regions, \"Sanity check\");\n+    log_debug(gc, alloc)(\"%sAllocator: Reserved %i regions for allocation.\", _alloc_partition_name, reserved_regions);\n+\n+    ShenandoahAffiliation affiliation = ALLOC_PARTITION == ShenandoahFreeSetPartitionId::OldCollector ? OLD_GENERATION : YOUNG_GENERATION;\n+    \/\/ Step 3: Install the new reserved alloc regions\n+    if (reserved_regions > 0) {\n+      int refreshed_regions = 0;\n+      for (int i = 0; i < reserved_regions; i++) {\n+        assert(reserved[i]->affiliation() == affiliation, \"Affiliation of reserved region must match, invalid affiliation: %s\", shenandoah_affiliation_name(reserved[i]->affiliation()));\n+        assert(_free_set->membership(reserved[i]->index()) == ShenandoahFreeSetPartitionId::NotFree, \"Reserved heap region must have been retired from free set.\");\n+        if (satisfy_alloc_req_first && reserved[i]->free_words() >= min_req_size) {\n+          bool ready_for_retire = false;\n+          *obj = allocate_in<false>(reserved[i], true, *req, *in_new_region, ready_for_retire);\n+          assert(*obj != nullptr, \"Should always succeed\");\n+          satisfy_alloc_req_first = false;\n+          if (ready_for_retire) {\n+            log_debug(gc, alloc)(\"%sAllocator: heap region %li has no space left after satisfying alloc req.\",\n+              _alloc_partition_name, reserved[i]->index());\n+            reserved[i]->unset_active_alloc_region();\n+            continue;\n+          }\n+        }\n+        log_debug(gc, alloc)(\"%sAllocator: Storing heap region %li to alloc region %i\",\n+          _alloc_partition_name, reserved[i]->index(), refreshable[i]->alloc_region_index);\n+        AtomicAccess::store(&refreshable[i]->address, reserved[i]);\n+        refreshed_regions++;\n+      }\n+\n+      if (refreshed_regions > 0) {\n+        \/\/ Increase _epoch_id by 1 when any of alloc regions has been refreshed.\n+        AtomicAccess::inc(&_epoch_id);\n+      }\n+    }\n+    return reserved_regions;\n+  }\n+\n+  return 0;\n+}\n+\n+template <ShenandoahFreeSetPartitionId ALLOC_PARTITION>\n+HeapWord* ShenandoahAllocator<ALLOC_PARTITION>::allocate(ShenandoahAllocRequest &req, bool &in_new_region) {\n+#ifdef ASSERT\n+  verify(req);\n+#endif \/\/ ASSERT\n+  if (ShenandoahHeapRegion::requires_humongous(req.size())) {\n+    in_new_region = true;\n+    ShenandoahHeapLocker locker(ShenandoahHeap::heap()->lock(), _yield_to_safepoint);\n+    return _free_set->allocate_contiguous(req, req.type() != ShenandoahAllocRequest::_alloc_cds \/*is_humongous*\/);\n+  }\n+  return attempt_allocation(req, in_new_region);\n+}\n+\n+template <ShenandoahFreeSetPartitionId ALLOC_PARTITION>\n+void ShenandoahAllocator<ALLOC_PARTITION>::release_alloc_regions() {\n+  assert_at_safepoint();\n+  shenandoah_assert_heaplocked();\n+  if (_alloc_region_count == 0u) {\n+    \/\/ no-op if _alloc_region_count is 0\n+    return;\n+  }\n+\n+  log_debug(gc, alloc)(\"%sAllocator: Releasing all alloc regions\", _alloc_partition_name);\n+  ShenandoahHeapAccountingUpdater accounting_updater(_free_set, ALLOC_PARTITION);\n+  size_t total_free_bytes = 0;\n+  size_t total_regions_to_unretire = 0;\n+\n+  for (uint i = 0; i < _alloc_region_count; i++) {\n+    ShenandoahAllocRegion& alloc_region = _alloc_regions[i];\n+    ShenandoahHeapRegion* r = alloc_region.address;\n+    if (r != nullptr) {\n+      log_debug(gc, alloc)(\"%sAllocator: Releasing heap region %li from alloc region %i\",\n+        _alloc_partition_name, r->index(), i);\n+      r->unset_active_alloc_region();\n+      alloc_region.address = nullptr;\n+      size_t free_bytes = r->free();\n+      if (free_bytes >= PLAB::min_size_bytes()) {\n+        total_free_bytes += free_bytes;\n+        total_regions_to_unretire++;\n+        _free_set->partitions()->unretire_to_partition(r, ALLOC_PARTITION);\n+        if (!r->has_allocs()) {\n+          log_debug(gc, alloc)(\"%sAllocator: Reverting heap region %li to FREE due to no alloc in the region\",\n+            _alloc_partition_name, r->index());\n+          r->make_empty();\n+          r->set_affiliation(FREE);\n+          _free_set->partitions()->increase_empty_region_counts(ALLOC_PARTITION, 1);\n+        }\n+      }\n+    }\n+    assert(alloc_region.address == nullptr, \"Alloc region is set to nullptr after release\");\n+  }\n+  _free_set->partitions()->decrease_used(ALLOC_PARTITION, total_free_bytes);\n+  _free_set->partitions()->increase_region_counts(ALLOC_PARTITION, total_regions_to_unretire);\n+  accounting_updater._need_update = true;\n+}\n+\n+template <ShenandoahFreeSetPartitionId ALLOC_PARTITION>\n+void ShenandoahAllocator<ALLOC_PARTITION>::reserve_alloc_regions() {\n+  shenandoah_assert_heaplocked();\n+  if (_alloc_region_count == 0u) {\n+    \/\/ no-op if _alloc_region_count is 0\n+    return;\n+  }\n+  ShenandoahHeapAccountingUpdater accounting_updater(_free_set, ALLOC_PARTITION);\n+  if (refresh_alloc_regions() > 0) {\n+    accounting_updater._need_update = true;\n+  }\n+}\n+\n+ShenandoahMutatorAllocator::ShenandoahMutatorAllocator(ShenandoahFreeSet* free_set) :\n+  ShenandoahAllocator((uint) ShenandoahMutatorAllocRegions, free_set) {\n+  _yield_to_safepoint = true;\n+}\n+\n+ShenandoahCollectorAllocator::ShenandoahCollectorAllocator(ShenandoahFreeSet* free_set) :\n+  ShenandoahAllocator((uint) ShenandoahCollectorAllocRegions, free_set) {\n+  _yield_to_safepoint = false;\n+}\n+\n+ShenandoahOldCollectorAllocator::ShenandoahOldCollectorAllocator(ShenandoahFreeSet* free_set) :\n+  ShenandoahAllocator(0u, free_set) {\n+  _yield_to_safepoint = false;\n+}\n+\n+HeapWord* ShenandoahOldCollectorAllocator::allocate(ShenandoahAllocRequest& req, bool& in_new_region) {\n+  shenandoah_assert_not_heaplocked();\n+#ifdef ASSERT\n+  verify(req);\n+#endif \/\/ ASSERT\n+  ShenandoahHeapLocker locker(ShenandoahHeap::heap()->lock(), _yield_to_safepoint);\n+  \/\/ Make sure the old generation has room for either evacuations or promotions before trying to allocate.\n+  auto old_gen = ShenandoahHeap::heap()->old_generation();\n+  if (!old_gen->can_allocate(req)) {\n+    return nullptr;\n+  }\n+\n+  HeapWord* obj = _free_set->allocate_for_collector(req, in_new_region);\n+  \/\/ Record the plab configuration for this result and register the object.\n+  if (obj != nullptr) {\n+    if (req.is_lab_alloc()) {\n+      old_gen->configure_plab_for_current_thread(req);\n+    } else {\n+      \/\/ Register the newly allocated object while we're holding the global lock since there's no synchronization\n+      \/\/ built in to the implementation of register_object().  There are potential races when multiple independent\n+      \/\/ threads are allocating objects, some of which might span the same card region.  For example, consider\n+      \/\/ a card table's memory region within which three objects are being allocated by three different threads:\n+      \/\/\n+      \/\/ objects being \"concurrently\" allocated:\n+      \/\/    [-----a------][-----b-----][--------------c------------------]\n+      \/\/            [---- card table memory range --------------]\n+      \/\/\n+      \/\/ Before any objects are allocated, this card's memory range holds no objects.  Note that allocation of object a\n+      \/\/ wants to set the starts-object, first-start, and last-start attributes of the preceding card region.\n+      \/\/ Allocation of object b wants to set the starts-object, first-start, and last-start attributes of this card region.\n+      \/\/ Allocation of object c also wants to set the starts-object, first-start, and last-start attributes of this\n+      \/\/ card region.\n+      \/\/\n+      \/\/ The thread allocating b and the thread allocating c can \"race\" in various ways, resulting in confusion, such as\n+      \/\/ last-start representing object b while first-start represents object c.  This is why we need to require all\n+      \/\/ register_object() invocations to be \"mutually exclusive\" with respect to each card's memory range.\n+      old_gen->card_scan()->register_object(obj);\n+\n+      if (req.is_promotion()) {\n+        \/\/ Shared promotion.\n+        const size_t actual_size = req.actual_size() * HeapWordSize;\n+        log_debug(gc, plab)(\"Expend shared promotion of %zu bytes\", actual_size);\n+        old_gen->expend_promoted(actual_size);\n+      }\n+    }\n+\n+    return obj;\n+  }\n+\n+  return nullptr;\n+}\n\\ No newline at end of file\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahAllocator.cpp","additions":503,"deletions":0,"binary":false,"changes":503,"status":"added"},{"patch":"@@ -0,0 +1,162 @@\n+\/*\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n+ * Copyright (c) 2025, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef SHARE_GC_SHENANDOAH_SHENANDOAHALLOCATOR_HPP\n+#define SHARE_GC_SHENANDOAH_SHENANDOAHALLOCATOR_HPP\n+\n+#include \"gc\/shenandoah\/shenandoahAllocRequest.hpp\"\n+#include \"gc\/shenandoah\/shenandoahFreeSetPartitionId.hpp\"\n+#include \"gc\/shenandoah\/shenandoahPadding.hpp\"\n+#include \"gc\/shenandoah\/shenandoahThreadLocalData.hpp\"\n+#include \"memory\/allocation.hpp\"\n+#include \"memory\/padded.hpp\"\n+#include \"runtime\/thread.hpp\"\n+#include \"utilities\/globalDefinitions.hpp\"\n+\n+class ShenandoahFreeSet;\n+class ShenandoahRegionPartitions;\n+class ShenandoahHeapRegion;\n+\n+template <ShenandoahFreeSetPartitionId ALLOC_PARTITION>\n+class ShenandoahAllocator : public CHeapObj<mtGC> {\n+protected:\n+  struct ShenandoahAllocRegion {\n+    ShenandoahHeapRegion* volatile  address;\n+    int                             alloc_region_index;\n+  };\n+\n+  PaddedEnd<ShenandoahAllocRegion>*  _alloc_regions;\n+  uint  const                        _alloc_region_count;\n+  ShenandoahFreeSet* const           _free_set;\n+  const char*                        _alloc_partition_name;\n+  bool                               _yield_to_safepoint = false;\n+  shenandoah_padding(0);\n+  volatile uint32_t                  _epoch_id = 0u; \/\/ epoch id of _alloc_regions, increase by 1 whenever refresh _alloc_regions.\n+  shenandoah_padding(1);\n+\n+\n+  \/\/ Start index of the shared alloc regions where the allocation will start from.\n+  \/\/ The alloc start index is stored in thread local data, each thread has it own start index to reduce contention.\n+  \/\/ The value is determined when this function is called at the first time from a thread.\n+  \/\/ For GC workers, the started index is calculated using worker id as: WorkerThread::worker_id() % _alloc_region_count;\n+  \/\/ for non GC worker, the value is calculated with random like: abs(os::random()) % _alloc_region_count.\n+  uint alloc_start_index();\n+\n+  \/\/ Attempt to allocate memory to satisfy alloc request.\n+  \/\/ If _alloc_region_count is not 0, it will try to allocate in shared alloc regions first with atomic operations w\/o\n+  \/\/ the need of global heap lock(fast path); when fast path fails, it will call attempt_allocation_slow which takes\n+  \/\/ global heap lock and try to refresh shared alloc regions if they are not refreshed by other mutator thread.\n+  \/\/ If _alloc_region_count is 0, no shared alloc region will be reserved, allocation is always done with global heap lock held.\n+  HeapWord* attempt_allocation(ShenandoahAllocRequest& req, bool& in_new_region);\n+\n+  \/\/ Slow path of allocation attempt. When fast path trying to allocate in shared alloc regions fails attempt_allocation_slow will\n+  \/\/ be called to refresh shared alloc regions and allocate memory for the alloc request.\n+  HeapWord* attempt_allocation_slow(ShenandoahAllocRequest& req, bool& in_new_region, uint regions_ready_for_refresh, uint32_t old_epoch_id);\n+\n+  \/\/ Attempt to allocate from a region in free set, rather than from any of shared alloc regions, it might be called in the conditions below:\n+  \/\/   1. _alloc_region_count is explicitly set to 0 to disable CAS allocator;\n+  \/\/   2. all the shared alloc regions are not ready to retire, nor have enough space for allocation.\n+  \/\/ Caller has to hold heap lock.\n+  HeapWord* attempt_allocation_from_free_set(ShenandoahAllocRequest& req, bool& in_new_region);\n+\n+  \/\/ Attempt to allocate in a shared alloc region using atomic operation without holding the heap lock.\n+  \/\/ Returns nullptr and overwrites regions_ready_for_refresh with the number of shared alloc regions that are ready\n+  \/\/ to be retired if it is unable to satisfy the allocation request from the existing shared alloc regions.\n+  template<bool HOLDING_HEAP_LOCK = false>\n+  HeapWord* attempt_allocation_in_alloc_regions(ShenandoahAllocRequest& req, bool& in_new_region, uint const alloc_start_index, uint &regions_ready_for_refresh);\n+\n+  \/\/ Allocate in a region, use atomic operations if template parameter ATOMIC is true.\n+  \/\/ When template parameter ATOMIC is false, heap lock is required.\n+  template <bool ATOMIC>\n+  HeapWord* allocate_in(ShenandoahHeapRegion* region, bool is_alloc_region, ShenandoahAllocRequest &req, bool &in_new_region, bool &ready_for_retire);\n+\n+  \/\/ Refresh new alloc regions, allocate the object in the new alloc region before making the new alloc region visible to other mutators.\n+  int refresh_alloc_regions(ShenandoahAllocRequest* req = nullptr, bool* in_new_region = nullptr, HeapWord** obj = nullptr);\n+\n+#ifdef ASSERT\n+  void verify(ShenandoahAllocRequest& req) {\n+    switch (ALLOC_PARTITION) {\n+      case ShenandoahFreeSetPartitionId::Mutator:\n+        assert(req.is_mutator_alloc(), \"Must be mutator alloc request\");\n+        assert(Thread::current()->is_Java_thread(), \"Must be Java thread\");\n+        break;\n+      case ShenandoahFreeSetPartitionId::Collector:\n+        assert(req.is_gc_alloc() && req.affiliation() == YOUNG_GENERATION, \"Must be gc alloc request in young gen\");\n+        break;\n+      case ShenandoahFreeSetPartitionId::OldCollector:\n+        assert(req.is_gc_alloc() && req.affiliation() == OLD_GENERATION, \"Must be gc alloc request in old gen\");\n+        break;\n+      default:\n+        assert(false, \"Should not be here\");\n+    }\n+  }\n+#endif\n+\n+public:\n+  static constexpr uint             MAX_ALLOC_REGION_COUNT = 128;\n+\n+  ShenandoahAllocator(uint alloc_region_count, ShenandoahFreeSet* free_set);\n+  virtual ~ShenandoahAllocator() { }\n+\n+  \/\/ Handle the allocation request - it is the entry point of memory allocation, including humongous allocation:\n+  \/\/ 1. for humongous allocation, it delegates to function ShenandoahFreeSet::allocate_contiguous;\n+  \/\/ 2. for others allocations, it calls function attempt_allocation.\n+  \/\/ Caller does not hold the heap lock.\n+  virtual HeapWord* allocate(ShenandoahAllocRequest& req, bool& in_new_region);\n+\n+  \/\/ Caller must hold the heap lock at safepoint. This causes all directly allocatable regions to be placed into\n+  \/\/ the appropriate ShenandoahFreeSet partition.\n+  \/\/ Collector calls this in preparation for choosing a collection set and\/or rebuilding the freeset.\n+  virtual void release_alloc_regions();\n+\n+  \/\/ Caller must hold the heap lock at safepoint. This causes us to set aside N regions as directly allocatable\n+  \/\/ by removing these regions from the relevant ShenandoahFreeSet partitions.\n+  \/\/ Collector calls this after rebuilding the freeset.\n+  virtual void reserve_alloc_regions();\n+};\n+\n+\/*\n+ * Allocator impl for mutator\n+ *\/\n+class ShenandoahMutatorAllocator : public ShenandoahAllocator<ShenandoahFreeSetPartitionId::Mutator> {\n+public:\n+  ShenandoahMutatorAllocator(ShenandoahFreeSet* free_set);\n+};\n+\n+class ShenandoahCollectorAllocator : public ShenandoahAllocator<ShenandoahFreeSetPartitionId::Collector> {\n+public:\n+  ShenandoahCollectorAllocator(ShenandoahFreeSet* free_set);\n+};\n+\n+\/\/ Currently ShenandoahOldCollectorAllocator delegate allocation handling to ShenandoahFreeSet,\n+\/\/ because of the complexity in plab allocation where we have specialized logic to handle card table size alignment.\n+\/\/ We will make ShenandoahOldCollectorAllocator use compare-and-swap\/atomic operation later.\n+class ShenandoahOldCollectorAllocator : public ShenandoahAllocator<ShenandoahFreeSetPartitionId::OldCollector> {\n+public:\n+  ShenandoahOldCollectorAllocator(ShenandoahFreeSet* free_set);\n+  HeapWord* allocate(ShenandoahAllocRequest& req, bool& in_new_region) override;\n+};\n+\n+#endif \/\/SHARE_GC_SHENANDOAH_SHENANDOAHALLOCATOR_HPP\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahAllocator.hpp","additions":162,"deletions":0,"binary":false,"changes":162,"status":"added"},{"patch":"@@ -94,0 +94,1 @@\n+  assert(!r->is_active_alloc_region(), \"Active alloc region can't be added to collection set\");\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahCollectionSet.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -744,0 +744,3 @@\n+  \/\/ Release all alloc regions at the beginning of final mark.\n+  heap->free_set()->release_alloc_regions_under_lock();\n+\n@@ -794,0 +797,9 @@\n+\n+    {\n+      ShenandoahHeapLocker locker(heap->lock());\n+      if (heap->is_evacuation_in_progress()) {\n+        \/\/ Reserve alloc regions for evacuation.\n+        heap->free_set()->collector_allocator()->reserve_alloc_regions();\n+      }\n+      heap->free_set()->mutator_allocator()->reserve_alloc_regions();\n+    }\n@@ -1213,0 +1225,2 @@\n+  heap->free_set()->release_alloc_regions_under_lock();\n+\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahConcurrentGC.cpp","additions":14,"deletions":0,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -343,0 +343,3 @@\n+\n+  heap->free_set()->release_alloc_regions_under_lock();\n+\n@@ -383,0 +386,8 @@\n+\n+  {\n+    if (heap->is_evacuation_in_progress()) {\n+      \/\/ Reserve alloc regions for evacuation.\n+      ShenandoahHeapLocker locker(heap->lock());\n+      heap->free_set()->collector_allocator()->reserve_alloc_regions();\n+    }\n+  }\n@@ -430,0 +441,1 @@\n+  heap->free_set()->release_alloc_regions_under_lock();\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahDegeneratedGC.cpp","additions":12,"deletions":0,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -40,1 +40,1 @@\n-static const char* partition_name(ShenandoahFreeSetPartitionId t) {\n+const char* ShenandoahRegionPartitions::partition_name(ShenandoahFreeSetPartitionId t) {\n@@ -200,0 +200,3 @@\n+  recompute_total_young_used<\/* UsedByMutatorChanged *\/ true, \/*UsedByCollectorChanged *\/ true>();\n+  recompute_total_global_used<\/* UsedByMutatorChanged *\/ true, \/* UsedByCollectorChanged *\/ true,\n+                              \/* UsedByOldCollectorChanged *\/ false>();\n@@ -206,3 +209,0 @@\n-  recompute_total_young_used<\/* UsedByMutatorChanged *\/ true, \/*UsedByCollectorChanged *\/ true>();\n-  recompute_total_global_used<\/* UsedByMutatorChanged *\/ true, \/* UsedByCollectorChanged *\/ true,\n-                              \/* UsedByOldCollectorChanged *\/ false>();\n@@ -243,0 +243,9 @@\n+inline size_t ShenandoahFreeSet::alloc_capacity_words(ShenandoahHeapRegion *r) const {\n+  if (r->is_trash()) {\n+    \/\/ This would be recycled on allocation path\n+    return ShenandoahHeapRegion::region_size_words();\n+  } else {\n+    return r->free_words();\n+  }\n+}\n+\n@@ -405,19 +414,0 @@\n-void ShenandoahRegionPartitions::increase_used(ShenandoahFreeSetPartitionId which_partition, size_t bytes) {\n-  shenandoah_assert_heaplocked();\n-  assert (which_partition < NumPartitions, \"Partition must be valid\");\n-\n-  _used[int(which_partition)] += bytes;\n-  _available[int(which_partition)] -= bytes;\n-  assert (_used[int(which_partition)] <= _capacity[int(which_partition)],\n-          \"Must not use (%zu) more than capacity (%zu) after increase by %zu\",\n-          _used[int(which_partition)], _capacity[int(which_partition)], bytes);\n-}\n-\n-void ShenandoahRegionPartitions::decrease_used(ShenandoahFreeSetPartitionId which_partition, size_t bytes) {\n-  shenandoah_assert_heaplocked();\n-  assert (which_partition < NumPartitions, \"Partition must be valid\");\n-  assert (_used[int(which_partition)] >= bytes, \"Must not use less than zero after decrease\");\n-  _used[int(which_partition)] -= bytes;\n-  _available[int(which_partition)] += bytes;\n-}\n-\n@@ -442,8 +432,0 @@\n-\n-void ShenandoahRegionPartitions::increase_capacity(ShenandoahFreeSetPartitionId which_partition, size_t bytes) {\n-  shenandoah_assert_heaplocked();\n-  assert (which_partition < NumPartitions, \"Partition must be valid\");\n-  _capacity[int(which_partition)] += bytes;\n-  _available[int(which_partition)] += bytes;\n-}\n-\n@@ -493,18 +475,0 @@\n-void ShenandoahRegionPartitions::increase_region_counts(ShenandoahFreeSetPartitionId which_partition, size_t regions) {\n-  _region_counts[int(which_partition)] += regions;\n-}\n-\n-void ShenandoahRegionPartitions::decrease_region_counts(ShenandoahFreeSetPartitionId which_partition, size_t regions) {\n-  assert(_region_counts[int(which_partition)] >= regions, \"Cannot remove more regions than are present\");\n-  _region_counts[int(which_partition)] -= regions;\n-}\n-\n-void ShenandoahRegionPartitions::increase_empty_region_counts(ShenandoahFreeSetPartitionId which_partition, size_t regions) {\n-  _empty_region_counts[int(which_partition)] += regions;\n-}\n-\n-void ShenandoahRegionPartitions::decrease_empty_region_counts(ShenandoahFreeSetPartitionId which_partition, size_t regions) {\n-  assert(_empty_region_counts[int(which_partition)] >= regions, \"Cannot remove more regions than are present\");\n-  _empty_region_counts[int(which_partition)] -= regions;\n-}\n-\n@@ -659,1 +623,1 @@\n-  size_t waste_bytes = 0;\n+  size_t remnant_bytes = 0;\n@@ -667,3 +631,2 @@\n-    size_t fill_padding = _region_size_bytes - used_bytes;\n-    waste_bytes = fill_padding;\n-    increase_used(partition, fill_padding);\n+    remnant_bytes = _region_size_bytes - used_bytes;\n+    increase_used(partition, remnant_bytes);\n@@ -676,3 +639,2 @@\n-  \/\/ is retired and no more memory will be allocated from within it.\n-\n-  return waste_bytes;\n+  \/\/ is retired and no more memory will be allocated from within it\n+  return remnant_bytes;\n@@ -935,0 +897,1 @@\n+    ShenandoahHeapRegion* r = ShenandoahHeap::heap()->get_region(i);\n@@ -939,2 +902,1 @@\n-        assert(!validate_totals || (capacity != _region_size_bytes), \"Should not be retired if empty\");\n-        ShenandoahHeapRegion* r = ShenandoahHeap::heap()->get_region(i);\n+        assert(!validate_totals || r->is_active_alloc_region() || (capacity != _region_size_bytes), \"Should not be retired if empty\");\n@@ -948,1 +910,1 @@\n-            assert(r->is_young(), \"Must be young if not old\");\n+            assert(r->is_young(), \"Must be young if not old, region: %lu\", i);\n@@ -956,1 +918,1 @@\n-          assert(r->is_cset() || (capacity < PLAB::min_size() * HeapWordSize),\n+          assert(r->is_cset() || r->is_active_alloc_region() || (capacity < PLAB::min_size() * HeapWordSize),\n@@ -966,1 +928,1 @@\n-            assert(r->is_young(), \"Must be young if not old\");\n+            assert(r->is_young(), \"Must be young if not old, region: %lu\", i);\n@@ -979,1 +941,1 @@\n-        assert(capacity > 0, \"free regions must have allocation capacity\");\n+        assert(capacity > 0, \"free regions must have allocation capacity, region: %lu\", i);\n@@ -1207,0 +1169,3 @@\n+  _mutator_allocator = new ShenandoahMutatorAllocator(this);\n+  _collector_allocator = new ShenandoahCollectorAllocator(this);\n+  _old_collector_allocator = new ShenandoahOldCollectorAllocator(this);\n@@ -1296,46 +1261,0 @@\n-HeapWord* ShenandoahFreeSet::allocate_single(ShenandoahAllocRequest& req, bool& in_new_region) {\n-  shenandoah_assert_heaplocked();\n-\n-  \/\/ Scan the bitmap looking for a first fit.\n-  \/\/\n-  \/\/ Leftmost and rightmost bounds provide enough caching to walk bitmap efficiently. Normally,\n-  \/\/ we would find the region to allocate at right away.\n-  \/\/\n-  \/\/ Allocations are biased: GC allocations are taken from the high end of the heap.  Regular (and TLAB)\n-  \/\/ mutator allocations are taken from the middle of heap, below the memory reserved for Collector.\n-  \/\/ Humongous mutator allocations are taken from the bottom of the heap.\n-  \/\/\n-  \/\/ Free set maintains mutator and collector partitions.  Normally, each allocates only from its partition,\n-  \/\/ except in special cases when the collector steals regions from the mutator partition.\n-\n-  \/\/ Overwrite with non-zero (non-null) values only if necessary for allocation bookkeeping.\n-\n-  if (req.is_mutator_alloc()) {\n-    return allocate_for_mutator(req, in_new_region);\n-  } else {\n-    return allocate_for_collector(req, in_new_region);\n-  }\n-}\n-\n-HeapWord* ShenandoahFreeSet::allocate_for_mutator(ShenandoahAllocRequest &req, bool &in_new_region) {\n-  update_allocation_bias();\n-\n-  if (_partitions.is_empty(ShenandoahFreeSetPartitionId::Mutator)) {\n-    \/\/ There is no recovery. Mutator does not touch collector view at all.\n-    return nullptr;\n-  }\n-\n-  \/\/ Try to allocate in the mutator view\n-  if (_partitions.alloc_from_left_bias(ShenandoahFreeSetPartitionId::Mutator)) {\n-    \/\/ Allocate from low to high memory.  This keeps the range of fully empty regions more tightly packed.\n-    \/\/ Note that the most recently allocated regions tend not to be evacuated in a given GC cycle.  So this\n-    \/\/ tends to accumulate \"fragmented\" uncollected regions in high memory.\n-    ShenandoahLeftRightIterator iterator(&_partitions, ShenandoahFreeSetPartitionId::Mutator);\n-    return allocate_from_regions(iterator, req, in_new_region);\n-  }\n-\n-  \/\/ Allocate from high to low memory. This preserves low memory for humongous allocations.\n-  ShenandoahRightLeftIterator iterator(&_partitions, ShenandoahFreeSetPartitionId::Mutator);\n-  return allocate_from_regions(iterator, req, in_new_region);\n-}\n-\n@@ -1450,1 +1369,1 @@\n-size_t ShenandoahFreeSet::get_usable_free_words(size_t free_bytes) const {\n+size_t ShenandoahFreeSet::get_usable_free_words(size_t free_bytes) {\n@@ -1517,0 +1436,1 @@\n+\n@@ -1708,0 +1628,1 @@\n+      assert(!_heap->get_region(end)->is_active_alloc_region(), \"Must not\");\n@@ -1848,1 +1769,1 @@\n-bool ShenandoahFreeSet::transfer_one_region_from_mutator_to_old_collector(size_t idx, size_t alloc_capacity) {\n+bool ShenandoahFreeSet::transfer_one_region_from_mutator_to_old_collector(size_t idx, size_t alloc_capacity, const bool defer_accounting_recomputation) {\n@@ -1856,9 +1777,11 @@\n-    recompute_total_used<\/* UsedByMutatorChanged *\/ true,\n-                         \/* UsedByCollectorChanged *\/ false, \/* UsedByOldCollectorChanged *\/ true>();\n-    \/\/ Transferred region is unaffilliated, empty\n-    recompute_total_affiliated<\/* MutatorEmptiesChanged *\/ true, \/* CollectorEmptiesChanged *\/ false,\n-                               \/* OldCollectorEmptiesChanged *\/ true, \/* MutatorSizeChanged *\/ true,\n-                               \/* CollectorSizeChanged *\/ false, \/* OldCollectorSizeChanged *\/ true,\n-                               \/* AffiliatedChangesAreYoungNeutral *\/ true, \/* AffiliatedChangesAreGlobalNeutral *\/ true,\n-                               \/* UnaffiliatedChangesAreYoungNeutral *\/ false>();\n-    _partitions.assert_bounds(true);\n+    if (!defer_accounting_recomputation) {\n+      recompute_total_used<\/* UsedByMutatorChanged *\/ true,\n+                           \/* UsedByCollectorChanged *\/ false, \/* UsedByOldCollectorChanged *\/ true>();\n+      \/\/ Transferred region is unaffilliated, empty\n+      recompute_total_affiliated<\/* MutatorEmptiesChanged *\/ true, \/* CollectorEmptiesChanged *\/ false,\n+                                 \/* OldCollectorEmptiesChanged *\/ true, \/* MutatorSizeChanged *\/ true,\n+                                 \/* CollectorSizeChanged *\/ false, \/* OldCollectorSizeChanged *\/ true,\n+                                 \/* AffiliatedChangesAreYoungNeutral *\/ true, \/* AffiliatedChangesAreGlobalNeutral *\/ true,\n+                                 \/* UnaffiliatedChangesAreYoungNeutral *\/ false>();\n+      _partitions.assert_bounds(true);\n+    }\n@@ -1871,1 +1794,1 @@\n-bool ShenandoahFreeSet::flip_to_old_gc(ShenandoahHeapRegion* r) {\n+bool ShenandoahFreeSet::flip_to_old_gc(ShenandoahHeapRegion* r, const bool defer_accounting_recomputation) {\n@@ -1879,1 +1802,1 @@\n-  if (transfer_one_region_from_mutator_to_old_collector(idx, region_alloc_capacity)) {\n+  if (transfer_one_region_from_mutator_to_old_collector(idx, region_alloc_capacity, defer_accounting_recomputation)) {\n@@ -1910,8 +1833,10 @@\n-      \/\/ Should have no effect on used, since flipped regions are trashed: zero used *\/\n-      \/\/ Transferred regions are not affiliated, because they are empty (trash)\n-      recompute_total_affiliated<\/* MutatorEmptiesChanged *\/ true, \/* CollectorEmptiesChanged *\/ false,\n-                                 \/* OldCollectorEmptiesChanged *\/ true, \/* MutatorSizeChanged *\/ true,\n-                                 \/* CollectorSizeChanged *\/ false, \/* OldCollectorSizeChanged *\/ true,\n-                                 \/* AffiliatedChangesAreYoungNeutral *\/ true, \/* AffiliatedChangesAreGlobalNeutral *\/ true,\n-                                 \/* UnaffiliatedChangesAreYoungNeutral *\/ false>();\n-      _partitions.assert_bounds(true);\n+      if (!defer_accounting_recomputation) {\n+        \/\/ Should have no effect on used, since flipped regions are trashed: zero used *\/\n+        \/\/ Transferred regions are not affiliated, because they are empty (trash)\n+        recompute_total_affiliated<\/* MutatorEmptiesChanged *\/ true, \/* CollectorEmptiesChanged *\/ false,\n+                                   \/* OldCollectorEmptiesChanged *\/ true, \/* MutatorSizeChanged *\/ true,\n+                                   \/* CollectorSizeChanged *\/ false, \/* OldCollectorSizeChanged *\/ true,\n+                                   \/* AffiliatedChangesAreYoungNeutral *\/ true, \/* AffiliatedChangesAreGlobalNeutral *\/ true,\n+                                   \/* UnaffiliatedChangesAreYoungNeutral *\/ false>();\n+        _partitions.assert_bounds(true);\n+      }\n@@ -1931,1 +1856,1 @@\n-void ShenandoahFreeSet::flip_to_gc(ShenandoahHeapRegion* r) {\n+void ShenandoahFreeSet::flip_to_gc(ShenandoahHeapRegion* r, const bool defer_accounting_recomputation) {\n@@ -1940,9 +1865,11 @@\n-  recompute_total_used<\/* UsedByMutatorChanged *\/ true,\n-                       \/* UsedByCollectorChanged *\/ false, \/* UsedByOldCollectorChanged *\/ true>();\n-  \/\/ Transfer only affects unaffiliated regions, which stay in young\n-  recompute_total_affiliated<\/* MutatorEmptiesChanged *\/ true, \/* CollectorEmptiesChanged *\/ true,\n-                             \/* OldCollectorEmptiesChanged *\/ false, \/* MutatorSizeChanged *\/ true,\n-                             \/* CollectorSizeChanged *\/ true, \/* OldCollectorSizeChanged *\/ false,\n-                             \/* AffiliatedChangesAreYoungNeutral *\/ true, \/* AffiliatedChangesAreGlobalNeutral *\/ true,\n-                             \/* UnaffiliatedChangesAreYoungNeutral *\/ true>();\n-  _partitions.assert_bounds(true);\n+  if (!defer_accounting_recomputation) {\n+    recompute_total_used<\/* UsedByMutatorChanged *\/ true,\n+                         \/* UsedByCollectorChanged *\/ false, \/* UsedByOldCollectorChanged *\/ true>();\n+    \/\/ Transfer only affects unaffiliated regions, which stay in young\n+    recompute_total_affiliated<\/* MutatorEmptiesChanged *\/ true, \/* CollectorEmptiesChanged *\/ true,\n+                               \/* OldCollectorEmptiesChanged *\/ false, \/* MutatorSizeChanged *\/ true,\n+                               \/* CollectorSizeChanged *\/ true, \/* OldCollectorSizeChanged *\/ false,\n+                               \/* AffiliatedChangesAreYoungNeutral *\/ true, \/* AffiliatedChangesAreGlobalNeutral *\/ true,\n+                               \/* UnaffiliatedChangesAreYoungNeutral *\/ true>();\n+    _partitions.assert_bounds(true);\n+  }\n@@ -2027,0 +1954,1 @@\n+    assert(!region->is_active_alloc_region(), \"There should be no active alloc regions when choosing collection set\");\n@@ -2941,1 +2869,1 @@\n-            partition_name(partition_id),\n+            ShenandoahRegionPartitions::partition_name(partition_id),\n@@ -3117,0 +3045,12 @@\n+int ShenandoahFreeSet::reserve_alloc_regions(ShenandoahFreeSetPartitionId partition, int regions_to_reserve, size_t min_free_words, ShenandoahHeapRegion** reserved_regions) {\n+  switch (partition) {\n+    case ShenandoahFreeSetPartitionId::Mutator:\n+      return reserve_alloc_regions<ShenandoahFreeSetPartitionId::Mutator>(regions_to_reserve, min_free_words, reserved_regions);\n+    case ShenandoahFreeSetPartitionId::Collector:\n+      return reserve_alloc_regions<ShenandoahFreeSetPartitionId::Collector>(regions_to_reserve, min_free_words, reserved_regions);\n+    case ShenandoahFreeSetPartitionId::OldCollector:\n+      return reserve_alloc_regions<ShenandoahFreeSetPartitionId::OldCollector>(regions_to_reserve, min_free_words, reserved_regions);\n+    default:\n+      ShouldNotReachHere();\n+  }\n+}\n@@ -3118,1 +3058,3 @@\n-HeapWord* ShenandoahFreeSet::allocate(ShenandoahAllocRequest& req, bool& in_new_region) {\n+template<ShenandoahFreeSetPartitionId ALLOC_PARTITION>\n+int ShenandoahFreeSet::reserve_alloc_regions(int regions_to_reserve, size_t min_free_words, ShenandoahHeapRegion** reserved_regions) {\n+  assert(0 < regions_to_reserve && regions_to_reserve <= (int) ShenandoahAllocator<ALLOC_PARTITION>::MAX_ALLOC_REGION_COUNT, \"Sanity check\");\n@@ -3120,19 +3062,11 @@\n-  if (ShenandoahHeapRegion::requires_humongous(req.size())) {\n-    switch (req.type()) {\n-      case ShenandoahAllocRequest::_alloc_shared:\n-      case ShenandoahAllocRequest::_alloc_shared_gc:\n-        in_new_region = true;\n-        return allocate_contiguous(req, \/* is_humongous = *\/ true);\n-      case ShenandoahAllocRequest::_alloc_cds:\n-        in_new_region = true;\n-        return allocate_contiguous(req, \/* is_humongous = *\/ false);\n-      case ShenandoahAllocRequest::_alloc_plab:\n-      case ShenandoahAllocRequest::_alloc_gclab:\n-      case ShenandoahAllocRequest::_alloc_tlab:\n-        in_new_region = false;\n-        assert(false, \"Trying to allocate TLAB in humongous region: %zu\", req.size());\n-        return nullptr;\n-      default:\n-        ShouldNotReachHere();\n-        return nullptr;\n-    }\n+  if (ALLOC_PARTITION == ShenandoahFreeSetPartitionId::Mutator) {\n+    update_allocation_bias();\n+  }\n+\n+  if (_partitions.is_empty(ALLOC_PARTITION)) {\n+    return 0;\n+  }\n+\n+  if (_partitions.alloc_from_left_bias(ALLOC_PARTITION)) {\n+    ShenandoahLeftRightIterator iterator(&_partitions, ALLOC_PARTITION);\n+    return reserve_alloc_regions_internal<ALLOC_PARTITION>(iterator, regions_to_reserve, min_free_words, reserved_regions);\n@@ -3140,1 +3074,113 @@\n-    return allocate_single(req, in_new_region);\n+    ShenandoahRightLeftIterator iterator(&_partitions, ALLOC_PARTITION);\n+    return reserve_alloc_regions_internal<ALLOC_PARTITION>(iterator, regions_to_reserve, min_free_words, reserved_regions);\n+  }\n+}\n+\n+template<ShenandoahFreeSetPartitionId ALLOC_PARTITION, typename Iter>\n+int ShenandoahFreeSet::reserve_alloc_regions_internal(Iter iterator, int const regions_to_reserve, size_t min_free_words, ShenandoahHeapRegion** reserved_regions) {\n+  ShenandoahAffiliation affiliation = ALLOC_PARTITION == ShenandoahFreeSetPartitionId::OldCollector ? OLD_GENERATION : YOUNG_GENERATION;\n+  bool use_affiliated_first = ALLOC_PARTITION != ShenandoahFreeSetPartitionId::Mutator;\n+  ShenandoahHeapRegion* free_heap_regions[ShenandoahAllocator<ALLOC_PARTITION>::MAX_ALLOC_REGION_COUNT];\n+  int free_heap_region_count = 0;\n+  int reserved_regions_count = 0;\n+\n+  for (idx_t idx = iterator.current(); iterator.has_next() && reserved_regions_count < regions_to_reserve; idx = iterator.next()) {\n+    ShenandoahHeapRegion* r = _heap->get_region(idx);\n+    if (_heap->is_concurrent_weak_root_in_progress() && r->is_trash()) {\n+      continue;\n+    }\n+    if (use_affiliated_first && r->affiliation() != affiliation && r->affiliation() != FREE) {\n+      continue;\n+    }\n+    r->try_recycle_under_lock();\n+    assert(r->is_affiliated() || r->is_empty(), \"Affiliated or empty\");\n+    if (!r->is_empty() && r->affiliation() != affiliation) {\n+      continue;\n+    }\n+    size_t ac_words = alloc_capacity_words(r);\n+    if (ac_words >= min_free_words) {\n+      if (r->is_empty()) {\n+        assert(r->affiliation() == FREE, \"Empty region must be free\");\n+        if (use_affiliated_first) {\n+          if (free_heap_region_count < regions_to_reserve) {\n+            free_heap_regions[free_heap_region_count++] = r;\n+          }\n+          continue;\n+        }\n+        free_heap_regions[free_heap_region_count++] = r;\n+      }\n+      reserved_regions[reserved_regions_count++] = r;\n+    }\n+  }\n+\n+  if (reserved_regions_count == 0 && free_heap_region_count == 0) {\n+    return 0;\n+  }\n+\n+  int reserved_free_region_count = use_affiliated_first ? 0 : free_heap_region_count;\n+  if (use_affiliated_first && reserved_regions_count < regions_to_reserve && free_heap_region_count > 0) {\n+    while (reserved_regions_count < regions_to_reserve && reserved_free_region_count < free_heap_region_count) {\n+      reserved_regions[reserved_regions_count++] = free_heap_regions[reserved_free_region_count++];\n+    }\n+    \/\/ we have found enough regions, release the ones won't be used.\n+    int i = reserved_free_region_count;\n+    while (i < free_heap_region_count) {\n+      free_heap_regions[i++] = nullptr;\n+    }\n+  }\n+\n+  if (reserved_regions_count == 0) {\n+    return 0;\n+  }\n+\n+  if (reserved_free_region_count > 0) {\n+    partitions()->decrease_empty_region_counts(ALLOC_PARTITION, (size_t) reserved_free_region_count);;\n+    for (int i = 0; i < reserved_free_region_count; i++) {\n+      ShenandoahHeapRegion* r = free_heap_regions[i];\n+      r->set_affiliation(affiliation);\n+      r->make_regular_allocation(affiliation);\n+      if (affiliation == OLD_GENERATION) {\n+        \/\/ Any OLD region allocated during concurrent coalesce-and-fill does not need to be coalesced and filled because\n+        \/\/ all objects allocated within this region are above TAMS (and thus are implicitly marked).  In case this is an\n+        \/\/ OLD region and concurrent preparation for mixed evacuations visits this region before the start of the next\n+        \/\/ old-gen concurrent mark (i.e. this region is allocated following the start of old-gen concurrent mark but before\n+        \/\/ concurrent preparations for mixed evacuations are completed), we mark this region as not requiring any\n+        \/\/ coalesce-and-fill processing.\n+        r->end_preemptible_coalesce_and_fill();\n+        _heap->old_generation()->clear_cards_for(r);\n+      }\n+    }\n+  }\n+\n+  for (int i = 0; i < reserved_regions_count; i++) {\n+    ShenandoahHeapRegion* r = reserved_regions[i];\n+    assert(r->affiliation() == affiliation, \"Region must have been affiliated\");\n+    \/\/ The region still have capacity for at least one lab alloc\n+    size_t reserved_bytes = partitions()->retire_from_partition(ALLOC_PARTITION, r->index(), r->used());\n+    if (ALLOC_PARTITION == ShenandoahFreeSetPartitionId::Mutator) {\n+      increase_bytes_allocated(reserved_bytes);\n+    }\n+    r->set_active_alloc_region();\n+  }\n+\n+  return reserved_regions_count;\n+}\n+\n+ShenandoahHeapRegion* ShenandoahFreeSet::find_heap_region_for_allocation(ShenandoahFreeSetPartitionId partition, size_t min_free_words, bool is_lab_alloc, bool &new_region) {\n+  switch (partition) {\n+    case ShenandoahFreeSetPartitionId::Mutator:\n+      return find_heap_region_for_allocation<ShenandoahFreeSetPartitionId::Mutator>(min_free_words, is_lab_alloc, new_region);\n+    case ShenandoahFreeSetPartitionId::Collector:\n+      return find_heap_region_for_allocation<ShenandoahFreeSetPartitionId::Collector>(min_free_words, is_lab_alloc, new_region);\n+    case ShenandoahFreeSetPartitionId::OldCollector:\n+      return find_heap_region_for_allocation<ShenandoahFreeSetPartitionId::OldCollector>(min_free_words, is_lab_alloc, new_region);\n+    default:\n+      ShouldNotReachHere();\n+  }\n+}\n+\n+template<ShenandoahFreeSetPartitionId ALLOC_PARTITION>\n+ShenandoahHeapRegion* ShenandoahFreeSet::find_heap_region_for_allocation(size_t min_free_words, bool is_lab_alloc, bool &new_region) {\n+  shenandoah_assert_heaplocked();\n+  if (ALLOC_PARTITION == ShenandoahFreeSetPartitionId::Mutator) {\n+    update_allocation_bias();\n@@ -3142,0 +3188,25 @@\n+\n+  if (ALLOC_PARTITION == ShenandoahFreeSetPartitionId::Mutator && _partitions.is_empty(ALLOC_PARTITION)) {\n+    return nullptr;\n+  }\n+\n+  ShenandoahHeapRegion* region = nullptr;\n+  if (!_partitions.is_empty(ALLOC_PARTITION)) {\n+    if (_partitions.alloc_from_left_bias(ALLOC_PARTITION)) {\n+      ShenandoahLeftRightIterator iterator(&_partitions, ALLOC_PARTITION);\n+      region = find_heap_region_for_allocation_internal<ALLOC_PARTITION>(iterator, min_free_words, is_lab_alloc, new_region);\n+    } else {\n+      ShenandoahRightLeftIterator iterator(&_partitions, ALLOC_PARTITION);\n+      region = find_heap_region_for_allocation_internal<ALLOC_PARTITION>(iterator, min_free_words, is_lab_alloc, new_region);\n+    }\n+  }\n+  if (region == nullptr && ALLOC_PARTITION != ShenandoahFreeSetPartitionId::Mutator &&\n+      _partitions.get_empty_region_counts(ShenandoahFreeSetPartitionId::Mutator) > 0) {\n+    \/\/ Steal one FREE region from Mutator view for gc\n+    region = steal_heap_region_from_mutator_for_allocation<ALLOC_PARTITION>();\n+    if (region != nullptr) {\n+      log_debug(gc, alloc)(\"Stealing Region %li from Mutator to %s for allocation.\", region->index(), ShenandoahRegionPartitions::partition_name(ALLOC_PARTITION));\n+      new_region = true;\n+    }\n+  }\n+  return region;\n@@ -3144,0 +3215,90 @@\n+template<ShenandoahFreeSetPartitionId ALLOC_PARTITION, typename Iter>\n+ShenandoahHeapRegion* ShenandoahFreeSet::find_heap_region_for_allocation_internal(Iter iterator, size_t min_free_words, bool is_lab_alloc, bool &new_region) {\n+  ShenandoahHeapRegion* result = nullptr;\n+  ShenandoahHeapRegion* first_free_region = nullptr;\n+  ShenandoahAffiliation const affiliation = ALLOC_PARTITION == ShenandoahFreeSetPartitionId::OldCollector ? OLD_GENERATION : YOUNG_GENERATION;\n+  bool const use_affiliated_region_first = ALLOC_PARTITION != ShenandoahFreeSetPartitionId::Mutator;\n+  for (idx_t idx = iterator.current(); iterator.has_next(); idx = iterator.next()) {\n+    ShenandoahHeapRegion* r = _heap->get_region(idx);\n+    if (r->is_trash() && _heap->is_concurrent_weak_root_in_progress()) {\n+      continue;\n+    }\n+    \/\/ Found a region with enough capacity to meet the min_free_words condition, prepare the region and return it here\n+    r->try_recycle_under_lock();\n+    assert(r->is_affiliated() || r->is_empty(), \"Affiliated or empty.\");\n+    if (!r->is_empty() && r->affiliation() != affiliation) {\n+      continue;\n+    }\n+\n+    if (r->affiliation() == FREE && use_affiliated_region_first) {\n+      if (first_free_region == nullptr) {\n+        first_free_region = r;\n+      }\n+      continue;\n+    }\n+    if (r->is_empty()) {\n+      result = r;\n+      break;\n+    }\n+    size_t ac_words = alloc_capacity_words(r);\n+    if (is_lab_alloc && ac_words != ShenandoahHeapRegion::region_size_words()) {\n+      \/\/ For lab, must make sure the alloc capacity is still sufficient after aligning down;\n+      ac_words = align_down((ac_words * HeapWordSize) >> LogHeapWordSize, MinObjAlignment);\n+    }\n+    if (ac_words >= min_free_words) {\n+      result = r;\n+      break;\n+    }\n+  }\n+\n+  if (result == nullptr && use_affiliated_region_first && first_free_region != nullptr) {\n+    assert(first_free_region->is_empty(), \"Free region must be empty\");\n+    result = first_free_region;\n+  }\n+\n+  if (result != nullptr && result->is_empty()) {\n+    assert(result->affiliation() == FREE, \"New region must be free\");\n+    result->set_affiliation(affiliation);\n+    result->make_regular_allocation(affiliation);\n+    partitions()->one_region_is_no_longer_empty(ALLOC_PARTITION);\n+    new_region = true;\n+\n+    if (affiliation == OLD_GENERATION) {\n+      result->end_preemptible_coalesce_and_fill();\n+      _heap->old_generation()->clear_cards_for(result);\n+    }\n+  }\n+  return result;\n+}\n+\n+template<ShenandoahFreeSetPartitionId ALLOC_PARTITION>\n+ShenandoahHeapRegion* ShenandoahFreeSet::steal_heap_region_from_mutator_for_allocation() {\n+  assert(ALLOC_PARTITION == ShenandoahFreeSetPartitionId::Collector || ALLOC_PARTITION == ShenandoahFreeSetPartitionId::OldCollector, \"Must be\");\n+\n+  ShenandoahRightLeftIterator iterator(&_partitions, ShenandoahFreeSetPartitionId::Mutator, true);\n+  for (idx_t idx = iterator.current(); iterator.has_next(); idx = iterator.next()) {\n+    ShenandoahHeapRegion* r = _heap->get_region(idx);\n+    if (can_allocate_from(r)) {\n+      r->try_recycle_under_lock();\n+      if (r->is_empty()) {\n+        assert(r->affiliation() == FREE, \"Empty region must be free\");\n+        if (ALLOC_PARTITION == ShenandoahFreeSetPartitionId::OldCollector) {\n+          if (!flip_to_old_gc(r, true\/*delay_total_recomputation*\/)) {\n+            continue;\n+          }\n+        } else {\n+          flip_to_gc(r, true\/*delay_total_recomputation*\/);\n+        }\n+        log_debug(gc, free)(\"Flipped free region %zu to partition %s: \", idx, ShenandoahRegionPartitions::partition_name(ALLOC_PARTITION));\n+        ShenandoahAffiliation const affiliation = ALLOC_PARTITION == ShenandoahFreeSetPartitionId::OldCollector ? OLD_GENERATION : YOUNG_GENERATION;\n+        r->set_affiliation(affiliation);\n+        r->make_regular_allocation(affiliation);\n+        partitions()->one_region_is_no_longer_empty(ALLOC_PARTITION);\n+        return r;\n+      }\n+    }\n+  }\n+  return nullptr;\n+}\n+\n+\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahFreeSet.cpp","additions":322,"deletions":161,"binary":false,"changes":483,"status":"modified"},{"patch":"@@ -29,0 +29,1 @@\n+#include \"gc\/shenandoah\/shenandoahAllocator.hpp\"\n@@ -34,12 +35,0 @@\n-\/\/ Each ShenandoahHeapRegion is associated with a ShenandoahFreeSetPartitionId.\n-enum class ShenandoahFreeSetPartitionId : uint8_t {\n-  Mutator,                      \/\/ Region is in the Mutator free set: available memory is available to mutators.\n-  Collector,                    \/\/ Region is in the Collector free set: available memory is reserved for evacuations.\n-  OldCollector,                 \/\/ Region is in the Old Collector free set:\n-                                \/\/    available memory is reserved for old evacuations and for promotions.\n-  NotFree                       \/\/ Region is in no free set: it has no available memory.  Consult region affiliation\n-                                \/\/    to determine whether this retired region is young or old.  If young, the region\n-                                \/\/    is considered to be part of the Mutator partition.  (When we retire from the\n-                                \/\/    Collector partition, we decrease total_region_count for Collector and increaese\n-                                \/\/    for Mutator, making similar adjustments to used (net impact on available is neutral).\n-};\n@@ -52,0 +41,1 @@\n+friend class ShenandoahFreeSet;\n@@ -144,0 +134,2 @@\n+  static const char* partition_name(ShenandoahFreeSetPartitionId t);\n+\n@@ -199,1 +191,1 @@\n-  \/\/ Return the number of waste bytes (if any).\n+  \/\/ Return the number of remnant bytes (if any).\n@@ -283,2 +275,8 @@\n-  inline void increase_region_counts(ShenandoahFreeSetPartitionId which_partition, size_t regions);\n-  inline void decrease_region_counts(ShenandoahFreeSetPartitionId which_partition, size_t regions);\n+  inline void increase_region_counts(ShenandoahFreeSetPartitionId which_partition, size_t regions) {\n+    _region_counts[int(which_partition)] += regions;\n+  }\n+  inline void decrease_region_counts(ShenandoahFreeSetPartitionId which_partition, size_t regions) {\n+    assert(_region_counts[int(which_partition)] >= regions, \"Cannot remove more regions than are present\");\n+    _region_counts[int(which_partition)] -= regions;\n+  }\n+\n@@ -290,2 +288,9 @@\n-  inline void increase_empty_region_counts(ShenandoahFreeSetPartitionId which_partition, size_t regions);\n-  inline void decrease_empty_region_counts(ShenandoahFreeSetPartitionId which_partition, size_t regions);\n+  inline void increase_empty_region_counts(ShenandoahFreeSetPartitionId which_partition, size_t regions) {\n+    _empty_region_counts[int(which_partition)] += regions;\n+  }\n+\n+  inline void decrease_empty_region_counts(ShenandoahFreeSetPartitionId which_partition, size_t regions) {\n+    assert(_empty_region_counts[int(which_partition)] >= regions, \"Cannot remove more regions than are present\");\n+    _empty_region_counts[int(which_partition)] -= regions;\n+  }\n+\n@@ -297,1 +302,7 @@\n-  inline void increase_capacity(ShenandoahFreeSetPartitionId which_partition, size_t bytes);\n+  inline void increase_capacity(ShenandoahFreeSetPartitionId which_partition, size_t bytes) {\n+    shenandoah_assert_heaplocked();\n+    assert (which_partition < NumPartitions, \"Partition must be valid\");\n+    _capacity[int(which_partition)] += bytes;\n+    _available[int(which_partition)] += bytes;\n+  }\n+\n@@ -308,2 +319,17 @@\n-  inline void increase_used(ShenandoahFreeSetPartitionId which_partition, size_t bytes);\n-  inline void decrease_used(ShenandoahFreeSetPartitionId which_partition, size_t bytes);\n+  inline void increase_used(ShenandoahFreeSetPartitionId which_partition, size_t bytes) {\n+    shenandoah_assert_heaplocked();\n+    assert (which_partition < NumPartitions, \"Partition must be valid\");\n+    _used[int(which_partition)] += bytes;\n+    _available[int(which_partition)] -= bytes;\n+    assert (_used[int(which_partition)] <= _capacity[int(which_partition)],\n+            \"Must not use (%zu) more than capacity (%zu) after increase by %zu\",\n+            _used[int(which_partition)], _capacity[int(which_partition)], bytes);\n+  }\n+  inline void decrease_used(ShenandoahFreeSetPartitionId which_partition, size_t bytes) {\n+    shenandoah_assert_heaplocked();\n+    assert (which_partition < NumPartitions, \"Partition must be valid\");\n+    assert (_used[int(which_partition)] >= bytes, \"Must not use less than zero after decrease\");\n+    _used[int(which_partition)] -= bytes;\n+    _available[int(which_partition)] += bytes;\n+  }\n+\n@@ -376,1 +402,1 @@\n-    return _available[int(which_partition)];\n+    return available_in(which_partition);\n@@ -442,0 +468,3 @@\n+  ShenandoahMutatorAllocator* _mutator_allocator;\n+  ShenandoahCollectorAllocator* _collector_allocator;\n+  ShenandoahOldCollectorAllocator* _old_collector_allocator;\n@@ -489,7 +518,0 @@\n-  template<bool UsedByMutatorChanged, bool UsedByCollectorChanged, bool UsedByOldCollectorChanged>\n-  inline void recompute_total_used() {\n-    recompute_total_young_used<UsedByMutatorChanged, UsedByCollectorChanged>();\n-    recompute_total_old_used<UsedByOldCollectorChanged>();\n-    recompute_total_global_used<UsedByMutatorChanged, UsedByCollectorChanged, UsedByOldCollectorChanged>();\n-  }\n-\n@@ -508,45 +530,0 @@\n-  \/\/ If only affiliation changes are promote-in-place and generation sizes have not changed,\n-  \/\/    we have AffiliatedChangesAreGlobalNeutral\n-  \/\/ If only affiliation changes are non-empty regions moved from Mutator to Collector and young size has not changed,\n-  \/\/    we have AffiliatedChangesAreYoungNeutral\n-  \/\/ If only unaffiliated changes are empty regions from Mutator to\/from Collector, we have UnaffiliatedChangesAreYoungNeutral\n-  template<bool MutatorEmptiesChanged, bool CollectorEmptiesChanged, bool OldCollectorEmptiesChanged,\n-           bool MutatorSizeChanged, bool CollectorSizeChanged, bool OldCollectorSizeChanged,\n-           bool AffiliatedChangesAreYoungNeutral, bool AffiliatedChangesAreGlobalNeutral,\n-           bool UnaffiliatedChangesAreYoungNeutral>\n-  inline void recompute_total_affiliated() {\n-    shenandoah_assert_heaplocked();\n-    size_t region_size_bytes = ShenandoahHeapRegion::region_size_bytes();\n-    if (!UnaffiliatedChangesAreYoungNeutral && (MutatorEmptiesChanged || CollectorEmptiesChanged)) {\n-      _young_unaffiliated_regions = (_partitions.get_empty_region_counts(ShenandoahFreeSetPartitionId::Mutator) +\n-                                     _partitions.get_empty_region_counts(ShenandoahFreeSetPartitionId::Collector));\n-    }\n-    if (!AffiliatedChangesAreYoungNeutral &&\n-        (MutatorSizeChanged || CollectorSizeChanged || MutatorEmptiesChanged || CollectorEmptiesChanged)) {\n-      _young_affiliated_regions = ((_partitions.get_capacity(ShenandoahFreeSetPartitionId::Mutator) +\n-                                    _partitions.get_capacity(ShenandoahFreeSetPartitionId::Collector)) \/ region_size_bytes -\n-                                   _young_unaffiliated_regions);\n-    }\n-    if (OldCollectorSizeChanged || OldCollectorEmptiesChanged) {\n-      _old_affiliated_regions = (_partitions.get_capacity(ShenandoahFreeSetPartitionId::OldCollector) \/ region_size_bytes -\n-                                 _partitions.get_empty_region_counts(ShenandoahFreeSetPartitionId::OldCollector));\n-    }\n-    if (!AffiliatedChangesAreGlobalNeutral &&\n-        (MutatorEmptiesChanged || CollectorEmptiesChanged || OldCollectorEmptiesChanged)) {\n-      _global_unaffiliated_regions =\n-        _young_unaffiliated_regions + _partitions.get_empty_region_counts(ShenandoahFreeSetPartitionId::OldCollector);\n-    }\n-    if (!AffiliatedChangesAreGlobalNeutral &&\n-        (MutatorSizeChanged || CollectorSizeChanged || MutatorEmptiesChanged || CollectorEmptiesChanged ||\n-         OldCollectorSizeChanged || OldCollectorEmptiesChanged)) {\n-      _global_affiliated_regions = _young_affiliated_regions + _old_affiliated_regions;\n-    }\n-#ifdef ASSERT\n-    if (ShenandoahHeap::heap()->mode()->is_generational()) {\n-      assert(_young_affiliated_regions * ShenandoahHeapRegion::region_size_bytes() >= _total_young_used, \"sanity\");\n-      assert(_old_affiliated_regions * ShenandoahHeapRegion::region_size_bytes() >= _total_old_used, \"sanity\");\n-    }\n-    assert(_global_affiliated_regions * ShenandoahHeapRegion::region_size_bytes() >= _total_global_used, \"sanity\");\n-#endif\n-  }\n-\n@@ -557,14 +534,1 @@\n-  \/\/ While holding the heap lock, allocate memory for a single object or LAB  which is to be entirely contained\n-  \/\/ within a single HeapRegion as characterized by req.\n-  \/\/\n-  \/\/ Precondition: !ShenandoahHeapRegion::requires_humongous(req.size())\n-  HeapWord* allocate_single(ShenandoahAllocRequest& req, bool& in_new_region);\n-\n-  \/\/ While holding the heap lock, allocate memory for a humongous object which spans one or more regions that\n-  \/\/ were previously empty.  Regions that represent humongous objects are entirely dedicated to the humongous\n-  \/\/ object.  No other objects are packed into these regions.\n-  \/\/\n-  \/\/ Precondition: ShenandoahHeapRegion::requires_humongous(req.size())\n-  HeapWord* allocate_contiguous(ShenandoahAllocRequest& req, bool is_humongous);\n-\n-  bool transfer_one_region_from_mutator_to_old_collector(size_t idx, size_t alloc_capacity);\n+  bool transfer_one_region_from_mutator_to_old_collector(size_t idx, size_t alloc_capacity, bool defer_accounting_recomputation = false);\n@@ -579,1 +543,1 @@\n-  void flip_to_gc(ShenandoahHeapRegion* r);\n+  void flip_to_gc(ShenandoahHeapRegion* r, bool defer_accounting_recomputation = false);\n@@ -582,4 +546,1 @@\n-  bool flip_to_old_gc(ShenandoahHeapRegion* r);\n-\n-  \/\/ Handle allocation for mutator.\n-  HeapWord* allocate_for_mutator(ShenandoahAllocRequest &req, bool &in_new_region);\n+  bool flip_to_old_gc(ShenandoahHeapRegion* r, bool defer_accounting_recomputation = false);\n@@ -594,3 +555,0 @@\n-  \/\/ Handle allocation for collector (for evacuation).\n-  HeapWord* allocate_for_collector(ShenandoahAllocRequest& req, bool& in_new_region);\n-\n@@ -631,1 +589,0 @@\n-  size_t get_usable_free_words(size_t free_bytes) const;\n@@ -637,0 +594,12 @@\n+  template<ShenandoahFreeSetPartitionId ALLOC_PARTITION>\n+  int reserve_alloc_regions(int regions_to_reserve, size_t min_free_words, ShenandoahHeapRegion** reserved_regions);\n+\n+  template<ShenandoahFreeSetPartitionId ALLOC_PARTITION, typename Iter>\n+  int reserve_alloc_regions_internal(Iter iterator, int regions_to_reserve, size_t min_free_words, ShenandoahHeapRegion** reserved_regions);\n+\n+  template<ShenandoahFreeSetPartitionId ALLOC_PARTITION>\n+  ShenandoahHeapRegion* find_heap_region_for_allocation(size_t min_free_words, bool is_lab_alloc, bool &new_region);\n+\n+  template<ShenandoahFreeSetPartitionId ALLOC_PARTITION, typename Iter>\n+  ShenandoahHeapRegion* find_heap_region_for_allocation_internal(Iter iterator, size_t min_free_words, bool is_lab_alloc, bool &new_region);\n+\n@@ -642,0 +611,52 @@\n+  template<bool UsedByMutatorChanged, bool UsedByCollectorChanged, bool UsedByOldCollectorChanged>\n+  inline void recompute_total_used() {\n+    recompute_total_young_used<UsedByMutatorChanged, UsedByCollectorChanged>();\n+    recompute_total_old_used<UsedByOldCollectorChanged>();\n+    recompute_total_global_used<UsedByMutatorChanged, UsedByCollectorChanged, UsedByOldCollectorChanged>();\n+  }\n+\n+  \/\/ If only affiliation changes are promote-in-place and generation sizes have not changed,\n+  \/\/    we have AffiliatedChangesAreGlobalNeutral\n+  \/\/ If only affiliation changes are non-empty regions moved from Mutator to Collector and young size has not changed,\n+  \/\/    we have AffiliatedChangesAreYoungNeutral\n+  \/\/ If only unaffiliated changes are empty regions from Mutator to\/from Collector, we have UnaffiliatedChangesAreYoungNeutral\n+  template<bool MutatorEmptiesChanged, bool CollectorEmptiesChanged, bool OldCollectorEmptiesChanged,\n+           bool MutatorSizeChanged, bool CollectorSizeChanged, bool OldCollectorSizeChanged,\n+           bool AffiliatedChangesAreYoungNeutral, bool AffiliatedChangesAreGlobalNeutral,\n+           bool UnaffiliatedChangesAreYoungNeutral>\n+  inline void recompute_total_affiliated() {\n+    shenandoah_assert_heaplocked();\n+    size_t region_size_bytes = ShenandoahHeapRegion::region_size_bytes();\n+    if (!UnaffiliatedChangesAreYoungNeutral && (MutatorEmptiesChanged || CollectorEmptiesChanged)) {\n+      _young_unaffiliated_regions = (_partitions.get_empty_region_counts(ShenandoahFreeSetPartitionId::Mutator) +\n+                                     _partitions.get_empty_region_counts(ShenandoahFreeSetPartitionId::Collector));\n+    }\n+    if (!AffiliatedChangesAreYoungNeutral &&\n+        (MutatorSizeChanged || CollectorSizeChanged || MutatorEmptiesChanged || CollectorEmptiesChanged)) {\n+      _young_affiliated_regions = ((_partitions.get_capacity(ShenandoahFreeSetPartitionId::Mutator) +\n+                                    _partitions.get_capacity(ShenandoahFreeSetPartitionId::Collector)) \/ region_size_bytes -\n+                                   _young_unaffiliated_regions);\n+    }\n+    if (OldCollectorSizeChanged || OldCollectorEmptiesChanged) {\n+      _old_affiliated_regions = (_partitions.get_capacity(ShenandoahFreeSetPartitionId::OldCollector) \/ region_size_bytes -\n+                                 _partitions.get_empty_region_counts(ShenandoahFreeSetPartitionId::OldCollector));\n+    }\n+    if (!AffiliatedChangesAreGlobalNeutral &&\n+        (MutatorEmptiesChanged || CollectorEmptiesChanged || OldCollectorEmptiesChanged)) {\n+      _global_unaffiliated_regions =\n+        _young_unaffiliated_regions + _partitions.get_empty_region_counts(ShenandoahFreeSetPartitionId::OldCollector);\n+    }\n+    if (!AffiliatedChangesAreGlobalNeutral &&\n+        (MutatorSizeChanged || CollectorSizeChanged || MutatorEmptiesChanged || CollectorEmptiesChanged ||\n+         OldCollectorSizeChanged || OldCollectorEmptiesChanged)) {\n+      _global_affiliated_regions = _young_affiliated_regions + _old_affiliated_regions;\n+    }\n+#ifdef ASSERT\n+    if (ShenandoahHeap::heap()->mode()->is_generational()) {\n+      assert(_young_affiliated_regions * ShenandoahHeapRegion::region_size_bytes() >= _total_young_used, \"sanity\");\n+      assert(_old_affiliated_regions * ShenandoahHeapRegion::region_size_bytes() >= _total_old_used, \"sanity\");\n+    }\n+    assert(_global_affiliated_regions * ShenandoahHeapRegion::region_size_bytes() >= _total_global_used, \"sanity\");\n+#endif\n+  }\n+\n@@ -659,0 +680,1 @@\n+  inline size_t alloc_capacity_words(ShenandoahHeapRegion *r) const;\n@@ -660,0 +682,3 @@\n+  ShenandoahRegionPartitions* partitions() {\n+    return &_partitions;\n+  }\n@@ -789,1 +814,47 @@\n-  HeapWord* allocate(ShenandoahAllocRequest& req, bool& in_new_region);\n+  \/\/ Get the mutator allocator.\n+  inline ShenandoahMutatorAllocator* mutator_allocator() {\n+    return _mutator_allocator;\n+  }\n+\n+  \/\/ Get the collector allocator.\n+  inline ShenandoahCollectorAllocator* collector_allocator() {\n+    return _collector_allocator;\n+  }\n+\n+  \/\/ Get the old collector allocator.\n+  inline ShenandoahOldCollectorAllocator* old_collector_allocator() {\n+    return _old_collector_allocator;\n+  }\n+\n+  inline void release_alloc_regions() {\n+    mutator_allocator()->release_alloc_regions();\n+    collector_allocator()->release_alloc_regions();\n+  }\n+\n+  void release_alloc_regions_under_lock() {\n+    shenandoah_assert_not_heaplocked();\n+    ShenandoahHeapLocker locker(_heap->lock());\n+    release_alloc_regions();\n+  }\n+\n+  \/\/ Handle allocation for collector (for evacuation).\n+  HeapWord* allocate_for_collector(ShenandoahAllocRequest& req, bool& in_new_region);\n+\n+  \/\/ While holding the heap lock, allocate memory for a humongous object which spans one or more regions that\n+  \/\/ were previously empty.  Regions that represent humongous objects are entirely dedicated to the humongous\n+  \/\/ object.  No other objects are packed into these regions.\n+  \/\/\n+  \/\/ Precondition: ShenandoahHeapRegion::requires_humongous(req.size())\n+  HeapWord* allocate_contiguous(ShenandoahAllocRequest& req, bool is_humongous);\n+\n+  \/\/ Reserve number of alloc regions from given partition of FreeSets,\n+  \/\/ it ensures at least one region with sufficient capacity will be reserved.\n+  int reserve_alloc_regions(ShenandoahFreeSetPartitionId partition, int regions_to_reserve, size_t min_free_words, ShenandoahHeapRegion** reserved_regions);\n+\n+  \/\/ Find a heap region for allocation, the region must have min_free_words which is the minial needed for the allocation.\n+  \/\/ available_regions_seem_for_alloc is used to record the number of regions which have space for some allocation during the search.\n+  ShenandoahHeapRegion* find_heap_region_for_allocation(ShenandoahFreeSetPartitionId partition, size_t min_free_words, bool is_lab_alloc, bool &new_region);\n+\n+  \/\/ Steal one FREE region from mutator partition for allocation on Collector\/OldCollector partition.\n+  template<ShenandoahFreeSetPartitionId ALLOC_PARTITION>\n+  ShenandoahHeapRegion* steal_heap_region_from_mutator_for_allocation();\n@@ -853,0 +924,2 @@\n+\n+  static size_t get_usable_free_words(size_t free_bytes);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahFreeSet.hpp","additions":170,"deletions":97,"binary":false,"changes":267,"status":"modified"},{"patch":"@@ -0,0 +1,44 @@\n+\/*\n+* Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n+ * Copyright (c) 2025, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef SHARE_GC_SHENANDOAH_SHENANDOAHFREESETPARTITIONID_HPP\n+#define SHARE_GC_SHENANDOAH_SHENANDOAHFREESETPARTITIONID_HPP\n+\n+#include \"utilities\/globalDefinitions.hpp\"\n+\n+\/\/ Each ShenandoahHeapRegion is associated with a ShenandoahFreeSetPartitionId.\n+enum class ShenandoahFreeSetPartitionId : uint8_t {\n+  Mutator,                      \/\/ Region is in the Mutator free set: available memory is available to mutators.\n+  Collector,                    \/\/ Region is in the Collector free set: available memory is reserved for evacuations.\n+  OldCollector,                 \/\/ Region is in the Old Collector free set:\n+                                \/\/    available memory is reserved for old evacuations and for promotions.\n+  NotFree                       \/\/ Region is in no free set: it has no available memory.  Consult region affiliation\n+                                \/\/    to determine whether this retired region is young or old.  If young, the region\n+                                \/\/    is considered to be part of the Mutator partition.  (When we retire from the\n+                                \/\/    Collector partition, we decrease total_region_count for Collector and increaese\n+                                \/\/    for Mutator, making similar adjustments to used (net impact on available is neutral).\n+};\n+\n+#endif \/\/SHARE_GC_SHENANDOAH_SHENANDOAHFREESETPARTITIONID_HPP\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahFreeSetPartitionId.hpp","additions":44,"deletions":0,"binary":false,"changes":44,"status":"added"},{"patch":"@@ -220,0 +220,2 @@\n+  heap->free_set()->release_alloc_regions_under_lock();\n+\n@@ -1118,0 +1120,2 @@\n+\n+    heap->free_set()->collector_allocator()->release_alloc_regions();\n@@ -1128,0 +1132,2 @@\n+    heap->free_set()->mutator_allocator()->reserve_alloc_regions();\n+\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahFullGC.cpp","additions":6,"deletions":0,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -579,1 +579,1 @@\n-        if (!heap->is_concurrent_old_mark_in_progress() && tams == original_top) {\n+        if (!heap->is_concurrent_old_mark_in_progress() && tams == original_top && !r->is_active_alloc_region()) {\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahGeneration.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -141,1 +141,1 @@\n-  if (r->is_young() && r->is_active() && _heap->is_tenurable(r)) {\n+  if (r->is_young() && r->is_active() && _heap->is_tenurable(r) && !r->is_active_alloc_region()) {\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahGenerationalEvacuationTask.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -921,1 +921,1 @@\n-      result = allocate_memory_under_lock(req, in_new_region);\n+      result = _free_set->mutator_allocator()->allocate(req, in_new_region);\n@@ -953,1 +953,1 @@\n-        result = allocate_memory_under_lock(req, in_new_region);\n+        result = _free_set->mutator_allocator()->allocate(req, in_new_region);\n@@ -969,1 +969,1 @@\n-    result = allocate_memory_under_lock(req, in_new_region);\n+    result = allocate_memory_for_collector(req, in_new_region);\n@@ -999,48 +999,3 @@\n-HeapWord* ShenandoahHeap::allocate_memory_under_lock(ShenandoahAllocRequest& req, bool& in_new_region) {\n-  \/\/ If we are dealing with mutator allocation, then we may need to block for safepoint.\n-  \/\/ We cannot block for safepoint for GC allocations, because there is a high chance\n-  \/\/ we are already running at safepoint or from stack watermark machinery, and we cannot\n-  \/\/ block again.\n-  ShenandoahHeapLocker locker(lock(), req.is_mutator_alloc());\n-\n-  \/\/ Make sure the old generation has room for either evacuations or promotions before trying to allocate.\n-  if (req.is_old() && !old_generation()->can_allocate(req)) {\n-    return nullptr;\n-  }\n-\n-  \/\/ If TLAB request size is greater than available, allocate() will attempt to downsize request to fit within available\n-  \/\/ memory.\n-  HeapWord* result = _free_set->allocate(req, in_new_region);\n-\n-  \/\/ Record the plab configuration for this result and register the object.\n-  if (result != nullptr && req.is_old()) {\n-    if (req.is_lab_alloc()) {\n-      old_generation()->configure_plab_for_current_thread(req);\n-    } else {\n-      \/\/ Register the newly allocated object while we're holding the global lock since there's no synchronization\n-      \/\/ built in to the implementation of register_object().  There are potential races when multiple independent\n-      \/\/ threads are allocating objects, some of which might span the same card region.  For example, consider\n-      \/\/ a card table's memory region within which three objects are being allocated by three different threads:\n-      \/\/\n-      \/\/ objects being \"concurrently\" allocated:\n-      \/\/    [-----a------][-----b-----][--------------c------------------]\n-      \/\/            [---- card table memory range --------------]\n-      \/\/\n-      \/\/ Before any objects are allocated, this card's memory range holds no objects.  Note that allocation of object a\n-      \/\/ wants to set the starts-object, first-start, and last-start attributes of the preceding card region.\n-      \/\/ Allocation of object b wants to set the starts-object, first-start, and last-start attributes of this card region.\n-      \/\/ Allocation of object c also wants to set the starts-object, first-start, and last-start attributes of this\n-      \/\/ card region.\n-      \/\/\n-      \/\/ The thread allocating b and the thread allocating c can \"race\" in various ways, resulting in confusion, such as\n-      \/\/ last-start representing object b while first-start represents object c.  This is why we need to require all\n-      \/\/ register_object() invocations to be \"mutually exclusive\" with respect to each card's memory range.\n-      old_generation()->card_scan()->register_object(result);\n-\n-      if (req.is_promotion()) {\n-        \/\/ Shared promotion.\n-        const size_t actual_size = req.actual_size() * HeapWordSize;\n-        log_debug(gc, plab)(\"Expend shared promotion of %zu bytes\", actual_size);\n-        old_generation()->expend_promoted(actual_size);\n-      }\n-    }\n+HeapWord* ShenandoahHeap::allocate_memory_for_collector(ShenandoahAllocRequest& req, bool& in_new_region) {\n+  if (req.is_young()) {\n+    return _free_set->collector_allocator()->allocate(req, in_new_region);\n@@ -1048,2 +1003,1 @@\n-\n-  return result;\n+  return _free_set->old_collector_allocator()->allocate(req, in_new_region);\n@@ -2565,0 +2519,3 @@\n+    \/\/ Reserve alloc regions for mutator after finishing rebuild.\n+  _free_set->mutator_allocator()->reserve_alloc_regions();\n+\n@@ -2817,1 +2774,1 @@\n-    assert(r->is_regular(), \"Must be regular\");\n+    assert(r->is_regular(), \"Must be regular, state: %s\", r->region_state_to_string(r->state()));\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeap.cpp","additions":11,"deletions":54,"binary":false,"changes":65,"status":"modified"},{"patch":"@@ -682,1 +682,1 @@\n-  HeapWord* allocate_memory_under_lock(ShenandoahAllocRequest& request, bool& in_new_region);\n+  HeapWord* allocate_memory_for_collector(ShenandoahAllocRequest& request, bool& in_new_region);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeap.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -433,1 +433,1 @@\n-  AtomicAccess::store(_affiliations + r->index(), (uint8_t) new_affiliation);\n+  _affiliations[r->index()] = (uint8_t) new_affiliation;\n@@ -437,1 +437,1 @@\n-  return (ShenandoahAffiliation) AtomicAccess::load(_affiliations + index);\n+  return (ShenandoahAffiliation) _affiliations[index];\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeap.inline.hpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -92,0 +92,1 @@\n+  _active_alloc_region.unset();\n@@ -327,0 +328,4 @@\n+    case _regular:\n+      if (free() != region_size_bytes()) {\n+        report_illegal_transition(\"emptying\");\n+      }\n@@ -363,3 +368,3 @@\n-  _tlab_allocs = 0;\n-  _gclab_allocs = 0;\n-  _plab_allocs = 0;\n+  AtomicAccess::store(&_tlab_allocs, size_t(0));\n+  AtomicAccess::store(&_gclab_allocs, size_t(0));\n+  AtomicAccess::store(&_plab_allocs, size_t(0));\n@@ -369,1 +374,1 @@\n-  return used() - (_tlab_allocs + _gclab_allocs + _plab_allocs) * HeapWordSize;\n+  return used() - (AtomicAccess::load(&_tlab_allocs) + AtomicAccess::load(&_gclab_allocs) + AtomicAccess::load(&_plab_allocs)) * HeapWordSize;\n@@ -373,1 +378,1 @@\n-  return _tlab_allocs * HeapWordSize;\n+  return AtomicAccess::load(&_tlab_allocs) * HeapWordSize;\n@@ -377,1 +382,1 @@\n-  return _gclab_allocs * HeapWordSize;\n+  return AtomicAccess::load(&_gclab_allocs) * HeapWordSize;\n@@ -381,1 +386,5 @@\n-  return _plab_allocs * HeapWordSize;\n+  return AtomicAccess::load(&_plab_allocs) * HeapWordSize;\n+}\n+\n+bool ShenandoahHeapRegion::has_allocs() const {\n+  return top() > bottom();\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeapRegion.cpp","additions":16,"deletions":7,"binary":false,"changes":23,"status":"modified"},{"patch":"@@ -253,1 +253,1 @@\n-  HeapWord* _top;\n+  HeapWord* volatile _top;\n@@ -255,3 +255,3 @@\n-  size_t _tlab_allocs;\n-  size_t _gclab_allocs;\n-  size_t _plab_allocs;\n+  size_t volatile _tlab_allocs;\n+  size_t volatile _gclab_allocs;\n+  size_t volatile _plab_allocs;\n@@ -264,1 +264,1 @@\n-  uint _age;\n+  volatile uint _age;\n@@ -266,1 +266,1 @@\n-  CENSUS_NOISE(uint _youth;)   \/\/ tracks epochs of retrograde ageing (rejuvenation)\n+  CENSUS_NOISE(volatile uint _youth;)   \/\/ tracks epochs of retrograde ageing (rejuvenation)\n@@ -272,0 +272,2 @@\n+  ShenandoahSharedFlag _active_alloc_region; \/\/ Flag indicates that whether the region is an active alloc region.\n+\n@@ -382,0 +384,9 @@\n+  inline HeapWord* allocate_lab(const ShenandoahAllocRequest &req, size_t &actual_size);\n+\n+  \/\/ Atomic allocation using CAS, return nullptr if full or no enough space for the req\n+  inline HeapWord* allocate_atomic(size_t word_size, const ShenandoahAllocRequest &req, bool &ready_for_retire);\n+\n+  inline HeapWord* allocate_lab_atomic(const ShenandoahAllocRequest &req, size_t &actual_size, bool &ready_for_retire);\n+\n+  inline bool try_allocate(HeapWord* const obj, size_t const size);\n+\n@@ -447,2 +458,6 @@\n-  HeapWord* top() const         { return _top;     }\n-  void set_top(HeapWord* v)     { _top = v;        }\n+  HeapWord* top() const {\n+    return AtomicAccess::load(&_top);\n+  }\n+  void set_top(HeapWord* v) {\n+    AtomicAccess::store(&_top, v);\n+  }\n@@ -460,0 +475,2 @@\n+  size_t free_words() const     { return pointer_delta(end(), top()); }\n+\n@@ -472,0 +489,1 @@\n+  bool has_allocs() const;\n@@ -475,0 +493,1 @@\n+  inline void concurrent_set_update_watermark(HeapWord* w);\n@@ -483,2 +502,2 @@\n-  uint age() const { return _age; }\n-  CENSUS_NOISE(uint youth() const { return _youth; })\n+  uint age() const { return AtomicAccess::load(&_age); }\n+  CENSUS_NOISE(uint youth() const { return AtomicAccess::load(&_youth); })\n@@ -487,4 +506,5 @@\n-    const uint max_age = markWord::max_age;\n-    assert(_age <= max_age, \"Error\");\n-    if (_age++ >= max_age) {\n-      _age = max_age;   \/\/ clamp\n+    const uint current_age = age();\n+    assert(current_age <= markWord::max_age, \"Error\");\n+    if (current_age < markWord::max_age) {\n+      const uint old = AtomicAccess::cmpxchg(&_age, current_age, current_age + 1, memory_order_relaxed);\n+      assert(old == current_age || old == 0u, \"Only fail when any mutator reset the age.\");\n@@ -495,2 +515,5 @@\n-    CENSUS_NOISE(_youth += _age;)\n-    _age = 0;\n+    uint current = age();\n+    if (current != 0) {\n+      AtomicAccess::store(&_age, uint(0));\n+      CENSUS_NOISE(AtomicAccess::add(&_youth, current, memory_order_relaxed);)\n+    }\n@@ -499,1 +522,1 @@\n-  CENSUS_NOISE(void clear_youth() { _youth = 0; })\n+  CENSUS_NOISE(void clear_youth() { AtomicAccess::store(&_youth,  0u); })\n@@ -513,0 +536,13 @@\n+  inline void set_active_alloc_region() {\n+    assert(_active_alloc_region.is_unset(), \"Must be\");\n+    _active_alloc_region.set();\n+  }\n+\n+  inline void unset_active_alloc_region() {\n+    _active_alloc_region.unset();\n+  }\n+\n+  inline bool is_active_alloc_region() const {\n+    return _active_alloc_region.is_set();\n+  }\n+\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeapRegion.hpp","additions":53,"deletions":17,"binary":false,"changes":70,"status":"modified"},{"patch":"@@ -32,0 +32,1 @@\n+#include \"gc\/shared\/plab.hpp\"\n@@ -128,0 +129,102 @@\n+HeapWord* ShenandoahHeapRegion::allocate_lab(const ShenandoahAllocRequest& req, size_t &actual_size) {\n+  shenandoah_assert_heaplocked_or_safepoint();\n+  assert(req.is_lab_alloc(), \"Only lab alloc\");\n+  assert(this->affiliation() == req.affiliation(), \"Region affiliation should already be established\");\n+\n+  size_t adjusted_size = req.size();\n+  HeapWord* obj = nullptr;\n+  HeapWord* old_top = top();\n+  size_t free_words = align_down(byte_size(old_top, end()) >> LogHeapWordSize, MinObjAlignment);\n+  if (adjusted_size > free_words) {\n+    adjusted_size = free_words;\n+  }\n+  if (adjusted_size >= req.min_size()) {\n+    obj = allocate(adjusted_size, req);\n+    actual_size = adjusted_size;\n+    assert(obj == old_top, \"Must be\");\n+  }\n+  return obj;\n+}\n+\n+\/\/ Stack object to check if region is ready for retire after atomic allocation attempts,\n+\/\/ It tracks the free words of the region and check it at the exit of atomic allocation.\n+class ShenandoahHeapRegionReadyForRetireChecker : public StackObj {\n+public:\n+  size_t _remnant_free_words;\n+  bool &_ready_for_retire;\n+\n+  ShenandoahHeapRegionReadyForRetireChecker(bool &ready_for_retire) : _remnant_free_words(ShenandoahHeapRegion::region_size_words()), _ready_for_retire(ready_for_retire) {\n+    assert(!ready_for_retire, \"Sanity check\");\n+  }\n+\n+  ~ShenandoahHeapRegionReadyForRetireChecker() {\n+    if (_remnant_free_words < PLAB::min_size()) {\n+      _ready_for_retire = true;\n+    }\n+  }\n+};\n+\n+HeapWord* ShenandoahHeapRegion::allocate_atomic(size_t size, const ShenandoahAllocRequest& req, bool &ready_for_retire) {\n+  assert(is_object_aligned(size), \"alloc size breaks alignment: %zu\", size);\n+  assert(this->affiliation() == req.affiliation(), \"Region affiliation should already be established\");\n+  assert(this->is_regular() || this->is_regular_pinned(), \"must be a regular region\");\n+\n+  ShenandoahHeapRegionReadyForRetireChecker retire_checker(ready_for_retire);\n+  for (;\/*Always return in the loop*\/;) {\n+    HeapWord* obj = top();\n+    size_t free_words = pointer_delta( end(), obj);\n+    if (free_words >= size) {\n+      if (try_allocate(obj, size)) {\n+        reset_age();\n+        adjust_alloc_metadata(req, size);\n+        retire_checker._remnant_free_words = free_words - size;\n+        return obj;\n+      }\n+    } else {\n+      retire_checker._remnant_free_words = free_words;\n+      return nullptr;\n+    }\n+  }\n+}\n+\n+HeapWord* ShenandoahHeapRegion::allocate_lab_atomic(const ShenandoahAllocRequest& req, size_t &actual_size, bool &ready_for_retire) {\n+  assert(req.is_lab_alloc() && req.type() != ShenandoahAllocRequest::_alloc_plab, \"Only tlab\/gclab alloc\");\n+  assert(this->affiliation() == req.affiliation(), \"Region affiliation should already be established\");\n+  assert(this->is_regular() || this->is_regular_pinned(), \"must be a regular region\");\n+\n+  ShenandoahHeapRegionReadyForRetireChecker retire_checker(ready_for_retire);\n+  for (;\/*Always return in the loop*\/;) {\n+    size_t adjusted_size = req.size();\n+    HeapWord* obj = top();\n+    size_t free_words = pointer_delta(end(), obj);\n+    size_t aligned_free_words = align_down((free_words * HeapWordSize) >> LogHeapWordSize, MinObjAlignment);\n+    if (adjusted_size > aligned_free_words) {\n+      adjusted_size = aligned_free_words;\n+    }\n+    if (adjusted_size >= req.min_size()) {\n+      if (try_allocate(obj, adjusted_size)) {\n+        reset_age();\n+        actual_size = adjusted_size;\n+        adjust_alloc_metadata(req, adjusted_size);\n+        retire_checker._remnant_free_words = free_words - adjusted_size;\n+        return obj;\n+      }\n+    } else {\n+      retire_checker._remnant_free_words = free_words;\n+      log_trace(gc, free)(\"Failed to shrink TLAB or GCLAB request (%zu) in region %zu to %zu\"\n+                          \" because min_size() is %zu\", req.size(), index(), adjusted_size, req.min_size());\n+      return nullptr;\n+    }\n+  }\n+}\n+\n+bool ShenandoahHeapRegion::try_allocate(HeapWord* const obj, size_t const size) {\n+  HeapWord* new_top = obj + size;\n+  if (AtomicAccess::cmpxchg(&_top, obj, new_top) == obj) {\n+    assert(is_object_aligned(new_top), \"new top breaks alignment: \" PTR_FORMAT, p2i(new_top));\n+    assert(is_object_aligned(obj),     \"obj is not aligned: \"       PTR_FORMAT, p2i(obj));\n+    return true;\n+  }\n+  return false;\n+}\n+\n@@ -132,1 +235,1 @@\n-      _tlab_allocs += size;\n+      AtomicAccess::add(&_tlab_allocs, size, memory_order_relaxed);\n@@ -134,1 +237,1 @@\n-      _plab_allocs += size;\n+      AtomicAccess::add(&_plab_allocs, size, memory_order_relaxed);\n@@ -136,1 +239,1 @@\n-      _gclab_allocs += size;\n+      AtomicAccess::add(&_gclab_allocs, size, memory_order_relaxed);\n@@ -201,0 +304,10 @@\n+inline void ShenandoahHeapRegion::concurrent_set_update_watermark(HeapWord* w) {\n+  assert(bottom() <= w && w <= top(), \"within bounds\");\n+  HeapWord* watermark = nullptr;\n+  while ((watermark = AtomicAccess::load(&_update_watermark)) < w) {\n+    if (AtomicAccess::cmpxchg(&_update_watermark, watermark, w, memory_order_release) == watermark) {\n+      return;\n+    }\n+  }\n+}\n+\n@@ -229,1 +342,1 @@\n-  _top_before_promoted = _top;\n+  _top_before_promoted = top();\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeapRegion.inline.hpp","additions":117,"deletions":4,"binary":false,"changes":121,"status":"modified"},{"patch":"@@ -51,0 +51,3 @@\n+  \/\/ Release all alloc regions at the beginning of final mark.\n+  heap->free_set()->release_alloc_regions_under_lock();\n+\n@@ -76,0 +79,5 @@\n+    {\n+      ShenandoahHeapLocker locker(heap->lock());\n+      heap->free_set()->mutator_allocator()->reserve_alloc_regions();\n+    }\n+\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahOldGC.cpp","additions":8,"deletions":0,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -46,1 +46,3 @@\n-  _evacuation_stats(new ShenandoahEvacuationStats()) {\n+  _evacuation_stats(new ShenandoahEvacuationStats()),\n+  _mutator_allocator_start_index(UINT_MAX),\n+  _collector_allocator_start_index(UINT_MAX) {\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahThreadLocalData.cpp","additions":3,"deletions":1,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -86,0 +86,4 @@\n+  uint   _mutator_allocator_start_index;\n+\n+  uint   _collector_allocator_start_index;\n+\n@@ -283,0 +287,16 @@\n+\n+  static uint mutator_allocator_start_index() {\n+    return data(Thread::current())->_mutator_allocator_start_index;\n+  }\n+\n+  static void set_mutator_allocator_start_index(uint start_index) {\n+    data(Thread::current())->_mutator_allocator_start_index = start_index;\n+  }\n+\n+  static uint collector_allocator_start_index() {\n+    return data(Thread::current())->_collector_allocator_start_index;\n+  }\n+\n+  static void set_collector_allocator_start_index(uint start_index) {\n+    data(Thread::current())->_collector_allocator_start_index = start_index;\n+  }\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahThreadLocalData.hpp","additions":20,"deletions":0,"binary":false,"changes":20,"status":"modified"},{"patch":"@@ -380,2 +380,2 @@\n-    if (r->is_cset() || r->is_trash()) {\n-      \/\/ Count the entire cset or trashed (formerly cset) region as used\n+    if (r->is_cset() || r->is_trash() || r->is_active_alloc_region()) {\n+      \/\/ Count the entire cset, trashed (formerly cset) or alloc reserved region as used\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahVerifier.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -567,0 +567,9 @@\n+                                                                            \\\n+  product(uintx, ShenandoahMutatorAllocRegions, 8, EXPERIMENTAL,            \\\n+         \"Number of alloc regions for mutator allocation.\")                 \\\n+         range(0, 128)                                                      \\\n+                                                                            \\\n+  product(uintx, ShenandoahCollectorAllocRegions, 4, EXPERIMENTAL,          \\\n+         \"Number of alloc regions for collector allocation.\")               \\\n+         range(0, 32)                                                       \\\n+\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoah_globals.hpp","additions":9,"deletions":0,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -44,1 +44,1 @@\n-  nonstatic_field(ShenandoahHeapRegion, _top,                      HeapWord*)                         \\\n+  volatile_nonstatic_field(ShenandoahHeapRegion, _top,             HeapWord*)                         \\\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/vmStructs_shenandoah.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"}]}