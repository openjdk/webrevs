{"files":[{"patch":"@@ -40,0 +40,4 @@\n+size_t PLAB::min_size_bytes() {\n+  return min_size() * HeapWordSize;\n+}\n+\n","filename":"src\/hotspot\/share\/gc\/shared\/plab.cpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -81,0 +81,2 @@\n+  \/\/ Minimum PLAB size in bytes.\n+  static size_t min_size_bytes();\n","filename":"src\/hotspot\/share\/gc\/shared\/plab.hpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -29,0 +29,1 @@\n+#include \"gc\/shenandoah\/shenandoahFreeSet.hpp\"\n@@ -88,1 +89,1 @@\n-    } else if (region->is_regular()) {\n+    } else if (region->is_regular() && region->has_allocs()) {\n@@ -94,0 +95,3 @@\n+        if (region->reserved_for_direct_allocation()) {\n+          heap->free_set()->release_directly_allocatable_region(region);\n+        }\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/heuristics\/shenandoahGenerationalHeuristics.cpp","additions":5,"deletions":1,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -30,0 +30,1 @@\n+#include \"gc\/shenandoah\/shenandoahFreeSet.hpp\"\n@@ -108,1 +109,1 @@\n-    } else if (region->is_regular()) {\n+    } else if (region->is_regular() && region->has_allocs()) {\n@@ -114,0 +115,3 @@\n+        if (region->reserved_for_direct_allocation()) {\n+          heap->free_set()->release_directly_allocatable_region(region);\n+        }\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/heuristics\/shenandoahHeuristics.cpp","additions":5,"deletions":1,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -30,0 +30,1 @@\n+#include \"gc\/shenandoah\/shenandoahFreeSet.hpp\"\n@@ -105,0 +106,3 @@\n+    if (r->reserved_for_direct_allocation()) {\n+      _heap->free_set()->release_directly_allocatable_region(r);\n+    }\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahCollectionSet.cpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -35,0 +35,1 @@\n+#include \"gc\/shenandoah\/shenandoahUtils.hpp\"\n@@ -38,0 +39,1 @@\n+#include \"runtime\/interfaceSupport.inline.hpp\"\n@@ -281,1 +283,1 @@\n-void ShenandoahRegionPartitions::increase_used(ShenandoahFreeSetPartitionId which_partition, size_t bytes) {\n+inline void ShenandoahRegionPartitions::increase_used(ShenandoahFreeSetPartitionId which_partition, size_t bytes) {\n@@ -284,1 +286,0 @@\n-\n@@ -292,0 +293,10 @@\n+inline void ShenandoahRegionPartitions::decrease_used(ShenandoahFreeSetPartitionId which_partition, size_t bytes) {\n+  shenandoah_assert_heaplocked();\n+  assert (which_partition < NumPartitions, \"Partition must be valid\");\n+  _used[int(which_partition)] -= bytes;\n+  _available[int(which_partition)] += bytes;\n+  assert (_used[int(which_partition)] <= _capacity[int(which_partition)],\n+          \"Must not use (%zu) more than capacity (%zu) after decrease by %zu\",\n+          _used[int(which_partition)], _capacity[int(which_partition)], bytes);\n+}\n+\n@@ -450,0 +461,4 @@\n+  if (orig_partition == ShenandoahFreeSetPartitionId::Mutator && r->reserved_for_direct_allocation()) {\n+    ShenandoahHeap::heap()->free_set()->release_directly_allocatable_region(r);\n+  }\n+\n@@ -747,0 +762,43 @@\n+PaddedEnd<ShenandoahDirectlyAllocatableRegionAffinity::Affinity>* ShenandoahDirectlyAllocatableRegionAffinity::_affinity = nullptr;\n+THREAD_LOCAL Thread* ShenandoahDirectlyAllocatableRegionAffinity::_self = DIRECTLY_ALLOCATABLE_REGION_UNKNOWN_SELF;\n+THREAD_LOCAL uint ShenandoahDirectlyAllocatableRegionAffinity::_index = 0;\n+\n+uint ShenandoahDirectlyAllocatableRegionAffinity::index_slow() {\n+  \/\/ Set current thread\n+  if (_self == DIRECTLY_ALLOCATABLE_REGION_UNKNOWN_SELF) {\n+    _self = Thread::current();\n+  }\n+\n+  \/\/ Create a new random index where the thread will start allocation\n+  _index = static_cast<uint>(os::random()) % ShenandoahDirectlyAllocatableRegionCount;\n+\n+  \/\/ Update affinity table\n+  _affinity[_index]._thread = _self;\n+\n+  return _index;\n+}\n+\n+void ShenandoahDirectlyAllocatableRegionAffinity::initialize() {\n+  assert(_affinity == nullptr, \"Already initialized\");\n+  _affinity = PaddedArray<Affinity, mtGC>::create_unfreeable(ShenandoahDirectlyAllocatableRegionCount);\n+  for (uint32_t i = 0; i < ShenandoahDirectlyAllocatableRegionCount; i++) {\n+    _affinity[i]._thread = DIRECTLY_ALLOCATABLE_REGION_UNKNOWN_AFFINITY;\n+  }\n+}\n+\n+uint ShenandoahDirectlyAllocatableRegionAffinity::index() {\n+  assert(_affinity != nullptr, \"Not initialized\");\n+  \/\/ Fast path\n+  if (_affinity[_index]._thread == _self) {\n+    return _index;\n+  }\n+\n+  \/\/ Slow path\n+  return index_slow();\n+}\n+\n+void ShenandoahDirectlyAllocatableRegionAffinity::set_index(uint index) {\n+  _index = index;\n+  _affinity[_index]._thread = _self;\n+}\n+\n@@ -753,0 +811,5 @@\n+  _direct_allocation_regions = PaddedArray<ShenandoahDirectAllocationRegion, mtGC>::create_unfreeable(ShenandoahDirectlyAllocatableRegionCount);\n+  for (uint i = 0; i < ShenandoahDirectlyAllocatableRegionCount; i++) {\n+    _direct_allocation_regions[i]._address = nullptr;\n+  }\n+  ShenandoahDirectlyAllocatableRegionAffinity::initialize();\n@@ -786,1 +849,1 @@\n-    if (r->affiliation() == affiliation) {\n+    if (r->affiliation() == affiliation && !r->reserved_for_direct_allocation()) {\n@@ -883,1 +946,1 @@\n-    if (alloc_capacity(r) >= min_size * HeapWordSize) {\n+    if (!r->reserved_for_direct_allocation() && alloc_capacity(r) >= min_size * HeapWordSize) {\n@@ -951,1 +1014,1 @@\n-    if (can_allocate_from(r)) {\n+    if (can_allocate_from(r) && !r->reserved_for_direct_allocation()) {\n@@ -1143,1 +1206,1 @@\n-  if (((result == nullptr) && (ac < min_capacity)) || (alloc_capacity(r) < PLAB::min_size() * HeapWordSize)) {\n+  if (((result == nullptr) && (ac < min_capacity)) || (alloc_capacity(r) < PLAB::min_size_bytes())) {\n@@ -1205,1 +1268,2 @@\n-    while (!can_allocate_from(_heap->get_region(end))) {\n+    ShenandoahHeapRegion* region = _heap->get_region(end);\n+    while (!can_allocate_from(region) || region->reserved_for_direct_allocation()) {\n@@ -1228,0 +1292,1 @@\n+      region = _heap->get_region(end);\n@@ -1465,11 +1530,8 @@\n-          \/\/ Both young and old collected regions (trashed) are placed into the Mutator set\n-          _partitions.raw_assign_membership(idx, ShenandoahFreeSetPartitionId::Mutator);\n-          if (idx < mutator_leftmost) {\n-            mutator_leftmost = idx;\n-          }\n-          if (idx > mutator_rightmost) {\n-            mutator_rightmost = idx;\n-          }\n-          if (ac == region_size_bytes) {\n-            if (idx < mutator_leftmost_empty) {\n-              mutator_leftmost_empty = idx;\n+          if (!region->reserved_for_direct_allocation()) {\n+            \/\/ Both young and old collected regions (trashed) are placed into the Mutator set\n+            _partitions.raw_assign_membership(idx, ShenandoahFreeSetPartitionId::Mutator);\n+            if (idx < mutator_leftmost) {\n+              mutator_leftmost = idx;\n+            }\n+            if (idx > mutator_rightmost) {\n+              mutator_rightmost = idx;\n@@ -1477,2 +1539,7 @@\n-            if (idx > mutator_rightmost_empty) {\n-              mutator_rightmost_empty = idx;\n+            if (ac == region_size_bytes) {\n+              if (idx < mutator_leftmost_empty) {\n+                mutator_leftmost_empty = idx;\n+              }\n+              if (idx > mutator_rightmost_empty) {\n+                mutator_rightmost_empty = idx;\n+              }\n@@ -1480,0 +1547,7 @@\n+            mutator_regions++;\n+            mutator_used += (region_size_bytes - ac);\n+          } else {\n+            \/\/ region->reserved_for_direct_allocation(), a reserved region should count in mutator regions,\n+            \/\/ but it won't be assigned to Mutator in the partition table, the entire region is counted as used.\n+            mutator_regions++;\n+            mutator_used += region_size_bytes;\n@@ -1481,2 +1555,0 @@\n-          mutator_regions++;\n-          mutator_used += (region_size_bytes - ac);\n@@ -1994,1 +2066,0 @@\n-      size_t free = capacity() - used();\n@@ -1999,1 +2070,2 @@\n-      assert(free == total_free, \"Free memory should match\");\n+      \/\/TODO remove assert, it is not possible to mach since mutators may allocate on region w\/o acquiring lock\n+      \/\/assert(free == total_free, \"Free memory should match\");\n@@ -2098,0 +2170,320 @@\n+HeapWord* ShenandoahFreeSet::allocate_humongous(ShenandoahAllocRequest& req) {\n+  assert(ShenandoahHeapRegion::requires_humongous(req.size()), \"Must be humongous alloc\");\n+  ShenandoahHeapLocker locker(_heap->lock(), req.is_mutator_alloc());\n+  return allocate_contiguous(req, \/*is_humongous*\/true);\n+}\n+\n+HeapWord* ShenandoahFreeSet::allocate_contiguous_cds(ShenandoahAllocRequest& req) {\n+  assert(req.type() == ShenandoahAllocRequest::_alloc_cds, \"Must be CDS alloc.\");\n+  ShenandoahHeapLocker locker(_heap->lock(), req.is_mutator_alloc());\n+  return allocate_contiguous(req, \/*is_humongous*\/false);\n+}\n+\n+template<bool IS_TLAB>\n+HeapWord* ShenandoahFreeSet::cas_allocate_single_for_mutator(\n+  uint probe_start, uint probe_count, ShenandoahAllocRequest &req, bool &in_new_region) {\n+  HeapWord *obj = nullptr;\n+  uint i = 0u;\n+  while (i < probe_count) {\n+    {\n+      \/\/ Yield to Safepoint\n+      ThreadBlockInVM tbivm(JavaThread::current());\n+    }\n+    uint idx = (probe_start + i) % ShenandoahDirectlyAllocatableRegionCount;\n+    ShenandoahDirectAllocationRegion& shared_region = _direct_allocation_regions[idx];\n+    ShenandoahHeapRegion* r = nullptr;\n+    \/\/ Intentionally not using AtomicAccess::load, if a mutator see a stale region it will fail to allocate anyway.\n+    if ((r = shared_region._address) != nullptr && r->reserved_for_direct_allocation()) {\n+      obj = cas_allocate_in_for_mutator<IS_TLAB>(r, req, in_new_region);\n+      if (obj != nullptr) {\n+        return obj;\n+      }\n+    }\n+    i++;\n+  }\n+  return obj;\n+}\n+template<bool IS_TLAB>\n+HeapWord* ShenandoahFreeSet::try_allocate_single_for_mutator(ShenandoahAllocRequest &req, bool &in_new_region) {\n+  shenandoah_assert_not_heaplocked();\n+  assert(req.is_mutator_alloc(), \"Must be mutator allocation\");\n+  assert(req.is_young(), \"Mutator allocations always come from young generation.\");\n+  assert(!ShenandoahHeapRegion::requires_humongous(req.size()), \"Must not\");\n+  assert(req.type() == ShenandoahAllocRequest::_alloc_tlab ||\n+         req.type() == ShenandoahAllocRequest::_alloc_shared ||\n+         req.type() == ShenandoahAllocRequest::_alloc_cds,  \"Must be\");\n+\n+  uint start_idx = ShenandoahDirectlyAllocatableRegionAffinity::index();\n+  const uint max_probes = ShenandoahDirectAllocationMaxProbes;\n+  for (;;) {\n+    HeapWord* obj = nullptr;\n+    obj = cas_allocate_single_for_mutator<IS_TLAB>(start_idx, max_probes, req, in_new_region);\n+    if (obj != nullptr) {\n+      return obj;\n+    }\n+\n+    uint next_start_index = ShenandoahDirectlyAllocatableRegionCount;\n+    if (!try_allocate_directly_allocatable_regions(start_idx, false, req, obj, in_new_region, next_start_index)) {\n+      if (obj != nullptr) {\n+        return obj;\n+      }\n+\n+      \/\/ No new directly allocatable region, but an existing region with sufficient memory has been found.\n+      if (next_start_index != ShenandoahDirectlyAllocatableRegionCount) {\n+        start_idx = next_start_index;\n+      } else {\n+        \/\/ try to steal from other directly allocatable regions\n+        uint steal_alloc_start_idx = (start_idx + max_probes) % ShenandoahDirectlyAllocatableRegionCount;\n+        uint steal_alloc_probes = ShenandoahDirectlyAllocatableRegionCount - max_probes;\n+        obj = cas_allocate_single_for_mutator<IS_TLAB>(steal_alloc_start_idx,\n+                                                       steal_alloc_probes,\n+                                                       req,\n+                                                       in_new_region);\n+        if (obj != nullptr) {\n+          return obj;\n+        }\n+\n+        \/\/ No new directly allocatable region, no existing region directly allocatable region has sufficient memory.\n+        return nullptr;\n+      }\n+    } else {\n+      assert(obj != nullptr, \"Must be\");\n+      return obj;\n+    }\n+  }\n+}\n+\n+\/\/ Explicit specializations\n+template HeapWord* ShenandoahFreeSet::try_allocate_single_for_mutator<true>(ShenandoahAllocRequest &req, bool &in_new_region);\n+template HeapWord* ShenandoahFreeSet::try_allocate_single_for_mutator<false>(ShenandoahAllocRequest &req, bool &in_new_region);\n+\n+template<bool IS_TLAB>\n+HeapWord* ShenandoahFreeSet::cas_allocate_in_for_mutator(ShenandoahHeapRegion* region, ShenandoahAllocRequest &req, bool &in_new_region) {\n+  HeapWord* obj = nullptr;\n+  size_t actual_size = req.size();\n+  if (IS_TLAB) {\n+    obj = region->allocate_lab_atomic(req, actual_size);\n+  } else {\n+    obj = region->allocate_atomic(actual_size, req);\n+  }\n+  if (obj != nullptr) {\n+    assert(actual_size > 0, \"Must be\");\n+    req.set_actual_size(actual_size);\n+    if (pointer_delta(obj, region->bottom()) == actual_size) {\n+      \/\/ Set to true if it is the first object\/tlab allocated in the region.\n+      in_new_region = true;\n+    }\n+  }\n+  return obj;\n+}\n+\n+class DirectAllocatableRegionRefillClosure final : public ShenandoahHeapRegionIterationClosure {\n+  PaddedEnd<ShenandoahDirectAllocationRegion>* _direct_allocation_regions;\n+  \/\/ inclusive\n+  const uint _start_index;\n+  \/\/ exclusive\n+  const uint _probe_end_index;\n+  const bool _replace_all_eligible_regions;\n+  int _scanned_region;\n+  int _next_retire_eligible_region;\n+  ShenandoahFreeSet* const _free_set = ShenandoahHeap::heap()->free_set();\n+public:\n+  ShenandoahAllocRequest &_req;\n+  HeapWord* &_obj;\n+  bool &_in_new_region;\n+  bool _new_region_allocated = false;\n+  uint _next_region_with_sufficient_mem;\n+  const size_t _min_req_byte_size;\n+\n+  DirectAllocatableRegionRefillClosure(PaddedEnd<ShenandoahDirectAllocationRegion>* direct_allocation_regions, uint start_index, bool replace_all_eligible_regions, ShenandoahAllocRequest &req, HeapWord* &obj, bool &in_new_region)\n+    : _direct_allocation_regions(direct_allocation_regions),\n+      _start_index(start_index),\n+      _probe_end_index((start_index + 3) % ShenandoahDirectlyAllocatableRegionCount),\n+      _replace_all_eligible_regions(replace_all_eligible_regions),\n+      _scanned_region(0),\n+      _req(req),\n+      _obj(obj),\n+      _in_new_region(in_new_region),\n+      _next_region_with_sufficient_mem(ShenandoahDirectlyAllocatableRegionCount),\n+      _min_req_byte_size((req.type() == ShenandoahAllocRequest::_alloc_tlab ? req.min_size() : req.size()) * HeapWordSize) {\n+    _next_retire_eligible_region = find_next_retire_eligible_region();\n+  }\n+\n+  bool is_probing_region(const uint index) const {\n+    return !(index >= _probe_end_index && index < _start_index);\n+  }\n+\n+  int find_next_retire_eligible_region() {\n+    int const max_regions_to_scan = static_cast<int>(_replace_all_eligible_regions ? ShenandoahDirectlyAllocatableRegionCount\n+                                                                                   : ShenandoahDirectAllocationMaxProbes);\n+    while (_scanned_region < max_regions_to_scan) {\n+      const uint idx = (_start_index + (size_t) _scanned_region) % ShenandoahDirectlyAllocatableRegionCount;\n+      _scanned_region++;\n+      ShenandoahDirectAllocationRegion& shared_region = _direct_allocation_regions[idx];\n+      ShenandoahHeapRegion* const r = shared_region._address;\n+      \/\/ Found the first eligible region\n+      if (r == nullptr) {\n+        return idx;\n+      }\n+      if (r->free() < PLAB::min_size_bytes()) {\n+        assert(r->reserved_for_direct_allocation(), \"Must be direct allocation reserved region.\");\n+        AtomicAccess::store(&shared_region._address, static_cast<ShenandoahHeapRegion*>(nullptr));\n+        OrderAccess::fence();\n+        while (r->direct_alloc_mutators() > 0) {\n+          if (os::is_MP()) SpinPause();\n+          else os::naked_yield();\n+        }\n+        r->release_from_direct_allocation();\n+        return idx;\n+      }\n+\n+      if (_obj == nullptr && _next_region_with_sufficient_mem == ShenandoahDirectlyAllocatableRegionCount && r->free() >= _min_req_byte_size) {\n+        _next_region_with_sufficient_mem = idx;\n+      }\n+    }\n+    return -1;\n+  }\n+\n+  bool heap_region_do(ShenandoahHeapRegion *r) {\n+    if (_next_retire_eligible_region == -1 && _obj != nullptr) return true;\n+    size_t ac = _free_set->alloc_capacity(r);\n+    if (ac < _min_req_byte_size) return false;\n+    if (r->reserved_for_direct_allocation()) return false;\n+    if (ShenandoahHeap::heap()->is_concurrent_weak_root_in_progress() && r->is_trash()) {\n+      return false;\n+    }\n+\n+    if (r->is_trash() || r->is_empty()) {\n+      r->try_recycle_under_lock();\n+      assert(r->affiliation() == FREE, \"Empty region must be free\");\n+      r->set_affiliation(YOUNG_GENERATION);\n+      r->make_regular_allocation(YOUNG_GENERATION);\n+      ShenandoahHeap::heap()->generation_for(r->affiliation())->increment_affiliated_region_count();\n+    } else {\n+      assert(r->affiliation() != FREE, \"Non-empty region must not be free\");\n+      if (r->affiliation() == OLD_GENERATION) return  false;\n+    }\n+    assert(r->is_affiliated(), \"Region %zu must be affiliated\", r->index());\n+    assert(r->affiliation() == YOUNG_GENERATION, \"Region %zu must be affiliated with YOUNG_GENERATION\", r->index());\n+\n+    if (_obj == nullptr) {\n+      _in_new_region = r->is_empty();\n+      size_t actual_size = _req.size();\n+      _obj = _req.is_lab_alloc() ? r->allocate_lab(_req, actual_size) : r->allocate(actual_size, _req);\n+      assert(_obj != nullptr, \"Must have successfully allocated the object.\");\n+      _req.set_actual_size(actual_size);\n+      _free_set->partitions()->increase_used(ShenandoahFreeSetPartitionId::Mutator, actual_size * HeapWordSize);\n+      ac = _free_set->alloc_capacity(r);\n+    }\n+\n+    if (_next_retire_eligible_region != -1 && ac >= ShenandoahHeapRegion::max_tlab_size_bytes()) {\n+      \/\/ After satisfying object allocation, the region still has space to fit at least one tlab.\n+      ShenandoahDirectAllocationRegion& shared_region = _direct_allocation_regions[_next_retire_eligible_region];\n+      assert(AtomicAccess::load(&shared_region._address) == nullptr, \"Must have been released.\");\n+      r->reserve_for_direct_allocation();\n+      \/\/ Unassign the region from Mutator partition.\n+      _free_set->partitions()->raw_unassign_membership(r->index(), ShenandoahFreeSetPartitionId::Mutator);\n+      _free_set->partitions()->shrink_interval_if_boundary_modified(ShenandoahFreeSetPartitionId::Mutator, r->index());\n+      _free_set->partitions()->increase_used(ShenandoahFreeSetPartitionId::Mutator, ac);\n+      OrderAccess::fence();\n+      AtomicAccess::store(&shared_region._address, r);\n+      if (!_new_region_allocated) {\n+        _new_region_allocated = true;\n+      }\n+      _next_retire_eligible_region = find_next_retire_eligible_region();\n+    } else if (ac < PLAB::min_size_bytes()) {\n+      \/\/ if the remaining memory is less than PLAB:min_size_bytes(), retire this region\n+      _free_set->partitions()->retire_from_partition(ShenandoahFreeSetPartitionId::Mutator, r->index(), r->used());\n+      _free_set->partitions()->assert_bounds();\n+    }\n+\n+    return _next_retire_eligible_region == -1 && _obj != nullptr;\n+  }\n+};\n+\n+bool ShenandoahFreeSet::try_allocate_directly_allocatable_regions(uint start_index,\n+                                                                  bool replace_all_eligible_regions,\n+                                                                  ShenandoahAllocRequest &req,\n+                                                                  HeapWord* &obj,\n+                                                                  bool& in_new_region,\n+                                                                  uint& new_start_index) {\n+  assert(Thread::current()->is_Java_thread(), \"Must be mutator\");\n+  assert(start_index < ShenandoahDirectlyAllocatableRegionCount, \"Must be\");\n+  shenandoah_assert_not_heaplocked();\n+\n+  ShenandoahHeapLocker locker(ShenandoahHeap::heap()->lock(), true);\n+  DirectAllocatableRegionRefillClosure cl(_direct_allocation_regions, start_index, replace_all_eligible_regions, req, obj, in_new_region);\n+  iterate_regions_for_alloc<true, false>(&cl, false);\n+  if (cl._next_region_with_sufficient_mem != ShenandoahDirectlyAllocatableRegionCount && obj == nullptr) {\n+    new_start_index = cl._next_region_with_sufficient_mem;\n+  }\n+  return cl._new_region_allocated;\n+}\n+\n+void ShenandoahFreeSet::release_all_directly_allocatable_regions() {\n+  assert_at_safepoint();\n+  shenandoah_assert_heaplocked();\n+  for (uint i = 0; i < ShenandoahDirectlyAllocatableRegionCount; i++) {\n+    ShenandoahDirectAllocationRegion& shared_region = _direct_allocation_regions[i];\n+    ShenandoahHeapRegion* r = AtomicAccess::load_acquire(&shared_region._address);\n+    if (r != nullptr) {\n+      assert(r->reserved_for_direct_allocation(), \"Must be\");\n+      AtomicAccess::store(&shared_region._address, static_cast<ShenandoahHeapRegion*>(nullptr));\n+      r->release_from_direct_allocation();\n+    }\n+  }\n+}\n+\n+void ShenandoahFreeSet::release_directly_allocatable_region(ShenandoahHeapRegion* region) {\n+  assert_at_safepoint();\n+  shenandoah_assert_heaplocked();\n+  for (uint i = 0u; i < ShenandoahDirectlyAllocatableRegionCount; i++) {\n+    ShenandoahDirectAllocationRegion& shared_region = _direct_allocation_regions[i];\n+    if (AtomicAccess::load(&shared_region._address) == region) {\n+      AtomicAccess::store(&shared_region._address, static_cast<ShenandoahHeapRegion*>(nullptr));\n+      break;\n+    }\n+  }\n+  region->release_from_direct_allocation();\n+  if (region->free() >= PLAB::min_size_bytes()) {\n+    partitions()->raw_assign_membership(region->index(), ShenandoahFreeSetPartitionId::Mutator);\n+    partitions()->expand_interval_if_boundary_modified(ShenandoahFreeSetPartitionId::Mutator, region->index(), region->free());\n+    partitions()->decrease_used(ShenandoahFreeSetPartitionId::Mutator, region->free());\n+  }\n+}\n+\n+template<bool IS_MUTATOR, bool IS_OLD>\n+uint ShenandoahFreeSet::iterate_regions_for_alloc(ShenandoahHeapRegionIterationClosure* cl, bool use_empty) {\n+  assert((IS_MUTATOR && !IS_OLD) || !IS_MUTATOR, \"Sanity check\");\n+  ShenandoahFreeSetPartitionId partition = IS_MUTATOR ? ShenandoahFreeSetPartitionId::Mutator :\n+                                           (IS_OLD ? ShenandoahFreeSetPartitionId::OldCollector : ShenandoahFreeSetPartitionId::Mutator);\n+  if (_partitions.is_empty(partition)) {\n+    return 0u;\n+  }\n+\n+  if (IS_MUTATOR) {\n+    update_allocation_bias();\n+  }\n+\n+  if (_partitions.alloc_from_left_bias(partition)) {\n+    ShenandoahLeftRightIterator iterator(&_partitions, partition, use_empty);\n+    return iterate_regions_for_alloc(iterator, cl);\n+  } else {\n+    ShenandoahRightLeftIterator iterator(&_partitions, partition, use_empty);\n+    return iterate_regions_for_alloc(iterator, cl);\n+  }\n+}\n+\n+template<typename Iter>\n+uint ShenandoahFreeSet::iterate_regions_for_alloc(Iter& iterator, ShenandoahHeapRegionIterationClosure* cl) {\n+  uint regions_iterated = 0u;\n+  for (idx_t idx = iterator.current(); iterator.has_next(); idx = iterator.next()) {\n+    regions_iterated++;\n+    ShenandoahHeapRegion* r = _heap->get_region(idx);\n+    if (cl->heap_region_do(r)) {\n+      break;\n+    }\n+  }\n+  return regions_iterated;\n+}\n+\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahFreeSet.cpp","additions":416,"deletions":24,"binary":false,"changes":440,"status":"modified"},{"patch":"@@ -32,0 +32,1 @@\n+#include \"memory\/padded.inline.hpp\"\n@@ -47,1 +48,2 @@\n-\n+friend class ShenandoahFreeSet;\n+friend class DirectAllocatableRegionRefillClosure;\n@@ -122,0 +124,4 @@\n+  inline void raw_unassign_membership(size_t idx, ShenandoahFreeSetPartitionId p) {\n+    _membership[int(p)].clear_bit(idx);\n+  }\n+\n@@ -203,0 +209,1 @@\n+  inline void decrease_used(ShenandoahFreeSetPartitionId which_partition, size_t bytes);\n@@ -248,1 +255,1 @@\n-    return _available[int(which_partition)];\n+    return available_in(which_partition);\n@@ -256,0 +263,1 @@\n+    AtomicAccess::store(_capacity + int(which_partition), value);\n@@ -263,0 +271,1 @@\n+    AtomicAccess::store(_used + int(which_partition), value);\n@@ -290,0 +299,23 @@\n+#define DIRECTLY_ALLOCATABLE_REGION_UNKNOWN_AFFINITY ((Thread*)-1)\n+#define DIRECTLY_ALLOCATABLE_REGION_UNKNOWN_SELF     ((Thread*)-2)\n+\/\/ When mutator threads allocate from directly allocatable regions, ideally the allocation should be evenly\n+\/\/ distributed to all the directly allocatable regions, random is the best portable option for this, but with random\n+\/\/ distribution it may worsen memory locality, e.g. two consecutive allocation from same thread are randomly\n+\/\/ distributed to different allocatable regions. ShenandoahDirectlyAllocatableRegionAffinity solves\/mitigates\n+\/\/ the memory locality issue.\n+\/\/ The idea and code is borrowed from ZGC's CPU affinity, but with random number instead of CPU id.\n+class ShenandoahDirectlyAllocatableRegionAffinity : public AllStatic {\n+  struct Affinity {\n+    Thread* _thread;\n+  };\n+\n+  static PaddedEnd<Affinity>* _affinity;\n+  static THREAD_LOCAL Thread* _self;\n+  static THREAD_LOCAL uint    _index;\n+  static uint index_slow();\n+public:\n+  static void initialize();\n+  static uint index();\n+  static void set_index(uint index);\n+};\n+\n@@ -314,0 +346,10 @@\n+struct ShenandoahDirectAllocationRegion {\n+  ShenandoahHeapRegion* volatile _address = nullptr;\n+};\n+\n+class ShenandoahHeapRegionIterationClosure : public StackObj {\n+public:\n+  \/\/ Return true to break the iteration loop.\n+  virtual bool heap_region_do(ShenandoahHeapRegion *r) { return false; };\n+};\n+\n@@ -315,0 +357,1 @@\n+  friend class DirectAllocatableRegionRefillClosure;\n@@ -318,0 +361,1 @@\n+  PaddedEnd<ShenandoahDirectAllocationRegion>* _direct_allocation_regions;\n@@ -413,0 +457,19 @@\n+  template<bool IS_TLAB>\n+  HeapWord* cas_allocate_single_for_mutator(\n+    uint probe_start, uint probe_count, ShenandoahAllocRequest &req, bool &in_new_region);\n+\n+  template<bool IS_TLAB>\n+  HeapWord* cas_allocate_in_for_mutator(ShenandoahHeapRegion* region, ShenandoahAllocRequest &req, bool &in_new_region);\n+\n+  bool try_allocate_directly_allocatable_regions(uint start_index,\n+                                                 bool replace_all_eligible_regions,\n+                                                 ShenandoahAllocRequest &req,\n+                                                 HeapWord* &obj,\n+                                                 bool &in_new_region,\n+                                                 uint& new_start_index);\n+  template<bool IS_MUTATOR, bool IS_OLD>\n+  uint iterate_regions_for_alloc(ShenandoahHeapRegionIterationClosure* cl, bool use_empty);\n+\n+  template<typename Iter>\n+  uint iterate_regions_for_alloc(Iter& iterator, ShenandoahHeapRegionIterationClosure* cl);\n+\n@@ -421,0 +484,3 @@\n+  ShenandoahRegionPartitions* partitions() {\n+    return &_partitions;\n+  }\n@@ -487,0 +553,11 @@\n+  HeapWord* allocate_humongous(ShenandoahAllocRequest &req);\n+\n+  HeapWord* allocate_contiguous_cds(ShenandoahAllocRequest &req);\n+\n+  void release_all_directly_allocatable_regions();\n+\n+  void release_directly_allocatable_region(ShenandoahHeapRegion *region);\n+\n+  template<bool IS_TLAB>\n+  HeapWord* try_allocate_single_for_mutator(ShenandoahAllocRequest &req, bool &in_new_region);\n+\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahFreeSet.hpp","additions":79,"deletions":2,"binary":false,"changes":81,"status":"modified"},{"patch":"@@ -222,0 +222,5 @@\n+  {\n+    ShenandoahHeapLocker locker(heap->lock());\n+    heap->free_set()->release_all_directly_allocatable_regions();\n+  }\n+\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahFullGC.cpp","additions":5,"deletions":0,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -566,1 +566,1 @@\n-        if (!heap->is_concurrent_old_mark_in_progress() && tams == original_top) {\n+        if (!heap->is_concurrent_old_mark_in_progress() && tams == original_top && !r->reserved_for_direct_allocation()) {\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahGeneration.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -970,1 +970,1 @@\n-      result = allocate_memory_under_lock(req, in_new_region);\n+      result = allocate_memory_for_mutator(req, in_new_region);\n@@ -1002,1 +1002,1 @@\n-        result = allocate_memory_under_lock(req, in_new_region);\n+        result = allocate_memory_for_mutator(req, in_new_region);\n@@ -1047,0 +1047,20 @@\n+HeapWord* ShenandoahHeap::allocate_memory_for_mutator(ShenandoahAllocRequest& req, bool& in_new_region) {\n+  assert(req.is_mutator_alloc(), \"Sanity\");\n+  assert(!req.is_old(), \"Sanity\");\n+  shenandoah_assert_not_heaplocked();\n+  ShenandoahFreeSet* free_set = ShenandoahHeap::free_set();\n+  if (ShenandoahHeapRegion::requires_humongous(req.size())) {\n+    in_new_region = true;\n+    if (req.type() == ShenandoahAllocRequest::_alloc_cds) {\n+      return free_set->allocate_contiguous_cds(req);\n+    } else {\n+      return free_set->allocate_humongous(req);\n+    }\n+  }\n+  if (req.is_lab_alloc()) {\n+    return free_set->try_allocate_single_for_mutator<true>(req, in_new_region);\n+  } else {\n+    return free_set->try_allocate_single_for_mutator<false>(req, in_new_region);\n+  }\n+}\n+\n@@ -2860,1 +2880,1 @@\n-    assert(r->is_regular(), \"Must be regular\");\n+    assert(r->is_regular(), \"Must be regular, state: %s\", r->region_state_to_string(r->state()));\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeap.cpp","additions":23,"deletions":3,"binary":false,"changes":26,"status":"modified"},{"patch":"@@ -701,0 +701,1 @@\n+  HeapWord* allocate_memory_for_mutator(ShenandoahAllocRequest& request, bool& in_new_region);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeap.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -92,0 +92,1 @@\n+  _direct_alloc_reserved.unset();\n@@ -363,3 +364,3 @@\n-  _tlab_allocs = 0;\n-  _gclab_allocs = 0;\n-  _plab_allocs = 0;\n+  AtomicAccess::store(&_tlab_allocs, size_t(0));\n+  AtomicAccess::store(&_gclab_allocs, size_t(0));\n+  AtomicAccess::store(&_plab_allocs, size_t(0));\n@@ -369,1 +370,1 @@\n-  return used() - (_tlab_allocs + _gclab_allocs + _plab_allocs) * HeapWordSize;\n+  return used() - (AtomicAccess::load(&_tlab_allocs) + AtomicAccess::load(&_gclab_allocs) + AtomicAccess::load(&_plab_allocs)) * HeapWordSize;\n@@ -373,1 +374,1 @@\n-  return _tlab_allocs * HeapWordSize;\n+  return AtomicAccess::load(&_tlab_allocs) * HeapWordSize;\n@@ -377,1 +378,1 @@\n-  return _gclab_allocs * HeapWordSize;\n+  return AtomicAccess::load(&_gclab_allocs) * HeapWordSize;\n@@ -381,1 +382,5 @@\n-  return _plab_allocs * HeapWordSize;\n+  return AtomicAccess::load(&_plab_allocs) * HeapWordSize;\n+}\n+\n+bool ShenandoahHeapRegion::has_allocs() const {\n+  return top() > bottom();\n@@ -854,0 +859,2 @@\n+  assert(new_affiliation != OLD_GENERATION || !reserved_for_direct_allocation(), \"Reserved region can't move to old\");\n+\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeapRegion.cpp","additions":14,"deletions":7,"binary":false,"changes":21,"status":"modified"},{"patch":"@@ -253,1 +253,1 @@\n-  HeapWord* _top;\n+  HeapWord* volatile _top;\n@@ -255,3 +255,3 @@\n-  size_t _tlab_allocs;\n-  size_t _gclab_allocs;\n-  size_t _plab_allocs;\n+  size_t volatile _tlab_allocs;\n+  size_t volatile _gclab_allocs;\n+  size_t volatile _plab_allocs;\n@@ -264,2 +264,2 @@\n-  uint _age;\n-  CENSUS_NOISE(uint _youth;)   \/\/ tracks epochs of retrograde ageing (rejuvenation)\n+  volatile uint _age;\n+  CENSUS_NOISE(volatile uint _youth;)   \/\/ tracks epochs of retrograde ageing (rejuvenation)\n@@ -271,0 +271,14 @@\n+  ShenandoahSharedFlag _direct_alloc_reserved; \/\/ Flag to indicate that whether the region is reserved for lock-free direct allocation\n+  volatile int _direct_alloc_mutators = 0;\n+\n+  class DirectAllocMutatorCounter {\n+  private:\n+    int volatile* _counter;\n+  public:\n+    DirectAllocMutatorCounter(int volatile* counter): _counter(counter) {\n+      AtomicAccess::inc(_counter);\n+    }\n+    ~DirectAllocMutatorCounter() {\n+      AtomicAccess::dec(_counter);\n+    }\n+  };\n@@ -369,0 +383,9 @@\n+  inline HeapWord* allocate_lab(const ShenandoahAllocRequest &req, size_t &actual_size);\n+\n+  \/\/ Atomic allocation using CAS, return nullptr if full or no enough space for the req\n+  inline HeapWord* allocate_atomic(size_t word_size, const ShenandoahAllocRequest &req);\n+\n+  inline HeapWord* allocate_lab_atomic(const ShenandoahAllocRequest &req, size_t &actual_size);\n+\n+  inline bool try_allocate(HeapWord* const obj, size_t const size);\n+\n@@ -428,2 +451,6 @@\n-  HeapWord* top() const         { return _top;     }\n-  void set_top(HeapWord* v)     { _top = v;        }\n+  HeapWord* top() const {\n+    return AtomicAccess::load(&_top);\n+  }\n+  void set_top(HeapWord* v) {\n+    AtomicAccess::store(&_top, v);\n+  }\n@@ -453,0 +480,1 @@\n+  bool has_allocs() const;\n@@ -464,2 +492,2 @@\n-  uint age() const { return _age; }\n-  CENSUS_NOISE(uint youth() const { return _youth; })\n+  uint age() const { return AtomicAccess::load(&_age); }\n+  CENSUS_NOISE(uint youth() const { return AtomicAccess::load(&_youth); })\n@@ -468,4 +496,5 @@\n-    const uint max_age = markWord::max_age;\n-    assert(_age <= max_age, \"Error\");\n-    if (_age++ >= max_age) {\n-      _age = max_age;   \/\/ clamp\n+    const uint current_age = age();\n+    assert(current_age <= markWord::max_age, \"Error\");\n+    if (current_age < markWord::max_age) {\n+      const uint old = AtomicAccess::cmpxchg(&_age, current_age, current_age + 1);\n+      assert(old == current_age || old == 0u, \"Only fail when any mutator reset the age.\");\n@@ -476,2 +505,8 @@\n-    CENSUS_NOISE(_youth += _age;)\n-    _age = 0;\n+    uint current = age();\n+    uint old;\n+    while ((old = current) != 0u &&\n+           (current = AtomicAccess::cmpxchg(&_age, old, 0u)) != old &&\n+           current != 0u) { }\n+    if (current != 0u) {\n+      CENSUS_NOISE(AtomicAccess::add(&_youth, current, memory_order_relaxed);)\n+    }\n@@ -480,1 +515,1 @@\n-  CENSUS_NOISE(void clear_youth() { _youth = 0; })\n+  CENSUS_NOISE(void clear_youth() { AtomicAccess::store(&_youth,  0u); })\n@@ -494,0 +529,18 @@\n+  inline void reserve_for_direct_allocation() {\n+    assert(_direct_alloc_reserved.is_unset(), \"Must be\");\n+    _direct_alloc_reserved.set();\n+  }\n+\n+  inline void release_from_direct_allocation() {\n+    assert(_direct_alloc_reserved.is_set(), \"Must be\");\n+    _direct_alloc_reserved.unset();\n+  }\n+\n+  inline bool reserved_for_direct_allocation() const {\n+    return _direct_alloc_reserved.is_set();\n+  }\n+\n+  inline int direct_alloc_mutators() const {\n+    return AtomicAccess::load(&_direct_alloc_mutators);\n+  }\n+\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeapRegion.hpp","additions":70,"deletions":17,"binary":false,"changes":87,"status":"modified"},{"patch":"@@ -111,0 +111,83 @@\n+HeapWord* ShenandoahHeapRegion::allocate_lab(const ShenandoahAllocRequest& req, size_t &actual_size) {\n+  shenandoah_assert_heaplocked_or_safepoint();\n+  assert(req.is_lab_alloc(), \"Only lab alloc\");\n+  assert(this->affiliation() == req.affiliation(), \"Region affiliation should already be established\");\n+\n+  size_t adjusted_size = req.size();\n+  HeapWord* obj = nullptr;\n+  HeapWord* old_top = top();\n+  size_t free_words = align_down(byte_size(old_top, end()) >> LogHeapWordSize, MinObjAlignment);\n+  if (adjusted_size > free_words) {\n+    adjusted_size = free_words;\n+  }\n+  if (adjusted_size >= req.min_size()) {\n+    obj = allocate(adjusted_size, req);\n+    actual_size = adjusted_size;\n+    assert(obj == old_top, \"Must be\");\n+  }\n+  return obj;\n+}\n+\n+HeapWord* ShenandoahHeapRegion::allocate_atomic(size_t size, const ShenandoahAllocRequest& req) {\n+  DirectAllocMutatorCounter c(&_direct_alloc_mutators);\n+  assert(is_object_aligned(size), \"alloc size breaks alignment: %zu\", size);\n+  assert(this->affiliation() == req.affiliation(), \"Region affiliation should already be established\");\n+  assert(this->is_regular() || this->is_regular_pinned(), \"must be a regular region\");\n+\n+  for (;;) {\n+    if (!reserved_for_direct_allocation()) {\n+      return nullptr;\n+    }\n+    HeapWord* obj = top();\n+    if (pointer_delta(end(), obj) >= size) {\n+      if (try_allocate(obj, size)) {\n+        reset_age();\n+        adjust_alloc_metadata(req.type(), size);\n+        return obj;\n+      }\n+    } else {\n+      return nullptr;\n+    }\n+  }\n+}\n+\n+HeapWord* ShenandoahHeapRegion::allocate_lab_atomic(const ShenandoahAllocRequest& req, size_t &actual_size) {\n+  DirectAllocMutatorCounter c(&_direct_alloc_mutators);\n+  assert(req.is_lab_alloc(), \"Only lab alloc\");\n+  assert(this->affiliation() == req.affiliation(), \"Region affiliation should already be established\");\n+  assert(this->is_regular() || this->is_regular_pinned(), \"must be a regular region\");\n+  size_t adjusted_size = req.size();\n+  for (;;) {\n+    if (!reserved_for_direct_allocation()) {\n+      return nullptr;\n+    }\n+    HeapWord* obj = top();\n+    size_t free_words = align_down(byte_size(obj, end()) >> LogHeapWordSize, MinObjAlignment);\n+    if (adjusted_size > free_words) {\n+      adjusted_size = free_words;\n+    }\n+    if (adjusted_size >= req.min_size()) {\n+      if (try_allocate(obj, adjusted_size)) {\n+        reset_age();\n+        actual_size = adjusted_size;\n+        adjust_alloc_metadata(req.type(), adjusted_size);\n+        return obj;\n+      }\n+    } else {\n+      log_trace(gc, free)(\"Failed to shrink TLAB or GCLAB request (%zu) in region %zu to %zu\"\n+                          \" because min_size() is %zu\", req.size(), index(), adjusted_size, req.min_size());\n+      return nullptr;\n+    }\n+  }\n+}\n+\n+bool ShenandoahHeapRegion::try_allocate(HeapWord* const obj, size_t const size) {\n+  HeapWord* new_top = obj + size;\n+  if (AtomicAccess::cmpxchg(&_top, obj, new_top) == obj) {\n+    assert(is_object_aligned(new_top), \"new top breaks alignment: \" PTR_FORMAT, p2i(new_top));\n+    assert(is_object_aligned(obj),     \"obj is not aligned: \"       PTR_FORMAT, p2i(obj));\n+    return true;\n+  }\n+  return false;\n+}\n+\n@@ -119,1 +202,1 @@\n-      _tlab_allocs += size;\n+      AtomicAccess::add(&_tlab_allocs, size);\n@@ -122,1 +205,1 @@\n-      _gclab_allocs += size;\n+      AtomicAccess::add(&_gclab_allocs, size);\n@@ -125,1 +208,1 @@\n-      _plab_allocs += size;\n+      AtomicAccess::add(&_plab_allocs, size);\n@@ -219,1 +302,1 @@\n-  _top_before_promoted = _top;\n+  _top_before_promoted = top();\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeapRegion.inline.hpp","additions":87,"deletions":4,"binary":false,"changes":91,"status":"modified"},{"patch":"@@ -552,0 +552,11 @@\n+                                                                            \\\n+  product(uintx, ShenandoahDirectlyAllocatableRegionCount, 13, EXPERIMENTAL,\\\n+         \"Number of regions Shenandoah will pre-allocate for \"              \\\n+         \"direct allocation with CAS, the values should less than \"         \\\n+         \"number of CPU cores. Ideally it should be a prime number.\")       \\\n+         range(1, 128)                                                      \\\n+                                                                            \\\n+  product(uintx, ShenandoahDirectAllocationMaxProbes, 3, EXPERIMENTAL,      \\\n+         \"Max number of region to probe for direct allocation without \"     \\\n+         \"stealing space from other directly allocatable regions.    \")     \\\n+         range(1, 128)                                                      \\\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoah_globals.hpp","additions":11,"deletions":0,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -45,1 +45,1 @@\n-  nonstatic_field(ShenandoahHeapRegion, _top,                      HeapWord*)                         \\\n+  volatile_nonstatic_field(ShenandoahHeapRegion, _top,             HeapWord*)                         \\\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/vmStructs_shenandoah.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"}]}