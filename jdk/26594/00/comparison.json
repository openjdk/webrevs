{"files":[{"patch":"@@ -16164,32 +16164,0 @@\n-instruct cmpFastLock(rFlagsReg cr, iRegP object, iRegP box, iRegPNoSp tmp, iRegPNoSp tmp2, iRegPNoSp tmp3)\n-%{\n-  predicate(LockingMode != LM_LIGHTWEIGHT);\n-  match(Set cr (FastLock object box));\n-  effect(TEMP tmp, TEMP tmp2, TEMP tmp3);\n-\n-  ins_cost(5 * INSN_COST);\n-  format %{ \"fastlock $object,$box\\t! kills $tmp,$tmp2,$tmp3\" %}\n-\n-  ins_encode %{\n-    __ fast_lock($object$$Register, $box$$Register, $tmp$$Register, $tmp2$$Register, $tmp3$$Register);\n-  %}\n-\n-  ins_pipe(pipe_serial);\n-%}\n-\n-instruct cmpFastUnlock(rFlagsReg cr, iRegP object, iRegP box, iRegPNoSp tmp, iRegPNoSp tmp2)\n-%{\n-  predicate(LockingMode != LM_LIGHTWEIGHT);\n-  match(Set cr (FastUnlock object box));\n-  effect(TEMP tmp, TEMP tmp2);\n-\n-  ins_cost(5 * INSN_COST);\n-  format %{ \"fastunlock $object,$box\\t! kills $tmp, $tmp2\" %}\n-\n-  ins_encode %{\n-    __ fast_unlock($object$$Register, $box$$Register, $tmp$$Register, $tmp2$$Register);\n-  %}\n-\n-  ins_pipe(pipe_serial);\n-%}\n-\n@@ -16198,1 +16166,0 @@\n-  predicate(LockingMode == LM_LIGHTWEIGHT);\n@@ -16214,1 +16181,0 @@\n-  predicate(LockingMode == LM_LIGHTWEIGHT);\n","filename":"src\/hotspot\/cpu\/aarch64\/aarch64.ad","additions":0,"deletions":34,"binary":false,"changes":34,"status":"modified"},{"patch":"@@ -413,5 +413,1 @@\n-    if (LockingMode == LM_MONITOR) {\n-      __ b(*stub->entry());\n-    } else {\n-      __ unlock_object(r5, r4, r0, r6, *stub->entry());\n-    }\n+    __ unlock_object(r5, r4, r0, r6, *stub->entry());\n@@ -2487,7 +2483,1 @@\n-  if (LockingMode == LM_MONITOR) {\n-    if (op->info() != nullptr) {\n-      add_debug_info_for_null_check_here(op->info());\n-      __ null_check(obj, -1);\n-    }\n-    __ b(*op->stub()->entry());\n-  } else if (op->code() == lir_lock) {\n+  if (op->code() == lir_lock) {\n","filename":"src\/hotspot\/cpu\/aarch64\/c1_LIRAssembler_aarch64.cpp","additions":2,"deletions":12,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -63,2 +63,0 @@\n-  const int aligned_mask = BytesPerWord -1;\n-  const int hdr_offset = oopDesc::mark_offset_in_bytes();\n@@ -75,49 +73,2 @@\n-  if (LockingMode == LM_LIGHTWEIGHT) {\n-    lightweight_lock(disp_hdr, obj, hdr, temp, rscratch2, slow_case);\n-  } else if (LockingMode == LM_LEGACY) {\n-\n-    if (DiagnoseSyncOnValueBasedClasses != 0) {\n-      load_klass(hdr, obj);\n-      ldrb(hdr, Address(hdr, Klass::misc_flags_offset()));\n-      tst(hdr, KlassFlags::_misc_is_value_based_class);\n-      br(Assembler::NE, slow_case);\n-    }\n-\n-    Label done;\n-    \/\/ Load object header\n-    ldr(hdr, Address(obj, hdr_offset));\n-    \/\/ and mark it as unlocked\n-    orr(hdr, hdr, markWord::unlocked_value);\n-    \/\/ save unlocked object header into the displaced header location on the stack\n-    str(hdr, Address(disp_hdr, 0));\n-    \/\/ test if object header is still the same (i.e. unlocked), and if so, store the\n-    \/\/ displaced header address in the object header - if it is not the same, get the\n-    \/\/ object header instead\n-    lea(rscratch2, Address(obj, hdr_offset));\n-    cmpxchgptr(hdr, disp_hdr, rscratch2, rscratch1, done, \/*fallthough*\/nullptr);\n-    \/\/ if the object header was the same, we're done\n-    \/\/ if the object header was not the same, it is now in the hdr register\n-    \/\/ => test if it is a stack pointer into the same stack (recursive locking), i.e.:\n-    \/\/\n-    \/\/ 1) (hdr & aligned_mask) == 0\n-    \/\/ 2) sp <= hdr\n-    \/\/ 3) hdr <= sp + page_size\n-    \/\/\n-    \/\/ these 3 tests can be done by evaluating the following expression:\n-    \/\/\n-    \/\/ (hdr - sp) & (aligned_mask - page_size)\n-    \/\/\n-    \/\/ assuming both the stack pointer and page_size have their least\n-    \/\/ significant 2 bits cleared and page_size is a power of 2\n-    mov(rscratch1, sp);\n-    sub(hdr, hdr, rscratch1);\n-    ands(hdr, hdr, aligned_mask - (int)os::vm_page_size());\n-    \/\/ for recursive locking, the result is zero => save it in the displaced header\n-    \/\/ location (null in the displaced hdr location indicates recursive locking)\n-    str(hdr, Address(disp_hdr, 0));\n-    \/\/ otherwise we don't care about the result and handle locking via runtime call\n-    cbnz(hdr, slow_case);\n-    \/\/ done\n-    bind(done);\n-    inc_held_monitor_count(rscratch1);\n-  }\n+  lightweight_lock(disp_hdr, obj, hdr, temp, rscratch2, slow_case);\n+\n@@ -129,2 +80,0 @@\n-  const int aligned_mask = BytesPerWord -1;\n-  const int hdr_offset = oopDesc::mark_offset_in_bytes();\n@@ -132,9 +81,0 @@\n-  Label done;\n-\n-  if (LockingMode != LM_LIGHTWEIGHT) {\n-    \/\/ load displaced header\n-    ldr(hdr, Address(disp_hdr, 0));\n-    \/\/ if the loaded hdr is null we had recursive locking\n-    \/\/ if we had recursive locking, we are done\n-    cbz(hdr, done);\n-  }\n@@ -146,18 +86,1 @@\n-  if (LockingMode == LM_LIGHTWEIGHT) {\n-    lightweight_unlock(obj, hdr, temp, rscratch2, slow_case);\n-  } else if (LockingMode == LM_LEGACY) {\n-    \/\/ test if object header is pointing to the displaced header, and if so, restore\n-    \/\/ the displaced header in the object - if the object header is not pointing to\n-    \/\/ the displaced header, get the object header instead\n-    \/\/ if the object header was not pointing to the displaced header,\n-    \/\/ we do unlocking via runtime call\n-    if (hdr_offset) {\n-      lea(rscratch1, Address(obj, hdr_offset));\n-      cmpxchgptr(disp_hdr, hdr, rscratch1, rscratch2, done, &slow_case);\n-    } else {\n-      cmpxchgptr(disp_hdr, hdr, obj, rscratch2, done, &slow_case);\n-    }\n-    \/\/ done\n-    bind(done);\n-    dec_held_monitor_count(rscratch1);\n-  }\n+  lightweight_unlock(obj, hdr, temp, rscratch2, slow_case);\n","filename":"src\/hotspot\/cpu\/aarch64\/c1_MacroAssembler_aarch64.cpp","additions":3,"deletions":80,"binary":false,"changes":83,"status":"modified"},{"patch":"@@ -150,206 +150,0 @@\n-void C2_MacroAssembler::fast_lock(Register objectReg, Register boxReg, Register tmpReg,\n-                                  Register tmp2Reg, Register tmp3Reg) {\n-  Register oop = objectReg;\n-  Register box = boxReg;\n-  Register disp_hdr = tmpReg;\n-  Register tmp = tmp2Reg;\n-  Label cont;\n-  Label object_has_monitor;\n-  Label count, no_count;\n-\n-  assert(LockingMode != LM_LIGHTWEIGHT, \"lightweight locking should use fast_lock_lightweight\");\n-  assert_different_registers(oop, box, tmp, disp_hdr, rscratch2);\n-\n-  \/\/ Load markWord from object into displaced_header.\n-  ldr(disp_hdr, Address(oop, oopDesc::mark_offset_in_bytes()));\n-\n-  if (DiagnoseSyncOnValueBasedClasses != 0) {\n-    load_klass(tmp, oop);\n-    ldrb(tmp, Address(tmp, Klass::misc_flags_offset()));\n-    tst(tmp, KlassFlags::_misc_is_value_based_class);\n-    br(Assembler::NE, cont);\n-  }\n-\n-  \/\/ Check for existing monitor\n-  tbnz(disp_hdr, exact_log2(markWord::monitor_value), object_has_monitor);\n-\n-  if (LockingMode == LM_MONITOR) {\n-    tst(oop, oop); \/\/ Set NE to indicate 'failure' -> take slow-path. We know that oop != 0.\n-    b(cont);\n-  } else {\n-    assert(LockingMode == LM_LEGACY, \"must be\");\n-    \/\/ Set tmp to be (markWord of object | UNLOCK_VALUE).\n-    orr(tmp, disp_hdr, markWord::unlocked_value);\n-\n-    \/\/ Initialize the box. (Must happen before we update the object mark!)\n-    str(tmp, Address(box, BasicLock::displaced_header_offset_in_bytes()));\n-\n-    \/\/ Compare object markWord with an unlocked value (tmp) and if\n-    \/\/ equal exchange the stack address of our box with object markWord.\n-    \/\/ On failure disp_hdr contains the possibly locked markWord.\n-    cmpxchg(oop, tmp, box, Assembler::xword, \/*acquire*\/ true,\n-            \/*release*\/ true, \/*weak*\/ false, disp_hdr);\n-    br(Assembler::EQ, cont);\n-\n-    assert(oopDesc::mark_offset_in_bytes() == 0, \"offset of _mark is not 0\");\n-\n-    \/\/ If the compare-and-exchange succeeded, then we found an unlocked\n-    \/\/ object, will have now locked it will continue at label cont\n-\n-    \/\/ Check if the owner is self by comparing the value in the\n-    \/\/ markWord of object (disp_hdr) with the stack pointer.\n-    mov(rscratch1, sp);\n-    sub(disp_hdr, disp_hdr, rscratch1);\n-    mov(tmp, (address) (~(os::vm_page_size()-1) | markWord::lock_mask_in_place));\n-    \/\/ If condition is true we are cont and hence we can store 0 as the\n-    \/\/ displaced header in the box, which indicates that it is a recursive lock.\n-    ands(tmp\/*==0?*\/, disp_hdr, tmp);   \/\/ Sets flags for result\n-    str(tmp\/*==0, perhaps*\/, Address(box, BasicLock::displaced_header_offset_in_bytes()));\n-    b(cont);\n-  }\n-\n-  \/\/ Handle existing monitor.\n-  bind(object_has_monitor);\n-\n-  \/\/ Try to CAS owner (no owner => current thread's _monitor_owner_id).\n-  ldr(rscratch2, Address(rthread, JavaThread::monitor_owner_id_offset()));\n-  add(tmp, disp_hdr, (in_bytes(ObjectMonitor::owner_offset())-markWord::monitor_value));\n-  cmpxchg(tmp, zr, rscratch2, Assembler::xword, \/*acquire*\/ true,\n-          \/*release*\/ true, \/*weak*\/ false, tmp3Reg); \/\/ Sets flags for result\n-\n-  \/\/ Store a non-null value into the box to avoid looking like a re-entrant\n-  \/\/ lock. The fast-path monitor unlock code checks for\n-  \/\/ markWord::monitor_value so use markWord::unused_mark which has the\n-  \/\/ relevant bit set, and also matches ObjectSynchronizer::enter.\n-  mov(tmp, (address)markWord::unused_mark().value());\n-  str(tmp, Address(box, BasicLock::displaced_header_offset_in_bytes()));\n-\n-  br(Assembler::EQ, cont); \/\/ CAS success means locking succeeded\n-\n-  cmp(tmp3Reg, rscratch2);\n-  br(Assembler::NE, cont); \/\/ Check for recursive locking\n-\n-  \/\/ Recursive lock case\n-  increment(Address(disp_hdr, in_bytes(ObjectMonitor::recursions_offset()) - markWord::monitor_value), 1);\n-  \/\/ flag == EQ still from the cmp above, checking if this is a reentrant lock\n-\n-  bind(cont);\n-  \/\/ flag == EQ indicates success\n-  \/\/ flag == NE indicates failure\n-  br(Assembler::NE, no_count);\n-\n-  bind(count);\n-  if (LockingMode == LM_LEGACY) {\n-    inc_held_monitor_count(rscratch1);\n-  }\n-\n-  bind(no_count);\n-}\n-\n-void C2_MacroAssembler::fast_unlock(Register objectReg, Register boxReg, Register tmpReg,\n-                                    Register tmp2Reg) {\n-  Register oop = objectReg;\n-  Register box = boxReg;\n-  Register disp_hdr = tmpReg;\n-  Register owner_addr = tmpReg;\n-  Register tmp = tmp2Reg;\n-  Label cont;\n-  Label object_has_monitor;\n-  Label count, no_count;\n-  Label unlocked;\n-\n-  assert(LockingMode != LM_LIGHTWEIGHT, \"lightweight locking should use fast_unlock_lightweight\");\n-  assert_different_registers(oop, box, tmp, disp_hdr);\n-\n-  if (LockingMode == LM_LEGACY) {\n-    \/\/ Find the lock address and load the displaced header from the stack.\n-    ldr(disp_hdr, Address(box, BasicLock::displaced_header_offset_in_bytes()));\n-\n-    \/\/ If the displaced header is 0, we have a recursive unlock.\n-    cmp(disp_hdr, zr);\n-    br(Assembler::EQ, cont);\n-  }\n-\n-  \/\/ Handle existing monitor.\n-  ldr(tmp, Address(oop, oopDesc::mark_offset_in_bytes()));\n-  tbnz(tmp, exact_log2(markWord::monitor_value), object_has_monitor);\n-\n-  if (LockingMode == LM_MONITOR) {\n-    tst(oop, oop); \/\/ Set NE to indicate 'failure' -> take slow-path. We know that oop != 0.\n-    b(cont);\n-  } else {\n-    assert(LockingMode == LM_LEGACY, \"must be\");\n-    \/\/ Check if it is still a light weight lock, this is is true if we\n-    \/\/ see the stack address of the basicLock in the markWord of the\n-    \/\/ object.\n-\n-    cmpxchg(oop, box, disp_hdr, Assembler::xword, \/*acquire*\/ false,\n-            \/*release*\/ true, \/*weak*\/ false, tmp);\n-    b(cont);\n-  }\n-\n-  assert(oopDesc::mark_offset_in_bytes() == 0, \"offset of _mark is not 0\");\n-\n-  \/\/ Handle existing monitor.\n-  bind(object_has_monitor);\n-  STATIC_ASSERT(markWord::monitor_value <= INT_MAX);\n-  add(tmp, tmp, -(int)markWord::monitor_value); \/\/ monitor\n-\n-  ldr(disp_hdr, Address(tmp, ObjectMonitor::recursions_offset()));\n-\n-  Label notRecursive;\n-  cbz(disp_hdr, notRecursive);\n-\n-  \/\/ Recursive lock\n-  sub(disp_hdr, disp_hdr, 1u);\n-  str(disp_hdr, Address(tmp, ObjectMonitor::recursions_offset()));\n-  cmp(disp_hdr, disp_hdr); \/\/ Sets flags for result\n-  b(cont);\n-\n-  bind(notRecursive);\n-\n-  \/\/ Compute owner address.\n-  lea(owner_addr, Address(tmp, ObjectMonitor::owner_offset()));\n-\n-  \/\/ Set owner to null.\n-  \/\/ Release to satisfy the JMM\n-  stlr(zr, owner_addr);\n-  \/\/ We need a full fence after clearing owner to avoid stranding.\n-  \/\/ StoreLoad achieves this.\n-  membar(StoreLoad);\n-\n-  \/\/ Check if the entry_list is empty.\n-  ldr(rscratch1, Address(tmp, ObjectMonitor::entry_list_offset()));\n-  cmp(rscratch1, zr);\n-  br(Assembler::EQ, cont);     \/\/ If so we are done.\n-\n-  \/\/ Check if there is a successor.\n-  ldr(rscratch1, Address(tmp, ObjectMonitor::succ_offset()));\n-  cmp(rscratch1, zr);\n-  br(Assembler::NE, unlocked); \/\/ If so we are done.\n-\n-  \/\/ Save the monitor pointer in the current thread, so we can try to\n-  \/\/ reacquire the lock in SharedRuntime::monitor_exit_helper().\n-  str(tmp, Address(rthread, JavaThread::unlocked_inflated_monitor_offset()));\n-\n-  cmp(zr, rthread); \/\/ Set Flag to NE => slow path\n-  b(cont);\n-\n-  bind(unlocked);\n-  cmp(zr, zr); \/\/ Set Flag to EQ => fast path\n-\n-  \/\/ Intentional fall-through\n-\n-  bind(cont);\n-  \/\/ flag == EQ indicates success\n-  \/\/ flag == NE indicates failure\n-  br(Assembler::NE, no_count);\n-\n-  bind(count);\n-  if (LockingMode == LM_LEGACY) {\n-    dec_held_monitor_count(rscratch1);\n-  }\n-\n-  bind(no_count);\n-}\n-\n@@ -358,1 +152,0 @@\n-  assert(LockingMode == LM_LIGHTWEIGHT, \"must be\");\n@@ -515,1 +308,0 @@\n-  assert(LockingMode == LM_LIGHTWEIGHT, \"must be\");\n","filename":"src\/hotspot\/cpu\/aarch64\/c2_MacroAssembler_aarch64.cpp","additions":0,"deletions":208,"binary":false,"changes":208,"status":"modified"},{"patch":"@@ -45,3 +45,0 @@\n-  \/\/ Code used by cmpFastLock and cmpFastUnlock mach instructions in .ad file.\n-  void fast_lock(Register object, Register box, Register tmp, Register tmp2, Register tmp3);\n-  void fast_unlock(Register object, Register box, Register tmp, Register tmp2);\n","filename":"src\/hotspot\/cpu\/aarch64\/c2_MacroAssembler_aarch64.hpp","additions":0,"deletions":3,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -694,34 +694,0 @@\n-  if (LockingMode == LM_MONITOR) {\n-    call_VM_preemptable(noreg,\n-            CAST_FROM_FN_PTR(address, InterpreterRuntime::monitorenter),\n-            lock_reg);\n-  } else {\n-    Label count, done;\n-\n-    const Register swap_reg = r0;\n-    const Register tmp = c_rarg2;\n-    const Register obj_reg = c_rarg3; \/\/ Will contain the oop\n-    const Register tmp2 = c_rarg4;\n-    const Register tmp3 = c_rarg5;\n-\n-    const int obj_offset = in_bytes(BasicObjectLock::obj_offset());\n-    const int lock_offset = in_bytes(BasicObjectLock::lock_offset());\n-    const int mark_offset = lock_offset +\n-                            BasicLock::displaced_header_offset_in_bytes();\n-\n-    Label slow_case;\n-\n-    \/\/ Load object pointer into obj_reg %c_rarg3\n-    ldr(obj_reg, Address(lock_reg, obj_offset));\n-\n-    if (LockingMode == LM_LIGHTWEIGHT) {\n-      lightweight_lock(lock_reg, obj_reg, tmp, tmp2, tmp3, slow_case);\n-      b(done);\n-    } else if (LockingMode == LM_LEGACY) {\n-\n-      if (DiagnoseSyncOnValueBasedClasses != 0) {\n-        load_klass(tmp, obj_reg);\n-        ldrb(tmp, Address(tmp, Klass::misc_flags_offset()));\n-        tst(tmp, KlassFlags::_misc_is_value_based_class);\n-        br(Assembler::NE, slow_case);\n-      }\n@@ -729,55 +695,4 @@\n-      \/\/ Load (object->mark() | 1) into swap_reg\n-      ldr(rscratch1, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n-      orr(swap_reg, rscratch1, 1);\n-\n-      \/\/ Save (object->mark() | 1) into BasicLock's displaced header\n-      str(swap_reg, Address(lock_reg, mark_offset));\n-\n-      assert(lock_offset == 0,\n-             \"displached header must be first word in BasicObjectLock\");\n-\n-      Label fail;\n-      cmpxchg_obj_header(swap_reg, lock_reg, obj_reg, rscratch1, count, \/*fallthrough*\/nullptr);\n-\n-      \/\/ Fast check for recursive lock.\n-      \/\/\n-      \/\/ Can apply the optimization only if this is a stack lock\n-      \/\/ allocated in this thread. For efficiency, we can focus on\n-      \/\/ recently allocated stack locks (instead of reading the stack\n-      \/\/ base and checking whether 'mark' points inside the current\n-      \/\/ thread stack):\n-      \/\/  1) (mark & 7) == 0, and\n-      \/\/  2) sp <= mark < mark + os::pagesize()\n-      \/\/\n-      \/\/ Warning: sp + os::pagesize can overflow the stack base. We must\n-      \/\/ neither apply the optimization for an inflated lock allocated\n-      \/\/ just above the thread stack (this is why condition 1 matters)\n-      \/\/ nor apply the optimization if the stack lock is inside the stack\n-      \/\/ of another thread. The latter is avoided even in case of overflow\n-      \/\/ because we have guard pages at the end of all stacks. Hence, if\n-      \/\/ we go over the stack base and hit the stack of another thread,\n-      \/\/ this should not be in a writeable area that could contain a\n-      \/\/ stack lock allocated by that thread. As a consequence, a stack\n-      \/\/ lock less than page size away from sp is guaranteed to be\n-      \/\/ owned by the current thread.\n-      \/\/\n-      \/\/ These 3 tests can be done by evaluating the following\n-      \/\/ expression: ((mark - sp) & (7 - os::vm_page_size())),\n-      \/\/ assuming both stack pointer and pagesize have their\n-      \/\/ least significant 3 bits clear.\n-      \/\/ NOTE: the mark is in swap_reg %r0 as the result of cmpxchg\n-      \/\/ NOTE2: aarch64 does not like to subtract sp from rn so take a\n-      \/\/ copy\n-      mov(rscratch1, sp);\n-      sub(swap_reg, swap_reg, rscratch1);\n-      ands(swap_reg, swap_reg, (uint64_t)(7 - (int)os::vm_page_size()));\n-\n-      \/\/ Save the test result, for recursive case, the result is zero\n-      str(swap_reg, Address(lock_reg, mark_offset));\n-      br(Assembler::NE, slow_case);\n-\n-      bind(count);\n-      inc_held_monitor_count(rscratch1);\n-      b(done);\n-    }\n-    bind(slow_case);\n+  const Register tmp = c_rarg2;\n+  const Register obj_reg = c_rarg3; \/\/ Will contain the oop\n+  const Register tmp2 = c_rarg4;\n+  const Register tmp3 = c_rarg5;\n@@ -785,4 +700,2 @@\n-    \/\/ Call the runtime routine for slow case\n-    call_VM_preemptable(noreg,\n-            CAST_FROM_FN_PTR(address, InterpreterRuntime::monitorenter),\n-            lock_reg);\n+  \/\/ Load object pointer into obj_reg %c_rarg3\n+  ldr(obj_reg, Address(lock_reg, BasicObjectLock::obj_offset()));\n@@ -790,2 +703,12 @@\n-    bind(done);\n-  }\n+  Label slow_case, done;\n+  lightweight_lock(lock_reg, obj_reg, tmp, tmp2, tmp3, slow_case);\n+  b(done);\n+\n+  bind(slow_case);\n+\n+  \/\/ Call the runtime routine for slow case\n+  call_VM_preemptable(noreg,\n+          CAST_FROM_FN_PTR(address, InterpreterRuntime::monitorenter),\n+          lock_reg);\n+\n+  bind(done);\n@@ -810,20 +733,4 @@\n-  if (LockingMode == LM_MONITOR) {\n-    call_VM_leaf(CAST_FROM_FN_PTR(address, InterpreterRuntime::monitorexit), lock_reg);\n-  } else {\n-    Label count, done;\n-\n-    const Register swap_reg   = r0;\n-    const Register header_reg = c_rarg2;  \/\/ Will contain the old oopMark\n-    const Register obj_reg    = c_rarg3;  \/\/ Will contain the oop\n-    const Register tmp_reg    = c_rarg4;  \/\/ Temporary used by lightweight_unlock\n-\n-    save_bcp(); \/\/ Save in case of exception\n-\n-    if (LockingMode != LM_LIGHTWEIGHT) {\n-      \/\/ Convert from BasicObjectLock structure to object and BasicLock\n-      \/\/ structure Store the BasicLock address into %r0\n-      lea(swap_reg, Address(lock_reg, BasicObjectLock::lock_offset()));\n-    }\n-\n-    \/\/ Load oop into obj_reg(%c_rarg3)\n-    ldr(obj_reg, Address(lock_reg, BasicObjectLock::obj_offset()));\n+  const Register swap_reg   = r0;\n+  const Register header_reg = c_rarg2;  \/\/ Will contain the old oopMark\n+  const Register obj_reg    = c_rarg3;  \/\/ Will contain the oop\n+  const Register tmp_reg    = c_rarg4;  \/\/ Temporary used by lightweight_unlock\n@@ -831,2 +738,1 @@\n-    \/\/ Free entry\n-    str(zr, Address(lock_reg, BasicObjectLock::obj_offset()));\n+  save_bcp(); \/\/ Save in case of exception\n@@ -834,8 +740,2 @@\n-    Label slow_case;\n-    if (LockingMode == LM_LIGHTWEIGHT) {\n-      lightweight_unlock(obj_reg, header_reg, swap_reg, tmp_reg, slow_case);\n-      b(done);\n-    } else if (LockingMode == LM_LEGACY) {\n-      \/\/ Load the old header from BasicLock structure\n-      ldr(header_reg, Address(swap_reg,\n-                              BasicLock::displaced_header_offset_in_bytes()));\n+  \/\/ Load oop into obj_reg(%c_rarg3)\n+  ldr(obj_reg, Address(lock_reg, BasicObjectLock::obj_offset()));\n@@ -843,2 +743,2 @@\n-      \/\/ Test for recursion\n-      cbz(header_reg, count);\n+  \/\/ Free entry\n+  str(zr, Address(lock_reg, BasicObjectLock::obj_offset()));\n@@ -846,7 +746,3 @@\n-      \/\/ Atomic swap back the old header\n-      cmpxchg_obj_header(swap_reg, header_reg, obj_reg, rscratch1, count, &slow_case);\n-\n-      bind(count);\n-      dec_held_monitor_count(rscratch1);\n-      b(done);\n-    }\n+  Label slow_case, done;\n+  lightweight_unlock(obj_reg, header_reg, swap_reg, tmp_reg, slow_case);\n+  b(done);\n@@ -854,7 +750,6 @@\n-    bind(slow_case);\n-    \/\/ Call the runtime routine for slow case.\n-    str(obj_reg, Address(lock_reg, BasicObjectLock::obj_offset())); \/\/ restore obj\n-    call_VM_leaf(CAST_FROM_FN_PTR(address, InterpreterRuntime::monitorexit), lock_reg);\n-    bind(done);\n-    restore_bcp();\n-  }\n+  bind(slow_case);\n+  \/\/ Call the runtime routine for slow case.\n+  str(obj_reg, Address(lock_reg, BasicObjectLock::obj_offset())); \/\/ restore obj\n+  call_VM_leaf(CAST_FROM_FN_PTR(address, InterpreterRuntime::monitorexit), lock_reg);\n+  bind(done);\n+  restore_bcp();\n","filename":"src\/hotspot\/cpu\/aarch64\/interp_masm_aarch64.cpp","additions":36,"deletions":141,"binary":false,"changes":177,"status":"modified"},{"patch":"@@ -7100,1 +7100,0 @@\n-  assert(LockingMode == LM_LIGHTWEIGHT, \"only used with new lightweight locking\");\n@@ -7160,1 +7159,0 @@\n-  assert(LockingMode == LM_LIGHTWEIGHT, \"only used with new lightweight locking\");\n","filename":"src\/hotspot\/cpu\/aarch64\/macroAssembler_aarch64.cpp","additions":0,"deletions":2,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -1724,1 +1724,1 @@\n-  if (LockingMode != LM_LEGACY && method->is_object_wait0()) {\n+  if (method->is_object_wait0()) {\n@@ -1779,38 +1779,1 @@\n-    if (LockingMode == LM_MONITOR) {\n-      __ b(slow_path_lock);\n-    } else if (LockingMode == LM_LEGACY) {\n-      \/\/ Load (object->mark() | 1) into swap_reg %r0\n-      __ ldr(rscratch1, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n-      __ orr(swap_reg, rscratch1, 1);\n-\n-      \/\/ Save (object->mark() | 1) into BasicLock's displaced header\n-      __ str(swap_reg, Address(lock_reg, mark_word_offset));\n-\n-      \/\/ src -> dest iff dest == r0 else r0 <- dest\n-      __ cmpxchg_obj_header(r0, lock_reg, obj_reg, rscratch1, count, \/*fallthrough*\/nullptr);\n-\n-      \/\/ Hmm should this move to the slow path code area???\n-\n-      \/\/ Test if the oopMark is an obvious stack pointer, i.e.,\n-      \/\/  1) (mark & 3) == 0, and\n-      \/\/  2) sp <= mark < mark + os::pagesize()\n-      \/\/ These 3 tests can be done by evaluating the following\n-      \/\/ expression: ((mark - sp) & (3 - os::vm_page_size())),\n-      \/\/ assuming both stack pointer and pagesize have their\n-      \/\/ least significant 2 bits clear.\n-      \/\/ NOTE: the oopMark is in swap_reg %r0 as the result of cmpxchg\n-\n-      __ sub(swap_reg, sp, swap_reg);\n-      __ neg(swap_reg, swap_reg);\n-      __ ands(swap_reg, swap_reg, 3 - (int)os::vm_page_size());\n-\n-      \/\/ Save the test result, for recursive case, the result is zero\n-      __ str(swap_reg, Address(lock_reg, mark_word_offset));\n-      __ br(Assembler::NE, slow_path_lock);\n-\n-      __ bind(count);\n-      __ inc_held_monitor_count(rscratch1);\n-    } else {\n-      assert(LockingMode == LM_LIGHTWEIGHT, \"must be\");\n-      __ lightweight_lock(lock_reg, obj_reg, swap_reg, tmp, lock_tmp, slow_path_lock);\n-    }\n+    __ lightweight_lock(lock_reg, obj_reg, swap_reg, tmp, lock_tmp, slow_path_lock);\n@@ -1891,1 +1854,1 @@\n-  if (LockingMode != LM_LEGACY && method->is_object_wait0()) {\n+  if (method->is_object_wait0()) {\n@@ -1920,12 +1883,0 @@\n-    Label done, not_recursive;\n-\n-    if (LockingMode == LM_LEGACY) {\n-      \/\/ Simple recursive lock?\n-      __ ldr(rscratch1, Address(sp, lock_slot_offset * VMRegImpl::stack_slot_size));\n-      __ cbnz(rscratch1, not_recursive);\n-      __ dec_held_monitor_count(rscratch1);\n-      __ b(done);\n-    }\n-\n-    __ bind(not_recursive);\n-\n@@ -1937,17 +1888,1 @@\n-    if (LockingMode == LM_MONITOR) {\n-      __ b(slow_path_unlock);\n-    } else if (LockingMode == LM_LEGACY) {\n-      \/\/ get address of the stack lock\n-      __ lea(r0, Address(sp, lock_slot_offset * VMRegImpl::stack_slot_size));\n-      \/\/  get old displaced header\n-      __ ldr(old_hdr, Address(r0, 0));\n-\n-      \/\/ Atomic swap old header if oop still contains the stack lock\n-      Label count;\n-      __ cmpxchg_obj_header(r0, old_hdr, obj_reg, rscratch1, count, &slow_path_unlock);\n-      __ bind(count);\n-      __ dec_held_monitor_count(rscratch1);\n-    } else {\n-      assert(LockingMode == LM_LIGHTWEIGHT, \"\");\n-      __ lightweight_unlock(obj_reg, old_hdr, swap_reg, lock_tmp, slow_path_unlock);\n-    }\n+    __ lightweight_unlock(obj_reg, old_hdr, swap_reg, lock_tmp, slow_path_unlock);\n@@ -1960,2 +1895,0 @@\n-\n-    __ bind(done);\n","filename":"src\/hotspot\/cpu\/aarch64\/sharedRuntime_aarch64.cpp","additions":4,"deletions":71,"binary":false,"changes":75,"status":"modified"},{"patch":"@@ -1481,16 +1481,11 @@\n-  if (LockingMode != LM_LEGACY) {\n-    \/\/ Check preemption for Object.wait()\n-    Label not_preempted;\n-    __ ldr(rscratch1, Address(rthread, JavaThread::preempt_alternate_return_offset()));\n-    __ cbz(rscratch1, not_preempted);\n-    __ str(zr, Address(rthread, JavaThread::preempt_alternate_return_offset()));\n-    __ br(rscratch1);\n-    __ bind(native_return);\n-    __ restore_after_resume(true \/* is_native *\/);\n-    \/\/ reload result_handler\n-    __ ldr(result_handler, Address(rfp, frame::interpreter_frame_result_handler_offset*wordSize));\n-    __ bind(not_preempted);\n-  } else {\n-    \/\/ any pc will do so just use this one for LM_LEGACY to keep code together.\n-    __ bind(native_return);\n-  }\n+  \/\/ Check preemption for Object.wait()\n+  Label not_preempted;\n+  __ ldr(rscratch1, Address(rthread, JavaThread::preempt_alternate_return_offset()));\n+  __ cbz(rscratch1, not_preempted);\n+  __ str(zr, Address(rthread, JavaThread::preempt_alternate_return_offset()));\n+  __ br(rscratch1);\n+  __ bind(native_return);\n+  __ restore_after_resume(true \/* is_native *\/);\n+  \/\/ reload result_handler\n+  __ ldr(result_handler, Address(rfp, frame::interpreter_frame_result_handler_offset*wordSize));\n+  __ bind(not_preempted);\n","filename":"src\/hotspot\/cpu\/aarch64\/templateInterpreterGenerator_aarch64.cpp","additions":11,"deletions":16,"binary":false,"changes":27,"status":"modified"}]}