{"files":[{"patch":"@@ -88,1 +88,1 @@\n-  do_arch_blob(compiler, 20000 LP64_ONLY(+64000) WINDOWS_ONLY(+2000))   \\\n+  do_arch_blob(compiler, 20000 LP64_ONLY(+89000) WINDOWS_ONLY(+2000))   \\\n","filename":"src\/hotspot\/cpu\/x86\/stubDeclarations_x86.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -4210,0 +4210,2 @@\n+  generate_dilithium_stubs();\n+\n","filename":"src\/hotspot\/cpu\/x86\/stubGenerator_x86_64.cpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -494,0 +494,15 @@\n+  address generate_double_keccak();\n+\n+  \/\/ Dilithium stubs and helper functions\n+  void montmulEven(int outputReg, int inputReg1,  int inputReg2,\n+    int scratchReg1, int scratchReg2, int parCnt);\n+  void montmulEven(int outputReg, int inputReg11, int inputReg12,\n+    int inputReg13, int inputReg14,  int inputReg2,\n+    int scratchReg1, int scratchReg2);\n+  void generate_dilithium_stubs();\n+  address generate_dilithiumAlmostNtt_avx512();\n+  address generate_dilithiumAlmostInverseNtt_avx512();\n+  address generate_dilithiumNttMult_avx512();\n+  address generate_dilithiumMontMulByConstant_avx512();\n+  address generate_dilithiumDecomposePoly_avx512();\n+\n","filename":"src\/hotspot\/cpu\/x86\/stubGenerator_x86_64.hpp","additions":15,"deletions":0,"binary":false,"changes":15,"status":"modified"},{"patch":"@@ -0,0 +1,1404 @@\n+\/*\n+ * Copyright (c) 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#include \"asm\/assembler.hpp\"\n+#include \"asm\/assembler.inline.hpp\"\n+#include \"runtime\/stubRoutines.hpp\"\n+#include \"macroAssembler_x86.hpp\"\n+#include \"stubGenerator_x86_64.hpp\"\n+\n+#define __ _masm->\n+\n+#define xmm(i) as_XMMRegister(i)\n+\n+#ifdef PRODUCT\n+#define BLOCK_COMMENT(str) \/* nothing *\/\n+#else\n+#define BLOCK_COMMENT(str) __ block_comment(str)\n+#endif \/\/ PRODUCT\n+\n+#define BIND(label) bind(label); BLOCK_COMMENT(#label \":\")\n+\n+\/\/ Constants\n+\/\/\n+ATTRIBUTE_ALIGNED(64) static const uint32_t dilithiumAvx512Consts[] = {\n+    58728449, \/\/ montQInvModR\n+    8380417, \/\/ dilithium_q\n+    16382, \/\/ toMont((dilithium_n)^-1 (mod dilithium_q))\n+    2365951, \/\/ montRSquareModQ\n+    5373807 \/\/ addend for modular reduce\n+};\n+\n+static address dilithiumAvx512ConstsAddr() {\n+  return (address) dilithiumAvx512Consts;\n+}\n+\n+ATTRIBUTE_ALIGNED(64) static const uint32_t dilithiumAvx512Perms[] = {\n+     \/\/ collect montmul results into the destination register\n+    17, 1, 19, 3, 21, 5, 23, 7, 25, 9, 27, 11, 29, 13, 31, 15,\n+    \/\/ ntt\n+    0, 1, 2, 3, 4, 5, 6, 7, 16, 17, 18, 19, 20, 21, 22, 23,\n+    8, 9, 10, 11, 12, 13, 14, 15, 24, 25, 26, 27, 28, 29, 30, 31,\n+    0, 1, 2, 3, 16, 17, 18, 19, 8, 9, 10, 11, 24, 25, 26, 27,\n+    4, 5, 6, 7, 20, 21, 22, 23, 12, 13, 14, 15, 28, 29, 30, 31,\n+    0, 1, 16, 17, 4, 5, 20, 21, 8, 9, 24, 25, 12, 13, 28, 29,\n+    2, 3, 18, 19, 6, 7, 22, 23, 10, 11, 26, 27, 14, 15, 30, 31,\n+    0, 16, 2, 18, 4, 20, 6, 22, 8, 24, 10, 26, 12, 28, 14, 30,\n+    1, 17, 3, 19, 5, 21, 7, 23, 9, 25, 11, 27, 13, 29, 15, 31,\n+    0, 16, 1, 17, 2, 18, 3, 19, 4, 20, 5, 21, 6, 22, 7, 23,\n+    8, 24, 9, 25, 10, 26, 11, 27, 12, 28, 13, 29, 14, 30, 15, 31,\n+    \/\/ ntt inverse\n+    0, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30,\n+    1, 3, 5, 7, 9, 11, 13, 15, 17, 19, 21, 23, 25, 27, 29, 31,\n+    0, 16, 2, 18, 4, 20, 6, 22, 8, 24, 10, 26, 12, 28, 14, 30,\n+    1, 17, 3, 19, 5, 21, 7, 23, 9, 25, 11, 27, 13, 29, 15, 31,\n+    0, 1, 16, 17, 4, 5, 20, 21, 8, 9, 24, 25, 12, 13, 28, 29,\n+    2, 3, 18, 19, 6, 7, 22, 23, 10, 11, 26, 27, 14, 15, 30, 31,\n+    0, 1, 2, 3, 16, 17, 18, 19, 8, 9, 10, 11, 24, 25, 26, 27,\n+    4, 5, 6, 7, 20, 21, 22, 23, 12, 13, 14, 15, 28, 29, 30, 31,\n+    0, 1, 2, 3, 4, 5, 6, 7, 16, 17, 18, 19, 20, 21, 22, 23,\n+    8, 9, 10, 11, 12, 13, 14, 15, 24, 25, 26, 27, 28, 29, 30, 31\n+};\n+\n+static address dilithiumAvx512PermsAddr() {\n+  return (address) dilithiumAvx512Perms;\n+}\n+\n+void StubGenerator::generate_dilithium_stubs() {\n+  \/\/ Generate Dilithium intrinsics code\n+  if (UseDilithiumIntrinsics) {\n+      StubRoutines::_dilithiumAlmostNtt = generate_dilithiumAlmostNtt_avx512();\n+      StubRoutines::_dilithiumAlmostInverseNtt = generate_dilithiumAlmostInverseNtt_avx512();\n+      StubRoutines::_dilithiumNttMult = generate_dilithiumNttMult_avx512();\n+      StubRoutines::_dilithiumMontMulByConstant = generate_dilithiumMontMulByConstant_avx512();\n+      StubRoutines::_dilithiumDecomposePoly = generate_dilithiumDecomposePoly_avx512();\n+    }\n+}\n+\n+\/\/ We do Montgomery multiplications of two vectors of 16 ints each in 4 steps:\n+\/\/ 1. Do the multiplications of the corresponding even numbered slots into\n+\/\/    the odd numbered slots of a third register using montmulEven().\n+\/\/ 2. Swap the even and odd numbered slots of the original input registers.\n+\/\/ 3. Similar to step 1, but into a different output register.\n+\/\/ 4. Combine the outputs of step 1 and step 3 into the output of the Montgomery\n+\/\/    multiplication.\n+\/\/ (For levels 0-6 in the Ntt and levels 1-7 of the inverse Ntt we only swap the\n+\/\/ odd-even slots of the first multiplicand as in the second (zetas) the\n+\/\/ odd slots contain the same number as the corresponding even one.)\n+\n+\/\/ Montgomery multiplication of the *even* numbered slices of parCnt consecutive register pairs\n+\/\/ Zmm_inputReg1 to Zmm_(inputReg1+parCnt-1) and Zmm_inputReg2 to Zmm_(inputReg2+parCnt-1).\n+\/\/ The result goes to the *odd* numbered slices of Zmm_outputReg to Zmm_(outputReg1+parCnt-1).\n+\/\/ Zmm_31 should contain q and Zmm_30 should contain q^-1 mod 2^32 in all of their slices.\n+void StubGenerator::montmulEven(int outputReg, int inputReg1,  int inputReg2, int scratchReg1, int scratchReg2, int parCnt) {\n+\n+  for (int i = 0; i < parCnt; i++) {\n+    __ vpmuldq(xmm(i + scratchReg1), xmm(i + inputReg1), xmm((inputReg2 == 29) ? 29 : inputReg2 + i), Assembler::AVX_512bit);\n+  }\n+  for (int i = 0; i < parCnt; i++) {\n+    __ vpmulld(xmm(i + scratchReg2), xmm(i + scratchReg1), xmm30, Assembler::AVX_512bit);\n+  }\n+  for (int i = 0; i < parCnt; i++) {\n+    __ vpmuldq(xmm(i + scratchReg2), xmm(i + scratchReg2), xmm31, Assembler::AVX_512bit);\n+  }\n+  for (int i = 0; i < parCnt; i++) {\n+    __ evpsubd(xmm(i + outputReg), k0, xmm(i + scratchReg1), xmm(i + scratchReg2), false, Assembler::AVX_512bit);\n+  }\n+}\n+\n+\/\/ Similar to the 6-parameter montmulEven(), the difference is that here the input regs for the first\n+\/\/ arguments do not have to be consecutive and that parcnt is always 4, so it is not passed in.\n+void StubGenerator::montmulEven(int outputReg, int inputReg11, int inputReg12, int inputReg13, int inputReg14,\n+                                int inputReg2, int scratchReg1, int scratchReg2) {\n+\n+  int parCnt = 4;\n+\n+  __ vpmuldq(xmm(scratchReg1), xmm(inputReg11), xmm(inputReg2), Assembler::AVX_512bit);\n+  __ vpmuldq(xmm(scratchReg1 + 1), xmm(inputReg12), xmm(inputReg2 + 1), Assembler::AVX_512bit);\n+  __ vpmuldq(xmm(scratchReg1 + 2), xmm(inputReg13), xmm(inputReg2 + 2), Assembler::AVX_512bit);\n+  __ vpmuldq(xmm(scratchReg1 + 3), xmm(inputReg14), xmm(inputReg2 + 3), Assembler::AVX_512bit);\n+\n+  for (int i = 0; i < parCnt; i++) {\n+    __ vpmulld(xmm(i + scratchReg2), xmm(i + scratchReg1), xmm30, Assembler::AVX_512bit);\n+  }\n+  for (int i = 0; i < parCnt; i++) {\n+    __ vpmuldq(xmm(i + scratchReg2), xmm(i + scratchReg2), xmm31, Assembler::AVX_512bit);\n+  }\n+  for (int i = 0; i < parCnt; i++) {\n+    __ evpsubd(xmm(i + outputReg), k0, xmm(i + scratchReg1), xmm(i + scratchReg2), false, Assembler::AVX_512bit);\n+  }\n+}\n+\n+\/\/ Dilithium NTT function except for the final \"normalization\" to |coeff| < Q.\n+\/\/ Implements\n+\/\/ static int implDilithiumAlmostNtt(int[] coeffs, int zetas[]) {}\n+\/\/\n+\/\/ coeffs (int[256]) = c_rarg0\n+\/\/ zetas (int[256]) = c_rarg1\n+\/\/\n+\/\/\n+address StubGenerator::generate_dilithiumAlmostNtt_avx512() {\n+\n+  __ align(CodeEntryAlignment);\n+  StubGenStubId stub_id = dilithiumAlmostNtt_id;\n+  StubCodeMark mark(this, stub_id);\n+  address start = __ pc();\n+  __ enter();\n+\n+  Label L_loop, L_end;\n+\n+  const Register coeffs = c_rarg0;\n+  const Register zetas = c_rarg1;\n+\n+  const Register iterations = c_rarg2;\n+\n+  const Register dilithiumConsts = r10;\n+  const Register perms = r11;\n+\n+  __ lea(perms, ExternalAddress(dilithiumAvx512PermsAddr()));\n+  __ lea(dilithiumConsts, ExternalAddress(dilithiumAvx512ConstsAddr()));\n+\n+  __ evmovdqul(xmm28, Address(perms, 0), Assembler::AVX_512bit);\n+  __ evmovdqul(xmm29, Address(zetas, 0), Assembler::AVX_512bit);\n+  __ vpbroadcastd(xmm30, Address(dilithiumConsts, 0), Assembler::AVX_512bit); \/\/ q^-1 mod 2^32\n+  __ vpbroadcastd(xmm31, Address(dilithiumConsts, 4), Assembler::AVX_512bit); \/\/ q\n+\n+  \/\/ load all coefficients into the vector registers Zmm_0-Zmm_15,\n+  \/\/ 16 coefficients into each\n+  for (int i = 0; i < 16; i++) {\n+    __ evmovdqul(xmm(i), Address(coeffs, i * 64), Assembler::AVX_512bit);\n+  }\n+\n+  \/\/ level 0 and 1 can be done entirely in registers as the zetas on these\n+  \/\/ levels are the same for all the montmuls that we can do in parallel\n+\n+  \/\/ level 0\n+  montmulEven(20, 8, 29, 20, 16, 4);\n+\n+  for (int i = 0; i < 4; i++) {\n+    __ vpshufd(xmm(i + 24), xmm(i + 8), 0xB1, Assembler::AVX_512bit);\n+  }\n+\n+  montmulEven(16, 24, 29, 24, 16, 4);\n+\n+  for (int i = 0; i < 4; i++) {\n+    __ evpermt2d(xmm(i + 16), xmm28, xmm(i + 20), Assembler::AVX_512bit);\n+  }\n+\n+  for (int i = 0; i < 4; i++) {\n+    __ evpsubd(xmm(i + 8), k0, xmm(i), xmm(i + 16), false, Assembler::AVX_512bit);\n+  }\n+\n+  for (int i = 0; i < 4; i++) {\n+    __ evpaddd(xmm(i), k0, xmm(i), xmm(i + 16), false, Assembler::AVX_512bit);\n+  }\n+\n+  montmulEven(20, 12, 29, 20, 16, 4);\n+\n+  for (int i = 0; i < 4; i++) {\n+    __ vpshufd(xmm(i + 24), xmm(i + 12), 0xB1, Assembler::AVX_512bit);\n+  }\n+\n+  montmulEven(16, 24, 29, 24, 16, 4);\n+\n+  for (int i = 0; i < 4; i++) {\n+    __ evpermt2d(xmm(i + 16), xmm28, xmm(i + 20), Assembler::AVX_512bit);\n+  }\n+\n+  for (int i = 0; i < 4; i++) {\n+    __ evpsubd(xmm(i + 12), k0, xmm(i + 4), xmm(i + 16), false, Assembler::AVX_512bit);\n+  }\n+\n+  for (int i = 0; i < 4; i++) {\n+    __ evpaddd(xmm(i + 4), k0, xmm(i + 4), xmm(i + 16), false, Assembler::AVX_512bit);\n+  }\n+\n+  \/\/ level 1\n+  __ evmovdqul(xmm29, Address(zetas, 512), Assembler::AVX_512bit);\n+\n+  montmulEven(20, 4, 29, 20, 16, 4);\n+  for (int i = 0; i < 4; i++) {\n+    __ vpshufd(xmm(i + 24), xmm(i + 4), 0xB1, Assembler::AVX_512bit);\n+  }\n+\n+  montmulEven(16, 24, 29, 24, 16, 4);\n+\n+  for (int i = 0; i < 4; i++) {\n+    __ evpermt2d(xmm(i + 16), xmm28, xmm(i + 20), Assembler::AVX_512bit);\n+  }\n+\n+  for (int i = 0; i < 4; i++) {\n+    __ evpsubd(xmm(i + 4), k0, xmm(i), xmm(i + 16), false, Assembler::AVX_512bit);\n+  }\n+\n+  for (int i = 0; i < 4; i++) {\n+    __ evpaddd(xmm(i), k0, xmm(i), xmm(i + 16), false, Assembler::AVX_512bit);\n+  }\n+\n+  __ evmovdqul(xmm29, Address(zetas, 768), Assembler::AVX_512bit);\n+\n+  montmulEven(20, 12, 29, 20, 16, 4);\n+  for (int i = 0; i < 4; i++) {\n+    __ vpshufd(xmm(i + 24), xmm(i + 12), 0xB1, Assembler::AVX_512bit);\n+  }\n+\n+  montmulEven(16, 24, 29, 24, 16, 4);\n+\n+  for (int i = 0; i < 4; i++) {\n+    __ evpermt2d(xmm(i + 16), xmm28, xmm(i + 20), Assembler::AVX_512bit);\n+  }\n+\n+  for (int i = 0; i < 4; i++) {\n+    __ evpsubd(xmm(i + 12), k0, xmm(i + 8), xmm(i + 16), false, Assembler::AVX_512bit);\n+  }\n+\n+  for (int i = 0; i < 4; i++) {\n+    __ evpaddd(xmm(i + 8), k0, xmm(i + 8), xmm(i + 16), false, Assembler::AVX_512bit);\n+  }\n+\n+  \/\/ levels 2 to 7 are done in 2 batches, by first saving half of the coefficients\n+  \/\/ from level 1 into memory, doing all the level 2 to level 7 computations\n+  \/\/ on the remaining half in the vector registers, saving the result to\n+  \/\/ memory after level 7, then loading back the coefficients that we saved after\n+  \/\/ level 1 and do the same computation with those\n+\n+  for (int i = 0; i < 16; i++) {\n+    __ evmovdqul(Address(coeffs, i * 64), xmm(i), Assembler::AVX_512bit);\n+  }\n+\n+  __ movl(iterations, 2);\n+\n+  __ align(OptoLoopAlignment);\n+  __ BIND(L_loop);\n+\n+  __ subl(iterations, 1);\n+\n+  \/\/ level 2\n+  __ evmovdqul(xmm12, Address(zetas, 1024), Assembler::AVX_512bit);\n+  __ evmovdqul(xmm13, Address(zetas, 1088), Assembler::AVX_512bit);\n+  __ evmovdqul(xmm14, Address(zetas, 1152), Assembler::AVX_512bit);\n+  __ evmovdqul(xmm15, Address(zetas, 1216), Assembler::AVX_512bit);\n+\n+  montmulEven(20, 2, 3, 6, 7, 12, 20, 16);\n+\n+  __ vpshufd(xmm(8), xmm(2), 0xB1, Assembler::AVX_512bit);\n+  __ vpshufd(xmm(9), xmm(3), 0xB1, Assembler::AVX_512bit);\n+  __ vpshufd(xmm(10), xmm(6), 0xB1, Assembler::AVX_512bit);\n+  __ vpshufd(xmm(11), xmm(7), 0xB1, Assembler::AVX_512bit);\n+\n+  montmulEven(16, 8, 12, 24, 16, 4);\n+\n+  for (int i = 0; i < 4; i++) {\n+    __ evpermt2d(xmm(i + 16), xmm28, xmm(i + 20), Assembler::AVX_512bit);\n+  }\n+\n+  __ evpsubd(xmm(2), k0, xmm(0), xmm(16), false, Assembler::AVX_512bit);\n+  __ evpsubd(xmm(3), k0, xmm(1), xmm(17), false, Assembler::AVX_512bit);\n+  __ evpsubd(xmm(6), k0, xmm(4), xmm(18), false, Assembler::AVX_512bit);\n+  __ evpsubd(xmm(7), k0, xmm(5), xmm(19), false, Assembler::AVX_512bit);\n+\n+  __ evpaddd(xmm(0), k0, xmm(0), xmm(16), false, Assembler::AVX_512bit);\n+  __ evpaddd(xmm(1), k0, xmm(1), xmm(17), false, Assembler::AVX_512bit);\n+  __ evpaddd(xmm(4), k0, xmm(4), xmm(18), false, Assembler::AVX_512bit);\n+  __ evpaddd(xmm(5), k0, xmm(5), xmm(19), false, Assembler::AVX_512bit);\n+\n+  \/\/ level 3\n+  __ evmovdqul(xmm12, Address(zetas, 1536), Assembler::AVX_512bit);\n+  __ evmovdqul(xmm13, Address(zetas, 1600), Assembler::AVX_512bit);\n+  __ evmovdqul(xmm14, Address(zetas, 1664), Assembler::AVX_512bit);\n+  __ evmovdqul(xmm15, Address(zetas, 1728), Assembler::AVX_512bit);\n+\n+  montmulEven(20, 1, 3, 5, 7, 12, 20, 16);\n+\n+  __ vpshufd(xmm(8), xmm(1), 0xB1, Assembler::AVX_512bit);\n+  __ vpshufd(xmm(9), xmm(3), 0xB1, Assembler::AVX_512bit);\n+  __ vpshufd(xmm(10), xmm(5), 0xB1, Assembler::AVX_512bit);\n+  __ vpshufd(xmm(11), xmm(7), 0xB1, Assembler::AVX_512bit);\n+\n+  montmulEven(16, 8, 12, 24, 16, 4);\n+\n+  for (int i = 0; i < 4; i++) {\n+    __ evpermt2d(xmm(i + 16), xmm28, xmm(i + 20), Assembler::AVX_512bit);\n+  }\n+\n+  __ evpsubd(xmm(1), k0, xmm(0), xmm(16), false, Assembler::AVX_512bit);\n+  __ evpsubd(xmm(3), k0, xmm(2), xmm(17), false, Assembler::AVX_512bit);\n+  __ evpsubd(xmm(5), k0, xmm(4), xmm(18), false, Assembler::AVX_512bit);\n+  __ evpsubd(xmm(7), k0, xmm(6), xmm(19), false, Assembler::AVX_512bit);\n+\n+  __ evpaddd(xmm(0), k0, xmm(0), xmm(16), false, Assembler::AVX_512bit);\n+  __ evpaddd(xmm(2), k0, xmm(2), xmm(17), false, Assembler::AVX_512bit);\n+  __ evpaddd(xmm(4), k0, xmm(4), xmm(18), false, Assembler::AVX_512bit);\n+  __ evpaddd(xmm(6), k0, xmm(6), xmm(19), false, Assembler::AVX_512bit);\n+\n+  \/\/ level 4\n+  __ evmovdqul(xmm16, Address(perms, 64), Assembler::AVX_512bit);\n+  __ evmovdqul(xmm17, xmm16, Assembler::AVX_512bit);\n+  __ evmovdqul(xmm18, xmm16, Assembler::AVX_512bit);\n+  __ evmovdqul(xmm19, xmm16, Assembler::AVX_512bit);\n+  __ evmovdqul(xmm12, Address(perms, 128), Assembler::AVX_512bit);\n+  __ evmovdqul(xmm13, xmm12, Assembler::AVX_512bit);\n+  __ evmovdqul(xmm14, xmm12, Assembler::AVX_512bit);\n+  __ evmovdqul(xmm15, xmm12, Assembler::AVX_512bit);\n+\n+  for (int i = 0; i < 8; i += 2) {\n+    __ evpermi2d(xmm(i\/2 + 16), xmm(i), xmm(i + 1), Assembler::AVX_512bit);\n+  }\n+  for (int i = 0; i < 8; i += 2) {\n+    __ evpermi2d(xmm(i \/ 2 + 12), xmm(i), xmm(i + 1), Assembler::AVX_512bit);\n+  }\n+  __ evmovdqul(xmm0, Address(zetas, 4 * 512), Assembler::AVX_512bit);\n+  __ evmovdqul(xmm1, Address(zetas, 4 * 512 + 64), Assembler::AVX_512bit);\n+  __ evmovdqul(xmm2, Address(zetas, 4 * 512 + 128), Assembler::AVX_512bit);\n+  __ evmovdqul(xmm3, Address(zetas, 4 * 512 + 192), Assembler::AVX_512bit);\n+\n+  montmulEven(20, 0, 12, 4, 20, 4);\n+\n+  __ vpshufd(xmm(12), xmm(12), 0xB1, Assembler::AVX_512bit);\n+  __ vpshufd(xmm(13), xmm(13), 0xB1, Assembler::AVX_512bit);\n+  __ vpshufd(xmm(14), xmm(14), 0xB1, Assembler::AVX_512bit);\n+  __ vpshufd(xmm(15), xmm(15), 0xB1, Assembler::AVX_512bit);\n+\n+  montmulEven(12, 0, 12, 4, 24, 4);\n+\n+  for (int i = 0; i < 4; i++) {\n+    __ evpermt2d(xmm(i + 12), xmm28, xmm(i + 20), Assembler::AVX_512bit);\n+  }\n+\n+  __ evpsubd(xmm(1), k0, xmm(16), xmm(12), false, Assembler::AVX_512bit);\n+  __ evpsubd(xmm(3), k0, xmm(17), xmm(13), false, Assembler::AVX_512bit);\n+  __ evpsubd(xmm(5), k0, xmm(18), xmm(14), false, Assembler::AVX_512bit);\n+  __ evpsubd(xmm(7), k0, xmm(19), xmm(15), false, Assembler::AVX_512bit);\n+\n+  __ evpaddd(xmm(0), k0, xmm(16), xmm(12), false, Assembler::AVX_512bit);\n+  __ evpaddd(xmm(2), k0, xmm(17), xmm(13), false, Assembler::AVX_512bit);\n+  __ evpaddd(xmm(4), k0, xmm(18), xmm(14), false, Assembler::AVX_512bit);\n+  __ evpaddd(xmm(6), k0, xmm(19), xmm(15), false, Assembler::AVX_512bit);\n+\n+  \/\/ level 5\n+  __ evmovdqul(xmm16, Address(perms, 192), Assembler::AVX_512bit);\n+  __ evmovdqul(xmm17, xmm16, Assembler::AVX_512bit);\n+  __ evmovdqul(xmm18, xmm16, Assembler::AVX_512bit);\n+  __ evmovdqul(xmm19, xmm16, Assembler::AVX_512bit);\n+  __ evmovdqul(xmm12, Address(perms, 256), Assembler::AVX_512bit);\n+  __ evmovdqul(xmm13, xmm12, Assembler::AVX_512bit);\n+  __ evmovdqul(xmm14, xmm12, Assembler::AVX_512bit);\n+  __ evmovdqul(xmm15, xmm12, Assembler::AVX_512bit);\n+\n+  for (int i = 0; i < 8; i += 2) {\n+    __ evpermi2d(xmm(i\/2 + 16), xmm(i), xmm(i + 1), Assembler::AVX_512bit);\n+  }\n+  for (int i = 0; i < 8; i += 2) {\n+    __ evpermi2d(xmm(i \/ 2 + 12), xmm(i), xmm(i + 1), Assembler::AVX_512bit);\n+  }\n+  __ evmovdqul(xmm0, Address(zetas, 5 * 512), Assembler::AVX_512bit);\n+  __ evmovdqul(xmm1, Address(zetas, 5 * 512 + 64), Assembler::AVX_512bit);\n+  __ evmovdqul(xmm2, Address(zetas, 5 * 512 + 128), Assembler::AVX_512bit);\n+  __ evmovdqul(xmm3, Address(zetas, 5 * 512 + 192), Assembler::AVX_512bit);\n+\n+  montmulEven(20, 0, 12, 4, 20, 4);\n+\n+  __ vpshufd(xmm(12), xmm(12), 0xB1, Assembler::AVX_512bit);\n+  __ vpshufd(xmm(13), xmm(13), 0xB1, Assembler::AVX_512bit);\n+  __ vpshufd(xmm(14), xmm(14), 0xB1, Assembler::AVX_512bit);\n+  __ vpshufd(xmm(15), xmm(15), 0xB1, Assembler::AVX_512bit);\n+\n+  montmulEven(12, 0, 12, 4, 24, 4);\n+\n+  for (int i = 0; i < 4; i++) {\n+    __ evpermt2d(xmm(i + 12), xmm28, xmm(i + 20), Assembler::AVX_512bit);\n+  }\n+\n+  __ evpsubd(xmm(1), k0, xmm(16), xmm(12), false, Assembler::AVX_512bit);\n+  __ evpsubd(xmm(3), k0, xmm(17), xmm(13), false, Assembler::AVX_512bit);\n+  __ evpsubd(xmm(5), k0, xmm(18), xmm(14), false, Assembler::AVX_512bit);\n+  __ evpsubd(xmm(7), k0, xmm(19), xmm(15), false, Assembler::AVX_512bit);\n+\n+  __ evpaddd(xmm(0), k0, xmm(16), xmm(12), false, Assembler::AVX_512bit);\n+  __ evpaddd(xmm(2), k0, xmm(17), xmm(13), false, Assembler::AVX_512bit);\n+  __ evpaddd(xmm(4), k0, xmm(18), xmm(14), false, Assembler::AVX_512bit);\n+  __ evpaddd(xmm(6), k0, xmm(19), xmm(15), false, Assembler::AVX_512bit);\n+\n+  \/\/ level 6\n+  __ evmovdqul(xmm16, Address(perms, 320), Assembler::AVX_512bit);\n+  __ evmovdqul(xmm17, xmm16, Assembler::AVX_512bit);\n+  __ evmovdqul(xmm18, xmm16, Assembler::AVX_512bit);\n+  __ evmovdqul(xmm19, xmm16, Assembler::AVX_512bit);\n+  __ evmovdqul(xmm12, Address(perms, 384), Assembler::AVX_512bit);\n+  __ evmovdqul(xmm13, xmm12, Assembler::AVX_512bit);\n+  __ evmovdqul(xmm14, xmm12, Assembler::AVX_512bit);\n+  __ evmovdqul(xmm15, xmm12, Assembler::AVX_512bit);\n+\n+  for (int i = 0; i < 8; i += 2) {\n+    __ evpermi2d(xmm(i\/2 + 16), xmm(i), xmm(i + 1), Assembler::AVX_512bit);\n+  }\n+  for (int i = 0; i < 8; i += 2) {\n+    __ evpermi2d(xmm(i \/ 2 + 12), xmm(i), xmm(i + 1), Assembler::AVX_512bit);\n+  }\n+  __ evmovdqul(xmm0, Address(zetas, 6 * 512), Assembler::AVX_512bit);\n+  __ evmovdqul(xmm1, Address(zetas, 6 * 512 + 64), Assembler::AVX_512bit);\n+  __ evmovdqul(xmm2, Address(zetas, 6 * 512 + 128), Assembler::AVX_512bit);\n+  __ evmovdqul(xmm3, Address(zetas, 6 * 512 + 192), Assembler::AVX_512bit);\n+\n+  montmulEven(20, 0, 12, 4, 20, 4);\n+\n+  __ vpshufd(xmm(12), xmm(12), 0xB1, Assembler::AVX_512bit);\n+  __ vpshufd(xmm(13), xmm(13), 0xB1, Assembler::AVX_512bit);\n+  __ vpshufd(xmm(14), xmm(14), 0xB1, Assembler::AVX_512bit);\n+  __ vpshufd(xmm(15), xmm(15), 0xB1, Assembler::AVX_512bit);\n+\n+  montmulEven(12, 0, 12, 4, 24, 4);\n+\n+  for (int i = 0; i < 4; i++) {\n+    __ evpermt2d(xmm(i + 12), xmm28, xmm(i + 20), Assembler::AVX_512bit);\n+  }\n+\n+  __ evpsubd(xmm(1), k0, xmm(16), xmm(12), false, Assembler::AVX_512bit);\n+  __ evpsubd(xmm(3), k0, xmm(17), xmm(13), false, Assembler::AVX_512bit);\n+  __ evpsubd(xmm(5), k0, xmm(18), xmm(14), false, Assembler::AVX_512bit);\n+  __ evpsubd(xmm(7), k0, xmm(19), xmm(15), false, Assembler::AVX_512bit);\n+\n+  __ evpaddd(xmm(0), k0, xmm(16), xmm(12), false, Assembler::AVX_512bit);\n+  __ evpaddd(xmm(2), k0, xmm(17), xmm(13), false, Assembler::AVX_512bit);\n+  __ evpaddd(xmm(4), k0, xmm(18), xmm(14), false, Assembler::AVX_512bit);\n+  __ evpaddd(xmm(6), k0, xmm(19), xmm(15), false, Assembler::AVX_512bit);\n+\n+  \/\/ level 7\n+  __ evmovdqul(xmm16, Address(perms, 448), Assembler::AVX_512bit);\n+  __ evmovdqul(xmm17, xmm16, Assembler::AVX_512bit);\n+  __ evmovdqul(xmm18, xmm16, Assembler::AVX_512bit);\n+  __ evmovdqul(xmm19, xmm16, Assembler::AVX_512bit);\n+  __ evmovdqul(xmm12, Address(perms, 512), Assembler::AVX_512bit);\n+  __ evmovdqul(xmm13, xmm12, Assembler::AVX_512bit);\n+  __ evmovdqul(xmm14, xmm12, Assembler::AVX_512bit);\n+  __ evmovdqul(xmm15, xmm12, Assembler::AVX_512bit);\n+\n+  for (int i = 0; i < 8; i += 2) {\n+    __ evpermi2d(xmm(i \/ 2 + 16), xmm(i), xmm(i + 1), Assembler::AVX_512bit);\n+  }\n+  for (int i = 0; i < 8; i += 2) {\n+    __ evpermi2d(xmm(i \/ 2 + 12), xmm(i), xmm(i + 1), Assembler::AVX_512bit);\n+  }\n+  __ evmovdqul(xmm0, Address(zetas, 7 * 512), Assembler::AVX_512bit);\n+  __ evmovdqul(xmm1, Address(zetas, 7 * 512 + 64), Assembler::AVX_512bit);\n+  __ evmovdqul(xmm2, Address(zetas, 7 * 512 + 128), Assembler::AVX_512bit);\n+  __ evmovdqul(xmm3, Address(zetas, 7 * 512 + 192), Assembler::AVX_512bit);\n+\n+  montmulEven(20, 0, 12, 4, 20, 4);\n+\n+  __ vpshufd(xmm(12), xmm(12), 0xB1, Assembler::AVX_512bit);\n+  __ vpshufd(xmm(13), xmm(13), 0xB1, Assembler::AVX_512bit);\n+  __ vpshufd(xmm(14), xmm(14), 0xB1, Assembler::AVX_512bit);\n+  __ vpshufd(xmm(15), xmm(15), 0xB1, Assembler::AVX_512bit);\n+  __ vpshufd(xmm(0), xmm(0), 0xB1, Assembler::AVX_512bit);\n+  __ vpshufd(xmm(1), xmm(1), 0xB1, Assembler::AVX_512bit);\n+  __ vpshufd(xmm(2), xmm(2), 0xB1, Assembler::AVX_512bit);\n+  __ vpshufd(xmm(3), xmm(3), 0xB1, Assembler::AVX_512bit);\n+\n+  montmulEven(12, 0, 12, 4, 24, 4);\n+\n+  for (int i = 0; i < 4; i++) {\n+    __ evpermt2d(xmm(i + 12), xmm28, xmm(i + 20), Assembler::AVX_512bit);\n+  }\n+\n+  __ evpsubd(xmm(21), k0, xmm(16), xmm(12), false, Assembler::AVX_512bit);\n+  __ evpsubd(xmm(23), k0, xmm(17), xmm(13), false, Assembler::AVX_512bit);\n+  __ evpsubd(xmm(25), k0, xmm(18), xmm(14), false, Assembler::AVX_512bit);\n+  __ evpsubd(xmm(27), k0, xmm(19), xmm(15), false, Assembler::AVX_512bit);\n+\n+  __ evpaddd(xmm(20), k0, xmm(16), xmm(12), false, Assembler::AVX_512bit);\n+  __ evpaddd(xmm(22), k0, xmm(17), xmm(13), false, Assembler::AVX_512bit);\n+  __ evpaddd(xmm(24), k0, xmm(18), xmm(14), false, Assembler::AVX_512bit);\n+  __ evpaddd(xmm(26), k0, xmm(19), xmm(15), false, Assembler::AVX_512bit);\n+\n+  __ evmovdqul(xmm0, Address(perms, 576), Assembler::AVX_512bit);\n+  __ evmovdqul(xmm2, xmm0, Assembler::AVX_512bit);\n+  __ evmovdqul(xmm4, xmm0, Assembler::AVX_512bit);\n+  __ evmovdqul(xmm6, xmm0, Assembler::AVX_512bit);\n+  __ evmovdqul(xmm1, Address(perms, 640), Assembler::AVX_512bit);\n+  __ evmovdqul(xmm3, xmm1, Assembler::AVX_512bit);\n+  __ evmovdqul(xmm5, xmm1, Assembler::AVX_512bit);\n+  __ evmovdqul(xmm7, xmm1, Assembler::AVX_512bit);\n+\n+  for (int i = 0; i < 8; i += 2) {\n+    __ evpermi2d(xmm(i), xmm(i + 20), xmm(i + 21), Assembler::AVX_512bit);\n+    __ evpermi2d(xmm(i + 1), xmm(i + 20), xmm(i + 21), Assembler::AVX_512bit);\n+  }\n+\n+  __ cmpl(iterations, 0);\n+  __ jcc(Assembler::equal, L_end);\n+\n+  for (int i = 0; i < 8; i++) {\n+    __ evmovdqul(Address(coeffs, i * 64), xmm(i), Assembler::AVX_512bit);\n+  }\n+\n+  for (int i = 8; i < 16; i++) {\n+    __ evmovdqul(xmm(i - 8), Address(coeffs, i * 64), Assembler::AVX_512bit);\n+  }\n+\n+  __ addptr(zetas, 256);\n+\n+  __ jmp(L_loop);\n+\n+  __ BIND(L_end);\n+\n+  for (int i = 0; i < 8; i++) {\n+    __ evmovdqul(Address(coeffs, (i + 8) * 64), xmm(i), Assembler::AVX_512bit);\n+  }\n+\n+  __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n+  __ mov64(rax, 0); \/\/ return 0\n+  __ ret(0);\n+\n+  return start;\n+}\n+\n+\/\/ Dilithium Inverse NTT function except the final mod Q division by 2^256.\n+\/\/ Implements\n+\/\/ static int implDilithiumAlmostInverseNtt(int[] coeffs, int[] zetas) {}\n+\/\/\n+\/\/ coeffs (int[256]) = c_rarg0\n+\/\/ zetas (int[256]) = c_rarg1\n+address StubGenerator::generate_dilithiumAlmostInverseNtt_avx512() {\n+\n+  __ align(CodeEntryAlignment);\n+  StubGenStubId stub_id = dilithiumAlmostInverseNtt_id;\n+  StubCodeMark mark(this, stub_id);\n+  address start = __ pc();\n+  __ enter();\n+\n+  Label L_loop, L_end;\n+\n+  const Register coeffs = c_rarg0;\n+  const Register zetas = c_rarg1;\n+\n+  const Register iterations = c_rarg2;\n+\n+  const Register dilithiumConsts = r10;\n+  const Register perms = r11;\n+\n+  __ lea(perms, ExternalAddress(dilithiumAvx512PermsAddr()));\n+  __ lea(dilithiumConsts, ExternalAddress(dilithiumAvx512ConstsAddr()));\n+\n+  __ evmovdqul(xmm28, Address(perms, 0), Assembler::AVX_512bit);\n+  __ vpbroadcastd(xmm30, Address(dilithiumConsts, 0), Assembler::AVX_512bit); \/\/ q^-1 mod 2^32\n+  __ vpbroadcastd(xmm31, Address(dilithiumConsts, 4), Assembler::AVX_512bit); \/\/ q\n+\n+  \/\/ We do levels 0-6 in two batches, each batch entirely in the vector registers\n+  for (int i = 0; i < 8; i++) {\n+    __ evmovdqul(xmm(i), Address(coeffs, i * 64), Assembler::AVX_512bit);\n+  }\n+\n+  __ movl(iterations, 2);\n+\n+  __ align(OptoLoopAlignment);\n+  __ BIND(L_loop);\n+\n+  __ subl(iterations, 1);\n+\n+  \/\/ level 0\n+  __ evmovdqul(xmm8, Address(perms, 704), Assembler::AVX_512bit);\n+  __ evmovdqul(xmm9, xmm8, Assembler::AVX_512bit);\n+  __ evmovdqul(xmm10, xmm8, Assembler::AVX_512bit);\n+  __ evmovdqul(xmm11, xmm8, Assembler::AVX_512bit);\n+  __ evmovdqul(xmm12, Address(perms, 768), Assembler::AVX_512bit);\n+  __ evmovdqul(xmm13, xmm12, Assembler::AVX_512bit);\n+  __ evmovdqul(xmm14, xmm12, Assembler::AVX_512bit);\n+  __ evmovdqul(xmm15, xmm12, Assembler::AVX_512bit);\n+\n+  for (int i = 0; i < 8; i += 2) {\n+    __ evpermi2d(xmm(i \/ 2 + 8), xmm(i), xmm(i + 1), Assembler::AVX_512bit);\n+    __ evpermi2d(xmm(i \/ 2 + 12), xmm(i), xmm(i + 1), Assembler::AVX_512bit);\n+  }\n+\n+  __ evpaddd(xmm(0), k0, xmm(8), xmm(12), false, Assembler::AVX_512bit);\n+  __ evpaddd(xmm(1), k0, xmm(9), xmm(13), false, Assembler::AVX_512bit);\n+  __ evpaddd(xmm(2), k0, xmm(10), xmm(14), false, Assembler::AVX_512bit);\n+  __ evpaddd(xmm(3), k0, xmm(11), xmm(15), false, Assembler::AVX_512bit);\n+\n+  __ evpsubd(xmm(8), k0, xmm(8), xmm(12), false, Assembler::AVX_512bit);\n+  __ evpsubd(xmm(9), k0, xmm(9), xmm(13), false, Assembler::AVX_512bit);\n+  __ evpsubd(xmm(10), k0, xmm(10), xmm(14), false, Assembler::AVX_512bit);\n+  __ evpsubd(xmm(11), k0, xmm(11), xmm(15), false, Assembler::AVX_512bit);\n+\n+  __ evmovdqul(xmm4, Address(zetas, 0), Assembler::AVX_512bit);\n+  __ evmovdqul(xmm5, Address(zetas, 64), Assembler::AVX_512bit);\n+  __ evmovdqul(xmm6, Address(zetas, 128), Assembler::AVX_512bit);\n+  __ evmovdqul(xmm7, Address(zetas, 192), Assembler::AVX_512bit);\n+\n+  montmulEven(20, 4, 8, 16, 20, 4);\n+\n+  __ vpshufd(xmm(4), xmm(4), 0xB1, Assembler::AVX_512bit);\n+  __ vpshufd(xmm(5), xmm(5), 0xB1, Assembler::AVX_512bit);\n+  __ vpshufd(xmm(6), xmm(6), 0xB1, Assembler::AVX_512bit);\n+  __ vpshufd(xmm(7), xmm(7), 0xB1, Assembler::AVX_512bit);\n+  __ vpshufd(xmm(8), xmm(8), 0xB1, Assembler::AVX_512bit);\n+  __ vpshufd(xmm(9), xmm(9), 0xB1, Assembler::AVX_512bit);\n+  __ vpshufd(xmm(10), xmm(10), 0xB1, Assembler::AVX_512bit);\n+  __ vpshufd(xmm(11), xmm(11), 0xB1, Assembler::AVX_512bit);\n+\n+  montmulEven(4, 4, 8, 16, 24, 4);\n+\n+  for (int i = 0; i < 4; i++) {\n+    __ evpermt2d(xmm(i + 4), xmm28, xmm(i + 20), Assembler::AVX_512bit);\n+  }\n+\n+  \/\/ level 1\n+  __ evmovdqul(xmm8, Address(perms, 832), Assembler::AVX_512bit);\n+  __ evmovdqul(xmm9, xmm8, Assembler::AVX_512bit);\n+  __ evmovdqul(xmm10, xmm8, Assembler::AVX_512bit);\n+  __ evmovdqul(xmm11, xmm8, Assembler::AVX_512bit);\n+  __ evmovdqul(xmm12, Address(perms, 896), Assembler::AVX_512bit);\n+  __ evmovdqul(xmm13, xmm12, Assembler::AVX_512bit);\n+  __ evmovdqul(xmm14, xmm12, Assembler::AVX_512bit);\n+  __ evmovdqul(xmm15, xmm12, Assembler::AVX_512bit);\n+\n+  for (int i = 0; i < 4; i++) {\n+    __ evpermi2d(xmm(i + 8), xmm(i), xmm(i + 4), Assembler::AVX_512bit);\n+    __ evpermi2d(xmm(i + 12), xmm(i), xmm(i + 4), Assembler::AVX_512bit);\n+  }\n+\n+  __ evpaddd(xmm(0), k0, xmm(8), xmm(12), false, Assembler::AVX_512bit);\n+  __ evpaddd(xmm(1), k0, xmm(9), xmm(13), false, Assembler::AVX_512bit);\n+  __ evpaddd(xmm(2), k0, xmm(10), xmm(14), false, Assembler::AVX_512bit);\n+  __ evpaddd(xmm(3), k0, xmm(11), xmm(15), false, Assembler::AVX_512bit);\n+\n+  __ evpsubd(xmm(8), k0, xmm(8), xmm(12), false, Assembler::AVX_512bit);\n+  __ evpsubd(xmm(9), k0, xmm(9), xmm(13), false, Assembler::AVX_512bit);\n+  __ evpsubd(xmm(10), k0, xmm(10), xmm(14), false, Assembler::AVX_512bit);\n+  __ evpsubd(xmm(11), k0, xmm(11), xmm(15), false, Assembler::AVX_512bit);\n+\n+  __ evmovdqul(xmm4, Address(zetas, 512), Assembler::AVX_512bit);\n+  __ evmovdqul(xmm5, Address(zetas, 512 + 64), Assembler::AVX_512bit);\n+  __ evmovdqul(xmm6, Address(zetas, 512 + 128), Assembler::AVX_512bit);\n+  __ evmovdqul(xmm7, Address(zetas, 512 + 192), Assembler::AVX_512bit);\n+\n+  montmulEven(20, 4, 8, 16, 20, 4);\n+\n+  __ vpshufd(xmm(8), xmm(8), 0xB1, Assembler::AVX_512bit);\n+  __ vpshufd(xmm(9), xmm(9), 0xB1, Assembler::AVX_512bit);\n+  __ vpshufd(xmm(10), xmm(10), 0xB1, Assembler::AVX_512bit);\n+  __ vpshufd(xmm(11), xmm(11), 0xB1, Assembler::AVX_512bit);\n+\n+  montmulEven(4, 4, 8, 16, 24, 4);\n+\n+  for (int i = 0; i < 4; i++) {\n+    __ evpermt2d(xmm(i + 4), xmm28, xmm(i + 20), Assembler::AVX_512bit);\n+  }\n+\n+  \/\/ level 2\n+  __ evmovdqul(xmm8, Address(perms, 960), Assembler::AVX_512bit);\n+  __ evmovdqul(xmm9, xmm8, Assembler::AVX_512bit);\n+  __ evmovdqul(xmm10, xmm8, Assembler::AVX_512bit);\n+  __ evmovdqul(xmm11, xmm8, Assembler::AVX_512bit);\n+  __ evmovdqul(xmm12, Address(perms, 1024), Assembler::AVX_512bit);\n+  __ evmovdqul(xmm13, xmm12, Assembler::AVX_512bit);\n+  __ evmovdqul(xmm14, xmm12, Assembler::AVX_512bit);\n+  __ evmovdqul(xmm15, xmm12, Assembler::AVX_512bit);\n+\n+  for (int i = 0; i < 4; i++) {\n+    __ evpermi2d(xmm(i + 8), xmm(i), xmm(i + 4), Assembler::AVX_512bit);\n+    __ evpermi2d(xmm(i + 12), xmm(i), xmm(i + 4), Assembler::AVX_512bit);\n+  }\n+\n+  __ evpaddd(xmm(0), k0, xmm(8), xmm(12), false, Assembler::AVX_512bit);\n+  __ evpaddd(xmm(1), k0, xmm(9), xmm(13), false, Assembler::AVX_512bit);\n+  __ evpaddd(xmm(2), k0, xmm(10), xmm(14), false, Assembler::AVX_512bit);\n+  __ evpaddd(xmm(3), k0, xmm(11), xmm(15), false, Assembler::AVX_512bit);\n+\n+  __ evpsubd(xmm(8), k0, xmm(8), xmm(12), false, Assembler::AVX_512bit);\n+  __ evpsubd(xmm(9), k0, xmm(9), xmm(13), false, Assembler::AVX_512bit);\n+  __ evpsubd(xmm(10), k0, xmm(10), xmm(14), false, Assembler::AVX_512bit);\n+  __ evpsubd(xmm(11), k0, xmm(11), xmm(15), false, Assembler::AVX_512bit);\n+\n+  __ evmovdqul(xmm4, Address(zetas, 2 * 512), Assembler::AVX_512bit);\n+  __ evmovdqul(xmm5, Address(zetas, 2 * 512 + 64), Assembler::AVX_512bit);\n+  __ evmovdqul(xmm6, Address(zetas, 2 * 512 + 128), Assembler::AVX_512bit);\n+  __ evmovdqul(xmm7, Address(zetas, 2 * 512 + 192), Assembler::AVX_512bit);\n+\n+  montmulEven(20, 4, 8, 16, 20, 4);\n+\n+  __ vpshufd(xmm(8), xmm(8), 0xB1, Assembler::AVX_512bit);\n+  __ vpshufd(xmm(9), xmm(9), 0xB1, Assembler::AVX_512bit);\n+  __ vpshufd(xmm(10), xmm(10), 0xB1, Assembler::AVX_512bit);\n+  __ vpshufd(xmm(11), xmm(11), 0xB1, Assembler::AVX_512bit);\n+\n+  montmulEven(4, 4, 8, 16, 24, 4);\n+\n+  for (int i = 0; i < 4; i++) {\n+    __ evpermt2d(xmm(i + 4), xmm28, xmm(i + 20), Assembler::AVX_512bit);\n+  }\n+\n+  \/\/ level 3\n+  __ evmovdqul(xmm8, Address(perms, 1088), Assembler::AVX_512bit);\n+  __ evmovdqul(xmm9, xmm8, Assembler::AVX_512bit);\n+  __ evmovdqul(xmm10, xmm8, Assembler::AVX_512bit);\n+  __ evmovdqul(xmm11, xmm8, Assembler::AVX_512bit);\n+  __ evmovdqul(xmm12, Address(perms, 1152), Assembler::AVX_512bit);\n+  __ evmovdqul(xmm13, xmm12, Assembler::AVX_512bit);\n+  __ evmovdqul(xmm14, xmm12, Assembler::AVX_512bit);\n+  __ evmovdqul(xmm15, xmm12, Assembler::AVX_512bit);\n+\n+  for (int i = 0; i < 4; i++) {\n+    __ evpermi2d(xmm(i + 8), xmm(i), xmm(i + 4), Assembler::AVX_512bit);\n+    __ evpermi2d(xmm(i + 12), xmm(i), xmm(i + 4), Assembler::AVX_512bit);\n+  }\n+\n+  __ evpaddd(xmm(0), k0, xmm(8), xmm(12), false, Assembler::AVX_512bit);\n+  __ evpaddd(xmm(1), k0, xmm(9), xmm(13), false, Assembler::AVX_512bit);\n+  __ evpaddd(xmm(2), k0, xmm(10), xmm(14), false, Assembler::AVX_512bit);\n+  __ evpaddd(xmm(3), k0, xmm(11), xmm(15), false, Assembler::AVX_512bit);\n+\n+  __ evpsubd(xmm(8), k0, xmm(8), xmm(12), false, Assembler::AVX_512bit);\n+  __ evpsubd(xmm(9), k0, xmm(9), xmm(13), false, Assembler::AVX_512bit);\n+  __ evpsubd(xmm(10), k0, xmm(10), xmm(14), false, Assembler::AVX_512bit);\n+  __ evpsubd(xmm(11), k0, xmm(11), xmm(15), false, Assembler::AVX_512bit);\n+\n+  __ evmovdqul(xmm4, Address(zetas, 3 * 512), Assembler::AVX_512bit);\n+  __ evmovdqul(xmm5, Address(zetas, 3 * 512 + 64), Assembler::AVX_512bit);\n+  __ evmovdqul(xmm6, Address(zetas, 3 * 512 + 128), Assembler::AVX_512bit);\n+  __ evmovdqul(xmm7, Address(zetas, 3 * 512 + 192), Assembler::AVX_512bit);\n+\n+  montmulEven(20, 4, 8, 16, 20, 4);\n+\n+  __ vpshufd(xmm(8), xmm(8), 0xB1, Assembler::AVX_512bit);\n+  __ vpshufd(xmm(9), xmm(9), 0xB1, Assembler::AVX_512bit);\n+  __ vpshufd(xmm(10), xmm(10), 0xB1, Assembler::AVX_512bit);\n+  __ vpshufd(xmm(11), xmm(11), 0xB1, Assembler::AVX_512bit);\n+\n+  montmulEven(4, 4, 8, 16, 24, 4);\n+\n+  for (int i = 0; i < 4; i++) {\n+    __ evpermt2d(xmm(i + 4), xmm28, xmm(i + 20), Assembler::AVX_512bit);\n+  }\n+\n+  \/\/ level 4\n+  __ evmovdqul(xmm8, Address(perms, 1216), Assembler::AVX_512bit);\n+  __ evmovdqul(xmm9, xmm8, Assembler::AVX_512bit);\n+  __ evmovdqul(xmm10, xmm8, Assembler::AVX_512bit);\n+  __ evmovdqul(xmm11, xmm8, Assembler::AVX_512bit);\n+  __ evmovdqul(xmm12, Address(perms, 1280), Assembler::AVX_512bit);\n+  __ evmovdqul(xmm13, xmm12, Assembler::AVX_512bit);\n+  __ evmovdqul(xmm14, xmm12, Assembler::AVX_512bit);\n+  __ evmovdqul(xmm15, xmm12, Assembler::AVX_512bit);\n+\n+  for (int i = 0; i < 4; i++) {\n+    __ evpermi2d(xmm(i + 8), xmm(i), xmm(i + 4), Assembler::AVX_512bit);\n+    __ evpermi2d(xmm(i + 12), xmm(i), xmm(i + 4), Assembler::AVX_512bit);\n+  }\n+\n+  __ evpaddd(xmm(0), k0, xmm(8), xmm(12), false, Assembler::AVX_512bit);\n+  __ evpaddd(xmm(1), k0, xmm(9), xmm(13), false, Assembler::AVX_512bit);\n+  __ evpaddd(xmm(2), k0, xmm(10), xmm(14), false, Assembler::AVX_512bit);\n+  __ evpaddd(xmm(3), k0, xmm(11), xmm(15), false, Assembler::AVX_512bit);\n+\n+  __ evpsubd(xmm(8), k0, xmm(8), xmm(12), false, Assembler::AVX_512bit);\n+  __ evpsubd(xmm(9), k0, xmm(9), xmm(13), false, Assembler::AVX_512bit);\n+  __ evpsubd(xmm(10), k0, xmm(10), xmm(14), false, Assembler::AVX_512bit);\n+  __ evpsubd(xmm(11), k0, xmm(11), xmm(15), false, Assembler::AVX_512bit);\n+\n+  __ evmovdqul(xmm4, Address(zetas, 4 * 512), Assembler::AVX_512bit);\n+  __ evmovdqul(xmm5, Address(zetas, 4 * 512 + 64), Assembler::AVX_512bit);\n+  __ evmovdqul(xmm6, Address(zetas, 4 * 512 + 128), Assembler::AVX_512bit);\n+  __ evmovdqul(xmm7, Address(zetas, 4 * 512 + 192), Assembler::AVX_512bit);\n+\n+  montmulEven(20, 4, 8, 16, 20, 4);\n+\n+  __ vpshufd(xmm(8), xmm(8), 0xB1, Assembler::AVX_512bit);\n+  __ vpshufd(xmm(9), xmm(9), 0xB1, Assembler::AVX_512bit);\n+  __ vpshufd(xmm(10), xmm(10), 0xB1, Assembler::AVX_512bit);\n+  __ vpshufd(xmm(11), xmm(11), 0xB1, Assembler::AVX_512bit);\n+\n+  montmulEven(4, 4, 8, 16, 24, 4);\n+\n+  for (int i = 0; i < 4; i++) {\n+    __ evpermt2d(xmm(i + 4), xmm28, xmm(i + 20), Assembler::AVX_512bit);\n+  }\n+\n+  \/\/ level 5\n+  __ evpsubd(xmm(8), k0, xmm(0), xmm(1), false, Assembler::AVX_512bit);\n+  __ evpsubd(xmm(9), k0, xmm(4), xmm(5), false, Assembler::AVX_512bit);\n+  __ evpsubd(xmm(10), k0, xmm(2), xmm(3), false, Assembler::AVX_512bit);\n+  __ evpsubd(xmm(11), k0, xmm(6), xmm(7), false, Assembler::AVX_512bit);\n+\n+  __ evpaddd(xmm(0), k0, xmm(0), xmm(1), false, Assembler::AVX_512bit);\n+  __ evpaddd(xmm(1), k0, xmm(4), xmm(5), false, Assembler::AVX_512bit);\n+  __ evpaddd(xmm(2), k0, xmm(2), xmm(3), false, Assembler::AVX_512bit);\n+  __ evpaddd(xmm(3), k0, xmm(6), xmm(7), false, Assembler::AVX_512bit);;\n+\n+  __ evmovdqul(xmm4, Address(zetas, 5 * 512), Assembler::AVX_512bit);\n+  __ evmovdqul(xmm5, Address(zetas, 5 * 512 + 64), Assembler::AVX_512bit);\n+  __ evmovdqul(xmm6, Address(zetas, 5 * 512 + 128), Assembler::AVX_512bit);\n+  __ evmovdqul(xmm7, Address(zetas, 5 * 512 + 192), Assembler::AVX_512bit);\n+\n+  montmulEven(20, 4, 8, 16, 20, 4);\n+\n+  __ vpshufd(xmm(8), xmm(8), 0xB1, Assembler::AVX_512bit);\n+  __ vpshufd(xmm(9), xmm(9), 0xB1, Assembler::AVX_512bit);\n+  __ vpshufd(xmm(10), xmm(10), 0xB1, Assembler::AVX_512bit);\n+  __ vpshufd(xmm(11), xmm(11), 0xB1, Assembler::AVX_512bit);\n+\n+  montmulEven(4, 4, 8, 16, 24, 4);\n+\n+  for (int i = 0; i < 4; i++) {\n+    __ evpermt2d(xmm(i + 4), xmm28, xmm(i + 20), Assembler::AVX_512bit);\n+  }\n+\n+  \/\/ level 6\n+  __ evpsubd(xmm(8), k0, xmm(0), xmm(2), false, Assembler::AVX_512bit);\n+  __ evpsubd(xmm(9), k0, xmm(1), xmm(3), false, Assembler::AVX_512bit);\n+  __ evpsubd(xmm(10), k0, xmm(4), xmm(6), false, Assembler::AVX_512bit);\n+  __ evpsubd(xmm(11), k0, xmm(5), xmm(7), false, Assembler::AVX_512bit);\n+\n+  __ evpaddd(xmm(0), k0, xmm(0), xmm(2), false, Assembler::AVX_512bit);\n+  __ evpaddd(xmm(2), k0, xmm(4), xmm(6), false, Assembler::AVX_512bit);\n+  __ evpaddd(xmm(1), k0, xmm(1), xmm(3), false, Assembler::AVX_512bit);\n+  __ evpaddd(xmm(3), k0, xmm(5), xmm(7), false, Assembler::AVX_512bit);\n+\n+  __ evmovdqul(xmm4, Address(zetas, 6 * 512), Assembler::AVX_512bit);\n+  __ evmovdqul(xmm5, Address(zetas, 6 * 512 + 64), Assembler::AVX_512bit);\n+  __ evmovdqul(xmm6, Address(zetas, 6 * 512 + 128), Assembler::AVX_512bit);\n+  __ evmovdqul(xmm7, Address(zetas, 6 * 512 + 192), Assembler::AVX_512bit);\n+\n+  montmulEven(20, 4, 8, 16, 20, 4);\n+\n+  __ vpshufd(xmm(8), xmm(8), 0xB1, Assembler::AVX_512bit);\n+  __ vpshufd(xmm(9), xmm(9), 0xB1, Assembler::AVX_512bit);\n+  __ vpshufd(xmm(10), xmm(10), 0xB1, Assembler::AVX_512bit);\n+  __ vpshufd(xmm(11), xmm(11), 0xB1, Assembler::AVX_512bit);\n+\n+  montmulEven(4, 4, 8, 16, 24, 4);\n+\n+  for (int i = 0; i < 4; i++) {\n+    __ evpermt2d(xmm(i + 4), xmm28, xmm(i + 20), Assembler::AVX_512bit);\n+  }\n+\n+  __ cmpl(iterations, 0);\n+  __ jcc(Assembler::equal, L_end);\n+\n+  \/\/ save the coefficients of the first batch, adjust the zetas\n+  \/\/ and load the second batch of coefficients\n+  for (int i = 0; i < 8; i++) {\n+    __ evmovdqul(Address(coeffs, i * 64), xmm(i), Assembler::AVX_512bit);\n+  }\n+\n+  __ addptr(zetas, 256);\n+\n+  for (int i = 0; i < 8; i++) {\n+    __ evmovdqul(xmm(i), Address(coeffs, i * 64 + 512), Assembler::AVX_512bit);\n+  }\n+\n+  __ jmp(L_loop);\n+\n+  __ BIND(L_end);\n+\n+  \/\/ load the coeffs of the first batch of coefficients that were saved after\n+  \/\/ level 6 into Zmm_8-Zmm_15 and do the last level entirely in the vector registers\n+  for (int i = 0; i < 8; i++) {\n+    __ evmovdqul(xmm(i + 8), Address(coeffs, i * 64), Assembler::AVX_512bit);\n+  }\n+\n+  \/\/ level 7\n+  for (int i = 0; i < 8; i++) {\n+    __ evpsubd(xmm(i + 16), k0, xmm(i + 8), xmm(i), false, Assembler::AVX_512bit);\n+  }\n+\n+  for (int i = 0; i < 8; i++) {\n+    __ evpaddd(xmm(i), k0, xmm(i), xmm(i + 8), false, Assembler::AVX_512bit);\n+  }\n+\n+  for (int i = 0; i < 8; i++) {\n+    __ evmovdqul(Address(coeffs, i * 64), xmm(i), Assembler::AVX_512bit);\n+  }\n+\n+  __ evmovdqul(xmm29, Address(zetas, 7 * 512), Assembler::AVX_512bit);\n+\n+  montmulEven(4, 16, 29, 8, 12, 4);\n+\n+  __ vpshufd(xmm(16), xmm(16), 0xB1, Assembler::AVX_512bit);\n+  __ vpshufd(xmm(17), xmm(17), 0xB1, Assembler::AVX_512bit);\n+  __ vpshufd(xmm(18), xmm(18), 0xB1, Assembler::AVX_512bit);\n+  __ vpshufd(xmm(19), xmm(19), 0xB1, Assembler::AVX_512bit);\n+\n+  montmulEven(0, 16, 29, 8, 12, 4);\n+\n+  for (int i = 0; i < 4; i++) {\n+    __ evpermt2d(xmm(i), xmm28, xmm(i + 4), Assembler::AVX_512bit);\n+  }\n+\n+  for (int i = 0; i < 4; i++) {\n+    __ evmovdqul(Address(coeffs, i * 64 + 512), xmm(i), Assembler::AVX_512bit);\n+  }\n+\n+  montmulEven(4, 20, 29, 8, 12, 4);\n+\n+  __ vpshufd(xmm(20), xmm(20), 0xB1, Assembler::AVX_512bit);\n+  __ vpshufd(xmm(21), xmm(21), 0xB1, Assembler::AVX_512bit);\n+  __ vpshufd(xmm(22), xmm(22), 0xB1, Assembler::AVX_512bit);\n+  __ vpshufd(xmm(23), xmm(23), 0xB1, Assembler::AVX_512bit);\n+\n+  montmulEven(0, 20, 29, 8, 12, 4);\n+\n+  for (int i = 0; i < 4; i++) {\n+    __ evpermt2d(xmm(i), xmm28, xmm(i + 4), Assembler::AVX_512bit);\n+  }\n+\n+  for (int i = 0; i < 4; i++) {\n+    __ evmovdqul(Address(coeffs, i * 64 + 768), xmm(i), Assembler::AVX_512bit);\n+  }\n+\n+  __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n+  __ mov64(rax, 0); \/\/ return 0\n+  __ ret(0);\n+\n+  return start;\n+}\n+\n+\/\/ Dilithium multiply polynomials in the NTT domain.\n+\/\/ Implements\n+\/\/ static int implDilithiumNttMult(\n+\/\/              int[] result, int[] ntta, int[] nttb {}\n+\/\/\n+\/\/ result (int[256]) = c_rarg0\n+\/\/ poly1 (int[256]) = c_rarg1\n+\/\/ poly2 (int[256]) = c_rarg2\n+address StubGenerator::generate_dilithiumNttMult_avx512() {\n+\n+  __ align(CodeEntryAlignment);\n+  StubGenStubId stub_id = dilithiumNttMult_id;\n+  StubCodeMark mark(this, stub_id);\n+  address start = __ pc();\n+  __ enter();\n+\n+  Label L_loop;\n+\n+  const Register result = c_rarg0;\n+  const Register poly1 = c_rarg1;\n+  const Register poly2 = c_rarg2;\n+\n+  const Register dilithiumConsts = c_rarg3;\n+  const Register perms = r10;\n+  const Register len = r11;\n+\n+  __ lea(dilithiumConsts, ExternalAddress(dilithiumAvx512ConstsAddr()));\n+  __ lea(perms, ExternalAddress(dilithiumAvx512PermsAddr()));\n+\n+  __ vpbroadcastd(xmm30, Address(dilithiumConsts, 0), Assembler::AVX_512bit); \/\/ q^-1 mod 2^32\n+  __ vpbroadcastd(xmm31, Address(dilithiumConsts, 4), Assembler::AVX_512bit); \/\/ q\n+  __ vpbroadcastd(xmm29, Address(dilithiumConsts, 12), Assembler::AVX_512bit); \/\/ 2^64 mod q\n+  __ evmovdqul(xmm28, Address(perms, 0), Assembler::AVX_512bit);\n+\n+  __ movl(len, 4);\n+\n+  __ align(OptoLoopAlignment);\n+  __ BIND(L_loop);\n+\n+  for (int i = 0; i < 4; i++) {\n+    __ evmovdqul(xmm(i), Address(poly1, i * 64), Assembler::AVX_512bit);\n+    __ evmovdqul(xmm(i + 4), Address(poly2, i * 64), Assembler::AVX_512bit);\n+  }\n+\n+  montmulEven(8, 4, 29, 12, 16, 4);\n+  for (int i = 0; i < 4; i++) {\n+    __ vpshufd(xmm(i + 8), xmm(i + 8), 0xB1, Assembler::AVX_512bit);\n+  }\n+  montmulEven(8, 0, 8, 12, 16, 4);\n+  for (int i = 0; i < 4; i++) {\n+    __ vpshufd(xmm(i), xmm(i), 0xB1, Assembler::AVX_512bit);\n+    __ vpshufd(xmm(i + 4), xmm(i + 4), 0xB1, Assembler::AVX_512bit);\n+  }\n+  montmulEven(4, 4, 29, 12, 16, 4);\n+  for (int i = 0; i < 4; i++) {\n+    __ vpshufd(xmm(i + 4), xmm(i + 4), 0xB1, Assembler::AVX_512bit);\n+  }\n+  montmulEven(0, 0, 4, 12, 16, 4);\n+  for (int i = 0; i < 4; i++) {\n+    __ evpermt2d(xmm(i), xmm28, xmm(i + 8), Assembler::AVX_512bit);\n+  }\n+  for (int i = 0; i < 4; i++) {\n+    __ evmovdqul(Address(result, i * 64), xmm(i), Assembler::AVX_512bit);\n+  }\n+\n+  __ subl(len, 1);\n+  __ addptr(poly1, 256);\n+  __ addptr(poly2, 256);\n+  __ addptr(result, 256);\n+  __ cmpl(len, 0);\n+  __ jcc(Assembler::notEqual, L_loop);\n+\n+\n+  __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n+  __ mov64(rax, 0); \/\/ return 0\n+  __ ret(0);\n+\n+  return start;\n+}\n+\n+\/\/ Dilithium Motgomery multiply an array by a constant.\n+\/\/ Implements\n+\/\/ static int implDilithiumMontMulByConstant(int[] coeffs, int constant) {}\n+\/\/\n+\/\/ coeffs (int[256]) = c_rarg0\n+\/\/ constant (int) = c_rarg1\n+address StubGenerator::generate_dilithiumMontMulByConstant_avx512() {\n+\n+  __ align(CodeEntryAlignment);\n+  StubGenStubId stub_id = dilithiumMontMulByConstant_id;\n+  StubCodeMark mark(this, stub_id);\n+  address start = __ pc();\n+  __ enter();\n+\n+  Label L_loop;\n+\n+  const Register coeffs = c_rarg0;\n+  const Register constant = c_rarg1;\n+\n+  const Register perms = c_rarg2;\n+  const Register dilithiumConsts = c_rarg3;\n+  const Register len = r10;\n+\n+  __ lea(dilithiumConsts, ExternalAddress(dilithiumAvx512ConstsAddr()));\n+  __ lea(perms, ExternalAddress(dilithiumAvx512PermsAddr()));\n+\n+  __ vpbroadcastd(xmm30, Address(dilithiumConsts, 0), Assembler::AVX_512bit); \/\/ q^-1 mod 2^32\n+  __ vpbroadcastd(xmm31, Address(dilithiumConsts, 4), Assembler::AVX_512bit); \/\/ q\n+  __ evmovdqul(xmm28, Address(perms, 0), Assembler::AVX_512bit);\n+\n+  __ evpbroadcastd(xmm29, constant, Assembler::AVX_512bit); \/\/ constant multiplier\n+\n+  __ movl(len, 2);\n+\n+  __ align(OptoLoopAlignment);\n+  __ BIND(L_loop);\n+\n+  for (int i = 0; i < 8; i++) {\n+    __ evmovdqul(xmm(i), Address(coeffs, i * 64), Assembler::AVX_512bit);\n+  }\n+  montmulEven(8, 0, 29, 8, 16, 8);\n+  for (int i = 0; i < 8; i++) {\n+    __ vpshufd(xmm(i),xmm(i), 0xB1, Assembler::AVX_512bit);\n+  }\n+  montmulEven(0, 0, 29, 0, 16, 8);\n+  for (int i = 0; i < 8; i++) {\n+    __ evpermt2d(xmm(i), xmm28, xmm(i + 8), Assembler::AVX_512bit);\n+  }\n+  for (int i = 0; i < 8; i++) {\n+    __ evmovdqul(Address(coeffs, i * 64), xmm(i), Assembler::AVX_512bit);\n+  }\n+\n+  __ subl(len, 1);\n+  __ addptr(coeffs, 512);\n+  __ cmpl(len, 0);\n+  __ jcc(Assembler::notEqual, L_loop);\n+\n+  __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n+  __ mov64(rax, 0); \/\/ return 0\n+  __ ret(0);\n+\n+  return start;\n+}\n+\n+\/\/ Dilithium decompose poly.\n+\/\/ Implements\n+\/\/ static int implDilithiumDecomposePoly(int[] coeffs, int constant) {}\n+\/\/\n+\/\/ input (int[256]) = c_rarg0\n+\/\/ lowPart (int[256]) = c_rarg1\n+\/\/ highPart (int[256]) = c_rarg2\n+\/\/ twoGamma2  (int) = c_rarg3\n+\/\/ multiplier (int) = c_rarg4\n+address StubGenerator::generate_dilithiumDecomposePoly_avx512() {\n+\n+  __ align(CodeEntryAlignment);\n+  StubGenStubId stub_id = dilithiumDecomposePoly_id;\n+  StubCodeMark mark(this, stub_id);\n+  address start = __ pc();\n+  __ enter();\n+\n+  Label L_loop;\n+\n+  const Register input = c_rarg0;\n+  const Register lowPart = c_rarg1;\n+  const Register highPart = c_rarg2;\n+  const Register twoGamma2 = c_rarg3;\n+\n+  const Register len = c_rarg3;\n+  const Register dilithiumConsts = r9;\n+  const Register tmp = r10;\n+\n+  __ lea(dilithiumConsts, ExternalAddress(dilithiumAvx512ConstsAddr()));\n+\n+  __ xorl(tmp, tmp);\n+  __ evpbroadcastd(xmm24, tmp, Assembler::AVX_512bit); \/\/ 0\n+  __ addl(tmp, 1);\n+  __ evpbroadcastd(xmm25, tmp, Assembler::AVX_512bit); \/\/ 1\n+  __ vpbroadcastd(xmm30, Address(dilithiumConsts, 4), Assembler::AVX_512bit); \/\/ q\n+  __ vpbroadcastd(xmm31, Address(dilithiumConsts, 16), Assembler::AVX_512bit); \/\/ addend for mod q reduce\n+\n+  __ evpbroadcastd(xmm28, twoGamma2, Assembler::AVX_512bit); \/\/ 2 * gamma2\n+\n+  #ifndef _WIN64\n+    const Register multiplier = c_rarg4;\n+  #else\n+    const Address multiplier_mem(rbp, 6 * wordSize);\n+    const Register multiplier = c_rarg3;\n+    __ movptr(multiplier, multiplier_mem);\n+  #endif\n+  __ evpbroadcastd(xmm29, multiplier, Assembler::AVX_512bit); \/\/ multiplier for mod 2 * gamma2 reduce\n+\n+  __ evpsubd(xmm26, k0, xmm30, xmm25, false, Assembler::AVX_512bit); \/\/ q - 1\n+  __ evpsrad(xmm27, k0, xmm28, 1, false, Assembler::AVX_512bit); \/\/ gamma2\n+\n+  __ movl(len, 1024);\n+\n+  __ align(OptoLoopAlignment);\n+  __ BIND(L_loop);\n+\n+  __ evmovdqul(xmm0, Address(input, 0), Assembler::AVX_512bit);\n+  __ evmovdqul(xmm1, Address(input, 64), Assembler::AVX_512bit);\n+  __ evmovdqul(xmm2, Address(input, 128), Assembler::AVX_512bit);\n+  __ evmovdqul(xmm3, Address(input, 192), Assembler::AVX_512bit);\n+\n+  __ addptr(input, 256);\n+\n+  \/\/ rplus in xmm0\n+  \/\/          rplus = rplus - ((rplus + 5373807) >> 23) * dilithium_q;\n+  __ evpaddd(xmm4, k0, xmm0, xmm31, false, Assembler::AVX_512bit);\n+  __ evpaddd(xmm5, k0, xmm1, xmm31, false, Assembler::AVX_512bit);\n+  __ evpaddd(xmm6, k0, xmm2, xmm31, false, Assembler::AVX_512bit);\n+  __ evpaddd(xmm7, k0, xmm3, xmm31, false, Assembler::AVX_512bit);\n+\n+  __ evpsrad(xmm4, k0, xmm4, 23, false, Assembler::AVX_512bit);\n+  __ evpsrad(xmm5, k0, xmm5, 23, false, Assembler::AVX_512bit);\n+  __ evpsrad(xmm6, k0, xmm6, 23, false, Assembler::AVX_512bit);\n+  __ evpsrad(xmm7, k0, xmm7, 23, false, Assembler::AVX_512bit);\n+\n+  __ evpmulld(xmm4, k0, xmm4, xmm30, false, Assembler::AVX_512bit);\n+  __ evpmulld(xmm5, k0, xmm5, xmm30, false, Assembler::AVX_512bit);\n+  __ evpmulld(xmm6, k0, xmm6, xmm30, false, Assembler::AVX_512bit);\n+  __ evpmulld(xmm7, k0, xmm7, xmm30, false, Assembler::AVX_512bit);\n+\n+  __ evpsubd(xmm0, k0, xmm0, xmm4, false, Assembler::AVX_512bit);\n+  __ evpsubd(xmm1, k0, xmm1, xmm5, false, Assembler::AVX_512bit);\n+  __ evpsubd(xmm2, k0, xmm2, xmm6, false, Assembler::AVX_512bit);\n+  __ evpsubd(xmm3, k0, xmm3, xmm7, false, Assembler::AVX_512bit);\n+\n+            \/\/ rplus in xmm0\n+\n+\/\/            rplus = rplus + ((rplus >> 31) & dilithium_q);\n+  __ evpsrad(xmm4, k0, xmm0, 31, false, Assembler::AVX_512bit);\n+  __ evpsrad(xmm5, k0, xmm1, 31, false, Assembler::AVX_512bit);\n+  __ evpsrad(xmm6, k0, xmm2, 31, false, Assembler::AVX_512bit);\n+  __ evpsrad(xmm7, k0, xmm3, 31, false, Assembler::AVX_512bit);\n+\n+  __ evpandd(xmm4, k0, xmm4, xmm30, false, Assembler::AVX_512bit);\n+  __ evpandd(xmm5, k0, xmm5, xmm30, false, Assembler::AVX_512bit);\n+  __ evpandd(xmm6, k0, xmm6, xmm30, false, Assembler::AVX_512bit);\n+  __ evpandd(xmm7, k0, xmm7, xmm30, false, Assembler::AVX_512bit);\n+\n+  __ evpaddd(xmm0, k0, xmm0, xmm4, false, Assembler::AVX_512bit);\n+  __ evpaddd(xmm1, k0, xmm1, xmm5, false, Assembler::AVX_512bit);\n+  __ evpaddd(xmm2, k0, xmm2, xmm6, false, Assembler::AVX_512bit);\n+  __ evpaddd(xmm3, k0, xmm3, xmm7, false, Assembler::AVX_512bit);\n+\n+            \/\/ rplus in xmm0\n+\n+\/\/           int quotient = (rplus * multiplier) >> 22;\n+  __ evpmulld(xmm4, k0, xmm0, xmm29, false, Assembler::AVX_512bit);\n+  __ evpmulld(xmm5, k0, xmm1, xmm29, false, Assembler::AVX_512bit);\n+  __ evpmulld(xmm6, k0, xmm2, xmm29, false, Assembler::AVX_512bit);\n+  __ evpmulld(xmm7, k0, xmm3, xmm29, false, Assembler::AVX_512bit);\n+\n+  __ evpsrad(xmm4, k0, xmm4, 22, false, Assembler::AVX_512bit);\n+  __ evpsrad(xmm5, k0, xmm5, 22, false, Assembler::AVX_512bit);\n+  __ evpsrad(xmm6, k0, xmm6, 22, false, Assembler::AVX_512bit);\n+  __ evpsrad(xmm7, k0, xmm7, 22, false, Assembler::AVX_512bit);\n+\n+            \/\/ quotient in xmm4\n+\n+\/\/            int r0 = rplus - quotient * twoGamma2;\n+  __ evpmulld(xmm8, k0, xmm4, xmm28, false, Assembler::AVX_512bit);\n+  __ evpmulld(xmm9, k0, xmm5, xmm28, false, Assembler::AVX_512bit);\n+  __ evpmulld(xmm10, k0, xmm6, xmm28, false, Assembler::AVX_512bit);\n+  __ evpmulld(xmm11, k0, xmm7, xmm28, false, Assembler::AVX_512bit);\n+\n+  __ evpsubd(xmm8, k0, xmm0, xmm8, false, Assembler::AVX_512bit);\n+  __ evpsubd(xmm9, k0, xmm1, xmm9, false, Assembler::AVX_512bit);\n+  __ evpsubd(xmm10, k0, xmm2, xmm10, false, Assembler::AVX_512bit);\n+  __ evpsubd(xmm11, k0, xmm3, xmm11, false, Assembler::AVX_512bit);\n+\n+            \/\/ r0 in xmm8\n+\n+\/\/            int mask = (twoGamma2 - r0) >> 22;\n+  __ evpsubd(xmm12, k0, xmm28, xmm8, false, Assembler::AVX_512bit);\n+  __ evpsubd(xmm13, k0, xmm28, xmm9, false, Assembler::AVX_512bit);\n+  __ evpsubd(xmm14, k0, xmm28, xmm10, false, Assembler::AVX_512bit);\n+  __ evpsubd(xmm15, k0, xmm28, xmm11, false, Assembler::AVX_512bit);\n+\n+  __ evpsrad(xmm12, k0, xmm12, 22, false, Assembler::AVX_512bit);\n+  __ evpsrad(xmm13, k0, xmm13, 22, false, Assembler::AVX_512bit);\n+  __ evpsrad(xmm14, k0, xmm14, 22, false, Assembler::AVX_512bit);\n+  __ evpsrad(xmm15, k0, xmm15, 22, false, Assembler::AVX_512bit);\n+\n+            \/\/ mask in xmm12\n+\n+\/\/            r0 -= (mask & twoGamma2);\n+  __ evpandd(xmm16, k0, xmm12, xmm28, false, Assembler::AVX_512bit);\n+  __ evpandd(xmm17, k0, xmm13, xmm28, false, Assembler::AVX_512bit);\n+  __ evpandd(xmm18, k0, xmm14, xmm28, false, Assembler::AVX_512bit);\n+  __ evpandd(xmm19, k0, xmm15, xmm28, false, Assembler::AVX_512bit);\n+\n+  __ evpsubd(xmm8, k0, xmm8, xmm16, false, Assembler::AVX_512bit);\n+  __ evpsubd(xmm9, k0, xmm9, xmm17, false, Assembler::AVX_512bit);\n+  __ evpsubd(xmm10, k0, xmm10, xmm18, false, Assembler::AVX_512bit);\n+  __ evpsubd(xmm11, k0, xmm11, xmm19, false, Assembler::AVX_512bit);\n+\n+            \/\/ r0 in xmm8\n+\n+\/\/            quotient += (mask & 1);\n+  __ evpandd(xmm16, k0, xmm12, xmm25, false, Assembler::AVX_512bit);\n+  __ evpandd(xmm17, k0, xmm13, xmm25, false, Assembler::AVX_512bit);\n+  __ evpandd(xmm18, k0, xmm14, xmm25, false, Assembler::AVX_512bit);\n+  __ evpandd(xmm19, k0, xmm15, xmm25, false, Assembler::AVX_512bit);\n+\n+  __ evpaddd(xmm4, k0, xmm4, xmm16, false, Assembler::AVX_512bit);\n+  __ evpaddd(xmm5, k0, xmm5, xmm17, false, Assembler::AVX_512bit);\n+  __ evpaddd(xmm6, k0, xmm6, xmm18, false, Assembler::AVX_512bit);\n+  __ evpaddd(xmm7, k0, xmm7, xmm19, false, Assembler::AVX_512bit);\n+\n+\/\/            mask = (twoGamma2 \/ 2 - r0) >> 31;\n+  __ evpsubd(xmm12, k0, xmm27, xmm8, false, Assembler::AVX_512bit);\n+  __ evpsubd(xmm13, k0, xmm27, xmm9, false, Assembler::AVX_512bit);\n+  __ evpsubd(xmm14, k0, xmm27, xmm10, false, Assembler::AVX_512bit);\n+  __ evpsubd(xmm15, k0, xmm27, xmm11, false, Assembler::AVX_512bit);\n+\n+  __ evpsrad(xmm12, k0, xmm12, 31, false, Assembler::AVX_512bit);\n+  __ evpsrad(xmm13, k0, xmm13, 31, false, Assembler::AVX_512bit);\n+  __ evpsrad(xmm14, k0, xmm14, 31, false, Assembler::AVX_512bit);\n+  __ evpsrad(xmm15, k0, xmm15, 31, false, Assembler::AVX_512bit);\n+\n+\/\/            r0 -= (mask & twoGamma2);\n+  __ evpandd(xmm16, k0, xmm12, xmm28, false, Assembler::AVX_512bit);\n+  __ evpandd(xmm17, k0, xmm13, xmm28, false, Assembler::AVX_512bit);\n+  __ evpandd(xmm18, k0, xmm14, xmm28, false, Assembler::AVX_512bit);\n+  __ evpandd(xmm19, k0, xmm15, xmm28, false, Assembler::AVX_512bit);\n+\n+  __ evpsubd(xmm8, k0, xmm8, xmm16, false, Assembler::AVX_512bit);\n+  __ evpsubd(xmm9, k0, xmm9, xmm17, false, Assembler::AVX_512bit);\n+  __ evpsubd(xmm10, k0, xmm10, xmm18, false, Assembler::AVX_512bit);\n+  __ evpsubd(xmm11, k0, xmm11, xmm19, false, Assembler::AVX_512bit);\n+\n+            \/\/ r0 in xmm8\n+\n+\/\/            quotient += (mask & 1);\n+  __ evpandd(xmm16, k0, xmm12, xmm25, false, Assembler::AVX_512bit);\n+  __ evpandd(xmm17, k0, xmm13, xmm25, false, Assembler::AVX_512bit);\n+  __ evpandd(xmm18, k0, xmm14, xmm25, false, Assembler::AVX_512bit);\n+  __ evpandd(xmm19, k0, xmm15, xmm25, false, Assembler::AVX_512bit);\n+\n+  __ evpaddd(xmm4, k0, xmm4, xmm16, false, Assembler::AVX_512bit);\n+  __ evpaddd(xmm5, k0, xmm5, xmm17, false, Assembler::AVX_512bit);\n+  __ evpaddd(xmm6, k0, xmm6, xmm18, false, Assembler::AVX_512bit);\n+  __ evpaddd(xmm7, k0, xmm7, xmm19, false, Assembler::AVX_512bit);\n+\n+            \/\/ quotient in xmm4\n+\n+\/\/            int r1 = rplus - r0 - (dilithium_q - 1);\n+  __ evpsubd(xmm16, k0, xmm0, xmm8, false, Assembler::AVX_512bit);\n+  __ evpsubd(xmm17, k0, xmm1, xmm9, false, Assembler::AVX_512bit);\n+  __ evpsubd(xmm18, k0, xmm2, xmm10, false, Assembler::AVX_512bit);\n+  __ evpsubd(xmm19, k0, xmm3, xmm11, false, Assembler::AVX_512bit);\n+\n+  __ evpsubd(xmm16, k0, xmm16, xmm26, false, Assembler::AVX_512bit);\n+  __ evpsubd(xmm17, k0, xmm17, xmm26, false, Assembler::AVX_512bit);\n+  __ evpsubd(xmm18, k0, xmm18, xmm26, false, Assembler::AVX_512bit);\n+  __ evpsubd(xmm19, k0, xmm19, xmm26, false, Assembler::AVX_512bit);\n+\n+            \/\/ r1 in xmm16\n+\n+\/\/            r1 = (r1 | (-r1)) >> 31; \/\/ 0 if rplus - r0 == (dilithium_q - 1), -1 otherwise\n+  __ evpsubd(xmm20, k0, xmm24, xmm16, false, Assembler::AVX_512bit);\n+  __ evpsubd(xmm21, k0, xmm24, xmm17, false, Assembler::AVX_512bit);\n+  __ evpsubd(xmm22, k0, xmm24, xmm18, false, Assembler::AVX_512bit);\n+  __ evpsubd(xmm23, k0, xmm24, xmm19, false, Assembler::AVX_512bit);\n+\n+  __ evporq(xmm16, k0, xmm16, xmm20, false, Assembler::AVX_512bit);\n+  __ evporq(xmm17, k0, xmm17, xmm21, false, Assembler::AVX_512bit);\n+  __ evporq(xmm18, k0, xmm18, xmm22, false, Assembler::AVX_512bit);\n+  __ evporq(xmm19, k0, xmm19, xmm23, false, Assembler::AVX_512bit);\n+\n+  __ evpsubd(xmm12, k0, xmm24, xmm25, false, Assembler::AVX_512bit); \/\/ -1\n+\n+  __ evpsrad(xmm0, k0, xmm16, 31, false, Assembler::AVX_512bit);\n+  __ evpsrad(xmm1, k0, xmm17, 31, false, Assembler::AVX_512bit);\n+  __ evpsrad(xmm2, k0, xmm18, 31, false, Assembler::AVX_512bit);\n+  __ evpsrad(xmm3, k0, xmm19, 31, false, Assembler::AVX_512bit);\n+\n+            \/\/ r1 in xmm0\n+\n+\/\/            r0 += ~r1;\n+  __ evpxorq(xmm20, k0, xmm0, xmm12, false, Assembler::AVX_512bit);\n+  __ evpxorq(xmm21, k0, xmm1, xmm12, false, Assembler::AVX_512bit);\n+  __ evpxorq(xmm22, k0, xmm2, xmm12, false, Assembler::AVX_512bit);\n+  __ evpxorq(xmm23, k0, xmm3, xmm12, false, Assembler::AVX_512bit);\n+\n+  __ evpaddd(xmm8, k0, xmm8, xmm20, false, Assembler::AVX_512bit);\n+  __ evpaddd(xmm9, k0, xmm9, xmm21, false, Assembler::AVX_512bit);\n+  __ evpaddd(xmm10, k0, xmm10, xmm22, false, Assembler::AVX_512bit);\n+  __ evpaddd(xmm11, k0, xmm11, xmm23, false, Assembler::AVX_512bit);\n+\n+            \/\/ r0 in xmm8\n+\n+\/\/            r1 = r1 & quotient;\n+  __ evpandd(xmm0, k0, xmm4, xmm0, false, Assembler::AVX_512bit);\n+  __ evpandd(xmm1, k0, xmm5, xmm1, false, Assembler::AVX_512bit);\n+  __ evpandd(xmm2, k0, xmm6, xmm2, false, Assembler::AVX_512bit);\n+  __ evpandd(xmm3, k0, xmm7, xmm3, false, Assembler::AVX_512bit);\n+\n+\/\/             r1 in xmm0\n+\n+\/\/            lowPart[m] = r0;\n+\/\/            highPart[m] = r1;\n+  __ evmovdqul(Address(highPart, 0), xmm0, Assembler::AVX_512bit);\n+  __ evmovdqul(Address(highPart, 64), xmm1, Assembler::AVX_512bit);\n+  __ evmovdqul(Address(highPart, 128), xmm2, Assembler::AVX_512bit);\n+  __ evmovdqul(Address(highPart, 192), xmm3, Assembler::AVX_512bit);\n+\n+  __ evmovdqul(Address(lowPart, 0), xmm8, Assembler::AVX_512bit);\n+  __ evmovdqul(Address(lowPart, 64), xmm9, Assembler::AVX_512bit);\n+  __ evmovdqul(Address(lowPart, 128), xmm10, Assembler::AVX_512bit);\n+  __ evmovdqul(Address(lowPart, 192), xmm11, Assembler::AVX_512bit);\n+\n+  __ subl(len, 256);\n+  __ addptr(highPart, 256);\n+  __ addptr(lowPart, 256);\n+  __ cmpl(len, 0);\n+  __ jcc(Assembler::notEqual, L_loop);\n+\n+  __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n+  __ mov64(rax, 0); \/\/ return 0\n+  __ ret(0);\n+\n+  return start;\n+}\n","filename":"src\/hotspot\/cpu\/x86\/stubGenerator_x86_64_dilithium.cpp","additions":1404,"deletions":0,"binary":false,"changes":1404,"status":"added"},{"patch":"@@ -85,0 +85,1 @@\n+    StubRoutines::_double_keccak         = generate_double_keccak();\n@@ -180,0 +181,1 @@\n+  __ align(OptoLoopAlignment);\n@@ -234,0 +236,1 @@\n+  __ align(OptoLoopAlignment);\n@@ -260,1 +263,1 @@\n-  \/\/ rho and sigma steps).\n+  \/\/ rho and pi steps).\n@@ -282,1 +285,1 @@\n-  \/\/ The combined rho and sigma steps are done.\n+  \/\/ The combined rho and pi steps are done.\n@@ -338,0 +341,205 @@\n+\n+\/\/ Inputs:\n+\/\/   c_rarg0   - long[]  state0\n+\/\/   c_rarg1   - long[]  state1\n+\/\/\n+\/\/ Performs two keccak() computations in parallel. The steps of the\n+\/\/ two computations are executed interleaved.\n+address StubGenerator::generate_double_keccak() {\n+  __ align(CodeEntryAlignment);\n+  StubGenStubId stub_id = double_keccak_id;\n+  StubCodeMark mark(this, stub_id);\n+  address start = __ pc();\n+\n+  const Register state0 = c_rarg0;\n+  const Register state1 = c_rarg1;\n+\n+  const Register permsAndRots = c_rarg2;\n+  const Register round_consts = c_rarg3;\n+  const Register constant2use = r10;\n+  const Register roundsLeft = r11;\n+\n+  __ align(OptoLoopAlignment);\n+  Label rounds24_loop;\n+\n+  __ enter();\n+\n+  __ lea(permsAndRots, ExternalAddress(permsAndRotsAddr()));\n+  __ lea(round_consts, ExternalAddress(round_constsAddr()));\n+\n+  \/\/ set up the masks\n+  __ mov64(rax,1);\n+  __ kmovbl(k1, rax);\n+  __ addl(rax,2);\n+  __ kmovbl(k2, rax);\n+  __ addl(rax, 4);\n+  __ kmovbl(k3, rax);\n+  __ addl(rax, 8);\n+  __ kmovbl(k4, rax);\n+  __ addl(rax, 16);\n+  __ kmovbl(k5, rax);\n+\n+  \/\/ load the states\n+  __ evmovdquq(xmm0, k5, Address(state0, 0), false, Assembler::AVX_512bit);\n+  __ evmovdquq(xmm1, k5, Address(state0, 40), false, Assembler::AVX_512bit);\n+  __ evmovdquq(xmm2, k5, Address(state0, 80), false, Assembler::AVX_512bit);\n+  __ evmovdquq(xmm3, k5, Address(state0, 120), false, Assembler::AVX_512bit);\n+  __ evmovdquq(xmm4, k5, Address(state0, 160), false, Assembler::AVX_512bit);\n+\n+  __ evmovdquq(xmm10, k5, Address(state1, 0), false, Assembler::AVX_512bit);\n+  __ evmovdquq(xmm11, k5, Address(state1, 40), false, Assembler::AVX_512bit);\n+  __ evmovdquq(xmm12, k5, Address(state1, 80), false, Assembler::AVX_512bit);\n+  __ evmovdquq(xmm13, k5, Address(state1, 120), false, Assembler::AVX_512bit);\n+  __ evmovdquq(xmm14, k5, Address(state1, 160), false, Assembler::AVX_512bit);\n+\n+  \/\/ load the permutation and rotation constants\n+  __ evmovdquq(xmm17, Address(permsAndRots, 0), Assembler::AVX_512bit);\n+  __ evmovdquq(xmm18, Address(permsAndRots, 64), Assembler::AVX_512bit);\n+  __ evmovdquq(xmm19, Address(permsAndRots, 128), Assembler::AVX_512bit);\n+  __ evmovdquq(xmm20, Address(permsAndRots, 192), Assembler::AVX_512bit);\n+  __ evmovdquq(xmm21, Address(permsAndRots, 256), Assembler::AVX_512bit);\n+  __ evmovdquq(xmm22, Address(permsAndRots, 320), Assembler::AVX_512bit);\n+  __ evmovdquq(xmm23, Address(permsAndRots, 384), Assembler::AVX_512bit);\n+  __ evmovdquq(xmm24, Address(permsAndRots, 448), Assembler::AVX_512bit);\n+  __ evmovdquq(xmm25, Address(permsAndRots, 512), Assembler::AVX_512bit);\n+  __ evmovdquq(xmm26, Address(permsAndRots, 576), Assembler::AVX_512bit);\n+  __ evmovdquq(xmm27, Address(permsAndRots, 640), Assembler::AVX_512bit);\n+  __ evmovdquq(xmm28, Address(permsAndRots, 704), Assembler::AVX_512bit);\n+  __ evmovdquq(xmm29, Address(permsAndRots, 768), Assembler::AVX_512bit);\n+  __ evmovdquq(xmm30, Address(permsAndRots, 832), Assembler::AVX_512bit);\n+  __ evmovdquq(xmm31, Address(permsAndRots, 896), Assembler::AVX_512bit);\n+\n+  \/\/ there will be 24 keccak rounds\n+  \/\/ The same operations as the ones in generate_sha3_implCompress are\n+  \/\/ performed, but in parallel for two states: one in regs z0-z5, using z6\n+  \/\/ as the scratch register and the other in z10-z15, using z16 as the\n+  \/\/ scratch register.\n+  \/\/ The permutation and rotation constants, that are loaded into z17-z31,\n+  \/\/ are shared between the two computations.\n+  __ movl(roundsLeft, 24);\n+  \/\/ load round_constants base\n+  __ movptr(constant2use, round_consts);\n+\n+  __ align(OptoLoopAlignment);\n+  __ BIND(rounds24_loop);\n+  __ subl( roundsLeft, 1);\n+\n+  __ evmovdquw(xmm5, xmm0, Assembler::AVX_512bit);\n+  __ evmovdquw(xmm15, xmm10, Assembler::AVX_512bit);\n+  __ vpternlogq(xmm5, 150, xmm1, xmm2, Assembler::AVX_512bit);\n+  __ vpternlogq(xmm15, 150, xmm11, xmm12, Assembler::AVX_512bit);\n+  __ vpternlogq(xmm5, 150, xmm3, xmm4, Assembler::AVX_512bit);\n+  __ vpternlogq(xmm15, 150, xmm13, xmm14, Assembler::AVX_512bit);\n+  __ evprolq(xmm6, xmm5, 1, Assembler::AVX_512bit);\n+  __ evprolq(xmm16, xmm15, 1, Assembler::AVX_512bit);\n+  __ evpermt2q(xmm5, xmm30, xmm5, Assembler::AVX_512bit);\n+  __ evpermt2q(xmm15, xmm30, xmm15, Assembler::AVX_512bit);\n+  __ evpermt2q(xmm6, xmm31, xmm6, Assembler::AVX_512bit);\n+  __ evpermt2q(xmm16, xmm31, xmm16, Assembler::AVX_512bit);\n+  __ vpternlogq(xmm0, 150, xmm5, xmm6, Assembler::AVX_512bit);\n+  __ vpternlogq(xmm10, 150, xmm15, xmm16, Assembler::AVX_512bit);\n+  __ vpternlogq(xmm1, 150, xmm5, xmm6, Assembler::AVX_512bit);\n+  __ vpternlogq(xmm11, 150, xmm15, xmm16, Assembler::AVX_512bit);\n+  __ vpternlogq(xmm2, 150, xmm5, xmm6, Assembler::AVX_512bit);\n+  __ vpternlogq(xmm12, 150, xmm15, xmm16, Assembler::AVX_512bit);\n+  __ vpternlogq(xmm3, 150, xmm5, xmm6, Assembler::AVX_512bit);\n+  __ vpternlogq(xmm13, 150, xmm15, xmm16, Assembler::AVX_512bit);\n+  __ vpternlogq(xmm4, 150, xmm5, xmm6, Assembler::AVX_512bit);\n+  __ vpternlogq(xmm14, 150, xmm15, xmm16, Assembler::AVX_512bit);\n+  __ evpermt2q(xmm4, xmm17, xmm3, Assembler::AVX_512bit);\n+  __ evpermt2q(xmm14, xmm17, xmm13, Assembler::AVX_512bit);\n+  __ evpermt2q(xmm3, xmm18, xmm2, Assembler::AVX_512bit);\n+  __ evpermt2q(xmm13, xmm18, xmm12, Assembler::AVX_512bit);\n+  __ evpermt2q(xmm2, xmm17, xmm1, Assembler::AVX_512bit);\n+  __ evpermt2q(xmm12, xmm17, xmm11, Assembler::AVX_512bit);\n+  __ evpermt2q(xmm1, xmm19, xmm0, Assembler::AVX_512bit);\n+  __ evpermt2q(xmm11, xmm19, xmm10, Assembler::AVX_512bit);\n+  __ evpermt2q(xmm4, xmm20, xmm2, Assembler::AVX_512bit);\n+  __ evpermt2q(xmm14, xmm20, xmm12, Assembler::AVX_512bit);\n+  __ evprolvq(xmm1, xmm1, xmm27, Assembler::AVX_512bit);\n+  __ evprolvq(xmm11, xmm11, xmm27, Assembler::AVX_512bit);\n+  __ evprolvq(xmm3, xmm3, xmm28, Assembler::AVX_512bit);\n+  __ evprolvq(xmm13, xmm13, xmm28, Assembler::AVX_512bit);\n+  __ evprolvq(xmm4, xmm4, xmm29, Assembler::AVX_512bit);\n+  __ evprolvq(xmm14, xmm14, xmm29, Assembler::AVX_512bit);\n+  __ evmovdquw(xmm2, xmm1, Assembler::AVX_512bit);\n+  __ evmovdquw(xmm12, xmm11, Assembler::AVX_512bit);\n+  __ evmovdquw(xmm5, xmm3, Assembler::AVX_512bit);\n+  __ evmovdquw(xmm15, xmm13, Assembler::AVX_512bit);\n+  __ evpermt2q(xmm0, xmm21, xmm4, Assembler::AVX_512bit);\n+  __ evpermt2q(xmm10, xmm21, xmm14, Assembler::AVX_512bit);\n+  __ evpermt2q(xmm1, xmm22, xmm3, Assembler::AVX_512bit);\n+  __ evpermt2q(xmm11, xmm22, xmm13, Assembler::AVX_512bit);\n+  __ evpermt2q(xmm5, xmm22, xmm2, Assembler::AVX_512bit);\n+  __ evpermt2q(xmm15, xmm22, xmm12, Assembler::AVX_512bit);\n+  __ evmovdquw(xmm3, xmm1, Assembler::AVX_512bit);\n+  __ evmovdquw(xmm13, xmm11, Assembler::AVX_512bit);\n+  __ evmovdquw(xmm2, xmm5, Assembler::AVX_512bit);\n+  __ evmovdquw(xmm12, xmm15, Assembler::AVX_512bit);\n+  __ evpermt2q(xmm1, xmm23, xmm4, Assembler::AVX_512bit);\n+  __ evpermt2q(xmm11, xmm23, xmm14, Assembler::AVX_512bit);\n+  __ evpermt2q(xmm2, xmm24, xmm4, Assembler::AVX_512bit);\n+  __ evpermt2q(xmm12, xmm24, xmm14, Assembler::AVX_512bit);\n+  __ evpermt2q(xmm3, xmm25, xmm4, Assembler::AVX_512bit);\n+  __ evpermt2q(xmm13, xmm25, xmm14, Assembler::AVX_512bit);\n+  __ evpermt2q(xmm4, xmm26, xmm5, Assembler::AVX_512bit);\n+  __ evpermt2q(xmm14, xmm26, xmm15, Assembler::AVX_512bit);\n+\n+  __ evpermt2q(xmm5, xmm31, xmm0, Assembler::AVX_512bit);\n+  __ evpermt2q(xmm15, xmm31, xmm10, Assembler::AVX_512bit);\n+  __ evpermt2q(xmm6, xmm31, xmm5, Assembler::AVX_512bit);\n+  __ evpermt2q(xmm16, xmm31, xmm15, Assembler::AVX_512bit);\n+  __ vpternlogq(xmm0, 180, xmm6, xmm5, Assembler::AVX_512bit);\n+  __ vpternlogq(xmm10, 180, xmm16, xmm15, Assembler::AVX_512bit);\n+\n+  __ evpermt2q(xmm5, xmm31, xmm1, Assembler::AVX_512bit);\n+  __ evpermt2q(xmm15, xmm31, xmm11, Assembler::AVX_512bit);\n+  __ evpermt2q(xmm6, xmm31, xmm5, Assembler::AVX_512bit);\n+  __ evpermt2q(xmm16, xmm31, xmm15, Assembler::AVX_512bit);\n+  __ vpternlogq(xmm1, 180, xmm6, xmm5, Assembler::AVX_512bit);\n+  __ vpternlogq(xmm11, 180, xmm16, xmm15, Assembler::AVX_512bit);\n+\n+  __ evpxorq(xmm0, k1, xmm0, Address(constant2use, 0), true, Assembler::AVX_512bit);\n+  __ evpxorq(xmm10, k1, xmm10, Address(constant2use, 0), true, Assembler::AVX_512bit);\n+  __ addptr(constant2use, 8);\n+\n+  __ evpermt2q(xmm5, xmm31, xmm2, Assembler::AVX_512bit);\n+  __ evpermt2q(xmm15, xmm31, xmm12, Assembler::AVX_512bit);\n+  __ evpermt2q(xmm6, xmm31, xmm5, Assembler::AVX_512bit);\n+  __ evpermt2q(xmm16, xmm31, xmm15, Assembler::AVX_512bit);\n+  __ vpternlogq(xmm2, 180, xmm6, xmm5, Assembler::AVX_512bit);\n+  __ vpternlogq(xmm12, 180, xmm16, xmm15, Assembler::AVX_512bit);\n+\n+  __ evpermt2q(xmm5, xmm31, xmm3, Assembler::AVX_512bit);\n+  __ evpermt2q(xmm15, xmm31, xmm13, Assembler::AVX_512bit);\n+  __ evpermt2q(xmm6, xmm31, xmm5, Assembler::AVX_512bit);\n+  __ evpermt2q(xmm16, xmm31, xmm15, Assembler::AVX_512bit);\n+  __ vpternlogq(xmm3, 180, xmm6, xmm5, Assembler::AVX_512bit);\n+  __ vpternlogq(xmm13, 180, xmm16, xmm15, Assembler::AVX_512bit);\n+  __ evpermt2q(xmm5, xmm31, xmm4, Assembler::AVX_512bit);\n+  __ evpermt2q(xmm15, xmm31, xmm14, Assembler::AVX_512bit);\n+  __ evpermt2q(xmm6, xmm31, xmm5, Assembler::AVX_512bit);\n+  __ evpermt2q(xmm16, xmm31, xmm15, Assembler::AVX_512bit);\n+  __ vpternlogq(xmm4, 180, xmm6, xmm5, Assembler::AVX_512bit);\n+  __ vpternlogq(xmm14, 180, xmm16, xmm15, Assembler::AVX_512bit);\n+  __ cmpl(roundsLeft, 0);\n+  __ jcc(Assembler::notEqual, rounds24_loop);\n+\n+  \/\/ store the states\n+  __ evmovdquq(Address(state0, 0), k5, xmm0, true, Assembler::AVX_512bit);\n+  __ evmovdquq(Address(state0, 40), k5, xmm1, true, Assembler::AVX_512bit);\n+  __ evmovdquq(Address(state0, 80), k5, xmm2, true, Assembler::AVX_512bit);\n+  __ evmovdquq(Address(state0, 120), k5, xmm3, true, Assembler::AVX_512bit);\n+  __ evmovdquq(Address(state0, 160), k5, xmm4, true, Assembler::AVX_512bit);\n+\n+  __ evmovdquq(Address(state1, 0), k5, xmm10, true, Assembler::AVX_512bit);\n+  __ evmovdquq(Address(state1, 40), k5, xmm11, true, Assembler::AVX_512bit);\n+  __ evmovdquq(Address(state1, 80), k5, xmm12, true, Assembler::AVX_512bit);\n+  __ evmovdquq(Address(state1, 120), k5, xmm13, true, Assembler::AVX_512bit);\n+  __ evmovdquq(Address(state1, 160), k5, xmm14, true, Assembler::AVX_512bit);\n+\n+  __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n+  __ ret(0);\n+\n+  return start;\n+}\n","filename":"src\/hotspot\/cpu\/x86\/stubGenerator_x86_64_sha3.cpp","additions":210,"deletions":2,"binary":false,"changes":212,"status":"modified"},{"patch":"@@ -1249,0 +1249,14 @@\n+  \/\/ Dilithium Intrinsics\n+  \/\/ Currently we only have them for AVX512\n+#ifdef _LP64\n+  if (supports_evex() && supports_avx512bw()) {\n+      if (FLAG_IS_DEFAULT(UseDilithiumIntrinsics)) {\n+          UseDilithiumIntrinsics = true;\n+      }\n+  } else\n+#endif\n+   if (UseDilithiumIntrinsics) {\n+      warning(\"Intrinsics for ML-DSA are not available on this CPU.\");\n+      FLAG_SET_DEFAULT(UseDilithiumIntrinsics, false);\n+  }\n+\n","filename":"src\/hotspot\/cpu\/x86\/vm_version_x86.cpp","additions":14,"deletions":0,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -571,1 +571,1 @@\n- do_class(sun_security_provider_ML_DSA,      \"sun\/security\/provider\/ML_DSA\")                                            \\\n+  do_class(sun_security_provider_ML_DSA,      \"sun\/security\/provider\/ML_DSA\")                                           \\\n","filename":"src\/hotspot\/share\/classfile\/vmIntrinsics.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -743,0 +743,2 @@\n+  do_stub(compiler, double_keccak)                                      \\\n+  do_entry(compiler, double_keccak, double_keccak, double_keccak)       \\\n@@ -746,2 +748,0 @@\n-  do_stub(compiler, double_keccak)                                      \\\n-  do_entry(compiler, double_keccak, double_keccak, double_keccak)       \\\n","filename":"src\/hotspot\/share\/runtime\/stubDeclarations.hpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"}]}