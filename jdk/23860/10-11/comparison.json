{"files":[{"patch":"@@ -123,1 +123,1 @@\n-\/\/    the odd numbered slots of a third register using montmulEven().\n+\/\/    the odd numbered slots of a third register.\n@@ -131,37 +131,1 @@\n-\n-\/\/ Montgomery multiplication of the *even* numbered slices of sequences\n-\/\/ of parCnt consecutive registers. Zmm_inputReg1 and Zmm_inputReg2 are the\n-\/\/ starts of the two sequences. When inputReg2 == 29, we use that register as\n-\/\/ the second multiplicand in each multiplication.\n-\/\/ The result goes to the *odd* numbered slices of Zmm_outputReg.\n-\/\/ The parCnt long cosecutive sequences of registers that start with\n-\/\/ Zmm_scratch1 and Zmm_scrath2 are used as scratch registers, so their\n-\/\/ contents will be clobbered.\n-\/\/ The output register sequence  can overlap any of the input and scratch\n-\/\/ register sequences, however the two scratch register sequences should be\n-\/\/ non-overlapping.\n-\/\/ Zmm_31 should contain q and\n-\/\/ Zmm_30 should contain q^-1 mod 2^32 in all of their slices.\n-static void montmulEven(int outputReg, int inputReg1,  int inputReg2,\n-                        int scratchReg1, int scratchReg2,\n-                        int parCnt, MacroAssembler *_masm) {\n-  for (int i = 0; i < parCnt; i++) {\n-    __ vpmuldq(xmm(i + scratchReg1), xmm(i + inputReg1),\n-               xmm((inputReg2 == 29) ? 29 : inputReg2 + i), Assembler::AVX_512bit);\n-  }\n-  for (int i = 0; i < parCnt; i++) {\n-    __ vpmulld(xmm(i + scratchReg2), xmm(i + scratchReg1), montQInvModR,\n-               Assembler::AVX_512bit);\n-  }\n-  for (int i = 0; i < parCnt; i++) {\n-    __ vpmuldq(xmm(i + scratchReg2), xmm(i + scratchReg2), dilithium_q,\n-               Assembler::AVX_512bit);\n-  }\n-  for (int i = 0; i < parCnt; i++) {\n-    __ evpsubd(xmm(i + outputReg), k0, xmm(i + scratchReg1),\n-               xmm(i + scratchReg2), false, Assembler::AVX_512bit);\n-  }\n-}\n-\n-\/\/ Full Montgomery multiplication of the corresponding slices of two register\n-\/\/ sets of 4 registers each. The indexes of the registers to be multiplied\n+\/\/ The indexes of the registers to be multiplied\n@@ -238,18 +202,0 @@\n-\/\/ input in Zmm0-Zmm7, the constant is repeated in all slots of Zmm29\n-\/\/ qinvmodR and q are repeated in all slots of Zmm30 and Zmm31, resp.\n-\/\/ Zmm8-Zmm23 used as scratch registers\n-\/\/ result goes to Zmm0-Zmm7\n-static void montMulByConst128(MacroAssembler *_masm) {\n-  montmulEven(8, 0, 29, 8, 16, 8, _masm);\n-\n-  for (int i = 0; i < 8; i++) {\n-    __ vpshufd(xmm(i),xmm(i), 0xB1, Assembler::AVX_512bit);\n-  }\n-\n-  montmulEven(0, 0, 29, 0, 16, 8, _masm);\n-\n-  for (int i = 0; i < 8; i++) {\n-    __ evpermt2d(xmm(i), montMulPerm, xmm(i + 8), Assembler::AVX_512bit);\n-  }\n-}\n-\n@@ -347,0 +293,10 @@\n+\n+  \/\/ Each level represents one iteration of the outer for loop of the Java version\n+  \/\/ In each of these iterations half of the coefficients are (Montgomery)\n+  \/\/ multiplied by a zeta corresponding to the coefficient and then these\n+  \/\/ products will be added to and subtracted from the other half of the\n+  \/\/ coefficients. In each level we just collect the coefficients (using\n+  \/\/ evpermi2d() instructions where necessary, i.e. in levels 4-7) that need to\n+  \/\/ be multiplied by the zetas in one set, the rest to another set of vector\n+  \/\/ registers, then redistribute the addition\/substraction results.\n+\n@@ -537,0 +493,10 @@\n+  \/\/ Each level represents one iteration of the outer for loop of the\n+  \/\/ Java version.\n+  \/\/ In each of these iterations half of the coefficients are added to and\n+  \/\/ subtracted from the other half of the coefficients then the result of\n+  \/\/ the substartion is (Montgomery) multiplied by the corresponding zetas.\n+  \/\/ In each level we just collect the coefficients (using evpermi2d()\n+  \/\/ instructions where necessary, i.e. on levels 0-4) so that the results of\n+  \/\/ the additions and subtractions go to the vector registers so that they\n+  \/\/ align with each other and the zetas.\n+\n@@ -660,1 +626,2 @@\n-  montMulByConst128(_masm);\n+  montMul64(xmm0_3, xmm0_3, xmm29_29, xmm16_27, _masm);\n+  montMul64(xmm4_7, xmm4_7, xmm29_29, xmm16_27, _masm);\n@@ -764,1 +731,1 @@\n-  \/\/ the following four vector registers are used in montMulByConst128\n+  \/\/ the following four vector registers are used in montMul64\n@@ -781,1 +748,2 @@\n-  montMulByConst128(_masm);\n+  montMul64(xmm0_3, xmm0_3, xmm29_29, xmm16_27, _masm);\n+  montMul64(xmm4_7, xmm4_7, xmm29_29, xmm16_27, _masm);\n","filename":"src\/hotspot\/cpu\/x86\/stubGenerator_x86_64_dilithium.cpp","additions":27,"deletions":59,"binary":false,"changes":86,"status":"modified"},{"patch":"@@ -41,0 +41,2 @@\n+#define xmm(i) as_XMMRegister(i)\n+\n@@ -152,5 +154,3 @@\n-  __ evmovdquq(xmm0, k5, Address(state, 0), false, Assembler::AVX_512bit);\n-  __ evmovdquq(xmm1, k5, Address(state, 40), false, Assembler::AVX_512bit);\n-  __ evmovdquq(xmm2, k5, Address(state, 80), false, Assembler::AVX_512bit);\n-  __ evmovdquq(xmm3, k5, Address(state, 120), false, Assembler::AVX_512bit);\n-  __ evmovdquq(xmm4, k5, Address(state, 160), false, Assembler::AVX_512bit);\n+  for (int i = 0; i < 5; i++) {\n+    __ evmovdquq(xmm(i), k5, Address(state, i * 40), false, Assembler::AVX_512bit);\n+  }\n@@ -159,15 +159,3 @@\n-  __ evmovdquq(xmm17, Address(permsAndRots, 0), Assembler::AVX_512bit);\n-  __ evmovdquq(xmm18, Address(permsAndRots, 64), Assembler::AVX_512bit);\n-  __ evmovdquq(xmm19, Address(permsAndRots, 128), Assembler::AVX_512bit);\n-  __ evmovdquq(xmm20, Address(permsAndRots, 192), Assembler::AVX_512bit);\n-  __ evmovdquq(xmm21, Address(permsAndRots, 256), Assembler::AVX_512bit);\n-  __ evmovdquq(xmm22, Address(permsAndRots, 320), Assembler::AVX_512bit);\n-  __ evmovdquq(xmm23, Address(permsAndRots, 384), Assembler::AVX_512bit);\n-  __ evmovdquq(xmm24, Address(permsAndRots, 448), Assembler::AVX_512bit);\n-  __ evmovdquq(xmm25, Address(permsAndRots, 512), Assembler::AVX_512bit);\n-  __ evmovdquq(xmm26, Address(permsAndRots, 576), Assembler::AVX_512bit);\n-  __ evmovdquq(xmm27, Address(permsAndRots, 640), Assembler::AVX_512bit);\n-  __ evmovdquq(xmm28, Address(permsAndRots, 704), Assembler::AVX_512bit);\n-  __ evmovdquq(xmm29, Address(permsAndRots, 768), Assembler::AVX_512bit);\n-  __ evmovdquq(xmm30, Address(permsAndRots, 832), Assembler::AVX_512bit);\n-  __ evmovdquq(xmm31, Address(permsAndRots, 896), Assembler::AVX_512bit);\n+  for (int i = 0; i < 15; i++) {\n+    __ evmovdquq(xmm(i + 17), Address(permsAndRots, i * 64), Assembler::AVX_512bit);\n+  }\n@@ -320,5 +308,3 @@\n-  __ evmovdquq(Address(state, 0), k5, xmm0, true, Assembler::AVX_512bit);\n-  __ evmovdquq(Address(state, 40), k5, xmm1, true, Assembler::AVX_512bit);\n-  __ evmovdquq(Address(state, 80), k5, xmm2, true, Assembler::AVX_512bit);\n-  __ evmovdquq(Address(state, 120), k5, xmm3, true, Assembler::AVX_512bit);\n-  __ evmovdquq(Address(state, 160), k5, xmm4, true, Assembler::AVX_512bit);\n+  for (int i = 0; i < 5; i++) {\n+    __ evmovdquq(Address(state, i * 40), k5, xmm(i), true, Assembler::AVX_512bit);\n+  }\n@@ -376,11 +362,6 @@\n-  __ evmovdquq(xmm0, k5, Address(state0, 0), false, Assembler::AVX_512bit);\n-  __ evmovdquq(xmm1, k5, Address(state0, 40), false, Assembler::AVX_512bit);\n-  __ evmovdquq(xmm2, k5, Address(state0, 80), false, Assembler::AVX_512bit);\n-  __ evmovdquq(xmm3, k5, Address(state0, 120), false, Assembler::AVX_512bit);\n-  __ evmovdquq(xmm4, k5, Address(state0, 160), false, Assembler::AVX_512bit);\n-\n-  __ evmovdquq(xmm10, k5, Address(state1, 0), false, Assembler::AVX_512bit);\n-  __ evmovdquq(xmm11, k5, Address(state1, 40), false, Assembler::AVX_512bit);\n-  __ evmovdquq(xmm12, k5, Address(state1, 80), false, Assembler::AVX_512bit);\n-  __ evmovdquq(xmm13, k5, Address(state1, 120), false, Assembler::AVX_512bit);\n-  __ evmovdquq(xmm14, k5, Address(state1, 160), false, Assembler::AVX_512bit);\n+  for (int i = 0; i < 5; i++) {\n+    __ evmovdquq(xmm(i), k5, Address(state0, i * 40), false, Assembler::AVX_512bit);\n+  }\n+  for (int i = 0; i < 5; i++) {\n+    __ evmovdquq(xmm(10 + i), k5, Address(state1, i * 40), false, Assembler::AVX_512bit);\n+  }\n@@ -389,15 +370,4 @@\n-  __ evmovdquq(xmm17, Address(permsAndRots, 0), Assembler::AVX_512bit);\n-  __ evmovdquq(xmm18, Address(permsAndRots, 64), Assembler::AVX_512bit);\n-  __ evmovdquq(xmm19, Address(permsAndRots, 128), Assembler::AVX_512bit);\n-  __ evmovdquq(xmm20, Address(permsAndRots, 192), Assembler::AVX_512bit);\n-  __ evmovdquq(xmm21, Address(permsAndRots, 256), Assembler::AVX_512bit);\n-  __ evmovdquq(xmm22, Address(permsAndRots, 320), Assembler::AVX_512bit);\n-  __ evmovdquq(xmm23, Address(permsAndRots, 384), Assembler::AVX_512bit);\n-  __ evmovdquq(xmm24, Address(permsAndRots, 448), Assembler::AVX_512bit);\n-  __ evmovdquq(xmm25, Address(permsAndRots, 512), Assembler::AVX_512bit);\n-  __ evmovdquq(xmm26, Address(permsAndRots, 576), Assembler::AVX_512bit);\n-  __ evmovdquq(xmm27, Address(permsAndRots, 640), Assembler::AVX_512bit);\n-  __ evmovdquq(xmm28, Address(permsAndRots, 704), Assembler::AVX_512bit);\n-  __ evmovdquq(xmm29, Address(permsAndRots, 768), Assembler::AVX_512bit);\n-  __ evmovdquq(xmm30, Address(permsAndRots, 832), Assembler::AVX_512bit);\n-  __ evmovdquq(xmm31, Address(permsAndRots, 896), Assembler::AVX_512bit);\n+\n+  for (int i = 0; i < 15; i++) {\n+    __ evmovdquq(xmm(17 + i), Address(permsAndRots, i * 64), Assembler::AVX_512bit);\n+  }\n@@ -522,11 +492,6 @@\n-  __ evmovdquq(Address(state0, 0), k5, xmm0, true, Assembler::AVX_512bit);\n-  __ evmovdquq(Address(state0, 40), k5, xmm1, true, Assembler::AVX_512bit);\n-  __ evmovdquq(Address(state0, 80), k5, xmm2, true, Assembler::AVX_512bit);\n-  __ evmovdquq(Address(state0, 120), k5, xmm3, true, Assembler::AVX_512bit);\n-  __ evmovdquq(Address(state0, 160), k5, xmm4, true, Assembler::AVX_512bit);\n-\n-  __ evmovdquq(Address(state1, 0), k5, xmm10, true, Assembler::AVX_512bit);\n-  __ evmovdquq(Address(state1, 40), k5, xmm11, true, Assembler::AVX_512bit);\n-  __ evmovdquq(Address(state1, 80), k5, xmm12, true, Assembler::AVX_512bit);\n-  __ evmovdquq(Address(state1, 120), k5, xmm13, true, Assembler::AVX_512bit);\n-  __ evmovdquq(Address(state1, 160), k5, xmm14, true, Assembler::AVX_512bit);\n+  for (int i = 0; i < 5; i++) {\n+    __ evmovdquq(Address(state0, i * 40), k5, xmm(i), true, Assembler::AVX_512bit);\n+  }\n+  for (int i = 0; i < 5; i++) {\n+    __ evmovdquq(Address(state1, i * 40), k5, xmm(10 + i), true, Assembler::AVX_512bit);\n+  }\n","filename":"src\/hotspot\/cpu\/x86\/stubGenerator_x86_64_sha3.cpp","additions":27,"deletions":62,"binary":false,"changes":89,"status":"modified"}]}