{"files":[{"patch":"@@ -88,1 +88,1 @@\n-  do_arch_blob(compiler, 20000 LP64_ONLY(+64000) WINDOWS_ONLY(+2000))   \\\n+  do_arch_blob(compiler, 20000 LP64_ONLY(+89000) WINDOWS_ONLY(+2000))   \\\n","filename":"src\/hotspot\/cpu\/x86\/stubDeclarations_x86.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -4210,0 +4210,2 @@\n+  generate_dilithium_stubs();\n+\n","filename":"src\/hotspot\/cpu\/x86\/stubGenerator_x86_64.cpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -492,1 +492,0 @@\n-  address generate_sha3_implCompress(StubGenStubId stub_id);\n@@ -494,0 +493,2 @@\n+  \/\/ Dilithium stubs and helper functions\n+  void generate_dilithium_stubs();\n","filename":"src\/hotspot\/cpu\/x86\/stubGenerator_x86_64.hpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -0,0 +1,1058 @@\n+\/*\n+ * Copyright (c) 2025, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#include \"asm\/assembler.hpp\"\n+#include \"asm\/assembler.inline.hpp\"\n+#include \"runtime\/stubRoutines.hpp\"\n+#include \"macroAssembler_x86.hpp\"\n+#include \"stubGenerator_x86_64.hpp\"\n+\n+#define __ _masm->\n+\n+#define xmm(i) as_XMMRegister(i)\n+\n+#ifdef PRODUCT\n+#define BLOCK_COMMENT(str) \/* nothing *\/\n+#else\n+#define BLOCK_COMMENT(str) __ block_comment(str)\n+#endif \/\/ PRODUCT\n+\n+#define BIND(label) bind(label); BLOCK_COMMENT(#label \":\")\n+\n+\/\/ Constants\n+\/\/\n+ATTRIBUTE_ALIGNED(64) static const uint32_t dilithiumAvx512Consts[] = {\n+    58728449, \/\/ montQInvModR\n+    8380417, \/\/ dilithium_q\n+    16382, \/\/ toMont((dilithium_n)^-1 (mod dilithium_q))\n+    2365951, \/\/ montRSquareModQ\n+    5373807 \/\/ addend for modular reduce\n+};\n+\n+static address dilithiumAvx512ConstsAddr() {\n+  return (address) dilithiumAvx512Consts;\n+}\n+\n+const Register scratch = r10;\n+const XMMRegister montMulPerm = xmm28;\n+const XMMRegister montRSquareModQ = xmm29;\n+const XMMRegister montQInvModR = xmm30;\n+const XMMRegister dilithium_q = xmm31;\n+\n+\n+ATTRIBUTE_ALIGNED(64) static const uint32_t dilithiumAvx512Perms[] = {\n+     \/\/ collect montmul results into the destination register\n+    17, 1, 19, 3, 21, 5, 23, 7, 25, 9, 27, 11, 29, 13, 31, 15,\n+    \/\/ ntt\n+    0, 1, 2, 3, 4, 5, 6, 7, 16, 17, 18, 19, 20, 21, 22, 23,\n+    8, 9, 10, 11, 12, 13, 14, 15, 24, 25, 26, 27, 28, 29, 30, 31,\n+    0, 1, 2, 3, 16, 17, 18, 19, 8, 9, 10, 11, 24, 25, 26, 27,\n+    4, 5, 6, 7, 20, 21, 22, 23, 12, 13, 14, 15, 28, 29, 30, 31,\n+    0, 1, 16, 17, 4, 5, 20, 21, 8, 9, 24, 25, 12, 13, 28, 29,\n+    2, 3, 18, 19, 6, 7, 22, 23, 10, 11, 26, 27, 14, 15, 30, 31,\n+    0, 16, 2, 18, 4, 20, 6, 22, 8, 24, 10, 26, 12, 28, 14, 30,\n+    1, 17, 3, 19, 5, 21, 7, 23, 9, 25, 11, 27, 13, 29, 15, 31,\n+    0, 16, 1, 17, 2, 18, 3, 19, 4, 20, 5, 21, 6, 22, 7, 23,\n+    8, 24, 9, 25, 10, 26, 11, 27, 12, 28, 13, 29, 14, 30, 15, 31,\n+    \/\/ ntt inverse\n+    0, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30,\n+    1, 3, 5, 7, 9, 11, 13, 15, 17, 19, 21, 23, 25, 27, 29, 31,\n+    0, 16, 2, 18, 4, 20, 6, 22, 8, 24, 10, 26, 12, 28, 14, 30,\n+    1, 17, 3, 19, 5, 21, 7, 23, 9, 25, 11, 27, 13, 29, 15, 31,\n+    0, 1, 16, 17, 4, 5, 20, 21, 8, 9, 24, 25, 12, 13, 28, 29,\n+    2, 3, 18, 19, 6, 7, 22, 23, 10, 11, 26, 27, 14, 15, 30, 31,\n+    0, 1, 2, 3, 16, 17, 18, 19, 8, 9, 10, 11, 24, 25, 26, 27,\n+    4, 5, 6, 7, 20, 21, 22, 23, 12, 13, 14, 15, 28, 29, 30, 31,\n+    0, 1, 2, 3, 4, 5, 6, 7, 16, 17, 18, 19, 20, 21, 22, 23,\n+    8, 9, 10, 11, 12, 13, 14, 15, 24, 25, 26, 27, 28, 29, 30, 31\n+};\n+\n+static address dilithiumAvx512PermsAddr() {\n+  return (address) dilithiumAvx512Perms;\n+}\n+\n+\/\/ We do Montgomery multiplications of two vectors of 16 ints each in 4 steps:\n+\/\/ 1. Do the multiplications of the corresponding even numbered slots into\n+\/\/    the odd numbered slots of a third register using montmulEven().\n+\/\/ 2. Swap the even and odd numbered slots of the original input registers.\n+\/\/ 3. Similar to step 1, but into a different output register.\n+\/\/ 4. Combine the outputs of step 1 and step 3 into the output of the Montgomery\n+\/\/    multiplication.\n+\/\/ (For levels 0-6 in the Ntt and levels 1-7 of the inverse Ntt we only swap the\n+\/\/ odd-even slots of the first multiplicand as in the second (zetas) the\n+\/\/ odd slots contain the same number as the corresponding even one.)\n+\n+\/\/ Montgomery multiplication of the *even* numbered slices of sequences\n+\/\/ of parCnt consecutive registers. Zmm_inputReg1 and Zmm_inputReg2 are the\n+\/\/ starts of the two sequences. When inputReg2 == 29, we use that register as\n+\/\/ the second multiplicand in each multiplication.\n+\/\/ The result goes to the *odd* numbered slices of Zmm_outputReg.\n+\/\/ The parCnt long cosecutive sequences of registers that start with\n+\/\/ Zmm_scratch1 and Zmm_scrath2 are used as scratch registers, so their\n+\/\/ contents will be clobbered.\n+\/\/ The output register sequence  can overlap any of the input and scratch\n+\/\/ register sequences, however the two scratch register sequences should be\n+\/\/ non-overlapping.\n+\/\/ Zmm_31 should contain q and\n+\/\/ Zmm_30 should contain q^-1 mod 2^32 in all of their slices.\n+static void montmulEven(int outputReg, int inputReg1,  int inputReg2,\n+                        int scratchReg1, int scratchReg2,\n+                        int parCnt, MacroAssembler *_masm) {\n+  for (int i = 0; i < parCnt; i++) {\n+    __ vpmuldq(xmm(i + scratchReg1), xmm(i + inputReg1),\n+               xmm((inputReg2 == 29) ? 29 : inputReg2 + i), Assembler::AVX_512bit);\n+  }\n+  for (int i = 0; i < parCnt; i++) {\n+    __ vpmulld(xmm(i + scratchReg2), xmm(i + scratchReg1), montQInvModR,\n+               Assembler::AVX_512bit);\n+  }\n+  for (int i = 0; i < parCnt; i++) {\n+    __ vpmuldq(xmm(i + scratchReg2), xmm(i + scratchReg2), dilithium_q,\n+               Assembler::AVX_512bit);\n+  }\n+  for (int i = 0; i < parCnt; i++) {\n+    __ evpsubd(xmm(i + outputReg), k0, xmm(i + scratchReg1),\n+               xmm(i + scratchReg2), false, Assembler::AVX_512bit);\n+  }\n+}\n+\n+\/\/ Full Montgomery multiplication of the corresponding slices of two register\n+\/\/ sets of 4 registers each. The indexes of the registers to be multiplied\n+\/\/ are in inputRegs1[] and inputRegs[2].\n+\/\/ The results go to the registers whose indexes are in outputRegs.\n+\/\/ scratchRegs should contain 12 different register indexes.\n+\/\/ The set in outputRegs should not overlap with the set of the middle four\n+\/\/ scratch registers.\n+\/\/ The sets in inputRegs1 and inputRegs2 cannot overlap with the set of the\n+\/\/ first eight scratch registers.\n+\/\/ In most of the cases, the odd and the corresponding even slices of the\n+\/\/ registers indexed by the numbers in inputRegs2 will contain the same number,\n+\/\/ this should be indicated by calling this function with\n+\/\/ input2NeedsShuffle=false .\n+\/\/\n+static void montMul64(int outputRegs[], int inputRegs1[], int inputRegs2[],\n+                      int scratchRegs[], bool input2NeedsShuffle,\n+                      MacroAssembler *_masm) {\n+\n+  for (int i = 0; i < 4; i++) {\n+    __ vpmuldq(xmm(scratchRegs[i]), xmm(inputRegs1[i]), xmm(inputRegs2[i]),\n+               Assembler::AVX_512bit);\n+  }\n+  for (int i = 0; i < 4; i++) {\n+    __ vpmulld(xmm(scratchRegs[i + 4]), xmm(scratchRegs[i]), montQInvModR,\n+               Assembler::AVX_512bit);\n+  }\n+  for (int i = 0; i < 4; i++) {\n+    __ vpmuldq(xmm(scratchRegs[i + 4]), xmm(scratchRegs[i + 4]), dilithium_q,\n+               Assembler::AVX_512bit);\n+  }\n+  for (int i = 0; i < 4; i++) {\n+    __ evpsubd(xmm(scratchRegs[i + 4]), k0, xmm(scratchRegs[i]),\n+               xmm(scratchRegs[i + 4]), false, Assembler::AVX_512bit);\n+  }\n+\n+  for (int i = 0; i < 4; i++) {\n+    __ vpshufd(xmm(inputRegs1[i]), xmm(inputRegs1[i]), 0xB1,\n+               Assembler::AVX_512bit);\n+    if (input2NeedsShuffle) {\n+       __ vpshufd(xmm(inputRegs2[i]), xmm(inputRegs2[i]), 0xB1,\n+                  Assembler::AVX_512bit);\n+    }\n+  }\n+\n+  for (int i = 0; i < 4; i++) {\n+    __ vpmuldq(xmm(scratchRegs[i]), xmm(inputRegs1[i]), xmm(inputRegs2[i]),\n+               Assembler::AVX_512bit);\n+  }\n+  for (int i = 0; i < 4; i++) {\n+    __ vpmulld(xmm(scratchRegs[i + 8]), xmm(scratchRegs[i]), montQInvModR,\n+               Assembler::AVX_512bit);\n+  }\n+  for (int i = 0; i < 4; i++) {\n+    __ vpmuldq(xmm(scratchRegs[i + 8]), xmm(scratchRegs[i + 8]), dilithium_q,\n+               Assembler::AVX_512bit);\n+  }\n+  for (int i = 0; i < 4; i++) {\n+    __ evpsubd(xmm(outputRegs[i]), k0, xmm(scratchRegs[i]),\n+               xmm(scratchRegs[i + 8]), false, Assembler::AVX_512bit);\n+  }\n+\n+  for (int i = 0; i < 4; i++) {\n+    __ evpermt2d(xmm(outputRegs[i]), montMulPerm, xmm(scratchRegs[i + 4]),\n+                 Assembler::AVX_512bit);\n+  }\n+}\n+\n+static void montMul64(int outputRegs[], int inputRegs1[], int inputRegs2[],\n+                       int scratchRegs[], MacroAssembler *_masm) {\n+   montMul64(outputRegs, inputRegs1, inputRegs2, scratchRegs, false, _masm);\n+}\n+\n+\/\/ input in Zmm0-Zmm7, the constant is repeated in all slots of Zmm29\n+\/\/ qinvmodR and q are repeated in all slots of Zmm30 and Zmm31, resp.\n+\/\/ Zmm8-Zmm23 used as scratch registers\n+\/\/ result goes to Zmm0-Zmm7\n+static void montMulByConst128(MacroAssembler *_masm) {\n+  montmulEven(8, 0, 29, 8, 16, 8, _masm);\n+\n+  for (int i = 0; i < 8; i++) {\n+    __ vpshufd(xmm(i),xmm(i), 0xB1, Assembler::AVX_512bit);\n+  }\n+\n+  montmulEven(0, 0, 29, 0, 16, 8, _masm);\n+\n+  for (int i = 0; i < 8; i++) {\n+    __ evpermt2d(xmm(i), montMulPerm, xmm(i + 8), Assembler::AVX_512bit);\n+  }\n+}\n+\n+static void sub_add(int subResult[], int addResult[],\n+                    int input1[], int input2[], MacroAssembler *_masm) {\n+\n+  for (int i = 0; i < 4; i++) {\n+    __ evpsubd(xmm(subResult[i]), k0, xmm(input1[i]), xmm(input2[i]), false,\n+               Assembler::AVX_512bit);\n+  }\n+\n+  for (int i = 0; i < 4; i++) {\n+    __ evpaddd(xmm(addResult[i]), k0, xmm(input1[i]), xmm(input2[i]), false,\n+               Assembler::AVX_512bit);\n+  }\n+}\n+\n+static void loadZetas(int destinationRegs[], Register zetas,\n+                      int offset, int incr, MacroAssembler *_masm) {\n+  for (int i = 0; i < 4; i++) {\n+    __ evmovdqul(xmm(destinationRegs[i]), Address(zetas, offset + i * incr),\n+                 Assembler::AVX_512bit);\n+  }\n+}\n+\n+\n+static void loadPerm(int destinationRegs[], Register perms,\n+                      int offset, MacroAssembler *_masm) {\n+  __ evmovdqul(xmm(destinationRegs[0]), Address(perms, offset),\n+                 Assembler::AVX_512bit);\n+  for (int i = 1; i < 4; i++) {\n+      __ evmovdqul(xmm(destinationRegs[i]), xmm(destinationRegs[0]),\n+                   Assembler::AVX_512bit);\n+    }\n+}\n+\n+static int xmm0_3[] = {0, 1, 2, 3};\n+static int xmm0145[] = {0, 1, 4, 5};\n+static int xmm0246[] = {0, 2, 4, 6};\n+static int xmm0426[] = {0, 4, 2, 6};\n+static int xmm1357[] = {1, 3, 5, 7};\n+static int xmm1537[] = {1, 5, 3, 7};\n+static int xmm2367[] = {2, 3, 6, 7};\n+static int xmm4_7[] = {4, 5, 6, 7};\n+static int xmm8_11[] = {8, 9, 10, 11};\n+static int xmm12_15[] = {12, 13, 14, 15};\n+static int xmm16_19[] = {16, 17, 18, 19};\n+static int xmm20222426[] = {20, 22, 24, 26};\n+static int xmm21232527[] = {21, 23, 25, 27};\n+static int xmm24_27[] = {24, 25, 26, 27};\n+static int xmm4_20_24[] = {4, 5, 6, 7, 20, 21, 22, 23, 24, 25, 26, 27};\n+static int xmm16_27[] = {16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27};\n+static int xmm29_29[] = {29, 29, 29, 29};\n+\n+\/\/ Dilithium NTT function except for the final \"normalization\" to |coeff| < Q.\n+\/\/ Implements\n+\/\/ static int implDilithiumAlmostNtt(int[] coeffs, int zetas[]) {}\n+\/\/\n+\/\/ coeffs (int[256]) = c_rarg0\n+\/\/ zetas (int[256]) = c_rarg1\n+\/\/\n+\/\/\n+static address generate_dilithiumAlmostNtt_avx512(StubGenerator *stubgen,\n+                                                  MacroAssembler *_masm) {\n+\n+  __ align(CodeEntryAlignment);\n+  StubGenStubId stub_id = dilithiumAlmostNtt_id;\n+  StubCodeMark mark(stubgen, stub_id);\n+  address start = __ pc();\n+  __ enter();\n+\n+  Label L_loop, L_end;\n+\n+  const Register coeffs = c_rarg0;\n+  const Register zetas = c_rarg1;\n+  const Register iterations = c_rarg2;\n+\n+  const Register perms = r11;\n+\n+  __ lea(perms, ExternalAddress(dilithiumAvx512PermsAddr()));\n+\n+  __ evmovdqul(montMulPerm, Address(perms, 0), Assembler::AVX_512bit);\n+  __ evmovdqul(xmm29, Address(zetas, 0), Assembler::AVX_512bit);\n+  __ vpbroadcastd(montQInvModR, ExternalAddress(dilithiumAvx512ConstsAddr()),\n+                  Assembler::AVX_512bit, scratch); \/\/ q^-1 mod 2^32\n+  __ vpbroadcastd(dilithium_q, ExternalAddress(dilithiumAvx512ConstsAddr() + 4),\n+                  Assembler::AVX_512bit, scratch); \/\/ q\n+\n+\n+  \/\/ load all coefficients into the vector registers Zmm_0-Zmm_15,\n+  \/\/ 16 coefficients into each\n+  for (int i = 0; i < 16; i++) {\n+    __ evmovdqul(xmm(i), Address(coeffs, i * 64), Assembler::AVX_512bit);\n+  }\n+\n+  \/\/ level 0 and 1 can be done entirely in registers as the zetas on these\n+  \/\/ levels are the same for all the montmuls that we can do in parallel\n+\n+  \/\/ level 0\n+\n+  montMul64(xmm16_19, xmm8_11, xmm29_29, xmm16_27, _masm);\n+  sub_add(xmm8_11, xmm0_3, xmm0_3, xmm16_19, _masm);\n+  montMul64(xmm16_19, xmm12_15, xmm29_29, xmm16_27, _masm);\n+  __ evmovdqul(xmm29, Address(zetas, 512), Assembler::AVX_512bit); \/\/ for level 1\n+  sub_add(xmm12_15, xmm4_7, xmm4_7, xmm16_19, _masm);\n+\n+  \/\/ level 1\n+\n+  montMul64(xmm16_19, xmm4_7, xmm29_29, xmm16_27, _masm);\n+  __ evmovdqul(xmm29, Address(zetas, 768), Assembler::AVX_512bit);\n+  sub_add(xmm4_7, xmm0_3, xmm0_3, xmm16_19, _masm);\n+  montMul64(xmm16_19, xmm12_15, xmm29_29, xmm16_27, _masm);\n+  sub_add(xmm12_15, xmm8_11, xmm8_11, xmm16_19, _masm);\n+\n+  \/\/ levels 2 to 7 are done in 2 batches, by first saving half of the coefficients\n+  \/\/ from level 1 into memory, doing all the level 2 to level 7 computations\n+  \/\/ on the remaining half in the vector registers, saving the result to\n+  \/\/ memory after level 7, then loading back the coefficients that we saved after\n+  \/\/ level 1 and do the same computation with those\n+\n+  for (int i = 0; i < 16; i++) {\n+    __ evmovdqul(Address(coeffs, i * 64), xmm(i), Assembler::AVX_512bit);\n+  }\n+\n+  __ movl(iterations, 2);\n+\n+  __ align(OptoLoopAlignment);\n+  __ BIND(L_loop);\n+\n+  __ subl(iterations, 1);\n+\n+  \/\/ level 2\n+  loadZetas(xmm12_15, zetas, 2 * 512, 64, _masm);\n+  montMul64(xmm16_19, xmm2367, xmm12_15, xmm16_27, _masm);\n+  loadZetas(xmm12_15, zetas, 3 * 512, 64, _masm); \/\/ for level 3\n+  sub_add(xmm2367, xmm0145, xmm0145, xmm16_19, _masm);\n+\n+  \/\/ level 3\n+\n+  montMul64(xmm16_19, xmm1357, xmm12_15, xmm16_27, _masm);\n+  sub_add(xmm1357, xmm0246, xmm0246, xmm16_19, _masm);\n+\n+  \/\/ level 4\n+  loadPerm(xmm16_19, perms, 64, _masm);\n+  loadPerm(xmm12_15, perms, 128, _masm);\n+  loadZetas(xmm24_27, zetas, 4 * 512, 64, _masm); \/\/ for level 3\n+\n+  for (int i = 0; i < 8; i += 2) {\n+    __ evpermi2d(xmm(i\/2 + 16), xmm(i), xmm(i + 1), Assembler::AVX_512bit);\n+  }\n+  for (int i = 0; i < 8; i += 2) {\n+    __ evpermi2d(xmm(i \/ 2 + 12), xmm(i), xmm(i + 1), Assembler::AVX_512bit);\n+  }\n+\n+  montMul64(xmm12_15, xmm12_15, xmm24_27, xmm4_20_24, _masm);\n+  sub_add(xmm1357, xmm0246, xmm16_19, xmm12_15, _masm);\n+\n+  \/\/ level 5\n+  loadPerm(xmm16_19, perms, 192, _masm);\n+  loadPerm(xmm12_15, perms, 256, _masm);\n+  loadZetas(xmm24_27, zetas, 5 * 512, 64, _masm);\n+\n+  for (int i = 0; i < 8; i += 2) {\n+    __ evpermi2d(xmm(i\/2 + 16), xmm(i), xmm(i + 1), Assembler::AVX_512bit);\n+  }\n+  for (int i = 0; i < 8; i += 2) {\n+    __ evpermi2d(xmm(i \/ 2 + 12), xmm(i), xmm(i + 1), Assembler::AVX_512bit);\n+  }\n+\n+  montMul64(xmm12_15, xmm12_15, xmm24_27, xmm4_20_24, _masm);\n+  sub_add(xmm1357, xmm0246, xmm16_19, xmm12_15, _masm);\n+\n+  \/\/ level 6\n+  loadPerm(xmm16_19, perms, 320, _masm);\n+  loadPerm(xmm12_15, perms, 384, _masm);\n+  loadZetas(xmm24_27, zetas, 6 * 512, 64, _masm);\n+\n+  for (int i = 0; i < 8; i += 2) {\n+    __ evpermi2d(xmm(i\/2 + 16), xmm(i), xmm(i + 1), Assembler::AVX_512bit);\n+  }\n+  for (int i = 0; i < 8; i += 2) {\n+    __ evpermi2d(xmm(i \/ 2 + 12), xmm(i), xmm(i + 1), Assembler::AVX_512bit);\n+  }\n+\n+  montMul64(xmm12_15, xmm12_15, xmm24_27, xmm4_20_24, _masm);\n+  sub_add(xmm1357, xmm0246, xmm16_19, xmm12_15, _masm);\n+\n+  \/\/ level 7\n+  loadPerm(xmm16_19, perms, 448, _masm);\n+  loadPerm(xmm12_15, perms, 512, _masm);\n+  loadZetas(xmm24_27, zetas, 7 * 512, 64, _masm);\n+\n+  for (int i = 0; i < 8; i += 2) {\n+    __ evpermi2d(xmm(i \/ 2 + 16), xmm(i), xmm(i + 1), Assembler::AVX_512bit);\n+  }\n+  for (int i = 0; i < 8; i += 2) {\n+    __ evpermi2d(xmm(i \/ 2 + 12), xmm(i), xmm(i + 1), Assembler::AVX_512bit);\n+  }\n+\n+  montMul64(xmm12_15, xmm12_15, xmm24_27, xmm4_20_24, true, _masm);\n+  loadPerm(xmm0246, perms, 576, _masm);\n+  loadPerm(xmm1357, perms, 640, _masm);\n+  sub_add(xmm21232527, xmm20222426, xmm16_19, xmm12_15, _masm);\n+\n+  for (int i = 0; i < 8; i += 2) {\n+    __ evpermi2d(xmm(i), xmm(i + 20), xmm(i + 21), Assembler::AVX_512bit);\n+    __ evpermi2d(xmm(i + 1), xmm(i + 20), xmm(i + 21), Assembler::AVX_512bit);\n+  }\n+\n+  __ cmpl(iterations, 0);\n+  __ jcc(Assembler::equal, L_end);\n+\n+  for (int i = 0; i < 8; i++) {\n+    __ evmovdqul(Address(coeffs, i * 64), xmm(i), Assembler::AVX_512bit);\n+  }\n+\n+  for (int i = 8; i < 16; i++) {\n+    __ evmovdqul(xmm(i - 8), Address(coeffs, i * 64), Assembler::AVX_512bit);\n+  }\n+\n+  __ addptr(zetas, 256);\n+\n+  __ jmp(L_loop);\n+\n+  __ BIND(L_end);\n+\n+  for (int i = 0; i < 8; i++) {\n+    __ evmovdqul(Address(coeffs, (i + 8) * 64), xmm(i), Assembler::AVX_512bit);\n+  }\n+\n+  __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n+  __ mov64(rax, 0); \/\/ return 0\n+  __ ret(0);\n+\n+  return start;\n+}\n+\n+\/\/ Dilithium Inverse NTT function except the final mod Q division by 2^256.\n+\/\/ Implements\n+\/\/ static int implDilithiumAlmostInverseNtt(int[] coeffs, int[] zetas) {}\n+\/\/\n+\/\/ coeffs (int[256]) = c_rarg0\n+\/\/ zetas (int[256]) = c_rarg1\n+static address generate_dilithiumAlmostInverseNtt_avx512(StubGenerator *stubgen,\n+                                                         MacroAssembler *_masm) {\n+\n+  __ align(CodeEntryAlignment);\n+  StubGenStubId stub_id = dilithiumAlmostInverseNtt_id;\n+  StubCodeMark mark(stubgen, stub_id);\n+  address start = __ pc();\n+  __ enter();\n+\n+  Label L_loop, L_end;\n+\n+  const Register coeffs = c_rarg0;\n+  const Register zetas = c_rarg1;\n+\n+  const Register iterations = c_rarg2;\n+\n+  const Register perms = r11;\n+\n+  __ lea(perms, ExternalAddress(dilithiumAvx512PermsAddr()));\n+\n+  __ evmovdqul(montMulPerm, Address(perms, 0), Assembler::AVX_512bit);\n+__ vpbroadcastd(montQInvModR, ExternalAddress(dilithiumAvx512ConstsAddr()),\n+                Assembler::AVX_512bit, scratch); \/\/ q^-1 mod 2^32\n+__ vpbroadcastd(dilithium_q, ExternalAddress(dilithiumAvx512ConstsAddr() + 4),\n+                Assembler::AVX_512bit, scratch); \/\/ q\n+\n+  \/\/ We do levels 0-6 in two batches, each batch entirely in the vector registers\n+  for (int i = 0; i < 8; i++) {\n+    __ evmovdqul(xmm(i), Address(coeffs, i * 64), Assembler::AVX_512bit);\n+  }\n+\n+  __ movl(iterations, 2);\n+\n+  __ align(OptoLoopAlignment);\n+  __ BIND(L_loop);\n+\n+  __ subl(iterations, 1);\n+\n+  \/\/ level 0\n+  loadPerm(xmm8_11, perms, 704, _masm);\n+  loadPerm(xmm12_15, perms, 768, _masm);\n+\n+  for (int i = 0; i < 8; i += 2) {\n+    __ evpermi2d(xmm(i \/ 2 + 8), xmm(i), xmm(i + 1), Assembler::AVX_512bit);\n+    __ evpermi2d(xmm(i \/ 2 + 12), xmm(i), xmm(i + 1), Assembler::AVX_512bit);\n+  }\n+\n+  loadZetas(xmm4_7, zetas, 0, 64, _masm);\n+  sub_add(xmm24_27, xmm0_3, xmm8_11, xmm12_15, _masm);\n+  montMul64(xmm4_7, xmm4_7, xmm24_27, xmm16_27, true, _masm);\n+\n+  \/\/ level 1\n+  loadPerm(xmm8_11, perms, 832, _masm);\n+  loadPerm(xmm12_15, perms, 896, _masm);\n+\n+  for (int i = 0; i < 4; i++) {\n+    __ evpermi2d(xmm(i + 8), xmm(i), xmm(i + 4), Assembler::AVX_512bit);\n+    __ evpermi2d(xmm(i + 12), xmm(i), xmm(i + 4), Assembler::AVX_512bit);\n+  }\n+\n+  loadZetas(xmm4_7, zetas, 512, 64, _masm);\n+  sub_add(xmm24_27, xmm0_3, xmm8_11, xmm12_15, _masm);\n+  montMul64(xmm4_7, xmm24_27, xmm4_7, xmm16_27, _masm);\n+\n+  \/\/ level 2\n+  loadPerm(xmm8_11, perms, 960, _masm);\n+  loadPerm(xmm12_15, perms, 1024, _masm);\n+\n+  for (int i = 0; i < 4; i++) {\n+    __ evpermi2d(xmm(i + 8), xmm(i), xmm(i + 4), Assembler::AVX_512bit);\n+    __ evpermi2d(xmm(i + 12), xmm(i), xmm(i + 4), Assembler::AVX_512bit);\n+  }\n+\n+  loadZetas(xmm4_7, zetas, 2 * 512, 64, _masm);\n+  sub_add(xmm24_27, xmm0_3, xmm8_11, xmm12_15, _masm);\n+  montMul64(xmm4_7, xmm24_27, xmm4_7, xmm16_27, _masm);\n+\n+  \/\/ level 3\n+  loadPerm(xmm8_11, perms, 1088, _masm);\n+  loadPerm(xmm12_15, perms, 1152, _masm);\n+\n+  for (int i = 0; i < 4; i++) {\n+    __ evpermi2d(xmm(i + 8), xmm(i), xmm(i + 4), Assembler::AVX_512bit);\n+    __ evpermi2d(xmm(i + 12), xmm(i), xmm(i + 4), Assembler::AVX_512bit);\n+  }\n+\n+  loadZetas(xmm4_7, zetas, 3 * 512, 64, _masm);\n+  sub_add(xmm24_27, xmm0_3, xmm8_11, xmm12_15, _masm);\n+  montMul64(xmm4_7, xmm24_27, xmm4_7, xmm16_27, _masm);\n+\n+  \/\/ level 4\n+  loadPerm(xmm8_11, perms, 1216, _masm);\n+  loadPerm(xmm12_15, perms, 1280, _masm);\n+\n+  for (int i = 0; i < 4; i++) {\n+    __ evpermi2d(xmm(i + 8), xmm(i), xmm(i + 4), Assembler::AVX_512bit);\n+    __ evpermi2d(xmm(i + 12), xmm(i), xmm(i + 4), Assembler::AVX_512bit);\n+  }\n+\n+  loadZetas(xmm4_7, zetas, 4 * 512, 64, _masm);\n+  sub_add(xmm24_27, xmm0_3, xmm8_11, xmm12_15, _masm);\n+  montMul64(xmm4_7, xmm24_27, xmm4_7, xmm16_27, _masm);\n+\n+  \/\/ level 5\n+  loadZetas(xmm12_15, zetas, 5 * 512, 64, _masm);\n+  sub_add(xmm8_11, xmm0_3, xmm0426, xmm1537, _masm);\n+  montMul64(xmm4_7, xmm8_11, xmm12_15, xmm16_27, _masm);\n+\n+  \/\/ level 6\n+  loadZetas(xmm12_15, zetas, 6 * 512, 64, _masm);\n+  sub_add(xmm8_11, xmm0_3, xmm0145, xmm2367, _masm);\n+  montMul64(xmm4_7, xmm8_11, xmm12_15, xmm16_27, _masm);\n+\n+  __ cmpl(iterations, 0);\n+  __ jcc(Assembler::equal, L_end);\n+\n+  \/\/ save the coefficients of the first batch, adjust the zetas\n+  \/\/ and load the second batch of coefficients\n+  for (int i = 0; i < 8; i++) {\n+    __ evmovdqul(Address(coeffs, i * 64), xmm(i), Assembler::AVX_512bit);\n+  }\n+\n+  __ addptr(zetas, 256);\n+\n+  for (int i = 0; i < 8; i++) {\n+    __ evmovdqul(xmm(i), Address(coeffs, i * 64 + 512), Assembler::AVX_512bit);\n+  }\n+\n+  __ jmp(L_loop);\n+\n+  __ BIND(L_end);\n+\n+  \/\/ load the coeffs of the first batch of coefficients that were saved after\n+  \/\/ level 6 into Zmm_8-Zmm_15 and do the last level entirely in the vector\n+  \/\/ registers\n+  for (int i = 0; i < 8; i++) {\n+    __ evmovdqul(xmm(i + 8), Address(coeffs, i * 64), Assembler::AVX_512bit);\n+  }\n+\n+  \/\/ level 7\n+\n+  __ evmovdqul(xmm29, Address(zetas, 7 * 512), Assembler::AVX_512bit);\n+\n+  for (int i = 0; i < 8; i++) {\n+    __ evpaddd(xmm(i + 16), k0, xmm(i), xmm(i + 8), false, Assembler::AVX_512bit);\n+  }\n+\n+  for (int i = 0; i < 8; i++) {\n+    __ evpsubd(xmm(i), k0, xmm(i + 8), xmm(i), false, Assembler::AVX_512bit);\n+  }\n+\n+  for (int i = 0; i < 8; i++) {\n+    __ evmovdqul(Address(coeffs, i * 64), xmm(i + 16), Assembler::AVX_512bit);\n+  }\n+  montMulByConst128(_masm);\n+\n+  for (int i = 0; i < 8; i++) {\n+    __ evmovdqul(Address(coeffs, i * 64 + 512), xmm(i), Assembler::AVX_512bit);\n+  }\n+\n+  __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n+  __ mov64(rax, 0); \/\/ return 0\n+  __ ret(0);\n+\n+  return start;\n+}\n+\n+\/\/ Dilithium multiply polynomials in the NTT domain.\n+\/\/ Implements\n+\/\/ static int implDilithiumNttMult(\n+\/\/              int[] result, int[] ntta, int[] nttb {}\n+\/\/\n+\/\/ result (int[256]) = c_rarg0\n+\/\/ poly1 (int[256]) = c_rarg1\n+\/\/ poly2 (int[256]) = c_rarg2\n+static address generate_dilithiumNttMult_avx512(StubGenerator *stubgen,\n+                                                MacroAssembler *_masm) {\n+\n+  __ align(CodeEntryAlignment);\n+  StubGenStubId stub_id = dilithiumNttMult_id;\n+  StubCodeMark mark(stubgen, stub_id);\n+  address start = __ pc();\n+  __ enter();\n+\n+  Label L_loop;\n+\n+  const Register result = c_rarg0;\n+  const Register poly1 = c_rarg1;\n+  const Register poly2 = c_rarg2;\n+\n+  const Register perms = r10; \/\/ scratch reused after not needed any more\n+  const Register len = r11;\n+\n+  __ vpbroadcastd(montQInvModR, ExternalAddress(dilithiumAvx512ConstsAddr()),\n+                  Assembler::AVX_512bit, scratch); \/\/ q^-1 mod 2^32\n+  __ vpbroadcastd(dilithium_q, ExternalAddress(dilithiumAvx512ConstsAddr() + 4),\n+                  Assembler::AVX_512bit, scratch); \/\/ q\n+  __ vpbroadcastd(montRSquareModQ, ExternalAddress(dilithiumAvx512ConstsAddr() + 12),\n+                  Assembler::AVX_512bit, scratch); \/\/ 2^64 mod q\n+\n+  __ lea(perms, ExternalAddress(dilithiumAvx512PermsAddr()));\n+  __ evmovdqul(montMulPerm, Address(perms, 0), Assembler::AVX_512bit);\n+\n+  __ movl(len, 4);\n+\n+  __ align(OptoLoopAlignment);\n+  __ BIND(L_loop);\n+\n+  for (int i = 0; i < 4; i++) {\n+    __ evmovdqul(xmm(i + 4), Address(poly2, i * 64), Assembler::AVX_512bit);\n+    __ evmovdqul(xmm(i), Address(poly1, i * 64), Assembler::AVX_512bit);\n+  }\n+\n+  montMul64(xmm4_7, xmm4_7, xmm29_29, xmm16_27, _masm);\n+  montMul64(xmm0_3, xmm0_3, xmm4_7, xmm16_27, true, _masm);\n+\n+  for (int i = 0; i < 4; i++) {\n+    __ evmovdqul(Address(result, i * 64), xmm(i), Assembler::AVX_512bit);\n+  }\n+  __ subl(len, 1);\n+  __ addptr(poly1, 256);\n+  __ addptr(poly2, 256);\n+  __ addptr(result, 256);\n+  __ cmpl(len, 0);\n+  __ jcc(Assembler::notEqual, L_loop);\n+\n+  __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n+  __ mov64(rax, 0); \/\/ return 0\n+  __ ret(0);\n+\n+  return start;\n+}\n+\n+\/\/ Dilithium Motgomery multiply an array by a constant.\n+\/\/ Implements\n+\/\/ static int implDilithiumMontMulByConstant(int[] coeffs, int constant) {}\n+\/\/\n+\/\/ coeffs (int[256]) = c_rarg0\n+\/\/ constant (int) = c_rarg1\n+static address generate_dilithiumMontMulByConstant_avx512(StubGenerator *stubgen,\n+                                                          MacroAssembler *_masm) {\n+\n+  __ align(CodeEntryAlignment);\n+  StubGenStubId stub_id = dilithiumMontMulByConstant_id;\n+  StubCodeMark mark(stubgen, stub_id);\n+  address start = __ pc();\n+  __ enter();\n+\n+  Label L_loop;\n+\n+  const Register coeffs = c_rarg0;\n+  const Register rConstant = c_rarg1;\n+\n+  const Register perms = c_rarg2; \/\/ not used for argument\n+  const Register len = r11;\n+\n+  const XMMRegister constant = xmm29;\n+\n+  __ lea(perms, ExternalAddress(dilithiumAvx512PermsAddr()));\n+\n+  \/\/ the following four vector registers are used in montMulByConst128\n+  __ vpbroadcastd(montQInvModR, ExternalAddress(dilithiumAvx512ConstsAddr()),\n+                  Assembler::AVX_512bit, scratch); \/\/ q^-1 mod 2^32\n+  __ vpbroadcastd(dilithium_q, ExternalAddress(dilithiumAvx512ConstsAddr() + 4),\n+                  Assembler::AVX_512bit, scratch); \/\/ q\n+  __ evmovdqul(montMulPerm, Address(perms, 0), Assembler::AVX_512bit);\n+  __ evpbroadcastd(constant, rConstant, Assembler::AVX_512bit); \/\/ constant multiplier\n+\n+  __ movl(len, 2);\n+\n+  __ align(OptoLoopAlignment);\n+  __ BIND(L_loop);\n+\n+  for (int i = 0; i < 8; i++) {\n+    __ evmovdqul(xmm(i), Address(coeffs, i * 64), Assembler::AVX_512bit);\n+  }\n+\n+  montMulByConst128(_masm);\n+\n+  for (int i = 0; i < 8; i++) {\n+    __ evmovdqul(Address(coeffs, i * 64), xmm(i), Assembler::AVX_512bit);\n+  }\n+\n+  __ subl(len, 1);\n+  __ addptr(coeffs, 512);\n+  __ cmpl(len, 0);\n+  __ jcc(Assembler::notEqual, L_loop);\n+\n+  __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n+  __ mov64(rax, 0); \/\/ return 0\n+  __ ret(0);\n+\n+  return start;\n+}\n+\n+\/\/ Dilithium decompose poly.\n+\/\/ Implements\n+\/\/ static int implDilithiumDecomposePoly(int[] coeffs, int constant) {}\n+\/\/\n+\/\/ input (int[256]) = c_rarg0\n+\/\/ lowPart (int[256]) = c_rarg1\n+\/\/ highPart (int[256]) = c_rarg2\n+\/\/ twoGamma2  (int) = c_rarg3\n+\/\/ multiplier (int) = c_rarg4\n+static address generate_dilithiumDecomposePoly_avx512(StubGenerator *stubgen,\n+                                                      MacroAssembler *_masm) {\n+\n+  __ align(CodeEntryAlignment);\n+  StubGenStubId stub_id = dilithiumDecomposePoly_id;\n+  StubCodeMark mark(stubgen, stub_id);\n+  address start = __ pc();\n+  __ enter();\n+\n+  Label L_loop;\n+\n+  const Register input = c_rarg0;\n+  const Register lowPart = c_rarg1;\n+  const Register highPart = c_rarg2;\n+  const Register rTwoGamma2 = c_rarg3;\n+\n+  const Register len = r11;\n+  const XMMRegister zero = xmm24;\n+  const XMMRegister one = xmm25;\n+  const XMMRegister qMinus1 = xmm26;\n+  const XMMRegister gamma2 = xmm27;\n+  const XMMRegister twoGamma2 = xmm28;\n+  const XMMRegister barrettMultiplier = xmm29;\n+  const XMMRegister barrettAddend = xmm30;\n+\n+  __ xorl(scratch, scratch);\n+  __ evpbroadcastd(zero, scratch, Assembler::AVX_512bit); \/\/ 0\n+  __ addl(scratch, 1);\n+  __ evpbroadcastd(one, scratch, Assembler::AVX_512bit); \/\/ 1\n+  __ vpbroadcastd(dilithium_q, ExternalAddress(dilithiumAvx512ConstsAddr() + 4),\n+                  Assembler::AVX_512bit, scratch); \/\/ q^-1 mod 2^32\n+  __ vpbroadcastd(barrettAddend, ExternalAddress(dilithiumAvx512ConstsAddr() + 16),\n+                  Assembler::AVX_512bit, scratch); \/\/ q\n+\n+  __ evpbroadcastd(twoGamma2, rTwoGamma2, Assembler::AVX_512bit); \/\/ 2 * gamma2\n+\n+  #ifndef _WIN64\n+    const Register rMultiplier = c_rarg4;\n+  #else\n+    const Address multiplier_mem(rbp, 6 * wordSize);\n+    const Register rMultiplier = c_rarg3; \/\/ arg3 is already consumed, reused here\n+    __ movptr(rMultiplier, multiplier_mem);\n+  #endif\n+  __ evpbroadcastd(barrettMultiplier, rMultiplier,\n+                   Assembler::AVX_512bit); \/\/ multiplier for mod 2 * gamma2 reduce\n+\n+  __ evpsubd(qMinus1, k0, dilithium_q, one, false, Assembler::AVX_512bit); \/\/ q - 1\n+  __ evpsrad(gamma2, k0, twoGamma2, 1, false, Assembler::AVX_512bit); \/\/ gamma2\n+\n+  __ movl(len, 1024);\n+\n+  __ align(OptoLoopAlignment);\n+  __ BIND(L_loop);\n+\n+  __ evmovdqul(xmm0, Address(input, 0), Assembler::AVX_512bit);\n+  __ evmovdqul(xmm1, Address(input, 64), Assembler::AVX_512bit);\n+  __ evmovdqul(xmm2, Address(input, 128), Assembler::AVX_512bit);\n+  __ evmovdqul(xmm3, Address(input, 192), Assembler::AVX_512bit);\n+\n+  __ addptr(input, 256);\n+\n+  \/\/ rplus in xmm0\n+  \/\/ rplus = rplus - ((rplus + 5373807) >> 23) * dilithium_q;\n+  __ evpaddd(xmm4, k0, xmm0, barrettAddend, false, Assembler::AVX_512bit);\n+  __ evpaddd(xmm5, k0, xmm1, barrettAddend, false, Assembler::AVX_512bit);\n+  __ evpaddd(xmm6, k0, xmm2, barrettAddend, false, Assembler::AVX_512bit);\n+  __ evpaddd(xmm7, k0, xmm3, barrettAddend, false, Assembler::AVX_512bit);\n+\n+  __ evpsrad(xmm4, k0, xmm4, 23, false, Assembler::AVX_512bit);\n+  __ evpsrad(xmm5, k0, xmm5, 23, false, Assembler::AVX_512bit);\n+  __ evpsrad(xmm6, k0, xmm6, 23, false, Assembler::AVX_512bit);\n+  __ evpsrad(xmm7, k0, xmm7, 23, false, Assembler::AVX_512bit);\n+\n+  __ evpmulld(xmm4, k0, xmm4, dilithium_q, false, Assembler::AVX_512bit);\n+  __ evpmulld(xmm5, k0, xmm5, dilithium_q, false, Assembler::AVX_512bit);\n+  __ evpmulld(xmm6, k0, xmm6, dilithium_q, false, Assembler::AVX_512bit);\n+  __ evpmulld(xmm7, k0, xmm7, dilithium_q, false, Assembler::AVX_512bit);\n+\n+  __ evpsubd(xmm0, k0, xmm0, xmm4, false, Assembler::AVX_512bit);\n+  __ evpsubd(xmm1, k0, xmm1, xmm5, false, Assembler::AVX_512bit);\n+  __ evpsubd(xmm2, k0, xmm2, xmm6, false, Assembler::AVX_512bit);\n+  __ evpsubd(xmm3, k0, xmm3, xmm7, false, Assembler::AVX_512bit);\n+\n+  \/\/ rplus in xmm0\n+  \/\/ rplus = rplus + ((rplus >> 31) & dilithium_q);\n+  __ evpsrad(xmm4, k0, xmm0, 31, false, Assembler::AVX_512bit);\n+  __ evpsrad(xmm5, k0, xmm1, 31, false, Assembler::AVX_512bit);\n+  __ evpsrad(xmm6, k0, xmm2, 31, false, Assembler::AVX_512bit);\n+  __ evpsrad(xmm7, k0, xmm3, 31, false, Assembler::AVX_512bit);\n+\n+  __ evpandd(xmm4, k0, xmm4, dilithium_q, false, Assembler::AVX_512bit);\n+  __ evpandd(xmm5, k0, xmm5, dilithium_q, false, Assembler::AVX_512bit);\n+  __ evpandd(xmm6, k0, xmm6, dilithium_q, false, Assembler::AVX_512bit);\n+  __ evpandd(xmm7, k0, xmm7, dilithium_q, false, Assembler::AVX_512bit);\n+\n+  __ evpaddd(xmm0, k0, xmm0, xmm4, false, Assembler::AVX_512bit);\n+  __ evpaddd(xmm1, k0, xmm1, xmm5, false, Assembler::AVX_512bit);\n+  __ evpaddd(xmm2, k0, xmm2, xmm6, false, Assembler::AVX_512bit);\n+  __ evpaddd(xmm3, k0, xmm3, xmm7, false, Assembler::AVX_512bit);\n+\n+  \/\/ rplus in xmm0\n+  \/\/ int quotient = (rplus * barrettMultiplier) >> 22;\n+  __ evpmulld(xmm4, k0, xmm0, barrettMultiplier, false, Assembler::AVX_512bit);\n+  __ evpmulld(xmm5, k0, xmm1, barrettMultiplier, false, Assembler::AVX_512bit);\n+  __ evpmulld(xmm6, k0, xmm2, barrettMultiplier, false, Assembler::AVX_512bit);\n+  __ evpmulld(xmm7, k0, xmm3, barrettMultiplier, false, Assembler::AVX_512bit);\n+\n+  __ evpsrad(xmm4, k0, xmm4, 22, false, Assembler::AVX_512bit);\n+  __ evpsrad(xmm5, k0, xmm5, 22, false, Assembler::AVX_512bit);\n+  __ evpsrad(xmm6, k0, xmm6, 22, false, Assembler::AVX_512bit);\n+  __ evpsrad(xmm7, k0, xmm7, 22, false, Assembler::AVX_512bit);\n+\n+  \/\/ quotient in xmm4\n+  \/\/ int r0 = rplus - quotient * twoGamma2;\n+  __ evpmulld(xmm8, k0, xmm4, twoGamma2, false, Assembler::AVX_512bit);\n+  __ evpmulld(xmm9, k0, xmm5, twoGamma2, false, Assembler::AVX_512bit);\n+  __ evpmulld(xmm10, k0, xmm6, twoGamma2, false, Assembler::AVX_512bit);\n+  __ evpmulld(xmm11, k0, xmm7, twoGamma2, false, Assembler::AVX_512bit);\n+\n+  __ evpsubd(xmm8, k0, xmm0, xmm8, false, Assembler::AVX_512bit);\n+  __ evpsubd(xmm9, k0, xmm1, xmm9, false, Assembler::AVX_512bit);\n+  __ evpsubd(xmm10, k0, xmm2, xmm10, false, Assembler::AVX_512bit);\n+  __ evpsubd(xmm11, k0, xmm3, xmm11, false, Assembler::AVX_512bit);\n+\n+  \/\/ r0 in xmm8\n+  \/\/ int mask = (twoGamma2 - r0) >> 22;\n+  __ evpsubd(xmm12, k0, twoGamma2, xmm8, false, Assembler::AVX_512bit);\n+  __ evpsubd(xmm13, k0, twoGamma2, xmm9, false, Assembler::AVX_512bit);\n+  __ evpsubd(xmm14, k0, twoGamma2, xmm10, false, Assembler::AVX_512bit);\n+  __ evpsubd(xmm15, k0, twoGamma2, xmm11, false, Assembler::AVX_512bit);\n+\n+  __ evpsrad(xmm12, k0, xmm12, 22, false, Assembler::AVX_512bit);\n+  __ evpsrad(xmm13, k0, xmm13, 22, false, Assembler::AVX_512bit);\n+  __ evpsrad(xmm14, k0, xmm14, 22, false, Assembler::AVX_512bit);\n+  __ evpsrad(xmm15, k0, xmm15, 22, false, Assembler::AVX_512bit);\n+\n+  \/\/ mask in xmm12\n+  \/\/ r0 -= (mask & twoGamma2);\n+  __ evpandd(xmm16, k0, xmm12, twoGamma2, false, Assembler::AVX_512bit);\n+  __ evpandd(xmm17, k0, xmm13, twoGamma2, false, Assembler::AVX_512bit);\n+  __ evpandd(xmm18, k0, xmm14, twoGamma2, false, Assembler::AVX_512bit);\n+  __ evpandd(xmm19, k0, xmm15, twoGamma2, false, Assembler::AVX_512bit);\n+\n+  __ evpsubd(xmm8, k0, xmm8, xmm16, false, Assembler::AVX_512bit);\n+  __ evpsubd(xmm9, k0, xmm9, xmm17, false, Assembler::AVX_512bit);\n+  __ evpsubd(xmm10, k0, xmm10, xmm18, false, Assembler::AVX_512bit);\n+  __ evpsubd(xmm11, k0, xmm11, xmm19, false, Assembler::AVX_512bit);\n+\n+  \/\/ r0 in xmm8\n+  \/\/ quotient += (mask & 1);\n+  __ evpandd(xmm16, k0, xmm12, one, false, Assembler::AVX_512bit);\n+  __ evpandd(xmm17, k0, xmm13, one, false, Assembler::AVX_512bit);\n+  __ evpandd(xmm18, k0, xmm14, one, false, Assembler::AVX_512bit);\n+  __ evpandd(xmm19, k0, xmm15, one, false, Assembler::AVX_512bit);\n+\n+  __ evpaddd(xmm4, k0, xmm4, xmm16, false, Assembler::AVX_512bit);\n+  __ evpaddd(xmm5, k0, xmm5, xmm17, false, Assembler::AVX_512bit);\n+  __ evpaddd(xmm6, k0, xmm6, xmm18, false, Assembler::AVX_512bit);\n+  __ evpaddd(xmm7, k0, xmm7, xmm19, false, Assembler::AVX_512bit);\n+\n+  \/\/ mask = (twoGamma2 \/ 2 - r0) >> 31;\n+  __ evpsubd(xmm12, k0, gamma2, xmm8, false, Assembler::AVX_512bit);\n+  __ evpsubd(xmm13, k0, gamma2, xmm9, false, Assembler::AVX_512bit);\n+  __ evpsubd(xmm14, k0, gamma2, xmm10, false, Assembler::AVX_512bit);\n+  __ evpsubd(xmm15, k0, gamma2, xmm11, false, Assembler::AVX_512bit);\n+\n+  __ evpsrad(xmm12, k0, xmm12, 31, false, Assembler::AVX_512bit);\n+  __ evpsrad(xmm13, k0, xmm13, 31, false, Assembler::AVX_512bit);\n+  __ evpsrad(xmm14, k0, xmm14, 31, false, Assembler::AVX_512bit);\n+  __ evpsrad(xmm15, k0, xmm15, 31, false, Assembler::AVX_512bit);\n+\n+  \/\/ r0 -= (mask & twoGamma2);\n+  __ evpandd(xmm16, k0, xmm12, twoGamma2, false, Assembler::AVX_512bit);\n+  __ evpandd(xmm17, k0, xmm13, twoGamma2, false, Assembler::AVX_512bit);\n+  __ evpandd(xmm18, k0, xmm14, twoGamma2, false, Assembler::AVX_512bit);\n+  __ evpandd(xmm19, k0, xmm15, twoGamma2, false, Assembler::AVX_512bit);\n+\n+  __ evpsubd(xmm8, k0, xmm8, xmm16, false, Assembler::AVX_512bit);\n+  __ evpsubd(xmm9, k0, xmm9, xmm17, false, Assembler::AVX_512bit);\n+  __ evpsubd(xmm10, k0, xmm10, xmm18, false, Assembler::AVX_512bit);\n+  __ evpsubd(xmm11, k0, xmm11, xmm19, false, Assembler::AVX_512bit);\n+\n+  \/\/ r0 in xmm8\n+  \/\/ quotient += (mask & 1);\n+  __ evpandd(xmm16, k0, xmm12, one, false, Assembler::AVX_512bit);\n+  __ evpandd(xmm17, k0, xmm13, one, false, Assembler::AVX_512bit);\n+  __ evpandd(xmm18, k0, xmm14, one, false, Assembler::AVX_512bit);\n+  __ evpandd(xmm19, k0, xmm15, one, false, Assembler::AVX_512bit);\n+\n+  __ evpaddd(xmm4, k0, xmm4, xmm16, false, Assembler::AVX_512bit);\n+  __ evpaddd(xmm5, k0, xmm5, xmm17, false, Assembler::AVX_512bit);\n+  __ evpaddd(xmm6, k0, xmm6, xmm18, false, Assembler::AVX_512bit);\n+  __ evpaddd(xmm7, k0, xmm7, xmm19, false, Assembler::AVX_512bit);\n+\n+  \/\/ quotient in xmm4\n+  \/\/ int r1 = rplus - r0 - (dilithium_q - 1);\n+  __ evpsubd(xmm16, k0, xmm0, xmm8, false, Assembler::AVX_512bit);\n+  __ evpsubd(xmm17, k0, xmm1, xmm9, false, Assembler::AVX_512bit);\n+  __ evpsubd(xmm18, k0, xmm2, xmm10, false, Assembler::AVX_512bit);\n+  __ evpsubd(xmm19, k0, xmm3, xmm11, false, Assembler::AVX_512bit);\n+\n+  __ evpsubd(xmm16, k0, xmm16, xmm26, false, Assembler::AVX_512bit);\n+  __ evpsubd(xmm17, k0, xmm17, xmm26, false, Assembler::AVX_512bit);\n+  __ evpsubd(xmm18, k0, xmm18, xmm26, false, Assembler::AVX_512bit);\n+  __ evpsubd(xmm19, k0, xmm19, xmm26, false, Assembler::AVX_512bit);\n+\n+  \/\/ r1 in xmm16\n+  \/\/ r1 = (r1 | (-r1)) >> 31; \/\/ 0 if rplus - r0 == (dilithium_q - 1), -1 otherwise\n+  __ evpsubd(xmm20, k0, zero, xmm16, false, Assembler::AVX_512bit);\n+  __ evpsubd(xmm21, k0, zero, xmm17, false, Assembler::AVX_512bit);\n+  __ evpsubd(xmm22, k0, zero, xmm18, false, Assembler::AVX_512bit);\n+  __ evpsubd(xmm23, k0, zero, xmm19, false, Assembler::AVX_512bit);\n+\n+  __ evporq(xmm16, k0, xmm16, xmm20, false, Assembler::AVX_512bit);\n+  __ evporq(xmm17, k0, xmm17, xmm21, false, Assembler::AVX_512bit);\n+  __ evporq(xmm18, k0, xmm18, xmm22, false, Assembler::AVX_512bit);\n+  __ evporq(xmm19, k0, xmm19, xmm23, false, Assembler::AVX_512bit);\n+\n+  __ evpsubd(xmm12, k0, zero, one, false, Assembler::AVX_512bit); \/\/ -1\n+\n+  __ evpsrad(xmm0, k0, xmm16, 31, false, Assembler::AVX_512bit);\n+  __ evpsrad(xmm1, k0, xmm17, 31, false, Assembler::AVX_512bit);\n+  __ evpsrad(xmm2, k0, xmm18, 31, false, Assembler::AVX_512bit);\n+  __ evpsrad(xmm3, k0, xmm19, 31, false, Assembler::AVX_512bit);\n+\n+  \/\/ r1 in xmm0\n+  \/\/ r0 += ~r1;\n+  __ evpxorq(xmm20, k0, xmm0, xmm12, false, Assembler::AVX_512bit);\n+  __ evpxorq(xmm21, k0, xmm1, xmm12, false, Assembler::AVX_512bit);\n+  __ evpxorq(xmm22, k0, xmm2, xmm12, false, Assembler::AVX_512bit);\n+  __ evpxorq(xmm23, k0, xmm3, xmm12, false, Assembler::AVX_512bit);\n+\n+  __ evpaddd(xmm8, k0, xmm8, xmm20, false, Assembler::AVX_512bit);\n+  __ evpaddd(xmm9, k0, xmm9, xmm21, false, Assembler::AVX_512bit);\n+  __ evpaddd(xmm10, k0, xmm10, xmm22, false, Assembler::AVX_512bit);\n+  __ evpaddd(xmm11, k0, xmm11, xmm23, false, Assembler::AVX_512bit);\n+\n+  \/\/ r0 in xmm8\n+  \/\/ r1 = r1 & quotient;\n+  __ evpandd(xmm0, k0, xmm4, xmm0, false, Assembler::AVX_512bit);\n+  __ evpandd(xmm1, k0, xmm5, xmm1, false, Assembler::AVX_512bit);\n+  __ evpandd(xmm2, k0, xmm6, xmm2, false, Assembler::AVX_512bit);\n+  __ evpandd(xmm3, k0, xmm7, xmm3, false, Assembler::AVX_512bit);\n+\n+  \/\/ r1 in xmm0\n+  \/\/ lowPart[m] = r0;\n+  \/\/ highPart[m] = r1;\n+  __ evmovdqul(Address(highPart, 0), xmm0, Assembler::AVX_512bit);\n+  __ evmovdqul(Address(highPart, 64), xmm1, Assembler::AVX_512bit);\n+  __ evmovdqul(Address(highPart, 128), xmm2, Assembler::AVX_512bit);\n+  __ evmovdqul(Address(highPart, 192), xmm3, Assembler::AVX_512bit);\n+\n+  __ evmovdqul(Address(lowPart, 0), xmm8, Assembler::AVX_512bit);\n+  __ evmovdqul(Address(lowPart, 64), xmm9, Assembler::AVX_512bit);\n+  __ evmovdqul(Address(lowPart, 128), xmm10, Assembler::AVX_512bit);\n+  __ evmovdqul(Address(lowPart, 192), xmm11, Assembler::AVX_512bit);\n+\n+  __ subl(len, 256);\n+  __ addptr(highPart, 256);\n+  __ addptr(lowPart, 256);\n+  __ cmpl(len, 0);\n+  __ jcc(Assembler::notEqual, L_loop);\n+\n+  __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n+  __ mov64(rax, 0); \/\/ return 0\n+  __ ret(0);\n+\n+  return start;\n+}\n+\n+void StubGenerator::generate_dilithium_stubs() {\n+  \/\/ Generate Dilithium intrinsics code\n+  if (UseDilithiumIntrinsics) {\n+      StubRoutines::_dilithiumAlmostNtt =\n+        generate_dilithiumAlmostNtt_avx512(this, _masm);\n+      StubRoutines::_dilithiumAlmostInverseNtt =\n+        generate_dilithiumAlmostInverseNtt_avx512(this, _masm);\n+      StubRoutines::_dilithiumNttMult =\n+        generate_dilithiumNttMult_avx512(this, _masm);\n+      StubRoutines::_dilithiumMontMulByConstant =\n+        generate_dilithiumMontMulByConstant_avx512(this, _masm);\n+      StubRoutines::_dilithiumDecomposePoly =\n+        generate_dilithiumDecomposePoly_avx512(this, _masm);\n+  }\n+}\n","filename":"src\/hotspot\/cpu\/x86\/stubGenerator_x86_64_dilithium.cpp","additions":1058,"deletions":0,"binary":false,"changes":1058,"status":"added"},{"patch":"@@ -82,7 +82,0 @@\n-void StubGenerator::generate_sha3_stubs() {\n-  if (UseSHA3Intrinsics) {\n-    StubRoutines::_sha3_implCompress   = generate_sha3_implCompress(StubGenStubId::sha3_implCompress_id);\n-    StubRoutines::_sha3_implCompressMB = generate_sha3_implCompress(StubGenStubId::sha3_implCompressMB_id);\n-  }\n-}\n-\n@@ -98,1 +91,3 @@\n-address StubGenerator::generate_sha3_implCompress(StubGenStubId stub_id) {\n+static address generate_sha3_implCompress(StubGenStubId stub_id,\n+                                          StubGenerator *stubgen,\n+                                          MacroAssembler *_masm) {\n@@ -112,1 +107,1 @@\n-  StubCodeMark mark(this, stub_id);\n+  StubCodeMark mark(stubgen, stub_id);\n@@ -180,0 +175,1 @@\n+  __ align(OptoLoopAlignment);\n@@ -234,0 +230,1 @@\n+  __ align(OptoLoopAlignment);\n@@ -260,1 +257,1 @@\n-  \/\/ rho and sigma steps).\n+  \/\/ rho and pi steps).\n@@ -282,1 +279,1 @@\n-  \/\/ The combined rho and sigma steps are done.\n+  \/\/ The combined rho and pi steps are done.\n@@ -338,0 +335,215 @@\n+\n+\/\/ Inputs:\n+\/\/   c_rarg0   - long[]  state0\n+\/\/   c_rarg1   - long[]  state1\n+\/\/\n+\/\/ Performs two keccak() computations in parallel. The steps of the\n+\/\/ two computations are executed interleaved.\n+static address generate_double_keccak(StubGenerator *stubgen, MacroAssembler *_masm) {\n+  __ align(CodeEntryAlignment);\n+  StubGenStubId stub_id = double_keccak_id;\n+  StubCodeMark mark(stubgen, stub_id);\n+  address start = __ pc();\n+\n+  const Register state0 = c_rarg0;\n+  const Register state1 = c_rarg1;\n+\n+  const Register permsAndRots = c_rarg2;\n+  const Register round_consts = c_rarg3;\n+  const Register constant2use = r10;\n+  const Register roundsLeft = r11;\n+\n+  Label rounds24_loop;\n+\n+  __ enter();\n+\n+  __ lea(permsAndRots, ExternalAddress(permsAndRotsAddr()));\n+  __ lea(round_consts, ExternalAddress(round_constsAddr()));\n+\n+  \/\/ set up the masks\n+  __ mov64(rax,1);\n+  __ kmovbl(k1, rax);\n+  __ addl(rax,2);\n+  __ kmovbl(k2, rax);\n+  __ addl(rax, 4);\n+  __ kmovbl(k3, rax);\n+  __ addl(rax, 8);\n+  __ kmovbl(k4, rax);\n+  __ addl(rax, 16);\n+  __ kmovbl(k5, rax);\n+\n+  \/\/ load the states\n+  __ evmovdquq(xmm0, k5, Address(state0, 0), false, Assembler::AVX_512bit);\n+  __ evmovdquq(xmm1, k5, Address(state0, 40), false, Assembler::AVX_512bit);\n+  __ evmovdquq(xmm2, k5, Address(state0, 80), false, Assembler::AVX_512bit);\n+  __ evmovdquq(xmm3, k5, Address(state0, 120), false, Assembler::AVX_512bit);\n+  __ evmovdquq(xmm4, k5, Address(state0, 160), false, Assembler::AVX_512bit);\n+\n+  __ evmovdquq(xmm10, k5, Address(state1, 0), false, Assembler::AVX_512bit);\n+  __ evmovdquq(xmm11, k5, Address(state1, 40), false, Assembler::AVX_512bit);\n+  __ evmovdquq(xmm12, k5, Address(state1, 80), false, Assembler::AVX_512bit);\n+  __ evmovdquq(xmm13, k5, Address(state1, 120), false, Assembler::AVX_512bit);\n+  __ evmovdquq(xmm14, k5, Address(state1, 160), false, Assembler::AVX_512bit);\n+\n+  \/\/ load the permutation and rotation constants\n+  __ evmovdquq(xmm17, Address(permsAndRots, 0), Assembler::AVX_512bit);\n+  __ evmovdquq(xmm18, Address(permsAndRots, 64), Assembler::AVX_512bit);\n+  __ evmovdquq(xmm19, Address(permsAndRots, 128), Assembler::AVX_512bit);\n+  __ evmovdquq(xmm20, Address(permsAndRots, 192), Assembler::AVX_512bit);\n+  __ evmovdquq(xmm21, Address(permsAndRots, 256), Assembler::AVX_512bit);\n+  __ evmovdquq(xmm22, Address(permsAndRots, 320), Assembler::AVX_512bit);\n+  __ evmovdquq(xmm23, Address(permsAndRots, 384), Assembler::AVX_512bit);\n+  __ evmovdquq(xmm24, Address(permsAndRots, 448), Assembler::AVX_512bit);\n+  __ evmovdquq(xmm25, Address(permsAndRots, 512), Assembler::AVX_512bit);\n+  __ evmovdquq(xmm26, Address(permsAndRots, 576), Assembler::AVX_512bit);\n+  __ evmovdquq(xmm27, Address(permsAndRots, 640), Assembler::AVX_512bit);\n+  __ evmovdquq(xmm28, Address(permsAndRots, 704), Assembler::AVX_512bit);\n+  __ evmovdquq(xmm29, Address(permsAndRots, 768), Assembler::AVX_512bit);\n+  __ evmovdquq(xmm30, Address(permsAndRots, 832), Assembler::AVX_512bit);\n+  __ evmovdquq(xmm31, Address(permsAndRots, 896), Assembler::AVX_512bit);\n+\n+  \/\/ there will be 24 keccak rounds\n+  \/\/ The same operations as the ones in generate_sha3_implCompress are\n+  \/\/ performed, but in parallel for two states: one in regs z0-z5, using z6\n+  \/\/ as the scratch register and the other in z10-z15, using z16 as the\n+  \/\/ scratch register.\n+  \/\/ The permutation and rotation constants, that are loaded into z17-z31,\n+  \/\/ are shared between the two computations.\n+  __ movl(roundsLeft, 24);\n+  \/\/ load round_constants base\n+  __ movptr(constant2use, round_consts);\n+\n+  __ align(OptoLoopAlignment);\n+  __ BIND(rounds24_loop);\n+  __ subl( roundsLeft, 1);\n+\n+  __ evmovdquw(xmm5, xmm0, Assembler::AVX_512bit);\n+  __ evmovdquw(xmm15, xmm10, Assembler::AVX_512bit);\n+  __ vpternlogq(xmm5, 150, xmm1, xmm2, Assembler::AVX_512bit);\n+  __ vpternlogq(xmm15, 150, xmm11, xmm12, Assembler::AVX_512bit);\n+  __ vpternlogq(xmm5, 150, xmm3, xmm4, Assembler::AVX_512bit);\n+  __ vpternlogq(xmm15, 150, xmm13, xmm14, Assembler::AVX_512bit);\n+  __ evprolq(xmm6, xmm5, 1, Assembler::AVX_512bit);\n+  __ evprolq(xmm16, xmm15, 1, Assembler::AVX_512bit);\n+  __ evpermt2q(xmm5, xmm30, xmm5, Assembler::AVX_512bit);\n+  __ evpermt2q(xmm15, xmm30, xmm15, Assembler::AVX_512bit);\n+  __ evpermt2q(xmm6, xmm31, xmm6, Assembler::AVX_512bit);\n+  __ evpermt2q(xmm16, xmm31, xmm16, Assembler::AVX_512bit);\n+  __ vpternlogq(xmm0, 150, xmm5, xmm6, Assembler::AVX_512bit);\n+  __ vpternlogq(xmm10, 150, xmm15, xmm16, Assembler::AVX_512bit);\n+  __ vpternlogq(xmm1, 150, xmm5, xmm6, Assembler::AVX_512bit);\n+  __ vpternlogq(xmm11, 150, xmm15, xmm16, Assembler::AVX_512bit);\n+  __ vpternlogq(xmm2, 150, xmm5, xmm6, Assembler::AVX_512bit);\n+  __ vpternlogq(xmm12, 150, xmm15, xmm16, Assembler::AVX_512bit);\n+  __ vpternlogq(xmm3, 150, xmm5, xmm6, Assembler::AVX_512bit);\n+  __ vpternlogq(xmm13, 150, xmm15, xmm16, Assembler::AVX_512bit);\n+  __ vpternlogq(xmm4, 150, xmm5, xmm6, Assembler::AVX_512bit);\n+  __ vpternlogq(xmm14, 150, xmm15, xmm16, Assembler::AVX_512bit);\n+  __ evpermt2q(xmm4, xmm17, xmm3, Assembler::AVX_512bit);\n+  __ evpermt2q(xmm14, xmm17, xmm13, Assembler::AVX_512bit);\n+  __ evpermt2q(xmm3, xmm18, xmm2, Assembler::AVX_512bit);\n+  __ evpermt2q(xmm13, xmm18, xmm12, Assembler::AVX_512bit);\n+  __ evpermt2q(xmm2, xmm17, xmm1, Assembler::AVX_512bit);\n+  __ evpermt2q(xmm12, xmm17, xmm11, Assembler::AVX_512bit);\n+  __ evpermt2q(xmm1, xmm19, xmm0, Assembler::AVX_512bit);\n+  __ evpermt2q(xmm11, xmm19, xmm10, Assembler::AVX_512bit);\n+  __ evpermt2q(xmm4, xmm20, xmm2, Assembler::AVX_512bit);\n+  __ evpermt2q(xmm14, xmm20, xmm12, Assembler::AVX_512bit);\n+  __ evprolvq(xmm1, xmm1, xmm27, Assembler::AVX_512bit);\n+  __ evprolvq(xmm11, xmm11, xmm27, Assembler::AVX_512bit);\n+  __ evprolvq(xmm3, xmm3, xmm28, Assembler::AVX_512bit);\n+  __ evprolvq(xmm13, xmm13, xmm28, Assembler::AVX_512bit);\n+  __ evprolvq(xmm4, xmm4, xmm29, Assembler::AVX_512bit);\n+  __ evprolvq(xmm14, xmm14, xmm29, Assembler::AVX_512bit);\n+  __ evmovdquw(xmm2, xmm1, Assembler::AVX_512bit);\n+  __ evmovdquw(xmm12, xmm11, Assembler::AVX_512bit);\n+  __ evmovdquw(xmm5, xmm3, Assembler::AVX_512bit);\n+  __ evmovdquw(xmm15, xmm13, Assembler::AVX_512bit);\n+  __ evpermt2q(xmm0, xmm21, xmm4, Assembler::AVX_512bit);\n+  __ evpermt2q(xmm10, xmm21, xmm14, Assembler::AVX_512bit);\n+  __ evpermt2q(xmm1, xmm22, xmm3, Assembler::AVX_512bit);\n+  __ evpermt2q(xmm11, xmm22, xmm13, Assembler::AVX_512bit);\n+  __ evpermt2q(xmm5, xmm22, xmm2, Assembler::AVX_512bit);\n+  __ evpermt2q(xmm15, xmm22, xmm12, Assembler::AVX_512bit);\n+  __ evmovdquw(xmm3, xmm1, Assembler::AVX_512bit);\n+  __ evmovdquw(xmm13, xmm11, Assembler::AVX_512bit);\n+  __ evmovdquw(xmm2, xmm5, Assembler::AVX_512bit);\n+  __ evmovdquw(xmm12, xmm15, Assembler::AVX_512bit);\n+  __ evpermt2q(xmm1, xmm23, xmm4, Assembler::AVX_512bit);\n+  __ evpermt2q(xmm11, xmm23, xmm14, Assembler::AVX_512bit);\n+  __ evpermt2q(xmm2, xmm24, xmm4, Assembler::AVX_512bit);\n+  __ evpermt2q(xmm12, xmm24, xmm14, Assembler::AVX_512bit);\n+  __ evpermt2q(xmm3, xmm25, xmm4, Assembler::AVX_512bit);\n+  __ evpermt2q(xmm13, xmm25, xmm14, Assembler::AVX_512bit);\n+  __ evpermt2q(xmm4, xmm26, xmm5, Assembler::AVX_512bit);\n+  __ evpermt2q(xmm14, xmm26, xmm15, Assembler::AVX_512bit);\n+\n+  __ evpermt2q(xmm5, xmm31, xmm0, Assembler::AVX_512bit);\n+  __ evpermt2q(xmm15, xmm31, xmm10, Assembler::AVX_512bit);\n+  __ evpermt2q(xmm6, xmm31, xmm5, Assembler::AVX_512bit);\n+  __ evpermt2q(xmm16, xmm31, xmm15, Assembler::AVX_512bit);\n+  __ vpternlogq(xmm0, 180, xmm6, xmm5, Assembler::AVX_512bit);\n+  __ vpternlogq(xmm10, 180, xmm16, xmm15, Assembler::AVX_512bit);\n+\n+  __ evpermt2q(xmm5, xmm31, xmm1, Assembler::AVX_512bit);\n+  __ evpermt2q(xmm15, xmm31, xmm11, Assembler::AVX_512bit);\n+  __ evpermt2q(xmm6, xmm31, xmm5, Assembler::AVX_512bit);\n+  __ evpermt2q(xmm16, xmm31, xmm15, Assembler::AVX_512bit);\n+  __ vpternlogq(xmm1, 180, xmm6, xmm5, Assembler::AVX_512bit);\n+  __ vpternlogq(xmm11, 180, xmm16, xmm15, Assembler::AVX_512bit);\n+\n+  __ evpxorq(xmm0, k1, xmm0, Address(constant2use, 0), true, Assembler::AVX_512bit);\n+  __ evpxorq(xmm10, k1, xmm10, Address(constant2use, 0), true, Assembler::AVX_512bit);\n+  __ addptr(constant2use, 8);\n+\n+  __ evpermt2q(xmm5, xmm31, xmm2, Assembler::AVX_512bit);\n+  __ evpermt2q(xmm15, xmm31, xmm12, Assembler::AVX_512bit);\n+  __ evpermt2q(xmm6, xmm31, xmm5, Assembler::AVX_512bit);\n+  __ evpermt2q(xmm16, xmm31, xmm15, Assembler::AVX_512bit);\n+  __ vpternlogq(xmm2, 180, xmm6, xmm5, Assembler::AVX_512bit);\n+  __ vpternlogq(xmm12, 180, xmm16, xmm15, Assembler::AVX_512bit);\n+\n+  __ evpermt2q(xmm5, xmm31, xmm3, Assembler::AVX_512bit);\n+  __ evpermt2q(xmm15, xmm31, xmm13, Assembler::AVX_512bit);\n+  __ evpermt2q(xmm6, xmm31, xmm5, Assembler::AVX_512bit);\n+  __ evpermt2q(xmm16, xmm31, xmm15, Assembler::AVX_512bit);\n+  __ vpternlogq(xmm3, 180, xmm6, xmm5, Assembler::AVX_512bit);\n+  __ vpternlogq(xmm13, 180, xmm16, xmm15, Assembler::AVX_512bit);\n+  __ evpermt2q(xmm5, xmm31, xmm4, Assembler::AVX_512bit);\n+  __ evpermt2q(xmm15, xmm31, xmm14, Assembler::AVX_512bit);\n+  __ evpermt2q(xmm6, xmm31, xmm5, Assembler::AVX_512bit);\n+  __ evpermt2q(xmm16, xmm31, xmm15, Assembler::AVX_512bit);\n+  __ vpternlogq(xmm4, 180, xmm6, xmm5, Assembler::AVX_512bit);\n+  __ vpternlogq(xmm14, 180, xmm16, xmm15, Assembler::AVX_512bit);\n+  __ cmpl(roundsLeft, 0);\n+  __ jcc(Assembler::notEqual, rounds24_loop);\n+\n+  \/\/ store the states\n+  __ evmovdquq(Address(state0, 0), k5, xmm0, true, Assembler::AVX_512bit);\n+  __ evmovdquq(Address(state0, 40), k5, xmm1, true, Assembler::AVX_512bit);\n+  __ evmovdquq(Address(state0, 80), k5, xmm2, true, Assembler::AVX_512bit);\n+  __ evmovdquq(Address(state0, 120), k5, xmm3, true, Assembler::AVX_512bit);\n+  __ evmovdquq(Address(state0, 160), k5, xmm4, true, Assembler::AVX_512bit);\n+\n+  __ evmovdquq(Address(state1, 0), k5, xmm10, true, Assembler::AVX_512bit);\n+  __ evmovdquq(Address(state1, 40), k5, xmm11, true, Assembler::AVX_512bit);\n+  __ evmovdquq(Address(state1, 80), k5, xmm12, true, Assembler::AVX_512bit);\n+  __ evmovdquq(Address(state1, 120), k5, xmm13, true, Assembler::AVX_512bit);\n+  __ evmovdquq(Address(state1, 160), k5, xmm14, true, Assembler::AVX_512bit);\n+\n+  __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n+  __ ret(0);\n+\n+  return start;\n+}\n+\n+void StubGenerator::generate_sha3_stubs() {\n+  if (UseSHA3Intrinsics) {\n+    StubRoutines::_sha3_implCompress =\n+      generate_sha3_implCompress(StubGenStubId::sha3_implCompress_id, this, _masm);\n+    StubRoutines::_double_keccak =\n+      generate_double_keccak(this, _masm);\n+    StubRoutines::_sha3_implCompressMB =\n+      generate_sha3_implCompress(StubGenStubId::sha3_implCompressMB_id, this, _masm);\n+  }\n+}\n","filename":"src\/hotspot\/cpu\/x86\/stubGenerator_x86_64_sha3.cpp","additions":223,"deletions":11,"binary":false,"changes":234,"status":"modified"},{"patch":"@@ -1249,0 +1249,14 @@\n+  \/\/ Dilithium Intrinsics\n+  \/\/ Currently we only have them for AVX512\n+#ifdef _LP64\n+  if (supports_evex() && supports_avx512bw()) {\n+      if (FLAG_IS_DEFAULT(UseDilithiumIntrinsics)) {\n+          UseDilithiumIntrinsics = true;\n+      }\n+  } else\n+#endif\n+   if (UseDilithiumIntrinsics) {\n+      warning(\"Intrinsics for ML-DSA are not available on this CPU.\");\n+      FLAG_SET_DEFAULT(UseDilithiumIntrinsics, false);\n+  }\n+\n","filename":"src\/hotspot\/cpu\/x86\/vm_version_x86.cpp","additions":14,"deletions":0,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -571,1 +571,1 @@\n- do_class(sun_security_provider_ML_DSA,      \"sun\/security\/provider\/ML_DSA\")                                            \\\n+  do_class(sun_security_provider_ML_DSA,      \"sun\/security\/provider\/ML_DSA\")                                           \\\n","filename":"src\/hotspot\/share\/classfile\/vmIntrinsics.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -743,0 +743,2 @@\n+  do_stub(compiler, double_keccak)                                      \\\n+  do_entry(compiler, double_keccak, double_keccak, double_keccak)       \\\n@@ -746,2 +748,0 @@\n-  do_stub(compiler, double_keccak)                                      \\\n-  do_entry(compiler, double_keccak, double_keccak, double_keccak)       \\\n","filename":"src\/hotspot\/share\/runtime\/stubDeclarations.hpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -42,0 +42,10 @@\n+ * @run main Launcher\n+ *\/\n+\n+\/*\n+ * @test\n+ * @summary Test verifying the intrinsic implementation.\n+ * @bug 8342442 8345057\n+ * @library \/test\/lib\n+ * @modules java.base\/sun.security.provider\n+ * @run main\/othervm -Xcomp Launcher\n","filename":"test\/jdk\/sun\/security\/provider\/acvp\/Launcher.java","additions":10,"deletions":0,"binary":false,"changes":10,"status":"modified"}]}