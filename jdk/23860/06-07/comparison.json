{"files":[{"patch":"@@ -492,3 +492,0 @@\n-  address generate_sha3_implCompress(StubGenStubId stub_id);\n-\n-  address generate_double_keccak();\n@@ -497,5 +494,0 @@\n-  void montmulEven(int outputReg, int inputReg1,  int inputReg2,\n-    int scratchReg1, int scratchReg2, int parCnt);\n-  void montmulEven(int outputReg, int inputReg11, int inputReg12,\n-    int inputReg13, int inputReg14,  int inputReg2,\n-    int scratchReg1, int scratchReg2);\n@@ -503,6 +495,0 @@\n-  address generate_dilithiumAlmostNtt_avx512();\n-  address generate_dilithiumAlmostInverseNtt_avx512();\n-  address generate_dilithiumNttMult_avx512();\n-  address generate_dilithiumMontMulByConstant_avx512();\n-  address generate_dilithiumDecomposePoly_avx512();\n-\n","filename":"src\/hotspot\/cpu\/x86\/stubGenerator_x86_64.hpp","additions":0,"deletions":14,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -88,11 +88,0 @@\n-void StubGenerator::generate_dilithium_stubs() {\n-  \/\/ Generate Dilithium intrinsics code\n-  if (UseDilithiumIntrinsics) {\n-      StubRoutines::_dilithiumAlmostNtt = generate_dilithiumAlmostNtt_avx512();\n-      StubRoutines::_dilithiumAlmostInverseNtt = generate_dilithiumAlmostInverseNtt_avx512();\n-      StubRoutines::_dilithiumNttMult = generate_dilithiumNttMult_avx512();\n-      StubRoutines::_dilithiumMontMulByConstant = generate_dilithiumMontMulByConstant_avx512();\n-      StubRoutines::_dilithiumDecomposePoly = generate_dilithiumDecomposePoly_avx512();\n-  }\n-}\n-\n@@ -110,6 +99,16 @@\n-\/\/ Montgomery multiplication of the *even* numbered slices of parCnt consecutive register pairs\n-\/\/ Zmm_inputReg1 to Zmm_(inputReg1+parCnt-1) and Zmm_inputReg2 to Zmm_(inputReg2+parCnt-1).\n-\/\/ The result goes to the *odd* numbered slices of Zmm_outputReg to Zmm_(outputReg1+parCnt-1).\n-\/\/ Zmm_31 should contain q and Zmm_30 should contain q^-1 mod 2^32 in all of their slices.\n-void StubGenerator::montmulEven(int outputReg, int inputReg1,  int inputReg2, int scratchReg1, int scratchReg2, int parCnt) {\n-\n+\/\/ Montgomery multiplication of the *even* numbered slices of sequences\n+\/\/ of parCnt consecutive registers. Zmm_inputReg1 and Zmm_inputReg2 are the\n+\/\/ starts of the two sequences. When inputReg2 == 29, we use that register as\n+\/\/ the second multiplicand in each multiplication.\n+\/\/ The result goes to the *odd* numbered slices of Zmm_outputReg.\n+\/\/ The parCnt long cosecutive sequences of registers that start with\n+\/\/ Zmm_scratch1 and Zmm_scrath2 are used as scratch registers, so their\n+\/\/ contents will be clobbered.\n+\/\/ The output register sequence  can overlap any of the input and scratch\n+\/\/ register sequences, however the two scratch register sequences should be\n+\/\/ non-overlapping.\n+\/\/ Zmm_31 should contain q and\n+\/\/ Zmm_30 should contain q^-1 mod 2^32 in all of their slices.\n+static void montmulEven(int outputReg, int inputReg1,  int inputReg2,\n+                        int scratchReg1, int scratchReg2,\n+                        int parCnt, MacroAssembler *_masm) {\n@@ -117,1 +116,2 @@\n-    __ vpmuldq(xmm(i + scratchReg1), xmm(i + inputReg1), xmm((inputReg2 == 29) ? 29 : inputReg2 + i), Assembler::AVX_512bit);\n+    __ vpmuldq(xmm(i + scratchReg1), xmm(i + inputReg1),\n+               xmm((inputReg2 == 29) ? 29 : inputReg2 + i), Assembler::AVX_512bit);\n@@ -120,1 +120,2 @@\n-    __ vpmulld(xmm(i + scratchReg2), xmm(i + scratchReg1), xmm30, Assembler::AVX_512bit);\n+    __ vpmulld(xmm(i + scratchReg2), xmm(i + scratchReg1), xmm30,\n+               Assembler::AVX_512bit);\n@@ -123,1 +124,2 @@\n-    __ vpmuldq(xmm(i + scratchReg2), xmm(i + scratchReg2), xmm31, Assembler::AVX_512bit);\n+    __ vpmuldq(xmm(i + scratchReg2), xmm(i + scratchReg2), xmm31,\n+               Assembler::AVX_512bit);\n@@ -126,1 +128,2 @@\n-    __ evpsubd(xmm(i + outputReg), k0, xmm(i + scratchReg1), xmm(i + scratchReg2), false, Assembler::AVX_512bit);\n+    __ evpsubd(xmm(i + outputReg), k0, xmm(i + scratchReg1),\n+               xmm(i + scratchReg2), false, Assembler::AVX_512bit);\n@@ -130,4 +133,17 @@\n-\/\/ Similar to the 6-parameter montmulEven(), the difference is that here the input regs for the first\n-\/\/ arguments do not have to be consecutive and that parcnt is always 4, so it is not passed in.\n-void StubGenerator::montmulEven(int outputReg, int inputReg11, int inputReg12, int inputReg13, int inputReg14,\n-                                int inputReg2, int scratchReg1, int scratchReg2) {\n+\/\/ Full Montgomery multiplication of the corresponding slices of two register\n+\/\/ sets of 4 registers each. The indexes of the registers to be multiplied\n+\/\/ are in inputRegs1[] and inputRegs[2].\n+\/\/ The results go to the registers whose indexes are in outputRegs.\n+\/\/ scratchRegs should contain 12 different register indexes.\n+\/\/ The set in outputRegs should not overlap with the set of the middle four\n+\/\/ scratch registers.\n+\/\/ The sets in inputRegs1 and inputRegs2 cannot overlap with the set of the\n+\/\/ first eight scratch registers.\n+\/\/ In most of the cases, the odd and the corresponding even slices of the\n+\/\/ registers indexed by the numbers in inputRegs2 will contain the same number,\n+\/\/ this should be indicated by calling this function with\n+\/\/ input2NeedsShuffle=false .\n+\/\/\n+static void montMul64(int outputRegs[], int inputRegs1[], int inputRegs2[],\n+                      int scratchRegs[], bool input2NeedsShuffle,\n+                      MacroAssembler *_masm) {\n@@ -135,1 +151,16 @@\n-  int parCnt = 4;\n+  for (int i = 0; i < 4; i++) {\n+    __ vpmuldq(xmm(scratchRegs[i]), xmm(inputRegs1[i]), xmm(inputRegs2[i]),\n+               Assembler::AVX_512bit);\n+  }\n+  for (int i = 0; i < 4; i++) {\n+    __ vpmulld(xmm(scratchRegs[i + 4]), xmm(scratchRegs[i]), xmm30,\n+               Assembler::AVX_512bit);\n+  }\n+  for (int i = 0; i < 4; i++) {\n+    __ vpmuldq(xmm(scratchRegs[i + 4]), xmm(scratchRegs[i + 4]), xmm31,\n+               Assembler::AVX_512bit);\n+  }\n+  for (int i = 0; i < 4; i++) {\n+    __ evpsubd(xmm(scratchRegs[i + 4]), k0, xmm(scratchRegs[i]),\n+               xmm(scratchRegs[i + 4]), false, Assembler::AVX_512bit);\n+  }\n@@ -137,4 +168,8 @@\n-  __ vpmuldq(xmm(scratchReg1), xmm(inputReg11), xmm(inputReg2), Assembler::AVX_512bit);\n-  __ vpmuldq(xmm(scratchReg1 + 1), xmm(inputReg12), xmm(inputReg2 + 1), Assembler::AVX_512bit);\n-  __ vpmuldq(xmm(scratchReg1 + 2), xmm(inputReg13), xmm(inputReg2 + 2), Assembler::AVX_512bit);\n-  __ vpmuldq(xmm(scratchReg1 + 3), xmm(inputReg14), xmm(inputReg2 + 3), Assembler::AVX_512bit);\n+  for (int i = 0; i < 4; i++) {\n+    __ vpshufd(xmm(inputRegs1[i]), xmm(inputRegs1[i]), 0xB1,\n+               Assembler::AVX_512bit);\n+    if (input2NeedsShuffle) {\n+       __ vpshufd(xmm(inputRegs2[i]), xmm(inputRegs2[i]), 0xB1,\n+                  Assembler::AVX_512bit);\n+    }\n+  }\n@@ -142,2 +177,3 @@\n-  for (int i = 0; i < parCnt; i++) {\n-    __ vpmulld(xmm(i + scratchReg2), xmm(i + scratchReg1), xmm30, Assembler::AVX_512bit);\n+  for (int i = 0; i < 4; i++) {\n+    __ vpmuldq(xmm(scratchRegs[i]), xmm(inputRegs1[i]), xmm(inputRegs2[i]),\n+               Assembler::AVX_512bit);\n@@ -145,2 +181,3 @@\n-  for (int i = 0; i < parCnt; i++) {\n-    __ vpmuldq(xmm(i + scratchReg2), xmm(i + scratchReg2), xmm31, Assembler::AVX_512bit);\n+  for (int i = 0; i < 4; i++) {\n+    __ vpmulld(xmm(scratchRegs[i + 8]), xmm(scratchRegs[i]), xmm30,\n+               Assembler::AVX_512bit);\n@@ -148,2 +185,35 @@\n-  for (int i = 0; i < parCnt; i++) {\n-    __ evpsubd(xmm(i + outputReg), k0, xmm(i + scratchReg1), xmm(i + scratchReg2), false, Assembler::AVX_512bit);\n+  for (int i = 0; i < 4; i++) {\n+    __ vpmuldq(xmm(scratchRegs[i + 8]), xmm(scratchRegs[i + 8]), xmm31,\n+               Assembler::AVX_512bit);\n+  }\n+  for (int i = 0; i < 4; i++) {\n+    __ evpsubd(xmm(outputRegs[i]), k0, xmm(scratchRegs[i]),\n+               xmm(scratchRegs[i + 8]), false, Assembler::AVX_512bit);\n+  }\n+\n+  for (int i = 0; i < 4; i++) {\n+    __ evpermt2d(xmm(outputRegs[i]), xmm28, xmm(scratchRegs[i + 4]),\n+                 Assembler::AVX_512bit);\n+  }\n+}\n+\n+static void montMul64(int outputRegs[], int inputRegs1[], int inputRegs2[],\n+                       int scratchRegs[], MacroAssembler *_masm) {\n+   montMul64(outputRegs, inputRegs1, inputRegs2, scratchRegs, false, _masm);\n+}\n+\n+\/\/ input in Zmm0-Zmm7, the constant is repeated in all slots of Zmm29\n+\/\/ qinvmodR and q are repeated in all slots of Zmm30 and Zmm31, resp.\n+\/\/ Zmm8-Zmm23 used as scratch registers\n+\/\/ result goes to Zmm0-Zmm7\n+static void montMulByConst128(MacroAssembler *_masm) {\n+  montmulEven(8, 0, 29, 8, 16, 8, _masm);\n+\n+  for (int i = 0; i < 8; i++) {\n+    __ vpshufd(xmm(i),xmm(i), 0xB1, Assembler::AVX_512bit);\n+  }\n+\n+  montmulEven(0, 0, 29, 0, 16, 8, _masm);\n+\n+  for (int i = 0; i < 8; i++) {\n+    __ evpermt2d(xmm(i), xmm28, xmm(i + 8), Assembler::AVX_512bit);\n@@ -153,0 +223,32 @@\n+static void sub_add(int subResult[], int addResult[],\n+                    int input1[], int input2[], MacroAssembler *_masm) {\n+\n+  for (int i = 0; i < 4; i++) {\n+    __ evpsubd(xmm(subResult[i]), k0, xmm(input1[i]), xmm(input2[i]), false,\n+               Assembler::AVX_512bit);\n+  }\n+\n+  for (int i = 0; i < 4; i++) {\n+    __ evpaddd(xmm(addResult[i]), k0, xmm(input1[i]), xmm(input2[i]), false,\n+               Assembler::AVX_512bit);\n+  }\n+}\n+\n+static int xmm0_3[] = {0, 1, 2, 3};\n+static int xmm0145[] = {0, 1, 4, 5};\n+static int xmm0246[] = {0, 2, 4, 6};\n+static int xmm0426[] = {0, 4, 2, 6};\n+static int xmm1357[] = {1, 3, 5, 7};\n+static int xmm1537[] = {1, 5, 3, 7};\n+static int xmm2367[] = {2, 3, 6, 7};\n+static int xmm4_7[] = {4, 5, 6, 7};\n+static int xmm8_11[] = {8, 9, 10, 11};\n+static int xmm12_15[] = {12, 13, 14, 15};\n+static int xmm16_19[] = {16, 17, 18, 19};\n+static int xmm20222426[] = {20, 22, 24, 26};\n+static int xmm21232527[] = {21, 23, 25, 27};\n+static int xmm24_27[] = {24, 25, 26, 27};\n+static int xmm4_20_24[] = {4, 5, 6, 7, 20, 21, 22, 23, 24, 25, 26, 27};\n+static int xmm16_27[] = {16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27};\n+static int xmm29_29[] = {29, 29, 29, 29};\n+\n@@ -161,1 +263,1 @@\n-address StubGenerator::generate_dilithiumAlmostNtt_avx512() {\n+static address generate_dilithiumAlmostNtt_avx512(StubGenerator *stubgen, MacroAssembler *_masm) {\n@@ -165,1 +267,1 @@\n-  StubCodeMark mark(this, stub_id);\n+  StubCodeMark mark(stubgen, stub_id);\n@@ -197,25 +299,0 @@\n-  montmulEven(20, 8, 29, 20, 16, 4);\n-\n-  for (int i = 0; i < 4; i++) {\n-    __ vpshufd(xmm(i + 24), xmm(i + 8), 0xB1, Assembler::AVX_512bit);\n-  }\n-\n-  montmulEven(16, 24, 29, 24, 16, 4);\n-\n-  for (int i = 0; i < 4; i++) {\n-    __ evpermt2d(xmm(i + 16), xmm28, xmm(i + 20), Assembler::AVX_512bit);\n-  }\n-\n-  for (int i = 0; i < 4; i++) {\n-    __ evpsubd(xmm(i + 8), k0, xmm(i), xmm(i + 16), false, Assembler::AVX_512bit);\n-  }\n-\n-  for (int i = 0; i < 4; i++) {\n-    __ evpaddd(xmm(i), k0, xmm(i), xmm(i + 16), false, Assembler::AVX_512bit);\n-  }\n-\n-  montmulEven(20, 12, 29, 20, 16, 4);\n-\n-  for (int i = 0; i < 4; i++) {\n-    __ vpshufd(xmm(i + 24), xmm(i + 12), 0xB1, Assembler::AVX_512bit);\n-  }\n@@ -223,13 +300,5 @@\n-  montmulEven(16, 24, 29, 24, 16, 4);\n-\n-  for (int i = 0; i < 4; i++) {\n-    __ evpermt2d(xmm(i + 16), xmm28, xmm(i + 20), Assembler::AVX_512bit);\n-  }\n-\n-  for (int i = 0; i < 4; i++) {\n-    __ evpsubd(xmm(i + 12), k0, xmm(i + 4), xmm(i + 16), false, Assembler::AVX_512bit);\n-  }\n-\n-  for (int i = 0; i < 4; i++) {\n-    __ evpaddd(xmm(i + 4), k0, xmm(i + 4), xmm(i + 16), false, Assembler::AVX_512bit);\n-  }\n+  montMul64(xmm16_19, xmm8_11, xmm29_29, xmm16_27, _masm);\n+  sub_add(xmm8_11, xmm0_3, xmm0_3, xmm16_19, _masm);\n+  montMul64(xmm16_19, xmm12_15, xmm29_29, xmm16_27, _masm);\n+  __ evmovdqul(xmm29, Address(zetas, 512), Assembler::AVX_512bit); \/\/ for level 1\n+  sub_add(xmm12_15, xmm4_7, xmm4_7, xmm16_19, _masm);\n@@ -238,20 +307,0 @@\n-  __ evmovdqul(xmm29, Address(zetas, 512), Assembler::AVX_512bit);\n-\n-  montmulEven(20, 4, 29, 20, 16, 4);\n-  for (int i = 0; i < 4; i++) {\n-    __ vpshufd(xmm(i + 24), xmm(i + 4), 0xB1, Assembler::AVX_512bit);\n-  }\n-\n-  montmulEven(16, 24, 29, 24, 16, 4);\n-\n-  for (int i = 0; i < 4; i++) {\n-    __ evpermt2d(xmm(i + 16), xmm28, xmm(i + 20), Assembler::AVX_512bit);\n-  }\n-\n-  for (int i = 0; i < 4; i++) {\n-    __ evpsubd(xmm(i + 4), k0, xmm(i), xmm(i + 16), false, Assembler::AVX_512bit);\n-  }\n-\n-  for (int i = 0; i < 4; i++) {\n-    __ evpaddd(xmm(i), k0, xmm(i), xmm(i + 16), false, Assembler::AVX_512bit);\n-  }\n@@ -259,0 +308,1 @@\n+  montMul64(xmm16_19, xmm4_7, xmm29_29, xmm16_27, _masm);\n@@ -260,19 +310,3 @@\n-\n-  montmulEven(20, 12, 29, 20, 16, 4);\n-  for (int i = 0; i < 4; i++) {\n-    __ vpshufd(xmm(i + 24), xmm(i + 12), 0xB1, Assembler::AVX_512bit);\n-  }\n-\n-  montmulEven(16, 24, 29, 24, 16, 4);\n-\n-  for (int i = 0; i < 4; i++) {\n-    __ evpermt2d(xmm(i + 16), xmm28, xmm(i + 20), Assembler::AVX_512bit);\n-  }\n-\n-  for (int i = 0; i < 4; i++) {\n-    __ evpsubd(xmm(i + 12), k0, xmm(i + 8), xmm(i + 16), false, Assembler::AVX_512bit);\n-  }\n-\n-  for (int i = 0; i < 4; i++) {\n-    __ evpaddd(xmm(i + 8), k0, xmm(i + 8), xmm(i + 16), false, Assembler::AVX_512bit);\n-  }\n+  sub_add(xmm4_7, xmm0_3, xmm0_3, xmm16_19, _masm);\n+  montMul64(xmm16_19, xmm12_15, xmm29_29, xmm16_27, _masm);\n+  sub_add(xmm12_15, xmm8_11, xmm8_11, xmm16_19, _masm);\n@@ -303,8 +337,1 @@\n-  montmulEven(20, 2, 3, 6, 7, 12, 20, 16);\n-\n-  __ vpshufd(xmm(8), xmm(2), 0xB1, Assembler::AVX_512bit);\n-  __ vpshufd(xmm(9), xmm(3), 0xB1, Assembler::AVX_512bit);\n-  __ vpshufd(xmm(10), xmm(6), 0xB1, Assembler::AVX_512bit);\n-  __ vpshufd(xmm(11), xmm(7), 0xB1, Assembler::AVX_512bit);\n-\n-  montmulEven(16, 8, 12, 24, 16, 4);\n+  montMul64(xmm16_19, xmm2367, xmm12_15, xmm16_27, _masm);\n@@ -312,16 +339,1 @@\n-  for (int i = 0; i < 4; i++) {\n-    __ evpermt2d(xmm(i + 16), xmm28, xmm(i + 20), Assembler::AVX_512bit);\n-  }\n-\n-  __ evpsubd(xmm(2), k0, xmm(0), xmm(16), false, Assembler::AVX_512bit);\n-  __ evpsubd(xmm(3), k0, xmm(1), xmm(17), false, Assembler::AVX_512bit);\n-  __ evpsubd(xmm(6), k0, xmm(4), xmm(18), false, Assembler::AVX_512bit);\n-  __ evpsubd(xmm(7), k0, xmm(5), xmm(19), false, Assembler::AVX_512bit);\n-\n-  __ evpaddd(xmm(0), k0, xmm(0), xmm(16), false, Assembler::AVX_512bit);\n-  __ evpaddd(xmm(1), k0, xmm(1), xmm(17), false, Assembler::AVX_512bit);\n-  __ evpaddd(xmm(4), k0, xmm(4), xmm(18), false, Assembler::AVX_512bit);\n-  __ evpaddd(xmm(5), k0, xmm(5), xmm(19), false, Assembler::AVX_512bit);\n-\n-  \/\/ level 3\n-  __ evmovdqul(xmm12, Address(zetas, 1536), Assembler::AVX_512bit);\n+  __ evmovdqul(xmm12, Address(zetas, 1536), Assembler::AVX_512bit); \/\/ for level 3\n@@ -331,0 +343,1 @@\n+  sub_add(xmm2367, xmm0145, xmm0145, xmm16_19, _masm);\n@@ -332,17 +345,1 @@\n-  montmulEven(20, 1, 3, 5, 7, 12, 20, 16);\n-\n-  __ vpshufd(xmm(8), xmm(1), 0xB1, Assembler::AVX_512bit);\n-  __ vpshufd(xmm(9), xmm(3), 0xB1, Assembler::AVX_512bit);\n-  __ vpshufd(xmm(10), xmm(5), 0xB1, Assembler::AVX_512bit);\n-  __ vpshufd(xmm(11), xmm(7), 0xB1, Assembler::AVX_512bit);\n-\n-  montmulEven(16, 8, 12, 24, 16, 4);\n-\n-  for (int i = 0; i < 4; i++) {\n-    __ evpermt2d(xmm(i + 16), xmm28, xmm(i + 20), Assembler::AVX_512bit);\n-  }\n-\n-  __ evpsubd(xmm(1), k0, xmm(0), xmm(16), false, Assembler::AVX_512bit);\n-  __ evpsubd(xmm(3), k0, xmm(2), xmm(17), false, Assembler::AVX_512bit);\n-  __ evpsubd(xmm(5), k0, xmm(4), xmm(18), false, Assembler::AVX_512bit);\n-  __ evpsubd(xmm(7), k0, xmm(6), xmm(19), false, Assembler::AVX_512bit);\n+  \/\/ level 3\n@@ -350,4 +347,2 @@\n-  __ evpaddd(xmm(0), k0, xmm(0), xmm(16), false, Assembler::AVX_512bit);\n-  __ evpaddd(xmm(2), k0, xmm(2), xmm(17), false, Assembler::AVX_512bit);\n-  __ evpaddd(xmm(4), k0, xmm(4), xmm(18), false, Assembler::AVX_512bit);\n-  __ evpaddd(xmm(6), k0, xmm(6), xmm(19), false, Assembler::AVX_512bit);\n+  montMul64(xmm16_19, xmm1357, xmm12_15, xmm16_27, _masm);\n+  sub_add(xmm1357, xmm0246, xmm0246, xmm16_19, _masm);\n@@ -365,0 +360,5 @@\n+  __ evmovdqul(xmm24, Address(zetas, 4 * 512), Assembler::AVX_512bit);\n+  __ evmovdqul(xmm25, Address(zetas, 4 * 512 + 64), Assembler::AVX_512bit);\n+  __ evmovdqul(xmm26, Address(zetas, 4 * 512 + 128), Assembler::AVX_512bit);\n+  __ evmovdqul(xmm27, Address(zetas, 4 * 512 + 192), Assembler::AVX_512bit);\n+\n@@ -371,4 +371,0 @@\n-  __ evmovdqul(xmm0, Address(zetas, 4 * 512), Assembler::AVX_512bit);\n-  __ evmovdqul(xmm1, Address(zetas, 4 * 512 + 64), Assembler::AVX_512bit);\n-  __ evmovdqul(xmm2, Address(zetas, 4 * 512 + 128), Assembler::AVX_512bit);\n-  __ evmovdqul(xmm3, Address(zetas, 4 * 512 + 192), Assembler::AVX_512bit);\n@@ -376,22 +372,2 @@\n-  montmulEven(20, 0, 12, 4, 20, 4);\n-\n-  __ vpshufd(xmm(12), xmm(12), 0xB1, Assembler::AVX_512bit);\n-  __ vpshufd(xmm(13), xmm(13), 0xB1, Assembler::AVX_512bit);\n-  __ vpshufd(xmm(14), xmm(14), 0xB1, Assembler::AVX_512bit);\n-  __ vpshufd(xmm(15), xmm(15), 0xB1, Assembler::AVX_512bit);\n-\n-  montmulEven(12, 0, 12, 4, 24, 4);\n-\n-  for (int i = 0; i < 4; i++) {\n-    __ evpermt2d(xmm(i + 12), xmm28, xmm(i + 20), Assembler::AVX_512bit);\n-  }\n-\n-  __ evpsubd(xmm(1), k0, xmm(16), xmm(12), false, Assembler::AVX_512bit);\n-  __ evpsubd(xmm(3), k0, xmm(17), xmm(13), false, Assembler::AVX_512bit);\n-  __ evpsubd(xmm(5), k0, xmm(18), xmm(14), false, Assembler::AVX_512bit);\n-  __ evpsubd(xmm(7), k0, xmm(19), xmm(15), false, Assembler::AVX_512bit);\n-\n-  __ evpaddd(xmm(0), k0, xmm(16), xmm(12), false, Assembler::AVX_512bit);\n-  __ evpaddd(xmm(2), k0, xmm(17), xmm(13), false, Assembler::AVX_512bit);\n-  __ evpaddd(xmm(4), k0, xmm(18), xmm(14), false, Assembler::AVX_512bit);\n-  __ evpaddd(xmm(6), k0, xmm(19), xmm(15), false, Assembler::AVX_512bit);\n+  montMul64(xmm12_15, xmm12_15, xmm24_27, xmm4_20_24, _masm);\n+  sub_add(xmm1357, xmm0246, xmm16_19, xmm12_15, _masm);\n@@ -409,0 +385,5 @@\n+  __ evmovdqul(xmm24, Address(zetas, 5 * 512), Assembler::AVX_512bit);\n+  __ evmovdqul(xmm25, Address(zetas, 5 * 512 + 64), Assembler::AVX_512bit);\n+  __ evmovdqul(xmm26, Address(zetas, 5 * 512 + 128), Assembler::AVX_512bit);\n+  __ evmovdqul(xmm27, Address(zetas, 5 * 512 + 192), Assembler::AVX_512bit);\n+\n@@ -415,6 +396,0 @@\n-  __ evmovdqul(xmm0, Address(zetas, 5 * 512), Assembler::AVX_512bit);\n-  __ evmovdqul(xmm1, Address(zetas, 5 * 512 + 64), Assembler::AVX_512bit);\n-  __ evmovdqul(xmm2, Address(zetas, 5 * 512 + 128), Assembler::AVX_512bit);\n-  __ evmovdqul(xmm3, Address(zetas, 5 * 512 + 192), Assembler::AVX_512bit);\n-\n-  montmulEven(20, 0, 12, 4, 20, 4);\n@@ -422,20 +397,2 @@\n-  __ vpshufd(xmm(12), xmm(12), 0xB1, Assembler::AVX_512bit);\n-  __ vpshufd(xmm(13), xmm(13), 0xB1, Assembler::AVX_512bit);\n-  __ vpshufd(xmm(14), xmm(14), 0xB1, Assembler::AVX_512bit);\n-  __ vpshufd(xmm(15), xmm(15), 0xB1, Assembler::AVX_512bit);\n-\n-  montmulEven(12, 0, 12, 4, 24, 4);\n-\n-  for (int i = 0; i < 4; i++) {\n-    __ evpermt2d(xmm(i + 12), xmm28, xmm(i + 20), Assembler::AVX_512bit);\n-  }\n-\n-  __ evpsubd(xmm(1), k0, xmm(16), xmm(12), false, Assembler::AVX_512bit);\n-  __ evpsubd(xmm(3), k0, xmm(17), xmm(13), false, Assembler::AVX_512bit);\n-  __ evpsubd(xmm(5), k0, xmm(18), xmm(14), false, Assembler::AVX_512bit);\n-  __ evpsubd(xmm(7), k0, xmm(19), xmm(15), false, Assembler::AVX_512bit);\n-\n-  __ evpaddd(xmm(0), k0, xmm(16), xmm(12), false, Assembler::AVX_512bit);\n-  __ evpaddd(xmm(2), k0, xmm(17), xmm(13), false, Assembler::AVX_512bit);\n-  __ evpaddd(xmm(4), k0, xmm(18), xmm(14), false, Assembler::AVX_512bit);\n-  __ evpaddd(xmm(6), k0, xmm(19), xmm(15), false, Assembler::AVX_512bit);\n+  montMul64(xmm12_15, xmm12_15, xmm24_27, xmm4_20_24, _masm);\n+  sub_add(xmm1357, xmm0246, xmm16_19, xmm12_15, _masm);\n@@ -453,0 +410,5 @@\n+  __ evmovdqul(xmm24, Address(zetas, 6 * 512), Assembler::AVX_512bit);\n+  __ evmovdqul(xmm25, Address(zetas, 6 * 512 + 64), Assembler::AVX_512bit);\n+  __ evmovdqul(xmm26, Address(zetas, 6 * 512 + 128), Assembler::AVX_512bit);\n+  __ evmovdqul(xmm27, Address(zetas, 6 * 512 + 192), Assembler::AVX_512bit);\n+\n@@ -459,6 +421,0 @@\n-  __ evmovdqul(xmm0, Address(zetas, 6 * 512), Assembler::AVX_512bit);\n-  __ evmovdqul(xmm1, Address(zetas, 6 * 512 + 64), Assembler::AVX_512bit);\n-  __ evmovdqul(xmm2, Address(zetas, 6 * 512 + 128), Assembler::AVX_512bit);\n-  __ evmovdqul(xmm3, Address(zetas, 6 * 512 + 192), Assembler::AVX_512bit);\n-\n-  montmulEven(20, 0, 12, 4, 20, 4);\n@@ -466,20 +422,2 @@\n-  __ vpshufd(xmm(12), xmm(12), 0xB1, Assembler::AVX_512bit);\n-  __ vpshufd(xmm(13), xmm(13), 0xB1, Assembler::AVX_512bit);\n-  __ vpshufd(xmm(14), xmm(14), 0xB1, Assembler::AVX_512bit);\n-  __ vpshufd(xmm(15), xmm(15), 0xB1, Assembler::AVX_512bit);\n-\n-  montmulEven(12, 0, 12, 4, 24, 4);\n-\n-  for (int i = 0; i < 4; i++) {\n-    __ evpermt2d(xmm(i + 12), xmm28, xmm(i + 20), Assembler::AVX_512bit);\n-  }\n-\n-  __ evpsubd(xmm(1), k0, xmm(16), xmm(12), false, Assembler::AVX_512bit);\n-  __ evpsubd(xmm(3), k0, xmm(17), xmm(13), false, Assembler::AVX_512bit);\n-  __ evpsubd(xmm(5), k0, xmm(18), xmm(14), false, Assembler::AVX_512bit);\n-  __ evpsubd(xmm(7), k0, xmm(19), xmm(15), false, Assembler::AVX_512bit);\n-\n-  __ evpaddd(xmm(0), k0, xmm(16), xmm(12), false, Assembler::AVX_512bit);\n-  __ evpaddd(xmm(2), k0, xmm(17), xmm(13), false, Assembler::AVX_512bit);\n-  __ evpaddd(xmm(4), k0, xmm(18), xmm(14), false, Assembler::AVX_512bit);\n-  __ evpaddd(xmm(6), k0, xmm(19), xmm(15), false, Assembler::AVX_512bit);\n+  montMul64(xmm12_15, xmm12_15, xmm24_27, xmm4_20_24, _masm);\n+  sub_add(xmm1357, xmm0246, xmm16_19, xmm12_15, _masm);\n@@ -497,0 +435,5 @@\n+  __ evmovdqul(xmm24, Address(zetas, 7 * 512), Assembler::AVX_512bit);\n+  __ evmovdqul(xmm25, Address(zetas, 7 * 512 + 64), Assembler::AVX_512bit);\n+  __ evmovdqul(xmm26, Address(zetas, 7 * 512 + 128), Assembler::AVX_512bit);\n+  __ evmovdqul(xmm27, Address(zetas, 7 * 512 + 192), Assembler::AVX_512bit);\n+\n@@ -503,15 +446,0 @@\n-  __ evmovdqul(xmm0, Address(zetas, 7 * 512), Assembler::AVX_512bit);\n-  __ evmovdqul(xmm1, Address(zetas, 7 * 512 + 64), Assembler::AVX_512bit);\n-  __ evmovdqul(xmm2, Address(zetas, 7 * 512 + 128), Assembler::AVX_512bit);\n-  __ evmovdqul(xmm3, Address(zetas, 7 * 512 + 192), Assembler::AVX_512bit);\n-\n-  montmulEven(20, 0, 12, 4, 20, 4);\n-\n-  __ vpshufd(xmm(12), xmm(12), 0xB1, Assembler::AVX_512bit);\n-  __ vpshufd(xmm(13), xmm(13), 0xB1, Assembler::AVX_512bit);\n-  __ vpshufd(xmm(14), xmm(14), 0xB1, Assembler::AVX_512bit);\n-  __ vpshufd(xmm(15), xmm(15), 0xB1, Assembler::AVX_512bit);\n-  __ vpshufd(xmm(0), xmm(0), 0xB1, Assembler::AVX_512bit);\n-  __ vpshufd(xmm(1), xmm(1), 0xB1, Assembler::AVX_512bit);\n-  __ vpshufd(xmm(2), xmm(2), 0xB1, Assembler::AVX_512bit);\n-  __ vpshufd(xmm(3), xmm(3), 0xB1, Assembler::AVX_512bit);\n@@ -519,15 +447,1 @@\n-  montmulEven(12, 0, 12, 4, 24, 4);\n-\n-  for (int i = 0; i < 4; i++) {\n-    __ evpermt2d(xmm(i + 12), xmm28, xmm(i + 20), Assembler::AVX_512bit);\n-  }\n-\n-  __ evpsubd(xmm(21), k0, xmm(16), xmm(12), false, Assembler::AVX_512bit);\n-  __ evpsubd(xmm(23), k0, xmm(17), xmm(13), false, Assembler::AVX_512bit);\n-  __ evpsubd(xmm(25), k0, xmm(18), xmm(14), false, Assembler::AVX_512bit);\n-  __ evpsubd(xmm(27), k0, xmm(19), xmm(15), false, Assembler::AVX_512bit);\n-\n-  __ evpaddd(xmm(20), k0, xmm(16), xmm(12), false, Assembler::AVX_512bit);\n-  __ evpaddd(xmm(22), k0, xmm(17), xmm(13), false, Assembler::AVX_512bit);\n-  __ evpaddd(xmm(24), k0, xmm(18), xmm(14), false, Assembler::AVX_512bit);\n-  __ evpaddd(xmm(26), k0, xmm(19), xmm(15), false, Assembler::AVX_512bit);\n+  montMul64(xmm12_15, xmm12_15, xmm24_27, xmm4_20_24, true, _masm);\n@@ -544,0 +458,2 @@\n+  sub_add(xmm21232527, xmm20222426, xmm16_19, xmm12_15, _masm);\n+\n@@ -583,1 +499,2 @@\n-address StubGenerator::generate_dilithiumAlmostInverseNtt_avx512() {\n+static address generate_dilithiumAlmostInverseNtt_avx512(StubGenerator *stubgen,\n+                                                         MacroAssembler *_masm) {\n@@ -587,1 +504,1 @@\n-  StubCodeMark mark(this, stub_id);\n+  StubCodeMark mark(stubgen, stub_id);\n@@ -635,10 +552,0 @@\n-  __ evpaddd(xmm(0), k0, xmm(8), xmm(12), false, Assembler::AVX_512bit);\n-  __ evpaddd(xmm(1), k0, xmm(9), xmm(13), false, Assembler::AVX_512bit);\n-  __ evpaddd(xmm(2), k0, xmm(10), xmm(14), false, Assembler::AVX_512bit);\n-  __ evpaddd(xmm(3), k0, xmm(11), xmm(15), false, Assembler::AVX_512bit);\n-\n-  __ evpsubd(xmm(8), k0, xmm(8), xmm(12), false, Assembler::AVX_512bit);\n-  __ evpsubd(xmm(9), k0, xmm(9), xmm(13), false, Assembler::AVX_512bit);\n-  __ evpsubd(xmm(10), k0, xmm(10), xmm(14), false, Assembler::AVX_512bit);\n-  __ evpsubd(xmm(11), k0, xmm(11), xmm(15), false, Assembler::AVX_512bit);\n-\n@@ -650,16 +557,2 @@\n-  montmulEven(20, 4, 8, 16, 20, 4);\n-\n-  __ vpshufd(xmm(4), xmm(4), 0xB1, Assembler::AVX_512bit);\n-  __ vpshufd(xmm(5), xmm(5), 0xB1, Assembler::AVX_512bit);\n-  __ vpshufd(xmm(6), xmm(6), 0xB1, Assembler::AVX_512bit);\n-  __ vpshufd(xmm(7), xmm(7), 0xB1, Assembler::AVX_512bit);\n-  __ vpshufd(xmm(8), xmm(8), 0xB1, Assembler::AVX_512bit);\n-  __ vpshufd(xmm(9), xmm(9), 0xB1, Assembler::AVX_512bit);\n-  __ vpshufd(xmm(10), xmm(10), 0xB1, Assembler::AVX_512bit);\n-  __ vpshufd(xmm(11), xmm(11), 0xB1, Assembler::AVX_512bit);\n-\n-  montmulEven(4, 4, 8, 16, 24, 4);\n-\n-  for (int i = 0; i < 4; i++) {\n-    __ evpermt2d(xmm(i + 4), xmm28, xmm(i + 20), Assembler::AVX_512bit);\n-  }\n+  sub_add(xmm24_27, xmm0_3, xmm8_11, xmm12_15, _masm);\n+  montMul64(xmm4_7, xmm4_7, xmm24_27, xmm16_27, true, _masm);\n@@ -682,10 +575,0 @@\n-  __ evpaddd(xmm(0), k0, xmm(8), xmm(12), false, Assembler::AVX_512bit);\n-  __ evpaddd(xmm(1), k0, xmm(9), xmm(13), false, Assembler::AVX_512bit);\n-  __ evpaddd(xmm(2), k0, xmm(10), xmm(14), false, Assembler::AVX_512bit);\n-  __ evpaddd(xmm(3), k0, xmm(11), xmm(15), false, Assembler::AVX_512bit);\n-\n-  __ evpsubd(xmm(8), k0, xmm(8), xmm(12), false, Assembler::AVX_512bit);\n-  __ evpsubd(xmm(9), k0, xmm(9), xmm(13), false, Assembler::AVX_512bit);\n-  __ evpsubd(xmm(10), k0, xmm(10), xmm(14), false, Assembler::AVX_512bit);\n-  __ evpsubd(xmm(11), k0, xmm(11), xmm(15), false, Assembler::AVX_512bit);\n-\n@@ -697,12 +580,2 @@\n-  montmulEven(20, 4, 8, 16, 20, 4);\n-\n-  __ vpshufd(xmm(8), xmm(8), 0xB1, Assembler::AVX_512bit);\n-  __ vpshufd(xmm(9), xmm(9), 0xB1, Assembler::AVX_512bit);\n-  __ vpshufd(xmm(10), xmm(10), 0xB1, Assembler::AVX_512bit);\n-  __ vpshufd(xmm(11), xmm(11), 0xB1, Assembler::AVX_512bit);\n-\n-  montmulEven(4, 4, 8, 16, 24, 4);\n-\n-  for (int i = 0; i < 4; i++) {\n-    __ evpermt2d(xmm(i + 4), xmm28, xmm(i + 20), Assembler::AVX_512bit);\n-  }\n+  sub_add(xmm24_27, xmm0_3, xmm8_11, xmm12_15, _masm);\n+  montMul64(xmm4_7, xmm24_27, xmm4_7, xmm16_27, _masm);\n@@ -725,10 +598,0 @@\n-  __ evpaddd(xmm(0), k0, xmm(8), xmm(12), false, Assembler::AVX_512bit);\n-  __ evpaddd(xmm(1), k0, xmm(9), xmm(13), false, Assembler::AVX_512bit);\n-  __ evpaddd(xmm(2), k0, xmm(10), xmm(14), false, Assembler::AVX_512bit);\n-  __ evpaddd(xmm(3), k0, xmm(11), xmm(15), false, Assembler::AVX_512bit);\n-\n-  __ evpsubd(xmm(8), k0, xmm(8), xmm(12), false, Assembler::AVX_512bit);\n-  __ evpsubd(xmm(9), k0, xmm(9), xmm(13), false, Assembler::AVX_512bit);\n-  __ evpsubd(xmm(10), k0, xmm(10), xmm(14), false, Assembler::AVX_512bit);\n-  __ evpsubd(xmm(11), k0, xmm(11), xmm(15), false, Assembler::AVX_512bit);\n-\n@@ -740,12 +603,2 @@\n-  montmulEven(20, 4, 8, 16, 20, 4);\n-\n-  __ vpshufd(xmm(8), xmm(8), 0xB1, Assembler::AVX_512bit);\n-  __ vpshufd(xmm(9), xmm(9), 0xB1, Assembler::AVX_512bit);\n-  __ vpshufd(xmm(10), xmm(10), 0xB1, Assembler::AVX_512bit);\n-  __ vpshufd(xmm(11), xmm(11), 0xB1, Assembler::AVX_512bit);\n-\n-  montmulEven(4, 4, 8, 16, 24, 4);\n-\n-  for (int i = 0; i < 4; i++) {\n-    __ evpermt2d(xmm(i + 4), xmm28, xmm(i + 20), Assembler::AVX_512bit);\n-  }\n+  sub_add(xmm24_27, xmm0_3, xmm8_11, xmm12_15, _masm);\n+  montMul64(xmm4_7, xmm24_27, xmm4_7, xmm16_27, _masm);\n@@ -768,10 +621,0 @@\n-  __ evpaddd(xmm(0), k0, xmm(8), xmm(12), false, Assembler::AVX_512bit);\n-  __ evpaddd(xmm(1), k0, xmm(9), xmm(13), false, Assembler::AVX_512bit);\n-  __ evpaddd(xmm(2), k0, xmm(10), xmm(14), false, Assembler::AVX_512bit);\n-  __ evpaddd(xmm(3), k0, xmm(11), xmm(15), false, Assembler::AVX_512bit);\n-\n-  __ evpsubd(xmm(8), k0, xmm(8), xmm(12), false, Assembler::AVX_512bit);\n-  __ evpsubd(xmm(9), k0, xmm(9), xmm(13), false, Assembler::AVX_512bit);\n-  __ evpsubd(xmm(10), k0, xmm(10), xmm(14), false, Assembler::AVX_512bit);\n-  __ evpsubd(xmm(11), k0, xmm(11), xmm(15), false, Assembler::AVX_512bit);\n-\n@@ -783,12 +626,2 @@\n-  montmulEven(20, 4, 8, 16, 20, 4);\n-\n-  __ vpshufd(xmm(8), xmm(8), 0xB1, Assembler::AVX_512bit);\n-  __ vpshufd(xmm(9), xmm(9), 0xB1, Assembler::AVX_512bit);\n-  __ vpshufd(xmm(10), xmm(10), 0xB1, Assembler::AVX_512bit);\n-  __ vpshufd(xmm(11), xmm(11), 0xB1, Assembler::AVX_512bit);\n-\n-  montmulEven(4, 4, 8, 16, 24, 4);\n-\n-  for (int i = 0; i < 4; i++) {\n-    __ evpermt2d(xmm(i + 4), xmm28, xmm(i + 20), Assembler::AVX_512bit);\n-  }\n+  sub_add(xmm24_27, xmm0_3, xmm8_11, xmm12_15, _masm);\n+  montMul64(xmm4_7, xmm24_27, xmm4_7, xmm16_27, _masm);\n@@ -810,11 +643,0 @@\n-\n-  __ evpaddd(xmm(0), k0, xmm(8), xmm(12), false, Assembler::AVX_512bit);\n-  __ evpaddd(xmm(1), k0, xmm(9), xmm(13), false, Assembler::AVX_512bit);\n-  __ evpaddd(xmm(2), k0, xmm(10), xmm(14), false, Assembler::AVX_512bit);\n-  __ evpaddd(xmm(3), k0, xmm(11), xmm(15), false, Assembler::AVX_512bit);\n-\n-  __ evpsubd(xmm(8), k0, xmm(8), xmm(12), false, Assembler::AVX_512bit);\n-  __ evpsubd(xmm(9), k0, xmm(9), xmm(13), false, Assembler::AVX_512bit);\n-  __ evpsubd(xmm(10), k0, xmm(10), xmm(14), false, Assembler::AVX_512bit);\n-  __ evpsubd(xmm(11), k0, xmm(11), xmm(15), false, Assembler::AVX_512bit);\n-\n@@ -826,12 +648,2 @@\n-  montmulEven(20, 4, 8, 16, 20, 4);\n-\n-  __ vpshufd(xmm(8), xmm(8), 0xB1, Assembler::AVX_512bit);\n-  __ vpshufd(xmm(9), xmm(9), 0xB1, Assembler::AVX_512bit);\n-  __ vpshufd(xmm(10), xmm(10), 0xB1, Assembler::AVX_512bit);\n-  __ vpshufd(xmm(11), xmm(11), 0xB1, Assembler::AVX_512bit);\n-\n-  montmulEven(4, 4, 8, 16, 24, 4);\n-\n-  for (int i = 0; i < 4; i++) {\n-    __ evpermt2d(xmm(i + 4), xmm28, xmm(i + 20), Assembler::AVX_512bit);\n-  }\n+  sub_add(xmm24_27, xmm0_3, xmm8_11, xmm12_15, _masm);\n+  montMul64(xmm4_7, xmm24_27, xmm4_7, xmm16_27, _masm);\n@@ -840,21 +652,4 @@\n-  __ evpsubd(xmm(8), k0, xmm(0), xmm(1), false, Assembler::AVX_512bit);\n-  __ evpsubd(xmm(9), k0, xmm(4), xmm(5), false, Assembler::AVX_512bit);\n-  __ evpsubd(xmm(10), k0, xmm(2), xmm(3), false, Assembler::AVX_512bit);\n-  __ evpsubd(xmm(11), k0, xmm(6), xmm(7), false, Assembler::AVX_512bit);\n-\n-  __ evpaddd(xmm(0), k0, xmm(0), xmm(1), false, Assembler::AVX_512bit);\n-  __ evpaddd(xmm(1), k0, xmm(4), xmm(5), false, Assembler::AVX_512bit);\n-  __ evpaddd(xmm(2), k0, xmm(2), xmm(3), false, Assembler::AVX_512bit);\n-  __ evpaddd(xmm(3), k0, xmm(6), xmm(7), false, Assembler::AVX_512bit);;\n-\n-  __ evmovdqul(xmm4, Address(zetas, 5 * 512), Assembler::AVX_512bit);\n-  __ evmovdqul(xmm5, Address(zetas, 5 * 512 + 64), Assembler::AVX_512bit);\n-  __ evmovdqul(xmm6, Address(zetas, 5 * 512 + 128), Assembler::AVX_512bit);\n-  __ evmovdqul(xmm7, Address(zetas, 5 * 512 + 192), Assembler::AVX_512bit);\n-\n-  montmulEven(20, 4, 8, 16, 20, 4);\n-\n-  __ vpshufd(xmm(8), xmm(8), 0xB1, Assembler::AVX_512bit);\n-  __ vpshufd(xmm(9), xmm(9), 0xB1, Assembler::AVX_512bit);\n-  __ vpshufd(xmm(10), xmm(10), 0xB1, Assembler::AVX_512bit);\n-  __ vpshufd(xmm(11), xmm(11), 0xB1, Assembler::AVX_512bit);\n+  __ evmovdqul(xmm12, Address(zetas, 5 * 512), Assembler::AVX_512bit);\n+  __ evmovdqul(xmm13, Address(zetas, 5 * 512 + 64), Assembler::AVX_512bit);\n+  __ evmovdqul(xmm14, Address(zetas, 5 * 512 + 128), Assembler::AVX_512bit);\n+  __ evmovdqul(xmm15, Address(zetas, 5 * 512 + 192), Assembler::AVX_512bit);\n@@ -862,5 +657,2 @@\n-  montmulEven(4, 4, 8, 16, 24, 4);\n-\n-  for (int i = 0; i < 4; i++) {\n-    __ evpermt2d(xmm(i + 4), xmm28, xmm(i + 20), Assembler::AVX_512bit);\n-  }\n+  sub_add(xmm8_11, xmm0_3, xmm0426, xmm1537, _masm);\n+  montMul64(xmm4_7, xmm8_11, xmm12_15, xmm16_27, _masm);\n@@ -869,21 +661,4 @@\n-  __ evpsubd(xmm(8), k0, xmm(0), xmm(2), false, Assembler::AVX_512bit);\n-  __ evpsubd(xmm(9), k0, xmm(1), xmm(3), false, Assembler::AVX_512bit);\n-  __ evpsubd(xmm(10), k0, xmm(4), xmm(6), false, Assembler::AVX_512bit);\n-  __ evpsubd(xmm(11), k0, xmm(5), xmm(7), false, Assembler::AVX_512bit);\n-\n-  __ evpaddd(xmm(0), k0, xmm(0), xmm(2), false, Assembler::AVX_512bit);\n-  __ evpaddd(xmm(2), k0, xmm(4), xmm(6), false, Assembler::AVX_512bit);\n-  __ evpaddd(xmm(1), k0, xmm(1), xmm(3), false, Assembler::AVX_512bit);\n-  __ evpaddd(xmm(3), k0, xmm(5), xmm(7), false, Assembler::AVX_512bit);\n-\n-  __ evmovdqul(xmm4, Address(zetas, 6 * 512), Assembler::AVX_512bit);\n-  __ evmovdqul(xmm5, Address(zetas, 6 * 512 + 64), Assembler::AVX_512bit);\n-  __ evmovdqul(xmm6, Address(zetas, 6 * 512 + 128), Assembler::AVX_512bit);\n-  __ evmovdqul(xmm7, Address(zetas, 6 * 512 + 192), Assembler::AVX_512bit);\n-\n-  montmulEven(20, 4, 8, 16, 20, 4);\n-\n-  __ vpshufd(xmm(8), xmm(8), 0xB1, Assembler::AVX_512bit);\n-  __ vpshufd(xmm(9), xmm(9), 0xB1, Assembler::AVX_512bit);\n-  __ vpshufd(xmm(10), xmm(10), 0xB1, Assembler::AVX_512bit);\n-  __ vpshufd(xmm(11), xmm(11), 0xB1, Assembler::AVX_512bit);\n+  __ evmovdqul(xmm12, Address(zetas, 6 * 512), Assembler::AVX_512bit);\n+  __ evmovdqul(xmm13, Address(zetas, 6 * 512 + 64), Assembler::AVX_512bit);\n+  __ evmovdqul(xmm14, Address(zetas, 6 * 512 + 128), Assembler::AVX_512bit);\n+  __ evmovdqul(xmm15, Address(zetas, 6 * 512 + 192), Assembler::AVX_512bit);\n@@ -891,1 +666,2 @@\n-  montmulEven(4, 4, 8, 16, 24, 4);\n+  sub_add(xmm8_11, xmm0_3, xmm0145, xmm2367, _masm);\n+  montMul64(xmm4_7, xmm8_11, xmm12_15, xmm16_27, _masm);\n@@ -917,1 +693,2 @@\n-  \/\/ level 6 into Zmm_8-Zmm_15 and do the last level entirely in the vector registers\n+  \/\/ level 6 into Zmm_8-Zmm_15 and do the last level entirely in the vector\n+  \/\/ registers\n@@ -923,3 +700,2 @@\n-  for (int i = 0; i < 8; i++) {\n-    __ evpsubd(xmm(i + 16), k0, xmm(i + 8), xmm(i), false, Assembler::AVX_512bit);\n-  }\n+\n+  __ evmovdqul(xmm29, Address(zetas, 7 * 512), Assembler::AVX_512bit);\n@@ -928,1 +704,1 @@\n-    __ evpaddd(xmm(i), k0, xmm(i), xmm(i + 8), false, Assembler::AVX_512bit);\n+    __ evpaddd(xmm(i + 16), k0, xmm(i), xmm(i + 8), false, Assembler::AVX_512bit);\n@@ -932,1 +708,1 @@\n-    __ evmovdqul(Address(coeffs, i * 64), xmm(i), Assembler::AVX_512bit);\n+    __ evpsubd(xmm(i), k0, xmm(i + 8), xmm(i), false, Assembler::AVX_512bit);\n@@ -935,13 +711,2 @@\n-  __ evmovdqul(xmm29, Address(zetas, 7 * 512), Assembler::AVX_512bit);\n-\n-  montmulEven(4, 16, 29, 8, 12, 4);\n-\n-  __ vpshufd(xmm(16), xmm(16), 0xB1, Assembler::AVX_512bit);\n-  __ vpshufd(xmm(17), xmm(17), 0xB1, Assembler::AVX_512bit);\n-  __ vpshufd(xmm(18), xmm(18), 0xB1, Assembler::AVX_512bit);\n-  __ vpshufd(xmm(19), xmm(19), 0xB1, Assembler::AVX_512bit);\n-\n-  montmulEven(0, 16, 29, 8, 12, 4);\n-\n-  for (int i = 0; i < 4; i++) {\n-    __ evpermt2d(xmm(i), xmm28, xmm(i + 4), Assembler::AVX_512bit);\n+  for (int i = 0; i < 8; i++) {\n+    __ evmovdqul(Address(coeffs, i * 64), xmm(i + 16), Assembler::AVX_512bit);\n@@ -949,0 +714,1 @@\n+  montMulByConst128(_masm);\n@@ -950,1 +716,1 @@\n-  for (int i = 0; i < 4; i++) {\n+  for (int i = 0; i < 8; i++) {\n@@ -954,17 +720,0 @@\n-  montmulEven(4, 20, 29, 8, 12, 4);\n-\n-  __ vpshufd(xmm(20), xmm(20), 0xB1, Assembler::AVX_512bit);\n-  __ vpshufd(xmm(21), xmm(21), 0xB1, Assembler::AVX_512bit);\n-  __ vpshufd(xmm(22), xmm(22), 0xB1, Assembler::AVX_512bit);\n-  __ vpshufd(xmm(23), xmm(23), 0xB1, Assembler::AVX_512bit);\n-\n-  montmulEven(0, 20, 29, 8, 12, 4);\n-\n-  for (int i = 0; i < 4; i++) {\n-    __ evpermt2d(xmm(i), xmm28, xmm(i + 4), Assembler::AVX_512bit);\n-  }\n-\n-  for (int i = 0; i < 4; i++) {\n-    __ evmovdqul(Address(coeffs, i * 64 + 768), xmm(i), Assembler::AVX_512bit);\n-  }\n-\n@@ -986,1 +735,2 @@\n-address StubGenerator::generate_dilithiumNttMult_avx512() {\n+static address generate_dilithiumNttMult_avx512(StubGenerator *stubgen,\n+                                                MacroAssembler *_masm) {\n@@ -990,1 +740,1 @@\n-  StubCodeMark mark(this, stub_id);\n+  StubCodeMark mark(stubgen, stub_id);\n@@ -1000,1 +750,1 @@\n-  const Register dilithiumConsts = c_rarg3;\n+  const Register dilithiumConsts = c_rarg3; \/\/ not used for argument\n@@ -1018,1 +768,0 @@\n-    __ evmovdqul(xmm(i), Address(poly1, i * 64), Assembler::AVX_512bit);\n@@ -1020,0 +769,1 @@\n+    __ evmovdqul(xmm(i), Address(poly1, i * 64), Assembler::AVX_512bit);\n@@ -1022,17 +772,4 @@\n-  montmulEven(8, 4, 29, 12, 16, 4);\n-  for (int i = 0; i < 4; i++) {\n-    __ vpshufd(xmm(i + 8), xmm(i + 8), 0xB1, Assembler::AVX_512bit);\n-  }\n-  montmulEven(8, 0, 8, 12, 16, 4);\n-  for (int i = 0; i < 4; i++) {\n-    __ vpshufd(xmm(i), xmm(i), 0xB1, Assembler::AVX_512bit);\n-    __ vpshufd(xmm(i + 4), xmm(i + 4), 0xB1, Assembler::AVX_512bit);\n-  }\n-  montmulEven(4, 4, 29, 12, 16, 4);\n-  for (int i = 0; i < 4; i++) {\n-    __ vpshufd(xmm(i + 4), xmm(i + 4), 0xB1, Assembler::AVX_512bit);\n-  }\n-  montmulEven(0, 0, 4, 12, 16, 4);\n-  for (int i = 0; i < 4; i++) {\n-    __ evpermt2d(xmm(i), xmm28, xmm(i + 8), Assembler::AVX_512bit);\n-  }\n+  montMul64(xmm4_7, xmm4_7, xmm29_29, xmm16_27, _masm);\n+\n+  montMul64(xmm0_3, xmm0_3, xmm4_7, xmm16_27, true, _masm);\n+\n@@ -1042,1 +779,0 @@\n-\n@@ -1050,1 +786,0 @@\n-\n@@ -1064,1 +799,2 @@\n-address StubGenerator::generate_dilithiumMontMulByConstant_avx512() {\n+static address generate_dilithiumMontMulByConstant_avx512(StubGenerator *stubgen,\n+                                                          MacroAssembler *_masm) {\n@@ -1068,1 +804,1 @@\n-  StubCodeMark mark(this, stub_id);\n+  StubCodeMark mark(stubgen, stub_id);\n@@ -1077,2 +813,2 @@\n-  const Register perms = c_rarg2;\n-  const Register dilithiumConsts = c_rarg3;\n+  const Register perms = c_rarg2; \/\/ not used for argument\n+  const Register dilithiumConsts = c_rarg3; \/\/ not used for argument\n@@ -1098,8 +834,3 @@\n-  montmulEven(8, 0, 29, 8, 16, 8);\n-  for (int i = 0; i < 8; i++) {\n-    __ vpshufd(xmm(i),xmm(i), 0xB1, Assembler::AVX_512bit);\n-  }\n-  montmulEven(0, 0, 29, 0, 16, 8);\n-  for (int i = 0; i < 8; i++) {\n-    __ evpermt2d(xmm(i), xmm28, xmm(i + 8), Assembler::AVX_512bit);\n-  }\n+\n+  montMulByConst128(_masm);\n+\n@@ -1131,1 +862,2 @@\n-address StubGenerator::generate_dilithiumDecomposePoly_avx512() {\n+static address generate_dilithiumDecomposePoly_avx512(StubGenerator *stubgen,\n+                                                      MacroAssembler *_masm) {\n@@ -1135,1 +867,1 @@\n-  StubCodeMark mark(this, stub_id);\n+  StubCodeMark mark(stubgen, stub_id);\n@@ -1146,3 +878,3 @@\n-  const Register len = c_rarg3;\n-  const Register dilithiumConsts = r9;\n-  const Register tmp = r10;\n+  const Register len = c_rarg3; \/\/ len is used only after twoGamma2 is consumed\n+  const Register dilithiumConsts = r10;\n+  const Register tmp = r11;\n@@ -1186,1 +918,1 @@\n-  \/\/          rplus = rplus - ((rplus + 5373807) >> 23) * dilithium_q;\n+  \/\/ rplus = rplus - ((rplus + 5373807) >> 23) * dilithium_q;\n@@ -1207,3 +939,2 @@\n-            \/\/ rplus in xmm0\n-\n-\/\/            rplus = rplus + ((rplus >> 31) & dilithium_q);\n+  \/\/ rplus in xmm0\n+  \/\/ rplus = rplus + ((rplus >> 31) & dilithium_q);\n@@ -1225,3 +956,2 @@\n-            \/\/ rplus in xmm0\n-\n-\/\/           int quotient = (rplus * multiplier) >> 22;\n+  \/\/ rplus in xmm0\n+  \/\/ int quotient = (rplus * multiplier) >> 22;\n@@ -1238,3 +968,2 @@\n-            \/\/ quotient in xmm4\n-\n-\/\/            int r0 = rplus - quotient * twoGamma2;\n+  \/\/ quotient in xmm4\n+  \/\/ int r0 = rplus - quotient * twoGamma2;\n@@ -1251,3 +980,2 @@\n-            \/\/ r0 in xmm8\n-\n-\/\/            int mask = (twoGamma2 - r0) >> 22;\n+  \/\/ r0 in xmm8\n+  \/\/ int mask = (twoGamma2 - r0) >> 22;\n@@ -1264,3 +992,2 @@\n-            \/\/ mask in xmm12\n-\n-\/\/            r0 -= (mask & twoGamma2);\n+  \/\/ mask in xmm12\n+  \/\/ r0 -= (mask & twoGamma2);\n@@ -1277,3 +1004,2 @@\n-            \/\/ r0 in xmm8\n-\n-\/\/            quotient += (mask & 1);\n+  \/\/ r0 in xmm8\n+  \/\/ quotient += (mask & 1);\n@@ -1290,1 +1016,1 @@\n-\/\/            mask = (twoGamma2 \/ 2 - r0) >> 31;\n+  \/\/ mask = (twoGamma2 \/ 2 - r0) >> 31;\n@@ -1301,1 +1027,1 @@\n-\/\/            r0 -= (mask & twoGamma2);\n+  \/\/ r0 -= (mask & twoGamma2);\n@@ -1312,3 +1038,2 @@\n-            \/\/ r0 in xmm8\n-\n-\/\/            quotient += (mask & 1);\n+  \/\/ r0 in xmm8\n+  \/\/ quotient += (mask & 1);\n@@ -1325,3 +1050,2 @@\n-            \/\/ quotient in xmm4\n-\n-\/\/            int r1 = rplus - r0 - (dilithium_q - 1);\n+  \/\/ quotient in xmm4\n+  \/\/ int r1 = rplus - r0 - (dilithium_q - 1);\n@@ -1338,3 +1062,2 @@\n-            \/\/ r1 in xmm16\n-\n-\/\/            r1 = (r1 | (-r1)) >> 31; \/\/ 0 if rplus - r0 == (dilithium_q - 1), -1 otherwise\n+  \/\/ r1 in xmm16\n+  \/\/ r1 = (r1 | (-r1)) >> 31; \/\/ 0 if rplus - r0 == (dilithium_q - 1), -1 otherwise\n@@ -1358,3 +1081,2 @@\n-            \/\/ r1 in xmm0\n-\n-\/\/            r0 += ~r1;\n+  \/\/ r1 in xmm0\n+  \/\/ r0 += ~r1;\n@@ -1371,3 +1093,2 @@\n-            \/\/ r0 in xmm8\n-\n-\/\/            r1 = r1 & quotient;\n+  \/\/ r0 in xmm8\n+  \/\/ r1 = r1 & quotient;\n@@ -1379,4 +1100,3 @@\n-\/\/             r1 in xmm0\n-\n-\/\/            lowPart[m] = r0;\n-\/\/            highPart[m] = r1;\n+  \/\/ r1 in xmm0\n+  \/\/ lowPart[m] = r0;\n+  \/\/ highPart[m] = r1;\n@@ -1405,0 +1125,16 @@\n+\n+void StubGenerator::generate_dilithium_stubs() {\n+  \/\/ Generate Dilithium intrinsics code\n+  if (UseDilithiumIntrinsics) {\n+      StubRoutines::_dilithiumAlmostNtt =\n+        generate_dilithiumAlmostNtt_avx512(this, _masm);\n+      StubRoutines::_dilithiumAlmostInverseNtt =\n+        generate_dilithiumAlmostInverseNtt_avx512(this, _masm);\n+      StubRoutines::_dilithiumNttMult =\n+        generate_dilithiumNttMult_avx512(this, _masm);\n+      StubRoutines::_dilithiumMontMulByConstant =\n+        generate_dilithiumMontMulByConstant_avx512(this, _masm);\n+      StubRoutines::_dilithiumDecomposePoly =\n+        generate_dilithiumDecomposePoly_avx512(this, _masm);\n+  }\n+}\n","filename":"src\/hotspot\/cpu\/x86\/stubGenerator_x86_64_dilithium.cpp","additions":286,"deletions":550,"binary":false,"changes":836,"status":"modified"},{"patch":"@@ -82,8 +82,0 @@\n-void StubGenerator::generate_sha3_stubs() {\n-  if (UseSHA3Intrinsics) {\n-    StubRoutines::_sha3_implCompress   = generate_sha3_implCompress(StubGenStubId::sha3_implCompress_id);\n-    StubRoutines::_double_keccak         = generate_double_keccak();\n-    StubRoutines::_sha3_implCompressMB = generate_sha3_implCompress(StubGenStubId::sha3_implCompressMB_id);\n-  }\n-}\n-\n@@ -99,1 +91,3 @@\n-address StubGenerator::generate_sha3_implCompress(StubGenStubId stub_id) {\n+static address generate_sha3_implCompress(StubGenStubId stub_id,\n+                                          StubGenerator *stubgen,\n+                                          MacroAssembler *_masm) {\n@@ -113,1 +107,1 @@\n-  StubCodeMark mark(this, stub_id);\n+  StubCodeMark mark(stubgen, stub_id);\n@@ -348,1 +342,1 @@\n-address StubGenerator::generate_double_keccak() {\n+static address generate_double_keccak(StubGenerator *stubgen, MacroAssembler *_masm) {\n@@ -351,1 +345,1 @@\n-  StubCodeMark mark(this, stub_id);\n+  StubCodeMark mark(stubgen, stub_id);\n@@ -545,0 +539,11 @@\n+\n+void StubGenerator::generate_sha3_stubs() {\n+  if (UseSHA3Intrinsics) {\n+    StubRoutines::_sha3_implCompress =\n+      generate_sha3_implCompress(StubGenStubId::sha3_implCompress_id, this, _masm);\n+    StubRoutines::_double_keccak =\n+      generate_double_keccak(this, _masm);\n+    StubRoutines::_sha3_implCompressMB =\n+      generate_sha3_implCompress(StubGenStubId::sha3_implCompressMB_id, this, _masm);\n+  }\n+}\n","filename":"src\/hotspot\/cpu\/x86\/stubGenerator_x86_64_sha3.cpp","additions":17,"deletions":12,"binary":false,"changes":29,"status":"modified"}]}