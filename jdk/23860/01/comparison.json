{"files":[{"patch":"@@ -88,1 +88,1 @@\n-  do_arch_blob(compiler, 20000 LP64_ONLY(+64000) WINDOWS_ONLY(+2000))   \\\n+  do_arch_blob(compiler, 20000 LP64_ONLY(+89000) WINDOWS_ONLY(+2000))   \\\n","filename":"src\/hotspot\/cpu\/x86\/stubDeclarations_x86.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -4210,0 +4210,2 @@\n+  generate_dilithium_stubs();\n+\n","filename":"src\/hotspot\/cpu\/x86\/stubGenerator_x86_64.cpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -494,0 +494,15 @@\n+  address generate_double_keccak();\n+\n+  \/\/ Dilithium stubs and helper functions\n+  void montmulEven(int outputReg, int inputReg1,  int inputReg2,\n+    int scratchReg1, int scratchReg2, int parCnt);\n+  void montmulEven(int outputReg, int inputReg11, int inputReg12,\n+    int inputReg13, int inputReg14,  int inputReg2,\n+    int scratchReg1, int scratchReg2);\n+  void generate_dilithium_stubs();\n+  address generate_dilithiumAlmostNtt_avx512();\n+  address generate_dilithiumAlmostInverseNtt_avx512();\n+  address generate_dilithiumNttMult_avx512();\n+  address generate_dilithiumMontMulByConstant_avx512();\n+  address generate_dilithiumDecomposePoly_avx512();\n+\n","filename":"src\/hotspot\/cpu\/x86\/stubGenerator_x86_64.hpp","additions":15,"deletions":0,"binary":false,"changes":15,"status":"modified"},{"patch":"@@ -0,0 +1,1399 @@\n+\/*\n+ * Copyright (c) 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#include \"asm\/assembler.hpp\"\n+#include \"asm\/assembler.inline.hpp\"\n+#include \"runtime\/stubRoutines.hpp\"\n+#include \"macroAssembler_x86.hpp\"\n+#include \"stubGenerator_x86_64.hpp\"\n+\n+#define __ _masm->\n+\n+#define xmm(i) as_XMMRegister(i)\n+\n+#ifdef PRODUCT\n+#define BLOCK_COMMENT(str) \/* nothing *\/\n+#else\n+#define BLOCK_COMMENT(str) __ block_comment(str)\n+#endif \/\/ PRODUCT\n+\n+#define BIND(label) bind(label); BLOCK_COMMENT(#label \":\")\n+\n+\/\/ Constants\n+\/\/\n+ATTRIBUTE_ALIGNED(64) static const uint32_t dilithiumAvx512Consts[] = {\n+    58728449, \/\/ montQInvModR\n+    8380417, \/\/ dilithium_q\n+    16382, \/\/ toMont((dilithium_n)^-1 (mod dilithium_q))\n+    2365951, \/\/ montRSquareModQ\n+    5373807 \/\/ addend for modular reduce\n+};\n+\n+static address dilithiumAvx512ConstsAddr() {\n+  return (address) dilithiumAvx512Consts;\n+}\n+\n+ATTRIBUTE_ALIGNED(64) static const uint32_t dilithiumAvx512Perms[] = {\n+     \/\/ collect montmul results into the destination register\n+    17, 1, 19, 3, 21, 5, 23, 7, 25, 9, 27, 11, 29, 13, 31, 15,\n+    \/\/ ntt\n+    0, 1, 2, 3, 4, 5, 6, 7, 16, 17, 18, 19, 20, 21, 22, 23,\n+    8, 9, 10, 11, 12, 13, 14, 15, 24, 25, 26, 27, 28, 29, 30, 31,\n+    0, 1, 2, 3, 16, 17, 18, 19, 8, 9, 10, 11, 24, 25, 26, 27,\n+    4, 5, 6, 7, 20, 21, 22, 23, 12, 13, 14, 15, 28, 29, 30, 31,\n+    0, 1, 16, 17, 4, 5, 20, 21, 8, 9, 24, 25, 12, 13, 28, 29,\n+    2, 3, 18, 19, 6, 7, 22, 23, 10, 11, 26, 27, 14, 15, 30, 31,\n+    0, 16, 2, 18, 4, 20, 6, 22, 8, 24, 10, 26, 12, 28, 14, 30,\n+    1, 17, 3, 19, 5, 21, 7, 23, 9, 25, 11, 27, 13, 29, 15, 31,\n+    0, 16, 1, 17, 2, 18, 3, 19, 4, 20, 5, 21, 6, 22, 7, 23,\n+    8, 24, 9, 25, 10, 26, 11, 27, 12, 28, 13, 29, 14, 30, 15, 31,\n+    \/\/ ntt inverse\n+    0, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30,\n+    1, 3, 5, 7, 9, 11, 13, 15, 17, 19, 21, 23, 25, 27, 29, 31,\n+    0, 16, 2, 18, 4, 20, 6, 22, 8, 24, 10, 26, 12, 28, 14, 30,\n+    1, 17, 3, 19, 5, 21, 7, 23, 9, 25, 11, 27, 13, 29, 15, 31,\n+    0, 1, 16, 17, 4, 5, 20, 21, 8, 9, 24, 25, 12, 13, 28, 29,\n+    2, 3, 18, 19, 6, 7, 22, 23, 10, 11, 26, 27, 14, 15, 30, 31,\n+    0, 1, 2, 3, 16, 17, 18, 19, 8, 9, 10, 11, 24, 25, 26, 27,\n+    4, 5, 6, 7, 20, 21, 22, 23, 12, 13, 14, 15, 28, 29, 30, 31,\n+    0, 1, 2, 3, 4, 5, 6, 7, 16, 17, 18, 19, 20, 21, 22, 23,\n+    8, 9, 10, 11, 12, 13, 14, 15, 24, 25, 26, 27, 28, 29, 30, 31\n+};\n+\n+static address dilithiumAvx512PermsAddr() {\n+  return (address) dilithiumAvx512Perms;\n+}\n+\n+void StubGenerator::generate_dilithium_stubs() {\n+  \/\/ Generate Dilithium intrinsics code\n+  if (UseDilithiumIntrinsics) {\n+      StubRoutines::_dilithiumAlmostNtt = generate_dilithiumAlmostNtt_avx512();\n+      StubRoutines::_dilithiumAlmostInverseNtt = generate_dilithiumAlmostInverseNtt_avx512();\n+      StubRoutines::_dilithiumNttMult = generate_dilithiumNttMult_avx512();\n+      StubRoutines::_dilithiumMontMulByConstant = generate_dilithiumMontMulByConstant_avx512();\n+      StubRoutines::_dilithiumDecomposePoly = generate_dilithiumDecomposePoly_avx512();\n+    }\n+}\n+\n+\/\/ We do Montgomery multiplications of two vectors of 16 ints each in 4 steps:\n+\/\/ 1. Do the multiplications of the corresponding even numbered slots into\n+\/\/    the odd numbered slots of a third register using montmulEven().\n+\/\/ 2. Swap the even and odd numbered slots of the original input registers.\n+\/\/ 3. Similar to step 1, but into a different output register.\n+\/\/ 4. Combine the outputs of step 1 and step 3 into the output of the Montgomery\n+\/\/    multiplication.\n+\/\/ (For levels 0-6 in the Ntt and levels 1-7 of the inverse Ntt we only swap the\n+\/\/ odd-even slots of the first multiplicand as in the second (zetas) the\n+\/\/ odd slots contain the same number as the corresponding even one.)\n+\n+\/\/ Montgomery multiplication of the *even* numbered slices of parCnt consecutive register pairs\n+\/\/ Zmm_inputReg1 to Zmm_(inputReg1+parCnt-1) and Zmm_inputReg2 to Zmm_(inputReg2+parCnt-1).\n+\/\/ The result goes to the *odd* numbered slices of Zmm_outputReg to Zmm_(outputReg1+parCnt-1).\n+\/\/ Zmm_31 should contain q and Zmm_30 should contain q^-1 mod 2^32 in all of their slices.\n+void StubGenerator::montmulEven(int outputReg, int inputReg1,  int inputReg2, int scratchReg1, int scratchReg2, int parCnt) {\n+\n+  for (int i = 0; i < parCnt; i++) {\n+    __ vpmuldq(xmm(i + scratchReg1), xmm(i + inputReg1), xmm((inputReg2 == 29) ? 29 : inputReg2 + i), Assembler::AVX_512bit);\n+  }\n+  for (int i = 0; i < parCnt; i++) {\n+    __ vpmulld(xmm(i + scratchReg2), xmm(i + scratchReg1), xmm30, Assembler::AVX_512bit);\n+  }\n+  for (int i = 0; i < parCnt; i++) {\n+    __ vpmuldq(xmm(i + scratchReg2), xmm(i + scratchReg2), xmm31, Assembler::AVX_512bit);\n+  }\n+  for (int i = 0; i < parCnt; i++) {\n+    __ evpsubd(xmm(i + outputReg), k0, xmm(i + scratchReg1), xmm(i + scratchReg2), false, Assembler::AVX_512bit);\n+  }\n+}\n+\n+\/\/ Similar to the 6-parameter montmulEven(), the difference is that here the input regs for the first\n+\/\/ arguments do not have to be consecutive and that parcnt is always 4, so it is not passed in.\n+void StubGenerator::montmulEven(int outputReg, int inputReg11, int inputReg12, int inputReg13, int inputReg14,\n+                                int inputReg2, int scratchReg1, int scratchReg2) {\n+\n+  int parCnt = 4;\n+\n+  __ vpmuldq(xmm(scratchReg1), xmm(inputReg11), xmm(inputReg2), Assembler::AVX_512bit);\n+  __ vpmuldq(xmm(scratchReg1 + 1), xmm(inputReg12), xmm(inputReg2 + 1), Assembler::AVX_512bit);\n+  __ vpmuldq(xmm(scratchReg1 + 2), xmm(inputReg13), xmm(inputReg2 + 2), Assembler::AVX_512bit);\n+  __ vpmuldq(xmm(scratchReg1 + 3), xmm(inputReg14), xmm(inputReg2 + 3), Assembler::AVX_512bit);\n+\n+  for (int i = 0; i < parCnt; i++) {\n+    __ vpmulld(xmm(i + scratchReg2), xmm(i + scratchReg1), xmm30, Assembler::AVX_512bit);\n+  }\n+  for (int i = 0; i < parCnt; i++) {\n+    __ vpmuldq(xmm(i + scratchReg2), xmm(i + scratchReg2), xmm31, Assembler::AVX_512bit);\n+  }\n+  for (int i = 0; i < parCnt; i++) {\n+    __ evpsubd(xmm(i + outputReg), k0, xmm(i + scratchReg1), xmm(i + scratchReg2), false, Assembler::AVX_512bit);\n+  }\n+}\n+\n+\/\/ Dilithium NTT function except for the final \"normalization\" to |coeff| < Q.\n+\/\/ Implements\n+\/\/ static int implDilithiumAlmostNtt(int[] coeffs, int zetas[]) {}\n+\/\/\n+\/\/ coeffs (int[256]) = c_rarg0\n+\/\/ zetas (int[256]) = c_rarg1\n+\/\/\n+\/\/\n+address StubGenerator::generate_dilithiumAlmostNtt_avx512() {\n+\n+  __ align(CodeEntryAlignment);\n+  StubGenStubId stub_id = dilithiumAlmostNtt_id;\n+  StubCodeMark mark(this, stub_id);\n+  address start = __ pc();\n+  __ enter();\n+\n+  Label L_loop, L_end;\n+\n+  const Register coeffs = c_rarg0;\n+  const Register zetas = c_rarg1;\n+\n+  const Register iterations = c_rarg2;\n+\n+  const Register dilithiumConsts = r10;\n+  const Register perms = r11;\n+\n+  __ lea(perms, ExternalAddress(dilithiumAvx512PermsAddr()));\n+  __ lea(dilithiumConsts, ExternalAddress(dilithiumAvx512ConstsAddr()));\n+\n+  __ evmovdqul(xmm28, Address(perms, 0), Assembler::AVX_512bit);\n+  __ evmovdqul(xmm29, Address(zetas, 0), Assembler::AVX_512bit);\n+  __ vpbroadcastd(xmm30, Address(dilithiumConsts, 0), Assembler::AVX_512bit); \/\/ q^-1 mod 2^32\n+  __ vpbroadcastd(xmm31, Address(dilithiumConsts, 4), Assembler::AVX_512bit); \/\/ q\n+\n+  \/\/ load all coefficients into the vector registers Zmm_0-Zmm_15,\n+  \/\/ 16 coefficients into each\n+  for (int i = 0; i < 16; i++) {\n+    __ evmovdqul(xmm(i), Address(coeffs, i * 64), Assembler::AVX_512bit);\n+  }\n+\n+  \/\/ level 0 and 1 can be done entirely in registers as the zetas on these\n+  \/\/ levels are the same for all the montmuls that we can do in parallel\n+\n+  \/\/ level 0\n+  montmulEven(20, 8, 29, 20, 16, 4);\n+\n+  for (int i = 0; i < 4; i++) {\n+    __ vpshufd(xmm(i + 24), xmm(i + 8), 0xB1, Assembler::AVX_512bit);\n+  }\n+\n+  montmulEven(16, 24, 29, 24, 16, 4);\n+\n+  for (int i = 0; i < 4; i++) {\n+    __ evpermt2d(xmm(i + 16), xmm28, xmm(i + 20), Assembler::AVX_512bit);\n+  }\n+\n+  for (int i = 0; i < 4; i++) {\n+    __ evpsubd(xmm(i + 8), k0, xmm(i), xmm(i + 16), false, Assembler::AVX_512bit);\n+  }\n+\n+  for (int i = 0; i < 4; i++) {\n+    __ evpaddd(xmm(i), k0, xmm(i), xmm(i + 16), false, Assembler::AVX_512bit);\n+  }\n+\n+  montmulEven(20, 12, 29, 20, 16, 4);\n+\n+  for (int i = 0; i < 4; i++) {\n+    __ vpshufd(xmm(i + 24), xmm(i + 12), 0xB1, Assembler::AVX_512bit);\n+  }\n+\n+  montmulEven(16, 24, 29, 24, 16, 4);\n+\n+  for (int i = 0; i < 4; i++) {\n+    __ evpermt2d(xmm(i + 16), xmm28, xmm(i + 20), Assembler::AVX_512bit);\n+  }\n+\n+  for (int i = 0; i < 4; i++) {\n+    __ evpsubd(xmm(i + 12), k0, xmm(i + 4), xmm(i + 16), false, Assembler::AVX_512bit);\n+  }\n+\n+  for (int i = 0; i < 4; i++) {\n+    __ evpaddd(xmm(i + 4), k0, xmm(i + 4), xmm(i + 16), false, Assembler::AVX_512bit);\n+  }\n+\n+  \/\/ level 1\n+  __ evmovdqul(xmm29, Address(zetas, 512), Assembler::AVX_512bit);\n+\n+  montmulEven(20, 4, 29, 20, 16, 4);\n+  for (int i = 0; i < 4; i++) {\n+    __ vpshufd(xmm(i + 24), xmm(i + 4), 0xB1, Assembler::AVX_512bit);\n+  }\n+\n+  montmulEven(16, 24, 29, 24, 16, 4);\n+\n+  for (int i = 0; i < 4; i++) {\n+    __ evpermt2d(xmm(i + 16), xmm28, xmm(i + 20), Assembler::AVX_512bit);\n+  }\n+\n+  for (int i = 0; i < 4; i++) {\n+    __ evpsubd(xmm(i + 4), k0, xmm(i), xmm(i + 16), false, Assembler::AVX_512bit);\n+  }\n+\n+  for (int i = 0; i < 4; i++) {\n+    __ evpaddd(xmm(i), k0, xmm(i), xmm(i + 16), false, Assembler::AVX_512bit);\n+  }\n+\n+  __ evmovdqul(xmm29, Address(zetas, 768), Assembler::AVX_512bit);\n+\n+  montmulEven(20, 12, 29, 20, 16, 4);\n+  for (int i = 0; i < 4; i++) {\n+    __ vpshufd(xmm(i + 24), xmm(i + 12), 0xB1, Assembler::AVX_512bit);\n+  }\n+\n+  montmulEven(16, 24, 29, 24, 16, 4);\n+\n+  for (int i = 0; i < 4; i++) {\n+    __ evpermt2d(xmm(i + 16), xmm28, xmm(i + 20), Assembler::AVX_512bit);\n+  }\n+\n+  for (int i = 0; i < 4; i++) {\n+    __ evpsubd(xmm(i + 12), k0, xmm(i + 8), xmm(i + 16), false, Assembler::AVX_512bit);\n+  }\n+\n+  for (int i = 0; i < 4; i++) {\n+    __ evpaddd(xmm(i + 8), k0, xmm(i + 8), xmm(i + 16), false, Assembler::AVX_512bit);\n+  }\n+\n+  \/\/ levels 2 to 7 are done in 2 batches, by first saving half of the coefficients\n+  \/\/ from level 1 into memory, doing all the level 2 to level 7 computations\n+  \/\/ on the remaining half in the vector registers, saving the result to\n+  \/\/ memory after level 7, then loading back the coefficients that we saved after\n+  \/\/ level 1 and do the same computation with those\n+\n+  for (int i = 0; i < 16; i++) {\n+    __ evmovdqul(Address(coeffs, i * 64), xmm(i), Assembler::AVX_512bit);\n+  }\n+\n+  __ movl(iterations, 2);\n+\n+  __ BIND(L_loop);\n+\n+  __ subl(iterations, 1);\n+\n+  \/\/ level 2\n+  __ evmovdqul(xmm12, Address(zetas, 1024), Assembler::AVX_512bit);\n+  __ evmovdqul(xmm13, Address(zetas, 1088), Assembler::AVX_512bit);\n+  __ evmovdqul(xmm14, Address(zetas, 1152), Assembler::AVX_512bit);\n+  __ evmovdqul(xmm15, Address(zetas, 1216), Assembler::AVX_512bit);\n+\n+  montmulEven(20, 2, 3, 6, 7, 12, 20, 16);\n+\n+  __ vpshufd(xmm(8), xmm(2), 0xB1, Assembler::AVX_512bit);\n+  __ vpshufd(xmm(9), xmm(3), 0xB1, Assembler::AVX_512bit);\n+  __ vpshufd(xmm(10), xmm(6), 0xB1, Assembler::AVX_512bit);\n+  __ vpshufd(xmm(11), xmm(7), 0xB1, Assembler::AVX_512bit);\n+\n+  montmulEven(16, 8, 12, 24, 16, 4);\n+\n+  for (int i = 0; i < 4; i++) {\n+    __ evpermt2d(xmm(i + 16), xmm28, xmm(i + 20), Assembler::AVX_512bit);\n+  }\n+\n+  __ evpsubd(xmm(2), k0, xmm(0), xmm(16), false, Assembler::AVX_512bit);\n+  __ evpsubd(xmm(3), k0, xmm(1), xmm(17), false, Assembler::AVX_512bit);\n+  __ evpsubd(xmm(6), k0, xmm(4), xmm(18), false, Assembler::AVX_512bit);\n+  __ evpsubd(xmm(7), k0, xmm(5), xmm(19), false, Assembler::AVX_512bit);\n+\n+  __ evpaddd(xmm(0), k0, xmm(0), xmm(16), false, Assembler::AVX_512bit);\n+  __ evpaddd(xmm(1), k0, xmm(1), xmm(17), false, Assembler::AVX_512bit);\n+  __ evpaddd(xmm(4), k0, xmm(4), xmm(18), false, Assembler::AVX_512bit);\n+  __ evpaddd(xmm(5), k0, xmm(5), xmm(19), false, Assembler::AVX_512bit);\n+\n+  \/\/ level 3\n+  __ evmovdqul(xmm12, Address(zetas, 1536), Assembler::AVX_512bit);\n+  __ evmovdqul(xmm13, Address(zetas, 1600), Assembler::AVX_512bit);\n+  __ evmovdqul(xmm14, Address(zetas, 1664), Assembler::AVX_512bit);\n+  __ evmovdqul(xmm15, Address(zetas, 1728), Assembler::AVX_512bit);\n+\n+  montmulEven(20, 1, 3, 5, 7, 12, 20, 16);\n+\n+  __ vpshufd(xmm(8), xmm(1), 0xB1, Assembler::AVX_512bit);\n+  __ vpshufd(xmm(9), xmm(3), 0xB1, Assembler::AVX_512bit);\n+  __ vpshufd(xmm(10), xmm(5), 0xB1, Assembler::AVX_512bit);\n+  __ vpshufd(xmm(11), xmm(7), 0xB1, Assembler::AVX_512bit);\n+\n+  montmulEven(16, 8, 12, 24, 16, 4);\n+\n+  for (int i = 0; i < 4; i++) {\n+    __ evpermt2d(xmm(i + 16), xmm28, xmm(i + 20), Assembler::AVX_512bit);\n+  }\n+\n+  __ evpsubd(xmm(1), k0, xmm(0), xmm(16), false, Assembler::AVX_512bit);\n+  __ evpsubd(xmm(3), k0, xmm(2), xmm(17), false, Assembler::AVX_512bit);\n+  __ evpsubd(xmm(5), k0, xmm(4), xmm(18), false, Assembler::AVX_512bit);\n+  __ evpsubd(xmm(7), k0, xmm(6), xmm(19), false, Assembler::AVX_512bit);\n+\n+  __ evpaddd(xmm(0), k0, xmm(0), xmm(16), false, Assembler::AVX_512bit);\n+  __ evpaddd(xmm(2), k0, xmm(2), xmm(17), false, Assembler::AVX_512bit);\n+  __ evpaddd(xmm(4), k0, xmm(4), xmm(18), false, Assembler::AVX_512bit);\n+  __ evpaddd(xmm(6), k0, xmm(6), xmm(19), false, Assembler::AVX_512bit);\n+\n+  \/\/ level 4\n+  __ evmovdqul(xmm16, Address(perms, 64), Assembler::AVX_512bit);\n+  __ evmovdqul(xmm17, xmm16, Assembler::AVX_512bit);\n+  __ evmovdqul(xmm18, xmm16, Assembler::AVX_512bit);\n+  __ evmovdqul(xmm19, xmm16, Assembler::AVX_512bit);\n+  __ evmovdqul(xmm12, Address(perms, 128), Assembler::AVX_512bit);\n+  __ evmovdqul(xmm13, xmm12, Assembler::AVX_512bit);\n+  __ evmovdqul(xmm14, xmm12, Assembler::AVX_512bit);\n+  __ evmovdqul(xmm15, xmm12, Assembler::AVX_512bit);\n+\n+  for (int i = 0; i < 8; i += 2) {\n+    __ evpermi2d(xmm(i\/2 + 16), xmm(i), xmm(i + 1), Assembler::AVX_512bit);\n+  }\n+  for (int i = 0; i < 8; i += 2) {\n+    __ evpermi2d(xmm(i \/ 2 + 12), xmm(i), xmm(i + 1), Assembler::AVX_512bit);\n+  }\n+  __ evmovdqul(xmm0, Address(zetas, 4 * 512), Assembler::AVX_512bit);\n+  __ evmovdqul(xmm1, Address(zetas, 4 * 512 + 64), Assembler::AVX_512bit);\n+  __ evmovdqul(xmm2, Address(zetas, 4 * 512 + 128), Assembler::AVX_512bit);\n+  __ evmovdqul(xmm3, Address(zetas, 4 * 512 + 192), Assembler::AVX_512bit);\n+\n+  montmulEven(20, 0, 12, 4, 20, 4);\n+\n+  __ vpshufd(xmm(12), xmm(12), 0xB1, Assembler::AVX_512bit);\n+  __ vpshufd(xmm(13), xmm(13), 0xB1, Assembler::AVX_512bit);\n+  __ vpshufd(xmm(14), xmm(14), 0xB1, Assembler::AVX_512bit);\n+  __ vpshufd(xmm(15), xmm(15), 0xB1, Assembler::AVX_512bit);\n+\n+  montmulEven(12, 0, 12, 4, 24, 4);\n+\n+  for (int i = 0; i < 4; i++) {\n+    __ evpermt2d(xmm(i + 12), xmm28, xmm(i + 20), Assembler::AVX_512bit);\n+  }\n+\n+  __ evpsubd(xmm(1), k0, xmm(16), xmm(12), false, Assembler::AVX_512bit);\n+  __ evpsubd(xmm(3), k0, xmm(17), xmm(13), false, Assembler::AVX_512bit);\n+  __ evpsubd(xmm(5), k0, xmm(18), xmm(14), false, Assembler::AVX_512bit);\n+  __ evpsubd(xmm(7), k0, xmm(19), xmm(15), false, Assembler::AVX_512bit);\n+\n+  __ evpaddd(xmm(0), k0, xmm(16), xmm(12), false, Assembler::AVX_512bit);\n+  __ evpaddd(xmm(2), k0, xmm(17), xmm(13), false, Assembler::AVX_512bit);\n+  __ evpaddd(xmm(4), k0, xmm(18), xmm(14), false, Assembler::AVX_512bit);\n+  __ evpaddd(xmm(6), k0, xmm(19), xmm(15), false, Assembler::AVX_512bit);\n+\n+  \/\/ level 5\n+  __ evmovdqul(xmm16, Address(perms, 192), Assembler::AVX_512bit);\n+  __ evmovdqul(xmm17, xmm16, Assembler::AVX_512bit);\n+  __ evmovdqul(xmm18, xmm16, Assembler::AVX_512bit);\n+  __ evmovdqul(xmm19, xmm16, Assembler::AVX_512bit);\n+  __ evmovdqul(xmm12, Address(perms, 256), Assembler::AVX_512bit);\n+  __ evmovdqul(xmm13, xmm12, Assembler::AVX_512bit);\n+  __ evmovdqul(xmm14, xmm12, Assembler::AVX_512bit);\n+  __ evmovdqul(xmm15, xmm12, Assembler::AVX_512bit);\n+\n+  for (int i = 0; i < 8; i += 2) {\n+    __ evpermi2d(xmm(i\/2 + 16), xmm(i), xmm(i + 1), Assembler::AVX_512bit);\n+  }\n+  for (int i = 0; i < 8; i += 2) {\n+    __ evpermi2d(xmm(i \/ 2 + 12), xmm(i), xmm(i + 1), Assembler::AVX_512bit);\n+  }\n+  __ evmovdqul(xmm0, Address(zetas, 5 * 512), Assembler::AVX_512bit);\n+  __ evmovdqul(xmm1, Address(zetas, 5 * 512 + 64), Assembler::AVX_512bit);\n+  __ evmovdqul(xmm2, Address(zetas, 5 * 512 + 128), Assembler::AVX_512bit);\n+  __ evmovdqul(xmm3, Address(zetas, 5 * 512 + 192), Assembler::AVX_512bit);\n+\n+  montmulEven(20, 0, 12, 4, 20, 4);\n+\n+  __ vpshufd(xmm(12), xmm(12), 0xB1, Assembler::AVX_512bit);\n+  __ vpshufd(xmm(13), xmm(13), 0xB1, Assembler::AVX_512bit);\n+  __ vpshufd(xmm(14), xmm(14), 0xB1, Assembler::AVX_512bit);\n+  __ vpshufd(xmm(15), xmm(15), 0xB1, Assembler::AVX_512bit);\n+\n+  montmulEven(12, 0, 12, 4, 24, 4);\n+\n+  for (int i = 0; i < 4; i++) {\n+    __ evpermt2d(xmm(i + 12), xmm28, xmm(i + 20), Assembler::AVX_512bit);\n+  }\n+\n+  __ evpsubd(xmm(1), k0, xmm(16), xmm(12), false, Assembler::AVX_512bit);\n+  __ evpsubd(xmm(3), k0, xmm(17), xmm(13), false, Assembler::AVX_512bit);\n+  __ evpsubd(xmm(5), k0, xmm(18), xmm(14), false, Assembler::AVX_512bit);\n+  __ evpsubd(xmm(7), k0, xmm(19), xmm(15), false, Assembler::AVX_512bit);\n+\n+  __ evpaddd(xmm(0), k0, xmm(16), xmm(12), false, Assembler::AVX_512bit);\n+  __ evpaddd(xmm(2), k0, xmm(17), xmm(13), false, Assembler::AVX_512bit);\n+  __ evpaddd(xmm(4), k0, xmm(18), xmm(14), false, Assembler::AVX_512bit);\n+  __ evpaddd(xmm(6), k0, xmm(19), xmm(15), false, Assembler::AVX_512bit);\n+\n+  \/\/ level 6\n+  __ evmovdqul(xmm16, Address(perms, 320), Assembler::AVX_512bit);\n+  __ evmovdqul(xmm17, xmm16, Assembler::AVX_512bit);\n+  __ evmovdqul(xmm18, xmm16, Assembler::AVX_512bit);\n+  __ evmovdqul(xmm19, xmm16, Assembler::AVX_512bit);\n+  __ evmovdqul(xmm12, Address(perms, 384), Assembler::AVX_512bit);\n+  __ evmovdqul(xmm13, xmm12, Assembler::AVX_512bit);\n+  __ evmovdqul(xmm14, xmm12, Assembler::AVX_512bit);\n+  __ evmovdqul(xmm15, xmm12, Assembler::AVX_512bit);\n+\n+  for (int i = 0; i < 8; i += 2) {\n+    __ evpermi2d(xmm(i\/2 + 16), xmm(i), xmm(i + 1), Assembler::AVX_512bit);\n+  }\n+  for (int i = 0; i < 8; i += 2) {\n+    __ evpermi2d(xmm(i \/ 2 + 12), xmm(i), xmm(i + 1), Assembler::AVX_512bit);\n+  }\n+  __ evmovdqul(xmm0, Address(zetas, 6 * 512), Assembler::AVX_512bit);\n+  __ evmovdqul(xmm1, Address(zetas, 6 * 512 + 64), Assembler::AVX_512bit);\n+  __ evmovdqul(xmm2, Address(zetas, 6 * 512 + 128), Assembler::AVX_512bit);\n+  __ evmovdqul(xmm3, Address(zetas, 6 * 512 + 192), Assembler::AVX_512bit);\n+\n+  montmulEven(20, 0, 12, 4, 20, 4);\n+\n+  __ vpshufd(xmm(12), xmm(12), 0xB1, Assembler::AVX_512bit);\n+  __ vpshufd(xmm(13), xmm(13), 0xB1, Assembler::AVX_512bit);\n+  __ vpshufd(xmm(14), xmm(14), 0xB1, Assembler::AVX_512bit);\n+  __ vpshufd(xmm(15), xmm(15), 0xB1, Assembler::AVX_512bit);\n+\n+  montmulEven(12, 0, 12, 4, 24, 4);\n+\n+  for (int i = 0; i < 4; i++) {\n+    __ evpermt2d(xmm(i + 12), xmm28, xmm(i + 20), Assembler::AVX_512bit);\n+  }\n+\n+  __ evpsubd(xmm(1), k0, xmm(16), xmm(12), false, Assembler::AVX_512bit);\n+  __ evpsubd(xmm(3), k0, xmm(17), xmm(13), false, Assembler::AVX_512bit);\n+  __ evpsubd(xmm(5), k0, xmm(18), xmm(14), false, Assembler::AVX_512bit);\n+  __ evpsubd(xmm(7), k0, xmm(19), xmm(15), false, Assembler::AVX_512bit);\n+\n+  __ evpaddd(xmm(0), k0, xmm(16), xmm(12), false, Assembler::AVX_512bit);\n+  __ evpaddd(xmm(2), k0, xmm(17), xmm(13), false, Assembler::AVX_512bit);\n+  __ evpaddd(xmm(4), k0, xmm(18), xmm(14), false, Assembler::AVX_512bit);\n+  __ evpaddd(xmm(6), k0, xmm(19), xmm(15), false, Assembler::AVX_512bit);\n+\n+  \/\/ level 7\n+  __ evmovdqul(xmm16, Address(perms, 448), Assembler::AVX_512bit);\n+  __ evmovdqul(xmm17, xmm16, Assembler::AVX_512bit);\n+  __ evmovdqul(xmm18, xmm16, Assembler::AVX_512bit);\n+  __ evmovdqul(xmm19, xmm16, Assembler::AVX_512bit);\n+  __ evmovdqul(xmm12, Address(perms, 512), Assembler::AVX_512bit);\n+  __ evmovdqul(xmm13, xmm12, Assembler::AVX_512bit);\n+  __ evmovdqul(xmm14, xmm12, Assembler::AVX_512bit);\n+  __ evmovdqul(xmm15, xmm12, Assembler::AVX_512bit);\n+\n+  for (int i = 0; i < 8; i += 2) {\n+    __ evpermi2d(xmm(i \/ 2 + 16), xmm(i), xmm(i + 1), Assembler::AVX_512bit);\n+  }\n+  for (int i = 0; i < 8; i += 2) {\n+    __ evpermi2d(xmm(i \/ 2 + 12), xmm(i), xmm(i + 1), Assembler::AVX_512bit);\n+  }\n+  __ evmovdqul(xmm0, Address(zetas, 7 * 512), Assembler::AVX_512bit);\n+  __ evmovdqul(xmm1, Address(zetas, 7 * 512 + 64), Assembler::AVX_512bit);\n+  __ evmovdqul(xmm2, Address(zetas, 7 * 512 + 128), Assembler::AVX_512bit);\n+  __ evmovdqul(xmm3, Address(zetas, 7 * 512 + 192), Assembler::AVX_512bit);\n+\n+  montmulEven(20, 0, 12, 4, 20, 4);\n+\n+  __ vpshufd(xmm(12), xmm(12), 0xB1, Assembler::AVX_512bit);\n+  __ vpshufd(xmm(13), xmm(13), 0xB1, Assembler::AVX_512bit);\n+  __ vpshufd(xmm(14), xmm(14), 0xB1, Assembler::AVX_512bit);\n+  __ vpshufd(xmm(15), xmm(15), 0xB1, Assembler::AVX_512bit);\n+  __ vpshufd(xmm(0), xmm(0), 0xB1, Assembler::AVX_512bit);\n+  __ vpshufd(xmm(1), xmm(1), 0xB1, Assembler::AVX_512bit);\n+  __ vpshufd(xmm(2), xmm(2), 0xB1, Assembler::AVX_512bit);\n+  __ vpshufd(xmm(3), xmm(3), 0xB1, Assembler::AVX_512bit);\n+\n+  montmulEven(12, 0, 12, 4, 24, 4);\n+\n+  for (int i = 0; i < 4; i++) {\n+    __ evpermt2d(xmm(i + 12), xmm28, xmm(i + 20), Assembler::AVX_512bit);\n+  }\n+\n+  __ evpsubd(xmm(21), k0, xmm(16), xmm(12), false, Assembler::AVX_512bit);\n+  __ evpsubd(xmm(23), k0, xmm(17), xmm(13), false, Assembler::AVX_512bit);\n+  __ evpsubd(xmm(25), k0, xmm(18), xmm(14), false, Assembler::AVX_512bit);\n+  __ evpsubd(xmm(27), k0, xmm(19), xmm(15), false, Assembler::AVX_512bit);\n+\n+  __ evpaddd(xmm(20), k0, xmm(16), xmm(12), false, Assembler::AVX_512bit);\n+  __ evpaddd(xmm(22), k0, xmm(17), xmm(13), false, Assembler::AVX_512bit);\n+  __ evpaddd(xmm(24), k0, xmm(18), xmm(14), false, Assembler::AVX_512bit);\n+  __ evpaddd(xmm(26), k0, xmm(19), xmm(15), false, Assembler::AVX_512bit);\n+\n+  __ evmovdqul(xmm0, Address(perms, 576), Assembler::AVX_512bit);\n+  __ evmovdqul(xmm2, xmm0, Assembler::AVX_512bit);\n+  __ evmovdqul(xmm4, xmm0, Assembler::AVX_512bit);\n+  __ evmovdqul(xmm6, xmm0, Assembler::AVX_512bit);\n+  __ evmovdqul(xmm1, Address(perms, 640), Assembler::AVX_512bit);\n+  __ evmovdqul(xmm3, xmm1, Assembler::AVX_512bit);\n+  __ evmovdqul(xmm5, xmm1, Assembler::AVX_512bit);\n+  __ evmovdqul(xmm7, xmm1, Assembler::AVX_512bit);\n+\n+  for (int i = 0; i < 8; i += 2) {\n+    __ evpermi2d(xmm(i), xmm(i + 20), xmm(i + 21), Assembler::AVX_512bit);\n+    __ evpermi2d(xmm(i + 1), xmm(i + 20), xmm(i + 21), Assembler::AVX_512bit);\n+  }\n+\n+  __ cmpl(iterations, 0);\n+  __ jcc(Assembler::equal, L_end);\n+\n+  for (int i = 0; i < 8; i++) {\n+    __ evmovdqul(Address(coeffs, i * 64), xmm(i), Assembler::AVX_512bit);\n+  }\n+\n+  for (int i = 8; i < 16; i++) {\n+    __ evmovdqul(xmm(i - 8), Address(coeffs, i * 64), Assembler::AVX_512bit);\n+  }\n+\n+  __ addptr(zetas, 256);\n+\n+  __ jmp(L_loop);\n+\n+  __ BIND(L_end);\n+\n+  for (int i = 0; i < 8; i++) {\n+    __ evmovdqul(Address(coeffs, (i + 8) * 64), xmm(i), Assembler::AVX_512bit);\n+  }\n+\n+  __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n+  __ mov64(rax, 0); \/\/ return 0\n+  __ ret(0);\n+\n+  return start;\n+}\n+\n+\/\/ Dilithium Inverse NTT function except the final mod Q division by 2^256.\n+\/\/ Implements\n+\/\/ static int implDilithiumAlmostInverseNtt(int[] coeffs, int[] zetas) {}\n+\/\/\n+\/\/ coeffs (int[256]) = c_rarg0\n+\/\/ zetas (int[256]) = c_rarg1\n+address StubGenerator::generate_dilithiumAlmostInverseNtt_avx512() {\n+\n+  __ align(CodeEntryAlignment);\n+  StubGenStubId stub_id = dilithiumAlmostInverseNtt_id;\n+  StubCodeMark mark(this, stub_id);\n+  address start = __ pc();\n+  __ enter();\n+\n+  Label L_loop, L_end;\n+\n+  const Register coeffs = c_rarg0;\n+  const Register zetas = c_rarg1;\n+\n+  const Register iterations = c_rarg2;\n+\n+  const Register dilithiumConsts = r10;\n+  const Register perms = r11;\n+\n+  __ lea(perms, ExternalAddress(dilithiumAvx512PermsAddr()));\n+  __ lea(dilithiumConsts, ExternalAddress(dilithiumAvx512ConstsAddr()));\n+\n+  __ evmovdqul(xmm28, Address(perms, 0), Assembler::AVX_512bit);\n+  __ vpbroadcastd(xmm30, Address(dilithiumConsts, 0), Assembler::AVX_512bit); \/\/ q^-1 mod 2^32\n+  __ vpbroadcastd(xmm31, Address(dilithiumConsts, 4), Assembler::AVX_512bit); \/\/ q\n+\n+  \/\/ We do levels 0-6 in two batches, each batch entirely in the vector registers\n+  for (int i = 0; i < 8; i++) {\n+    __ evmovdqul(xmm(i), Address(coeffs, i * 64), Assembler::AVX_512bit);\n+  }\n+\n+  __ movl(iterations, 2);\n+\n+  __ BIND(L_loop);\n+\n+  __ subl(iterations, 1);\n+\n+  \/\/ level 0\n+  __ evmovdqul(xmm8, Address(perms, 704), Assembler::AVX_512bit);\n+  __ evmovdqul(xmm9, xmm8, Assembler::AVX_512bit);\n+  __ evmovdqul(xmm10, xmm8, Assembler::AVX_512bit);\n+  __ evmovdqul(xmm11, xmm8, Assembler::AVX_512bit);\n+  __ evmovdqul(xmm12, Address(perms, 768), Assembler::AVX_512bit);\n+  __ evmovdqul(xmm13, xmm12, Assembler::AVX_512bit);\n+  __ evmovdqul(xmm14, xmm12, Assembler::AVX_512bit);\n+  __ evmovdqul(xmm15, xmm12, Assembler::AVX_512bit);\n+\n+  for (int i = 0; i < 8; i += 2) {\n+    __ evpermi2d(xmm(i \/ 2 + 8), xmm(i), xmm(i + 1), Assembler::AVX_512bit);\n+    __ evpermi2d(xmm(i \/ 2 + 12), xmm(i), xmm(i + 1), Assembler::AVX_512bit);\n+  }\n+\n+  __ evpaddd(xmm(0), k0, xmm(8), xmm(12), false, Assembler::AVX_512bit);\n+  __ evpaddd(xmm(1), k0, xmm(9), xmm(13), false, Assembler::AVX_512bit);\n+  __ evpaddd(xmm(2), k0, xmm(10), xmm(14), false, Assembler::AVX_512bit);\n+  __ evpaddd(xmm(3), k0, xmm(11), xmm(15), false, Assembler::AVX_512bit);\n+\n+  __ evpsubd(xmm(8), k0, xmm(8), xmm(12), false, Assembler::AVX_512bit);\n+  __ evpsubd(xmm(9), k0, xmm(9), xmm(13), false, Assembler::AVX_512bit);\n+  __ evpsubd(xmm(10), k0, xmm(10), xmm(14), false, Assembler::AVX_512bit);\n+  __ evpsubd(xmm(11), k0, xmm(11), xmm(15), false, Assembler::AVX_512bit);\n+\n+  __ evmovdqul(xmm4, Address(zetas, 0), Assembler::AVX_512bit);\n+  __ evmovdqul(xmm5, Address(zetas, 64), Assembler::AVX_512bit);\n+  __ evmovdqul(xmm6, Address(zetas, 128), Assembler::AVX_512bit);\n+  __ evmovdqul(xmm7, Address(zetas, 192), Assembler::AVX_512bit);\n+\n+  montmulEven(20, 4, 8, 16, 20, 4);\n+\n+  __ vpshufd(xmm(4), xmm(4), 0xB1, Assembler::AVX_512bit);\n+  __ vpshufd(xmm(5), xmm(5), 0xB1, Assembler::AVX_512bit);\n+  __ vpshufd(xmm(6), xmm(6), 0xB1, Assembler::AVX_512bit);\n+  __ vpshufd(xmm(7), xmm(7), 0xB1, Assembler::AVX_512bit);\n+  __ vpshufd(xmm(8), xmm(8), 0xB1, Assembler::AVX_512bit);\n+  __ vpshufd(xmm(9), xmm(9), 0xB1, Assembler::AVX_512bit);\n+  __ vpshufd(xmm(10), xmm(10), 0xB1, Assembler::AVX_512bit);\n+  __ vpshufd(xmm(11), xmm(11), 0xB1, Assembler::AVX_512bit);\n+\n+  montmulEven(4, 4, 8, 16, 24, 4);\n+\n+  for (int i = 0; i < 4; i++) {\n+    __ evpermt2d(xmm(i + 4), xmm28, xmm(i + 20), Assembler::AVX_512bit);\n+  }\n+\n+  \/\/ level 1\n+  __ evmovdqul(xmm8, Address(perms, 832), Assembler::AVX_512bit);\n+  __ evmovdqul(xmm9, xmm8, Assembler::AVX_512bit);\n+  __ evmovdqul(xmm10, xmm8, Assembler::AVX_512bit);\n+  __ evmovdqul(xmm11, xmm8, Assembler::AVX_512bit);\n+  __ evmovdqul(xmm12, Address(perms, 896), Assembler::AVX_512bit);\n+  __ evmovdqul(xmm13, xmm12, Assembler::AVX_512bit);\n+  __ evmovdqul(xmm14, xmm12, Assembler::AVX_512bit);\n+  __ evmovdqul(xmm15, xmm12, Assembler::AVX_512bit);\n+\n+  for (int i = 0; i < 4; i++) {\n+    __ evpermi2d(xmm(i + 8), xmm(i), xmm(i + 4), Assembler::AVX_512bit);\n+    __ evpermi2d(xmm(i + 12), xmm(i), xmm(i + 4), Assembler::AVX_512bit);\n+  }\n+\n+  __ evpaddd(xmm(0), k0, xmm(8), xmm(12), false, Assembler::AVX_512bit);\n+  __ evpaddd(xmm(1), k0, xmm(9), xmm(13), false, Assembler::AVX_512bit);\n+  __ evpaddd(xmm(2), k0, xmm(10), xmm(14), false, Assembler::AVX_512bit);\n+  __ evpaddd(xmm(3), k0, xmm(11), xmm(15), false, Assembler::AVX_512bit);\n+\n+  __ evpsubd(xmm(8), k0, xmm(8), xmm(12), false, Assembler::AVX_512bit);\n+  __ evpsubd(xmm(9), k0, xmm(9), xmm(13), false, Assembler::AVX_512bit);\n+  __ evpsubd(xmm(10), k0, xmm(10), xmm(14), false, Assembler::AVX_512bit);\n+  __ evpsubd(xmm(11), k0, xmm(11), xmm(15), false, Assembler::AVX_512bit);\n+\n+  __ evmovdqul(xmm4, Address(zetas, 512), Assembler::AVX_512bit);\n+  __ evmovdqul(xmm5, Address(zetas, 512 + 64), Assembler::AVX_512bit);\n+  __ evmovdqul(xmm6, Address(zetas, 512 + 128), Assembler::AVX_512bit);\n+  __ evmovdqul(xmm7, Address(zetas, 512 + 192), Assembler::AVX_512bit);\n+\n+  montmulEven(20, 4, 8, 16, 20, 4);\n+\n+  __ vpshufd(xmm(8), xmm(8), 0xB1, Assembler::AVX_512bit);\n+  __ vpshufd(xmm(9), xmm(9), 0xB1, Assembler::AVX_512bit);\n+  __ vpshufd(xmm(10), xmm(10), 0xB1, Assembler::AVX_512bit);\n+  __ vpshufd(xmm(11), xmm(11), 0xB1, Assembler::AVX_512bit);\n+\n+  montmulEven(4, 4, 8, 16, 24, 4);\n+\n+  for (int i = 0; i < 4; i++) {\n+    __ evpermt2d(xmm(i + 4), xmm28, xmm(i + 20), Assembler::AVX_512bit);\n+  }\n+\n+  \/\/ level 2\n+  __ evmovdqul(xmm8, Address(perms, 960), Assembler::AVX_512bit);\n+  __ evmovdqul(xmm9, xmm8, Assembler::AVX_512bit);\n+  __ evmovdqul(xmm10, xmm8, Assembler::AVX_512bit);\n+  __ evmovdqul(xmm11, xmm8, Assembler::AVX_512bit);\n+  __ evmovdqul(xmm12, Address(perms, 1024), Assembler::AVX_512bit);\n+  __ evmovdqul(xmm13, xmm12, Assembler::AVX_512bit);\n+  __ evmovdqul(xmm14, xmm12, Assembler::AVX_512bit);\n+  __ evmovdqul(xmm15, xmm12, Assembler::AVX_512bit);\n+\n+  for (int i = 0; i < 4; i++) {\n+    __ evpermi2d(xmm(i + 8), xmm(i), xmm(i + 4), Assembler::AVX_512bit);\n+    __ evpermi2d(xmm(i + 12), xmm(i), xmm(i + 4), Assembler::AVX_512bit);\n+  }\n+\n+  __ evpaddd(xmm(0), k0, xmm(8), xmm(12), false, Assembler::AVX_512bit);\n+  __ evpaddd(xmm(1), k0, xmm(9), xmm(13), false, Assembler::AVX_512bit);\n+  __ evpaddd(xmm(2), k0, xmm(10), xmm(14), false, Assembler::AVX_512bit);\n+  __ evpaddd(xmm(3), k0, xmm(11), xmm(15), false, Assembler::AVX_512bit);\n+\n+  __ evpsubd(xmm(8), k0, xmm(8), xmm(12), false, Assembler::AVX_512bit);\n+  __ evpsubd(xmm(9), k0, xmm(9), xmm(13), false, Assembler::AVX_512bit);\n+  __ evpsubd(xmm(10), k0, xmm(10), xmm(14), false, Assembler::AVX_512bit);\n+  __ evpsubd(xmm(11), k0, xmm(11), xmm(15), false, Assembler::AVX_512bit);\n+\n+  __ evmovdqul(xmm4, Address(zetas, 2 * 512), Assembler::AVX_512bit);\n+  __ evmovdqul(xmm5, Address(zetas, 2 * 512 + 64), Assembler::AVX_512bit);\n+  __ evmovdqul(xmm6, Address(zetas, 2 * 512 + 128), Assembler::AVX_512bit);\n+  __ evmovdqul(xmm7, Address(zetas, 2 * 512 + 192), Assembler::AVX_512bit);\n+\n+  montmulEven(20, 4, 8, 16, 20, 4);\n+\n+  __ vpshufd(xmm(8), xmm(8), 0xB1, Assembler::AVX_512bit);\n+  __ vpshufd(xmm(9), xmm(9), 0xB1, Assembler::AVX_512bit);\n+  __ vpshufd(xmm(10), xmm(10), 0xB1, Assembler::AVX_512bit);\n+  __ vpshufd(xmm(11), xmm(11), 0xB1, Assembler::AVX_512bit);\n+\n+  montmulEven(4, 4, 8, 16, 24, 4);\n+\n+  for (int i = 0; i < 4; i++) {\n+    __ evpermt2d(xmm(i + 4), xmm28, xmm(i + 20), Assembler::AVX_512bit);\n+  }\n+\n+  \/\/ level 3\n+  __ evmovdqul(xmm8, Address(perms, 1088), Assembler::AVX_512bit);\n+  __ evmovdqul(xmm9, xmm8, Assembler::AVX_512bit);\n+  __ evmovdqul(xmm10, xmm8, Assembler::AVX_512bit);\n+  __ evmovdqul(xmm11, xmm8, Assembler::AVX_512bit);\n+  __ evmovdqul(xmm12, Address(perms, 1152), Assembler::AVX_512bit);\n+  __ evmovdqul(xmm13, xmm12, Assembler::AVX_512bit);\n+  __ evmovdqul(xmm14, xmm12, Assembler::AVX_512bit);\n+  __ evmovdqul(xmm15, xmm12, Assembler::AVX_512bit);\n+\n+  for (int i = 0; i < 4; i++) {\n+    __ evpermi2d(xmm(i + 8), xmm(i), xmm(i + 4), Assembler::AVX_512bit);\n+    __ evpermi2d(xmm(i + 12), xmm(i), xmm(i + 4), Assembler::AVX_512bit);\n+  }\n+\n+  __ evpaddd(xmm(0), k0, xmm(8), xmm(12), false, Assembler::AVX_512bit);\n+  __ evpaddd(xmm(1), k0, xmm(9), xmm(13), false, Assembler::AVX_512bit);\n+  __ evpaddd(xmm(2), k0, xmm(10), xmm(14), false, Assembler::AVX_512bit);\n+  __ evpaddd(xmm(3), k0, xmm(11), xmm(15), false, Assembler::AVX_512bit);\n+\n+  __ evpsubd(xmm(8), k0, xmm(8), xmm(12), false, Assembler::AVX_512bit);\n+  __ evpsubd(xmm(9), k0, xmm(9), xmm(13), false, Assembler::AVX_512bit);\n+  __ evpsubd(xmm(10), k0, xmm(10), xmm(14), false, Assembler::AVX_512bit);\n+  __ evpsubd(xmm(11), k0, xmm(11), xmm(15), false, Assembler::AVX_512bit);\n+\n+  __ evmovdqul(xmm4, Address(zetas, 3 * 512), Assembler::AVX_512bit);\n+  __ evmovdqul(xmm5, Address(zetas, 3 * 512 + 64), Assembler::AVX_512bit);\n+  __ evmovdqul(xmm6, Address(zetas, 3 * 512 + 128), Assembler::AVX_512bit);\n+  __ evmovdqul(xmm7, Address(zetas, 3 * 512 + 192), Assembler::AVX_512bit);\n+\n+  montmulEven(20, 4, 8, 16, 20, 4);\n+\n+  __ vpshufd(xmm(8), xmm(8), 0xB1, Assembler::AVX_512bit);\n+  __ vpshufd(xmm(9), xmm(9), 0xB1, Assembler::AVX_512bit);\n+  __ vpshufd(xmm(10), xmm(10), 0xB1, Assembler::AVX_512bit);\n+  __ vpshufd(xmm(11), xmm(11), 0xB1, Assembler::AVX_512bit);\n+\n+  montmulEven(4, 4, 8, 16, 24, 4);\n+\n+  for (int i = 0; i < 4; i++) {\n+    __ evpermt2d(xmm(i + 4), xmm28, xmm(i + 20), Assembler::AVX_512bit);\n+  }\n+\n+  \/\/ level 4\n+  __ evmovdqul(xmm8, Address(perms, 1216), Assembler::AVX_512bit);\n+  __ evmovdqul(xmm9, xmm8, Assembler::AVX_512bit);\n+  __ evmovdqul(xmm10, xmm8, Assembler::AVX_512bit);\n+  __ evmovdqul(xmm11, xmm8, Assembler::AVX_512bit);\n+  __ evmovdqul(xmm12, Address(perms, 1280), Assembler::AVX_512bit);\n+  __ evmovdqul(xmm13, xmm12, Assembler::AVX_512bit);\n+  __ evmovdqul(xmm14, xmm12, Assembler::AVX_512bit);\n+  __ evmovdqul(xmm15, xmm12, Assembler::AVX_512bit);\n+\n+  for (int i = 0; i < 4; i++) {\n+    __ evpermi2d(xmm(i + 8), xmm(i), xmm(i + 4), Assembler::AVX_512bit);\n+    __ evpermi2d(xmm(i + 12), xmm(i), xmm(i + 4), Assembler::AVX_512bit);\n+  }\n+\n+  __ evpaddd(xmm(0), k0, xmm(8), xmm(12), false, Assembler::AVX_512bit);\n+  __ evpaddd(xmm(1), k0, xmm(9), xmm(13), false, Assembler::AVX_512bit);\n+  __ evpaddd(xmm(2), k0, xmm(10), xmm(14), false, Assembler::AVX_512bit);\n+  __ evpaddd(xmm(3), k0, xmm(11), xmm(15), false, Assembler::AVX_512bit);\n+\n+  __ evpsubd(xmm(8), k0, xmm(8), xmm(12), false, Assembler::AVX_512bit);\n+  __ evpsubd(xmm(9), k0, xmm(9), xmm(13), false, Assembler::AVX_512bit);\n+  __ evpsubd(xmm(10), k0, xmm(10), xmm(14), false, Assembler::AVX_512bit);\n+  __ evpsubd(xmm(11), k0, xmm(11), xmm(15), false, Assembler::AVX_512bit);\n+\n+  __ evmovdqul(xmm4, Address(zetas, 4 * 512), Assembler::AVX_512bit);\n+  __ evmovdqul(xmm5, Address(zetas, 4 * 512 + 64), Assembler::AVX_512bit);\n+  __ evmovdqul(xmm6, Address(zetas, 4 * 512 + 128), Assembler::AVX_512bit);\n+  __ evmovdqul(xmm7, Address(zetas, 4 * 512 + 192), Assembler::AVX_512bit);\n+\n+  montmulEven(20, 4, 8, 16, 20, 4);\n+\n+  __ vpshufd(xmm(8), xmm(8), 0xB1, Assembler::AVX_512bit);\n+  __ vpshufd(xmm(9), xmm(9), 0xB1, Assembler::AVX_512bit);\n+  __ vpshufd(xmm(10), xmm(10), 0xB1, Assembler::AVX_512bit);\n+  __ vpshufd(xmm(11), xmm(11), 0xB1, Assembler::AVX_512bit);\n+\n+  montmulEven(4, 4, 8, 16, 24, 4);\n+\n+  for (int i = 0; i < 4; i++) {\n+    __ evpermt2d(xmm(i + 4), xmm28, xmm(i + 20), Assembler::AVX_512bit);\n+  }\n+\n+  \/\/ level 5\n+  __ evpsubd(xmm(8), k0, xmm(0), xmm(1), false, Assembler::AVX_512bit);\n+  __ evpsubd(xmm(9), k0, xmm(4), xmm(5), false, Assembler::AVX_512bit);\n+  __ evpsubd(xmm(10), k0, xmm(2), xmm(3), false, Assembler::AVX_512bit);\n+  __ evpsubd(xmm(11), k0, xmm(6), xmm(7), false, Assembler::AVX_512bit);\n+\n+  __ evpaddd(xmm(0), k0, xmm(0), xmm(1), false, Assembler::AVX_512bit);\n+  __ evpaddd(xmm(1), k0, xmm(4), xmm(5), false, Assembler::AVX_512bit);\n+  __ evpaddd(xmm(2), k0, xmm(2), xmm(3), false, Assembler::AVX_512bit);\n+  __ evpaddd(xmm(3), k0, xmm(6), xmm(7), false, Assembler::AVX_512bit);;\n+\n+  __ evmovdqul(xmm4, Address(zetas, 5 * 512), Assembler::AVX_512bit);\n+  __ evmovdqul(xmm5, Address(zetas, 5 * 512 + 64), Assembler::AVX_512bit);\n+  __ evmovdqul(xmm6, Address(zetas, 5 * 512 + 128), Assembler::AVX_512bit);\n+  __ evmovdqul(xmm7, Address(zetas, 5 * 512 + 192), Assembler::AVX_512bit);\n+\n+  montmulEven(20, 4, 8, 16, 20, 4);\n+\n+  __ vpshufd(xmm(8), xmm(8), 0xB1, Assembler::AVX_512bit);\n+  __ vpshufd(xmm(9), xmm(9), 0xB1, Assembler::AVX_512bit);\n+  __ vpshufd(xmm(10), xmm(10), 0xB1, Assembler::AVX_512bit);\n+  __ vpshufd(xmm(11), xmm(11), 0xB1, Assembler::AVX_512bit);\n+\n+  montmulEven(4, 4, 8, 16, 24, 4);\n+\n+  for (int i = 0; i < 4; i++) {\n+    __ evpermt2d(xmm(i + 4), xmm28, xmm(i + 20), Assembler::AVX_512bit);\n+  }\n+\n+  \/\/ level 6\n+  __ evpsubd(xmm(8), k0, xmm(0), xmm(2), false, Assembler::AVX_512bit);\n+  __ evpsubd(xmm(9), k0, xmm(1), xmm(3), false, Assembler::AVX_512bit);\n+  __ evpsubd(xmm(10), k0, xmm(4), xmm(6), false, Assembler::AVX_512bit);\n+  __ evpsubd(xmm(11), k0, xmm(5), xmm(7), false, Assembler::AVX_512bit);\n+\n+  __ evpaddd(xmm(0), k0, xmm(0), xmm(2), false, Assembler::AVX_512bit);\n+  __ evpaddd(xmm(2), k0, xmm(4), xmm(6), false, Assembler::AVX_512bit);\n+  __ evpaddd(xmm(1), k0, xmm(1), xmm(3), false, Assembler::AVX_512bit);\n+  __ evpaddd(xmm(3), k0, xmm(5), xmm(7), false, Assembler::AVX_512bit);\n+\n+  __ evmovdqul(xmm4, Address(zetas, 6 * 512), Assembler::AVX_512bit);\n+  __ evmovdqul(xmm5, Address(zetas, 6 * 512 + 64), Assembler::AVX_512bit);\n+  __ evmovdqul(xmm6, Address(zetas, 6 * 512 + 128), Assembler::AVX_512bit);\n+  __ evmovdqul(xmm7, Address(zetas, 6 * 512 + 192), Assembler::AVX_512bit);\n+\n+  montmulEven(20, 4, 8, 16, 20, 4);\n+\n+  __ vpshufd(xmm(8), xmm(8), 0xB1, Assembler::AVX_512bit);\n+  __ vpshufd(xmm(9), xmm(9), 0xB1, Assembler::AVX_512bit);\n+  __ vpshufd(xmm(10), xmm(10), 0xB1, Assembler::AVX_512bit);\n+  __ vpshufd(xmm(11), xmm(11), 0xB1, Assembler::AVX_512bit);\n+\n+  montmulEven(4, 4, 8, 16, 24, 4);\n+\n+  for (int i = 0; i < 4; i++) {\n+    __ evpermt2d(xmm(i + 4), xmm28, xmm(i + 20), Assembler::AVX_512bit);\n+  }\n+\n+  __ cmpl(iterations, 0);\n+  __ jcc(Assembler::equal, L_end);\n+\n+  \/\/ save the coefficients of the first batch, adjust the zetas\n+  \/\/ and load the second batch of coefficients\n+  for (int i = 0; i < 8; i++) {\n+    __ evmovdqul(Address(coeffs, i * 64), xmm(i), Assembler::AVX_512bit);\n+  }\n+\n+  __ addptr(zetas, 256);\n+\n+  for (int i = 0; i < 8; i++) {\n+    __ evmovdqul(xmm(i), Address(coeffs, i * 64 + 512), Assembler::AVX_512bit);\n+  }\n+\n+  __ jmp(L_loop);\n+\n+  __ BIND(L_end);\n+\n+  \/\/ load the coeffs of the first batch of coefficients that were saved after\n+  \/\/ level 6 into Zmm_8-Zmm_15 and do the last level entirely in the vector registers\n+  for (int i = 0; i < 8; i++) {\n+    __ evmovdqul(xmm(i + 8), Address(coeffs, i * 64), Assembler::AVX_512bit);\n+  }\n+\n+  \/\/ level 7\n+  for (int i = 0; i < 8; i++) {\n+    __ evpsubd(xmm(i + 16), k0, xmm(i + 8), xmm(i), false, Assembler::AVX_512bit);\n+  }\n+\n+  for (int i = 0; i < 8; i++) {\n+    __ evpaddd(xmm(i), k0, xmm(i), xmm(i + 8), false, Assembler::AVX_512bit);\n+  }\n+\n+  for (int i = 0; i < 8; i++) {\n+    __ evmovdqul(Address(coeffs, i * 64), xmm(i), Assembler::AVX_512bit);\n+  }\n+\n+  __ evmovdqul(xmm29, Address(zetas, 7 * 512), Assembler::AVX_512bit);\n+\n+  montmulEven(4, 16, 29, 8, 12, 4);\n+\n+  __ vpshufd(xmm(16), xmm(16), 0xB1, Assembler::AVX_512bit);\n+  __ vpshufd(xmm(17), xmm(17), 0xB1, Assembler::AVX_512bit);\n+  __ vpshufd(xmm(18), xmm(18), 0xB1, Assembler::AVX_512bit);\n+  __ vpshufd(xmm(19), xmm(19), 0xB1, Assembler::AVX_512bit);\n+\n+  montmulEven(0, 16, 29, 8, 12, 4);\n+\n+  for (int i = 0; i < 4; i++) {\n+    __ evpermt2d(xmm(i), xmm28, xmm(i + 4), Assembler::AVX_512bit);\n+  }\n+\n+  for (int i = 0; i < 4; i++) {\n+    __ evmovdqul(Address(coeffs, i * 64 + 512), xmm(i), Assembler::AVX_512bit);\n+  }\n+\n+  montmulEven(4, 20, 29, 8, 12, 4);\n+\n+  __ vpshufd(xmm(20), xmm(20), 0xB1, Assembler::AVX_512bit);\n+  __ vpshufd(xmm(21), xmm(21), 0xB1, Assembler::AVX_512bit);\n+  __ vpshufd(xmm(22), xmm(22), 0xB1, Assembler::AVX_512bit);\n+  __ vpshufd(xmm(23), xmm(23), 0xB1, Assembler::AVX_512bit);\n+\n+  montmulEven(0, 20, 29, 8, 12, 4);\n+\n+  for (int i = 0; i < 4; i++) {\n+    __ evpermt2d(xmm(i), xmm28, xmm(i + 4), Assembler::AVX_512bit);\n+  }\n+\n+  for (int i = 0; i < 4; i++) {\n+    __ evmovdqul(Address(coeffs, i * 64 + 768), xmm(i), Assembler::AVX_512bit);\n+  }\n+\n+  __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n+  __ mov64(rax, 0); \/\/ return 0\n+  __ ret(0);\n+\n+  return start;\n+}\n+\n+\/\/ Dilithium multiply polynomials in the NTT domain.\n+\/\/ Implements\n+\/\/ static int implDilithiumNttMult(\n+\/\/              int[] result, int[] ntta, int[] nttb {}\n+\/\/\n+\/\/ result (int[256]) = c_rarg0\n+\/\/ poly1 (int[256]) = c_rarg1\n+\/\/ poly2 (int[256]) = c_rarg2\n+address StubGenerator::generate_dilithiumNttMult_avx512() {\n+\n+  __ align(CodeEntryAlignment);\n+  StubGenStubId stub_id = dilithiumNttMult_id;\n+  StubCodeMark mark(this, stub_id);\n+  address start = __ pc();\n+  __ enter();\n+\n+  Label L_loop;\n+\n+  const Register result = c_rarg0;\n+  const Register poly1 = c_rarg1;\n+  const Register poly2 = c_rarg2;\n+\n+  const Register dilithiumConsts = c_rarg3;\n+  const Register perms = r10;\n+  const Register len = r11;\n+\n+  __ lea(dilithiumConsts, ExternalAddress(dilithiumAvx512ConstsAddr()));\n+  __ lea(perms, ExternalAddress(dilithiumAvx512PermsAddr()));\n+\n+  __ vpbroadcastd(xmm30, Address(dilithiumConsts, 0), Assembler::AVX_512bit); \/\/ q^-1 mod 2^32\n+  __ vpbroadcastd(xmm31, Address(dilithiumConsts, 4), Assembler::AVX_512bit); \/\/ q\n+  __ vpbroadcastd(xmm29, Address(dilithiumConsts, 12), Assembler::AVX_512bit); \/\/ 2^64 mod q\n+  __ evmovdqul(xmm28, Address(perms, 0), Assembler::AVX_512bit);\n+\n+  __ movl(len, 4);\n+\n+  __ BIND(L_loop);\n+\n+  for (int i = 0; i < 4; i++) {\n+    __ evmovdqul(xmm(i), Address(poly1, i * 64), Assembler::AVX_512bit);\n+    __ evmovdqul(xmm(i + 4), Address(poly2, i * 64), Assembler::AVX_512bit);\n+  }\n+\n+  montmulEven(8, 4, 29, 12, 16, 4);\n+  for (int i = 0; i < 4; i++) {\n+    __ vpshufd(xmm(i + 8), xmm(i + 8), 0xB1, Assembler::AVX_512bit);\n+  }\n+  montmulEven(8, 0, 8, 12, 16, 4);\n+  for (int i = 0; i < 4; i++) {\n+    __ vpshufd(xmm(i), xmm(i), 0xB1, Assembler::AVX_512bit);\n+    __ vpshufd(xmm(i + 4), xmm(i + 4), 0xB1, Assembler::AVX_512bit);\n+  }\n+  montmulEven(4, 4, 29, 12, 16, 4);\n+  for (int i = 0; i < 4; i++) {\n+    __ vpshufd(xmm(i + 4), xmm(i + 4), 0xB1, Assembler::AVX_512bit);\n+  }\n+  montmulEven(0, 0, 4, 12, 16, 4);\n+  for (int i = 0; i < 4; i++) {\n+    __ evpermt2d(xmm(i), xmm28, xmm(i + 8), Assembler::AVX_512bit);\n+  }\n+  for (int i = 0; i < 4; i++) {\n+    __ evmovdqul(Address(result, i * 64), xmm(i), Assembler::AVX_512bit);\n+  }\n+\n+  __ subl(len, 1);\n+  __ addptr(poly1, 256);\n+  __ addptr(poly2, 256);\n+  __ addptr(result, 256);\n+  __ cmpl(len, 0);\n+  __ jcc(Assembler::notEqual, L_loop);\n+\n+\n+  __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n+  __ mov64(rax, 0); \/\/ return 0\n+  __ ret(0);\n+\n+  return start;\n+}\n+\n+\/\/ Dilithium Motgomery multiply an array by a constant.\n+\/\/ Implements\n+\/\/ static int implDilithiumMontMulByConstant(int[] coeffs, int constant) {}\n+\/\/\n+\/\/ coeffs (int[256]) = c_rarg0\n+\/\/ constant (int) = c_rarg1\n+address StubGenerator::generate_dilithiumMontMulByConstant_avx512() {\n+\n+  __ align(CodeEntryAlignment);\n+  StubGenStubId stub_id = dilithiumMontMulByConstant_id;\n+  StubCodeMark mark(this, stub_id);\n+  address start = __ pc();\n+  __ enter();\n+\n+  Label L_loop;\n+\n+  const Register coeffs = c_rarg0;\n+  const Register constant = c_rarg1;\n+\n+  const Register perms = c_rarg2;\n+  const Register dilithiumConsts = c_rarg3;\n+  const Register len = r10;\n+\n+  __ lea(dilithiumConsts, ExternalAddress(dilithiumAvx512ConstsAddr()));\n+  __ lea(perms, ExternalAddress(dilithiumAvx512PermsAddr()));\n+\n+  __ vpbroadcastd(xmm30, Address(dilithiumConsts, 0), Assembler::AVX_512bit); \/\/ q^-1 mod 2^32\n+  __ vpbroadcastd(xmm31, Address(dilithiumConsts, 4), Assembler::AVX_512bit); \/\/ q\n+  __ evmovdqul(xmm28, Address(perms, 0), Assembler::AVX_512bit);\n+\n+  __ evpbroadcastd(xmm29, constant, Assembler::AVX_512bit); \/\/ constant multiplier\n+\n+  __ movl(len, 2);\n+\n+  __ BIND(L_loop);\n+\n+  for (int i = 0; i < 8; i++) {\n+    __ evmovdqul(xmm(i), Address(coeffs, i * 64), Assembler::AVX_512bit);\n+  }\n+  montmulEven(8, 0, 29, 8, 16, 8);\n+  for (int i = 0; i < 8; i++) {\n+    __ vpshufd(xmm(i),xmm(i), 0xB1, Assembler::AVX_512bit);\n+  }\n+  montmulEven(0, 0, 29, 0, 16, 8);\n+  for (int i = 0; i < 8; i++) {\n+    __ evpermt2d(xmm(i), xmm28, xmm(i + 8), Assembler::AVX_512bit);\n+  }\n+  for (int i = 0; i < 8; i++) {\n+    __ evmovdqul(Address(coeffs, i * 64), xmm(i), Assembler::AVX_512bit);\n+  }\n+\n+  __ subl(len, 1);\n+  __ addptr(coeffs, 512);\n+  __ cmpl(len, 0);\n+  __ jcc(Assembler::notEqual, L_loop);\n+\n+  __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n+  __ mov64(rax, 0); \/\/ return 0\n+  __ ret(0);\n+\n+  return start;\n+}\n+\n+\/\/ Dilithium decompose poly.\n+\/\/ Implements\n+\/\/ static int implDilithiumDecomposePoly(int[] coeffs, int constant) {}\n+\/\/\n+\/\/ input (int[256]) = c_rarg0\n+\/\/ lowPart (int[256]) = c_rarg1\n+\/\/ highPart (int[256]) = c_rarg2\n+\/\/ twoGamma2  (int) = c_rarg3\n+\/\/ multiplier (int) = c_rarg4\n+address StubGenerator::generate_dilithiumDecomposePoly_avx512() {\n+\n+  __ align(CodeEntryAlignment);\n+  StubGenStubId stub_id = dilithiumDecomposePoly_id;\n+  StubCodeMark mark(this, stub_id);\n+  address start = __ pc();\n+  __ enter();\n+\n+  Label L_loop;\n+\n+  const Register input = c_rarg0;\n+  const Register lowPart = c_rarg1;\n+  const Register highPart = c_rarg2;\n+  const Register twoGamma2 = c_rarg3;\n+\n+  const Register len = c_rarg3;\n+  const Register dilithiumConsts = r9;\n+  const Register tmp = r10;\n+\n+  __ lea(dilithiumConsts, ExternalAddress(dilithiumAvx512ConstsAddr()));\n+\n+  __ xorl(tmp, tmp);\n+  __ evpbroadcastd(xmm24, tmp, Assembler::AVX_512bit); \/\/ 0\n+  __ addl(tmp, 1);\n+  __ evpbroadcastd(xmm25, tmp, Assembler::AVX_512bit); \/\/ 1\n+  __ vpbroadcastd(xmm30, Address(dilithiumConsts, 4), Assembler::AVX_512bit); \/\/ q\n+  __ vpbroadcastd(xmm31, Address(dilithiumConsts, 16), Assembler::AVX_512bit); \/\/ addend for mod q reduce\n+\n+  __ evpbroadcastd(xmm28, twoGamma2, Assembler::AVX_512bit); \/\/ 2 * gamma2\n+\n+  #ifndef _WIN64\n+    const Register multiplier = c_rarg4;\n+  #else\n+    const Address multiplier_mem(rbp, 6 * wordSize);\n+    const Register multiplier = c_rarg3;\n+    __ movptr(multiplier, multiplier_mem);\n+  #endif\n+  __ evpbroadcastd(xmm29, multiplier, Assembler::AVX_512bit); \/\/ multiplier for mod 2 * gamma2 reduce\n+\n+  __ evpsubd(xmm26, k0, xmm30, xmm25, false, Assembler::AVX_512bit); \/\/ q - 1\n+  __ evpsrad(xmm27, k0, xmm28, 1, false, Assembler::AVX_512bit); \/\/ gamma2\n+\n+  __ movl(len, 1024);\n+\n+  __ BIND(L_loop);\n+\n+  __ evmovdqul(xmm0, Address(input, 0), Assembler::AVX_512bit);\n+  __ evmovdqul(xmm1, Address(input, 64), Assembler::AVX_512bit);\n+  __ evmovdqul(xmm2, Address(input, 128), Assembler::AVX_512bit);\n+  __ evmovdqul(xmm3, Address(input, 192), Assembler::AVX_512bit);\n+\n+  __ addptr(input, 256);\n+\n+  \/\/ rplus in xmm0\n+  \/\/          rplus = rplus - ((rplus + 5373807) >> 23) * dilithium_q;\n+  __ evpaddd(xmm4, k0, xmm0, xmm31, false, Assembler::AVX_512bit);\n+  __ evpaddd(xmm5, k0, xmm1, xmm31, false, Assembler::AVX_512bit);\n+  __ evpaddd(xmm6, k0, xmm2, xmm31, false, Assembler::AVX_512bit);\n+  __ evpaddd(xmm7, k0, xmm3, xmm31, false, Assembler::AVX_512bit);\n+\n+  __ evpsrad(xmm4, k0, xmm4, 23, false, Assembler::AVX_512bit);\n+  __ evpsrad(xmm5, k0, xmm5, 23, false, Assembler::AVX_512bit);\n+  __ evpsrad(xmm6, k0, xmm6, 23, false, Assembler::AVX_512bit);\n+  __ evpsrad(xmm7, k0, xmm7, 23, false, Assembler::AVX_512bit);\n+\n+  __ evpmulld(xmm4, k0, xmm4, xmm30, false, Assembler::AVX_512bit);\n+  __ evpmulld(xmm5, k0, xmm5, xmm30, false, Assembler::AVX_512bit);\n+  __ evpmulld(xmm6, k0, xmm6, xmm30, false, Assembler::AVX_512bit);\n+  __ evpmulld(xmm7, k0, xmm7, xmm30, false, Assembler::AVX_512bit);\n+\n+  __ evpsubd(xmm0, k0, xmm0, xmm4, false, Assembler::AVX_512bit);\n+  __ evpsubd(xmm1, k0, xmm1, xmm5, false, Assembler::AVX_512bit);\n+  __ evpsubd(xmm2, k0, xmm2, xmm6, false, Assembler::AVX_512bit);\n+  __ evpsubd(xmm3, k0, xmm3, xmm7, false, Assembler::AVX_512bit);\n+\n+            \/\/ rplus in xmm0\n+\n+\/\/            rplus = rplus + ((rplus >> 31) & dilithium_q);\n+  __ evpsrad(xmm4, k0, xmm0, 31, false, Assembler::AVX_512bit);\n+  __ evpsrad(xmm5, k0, xmm1, 31, false, Assembler::AVX_512bit);\n+  __ evpsrad(xmm6, k0, xmm2, 31, false, Assembler::AVX_512bit);\n+  __ evpsrad(xmm7, k0, xmm3, 31, false, Assembler::AVX_512bit);\n+\n+  __ evpandd(xmm4, k0, xmm4, xmm30, false, Assembler::AVX_512bit);\n+  __ evpandd(xmm5, k0, xmm5, xmm30, false, Assembler::AVX_512bit);\n+  __ evpandd(xmm6, k0, xmm6, xmm30, false, Assembler::AVX_512bit);\n+  __ evpandd(xmm7, k0, xmm7, xmm30, false, Assembler::AVX_512bit);\n+\n+  __ evpaddd(xmm0, k0, xmm0, xmm4, false, Assembler::AVX_512bit);\n+  __ evpaddd(xmm1, k0, xmm1, xmm5, false, Assembler::AVX_512bit);\n+  __ evpaddd(xmm2, k0, xmm2, xmm6, false, Assembler::AVX_512bit);\n+  __ evpaddd(xmm3, k0, xmm3, xmm7, false, Assembler::AVX_512bit);\n+\n+            \/\/ rplus in xmm0\n+\n+\/\/           int quotient = (rplus * multiplier) >> 22;\n+  __ evpmulld(xmm4, k0, xmm0, xmm29, false, Assembler::AVX_512bit);\n+  __ evpmulld(xmm5, k0, xmm1, xmm29, false, Assembler::AVX_512bit);\n+  __ evpmulld(xmm6, k0, xmm2, xmm29, false, Assembler::AVX_512bit);\n+  __ evpmulld(xmm7, k0, xmm3, xmm29, false, Assembler::AVX_512bit);\n+\n+  __ evpsrad(xmm4, k0, xmm4, 22, false, Assembler::AVX_512bit);\n+  __ evpsrad(xmm5, k0, xmm5, 22, false, Assembler::AVX_512bit);\n+  __ evpsrad(xmm6, k0, xmm6, 22, false, Assembler::AVX_512bit);\n+  __ evpsrad(xmm7, k0, xmm7, 22, false, Assembler::AVX_512bit);\n+\n+            \/\/ quotient in xmm4\n+\n+\/\/            int r0 = rplus - quotient * twoGamma2;\n+  __ evpmulld(xmm8, k0, xmm4, xmm28, false, Assembler::AVX_512bit);\n+  __ evpmulld(xmm9, k0, xmm5, xmm28, false, Assembler::AVX_512bit);\n+  __ evpmulld(xmm10, k0, xmm6, xmm28, false, Assembler::AVX_512bit);\n+  __ evpmulld(xmm11, k0, xmm7, xmm28, false, Assembler::AVX_512bit);\n+\n+  __ evpsubd(xmm8, k0, xmm0, xmm8, false, Assembler::AVX_512bit);\n+  __ evpsubd(xmm9, k0, xmm1, xmm9, false, Assembler::AVX_512bit);\n+  __ evpsubd(xmm10, k0, xmm2, xmm10, false, Assembler::AVX_512bit);\n+  __ evpsubd(xmm11, k0, xmm3, xmm11, false, Assembler::AVX_512bit);\n+\n+            \/\/ r0 in xmm8\n+\n+\/\/            int mask = (twoGamma2 - r0) >> 22;\n+  __ evpsubd(xmm12, k0, xmm28, xmm8, false, Assembler::AVX_512bit);\n+  __ evpsubd(xmm13, k0, xmm28, xmm9, false, Assembler::AVX_512bit);\n+  __ evpsubd(xmm14, k0, xmm28, xmm10, false, Assembler::AVX_512bit);\n+  __ evpsubd(xmm15, k0, xmm28, xmm11, false, Assembler::AVX_512bit);\n+\n+  __ evpsrad(xmm12, k0, xmm12, 22, false, Assembler::AVX_512bit);\n+  __ evpsrad(xmm13, k0, xmm13, 22, false, Assembler::AVX_512bit);\n+  __ evpsrad(xmm14, k0, xmm14, 22, false, Assembler::AVX_512bit);\n+  __ evpsrad(xmm15, k0, xmm15, 22, false, Assembler::AVX_512bit);\n+\n+            \/\/ mask in xmm12\n+\n+\/\/            r0 -= (mask & twoGamma2);\n+  __ evpandd(xmm16, k0, xmm12, xmm28, false, Assembler::AVX_512bit);\n+  __ evpandd(xmm17, k0, xmm13, xmm28, false, Assembler::AVX_512bit);\n+  __ evpandd(xmm18, k0, xmm14, xmm28, false, Assembler::AVX_512bit);\n+  __ evpandd(xmm19, k0, xmm15, xmm28, false, Assembler::AVX_512bit);\n+\n+  __ evpsubd(xmm8, k0, xmm8, xmm16, false, Assembler::AVX_512bit);\n+  __ evpsubd(xmm9, k0, xmm9, xmm17, false, Assembler::AVX_512bit);\n+  __ evpsubd(xmm10, k0, xmm10, xmm18, false, Assembler::AVX_512bit);\n+  __ evpsubd(xmm11, k0, xmm11, xmm19, false, Assembler::AVX_512bit);\n+\n+            \/\/ r0 in xmm8\n+\n+\/\/            quotient += (mask & 1);\n+  __ evpandd(xmm16, k0, xmm12, xmm25, false, Assembler::AVX_512bit);\n+  __ evpandd(xmm17, k0, xmm13, xmm25, false, Assembler::AVX_512bit);\n+  __ evpandd(xmm18, k0, xmm14, xmm25, false, Assembler::AVX_512bit);\n+  __ evpandd(xmm19, k0, xmm15, xmm25, false, Assembler::AVX_512bit);\n+\n+  __ evpaddd(xmm4, k0, xmm4, xmm16, false, Assembler::AVX_512bit);\n+  __ evpaddd(xmm5, k0, xmm5, xmm17, false, Assembler::AVX_512bit);\n+  __ evpaddd(xmm6, k0, xmm6, xmm18, false, Assembler::AVX_512bit);\n+  __ evpaddd(xmm7, k0, xmm7, xmm19, false, Assembler::AVX_512bit);\n+\n+\/\/            mask = (twoGamma2 \/ 2 - r0) >> 31;\n+  __ evpsubd(xmm12, k0, xmm27, xmm8, false, Assembler::AVX_512bit);\n+  __ evpsubd(xmm13, k0, xmm27, xmm9, false, Assembler::AVX_512bit);\n+  __ evpsubd(xmm14, k0, xmm27, xmm10, false, Assembler::AVX_512bit);\n+  __ evpsubd(xmm15, k0, xmm27, xmm11, false, Assembler::AVX_512bit);\n+\n+  __ evpsrad(xmm12, k0, xmm12, 31, false, Assembler::AVX_512bit);\n+  __ evpsrad(xmm13, k0, xmm13, 31, false, Assembler::AVX_512bit);\n+  __ evpsrad(xmm14, k0, xmm14, 31, false, Assembler::AVX_512bit);\n+  __ evpsrad(xmm15, k0, xmm15, 31, false, Assembler::AVX_512bit);\n+\n+\/\/            r0 -= (mask & twoGamma2);\n+  __ evpandd(xmm16, k0, xmm12, xmm28, false, Assembler::AVX_512bit);\n+  __ evpandd(xmm17, k0, xmm13, xmm28, false, Assembler::AVX_512bit);\n+  __ evpandd(xmm18, k0, xmm14, xmm28, false, Assembler::AVX_512bit);\n+  __ evpandd(xmm19, k0, xmm15, xmm28, false, Assembler::AVX_512bit);\n+\n+  __ evpsubd(xmm8, k0, xmm8, xmm16, false, Assembler::AVX_512bit);\n+  __ evpsubd(xmm9, k0, xmm9, xmm17, false, Assembler::AVX_512bit);\n+  __ evpsubd(xmm10, k0, xmm10, xmm18, false, Assembler::AVX_512bit);\n+  __ evpsubd(xmm11, k0, xmm11, xmm19, false, Assembler::AVX_512bit);\n+\n+            \/\/ r0 in xmm8\n+\n+\/\/            quotient += (mask & 1);\n+  __ evpandd(xmm16, k0, xmm12, xmm25, false, Assembler::AVX_512bit);\n+  __ evpandd(xmm17, k0, xmm13, xmm25, false, Assembler::AVX_512bit);\n+  __ evpandd(xmm18, k0, xmm14, xmm25, false, Assembler::AVX_512bit);\n+  __ evpandd(xmm19, k0, xmm15, xmm25, false, Assembler::AVX_512bit);\n+\n+  __ evpaddd(xmm4, k0, xmm4, xmm16, false, Assembler::AVX_512bit);\n+  __ evpaddd(xmm5, k0, xmm5, xmm17, false, Assembler::AVX_512bit);\n+  __ evpaddd(xmm6, k0, xmm6, xmm18, false, Assembler::AVX_512bit);\n+  __ evpaddd(xmm7, k0, xmm7, xmm19, false, Assembler::AVX_512bit);\n+\n+            \/\/ quotient in xmm4\n+\n+\/\/            int r1 = rplus - r0 - (dilithium_q - 1);\n+  __ evpsubd(xmm16, k0, xmm0, xmm8, false, Assembler::AVX_512bit);\n+  __ evpsubd(xmm17, k0, xmm1, xmm9, false, Assembler::AVX_512bit);\n+  __ evpsubd(xmm18, k0, xmm2, xmm10, false, Assembler::AVX_512bit);\n+  __ evpsubd(xmm19, k0, xmm3, xmm11, false, Assembler::AVX_512bit);\n+\n+  __ evpsubd(xmm16, k0, xmm16, xmm26, false, Assembler::AVX_512bit);\n+  __ evpsubd(xmm17, k0, xmm17, xmm26, false, Assembler::AVX_512bit);\n+  __ evpsubd(xmm18, k0, xmm18, xmm26, false, Assembler::AVX_512bit);\n+  __ evpsubd(xmm19, k0, xmm19, xmm26, false, Assembler::AVX_512bit);\n+\n+            \/\/ r1 in xmm16\n+\n+\/\/            r1 = (r1 | (-r1)) >> 31; \/\/ 0 if rplus - r0 == (dilithium_q - 1), -1 otherwise\n+  __ evpsubd(xmm20, k0, xmm24, xmm16, false, Assembler::AVX_512bit);\n+  __ evpsubd(xmm21, k0, xmm24, xmm17, false, Assembler::AVX_512bit);\n+  __ evpsubd(xmm22, k0, xmm24, xmm18, false, Assembler::AVX_512bit);\n+  __ evpsubd(xmm23, k0, xmm24, xmm19, false, Assembler::AVX_512bit);\n+\n+  __ evporq(xmm16, k0, xmm16, xmm20, false, Assembler::AVX_512bit);\n+  __ evporq(xmm17, k0, xmm17, xmm21, false, Assembler::AVX_512bit);\n+  __ evporq(xmm18, k0, xmm18, xmm22, false, Assembler::AVX_512bit);\n+  __ evporq(xmm19, k0, xmm19, xmm23, false, Assembler::AVX_512bit);\n+\n+  __ evpsubd(xmm12, k0, xmm24, xmm25, false, Assembler::AVX_512bit); \/\/ -1\n+\n+  __ evpsrad(xmm0, k0, xmm16, 31, false, Assembler::AVX_512bit);\n+  __ evpsrad(xmm1, k0, xmm17, 31, false, Assembler::AVX_512bit);\n+  __ evpsrad(xmm2, k0, xmm18, 31, false, Assembler::AVX_512bit);\n+  __ evpsrad(xmm3, k0, xmm19, 31, false, Assembler::AVX_512bit);\n+\n+            \/\/ r1 in xmm0\n+\n+\/\/            r0 += ~r1;\n+  __ evpxorq(xmm20, k0, xmm0, xmm12, false, Assembler::AVX_512bit);\n+  __ evpxorq(xmm21, k0, xmm1, xmm12, false, Assembler::AVX_512bit);\n+  __ evpxorq(xmm22, k0, xmm2, xmm12, false, Assembler::AVX_512bit);\n+  __ evpxorq(xmm23, k0, xmm3, xmm12, false, Assembler::AVX_512bit);\n+\n+  __ evpaddd(xmm8, k0, xmm8, xmm20, false, Assembler::AVX_512bit);\n+  __ evpaddd(xmm9, k0, xmm9, xmm21, false, Assembler::AVX_512bit);\n+  __ evpaddd(xmm10, k0, xmm10, xmm22, false, Assembler::AVX_512bit);\n+  __ evpaddd(xmm11, k0, xmm11, xmm23, false, Assembler::AVX_512bit);\n+\n+            \/\/ r0 in xmm8\n+\n+\/\/            r1 = r1 & quotient;\n+  __ evpandd(xmm0, k0, xmm4, xmm0, false, Assembler::AVX_512bit);\n+  __ evpandd(xmm1, k0, xmm5, xmm1, false, Assembler::AVX_512bit);\n+  __ evpandd(xmm2, k0, xmm6, xmm2, false, Assembler::AVX_512bit);\n+  __ evpandd(xmm3, k0, xmm7, xmm3, false, Assembler::AVX_512bit);\n+\n+\/\/             r1 in xmm0\n+\n+\/\/            lowPart[m] = r0;\n+\/\/            highPart[m] = r1;\n+  __ evmovdqul(Address(highPart, 0), xmm0, Assembler::AVX_512bit);\n+  __ evmovdqul(Address(highPart, 64), xmm1, Assembler::AVX_512bit);\n+  __ evmovdqul(Address(highPart, 128), xmm2, Assembler::AVX_512bit);\n+  __ evmovdqul(Address(highPart, 192), xmm3, Assembler::AVX_512bit);\n+\n+  __ evmovdqul(Address(lowPart, 0), xmm8, Assembler::AVX_512bit);\n+  __ evmovdqul(Address(lowPart, 64), xmm9, Assembler::AVX_512bit);\n+  __ evmovdqul(Address(lowPart, 128), xmm10, Assembler::AVX_512bit);\n+  __ evmovdqul(Address(lowPart, 192), xmm11, Assembler::AVX_512bit);\n+\n+  __ subl(len, 256);\n+  __ addptr(highPart, 256);\n+  __ addptr(lowPart, 256);\n+  __ cmpl(len, 0);\n+  __ jcc(Assembler::notEqual, L_loop);\n+\n+  __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n+  __ mov64(rax, 0); \/\/ return 0\n+  __ ret(0);\n+\n+  return start;\n+}\n","filename":"src\/hotspot\/cpu\/x86\/stubGenerator_x86_64_dilithium.cpp","additions":1399,"deletions":0,"binary":false,"changes":1399,"status":"added"},{"patch":"@@ -85,0 +85,1 @@\n+    StubRoutines::_double_keccak         = generate_double_keccak();\n@@ -260,1 +261,1 @@\n-  \/\/ rho and sigma steps).\n+  \/\/ rho and pi steps).\n@@ -282,1 +283,1 @@\n-  \/\/ The combined rho and sigma steps are done.\n+  \/\/ The combined rho and pi steps are done.\n@@ -338,0 +339,203 @@\n+\n+\/\/ Inputs:\n+\/\/   c_rarg0   - long[]  state0\n+\/\/   c_rarg1   - long[]  state1\n+\/\/\n+\/\/ Performs two keccak() computations in parallel. The steps of the\n+\/\/ two computations are executed interleaved.\n+address StubGenerator::generate_double_keccak() {\n+  __ align(CodeEntryAlignment);\n+  StubGenStubId stub_id = double_keccak_id;\n+  StubCodeMark mark(this, stub_id);\n+  address start = __ pc();\n+\n+  const Register state0 = c_rarg0;\n+  const Register state1 = c_rarg1;\n+\n+  const Register permsAndRots = c_rarg2;\n+  const Register round_consts = c_rarg3;\n+  const Register constant2use = r10;\n+  const Register roundsLeft = r11;\n+\n+  Label rounds24_loop;\n+\n+  __ enter();\n+\n+  __ lea(permsAndRots, ExternalAddress(permsAndRotsAddr()));\n+  __ lea(round_consts, ExternalAddress(round_constsAddr()));\n+\n+  \/\/ set up the masks\n+  __ mov64(rax,1);\n+  __ kmovbl(k1, rax);\n+  __ addl(rax,2);\n+  __ kmovbl(k2, rax);\n+  __ addl(rax, 4);\n+  __ kmovbl(k3, rax);\n+  __ addl(rax, 8);\n+  __ kmovbl(k4, rax);\n+  __ addl(rax, 16);\n+  __ kmovbl(k5, rax);\n+\n+  \/\/ load the states\n+  __ evmovdquq(xmm0, k5, Address(state0, 0), false, Assembler::AVX_512bit);\n+  __ evmovdquq(xmm1, k5, Address(state0, 40), false, Assembler::AVX_512bit);\n+  __ evmovdquq(xmm2, k5, Address(state0, 80), false, Assembler::AVX_512bit);\n+  __ evmovdquq(xmm3, k5, Address(state0, 120), false, Assembler::AVX_512bit);\n+  __ evmovdquq(xmm4, k5, Address(state0, 160), false, Assembler::AVX_512bit);\n+\n+  __ evmovdquq(xmm10, k5, Address(state1, 0), false, Assembler::AVX_512bit);\n+  __ evmovdquq(xmm11, k5, Address(state1, 40), false, Assembler::AVX_512bit);\n+  __ evmovdquq(xmm12, k5, Address(state1, 80), false, Assembler::AVX_512bit);\n+  __ evmovdquq(xmm13, k5, Address(state1, 120), false, Assembler::AVX_512bit);\n+  __ evmovdquq(xmm14, k5, Address(state1, 160), false, Assembler::AVX_512bit);\n+\n+  \/\/ load the permutation and rotation constants\n+  __ evmovdquq(xmm17, Address(permsAndRots, 0), Assembler::AVX_512bit);\n+  __ evmovdquq(xmm18, Address(permsAndRots, 64), Assembler::AVX_512bit);\n+  __ evmovdquq(xmm19, Address(permsAndRots, 128), Assembler::AVX_512bit);\n+  __ evmovdquq(xmm20, Address(permsAndRots, 192), Assembler::AVX_512bit);\n+  __ evmovdquq(xmm21, Address(permsAndRots, 256), Assembler::AVX_512bit);\n+  __ evmovdquq(xmm22, Address(permsAndRots, 320), Assembler::AVX_512bit);\n+  __ evmovdquq(xmm23, Address(permsAndRots, 384), Assembler::AVX_512bit);\n+  __ evmovdquq(xmm24, Address(permsAndRots, 448), Assembler::AVX_512bit);\n+  __ evmovdquq(xmm25, Address(permsAndRots, 512), Assembler::AVX_512bit);\n+  __ evmovdquq(xmm26, Address(permsAndRots, 576), Assembler::AVX_512bit);\n+  __ evmovdquq(xmm27, Address(permsAndRots, 640), Assembler::AVX_512bit);\n+  __ evmovdquq(xmm28, Address(permsAndRots, 704), Assembler::AVX_512bit);\n+  __ evmovdquq(xmm29, Address(permsAndRots, 768), Assembler::AVX_512bit);\n+  __ evmovdquq(xmm30, Address(permsAndRots, 832), Assembler::AVX_512bit);\n+  __ evmovdquq(xmm31, Address(permsAndRots, 896), Assembler::AVX_512bit);\n+\n+  \/\/ there will be 24 keccak rounds\n+  \/\/ The same operations as the ones in generate_sha3_implCompress are\n+  \/\/ performed, but in parallel for two states: one in regs z0-z5, using z6\n+  \/\/ as the scratch register and the other in z10-z15, using z16 as the\n+  \/\/ scratch register.\n+  \/\/ The permutation and rotation constants, that are loaded into z17-z31,\n+  \/\/ are shared between the two computations.\n+  __ movl(roundsLeft, 24);\n+  \/\/ load round_constants base\n+  __ movptr(constant2use, round_consts);\n+\n+  __ BIND(rounds24_loop);\n+  __ subl( roundsLeft, 1);\n+\n+  __ evmovdquw(xmm5, xmm0, Assembler::AVX_512bit);\n+  __ evmovdquw(xmm15, xmm10, Assembler::AVX_512bit);\n+  __ vpternlogq(xmm5, 150, xmm1, xmm2, Assembler::AVX_512bit);\n+  __ vpternlogq(xmm15, 150, xmm11, xmm12, Assembler::AVX_512bit);\n+  __ vpternlogq(xmm5, 150, xmm3, xmm4, Assembler::AVX_512bit);\n+  __ vpternlogq(xmm15, 150, xmm13, xmm14, Assembler::AVX_512bit);\n+  __ evprolq(xmm6, xmm5, 1, Assembler::AVX_512bit);\n+  __ evprolq(xmm16, xmm15, 1, Assembler::AVX_512bit);\n+  __ evpermt2q(xmm5, xmm30, xmm5, Assembler::AVX_512bit);\n+  __ evpermt2q(xmm15, xmm30, xmm15, Assembler::AVX_512bit);\n+  __ evpermt2q(xmm6, xmm31, xmm6, Assembler::AVX_512bit);\n+  __ evpermt2q(xmm16, xmm31, xmm16, Assembler::AVX_512bit);\n+  __ vpternlogq(xmm0, 150, xmm5, xmm6, Assembler::AVX_512bit);\n+  __ vpternlogq(xmm10, 150, xmm15, xmm16, Assembler::AVX_512bit);\n+  __ vpternlogq(xmm1, 150, xmm5, xmm6, Assembler::AVX_512bit);\n+  __ vpternlogq(xmm11, 150, xmm15, xmm16, Assembler::AVX_512bit);\n+  __ vpternlogq(xmm2, 150, xmm5, xmm6, Assembler::AVX_512bit);\n+  __ vpternlogq(xmm12, 150, xmm15, xmm16, Assembler::AVX_512bit);\n+  __ vpternlogq(xmm3, 150, xmm5, xmm6, Assembler::AVX_512bit);\n+  __ vpternlogq(xmm13, 150, xmm15, xmm16, Assembler::AVX_512bit);\n+  __ vpternlogq(xmm4, 150, xmm5, xmm6, Assembler::AVX_512bit);\n+  __ vpternlogq(xmm14, 150, xmm15, xmm16, Assembler::AVX_512bit);\n+  __ evpermt2q(xmm4, xmm17, xmm3, Assembler::AVX_512bit);\n+  __ evpermt2q(xmm14, xmm17, xmm13, Assembler::AVX_512bit);\n+  __ evpermt2q(xmm3, xmm18, xmm2, Assembler::AVX_512bit);\n+  __ evpermt2q(xmm13, xmm18, xmm12, Assembler::AVX_512bit);\n+  __ evpermt2q(xmm2, xmm17, xmm1, Assembler::AVX_512bit);\n+  __ evpermt2q(xmm12, xmm17, xmm11, Assembler::AVX_512bit);\n+  __ evpermt2q(xmm1, xmm19, xmm0, Assembler::AVX_512bit);\n+  __ evpermt2q(xmm11, xmm19, xmm10, Assembler::AVX_512bit);\n+  __ evpermt2q(xmm4, xmm20, xmm2, Assembler::AVX_512bit);\n+  __ evpermt2q(xmm14, xmm20, xmm12, Assembler::AVX_512bit);\n+  __ evprolvq(xmm1, xmm1, xmm27, Assembler::AVX_512bit);\n+  __ evprolvq(xmm11, xmm11, xmm27, Assembler::AVX_512bit);\n+  __ evprolvq(xmm3, xmm3, xmm28, Assembler::AVX_512bit);\n+  __ evprolvq(xmm13, xmm13, xmm28, Assembler::AVX_512bit);\n+  __ evprolvq(xmm4, xmm4, xmm29, Assembler::AVX_512bit);\n+  __ evprolvq(xmm14, xmm14, xmm29, Assembler::AVX_512bit);\n+  __ evmovdquw(xmm2, xmm1, Assembler::AVX_512bit);\n+  __ evmovdquw(xmm12, xmm11, Assembler::AVX_512bit);\n+  __ evmovdquw(xmm5, xmm3, Assembler::AVX_512bit);\n+  __ evmovdquw(xmm15, xmm13, Assembler::AVX_512bit);\n+  __ evpermt2q(xmm0, xmm21, xmm4, Assembler::AVX_512bit);\n+  __ evpermt2q(xmm10, xmm21, xmm14, Assembler::AVX_512bit);\n+  __ evpermt2q(xmm1, xmm22, xmm3, Assembler::AVX_512bit);\n+  __ evpermt2q(xmm11, xmm22, xmm13, Assembler::AVX_512bit);\n+  __ evpermt2q(xmm5, xmm22, xmm2, Assembler::AVX_512bit);\n+  __ evpermt2q(xmm15, xmm22, xmm12, Assembler::AVX_512bit);\n+  __ evmovdquw(xmm3, xmm1, Assembler::AVX_512bit);\n+  __ evmovdquw(xmm13, xmm11, Assembler::AVX_512bit);\n+  __ evmovdquw(xmm2, xmm5, Assembler::AVX_512bit);\n+  __ evmovdquw(xmm12, xmm15, Assembler::AVX_512bit);\n+  __ evpermt2q(xmm1, xmm23, xmm4, Assembler::AVX_512bit);\n+  __ evpermt2q(xmm11, xmm23, xmm14, Assembler::AVX_512bit);\n+  __ evpermt2q(xmm2, xmm24, xmm4, Assembler::AVX_512bit);\n+  __ evpermt2q(xmm12, xmm24, xmm14, Assembler::AVX_512bit);\n+  __ evpermt2q(xmm3, xmm25, xmm4, Assembler::AVX_512bit);\n+  __ evpermt2q(xmm13, xmm25, xmm14, Assembler::AVX_512bit);\n+  __ evpermt2q(xmm4, xmm26, xmm5, Assembler::AVX_512bit);\n+  __ evpermt2q(xmm14, xmm26, xmm15, Assembler::AVX_512bit);\n+\n+  __ evpermt2q(xmm5, xmm31, xmm0, Assembler::AVX_512bit);\n+  __ evpermt2q(xmm15, xmm31, xmm10, Assembler::AVX_512bit);\n+  __ evpermt2q(xmm6, xmm31, xmm5, Assembler::AVX_512bit);\n+  __ evpermt2q(xmm16, xmm31, xmm15, Assembler::AVX_512bit);\n+  __ vpternlogq(xmm0, 180, xmm6, xmm5, Assembler::AVX_512bit);\n+  __ vpternlogq(xmm10, 180, xmm16, xmm15, Assembler::AVX_512bit);\n+\n+  __ evpermt2q(xmm5, xmm31, xmm1, Assembler::AVX_512bit);\n+  __ evpermt2q(xmm15, xmm31, xmm11, Assembler::AVX_512bit);\n+  __ evpermt2q(xmm6, xmm31, xmm5, Assembler::AVX_512bit);\n+  __ evpermt2q(xmm16, xmm31, xmm15, Assembler::AVX_512bit);\n+  __ vpternlogq(xmm1, 180, xmm6, xmm5, Assembler::AVX_512bit);\n+  __ vpternlogq(xmm11, 180, xmm16, xmm15, Assembler::AVX_512bit);\n+\n+  __ evpxorq(xmm0, k1, xmm0, Address(constant2use, 0), true, Assembler::AVX_512bit);\n+  __ evpxorq(xmm10, k1, xmm10, Address(constant2use, 0), true, Assembler::AVX_512bit);\n+  __ addptr(constant2use, 8);\n+\n+  __ evpermt2q(xmm5, xmm31, xmm2, Assembler::AVX_512bit);\n+  __ evpermt2q(xmm15, xmm31, xmm12, Assembler::AVX_512bit);\n+  __ evpermt2q(xmm6, xmm31, xmm5, Assembler::AVX_512bit);\n+  __ evpermt2q(xmm16, xmm31, xmm15, Assembler::AVX_512bit);\n+  __ vpternlogq(xmm2, 180, xmm6, xmm5, Assembler::AVX_512bit);\n+  __ vpternlogq(xmm12, 180, xmm16, xmm15, Assembler::AVX_512bit);\n+\n+  __ evpermt2q(xmm5, xmm31, xmm3, Assembler::AVX_512bit);\n+  __ evpermt2q(xmm15, xmm31, xmm13, Assembler::AVX_512bit);\n+  __ evpermt2q(xmm6, xmm31, xmm5, Assembler::AVX_512bit);\n+  __ evpermt2q(xmm16, xmm31, xmm15, Assembler::AVX_512bit);\n+  __ vpternlogq(xmm3, 180, xmm6, xmm5, Assembler::AVX_512bit);\n+  __ vpternlogq(xmm13, 180, xmm16, xmm15, Assembler::AVX_512bit);\n+  __ evpermt2q(xmm5, xmm31, xmm4, Assembler::AVX_512bit);\n+  __ evpermt2q(xmm15, xmm31, xmm14, Assembler::AVX_512bit);\n+  __ evpermt2q(xmm6, xmm31, xmm5, Assembler::AVX_512bit);\n+  __ evpermt2q(xmm16, xmm31, xmm15, Assembler::AVX_512bit);\n+  __ vpternlogq(xmm4, 180, xmm6, xmm5, Assembler::AVX_512bit);\n+  __ vpternlogq(xmm14, 180, xmm16, xmm15, Assembler::AVX_512bit);\n+  __ cmpl(roundsLeft, 0);\n+  __ jcc(Assembler::notEqual, rounds24_loop);\n+\n+  \/\/ store the states\n+  __ evmovdquq(Address(state0, 0), k5, xmm0, true, Assembler::AVX_512bit);\n+  __ evmovdquq(Address(state0, 40), k5, xmm1, true, Assembler::AVX_512bit);\n+  __ evmovdquq(Address(state0, 80), k5, xmm2, true, Assembler::AVX_512bit);\n+  __ evmovdquq(Address(state0, 120), k5, xmm3, true, Assembler::AVX_512bit);\n+  __ evmovdquq(Address(state0, 160), k5, xmm4, true, Assembler::AVX_512bit);\n+\n+  __ evmovdquq(Address(state1, 0), k5, xmm10, true, Assembler::AVX_512bit);\n+  __ evmovdquq(Address(state1, 40), k5, xmm11, true, Assembler::AVX_512bit);\n+  __ evmovdquq(Address(state1, 80), k5, xmm12, true, Assembler::AVX_512bit);\n+  __ evmovdquq(Address(state1, 120), k5, xmm13, true, Assembler::AVX_512bit);\n+  __ evmovdquq(Address(state1, 160), k5, xmm14, true, Assembler::AVX_512bit);\n+\n+  __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n+  __ ret(0);\n+\n+  return start;\n+}\n","filename":"src\/hotspot\/cpu\/x86\/stubGenerator_x86_64_sha3.cpp","additions":206,"deletions":2,"binary":false,"changes":208,"status":"modified"},{"patch":"@@ -1249,0 +1249,14 @@\n+  \/\/ Dilithium Intrinsics\n+  \/\/ Currently we only have them for AVX512\n+#ifdef _LP64\n+  if (supports_evex() && supports_avx512bw()) {\n+      if (FLAG_IS_DEFAULT(UseDilithiumIntrinsics)) {\n+          UseDilithiumIntrinsics = true;\n+      }\n+  } else\n+#endif\n+   if (UseDilithiumIntrinsics) {\n+      warning(\"Intrinsics for ML-DSA are not available on this CPU.\");\n+      FLAG_SET_DEFAULT(UseDilithiumIntrinsics, false);\n+  }\n+\n","filename":"src\/hotspot\/cpu\/x86\/vm_version_x86.cpp","additions":14,"deletions":0,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -478,0 +478,1 @@\n+  case vmIntrinsics::_double_keccak:\n@@ -490,0 +491,7 @@\n+  case vmIntrinsics::_dilithiumAlmostNtt:\n+  case vmIntrinsics::_dilithiumAlmostInverseNtt:\n+  case vmIntrinsics::_dilithiumNttMult:\n+  case vmIntrinsics::_dilithiumMontMulByConstant:\n+  case vmIntrinsics::_dilithiumDecomposePoly:\n+    if (!UseDilithiumIntrinsics) return true;\n+    break;\n","filename":"src\/hotspot\/share\/classfile\/vmIntrinsics.cpp","additions":8,"deletions":0,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -519,0 +519,6 @@\n+  \/* support for sun.security.provider.SHAKE128Parallel *\/                                                              \\\n+  do_class(sun_security_provider_sha3_parallel,                \"sun\/security\/provider\/SHA3Parallel\")                    \\\n+   do_intrinsic(_double_keccak, sun_security_provider_sha3_parallel, double_keccak_name, double_keccak_signature, F_S)   \\\n+   do_name(     double_keccak_name,                                 \"doubleKeccak\")                                     \\\n+   do_signature(double_keccak_signature,                            \"([J[J)I\")                                          \\\n+                                                                                                                        \\\n@@ -564,0 +570,20 @@\n+  \/* support for sun.security.provider.ML_DSA *\/                                                                        \\\n+  do_class(sun_security_provider_ML_DSA,      \"sun\/security\/provider\/ML_DSA\")                                           \\\n+   do_signature(IaII_signature, \"([II)I\")                                                                               \\\n+   do_signature(IaIaI_signature, \"([I[I)I\")                                                                             \\\n+   do_signature(IaIaIaI_signature, \"([I[I[I)I\")                                                                         \\\n+   do_signature(IaIaIaIII_signature, \"([I[I[III)I\")                                                                     \\\n+  do_intrinsic(_dilithiumAlmostNtt, sun_security_provider_ML_DSA, dilithiumAlmostNtt_name, IaIaI_signature, F_S)        \\\n+   do_name(dilithiumAlmostNtt_name,                            \"implDilithiumAlmostNtt\")                                \\\n+  do_intrinsic(_dilithiumAlmostInverseNtt, sun_security_provider_ML_DSA,                                                \\\n+                dilithiumAlmostInverseNtt_name, IaIaI_signature, F_S)                                                   \\\n+   do_name(dilithiumAlmostInverseNtt_name,                     \"implDilithiumAlmostInverseNtt\")                         \\\n+  do_intrinsic(_dilithiumNttMult, sun_security_provider_ML_DSA, dilithiumNttMult_name, IaIaIaI_signature, F_S)          \\\n+   do_name(dilithiumNttMult_name,                              \"implDilithiumNttMult\")                                  \\\n+  do_intrinsic(_dilithiumMontMulByConstant, sun_security_provider_ML_DSA,                                               \\\n+                dilithiumMontMulByConstant_name, IaII_signature, F_S)                                                   \\\n+   do_name(dilithiumMontMulByConstant_name,                    \"implDilithiumMontMulByConstant\")                        \\\n+  do_intrinsic(_dilithiumDecomposePoly, sun_security_provider_ML_DSA,                                                   \\\n+                dilithiumDecomposePoly_name, IaIaIaIII_signature, F_S)                                                  \\\n+   do_name(dilithiumDecomposePoly_name,                    \"implDilithiumDecomposePoly\")                                \\\n+                                                                                                                        \\\n","filename":"src\/hotspot\/share\/classfile\/vmIntrinsics.hpp","additions":26,"deletions":0,"binary":false,"changes":26,"status":"modified"},{"patch":"@@ -397,0 +397,1 @@\n+  static_field(StubRoutines,                _double_keccak,                                   address)                               \\\n@@ -398,0 +399,5 @@\n+  static_field(StubRoutines,                _dilithiumAlmostNtt,                              address)                               \\\n+  static_field(StubRoutines,                _dilithiumAlmostInverseNtt,                       address)                               \\\n+  static_field(StubRoutines,                _dilithiumNttMult,                                address)                               \\\n+  static_field(StubRoutines,                _dilithiumMontMulByConstant,                      address)                               \\\n+  static_field(StubRoutines,                _dilithiumDecomposePoly,                          address)                               \\\n","filename":"src\/hotspot\/share\/jvmci\/vmStructs_jvmci.cpp","additions":6,"deletions":0,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -783,0 +783,1 @@\n+  case vmIntrinsics::_double_keccak:\n@@ -792,0 +793,5 @@\n+  case vmIntrinsics::_dilithiumAlmostNtt:\n+  case vmIntrinsics::_dilithiumAlmostInverseNtt:\n+  case vmIntrinsics::_dilithiumNttMult:\n+  case vmIntrinsics::_dilithiumMontMulByConstant:\n+  case vmIntrinsics::_dilithiumDecomposePoly:\n","filename":"src\/hotspot\/share\/opto\/c2compiler.cpp","additions":6,"deletions":0,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -2195,0 +2195,5 @@\n+                  strcmp(call->as_CallLeaf()->_name, \"dilithiumAlmostNtt\") == 0 ||\n+                  strcmp(call->as_CallLeaf()->_name, \"dilithiumAlmostInverseNtt\") == 0 ||\n+                  strcmp(call->as_CallLeaf()->_name, \"dilithiumNttMult\") == 0 ||\n+                  strcmp(call->as_CallLeaf()->_name, \"dilithiumMontMulByConstant\") == 0 ||\n+                  strcmp(call->as_CallLeaf()->_name, \"dilithiumDecomposePoly\") == 0 ||\n@@ -2206,0 +2211,1 @@\n+                  strcmp(call->as_CallLeaf()->_name, \"double_keccak\") == 0 ||\n","filename":"src\/hotspot\/share\/opto\/escape.cpp","additions":6,"deletions":0,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -597,0 +597,2 @@\n+  case vmIntrinsics::_double_keccak:\n+    return inline_double_keccak();\n@@ -627,0 +629,10 @@\n+  case vmIntrinsics::_dilithiumAlmostNtt:\n+    return inline_dilithiumAlmostNtt();\n+  case vmIntrinsics::_dilithiumAlmostInverseNtt:\n+    return inline_dilithiumAlmostInverseNtt();\n+  case vmIntrinsics::_dilithiumNttMult:\n+    return inline_dilithiumNttMult();\n+  case vmIntrinsics::_dilithiumMontMulByConstant:\n+    return inline_dilithiumMontMulByConstant();\n+  case vmIntrinsics::_dilithiumDecomposePoly:\n+    return inline_dilithiumDecomposePoly();\n@@ -7591,0 +7603,170 @@\n+\/\/------------------------------inline_dilithiumAlmostNtt\n+bool LibraryCallKit::inline_dilithiumAlmostNtt() {\n+  address stubAddr;\n+  const char *stubName;\n+  assert(UseDilithiumIntrinsics, \"need Dilithium intrinsics support\");\n+  assert(callee()->signature()->size() == 2, \"dilithiumAlmostNtt has 2 parameters\");\n+\n+  stubAddr = StubRoutines::dilithiumAlmostNtt();\n+  stubName = \"dilithiumAlmostNtt\";\n+  if (!stubAddr) return false;\n+\n+  Node* coeffs          = argument(0);\n+  Node* ntt_zetas        = argument(1);\n+\n+  coeffs = must_be_not_null(coeffs, true);\n+  ntt_zetas = must_be_not_null(ntt_zetas, true);\n+\n+  Node* coeffs_start  = array_element_address(coeffs, intcon(0), T_INT);\n+  assert(coeffs_start, \"coeffs is null\");\n+  Node* ntt_zetas_start  = array_element_address(ntt_zetas, intcon(0), T_INT);\n+  assert(ntt_zetas_start, \"ntt_zetas is null\");\n+  Node* dilithiumAlmostNtt = make_runtime_call(RC_LEAF|RC_NO_FP,\n+                                  OptoRuntime::dilithiumAlmostNtt_Type(),\n+                                  stubAddr, stubName, TypePtr::BOTTOM,\n+                                  coeffs_start, ntt_zetas_start);\n+  \/\/ return an int\n+  Node* retvalue = _gvn.transform(new ProjNode(dilithiumAlmostNtt, TypeFunc::Parms));\n+  set_result(retvalue);\n+  return true;\n+}\n+\n+\/\/------------------------------inline_dilithiumAlmostInverseNtt\n+bool LibraryCallKit::inline_dilithiumAlmostInverseNtt() {\n+  address stubAddr;\n+  const char *stubName;\n+  assert(UseDilithiumIntrinsics, \"need Dilithium intrinsics support\");\n+  assert(callee()->signature()->size() == 2, \"dilithiumAlmostInverseNtt has 2 parameters\");\n+\n+  stubAddr = StubRoutines::dilithiumAlmostInverseNtt();\n+  stubName = \"dilithiumAlmostInverseNtt\";\n+  if (!stubAddr) return false;\n+\n+  Node* coeffs          = argument(0);\n+  Node* zetas           = argument(1);\n+\n+  coeffs = must_be_not_null(coeffs, true);\n+  zetas = must_be_not_null(zetas, true);\n+\n+  Node* coeffs_start  = array_element_address(coeffs, intcon(0), T_INT);\n+  assert(coeffs_start, \"coeffs is null\");\n+  Node* zetas_start  = array_element_address(zetas, intcon(0), T_INT);\n+  assert(zetas_start, \"inverseNtt_zetas is null\");\n+  Node* dilithiumAlmostInverseNtt = make_runtime_call(RC_LEAF|RC_NO_FP,\n+                                  OptoRuntime::dilithiumAlmostInverseNtt_Type(),\n+                                  stubAddr, stubName, TypePtr::BOTTOM,\n+                                  coeffs_start, zetas_start);\n+\n+  \/\/ return an int\n+  Node* retvalue = _gvn.transform(new ProjNode(dilithiumAlmostInverseNtt, TypeFunc::Parms));\n+  set_result(retvalue);\n+  return true;\n+}\n+\n+\/\/------------------------------inline_dilithiumNttMult\n+bool LibraryCallKit::inline_dilithiumNttMult() {\n+  address stubAddr;\n+  const char *stubName;\n+  assert(UseDilithiumIntrinsics, \"need Dilithium intrinsics support\");\n+  assert(callee()->signature()->size() == 3, \"dilithiumNttMult has 3 parameters\");\n+\n+  stubAddr = StubRoutines::dilithiumNttMult();\n+  stubName = \"dilithiumNttMult\";\n+  if (!stubAddr) return false;\n+\n+  Node* result          = argument(0);\n+  Node* ntta            = argument(1);\n+  Node* nttb            = argument(2);\n+\n+  result = must_be_not_null(result, true);\n+  ntta = must_be_not_null(ntta, true);\n+  nttb = must_be_not_null(nttb, true);\n+\n+  Node* result_start  = array_element_address(result, intcon(0), T_INT);\n+  assert(result_start, \"result is null\");\n+  Node* ntta_start  = array_element_address(ntta, intcon(0), T_INT);\n+  assert(ntta_start, \"ntta is null\");\n+  Node* nttb_start  = array_element_address(nttb, intcon(0), T_INT);\n+  assert(nttb_start, \"nttb is null\");\n+  Node* dilithiumNttMult = make_runtime_call(RC_LEAF|RC_NO_FP,\n+                                  OptoRuntime::dilithiumNttMult_Type(),\n+                                  stubAddr, stubName, TypePtr::BOTTOM,\n+                                  result_start, ntta_start, nttb_start);\n+\n+  \/\/ return an int\n+  Node* retvalue = _gvn.transform(new ProjNode(dilithiumNttMult, TypeFunc::Parms));\n+  set_result(retvalue);\n+\n+  return true;\n+}\n+\n+\/\/------------------------------inline_dilithiumMontMulByConstant\n+bool LibraryCallKit::inline_dilithiumMontMulByConstant() {\n+  address stubAddr;\n+  const char *stubName;\n+  assert(UseDilithiumIntrinsics, \"need Dilithium intrinsics support\");\n+  assert(callee()->signature()->size() == 2, \"dilithiumMontMulByConstant has 2 parameters\");\n+\n+  stubAddr = StubRoutines::dilithiumMontMulByConstant();\n+  stubName = \"dilithiumMontMulByConstant\";\n+  if (!stubAddr) return false;\n+\n+  Node* coeffs          = argument(0);\n+  Node* constant        = argument(1);\n+\n+  coeffs = must_be_not_null(coeffs, true);\n+\n+  Node* coeffs_start  = array_element_address(coeffs, intcon(0), T_INT);\n+  assert(coeffs_start, \"coeffs is null\");\n+  Node* dilithiumMontMulByConstant = make_runtime_call(RC_LEAF|RC_NO_FP,\n+                                  OptoRuntime::dilithiumMontMulByConstant_Type(),\n+                                  stubAddr, stubName, TypePtr::BOTTOM,\n+                                  coeffs_start, constant);\n+\n+  \/\/ return an int\n+  Node* retvalue = _gvn.transform(new ProjNode(dilithiumMontMulByConstant, TypeFunc::Parms));\n+  set_result(retvalue);\n+  return true;\n+}\n+\n+\n+\/\/------------------------------inline_dilithiumDecomposePoly\n+bool LibraryCallKit::inline_dilithiumDecomposePoly() {\n+  address stubAddr;\n+  const char *stubName;\n+  assert(UseDilithiumIntrinsics, \"need Dilithium intrinsics support\");\n+  assert(callee()->signature()->size() == 5, \"dilithiumDecomposePoly has 5 parameters\");\n+\n+  stubAddr = StubRoutines::dilithiumDecomposePoly();\n+  stubName = \"dilithiumDecomposePoly\";\n+  if (!stubAddr) return false;\n+\n+  Node* input          = argument(0);\n+  Node* lowPart        = argument(1);\n+  Node* highPart       = argument(2);\n+  Node* twoGamma2      = argument(3);\n+  Node* multiplier     = argument(4);\n+\n+  input = must_be_not_null(input, true);\n+  lowPart = must_be_not_null(lowPart, true);\n+  highPart = must_be_not_null(highPart, true);\n+\n+  Node* input_start  = array_element_address(input, intcon(0), T_INT);\n+  assert(input_start, \"input is null\");\n+  Node* lowPart_start  = array_element_address(lowPart, intcon(0), T_INT);\n+  assert(lowPart_start, \"lowPart is null\");\n+  Node* highPart_start  = array_element_address(highPart, intcon(0), T_INT);\n+  assert(highPart_start, \"highPart is null\");\n+\n+  Node* dilithiumDecomposePoly = make_runtime_call(RC_LEAF|RC_NO_FP,\n+                                  OptoRuntime::dilithiumDecomposePoly_Type(),\n+                                  stubAddr, stubName, TypePtr::BOTTOM,\n+                                  input_start, lowPart_start, highPart_start,\n+                                  twoGamma2, multiplier);\n+\n+  \/\/ return an int\n+  Node* retvalue = _gvn.transform(new ProjNode(dilithiumDecomposePoly, TypeFunc::Parms));\n+  set_result(retvalue);\n+  return true;\n+}\n+\n@@ -7854,0 +8036,32 @@\n+\/\/------------------------------inline_double_keccak\n+bool LibraryCallKit::inline_double_keccak() {\n+  address stubAddr;\n+  const char *stubName;\n+  assert(UseSHA3Intrinsics, \"need SHA3 intrinsics support\");\n+  assert(callee()->signature()->size() == 2, \"double_keccak has 2 parameters\");\n+\n+  stubAddr = StubRoutines::double_keccak();\n+  stubName = \"double_keccak\";\n+  if (!stubAddr) return false;\n+\n+  Node* status0        = argument(0);\n+  Node* status1        = argument(1);\n+\n+  status0 = must_be_not_null(status0, true);\n+  status1 = must_be_not_null(status1, true);\n+\n+  Node* status0_start  = array_element_address(status0, intcon(0), T_LONG);\n+  assert(status0_start, \"status0 is null\");\n+  Node* status1_start  = array_element_address(status1, intcon(0), T_LONG);\n+  assert(status1_start, \"status1 is null\");\n+  Node* double_keccak = make_runtime_call(RC_LEAF|RC_NO_FP,\n+                                  OptoRuntime::double_keccak_Type(),\n+                                  stubAddr, stubName, TypePtr::BOTTOM,\n+                                  status0_start, status1_start);\n+  \/\/ return an int\n+  Node* retvalue = _gvn.transform(new ProjNode(double_keccak, TypeFunc::Parms));\n+  set_result(retvalue);\n+  return true;\n+}\n+\n+\n","filename":"src\/hotspot\/share\/opto\/library_call.cpp","additions":214,"deletions":0,"binary":false,"changes":214,"status":"modified"},{"patch":"@@ -319,0 +319,5 @@\n+  bool inline_dilithiumAlmostNtt();\n+  bool inline_dilithiumAlmostInverseNtt();\n+  bool inline_dilithiumNttMult();\n+  bool inline_dilithiumMontMulByConstant();\n+  bool inline_dilithiumDecomposePoly();\n@@ -325,0 +330,1 @@\n+  bool inline_double_keccak();\n","filename":"src\/hotspot\/share\/opto\/library_call.hpp","additions":6,"deletions":0,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -232,0 +232,1 @@\n+const TypeFunc* OptoRuntime::_double_keccak_Type                  = nullptr;\n@@ -241,0 +242,7 @@\n+\n+const TypeFunc* OptoRuntime::_dilithiumAlmostNtt_Type             = nullptr;\n+const TypeFunc* OptoRuntime::_dilithiumAlmostInverseNtt_Type      = nullptr;\n+const TypeFunc* OptoRuntime::_dilithiumNttMult_Type               = nullptr;\n+const TypeFunc* OptoRuntime::_dilithiumMontMulByConstant_Type     = nullptr;\n+const TypeFunc* OptoRuntime::_dilithiumDecomposePoly_Type         = nullptr;\n+\n@@ -1172,0 +1180,3 @@\n+\/*\n+ * int implCompressMultiBlock(byte[] b, int ofs, int limit)\n+ *\/\n@@ -1193,0 +1204,19 @@\n+\/\/ SHAKE128Parallel doubleKeccak function\n+static const TypeFunc* make_double_keccak_Type() {\n+    int argcnt = 2;\n+\n+    const Type** fields = TypeTuple::fields(argcnt);\n+    int argp = TypeFunc::Parms;\n+    fields[argp++] = TypePtr::NOTNULL;      \/\/ status0\n+    fields[argp++] = TypePtr::NOTNULL;      \/\/ status1\n+\n+    assert(argp == TypeFunc::Parms + argcnt, \"correct decoding\");\n+    const TypeTuple* domain = TypeTuple::make(TypeFunc::Parms + argcnt, fields);\n+\n+    \/\/ result type needed\n+    fields = TypeTuple::fields(1);\n+    fields[TypeFunc::Parms + 0] = TypeInt::INT;\n+    const TypeTuple* range = TypeTuple::make(TypeFunc::Parms + 1, fields);\n+    return TypeFunc::make(domain, range);\n+}\n+\n@@ -1378,0 +1408,99 @@\n+\/\/ Dilithium NTT function except for the final \"normalization\" to |coeff| < Q\n+static const TypeFunc* make_dilithiumAlmostNtt_Type() {\n+    int argcnt = 2;\n+\n+    const Type** fields = TypeTuple::fields(argcnt);\n+    int argp = TypeFunc::Parms;\n+    fields[argp++] = TypePtr::NOTNULL;      \/\/ coeffs\n+    fields[argp++] = TypePtr::NOTNULL;      \/\/ NTT zetas\n+\n+    assert(argp == TypeFunc::Parms + argcnt, \"correct decoding\");\n+    const TypeTuple* domain = TypeTuple::make(TypeFunc::Parms + argcnt, fields);\n+\n+    \/\/ result type needed\n+    fields = TypeTuple::fields(1);\n+    fields[TypeFunc::Parms + 0] = TypeInt::INT;\n+    const TypeTuple* range = TypeTuple::make(TypeFunc::Parms + 1, fields);\n+    return TypeFunc::make(domain, range);\n+}\n+\n+\/\/ Dilithium inverse NTT function except the final mod Q division by 2^256\n+static const TypeFunc* make_dilithiumAlmostInverseNtt_Type() {\n+    int argcnt = 2;\n+\n+    const Type** fields = TypeTuple::fields(argcnt);\n+    int argp = TypeFunc::Parms;\n+    fields[argp++] = TypePtr::NOTNULL;      \/\/ coeffs\n+    fields[argp++] = TypePtr::NOTNULL;      \/\/ inverse NTT zetas\n+\n+    assert(argp == TypeFunc::Parms + argcnt, \"correct decoding\");\n+    const TypeTuple* domain = TypeTuple::make(TypeFunc::Parms + argcnt, fields);\n+\n+    \/\/ result type needed\n+    fields = TypeTuple::fields(1);\n+    fields[TypeFunc::Parms + 0] = TypeInt::INT;\n+    const TypeTuple* range = TypeTuple::make(TypeFunc::Parms + 1, fields);\n+    return TypeFunc::make(domain, range);\n+}\n+\n+\/\/ Dilithium NTT multiply function\n+static const TypeFunc* make_dilithiumNttMult_Type() {\n+    int argcnt = 3;\n+\n+    const Type** fields = TypeTuple::fields(argcnt);\n+    int argp = TypeFunc::Parms;\n+    fields[argp++] = TypePtr::NOTNULL;      \/\/ result\n+    fields[argp++] = TypePtr::NOTNULL;      \/\/ ntta\n+    fields[argp++] = TypePtr::NOTNULL;      \/\/ nttb\n+\n+    assert(argp == TypeFunc::Parms + argcnt, \"correct decoding\");\n+    const TypeTuple* domain = TypeTuple::make(TypeFunc::Parms + argcnt, fields);\n+\n+    \/\/ result type needed\n+    fields = TypeTuple::fields(1);\n+    fields[TypeFunc::Parms + 0] = TypeInt::INT;\n+    const TypeTuple* range = TypeTuple::make(TypeFunc::Parms + 1, fields);\n+    return TypeFunc::make(domain, range);\n+}\n+\n+\/\/ Dilithium Montgomery multiply a polynome coefficient array by a constant\n+static const TypeFunc* make_dilithiumMontMulByConstant_Type() {\n+    int argcnt = 2;\n+\n+    const Type** fields = TypeTuple::fields(argcnt);\n+    int argp = TypeFunc::Parms;\n+    fields[argp++] = TypePtr::NOTNULL;      \/\/ coeffs\n+    fields[argp++] = TypeInt::INT;          \/\/ constant multiplier\n+\n+    assert(argp == TypeFunc::Parms + argcnt, \"correct decoding\");\n+    const TypeTuple* domain = TypeTuple::make(TypeFunc::Parms + argcnt, fields);\n+\n+    \/\/ result type needed\n+    fields = TypeTuple::fields(1);\n+    fields[TypeFunc::Parms + 0] = TypeInt::INT;\n+    const TypeTuple* range = TypeTuple::make(TypeFunc::Parms + 1, fields);\n+    return TypeFunc::make(domain, range);\n+}\n+\n+\/\/ Dilithium decompose polynomial\n+static const TypeFunc* make_dilithiumDecomposePoly_Type() {\n+    int argcnt = 5;\n+\n+    const Type** fields = TypeTuple::fields(argcnt);\n+    int argp = TypeFunc::Parms;\n+    fields[argp++] = TypePtr::NOTNULL;      \/\/ input\n+    fields[argp++] = TypePtr::NOTNULL;      \/\/ lowPart\n+    fields[argp++] = TypePtr::NOTNULL;      \/\/ highPart\n+    fields[argp++] = TypeInt::INT;          \/\/ 2 * gamma2\n+    fields[argp++] = TypeInt::INT;          \/\/ multiplier\n+\n+    assert(argp == TypeFunc::Parms + argcnt, \"correct decoding\");\n+    const TypeTuple* domain = TypeTuple::make(TypeFunc::Parms + argcnt, fields);\n+\n+    \/\/ result type needed\n+    fields = TypeTuple::fields(1);\n+    fields[TypeFunc::Parms + 0] = TypeInt::INT;\n+    const TypeTuple* range = TypeTuple::make(TypeFunc::Parms + 1, fields);\n+    return TypeFunc::make(domain, range);\n+}\n+\n@@ -1981,0 +2110,1 @@\n+  _double_keccak_Type                 = make_double_keccak_Type();\n@@ -1990,0 +2120,7 @@\n+\n+  _dilithiumAlmostNtt_Type            = make_dilithiumAlmostNtt_Type();\n+  _dilithiumAlmostInverseNtt_Type     = make_dilithiumAlmostInverseNtt_Type();\n+  _dilithiumNttMult_Type              = make_dilithiumNttMult_Type();\n+  _dilithiumMontMulByConstant_Type    = make_dilithiumMontMulByConstant_Type();\n+  _dilithiumDecomposePoly_Type        = make_dilithiumDecomposePoly_Type();\n+\n","filename":"src\/hotspot\/share\/opto\/runtime.cpp","additions":137,"deletions":0,"binary":false,"changes":137,"status":"modified"},{"patch":"@@ -173,0 +173,1 @@\n+  static const TypeFunc* _double_keccak_Type;\n@@ -182,0 +183,7 @@\n+\n+  static const TypeFunc* _dilithiumAlmostNtt_Type;\n+  static const TypeFunc* _dilithiumAlmostInverseNtt_Type;\n+  static const TypeFunc* _dilithiumNttMult_Type;\n+  static const TypeFunc* _dilithiumMontMulByConstant_Type;\n+  static const TypeFunc* _dilithiumDecomposePoly_Type;\n+\n@@ -528,0 +536,5 @@\n+  static inline const TypeFunc* double_keccak_Type() {\n+    assert(_double_keccak_Type != nullptr, \"should be initialized\");\n+    return _double_keccak_Type;\n+  }\n+\n@@ -576,0 +589,25 @@\n+  static const TypeFunc* dilithiumAlmostNtt_Type() {\n+    assert(_dilithiumAlmostNtt_Type != nullptr, \"should be initialized\");\n+    return _dilithiumAlmostNtt_Type;\n+  }\n+\n+  static const TypeFunc* dilithiumAlmostInverseNtt_Type() {\n+    assert(_dilithiumAlmostInverseNtt_Type != nullptr, \"should be initialized\");\n+    return _dilithiumAlmostInverseNtt_Type;\n+  }\n+\n+  static const TypeFunc* dilithiumNttMult_Type() {\n+    assert(_dilithiumNttMult_Type != nullptr, \"should be initialized\");\n+    return _dilithiumNttMult_Type;\n+  }\n+\n+  static const TypeFunc* dilithiumMontMulByConstant_Type() {\n+    assert(_dilithiumMontMulByConstant_Type != nullptr, \"should be initialized\");\n+    return _dilithiumMontMulByConstant_Type;\n+  }\n+\n+  static const TypeFunc* dilithiumDecomposePoly_Type() {\n+    assert(_dilithiumDecomposePoly_Type != nullptr, \"should be initialized\");\n+    return _dilithiumDecomposePoly_Type;\n+  }\n+\n","filename":"src\/hotspot\/share\/opto\/runtime.hpp","additions":38,"deletions":0,"binary":false,"changes":38,"status":"modified"},{"patch":"@@ -327,0 +327,2 @@\n+  product(bool, UseDilithiumIntrinsics, false, DIAGNOSTIC,                  \\\n+          \"Use intrinsics for the vectorized version of Dilithium\")         \\\n","filename":"src\/hotspot\/share\/runtime\/globals.hpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -681,0 +681,15 @@\n+  do_stub(compiler, dilithiumAlmostNtt)                                 \\\n+  do_entry(compiler, dilithiumAlmostNtt,                                \\\n+           dilithiumAlmostNtt, dilithiumAlmostNtt)                      \\\n+  do_stub(compiler, dilithiumAlmostInverseNtt)                          \\\n+  do_entry(compiler, dilithiumAlmostInverseNtt,                         \\\n+           dilithiumAlmostInverseNtt, dilithiumAlmostInverseNtt)        \\\n+  do_stub(compiler, dilithiumNttMult)                                   \\\n+  do_entry(compiler, dilithiumNttMult,                                  \\\n+           dilithiumNttMult, dilithiumNttMult)                          \\\n+  do_stub(compiler, dilithiumMontMulByConstant)                         \\\n+  do_entry(compiler, dilithiumMontMulByConstant,                        \\\n+           dilithiumMontMulByConstant, dilithiumMontMulByConstant)      \\\n+  do_stub(compiler, dilithiumDecomposePoly)                             \\\n+  do_entry(compiler, dilithiumDecomposePoly,                            \\\n+           dilithiumDecomposePoly, dilithiumDecomposePoly)              \\\n@@ -728,0 +743,2 @@\n+  do_stub(compiler, double_keccak)                                      \\\n+  do_entry(compiler, double_keccak, double_keccak, double_keccak)       \\\n","filename":"src\/hotspot\/share\/runtime\/stubDeclarations.hpp","additions":17,"deletions":0,"binary":false,"changes":17,"status":"modified"},{"patch":"@@ -28,0 +28,1 @@\n+import jdk.internal.vm.annotation.IntrinsicCandidate;\n@@ -30,0 +31,1 @@\n+import sun.security.provider.SHA3Parallel.Shake128Parallel;\n@@ -31,0 +33,1 @@\n+import java.security.InvalidAlgorithmParameterException;\n@@ -47,1 +50,1 @@\n-\n+    private static final int SHAKE128_BLOCK_SIZE = 168; \/\/ the block length for SHAKE128\n@@ -101,33 +104,274 @@\n-    private static final int[] MONT_ZETAS_FOR_INVERSE_NTT = new int[]{\n-        -1976782, 846154, -1400424, -3937738, 1362209, 48306, -3919660, 554416,\n-        3545687, -1612842, 976891, -183443, 2286327, 420899, 2235985, 2939036,\n-        3833893, 260646, 1104333, 1667432, -1910376, 1803090, -1723600, 426683,\n-        -472078, -1717735, 975884, -2213111, -269760, -3866901, -3523897, 3038916,\n-        1799107, 3694233, -1652634, -810149, -3014001, -1616392, -162844, 3183426,\n-        1207385, -185531, -3369112, -1957272, 164721, -2454455, -2432395, 2013608,\n-        3776993, -594136, 3724270, 2584293, 1846953, 1671176, 2831860, 542412,\n-        -3406031, -2235880, -777191, -1500165, 1374803, 2546312, -1917081, 1279661,\n-        1962642, -3306115, -1312455, 451100, 1430225, 3318210, -1237275, 1333058,\n-        1050970, -1903435, -1869119, 2994039, 3548272, -2635921, -1250494, 3767016,\n-        -1595974, -2486353, -1247620, -4055324, -1265009, 2590150, -2691481, -2842341,\n-        -203044, -1735879, 3342277, -3437287, -4108315, 2437823, -286988, -342297,\n-        3595838, 768622, 525098, 3556995, -3207046, -2031748, 3122442, 655327,\n-        522500, 43260, 1613174, -495491, -819034, -909542, -1859098, -900702,\n-        3193378, 1197226, 3759364, 3520352, -3513181, 1235728, -2434439, -266997,\n-        3562462, 2446433, -2244091, 3342478, -3817976, -2316500, -3407706, -2091667,\n-        -3839961, 3628969, 3881060, 3019102, 1439742, 812732, 1584928, -1285669,\n-        -1341330, -1315589, 177440, 2409325, 1851402, -3159746, 3553272, -189548,\n-        1316856, -759969, 210977, -2389356, 3249728, -1653064, 8578, 3724342,\n-        -3958618, -904516, 1100098, -44288, -3097992, -508951, -264944, 3343383,\n-        1430430, -1852771, -1349076, 381987, 1308169, 22981, 1228525, 671102,\n-        2477047, 411027, 3693493, 2967645, -2715295, -2147896, 983419, -3412210,\n-        -126922, 3632928, 3157330, 3190144, 1000202, 4083598, -1939314, 1257611,\n-        1585221, -2176455, -3475950, 1452451, 3041255, 3677745, 1528703, 3930395,\n-        2797779, -2071892, 2556880, -3900724, -3881043, -954230, -531354, -811944,\n-        -3699596, 1600420, 2140649, -3507263, 3821735, -3505694, 1643818, 1699267,\n-        539299, -2348700, 300467, -3539968, 2867647, -3574422, 3043716, 3861115,\n-        -3915439, 2537516, 3592148, 1661693, -3530437, -3077325, -95776, -2706023,\n-        -280005, -4010497, 19422, -1757237, 3277672, 1399561, 3859737, 2118186,\n-        2108549, -2619752, 1119584, 549488, -3585928, 1079900, -1024112, -2725464,\n-        -2680103, -3111497, 2884855, -3119733, 2091905, 359251, -2353451, -1826347,\n-        -466468, 876248, 777960, -237124, 518909, 2608894, -25847\n+    private static final int[] MONT_ZETAS_FOR_VECTOR_NTT = new int[]{\n+            25847, 25847, 25847, 25847, 25847, 25847, 25847, 25847,\n+            25847, 25847, 25847, 25847, 25847, 25847, 25847, 25847,\n+            25847, 25847, 25847, 25847, 25847, 25847, 25847, 25847,\n+            25847, 25847, 25847, 25847, 25847, 25847, 25847, 25847,\n+            25847, 25847, 25847, 25847, 25847, 25847, 25847, 25847,\n+            25847, 25847, 25847, 25847, 25847, 25847, 25847, 25847,\n+            25847, 25847, 25847, 25847, 25847, 25847, 25847, 25847,\n+            25847, 25847, 25847, 25847, 25847, 25847, 25847, 25847,\n+            25847, 25847, 25847, 25847, 25847, 25847, 25847, 25847,\n+            25847, 25847, 25847, 25847, 25847, 25847, 25847, 25847,\n+            25847, 25847, 25847, 25847, 25847, 25847, 25847, 25847,\n+            25847, 25847, 25847, 25847, 25847, 25847, 25847, 25847,\n+            25847, 25847, 25847, 25847, 25847, 25847, 25847, 25847,\n+            25847, 25847, 25847, 25847, 25847, 25847, 25847, 25847,\n+            25847, 25847, 25847, 25847, 25847, 25847, 25847, 25847,\n+            25847, 25847, 25847, 25847, 25847, 25847, 25847, 25847,\n+\n+            -2608894, -2608894, -2608894, -2608894, -2608894, -2608894, -2608894, -2608894,\n+            -2608894, -2608894, -2608894, -2608894, -2608894, -2608894, -2608894, -2608894,\n+            -2608894, -2608894, -2608894, -2608894, -2608894, -2608894, -2608894, -2608894,\n+            -2608894, -2608894, -2608894, -2608894, -2608894, -2608894, -2608894, -2608894,\n+            -2608894, -2608894, -2608894, -2608894, -2608894, -2608894, -2608894, -2608894,\n+            -2608894, -2608894, -2608894, -2608894, -2608894, -2608894, -2608894, -2608894,\n+            -2608894, -2608894, -2608894, -2608894, -2608894, -2608894, -2608894, -2608894,\n+            -2608894, -2608894, -2608894, -2608894, -2608894, -2608894, -2608894, -2608894,\n+            -518909, -518909, -518909, -518909, -518909, -518909, -518909, -518909,\n+            -518909, -518909, -518909, -518909, -518909, -518909, -518909, -518909,\n+            -518909, -518909, -518909, -518909, -518909, -518909, -518909, -518909,\n+            -518909, -518909, -518909, -518909, -518909, -518909, -518909, -518909,\n+            -518909, -518909, -518909, -518909, -518909, -518909, -518909, -518909,\n+            -518909, -518909, -518909, -518909, -518909, -518909, -518909, -518909,\n+            -518909, -518909, -518909, -518909, -518909, -518909, -518909, -518909,\n+            -518909, -518909, -518909, -518909, -518909, -518909, -518909, -518909,\n+\n+            237124, 237124, 237124, 237124, 237124, 237124, 237124, 237124,\n+            237124, 237124, 237124, 237124, 237124, 237124, 237124, 237124,\n+            237124, 237124, 237124, 237124, 237124, 237124, 237124, 237124,\n+            237124, 237124, 237124, 237124, 237124, 237124, 237124, 237124,\n+            -777960, -777960, -777960, -777960, -777960, -777960, -777960, -777960,\n+            -777960, -777960, -777960, -777960, -777960, -777960, -777960, -777960,\n+            -777960, -777960, -777960, -777960, -777960, -777960, -777960, -777960,\n+            -777960, -777960, -777960, -777960, -777960, -777960, -777960, -777960,\n+            -876248, -876248, -876248, -876248, -876248, -876248, -876248, -876248,\n+            -876248, -876248, -876248, -876248, -876248, -876248, -876248, -876248,\n+            -876248, -876248, -876248, -876248, -876248, -876248, -876248, -876248,\n+            -876248, -876248, -876248, -876248, -876248, -876248, -876248, -876248,\n+            466468, 466468, 466468, 466468, 466468, 466468, 466468, 466468,\n+            466468, 466468, 466468, 466468, 466468, 466468, 466468, 466468,\n+            466468, 466468, 466468, 466468, 466468, 466468, 466468, 466468,\n+            466468, 466468, 466468, 466468, 466468, 466468, 466468, 466468,\n+\n+            1826347, 1826347, 1826347, 1826347, 1826347, 1826347, 1826347, 1826347,\n+            1826347, 1826347, 1826347, 1826347, 1826347, 1826347, 1826347, 1826347,\n+            2353451, 2353451, 2353451, 2353451, 2353451, 2353451, 2353451, 2353451,\n+            2353451, 2353451, 2353451, 2353451, 2353451, 2353451, 2353451, 2353451,\n+            -359251, -359251, -359251, -359251, -359251, -359251, -359251, -359251,\n+            -359251, -359251, -359251, -359251, -359251, -359251, -359251, -359251,\n+            -2091905, -2091905, -2091905, -2091905, -2091905, -2091905, -2091905, -2091905,\n+            -2091905, -2091905, -2091905, -2091905, -2091905, -2091905, -2091905, -2091905,\n+            3119733, 3119733, 3119733, 3119733, 3119733, 3119733, 3119733, 3119733,\n+            3119733, 3119733, 3119733, 3119733, 3119733, 3119733, 3119733, 3119733,\n+            -2884855, -2884855, -2884855, -2884855, -2884855, -2884855, -2884855, -2884855,\n+            -2884855, -2884855, -2884855, -2884855, -2884855, -2884855, -2884855, -2884855,\n+            3111497, 3111497, 3111497, 3111497, 3111497, 3111497, 3111497, 3111497,\n+            3111497, 3111497, 3111497, 3111497, 3111497, 3111497, 3111497, 3111497,\n+            2680103, 2680103, 2680103, 2680103, 2680103, 2680103, 2680103, 2680103,\n+            2680103, 2680103, 2680103, 2680103, 2680103, 2680103, 2680103, 2680103,\n+\n+            2725464, 2725464, 2725464, 2725464, 2725464, 2725464, 2725464, 2725464,\n+            1024112, 1024112, 1024112, 1024112, 1024112, 1024112, 1024112, 1024112,\n+            -1079900, -1079900, -1079900, -1079900, -1079900, -1079900, -1079900, -1079900,\n+            3585928, 3585928, 3585928, 3585928, 3585928, 3585928, 3585928, 3585928,\n+            -549488, -549488, -549488, -549488, -549488, -549488, -549488, -549488,\n+            -1119584, -1119584, -1119584, -1119584, -1119584, -1119584, -1119584, -1119584,\n+            2619752, 2619752, 2619752, 2619752, 2619752, 2619752, 2619752, 2619752,\n+            -2108549, -2108549, -2108549, -2108549, -2108549, -2108549, -2108549, -2108549,\n+            -2118186, -2118186, -2118186, -2118186, -2118186, -2118186, -2118186, -2118186,\n+            -3859737, -3859737, -3859737, -3859737, -3859737, -3859737, -3859737, -3859737,\n+            -1399561, -1399561, -1399561, -1399561, -1399561, -1399561, -1399561, -1399561,\n+            -3277672, -3277672, -3277672, -3277672, -3277672, -3277672, -3277672, -3277672,\n+            1757237, 1757237, 1757237, 1757237, 1757237, 1757237, 1757237, 1757237,\n+            -19422, -19422, -19422, -19422, -19422, -19422, -19422, -19422,\n+            4010497, 4010497, 4010497, 4010497, 4010497, 4010497, 4010497, 4010497,\n+            280005, 280005, 280005, 280005, 280005, 280005, 280005, 280005,\n+\n+            2706023, 2706023, 2706023, 2706023, 95776, 95776, 95776, 95776,\n+            3077325, 3077325, 3077325, 3077325, 3530437, 3530437, 3530437, 3530437,\n+            -1661693, -1661693, -1661693, -1661693, -3592148, -3592148, -3592148, -3592148,\n+            -2537516, -2537516, -2537516, -2537516, 3915439, 3915439, 3915439, 3915439,\n+            -3861115, -3861115, -3861115, -3861115, -3043716, -3043716, -3043716, -3043716,\n+            3574422, 3574422, 3574422, 3574422, -2867647, -2867647, -2867647, -2867647,\n+            3539968, 3539968, 3539968, 3539968, -300467, -300467, -300467, -300467,\n+            2348700, 2348700, 2348700, 2348700, -539299, -539299, -539299, -539299,\n+            -1699267, -1699267, -1699267, -1699267, -1643818, -1643818, -1643818, -1643818,\n+            3505694, 3505694, 3505694, 3505694, -3821735, -3821735, -3821735, -3821735,\n+            3507263, 3507263, 3507263, 3507263, -2140649, -2140649, -2140649, -2140649,\n+            -1600420, -1600420, -1600420, -1600420, 3699596, 3699596, 3699596, 3699596,\n+            811944, 811944, 811944, 811944, 531354, 531354, 531354, 531354,\n+            954230, 954230, 954230, 954230, 3881043, 3881043, 3881043, 3881043,\n+            3900724, 3900724, 3900724, 3900724, -2556880, -2556880, -2556880, -2556880,\n+            2071892, 2071892, 2071892, 2071892, -2797779, -2797779, -2797779, -2797779,\n+\n+            -3930395, -3930395, -1528703, -1528703, -3677745, -3677745, -3041255, -3041255,\n+            -1452451, -1452451, 3475950, 3475950, 2176455, 2176455, -1585221, -1585221,\n+            -1257611, -1257611, 1939314, 1939314, -4083598, -4083598, -1000202, -1000202,\n+            -3190144, -3190144, -3157330, -3157330, -3632928, -3632928, 126922, 126922,\n+            3412210, 3412210, -983419, -983419, 2147896, 2147896, 2715295, 2715295,\n+            -2967645, -2967645, -3693493, -3693493, -411027, -411027, -2477047, -2477047,\n+            -671102, -671102, -1228525, -1228525, -22981, -22981, -1308169, -1308169,\n+            -381987, -381987, 1349076, 1349076, 1852771, 1852771, -1430430, -1430430,\n+            -3343383, -3343383, 264944, 264944, 508951, 508951, 3097992, 3097992,\n+            44288, 44288, -1100098, -1100098, 904516, 904516, 3958618, 3958618,\n+            -3724342, -3724342, -8578, -8578, 1653064, 1653064, -3249728, -3249728,\n+            2389356, 2389356, -210977, -210977, 759969, 759969, -1316856, -1316856,\n+            189548, 189548, -3553272, -3553272, 3159746, 3159746, -1851402, -1851402,\n+            -2409325, -2409325, -177440, -177440, 1315589, 1315589, 1341330, 1341330,\n+            1285669, 1285669, -1584928, -1584928, -812732, -812732, -1439742, -1439742,\n+            -3019102, -3019102, -3881060, -3881060, -3628969, -3628969, 3839961, 3839961,\n+\n+            2091667, 3407706, 2316500, 3817976, -3342478, 2244091, -2446433, -3562462,\n+            266997, 2434439, -1235728, 3513181, -3520352, -3759364, -1197226, -3193378,\n+            900702, 1859098, 909542, 819034, 495491, -1613174, -43260, -522500,\n+            -655327, -3122442, 2031748, 3207046, -3556995, -525098, -768622, -3595838,\n+            342297, 286988, -2437823, 4108315, 3437287, -3342277, 1735879, 203044,\n+            2842341, 2691481, -2590150, 1265009, 4055324, 1247620, 2486353, 1595974,\n+            -3767016, 1250494, 2635921, -3548272, -2994039, 1869119, 1903435, -1050970,\n+            -1333058, 1237275, -3318210, -1430225, -451100, 1312455, 3306115, -1962642,\n+            -1279661, 1917081, -2546312, -1374803, 1500165, 777191, 2235880, 3406031,\n+            -542412, -2831860, -1671176, -1846953, -2584293, -3724270, 594136, -3776993,\n+            -2013608, 2432395, 2454455, -164721, 1957272, 3369112, 185531, -1207385,\n+            -3183426, 162844, 1616392, 3014001, 810149, 1652634, -3694233, -1799107,\n+            -3038916, 3523897, 3866901, 269760, 2213111, -975884, 1717735, 472078,\n+            -426683, 1723600, -1803090, 1910376, -1667432, -1104333, -260646, -3833893,\n+            -2939036, -2235985, -420899, -2286327, 183443, -976891, 1612842, -3545687,\n+            -554416, 3919660, -48306, -1362209, 3937738, 1400424, -846154, 1976782\n+    };\n+\n+    private static final int[] MONT_ZETAS_FOR_VECTOR_INVERSE_NTT = new int[]{\n+            -1976782, 846154, -1400424, -3937738, 1362209, 48306, -3919660, 554416,\n+            3545687, -1612842, 976891, -183443, 2286327, 420899, 2235985, 2939036,\n+            3833893, 260646, 1104333, 1667432, -1910376, 1803090, -1723600, 426683,\n+            -472078, -1717735, 975884, -2213111, -269760, -3866901, -3523897, 3038916,\n+            1799107, 3694233, -1652634, -810149, -3014001, -1616392, -162844, 3183426,\n+            1207385, -185531, -3369112, -1957272, 164721, -2454455, -2432395, 2013608,\n+            3776993, -594136, 3724270, 2584293, 1846953, 1671176, 2831860, 542412,\n+            -3406031, -2235880, -777191, -1500165, 1374803, 2546312, -1917081, 1279661,\n+            1962642, -3306115, -1312455, 451100, 1430225, 3318210, -1237275, 1333058,\n+            1050970, -1903435, -1869119, 2994039, 3548272, -2635921, -1250494, 3767016,\n+            -1595974, -2486353, -1247620, -4055324, -1265009, 2590150, -2691481, -2842341,\n+            -203044, -1735879, 3342277, -3437287, -4108315, 2437823, -286988, -342297,\n+            3595838, 768622, 525098, 3556995, -3207046, -2031748, 3122442, 655327,\n+            522500, 43260, 1613174, -495491, -819034, -909542, -1859098, -900702,\n+            3193378, 1197226, 3759364, 3520352, -3513181, 1235728, -2434439, -266997,\n+            3562462, 2446433, -2244091, 3342478, -3817976, -2316500, -3407706, -2091667,\n+\n+            -3839961, -3839961, 3628969, 3628969, 3881060, 3881060, 3019102, 3019102,\n+            1439742, 1439742, 812732, 812732, 1584928, 1584928, -1285669, -1285669,\n+            -1341330, - 1341330, -1315589, -1315589, 177440, 177440, 2409325, 2409325,\n+            1851402, 1851402, -3159746, -3159746, 3553272, 3553272, -189548, -189548,\n+            1316856, 1316856, -759969, -759969, 210977, 210977, -2389356, -2389356,\n+            3249728, 3249728, -1653064, -1653064, 8578, 8578, 3724342, 3724342,\n+            -3958618, -3958618, -904516, -904516, 1100098, 1100098, -44288, -44288,\n+            -3097992, -3097992, -508951, -508951, -264944, -264944, 3343383, 3343383,\n+            1430430, 1430430, -1852771, -1852771, -1349076, -1349076, 381987, 381987,\n+            1308169, 1308169, 22981, 22981, 1228525, 1228525, 671102, 671102,\n+            2477047, 2477047, 411027, 411027, 3693493, 3693493, 2967645, 2967645,\n+            -2715295, -2715295, -2147896, -2147896, 983419, 983419, -3412210, -3412210,\n+            -126922, -126922, 3632928, 3632928, 3157330, 3157330, 3190144, 3190144,\n+            1000202, 1000202, 4083598, 4083598, -1939314, -1939314, 1257611, 1257611,\n+            1585221, 1585221, -2176455, -2176455, -3475950, -3475950, 1452451, 1452451,\n+            3041255, 3041255, 3677745, 3677745, 1528703, 1528703, 3930395, 3930395,\n+\n+            2797779, 2797779, 2797779, 2797779, -2071892, -2071892, -2071892, -2071892,\n+            2556880, 2556880, 2556880, 2556880, -3900724, -3900724, -3900724, -3900724,\n+            -3881043, -3881043, -3881043, -3881043, -954230, -954230, -954230, -954230,\n+            -531354, -531354, -531354, -531354, -811944, -811944, -811944, -811944,\n+            -3699596, -3699596, -3699596, -3699596, 1600420, 1600420, 1600420, 1600420,\n+            2140649, 2140649, 2140649, 2140649, -3507263, -3507263, -3507263, -3507263,\n+            3821735, 3821735, 3821735, 3821735, -3505694, -3505694, -3505694, -3505694,\n+            1643818, 1643818, 1643818, 1643818, 1699267, 1699267, 1699267, 1699267,\n+            539299, 539299, 539299, 539299, -2348700, -2348700, -2348700, -2348700,\n+            300467, 300467, 300467, 300467, -3539968, -3539968, -3539968, -3539968,\n+            2867647, 2867647, 2867647, 2867647, -3574422, -3574422, -3574422, -3574422,\n+            3043716, 3043716, 3043716, 3043716, 3861115, 3861115, 3861115, 3861115,\n+            -3915439, -3915439, -3915439, -3915439, 2537516, 2537516, 2537516, 2537516,\n+            3592148, 3592148, 3592148, 3592148, 1661693, 1661693, 1661693, 1661693,\n+            -3530437, -3530437, -3530437, -3530437, -3077325, -3077325, -3077325, -3077325,\n+            -95776, -95776, -95776, -95776, -2706023, -2706023, -2706023, -2706023,\n+\n+            -280005, -280005, -280005, -280005, -280005, -280005, -280005, -280005,\n+            -4010497, -4010497, -4010497, -4010497, -4010497, -4010497, -4010497, -4010497,\n+            19422, 19422, 19422, 19422, 19422, 19422, 19422, 19422,\n+            -1757237, -1757237, -1757237, -1757237, -1757237, -1757237, -1757237, -1757237,\n+            3277672, 3277672, 3277672, 3277672, 3277672, 3277672, 3277672, 3277672,\n+            1399561, 1399561, 1399561, 1399561, 1399561, 1399561, 1399561, 1399561,\n+            3859737, 3859737, 3859737, 3859737, 3859737, 3859737, 3859737, 3859737,\n+            2118186, 2118186, 2118186, 2118186, 2118186, 2118186, 2118186, 2118186,\n+            2108549, 2108549, 2108549, 2108549, 2108549, 2108549, 2108549, 2108549,\n+            -2619752, -2619752, -2619752, -2619752, -2619752, -2619752, -2619752, -2619752,\n+            1119584, 1119584, 1119584, 1119584, 1119584, 1119584, 1119584, 1119584,\n+            549488, 549488, 549488, 549488, 549488, 549488, 549488, 549488,\n+            -3585928, -3585928, -3585928, -3585928, -3585928, -3585928, -3585928, -3585928,\n+            1079900, 1079900, 1079900, 1079900, 1079900, 1079900, 1079900, 1079900,\n+            -1024112, -1024112, -1024112, -1024112, -1024112, -1024112, -1024112, -1024112,\n+            -2725464, -2725464, -2725464, -2725464, -2725464, -2725464, -2725464, -2725464,\n+\n+            -2680103, -2680103, -2680103, -2680103, -2680103, -2680103, -2680103, -2680103,\n+            -2680103, -2680103, -2680103, -2680103, -2680103, -2680103, -2680103, -2680103,\n+            -3111497, -3111497, -3111497, -3111497, -3111497, -3111497, -3111497, -3111497,\n+            -3111497, -3111497, -3111497, -3111497, -3111497, -3111497, -3111497, -3111497,\n+            2884855, 2884855, 2884855, 2884855, 2884855, 2884855, 2884855, 2884855,\n+            2884855, 2884855, 2884855, 2884855, 2884855, 2884855, 2884855, 2884855,\n+            -3119733, -3119733, -3119733, -3119733, -3119733, -3119733, -3119733, -3119733,\n+            -3119733, -3119733, -3119733, -3119733, -3119733, -3119733, -3119733, -3119733,\n+            2091905, 2091905, 2091905, 2091905, 2091905, 2091905, 2091905, 2091905,\n+            2091905, 2091905, 2091905, 2091905, 2091905, 2091905, 2091905, 2091905,\n+            359251, 359251, 359251, 359251, 359251, 359251, 359251, 359251,\n+            359251, 359251, 359251, 359251, 359251, 359251, 359251, 359251,\n+            -2353451, -2353451, -2353451, -2353451, -2353451, -2353451, -2353451, -2353451,\n+            -2353451, -2353451, -2353451, -2353451, -2353451, -2353451, -2353451, -2353451,\n+            -1826347, -1826347, -1826347, -1826347, -1826347, -1826347, -1826347, -1826347,\n+            -1826347, -1826347, -1826347, -1826347, -1826347, -1826347, -1826347, -1826347,\n+\n+            -466468, -466468, -466468, -466468, -466468, -466468, -466468, -466468,\n+            -466468, -466468, -466468, -466468, -466468, -466468, -466468, -466468,\n+            -466468, -466468, -466468, -466468, -466468, -466468, -466468, -466468,\n+            -466468, -466468, -466468, -466468, -466468, -466468, -466468, -466468,\n+            876248, 876248, 876248, 876248, 876248, 876248, 876248, 876248,\n+            876248, 876248, 876248, 876248, 876248, 876248, 876248, 876248,\n+            876248, 876248, 876248, 876248, 876248, 876248, 876248, 876248,\n+            876248, 876248, 876248, 876248, 876248, 876248, 876248, 876248,\n+            777960, 777960, 777960, 777960, 777960, 777960, 777960, 777960,\n+            777960, 777960, 777960, 777960, 777960, 777960, 777960, 777960,\n+            777960, 777960, 777960, 777960, 777960, 777960, 777960, 777960,\n+            777960, 777960, 777960, 777960, 777960, 777960, 777960, 777960,\n+            -237124, -237124, -237124, -237124, -237124, -237124, -237124, -237124,\n+            -237124, -237124, -237124, -237124, -237124, -237124, -237124, -237124,\n+            -237124, -237124, -237124, -237124, -237124, -237124, -237124, -237124,\n+            -237124, -237124, -237124, -237124, -237124, -237124, -237124, -237124,\n+\n+            518909, 518909, 518909, 518909, 518909, 518909, 518909, 518909,\n+            518909, 518909, 518909, 518909, 518909, 518909, 518909, 518909,\n+            518909, 518909, 518909, 518909, 518909, 518909, 518909, 518909,\n+            518909, 518909, 518909, 518909, 518909, 518909, 518909, 518909,\n+            518909, 518909, 518909, 518909, 518909, 518909, 518909, 518909,\n+            518909, 518909, 518909, 518909, 518909, 518909, 518909, 518909,\n+            518909, 518909, 518909, 518909, 518909, 518909, 518909, 518909,\n+            518909, 518909, 518909, 518909, 518909, 518909, 518909, 518909,\n+            2608894, 2608894, 2608894, 2608894, 2608894, 2608894, 2608894, 2608894,\n+            2608894, 2608894, 2608894, 2608894, 2608894, 2608894, 2608894, 2608894,\n+            2608894, 2608894, 2608894, 2608894, 2608894, 2608894, 2608894, 2608894,\n+            2608894, 2608894, 2608894, 2608894, 2608894, 2608894, 2608894, 2608894,\n+            2608894, 2608894, 2608894, 2608894, 2608894, 2608894, 2608894, 2608894,\n+            2608894, 2608894, 2608894, 2608894, 2608894, 2608894, 2608894, 2608894,\n+            2608894, 2608894, 2608894, 2608894, 2608894, 2608894, 2608894, 2608894,\n+            2608894, 2608894, 2608894, 2608894, 2608894, 2608894, 2608894, 2608894,\n+\n+            -25847, -25847, -25847, -25847, -25847, -25847, -25847, -25847,\n+            -25847, -25847, -25847, -25847, -25847, -25847, -25847, -25847,\n+            -25847, -25847, -25847, -25847, -25847, -25847, -25847, -25847,\n+            -25847, -25847, -25847, -25847, -25847, -25847, -25847, -25847,\n+            -25847, -25847, -25847, -25847, -25847, -25847, -25847, -25847,\n+            -25847, -25847, -25847, -25847, -25847, -25847, -25847, -25847,\n+            -25847, -25847, -25847, -25847, -25847, -25847, -25847, -25847,\n+            -25847, -25847, -25847, -25847, -25847, -25847, -25847, -25847,\n+            -25847, -25847, -25847, -25847, -25847, -25847, -25847, -25847,\n+            -25847, -25847, -25847, -25847, -25847, -25847, -25847, -25847,\n+            -25847, -25847, -25847, -25847, -25847, -25847, -25847, -25847,\n+            -25847, -25847, -25847, -25847, -25847, -25847, -25847, -25847,\n+            -25847, -25847, -25847, -25847, -25847, -25847, -25847, -25847,\n+            -25847, -25847, -25847, -25847, -25847, -25847, -25847, -25847,\n+            -25847, -25847, -25847, -25847, -25847, -25847, -25847, -25847,\n+            -25847, -25847, -25847, -25847, -25847, -25847, -25847, -25847\n@@ -851,5 +1095,1 @@\n-    int[][][] generateA(byte[] seed) {\n-        int blockSize = 168;  \/\/ the size of one block of SHAKE128 output\n-        var xof = new SHAKE128(0);\n-        byte[] xofSeed = new byte[A_SEED_LEN + 2];\n-        System.arraycopy(seed, 0, xofSeed, 0, A_SEED_LEN);\n+    private int[][][] generateA(byte[] seed) {\n@@ -858,25 +1098,68 @@\n-        for (int i = 0; i < mlDsa_k; i++) {\n-            for (int j = 0; j < mlDsa_l; j++) {\n-                xofSeed[A_SEED_LEN] = (byte) j;\n-                xofSeed[A_SEED_LEN + 1] = (byte) i;\n-                xof.reset();\n-                xof.update(xofSeed);\n-\n-                byte[] rawAij = new byte[blockSize];\n-                int[] aij = new int[ML_DSA_N];\n-                int ofs = 0;\n-                int rawOfs = blockSize;\n-                int tmp;\n-                while (ofs < ML_DSA_N) {\n-                    if (rawOfs == blockSize) {\n-                        \/\/ works because 3 divides blockSize (=168)\n-                        xof.squeeze(rawAij, 0, blockSize);\n-                        rawOfs = 0;\n-                    }\n-                    tmp = (rawAij[rawOfs] & 0xFF) +\n-                            ((rawAij[rawOfs + 1] & 0xFF) << 8) +\n-                            ((rawAij[rawOfs + 2] & 0x7F) << 16);\n-                    rawOfs += 3;\n-                    if (tmp < ML_DSA_Q) {\n-                        aij[ofs] = tmp;\n-                        ofs++;\n+        int nrPar = 2;\n+        int rhoLen = seed.length;\n+        byte[] seedBuf = new byte[SHAKE128_BLOCK_SIZE];\n+        System.arraycopy(seed, 0, seedBuf, 0, seed.length);\n+        seedBuf[rhoLen + 2] = 0x1F;\n+        seedBuf[SHAKE128_BLOCK_SIZE - 1] = (byte)0x80;\n+        byte[][] xofBufArr = new byte[nrPar][SHAKE128_BLOCK_SIZE];\n+        int[] iIndex = new int[nrPar];\n+        int[] jIndex = new int[nrPar];\n+\n+        int[] parsedBuf = new int[SHAKE128_BLOCK_SIZE \/ 3];\n+\n+        int parInd = 0;\n+        boolean allDone;\n+        int[] ofs = new int[nrPar];\n+        Arrays.fill(ofs, 0);\n+        int[][] aij = new int[nrPar][];\n+        try {\n+            Shake128Parallel parXof = new Shake128Parallel(xofBufArr);\n+\n+            for (int i = 0; i < mlDsa_k; i++) {\n+                for (int j = 0; j < mlDsa_l; j++) {\n+                    xofBufArr[parInd] = seedBuf.clone();\n+                    xofBufArr[parInd][rhoLen] = (byte) j;\n+                    xofBufArr[parInd][rhoLen + 1] = (byte) i;\n+                    iIndex[parInd] = i;\n+                    jIndex[parInd] = j;\n+                    ofs[parInd] = 0;\n+                    aij[parInd] = new int[ML_DSA_N];\n+                    parInd++;\n+\n+                    if ((parInd == nrPar) ||\n+                            ((i == mlDsa_k - 1) && (j == mlDsa_l - 1))) {\n+                        parXof.reset(xofBufArr);\n+\n+                        allDone = false;\n+                        while (!allDone) {\n+                            allDone = true;\n+                            parXof.squeezeBlock();\n+                            for (int k = 0; k < parInd; k++) {\n+                                int parsedOfs = 0;\n+                                int tmp;\n+                                if (ofs[k] < ML_DSA_N) {\n+                                    for (int l = 0; l < SHAKE128_BLOCK_SIZE; l += 3) {\n+                                        byte[] rawBuf = xofBufArr[k];\n+                                        parsedBuf[l \/ 3] = (rawBuf[l] & 0xFF) +\n+                                                ((rawBuf[l + 1] & 0xFF) << 8) +\n+                                                ((rawBuf[l + 2] & 0x7F) << 16);\n+                                    }\n+                                }\n+                                while ((ofs[k] < ML_DSA_N) &&\n+                                        (parsedOfs < SHAKE128_BLOCK_SIZE \/ 3)) {\n+                                    tmp = parsedBuf[parsedOfs++];\n+                                    if (tmp < ML_DSA_Q) {\n+                                        aij[k][ofs[k]] = tmp;\n+                                        ofs[k]++;\n+                                    }\n+                                }\n+                                if (ofs[k] < ML_DSA_N) {\n+                                    allDone = false;\n+                                }\n+                            }\n+                        }\n+\n+                        for (int k = 0; k < parInd; k++) {\n+                            a[iIndex[k]][jIndex[k]] = aij[k];\n+                        }\n+                        parInd = 0;\n@@ -885,1 +1168,0 @@\n-                a[i][j] = aij;\n@@ -887,0 +1169,3 @@\n+        } catch (InvalidAlgorithmParameterException e) {\n+            \/\/ This should never happen since xofBufArr is of the correct size\n+            throw new RuntimeException(\"Internal error.\");\n@@ -888,0 +1173,1 @@\n+\n@@ -982,1 +1268,1 @@\n-            ML_DSA.mlDsaDecomposePoly(input[i], lowPart[i],\n+            mlDsaDecomposePoly(input[i], lowPart[i],\n@@ -1014,1 +1300,1 @@\n-        int[][] lowPart = new int[mlDsa_k][ML_DSA_N];\n+        int[][] lowPart = r;\n@@ -1033,1 +1319,12 @@\n-    public static int[] mlDsaNtt(int[] coeffs) {\n+    public static void mlDsaNtt(int[] coeffs) {\n+        implDilithiumAlmostNtt(coeffs, MONT_ZETAS_FOR_VECTOR_NTT);\n+        implDilithiumMontMulByConstant(coeffs,  MONT_R_MOD_Q);\n+    }\n+\n+    @IntrinsicCandidate\n+    static int implDilithiumAlmostNtt(int[] coeffs, int[] zetas) {\n+        implDilithiumAlmostNttJava(coeffs);\n+        return 1;\n+    }\n+\n+    static void implDilithiumAlmostNttJava(int[] coeffs) {\n@@ -1046,2 +1343,0 @@\n-        montMulByConstant(coeffs,  MONT_R_MOD_Q);\n-        return coeffs;\n@@ -1050,1 +1345,12 @@\n-    public static int[] mlDsaInverseNtt(int[] coeffs) {\n+    public static void mlDsaInverseNtt(int[] coeffs) {\n+        implDilithiumAlmostInverseNtt(coeffs, MONT_ZETAS_FOR_VECTOR_INVERSE_NTT);\n+        implDilithiumMontMulByConstant(coeffs, MONT_DIM_INVERSE);\n+    }\n+\n+    @IntrinsicCandidate\n+    static int implDilithiumAlmostInverseNtt(int[] coeffs, int[] zetas) {\n+        implDilithiumAlmostInverseNttJava(coeffs);\n+        return 1;\n+    }\n+\n+    static void implDilithiumAlmostInverseNttJava(int[] coeffs) {\n@@ -1052,1 +1358,1 @@\n-        int m = 0;\n+        int m = MONT_ZETAS_FOR_NTT.length - 1;\n@@ -1059,1 +1365,1 @@\n-                            MONT_ZETAS_FOR_INVERSE_NTT[m]);\n+                            -MONT_ZETAS_FOR_NTT[m]);\n@@ -1061,1 +1367,1 @@\n-                m++;\n+                m--;\n@@ -1064,2 +1370,0 @@\n-        montMulByConstant(coeffs, MONT_DIM_INVERSE);\n-        return coeffs;\n@@ -1081,0 +1385,11 @@\n+        implDilithiumNttMult(product, coeffs1, coeffs2);\n+    }\n+\n+\n+    @IntrinsicCandidate\n+    static int implDilithiumNttMult(int[] product, int[] coeffs1, int[] coeffs2) {\n+        implDilithiumNttMultJava(product, coeffs1, coeffs2);\n+        return 1;\n+    }\n+\n+    static void implDilithiumNttMultJava(int[] product, int[] coeffs1, int[] coeffs2) {\n@@ -1086,1 +1401,7 @@\n-    public static void montMulByConstant(int[] coeffs, int constant) {\n+    @IntrinsicCandidate\n+    static int implDilithiumMontMulByConstant(int[] coeffs, int constant) {\n+        implDilithiumMontMulByConstantJava(coeffs, constant);\n+        return 1;\n+    }\n+\n+    static void implDilithiumMontMulByConstantJava(int[] coeffs, int constant) {\n@@ -1094,0 +1415,13 @@\n+        implDilithiumDecomposePoly(input, lowPart, highPart,twoGamma2, multiplier);\n+    }\n+\n+    @IntrinsicCandidate\n+    static int implDilithiumDecomposePoly(int[] input, int[] lowPart, int[] highPart,\n+                                          int twoGamma2, int multiplier) {\n+        decomposePolyJava(input, lowPart, highPart, twoGamma2, multiplier);\n+        return 1;\n+    }\n+\n+    static void decomposePolyJava(int[] input, int[] lowPart, int[] highPart,\n+                                 int twoGamma2, int multiplier) {\n+        int dilithiumBarrettAddend = 5373807;\n@@ -1096,5 +1430,12 @@\n-            rplus = rplus - ((rplus + 5373807) >> 23) * ML_DSA_Q;\n-            rplus = rplus + ((rplus >> 31) & ML_DSA_Q);\n-            int r0 = rplus - ((rplus * multiplier) >> 22) * twoGamma2;\n-            r0 -= (((twoGamma2 - r0) >> 22) & twoGamma2);\n-            r0 -= (((twoGamma2 \/ 2 - r0) >> 31) & twoGamma2);\n+            rplus -= ((rplus + dilithiumBarrettAddend) >> 23) * ML_DSA_Q;\n+            rplus += ((rplus >> 31) & ML_DSA_Q);\n+\n+            int quotient = (rplus * multiplier) >> 22;\n+            int r0 = rplus - quotient * twoGamma2;\n+            int mask = (twoGamma2 - r0) >> 22;\n+            r0 -= (mask & twoGamma2);\n+            quotient += (mask & 1);\n+            mask = (twoGamma2 \/ 2 - r0) >> 31;\n+            r0 -= (mask & twoGamma2);\n+            quotient += (mask & 1);\n+\n@@ -1102,1 +1443,1 @@\n-            r1 = (r1 | (-r1)) >> 31;\n+            r1 = (r1 | (-r1)) >> 31; \/\/ 0 if rplus - r0 == (dilithium_q - 1), -1 otherwise\n@@ -1104,1 +1445,3 @@\n-            r1 = r1 & ((rplus - r0) \/ twoGamma2);\n+            \/\/ quotient = (rplus - r0) \/ twoGamma2;\n+            r1 = r1 & quotient;\n+\n@@ -1210,0 +1553,1 @@\n+    \/\/ see e.g. Algorithm 3 in https:\/\/eprint.iacr.org\/2018\/039.pdf\n","filename":"src\/java.base\/share\/classes\/sun\/security\/provider\/ML_DSA.java","additions":428,"deletions":84,"binary":false,"changes":512,"status":"modified"}]}