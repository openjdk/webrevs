{"files":[{"patch":"@@ -3575,0 +3575,17 @@\n+void Assembler::evmovdquw(XMMRegister dst, XMMRegister src, int vector_len) {\n+  \/\/ Unmasked instruction\n+  evmovdquw(dst, k0, src, \/*merge*\/ false, vector_len);\n+}\n+\n+void Assembler::evmovdquw(XMMRegister dst, KRegister mask, XMMRegister src, bool merge, int vector_len) {\n+  assert(vector_len <= AVX_256bit ? VM_Version::supports_avx512vlbw() : VM_Version::supports_avx512bw(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  attributes.set_is_evex_instruction();\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), 0, src->encoding(), VEX_SIMD_F2, VEX_OPCODE_0F, &attributes);\n+  emit_int16(0x6F, (0xC0 | encode));\n+}\n+\n@@ -4736,0 +4753,16 @@\n+void Assembler::evpermi2w(XMMRegister dst, XMMRegister nds, XMMRegister src, int vector_len) {\n+  assert(vector_len <= AVX_256bit ? VM_Version::supports_avx512vlbw() : VM_Version::supports_avx512bw(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int16(0x75, (0xC0 | encode));\n+}\n+\n+void Assembler::evpermi2d(XMMRegister dst, XMMRegister nds, XMMRegister src, int vector_len) {\n+  assert(vector_len <= AVX_256bit ? VM_Version::supports_avx512vlbw() : VM_Version::supports_avx512bw(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int16(0x76, (0xC0 | encode));\n+}\n+\n@@ -4752,0 +4785,24 @@\n+void Assembler::evpermt2w(XMMRegister dst, XMMRegister nds, XMMRegister src, int vector_len) {\n+  assert(vector_len <= AVX_256bit ? VM_Version::supports_avx512vlbw() : VM_Version::supports_avx512bw(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int16(0x7D, (0xC0 | encode));\n+}\n+\n+void Assembler::evpermt2d(XMMRegister dst, XMMRegister nds, XMMRegister src, int vector_len) {\n+  assert(vector_len <= AVX_256bit ? VM_Version::supports_avx512vlbw() : VM_Version::supports_avx512bw(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int16(0x7E, (0xC0 | encode));\n+}\n+\n+void Assembler::evpermt2q(XMMRegister dst, XMMRegister nds, XMMRegister src, int vector_len) {\n+  assert(vector_len <= AVX_256bit ? VM_Version::supports_avx512vlbw() : VM_Version::supports_avx512bw(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int16(0x7E, (0xC0 | encode));\n+}\n+\n@@ -8516,0 +8573,9 @@\n+void Assembler::vpmuldq(XMMRegister dst, XMMRegister nds, XMMRegister src, int vector_len) {\n+  assert(vector_len == AVX_128bit ? VM_Version::supports_avx() :\n+        (vector_len == AVX_256bit ? VM_Version::supports_avx2() : VM_Version::supports_evex()), \"\");\n+  \/\/ TODO check what legacy_mode needs to be set to\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ true);\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int16(0x28, (0xC0 | encode));\n+}\n+\n@@ -10493,0 +10559,12 @@\n+void Assembler::evpmulhw(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len) {\n+  assert(VM_Version::supports_avx512bw() && (vector_len == AVX_512bit || VM_Version::supports_avx512vl()), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false,\/* legacy_mode *\/ false, \/* no_mask_reg *\/ false,\/* uses_vl *\/ true);\n+  attributes.set_is_evex_instruction();\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int16((unsigned char)0xE5, (0xC0 | encode));\n+}\n+\n","filename":"src\/hotspot\/cpu\/x86\/assembler_x86.cpp","additions":78,"deletions":0,"binary":false,"changes":78,"status":"modified"},{"patch":"@@ -1758,0 +1758,1 @@\n+  void evmovdquw(XMMRegister dst, XMMRegister src, int vector_len);\n@@ -1964,0 +1965,2 @@\n+  void evpermi2w(XMMRegister dst, XMMRegister nds, XMMRegister src, int vector_len);\n+  void evpermi2d(XMMRegister dst, XMMRegister nds, XMMRegister src, int vector_len);\n@@ -1966,0 +1969,3 @@\n+  void evpermt2w(XMMRegister dst, XMMRegister nds, XMMRegister src, int vector_len);\n+  void evpermt2d(XMMRegister dst, XMMRegister nds, XMMRegister src, int vector_len);\n+  void evpermt2q(XMMRegister dst, XMMRegister nds, XMMRegister src, int vector_len);\n@@ -2698,0 +2704,1 @@\n+  void evpmulhw(XMMRegister dst, KRegister mask, XMMRegister nds, XMMRegister src, bool merge, int vector_len);\n@@ -2834,0 +2841,1 @@\n+  void vpmuldq(XMMRegister dst, XMMRegister nds, XMMRegister src, int vector_len);\n","filename":"src\/hotspot\/cpu\/x86\/assembler_x86.hpp","additions":8,"deletions":0,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -1294,0 +1294,1 @@\n+  void evmovdquw(XMMRegister dst, XMMRegister src, int vector_len) { Assembler::evmovdquw(dst, src, vector_len); }\n@@ -1504,0 +1505,2 @@\n+  void vpmuldq(XMMRegister dst, XMMRegister nds, XMMRegister    src, int vector_len) { Assembler::vpmuldq(dst, nds, src, vector_len); }\n+\n@@ -1513,0 +1516,3 @@\n+  void evpsrad(XMMRegister dst, XMMRegister nds, XMMRegister shift, int vector_len);\n+  void evpsrad(XMMRegister dst, XMMRegister nds, int         shift, int vector_len);\n+\n@@ -1516,0 +1522,3 @@\n+  void evpsllw(XMMRegister dst, KRegister mask, XMMRegister src, int shift, bool merge, int vector_len) {\n+    Assembler::evpsllw(dst, mask, src, shift, merge, vector_len);\n+  }\n@@ -1560,0 +1569,3 @@\n+  void evpsraw(XMMRegister dst, KRegister mask, XMMRegister src, int shift, bool merge, int vector_len) {\n+    Assembler::evpsraw(dst, mask, src, shift, merge, vector_len);\n+  }\n@@ -1567,0 +1579,3 @@\n+  void evpsrad(XMMRegister dst, KRegister mask, XMMRegister src, int shift, bool merge, int vector_len) {\n+    Assembler::evpsrad(dst, mask, src, shift, merge, vector_len);\n+  }\n","filename":"src\/hotspot\/cpu\/x86\/macroAssembler_x86.hpp","additions":15,"deletions":0,"binary":false,"changes":15,"status":"modified"},{"patch":"@@ -4009,0 +4009,2 @@\n+  generate_sha3_stubs();\n+\n","filename":"src\/hotspot\/cpu\/x86\/stubGenerator_x86_64.cpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -489,0 +489,4 @@\n+  \/\/ SHA3 stubs\n+  void generate_sha3_stubs();\n+  address generate_sha3_implCompress(bool multiBlock, const char *name);\n+\n","filename":"src\/hotspot\/cpu\/x86\/stubGenerator_x86_64.hpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -0,0 +1,308 @@\n+\/*\n+ * Copyright (c) 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"asm\/assembler.hpp\"\n+#include \"asm\/assembler.inline.hpp\"\n+#include \"runtime\/stubRoutines.hpp\"\n+#include \"macroAssembler_x86.hpp\"\n+#include \"stubGenerator_x86_64.hpp\"\n+\n+#define __ _masm->\n+\n+#ifdef PRODUCT\n+#define BLOCK_COMMENT(str) \/* nothing *\/\n+#else\n+#define BLOCK_COMMENT(str) __ block_comment(str)\n+#endif \/\/ PRODUCT\n+\n+#define BIND(label) bind(label); BLOCK_COMMENT(#label \":\")\n+\n+\/\/ Constants\n+ATTRIBUTE_ALIGNED(64) static const uint64_t round_consts_arr[24] = {\n+      0x0000000000000001L, 0x0000000000008082L, 0x800000000000808AL,\n+      0x8000000080008000L, 0x000000000000808BL, 0x0000000080000001L,\n+      0x8000000080008081L, 0x8000000000008009L, 0x000000000000008AL,\n+      0x0000000000000088L, 0x0000000080008009L, 0x000000008000000AL,\n+      0x000000008000808BL, 0x800000000000008BL, 0x8000000000008089L,\n+      0x8000000000008003L, 0x8000000000008002L, 0x8000000000000080L,\n+      0x000000000000800AL, 0x800000008000000AL, 0x8000000080008081L,\n+      0x8000000000008080L, 0x0000000080000001L, 0x8000000080008008L\n+    };\n+\n+ATTRIBUTE_ALIGNED(64) static const uint64_t permsAndRots[] = {\n+    \/\/ permutation in combined rho and pi\n+    9, 2, 11, 0, 1, 2, 3, 4,   \/\/ step 1 and 3\n+    8, 1, 9, 2, 11, 4, 12, 0,  \/\/ step 2\n+    9, 2, 10, 3, 11, 4, 12, 0, \/\/ step 4\n+    8, 9, 2, 3, 4, 5, 6, 7,    \/\/ step 5\n+    0, 8, 9, 10, 15, 0, 0, 0,  \/\/ step 6\n+    4, 5, 8, 9, 6, 7, 10, 11,  \/\/ step 7 and 8\n+    0, 1, 2, 3, 13, 0, 0, 0,   \/\/ step 9\n+    2, 3, 0, 1, 11, 0, 0, 0,   \/\/ step 10\n+    4, 5, 6, 7, 14, 0, 0, 0,   \/\/ step 11\n+    14, 15, 12, 13, 4, 0, 0, 0, \/\/ step 12\n+    \/\/ size of rotations (after step 5)\n+    1, 6, 62, 55, 28, 20, 27, 36,\n+    3, 45, 10, 15, 25, 8, 39, 41,\n+    44, 43, 21, 18, 2, 61, 56, 14,\n+    \/\/ rotation of row elements\n+    12, 8, 9, 10, 11, 5, 6, 7,\n+    9, 10, 11, 12, 8, 5, 6, 7\n+};\n+\n+static address round_constsAddr() {\n+  return (address) round_consts_arr;\n+}\n+\n+static address permsAndRotsAddr() {\n+  return (address) permsAndRots;\n+}\n+\n+void StubGenerator::generate_sha3_stubs() {\n+  if (UseSHA3Intrinsics) {\n+    if (VM_Version::supports_evex()) {\n+      StubRoutines::_sha3_implCompress     = generate_sha3_implCompress(false,   \"sha3_implCompress\");\n+      StubRoutines::_sha3_implCompressMB   = generate_sha3_implCompress(true,    \"sha3_implCompressMB\");\n+    }\n+  }\n+}\n+\n+\/\/ Arguments:\n+\/\/\n+\/\/ Inputs:\n+\/\/   c_rarg0   - byte[]  source+offset\n+\/\/   c_rarg1   - long[]  SHA3.state\n+\/\/   c_rarg2   - int     block_size\n+\/\/   c_rarg3   - int     offset\n+\/\/   c_rarg4   - int     limit\n+\/\/\n+address StubGenerator::generate_sha3_implCompress(bool multiBlock, const char *name) {\n+  __ align(CodeEntryAlignment);\n+  StubCodeMark mark(this, \"StubRoutines\", name);\n+  address start = __ pc();\n+\n+  const Register buf          = c_rarg0;\n+  const Register state        = c_rarg1;\n+  const Register block_size   = c_rarg2;\n+  const Register ofs          = c_rarg3;\n+#ifndef _WIN64\n+  const Register limit        = c_rarg4;\n+#else\n+  const Address limit_mem(rbp, 6 * wordSize);\n+  const Address limit = r12;\n+#endif\n+\n+  const Register permsAndRots = r10;\n+  const Register round_consts = r11;\n+  const Register constant2use = r13;\n+  const Register roundsLeft = r14;\n+\n+  Label sha3_loop;\n+  Label rounds24_loop, block104, block136, block144, block168;\n+\n+  __ enter();\n+\n+  __ push(r12);\n+  __ push(r13);\n+  __ push(r14);\n+\n+#ifdef _WIN64\n+  \/\/ on win64, fill limit from stack position\n+  __ movptr(limit, limit_mem);\n+#endif\n+\n+  __ lea(permsAndRots, ExternalAddress(permsAndRotsAddr()));\n+  __ lea(round_consts, ExternalAddress(round_constsAddr()));\n+\n+  \/\/ set up the masks\n+  __ mov64(rax,1);\n+  __ kmovbl(k1, rax);\n+  __ addl(rax,2);\n+  __ kmovbl(k2, rax);\n+  __ addl(rax, 4);\n+  __ kmovbl(k3, rax);\n+  __ addl(rax, 8);\n+  __ kmovbl(k4, rax);\n+  __ addl(rax, 16);\n+  __ kmovbl(k5, rax);\n+\n+  \/\/ load the state\n+  __ evmovdquq(xmm0, k5, Address(state, 0), false, Assembler::AVX_512bit);\n+  __ evmovdquq(xmm1, k5, Address(state, 40), false, Assembler::AVX_512bit);\n+  __ evmovdquq(xmm2, k5, Address(state, 80), false, Assembler::AVX_512bit);\n+  __ evmovdquq(xmm3, k5, Address(state, 120), false, Assembler::AVX_512bit);\n+  __ evmovdquq(xmm4, k5, Address(state, 160), false, Assembler::AVX_512bit);\n+\n+  \/\/ load the permutation and rotation constants\n+  __ evmovdquq(xmm17, Address(permsAndRots, 0), Assembler::AVX_512bit);\n+  __ evmovdquq(xmm18, Address(permsAndRots, 64), Assembler::AVX_512bit);\n+  __ evmovdquq(xmm19, Address(permsAndRots, 128), Assembler::AVX_512bit);\n+  __ evmovdquq(xmm20, Address(permsAndRots, 192), Assembler::AVX_512bit);\n+  __ evmovdquq(xmm21, Address(permsAndRots, 256), Assembler::AVX_512bit);\n+  __ evmovdquq(xmm22, Address(permsAndRots, 320), Assembler::AVX_512bit);\n+  __ evmovdquq(xmm23, Address(permsAndRots, 384), Assembler::AVX_512bit);\n+  __ evmovdquq(xmm24, Address(permsAndRots, 448), Assembler::AVX_512bit);\n+  __ evmovdquq(xmm25, Address(permsAndRots, 512), Assembler::AVX_512bit);\n+  __ evmovdquq(xmm26, Address(permsAndRots, 576), Assembler::AVX_512bit);\n+  __ evmovdquq(xmm27, Address(permsAndRots, 640), Assembler::AVX_512bit);\n+  __ evmovdquq(xmm28, Address(permsAndRots, 704), Assembler::AVX_512bit);\n+  __ evmovdquq(xmm29, Address(permsAndRots, 768), Assembler::AVX_512bit);\n+  __ evmovdquq(xmm30, Address(permsAndRots, 832), Assembler::AVX_512bit);\n+  __ evmovdquq(xmm31, Address(permsAndRots, 896), Assembler::AVX_512bit);\n+\n+  __ BIND(sha3_loop);\n+\n+  \/\/ there will be 24 keccak rounds\n+  __ movl(roundsLeft, 24);\n+  \/\/ load round_constants base\n+  __ movptr(constant2use, round_consts);\n+\n+  \/\/ load input: 72, 104, 136, 144 or 168 bytes\n+  \/\/ i.e. 5+4, 2*5+3, 3*5+2, 3*5+3 or 4*5+1 longs\n+  __ evpxorq(xmm0, k5, xmm0, Address(buf, 0), true, Assembler::AVX_512bit);\n+\n+  \/\/ if(blockSize == 72) SHA3-512\n+  __ cmpl(block_size, 72);\n+  __ jcc(Assembler::notEqual, block104);\n+  __ evpxorq(xmm1, k4, xmm1, Address(buf, 40), true, Assembler::AVX_512bit);\n+  __ jmp(rounds24_loop);\n+\n+  \/\/ if(blockSize == 104) SHA3-384\n+  __ BIND(block104);\n+  __ cmpl(block_size, 104);\n+  __ jcc(Assembler::notEqual, block136);\n+  __ evpxorq(xmm1, k5, xmm1, Address(buf, 40), true, Assembler::AVX_512bit);\n+  __ evpxorq(xmm2, k3, xmm2, Address(buf, 80), true, Assembler::AVX_512bit);\n+  __ jmp(rounds24_loop);\n+\n+  \/\/ if(blockSize == 136) SHA3-256 and SHAKE256\n+  __ BIND(block136);\n+  __ cmpl(block_size, 136);\n+  __ jcc(Assembler::notEqual, block144);\n+  __ evpxorq(xmm1, k5, xmm1, Address(buf, 40), true, Assembler::AVX_512bit);\n+  __ evpxorq(xmm2, k5, xmm2, Address(buf, 80), true, Assembler::AVX_512bit);\n+  __ evpxorq(xmm3, k2, xmm3, Address(buf, 120), true, Assembler::AVX_512bit);\n+  __ jmp(rounds24_loop);\n+\n+  \/\/ if(blockSize == 144) SHA3-224\n+  __ BIND(block144);\n+  __ cmpl(block_size, 144);\n+  __ jcc(Assembler::notEqual, block168);\n+  __ evpxorq(xmm1, k5, xmm1, Address(buf, 40), true, Assembler::AVX_512bit);\n+  __ evpxorq(xmm2, k5, xmm2, Address(buf, 80), true, Assembler::AVX_512bit);\n+  __ evpxorq(xmm3, k3, xmm3, Address(buf, 120), true, Assembler::AVX_512bit);\n+  __ jmp(rounds24_loop);\n+\n+  \/\/ if(blockSize == 168) SHAKE128\n+  __ BIND(block168);\n+  __ evpxorq(xmm1, k5, xmm1, Address(buf, 40), true, Assembler::AVX_512bit);\n+  __ evpxorq(xmm2, k5, xmm2, Address(buf, 80), true, Assembler::AVX_512bit);\n+  __ evpxorq(xmm3, k5, xmm3, Address(buf, 120), true, Assembler::AVX_512bit);\n+  __ evpxorq(xmm4, k1, xmm4, Address(buf, 160), true, Assembler::AVX_512bit);\n+\n+  __ BIND(rounds24_loop);\n+  __ subl( roundsLeft, 1);\n+\n+  __ evmovdquw(xmm5, xmm0, Assembler::AVX_512bit);\n+  __ vpternlogq(xmm5, 150, xmm1, xmm2, Assembler::AVX_512bit);\n+  __ vpternlogq(xmm5, 150, xmm3, xmm4, Assembler::AVX_512bit);\n+  __ evprolq(xmm6, xmm5, 1, Assembler::AVX_512bit);\n+  __ evpermt2q(xmm5, xmm30, xmm5, Assembler::AVX_512bit);\n+  __ evpermt2q(xmm6, xmm31, xmm6, Assembler::AVX_512bit);\n+  __ vpternlogq(xmm0, 150, xmm5, xmm6, Assembler::AVX_512bit);\n+  __ vpternlogq(xmm1, 150, xmm5, xmm6, Assembler::AVX_512bit);\n+  __ vpternlogq(xmm2, 150, xmm5, xmm6, Assembler::AVX_512bit);\n+  __ vpternlogq(xmm3, 150, xmm5, xmm6, Assembler::AVX_512bit);\n+  __ vpternlogq(xmm4, 150, xmm5, xmm6, Assembler::AVX_512bit);\n+  __ evpermt2q(xmm4, xmm17, xmm3, Assembler::AVX_512bit);\n+  __ evpermt2q(xmm3, xmm18, xmm2, Assembler::AVX_512bit);\n+  __ evpermt2q(xmm2, xmm17, xmm1, Assembler::AVX_512bit);\n+  __ evpermt2q(xmm1, xmm19, xmm0, Assembler::AVX_512bit);\n+  __ evpermt2q(xmm4, xmm20, xmm2, Assembler::AVX_512bit);\n+  __ evprolvq(xmm1, xmm1, xmm27, Assembler::AVX_512bit);\n+  __ evprolvq(xmm3, xmm3, xmm28, Assembler::AVX_512bit);\n+  __ evprolvq(xmm4, xmm4, xmm29, Assembler::AVX_512bit);\n+  __ evmovdquw(xmm2, xmm1, Assembler::AVX_512bit);\n+  __ evmovdquw(xmm5, xmm3, Assembler::AVX_512bit);\n+  __ evpermt2q(xmm0, xmm21, xmm4, Assembler::AVX_512bit);\n+  __ evpermt2q(xmm1, xmm22, xmm3, Assembler::AVX_512bit);\n+  __ evpermt2q(xmm5, xmm22, xmm2, Assembler::AVX_512bit);\n+  __ evmovdquw(xmm3, xmm1, Assembler::AVX_512bit);\n+  __ evmovdquw(xmm2, xmm5, Assembler::AVX_512bit);\n+  __ evpermt2q(xmm1, xmm23, xmm4, Assembler::AVX_512bit);\n+  __ evpermt2q(xmm2, xmm24, xmm4, Assembler::AVX_512bit);\n+  __ evpermt2q(xmm3, xmm25, xmm4, Assembler::AVX_512bit);\n+  __ evpermt2q(xmm4, xmm26, xmm5, Assembler::AVX_512bit);\n+\n+  __ evpermt2q(xmm5, xmm31, xmm0, Assembler::AVX_512bit);\n+  __ evpermt2q(xmm6, xmm31, xmm5, Assembler::AVX_512bit);\n+  __ vpternlogq(xmm0, 180, xmm6, xmm5, Assembler::AVX_512bit);\n+\n+  __ evpermt2q(xmm5, xmm31, xmm1, Assembler::AVX_512bit);\n+  __ evpermt2q(xmm6, xmm31, xmm5, Assembler::AVX_512bit);\n+  __ vpternlogq(xmm1, 180, xmm6, xmm5, Assembler::AVX_512bit);\n+\n+  __ evpxorq(xmm0, k1, xmm0, Address(constant2use, 0), true, Assembler::AVX_512bit);\n+  __ addptr(constant2use, 8);\n+\n+  __ evpermt2q(xmm5, xmm31, xmm2, Assembler::AVX_512bit);\n+  __ evpermt2q(xmm6, xmm31, xmm5, Assembler::AVX_512bit);\n+  __ vpternlogq(xmm2, 180, xmm6, xmm5, Assembler::AVX_512bit);\n+\n+  __ evpermt2q(xmm5, xmm31, xmm3, Assembler::AVX_512bit);\n+  __ evpermt2q(xmm6, xmm31, xmm5, Assembler::AVX_512bit);\n+  __ vpternlogq(xmm3, 180, xmm6, xmm5, Assembler::AVX_512bit);\n+  __ evpermt2q(xmm5, xmm31, xmm4, Assembler::AVX_512bit);\n+  __ evpermt2q(xmm6, xmm31, xmm5, Assembler::AVX_512bit);\n+  __ vpternlogq(xmm4, 180, xmm6, xmm5, Assembler::AVX_512bit);\n+  __ cmpl(roundsLeft, 0);\n+  __ jcc(Assembler::notEqual, rounds24_loop);\n+\n+  if (multiBlock) {\n+    __ addptr(buf, block_size);\n+    __ addl(ofs, block_size);\n+    __ cmpl(ofs, limit);\n+    __ jcc(Assembler::lessEqual, sha3_loop);\n+    __ movq(rax, ofs); \/\/ return ofs\n+  } else {\n+    __ mov64(rax, 0);\n+  }\n+\n+  \/\/ store the state\n+  __ evmovdquq(Address(state, 0), k5, xmm0, true, Assembler::AVX_512bit);\n+  __ evmovdquq(Address(state, 40), k5, xmm1, true, Assembler::AVX_512bit);\n+  __ evmovdquq(Address(state, 80), k5, xmm2, true, Assembler::AVX_512bit);\n+  __ evmovdquq(Address(state, 120), k5, xmm3, true, Assembler::AVX_512bit);\n+  __ evmovdquq(Address(state, 160), k5, xmm4, true, Assembler::AVX_512bit);\n+\n+  __ pop(r14);\n+  __ pop(r13);\n+  __ pop(r12);\n+\n+  __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n+  __ ret(0);\n+\n+  return start;\n+}\n","filename":"src\/hotspot\/cpu\/x86\/stubGenerator_x86_64_sha3.cpp","additions":308,"deletions":0,"binary":false,"changes":308,"status":"added"},{"patch":"@@ -1316,3 +1316,9 @@\n-  if (UseSHA3Intrinsics) {\n-    warning(\"Intrinsics for SHA3-224, SHA3-256, SHA3-384 and SHA3-512 crypto hash functions not available on this CPU.\");\n-    FLAG_SET_DEFAULT(UseSHA3Intrinsics, false);\n+  if (UseAVX > 2) {\n+      if (FLAG_IS_DEFAULT(UseSHA3Intrinsics)) {\n+          UseSHA3Intrinsics = true;\n+      }\n+  } else if (UseSHA3Intrinsics) {\n+      if (!FLAG_IS_DEFAULT(UseSHA3Intrinsics)) {\n+          warning(\"SHA3 intrinsics require AVX512 instructions\");\n+      }\n+      FLAG_SET_DEFAULT(UseSHA3Intrinsics, false);\n","filename":"src\/hotspot\/cpu\/x86\/vm_version_x86.cpp","additions":9,"deletions":3,"binary":false,"changes":12,"status":"modified"}]}