[{"commit":{"message":"8366444: Add support for add\/mul reduction operations for Float16\n\nThis patch adds mid-end support for vectorized add\/mul reduction\noperations for half floats. It also includes backend aarch64 support for\nthese operations. Only vectorization support through autovectorization\nis added as VectorAPI currently does not support Float16 vector species.\n\nBoth add and mul reduction vectorized through autovectorization mandate\nthe implementation to be strictly ordered. The following is how each of\nthese reductions is implemented for different aarch64 targets -\n\nFor AddReduction :\nOn Neon only targets (UseSVE = 0): Generates scalarized additions\nusing the scalar \"fadd\" instruction for both 8B and 16B vector lengths.\nThis is because Neon does not provide a direct instruction for computing\nstrictly ordered floating point add reduction.\n\nOn SVE targets (UseSVE > 0): Generates the \"fadda\" instruction which\ncomputes add reduction for floating point in strict order.\n\nFor MulReduction :\nBoth Neon and SVE do not provide a direct instruction for computing\nstrictly ordered floating point multiply reduction. For vector lengths\nof 8B and 16B, a scalarized sequence of scalar \"fmul\" instructions is\ngenerated and multiply reduction for vector lengths > 16B is not\nsupported.\n\nBelow is the performance of the two newly added microbenchmarks in\nFloat16OperationsBenchmark.java tested on three different aarch64\nmachines and with varying MaxVectorSize -\n\nNote: On all machines, the score (ops\/ms) is compared with the master\nbranch without this patch which generates a sequence of loads (\"ldrsh\")\nto load the FP16 value into an FPR and a scalar \"fadd\/fmul\" to\nadd\/multiply the loaded value to the running sum\/product. The ratios\ngiven below are the ratios between the throughput with this patch and\nthe throughput without this patch.\nRatio > 1 indicates the performance with this patch is better than the\nmaster branch.\n\nN1 (UseSVE = 0, max vector length = 16B):\nBenchmark         vectorDim  Mode   Cnt  8B     16B\nReductionAddFP16  256        thrpt  9    1.41   1.40\nReductionAddFP16  512        thrpt  9    1.41   1.41\nReductionAddFP16  1024       thrpt  9    1.43   1.40\nReductionAddFP16  2048       thrpt  9    1.43   1.40\nReductionMulFP16  256        thrpt  9    1.22   1.22\nReductionMulFP16  512        thrpt  9    1.21   1.23\nReductionMulFP16  1024       thrpt  9    1.21   1.22\nReductionMulFP16  2048       thrpt  9    1.20   1.22\n\nOn N1, the scalarized sequence of fadd\/fmul are generated for both\nMaxVectorSize of 8B and 16B for add reduction and mul reduction\nrespectively.\n\nV1 (UseSVE = 1, max vector length = 32B):\nBenchmark         vectorDim  Mode   Cnt  8B     16B     32B\nReductionAddFP16  256        thrpt  9    1.11   1.75    2.02\nReductionAddFP16  512        thrpt  9    1.02   1.64    1.93\nReductionAddFP16  1024       thrpt  9    1.02   1.59    1.85\nReductionAddFP16  2048       thrpt  9    1.02   1.56    1.80\nReductionMulFP16  256        thrpt  9    1.12   0.99    1.09\nReductionMulFP16  512        thrpt  9    1.04   1.01    1.04\nReductionMulFP16  1024       thrpt  9    1.02   1.02    1.00\nReductionMulFP16  2048       thrpt  9    1.01   1.01    1.00\n\nOn V1, for MaxVectorSize = 8: scalarized fadd\/fmul sequence will be\ngenerated for AddReductionVHF\/MulReductionVHF as UseSVE defaults to 0\n[2].\nFor MaxVectorSize = 16: scalarized \"fmul\" sequence is generated for\nMulReductionVHF and \"fadda\" is generated for AddReductionVHF which\nfetches signficant gains.\nFor MaxVectorSize = 32: Autovectorization of MulReductionVHF is disabled\nfor MaxVectorSize > 16B so the autovectorizer checks for maximal\nimplemented size[1] which is 16B and generates scalarized \"fmul\"\nsequence for 16B in this case. For AddReductionVHF, it generates the\n\"fadda\" instruction.\n\nV2 (UseSVE = 2, max vector length = 16B)\nBenchmark         vectorDim  Mode   Cnt  8B     16B\nReductionAddFP16  256        thrpt  9    1.16   1.70\nReductionAddFP16  512        thrpt  9    1.02   1.61\nReductionAddFP16  1024       thrpt  9    1.01   1.53\nReductionAddFP16  2048       thrpt  9    1.00   1.49\nReductionMulFP16  256        thrpt  9    1.18   0.99\nReductionMulFP16  512        thrpt  9    1.04   1.01\nReductionMulFP16  1024       thrpt  9    1.02   1.02\nReductionMulFP16  2048       thrpt  9    1.01   1.01\n\nOn V2, for MaxVectorSize = 8: scalarized fadd\/fmul sequence will be\ngenerated as UseSVE defaults to 0 [2].\nFor MaxVectorSize = 16: \"fadda\" instruction is generated for\nAddReductionVHF which results in significant gains in performance. For\nMulReductionVHF, the scalarized \"fmul\" sequence will be generated.\n\nTesting:\nhotspot_all, jdk(tiers1-3) and langtools(tier1) all pass on N1\/V1\/V2.\n\n[1] https:\/\/github.com\/openjdk\/jdk\/blob\/a272696813f2e5e896ac9de9985246aaeb9d476c\/src\/hotspot\/share\/opto\/superword.cpp#L1677\n[2] https:\/\/github.com\/openjdk\/jdk\/blob\/a272696813f2e5e896ac9de9985246aaeb9d476c\/src\/hotspot\/cpu\/aarch64\/vm_version_aarch64.cpp#L479"},"files":[{"filename":"src\/hotspot\/cpu\/aarch64\/aarch64_vector.ad"},{"filename":"src\/hotspot\/cpu\/aarch64\/aarch64_vector_ad.m4"},{"filename":"src\/hotspot\/cpu\/aarch64\/c2_MacroAssembler_aarch64.cpp"},{"filename":"src\/hotspot\/cpu\/aarch64\/c2_MacroAssembler_aarch64.hpp"},{"filename":"src\/hotspot\/share\/adlc\/formssel.cpp"},{"filename":"src\/hotspot\/share\/opto\/classes.hpp"},{"filename":"src\/hotspot\/share\/opto\/compile.cpp"},{"filename":"src\/hotspot\/share\/opto\/vectornode.cpp"},{"filename":"src\/hotspot\/share\/opto\/vectornode.hpp"},{"filename":"test\/hotspot\/jtreg\/compiler\/lib\/ir_framework\/IRNode.java"},{"filename":"test\/hotspot\/jtreg\/compiler\/vectorization\/TestFloat16VectorOperations.java"},{"filename":"test\/micro\/org\/openjdk\/bench\/jdk\/incubator\/vector\/Float16OperationsBenchmark.java"}],"sha":"b8eb35bafbe6d1b4e13f8175fc7ac89eb27d8815"}]