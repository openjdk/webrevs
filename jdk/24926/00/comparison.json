{"files":[{"patch":"@@ -213,1 +213,1 @@\n-  \/\/ Mark and visited bits for an LCA calculation in insert_anti_dependences.\n+  \/\/ Mark and visited bits for an LCA calculation in raise_above_anti_dependences.\n@@ -490,1 +490,1 @@\n-  Block* insert_anti_dependences(Block* LCA, Node* load, bool verify = false);\n+  Block* raise_above_anti_dependences(Block* LCA, Node* load, bool verify = false);\n@@ -493,1 +493,1 @@\n-    const_cast<PhaseCFG*>(this)->insert_anti_dependences(LCA, load, true);\n+    const_cast<PhaseCFG*>(this)->raise_above_anti_dependences(LCA, load, true);\n","filename":"src\/hotspot\/share\/opto\/block.hpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -459,0 +459,1 @@\n+  assert(early->dominates(LCA), \"precondition failed\");\n@@ -473,1 +474,1 @@\n-      assert(early->dominates(LCA), \"early is high enough\");\n+      assert(early->dominates(LCA), \"unsound LCA update\");\n@@ -546,1 +547,1 @@\n-\/\/ This function is used by insert_anti_dependences to find unrelated loads for stores in implicit null checks.\n+\/\/ This function is used by raise_above_anti_dependences to find unrelated loads for stores in implicit null checks.\n@@ -600,1 +601,1 @@\n-    \/\/ times as well. When PhaseCFG::insert_anti_dependences() goes over\n+    \/\/ times as well. When PhaseCFG::raise_above_anti_dependences() goes over\n@@ -665,5 +666,6 @@\n-\/\/--------------------------insert_anti_dependences---------------------------\n-\/\/ A load may need to witness memory that nearby stores can overwrite.\n-\/\/ For each nearby store, either insert an \"anti-dependence\" edge\n-\/\/ from the load to the store, or else move LCA upward to force the\n-\/\/ load to (eventually) be scheduled in a block above the store.\n+\/\/------------------------raise_above_anti_dependences---------------------------\n+\/\/ The argument load has a current scheduling range in the dominator tree that\n+\/\/ starts at the load's early block (computed in schedule_early) and ends at\n+\/\/ the argument LCA block. However, there may still exist anti-dependent stores\n+\/\/ in between the early block and the LCA that overwrite memory that the load\n+\/\/ must witness. For such stores, we must\n@@ -671,2 +673,5 @@\n-\/\/ Do not add edges to stores on distinct control-flow paths;\n-\/\/ only add edges to stores which might interfere.\n+\/\/   1. raise the load's LCA to force the load to (eventually) be scheduled at\n+\/\/      latest in the stores's block, and\n+\/\/   2. if the load may get scheduled in the store's block, additionally insert\n+\/\/      an anti-dependence edge from the load to the store to ensure LCM\n+\/\/      schedules the load before the store within the block.\n@@ -674,8 +679,46 @@\n-\/\/ Return the (updated) LCA.  There will not be any possibly interfering\n-\/\/ store between the load's \"early block\" and the updated LCA.\n-\/\/ Any stores in the updated LCA will have new precedence edges\n-\/\/ back to the load.  The caller is expected to schedule the load\n-\/\/ in the LCA, in which case the precedence edges will make LCM\n-\/\/ preserve anti-dependences.  The caller may also hoist the load\n-\/\/ above the LCA, if it is not the early block.\n-Block* PhaseCFG::insert_anti_dependences(Block* LCA, Node* load, bool verify) {\n+\/\/ For a given store, we say that the store is on a _distinct_ control-flow\n+\/\/ path relative to the load if there are no paths from early to LCA that go\n+\/\/ through the store's block. Such stores are not anti-dependent, and there is\n+\/\/ no need to update the LCA nor to add anti-depencence edges.\n+\/\/\n+\/\/ Due to the presence of loops, we must also raise the LCA above\n+\/\/ anti-dependent memory Phis. We defer the details (see later comments in the\n+\/\/ method) and for now look at an example without loops.\n+\/\/\n+\/\/          CFG               DOMINATOR TREE\n+\/\/\n+\/\/       B1 (early,L)              B1\n+\/\/       |\\________                \/\\\\___\n+\/\/       |         \\              \/  \\   \\\n+\/\/       B2 (L,S)   \\            B2  B7  B6\n+\/\/      \/  \\         \\           \/\\\\___\n+\/\/     B3  B4 (S)    B7 (S)     \/  \\   \\\n+\/\/      \\  \/         \/         B3  B4  B5\n+\/\/       B5 (LCA,L) \/\n+\/\/        \\    ____\/\n+\/\/         \\  \/\n+\/\/          B6\n+\/\/\n+\/\/ Here, the load's scheduling range when calling raise_above_anti_dependences\n+\/\/ is between early and LCA in the dominator tree, i.e., in block B1, B2, or B5\n+\/\/ (indicated with \"L\"). However, there are a number of stores (indicated with\n+\/\/ \"S\") that overwrite the memory which the load must witness. First, consider\n+\/\/ the store in B4. We cannot legally schedule the load in B4, so an\n+\/\/ anti-dependence edge is redundant. However, we must raise the LCA above\n+\/\/ B4, which means that the updated LCA is B2. Now, consider the store in B2.\n+\/\/ Raising the LCA above B2 has no effect, because B2 is on the dominator tree\n+\/\/ branch between early and the current LCA (in fact, B2 is the current LCA).\n+\/\/ If we, eventually, decide to schedule the load in B2, it could happen that\n+\/\/ LCM decides to place the load after the anti-dependent store in B2.\n+\/\/ Therefore, we now need to add an anti-dependence edge between the load and\n+\/\/ the B2 store, ensuring that the load is scheduled before the store. Finally,\n+\/\/ the store in B7 is on a distinct control-flow path. Therefore, B7 requires\n+\/\/ no action.\n+\/\/\n+\/\/ The raise_above_anti_dependences method returns the updated LCA and ensures\n+\/\/ there are no anti-dependent stores between the load's early block and the\n+\/\/ updated LCA. Any stores in the updated LCA will have new anti-dependence\n+\/\/ edges back to the load. The caller is expected to eventually schedule the\n+\/\/ load in the LCA, but may also hoist the load above the LCA, if it is not the\n+\/\/ early block.\n+Block* PhaseCFG::raise_above_anti_dependences(Block* LCA, Node* load, bool verify) {\n@@ -715,1 +758,1 @@\n-  \/\/ by the unique point in the dom tree where all memory effects\n+  \/\/ by the unique point in the dominator tree where all memory effects\n@@ -732,4 +775,1 @@\n-  ResourceArea* area = Thread::current()->resource_area();\n-  DefUseMemStatesQueue worklist_def_use_mem_states(area); \/\/ prior memory state to store and possible-def to explore\n-  Node_List non_early_stores(area); \/\/ all relevant stores outside of early\n-  bool must_raise_LCA = false;\n+  assert(early->dominates(LCA_orig), \"precondition failed\");\n@@ -737,2 +777,1 @@\n-  \/\/ 'load' uses some memory state; look for users of the same state.\n-  \/\/ Recurse through MergeMem nodes to the stores that use them.\n+  ResourceArea* area = Thread::current()->resource_area();\n@@ -740,5 +779,5 @@\n-  \/\/ Each of these stores is a possible definition of memory\n-  \/\/ that 'load' needs to use.  We need to force 'load'\n-  \/\/ to occur before each such store.  When the store is in\n-  \/\/ the same block as 'load', we insert an anti-dependence\n-  \/\/ edge load->store.\n+  \/\/ Bookkeeping of possibly anti-dependent stores that we find outside of the\n+  \/\/ early block and that may need anti-dependence edges. For efficiency, we\n+  \/\/ use a lazy approach to add anti-dependence edges, and only add them at the\n+  \/\/ very end when we know the final updated LCA.\n+  Node_List non_early_stores(area);\n@@ -746,7 +785,3 @@\n-  \/\/ The relevant stores \"nearby\" the load consist of a tree rooted\n-  \/\/ at initial_mem, with internal nodes of type MergeMem.\n-  \/\/ Therefore, the branches visited by the worklist are of this form:\n-  \/\/    initial_mem -> (MergeMem ->)* Memory state modifying node\n-  \/\/ Memory state modifying nodes include Store and Phi nodes and any node for which needs_anti_dependence_check()\n-  \/\/ returns false.\n-  \/\/ The anti-dependence constraints apply only to the fringe of this tree.\n+  \/\/ Flag that indicates if we must attempt to raise the LCA after the main\n+  \/\/ worklist loop below.\n+  bool must_raise_LCA = false;\n@@ -754,0 +789,1 @@\n+  \/\/ The input load uses some memory state (initial_mem).\n@@ -755,0 +791,22 @@\n+  \/\/ To find anti-dependences we must look for users of the same memory state.\n+  \/\/ To do this, we search the memory graph downwards from initial_mem. During\n+  \/\/ this search, we encounter different types of nodes that we handle\n+  \/\/ according to the following three categories:\n+  \/\/\n+  \/\/ - MergeMems\n+  \/\/ - Memory-state-modifying nodes (informally referred to as \"stores\" above\n+  \/\/   and below)\n+  \/\/ - Memory Phis\n+  \/\/\n+  \/\/ MergeMems do not modify the memory state. Anti-dependent stores or memory\n+  \/\/ Phis may, however, exist downstream of MergeMems. Therefore, we must\n+  \/\/ permit the search to continue through MergeMems. Memory-state-modifying\n+  \/\/ nodes may raise the LCA and may potentially also require an\n+  \/\/ anti-dependence edge. Memory Phis may raise the LCA but never require\n+  \/\/ anti-dependence edges. See the comments throughout the worklist loop below\n+  \/\/ for further details.\n+  \/\/\n+  \/\/ It may be useful to think of the anti-dependence search as traversing a\n+  \/\/ tree rooted at initial_mem, with internal nodes of type MergeMem and\n+  \/\/ memory Phis and memory-state-modifying nodes as (potentially repeated)\n+  \/\/ leaves.\n@@ -770,6 +828,26 @@\n-  worklist_def_use_mem_states.push(nullptr, initial_mem);\n-  while (worklist_def_use_mem_states.is_nonempty()) {\n-    \/\/ Examine a nearby store to see if it might interfere with our load.\n-    Node* def_mem_state = worklist_def_use_mem_states.top_def();\n-    Node* use_mem_state = worklist_def_use_mem_states.top_use();\n-    worklist_def_use_mem_states.pop();\n+  \/\/ To administer the search, we use a worklist consisting of (def,use)-pairs\n+  \/\/ of memory states, corresponding to edges in the search tree (and edges\n+  \/\/ in the memory graph). We need to keep track of search tree edges in the\n+  \/\/ worklist rather than individual nodes due to memory Phis (see details\n+  \/\/ below).\n+  DefUseMemStatesQueue worklist(area);\n+  \/\/ We start the search at initial_mem and indicate the search root with the\n+  \/\/ edge (nullptr, initial_mem).\n+  worklist.push(nullptr, initial_mem);\n+\n+  \/\/ The worklist loop\n+  while (worklist.is_nonempty()) {\n+    \/\/ Pop the next edge from the worklist\n+    Node* def_mem_state = worklist.top_def();\n+    Node* use_mem_state = worklist.top_use();\n+    worklist.pop();\n+\n+    \/\/ We are either\n+    \/\/ - at the root of the search with the edge (nullptr, initial_mem),\n+    \/\/ - just past initial_mem with the edge (initial_mem, use_mem_state), or\n+    \/\/ - just past a MergeMem with the edge (MergeMem, use_mem_state).\n+    \/\/ we have passed a MergeMem and are now at an edge\n+    \/\/ (MergeMem, use_mem_state).\n+    assert(def_mem_state == nullptr || def_mem_state == initial_mem ||\n+               def_mem_state->is_MergeMem(),\n+           \"invariant failed\");\n@@ -790,9 +868,7 @@\n-    \/\/ MergeMems do not directly have anti-deps.\n-    \/\/ Treat them as internal nodes in a forward tree of memory states,\n-    \/\/ the leaves of which are each a 'possible-def'.\n-    if (use_mem_state == initial_mem    \/\/ root (exclusive) of tree we are searching\n-        || op == Op_MergeMem    \/\/ internal node of tree we are searching\n-        ) {\n-      def_mem_state = use_mem_state;   \/\/ It's not a possibly interfering store.\n-      if (use_mem_state == initial_mem)\n-        initial_mem = nullptr;  \/\/ only process initial memory once\n+    \/\/ If we are either at the search root or have found a MergeMem, we step\n+    \/\/ past use_mem_state and populate the search worklist with edges\n+    \/\/ (use_mem_state, child) for use_mem_state's children.\n+    if (def_mem_state == nullptr \/\/ root (exclusive) of tree we are searching\n+        || op == Op_MergeMem     \/\/ internal node of tree we are searching\n+    ) {\n+      def_mem_state = use_mem_state;\n@@ -803,2 +879,3 @@\n-          \/\/ use_mem_state is also a kind of load (i.e. needs_anti_dependence_check), and it is not a memory state\n-          \/\/ modifying node (store, Phi or MergeMem). Hence, load can't be anti dependent on this node.\n+          \/\/ use_mem_state is also a kind of load (i.e.,\n+          \/\/ needs_anti_dependence_check), and it is not a store nor a memory\n+          \/\/ Phi. Hence, it is not anti-dependent on the load.\n@@ -807,1 +884,1 @@\n-        worklist_def_use_mem_states.push(def_mem_state, use_mem_state);\n+        worklist.push(def_mem_state, use_mem_state);\n@@ -809,0 +886,2 @@\n+      \/\/ Nothing more to do for the current (nullptr, initial_mem) or\n+      \/\/ (initial_mem\/MergeMem, MergeMem) edge, move on.\n@@ -812,0 +891,3 @@\n+    \/\/ At this point, use_mem_state is either a store or a memory Phi.\n+    assert(!use_mem_state->is_MergeMem(), \"invariant failed\");\n+\n@@ -814,3 +896,3 @@\n-    \/\/ Compute the alias index.  Loads and stores with different alias\n-    \/\/ indices do not need anti-dependence edges.  Wide MemBar's are\n-    \/\/ anti-dependent on everything (except immutable memories).\n+    \/\/ Compute the alias index. If the use_mem_state has an alias index\n+    \/\/ different from the load's, it is not anti-dependent. Wide MemBar's\n+    \/\/ are anti-dependent with everything (except immutable memories).\n@@ -823,1 +905,1 @@\n-      MachNode* mstore = use_mem_state->as_Mach();\n+      MachNode* muse = use_mem_state->as_Mach();\n@@ -829,3 +911,3 @@\n-        if (mstore->ideal_Opcode() == Op_CallStaticJava) {\n-          assert(mstore->is_MachSafePoint(), \"\");\n-          MachSafePointNode* ms = (MachSafePointNode*) mstore;\n+        if (muse->ideal_Opcode() == Op_CallStaticJava) {\n+          assert(muse->is_MachSafePoint(), \"\");\n+          MachSafePointNode* ms = (MachSafePointNode*)muse;\n@@ -836,1 +918,1 @@\n-            \/\/ (other than Raw) and so do not require anti-dependence edges.\n+            \/\/ (other than Raw) and so are not anti-dependent.\n@@ -843,1 +925,1 @@\n-        if (mstore->ideal_Opcode() == Op_SafePoint)\n+        if (muse->ideal_Opcode() == Op_SafePoint) {\n@@ -845,0 +927,1 @@\n+        }\n@@ -849,1 +932,1 @@\n-        \/\/ Inserting an anti-dep between such a safepoint and a use\n+        \/\/ Inserting an anti-dependence edge between such a safepoint and a use\n@@ -853,1 +936,1 @@\n-        if (mstore->ideal_Opcode() == Op_SafePoint && load->in(0) == mstore)\n+        if (muse->ideal_Opcode() == Op_SafePoint && load->in(0) == muse) {\n@@ -855,0 +938,1 @@\n+        }\n@@ -858,6 +942,17 @@\n-    \/\/ Identify a block that the current load must be above,\n-    \/\/ or else observe that 'store' is all the way up in the\n-    \/\/ earliest legal block for 'load'.  In the latter case,\n-    \/\/ immediately insert an anti-dependence edge.\n-    Block* store_block = get_block_for_node(use_mem_state);\n-    assert(store_block != nullptr, \"unused killing projections skipped above\");\n+    \/\/ Determine the block of the use_mem_state.\n+    Block* use_mem_state_block = get_block_for_node(use_mem_state);\n+    assert(use_mem_state_block != nullptr,\n+           \"unused killing projections skipped above\");\n+\n+    \/\/ For efficiency, we take a lazy approach to both raising the LCA and\n+    \/\/ adding anti-dependence edges. In this worklist loop, we only mark blocks\n+    \/\/ which we must raise the LCA above (set_raise_LCA_mark), and keep\n+    \/\/ track of nodes that potentially need anti-dependence edges\n+    \/\/ (non_early_stores). The only exceptions to this is if we\n+    \/\/ immediately see that we have to raise the LCA all the way to the early\n+    \/\/ block, and if we find stores in the early block (which always need\n+    \/\/ anti-dependence edges).\n+    \/\/\n+    \/\/ After the worklist loop, we perform an efficient combined LCA-raising\n+    \/\/ operation over all marks and then only add anti-dependence edges where\n+    \/\/ strictly necessary according to the new raised LCA.\n@@ -866,2 +961,25 @@\n-      \/\/ Loop-phis need to raise load before input. (Other phis are treated\n-      \/\/ as store below.)\n+      \/\/ We have reached a memory Phi node. On our search from initial_mem to\n+      \/\/ the Phi, we have found no anti-dependences (otherwise, we would have\n+      \/\/ already terminated the search along this branch). Consider the example\n+      \/\/ below, indicating a Phi node and its node inputs (we omit the control\n+      \/\/ input).\n+      \/\/\n+      \/\/    def_mem_state\n+      \/\/          |\n+      \/\/          |??\n+      \/\/          |||\n+      \/\/          Phi\n+      \/\/\n+      \/\/ We reached the Phi from def_mem_state and know that, on this\n+      \/\/ particular input, the memory that the load must witness is not\n+      \/\/ overwritten. However, for the Phi's other inputs (? in the\n+      \/\/ illustration), we have no information and must thus conservatively\n+      \/\/ assume that the load's memory is overwritten at and below the Phi.\n+      \/\/\n+      \/\/ It is impossible to schedule the load before the Phi in\n+      \/\/ the same block as the Phi (use_mem_state_block), and anti-dependence\n+      \/\/ edges are, therefore, redundant. We must, however, find the\n+      \/\/ predecessor block of use_mem_state_block that corresponds to\n+      \/\/ def_mem_state, and raise the LCA above that block. Note that this block\n+      \/\/ is not necessarily def_mem_state's block! See the continuation of our\n+      \/\/ previous example below (now illustrating blocks instead of nodes)\n@@ -869,3 +987,8 @@\n-      \/\/ 'load' uses memory which is one (or more) of the Phi's inputs.\n-      \/\/ It must be scheduled not before the Phi, but rather before\n-      \/\/ each of the relevant Phi inputs.\n+      \/\/    def_mem_state's block\n+      \/\/          |\n+      \/\/          |\n+      \/\/      pred_block\n+      \/\/          |\n+      \/\/          |   ?   ?\n+      \/\/          |   |   |\n+      \/\/      use_mem_state_block\n@@ -873,3 +996,2 @@\n-      \/\/ Instead of finding the LCA of all inputs to a Phi that match 'mem',\n-      \/\/ we mark each corresponding predecessor block and do a combined\n-      \/\/ hoisting operation later (raise_LCA_above_marks).\n+      \/\/ Here, we must raise the LCA above pred_block rather than\n+      \/\/ def_mem_state's block.\n@@ -877,1 +999,1 @@\n-      \/\/ Do not assert(store_block != early, \"Phi merging memory after access\")\n+      \/\/ Do not assert(use_mem_state_block != early, \"Phi merging memory after access\")\n@@ -879,0 +1001,4 @@\n+      if (LCA == early) {\n+        \/\/ Don't bother if LCA is already raised all the way\n+        continue;\n+      }\n@@ -883,1 +1009,1 @@\n-          Block* pred_block = get_block_for_node(store_block->pred(j));\n+          Block* pred_block = get_block_for_node(use_mem_state_block->pred(j));\n@@ -885,3 +1011,1 @@\n-            \/\/ If any predecessor of the Phi matches the load's \"early block\",\n-            \/\/ we do not need a precedence edge between the Phi and 'load'\n-            \/\/ since the load will be forced into a block preceding the Phi.\n+            \/\/ Lazily set the LCA mark\n@@ -889,2 +1013,0 @@\n-            assert(!LCA_orig->dominates(pred_block) ||\n-                   early->dominates(pred_block), \"early is high enough\");\n@@ -892,3 +1014,4 @@\n-          } else {\n-            \/\/ anti-dependent upon PHI pinned below 'early', no edge needed\n-            LCA = early;             \/\/ but can not schedule below 'early'\n+          } else \/* if (pred_block == early *\/ {\n+            \/\/ We know already now that we must raise LCA all the way to early.\n+            LCA = early;\n+            \/\/ This turns off the process of gathering non_early_stores.\n@@ -899,12 +1022,8 @@\n-    } else if (store_block != early) {\n-      \/\/ 'store' is between the current LCA and earliest possible block.\n-      \/\/ Label its block, and decide later on how to raise the LCA\n-      \/\/ to include the effect on LCA of this store.\n-      \/\/ If this store's block gets chosen as the raised LCA, we\n-      \/\/ will find him on the non_early_stores list and stick him\n-      \/\/ with a precedence edge.\n-      \/\/ (But, don't bother if LCA is already raised all the way.)\n-      if (LCA != early && !unrelated_load_in_store_null_block(use_mem_state, load)) {\n-        store_block->set_raise_LCA_mark(load_index);\n-        must_raise_LCA = true;\n-        non_early_stores.push(use_mem_state);\n+    } else if (use_mem_state_block != early) {\n+      \/\/ We found an anti-dependent store outside the load's 'early' block.\n+      \/\/ The store may be between the current LCA and earliest possible block\n+      \/\/ (but it could very well also be on a distinct control-flow path).\n+      \/\/ Lazily set the LCA mark and push to non_early_stores.\n+      if (LCA == early) {\n+        \/\/ Don't bother if LCA is already raised all the way\n+        continue;\n@@ -912,4 +1031,10 @@\n-    } else {\n-      \/\/ Found a possibly-interfering store in the load's 'early' block.\n-      \/\/ This means 'load' cannot sink at all in the dominator tree.\n-      \/\/ Add an anti-dep edge, and squeeze 'load' into the highest block.\n+      if (unrelated_load_in_store_null_block(use_mem_state, load)) {\n+        continue;\n+      }\n+      use_mem_state_block->set_raise_LCA_mark(load_index);\n+      must_raise_LCA = true;\n+      non_early_stores.push(use_mem_state);\n+    } else \/* if (use_mem_state_block == early) *\/ {\n+      \/\/ We found an anti-dependent store in the load's 'early' block.\n+      \/\/ Therefore, we know already now that we must raise LCA all the way to\n+      \/\/ early and that we need to add an anti-dependence edge to the store.\n@@ -927,1 +1052,1 @@\n-  \/\/ (Worklist is now empty; all nearby stores have been visited.)\n+  \/\/ (Worklist is now empty; we have visited all possible anti-dependences.)\n@@ -932,1 +1057,3 @@\n-  if (LCA == early)  return LCA;\n+  if (LCA == early) {\n+    return LCA;\n+  }\n@@ -934,11 +1061,6 @@\n-  \/\/ We get here only if there are no possibly-interfering stores\n-  \/\/ in the load's 'early' block.  Move LCA up above all predecessors\n-  \/\/ which contain stores we have noted.\n-  \/\/\n-  \/\/ The raised LCA block can be a home to such interfering stores,\n-  \/\/ but its predecessors must not contain any such stores.\n-  \/\/\n-  \/\/ The raised LCA will be a lower bound for placing the load,\n-  \/\/ preventing the load from sinking past any block containing\n-  \/\/ a store that may invalidate the memory state required by 'load'.\n-  if (must_raise_LCA)\n+  \/\/ We get here only if there are no anti-dependent stores in the load's\n+  \/\/ 'early' block and if no memory Phi has forced LCA to the early block. Now\n+  \/\/ we must raise the LCA above the blocks for all the anti-dependent stores\n+  \/\/ and above the predecessor blocks of anti-dependent memory Phis we reached\n+  \/\/ during the search.\n+  if (must_raise_LCA) {\n@@ -946,1 +1068,8 @@\n-  if (LCA == early)  return LCA;\n+  }\n+\n+  \/\/ If LCA == early at this point, there were no stores that required\n+  \/\/ anti-dependence edges in the early block. Otherwise, we would have eagerly\n+  \/\/ raised the LCA to early already in the worklist loop.\n+  if (LCA == early) {\n+    return LCA;\n+  }\n@@ -948,3 +1077,16 @@\n-  \/\/ Insert anti-dependence edges from 'load' to each store\n-  \/\/ in the non-early LCA block.\n-  \/\/ Mine the non_early_stores list for such stores.\n+  \/\/ The raised LCA block can now be a home to anti-dependent stores for which\n+  \/\/ we still need to add anti-dependence edges, but no LCA predecessor block\n+  \/\/ contains any such stores (otherwise, we would have raised the LCA even\n+  \/\/ higher).\n+\n+  \/\/ The raised LCA will be a lower bound for placing the load, preventing the\n+  \/\/ load from sinking past any block containing a store that may overwrite\n+  \/\/ memory that the load must witness.\n+\n+  \/\/ Now we need to insert the necessary anti-dependence edges from 'load' to\n+  \/\/ each store in the non-early LCA block. We have recorded all such potential\n+  \/\/ stores in non_early_stores.\n+  \/\/\n+  \/\/ If LCA->raise_LCA_mark() != load_index, it means that we raised the LCA to\n+  \/\/ a block in which we did not find any anti-dependent stores. So, no need to\n+  \/\/ search for any such stores.\n@@ -956,1 +1098,2 @@\n-        \/\/ add anti_dependence from store to load in its own block\n+        \/\/ Add anti-dependence edge from the load to the store in the non-early\n+        \/\/ LCA.\n@@ -965,5 +1108,0 @@\n-        \/\/ Any other stores we found must be either inside the new LCA\n-        \/\/ or else outside the original LCA.  In the latter case, they\n-        \/\/ did not interfere with any use of 'load'.\n-        assert(LCA->dominates(store_block)\n-               || !LCA_orig->dominates(store_block), \"no stray stores\");\n@@ -974,0 +1112,2 @@\n+  assert(LCA->dominates(LCA_orig), \"unsound updated LCA\");\n+\n@@ -1535,1 +1675,1 @@\n-      LCA = insert_anti_dependences(LCA, self);\n+      LCA = raise_above_anti_dependences(LCA, self);\n","filename":"src\/hotspot\/share\/opto\/gcm.cpp","additions":270,"deletions":130,"binary":false,"changes":400,"status":"modified"},{"patch":"@@ -494,1 +494,1 @@\n-        insert_anti_dependences(block, n);\n+        raise_above_anti_dependences(block, n);\n@@ -1366,1 +1366,1 @@\n-        insert_anti_dependences(sb, clone);\n+        raise_above_anti_dependences(sb, clone);\n","filename":"src\/hotspot\/share\/opto\/lcm.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -27,1 +27,1 @@\n- * @summary C2: high memory usage in PhaseCFG::insert_anti_dependences()\n+ * @summary C2: high memory usage in PhaseCFG::raise_above_anti_dependences()\n","filename":"test\/hotspot\/jtreg\/compiler\/codegen\/TestAntiDependenciesHighMemUsage.java","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -27,1 +27,1 @@\n- * @summary C2: high memory usage in PhaseCFG::insert_anti_dependences()\n+ * @summary C2: high memory usage in PhaseCFG::raise_above_anti_dependences()\n","filename":"test\/hotspot\/jtreg\/compiler\/codegen\/TestAntiDependenciesHighMemUsage2.java","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2020, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2020, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -141,1 +141,1 @@\n-    \/\/ Triggers an assert in PhaseCFG::insert_anti_dependences if loop strip mining verification is disabled:\n+    \/\/ Triggers an assert in PhaseCFG::raise_above_anti_dependences if loop strip mining verification is disabled:\n","filename":"test\/hotspot\/jtreg\/compiler\/loopopts\/TestSplitIfPinnedLoadInStripMinedLoop.java","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"}]}