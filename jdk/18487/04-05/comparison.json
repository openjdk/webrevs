{"files":[{"patch":"@@ -2864,0 +2864,23 @@\n+#undef INSN\n+\n+#define INSN(NAME, op1, op2)                                                                       \\\n+  void NAME(FloatRegister Vd, SIMD_Arrangement T, FloatRegister Vn, FloatRegister Vm, int index) { \\\n+    starti;                                                                                        \\\n+    assert(T == T4H || T == T8H || T == T2S || T == T4S, \"invalid arrangement\");                   \\\n+    assert(index >= 0 &&                                                                           \\\n+               ((T == T2S && index <= 1) || (T != T2S && index <= 3) || (T == T8H && index <= 7)), \\\n+           \"invalid index\");                                                                       \\\n+    assert((T != T4H && T != T8H) || Vm->encoding() < 16, \"invalid source SIMD&FP register\");      \\\n+    f(0, 31), f((int)T & 1, 30), f(op1, 29);                                                       \\\n+    f(0b01111, 28, 24);                                                                            \\\n+    if (T == T4H || T == T8H) {                                                                    \\\n+      f(0b01, 23, 22), f(index & 0b11, 21, 20), rf(Vm, 16), f(op2, 15, 12), f(index >> 2 & 1, 11); \\\n+    } else {                                                                                       \\\n+      f(0b10, 23, 22), f(index & 1, 21), rf(Vm, 16), f(op2, 15, 12), f(index >> 1, 11);            \\\n+    }                                                                                              \\\n+    f(0, 10), rf(Vn, 5), rf(Vd, 0);                                                                \\\n+  }\n+\n+  \/\/ MUL - Vector - Scalar\n+  INSN(mulv, 0, 0b1000);\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/assembler_aarch64.hpp","additions":23,"deletions":0,"binary":false,"changes":23,"status":"modified"},{"patch":"@@ -36,0 +36,1 @@\n+#include \"utilities\/powerOfTwo.hpp\"\n@@ -56,1 +57,1 @@\n-  Label TAIL, RELATIVE;\n+  Label TAIL, STUB_SWITCH, STUB_SWITCH_OUT, LOOP, RELATIVE, LARGE, DONE;\n@@ -58,0 +59,10 @@\n+  \/\/ Vectorization factor. Number of array elements loaded to one SIMD&FP registers by the stubs. We\n+  \/\/ use 8H load arrangements for chars and shorts and 8B for booleans and bytes. It's possible to\n+  \/\/ use 4H for chars and shorts instead, but using 8H gives better performance.\n+  const size_t vf = eltype == T_BOOLEAN || eltype == T_BYTE ? 8\n+                    : eltype == T_CHAR || eltype == T_SHORT ? 8\n+                    : eltype == T_INT                       ? 4\n+                                                            : 0;\n+  guarantee(vf, \"unsupported eltype\");\n+\n+  \/\/ Unroll factor for the scalar loop below. The value is chosen based on performance analysis.\n@@ -59,5 +70,0 @@\n-  const size_t loop_factor = eltype == T_BOOLEAN || eltype == T_BYTE ? 32\n-                             : eltype == T_CHAR || eltype == T_SHORT ? 16\n-                             : eltype == T_INT                       ? 16\n-                                                                     : 0;\n-  guarantee(loop_factor, \"unsupported eltype\");\n@@ -85,11 +91,6 @@\n-  subsw(cnt, cnt, loop_factor);\n-  br(Assembler::LO, TAIL);\n-\n-  RuntimeAddress stub = RuntimeAddress(StubRoutines::aarch64::large_arrays_hashcode(eltype));\n-  assert(stub.target() != nullptr, \"array_hashcode stub has not been generated\");\n-  address tpc = trampoline_call(stub);\n-  if (tpc == nullptr) {\n-    DEBUG_ONLY(reset_labels(TAIL, RELATIVE));\n-    postcond(pc() == badAddress);\n-    return nullptr;\n-  }\n+  \/\/ large_arrays_hashcode(T_INT) performs worse than the scalar loop below when the Neon loop\n+  \/\/ implemented by the stub executes just once. Call the stub only if at least two iteration will\n+  \/\/ be executed.\n+  const size_t large_threshold = eltype == T_INT ? vf * 2 : vf;\n+  cmpw(cnt, large_threshold);\n+  br(Assembler::HS, LARGE);\n@@ -99,0 +100,3 @@\n+  \/\/ The orr performs (r - lc) % uf where uf = unroll_factor. The subtract shifted by 3\n+  \/\/ offsets past |(r - lc) % uf| pairs of load + madd insns i.e. it only executes\n+  \/\/ r % uf load + madds. Iteration eats up the remainder, uf elements at a time.\n@@ -100,1 +104,1 @@\n-  orr(tmp2, cnt, 0x1fff ^ (unroll_factor - 1));\n+  andr(tmp2, cnt, unroll_factor - 1);\n@@ -104,2 +108,0 @@\n-  addw(cnt, cnt, loop_factor);\n-\n@@ -108,1 +110,1 @@\n-  bind(RELATIVE);\n+  bind(LOOP);\n@@ -113,0 +115,1 @@\n+  bind(RELATIVE);\n@@ -114,1 +117,16 @@\n-  br(Assembler::HS, RELATIVE);\n+  br(Assembler::HS, LOOP);\n+\n+  b(DONE);\n+\n+  bind(LARGE);\n+\n+  RuntimeAddress stub = RuntimeAddress(StubRoutines::aarch64::large_arrays_hashcode(eltype));\n+  assert(stub.target() != nullptr, \"array_hashcode stub has not been generated\");\n+  address tpc = trampoline_call(stub);\n+  if (tpc == nullptr) {\n+    DEBUG_ONLY(reset_labels(TAIL, RELATIVE));\n+    postcond(pc() == badAddress);\n+    return nullptr;\n+  }\n+\n+  bind(DONE);\n","filename":"src\/hotspot\/cpu\/aarch64\/c2_MacroAssembler_aarch64.cpp","additions":40,"deletions":22,"binary":false,"changes":62,"status":"modified"},{"patch":"@@ -56,0 +56,1 @@\n+#include \"utilities\/debug.hpp\"\n@@ -5315,0 +5316,23 @@\n+  void large_arrays_hashcode_elload(Register dst, Address src, BasicType eltype) {\n+    switch (eltype) {\n+    \/\/ T_BOOLEAN used as surrogate for unsigned byte\n+    case T_BOOLEAN:\n+      __ ldrb(dst, src);\n+      break;\n+    case T_BYTE:\n+      __ ldrsb(dst, src);\n+      break;\n+    case T_SHORT:\n+      __ ldrsh(dst, src);\n+      break;\n+    case T_CHAR:\n+      __ ldrh(dst, src);\n+      break;\n+    case T_INT:\n+      __ ldrw(dst, src);\n+      break;\n+    default:\n+      ShouldNotReachHere();\n+    }\n+  }\n+\n@@ -5320,7 +5344,6 @@\n-    Register result = r0, ary = r1, cnt = r2;\n-    FloatRegister vdata0 = v3, vdata1 = v2, vdata2 = v1, vdata3 = v0;\n-    FloatRegister vhalf0 = v13, vhalf1 = v12, vhalf2 = v11, vhalf3 = v10;\n-    FloatRegister vmul0 = v4, vmul1 = v5, vmul2 = v6, vmul3 = v7;\n-    FloatRegister vpow = v8;  \/\/ <31^(4*k+3), ..., 31^(4*k+0)>\n-    FloatRegister vpowm = v9; \/\/ multiple of loop factor power of 31,\n-                              \/\/ i.e. <31^16, ..., 31^16> for ints\n+    const Register result = r0, ary = r1, cnt = r2;\n+    const FloatRegister vdata0 = v3, vdata1 = v2, vdata2 = v1, vdata3 = v0;\n+    const FloatRegister vhalf0 = v13, vhalf1 = v12, vhalf2 = v11, vhalf3 = v10;\n+    const FloatRegister vmul0 = v4, vmul1 = v5, vmul2 = v6, vmul3 = v7;\n+    const FloatRegister vpow = v8;  \/\/ powers of 31: <31^3, ..., 31^0>\n+    const FloatRegister vpowm = v9;\n@@ -5332,1 +5355,8 @@\n-    Label LOOP;\n+    Label SMALL_LOOP, LARGE_LOOP_PREHEADER, LARGE_LOOP, TAIL, TAIL_SHORTCUT, RELATIVE;\n+\n+    \/\/ Vectorization factor\n+    const size_t vf = eltype == T_BOOLEAN || eltype == T_BYTE ? 8\n+                      : eltype == T_CHAR || eltype == T_SHORT ? 8\n+                      : eltype == T_INT                       ? 4\n+                                                              : 0;\n+    guarantee(vf, \"unsupported eltype\");\n@@ -5334,5 +5364,5 @@\n-    const size_t loop_factor = eltype == T_BOOLEAN || eltype == T_BYTE ? 32\n-                               : eltype == T_CHAR || eltype == T_SHORT ? 16\n-                               : eltype == T_INT                       ? 16\n-                                                                       : 0;\n-    guarantee(loop_factor, \"unsupported eltype\");\n+    \/\/ Unroll factor\n+    const size_t uf = 4;\n+\n+    \/\/ Effective vectorization factor\n+    const size_t evf = vf * uf;\n@@ -5369,1 +5399,0 @@\n-    size_t bytes_per_iteration = loop_factor * type2aelembytes(eltype);\n@@ -5372,1 +5401,1 @@\n-        : eltype == T_CHAR || eltype == T_SHORT ? Assembler::T4H\n+        : eltype == T_CHAR || eltype == T_SHORT ? Assembler::T8H\n@@ -5377,5 +5406,130 @@\n-    if (eltype == T_INT || eltype == T_CHAR || eltype == T_SHORT) {\n-      \/\/ 31^16\n-      __ movw(rscratch1, intpow(31U, 16));\n-      __ dup(vpowm, Assembler::T4S, rscratch1);\n-    } else if (eltype == T_BOOLEAN || eltype == T_BYTE) {\n+    const int multiply_by_halves =\n+        load_arrangement == Assembler::T4S || load_arrangement == Assembler::T4H   ? false\n+        : load_arrangement == Assembler::T8B || load_arrangement == Assembler::T8H ? true\n+                                                                                   : -1;\n+    guarantee(multiply_by_halves != -1, \"unknown multiplication algorithm\");\n+\n+    const int small_loop_size = load_arrangement == Assembler::T4S   ? 20  \/\/ 5 insts\n+                                : load_arrangement == Assembler::T4H ? 24  \/\/ 6 insts\n+                                : load_arrangement == Assembler::T8H ? 36  \/\/ 9 insts\n+                                : load_arrangement == Assembler::T8B ? 40  \/\/ 10 insts\n+                                                                     : -1; \/\/ invalid\n+    guarantee(small_loop_size != -1, \"invalid small_loop_size\");\n+\n+    \/\/ Put 0-3'th powers of 31 into a single SIMD register together. The register will be used in\n+    \/\/ the SMALL and LARGE LOOPS' EPILOQUES. The initialization is hoisted here and the register's\n+    \/\/ value shouldn't change throughout both loops.\n+    __ movw(rscratch1, intpow(31U, 3));\n+    __ mov(vpow, Assembler::S, 0, rscratch1);\n+    __ movw(rscratch1, intpow(31U, 2));\n+    __ mov(vpow, Assembler::S, 1, rscratch1);\n+    __ movw(rscratch1, intpow(31U, 1));\n+    __ mov(vpow, Assembler::S, 2, rscratch1);\n+    __ movw(rscratch1, intpow(31U, 0));\n+    __ mov(vpow, Assembler::S, 3, rscratch1);\n+\n+    __ mov(vmul0, Assembler::T16B, 0);\n+    __ mov(vmul0, Assembler::S, 3, result);\n+\n+    __ andr(rscratch2, cnt, (uf - 1) * vf);\n+    __ cbz(rscratch2, LARGE_LOOP_PREHEADER);\n+\n+    __ movw(rscratch1, intpow(31U, multiply_by_halves ? vf \/ 2 : vf));\n+    __ mov(vpowm, Assembler::S, 0, rscratch1);\n+\n+    if (small_loop_size % 32 > 32 - __ offset() % 32) {\n+      __ align(32);\n+    }\n+\n+    auto start = __ offset();\n+    __ bind(SMALL_LOOP);\n+\n+    __ ld1(vdata0, load_arrangement, Address(__ post(ary, vf * type2aelembytes(eltype))));\n+    __ mulv(vmul0, Assembler::T4S, vmul0, vpowm, 0);\n+    __ subsw(rscratch2, rscratch2, vf);\n+\n+    if (load_arrangement == Assembler::T8B) {\n+      \/\/ Extend 8B to 8H to be able to use vector multiply\n+      \/\/ instructions\n+      assert(load_arrangement == Assembler::T8B, \"expected to extend 8B to 8H\");\n+      if (is_signed_subword_type(eltype)) {\n+        __ sxtl(vdata0, Assembler::T8H, vdata0, load_arrangement);\n+      } else {\n+        __ uxtl(vdata0, Assembler::T8H, vdata0, load_arrangement);\n+      }\n+    }\n+\n+    if (load_arrangement == Assembler::T4S) {\n+      __ addv(vmul0, load_arrangement, vmul0, vdata0);\n+    } else if (load_arrangement == Assembler::T8B || load_arrangement == Assembler::T4H ||\n+               load_arrangement == Assembler::T8H) {\n+      assert(is_subword_type(eltype), \"subword type expected\");\n+      if (is_signed_subword_type(eltype)) {\n+        __ sxtl(vhalf0, Assembler::T4S, vdata0, Assembler::T4H);\n+      } else {\n+        __ uxtl(vhalf0, Assembler::T4S, vdata0, Assembler::T4H);\n+      }\n+      __ addv(vmul0, Assembler::T4S, vmul0, vhalf0);\n+    } else {\n+      __ should_not_reach_here();\n+    }\n+\n+    \/\/ Process the upper half of a vector\n+    if (load_arrangement == Assembler::T8B || load_arrangement == Assembler::T8H) {\n+      __ mulv(vmul0, Assembler::T4S, vmul0, vpowm, 0);\n+      if (is_signed_subword_type(eltype)) {\n+        __ sshll2(vhalf0, Assembler::T4S, vdata0, Assembler::T8H, 0);\n+      } else {\n+        __ ushll2(vhalf0, Assembler::T4S, vdata0, Assembler::T8H, 0);\n+      }\n+      __ addv(vmul0, Assembler::T4S, vmul0, vhalf0);\n+    }\n+\n+    __ br(Assembler::HI, SMALL_LOOP);\n+    guarantee(__ offset() - start == small_loop_size, \"Incorrect small_loop_size\");\n+\n+    \/\/ SMALL LOOP'S EPILOQUE\n+\n+    __ lsr(rscratch2, cnt, exact_log2(evf));\n+    __ cbnz(rscratch2, LARGE_LOOP_PREHEADER);\n+\n+    __ mulv(vmul0, Assembler::T4S, vmul0, vpow);\n+    __ addv(vmul0, Assembler::T4S, vmul0);\n+    __ umov(result, vmul0, Assembler::S, 0);\n+\n+    \/\/ TAIL\n+\n+    const int tail_size = (8 + (vf - 1) * 2) * 4; \/\/ 14 or 22 insts\n+    if (tail_size % 32 > 32 - __ offset() % 32) {\n+      __ align(32);\n+    }\n+\n+    __ bind(TAIL);\n+    start = __ offset();\n+    assert(is_power_of_2(vf), \"can't use this value to calculate the jump target PC\");\n+    __ andr(rscratch2, cnt, vf - 1);\n+    __ bind(TAIL_SHORTCUT);\n+    __ adr(rscratch1, RELATIVE);\n+    __ sub(rscratch1, rscratch1, rscratch2, ext::uxtw, 3);\n+    __ movw(rscratch2, 0x1f);\n+    __ br(rscratch1);\n+\n+    for (size_t i = 0; i < vf - 1; ++i) {\n+      large_arrays_hashcode_elload(rscratch1, Address(__ post(ary, type2aelembytes(eltype))),\n+                                   eltype);\n+      __ maddw(result, result, rscratch2, rscratch1);\n+    }\n+    __ bind(RELATIVE);\n+\n+    __ leave();\n+    __ ret(lr);\n+    guarantee(__ offset() - start == tail_size, \"unexptected size of the tail code block\");\n+\n+    \/\/ LARGE LOOP\n+\n+    __ align(32);\n+    __ bind(LARGE_LOOP_PREHEADER);\n+\n+    __ lsr(rscratch2, cnt, exact_log2(evf));\n+\n+    if (multiply_by_halves) {\n@@ -5383,2 +5537,2 @@\n-      __ movw(rscratch1, intpow(31U, 4));\n-      __ dup(vpow, Assembler::T4S, rscratch1);\n+      __ movw(rscratch1, intpow(31U, vf \/ 2));\n+      __ mov(vpowm, Assembler::S, 1, rscratch1);\n@@ -5386,2 +5540,2 @@\n-      __ movw(rscratch1, intpow(31U, 28));\n-      __ dup(vpowm, Assembler::T4S, rscratch1);\n+      __ movw(rscratch1, intpow(31U, evf - vf \/ 2));\n+      __ mov(vpowm, Assembler::S, 0, rscratch1);\n@@ -5389,1 +5543,3 @@\n-      __ should_not_reach_here();\n+      \/\/ 31^16\n+      __ movw(rscratch1, intpow(31U, evf));\n+      __ mov(vpowm, Assembler::S, 0, rscratch1);\n@@ -5395,2 +5551,0 @@\n-    __ mov(vmul0, Assembler::T16B, 0);\n-    __ mov(vmul0, Assembler::S, 3, result);\n@@ -5398,1 +5552,10 @@\n-    __ bind(LOOP);\n+    const int large_loop_size = load_arrangement == Assembler::T4S   ? 44  \/\/ 11 insts\n+                                : load_arrangement == Assembler::T4H ? 60  \/\/ 15 insts\n+                                : load_arrangement == Assembler::T8H ? 108 \/\/ 27 insts\n+                                : load_arrangement == Assembler::T8B ? 124 \/\/ 31 insts\n+                                                                     : -1; \/\/ invalid\n+    guarantee(large_loop_size != -1, \"invalid small_loop_size\");\n+\n+    if (large_loop_size % 32 > 32 - __ offset() % 32) {\n+      __ align(32);\n+    }\n@@ -5400,4 +5563,7 @@\n-    __ mulv(vmul3, Assembler::T4S, vmul3, vpowm);\n-    __ mulv(vmul2, Assembler::T4S, vmul2, vpowm);\n-    __ mulv(vmul1, Assembler::T4S, vmul1, vpowm);\n-    __ mulv(vmul0, Assembler::T4S, vmul0, vpowm);\n+    start = __ offset();\n+    __ bind(LARGE_LOOP);\n+\n+    __ mulv(vmul3, Assembler::T4S, vmul3, vpowm, 0);\n+    __ mulv(vmul2, Assembler::T4S, vmul2, vpowm, 0);\n+    __ mulv(vmul1, Assembler::T4S, vmul1, vpowm, 0);\n+    __ mulv(vmul0, Assembler::T4S, vmul0, vpowm, 0);\n@@ -5406,1 +5572,1 @@\n-           Address(__ post(ary, bytes_per_iteration)));\n+           Address(__ post(ary, evf * type2aelembytes(eltype))));\n@@ -5408,1 +5574,1 @@\n-    if (eltype == T_BOOLEAN || eltype == T_BYTE) {\n+    if (load_arrangement == Assembler::T8B) {\n@@ -5430,1 +5596,2 @@\n-    } else if (load_arrangement == Assembler::T4H || load_arrangement == Assembler::T8B) {\n+    } else if (load_arrangement == Assembler::T8B || load_arrangement == Assembler::T4H ||\n+               load_arrangement == Assembler::T8H) {\n@@ -5452,5 +5619,5 @@\n-    if (load_arrangement == Assembler::T8B) {\n-      __ mulv(vmul3, Assembler::T4S, vmul3, vpow);\n-      __ mulv(vmul2, Assembler::T4S, vmul2, vpow);\n-      __ mulv(vmul1, Assembler::T4S, vmul1, vpow);\n-      __ mulv(vmul0, Assembler::T4S, vmul0, vpow);\n+    if (load_arrangement == Assembler::T8B || load_arrangement == Assembler::T8H) {\n+      __ mulv(vmul3, Assembler::T4S, vmul3, vpowm, 1);\n+      __ mulv(vmul2, Assembler::T4S, vmul2, vpowm, 1);\n+      __ mulv(vmul1, Assembler::T4S, vmul1, vpowm, 1);\n+      __ mulv(vmul0, Assembler::T4S, vmul0, vpowm, 1);\n@@ -5474,2 +5641,3 @@\n-    __ subsw(cnt, cnt, loop_factor);\n-    __ br(Assembler::HS, LOOP);\n+    __ subsw(rscratch2, rscratch2, 1);\n+    __ br(Assembler::HI, LARGE_LOOP);\n+    guarantee(__ offset() - start == large_loop_size, \"Incorrect large_loop_size\");\n@@ -5477,9 +5645,3 @@\n-    \/\/ Put 0-3'th powers of 31 into a single SIMD register together.\n-    __ movw(rscratch1, intpow(31U, 3));\n-    __ movw(rscratch2, intpow(31U, 2));\n-    __ mov(vpow, Assembler::S, 0, rscratch1);\n-    __ mov(vpow, Assembler::S, 1, rscratch2);\n-    __ movw(rscratch1, intpow(31U, 1));\n-    __ movw(rscratch2, intpow(31U, 0));\n-    __ mov(vpow, Assembler::S, 2, rscratch1);\n-    __ mov(vpow, Assembler::S, 3, rscratch2);\n+    __ mulv(vmul3, Assembler::T4S, vmul3, vpow);\n+    __ addv(vmul3, Assembler::T4S, vmul3);\n+    __ umov(result, vmul3, Assembler::S, 0);\n@@ -5487,3 +5649,1 @@\n-    __ mulv(vmul0, Assembler::T4S, vmul0, vpow);\n-    __ addv(vmul0, Assembler::T4S, vmul0);\n-    __ umov(result, vmul0, Assembler::S, 0);\n+    __ mov(rscratch2, intpow(31U, vf));\n@@ -5491,9 +5651,4 @@\n-    if (eltype == T_INT || eltype == T_SHORT || eltype == T_CHAR) {\n-      \/\/ 31^4\n-      __ movw(rscratch1, intpow(31U, 4));\n-    } else {\n-      \/\/ 31^8 - the algorithm loads 32 elements to 4 registers per\n-      \/\/ iteration, so 8 = 32 \/ 4\n-      __ movw(rscratch1, intpow(31U, 8));\n-    }\n-    __ dup(vpowm, Assembler::T4S, rscratch1);\n+    __ mulv(vmul2, Assembler::T4S, vmul2, vpow);\n+    __ addv(vmul2, Assembler::T4S, vmul2);\n+    __ umov(rscratch1, vmul2, Assembler::S, 0);\n+    __ maddw(result, result, rscratch2, rscratch1);\n@@ -5501,2 +5656,0 @@\n-    \/\/ <31^7, ... ,31^4> = <31^3, ... ,31^0> * (31^4 or 31^8)\n-    __ mulv(vpow, Assembler::T4S, vpow, vpowm);\n@@ -5506,1 +5659,1 @@\n-    __ addw(result, result, rscratch1);\n+    __ maddw(result, result, rscratch2, rscratch1);\n@@ -5508,5 +5661,4 @@\n-    __ mulv(vpow, Assembler::T4S, vpow, vpowm);\n-    __ mulv(vmul2, Assembler::T4S, vmul2, vpow);\n-    __ addv(vmul2, Assembler::T4S, vmul2);\n-    __ umov(rscratch1, vmul2, Assembler::S, 0);\n-    __ addw(result, result, rscratch1);\n+    __ mulv(vmul0, Assembler::T4S, vmul0, vpow);\n+    __ addv(vmul0, Assembler::T4S, vmul0);\n+    __ umov(rscratch1, vmul0, Assembler::S, 0);\n+    __ maddw(result, result, rscratch2, rscratch1);\n@@ -5514,5 +5666,2 @@\n-    __ mulv(vpow, Assembler::T4S, vpow, vpowm);\n-    __ mulv(vmul3, Assembler::T4S, vmul3, vpow);\n-    __ addv(vmul3, Assembler::T4S, vmul3);\n-    __ umov(rscratch1, vmul3, Assembler::S, 0);\n-    __ addw(result, result, rscratch1);\n+    __ andr(rscratch2, cnt, vf - 1);\n+    __ cbnz(rscratch2, TAIL_SHORTCUT);\n@@ -5522,0 +5671,1 @@\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/stubGenerator_aarch64.cpp","additions":226,"deletions":76,"binary":false,"changes":302,"status":"modified"},{"patch":"@@ -2,1 +2,0 @@\n- * Copyright (c) 2024, Oracle and\/or its affiliates. All rights reserved.\n","filename":"src\/hotspot\/share\/utilities\/intpow.hpp","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"}]}