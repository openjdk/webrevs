{"files":[{"patch":"@@ -4934,0 +4934,54 @@\n+operand vRegD_V12()\n+%{\n+  constraint(ALLOC_IN_RC(v12_reg));\n+  match(RegD);\n+  op_cost(0);\n+  format %{ %}\n+  interface(REG_INTER);\n+%}\n+\n+operand vRegD_V13()\n+%{\n+  constraint(ALLOC_IN_RC(v13_reg));\n+  match(RegD);\n+  op_cost(0);\n+  format %{ %}\n+  interface(REG_INTER);\n+%}\n+\n+operand vRegD_V14()\n+%{\n+  constraint(ALLOC_IN_RC(v14_reg));\n+  match(RegD);\n+  op_cost(0);\n+  format %{ %}\n+  interface(REG_INTER);\n+%}\n+\n+operand vRegD_V15()\n+%{\n+  constraint(ALLOC_IN_RC(v15_reg));\n+  match(RegD);\n+  op_cost(0);\n+  format %{ %}\n+  interface(REG_INTER);\n+%}\n+\n+operand vRegD_V16()\n+%{\n+  constraint(ALLOC_IN_RC(v16_reg));\n+  match(RegD);\n+  op_cost(0);\n+  format %{ %}\n+  interface(REG_INTER);\n+%}\n+\n+operand vRegD_V17()\n+%{\n+  constraint(ALLOC_IN_RC(v17_reg));\n+  match(RegD);\n+  op_cost(0);\n+  format %{ %}\n+  interface(REG_INTER);\n+%}\n+\n@@ -16554,0 +16608,24 @@\n+instruct arrays_hashcode(iRegP_R1 ary, iRegI_R2 cnt, iRegI_R0 result, immI basic_type,\n+                         vRegD_V0 vtmp0, vRegD_V1 vtmp1, vRegD_V2 vtmp2, vRegD_V3 vtmp3,\n+                         vRegD_V4 vtmp4, vRegD_V5 vtmp5, vRegD_V6 vtmp6, vRegD_V7 vtmp7,\n+                         vRegD_V12 vtmp8, vRegD_V13 vtmp9, vRegD_V14 vtmp10,\n+                         vRegD_V15 vtmp11, vRegD_V16 vtmp12, vRegD_V17 vtmp13,\n+                         rFlagsReg cr)\n+%{\n+  match(Set result (VectorizedHashCode (Binary ary cnt) (Binary result basic_type)));\n+  effect(TEMP vtmp0, TEMP vtmp1, TEMP vtmp2, TEMP vtmp3, TEMP vtmp4, TEMP vtmp5, TEMP vtmp6,\n+         TEMP vtmp7, TEMP vtmp8, TEMP vtmp9, TEMP vtmp10, TEMP vtmp11, TEMP vtmp12, TEMP vtmp13,\n+         USE_KILL ary, USE_KILL cnt, USE basic_type, KILL cr);\n+\n+  format %{ \"Array HashCode array[] $ary,$cnt,$result,$basic_type -> $result   \/\/ KILL all\" %}\n+  ins_encode %{\n+    address tpc = __ arrays_hashcode($ary$$Register, $cnt$$Register, $result$$Register,\n+                                     (BasicType)$basic_type$$constant);\n+    if (tpc == nullptr) {\n+      ciEnv::current()->record_failure(\"CodeCache is full\");\n+      return;\n+    }\n+  %}\n+  ins_pipe(pipe_class_memory);\n+%}\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/aarch64.ad","additions":78,"deletions":0,"binary":false,"changes":78,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -2590,0 +2590,1 @@\n+  INSN(smlalv, 0, 0b100000, false); \/\/ accepted arrangements: T8B, T16B, T4H, T8H, T2S, T4S\n@@ -2863,0 +2864,23 @@\n+#undef INSN\n+\n+#define INSN(NAME, op1, op2)                                                                       \\\n+  void NAME(FloatRegister Vd, SIMD_Arrangement T, FloatRegister Vn, FloatRegister Vm, int index) { \\\n+    starti;                                                                                        \\\n+    assert(T == T4H || T == T8H || T == T2S || T == T4S, \"invalid arrangement\");                   \\\n+    assert(index >= 0 &&                                                                           \\\n+               ((T == T2S && index <= 1) || (T != T2S && index <= 3) || (T == T8H && index <= 7)), \\\n+           \"invalid index\");                                                                       \\\n+    assert((T != T4H && T != T8H) || Vm->encoding() < 16, \"invalid source SIMD&FP register\");      \\\n+    f(0, 31), f((int)T & 1, 30), f(op1, 29);                                                       \\\n+    f(0b01111, 28, 24);                                                                            \\\n+    if (T == T4H || T == T8H) {                                                                    \\\n+      f(0b01, 23, 22), f(index & 0b11, 21, 20), rf(Vm, 16), f(op2, 15, 12), f(index >> 2 & 1, 11); \\\n+    } else {                                                                                       \\\n+      f(0b10, 23, 22), f(index & 1, 21), rf(Vm, 16), f(op2, 15, 12), f(index >> 1, 11);            \\\n+    }                                                                                              \\\n+    f(0, 10), rf(Vn, 5), rf(Vd, 0);                                                                \\\n+  }\n+\n+  \/\/ MUL - Vector - Scalar\n+  INSN(mulv, 0, 0b1000);\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/assembler_aarch64.hpp","additions":25,"deletions":1,"binary":false,"changes":26,"status":"modified"},{"patch":"@@ -36,0 +36,1 @@\n+#include \"utilities\/powerOfTwo.hpp\"\n@@ -49,0 +50,113 @@\n+\/\/ jdk.internal.util.ArraysSupport.vectorizedHashCode\n+address C2_MacroAssembler::arrays_hashcode(Register ary, Register cnt, Register result,\n+                                           BasicType eltype) {\n+  assert_different_registers(ary, cnt, result, rscratch1, rscratch2);\n+\n+  Register tmp1 = rscratch1, tmp2 = rscratch2;\n+\n+  Label TAIL, STUB_SWITCH, STUB_SWITCH_OUT, LOOP, RELATIVE, LARGE, DONE;\n+\n+  \/\/ Vectorization factor. Number of array elements loaded to one SIMD&FP registers by the stubs. We\n+  \/\/ use 8H load arrangements for chars and shorts and 8B for booleans and bytes. It's possible to\n+  \/\/ use 4H for chars and shorts instead, but using 8H gives better performance.\n+  const size_t vf = eltype == T_BOOLEAN || eltype == T_BYTE ? 8\n+                    : eltype == T_CHAR || eltype == T_SHORT ? 8\n+                    : eltype == T_INT                       ? 4\n+                                                            : 0;\n+  guarantee(vf, \"unsupported eltype\");\n+\n+  \/\/ Unroll factor for the scalar loop below. The value is chosen based on performance analysis.\n+  const size_t unroll_factor = 4;\n+\n+  switch (eltype) {\n+  case T_BOOLEAN:\n+    BLOCK_COMMENT(\"arrays_hashcode(unsigned byte) {\");\n+    break;\n+  case T_CHAR:\n+    BLOCK_COMMENT(\"arrays_hashcode(char) {\");\n+    break;\n+  case T_BYTE:\n+    BLOCK_COMMENT(\"arrays_hashcode(byte) {\");\n+    break;\n+  case T_SHORT:\n+    BLOCK_COMMENT(\"arrays_hashcode(short) {\");\n+    break;\n+  case T_INT:\n+    BLOCK_COMMENT(\"arrays_hashcode(int) {\");\n+    break;\n+  default:\n+    ShouldNotReachHere();\n+  }\n+\n+  \/\/ large_arrays_hashcode(T_INT) performs worse than the scalar loop below when the Neon loop\n+  \/\/ implemented by the stub executes just once. Call the stub only if at least two iteration will\n+  \/\/ be executed.\n+  const size_t large_threshold = eltype == T_INT ? vf * 2 : vf;\n+  cmpw(cnt, large_threshold);\n+  br(Assembler::HS, LARGE);\n+\n+  bind(TAIL);\n+\n+  \/\/ The orr performs (r - lc) % uf where uf = unroll_factor. The subtract shifted by 3\n+  \/\/ offsets past |(r - lc) % uf| pairs of load + madd insns i.e. it only executes\n+  \/\/ r % uf load + madds. Iteration eats up the remainder, uf elements at a time.\n+  assert(is_power_of_2(unroll_factor), \"can't use this value to calculate the jump target PC\");\n+  andr(tmp2, cnt, unroll_factor - 1);\n+  adr(tmp1, RELATIVE);\n+  sub(tmp1, tmp1, tmp2, ext::sxtw, 3);\n+  movw(tmp2, 0x1f);\n+  br(tmp1);\n+\n+  bind(LOOP);\n+  for (size_t i = 0; i < unroll_factor; ++i) {\n+    arrays_hashcode_elload(tmp1, Address(post(ary, type2aelembytes(eltype))), eltype);\n+    maddw(result, result, tmp2, tmp1);\n+  }\n+  bind(RELATIVE);\n+  subsw(cnt, cnt, unroll_factor);\n+  br(Assembler::HS, LOOP);\n+\n+  b(DONE);\n+\n+  bind(LARGE);\n+\n+  RuntimeAddress stub = RuntimeAddress(StubRoutines::aarch64::large_arrays_hashcode(eltype));\n+  assert(stub.target() != nullptr, \"array_hashcode stub has not been generated\");\n+  address tpc = trampoline_call(stub);\n+  if (tpc == nullptr) {\n+    DEBUG_ONLY(reset_labels(TAIL, RELATIVE));\n+    postcond(pc() == badAddress);\n+    return nullptr;\n+  }\n+\n+  bind(DONE);\n+\n+  BLOCK_COMMENT(\"} \/\/ arrays_hashcode\");\n+\n+  postcond(pc() != badAddress);\n+  return pc();\n+}\n+\n+void C2_MacroAssembler::arrays_hashcode_elload(Register dst, Address src, BasicType eltype) {\n+  switch (eltype) {\n+  \/\/ T_BOOLEAN used as surrogate for unsigned byte\n+  case T_BOOLEAN:\n+    ldrb(dst, src);\n+    break;\n+  case T_BYTE:\n+    ldrsb(dst, src);\n+    break;\n+  case T_SHORT:\n+    ldrsh(dst, src);\n+    break;\n+  case T_CHAR:\n+    ldrh(dst, src);\n+    break;\n+  case T_INT:\n+    ldrw(dst, src);\n+    break;\n+  default:\n+    ShouldNotReachHere();\n+  }\n+}\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/c2_MacroAssembler_aarch64.cpp","additions":114,"deletions":0,"binary":false,"changes":114,"status":"modified"},{"patch":"@@ -37,0 +37,4 @@\n+  \/\/ Helper functions for arrays_hashcode.\n+  void arrays_hashcode_elload(Register dst, Address src, BasicType eltype);\n+  int arrays_hashcode_elsize(BasicType eltype);\n+\n@@ -38,0 +42,3 @@\n+  \/\/ jdk.internal.util.ArraysSupport.vectorizedHashCode\n+  address arrays_hashcode(Register ary, Register cnt, Register result, BasicType eltype);\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/c2_MacroAssembler_aarch64.hpp","additions":7,"deletions":0,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -56,0 +56,1 @@\n+#include \"utilities\/debug.hpp\"\n@@ -57,0 +58,1 @@\n+#include \"utilities\/intpow.hpp\"\n@@ -5314,0 +5316,359 @@\n+  void large_arrays_hashcode_elload(Register dst, Address src, BasicType eltype) {\n+    switch (eltype) {\n+    \/\/ T_BOOLEAN used as surrogate for unsigned byte\n+    case T_BOOLEAN:\n+      __ ldrb(dst, src);\n+      break;\n+    case T_BYTE:\n+      __ ldrsb(dst, src);\n+      break;\n+    case T_SHORT:\n+      __ ldrsh(dst, src);\n+      break;\n+    case T_CHAR:\n+      __ ldrh(dst, src);\n+      break;\n+    case T_INT:\n+      __ ldrw(dst, src);\n+      break;\n+    default:\n+      ShouldNotReachHere();\n+    }\n+  }\n+\n+  \/\/ result = r0 - return value. Contains initial hashcode value on entry.\n+  \/\/ ary = r1 - array address\n+  \/\/ cnt = r2 - elements count\n+  \/\/ Clobbers: v0-v13, rscratch1, rscratch2\n+  address generate_large_arrays_hashcode(BasicType eltype) {\n+    const Register result = r0, ary = r1, cnt = r2;\n+    const FloatRegister vdata0 = v3, vdata1 = v2, vdata2 = v1, vdata3 = v0;\n+    const FloatRegister vhalf0 = v13, vhalf1 = v12, vhalf2 = v11, vhalf3 = v10;\n+    const FloatRegister vmul0 = v4, vmul1 = v5, vmul2 = v6, vmul3 = v7;\n+    const FloatRegister vpow = v8;  \/\/ powers of 31: <31^3, ..., 31^0>\n+    const FloatRegister vpowm = v9;\n+\n+    assert_different_registers(ary, cnt, result);\n+    assert_different_registers(vdata0, vdata1, vdata2, vdata3, vhalf0, vhalf1, vhalf2, vhalf3,\n+                               vmul0, vmul1, vmul2, vmul3, vpow, vpowm);\n+\n+    Label SMALL_LOOP, LARGE_LOOP_PREHEADER, LARGE_LOOP, TAIL, TAIL_SHORTCUT, RELATIVE;\n+\n+    \/\/ Vectorization factor\n+    const size_t vf = eltype == T_BOOLEAN || eltype == T_BYTE ? 8\n+                      : eltype == T_CHAR || eltype == T_SHORT ? 8\n+                      : eltype == T_INT                       ? 4\n+                                                              : 0;\n+    guarantee(vf, \"unsupported eltype\");\n+\n+    \/\/ Unroll factor\n+    const size_t uf = 4;\n+\n+    \/\/ Effective vectorization factor\n+    const size_t evf = vf * uf;\n+\n+    __ align(CodeEntryAlignment);\n+\n+    const char *mark_name = \"\";\n+    switch (eltype) {\n+    case T_BOOLEAN:\n+      mark_name = \"_large_arrays_hashcode_boolean\";\n+      break;\n+    case T_BYTE:\n+      mark_name = \"_large_arrays_hashcode_byte\";\n+      break;\n+    case T_CHAR:\n+      mark_name = \"_large_arrays_hashcode_char\";\n+      break;\n+    case T_SHORT:\n+      mark_name = \"_large_arrays_hashcode_short\";\n+      break;\n+    case T_INT:\n+      mark_name = \"_large_arrays_hashcode_int\";\n+      break;\n+    default:\n+      mark_name = \"_large_arrays_hashcode_incorrect_type\";\n+      __ should_not_reach_here();\n+    };\n+\n+    StubCodeMark mark(this, \"StubRoutines\", mark_name);\n+\n+    address entry = __ pc();\n+    __ enter();\n+\n+    Assembler::SIMD_Arrangement load_arrangement =\n+        eltype == T_BOOLEAN || eltype == T_BYTE ? Assembler::T8B\n+        : eltype == T_CHAR || eltype == T_SHORT ? Assembler::T8H\n+        : eltype == T_INT                       ? Assembler::T4S\n+                                                : Assembler::INVALID_ARRANGEMENT;\n+    guarantee(load_arrangement != Assembler::INVALID_ARRANGEMENT, \"invalid arrangement\");\n+\n+    const int multiply_by_halves =\n+        load_arrangement == Assembler::T4S || load_arrangement == Assembler::T4H   ? false\n+        : load_arrangement == Assembler::T8B || load_arrangement == Assembler::T8H ? true\n+                                                                                   : -1;\n+    guarantee(multiply_by_halves != -1, \"unknown multiplication algorithm\");\n+\n+    const int small_loop_size = load_arrangement == Assembler::T4S   ? 20  \/\/ 5 insts\n+                                : load_arrangement == Assembler::T4H ? 24  \/\/ 6 insts\n+                                : load_arrangement == Assembler::T8H ? 36  \/\/ 9 insts\n+                                : load_arrangement == Assembler::T8B ? 40  \/\/ 10 insts\n+                                                                     : -1; \/\/ invalid\n+    guarantee(small_loop_size != -1, \"invalid small_loop_size\");\n+\n+    \/\/ Put 0-3'th powers of 31 into a single SIMD register together. The register will be used in\n+    \/\/ the SMALL and LARGE LOOPS' epilogues. The initialization is hoisted here and the register's\n+    \/\/ value shouldn't change throughout both loops.\n+    __ movw(rscratch1, intpow(31U, 3));\n+    __ mov(vpow, Assembler::S, 0, rscratch1);\n+    __ movw(rscratch1, intpow(31U, 2));\n+    __ mov(vpow, Assembler::S, 1, rscratch1);\n+    __ movw(rscratch1, intpow(31U, 1));\n+    __ mov(vpow, Assembler::S, 2, rscratch1);\n+    __ movw(rscratch1, intpow(31U, 0));\n+    __ mov(vpow, Assembler::S, 3, rscratch1);\n+\n+    __ mov(vmul0, Assembler::T16B, 0);\n+    __ mov(vmul0, Assembler::S, 3, result);\n+\n+    __ andr(rscratch2, cnt, (uf - 1) * vf);\n+    __ cbz(rscratch2, LARGE_LOOP_PREHEADER);\n+\n+    __ movw(rscratch1, intpow(31U, multiply_by_halves ? vf \/ 2 : vf));\n+    __ mov(vpowm, Assembler::S, 0, rscratch1);\n+\n+    if (small_loop_size % 32 > 32 - __ offset() % 32) {\n+      __ align(32);\n+    }\n+\n+    auto start = __ offset();\n+    __ bind(SMALL_LOOP);\n+\n+    __ ld1(vdata0, load_arrangement, Address(__ post(ary, vf * type2aelembytes(eltype))));\n+    __ mulv(vmul0, Assembler::T4S, vmul0, vpowm, 0);\n+    __ subsw(rscratch2, rscratch2, vf);\n+\n+    if (load_arrangement == Assembler::T8B) {\n+      \/\/ Extend 8B to 8H to be able to use vector multiply\n+      \/\/ instructions\n+      assert(load_arrangement == Assembler::T8B, \"expected to extend 8B to 8H\");\n+      if (is_signed_subword_type(eltype)) {\n+        __ sxtl(vdata0, Assembler::T8H, vdata0, load_arrangement);\n+      } else {\n+        __ uxtl(vdata0, Assembler::T8H, vdata0, load_arrangement);\n+      }\n+    }\n+\n+    if (load_arrangement == Assembler::T4S) {\n+      __ addv(vmul0, load_arrangement, vmul0, vdata0);\n+    } else if (load_arrangement == Assembler::T8B || load_arrangement == Assembler::T4H ||\n+               load_arrangement == Assembler::T8H) {\n+      assert(is_subword_type(eltype), \"subword type expected\");\n+      if (is_signed_subword_type(eltype)) {\n+        __ sxtl(vhalf0, Assembler::T4S, vdata0, Assembler::T4H);\n+      } else {\n+        __ uxtl(vhalf0, Assembler::T4S, vdata0, Assembler::T4H);\n+      }\n+      __ addv(vmul0, Assembler::T4S, vmul0, vhalf0);\n+    } else {\n+      __ should_not_reach_here();\n+    }\n+\n+    \/\/ Process the upper half of a vector\n+    if (load_arrangement == Assembler::T8B || load_arrangement == Assembler::T8H) {\n+      __ mulv(vmul0, Assembler::T4S, vmul0, vpowm, 0);\n+      if (is_signed_subword_type(eltype)) {\n+        __ sshll2(vhalf0, Assembler::T4S, vdata0, Assembler::T8H, 0);\n+      } else {\n+        __ ushll2(vhalf0, Assembler::T4S, vdata0, Assembler::T8H, 0);\n+      }\n+      __ addv(vmul0, Assembler::T4S, vmul0, vhalf0);\n+    }\n+\n+    __ br(Assembler::HI, SMALL_LOOP);\n+    guarantee(__ offset() - start == small_loop_size, \"Incorrect small_loop_size\");\n+\n+    \/\/ SMALL LOOP'S EPILOQUE\n+\n+    __ lsr(rscratch2, cnt, exact_log2(evf));\n+    __ cbnz(rscratch2, LARGE_LOOP_PREHEADER);\n+\n+    __ mulv(vmul0, Assembler::T4S, vmul0, vpow);\n+    __ addv(vmul0, Assembler::T4S, vmul0);\n+    __ umov(result, vmul0, Assembler::S, 0);\n+\n+    \/\/ TAIL\n+\n+    const int tail_size = (8 + (vf - 1) * 2) * 4; \/\/ 14 or 22 insts\n+    if (tail_size % 32 > 32 - __ offset() % 32) {\n+      __ align(32);\n+    }\n+\n+    __ bind(TAIL);\n+    start = __ offset();\n+    assert(is_power_of_2(vf), \"can't use this value to calculate the jump target PC\");\n+    __ andr(rscratch2, cnt, vf - 1);\n+    __ bind(TAIL_SHORTCUT);\n+    __ adr(rscratch1, RELATIVE);\n+    __ sub(rscratch1, rscratch1, rscratch2, ext::uxtw, 3);\n+    __ movw(rscratch2, 0x1f);\n+    __ br(rscratch1);\n+\n+    for (size_t i = 0; i < vf - 1; ++i) {\n+      large_arrays_hashcode_elload(rscratch1, Address(__ post(ary, type2aelembytes(eltype))),\n+                                   eltype);\n+      __ maddw(result, result, rscratch2, rscratch1);\n+    }\n+    __ bind(RELATIVE);\n+\n+    __ leave();\n+    __ ret(lr);\n+    guarantee(__ offset() - start == tail_size, \"unexptected size of the tail code block\");\n+\n+    \/\/ LARGE LOOP\n+\n+    __ align(32);\n+    __ bind(LARGE_LOOP_PREHEADER);\n+\n+    __ lsr(rscratch2, cnt, exact_log2(evf));\n+\n+    if (multiply_by_halves) {\n+      \/\/ 31^4 - multiplier between lower and upper parts of a register\n+      __ movw(rscratch1, intpow(31U, vf \/ 2));\n+      __ mov(vpowm, Assembler::S, 1, rscratch1);\n+      \/\/ 31^28 - remainder of the iteraion multiplier, 28 = 32 - 4\n+      __ movw(rscratch1, intpow(31U, evf - vf \/ 2));\n+      __ mov(vpowm, Assembler::S, 0, rscratch1);\n+    } else {\n+      \/\/ 31^16\n+      __ movw(rscratch1, intpow(31U, evf));\n+      __ mov(vpowm, Assembler::S, 0, rscratch1);\n+    }\n+\n+    __ mov(vmul3, Assembler::T16B, 0);\n+    __ mov(vmul2, Assembler::T16B, 0);\n+    __ mov(vmul1, Assembler::T16B, 0);\n+\n+    const int large_loop_size = load_arrangement == Assembler::T4S   ? 44  \/\/ 11 insts\n+                                : load_arrangement == Assembler::T4H ? 60  \/\/ 15 insts\n+                                : load_arrangement == Assembler::T8H ? 108 \/\/ 27 insts\n+                                : load_arrangement == Assembler::T8B ? 124 \/\/ 31 insts\n+                                                                     : -1; \/\/ invalid\n+    guarantee(large_loop_size != -1, \"invalid small_loop_size\");\n+\n+    if (large_loop_size % 32 > 32 - __ offset() % 32) {\n+      __ align(32);\n+    }\n+\n+    start = __ offset();\n+    __ bind(LARGE_LOOP);\n+\n+    __ mulv(vmul3, Assembler::T4S, vmul3, vpowm, 0);\n+    __ mulv(vmul2, Assembler::T4S, vmul2, vpowm, 0);\n+    __ mulv(vmul1, Assembler::T4S, vmul1, vpowm, 0);\n+    __ mulv(vmul0, Assembler::T4S, vmul0, vpowm, 0);\n+\n+    __ ld1(vdata3, vdata2, vdata1, vdata0, load_arrangement,\n+           Address(__ post(ary, evf * type2aelembytes(eltype))));\n+\n+    if (load_arrangement == Assembler::T8B) {\n+      \/\/ Extend 8B to 8H to be able to use vector multiply\n+      \/\/ instructions\n+      assert(load_arrangement == Assembler::T8B, \"expected to extend 8B to 8H\");\n+      if (is_signed_subword_type(eltype)) {\n+        __ sxtl(vdata3, Assembler::T8H, vdata3, load_arrangement);\n+        __ sxtl(vdata2, Assembler::T8H, vdata2, load_arrangement);\n+        __ sxtl(vdata1, Assembler::T8H, vdata1, load_arrangement);\n+        __ sxtl(vdata0, Assembler::T8H, vdata0, load_arrangement);\n+      } else {\n+        __ uxtl(vdata3, Assembler::T8H, vdata3, load_arrangement);\n+        __ uxtl(vdata2, Assembler::T8H, vdata2, load_arrangement);\n+        __ uxtl(vdata1, Assembler::T8H, vdata1, load_arrangement);\n+        __ uxtl(vdata0, Assembler::T8H, vdata0, load_arrangement);\n+      }\n+    }\n+\n+    if (load_arrangement == Assembler::T4S) {\n+      __ addv(vmul3, load_arrangement, vmul3, vdata3);\n+      __ addv(vmul2, load_arrangement, vmul2, vdata2);\n+      __ addv(vmul1, load_arrangement, vmul1, vdata1);\n+      __ addv(vmul0, load_arrangement, vmul0, vdata0);\n+    } else if (load_arrangement == Assembler::T8B || load_arrangement == Assembler::T4H ||\n+               load_arrangement == Assembler::T8H) {\n+      assert(is_subword_type(eltype), \"subword type expected\");\n+      if (is_signed_subword_type(eltype)) {\n+        __ sxtl(vhalf3, Assembler::T4S, vdata3, Assembler::T4H);\n+        __ sxtl(vhalf2, Assembler::T4S, vdata2, Assembler::T4H);\n+        __ sxtl(vhalf1, Assembler::T4S, vdata1, Assembler::T4H);\n+        __ sxtl(vhalf0, Assembler::T4S, vdata0, Assembler::T4H);\n+      } else {\n+        __ uxtl(vhalf3, Assembler::T4S, vdata3, Assembler::T4H);\n+        __ uxtl(vhalf2, Assembler::T4S, vdata2, Assembler::T4H);\n+        __ uxtl(vhalf1, Assembler::T4S, vdata1, Assembler::T4H);\n+        __ uxtl(vhalf0, Assembler::T4S, vdata0, Assembler::T4H);\n+      }\n+      __ addv(vmul3, Assembler::T4S, vmul3, vhalf3);\n+      __ addv(vmul2, Assembler::T4S, vmul2, vhalf2);\n+      __ addv(vmul1, Assembler::T4S, vmul1, vhalf1);\n+      __ addv(vmul0, Assembler::T4S, vmul0, vhalf0);\n+    } else {\n+      __ should_not_reach_here();\n+    }\n+\n+    \/\/ Process the upper half of a vector\n+    if (load_arrangement == Assembler::T8B || load_arrangement == Assembler::T8H) {\n+      __ mulv(vmul3, Assembler::T4S, vmul3, vpowm, 1);\n+      __ mulv(vmul2, Assembler::T4S, vmul2, vpowm, 1);\n+      __ mulv(vmul1, Assembler::T4S, vmul1, vpowm, 1);\n+      __ mulv(vmul0, Assembler::T4S, vmul0, vpowm, 1);\n+      if (is_signed_subword_type(eltype)) {\n+        __ sshll2(vhalf3, Assembler::T4S, vdata3, Assembler::T8H, 0);\n+        __ sshll2(vhalf2, Assembler::T4S, vdata2, Assembler::T8H, 0);\n+        __ sshll2(vhalf1, Assembler::T4S, vdata1, Assembler::T8H, 0);\n+        __ sshll2(vhalf0, Assembler::T4S, vdata0, Assembler::T8H, 0);\n+      } else {\n+        __ ushll2(vhalf3, Assembler::T4S, vdata3, Assembler::T8H, 0);\n+        __ ushll2(vhalf2, Assembler::T4S, vdata2, Assembler::T8H, 0);\n+        __ ushll2(vhalf1, Assembler::T4S, vdata1, Assembler::T8H, 0);\n+        __ ushll2(vhalf0, Assembler::T4S, vdata0, Assembler::T8H, 0);\n+      }\n+      __ addv(vmul3, Assembler::T4S, vmul3, vhalf3);\n+      __ addv(vmul2, Assembler::T4S, vmul2, vhalf2);\n+      __ addv(vmul1, Assembler::T4S, vmul1, vhalf1);\n+      __ addv(vmul0, Assembler::T4S, vmul0, vhalf0);\n+    }\n+\n+    __ subsw(rscratch2, rscratch2, 1);\n+    __ br(Assembler::HI, LARGE_LOOP);\n+    guarantee(__ offset() - start == large_loop_size, \"Incorrect large_loop_size\");\n+\n+    __ mulv(vmul3, Assembler::T4S, vmul3, vpow);\n+    __ addv(vmul3, Assembler::T4S, vmul3);\n+    __ umov(result, vmul3, Assembler::S, 0);\n+\n+    __ mov(rscratch2, intpow(31U, vf));\n+\n+    __ mulv(vmul2, Assembler::T4S, vmul2, vpow);\n+    __ addv(vmul2, Assembler::T4S, vmul2);\n+    __ umov(rscratch1, vmul2, Assembler::S, 0);\n+    __ maddw(result, result, rscratch2, rscratch1);\n+\n+    __ mulv(vmul1, Assembler::T4S, vmul1, vpow);\n+    __ addv(vmul1, Assembler::T4S, vmul1);\n+    __ umov(rscratch1, vmul1, Assembler::S, 0);\n+    __ maddw(result, result, rscratch2, rscratch1);\n+\n+    __ mulv(vmul0, Assembler::T4S, vmul0, vpow);\n+    __ addv(vmul0, Assembler::T4S, vmul0);\n+    __ umov(rscratch1, vmul0, Assembler::S, 0);\n+    __ maddw(result, result, rscratch2, rscratch1);\n+\n+    __ andr(rscratch2, cnt, vf - 1);\n+    __ cbnz(rscratch2, TAIL_SHORTCUT);\n+\n+    __ leave();\n+    __ ret(lr);\n+\n+    return entry;\n+  }\n+\n@@ -8260,0 +8621,7 @@\n+    \/\/ arrays_hascode stub for large arrays.\n+    StubRoutines::aarch64::_large_arrays_hashcode_boolean = generate_large_arrays_hashcode(T_BOOLEAN);\n+    StubRoutines::aarch64::_large_arrays_hashcode_byte = generate_large_arrays_hashcode(T_BYTE);\n+    StubRoutines::aarch64::_large_arrays_hashcode_char = generate_large_arrays_hashcode(T_CHAR);\n+    StubRoutines::aarch64::_large_arrays_hashcode_int = generate_large_arrays_hashcode(T_INT);\n+    StubRoutines::aarch64::_large_arrays_hashcode_short = generate_large_arrays_hashcode(T_SHORT);\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/stubGenerator_aarch64.cpp","additions":368,"deletions":0,"binary":false,"changes":368,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2003, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2003, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -51,0 +51,5 @@\n+address StubRoutines::aarch64::_large_arrays_hashcode_boolean = nullptr;\n+address StubRoutines::aarch64::_large_arrays_hashcode_byte = nullptr;\n+address StubRoutines::aarch64::_large_arrays_hashcode_char = nullptr;\n+address StubRoutines::aarch64::_large_arrays_hashcode_int = nullptr;\n+address StubRoutines::aarch64::_large_arrays_hashcode_short = nullptr;\n","filename":"src\/hotspot\/cpu\/aarch64\/stubRoutines_aarch64.cpp","additions":6,"deletions":1,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2003, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2003, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -65,0 +65,5 @@\n+  static address _large_arrays_hashcode_boolean;\n+  static address _large_arrays_hashcode_byte;\n+  static address _large_arrays_hashcode_char;\n+  static address _large_arrays_hashcode_int;\n+  static address _large_arrays_hashcode_short;\n@@ -148,0 +153,19 @@\n+  static address large_arrays_hashcode(BasicType eltype) {\n+    switch (eltype) {\n+    case T_BOOLEAN:\n+      return _large_arrays_hashcode_boolean;\n+    case T_BYTE:\n+      return _large_arrays_hashcode_byte;\n+    case T_CHAR:\n+      return _large_arrays_hashcode_char;\n+    case T_SHORT:\n+      return _large_arrays_hashcode_short;\n+    case T_INT:\n+      return _large_arrays_hashcode_int;\n+    default:\n+      ShouldNotReachHere();\n+    }\n+\n+    return nullptr;\n+  }\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/stubRoutines_aarch64.hpp","additions":25,"deletions":1,"binary":false,"changes":26,"status":"modified"},{"patch":"@@ -568,0 +568,4 @@\n+\n+  if (FLAG_IS_DEFAULT(UseVectorizedHashCodeIntrinsic)) {\n+    FLAG_SET_DEFAULT(UseVectorizedHashCodeIntrinsic, true);\n+  }\n","filename":"src\/hotspot\/cpu\/aarch64\/vm_version_aarch64.cpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -0,0 +1,45 @@\n+\/*\n+ * Copyright (c) 2024, Arm Limited. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef SHARE_UTILITIES_INTPOW_HPP\n+#define SHARE_UTILITIES_INTPOW_HPP\n+\n+#include \"metaprogramming\/enableIf.hpp\"\n+#include <limits>\n+#include <type_traits>\n+\n+template <typename T, ENABLE_IF(std::is_integral<T>::value && std::is_unsigned<T>::value)>\n+static constexpr T intpow(T v, unsigned p) {\n+  if (p == 0) {\n+    return 1;\n+  }\n+\n+  \/\/ We use exponentiation by squaring to calculate the required power.\n+  T a = intpow(v, p \/ 2);\n+  T b = (p % 2) ? v : 1;\n+\n+  return a * a * b;\n+}\n+\n+#endif \/\/ SHARE_UTILITIES_INTPOW_HPP\n","filename":"src\/hotspot\/share\/utilities\/intpow.hpp","additions":45,"deletions":0,"binary":false,"changes":45,"status":"added"}]}