{"files":[{"patch":"@@ -71,3 +71,3 @@\n-void ShenandoahAdaptiveHeuristics::choose_collection_set_from_regiondata(ShenandoahCollectionSet* cset,\n-                                                                         RegionData* data, size_t size,\n-                                                                         size_t actual_free) {\n+size_t ShenandoahAdaptiveHeuristics::choose_collection_set_from_regiondata(ShenandoahCollectionSet* cset,\n+                                                                           RegionData* data, size_t size,\n+                                                                           size_t actual_free) {\n@@ -127,0 +127,1 @@\n+  return 0;\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/heuristics\/shenandoahAdaptiveHeuristics.cpp","additions":4,"deletions":3,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -111,3 +111,3 @@\n-  virtual void choose_collection_set_from_regiondata(ShenandoahCollectionSet* cset,\n-                                                     RegionData* data, size_t size,\n-                                                     size_t actual_free) override;\n+  virtual size_t choose_collection_set_from_regiondata(ShenandoahCollectionSet* cset,\n+                                                       RegionData* data, size_t size,\n+                                                       size_t actual_free) override;\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/heuristics\/shenandoahAdaptiveHeuristics.hpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -42,3 +42,3 @@\n-void ShenandoahAggressiveHeuristics::choose_collection_set_from_regiondata(ShenandoahCollectionSet* cset,\n-                                                                           RegionData* data, size_t size,\n-                                                                           size_t free) {\n+size_t ShenandoahAggressiveHeuristics::choose_collection_set_from_regiondata(ShenandoahCollectionSet* cset,\n+                                                                             RegionData* data, size_t size,\n+                                                                             size_t free) {\n@@ -51,0 +51,1 @@\n+  return 0;\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/heuristics\/shenandoahAggressiveHeuristics.cpp","additions":4,"deletions":3,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -38,3 +38,3 @@\n-  virtual void choose_collection_set_from_regiondata(ShenandoahCollectionSet* cset,\n-                                                     RegionData* data, size_t size,\n-                                                     size_t free);\n+  virtual size_t choose_collection_set_from_regiondata(ShenandoahCollectionSet* cset,\n+                                                       RegionData* data, size_t size,\n+                                                       size_t free);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/heuristics\/shenandoahAggressiveHeuristics.hpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -79,3 +79,3 @@\n-void ShenandoahCompactHeuristics::choose_collection_set_from_regiondata(ShenandoahCollectionSet* cset,\n-                                                                        RegionData* data, size_t size,\n-                                                                        size_t actual_free) {\n+size_t ShenandoahCompactHeuristics::choose_collection_set_from_regiondata(ShenandoahCollectionSet* cset,\n+                                                                          RegionData* data, size_t size,\n+                                                                          size_t actual_free) {\n@@ -100,0 +100,1 @@\n+  return 0;\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/heuristics\/shenandoahCompactHeuristics.cpp","additions":4,"deletions":3,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -40,3 +40,3 @@\n-  virtual void choose_collection_set_from_regiondata(ShenandoahCollectionSet* cset,\n-                                                     RegionData* data, size_t size,\n-                                                     size_t actual_free);\n+  virtual size_t choose_collection_set_from_regiondata(ShenandoahCollectionSet* cset,\n+                                                       RegionData* data, size_t size,\n+                                                       size_t actual_free);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/heuristics\/shenandoahCompactHeuristics.hpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -40,1 +40,1 @@\n-void ShenandoahGenerationalHeuristics::choose_collection_set(ShenandoahCollectionSet* collection_set) {\n+size_t ShenandoahGenerationalHeuristics::choose_collection_set(ShenandoahCollectionSet* collection_set) {\n@@ -171,1 +171,0 @@\n-\n@@ -173,5 +172,0 @@\n-  if (doing_promote_in_place || (preselected_candidates > 0) || (immediate_percent <= ShenandoahImmediateThreshold)) {\n-    \/\/ Only young collections need to prime the collection set.\n-    if (_generation->is_young()) {\n-      heap->old_generation()->heuristics()->prime_collection_set(collection_set);\n-    }\n@@ -179,0 +173,2 @@\n+  size_t add_regions_to_old = 0;\n+  if (doing_promote_in_place || (preselected_candidates > 0) || (immediate_percent <= ShenandoahImmediateThreshold)) {\n@@ -180,1 +176,1 @@\n-    choose_collection_set_from_regiondata(collection_set, candidates, cand_idx, immediate_garbage + free);\n+    add_regions_to_old = choose_collection_set_from_regiondata(collection_set, candidates, cand_idx, immediate_garbage + free);\n@@ -197,0 +193,1 @@\n+  return add_regions_to_old;\n@@ -213,7 +210,0 @@\n-\n-      \/\/ r->used() is r->garbage() + r->get_live_data_bytes()\n-      \/\/ Since all live data in this region is being evacuated from young-gen, it is as if this memory\n-      \/\/ is garbage insofar as young-gen is concerned.  Counting this as garbage reduces the need to\n-      \/\/ reclaim highly utilized young-gen regions just for the sake of finding min_garbage to reclaim\n-      \/\/ within young-gen memory.\n-\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/heuristics\/shenandoahGenerationalHeuristics.cpp","additions":5,"deletions":15,"binary":false,"changes":20,"status":"modified"},{"patch":"@@ -47,1 +47,1 @@\n-  void choose_collection_set(ShenandoahCollectionSet* collection_set) override;\n+  size_t choose_collection_set(ShenandoahCollectionSet* collection_set) override;\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/heuristics\/shenandoahGenerationalHeuristics.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -27,0 +27,1 @@\n+#include \"gc\/shenandoah\/shenandoahAsserts.hpp\"\n@@ -38,3 +39,3 @@\n-void ShenandoahGlobalHeuristics::choose_collection_set_from_regiondata(ShenandoahCollectionSet* cset,\n-                                                                       RegionData* data, size_t size,\n-                                                                       size_t actual_free) {\n+size_t ShenandoahGlobalHeuristics::choose_collection_set_from_regiondata(ShenandoahCollectionSet* cset,\n+                                                                         RegionData* data, size_t size,\n+                                                                         size_t actual_free) {\n@@ -45,0 +46,1 @@\n+  return 0;\n@@ -52,0 +54,1 @@\n+  shenandoah_assert_heaplocked_or_safepoint();\n@@ -53,0 +56,1 @@\n+  auto free_set = heap->free_set();\n@@ -55,0 +59,1 @@\n+\n@@ -59,0 +64,1 @@\n+  size_t original_young_evac_reserve = young_evac_reserve;\n@@ -60,12 +66,3 @@\n-  size_t max_young_cset = (size_t) (young_evac_reserve \/ ShenandoahEvacWaste);\n-  size_t young_cur_cset = 0;\n-  size_t max_old_cset = (size_t) (old_evac_reserve \/ ShenandoahOldEvacWaste);\n-  size_t old_cur_cset = 0;\n-\n-  \/\/ Figure out how many unaffiliated young regions are dedicated to mutator and to evacuator.  Allow the young\n-  \/\/ collector's unaffiliated regions to be transferred to old-gen if old-gen has more easily reclaimed garbage\n-  \/\/ than young-gen.  At the end of this cycle, any excess regions remaining in old-gen will be transferred back\n-  \/\/ to young.  Do not transfer the mutator's unaffiliated regions to old-gen.  Those must remain available\n-  \/\/ to the mutator as it needs to be able to consume this memory during concurrent GC.\n-\n-  size_t unaffiliated_young_regions = heap->young_generation()->free_unaffiliated_regions();\n+  size_t old_promo_reserve = heap->old_generation()->get_promoted_reserve();\n+\n+  size_t unaffiliated_young_regions = free_set->collector_unaffiliated_regions();\n@@ -73,0 +70,8 @@\n+  size_t unaffiliated_old_regions = free_set->old_collector_unaffiliated_regions();\n+  size_t unaffiliated_old_memory = unaffiliated_old_regions * region_size_bytes;\n+\n+  \/\/ Figure out how many unaffiliated regions are dedicated to Collector and OldCollector reserves.  Let these\n+  \/\/ be shuffled between young and old generations in order to expedite evacuation of whichever regions have the\n+  \/\/ most garbage, regardless of whether these garbage-first regions reside in young or old generation.\n+  \/\/ Excess reserves will be transferred back to the mutator after collection set has been chosen.  At the end\n+  \/\/ of evacuation, any reserves not consumed by evacuation will also be transferred to the mutator free set.\n@@ -74,5 +79,7 @@\n-  if (unaffiliated_young_memory > max_young_cset) {\n-    size_t unaffiliated_mutator_memory = unaffiliated_young_memory - max_young_cset;\n-    unaffiliated_young_memory -= unaffiliated_mutator_memory;\n-    unaffiliated_young_regions = unaffiliated_young_memory \/ region_size_bytes; \/\/ round down\n-    unaffiliated_young_memory = unaffiliated_young_regions * region_size_bytes;\n+  \/\/ Truncate reserves to only target unaffiliated memory\n+  size_t shared_reserve_regions = 0;\n+  if (young_evac_reserve > unaffiliated_young_memory) {\n+    shared_reserve_regions += unaffiliated_young_regions;\n+  } else {\n+    size_t delta_regions = young_evac_reserve \/ region_size_bytes;\n+    shared_reserve_regions += delta_regions;\n@@ -80,0 +87,17 @@\n+  young_evac_reserve = 0;\n+  size_t total_old_reserve = old_evac_reserve + old_promo_reserve;\n+  if (total_old_reserve > unaffiliated_old_memory) {\n+    \/\/ Give all the unaffiliated memory to the shared reserves.  Leave the rest for promo reserve.\n+    shared_reserve_regions += unaffiliated_old_regions;\n+    old_promo_reserve = total_old_reserve - unaffiliated_old_memory;\n+  } else {\n+    size_t delta_regions = old_evac_reserve \/ region_size_bytes;\n+    shared_reserve_regions += delta_regions;\n+  }\n+  old_evac_reserve = 0;\n+  assert(shared_reserve_regions <=\n+         (heap->young_generation()->free_unaffiliated_regions() + heap->old_generation()->free_unaffiliated_regions()),\n+         \"simple math\");\n+\n+  size_t shared_reserves = shared_reserve_regions * region_size_bytes;\n+  size_t committed_from_shared_reserves = 0;\n@@ -81,2 +105,3 @@\n-  \/\/ We'll affiliate these unaffiliated regions with either old or young, depending on need.\n-  max_young_cset -= unaffiliated_young_memory;\n+  size_t promo_bytes = 0;\n+  size_t old_evac_bytes = 0;\n+  size_t young_evac_bytes = 0;\n@@ -84,2 +109,3 @@\n-  \/\/ Keep track of how many regions we plan to transfer from young to old.\n-  size_t regions_transferred_to_old = 0;\n+  size_t consumed_by_promo = 0;        \/\/ promo_bytes * ShenandoahPromoEvacWaste\n+  size_t consumed_by_old_evac = 0;     \/\/ old_evac_bytes * ShenandoahOldEvacWaste\n+  size_t consumed_by_young_evac = 0;   \/\/ young_evac_bytes * ShenandoahEvacWaste\n@@ -87,1 +113,4 @@\n-  size_t free_target = (capacity * ShenandoahMinFreeThreshold) \/ 100 + max_young_cset;\n+  \/\/ Of the memory reclaimed by GC, some of this will need to be reserved for the next GC collection.  Use the current\n+  \/\/ young reserve as an approximation of the future Collector reserve requirement.  Try to end with at least\n+  \/\/ (capacity * ShenandoahMinFreeThreshold) \/ 100 bytes available to the mutator.\n+  size_t free_target = (capacity * ShenandoahMinFreeThreshold) \/ 100 + original_young_evac_reserve;\n@@ -90,6 +119,3 @@\n-  log_info(gc, ergo)(\"Adaptive CSet Selection for GLOBAL. Max Young Evacuation: %zu\"\n-                     \"%s, Max Old Evacuation: %zu%s, Max Either Evacuation: %zu%s, Actual Free: %zu%s.\",\n-                     byte_size_in_proper_unit(max_young_cset), proper_unit_for_byte_size(max_young_cset),\n-                     byte_size_in_proper_unit(max_old_cset), proper_unit_for_byte_size(max_old_cset),\n-                     byte_size_in_proper_unit(unaffiliated_young_memory), proper_unit_for_byte_size(unaffiliated_young_memory),\n-                     byte_size_in_proper_unit(actual_free), proper_unit_for_byte_size(actual_free));\n+  size_t aged_regions_promoted = 0;\n+  size_t young_regions_evacuated = 0;\n+  size_t old_regions_evacuated = 0;\n@@ -97,0 +123,6 @@\n+  log_info(gc, ergo)(\"Adaptive CSet Selection for GLOBAL. Discretionary evacuation budget (for either old or young): %zu%s\"\n+                     \", Actual Free: %zu%s.\",\n+                      byte_size_in_proper_unit(shared_reserves), proper_unit_for_byte_size(shared_reserves),\n+                      byte_size_in_proper_unit(actual_free), proper_unit_for_byte_size(actual_free));\n+\n+  size_t cur_garbage = cur_young_garbage;\n@@ -101,7 +133,14 @@\n-    if (r->is_old() || heap->is_tenurable(r)) {\n-      size_t new_cset = old_cur_cset + r->get_live_data_bytes();\n-      if ((r->garbage() > garbage_threshold)) {\n-        while ((new_cset > max_old_cset) && (unaffiliated_young_regions > 0)) {\n-          unaffiliated_young_regions--;\n-          regions_transferred_to_old++;\n-          max_old_cset += region_size_bytes \/ ShenandoahOldEvacWaste;\n+    size_t region_garbage = r->garbage();\n+    size_t new_garbage = cur_garbage + region_garbage;\n+    bool add_regardless = (region_garbage > ignore_threshold) && (new_garbage < min_garbage);\n+    size_t live_bytes = r->get_live_data_bytes();\n+    if (add_regardless || (region_garbage >= garbage_threshold)) {\n+      if (r->is_old()) {\n+        size_t anticipated_consumption = (size_t) (live_bytes * ShenandoahOldEvacWaste);\n+        size_t new_old_consumption = consumed_by_old_evac + anticipated_consumption;\n+        size_t new_old_evac_reserve = old_evac_reserve;\n+        size_t proposed_old_region_expansion = 0;\n+        while ((new_old_consumption > new_old_evac_reserve) && (committed_from_shared_reserves < shared_reserves)) {\n+          committed_from_shared_reserves += region_size_bytes;\n+          proposed_old_region_expansion++;\n+          new_old_evac_reserve += region_size_bytes;\n@@ -109,16 +148,65 @@\n-      }\n-      if ((new_cset <= max_old_cset) && (r->garbage() > garbage_threshold)) {\n-        add_region = true;\n-        old_cur_cset = new_cset;\n-      }\n-    } else {\n-      assert(r->is_young() && !heap->is_tenurable(r), \"DeMorgan's law (assuming r->is_affiliated)\");\n-      size_t new_cset = young_cur_cset + r->get_live_data_bytes();\n-      size_t region_garbage = r->garbage();\n-      size_t new_garbage = cur_young_garbage + region_garbage;\n-      bool add_regardless = (region_garbage > ignore_threshold) && (new_garbage < min_garbage);\n-\n-      if (add_regardless || (r->garbage() > garbage_threshold)) {\n-        while ((new_cset > max_young_cset) && (unaffiliated_young_regions > 0)) {\n-          unaffiliated_young_regions--;\n-          max_young_cset += region_size_bytes \/ ShenandoahEvacWaste;\n+        \/\/ If this region has free memory and we choose to place it in the collection set, its free memory is no longer\n+        \/\/ available to hold promotion results.  So we behave as if its free memory is consumed within the promotion reserve.\n+        size_t anticipated_loss_from_promo_reserve = r->free();\n+        size_t new_promo_consumption = consumed_by_promo + anticipated_loss_from_promo_reserve;\n+        size_t new_promo_reserve = old_promo_reserve;\n+        while ((new_promo_consumption > new_promo_reserve) && (committed_from_shared_reserves < shared_reserves)) {\n+          committed_from_shared_reserves += region_size_bytes;\n+          proposed_old_region_expansion++;\n+          new_promo_reserve += region_size_bytes;\n+        }\n+        if ((new_old_consumption <= new_old_evac_reserve) && (new_promo_consumption <= new_promo_reserve)) {\n+          add_region = true;\n+          old_evac_reserve = new_old_evac_reserve;\n+          old_promo_reserve = new_promo_reserve;\n+          old_evac_bytes += live_bytes;\n+          consumed_by_old_evac = new_old_consumption;\n+          consumed_by_promo = new_promo_consumption;\n+          cur_garbage = new_garbage;\n+          old_regions_evacuated++;\n+        } else {\n+          \/\/ We failed to sufficiently expand old so unwind proposed expansion\n+          committed_from_shared_reserves -= proposed_old_region_expansion * region_size_bytes;\n+        }\n+      } else if (heap->is_tenurable(r)) {\n+        size_t anticipated_consumption = (size_t) (live_bytes * ShenandoahPromoEvacWaste);\n+        size_t new_promo_consumption = consumed_by_promo + anticipated_consumption;\n+        size_t new_promo_reserve = old_promo_reserve;\n+        size_t proposed_old_region_expansion = 0;\n+        while ((new_promo_consumption > new_promo_reserve) && (committed_from_shared_reserves < shared_reserves)) {\n+          committed_from_shared_reserves += region_size_bytes;\n+          proposed_old_region_expansion++;\n+          new_promo_reserve += region_size_bytes;\n+        }\n+        if (new_promo_consumption <= new_promo_reserve) {\n+          add_region = true;\n+          old_promo_reserve = new_promo_reserve;\n+          promo_bytes += live_bytes;\n+          consumed_by_promo = new_promo_consumption;\n+          cur_garbage = new_garbage;\n+          aged_regions_promoted++;\n+        } else {\n+          \/\/ We failed to sufficiently expand old so unwind proposed expansion\n+          committed_from_shared_reserves -= proposed_old_region_expansion * region_size_bytes;\n+        }\n+      } else {\n+        assert(r->is_young() && !heap->is_tenurable(r), \"DeMorgan's law (assuming r->is_affiliated)\");\n+        size_t anticipated_consumption = (size_t) (live_bytes * ShenandoahEvacWaste);\n+        size_t new_young_evac_consumption = consumed_by_young_evac + anticipated_consumption;\n+        size_t new_young_evac_reserve = young_evac_reserve;\n+        size_t proposed_young_region_expansion = 0;\n+        while ((new_young_evac_consumption > new_young_evac_reserve) && (committed_from_shared_reserves < shared_reserves)) {\n+          committed_from_shared_reserves += region_size_bytes;\n+          proposed_young_region_expansion++;\n+          new_young_evac_reserve += region_size_bytes;\n+        }\n+        if (new_young_evac_consumption <= new_young_evac_reserve) {\n+          add_region = true;\n+          young_evac_reserve = new_young_evac_reserve;\n+          young_evac_bytes += live_bytes;\n+          consumed_by_young_evac = new_young_evac_consumption;\n+          cur_garbage = new_garbage;\n+          young_regions_evacuated++;\n+        } else {\n+          \/\/ We failed to sufficiently expand old so unwind proposed expansion\n+          committed_from_shared_reserves -= proposed_young_region_expansion * region_size_bytes;\n@@ -126,5 +214,0 @@\n-      }\n-      if ((new_cset <= max_young_cset) && (add_regardless || (region_garbage > garbage_threshold))) {\n-        add_region = true;\n-        young_cur_cset = new_cset;\n-        cur_young_garbage = new_garbage;\n@@ -137,4 +220,5 @@\n-  if (regions_transferred_to_old > 0) {\n-    assert(young_evac_reserve > regions_transferred_to_old * region_size_bytes, \"young reserve cannot be negative\");\n-    heap->young_generation()->set_evacuation_reserve(young_evac_reserve - regions_transferred_to_old * region_size_bytes);\n-    heap->old_generation()->set_evacuation_reserve(old_evac_reserve + regions_transferred_to_old * region_size_bytes);\n+\n+  if (committed_from_shared_reserves < shared_reserves) {\n+    \/\/ Give all the rest to promotion\n+    old_promo_reserve += (shared_reserves - committed_from_shared_reserves);\n+    \/\/ dead code: committed_from_shared_reserves = shared_reserves;\n@@ -142,0 +226,36 @@\n+\n+  \/\/ Consider the effects of round-off:\n+  \/\/  1. We know that the sum over each evacuation mutiplied by Evacuation Waste is <= total evacuation reserve\n+  \/\/  2. However, the reserve for each individual evacuation may be rounded down.  In the worst case, we will be over budget\n+  \/\/     by the number of regions evacuated, since each region's reserve might be under-estimated by at most 1\n+  \/\/  3. Likewise, if we take the sum of bytes evacuated and multiply this by the Evacuation Waste and then round down\n+  \/\/     to nearest integer, the calculated reserve will underestimate the true reserve needs by at most 1.\n+  \/\/  4. This explains the adjustments to subtotals in the assert statements below.\n+  assert(young_evac_bytes * ShenandoahEvacWaste <= young_evac_reserve + young_regions_evacuated,\n+         \"budget: %zu <= %zu\", (size_t) (young_evac_bytes * ShenandoahEvacWaste), young_evac_reserve);\n+  assert(old_evac_bytes * ShenandoahOldEvacWaste <= old_evac_reserve + old_regions_evacuated,\n+         \"budget: %zu <= %zu\", (size_t) (old_evac_bytes * ShenandoahOldEvacWaste), old_evac_reserve);\n+  assert(promo_bytes * ShenandoahPromoEvacWaste <= old_promo_reserve + aged_regions_promoted,\n+         \"budget: %zu <= %zu\", (size_t) (promo_bytes * ShenandoahPromoEvacWaste), old_promo_reserve);\n+  assert(young_evac_reserve + old_evac_reserve + old_promo_reserve <=\n+         heap->young_generation()->get_evacuation_reserve() + heap->old_generation()->get_evacuation_reserve() +\n+         heap->old_generation()->get_promoted_reserve(), \"Exceeded budget\");\n+\n+  if (heap->young_generation()->get_evacuation_reserve() < young_evac_reserve) {\n+    size_t delta_bytes = young_evac_reserve - heap->young_generation()->get_evacuation_reserve();\n+    size_t delta_regions = delta_bytes \/ region_size_bytes;\n+    size_t regions_to_transfer = MIN2(unaffiliated_old_regions, delta_regions);\n+    log_info(gc)(\"Global GC moves %zu unaffiliated regions from old collector to young collector reserves\", regions_to_transfer);\n+    ssize_t negated_regions = -regions_to_transfer;\n+    heap->free_set()->move_unaffiliated_regions_from_collector_to_old_collector(negated_regions);\n+  } else if (heap->young_generation()->get_evacuation_reserve() > young_evac_reserve) {\n+    size_t delta_bytes = heap->young_generation()->get_evacuation_reserve() - young_evac_reserve;\n+    size_t delta_regions = delta_bytes \/ region_size_bytes;\n+    size_t regions_to_transfer = MIN2(unaffiliated_young_regions, delta_regions);\n+    log_info(gc)(\"Global GC moves %zu unaffiliated regions from young collector to old collector reserves\", regions_to_transfer);\n+    heap->free_set()->move_unaffiliated_regions_from_collector_to_old_collector(regions_to_transfer);\n+  }\n+\n+  heap->young_generation()->set_evacuation_reserve(young_evac_reserve);\n+  heap->old_generation()->set_evacuation_reserve(old_evac_reserve);\n+  heap->old_generation()->set_promoted_reserve(old_promo_reserve);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/heuristics\/shenandoahGlobalHeuristics.cpp","additions":183,"deletions":63,"binary":false,"changes":246,"status":"modified"},{"patch":"@@ -42,3 +42,3 @@\n-  void choose_collection_set_from_regiondata(ShenandoahCollectionSet* cset,\n-                                             RegionData* data, size_t size,\n-                                             size_t actual_free) override;\n+  size_t choose_collection_set_from_regiondata(ShenandoahCollectionSet* cset,\n+                                               RegionData* data, size_t size,\n+                                               size_t actual_free) override;\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/heuristics\/shenandoahGlobalHeuristics.hpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -75,1 +75,1 @@\n-void ShenandoahHeuristics::choose_collection_set(ShenandoahCollectionSet* collection_set) {\n+size_t ShenandoahHeuristics::choose_collection_set(ShenandoahCollectionSet* collection_set) {\n@@ -156,1 +156,0 @@\n-\n@@ -158,0 +157,1 @@\n+  return 0;\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/heuristics\/shenandoahHeuristics.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -186,3 +186,6 @@\n-  virtual void choose_collection_set_from_regiondata(ShenandoahCollectionSet* set,\n-                                                     RegionData* data, size_t data_size,\n-                                                     size_t free) = 0;\n+  \/\/ This is a helper function to choose_collection_set(), returning the number of regions that need to be transferred to\n+  \/\/ the old reserve from the young reserve in order to effectively evacuate the chosen collection set.  In non-generational\n+  \/\/ mode, the return value is 0.\n+  virtual size_t choose_collection_set_from_regiondata(ShenandoahCollectionSet* set,\n+                                                       RegionData* data, size_t data_size,\n+                                                       size_t free) = 0;\n@@ -236,1 +239,3 @@\n-  virtual void choose_collection_set(ShenandoahCollectionSet* collection_set);\n+  \/\/ Choose the collection set, returning the number of regions that need to be transferred to the old reserve from the young\n+  \/\/ reserve in order to effectively evacuate the chosen collection set.  In non-generational mode, the return value is 0.\n+  virtual size_t choose_collection_set(ShenandoahCollectionSet* collection_set);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/heuristics\/shenandoahHeuristics.hpp","additions":9,"deletions":4,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -29,0 +29,1 @@\n+#include \"gc\/shenandoah\/shenandoahFreeSet.hpp\"\n@@ -32,0 +33,1 @@\n+#include \"gc\/shenandoah\/shenandoahYoungGeneration.hpp\"\n@@ -80,3 +82,4 @@\n-  if (unprocessed_old_collection_candidates() == 0) {\n-    return false;\n-  }\n+  _mixed_evac_cset = collection_set;\n+  _included_old_regions = 0;\n+  _evacuated_old_bytes = 0;\n+  _collected_old_bytes = 0;\n@@ -88,1 +91,2 @@\n-    log_info(gc, ergo)(\"Remaining \" UINT32_FORMAT \" old regions are being coalesced and filled\", unprocessed_old_collection_candidates());\n+    log_info(gc, ergo)(\"Remaining \" UINT32_FORMAT\n+                       \" old regions are being coalesced and filled\", unprocessed_old_collection_candidates());\n@@ -114,10 +118,22 @@\n-  const size_t old_evacuation_reserve = _old_generation->get_evacuation_reserve();\n-  const size_t old_evacuation_budget = (size_t) ((double) old_evacuation_reserve \/ ShenandoahOldEvacWaste);\n-  size_t unfragmented_available = _old_generation->free_unaffiliated_regions() * ShenandoahHeapRegion::region_size_bytes();\n-  size_t fragmented_available;\n-  size_t excess_fragmented_available;\n-\n-  if (unfragmented_available > old_evacuation_budget) {\n-    unfragmented_available = old_evacuation_budget;\n-    fragmented_available = 0;\n-    excess_fragmented_available = 0;\n+  _old_evacuation_reserve = _old_generation->get_evacuation_reserve();\n+  _old_evacuation_budget = (size_t) ((double) _old_evacuation_reserve \/ ShenandoahOldEvacWaste);\n+\n+  \/\/ fragmented_available is the amount of memory within partially consumed old regions that may be required to\n+  \/\/ hold the results of old evacuations.  If all of the memory required by the old evacuation reserve is available\n+  \/\/ in unfragmented regions (unaffiliated old regions), then fragmented_available is zero because we do not need\n+  \/\/ to evacuate into the existing partially consumed old regions.\n+\n+  \/\/ if fragmented_available is non-zero, excess_fragmented_old_budget represents the amount of fragmented memory\n+  \/\/ that is available within old, but is not required to hold the resuilts of old evacuation.  As old-gen regions\n+  \/\/ are added into the collection set, their free memory is subtracted from excess_fragmented_old_budget until the\n+  \/\/ excess is exhausted.  For old-gen regions subsequently added to the collection set, their free memory is\n+  \/\/ subtracted from fragmented_available and from the old_evacuation_budget (since the budget decreases when this\n+  \/\/ fragmented_available memory decreases).  After fragmented_available has been exhausted, any further old regions\n+  \/\/ selected for the cset do not further decrease the old_evacuation_budget because all further evacuation is targeted\n+  \/\/ to unfragmented regions.\n+\n+  size_t unaffiliated_available = _old_generation->free_unaffiliated_regions() * ShenandoahHeapRegion::region_size_bytes();\n+  if (unaffiliated_available > _old_evacuation_reserve) {\n+    _unspent_unfragmented_old_budget = _old_evacuation_budget;\n+    _unspent_fragmented_old_budget = 0;\n+    _excess_fragmented_old_budget = 0;\n@@ -125,6 +141,6 @@\n-    assert(_old_generation->available() >= old_evacuation_budget, \"Cannot budget more than is available\");\n-    fragmented_available = _old_generation->available() - unfragmented_available;\n-    assert(fragmented_available + unfragmented_available >= old_evacuation_budget, \"Budgets do not add up\");\n-    if (fragmented_available + unfragmented_available > old_evacuation_budget) {\n-      excess_fragmented_available = (fragmented_available + unfragmented_available) - old_evacuation_budget;\n-      fragmented_available -= excess_fragmented_available;\n+    assert(_old_generation->available() >= _old_evacuation_reserve, \"Cannot reserve more than is available\");\n+    size_t affiliated_available = _old_generation->available() - unaffiliated_available;\n+    assert(affiliated_available + unaffiliated_available >= _old_evacuation_reserve, \"Budgets do not add up\");\n+    if (affiliated_available + unaffiliated_available > _old_evacuation_reserve) {\n+      _excess_fragmented_old_budget = (affiliated_available + unaffiliated_available) - _old_evacuation_reserve;\n+      affiliated_available -= _excess_fragmented_old_budget;\n@@ -132,0 +148,2 @@\n+    _unspent_fragmented_old_budget = (size_t) ((double) affiliated_available \/ ShenandoahOldEvacWaste);\n+    _unspent_unfragmented_old_budget = (size_t) ((double) unaffiliated_available \/ ShenandoahOldEvacWaste);\n@@ -134,3 +152,2 @@\n-  size_t remaining_old_evacuation_budget = old_evacuation_budget;\n-  log_debug(gc)(\"Choose old regions for mixed collection: old evacuation budget: %zu%s, candidates: %u\",\n-                byte_size_in_proper_unit(old_evacuation_budget), proper_unit_for_byte_size(old_evacuation_budget),\n+  log_debug(gc)(\"Choose old regions for mixed collection: old evacuation budget: \" PROPERFMT \", candidates: %u\",\n+                PROPERFMTARGS(_old_evacuation_budget),\n@@ -138,120 +155,1 @@\n-\n-  size_t lost_evacuation_capacity = 0;\n-\n-  \/\/ The number of old-gen regions that were selected as candidates for collection at the end of the most recent old-gen\n-  \/\/ concurrent marking phase and have not yet been collected is represented by unprocessed_old_collection_candidates().\n-  \/\/ Candidate regions are ordered according to increasing amount of live data.  If there is not sufficient room to\n-  \/\/ evacuate region N, then there is no need to even consider evacuating region N+1.\n-  while (unprocessed_old_collection_candidates() > 0) {\n-    \/\/ Old collection candidates are sorted in order of decreasing garbage contained therein.\n-    ShenandoahHeapRegion* r = next_old_collection_candidate();\n-    if (r == nullptr) {\n-      break;\n-    }\n-    assert(r->is_regular(), \"There should be no humongous regions in the set of mixed-evac candidates\");\n-\n-    \/\/ If region r is evacuated to fragmented memory (to free memory within a partially used region), then we need\n-    \/\/ to decrease the capacity of the fragmented memory by the scaled loss.\n-\n-    const size_t live_data_for_evacuation = r->get_live_data_bytes();\n-    size_t lost_available = r->free();\n-\n-    if ((lost_available > 0) && (excess_fragmented_available > 0)) {\n-      if (lost_available < excess_fragmented_available) {\n-        excess_fragmented_available -= lost_available;\n-        lost_evacuation_capacity -= lost_available;\n-        lost_available  = 0;\n-      } else {\n-        lost_available -= excess_fragmented_available;\n-        lost_evacuation_capacity -= excess_fragmented_available;\n-        excess_fragmented_available = 0;\n-      }\n-    }\n-    size_t scaled_loss = (size_t) ((double) lost_available \/ ShenandoahOldEvacWaste);\n-    if ((lost_available > 0) && (fragmented_available > 0)) {\n-      if (scaled_loss + live_data_for_evacuation < fragmented_available) {\n-        fragmented_available -= scaled_loss;\n-        scaled_loss = 0;\n-      } else {\n-        \/\/ We will have to allocate this region's evacuation memory from unfragmented memory, so don't bother\n-        \/\/ to decrement scaled_loss\n-      }\n-    }\n-    if (scaled_loss > 0) {\n-      \/\/ We were not able to account for the lost free memory within fragmented memory, so we need to take this\n-      \/\/ allocation out of unfragmented memory.  Unfragmented memory does not need to account for loss of free.\n-      if (live_data_for_evacuation > unfragmented_available) {\n-        \/\/ There is no room to evacuate this region or any that come after it in within the candidates array.\n-        log_debug(gc, cset)(\"Not enough unfragmented memory (%zu) to hold evacuees (%zu) from region: (%zu)\",\n-                            unfragmented_available, live_data_for_evacuation, r->index());\n-        break;\n-      } else {\n-        unfragmented_available -= live_data_for_evacuation;\n-      }\n-    } else {\n-      \/\/ Since scaled_loss == 0, we have accounted for the loss of free memory, so we can allocate from either\n-      \/\/ fragmented or unfragmented available memory.  Use up the fragmented memory budget first.\n-      size_t evacuation_need = live_data_for_evacuation;\n-\n-      if (evacuation_need > fragmented_available) {\n-        evacuation_need -= fragmented_available;\n-        fragmented_available = 0;\n-      } else {\n-        fragmented_available -= evacuation_need;\n-        evacuation_need = 0;\n-      }\n-      if (evacuation_need > unfragmented_available) {\n-        \/\/ There is no room to evacuate this region or any that come after it in within the candidates array.\n-        log_debug(gc, cset)(\"Not enough unfragmented memory (%zu) to hold evacuees (%zu) from region: (%zu)\",\n-                            unfragmented_available, live_data_for_evacuation, r->index());\n-        break;\n-      } else {\n-        unfragmented_available -= evacuation_need;\n-        \/\/ dead code: evacuation_need == 0;\n-      }\n-    }\n-    collection_set->add_region(r);\n-    included_old_regions++;\n-    evacuated_old_bytes += live_data_for_evacuation;\n-    collected_old_bytes += r->garbage();\n-    consume_old_collection_candidate();\n-  }\n-\n-  if (_first_pinned_candidate != NOT_FOUND) {\n-    \/\/ Need to deal with pinned regions\n-    slide_pinned_regions_to_front();\n-  }\n-  decrease_unprocessed_old_collection_candidates_live_memory(evacuated_old_bytes);\n-  if (included_old_regions > 0) {\n-    log_info(gc, ergo)(\"Old-gen piggyback evac (\" UINT32_FORMAT \" regions, evacuating \" PROPERFMT \", reclaiming: \" PROPERFMT \")\",\n-                  included_old_regions, PROPERFMTARGS(evacuated_old_bytes), PROPERFMTARGS(collected_old_bytes));\n-  }\n-\n-  if (unprocessed_old_collection_candidates() == 0) {\n-    \/\/ We have added the last of our collection candidates to a mixed collection.\n-    \/\/ Any triggers that occurred during mixed evacuations may no longer be valid.  They can retrigger if appropriate.\n-    clear_triggers();\n-\n-    _old_generation->complete_mixed_evacuations();\n-  } else if (included_old_regions == 0) {\n-    \/\/ We have candidates, but none were included for evacuation - are they all pinned?\n-    \/\/ or did we just not have enough room for any of them in this collection set?\n-    \/\/ We don't want a region with a stuck pin to prevent subsequent old collections, so\n-    \/\/ if they are all pinned we transition to a state that will allow us to make these uncollected\n-    \/\/ (pinned) regions parsable.\n-    if (all_candidates_are_pinned()) {\n-      log_info(gc, ergo)(\"All candidate regions \" UINT32_FORMAT \" are pinned\", unprocessed_old_collection_candidates());\n-      _old_generation->abandon_mixed_evacuations();\n-    } else {\n-      log_info(gc, ergo)(\"No regions selected for mixed collection. \"\n-                         \"Old evacuation budget: \" PROPERFMT \", Remaining evacuation budget: \" PROPERFMT\n-                         \", Lost capacity: \" PROPERFMT\n-                         \", Next candidate: \" UINT32_FORMAT \", Last candidate: \" UINT32_FORMAT,\n-                         PROPERFMTARGS(old_evacuation_reserve),\n-                         PROPERFMTARGS(remaining_old_evacuation_budget),\n-                         PROPERFMTARGS(lost_evacuation_capacity),\n-                         _next_old_collection_candidate, _last_old_collection_candidate);\n-    }\n-  }\n-\n-  return (included_old_regions > 0);\n+  return add_old_regions_to_cset();\n@@ -331,0 +229,181 @@\n+bool ShenandoahOldHeuristics::add_old_regions_to_cset() {\n+  if (unprocessed_old_collection_candidates() == 0) {\n+    return false;\n+  }\n+  _first_pinned_candidate = NOT_FOUND;\n+\n+  \/\/ The number of old-gen regions that were selected as candidates for collection at the end of the most recent old-gen\n+  \/\/ concurrent marking phase and have not yet been collected is represented by unprocessed_old_collection_candidates().\n+  \/\/ Candidate regions are ordered according to increasing amount of live data.  If there is not sufficient room to\n+  \/\/ evacuate region N, then there is no need to even consider evacuating region N+1.\n+  while (unprocessed_old_collection_candidates() > 0) {\n+    \/\/ Old collection candidates are sorted in order of decreasing garbage contained therein.\n+    ShenandoahHeapRegion* r = next_old_collection_candidate();\n+    if (r == nullptr) {\n+      break;\n+    }\n+    assert(r->is_regular(), \"There should be no humongous regions in the set of mixed-evac candidates\");\n+\n+    \/\/ If region r is evacuated to fragmented memory (to free memory within a partially used region), then we need\n+    \/\/ to decrease the capacity of the fragmented memory by the scaled loss.\n+\n+    const size_t live_data_for_evacuation = r->get_live_data_bytes();\n+    size_t lost_available = r->free();\n+\n+    ssize_t fragmented_delta = 0;\n+    ssize_t unfragmented_delta = 0;\n+    ssize_t excess_delta = 0;\n+\n+    \/\/ We must decrease our mixed-evacuation budgets proportional to the lost available memory.  This memory that is no\n+    \/\/ longer available was likely \"promised\" to promotions, so we must decrease our mixed evacuations now.\n+    \/\/ (e.g. if we loose 14 bytes of available old memory, we must decrease the evacuation budget by 10 bytes.)\n+    size_t scaled_loss = (size_t) (((double) lost_available) \/ ShenandoahOldEvacWaste);\n+    if (lost_available > 0) {\n+      \/\/ We need to subtract lost_available from our working evacuation budgets\n+      if (scaled_loss < _excess_fragmented_old_budget) {\n+        excess_delta -= scaled_loss;\n+        _excess_fragmented_old_budget -= scaled_loss;\n+      } else {\n+        excess_delta -= _excess_fragmented_old_budget;\n+        _excess_fragmented_old_budget = 0;\n+      }\n+\n+      if (scaled_loss < _unspent_fragmented_old_budget) {\n+        _unspent_fragmented_old_budget -= scaled_loss;\n+        fragmented_delta = -scaled_loss;\n+        scaled_loss = 0;\n+      } else {\n+        scaled_loss -= _unspent_fragmented_old_budget;\n+        fragmented_delta = -_unspent_fragmented_old_budget;\n+        _unspent_fragmented_old_budget = 0;\n+      }\n+\n+      if (scaled_loss < _unspent_unfragmented_old_budget) {\n+        _unspent_unfragmented_old_budget -= scaled_loss;\n+        unfragmented_delta = -scaled_loss;\n+        scaled_loss = 0;\n+      } else {\n+        scaled_loss -= _unspent_unfragmented_old_budget;\n+        fragmented_delta = -_unspent_unfragmented_old_budget;\n+        _unspent_unfragmented_old_budget = 0;\n+      }\n+    }\n+\n+    \/\/ Allocate replica from unfragmented memory if that exists\n+    size_t evacuation_need = live_data_for_evacuation;\n+    if (evacuation_need < _unspent_unfragmented_old_budget) {\n+      _unspent_unfragmented_old_budget -= evacuation_need;\n+    } else {\n+      if (_unspent_unfragmented_old_budget > 0) {\n+        evacuation_need -= _unspent_unfragmented_old_budget;\n+        unfragmented_delta -= _unspent_unfragmented_old_budget;\n+        _unspent_unfragmented_old_budget = 0;\n+      }\n+      \/\/ Take the remaining allocation out of fragmented available\n+      if (_unspent_fragmented_old_budget > evacuation_need) {\n+        _unspent_fragmented_old_budget -= evacuation_need;\n+      } else {\n+        \/\/ We cannot add this region into the collection set.  We're done.  Undo the adjustments to available.\n+        _unspent_fragmented_old_budget -= fragmented_delta;\n+        _unspent_unfragmented_old_budget -= unfragmented_delta;\n+        _excess_fragmented_old_budget -= excess_delta;\n+        break;\n+      }\n+    }\n+    _mixed_evac_cset->add_region(r);\n+    _included_old_regions++;\n+    _evacuated_old_bytes += live_data_for_evacuation;\n+    _collected_old_bytes += r->garbage();\n+    consume_old_collection_candidate();\n+  }\n+  return true;\n+}\n+\n+bool ShenandoahOldHeuristics::finalize_mixed_evacs() {\n+  if (_first_pinned_candidate != NOT_FOUND) {\n+    \/\/ Need to deal with pinned regions\n+    slide_pinned_regions_to_front();\n+  }\n+  decrease_unprocessed_old_collection_candidates_live_memory(_evacuated_old_bytes);\n+  if (_included_old_regions > 0) {\n+    log_info(gc)(\"Old-gen mixed evac (%zu regions, evacuating %zu%s, reclaiming: %zu%s)\",\n+                 _included_old_regions,\n+                 byte_size_in_proper_unit(_evacuated_old_bytes), proper_unit_for_byte_size(_evacuated_old_bytes),\n+                 byte_size_in_proper_unit(_collected_old_bytes), proper_unit_for_byte_size(_collected_old_bytes));\n+  }\n+\n+  if (unprocessed_old_collection_candidates() == 0) {\n+    \/\/ We have added the last of our collection candidates to a mixed collection.\n+    \/\/ Any triggers that occurred during mixed evacuations may no longer be valid.  They can retrigger if appropriate.\n+    clear_triggers();\n+    _old_generation->complete_mixed_evacuations();\n+  } else if (_included_old_regions == 0) {\n+    \/\/ We have candidates, but none were included for evacuation - are they all pinned?\n+    \/\/ or did we just not have enough room for any of them in this collection set?\n+    \/\/ We don't want a region with a stuck pin to prevent subsequent old collections, so\n+    \/\/ if they are all pinned we transition to a state that will allow us to make these uncollected\n+    \/\/ (pinned) regions parsable.\n+    if (all_candidates_are_pinned()) {\n+      log_info(gc)(\"All candidate regions \" UINT32_FORMAT \" are pinned\", unprocessed_old_collection_candidates());\n+      _old_generation->abandon_mixed_evacuations();\n+    } else {\n+      log_info(gc)(\"No regions selected for mixed collection. \"\n+                   \"Old evacuation budget: \" PROPERFMT \", Next candidate: \" UINT32_FORMAT \", Last candidate: \" UINT32_FORMAT,\n+                   PROPERFMTARGS(_old_evacuation_reserve),\n+                   _next_old_collection_candidate, _last_old_collection_candidate);\n+    }\n+  }\n+  return (_included_old_regions > 0);\n+}\n+\n+bool ShenandoahOldHeuristics::top_off_collection_set(size_t &add_regions_to_old) {\n+  if (unprocessed_old_collection_candidates() == 0) {\n+    add_regions_to_old = 0;\n+    return false;\n+  } else {\n+    ShenandoahYoungGeneration* young_generation = _heap->young_generation();\n+    size_t young_unaffiliated_regions = young_generation->free_unaffiliated_regions();\n+    size_t max_young_cset = young_generation->get_evacuation_reserve();\n+\n+    \/\/ We have budgeted to assure the live_bytes_in_tenurable_regions() get evacuated into old generation.  Young reserves\n+    \/\/ only for untenurable region evacuations.\n+    size_t planned_young_evac = _mixed_evac_cset->get_live_bytes_in_untenurable_regions();\n+    size_t consumed_from_young_cset = (size_t) (planned_young_evac * ShenandoahEvacWaste);\n+\n+    size_t region_size_bytes = ShenandoahHeapRegion::region_size_bytes();\n+    size_t regions_required_for_collector_reserve = (consumed_from_young_cset + region_size_bytes - 1) \/ region_size_bytes;\n+\n+    assert(consumed_from_young_cset <= max_young_cset, \"sanity\");\n+    assert(max_young_cset <= young_unaffiliated_regions * region_size_bytes, \"sanity\");\n+\n+    size_t regions_for_old_expansion;\n+    if (consumed_from_young_cset < max_young_cset) {\n+      size_t excess_young_reserves = max_young_cset - consumed_from_young_cset;\n+      \/\/ We can only transfer empty regions from young to old.  Furthermore, we must be careful to assure that the young\n+      \/\/ Collector reserve that remains after transfer is comprised entirely of empty (unaffiliated) regions.\n+      size_t consumed_unaffiliated_regions = (consumed_from_young_cset + region_size_bytes - 1) \/ region_size_bytes;\n+      size_t available_unaffiliated_regions = ((young_unaffiliated_regions > consumed_unaffiliated_regions)?\n+                                               young_unaffiliated_regions - consumed_unaffiliated_regions: 0);\n+      regions_for_old_expansion = MIN2(available_unaffiliated_regions, excess_young_reserves \/ region_size_bytes);\n+    } else {\n+      regions_for_old_expansion = 0;\n+    }\n+    if (regions_for_old_expansion > 0) {\n+      log_info(gc)(\"Augmenting old-gen evacuation budget from unexpended young-generation reserve by %zu regions\",\n+                   regions_for_old_expansion);\n+      add_regions_to_old = regions_for_old_expansion;\n+      size_t budget_supplement = region_size_bytes * regions_for_old_expansion;\n+      size_t supplement_without_waste = (size_t) (((double) budget_supplement) \/ ShenandoahOldEvacWaste);\n+      _old_evacuation_budget += supplement_without_waste;\n+      _unspent_unfragmented_old_budget += supplement_without_waste;\n+      _old_generation->augment_evacuation_reserve(budget_supplement);\n+      young_generation->set_evacuation_reserve(max_young_cset - budget_supplement);\n+\n+      return add_old_regions_to_cset();\n+    } else {\n+      add_regions_to_old = 0;\n+      return false;\n+    }\n+  }\n+}\n+\n@@ -339,1 +418,0 @@\n-\n@@ -358,4 +436,4 @@\n-        \/\/ Only place regular or pinned regions with live data into the candidate set.\n-        \/\/ Pinned regions cannot be evacuated, but we are not actually choosing candidates\n-        \/\/ for the collection set here. That happens later during the next young GC cycle,\n-        \/\/ by which time, the pinned region may no longer be pinned.\n+      \/\/ Only place regular or pinned regions with live data into the candidate set.\n+      \/\/ Pinned regions cannot be evacuated, but we are not actually choosing candidates\n+      \/\/ for the collection set here. That happens later during the next young GC cycle,\n+      \/\/ by which time, the pinned region may no longer be pinned.\n@@ -564,0 +642,1 @@\n+  _live_bytes_in_unprocessed_candidates = 0;\n@@ -808,3 +887,3 @@\n-void ShenandoahOldHeuristics::choose_collection_set_from_regiondata(ShenandoahCollectionSet* set,\n-                                                                    ShenandoahHeuristics::RegionData* data,\n-                                                                    size_t data_size, size_t free) {\n+size_t ShenandoahOldHeuristics::choose_collection_set_from_regiondata(ShenandoahCollectionSet* set,\n+                                                                      ShenandoahHeuristics::RegionData* data,\n+                                                                      size_t data_size, size_t free) {\n@@ -812,0 +891,1 @@\n+  return 0;\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/heuristics\/shenandoahOldHeuristics.cpp","additions":231,"deletions":151,"binary":false,"changes":382,"status":"modified"},{"patch":"@@ -105,0 +105,24 @@\n+  \/\/ State variables involved in construction of a mixed-evacuation collection set.  These variables are initialized\n+  \/\/ when client code invokes prime_collection_set().  They are consulted, and sometimes modified, when client code\n+  \/\/ calls top_off_collection_set() to possibly expand the number of old-gen regions in a mixed evacuation cset, and by\n+  \/\/ finalize_mixed_evacs(), which prepares the way for mixed evacuations to begin.\n+  ShenandoahCollectionSet* _mixed_evac_cset;\n+  size_t _evacuated_old_bytes;\n+  size_t _collected_old_bytes;\n+  size_t _included_old_regions;\n+  size_t _old_evacuation_reserve;\n+  size_t _old_evacuation_budget;\n+\n+  \/\/ This represents the amount of memory that can be evacuated from old into initially empty regions during a mixed evacuation.\n+  \/\/ This is the total amount of unfragmented free memory in old divided by ShenandoahOldEvacWaste.\n+  size_t _unspent_unfragmented_old_budget;\n+\n+  \/\/ This represents the amount of memory that can be evacuated from old into initially non-empty regions during a mixed\n+  \/\/ evacuation.  This is the total amount of initially fragmented free memory in old divided by ShenandoahOldEvacWaste.\n+  size_t _unspent_fragmented_old_budget;\n+\n+  \/\/ If there is more available memory in old than is required by the intended mixed evacuation, the amount of excess\n+  \/\/ memory is represented by _excess_fragmented_old.  To convert this value into a promotion budget, multiply by\n+  \/\/ ShenandoahOldEvacWaste and divide by ShenandoahPromoWaste.\n+  size_t _excess_fragmented_old_budget;\n+\n@@ -134,1 +158,9 @@\n-  void choose_collection_set_from_regiondata(ShenandoahCollectionSet* set, RegionData* data, size_t data_size, size_t free) override;\n+  size_t\n+  choose_collection_set_from_regiondata(ShenandoahCollectionSet* set, RegionData* data, size_t data_size, size_t free) override;\n+\n+  \/\/ This internal helper routine adds as many mixed evacuation candidate regions as fit within the old-gen evacuation budget\n+  \/\/ to the collection set.  This may be called twice to prepare for any given mixed evacuation cycle, the first time with\n+  \/\/ a conservative old evacuation budget, and the second time with a larger more aggressive old evacuation budget.  Returns\n+  \/\/ true iff we need to finalize mixed evacs.  (If no regions are added to the collection set, there is no need to finalize\n+  \/\/ mixed evacuations.)\n+  bool add_old_regions_to_cset();\n@@ -142,2 +174,16 @@\n-  \/\/ Return true iff the collection set is primed with at least one old-gen region.\n-  bool prime_collection_set(ShenandoahCollectionSet* set);\n+  \/\/ Initialize instance variables to support the preparation of a mixed-evacuation collection set.  Adds as many\n+  \/\/ old candidate regions into the collection set as can fit within the iniital conservative old evacuation budget.\n+  \/\/ Returns true iff we need to finalize mixed evacs.\n+  bool prime_collection_set(ShenandoahCollectionSet* collection_set);\n+\n+  \/\/ If young evacuation did not consume all of its available evacuation reserve, add as many additional mixed-\n+  \/\/ evacuation candidate regions into the collection set as will fit within this excess repurposed reserved.\n+  \/\/ Returns true iff we need to finalize mixed evacs.  Upon return, the var parameter regions_to_xfer holds the\n+  \/\/ number of regions to transfer from young to old.\n+  bool top_off_collection_set(size_t &add_regions_to_old);\n+\n+  \/\/ Having added all eligible mixed-evacuation candidates to the collection set, this function updates the total count\n+  \/\/ of how much old-gen memory remains to be evacuated and adjusts the representation of old-gen regions that remain to\n+  \/\/ be evacuated, giving special attention to regions that are currently pinned.  It outputs relevant log messages and\n+  \/\/ returns true iff the collection set holds at least one unpinned mixed evacuation candidate.\n+  bool finalize_mixed_evacs();\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/heuristics\/shenandoahOldHeuristics.hpp","additions":49,"deletions":3,"binary":false,"changes":52,"status":"modified"},{"patch":"@@ -53,3 +53,3 @@\n-void ShenandoahPassiveHeuristics::choose_collection_set_from_regiondata(ShenandoahCollectionSet* cset,\n-                                                                        RegionData* data, size_t size,\n-                                                                        size_t actual_free) {\n+size_t ShenandoahPassiveHeuristics::choose_collection_set_from_regiondata(ShenandoahCollectionSet* cset,\n+                                                                          RegionData* data, size_t size,\n+                                                                          size_t actual_free) {\n@@ -79,0 +79,1 @@\n+  return 0;\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/heuristics\/shenandoahPassiveHeuristics.cpp","additions":4,"deletions":3,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -49,3 +49,3 @@\n-  virtual void choose_collection_set_from_regiondata(ShenandoahCollectionSet* set,\n-                                                     RegionData* data, size_t data_size,\n-                                                     size_t free);\n+  virtual size_t choose_collection_set_from_regiondata(ShenandoahCollectionSet* set,\n+                                                       RegionData* data, size_t data_size,\n+                                                       size_t free);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/heuristics\/shenandoahPassiveHeuristics.hpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -62,3 +62,3 @@\n-void ShenandoahStaticHeuristics::choose_collection_set_from_regiondata(ShenandoahCollectionSet* cset,\n-                                                                       RegionData* data, size_t size,\n-                                                                       size_t free) {\n+size_t ShenandoahStaticHeuristics::choose_collection_set_from_regiondata(ShenandoahCollectionSet* cset,\n+                                                                         RegionData* data, size_t size,\n+                                                                         size_t free) {\n@@ -73,0 +73,1 @@\n+  return 0;\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/heuristics\/shenandoahStaticHeuristics.cpp","additions":4,"deletions":3,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -43,3 +43,3 @@\n-  virtual void choose_collection_set_from_regiondata(ShenandoahCollectionSet* cset,\n-                                                     RegionData* data, size_t size,\n-                                                     size_t free);\n+  virtual size_t choose_collection_set_from_regiondata(ShenandoahCollectionSet* cset,\n+                                                       RegionData* data, size_t size,\n+                                                       size_t free);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/heuristics\/shenandoahStaticHeuristics.hpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -36,1 +36,1 @@\n-        : ShenandoahGenerationalHeuristics(generation) {\n+    : ShenandoahGenerationalHeuristics(generation) {\n@@ -40,1 +40,1 @@\n-void ShenandoahYoungHeuristics::choose_collection_set_from_regiondata(ShenandoahCollectionSet* cset,\n+size_t ShenandoahYoungHeuristics::choose_collection_set_from_regiondata(ShenandoahCollectionSet* cset,\n@@ -51,0 +51,2 @@\n+  ShenandoahGenerationalHeap* heap = ShenandoahGenerationalHeap::heap();\n+  bool need_to_finalize_mixed = heap->old_generation()->heuristics()->prime_collection_set(cset);\n@@ -58,0 +60,11 @@\n+\n+  \/\/ Especially when young-gen trigger is expedited in order to finish mixed evacuations, there may not be\n+  \/\/ enough consolidated garbage to make effective use of young-gen evacuation reserve.  If there is still\n+  \/\/ young-gen reserve available following selection of the young-gen collection set, see if we can use\n+  \/\/ this memory to expand the old-gen evacuation collection set.\n+  size_t add_regions_to_old;\n+  need_to_finalize_mixed |= heap->old_generation()->heuristics()->top_off_collection_set(add_regions_to_old);\n+  if (need_to_finalize_mixed) {\n+    heap->old_generation()->heuristics()->finalize_mixed_evacs();\n+  }\n+  return add_regions_to_old;\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/heuristics\/shenandoahYoungHeuristics.cpp","additions":15,"deletions":2,"binary":false,"changes":17,"status":"modified"},{"patch":"@@ -41,3 +41,3 @@\n-  void choose_collection_set_from_regiondata(ShenandoahCollectionSet* cset,\n-                                             RegionData* data, size_t size,\n-                                             size_t actual_free) override;\n+  size_t choose_collection_set_from_regiondata(ShenandoahCollectionSet* cset,\n+                                               RegionData* data, size_t size,\n+                                               size_t actual_free) override;\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/heuristics\/shenandoahYoungHeuristics.hpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -53,0 +53,2 @@\n+  _young_available_bytes_collected(0),\n+  _old_available_bytes_collected(0),\n@@ -107,0 +109,1 @@\n+    _old_available_bytes_collected += free;\n@@ -143,0 +146,1 @@\n+  _old_available_bytes_collected = 0;\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahCollectionSet.cpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -78,0 +78,4 @@\n+  \/\/ When a region having memory available to be allocated is added to the collection set, the region's available memory\n+  \/\/ should be subtracted from what's available.\n+  size_t                _old_available_bytes_collected;\n+\n@@ -124,0 +128,3 @@\n+  \/\/ Returns the amount of free bytes in old regions in the collection set.\n+  size_t get_old_available_bytes_collected() const { return _old_available_bytes_collected; }\n+\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahCollectionSet.hpp","additions":7,"deletions":0,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -207,2 +207,0 @@\n-    entry_concurrent_update_refs_prepare(heap);\n-\n@@ -210,0 +208,1 @@\n+    entry_concurrent_update_refs_prepare(heap);\n@@ -230,0 +229,1 @@\n+    _abbreviated = true;\n@@ -238,1 +238,0 @@\n-    _abbreviated = true;\n@@ -285,1 +284,0 @@\n-\n@@ -539,0 +537,6 @@\n+  if (!heap->is_evacuation_in_progress()) {\n+    \/\/ This is an abbreviated cycle.  Rebuild the freeset in order to establish reserves for the next GC cycle.  Doing\n+    \/\/ the rebuild ASAP also expedites availability of immediate trash, reducing the likelihood that we will degenerate\n+    \/\/ during promote-in-place processing.\n+    heap->rebuild_free_set(true \/*concurrent*\/);\n+  }\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahConcurrentGC.cpp","additions":8,"deletions":4,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -329,1 +329,1 @@\n-  shenandoah_assert_heaplocked();\n+  shenandoah_assert_heaplocked_or_safepoint();\n@@ -442,0 +442,7 @@\n+void ShenandoahRegionPartitions::set_used_by(ShenandoahFreeSetPartitionId which_partition, size_t value) {\n+  shenandoah_assert_heaplocked();\n+  assert (which_partition < NumPartitions, \"selected free set must be valid\");\n+  _used[int(which_partition)] = value;\n+  _available[int(which_partition)] = _capacity[int(which_partition)] - value;\n+}\n+\n@@ -903,1 +910,1 @@\n-void ShenandoahRegionPartitions::assert_bounds(bool validate_totals) {\n+void ShenandoahRegionPartitions::assert_bounds() {\n@@ -939,1 +946,1 @@\n-        assert(!validate_totals || (capacity != _region_size_bytes), \"Should not be retired if empty\");\n+        assert(capacity != _region_size_bytes, \"Should not be retired if empty\");\n@@ -979,0 +986,1 @@\n+        ShenandoahHeapRegion* r = ShenandoahHeap::heap()->get_region(i);\n@@ -984,1 +992,0 @@\n-\n@@ -1023,1 +1030,1 @@\n-          \"Mutator free regions before the leftmost: %zd, bound %zd\",\n+          \"Mutator free region before the leftmost: %zd, bound %zd\",\n@@ -1026,1 +1033,1 @@\n-          \"Mutator free regions past the rightmost: %zd, bound %zd\",\n+          \"Mutator free region past the rightmost: %zd, bound %zd\",\n@@ -1031,6 +1038,6 @@\n-  assert (beg_off >= leftmost_empty(ShenandoahFreeSetPartitionId::Mutator),\n-          \"Mutator free empty regions before the leftmost: %zd, bound %zd\",\n-          beg_off, leftmost_empty(ShenandoahFreeSetPartitionId::Mutator));\n-  assert (end_off <= rightmost_empty(ShenandoahFreeSetPartitionId::Mutator),\n-          \"Mutator free empty regions past the rightmost: %zd, bound %zd\",\n-          end_off, rightmost_empty(ShenandoahFreeSetPartitionId::Mutator));\n+  assert (beg_off >= _leftmosts_empty[int(ShenandoahFreeSetPartitionId::Mutator)],\n+          \"free empty region (%zd) before the leftmost bound %zd\",\n+          beg_off, _leftmosts_empty[int(ShenandoahFreeSetPartitionId::Mutator)]);\n+  assert (end_off <= _rightmosts_empty[int(ShenandoahFreeSetPartitionId::Mutator)],\n+          \"free empty region (%zd) past the rightmost bound %zd\",\n+          end_off, _rightmosts_empty[int(ShenandoahFreeSetPartitionId::Mutator)]);\n@@ -1056,1 +1063,1 @@\n-          \"Collector free regions before the leftmost: %zd, bound %zd\",\n+          \"Collector free region before the leftmost: %zd, bound %zd\",\n@@ -1059,1 +1066,1 @@\n-          \"Collector free regions past the rightmost: %zd, bound %zd\",\n+          \"Collector free region past the rightmost: %zd, bound %zd\",\n@@ -1065,2 +1072,2 @@\n-          \"Collector free empty regions before the leftmost: %zd, bound %zd\",\n-          beg_off, leftmost_empty(ShenandoahFreeSetPartitionId::Collector));\n+          \"Collector free empty region before the leftmost: %zd, bound %zd\",\n+          beg_off, _leftmosts_empty[int(ShenandoahFreeSetPartitionId::Collector)]);\n@@ -1068,2 +1075,2 @@\n-          \"Collector free empty regions past the rightmost: %zd, bound %zd\",\n-          end_off, rightmost_empty(ShenandoahFreeSetPartitionId::Collector));\n+          \"Collector free empty region past the rightmost: %zd, bound %zd\",\n+          end_off, _rightmosts_empty[int(ShenandoahFreeSetPartitionId::Collector)]);\n@@ -1086,0 +1093,2 @@\n+  \/\/ Concurrent recycling of trash recycles a region (changing its state from is_trash to is_empty without the heap lock),\n+\n@@ -1090,2 +1099,1 @@\n-  assert (beg_off >= leftmost(ShenandoahFreeSetPartitionId::OldCollector),\n-          \"OldCollector free regions before the leftmost: %zd, bound %zd\",\n+  assert (beg_off >= leftmost(ShenandoahFreeSetPartitionId::OldCollector), \"free regions before the leftmost: %zd, bound %zd\",\n@@ -1093,2 +1101,1 @@\n-  assert (end_off <= rightmost(ShenandoahFreeSetPartitionId::OldCollector),\n-          \"OldCollector free regions past the rightmost: %zd, bound %zd\",\n+  assert (end_off <= rightmost(ShenandoahFreeSetPartitionId::OldCollector), \"free regions past the rightmost: %zd, bound %zd\",\n@@ -1100,2 +1107,5 @@\n-          \"OldCollector free empty regions before the leftmost: %zd, bound %zd\",\n-          beg_off, leftmost_empty(ShenandoahFreeSetPartitionId::OldCollector));\n+          \"free empty region (%zd) before the leftmost bound %zd, region %s trash\",\n+          beg_off, _leftmosts_empty[int(ShenandoahFreeSetPartitionId::OldCollector)],\n+          ((beg_off >= _max)? \"out of bounds is not\":\n+           (ShenandoahHeap::heap()->get_region(_leftmosts_empty[int(ShenandoahFreeSetPartitionId::OldCollector)])->is_trash()?\n+            \"is\": \"is not\")));\n@@ -1103,83 +1113,83 @@\n-          \"OldCollector free empty regions past the rightmost: %zd, bound %zd\",\n-          end_off, rightmost_empty(ShenandoahFreeSetPartitionId::OldCollector));\n-\n-  if (validate_totals) {\n-    \/\/ young_retired_regions need to be added to either Mutator or Collector partitions, 100% used.\n-    \/\/ Give enough of young_retired_regions, young_retired_capacity, young_retired_user\n-    \/\/  to the Mutator partition to top it off so that it matches the running totals.\n-    \/\/\n-    \/\/ Give any remnants to the Collector partition.  After topping off the Collector partition, its values\n-    \/\/  should also match running totals.\n-\n-    assert(young_retired_regions * _region_size_bytes == young_retired_capacity, \"sanity\");\n-    assert(young_retired_capacity == young_retired_used, \"sanity\");\n-\n-\n-    assert(capacities[int(ShenandoahFreeSetPartitionId::OldCollector)]\n-           == _capacity[int(ShenandoahFreeSetPartitionId::OldCollector)], \"Old collector capacities must match\");\n-    assert(used[int(ShenandoahFreeSetPartitionId::OldCollector)]\n-           == _used[int(ShenandoahFreeSetPartitionId::OldCollector)], \"Old collector used must match\");\n-    assert(regions[int(ShenandoahFreeSetPartitionId::OldCollector)]\n-           == _capacity[int(ShenandoahFreeSetPartitionId::OldCollector)] \/ _region_size_bytes, \"Old collector regions must match\");\n-    assert(_capacity[int(ShenandoahFreeSetPartitionId::OldCollector)]\n-           >= _used[int(ShenandoahFreeSetPartitionId::OldCollector)], \"Old Collector capacity must be >= used\");\n-    assert(_available[int(ShenandoahFreeSetPartitionId::OldCollector)] ==\n-           (_capacity[int(ShenandoahFreeSetPartitionId::OldCollector)] - _used[int(ShenandoahFreeSetPartitionId::OldCollector)]),\n-           \"Old Collector available must equal capacity minus used\");\n-    assert(_humongous_waste[int(ShenandoahFreeSetPartitionId::OldCollector)] ==\n-           humongous_waste[int(ShenandoahFreeSetPartitionId::OldCollector)], \"Old Collector humongous waste must match\");\n-\n-    assert(_capacity[int(ShenandoahFreeSetPartitionId::Mutator)] >= capacities[int(ShenandoahFreeSetPartitionId::Mutator)],\n-           \"Capacity total must be >= counted tally\");\n-    size_t mutator_capacity_shortfall =\n-      _capacity[int(ShenandoahFreeSetPartitionId::Mutator)] - capacities[int(ShenandoahFreeSetPartitionId::Mutator)];\n-    assert(mutator_capacity_shortfall <= young_retired_capacity, \"sanity\");\n-    capacities[int(ShenandoahFreeSetPartitionId::Mutator)] += mutator_capacity_shortfall;\n-    young_retired_capacity -= mutator_capacity_shortfall;\n-    capacities[int(ShenandoahFreeSetPartitionId::Collector)] += young_retired_capacity;\n-\n-\n-    assert(_used[int(ShenandoahFreeSetPartitionId::Mutator)] >= used[int(ShenandoahFreeSetPartitionId::Mutator)],\n-           \"Used total must be >= counted tally\");\n-    size_t mutator_used_shortfall =\n-      _used[int(ShenandoahFreeSetPartitionId::Mutator)] - used[int(ShenandoahFreeSetPartitionId::Mutator)];\n-    assert(mutator_used_shortfall <= young_retired_used, \"sanity\");\n-    used[int(ShenandoahFreeSetPartitionId::Mutator)] += mutator_used_shortfall;\n-    young_retired_used -= mutator_used_shortfall;\n-    used[int(ShenandoahFreeSetPartitionId::Collector)] += young_retired_used;\n-\n-    assert(_capacity[int(ShenandoahFreeSetPartitionId::Mutator)] \/ _region_size_bytes\n-           >= regions[int(ShenandoahFreeSetPartitionId::Mutator)], \"Region total must be >= counted tally\");\n-    size_t mutator_regions_shortfall = (_capacity[int(ShenandoahFreeSetPartitionId::Mutator)] \/ _region_size_bytes\n-                                        - regions[int(ShenandoahFreeSetPartitionId::Mutator)]);\n-    assert(mutator_regions_shortfall <= young_retired_regions, \"sanity\");\n-    regions[int(ShenandoahFreeSetPartitionId::Mutator)] += mutator_regions_shortfall;\n-    young_retired_regions -= mutator_regions_shortfall;\n-    regions[int(ShenandoahFreeSetPartitionId::Collector)] += young_retired_regions;\n-\n-    assert(capacities[int(ShenandoahFreeSetPartitionId::Collector)] == _capacity[int(ShenandoahFreeSetPartitionId::Collector)],\n-           \"Collector capacities must match\");\n-    assert(used[int(ShenandoahFreeSetPartitionId::Collector)] == _used[int(ShenandoahFreeSetPartitionId::Collector)],\n-           \"Collector used must match\");\n-    assert(regions[int(ShenandoahFreeSetPartitionId::Collector)]\n-           == _capacity[int(ShenandoahFreeSetPartitionId::Collector)] \/ _region_size_bytes, \"Collector regions must match\");\n-    assert(_capacity[int(ShenandoahFreeSetPartitionId::Collector)] >= _used[int(ShenandoahFreeSetPartitionId::Collector)],\n-           \"Collector Capacity must be >= used\");\n-    assert(_available[int(ShenandoahFreeSetPartitionId::Collector)] ==\n-           (_capacity[int(ShenandoahFreeSetPartitionId::Collector)] - _used[int(ShenandoahFreeSetPartitionId::Collector)]),\n-           \"Collector Available must equal capacity minus used\");\n-\n-    assert(capacities[int(ShenandoahFreeSetPartitionId::Mutator)] == _capacity[int(ShenandoahFreeSetPartitionId::Mutator)],\n-           \"Mutator capacities must match\");\n-    assert(used[int(ShenandoahFreeSetPartitionId::Mutator)] == _used[int(ShenandoahFreeSetPartitionId::Mutator)],\n-           \"Mutator used must match\");\n-    assert(regions[int(ShenandoahFreeSetPartitionId::Mutator)]\n-           == _capacity[int(ShenandoahFreeSetPartitionId::Mutator)] \/ _region_size_bytes, \"Mutator regions must match\");\n-    assert(_capacity[int(ShenandoahFreeSetPartitionId::Mutator)] >= _used[int(ShenandoahFreeSetPartitionId::Mutator)],\n-           \"Mutator capacity must be >= used\");\n-    assert(_available[int(ShenandoahFreeSetPartitionId::Mutator)] ==\n-           (_capacity[int(ShenandoahFreeSetPartitionId::Mutator)] - _used[int(ShenandoahFreeSetPartitionId::Mutator)]),\n-           \"Mutator available must equal capacity minus used\");\n-    assert(_humongous_waste[int(ShenandoahFreeSetPartitionId::Mutator)] == young_humongous_waste,\n-           \"Mutator humongous waste must match\");\n-  }\n+          \"free empty region (%zd) past the rightmost bound %zd, region %s trash\",\n+          end_off, _rightmosts_empty[int(ShenandoahFreeSetPartitionId::OldCollector)],\n+          ((end_off < 0)? \"out of bounds is not\" :\n+           (ShenandoahHeap::heap()->get_region(_rightmosts_empty[int(ShenandoahFreeSetPartitionId::OldCollector)])->is_trash()?\n+            \"is\": \"is not\")));\n+\n+  \/\/ young_retired_regions need to be added to either Mutator or Collector partitions, 100% used.\n+  \/\/ Give enough of young_retired_regions, young_retired_capacity, young_retired_user\n+  \/\/  to the Mutator partition to top it off so that it matches the running totals.\n+  \/\/\n+  \/\/ Give any remnants to the Collector partition.  After topping off the Collector partition, its values\n+  \/\/  should also match running totals.\n+  assert(young_retired_regions * _region_size_bytes == young_retired_capacity, \"sanity\");\n+  assert(young_retired_capacity == young_retired_used, \"sanity\");\n+\n+  assert(capacities[int(ShenandoahFreeSetPartitionId::OldCollector)]\n+         == _capacity[int(ShenandoahFreeSetPartitionId::OldCollector)], \"Old collector capacities must match (%zu != %zu)\",\n+         capacities[int(ShenandoahFreeSetPartitionId::OldCollector)],\n+         _capacity[int(ShenandoahFreeSetPartitionId::OldCollector)]);\n+  assert(used[int(ShenandoahFreeSetPartitionId::OldCollector)]\n+         == _used[int(ShenandoahFreeSetPartitionId::OldCollector)], \"Old collector used must match\");\n+  assert(regions[int(ShenandoahFreeSetPartitionId::OldCollector)]\n+         == _capacity[int(ShenandoahFreeSetPartitionId::OldCollector)] \/ _region_size_bytes, \"Old collector regions must match\");\n+  assert(_capacity[int(ShenandoahFreeSetPartitionId::OldCollector)]\n+         >= _used[int(ShenandoahFreeSetPartitionId::OldCollector)], \"Old Collector capacity must be >= used\");\n+  assert(_available[int(ShenandoahFreeSetPartitionId::OldCollector)] ==\n+         (_capacity[int(ShenandoahFreeSetPartitionId::OldCollector)] - _used[int(ShenandoahFreeSetPartitionId::OldCollector)]),\n+         \"Old Collector available must equal capacity minus used\");\n+  assert(_humongous_waste[int(ShenandoahFreeSetPartitionId::OldCollector)] ==\n+         humongous_waste[int(ShenandoahFreeSetPartitionId::OldCollector)], \"Old Collector humongous waste must match\");\n+\n+  assert(_capacity[int(ShenandoahFreeSetPartitionId::Mutator)] >= capacities[int(ShenandoahFreeSetPartitionId::Mutator)],\n+         \"Capacity total must be >= counted tally\");\n+  size_t mutator_capacity_shortfall =\n+    _capacity[int(ShenandoahFreeSetPartitionId::Mutator)] - capacities[int(ShenandoahFreeSetPartitionId::Mutator)];\n+  assert(mutator_capacity_shortfall <= young_retired_capacity, \"sanity\");\n+  capacities[int(ShenandoahFreeSetPartitionId::Mutator)] += mutator_capacity_shortfall;\n+  young_retired_capacity -= mutator_capacity_shortfall;\n+  capacities[int(ShenandoahFreeSetPartitionId::Collector)] += young_retired_capacity;\n+\n+  assert(_used[int(ShenandoahFreeSetPartitionId::Mutator)] >= used[int(ShenandoahFreeSetPartitionId::Mutator)],\n+         \"Used total must be >= counted tally\");\n+  size_t mutator_used_shortfall =\n+    _used[int(ShenandoahFreeSetPartitionId::Mutator)] - used[int(ShenandoahFreeSetPartitionId::Mutator)];\n+  assert(mutator_used_shortfall <= young_retired_used, \"sanity\");\n+  used[int(ShenandoahFreeSetPartitionId::Mutator)] += mutator_used_shortfall;\n+  young_retired_used -= mutator_used_shortfall;\n+  used[int(ShenandoahFreeSetPartitionId::Collector)] += young_retired_used;\n+\n+  assert(_capacity[int(ShenandoahFreeSetPartitionId::Mutator)] \/ _region_size_bytes\n+         >= regions[int(ShenandoahFreeSetPartitionId::Mutator)], \"Region total must be >= counted tally\");\n+  size_t mutator_regions_shortfall = (_capacity[int(ShenandoahFreeSetPartitionId::Mutator)] \/ _region_size_bytes\n+                                      - regions[int(ShenandoahFreeSetPartitionId::Mutator)]);\n+  assert(mutator_regions_shortfall <= young_retired_regions, \"sanity\");\n+  regions[int(ShenandoahFreeSetPartitionId::Mutator)] += mutator_regions_shortfall;\n+  young_retired_regions -= mutator_regions_shortfall;\n+  regions[int(ShenandoahFreeSetPartitionId::Collector)] += young_retired_regions;\n+\n+  assert(capacities[int(ShenandoahFreeSetPartitionId::Collector)] == _capacity[int(ShenandoahFreeSetPartitionId::Collector)],\n+         \"Collector capacities must match\");\n+  assert(used[int(ShenandoahFreeSetPartitionId::Collector)] == _used[int(ShenandoahFreeSetPartitionId::Collector)],\n+         \"Collector used must match\");\n+  assert(regions[int(ShenandoahFreeSetPartitionId::Collector)]\n+         == _capacity[int(ShenandoahFreeSetPartitionId::Collector)] \/ _region_size_bytes, \"Collector regions must match\");\n+  assert(_capacity[int(ShenandoahFreeSetPartitionId::Collector)] >= _used[int(ShenandoahFreeSetPartitionId::Collector)],\n+         \"Collector Capacity must be >= used\");\n+  assert(_available[int(ShenandoahFreeSetPartitionId::Collector)] ==\n+         (_capacity[int(ShenandoahFreeSetPartitionId::Collector)] - _used[int(ShenandoahFreeSetPartitionId::Collector)]),\n+         \"Collector Available must equal capacity minus used\");\n+\n+  assert(capacities[int(ShenandoahFreeSetPartitionId::Mutator)] == _capacity[int(ShenandoahFreeSetPartitionId::Mutator)],\n+         \"Mutator capacities must match\");\n+  assert(used[int(ShenandoahFreeSetPartitionId::Mutator)] == _used[int(ShenandoahFreeSetPartitionId::Mutator)],\n+         \"Mutator used must match\");\n+  assert(regions[int(ShenandoahFreeSetPartitionId::Mutator)]\n+         == _capacity[int(ShenandoahFreeSetPartitionId::Mutator)] \/ _region_size_bytes, \"Mutator regions must match\");\n+  assert(_capacity[int(ShenandoahFreeSetPartitionId::Mutator)] >= _used[int(ShenandoahFreeSetPartitionId::Mutator)],\n+         \"Mutator capacity must be >= used\");\n+  assert(_available[int(ShenandoahFreeSetPartitionId::Mutator)] ==\n+         (_capacity[int(ShenandoahFreeSetPartitionId::Mutator)] - _used[int(ShenandoahFreeSetPartitionId::Mutator)]),\n+         \"Mutator available must equal capacity minus used\");\n+  assert(_humongous_waste[int(ShenandoahFreeSetPartitionId::Mutator)] == young_humongous_waste,\n+         \"Mutator humongous waste must match\");\n@@ -1209,0 +1219,30 @@\n+void ShenandoahFreeSet::move_unaffiliated_regions_from_collector_to_old_collector(ssize_t count) {\n+  shenandoah_assert_heaplocked();\n+  size_t region_size_bytes =  ShenandoahHeapRegion::region_size_bytes();\n+\n+  size_t old_capacity = _partitions.get_capacity(ShenandoahFreeSetPartitionId::OldCollector);\n+  size_t collector_capacity = _partitions.get_capacity(ShenandoahFreeSetPartitionId::Collector);\n+  if (count > 0) {\n+    size_t ucount = count;\n+    size_t bytes_moved = ucount * region_size_bytes;\n+    assert(collector_capacity >= bytes_moved, \"Cannot transfer\");\n+    assert(_partitions.get_empty_region_counts(ShenandoahFreeSetPartitionId::Collector) >= ucount,\n+           \"Cannot transfer %zu of %zu\", ucount, _partitions.get_empty_region_counts(ShenandoahFreeSetPartitionId::Collector));\n+    _partitions.decrease_empty_region_counts(ShenandoahFreeSetPartitionId::Collector, ucount);\n+    _partitions.set_capacity_of(ShenandoahFreeSetPartitionId::Collector, collector_capacity - bytes_moved);\n+    _partitions.set_capacity_of(ShenandoahFreeSetPartitionId::OldCollector, old_capacity + bytes_moved);\n+    _partitions.increase_empty_region_counts(ShenandoahFreeSetPartitionId::OldCollector, ucount);\n+  } else if (count < 0) {\n+    size_t ucount = -count;\n+    size_t bytes_moved = ucount * region_size_bytes;\n+    assert(old_capacity >= bytes_moved, \"Cannot transfer\");\n+    assert(_partitions.get_empty_region_counts(ShenandoahFreeSetPartitionId::OldCollector) >= ucount,\n+           \"Cannot transfer %zu of %zu\", ucount, _partitions.get_empty_region_counts(ShenandoahFreeSetPartitionId::OldCollector));\n+    _partitions.decrease_empty_region_counts(ShenandoahFreeSetPartitionId::OldCollector, ucount);\n+    _partitions.set_capacity_of(ShenandoahFreeSetPartitionId::OldCollector, old_capacity - bytes_moved);\n+    _partitions.set_capacity_of(ShenandoahFreeSetPartitionId::Collector, collector_capacity + bytes_moved);\n+    _partitions.increase_empty_region_counts(ShenandoahFreeSetPartitionId::Collector, ucount);\n+  }\n+  \/\/ else, do nothing\n+}\n+\n@@ -1264,1 +1304,1 @@\n-  _partitions.assert_bounds(true);\n+  _partitions.assert_bounds();\n@@ -1499,0 +1539,4 @@\n+  \/\/ We must call try_recycle_under_lock() even if !r->is_trash().  The reason is that if r is being recycled at this\n+  \/\/ moment by a GC worker thread, it may appear to be not trash even though it has not yet been fully recycled.  If\n+  \/\/ we proceed without waiting for the worker to finish recycling the region, the worker thread may overwrite the\n+  \/\/ region's affiliation with FREE after we set the region's affiliation to req.afiliation() below\n@@ -1501,1 +1545,0 @@\n-\n@@ -1671,1 +1714,1 @@\n-  _partitions.assert_bounds(true);\n+  _partitions.assert_bounds();\n@@ -1802,0 +1845,1 @@\n+\n@@ -1822,1 +1866,1 @@\n-  _partitions.assert_bounds(true);\n+  _partitions.assert_bounds();\n@@ -1829,1 +1873,3 @@\n-    r->try_recycle();\n+    if (r->is_trash()) {\n+      r->try_recycle();\n+    }\n@@ -1864,1 +1910,1 @@\n-    _partitions.assert_bounds(true);\n+    _partitions.assert_bounds();\n@@ -1917,1 +1963,1 @@\n-      _partitions.assert_bounds(true);\n+      _partitions.assert_bounds();\n@@ -1948,1 +1994,1 @@\n-  _partitions.assert_bounds(true);\n+  _partitions.assert_bounds();\n@@ -2028,2 +2074,2 @@\n-      \/\/ Trashed regions represent immediate garbage identified by final mark and regions that had been in the collection\n-      \/\/ partition but have not yet been \"cleaned up\" following update refs.\n+      \/\/ Trashed regions represent regions that had been in the collection set (or may have been identified as immediate garbage)\n+      \/\/ but have not yet been \"cleaned up\".  The cset regions are not \"trashed\" until we have finished update refs.\n@@ -2031,0 +2077,6 @@\n+        \/\/ We're going to place this region into the Mutator set.  We increment old_trashed_regions because this count represents\n+        \/\/ regions that the old generation is entitled to without any transfer from young.  We do not place this region into\n+        \/\/ the OldCollector partition at this time.  Instead, we let reserve_regions() decide whether to place this region\n+        \/\/ into the OldCollector partition.  Deferring the decision allows reserve_regions() to more effectively pack the\n+        \/\/ OldCollector regions into high-address memory.  We do not adjust capacities of old and young generations at this\n+        \/\/ time.  At the end of finish_rebuild(), the capacities are adjusted based on the results of reserve_regions().\n@@ -2037,1 +2089,2 @@\n-      \/\/ count both humongous and regular regions, but don't count trash (cset) regions.\n+      \/\/ We count humongous and regular regions as \"old regions\".  We do not count trashed regions that are old.  Those\n+      \/\/ are counted (above) as old_trashed_regions.\n@@ -2051,1 +2104,1 @@\n-          \/\/ Both young and old collected regions (trashed) are placed into the Mutator set\n+          \/\/ Both young and old (possibly immediately) collected regions (trashed) are placed into the Mutator set\n@@ -2114,4 +2167,13 @@\n-          oop obj = cast_to_oop(region->bottom());\n-          size_t byte_size = obj->size() * HeapWordSize;\n-          size_t region_span = ShenandoahHeapRegion::required_regions(byte_size);\n-          humongous_waste_bytes = region_span * ShenandoahHeapRegion::region_size_bytes() - byte_size;\n+          \/\/ Since rebuild does not necessarily happen at a safepoint, a newly allocated humongous object may not have been\n+          \/\/ fully initialized.  Therefore, we cannot safely consult its header.\n+          ShenandoahHeapRegion* last_of_humongous_continuation = region;\n+          size_t next_idx;\n+          for (next_idx = idx + 1; next_idx < num_regions; next_idx++) {\n+            ShenandoahHeapRegion* humongous_cont_candidate = _heap->get_region(next_idx);\n+            if (!humongous_cont_candidate->is_humongous_continuation()) {\n+              break;\n+            }\n+            last_of_humongous_continuation = humongous_cont_candidate;\n+          }\n+          \/\/ For humongous regions, used() is established while holding the global heap lock so it is reliable here\n+          humongous_waste_bytes = ShenandoahHeapRegion::region_size_bytes() - last_of_humongous_continuation->used();\n@@ -2186,1 +2248,1 @@\n-  _partitions.assert_bounds(true);\n+  _partitions.assert_bounds();\n@@ -2224,1 +2286,1 @@\n-  _partitions.assert_bounds(true);\n+  _partitions.assert_bounds();\n@@ -2306,1 +2368,1 @@\n-  _partitions.assert_bounds(true);\n+  _partitions.assert_bounds();\n@@ -2373,1 +2435,1 @@\n-  _partitions.assert_bounds(true);\n+  _partitions.assert_bounds();\n@@ -2448,1 +2510,1 @@\n-  _partitions.assert_bounds(true);\n+  _partitions.assert_bounds();\n@@ -2510,2 +2572,2 @@\n-void ShenandoahFreeSet::finish_rebuild(size_t young_trashed_regions, size_t old_trashed_regions, size_t old_region_count,\n-                                       bool have_evacuation_reserves) {\n+\n+void ShenandoahFreeSet::finish_rebuild(size_t young_cset_regions, size_t old_cset_regions, size_t old_region_count) {\n@@ -2516,2 +2578,1 @@\n-    compute_young_and_old_reserves(young_trashed_regions, old_trashed_regions, have_evacuation_reserves,\n-                                   young_reserve, old_reserve);\n+    compute_young_and_old_reserves(young_cset_regions, old_cset_regions, young_reserve, old_reserve);\n@@ -2534,1 +2595,1 @@\n-  _partitions.assert_bounds(true);\n+  _partitions.assert_bounds();\n@@ -2536,0 +2597,33 @@\n+  if (_heap->mode()->is_generational()) {\n+    \/\/ Clear the region balance until it is adjusted in preparation for a subsequent GC cycle.\n+    _heap->old_generation()->set_region_balance(0);\n+  }\n+}\n+\n+\n+\/\/ Reduce old reserve (when there are insufficient resources to satisfy the original request).\n+void ShenandoahFreeSet::reduce_old_reserve(size_t adjusted_old_reserve, size_t requested_old_reserve) {\n+  ShenandoahOldGeneration* const old_generation = _heap->old_generation();\n+  size_t requested_promoted_reserve = old_generation->get_promoted_reserve();\n+  size_t requested_old_evac_reserve = old_generation->get_evacuation_reserve();\n+  assert(adjusted_old_reserve < requested_old_reserve, \"Only allow reduction\");\n+  assert(requested_promoted_reserve + requested_old_evac_reserve >= adjusted_old_reserve, \"Sanity\");\n+  size_t delta = requested_old_reserve - adjusted_old_reserve;\n+\n+  if (requested_promoted_reserve >= delta) {\n+    requested_promoted_reserve -= delta;\n+    old_generation->set_promoted_reserve(requested_promoted_reserve);\n+  } else {\n+    delta -= requested_promoted_reserve;\n+    requested_promoted_reserve = 0;\n+    requested_old_evac_reserve -= delta;\n+    old_generation->set_promoted_reserve(requested_promoted_reserve);\n+    old_generation->set_evacuation_reserve(requested_old_evac_reserve);\n+  }\n+}\n+\n+\/\/ Reduce young reserve (when there are insufficient resources to satisfy the original request).\n+void ShenandoahFreeSet::reduce_young_reserve(size_t adjusted_young_reserve, size_t requested_young_reserve) {\n+  ShenandoahYoungGeneration* const young_generation = _heap->young_generation();\n+  assert(adjusted_young_reserve < requested_young_reserve, \"Only allow reduction\");\n+  young_generation->set_evacuation_reserve(adjusted_young_reserve);\n@@ -2552,1 +2646,0 @@\n-                                                       bool have_evacuation_reserves,\n@@ -2569,0 +2662,9 @@\n+  assert(young_capacity >= young_generation->used(),\n+         \"Young capacity (%zu) must exceed used (%zu)\", young_capacity, young_generation->used());\n+\n+  size_t young_available = young_capacity - young_generation->used();\n+  young_available += young_trashed_regions * region_size_bytes;\n+\n+  assert(young_available >= young_unaffiliated_regions * region_size_bytes, \"sanity\");\n+  assert(old_available >= old_unaffiliated_regions * region_size_bytes, \"sanity\");\n+\n@@ -2588,0 +2690,1 @@\n+    young_available += xfer_bytes;\n@@ -2596,26 +2699,7 @@\n-  if (have_evacuation_reserves) {\n-    \/\/ We are rebuilding at the end of final mark, having already established evacuation budgets for this GC pass.\n-    const size_t promoted_reserve = old_generation->get_promoted_reserve();\n-    const size_t old_evac_reserve = old_generation->get_evacuation_reserve();\n-    young_reserve_result = young_generation->get_evacuation_reserve();\n-    old_reserve_result = promoted_reserve + old_evac_reserve;\n-    if (old_reserve_result > old_available) {\n-      \/\/ Try to transfer memory from young to old.\n-      size_t old_deficit = old_reserve_result - old_available;\n-      size_t old_region_deficit = (old_deficit + region_size_bytes - 1) \/ region_size_bytes;\n-      if (young_unaffiliated_regions < old_region_deficit) {\n-        old_region_deficit = young_unaffiliated_regions;\n-      }\n-      young_unaffiliated_regions -= old_region_deficit;\n-      old_unaffiliated_regions += old_region_deficit;\n-      old_region_balance -= old_region_deficit;\n-      old_generation->set_region_balance(old_region_balance);\n-    }\n-  } else {\n-    \/\/ We are rebuilding at end of GC, so we set aside budgets specified on command line (or defaults)\n-    young_reserve_result = (young_capacity * ShenandoahEvacReserve) \/ 100;\n-    \/\/ The auto-sizer has already made old-gen large enough to hold all anticipated evacuations and promotions.\n-    \/\/ Affiliated old-gen regions are already in the OldCollector free set.  Add in the relevant number of\n-    \/\/ unaffiliated regions.\n-    old_reserve_result = old_available;\n-  }\n+  const size_t promoted_reserve = old_generation->get_promoted_reserve();\n+  const size_t old_evac_reserve = old_generation->get_evacuation_reserve();\n+  young_reserve_result = young_generation->get_evacuation_reserve();\n+  old_reserve_result = promoted_reserve + old_evac_reserve;\n+  assert(old_reserve_result + young_reserve_result <= old_available + young_available,\n+         \"Cannot reserve (%zu + %zu + %zu) more than is available: %zu + %zu\",\n+         promoted_reserve, old_evac_reserve, young_reserve_result, old_available, young_available);\n@@ -2628,1 +2712,1 @@\n-      _partitions.capacity_of(ShenandoahFreeSetPartitionId::OldCollector) + old_unaffiliated_regions * region_size_bytes) {\n+      _partitions.available_in(ShenandoahFreeSetPartitionId::OldCollector) + old_unaffiliated_regions * region_size_bytes) {\n@@ -2630,1 +2714,1 @@\n-      _partitions.capacity_of(ShenandoahFreeSetPartitionId::OldCollector) + old_unaffiliated_regions * region_size_bytes;\n+      _partitions.available_in(ShenandoahFreeSetPartitionId::OldCollector) + old_unaffiliated_regions * region_size_bytes;\n@@ -2794,13 +2878,11 @@\n-      if (p == ShenandoahFreeSetPartitionId::Collector) {\n-        if (ac != region_size_bytes) {\n-          young_used_regions++;\n-          young_used_bytes = region_size_bytes - ac;\n-        }\n-        \/\/ else, unaffiliated region has no used\n-      } else if (p == ShenandoahFreeSetPartitionId::OldCollector) {\n-        if (ac != region_size_bytes) {\n-          old_used_regions++;\n-          old_used_bytes = region_size_bytes - ac;\n-        }\n-        \/\/ else, unaffiliated region has no used\n-      } else if (p == ShenandoahFreeSetPartitionId::NotFree) {\n+      assert(p != ShenandoahFreeSetPartitionId::Collector, \"Collector regions must be converted from Mutator regions\");\n+      if (p == ShenandoahFreeSetPartitionId::OldCollector) {\n+        assert(!r->is_empty(), \"Empty regions should be in Mutator partition at entry to reserve_regions\");\n+        old_used_regions++;\n+        old_used_bytes = region_size_bytes - ac;\n+        \/\/ This region is within the range for OldCollector partition, as established by find_regions_with_alloc_capacity()\n+        assert((_partitions.leftmost(ShenandoahFreeSetPartitionId::OldCollector) <= idx) &&\n+               (_partitions.rightmost(ShenandoahFreeSetPartitionId::OldCollector) >= idx),\n+               \"find_regions_with_alloc_capacity() should have established this is in range\");\n+      } else {\n+        assert(p == ShenandoahFreeSetPartitionId::NotFree, \"sanity\");\n@@ -2816,15 +2898,0 @@\n-      } else {\n-        assert(p == ShenandoahFreeSetPartitionId::OldCollector, \"Not mutator and not NotFree, so must be OldCollector\");\n-        assert(!r->is_empty(), \"Empty regions should be in Mutator partition at entry to reserve_regions\");\n-        if (idx < old_collector_low_idx) {\n-          old_collector_low_idx = idx;\n-        }\n-        if (idx > old_collector_high_idx) {\n-          old_collector_high_idx = idx;\n-        }\n-        if (idx < old_collector_empty_low_idx) {\n-          old_collector_empty_low_idx = idx;\n-        }\n-        if (idx > old_collector_empty_high_idx) {\n-          old_collector_empty_high_idx = idx;\n-        }\n@@ -2859,3 +2926,5 @@\n-  _partitions.expand_interval_if_range_modifies_either_boundary(ShenandoahFreeSetPartitionId::Collector,\n-                                                                collector_low_idx, collector_high_idx,\n-                                                                collector_empty_low_idx, collector_empty_high_idx);\n+  _partitions.establish_interval(ShenandoahFreeSetPartitionId::Mutator,\n+                                 mutator_low_idx, mutator_high_idx, mutator_empty_low_idx, mutator_empty_high_idx);\n+  _partitions.establish_interval(ShenandoahFreeSetPartitionId::Collector,\n+                                 collector_low_idx, collector_high_idx, collector_empty_low_idx, collector_empty_high_idx);\n+\n@@ -2865,2 +2934,0 @@\n-  _partitions.establish_interval(ShenandoahFreeSetPartitionId::Mutator,\n-                                 mutator_low_idx, mutator_high_idx, mutator_empty_low_idx, mutator_empty_high_idx);\n@@ -2875,1 +2942,1 @@\n-  _partitions.assert_bounds(true);\n+  _partitions.assert_bounds();\n@@ -2881,0 +2948,2 @@\n+      assert(_heap->mode()->is_generational(), \"to_old_reserve > 0 implies generational mode\");\n+      reduce_old_reserve(old_reserve, to_reserve_old);\n@@ -2884,0 +2953,3 @@\n+      if (_heap->mode()->is_generational()) {\n+        reduce_young_reserve(reserve, to_reserve);\n+      }\n@@ -2885,1 +2957,1 @@\n-                          PROPERFMTARGS(to_reserve), PROPERFMTARGS(reserve));\n+                         PROPERFMTARGS(to_reserve), PROPERFMTARGS(reserve));\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahFreeSet.cpp","additions":269,"deletions":197,"binary":false,"changes":466,"status":"modified"},{"patch":"@@ -227,0 +227,4 @@\n+  \/\/ For recycled region r in the OldCollector partition but possibly not within the interval for empty OldCollector regions,\n+  \/\/ expand the empty interval to include this region.\n+  inline void adjust_interval_for_recycled_old_region_under_lock(ShenandoahHeapRegion* r);\n+\n@@ -376,6 +380,1 @@\n-  inline void set_used_by(ShenandoahFreeSetPartitionId which_partition, size_t value) {\n-    shenandoah_assert_heaplocked();\n-    assert (which_partition < NumPartitions, \"selected free set must be valid\");\n-    _used[int(which_partition)] = value;\n-    _available[int(which_partition)] = _capacity[int(which_partition)] - value;\n-  }\n+  inline void set_used_by(ShenandoahFreeSetPartitionId which_partition, size_t value);\n@@ -405,1 +404,1 @@\n-  void assert_bounds(bool validate_totals) NOT_DEBUG_RETURN;\n+  void assert_bounds() NOT_DEBUG_RETURN;\n@@ -637,0 +636,3 @@\n+  void reduce_young_reserve(size_t adjusted_young_reserve, size_t requested_young_reserve);\n+  void reduce_old_reserve(size_t adjusted_old_reserve, size_t requested_old_reserve);\n+\n@@ -638,0 +640,1 @@\n+\n@@ -688,1 +691,4 @@\n-  size_t global_unaffiliated_regions() {\n+  \/\/ A negative argument results in moving from old_collector to collector\n+  void move_unaffiliated_regions_from_collector_to_old_collector(ssize_t regions);\n+\n+  inline size_t global_unaffiliated_regions() {\n@@ -692,1 +698,1 @@\n-  size_t young_unaffiliated_regions() {\n+  inline size_t young_unaffiliated_regions() {\n@@ -696,1 +702,5 @@\n-  size_t old_unaffiliated_regions() {\n+  inline size_t collector_unaffiliated_regions() {\n+    return _partitions.get_empty_region_counts(ShenandoahFreeSetPartitionId::Collector);\n+  }\n+\n+  inline size_t old_collector_unaffiliated_regions() {\n@@ -700,1 +710,5 @@\n-  size_t young_affiliated_regions() {\n+  inline size_t old_unaffiliated_regions() {\n+    return _partitions.get_empty_region_counts(ShenandoahFreeSetPartitionId::OldCollector);\n+  }\n+\n+  inline size_t young_affiliated_regions() {\n@@ -704,1 +718,1 @@\n-  size_t old_affiliated_regions() {\n+  inline size_t old_affiliated_regions() {\n@@ -708,1 +722,1 @@\n-  size_t global_affiliated_regions() {\n+  inline size_t global_affiliated_regions() {\n@@ -712,1 +726,1 @@\n-  size_t total_young_regions() {\n+  inline size_t total_young_regions() {\n@@ -716,1 +730,1 @@\n-  size_t total_old_regions() {\n+  inline size_t total_old_regions() {\n@@ -728,2 +742,3 @@\n-  \/\/ young_cset_regions is the number of regions currently in the young cset if we are starting to evacuate, or zero\n-  \/\/   old_cset_regions is the number of regions currently in the old cset if we are starting a mixed evacuation, or zero\n+  \/\/ young_trashed_regions is the number of trashed regions (immediate garbage at final mark, cset regions after update refs)\n+  \/\/   old_trashed_regions is the number of trashed regions\n+  \/\/                       (immediate garbage at final old mark, cset regions after update refs for mixed evac)\n@@ -733,1 +748,1 @@\n-  void prepare_to_rebuild(size_t &young_cset_regions, size_t &old_cset_regions,\n+  void prepare_to_rebuild(size_t &young_trashed_regions, size_t &old_trashed_regions,\n@@ -737,4 +752,5 @@\n-  \/\/ hold the results of evacuating to young-gen and to old-gen, and have_evacuation_reserves should be true.\n-  \/\/ These quantities, stored as reserves for their respective generations, are consulted prior to rebuilding\n-  \/\/ the free set (ShenandoahFreeSet) in preparation for evacuation.  When the free set is rebuilt, we make sure\n-  \/\/ to reserve sufficient memory in the collector and old_collector sets to hold evacuations.\n+  \/\/ hold the results of evacuating to young-gen and to old-gen.  These quantities, stored in reserves for their\n+  \/\/ respective generations, are consulted prior to rebuilding the free set (ShenandoahFreeSet) in preparation for\n+  \/\/ evacuation.  When the free set is rebuilt, we make sure to reserve sufficient memory in the collector and\n+  \/\/ old_collector sets to hold evacuations.  Likewise, at the end of update refs, we rebuild the free set in order\n+  \/\/ to set aside reserves to be consumed during the next GC cycle.\n@@ -742,10 +758,3 @@\n-  \/\/ We also rebuild the free set at the end of GC, as we prepare to idle GC until the next trigger.  In this case,\n-  \/\/ have_evacuation_reserves is false because we don't yet know how much memory will need to be evacuated in the\n-  \/\/ next GC cycle.  When have_evacuation_reserves is false, the free set rebuild operation reserves for the collector\n-  \/\/ and old_collector sets based on alternative mechanisms, such as ShenandoahEvacReserve, ShenandoahOldEvacReserve, and\n-  \/\/ ShenandoahOldCompactionReserve.  In a future planned enhancement, the reserve for old_collector set when the\n-  \/\/ evacuation reserves are unknown, is based in part on anticipated promotion as determined by analysis of live data\n-  \/\/ found during the previous GC pass which is one less than the current tenure age.\n-  \/\/\n-  \/\/ young_cset_regions is the number of regions currently in the young cset if we are starting to evacuate, or zero\n-  \/\/   old_cset_regions is the number of regions currently in the old cset if we are starting a mixed evacuation, or zero\n+  \/\/ young_trashed_regions is the number of trashed regions (immediate garbage at final mark, cset regions after update refs)\n+  \/\/   old_trashed_regions is the number of trashed regions\n+  \/\/                       (immediate garbage at final old mark, cset regions after update refs for mixed evac)\n@@ -753,5 +762,1 @@\n-  \/\/ have_evacuation_reserves is true iff the desired values of young-gen and old-gen evacuation reserves and old-gen\n-  \/\/                    promotion reserve have been precomputed (and can be obtained by invoking\n-  \/\/                    <generation>->get_evacuation_reserve() or old_gen->get_promoted_reserve()\n-  void finish_rebuild(size_t young_cset_regions, size_t old_cset_regions, size_t num_old_regions,\n-                      bool have_evacuation_reserves = false);\n+  void finish_rebuild(size_t young_trashed_regions, size_t old_trashed_regions, size_t num_old_regions);\n@@ -809,0 +814,5 @@\n+  \/\/ Use this version of available() if the heap lock is held.\n+  inline size_t available_locked() const {\n+    return _partitions.available_in(ShenandoahFreeSetPartitionId::Mutator);\n+  }\n+\n@@ -810,2 +820,6 @@\n-  inline size_t humongous_waste_in_mutator() const { return _partitions.humongous_waste(ShenandoahFreeSetPartitionId::Mutator); }\n-  inline size_t humongous_waste_in_old() const { return _partitions.humongous_waste(ShenandoahFreeSetPartitionId::OldCollector); }\n+  inline size_t humongous_waste_in_mutator() const {\n+    return _partitions.humongous_waste(ShenandoahFreeSetPartitionId::Mutator);\n+  }\n+  inline size_t humongous_waste_in_old() const {\n+    return _partitions.humongous_waste(ShenandoahFreeSetPartitionId::OldCollector);\n+  }\n@@ -877,1 +891,1 @@\n-  void compute_young_and_old_reserves(size_t young_cset_regions, size_t old_cset_regions, bool have_evacuation_reserves,\n+  void compute_young_and_old_reserves(size_t young_cset_regions, size_t old_cset_regions,\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahFreeSet.hpp","additions":54,"deletions":40,"binary":false,"changes":94,"status":"modified"},{"patch":"@@ -525,0 +525,1 @@\n+      \/\/ No need to adjust_interval_for_recycled_old_region.  That will be taken care of during freeset rebuild.\n@@ -969,0 +970,1 @@\n+      \/\/ No need to adjust_interval_for_recycled_old_region.  That will be taken care of during freeset rebuild.\n@@ -1116,2 +1118,0 @@\n-    size_t young_cset_regions, old_cset_regions, first_old, last_old, num_old;\n-    ShenandoahFreeSet* free_set = heap->free_set();\n@@ -1119,1 +1119,3 @@\n-      free_set->prepare_to_rebuild(young_cset_regions, old_cset_regions, first_old, last_old, num_old);\n+      ShenandoahFreeSet* free_set = heap->free_set();\n+      size_t young_trashed_regions, old_trashed_regions, first_old, last_old, num_old;\n+      free_set->prepare_to_rebuild(young_trashed_regions, old_trashed_regions, first_old, last_old, num_old);\n@@ -1125,1 +1127,1 @@\n-      free_set->finish_rebuild(young_cset_regions, old_cset_regions, num_old);\n+      free_set->finish_rebuild(young_trashed_regions, old_trashed_regions, num_old);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahFullGC.cpp","additions":6,"deletions":4,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -253,0 +253,1 @@\n+  const size_t region_size_bytes = ShenandoahHeapRegion::region_size_bytes();\n@@ -266,3 +267,3 @@\n-  \/\/ maximum_young_evacuation_reserve is upper bound on memory to be evacuated out of young\n-  const size_t maximum_young_evacuation_reserve = (young_generation->max_capacity() * ShenandoahEvacReserve) \/ 100;\n-  size_t young_evacuation_reserve = MIN2(maximum_young_evacuation_reserve, young_generation->available_with_reserve());\n+  \/\/ maximum_young_evacuation_reserve is upper bound on memory to be evacuated into young Collector Reserve.  This is\n+  \/\/ bounded at the end of previous GC cycle, based on available memory and balancing of evacuation to old and young.\n+  size_t maximum_young_evacuation_reserve = young_generation->get_evacuation_reserve();\n@@ -274,1 +275,1 @@\n-  \/\/ Let SOEP = ShenandoahOldEvacRatioPercent,\n+  \/\/ Let SOEP = ShenandoahOldEvacPercent,\n@@ -286,1 +287,1 @@\n-  assert(ShenandoahOldEvacRatioPercent <= 100, \"Error\");\n+  assert(ShenandoahOldEvacPercent <= 100, \"Error\");\n@@ -288,2 +289,2 @@\n-  const size_t maximum_old_evacuation_reserve = (ShenandoahOldEvacRatioPercent == 100) ?\n-    old_available : MIN2((maximum_young_evacuation_reserve * ShenandoahOldEvacRatioPercent) \/ (100 - ShenandoahOldEvacRatioPercent),\n+  const size_t maximum_old_evacuation_reserve = (ShenandoahOldEvacPercent == 100) ?\n+    old_available : MIN2((maximum_young_evacuation_reserve * ShenandoahOldEvacPercent) \/ (100 - ShenandoahOldEvacPercent),\n@@ -292,0 +293,2 @@\n+  \/\/ In some cases, maximum_old_reserve < old_available (when limited by ShenandoahOldEvacPercent)\n+  \/\/ This limit affects mixed evacuations, but does not affect promotions.\n@@ -308,4 +311,2 @@\n-    \/\/ Set old_promo_reserve to enforce that no regions are preselected for promotion.  Such regions typically\n-    \/\/ have relatively high memory utilization.  We still call select_aged_regions() because this will prepare for\n-    \/\/ promotions in place, if relevant.\n-    old_promo_reserve = 0;\n+    \/\/ Use remnant of old_available to hold promotions.\n+    old_promo_reserve = old_available - maximum_old_evacuation_reserve;\n@@ -322,1 +323,1 @@\n-    old_promo_reserve = 0;\n+    old_promo_reserve = old_available - maximum_old_evacuation_reserve;\n@@ -325,1 +326,1 @@\n-    old_evacuation_reserve = 0;\n+    old_evacuation_reserve = old_available - maximum_old_evacuation_reserve;\n@@ -330,0 +331,1 @@\n+\n@@ -333,1 +335,1 @@\n-  const size_t old_free_unfragmented = old_generation->free_unaffiliated_regions() * ShenandoahHeapRegion::region_size_bytes();\n+  const size_t old_free_unfragmented = old_generation->free_unaffiliated_regions() * region_size_bytes;\n@@ -337,4 +339,2 @@\n-    \/\/ Let promo consume fragments of old-gen memory if not global\n-    if (!is_global()) {\n-      old_promo_reserve += delta;\n-    }\n+    \/\/ Let promo consume fragments of old-gen memory\n+    old_promo_reserve += delta;\n@@ -343,4 +343,9 @@\n-  \/\/ Preselect regions for promotion by evacuation (obtaining the live data to seed promoted_reserve),\n-  \/\/ and identify regions that will promote in place. These use the tenuring threshold.\n-  const size_t consumed_by_advance_promotion = select_aged_regions(old_promo_reserve);\n-  assert(consumed_by_advance_promotion <= maximum_old_evacuation_reserve, \"Cannot promote more than available old-gen memory\");\n+  \/\/ If is_global(), we let garbage-first heuristic determine cset membership.  Otherwise, we give priority\n+  \/\/ to tenurable regions by preselecting regions for promotion by evacuation (obtaining the live data to seed promoted_reserve).\n+  \/\/ This also identifies regions that will be promoted in place. These use the tenuring threshold.\n+  const size_t consumed_by_advance_promotion = select_aged_regions(is_global()? 0: old_promo_reserve);\n+  assert(consumed_by_advance_promotion <= old_promo_reserve, \"Do not promote more than budgeted\");\n+\n+  \/\/ The young evacuation reserve can be no larger than young_unaffiliated.  Planning to evacuate into partially consumed\n+  \/\/ young regions is doomed to failure if any of those partially consumed regions is selected for the collection set.\n+  size_t young_unaffiliated = young_generation->free_unaffiliated_regions() * region_size_bytes;\n@@ -351,1 +356,1 @@\n-  young_evacuation_reserve = MIN2(young_evacuation_reserve, young_generation->available_with_reserve());\n+  size_t young_evacuation_reserve = MIN2(maximum_young_evacuation_reserve, young_unaffiliated);\n@@ -355,1 +360,2 @@\n-  \/\/ of old evacuation failure.\n+  \/\/ of old evacuation failure.  Leave this memory in the promoted reserve as it may be targeted by opportunistic\n+  \/\/ promotions (found during evacuation of young regions).\n@@ -358,1 +364,1 @@\n-  old_generation->set_promoted_reserve(consumed_by_advance_promotion);\n+  old_generation->set_promoted_reserve(old_promo_reserve);\n@@ -366,2 +372,2 @@\n-\/\/\n-void ShenandoahGeneration::adjust_evacuation_budgets(ShenandoahHeap* const heap, ShenandoahCollectionSet* const collection_set) {\n+void ShenandoahGeneration::adjust_evacuation_budgets(ShenandoahHeap* const heap,\n+                                                     ShenandoahCollectionSet* const collection_set, size_t add_regions_to_old) {\n@@ -401,1 +407,2 @@\n-    log_debug(gc, cset)(\"Shrinking old evac reserve to match old_evac_commited: \" PROPERFMT, PROPERFMTARGS(old_evacuated_committed));\n+    log_debug(gc, cset)(\"Shrinking old evac reserve to match old_evac_commited: \" PROPERFMT,\n+                        PROPERFMTARGS(old_evacuated_committed));\n@@ -412,2 +419,3 @@\n-  size_t total_young_available = young_generation->available_with_reserve();\n-  assert(young_evacuated_reserve_used <= total_young_available, \"Cannot evacuate more than is available in young\");\n+  size_t total_young_available = young_generation->available_with_reserve() - add_regions_to_old * region_size_bytes;;\n+  assert(young_evacuated_reserve_used <= total_young_available, \"Cannot evacuate (%zu) more than is available in young (%zu)\",\n+         young_evacuated_reserve_used, total_young_available);\n@@ -416,1 +424,6 @@\n-  size_t old_available = old_generation->available();\n+  \/\/ We have not yet rebuilt the free set.  Some of the memory that is thought to be avaiable within old may no\n+  \/\/ longer be available if that memory had been free within regions that were selected for the collection set.\n+  \/\/ Make the necessary adjustments to old_available.\n+  size_t old_available =\n+    old_generation->available() + add_regions_to_old * region_size_bytes - collection_set->get_old_available_bytes_collected();\n+\n@@ -425,0 +438,1 @@\n+\n@@ -428,1 +442,7 @@\n-    young_advance_promoted_reserve_used = old_available - old_evacuated_committed;\n+    if (old_available > old_evacuated_committed) {\n+      young_advance_promoted_reserve_used = old_available - old_evacuated_committed;\n+    } else {\n+      young_advance_promoted_reserve_used = 0;\n+      old_evacuated_committed = old_available;\n+    }\n+    \/\/ TODO: reserve for full promotion reserve, not just for advance (preselected) promotion\n@@ -435,1 +455,1 @@\n-  size_t unaffiliated_old_regions = old_generation->free_unaffiliated_regions();\n+  size_t unaffiliated_old_regions = old_generation->free_unaffiliated_regions() + add_regions_to_old;\n@@ -437,3 +457,2 @@\n-  assert(old_available >= unaffiliated_old,\n-         \"Unaffiliated old (%zu is %zu * %zu) is a subset of old available (%zu)\",\n-         unaffiliated_old, unaffiliated_old_regions, region_size_bytes, old_available);\n+  assert(unaffiliated_old >= old_evacuated_committed, \"Do not evacuate (%zu) more than unaffiliated old (%zu)\",\n+         old_evacuated_committed, unaffiliated_old);\n@@ -457,2 +476,2 @@\n-  \/\/ runway during evacuation and update-refs.\n-  size_t regions_to_xfer = 0;\n+  \/\/ runway during evacuation and update-refs.  We may make further adjustments to balance.\n+  ssize_t add_regions_to_young = 0;\n@@ -462,1 +481,1 @@\n-      regions_to_xfer = unaffiliated_old_regions;\n+      add_regions_to_young = unaffiliated_old_regions;\n@@ -467,1 +486,1 @@\n-    regions_to_xfer = MIN2(excess_regions, unaffiliated_old_regions);\n+    add_regions_to_young = MIN2(excess_regions, unaffiliated_old_regions);\n@@ -469,2 +488,4 @@\n-  if (regions_to_xfer > 0) {\n-    excess_old -= regions_to_xfer * region_size_bytes;\n+\n+  if (add_regions_to_young > 0) {\n+    assert(excess_old >= add_regions_to_young * region_size_bytes, \"Cannot xfer more than excess old\");\n+    excess_old -= add_regions_to_young * region_size_bytes;\n@@ -478,0 +499,1 @@\n+\n@@ -785,1 +807,1 @@\n-      \/\/ place, and preselect older regions that will be promoted by evacuation.\n+      \/\/ place, and preselected older regions that will be promoted by evacuation.\n@@ -788,8 +810,4 @@\n-      \/\/ Choose the collection set, including the regions preselected above for\n-      \/\/ promotion into the old generation.\n-      _heuristics->choose_collection_set(collection_set);\n-      if (!collection_set->is_empty()) {\n-        \/\/ only make use of evacuation budgets when we are evacuating\n-        adjust_evacuation_budgets(heap, collection_set);\n-      }\n-\n+      \/\/ Choose the collection set, including the regions preselected above for promotion into the old generation.\n+      size_t add_regions_to_old = _heuristics->choose_collection_set(collection_set);\n+      \/\/ Even if collection_set->is_empty(), we want to adjust budgets, making reserves available to mutator.\n+      adjust_evacuation_budgets(heap, collection_set, add_regions_to_old);\n@@ -819,4 +837,3 @@\n-    \/\/ We are preparing for evacuation.  At this time, we ignore cset region tallies.\n-    size_t young_cset_regions, old_cset_regions, first_old, last_old, num_old;\n-    _free_set->prepare_to_rebuild(young_cset_regions, old_cset_regions, first_old, last_old, num_old);\n-\n+    \/\/ We are preparing for evacuation.\n+    size_t young_trashed_regions, old_trashed_regions, first_old, last_old, num_old;\n+    _free_set->prepare_to_rebuild(young_trashed_regions, old_trashed_regions, first_old, last_old, num_old);\n@@ -825,1 +842,3 @@\n-      gen_heap->compute_old_generation_balance(young_cset_regions, old_cset_regions);\n+    size_t allocation_runway =\n+      gen_heap->young_generation()->heuristics()->bytes_of_allocation_runway_before_gc_trigger(young_trashed_regions);\n+      gen_heap->compute_old_generation_balance(allocation_runway, old_trashed_regions, young_trashed_regions);\n@@ -827,3 +846,1 @@\n-\n-    \/\/ Free set construction uses reserve quantities, because they are known to be valid here\n-    _free_set->finish_rebuild(young_cset_regions, old_cset_regions, num_old, true);\n+    _free_set->finish_rebuild(young_trashed_regions, old_trashed_regions, num_old);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahGeneration.cpp","additions":76,"deletions":59,"binary":false,"changes":135,"status":"modified"},{"patch":"@@ -66,1 +66,2 @@\n-  \/\/ Adjust evacuation budgets after choosing collection set.\n+  \/\/ Adjust evacuation budgets after choosing collection set.  The argument regions_to_xfer represents regions to be\n+  \/\/ transfered to old based on decisions made in top_off_collection_set()\n@@ -68,1 +69,1 @@\n-                                 ShenandoahCollectionSet* collection_set);\n+                                 ShenandoahCollectionSet* collection_set, size_t regions_to_xfer);\n@@ -147,0 +148,16 @@\n+  \/\/ Upon return from prepare_regions_and_collection_set(), certain parameters have been established to govern the\n+  \/\/ evacuation efforts that are about to begin.  In particular:\n+  \/\/\n+  \/\/ old_generation->get_promoted_reserve() represents the amount of memory within old-gen's available memory that has\n+  \/\/   been set aside to hold objects promoted from young-gen memory.  This represents an estimated percentage\n+  \/\/   of the live young-gen memory within the collection set.  If there is more data ready to be promoted than\n+  \/\/   can fit within this reserve, the promotion of some objects will be deferred until a subsequent evacuation\n+  \/\/   pass.\n+  \/\/\n+  \/\/ old_generation->get_evacuation_reserve() represents the amount of memory within old-gen's available memory that has been\n+  \/\/  set aside to hold objects evacuated from the old-gen collection set.\n+  \/\/\n+  \/\/ young_generation->get_evacuation_reserve() represents the amount of memory within young-gen's available memory that has\n+  \/\/  been set aside to hold objects evacuated from the young-gen collection set.  Conservatively, this value\n+  \/\/  equals the entire amount of live young-gen memory within the collection set, even though some of this memory\n+  \/\/  will likely be promoted.\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahGeneration.hpp","additions":19,"deletions":2,"binary":false,"changes":21,"status":"modified"},{"patch":"@@ -58,3 +58,0 @@\n-  \/\/ No need for old_gen->increase_used() as this was done when plabs were allocated.\n-  heap->reset_generation_reserves();\n-\n@@ -159,2 +156,5 @@\n-  \/\/ Invoke this in case we are able to transfer memory from OLD to YOUNG.\n-  heap->compute_old_generation_balance(0, 0);\n+\n+  \/\/ Invoke this in case we are able to transfer memory from OLD to YOUNG\n+  size_t allocation_runway =\n+    heap->young_generation()->heuristics()->bytes_of_allocation_runway_before_gc_trigger(0L);\n+  heap->compute_old_generation_balance(allocation_runway, 0, 0);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahGenerationalFullGC.cpp","additions":5,"deletions":5,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -302,1 +302,1 @@\n-      \/\/ We choose not to promote objects smaller than PLAB::min_size() by way of shared allocations, as this is too\n+      \/\/ We choose not to promote objects smaller than size_threshold by way of shared allocations as this is too\n@@ -304,1 +304,1 @@\n-      \/\/ evacuation pass.  This condition is denoted by: is_promotion && has_plab && (size <= PLAB::min_size())\n+      \/\/ evacuation pass.  This condition is denoted by: is_promotion && has_plab && (size <= size_threshhold).\n@@ -579,7 +579,6 @@\n-\/\/ xfer_limit, and any surplus is transferred to the young generation.\n-\/\/\n-\/\/ xfer_limit is the maximum we're able to transfer from young to old based on either:\n-\/\/  1. an assumption that we will be able to replenish memory \"borrowed\" from young at the end of collection, or\n-\/\/  2. there is sufficient excess in the allocation runway during GC idle cycles\n-void ShenandoahGenerationalHeap::compute_old_generation_balance(size_t old_xfer_limit, size_t old_cset_regions) {\n-\n+\/\/ mutator_xfer_limit, and any surplus is transferred to the young generation.  mutator_xfer_limit is\n+\/\/ the maximum we're able to transfer from young to old.  This is called at the end of GC, as we prepare\n+\/\/ for the idle span that precedes the next GC.\n+void ShenandoahGenerationalHeap::compute_old_generation_balance(size_t mutator_xfer_limit,\n+                                                                size_t old_trashed_regions, size_t young_trashed_regions) {\n+  shenandoah_assert_heaplocked();\n@@ -591,1 +590,1 @@\n-  \/\/ Let SOEP = ShenandoahOldEvacRatioPercent,\n+  \/\/ Let SOEP = ShenandoahOldEvacPercent,\n@@ -603,12 +602,1 @@\n-  assert(ShenandoahOldEvacRatioPercent <= 100, \"Error\");\n-  const size_t old_available = old_generation()->available();\n-  \/\/ The free set will reserve this amount of memory to hold young evacuations\n-  const size_t young_reserve = (young_generation()->max_capacity() * ShenandoahEvacReserve) \/ 100;\n-\n-  \/\/ In the case that ShenandoahOldEvacRatioPercent equals 100, max_old_reserve is limited only by xfer_limit.\n-\n-  const double bound_on_old_reserve = old_available + old_xfer_limit + young_reserve;\n-  const double max_old_reserve = ((ShenandoahOldEvacRatioPercent == 100)? bound_on_old_reserve:\n-                                  MIN2(double(young_reserve * ShenandoahOldEvacRatioPercent)\n-                                       \/ double(100 - ShenandoahOldEvacRatioPercent), bound_on_old_reserve));\n-\n+  assert(ShenandoahOldEvacPercent <= 100, \"Error\");\n@@ -617,0 +605,28 @@\n+  ShenandoahOldGeneration* old_gen = old_generation();\n+  size_t old_capacity = old_gen->max_capacity();\n+  size_t old_usage = old_gen->used(); \/\/ includes humongous waste\n+  size_t old_available = ((old_capacity >= old_usage)? old_capacity - old_usage: 0) + old_trashed_regions * region_size_bytes;\n+\n+  ShenandoahYoungGeneration* young_gen = young_generation();\n+  size_t young_capacity = young_gen->max_capacity();\n+  size_t young_usage = young_gen->used(); \/\/ includes humongous waste\n+  size_t young_available = ((young_capacity >= young_usage)? young_capacity - young_usage: 0);\n+  size_t freeset_available = free_set()->available_locked();\n+  if (young_available > freeset_available) {\n+    young_available = freeset_available;\n+  }\n+  young_available += young_trashed_regions * region_size_bytes;\n+\n+  \/\/ The free set will reserve this amount of memory to hold young evacuations (initialized to the ideal reserve)\n+  size_t young_reserve = (young_generation()->max_capacity() * ShenandoahEvacReserve) \/ 100;\n+\n+  \/\/ If ShenandoahOldEvacPercent equals 100, max_old_reserve is limited only by mutator_xfer_limit and young_reserve\n+  const size_t bound_on_old_reserve = ((old_available + mutator_xfer_limit + young_reserve) * ShenandoahOldEvacPercent) \/ 100;\n+  size_t proposed_max_old = ((ShenandoahOldEvacPercent == 100)?\n+                             bound_on_old_reserve:\n+                             MIN2((young_reserve * ShenandoahOldEvacPercent) \/ (100 - ShenandoahOldEvacPercent),\n+                                  bound_on_old_reserve));\n+  if (young_reserve > young_available) {\n+    young_reserve = young_available;\n+  }\n+\n@@ -618,2 +634,16 @@\n-  double reserve_for_mixed = 0;\n-  if (old_generation()->has_unprocessed_collection_candidates()) {\n+  size_t reserve_for_mixed = 0;\n+  const size_t old_fragmented_available =\n+    old_available - (old_generation()->free_unaffiliated_regions() + old_trashed_regions) * region_size_bytes;\n+\n+  if (old_fragmented_available > proposed_max_old) {\n+    \/\/ After we've promoted regions in place, there may be an abundance of old-fragmented available memory,\n+    \/\/ even more than the desired percentage for old reserve.  We cannot transfer these fragmented regions back\n+    \/\/ to young.  Instead we make the best of the situation by using this fragmented memory for both promotions\n+    \/\/ and evacuations.\n+    proposed_max_old = old_fragmented_available;\n+  }\n+  size_t reserve_for_promo = old_fragmented_available;\n+  const size_t max_old_reserve = proposed_max_old;\n+  const size_t mixed_candidate_live_memory = old_generation()->unprocessed_collection_candidates_live_memory();\n+  const bool doing_mixed = (mixed_candidate_live_memory > 0);\n+  if (doing_mixed) {\n@@ -622,2 +652,1 @@\n-    const double max_evac_need =\n-      (double(old_generation()->unprocessed_collection_candidates_live_memory()) * ShenandoahOldEvacWaste);\n+    const size_t max_evac_need = (size_t) (mixed_candidate_live_memory * ShenandoahOldEvacWaste);\n@@ -626,5 +655,14 @@\n-    const double old_fragmented_available =\n-      double(old_available - old_generation()->free_unaffiliated_regions() * region_size_bytes);\n-    reserve_for_mixed = max_evac_need + old_fragmented_available;\n-    if (reserve_for_mixed > max_old_reserve) {\n-      reserve_for_mixed = max_old_reserve;\n+\n+    \/\/ We prefer to evacuate all of mixed into unfragmented memory, and will expand old in order to do so, unless\n+    \/\/ we already have too much fragmented available memory in old.\n+    reserve_for_mixed = max_evac_need;\n+    if (reserve_for_mixed + reserve_for_promo > max_old_reserve) {\n+      \/\/ In this case, we'll allow old-evac to target some of the fragmented old memory.\n+      size_t excess_reserves = (reserve_for_mixed + reserve_for_promo) - max_old_reserve;\n+      if (reserve_for_promo > excess_reserves) {\n+        reserve_for_promo -= excess_reserves;\n+      } else {\n+        excess_reserves -= reserve_for_promo;\n+        reserve_for_promo = 0;\n+        reserve_for_mixed -= excess_reserves;\n+      }\n@@ -634,2 +672,2 @@\n-  \/\/ Decide how much space we should reserve for promotions from young\n-  size_t reserve_for_promo = 0;\n+  \/\/ Decide how much additional space we should reserve for promotions from young.  We give priority to mixed evacations\n+  \/\/ over promotions.\n@@ -639,4 +677,15 @@\n-    \/\/ We're promoting and have a bound on the maximum amount that can be promoted\n-    assert(max_old_reserve >= reserve_for_mixed, \"Sanity\");\n-    const size_t available_for_promotions = max_old_reserve - reserve_for_mixed;\n-    reserve_for_promo = MIN2((size_t)(promo_load * ShenandoahPromoEvacWaste), available_for_promotions);\n+    \/\/ We've already set aside all of the fragmented available memory within old-gen to represent old objects\n+    \/\/ to be promoted from young generation.  promo_load represents the memory that we anticipate to be promoted\n+    \/\/ from regions that have reached tenure age.  In the ideal, we will always use fragmented old-gen memory\n+    \/\/ to hold individually promoted objects and will use unfragmented old-gen memory to represent the old-gen\n+    \/\/ evacuation workloa.\n+\n+    \/\/ We're promoting and have an estimate of memory to be promoted from aged regions\n+    assert(max_old_reserve >= (reserve_for_mixed + reserve_for_promo), \"Sanity\");\n+    const size_t available_for_additional_promotions = max_old_reserve - (reserve_for_mixed + reserve_for_promo);\n+    size_t promo_need = (size_t)(promo_load * ShenandoahPromoEvacWaste);\n+    if (promo_need > reserve_for_promo) {\n+      reserve_for_promo += MIN2(promo_need - reserve_for_promo, available_for_additional_promotions);\n+    }\n+    \/\/ We've already reserved all the memory required for the promo_load, and possibly more.  The excess\n+    \/\/ can be consumed by objects promoted from regions that have not yet reached tenure age.\n@@ -645,3 +694,2 @@\n-  \/\/ This is the total old we want to ideally reserve\n-  const size_t old_reserve = reserve_for_mixed + reserve_for_promo;\n-  assert(old_reserve <= max_old_reserve, \"cannot reserve more than max for old evacuations\");\n+  \/\/ This is the total old we want to reserve (initialized to the ideal reserve)\n+  size_t old_reserve = reserve_for_mixed + reserve_for_promo;\n@@ -650,2 +698,8 @@\n-  const size_t max_old_available = old_generation()->available() + old_cset_regions * region_size_bytes;\n-  if (max_old_available >= old_reserve) {\n+  size_t old_region_deficit = 0;\n+  size_t old_region_surplus = 0;\n+\n+  size_t mutator_region_xfer_limit = mutator_xfer_limit \/ region_size_bytes;\n+  \/\/ align the mutator_xfer_limit on region size\n+  mutator_xfer_limit = mutator_region_xfer_limit * region_size_bytes;\n+\n+  if (old_available >= old_reserve) {\n@@ -653,3 +707,4 @@\n-    const size_t old_surplus = (max_old_available - old_reserve) \/ region_size_bytes;\n-    const size_t unaffiliated_old_regions = old_generation()->free_unaffiliated_regions() + old_cset_regions;\n-    const size_t old_region_surplus = MIN2(old_surplus, unaffiliated_old_regions);\n+    const size_t old_surplus = old_available - old_reserve;\n+    old_region_surplus = old_surplus \/ region_size_bytes;\n+    const size_t unaffiliated_old_regions = old_generation()->free_unaffiliated_regions() + old_trashed_regions;\n+    old_region_surplus = MIN2(old_region_surplus, unaffiliated_old_regions);\n@@ -657,0 +712,5 @@\n+  } else if (old_available + mutator_xfer_limit >= old_reserve) {\n+    \/\/ Mutator's xfer limit is sufficient to satisfy our need: transfer all memory from there\n+    size_t old_deficit = old_reserve - old_available;\n+    old_region_deficit = (old_deficit + region_size_bytes - 1) \/ region_size_bytes;\n+    old_generation()->set_region_balance(0 - checked_cast<ssize_t>(old_region_deficit));\n@@ -658,11 +718,37 @@\n-    \/\/ We are running a deficit which we'd like to fill from young.\n-    \/\/ Ignore that this will directly impact young_generation()->max_capacity(),\n-    \/\/ indirectly impacting young_reserve and old_reserve.  These computations are conservative.\n-    \/\/ Note that deficit is rounded up by one region.\n-    const size_t old_need = (old_reserve - max_old_available + region_size_bytes - 1) \/ region_size_bytes;\n-    const size_t max_old_region_xfer = old_xfer_limit \/ region_size_bytes;\n-\n-    \/\/ Round down the regions we can transfer from young to old. If we're running short\n-    \/\/ on young-gen memory, we restrict the xfer. Old-gen collection activities will be\n-    \/\/ curtailed if the budget is restricted.\n-    const size_t old_region_deficit = MIN2(old_need, max_old_region_xfer);\n+   \/\/ We'll try to xfer from both mutator excess and from young collector reserve\n+    size_t available_reserves = old_available + young_reserve + mutator_xfer_limit;\n+    size_t old_entitlement = (available_reserves  * ShenandoahOldEvacPercent) \/ 100;\n+\n+    \/\/ Round old_entitlement down to nearest multiple of regions to be transferred to old\n+    size_t entitled_xfer = old_entitlement - old_available;\n+    entitled_xfer = region_size_bytes * (entitled_xfer \/ region_size_bytes);\n+    size_t unaffiliated_young_regions = young_generation()->free_unaffiliated_regions();\n+    size_t unaffiliated_young_memory = unaffiliated_young_regions * region_size_bytes;\n+    if (entitled_xfer > unaffiliated_young_memory) {\n+      entitled_xfer = unaffiliated_young_memory;\n+    }\n+    old_entitlement = old_available + entitled_xfer;\n+    if (old_entitlement < old_reserve) {\n+      \/\/ There's not enough memory to satisfy our desire.  Scale back our old-gen intentions.\n+      size_t budget_overrun = old_reserve - old_entitlement;;\n+      if (reserve_for_promo > budget_overrun) {\n+        reserve_for_promo -= budget_overrun;\n+        old_reserve -= budget_overrun;\n+      } else {\n+        budget_overrun -= reserve_for_promo;\n+        reserve_for_promo = 0;\n+        reserve_for_mixed = (reserve_for_mixed > budget_overrun)? reserve_for_mixed - budget_overrun: 0;\n+        old_reserve = reserve_for_promo + reserve_for_mixed;\n+      }\n+    }\n+\n+    \/\/ Because of adjustments above, old_reserve may be smaller now than it was when we tested the branch\n+    \/\/   condition above: \"(old_available + mutator_xfer_limit >= old_reserve)\n+    \/\/ Therefore, we do NOT know that: mutator_xfer_limit < old_reserve - old_available\n+\n+    size_t old_deficit = old_reserve - old_available;\n+    old_region_deficit = (old_deficit + region_size_bytes - 1) \/ region_size_bytes;\n+\n+    \/\/ Shrink young_reserve to account for loan to old reserve\n+    const size_t reserve_xfer_regions = old_region_deficit - mutator_region_xfer_limit;\n+    young_reserve -= reserve_xfer_regions * region_size_bytes;\n@@ -671,1 +757,0 @@\n-}\n@@ -673,5 +758,9 @@\n-void ShenandoahGenerationalHeap::reset_generation_reserves() {\n-  ShenandoahHeapLocker locker(lock());\n-  young_generation()->set_evacuation_reserve(0);\n-  old_generation()->set_evacuation_reserve(0);\n-  old_generation()->set_promoted_reserve(0);\n+  assert(old_region_deficit == 0 || old_region_surplus == 0, \"Only surplus or deficit, never both\");\n+  assert(young_reserve + reserve_for_mixed + reserve_for_promo <= old_available + young_available,\n+         \"Cannot reserve more memory than is available: %zu + %zu + %zu <= %zu + %zu\",\n+         young_reserve, reserve_for_mixed, reserve_for_promo, old_available, young_available);\n+\n+  \/\/ deficit\/surplus adjustments to generation sizes will precede rebuild\n+  young_generation()->set_evacuation_reserve(young_reserve);\n+  old_generation()->set_evacuation_reserve(reserve_for_mixed);\n+  old_generation()->set_promoted_reserve(reserve_for_promo);\n@@ -1018,4 +1107,0 @@\n-  \/\/ In case degeneration interrupted concurrent evacuation or update references, we need to clean up\n-  \/\/ transient state. Otherwise, these actions have no effect.\n-  reset_generation_reserves();\n-\n@@ -1039,1 +1124,0 @@\n-  reset_generation_reserves();\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahGenerationalHeap.cpp","additions":151,"deletions":67,"binary":false,"changes":218,"status":"modified"},{"patch":"@@ -139,1 +139,1 @@\n-  void compute_old_generation_balance(size_t old_xfer_limit, size_t old_cset_regions);\n+  void compute_old_generation_balance(size_t old_xfer_limit, size_t old_trashed_regions, size_t young_trashed_regions);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahGenerationalHeap.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -428,0 +428,8 @@\n+\n+    if (mode()->is_generational()) {\n+      size_t young_reserve = (soft_max_capacity() * ShenandoahEvacReserve) \/ 100;\n+      young_generation()->set_evacuation_reserve(young_reserve);\n+      old_generation()->set_evacuation_reserve((size_t) 0);\n+      old_generation()->set_promoted_reserve((size_t) 0);\n+    }\n+\n@@ -430,0 +438,1 @@\n+\n@@ -431,2 +440,2 @@\n-    size_t young_cset_regions, old_cset_regions, first_old, last_old, num_old;\n-    _free_set->prepare_to_rebuild(young_cset_regions, old_cset_regions, first_old, last_old, num_old);\n+    size_t young_trashed_regions, old_trashed_regions, first_old, last_old, num_old;\n+    _free_set->prepare_to_rebuild(young_trashed_regions, old_trashed_regions, first_old, last_old, num_old);\n@@ -439,1 +448,1 @@\n-      gen_heap->compute_old_generation_balance(allocation_runway, old_cset_regions);\n+      gen_heap->compute_old_generation_balance(allocation_runway, old_trashed_regions, young_trashed_regions);\n@@ -441,1 +450,1 @@\n-    _free_set->finish_rebuild(young_cset_regions, old_cset_regions, num_old);\n+    _free_set->finish_rebuild(young_trashed_regions, old_trashed_regions, num_old);\n@@ -2524,4 +2533,1 @@\n-void ShenandoahHeap::rebuild_free_set(bool concurrent) {\n-  ShenandoahGCPhase phase(concurrent ?\n-                          ShenandoahPhaseTimings::final_update_refs_rebuild_freeset :\n-                          ShenandoahPhaseTimings::degen_gc_final_update_refs_rebuild_freeset);\n+void ShenandoahHeap::rebuild_free_set_within_phase() {\n@@ -2529,2 +2535,2 @@\n-  size_t young_cset_regions, old_cset_regions, first_old_region, last_old_region, old_region_count;\n-  _free_set->prepare_to_rebuild(young_cset_regions, old_cset_regions, first_old_region, last_old_region, old_region_count);\n+  size_t young_trashed_regions, old_trashed_regions, first_old_region, last_old_region, old_region_count;\n+  _free_set->prepare_to_rebuild(young_trashed_regions, old_trashed_regions, first_old_region, last_old_region, old_region_count);\n@@ -2549,10 +2555,2 @@\n-      gen_heap->young_generation()->heuristics()->bytes_of_allocation_runway_before_gc_trigger(young_cset_regions);\n-    gen_heap->compute_old_generation_balance(allocation_runway, old_cset_regions);\n-\n-    \/\/ Total old_available may have been expanded to hold anticipated promotions.  We trigger if the fragmented available\n-    \/\/ memory represents more than 16 regions worth of data.  Note that fragmentation may increase when we promote regular\n-    \/\/ regions in place when many of these regular regions have an abundant amount of available memory within them.\n-    \/\/ Fragmentation will decrease as promote-by-copy consumes the available memory within these partially consumed regions.\n-    \/\/\n-    \/\/ We consider old-gen to have excessive fragmentation if more than 12.5% of old-gen is free memory that resides\n-    \/\/ within partially consumed regions of memory.\n+      gen_heap->young_generation()->heuristics()->bytes_of_allocation_runway_before_gc_trigger(young_trashed_regions);\n+    gen_heap->compute_old_generation_balance(allocation_runway, old_trashed_regions, young_trashed_regions);\n@@ -2561,1 +2559,1 @@\n-  _free_set->finish_rebuild(young_cset_regions, old_cset_regions, old_region_count);\n+  _free_set->finish_rebuild(young_trashed_regions, old_trashed_regions, old_region_count);\n@@ -2570,0 +2568,7 @@\n+void ShenandoahHeap::rebuild_free_set(bool concurrent) {\n+  ShenandoahGCPhase phase(concurrent ?\n+                          ShenandoahPhaseTimings::final_update_refs_rebuild_freeset :\n+                          ShenandoahPhaseTimings::degen_gc_final_update_refs_rebuild_freeset);\n+  rebuild_free_set_within_phase();\n+}\n+\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeap.cpp","additions":26,"deletions":21,"binary":false,"changes":47,"status":"modified"},{"patch":"@@ -484,0 +484,1 @@\n+  \/\/ The following two functions rebuild the free set at the end of GC, in preparation for an idle phase.\n@@ -485,0 +486,1 @@\n+  void rebuild_free_set_within_phase();\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeap.hpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -598,0 +598,2 @@\n+    \/\/ Otherwise, the mutator might see region as fully recycled and might change its affiliation only to have\n+    \/\/ the racing GC worker thread overwrite its affiliation to FREE.\n@@ -608,0 +610,2 @@\n+\/\/ Note that return from try_recycle() does not mean the region has been recycled.  It only means that\n+\/\/ some GC worker thread has taken responsibility to recycle the region, eventually.\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeapRegion.cpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -131,2 +131,0 @@\n-  heap->free_set()->log_status_under_lock();\n-\n@@ -141,2 +139,5 @@\n-  size_t allocation_runway = heap->young_generation()->heuristics()->bytes_of_allocation_runway_before_gc_trigger(0);\n-  heap->compute_old_generation_balance(allocation_runway, 0);\n+  \/\/ After concurrent old marking finishes, we reclaim immediate garbage. Further, we may also want to expand OLD in order\n+  \/\/ to make room for anticipated promotions and\/or for mixed evacuations.  Mixed evacuations are especially likely to\n+  \/\/ follow the end of OLD marking.\n+  heap->rebuild_free_set_within_phase();\n+  heap->free_set()->log_status_under_lock();\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahOldGC.cpp","additions":5,"deletions":4,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -430,2 +430,1 @@\n-    gen_heap->compute_old_generation_balance(allocation_runway, old_trash_regions);\n-\n+    gen_heap->compute_old_generation_balance(allocation_runway, old_trash_regions, young_trash_regions);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahOldGeneration.cpp","additions":1,"deletions":2,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -69,2 +69,2 @@\n-  \/\/ Represents the quantity of live bytes we expect to promote during the next evacuation\n-  \/\/ cycle. This value is used by the young heuristic to trigger mixed collections.\n+  \/\/ Represents the quantity of live bytes we expect to promote during the next GC cycle, either by\n+  \/\/ evacuation or by promote-in-place.  This value is used by the young heuristic to trigger mixed collections.\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahOldGeneration.hpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -246,2 +246,1 @@\n-  \/\/ For HumongousRegion:s it's more efficient to jump directly to the\n-  \/\/ start region.\n+  \/\/ For humongous regions it's more efficient to jump directly to the start region.\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahScanRemembered.cpp","additions":1,"deletions":2,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -423,1 +423,8 @@\n-  size_t non_trashed_span() const { return (_regions - _trashed_regions) * ShenandoahHeapRegion::region_size_bytes(); }\n+  size_t non_trashed_span() const {\n+    assert(_regions >= _trashed_regions, \"sanity\");\n+    return (_regions - _trashed_regions) * ShenandoahHeapRegion::region_size_bytes();\n+  }\n+  size_t non_trashed_committed() const {\n+    assert(_committed >= _trashed_regions * ShenandoahHeapRegion::region_size_bytes(), \"sanity\");\n+    return _committed - (_trashed_regions * ShenandoahHeapRegion::region_size_bytes());\n+  }\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahVerifier.cpp","additions":8,"deletions":1,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -403,21 +403,14 @@\n-  product(uintx, ShenandoahOldEvacRatioPercent, 75, EXPERIMENTAL,           \\\n-          \"The maximum proportion of evacuation from old-gen memory, \"      \\\n-          \"expressed as a percentage. The default value 75 denotes that \"   \\\n-          \"no more than 75% of the collection set evacuation workload may \" \\\n-          \"be towards evacuation of old-gen heap regions. This limits both \"\\\n-          \"the promotion of aged regions and the compaction of existing \"   \\\n-          \"old regions. A value of 75 denotes that the total evacuation \"   \\\n-          \"work may increase to up to four times the young gen evacuation \" \\\n-          \"work. A larger value allows quicker promotion and allows \"       \\\n-          \"a smaller number of mixed evacuations to process \"               \\\n-          \"the entire list of old-gen collection candidates at the cost \"   \\\n-          \"of an increased disruption of the normal cadence of young-gen \"  \\\n-          \"collections.  A value of 100 allows a mixed evacuation to \"      \\\n-          \"focus entirely on old-gen memory, allowing no young-gen \"        \\\n-          \"regions to be collected, likely resulting in subsequent \"        \\\n-          \"allocation failures because the allocation pool is not \"         \\\n-          \"replenished.  A value of 0 allows a mixed evacuation to \"        \\\n-          \"focus entirely on young-gen memory, allowing no old-gen \"        \\\n-          \"regions to be collected, likely resulting in subsequent \"        \\\n-          \"promotion failures and triggering of stop-the-world full GC \"    \\\n-          \"events.\")                                                        \\\n+  product(uintx, ShenandoahOldEvacPercent, 75, EXPERIMENTAL,                \\\n+          \"The maximum evacuation to old-gen expressed as a percent of \"    \\\n+          \"the total live memory within the collection set.  With the \"     \\\n+          \"default setting, if collection set evacuates X, no more than \"   \\\n+          \"75% of X may hold objects evacuated from old or promoted to \"    \\\n+          \"old from young.  A value of 100 allows the entire collection \"   \\\n+          \"set to be comprised of old-gen regions and young regions that \"  \\\n+          \"have reached the tenure age.  Larger values allow fewer mixed \"  \\\n+          \"evacuations to reclaim all the garbage from old.  Smaller \"      \\\n+          \"values result in less variation in GC cycle times between \"      \\\n+          \"young vs. mixed cycles.  A value of 0 prevents mixed \"           \\\n+          \"evacations from running and blocks promotion of aged regions \"   \\\n+          \"by evacuation.  Setting the value to 0 does not prevent \"        \\\n+          \"regions from being promoted in place.\")                          \\\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoah_globals.hpp","additions":14,"deletions":21,"binary":false,"changes":35,"status":"modified"},{"patch":"@@ -204,1 +204,3 @@\n-  _heuristics->prime_collection_set(_collection_set);\n+  if (_heuristics->prime_collection_set(_collection_set)) {\n+    _heuristics->finalize_mixed_evacs();\n+  }\n@@ -217,1 +219,3 @@\n-  _heuristics->prime_collection_set(_collection_set);\n+  if (_heuristics->prime_collection_set(_collection_set)) {\n+    _heuristics->finalize_mixed_evacs();\n+  }\n@@ -229,1 +233,3 @@\n-  _heuristics->prime_collection_set(_collection_set);\n+  if (_heuristics->prime_collection_set(_collection_set)) {\n+    _heuristics->finalize_mixed_evacs();\n+  }\n@@ -251,1 +257,3 @@\n-  _heuristics->prime_collection_set(_collection_set);\n+  if (_heuristics->prime_collection_set(_collection_set)) {\n+    _heuristics->finalize_mixed_evacs();\n+  }\n@@ -264,1 +272,3 @@\n-  _heuristics->prime_collection_set(_collection_set);\n+  if (_heuristics->prime_collection_set(_collection_set)) {\n+    _heuristics->finalize_mixed_evacs();\n+  }\n@@ -281,1 +291,3 @@\n-  _heuristics->prime_collection_set(_collection_set);\n+  if (_heuristics->prime_collection_set(_collection_set)) {\n+    _heuristics->finalize_mixed_evacs();\n+  }\n@@ -288,1 +300,3 @@\n-  _heuristics->prime_collection_set(_collection_set);\n+  if (_heuristics->prime_collection_set(_collection_set)) {\n+    _heuristics->finalize_mixed_evacs();\n+  }\n@@ -304,1 +318,3 @@\n-  _heuristics->prime_collection_set(_collection_set);\n+  if (_heuristics->prime_collection_set(_collection_set)) {\n+    _heuristics->finalize_mixed_evacs();\n+  }\n@@ -312,1 +328,3 @@\n-  _heuristics->prime_collection_set(_collection_set);\n+  if (_heuristics->prime_collection_set(_collection_set)) {\n+    _heuristics->finalize_mixed_evacs();\n+  }\n@@ -330,1 +348,3 @@\n-  _heuristics->prime_collection_set(_collection_set);\n+  if (_heuristics->prime_collection_set(_collection_set)) {\n+    _heuristics->finalize_mixed_evacs();\n+  }\n@@ -339,1 +359,3 @@\n-  _heuristics->prime_collection_set(_collection_set);\n+  if (_heuristics->prime_collection_set(_collection_set)) {\n+    _heuristics->finalize_mixed_evacs();\n+  }\n@@ -357,1 +379,3 @@\n-  _heuristics->prime_collection_set(_collection_set);\n+  if (_heuristics->prime_collection_set(_collection_set)) {\n+    _heuristics->finalize_mixed_evacs();\n+  }\n","filename":"test\/hotspot\/gtest\/gc\/shenandoah\/test_shenandoahOldHeuristic.cpp","additions":36,"deletions":12,"binary":false,"changes":48,"status":"modified"}]}