{"files":[{"patch":"@@ -178,0 +178,1 @@\n+    bool need_to_finalize_mixed = false;\n@@ -179,1 +180,1 @@\n-      heap->old_generation()->heuristics()->prime_collection_set(collection_set);\n+      need_to_finalize_mixed = heap->old_generation()->heuristics()->prime_collection_set(collection_set);\n@@ -184,0 +185,11 @@\n+\n+    if (_generation->is_young()) {\n+      \/\/ Especially when young-gen trigger is expedited in order to finish mixed evacuations, there may not be\n+      \/\/ enough consolidated garbage to make effective use of young-gen evacuation reserve.  If there is still\n+      \/\/ young-gen reserve available following selection of the young-gen collection set, see if we can use\n+      \/\/ this memory to expand the old-gen evacuation collection set.\n+      need_to_finalize_mixed |= heap->old_generation()->heuristics()->top_off_collection_set();\n+      if (need_to_finalize_mixed) {\n+        heap->old_generation()->heuristics()->finalize_mixed_evacs();\n+      }\n+    }\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/heuristics\/shenandoahGenerationalHeuristics.cpp","additions":13,"deletions":1,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -56,1 +56,2 @@\n-  size_t capacity = heap->young_generation()->max_capacity();\n+  size_t young_capacity = heap->young_generation()->max_capacity();\n+  size_t old_capacity = heap->old_generation()->max_capacity();\n@@ -63,10 +64,0 @@\n-  size_t max_young_cset = (size_t) (young_evac_reserve \/ ShenandoahEvacWaste);\n-  size_t young_cur_cset = 0;\n-  size_t max_old_cset = (size_t) (old_evac_reserve \/ ShenandoahOldEvacWaste);\n-  size_t old_cur_cset = 0;\n-\n-  \/\/ Figure out how many unaffiliated young regions are dedicated to mutator and to evacuator.  Allow the young\n-  \/\/ collector's unaffiliated regions to be transferred to old-gen if old-gen has more easily reclaimed garbage\n-  \/\/ than young-gen.  At the end of this cycle, any excess regions remaining in old-gen will be transferred back\n-  \/\/ to young.  Do not transfer the mutator's unaffiliated regions to old-gen.  Those must remain available\n-  \/\/ to the mutator as it needs to be able to consume this memory during concurrent GC.\n@@ -76,6 +67,24 @@\n-\n-  if (unaffiliated_young_memory > max_young_cset) {\n-    size_t unaffiliated_mutator_memory = unaffiliated_young_memory - max_young_cset;\n-    unaffiliated_young_memory -= unaffiliated_mutator_memory;\n-    unaffiliated_young_regions = unaffiliated_young_memory \/ region_size_bytes; \/\/ round down\n-    unaffiliated_young_memory = unaffiliated_young_regions * region_size_bytes;\n+  size_t unaffiliated_old_regions = heap->old_generation()->free_unaffiliated_regions();\n+  size_t unaffiliated_old_memory = unaffiliated_old_regions * region_size_bytes;\n+\n+  \/\/ Figure out how many unaffiliated regions are dedicated to Collector and OldCollector reserves.  Let these\n+  \/\/ be shuffled between young and old generations in order to expedite evacuation of whichever regions have the\n+  \/\/ most garbage, regardless of whether these garbage-first regions reside in young or old generation.\n+  \/\/ Excess reserves will be transferred back to the mutator after collection set has been chosen.  At the end\n+  \/\/ of evacuation, any reserves not consumed by evacuation will also be transferred to the mutator free set.\n+  size_t shared_reserve_regions = 0;\n+  if (young_evac_reserve > unaffiliated_young_memory) {\n+    young_evac_reserve -= unaffiliated_young_memory;\n+    shared_reserve_regions += unaffiliated_young_memory \/ region_size_bytes;\n+  } else {\n+    size_t delta_regions = young_evac_reserve \/ region_size_bytes;\n+    shared_reserve_regions += delta_regions;\n+    young_evac_reserve -= delta_regions * region_size_bytes;\n+  }\n+  if (old_evac_reserve > unaffiliated_old_memory) {\n+    old_evac_reserve -= unaffiliated_old_memory;\n+    shared_reserve_regions += unaffiliated_old_memory \/ region_size_bytes;\n+  } else {\n+    size_t delta_regions = old_evac_reserve \/ region_size_bytes;\n+    shared_reserve_regions += delta_regions;\n+    old_evac_reserve -= delta_regions * region_size_bytes;\n@@ -84,2 +93,6 @@\n-  \/\/ We'll affiliate these unaffiliated regions with either old or young, depending on need.\n-  max_young_cset -= unaffiliated_young_memory;\n+  size_t shared_reserves = shared_reserve_regions * region_size_bytes;\n+  size_t committed_from_shared_reserves = 0;\n+  size_t max_young_cset = (size_t) (young_evac_reserve \/ ShenandoahEvacWaste);\n+  size_t young_cur_cset = 0;\n+  size_t max_old_cset = (size_t) (old_evac_reserve \/ ShenandoahOldEvacWaste);\n+  size_t old_cur_cset = 0;\n@@ -87,2 +100,3 @@\n-  \/\/ Keep track of how many regions we plan to transfer from young to old.\n-  size_t regions_transferred_to_old = 0;\n+  size_t promo_bytes = 0;\n+  size_t old_evac_bytes = 0;\n+  size_t young_evac_bytes = 0;\n@@ -90,1 +104,3 @@\n-  size_t free_target = (capacity * ShenandoahMinFreeThreshold) \/ 100 + max_young_cset;\n+  size_t max_total_cset = (max_young_cset + max_old_cset +\n+                           (size_t) (shared_reserve_regions * region_size_bytes) \/ ShenandoahOldEvacWaste);\n+  size_t free_target = ((young_capacity + old_capacity) * ShenandoahMinFreeThreshold) \/ 100 + max_total_cset;\n@@ -94,1 +110,1 @@\n-                     \"%s, Max Old Evacuation: %zu%s, Actual Free: %zu%s.\",\n+                     \"%s, Max Old Evacuation: %zu%s, Discretionary additional evacuation: %zu%s, Actual Free: %zu%s.\",\n@@ -97,0 +113,1 @@\n+                     byte_size_in_proper_unit(shared_reserves), proper_unit_for_byte_size(shared_reserves),\n@@ -99,0 +116,1 @@\n+  size_t cur_garbage = cur_young_garbage;\n@@ -103,0 +121,3 @@\n+    size_t region_garbage = r->garbage();\n+    size_t new_garbage = cur_garbage + region_garbage;\n+    bool add_regardless = (region_garbage > ignore_threshold) && (new_garbage < min_garbage);\n@@ -104,5 +125,9 @@\n-      size_t new_cset = old_cur_cset + r->get_live_data_bytes();\n-      if ((r->garbage() > garbage_threshold)) {\n-        while ((new_cset > max_old_cset) && (unaffiliated_young_regions > 0)) {\n-          unaffiliated_young_regions--;\n-          regions_transferred_to_old++;\n+      if (add_regardless || (region_garbage > garbage_threshold)) {\n+        size_t live_bytes = r->get_live_data_bytes();\n+        size_t new_cset = old_cur_cset + r->get_live_data_bytes();\n+        \/\/ May need multiple reserve regions to evacuate a single region, depending on live data bytes and ShenandoahOldEvacWaste\n+        size_t orig_max_old_cset = max_old_cset;\n+        size_t proposed_old_region_consumption = 0;\n+        while ((new_cset > max_old_cset) && (committed_from_shared_reserves < shared_reserves)) {\n+          committed_from_shared_reserves += region_size_bytes;\n+          proposed_old_region_consumption++;\n@@ -111,4 +136,15 @@\n-      }\n-      if ((new_cset <= max_old_cset) && (r->garbage() > garbage_threshold)) {\n-        add_region = true;\n-        old_cur_cset = new_cset;\n+        \/\/ We already know: add_regardless || region_garbage > garbage_threshold\n+        if (new_cset <= max_old_cset) {\n+          add_region = true;\n+          old_cur_cset = new_cset;\n+          cur_garbage = new_garbage;\n+          if (r->is_old()) {\n+            old_evac_bytes += live_bytes;\n+          } else {\n+            promo_bytes += live_bytes;\n+          }\n+        } else {\n+          \/\/ We failed to sufficiently expand old, so unwind proposed expansion\n+          max_old_cset = orig_max_old_cset;\n+          committed_from_shared_reserves -= proposed_old_region_consumption * region_size_bytes;\n+        }\n@@ -118,8 +154,9 @@\n-      size_t new_cset = young_cur_cset + r->get_live_data_bytes();\n-      size_t region_garbage = r->garbage();\n-      size_t new_garbage = cur_young_garbage + region_garbage;\n-      bool add_regardless = (region_garbage > ignore_threshold) && (new_garbage < min_garbage);\n-\n-      if (add_regardless || (r->garbage() > garbage_threshold)) {\n-        while ((new_cset > max_young_cset) && (unaffiliated_young_regions > 0)) {\n-          unaffiliated_young_regions--;\n+      if (add_regardless || (region_garbage > garbage_threshold)) {\n+        size_t live_bytes = r->get_live_data_bytes();\n+        size_t new_cset = young_cur_cset + live_bytes;\n+        \/\/ May need multiple reserve regions to evacuate a single region, depending on live data bytes and ShenandoahEvacWaste\n+        size_t orig_max_young_cset = max_young_cset;\n+        size_t proposed_young_region_consumption = 0;\n+        while ((new_cset > max_young_cset) && (committed_from_shared_reserves < shared_reserves)) {\n+          committed_from_shared_reserves += region_size_bytes;\n+          proposed_young_region_consumption++;\n@@ -128,5 +165,11 @@\n-      }\n-      if ((new_cset <= max_young_cset) && (add_regardless || (region_garbage > garbage_threshold))) {\n-        add_region = true;\n-        young_cur_cset = new_cset;\n-        cur_young_garbage = new_garbage;\n+        \/\/ We already know: add_regardless || region_garbage > garbage_threshold\n+        if (new_cset <= max_young_cset) {\n+          add_region = true;\n+          young_cur_cset = new_cset;\n+          cur_garbage = new_garbage;\n+          young_evac_bytes += live_bytes;\n+        } else {\n+          \/\/ We failed to sufficiently expand young, so unwind proposed expansion\n+          max_young_cset = orig_max_young_cset;\n+          committed_from_shared_reserves -= proposed_young_region_consumption * region_size_bytes;\n+        }\n@@ -140,5 +183,3 @@\n-  if (regions_transferred_to_old > 0) {\n-    heap->generation_sizer()->force_transfer_to_old(regions_transferred_to_old);\n-    heap->young_generation()->set_evacuation_reserve(young_evac_reserve - regions_transferred_to_old * region_size_bytes);\n-    heap->old_generation()->set_evacuation_reserve(old_evac_reserve + regions_transferred_to_old * region_size_bytes);\n-  }\n+  heap->young_generation()->set_evacuation_reserve((size_t) (young_evac_bytes * ShenandoahEvacWaste));\n+  heap->old_generation()->set_evacuation_reserve((size_t) (old_evac_bytes * ShenandoahOldEvacWaste));\n+  heap->old_generation()->set_promoted_reserve((size_t) (promo_bytes * ShenandoahPromoEvacWaste));\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/heuristics\/shenandoahGlobalHeuristics.cpp","additions":91,"deletions":50,"binary":false,"changes":141,"status":"modified"},{"patch":"@@ -29,0 +29,1 @@\n+#include \"gc\/shenandoah\/shenandoahFreeSet.hpp\"\n@@ -32,0 +33,1 @@\n+#include \"gc\/shenandoah\/shenandoahYoungGeneration.hpp\"\n@@ -79,3 +81,4 @@\n-  if (unprocessed_old_collection_candidates() == 0) {\n-    return false;\n-  }\n+  _mixed_evac_cset = collection_set;\n+  _included_old_regions = 0;\n+  _evacuated_old_bytes = 0;\n+  _collected_old_bytes = 0;\n@@ -91,6 +94,0 @@\n-  _first_pinned_candidate = NOT_FOUND;\n-\n-  uint included_old_regions = 0;\n-  size_t evacuated_old_bytes = 0;\n-  size_t collected_old_bytes = 0;\n-\n@@ -102,10 +99,22 @@\n-  const size_t old_evacuation_reserve = _old_generation->get_evacuation_reserve();\n-  const size_t old_evacuation_budget = (size_t) ((double) old_evacuation_reserve \/ ShenandoahOldEvacWaste);\n-  size_t unfragmented_available = _old_generation->free_unaffiliated_regions() * ShenandoahHeapRegion::region_size_bytes();\n-  size_t fragmented_available;\n-  size_t excess_fragmented_available;\n-\n-  if (unfragmented_available > old_evacuation_budget) {\n-    unfragmented_available = old_evacuation_budget;\n-    fragmented_available = 0;\n-    excess_fragmented_available = 0;\n+  _old_evacuation_reserve = _old_generation->get_evacuation_reserve();\n+  _old_evacuation_budget = (size_t) ((double) _old_evacuation_reserve \/ ShenandoahOldEvacWaste);\n+\n+  \/\/ fragmented_available is the amount of memory within partially consumed old regions that may be required to\n+  \/\/ hold the results of old evacuations.  If all of the memory required by the old evacuation reserve is available\n+  \/\/ in unfragmented regions (unaffiliated old regions), then fragmented_available is zero because we do not need\n+  \/\/ to evacuate into the existing partially consumed old regions.\n+\n+  \/\/ if fragmented_available is non-zero, excess_fragmented_available represents the amount of fragmented memory\n+  \/\/ that is available within old, but is not required to hold the resuilts of old evacuation.  As old-gen regions\n+  \/\/ are added into the collection set, their free memory is subtracted from excess_fragmented_available until the\n+  \/\/ excess is exhausted.  For old-gen regions subsequently added to the collection set, their free memory is\n+  \/\/ subtracted from fragmented_available and from the old_evacuation_budget (since the budget decreases when this\n+  \/\/ fragmented_available memory decreases).  After fragmented_available has been exhausted, any further old regions\n+  \/\/ selected for the cset do not further decrease the old_evacuation_budget because all further evacuation is targeted\n+  \/\/ to unfragmented regions.\n+\n+  size_t unaffiliated_available = _old_generation->free_unaffiliated_regions() * ShenandoahHeapRegion::region_size_bytes();\n+  if (unaffiliated_available > _old_evacuation_reserve) {\n+    _unfragmented_available = _old_evacuation_budget;\n+    _fragmented_available = 0;\n+    _excess_fragmented_available = 0;\n@@ -113,6 +122,6 @@\n-    assert(_old_generation->available() >= old_evacuation_budget, \"Cannot budget more than is available\");\n-    fragmented_available = _old_generation->available() - unfragmented_available;\n-    assert(fragmented_available + unfragmented_available >= old_evacuation_budget, \"Budgets do not add up\");\n-    if (fragmented_available + unfragmented_available > old_evacuation_budget) {\n-      excess_fragmented_available = (fragmented_available + unfragmented_available) - old_evacuation_budget;\n-      fragmented_available -= excess_fragmented_available;\n+    assert(_old_generation->available() >= _old_evacuation_reserve, \"Cannot reserve more than is available\");\n+    size_t affiliated_available = _old_generation->available() - unaffiliated_available;\n+    assert(affiliated_available + unaffiliated_available >= _old_evacuation_reserve, \"Budgets do not add up\");\n+    if (affiliated_available + unaffiliated_available > _old_evacuation_reserve) {\n+      _excess_fragmented_available = (affiliated_available + unaffiliated_available) - _old_evacuation_reserve;\n+      affiliated_available -= _excess_fragmented_available;\n@@ -120,0 +129,2 @@\n+    _fragmented_available = (size_t) ((double) affiliated_available \/ ShenandoahOldEvacWaste);\n+    _unfragmented_available = (size_t) ((double) unaffiliated_available \/ ShenandoahOldEvacWaste);\n@@ -122,1 +133,0 @@\n-  size_t remaining_old_evacuation_budget = old_evacuation_budget;\n@@ -124,1 +134,1 @@\n-                byte_size_in_proper_unit(old_evacuation_budget), proper_unit_for_byte_size(old_evacuation_budget),\n+                byte_size_in_proper_unit(_old_evacuation_budget), proper_unit_for_byte_size(_old_evacuation_budget),\n@@ -126,116 +136,1 @@\n-\n-  size_t lost_evacuation_capacity = 0;\n-\n-  \/\/ The number of old-gen regions that were selected as candidates for collection at the end of the most recent old-gen\n-  \/\/ concurrent marking phase and have not yet been collected is represented by unprocessed_old_collection_candidates().\n-  \/\/ Candidate regions are ordered according to increasing amount of live data.  If there is not sufficient room to\n-  \/\/ evacuate region N, then there is no need to even consider evacuating region N+1.\n-  while (unprocessed_old_collection_candidates() > 0) {\n-    \/\/ Old collection candidates are sorted in order of decreasing garbage contained therein.\n-    ShenandoahHeapRegion* r = next_old_collection_candidate();\n-    if (r == nullptr) {\n-      break;\n-    }\n-    assert(r->is_regular(), \"There should be no humongous regions in the set of mixed-evac candidates\");\n-\n-    \/\/ If region r is evacuated to fragmented memory (to free memory within a partially used region), then we need\n-    \/\/ to decrease the capacity of the fragmented memory by the scaled loss.\n-\n-    size_t live_data_for_evacuation = r->get_live_data_bytes();\n-    size_t lost_available = r->free();\n-\n-    if ((lost_available > 0) && (excess_fragmented_available > 0)) {\n-      if (lost_available < excess_fragmented_available) {\n-        excess_fragmented_available -= lost_available;\n-        lost_evacuation_capacity -= lost_available;\n-        lost_available  = 0;\n-      } else {\n-        lost_available -= excess_fragmented_available;\n-        lost_evacuation_capacity -= excess_fragmented_available;\n-        excess_fragmented_available = 0;\n-      }\n-    }\n-    size_t scaled_loss = (size_t) ((double) lost_available \/ ShenandoahOldEvacWaste);\n-    if ((lost_available > 0) && (fragmented_available > 0)) {\n-      if (scaled_loss + live_data_for_evacuation < fragmented_available) {\n-        fragmented_available -= scaled_loss;\n-        scaled_loss = 0;\n-      } else {\n-        \/\/ We will have to allocate this region's evacuation memory from unfragmented memory, so don't bother\n-        \/\/ to decrement scaled_loss\n-      }\n-    }\n-    if (scaled_loss > 0) {\n-      \/\/ We were not able to account for the lost free memory within fragmented memory, so we need to take this\n-      \/\/ allocation out of unfragmented memory.  Unfragmented memory does not need to account for loss of free.\n-      if (live_data_for_evacuation > unfragmented_available) {\n-        \/\/ There is not room to evacuate this region or any that come after it in within the candidates array.\n-        break;\n-      } else {\n-        unfragmented_available -= live_data_for_evacuation;\n-      }\n-    } else {\n-      \/\/ Since scaled_loss == 0, we have accounted for the loss of free memory, so we can allocate from either\n-      \/\/ fragmented or unfragmented available memory.  Use up the fragmented memory budget first.\n-      size_t evacuation_need = live_data_for_evacuation;\n-\n-      if (evacuation_need > fragmented_available) {\n-        evacuation_need -= fragmented_available;\n-        fragmented_available = 0;\n-      } else {\n-        fragmented_available -= evacuation_need;\n-        evacuation_need = 0;\n-      }\n-      if (evacuation_need > unfragmented_available) {\n-        \/\/ There is not room to evacuate this region or any that come after it in within the candidates array.\n-        break;\n-      } else {\n-        unfragmented_available -= evacuation_need;\n-        \/\/ dead code: evacuation_need == 0;\n-      }\n-    }\n-    collection_set->add_region(r);\n-    included_old_regions++;\n-    evacuated_old_bytes += live_data_for_evacuation;\n-    collected_old_bytes += r->garbage();\n-    consume_old_collection_candidate();\n-  }\n-\n-  if (_first_pinned_candidate != NOT_FOUND) {\n-    \/\/ Need to deal with pinned regions\n-    slide_pinned_regions_to_front();\n-  }\n-  decrease_unprocessed_old_collection_candidates_live_memory(evacuated_old_bytes);\n-  if (included_old_regions > 0) {\n-    log_info(gc, ergo)(\"Old-gen piggyback evac (\" UINT32_FORMAT \" regions, evacuating \" PROPERFMT \", reclaiming: \" PROPERFMT \")\",\n-                  included_old_regions, PROPERFMTARGS(evacuated_old_bytes), PROPERFMTARGS(collected_old_bytes));\n-  }\n-\n-  if (unprocessed_old_collection_candidates() == 0) {\n-    \/\/ We have added the last of our collection candidates to a mixed collection.\n-    \/\/ Any triggers that occurred during mixed evacuations may no longer be valid.  They can retrigger if appropriate.\n-    clear_triggers();\n-\n-    _old_generation->complete_mixed_evacuations();\n-  } else if (included_old_regions == 0) {\n-    \/\/ We have candidates, but none were included for evacuation - are they all pinned?\n-    \/\/ or did we just not have enough room for any of them in this collection set?\n-    \/\/ We don't want a region with a stuck pin to prevent subsequent old collections, so\n-    \/\/ if they are all pinned we transition to a state that will allow us to make these uncollected\n-    \/\/ (pinned) regions parsable.\n-    if (all_candidates_are_pinned()) {\n-      log_info(gc, ergo)(\"All candidate regions \" UINT32_FORMAT \" are pinned\", unprocessed_old_collection_candidates());\n-      _old_generation->abandon_mixed_evacuations();\n-    } else {\n-      log_info(gc, ergo)(\"No regions selected for mixed collection. \"\n-                         \"Old evacuation budget: \" PROPERFMT \", Remaining evacuation budget: \" PROPERFMT\n-                         \", Lost capacity: \" PROPERFMT\n-                         \", Next candidate: \" UINT32_FORMAT \", Last candidate: \" UINT32_FORMAT,\n-                         PROPERFMTARGS(old_evacuation_reserve),\n-                         PROPERFMTARGS(remaining_old_evacuation_budget),\n-                         PROPERFMTARGS(lost_evacuation_capacity),\n-                         _next_old_collection_candidate, _last_old_collection_candidate);\n-    }\n-  }\n-\n-  return (included_old_regions > 0);\n+  return add_old_regions_to_cset();\n@@ -315,0 +210,148 @@\n+bool ShenandoahOldHeuristics::add_old_regions_to_cset() {\n+  if (unprocessed_old_collection_candidates() == 0) {\n+    return false;\n+  }\n+  _first_pinned_candidate = NOT_FOUND;\n+\n+  \/\/ The number of old-gen regions that were selected as candidates for collection at the end of the most recent old-gen\n+  \/\/ concurrent marking phase and have not yet been collected is represented by unprocessed_old_collection_candidates().\n+  \/\/ Candidate regions are ordered according to increasing amount of live data.  If there is not sufficient room to\n+  \/\/ evacuate region N, then there is no need to even consider evacuating region N+1.\n+  while (unprocessed_old_collection_candidates() > 0) {\n+    \/\/ Old collection candidates are sorted in order of decreasing garbage contained therein.\n+    ShenandoahHeapRegion* r = next_old_collection_candidate();\n+    if (r == nullptr) {\n+      break;\n+    }\n+    assert(r->is_regular(), \"There should be no humongous regions in the set of mixed-evac candidates\");\n+\n+    \/\/ If region r is evacuated to fragmented memory (to free memory within a partially used region), then we need\n+    \/\/ to decrease the capacity of the fragmented memory by the scaled loss.\n+\n+    size_t live_data_for_evacuation = r->get_live_data_bytes();\n+    size_t lost_available = r->free();\n+    if ((lost_available > 0) && (_excess_fragmented_available > 0)) {\n+      if (lost_available < _excess_fragmented_available) {\n+        _excess_fragmented_available -= lost_available;\n+        lost_available = 0;\n+      } else {\n+        lost_available -= _excess_fragmented_available;\n+        _excess_fragmented_available = 0;\n+      }\n+    }\n+\n+    ssize_t fragmented_delta = 0;\n+    ssize_t unfragmented_delta = 0;\n+    size_t scaled_loss = (size_t) ((double) lost_available \/ ShenandoahOldEvacWaste);\n+    if ((lost_available > 0) && (_fragmented_available > 0)) {\n+      if (scaled_loss < _fragmented_available) {\n+        _fragmented_available -= scaled_loss;\n+        fragmented_delta = -scaled_loss;\n+        scaled_loss = 0;\n+      } else {\n+        scaled_loss -= _fragmented_available;\n+        fragmented_delta = -_fragmented_available;\n+        _fragmented_available = 0;\n+      }\n+    }\n+    \/\/ Allocate replica from unfragmented memory if that exists\n+    size_t evacuation_need = live_data_for_evacuation;\n+    if (evacuation_need < _unfragmented_available) {\n+      _unfragmented_available -= evacuation_need;;\n+    } else {\n+      if (_unfragmented_available > 0) {\n+        evacuation_need -= _unfragmented_available;\n+        unfragmented_delta = -_unfragmented_available;\n+        _unfragmented_available = 0;\n+      }\n+      \/\/ Take the remaining allocation out of fragmented available\n+      if (_fragmented_available > evacuation_need) {\n+        _fragmented_available -= evacuation_need;\n+      } else {\n+        \/\/ We cannot add this region into the collection set.  We're done.  Undo the adjustments to available.\n+        _fragmented_available -= fragmented_delta;\n+        _unfragmented_available -= unfragmented_delta;\n+        break;\n+      }\n+    }\n+    _mixed_evac_cset->add_region(r);\n+    _included_old_regions++;\n+    _evacuated_old_bytes += live_data_for_evacuation;\n+    _collected_old_bytes += r->garbage();\n+    consume_old_collection_candidate();\n+  }\n+  return true;\n+}\n+\n+bool ShenandoahOldHeuristics::finalize_mixed_evacs() {\n+  if (_first_pinned_candidate != NOT_FOUND) {\n+    \/\/ Need to deal with pinned regions\n+    slide_pinned_regions_to_front();\n+  }\n+  decrease_unprocessed_old_collection_candidates_live_memory(_evacuated_old_bytes);\n+  if (_included_old_regions > 0) {\n+    log_info(gc)(\"Old-gen mixed evac (%zu regions, evacuating %zu%s, reclaiming: %zu%s)\",\n+                 _included_old_regions,\n+                 byte_size_in_proper_unit(_evacuated_old_bytes), proper_unit_for_byte_size(_evacuated_old_bytes),\n+                 byte_size_in_proper_unit(_collected_old_bytes), proper_unit_for_byte_size(_collected_old_bytes));\n+  }\n+\n+  if (unprocessed_old_collection_candidates() == 0) {\n+    \/\/ We have added the last of our collection candidates to a mixed collection.\n+    \/\/ Any triggers that occurred during mixed evacuations may no longer be valid.  They can retrigger if appropriate.\n+    clear_triggers();\n+    _old_generation->complete_mixed_evacuations();\n+  } else if (_included_old_regions == 0) {\n+    \/\/ We have candidates, but none were included for evacuation - are they all pinned?\n+    \/\/ or did we just not have enough room for any of them in this collection set?\n+    \/\/ We don't want a region with a stuck pin to prevent subsequent old collections, so\n+    \/\/ if they are all pinned we transition to a state that will allow us to make these uncollected\n+    \/\/ (pinned) regions parsable.\n+    if (all_candidates_are_pinned()) {\n+      log_info(gc)(\"All candidate regions \" UINT32_FORMAT \" are pinned\", unprocessed_old_collection_candidates());\n+      _old_generation->abandon_mixed_evacuations();\n+    } else {\n+      log_info(gc)(\"No regions selected for mixed collection. \"\n+                   \"Old evacuation budget: \" PROPERFMT \", Next candidate: \" UINT32_FORMAT \", Last candidate: \" UINT32_FORMAT,\n+                   PROPERFMTARGS(_old_evacuation_reserve),\n+                   _next_old_collection_candidate, _last_old_collection_candidate);\n+    }\n+  }\n+  return (_included_old_regions > 0);\n+}\n+\n+bool ShenandoahOldHeuristics::top_off_collection_set() {\n+  if (unprocessed_old_collection_candidates() == 0) {\n+    return false;\n+  } else {\n+    ShenandoahYoungGeneration* young_generation = _heap->young_generation();\n+    size_t young_unaffiliated_regions = young_generation->free_unaffiliated_regions();\n+    size_t max_young_cset = young_generation->get_evacuation_reserve();\n+    size_t planned_young_evac = _mixed_evac_cset->get_young_bytes_reserved_for_evacuation();\n+    size_t consumed_from_young_cset = (size_t) (planned_young_evac * ShenandoahEvacWaste);\n+    size_t available_to_loan_from_young_reserve = ((consumed_from_young_cset >= max_young_cset)?\n+                                                   0: max_young_cset - consumed_from_young_cset);\n+    size_t region_size_bytes = ShenandoahHeapRegion::region_size_bytes();\n+    if ((young_unaffiliated_regions == 0) || (available_to_loan_from_young_reserve < region_size_bytes)) {\n+      return false;\n+    } else {\n+      size_t regions_for_old_expansion = (available_to_loan_from_young_reserve \/ region_size_bytes);\n+      if (regions_for_old_expansion > young_unaffiliated_regions) {\n+        regions_for_old_expansion = young_unaffiliated_regions;\n+      }\n+      log_info(gc)(\"Augmenting old-gen evacuation budget from unexpended young-generation reserve by %zu regions\",\n+                   regions_for_old_expansion);\n+      _heap->generation_sizer()->force_transfer_to_old(regions_for_old_expansion);\n+      size_t budget_supplement = region_size_bytes * regions_for_old_expansion;\n+      size_t supplement_after_waste = (size_t) (((double) budget_supplement) \/ ShenandoahOldEvacWaste);\n+      _old_evacuation_budget += supplement_after_waste;\n+      _unfragmented_available += supplement_after_waste;\n+\n+      _old_generation->augment_evacuation_reserve(budget_supplement);\n+      young_generation->set_evacuation_reserve(max_young_cset - budget_supplement);\n+\n+      return add_old_regions_to_cset();\n+    }\n+  }\n+}\n+\n@@ -323,1 +366,3 @@\n-\n+#ifdef ASSERT\n+  bool reclaimed_immediate = false;\n+#endif\n@@ -336,4 +381,4 @@\n-        \/\/ Only place regular or pinned regions with live data into the candidate set.\n-        \/\/ Pinned regions cannot be evacuated, but we are not actually choosing candidates\n-        \/\/ for the collection set here. That happens later during the next young GC cycle,\n-        \/\/ by which time, the pinned region may no longer be pinned.\n+      \/\/ Only place regular or pinned regions with live data into the candidate set.\n+      \/\/ Pinned regions cannot be evacuated, but we are not actually choosing candidates\n+      \/\/ for the collection set here. That happens later during the next young GC cycle,\n+      \/\/ by which time, the pinned region may no longer be pinned.\n@@ -342,0 +387,8 @@\n+#ifdef ASSERT\n+        if (!reclaimed_immediate) {\n+          reclaimed_immediate = true;\n+          \/\/ Inform the free-set that old trash regions may temporarily violate OldCollector bounds\n+          shenandoah_assert_heaplocked();\n+          heap->free_set()->advise_of_old_trash();\n+        }\n+#endif\n@@ -360,0 +413,8 @@\n+#ifdef ASSERT\n+        if (!reclaimed_immediate) {\n+          reclaimed_immediate = true;\n+          \/\/ Inform the free-set that old trash regions may temporarily violate OldCollector bounds\n+          shenandoah_assert_heaplocked();\n+          heap->free_set()->advise_of_old_trash();\n+        }\n+#endif\n@@ -538,0 +599,1 @@\n+  _live_bytes_in_unprocessed_candidates = 0;\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/heuristics\/shenandoahOldHeuristics.cpp","additions":210,"deletions":148,"binary":false,"changes":358,"status":"modified"},{"patch":"@@ -105,0 +105,14 @@\n+  \/\/ State variables involved in construction of a mixed-evacuation collection set.  These variables are initialized\n+  \/\/ when client code invokes prime_collection_set().  They are consulted, and sometimes modified, when client code\n+  \/\/ calls top_off_collection_set() to possibly expand the number old-gen regions in a mixed evacuation cset, and by\n+  \/\/ finalize_mixed_evacs(), which prepares the way for mixed evacuations to begin.\n+  ShenandoahCollectionSet* _mixed_evac_cset;\n+  size_t _evacuated_old_bytes;\n+  size_t _collected_old_bytes;\n+  size_t _included_old_regions;\n+  size_t _old_evacuation_reserve;\n+  size_t _old_evacuation_budget;\n+  size_t _unfragmented_available;\n+  size_t _fragmented_available;\n+  size_t _excess_fragmented_available;\n+\n@@ -125,0 +139,7 @@\n+  \/\/ This internal helper route adds as many mixed evacuation candidate regions as fit within the old-gen evacuation budget\n+  \/\/ to the collection set.  This may be called twice to prepare for any given mixed evacuation cycle, the first time with\n+  \/\/ a conservative old evacuation budget, and the second time with a larger more aggressive old evacuation budget.  Returns\n+  \/\/ true iff we need to finalize mixed evacs.  (If no regions are added to the collection set, there is no need to finalize\n+  \/\/ mixed evacuations.)\n+  bool add_old_regions_to_cset();\n+\n@@ -131,2 +152,15 @@\n-  \/\/ Return true iff the collection set is primed with at least one old-gen region.\n-  bool prime_collection_set(ShenandoahCollectionSet* set);\n+  \/\/ Initialize instance variables to support the preparation of a mixed-evacuation collection set.  Adds as many\n+  \/\/ old candidate regions into the collection set as can fit within the iniital conservative old evacuation budget.\n+  \/\/ Returns true iff we need to finalize mixed evacs.\n+  bool prime_collection_set(ShenandoahCollectionSet* collection_set);\n+\n+  \/\/ If young evacuation did not consume all of its available evacuation reserve, add as many additional mixed-\n+  \/\/ evacuation candidate regions into the collection set as will fit within this excess repurposed reserved.\n+  \/\/ Returns true iff we need to finalize mixed evacs.\n+  bool top_off_collection_set();\n+\n+  \/\/ Having added all eligible mixed-evacuation candidates to the collection set, this function updates the total count\n+  \/\/ of how much old-gen memory remains to be evacuated and adjusts the representation of old-gen regions that remain to\n+  \/\/ be evacuated, giving special attention to regions that are currently pinned.  It outputs relevant log messages and\n+  \/\/ returns true iff the collection set holds at least one unpinned mixed evacuation candidate.\n+  bool finalize_mixed_evacs();\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/heuristics\/shenandoahOldHeuristics.hpp","additions":36,"deletions":2,"binary":false,"changes":38,"status":"modified"},{"patch":"@@ -36,1 +36,1 @@\n-        : ShenandoahGenerationalHeuristics(generation) {\n+    : ShenandoahGenerationalHeuristics(generation) {\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/heuristics\/shenandoahYoungHeuristics.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -163,2 +163,0 @@\n-  assert(heap->is_concurrent_weak_root_in_progress(), \"Must be doing weak roots now\");\n-\n@@ -205,2 +203,0 @@\n-    entry_concurrent_update_refs_prepare(heap);\n-\n@@ -208,0 +204,1 @@\n+    entry_concurrent_update_refs_prepare(heap);\n@@ -228,0 +225,2 @@\n+    _abbreviated = true;\n+\n@@ -236,1 +235,0 @@\n-    _abbreviated = true;\n@@ -295,0 +293,6 @@\n+\n+  \/\/ After an abbreviated cycle, we reclaim immediate garbage.  Rebuild the freeset in order to establish\n+  \/\/ reserves for the next GC cycle.\n+  assert(_abbreviated, \"Only rebuild free set for abbreviated and old-marking cycles\");\n+  heap->rebuild_free_set(true \/*concurrent*\/);\n+\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahConcurrentGC.cpp","additions":9,"deletions":5,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -223,1 +223,1 @@\n-  shenandoah_assert_heaplocked();\n+  shenandoah_assert_heaplocked_or_safepoint();\n@@ -350,0 +350,27 @@\n+inline void ShenandoahRegionPartitions::adjust_interval_for_recycled_old_region_under_lock(ShenandoahHeapRegion* r) {\n+  assert(!r->is_trash() && (r->free() == _region_size_bytes), \"Bad argument\");\n+\n+  shenandoah_assert_heaplocked();\n+  idx_t idx = (idx_t) r->index();\n+  ShenandoahFreeSetPartitionId old_partition = ShenandoahFreeSetPartitionId::OldCollector;\n+  assert(_membership[int(old_partition)].is_set(idx), \"Region should be in OldCollector reserve\");\n+\n+  \/\/ Note that a recycled old trashed region may be in any one of the free set partitions according to the following scenarios:\n+  \/\/  1. The old region had already been retired, so it was NotFree, and we have not rebuilt free set, so region is still NotFree\n+  \/\/  2. We recycled the region but we have not yet rebuilt the free set, so it is still in the OldCollector region.\n+  \/\/  3. We have found regions with alloc capacity but have not yet reserved_regions, so this is in Mutator set, and\n+  \/\/     the act of placing the region into the Mutator set properly adjusts interval for Mutator set.\n+  \/\/  4. During reserve_regions(), we moved this region into the Collector set, and the act of placing this region into\n+  \/\/     Collector set properly adjusts the interval for the Collector set.\n+  \/\/  5. During reserve_regions, we moved this region into the OldCollector set, and the act of placing this region into\n+  \/\/     OldCollector set properly adjusts the interval for the OldCollector set.\n+  \/\/ Only case 2 needs to be fixed up here.\n+  assert(_leftmosts[int(old_partition)] <= idx && _rightmosts[int(old_partition)] >= idx, \"sanity\");\n+  if (_leftmosts_empty[int(old_partition)] > idx) {\n+    _leftmosts_empty[int(old_partition)] = idx;\n+  }\n+  if (_rightmosts_empty[int(old_partition)] < idx) {\n+    _rightmosts_empty[int(old_partition)] = idx;\n+  }\n+}\n+\n@@ -388,0 +415,1 @@\n+  shenandoah_assert_heaplocked_or_safepoint();\n@@ -598,1 +626,1 @@\n-void ShenandoahRegionPartitions::assert_bounds() {\n+void ShenandoahRegionPartitions::assert_bounds(bool old_trash_not_in_bounds) {\n@@ -624,0 +652,21 @@\n+        ShenandoahHeapRegion* r = ShenandoahHeap::heap()->get_region(i);\n+\n+        \/\/ When old_trash_not_in_bounds, an old trashed region might reside in:\n+        \/\/ 1. NotFree if the region had already been retired\n+        \/\/ 2. OldCollector because the region was originally in OldCollector when it was identified as immediate garbage, or\n+        \/\/ 3. Mutator because we have run find_regions_with_alloc_capacity(), or\n+        \/\/ 4. Collector because reserve_regions moved from Mutator to Collector but we have not yet recycled the trash\n+        \/\/ 5. OldCollector because reserve_regions moved from Mutator to OldCollector but we have not yet recycled the trash\n+\n+        \/\/ In case 1, there is no issue with empty-free intervals.\n+        \/\/ In cases 3 - 5, there is no issue with empty-free intervals because the act of moving the region into the partition\n+        \/\/    causes the empty-free interval to be updated.\n+        \/\/ Only in case 2 do we need to disable the assert checking, but it is difficult to distinguish case 2 from case 5,\n+        \/\/    so we do not assert bounds for case 2 or case 5.\n+\n+        if (old_trash_not_in_bounds && (partition == ShenandoahFreeSetPartitionId::OldCollector) && r->is_old() && r->is_trash()) {\n+          \/\/ If Old trash has been identified but we have not yet rebuilt the freeset to acount for the trashed regions,\n+          \/\/ or if old trash has not yet been recycled, do not expect these trash regions to be within the OldCollector\n+          \/\/ partition's bounds.\n+          continue;\n+        }\n@@ -671,6 +720,6 @@\n-  assert (beg_off >= leftmost_empty(ShenandoahFreeSetPartitionId::Mutator),\n-          \"free empty regions before the leftmost: %zd, bound %zd\",\n-          beg_off, leftmost_empty(ShenandoahFreeSetPartitionId::Mutator));\n-  assert (end_off <= rightmost_empty(ShenandoahFreeSetPartitionId::Mutator),\n-          \"free empty regions past the rightmost: %zd, bound %zd\",\n-          end_off, rightmost_empty(ShenandoahFreeSetPartitionId::Mutator));\n+  assert (beg_off >= _leftmosts_empty[int(ShenandoahFreeSetPartitionId::Mutator)],\n+          \"free empty region (%zd) before the leftmost bound %zd\",\n+          beg_off, _leftmosts_empty[int(ShenandoahFreeSetPartitionId::Mutator)]);\n+  assert (end_off <= _rightmosts_empty[int(ShenandoahFreeSetPartitionId::Mutator)],\n+          \"free empty region (%zd) past the rightmost bound %zd\",\n+          end_off, _rightmosts_empty[int(ShenandoahFreeSetPartitionId::Mutator)]);\n@@ -705,2 +754,2 @@\n-          \"free empty regions before the leftmost: %zd, bound %zd\",\n-          beg_off, leftmost_empty(ShenandoahFreeSetPartitionId::Collector));\n+          \"free empty region (%zd) before the leftmost bound %zd\",\n+          beg_off, _leftmosts_empty[int(ShenandoahFreeSetPartitionId::Collector)]);\n@@ -708,2 +757,2 @@\n-          \"free empty regions past the rightmost: %zd, bound %zd\",\n-          end_off, rightmost_empty(ShenandoahFreeSetPartitionId::Collector));\n+          \"free empty region (%zd) past the rightmost bound %zd\",\n+          end_off, _rightmosts_empty[int(ShenandoahFreeSetPartitionId::Collector)]);\n@@ -726,1 +775,11 @@\n-  \/\/ If OldCollector partition is empty, leftmosts will both equal max, rightmosts will both equal zero.\n+  \/\/ Concurrent recycling of trash first recycles a region (changing its state from is_trash to is_empty without the heap lock),\n+  \/\/ then it acquires the heap lock, then it adjusts the partition for the newly recycled region and releases the lock.  After\n+  \/\/ all trashed regions have been recycled, we grab the heap lock again and clear the _old_trash_not_in_bounds flag.\n+  \/\/\n+  \/\/ Bottom line: if _old_trash_not_in_bounds, the ranges of old regions detected by examination of all region states may\n+  \/\/ be larger than the spans reported by leftmosts(OldColector) and rightmosts(OldCollector) and by the spans represented\n+  \/\/ by _leftmosts_empty[OldCollector] and _rightmosts_empty[OldCollector]\n+  \/\/ \n+\n+  \/\/ If OldCollector partition is empty and !old_trash_not_in_bounds:\n+  \/\/    leftmosts will both equal max, rightmosts will both equal zero.\n@@ -730,1 +789,1 @@\n-  assert (beg_off >= leftmost(ShenandoahFreeSetPartitionId::OldCollector),\n+  assert (old_trash_not_in_bounds || (beg_off >= leftmost(ShenandoahFreeSetPartitionId::OldCollector)),\n@@ -733,1 +792,1 @@\n-  assert (end_off <= rightmost(ShenandoahFreeSetPartitionId::OldCollector),\n+  assert (old_trash_not_in_bounds || (end_off <= rightmost(ShenandoahFreeSetPartitionId::OldCollector)),\n@@ -739,6 +798,12 @@\n-  assert (beg_off >= _leftmosts_empty[int(ShenandoahFreeSetPartitionId::OldCollector)],\n-          \"free empty regions before the leftmost: %zd, bound %zd\",\n-          beg_off, leftmost_empty(ShenandoahFreeSetPartitionId::OldCollector));\n-  assert (end_off <= _rightmosts_empty[int(ShenandoahFreeSetPartitionId::OldCollector)],\n-          \"free empty regions past the rightmost: %zd, bound %zd\",\n-          end_off, rightmost_empty(ShenandoahFreeSetPartitionId::OldCollector));\n+  assert (old_trash_not_in_bounds || (beg_off >= _leftmosts_empty[int(ShenandoahFreeSetPartitionId::OldCollector)]),\n+          \"free empty region (%zd) before the leftmost bound %zd, old_trash_not_in_bounds: no, region %s trash\",\n+          beg_off, _leftmosts_empty[int(ShenandoahFreeSetPartitionId::OldCollector)],\n+          ((beg_off >= _max)? \"out of bounds is not\":\n+           (ShenandoahHeap::heap()->get_region(_leftmosts_empty[int(ShenandoahFreeSetPartitionId::OldCollector)])->is_trash()?\n+            \"is\": \"is not\")));\n+  assert (old_trash_not_in_bounds || (end_off <= _rightmosts_empty[int(ShenandoahFreeSetPartitionId::OldCollector)]),\n+          \"free empty region (%zd) past the rightmost bound %zd, old_trash_not_in_bounds: no, region %s trash\",\n+          end_off, _rightmosts_empty[int(ShenandoahFreeSetPartitionId::OldCollector)],\n+          ((end_off < 0)? \"out of bounds is not\" :\n+           (ShenandoahHeap::heap()->get_region(_rightmosts_empty[int(ShenandoahFreeSetPartitionId::OldCollector)])->is_trash()?\n+            \"is\": \"is not\")));\n@@ -751,0 +816,3 @@\n+#ifdef ASSERT\n+  _old_trash_not_in_bounds(false),\n+#endif\n@@ -1030,0 +1098,4 @@\n+  \/\/ We must call try_recycle_under_lock() even if !r->is_trash().  The reason is that if r is being recycled at this\n+  \/\/ moment by a GC worker thread, it may appear to be not trash even though it has not yet been fully recycled.  If\n+  \/\/ we proceed without waiting for the worker to finish recycling the region, the worker thread may overwrite the\n+  \/\/ region's affiliation with FREE after we set the region's affiliation to req.afiliation() below\n@@ -1031,0 +1103,9 @@\n+#ifdef ASSERT\n+  assert(!r->is_trash(), \"try_recycle_under_lock() should assure that region is recycled\");\n+  if (_old_trash_not_in_bounds &&\n+      r->is_empty() && _partitions.in_free_set(ShenandoahFreeSetPartitionId::OldCollector, r->index())) {\n+    \/\/ Note: if assertions are not enforced, there's no rush to adjust this interval.  We'll adjust the\n+    \/\/ interval when we eventually rebuild the free set.\n+    _partitions.adjust_interval_for_recycled_old_region_under_lock(r);\n+  }\n+#endif\n@@ -1032,1 +1113,0 @@\n-\n@@ -1036,0 +1116,6 @@\n+#undef KELVIN_DEBUG\n+#ifdef KELVIN_DEBUG\n+    log_info(gc)(\"Using new region (%zu) for %s (\" PTR_FORMAT \"), region affiliation: %s, req affiliation: %s\",\n+                 r->index(), ShenandoahAllocRequest::alloc_type_to_string(req.type()), p2i(&req),\n+                 r->affiliation_name(), req.affiliation_name());\n+#endif\n@@ -1167,1 +1253,3 @@\n-    _partitions.assert_bounds();\n+#ifdef ASSERT\n+    _partitions.assert_bounds(_old_trash_not_in_bounds);\n+#endif\n@@ -1242,0 +1330,4 @@\n+    \/\/ We must call try_recycle_under_lock() even if !r->is_trash().  The reason is that if r is being recycled at this\n+    \/\/ moment by a GC worker thread, it may appear to be not trash even though it has not yet been fully recycled.  If\n+    \/\/ we proceed without waiting for the worker to finish recycling the region, the worker thread may overwrite the\n+    \/\/ region's affiliation with FREE after we set the region's affiliation to req.afiliation() below\n@@ -1243,0 +1335,2 @@\n+    assert(!r->is_trash(), \"try_recycle_under_lock() should assure that region is recycled\");\n+    assert(!_partitions.in_free_set(ShenandoahFreeSetPartitionId::OldCollector, r->index()), \"Allocate contiguous in young\");\n@@ -1276,1 +1370,3 @@\n-  _partitions.assert_bounds();\n+#ifdef ASSERT\n+  _partitions.assert_bounds(_old_trash_not_in_bounds);\n+#endif\n@@ -1285,0 +1381,4 @@\n+  ShenandoahRegionPartitions* _partitions;\n+#ifdef ASSERT\n+  bool _old_trash_not_in_bounds;\n+#endif\n@@ -1286,1 +1386,10 @@\n-  ShenandoahRecycleTrashedRegionClosure(): ShenandoahHeapRegionClosure() {}\n+#ifdef ASSERT\n+  ShenandoahRecycleTrashedRegionClosure(ShenandoahRegionPartitions* partitions, bool old_trash_not_in_bounds) :\n+    ShenandoahHeapRegionClosure(),\n+    _partitions(partitions),\n+    _old_trash_not_in_bounds(old_trash_not_in_bounds) {}\n+#else\n+  ShenandoahRecycleTrashedRegionClosure(ShenandoahRegionPartitions* partitions) :\n+    ShenandoahHeapRegionClosure(),\n+    _partitions(partitions) {}\n+#endif\n@@ -1289,1 +1398,15 @@\n-    r->try_recycle();\n+    if (r->is_trash()) {\n+#ifdef ASSERT\n+      ShenandoahHeapLocker locker(ShenandoahHeap::heap()->lock());\n+      r->try_recycle_under_lock();\n+      assert(!r->is_trash(), \"try_recycle_under_lock() should assure that region is recycled\");\n+      if (_old_trash_not_in_bounds &&\n+          r->is_empty() && _partitions->in_free_set(ShenandoahFreeSetPartitionId::OldCollector, r->index())) {\n+        \/\/ Note: if assertions are not enforced, there's no rush to adjust this interval.  We'll adjust the\n+        \/\/ interval when we eventually rebuild the free set.\n+        _partitions->adjust_interval_for_recycled_old_region_under_lock(r);\n+      }\n+#else\n+      r->try_recycle();\n+#endif\n+    }\n@@ -1303,2 +1426,5 @@\n-\n-  ShenandoahRecycleTrashedRegionClosure closure;\n+#ifdef ASSERT\n+  ShenandoahRecycleTrashedRegionClosure closure(&_partitions, _old_trash_not_in_bounds);\n+#else\n+  ShenandoahRecycleTrashedRegionClosure closure(&_partitions);\n+#endif\n@@ -1306,0 +1432,4 @@\n+#ifdef ASSERT\n+  ShenandoahHeapLocker locker(_heap->lock());\n+  _old_trash_not_in_bounds = false;\n+#endif\n@@ -1316,1 +1446,0 @@\n-\n@@ -1321,1 +1450,3 @@\n-    _partitions.assert_bounds();\n+#ifdef ASSERT\n+    _partitions.assert_bounds(_old_trash_not_in_bounds);\n+#endif\n@@ -1353,2 +1484,3 @@\n-\n-      _partitions.assert_bounds();\n+#ifdef ASSERT\n+      _partitions.assert_bounds(_old_trash_not_in_bounds);\n+#endif\n@@ -1378,2 +1510,3 @@\n-  _partitions.assert_bounds();\n-\n+#ifdef ASSERT\n+  _partitions.assert_bounds(_old_trash_not_in_bounds);\n+#endif\n@@ -1430,2 +1563,2 @@\n-      \/\/ Trashed regions represent regions that had been in the collection partition but have not yet been \"cleaned up\".\n-      \/\/ The cset regions are not \"trashed\" until we have finished update refs.\n+      \/\/ Trashed regions represent regions that had been in the collection set (or may have been identified as immediate garbage)\n+      \/\/ but have not yet been \"cleaned up\".  The cset regions are not \"trashed\" until we have finished update refs.\n@@ -1433,0 +1566,7 @@\n+        ShenandoahGenerationalHeap* gen_heap = ShenandoahGenerationalHeap::heap();\n+        \/\/ We're going to place this region into the Mutator set.  We increment old_cset_regions because this count represents\n+        \/\/ regions that the old generation is entitled to without any transfer from young.  We do not place this region into\n+        \/\/ the OldCollector partition at this time.  Instead, we let reserve_regions() decide whether to place this region\n+        \/\/ into the OldCollector partition.  Deferring the decision allows reserve_regions() to more effectively pack the\n+        \/\/ OldCollector regions into high-address memory.  We do not adjust capacities of old and young generations at this\n+        \/\/ time.  At the end of finish_rebuild(), the capacities are adjusted based on the results of reserve_regions().\n@@ -1453,1 +1593,1 @@\n-          \/\/ Both young and old collected regions (trashed) are placed into the Mutator set\n+          \/\/ Both young and old (possibly immediately) collected regions (trashed) are placed into the Mutator set\n@@ -1590,1 +1730,3 @@\n-      ShenandoahGenerationalHeap::cast(_heap)->generation_sizer()->transfer_to_young(old_collector_regions);\n+      \/\/ A conditional transfer of heap regions may fail if soft_max_capacity is smaller than max_capacity, such as\n+      \/\/ when NewRatio has been specified on the command line.  Force the transfer here.\n+      ShenandoahGenerationalHeap::cast(_heap)->generation_sizer()->force_transfer_to_young(old_collector_regions);\n@@ -1611,0 +1753,5 @@\n+void ShenandoahFreeSet::rebuild() {\n+  size_t young_cset_regions, old_cset_regions, first_old_region, last_old_region, old_region_count;\n+  prepare_to_rebuild(young_cset_regions, old_cset_regions, first_old_region, last_old_region, old_region_count);\n+  finish_rebuild(young_cset_regions, old_cset_regions, old_region_count);\n+}\n@@ -1636,0 +1783,1 @@\n+\n@@ -1653,2 +1801,1 @@\n-void ShenandoahFreeSet::finish_rebuild(size_t young_cset_regions, size_t old_cset_regions, size_t old_region_count,\n-                                       bool have_evacuation_reserves) {\n+void ShenandoahFreeSet::finish_rebuild(size_t young_cset_regions, size_t old_cset_regions, size_t old_region_count) {\n@@ -1659,2 +1806,1 @@\n-    compute_young_and_old_reserves(young_cset_regions, old_cset_regions, have_evacuation_reserves,\n-                                   young_reserve, old_reserve);\n+    compute_young_and_old_reserves(young_cset_regions, old_cset_regions, young_reserve, old_reserve);\n@@ -1666,1 +1812,1 @@\n-  \/\/ Move some of the mutator regions in the Collector and OldCollector partitions in order to satisfy\n+  \/\/ Move some of the mutator regions into the Collector and OldCollector partitions in order to satisfy\n@@ -1672,1 +1818,3 @@\n-  _partitions.assert_bounds();\n+#ifdef ASSERT\n+  _partitions.assert_bounds(_old_trash_not_in_bounds);\n+#endif\n@@ -1674,0 +1822,32 @@\n+  \/\/ Even though we have finished rebuild, old trashed regions may not yet have been recycled, so leave\n+  \/\/ _old_trash_not_in_bounds as is.  Following rebuild, old trashed regions may reside in Mutator, Collector,\n+  \/\/ or OldCollector partitions.\n+}\n+\n+\n+\/\/ Reduce old reserve (when there are insufficient resources to satisfy the original request).\n+void ShenandoahFreeSet::reduce_old_reserve(size_t adjusted_old_reserve, size_t requested_old_reserve) {\n+  ShenandoahOldGeneration* const old_generation = _heap->old_generation();\n+  size_t requested_promoted_reserve = old_generation->get_promoted_reserve();\n+  size_t requested_old_evac_reserve = old_generation->get_evacuation_reserve();\n+  assert(adjusted_old_reserve < requested_old_reserve, \"Only allow reduction\");\n+  assert(requested_promoted_reserve + requested_old_evac_reserve >= adjusted_old_reserve, \"Sanity\");\n+  size_t delta = requested_old_reserve - adjusted_old_reserve;\n+\n+  if (requested_promoted_reserve >= delta) {\n+    requested_promoted_reserve -= delta;\n+    old_generation->set_promoted_reserve(requested_promoted_reserve);\n+  } else {\n+    delta -= requested_promoted_reserve;\n+    requested_promoted_reserve = 0;\n+    requested_old_evac_reserve -= delta;\n+    old_generation->set_promoted_reserve(requested_promoted_reserve);\n+    old_generation->set_evacuation_reserve(requested_old_evac_reserve);\n+  }\n+}\n+\n+\/\/ Reduce young reserve (when there are insufficient resources to satisfy the original request).\n+void ShenandoahFreeSet::reduce_young_reserve(size_t adjusted_young_reserve, size_t requested_young_reserve) {\n+  ShenandoahYoungGeneration* const young_generation = _heap->young_generation();\n+  assert(adjusted_young_reserve < requested_young_reserve, \"Only allow reduction\");\n+  young_generation->set_evacuation_reserve(adjusted_young_reserve);\n@@ -1677,1 +1857,0 @@\n-                                                       bool have_evacuation_reserves,\n@@ -1683,1 +1862,1 @@\n-  size_t old_available = old_generation->available();\n+  size_t old_available = old_generation->available() + old_cset_regions * region_size_bytes;\n@@ -1693,0 +1872,10 @@\n+  assert(young_capacity >= (young_generation->used() + young_generation->get_humongous_waste()),\n+         \"Young capacity (%zu) must exceed used (%zu) plus humongous waste (%zu)\",\n+         young_capacity, young_generation->used(), young_generation->get_humongous_waste());\n+\n+  size_t young_available = young_capacity - (young_generation->used() + young_generation->get_humongous_waste());\n+  young_available += young_cset_regions * region_size_bytes;\n+\n+  assert(young_available >= young_unaffiliated_regions * region_size_bytes, \"sanity\");\n+  assert(old_available >= old_unaffiliated_regions * region_size_bytes, \"sanity\");\n+\n@@ -1708,0 +1897,1 @@\n+    young_available += xfer_bytes;\n@@ -1716,17 +1906,8 @@\n-  if (have_evacuation_reserves) {\n-    \/\/ We are rebuilding at the end of final mark, having already established evacuation budgets for this GC pass.\n-    const size_t promoted_reserve = old_generation->get_promoted_reserve();\n-    const size_t old_evac_reserve = old_generation->get_evacuation_reserve();\n-    young_reserve_result = young_generation->get_evacuation_reserve();\n-    old_reserve_result = promoted_reserve + old_evac_reserve;\n-    assert(old_reserve_result <= old_available,\n-           \"Cannot reserve (%zu + %zu) more OLD than is available: %zu\",\n-           promoted_reserve, old_evac_reserve, old_available);\n-  } else {\n-    \/\/ We are rebuilding at end of GC, so we set aside budgets specified on command line (or defaults)\n-    young_reserve_result = (young_capacity * ShenandoahEvacReserve) \/ 100;\n-    \/\/ The auto-sizer has already made old-gen large enough to hold all anticipated evacuations and promotions.\n-    \/\/ Affiliated old-gen regions are already in the OldCollector free set.  Add in the relevant number of\n-    \/\/ unaffiliated regions.\n-    old_reserve_result = old_available;\n-  }\n+\n+  const size_t promoted_reserve = old_generation->get_promoted_reserve();\n+  const size_t old_evac_reserve = old_generation->get_evacuation_reserve();\n+  young_reserve_result = young_generation->get_evacuation_reserve();\n+  old_reserve_result = promoted_reserve + old_evac_reserve;\n+  assert(old_reserve_result + young_reserve_result <= old_available + young_available,\n+         \"Cannot reserve (%zu + %zu + %zu) more than is available: %zu + %zu\",\n+         promoted_reserve, old_evac_reserve, young_reserve_result, old_available, young_available);\n@@ -1739,1 +1920,1 @@\n-      _partitions.capacity_of(ShenandoahFreeSetPartitionId::OldCollector) + old_unaffiliated_regions * region_size_bytes) {\n+      _partitions.available_in(ShenandoahFreeSetPartitionId::OldCollector) + old_unaffiliated_regions * region_size_bytes) {\n@@ -1741,1 +1922,1 @@\n-      _partitions.capacity_of(ShenandoahFreeSetPartitionId::OldCollector) + old_unaffiliated_regions * region_size_bytes;\n+      _partitions.available_in(ShenandoahFreeSetPartitionId::OldCollector) + old_unaffiliated_regions * region_size_bytes;\n@@ -1817,10 +1998,11 @@\n-  if (LogTarget(Info, gc, free)::is_enabled()) {\n-    size_t old_reserve = _partitions.available_in(ShenandoahFreeSetPartitionId::OldCollector);\n-    if (old_reserve < to_reserve_old) {\n-      log_info(gc, free)(\"Wanted \" PROPERFMT \" for old reserve, but only reserved: \" PROPERFMT,\n-                         PROPERFMTARGS(to_reserve_old), PROPERFMTARGS(old_reserve));\n-    }\n-    size_t reserve = _partitions.available_in(ShenandoahFreeSetPartitionId::Collector);\n-    if (reserve < to_reserve) {\n-      log_info(gc, free)(\"Wanted \" PROPERFMT \" for young reserve, but only reserved: \" PROPERFMT,\n-                          PROPERFMTARGS(to_reserve), PROPERFMTARGS(reserve));\n+  size_t old_reserve = _partitions.capacity_of(ShenandoahFreeSetPartitionId::OldCollector);\n+  if (old_reserve < to_reserve_old) {\n+    assert(_heap->mode()->is_generational(), \"to_old_reserve > 0 implies generational mode\");\n+    reduce_old_reserve(old_reserve, to_reserve_old);\n+    log_info(gc, free)(\"Wanted \" PROPERFMT \" for old reserve, but only reserved: \" PROPERFMT,\n+                       PROPERFMTARGS(to_reserve_old), PROPERFMTARGS(old_reserve));\n+  }\n+  size_t young_reserve = _partitions.capacity_of(ShenandoahFreeSetPartitionId::Collector);\n+  if (young_reserve < to_reserve) {\n+    if (_heap->mode()->is_generational()) {\n+      reduce_young_reserve(young_reserve, to_reserve);\n@@ -1828,0 +2010,2 @@\n+    log_info(gc, free)(\"Wanted \" PROPERFMT \" for young reserve, but only reserved: \" PROPERFMT,\n+                       PROPERFMTARGS(to_reserve), PROPERFMTARGS(young_reserve));\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahFreeSet.cpp","additions":259,"deletions":75,"binary":false,"changes":334,"status":"modified"},{"patch":"@@ -154,0 +154,4 @@\n+  \/\/ For recycled region r in the OldCollector partition but possibly not within the interval for empty OldCollector regions,\n+  \/\/ expand the empty interval to include this region.\n+  inline void adjust_interval_for_recycled_old_region_under_lock(ShenandoahHeapRegion* r);\n+\n@@ -254,0 +258,1 @@\n+    shenandoah_assert_heaplocked_or_safepoint();\n@@ -261,0 +266,1 @@\n+    shenandoah_assert_heaplocked_or_safepoint();\n@@ -267,0 +273,1 @@\n+#ifdef ASSERT\n@@ -287,1 +294,2 @@\n-  void assert_bounds() NOT_DEBUG_RETURN;\n+  void assert_bounds(bool old_trash_not_in_bounds) NOT_DEBUG_RETURN;\n+#endif\n@@ -326,0 +334,4 @@\n+#ifdef ASSERT\n+  bool _old_trash_not_in_bounds;\n+#endif\n+\n@@ -410,0 +422,3 @@\n+  void reduce_young_reserve(size_t adjusted_young_reserve, size_t requested_young_reserve);\n+  void reduce_old_reserve(size_t adjusted_old_reserve, size_t requested_old_reserve);\n+\n@@ -424,0 +439,6 @@\n+  \/\/ Rebuild the free set.  This combines the effects of prepare_to_rebuild() and finish_rebuild() with no intervening\n+  \/\/ efforts to rebalance generation sizes.  When the free set is rebuild, we reserve sufficient memory within the\n+  \/\/ collector partition (and, for generational mode, the old collector partition) based on the amount reserved\n+  \/\/ by heuristics to support the next planned evacuation effort.\n+  void rebuild();\n+\n@@ -435,12 +456,4 @@\n-  \/\/ hold the results of evacuating to young-gen and to old-gen, and have_evacuation_reserves should be true.\n-  \/\/ These quantities, stored as reserves for their respective generations, are consulted prior to rebuilding\n-  \/\/ the free set (ShenandoahFreeSet) in preparation for evacuation.  When the free set is rebuilt, we make sure\n-  \/\/ to reserve sufficient memory in the collector and old_collector sets to hold evacuations.\n-  \/\/\n-  \/\/ We also rebuild the free set at the end of GC, as we prepare to idle GC until the next trigger.  In this case,\n-  \/\/ have_evacuation_reserves is false because we don't yet know how much memory will need to be evacuated in the\n-  \/\/ next GC cycle.  When have_evacuation_reserves is false, the free set rebuild operation reserves for the collector\n-  \/\/ and old_collector sets based on alternative mechanisms, such as ShenandoahEvacReserve, ShenandoahOldEvacReserve, and\n-  \/\/ ShenandoahOldCompactionReserve.  In a future planned enhancement, the reserve for old_collector set when the\n-  \/\/ evacuation reserves are unknown, is based in part on anticipated promotion as determined by analysis of live data\n-  \/\/ found during the previous GC pass which is one less than the current tenure age.\n+  \/\/ hold the results of evacuating to young-gen and to old-gen.  These quantities, stored in reserves for their,\n+  \/\/ respective generations, are consulted prior to rebuilding the free set (ShenandoahFreeSet) in preparation for\n+  \/\/ evacuation.  When the free set is rebuilt, we make sure to reserve sufficient memory in the collector and\n+  \/\/ old_collector sets to hold evacuations.\n@@ -451,5 +464,1 @@\n-  \/\/ have_evacuation_reserves is true iff the desired values of young-gen and old-gen evacuation reserves and old-gen\n-  \/\/                    promotion reserve have been precomputed (and can be obtained by invoking\n-  \/\/                    <generation>->get_evacuation_reserve() or old_gen->get_promoted_reserve()\n-  void finish_rebuild(size_t young_cset_regions, size_t old_cset_regions, size_t num_old_regions,\n-                      bool have_evacuation_reserves = false);\n+  void finish_rebuild(size_t young_cset_regions, size_t old_cset_regions, size_t num_old_regions);\n@@ -467,0 +476,8 @@\n+#ifdef ASSERT\n+  \/\/ Advise FreeSet that old trash regions have not yet been accounted for in OldCollector partition bounds\n+  void advise_of_old_trash() {\n+    shenandoah_assert_heaplocked();\n+    _old_trash_not_in_bounds = true;\n+  }\n+#endif\n+\n@@ -485,0 +502,5 @@\n+  \/\/ Use this version of available() you the heap lock is held.\n+  inline size_t available_locked() const {\n+    return _partitions.available_in(ShenandoahFreeSetPartitionId::Mutator);\n+  }\n+\n@@ -546,1 +568,1 @@\n-  void compute_young_and_old_reserves(size_t young_cset_regions, size_t old_cset_regions, bool have_evacuation_reserves,\n+  void compute_young_and_old_reserves(size_t young_cset_regions, size_t old_cset_regions,\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahFreeSet.hpp","additions":41,"deletions":19,"binary":false,"changes":60,"status":"modified"},{"patch":"@@ -242,1 +242,0 @@\n-  ShenandoahGenerationalHeap::TransferResult result;\n@@ -256,8 +255,1 @@\n-    result = phase5_epilog();\n-  }\n-  if (heap->mode()->is_generational()) {\n-    LogTarget(Info, gc, ergo) lt;\n-    if (lt.is_enabled()) {\n-      LogStream ls(lt);\n-      result.print_on(\"Full GC\", &ls);\n-    }\n+    phase5_epilog();\n@@ -537,0 +529,1 @@\n+      \/\/ No need to adjust_interval_for_recycled_old_region.  That will be taken care of during freeset rebuild.\n@@ -986,0 +979,1 @@\n+      \/\/ No need to adjust_interval_for_recycled_old_region.  That will be taken care of during freeset rebuild.\n@@ -1131,1 +1125,1 @@\n-ShenandoahGenerationalHeap::TransferResult ShenandoahFullGC::phase5_epilog() {\n+void ShenandoahFullGC::phase5_epilog() {\n@@ -1134,1 +1128,0 @@\n-  ShenandoahGenerationalHeap::TransferResult result;\n@@ -1156,11 +1149,1 @@\n-    size_t young_cset_regions, old_cset_regions;\n-    size_t first_old, last_old, num_old;\n-    heap->free_set()->prepare_to_rebuild(young_cset_regions, old_cset_regions, first_old, last_old, num_old);\n-\n-    \/\/ We also do not expand old generation size following Full GC because we have scrambled age populations and\n-    \/\/ no longer have objects separated by age into distinct regions.\n-    if (heap->mode()->is_generational()) {\n-      ShenandoahGenerationalFullGC::compute_balances();\n-    }\n-\n-    heap->free_set()->finish_rebuild(young_cset_regions, old_cset_regions, num_old);\n+    heap->free_set()->rebuild();\n@@ -1180,1 +1163,0 @@\n-    result = ShenandoahGenerationalFullGC::balance_generations_after_rebuilding_free_set();\n@@ -1183,1 +1165,0 @@\n-  return result;\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahFullGC.cpp","additions":5,"deletions":24,"binary":false,"changes":29,"status":"modified"},{"patch":"@@ -85,1 +85,1 @@\n-  ShenandoahGenerationalHeap::TransferResult phase5_epilog();\n+  void phase5_epilog();\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahFullGC.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -273,2 +273,3 @@\n-  \/\/ maximum_young_evacuation_reserve is upper bound on memory to be evacuated out of young\n-  const size_t maximum_young_evacuation_reserve = (young_generation->max_capacity() * ShenandoahEvacReserve) \/ 100;\n+  \/\/ maximum_young_evacuation_reserve is upper bound on memory to be evacuated into young Collector Reserve.  This is\n+  \/\/ bounded at the end of previous GC cycle, based on available memory and balancing of evacuation to old and young.\n+  const size_t maximum_young_evacuation_reserve = young_generation->get_evacuation_reserve();\n@@ -281,1 +282,1 @@\n-  \/\/ Let SOEP = ShenandoahOldEvacRatioPercent,\n+  \/\/ Let SOEP = ShenandoahOldEvacPercent,\n@@ -293,1 +294,1 @@\n-  assert(ShenandoahOldEvacRatioPercent <= 100, \"Error\");\n+  assert(ShenandoahOldEvacPercent <= 100, \"Error\");\n@@ -295,2 +296,2 @@\n-  const size_t maximum_old_evacuation_reserve = (ShenandoahOldEvacRatioPercent == 100) ?\n-    old_available : MIN2((maximum_young_evacuation_reserve * ShenandoahOldEvacRatioPercent) \/ (100 - ShenandoahOldEvacRatioPercent),\n+  const size_t maximum_old_evacuation_reserve = (ShenandoahOldEvacPercent == 100) ?\n+    old_available : MIN2((maximum_young_evacuation_reserve * ShenandoahOldEvacPercent) \/ (100 - ShenandoahOldEvacPercent),\n@@ -368,1 +369,0 @@\n-\/\/\n@@ -470,4 +470,1 @@\n-    bool result = ShenandoahGenerationalHeap::cast(heap)->generation_sizer()->transfer_to_young(regions_to_xfer);\n-    assert(excess_old >= regions_to_xfer * region_size_bytes,\n-           \"Cannot transfer (%zu, %zu) more than excess old (%zu)\",\n-           regions_to_xfer, region_size_bytes, excess_old);\n+    assert(excess_old >= regions_to_xfer * region_size_bytes, \"Cannot xfer more than excess old\");\n@@ -475,2 +472,3 @@\n-    log_debug(gc, ergo)(\"%s transferred %zu excess regions to young before start of evacuation\",\n-                       result? \"Successfully\": \"Unsuccessfully\", regions_to_xfer);\n+    ShenandoahGenerationalHeap::cast(heap)->generation_sizer()->force_transfer_to_young(regions_to_xfer);\n+    log_debug(gc, ergo)(\"Transferred %zu excess regions to young before start of evacuation\",\n+                        regions_to_xfer);\n@@ -718,1 +716,1 @@\n-      \/\/ place, and preselect older regions that will be promoted by evacuation.\n+      \/\/ place and preselected older regions that will be promoted by evacuation.\n@@ -721,2 +719,1 @@\n-      \/\/ Choose the collection set, including the regions preselected above for\n-      \/\/ promotion into the old generation.\n+      \/\/ Choose the collection set, including the regions preselected above for promotion into the old generation.\n@@ -724,4 +721,2 @@\n-      if (!collection_set->is_empty()) {\n-        \/\/ only make use of evacuation budgets when we are evacuating\n-        adjust_evacuation_budgets(heap, collection_set);\n-      }\n+      \/\/ Even if collection_set->is_empty(), we want to adjust budgets, making reserves available to mutator.\n+      adjust_evacuation_budgets(heap, collection_set);\n@@ -751,7 +746,1 @@\n-    size_t young_cset_regions, old_cset_regions;\n-\n-    \/\/ We are preparing for evacuation.  At this time, we ignore cset region tallies.\n-    size_t first_old, last_old, num_old;\n-    heap->free_set()->prepare_to_rebuild(young_cset_regions, old_cset_regions, first_old, last_old, num_old);\n-    \/\/ Free set construction uses reserve quantities, because they are known to be valid here\n-    heap->free_set()->finish_rebuild(young_cset_regions, old_cset_regions, num_old, true);\n+    heap->free_set()->rebuild();\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahGeneration.cpp","additions":16,"deletions":27,"binary":false,"changes":43,"status":"modified"},{"patch":"@@ -175,0 +175,16 @@\n+  \/\/ Upon return from prepare_regions_and_collection_set(), certain parameters have been established to govern the\n+  \/\/ evacuation efforts that are about to begin.  In particular:\n+  \/\/\n+  \/\/ old_generation->get_promoted_reserve() represents the amount of memory within old-gen's available memory that has\n+  \/\/   been set aside to hold objects promoted from young-gen memory.  This represents an estimated percentage\n+  \/\/   of the live young-gen memory within the collection set.  If there is more data ready to be promoted than\n+  \/\/   can fit within this reserve, the promotion of some objects will be deferred until a subsequent evacuation\n+  \/\/   pass.\n+  \/\/\n+  \/\/ old_generation->get_evacuation_reserve() represents the amount of memory within old-gen's available memory that has been\n+  \/\/  set aside to hold objects evacuated from the old-gen collection set.\n+  \/\/\n+  \/\/ young_generation->get_evacuation_reserve() represents the amount of memory within young-gen's available memory that has\n+  \/\/  been set aside to hold objects evacuated from the young-gen collection set.  Conservatively, this value\n+  \/\/  equals the entire amount of live young-gen memory within the collection set, even though some of this memory\n+  \/\/  will likely be promoted.\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahGeneration.hpp","additions":16,"deletions":0,"binary":false,"changes":16,"status":"modified"},{"patch":"@@ -140,2 +140,2 @@\n-  log_info(gc, ergo)(\"Transfer %zu region(s) from %s to %s, yielding increased size: \" PROPERFMT,\n-                     regions, src->name(), dst->name(), PROPERFMTARGS(new_size));\n+  log_develop_debug(gc, ergo)(\"Transfer %zu region(s) from %s to %s, yielding increased size: \" PROPERFMT,\n+                              regions, src->name(), dst->name(), PROPERFMTARGS(new_size));\n@@ -181,0 +181,14 @@\n+\n+void ShenandoahGenerationSizer::force_transfer_to_old(size_t regions) const {\n+  ShenandoahGenerationalHeap* heap = ShenandoahGenerationalHeap::heap();\n+  ShenandoahGeneration* old_gen = heap->old_generation();\n+  ShenandoahGeneration* young_gen = heap->young_generation();\n+  const size_t bytes_to_transfer = regions * ShenandoahHeapRegion::region_size_bytes();\n+\n+  young_gen->decrease_capacity(bytes_to_transfer);\n+  old_gen->increase_capacity(bytes_to_transfer);\n+  const size_t new_size = old_gen->max_capacity();\n+  log_develop_debug(gc, ergo)(\"Forcing transfer of %zu region(s) from %s to %s, yielding increased size: \" PROPERFMT,\n+                              regions, young_gen->name(), old_gen->name(), PROPERFMTARGS(new_size));\n+}\n+\n@@ -183,1 +197,1 @@\n-void ShenandoahGenerationSizer::force_transfer_to_old(size_t regions) const {\n+void ShenandoahGenerationSizer::promote_regions_in_place(size_t regions) const {\n@@ -192,2 +206,2 @@\n-  log_info(gc, ergo)(\"Forcing transfer of %zu region(s) from %s to %s, yielding increased size: \" PROPERFMT,\n-                     regions, young_gen->name(), old_gen->name(), PROPERFMTARGS(new_size));\n+  log_develop_debug(gc, ergo)(\"Promoting %zu regions in place, changing affiliations from %s to %s, yielding size: \" PROPERFMT,\n+                              regions, young_gen->name(), old_gen->name(), PROPERFMTARGS(new_size));\n@@ -196,0 +210,14 @@\n+\/\/ This is used to transfer excess old-gen regions to young at the start of evacuation after collection set is determined.\n+void ShenandoahGenerationSizer::force_transfer_to_young(size_t regions) const {\n+  ShenandoahGenerationalHeap* heap = ShenandoahGenerationalHeap::heap();\n+  ShenandoahGeneration* old_gen = heap->old_generation();\n+  ShenandoahGeneration* young_gen = heap->young_generation();\n+  const size_t bytes_to_transfer = regions * ShenandoahHeapRegion::region_size_bytes();\n+\n+  young_gen->increase_capacity(bytes_to_transfer);\n+  old_gen->decrease_capacity(bytes_to_transfer);\n+  const size_t new_size = young_gen->max_capacity();\n+  log_develop_debug(gc, ergo)\n+    (\"Rebalancing regions after rebuild free set, moving %zu region(s) from %s to %s, yielding increased size: \" PROPERFMT,\n+     regions, old_gen->name(), young_gen->name(), PROPERFMTARGS(new_size));\n+}\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahGenerationSizer.cpp","additions":33,"deletions":5,"binary":false,"changes":38,"status":"modified"},{"patch":"@@ -89,1 +89,6 @@\n-  \/\/ force transfer is used when we promote humongous objects.  May violate min\/max limits on generation sizes\n+  \/\/ Use this to adjust generation sizes when we promote humongous objects or promote regular regions in place.\n+  \/\/ May violate min\/max limits on generation sizes.\n+  void promote_regions_in_place(size_t regions) const;\n+\n+  \/\/ Force transfer is used to adjust accounting of regions in generations after rebuilding free set.\n+  \/\/ May violate min\/max limits on generation sizes.\n@@ -91,0 +96,4 @@\n+\n+  \/\/ Force transfer is used when we have excess old and we have confirmed that old unaffiliated >= regions.\n+  \/\/ May violate min\/max limits on generation sizes.\n+  void force_transfer_to_young(size_t regions) const;\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahGenerationSizer.hpp","additions":10,"deletions":1,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -247,2 +247,2 @@\n-    \/\/ transfer_to_old() increases capacity of old and decreases capacity of young\n-    _heap->generation_sizer()->force_transfer_to_old(1);\n+    \/\/ promote_regions_in_place() increases capacity of old and decreases capacity of young\n+    _heap->generation_sizer()->promote_regions_in_place(1);\n@@ -289,2 +289,2 @@\n-    \/\/ transfer_to_old() increases capacity of old and decreases capacity of young\n-    _heap->generation_sizer()->force_transfer_to_old(spanned_regions);\n+    \/\/ promote_regions_in_place() increases capacity of old and decreases capacity of young\n+    _heap->generation_sizer()->promote_regions_in_place(spanned_regions);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahGenerationalEvacuationTask.cpp","additions":4,"deletions":4,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -59,3 +59,0 @@\n-  \/\/ No need for old_gen->increase_used() as this was done when plabs were allocated.\n-  heap->reset_generation_reserves();\n-\n@@ -108,0 +105,3 @@\n+\/\/ Full GC has scattered aged objects throughout the heap.  There are no more aged regions, so there is no anticipated\n+\/\/ promotion.  Furthermore, Full GC has cancelled any ongoing mixed evacuation efforts so there are no anticipated old-gen\n+\/\/ evacuations.  Size old-gen to represent its current usage by setting the balance.  This feeds into rebuild of freeset.\n@@ -118,0 +118,1 @@\n+  ssize_t region_balance;\n@@ -120,1 +121,2 @@\n-    gen_heap->generation_sizer()->transfer_to_young(excess_old_regions);\n+    \/\/ Since the act of FullGC does not honor old and young budgets, excess_old_regions are conceptually unaffiliated.\n+    region_balance = checked_cast<ssize_t>(excess_old_regions);\n@@ -122,0 +124,2 @@\n+    \/\/ Since the old_usage already consumes more regions than in old_capacity, we know these regions are not affiliated young,\n+    \/\/ so arrange to transfer them.\n@@ -123,1 +127,20 @@\n-    gen_heap->generation_sizer()->force_transfer_to_old(old_regions_deficit);\n+    region_balance = 0 - checked_cast<ssize_t>(old_regions_deficit);\n+  } else {\n+    region_balance = 0;\n+  }\n+  old_gen->set_region_balance(region_balance);\n+  \/\/ Rebuild free set will log adjustments to generation sizes.\n+\n+  ShenandoahYoungGeneration* const young_gen = gen_heap->young_generation();\n+  size_t anticipated_young_capacity = young_gen->max_capacity() + region_balance * ShenandoahHeapRegion::region_size_bytes();\n+  size_t young_usage = young_gen->used_regions_size();\n+  assert(anticipated_young_capacity >= young_usage, \"sanity\");\n+\n+  size_t anticipated_max_collector_reserve = anticipated_young_capacity - young_usage;\n+  size_t desired_collector_reserve = (anticipated_young_capacity * ShenandoahEvacReserve) \/ 100;\n+  size_t young_reserve;\n+  if (desired_collector_reserve > anticipated_max_collector_reserve) {\n+    \/\/ Trigger next concurrent GC immediately\n+    young_reserve = anticipated_max_collector_reserve;\n+  } else {\n+    young_reserve = desired_collector_reserve;\n@@ -126,4 +149,2 @@\n-  log_info(gc, ergo)(\"FullGC done: young usage: \" PROPERFMT \", old usage: \" PROPERFMT,\n-               PROPERFMTARGS(gen_heap->young_generation()->used()),\n-               PROPERFMTARGS(old_gen->used()));\n-}\n+  size_t reserve_for_promo = 0;\n+  size_t reserve_for_mixed = 0;\n@@ -131,2 +152,4 @@\n-ShenandoahGenerationalHeap::TransferResult ShenandoahGenerationalFullGC::balance_generations_after_rebuilding_free_set() {\n-  return ShenandoahGenerationalHeap::heap()->balance_generations();\n+  \/\/ Reserves feed into rebuild calculations\n+  young_gen->set_evacuation_reserve(young_reserve);\n+  old_gen->set_evacuation_reserve(reserve_for_mixed);\n+  old_gen->set_promoted_reserve(reserve_for_promo);\n@@ -182,9 +205,0 @@\n-void ShenandoahGenerationalFullGC::compute_balances() {\n-  auto heap = ShenandoahGenerationalHeap::heap();\n-\n-  \/\/ In case this Full GC resulted from degeneration, clear the tally on anticipated promotion.\n-  heap->old_generation()->set_promotion_potential(0);\n-  \/\/ Invoke this in case we are able to transfer memory from OLD to YOUNG.\n-  heap->compute_old_generation_balance(0, 0);\n-}\n-\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahGenerationalFullGC.cpp","additions":34,"deletions":20,"binary":false,"changes":54,"status":"modified"},{"patch":"@@ -59,8 +59,0 @@\n-  \/\/ Rebuilding the free set may have resulted in regions being pulled in to the old generation\n-  \/\/ evacuation reserve. For this reason, we must update the usage and capacity of the generations\n-  \/\/ again. In the distant past, the free set did not know anything about generations, so we had\n-  \/\/ a layer built above it to represent how much young\/old memory was available. This layer is\n-  \/\/ redundant and adds complexity. We would like to one day remove it. Until then, we must keep it\n-  \/\/ synchronized with the free set's view of things.\n-  static ShenandoahGenerationalHeap::TransferResult balance_generations_after_rebuilding_free_set();\n-\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahGenerationalFullGC.hpp","additions":0,"deletions":8,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -238,1 +238,1 @@\n-                                        ShenandoahAffiliation target_gen) {\n+                                                    ShenandoahAffiliation target_gen) {\n@@ -298,1 +298,7 @@\n-      if (!is_promotion || !has_plab || (size > PLAB::min_size())) {\n+\n+      \/\/ Reduce, but do not totally eliminate promotion by shared allocation.  Shared allocations are normally\n+      \/\/ not a good thing.  Usually is much better to evacuate into a young-gen GCLAB than promote to old-gen with a\n+      \/\/ shared allocation.  Objects above a particular threshold size (6 * min-size) are considered to be worth the\n+      \/\/ effort required to promote by shared allocation.\n+      static size_t size_threshhold = MIN2(PLAB::max_size(), PLAB::min_size() * 6);\n+      if (!is_promotion || !has_plab || (size > size_threshhold)) {\n@@ -304,1 +310,1 @@\n-      \/\/ We choose not to promote objects smaller than PLAB::min_size() by way of shared allocations, as this is too\n+      \/\/ We choose not to promote objects smaller than size_threshold by way of shared allocations as this is too\n@@ -306,1 +312,1 @@\n-      \/\/ evacuation pass.  This condition is denoted by: is_promotion && has_plab && (size <= PLAB::min_size())\n+      \/\/ evacuation pass.  This condition is denoted by: is_promotion && has_plab && (size <= size_threshhold).\n@@ -584,29 +590,0 @@\n-ShenandoahGenerationalHeap::TransferResult ShenandoahGenerationalHeap::balance_generations() {\n-  shenandoah_assert_heaplocked_or_safepoint();\n-\n-  ShenandoahOldGeneration* old_gen = old_generation();\n-  const ssize_t old_region_balance = old_gen->get_region_balance();\n-  old_gen->set_region_balance(0);\n-\n-  if (old_region_balance > 0) {\n-    const auto old_region_surplus = checked_cast<size_t>(old_region_balance);\n-    const bool success = generation_sizer()->transfer_to_young(old_region_surplus);\n-    return TransferResult {\n-      success, old_region_surplus, \"young\"\n-    };\n-  }\n-\n-  if (old_region_balance < 0) {\n-    const auto old_region_deficit = checked_cast<size_t>(-old_region_balance);\n-    const bool success = generation_sizer()->transfer_to_old(old_region_deficit);\n-    if (!success) {\n-      old_gen->handle_failed_transfer();\n-    }\n-    return TransferResult {\n-      success, old_region_deficit, \"old\"\n-    };\n-  }\n-\n-  return TransferResult {true, 0, \"none\"};\n-}\n-\n@@ -615,4 +592,5 @@\n-\/\/ xfer_limit, and any surplus is transferred to the young generation.\n-\/\/ xfer_limit is the maximum we're able to transfer from young to old.\n-void ShenandoahGenerationalHeap::compute_old_generation_balance(size_t old_xfer_limit, size_t old_cset_regions) {\n-\n+\/\/ mutator_xfer_limit, and any surplus is transferred to the young generation.  mutator_xfer_limit is\n+\/\/  the maximum we're able to transfer from young to old.\n+void ShenandoahGenerationalHeap::compute_old_generation_balance(size_t mutator_xfer_limit,\n+                                                                size_t old_cset_regions, size_t young_cset_regions) {\n+  shenandoah_assert_heaplocked();\n@@ -624,1 +602,1 @@\n-  \/\/ Let SOEP = ShenandoahOldEvacRatioPercent,\n+  \/\/ Let SOEP = ShenandoahOldEvacPercent,\n@@ -636,12 +614,1 @@\n-  assert(ShenandoahOldEvacRatioPercent <= 100, \"Error\");\n-  const size_t old_available = old_generation()->available();\n-  \/\/ The free set will reserve this amount of memory to hold young evacuations\n-  const size_t young_reserve = (young_generation()->max_capacity() * ShenandoahEvacReserve) \/ 100;\n-\n-  \/\/ In the case that ShenandoahOldEvacRatioPercent equals 100, max_old_reserve is limited only by xfer_limit.\n-\n-  const double bound_on_old_reserve = old_available + old_xfer_limit + young_reserve;\n-  const double max_old_reserve = (ShenandoahOldEvacRatioPercent == 100)?\n-                                 bound_on_old_reserve: MIN2(double(young_reserve * ShenandoahOldEvacRatioPercent) \/ double(100 - ShenandoahOldEvacRatioPercent),\n-                                                            bound_on_old_reserve);\n-\n+  assert(ShenandoahOldEvacPercent <= 100, \"Error\");\n@@ -650,0 +617,28 @@\n+  ShenandoahOldGeneration* old_gen = old_generation();\n+  size_t old_capacity = old_gen->max_capacity();\n+  size_t old_usage = old_gen->used_including_humongous_waste();\n+  size_t old_available = ((old_capacity >= old_usage)? old_capacity - old_usage: 0) + old_cset_regions * region_size_bytes;\n+\n+  ShenandoahYoungGeneration* young_gen = young_generation();\n+  size_t young_capacity = young_gen->max_capacity();\n+  size_t young_usage = young_gen->used_including_humongous_waste();\n+  size_t young_available = ((young_capacity >= young_usage)? young_capacity - young_usage: 0);\n+  size_t freeset_available = free_set()->available_locked();\n+  if (young_available > freeset_available) {\n+    young_available = freeset_available;\n+  }\n+  young_available += young_cset_regions * region_size_bytes;\n+\n+  \/\/ The free set will reserve this amount of memory to hold young evacuations (initialized to the ideal reserve)\n+  size_t young_reserve = (young_generation()->max_capacity() * ShenandoahEvacReserve) \/ 100;\n+\n+  \/\/ If ShenandoahOldEvacPercent equals 100, max_old_reserve is limited only by mutator_xfer_limit and young_reserve\n+  const size_t bound_on_old_reserve = ((old_available + mutator_xfer_limit + young_reserve) * ShenandoahOldEvacPercent) \/ 100;\n+  size_t proposed_max_old = ((ShenandoahOldEvacPercent == 100)?\n+                             bound_on_old_reserve:\n+                             MIN2((young_reserve * ShenandoahOldEvacPercent) \/ (100 - ShenandoahOldEvacPercent),\n+                                  bound_on_old_reserve));\n+  if (young_reserve > young_available) {\n+    young_reserve = young_available;\n+  }\n+\n@@ -651,2 +646,16 @@\n-  double reserve_for_mixed = 0;\n-  if (old_generation()->has_unprocessed_collection_candidates()) {\n+  size_t reserve_for_mixed = 0;\n+  const size_t old_fragmented_available =\n+    old_available - (old_generation()->free_unaffiliated_regions() + old_cset_regions) * region_size_bytes;\n+\n+  if (old_fragmented_available > proposed_max_old) {\n+    \/\/ After we've promoted regions in place, there may be an abundance of old-fragmented available memory,\n+    \/\/ even more than the desired percentage for old reserve.  We cannot transfer these fragmented regions back\n+    \/\/ to young.  Instead we make the best of the situation by using this fragmented memory for both promotions\n+    \/\/ and evacuations.\n+    proposed_max_old = old_fragmented_available;\n+  }\n+  size_t reserve_for_promo = old_fragmented_available;\n+  const size_t max_old_reserve = proposed_max_old;\n+  const size_t mixed_candidate_live_memory = old_generation()->unprocessed_collection_candidates_live_memory();\n+  const bool doing_mixed = (mixed_candidate_live_memory > 0);\n+  if (doing_mixed) {\n@@ -655,1 +664,1 @@\n-    const double max_evac_need = (double(old_generation()->unprocessed_collection_candidates_live_memory()) * ShenandoahOldEvacWaste);\n+    const size_t max_evac_need = (size_t) (mixed_candidate_live_memory * ShenandoahOldEvacWaste);\n@@ -658,4 +667,14 @@\n-    const double old_fragmented_available = double(old_available - old_generation()->free_unaffiliated_regions() * region_size_bytes);\n-    reserve_for_mixed = max_evac_need + old_fragmented_available;\n-    if (reserve_for_mixed > max_old_reserve) {\n-      reserve_for_mixed = max_old_reserve;\n+\n+    \/\/ We prefer to evacuate all of mixed into unfragmented memory, and will expand old in order to do so, unless\n+    \/\/ we already have too much fragmented available memory in old.\n+    reserve_for_mixed = max_evac_need;\n+    if (reserve_for_mixed + reserve_for_promo > max_old_reserve) {\n+      \/\/ In this case, we'll allow old-evac to target some of the fragmented old memory.\n+      size_t excess_reserves = (reserve_for_mixed + reserve_for_promo) - max_old_reserve;\n+      if (reserve_for_promo > excess_reserves) {\n+        reserve_for_promo -= excess_reserves;\n+      } else {\n+        excess_reserves -= reserve_for_promo;\n+        reserve_for_promo = 0;\n+        reserve_for_mixed -= excess_reserves;\n+      }\n@@ -665,2 +684,2 @@\n-  \/\/ Decide how much space we should reserve for promotions from young\n-  size_t reserve_for_promo = 0;\n+  \/\/ Decide how much additional space we should reserve for promotions from young.  We give priority to mixed evacations\n+  \/\/ over promotions.\n@@ -670,4 +689,15 @@\n-    \/\/ We're promoting and have a bound on the maximum amount that can be promoted\n-    assert(max_old_reserve >= reserve_for_mixed, \"Sanity\");\n-    const size_t available_for_promotions = max_old_reserve - reserve_for_mixed;\n-    reserve_for_promo = MIN2((size_t)(promo_load * ShenandoahPromoEvacWaste), available_for_promotions);\n+    \/\/ We've already set aside all of the fragmented available memory within old-gen to represent old objects\n+    \/\/ to be promoted from young generation.  promo_load represents the memory that we anticipate to be promoted\n+    \/\/ from regions that have reached tenure age.  In the ideal, we will always use fragmented old-gen memory\n+    \/\/ to hold individually promoted objects and will use unfragmented old-gen memory to represent the old-gen\n+    \/\/ evacuation workloa.\n+\n+    \/\/ We're promoting and have an esimate of memory to be promoted from aged regions\n+    assert(max_old_reserve >= (reserve_for_mixed + reserve_for_promo), \"Sanity\");\n+    const size_t available_for_additional_promotions = max_old_reserve - (reserve_for_mixed + reserve_for_promo);\n+    size_t promo_need = (size_t)(promo_load * ShenandoahPromoEvacWaste);\n+    if (promo_need > reserve_for_promo) {\n+      reserve_for_promo += MIN2(promo_need - reserve_for_promo, available_for_additional_promotions);\n+    }\n+    \/\/ We've already reserved all the memory required for the promo_load, and possibly more.  The excess\n+    \/\/ can be consumed by objects promoted from regions that have not yet reached tenure age.\n@@ -676,3 +706,2 @@\n-  \/\/ This is the total old we want to ideally reserve\n-  const size_t old_reserve = reserve_for_mixed + reserve_for_promo;\n-  assert(old_reserve <= max_old_reserve, \"cannot reserve more than max for old evacuations\");\n+  \/\/ This is the total old we want to reserve (initialized to the ideal reserve)\n+  size_t old_reserve = reserve_for_mixed + reserve_for_promo;\n@@ -681,2 +710,8 @@\n-  const size_t max_old_available = old_generation()->available() + old_cset_regions * region_size_bytes;\n-  if (max_old_available >= old_reserve) {\n+  size_t old_region_deficit = 0;\n+  size_t old_region_surplus = 0;\n+\n+  size_t mutator_region_xfer_limit = mutator_xfer_limit \/ region_size_bytes;\n+  \/\/ align the mutator_xfer_limit on region size\n+  mutator_xfer_limit = mutator_region_xfer_limit * region_size_bytes;\n+\n+  if (old_available >= old_reserve) {\n@@ -684,1 +719,2 @@\n-    const size_t old_surplus = (max_old_available - old_reserve) \/ region_size_bytes;\n+    const size_t old_surplus = old_available - old_reserve;\n+    old_region_surplus = old_surplus \/ region_size_bytes;\n@@ -686,1 +722,1 @@\n-    const size_t old_region_surplus = MIN2(old_surplus, unaffiliated_old_regions);\n+    old_region_surplus = MIN2(old_region_surplus, unaffiliated_old_regions);\n@@ -688,0 +724,5 @@\n+  } else if (old_available + mutator_xfer_limit >= old_reserve) {\n+    \/\/ Mutator's xfer limit is sufficient to satisfy our need: transfer all memory from there\n+    size_t old_deficit = old_reserve - old_available;\n+    old_region_deficit = (old_deficit + region_size_bytes - 1) \/ region_size_bytes;\n+    old_generation()->set_region_balance(0 - checked_cast<ssize_t>(old_region_deficit));\n@@ -689,11 +730,37 @@\n-    \/\/ We are running a deficit which we'd like to fill from young.\n-    \/\/ Ignore that this will directly impact young_generation()->max_capacity(),\n-    \/\/ indirectly impacting young_reserve and old_reserve.  These computations are conservative.\n-    \/\/ Note that deficit is rounded up by one region.\n-    const size_t old_need = (old_reserve - max_old_available + region_size_bytes - 1) \/ region_size_bytes;\n-    const size_t max_old_region_xfer = old_xfer_limit \/ region_size_bytes;\n-\n-    \/\/ Round down the regions we can transfer from young to old. If we're running short\n-    \/\/ on young-gen memory, we restrict the xfer. Old-gen collection activities will be\n-    \/\/ curtailed if the budget is restricted.\n-    const size_t old_region_deficit = MIN2(old_need, max_old_region_xfer);\n+   \/\/ We'll try to xfer from both mutator excess and from young collector reserve\n+    size_t available_reserves = old_available + young_reserve + mutator_xfer_limit;\n+    size_t old_entitlement = (available_reserves  * ShenandoahOldEvacPercent) \/ 100;\n+\n+    \/\/ Round old_entitlement down to nearest multiple of regions to be transferred to old\n+    size_t entitled_xfer = old_entitlement - old_available;\n+    entitled_xfer = region_size_bytes * (entitled_xfer \/ region_size_bytes);\n+    size_t unaffiliated_young_regions = young_generation()->free_unaffiliated_regions();\n+    size_t unaffiliated_young_memory = unaffiliated_young_regions * region_size_bytes;\n+    if (entitled_xfer > unaffiliated_young_memory) {\n+      entitled_xfer = unaffiliated_young_memory;\n+    }\n+    old_entitlement = old_available + entitled_xfer;\n+    if (old_entitlement < old_reserve) {\n+      \/\/ There's not enough memory to satisfy our desire.  Scale back our old-gen intentions.\n+      size_t budget_overrun = old_reserve - old_entitlement;;\n+      if (reserve_for_promo > budget_overrun) {\n+        reserve_for_promo -= budget_overrun;\n+        old_reserve -= budget_overrun;\n+      } else {\n+        budget_overrun -= reserve_for_promo;\n+        reserve_for_promo = 0;\n+        reserve_for_mixed = (reserve_for_mixed > budget_overrun)? reserve_for_mixed - budget_overrun: 0;\n+        old_reserve = reserve_for_promo + reserve_for_mixed;\n+      }\n+    }\n+\n+    \/\/ Because of adjustments above, old_reserve may be smaller now than it was when we tested the branch\n+    \/\/   condition above: \"(old_available + mutator_xfer_limit >= old_reserve)\n+    \/\/ Therefore, we do NOT know that: mutator_xfer_limit < old_reserve - old_available\n+\n+    size_t old_deficit = old_reserve - old_available;\n+    old_region_deficit = (old_deficit + region_size_bytes - 1) \/ region_size_bytes;\n+\n+    \/\/ Shrink young_reserve to account for loan to old reserve\n+    const size_t reserve_xfer_regions = old_region_deficit - mutator_region_xfer_limit;\n+    young_reserve -= reserve_xfer_regions * region_size_bytes;\n@@ -702,1 +769,0 @@\n-}\n@@ -704,5 +770,4 @@\n-void ShenandoahGenerationalHeap::reset_generation_reserves() {\n-  young_generation()->set_evacuation_reserve(0);\n-  old_generation()->set_evacuation_reserve(0);\n-  old_generation()->set_promoted_reserve(0);\n-}\n+  assert(old_region_deficit == 0 || old_region_surplus == 0, \"Only surplus or deficit, never both\");\n+  assert(young_reserve + reserve_for_mixed + reserve_for_promo <= old_available + young_available,\n+         \"Cannot reserve more memory than is available: %zu + %zu + %zu <= %zu + %zu\",\n+         young_reserve, reserve_for_mixed, reserve_for_promo, old_available, young_available);\n@@ -710,11 +775,4 @@\n-void ShenandoahGenerationalHeap::TransferResult::print_on(const char* when, outputStream* ss) const {\n-  auto heap = ShenandoahGenerationalHeap::heap();\n-  ShenandoahYoungGeneration* const young_gen = heap->young_generation();\n-  ShenandoahOldGeneration* const old_gen = heap->old_generation();\n-  const size_t young_available = young_gen->available();\n-  const size_t old_available = old_gen->available();\n-  ss->print_cr(\"After %s, %s %zu regions to %s to prepare for next gc, old available: \"\n-                     PROPERFMT \", young_available: \" PROPERFMT,\n-                     when,\n-                     success? \"successfully transferred\": \"failed to transfer\", region_count, region_destination,\n-                     PROPERFMTARGS(old_available), PROPERFMTARGS(young_available));\n+  \/\/ deficit\/surplus adjustments to generation sizes will precede rebuild\n+  young_generation()->set_evacuation_reserve(young_reserve);\n+  old_generation()->set_evacuation_reserve(reserve_for_mixed);\n+  old_generation()->set_promoted_reserve(reserve_for_promo);\n@@ -1078,12 +1136,0 @@\n-  \/\/ We defer generation resizing actions until after cset regions have been recycled.\n-  TransferResult result = balance_generations();\n-  LogTarget(Info, gc, ergo) lt;\n-  if (lt.is_enabled()) {\n-    LogStream ls(lt);\n-    result.print_on(\"Degenerated GC\", &ls);\n-  }\n-\n-  \/\/ In case degeneration interrupted concurrent evacuation or update references, we need to clean up\n-  \/\/ transient state. Otherwise, these actions have no effect.\n-  reset_generation_reserves();\n-\n@@ -1107,14 +1153,0 @@\n-\n-  TransferResult result;\n-  {\n-    ShenandoahHeapLocker locker(lock());\n-\n-    result = balance_generations();\n-    reset_generation_reserves();\n-  }\n-\n-  LogTarget(Info, gc, ergo) lt;\n-  if (lt.is_enabled()) {\n-    LogStream ls(lt);\n-    result.print_on(\"Concurrent GC\", &ls);\n-  }\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahGenerationalHeap.cpp","additions":156,"deletions":124,"binary":false,"changes":280,"status":"modified"},{"patch":"@@ -131,9 +131,0 @@\n-  \/\/ Used for logging the result of a region transfer outside the heap lock\n-  struct TransferResult {\n-    bool success;\n-    size_t region_count;\n-    const char* region_destination;\n-\n-    void print_on(const char* when, outputStream* ss) const;\n-  };\n-\n@@ -142,3 +133,0 @@\n-  \/\/ Zeros out the evacuation and promotion reserves\n-  void reset_generation_reserves();\n-\n@@ -146,4 +134,1 @@\n-  void compute_old_generation_balance(size_t old_xfer_limit, size_t old_cset_regions);\n-\n-  \/\/ Transfers surplus old regions to young, or takes regions from young to satisfy old region deficit\n-  TransferResult balance_generations();\n+  void compute_old_generation_balance(size_t old_xfer_limit, size_t old_cset_regions, size_t young_cset_regions);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahGenerationalHeap.hpp","additions":1,"deletions":16,"binary":false,"changes":17,"status":"modified"},{"patch":"@@ -423,7 +423,7 @@\n-\n-    size_t young_cset_regions, old_cset_regions;\n-\n-    \/\/ We are initializing free set.  We ignore cset region tallies.\n-    size_t first_old, last_old, num_old;\n-    _free_set->prepare_to_rebuild(young_cset_regions, old_cset_regions, first_old, last_old, num_old);\n-    _free_set->finish_rebuild(young_cset_regions, old_cset_regions, num_old);\n+    if (mode()->is_generational()) {\n+      size_t young_reserve = (young_generation()->max_capacity() * ShenandoahEvacReserve) \/ 100;\n+      young_generation()->set_evacuation_reserve(young_reserve);\n+      old_generation()->set_evacuation_reserve((size_t) 0);\n+      old_generation()->set_promoted_reserve((size_t) 0);\n+    }\n+    _free_set->rebuild();\n@@ -2540,4 +2540,1 @@\n-void ShenandoahHeap::rebuild_free_set(bool concurrent) {\n-  ShenandoahGCPhase phase(concurrent ?\n-                          ShenandoahPhaseTimings::final_update_refs_rebuild_freeset :\n-                          ShenandoahPhaseTimings::degen_gc_final_update_refs_rebuild_freeset);\n+void ShenandoahHeap::rebuild_free_set_within_phase() {\n@@ -2566,9 +2563,1 @@\n-    gen_heap->compute_old_generation_balance(allocation_runway, old_cset_regions);\n-\n-    \/\/ Total old_available may have been expanded to hold anticipated promotions.  We trigger if the fragmented available\n-    \/\/ memory represents more than 16 regions worth of data.  Note that fragmentation may increase when we promote regular\n-    \/\/ regions in place when many of these regular regions have an abundant amount of available memory within them.  Fragmentation\n-    \/\/ will decrease as promote-by-copy consumes the available memory within these partially consumed regions.\n-    \/\/\n-    \/\/ We consider old-gen to have excessive fragmentation if more than 12.5% of old-gen is free memory that resides\n-    \/\/ within partially consumed regions of memory.\n+    gen_heap->compute_old_generation_balance(allocation_runway, old_cset_regions, young_cset_regions);\n@@ -2586,0 +2575,7 @@\n+void ShenandoahHeap::rebuild_free_set(bool concurrent) {\n+  ShenandoahGCPhase phase(concurrent ?\n+                          ShenandoahPhaseTimings::final_update_refs_rebuild_freeset :\n+                          ShenandoahPhaseTimings::degen_gc_final_update_refs_rebuild_freeset);\n+  rebuild_free_set_within_phase();\n+}\n+\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeap.cpp","additions":16,"deletions":20,"binary":false,"changes":36,"status":"modified"},{"patch":"@@ -501,0 +501,1 @@\n+  void rebuild_free_set_within_phase();\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeap.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -606,0 +606,2 @@\n+    \/\/ Otherwise, the mutator might see region as fully recycled and might change its affiliation only to have\n+    \/\/ the racing GC worker thread overwrite its affiliation to FREE.\n@@ -616,0 +618,2 @@\n+\/\/ Note that return from try_recycle() does not mean the region has been recycled.  It only means that\n+\/\/ some GC worker thread has taken responsibility to recycle the region, \"eventually\".\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeapRegion.cpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -137,2 +137,0 @@\n-  heap->free_set()->log_status_under_lock();\n-\n@@ -147,4 +145,5 @@\n-  \/\/ We do not rebuild_free following increments of old marking because memory has not been reclaimed. However, we may\n-  \/\/ need to transfer memory to OLD in order to efficiently support the mixed evacuations that might immediately follow.\n-  size_t allocation_runway = heap->young_generation()->heuristics()->bytes_of_allocation_runway_before_gc_trigger(0);\n-  heap->compute_old_generation_balance(allocation_runway, 0);\n+  \/\/ After concurrent old marking finishes, we reclaim immediate garbage. Further, we may also want to expand OLD in order\n+  \/\/ to make room for anticipated promotions and\/or for mixed evacuations.  Mixed evacuations are especially likely to\n+  \/\/ follow the end of OLD marking.\n+  heap->rebuild_free_set_within_phase();\n+  heap->free_set()->log_status_under_lock();\n@@ -152,5 +151,0 @@\n-  ShenandoahGenerationalHeap::TransferResult result;\n-  {\n-    ShenandoahHeapLocker locker(heap->lock());\n-    result = heap->balance_generations();\n-  }\n@@ -158,5 +152,0 @@\n-  LogTarget(Info, gc, ergo) lt;\n-  if (lt.is_enabled()) {\n-    LogStream ls(lt);\n-    result.print_on(\"Old Mark\", &ls);\n-  }\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahOldGC.cpp","additions":5,"deletions":16,"binary":false,"changes":21,"status":"modified"},{"patch":"@@ -506,15 +506,0 @@\n-\n-  {\n-    \/\/ Though we did not choose a collection set above, we still may have\n-    \/\/ freed up immediate garbage regions so proceed with rebuilding the free set.\n-    ShenandoahGCPhase phase(concurrent ?\n-        ShenandoahPhaseTimings::final_rebuild_freeset :\n-        ShenandoahPhaseTimings::degen_gc_final_rebuild_freeset);\n-    ShenandoahHeapLocker locker(heap->lock());\n-    size_t cset_young_regions, cset_old_regions;\n-    size_t first_old, last_old, num_old;\n-    heap->free_set()->prepare_to_rebuild(cset_young_regions, cset_old_regions, first_old, last_old, num_old);\n-    \/\/ This is just old-gen completion.  No future budgeting required here.  The only reason to rebuild the freeset here\n-    \/\/ is in case there was any immediate old garbage identified.\n-    heap->free_set()->finish_rebuild(cset_young_regions, cset_old_regions, num_old);\n-  }\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahOldGeneration.cpp","additions":0,"deletions":15,"binary":false,"changes":15,"status":"modified"},{"patch":"@@ -68,2 +68,2 @@\n-  \/\/ Represents the quantity of live bytes we expect to promote in place during the next\n-  \/\/ evacuation cycle. This value is used by the young heuristic to trigger mixed collections.\n+  \/\/ Represents the quantity of live bytes we expect to promote during the next GC cycle, either by\n+  \/\/ evacuation or by promote-in-place.  This value is used by the young heuristic to trigger mixed collections.\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahOldGeneration.hpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -237,1 +237,1 @@\n-  \/\/ For HumongousRegion:s it's more efficient to jump directly to the\n+  \/\/ For HumongousRegions it's more efficient to jump directly to the\n@@ -248,2 +248,0 @@\n-  HeapWord* p = nullptr;\n-  oop obj = cast_to_oop(p);\n@@ -265,1 +263,1 @@\n-  p = _rs->addr_for_card_index(cur_index) + offset;\n+  HeapWord* p = _rs->addr_for_card_index(cur_index) + offset;\n@@ -282,1 +280,1 @@\n-  NOT_PRODUCT(obj = cast_to_oop(p);)\n+  oop obj = cast_to_oop(p);\n@@ -284,1 +282,1 @@\n-#define WALK_FORWARD_IN_BLOCK_START false\n+#define WALK_FORWARD_IN_BLOCK_START true\n@@ -287,0 +285,2 @@\n+    obj = cast_to_oop(p);\n+    assert(oopDesc::is_oop(obj), \"Should be an object\");\n@@ -289,1 +289,11 @@\n-  assert(p + obj->size() > left, \"obj should end after left\");\n+#ifdef ASSERT\n+  if (p + obj->size() <= left) {\n+    const CardValue* const wtbm = _rs->card_table()->write_byte_map();\n+    const CardValue* const rtbm = _rs->card_table()->read_byte_map();\n+    log_info(gc)(\"Anticipating assert failure, card[%zu] is %s in read table, %s in write table\", cur_index, \n+                 (rtbm[cur_index] ==CardTable::dirty_card_val())? \"dirty\": \"clean\",\n+                 (wtbm[cur_index] ==CardTable::dirty_card_val())? \"dirty\": \"clean\");\n+  }\n+#endif\n+  assert(p < left, \"p should start before left end of card\");\n+  assert(p + obj->size() > left, \"obj should end after left end of card\");\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahScanRemembered.cpp","additions":17,"deletions":7,"binary":false,"changes":24,"status":"modified"},{"patch":"@@ -231,0 +231,6 @@\n+#define KELVIN_DEBUG\n+#ifdef KELVIN_DEBUG\n+  inline ShenandoahCardTable* card_table() {\n+    return _card_table;\n+  }\n+#endif\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahScanRemembered.hpp","additions":6,"deletions":0,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -398,1 +398,8 @@\n-  size_t non_trashed_span() const { return (_regions - _trashed_regions) * ShenandoahHeapRegion::region_size_bytes(); }\n+  size_t non_trashed_span() const {\n+    assert(_regions >= _trashed_regions, \"sanity\");\n+    return (_regions - _trashed_regions) * ShenandoahHeapRegion::region_size_bytes();\n+  }\n+  size_t non_trashed_committed() const {\n+    assert(_committed >= _trashed_regions * ShenandoahHeapRegion::region_size_bytes(), \"sanity\");\n+    return _committed - (_trashed_regions * ShenandoahHeapRegion::region_size_bytes());\n+  }\n@@ -436,0 +443,1 @@\n+    size_t generation_max_capacity = generation->max_capacity();\n@@ -441,0 +449,4 @@\n+    guarantee(stats.non_trashed_committed() <= generation_max_capacity,\n+              \"%s: generation (%s) non_trashed_committed: \" PROPERFMT \" must not exceed generation capacity: \" PROPERFMT,\n+              label, generation->name(), PROPERFMTARGS(stats.non_trashed_committed()), PROPERFMTARGS(generation_max_capacity));\n+\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahVerifier.cpp","additions":13,"deletions":1,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -376,21 +376,14 @@\n-  product(uintx, ShenandoahOldEvacRatioPercent, 75, EXPERIMENTAL,           \\\n-          \"The maximum proportion of evacuation from old-gen memory, \"      \\\n-          \"expressed as a percentage. The default value 75 denotes that no\" \\\n-          \"more than 75% of the collection set evacuation workload may be \" \\\n-          \"towards evacuation of old-gen heap regions. This limits both the\"\\\n-          \"promotion of aged regions and the compaction of existing old \"   \\\n-          \"regions.  A value of 75 denotes that the total evacuation work\"  \\\n-          \"may increase to up to four times the young gen evacuation work.\" \\\n-          \"A larger value allows quicker promotion and allows\"              \\\n-          \"a smaller number of mixed evacuations to process \"               \\\n-          \"the entire list of old-gen collection candidates at the cost \"   \\\n-          \"of an increased disruption of the normal cadence of young-gen \"  \\\n-          \"collections.  A value of 100 allows a mixed evacuation to \"      \\\n-          \"focus entirely on old-gen memory, allowing no young-gen \"        \\\n-          \"regions to be collected, likely resulting in subsequent \"        \\\n-          \"allocation failures because the allocation pool is not \"         \\\n-          \"replenished.  A value of 0 allows a mixed evacuation to\"         \\\n-          \"focus entirely on young-gen memory, allowing no old-gen \"        \\\n-          \"regions to be collected, likely resulting in subsequent \"        \\\n-          \"promotion failures and triggering of stop-the-world full GC \"    \\\n-          \"events.\")                                                        \\\n+  product(uintx, ShenandoahOldEvacPercent, 75, EXPERIMENTAL,                \\\n+          \"The maximum evacuation to old-gen expressed as a percent of \"    \\\n+          \"the total live memory within the collection set.  With the \"     \\\n+          \"default setting, if collection set evacuates X, no more than \"   \\\n+          \"75% of X may hold object evacuated from old or promoted to old \" \\\n+          \"from young.  A value of 100 allows the entire collection set \"   \\\n+          \"to be comprised of old-gen regions and young regions that have \" \\\n+          \"reached the tenure age.  Larger values allow fewer mixed \"       \\\n+          \"evacuations to reclaim all the garbage from old.  Smaller \"      \\\n+          \"values result in less variation in GC cycle times between \"      \\\n+          \"young vs. mixed cycles.  A value of 0 prevents mixed \"           \\\n+          \"evacations from running and blocks promotion of aged regions \"   \\\n+          \"by evacuation.  Setting the value to 0 does not prevent \"        \\\n+          \"regions from being promoted in place.\")                          \\\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoah_globals.hpp","additions":14,"deletions":21,"binary":false,"changes":35,"status":"modified"},{"patch":"@@ -203,1 +203,3 @@\n-  _heuristics->prime_collection_set(_collection_set);\n+  if (_heuristics->prime_collection_set(_collection_set)) {\n+    _heuristics->finalize_mixed_evacs();\n+  }\n@@ -216,1 +218,3 @@\n-  _heuristics->prime_collection_set(_collection_set);\n+  if (_heuristics->prime_collection_set(_collection_set)) {\n+    _heuristics->finalize_mixed_evacs();\n+  }\n@@ -228,1 +232,3 @@\n-  _heuristics->prime_collection_set(_collection_set);\n+  if (_heuristics->prime_collection_set(_collection_set)) {\n+    _heuristics->finalize_mixed_evacs();\n+  }\n@@ -250,1 +256,3 @@\n-  _heuristics->prime_collection_set(_collection_set);\n+  if (_heuristics->prime_collection_set(_collection_set)) {\n+    _heuristics->finalize_mixed_evacs();\n+  }\n@@ -263,1 +271,3 @@\n-  _heuristics->prime_collection_set(_collection_set);\n+  if (_heuristics->prime_collection_set(_collection_set)) {\n+    _heuristics->finalize_mixed_evacs();\n+  }\n@@ -280,1 +290,3 @@\n-  _heuristics->prime_collection_set(_collection_set);\n+  if (_heuristics->prime_collection_set(_collection_set)) {\n+    _heuristics->finalize_mixed_evacs();\n+  }\n@@ -287,1 +299,3 @@\n-  _heuristics->prime_collection_set(_collection_set);\n+  if (_heuristics->prime_collection_set(_collection_set)) {\n+    _heuristics->finalize_mixed_evacs();\n+  }\n@@ -303,1 +317,3 @@\n-  _heuristics->prime_collection_set(_collection_set);\n+  if (_heuristics->prime_collection_set(_collection_set)) {\n+    _heuristics->finalize_mixed_evacs();\n+  }\n@@ -311,1 +327,3 @@\n-  _heuristics->prime_collection_set(_collection_set);\n+  if (_heuristics->prime_collection_set(_collection_set)) {\n+    _heuristics->finalize_mixed_evacs();\n+  }\n@@ -329,1 +347,3 @@\n-  _heuristics->prime_collection_set(_collection_set);\n+  if (_heuristics->prime_collection_set(_collection_set)) {\n+    _heuristics->finalize_mixed_evacs();\n+  }\n@@ -338,1 +358,3 @@\n-  _heuristics->prime_collection_set(_collection_set);\n+  if (_heuristics->prime_collection_set(_collection_set)) {\n+    _heuristics->finalize_mixed_evacs();\n+  }\n@@ -356,1 +378,3 @@\n-  _heuristics->prime_collection_set(_collection_set);\n+  if (_heuristics->prime_collection_set(_collection_set)) {\n+    _heuristics->finalize_mixed_evacs();\n+  }\n","filename":"test\/hotspot\/gtest\/gc\/shenandoah\/test_shenandoahOldHeuristic.cpp","additions":36,"deletions":12,"binary":false,"changes":48,"status":"modified"}]}