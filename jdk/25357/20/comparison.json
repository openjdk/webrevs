{"files":[{"patch":"@@ -37,0 +37,4 @@\n+#undef KELVIN_DEBUG\n+#ifdef KELVIN_DEBUG\n+#include \"gc\/shenandoah\/shenandoahYoungGeneration.hpp\"\n+#endif\n@@ -106,0 +110,7 @@\n+#undef KELVIN_DEBUG\n+#ifdef KELVIN_DEBUG\n+  ShenandoahYoungGeneration* young_gen = ShenandoahHeap::heap()->young_generation();\n+  log_info(gc)(\"ShenAdaptiveHeuristics::choose_collection_set_from_regiondata(), young available_with_reserve(): %zu\",\n+               young_gen->available_with_reserve());\n+#endif\n+\n@@ -126,0 +137,4 @@\n+#ifdef KELVIN_DEBUG\n+      log_info(gc)(\" after adding region %zu to cset with live_data: %zu, garbage: %zu, young available_with_reserve: %zu\",\n+                   r->index(), r->get_live_data_bytes(), r->garbage(), young_gen->available_with_reserve());\n+#endif\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/heuristics\/shenandoahAdaptiveHeuristics.cpp","additions":15,"deletions":0,"binary":false,"changes":15,"status":"modified"},{"patch":"@@ -40,1 +40,1 @@\n-void ShenandoahGenerationalHeuristics::choose_collection_set(ShenandoahCollectionSet* collection_set) {\n+ssize_t ShenandoahGenerationalHeuristics::choose_collection_set(ShenandoahCollectionSet* collection_set) {\n@@ -66,0 +66,4 @@\n+#undef KELVIN_DEBUG\n+#ifdef KELVIN_DEBUG\n+  log_info(gc)(\"ShenandoahGenerationalHeuristics::choose_collection_set()\");\n+#endif\n@@ -173,0 +177,6 @@\n+#ifdef KELVIN_DEBUG\n+  log_info(gc)(\" choose_collection_set() finds doing_promote_in_place: %s, preselected_candidates: %zu, immediate_percent: %zu\",\n+               doing_promote_in_place? \"true\": \"false\", preselected_candidates, immediate_percent);\n+#endif\n+\n+  ssize_t add_regions_to_old = 0;\n@@ -175,0 +185,1 @@\n+    bool need_to_finalize_mixed = false;\n@@ -176,1 +187,1 @@\n-      heap->old_generation()->heuristics()->prime_collection_set(collection_set);\n+      need_to_finalize_mixed = heap->old_generation()->heuristics()->prime_collection_set(collection_set);\n@@ -181,0 +192,11 @@\n+\n+    if (_generation->is_young()) {\n+      \/\/ Especially when young-gen trigger is expedited in order to finish mixed evacuations, there may not be\n+      \/\/ enough consolidated garbage to make effective use of young-gen evacuation reserve.  If there is still\n+      \/\/ young-gen reserve available following selection of the young-gen collection set, see if we can use\n+      \/\/ this memory to expand the old-gen evacuation collection set.\n+      need_to_finalize_mixed |= heap->old_generation()->heuristics()->top_off_collection_set(add_regions_to_old);\n+      if (need_to_finalize_mixed) {\n+        heap->old_generation()->heuristics()->finalize_mixed_evacs();\n+      }\n+    }\n@@ -197,0 +219,1 @@\n+  return add_regions_to_old;\n@@ -213,7 +236,0 @@\n-\n-      \/\/ r->used() is r->garbage() + r->get_live_data_bytes()\n-      \/\/ Since all live data in this region is being evacuated from young-gen, it is as if this memory\n-      \/\/ is garbage insofar as young-gen is concerned.  Counting this as garbage reduces the need to\n-      \/\/ reclaim highly utilized young-gen regions just for the sake of finding min_garbage to reclaim\n-      \/\/ within young-gen memory.\n-\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/heuristics\/shenandoahGenerationalHeuristics.cpp","additions":25,"deletions":9,"binary":false,"changes":34,"status":"modified"},{"patch":"@@ -47,1 +47,1 @@\n-  void choose_collection_set(ShenandoahCollectionSet* collection_set) override;\n+  ssize_t choose_collection_set(ShenandoahCollectionSet* collection_set) override;\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/heuristics\/shenandoahGenerationalHeuristics.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -55,0 +55,1 @@\n+\n@@ -60,10 +61,0 @@\n-  size_t max_young_cset = (size_t) (young_evac_reserve \/ ShenandoahEvacWaste);\n-  size_t young_cur_cset = 0;\n-  size_t max_old_cset = (size_t) (old_evac_reserve \/ ShenandoahOldEvacWaste);\n-  size_t old_cur_cset = 0;\n-\n-  \/\/ Figure out how many unaffiliated young regions are dedicated to mutator and to evacuator.  Allow the young\n-  \/\/ collector's unaffiliated regions to be transferred to old-gen if old-gen has more easily reclaimed garbage\n-  \/\/ than young-gen.  At the end of this cycle, any excess regions remaining in old-gen will be transferred back\n-  \/\/ to young.  Do not transfer the mutator's unaffiliated regions to old-gen.  Those must remain available\n-  \/\/ to the mutator as it needs to be able to consume this memory during concurrent GC.\n@@ -73,6 +64,24 @@\n-\n-  if (unaffiliated_young_memory > max_young_cset) {\n-    size_t unaffiliated_mutator_memory = unaffiliated_young_memory - max_young_cset;\n-    unaffiliated_young_memory -= unaffiliated_mutator_memory;\n-    unaffiliated_young_regions = unaffiliated_young_memory \/ region_size_bytes; \/\/ round down\n-    unaffiliated_young_memory = unaffiliated_young_regions * region_size_bytes;\n+  size_t unaffiliated_old_regions = heap->old_generation()->free_unaffiliated_regions();\n+  size_t unaffiliated_old_memory = unaffiliated_old_regions * region_size_bytes;\n+\n+  \/\/ Figure out how many unaffiliated regions are dedicated to Collector and OldCollector reserves.  Let these\n+  \/\/ be shuffled between young and old generations in order to expedite evacuation of whichever regions have the\n+  \/\/ most garbage, regardless of whether these garbage-first regions reside in young or old generation.\n+  \/\/ Excess reserves will be transferred back to the mutator after collection set has been chosen.  At the end\n+  \/\/ of evacuation, any reserves not consumed by evacuation will also be transferred to the mutator free set.\n+  size_t shared_reserve_regions = 0;\n+  if (young_evac_reserve > unaffiliated_young_memory) {\n+    young_evac_reserve -= unaffiliated_young_memory;\n+    shared_reserve_regions += unaffiliated_young_memory \/ region_size_bytes;\n+  } else {\n+    size_t delta_regions = young_evac_reserve \/ region_size_bytes;\n+    shared_reserve_regions += delta_regions;\n+    young_evac_reserve -= delta_regions * region_size_bytes;\n+  }\n+  if (old_evac_reserve > unaffiliated_old_memory) {\n+    old_evac_reserve -= unaffiliated_old_memory;\n+    shared_reserve_regions += unaffiliated_old_memory \/ region_size_bytes;\n+  } else {\n+    size_t delta_regions = old_evac_reserve \/ region_size_bytes;\n+    shared_reserve_regions += delta_regions;\n+    old_evac_reserve -= delta_regions * region_size_bytes;\n@@ -81,2 +90,6 @@\n-  \/\/ We'll affiliate these unaffiliated regions with either old or young, depending on need.\n-  max_young_cset -= unaffiliated_young_memory;\n+  size_t shared_reserves = shared_reserve_regions * region_size_bytes;\n+  size_t committed_from_shared_reserves = 0;\n+  size_t max_young_cset = (size_t) (young_evac_reserve \/ ShenandoahEvacWaste);\n+  size_t young_cur_cset = 0;\n+  size_t max_old_cset = (size_t) (old_evac_reserve \/ ShenandoahOldEvacWaste);\n+  size_t old_cur_cset = 0;\n@@ -84,2 +97,3 @@\n-  \/\/ Keep track of how many regions we plan to transfer from young to old.\n-  size_t regions_transferred_to_old = 0;\n+  size_t promo_bytes = 0;\n+  size_t old_evac_bytes = 0;\n+  size_t young_evac_bytes = 0;\n@@ -87,1 +101,3 @@\n-  size_t free_target = (capacity * ShenandoahMinFreeThreshold) \/ 100 + max_young_cset;\n+  size_t max_total_cset = (max_young_cset + max_old_cset +\n+                           (size_t) (shared_reserve_regions * region_size_bytes) \/ ShenandoahOldEvacWaste);\n+  size_t free_target = (capacity * ShenandoahMinFreeThreshold) \/ 100 + max_total_cset;\n@@ -91,1 +107,1 @@\n-                     \"%s, Max Old Evacuation: %zu%s, Max Either Evacuation: %zu%s, Actual Free: %zu%s.\",\n+                     \"%s, Max Old Evacuation: %zu%s, Discretionary additional evacuation: %zu%s, Actual Free: %zu%s.\",\n@@ -94,1 +110,1 @@\n-                     byte_size_in_proper_unit(unaffiliated_young_memory), proper_unit_for_byte_size(unaffiliated_young_memory),\n+                     byte_size_in_proper_unit(shared_reserves), proper_unit_for_byte_size(shared_reserves),\n@@ -97,0 +113,1 @@\n+  size_t cur_garbage = cur_young_garbage;\n@@ -101,0 +118,3 @@\n+    size_t region_garbage = r->garbage();\n+    size_t new_garbage = cur_garbage + region_garbage;\n+    bool add_regardless = (region_garbage > ignore_threshold) && (new_garbage < min_garbage);\n@@ -102,5 +122,10 @@\n-      size_t new_cset = old_cur_cset + r->get_live_data_bytes();\n-      if ((r->garbage() > garbage_threshold)) {\n-        while ((new_cset > max_old_cset) && (unaffiliated_young_regions > 0)) {\n-          unaffiliated_young_regions--;\n-          regions_transferred_to_old++;\n+      if (add_regardless || (region_garbage > garbage_threshold)) {\n+        size_t live_bytes = r->get_live_data_bytes();\n+        size_t new_cset = old_cur_cset + r->get_live_data_bytes();\n+        \/\/ May need to reserve multiple regions to hold the evacuations from a single region, depending on live data bytes\n+        \/\/ and ShenandoahOldEvacWaste\n+        size_t orig_max_old_cset = max_old_cset;\n+        size_t proposed_old_region_consumption = 0;\n+        while ((new_cset > max_old_cset) && (committed_from_shared_reserves < shared_reserves)) {\n+          committed_from_shared_reserves += region_size_bytes;\n+          proposed_old_region_consumption++;\n@@ -109,4 +134,15 @@\n-      }\n-      if ((new_cset <= max_old_cset) && (r->garbage() > garbage_threshold)) {\n-        add_region = true;\n-        old_cur_cset = new_cset;\n+        \/\/ We already know: add_regardless || region_garbage > garbage_threshold\n+        if (new_cset <= max_old_cset) {\n+          add_region = true;\n+          old_cur_cset = new_cset;\n+          cur_garbage = new_garbage;\n+          if (r->is_old()) {\n+            old_evac_bytes += live_bytes;\n+          } else {\n+            promo_bytes += live_bytes;\n+          }\n+        } else {\n+          \/\/ We failed to sufficiently expand old, so unwind proposed expansion\n+          max_old_cset = orig_max_old_cset;\n+          committed_from_shared_reserves -= proposed_old_region_consumption * region_size_bytes;\n+        }\n@@ -116,8 +152,9 @@\n-      size_t new_cset = young_cur_cset + r->get_live_data_bytes();\n-      size_t region_garbage = r->garbage();\n-      size_t new_garbage = cur_young_garbage + region_garbage;\n-      bool add_regardless = (region_garbage > ignore_threshold) && (new_garbage < min_garbage);\n-\n-      if (add_regardless || (r->garbage() > garbage_threshold)) {\n-        while ((new_cset > max_young_cset) && (unaffiliated_young_regions > 0)) {\n-          unaffiliated_young_regions--;\n+      if (add_regardless || (region_garbage > garbage_threshold)) {\n+        size_t live_bytes = r->get_live_data_bytes();\n+        size_t new_cset = young_cur_cset + live_bytes;\n+        \/\/ May need multiple reserve regions to evacuate a single region, depending on live data bytes and ShenandoahEvacWaste\n+        size_t orig_max_young_cset = max_young_cset;\n+        size_t proposed_young_region_consumption = 0;\n+        while ((new_cset > max_young_cset) && (committed_from_shared_reserves < shared_reserves)) {\n+          committed_from_shared_reserves += region_size_bytes;\n+          proposed_young_region_consumption++;\n@@ -126,5 +163,11 @@\n-      }\n-      if ((new_cset <= max_young_cset) && (add_regardless || (region_garbage > garbage_threshold))) {\n-        add_region = true;\n-        young_cur_cset = new_cset;\n-        cur_young_garbage = new_garbage;\n+        \/\/ We already know: add_regardless || region_garbage > garbage_threshold\n+        if (new_cset <= max_young_cset) {\n+          add_region = true;\n+          young_cur_cset = new_cset;\n+          cur_garbage = new_garbage;\n+          young_evac_bytes += live_bytes;\n+        } else {\n+          \/\/ We failed to sufficiently expand young, so unwind proposed expansion\n+          max_young_cset = orig_max_young_cset;\n+          committed_from_shared_reserves -= proposed_young_region_consumption * region_size_bytes;\n+        }\n@@ -137,5 +180,3 @@\n-  if (regions_transferred_to_old > 0) {\n-    assert(young_evac_reserve > regions_transferred_to_old * region_size_bytes, \"young reserve cannot be negative\");\n-    heap->young_generation()->set_evacuation_reserve(young_evac_reserve - regions_transferred_to_old * region_size_bytes);\n-    heap->old_generation()->set_evacuation_reserve(old_evac_reserve + regions_transferred_to_old * region_size_bytes);\n-  }\n+  heap->young_generation()->set_evacuation_reserve((size_t) (young_evac_bytes * ShenandoahEvacWaste));\n+  heap->old_generation()->set_evacuation_reserve((size_t) (old_evac_bytes * ShenandoahOldEvacWaste));\n+  heap->old_generation()->set_promoted_reserve((size_t) (promo_bytes * ShenandoahPromoEvacWaste));\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/heuristics\/shenandoahGlobalHeuristics.cpp","additions":91,"deletions":50,"binary":false,"changes":141,"status":"modified"},{"patch":"@@ -75,1 +75,1 @@\n-void ShenandoahHeuristics::choose_collection_set(ShenandoahCollectionSet* collection_set) {\n+ssize_t ShenandoahHeuristics::choose_collection_set(ShenandoahCollectionSet* collection_set) {\n@@ -99,0 +99,4 @@\n+#undef KELVIN_DEBUG\n+#ifdef KELVIN_DEBUG\n+  log_info(gc)(\"ShenandoahHeuristics::choose_collection_set()\");\n+#endif\n@@ -153,0 +157,3 @@\n+#ifdef KELVIN_DEBUG\n+  log_info(gc)(\" choose_collection_set() finds immediate_percent: %zu\", immediate_percent);\n+#endif\n@@ -156,1 +163,0 @@\n-\n@@ -158,0 +164,1 @@\n+  return 0;\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/heuristics\/shenandoahHeuristics.cpp","additions":9,"deletions":2,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -229,1 +229,1 @@\n-  virtual void choose_collection_set(ShenandoahCollectionSet* collection_set);\n+  virtual ssize_t choose_collection_set(ShenandoahCollectionSet* collection_set);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/heuristics\/shenandoahHeuristics.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -29,0 +29,1 @@\n+#include \"gc\/shenandoah\/shenandoahFreeSet.hpp\"\n@@ -32,0 +33,1 @@\n+#include \"gc\/shenandoah\/shenandoahYoungGeneration.hpp\"\n@@ -79,3 +81,9 @@\n-  if (unprocessed_old_collection_candidates() == 0) {\n-    return false;\n-  }\n+  _mixed_evac_cset = collection_set;\n+  _included_old_regions = 0;\n+  _evacuated_old_bytes = 0;\n+  _collected_old_bytes = 0;\n+\n+#undef KELVIN_DEBUG\n+#ifdef KELVIN_DEBUG\n+  log_info(gc)(\"prime_collection_set(), %s preparing for mark\",  _old_generation->is_preparing_for_mark()? \"is\": \"is not\");\n+#endif\n@@ -91,6 +99,0 @@\n-  _first_pinned_candidate = NOT_FOUND;\n-\n-  uint included_old_regions = 0;\n-  size_t evacuated_old_bytes = 0;\n-  size_t collected_old_bytes = 0;\n-\n@@ -102,10 +104,26 @@\n-  const size_t old_evacuation_reserve = _old_generation->get_evacuation_reserve();\n-  const size_t old_evacuation_budget = (size_t) ((double) old_evacuation_reserve \/ ShenandoahOldEvacWaste);\n-  size_t unfragmented_available = _old_generation->free_unaffiliated_regions() * ShenandoahHeapRegion::region_size_bytes();\n-  size_t fragmented_available;\n-  size_t excess_fragmented_available;\n-\n-  if (unfragmented_available > old_evacuation_budget) {\n-    unfragmented_available = old_evacuation_budget;\n-    fragmented_available = 0;\n-    excess_fragmented_available = 0;\n+  _old_evacuation_reserve = _old_generation->get_evacuation_reserve();\n+  _old_evacuation_budget = (size_t) ((double) _old_evacuation_reserve \/ ShenandoahOldEvacWaste);\n+\n+  \/\/ fragmented_available is the amount of memory within partially consumed old regions that may be required to\n+  \/\/ hold the results of old evacuations.  If all of the memory required by the old evacuation reserve is available\n+  \/\/ in unfragmented regions (unaffiliated old regions), then fragmented_available is zero because we do not need\n+  \/\/ to evacuate into the existing partially consumed old regions.\n+\n+  \/\/ if fragmented_available is non-zero, excess_fragmented_old_budget represents the amount of fragmented memory\n+  \/\/ that is available within old, but is not required to hold the resuilts of old evacuation.  As old-gen regions\n+  \/\/ are added into the collection set, their free memory is subtracted from excess_fragmented_old_budget until the\n+  \/\/ excess is exhausted.  For old-gen regions subsequently added to the collection set, their free memory is\n+  \/\/ subtracted from fragmented_available and from the old_evacuation_budget (since the budget decreases when this\n+  \/\/ fragmented_available memory decreases).  After fragmented_available has been exhausted, any further old regions\n+  \/\/ selected for the cset do not further decrease the old_evacuation_budget because all further evacuation is targeted\n+  \/\/ to unfragmented regions.\n+\n+  size_t unaffiliated_available = _old_generation->free_unaffiliated_regions() * ShenandoahHeapRegion::region_size_bytes();\n+  if (unaffiliated_available > _old_evacuation_reserve) {\n+    _unspent_unfragmented_old_budget = _old_evacuation_budget;\n+    _unspent_fragmented_old_budget = 0;\n+#ifdef KELVIN_DEBUG\n+    log_info(gc)(\" setting _unspent_unfragmented_old_budget to %zu, _unspent_fragmented_old_budget to zero\",\n+                 _unspent_unfragmented_old_budget);\n+#endif\n+    _excess_fragmented_old_budget = 0;\n@@ -113,6 +131,6 @@\n-    assert(_old_generation->available() >= old_evacuation_budget, \"Cannot budget more than is available\");\n-    fragmented_available = _old_generation->available() - unfragmented_available;\n-    assert(fragmented_available + unfragmented_available >= old_evacuation_budget, \"Budgets do not add up\");\n-    if (fragmented_available + unfragmented_available > old_evacuation_budget) {\n-      excess_fragmented_available = (fragmented_available + unfragmented_available) - old_evacuation_budget;\n-      fragmented_available -= excess_fragmented_available;\n+    assert(_old_generation->available() >= _old_evacuation_reserve, \"Cannot reserve more than is available\");\n+    size_t affiliated_available = _old_generation->available() - unaffiliated_available;\n+    assert(affiliated_available + unaffiliated_available >= _old_evacuation_reserve, \"Budgets do not add up\");\n+    if (affiliated_available + unaffiliated_available > _old_evacuation_reserve) {\n+      _excess_fragmented_old_budget = (affiliated_available + unaffiliated_available) - _old_evacuation_reserve;\n+      affiliated_available -= _excess_fragmented_old_budget;\n@@ -120,0 +138,10 @@\n+    _unspent_fragmented_old_budget = (size_t) ((double) affiliated_available \/ ShenandoahOldEvacWaste);\n+    _unspent_unfragmented_old_budget = (size_t) ((double) unaffiliated_available \/ ShenandoahOldEvacWaste);\n+#ifdef KELVIN_DEBUG\n+    log_info(gc)(\"               _old_evacuation_reserve: %zu\", _old_evacuation_reserve);\n+    log_info(gc)(\"                  affiliated_available: %zu\", affiliated_available);\n+    log_info(gc)(\"        _unspent_fragmented_old_budget: %zu\", _unspent_fragmented_old_budget);\n+    log_info(gc)(\"                unaffiliated_available: %zu\", unaffiliated_available);\n+    log_info(gc)(\"      _unspent_unfragmented_old_budget: %zu\", _unspent_unfragmented_old_budget);\n+    log_info(gc)(\"          _excess_fragmented_old_budget: %zu\", _excess_fragmented_old_budget);\n+#endif\n@@ -122,1 +150,0 @@\n-  size_t remaining_old_evacuation_budget = old_evacuation_budget;\n@@ -124,1 +151,1 @@\n-                byte_size_in_proper_unit(old_evacuation_budget), proper_unit_for_byte_size(old_evacuation_budget),\n+                byte_size_in_proper_unit(_old_evacuation_budget), proper_unit_for_byte_size(_old_evacuation_budget),\n@@ -126,120 +153,1 @@\n-\n-  size_t lost_evacuation_capacity = 0;\n-\n-  \/\/ The number of old-gen regions that were selected as candidates for collection at the end of the most recent old-gen\n-  \/\/ concurrent marking phase and have not yet been collected is represented by unprocessed_old_collection_candidates().\n-  \/\/ Candidate regions are ordered according to increasing amount of live data.  If there is not sufficient room to\n-  \/\/ evacuate region N, then there is no need to even consider evacuating region N+1.\n-  while (unprocessed_old_collection_candidates() > 0) {\n-    \/\/ Old collection candidates are sorted in order of decreasing garbage contained therein.\n-    ShenandoahHeapRegion* r = next_old_collection_candidate();\n-    if (r == nullptr) {\n-      break;\n-    }\n-    assert(r->is_regular(), \"There should be no humongous regions in the set of mixed-evac candidates\");\n-\n-    \/\/ If region r is evacuated to fragmented memory (to free memory within a partially used region), then we need\n-    \/\/ to decrease the capacity of the fragmented memory by the scaled loss.\n-\n-    const size_t live_data_for_evacuation = r->get_live_data_bytes();\n-    size_t lost_available = r->free();\n-\n-    if ((lost_available > 0) && (excess_fragmented_available > 0)) {\n-      if (lost_available < excess_fragmented_available) {\n-        excess_fragmented_available -= lost_available;\n-        lost_evacuation_capacity -= lost_available;\n-        lost_available  = 0;\n-      } else {\n-        lost_available -= excess_fragmented_available;\n-        lost_evacuation_capacity -= excess_fragmented_available;\n-        excess_fragmented_available = 0;\n-      }\n-    }\n-    size_t scaled_loss = (size_t) ((double) lost_available \/ ShenandoahOldEvacWaste);\n-    if ((lost_available > 0) && (fragmented_available > 0)) {\n-      if (scaled_loss + live_data_for_evacuation < fragmented_available) {\n-        fragmented_available -= scaled_loss;\n-        scaled_loss = 0;\n-      } else {\n-        \/\/ We will have to allocate this region's evacuation memory from unfragmented memory, so don't bother\n-        \/\/ to decrement scaled_loss\n-      }\n-    }\n-    if (scaled_loss > 0) {\n-      \/\/ We were not able to account for the lost free memory within fragmented memory, so we need to take this\n-      \/\/ allocation out of unfragmented memory.  Unfragmented memory does not need to account for loss of free.\n-      if (live_data_for_evacuation > unfragmented_available) {\n-        \/\/ There is no room to evacuate this region or any that come after it in within the candidates array.\n-        log_debug(gc, cset)(\"Not enough unfragmented memory (%zu) to hold evacuees (%zu) from region: (%zu)\",\n-                            unfragmented_available, live_data_for_evacuation, r->index());\n-        break;\n-      } else {\n-        unfragmented_available -= live_data_for_evacuation;\n-      }\n-    } else {\n-      \/\/ Since scaled_loss == 0, we have accounted for the loss of free memory, so we can allocate from either\n-      \/\/ fragmented or unfragmented available memory.  Use up the fragmented memory budget first.\n-      size_t evacuation_need = live_data_for_evacuation;\n-\n-      if (evacuation_need > fragmented_available) {\n-        evacuation_need -= fragmented_available;\n-        fragmented_available = 0;\n-      } else {\n-        fragmented_available -= evacuation_need;\n-        evacuation_need = 0;\n-      }\n-      if (evacuation_need > unfragmented_available) {\n-        \/\/ There is no room to evacuate this region or any that come after it in within the candidates array.\n-        log_debug(gc, cset)(\"Not enough unfragmented memory (%zu) to hold evacuees (%zu) from region: (%zu)\",\n-                            unfragmented_available, live_data_for_evacuation, r->index());\n-        break;\n-      } else {\n-        unfragmented_available -= evacuation_need;\n-        \/\/ dead code: evacuation_need == 0;\n-      }\n-    }\n-    collection_set->add_region(r);\n-    included_old_regions++;\n-    evacuated_old_bytes += live_data_for_evacuation;\n-    collected_old_bytes += r->garbage();\n-    consume_old_collection_candidate();\n-  }\n-\n-  if (_first_pinned_candidate != NOT_FOUND) {\n-    \/\/ Need to deal with pinned regions\n-    slide_pinned_regions_to_front();\n-  }\n-  decrease_unprocessed_old_collection_candidates_live_memory(evacuated_old_bytes);\n-  if (included_old_regions > 0) {\n-    log_info(gc, ergo)(\"Old-gen piggyback evac (\" UINT32_FORMAT \" regions, evacuating \" PROPERFMT \", reclaiming: \" PROPERFMT \")\",\n-                  included_old_regions, PROPERFMTARGS(evacuated_old_bytes), PROPERFMTARGS(collected_old_bytes));\n-  }\n-\n-  if (unprocessed_old_collection_candidates() == 0) {\n-    \/\/ We have added the last of our collection candidates to a mixed collection.\n-    \/\/ Any triggers that occurred during mixed evacuations may no longer be valid.  They can retrigger if appropriate.\n-    clear_triggers();\n-\n-    _old_generation->complete_mixed_evacuations();\n-  } else if (included_old_regions == 0) {\n-    \/\/ We have candidates, but none were included for evacuation - are they all pinned?\n-    \/\/ or did we just not have enough room for any of them in this collection set?\n-    \/\/ We don't want a region with a stuck pin to prevent subsequent old collections, so\n-    \/\/ if they are all pinned we transition to a state that will allow us to make these uncollected\n-    \/\/ (pinned) regions parsable.\n-    if (all_candidates_are_pinned()) {\n-      log_info(gc, ergo)(\"All candidate regions \" UINT32_FORMAT \" are pinned\", unprocessed_old_collection_candidates());\n-      _old_generation->abandon_mixed_evacuations();\n-    } else {\n-      log_info(gc, ergo)(\"No regions selected for mixed collection. \"\n-                         \"Old evacuation budget: \" PROPERFMT \", Remaining evacuation budget: \" PROPERFMT\n-                         \", Lost capacity: \" PROPERFMT\n-                         \", Next candidate: \" UINT32_FORMAT \", Last candidate: \" UINT32_FORMAT,\n-                         PROPERFMTARGS(old_evacuation_reserve),\n-                         PROPERFMTARGS(remaining_old_evacuation_budget),\n-                         PROPERFMTARGS(lost_evacuation_capacity),\n-                         _next_old_collection_candidate, _last_old_collection_candidate);\n-    }\n-  }\n-\n-  return (included_old_regions > 0);\n+  return add_old_regions_to_cset();\n@@ -319,0 +227,265 @@\n+bool ShenandoahOldHeuristics::add_old_regions_to_cset() {\n+  if (unprocessed_old_collection_candidates() == 0) {\n+    return false;\n+  }\n+  _first_pinned_candidate = NOT_FOUND;\n+#undef KELVIN_DEBUG\n+#ifdef KELVIN_DEBUG\n+  size_t region_size_bytes = ShenandoahHeapRegion::region_size_bytes();\n+  ShenandoahOldGeneration* old_gen = _heap->old_generation();\n+  log_info(gc)(\"add_old_regions_to_cset() with non-zero unprocessed candidates, old available: %zu\",\n+               old_gen->available());\n+  log_info(gc)(\"          _excess_fragmented_old_budget: %zu\", _excess_fragmented_old_budget);\n+  log_info(gc)(\"        _unspent_fragmented_old_budget: %zu\", _unspent_fragmented_old_budget);\n+  log_info(gc)(\"      _unspent_unfragmented_old_budget: %zu\", _unspent_unfragmented_old_budget);\n+  log_info(gc)(\"                  _evacuated_old_bytes: %zu\", _evacuated_old_bytes);\n+  log_info(gc)(\"                  _collected_old_bytes: %zu\", _collected_old_bytes);\n+#endif\n+\n+  \/\/ The number of old-gen regions that were selected as candidates for collection at the end of the most recent old-gen\n+  \/\/ concurrent marking phase and have not yet been collected is represented by unprocessed_old_collection_candidates().\n+  \/\/ Candidate regions are ordered according to increasing amount of live data.  If there is not sufficient room to\n+  \/\/ evacuate region N, then there is no need to even consider evacuating region N+1.\n+  while (unprocessed_old_collection_candidates() > 0) {\n+    \/\/ Old collection candidates are sorted in order of decreasing garbage contained therein.\n+    ShenandoahHeapRegion* r = next_old_collection_candidate();\n+    if (r == nullptr) {\n+      break;\n+    }\n+    assert(r->is_regular(), \"There should be no humongous regions in the set of mixed-evac candidates\");\n+\n+    \/\/ If region r is evacuated to fragmented memory (to free memory within a partially used region), then we need\n+    \/\/ to decrease the capacity of the fragmented memory by the scaled loss.\n+\n+    const size_t live_data_for_evacuation = r->get_live_data_bytes();\n+    size_t lost_available = r->free();\n+\n+#ifdef KELVIN_DEBUG\n+    log_info(gc)(\" Consider adding region: %zu, with live_bytes: %zu, garbage: %zu, and available: %zu\",\n+                 r->index(), live_data_for_evacuation, r->garbage(), lost_available);\n+    ssize_t delta = region_size_bytes - (live_data_for_evacuation + r->garbage() + lost_available);\n+    log_info(gc)(\" We expect that garbage plus live + lost_available is region_size_bytes, off by: %zd\", delta);\n+#endif\n+\n+    ssize_t fragmented_delta = 0;\n+    ssize_t unfragmented_delta = 0;\n+    ssize_t excess_delta = 0;\n+\n+    \/\/ We must decrease our mixed-evacuation budgets proportional to the lost available memory.  This memory that is no\n+    \/\/ longer available was likely \"promised\" to promotions, so we must decrease our mixed evacuations now.\n+    \/\/ (e.g. if we loose 14 bytes of available old memory, we must decrease the evacuation budget by 10 bytes.)\n+    size_t scaled_loss = (size_t) (((double) lost_available) \/ ShenandoahOldEvacWaste);\n+#ifdef KELVIN_DEBUG\n+    log_info(gc)(\" Computed scaled_loss: %zu\", scaled_loss);\n+#endif\n+    if (lost_available > 0) {\n+      \/\/ We need to subtract lost_available from our working evacuation budgets\n+      if (scaled_loss < _excess_fragmented_old_budget) {\n+        excess_delta -= scaled_loss;\n+        _excess_fragmented_old_budget -= scaled_loss;\n+      } else {\n+        excess_delta -= _excess_fragmented_old_budget;\n+        _excess_fragmented_old_budget = 0;\n+      }\n+\n+      if (scaled_loss < _unspent_fragmented_old_budget) {\n+        _unspent_fragmented_old_budget -= scaled_loss;\n+        fragmented_delta = -scaled_loss;\n+#ifdef KELVIN_DEBUG\n+        log_info(gc)(\" Decrease _unspent_fragmented_old_budget by scaled_loss: %zu to yield: %zu\",\n+                     scaled_loss, _unspent_fragmented_old_budget);\n+#endif\n+        scaled_loss = 0;\n+      } else {\n+        scaled_loss -= _unspent_fragmented_old_budget;\n+        fragmented_delta = -_unspent_fragmented_old_budget;\n+        _unspent_fragmented_old_budget = 0;\n+#ifdef KELVIN_DEBUG\n+        log_info(gc)(\" Zeroing _unspent_fragmented_old_budget, scaled_loss is: %zu\", scaled_loss);\n+#endif\n+      }\n+\n+      if (scaled_loss < _unspent_unfragmented_old_budget) {\n+        _unspent_unfragmented_old_budget -= scaled_loss;\n+        unfragmented_delta = -scaled_loss;\n+#ifdef KELVIN_DEBUG\n+        log_info(gc)(\" Decrease _unspent_unfragmented_old_budget by scaled_loss: %zu to yield: %zu\",\n+                     scaled_loss, _unspent_unfragmented_old_budget);\n+#endif\n+        scaled_loss = 0;\n+      } else {\n+        scaled_loss -= _unspent_unfragmented_old_budget;\n+        fragmented_delta = -_unspent_unfragmented_old_budget;\n+        _unspent_unfragmented_old_budget = 0;\n+#ifdef KELVIN_DEBUG\n+        log_info(gc)(\" Zeroing _unspent_unfragmented_old_budget\");\n+#endif\n+      }\n+    }\n+\n+    \/\/ Allocate replica from unfragmented memory if that exists\n+    size_t evacuation_need = live_data_for_evacuation;\n+    if (evacuation_need < _unspent_unfragmented_old_budget) {\n+      _unspent_unfragmented_old_budget -= evacuation_need;\n+#ifdef KELVIN_DEBUG\n+      log_info(gc)(\" Decrementing _unspent_unfragmented_old_budget by evacuation_need: %zu, yielding: %zu\",\n+                   evacuation_need, _unspent_unfragmented_old_budget);\n+#endif\n+    } else {\n+      if (_unspent_unfragmented_old_budget > 0) {\n+        evacuation_need -= _unspent_unfragmented_old_budget;\n+        unfragmented_delta -= _unspent_unfragmented_old_budget;\n+        _unspent_unfragmented_old_budget = 0;\n+#ifdef KELVIN_DEBUG\n+        log_info(gc)(\" Zeroing _unspent_unfragmented_old_budget\");\n+#endif\n+      }\n+      \/\/ Take the remaining allocation out of fragmented available\n+      if (_unspent_fragmented_old_budget > evacuation_need) {\n+        _unspent_fragmented_old_budget -= evacuation_need;\n+#ifdef KELVIN_DEBUG\n+        log_info(gc)(\" Decreasing _unspent_fragmented_old_budget by %zu to yield: %zu\", evacuation_need, _unspent_fragmented_old_budget);\n+#endif\n+      } else {\n+        \/\/ We cannot add this region into the collection set.  We're done.  Undo the adjustments to available.\n+        _unspent_fragmented_old_budget -= fragmented_delta;\n+        _unspent_unfragmented_old_budget -= unfragmented_delta;\n+        _excess_fragmented_old_budget -= excess_delta;\n+#ifdef KELVIN_DEBUG\n+        log_info(gc)(\" Never mind, region %zu cannot be added to cset.  Breaking out of loop after ...\", r->index());\n+        log_info(gc)(\"    restoring _unspent_fragmented_old_budget to %zu\", _unspent_fragmented_old_budget);\n+        log_info(gc)(\"  restoring _unspent_unfragmented_old_budget to %zu\", _unspent_unfragmented_old_budget);\n+        log_info(gc)(\"     restoring _excess_fragmented_old_budget to %zu\", _excess_fragmented_old_budget);\n+#endif\n+        break;\n+      }\n+    }\n+    _mixed_evac_cset->add_region(r);\n+    _included_old_regions++;\n+    _evacuated_old_bytes += live_data_for_evacuation;\n+    _collected_old_bytes += r->garbage();\n+    consume_old_collection_candidate();\n+#ifdef KELVIN_DEBUG\n+    log_info(gc)(\" Successfully added region %zu to collection set, old_available is now: %zu\", r->index(),\n+                 old_gen->available());\n+    log_info(gc)(\" _evacuated_old_bytes is now %zu\", _evacuated_old_bytes);\n+    log_info(gc)(\" _collected_old_bytes is now %zu\", _collected_old_bytes);\n+#endif\n+  }\n+  return true;\n+}\n+\n+bool ShenandoahOldHeuristics::finalize_mixed_evacs() {\n+  if (_first_pinned_candidate != NOT_FOUND) {\n+    \/\/ Need to deal with pinned regions\n+    slide_pinned_regions_to_front();\n+  }\n+  decrease_unprocessed_old_collection_candidates_live_memory(_evacuated_old_bytes);\n+  if (_included_old_regions > 0) {\n+    log_info(gc)(\"Old-gen mixed evac (%zu regions, evacuating %zu%s, reclaiming: %zu%s)\",\n+                 _included_old_regions,\n+                 byte_size_in_proper_unit(_evacuated_old_bytes), proper_unit_for_byte_size(_evacuated_old_bytes),\n+                 byte_size_in_proper_unit(_collected_old_bytes), proper_unit_for_byte_size(_collected_old_bytes));\n+  }\n+\n+  if (unprocessed_old_collection_candidates() == 0) {\n+    \/\/ We have added the last of our collection candidates to a mixed collection.\n+    \/\/ Any triggers that occurred during mixed evacuations may no longer be valid.  They can retrigger if appropriate.\n+    clear_triggers();\n+    _old_generation->complete_mixed_evacuations();\n+  } else if (_included_old_regions == 0) {\n+    \/\/ We have candidates, but none were included for evacuation - are they all pinned?\n+    \/\/ or did we just not have enough room for any of them in this collection set?\n+    \/\/ We don't want a region with a stuck pin to prevent subsequent old collections, so\n+    \/\/ if they are all pinned we transition to a state that will allow us to make these uncollected\n+    \/\/ (pinned) regions parsable.\n+    if (all_candidates_are_pinned()) {\n+      log_info(gc)(\"All candidate regions \" UINT32_FORMAT \" are pinned\", unprocessed_old_collection_candidates());\n+      _old_generation->abandon_mixed_evacuations();\n+    } else {\n+      log_info(gc)(\"No regions selected for mixed collection. \"\n+                   \"Old evacuation budget: \" PROPERFMT \", Next candidate: \" UINT32_FORMAT \", Last candidate: \" UINT32_FORMAT,\n+                   PROPERFMTARGS(_old_evacuation_reserve),\n+                   _next_old_collection_candidate, _last_old_collection_candidate);\n+    }\n+  }\n+  return (_included_old_regions > 0);\n+}\n+\n+bool ShenandoahOldHeuristics::top_off_collection_set(ssize_t &add_regions_to_old) {\n+  if (unprocessed_old_collection_candidates() == 0) {\n+    return false;\n+  } else {\n+    ShenandoahYoungGeneration* young_generation = _heap->young_generation();\n+    size_t young_unaffiliated_regions = young_generation->free_unaffiliated_regions();\n+    size_t max_young_cset = young_generation->get_evacuation_reserve();\n+\n+    \/\/ We have budgeted to assure the live_bytes_in_tenurable_regions() get evacuated into old generation.  Young reserves\n+    \/\/ only for untenurable region evacuations.\n+    size_t planned_young_evac = _mixed_evac_cset->get_live_bytes_in_untenurable_regions();\n+    size_t consumed_from_young_cset = (size_t) (planned_young_evac * ShenandoahEvacWaste);\n+\n+    size_t region_size_bytes = ShenandoahHeapRegion::region_size_bytes();\n+    size_t regions_required_for_collector_reserve = (consumed_from_young_cset + region_size_bytes - 1) \/ region_size_bytes;\n+\n+#undef KELVIN_DEBUG\n+#ifdef KELVIN_DEBUG\n+    ShenandoahOldGeneration* old_gen = _heap->old_generation();\n+    log_info(gc)(\"top_off_collection_set(), young available with reserves: %zu\", old_gen->available());\n+    log_info(gc)(\" young unaffiliated regions: %zu, bytes: %zu\", young_unaffiliated_regions,\n+                 young_unaffiliated_regions * region_size_bytes);\n+    log_info(gc)(\" young evacuation reserve with waste: %zu\", max_young_cset);\n+    log_info(gc)(\" planed_young_evac: %zu, consumed_from_young_cset: %zu\", planned_young_evac, consumed_from_young_cset);\n+    log_info(gc)(\" regions required for collector reserve: %zu\", regions_required_for_collector_reserve);\n+#endif\n+\n+    assert(consumed_from_young_cset <= max_young_cset, \"sanity\");\n+    assert(max_young_cset <= young_unaffiliated_regions * region_size_bytes, \"sanity\");\n+\n+    size_t regions_for_old_expansion;\n+    if (consumed_from_young_cset < max_young_cset) {\n+      size_t excess_young_reserves = max_young_cset - consumed_from_young_cset;\n+      \/\/ We can only transfer empty regions from young to old.  Furthermore, we must be careful to assure that the young\n+      \/\/ Collector reserve that remains after transfer is comprised entirely of empty (unaffiliated) regions.\n+      size_t consumed_unaffiliated_regions = (consumed_from_young_cset + region_size_bytes - 1) \/ region_size_bytes;\n+      size_t available_unaffiliated_regions = ((young_unaffiliated_regions > consumed_unaffiliated_regions)?\n+                                               young_unaffiliated_regions - consumed_unaffiliated_regions: 0);\n+#ifdef KELVIN_DEBUG\n+      log_info(gc)(\" excess_young_reserves: %zu\", excess_young_reserves);\n+      log_info(gc)(\" consumed_unaffiliated_regions: %zu\", consumed_unaffiliated_regions);\n+      log_info(gc)(\" available_unaffiliated_regions: %zu\", available_unaffiliated_regions);\n+#endif\n+      regions_for_old_expansion = MIN2(available_unaffiliated_regions, excess_young_reserves \/ region_size_bytes);\n+    } else {\n+      regions_for_old_expansion = 0;\n+    }\n+#ifdef KELVIN_DEBUG\n+    log_info(gc)(\" regions_for_old_expansion: %zu\", regions_for_old_expansion);\n+#endif\n+    if (regions_for_old_expansion > 0) {\n+      log_info(gc)(\"Augmenting old-gen evacuation budget from unexpended young-generation reserve by %zu regions\",\n+                   regions_for_old_expansion);\n+      add_regions_to_old = regions_for_old_expansion;\n+      size_t budget_supplement = region_size_bytes * regions_for_old_expansion;\n+      size_t supplement_without_waste = (size_t) (((double) budget_supplement) \/ ShenandoahOldEvacWaste);\n+      _old_evacuation_budget += supplement_without_waste;\n+      _unspent_unfragmented_old_budget += supplement_without_waste;\n+#ifdef KELVIN_DEBUG\n+      log_info(gc)(\" budget_supplement: %zu, without waste: %zu\", budget_supplement, supplement_without_waste);\n+      log_info(gc)(\" expanded _old_evacuation_budget: %zu\", _old_evacuation_budget);\n+      log_info(gc)(\" expanded _unspent_unfragmented_old_budget: %zu\", _unspent_unfragmented_old_budget);\n+      log_info(gc)(\" aumgenting old_gen->evacuatino_reserve() by %zu\", budget_supplement);\n+      log_info(gc)(\" decreasing young_gen->evacuation_reserve() by the same\");\n+#endif\n+\n+      _old_generation->augment_evacuation_reserve(budget_supplement);\n+      young_generation->set_evacuation_reserve(max_young_cset - budget_supplement);\n+\n+      return add_old_regions_to_cset();\n+    } else {\n+      add_regions_to_old = 0;\n+      return false;\n+    }\n+  }\n+}\n+\n@@ -327,1 +500,3 @@\n-\n+#ifdef ASSERT\n+  bool reclaimed_immediate = false;\n+#endif\n@@ -340,4 +515,4 @@\n-        \/\/ Only place regular or pinned regions with live data into the candidate set.\n-        \/\/ Pinned regions cannot be evacuated, but we are not actually choosing candidates\n-        \/\/ for the collection set here. That happens later during the next young GC cycle,\n-        \/\/ by which time, the pinned region may no longer be pinned.\n+      \/\/ Only place regular or pinned regions with live data into the candidate set.\n+      \/\/ Pinned regions cannot be evacuated, but we are not actually choosing candidates\n+      \/\/ for the collection set here. That happens later during the next young GC cycle,\n+      \/\/ by which time, the pinned region may no longer be pinned.\n@@ -346,0 +521,8 @@\n+#ifdef ASSERT\n+        if (!reclaimed_immediate) {\n+          reclaimed_immediate = true;\n+          \/\/ Inform the free-set that old trash regions may temporarily violate OldCollector bounds\n+          shenandoah_assert_heaplocked();\n+          heap->free_set()->advise_of_old_trash();\n+        }\n+#endif\n@@ -364,0 +547,8 @@\n+#ifdef ASSERT\n+        if (!reclaimed_immediate) {\n+          reclaimed_immediate = true;\n+          \/\/ Inform the free-set that old trash regions may temporarily violate OldCollector bounds\n+          shenandoah_assert_heaplocked();\n+          heap->free_set()->advise_of_old_trash();\n+        }\n+#endif\n@@ -542,0 +733,1 @@\n+  _live_bytes_in_unprocessed_candidates = 0;\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/heuristics\/shenandoahOldHeuristics.cpp","additions":344,"deletions":152,"binary":false,"changes":496,"status":"modified"},{"patch":"@@ -105,0 +105,24 @@\n+  \/\/ State variables involved in construction of a mixed-evacuation collection set.  These variables are initialized\n+  \/\/ when client code invokes prime_collection_set().  They are consulted, and sometimes modified, when client code\n+  \/\/ calls top_off_collection_set() to possibly expand the number of old-gen regions in a mixed evacuation cset, and by\n+  \/\/ finalize_mixed_evacs(), which prepares the way for mixed evacuations to begin.\n+  ShenandoahCollectionSet* _mixed_evac_cset;\n+  size_t _evacuated_old_bytes;\n+  size_t _collected_old_bytes;\n+  size_t _included_old_regions;\n+  size_t _old_evacuation_reserve;\n+  size_t _old_evacuation_budget;\n+\n+  \/\/ This represents the amount of memory that can be evacuated from old into initially empty regions during a mixed evacuation.\n+  \/\/ This is the total amount of unfragmented free memory in old divided by ShenandoahOldEvacWaste.\n+  size_t _unspent_unfragmented_old_budget;\n+\n+  \/\/ This represents the amount of memory that can be evacuated from old into initially non-empty regions during a mixed\n+  \/\/ evacuation.  This is the total amount of initially fragmented free memory in old divided by ShenandoahOldEvacWaste.\n+  size_t _unspent_fragmented_old_budget;\n+\n+  \/\/ If there is more available memory in old than is required by the intended mixed evacuation, the amount of excess\n+  \/\/ memory is represented by _excess_fragmented_old.  To convert this value into a promotion budget, multiply by\n+  \/\/ ShenandoahOldEvacWaste and divide by ShenandoahPromoWaste.\n+  size_t _excess_fragmented_old_budget;\n+\n@@ -125,0 +149,7 @@\n+  \/\/ This internal helper routine adds as many mixed evacuation candidate regions as fit within the old-gen evacuation budget\n+  \/\/ to the collection set.  This may be called twice to prepare for any given mixed evacuation cycle, the first time with\n+  \/\/ a conservative old evacuation budget, and the second time with a larger more aggressive old evacuation budget.  Returns\n+  \/\/ true iff we need to finalize mixed evacs.  (If no regions are added to the collection set, there is no need to finalize\n+  \/\/ mixed evacuations.)\n+  bool add_old_regions_to_cset();\n+\n@@ -131,2 +162,16 @@\n-  \/\/ Return true iff the collection set is primed with at least one old-gen region.\n-  bool prime_collection_set(ShenandoahCollectionSet* set);\n+  \/\/ Initialize instance variables to support the preparation of a mixed-evacuation collection set.  Adds as many\n+  \/\/ old candidate regions into the collection set as can fit within the iniital conservative old evacuation budget.\n+  \/\/ Returns true iff we need to finalize mixed evacs.\n+  bool prime_collection_set(ShenandoahCollectionSet* collection_set);\n+\n+  \/\/ If young evacuation did not consume all of its available evacuation reserve, add as many additional mixed-\n+  \/\/ evacuation candidate regions into the collection set as will fit within this excess repurposed reserved.\n+  \/\/ Returns true iff we need to finalize mixed evacs.  Upon return, the var parameter regions_to_xfer holds the\n+  \/\/ number of regions to transfer from young to old.\n+  bool top_off_collection_set(ssize_t &add_regions_to_old);\n+\n+  \/\/ Having added all eligible mixed-evacuation candidates to the collection set, this function updates the total count\n+  \/\/ of how much old-gen memory remains to be evacuated and adjusts the representation of old-gen regions that remain to\n+  \/\/ be evacuated, giving special attention to regions that are currently pinned.  It outputs relevant log messages and\n+  \/\/ returns true iff the collection set holds at least one unpinned mixed evacuation candidate.\n+  bool finalize_mixed_evacs();\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/heuristics\/shenandoahOldHeuristics.hpp","additions":47,"deletions":2,"binary":false,"changes":49,"status":"modified"},{"patch":"@@ -36,1 +36,1 @@\n-        : ShenandoahGenerationalHeuristics(generation) {\n+    : ShenandoahGenerationalHeuristics(generation) {\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/heuristics\/shenandoahYoungHeuristics.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -53,0 +53,2 @@\n+  _young_available_bytes_collected(0),\n+  _old_available_bytes_collected(0),\n@@ -107,0 +109,1 @@\n+    _old_available_bytes_collected += free;\n@@ -143,0 +146,1 @@\n+  _old_available_bytes_collected = 0;\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahCollectionSet.cpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -78,0 +78,4 @@\n+  \/\/ When a region having memory available to be allocated is added to the collection set, the region's available memory\n+  \/\/ should be subtracted from what's available.\n+  size_t                _old_available_bytes_collected;\n+  \n@@ -124,0 +128,3 @@\n+  \/\/ Returns the amount of free bytes in old regions in the collection set.\n+  size_t get_old_available_bytes_collected() const { return _old_available_bytes_collected; }\n+\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahCollectionSet.hpp","additions":7,"deletions":0,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -165,2 +165,0 @@\n-  assert(heap->is_concurrent_weak_root_in_progress(), \"Must be doing weak roots now\");\n-\n@@ -207,2 +205,0 @@\n-    entry_concurrent_update_refs_prepare(heap);\n-\n@@ -210,0 +206,1 @@\n+    entry_concurrent_update_refs_prepare(heap);\n@@ -230,0 +227,1 @@\n+    _abbreviated = true;\n@@ -238,1 +236,0 @@\n-    _abbreviated = true;\n@@ -282,0 +279,4 @@\n+  \/\/ After an abbreviated cycle, we reclaim immediate garbage.  Rebuild the freeset in order to establish\n+  \/\/ reserves for the next GC cycle.\n+  assert(_abbreviated, \"Only rebuild free set for abbreviated and old-marking cycles\");\n+  heap->rebuild_free_set(true \/*concurrent*\/);\n@@ -285,1 +286,0 @@\n-\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahConcurrentGC.cpp","additions":6,"deletions":6,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -330,1 +330,1 @@\n-  shenandoah_assert_heaplocked();\n+  shenandoah_assert_heaplocked_or_safepoint();\n@@ -443,0 +443,7 @@\n+void ShenandoahRegionPartitions::set_used_by(ShenandoahFreeSetPartitionId which_partition, size_t value) {\n+  shenandoah_assert_heaplocked();\n+  assert (which_partition < NumPartitions, \"selected free set must be valid\");\n+  _used[int(which_partition)] = value;\n+  _available[int(which_partition)] = _capacity[int(which_partition)] - value;\n+}\n+\n@@ -635,0 +642,27 @@\n+inline void ShenandoahRegionPartitions::adjust_interval_for_recycled_old_region_under_lock(ShenandoahHeapRegion* r) {\n+  assert(!r->is_trash() && (r->free() == _region_size_bytes), \"Bad argument\");\n+\n+  shenandoah_assert_heaplocked();\n+  idx_t idx = (idx_t) r->index();\n+  ShenandoahFreeSetPartitionId old_partition = ShenandoahFreeSetPartitionId::OldCollector;\n+  assert(_membership[int(old_partition)].is_set(idx), \"Region should be in OldCollector reserve\");\n+\n+  \/\/ Note that a recycled old trashed region may be in any one of the free set partitions according to the following scenarios:\n+  \/\/  1. The old region had already been retired, so it was NotFree, and we have not rebuilt free set. Region is still NotFree.\n+  \/\/  2. We recycled the region but we have not yet rebuilt the free set, so it is still in the OldCollector region.\n+  \/\/  3. We have found regions with alloc capacity but have not yet reserved_regions, so this is in Mutator set, and\n+  \/\/     the act of placing the region into the Mutator set properly adjusts interval for Mutator set.\n+  \/\/  4. During reserve_regions(), we moved this region into the Collector set, and the act of placing this region into\n+  \/\/     Collector set properly adjusts the interval for the Collector set.\n+  \/\/  5. During reserve_regions, we moved this region into the OldCollector set, and the act of placing this region into\n+  \/\/     OldCollector set properly adjusts the interval for the OldCollector set.\n+  \/\/ Only case 2 needs to be fixed up here.\n+  assert(_leftmosts[int(old_partition)] <= idx && _rightmosts[int(old_partition)] >= idx, \"sanity\");\n+  if (_leftmosts_empty[int(old_partition)] > idx) {\n+    _leftmosts_empty[int(old_partition)] = idx;\n+  }\n+  if (_rightmosts_empty[int(old_partition)] < idx) {\n+    _rightmosts_empty[int(old_partition)] = idx;\n+  }\n+}\n+\n@@ -904,1 +938,1 @@\n-void ShenandoahRegionPartitions::assert_bounds(bool validate_totals) {\n+void ShenandoahRegionPartitions::assert_bounds(bool validate_totals, bool old_trash_not_in_bounds) {\n@@ -980,0 +1014,21 @@\n+        \/\/ When old_trash_not_in_bounds, an old trashed region might reside in:\n+        \/\/ 1. NotFree if the region had already been retired\n+        \/\/ 2. OldCollector because the region was originally in OldCollector when it was identified as immediate garbage, or\n+        \/\/ 3. Mutator because we have run find_regions_with_alloc_capacity(), or\n+        \/\/ 4. Collector because reserve_regions moved from Mutator to Collector but we have not yet recycled the trash\n+        \/\/ 5. OldCollector because reserve_regions moved from Mutator to OldCollector but we have not yet recycled the trash\n+\n+        \/\/ In case 1, there is no issue with empty-free intervals.\n+        \/\/ In cases 3 - 5, there is no issue with empty-free intervals because the act of moving the region into the partition\n+        \/\/    causes the empty-free interval to be updated.\n+        \/\/ Only in case 2 do we need to disable the assert checking, but it is difficult to distinguish case 2 from case 5,\n+        \/\/    so we do not assert bounds for case 2 or case 5.\n+\n+        ShenandoahHeapRegion* r = ShenandoahHeap::heap()->get_region(i);\n+        if (old_trash_not_in_bounds &&\n+            (partition == ShenandoahFreeSetPartitionId::OldCollector) && r->is_old() && r->is_trash()) {\n+          \/\/ If Old trash has been identified but we have not yet rebuilt the freeset to acount for the trashed regions,\n+          \/\/ or if old trash has not yet been recycled, do not expect these trash regions to be within the OldCollector\n+          \/\/ partition's bounds.\n+          continue;\n+        }\n@@ -1024,1 +1079,1 @@\n-          \"Mutator free regions before the leftmost: %zd, bound %zd\",\n+          \"Mutator free region before the leftmost: %zd, bound %zd\",\n@@ -1027,1 +1082,1 @@\n-          \"Mutator free regions past the rightmost: %zd, bound %zd\",\n+          \"Mutator free region past the rightmost: %zd, bound %zd\",\n@@ -1032,6 +1087,6 @@\n-  assert (beg_off >= leftmost_empty(ShenandoahFreeSetPartitionId::Mutator),\n-          \"Mutator free empty regions before the leftmost: %zd, bound %zd\",\n-          beg_off, leftmost_empty(ShenandoahFreeSetPartitionId::Mutator));\n-  assert (end_off <= rightmost_empty(ShenandoahFreeSetPartitionId::Mutator),\n-          \"Mutator free empty regions past the rightmost: %zd, bound %zd\",\n-          end_off, rightmost_empty(ShenandoahFreeSetPartitionId::Mutator));\n+  assert (beg_off >= _leftmosts_empty[int(ShenandoahFreeSetPartitionId::Mutator)],\n+          \"free empty region (%zd) before the leftmost bound %zd\",\n+          beg_off, _leftmosts_empty[int(ShenandoahFreeSetPartitionId::Mutator)]);\n+  assert (end_off <= _rightmosts_empty[int(ShenandoahFreeSetPartitionId::Mutator)],\n+          \"free empty region (%zd) past the rightmost bound %zd\",\n+          end_off, _rightmosts_empty[int(ShenandoahFreeSetPartitionId::Mutator)]);\n@@ -1057,1 +1112,1 @@\n-          \"Collector free regions before the leftmost: %zd, bound %zd\",\n+          \"Collector free region before the leftmost: %zd, bound %zd\",\n@@ -1060,1 +1115,1 @@\n-          \"Collector free regions past the rightmost: %zd, bound %zd\",\n+          \"Collector free region past the rightmost: %zd, bound %zd\",\n@@ -1066,2 +1121,2 @@\n-          \"Collector free empty regions before the leftmost: %zd, bound %zd\",\n-          beg_off, leftmost_empty(ShenandoahFreeSetPartitionId::Collector));\n+          \"Collector free empty region before the leftmost: %zd, bound %zd\",\n+          beg_off, _leftmosts_empty[int(ShenandoahFreeSetPartitionId::Collector)]);\n@@ -1069,2 +1124,2 @@\n-          \"Collector free empty regions past the rightmost: %zd, bound %zd\",\n-          end_off, rightmost_empty(ShenandoahFreeSetPartitionId::Collector));\n+          \"Collector free empty region past the rightmost: %zd, bound %zd\",\n+          end_off, _rightmosts_empty[int(ShenandoahFreeSetPartitionId::Collector)]);\n@@ -1087,1 +1142,10 @@\n-  \/\/ If OldCollector partition is empty, leftmosts will both equal max, rightmosts will both equal zero.\n+  \/\/ Concurrent recycling of trash first recycles a region (changing its state from is_trash to is_empty without the heap lock),\n+  \/\/ then it acquires the heap lock, then it adjusts the partition for the newly recycled region and releases the lock.  After\n+  \/\/ all trashed regions have been recycled, we grab the heap lock again and clear the _old_trash_not_in_bounds flag.\n+  \/\/\n+  \/\/ Bottom line: if _old_trash_not_in_bounds, the ranges of old regions detected by examination of all region states may\n+  \/\/ be larger than the spans reported by leftmosts(OldColector) and rightmosts(OldCollector) and by the spans represented\n+  \/\/ by _leftmosts_empty[OldCollector] and _rightmosts_empty[OldCollector]\n+\n+  \/\/ If OldCollector partition is empty and !old_trash_not_in_bounds:\n+  \/\/    leftmosts will both equal max, rightmosts will both equal zero.\n@@ -1091,2 +1155,2 @@\n-  assert (beg_off >= leftmost(ShenandoahFreeSetPartitionId::OldCollector),\n-          \"OldCollector free regions before the leftmost: %zd, bound %zd\",\n+  assert (old_trash_not_in_bounds || (beg_off >= leftmost(ShenandoahFreeSetPartitionId::OldCollector)),\n+          \"free regions before the leftmost: %zd, bound %zd\",\n@@ -1094,2 +1158,2 @@\n-  assert (end_off <= rightmost(ShenandoahFreeSetPartitionId::OldCollector),\n-          \"OldCollector free regions past the rightmost: %zd, bound %zd\",\n+  assert (old_trash_not_in_bounds || (end_off <= rightmost(ShenandoahFreeSetPartitionId::OldCollector)),\n+          \"free regions past the rightmost: %zd, bound %zd\",\n@@ -1100,6 +1164,12 @@\n-  assert (beg_off >= _leftmosts_empty[int(ShenandoahFreeSetPartitionId::OldCollector)],\n-          \"OldCollector free empty regions before the leftmost: %zd, bound %zd\",\n-          beg_off, leftmost_empty(ShenandoahFreeSetPartitionId::OldCollector));\n-  assert (end_off <= _rightmosts_empty[int(ShenandoahFreeSetPartitionId::OldCollector)],\n-          \"OldCollector free empty regions past the rightmost: %zd, bound %zd\",\n-          end_off, rightmost_empty(ShenandoahFreeSetPartitionId::OldCollector));\n+  assert (old_trash_not_in_bounds || (beg_off >= _leftmosts_empty[int(ShenandoahFreeSetPartitionId::OldCollector)]),\n+          \"free empty region (%zd) before the leftmost bound %zd, old_trash_not_in_bounds: no, region %s trash\",\n+          beg_off, _leftmosts_empty[int(ShenandoahFreeSetPartitionId::OldCollector)],\n+          ((beg_off >= _max)? \"out of bounds is not\":\n+           (ShenandoahHeap::heap()->get_region(_leftmosts_empty[int(ShenandoahFreeSetPartitionId::OldCollector)])->is_trash()?\n+            \"is\": \"is not\")));\n+  assert (old_trash_not_in_bounds || (end_off <= _rightmosts_empty[int(ShenandoahFreeSetPartitionId::OldCollector)]),\n+          \"free empty region (%zd) past the rightmost bound %zd, old_trash_not_in_bounds: no, region %s trash\",\n+          end_off, _rightmosts_empty[int(ShenandoahFreeSetPartitionId::OldCollector)],\n+          ((end_off < 0)? \"out of bounds is not\" :\n+           (ShenandoahHeap::heap()->get_region(_rightmosts_empty[int(ShenandoahFreeSetPartitionId::OldCollector)])->is_trash()?\n+            \"is\": \"is not\")));\n@@ -1194,0 +1264,1 @@\n+  _old_trash_not_in_bounds(false),\n@@ -1265,1 +1336,1 @@\n-  _partitions.assert_bounds(true);\n+  _partitions.assert_bounds(true, _old_trash_not_in_bounds);\n@@ -1500,0 +1571,4 @@\n+  \/\/ We must call try_recycle_under_lock() even if !r->is_trash().  The reason is that if r is being recycled at this\n+  \/\/ moment by a GC worker thread, it may appear to be not trash even though it has not yet been fully recycled.  If\n+  \/\/ we proceed without waiting for the worker to finish recycling the region, the worker thread may overwrite the\n+  \/\/ region's affiliation with FREE after we set the region's affiliation to req.afiliation() below\n@@ -1501,0 +1576,9 @@\n+#ifdef ASSERT\n+  assert(!r->is_trash(), \"try_recycle_under_lock() should assure that region is recycled\");\n+  if (_old_trash_not_in_bounds &&\n+      r->is_empty() && _partitions.in_free_set(ShenandoahFreeSetPartitionId::OldCollector, r->index())) {\n+    \/\/ Note: if assertions are not enforced, there's no rush to adjust this interval.  We'll adjust the\n+    \/\/ interval when we eventually rebuild the free set.\n+    _partitions.adjust_interval_for_recycled_old_region_under_lock(r);\n+  }\n+#endif\n@@ -1502,1 +1586,0 @@\n-\n@@ -1679,1 +1762,1 @@\n-  _partitions.assert_bounds(true);\n+  _partitions.assert_bounds(true, _old_trash_not_in_bounds);\n@@ -1811,0 +1894,1 @@\n+\n@@ -1831,1 +1915,1 @@\n-  _partitions.assert_bounds(true);\n+  _partitions.assert_bounds(true, _old_trash_not_in_bounds);\n@@ -1841,0 +1925,3 @@\n+#ifdef ASSERT\n+  bool _old_trash_not_in_bounds;\n+#endif\n@@ -1928,3 +2015,13 @@\n-  ShenandoahRecycleTrashedRegionClosure(ShenandoahRegionPartitions* p): ShenandoahHeapRegionClosure() {\n-    _partitions = p;\n-    _recycled_region_count = 0;\n+#ifdef ASSERT\n+  ShenandoahRecycleTrashedRegionClosure(ShenandoahRegionPartitions* partitions, bool old_trash_not_in_bounds):\n+    ShenandoahHeapRegionClosure(),\n+    _old_trash_not_in_bounds(old_trash_not_in_bounds),\n+    _partitions(partitions),\n+    _recycled_region_count(0)\n+#else\n+  ShenandoahRecycleTrashedRegionClosure(ShenandoahRegionPartitions* partitions):\n+    ShenandoahHeapRegionClosure(),\n+    _partitions(partitions),\n+    _recycled_region_count(0)\n+#endif\n+  {\n@@ -1938,1 +2035,15 @@\n-    r->try_recycle();\n+    if (r->is_trash()) {\n+#ifdef ASSERT\n+      ShenandoahHeapLocker locker(ShenandoahHeap::heap()->lock());\n+      r->try_recycle_under_lock();\n+      assert(!r->is_trash(), \"try_recycle_under_lock() should assure that region is recycled\");\n+      if (_old_trash_not_in_bounds &&\n+          r->is_empty() && _partitions->in_free_set(ShenandoahFreeSetPartitionId::OldCollector, r->index())) {\n+        \/\/ Note: if assertions are not enforced, there's no rush to adjust this interval.  We'll adjust the\n+        \/\/ interval when we eventually rebuild the free set.\n+        _partitions->adjust_interval_for_recycled_old_region_under_lock(r);\n+      }\n+#else\n+      r->try_recycle();\n+#endif\n+    }\n@@ -1952,1 +2063,3 @@\n-\n+#ifdef ASSERT\n+  ShenandoahRecycleTrashedRegionClosure closure(&_partitions, _old_trash_not_in_bounds);\n+#else\n@@ -1954,0 +2067,1 @@\n+#endif\n@@ -1955,0 +2069,4 @@\n+#ifdef ASSERT\n+  ShenandoahHeapLocker locker(_heap->lock());\n+  _old_trash_not_in_bounds = false;\n+#endif\n@@ -1975,1 +2093,1 @@\n-    _partitions.assert_bounds(true);\n+    _partitions.assert_bounds(true, _old_trash_not_in_bounds);\n@@ -2029,1 +2147,1 @@\n-      _partitions.assert_bounds(true);\n+      _partitions.assert_bounds(true, _old_trash_not_in_bounds);\n@@ -2060,1 +2178,1 @@\n-  _partitions.assert_bounds(true);\n+  _partitions.assert_bounds(true, _old_trash_not_in_bounds);\n@@ -2141,2 +2259,2 @@\n-      \/\/ Trashed regions represent immediate garbage identified by final mark and regions that had been in the collection\n-      \/\/ partition but have not yet been \"cleaned up\" following update refs.\n+      \/\/ Trashed regions represent regions that had been in the collection set (or may have been identified as immediate garbage)\n+      \/\/ but have not yet been \"cleaned up\".  The cset regions are not \"trashed\" until we have finished update refs.\n@@ -2144,0 +2262,6 @@\n+        \/\/ We're going to place this region into the Mutator set.  We increment old_trashed_regions because this count represents\n+        \/\/ regions that the old generation is entitled to without any transfer from young.  We do not place this region into\n+        \/\/ the OldCollector partition at this time.  Instead, we let reserve_regions() decide whether to place this region\n+        \/\/ into the OldCollector partition.  Deferring the decision allows reserve_regions() to more effectively pack the\n+        \/\/ OldCollector regions into high-address memory.  We do not adjust capacities of old and young generations at this\n+        \/\/ time.  At the end of finish_rebuild(), the capacities are adjusted based on the results of reserve_regions().\n@@ -2164,1 +2288,1 @@\n-          \/\/ Both young and old collected regions (trashed) are placed into the Mutator set\n+          \/\/ Both young and old (possibly immediately) collected regions (trashed) are placed into the Mutator set\n@@ -2228,4 +2352,13 @@\n-          oop obj = cast_to_oop(region->bottom());\n-          size_t byte_size = obj->size() * HeapWordSize;\n-          size_t region_span = ShenandoahHeapRegion::required_regions(byte_size);\n-          humongous_waste_bytes = region_span * ShenandoahHeapRegion::region_size_bytes() - byte_size;\n+          \/\/ Since rebuild does not necessarily happen at a safepoint, a newly allocated humongous object may not have been\n+          \/\/ fully initialized.  Therefore, we cannot safely consult its header.\n+          ShenandoahHeapRegion* last_of_humongous_continuation = region;\n+          size_t next_idx;\n+          for (next_idx = idx + 1; next_idx < num_regions; next_idx++) {\n+            ShenandoahHeapRegion* humongous_cont_candidate = _heap->get_region(next_idx);\n+            if (!humongous_cont_candidate->is_humongous_continuation()) {\n+              break;\n+            }\n+            last_of_humongous_continuation = humongous_cont_candidate;\n+          }\n+          \/\/ For humongous regions, used() is established while holding the global heap lock so it is reliable here\n+          humongous_waste_bytes = ShenandoahHeapRegion::region_size_bytes() - last_of_humongous_continuation->used();\n@@ -2300,1 +2433,1 @@\n-  _partitions.assert_bounds(true);\n+  _partitions.assert_bounds(true, _old_trash_not_in_bounds);\n@@ -2338,1 +2471,1 @@\n-  _partitions.assert_bounds(true);\n+  _partitions.assert_bounds(true, _old_trash_not_in_bounds);\n@@ -2420,1 +2553,1 @@\n-  _partitions.assert_bounds(true);\n+  _partitions.assert_bounds(true, _old_trash_not_in_bounds);\n@@ -2487,1 +2620,1 @@\n-  _partitions.assert_bounds(true);\n+  _partitions.assert_bounds(true, _old_trash_not_in_bounds);\n@@ -2562,1 +2695,1 @@\n-  _partitions.assert_bounds(true);\n+  _partitions.assert_bounds(true, _old_trash_not_in_bounds);\n@@ -2608,0 +2741,8 @@\n+#ifdef KELVIN_DEPRECATE\n+void ShenandoahFreeSet::rebuild() {\n+  size_t young_trashed_regions, old_trashed_regions, first_old_region, last_old_region, old_region_count;\n+  prepare_to_rebuild(young_trashed_regions, old_trashed_regions, first_old_region, last_old_region, old_region_count);\n+  finish_rebuild(young_trashed_regions, old_trashed_regions, old_region_count);\n+}\n+#endif\n+\n@@ -2620,2 +2761,2 @@\n-void ShenandoahFreeSet::finish_rebuild(size_t young_trashed_regions, size_t old_trashed_regions, size_t old_region_count,\n-                                       bool have_evacuation_reserves) {\n+\n+void ShenandoahFreeSet::finish_rebuild(size_t young_cset_regions, size_t old_cset_regions, size_t old_region_count) {\n@@ -2626,2 +2767,1 @@\n-    compute_young_and_old_reserves(young_trashed_regions, old_trashed_regions, have_evacuation_reserves,\n-                                   young_reserve, old_reserve);\n+    compute_young_and_old_reserves(young_cset_regions, old_cset_regions, young_reserve, old_reserve);\n@@ -2641,1 +2781,1 @@\n-  _partitions.assert_bounds(true);\n+  _partitions.assert_bounds(true, _old_trash_not_in_bounds);\n@@ -2643,0 +2783,36 @@\n+  if (_heap->mode()->is_generational()) {\n+    \/\/ Clear the region balance until it is adjusted in preparation for a subsequent GC cycle.\n+    _heap->old_generation()->set_region_balance(0);\n+  }\n+  \/\/ Even though we have finished rebuild, old trashed regions may not yet have been recycled, so leave\n+  \/\/ _old_trash_not_in_bounds as is.  Following rebuild, old trashed regions may reside in Mutator, Collector,\n+  \/\/ or OldCollector partitions.\n+}\n+\n+\n+\/\/ Reduce old reserve (when there are insufficient resources to satisfy the original request).\n+void ShenandoahFreeSet::reduce_old_reserve(size_t adjusted_old_reserve, size_t requested_old_reserve) {\n+  ShenandoahOldGeneration* const old_generation = _heap->old_generation();\n+  size_t requested_promoted_reserve = old_generation->get_promoted_reserve();\n+  size_t requested_old_evac_reserve = old_generation->get_evacuation_reserve();\n+  assert(adjusted_old_reserve < requested_old_reserve, \"Only allow reduction\");\n+  assert(requested_promoted_reserve + requested_old_evac_reserve >= adjusted_old_reserve, \"Sanity\");\n+  size_t delta = requested_old_reserve - adjusted_old_reserve;\n+\n+  if (requested_promoted_reserve >= delta) {\n+    requested_promoted_reserve -= delta;\n+    old_generation->set_promoted_reserve(requested_promoted_reserve);\n+  } else {\n+    delta -= requested_promoted_reserve;\n+    requested_promoted_reserve = 0;\n+    requested_old_evac_reserve -= delta;\n+    old_generation->set_promoted_reserve(requested_promoted_reserve);\n+    old_generation->set_evacuation_reserve(requested_old_evac_reserve);\n+  }\n+}\n+\n+\/\/ Reduce young reserve (when there are insufficient resources to satisfy the original request).\n+void ShenandoahFreeSet::reduce_young_reserve(size_t adjusted_young_reserve, size_t requested_young_reserve) {\n+  ShenandoahYoungGeneration* const young_generation = _heap->young_generation();\n+  assert(adjusted_young_reserve < requested_young_reserve, \"Only allow reduction\");\n+  young_generation->set_evacuation_reserve(adjusted_young_reserve);\n@@ -2659,1 +2835,0 @@\n-                                                       bool have_evacuation_reserves,\n@@ -2676,0 +2851,17 @@\n+  assert(young_capacity >= young_generation->used(),\n+         \"Young capacity (%zu) must exceed used (%zu)\", young_capacity, young_generation->used());\n+\n+  size_t young_available = young_capacity - young_generation->used();\n+  young_available += young_trashed_regions * region_size_bytes;\n+\n+#undef KELVIN_DEBUG\n+#ifdef KELVIN_DEBUG\n+  log_info(gc)(\"compute_young_and_old_reserves(%zu, %zu)\", young_trashed_regions, old_trashed_regions);\n+  log_info(gc)(\"  (should have called compute_old_generation_balance() before here.\");\n+  log_info(gc)(\"   old_available: %zu (including %zu unaffiliated regions)\", old_available, old_unaffiliated_regions);\n+  log_info(gc)(\" young_available: %zu (including %zu unaffiliated regions)\", young_available, young_unaffiliated_regions);\n+#endif\n+\n+  assert(young_available >= young_unaffiliated_regions * region_size_bytes, \"sanity\");\n+  assert(old_available >= old_unaffiliated_regions * region_size_bytes, \"sanity\");\n+\n@@ -2680,0 +2872,3 @@\n+#ifdef KELVIN_DEBUG\n+  log_info(gc)(\" old_region_balance is %zd\", old_region_balance);\n+#endif\n@@ -2695,0 +2890,1 @@\n+    young_available += xfer_bytes;\n@@ -2703,26 +2899,7 @@\n-  if (have_evacuation_reserves) {\n-    \/\/ We are rebuilding at the end of final mark, having already established evacuation budgets for this GC pass.\n-    const size_t promoted_reserve = old_generation->get_promoted_reserve();\n-    const size_t old_evac_reserve = old_generation->get_evacuation_reserve();\n-    young_reserve_result = young_generation->get_evacuation_reserve();\n-    old_reserve_result = promoted_reserve + old_evac_reserve;\n-    if (old_reserve_result > old_available) {\n-      \/\/ Try to transfer memory from young to old.\n-      size_t old_deficit = old_reserve_result - old_available;\n-      size_t old_region_deficit = (old_deficit + region_size_bytes - 1) \/ region_size_bytes;\n-      if (young_unaffiliated_regions < old_region_deficit) {\n-        old_region_deficit = young_unaffiliated_regions;\n-      }\n-      young_unaffiliated_regions -= old_region_deficit;\n-      old_unaffiliated_regions += old_region_deficit;\n-      old_region_balance -= old_region_deficit;\n-      old_generation->set_region_balance(old_region_balance);\n-    }\n-  } else {\n-    \/\/ We are rebuilding at end of GC, so we set aside budgets specified on command line (or defaults)\n-    young_reserve_result = (young_capacity * ShenandoahEvacReserve) \/ 100;\n-    \/\/ The auto-sizer has already made old-gen large enough to hold all anticipated evacuations and promotions.\n-    \/\/ Affiliated old-gen regions are already in the OldCollector free set.  Add in the relevant number of\n-    \/\/ unaffiliated regions.\n-    old_reserve_result = old_available;\n-  }\n+  const size_t promoted_reserve = old_generation->get_promoted_reserve();\n+  const size_t old_evac_reserve = old_generation->get_evacuation_reserve();\n+  young_reserve_result = young_generation->get_evacuation_reserve();\n+  old_reserve_result = promoted_reserve + old_evac_reserve;\n+  assert(old_reserve_result + young_reserve_result <= old_available + young_available,\n+         \"Cannot reserve (%zu + %zu + %zu) more than is available: %zu + %zu\",\n+         promoted_reserve, old_evac_reserve, young_reserve_result, old_available, young_available);\n@@ -2735,1 +2912,1 @@\n-      _partitions.capacity_of(ShenandoahFreeSetPartitionId::OldCollector) + old_unaffiliated_regions * region_size_bytes) {\n+      _partitions.available_in(ShenandoahFreeSetPartitionId::OldCollector) + old_unaffiliated_regions * region_size_bytes) {\n@@ -2737,1 +2914,1 @@\n-      _partitions.capacity_of(ShenandoahFreeSetPartitionId::OldCollector) + old_unaffiliated_regions * region_size_bytes;\n+      _partitions.available_in(ShenandoahFreeSetPartitionId::OldCollector) + old_unaffiliated_regions * region_size_bytes;\n@@ -2982,1 +3159,1 @@\n-  _partitions.assert_bounds(true);\n+  _partitions.assert_bounds(true, _old_trash_not_in_bounds);\n@@ -2988,0 +3165,2 @@\n+      assert(_heap->mode()->is_generational(), \"to_old_reserve > 0 implies generational mode\");\n+      reduce_old_reserve(old_reserve, to_reserve_old);\n@@ -2991,0 +3170,3 @@\n+      if (_heap->mode()->is_generational()) {\n+        reduce_young_reserve(reserve, to_reserve);\n+      }\n@@ -2992,1 +3174,1 @@\n-                          PROPERFMTARGS(to_reserve), PROPERFMTARGS(reserve));\n+                         PROPERFMTARGS(to_reserve), PROPERFMTARGS(reserve));\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahFreeSet.cpp","additions":269,"deletions":87,"binary":false,"changes":356,"status":"modified"},{"patch":"@@ -224,0 +224,4 @@\n+  \/\/ For recycled region r in the OldCollector partition but possibly not within the interval for empty OldCollector regions,\n+  \/\/ expand the empty interval to include this region.\n+  inline void adjust_interval_for_recycled_old_region_under_lock(ShenandoahHeapRegion* r);\n+\n@@ -380,6 +384,1 @@\n-  inline void set_used_by(ShenandoahFreeSetPartitionId which_partition, size_t value) {\n-    shenandoah_assert_heaplocked();\n-    assert (which_partition < NumPartitions, \"selected free set must be valid\");\n-    _used[int(which_partition)] = value;\n-    _available[int(which_partition)] = _capacity[int(which_partition)] - value;\n-  }\n+  inline void set_used_by(ShenandoahFreeSetPartitionId which_partition, size_t value);\n@@ -409,1 +408,6 @@\n-  void assert_bounds(bool validate_totals) NOT_DEBUG_RETURN;\n+  \/\/ Iff validate_totals is true, assert_bounds() confirms not only that bounds are correct, but also that total\n+  \/\/ capacities and used within each partition are correct.\n+  \/\/\n+  \/\/ The old_trash_not_in_bounds argument denotes that some old trash has not yet been recycled.  In this scenario,\n+  \/\/ assert_bounds() allows that certain old regions do not reside within the bounds for that partition.\n+  void assert_bounds(bool validate_totals, bool old_trash_not_in_bounds) NOT_DEBUG_RETURN;\n@@ -446,0 +450,3 @@\n+  \/\/ Value is a don't care in release builds\n+  bool _old_trash_not_in_bounds;\n+\n@@ -632,0 +639,3 @@\n+  void reduce_young_reserve(size_t adjusted_young_reserve, size_t requested_young_reserve);\n+  void reduce_old_reserve(size_t adjusted_old_reserve, size_t requested_old_reserve);\n+\n@@ -718,0 +728,8 @@\n+#ifdef KELVIN_DEPRECATE\n+  \/\/ Rebuild the free set.  This combines the effects of prepare_to_rebuild() and finish_rebuild() with no intervening\n+  \/\/ efforts to rebalance generation sizes.  When the free set is rebuild, we reserve sufficient memory within the\n+  \/\/ collector partition (and, for generational mode, the old collector partition) based on the amount reserved\n+  \/\/ by heuristics to support the next planned evacuation effort.\n+  void rebuild();\n+#endif\n+\n@@ -720,2 +738,3 @@\n-  \/\/ young_cset_regions is the number of regions currently in the young cset if we are starting to evacuate, or zero\n-  \/\/   old_cset_regions is the number of regions currently in the old cset if we are starting a mixed evacuation, or zero\n+  \/\/ young_trashed_regions is the number of trashed regions (immediate garbage at final mark, cset regions after update refs)\n+  \/\/   old_trashed_regions is the number of trashed regions\n+  \/\/                       (immediate garbage at final old mark, cset regions after update refs for mixed evac)\n@@ -725,1 +744,1 @@\n-  void prepare_to_rebuild(size_t &young_cset_regions, size_t &old_cset_regions,\n+  void prepare_to_rebuild(size_t &young_trashed_regions, size_t &old_trashed_regions,\n@@ -729,12 +748,5 @@\n-  \/\/ hold the results of evacuating to young-gen and to old-gen, and have_evacuation_reserves should be true.\n-  \/\/ These quantities, stored as reserves for their respective generations, are consulted prior to rebuilding\n-  \/\/ the free set (ShenandoahFreeSet) in preparation for evacuation.  When the free set is rebuilt, we make sure\n-  \/\/ to reserve sufficient memory in the collector and old_collector sets to hold evacuations.\n-  \/\/\n-  \/\/ We also rebuild the free set at the end of GC, as we prepare to idle GC until the next trigger.  In this case,\n-  \/\/ have_evacuation_reserves is false because we don't yet know how much memory will need to be evacuated in the\n-  \/\/ next GC cycle.  When have_evacuation_reserves is false, the free set rebuild operation reserves for the collector\n-  \/\/ and old_collector sets based on alternative mechanisms, such as ShenandoahEvacReserve, ShenandoahOldEvacReserve, and\n-  \/\/ ShenandoahOldCompactionReserve.  In a future planned enhancement, the reserve for old_collector set when the\n-  \/\/ evacuation reserves are unknown, is based in part on anticipated promotion as determined by analysis of live data\n-  \/\/ found during the previous GC pass which is one less than the current tenure age.\n+  \/\/ hold the results of evacuating to young-gen and to old-gen.  These quantities, stored in reserves for their\n+  \/\/ respective generations, are consulted prior to rebuilding the free set (ShenandoahFreeSet) in preparation for\n+  \/\/ evacuation.  When the free set is rebuilt, we make sure to reserve sufficient memory in the collector and\n+  \/\/ old_collector sets to hold evacuations.  Likewise, at the end of update refs, we rebuild the free set in order\n+  \/\/ to set aside reserves to be consumed during the next GC cycle.\n@@ -742,2 +754,3 @@\n-  \/\/ young_cset_regions is the number of regions currently in the young cset if we are starting to evacuate, or zero\n-  \/\/   old_cset_regions is the number of regions currently in the old cset if we are starting a mixed evacuation, or zero\n+  \/\/ young_trashed_regions is the number of trashed regions (immediate garbage at final mark, cset regions after update refs)\n+  \/\/   old_trashed_regions is the number of trashed regions\n+  \/\/                       (immediate garbage at final old mark, cset regions after update refs for mixed evac)\n@@ -745,5 +758,1 @@\n-  \/\/ have_evacuation_reserves is true iff the desired values of young-gen and old-gen evacuation reserves and old-gen\n-  \/\/                    promotion reserve have been precomputed (and can be obtained by invoking\n-  \/\/                    <generation>->get_evacuation_reserve() or old_gen->get_promoted_reserve()\n-  void finish_rebuild(size_t young_cset_regions, size_t old_cset_regions, size_t num_old_regions,\n-                      bool have_evacuation_reserves = false);\n+  void finish_rebuild(size_t young_trashed_regions, size_t old_trashed_regions, size_t num_old_regions);\n@@ -761,0 +770,8 @@\n+#ifdef ASSERT\n+  \/\/ Advise FreeSet that old trash regions have not yet been accounted for in OldCollector partition bounds\n+  void advise_of_old_trash() {\n+    shenandoah_assert_heaplocked();\n+    _old_trash_not_in_bounds = true;\n+  }\n+#endif\n+\n@@ -781,0 +798,5 @@\n+  \/\/ Use this version of available() if the heap lock is held.\n+  inline size_t available_locked() const {\n+    return _partitions.available_in(ShenandoahFreeSetPartitionId::Mutator);\n+  }\n+\n@@ -782,2 +804,6 @@\n-  inline size_t humongous_waste_in_mutator() const { return _partitions.humongous_waste(ShenandoahFreeSetPartitionId::Mutator); }\n-  inline size_t humongous_waste_in_old() const { return _partitions.humongous_waste(ShenandoahFreeSetPartitionId::OldCollector); }\n+  inline size_t humongous_waste_in_mutator() const {\n+    return _partitions.humongous_waste(ShenandoahFreeSetPartitionId::Mutator);\n+  }\n+  inline size_t humongous_waste_in_old() const {\n+    return _partitions.humongous_waste(ShenandoahFreeSetPartitionId::OldCollector);\n+  }\n@@ -849,1 +875,1 @@\n-  void compute_young_and_old_reserves(size_t young_cset_regions, size_t old_cset_regions, bool have_evacuation_reserves,\n+  void compute_young_and_old_reserves(size_t young_cset_regions, size_t old_cset_regions,\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahFreeSet.hpp","additions":58,"deletions":32,"binary":false,"changes":90,"status":"modified"},{"patch":"@@ -525,0 +525,1 @@\n+      \/\/ No need to adjust_interval_for_recycled_old_region.  That will be taken care of during freeset rebuild.\n@@ -969,0 +970,1 @@\n+      \/\/ No need to adjust_interval_for_recycled_old_region.  That will be taken care of during freeset rebuild.\n@@ -1116,3 +1118,0 @@\n-    size_t young_cset_regions, old_cset_regions;\n-    size_t first_old, last_old, num_old;\n-    heap->free_set()->prepare_to_rebuild(young_cset_regions, old_cset_regions, first_old, last_old, num_old);\n@@ -1122,0 +1121,2 @@\n+    size_t young_trashed_regions, old_trashed_regions, first_old, last_old, num_old;\n+    heap->free_set()->prepare_to_rebuild(young_trashed_regions, old_trashed_regions, first_old, last_old, num_old);\n@@ -1125,2 +1126,1 @@\n-\n-    heap->free_set()->finish_rebuild(young_cset_regions, old_cset_regions, num_old);\n+    heap->free_set()->finish_rebuild(young_trashed_regions, old_trashed_regions, num_old);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahFullGC.cpp","additions":5,"deletions":5,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -250,0 +250,1 @@\n+  const size_t region_size_bytes = ShenandoahHeapRegion::region_size_bytes();\n@@ -251,0 +252,4 @@\n+#undef KELVIN_DEBUG\n+#ifdef KELVIN_DEBUG\n+  log_info(gc)(\"compute_evacuation_budgets()\");\n+#endif\n@@ -263,3 +268,7 @@\n-  \/\/ maximum_young_evacuation_reserve is upper bound on memory to be evacuated out of young\n-  const size_t maximum_young_evacuation_reserve = (young_generation->max_capacity() * ShenandoahEvacReserve) \/ 100;\n-  size_t young_evacuation_reserve = MIN2(maximum_young_evacuation_reserve, young_generation->available_with_reserve());\n+  \/\/ maximum_young_evacuation_reserve is upper bound on memory to be evacuated into young Collector Reserve.  This is\n+  \/\/ bounded at the end of previous GC cycle, based on available memory and balancing of evacuation to old and young.\n+  size_t maximum_young_evacuation_reserve = young_generation->get_evacuation_reserve();\n+#ifdef KELVIN_DEBUG\n+  log_info(gc)(\" maximum_young_evacuation_reserve: %zu, available_with_reserve: %zu\",\n+               maximum_young_evacuation_reserve, young_generation->available_with_reserve());\n+#endif\n@@ -271,1 +280,1 @@\n-  \/\/ Let SOEP = ShenandoahOldEvacRatioPercent,\n+  \/\/ Let SOEP = ShenandoahOldEvacPercent,\n@@ -283,1 +292,1 @@\n-  assert(ShenandoahOldEvacRatioPercent <= 100, \"Error\");\n+  assert(ShenandoahOldEvacPercent <= 100, \"Error\");\n@@ -285,2 +294,2 @@\n-  const size_t maximum_old_evacuation_reserve = (ShenandoahOldEvacRatioPercent == 100) ?\n-    old_available : MIN2((maximum_young_evacuation_reserve * ShenandoahOldEvacRatioPercent) \/ (100 - ShenandoahOldEvacRatioPercent),\n+  const size_t maximum_old_evacuation_reserve = (ShenandoahOldEvacPercent == 100) ?\n+    old_available : MIN2((maximum_young_evacuation_reserve * ShenandoahOldEvacPercent) \/ (100 - ShenandoahOldEvacPercent),\n@@ -289,0 +298,10 @@\n+  \/\/ In some cases, maximum_old_reserve < old_available (when limited by ShenandoahOldEvacPercent)\n+  \/\/ This limit affects mixed evacuations, but does not affect promotions.\n+\n+\n+#ifdef KELVIN_DEBUG\n+  log_info(gc)(\" old_available: %zu, ShenandoahOldEvacPercent limit: %zu\",\n+               old_available, (maximum_young_evacuation_reserve * ShenandoahOldEvacPercent) \/ (100 - ShenandoahOldEvacPercent));\n+  log_info(gc)(\" old_unaffiliated is: %zu\", old_generation->free_unaffiliated_regions() * region_size_bytes);\n+  log_info(gc)(\" maximum_old_evacuation_reserve: %zu\", maximum_old_evacuation_reserve);\n+#endif\n@@ -305,4 +324,2 @@\n-    \/\/ Set old_promo_reserve to enforce that no regions are preselected for promotion.  Such regions typically\n-    \/\/ have relatively high memory utilization.  We still call select_aged_regions() because this will prepare for\n-    \/\/ promotions in place, if relevant.\n-    old_promo_reserve = 0;\n+    \/\/ Use remnant of old_available to hold promotions.\n+    old_promo_reserve = old_available - maximum_old_evacuation_reserve;\n@@ -319,1 +336,1 @@\n-    old_promo_reserve = 0;\n+    old_promo_reserve = old_available - maximum_old_evacuation_reserve;\n@@ -322,1 +339,1 @@\n-    old_evacuation_reserve = 0;\n+    old_evacuation_reserve = old_available - maximum_old_evacuation_reserve;\n@@ -327,0 +344,1 @@\n+\n@@ -330,1 +348,1 @@\n-  const size_t old_free_unfragmented = old_generation->free_unaffiliated_regions() * ShenandoahHeapRegion::region_size_bytes();\n+  const size_t old_free_unfragmented = old_generation->free_unaffiliated_regions() * region_size_bytes;\n@@ -334,4 +352,2 @@\n-    \/\/ Let promo consume fragments of old-gen memory if not global\n-    if (!is_global()) {\n-      old_promo_reserve += delta;\n-    }\n+    \/\/ Let promo consume fragments of old-gen memory\n+    old_promo_reserve += delta;\n@@ -340,3 +356,4 @@\n-  \/\/ Preselect regions for promotion by evacuation (obtaining the live data to seed promoted_reserve),\n-  \/\/ and identify regions that will promote in place. These use the tenuring threshold.\n-  const size_t consumed_by_advance_promotion = select_aged_regions(old_promo_reserve);\n+  \/\/ If is_global(), we let garbage-first heuristic determine cset membership.  Otherwise, we give priority\n+  \/\/ to tenurable regions by preselecting regions for promotion by evacuation (obtaining the live data to seed promoted_reserve).\n+  \/\/ This also identifies regions that will be promoted in place. These use the tenuring threshold.\n+  const size_t consumed_by_advance_promotion = select_aged_regions(is_global()? 0: old_promo_reserve);\n@@ -344,0 +361,5 @@\n+  assert(consumed_by_advance_promotion <= old_promo_reserve, \"Do not promote more than budgeted\");\n+\n+  \/\/ The young evacuation reserve can be no larger than young_unaffiliated.  Planning to evacuate into partially consumed\n+  \/\/ young regions is doomed to failure if any of those partially consumed regions is selected for the collection set.\n+  size_t young_unaffiliated = young_generation->free_unaffiliated_regions() * region_size_bytes;\n@@ -348,1 +370,9 @@\n-  young_evacuation_reserve = MIN2(young_evacuation_reserve, young_generation->available_with_reserve());\n+  size_t young_evacuation_reserve = MIN2(maximum_young_evacuation_reserve, young_unaffiliated);\n+#ifdef KELVIN_DEBUG\n+  log_info(gc)(\" young_evacuation_reserve: %zu is MIN2(%zu, %zu)\",\n+               young_evacuation_reserve, maximum_young_evacuation_reserve, young_unaffiliated);\n+  log_info(gc)(\"   (previously, was bounded by %zu instead of %zu)\",\n+               young_generation->available_with_reserve(), young_unaffiliated);\n+  log_info(gc)(\" setting reserves to young: %zu, old evac: %zu, consumed_by_advance_promotin: %zu\",\n+               young_evacuation_reserve, old_evacuation_reserve, consumed_by_advance_promotion);\n+#endif\n@@ -363,2 +393,2 @@\n-\/\/\n-void ShenandoahGeneration::adjust_evacuation_budgets(ShenandoahHeap* const heap, ShenandoahCollectionSet* const collection_set) {\n+void ShenandoahGeneration::adjust_evacuation_budgets(ShenandoahHeap* const heap,\n+                                                     ShenandoahCollectionSet* const collection_set, ssize_t add_regions_to_old) {\n@@ -389,0 +419,6 @@\n+#ifdef KELVIN_DEBUG\n+  log_info(gc)(\"adjust_evacuation_budgets\");\n+  log_info(gc)(\" old_evacuated: %zu, old_evacuated_committed: %zu, reserved: %zu\",\n+               old_evacuated, old_evacuated_committed, old_evacuation_reserve);\n+#endif\n+\n@@ -394,0 +430,3 @@\n+#ifdef KELVIN_DEBUG\n+    log_info(gc)(\" Truncating old_evacuated_committed to reserve due to round-off errors\");\n+#endif\n@@ -398,1 +437,2 @@\n-    log_debug(gc, cset)(\"Shrinking old evac reserve to match old_evac_commited: \" PROPERFMT, PROPERFMTARGS(old_evacuated_committed));\n+    log_debug(gc, cset)(\"Shrinking old evac reserve to match old_evac_commited: \" PROPERFMT,\n+                        PROPERFMTARGS(old_evacuated_committed));\n@@ -401,0 +441,3 @@\n+#ifdef KELVIN_DEBUG\n+    log_info(gc)(\" Shrinking old_evacuation reserve to %zu (use it or lose it)\", old_evacuation_reserve);\n+#endif\n@@ -409,1 +452,1 @@\n-  size_t total_young_available = young_generation->available_with_reserve();\n+  size_t total_young_available = young_generation->available_with_reserve() - add_regions_to_old * region_size_bytes;;\n@@ -411,0 +454,7 @@\n+#ifdef KELVIN_DEBUG\n+  log_info(gc)(\" total_young_available: %zu computed from %zu - %zu * %zu\", total_young_available,\n+               young_generation->available_with_reserve(), add_regions_to_old, region_size_bytes);\n+  size_t alternative_young = young_generation->available_with_reserve() - collection_set->get_young_available_bytes_collected();\n+  log_info(gc)(\" alternative computation: %zu = %zu - %zu\", alternative_young,\n+                 young_generation->available_with_reserve(), collection_set->get_young_available_bytes_collected());\n+#endif\n@@ -413,1 +463,13 @@\n-  size_t old_available = old_generation->available();\n+  \/\/ We have not yet rebuilt the free set.  Memory that is available at this moment may not be available after the\n+  \/\/ collection set is constructed because some of the available memory may be contained within regions that are to be\n+  \/\/ selected for the collection set.  We were \n+\n+  size_t old_available =\n+    old_generation->available() + add_regions_to_old * region_size_bytes - collection_set->get_old_available_bytes_collected();\n+#ifdef KELVIN_DEBUG\n+  log_info(gc)(\" old_available computed as %zu (%zu + %zu * %zu - %zu)\",\n+               old_available, add_regions_to_old, region_size_bytes, old_generation->available(), collection_set->get_old_available_bytes_collected());\n+  log_info(gc)(\"   (previously computed as simply old_gen->available(): %zu)\", old_generation->available());\n+  log_info(gc)(\"   How can I be sure that cset regions are not in the available?\");\n+#endif\n+\n@@ -422,0 +484,6 @@\n+#ifdef KELVIN_DEBUG\n+    log_info(gc)(\" young_advance_promote_reserve_used: %zu, old_available: %zu, old_evacuated_committed: %zu, delta: %zu\",\n+                 young_advance_promoted_reserve_used, old_available, old_evacuated_committed,\n+                 old_available - old_evacuated_committed);\n+#endif\n+\n@@ -425,1 +493,15 @@\n-    young_advance_promoted_reserve_used = old_available - old_evacuated_committed;\n+    if (old_available > old_evacuated_committed) {\n+      young_advance_promoted_reserve_used = old_available - old_evacuated_committed;\n+#ifdef KELVIN_DEBUG\n+      log_info(gc)(\" overwriting young_advanced_promoted_reserved_used with updated value: %zu\",\n+                   young_advance_promoted_reserve_used);\n+#endif\n+    } else {\n+      young_advance_promoted_reserve_used = 0;\n+      old_evacuated_committed = old_available;\n+#ifdef KELVIN_DEBUG\n+      log_info(gc)(\" overwriting young_advanced_promoted_reserved_used with updated value: %zu\",\n+                   young_advance_promoted_reserve_used);\n+#endif\n+    }\n+    \/\/ TODO: reserve for full promotion reserve, not just for advance (preselected) promotion\n@@ -432,1 +514,1 @@\n-  size_t unaffiliated_old_regions = old_generation->free_unaffiliated_regions();\n+  size_t unaffiliated_old_regions = old_generation->free_unaffiliated_regions() + add_regions_to_old;\n@@ -434,3 +516,6 @@\n-  assert(old_available >= unaffiliated_old,\n-         \"Unaffiliated old (%zu is %zu * %zu) is a subset of old available (%zu)\",\n-         unaffiliated_old, unaffiliated_old_regions, region_size_bytes, old_available);\n+  assert(unaffiliated_old >= old_evacuated_committed, \"Do not evacuate (%zu) more than unaffiliated old (%zu)\",\n+         old_evacuated_committed, unaffiliated_old);\n+\n+#ifdef KELVIN_DEBUG\n+  log_info(gc)(\" unaffiliated_old: %zu should be >= old_evacuated_committed: %zu\", unaffiliated_old, old_evacuated_committed);\n+#endif\n@@ -445,0 +530,3 @@\n+#ifdef KELVIN_DEBUG\n+        log_info(gc)(\" giveaway_regions: %zu, excess_old: %zu\", giveaway_regions, excess_old);\n+#endif\n@@ -454,2 +542,2 @@\n-  \/\/ runway during evacuation and update-refs.\n-  size_t regions_to_xfer = 0;\n+  \/\/ runway during evacuation and update-refs.  We may make further adjustments to balance.\n+  ssize_t add_regions_to_young = 0;\n@@ -459,1 +547,5 @@\n-      regions_to_xfer = unaffiliated_old_regions;\n+      add_regions_to_young = unaffiliated_old_regions;\n+#ifdef KELVIN_DEBUG\n+      log_info(gc)(\" Transferring %zu regions to Mutator because excess_old > unaffiliated_old and unaffiliated_regions > 0\",\n+                   regions_to_xfer);\n+#endif\n@@ -464,1 +556,5 @@\n-    regions_to_xfer = MIN2(excess_regions, unaffiliated_old_regions);\n+    add_regions_to_young = MIN2(excess_regions, unaffiliated_old_regions);\n+#ifdef KELVIN_DEBUG\n+    log_info(gc)(\" Transferring %zu regions to Mutator because excess_old <= unaffiliated_old and unaffiliated_regions > 0\",\n+                 regions_to_xfer);\n+#endif\n@@ -466,2 +562,4 @@\n-  if (regions_to_xfer > 0) {\n-    excess_old -= regions_to_xfer * region_size_bytes;\n+\n+  if (add_regions_to_young > 0) {\n+    assert(excess_old >= add_regions_to_young * region_size_bytes, \"Cannot xfer more than excess old\");\n+    excess_old -= add_regions_to_young * region_size_bytes;\n@@ -475,0 +573,6 @@\n+\n+#ifdef KELVIN_DEBUG\n+  log_info(gc)(\" Adjusting total_promotion_reserve to %zu (expanding previous value by %zu)\", total_promotion_reserve, excess_old);\n+  log_info(gc)(\"   (regions are relinquished for mutator because we shrunk the old evacuation reserve above)\");\n+#endif\n+\n@@ -783,1 +887,1 @@\n-      \/\/ place, and preselect older regions that will be promoted by evacuation.\n+      \/\/ place, and preselected older regions that will be promoted by evacuation.\n@@ -786,8 +890,4 @@\n-      \/\/ Choose the collection set, including the regions preselected above for\n-      \/\/ promotion into the old generation.\n-      _heuristics->choose_collection_set(collection_set);\n-      if (!collection_set->is_empty()) {\n-        \/\/ only make use of evacuation budgets when we are evacuating\n-        adjust_evacuation_budgets(heap, collection_set);\n-      }\n-\n+      \/\/ Choose the collection set, including the regions preselected above for promotion into the old generation.\n+      ssize_t add_regions_to_old = _heuristics->choose_collection_set(collection_set);\n+      \/\/ Even if collection_set->is_empty(), we want to adjust budgets, making reserves available to mutator.\n+      adjust_evacuation_budgets(heap, collection_set, add_regions_to_old);\n@@ -816,5 +916,0 @@\n-    size_t young_cset_regions, old_cset_regions;\n-\n-    \/\/ We are preparing for evacuation.  At this time, we ignore cset region tallies.\n-    size_t first_old, last_old, num_old;\n-    _free_set->prepare_to_rebuild(young_cset_regions, old_cset_regions, first_old, last_old, num_old);\n@@ -822,0 +917,3 @@\n+    \/\/ We are preparing for evacuation.\n+    size_t young_trashed_regions, old_trashed_regions, first_old, last_old, num_old;\n+    _free_set->prepare_to_rebuild(young_trashed_regions, old_trashed_regions, first_old, last_old, num_old);\n@@ -824,1 +922,3 @@\n-      gen_heap->compute_old_generation_balance(young_cset_regions, old_cset_regions);\n+    size_t allocation_runway =\n+      gen_heap->young_generation()->heuristics()->bytes_of_allocation_runway_before_gc_trigger(young_trashed_regions);\n+      gen_heap->compute_old_generation_balance(allocation_runway, old_trashed_regions, young_trashed_regions);\n@@ -826,3 +926,1 @@\n-\n-    \/\/ Free set construction uses reserve quantities, because they are known to be valid here\n-    _free_set->finish_rebuild(young_cset_regions, old_cset_regions, num_old, true);\n+    _free_set->finish_rebuild(young_trashed_regions, old_trashed_regions, num_old);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahGeneration.cpp","additions":154,"deletions":56,"binary":false,"changes":210,"status":"modified"},{"patch":"@@ -66,1 +66,2 @@\n-  \/\/ Adjust evacuation budgets after choosing collection set.\n+  \/\/ Adjust evacuation budgets after choosing collection set.  The argument regions_to_xfer represents regions to be\n+  \/\/ transfered to old based on decisions made in top_off_collection_set()\n@@ -68,1 +69,1 @@\n-                                 ShenandoahCollectionSet* collection_set);\n+                                 ShenandoahCollectionSet* collection_set, ssize_t regions_to_xfer);\n@@ -147,0 +148,16 @@\n+  \/\/ Upon return from prepare_regions_and_collection_set(), certain parameters have been established to govern the\n+  \/\/ evacuation efforts that are about to begin.  In particular:\n+  \/\/\n+  \/\/ old_generation->get_promoted_reserve() represents the amount of memory within old-gen's available memory that has\n+  \/\/   been set aside to hold objects promoted from young-gen memory.  This represents an estimated percentage\n+  \/\/   of the live young-gen memory within the collection set.  If there is more data ready to be promoted than\n+  \/\/   can fit within this reserve, the promotion of some objects will be deferred until a subsequent evacuation\n+  \/\/   pass.\n+  \/\/\n+  \/\/ old_generation->get_evacuation_reserve() represents the amount of memory within old-gen's available memory that has been\n+  \/\/  set aside to hold objects evacuated from the old-gen collection set.\n+  \/\/\n+  \/\/ young_generation->get_evacuation_reserve() represents the amount of memory within young-gen's available memory that has\n+  \/\/  been set aside to hold objects evacuated from the young-gen collection set.  Conservatively, this value\n+  \/\/  equals the entire amount of live young-gen memory within the collection set, even though some of this memory\n+  \/\/  will likely be promoted.\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahGeneration.hpp","additions":19,"deletions":2,"binary":false,"changes":21,"status":"modified"},{"patch":"@@ -58,3 +58,0 @@\n-  \/\/ No need for old_gen->increase_used() as this was done when plabs were allocated.\n-  heap->reset_generation_reserves();\n-\n@@ -159,2 +156,5 @@\n-  \/\/ Invoke this in case we are able to transfer memory from OLD to YOUNG.\n-  heap->compute_old_generation_balance(0, 0);\n+\n+  \/\/ Invoke this in case we are able to transfer memory from OLD to YOUNG\n+  size_t allocation_runway =\n+    heap->young_generation()->heuristics()->bytes_of_allocation_runway_before_gc_trigger(0L);\n+  heap->compute_old_generation_balance(allocation_runway, 0, 0);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahGenerationalFullGC.cpp","additions":5,"deletions":5,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -241,1 +241,1 @@\n-                                        ShenandoahAffiliation target_gen) {\n+                                                    ShenandoahAffiliation target_gen) {\n@@ -302,1 +302,7 @@\n-      if (!is_promotion || !has_plab || (size > PLAB::min_size())) {\n+\n+      \/\/ Reduce, but do not totally eliminate promotion by shared allocation.  Shared allocations are normally\n+      \/\/ not a good thing.  Usually is much better to evacuate into a young-gen GCLAB than promote to old-gen with a\n+      \/\/ shared allocation.  Objects above a particular threshold size (6 * min-size) are considered to be worth the\n+      \/\/ effort required to promote by shared allocation.\n+      static size_t size_threshhold = MIN2(PLAB::max_size(), PLAB::min_size() * 6);\n+      if (!is_promotion || !has_plab || (size > size_threshhold)) {\n@@ -308,1 +314,1 @@\n-      \/\/ We choose not to promote objects smaller than PLAB::min_size() by way of shared allocations, as this is too\n+      \/\/ We choose not to promote objects smaller than size_threshold by way of shared allocations as this is too\n@@ -310,1 +316,1 @@\n-      \/\/ evacuation pass.  This condition is denoted by: is_promotion && has_plab && (size <= PLAB::min_size())\n+      \/\/ evacuation pass.  This condition is denoted by: is_promotion && has_plab && (size <= size_threshhold).\n@@ -591,7 +597,6 @@\n-\/\/ xfer_limit, and any surplus is transferred to the young generation.\n-\/\/\n-\/\/ xfer_limit is the maximum we're able to transfer from young to old based on either:\n-\/\/  1. an assumption that we will be able to replenish memory \"borrowed\" from young at the end of collection, or\n-\/\/  2. there is sufficient excess in the allocation runway during GC idle cycles\n-void ShenandoahGenerationalHeap::compute_old_generation_balance(size_t old_xfer_limit, size_t old_cset_regions) {\n-\n+\/\/ mutator_xfer_limit, and any surplus is transferred to the young generation.  mutator_xfer_limit is\n+\/\/ the maximum we're able to transfer from young to old.  This is called at the end of GC, as we prepare\n+\/\/ for the idle span that precedes the next GC.\n+void ShenandoahGenerationalHeap::compute_old_generation_balance(size_t mutator_xfer_limit,\n+                                                                size_t old_trashed_regions, size_t young_trashed_regions) {\n+  shenandoah_assert_heaplocked();\n@@ -603,1 +608,1 @@\n-  \/\/ Let SOEP = ShenandoahOldEvacRatioPercent,\n+  \/\/ Let SOEP = ShenandoahOldEvacPercent,\n@@ -614,12 +619,4 @@\n-  \/\/ We have to be careful in the event that SOEP is set to 100 by the user.\n-  assert(ShenandoahOldEvacRatioPercent <= 100, \"Error\");\n-  const size_t old_available = old_generation()->available();\n-  \/\/ The free set will reserve this amount of memory to hold young evacuations\n-  const size_t young_reserve = (young_generation()->max_capacity() * ShenandoahEvacReserve) \/ 100;\n-\n-  \/\/ In the case that ShenandoahOldEvacRatioPercent equals 100, max_old_reserve is limited only by xfer_limit.\n-\n-  const double bound_on_old_reserve = old_available + old_xfer_limit + young_reserve;\n-  const double max_old_reserve = ((ShenandoahOldEvacRatioPercent == 100)? bound_on_old_reserve:\n-                                  MIN2(double(young_reserve * ShenandoahOldEvacRatioPercent)\n-                                       \/ double(100 - ShenandoahOldEvacRatioPercent), bound_on_old_reserve));\n+#undef KELVIN_DEBUG\n+#ifdef KELVIN_DEBUG\n+  log_info(gc)(\"compute_old_generation_balance(%zu, %zu, %zu)\", mutator_xfer_limit, old_trashed_regions, young_trashed_regions);\n+#endif\n@@ -627,0 +624,2 @@\n+  \/\/ We have to be careful in the event that SOEP is set to 100 by the user.\n+  assert(ShenandoahOldEvacPercent <= 100, \"Error\");\n@@ -629,0 +628,28 @@\n+  ShenandoahOldGeneration* old_gen = old_generation();\n+  size_t old_capacity = old_gen->max_capacity();\n+  size_t old_usage = old_gen->used(); \/\/ includes humongous waste\n+  size_t old_available = ((old_capacity >= old_usage)? old_capacity - old_usage: 0) + old_trashed_regions * region_size_bytes;\n+\n+  ShenandoahYoungGeneration* young_gen = young_generation();\n+  size_t young_capacity = young_gen->max_capacity();\n+  size_t young_usage = young_gen->used(); \/\/ includes humongous waste\n+  size_t young_available = ((young_capacity >= young_usage)? young_capacity - young_usage: 0);\n+  size_t freeset_available = free_set()->available_locked();\n+  if (young_available > freeset_available) {\n+    young_available = freeset_available;\n+  }\n+  young_available += young_trashed_regions * region_size_bytes;\n+\n+  \/\/ The free set will reserve this amount of memory to hold young evacuations (initialized to the ideal reserve)\n+  size_t young_reserve = (young_generation()->max_capacity() * ShenandoahEvacReserve) \/ 100;\n+\n+  \/\/ If ShenandoahOldEvacPercent equals 100, max_old_reserve is limited only by mutator_xfer_limit and young_reserve\n+  const size_t bound_on_old_reserve = ((old_available + mutator_xfer_limit + young_reserve) * ShenandoahOldEvacPercent) \/ 100;\n+  size_t proposed_max_old = ((ShenandoahOldEvacPercent == 100)?\n+                             bound_on_old_reserve:\n+                             MIN2((young_reserve * ShenandoahOldEvacPercent) \/ (100 - ShenandoahOldEvacPercent),\n+                                  bound_on_old_reserve));\n+  if (young_reserve > young_available) {\n+    young_reserve = young_available;\n+  }\n+\n@@ -630,2 +657,16 @@\n-  double reserve_for_mixed = 0;\n-  if (old_generation()->has_unprocessed_collection_candidates()) {\n+  size_t reserve_for_mixed = 0;\n+  const size_t old_fragmented_available =\n+    old_available - (old_generation()->free_unaffiliated_regions() + old_trashed_regions) * region_size_bytes;\n+\n+  if (old_fragmented_available > proposed_max_old) {\n+    \/\/ After we've promoted regions in place, there may be an abundance of old-fragmented available memory,\n+    \/\/ even more than the desired percentage for old reserve.  We cannot transfer these fragmented regions back\n+    \/\/ to young.  Instead we make the best of the situation by using this fragmented memory for both promotions\n+    \/\/ and evacuations.\n+    proposed_max_old = old_fragmented_available;\n+  }\n+  size_t reserve_for_promo = old_fragmented_available;\n+  const size_t max_old_reserve = proposed_max_old;\n+  const size_t mixed_candidate_live_memory = old_generation()->unprocessed_collection_candidates_live_memory();\n+  const bool doing_mixed = (mixed_candidate_live_memory > 0);\n+  if (doing_mixed) {\n@@ -634,2 +675,1 @@\n-    const double max_evac_need =\n-      (double(old_generation()->unprocessed_collection_candidates_live_memory()) * ShenandoahOldEvacWaste);\n+    const size_t max_evac_need = (size_t) (mixed_candidate_live_memory * ShenandoahOldEvacWaste);\n@@ -638,5 +678,14 @@\n-    const double old_fragmented_available =\n-      double(old_available - old_generation()->free_unaffiliated_regions() * region_size_bytes);\n-    reserve_for_mixed = max_evac_need + old_fragmented_available;\n-    if (reserve_for_mixed > max_old_reserve) {\n-      reserve_for_mixed = max_old_reserve;\n+\n+    \/\/ We prefer to evacuate all of mixed into unfragmented memory, and will expand old in order to do so, unless\n+    \/\/ we already have too much fragmented available memory in old.\n+    reserve_for_mixed = max_evac_need;\n+    if (reserve_for_mixed + reserve_for_promo > max_old_reserve) {\n+      \/\/ In this case, we'll allow old-evac to target some of the fragmented old memory.\n+      size_t excess_reserves = (reserve_for_mixed + reserve_for_promo) - max_old_reserve;\n+      if (reserve_for_promo > excess_reserves) {\n+        reserve_for_promo -= excess_reserves;\n+      } else {\n+        excess_reserves -= reserve_for_promo;\n+        reserve_for_promo = 0;\n+        reserve_for_mixed -= excess_reserves;\n+      }\n@@ -646,2 +695,2 @@\n-  \/\/ Decide how much space we should reserve for promotions from young\n-  size_t reserve_for_promo = 0;\n+  \/\/ Decide how much additional space we should reserve for promotions from young.  We give priority to mixed evacations\n+  \/\/ over promotions.\n@@ -651,4 +700,15 @@\n-    \/\/ We're promoting and have a bound on the maximum amount that can be promoted\n-    assert(max_old_reserve >= reserve_for_mixed, \"Sanity\");\n-    const size_t available_for_promotions = max_old_reserve - reserve_for_mixed;\n-    reserve_for_promo = MIN2((size_t)(promo_load * ShenandoahPromoEvacWaste), available_for_promotions);\n+    \/\/ We've already set aside all of the fragmented available memory within old-gen to represent old objects\n+    \/\/ to be promoted from young generation.  promo_load represents the memory that we anticipate to be promoted\n+    \/\/ from regions that have reached tenure age.  In the ideal, we will always use fragmented old-gen memory\n+    \/\/ to hold individually promoted objects and will use unfragmented old-gen memory to represent the old-gen\n+    \/\/ evacuation workloa.\n+\n+    \/\/ We're promoting and have an esimate of memory to be promoted from aged regions\n+    assert(max_old_reserve >= (reserve_for_mixed + reserve_for_promo), \"Sanity\");\n+    const size_t available_for_additional_promotions = max_old_reserve - (reserve_for_mixed + reserve_for_promo);\n+    size_t promo_need = (size_t)(promo_load * ShenandoahPromoEvacWaste);\n+    if (promo_need > reserve_for_promo) {\n+      reserve_for_promo += MIN2(promo_need - reserve_for_promo, available_for_additional_promotions);\n+    }\n+    \/\/ We've already reserved all the memory required for the promo_load, and possibly more.  The excess\n+    \/\/ can be consumed by objects promoted from regions that have not yet reached tenure age.\n@@ -657,3 +717,2 @@\n-  \/\/ This is the total old we want to ideally reserve\n-  const size_t old_reserve = reserve_for_mixed + reserve_for_promo;\n-  assert(old_reserve <= max_old_reserve, \"cannot reserve more than max for old evacuations\");\n+  \/\/ This is the total old we want to reserve (initialized to the ideal reserve)\n+  size_t old_reserve = reserve_for_mixed + reserve_for_promo;\n@@ -662,2 +721,8 @@\n-  const size_t max_old_available = old_generation()->available() + old_cset_regions * region_size_bytes;\n-  if (max_old_available >= old_reserve) {\n+  size_t old_region_deficit = 0;\n+  size_t old_region_surplus = 0;\n+\n+  size_t mutator_region_xfer_limit = mutator_xfer_limit \/ region_size_bytes;\n+  \/\/ align the mutator_xfer_limit on region size\n+  mutator_xfer_limit = mutator_region_xfer_limit * region_size_bytes;\n+\n+  if (old_available >= old_reserve) {\n@@ -665,3 +730,7 @@\n-    const size_t old_surplus = (max_old_available - old_reserve) \/ region_size_bytes;\n-    const size_t unaffiliated_old_regions = old_generation()->free_unaffiliated_regions() + old_cset_regions;\n-    const size_t old_region_surplus = MIN2(old_surplus, unaffiliated_old_regions);\n+    const size_t old_surplus = old_available - old_reserve;\n+    old_region_surplus = old_surplus \/ region_size_bytes;\n+    const size_t unaffiliated_old_regions = old_generation()->free_unaffiliated_regions() + old_trashed_regions;\n+    old_region_surplus = MIN2(old_region_surplus, unaffiliated_old_regions);\n+#ifdef KELVIN_DEBUG\n+    log_info(gc)(\" setting region balance to surplus: %zd\", old_region_surplus);\n+#endif\n@@ -669,0 +738,8 @@\n+  } else if (old_available + mutator_xfer_limit >= old_reserve) {\n+    \/\/ Mutator's xfer limit is sufficient to satisfy our need: transfer all memory from there\n+    size_t old_deficit = old_reserve - old_available;\n+    old_region_deficit = (old_deficit + region_size_bytes - 1) \/ region_size_bytes;\n+#ifdef KELVIN_DEBUG\n+    log_info(gc)(\" setting region balance to deficit: %zd\", old_region_deficit);\n+#endif\n+    old_generation()->set_region_balance(0 - checked_cast<ssize_t>(old_region_deficit));\n@@ -670,11 +747,40 @@\n-    \/\/ We are running a deficit which we'd like to fill from young.\n-    \/\/ Ignore that this will directly impact young_generation()->max_capacity(),\n-    \/\/ indirectly impacting young_reserve and old_reserve.  These computations are conservative.\n-    \/\/ Note that deficit is rounded up by one region.\n-    const size_t old_need = (old_reserve - max_old_available + region_size_bytes - 1) \/ region_size_bytes;\n-    const size_t max_old_region_xfer = old_xfer_limit \/ region_size_bytes;\n-\n-    \/\/ Round down the regions we can transfer from young to old. If we're running short\n-    \/\/ on young-gen memory, we restrict the xfer. Old-gen collection activities will be\n-    \/\/ curtailed if the budget is restricted.\n-    const size_t old_region_deficit = MIN2(old_need, max_old_region_xfer);\n+   \/\/ We'll try to xfer from both mutator excess and from young collector reserve\n+    size_t available_reserves = old_available + young_reserve + mutator_xfer_limit;\n+    size_t old_entitlement = (available_reserves  * ShenandoahOldEvacPercent) \/ 100;\n+\n+    \/\/ Round old_entitlement down to nearest multiple of regions to be transferred to old\n+    size_t entitled_xfer = old_entitlement - old_available;\n+    entitled_xfer = region_size_bytes * (entitled_xfer \/ region_size_bytes);\n+    size_t unaffiliated_young_regions = young_generation()->free_unaffiliated_regions();\n+    size_t unaffiliated_young_memory = unaffiliated_young_regions * region_size_bytes;\n+    if (entitled_xfer > unaffiliated_young_memory) {\n+      entitled_xfer = unaffiliated_young_memory;\n+    }\n+    old_entitlement = old_available + entitled_xfer;\n+    if (old_entitlement < old_reserve) {\n+      \/\/ There's not enough memory to satisfy our desire.  Scale back our old-gen intentions.\n+      size_t budget_overrun = old_reserve - old_entitlement;;\n+      if (reserve_for_promo > budget_overrun) {\n+        reserve_for_promo -= budget_overrun;\n+        old_reserve -= budget_overrun;\n+      } else {\n+        budget_overrun -= reserve_for_promo;\n+        reserve_for_promo = 0;\n+        reserve_for_mixed = (reserve_for_mixed > budget_overrun)? reserve_for_mixed - budget_overrun: 0;\n+        old_reserve = reserve_for_promo + reserve_for_mixed;\n+      }\n+    }\n+\n+    \/\/ Because of adjustments above, old_reserve may be smaller now than it was when we tested the branch\n+    \/\/   condition above: \"(old_available + mutator_xfer_limit >= old_reserve)\n+    \/\/ Therefore, we do NOT know that: mutator_xfer_limit < old_reserve - old_available\n+\n+    size_t old_deficit = old_reserve - old_available;\n+    old_region_deficit = (old_deficit + region_size_bytes - 1) \/ region_size_bytes;\n+\n+    \/\/ Shrink young_reserve to account for loan to old reserve\n+    const size_t reserve_xfer_regions = old_region_deficit - mutator_region_xfer_limit;\n+    young_reserve -= reserve_xfer_regions * region_size_bytes;\n+#ifdef KELVIN_DEBUG\n+    log_info(gc)(\" setting region balance to deficit: %zd\", old_region_deficit);\n+#endif\n@@ -683,1 +789,0 @@\n-}\n@@ -685,6 +790,4 @@\n-void ShenandoahGenerationalHeap::reset_generation_reserves() {\n-  ShenandoahHeapLocker locker(lock());\n-  young_generation()->set_evacuation_reserve(0);\n-  old_generation()->set_evacuation_reserve(0);\n-  old_generation()->set_promoted_reserve(0);\n-}\n+#ifdef KELVIN_DEBUG\n+  log_info(gc)(\"compute_old_generation_balance() setting evac_reserve: %zu, old evac reserve: %zu, promo reserve: %zu\",\n+               young_reserve, reserve_for_mixed, reserve_for_promo);\n+#endif\n@@ -692,11 +795,9 @@\n-void ShenandoahGenerationalHeap::TransferResult::print_on(const char* when, outputStream* ss) const {\n-  auto heap = ShenandoahGenerationalHeap::heap();\n-  ShenandoahYoungGeneration* const young_gen = heap->young_generation();\n-  ShenandoahOldGeneration* const old_gen = heap->old_generation();\n-  const size_t young_available = young_gen->available();\n-  const size_t old_available = old_gen->available();\n-  ss->print_cr(\"After %s, %s %zu regions to %s to prepare for next gc, old available: \"\n-                     PROPERFMT \", young_available: \" PROPERFMT,\n-                     when,\n-                     success? \"successfully transferred\": \"failed to transfer\", region_count, region_destination,\n-                     PROPERFMTARGS(old_available), PROPERFMTARGS(young_available));\n+  assert(old_region_deficit == 0 || old_region_surplus == 0, \"Only surplus or deficit, never both\");\n+  assert(young_reserve + reserve_for_mixed + reserve_for_promo <= old_available + young_available,\n+         \"Cannot reserve more memory than is available: %zu + %zu + %zu <= %zu + %zu\",\n+         young_reserve, reserve_for_mixed, reserve_for_promo, old_available, young_available);\n+\n+  \/\/ deficit\/surplus adjustments to generation sizes will precede rebuild\n+  young_generation()->set_evacuation_reserve(young_reserve);\n+  old_generation()->set_evacuation_reserve(reserve_for_mixed);\n+  old_generation()->set_promoted_reserve(reserve_for_promo);\n@@ -1043,4 +1144,0 @@\n-  \/\/ In case degeneration interrupted concurrent evacuation or update references, we need to clean up\n-  \/\/ transient state. Otherwise, these actions have no effect.\n-  reset_generation_reserves();\n-\n@@ -1064,1 +1161,0 @@\n-  reset_generation_reserves();\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahGenerationalHeap.cpp","additions":177,"deletions":81,"binary":false,"changes":258,"status":"modified"},{"patch":"@@ -133,12 +133,0 @@\n-  \/\/ Used for logging the result of a region transfer outside the heap lock\n-  struct TransferResult {\n-    bool success;\n-    size_t region_count;\n-    const char* region_destination;\n-\n-    void print_on(const char* when, outputStream* ss) const;\n-  };\n-\n-  \/\/ Zeros out the evacuation and promotion reserves\n-  void reset_generation_reserves();\n-\n@@ -146,4 +134,1 @@\n-  void compute_old_generation_balance(size_t old_xfer_limit, size_t old_cset_regions);\n-\n-  \/\/ Transfers surplus old regions to young, or takes regions from young to satisfy old region deficit\n-  TransferResult balance_generations();\n+  void compute_old_generation_balance(size_t old_xfer_limit, size_t old_trashed_regions, size_t young_trashed_regions);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahGenerationalHeap.hpp","additions":1,"deletions":16,"binary":false,"changes":17,"status":"modified"},{"patch":"@@ -428,1 +428,0 @@\n-    _free_set = new ShenandoahFreeSet(this, _num_regions);\n@@ -430,0 +429,6 @@\n+    if (mode()->is_generational()) {\n+      size_t young_reserve = (soft_max_capacity() * ShenandoahEvacReserve) \/ 100;\n+      young_generation()->set_evacuation_reserve(young_reserve);\n+      old_generation()->set_evacuation_reserve((size_t) 0);\n+      old_generation()->set_promoted_reserve((size_t) 0);\n+    }\n@@ -431,0 +436,1 @@\n+    _free_set = new ShenandoahFreeSet(this, _num_regions);\n@@ -432,0 +438,1 @@\n+\n@@ -433,2 +440,2 @@\n-    size_t young_cset_regions, old_cset_regions, first_old, last_old, num_old;\n-    _free_set->prepare_to_rebuild(young_cset_regions, old_cset_regions, first_old, last_old, num_old);\n+    size_t young_trashed_regions, old_trashed_regions, first_old, last_old, num_old;\n+    _free_set->prepare_to_rebuild(young_trashed_regions, old_trashed_regions, first_old, last_old, num_old);\n@@ -441,1 +448,1 @@\n-      gen_heap->compute_old_generation_balance(allocation_runway, old_cset_regions);\n+      gen_heap->compute_old_generation_balance(allocation_runway, old_trashed_regions, young_trashed_regions);\n@@ -443,1 +450,1 @@\n-    _free_set->finish_rebuild(young_cset_regions, old_cset_regions, num_old);\n+    _free_set->finish_rebuild(young_trashed_regions, old_trashed_regions, num_old);\n@@ -2518,4 +2525,1 @@\n-void ShenandoahHeap::rebuild_free_set(bool concurrent) {\n-  ShenandoahGCPhase phase(concurrent ?\n-                          ShenandoahPhaseTimings::final_update_refs_rebuild_freeset :\n-                          ShenandoahPhaseTimings::degen_gc_final_update_refs_rebuild_freeset);\n+void ShenandoahHeap::rebuild_free_set_within_phase() {\n@@ -2523,3 +2527,2 @@\n-  size_t young_cset_regions, old_cset_regions;\n-  size_t first_old_region, last_old_region, old_region_count;\n-  _free_set->prepare_to_rebuild(young_cset_regions, old_cset_regions, first_old_region, last_old_region, old_region_count);\n+  size_t young_trashed_regions, old_trashed_regions, first_old_region, last_old_region, old_region_count;\n+  _free_set->prepare_to_rebuild(young_trashed_regions, old_trashed_regions, first_old_region, last_old_region, old_region_count);\n@@ -2543,10 +2546,3 @@\n-    size_t allocation_runway = gen_heap->young_generation()->heuristics()->bytes_of_allocation_runway_before_gc_trigger(young_cset_regions);\n-    gen_heap->compute_old_generation_balance(allocation_runway, old_cset_regions);\n-\n-    \/\/ Total old_available may have been expanded to hold anticipated promotions.  We trigger if the fragmented available\n-    \/\/ memory represents more than 16 regions worth of data.  Note that fragmentation may increase when we promote regular\n-    \/\/ regions in place when many of these regular regions have an abundant amount of available memory within them.  Fragmentation\n-    \/\/ will decrease as promote-by-copy consumes the available memory within these partially consumed regions.\n-    \/\/\n-    \/\/ We consider old-gen to have excessive fragmentation if more than 12.5% of old-gen is free memory that resides\n-    \/\/ within partially consumed regions of memory.\n+    size_t allocation_runway =\n+      gen_heap->young_generation()->heuristics()->bytes_of_allocation_runway_before_gc_trigger(young_trashed_regions);\n+    gen_heap->compute_old_generation_balance(allocation_runway, old_trashed_regions, young_trashed_regions);\n@@ -2555,1 +2551,1 @@\n-  _free_set->finish_rebuild(young_cset_regions, old_cset_regions, old_region_count);\n+  _free_set->finish_rebuild(young_trashed_regions, old_trashed_regions, old_region_count);\n@@ -2564,0 +2560,7 @@\n+void ShenandoahHeap::rebuild_free_set(bool concurrent) {\n+  ShenandoahGCPhase phase(concurrent ?\n+                          ShenandoahPhaseTimings::final_update_refs_rebuild_freeset :\n+                          ShenandoahPhaseTimings::degen_gc_final_update_refs_rebuild_freeset);\n+  rebuild_free_set_within_phase();\n+}\n+\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeap.cpp","additions":26,"deletions":23,"binary":false,"changes":49,"status":"modified"},{"patch":"@@ -483,0 +483,1 @@\n+  \/\/ The following two functions rebuild the free set at the of GC, in preparation for an idle phase.\n@@ -484,0 +485,1 @@\n+  void rebuild_free_set_within_phase();\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeap.hpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -596,0 +596,2 @@\n+    \/\/ Otherwise, the mutator might see region as fully recycled and might change its affiliation only to have\n+    \/\/ the racing GC worker thread overwrite its affiliation to FREE.\n@@ -606,0 +608,2 @@\n+\/\/ Note that return from try_recycle() does not mean the region has been recycled.  It only means that\n+\/\/ some GC worker thread has taken responsibility to recycle the region, eventually.\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeapRegion.cpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -131,2 +131,0 @@\n-  heap->free_set()->log_status_under_lock();\n-\n@@ -141,2 +139,5 @@\n-  size_t allocation_runway = heap->young_generation()->heuristics()->bytes_of_allocation_runway_before_gc_trigger(0);\n-  heap->compute_old_generation_balance(allocation_runway, 0);\n+  \/\/ After concurrent old marking finishes, we reclaim immediate garbage. Further, we may also want to expand OLD in order\n+  \/\/ to make room for anticipated promotions and\/or for mixed evacuations.  Mixed evacuations are especially likely to\n+  \/\/ follow the end of OLD marking.\n+  heap->rebuild_free_set_within_phase();\n+  heap->free_set()->log_status_under_lock();\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahOldGC.cpp","additions":5,"deletions":4,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -413,2 +413,1 @@\n-    size_t young_trash_regions, old_trash_regions;\n-    size_t first_old, last_old, num_old;\n+    size_t young_trash_regions, old_trash_regions, first_old, last_old, num_old;\n@@ -424,2 +423,1 @@\n-    gen_heap->compute_old_generation_balance(allocation_runway, old_trash_regions);\n-\n+    gen_heap->compute_old_generation_balance(allocation_runway, old_trash_regions, young_trash_regions);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahOldGeneration.cpp","additions":2,"deletions":4,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -69,2 +69,2 @@\n-  \/\/ Represents the quantity of live bytes we expect to promote during the next evacuation\n-  \/\/ cycle. This value is used by the young heuristic to trigger mixed collections.\n+  \/\/ Represents the quantity of live bytes we expect to promote during the next GC cycle, either by\n+  \/\/ evacuation or by promote-in-place.  This value is used by the young heuristic to trigger mixed collections.\n@@ -149,0 +149,4 @@\n+#undef KELVIN_VERBOSE\n+#ifdef KELVIN_VERBOSE\n+    log_info(gc)(\"old_generation->set_region_balance(%zd)\", balance);\n+#endif\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahOldGeneration.hpp","additions":6,"deletions":2,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -246,2 +246,1 @@\n-  \/\/ For HumongousRegion:s it's more efficient to jump directly to the\n-  \/\/ start region.\n+  \/\/ For humongous regions it's more efficient to jump directly to the start region.\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahScanRemembered.cpp","additions":1,"deletions":2,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -423,1 +423,8 @@\n-  size_t non_trashed_span() const { return (_regions - _trashed_regions) * ShenandoahHeapRegion::region_size_bytes(); }\n+  size_t non_trashed_span() const {\n+    assert(_regions >= _trashed_regions, \"sanity\");\n+    return (_regions - _trashed_regions) * ShenandoahHeapRegion::region_size_bytes();\n+  }\n+  size_t non_trashed_committed() const {\n+    assert(_committed >= _trashed_regions * ShenandoahHeapRegion::region_size_bytes(), \"sanity\");\n+    return _committed - (_trashed_regions * ShenandoahHeapRegion::region_size_bytes());\n+  }\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahVerifier.cpp","additions":8,"deletions":1,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -388,21 +388,14 @@\n-  product(uintx, ShenandoahOldEvacRatioPercent, 75, EXPERIMENTAL,           \\\n-          \"The maximum proportion of evacuation from old-gen memory, \"      \\\n-          \"expressed as a percentage. The default value 75 denotes that \"   \\\n-          \"no more than 75% of the collection set evacuation workload may \" \\\n-          \"be towards evacuation of old-gen heap regions. This limits both \"\\\n-          \"the promotion of aged regions and the compaction of existing \"   \\\n-          \"old regions. A value of 75 denotes that the total evacuation \"   \\\n-          \"work may increase to up to four times the young gen evacuation \" \\\n-          \"work. A larger value allows quicker promotion and allows \"       \\\n-          \"a smaller number of mixed evacuations to process \"               \\\n-          \"the entire list of old-gen collection candidates at the cost \"   \\\n-          \"of an increased disruption of the normal cadence of young-gen \"  \\\n-          \"collections.  A value of 100 allows a mixed evacuation to \"      \\\n-          \"focus entirely on old-gen memory, allowing no young-gen \"        \\\n-          \"regions to be collected, likely resulting in subsequent \"        \\\n-          \"allocation failures because the allocation pool is not \"         \\\n-          \"replenished.  A value of 0 allows a mixed evacuation to \"        \\\n-          \"focus entirely on young-gen memory, allowing no old-gen \"        \\\n-          \"regions to be collected, likely resulting in subsequent \"        \\\n-          \"promotion failures and triggering of stop-the-world full GC \"    \\\n-          \"events.\")                                                        \\\n+  product(uintx, ShenandoahOldEvacPercent, 75, EXPERIMENTAL,                \\\n+          \"The maximum evacuation to old-gen expressed as a percent of \"    \\\n+          \"the total live memory within the collection set.  With the \"     \\\n+          \"default setting, if collection set evacuates X, no more than \"   \\\n+          \"75% of X may hold objects evacuated from old or promoted to \"    \\\n+          \"old from young.  A value of 100 allows the entire collection \"   \\\n+          \"set to be comprised of old-gen regions and young regions that \"  \\\n+          \"have reached the tenure age.  Larger values allow fewer mixed \"  \\\n+          \"evacuations to reclaim all the garbage from old.  Smaller \"      \\\n+          \"values result in less variation in GC cycle times between \"      \\\n+          \"young vs. mixed cycles.  A value of 0 prevents mixed \"           \\\n+          \"evacations from running and blocks promotion of aged regions \"   \\\n+          \"by evacuation.  Setting the value to 0 does not prevent \"        \\\n+          \"regions from being promoted in place.\")                          \\\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoah_globals.hpp","additions":14,"deletions":21,"binary":false,"changes":35,"status":"modified"},{"patch":"@@ -204,1 +204,3 @@\n-  _heuristics->prime_collection_set(_collection_set);\n+  if (_heuristics->prime_collection_set(_collection_set)) {\n+    _heuristics->finalize_mixed_evacs();\n+  }\n@@ -217,1 +219,3 @@\n-  _heuristics->prime_collection_set(_collection_set);\n+  if (_heuristics->prime_collection_set(_collection_set)) {\n+    _heuristics->finalize_mixed_evacs();\n+  }\n@@ -229,1 +233,3 @@\n-  _heuristics->prime_collection_set(_collection_set);\n+  if (_heuristics->prime_collection_set(_collection_set)) {\n+    _heuristics->finalize_mixed_evacs();\n+  }\n@@ -251,1 +257,3 @@\n-  _heuristics->prime_collection_set(_collection_set);\n+  if (_heuristics->prime_collection_set(_collection_set)) {\n+    _heuristics->finalize_mixed_evacs();\n+  }\n@@ -264,1 +272,3 @@\n-  _heuristics->prime_collection_set(_collection_set);\n+  if (_heuristics->prime_collection_set(_collection_set)) {\n+    _heuristics->finalize_mixed_evacs();\n+  }\n@@ -281,1 +291,3 @@\n-  _heuristics->prime_collection_set(_collection_set);\n+  if (_heuristics->prime_collection_set(_collection_set)) {\n+    _heuristics->finalize_mixed_evacs();\n+  }\n@@ -288,1 +300,3 @@\n-  _heuristics->prime_collection_set(_collection_set);\n+  if (_heuristics->prime_collection_set(_collection_set)) {\n+    _heuristics->finalize_mixed_evacs();\n+  }\n@@ -304,1 +318,3 @@\n-  _heuristics->prime_collection_set(_collection_set);\n+  if (_heuristics->prime_collection_set(_collection_set)) {\n+    _heuristics->finalize_mixed_evacs();\n+  }\n@@ -312,1 +328,3 @@\n-  _heuristics->prime_collection_set(_collection_set);\n+  if (_heuristics->prime_collection_set(_collection_set)) {\n+    _heuristics->finalize_mixed_evacs();\n+  }\n@@ -330,1 +348,3 @@\n-  _heuristics->prime_collection_set(_collection_set);\n+  if (_heuristics->prime_collection_set(_collection_set)) {\n+    _heuristics->finalize_mixed_evacs();\n+  }\n@@ -339,1 +359,3 @@\n-  _heuristics->prime_collection_set(_collection_set);\n+  if (_heuristics->prime_collection_set(_collection_set)) {\n+    _heuristics->finalize_mixed_evacs();\n+  }\n@@ -357,1 +379,3 @@\n-  _heuristics->prime_collection_set(_collection_set);\n+  if (_heuristics->prime_collection_set(_collection_set)) {\n+    _heuristics->finalize_mixed_evacs();\n+  }\n","filename":"test\/hotspot\/gtest\/gc\/shenandoah\/test_shenandoahOldHeuristic.cpp","additions":36,"deletions":12,"binary":false,"changes":48,"status":"modified"}]}