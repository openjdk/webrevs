{"files":[{"patch":"@@ -175,0 +175,1 @@\n+    bool need_to_finalize_mixed = false;\n@@ -176,1 +177,1 @@\n-      heap->old_generation()->heuristics()->prime_collection_set(collection_set);\n+      need_to_finalize_mixed = heap->old_generation()->heuristics()->prime_collection_set(collection_set);\n@@ -181,0 +182,11 @@\n+\n+    if (_generation->is_young()) {\n+      \/\/ Especially when young-gen trigger is expedited in order to finish mixed evacuations, there may not be\n+      \/\/ enough consolidated garbage to make effective use of young-gen evacuation reserve.  If there is still\n+      \/\/ young-gen reserve available following selection of the young-gen collection set, see if we can use\n+      \/\/ this memory to expand the old-gen evacuation collection set.\n+      need_to_finalize_mixed |= heap->old_generation()->heuristics()->top_off_collection_set();\n+      if (need_to_finalize_mixed) {\n+        heap->old_generation()->heuristics()->finalize_mixed_evacs();\n+      }\n+    }\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/heuristics\/shenandoahGenerationalHeuristics.cpp","additions":13,"deletions":1,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -55,0 +55,1 @@\n+\n@@ -60,10 +61,0 @@\n-  size_t max_young_cset = (size_t) (young_evac_reserve \/ ShenandoahEvacWaste);\n-  size_t young_cur_cset = 0;\n-  size_t max_old_cset = (size_t) (old_evac_reserve \/ ShenandoahOldEvacWaste);\n-  size_t old_cur_cset = 0;\n-\n-  \/\/ Figure out how many unaffiliated young regions are dedicated to mutator and to evacuator.  Allow the young\n-  \/\/ collector's unaffiliated regions to be transferred to old-gen if old-gen has more easily reclaimed garbage\n-  \/\/ than young-gen.  At the end of this cycle, any excess regions remaining in old-gen will be transferred back\n-  \/\/ to young.  Do not transfer the mutator's unaffiliated regions to old-gen.  Those must remain available\n-  \/\/ to the mutator as it needs to be able to consume this memory during concurrent GC.\n@@ -73,6 +64,24 @@\n-\n-  if (unaffiliated_young_memory > max_young_cset) {\n-    size_t unaffiliated_mutator_memory = unaffiliated_young_memory - max_young_cset;\n-    unaffiliated_young_memory -= unaffiliated_mutator_memory;\n-    unaffiliated_young_regions = unaffiliated_young_memory \/ region_size_bytes; \/\/ round down\n-    unaffiliated_young_memory = unaffiliated_young_regions * region_size_bytes;\n+  size_t unaffiliated_old_regions = heap->old_generation()->free_unaffiliated_regions();\n+  size_t unaffiliated_old_memory = unaffiliated_old_regions * region_size_bytes;\n+\n+  \/\/ Figure out how many unaffiliated regions are dedicated to Collector and OldCollector reserves.  Let these\n+  \/\/ be shuffled between young and old generations in order to expedite evacuation of whichever regions have the\n+  \/\/ most garbage, regardless of whether these garbage-first regions reside in young or old generation.\n+  \/\/ Excess reserves will be transferred back to the mutator after collection set has been chosen.  At the end\n+  \/\/ of evacuation, any reserves not consumed by evacuation will also be transferred to the mutator free set.\n+  size_t shared_reserve_regions = 0;\n+  if (young_evac_reserve > unaffiliated_young_memory) {\n+    young_evac_reserve -= unaffiliated_young_memory;\n+    shared_reserve_regions += unaffiliated_young_memory \/ region_size_bytes;\n+  } else {\n+    size_t delta_regions = young_evac_reserve \/ region_size_bytes;\n+    shared_reserve_regions += delta_regions;\n+    young_evac_reserve -= delta_regions * region_size_bytes;\n+  }\n+  if (old_evac_reserve > unaffiliated_old_memory) {\n+    old_evac_reserve -= unaffiliated_old_memory;\n+    shared_reserve_regions += unaffiliated_old_memory \/ region_size_bytes;\n+  } else {\n+    size_t delta_regions = old_evac_reserve \/ region_size_bytes;\n+    shared_reserve_regions += delta_regions;\n+    old_evac_reserve -= delta_regions * region_size_bytes;\n@@ -81,2 +90,6 @@\n-  \/\/ We'll affiliate these unaffiliated regions with either old or young, depending on need.\n-  max_young_cset -= unaffiliated_young_memory;\n+  size_t shared_reserves = shared_reserve_regions * region_size_bytes;\n+  size_t committed_from_shared_reserves = 0;\n+  size_t max_young_cset = (size_t) (young_evac_reserve \/ ShenandoahEvacWaste);\n+  size_t young_cur_cset = 0;\n+  size_t max_old_cset = (size_t) (old_evac_reserve \/ ShenandoahOldEvacWaste);\n+  size_t old_cur_cset = 0;\n@@ -84,2 +97,3 @@\n-  \/\/ Keep track of how many regions we plan to transfer from young to old.\n-  size_t regions_transferred_to_old = 0;\n+  size_t promo_bytes = 0;\n+  size_t old_evac_bytes = 0;\n+  size_t young_evac_bytes = 0;\n@@ -87,1 +101,3 @@\n-  size_t free_target = (capacity * ShenandoahMinFreeThreshold) \/ 100 + max_young_cset;\n+  size_t max_total_cset = (max_young_cset + max_old_cset +\n+                           (size_t) (shared_reserve_regions * region_size_bytes) \/ ShenandoahOldEvacWaste);\n+  size_t free_target = (capacity * ShenandoahMinFreeThreshold) \/ 100 + max_total_cset;\n@@ -91,1 +107,1 @@\n-                     \"%s, Max Old Evacuation: %zu%s, Max Either Evacuation: %zu%s, Actual Free: %zu%s.\",\n+                     \"%s, Max Old Evacuation: %zu%s, Discretionary additional evacuation: %zu%s, Actual Free: %zu%s.\",\n@@ -94,1 +110,1 @@\n-                     byte_size_in_proper_unit(unaffiliated_young_memory), proper_unit_for_byte_size(unaffiliated_young_memory),\n+                     byte_size_in_proper_unit(shared_reserves), proper_unit_for_byte_size(shared_reserves),\n@@ -97,0 +113,1 @@\n+  size_t cur_garbage = cur_young_garbage;\n@@ -101,0 +118,3 @@\n+    size_t region_garbage = r->garbage();\n+    size_t new_garbage = cur_garbage + region_garbage;\n+    bool add_regardless = (region_garbage > ignore_threshold) && (new_garbage < min_garbage);\n@@ -102,5 +122,10 @@\n-      size_t new_cset = old_cur_cset + r->get_live_data_bytes();\n-      if ((r->garbage() > garbage_threshold)) {\n-        while ((new_cset > max_old_cset) && (unaffiliated_young_regions > 0)) {\n-          unaffiliated_young_regions--;\n-          regions_transferred_to_old++;\n+      if (add_regardless || (region_garbage > garbage_threshold)) {\n+        size_t live_bytes = r->get_live_data_bytes();\n+        size_t new_cset = old_cur_cset + r->get_live_data_bytes();\n+        \/\/ May need to reserve multiple regions to hold the evacuations from a single region, depending on live data bytes\n+        \/\/ and ShenandoahOldEvacWaste\n+        size_t orig_max_old_cset = max_old_cset;\n+        size_t proposed_old_region_consumption = 0;\n+        while ((new_cset > max_old_cset) && (committed_from_shared_reserves < shared_reserves)) {\n+          committed_from_shared_reserves += region_size_bytes;\n+          proposed_old_region_consumption++;\n@@ -109,4 +134,15 @@\n-      }\n-      if ((new_cset <= max_old_cset) && (r->garbage() > garbage_threshold)) {\n-        add_region = true;\n-        old_cur_cset = new_cset;\n+        \/\/ We already know: add_regardless || region_garbage > garbage_threshold\n+        if (new_cset <= max_old_cset) {\n+          add_region = true;\n+          old_cur_cset = new_cset;\n+          cur_garbage = new_garbage;\n+          if (r->is_old()) {\n+            old_evac_bytes += live_bytes;\n+          } else {\n+            promo_bytes += live_bytes;\n+          }\n+        } else {\n+          \/\/ We failed to sufficiently expand old, so unwind proposed expansion\n+          max_old_cset = orig_max_old_cset;\n+          committed_from_shared_reserves -= proposed_old_region_consumption * region_size_bytes;\n+        }\n@@ -116,8 +152,9 @@\n-      size_t new_cset = young_cur_cset + r->get_live_data_bytes();\n-      size_t region_garbage = r->garbage();\n-      size_t new_garbage = cur_young_garbage + region_garbage;\n-      bool add_regardless = (region_garbage > ignore_threshold) && (new_garbage < min_garbage);\n-\n-      if (add_regardless || (r->garbage() > garbage_threshold)) {\n-        while ((new_cset > max_young_cset) && (unaffiliated_young_regions > 0)) {\n-          unaffiliated_young_regions--;\n+      if (add_regardless || (region_garbage > garbage_threshold)) {\n+        size_t live_bytes = r->get_live_data_bytes();\n+        size_t new_cset = young_cur_cset + live_bytes;\n+        \/\/ May need multiple reserve regions to evacuate a single region, depending on live data bytes and ShenandoahEvacWaste\n+        size_t orig_max_young_cset = max_young_cset;\n+        size_t proposed_young_region_consumption = 0;\n+        while ((new_cset > max_young_cset) && (committed_from_shared_reserves < shared_reserves)) {\n+          committed_from_shared_reserves += region_size_bytes;\n+          proposed_young_region_consumption++;\n@@ -126,5 +163,11 @@\n-      }\n-      if ((new_cset <= max_young_cset) && (add_regardless || (region_garbage > garbage_threshold))) {\n-        add_region = true;\n-        young_cur_cset = new_cset;\n-        cur_young_garbage = new_garbage;\n+        \/\/ We already know: add_regardless || region_garbage > garbage_threshold\n+        if (new_cset <= max_young_cset) {\n+          add_region = true;\n+          young_cur_cset = new_cset;\n+          cur_garbage = new_garbage;\n+          young_evac_bytes += live_bytes;\n+        } else {\n+          \/\/ We failed to sufficiently expand young, so unwind proposed expansion\n+          max_young_cset = orig_max_young_cset;\n+          committed_from_shared_reserves -= proposed_young_region_consumption * region_size_bytes;\n+        }\n@@ -137,5 +180,3 @@\n-  if (regions_transferred_to_old > 0) {\n-    assert(young_evac_reserve > regions_transferred_to_old * region_size_bytes, \"young reserve cannot be negative\");\n-    heap->young_generation()->set_evacuation_reserve(young_evac_reserve - regions_transferred_to_old * region_size_bytes);\n-    heap->old_generation()->set_evacuation_reserve(old_evac_reserve + regions_transferred_to_old * region_size_bytes);\n-  }\n+  heap->young_generation()->set_evacuation_reserve((size_t) (young_evac_bytes * ShenandoahEvacWaste));\n+  heap->old_generation()->set_evacuation_reserve((size_t) (old_evac_bytes * ShenandoahOldEvacWaste));\n+  heap->old_generation()->set_promoted_reserve((size_t) (promo_bytes * ShenandoahPromoEvacWaste));\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/heuristics\/shenandoahGlobalHeuristics.cpp","additions":91,"deletions":50,"binary":false,"changes":141,"status":"modified"},{"patch":"@@ -29,0 +29,1 @@\n+#include \"gc\/shenandoah\/shenandoahFreeSet.hpp\"\n@@ -32,0 +33,1 @@\n+#include \"gc\/shenandoah\/shenandoahYoungGeneration.hpp\"\n@@ -79,3 +81,4 @@\n-  if (unprocessed_old_collection_candidates() == 0) {\n-    return false;\n-  }\n+  _mixed_evac_cset = collection_set;\n+  _included_old_regions = 0;\n+  _evacuated_old_bytes = 0;\n+  _collected_old_bytes = 0;\n@@ -91,6 +94,0 @@\n-  _first_pinned_candidate = NOT_FOUND;\n-\n-  uint included_old_regions = 0;\n-  size_t evacuated_old_bytes = 0;\n-  size_t collected_old_bytes = 0;\n-\n@@ -102,10 +99,22 @@\n-  const size_t old_evacuation_reserve = _old_generation->get_evacuation_reserve();\n-  const size_t old_evacuation_budget = (size_t) ((double) old_evacuation_reserve \/ ShenandoahOldEvacWaste);\n-  size_t unfragmented_available = _old_generation->free_unaffiliated_regions() * ShenandoahHeapRegion::region_size_bytes();\n-  size_t fragmented_available;\n-  size_t excess_fragmented_available;\n-\n-  if (unfragmented_available > old_evacuation_budget) {\n-    unfragmented_available = old_evacuation_budget;\n-    fragmented_available = 0;\n-    excess_fragmented_available = 0;\n+  _old_evacuation_reserve = _old_generation->get_evacuation_reserve();\n+  _old_evacuation_budget = (size_t) ((double) _old_evacuation_reserve \/ ShenandoahOldEvacWaste);\n+\n+  \/\/ fragmented_available is the amount of memory within partially consumed old regions that may be required to\n+  \/\/ hold the results of old evacuations.  If all of the memory required by the old evacuation reserve is available\n+  \/\/ in unfragmented regions (unaffiliated old regions), then fragmented_available is zero because we do not need\n+  \/\/ to evacuate into the existing partially consumed old regions.\n+\n+  \/\/ if fragmented_available is non-zero, excess_fragmented_available represents the amount of fragmented memory\n+  \/\/ that is available within old, but is not required to hold the resuilts of old evacuation.  As old-gen regions\n+  \/\/ are added into the collection set, their free memory is subtracted from excess_fragmented_available until the\n+  \/\/ excess is exhausted.  For old-gen regions subsequently added to the collection set, their free memory is\n+  \/\/ subtracted from fragmented_available and from the old_evacuation_budget (since the budget decreases when this\n+  \/\/ fragmented_available memory decreases).  After fragmented_available has been exhausted, any further old regions\n+  \/\/ selected for the cset do not further decrease the old_evacuation_budget because all further evacuation is targeted\n+  \/\/ to unfragmented regions.\n+\n+  size_t unaffiliated_available = _old_generation->free_unaffiliated_regions() * ShenandoahHeapRegion::region_size_bytes();\n+  if (unaffiliated_available > _old_evacuation_reserve) {\n+    _unfragmented_available = _old_evacuation_budget;\n+    _fragmented_available = 0;\n+    _excess_fragmented_available = 0;\n@@ -113,6 +122,6 @@\n-    assert(_old_generation->available() >= old_evacuation_budget, \"Cannot budget more than is available\");\n-    fragmented_available = _old_generation->available() - unfragmented_available;\n-    assert(fragmented_available + unfragmented_available >= old_evacuation_budget, \"Budgets do not add up\");\n-    if (fragmented_available + unfragmented_available > old_evacuation_budget) {\n-      excess_fragmented_available = (fragmented_available + unfragmented_available) - old_evacuation_budget;\n-      fragmented_available -= excess_fragmented_available;\n+    assert(_old_generation->available() >= _old_evacuation_reserve, \"Cannot reserve more than is available\");\n+    size_t affiliated_available = _old_generation->available() - unaffiliated_available;\n+    assert(affiliated_available + unaffiliated_available >= _old_evacuation_reserve, \"Budgets do not add up\");\n+    if (affiliated_available + unaffiliated_available > _old_evacuation_reserve) {\n+      _excess_fragmented_available = (affiliated_available + unaffiliated_available) - _old_evacuation_reserve;\n+      affiliated_available -= _excess_fragmented_available;\n@@ -120,0 +129,2 @@\n+    _fragmented_available = (size_t) ((double) affiliated_available \/ ShenandoahOldEvacWaste);\n+    _unfragmented_available = (size_t) ((double) unaffiliated_available \/ ShenandoahOldEvacWaste);\n@@ -122,1 +133,0 @@\n-  size_t remaining_old_evacuation_budget = old_evacuation_budget;\n@@ -124,1 +134,1 @@\n-                byte_size_in_proper_unit(old_evacuation_budget), proper_unit_for_byte_size(old_evacuation_budget),\n+                byte_size_in_proper_unit(_old_evacuation_budget), proper_unit_for_byte_size(_old_evacuation_budget),\n@@ -126,120 +136,1 @@\n-\n-  size_t lost_evacuation_capacity = 0;\n-\n-  \/\/ The number of old-gen regions that were selected as candidates for collection at the end of the most recent old-gen\n-  \/\/ concurrent marking phase and have not yet been collected is represented by unprocessed_old_collection_candidates().\n-  \/\/ Candidate regions are ordered according to increasing amount of live data.  If there is not sufficient room to\n-  \/\/ evacuate region N, then there is no need to even consider evacuating region N+1.\n-  while (unprocessed_old_collection_candidates() > 0) {\n-    \/\/ Old collection candidates are sorted in order of decreasing garbage contained therein.\n-    ShenandoahHeapRegion* r = next_old_collection_candidate();\n-    if (r == nullptr) {\n-      break;\n-    }\n-    assert(r->is_regular(), \"There should be no humongous regions in the set of mixed-evac candidates\");\n-\n-    \/\/ If region r is evacuated to fragmented memory (to free memory within a partially used region), then we need\n-    \/\/ to decrease the capacity of the fragmented memory by the scaled loss.\n-\n-    const size_t live_data_for_evacuation = r->get_live_data_bytes();\n-    size_t lost_available = r->free();\n-\n-    if ((lost_available > 0) && (excess_fragmented_available > 0)) {\n-      if (lost_available < excess_fragmented_available) {\n-        excess_fragmented_available -= lost_available;\n-        lost_evacuation_capacity -= lost_available;\n-        lost_available  = 0;\n-      } else {\n-        lost_available -= excess_fragmented_available;\n-        lost_evacuation_capacity -= excess_fragmented_available;\n-        excess_fragmented_available = 0;\n-      }\n-    }\n-    size_t scaled_loss = (size_t) ((double) lost_available \/ ShenandoahOldEvacWaste);\n-    if ((lost_available > 0) && (fragmented_available > 0)) {\n-      if (scaled_loss + live_data_for_evacuation < fragmented_available) {\n-        fragmented_available -= scaled_loss;\n-        scaled_loss = 0;\n-      } else {\n-        \/\/ We will have to allocate this region's evacuation memory from unfragmented memory, so don't bother\n-        \/\/ to decrement scaled_loss\n-      }\n-    }\n-    if (scaled_loss > 0) {\n-      \/\/ We were not able to account for the lost free memory within fragmented memory, so we need to take this\n-      \/\/ allocation out of unfragmented memory.  Unfragmented memory does not need to account for loss of free.\n-      if (live_data_for_evacuation > unfragmented_available) {\n-        \/\/ There is no room to evacuate this region or any that come after it in within the candidates array.\n-        log_debug(gc, cset)(\"Not enough unfragmented memory (%zu) to hold evacuees (%zu) from region: (%zu)\",\n-                            unfragmented_available, live_data_for_evacuation, r->index());\n-        break;\n-      } else {\n-        unfragmented_available -= live_data_for_evacuation;\n-      }\n-    } else {\n-      \/\/ Since scaled_loss == 0, we have accounted for the loss of free memory, so we can allocate from either\n-      \/\/ fragmented or unfragmented available memory.  Use up the fragmented memory budget first.\n-      size_t evacuation_need = live_data_for_evacuation;\n-\n-      if (evacuation_need > fragmented_available) {\n-        evacuation_need -= fragmented_available;\n-        fragmented_available = 0;\n-      } else {\n-        fragmented_available -= evacuation_need;\n-        evacuation_need = 0;\n-      }\n-      if (evacuation_need > unfragmented_available) {\n-        \/\/ There is no room to evacuate this region or any that come after it in within the candidates array.\n-        log_debug(gc, cset)(\"Not enough unfragmented memory (%zu) to hold evacuees (%zu) from region: (%zu)\",\n-                            unfragmented_available, live_data_for_evacuation, r->index());\n-        break;\n-      } else {\n-        unfragmented_available -= evacuation_need;\n-        \/\/ dead code: evacuation_need == 0;\n-      }\n-    }\n-    collection_set->add_region(r);\n-    included_old_regions++;\n-    evacuated_old_bytes += live_data_for_evacuation;\n-    collected_old_bytes += r->garbage();\n-    consume_old_collection_candidate();\n-  }\n-\n-  if (_first_pinned_candidate != NOT_FOUND) {\n-    \/\/ Need to deal with pinned regions\n-    slide_pinned_regions_to_front();\n-  }\n-  decrease_unprocessed_old_collection_candidates_live_memory(evacuated_old_bytes);\n-  if (included_old_regions > 0) {\n-    log_info(gc, ergo)(\"Old-gen piggyback evac (\" UINT32_FORMAT \" regions, evacuating \" PROPERFMT \", reclaiming: \" PROPERFMT \")\",\n-                  included_old_regions, PROPERFMTARGS(evacuated_old_bytes), PROPERFMTARGS(collected_old_bytes));\n-  }\n-\n-  if (unprocessed_old_collection_candidates() == 0) {\n-    \/\/ We have added the last of our collection candidates to a mixed collection.\n-    \/\/ Any triggers that occurred during mixed evacuations may no longer be valid.  They can retrigger if appropriate.\n-    clear_triggers();\n-\n-    _old_generation->complete_mixed_evacuations();\n-  } else if (included_old_regions == 0) {\n-    \/\/ We have candidates, but none were included for evacuation - are they all pinned?\n-    \/\/ or did we just not have enough room for any of them in this collection set?\n-    \/\/ We don't want a region with a stuck pin to prevent subsequent old collections, so\n-    \/\/ if they are all pinned we transition to a state that will allow us to make these uncollected\n-    \/\/ (pinned) regions parsable.\n-    if (all_candidates_are_pinned()) {\n-      log_info(gc, ergo)(\"All candidate regions \" UINT32_FORMAT \" are pinned\", unprocessed_old_collection_candidates());\n-      _old_generation->abandon_mixed_evacuations();\n-    } else {\n-      log_info(gc, ergo)(\"No regions selected for mixed collection. \"\n-                         \"Old evacuation budget: \" PROPERFMT \", Remaining evacuation budget: \" PROPERFMT\n-                         \", Lost capacity: \" PROPERFMT\n-                         \", Next candidate: \" UINT32_FORMAT \", Last candidate: \" UINT32_FORMAT,\n-                         PROPERFMTARGS(old_evacuation_reserve),\n-                         PROPERFMTARGS(remaining_old_evacuation_budget),\n-                         PROPERFMTARGS(lost_evacuation_capacity),\n-                         _next_old_collection_candidate, _last_old_collection_candidate);\n-    }\n-  }\n-\n-  return (included_old_regions > 0);\n+  return add_old_regions_to_cset();\n@@ -319,0 +210,149 @@\n+bool ShenandoahOldHeuristics::add_old_regions_to_cset() {\n+  if (unprocessed_old_collection_candidates() == 0) {\n+    return false;\n+  }\n+  _first_pinned_candidate = NOT_FOUND;\n+\n+  \/\/ The number of old-gen regions that were selected as candidates for collection at the end of the most recent old-gen\n+  \/\/ concurrent marking phase and have not yet been collected is represented by unprocessed_old_collection_candidates().\n+  \/\/ Candidate regions are ordered according to increasing amount of live data.  If there is not sufficient room to\n+  \/\/ evacuate region N, then there is no need to even consider evacuating region N+1.\n+  while (unprocessed_old_collection_candidates() > 0) {\n+    \/\/ Old collection candidates are sorted in order of decreasing garbage contained therein.\n+    ShenandoahHeapRegion* r = next_old_collection_candidate();\n+    if (r == nullptr) {\n+      break;\n+    }\n+    assert(r->is_regular(), \"There should be no humongous regions in the set of mixed-evac candidates\");\n+\n+    \/\/ If region r is evacuated to fragmented memory (to free memory within a partially used region), then we need\n+    \/\/ to decrease the capacity of the fragmented memory by the scaled loss.\n+\n+    const size_t live_data_for_evacuation = r->get_live_data_bytes();\n+    size_t lost_available = r->free();\n+\n+    if ((lost_available > 0) && (_excess_fragmented_available > 0)) {\n+      if (lost_available < _excess_fragmented_available) {\n+        _excess_fragmented_available -= lost_available;\n+        lost_available = 0;\n+      } else {\n+        lost_available -= _excess_fragmented_available;\n+        _excess_fragmented_available = 0;\n+      }\n+    }\n+\n+    ssize_t fragmented_delta = 0;\n+    ssize_t unfragmented_delta = 0;\n+    size_t scaled_loss = (size_t) ((double) lost_available \/ ShenandoahOldEvacWaste);\n+    if ((lost_available > 0) && (_fragmented_available > 0)) {\n+      if (scaled_loss < _fragmented_available) {\n+        _fragmented_available -= scaled_loss;\n+        fragmented_delta = -scaled_loss;\n+        scaled_loss = 0;\n+      } else {\n+        scaled_loss -= _fragmented_available;\n+        fragmented_delta = -_fragmented_available;\n+        _fragmented_available = 0;\n+      }\n+    }\n+    \/\/ Allocate replica from unfragmented memory if that exists\n+    size_t evacuation_need = live_data_for_evacuation;\n+    if (evacuation_need < _unfragmented_available) {\n+      _unfragmented_available -= evacuation_need;;\n+    } else {\n+      if (_unfragmented_available > 0) {\n+        evacuation_need -= _unfragmented_available;\n+        unfragmented_delta = -_unfragmented_available;\n+        _unfragmented_available = 0;\n+      }\n+      \/\/ Take the remaining allocation out of fragmented available\n+      if (_fragmented_available > evacuation_need) {\n+        _fragmented_available -= evacuation_need;\n+      } else {\n+        \/\/ We cannot add this region into the collection set.  We're done.  Undo the adjustments to available.\n+        _fragmented_available -= fragmented_delta;\n+        _unfragmented_available -= unfragmented_delta;\n+        break;\n+      }\n+    }\n+    _mixed_evac_cset->add_region(r);\n+    _included_old_regions++;\n+    _evacuated_old_bytes += live_data_for_evacuation;\n+    _collected_old_bytes += r->garbage();\n+    consume_old_collection_candidate();\n+  }\n+  return true;\n+}\n+\n+bool ShenandoahOldHeuristics::finalize_mixed_evacs() {\n+  if (_first_pinned_candidate != NOT_FOUND) {\n+    \/\/ Need to deal with pinned regions\n+    slide_pinned_regions_to_front();\n+  }\n+  decrease_unprocessed_old_collection_candidates_live_memory(_evacuated_old_bytes);\n+  if (_included_old_regions > 0) {\n+    log_info(gc)(\"Old-gen mixed evac (%zu regions, evacuating %zu%s, reclaiming: %zu%s)\",\n+                 _included_old_regions,\n+                 byte_size_in_proper_unit(_evacuated_old_bytes), proper_unit_for_byte_size(_evacuated_old_bytes),\n+                 byte_size_in_proper_unit(_collected_old_bytes), proper_unit_for_byte_size(_collected_old_bytes));\n+  }\n+\n+  if (unprocessed_old_collection_candidates() == 0) {\n+    \/\/ We have added the last of our collection candidates to a mixed collection.\n+    \/\/ Any triggers that occurred during mixed evacuations may no longer be valid.  They can retrigger if appropriate.\n+    clear_triggers();\n+    _old_generation->complete_mixed_evacuations();\n+  } else if (_included_old_regions == 0) {\n+    \/\/ We have candidates, but none were included for evacuation - are they all pinned?\n+    \/\/ or did we just not have enough room for any of them in this collection set?\n+    \/\/ We don't want a region with a stuck pin to prevent subsequent old collections, so\n+    \/\/ if they are all pinned we transition to a state that will allow us to make these uncollected\n+    \/\/ (pinned) regions parsable.\n+    if (all_candidates_are_pinned()) {\n+      log_info(gc)(\"All candidate regions \" UINT32_FORMAT \" are pinned\", unprocessed_old_collection_candidates());\n+      _old_generation->abandon_mixed_evacuations();\n+    } else {\n+      log_info(gc)(\"No regions selected for mixed collection. \"\n+                   \"Old evacuation budget: \" PROPERFMT \", Next candidate: \" UINT32_FORMAT \", Last candidate: \" UINT32_FORMAT,\n+                   PROPERFMTARGS(_old_evacuation_reserve),\n+                   _next_old_collection_candidate, _last_old_collection_candidate);\n+    }\n+  }\n+  return (_included_old_regions > 0);\n+}\n+\n+bool ShenandoahOldHeuristics::top_off_collection_set() {\n+  if (unprocessed_old_collection_candidates() == 0) {\n+    return false;\n+  } else {\n+    ShenandoahYoungGeneration* young_generation = _heap->young_generation();\n+    size_t young_unaffiliated_regions = young_generation->free_unaffiliated_regions();\n+    size_t max_young_cset = young_generation->get_evacuation_reserve();\n+    size_t planned_young_evac =\n+      _mixed_evac_cset->get_live_bytes_in_untenurable_regions() + _mixed_evac_cset->get_live_bytes_in_tenurable_regions();\n+    size_t consumed_from_young_cset = (size_t) (planned_young_evac * ShenandoahEvacWaste);\n+    size_t available_to_loan_from_young_reserve = ((consumed_from_young_cset >= max_young_cset)?\n+                                                   0: max_young_cset - consumed_from_young_cset);\n+    size_t region_size_bytes = ShenandoahHeapRegion::region_size_bytes();\n+    if ((young_unaffiliated_regions == 0) || (available_to_loan_from_young_reserve < region_size_bytes)) {\n+      return false;\n+    } else {\n+      size_t regions_for_old_expansion = (available_to_loan_from_young_reserve \/ region_size_bytes);\n+      if (regions_for_old_expansion > young_unaffiliated_regions) {\n+        regions_for_old_expansion = young_unaffiliated_regions;\n+      }\n+      log_info(gc)(\"Augmenting old-gen evacuation budget from unexpended young-generation reserve by %zu regions\",\n+                   regions_for_old_expansion);\n+      size_t budget_supplement = region_size_bytes * regions_for_old_expansion;\n+      size_t supplement_after_waste = (size_t) (((double) budget_supplement) \/ ShenandoahOldEvacWaste);\n+      _old_evacuation_budget += supplement_after_waste;\n+      _unfragmented_available += supplement_after_waste;\n+\n+      _old_generation->augment_evacuation_reserve(budget_supplement);\n+      young_generation->set_evacuation_reserve(max_young_cset - budget_supplement);\n+\n+      return add_old_regions_to_cset();\n+    }\n+  }\n+}\n+\n@@ -327,1 +367,3 @@\n-\n+#ifdef ASSERT\n+  bool reclaimed_immediate = false;\n+#endif\n@@ -340,4 +382,4 @@\n-        \/\/ Only place regular or pinned regions with live data into the candidate set.\n-        \/\/ Pinned regions cannot be evacuated, but we are not actually choosing candidates\n-        \/\/ for the collection set here. That happens later during the next young GC cycle,\n-        \/\/ by which time, the pinned region may no longer be pinned.\n+      \/\/ Only place regular or pinned regions with live data into the candidate set.\n+      \/\/ Pinned regions cannot be evacuated, but we are not actually choosing candidates\n+      \/\/ for the collection set here. That happens later during the next young GC cycle,\n+      \/\/ by which time, the pinned region may no longer be pinned.\n@@ -346,0 +388,8 @@\n+#ifdef ASSERT\n+        if (!reclaimed_immediate) {\n+          reclaimed_immediate = true;\n+          \/\/ Inform the free-set that old trash regions may temporarily violate OldCollector bounds\n+          shenandoah_assert_heaplocked();\n+          heap->free_set()->advise_of_old_trash();\n+        }\n+#endif\n@@ -364,0 +414,8 @@\n+#ifdef ASSERT\n+        if (!reclaimed_immediate) {\n+          reclaimed_immediate = true;\n+          \/\/ Inform the free-set that old trash regions may temporarily violate OldCollector bounds\n+          shenandoah_assert_heaplocked();\n+          heap->free_set()->advise_of_old_trash();\n+        }\n+#endif\n@@ -542,0 +600,1 @@\n+  _live_bytes_in_unprocessed_candidates = 0;\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/heuristics\/shenandoahOldHeuristics.cpp","additions":211,"deletions":152,"binary":false,"changes":363,"status":"modified"},{"patch":"@@ -105,0 +105,14 @@\n+  \/\/ State variables involved in construction of a mixed-evacuation collection set.  These variables are initialized\n+  \/\/ when client code invokes prime_collection_set().  They are consulted, and sometimes modified, when client code\n+  \/\/ calls top_off_collection_set() to possibly expand the number of old-gen regions in a mixed evacuation cset, and by\n+  \/\/ finalize_mixed_evacs(), which prepares the way for mixed evacuations to begin.\n+  ShenandoahCollectionSet* _mixed_evac_cset;\n+  size_t _evacuated_old_bytes;\n+  size_t _collected_old_bytes;\n+  size_t _included_old_regions;\n+  size_t _old_evacuation_reserve;\n+  size_t _old_evacuation_budget;\n+  size_t _unfragmented_available;\n+  size_t _fragmented_available;\n+  size_t _excess_fragmented_available;\n+\n@@ -125,0 +139,7 @@\n+  \/\/ This internal helper routine adds as many mixed evacuation candidate regions as fit within the old-gen evacuation budget\n+  \/\/ to the collection set.  This may be called twice to prepare for any given mixed evacuation cycle, the first time with\n+  \/\/ a conservative old evacuation budget, and the second time with a larger more aggressive old evacuation budget.  Returns\n+  \/\/ true iff we need to finalize mixed evacs.  (If no regions are added to the collection set, there is no need to finalize\n+  \/\/ mixed evacuations.)\n+  bool add_old_regions_to_cset();\n+\n@@ -131,2 +152,15 @@\n-  \/\/ Return true iff the collection set is primed with at least one old-gen region.\n-  bool prime_collection_set(ShenandoahCollectionSet* set);\n+  \/\/ Initialize instance variables to support the preparation of a mixed-evacuation collection set.  Adds as many\n+  \/\/ old candidate regions into the collection set as can fit within the iniital conservative old evacuation budget.\n+  \/\/ Returns true iff we need to finalize mixed evacs.\n+  bool prime_collection_set(ShenandoahCollectionSet* collection_set);\n+\n+  \/\/ If young evacuation did not consume all of its available evacuation reserve, add as many additional mixed-\n+  \/\/ evacuation candidate regions into the collection set as will fit within this excess repurposed reserved.\n+  \/\/ Returns true iff we need to finalize mixed evacs.\n+  bool top_off_collection_set();\n+\n+  \/\/ Having added all eligible mixed-evacuation candidates to the collection set, this function updates the total count\n+  \/\/ of how much old-gen memory remains to be evacuated and adjusts the representation of old-gen regions that remain to\n+  \/\/ be evacuated, giving special attention to regions that are currently pinned.  It outputs relevant log messages and\n+  \/\/ returns true iff the collection set holds at least one unpinned mixed evacuation candidate.\n+  bool finalize_mixed_evacs();\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/heuristics\/shenandoahOldHeuristics.hpp","additions":36,"deletions":2,"binary":false,"changes":38,"status":"modified"},{"patch":"@@ -36,1 +36,1 @@\n-        : ShenandoahGenerationalHeuristics(generation) {\n+    : ShenandoahGenerationalHeuristics(generation) {\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/heuristics\/shenandoahYoungHeuristics.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -165,2 +165,0 @@\n-  assert(heap->is_concurrent_weak_root_in_progress(), \"Must be doing weak roots now\");\n-\n@@ -207,2 +205,0 @@\n-    entry_concurrent_update_refs_prepare(heap);\n-\n@@ -210,0 +206,1 @@\n+    entry_concurrent_update_refs_prepare(heap);\n@@ -230,0 +227,1 @@\n+    _abbreviated = true;\n@@ -238,1 +236,0 @@\n-    _abbreviated = true;\n@@ -282,0 +279,4 @@\n+  \/\/ After an abbreviated cycle, we reclaim immediate garbage.  Rebuild the freeset in order to establish\n+  \/\/ reserves for the next GC cycle.\n+  assert(_abbreviated, \"Only rebuild free set for abbreviated and old-marking cycles\");\n+  heap->rebuild_free_set(true \/*concurrent*\/);\n@@ -285,1 +286,0 @@\n-\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahConcurrentGC.cpp","additions":6,"deletions":6,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -330,1 +330,1 @@\n-  shenandoah_assert_heaplocked();\n+  shenandoah_assert_heaplocked_or_safepoint();\n@@ -443,0 +443,7 @@\n+void ShenandoahRegionPartitions::set_used_by(ShenandoahFreeSetPartitionId which_partition, size_t value) {\n+  shenandoah_assert_heaplocked();\n+  assert (which_partition < NumPartitions, \"selected free set must be valid\");\n+  _used[int(which_partition)] = value;\n+  _available[int(which_partition)] = _capacity[int(which_partition)] - value;\n+}\n+\n@@ -635,0 +642,27 @@\n+inline void ShenandoahRegionPartitions::adjust_interval_for_recycled_old_region_under_lock(ShenandoahHeapRegion* r) {\n+  assert(!r->is_trash() && (r->free() == _region_size_bytes), \"Bad argument\");\n+\n+  shenandoah_assert_heaplocked();\n+  idx_t idx = (idx_t) r->index();\n+  ShenandoahFreeSetPartitionId old_partition = ShenandoahFreeSetPartitionId::OldCollector;\n+  assert(_membership[int(old_partition)].is_set(idx), \"Region should be in OldCollector reserve\");\n+\n+  \/\/ Note that a recycled old trashed region may be in any one of the free set partitions according to the following scenarios:\n+  \/\/  1. The old region had already been retired, so it was NotFree, and we have not rebuilt free set. Region is still NotFree.\n+  \/\/  2. We recycled the region but we have not yet rebuilt the free set, so it is still in the OldCollector region.\n+  \/\/  3. We have found regions with alloc capacity but have not yet reserved_regions, so this is in Mutator set, and\n+  \/\/     the act of placing the region into the Mutator set properly adjusts interval for Mutator set.\n+  \/\/  4. During reserve_regions(), we moved this region into the Collector set, and the act of placing this region into\n+  \/\/     Collector set properly adjusts the interval for the Collector set.\n+  \/\/  5. During reserve_regions, we moved this region into the OldCollector set, and the act of placing this region into\n+  \/\/     OldCollector set properly adjusts the interval for the OldCollector set.\n+  \/\/ Only case 2 needs to be fixed up here.\n+  assert(_leftmosts[int(old_partition)] <= idx && _rightmosts[int(old_partition)] >= idx, \"sanity\");\n+  if (_leftmosts_empty[int(old_partition)] > idx) {\n+    _leftmosts_empty[int(old_partition)] = idx;\n+  }\n+  if (_rightmosts_empty[int(old_partition)] < idx) {\n+    _rightmosts_empty[int(old_partition)] = idx;\n+  }\n+}\n+\n@@ -904,1 +938,1 @@\n-void ShenandoahRegionPartitions::assert_bounds(bool validate_totals) {\n+void ShenandoahRegionPartitions::assert_bounds(bool validate_totals, bool old_trash_not_in_bounds) {\n@@ -980,0 +1014,21 @@\n+        \/\/ When old_trash_not_in_bounds, an old trashed region might reside in:\n+        \/\/ 1. NotFree if the region had already been retired\n+        \/\/ 2. OldCollector because the region was originally in OldCollector when it was identified as immediate garbage, or\n+        \/\/ 3. Mutator because we have run find_regions_with_alloc_capacity(), or\n+        \/\/ 4. Collector because reserve_regions moved from Mutator to Collector but we have not yet recycled the trash\n+        \/\/ 5. OldCollector because reserve_regions moved from Mutator to OldCollector but we have not yet recycled the trash\n+\n+        \/\/ In case 1, there is no issue with empty-free intervals.\n+        \/\/ In cases 3 - 5, there is no issue with empty-free intervals because the act of moving the region into the partition\n+        \/\/    causes the empty-free interval to be updated.\n+        \/\/ Only in case 2 do we need to disable the assert checking, but it is difficult to distinguish case 2 from case 5,\n+        \/\/    so we do not assert bounds for case 2 or case 5.\n+\n+        ShenandoahHeapRegion* r = ShenandoahHeap::heap()->get_region(i);\n+        if (old_trash_not_in_bounds &&\n+            (partition == ShenandoahFreeSetPartitionId::OldCollector) && r->is_old() && r->is_trash()) {\n+          \/\/ If Old trash has been identified but we have not yet rebuilt the freeset to acount for the trashed regions,\n+          \/\/ or if old trash has not yet been recycled, do not expect these trash regions to be within the OldCollector\n+          \/\/ partition's bounds.\n+          continue;\n+        }\n@@ -1024,1 +1079,1 @@\n-          \"Mutator free regions before the leftmost: %zd, bound %zd\",\n+          \"Mutator free region before the leftmost: %zd, bound %zd\",\n@@ -1027,1 +1082,1 @@\n-          \"Mutator free regions past the rightmost: %zd, bound %zd\",\n+          \"Mutator free region past the rightmost: %zd, bound %zd\",\n@@ -1032,6 +1087,6 @@\n-  assert (beg_off >= leftmost_empty(ShenandoahFreeSetPartitionId::Mutator),\n-          \"Mutator free empty regions before the leftmost: %zd, bound %zd\",\n-          beg_off, leftmost_empty(ShenandoahFreeSetPartitionId::Mutator));\n-  assert (end_off <= rightmost_empty(ShenandoahFreeSetPartitionId::Mutator),\n-          \"Mutator free empty regions past the rightmost: %zd, bound %zd\",\n-          end_off, rightmost_empty(ShenandoahFreeSetPartitionId::Mutator));\n+  assert (beg_off >= _leftmosts_empty[int(ShenandoahFreeSetPartitionId::Mutator)],\n+          \"free empty region (%zd) before the leftmost bound %zd\",\n+          beg_off, _leftmosts_empty[int(ShenandoahFreeSetPartitionId::Mutator)]);\n+  assert (end_off <= _rightmosts_empty[int(ShenandoahFreeSetPartitionId::Mutator)],\n+          \"free empty region (%zd) past the rightmost bound %zd\",\n+          end_off, _rightmosts_empty[int(ShenandoahFreeSetPartitionId::Mutator)]);\n@@ -1057,1 +1112,1 @@\n-          \"Collector free regions before the leftmost: %zd, bound %zd\",\n+          \"Collector free region before the leftmost: %zd, bound %zd\",\n@@ -1060,1 +1115,1 @@\n-          \"Collector free regions past the rightmost: %zd, bound %zd\",\n+          \"Collector free region past the rightmost: %zd, bound %zd\",\n@@ -1066,2 +1121,2 @@\n-          \"Collector free empty regions before the leftmost: %zd, bound %zd\",\n-          beg_off, leftmost_empty(ShenandoahFreeSetPartitionId::Collector));\n+          \"Collector free empty region before the leftmost: %zd, bound %zd\",\n+          beg_off, _leftmosts_empty[int(ShenandoahFreeSetPartitionId::Collector)]);\n@@ -1069,2 +1124,2 @@\n-          \"Collector free empty regions past the rightmost: %zd, bound %zd\",\n-          end_off, rightmost_empty(ShenandoahFreeSetPartitionId::Collector));\n+          \"Collector free empty region past the rightmost: %zd, bound %zd\",\n+          end_off, _rightmosts_empty[int(ShenandoahFreeSetPartitionId::Collector)]);\n@@ -1087,1 +1142,10 @@\n-  \/\/ If OldCollector partition is empty, leftmosts will both equal max, rightmosts will both equal zero.\n+  \/\/ Concurrent recycling of trash first recycles a region (changing its state from is_trash to is_empty without the heap lock),\n+  \/\/ then it acquires the heap lock, then it adjusts the partition for the newly recycled region and releases the lock.  After\n+  \/\/ all trashed regions have been recycled, we grab the heap lock again and clear the _old_trash_not_in_bounds flag.\n+  \/\/\n+  \/\/ Bottom line: if _old_trash_not_in_bounds, the ranges of old regions detected by examination of all region states may\n+  \/\/ be larger than the spans reported by leftmosts(OldColector) and rightmosts(OldCollector) and by the spans represented\n+  \/\/ by _leftmosts_empty[OldCollector] and _rightmosts_empty[OldCollector]\n+\n+  \/\/ If OldCollector partition is empty and !old_trash_not_in_bounds:\n+  \/\/    leftmosts will both equal max, rightmosts will both equal zero.\n@@ -1091,2 +1155,2 @@\n-  assert (beg_off >= leftmost(ShenandoahFreeSetPartitionId::OldCollector),\n-          \"OldCollector free regions before the leftmost: %zd, bound %zd\",\n+  assert (old_trash_not_in_bounds || (beg_off >= leftmost(ShenandoahFreeSetPartitionId::OldCollector)),\n+          \"free regions before the leftmost: %zd, bound %zd\",\n@@ -1094,2 +1158,2 @@\n-  assert (end_off <= rightmost(ShenandoahFreeSetPartitionId::OldCollector),\n-          \"OldCollector free regions past the rightmost: %zd, bound %zd\",\n+  assert (old_trash_not_in_bounds || (end_off <= rightmost(ShenandoahFreeSetPartitionId::OldCollector)),\n+          \"free regions past the rightmost: %zd, bound %zd\",\n@@ -1100,6 +1164,12 @@\n-  assert (beg_off >= _leftmosts_empty[int(ShenandoahFreeSetPartitionId::OldCollector)],\n-          \"OldCollector free empty regions before the leftmost: %zd, bound %zd\",\n-          beg_off, leftmost_empty(ShenandoahFreeSetPartitionId::OldCollector));\n-  assert (end_off <= _rightmosts_empty[int(ShenandoahFreeSetPartitionId::OldCollector)],\n-          \"OldCollector free empty regions past the rightmost: %zd, bound %zd\",\n-          end_off, rightmost_empty(ShenandoahFreeSetPartitionId::OldCollector));\n+  assert (old_trash_not_in_bounds || (beg_off >= _leftmosts_empty[int(ShenandoahFreeSetPartitionId::OldCollector)]),\n+          \"free empty region (%zd) before the leftmost bound %zd, old_trash_not_in_bounds: no, region %s trash\",\n+          beg_off, _leftmosts_empty[int(ShenandoahFreeSetPartitionId::OldCollector)],\n+          ((beg_off >= _max)? \"out of bounds is not\":\n+           (ShenandoahHeap::heap()->get_region(_leftmosts_empty[int(ShenandoahFreeSetPartitionId::OldCollector)])->is_trash()?\n+            \"is\": \"is not\")));\n+  assert (old_trash_not_in_bounds || (end_off <= _rightmosts_empty[int(ShenandoahFreeSetPartitionId::OldCollector)]),\n+          \"free empty region (%zd) past the rightmost bound %zd, old_trash_not_in_bounds: no, region %s trash\",\n+          end_off, _rightmosts_empty[int(ShenandoahFreeSetPartitionId::OldCollector)],\n+          ((end_off < 0)? \"out of bounds is not\" :\n+           (ShenandoahHeap::heap()->get_region(_rightmosts_empty[int(ShenandoahFreeSetPartitionId::OldCollector)])->is_trash()?\n+            \"is\": \"is not\")));\n@@ -1194,0 +1264,3 @@\n+#ifdef ASSERT\n+  _old_trash_not_in_bounds(false),\n+#endif\n@@ -1265,1 +1338,1 @@\n-  _partitions.assert_bounds(true);\n+  _partitions.assert_bounds(true, _old_trash_not_in_bounds);\n@@ -1508,0 +1581,4 @@\n+  \/\/ We must call try_recycle_under_lock() even if !r->is_trash().  The reason is that if r is being recycled at this\n+  \/\/ moment by a GC worker thread, it may appear to be not trash even though it has not yet been fully recycled.  If\n+  \/\/ we proceed without waiting for the worker to finish recycling the region, the worker thread may overwrite the\n+  \/\/ region's affiliation with FREE after we set the region's affiliation to req.afiliation() below\n@@ -1509,0 +1586,9 @@\n+#ifdef ASSERT\n+  assert(!r->is_trash(), \"try_recycle_under_lock() should assure that region is recycled\");\n+  if (_old_trash_not_in_bounds &&\n+      r->is_empty() && _partitions.in_free_set(ShenandoahFreeSetPartitionId::OldCollector, r->index())) {\n+    \/\/ Note: if assertions are not enforced, there's no rush to adjust this interval.  We'll adjust the\n+    \/\/ interval when we eventually rebuild the free set.\n+    _partitions.adjust_interval_for_recycled_old_region_under_lock(r);\n+  }\n+#endif\n@@ -1510,1 +1596,0 @@\n-\n@@ -1695,1 +1780,1 @@\n-  _partitions.assert_bounds(true);\n+  _partitions.assert_bounds(true, _old_trash_not_in_bounds);\n@@ -1827,0 +1912,6 @@\n+\n+  \/\/ KELVIN TODO: SHOULD I USE \n+  \/\/ retire_range_from_partition() will adjust bounds on Mutator free set if appropriate\n+  \/\/   _partitions.retire_range_from_partition(ShenandoahFreeSetPartitionId::Mutator, beg, end);\n+  \/\/ instead of multiple calls to _partitions.retire_from_partition(Mutator, i, r->used()) abovev?\n+\n@@ -1847,1 +1938,1 @@\n-  _partitions.assert_bounds(true);\n+  _partitions.assert_bounds(true, _old_trash_not_in_bounds);\n@@ -1857,0 +1948,3 @@\n+#ifdef ASSERT\n+  bool _old_trash_not_in_bounds;\n+#endif\n@@ -1944,3 +2038,13 @@\n-  ShenandoahRecycleTrashedRegionClosure(ShenandoahRegionPartitions* p): ShenandoahHeapRegionClosure() {\n-    _partitions = p;\n-    _recycled_region_count = 0;\n+#ifdef ASSERT\n+  ShenandoahRecycleTrashedRegionClosure(ShenandoahRegionPartitions* partitions, bool old_trash_not_in_bounds):\n+    ShenandoahHeapRegionClosure(),\n+    _old_trash_not_in_bounds(old_trash_not_in_bounds),\n+    _partitions(partitions),\n+    _recycled_region_count(0)\n+#else\n+  ShenandoahRecycleTrashedRegionClosure(ShenandoahRegionPartitions* partitions):\n+    ShenandoahHeapRegionClosure(),\n+    _partitions(partitions),\n+    _recycled_region_count(0)\n+#endif\n+  {\n@@ -1954,1 +2058,15 @@\n-    r->try_recycle();\n+    if (r->is_trash()) {\n+#ifdef ASSERT\n+      ShenandoahHeapLocker locker(ShenandoahHeap::heap()->lock());\n+      r->try_recycle_under_lock();\n+      assert(!r->is_trash(), \"try_recycle_under_lock() should assure that region is recycled\");\n+      if (_old_trash_not_in_bounds &&\n+          r->is_empty() && _partitions->in_free_set(ShenandoahFreeSetPartitionId::OldCollector, r->index())) {\n+        \/\/ Note: if assertions are not enforced, there's no rush to adjust this interval.  We'll adjust the\n+        \/\/ interval when we eventually rebuild the free set.\n+        _partitions->adjust_interval_for_recycled_old_region_under_lock(r);\n+      }\n+#else\n+      r->try_recycle();\n+#endif\n+    }\n@@ -1968,1 +2086,3 @@\n-\n+#ifdef ASSERT\n+  ShenandoahRecycleTrashedRegionClosure closure(&_partitions, _old_trash_not_in_bounds);\n+#else\n@@ -1970,0 +2090,1 @@\n+#endif\n@@ -1971,0 +2092,4 @@\n+#ifdef ASSERT\n+  ShenandoahHeapLocker locker(_heap->lock());\n+  _old_trash_not_in_bounds = false;\n+#endif\n@@ -1991,1 +2116,1 @@\n-    _partitions.assert_bounds(true);\n+    _partitions.assert_bounds(true, _old_trash_not_in_bounds);\n@@ -2045,1 +2170,1 @@\n-      _partitions.assert_bounds(true);\n+      _partitions.assert_bounds(true, _old_trash_not_in_bounds);\n@@ -2076,1 +2201,1 @@\n-  _partitions.assert_bounds(true);\n+  _partitions.assert_bounds(true, _old_trash_not_in_bounds);\n@@ -2157,2 +2282,2 @@\n-      \/\/ Trashed regions represent immediate garbage identified by final mark and regions that had been in the collection\n-      \/\/ partition but have not yet been \"cleaned up\" following update refs.\n+      \/\/ Trashed regions represent regions that had been in the collection set (or may have been identified as immediate garbage)\n+      \/\/ but have not yet been \"cleaned up\".  The cset regions are not \"trashed\" until we have finished update refs.\n@@ -2160,0 +2285,6 @@\n+        \/\/ We're going to place this region into the Mutator set.  We increment old_trashed_regions because this count represents\n+        \/\/ regions that the old generation is entitled to without any transfer from young.  We do not place this region into\n+        \/\/ the OldCollector partition at this time.  Instead, we let reserve_regions() decide whether to place this region\n+        \/\/ into the OldCollector partition.  Deferring the decision allows reserve_regions() to more effectively pack the\n+        \/\/ OldCollector regions into high-address memory.  We do not adjust capacities of old and young generations at this\n+        \/\/ time.  At the end of finish_rebuild(), the capacities are adjusted based on the results of reserve_regions().\n@@ -2180,1 +2311,1 @@\n-          \/\/ Both young and old collected regions (trashed) are placed into the Mutator set\n+          \/\/ Both young and old (possibly immediately) collected regions (trashed) are placed into the Mutator set\n@@ -2316,1 +2447,1 @@\n-  _partitions.assert_bounds(true);\n+  _partitions.assert_bounds(true, _old_trash_not_in_bounds);\n@@ -2354,1 +2485,3 @@\n-  _partitions.assert_bounds(true);\n+#ifdef ASSERT\n+  _partitions.assert_bounds(true, _old_trash_not_in_bounds);\n+#endif\n@@ -2436,1 +2569,1 @@\n-  _partitions.assert_bounds(true);\n+  _partitions.assert_bounds(true, _old_trash_not_in_bounds);\n@@ -2503,1 +2636,1 @@\n-  _partitions.assert_bounds(true);\n+  _partitions.assert_bounds(true, _old_trash_not_in_bounds);\n@@ -2578,1 +2711,1 @@\n-  _partitions.assert_bounds(true);\n+  _partitions.assert_bounds(true, _old_trash_not_in_bounds);\n@@ -2624,0 +2757,6 @@\n+void ShenandoahFreeSet::rebuild() {\n+  size_t young_cset_regions, old_cset_regions, first_old_region, last_old_region, old_region_count;\n+  prepare_to_rebuild(young_cset_regions, old_cset_regions, first_old_region, last_old_region, old_region_count);\n+  finish_rebuild(young_cset_regions, old_cset_regions, old_region_count);\n+}\n+\n@@ -2636,2 +2775,2 @@\n-void ShenandoahFreeSet::finish_rebuild(size_t young_trashed_regions, size_t old_trashed_regions, size_t old_region_count,\n-                                       bool have_evacuation_reserves) {\n+\n+void ShenandoahFreeSet::finish_rebuild(size_t young_cset_regions, size_t old_cset_regions, size_t old_region_count) {\n@@ -2642,2 +2781,1 @@\n-    compute_young_and_old_reserves(young_trashed_regions, old_trashed_regions, have_evacuation_reserves,\n-                                   young_reserve, old_reserve);\n+    compute_young_and_old_reserves(young_cset_regions, old_cset_regions, young_reserve, old_reserve);\n@@ -2657,1 +2795,1 @@\n-  _partitions.assert_bounds(true);\n+  _partitions.assert_bounds(true, _old_trash_not_in_bounds);\n@@ -2659,0 +2797,32 @@\n+  \/\/ Even though we have finished rebuild, old trashed regions may not yet have been recycled, so leave\n+  \/\/ _old_trash_not_in_bounds as is.  Following rebuild, old trashed regions may reside in Mutator, Collector,\n+  \/\/ or OldCollector partitions.\n+}\n+\n+\n+\/\/ Reduce old reserve (when there are insufficient resources to satisfy the original request).\n+void ShenandoahFreeSet::reduce_old_reserve(size_t adjusted_old_reserve, size_t requested_old_reserve) {\n+  ShenandoahOldGeneration* const old_generation = _heap->old_generation();\n+  size_t requested_promoted_reserve = old_generation->get_promoted_reserve();\n+  size_t requested_old_evac_reserve = old_generation->get_evacuation_reserve();\n+  assert(adjusted_old_reserve < requested_old_reserve, \"Only allow reduction\");\n+  assert(requested_promoted_reserve + requested_old_evac_reserve >= adjusted_old_reserve, \"Sanity\");\n+  size_t delta = requested_old_reserve - adjusted_old_reserve;\n+\n+  if (requested_promoted_reserve >= delta) {\n+    requested_promoted_reserve -= delta;\n+    old_generation->set_promoted_reserve(requested_promoted_reserve);\n+  } else {\n+    delta -= requested_promoted_reserve;\n+    requested_promoted_reserve = 0;\n+    requested_old_evac_reserve -= delta;\n+    old_generation->set_promoted_reserve(requested_promoted_reserve);\n+    old_generation->set_evacuation_reserve(requested_old_evac_reserve);\n+  }\n+}\n+\n+\/\/ Reduce young reserve (when there are insufficient resources to satisfy the original request).\n+void ShenandoahFreeSet::reduce_young_reserve(size_t adjusted_young_reserve, size_t requested_young_reserve) {\n+  ShenandoahYoungGeneration* const young_generation = _heap->young_generation();\n+  assert(adjusted_young_reserve < requested_young_reserve, \"Only allow reduction\");\n+  young_generation->set_evacuation_reserve(adjusted_young_reserve);\n@@ -2675,1 +2845,0 @@\n-                                                       bool have_evacuation_reserves,\n@@ -2681,1 +2850,1 @@\n-  size_t old_available = old_generation->available();\n+  size_t old_available = old_generation->available() + old_trashed_regions * region_size_bytes;\n@@ -2692,0 +2861,9 @@\n+  assert(young_capacity >= young_generation->used(),\n+         \"Young capacity (%zu) must exceed used (%zu)\", young_capacity, young_generation->used());\n+\n+  size_t young_available = young_capacity - young_generation->used();\n+  young_available += young_trashed_regions * region_size_bytes;\n+\n+  assert(young_available >= young_unaffiliated_regions * region_size_bytes, \"sanity\");\n+  assert(old_available >= old_unaffiliated_regions * region_size_bytes, \"sanity\");\n+  \n@@ -2711,0 +2889,1 @@\n+    young_available += xfer_bytes;\n@@ -2719,26 +2898,7 @@\n-  if (have_evacuation_reserves) {\n-    \/\/ We are rebuilding at the end of final mark, having already established evacuation budgets for this GC pass.\n-    const size_t promoted_reserve = old_generation->get_promoted_reserve();\n-    const size_t old_evac_reserve = old_generation->get_evacuation_reserve();\n-    young_reserve_result = young_generation->get_evacuation_reserve();\n-    old_reserve_result = promoted_reserve + old_evac_reserve;\n-    if (old_reserve_result > old_available) {\n-      \/\/ Try to transfer memory from young to old.\n-      size_t old_deficit = old_reserve_result - old_available;\n-      size_t old_region_deficit = (old_deficit + region_size_bytes - 1) \/ region_size_bytes;\n-      if (young_unaffiliated_regions < old_region_deficit) {\n-        old_region_deficit = young_unaffiliated_regions;\n-      }\n-      young_unaffiliated_regions -= old_region_deficit;\n-      old_unaffiliated_regions += old_region_deficit;\n-      old_region_balance -= old_region_deficit;\n-      old_generation->set_region_balance(old_region_balance);\n-    }\n-  } else {\n-    \/\/ We are rebuilding at end of GC, so we set aside budgets specified on command line (or defaults)\n-    young_reserve_result = (young_capacity * ShenandoahEvacReserve) \/ 100;\n-    \/\/ The auto-sizer has already made old-gen large enough to hold all anticipated evacuations and promotions.\n-    \/\/ Affiliated old-gen regions are already in the OldCollector free set.  Add in the relevant number of\n-    \/\/ unaffiliated regions.\n-    old_reserve_result = old_available;\n-  }\n+  const size_t promoted_reserve = old_generation->get_promoted_reserve();\n+  const size_t old_evac_reserve = old_generation->get_evacuation_reserve();\n+  young_reserve_result = young_generation->get_evacuation_reserve();\n+  old_reserve_result = promoted_reserve + old_evac_reserve;\n+  assert(old_reserve_result + young_reserve_result <= old_available + young_available,\n+         \"Cannot reserve (%zu + %zu + %zu) more than is available: %zu + %zu\",\n+         promoted_reserve, old_evac_reserve, young_reserve_result, old_available, young_available);\n@@ -2751,1 +2911,1 @@\n-      _partitions.capacity_of(ShenandoahFreeSetPartitionId::OldCollector) + old_unaffiliated_regions * region_size_bytes) {\n+      _partitions.available_in(ShenandoahFreeSetPartitionId::OldCollector) + old_unaffiliated_regions * region_size_bytes) {\n@@ -2753,1 +2913,1 @@\n-      _partitions.capacity_of(ShenandoahFreeSetPartitionId::OldCollector) + old_unaffiliated_regions * region_size_bytes;\n+      _partitions.available_in(ShenandoahFreeSetPartitionId::OldCollector) + old_unaffiliated_regions * region_size_bytes;\n@@ -2998,1 +3158,1 @@\n-  _partitions.assert_bounds(true);\n+  _partitions.assert_bounds(true, _old_trash_not_in_bounds);\n@@ -3004,0 +3164,2 @@\n+      assert(_heap->mode()->is_generational(), \"to_old_reserve > 0 implies generational mode\");\n+      reduce_old_reserve(old_reserve, to_reserve_old);\n@@ -3007,0 +3169,3 @@\n+      if (_heap->mode()->is_generational()) {\n+        reduce_young_reserve(reserve, to_reserve);\n+      }\n@@ -3008,1 +3173,1 @@\n-                          PROPERFMTARGS(to_reserve), PROPERFMTARGS(reserve));\n+                         PROPERFMTARGS(to_reserve), PROPERFMTARGS(reserve));\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahFreeSet.cpp","additions":249,"deletions":84,"binary":false,"changes":333,"status":"modified"},{"patch":"@@ -227,0 +227,4 @@\n+  \/\/ For recycled region r in the OldCollector partition but possibly not within the interval for empty OldCollector regions,\n+  \/\/ expand the empty interval to include this region.\n+  inline void adjust_interval_for_recycled_old_region_under_lock(ShenandoahHeapRegion* r);\n+\n@@ -383,6 +387,1 @@\n-  inline void set_used_by(ShenandoahFreeSetPartitionId which_partition, size_t value) {\n-    shenandoah_assert_heaplocked();\n-    assert (which_partition < NumPartitions, \"selected free set must be valid\");\n-    _used[int(which_partition)] = value;\n-    _available[int(which_partition)] = _capacity[int(which_partition)] - value;\n-  }\n+  inline void set_used_by(ShenandoahFreeSetPartitionId which_partition, size_t value);\n@@ -412,1 +411,6 @@\n-  void assert_bounds(bool validate_totals) NOT_DEBUG_RETURN;\n+  \/\/ Iff validate_totals is true, assert_bounds() confirms not only that bounds are correct, but also that total\n+  \/\/ capacities and used within each partition are correct.\n+  \/\/\n+  \/\/ The old_trash_not_in_bounds argument denotes that some old trash has not yet been recycled.  In this scenario,\n+  \/\/ assert_bounds() allows that certain old regions do not reside within the bounds for that partition.\n+  void assert_bounds(bool validate_totals, bool old_trash_not_in_bounds) NOT_DEBUG_RETURN;\n@@ -449,0 +453,4 @@\n+#ifdef ASSERT\n+  bool _old_trash_not_in_bounds;\n+#endif\n+\n@@ -635,0 +643,3 @@\n+  void reduce_young_reserve(size_t adjusted_young_reserve, size_t requested_young_reserve);\n+  void reduce_old_reserve(size_t adjusted_old_reserve, size_t requested_old_reserve);\n+\n@@ -721,0 +732,6 @@\n+  \/\/ Rebuild the free set.  This combines the effects of prepare_to_rebuild() and finish_rebuild() with no intervening\n+  \/\/ efforts to rebalance generation sizes.  When the free set is rebuild, we reserve sufficient memory within the\n+  \/\/ collector partition (and, for generational mode, the old collector partition) based on the amount reserved\n+  \/\/ by heuristics to support the next planned evacuation effort.\n+  void rebuild();\n+\n@@ -732,12 +749,4 @@\n-  \/\/ hold the results of evacuating to young-gen and to old-gen, and have_evacuation_reserves should be true.\n-  \/\/ These quantities, stored as reserves for their respective generations, are consulted prior to rebuilding\n-  \/\/ the free set (ShenandoahFreeSet) in preparation for evacuation.  When the free set is rebuilt, we make sure\n-  \/\/ to reserve sufficient memory in the collector and old_collector sets to hold evacuations.\n-  \/\/\n-  \/\/ We also rebuild the free set at the end of GC, as we prepare to idle GC until the next trigger.  In this case,\n-  \/\/ have_evacuation_reserves is false because we don't yet know how much memory will need to be evacuated in the\n-  \/\/ next GC cycle.  When have_evacuation_reserves is false, the free set rebuild operation reserves for the collector\n-  \/\/ and old_collector sets based on alternative mechanisms, such as ShenandoahEvacReserve, ShenandoahOldEvacReserve, and\n-  \/\/ ShenandoahOldCompactionReserve.  In a future planned enhancement, the reserve for old_collector set when the\n-  \/\/ evacuation reserves are unknown, is based in part on anticipated promotion as determined by analysis of live data\n-  \/\/ found during the previous GC pass which is one less than the current tenure age.\n+  \/\/ hold the results of evacuating to young-gen and to old-gen.  These quantities, stored in reserves for their\n+  \/\/ respective generations, are consulted prior to rebuilding the free set (ShenandoahFreeSet) in preparation for\n+  \/\/ evacuation.  When the free set is rebuilt, we make sure to reserve sufficient memory in the collector and\n+  \/\/ old_collector sets to hold evacuations.\n@@ -748,5 +757,1 @@\n-  \/\/ have_evacuation_reserves is true iff the desired values of young-gen and old-gen evacuation reserves and old-gen\n-  \/\/                    promotion reserve have been precomputed (and can be obtained by invoking\n-  \/\/                    <generation>->get_evacuation_reserve() or old_gen->get_promoted_reserve()\n-  void finish_rebuild(size_t young_cset_regions, size_t old_cset_regions, size_t num_old_regions,\n-                      bool have_evacuation_reserves = false);\n+  void finish_rebuild(size_t young_cset_regions, size_t old_cset_regions, size_t num_old_regions);\n@@ -764,0 +769,8 @@\n+#ifdef ASSERT\n+  \/\/ Advise FreeSet that old trash regions have not yet been accounted for in OldCollector partition bounds\n+  void advise_of_old_trash() {\n+    shenandoah_assert_heaplocked();\n+    _old_trash_not_in_bounds = true;\n+  }\n+#endif\n+\n@@ -784,0 +797,5 @@\n+  \/\/ Use this version of available() if the heap lock is held.\n+  inline size_t available_locked() const {\n+    return _partitions.available_in(ShenandoahFreeSetPartitionId::Mutator);\n+  }\n+\n@@ -785,2 +803,6 @@\n-  inline size_t humongous_waste_in_mutator() const { return _partitions.humongous_waste(ShenandoahFreeSetPartitionId::Mutator); }\n-  inline size_t humongous_waste_in_old() const { return _partitions.humongous_waste(ShenandoahFreeSetPartitionId::OldCollector); }\n+  inline size_t humongous_waste_in_mutator() const {\n+    return _partitions.humongous_waste(ShenandoahFreeSetPartitionId::Mutator);\n+  }\n+  inline size_t humongous_waste_in_old() const {\n+    return _partitions.humongous_waste(ShenandoahFreeSetPartitionId::OldCollector);\n+  }\n@@ -852,1 +874,1 @@\n-  void compute_young_and_old_reserves(size_t young_cset_regions, size_t old_cset_regions, bool have_evacuation_reserves,\n+  void compute_young_and_old_reserves(size_t young_cset_regions, size_t old_cset_regions,\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahFreeSet.hpp","additions":49,"deletions":27,"binary":false,"changes":76,"status":"modified"},{"patch":"@@ -525,0 +525,1 @@\n+      \/\/ No need to adjust_interval_for_recycled_old_region.  That will be taken care of during freeset rebuild.\n@@ -969,0 +970,1 @@\n+      \/\/ No need to adjust_interval_for_recycled_old_region.  That will be taken care of during freeset rebuild.\n@@ -1116,11 +1118,1 @@\n-    size_t young_cset_regions, old_cset_regions;\n-    size_t first_old, last_old, num_old;\n-    heap->free_set()->prepare_to_rebuild(young_cset_regions, old_cset_regions, first_old, last_old, num_old);\n-\n-    \/\/ We also do not expand old generation size following Full GC because we have scrambled age populations and\n-    \/\/ no longer have objects separated by age into distinct regions.\n-    if (heap->mode()->is_generational()) {\n-      ShenandoahGenerationalFullGC::compute_balances();\n-    }\n-\n-    heap->free_set()->finish_rebuild(young_cset_regions, old_cset_regions, num_old);\n+    heap->free_set()->rebuild();\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahFullGC.cpp","additions":3,"deletions":11,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -263,2 +263,3 @@\n-  \/\/ maximum_young_evacuation_reserve is upper bound on memory to be evacuated out of young\n-  const size_t maximum_young_evacuation_reserve = (young_generation->max_capacity() * ShenandoahEvacReserve) \/ 100;\n+  \/\/ maximum_young_evacuation_reserve is upper bound on memory to be evacuated into young Collector Reserve.  This is\n+  \/\/ bounded at the end of previous GC cycle, based on available memory and balancing of evacuation to old and young.\n+  size_t maximum_young_evacuation_reserve = young_generation->get_evacuation_reserve();\n@@ -271,1 +272,1 @@\n-  \/\/ Let SOEP = ShenandoahOldEvacRatioPercent,\n+  \/\/ Let SOEP = ShenandoahOldEvacPercent,\n@@ -283,1 +284,1 @@\n-  assert(ShenandoahOldEvacRatioPercent <= 100, \"Error\");\n+  assert(ShenandoahOldEvacPercent <= 100, \"Error\");\n@@ -285,2 +286,2 @@\n-  const size_t maximum_old_evacuation_reserve = (ShenandoahOldEvacRatioPercent == 100) ?\n-    old_available : MIN2((maximum_young_evacuation_reserve * ShenandoahOldEvacRatioPercent) \/ (100 - ShenandoahOldEvacRatioPercent),\n+  const size_t maximum_old_evacuation_reserve = (ShenandoahOldEvacPercent == 100) ?\n+    old_available : MIN2((maximum_young_evacuation_reserve * ShenandoahOldEvacPercent) \/ (100 - ShenandoahOldEvacPercent),\n@@ -363,1 +364,0 @@\n-\/\/\n@@ -467,0 +467,1 @@\n+    assert(excess_old >= regions_to_xfer * region_size_bytes, \"Cannot xfer more than excess old\");\n@@ -783,1 +784,1 @@\n-      \/\/ place, and preselect older regions that will be promoted by evacuation.\n+      \/\/ place, and preselected older regions that will be promoted by evacuation.\n@@ -786,2 +787,1 @@\n-      \/\/ Choose the collection set, including the regions preselected above for\n-      \/\/ promotion into the old generation.\n+      \/\/ Choose the collection set, including the regions preselected above for promotion into the old generation.\n@@ -789,4 +789,2 @@\n-      if (!collection_set->is_empty()) {\n-        \/\/ only make use of evacuation budgets when we are evacuating\n-        adjust_evacuation_budgets(heap, collection_set);\n-      }\n+      \/\/ Even if collection_set->is_empty(), we want to adjust budgets, making reserves available to mutator.\n+      adjust_evacuation_budgets(heap, collection_set);\n@@ -816,13 +814,1 @@\n-    size_t young_cset_regions, old_cset_regions;\n-\n-    \/\/ We are preparing for evacuation.  At this time, we ignore cset region tallies.\n-    size_t first_old, last_old, num_old;\n-    _free_set->prepare_to_rebuild(young_cset_regions, old_cset_regions, first_old, last_old, num_old);\n-\n-    if (heap->mode()->is_generational()) {\n-      ShenandoahGenerationalHeap* gen_heap = ShenandoahGenerationalHeap::heap();\n-      gen_heap->compute_old_generation_balance(young_cset_regions, old_cset_regions);\n-    }\n-\n-    \/\/ Free set construction uses reserve quantities, because they are known to be valid here\n-    _free_set->finish_rebuild(young_cset_regions, old_cset_regions, num_old, true);\n+    heap->free_set()->rebuild();\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahGeneration.cpp","additions":13,"deletions":27,"binary":false,"changes":40,"status":"modified"},{"patch":"@@ -147,0 +147,16 @@\n+  \/\/ Upon return from prepare_regions_and_collection_set(), certain parameters have been established to govern the\n+  \/\/ evacuation efforts that are about to begin.  In particular:\n+  \/\/\n+  \/\/ old_generation->get_promoted_reserve() represents the amount of memory within old-gen's available memory that has\n+  \/\/   been set aside to hold objects promoted from young-gen memory.  This represents an estimated percentage\n+  \/\/   of the live young-gen memory within the collection set.  If there is more data ready to be promoted than\n+  \/\/   can fit within this reserve, the promotion of some objects will be deferred until a subsequent evacuation\n+  \/\/   pass.\n+  \/\/\n+  \/\/ old_generation->get_evacuation_reserve() represents the amount of memory within old-gen's available memory that has been\n+  \/\/  set aside to hold objects evacuated from the old-gen collection set.\n+  \/\/\n+  \/\/ young_generation->get_evacuation_reserve() represents the amount of memory within young-gen's available memory that has\n+  \/\/  been set aside to hold objects evacuated from the young-gen collection set.  Conservatively, this value\n+  \/\/  equals the entire amount of live young-gen memory within the collection set, even though some of this memory\n+  \/\/  will likely be promoted.\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahGeneration.hpp","additions":16,"deletions":0,"binary":false,"changes":16,"status":"modified"},{"patch":"@@ -58,3 +58,0 @@\n-  \/\/ No need for old_gen->increase_used() as this was done when plabs were allocated.\n-  heap->reset_generation_reserves();\n-\n@@ -154,9 +151,0 @@\n-void ShenandoahGenerationalFullGC::compute_balances() {\n-  auto heap = ShenandoahGenerationalHeap::heap();\n-\n-  \/\/ In case this Full GC resulted from degeneration, clear the tally on anticipated promotion.\n-  heap->old_generation()->set_promotion_potential(0);\n-  \/\/ Invoke this in case we are able to transfer memory from OLD to YOUNG.\n-  heap->compute_old_generation_balance(0, 0);\n-}\n-\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahGenerationalFullGC.cpp","additions":0,"deletions":12,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -241,1 +241,1 @@\n-                                        ShenandoahAffiliation target_gen) {\n+                                                    ShenandoahAffiliation target_gen) {\n@@ -302,1 +302,7 @@\n-      if (!is_promotion || !has_plab || (size > PLAB::min_size())) {\n+\n+      \/\/ Reduce, but do not totally eliminate promotion by shared allocation.  Shared allocations are normally\n+      \/\/ not a good thing.  Usually is much better to evacuate into a young-gen GCLAB than promote to old-gen with a\n+      \/\/ shared allocation.  Objects above a particular threshold size (6 * min-size) are considered to be worth the\n+      \/\/ effort required to promote by shared allocation.\n+      static size_t size_threshhold = MIN2(PLAB::max_size(), PLAB::min_size() * 6);\n+      if (!is_promotion || !has_plab || (size > size_threshhold)) {\n@@ -308,1 +314,1 @@\n-      \/\/ We choose not to promote objects smaller than PLAB::min_size() by way of shared allocations, as this is too\n+      \/\/ We choose not to promote objects smaller than size_threshold by way of shared allocations as this is too\n@@ -310,1 +316,1 @@\n-      \/\/ evacuation pass.  This condition is denoted by: is_promotion && has_plab && (size <= PLAB::min_size())\n+      \/\/ evacuation pass.  This condition is denoted by: is_promotion && has_plab && (size <= size_threshhold).\n@@ -591,7 +597,5 @@\n-\/\/ xfer_limit, and any surplus is transferred to the young generation.\n-\/\/\n-\/\/ xfer_limit is the maximum we're able to transfer from young to old based on either:\n-\/\/  1. an assumption that we will be able to replenish memory \"borrowed\" from young at the end of collection, or\n-\/\/  2. there is sufficient excess in the allocation runway during GC idle cycles\n-void ShenandoahGenerationalHeap::compute_old_generation_balance(size_t old_xfer_limit, size_t old_cset_regions) {\n-\n+\/\/ mutator_xfer_limit, and any surplus is transferred to the young generation.  mutator_xfer_limit is\n+\/\/ the maximum we're able to transfer from young to old.\n+void ShenandoahGenerationalHeap::compute_old_generation_balance(size_t mutator_xfer_limit,\n+                                                                size_t old_cset_regions, size_t young_cset_regions) {\n+  shenandoah_assert_heaplocked();\n@@ -603,1 +607,1 @@\n-  \/\/ Let SOEP = ShenandoahOldEvacRatioPercent,\n+  \/\/ Let SOEP = ShenandoahOldEvacPercent,\n@@ -615,12 +619,1 @@\n-  assert(ShenandoahOldEvacRatioPercent <= 100, \"Error\");\n-  const size_t old_available = old_generation()->available();\n-  \/\/ The free set will reserve this amount of memory to hold young evacuations\n-  const size_t young_reserve = (young_generation()->max_capacity() * ShenandoahEvacReserve) \/ 100;\n-\n-  \/\/ In the case that ShenandoahOldEvacRatioPercent equals 100, max_old_reserve is limited only by xfer_limit.\n-\n-  const double bound_on_old_reserve = old_available + old_xfer_limit + young_reserve;\n-  const double max_old_reserve = ((ShenandoahOldEvacRatioPercent == 100)? bound_on_old_reserve:\n-                                  MIN2(double(young_reserve * ShenandoahOldEvacRatioPercent)\n-                                       \/ double(100 - ShenandoahOldEvacRatioPercent), bound_on_old_reserve));\n-\n+  assert(ShenandoahOldEvacPercent <= 100, \"Error\");\n@@ -629,0 +622,28 @@\n+  ShenandoahOldGeneration* old_gen = old_generation();\n+  size_t old_capacity = old_gen->max_capacity();\n+  size_t old_usage = old_gen->used(); \/\/ includes humongous waste\n+  size_t old_available = ((old_capacity >= old_usage)? old_capacity - old_usage: 0) + old_cset_regions * region_size_bytes;\n+\n+  ShenandoahYoungGeneration* young_gen = young_generation();\n+  size_t young_capacity = young_gen->max_capacity();\n+  size_t young_usage = young_gen->used(); \/\/ includes humongous waste\n+  size_t young_available = ((young_capacity >= young_usage)? young_capacity - young_usage: 0);\n+  size_t freeset_available = free_set()->available_locked();\n+  if (young_available > freeset_available) {\n+    young_available = freeset_available;\n+  }\n+  young_available += young_cset_regions * region_size_bytes;\n+\n+  \/\/ The free set will reserve this amount of memory to hold young evacuations (initialized to the ideal reserve)\n+  size_t young_reserve = (young_generation()->max_capacity() * ShenandoahEvacReserve) \/ 100;\n+\n+  \/\/ If ShenandoahOldEvacPercent equals 100, max_old_reserve is limited only by mutator_xfer_limit and young_reserve\n+  const size_t bound_on_old_reserve = ((old_available + mutator_xfer_limit + young_reserve) * ShenandoahOldEvacPercent) \/ 100;\n+  size_t proposed_max_old = ((ShenandoahOldEvacPercent == 100)?\n+                             bound_on_old_reserve:\n+                             MIN2((young_reserve * ShenandoahOldEvacPercent) \/ (100 - ShenandoahOldEvacPercent),\n+                                  bound_on_old_reserve));\n+  if (young_reserve > young_available) {\n+    young_reserve = young_available;\n+  }\n+\n@@ -630,2 +651,16 @@\n-  double reserve_for_mixed = 0;\n-  if (old_generation()->has_unprocessed_collection_candidates()) {\n+  size_t reserve_for_mixed = 0;\n+  const size_t old_fragmented_available =\n+    old_available - (old_generation()->free_unaffiliated_regions() + old_cset_regions) * region_size_bytes;\n+\n+  if (old_fragmented_available > proposed_max_old) {\n+    \/\/ After we've promoted regions in place, there may be an abundance of old-fragmented available memory,\n+    \/\/ even more than the desired percentage for old reserve.  We cannot transfer these fragmented regions back\n+    \/\/ to young.  Instead we make the best of the situation by using this fragmented memory for both promotions\n+    \/\/ and evacuations.\n+    proposed_max_old = old_fragmented_available;\n+  }\n+  size_t reserve_for_promo = old_fragmented_available;\n+  const size_t max_old_reserve = proposed_max_old;\n+  const size_t mixed_candidate_live_memory = old_generation()->unprocessed_collection_candidates_live_memory();\n+  const bool doing_mixed = (mixed_candidate_live_memory > 0);\n+  if (doing_mixed) {\n@@ -634,2 +669,1 @@\n-    const double max_evac_need =\n-      (double(old_generation()->unprocessed_collection_candidates_live_memory()) * ShenandoahOldEvacWaste);\n+    const size_t max_evac_need = (size_t) (mixed_candidate_live_memory * ShenandoahOldEvacWaste);\n@@ -638,5 +672,14 @@\n-    const double old_fragmented_available =\n-      double(old_available - old_generation()->free_unaffiliated_regions() * region_size_bytes);\n-    reserve_for_mixed = max_evac_need + old_fragmented_available;\n-    if (reserve_for_mixed > max_old_reserve) {\n-      reserve_for_mixed = max_old_reserve;\n+\n+    \/\/ We prefer to evacuate all of mixed into unfragmented memory, and will expand old in order to do so, unless\n+    \/\/ we already have too much fragmented available memory in old.\n+    reserve_for_mixed = max_evac_need;\n+    if (reserve_for_mixed + reserve_for_promo > max_old_reserve) {\n+      \/\/ In this case, we'll allow old-evac to target some of the fragmented old memory.\n+      size_t excess_reserves = (reserve_for_mixed + reserve_for_promo) - max_old_reserve;\n+      if (reserve_for_promo > excess_reserves) {\n+        reserve_for_promo -= excess_reserves;\n+      } else {\n+        excess_reserves -= reserve_for_promo;\n+        reserve_for_promo = 0;\n+        reserve_for_mixed -= excess_reserves;\n+      }\n@@ -646,2 +689,2 @@\n-  \/\/ Decide how much space we should reserve for promotions from young\n-  size_t reserve_for_promo = 0;\n+  \/\/ Decide how much additional space we should reserve for promotions from young.  We give priority to mixed evacations\n+  \/\/ over promotions.\n@@ -651,4 +694,15 @@\n-    \/\/ We're promoting and have a bound on the maximum amount that can be promoted\n-    assert(max_old_reserve >= reserve_for_mixed, \"Sanity\");\n-    const size_t available_for_promotions = max_old_reserve - reserve_for_mixed;\n-    reserve_for_promo = MIN2((size_t)(promo_load * ShenandoahPromoEvacWaste), available_for_promotions);\n+    \/\/ We've already set aside all of the fragmented available memory within old-gen to represent old objects\n+    \/\/ to be promoted from young generation.  promo_load represents the memory that we anticipate to be promoted\n+    \/\/ from regions that have reached tenure age.  In the ideal, we will always use fragmented old-gen memory\n+    \/\/ to hold individually promoted objects and will use unfragmented old-gen memory to represent the old-gen\n+    \/\/ evacuation workloa.\n+\n+    \/\/ We're promoting and have an esimate of memory to be promoted from aged regions\n+    assert(max_old_reserve >= (reserve_for_mixed + reserve_for_promo), \"Sanity\");\n+    const size_t available_for_additional_promotions = max_old_reserve - (reserve_for_mixed + reserve_for_promo);\n+    size_t promo_need = (size_t)(promo_load * ShenandoahPromoEvacWaste);\n+    if (promo_need > reserve_for_promo) {\n+      reserve_for_promo += MIN2(promo_need - reserve_for_promo, available_for_additional_promotions);\n+    }\n+    \/\/ We've already reserved all the memory required for the promo_load, and possibly more.  The excess\n+    \/\/ can be consumed by objects promoted from regions that have not yet reached tenure age.\n@@ -657,3 +711,2 @@\n-  \/\/ This is the total old we want to ideally reserve\n-  const size_t old_reserve = reserve_for_mixed + reserve_for_promo;\n-  assert(old_reserve <= max_old_reserve, \"cannot reserve more than max for old evacuations\");\n+  \/\/ This is the total old we want to reserve (initialized to the ideal reserve)\n+  size_t old_reserve = reserve_for_mixed + reserve_for_promo;\n@@ -662,2 +715,8 @@\n-  const size_t max_old_available = old_generation()->available() + old_cset_regions * region_size_bytes;\n-  if (max_old_available >= old_reserve) {\n+  size_t old_region_deficit = 0;\n+  size_t old_region_surplus = 0;\n+\n+  size_t mutator_region_xfer_limit = mutator_xfer_limit \/ region_size_bytes;\n+  \/\/ align the mutator_xfer_limit on region size\n+  mutator_xfer_limit = mutator_region_xfer_limit * region_size_bytes;\n+\n+  if (old_available >= old_reserve) {\n@@ -665,1 +724,2 @@\n-    const size_t old_surplus = (max_old_available - old_reserve) \/ region_size_bytes;\n+    const size_t old_surplus = old_available - old_reserve;\n+    old_region_surplus = old_surplus \/ region_size_bytes;\n@@ -667,1 +727,1 @@\n-    const size_t old_region_surplus = MIN2(old_surplus, unaffiliated_old_regions);\n+    old_region_surplus = MIN2(old_region_surplus, unaffiliated_old_regions);\n@@ -669,0 +729,5 @@\n+  } else if (old_available + mutator_xfer_limit >= old_reserve) {\n+    \/\/ Mutator's xfer limit is sufficient to satisfy our need: transfer all memory from there\n+    size_t old_deficit = old_reserve - old_available;\n+    old_region_deficit = (old_deficit + region_size_bytes - 1) \/ region_size_bytes;\n+    old_generation()->set_region_balance(0 - checked_cast<ssize_t>(old_region_deficit));\n@@ -670,11 +735,37 @@\n-    \/\/ We are running a deficit which we'd like to fill from young.\n-    \/\/ Ignore that this will directly impact young_generation()->max_capacity(),\n-    \/\/ indirectly impacting young_reserve and old_reserve.  These computations are conservative.\n-    \/\/ Note that deficit is rounded up by one region.\n-    const size_t old_need = (old_reserve - max_old_available + region_size_bytes - 1) \/ region_size_bytes;\n-    const size_t max_old_region_xfer = old_xfer_limit \/ region_size_bytes;\n-\n-    \/\/ Round down the regions we can transfer from young to old. If we're running short\n-    \/\/ on young-gen memory, we restrict the xfer. Old-gen collection activities will be\n-    \/\/ curtailed if the budget is restricted.\n-    const size_t old_region_deficit = MIN2(old_need, max_old_region_xfer);\n+   \/\/ We'll try to xfer from both mutator excess and from young collector reserve\n+    size_t available_reserves = old_available + young_reserve + mutator_xfer_limit;\n+    size_t old_entitlement = (available_reserves  * ShenandoahOldEvacPercent) \/ 100;\n+\n+    \/\/ Round old_entitlement down to nearest multiple of regions to be transferred to old\n+    size_t entitled_xfer = old_entitlement - old_available;\n+    entitled_xfer = region_size_bytes * (entitled_xfer \/ region_size_bytes);\n+    size_t unaffiliated_young_regions = young_generation()->free_unaffiliated_regions();\n+    size_t unaffiliated_young_memory = unaffiliated_young_regions * region_size_bytes;\n+    if (entitled_xfer > unaffiliated_young_memory) {\n+      entitled_xfer = unaffiliated_young_memory;\n+    }\n+    old_entitlement = old_available + entitled_xfer;\n+    if (old_entitlement < old_reserve) {\n+      \/\/ There's not enough memory to satisfy our desire.  Scale back our old-gen intentions.\n+      size_t budget_overrun = old_reserve - old_entitlement;;\n+      if (reserve_for_promo > budget_overrun) {\n+        reserve_for_promo -= budget_overrun;\n+        old_reserve -= budget_overrun;\n+      } else {\n+        budget_overrun -= reserve_for_promo;\n+        reserve_for_promo = 0;\n+        reserve_for_mixed = (reserve_for_mixed > budget_overrun)? reserve_for_mixed - budget_overrun: 0;\n+        old_reserve = reserve_for_promo + reserve_for_mixed;\n+      }\n+    }\n+\n+    \/\/ Because of adjustments above, old_reserve may be smaller now than it was when we tested the branch\n+    \/\/   condition above: \"(old_available + mutator_xfer_limit >= old_reserve)\n+    \/\/ Therefore, we do NOT know that: mutator_xfer_limit < old_reserve - old_available\n+\n+    size_t old_deficit = old_reserve - old_available;\n+    old_region_deficit = (old_deficit + region_size_bytes - 1) \/ region_size_bytes;\n+\n+    \/\/ Shrink young_reserve to account for loan to old reserve\n+    const size_t reserve_xfer_regions = old_region_deficit - mutator_region_xfer_limit;\n+    young_reserve -= reserve_xfer_regions * region_size_bytes;\n@@ -683,0 +774,10 @@\n+\n+  assert(old_region_deficit == 0 || old_region_surplus == 0, \"Only surplus or deficit, never both\");\n+  assert(young_reserve + reserve_for_mixed + reserve_for_promo <= old_available + young_available,\n+         \"Cannot reserve more memory than is available: %zu + %zu + %zu <= %zu + %zu\",\n+         young_reserve, reserve_for_mixed, reserve_for_promo, old_available, young_available);\n+\n+  \/\/ deficit\/surplus adjustments to generation sizes will precede rebuild\n+  young_generation()->set_evacuation_reserve(young_reserve);\n+  old_generation()->set_evacuation_reserve(reserve_for_mixed);\n+  old_generation()->set_promoted_reserve(reserve_for_promo);\n@@ -692,13 +793,0 @@\n-void ShenandoahGenerationalHeap::TransferResult::print_on(const char* when, outputStream* ss) const {\n-  auto heap = ShenandoahGenerationalHeap::heap();\n-  ShenandoahYoungGeneration* const young_gen = heap->young_generation();\n-  ShenandoahOldGeneration* const old_gen = heap->old_generation();\n-  const size_t young_available = young_gen->available();\n-  const size_t old_available = old_gen->available();\n-  ss->print_cr(\"After %s, %s %zu regions to %s to prepare for next gc, old available: \"\n-                     PROPERFMT \", young_available: \" PROPERFMT,\n-                     when,\n-                     success? \"successfully transferred\": \"failed to transfer\", region_count, region_destination,\n-                     PROPERFMTARGS(old_available), PROPERFMTARGS(young_available));\n-}\n-\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahGenerationalHeap.cpp","additions":158,"deletions":70,"binary":false,"changes":228,"status":"modified"},{"patch":"@@ -133,9 +133,0 @@\n-  \/\/ Used for logging the result of a region transfer outside the heap lock\n-  struct TransferResult {\n-    bool success;\n-    size_t region_count;\n-    const char* region_destination;\n-\n-    void print_on(const char* when, outputStream* ss) const;\n-  };\n-\n@@ -146,4 +137,1 @@\n-  void compute_old_generation_balance(size_t old_xfer_limit, size_t old_cset_regions);\n-\n-  \/\/ Transfers surplus old regions to young, or takes regions from young to satisfy old region deficit\n-  TransferResult balance_generations();\n+  void compute_old_generation_balance(size_t old_xfer_limit, size_t old_cset_regions, size_t young_cset_regions);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahGenerationalHeap.hpp","additions":1,"deletions":13,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -428,2 +428,0 @@\n-    _free_set = new ShenandoahFreeSet(this, _num_regions);\n-\n@@ -431,4 +429,0 @@\n-    post_initialize_heuristics();\n-    \/\/ We are initializing free set.  We ignore cset region tallies.\n-    size_t young_cset_regions, old_cset_regions, first_old, last_old, num_old;\n-    _free_set->prepare_to_rebuild(young_cset_regions, old_cset_regions, first_old, last_old, num_old);\n@@ -436,6 +430,4 @@\n-      ShenandoahGenerationalHeap* gen_heap = ShenandoahGenerationalHeap::heap();\n-      \/\/ We cannot call\n-      \/\/  gen_heap->young_generation()->heuristics()->bytes_of_allocation_runway_before_gc_trigger(young_cset_regions)\n-      \/\/ until after the heap is fully initialized.  So we make up a safe value here.\n-      size_t allocation_runway = InitialHeapSize \/ 2;\n-      gen_heap->compute_old_generation_balance(allocation_runway, old_cset_regions);\n+      size_t young_reserve = (soft_max_capacity() * ShenandoahEvacReserve) \/ 100;\n+      young_generation()->set_evacuation_reserve(young_reserve);\n+      old_generation()->set_evacuation_reserve((size_t) 0);\n+      old_generation()->set_promoted_reserve((size_t) 0);\n@@ -443,1 +435,4 @@\n-    _free_set->finish_rebuild(young_cset_regions, old_cset_regions, num_old);\n+\n+    _free_set = new ShenandoahFreeSet(this, _num_regions);\n+    post_initialize_heuristics();\n+    _free_set->rebuild();\n@@ -2518,4 +2513,1 @@\n-void ShenandoahHeap::rebuild_free_set(bool concurrent) {\n-  ShenandoahGCPhase phase(concurrent ?\n-                          ShenandoahPhaseTimings::final_update_refs_rebuild_freeset :\n-                          ShenandoahPhaseTimings::degen_gc_final_update_refs_rebuild_freeset);\n+void ShenandoahHeap::rebuild_free_set_within_phase() {\n@@ -2543,10 +2535,3 @@\n-    size_t allocation_runway = gen_heap->young_generation()->heuristics()->bytes_of_allocation_runway_before_gc_trigger(young_cset_regions);\n-    gen_heap->compute_old_generation_balance(allocation_runway, old_cset_regions);\n-\n-    \/\/ Total old_available may have been expanded to hold anticipated promotions.  We trigger if the fragmented available\n-    \/\/ memory represents more than 16 regions worth of data.  Note that fragmentation may increase when we promote regular\n-    \/\/ regions in place when many of these regular regions have an abundant amount of available memory within them.  Fragmentation\n-    \/\/ will decrease as promote-by-copy consumes the available memory within these partially consumed regions.\n-    \/\/\n-    \/\/ We consider old-gen to have excessive fragmentation if more than 12.5% of old-gen is free memory that resides\n-    \/\/ within partially consumed regions of memory.\n+    size_t allocation_runway =\n+      gen_heap->young_generation()->heuristics()->bytes_of_allocation_runway_before_gc_trigger(young_cset_regions);\n+    gen_heap->compute_old_generation_balance(allocation_runway, old_cset_regions, young_cset_regions);\n@@ -2564,0 +2549,7 @@\n+void ShenandoahHeap::rebuild_free_set(bool concurrent) {\n+  ShenandoahGCPhase phase(concurrent ?\n+                          ShenandoahPhaseTimings::final_update_refs_rebuild_freeset :\n+                          ShenandoahPhaseTimings::degen_gc_final_update_refs_rebuild_freeset);\n+  rebuild_free_set_within_phase();\n+}\n+\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeap.cpp","additions":19,"deletions":27,"binary":false,"changes":46,"status":"modified"},{"patch":"@@ -483,0 +483,1 @@\n+  void rebuild_free_set_within_phase();\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeap.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -596,0 +596,2 @@\n+    \/\/ Otherwise, the mutator might see region as fully recycled and might change its affiliation only to have\n+    \/\/ the racing GC worker thread overwrite its affiliation to FREE.\n@@ -606,0 +608,2 @@\n+\/\/ Note that return from try_recycle() does not mean the region has been recycled.  It only means that\n+\/\/ some GC worker thread has taken responsibility to recycle the region, eventually.\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeapRegion.cpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -131,2 +131,0 @@\n-  heap->free_set()->log_status_under_lock();\n-\n@@ -141,2 +139,5 @@\n-  size_t allocation_runway = heap->young_generation()->heuristics()->bytes_of_allocation_runway_before_gc_trigger(0);\n-  heap->compute_old_generation_balance(allocation_runway, 0);\n+  \/\/ After concurrent old marking finishes, we reclaim immediate garbage. Further, we may also want to expand OLD in order\n+  \/\/ to make room for anticipated promotions and\/or for mixed evacuations.  Mixed evacuations are especially likely to\n+  \/\/ follow the end of OLD marking.\n+  heap->rebuild_free_set_within_phase();\n+  heap->free_set()->log_status_under_lock();\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahOldGC.cpp","additions":5,"deletions":4,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -424,2 +424,1 @@\n-    gen_heap->compute_old_generation_balance(allocation_runway, old_trash_regions);\n-\n+    gen_heap->compute_old_generation_balance(allocation_runway, old_trash_regions, young_trash_regions);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahOldGeneration.cpp","additions":1,"deletions":2,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -69,2 +69,2 @@\n-  \/\/ Represents the quantity of live bytes we expect to promote during the next evacuation\n-  \/\/ cycle. This value is used by the young heuristic to trigger mixed collections.\n+  \/\/ Represents the quantity of live bytes we expect to promote during the next GC cycle, either by\n+  \/\/ evacuation or by promote-in-place.  This value is used by the young heuristic to trigger mixed collections.\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahOldGeneration.hpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -249,2 +249,1 @@\n-  \/\/ For HumongousRegion:s it's more efficient to jump directly to the\n-  \/\/ start region.\n+  \/\/ For humongous regions it's more efficient to jump directly to the start region.\n@@ -260,2 +259,0 @@\n-  HeapWord* p = nullptr;\n-  oop obj = cast_to_oop(p);\n@@ -277,1 +274,1 @@\n-  p = _rs->addr_for_card_index(cur_index) + offset;\n+  HeapWord* p = _rs->addr_for_card_index(cur_index) + offset;\n@@ -294,1 +291,1 @@\n-  NOT_PRODUCT(obj = cast_to_oop(p);)\n+  oop obj = cast_to_oop(p);\n@@ -299,0 +296,2 @@\n+    obj = cast_to_oop(p);\n+    assert(oopDesc::is_oop(obj), \"Should be an object\");\n@@ -301,1 +300,2 @@\n-  assert(p + obj->size() > left, \"obj should end after left\");\n+  assert(p < left, \"p should start before left end of card\");\n+  assert(p + obj->size() > left, \"obj should end after left end of card\");\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahScanRemembered.cpp","additions":7,"deletions":7,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -423,1 +423,8 @@\n-  size_t non_trashed_span() const { return (_regions - _trashed_regions) * ShenandoahHeapRegion::region_size_bytes(); }\n+  size_t non_trashed_span() const {\n+    assert(_regions >= _trashed_regions, \"sanity\");\n+    return (_regions - _trashed_regions) * ShenandoahHeapRegion::region_size_bytes();\n+  }\n+  size_t non_trashed_committed() const {\n+    assert(_committed >= _trashed_regions * ShenandoahHeapRegion::region_size_bytes(), \"sanity\");\n+    return _committed - (_trashed_regions * ShenandoahHeapRegion::region_size_bytes());\n+  }\n@@ -461,0 +468,14 @@\n+#ifdef KELVIN_DEPRECATE_FROM_HEAD\n+    \/\/ this code is replaced with the following three lines from MASTER\n+    size_t generation_max_capacity = generation->max_capacity();\n+    if (adjust_for_padding && (generation->is_young() || generation->is_global())) {\n+      size_t pad = heap->old_generation()->get_pad_for_promote_in_place();\n+      generation_used += pad;\n+    }\n+\n+    guarantee(stats.non_trashed_committed() <= generation_max_capacity,\n+              \"%s: generation (%s) non_trashed_committed: \" PROPERFMT \" must not exceed generation capacity: \" PROPERFMT,\n+              label, generation->name(), PROPERFMTARGS(stats.non_trashed_committed()), PROPERFMTARGS(generation_max_capacity));\n+\n+    guarantee(stats.used() == generation_used,\n+#endif\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahVerifier.cpp","additions":22,"deletions":1,"binary":false,"changes":23,"status":"modified"},{"patch":"@@ -388,21 +388,14 @@\n-  product(uintx, ShenandoahOldEvacRatioPercent, 75, EXPERIMENTAL,           \\\n-          \"The maximum proportion of evacuation from old-gen memory, \"      \\\n-          \"expressed as a percentage. The default value 75 denotes that \"   \\\n-          \"no more than 75% of the collection set evacuation workload may \" \\\n-          \"be towards evacuation of old-gen heap regions. This limits both \"\\\n-          \"the promotion of aged regions and the compaction of existing \"   \\\n-          \"old regions. A value of 75 denotes that the total evacuation \"   \\\n-          \"work may increase to up to four times the young gen evacuation \" \\\n-          \"work. A larger value allows quicker promotion and allows \"       \\\n-          \"a smaller number of mixed evacuations to process \"               \\\n-          \"the entire list of old-gen collection candidates at the cost \"   \\\n-          \"of an increased disruption of the normal cadence of young-gen \"  \\\n-          \"collections.  A value of 100 allows a mixed evacuation to \"      \\\n-          \"focus entirely on old-gen memory, allowing no young-gen \"        \\\n-          \"regions to be collected, likely resulting in subsequent \"        \\\n-          \"allocation failures because the allocation pool is not \"         \\\n-          \"replenished.  A value of 0 allows a mixed evacuation to \"        \\\n-          \"focus entirely on young-gen memory, allowing no old-gen \"        \\\n-          \"regions to be collected, likely resulting in subsequent \"        \\\n-          \"promotion failures and triggering of stop-the-world full GC \"    \\\n-          \"events.\")                                                        \\\n+  product(uintx, ShenandoahOldEvacPercent, 75, EXPERIMENTAL,                \\\n+          \"The maximum evacuation to old-gen expressed as a percent of \"    \\\n+          \"the total live memory within the collection set.  With the \"     \\\n+          \"default setting, if collection set evacuates X, no more than \"   \\\n+          \"75% of X may hold objects evacuated from old or promoted to \"    \\\n+          \"old from young.  A value of 100 allows the entire collection \"   \\\n+          \"set to be comprised of old-gen regions and young regions that \"  \\\n+          \"have reached the tenure age.  Larger values allow fewer mixed \"  \\\n+          \"evacuations to reclaim all the garbage from old.  Smaller \"      \\\n+          \"values result in less variation in GC cycle times between \"      \\\n+          \"young vs. mixed cycles.  A value of 0 prevents mixed \"           \\\n+          \"evacations from running and blocks promotion of aged regions \"   \\\n+          \"by evacuation.  Setting the value to 0 does not prevent \"        \\\n+          \"regions from being promoted in place.\")                          \\\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoah_globals.hpp","additions":14,"deletions":21,"binary":false,"changes":35,"status":"modified"},{"patch":"@@ -204,1 +204,3 @@\n-  _heuristics->prime_collection_set(_collection_set);\n+  if (_heuristics->prime_collection_set(_collection_set)) {\n+    _heuristics->finalize_mixed_evacs();\n+  }\n@@ -217,1 +219,3 @@\n-  _heuristics->prime_collection_set(_collection_set);\n+  if (_heuristics->prime_collection_set(_collection_set)) {\n+    _heuristics->finalize_mixed_evacs();\n+  }\n@@ -229,1 +233,3 @@\n-  _heuristics->prime_collection_set(_collection_set);\n+  if (_heuristics->prime_collection_set(_collection_set)) {\n+    _heuristics->finalize_mixed_evacs();\n+  }\n@@ -251,1 +257,3 @@\n-  _heuristics->prime_collection_set(_collection_set);\n+  if (_heuristics->prime_collection_set(_collection_set)) {\n+    _heuristics->finalize_mixed_evacs();\n+  }\n@@ -264,1 +272,3 @@\n-  _heuristics->prime_collection_set(_collection_set);\n+  if (_heuristics->prime_collection_set(_collection_set)) {\n+    _heuristics->finalize_mixed_evacs();\n+  }\n@@ -281,1 +291,3 @@\n-  _heuristics->prime_collection_set(_collection_set);\n+  if (_heuristics->prime_collection_set(_collection_set)) {\n+    _heuristics->finalize_mixed_evacs();\n+  }\n@@ -288,1 +300,3 @@\n-  _heuristics->prime_collection_set(_collection_set);\n+  if (_heuristics->prime_collection_set(_collection_set)) {\n+    _heuristics->finalize_mixed_evacs();\n+  }\n@@ -304,1 +318,3 @@\n-  _heuristics->prime_collection_set(_collection_set);\n+  if (_heuristics->prime_collection_set(_collection_set)) {\n+    _heuristics->finalize_mixed_evacs();\n+  }\n@@ -312,1 +328,3 @@\n-  _heuristics->prime_collection_set(_collection_set);\n+  if (_heuristics->prime_collection_set(_collection_set)) {\n+    _heuristics->finalize_mixed_evacs();\n+  }\n@@ -330,1 +348,3 @@\n-  _heuristics->prime_collection_set(_collection_set);\n+  if (_heuristics->prime_collection_set(_collection_set)) {\n+    _heuristics->finalize_mixed_evacs();\n+  }\n@@ -339,1 +359,3 @@\n-  _heuristics->prime_collection_set(_collection_set);\n+  if (_heuristics->prime_collection_set(_collection_set)) {\n+    _heuristics->finalize_mixed_evacs();\n+  }\n@@ -357,1 +379,3 @@\n-  _heuristics->prime_collection_set(_collection_set);\n+  if (_heuristics->prime_collection_set(_collection_set)) {\n+    _heuristics->finalize_mixed_evacs();\n+  }\n","filename":"test\/hotspot\/gtest\/gc\/shenandoah\/test_shenandoahOldHeuristic.cpp","additions":36,"deletions":12,"binary":false,"changes":48,"status":"modified"}]}