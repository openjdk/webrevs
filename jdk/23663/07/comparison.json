{"files":[{"patch":"@@ -61,20 +61,0 @@\n-\n-\/\/ convenience methods for splitting 8-way vector register sequences\n-\/\/ in half -- needed because vector operations can normally only be\n-\/\/ benefit from 4-way instruction parallelism\n-\n-VSeq<4> vs_front(const VSeq<8>& v) {\n-  return VSeq<4>(v.base(), v.delta());\n-}\n-\n-VSeq<4> vs_back(const VSeq<8>& v) {\n-  return VSeq<4>(v.base() + 4 * v.delta(), v.delta());\n-}\n-\n-VSeq<4> vs_even(const VSeq<8>& v) {\n-  return VSeq<4>(v.base(), v.delta() * 2);\n-}\n-\n-VSeq<4> vs_odd(const VSeq<8>& v) {\n-  return VSeq<4>(v.base() + 1, v.delta() * 2);\n-}\n","filename":"src\/hotspot\/cpu\/aarch64\/register_aarch64.cpp","additions":0,"deletions":20,"binary":false,"changes":20,"status":"modified"},{"patch":"@@ -439,0 +439,3 @@\n+\/\/ helper macro for computing register masks\n+#define VS_MASK_BIT(base, delta, i) (1 << (base + delta * i))\n+\n@@ -441,2 +444,0 @@\n-  static_assert(N <= 8, \"vector sequence length must not exceed 8\");\n-  static_assert((N & (N - 1)) == 0, \"vector sequence length must be power of two\");\n@@ -449,3 +450,3 @@\n-    assert (_base >= 0, \"invalid base register\");\n-    assert (_delta >= 0, \"invalid register delta\");\n-    assert ((_base + (N - 1) * _delta) < 32, \"range exceeded\");\n+    assert (_base >= 0 && _base <= 31, \"invalid base register\");\n+    assert ((_base + (N - 1) * _delta) >= 0, \"register range underflow\");\n+    assert ((_base + (N - 1) * _delta) < 32, \"register range overflow\");\n@@ -460,1 +461,0 @@\n-    int bit = 1 << _base;\n@@ -462,1 +462,1 @@\n-      m |= bit << (i * _delta);\n+      m |= VS_MASK_BIT(_base, _delta, i);\n@@ -468,0 +468,1 @@\n+  bool is_constant() const { return _delta == 0; }\n@@ -470,8 +471,1 @@\n-\/\/ declare convenience methods for splitting vector register sequences\n-\n-VSeq<4> vs_front(const VSeq<8>& v);\n-VSeq<4> vs_back(const VSeq<8>& v);\n-VSeq<4> vs_even(const VSeq<8>& v);\n-VSeq<4> vs_odd(const VSeq<8>& v);\n-\n-\/\/ methods for use in asserts to check VSeq inputs and oupts are\n+\/\/ methods for use in asserts to check VSeq inputs and outputs are\n@@ -483,0 +477,69 @@\n+\/\/ method for use in asserts to check whether registers appearing in\n+\/\/ an output sequence will be written before they are read from an\n+\/\/ input sequence.\n+\n+template<int N> bool vs_write_before_read(const VSeq<N>& vout, const VSeq<N>& vin) {\n+  int b_in = vin.base();\n+  int d_in = vin.delta();\n+  int b_out = vout.base();\n+  int d_out = vout.delta();\n+  int bit_in = 1 << b_in;\n+  int bit_out = 1 << b_out;\n+  int mask_read = vin.mask();   \/\/ all pending reads\n+  int mask_write = 0;         \/\/ no writes as yet\n+\n+\n+  for (int i = 0; i < N - 1; i++) {\n+    \/\/ check whether a pending read clashes with a write\n+    if ((mask_write & mask_read) != 0) {\n+      return true;\n+    }\n+    \/\/ remove the pending input (so long as this is a constant\n+    \/\/ sequence)\n+    if (d_in != 0) {\n+      mask_read ^= VS_MASK_BIT(b_in, d_in, i);\n+    }\n+    \/\/ record the next write\n+    mask_write |= VS_MASK_BIT(b_out, d_out, i);\n+  }\n+  \/\/ no write before read\n+  return false;\n+}\n+\n+\/\/ convenience methods for splitting 8-way or 4-way vector register\n+\/\/ sequences in half -- needed because vector operations can normally\n+\/\/ benefit from 4-way instruction parallelism or, occasionally, 2-way\n+\/\/ parallelism\n+\n+template<int N>\n+VSeq<N\/2> vs_front(const VSeq<N>& v) {\n+  static_assert(N > 0 && ((N & 1) == 0), \"sequence length must be even\");\n+  return VSeq<N\/2>(v.base(), v.delta());\n+}\n+\n+template<int N>\n+VSeq<N\/2> vs_back(const VSeq<N>& v) {\n+  static_assert(N > 0 && ((N & 1) == 0), \"sequence length must be even\");\n+  return VSeq<N\/2>(v.base() + N \/ 2 * v.delta(), v.delta());\n+}\n+\n+template<int N>\n+VSeq<N\/2> vs_even(const VSeq<N>& v) {\n+  static_assert(N > 0 && ((N & 1) == 0), \"sequence length must be even\");\n+  return VSeq<N\/2>(v.base(), v.delta() * 2);\n+}\n+\n+template<int N>\n+VSeq<N\/2> vs_odd(const VSeq<N>& v) {\n+  static_assert(N > 0 && ((N & 1) == 0), \"sequence length must be even\");\n+  return VSeq<N\/2>(v.base() + v.delta(), v.delta() * 2);\n+}\n+\n+\/\/ convenience method to construct a vector register sequence that\n+\/\/ indexes its elements in reverse order to the original\n+\n+template<int N>\n+VSeq<N> vs_reverse(const VSeq<N>& v) {\n+  return VSeq<N>(v.base() + (N - 1) * v.delta(), -v.delta());\n+}\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/register_aarch64.hpp","additions":78,"deletions":15,"binary":false,"changes":93,"status":"modified"},{"patch":"@@ -47,1 +47,1 @@\n-  do_arch_blob(compiler, 55000 ZGC_ONLY(+5000))                         \\\n+  do_arch_blob(compiler, 75000 ZGC_ONLY(+5000))                         \\\n","filename":"src\/hotspot\/cpu\/aarch64\/stubDeclarations_aarch64.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -4654,0 +4654,5 @@\n+    \/\/ output must not be constant\n+    assert(N == 1  || !v.is_constant(), \"cannot output multiple values to a constant vector\");\n+    \/\/ output cannot overwrite pending inputs\n+    assert(!vs_write_before_read(v, v1), \"output overwrites input\");\n+    assert(!vs_write_before_read(v, v2), \"output overwrites input\");\n@@ -4662,0 +4667,5 @@\n+    \/\/ output must not be constant\n+    assert(N == 1  || !v.is_constant(), \"cannot output multiple values to a constant vector\");\n+    \/\/ output cannot overwrite pending inputs\n+    assert(!vs_write_before_read(v, v1), \"output overwrites input\");\n+    assert(!vs_write_before_read(v, v2), \"output overwrites input\");\n@@ -4670,0 +4680,5 @@\n+    \/\/ output must not be constant\n+    assert(N == 1  || !v.is_constant(), \"cannot output multiple values to a constant vector\");\n+    \/\/ output cannot overwrite pending inputs\n+    assert(!vs_write_before_read(v, v1), \"output overwrites input\");\n+    assert(!vs_write_before_read(v, v2), \"output overwrites input\");\n@@ -4677,0 +4692,4 @@\n+    \/\/ output must not be constant\n+    assert(N == 1  || !v.is_constant(), \"cannot output multiple values to a constant vector\");\n+    \/\/ output cannot overwrite pending inputs\n+    assert(!vs_write_before_read(v, v1), \"output overwrites input\");\n@@ -4685,0 +4704,4 @@\n+    \/\/ output must not be constant\n+    assert(N == 1  || !v.is_constant(), \"cannot output multiple values to a constant vector\");\n+    \/\/ output cannot overwrite pending inputs\n+    assert(!vs_write_before_read(v, v1), \"output overwrites input\");\n@@ -4692,0 +4715,5 @@\n+    \/\/ output must not be constant\n+    assert(N == 1  || !v.is_constant(), \"cannot output multiple values to a constant vector\");\n+    \/\/ output cannot overwrite pending inputs\n+    assert(!vs_write_before_read(v, v1), \"output overwrites input\");\n+    assert(!vs_write_before_read(v, v2), \"output overwrites input\");\n@@ -4699,0 +4727,5 @@\n+    \/\/ output must not be constant\n+    assert(N == 1  || !v.is_constant(), \"cannot output multiple values to a constant vector\");\n+    \/\/ output cannot overwrite pending inputs\n+    assert(!vs_write_before_read(v, v1), \"output overwrites input\");\n+    assert(!vs_write_before_read(v, v2), \"output overwrites input\");\n@@ -4705,1 +4738,5 @@\n-    void vs_notr(const VSeq<N>& v, const VSeq<N>& v1) {\n+  void vs_notr(const VSeq<N>& v, const VSeq<N>& v1) {\n+    \/\/ output must not be constant\n+    assert(N == 1  || !v.is_constant(), \"cannot output multiple values to a constant vector\");\n+    \/\/ output cannot overwrite pending inputs\n+    assert(!vs_write_before_read(v, v1), \"output overwrites input\");\n@@ -4711,0 +4748,24 @@\n+  template<int N>\n+  void vs_sqdmulh(const VSeq<N>& v, Assembler::SIMD_Arrangement T, const VSeq<N>& v1, const VSeq<N>& v2) {\n+    \/\/ output must not be constant\n+    assert(N == 1  || !v.is_constant(), \"cannot output multiple values to a constant vector\");\n+    \/\/ output cannot overwrite pending inputs\n+    assert(!vs_write_before_read(v, v1), \"output overwrites input\");\n+    assert(!vs_write_before_read(v, v2), \"output overwrites input\");\n+    for (int i = 0; i < N; i++) {\n+      __ sqdmulh(v[i], T, v1[i], v2[i]);\n+    }\n+  }\n+\n+  template<int N>\n+  void vs_mlsv(const VSeq<N>& v, Assembler::SIMD_Arrangement T, const VSeq<N>& v1, VSeq<N>& v2) {\n+    \/\/ output must not be constant\n+    assert(N == 1  || !v.is_constant(), \"cannot output multiple values to a constant vector\");\n+    \/\/ output cannot overwrite pending inputs\n+    assert(!vs_write_before_read(v, v1), \"output overwrites input\");\n+    assert(!vs_write_before_read(v, v2), \"output overwrites input\");\n+    for (int i = 0; i < N; i++) {\n+      __ mlsv(v[i], T, v1[i], v2[i]);\n+    }\n+  }\n+\n@@ -4726,0 +4787,1 @@\n+    static_assert((N & (N - 1)) == 0, \"sequence length must be even\");\n@@ -4736,0 +4798,1 @@\n+    static_assert((N & (N - 1)) == 0, \"sequence length must be even\");\n@@ -4741,0 +4804,43 @@\n+  \/\/ load N\/2 pairs of quadword values from memory de-interleaved into\n+  \/\/ N vector registers 2 at a time via the address supplied in base\n+  \/\/ using post-increment addressing.\n+  template<int N>\n+  void vs_ld2_post(const VSeq<N>& v, Assembler::SIMD_Arrangement T, Register base) {\n+    static_assert((N & (N - 1)) == 0, \"sequence length must be even\");\n+    for (int i = 0; i < N; i += 2) {\n+      __ ld2(v[i], v[i+1], T, __ post(base, 32));\n+    }\n+  }\n+\n+  \/\/ store N vector registers interleaved into N\/2 pairs of quadword\n+  \/\/ memory locations via the address supplied in base using\n+  \/\/ post-increment addressing.\n+  template<int N>\n+  void vs_st2_post(const VSeq<N>& v, Assembler::SIMD_Arrangement T, Register base) {\n+    static_assert((N & (N - 1)) == 0, \"sequence length must be even\");\n+    for (int i = 0; i < N; i += 2) {\n+      __ st2(v[i], v[i+1], T, __ post(base, 32));\n+    }\n+  }\n+\n+  \/\/ load N quadword values from memory de-interleaved into N vector\n+  \/\/ registers 3 elements at a time via the address supplied in base.\n+  template<int N>\n+  void vs_ld3(const VSeq<N>& v, Assembler::SIMD_Arrangement T, Register base) {\n+    static_assert(N == ((N \/ 3) * 3), \"sequence length must be multiple of 3\");\n+    for (int i = 0; i < N; i += 3) {\n+      __ ld3(v[i], v[i+1], v[i+2], T, base);\n+    }\n+  }\n+\n+  \/\/ load N quadword values from memory de-interleaved into N vector\n+  \/\/ registers 3 elements at a time via the address supplied in base\n+  \/\/ using post-increment addressing.\n+  template<int N>\n+  void vs_ld3_post(const VSeq<N>& v, Assembler::SIMD_Arrangement T, Register base) {\n+    static_assert(N == ((N \/ 3) * 3), \"sequence length must be multiple of 3\");\n+    for (int i = 0; i < N; i += 3) {\n+      __ ld3(v[i], v[i+1], v[i+2], T, __ post(base, 48));\n+    }\n+  }\n+\n@@ -4813,2 +4919,1 @@\n-  \/\/ Helper routines for various flavours of dilithium montgomery\n-  \/\/ multiply\n+  \/\/ Helper routines for various flavours of Montgomery multiply\n@@ -4816,2 +4921,6 @@\n-  \/\/ Perform 16 32-bit Montgomery multiplications in parallel\n-  \/\/ See the montMul() method of the sun.security.provider.ML_DSA class.\n+  \/\/ Perform 16 32-bit (4x4S) or 32 16-bit (4 x 8H) Montgomery\n+  \/\/ multiplications in parallel\n+  \/\/\n+\n+  \/\/ See the montMul() method of the sun.security.provider.ML_DSA\n+  \/\/ class.\n@@ -4819,6 +4928,6 @@\n-  \/\/ Computes 4x4S results\n-  \/\/    a = b * c * 2^-32 mod MONT_Q\n-  \/\/ Inputs:  vb, vc - 4x4S vector register sequences\n-  \/\/          vq - 2x4S constants <MONT_Q, MONT_Q_INV_MOD_R>\n-  \/\/ Temps:   vtmp - 4x4S vector sequence trashed after call\n-  \/\/ Outputs: va - 4x4S vector register sequences\n+  \/\/ Computes 4x4S results or 8x8H results\n+  \/\/    a = b * c * 2^MONT_R_BITS mod MONT_Q\n+  \/\/ Inputs:  vb, vc - 4x4S or 4x8H vector register sequences\n+  \/\/          vq - 2x4S or 2x8H constants <MONT_Q, MONT_Q_INV_MOD_R>\n+  \/\/ Temps:   vtmp - 4x4S or 4x8H vector sequence trashed after call\n+  \/\/ Outputs: va - 4x4S or 4x8H vector register sequences\n@@ -4827,3 +4936,6 @@\n-  \/\/ n.b. MONT_R_BITS is 32, so the right shift by it is implicit.\n-  void dilithium_montmul16(const VSeq<4>& va, const VSeq<4>& vb, const VSeq<4>& vc,\n-                    const VSeq<4>& vtmp, const VSeq<2>& vq) {\n+  \/\/ va must have a non-zero delta i.e. it must not be a constant vseq.\n+  \/\/ n.b. MONT_R_BITS is 16 or 32, so the right shift by it is implicit.\n+  void vs_montmul4(const VSeq<4>& va, const VSeq<4>& vb, const VSeq<4>& vc,\n+                   Assembler::SIMD_Arrangement T,\n+                   const VSeq<4>& vtmp, const VSeq<2>& vq) {\n+    assert (T == __ T4S || T == __ T8H, \"invalid arrangement for montmul\");\n@@ -4843,0 +4955,1 @@\n+    assert(!va.is_constant(), \"output vector must identify 4 different registers\");\n@@ -4846,2 +4959,2 @@\n-      __ sqdmulh(vtmp[i], __ T4S, vb[i], vc[i]); \/\/ aHigh = hi32(2 * b * c)\n-      __ mulv(va[i], __ T4S, vb[i], vc[i]);    \/\/ aLow = lo32(b * c)\n+      __ sqdmulh(vtmp[i], T, vb[i], vc[i]); \/\/ aHigh = hi32(2 * b * c)\n+      __ mulv(va[i], T, vb[i], vc[i]);    \/\/ aLow = lo32(b * c)\n@@ -4851,1 +4964,1 @@\n-      __ mulv(va[i], __ T4S, va[i], vq[0]);     \/\/ m = aLow * qinv\n+      __ mulv(va[i], T, va[i], vq[0]);     \/\/ m = aLow * qinv\n@@ -4855,1 +4968,1 @@\n-      __ sqdmulh(va[i], __ T4S, va[i], vq[1]);  \/\/ n = hi32(2 * m * q)\n+      __ sqdmulh(va[i], T, va[i], vq[1]);  \/\/ n = hi32(2 * m * q)\n@@ -4859,1 +4972,1 @@\n-      __ shsubv(va[i], __ T4S, vtmp[i], va[i]);   \/\/ a = (aHigh - n) \/ 2\n+      __ shsubv(va[i], T, vtmp[i], va[i]);   \/\/ a = (aHigh - n) \/ 2\n@@ -4863,2 +4976,2 @@\n-  \/\/ Perform 2x16 32-bit Montgomery multiplications in parallel\n-  \/\/ See the montMul() method of the sun.security.provider.ML_DSA class.\n+  \/\/ Perform 8 32-bit (4x4S) or 16 16-bit (2 x 8H) Montgomery\n+  \/\/ multiplications in parallel\n@@ -4866,6 +4979,10 @@\n-  \/\/ Computes 8x4S results\n-  \/\/    a = b * c * 2^-32 mod MONT_Q\n-  \/\/ Inputs:  vb, vc - 8x4S vector register sequences\n-  \/\/          vq - 2x4S constants <MONT_Q, MONT_Q_INV_MOD_R>\n-  \/\/ Temps:   vtmp - 4x4S vector sequence trashed after call\n-  \/\/ Outputs: va - 8x4S vector register sequences\n+\n+  \/\/ See the montMul() method of the sun.security.provider.ML_DSA\n+  \/\/ class.\n+  \/\/\n+  \/\/ Computes 4x4S results or 8x8H results\n+  \/\/    a = b * c * 2^MONT_R_BITS mod MONT_Q\n+  \/\/ Inputs:  vb, vc - 4x4S or 4x8H vector register sequences\n+  \/\/          vq - 2x4S or 2x8H constants <MONT_Q, MONT_Q_INV_MOD_R>\n+  \/\/ Temps:   vtmp - 4x4S or 4x8H vector sequence trashed after call\n+  \/\/ Outputs: va - 4x4S or 4x8H vector register sequences\n@@ -4874,3 +4991,65 @@\n-  \/\/ n.b. MONT_R_BITS is 32, so the right shift by it is implicit.\n-  void vs_montmul32(const VSeq<8>& va, const VSeq<8>& vb, const VSeq<8>& vc,\n-                    const VSeq<4>& vtmp, const VSeq<2>& vq) {\n+  \/\/ va must have a non-zero delta i.e. it must not be a constant vseq.\n+  \/\/ n.b. MONT_R_BITS is 16 or 32, so the right shift by it is implicit.\n+  void vs_montmul2(const VSeq<2>& va, const VSeq<2>& vb, const VSeq<2>& vc,\n+                   Assembler::SIMD_Arrangement T,\n+                   const VSeq<2>& vtmp, const VSeq<2>& vq) {\n+    assert (T == __ T4S || T == __ T8H, \"invalid arrangement for montmul\");\n+    assert(vs_disjoint(vb, vc), \"vb and vc overlap\");\n+    assert(vs_disjoint(vb, vq), \"vb and vq overlap\");\n+    assert(vs_disjoint(vb, vtmp), \"vb and vtmp overlap\");\n+\n+    assert(vs_disjoint(vc, vq), \"vc and vq overlap\");\n+    assert(vs_disjoint(vc, vtmp), \"vc and vtmp overlap\");\n+\n+    assert(vs_disjoint(vq, vtmp), \"vq and vtmp overlap\");\n+\n+    assert(vs_disjoint(va, vc) || vs_same(va, vc), \"va and vc neither disjoint nor equal\");\n+    assert(vs_disjoint(va, vb), \"va and vb overlap\");\n+    assert(vs_disjoint(va, vq), \"va and vq overlap\");\n+    assert(vs_disjoint(va, vtmp), \"va and vtmp overlap\");\n+    assert(!va.is_constant(), \"output vector must identify 2 different registers\");\n+\n+    \/\/ schedule 2 streams of instructions across the vector sequences\n+    for (int i = 0; i < 2; i++) {\n+      __ sqdmulh(vtmp[i], T, vb[i], vc[i]); \/\/ aHigh = hi32(2 * b * c)\n+      __ mulv(va[i], T, vb[i], vc[i]);    \/\/ aLow = lo32(b * c)\n+    }\n+\n+    for (int i = 0; i < 2; i++) {\n+      __ mulv(va[i], T, va[i], vq[0]);     \/\/ m = aLow * qinv\n+    }\n+\n+    for (int i = 0; i < 2; i++) {\n+      __ sqdmulh(va[i], T, va[i], vq[1]);  \/\/ n = hi32(2 * m * q)\n+    }\n+\n+    for (int i = 0; i < 2; i++) {\n+      __ shsubv(va[i], T, vtmp[i], va[i]);   \/\/ a = (aHigh - n) \/ 2\n+    }\n+  }\n+\n+  \/\/ Perform 16 16-bit Montgomery multiplications in parallel.\n+  void kyber_montmul16(const VSeq<2>& va, const VSeq<2>& vb, const VSeq<2>& vc,\n+                       const VSeq<2>& vtmp, const VSeq<2>& vq) {\n+    \/\/ Use the helper routine to schedule a 2x8H Montgomery multiply.\n+    \/\/ It will assert that the register use is valid\n+    vs_montmul2(va, vb, vc, __ T8H, vtmp, vq);\n+  }\n+\n+  \/\/ Perform 32 16-bit Montgomery multiplications in parallel.\n+  void kyber_montmul32(const VSeq<4>& va, const VSeq<4>& vb, const VSeq<4>& vc,\n+                       const VSeq<4>& vtmp, const VSeq<2>& vq) {\n+    \/\/ Use the helper routine to schedule a 4x8H Montgomery multiply.\n+    \/\/ It will assert that the register use is valid\n+    vs_montmul4(va, vb, vc, __ T8H, vtmp, vq);\n+  }\n+\n+  \/\/ Perform 64 16-bit Montgomery multiplications in parallel.\n+  void kyber_montmul64(const VSeq<8>& va, const VSeq<8>& vb, const VSeq<8>& vc,\n+                       const VSeq<4>& vtmp, const VSeq<2>& vq) {\n+    \/\/ Schedule two successive 4x8H multiplies via the montmul helper\n+    \/\/ on the front and back halves of va, vb and vc. The helper will\n+    \/\/ assert that the register use has no overlap conflicts on each\n+    \/\/ individual call but we also need to ensure that the necessary\n+    \/\/ disjoint\/equality constraints are met across both calls.\n+\n@@ -4894,2 +5073,2 @@\n-    \/\/ we need to multiply the front and back halves of each sequence\n-    \/\/ 4x4S at a time because\n+    \/\/ we multiply the front and back halves of each sequence 4 at a\n+    \/\/ time because\n@@ -4904,2 +5083,2 @@\n-    dilithium_montmul16(vs_front(va), vs_front(vb), vs_front(vc), vtmp, vq);\n-    dilithium_montmul16(vs_back(va), vs_back(vb), vs_back(vc), vtmp, vq);\n+    vs_montmul4(vs_front(va), vs_front(vb), vs_front(vc), __ T8H, vtmp, vq);\n+    vs_montmul4(vs_back(va), vs_back(vb), vs_back(vc), __ T8H, vtmp, vq);\n@@ -4908,1 +5087,11 @@\n-  \/\/ perform combined montmul then add\/sub on 4x4S vectors\n+  void kyber_montmul32_sub_add(const VSeq<4>& va0, const VSeq<4>& va1,\n+                               const VSeq<4>& vc,\n+                               const VSeq<4>& vtmp,\n+                               const VSeq<2>& vq) {\n+    \/\/ compute a = montmul(a1, c)\n+    kyber_montmul32(vc, va1, vc, vtmp, vq);\n+    \/\/ ouptut a1 = a0 - a\n+    vs_subv(va1, __ T8H, va0, vc);\n+    \/\/    and a0 = a0 + a\n+    vs_addv(va0, __ T8H, va0, vc);\n+  }\n@@ -4910,2 +5099,1185 @@\n-  void dilithium_montmul16_sub_add(const VSeq<4>& va0, const VSeq<4>& va1, const VSeq<4>& vc,\n-                                   const VSeq<4>& vtmp, const VSeq<2>& vq) {\n+  void kyber_sub_add_montmul32(const VSeq<4>& va0, const VSeq<4>& va1,\n+                               const VSeq<4>& vb,\n+                               const VSeq<4>& vtmp1,\n+                               const VSeq<4>& vtmp2,\n+                               const VSeq<2>& vq) {\n+    \/\/ compute c = a0 - a1\n+    vs_subv(vtmp1, __ T8H, va0, va1);\n+    \/\/ output a0 = a0 + a1\n+    vs_addv(va0, __ T8H, va0, va1);\n+    \/\/ output a1 = b montmul c\n+    kyber_montmul32(va1, vtmp1, vb, vtmp2, vq);\n+  }\n+\n+  void load64shorts(const VSeq<8>& v, Register shorts) {\n+    vs_ldpq_post(v, shorts);\n+  }\n+\n+  void load32shorts(const VSeq<4>& v, Register shorts) {\n+    vs_ldpq_post(v, shorts);\n+  }\n+\n+  void store64shorts(VSeq<8> v, Register tmpAddr) {\n+    vs_stpq_post(v, tmpAddr);\n+  }\n+\n+  \/\/ Kyber NTT function.\n+  \/\/ Implements\n+  \/\/ static int implKyberNtt(short[] poly, short[] ntt_zetas) {}\n+  \/\/\n+  \/\/ coeffs (short[256]) = c_rarg0\n+  \/\/ ntt_zetas (short[256]) = c_rarg1\n+  address generate_kyberNtt() {\n+\n+    __ align(CodeEntryAlignment);\n+    StubGenStubId stub_id = StubGenStubId::kyberNtt_id;\n+    StubCodeMark mark(this, stub_id);\n+    address start = __ pc();\n+    __ enter();\n+\n+    const Register coeffs = c_rarg0;\n+    const Register zetas = c_rarg1;\n+\n+    const Register kyberConsts = r10;\n+    const Register tmpAddr = r11;\n+\n+    VSeq<8> vs1(0), vs2(16), vs3(24);  \/\/ 3 sets of 8x8H inputs\/outputs\n+    VSeq<4> vtmp = vs_front(vs3);      \/\/ n.b. tmp registers overlap vs3\n+    VSeq<2> vq(30);                    \/\/ n.b. constants overlap vs3\n+\n+    __ lea(kyberConsts, ExternalAddress((address) StubRoutines::aarch64::_kyberConsts));\n+    \/\/ load the montmul constants\n+    vs_ldpq(vq, kyberConsts);\n+\n+    \/\/ Each level corresponds to an iteration of the outermost loop of the\n+    \/\/ Java method seilerNTT(int[] coeffs). There are some differences\n+    \/\/ from what is done in the seilerNTT() method, though:\n+    \/\/ 1. The computation is using 16-bit signed values, we do not convert them\n+    \/\/ to ints here.\n+    \/\/ 2. The zetas are delivered in a bigger array, 128 zetas are stored in\n+    \/\/ this array for each level, it is easier that way to fill up the vector\n+    \/\/ registers.\n+    \/\/ 3. In the seilerNTT() method we use R = 2^20 for the Montgomery\n+    \/\/ multiplications (this is because that way there should not be any\n+    \/\/ overflow during the inverse NTT computation), here we usr R = 2^16 so\n+    \/\/ that we can use the 16-bit arithmetic in the vector unit.\n+    \/\/\n+    \/\/ On each level, we fill up the vector registers in such a way that the\n+    \/\/ array elements that need to be multiplied by the zetas go into one\n+    \/\/ set of vector registers while the corresponding ones that don't need to\n+    \/\/ be multiplied, go into another set.\n+    \/\/ We can do 32 Montgomery multiplications in parallel, using 12 vector\n+    \/\/ registers interleaving the steps of 4 identical computations,\n+    \/\/ each done on 8 16-bit values per register.\n+\n+    \/\/ At levels 0-3 the coefficients multiplied by or added\/subtracted\n+    \/\/ to the zetas occur in discrete blocks whose size is some multiple\n+    \/\/ of 32.\n+\n+    \/\/ level 0\n+    __ add(tmpAddr, coeffs, 256);\n+    load64shorts(vs1, tmpAddr);\n+    load64shorts(vs2, zetas);\n+    kyber_montmul64(vs2, vs1, vs2, vtmp, vq);\n+    __ add(tmpAddr, coeffs, 0);\n+    load64shorts(vs1, tmpAddr);\n+    vs_subv(vs3, __ T8H, vs1, vs2); \/\/ n.b. trashes vq\n+    vs_addv(vs1, __ T8H, vs1, vs2);\n+    __ add(tmpAddr, coeffs, 0);\n+    vs_stpq_post(vs1, tmpAddr);\n+    __ add(tmpAddr, coeffs, 256);\n+    vs_stpq_post(vs3, tmpAddr);\n+    \/\/ restore montmul constants\n+    vs_ldpq(vq, kyberConsts);\n+    load64shorts(vs1, tmpAddr);\n+    load64shorts(vs2, zetas);\n+    kyber_montmul64(vs2, vs1, vs2, vtmp, vq);\n+    __ add(tmpAddr, coeffs, 128);\n+    load64shorts(vs1, tmpAddr);\n+    vs_subv(vs3, __ T8H, vs1, vs2); \/\/ n.b. trashes vq\n+    vs_addv(vs1, __ T8H, vs1, vs2);\n+    __ add(tmpAddr, coeffs, 128);\n+    store64shorts(vs1, tmpAddr);\n+    __ add(tmpAddr, coeffs, 384);\n+    store64shorts(vs3, tmpAddr);\n+\n+    \/\/ level 1\n+    \/\/ restore montmul constants\n+    vs_ldpq(vq, kyberConsts);\n+    __ add(tmpAddr, coeffs, 128);\n+    load64shorts(vs1, tmpAddr);\n+    load64shorts(vs2, zetas);\n+    kyber_montmul64(vs2, vs1, vs2, vtmp, vq);\n+    __ add(tmpAddr, coeffs, 0);\n+    load64shorts(vs1, tmpAddr);\n+    vs_subv(vs3, __ T8H, vs1, vs2); \/\/ n.b. trashes vq\n+    vs_addv(vs1, __ T8H, vs1, vs2);\n+    __ add(tmpAddr, coeffs, 0);\n+    store64shorts(vs1, tmpAddr);\n+    store64shorts(vs3, tmpAddr);\n+    vs_ldpq(vq, kyberConsts);\n+    __ add(tmpAddr, coeffs, 384);\n+    load64shorts(vs1, tmpAddr);\n+    load64shorts(vs2, zetas);\n+    kyber_montmul64(vs2, vs1, vs2, vtmp, vq);\n+    __ add(tmpAddr, coeffs, 256);\n+    load64shorts(vs1, tmpAddr);\n+    vs_subv(vs3, __ T8H, vs1, vs2); \/\/ n.b. trashes vq\n+    vs_addv(vs1, __ T8H, vs1, vs2);\n+    __ add(tmpAddr, coeffs, 256);\n+    store64shorts(vs1, tmpAddr);\n+    store64shorts(vs3, tmpAddr);\n+\n+    \/\/ level 2\n+    vs_ldpq(vq, kyberConsts);\n+    int offsets1[4] = { 0, 32, 128, 160 };\n+    vs_ldpq_indexed(vs1, coeffs, 64, offsets1);\n+    load64shorts(vs2, zetas);\n+    kyber_montmul64(vs2, vs1, vs2, vtmp, vq);\n+    vs_ldpq_indexed(vs1, coeffs, 0, offsets1);\n+    \/\/ kyber_subv_addv64();\n+    vs_subv(vs3, __ T8H, vs1, vs2); \/\/ n.b. trashes vq\n+    vs_addv(vs1, __ T8H, vs1, vs2);\n+    __ add(tmpAddr, coeffs, 0);\n+    vs_stpq_post(vs_front(vs1), tmpAddr);\n+    vs_stpq_post(vs_front(vs3), tmpAddr);\n+    vs_stpq_post(vs_back(vs1), tmpAddr);\n+    vs_stpq_post(vs_back(vs3), tmpAddr);\n+    vs_ldpq(vq, kyberConsts);\n+    vs_ldpq_indexed(vs1, tmpAddr, 64, offsets1);\n+    load64shorts(vs2, zetas);\n+    kyber_montmul64(vs2, vs1, vs2, vtmp, vq);\n+    vs_ldpq_indexed(vs1,  coeffs, 256, offsets1);\n+    \/\/ kyber_subv_addv64();\n+    vs_subv(vs3, __ T8H, vs1, vs2); \/\/ n.b. trashes vq\n+    vs_addv(vs1, __ T8H, vs1, vs2);\n+    __ add(tmpAddr, coeffs, 256);\n+    vs_stpq_post(vs_front(vs1), tmpAddr);\n+    vs_stpq_post(vs_front(vs3), tmpAddr);\n+    vs_stpq_post(vs_back(vs1), tmpAddr);\n+    vs_stpq_post(vs_back(vs3), tmpAddr);\n+\n+    \/\/ level 3\n+    vs_ldpq(vq, kyberConsts);\n+    int offsets2[4] = { 0, 64, 128, 192 };\n+    vs_ldpq_indexed(vs1, coeffs, 32, offsets2);\n+    load64shorts(vs2, zetas);\n+    kyber_montmul64(vs2, vs1, vs2, vtmp, vq);\n+    vs_ldpq_indexed(vs1, coeffs, 0, offsets2);\n+    vs_subv(vs3, __ T8H, vs1, vs2); \/\/ n.b. trashes vq\n+    vs_addv(vs1, __ T8H, vs1, vs2);\n+    vs_stpq_indexed(vs1, coeffs, 0, offsets2);\n+    vs_stpq_indexed(vs3, coeffs, 32, offsets2);\n+\n+    vs_ldpq(vq, kyberConsts);\n+    vs_ldpq_indexed(vs1, coeffs, 256 + 32, offsets2);\n+    load64shorts(vs2, zetas);\n+    kyber_montmul64(vs2, vs1, vs2, vtmp, vq);\n+    vs_ldpq_indexed(vs1, coeffs, 256, offsets2);\n+    vs_subv(vs3, __ T8H, vs1, vs2); \/\/ n.b. trashes vq\n+    vs_addv(vs1, __ T8H, vs1, vs2);\n+    vs_stpq_indexed(vs1, coeffs, 256, offsets2);\n+    vs_stpq_indexed(vs3, coeffs, 256 + 32, offsets2);\n+\n+    \/\/ level 4\n+    \/\/ At level 4 coefficients occur in 8 discrete blocks of size 16\n+    \/\/ so they are loaded using employing an ldr at 8 distinct offsets.\n+\n+    vs_ldpq(vq, kyberConsts);\n+    int offsets3[8] = { 0, 32, 64, 96, 128, 160, 192, 224 };\n+    vs_ldr_indexed(vs1, __ Q, coeffs, 16, offsets3);\n+    load64shorts(vs2, zetas);\n+    kyber_montmul64(vs2, vs1, vs2, vtmp, vq);\n+    vs_ldr_indexed(vs1, __ Q, coeffs, 0, offsets3);\n+    vs_subv(vs3, __ T8H, vs1, vs2); \/\/ n.b. trashes vq\n+    vs_addv(vs1, __ T8H, vs1, vs2);\n+    vs_str_indexed(vs1, __ Q, coeffs, 0, offsets3);\n+    vs_str_indexed(vs3, __ Q, coeffs, 16, offsets3);\n+\n+    vs_ldpq(vq, kyberConsts);\n+    vs_ldr_indexed(vs1, __ Q, coeffs, 256 + 16, offsets3);\n+    load64shorts(vs2, zetas);\n+    kyber_montmul64(vs2, vs1, vs2, vtmp, vq);\n+    vs_ldr_indexed(vs1, __ Q, coeffs, 256, offsets3);\n+    vs_subv(vs3, __ T8H, vs1, vs2); \/\/ n.b. trashes vq\n+    vs_addv(vs1, __ T8H, vs1, vs2);\n+    vs_str_indexed(vs1, __ Q, coeffs, 256, offsets3);\n+    vs_str_indexed(vs3, __ Q, coeffs, 256 + 16, offsets3);\n+\n+    \/\/ level 5\n+    \/\/ At level 5 related coefficients occur in discrete blocks of size 8 so\n+    \/\/ need to be loaded interleaved using an ld2 operation with arrangement 2D.\n+\n+    vs_ldpq(vq, kyberConsts);\n+    int offsets4[4] = { 0, 32, 64, 96 };\n+    vs_ld2_indexed(vs1, __ T2D, coeffs, tmpAddr, 0, offsets4);\n+    load32shorts(vs_front(vs2), zetas);\n+    kyber_montmul32_sub_add(vs_even(vs1), vs_odd(vs1), vs_front(vs2), vtmp, vq);\n+    vs_st2_indexed(vs1, __ T2D, coeffs, tmpAddr, 0, offsets4);\n+    vs_ld2_indexed(vs1, __ T2D, coeffs, tmpAddr, 128, offsets4);\n+    load32shorts(vs_front(vs2), zetas);\n+    kyber_montmul32_sub_add(vs_even(vs1), vs_odd(vs1), vs_front(vs2), vtmp, vq);\n+    vs_st2_indexed(vs1, __ T2D, coeffs, tmpAddr, 128, offsets4);\n+    vs_ld2_indexed(vs1, __ T2D, coeffs, tmpAddr, 256, offsets4);\n+    load32shorts(vs_front(vs2), zetas);\n+    kyber_montmul32_sub_add(vs_even(vs1), vs_odd(vs1), vs_front(vs2), vtmp, vq);\n+    vs_st2_indexed(vs1, __ T2D, coeffs, tmpAddr, 256, offsets4);\n+\n+    vs_ld2_indexed(vs1, __ T2D, coeffs, tmpAddr, 384, offsets4);\n+    load32shorts(vs_front(vs2), zetas);\n+    kyber_montmul32_sub_add(vs_even(vs1), vs_odd(vs1), vs_front(vs2), vtmp, vq);\n+    vs_st2_indexed(vs1, __ T2D, coeffs, tmpAddr, 384, offsets4);\n+\n+    \/\/ level 6\n+    \/\/ At level 6 related coefficients occur in discrete blocks of size 4 so\n+    \/\/ need to be loaded interleaved using an ld2 operation with arrangement 4S.\n+\n+    vs_ld2_indexed(vs1, __ T4S, coeffs, tmpAddr, 0, offsets4);\n+    load32shorts(vs_front(vs2), zetas);\n+    kyber_montmul32_sub_add(vs_even(vs1), vs_odd(vs1), vs_front(vs2), vtmp, vq);\n+    vs_st2_indexed(vs1, __ T4S, coeffs, tmpAddr, 0, offsets4);\n+    vs_ld2_indexed(vs1, __ T4S, coeffs, tmpAddr, 128, offsets4);\n+    \/\/ __ ldpq(v18, v19, __ post(zetas, 32));\n+    load32shorts(vs_front(vs2), zetas);\n+    kyber_montmul32_sub_add(vs_even(vs1), vs_odd(vs1), vs_front(vs2), vtmp, vq);\n+    vs_st2_indexed(vs1, __ T4S, coeffs, tmpAddr, 128, offsets4);\n+\n+    vs_ld2_indexed(vs1, __ T4S, coeffs, tmpAddr, 256, offsets4);\n+    load32shorts(vs_front(vs2), zetas);\n+    kyber_montmul32_sub_add(vs_even(vs1), vs_odd(vs1), vs_front(vs2), vtmp, vq);\n+    vs_st2_indexed(vs1, __ T4S, coeffs, tmpAddr, 256, offsets4);\n+\n+    vs_ld2_indexed(vs1, __ T4S, coeffs, tmpAddr, 384, offsets4);\n+    load32shorts(vs_front(vs2), zetas);\n+    kyber_montmul32_sub_add(vs_even(vs1), vs_odd(vs1), vs_front(vs2), vtmp, vq);\n+    vs_st2_indexed(vs1, __ T4S, coeffs, tmpAddr, 384, offsets4);\n+\n+    __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n+    __ mov(r0, zr); \/\/ return 0\n+    __ ret(lr);\n+\n+    return start;\n+  }\n+\n+  \/\/ Kyber Inverse NTT function\n+  \/\/ Implements\n+  \/\/ static int implKyberInverseNtt(short[] poly, short[] zetas) {}\n+  \/\/\n+  \/\/ coeffs (short[256]) = c_rarg0\n+  \/\/ ntt_zetas (short[256]) = c_rarg1\n+  address generate_kyberInverseNtt() {\n+\n+    __ align(CodeEntryAlignment);\n+    StubGenStubId stub_id = StubGenStubId::kyberInverseNtt_id;\n+    StubCodeMark mark(this, stub_id);\n+    address start = __ pc();\n+    __ enter();\n+\n+    const Register coeffs = c_rarg0;\n+    const Register zetas = c_rarg1;\n+\n+    const Register kyberConsts = r10;\n+    const Register tmpAddr = r11;\n+    const Register tmpAddr2 = c_rarg2;\n+\n+    VSeq<8> vs1(0), vs2(16), vs3(24);  \/\/ 3 sets of 8x8H inputs\/outputs\n+    VSeq<4> vtmp = vs_front(vs3);      \/\/ n.b. tmp registers overlap vs3\n+    VSeq<2> vq(30);                    \/\/ n.b. constants overlap vs3\n+\n+    __ lea(kyberConsts,\n+             ExternalAddress((address) StubRoutines::aarch64::_kyberConsts));\n+\n+    \/\/ level 0\n+    \/\/ At level 0 related coefficients occur in discrete blocks of size 4 so\n+    \/\/ need to be loaded interleaved using an ld2 operation with arrangement 4S.\n+\n+    vs_ldpq(vq, kyberConsts);\n+    int offsets4[4] = { 0, 32, 64, 96 };\n+    vs_ld2_indexed(vs1, __ T4S, coeffs, tmpAddr, 0, offsets4);\n+    load32shorts(vs_front(vs2), zetas);\n+    kyber_sub_add_montmul32(vs_even(vs1), vs_odd(vs1),\n+                            vs_front(vs2), vs_back(vs2), vtmp, vq);\n+    vs_st2_indexed(vs1, __ T4S, coeffs, tmpAddr, 0, offsets4);\n+    vs_ld2_indexed(vs1, __ T4S, coeffs, tmpAddr, 128, offsets4);\n+    load32shorts(vs_front(vs2), zetas);\n+    kyber_sub_add_montmul32(vs_even(vs1), vs_odd(vs1),\n+                            vs_front(vs2), vs_back(vs2), vtmp, vq);\n+    vs_st2_indexed(vs1, __ T4S, coeffs, tmpAddr, 128, offsets4);\n+    vs_ld2_indexed(vs1, __ T4S, coeffs, tmpAddr, 256, offsets4);\n+    load32shorts(vs_front(vs2), zetas);\n+    kyber_sub_add_montmul32(vs_even(vs1), vs_odd(vs1),\n+                            vs_front(vs2), vs_back(vs2), vtmp, vq);\n+    vs_st2_indexed(vs1, __ T4S, coeffs, tmpAddr, 256, offsets4);\n+    vs_ld2_indexed(vs1, __ T4S, coeffs, tmpAddr, 384, offsets4);\n+    load32shorts(vs_front(vs2), zetas);\n+    kyber_sub_add_montmul32(vs_even(vs1), vs_odd(vs1),\n+                            vs_front(vs2), vs_back(vs2), vtmp, vq);\n+    vs_st2_indexed(vs1, __ T4S, coeffs, tmpAddr, 384, offsets4);\n+\n+    \/\/ level 1\n+    \/\/ At level 1 related coefficients occur in discrete blocks of size 8 so\n+    \/\/ need to be loaded interleaved using an ld2 operation with arrangement 2D.\n+\n+    vs_ld2_indexed(vs1, __ T2D, coeffs, tmpAddr, 0, offsets4);\n+    load32shorts(vs_front(vs2), zetas);\n+    kyber_sub_add_montmul32(vs_even(vs1), vs_odd(vs1),\n+                            vs_front(vs2), vs_back(vs2), vtmp, vq);\n+    vs_st2_indexed(vs1, __ T2D, coeffs, tmpAddr, 0, offsets4);\n+    vs_ld2_indexed(vs1, __ T2D, coeffs, tmpAddr, 128, offsets4);\n+    load32shorts(vs_front(vs2), zetas);\n+    kyber_sub_add_montmul32(vs_even(vs1), vs_odd(vs1),\n+                            vs_front(vs2), vs_back(vs2), vtmp, vq);\n+    vs_st2_indexed(vs1, __ T2D, coeffs, tmpAddr, 128, offsets4);\n+\n+    vs_ld2_indexed(vs1, __ T2D, coeffs, tmpAddr, 256, offsets4);\n+    load32shorts(vs_front(vs2), zetas);\n+    kyber_sub_add_montmul32(vs_even(vs1), vs_odd(vs1),\n+                            vs_front(vs2), vs_back(vs2), vtmp, vq);\n+    vs_st2_indexed(vs1, __ T2D, coeffs, tmpAddr, 256, offsets4);\n+    vs_ld2_indexed(vs1, __ T2D, coeffs, tmpAddr, 384, offsets4);\n+    load32shorts(vs_front(vs2), zetas);\n+    kyber_sub_add_montmul32(vs_even(vs1), vs_odd(vs1),\n+                            vs_front(vs2), vs_back(vs2), vtmp, vq);\n+    vs_st2_indexed(vs1, __ T2D, coeffs, tmpAddr, 384, offsets4);\n+\n+    \/\/ level 2\n+    \/\/ At level 2 coefficients occur in 8 discrete blocks of size 16\n+    \/\/ so they are loaded using employing an ldr at 8 distinct offsets.\n+\n+    int offsets3[8] = { 0, 32, 64, 96, 128, 160, 192, 224 };\n+    vs_ldr_indexed(vs1, __ Q, coeffs, 0, offsets3);\n+    vs_ldr_indexed(vs2, __ Q, coeffs, 16, offsets3);\n+    vs_addv(vs3, __ T8H, vs1, vs2); \/\/ n.b. trashes vq\n+    vs_subv(vs1, __ T8H, vs1, vs2);\n+    vs_str_indexed(vs3, __ Q, coeffs, 0, offsets3);\n+    load64shorts(vs2, zetas);\n+    vs_ldpq(vq, kyberConsts);\n+    kyber_montmul64(vs2, vs1, vs2, vtmp, vq);\n+    vs_str_indexed(vs2, __ Q, coeffs, 16, offsets3);\n+\n+    vs_ldr_indexed(vs1, __ Q, coeffs, 256, offsets3);\n+    vs_ldr_indexed(vs2, __ Q, coeffs, 256 + 16, offsets3);\n+    vs_addv(vs3, __ T8H, vs1, vs2); \/\/ n.b. trashes vq\n+    vs_subv(vs1, __ T8H, vs1, vs2);\n+    vs_str_indexed(vs3, __ Q, coeffs, 256, offsets3);\n+    load64shorts(vs2, zetas);\n+    vs_ldpq(vq, kyberConsts);\n+    kyber_montmul64(vs2, vs1, vs2, vtmp, vq);\n+    vs_str_indexed(vs2, __ Q, coeffs, 256 + 16, offsets3);\n+\n+    \/\/ Barrett reduction at indexes where overflow may happen\n+\n+    \/\/ load q and the multiplier for the Barrett reduction\n+    __ add(tmpAddr, kyberConsts, 16);\n+    vs_ldpq(vq, tmpAddr);\n+\n+    VSeq<8> vq1 = VSeq<8>(vq[0], 0); \/\/ 2 constant 8 sequences\n+    VSeq<8> vq2 = VSeq<8>(vq[1], 0); \/\/ for above two kyber constants\n+    VSeq<8> vq3 = VSeq<8>(v29, 0);   \/\/ 3rd sequence for const montmul\n+    vs_ldr_indexed(vs1, __ Q, coeffs, 0, offsets3);\n+    vs_sqdmulh(vs2, __ T8H, vs1, vq2);\n+    vs_sshr(vs2, __ T8H, vs2, 11);\n+    vs_mlsv(vs1, __ T8H, vs2, vq1);\n+    vs_str_indexed(vs1, __ Q, coeffs, 0, offsets3);\n+    vs_ldr_indexed(vs1, __ Q, coeffs, 256, offsets3);\n+    vs_sqdmulh(vs2, __ T8H, vs1, vq2);\n+    vs_sshr(vs2, __ T8H, vs2, 11);\n+    vs_mlsv(vs1, __ T8H, vs2, vq1);\n+    vs_str_indexed(vs1, __ Q, coeffs, 256, offsets3);\n+\n+    \/\/ level 3\n+    \/\/ From level 3 upwards coefficients occur in discrete blocks whose size is\n+    \/\/ some multiple of 32 so can be loaded using ldpq and suitable indexes.\n+\n+    int offsets2[4] = { 0, 64, 128, 192 };\n+    vs_ldpq_indexed(vs1, coeffs, 0, offsets2);\n+    vs_ldpq_indexed(vs2, coeffs, 32, offsets2);\n+    vs_addv(vs3, __ T8H, vs1, vs2); \/\/ n.b. trashes vq\n+    vs_subv(vs1, __ T8H, vs1, vs2);\n+    vs_stpq_indexed(vs3, coeffs, 0, offsets2);\n+    load64shorts(vs2, zetas);\n+    vs_ldpq(vq, kyberConsts);\n+    kyber_montmul64(vs2, vs1, vs2, vtmp, vq);\n+    vs_stpq_indexed(vs2, coeffs, 32, offsets2);\n+\n+    vs_ldpq_indexed(vs1, coeffs, 256, offsets2);\n+    vs_ldpq_indexed(vs2, coeffs, 256 + 32, offsets2);\n+    vs_addv(vs3, __ T8H, vs1, vs2); \/\/ n.b. trashes vq\n+    vs_subv(vs1, __ T8H, vs1, vs2);\n+    vs_stpq_indexed(vs3, coeffs, 256, offsets2);\n+    load64shorts(vs2, zetas);\n+    vs_ldpq(vq, kyberConsts);\n+    kyber_montmul64(vs2, vs1, vs2, vtmp, vq);\n+    vs_stpq_indexed(vs2, coeffs, 256 + 32, offsets2);\n+\n+    \/\/ level 4\n+\n+    int offsets1[4] = { 0, 32, 128, 160 };\n+    vs_ldpq_indexed(vs1, coeffs, 0, offsets1);\n+    vs_ldpq_indexed(vs2, coeffs, 64, offsets1);\n+    vs_addv(vs3, __ T8H, vs1, vs2); \/\/ n.b. trashes vq\n+    vs_subv(vs1, __ T8H, vs1, vs2);\n+    vs_stpq_indexed(vs3, coeffs, 0, offsets1);\n+    load64shorts(vs2, zetas);\n+    vs_ldpq(vq, kyberConsts);\n+    kyber_montmul64(vs2, vs1, vs2, vtmp, vq);\n+    vs_stpq_indexed(vs2, coeffs, 64, offsets1);\n+\n+    vs_ldpq_indexed(vs1, coeffs, 256, offsets1);\n+    vs_ldpq_indexed(vs2, coeffs, 256 + 64, offsets1);\n+    vs_addv(vs3, __ T8H, vs1, vs2); \/\/ n.b. trashes vq\n+    vs_subv(vs1, __ T8H, vs1, vs2);\n+    vs_stpq_indexed(vs3, coeffs, 256, offsets1);\n+    load64shorts(vs2, zetas);\n+    vs_ldpq(vq, kyberConsts);\n+    kyber_montmul64(vs2, vs1, vs2, vtmp, vq);\n+    vs_stpq_indexed(vs2, coeffs, 256 + 64, offsets1);\n+\n+    \/\/ level 5\n+\n+    __ add(tmpAddr, coeffs, 0);\n+    load64shorts(vs1, tmpAddr);\n+    __ add(tmpAddr, coeffs, 128);\n+    load64shorts(vs2, tmpAddr);\n+    vs_addv(vs3, __ T8H, vs1, vs2); \/\/ n.b. trashes vq\n+    vs_subv(vs1, __ T8H, vs1, vs2);\n+    __ add(tmpAddr, coeffs, 0);\n+    store64shorts(vs3, tmpAddr);\n+    load64shorts(vs2, zetas);\n+    vs_ldpq(vq, kyberConsts);\n+    kyber_montmul64(vs2, vs1, vs2, vtmp, vq);\n+    __ add(tmpAddr, coeffs, 128);\n+    store64shorts(vs2, tmpAddr);\n+\n+    load64shorts(vs1, tmpAddr);\n+    __ add(tmpAddr, coeffs, 384);\n+    load64shorts(vs2, tmpAddr);\n+    vs_addv(vs3, __ T8H, vs1, vs2); \/\/ n.b. trashes vq\n+    vs_subv(vs1, __ T8H, vs1, vs2);\n+    __ add(tmpAddr, coeffs, 256);\n+    store64shorts(vs3, tmpAddr);\n+    load64shorts(vs2, zetas);\n+    vs_ldpq(vq, kyberConsts);\n+    kyber_montmul64(vs2, vs1, vs2, vtmp, vq);\n+    __ add(tmpAddr, coeffs, 384);\n+    store64shorts(vs2, tmpAddr);\n+\n+    \/\/ Barrett reduction at indexes where overflow may happen\n+\n+    \/\/ load q and the multiplier for the Barrett reduction\n+    __ add(tmpAddr, kyberConsts, 16);\n+    vs_ldpq(vq, tmpAddr);\n+\n+    int offsets0[2] = { 0, 256 };\n+    vs_ldpq_indexed(vs_front(vs1), coeffs, 0, offsets0);\n+    vs_sqdmulh(vs2, __ T8H, vs1, vq2);\n+    vs_sshr(vs2, __ T8H, vs2, 11);\n+    vs_mlsv(vs1, __ T8H, vs2, vq1);\n+    vs_stpq_indexed(vs_front(vs1), coeffs, 0, offsets0);\n+\n+    \/\/ level 6\n+\n+    __ add(tmpAddr, coeffs, 0);\n+    load64shorts(vs1, tmpAddr);\n+    __ add(tmpAddr, coeffs, 256);\n+    load64shorts(vs2, tmpAddr);\n+    vs_addv(vs3, __ T8H, vs1, vs2); \/\/ n.b. trashes vq\n+    vs_subv(vs1, __ T8H, vs1, vs2);\n+    __ add(tmpAddr, coeffs, 0);\n+    store64shorts(vs3, tmpAddr);\n+    load64shorts(vs2, zetas);\n+    vs_ldpq(vq, kyberConsts);\n+    kyber_montmul64(vs2, vs1, vs2, vtmp, vq);\n+    __ add(tmpAddr, coeffs, 256);\n+    store64shorts(vs2, tmpAddr);\n+\n+    __ add(tmpAddr, coeffs, 128);\n+    load64shorts(vs1, tmpAddr);\n+    __ add(tmpAddr, coeffs, 384);\n+    load64shorts(vs2, tmpAddr);\n+    vs_addv(vs3, __ T8H, vs1, vs2); \/\/ n.b. trashes vq\n+    vs_subv(vs1, __ T8H, vs1, vs2);\n+    __ add(tmpAddr, coeffs, 128);\n+    store64shorts(vs3, tmpAddr);\n+    load64shorts(vs2, zetas);\n+    vs_ldpq(vq, kyberConsts);\n+    kyber_montmul64(vs2, vs1, vs2, vtmp, vq);\n+    __ add(tmpAddr, coeffs, 384);\n+    store64shorts(vs2, tmpAddr);\n+\n+    \/\/ multiply by 2^-n\n+\n+    \/\/ load toMont(2^-n mod q)\n+    __ add(tmpAddr, kyberConsts, 48);\n+    __ ldr(v29, __ Q, tmpAddr);\n+\n+    vs_ldpq(vq, kyberConsts);\n+    __ add(tmpAddr, coeffs, 0);\n+    load64shorts(vs1, tmpAddr);\n+    kyber_montmul64(vs2, vs1, vq3, vtmp, vq);\n+    __ add(tmpAddr, coeffs, 0);\n+    store64shorts(vs2, tmpAddr);\n+\n+    \/\/ now tmpAddr contains coeffs + 128 because store64shorts adjusted it so\n+    load64shorts(vs1, tmpAddr);\n+    kyber_montmul64(vs2, vs1, vq3, vtmp, vq);\n+    __ add(tmpAddr, coeffs, 128);\n+    store64shorts(vs2, tmpAddr);\n+\n+    \/\/ now tmpAddr contains coeffs + 256\n+    load64shorts(vs1, tmpAddr);\n+    kyber_montmul64(vs2, vs1, vq3, vtmp, vq);\n+    __ add(tmpAddr, coeffs, 256);\n+    store64shorts(vs2, tmpAddr);\n+\n+    \/\/ now tmpAddr contains coeffs + 384\n+    load64shorts(vs1, tmpAddr);\n+    kyber_montmul64(vs2, vs1, vq3, vtmp, vq);\n+    __ add(tmpAddr, coeffs, 384);\n+    store64shorts(vs2, tmpAddr);\n+\n+    __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n+    __ mov(r0, zr); \/\/ return 0\n+    __ ret(lr);\n+\n+    return start;\n+  }\n+\n+  \/\/ Kyber multiply polynomials in the NTT domain.\n+  \/\/ Implements\n+  \/\/ static int implKyberNttMult(\n+  \/\/              short[] result, short[] ntta, short[] nttb, short[] zetas) {}\n+  \/\/\n+  \/\/ result (short[256]) = c_rarg0\n+  \/\/ ntta (short[256]) = c_rarg1\n+  \/\/ nttb (short[256]) = c_rarg2\n+  \/\/ zetas (short[128]) = c_rarg3\n+  address generate_kyberNttMult() {\n+\n+    __ align(CodeEntryAlignment);\n+    StubGenStubId stub_id = StubGenStubId::kyberNttMult_id;\n+    StubCodeMark mark(this, stub_id);\n+    address start = __ pc();\n+    __ enter();\n+\n+    const Register result = c_rarg0;\n+    const Register ntta = c_rarg1;\n+    const Register nttb = c_rarg2;\n+    const Register zetas = c_rarg3;\n+\n+    const Register kyberConsts = r10;\n+    const Register limit = r11;\n+\n+    VSeq<4> vs1(0), vs2(4);  \/\/ 4 sets of 8x8H inputs\/outputs\/tmps\n+    VSeq<4> vs3(16), vs4(20);\n+    VSeq<2> vq(30);          \/\/ pair of constants for montmul: q, qinv\n+    VSeq<2> vz(28);          \/\/ pair of zetas\n+    VSeq<4> vc(27, 0);       \/\/ constant sequence for montmul: montRSquareModQ\n+\n+    __ lea(kyberConsts,\n+             ExternalAddress((address) StubRoutines::aarch64::_kyberConsts));\n+\n+    Label kyberNttMult_loop;\n+\n+    __ add(limit, result, 512);\n+\n+    \/\/ load q and qinv\n+    vs_ldpq(vq, kyberConsts);\n+\n+    \/\/ load R^2 mod q (to convert back from Montgomery representation)\n+    __ add(kyberConsts, kyberConsts, 64);\n+    __ ldr(v27, __ Q, kyberConsts);\n+\n+    __ BIND(kyberNttMult_loop);\n+    \/\/ load 16 zetas\n+    vs_ldpq_post(vz, zetas);\n+    \/\/ load 2 sets of 32 coefficients from the two input arrays\n+    vs_ld2_post(vs_front(vs1), __ T8H, ntta);\n+    vs_ld2_post(vs_back(vs1), __ T8H, nttb);\n+    vs_ld2_post(vs_front(vs4), __ T8H, ntta);\n+    vs_ld2_post(vs_back(vs4), __ T8H, nttb);\n+    \/\/ montmul the first and second pair of values loaded into vs1\n+    \/\/ in order and then with one pair reversed storing the  two\n+    \/\/ results in vs3\n+    kyber_montmul16(vs_front(vs3), vs_front(vs1), vs_back(vs1), vs_front(vs2), vq);\n+    kyber_montmul16(vs_back(vs3),\n+                    vs_front(vs1), vs_reverse(vs_back(vs1)), vs_back(vs2), vq);\n+    \/\/ montmul the first and second pair of values loaded into vs4\n+    \/\/ in order and then with one pair reversed storing the two\n+    \/\/ results in vs1\n+    kyber_montmul16(vs_front(vs1), vs_front(vs4), vs_back(vs4), vs_front(vs2), vq);\n+    kyber_montmul16(vs_back(vs1),\n+                    vs_front(vs4), vs_reverse(vs_back(vs4)), vs_back(vs2), vq);\n+    \/\/ for each pair of results pick the second value in the first\n+    \/\/ pair to create a sequence that we montmul by the zetas\n+    \/\/ i.e. we want sequence <vs3[1], vs1[1]>\n+    int delta = vs1[1]->encoding() - vs3[1]->encoding();\n+    VSeq<2> vs5(vs3[1], delta);\n+    kyber_montmul16(vs5, vz, vs5, vs_front(vs2), vq);\n+    \/\/ add results in pairs storing in vs3\n+    vs_addv(vs_front(vs3), __ T8H, vs_even(vs3), vs_odd(vs3));\n+    vs_addv(vs_back(vs3), __ T8H, vs_even(vs1), vs_odd(vs1));\n+    \/\/ montmul result by constant vc and store result in vs1\n+    kyber_montmul32(vs1, vs3, vc, vs2, vq);\n+    \/\/ store the four results as two interleaved pairs of\n+    \/\/ quadwords\n+    vs_st2_post(vs1, __ T8H, result);\n+\n+    __ cmp(result, limit);\n+    __ br(Assembler::NE, kyberNttMult_loop);\n+\n+    __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n+    __ mov(r0, zr); \/\/ return 0\n+    __ ret(lr);\n+\n+    return start;\n+  }\n+\n+  \/\/ Kyber add 2 polynomials.\n+  \/\/ Implements\n+  \/\/ static int implKyberAddPoly(short[] result, short[] a, short[] b) {}\n+  \/\/\n+  \/\/ result (short[256]) = c_rarg0\n+  \/\/ a (short[256]) = c_rarg1\n+  \/\/ b (short[256]) = c_rarg2\n+  address generate_kyberAddPoly_2() {\n+\n+    __ align(CodeEntryAlignment);\n+    StubGenStubId stub_id = StubGenStubId::kyberAddPoly_2_id;\n+    StubCodeMark mark(this, stub_id);\n+    address start = __ pc();\n+    __ enter();\n+\n+    const Register result = c_rarg0;\n+    const Register a = c_rarg1;\n+    const Register b = c_rarg2;\n+\n+    const Register kyberConsts = r11;\n+\n+    \/\/ We sum 256 sets of values in total i.e. 32 x 8H quadwords.\n+    \/\/ So, we can load, add and store the data in 3 groups of 11,\n+    \/\/ 11 and 10 at a time i.e. we need to map sets of 10 or 11\n+    \/\/ registers. A further constraint is that the mapping needs\n+    \/\/ to skip callee saves. So, we allocate the register\n+    \/\/ sequences using two 8 sequences, two 2 sequences and two\n+    \/\/ single registers.\n+    VSeq<8> vs1_1(0);\n+    VSeq<2> vs1_2(16);\n+    FloatRegister vs1_3 = v28;\n+    VSeq<8> vs2_1(18);\n+    VSeq<2> vs2_2(26);\n+    FloatRegister vs2_3 = v29;\n+\n+    \/\/ two constant vector sequences\n+    VSeq<8> vc_1(31, 0);\n+    VSeq<2> vc_2(31, 0);\n+\n+    FloatRegister vc_3 = v31;\n+    __ lea(kyberConsts,\n+             ExternalAddress((address) StubRoutines::aarch64::_kyberConsts));\n+\n+    __ ldr(vc_3, __ Q, Address(kyberConsts, 16)); \/\/ q\n+    for (int i = 0; i < 3; i++) {\n+      \/\/ load 80 or 88 values from a into vs1_1\/2\/3\n+      vs_ldpq_post(vs1_1, a);\n+      vs_ldpq_post(vs1_2, a);\n+      if (i < 2) {\n+        __ ldr(vs1_3, __ Q, __ post(a, 16));\n+      }\n+      \/\/ load 80 or 88 values from b into vs2_1\/2\/3\n+      vs_ldpq_post(vs2_1, b);\n+      vs_ldpq_post(vs2_2, b);\n+      if (i < 2) {\n+        __ ldr(vs2_3, __ Q, __ post(b, 16));\n+      }\n+      \/\/ sum 80 or 88 values across vs1 and vs2 into vs1\n+      vs_addv(vs1_1, __ T8H, vs1_1, vs2_1);\n+      vs_addv(vs1_2, __ T8H, vs1_2, vs2_2);\n+      if (i < 2) {\n+        __ addv(vs1_3, __ T8H, vs1_3, vs2_3);\n+      }\n+      \/\/ add constant to all 80 or 88 results\n+      vs_addv(vs1_1, __ T8H, vs1_1, vc_1);\n+      vs_addv(vs1_2, __ T8H, vs1_2, vc_2);\n+      if (i < 2) {\n+        __ addv(vs1_3, __ T8H, vs1_3, vc_3);\n+      }\n+      \/\/ store 80 or 88 values\n+      vs_stpq_post(vs1_1, result);\n+      vs_stpq_post(vs1_2, result);\n+      if (i < 2) {\n+        __ str(vs1_3, __ Q, __ post(result, 16));\n+      }\n+    }\n+\n+    __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n+    __ mov(r0, zr); \/\/ return 0\n+    __ ret(lr);\n+\n+    return start;\n+  }\n+\n+  \/\/ Kyber add 3 polynomials.\n+  \/\/ Implements\n+  \/\/ static int implKyberAddPoly(short[] result, short[] a, short[] b, short[] c) {}\n+  \/\/\n+  \/\/ result (short[256]) = c_rarg0\n+  \/\/ a (short[256]) = c_rarg1\n+  \/\/ b (short[256]) = c_rarg2\n+  \/\/ c (short[256]) = c_rarg3\n+  address generate_kyberAddPoly_3() {\n+\n+    __ align(CodeEntryAlignment);\n+    StubGenStubId stub_id = StubGenStubId::kyberAddPoly_3_id;\n+    StubCodeMark mark(this, stub_id);\n+    address start = __ pc();\n+    __ enter();\n+\n+    const Register result = c_rarg0;\n+    const Register a = c_rarg1;\n+    const Register b = c_rarg2;\n+    const Register c = c_rarg3;\n+\n+    const Register kyberConsts = r11;\n+\n+    \/\/ As above we sum 256 sets of values in total i.e. 32 x 8H\n+    \/\/ quadwords.  So, we can load, add and store the data in 3\n+    \/\/ groups of 11, 11 and 10 at a time i.e. we need to map sets\n+    \/\/ of 10 or 11 registers. A further constraint is that the\n+    \/\/ mapping needs to skip callee saves. So, we allocate the\n+    \/\/ register sequences using two 8 sequences, two 2 sequences\n+    \/\/ and two single registers.\n+    VSeq<8> vs1_1(0);\n+    VSeq<2> vs1_2(16);\n+    FloatRegister vs1_3 = v28;\n+    VSeq<8> vs2_1(18);\n+    VSeq<2> vs2_2(26);\n+    FloatRegister vs2_3 = v29;\n+\n+    \/\/ two constant vector sequences\n+    VSeq<8> vc_1(31, 0);\n+    VSeq<2> vc_2(31, 0);\n+\n+    FloatRegister vc_3 = v31;\n+\n+    __ lea(kyberConsts,\n+             ExternalAddress((address) StubRoutines::aarch64::_kyberConsts));\n+\n+    __ ldr(vc_3, __ Q, Address(kyberConsts, 16)); \/\/ q\n+    for (int i = 0; i < 3; i++) {\n+      \/\/ load 80 or 88 values from a into vs1_1\/2\/3\n+      vs_ldpq_post(vs1_1, a);\n+      vs_ldpq_post(vs1_2, a);\n+      if (i < 2) {\n+        __ ldr(vs1_3, __ Q, __ post(a, 16));\n+      }\n+      \/\/ load 80 or 88 values from b into vs2_1\/2\/3\n+      vs_ldpq_post(vs2_1, b);\n+      vs_ldpq_post(vs2_2, b);\n+      if (i < 2) {\n+        __ ldr(vs2_3, __ Q, __ post(b, 16));\n+      }\n+      \/\/ sum 80 or 88 values across vs1 and vs2 into vs1\n+      vs_addv(vs1_1, __ T8H, vs1_1, vs2_1);\n+      vs_addv(vs1_2, __ T8H, vs1_2, vs2_2);\n+      if (i < 2) {\n+        __ addv(vs1_3, __ T8H, vs1_3, vs2_3);\n+      }\n+      \/\/ load 80 or 88 values from c into vs2_1\/2\/3\n+      vs_ldpq_post(vs2_1, c);\n+      vs_ldpq_post(vs2_2, c);\n+      if (i < 2) {\n+        __ ldr(vs2_3, __ Q, __ post(c, 16));\n+      }\n+      \/\/ sum 80 or 88 values across vs1 and vs2 into vs1\n+      vs_addv(vs1_1, __ T8H, vs1_1, vs2_1);\n+      vs_addv(vs1_2, __ T8H, vs1_2, vs2_2);\n+      if (i < 2) {\n+        __ addv(vs1_3, __ T8H, vs1_3, vs2_3);\n+      }\n+      \/\/ add constant to all 80 or 88 results\n+      vs_addv(vs1_1, __ T8H, vs1_1, vc_1);\n+      vs_addv(vs1_2, __ T8H, vs1_2, vc_2);\n+      if (i < 2) {\n+        __ addv(vs1_3, __ T8H, vs1_3, vc_3);\n+      }\n+      \/\/ store 80 or 88 values\n+      vs_stpq_post(vs1_1, result);\n+      vs_stpq_post(vs1_2, result);\n+      if (i < 2) {\n+        __ str(vs1_3, __ Q, __ post(result, 16));\n+      }\n+    }\n+\n+    __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n+    __ mov(r0, zr); \/\/ return 0\n+    __ ret(lr);\n+\n+    return start;\n+  }\n+\n+  \/\/ Kyber parse XOF output to polynomial coefficient candidates\n+  \/\/ or decodePoly(12, ...).\n+  \/\/ Implements\n+  \/\/ static int implKyber12To16(\n+  \/\/         byte[] condensed, int index, short[] parsed, int parsedLength) {}\n+  \/\/\n+  \/\/ (parsedLength or (parsedLength - 48) must be divisible by 64.)\n+  \/\/\n+  \/\/ condensed (byte[]) = c_rarg0\n+  \/\/ condensedIndex = c_rarg1\n+  \/\/ parsed (short[112 or 256]) = c_rarg2\n+  \/\/ parsedLength (112 or 256) = c_rarg3\n+  address generate_kyber12To16() {\n+    Label L_F00, L_loop, L_end;\n+\n+    __ BIND(L_F00);\n+    __ emit_int64(0x0f000f000f000f00);\n+    __ emit_int64(0x0f000f000f000f00);\n+\n+    __ align(CodeEntryAlignment);\n+    StubGenStubId stub_id = StubGenStubId::kyber12To16_id;\n+    StubCodeMark mark(this, stub_id);\n+    address start = __ pc();\n+    __ enter();\n+\n+    const Register condensed = c_rarg0;\n+    const Register condensedOffs = c_rarg1;\n+    const Register parsed = c_rarg2;\n+    const Register parsedLength = c_rarg3;\n+\n+    const Register tmpAddr = r11;\n+\n+    \/\/ Data is input 96 bytes at a time i.e. in groups of 6 x 16B\n+    \/\/ quadwords so we need a 6 vector sequence for the inputs.\n+    \/\/ Parsing produces 64 shorts, employing two 8 vector\n+    \/\/ sequences to store and combine the intermediate data.\n+    VSeq<6> vin(24);\n+    VSeq<8> va(0), vb(16);\n+\n+    __ adr(tmpAddr, L_F00);\n+    __ ldr(v31, __ Q, tmpAddr); \/\/ 8H times 0x0f00\n+    __ add(condensed, condensed, condensedOffs);\n+\n+    __ BIND(L_loop);\n+    \/\/ load 96 (6 x 16B) byte values\n+    vs_ld3_post(vin, __ T16B, condensed);\n+\n+    \/\/ The front half of sequence vin (vin[0], vin[1] and vin[2])\n+    \/\/ holds 48 (16x3) contiguous bytes from memory striped\n+    \/\/ horizontally across each of the 16 byte lanes. Equivalently,\n+    \/\/ that is 16 pairs of 12-bit integers. Likewise the back half\n+    \/\/ holds the next 48 bytes in the same arrangement.\n+\n+    \/\/ Each vector in the front half can also be viewed as a vertical\n+    \/\/ strip across the 16 pairs of 12 bit integers. Each byte in\n+    \/\/ vin[0] stores the low 8 bits of the first int in a pair. Each\n+    \/\/ byte in vin[1] stores the high 4 bits of the first int and the\n+    \/\/ low 4 bits of the second int. Each byte in vin[2] stores the\n+    \/\/ high 8 bits of the second int. Likewise the vectors in second\n+    \/\/ half.\n+\n+    \/\/ Converting the data to 16-bit shorts requires first of all\n+    \/\/ expanding each of the 6 x 16B vectors into 6 corresponding\n+    \/\/ pairs of 8H vectors. Mask, shift and add operations on the\n+    \/\/ resulting vector pairs can be used to combine 4 and 8 bit\n+    \/\/ parts of related 8H vector elements.\n+    \/\/\n+    \/\/ The middle vectors (vin[2] and vin[5]) are actually expanded\n+    \/\/ twice, one copy manipulated to provide the lower 4 bits\n+    \/\/ belonging to the first short in a pair and another copy\n+    \/\/ manipulated to provide the higher 4 bits belonging to the\n+    \/\/ second short in a pair. This is why the the vector sequences va\n+    \/\/ and vb used to hold the expanded 8H elements are of length 8.\n+\n+    \/\/ Expand vin[0] into va[0:1], and vin[1] into va[2:3] and va[4:5]\n+    \/\/ n.b. target elements 2 and 3 duplicate elements 4 and 5\n+    __ ushll(va[0], __ T8H, vin[0], __ T8B, 0);\n+    __ ushll2(va[1], __ T8H, vin[0], __ T16B, 0);\n+    __ ushll(va[2], __ T8H, vin[1], __ T8B, 0);\n+    __ ushll2(va[3], __ T8H, vin[1], __ T16B, 0);\n+    __ ushll(va[4], __ T8H, vin[1], __ T8B, 0);\n+    __ ushll2(va[5], __ T8H, vin[1], __ T16B, 0);\n+\n+    \/\/ likewise expand vin[3] into vb[0:1], and vin[4] into vb[2:3]\n+    \/\/ and vb[4:5]\n+    __ ushll(vb[0], __ T8H, vin[3], __ T8B, 0);\n+    __ ushll2(vb[1], __ T8H, vin[3], __ T16B, 0);\n+    __ ushll(vb[2], __ T8H, vin[4], __ T8B, 0);\n+    __ ushll2(vb[3], __ T8H, vin[4], __ T16B, 0);\n+    __ ushll(vb[4], __ T8H, vin[4], __ T8B, 0);\n+    __ ushll2(vb[5], __ T8H, vin[4], __ T16B, 0);\n+\n+    \/\/ shift lo byte of copy 1 of the middle stripe into the high byte\n+    __ shl(va[2], __ T8H, va[2], 8);\n+    __ shl(va[3], __ T8H, va[3], 8);\n+    __ shl(vb[2], __ T8H, vb[2], 8);\n+    __ shl(vb[3], __ T8H, vb[3], 8);\n+\n+    \/\/ expand vin[2] into va[6:7] and vin[5] into vb[6:7] but this\n+    \/\/ time pre-shifted by 4 to ensure top bits of input 12-bit int\n+    \/\/ are in bit positions [4..11].\n+    __ ushll(va[6], __ T8H, vin[2], __ T8B, 4);\n+    __ ushll2(va[7], __ T8H, vin[2], __ T16B, 4);\n+    __ ushll(vb[6], __ T8H, vin[5], __ T8B, 4);\n+    __ ushll2(vb[7], __ T8H, vin[5], __ T16B, 4);\n+\n+    \/\/ mask hi 4 bits of the 1st 12-bit int in a pair from copy1 and\n+    \/\/ shift lo 4 bits of the 2nd 12-bit int in a pair to the bottom of\n+    \/\/ copy2\n+    __ andr(va[2], __ T16B, va[2], v31);\n+    __ andr(va[3], __ T16B, va[3], v31);\n+    __ ushr(va[4], __ T8H, va[4], 4);\n+    __ ushr(va[5], __ T8H, va[5], 4);\n+    __ andr(vb[2], __ T16B, vb[2], v31);\n+    __ andr(vb[3], __ T16B, vb[3], v31);\n+    __ ushr(vb[4], __ T8H, vb[4], 4);\n+    __ ushr(vb[5], __ T8H, vb[5], 4);\n+\n+    \/\/ sum hi 4 bits and lo 8 bits of the 1st 12-bit int in each pair and\n+    \/\/ hi 8 bits plus lo 4 bits of the 2nd 12-bit int in each pair\n+    \/\/ n.b. the ordering ensures: i) inputs are consumed before they\n+    \/\/ are overwritten ii) the order of 16-bit results across successive\n+    \/\/ pairs of vectors in va and then vb reflects the order of the\n+    \/\/ corresponding 12-bit inputs\n+    __ addv(va[0], __ T8H, va[0], va[2]);\n+    __ addv(va[2], __ T8H, va[1], va[3]);\n+    __ addv(va[1], __ T8H, va[4], va[6]);\n+    __ addv(va[3], __ T8H, va[5], va[7]);\n+    __ addv(vb[0], __ T8H, vb[0], vb[2]);\n+    __ addv(vb[2], __ T8H, vb[1], vb[3]);\n+    __ addv(vb[1], __ T8H, vb[4], vb[6]);\n+    __ addv(vb[3], __ T8H, vb[5], vb[7]);\n+\n+    \/\/ store 64 results interleaved as shorts\n+    vs_st2_post(vs_front(va), __ T8H, parsed);\n+    vs_st2_post(vs_front(vb), __ T8H, parsed);\n+\n+    __ sub(parsedLength, parsedLength, 64);\n+    __ cmp(parsedLength, (u1)64);\n+    __ br(Assembler::GE, L_loop);\n+    __ cbz(parsedLength, L_end);\n+\n+    \/\/ if anything is left it should be a final 72 bytes of input\n+    \/\/ i.e. a final 48 12-bit values. so we handle this by loading\n+    \/\/ 48 bytes into all 16B lanes of front(vin) and only 24\n+    \/\/ bytes into the lower 8B lane of back(vin)\n+    vs_ld3_post(vs_front(vin), __ T16B, condensed);\n+    vs_ld3(vs_back(vin), __ T8B, condensed);\n+\n+    \/\/ Expand vin[0] into va[0:1], and vin[1] into va[2:3] and va[4:5]\n+    \/\/ n.b. target elements 2 and 3 of va duplicate elements 4 and\n+    \/\/ 5 and target element 2 of vb duplicates element 4.\n+    __ ushll(va[0], __ T8H, vin[0], __ T8B, 0);\n+    __ ushll2(va[1], __ T8H, vin[0], __ T16B, 0);\n+    __ ushll(va[2], __ T8H, vin[1], __ T8B, 0);\n+    __ ushll2(va[3], __ T8H, vin[1], __ T16B, 0);\n+    __ ushll(va[4], __ T8H, vin[1], __ T8B, 0);\n+    __ ushll2(va[5], __ T8H, vin[1], __ T16B, 0);\n+\n+    \/\/ This time expand just the lower 8 lanes\n+    __ ushll(vb[0], __ T8H, vin[3], __ T8B, 0);\n+    __ ushll(vb[2], __ T8H, vin[4], __ T8B, 0);\n+    __ ushll(vb[4], __ T8H, vin[4], __ T8B, 0);\n+\n+    \/\/ shift lo byte of copy 1 of the middle stripe into the high byte\n+    __ shl(va[2], __ T8H, va[2], 8);\n+    __ shl(va[3], __ T8H, va[3], 8);\n+    __ shl(vb[2], __ T8H, vb[2], 8);\n+\n+    \/\/ expand vin[2] into va[6:7] and lower 8 lanes of vin[5] into\n+    \/\/ vb[6] pre-shifted by 4 to ensure top bits of the input 12-bit\n+    \/\/ int are in bit positions [4..11].\n+    __ ushll(va[6], __ T8H, vin[2], __ T8B, 4);\n+    __ ushll2(va[7], __ T8H, vin[2], __ T16B, 4);\n+    __ ushll(vb[6], __ T8H, vin[5], __ T8B, 4);\n+\n+    \/\/ mask hi 4 bits of each 1st 12-bit int in pair from copy1 and\n+    \/\/ shift lo 4 bits of each 2nd 12-bit int in pair to bottom of\n+    \/\/ copy2\n+    __ andr(va[2], __ T16B, va[2], v31);\n+    __ andr(va[3], __ T16B, va[3], v31);\n+    __ ushr(va[4], __ T8H, va[4], 4);\n+    __ ushr(va[5], __ T8H, va[5], 4);\n+    __ andr(vb[2], __ T16B, vb[2], v31);\n+    __ ushr(vb[4], __ T8H, vb[4], 4);\n+\n+\n+\n+    \/\/ sum hi 4 bits and lo 8 bits of each 1st 12-bit int in pair and\n+    \/\/ hi 8 bits plus lo 4 bits of each 2nd 12-bit int in pair\n+\n+    \/\/ n.b. ordering ensures: i) inputs are consumed before they are\n+    \/\/ overwritten ii) order of 16-bit results across succsessive\n+    \/\/ pairs of vectors in va and then lower half of vb reflects order\n+    \/\/ of corresponding 12-bit inputs\n+    __ addv(va[0], __ T8H, va[0], va[2]);\n+    __ addv(va[2], __ T8H, va[1], va[3]);\n+    __ addv(va[1], __ T8H, va[4], va[6]);\n+    __ addv(va[3], __ T8H, va[5], va[7]);\n+    __ addv(vb[0], __ T8H, vb[0], vb[2]);\n+    __ addv(vb[1], __ T8H, vb[4], vb[6]);\n+\n+    \/\/ store 48 results interleaved as shorts\n+    vs_st2_post(vs_front(va), __ T8H, parsed);\n+    vs_st2_post(vs_front(vs_front(vb)), __ T8H, parsed);\n+\n+    __ BIND(L_end);\n+\n+    __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n+    __ mov(r0, zr); \/\/ return 0\n+    __ ret(lr);\n+\n+    return start;\n+  }\n+\n+  \/\/ Kyber Barrett reduce function.\n+  \/\/ Implements\n+  \/\/ static int implKyberBarrettReduce(short[] coeffs) {}\n+  \/\/\n+  \/\/ coeffs (short[256]) = c_rarg0\n+  address generate_kyberBarrettReduce() {\n+\n+    __ align(CodeEntryAlignment);\n+    StubGenStubId stub_id = StubGenStubId::kyberBarrettReduce_id;\n+    StubCodeMark mark(this, stub_id);\n+    address start = __ pc();\n+    __ enter();\n+\n+    const Register coeffs = c_rarg0;\n+\n+    const Register kyberConsts = r10;\n+    const Register result = r11;\n+\n+    \/\/ As above we process 256 sets of values in total i.e. 32 x\n+    \/\/ 8H quadwords. So, we can load, add and store the data in 3\n+    \/\/ groups of 11, 11 and 10 at a time i.e. we need to map sets\n+    \/\/ of 10 or 11 registers. A further constraint is that the\n+    \/\/ mapping needs to skip callee saves. So, we allocate the\n+    \/\/ register sequences using two 8 sequences, two 2 sequences\n+    \/\/ and two single registers.\n+    VSeq<8> vs1_1(0);\n+    VSeq<2> vs1_2(16);\n+    FloatRegister vs1_3 = v28;\n+    VSeq<8> vs2_1(18);\n+    VSeq<2> vs2_2(26);\n+    FloatRegister vs2_3 = v29;\n+\n+    \/\/ we also need a pair of corresponding constant sequences\n+\n+    VSeq<8> vc1_1(30, 0);\n+    VSeq<2> vc1_2(30, 0);\n+    FloatRegister vc1_3 = v30; \/\/ for kyber_q\n+\n+    VSeq<8> vc2_1(31, 0);\n+    VSeq<2> vc2_2(31, 0);\n+    FloatRegister vc2_3 = v31; \/\/ for kyberBarrettMultiplier\n+\n+    __ add(result, coeffs, 0);\n+    __ lea(kyberConsts,\n+             ExternalAddress((address) StubRoutines::aarch64::_kyberConsts));\n+\n+    \/\/ load q and the multiplier for the Barrett reduction\n+    __ add(kyberConsts, kyberConsts, 16);\n+    __ ldpq(vc1_3, vc2_3, kyberConsts);\n+\n+    for (int i = 0; i < 3; i++) {\n+      \/\/ load 80 or 88 coefficients\n+      vs_ldpq_post(vs1_1, coeffs);\n+      vs_ldpq_post(vs1_2, coeffs);\n+      if (i < 2) {\n+        __ ldr(vs1_3, __ Q, __ post(coeffs, 16));\n+      }\n+\n+      \/\/ vs2 <- (2 * vs1 * kyberBarrettMultiplier) >> 16\n+      vs_sqdmulh(vs2_1, __ T8H, vs1_1, vc2_1);\n+      vs_sqdmulh(vs2_2, __ T8H, vs1_2, vc2_2);\n+      if (i < 2) {\n+        __ sqdmulh(vs2_3, __ T8H, vs1_3, vc2_3);\n+      }\n+\n+      \/\/ vs2 <- (vs1 * kyberBarrettMultiplier) >> 26\n+      vs_sshr(vs2_1, __ T8H, vs2_1, 11);\n+      vs_sshr(vs2_2, __ T8H, vs2_2, 11);\n+      if (i < 2) {\n+        __ sshr(vs2_3, __ T8H, vs2_3, 11);\n+      }\n+\n+      \/\/ vs1 <- vs1 - vs2 * kyber_q\n+      vs_mlsv(vs1_1, __ T8H, vs2_1, vc1_1);\n+      vs_mlsv(vs1_2, __ T8H, vs2_2, vc1_2);\n+      if (i < 2) {\n+        __ mlsv(vs1_3, __ T8H, vs2_3, vc1_3);\n+      }\n+\n+      vs_stpq_post(vs1_1, result);\n+      vs_stpq_post(vs1_2, result);\n+      if (i < 2) {\n+        __ str(vs1_3, __ Q, __ post(result, 16));\n+      }\n+    }\n+\n+    __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n+    __ mov(r0, zr); \/\/ return 0\n+    __ ret(lr);\n+\n+    return start;\n+  }\n+\n+\n+  \/\/ Dilithium-specific montmul helper routines that generate parallel\n+  \/\/ code for, respectively, a single 4x4s vector sequence montmul or\n+  \/\/ two such multiplies in a row.\n+\n+  \/\/ Perform 16 32-bit Montgomery multiplications in parallel\n+  void dilithium_montmul16(const VSeq<4>& va, const VSeq<4>& vb, const VSeq<4>& vc,\n+                           const VSeq<4>& vtmp, const VSeq<2>& vq) {\n+    \/\/ Use the helper routine to schedule a 4x4S Montgomery multiply.\n+    \/\/ It will assert that the register use is valid\n+    vs_montmul4(va, vb, vc, __ T4S, vtmp, vq);\n+  }\n+\n+  \/\/ Perform 2x16 32-bit Montgomery multiplications in parallel\n+  void dilithium_montmul32(const VSeq<8>& va, const VSeq<8>& vb, const VSeq<8>& vc,\n+                           const VSeq<4>& vtmp, const VSeq<2>& vq) {\n+    \/\/ Schedule two successive 4x4S multiplies via the montmul helper\n+    \/\/ on the front and back halves of va, vb and vc. The helper will\n+    \/\/ assert that the register use has no overlap conflicts on each\n+    \/\/ individual call but we also need to ensure that the necessary\n+    \/\/ disjoint\/equality constraints are met across both calls.\n+\n+    \/\/ vb, vc, vtmp and vq must be disjoint. va must either be\n+    \/\/ disjoint from all other registers or equal vc\n+\n+    assert(vs_disjoint(vb, vc), \"vb and vc overlap\");\n+    assert(vs_disjoint(vb, vq), \"vb and vq overlap\");\n+    assert(vs_disjoint(vb, vtmp), \"vb and vtmp overlap\");\n+\n+    assert(vs_disjoint(vc, vq), \"vc and vq overlap\");\n+    assert(vs_disjoint(vc, vtmp), \"vc and vtmp overlap\");\n+\n+    assert(vs_disjoint(vq, vtmp), \"vq and vtmp overlap\");\n+\n+    assert(vs_disjoint(va, vc) || vs_same(va, vc), \"va and vc neither disjoint nor equal\");\n+    assert(vs_disjoint(va, vb), \"va and vb overlap\");\n+    assert(vs_disjoint(va, vq), \"va and vq overlap\");\n+    assert(vs_disjoint(va, vtmp), \"va and vtmp overlap\");\n+\n+    \/\/ We multiply the front and back halves of each sequence 4 at a\n+    \/\/ time because\n+    \/\/\n+    \/\/ 1) we are currently only able to get 4-way instruction\n+    \/\/ parallelism at best\n+    \/\/\n+    \/\/ 2) we need registers for the constants in vq and temporary\n+    \/\/ scratch registers to hold intermediate results so vtmp can only\n+    \/\/ be a VSeq<4> which means we only have 4 scratch slots.\n+\n+    vs_montmul4(vs_front(va), vs_front(vb), vs_front(vc), __ T4S, vtmp, vq);\n+    vs_montmul4(vs_back(va), vs_back(vb), vs_back(vc), __ T4S, vtmp, vq);\n+  }\n+\n+  \/\/ Perform combined montmul then add\/sub on 4x4S vectors.\n+  void dilithium_montmul16_sub_add(\n+          const VSeq<4>& va0, const VSeq<4>& va1, const VSeq<4>& vc,\n+          const VSeq<4>& vtmp, const VSeq<2>& vq) {\n@@ -4920,4 +6292,4 @@\n-  \/\/ perform combined add\/sub then montul on 4x4S vectors\n-\n-  void dilithium_sub_add_montmul16(const VSeq<4>& va0, const VSeq<4>& va1, const VSeq<4>& vb,\n-                                   const VSeq<4>& vtmp1, const VSeq<4>& vtmp2, const VSeq<2>& vq) {\n+  \/\/ Perform combined add\/sub then montul on 4x4S vectors.\n+  void dilithium_sub_add_montmul16(\n+          const VSeq<4>& va0, const VSeq<4>& va1, const VSeq<4>& vb,\n+          const VSeq<4>& vtmp1, const VSeq<4>& vtmp2, const VSeq<2>& vq) {\n@@ -4966,1 +6338,1 @@\n-      \/\/ for levels 1 - 4 we simply load 2 x 4 adjacent values at a\n+      \/\/ For levels 1 - 4 we simply load 2 x 4 adjacent values at a\n@@ -4969,1 +6341,1 @@\n-      \/\/ pair instructions with arrangement 4S\n+      \/\/ pair instructions with arrangement 4S.\n@@ -4978,1 +6350,1 @@\n-        vs_montmul32(vs2, vs1, vs2, vtmp, vq);\n+        dilithium_montmul32(vs2, vs1, vs2, vtmp, vq);\n@@ -5032,2 +6404,2 @@\n-    int offsets[4] = {0, 32, 64, 96};\n-    int offsets1[8] = {16, 48, 80, 112, 144, 176, 208, 240 };\n+    int offsets[4] = { 0, 32, 64, 96};\n+    int offsets1[8] = { 16, 48, 80, 112, 144, 176, 208, 240 };\n@@ -5036,1 +6408,2 @@\n-    __ lea(dilithiumConsts, ExternalAddress((address) StubRoutines::aarch64::_dilithiumConsts));\n+    __ lea(dilithiumConsts,\n+             ExternalAddress((address) StubRoutines::aarch64::_dilithiumConsts));\n@@ -5038,1 +6411,1 @@\n-    \/\/ Each level represents one iteration of the outer for loop of the Java version\n+    \/\/ Each level represents one iteration of the outer for loop of the Java version.\n@@ -5045,1 +6418,1 @@\n-    \/\/ at level 5 the coefficients we need to combine with the zetas\n+    \/\/ At level 5 the coefficients we need to combine with the zetas\n@@ -5059,1 +6432,1 @@\n-      vs_montmul32(vs2, vs1, vs2, vtmp, vq);\n+      dilithium_montmul32(vs2, vs1, vs2, vtmp, vq);\n@@ -5071,1 +6444,1 @@\n-    \/\/ at level 6 the coefficients we need to combine with the zetas\n+    \/\/ At level 6 the coefficients we need to combine with the zetas\n@@ -5099,1 +6472,1 @@\n-    \/\/ at level 7 the coefficients we need to combine with the zetas\n+    \/\/ At level 7 the coefficients we need to combine with the zetas\n@@ -5171,1 +6544,1 @@\n-      \/\/ for levels 3 - 7 we simply load 2 x 4 adjacent values at a\n+      \/\/ For levels 3 - 7 we simply load 2 x 4 adjacent values at a\n@@ -5174,1 +6547,1 @@\n-      \/\/ pair instructions with arrangement 4S\n+      \/\/ pair instructions with arrangement 4S.\n@@ -5191,1 +6564,1 @@\n-        vs_montmul32(vs2, vs1, vs2, vtmp, vq);\n+        dilithium_montmul32(vs2, vs1, vs2, vtmp, vq);\n@@ -5242,1 +6615,2 @@\n-    __ lea(dilithiumConsts, ExternalAddress((address) StubRoutines::aarch64::_dilithiumConsts));\n+    __ lea(dilithiumConsts,\n+             ExternalAddress((address) StubRoutines::aarch64::_dilithiumConsts));\n@@ -5245,1 +6619,0 @@\n-    \/\/ level0\n@@ -5251,1 +6624,1 @@\n-    \/\/ load and store the values using an ld2\/st2 with arrangement 4S\n+    \/\/ load and store the values using an ld2\/st2 with arrangement 4S.\n@@ -5273,1 +6646,1 @@\n-    \/\/ values an ld2\/st2 with arrangement 2D\n+    \/\/ values an ld2\/st2 with arrangement 2D.\n@@ -5309,1 +6682,1 @@\n-      vs_montmul32(vs2, vs1, vs2, vtmp, vq);\n+      dilithium_montmul32(vs2, vs1, vs2, vtmp, vq);\n@@ -5322,1 +6695,0 @@\n-\n@@ -5356,1 +6728,2 @@\n-    __ lea(dilithiumConsts, ExternalAddress((address) StubRoutines::aarch64::_dilithiumConsts));\n+    __ lea(dilithiumConsts,\n+             ExternalAddress((address) StubRoutines::aarch64::_dilithiumConsts));\n@@ -5373,1 +6746,1 @@\n-    vs_montmul32(vs2, vs1, vs2, vtmp, vq);\n+    dilithium_montmul32(vs2, vs1, vs2, vtmp, vq);\n@@ -5375,1 +6748,1 @@\n-    vs_montmul32(vs2, vrsquare, vs2, vtmp, vq);\n+    dilithium_montmul32(vs2, vrsquare, vs2, vtmp, vq);\n@@ -5388,1 +6761,0 @@\n-\n@@ -5416,1 +6788,1 @@\n-    VSeq<4> vtmp = vs_front(vs3);         \/\/ n.b. tmp registers overlap vs3\n+    VSeq<4> vtmp = vs_front(vs3);      \/\/ n.b. tmp registers overlap vs3\n@@ -5422,1 +6794,2 @@\n-    __ lea(dilithiumConsts, ExternalAddress((address) StubRoutines::aarch64::_dilithiumConsts));\n+    __ lea(dilithiumConsts,\n+             ExternalAddress((address) StubRoutines::aarch64::_dilithiumConsts));\n@@ -5436,1 +6809,1 @@\n-    vs_montmul32(vs2, vconst, vs2, vtmp, vq);\n+    dilithium_montmul32(vs2, vconst, vs2, vtmp, vq);\n@@ -5449,1 +6822,0 @@\n-\n@@ -5480,1 +6852,2 @@\n-    VSeq<4> vs1(0), vs2(4), vs3(8); \/\/ 6 independent sets of 4x4s values\n+    \/\/ 6 independent sets of 4x4s values\n+    VSeq<4> vs1(0), vs2(4), vs3(8);\n@@ -5482,1 +6855,3 @@\n-    VSeq<4> one(25, 0);            \/\/ 7 constants for cross-multiplying\n+\n+    \/\/ 7 constants for cross-multiplying\n+    VSeq<4> one(25, 0);\n@@ -5492,1 +6867,2 @@\n-    __ lea(dilithiumConsts, ExternalAddress((address) StubRoutines::aarch64::_dilithiumConsts));\n+    __ lea(dilithiumConsts,\n+             ExternalAddress((address) StubRoutines::aarch64::_dilithiumConsts));\n@@ -5589,1 +6965,0 @@\n-\n@@ -5605,1 +6980,0 @@\n-\n@@ -10006,0 +11380,10 @@\n+    if (UseKyberIntrinsics) {\n+      StubRoutines::_kyberNtt = generate_kyberNtt();\n+      StubRoutines::_kyberInverseNtt = generate_kyberInverseNtt();\n+      StubRoutines::_kyberNttMult = generate_kyberNttMult();\n+      StubRoutines::_kyberAddPoly_2 = generate_kyberAddPoly_2();\n+      StubRoutines::_kyberAddPoly_3 = generate_kyberAddPoly_3();\n+      StubRoutines::_kyber12To16 = generate_kyber12To16();\n+      StubRoutines::_kyberBarrettReduce = generate_kyberBarrettReduce();\n+    }\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/stubGenerator_aarch64.cpp","additions":1458,"deletions":74,"binary":false,"changes":1532,"status":"modified"},{"patch":"@@ -51,0 +51,11 @@\n+ATTRIBUTE_ALIGNED(64) uint16_t StubRoutines::aarch64::_kyberConsts[] =\n+{\n+    \/\/ Because we sometimes load these in pairs, montQInvModR, kyber_q\n+    \/\/ and kyberBarrettMultiplier should stay together and in this order.\n+    0xF301, 0xF301, 0xF301, 0xF301, 0xF301, 0xF301, 0xF301, 0xF301, \/\/ montQInvModR\n+    0x0D01, 0x0D01, 0x0D01, 0x0D01, 0x0D01, 0x0D01, 0x0D01, 0x0D01, \/\/ kyber_q\n+    0x4EBF, 0x4EBF, 0x4EBF, 0x4EBF, 0x4EBF, 0x4EBF, 0x4EBF, 0x4EBF, \/\/ kyberBarrettMultiplier\n+    0x0200, 0x0200, 0x0200, 0x0200, 0x0200, 0x0200, 0x0200, 0x0200, \/\/ toMont((kyber_n \/ 2)^-1 (mod kyber_q))\n+    0x0549, 0x0549, 0x0549, 0x0549, 0x0549, 0x0549, 0x0549, 0x0549  \/\/ montRSquareModQ\n+};\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/stubRoutines_aarch64.cpp","additions":11,"deletions":0,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -113,0 +113,1 @@\n+  static uint16_t  _kyberConsts[];\n","filename":"src\/hotspot\/cpu\/aarch64\/stubRoutines_aarch64.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -417,0 +417,11 @@\n+  if (_features & CPU_ASIMD) {\n+      if (FLAG_IS_DEFAULT(UseKyberIntrinsics)) {\n+          UseKyberIntrinsics = true;\n+      }\n+  } else if (UseKyberIntrinsics) {\n+      if (!FLAG_IS_DEFAULT(UseKyberIntrinsics)) {\n+          warning(\"Kyber intrinsics require ASIMD instructions\");\n+      }\n+      FLAG_SET_DEFAULT(UseKyberIntrinsics, false);\n+  }\n+\n@@ -423,1 +434,1 @@\n-          warning(\"Dilithium intrinsic requires ASIMD instructions\");\n+          warning(\"Dilithium intrinsics require ASIMD instructions\");\n@@ -706,0 +717,1 @@\n+  fprintf(stderr, \"_features_string = \\\"%s\\\"\", _features_string);\n","filename":"src\/hotspot\/cpu\/aarch64\/vm_version_aarch64.cpp","additions":13,"deletions":1,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -491,0 +491,8 @@\n+  case vmIntrinsics::_kyberNtt:\n+  case vmIntrinsics::_kyberInverseNtt:\n+  case vmIntrinsics::_kyberNttMult:\n+  case vmIntrinsics::_kyberAddPoly_2:\n+  case vmIntrinsics::_kyberAddPoly_3:\n+  case vmIntrinsics::_kyber12To16:\n+  case vmIntrinsics::_kyberBarrettReduce:\n+    if (!UseKyberIntrinsics) return true;\n","filename":"src\/hotspot\/share\/classfile\/vmIntrinsics.cpp","additions":8,"deletions":0,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -572,0 +572,21 @@\n+  \/* support for com.sun.crypto.provider.ML_KEM *\/                                                                      \\\n+  do_class(com_sun_crypto_provider_ML_KEM,      \"com\/sun\/crypto\/provider\/ML_KEM\")                                       \\\n+   do_signature(SaSaSaSaI_signature, \"([S[S[S[S)I\")                                                                     \\\n+   do_signature(BaISaII_signature, \"([BI[SI)I\")                                                                         \\\n+   do_signature(SaSaSaI_signature, \"([S[S[S)I\")                                                                         \\\n+   do_signature(SaSaI_signature, \"([S[S)I\")                                                                             \\\n+   do_signature(SaI_signature, \"([S)I\")                                                                                 \\\n+   do_name(kyberAddPoly_name,                             \"implKyberAddPoly\")                                           \\\n+  do_intrinsic(_kyberNtt, com_sun_crypto_provider_ML_KEM, kyberNtt_name, SaSaI_signature, F_S)                          \\\n+   do_name(kyberNtt_name,                                  \"implKyberNtt\")                                              \\\n+  do_intrinsic(_kyberInverseNtt, com_sun_crypto_provider_ML_KEM, kyberInverseNtt_name, SaSaI_signature, F_S)            \\\n+   do_name(kyberInverseNtt_name,                           \"implKyberInverseNtt\")                                       \\\n+  do_intrinsic(_kyberNttMult, com_sun_crypto_provider_ML_KEM, kyberNttMult_name, SaSaSaSaI_signature, F_S)              \\\n+   do_name(kyberNttMult_name,                              \"implKyberNttMult\")                                          \\\n+  do_intrinsic(_kyberAddPoly_2, com_sun_crypto_provider_ML_KEM, kyberAddPoly_name, SaSaSaI_signature, F_S)              \\\n+  do_intrinsic(_kyberAddPoly_3, com_sun_crypto_provider_ML_KEM, kyberAddPoly_name, SaSaSaSaI_signature, F_S)            \\\n+  do_intrinsic(_kyber12To16, com_sun_crypto_provider_ML_KEM, kyber12To16_name, BaISaII_signature, F_S)                  \\\n+   do_name(kyber12To16_name,                             \"implKyber12To16\")                                             \\\n+  do_intrinsic(_kyberBarrettReduce, com_sun_crypto_provider_ML_KEM, kyberBarrettReduce_name, SaI_signature, F_S)        \\\n+   do_name(kyberBarrettReduce_name,                        \"implKyberBarrettReduce\")                                    \\\n+                                                                                                                        \\\n","filename":"src\/hotspot\/share\/classfile\/vmIntrinsics.hpp","additions":21,"deletions":0,"binary":false,"changes":21,"status":"modified"},{"patch":"@@ -398,0 +398,7 @@\n+  static_field(StubRoutines,                _kyberNtt,                                        address)                               \\\n+  static_field(StubRoutines,                _kyberInverseNtt,                                 address)                               \\\n+  static_field(StubRoutines,                _kyberNttMult,                                    address)                               \\\n+  static_field(StubRoutines,                _kyberAddPoly_2,                                  address)                               \\\n+  static_field(StubRoutines,                _kyberAddPoly_3,                                  address)                               \\\n+  static_field(StubRoutines,                _kyber12To16,                                     address)                               \\\n+  static_field(StubRoutines,                _kyberBarrettReduce,                              address)                               \\\n","filename":"src\/hotspot\/share\/jvmci\/vmStructs_jvmci.cpp","additions":7,"deletions":0,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -795,0 +795,7 @@\n+  case vmIntrinsics::_kyberNtt:\n+  case vmIntrinsics::_kyberInverseNtt:\n+  case vmIntrinsics::_kyberNttMult:\n+  case vmIntrinsics::_kyberAddPoly_2:\n+  case vmIntrinsics::_kyberAddPoly_3:\n+  case vmIntrinsics::_kyber12To16:\n+  case vmIntrinsics::_kyberBarrettReduce:\n","filename":"src\/hotspot\/share\/opto\/c2compiler.cpp","additions":7,"deletions":0,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -2195,0 +2195,7 @@\n+                  strcmp(call->as_CallLeaf()->_name, \"kyberNtt\") == 0 ||\n+                  strcmp(call->as_CallLeaf()->_name, \"kyberInverseNtt\") == 0 ||\n+                  strcmp(call->as_CallLeaf()->_name, \"kyberNttMult\") == 0 ||\n+                  strcmp(call->as_CallLeaf()->_name, \"kyberAddPoly_2\") == 0 ||\n+                  strcmp(call->as_CallLeaf()->_name, \"kyberAddPoly_3\") == 0 ||\n+                  strcmp(call->as_CallLeaf()->_name, \"kyber12To16\") == 0 ||\n+                  strcmp(call->as_CallLeaf()->_name, \"kyberBarrettReduce\") == 0 ||\n","filename":"src\/hotspot\/share\/opto\/escape.cpp","additions":7,"deletions":0,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -629,0 +629,14 @@\n+  case vmIntrinsics::_kyberNtt:\n+    return inline_kyberNtt();\n+  case vmIntrinsics::_kyberInverseNtt:\n+    return inline_kyberInverseNtt();\n+  case vmIntrinsics::_kyberNttMult:\n+    return inline_kyberNttMult();\n+  case vmIntrinsics::_kyberAddPoly_2:\n+    return inline_kyberAddPoly_2();\n+  case vmIntrinsics::_kyberAddPoly_3:\n+    return inline_kyberAddPoly_3();\n+  case vmIntrinsics::_kyber12To16:\n+    return inline_kyber12To16();\n+  case vmIntrinsics::_kyberBarrettReduce:\n+    return inline_kyberBarrettReduce();\n@@ -7655,0 +7669,239 @@\n+\/\/------------------------------inline_kyberNtt\n+bool LibraryCallKit::inline_kyberNtt() {\n+  address stubAddr;\n+  const char *stubName;\n+  assert(UseKyberIntrinsics, \"need Kyber intrinsics support\");\n+  assert(callee()->signature()->size() == 2, \"kyberNtt has 2 parameters\");\n+\n+  stubAddr = StubRoutines::kyberNtt();\n+  stubName = \"kyberNtt\";\n+  if (!stubAddr) return false;\n+\n+  Node* coeffs          = argument(0);\n+  Node* ntt_zetas        = argument(1);\n+\n+  coeffs = must_be_not_null(coeffs, true);\n+  ntt_zetas = must_be_not_null(ntt_zetas, true);\n+\n+  Node* coeffs_start  = array_element_address(coeffs, intcon(0), T_SHORT);\n+  assert(coeffs_start, \"coeffs is null\");\n+  Node* ntt_zetas_start  = array_element_address(ntt_zetas, intcon(0), T_SHORT);\n+  assert(ntt_zetas_start, \"ntt_zetas is null\");\n+  Node* kyberNtt = make_runtime_call(RC_LEAF|RC_NO_FP,\n+                                  OptoRuntime::kyberNtt_Type(),\n+                                  stubAddr, stubName, TypePtr::BOTTOM,\n+                                  coeffs_start, ntt_zetas_start);\n+  \/\/ return an int\n+  Node* retvalue = _gvn.transform(new ProjNode(kyberNtt, TypeFunc::Parms));\n+  set_result(retvalue);\n+  return true;\n+}\n+\n+\/\/------------------------------inline_kyberInverseNtt\n+bool LibraryCallKit::inline_kyberInverseNtt() {\n+  address stubAddr;\n+  const char *stubName;\n+  assert(UseKyberIntrinsics, \"need Kyber intrinsics support\");\n+  assert(callee()->signature()->size() == 2, \"kyberInverseNtt has 2 parameters\");\n+\n+  stubAddr = StubRoutines::kyberInverseNtt();\n+  stubName = \"kyberInverseNtt\";\n+  if (!stubAddr) return false;\n+\n+  Node* coeffs          = argument(0);\n+  Node* zetas           = argument(1);\n+\n+  coeffs = must_be_not_null(coeffs, true);\n+  zetas = must_be_not_null(zetas, true);\n+\n+  Node* coeffs_start  = array_element_address(coeffs, intcon(0), T_SHORT);\n+  assert(coeffs_start, \"coeffs is null\");\n+  Node* zetas_start  = array_element_address(zetas, intcon(0), T_SHORT);\n+  assert(zetas_start, \"inverseNtt_zetas is null\");\n+  Node* kyberInverseNtt = make_runtime_call(RC_LEAF|RC_NO_FP,\n+                                  OptoRuntime::kyberInverseNtt_Type(),\n+                                  stubAddr, stubName, TypePtr::BOTTOM,\n+                                  coeffs_start, zetas_start);\n+\n+  \/\/ return an int\n+  Node* retvalue = _gvn.transform(new ProjNode(kyberInverseNtt, TypeFunc::Parms));\n+  set_result(retvalue);\n+  return true;\n+}\n+\n+\/\/------------------------------inline_kyberNttMult\n+bool LibraryCallKit::inline_kyberNttMult() {\n+  address stubAddr;\n+  const char *stubName;\n+  assert(UseKyberIntrinsics, \"need Kyber intrinsics support\");\n+  assert(callee()->signature()->size() == 4, \"kyberNttMult has 4 parameters\");\n+\n+  stubAddr = StubRoutines::kyberNttMult();\n+  stubName = \"kyberNttMult\";\n+  if (!stubAddr) return false;\n+\n+  Node* result          = argument(0);\n+  Node* ntta            = argument(1);\n+  Node* nttb            = argument(2);\n+  Node* zetas           = argument(3);\n+\n+  result = must_be_not_null(result, true);\n+  ntta = must_be_not_null(ntta, true);\n+  nttb = must_be_not_null(nttb, true);\n+  zetas = must_be_not_null(zetas, true);\n+  Node* result_start  = array_element_address(result, intcon(0), T_SHORT);\n+  assert(result_start, \"result is null\");\n+  Node* ntta_start  = array_element_address(ntta, intcon(0), T_SHORT);\n+  assert(ntta_start, \"ntta is null\");\n+  Node* nttb_start  = array_element_address(nttb, intcon(0), T_SHORT);\n+  assert(nttb_start, \"nttb is null\");\n+  Node* zetas_start  = array_element_address(zetas, intcon(0), T_SHORT);\n+  assert(zetas_start, \"nttMult_zetas is null\");\n+  Node* kyberNttMult = make_runtime_call(RC_LEAF|RC_NO_FP,\n+                                  OptoRuntime::kyberNttMult_Type(),\n+                                  stubAddr, stubName, TypePtr::BOTTOM,\n+                                  result_start, ntta_start, nttb_start,\n+                                  zetas_start);\n+\n+  \/\/ return an int\n+  Node* retvalue = _gvn.transform(new ProjNode(kyberNttMult, TypeFunc::Parms));\n+  set_result(retvalue);\n+\n+  return true;\n+}\n+\n+\/\/------------------------------inline_kyberAddPoly_2\n+bool LibraryCallKit::inline_kyberAddPoly_2() {\n+  address stubAddr;\n+  const char *stubName;\n+  assert(UseKyberIntrinsics, \"need Kyber intrinsics support\");\n+  assert(callee()->signature()->size() == 3, \"kyberAddPoly_2 has 3 parameters\");\n+\n+  stubAddr = StubRoutines::kyberAddPoly_2();\n+  stubName = \"kyberAddPoly_2\";\n+  if (!stubAddr) return false;\n+\n+  Node* result          = argument(0);\n+  Node* a               = argument(1);\n+  Node* b               = argument(2);\n+\n+  result = must_be_not_null(result, true);\n+  a = must_be_not_null(a, true);\n+  b = must_be_not_null(b, true);\n+\n+  Node* result_start  = array_element_address(result, intcon(0), T_SHORT);\n+  assert(result_start, \"result is null\");\n+  Node* a_start  = array_element_address(a, intcon(0), T_SHORT);\n+  assert(a_start, \"a is null\");\n+  Node* b_start  = array_element_address(b, intcon(0), T_SHORT);\n+  assert(b_start, \"b is null\");\n+  Node* kyberAddPoly_2 = make_runtime_call(RC_LEAF|RC_NO_FP,\n+                                  OptoRuntime::kyberAddPoly_2_Type(),\n+                                  stubAddr, stubName, TypePtr::BOTTOM,\n+                                  result_start, a_start, b_start);\n+  \/\/ return an int\n+  Node* retvalue = _gvn.transform(new ProjNode(kyberAddPoly_2, TypeFunc::Parms));\n+  set_result(retvalue);\n+  return true;\n+}\n+\n+\/\/------------------------------inline_kyberAddPoly_3\n+bool LibraryCallKit::inline_kyberAddPoly_3() {\n+  address stubAddr;\n+  const char *stubName;\n+  assert(UseKyberIntrinsics, \"need Kyber intrinsics support\");\n+  assert(callee()->signature()->size() == 4, \"kyberAddPoly_3 has 4 parameters\");\n+\n+  stubAddr = StubRoutines::kyberAddPoly_3();\n+  stubName = \"kyberAddPoly_3\";\n+  if (!stubAddr) return false;\n+\n+  Node* result          = argument(0);\n+  Node* a               = argument(1);\n+  Node* b               = argument(2);\n+  Node* c               = argument(3);\n+\n+  result = must_be_not_null(result, true);\n+  a = must_be_not_null(a, true);\n+  b = must_be_not_null(b, true);\n+  c = must_be_not_null(c, true);\n+\n+  Node* result_start  = array_element_address(result, intcon(0), T_SHORT);\n+  assert(result_start, \"result is null\");\n+  Node* a_start  = array_element_address(a, intcon(0), T_SHORT);\n+  assert(a_start, \"a is null\");\n+  Node* b_start  = array_element_address(b, intcon(0), T_SHORT);\n+  assert(b_start, \"b is null\");\n+  Node* c_start  = array_element_address(c, intcon(0), T_SHORT);\n+  assert(c_start, \"c is null\");\n+  Node* kyberAddPoly_3 = make_runtime_call(RC_LEAF|RC_NO_FP,\n+                                  OptoRuntime::kyberAddPoly_3_Type(),\n+                                  stubAddr, stubName, TypePtr::BOTTOM,\n+                                  result_start, a_start, b_start, c_start);\n+  \/\/ return an int\n+  Node* retvalue = _gvn.transform(new ProjNode(kyberAddPoly_3, TypeFunc::Parms));\n+  set_result(retvalue);\n+  return true;\n+}\n+\n+\/\/------------------------------inline_kyber12To16\n+bool LibraryCallKit::inline_kyber12To16() {\n+  address stubAddr;\n+  const char *stubName;\n+  assert(UseKyberIntrinsics, \"need Kyber intrinsics support\");\n+  assert(callee()->signature()->size() == 4, \"kyber12To16 has 4 parameters\");\n+\n+  stubAddr = StubRoutines::kyber12To16();\n+  stubName = \"kyber12To16\";\n+  if (!stubAddr) return false;\n+\n+  Node* condensed       = argument(0);\n+  Node* condensedOffs   = argument(1);\n+  Node* parsed          = argument(2);\n+  Node* parsedLength    = argument(3);\n+\n+  condensed = must_be_not_null(condensed, true);\n+  parsed = must_be_not_null(parsed, true);\n+\n+  Node* condensed_start  = array_element_address(condensed, intcon(0), T_BYTE);\n+  assert(condensed_start, \"condensed is null\");\n+  Node* parsed_start  = array_element_address(parsed, intcon(0), T_SHORT);\n+  assert(parsed_start, \"parsed is null\");\n+  Node* kyber12To16 = make_runtime_call(RC_LEAF|RC_NO_FP,\n+                                  OptoRuntime::kyber12To16_Type(),\n+                                  stubAddr, stubName, TypePtr::BOTTOM,\n+                                  condensed_start, condensedOffs, parsed_start, parsedLength);\n+  \/\/ return an int\n+  Node* retvalue = _gvn.transform(new ProjNode(kyber12To16, TypeFunc::Parms));\n+  set_result(retvalue);\n+  return true;\n+\n+}\n+\n+\/\/------------------------------inline_kyberBarrettReduce\n+bool LibraryCallKit::inline_kyberBarrettReduce() {\n+  address stubAddr;\n+  const char *stubName;\n+  assert(UseKyberIntrinsics, \"need Kyber intrinsics support\");\n+  assert(callee()->signature()->size() == 1, \"kyberBarrettReduce has 1 parameters\");\n+\n+  stubAddr = StubRoutines::kyberBarrettReduce();\n+  stubName = \"kyberBarrettReduce\";\n+  if (!stubAddr) return false;\n+\n+  Node* coeffs          = argument(0);\n+\n+  coeffs = must_be_not_null(coeffs, true);\n+\n+  Node* coeffs_start  = array_element_address(coeffs, intcon(0), T_SHORT);\n+  assert(coeffs_start, \"coeffs is null\");\n+  Node* kyberBarrettReduce = make_runtime_call(RC_LEAF|RC_NO_FP,\n+                                  OptoRuntime::kyberBarrettReduce_Type(),\n+                                  stubAddr, stubName, TypePtr::BOTTOM,\n+                                  coeffs_start);\n+  \/\/ return an int\n+  Node* retvalue = _gvn.transform(new ProjNode(kyberBarrettReduce, TypeFunc::Parms));\n+  set_result(retvalue);\n+  return true;\n+}\n+\n@@ -7711,1 +7964,0 @@\n-\n@@ -7732,0 +7984,1 @@\n+  Node* zetas           = argument(3);\n@@ -7736,0 +7989,1 @@\n+  zetas = must_be_not_null(zetas, true);\n","filename":"src\/hotspot\/share\/opto\/library_call.cpp","additions":255,"deletions":1,"binary":false,"changes":256,"status":"modified"},{"patch":"@@ -318,0 +318,7 @@\n+  bool inline_kyberNtt();\n+  bool inline_kyberInverseNtt();\n+  bool inline_kyberNttMult();\n+  bool inline_kyberAddPoly_2();\n+  bool inline_kyberAddPoly_3();\n+  bool inline_kyber12To16();\n+  bool inline_kyberBarrettReduce();\n","filename":"src\/hotspot\/share\/opto\/library_call.hpp","additions":7,"deletions":0,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -243,1 +243,7 @@\n-\n+const TypeFunc* OptoRuntime::_kyberNtt_Type                       = nullptr;\n+const TypeFunc* OptoRuntime::_kyberInverseNtt_Type                = nullptr;\n+const TypeFunc* OptoRuntime::_kyberNttMult_Type                   = nullptr;\n+const TypeFunc* OptoRuntime::_kyberAddPoly_2_Type                 = nullptr;\n+const TypeFunc* OptoRuntime::_kyberAddPoly_3_Type                 = nullptr;\n+const TypeFunc* OptoRuntime::_kyber12To16_Type                    = nullptr;\n+const TypeFunc* OptoRuntime::_kyberBarrettReduce_Type             = nullptr;\n@@ -249,1 +255,0 @@\n-\n@@ -1410,0 +1415,140 @@\n+\/\/ Kyber NTT function\n+static const TypeFunc* make_kyberNtt_Type() {\n+    int argcnt = 2;\n+\n+    const Type** fields = TypeTuple::fields(argcnt);\n+    int argp = TypeFunc::Parms;\n+    fields[argp++] = TypePtr::NOTNULL;      \/\/ coeffs\n+    fields[argp++] = TypePtr::NOTNULL;      \/\/ NTT zetas\n+\n+    assert(argp == TypeFunc::Parms + argcnt, \"correct decoding\");\n+    const TypeTuple* domain = TypeTuple::make(TypeFunc::Parms + argcnt, fields);\n+\n+    \/\/ result type needed\n+    fields = TypeTuple::fields(1);\n+    fields[TypeFunc::Parms + 0] = TypeInt::INT;\n+    const TypeTuple* range = TypeTuple::make(TypeFunc::Parms + 1, fields);\n+    return TypeFunc::make(domain, range);\n+}\n+\n+\/\/ Kyber inverse NTT function\n+static const TypeFunc* make_kyberInverseNtt_Type() {\n+    int argcnt = 2;\n+\n+    const Type** fields = TypeTuple::fields(argcnt);\n+    int argp = TypeFunc::Parms;\n+    fields[argp++] = TypePtr::NOTNULL;      \/\/ coeffs\n+    fields[argp++] = TypePtr::NOTNULL;      \/\/ inverse NTT zetas\n+\n+    assert(argp == TypeFunc::Parms + argcnt, \"correct decoding\");\n+    const TypeTuple* domain = TypeTuple::make(TypeFunc::Parms + argcnt, fields);\n+\n+    \/\/ result type needed\n+    fields = TypeTuple::fields(1);\n+    fields[TypeFunc::Parms + 0] = TypeInt::INT;\n+    const TypeTuple* range = TypeTuple::make(TypeFunc::Parms + 1, fields);\n+    return TypeFunc::make(domain, range);\n+}\n+\n+\/\/ Kyber NTT multiply function\n+static const TypeFunc* make_kyberNttMult_Type() {\n+    int argcnt = 4;\n+\n+    const Type** fields = TypeTuple::fields(argcnt);\n+    int argp = TypeFunc::Parms;\n+    fields[argp++] = TypePtr::NOTNULL;      \/\/ result\n+    fields[argp++] = TypePtr::NOTNULL;      \/\/ ntta\n+    fields[argp++] = TypePtr::NOTNULL;      \/\/ nttb\n+    fields[argp++] = TypePtr::NOTNULL;      \/\/ NTT multiply zetas\n+\n+    assert(argp == TypeFunc::Parms + argcnt, \"correct decoding\");\n+    const TypeTuple* domain = TypeTuple::make(TypeFunc::Parms + argcnt, fields);\n+\n+    \/\/ result type needed\n+    fields = TypeTuple::fields(1);\n+    fields[TypeFunc::Parms + 0] = TypeInt::INT;\n+    const TypeTuple* range = TypeTuple::make(TypeFunc::Parms + 1, fields);\n+    return TypeFunc::make(domain, range);\n+}\n+\/\/ Kyber add 2 polynomials function\n+static const TypeFunc* make_kyberAddPoly_2_Type() {\n+    int argcnt = 3;\n+\n+    const Type** fields = TypeTuple::fields(argcnt);\n+    int argp = TypeFunc::Parms;\n+    fields[argp++] = TypePtr::NOTNULL;      \/\/ result\n+    fields[argp++] = TypePtr::NOTNULL;      \/\/ a\n+    fields[argp++] = TypePtr::NOTNULL;      \/\/ b\n+\n+    assert(argp == TypeFunc::Parms + argcnt, \"correct decoding\");\n+    const TypeTuple* domain = TypeTuple::make(TypeFunc::Parms + argcnt, fields);\n+\n+    \/\/ result type needed\n+    fields = TypeTuple::fields(1);\n+    fields[TypeFunc::Parms + 0] = TypeInt::INT;\n+    const TypeTuple* range = TypeTuple::make(TypeFunc::Parms + 1, fields);\n+    return TypeFunc::make(domain, range);\n+}\n+\n+\n+\/\/ Kyber add 3 polynomials function\n+static const TypeFunc* make_kyberAddPoly_3_Type() {\n+    int argcnt = 4;\n+\n+    const Type** fields = TypeTuple::fields(argcnt);\n+    int argp = TypeFunc::Parms;\n+    fields[argp++] = TypePtr::NOTNULL;      \/\/ result\n+    fields[argp++] = TypePtr::NOTNULL;      \/\/ a\n+    fields[argp++] = TypePtr::NOTNULL;      \/\/ b\n+    fields[argp++] = TypePtr::NOTNULL;      \/\/ c\n+\n+    assert(argp == TypeFunc::Parms + argcnt, \"correct decoding\");\n+    const TypeTuple* domain = TypeTuple::make(TypeFunc::Parms + argcnt, fields);\n+\n+    \/\/ result type needed\n+    fields = TypeTuple::fields(1);\n+    fields[TypeFunc::Parms + 0] = TypeInt::INT;\n+    const TypeTuple* range = TypeTuple::make(TypeFunc::Parms + 1, fields);\n+    return TypeFunc::make(domain, range);\n+}\n+\n+\n+\/\/ Kyber XOF output parsing into polynomial coefficients candidates\n+\/\/ or decompress(12,...) function\n+static const TypeFunc* make_kyber12To16_Type() {\n+    int argcnt = 4;\n+\n+    const Type** fields = TypeTuple::fields(argcnt);\n+    int argp = TypeFunc::Parms;\n+    fields[argp++] = TypePtr::NOTNULL;      \/\/ condensed\n+    fields[argp++] = TypeInt::INT;          \/\/ condensedOffs\n+    fields[argp++] = TypePtr::NOTNULL;      \/\/ parsed\n+    fields[argp++] = TypeInt::INT;          \/\/ parsedLength\n+\n+    assert(argp == TypeFunc::Parms + argcnt, \"correct decoding\");\n+    const TypeTuple* domain = TypeTuple::make(TypeFunc::Parms + argcnt, fields);\n+\n+    \/\/ result type needed\n+    fields = TypeTuple::fields(1);\n+    fields[TypeFunc::Parms + 0] = TypeInt::INT;\n+    const TypeTuple* range = TypeTuple::make(TypeFunc::Parms + 1, fields);\n+    return TypeFunc::make(domain, range);\n+}\n+\n+\/\/ Kyber Barrett reduce function\n+static const TypeFunc* make_kyberBarrettReduce_Type() {\n+    int argcnt = 1;\n+\n+    const Type** fields = TypeTuple::fields(argcnt);\n+    int argp = TypeFunc::Parms;\n+    fields[argp++] = TypePtr::NOTNULL;      \/\/ coeffs\n+    assert(argp == TypeFunc::Parms + argcnt, \"correct decoding\");\n+    const TypeTuple* domain = TypeTuple::make(TypeFunc::Parms + argcnt, fields);\n+\n+    \/\/ result type needed\n+    fields = TypeTuple::fields(1);\n+    fields[TypeFunc::Parms + 0] = TypeInt::INT;\n+    const TypeTuple* range = TypeTuple::make(TypeFunc::Parms + 1, fields);\n+    return TypeFunc::make(domain, range);\n+}\n+\n@@ -2122,1 +2267,7 @@\n-\n+  _kyberNtt_Type                      = make_kyberNtt_Type();\n+  _kyberInverseNtt_Type               = make_kyberInverseNtt_Type();\n+  _kyberNttMult_Type                  = make_kyberNttMult_Type();\n+  _kyberAddPoly_2_Type                = make_kyberAddPoly_2_Type();\n+  _kyberAddPoly_3_Type                = make_kyberAddPoly_3_Type();\n+  _kyber12To16_Type                   = make_kyber12To16_Type();\n+  _kyberBarrettReduce_Type            = make_kyberBarrettReduce_Type();\n@@ -2128,1 +2279,0 @@\n-\n","filename":"src\/hotspot\/share\/opto\/runtime.cpp","additions":154,"deletions":4,"binary":false,"changes":158,"status":"modified"},{"patch":"@@ -183,0 +183,7 @@\n+  static const TypeFunc* _kyberNtt_Type;\n+  static const TypeFunc* _kyberInverseNtt_Type;\n+  static const TypeFunc* _kyberNttMult_Type;\n+  static const TypeFunc* _kyberAddPoly_2_Type;\n+  static const TypeFunc* _kyberAddPoly_3_Type;\n+  static const TypeFunc* _kyber12To16_Type;\n+  static const TypeFunc* _kyberBarrettReduce_Type;\n@@ -471,0 +478,4 @@\n+\/\/  static const TypeFunc* digestBase_implCompress_Type(bool is_sha3);\n+\/\/  static const TypeFunc* digestBase_implCompressMB_Type(bool is_sha3);\n+\/\/  static const TypeFunc* double_keccak_Type();\n+\n@@ -587,0 +598,35 @@\n+  static const TypeFunc* kyberNtt_Type() {\n+    assert(_kyberNtt_Type != nullptr, \"should be initialized\");\n+    return _kyberNtt_Type;\n+  }\n+\n+  static const TypeFunc* kyberInverseNtt_Type() {\n+    assert(_kyberInverseNtt_Type != nullptr, \"should be initialized\");\n+    return _kyberInverseNtt_Type;\n+  }\n+\n+  static const TypeFunc* kyberNttMult_Type() {\n+    assert(_kyberNttMult_Type != nullptr, \"should be initialized\");\n+    return _kyberNttMult_Type;\n+  }\n+\n+  static const TypeFunc* kyberAddPoly_2_Type() {\n+    assert(_kyberAddPoly_2_Type != nullptr, \"should be initialized\");\n+    return _kyberAddPoly_2_Type;\n+  }\n+\n+  static const TypeFunc* kyberAddPoly_3_Type() {\n+    assert(_kyberAddPoly_3_Type != nullptr, \"should be initialized\");\n+    return _kyberAddPoly_3_Type;\n+  }\n+\n+  static const TypeFunc* kyber12To16_Type() {\n+    assert(_kyber12To16_Type != nullptr, \"should be initialized\");\n+    return _kyber12To16_Type;\n+  }\n+\n+  static const TypeFunc* kyberBarrettReduce_Type() {\n+    assert(_kyberBarrettReduce_Type != nullptr, \"should be initialized\");\n+    return _kyberBarrettReduce_Type;\n+  }\n+\n","filename":"src\/hotspot\/share\/opto\/runtime.hpp","additions":46,"deletions":0,"binary":false,"changes":46,"status":"modified"},{"patch":"@@ -328,0 +328,2 @@\n+  product(bool, UseKyberIntrinsics, false, DIAGNOSTIC,                      \\\n+          \"Use intrinsics for the vectorized version of Kyber\")             \\\n","filename":"src\/hotspot\/share\/runtime\/globals.hpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -681,0 +681,15 @@\n+  do_stub(compiler, kyberNtt)                                           \\\n+  do_entry(compiler, kyberNtt, kyberNtt, kyberNtt)                      \\\n+  do_stub(compiler, kyberInverseNtt)                                    \\\n+  do_entry(compiler, kyberInverseNtt, kyberInverseNtt, kyberInverseNtt) \\\n+  do_stub(compiler, kyberNttMult)                                       \\\n+  do_entry(compiler, kyberNttMult, kyberNttMult, kyberNttMult)          \\\n+  do_stub(compiler, kyberAddPoly_2)                                     \\\n+  do_entry(compiler, kyberAddPoly_2, kyberAddPoly_2, kyberAddPoly_2)    \\\n+  do_stub(compiler, kyberAddPoly_3)                                     \\\n+  do_entry(compiler, kyberAddPoly_3, kyberAddPoly_3, kyberAddPoly_3)    \\\n+  do_stub(compiler, kyber12To16)                                        \\\n+  do_entry(compiler, kyber12To16, kyber12To16, kyber12To16)             \\\n+  do_stub(compiler, kyberBarrettReduce)                                 \\\n+  do_entry(compiler, kyberBarrettReduce, kyberBarrettReduce,            \\\n+           kyberBarrettReduce)                                          \\\n@@ -743,0 +758,2 @@\n+  do_stub(compiler, double_keccak)                                      \\\n+  do_entry(compiler, double_keccak, double_keccak, double_keccak)       \\\n@@ -746,2 +763,0 @@\n-  do_stub(compiler, double_keccak)                                      \\\n-  do_entry(compiler, double_keccak, double_keccak, double_keccak)       \\\n","filename":"src\/hotspot\/share\/runtime\/stubDeclarations.hpp","additions":17,"deletions":2,"binary":false,"changes":19,"status":"modified"},{"patch":"@@ -31,0 +31,1 @@\n+import jdk.internal.vm.annotation.IntrinsicCandidate;\n@@ -74,0 +75,262 @@\n+    private static final short[] montZetasForVectorNttArr = new short[]{\n+            \/\/ level 0\n+            -758, -758, -758, -758, -758, -758, -758, -758,\n+            -758, -758, -758, -758, -758, -758, -758, -758,\n+            -758, -758, -758, -758, -758, -758, -758, -758,\n+            -758, -758, -758, -758, -758, -758, -758, -758,\n+            -758, -758, -758, -758, -758, -758, -758, -758,\n+            -758, -758, -758, -758, -758, -758, -758, -758,\n+            -758, -758, -758, -758, -758, -758, -758, -758,\n+            -758, -758, -758, -758, -758, -758, -758, -758,\n+            -758, -758, -758, -758, -758, -758, -758, -758,\n+            -758, -758, -758, -758, -758, -758, -758, -758,\n+            -758, -758, -758, -758, -758, -758, -758, -758,\n+            -758, -758, -758, -758, -758, -758, -758, -758,\n+            -758, -758, -758, -758, -758, -758, -758, -758,\n+            -758, -758, -758, -758, -758, -758, -758, -758,\n+            -758, -758, -758, -758, -758, -758, -758, -758,\n+            -758, -758, -758, -758, -758, -758, -758, -758,\n+            \/\/ level 1\n+            -359, -359, -359, -359, -359, -359, -359, -359,\n+            -359, -359, -359, -359, -359, -359, -359, -359,\n+            -359, -359, -359, -359, -359, -359, -359, -359,\n+            -359, -359, -359, -359, -359, -359, -359, -359,\n+            -359, -359, -359, -359, -359, -359, -359, -359,\n+            -359, -359, -359, -359, -359, -359, -359, -359,\n+            -359, -359, -359, -359, -359, -359, -359, -359,\n+            -359, -359, -359, -359, -359, -359, -359, -359,\n+            -1517, -1517, -1517, -1517, -1517, -1517, -1517, -1517,\n+            -1517, -1517, -1517, -1517, -1517, -1517, -1517, -1517,\n+            -1517, -1517, -1517, -1517, -1517, -1517, -1517, -1517,\n+            -1517, -1517, -1517, -1517, -1517, -1517, -1517, -1517,\n+            -1517, -1517, -1517, -1517, -1517, -1517, -1517, -1517,\n+            -1517, -1517, -1517, -1517, -1517, -1517, -1517, -1517,\n+            -1517, -1517, -1517, -1517, -1517, -1517, -1517, -1517,\n+            -1517, -1517, -1517, -1517, -1517, -1517, -1517, -1517,\n+            \/\/ level 2\n+            1493, 1493, 1493, 1493, 1493, 1493, 1493, 1493,\n+            1493, 1493, 1493, 1493, 1493, 1493, 1493, 1493,\n+            1493, 1493, 1493, 1493, 1493, 1493, 1493, 1493,\n+            1493, 1493, 1493, 1493, 1493, 1493, 1493, 1493,\n+            1422, 1422, 1422, 1422, 1422, 1422, 1422, 1422,\n+            1422, 1422, 1422, 1422, 1422, 1422, 1422, 1422,\n+            1422, 1422, 1422, 1422, 1422, 1422, 1422, 1422,\n+            1422, 1422, 1422, 1422, 1422, 1422, 1422, 1422,\n+            287, 287, 287, 287, 287, 287, 287, 287,\n+            287, 287, 287, 287, 287, 287, 287, 287,\n+            287, 287, 287, 287, 287, 287, 287, 287,\n+            287, 287, 287, 287, 287, 287, 287, 287,\n+            202, 202, 202, 202, 202, 202, 202, 202,\n+            202, 202, 202, 202, 202, 202, 202, 202,\n+            202, 202, 202, 202, 202, 202, 202, 202,\n+            202, 202, 202, 202, 202, 202, 202, 202,\n+            \/\/ level 3\n+            -171, -171, -171, -171, -171, -171, -171, -171,\n+            -171, -171, -171, -171, -171, -171, -171, -171,\n+            622, 622, 622, 622, 622, 622, 622, 622,\n+            622, 622, 622, 622, 622, 622, 622, 622,\n+            1577, 1577, 1577, 1577, 1577, 1577, 1577, 1577,\n+            1577, 1577, 1577, 1577, 1577, 1577, 1577, 1577,\n+            182, 182, 182, 182, 182, 182, 182, 182,\n+            182, 182, 182, 182, 182, 182, 182, 182,\n+            962, 962, 962, 962, 962, 962, 962, 962,\n+            962, 962, 962, 962, 962, 962, 962, 962,\n+            -1202, -1202, -1202, -1202, -1202, -1202, -1202, -1202,\n+            -1202, -1202, -1202, -1202, -1202, -1202, -1202, -1202,\n+            -1474, -1474, -1474, -1474, -1474, -1474, -1474, -1474,\n+            -1474, -1474, -1474, -1474, -1474, -1474, -1474, -1474,\n+            1468, 1468, 1468, 1468, 1468, 1468, 1468, 1468,\n+            1468, 1468, 1468, 1468, 1468, 1468, 1468, 1468,\n+            \/\/ level 4\n+            573, 573, 573, 573, 573, 573, 573, 573,\n+            -1325, -1325, -1325, -1325, -1325, -1325, -1325, -1325,\n+            264, 264, 264, 264, 264, 264, 264, 264,\n+            383, 383, 383, 383, 383, 383, 383, 383,\n+            -829, -829, -829, -829, -829, -829, -829, -829,\n+            1458, 1458, 1458, 1458, 1458, 1458, 1458, 1458,\n+            -1602, -1602, -1602, -1602, -1602, -1602, -1602, -1602,\n+            -130, -130, -130, -130, -130, -130, -130, -130,\n+            -681, -681, -681, -681, -681, -681, -681, -681,\n+            1017, 1017, 1017, 1017, 1017, 1017, 1017, 1017,\n+            732, 732, 732, 732, 732, 732, 732, 732,\n+            608, 608, 608, 608, 608, 608, 608, 608,\n+            -1542, -1542, -1542, -1542, -1542, -1542, -1542, -1542,\n+            411, 411, 411, 411, 411, 411, 411, 411,\n+            -205, -205, -205, -205, -205, -205, -205, -205,\n+            -1571, -1571, -1571, -1571, -1571, -1571, -1571, -1571,\n+            \/\/ level 5\n+            1223, 1223, 1223, 1223, 652, 652, 652, 652,\n+            -552, -552, -552, -552, 1015, 1015, 1015, 1015,\n+            -1293, -1293, -1293, -1293, 1491, 1491, 1491, 1491,\n+            -282, -282, -282, -282, -1544, -1544, -1544, -1544,\n+            516, 516, 516, 516, -8, -8, -8, -8,\n+            -320, -320, -320, -320, -666, -666, -666, -666,\n+            1711, 1711, 1711, 1711, -1162, -1162, -1162, -1162,\n+            126, 126, 126, 126, 1469, 1469, 1469, 1469,\n+            -853, -853, -853, -853, -90, -90, -90, -90,\n+            -271, -271, -271, -271, 830, 830, 830, 830,\n+            107, 107, 107, 107, -1421, -1421, -1421, -1421,\n+            -247, -247, -247, -247, -951, -951, -951, -951,\n+            -398, -398, -398, -398, 961, 961, 961, 961,\n+            -1508, -1508, -1508, -1508, -725, -725, -725, -725,\n+            448, 448, 448, 448, -1065, -1065, -1065, -1065,\n+            677, 677, 677, 677, -1275, -1275, -1275, -1275,\n+            \/\/ level 6\n+            -1103, -1103, 430, 430, 555, 555, 843, 843,\n+            -1251, -1251, 871, 871, 1550, 1550, 105, 105,\n+            422, 422, 587, 587, 177, 177, -235, -235,\n+            -291, -291, -460, -460, 1574, 1574, 1653, 1653,\n+            -246, -246, 778, 778, 1159, 1159, -147, -147,\n+            -777, -777, 1483, 1483, -602, -602, 1119, 1119,\n+            -1590, -1590, 644, 644, -872, -872, 349, 349,\n+            418, 418, 329, 329, -156, -156, -75, -75,\n+            817, 817, 1097, 1097, 603, 603, 610, 610,\n+            1322, 1322, -1285, -1285, -1465, -1465, 384, 384,\n+            -1215, -1215, -136, -136, 1218, 1218, -1335, -1335,\n+            -874, -874, 220, 220, -1187, -1187, 1670, 1670,\n+            -1185, -1185, -1530, -1530, -1278, -1278, 794, 794,\n+            -1510, -1510, -854, -854, -870, -870, 478, 478,\n+            -108, -108, -308, -308, 996, 996, 991, 991,\n+            958, 958, -1460, -1460, 1522, 1522, 1628, 1628\n+    };\n+    private static final int[] MONT_ZETAS_FOR_INVERSE_NTT = new int[]{\n+            584, -1049, 57, 1317, 789, 709, 1599, -1601,\n+            -990, 604, 348, 857, 612, 474, 1177, -1014,\n+            -88, -982, -191, 668, 1386, 486, -1153, -534,\n+            514, 137, 586, -1178, 227, 339, -907, 244,\n+            1200, -833, 1394, -30, 1074, 636, -317, -1192,\n+            -1259, -355, -425, -884, -977, 1430, 868, 607,\n+            184, 1448, 702, 1327, 431, 497, 595, -94,\n+            1649, -1497, -620, 42, -172, 1107, -222, 1003,\n+            426, -845, 395, -510, 1613, 825, 1269, -290,\n+            -1429, 623, -567, 1617, 36, 1007, 1440, 332,\n+            -201, 1313, -1382, -744, 669, -1538, 128, -1598,\n+            1401, 1183, -553, 714, 405, -1155, -445, 406,\n+            -1496, -49, 82, 1369, 259, 1604, 373, 909,\n+            -1249, -1000, -25, -52, 530, -895, 1226, 819,\n+            -185, 281, -742, 1253, 417, 1400, 35, -593,\n+            97, -1263, 551, -585, 969, -914, -1188\n+    };\n+\n+    private static final short[] montZetasForVectorInverseNttArr = new short[]{\n+            \/\/ level 0\n+            -1628, -1628, -1522, -1522, 1460, 1460, -958, -958,\n+            -991, -991, -996, -996, 308, 308, 108, 108,\n+            -478, -478, 870, 870, 854, 854, 1510, 1510,\n+            -794, -794, 1278, 1278, 1530, 1530, 1185, 1185,\n+            1659, 1659, 1187, 1187, -220, -220, 874, 874,\n+            1335, 1335, -1218, -1218, 136, 136, 1215, 1215,\n+            -384, -384, 1465, 1465, 1285, 1285, -1322, -1322,\n+            -610, -610, -603, -603, -1097, -1097, -817, -817,\n+            75, 75, 156, 156, -329, -329, -418, -418,\n+            -349, -349, 872, 872, -644, -644, 1590, 1590,\n+            -1119, -1119, 602, 602, -1483, -1483, 777, 777,\n+            147, 147, -1159, -1159, -778, -778, 246, 246,\n+            -1653, -1653, -1574, -1574, 460, 460, 291, 291,\n+            235, 235, -177, -177, -587, -587, -422, -422,\n+            -105, -105, -1550, -1550, -871, -871, 1251, 1251,\n+            -843, -843, -555, -555, -430, -430, 1103, 1103,\n+            \/\/ level 1\n+            1275, 1275, 1275, 1275, -677, -677, -677, -677,\n+            1065, 1065, 1065, 1065, -448, -448, -448, -448,\n+            725, 725, 725, 725, 1508, 1508, 1508, 1508,\n+            -961, -961, -961, -961, 398, 398, 398, 398,\n+            951, 951, 951, 951, 247, 247, 247, 247,\n+            1421, 1421, 1421, 1421, -107, -107, -107, -107,\n+            -830, -830, -830, -830, 271, 271, 271, 271,\n+            90, 90, 90, 90, 853, 853, 853, 853,\n+            -1469, -1469, -1469, -1469, -126, -126, -126, -126,\n+            1162, 1162, 1162, 1162, 1618, 1618, 1618, 1618,\n+            666, 666, 666, 666, 320, 320, 320, 320,\n+            8, 8, 8, 8, -516, -516, -516, -516,\n+            1544, 1544, 1544, 1544, 282, 282, 282, 282,\n+            -1491, -1491, -1491, -1491, 1293, 1293, 1293, 1293,\n+            -1015, -1015, -1015, -1015, 552, 552, 552, 552,\n+            -652, -652, -652, -652, -1223, -1223, -1223, -1223,\n+            \/\/ level 2\n+            1571, 1571, 1571, 1571, 1571, 1571, 1571, 1571,\n+            205, 205, 205, 205, 205, 205, 205, 205,\n+            -411, -411, -411, -411, -411, -411, -411, -411,\n+            1542, 1542, 1542, 1542, 1542, 1542, 1542, 1542,\n+            -608, -608, -608, -608, -608, -608, -608, -608,\n+            -732, -732, -732, -732, -732, -732, -732, -732,\n+            -1017, -1017, -1017, -1017, -1017, -1017, -1017, -1017,\n+            681, 681, 681, 681, 681, 681, 681, 681,\n+            130, 130, 130, 130, 130, 130, 130, 130,\n+            1602, 1602, 1602, 1602, 1602, 1602, 1602, 1602,\n+            -1458, -1458, -1458, -1458, -1458, -1458, -1458, -1458,\n+            829, 829, 829, 829, 829, 829, 829, 829,\n+            -383, -383, -383, -383, -383, -383, -383, -383,\n+            -264, -264, -264, -264, -264, -264, -264, -264,\n+            1325, 1325, 1325, 1325, 1325, 1325, 1325, 1325,\n+            -573, -573, -573, -573, -573, -573, -573, -573,\n+            \/\/ level 3\n+            -1468, -1468, -1468, -1468, -1468, -1468, -1468, -1468,\n+            -1468, -1468, -1468, -1468, -1468, -1468, -1468, -1468,\n+            1474, 1474, 1474, 1474, 1474, 1474, 1474, 1474,\n+            1474, 1474, 1474, 1474, 1474, 1474, 1474, 1474,\n+            1202, 1202, 1202, 1202, 1202, 1202, 1202, 1202,\n+            1202, 1202, 1202, 1202, 1202, 1202, 1202, 1202,\n+            -962, -962, -962, -962, -962, -962, -962, -962,\n+            -962, -962, -962, -962, -962, -962, -962, -962,\n+            -182, -182, -182, -182, -182, -182, -182, -182,\n+            -182, -182, -182, -182, -182, -182, -182, -182,\n+            -1577, -1577, -1577, -1577, -1577, -1577, -1577, -1577,\n+            -1577, -1577, -1577, -1577, -1577, -1577, -1577, -1577,\n+            -622, -622, -622, -622, -622, -622, -622, -622,\n+            -622, -622, -622, -622, -622, -622, -622, -622,\n+            171, 171, 171, 171, 171, 171, 171, 171,\n+            171, 171, 171, 171, 171, 171, 171, 171,\n+            \/\/ level 4\n+            -202, -202, -202, -202, -202, -202, -202, -202,\n+            -202, -202, -202, -202, -202, -202, -202, -202,\n+            -202, -202, -202, -202, -202, -202, -202, -202,\n+            -202, -202, -202, -202, -202, -202, -202, -202,\n+            -287, -287, -287, -287, -287, -287, -287, -287,\n+            -287, -287, -287, -287, -287, -287, -287, -287,\n+            -287, -287, -287, -287, -287, -287, -287, -287,\n+            -287, -287, -287, -287, -287, -287, -287, -287,\n+            -1422, -1422, -1422, -1422, -1422, -1422, -1422, -1422,\n+            -1422, -1422, -1422, -1422, -1422, -1422, -1422, -1422,\n+            -1422, -1422, -1422, -1422, -1422, -1422, -1422, -1422,\n+            -1422, -1422, -1422, -1422, -1422, -1422, -1422, -1422,\n+            -1493, -1493, -1493, -1493, -1493, -1493, -1493, -1493,\n+            -1493, -1493, -1493, -1493, -1493, -1493, -1493, -1493,\n+            -1493, -1493, -1493, -1493, -1493, -1493, -1493, -1493,\n+            -1493, -1493, -1493, -1493, -1493, -1493, -1493, -1493,\n+            \/\/ level 5\n+            1517, 1517, 1517, 1517, 1517, 1517, 1517, 1517,\n+            1517, 1517, 1517, 1517, 1517, 1517, 1517, 1517,\n+            1517, 1517, 1517, 1517, 1517, 1517, 1517, 1517,\n+            1517, 1517, 1517, 1517, 1517, 1517, 1517, 1517,\n+            1517, 1517, 1517, 1517, 1517, 1517, 1517, 1517,\n+            1517, 1517, 1517, 1517, 1517, 1517, 1517, 1517,\n+            1517, 1517, 1517, 1517, 1517, 1517, 1517, 1517,\n+            1517, 1517, 1517, 1517, 1517, 1517, 1517, 1517,\n+            359, 359, 359, 359, 359, 359, 359, 359,\n+            359, 359, 359, 359, 359, 359, 359, 359,\n+            359, 359, 359, 359, 359, 359, 359, 359,\n+            359, 359, 359, 359, 359, 359, 359, 359,\n+            359, 359, 359, 359, 359, 359, 359, 359,\n+            359, 359, 359, 359, 359, 359, 359, 359,\n+            359, 359, 359, 359, 359, 359, 359, 359,\n+            359, 359, 359, 359, 359, 359, 359, 359,\n+            \/\/ level 6\n+            758, 758, 758, 758, 758, 758, 758, 758,\n+            758, 758, 758, 758, 758, 758, 758, 758,\n+            758, 758, 758, 758, 758, 758, 758, 758,\n+            758, 758, 758, 758, 758, 758, 758, 758,\n+            758, 758, 758, 758, 758, 758, 758, 758,\n+            758, 758, 758, 758, 758, 758, 758, 758,\n+            758, 758, 758, 758, 758, 758, 758, 758,\n+            758, 758, 758, 758, 758, 758, 758, 758,\n+            758, 758, 758, 758, 758, 758, 758, 758,\n+            758, 758, 758, 758, 758, 758, 758, 758,\n+            758, 758, 758, 758, 758, 758, 758, 758,\n+            758, 758, 758, 758, 758, 758, 758, 758,\n+            758, 758, 758, 758, 758, 758, 758, 758,\n+            758, 758, 758, 758, 758, 758, 758, 758,\n+            758, 758, 758, 758, 758, 758, 758, 758,\n+            758, 758, 758, 758, 758, 758, 758, 758\n+    };\n+\n@@ -92,0 +355,18 @@\n+    private static final short[] montZetasForVectorNttMultArr = new short[]{\n+            -1103, 1103, 430, -430, 555, -555, 843, -843,\n+            -1251, 1251, 871, -871, 1550, -1550, 105, -105,\n+            422, -422, 587, -587, 177, -177, -235, 235,\n+            -291, 291, -460, 460, 1574, -1574, 1653, -1653,\n+            -246, 246, 778, -778, 1159, -1159, -147, 147,\n+            -777, 777, 1483, -1483, -602, 602, 1119, -1119,\n+            -1590, 1590, 644, -644, -872, 872, 349, -349,\n+            418, -418, 329, -329, -156, 156, -75, 75,\n+            817, -817, 1097, -1097, 603, -603, 610, -610,\n+            1322, -1322, -1285, 1285, -1465, 1465, 384, -384,\n+            -1215, 1215, -136, 136, 1218, -1218, -1335, 1335,\n+            -874, 874, 220, -220, -1187, 1187, 1670, 1659,\n+            -1185, 1185, -1530, 1530, -1278, 1278, 794, -794,\n+            -1510, 1510, -854, 854, -870, 870, 478, -478,\n+            -108, 108, -308, 308, 996, -996, 991, -991,\n+            958, -958, -1460, 1460, 1522, -1522, 1628, -1628\n+    };\n@@ -264,1 +545,1 @@\n-        } catch (NoSuchAlgorithmException e){\n+        } catch (NoSuchAlgorithmException e) {\n@@ -371,1 +652,1 @@\n-        mlKemG.update((byte)mlKem_k);\n+\/\/        mlKemG.update((byte)mlKem_k);\n@@ -530,1 +811,1 @@\n-                    xofBufArr[parInd] = seedBuf.clone();\n+                    System.arraycopy(seedBuf, 0, xofBufArr[parInd], 0, seedBuf.length);\n@@ -710,3 +991,7 @@\n-    \/\/ The elements of poly should be in the range [-ML_KEM_Q, ML_KEM_Q]\n-    \/\/ The elements of poly at return will be in the range of [0, ML_KEM_Q]\n-    private void mlKemNTT(short[] poly) {\n+    @IntrinsicCandidate\n+    static int implKyberNtt(short[] poly, short[] ntt_zetas) {\n+        implKyberNttJava(poly);\n+        return 1;\n+    }\n+\n+    static void implKyberNttJava(short[] poly) {\n@@ -721,0 +1006,7 @@\n+    }\n+\n+    \/\/ The elements of poly should be in the range [-mlKem_q, mlKem_q]\n+    \/\/ The elements of poly at return will be in the range of [0, mlKem_q]\n+    private void mlKemNTT(short[] poly) {\n+        assert poly.length == ML_KEM_N;\n+        implKyberNtt(poly, montZetasForVectorNttArr);\n@@ -724,3 +1016,7 @@\n-    \/\/ Works in place, but also returns its (modified) input so that it can\n-    \/\/ be used in expressions\n-    private short[] mlKemInverseNTT(short[] poly) {\n+    @IntrinsicCandidate\n+    static int implKyberInverseNtt(short[] poly, short[] zetas) {\n+        implKyberInverseNttJava(poly);\n+        return 1;\n+    }\n+\n+    static void implKyberInverseNttJava(short[] poly) {\n@@ -735,0 +1031,7 @@\n+    }\n+\n+    \/\/ Works in place, but also returns its (modified) input so that it can\n+    \/\/ be used in expressions\n+    private short[] mlKemInverseNTT(short[] poly) {\n+        assert(poly.length == ML_KEM_N);\n+        implKyberInverseNtt(poly, montZetasForVectorInverseNttArr);\n@@ -825,4 +1128,8 @@\n-    \/\/ Multiplies two polynomials represented in the NTT domain.\n-    \/\/ The result is a representation of the product still in the NTT domain.\n-    \/\/ The coefficients in the result are in the range (-ML_KEM_Q, ML_KEM_Q).\n-    private void nttMult(short[] result, short[] ntta, short[] nttb) {\n+    @IntrinsicCandidate\n+    static int implKyberNttMult(short[] result, short[] ntta, short[] nttb,\n+                                short[] zetas) {\n+        implKyberNttMultJava(result, ntta, nttb);\n+        return 1;\n+    }\n+\n+    static void implKyberNttMultJava(short[] result, short[] ntta, short[] nttb) {\n@@ -830,0 +1137,1 @@\n+\n@@ -842,0 +1150,9 @@\n+    \/\/ Multiplies two polynomials represented in the NTT domain.\n+    \/\/ The result is a representation of the product still in the NTT domain.\n+    \/\/ The coefficients in the result are in the range (-mlKem_q, mlKem_q).\n+    private void nttMult(short[] result, short[] ntta, short[] nttb) {\n+        assert (result.length == ML_KEM_N) && (ntta.length == ML_KEM_N &&\n+                (nttb.length == ML_KEM_N));\n+        implKyberNttMult(result, ntta, nttb, montZetasForVectorNttMultArr);\n+    }\n+\n@@ -856,0 +1173,14 @@\n+    @IntrinsicCandidate\n+    static int implKyberAddPoly(short[] result, short[] a, short[] b) {\n+        implKyberAddPolyJava(result, a, b);\n+        return 1;\n+    }\n+\n+    static void implKyberAddPolyJava(short[] result, short[] a, short[] b) {\n+        for (int m = 0; m < ML_KEM_N; m++) {\n+            int r = a[m] + b[m] + ML_KEM_Q; \/\/ This makes r > - ML_KEM_Q\n+            a[m] = (short) r;\n+        }\n+        mlKemBarrettReduce(a);\n+    }\n+\n@@ -861,1 +1192,13 @@\n-    private void mlKemAddPoly(short[] a, short[] b) {\n+    private short[] mlKemAddPoly(short[] a, short[] b) {\n+        assert (a.length == ML_KEM_N) && (b.length == ML_KEM_N);\n+        implKyberAddPoly(a, a, b);\n+        return a;\n+    }\n+\n+    @IntrinsicCandidate\n+    static int implKyberAddPoly(short[] result, short[] a, short[] b, short[] c) {\n+        implKyberAddPolyJava(result, a, b, c);\n+        return 1;\n+    }\n+\n+    static void implKyberAddPolyJava(short[] result, short[] a, short[] b, short[] c) {\n@@ -863,2 +1206,2 @@\n-            int r = a[m] + b[m] + ML_KEM_Q; \/\/ This makes r > -ML_KEM_Q\n-            a[m] = (short) r;\n+            int r = a[m] + b[m] + c[m] + 2 * ML_KEM_Q; \/\/ This makes r > - ML_KEM_Q\n+            result[m] = (short) r;\n@@ -874,4 +1217,3 @@\n-        for (int m = 0; m < ML_KEM_N; m++) {\n-            int r = a[m] + b[m] + c[m] + 2 * ML_KEM_Q; \/\/ This makes r > - ML_KEM_Q\n-            a[m] = (short) r;\n-        }\n+        assert (a.length == ML_KEM_N) && (b.length == ML_KEM_N) &&\n+                (c.length == ML_KEM_N);\n+        implKyberAddPoly(a, a, b, c);\n@@ -1000,8 +1342,5 @@\n-    \/\/ The intrinsic implementations assume that the input and output buffers\n-    \/\/ are such that condensed can be read in 192-byte chunks and\n-    \/\/ parsed can be written in 128 shorts chunks. In other words,\n-    \/\/ if (i - 1) * 128 < parsedLengths <= i * 128 then\n-    \/\/ parsed.size should be at least i * 128 and\n-    \/\/ condensed.size should be at least index + i * 192\n-    private void twelve2Sixteen(byte[] condensed, int index,\n-                                short[] parsed, int parsedLength) {\n+    @IntrinsicCandidate\n+    private static int implKyber12To16(byte[] condensed, int index, short[] parsed, int parsedLength) {\n+        implKyber12To16Java(condensed, index, parsed, parsedLength);\n+        return 1;\n+    }\n@@ -1009,0 +1348,1 @@\n+    private static void implKyber12To16Java(byte[] condensed, int index, short[] parsed, int parsedLength) {\n@@ -1017,0 +1357,19 @@\n+    \/\/ The intrinsic implementations assume that the input and output buffers\n+    \/\/ are such that condensed can be read in 96-byte chunks and\n+    \/\/ parsed can be written in 64 shorts chunks except for the last chunk\n+    \/\/ that can be either 48 or 64 shorts. In other words,\n+    \/\/ if (i - 1) * 64 < parsedLengths <= i * 64 then\n+    \/\/ parsed.length should be either i * 64 or (i-1) * 64 + 48 and\n+    \/\/ condensed.length should be at least index + i * 96.\n+    private void twelve2Sixteen(byte[] condensed, int index,\n+                                short[] parsed, int parsedLength) {\n+        int i = parsedLength \/ 64;\n+        int remainder = parsedLength - i * 64;\n+        if (remainder != 0) {\n+            i++;\n+        }\n+        assert (((remainder != 0) && (remainder != 48)) ||\n+            index + i * 96 > condensed.length);\n+        implKyber12To16(condensed, index, parsed, parsedLength);\n+    }\n+\n@@ -1155,0 +1514,13 @@\n+    @IntrinsicCandidate\n+    static int implKyberBarrettReduce(short[] coeffs) {\n+        implKyberBarrettReduceJava(coeffs);\n+        return 1;\n+    }\n+\n+    static void implKyberBarrettReduceJava(short[] poly) {\n+        for (int m = 0; m < ML_KEM_N; m++) {\n+            int tmp = ((int) poly[m] * BARRETT_MULTIPLIER) >> BARRETT_SHIFT;\n+            poly[m] = (short) (poly[m] - tmp * ML_KEM_Q);\n+        }\n+    }\n+\n@@ -1164,5 +1536,3 @@\n-    private void mlKemBarrettReduce(short[] poly) {\n-        for (int m = 0; m < ML_KEM_N; m++) {\n-            int tmp = ((int) poly[m] * BARRETT_MULTIPLIER) >> BARRETT_SHIFT;\n-            poly[m] = (short) (poly[m] - tmp * ML_KEM_Q);\n-        }\n+    private static void mlKemBarrettReduce(short[] poly) {\n+        assert poly.length == ML_KEM_N;\n+        implKyberBarrettReduce(poly);\n","filename":"src\/java.base\/share\/classes\/com\/sun\/crypto\/provider\/ML_KEM.java","additions":403,"deletions":33,"binary":false,"changes":436,"status":"modified"},{"patch":"@@ -1553,1 +1553,1 @@\n-    \/\/ see e.g. Algorithm 3 in https:\/\/eprint.iacr.org\/2018\/039.pdf\n+    \/\/ See e.g. Algorithm 3 in https:\/\/eprint.iacr.org\/2018\/039.pdf\n","filename":"src\/java.base\/share\/classes\/sun\/security\/provider\/ML_DSA.java","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"}]}