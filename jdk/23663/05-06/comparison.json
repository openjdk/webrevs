{"files":[{"patch":"@@ -61,20 +61,0 @@\n-\n-\/\/ convenience methods for splitting 8-way vector register sequences\n-\/\/ in half -- needed because vector operations can normally only be\n-\/\/ benefit from 4-way instruction parallelism\n-\n-VSeq<4> vs_front(const VSeq<8>& v) {\n-  return VSeq<4>(v.base(), v.delta());\n-}\n-\n-VSeq<4> vs_back(const VSeq<8>& v) {\n-  return VSeq<4>(v.base() + 4 * v.delta(), v.delta());\n-}\n-\n-VSeq<4> vs_even(const VSeq<8>& v) {\n-  return VSeq<4>(v.base(), v.delta() * 2);\n-}\n-\n-VSeq<4> vs_odd(const VSeq<8>& v) {\n-  return VSeq<4>(v.base() + 1, v.delta() * 2);\n-}\n","filename":"src\/hotspot\/cpu\/aarch64\/register_aarch64.cpp","additions":0,"deletions":20,"binary":false,"changes":20,"status":"modified"},{"patch":"@@ -439,0 +439,3 @@\n+\/\/ helper macro for computing register masks\n+#define VS_MASK_BIT(base, delta, i) (1 << (base + delta * i))\n+\n@@ -441,2 +444,0 @@\n-  static_assert(N <= 8, \"vector sequence length must not exceed 8\");\n-  static_assert((N & (N - 1)) == 0, \"vector sequence length must be power of two\");\n@@ -449,3 +450,3 @@\n-    assert (_base >= 0, \"invalid base register\");\n-    assert (_delta >= 0, \"invalid register delta\");\n-    assert ((_base + (N - 1) * _delta) < 32, \"range exceeded\");\n+    assert (_base >= 0 && _base <= 31, \"invalid base register\");\n+    assert ((_base + (N - 1) * _delta) >= 0, \"register range underflow\");\n+    assert ((_base + (N - 1) * _delta) < 32, \"register range overflow\");\n@@ -460,1 +461,0 @@\n-    int bit = 1 << _base;\n@@ -462,1 +462,1 @@\n-      m |= bit << (i * _delta);\n+      m |= VS_MASK_BIT(_base, _delta, i);\n@@ -468,0 +468,1 @@\n+  bool is_constant() const { return _delta == 0; }\n@@ -470,8 +471,1 @@\n-\/\/ declare convenience methods for splitting vector register sequences\n-\n-VSeq<4> vs_front(const VSeq<8>& v);\n-VSeq<4> vs_back(const VSeq<8>& v);\n-VSeq<4> vs_even(const VSeq<8>& v);\n-VSeq<4> vs_odd(const VSeq<8>& v);\n-\n-\/\/ methods for use in asserts to check VSeq inputs and oupts are\n+\/\/ methods for use in asserts to check VSeq inputs and outputs are\n@@ -483,0 +477,69 @@\n+\/\/ method for use in asserts to check whether registers appearing in\n+\/\/ an output sequence will be written before they are read from an\n+\/\/ input sequence.\n+\n+template<int N> bool vs_write_before_read(const VSeq<N>& vout, const VSeq<N>& vin) {\n+  int b_in = vin.base();\n+  int d_in = vin.delta();\n+  int b_out = vout.base();\n+  int d_out = vout.delta();\n+  int bit_in = 1 << b_in;\n+  int bit_out = 1 << b_out;\n+  int mask_read = vin.mask();   \/\/ all pending reads\n+  int mask_write = 0;         \/\/ no writes as yet\n+\n+\n+  for (int i = 0; i < N - 1; i++) {\n+    \/\/ check whether a pending read clashes with a write\n+    if ((mask_write & mask_read) != 0) {\n+      return true;\n+    }\n+    \/\/ remove the pending input (so long as this is a constant\n+    \/\/ sequence)\n+    if (d_in != 0) {\n+      mask_read ^= VS_MASK_BIT(b_in, d_in, i);\n+    }\n+    \/\/ record the next write\n+    mask_write |= VS_MASK_BIT(b_out, d_out, i);\n+  }\n+  \/\/ no write before read\n+  return false;\n+}\n+\n+\/\/ convenience methods for splitting 8-way of 4-way vector register\n+\/\/ sequences in half -- needed because vector operations can normally\n+\/\/ benefit from 4-way instruction parallelism or, occasionally, 2-way\n+\/\/ parallelism\n+\n+template<int N>\n+VSeq<N\/2> vs_front(const VSeq<N>& v) {\n+  static_assert(N > 0 && ((N & 1) == 0), \"sequence length must be even\");\n+  return VSeq<N\/2>(v.base(), v.delta());\n+}\n+\n+template<int N>\n+VSeq<N\/2> vs_back(const VSeq<N>& v) {\n+  static_assert(N > 0 && ((N & 1) == 0), \"sequence length must be even\");\n+  return VSeq<N\/2>(v.base() + N \/ 2 * v.delta(), v.delta());\n+}\n+\n+template<int N>\n+VSeq<N\/2> vs_even(const VSeq<N>& v) {\n+  static_assert(N > 0 && ((N & 1) == 0), \"sequence length must be even\");\n+  return VSeq<N\/2>(v.base(), v.delta() * 2);\n+}\n+\n+template<int N>\n+VSeq<N\/2> vs_odd(const VSeq<N>& v) {\n+  static_assert(N > 0 && ((N & 1) == 0), \"sequence length must be even\");\n+  return VSeq<N\/2>(v.base() + v.delta(), v.delta() * 2);\n+}\n+\n+\/\/ convenience method to construct a vector register sequence that\n+\/\/ indexes its elements in reverse order to the original\n+\n+template<int N>\n+VSeq<N> vs_reverse(const VSeq<N>& v) {\n+  return VSeq<N>(v.base() + (N - 1) * v.delta(), -v.delta());\n+}\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/register_aarch64.hpp","additions":78,"deletions":15,"binary":false,"changes":93,"status":"modified"},{"patch":"@@ -4646,5 +4646,271 @@\n-  void kyber_load64coeffs(int d0, Register tmpAddr) {\n-    __ ldpq(as_FloatRegister(d0), as_FloatRegister(d0 + 1), __ post(tmpAddr, 32));\n-    __ ldpq(as_FloatRegister(d0 + 2), as_FloatRegister(d0 + 3), __ post(tmpAddr, 32));\n-    __ ldpq(as_FloatRegister(d0 + 4), as_FloatRegister(d0 + 5), __ post(tmpAddr, 32));\n-    __ ldpq(as_FloatRegister(d0 + 6), as_FloatRegister(d0 + 7), tmpAddr);\n+  \/\/ Helpers to schedule parallel operation bundles across vector\n+  \/\/ register sequences of size 2, 4 or 8.\n+\n+  \/\/ Implement various primitive computations across vector sequences\n+\n+  template<int N>\n+  void vs_addv(const VSeq<N>& v, Assembler::SIMD_Arrangement T,\n+               const VSeq<N>& v1, const VSeq<N>& v2) {\n+    \/\/ output must not be constant\n+    assert(N == 1  || !v.is_constant(), \"cannot output multiple values to a constant vector\");\n+    \/\/ output cannot overwrite pending inputs\n+    assert(!vs_write_before_read(v, v1), \"output overwrites input\");\n+    assert(!vs_write_before_read(v, v2), \"output overwrites input\");\n+    for (int i = 0; i < N; i++) {\n+      __ addv(v[i], T, v1[i], v2[i]);\n+    }\n+  }\n+\n+  template<int N>\n+  void vs_subv(const VSeq<N>& v, Assembler::SIMD_Arrangement T,\n+               const VSeq<N>& v1, const VSeq<N>& v2) {\n+    \/\/ output must not be constant\n+    assert(N == 1  || !v.is_constant(), \"cannot output multiple values to a constant vector\");\n+    \/\/ output cannot overwrite pending inputs\n+    assert(!vs_write_before_read(v, v1), \"output overwrites input\");\n+    assert(!vs_write_before_read(v, v2), \"output overwrites input\");\n+    for (int i = 0; i < N; i++) {\n+      __ subv(v[i], T, v1[i], v2[i]);\n+    }\n+  }\n+\n+  template<int N>\n+  void vs_mulv(const VSeq<N>& v, Assembler::SIMD_Arrangement T,\n+               const VSeq<N>& v1, const VSeq<N>& v2) {\n+    \/\/ output must not be constant\n+    assert(N == 1  || !v.is_constant(), \"cannot output multiple values to a constant vector\");\n+    \/\/ output cannot overwrite pending inputs\n+    assert(!vs_write_before_read(v, v1), \"output overwrites input\");\n+    assert(!vs_write_before_read(v, v2), \"output overwrites input\");\n+    for (int i = 0; i < N; i++) {\n+      __ mulv(v[i], T, v1[i], v2[i]);\n+    }\n+  }\n+\n+  template<int N>\n+  void vs_negr(const VSeq<N>& v, Assembler::SIMD_Arrangement T, const VSeq<N>& v1) {\n+    \/\/ output must not be constant\n+    assert(N == 1  || !v.is_constant(), \"cannot output multiple values to a constant vector\");\n+    \/\/ output cannot overwrite pending inputs\n+    assert(!vs_write_before_read(v, v1), \"output overwrites input\");\n+    for (int i = 0; i < N; i++) {\n+      __ negr(v[i], T, v1[i]);\n+    }\n+  }\n+\n+  template<int N>\n+  void vs_sshr(const VSeq<N>& v, Assembler::SIMD_Arrangement T,\n+               const VSeq<N>& v1, int shift) {\n+    \/\/ output must not be constant\n+    assert(N == 1  || !v.is_constant(), \"cannot output multiple values to a constant vector\");\n+    \/\/ output cannot overwrite pending inputs\n+    assert(!vs_write_before_read(v, v1), \"output overwrites input\");\n+    for (int i = 0; i < N; i++) {\n+      __ sshr(v[i], T, v1[i], shift);\n+    }\n+  }\n+\n+  template<int N>\n+  void vs_andr(const VSeq<N>& v, const VSeq<N>& v1, const VSeq<N>& v2) {\n+    \/\/ output must not be constant\n+    assert(N == 1  || !v.is_constant(), \"cannot output multiple values to a constant vector\");\n+    \/\/ output cannot overwrite pending inputs\n+    assert(!vs_write_before_read(v, v1), \"output overwrites input\");\n+    assert(!vs_write_before_read(v, v2), \"output overwrites input\");\n+    for (int i = 0; i < N; i++) {\n+      __ andr(v[i], __ T16B, v1[i], v2[i]);\n+    }\n+  }\n+\n+  template<int N>\n+  void vs_orr(const VSeq<N>& v, const VSeq<N>& v1, const VSeq<N>& v2) {\n+    \/\/ output must not be constant\n+    assert(N == 1  || !v.is_constant(), \"cannot output multiple values to a constant vector\");\n+    \/\/ output cannot overwrite pending inputs\n+    assert(!vs_write_before_read(v, v1), \"output overwrites input\");\n+    assert(!vs_write_before_read(v, v2), \"output overwrites input\");\n+    for (int i = 0; i < N; i++) {\n+      __ orr(v[i], __ T16B, v1[i], v2[i]);\n+    }\n+  }\n+\n+  template<int N>\n+  void vs_notr(const VSeq<N>& v, const VSeq<N>& v1) {\n+    \/\/ output must not be constant\n+    assert(N == 1  || !v.is_constant(), \"cannot output multiple values to a constant vector\");\n+    \/\/ output cannot overwrite pending inputs\n+    assert(!vs_write_before_read(v, v1), \"output overwrites input\");\n+    for (int i = 0; i < N; i++) {\n+      __ notr(v[i], __ T16B, v1[i]);\n+    }\n+  }\n+\n+  template<int N>\n+  void vs_sqdmulh(const VSeq<N>& v, Assembler::SIMD_Arrangement T, const VSeq<N>& v1, const VSeq<N>& v2) {\n+    \/\/ output must not be constant\n+    assert(N == 1  || !v.is_constant(), \"cannot output multiple values to a constant vector\");\n+    \/\/ output cannot overwrite pending inputs\n+    assert(!vs_write_before_read(v, v1), \"output overwrites input\");\n+    assert(!vs_write_before_read(v, v2), \"output overwrites input\");\n+    for (int i = 0; i < N; i++) {\n+      __ sqdmulh(v[i], T, v1[i], v2[i]);\n+    }\n+  }\n+\n+  template<int N>\n+  void vs_mlsv(const VSeq<N>& v, Assembler::SIMD_Arrangement T, const VSeq<N>& v1, VSeq<N>& v2) {\n+    \/\/ output must not be constant\n+    assert(N == 1  || !v.is_constant(), \"cannot output multiple values to a constant vector\");\n+    \/\/ output cannot overwrite pending inputs\n+    assert(!vs_write_before_read(v, v1), \"output overwrites input\");\n+    assert(!vs_write_before_read(v, v2), \"output overwrites input\");\n+    for (int i = 0; i < N; i++) {\n+      __ mlsv(v[i], T, v1[i], v2[i]);\n+    }\n+  }\n+\n+  \/\/ load N\/2 successive pairs of quadword values from memory in order\n+  \/\/ into N successive vector registers of the sequence via the\n+  \/\/ address supplied in base.\n+  template<int N>\n+  void vs_ldpq(const VSeq<N>& v, Register base) {\n+    for (int i = 0; i < N; i += 2) {\n+      __ ldpq(v[i], v[i+1], Address(base, 32 * i));\n+    }\n+  }\n+\n+  \/\/ load N\/2 successive pairs of quadword values from memory in order\n+  \/\/ into N vector registers of the sequence via the address supplied\n+  \/\/ in base using post-increment addressing\n+  template<int N>\n+  void vs_ldpq_post(const VSeq<N>& v, Register base) {\n+    static_assert((N & (N - 1)) == 0, \"sequence length must be even\");\n+    for (int i = 0; i < N; i += 2) {\n+      __ ldpq(v[i], v[i+1], __ post(base, 32));\n+    }\n+  }\n+\n+  \/\/ store N successive vector registers of the sequence into N\/2\n+  \/\/ successive pairs of quadword memory locations via the address\n+  \/\/ supplied in base using post-increment addressing\n+  template<int N>\n+  void vs_stpq_post(const VSeq<N>& v, Register base) {\n+    static_assert((N & (N - 1)) == 0, \"sequence length must be even\");\n+    for (int i = 0; i < N; i += 2) {\n+      __ stpq(v[i], v[i+1], __ post(base, 32));\n+    }\n+  }\n+\n+  \/\/ load N\/2 pairs of quadword values from memory de-interleaved into\n+  \/\/ N vector registers 2 at a time via the address supplied in base\n+  \/\/ using post-increment addressing.\n+  template<int N>\n+  void vs_ld2_post(const VSeq<N>& v, Assembler::SIMD_Arrangement T, Register base) {\n+    static_assert((N & (N - 1)) == 0, \"sequence length must be even\");\n+    for (int i = 0; i < N; i += 2) {\n+      __ ld2(v[i], v[i+1], T, __ post(base, 32));\n+    }\n+  }\n+\n+  \/\/ store N vector registers interleaved into N\/2 pairs of quadword\n+  \/\/ memory locations via the address supplied in base using\n+  \/\/ post-increment addressing.\n+  template<int N>\n+  void vs_st2_post(const VSeq<N>& v, Assembler::SIMD_Arrangement T, Register base) {\n+    static_assert((N & (N - 1)) == 0, \"sequence length must be even\");\n+    for (int i = 0; i < N; i += 2) {\n+      __ st2(v[i], v[i+1], T, __ post(base, 32));\n+    }\n+  }\n+\n+  \/\/ load N quadword values from memory de-interleaved into N vector\n+  \/\/ registers 3 elements at a time via the address supplied in base.\n+  template<int N>\n+  void vs_ld3(const VSeq<N>& v, Assembler::SIMD_Arrangement T, Register base) {\n+    static_assert(N == ((N \/ 3) * 3), \"sequence length must be multiple of 3\");\n+    for (int i = 0; i < N; i += 3) {\n+      __ ld3(v[i], v[i+1], v[i+2], T, base);\n+    }\n+  }\n+\n+  \/\/ load N quadword values from memory de-interleaved into N vector\n+  \/\/ registers 3 elements at a time via the address supplied in base\n+  \/\/ using post-increment addressing.\n+  template<int N>\n+  void vs_ld3_post(const VSeq<N>& v, Assembler::SIMD_Arrangement T, Register base) {\n+    static_assert(N == ((N \/ 3) * 3), \"sequence length must be multiple of 3\");\n+    for (int i = 0; i < N; i += 3) {\n+      __ ld3(v[i], v[i+1], v[i+2], T, __ post(base, 48));\n+    }\n+  }\n+\n+  \/\/ load N\/2 pairs of quadword values from memory into N vector\n+  \/\/ registers via the address supplied in base with each pair indexed\n+  \/\/ using the the start offset plus the corresponding entry in the\n+  \/\/ offsets array\n+  template<int N>\n+  void vs_ldpq_indexed(const VSeq<N>& v, Register base, int start, int (&offsets)[N\/2]) {\n+    for (int i = 0; i < N\/2; i++) {\n+      __ ldpq(v[2*i], v[2*i+1], Address(base, start + offsets[i]));\n+    }\n+  }\n+\n+  \/\/ store N vector registers into N\/2 pairs of quadword memory\n+  \/\/ locations via the address supplied in base with each pair indexed\n+  \/\/ using the the start offset plus the corresponding entry in the\n+  \/\/ offsets array\n+  template<int N>\n+  void vs_stpq_indexed(const VSeq<N>& v, Register base, int start, int offsets[N\/2]) {\n+    for (int i = 0; i < N\/2; i++) {\n+      __ stpq(v[2*i], v[2*i+1], Address(base, start + offsets[i]));\n+    }\n+  }\n+\n+  \/\/ load N single quadword values from memory into N vector registers\n+  \/\/ via the address supplied in base with each value indexed using\n+  \/\/ the the start offset plus the corresponding entry in the offsets\n+  \/\/ array\n+  template<int N>\n+  void vs_ldr_indexed(const VSeq<N>& v, Assembler::SIMD_RegVariant T, Register base,\n+                      int start, int (&offsets)[N]) {\n+    for (int i = 0; i < N; i++) {\n+      __ ldr(v[i], T, Address(base, start + offsets[i]));\n+    }\n+  }\n+\n+  \/\/ store N vector registers into N single quadword memory locations\n+  \/\/ via the address supplied in base with each value indexed using\n+  \/\/ the the start offset plus the corresponding entry in the offsets\n+  \/\/ array\n+  template<int N>\n+  void vs_str_indexed(const VSeq<N>& v, Assembler::SIMD_RegVariant T, Register base,\n+                      int start, int (&offsets)[N]) {\n+    for (int i = 0; i < N; i++) {\n+      __ str(v[i], T, Address(base, start + offsets[i]));\n+    }\n+  }\n+\n+  \/\/ load N\/2 pairs of quadword values from memory de-interleaved into\n+  \/\/ N vector registers 2 at a time via the address supplied in base\n+  \/\/ with each pair indexed using the the start offset plus the\n+  \/\/ corresponding entry in the offsets array\n+  template<int N>\n+  void vs_ld2_indexed(const VSeq<N>& v, Assembler::SIMD_Arrangement T, Register base,\n+                      Register tmp, int start, int (&offsets)[N\/2]) {\n+    for (int i = 0; i < N\/2; i++) {\n+      __ add(tmp, base, start + offsets[i]);\n+      __ ld2(v[2*i], v[2*i+1], T, tmp);\n+    }\n+  }\n+\n+  \/\/ store N vector registers 2 at a time interleaved into N\/2 pairs\n+  \/\/ of quadword memory locations via the address supplied in base\n+  \/\/ with each pair indexed using the the start offset plus the\n+  \/\/ corresponding entry in the offsets array\n+  template<int N>\n+  void vs_st2_indexed(const VSeq<N>& v, Assembler::SIMD_Arrangement T, Register base,\n+                      Register tmp, int start, int (&offsets)[N\/2]) {\n+    for (int i = 0; i < N\/2; i++) {\n+      __ add(tmp, base, start + offsets[i]);\n+      __ st2(v[2*i], v[2*i+1], T, tmp);\n+    }\n@@ -4653,5 +4919,55 @@\n-  void kyber_load64zetas(Register zetas) {\n-    __ ldpq(v16, v17, __ post(zetas, 32));\n-    __ ldpq(v18, v19, __ post(zetas, 32));\n-    __ ldpq(v20, v21, __ post(zetas, 32));\n-    __ ldpq(v22, v23, __ post(zetas, 32));\n+  \/\/ Helper routines for various flavours of Montgomery multiply\n+\n+  \/\/ Perform 16 32-bit (4x4S) or 32 16-bit (4 x 8H) Montgomery\n+  \/\/ multiplications in parallel\n+  \/\/\n+\n+  \/\/ See the montMul() method of the sun.security.provider.ML_DSA\n+  \/\/ class.\n+  \/\/\n+  \/\/ Computes 4x4S results or 8x8H results\n+  \/\/    a = b * c * 2^MONT_R_BITS mod MONT_Q\n+  \/\/ Inputs:  vb, vc - 4x4S or 4x8H vector register sequences\n+  \/\/          vq - 2x4S or 2x8H constants <MONT_Q, MONT_Q_INV_MOD_R>\n+  \/\/ Temps:   vtmp - 4x4S or 4x8H vector sequence trashed after call\n+  \/\/ Outputs: va - 4x4S or 4x8H vector register sequences\n+  \/\/ vb, vc, vtmp and vq must all be disjoint\n+  \/\/ va must be disjoint from all other inputs\/temps or must equal vc\n+  \/\/ va must have a non-zero delta i.e. it must not be a constant vseq.\n+  \/\/ n.b. MONT_R_BITS is 16 or 32, so the right shift by it is implicit.\n+  void vs_montmul4(const VSeq<4>& va, const VSeq<4>& vb, const VSeq<4>& vc,\n+                   Assembler::SIMD_Arrangement T,\n+                   const VSeq<4>& vtmp, const VSeq<2>& vq) {\n+    assert (T == __ T4S || T == __ T8H, \"invalid arrangement for montmul\");\n+    assert(vs_disjoint(vb, vc), \"vb and vc overlap\");\n+    assert(vs_disjoint(vb, vq), \"vb and vq overlap\");\n+    assert(vs_disjoint(vb, vtmp), \"vb and vtmp overlap\");\n+\n+    assert(vs_disjoint(vc, vq), \"vc and vq overlap\");\n+    assert(vs_disjoint(vc, vtmp), \"vc and vtmp overlap\");\n+\n+    assert(vs_disjoint(vq, vtmp), \"vq and vtmp overlap\");\n+\n+    assert(vs_disjoint(va, vc) || vs_same(va, vc), \"va and vc neither disjoint nor equal\");\n+    assert(vs_disjoint(va, vb), \"va and vb overlap\");\n+    assert(vs_disjoint(va, vq), \"va and vq overlap\");\n+    assert(vs_disjoint(va, vtmp), \"va and vtmp overlap\");\n+    assert(!va.is_constant(), \"output vector must identify 4 different registers\");\n+\n+    \/\/ schedule 4 streams of instructions across the vector sequences\n+    for (int i = 0; i < 4; i++) {\n+      __ sqdmulh(vtmp[i], T, vb[i], vc[i]); \/\/ aHigh = hi32(2 * b * c)\n+      __ mulv(va[i], T, vb[i], vc[i]);    \/\/ aLow = lo32(b * c)\n+    }\n+\n+    for (int i = 0; i < 4; i++) {\n+      __ mulv(va[i], T, va[i], vq[0]);     \/\/ m = aLow * qinv\n+    }\n+\n+    for (int i = 0; i < 4; i++) {\n+      __ sqdmulh(va[i], T, va[i], vq[1]);  \/\/ n = hi32(2 * m * q)\n+    }\n+\n+    for (int i = 0; i < 4; i++) {\n+      __ shsubv(va[i], T, vtmp[i], va[i]);   \/\/ a = (aHigh - n) \/ 2\n+    }\n@@ -4660,5 +4976,48 @@\n-  void kyber_montmul64(bool by_constant) {\n-    int vr16 = 16, vr17 = 17, vr18 = 18, vr19 = 19;\n-    int vr20 = 20, vr21 = 21, vr22 = 22, vr23 = 23;\n-    if (by_constant) {\n-      vr16 = vr17 = vr18 = vr19 = vr20 = vr21 = vr22 = vr23 = 29;\n+  \/\/ Perform 8 32-bit (4x4S) or 16 16-bit (2 x 8H) Montgomery\n+  \/\/ multiplications in parallel\n+  \/\/\n+\n+  \/\/ See the montMul() method of the sun.security.provider.ML_DSA\n+  \/\/ class.\n+  \/\/\n+  \/\/ Computes 4x4S results or 8x8H results\n+  \/\/    a = b * c * 2^MONT_R_BITS mod MONT_Q\n+  \/\/ Inputs:  vb, vc - 4x4S or 4x8H vector register sequences\n+  \/\/          vq - 2x4S or 2x8H constants <MONT_Q, MONT_Q_INV_MOD_R>\n+  \/\/ Temps:   vtmp - 4x4S or 4x8H vector sequence trashed after call\n+  \/\/ Outputs: va - 4x4S or 4x8H vector register sequences\n+  \/\/ vb, vc, vtmp and vq must all be disjoint\n+  \/\/ va must be disjoint from all other inputs\/temps or must equal vc\n+  \/\/ va must have a non-zero delta i.e. it must not be a constant vseq.\n+  \/\/ n.b. MONT_R_BITS is 16 or 32, so the right shift by it is implicit.\n+  void vs_montmul2(const VSeq<2>& va, const VSeq<2>& vb, const VSeq<2>& vc,\n+                   Assembler::SIMD_Arrangement T,\n+                   const VSeq<2>& vtmp, const VSeq<2>& vq) {\n+    assert (T == __ T4S || T == __ T8H, \"invalid arrangement for montmul\");\n+    assert(vs_disjoint(vb, vc), \"vb and vc overlap\");\n+    assert(vs_disjoint(vb, vq), \"vb and vq overlap\");\n+    assert(vs_disjoint(vb, vtmp), \"vb and vtmp overlap\");\n+\n+    assert(vs_disjoint(vc, vq), \"vc and vq overlap\");\n+    assert(vs_disjoint(vc, vtmp), \"vc and vtmp overlap\");\n+\n+    assert(vs_disjoint(vq, vtmp), \"vq and vtmp overlap\");\n+\n+    assert(vs_disjoint(va, vc) || vs_same(va, vc), \"va and vc neither disjoint nor equal\");\n+    assert(vs_disjoint(va, vb), \"va and vb overlap\");\n+    assert(vs_disjoint(va, vq), \"va and vq overlap\");\n+    assert(vs_disjoint(va, vtmp), \"va and vtmp overlap\");\n+    assert(!va.is_constant(), \"output vector must identify 2 different registers\");\n+\n+    \/\/ schedule 2 streams of i<nstructions across the vector sequences\n+    for (int i = 0; i < 2; i++) {\n+      __ sqdmulh(vtmp[i], T, vb[i], vc[i]); \/\/ aHigh = hi32(2 * b * c)\n+      __ mulv(va[i], T, vb[i], vc[i]);    \/\/ aLow = lo32(b * c)\n+    }\n+\n+    for (int i = 0; i < 2; i++) {\n+      __ mulv(va[i], T, va[i], vq[0]);     \/\/ m = aLow * qinv\n+    }\n+\n+    for (int i = 0; i < 2; i++) {\n+      __ sqdmulh(va[i], T, va[i], vq[1]);  \/\/ n = hi32(2 * m * q)\n@@ -4666,40 +5025,60 @@\n-    __ sqdmulh(v24, __ T8H, v0, as_FloatRegister(vr16));\n-    __ mulv(v16, __ T8H, v0, as_FloatRegister(vr16));\n-    __ sqdmulh(v25, __ T8H, v1, as_FloatRegister(vr17));\n-    __ mulv(v17, __ T8H, v1, as_FloatRegister(vr17));\n-    __ sqdmulh(v26, __ T8H, v2, as_FloatRegister(vr18));\n-    __ mulv(v18, __ T8H, v2, as_FloatRegister(vr18));\n-    __ sqdmulh(v27, __ T8H, v3, as_FloatRegister(vr19));\n-    __ mulv(v19, __ T8H, v3, as_FloatRegister(vr19));\n-    __ mulv(v16, __ T8H, v16, v30);\n-    __ mulv(v17, __ T8H, v17, v30);\n-    __ mulv(v18, __ T8H, v18, v30);\n-    __ mulv(v19, __ T8H, v19, v30);\n-    __ sqdmulh(v16, __ T8H, v16, v31);\n-    __ sqdmulh(v17, __ T8H, v17, v31);\n-    __ sqdmulh(v18, __ T8H, v18, v31);\n-    __ sqdmulh(v19, __ T8H, v19, v31);\n-    __ shsubv(v16, __ T8H, v24, v16);\n-    __ shsubv(v17, __ T8H, v25, v17);\n-    __ shsubv(v18, __ T8H, v26, v18);\n-    __ shsubv(v19, __ T8H, v27, v19);\n-    __ sqdmulh(v24, __ T8H, v4, as_FloatRegister(vr20));\n-    __ mulv(v20, __ T8H, v4, as_FloatRegister(vr20));\n-    __ sqdmulh(v25, __ T8H, v5, as_FloatRegister(vr21));\n-    __ mulv(v21, __ T8H, v5, as_FloatRegister(vr21));\n-    __ sqdmulh(v26, __ T8H, v6, as_FloatRegister(vr22));\n-    __ mulv(v22, __ T8H, v6, as_FloatRegister(vr22));\n-    __ sqdmulh(v27, __ T8H, v7, as_FloatRegister(vr23));\n-    __ mulv(v23, __ T8H, v7, as_FloatRegister(vr23));\n-    __ mulv(v20, __ T8H, v20, v30);\n-    __ mulv(v21, __ T8H, v21, v30);\n-    __ mulv(v22, __ T8H, v22, v30);\n-    __ mulv(v23, __ T8H, v23, v30);\n-    __ sqdmulh(v20, __ T8H, v20, v31);\n-    __ sqdmulh(v21, __ T8H, v21, v31);\n-    __ sqdmulh(v22, __ T8H, v22, v31);\n-    __ sqdmulh(v23, __ T8H, v23, v31);\n-    __ shsubv(v20, __ T8H, v24, v20);\n-    __ shsubv(v21, __ T8H, v25, v21);\n-    __ shsubv(v22, __ T8H, v26, v22);\n-    __ shsubv(v23, __ T8H, v27, v23);\n+\n+    for (int i = 0; i < 2; i++) {\n+      __ shsubv(va[i], T, vtmp[i], va[i]);   \/\/ a = (aHigh - n) \/ 2\n+    }\n+  }\n+\n+  \/\/ Perform 16 16-bit Montgomery multiplications in parallel.\n+  void kyber_montmul16(const VSeq<2>& va, const VSeq<2>& vb, const VSeq<2>& vc,\n+                       const VSeq<2>& vtmp, const VSeq<2>& vq) {\n+    \/\/ Use the helper routine to schedule a 2x8H Montgomery multiply.\n+    \/\/ It will assert that the register use is valid\n+    vs_montmul2(va, vb, vc, __ T8H, vtmp, vq);\n+  }\n+\n+  \/\/ Perform 32 16-bit Montgomery multiplications in parallel.\n+  void kyber_montmul32(const VSeq<4>& va, const VSeq<4>& vb, const VSeq<4>& vc,\n+                       const VSeq<4>& vtmp, const VSeq<2>& vq) {\n+    \/\/ Use the helper routine to schedule a 4x8H Montgomery multiply.\n+    \/\/ It will assert that the register use is valid\n+    vs_montmul4(va, vb, vc, __ T8H, vtmp, vq);\n+  }\n+\n+  \/\/ Perform 64 16-bit Montgomery multiplications in parallel.\n+  void kyber_montmul64(const VSeq<8>& va, const VSeq<8>& vb, const VSeq<8>& vc,\n+                       const VSeq<4>& vtmp, const VSeq<2>& vq) {\n+    \/\/ Schedule two successive 4x8H multiplies via the montmul helper\n+    \/\/ on the front and back halves of va, vb and vc. The helper will\n+    \/\/ assert that the register use has no overlap conflicts on each\n+    \/\/ individual call but we also need to ensure that the necessary\n+    \/\/ disjoint\/equality constraints are met across both calls.\n+\n+    \/\/ vb, vc, vtmp and vq must be disjoint. va must either be\n+    \/\/ disjoint from all other registers or equal vc\n+\n+    assert(vs_disjoint(vb, vc), \"vb and vc overlap\");\n+    assert(vs_disjoint(vb, vq), \"vb and vq overlap\");\n+    assert(vs_disjoint(vb, vtmp), \"vb and vtmp overlap\");\n+\n+    assert(vs_disjoint(vc, vq), \"vc and vq overlap\");\n+    assert(vs_disjoint(vc, vtmp), \"vc and vtmp overlap\");\n+\n+    assert(vs_disjoint(vq, vtmp), \"vq and vtmp overlap\");\n+\n+    assert(vs_disjoint(va, vc) || vs_same(va, vc), \"va and vc neither disjoint nor equal\");\n+    assert(vs_disjoint(va, vb), \"va and vb overlap\");\n+    assert(vs_disjoint(va, vq), \"va and vq overlap\");\n+    assert(vs_disjoint(va, vtmp), \"va and vtmp overlap\");\n+\n+    \/\/ we multiply the front and back halves of each sequence 4 at a\n+    \/\/ time because\n+    \/\/\n+    \/\/ 1) we are currently only able to get 4-way instruction\n+    \/\/ parallelism at best\n+    \/\/\n+    \/\/ 2) we need registers for the constants in vq and temporary\n+    \/\/ scratch registers to hold intermediate results so vtmp can only\n+    \/\/ be a VSeq<4> which means we only have 4 scratch slots\n+\n+    vs_montmul4(vs_front(va), vs_front(vb), vs_front(vc), __ T8H, vtmp, vq);\n+    vs_montmul4(vs_back(va), vs_back(vb), vs_back(vc), __ T8H, vtmp, vq);\n@@ -4708,17 +5087,10 @@\n-  void kyber_subv_addv64() {\n-    __ subv(v24, __ T8H, v0, v16);\n-    __ subv(v25, __ T8H, v1, v17);\n-    __ subv(v26, __ T8H, v2, v18);\n-    __ subv(v27, __ T8H, v3, v19);\n-    __ subv(v28, __ T8H, v4, v20);\n-    __ subv(v29, __ T8H, v5, v21);\n-    __ subv(v30, __ T8H, v6, v22);\n-    __ subv(v31, __ T8H, v7, v23);\n-    __ addv(v0, __ T8H, v0, v16);\n-    __ addv(v1, __ T8H, v1, v17);\n-    __ addv(v2, __ T8H, v2, v18);\n-    __ addv(v3, __ T8H, v3, v19);\n-    __ addv(v4, __ T8H, v4, v20);\n-    __ addv(v5, __ T8H, v5, v21);\n-    __ addv(v6, __ T8H, v6, v22);\n-    __ addv(v7, __ T8H, v7, v23);\n+  void kyber_montmul32_sub_add(const VSeq<4>& va0, const VSeq<4>& va1,\n+                               const VSeq<4>& vc,\n+                               const VSeq<4>& vtmp,\n+                               const VSeq<2>& vq) {\n+    \/\/ compute a = montmul(a1, c)\n+    kyber_montmul32(vc, va1, vc, vtmp, vq);\n+    \/\/ ouptut a1 = a0 - a\n+    vs_subv(va1, __ T8H, va0, vc);\n+    \/\/    and a0 = a0 + a\n+    vs_addv(va0, __ T8H, va0, vc);\n@@ -4727,5 +5099,11 @@\n-  void kyber_store64coeffs(int i0, Register tmpAddr) {\n-    __ stpq(as_FloatRegister(i0), as_FloatRegister(i0 + 1), __ post(tmpAddr, 32));\n-    __ stpq(as_FloatRegister(i0 + 2), as_FloatRegister(i0 + 3), __ post(tmpAddr, 32));\n-    __ stpq(as_FloatRegister(i0 + 4), as_FloatRegister(i0 + 5), __ post(tmpAddr, 32));\n-    __ stpq(as_FloatRegister(i0 + 6), as_FloatRegister(i0 + 7), __ post(tmpAddr, 32));\n+  void kyber_sub_add_montmul32(const VSeq<4>& va0, const VSeq<4>& va1,\n+                               const VSeq<4>& vb,\n+                               const VSeq<4>& vtmp1,\n+                               const VSeq<4>& vtmp2,\n+                               const VSeq<2>& vq) {\n+    \/\/ compute c = a0 - a1\n+    vs_subv(vtmp1, __ T8H, va0, va1);\n+    \/\/ output a0 = a0 + a1\n+    vs_addv(va0, __ T8H, va0, va1);\n+    \/\/ output a1 = b montmul c\n+    kyber_montmul32(va1, vtmp1, vb, vtmp2, vq);\n@@ -4734,29 +5112,2 @@\n-  void kyber_montmul_subv_addv32() {\n-    __ sqdmulh(v24, __ T8H, v1, v16);\n-    __ mulv(v16, __ T8H, v1, v16);\n-    __ sqdmulh(v25, __ T8H, v3, v17);\n-    __ mulv(v17, __ T8H, v3, v17);\n-    __ sqdmulh(v26, __ T8H, v5, v18);\n-    __ mulv(v18, __ T8H, v5, v18);\n-    __ sqdmulh(v27, __ T8H, v7, v19);\n-    __ mulv(v19, __ T8H, v7, v19);\n-    __ mulv(v16, __ T8H, v16, v30);\n-    __ mulv(v17, __ T8H, v17, v30);\n-    __ mulv(v18, __ T8H, v18, v30);\n-    __ mulv(v19, __ T8H, v19, v30);\n-    __ sqdmulh(v16, __ T8H, v16, v31);\n-    __ sqdmulh(v17, __ T8H, v17, v31);\n-    __ sqdmulh(v18, __ T8H, v18, v31);\n-    __ sqdmulh(v19, __ T8H, v19, v31);\n-    __ shsubv(v16, __ T8H, v24, v16);\n-    __ shsubv(v17, __ T8H, v25, v17);\n-    __ shsubv(v18, __ T8H, v26, v18);\n-    __ shsubv(v19, __ T8H, v27, v19);\n-    __ subv(v1, __ T8H, v0, v16);\n-    __ subv(v3, __ T8H, v2, v17);\n-    __ subv(v5, __ T8H, v4, v18);\n-    __ subv(v7, __ T8H, v6, v19);\n-    __ addv(v0, __ T8H, v0, v16);\n-    __ addv(v2, __ T8H, v2, v17);\n-    __ addv(v4, __ T8H, v4, v18);\n-    __ addv(v6, __ T8H, v6, v19);\n+  void load64shorts(const VSeq<8>& v, Register shorts) {\n+    vs_ldpq_post(v, shorts);\n@@ -4765,29 +5116,2 @@\n-  void kyber_subv_addv_montmul32() {\n-    __ subv(v20, __ T8H, v0, v1);\n-    __ subv(v21, __ T8H, v2, v3);\n-    __ subv(v22, __ T8H, v4, v5);\n-    __ subv(v23, __ T8H, v6, v7);\n-    __ addv(v0, __ T8H, v0, v1);\n-    __ addv(v2, __ T8H, v2, v3);\n-    __ addv(v4, __ T8H, v4, v5);\n-    __ addv(v6, __ T8H, v6, v7);\n-    __ sqdmulh(v24, __ T8H, v20, v16);\n-    __ mulv(v1, __ T8H, v20, v16);\n-    __ sqdmulh(v25, __ T8H, v21, v17);\n-    __ mulv(v3, __ T8H, v21, v17);\n-    __ sqdmulh(v26, __ T8H, v22, v18);\n-    __ mulv(v5, __ T8H, v22, v18);\n-    __ sqdmulh(v27, __ T8H, v23, v19);\n-    __ mulv(v7, __ T8H, v23, v19);\n-    __ mulv(v1, __ T8H, v1, v30);\n-    __ mulv(v3, __ T8H, v3, v30);\n-    __ mulv(v5, __ T8H, v5, v30);\n-    __ mulv(v7, __ T8H, v7, v30);\n-    __ sqdmulh(v1, __ T8H, v1, v31);\n-    __ sqdmulh(v3, __ T8H, v3, v31);\n-    __ sqdmulh(v5, __ T8H, v5, v31);\n-    __ sqdmulh(v7, __ T8H, v7, v31);\n-    __ shsubv(v1, __ T8H, v24, v1);\n-    __ shsubv(v3, __ T8H, v25, v3);\n-    __ shsubv(v5, __ T8H, v26, v5);\n-    __ shsubv(v7, __ T8H, v27, v7);\n+  void load32shorts(const VSeq<4>& v, Register shorts) {\n+    vs_ldpq_post(v, shorts);\n@@ -4796,17 +5120,2 @@\n-  void kyber_addv_subv64() {\n-    __ addv(v24, __ T8H, v0, v16);\n-    __ addv(v25, __ T8H, v1, v17);\n-    __ addv(v26, __ T8H, v2, v18);\n-    __ addv(v27, __ T8H, v3, v19);\n-    __ addv(v28, __ T8H, v4, v20);\n-    __ addv(v29, __ T8H, v5, v21);\n-    __ addv(v30, __ T8H, v6, v22);\n-    __ addv(v31, __ T8H, v7, v23);\n-    __ subv(v0, __ T8H, v0, v16);\n-    __ subv(v1, __ T8H, v1, v17);\n-    __ subv(v2, __ T8H, v2, v18);\n-    __ subv(v3, __ T8H, v3, v19);\n-    __ subv(v4, __ T8H, v4, v20);\n-    __ subv(v5, __ T8H, v5, v21);\n-    __ subv(v6, __ T8H, v6, v22);\n-    __ subv(v7, __ T8H, v7, v23);\n+  void store64shorts(VSeq<8> v, Register tmpAddr) {\n+    vs_stpq_post(v, tmpAddr);\n@@ -4835,0 +5144,4 @@\n+    VSeq<8> vs1(0), vs2(16), vs3(24);  \/\/ 3 sets of 8x8H inputs\/outputs\n+    VSeq<4> vtmp = vs_front(vs3);      \/\/ n.b. tmp registers overlap vs3\n+    VSeq<2> vq(30);                    \/\/ n.b. constants overlap vs3\n+\n@@ -4836,1 +5149,2 @@\n-    __ ldpq(v30, v31, kyberConsts);\n+    \/\/ load the montmul constants\n+    vs_ldpq(vq, kyberConsts);\n@@ -4859,3 +5173,3 @@\n-    kyber_load64coeffs(0, tmpAddr);\n-    kyber_load64zetas(zetas);\n-    kyber_montmul64(false);\n+    load64shorts(vs1, tmpAddr);\n+    load64shorts(vs2, zetas);\n+    kyber_montmul64(vs2, vs1, vs2, vtmp, vq);\n@@ -4863,2 +5177,3 @@\n-    kyber_load64coeffs(0, tmpAddr);\n-    kyber_subv_addv64();\n+    load64shorts(vs1, tmpAddr);\n+    vs_subv(vs3, __ T8H, vs1, vs2); \/\/ n.b. trashes vq\n+    vs_addv(vs1, __ T8H, vs1, vs2);\n@@ -4866,1 +5181,1 @@\n-    kyber_store64coeffs(0, tmpAddr);\n+    vs_stpq_post(vs1, tmpAddr);\n@@ -4868,5 +5183,6 @@\n-    kyber_store64coeffs(24, tmpAddr);\n-    __ ldpq(v30, v31, kyberConsts);\n-    kyber_load64coeffs(0, tmpAddr);\n-    kyber_load64zetas(zetas);\n-    kyber_montmul64(false);\n+    vs_stpq_post(vs3, tmpAddr);\n+    \/\/ restore montmul constants\n+    vs_ldpq(vq, kyberConsts);\n+    load64shorts(vs1, tmpAddr);\n+    load64shorts(vs2, zetas);\n+    kyber_montmul64(vs2, vs1, vs2, vtmp, vq);\n@@ -4874,2 +5190,3 @@\n-    kyber_load64coeffs(0, tmpAddr);\n-    kyber_subv_addv64();\n+    load64shorts(vs1, tmpAddr);\n+    vs_subv(vs3, __ T8H, vs1, vs2); \/\/ n.b. trashes vq\n+    vs_addv(vs1, __ T8H, vs1, vs2);\n@@ -4877,1 +5194,1 @@\n-    kyber_store64coeffs(0, tmpAddr);\n+    store64shorts(vs1, tmpAddr);\n@@ -4879,1 +5196,2 @@\n-    kyber_store64coeffs(24, tmpAddr);\n+    store64shorts(vs3, tmpAddr);\n+\n@@ -4881,1 +5199,2 @@\n-    __ ldpq(v30, v31, kyberConsts);\n+    \/\/ restore montmul constants\n+    vs_ldpq(vq, kyberConsts);\n@@ -4883,3 +5202,3 @@\n-    kyber_load64coeffs(0, tmpAddr);\n-    kyber_load64zetas(zetas);\n-    kyber_montmul64(false);\n+    load64shorts(vs1, tmpAddr);\n+    load64shorts(vs2, zetas);\n+    kyber_montmul64(vs2, vs1, vs2, vtmp, vq);\n@@ -4887,2 +5206,3 @@\n-    kyber_load64coeffs(0, tmpAddr);\n-    kyber_subv_addv64();\n+    load64shorts(vs1, tmpAddr);\n+    vs_subv(vs3, __ T8H, vs1, vs2); \/\/ n.b. trashes vq\n+    vs_addv(vs1, __ T8H, vs1, vs2);\n@@ -4890,3 +5210,3 @@\n-    kyber_store64coeffs(0, tmpAddr);\n-    kyber_store64coeffs(24, tmpAddr);\n-    __ ldpq(v30, v31, kyberConsts);\n+    store64shorts(vs1, tmpAddr);\n+    store64shorts(vs3, tmpAddr);\n+    vs_ldpq(vq, kyberConsts);\n@@ -4894,3 +5214,3 @@\n-    kyber_load64coeffs(0, tmpAddr);\n-    kyber_load64zetas(zetas);\n-    kyber_montmul64(false);\n+    load64shorts(vs1, tmpAddr);\n+    load64shorts(vs2, zetas);\n+    kyber_montmul64(vs2, vs1, vs2, vtmp, vq);\n@@ -4898,2 +5218,3 @@\n-    kyber_load64coeffs(0, tmpAddr);\n-    kyber_subv_addv64();\n+    load64shorts(vs1, tmpAddr);\n+    vs_subv(vs3, __ T8H, vs1, vs2); \/\/ n.b. trashes vq\n+    vs_addv(vs1, __ T8H, vs1, vs2);\n@@ -4901,2 +5222,3 @@\n-    kyber_store64coeffs(0, tmpAddr);\n-    kyber_store64coeffs(24, tmpAddr);\n+    store64shorts(vs1, tmpAddr);\n+    store64shorts(vs3, tmpAddr);\n+\n@@ -4904,8 +5226,9 @@\n-    __ ldpq(v30, v31, kyberConsts);\n-    __ add(tmpAddr, coeffs, 64);\n-    __ ldpq(v0, v1, __ post(tmpAddr, 32));\n-    __ ldpq(v2, v3, __ post(tmpAddr, 96));\n-    __ ldpq(v4, v5, __ post(tmpAddr, 32));\n-    __ ldpq(v6, v7, tmpAddr);\n-    kyber_load64zetas(zetas);\n-    kyber_montmul64(false);\n+    vs_ldpq(vq, kyberConsts);\n+    int offsets1[4] = { 0, 32, 128, 160 };\n+    vs_ldpq_indexed(vs1, coeffs, 64, offsets1);\n+    load64shorts(vs2, zetas);\n+    kyber_montmul64(vs2, vs1, vs2, vtmp, vq);\n+    vs_ldpq_indexed(vs1, coeffs, 0, offsets1);\n+    \/\/ kyber_subv_addv64();\n+    vs_subv(vs3, __ T8H, vs1, vs2); \/\/ n.b. trashes vq\n+    vs_addv(vs1, __ T8H, vs1, vs2);\n@@ -4913,27 +5236,12 @@\n-    __ ldpq(v0, v1, __ post(tmpAddr, 32));\n-    __ ldpq(v2, v3, __ post(tmpAddr, 96));\n-    __ ldpq(v4, v5, __ post(tmpAddr, 32));\n-    __ ldpq(v6, v7, tmpAddr);\n-    kyber_subv_addv64();\n-    __ add(tmpAddr, coeffs, 0);\n-    __ stpq(v0, v1, __ post(tmpAddr, 32));\n-    __ stpq(v2, v3, __ post(tmpAddr, 32));\n-    __ stpq(v24, v25, __ post(tmpAddr, 32));\n-    __ stpq(v26, v27, __ post(tmpAddr, 32));\n-    __ stpq(v4, v5, __ post(tmpAddr, 32));\n-    __ stpq(v6, v7, __ post(tmpAddr, 32));\n-    __ stpq(v28, v29, __ post(tmpAddr, 32));\n-    __ stpq(v30, v31, __ post(tmpAddr, 96));\n-    __ ldpq(v30, v31, kyberConsts);\n-    __ ldpq(v0, v1, __ post(tmpAddr, 32));\n-    __ ldpq(v2, v3, __ post(tmpAddr, 96));\n-    __ ldpq(v4, v5, __ post(tmpAddr, 32));\n-    __ ldpq(v6, v7, tmpAddr);\n-    kyber_load64zetas(zetas);\n-    kyber_montmul64(false);\n-    __ add(tmpAddr, coeffs, 256);\n-    __ ldpq(v0, v1, __ post(tmpAddr, 32));\n-    __ ldpq(v2, v3, __ post(tmpAddr, 96));\n-    __ ldpq(v4, v5, __ post(tmpAddr, 32));\n-    __ ldpq(v6, v7, tmpAddr);\n-    kyber_subv_addv64();\n+    vs_stpq_post(vs_front(vs1), tmpAddr);\n+    vs_stpq_post(vs_front(vs3), tmpAddr);\n+    vs_stpq_post(vs_back(vs1), tmpAddr);\n+    vs_stpq_post(vs_back(vs3), tmpAddr);\n+    vs_ldpq(vq, kyberConsts);\n+    vs_ldpq_indexed(vs1, tmpAddr, 64, offsets1);\n+    load64shorts(vs2, zetas);\n+    kyber_montmul64(vs2, vs1, vs2, vtmp, vq);\n+    vs_ldpq_indexed(vs1,  coeffs, 256, offsets1);\n+    \/\/ kyber_subv_addv64();\n+    vs_subv(vs3, __ T8H, vs1, vs2); \/\/ n.b. trashes vq\n+    vs_addv(vs1, __ T8H, vs1, vs2);\n@@ -4941,8 +5249,5 @@\n-    __ stpq(v0, v1, __ post(tmpAddr, 32));\n-    __ stpq(v2, v3, __ post(tmpAddr, 32));\n-    __ stpq(v24, v25, __ post(tmpAddr, 32));\n-    __ stpq(v26, v27, __ post(tmpAddr, 32));\n-    __ stpq(v4, v5, __ post(tmpAddr, 32));\n-    __ stpq(v6, v7, __ post(tmpAddr, 32));\n-    __ stpq(v28, v29, __ post(tmpAddr, 32));\n-    __ stpq(v30, v31, tmpAddr);\n+    vs_stpq_post(vs_front(vs1), tmpAddr);\n+    vs_stpq_post(vs_front(vs3), tmpAddr);\n+    vs_stpq_post(vs_back(vs1), tmpAddr);\n+    vs_stpq_post(vs_back(vs3), tmpAddr);\n+\n@@ -4950,46 +5255,21 @@\n-    __ ldpq(v30, v31, kyberConsts);\n-    __ add(tmpAddr, coeffs, 32);\n-    __ ldpq(v0, v1, __ post(tmpAddr, 64));\n-    __ ldpq(v2, v3, __ post(tmpAddr, 64));\n-    __ ldpq(v4, v5, __ post(tmpAddr, 64));\n-    __ ldpq(v6, v7, tmpAddr);\n-    kyber_load64zetas(zetas);\n-    kyber_montmul64(false);\n-    __ add(tmpAddr, coeffs, 0);\n-    __ ldpq(v0, v1, __ post(tmpAddr, 64));\n-    __ ldpq(v2, v3, __ post(tmpAddr, 64));\n-    __ ldpq(v4, v5, __ post(tmpAddr, 64));\n-    __ ldpq(v6, v7, tmpAddr);\n-    kyber_subv_addv64();\n-    __ add(tmpAddr, coeffs, 0);\n-    __ stpq(v0, v1, __ post(tmpAddr, 32));\n-    __ stpq(v24, v25, __ post(tmpAddr, 32));\n-    __ stpq(v2, v3, __ post(tmpAddr, 32));\n-    __ stpq(v26, v27, __ post(tmpAddr, 32));\n-    __ stpq(v4, v5, __ post(tmpAddr, 32));\n-    __ stpq(v28, v29, __ post(tmpAddr, 32));\n-    __ stpq(v6, v7, __ post(tmpAddr, 32));\n-    __ stpq(v30, v31, __ post(tmpAddr, 64));\n-\n-    __ ldpq(v30, v31, kyberConsts);\n-    __ ldpq(v0, v1, __ post(tmpAddr, 64));\n-    __ ldpq(v2, v3, __ post(tmpAddr, 64));\n-    __ ldpq(v4, v5, __ post(tmpAddr, 64));\n-    __ ldpq(v6, v7, tmpAddr);\n-    kyber_load64zetas(zetas);\n-    kyber_montmul64(false);\n-    __ add(tmpAddr, coeffs, 256);\n-    __ ldpq(v0, v1, __ post(tmpAddr, 64));\n-    __ ldpq(v2, v3, __ post(tmpAddr, 64));\n-    __ ldpq(v4, v5, __ post(tmpAddr, 64));\n-    __ ldpq(v6, v7, tmpAddr);\n-    kyber_subv_addv64();\n-    __ add(tmpAddr, coeffs, 256);\n-    __ stpq(v0, v1, __ post(tmpAddr, 32));\n-    __ stpq(v24, v25, __ post(tmpAddr, 32));\n-    __ stpq(v2, v3, __ post(tmpAddr, 32));\n-    __ stpq(v26, v27, __ post(tmpAddr, 32));\n-    __ stpq(v4, v5, __ post(tmpAddr, 32));\n-    __ stpq(v28, v29, __ post(tmpAddr, 32));\n-    __ stpq(v6, v7, __ post(tmpAddr, 32));\n-    __ stpq(v30, v31, tmpAddr);\n+    vs_ldpq(vq, kyberConsts);\n+    int offsets2[4] = { 0, 64, 128, 192 };\n+    vs_ldpq_indexed(vs1, coeffs, 32, offsets2);\n+    load64shorts(vs2, zetas);\n+    kyber_montmul64(vs2, vs1, vs2, vtmp, vq);\n+    vs_ldpq_indexed(vs1, coeffs, 0, offsets2);\n+    vs_subv(vs3, __ T8H, vs1, vs2); \/\/ n.b. trashes vq\n+    vs_addv(vs1, __ T8H, vs1, vs2);\n+    vs_stpq_indexed(vs1, coeffs, 0, offsets2);\n+    vs_stpq_indexed(vs3, coeffs, 32, offsets2);\n+\n+    vs_ldpq(vq, kyberConsts);\n+    vs_ldpq_indexed(vs1, coeffs, 256 + 32, offsets2);\n+    load64shorts(vs2, zetas);\n+    kyber_montmul64(vs2, vs1, vs2, vtmp, vq);\n+    vs_ldpq_indexed(vs1, coeffs, 256, offsets2);\n+    vs_subv(vs3, __ T8H, vs1, vs2); \/\/ n.b. trashes vq\n+    vs_addv(vs1, __ T8H, vs1, vs2);\n+    vs_stpq_indexed(vs1, coeffs, 256, offsets2);\n+    vs_stpq_indexed(vs3, coeffs, 256 + 32, offsets2);\n+\n@@ -4997,78 +5277,21 @@\n-    __ ldpq(v30, v31, kyberConsts);\n-    __ add(tmpAddr, coeffs, 16);\n-    __ ldr(v0, __ Q, __ post(tmpAddr, 32));\n-    __ ldr(v1, __ Q, __ post(tmpAddr, 32));\n-    __ ldr(v2, __ Q, __ post(tmpAddr, 32));\n-    __ ldr(v3, __ Q, __ post(tmpAddr, 32));\n-    __ ldr(v4, __ Q, __ post(tmpAddr, 32));\n-    __ ldr(v5, __ Q, __ post(tmpAddr, 32));\n-    __ ldr(v6, __ Q, __ post(tmpAddr, 32));\n-    __ ldr(v7, __ Q, tmpAddr);\n-    kyber_load64zetas(zetas);\n-    kyber_montmul64(false);\n-    __ add(tmpAddr, coeffs, 0);\n-    __ ldr(v0, __ Q, __ post(tmpAddr, 32));\n-    __ ldr(v1, __ Q, __ post(tmpAddr, 32));\n-    __ ldr(v2, __ Q, __ post(tmpAddr, 32));\n-    __ ldr(v3, __ Q, __ post(tmpAddr, 32));\n-    __ ldr(v4, __ Q, __ post(tmpAddr, 32));\n-    __ ldr(v5, __ Q, __ post(tmpAddr, 32));\n-    __ ldr(v6, __ Q, __ post(tmpAddr, 32));\n-    __ ldr(v7, __ Q, tmpAddr);\n-    kyber_subv_addv64();\n-    __ add(tmpAddr, coeffs, 0);\n-    __ str(v0, __ Q, __ post(tmpAddr, 16));\n-    __ str(v24, __ Q, __ post(tmpAddr, 16));\n-    __ str(v1, __ Q, __ post(tmpAddr, 16));\n-    __ str(v25, __ Q, __ post(tmpAddr, 16));\n-    __ str(v2, __ Q, __ post(tmpAddr, 16));\n-    __ str(v26, __ Q, __ post(tmpAddr, 16));\n-    __ str(v3, __ Q, __ post(tmpAddr, 16));\n-    __ str(v27, __ Q, __ post(tmpAddr, 16));\n-    __ str(v4, __ Q, __ post(tmpAddr, 16));\n-    __ str(v28, __ Q, __ post(tmpAddr, 16));\n-    __ str(v5, __ Q, __ post(tmpAddr, 16));\n-    __ str(v29, __ Q, __ post(tmpAddr, 16));\n-    __ str(v6, __ Q, __ post(tmpAddr, 16));\n-    __ str(v30, __ Q, __ post(tmpAddr, 16));\n-    __ str(v7, __ Q, __ post(tmpAddr, 16));\n-    __ str(v31, __ Q, __ post(tmpAddr, 32));\n-\n-    __ ldpq(v30, v31, kyberConsts);\n-    __ ldr(v0, __ Q, __ post(tmpAddr, 32));\n-    __ ldr(v1, __ Q, __ post(tmpAddr, 32));\n-    __ ldr(v2, __ Q, __ post(tmpAddr, 32));\n-    __ ldr(v3, __ Q, __ post(tmpAddr, 32));\n-    __ ldr(v4, __ Q, __ post(tmpAddr, 32));\n-    __ ldr(v5, __ Q, __ post(tmpAddr, 32));\n-    __ ldr(v6, __ Q, __ post(tmpAddr, 32));\n-    __ ldr(v7, __ Q, tmpAddr);\n-    kyber_load64zetas(zetas);\n-    kyber_montmul64(false);\n-    __ add(tmpAddr, coeffs, 256);\n-    __ ldr(v0, __ Q, __ post(tmpAddr, 32));\n-    __ ldr(v1, __ Q, __ post(tmpAddr, 32));\n-    __ ldr(v2, __ Q, __ post(tmpAddr, 32));\n-    __ ldr(v3, __ Q, __ post(tmpAddr, 32));\n-    __ ldr(v4, __ Q, __ post(tmpAddr, 32));\n-    __ ldr(v5, __ Q, __ post(tmpAddr, 32));\n-    __ ldr(v6, __ Q, __ post(tmpAddr, 32));\n-    __ ldr(v7, __ Q, tmpAddr);\n-    kyber_subv_addv64();\n-    __ add(tmpAddr, coeffs, 256);\n-    __ str(v0, __ Q, __ post(tmpAddr, 16));\n-    __ str(v24, __ Q, __ post(tmpAddr, 16));\n-    __ str(v1, __ Q, __ post(tmpAddr, 16));\n-    __ str(v25, __ Q, __ post(tmpAddr, 16));\n-    __ str(v2, __ Q, __ post(tmpAddr, 16));\n-    __ str(v26, __ Q, __ post(tmpAddr, 16));\n-    __ str(v3, __ Q, __ post(tmpAddr, 16));\n-    __ str(v27, __ Q, __ post(tmpAddr, 16));\n-    __ str(v4, __ Q, __ post(tmpAddr, 16));\n-    __ str(v28, __ Q, __ post(tmpAddr, 16));\n-    __ str(v5, __ Q, __ post(tmpAddr, 16));\n-    __ str(v29, __ Q, __ post(tmpAddr, 16));\n-    __ str(v6, __ Q, __ post(tmpAddr, 16));\n-    __ str(v30, __ Q, __ post(tmpAddr, 16));\n-    __ str(v7, __ Q, __ post(tmpAddr, 16));\n-    __ str(v31, __ Q, tmpAddr);\n+    vs_ldpq(vq, kyberConsts);\n+    int offsets3[8] = { 0, 32, 64, 96, 128, 160, 192, 224 };\n+    vs_ldr_indexed(vs1, __ Q, coeffs, 16, offsets3);\n+    load64shorts(vs2, zetas);\n+    kyber_montmul64(vs2, vs1, vs2, vtmp, vq);\n+    vs_ldr_indexed(vs1, __ Q, coeffs, 0, offsets3);\n+    vs_subv(vs3, __ T8H, vs1, vs2); \/\/ n.b. trashes vq\n+    vs_addv(vs1, __ T8H, vs1, vs2);\n+    vs_str_indexed(vs1, __ Q, coeffs, 0, offsets3);\n+    vs_str_indexed(vs3, __ Q, coeffs, 16, offsets3);\n+\n+    vs_ldpq(vq, kyberConsts);\n+    vs_ldr_indexed(vs1, __ Q, coeffs, 256 + 16, offsets3);\n+    load64shorts(vs2, zetas);\n+    kyber_montmul64(vs2, vs1, vs2, vtmp, vq);\n+    vs_ldr_indexed(vs1, __ Q, coeffs, 256, offsets3);\n+    vs_subv(vs3, __ T8H, vs1, vs2); \/\/ n.b. trashes vq\n+    vs_addv(vs1, __ T8H, vs1, vs2);\n+    vs_str_indexed(vs1, __ Q, coeffs, 256, offsets3);\n+    vs_str_indexed(vs3, __ Q, coeffs, 256 + 16, offsets3);\n+\n@@ -5076,53 +5299,20 @@\n-    __ ldpq(v30, v31, kyberConsts);\n-    __ add(tmpAddr, coeffs, 0);\n-    __ ld2(v0, v1, __ T2D, __ post(tmpAddr, 32));\n-    __ ld2(v2, v3, __ T2D, __ post(tmpAddr, 32));\n-    __ ld2(v4, v5, __ T2D, __ post(tmpAddr, 32));\n-    __ ld2(v6, v7, __ T2D, tmpAddr);\n-    __ ldpq(v16, v17, __ post(zetas, 32));\n-    __ ldpq(v18, v19, __ post(zetas, 32));\n-    kyber_montmul_subv_addv32();\n-    __ add(tmpAddr, coeffs, 0);\n-    __ st2(v0, v1, __ T2D, __ post(tmpAddr, 32));\n-    __ st2(v2, v3, __ T2D, __ post(tmpAddr, 32));\n-    __ st2(v4, v5, __ T2D, __ post(tmpAddr, 32));\n-    __ st2(v6, v7, __ T2D, __ post(tmpAddr, 32));\n-\n-    __ ld2(v0, v1, __ T2D, __ post(tmpAddr, 32));\n-    __ ld2(v2, v3, __ T2D, __ post(tmpAddr, 32));\n-    __ ld2(v4, v5, __ T2D, __ post(tmpAddr, 32));\n-    __ ld2(v6, v7, __ T2D, tmpAddr);\n-    __ ldpq(v16, v17, __ post(zetas, 32));\n-    __ ldpq(v18, v19, __ post(zetas, 32));\n-    kyber_montmul_subv_addv32();\n-    __ add(tmpAddr, coeffs, 128);\n-    __ st2(v0, v1, __ T2D, __ post(tmpAddr, 32));\n-    __ st2(v2, v3, __ T2D, __ post(tmpAddr, 32));\n-    __ st2(v4, v5, __ T2D, __ post(tmpAddr, 32));\n-    __ st2(v6, v7, __ T2D, __ post(tmpAddr, 32));\n-\n-    __ ld2(v0, v1, __ T2D, __ post(tmpAddr, 32));\n-    __ ld2(v2, v3, __ T2D, __ post(tmpAddr, 32));\n-    __ ld2(v4, v5, __ T2D, __ post(tmpAddr, 32));\n-    __ ld2(v6, v7, __ T2D, tmpAddr);\n-    __ ldpq(v16, v17, __ post(zetas, 32));\n-    __ ldpq(v18, v19, __ post(zetas, 32));\n-    kyber_montmul_subv_addv32();\n-    __ add(tmpAddr, coeffs, 256);\n-    __ st2(v0, v1, __ T2D, __ post(tmpAddr, 32));\n-    __ st2(v2, v3, __ T2D, __ post(tmpAddr, 32));\n-    __ st2(v4, v5, __ T2D, __ post(tmpAddr, 32));\n-    __ st2(v6, v7, __ T2D, __ post(tmpAddr, 32));\n-\n-    __ ld2(v0, v1, __ T2D, __ post(tmpAddr, 32));\n-    __ ld2(v2, v3, __ T2D, __ post(tmpAddr, 32));\n-    __ ld2(v4, v5, __ T2D, __ post(tmpAddr, 32));\n-    __ ld2(v6, v7, __ T2D, tmpAddr);\n-    __ ldpq(v16, v17, __ post(zetas, 32));\n-    __ ldpq(v18, v19, __ post(zetas, 32));\n-    kyber_montmul_subv_addv32();\n-    __ add(tmpAddr, coeffs, 384);\n-    __ st2(v0, v1, __ T2D, __ post(tmpAddr, 32));\n-    __ st2(v2, v3, __ T2D, __ post(tmpAddr, 32));\n-    __ st2(v4, v5, __ T2D, __ post(tmpAddr, 32));\n-    __ st2(v6, v7, __ T2D, tmpAddr);\n+    vs_ldpq(vq, kyberConsts);\n+    int offsets4[4] = { 0, 32, 64, 96 };\n+    vs_ld2_indexed(vs1, __ T2D, coeffs, tmpAddr, 0, offsets4);\n+    load32shorts(vs_front(vs2), zetas);\n+    kyber_montmul32_sub_add(vs_even(vs1), vs_odd(vs1), vs_front(vs2), vtmp, vq);\n+    vs_st2_indexed(vs1, __ T2D, coeffs, tmpAddr, 0, offsets4);\n+    vs_ld2_indexed(vs1, __ T2D, coeffs, tmpAddr, 128, offsets4);\n+    load32shorts(vs_front(vs2), zetas);\n+    kyber_montmul32_sub_add(vs_even(vs1), vs_odd(vs1), vs_front(vs2), vtmp, vq);\n+    vs_st2_indexed(vs1, __ T2D, coeffs, tmpAddr, 128, offsets4);\n+    vs_ld2_indexed(vs1, __ T2D, coeffs, tmpAddr, 256, offsets4);\n+    load32shorts(vs_front(vs2), zetas);\n+    kyber_montmul32_sub_add(vs_even(vs1), vs_odd(vs1), vs_front(vs2), vtmp, vq);\n+    vs_st2_indexed(vs1, __ T2D, coeffs, tmpAddr, 256, offsets4);\n+\n+    vs_ld2_indexed(vs1, __ T2D, coeffs, tmpAddr, 384, offsets4);\n+    load32shorts(vs_front(vs2), zetas);\n+    kyber_montmul32_sub_add(vs_even(vs1), vs_odd(vs1), vs_front(vs2), vtmp, vq);\n+    vs_st2_indexed(vs1, __ T2D, coeffs, tmpAddr, 384, offsets4);\n+\n@@ -5130,52 +5320,19 @@\n-    __ add(tmpAddr, coeffs, 0);\n-    __ ld2(v0, v1, __ T4S, __ post(tmpAddr, 32));\n-    __ ld2(v2, v3, __ T4S, __ post(tmpAddr, 32));\n-    __ ld2(v4, v5, __ T4S, __ post(tmpAddr, 32));\n-    __ ld2(v6, v7, __ T4S, tmpAddr);\n-    __ ldpq(v16, v17, __ post(zetas, 32));\n-    __ ldpq(v18, v19, __ post(zetas, 32));\n-    kyber_montmul_subv_addv32();\n-    __ add(tmpAddr, coeffs, 0);\n-    __ st2(v0, v1, __ T4S, __ post(tmpAddr, 32));\n-    __ st2(v2, v3, __ T4S, __ post(tmpAddr, 32));\n-    __ st2(v4, v5, __ T4S, __ post(tmpAddr, 32));\n-    __ st2(v6, v7, __ T4S, __ post(tmpAddr, 32));\n-\n-    __ ld2(v0, v1, __ T4S, __ post(tmpAddr, 32));\n-    __ ld2(v2, v3, __ T4S, __ post(tmpAddr, 32));\n-    __ ld2(v4, v5, __ T4S, __ post(tmpAddr, 32));\n-    __ ld2(v6, v7, __ T4S, tmpAddr);\n-    __ ldpq(v16, v17, __ post(zetas, 32));\n-    __ ldpq(v18, v19, __ post(zetas, 32));\n-    kyber_montmul_subv_addv32();\n-    __ add(tmpAddr, coeffs, 128);\n-    __ st2(v0, v1, __ T4S, __ post(tmpAddr, 32));\n-    __ st2(v2, v3, __ T4S, __ post(tmpAddr, 32));\n-    __ st2(v4, v5, __ T4S, __ post(tmpAddr, 32));\n-    __ st2(v6, v7, __ T4S, __ post(tmpAddr, 32));\n-\n-    __ ld2(v0, v1, __ T4S, __ post(tmpAddr, 32));\n-    __ ld2(v2, v3, __ T4S, __ post(tmpAddr, 32));\n-    __ ld2(v4, v5, __ T4S, __ post(tmpAddr, 32));\n-    __ ld2(v6, v7, __ T4S, tmpAddr);\n-    __ ldpq(v16, v17, __ post(zetas, 32));\n-    __ ldpq(v18, v19, __ post(zetas, 32));\n-    kyber_montmul_subv_addv32();\n-    __ add(tmpAddr, coeffs, 256);\n-    __ st2(v0, v1, __ T4S, __ post(tmpAddr, 32));\n-    __ st2(v2, v3, __ T4S, __ post(tmpAddr, 32));\n-    __ st2(v4, v5, __ T4S, __ post(tmpAddr, 32));\n-    __ st2(v6, v7, __ T4S, __ post(tmpAddr, 32));\n-\n-    __ ld2(v0, v1, __ T4S, __ post(tmpAddr, 32));\n-    __ ld2(v2, v3, __ T4S, __ post(tmpAddr, 32));\n-    __ ld2(v4, v5, __ T4S, __ post(tmpAddr, 32));\n-    __ ld2(v6, v7, __ T4S, tmpAddr);\n-    __ ldpq(v16, v17, __ post(zetas, 32));\n-    __ ldpq(v18, v19, zetas);\n-    kyber_montmul_subv_addv32();\n-    __ add(tmpAddr, coeffs, 384);\n-    __ st2(v0, v1, __ T4S, __ post(tmpAddr, 32));\n-    __ st2(v2, v3, __ T4S, __ post(tmpAddr, 32));\n-    __ st2(v4, v5, __ T4S, __ post(tmpAddr, 32));\n-    __ st2(v6, v7, __ T4S, tmpAddr);\n+    vs_ld2_indexed(vs1, __ T4S, coeffs, tmpAddr, 0, offsets4);\n+    load32shorts(vs_front(vs2), zetas);\n+    kyber_montmul32_sub_add(vs_even(vs1), vs_odd(vs1), vs_front(vs2), vtmp, vq);\n+    vs_st2_indexed(vs1, __ T4S, coeffs, tmpAddr, 0, offsets4);\n+    vs_ld2_indexed(vs1, __ T4S, coeffs, tmpAddr, 128, offsets4);\n+    \/\/ __ ldpq(v18, v19, __ post(zetas, 32));\n+    load32shorts(vs_front(vs2), zetas);\n+    kyber_montmul32_sub_add(vs_even(vs1), vs_odd(vs1), vs_front(vs2), vtmp, vq);\n+    vs_st2_indexed(vs1, __ T4S, coeffs, tmpAddr, 128, offsets4);\n+\n+    vs_ld2_indexed(vs1, __ T4S, coeffs, tmpAddr, 256, offsets4);\n+    load32shorts(vs_front(vs2), zetas);\n+    kyber_montmul32_sub_add(vs_even(vs1), vs_odd(vs1), vs_front(vs2), vtmp, vq);\n+    vs_st2_indexed(vs1, __ T4S, coeffs, tmpAddr, 256, offsets4);\n+\n+    vs_ld2_indexed(vs1, __ T4S, coeffs, tmpAddr, 384, offsets4);\n+    load32shorts(vs_front(vs2), zetas);\n+    kyber_montmul32_sub_add(vs_even(vs1), vs_odd(vs1), vs_front(vs2), vtmp, vq);\n+    vs_st2_indexed(vs1, __ T4S, coeffs, tmpAddr, 384, offsets4);\n@@ -5190,21 +5347,0 @@\n-  void kyber_sqdmulh32(int i0) {\n-    __ sqdmulh(as_FloatRegister(i0 + 18), __ T8H, as_FloatRegister(i0), v31);\n-    __ sqdmulh(as_FloatRegister(i0 + 19), __ T8H, as_FloatRegister(i0 + 1), v31);\n-    __ sqdmulh(as_FloatRegister(i0 + 20), __ T8H, as_FloatRegister(i0 + 2), v31);\n-    __ sqdmulh(as_FloatRegister(i0 + 21), __ T8H, as_FloatRegister(i0 + 3), v31);\n-  }\n-\n-  void kyber_sshr32(int i0) {\n-    __ sshr(as_FloatRegister(i0), __ T8H, as_FloatRegister(i0), 11);\n-    __ sshr(as_FloatRegister(i0 + 1), __ T8H, as_FloatRegister(i0 + 1), 11);\n-    __ sshr(as_FloatRegister(i0 + 2), __ T8H, as_FloatRegister(i0 + 2), 11);\n-    __ sshr(as_FloatRegister(i0 + 3), __ T8H, as_FloatRegister(i0 + 3), 11);\n-  }\n-\n-  void kyber_mlsv32(int o0) {\n-    __ mlsv(as_FloatRegister(o0), __ T8H, as_FloatRegister(o0 + 18), v30);\n-    __ mlsv(as_FloatRegister(o0 + 1), __ T8H, as_FloatRegister(o0 + 19), v30);\n-    __ mlsv(as_FloatRegister(o0 + 2), __ T8H, as_FloatRegister(o0 + 20), v30);\n-    __ mlsv(as_FloatRegister(o0 + 3), __ T8H, as_FloatRegister(o0 + 21), v30);\n-  }\n-\n@@ -5232,1 +5368,6 @@\n-    __ lea(kyberConsts, ExternalAddress((address) StubRoutines::aarch64::_kyberConsts));\n+    VSeq<8> vs1(0), vs2(16), vs3(24);  \/\/ 3 sets of 8x8H inputs\/outputs\n+    VSeq<4> vtmp = vs_front(vs3);      \/\/ n.b. tmp registers overlap vs3\n+    VSeq<2> vq(30);                    \/\/ n.b. constants overlap vs3\n+\n+    __ lea(kyberConsts,\n+             ExternalAddress((address) StubRoutines::aarch64::_kyberConsts));\n@@ -5235,51 +5376,23 @@\n-    __ ldpq(v30, v31, kyberConsts);\n-    __ add(tmpAddr, coeffs, 0);\n-    __ ld2(v0, v1, __ T4S, __ post(tmpAddr, 32));\n-    __ ld2(v2, v3, __ T4S, __ post(tmpAddr, 32));\n-    __ ld2(v4, v5, __ T4S, __ post(tmpAddr, 32));\n-    __ ld2(v6, v7, __ T4S, tmpAddr);\n-    __ ldpq(v16, v17, __ post(zetas, 32));\n-    __ ldpq(v18, v19, __ post(zetas, 32));\n-    kyber_subv_addv_montmul32();\n-    __ add(tmpAddr, coeffs, 0);\n-    __ st2(v0, v1, __ T4S, __ post(tmpAddr, 32));\n-    __ st2(v2, v3, __ T4S, __ post(tmpAddr, 32));\n-    __ st2(v4, v5, __ T4S, __ post(tmpAddr, 32));\n-    __ st2(v6, v7, __ T4S, __ post(tmpAddr, 32));\n-    __ ld2(v0, v1, __ T4S, __ post(tmpAddr, 32));\n-    __ ld2(v2, v3, __ T4S, __ post(tmpAddr, 32));\n-    __ ld2(v4, v5, __ T4S, __ post(tmpAddr, 32));\n-    __ ld2(v6, v7, __ T4S, tmpAddr);\n-    __ ldpq(v16, v17, __ post(zetas, 32));\n-    __ ldpq(v18, v19, __ post(zetas, 32));\n-    kyber_subv_addv_montmul32();\n-    __ add(tmpAddr, coeffs, 128);\n-    __ st2(v0, v1, __ T4S, __ post(tmpAddr, 32));\n-    __ st2(v2, v3, __ T4S, __ post(tmpAddr, 32));\n-    __ st2(v4, v5, __ T4S, __ post(tmpAddr, 32));\n-    __ st2(v6, v7, __ T4S, __ post(tmpAddr, 32));\n-\n-    __ ld2(v0, v1, __ T4S, __ post(tmpAddr, 32));\n-    __ ld2(v2, v3, __ T4S, __ post(tmpAddr, 32));\n-    __ ld2(v4, v5, __ T4S, __ post(tmpAddr, 32));\n-    __ ld2(v6, v7, __ T4S, tmpAddr);\n-    __ ldpq(v16, v17, __ post(zetas, 32));\n-    __ ldpq(v18, v19, __ post(zetas, 32));\n-    kyber_subv_addv_montmul32();\n-    __ add(tmpAddr, coeffs, 256);\n-    __ st2(v0, v1, __ T4S, __ post(tmpAddr, 32));\n-    __ st2(v2, v3, __ T4S, __ post(tmpAddr, 32));\n-    __ st2(v4, v5, __ T4S, __ post(tmpAddr, 32));\n-    __ st2(v6, v7, __ T4S, __ post(tmpAddr, 32));\n-    __ ld2(v0, v1, __ T4S, __ post(tmpAddr, 32));\n-    __ ld2(v2, v3, __ T4S, __ post(tmpAddr, 32));\n-    __ ld2(v4, v5, __ T4S, __ post(tmpAddr, 32));\n-    __ ld2(v6, v7, __ T4S, tmpAddr);\n-    __ ldpq(v16, v17, __ post(zetas, 32));\n-    __ ldpq(v18, v19, __ post(zetas, 32));\n-    kyber_subv_addv_montmul32();\n-    __ add(tmpAddr, coeffs, 384);\n-    __ st2(v0, v1, __ T4S, __ post(tmpAddr, 32));\n-    __ st2(v2, v3, __ T4S, __ post(tmpAddr, 32));\n-    __ st2(v4, v5, __ T4S, __ post(tmpAddr, 32));\n-    __ st2(v6, v7, __ T4S, tmpAddr);\n+    vs_ldpq(vq, kyberConsts);\n+    int offsets4[4] = { 0, 32, 64, 96 };\n+    vs_ld2_indexed(vs1, __ T4S, coeffs, tmpAddr, 0, offsets4);\n+    load32shorts(vs_front(vs2), zetas);\n+    kyber_sub_add_montmul32(vs_even(vs1), vs_odd(vs1),\n+                            vs_front(vs2), vs_back(vs2), vtmp, vq);\n+    vs_st2_indexed(vs1, __ T4S, coeffs, tmpAddr, 0, offsets4);\n+    vs_ld2_indexed(vs1, __ T4S, coeffs, tmpAddr, 128, offsets4);\n+    load32shorts(vs_front(vs2), zetas);\n+    kyber_sub_add_montmul32(vs_even(vs1), vs_odd(vs1),\n+                            vs_front(vs2), vs_back(vs2), vtmp, vq);\n+    vs_st2_indexed(vs1, __ T4S, coeffs, tmpAddr, 128, offsets4);\n+    vs_ld2_indexed(vs1, __ T4S, coeffs, tmpAddr, 256, offsets4);\n+    load32shorts(vs_front(vs2), zetas);\n+    kyber_sub_add_montmul32(vs_even(vs1), vs_odd(vs1),\n+                            vs_front(vs2), vs_back(vs2), vtmp, vq);\n+    vs_st2_indexed(vs1, __ T4S, coeffs, tmpAddr, 256, offsets4);\n+    vs_ld2_indexed(vs1, __ T4S, coeffs, tmpAddr, 384, offsets4);\n+    load32shorts(vs_front(vs2), zetas);\n+    kyber_sub_add_montmul32(vs_even(vs1), vs_odd(vs1),\n+                            vs_front(vs2), vs_back(vs2), vtmp, vq);\n+    vs_st2_indexed(vs1, __ T4S, coeffs, tmpAddr, 384, offsets4);\n+\n@@ -5287,50 +5400,22 @@\n-    __ add(tmpAddr, coeffs, 0);\n-    __ ld2(v0, v1, __ T2D, __ post(tmpAddr, 32));\n-    __ ld2(v2, v3, __ T2D, __ post(tmpAddr, 32));\n-    __ ld2(v4, v5, __ T2D, __ post(tmpAddr, 32));\n-    __ ld2(v6, v7, __ T2D, tmpAddr);\n-    __ ldpq(v16, v17, __ post(zetas, 32));\n-    __ ldpq(v18, v19, __ post(zetas, 32));\n-    kyber_subv_addv_montmul32();\n-    __ add(tmpAddr, coeffs, 0);\n-    __ st2(v0, v1, __ T2D, __ post(tmpAddr, 32));\n-    __ st2(v2, v3, __ T2D, __ post(tmpAddr, 32));\n-    __ st2(v4, v5, __ T2D, __ post(tmpAddr, 32));\n-    __ st2(v6, v7, __ T2D, __ post(tmpAddr, 32));\n-    __ ld2(v0, v1, __ T2D, __ post(tmpAddr, 32));\n-    __ ld2(v2, v3, __ T2D, __ post(tmpAddr, 32));\n-    __ ld2(v4, v5, __ T2D, __ post(tmpAddr, 32));\n-    __ ld2(v6, v7, __ T2D, tmpAddr);\n-    __ ldpq(v16, v17, __ post(zetas, 32));\n-    __ ldpq(v18, v19, __ post(zetas, 32));\n-    kyber_subv_addv_montmul32();\n-    __ add(tmpAddr, coeffs, 128);\n-    __ st2(v0, v1, __ T2D, __ post(tmpAddr, 32));\n-    __ st2(v2, v3, __ T2D, __ post(tmpAddr, 32));\n-    __ st2(v4, v5, __ T2D, __ post(tmpAddr, 32));\n-    __ st2(v6, v7, __ T2D, __ post(tmpAddr, 32));\n-\n-    __ ld2(v0, v1, __ T2D, __ post(tmpAddr, 32));\n-    __ ld2(v2, v3, __ T2D, __ post(tmpAddr, 32));\n-    __ ld2(v4, v5, __ T2D, __ post(tmpAddr, 32));\n-    __ ld2(v6, v7, __ T2D, tmpAddr);\n-    __ ldpq(v16, v17, __ post(zetas, 32));\n-    __ ldpq(v18, v19, __ post(zetas, 32));\n-    kyber_subv_addv_montmul32();\n-    __ add(tmpAddr, coeffs, 256);\n-    __ st2(v0, v1, __ T2D, __ post(tmpAddr, 32));\n-    __ st2(v2, v3, __ T2D, __ post(tmpAddr, 32));\n-    __ st2(v4, v5, __ T2D, __ post(tmpAddr, 32));\n-    __ st2(v6, v7, __ T2D, __ post(tmpAddr, 32));\n-    __ ld2(v0, v1, __ T2D, __ post(tmpAddr, 32));\n-    __ ld2(v2, v3, __ T2D, __ post(tmpAddr, 32));\n-    __ ld2(v4, v5, __ T2D, __ post(tmpAddr, 32));\n-    __ ld2(v6, v7, __ T2D, tmpAddr);\n-    __ ldpq(v16, v17, __ post(zetas, 32));\n-    __ ldpq(v18, v19, __ post(zetas, 32));\n-    kyber_subv_addv_montmul32();\n-    __ add(tmpAddr, coeffs, 384);\n-    __ st2(v0, v1, __ T2D, __ post(tmpAddr, 32));\n-    __ st2(v2, v3, __ T2D, __ post(tmpAddr, 32));\n-    __ st2(v4, v5, __ T2D, __ post(tmpAddr, 32));\n-    __ st2(v6, v7, __ T2D, tmpAddr);\n+    vs_ld2_indexed(vs1, __ T2D, coeffs, tmpAddr, 0, offsets4);\n+    load32shorts(vs_front(vs2), zetas);\n+    kyber_sub_add_montmul32(vs_even(vs1), vs_odd(vs1),\n+                            vs_front(vs2), vs_back(vs2), vtmp, vq);\n+    vs_st2_indexed(vs1, __ T2D, coeffs, tmpAddr, 0, offsets4);\n+    vs_ld2_indexed(vs1, __ T2D, coeffs, tmpAddr, 128, offsets4);\n+    load32shorts(vs_front(vs2), zetas);\n+    kyber_sub_add_montmul32(vs_even(vs1), vs_odd(vs1),\n+                            vs_front(vs2), vs_back(vs2), vtmp, vq);\n+    vs_st2_indexed(vs1, __ T2D, coeffs, tmpAddr, 128, offsets4);\n+\n+    vs_ld2_indexed(vs1, __ T2D, coeffs, tmpAddr, 256, offsets4);\n+    load32shorts(vs_front(vs2), zetas);\n+    kyber_sub_add_montmul32(vs_even(vs1), vs_odd(vs1),\n+                            vs_front(vs2), vs_back(vs2), vtmp, vq);\n+    vs_st2_indexed(vs1, __ T2D, coeffs, tmpAddr, 256, offsets4);\n+    vs_ld2_indexed(vs1, __ T2D, coeffs, tmpAddr, 384, offsets4);\n+    load32shorts(vs_front(vs2), zetas);\n+    kyber_sub_add_montmul32(vs_even(vs1), vs_odd(vs1),\n+                            vs_front(vs2), vs_back(vs2), vtmp, vq);\n+    vs_st2_indexed(vs1, __ T2D, coeffs, tmpAddr, 384, offsets4);\n+\n@@ -5338,78 +5423,21 @@\n-    __ add(tmpAddr, coeffs, 0);\n-    __ ldr(v0, __ Q, __ post(tmpAddr, 16));\n-    __ ldr(v16, __ Q, __ post(tmpAddr, 16));\n-    __ ldr(v1, __ Q, __ post(tmpAddr, 16));\n-    __ ldr(v17, __ Q, __ post(tmpAddr, 16));\n-    __ ldr(v2, __ Q, __ post(tmpAddr, 16));\n-    __ ldr(v18, __ Q, __ post(tmpAddr, 16));\n-    __ ldr(v3, __ Q, __ post(tmpAddr, 16));\n-    __ ldr(v19, __ Q, __ post(tmpAddr, 16));\n-    __ ldr(v4, __ Q, __ post(tmpAddr, 16));\n-    __ ldr(v20, __ Q, __ post(tmpAddr, 16));\n-    __ ldr(v5, __ Q, __ post(tmpAddr, 16));\n-    __ ldr(v21, __ Q, __ post(tmpAddr, 16));\n-    __ ldr(v6, __ Q, __ post(tmpAddr, 16));\n-    __ ldr(v22, __ Q, __ post(tmpAddr, 16));\n-    __ ldr(v7, __ Q, __ post(tmpAddr, 16));\n-    __ ldr(v23, __ Q, tmpAddr);\n-    kyber_addv_subv64();\n-    __ add(tmpAddr, coeffs, 0);\n-    __ str(v24, __ Q, __ post(tmpAddr, 32));\n-    __ str(v25, __ Q, __ post(tmpAddr, 32));\n-    __ str(v26, __ Q, __ post(tmpAddr, 32));\n-    __ str(v27, __ Q, __ post(tmpAddr, 32));\n-    __ str(v28, __ Q, __ post(tmpAddr, 32));\n-    __ str(v29, __ Q, __ post(tmpAddr, 32));\n-    __ str(v30, __ Q, __ post(tmpAddr, 32));\n-    __ str(v31, __ Q, tmpAddr);\n-    kyber_load64zetas(zetas);\n-    __ ldpq(v30, v31, kyberConsts);\n-    kyber_montmul64(false);\n-    __ add(tmpAddr, coeffs, 16);\n-    __ str(v16, __ Q, __ post(tmpAddr, 32));\n-    __ str(v17, __ Q, __ post(tmpAddr, 32));\n-    __ str(v18, __ Q, __ post(tmpAddr, 32));\n-    __ str(v19, __ Q, __ post(tmpAddr, 32));\n-    __ str(v20, __ Q, __ post(tmpAddr, 32));\n-    __ str(v21, __ Q, __ post(tmpAddr, 32));\n-    __ str(v22, __ Q, __ post(tmpAddr, 32));\n-    __ str(v23, __ Q, __ post(tmpAddr, 16));\n-\n-    __ ldr(v0, __ Q, __ post(tmpAddr, 16));\n-    __ ldr(v16, __ Q, __ post(tmpAddr, 16));\n-    __ ldr(v1, __ Q, __ post(tmpAddr, 16));\n-    __ ldr(v17, __ Q, __ post(tmpAddr, 16));\n-    __ ldr(v2, __ Q, __ post(tmpAddr, 16));\n-    __ ldr(v18, __ Q, __ post(tmpAddr, 16));\n-    __ ldr(v3, __ Q, __ post(tmpAddr, 16));\n-    __ ldr(v19, __ Q, __ post(tmpAddr, 16));\n-    __ ldr(v4, __ Q, __ post(tmpAddr, 16));\n-    __ ldr(v20, __ Q, __ post(tmpAddr, 16));\n-    __ ldr(v5, __ Q, __ post(tmpAddr, 16));\n-    __ ldr(v21, __ Q, __ post(tmpAddr, 16));\n-    __ ldr(v6, __ Q, __ post(tmpAddr, 16));\n-    __ ldr(v22, __ Q, __ post(tmpAddr, 16));\n-    __ ldr(v7, __ Q, __ post(tmpAddr, 16));\n-    __ ldr(v23, __ Q, tmpAddr);\n-    kyber_addv_subv64();\n-    __ add(tmpAddr, coeffs, 256);\n-    __ str(v24, __ Q, __ post(tmpAddr, 32));\n-    __ str(v25, __ Q, __ post(tmpAddr, 32));\n-    __ str(v26, __ Q, __ post(tmpAddr, 32));\n-    __ str(v27, __ Q, __ post(tmpAddr, 32));\n-    __ str(v28, __ Q, __ post(tmpAddr, 32));\n-    __ str(v29, __ Q, __ post(tmpAddr, 32));\n-    __ str(v30, __ Q, __ post(tmpAddr, 32));\n-    __ str(v31, __ Q, tmpAddr);\n-    kyber_load64zetas(zetas);\n-    __ ldpq(v30, v31, kyberConsts);\n-    kyber_montmul64(false);\n-    __ add(tmpAddr, coeffs, 272);\n-    __ str(v16, __ Q, __ post(tmpAddr, 32));\n-    __ str(v17, __ Q, __ post(tmpAddr, 32));\n-    __ str(v18, __ Q, __ post(tmpAddr, 32));\n-    __ str(v19, __ Q, __ post(tmpAddr, 32));\n-    __ str(v20, __ Q, __ post(tmpAddr, 32));\n-    __ str(v21, __ Q, __ post(tmpAddr, 32));\n-    __ str(v22, __ Q, __ post(tmpAddr, 32));\n-    __ str(v23, __ Q, tmpAddr);\n+    int offsets3[8] = { 0, 32, 64, 96, 128, 160, 192, 224 };\n+    vs_ldr_indexed(vs1, __ Q, coeffs, 0, offsets3);\n+    vs_ldr_indexed(vs2, __ Q, coeffs, 16, offsets3);\n+    vs_addv(vs3, __ T8H, vs1, vs2); \/\/ n.b. trashes vq\n+    vs_subv(vs1, __ T8H, vs1, vs2);\n+    vs_str_indexed(vs3, __ Q, coeffs, 0, offsets3);\n+    load64shorts(vs2, zetas);\n+    vs_ldpq(vq, kyberConsts);\n+    kyber_montmul64(vs2, vs1, vs2, vtmp, vq);\n+    vs_str_indexed(vs2, __ Q, coeffs, 16, offsets3);\n+\n+    vs_ldr_indexed(vs1, __ Q, coeffs, 256, offsets3);\n+    vs_ldr_indexed(vs2, __ Q, coeffs, 256 + 16, offsets3);\n+    vs_addv(vs3, __ T8H, vs1, vs2); \/\/ n.b. trashes vq\n+    vs_subv(vs1, __ T8H, vs1, vs2);\n+    vs_str_indexed(vs3, __ Q, coeffs, 256, offsets3);\n+    load64shorts(vs2, zetas);\n+    vs_ldpq(vq, kyberConsts);\n+    kyber_montmul64(vs2, vs1, vs2, vtmp, vq);\n+    vs_str_indexed(vs2, __ Q, coeffs, 256 + 16, offsets3);\n+\n@@ -5417,0 +5445,2 @@\n+\n+    \/\/ load q and the multiplier for the Barrett reduction\n@@ -5418,48 +5448,16 @@\n-    __ ldpq(v30, v31, tmpAddr);\n-    __ add(tmpAddr, coeffs, 0);\n-    __ ldr(v0, __ Q, __ post(tmpAddr, 32));\n-    __ ldr(v1, __ Q, __ post(tmpAddr, 32));\n-    __ ldr(v2, __ Q, __ post(tmpAddr, 32));\n-    __ ldr(v3, __ Q, __ post(tmpAddr, 32));\n-    __ ldr(v4, __ Q, __ post(tmpAddr, 32));\n-    __ ldr(v5, __ Q, __ post(tmpAddr, 32));\n-    __ ldr(v6, __ Q, __ post(tmpAddr, 32));\n-    __ ldr(v7, __ Q, tmpAddr);\n-    kyber_sqdmulh32(0);\n-    kyber_sqdmulh32(4);\n-    kyber_sshr32(18);\n-    kyber_sshr32(22);\n-    kyber_mlsv32(0);\n-    kyber_mlsv32(4);\n-    __ add(tmpAddr, coeffs, 0);\n-    __ str(v0, __ Q, __ post(tmpAddr, 32));\n-    __ str(v1, __ Q, __ post(tmpAddr, 32));\n-    __ str(v2, __ Q, __ post(tmpAddr, 32));\n-    __ str(v3, __ Q, __ post(tmpAddr, 32));\n-    __ str(v4, __ Q, __ post(tmpAddr, 32));\n-    __ str(v5, __ Q, __ post(tmpAddr, 32));\n-    __ str(v6, __ Q, __ post(tmpAddr, 32));\n-    __ str(v7, __ Q, __ post(tmpAddr, 32));\n-    __ ldr(v0, __ Q, __ post(tmpAddr, 32));\n-    __ ldr(v1, __ Q, __ post(tmpAddr, 32));\n-    __ ldr(v2, __ Q, __ post(tmpAddr, 32));\n-    __ ldr(v3, __ Q, __ post(tmpAddr, 32));\n-    __ ldr(v4, __ Q, __ post(tmpAddr, 32));\n-    __ ldr(v5, __ Q, __ post(tmpAddr, 32));\n-    __ ldr(v6, __ Q, __ post(tmpAddr, 32));\n-    __ ldr(v7, __ Q, tmpAddr);\n-    kyber_sqdmulh32(0);\n-    kyber_sqdmulh32(4);\n-    kyber_sshr32(18);\n-    kyber_sshr32(22);\n-    kyber_mlsv32(0);\n-    kyber_mlsv32(4);\n-    __ add(tmpAddr, coeffs, 256);\n-    __ str(v0, __ Q, __ post(tmpAddr, 32));\n-    __ str(v1, __ Q, __ post(tmpAddr, 32));\n-    __ str(v2, __ Q, __ post(tmpAddr, 32));\n-    __ str(v3, __ Q, __ post(tmpAddr, 32));\n-    __ str(v4, __ Q, __ post(tmpAddr, 32));\n-    __ str(v5, __ Q, __ post(tmpAddr, 32));\n-    __ str(v6, __ Q, __ post(tmpAddr, 32));\n-    __ str(v7, __ Q, tmpAddr);\n+    vs_ldpq(vq, tmpAddr);\n+\n+    VSeq<8> vq1 = VSeq<8>(vq[0], 0); \/\/ 2 constant 8 sequences\n+    VSeq<8> vq2 = VSeq<8>(vq[1], 0); \/\/ for above two kyber constants\n+    VSeq<8> vq3 = VSeq<8>(v29, 0);   \/\/ 3rd sequence for const montmul\n+    vs_ldr_indexed(vs1, __ Q, coeffs, 0, offsets3);\n+    vs_sqdmulh(vs2, __ T8H, vs1, vq2);\n+    vs_sshr(vs2, __ T8H, vs2, 11);\n+    vs_mlsv(vs1, __ T8H, vs2, vq1);\n+    vs_str_indexed(vs1, __ Q, coeffs, 0, offsets3);\n+    vs_ldr_indexed(vs1, __ Q, coeffs, 256, offsets3);\n+    vs_sqdmulh(vs2, __ T8H, vs1, vq2);\n+    vs_sshr(vs2, __ T8H, vs2, 11);\n+    vs_mlsv(vs1, __ T8H, vs2, vq1);\n+    vs_str_indexed(vs1, __ Q, coeffs, 256, offsets3);\n+\n@@ -5467,46 +5465,21 @@\n-    __ add(tmpAddr, coeffs, 0);\n-    __ ldpq(v0, v1, __ post(tmpAddr, 32));\n-    __ ldpq(v16, v17, __ post(tmpAddr, 32));\n-    __ ldpq(v2, v3, __ post(tmpAddr, 32));\n-    __ ldpq(v18, v19, __ post(tmpAddr, 32));\n-    __ ldpq(v4, v5, __ post(tmpAddr, 32));\n-    __ ldpq(v20, v21, __ post(tmpAddr, 32));\n-    __ ldpq(v6, v7, __ post(tmpAddr, 32));\n-    __ ldpq(v22, v23, tmpAddr);\n-    kyber_addv_subv64();\n-    __ add(tmpAddr, coeffs, 0);\n-    __ stpq(v24, v25, __ post(tmpAddr, 64));\n-    __ stpq(v26, v27, __ post(tmpAddr, 64));\n-    __ stpq(v28, v29, __ post(tmpAddr, 64));\n-    __ stpq(v30, v31, tmpAddr);\n-    kyber_load64zetas(zetas);\n-    __ ldpq(v30, v31, kyberConsts);\n-    kyber_montmul64(false);\n-    __ add(tmpAddr, coeffs, 32);\n-    __ stpq(v16, v17, __ post(tmpAddr, 64));\n-    __ stpq(v18, v19, __ post(tmpAddr, 64));\n-    __ stpq(v20, v21, __ post(tmpAddr, 64));\n-    __ stpq(v22, v23, __ post(tmpAddr, 32));\n-\n-    __ ldpq(v0, v1, __ post(tmpAddr, 32));\n-    __ ldpq(v16, v17, __ post(tmpAddr, 32));\n-    __ ldpq(v2, v3, __ post(tmpAddr, 32));\n-    __ ldpq(v18, v19, __ post(tmpAddr, 32));\n-    __ ldpq(v4, v5, __ post(tmpAddr, 32));\n-    __ ldpq(v20, v21, __ post(tmpAddr, 32));\n-    __ ldpq(v6, v7, __ post(tmpAddr, 32));\n-    __ ldpq(v22, v23, tmpAddr);\n-    kyber_addv_subv64();\n-    __ add(tmpAddr, coeffs, 256);\n-    __ stpq(v24, v25, __ post(tmpAddr, 64));\n-    __ stpq(v26, v27, __ post(tmpAddr, 64));\n-    __ stpq(v28, v29, __ post(tmpAddr, 64));\n-    __ stpq(v30, v31, tmpAddr);\n-    kyber_load64zetas(zetas);\n-    __ ldpq(v30, v31, kyberConsts);\n-    kyber_montmul64(false);\n-    __ add(tmpAddr, coeffs, 288);\n-    __ stpq(v16, v17, __ post(tmpAddr, 64));\n-    __ stpq(v18, v19, __ post(tmpAddr, 64));\n-    __ stpq(v20, v21, __ post(tmpAddr, 64));\n-    __ stpq(v22, v23, tmpAddr);\n+    int offsets2[4] = { 0, 64, 128, 192 };\n+    vs_ldpq_indexed(vs1, coeffs, 0, offsets2);\n+    vs_ldpq_indexed(vs2, coeffs, 32, offsets2);\n+    vs_addv(vs3, __ T8H, vs1, vs2); \/\/ n.b. trashes vq\n+    vs_subv(vs1, __ T8H, vs1, vs2);\n+    vs_stpq_indexed(vs3, coeffs, 0, offsets2);\n+    load64shorts(vs2, zetas);\n+    vs_ldpq(vq, kyberConsts);\n+    kyber_montmul64(vs2, vs1, vs2, vtmp, vq);\n+    vs_stpq_indexed(vs2, coeffs, 32, offsets2);\n+\n+    vs_ldpq_indexed(vs1, coeffs, 256, offsets2);\n+    vs_ldpq_indexed(vs2, coeffs, 256 + 32, offsets2);\n+    vs_addv(vs3, __ T8H, vs1, vs2); \/\/ n.b. trashes vq\n+    vs_subv(vs1, __ T8H, vs1, vs2);\n+    vs_stpq_indexed(vs3, coeffs, 256, offsets2);\n+    load64shorts(vs2, zetas);\n+    vs_ldpq(vq, kyberConsts);\n+    kyber_montmul64(vs2, vs1, vs2, vtmp, vq);\n+    vs_stpq_indexed(vs2, coeffs, 256 + 32, offsets2);\n+\n@@ -5514,46 +5487,21 @@\n-    __ add(tmpAddr, coeffs, 0);\n-    __ ldpq(v0, v1, __ post(tmpAddr, 32));\n-    __ ldpq(v2, v3, __ post(tmpAddr, 32));\n-    __ ldpq(v16, v17, __ post(tmpAddr, 32));\n-    __ ldpq(v18, v19, __ post(tmpAddr, 32));\n-    __ ldpq(v4, v5, __ post(tmpAddr, 32));\n-    __ ldpq(v6, v7, __ post(tmpAddr, 32));\n-    __ ldpq(v20, v21, __ post(tmpAddr, 32));\n-    __ ldpq(v22, v23, tmpAddr);\n-    kyber_addv_subv64();\n-    __ add(tmpAddr, coeffs, 0);\n-    __ stpq(v24, v25, __ post(tmpAddr, 32));\n-    __ stpq(v26, v27, __ post(tmpAddr, 96));\n-    __ stpq(v28, v29, __ post(tmpAddr, 32));\n-    __ stpq(v30, v31, tmpAddr);\n-    kyber_load64zetas(zetas);\n-    __ ldpq(v30, v31, kyberConsts);\n-    kyber_montmul64(false);\n-    __ add(tmpAddr, coeffs, 64);\n-    __ stpq(v16, v17, __ post(tmpAddr, 32));\n-    __ stpq(v18, v19, __ post(tmpAddr, 96));\n-    __ stpq(v20, v21, __ post(tmpAddr, 32));\n-    __ stpq(v22, v23, __ post(tmpAddr, 32));\n-\n-    __ ldpq(v0, v1, __ post(tmpAddr, 32));\n-    __ ldpq(v2, v3, __ post(tmpAddr, 32));\n-    __ ldpq(v16, v17, __ post(tmpAddr, 32));\n-    __ ldpq(v18, v19, __ post(tmpAddr, 32));\n-    __ ldpq(v4, v5, __ post(tmpAddr, 32));\n-    __ ldpq(v6, v7, __ post(tmpAddr, 32));\n-    __ ldpq(v20, v21, __ post(tmpAddr, 32));\n-    __ ldpq(v22, v23, tmpAddr);\n-    kyber_addv_subv64();\n-    __ add(tmpAddr, coeffs, 256);\n-    __ stpq(v24, v25, __ post(tmpAddr, 32));\n-    __ stpq(v26, v27, __ post(tmpAddr, 96));\n-    __ stpq(v28, v29, __ post(tmpAddr, 32));\n-    __ stpq(v30, v31, tmpAddr);\n-    kyber_load64zetas(zetas);\n-    __ ldpq(v30, v31, kyberConsts);\n-    kyber_montmul64(false);\n-    __ add(tmpAddr, coeffs, 320);\n-    __ stpq(v16, v17, __ post(tmpAddr, 32));\n-    __ stpq(v18, v19, __ post(tmpAddr, 96));\n-    __ stpq(v20, v21, __ post(tmpAddr, 32));\n-    __ stpq(v22, v23, tmpAddr);\n+    int offsets1[4] = { 0, 32, 128, 160 };\n+    vs_ldpq_indexed(vs1, coeffs, 0, offsets1);\n+    vs_ldpq_indexed(vs2, coeffs, 64, offsets1);\n+    vs_addv(vs3, __ T8H, vs1, vs2); \/\/ n.b. trashes vq\n+    vs_subv(vs1, __ T8H, vs1, vs2);\n+    vs_stpq_indexed(vs3, coeffs, 0, offsets1);\n+    load64shorts(vs2, zetas);\n+    vs_ldpq(vq, kyberConsts);\n+    kyber_montmul64(vs2, vs1, vs2, vtmp, vq);\n+    vs_stpq_indexed(vs2, coeffs, 64, offsets1);\n+\n+    vs_ldpq_indexed(vs1, coeffs, 256, offsets1);\n+    vs_ldpq_indexed(vs2, coeffs, 256 + 64, offsets1);\n+    vs_addv(vs3, __ T8H, vs1, vs2); \/\/ n.b. trashes vq\n+    vs_subv(vs1, __ T8H, vs1, vs2);\n+    vs_stpq_indexed(vs3, coeffs, 256, offsets1);\n+    load64shorts(vs2, zetas);\n+    vs_ldpq(vq, kyberConsts);\n+    kyber_montmul64(vs2, vs1, vs2, vtmp, vq);\n+    vs_stpq_indexed(vs2, coeffs, 256 + 64, offsets1);\n+\n@@ -5562,1 +5510,1 @@\n-    kyber_load64coeffs(0, tmpAddr);\n+    load64shorts(vs1, tmpAddr);\n@@ -5564,2 +5512,3 @@\n-    kyber_load64coeffs(16, tmpAddr);\n-    kyber_addv_subv64();\n+    load64shorts(vs2, tmpAddr);\n+    vs_addv(vs3, __ T8H, vs1, vs2); \/\/ n.b. trashes vq\n+    vs_subv(vs1, __ T8H, vs1, vs2);\n@@ -5567,4 +5516,4 @@\n-    kyber_store64coeffs(24, tmpAddr);\n-    kyber_load64zetas(zetas);\n-    __ ldpq(v30, v31, kyberConsts);\n-    kyber_montmul64(false);\n+    store64shorts(vs3, tmpAddr);\n+    load64shorts(vs2, zetas);\n+    vs_ldpq(vq, kyberConsts);\n+    kyber_montmul64(vs2, vs1, vs2, vtmp, vq);\n@@ -5572,1 +5521,1 @@\n-    kyber_store64coeffs(16, tmpAddr);\n+    store64shorts(vs2, tmpAddr);\n@@ -5574,1 +5523,1 @@\n-    kyber_load64coeffs(0, tmpAddr);\n+    load64shorts(vs1, tmpAddr);\n@@ -5576,2 +5525,3 @@\n-    kyber_load64coeffs(16, tmpAddr);\n-    kyber_addv_subv64();\n+    load64shorts(vs2, tmpAddr);\n+    vs_addv(vs3, __ T8H, vs1, vs2); \/\/ n.b. trashes vq\n+    vs_subv(vs1, __ T8H, vs1, vs2);\n@@ -5579,4 +5529,4 @@\n-    kyber_store64coeffs(24, tmpAddr);\n-    kyber_load64zetas(zetas);\n-    __ ldpq(v30, v31, kyberConsts);\n-    kyber_montmul64(false);\n+    store64shorts(vs3, tmpAddr);\n+    load64shorts(vs2, zetas);\n+    vs_ldpq(vq, kyberConsts);\n+    kyber_montmul64(vs2, vs1, vs2, vtmp, vq);\n@@ -5584,1 +5534,2 @@\n-    kyber_store64coeffs(16, tmpAddr);\n+    store64shorts(vs2, tmpAddr);\n+\n@@ -5586,0 +5537,2 @@\n+\n+    \/\/ load q and the multiplier for the Barrett reduction\n@@ -5587,10 +5540,9 @@\n-    __ ldpq(v30, v31, tmpAddr);\n-    __ add(tmpAddr, coeffs, 0);\n-    __ ldpq(v0, v1, __ post(tmpAddr, 256));\n-    __ ldpq(v2, v3, tmpAddr);\n-    kyber_sqdmulh32(0);\n-    kyber_sshr32(18);\n-    kyber_mlsv32(0);\n-    __ add(tmpAddr, coeffs, 0);\n-    __ stpq(v0, v1, __ post(tmpAddr, 256));\n-    __ stpq(v2, v3, tmpAddr);\n+    vs_ldpq(vq, tmpAddr);\n+\n+    int offsets0[2] = { 0, 256 };\n+    vs_ldpq_indexed(vs_front(vs1), coeffs, 0, offsets0);\n+    vs_sqdmulh(vs2, __ T8H, vs1, vq2);\n+    vs_sshr(vs2, __ T8H, vs2, 11);\n+    vs_mlsv(vs1, __ T8H, vs2, vq1);\n+    vs_stpq_indexed(vs_front(vs1), coeffs, 0, offsets0);\n+\n@@ -5599,1 +5551,1 @@\n-    kyber_load64coeffs(0, tmpAddr);\n+    load64shorts(vs1, tmpAddr);\n@@ -5601,2 +5553,3 @@\n-    kyber_load64coeffs(16, tmpAddr);\n-    kyber_addv_subv64();\n+    load64shorts(vs2, tmpAddr);\n+    vs_addv(vs3, __ T8H, vs1, vs2); \/\/ n.b. trashes vq\n+    vs_subv(vs1, __ T8H, vs1, vs2);\n@@ -5604,4 +5557,4 @@\n-    kyber_store64coeffs(24, tmpAddr);\n-    kyber_load64zetas(zetas);\n-    __ ldpq(v30, v31, kyberConsts);\n-    kyber_montmul64(false);\n+    store64shorts(vs3, tmpAddr);\n+    load64shorts(vs2, zetas);\n+    vs_ldpq(vq, kyberConsts);\n+    kyber_montmul64(vs2, vs1, vs2, vtmp, vq);\n@@ -5609,1 +5562,1 @@\n-    kyber_store64coeffs(16, tmpAddr);\n+    store64shorts(vs2, tmpAddr);\n@@ -5612,1 +5565,1 @@\n-    kyber_load64coeffs(0, tmpAddr);\n+    load64shorts(vs1, tmpAddr);\n@@ -5614,2 +5567,3 @@\n-    kyber_load64coeffs(16, tmpAddr);\n-    kyber_addv_subv64();\n+    load64shorts(vs2, tmpAddr);\n+    vs_addv(vs3, __ T8H, vs1, vs2); \/\/ n.b. trashes vq\n+    vs_subv(vs1, __ T8H, vs1, vs2);\n@@ -5617,4 +5571,4 @@\n-    kyber_store64coeffs(24, tmpAddr);\n-    kyber_load64zetas(zetas);\n-    __ ldpq(v30, v31, kyberConsts);\n-    kyber_montmul64(false);\n+    store64shorts(vs3, tmpAddr);\n+    load64shorts(vs2, zetas);\n+    vs_ldpq(vq, kyberConsts);\n+    kyber_montmul64(vs2, vs1, vs2, vtmp, vq);\n@@ -5622,1 +5576,2 @@\n-    kyber_store64coeffs(16, tmpAddr);\n+    store64shorts(vs2, tmpAddr);\n+\n@@ -5624,0 +5579,2 @@\n+\n+    \/\/ load toMont(2^-n mod q)\n@@ -5626,1 +5583,2 @@\n-    __ ldpq(v30, v31, kyberConsts);\n+\n+    vs_ldpq(vq, kyberConsts);\n@@ -5628,2 +5586,2 @@\n-    kyber_load64coeffs(0, tmpAddr);\n-    kyber_montmul64(true);\n+    load64shorts(vs1, tmpAddr);\n+    kyber_montmul64(vs2, vs1, vq3, vtmp, vq);\n@@ -5631,1 +5589,1 @@\n-    kyber_store64coeffs(16, tmpAddr);\n+    store64shorts(vs2, tmpAddr);\n@@ -5633,2 +5591,2 @@\n-    kyber_load64coeffs(0, tmpAddr);\n-    kyber_montmul64(true);\n+    load64shorts(vs1, tmpAddr);\n+    kyber_montmul64(vs2, vs1, vq3, vtmp, vq);\n@@ -5636,1 +5594,1 @@\n-    kyber_store64coeffs(16, tmpAddr);\n+    store64shorts(vs2, tmpAddr);\n@@ -5638,2 +5596,2 @@\n-    kyber_load64coeffs(0, tmpAddr);\n-    kyber_montmul64(true);\n+    load64shorts(vs1, tmpAddr);\n+    kyber_montmul64(vs2, vs1, vq3, vtmp, vq);\n@@ -5641,1 +5599,1 @@\n-    kyber_store64coeffs(16, tmpAddr);\n+    store64shorts(vs2, tmpAddr);\n@@ -5643,4 +5601,4 @@\n-    kyber_load64coeffs(0, tmpAddr);\n-    kyber_montmul64(true);\n-   __ add(tmpAddr, coeffs, 384);\n-    kyber_store64coeffs(16, tmpAddr);\n+    load64shorts(vs1, tmpAddr);\n+    kyber_montmul64(vs2, vs1, vq3, vtmp, vq);\n+    __ add(tmpAddr, coeffs, 384);\n+    store64shorts(vs2, tmpAddr);\n@@ -5658,1 +5616,1 @@\n-  \/\/          short[] result, short[] ntta, short[] nttb, short[] zetas) {}\n+  \/\/              short[] result, short[] ntta, short[] nttb, short[] zetas) {}\n@@ -5680,1 +5638,8 @@\n-    __ lea(kyberConsts, ExternalAddress((address) StubRoutines::aarch64::_kyberConsts));\n+    VSeq<4> vs1(0), vs2(4);  \/\/ 4 sets of 8x8H inputs\/outputs\/tmps\n+    VSeq<4> vs3(16), vs4(20);\n+    VSeq<2> vq(30);          \/\/ pair of constants for montmul\n+    VSeq<2> vz(28);          \/\/ pair of zetas\n+    VSeq<4> vc(27, 0);       \/\/ constant sequence for montmul\n+\n+    __ lea(kyberConsts,\n+             ExternalAddress((address) StubRoutines::aarch64::_kyberConsts));\n@@ -5686,1 +5651,5 @@\n-    __ ldpq(v30, v31, __ post(kyberConsts, 64));\n+    \/\/ load q and qinv\n+    vs_ldpq(vq, kyberConsts);\n+\n+    \/\/ load R^2 mod q (to convert back from Montgomery representation)\n+    __ add(kyberConsts, kyberConsts, 64);\n@@ -5690,82 +5659,33 @@\n-    __ ldpq(v28, v29, __ post(zetas, 32));\n-    __ ld2(v0, v1, __ T8H, __ post(ntta, 32));\n-    __ ld2(v2, v3, __ T8H, __ post(nttb, 32));\n-    __ ld2(v20, v21, __ T8H, __ post(ntta, 32));\n-    __ ld2(v22, v23, __ T8H, __ post(nttb, 32));\n-    \/\/ montmul\n-    __ sqdmulh(v5, __ T8H, v1, v3);\n-    __ mulv(v17, __ T8H, v1, v3);\n-    __ sqdmulh(v4, __ T8H, v0, v2);\n-    __ mulv(v16, __ T8H, v0, v2);\n-    __ sqdmulh(v6, __ T8H, v0, v3);\n-    __ mulv(v18, __ T8H, v0, v3);\n-    __ sqdmulh(v7, __ T8H, v1, v2);\n-    __ mulv(v19, __ T8H, v1, v2);\n-    __ mulv(v17, __ T8H, v17, v30);\n-    __ mulv(v16, __ T8H, v16, v30);\n-    __ mulv(v18, __ T8H, v18, v30);\n-    __ mulv(v19, __ T8H, v19, v30);\n-    __ sqdmulh(v17, __ T8H, v17, v31);\n-    __ sqdmulh(v16, __ T8H, v16, v31);\n-    __ sqdmulh(v18, __ T8H, v18, v31);\n-    __ sqdmulh(v19, __ T8H, v19, v31);\n-    __ shsubv(v17, __ T8H, v5, v17);\n-    __ shsubv(v16, __ T8H, v4, v16);\n-    __ shsubv(v18, __ T8H, v6, v18);\n-    __ shsubv(v19, __ T8H, v7, v19);\n-    __ sqdmulh(v5, __ T8H, v21, v23);\n-    __ mulv(v1, __ T8H, v21, v23);\n-    __ sqdmulh(v4, __ T8H, v20, v22);\n-    __ mulv(v0, __ T8H, v20, v22);\n-    __ sqdmulh(v6, __ T8H, v20, v23);\n-    __ mulv(v2, __ T8H, v20, v23);\n-    __ sqdmulh(v7, __ T8H, v21, v22);\n-    __ mulv(v3, __ T8H, v21, v22);\n-    __ mulv(v1, __ T8H, v1, v30);\n-    __ mulv(v0, __ T8H, v0, v30);\n-    __ mulv(v2, __ T8H, v2, v30);\n-    __ mulv(v3, __ T8H, v3, v30);\n-    __ sqdmulh(v1, __ T8H, v1, v31);\n-    __ sqdmulh(v0, __ T8H, v0, v31);\n-    __ sqdmulh(v2, __ T8H, v2, v31);\n-    __ sqdmulh(v3, __ T8H, v3, v31);\n-    __ shsubv(v1, __ T8H, v5, v1);\n-    __ shsubv(v0, __ T8H, v4, v0);\n-    __ shsubv(v2, __ T8H, v6, v2);\n-    __ shsubv(v3, __ T8H, v7, v3);\n-    __ sqdmulh(v5, __ T8H, v17, v28);\n-    __ mulv(v17, __ T8H, v17, v28);\n-    __ sqdmulh(v4, __ T8H, v1, v29);\n-    __ mulv(v1, __ T8H, v1, v29);\n-    __ mulv(v17, __ T8H, v17, v30);\n-    __ mulv(v1, __ T8H, v1, v30);\n-    __ sqdmulh(v17, __ T8H, v17, v31);\n-    __ sqdmulh(v1, __ T8H, v1, v31);\n-    __ shsubv(v17, __ T8H, v5, v17);\n-    __ shsubv(v1, __ T8H, v4, v1);\n-    __ addv(v16, __ T8H, v16, v17);\n-    __ addv(v17, __ T8H, v18, v19);\n-    __ addv(v18, __ T8H, v0, v1);\n-    __ addv(v19, __ T8H, v2, v3);\n-    __ sqdmulh(v5, __ T8H, v16, v27);\n-    __ mulv(v0, __ T8H, v16, v27);\n-    __ sqdmulh(v4, __ T8H, v17, v27);\n-    __ mulv(v1, __ T8H, v17, v27);\n-    __ sqdmulh(v6, __ T8H, v18, v27);\n-    __ mulv(v2, __ T8H, v18, v27);\n-    __ sqdmulh(v7, __ T8H, v19, v27);\n-    __ mulv(v3, __ T8H, v19, v27);\n-    __ mulv(v0, __ T8H, v0, v30);\n-    __ mulv(v1, __ T8H, v1, v30);\n-    __ mulv(v2, __ T8H, v2, v30);\n-    __ mulv(v3, __ T8H, v3, v30);\n-    __ sqdmulh(v0, __ T8H, v0, v31);\n-    __ sqdmulh(v1, __ T8H, v1, v31);\n-    __ sqdmulh(v2, __ T8H, v2, v31);\n-    __ sqdmulh(v3, __ T8H, v3, v31);\n-    __ shsubv(v0, __ T8H, v5, v0);\n-    __ shsubv(v1, __ T8H, v4, v1);\n-    __ shsubv(v2, __ T8H, v6, v2);\n-    __ shsubv(v3, __ T8H, v7, v3);\n-    __ st2(v0, v1, __ T8H, __ post(result, 32));\n-    __ st2(v2, v3, __ T8H, __ post(result, 32));\n+    \/\/ load 16 zetas\n+    vs_ldpq_post(vz, zetas);\n+    \/\/ load 2 sets of 32 coefficients from the two input arrays\n+    vs_ld2_post(vs_front(vs1), __ T8H, ntta);\n+    vs_ld2_post(vs_back(vs1), __ T8H, nttb);\n+    vs_ld2_post(vs_front(vs4), __ T8H, ntta);\n+    vs_ld2_post(vs_back(vs4), __ T8H, nttb);\n+    \/\/ montmul the first and second pair of values loaded into vs1\n+    \/\/ in order and then with one pair reversed storing the  two\n+    \/\/ results in vs3\n+    kyber_montmul16(vs_front(vs3), vs_front(vs1), vs_back(vs1), vs_front(vs2), vq);\n+    kyber_montmul16(vs_back(vs3),\n+                    vs_front(vs1), vs_reverse(vs_back(vs1)), vs_back(vs2), vq);\n+    \/\/ montmul the first and second pair of values loaded into vs4\n+    \/\/ in order and then with one pair reversed storing the two\n+    \/\/ results in vs1\n+    kyber_montmul16(vs_front(vs1), vs_front(vs4), vs_back(vs4), vs_front(vs2), vq);\n+    kyber_montmul16(vs_back(vs1),\n+                    vs_front(vs4), vs_reverse(vs_back(vs4)), vs_back(vs2), vq);\n+    \/\/ for each pair of results pick the second value in the first\n+    \/\/ pair to create a sequence that we montmul by the zetas\n+    \/\/ i.e. we want sequence <vs3[1], vs1[1]>\n+    int delta = vs1[1]->encoding() - vs3[1]->encoding();\n+    VSeq<2> vs5(vs3[1], delta);\n+    kyber_montmul16(vs5, vz, vs5, vs_front(vs2), vq);\n+    \/\/ add results in pairs storing in vs3\n+    vs_addv(vs_front(vs3), __ T8H, vs_even(vs3), vs_odd(vs3));\n+    vs_addv(vs_back(vs3), __ T8H, vs_even(vs1), vs_odd(vs1));\n+    \/\/ montmul result by constant vc and store result in vs1\n+    kyber_montmul32(vs1, vs3, vc, vs2, vq);\n+    \/\/ store the four results as two interleaved pairs of\n+    \/\/ quadwords\n+    vs_st2_post(vs1, __ T8H, result);\n@@ -5774,37 +5694,1 @@\n-    __ br(Assembler::NE, kyberNttMult_loop);\n-\n-    __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n-    __ mov(r0, zr); \/\/ return 0\n-    __ ret(lr);\n-\n-    return start;\n-  }\n-\n-  void kyber_load80v0_v17(const Register a) {\n-    __ ldpq(v0, v1, __ post(a, 32));\n-    __ ldpq(v2, v3, __ post(a, 32));\n-    __ ldpq(v4, v5, __ post(a, 32));\n-    __ ldpq(v6, v7, __ post(a, 32));\n-    __ ldpq(v16, v17, __ post(a, 32));\n-  }\n-\n-  void kyber_load80v18_v27(const Register b) {\n-    __ ldpq(v18, v19, __ post(b, 32));\n-    __ ldpq(v20, v21, __ post(b, 32));\n-    __ ldpq(v22, v23, __ post(b, 32));\n-    __ ldpq(v24, v25, __ post(b, 32));\n-    __ ldpq(v26, v27, __ post(b, 32));\n-  }\n-\n-  void kyber_addv80() {\n-    __ addv(v0, __ T8H, v18, v0);\n-    __ addv(v1, __ T8H, v19, v1);\n-    __ addv(v2, __ T8H, v20, v2);\n-    __ addv(v3, __ T8H, v21, v3);\n-    __ addv(v4, __ T8H, v22, v4);\n-    __ addv(v5, __ T8H, v23, v5);\n-    __ addv(v6, __ T8H, v24, v6);\n-    __ addv(v7, __ T8H, v25, v7);\n-    __ addv(v16, __ T8H, v26, v16);\n-    __ addv(v17, __ T8H, v27, v17);\n-  }\n+    __ br(Assembler::NE, kyberNttMult_loop);\n@@ -5812,12 +5696,3 @@\n-  void kyber_addv_v31_80() {\n-    __ addv(v0, __ T8H, v31, v0);\n-    __ addv(v1, __ T8H, v31, v1);\n-    __ addv(v2, __ T8H, v31, v2);\n-    __ addv(v3, __ T8H, v31, v3);\n-    __ addv(v4, __ T8H, v31, v4);\n-    __ addv(v5, __ T8H, v31, v5);\n-    __ addv(v6, __ T8H, v31, v6);\n-    __ addv(v7, __ T8H, v31, v7);\n-    __ addv(v16, __ T8H, v31, v16);\n-    __ addv(v17, __ T8H, v31, v17);\n-  }\n+    __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n+    __ mov(r0, zr); \/\/ return 0\n+    __ ret(lr);\n@@ -5825,6 +5700,1 @@\n-  void kyber_store80(const Register result) {\n-    __ stpq(v0, v1, __ post(result, 32));\n-    __ stpq(v2, v3, __ post(result, 32));\n-    __ stpq(v4, v5, __ post(result, 32));\n-    __ stpq(v6, v7, __ post(result, 32));\n-    __ stpq(v16, v17, __ post(result, 32));\n+    return start;\n@@ -5854,29 +5724,55 @@\n-    __ lea(kyberConsts, ExternalAddress((address) StubRoutines::aarch64::_kyberConsts));\n-\n-    __ add(kyberConsts, kyberConsts, 16);\n-    __ ldr(v31, __ Q, kyberConsts);\n-    kyber_load80v0_v17(a);\n-    __ ldr(v28, __ Q, __ post(a, 16));\n-    kyber_load80v18_v27(b);\n-    __ ldr(v29, __ Q, __ post(b, 16));\n-    kyber_addv80();\n-    __ addv(v28, __ T8H, v28, v29);\n-    kyber_addv_v31_80();\n-    __ addv(v28, __ T8H, v28, v31);\n-    kyber_store80(result);\n-    __ str(v28, __ Q, __ post(result, 16));\n-    kyber_load80v0_v17(a);\n-    __ ldr(v28, __ Q, __ post(a, 16));\n-    kyber_load80v18_v27(b);\n-    __ ldr(v29, __ Q, __ post(b, 16));\n-    kyber_addv80();\n-    __ addv(v28, __ T8H, v28, v29);\n-    kyber_addv_v31_80();\n-    __ addv(v28, __ T8H, v28, v31);\n-    kyber_store80(result);\n-    __ str(v28, __ Q, __ post(result, 16));\n-    kyber_load80v0_v17(a);\n-    kyber_load80v18_v27(b);\n-    kyber_addv80();\n-    kyber_addv_v31_80();\n-    kyber_store80(result);\n+    \/\/ We sum 256 sets of values in total i.e. 32 x 8H quadwords.\n+    \/\/ So, we can load, add and store the data in 3 groups of 11,\n+    \/\/ 11 and 10 at a time i.e. we need to map sets of 10 or 11\n+    \/\/ registers. A further constraint is that the mapping needs\n+    \/\/ to skip callee saves. So, we allocate the register\n+    \/\/ sequences using two 8 sequences, two 2 sequences and two\n+    \/\/ single registers.\n+    VSeq<8> vs1_1(0);\n+    VSeq<2> vs1_2(16);\n+    FloatRegister vs1_3 = v28;\n+    VSeq<8> vs2_1(18);\n+    VSeq<2> vs2_2(26);\n+    FloatRegister vs2_3 = v29;\n+\n+    \/\/ two constant vector sequences\n+    VSeq<8> vc_1(31, 0);\n+    VSeq<2> vc_2(31, 0);\n+\n+    FloatRegister vc_3 = v31;\n+    __ lea(kyberConsts,\n+             ExternalAddress((address) StubRoutines::aarch64::_kyberConsts));\n+\n+    __ ldr(vc_3, __ Q, Address(kyberConsts, 16)); \/\/ q\n+    for (int i = 0; i < 3; i++) {\n+      \/\/ load 80 or 88 values from a into vs1_1\/2\/3\n+      vs_ldpq_post(vs1_1, a);\n+      vs_ldpq_post(vs1_2, a);\n+      if (i < 2) {\n+        __ ldr(vs1_3, __ Q, __ post(a, 16));\n+      }\n+      \/\/ load 80 or 88 values from b into vs2_1\/2\/3\n+      vs_ldpq_post(vs2_1, b);\n+      vs_ldpq_post(vs2_2, b);\n+      if (i < 2) {\n+        __ ldr(vs2_3, __ Q, __ post(b, 16));\n+      }\n+      \/\/ sum 80 or 88 values across vs1 and vs2 into vs1\n+      vs_addv(vs1_1, __ T8H, vs1_1, vs2_1);\n+      vs_addv(vs1_2, __ T8H, vs1_2, vs2_2);\n+      if (i < 2) {\n+        __ addv(vs1_3, __ T8H, vs1_3, vs2_3);\n+      }\n+      \/\/ add constant to all 80 or 88 results\n+      vs_addv(vs1_1, __ T8H, vs1_1, vc_1);\n+      vs_addv(vs1_2, __ T8H, vs1_2, vc_2);\n+      if (i < 2) {\n+        __ addv(vs1_3, __ T8H, vs1_3, vc_3);\n+      }\n+      \/\/ store 80 or 88 values\n+      vs_stpq_post(vs1_1, result);\n+      vs_stpq_post(vs1_2, result);\n+      if (i < 2) {\n+        __ str(vs1_3, __ Q, __ post(result, 16));\n+      }\n+    }\n@@ -5914,40 +5810,68 @@\n-    __ lea(kyberConsts, ExternalAddress((address) StubRoutines::aarch64::_kyberConsts));\n-\n-    __ add(kyberConsts, kyberConsts, 16);\n-    __ ldr(v31, __ Q, kyberConsts);\n-    __ addv(v31, __ T8H, v31, v31);\n-    kyber_load80v0_v17(a);\n-    __ ldr(v28, __ Q, __ post(a, 16));\n-    kyber_load80v18_v27(b);\n-    __ ldr(v29, __ Q, __ post(b, 16));\n-    kyber_addv80();\n-    __ addv(v28, __ T8H, v28, v29);\n-    kyber_load80v18_v27(c);\n-    __ ldr(v29, __ Q, __ post(c, 16));\n-    kyber_addv80();\n-    __ addv(v28, __ T8H, v28, v29);\n-    kyber_addv_v31_80();\n-    __ addv(v28, __ T8H, v28, v31);\n-    kyber_store80(result);\n-    __ str(v28, __ Q, __ post(result, 16));\n-    kyber_load80v0_v17(a);\n-    __ ldr(v28, __ Q, __ post(a, 16));\n-    kyber_load80v18_v27(b);\n-    __ ldr(v29, __ Q, __ post(b, 16));\n-    kyber_addv80();\n-    __ addv(v28, __ T8H, v28, v29);\n-    kyber_load80v18_v27(c);\n-    __ ldr(v29, __ Q, __ post(c, 16));\n-    kyber_addv80();\n-    __ addv(v28, __ T8H, v28, v29);\n-    kyber_addv_v31_80();\n-    __ addv(v28, __ T8H, v28, v31);\n-    kyber_store80(result);\n-    __ str(v28, __ Q, __ post(result, 16));\n-    kyber_load80v0_v17(a);\n-    kyber_load80v18_v27(b);\n-    kyber_addv80();\n-    kyber_load80v18_v27(c);\n-    kyber_addv80();\n-    kyber_addv_v31_80();\n-    kyber_store80(result);\n+    \/\/ As above we sum 256 sets of values in total i.e. 32 x 8H\n+    \/\/ quadwords.  So, we can load, add and store the data in 3\n+    \/\/ groups of 11, 11 and 10 at a time i.e. we need to map sets\n+    \/\/ of 10 or 11 registers. A further constraint is that the\n+    \/\/ mapping needs to skip callee saves. So, we allocate the\n+    \/\/ register sequences using two 8 sequences, two 2 sequences\n+    \/\/ and two single registers.\n+    VSeq<8> vs1_1(0);\n+    VSeq<2> vs1_2(16);\n+    FloatRegister vs1_3 = v28;\n+    VSeq<8> vs2_1(18);\n+    VSeq<2> vs2_2(26);\n+    FloatRegister vs2_3 = v29;\n+\n+    \/\/ two constant vector sequences\n+    VSeq<8> vc_1(31, 0);\n+    VSeq<2> vc_2(31, 0);\n+\n+    FloatRegister vc_3 = v31;\n+\n+    __ lea(kyberConsts,\n+             ExternalAddress((address) StubRoutines::aarch64::_kyberConsts));\n+\n+    __ ldr(vc_3, __ Q, Address(kyberConsts, 16)); \/\/ q\n+    for (int i = 0; i < 3; i++) {\n+      \/\/ load 80 or 88 values from a into vs1_1\/2\/3\n+      vs_ldpq_post(vs1_1, a);\n+      vs_ldpq_post(vs1_2, a);\n+      if (i < 2) {\n+        __ ldr(vs1_3, __ Q, __ post(a, 16));\n+      }\n+      \/\/ load 80 or 88 values from b into vs2_1\/2\/3\n+      vs_ldpq_post(vs2_1, b);\n+      vs_ldpq_post(vs2_2, b);\n+      if (i < 2) {\n+        __ ldr(vs2_3, __ Q, __ post(b, 16));\n+      }\n+      \/\/ sum 80 or 88 values across vs1 and vs2 into vs1\n+      vs_addv(vs1_1, __ T8H, vs1_1, vs2_1);\n+      vs_addv(vs1_2, __ T8H, vs1_2, vs2_2);\n+      if (i < 2) {\n+        __ addv(vs1_3, __ T8H, vs1_3, vs2_3);\n+      }\n+      \/\/ load 80 or 88 values from c into vs2_1\/2\/3\n+      vs_ldpq_post(vs2_1, c);\n+      vs_ldpq_post(vs2_2, c);\n+      if (i < 2) {\n+        __ ldr(vs2_3, __ Q, __ post(c, 16));\n+      }\n+      \/\/ sum 80 or 88 values across vs1 and vs2 into vs1\n+      vs_addv(vs1_1, __ T8H, vs1_1, vs2_1);\n+      vs_addv(vs1_2, __ T8H, vs1_2, vs2_2);\n+      if (i < 2) {\n+        __ addv(vs1_3, __ T8H, vs1_3, vs2_3);\n+      }\n+      \/\/ add constant to all 80 or 88 results\n+      vs_addv(vs1_1, __ T8H, vs1_1, vc_1);\n+      vs_addv(vs1_2, __ T8H, vs1_2, vc_2);\n+      if (i < 2) {\n+        __ addv(vs1_3, __ T8H, vs1_3, vc_3);\n+      }\n+      \/\/ store 80 or 88 values\n+      vs_stpq_post(vs1_1, result);\n+      vs_stpq_post(vs1_2, result);\n+      if (i < 2) {\n+        __ str(vs1_3, __ Q, __ post(result, 16));\n+      }\n+    }\n@@ -5981,5 +5905,5 @@\n-     __ align(CodeEntryAlignment);\n-     StubGenStubId stub_id = StubGenStubId::kyber12To16_id;\n-     StubCodeMark mark(this, stub_id);\n-     address start = __ pc();\n-     __ enter();\n+    __ align(CodeEntryAlignment);\n+    StubGenStubId stub_id = StubGenStubId::kyber12To16_id;\n+    StubCodeMark mark(this, stub_id);\n+    address start = __ pc();\n+    __ enter();\n@@ -5994,0 +5918,7 @@\n+    \/\/ Data is input 96 bytes at a time i.e. in groups of 6 x 16B\n+    \/\/ quadwords so we need a 6 vector sequence for the inputs.\n+    \/\/ Parsing produces 64 shorts, employing two 8 vector\n+    \/\/ sequences to store and combine the intermediate data.\n+    VSeq<6> vin(24);\n+    VSeq<8> va(0), vb(16);\n+\n@@ -5995,1 +5926,1 @@\n-    __ ldr(v31, __ Q, tmpAddr);\n+    __ ldr(v31, __ Q, tmpAddr); \/\/ 8H times 0x0f00\n@@ -5999,44 +5930,57 @@\n-    __ ld3(v24, v25, v26, __ T16B, __ post(condensed, 48));\n-    __ ld3(v27, v28, v29, __ T16B, __ post(condensed, 48));\n-\n-    __ ushll(v0, __ T8H, v24, __ T8B, 0);\n-    __ ushll2(v1, __ T8H, v24, __ T16B, 0);\n-    __ ushll(v2, __ T8H, v25, __ T8B, 0);\n-    __ ushll2(v3, __ T8H, v25, __ T16B, 0);\n-    __ ushll(v4, __ T8H, v25, __ T8B, 0);\n-    __ ushll2(v5, __ T8H, v25, __ T16B, 0);\n-    __ ushll(v16, __ T8H, v27, __ T8B, 0);\n-    __ ushll2(v17, __ T8H, v27, __ T16B, 0);\n-    __ ushll(v18, __ T8H, v28, __ T8B, 0);\n-    __ ushll2(v19, __ T8H, v28, __ T16B, 0);\n-    __ ushll(v20, __ T8H, v28, __ T8B, 0);\n-    __ ushll2(v21, __ T8H, v28, __ T16B, 0);\n-    __ shl(v2, __ T8H, v2, 8);\n-    __ shl(v3, __ T8H, v3, 8);\n-    __ shl(v18, __ T8H, v18, 8);\n-    __ shl(v19, __ T8H, v19, 8);\n-    __ ushll(v6, __ T8H, v26, __ T8B, 4);\n-    __ ushll2(v7, __ T8H, v26, __ T16B, 4);\n-    __ ushll(v22, __ T8H, v29, __ T8B, 4);\n-    __ ushll2(v23, __ T8H, v29, __ T16B, 4);\n-    __ andr(v2, __ T16B, v2, v31);\n-    __ andr(v3, __ T16B, v3, v31);\n-    __ ushr(v4, __ T8H, v4, 4);\n-    __ ushr(v5, __ T8H, v5, 4);\n-    __ andr(v18, __ T16B, v18, v31);\n-    __ andr(v19, __ T16B, v19, v31);\n-    __ ushr(v20, __ T8H, v20, 4);\n-    __ ushr(v21, __ T8H, v21, 4);\n-    __ addv(v0, __ T8H, v0, v2);\n-    __ addv(v2, __ T8H, v1, v3);\n-    __ addv(v1, __ T8H, v4, v6);\n-    __ addv(v3, __ T8H, v5, v7);\n-    __ addv(v16, __ T8H, v16, v18);\n-    __ addv(v18, __ T8H, v17, v19);\n-    __ addv(v17, __ T8H, v20, v22);\n-    __ addv(v19, __ T8H, v21, v23);\n-\n-    __ st2(v0, v1, __ T8H, __ post(parsed, 32));\n-    __ st2(v2, v3, __ T8H, __ post(parsed, 32));\n-    __ st2(v16, v17, __ T8H, __ post(parsed, 32));\n-    __ st2(v18, v19, __ T8H, __ post(parsed, 32));\n+    \/\/ load 96 (6 x 16B) byte values\n+    vs_ld3_post(vin, __ T16B, condensed);\n+\n+    \/\/ expand groups of input bytes in vin to shorts in va and vb\n+    \/\/ n.b. target elements 2 and 3 duplicate elements 4 and 5\n+    __ ushll(va[0], __ T8H, vin[0], __ T8B, 0);\n+    __ ushll2(va[1], __ T8H, vin[0], __ T16B, 0);\n+    __ ushll(va[2], __ T8H, vin[1], __ T8B, 0);\n+    __ ushll2(va[3], __ T8H, vin[1], __ T16B, 0);\n+    __ ushll(va[4], __ T8H, vin[1], __ T8B, 0);\n+    __ ushll2(va[5], __ T8H, vin[1], __ T16B, 0);\n+\n+    __ ushll(vb[0], __ T8H, vin[3], __ T8B, 0);\n+    __ ushll2(vb[1], __ T8H, vin[3], __ T16B, 0);\n+    __ ushll(vb[2], __ T8H, vin[4], __ T8B, 0);\n+    __ ushll2(vb[3], __ T8H, vin[4], __ T16B, 0);\n+    __ ushll(vb[4], __ T8H, vin[4], __ T8B, 0);\n+    __ ushll2(vb[5], __ T8H, vin[4], __ T16B, 0);\n+\n+    \/\/ offset duplicated elements in va and vb by 8\n+    __ shl(va[2], __ T8H, va[2], 8);\n+    __ shl(va[3], __ T8H, va[3], 8);\n+    __ shl(vb[2], __ T8H, vb[2], 8);\n+    __ shl(vb[3], __ T8H, vb[3], 8);\n+\n+    \/\/ expand remaining input bytes in vin to shorts in va and vb\n+    \/\/ but this time pre-shifted by 4\n+    __ ushll(va[6], __ T8H, vin[2], __ T8B, 4);\n+    __ ushll2(va[7], __ T8H, vin[2], __ T16B, 4);\n+    __ ushll(vb[6], __ T8H, vin[5], __ T8B, 4);\n+    __ ushll2(vb[7], __ T8H, vin[5], __ T16B, 4);\n+\n+    \/\/ split the duplicated 8 bit values into two distinct 4 bit\n+    \/\/ upper and lower halves using a mask or a shift\n+    __ andr(va[2], __ T16B, va[2], v31);\n+    __ andr(va[3], __ T16B, va[3], v31);\n+    __ ushr(va[4], __ T8H, va[4], 4);\n+    __ ushr(va[5], __ T8H, va[5], 4);\n+    __ andr(vb[2], __ T16B, vb[2], v31);\n+    __ andr(vb[3], __ T16B, vb[3], v31);\n+    __ ushr(vb[4], __ T8H, vb[4], 4);\n+    __ ushr(vb[5], __ T8H, vb[5], 4);\n+\n+    \/\/ sum resulting short values into the front halves of va and\n+    \/\/ vb pairing registers offset by stride 2\n+    __ addv(va[0], __ T8H, va[0], va[2]);\n+    __ addv(va[2], __ T8H, va[1], va[3]);\n+    __ addv(va[1], __ T8H, va[4], va[6]);\n+    __ addv(va[3], __ T8H, va[5], va[7]);\n+    __ addv(vb[0], __ T8H, vb[0], vb[2]);\n+    __ addv(vb[2], __ T8H, vb[1], vb[3]);\n+    __ addv(vb[1], __ T8H, vb[4], vb[6]);\n+    __ addv(vb[3], __ T8H, vb[5], vb[7]);\n+\n+    \/\/ store results interleaved as shorts\n+    vs_st2_post(vs_front(va), __ T8H, parsed);\n+    vs_st2_post(vs_front(vb), __ T8H, parsed);\n@@ -6049,34 +5993,52 @@\n-    __ ld3(v24, v25, v26, __ T16B, __ post(condensed, 48));\n-    __ ld3(v27, v28, v29, __ T8B, condensed);\n-\n-    __ ushll(v0, __ T8H, v24, __ T8B, 0);\n-    __ ushll2(v1, __ T8H, v24, __ T16B, 0);\n-    __ ushll(v2, __ T8H, v25, __ T8B, 0);\n-    __ ushll2(v3, __ T8H, v25, __ T16B, 0);\n-    __ ushll(v4, __ T8H, v25, __ T8B, 0);\n-    __ ushll2(v5, __ T8H, v25, __ T16B, 0);\n-    __ ushll(v16, __ T8H, v27, __ T8B, 0);\n-    __ ushll(v18, __ T8H, v28, __ T8B, 0);\n-    __ ushll(v20, __ T8H, v28, __ T8B, 0);\n-    __ shl(v2, __ T8H, v2, 8);\n-    __ shl(v3, __ T8H, v3, 8);\n-    __ shl(v18, __ T8H, v18, 8);\n-    __ ushll(v6, __ T8H, v26, __ T8B, 4);\n-    __ ushll2(v7, __ T8H, v26, __ T16B, 4);\n-    __ ushll(v22, __ T8H, v29, __ T8B, 4);\n-    __ andr(v2, __ T16B, v2, v31);\n-    __ andr(v3, __ T16B, v3, v31);\n-    __ ushr(v4, __ T8H, v4, 4);\n-    __ ushr(v5, __ T8H, v5, 4);\n-    __ andr(v18, __ T16B, v18, v31);\n-    __ ushr(v20, __ T8H, v20, 4);\n-    __ addv(v0, __ T8H, v0, v2);\n-    __ addv(v2, __ T8H, v1, v3);\n-    __ addv(v1, __ T8H, v4, v6);\n-    __ addv(v3, __ T8H, v5, v7);\n-    __ addv(v16, __ T8H, v16, v18);\n-    __ addv(v17, __ T8H, v20, v22);\n-\n-    __ st2(v0, v1, __ T8H, __ post(parsed, 32));\n-    __ st2(v2, v3, __ T8H, __ post(parsed, 32));\n-    __ st2(v16, v17, __ T8H, __ post(parsed, 32));\n+    \/\/ if anything is left it should be a final 72 bytes. so we\n+    \/\/ load 48 bytes into both lanes of front(vin) and 24 bytes\n+    \/\/ into the lower lane of back(vin)\n+    vs_ld3_post(vs_front(vin), __ T16B, condensed);\n+    vs_ld3(vs_back(vin), __ T8B, condensed);\n+\n+    \/\/ expand groups of input bytes in vin to shorts in va and vb\n+    \/\/ n.b. target elements 2 and 3 of va duplicate elements 4 and\n+    \/\/ 5 and target element 2 of vb duplicates element 4.\n+    __ ushll(va[0], __ T8H, vin[0], __ T8B, 0);\n+    __ ushll2(va[1], __ T8H, vin[0], __ T16B, 0);\n+    __ ushll(va[2], __ T8H, vin[1], __ T8B, 0);\n+    __ ushll2(va[3], __ T8H, vin[1], __ T16B, 0);\n+    __ ushll(va[4], __ T8H, vin[1], __ T8B, 0);\n+    __ ushll2(va[5], __ T8H, vin[1], __ T16B, 0);\n+\n+    __ ushll(vb[0], __ T8H, vin[3], __ T8B, 0);\n+    __ ushll(vb[2], __ T8H, vin[4], __ T8B, 0);\n+    __ ushll(vb[4], __ T8H, vin[4], __ T8B, 0);\n+\n+    \/\/ offset duplicated elements in va and vb by 8\n+    __ shl(va[2], __ T8H, va[2], 8);\n+    __ shl(va[3], __ T8H, va[3], 8);\n+    __ shl(vb[2], __ T8H, vb[2], 8);\n+\n+    \/\/ expand remaining input bytes in vin to shorts in va and vb\n+    \/\/ but this time pre-shifted by 4\n+    __ ushll(va[6], __ T8H, vin[2], __ T8B, 4);\n+    __ ushll2(va[7], __ T8H, vin[2], __ T16B, 4);\n+    __ ushll(vb[6], __ T8H, vin[5], __ T8B, 4);\n+\n+    \/\/ split the duplicated 8 bit values into two distinct 4 bit\n+    \/\/ upper and lower halves using a mask or a shift\n+    __ andr(va[2], __ T16B, va[2], v31);\n+    __ andr(va[3], __ T16B, va[3], v31);\n+    __ ushr(va[4], __ T8H, va[4], 4);\n+    __ ushr(va[5], __ T8H, va[5], 4);\n+    __ andr(vb[2], __ T16B, vb[2], v31);\n+    __ ushr(vb[4], __ T8H, vb[4], 4);\n+\n+    \/\/ sum resulting short values into the front halves of va and\n+    \/\/ vb pairing registers offset by stride 2\n+    __ addv(va[0], __ T8H, va[0], va[2]);\n+    __ addv(va[2], __ T8H, va[1], va[3]);\n+    __ addv(va[1], __ T8H, va[4], va[6]);\n+    __ addv(va[3], __ T8H, va[5], va[7]);\n+    __ addv(vb[0], __ T8H, vb[0], vb[2]);\n+    __ addv(vb[1], __ T8H, vb[4], vb[6]);\n+\n+    \/\/ store results interleaved as shorts\n+    vs_st2_post(vs_front(va), __ T8H, parsed);\n+    vs_st2_post(vs_front(vs_front(vb)), __ T8H, parsed);\n@@ -6093,22 +6055,1 @@\n-  void kyber_sqdmulh80() {\n-    kyber_sqdmulh32(0);\n-    kyber_sqdmulh32(4);\n-    __ sqdmulh(v26, __ T8H, v16, v31);\n-    __ sqdmulh(v27, __ T8H, v17, v31);\n-  }\n-\n-  void kyber_sshr80() {\n-    kyber_sshr32(18);\n-    kyber_sshr32(22);\n-    __ sshr(v26, __ T8H, v26, 11);\n-    __ sshr(v27, __ T8H, v27, 11);\n-  }\n-\n-  void kyber_mlsv80() {\n-    kyber_mlsv32(0);\n-    kyber_mlsv32(4);\n-    __ mlsv(v16, __ T8H, v26, v30);\n-    __ mlsv(v17, __ T8H, v27, v30);\n-  }\n-\n-  \/\/ Kyber barrett reduce function.\n+  \/\/ Kyber Barrett reduce function.\n@@ -6132,0 +6073,23 @@\n+    \/\/ As above we process 256 sets of values in total i.e. 32 x\n+    \/\/ 8H quadwords. So, we can load, add and store the data in 3\n+    \/\/ groups of 11, 11 and 10 at a time i.e. we need to map sets\n+    \/\/ of 10 or 11 registers. A further constraint is that the\n+    \/\/ mapping needs to skip callee saves. So, we allocate the\n+    \/\/ register sequences using two 8 sequences, two 2 sequences\n+    \/\/ and two single registers.\n+    VSeq<8> vs1_1(0);\n+    VSeq<2> vs1_2(16);\n+    FloatRegister vs1_3 = v28;\n+    VSeq<8> vs2_1(18);\n+    VSeq<2> vs2_2(26);\n+    FloatRegister vs2_3 = v29;\n+\n+    \/\/ we also need a pair of corresponding constant sequences\n+    VSeq<8> vc1_1(30, 0);\n+    VSeq<2> vc1_2(30, 0);\n+\n+    FloatRegister vc1_3 = v30;\n+    VSeq<8> vc2_1(31, 0);\n+    VSeq<2> vc2_2(31, 0);\n+    FloatRegister vc2_3 = v31;\n+\n@@ -6133,1 +6097,2 @@\n-    __ lea(kyberConsts, ExternalAddress((address) StubRoutines::aarch64::_kyberConsts));\n+    __ lea(kyberConsts,\n+             ExternalAddress((address) StubRoutines::aarch64::_kyberConsts));\n@@ -6135,0 +6100,1 @@\n+    \/\/ load q and the multiplier for the Barrett reduction\n@@ -6136,28 +6102,30 @@\n-    __ ldpq(v30, v31, kyberConsts);\n-\n-    kyber_load80v0_v17(coeffs);\n-    __ ldr(v28, __ Q, __ post(coeffs, 16));\n-    kyber_sqdmulh80();\n-    __ sqdmulh(v29, __ T8H, v28, v31);\n-    kyber_sshr80();\n-    __ sshr(v29, __ T8H, v29, 11);\n-    kyber_mlsv80();\n-    __ mlsv(v28, __ T8H, v29, v30);\n-    kyber_store80(result);\n-    __ str(v28, __ Q, __ post(result, 16));\n-\n-    kyber_load80v0_v17(coeffs);\n-    __ ldr(v28, __ Q, __ post(coeffs, 16));\n-    kyber_sqdmulh80();     __ sqdmulh(v29, __ T8H, v28, v31);\n-    kyber_sshr80();\n-    __ sshr(v29, __ T8H, v29, 11);\n-    kyber_mlsv80();\n-    __ mlsv(v28, __ T8H, v29, v30);\n-    kyber_store80(result);\n-    __ str(v28, __ Q, __ post(result, 16));\n-\n-    kyber_load80v0_v17(coeffs);\n-    kyber_sqdmulh80();\n-    kyber_sshr80();\n-    kyber_mlsv80();\n-    kyber_store80(result);\n+    __ ldpq(vc1_3, vc2_3, kyberConsts);\n+\n+    for (int i = 0; i < 3; i++) {\n+      \/\/ load 80 or 88 coefficients\n+      vs_ldpq_post(vs1_1, coeffs);\n+      vs_ldpq_post(vs1_2, coeffs);\n+      if (i < 2) {\n+        __ ldr(vs1_3, __ Q, __ post(coeffs, 16));\n+      }\n+      vs_sqdmulh(vs2_1, __ T8H, vs1_1, vc2_1);\n+      vs_sqdmulh(vs2_2, __ T8H, vs1_2, vc2_2);\n+      if (i < 2) {\n+        __ sqdmulh(vs2_3, __ T8H, vs1_3, vc2_3);\n+      }\n+      vs_sshr(vs2_1, __ T8H, vs2_1, 11);\n+      vs_sshr(vs2_2, __ T8H, vs2_2, 11);\n+      if (i < 2) {\n+        __ sshr(vs2_3, __ T8H, vs2_3, 11);\n+      }\n+      vs_mlsv(vs1_1, __ T8H, vs2_1, vc1_1);\n+      vs_mlsv(vs1_2, __ T8H, vs2_2, vc1_2);\n+      if (i < 2) {\n+        __ mlsv(vs1_3, __ T8H, vs2_3, vc1_3);\n+      }\n+      vs_stpq_post(vs1_1, result);\n+      vs_stpq_post(vs1_2, result);\n+      if (i < 2) {\n+        __ str(vs1_3, __ Q, __ post(result, 16));\n+      }\n+    }\n@@ -6172,167 +6140,0 @@\n-  \/\/ Helpers to schedule parallel operation bundles across vector\n-  \/\/ register sequences of size 2, 4 or 8.\n-\n-  \/\/ Implement various primitive computations across vector sequences\n-\n-  template<int N>\n-  void vs_addv(const VSeq<N>& v, Assembler::SIMD_Arrangement T,\n-               const VSeq<N>& v1, const VSeq<N>& v2) {\n-    for (int i = 0; i < N; i++) {\n-      __ addv(v[i], T, v1[i], v2[i]);\n-    }\n-\n-  }\n-\n-  template<int N>\n-  void vs_subv(const VSeq<N>& v, Assembler::SIMD_Arrangement T,\n-               const VSeq<N>& v1, const VSeq<N>& v2) {\n-    for (int i = 0; i < N; i++) {\n-      __ subv(v[i], T, v1[i], v2[i]);\n-    }\n-  }\n-\n-  template<int N>\n-  void vs_mulv(const VSeq<N>& v, Assembler::SIMD_Arrangement T,\n-               const VSeq<N>& v1, const VSeq<N>& v2) {\n-    for (int i = 0; i < N; i++) {\n-      __ mulv(v[i], T, v1[i], v2[i]);\n-    }\n-  }\n-\n-  template<int N>\n-  void vs_negr(const VSeq<N>& v, Assembler::SIMD_Arrangement T, const VSeq<N>& v1) {\n-    for (int i = 0; i < N; i++) {\n-      __ negr(v[i], T, v1[i]);\n-    }\n-  }\n-\n-  template<int N>\n-  void vs_sshr(const VSeq<N>& v, Assembler::SIMD_Arrangement T,\n-               const VSeq<N>& v1, int shift) {\n-    for (int i = 0; i < N; i++) {\n-      __ sshr(v[i], T, v1[i], shift);\n-    }\n-  }\n-\n-  template<int N>\n-  void vs_andr(const VSeq<N>& v, const VSeq<N>& v1, const VSeq<N>& v2) {\n-    for (int i = 0; i < N; i++) {\n-      __ andr(v[i], __ T16B, v1[i], v2[i]);\n-    }\n-  }\n-\n-  template<int N>\n-  void vs_orr(const VSeq<N>& v, const VSeq<N>& v1, const VSeq<N>& v2) {\n-    for (int i = 0; i < N; i++) {\n-      __ orr(v[i], __ T16B, v1[i], v2[i]);\n-    }\n-  }\n-\n-  template<int N>\n-    void vs_notr(const VSeq<N>& v, const VSeq<N>& v1) {\n-    for (int i = 0; i < N; i++) {\n-      __ notr(v[i], __ T16B, v1[i]);\n-    }\n-  }\n-\n-  \/\/ load N\/2 successive pairs of quadword values from memory in order\n-  \/\/ into N successive vector registers of the sequence via the\n-  \/\/ address supplied in base.\n-  template<int N>\n-  void vs_ldpq(const VSeq<N>& v, Register base) {\n-    for (int i = 0; i < N; i += 2) {\n-      __ ldpq(v[i], v[i+1], Address(base, 32 * i));\n-    }\n-  }\n-\n-  \/\/ load N\/2 successive pairs of quadword values from memory in order\n-  \/\/ into N vector registers of the sequence via the address supplied\n-  \/\/ in base using post-increment addressing\n-  template<int N>\n-  void vs_ldpq_post(const VSeq<N>& v, Register base) {\n-    for (int i = 0; i < N; i += 2) {\n-      __ ldpq(v[i], v[i+1], __ post(base, 32));\n-    }\n-  }\n-\n-  \/\/ store N successive vector registers of the sequence into N\/2\n-  \/\/ successive pairs of quadword memory locations via the address\n-  \/\/ supplied in base using post-increment addressing\n-  template<int N>\n-  void vs_stpq_post(const VSeq<N>& v, Register base) {\n-    for (int i = 0; i < N; i += 2) {\n-      __ stpq(v[i], v[i+1], __ post(base, 32));\n-    }\n-  }\n-\n-  \/\/ load N\/2 pairs of quadword values from memory into N vector\n-  \/\/ registers via the address supplied in base with each pair indexed\n-  \/\/ using the the start offset plus the corresponding entry in the\n-  \/\/ offsets array\n-  template<int N>\n-  void vs_ldpq_indexed(const VSeq<N>& v, Register base, int start, int (&offsets)[N\/2]) {\n-    for (int i = 0; i < N\/2; i++) {\n-      __ ldpq(v[2*i], v[2*i+1], Address(base, start + offsets[i]));\n-    }\n-  }\n-\n-  \/\/ store N vector registers into N\/2 pairs of quadword memory\n-  \/\/ locations via the address supplied in base with each pair indexed\n-  \/\/ using the the start offset plus the corresponding entry in the\n-  \/\/ offsets array\n-  template<int N>\n-  void vs_stpq_indexed(const VSeq<N>& v, Register base, int start, int offsets[N\/2]) {\n-    for (int i = 0; i < N\/2; i++) {\n-      __ stpq(v[2*i], v[2*i+1], Address(base, start + offsets[i]));\n-    }\n-  }\n-\n-  \/\/ load N single quadword values from memory into N vector registers\n-  \/\/ via the address supplied in base with each value indexed using\n-  \/\/ the the start offset plus the corresponding entry in the offsets\n-  \/\/ array\n-  template<int N>\n-  void vs_ldr_indexed(const VSeq<N>& v, Assembler::SIMD_RegVariant T, Register base,\n-                      int start, int (&offsets)[N]) {\n-    for (int i = 0; i < N; i++) {\n-      __ ldr(v[i], T, Address(base, start + offsets[i]));\n-    }\n-  }\n-\n-  \/\/ store N vector registers into N single quadword memory locations\n-  \/\/ via the address supplied in base with each value indexed using\n-  \/\/ the the start offset plus the corresponding entry in the offsets\n-  \/\/ array\n-  template<int N>\n-  void vs_str_indexed(const VSeq<N>& v, Assembler::SIMD_RegVariant T, Register base,\n-                      int start, int (&offsets)[N]) {\n-    for (int i = 0; i < N; i++) {\n-      __ str(v[i], T, Address(base, start + offsets[i]));\n-    }\n-  }\n-\n-  \/\/ load N\/2 pairs of quadword values from memory de-interleaved into\n-  \/\/ N vector registers 2 at a time via the address supplied in base\n-  \/\/ with each pair indexed using the the start offset plus the\n-  \/\/ corresponding entry in the offsets array\n-  template<int N>\n-  void vs_ld2_indexed(const VSeq<N>& v, Assembler::SIMD_Arrangement T, Register base,\n-                      Register tmp, int start, int (&offsets)[N\/2]) {\n-    for (int i = 0; i < N\/2; i++) {\n-      __ add(tmp, base, start + offsets[i]);\n-      __ ld2(v[2*i], v[2*i+1], T, tmp);\n-    }\n-  }\n-\n-  \/\/ store N vector registers 2 at a time interleaved into N\/2 pairs\n-  \/\/ of quadword memory locations via the address supplied in base\n-  \/\/ with each pair indexed using the the start offset plus the\n-  \/\/ corresponding entry in the offsets array\n-  template<int N>\n-  void vs_st2_indexed(const VSeq<N>& v, Assembler::SIMD_Arrangement T, Register base,\n-                      Register tmp, int start, int (&offsets)[N\/2]) {\n-    for (int i = 0; i < N\/2; i++) {\n-      __ add(tmp, base, start + offsets[i]);\n-      __ st2(v[2*i], v[2*i+1], T, tmp);\n-    }\n-  }\n@@ -6340,2 +6141,3 @@\n-  \/\/ Helper routines for various flavours of dilithium montgomery\n-  \/\/ multiply\n+  \/\/ Dilithium-specific montmul helper routines that generate parallel\n+  \/\/ code for, respectively, a single 4x4s vector sequence montmul or\n+  \/\/ two such multiplies in a row.\n@@ -6344,11 +6146,0 @@\n-  \/\/ See the montMul() method of the sun.security.provider.ML_DSA class.\n-  \/\/\n-  \/\/ Computes 4x4S results\n-  \/\/    a = b * c * 2^-32 mod MONT_Q\n-  \/\/ Inputs:  vb, vc - 4x4S vector register sequences\n-  \/\/          vq - 2x4S constants <MONT_Q, MONT_Q_INV_MOD_R>\n-  \/\/ Temps:   vtmp - 4x4S vector sequence trashed after call\n-  \/\/ Outputs: va - 4x4S vector register sequences\n-  \/\/ vb, vc, vtmp and vq must all be disjoint\n-  \/\/ va must be disjoint from all other inputs\/temps or must equal vc\n-  \/\/ n.b. MONT_R_BITS is 32, so the right shift by it is implicit.\n@@ -6356,32 +6147,4 @@\n-                    const VSeq<4>& vtmp, const VSeq<2>& vq) {\n-    assert(vs_disjoint(vb, vc), \"vb and vc overlap\");\n-    assert(vs_disjoint(vb, vq), \"vb and vq overlap\");\n-    assert(vs_disjoint(vb, vtmp), \"vb and vtmp overlap\");\n-\n-    assert(vs_disjoint(vc, vq), \"vc and vq overlap\");\n-    assert(vs_disjoint(vc, vtmp), \"vc and vtmp overlap\");\n-\n-    assert(vs_disjoint(vq, vtmp), \"vq and vtmp overlap\");\n-\n-    assert(vs_disjoint(va, vc) || vs_same(va, vc), \"va and vc neither disjoint nor equal\");\n-    assert(vs_disjoint(va, vb), \"va and vb overlap\");\n-    assert(vs_disjoint(va, vq), \"va and vq overlap\");\n-    assert(vs_disjoint(va, vtmp), \"va and vtmp overlap\");\n-\n-    \/\/ schedule 4 streams of instructions across the vector sequences\n-    for (int i = 0; i < 4; i++) {\n-      __ sqdmulh(vtmp[i], __ T4S, vb[i], vc[i]); \/\/ aHigh = hi32(2 * b * c)\n-      __ mulv(va[i], __ T4S, vb[i], vc[i]);    \/\/ aLow = lo32(b * c)\n-    }\n-\n-    for (int i = 0; i < 4; i++) {\n-      __ mulv(va[i], __ T4S, va[i], vq[0]);     \/\/ m = aLow * qinv\n-    }\n-\n-    for (int i = 0; i < 4; i++) {\n-      __ sqdmulh(va[i], __ T4S, va[i], vq[1]);  \/\/ n = hi32(2 * m * q)\n-    }\n-\n-    for (int i = 0; i < 4; i++) {\n-      __ shsubv(va[i], __ T4S, vtmp[i], va[i]);   \/\/ a = (aHigh - n) \/ 2\n-    }\n+                           const VSeq<4>& vtmp, const VSeq<2>& vq) {\n+    \/\/ Use the helper routine to schedule a 4x4S Montgomery multiply.\n+    \/\/ It will assert that the register use is valid\n+    vs_montmul4(va, vb, vc, __ T4S, vtmp, vq);\n@@ -6391,13 +6154,8 @@\n-  \/\/ See the montMul() method of the sun.security.provider.ML_DSA class.\n-  \/\/\n-  \/\/ Computes 8x4S results\n-  \/\/    a = b * c * 2^-32 mod MONT_Q\n-  \/\/ Inputs:  vb, vc - 8x4S vector register sequences\n-  \/\/          vq - 2x4S constants <MONT_Q, MONT_Q_INV_MOD_R>\n-  \/\/ Temps:   vtmp - 4x4S vector sequence trashed after call\n-  \/\/ Outputs: va - 8x4S vector register sequences\n-  \/\/ vb, vc, vtmp and vq must all be disjoint\n-  \/\/ va must be disjoint from all other inputs\/temps or must equal vc\n-  \/\/ n.b. MONT_R_BITS is 32, so the right shift by it is implicit.\n-  void vs_montmul32(const VSeq<8>& va, const VSeq<8>& vb, const VSeq<8>& vc,\n-                    const VSeq<4>& vtmp, const VSeq<2>& vq) {\n+  void dilithium_montmul32(const VSeq<8>& va, const VSeq<8>& vb, const VSeq<8>& vc,\n+                           const VSeq<4>& vtmp, const VSeq<2>& vq) {\n+    \/\/ Schedule two successive 4x4S multiplies via the montmul helper\n+    \/\/ on the front and back halves of va, vb and vc. The helper will\n+    \/\/ assert that the register use has no overlap conflicts on each\n+    \/\/ individual call but we also need to ensure that the necessary\n+    \/\/ disjoint\/equality constraints are met across both calls.\n+\n@@ -6421,2 +6179,2 @@\n-    \/\/ we need to multiply the front and back halves of each sequence\n-    \/\/ 4x4S at a time because\n+    \/\/ We multiply the front and back halves of each sequence 4 at a\n+    \/\/ time because\n@@ -6429,1 +6187,1 @@\n-    \/\/ be a VSeq<4> which means we only have 4 scratch slots\n+    \/\/ be a VSeq<4> which means we only have 4 scratch slots.\n@@ -6431,2 +6189,2 @@\n-    dilithium_montmul16(vs_front(va), vs_front(vb), vs_front(vc), vtmp, vq);\n-    dilithium_montmul16(vs_back(va), vs_back(vb), vs_back(vc), vtmp, vq);\n+    vs_montmul4(vs_front(va), vs_front(vb), vs_front(vc), __ T4S, vtmp, vq);\n+    vs_montmul4(vs_back(va), vs_back(vb), vs_back(vc), __ T4S, vtmp, vq);\n@@ -6435,4 +6193,4 @@\n-  \/\/ perform combined montmul then add\/sub on 4x4S vectors\n-\n-  void dilithium_montmul16_sub_add(const VSeq<4>& va0, const VSeq<4>& va1, const VSeq<4>& vc,\n-                                   const VSeq<4>& vtmp, const VSeq<2>& vq) {\n+  \/\/ Perform combined montmul then add\/sub on 4x4S vectors.\n+  void dilithium_montmul16_sub_add(\n+          const VSeq<4>& va0, const VSeq<4>& va1, const VSeq<4>& vc,\n+          const VSeq<4>& vtmp, const VSeq<2>& vq) {\n@@ -6447,4 +6205,4 @@\n-  \/\/ perform combined add\/sub then montul on 4x4S vectors\n-\n-  void dilithium_sub_add_montmul16(const VSeq<4>& va0, const VSeq<4>& va1, const VSeq<4>& vb,\n-                                   const VSeq<4>& vtmp1, const VSeq<4>& vtmp2, const VSeq<2>& vq) {\n+  \/\/ Perform combined add\/sub then montul on 4x4S vectors.\n+  void dilithium_sub_add_montmul16(\n+          const VSeq<4>& va0, const VSeq<4>& va1, const VSeq<4>& vb,\n+          const VSeq<4>& vtmp1, const VSeq<4>& vtmp2, const VSeq<2>& vq) {\n@@ -6493,1 +6251,1 @@\n-      \/\/ for levels 1 - 4 we simply load 2 x 4 adjacent values at a\n+      \/\/ For levels 1 - 4 we simply load 2 x 4 adjacent values at a\n@@ -6496,1 +6254,1 @@\n-      \/\/ pair instructions with arrangement 4S\n+      \/\/ pair instructions with arrangement 4S.\n@@ -6505,1 +6263,1 @@\n-        vs_montmul32(vs2, vs1, vs2, vtmp, vq);\n+        dilithium_montmul32(vs2, vs1, vs2, vtmp, vq);\n@@ -6559,2 +6317,2 @@\n-    int offsets[4] = {0, 32, 64, 96};\n-    int offsets1[8] = {16, 48, 80, 112, 144, 176, 208, 240 };\n+    int offsets[4] = { 0, 32, 64, 96};\n+    int offsets1[8] = { 16, 48, 80, 112, 144, 176, 208, 240 };\n@@ -6563,1 +6321,2 @@\n-    __ lea(dilithiumConsts, ExternalAddress((address) StubRoutines::aarch64::_dilithiumConsts));\n+    __ lea(dilithiumConsts,\n+             ExternalAddress((address) StubRoutines::aarch64::_dilithiumConsts));\n@@ -6565,1 +6324,1 @@\n-    \/\/ Each level represents one iteration of the outer for loop of the Java version\n+    \/\/ Each level represents one iteration of the outer for loop of the Java version.\n@@ -6572,1 +6331,1 @@\n-    \/\/ at level 5 the coefficients we need to combine with the zetas\n+    \/\/ At level 5 the coefficients we need to combine with the zetas\n@@ -6586,1 +6345,1 @@\n-      vs_montmul32(vs2, vs1, vs2, vtmp, vq);\n+      dilithium_montmul32(vs2, vs1, vs2, vtmp, vq);\n@@ -6598,1 +6357,1 @@\n-    \/\/ at level 6 the coefficients we need to combine with the zetas\n+    \/\/ At level 6 the coefficients we need to combine with the zetas\n@@ -6626,1 +6385,1 @@\n-    \/\/ at level 7 the coefficients we need to combine with the zetas\n+    \/\/ At level 7 the coefficients we need to combine with the zetas\n@@ -6698,1 +6457,1 @@\n-      \/\/ for levels 3 - 7 we simply load 2 x 4 adjacent values at a\n+      \/\/ For levels 3 - 7 we simply load 2 x 4 adjacent values at a\n@@ -6701,1 +6460,1 @@\n-      \/\/ pair instructions with arrangement 4S\n+      \/\/ pair instructions with arrangement 4S.\n@@ -6718,1 +6477,1 @@\n-        vs_montmul32(vs2, vs1, vs2, vtmp, vq);\n+        dilithium_montmul32(vs2, vs1, vs2, vtmp, vq);\n@@ -6769,1 +6528,2 @@\n-    __ lea(dilithiumConsts, ExternalAddress((address) StubRoutines::aarch64::_dilithiumConsts));\n+    __ lea(dilithiumConsts,\n+             ExternalAddress((address) StubRoutines::aarch64::_dilithiumConsts));\n@@ -6772,1 +6532,0 @@\n-    \/\/ level0\n@@ -6778,1 +6537,1 @@\n-    \/\/ load and store the values using an ld2\/st2 with arrangement 4S\n+    \/\/ load and store the values using an ld2\/st2 with arrangement 4S.\n@@ -6800,1 +6559,1 @@\n-    \/\/ values an ld2\/st2 with arrangement 2D\n+    \/\/ values an ld2\/st2 with arrangement 2D.\n@@ -6836,1 +6595,1 @@\n-      vs_montmul32(vs2, vs1, vs2, vtmp, vq);\n+      dilithium_montmul32(vs2, vs1, vs2, vtmp, vq);\n@@ -6849,1 +6608,0 @@\n-\n@@ -6883,1 +6641,2 @@\n-    __ lea(dilithiumConsts, ExternalAddress((address) StubRoutines::aarch64::_dilithiumConsts));\n+    __ lea(dilithiumConsts,\n+             ExternalAddress((address) StubRoutines::aarch64::_dilithiumConsts));\n@@ -6900,1 +6659,1 @@\n-    vs_montmul32(vs2, vs1, vs2, vtmp, vq);\n+    dilithium_montmul32(vs2, vs1, vs2, vtmp, vq);\n@@ -6902,1 +6661,1 @@\n-    vs_montmul32(vs2, vrsquare, vs2, vtmp, vq);\n+    dilithium_montmul32(vs2, vrsquare, vs2, vtmp, vq);\n@@ -6915,1 +6674,0 @@\n-\n@@ -6943,1 +6701,1 @@\n-    VSeq<4> vtmp = vs_front(vs3);         \/\/ n.b. tmp registers overlap vs3\n+    VSeq<4> vtmp = vs_front(vs3);      \/\/ n.b. tmp registers overlap vs3\n@@ -6949,1 +6707,2 @@\n-    __ lea(dilithiumConsts, ExternalAddress((address) StubRoutines::aarch64::_dilithiumConsts));\n+    __ lea(dilithiumConsts,\n+             ExternalAddress((address) StubRoutines::aarch64::_dilithiumConsts));\n@@ -6963,1 +6722,1 @@\n-    vs_montmul32(vs2, vconst, vs2, vtmp, vq);\n+    dilithium_montmul32(vs2, vconst, vs2, vtmp, vq);\n@@ -6976,1 +6735,0 @@\n-\n@@ -7007,1 +6765,2 @@\n-    VSeq<4> vs1(0), vs2(4), vs3(8); \/\/ 6 independent sets of 4x4s values\n+    \/\/ 6 independent sets of 4x4s values\n+    VSeq<4> vs1(0), vs2(4), vs3(8);\n@@ -7009,1 +6768,3 @@\n-    VSeq<4> one(25, 0);            \/\/ 7 constants for cross-multiplying\n+\n+    \/\/ 7 constants for cross-multiplying\n+    VSeq<4> one(25, 0);\n@@ -7019,1 +6780,2 @@\n-    __ lea(dilithiumConsts, ExternalAddress((address) StubRoutines::aarch64::_dilithiumConsts));\n+    __ lea(dilithiumConsts,\n+             ExternalAddress((address) StubRoutines::aarch64::_dilithiumConsts));\n@@ -7116,1 +6878,0 @@\n-\n@@ -7132,1 +6893,0 @@\n-\n","filename":"src\/hotspot\/cpu\/aarch64\/stubGenerator_aarch64.cpp","additions":1228,"deletions":1468,"binary":false,"changes":2696,"status":"modified"},{"patch":"@@ -53,6 +53,7 @@\n-    0xF301, 0xF301, 0xF301, 0xF301, 0xF301, 0xF301, 0xF301, 0xF301,\n-    0x0D01, 0x0D01, 0x0D01, 0x0D01, 0x0D01, 0x0D01, 0x0D01, 0x0D01,\n-    0x4EBF, 0x4EBF, 0x4EBF, 0x4EBF, 0x4EBF, 0x4EBF, 0x4EBF, 0x4EBF,\n-    0x0200, 0x0200, 0x0200, 0x0200, 0x0200, 0x0200, 0x0200, 0x0200,\n-    0x0549, 0x0549, 0x0549, 0x0549, 0x0549, 0x0549, 0x0549, 0x0549,\n-    0x0F00, 0x0F00, 0x0F00, 0x0F00, 0x0F00, 0x0F00, 0x0F00, 0x0F00\n+    \/\/ Because we sometimes load these in pairs, montQInvModR, kyber_q\n+    \/\/ and kyberBarrettMultiplier should stay together and in this order.\n+    0xF301, 0xF301, 0xF301, 0xF301, 0xF301, 0xF301, 0xF301, 0xF301, \/\/ montQInvModR\n+    0x0D01, 0x0D01, 0x0D01, 0x0D01, 0x0D01, 0x0D01, 0x0D01, 0x0D01, \/\/ kyber_q\n+    0x4EBF, 0x4EBF, 0x4EBF, 0x4EBF, 0x4EBF, 0x4EBF, 0x4EBF, 0x4EBF, \/\/ kyberBarrettMultiplier\n+    0x0200, 0x0200, 0x0200, 0x0200, 0x0200, 0x0200, 0x0200, 0x0200, \/\/ toMont((kyber_n \/ 2)^-1 (mod kyber_q))\n+    0x0549, 0x0549, 0x0549, 0x0549, 0x0549, 0x0549, 0x0549, 0x0549  \/\/ montRSquareModQ\n","filename":"src\/hotspot\/cpu\/aarch64\/stubRoutines_aarch64.cpp","additions":7,"deletions":6,"binary":false,"changes":13,"status":"modified"}]}