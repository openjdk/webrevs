{"files":[{"patch":"@@ -2555,0 +2555,5 @@\n+  INSN1(ld1, 0b001101010, 0b0000);\n+  INSN2(ld2, 0b001101011, 0b0000);\n+  INSN3(ld3, 0b001101010, 0b0010);\n+  INSN4(ld4, 0b001101011, 0b0010);\n+\n@@ -2589,0 +2594,1 @@\n+    if (strcmp(#NAME, \"sqdmulh\") == 0) guarantee(T != T8B && T != T16B, \"incorrect arrangement\");   \\\n@@ -2612,0 +2618,2 @@\n+  INSN(sqdmulh,0, 0b101101, false); \/\/ accepted arrangements: T4H, T8H, T2S, T4S\n+  INSN(shsubv, 0, 0b001001, false); \/\/ accepted arrangements: T8B, T16B, T4H, T8H, T2S, T4S\n","filename":"src\/hotspot\/cpu\/aarch64\/assembler_aarch64.hpp","additions":8,"deletions":0,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -47,1 +47,1 @@\n-  do_arch_blob(compiler, 35000 ZGC_ONLY(+5000))                         \\\n+  do_arch_blob(compiler, 55000 ZGC_ONLY(+5000))                         \\\n","filename":"src\/hotspot\/cpu\/aarch64\/stubDeclarations_aarch64.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -4066,0 +4066,87 @@\n+  \/\/ Execute one round of keccak of two computations in parallel.\n+  \/\/ One of the states should be loaded into the lower halves of\n+  \/\/ the vector registers v0-v24, the other should be loaded into\n+  \/\/ the upper halves of those registers. The ld1r instruction loads\n+  \/\/ the round constant into both halves.\n+  \/\/ All vector instructions that are used operate on both register\n+  \/\/ halves in parallel.\n+  \/\/ If only a single computation is needed, one can only load the lower halves.\n+  void keccak_round(Register rscratch1) {\n+    __ eor3(v29, __ T16B, v4, v9, v14);\n+    __ eor3(v26, __ T16B, v1, v6, v11);\n+    __ eor3(v28, __ T16B, v3, v8, v13);\n+    __ eor3(v25, __ T16B, v0, v5, v10);\n+    __ eor3(v27, __ T16B, v2, v7, v12);\n+    __ eor3(v29, __ T16B, v29, v19, v24);\n+    __ eor3(v26, __ T16B, v26, v16, v21);\n+    __ eor3(v28, __ T16B, v28, v18, v23);\n+    __ eor3(v25, __ T16B, v25, v15, v20);\n+    __ eor3(v27, __ T16B, v27, v17, v22);\n+\n+    __ rax1(v30, __ T2D, v29, v26);\n+    __ rax1(v26, __ T2D, v26, v28);\n+    __ rax1(v28, __ T2D, v28, v25);\n+    __ rax1(v25, __ T2D, v25, v27);\n+    __ rax1(v27, __ T2D, v27, v29);\n+\n+    __ eor(v0, __ T16B, v0, v30);\n+    __ xar(v29, __ T2D, v1,  v25, (64 - 1));\n+    __ xar(v1,  __ T2D, v6,  v25, (64 - 44));\n+    __ xar(v6,  __ T2D, v9,  v28, (64 - 20));\n+    __ xar(v9,  __ T2D, v22, v26, (64 - 61));\n+    __ xar(v22, __ T2D, v14, v28, (64 - 39));\n+    __ xar(v14, __ T2D, v20, v30, (64 - 18));\n+    __ xar(v31, __ T2D, v2,  v26, (64 - 62));\n+    __ xar(v2,  __ T2D, v12, v26, (64 - 43));\n+    __ xar(v12, __ T2D, v13, v27, (64 - 25));\n+    __ xar(v13, __ T2D, v19, v28, (64 - 8));\n+    __ xar(v19, __ T2D, v23, v27, (64 - 56));\n+    __ xar(v23, __ T2D, v15, v30, (64 - 41));\n+    __ xar(v15, __ T2D, v4,  v28, (64 - 27));\n+    __ xar(v28, __ T2D, v24, v28, (64 - 14));\n+    __ xar(v24, __ T2D, v21, v25, (64 - 2));\n+    __ xar(v8,  __ T2D, v8,  v27, (64 - 55));\n+    __ xar(v4,  __ T2D, v16, v25, (64 - 45));\n+    __ xar(v16, __ T2D, v5,  v30, (64 - 36));\n+    __ xar(v5,  __ T2D, v3,  v27, (64 - 28));\n+    __ xar(v27, __ T2D, v18, v27, (64 - 21));\n+    __ xar(v3,  __ T2D, v17, v26, (64 - 15));\n+    __ xar(v25, __ T2D, v11, v25, (64 - 10));\n+    __ xar(v26, __ T2D, v7,  v26, (64 - 6));\n+    __ xar(v30, __ T2D, v10, v30, (64 - 3));\n+\n+    __ bcax(v20, __ T16B, v31, v22, v8);\n+    __ bcax(v21, __ T16B, v8,  v23, v22);\n+    __ bcax(v22, __ T16B, v22, v24, v23);\n+    __ bcax(v23, __ T16B, v23, v31, v24);\n+    __ bcax(v24, __ T16B, v24, v8,  v31);\n+\n+    __ ld1r(v31, __ T2D, __ post(rscratch1, 8));\n+\n+    __ bcax(v17, __ T16B, v25, v19, v3);\n+    __ bcax(v18, __ T16B, v3,  v15, v19);\n+    __ bcax(v19, __ T16B, v19, v16, v15);\n+    __ bcax(v15, __ T16B, v15, v25, v16);\n+    __ bcax(v16, __ T16B, v16, v3,  v25);\n+\n+    __ bcax(v10, __ T16B, v29, v12, v26);\n+    __ bcax(v11, __ T16B, v26, v13, v12);\n+    __ bcax(v12, __ T16B, v12, v14, v13);\n+    __ bcax(v13, __ T16B, v13, v29, v14);\n+    __ bcax(v14, __ T16B, v14, v26, v29);\n+\n+    __ bcax(v7, __ T16B, v30, v9,  v4);\n+    __ bcax(v8, __ T16B, v4,  v5,  v9);\n+    __ bcax(v9, __ T16B, v9,  v6,  v5);\n+    __ bcax(v5, __ T16B, v5,  v30, v6);\n+    __ bcax(v6, __ T16B, v6,  v4,  v30);\n+\n+    __ bcax(v3, __ T16B, v27, v0,  v28);\n+    __ bcax(v4, __ T16B, v28, v1,  v0);\n+    __ bcax(v0, __ T16B, v0,  v2,  v1);\n+    __ bcax(v1, __ T16B, v1,  v27, v2);\n+    __ bcax(v2, __ T16B, v2,  v28, v27);\n+\n+    __ eor(v0, __ T16B, v0, v31);\n+  }\n+\n@@ -4170,1 +4257,1 @@\n-    \/\/ block_size == 144, bit5 == 0, SHA3-244\n+    \/\/ block_size == 144, bit5 == 0, SHA3-224\n@@ -4199,76 +4286,1 @@\n-    __ eor3(v29, __ T16B, v4, v9, v14);\n-    __ eor3(v26, __ T16B, v1, v6, v11);\n-    __ eor3(v28, __ T16B, v3, v8, v13);\n-    __ eor3(v25, __ T16B, v0, v5, v10);\n-    __ eor3(v27, __ T16B, v2, v7, v12);\n-    __ eor3(v29, __ T16B, v29, v19, v24);\n-    __ eor3(v26, __ T16B, v26, v16, v21);\n-    __ eor3(v28, __ T16B, v28, v18, v23);\n-    __ eor3(v25, __ T16B, v25, v15, v20);\n-    __ eor3(v27, __ T16B, v27, v17, v22);\n-\n-    __ rax1(v30, __ T2D, v29, v26);\n-    __ rax1(v26, __ T2D, v26, v28);\n-    __ rax1(v28, __ T2D, v28, v25);\n-    __ rax1(v25, __ T2D, v25, v27);\n-    __ rax1(v27, __ T2D, v27, v29);\n-\n-    __ eor(v0, __ T16B, v0, v30);\n-    __ xar(v29, __ T2D, v1,  v25, (64 - 1));\n-    __ xar(v1,  __ T2D, v6,  v25, (64 - 44));\n-    __ xar(v6,  __ T2D, v9,  v28, (64 - 20));\n-    __ xar(v9,  __ T2D, v22, v26, (64 - 61));\n-    __ xar(v22, __ T2D, v14, v28, (64 - 39));\n-    __ xar(v14, __ T2D, v20, v30, (64 - 18));\n-    __ xar(v31, __ T2D, v2,  v26, (64 - 62));\n-    __ xar(v2,  __ T2D, v12, v26, (64 - 43));\n-    __ xar(v12, __ T2D, v13, v27, (64 - 25));\n-    __ xar(v13, __ T2D, v19, v28, (64 - 8));\n-    __ xar(v19, __ T2D, v23, v27, (64 - 56));\n-    __ xar(v23, __ T2D, v15, v30, (64 - 41));\n-    __ xar(v15, __ T2D, v4,  v28, (64 - 27));\n-    __ xar(v28, __ T2D, v24, v28, (64 - 14));\n-    __ xar(v24, __ T2D, v21, v25, (64 - 2));\n-    __ xar(v8,  __ T2D, v8,  v27, (64 - 55));\n-    __ xar(v4,  __ T2D, v16, v25, (64 - 45));\n-    __ xar(v16, __ T2D, v5,  v30, (64 - 36));\n-    __ xar(v5,  __ T2D, v3,  v27, (64 - 28));\n-    __ xar(v27, __ T2D, v18, v27, (64 - 21));\n-    __ xar(v3,  __ T2D, v17, v26, (64 - 15));\n-    __ xar(v25, __ T2D, v11, v25, (64 - 10));\n-    __ xar(v26, __ T2D, v7,  v26, (64 - 6));\n-    __ xar(v30, __ T2D, v10, v30, (64 - 3));\n-\n-    __ bcax(v20, __ T16B, v31, v22, v8);\n-    __ bcax(v21, __ T16B, v8,  v23, v22);\n-    __ bcax(v22, __ T16B, v22, v24, v23);\n-    __ bcax(v23, __ T16B, v23, v31, v24);\n-    __ bcax(v24, __ T16B, v24, v8,  v31);\n-\n-    __ ld1r(v31, __ T2D, __ post(rscratch1, 8));\n-\n-    __ bcax(v17, __ T16B, v25, v19, v3);\n-    __ bcax(v18, __ T16B, v3,  v15, v19);\n-    __ bcax(v19, __ T16B, v19, v16, v15);\n-    __ bcax(v15, __ T16B, v15, v25, v16);\n-    __ bcax(v16, __ T16B, v16, v3,  v25);\n-\n-    __ bcax(v10, __ T16B, v29, v12, v26);\n-    __ bcax(v11, __ T16B, v26, v13, v12);\n-    __ bcax(v12, __ T16B, v12, v14, v13);\n-    __ bcax(v13, __ T16B, v13, v29, v14);\n-    __ bcax(v14, __ T16B, v14, v26, v29);\n-\n-    __ bcax(v7, __ T16B, v30, v9,  v4);\n-    __ bcax(v8, __ T16B, v4,  v5,  v9);\n-    __ bcax(v9, __ T16B, v9,  v6,  v5);\n-    __ bcax(v5, __ T16B, v5,  v30, v6);\n-    __ bcax(v6, __ T16B, v6,  v4,  v30);\n-\n-    __ bcax(v3, __ T16B, v27, v0,  v28);\n-    __ bcax(v4, __ T16B, v28, v1,  v0);\n-    __ bcax(v0, __ T16B, v0,  v2,  v1);\n-    __ bcax(v1, __ T16B, v1,  v27, v2);\n-    __ bcax(v2, __ T16B, v2,  v28, v27);\n-\n-    __ eor(v0, __ T16B, v0, v31);\n+    keccak_round(rscratch1);\n@@ -4293,0 +4305,1 @@\n+    \/\/ restore callee-saved registers\n@@ -4303,0 +4316,90 @@\n+  \/\/ Inputs:\n+  \/\/   c_rarg0   - long[]  state0\n+  \/\/   c_rarg1   - long[]  state1\n+  address generate_double_keccak() {\n+    static const uint64_t round_consts[24] = {\n+      0x0000000000000001L, 0x0000000000008082L, 0x800000000000808AL,\n+      0x8000000080008000L, 0x000000000000808BL, 0x0000000080000001L,\n+      0x8000000080008081L, 0x8000000000008009L, 0x000000000000008AL,\n+      0x0000000000000088L, 0x0000000080008009L, 0x000000008000000AL,\n+      0x000000008000808BL, 0x800000000000008BL, 0x8000000000008089L,\n+      0x8000000000008003L, 0x8000000000008002L, 0x8000000000000080L,\n+      0x000000000000800AL, 0x800000008000000AL, 0x8000000080008081L,\n+      0x8000000000008080L, 0x0000000080000001L, 0x8000000080008008L\n+    };\n+\n+    \/\/ Implements the double_keccak() method of the\n+    \/\/ sun.secyrity.provider.SHA3Parallel class\n+    __ align(CodeEntryAlignment);\n+    StubCodeMark mark(this, \"StubRoutines\", \"double_keccak\");\n+    address start = __ pc();\n+    __ enter();\n+\n+    Register state0        = c_rarg0;\n+    Register state1        = c_rarg1;\n+\n+    Label rounds24_loop;\n+\n+    \/\/ save callee-saved registers\n+    __ stpd(v8, v9, __ pre(sp, -64));\n+    __ stpd(v10, v11, Address(sp, 16));\n+    __ stpd(v12, v13, Address(sp, 32));\n+    __ stpd(v14, v15, Address(sp, 48));\n+\n+    \/\/ load states\n+    __ add(rscratch1, state0, 32);\n+    __ ld4(v0, v1, v2,  v3, __ D, 0,  state0);\n+    __ ld4(v4, v5, v6,  v7, __ D, 0, __ post(rscratch1, 32));\n+    __ ld4(v8, v9, v10, v11, __ D, 0, __ post(rscratch1, 32));\n+    __ ld4(v12, v13, v14, v15, __ D, 0, __ post(rscratch1, 32));\n+    __ ld4(v16, v17, v18, v19, __ D, 0, __ post(rscratch1, 32));\n+    __ ld4(v20, v21, v22, v23, __ D, 0, __ post(rscratch1, 32));\n+    __ ld1(v24, __ D, 0, rscratch1);\n+    __ add(rscratch1, state1, 32);\n+    __ ld4(v0, v1, v2,  v3,  __ D, 1, state1);\n+    __ ld4(v4, v5, v6,  v7, __ D, 1, __ post(rscratch1, 32));\n+    __ ld4(v8, v9, v10, v11, __ D, 1, __ post(rscratch1, 32));\n+    __ ld4(v12, v13, v14, v15, __ D, 1, __ post(rscratch1, 32));\n+    __ ld4(v16, v17, v18, v19, __ D, 1, __ post(rscratch1, 32));\n+    __ ld4(v20, v21, v22, v23, __ D, 1, __ post(rscratch1, 32));\n+    __ ld1(v24, __ D, 1, rscratch1);\n+\n+    \/\/ 24 keccak rounds\n+    __ movw(rscratch2, 24);\n+\n+    \/\/ load round_constants base\n+    __ lea(rscratch1, ExternalAddress((address) round_consts));\n+\n+    __ BIND(rounds24_loop);\n+    __ subw(rscratch2, rscratch2, 1);\n+    keccak_round(rscratch1);\n+    __ cbnzw(rscratch2, rounds24_loop);\n+\n+    __ st4(v0, v1, v2,  v3,  __ D, 0, __ post(state0, 32));\n+    __ st4(v4, v5, v6,  v7,  __ D, 0, __ post(state0, 32));\n+    __ st4(v8, v9, v10, v11, __ D, 0, __ post(state0, 32));\n+    __ st4(v12, v13, v14, v15, __ D, 0, __ post(state0, 32));\n+    __ st4(v16, v17, v18, v19, __ D, 0, __ post(state0, 32));\n+    __ st4(v20, v21, v22, v23, __ D, 0, __ post(state0, 32));\n+    __ st1(v24, __ D, 0, state0);\n+    __ st4(v0, v1, v2,  v3,  __ D, 1, __ post(state1, 32));\n+    __ st4(v4, v5, v6,  v7, __ D, 1, __ post(state1, 32));\n+    __ st4(v8, v9, v10, v11, __ D, 1, __ post(state1, 32));\n+    __ st4(v12, v13, v14, v15, __ D, 1, __ post(state1, 32));\n+    __ st4(v16, v17, v18, v19, __ D, 1, __ post(state1, 32));\n+    __ st4(v20, v21, v22, v23, __ D, 1, __ post(state1, 32));\n+    __ st1(v24, __ D, 1, state1);\n+\n+    \/\/ restore callee-saved vector registers\n+    __ ldpd(v14, v15, Address(sp, 48));\n+    __ ldpd(v12, v13, Address(sp, 32));\n+    __ ldpd(v10, v11, Address(sp, 16));\n+    __ ldpd(v8, v9, __ post(sp, 64));\n+\n+    __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n+    __ mov(r0, zr); \/\/ return 0\n+    __ ret(lr);\n+\n+    return start;\n+  }\n+\n@@ -4541,0 +4644,1527 @@\n+  void kyber_load64coeffs(int d0, Register tmpAddr) {\n+    __ ldpq(as_FloatRegister(d0), as_FloatRegister(d0 + 1), __ post(tmpAddr, 32));\n+    __ ldpq(as_FloatRegister(d0 + 2), as_FloatRegister(d0 + 3), __ post(tmpAddr, 32));\n+    __ ldpq(as_FloatRegister(d0 + 4), as_FloatRegister(d0 + 5), __ post(tmpAddr, 32));\n+    __ ldpq(as_FloatRegister(d0 + 6), as_FloatRegister(d0 + 7), tmpAddr);\n+  }\n+\n+  void kyber_load64zetas(Register zetas) {\n+    __ ldpq(v16, v17, __ post(zetas, 32));\n+    __ ldpq(v18, v19, __ post(zetas, 32));\n+    __ ldpq(v20, v21, __ post(zetas, 32));\n+    __ ldpq(v22, v23, __ post(zetas, 32));\n+  }\n+\n+  void kyber_montmul64(bool by_constant) {\n+    int vr16 = 16, vr17 = 17, vr18 = 18, vr19 = 19;\n+    int vr20 = 20, vr21 = 21, vr22 = 22, vr23 = 23;\n+    if (by_constant) {\n+      vr16 = vr17 = vr18 = vr19 = vr20 = vr21 = vr22 = vr23 = 29;\n+    }\n+    __ sqdmulh(v24, __ T8H, v0, as_FloatRegister(vr16));\n+    __ mulv(v16, __ T8H, v0, as_FloatRegister(vr16));\n+    __ sqdmulh(v25, __ T8H, v1, as_FloatRegister(vr17));\n+    __ mulv(v17, __ T8H, v1, as_FloatRegister(vr17));\n+    __ sqdmulh(v26, __ T8H, v2, as_FloatRegister(vr18));\n+    __ mulv(v18, __ T8H, v2, as_FloatRegister(vr18));\n+    __ sqdmulh(v27, __ T8H, v3, as_FloatRegister(vr19));\n+    __ mulv(v19, __ T8H, v3, as_FloatRegister(vr19));\n+    __ mulv(v16, __ T8H, v16, v30);\n+    __ mulv(v17, __ T8H, v17, v30);\n+    __ mulv(v18, __ T8H, v18, v30);\n+    __ mulv(v19, __ T8H, v19, v30);\n+    __ sqdmulh(v16, __ T8H, v16, v31);\n+    __ sqdmulh(v17, __ T8H, v17, v31);\n+    __ sqdmulh(v18, __ T8H, v18, v31);\n+    __ sqdmulh(v19, __ T8H, v19, v31);\n+    __ shsubv(v16, __ T8H, v24, v16);\n+    __ shsubv(v17, __ T8H, v25, v17);\n+    __ shsubv(v18, __ T8H, v26, v18);\n+    __ shsubv(v19, __ T8H, v27, v19);\n+    __ sqdmulh(v24, __ T8H, v4, as_FloatRegister(vr20));\n+    __ mulv(v20, __ T8H, v4, as_FloatRegister(vr20));\n+    __ sqdmulh(v25, __ T8H, v5, as_FloatRegister(vr21));\n+    __ mulv(v21, __ T8H, v5, as_FloatRegister(vr21));\n+    __ sqdmulh(v26, __ T8H, v6, as_FloatRegister(vr22));\n+    __ mulv(v22, __ T8H, v6, as_FloatRegister(vr22));\n+    __ sqdmulh(v27, __ T8H, v7, as_FloatRegister(vr23));\n+    __ mulv(v23, __ T8H, v7, as_FloatRegister(vr23));\n+    __ mulv(v20, __ T8H, v20, v30);\n+    __ mulv(v21, __ T8H, v21, v30);\n+    __ mulv(v22, __ T8H, v22, v30);\n+    __ mulv(v23, __ T8H, v23, v30);\n+    __ sqdmulh(v20, __ T8H, v20, v31);\n+    __ sqdmulh(v21, __ T8H, v21, v31);\n+    __ sqdmulh(v22, __ T8H, v22, v31);\n+    __ sqdmulh(v23, __ T8H, v23, v31);\n+    __ shsubv(v20, __ T8H, v24, v20);\n+    __ shsubv(v21, __ T8H, v25, v21);\n+    __ shsubv(v22, __ T8H, v26, v22);\n+    __ shsubv(v23, __ T8H, v27, v23);\n+  }\n+\n+  void kyber_subv_addv64() {\n+    __ subv(v24, __ T8H, v0, v16);\n+    __ subv(v25, __ T8H, v1, v17);\n+    __ subv(v26, __ T8H, v2, v18);\n+    __ subv(v27, __ T8H, v3, v19);\n+    __ subv(v28, __ T8H, v4, v20);\n+    __ subv(v29, __ T8H, v5, v21);\n+    __ subv(v30, __ T8H, v6, v22);\n+    __ subv(v31, __ T8H, v7, v23);\n+    __ addv(v0, __ T8H, v0, v16);\n+    __ addv(v1, __ T8H, v1, v17);\n+    __ addv(v2, __ T8H, v2, v18);\n+    __ addv(v3, __ T8H, v3, v19);\n+    __ addv(v4, __ T8H, v4, v20);\n+    __ addv(v5, __ T8H, v5, v21);\n+    __ addv(v6, __ T8H, v6, v22);\n+    __ addv(v7, __ T8H, v7, v23);\n+  }\n+\n+  void kyber_store64coeffs(int i0, Register tmpAddr) {\n+    __ stpq(as_FloatRegister(i0), as_FloatRegister(i0 + 1), __ post(tmpAddr, 32));\n+    __ stpq(as_FloatRegister(i0 + 2), as_FloatRegister(i0 + 3), __ post(tmpAddr, 32));\n+    __ stpq(as_FloatRegister(i0 + 4), as_FloatRegister(i0 + 5), __ post(tmpAddr, 32));\n+    __ stpq(as_FloatRegister(i0 + 6), as_FloatRegister(i0 + 7), __ post(tmpAddr, 32));\n+  }\n+\n+  void kyber_montmul_subv_addv32() {\n+    __ sqdmulh(v24, __ T8H, v1, v16);\n+    __ mulv(v16, __ T8H, v1, v16);\n+    __ sqdmulh(v25, __ T8H, v3, v17);\n+    __ mulv(v17, __ T8H, v3, v17);\n+    __ sqdmulh(v26, __ T8H, v5, v18);\n+    __ mulv(v18, __ T8H, v5, v18);\n+    __ sqdmulh(v27, __ T8H, v7, v19);\n+    __ mulv(v19, __ T8H, v7, v19);\n+    __ mulv(v16, __ T8H, v16, v30);\n+    __ mulv(v17, __ T8H, v17, v30);\n+    __ mulv(v18, __ T8H, v18, v30);\n+    __ mulv(v19, __ T8H, v19, v30);\n+    __ sqdmulh(v16, __ T8H, v16, v31);\n+    __ sqdmulh(v17, __ T8H, v17, v31);\n+    __ sqdmulh(v18, __ T8H, v18, v31);\n+    __ sqdmulh(v19, __ T8H, v19, v31);\n+    __ shsubv(v16, __ T8H, v24, v16);\n+    __ shsubv(v17, __ T8H, v25, v17);\n+    __ shsubv(v18, __ T8H, v26, v18);\n+    __ shsubv(v19, __ T8H, v27, v19);\n+    __ subv(v1, __ T8H, v0, v16);\n+    __ subv(v3, __ T8H, v2, v17);\n+    __ subv(v5, __ T8H, v4, v18);\n+    __ subv(v7, __ T8H, v6, v19);\n+    __ addv(v0, __ T8H, v0, v16);\n+    __ addv(v2, __ T8H, v2, v17);\n+    __ addv(v4, __ T8H, v4, v18);\n+    __ addv(v6, __ T8H, v6, v19);\n+  }\n+\n+  void kyber_subv_addv_montmul32() {\n+    __ subv(v20, __ T8H, v0, v1);\n+    __ subv(v21, __ T8H, v2, v3);\n+    __ subv(v22, __ T8H, v4, v5);\n+    __ subv(v23, __ T8H, v6, v7);\n+    __ addv(v0, __ T8H, v0, v1);\n+    __ addv(v2, __ T8H, v2, v3);\n+    __ addv(v4, __ T8H, v4, v5);\n+    __ addv(v6, __ T8H, v6, v7);\n+    __ sqdmulh(v24, __ T8H, v20, v16);\n+    __ mulv(v1, __ T8H, v20, v16);\n+    __ sqdmulh(v25, __ T8H, v21, v17);\n+    __ mulv(v3, __ T8H, v21, v17);\n+    __ sqdmulh(v26, __ T8H, v22, v18);\n+    __ mulv(v5, __ T8H, v22, v18);\n+    __ sqdmulh(v27, __ T8H, v23, v19);\n+    __ mulv(v7, __ T8H, v23, v19);\n+    __ mulv(v1, __ T8H, v1, v30);\n+    __ mulv(v3, __ T8H, v3, v30);\n+    __ mulv(v5, __ T8H, v5, v30);\n+    __ mulv(v7, __ T8H, v7, v30);\n+    __ sqdmulh(v1, __ T8H, v1, v31);\n+    __ sqdmulh(v3, __ T8H, v3, v31);\n+    __ sqdmulh(v5, __ T8H, v5, v31);\n+    __ sqdmulh(v7, __ T8H, v7, v31);\n+    __ shsubv(v1, __ T8H, v24, v1);\n+    __ shsubv(v3, __ T8H, v25, v3);\n+    __ shsubv(v5, __ T8H, v26, v5);\n+    __ shsubv(v7, __ T8H, v27, v7);\n+  }\n+\n+  void kyber_addv_subv64() {\n+    __ addv(v24, __ T8H, v0, v16);\n+    __ addv(v25, __ T8H, v1, v17);\n+    __ addv(v26, __ T8H, v2, v18);\n+    __ addv(v27, __ T8H, v3, v19);\n+    __ addv(v28, __ T8H, v4, v20);\n+    __ addv(v29, __ T8H, v5, v21);\n+    __ addv(v30, __ T8H, v6, v22);\n+    __ addv(v31, __ T8H, v7, v23);\n+    __ subv(v0, __ T8H, v0, v16);\n+    __ subv(v1, __ T8H, v1, v17);\n+    __ subv(v2, __ T8H, v2, v18);\n+    __ subv(v3, __ T8H, v3, v19);\n+    __ subv(v4, __ T8H, v4, v20);\n+    __ subv(v5, __ T8H, v5, v21);\n+    __ subv(v6, __ T8H, v6, v22);\n+    __ subv(v7, __ T8H, v7, v23);\n+  }\n+\n+  \/\/ Kyber NTT function.\n+  \/\/ Implements\n+  \/\/ static int implKyberNtt(short[] poly, short[] ntt_zetas) {}\n+  \/\/\n+  \/\/ coeffs (short[256]) = c_rarg0\n+  \/\/ ntt_zetas (short[256]) = c_rarg1\n+  address generate_kyberNtt() {\n+\n+    __ align(CodeEntryAlignment);\n+    StubGenStubId stub_id = StubGenStubId::kyberNtt_id;\n+    StubCodeMark mark(this, stub_id);\n+    address start = __ pc();\n+    __ enter();\n+\n+    const Register coeffs = c_rarg0;\n+    const Register zetas = c_rarg1;\n+\n+    const Register kyberConsts = r10;\n+    const Register tmpAddr = r11;\n+\n+    __ lea(kyberConsts, ExternalAddress((address) StubRoutines::aarch64::_kyberConsts));\n+    __ ldpq(v30, v31, kyberConsts);\n+\n+    \/\/ Each level corresponds to an iteration of the outermost loop of the\n+    \/\/ Java method seilerNTT(int[] coeffs). There are some differences\n+    \/\/ from what is done in the seilerNTT() method, though:\n+    \/\/ 1. The computation is using 16-bit signed values, we do not convert them\n+    \/\/ to ints here.\n+    \/\/ 2. The zetas are delivered in a bigger array, 128 zetas are stored in\n+    \/\/ this array for each level, it is easier that way to fill up the vector\n+    \/\/ registers.\n+    \/\/ 3. In the seilerNTT() method we use R = 2^20 for the Montgomery\n+    \/\/ multiplications (this is because that way there should not be any\n+    \/\/ overflow during the inverse NTT computation), here we usr R = 2^16 so\n+    \/\/ that we can use the 16-bit arithmetic in the vector unit.\n+    \/\/\n+    \/\/ On each level, we fill up the vector registers in such a way that the\n+    \/\/ array elements that need to be multiplied by the zetas be in one\n+    \/\/ set of vector registers while the corresponding ones that don't need to\n+    \/\/ be multiplied, in another set. We can do 32 Montgomery multiplications\n+    \/\/ in parallel, using 12 vector registers interleaving the steps of 4\n+    \/\/ identical computations, each done on 8 16-bit values per register.\n+    \/\/ level 0\n+    __ add(tmpAddr, coeffs, 256);\n+    kyber_load64coeffs(0, tmpAddr);\n+    kyber_load64zetas(zetas);\n+    kyber_montmul64(false);\n+    __ add(tmpAddr, coeffs, 0);\n+    kyber_load64coeffs(0, tmpAddr);\n+    kyber_subv_addv64();\n+    __ add(tmpAddr, coeffs, 0);\n+    kyber_store64coeffs(0, tmpAddr);\n+    __ add(tmpAddr, coeffs, 256);\n+    kyber_store64coeffs(24, tmpAddr);\n+    __ ldpq(v30, v31, kyberConsts);\n+    kyber_load64coeffs(0, tmpAddr);\n+    kyber_load64zetas(zetas);\n+    kyber_montmul64(false);\n+    __ add(tmpAddr, coeffs, 128);\n+    kyber_load64coeffs(0, tmpAddr);\n+    kyber_subv_addv64();\n+    __ add(tmpAddr, coeffs, 128);\n+    kyber_store64coeffs(0, tmpAddr);\n+    __ add(tmpAddr, coeffs, 384);\n+    kyber_store64coeffs(24, tmpAddr);\n+    \/\/ level 1\n+    __ ldpq(v30, v31, kyberConsts);\n+    __ add(tmpAddr, coeffs, 128);\n+    kyber_load64coeffs(0, tmpAddr);\n+    kyber_load64zetas(zetas);\n+    kyber_montmul64(false);\n+    __ add(tmpAddr, coeffs, 0);\n+    kyber_load64coeffs(0, tmpAddr);\n+    kyber_subv_addv64();\n+    __ add(tmpAddr, coeffs, 0);\n+    kyber_store64coeffs(0, tmpAddr);\n+    kyber_store64coeffs(24, tmpAddr);\n+    __ ldpq(v30, v31, kyberConsts);\n+    __ add(tmpAddr, coeffs, 384);\n+    kyber_load64coeffs(0, tmpAddr);\n+    kyber_load64zetas(zetas);\n+    kyber_montmul64(false);\n+    __ add(tmpAddr, coeffs, 256);\n+    kyber_load64coeffs(0, tmpAddr);\n+    kyber_subv_addv64();\n+    __ add(tmpAddr, coeffs, 256);\n+    kyber_store64coeffs(0, tmpAddr);\n+    kyber_store64coeffs(24, tmpAddr);\n+    \/\/ level 2\n+    __ ldpq(v30, v31, kyberConsts);\n+    __ add(tmpAddr, coeffs, 64);\n+    __ ldpq(v0, v1, __ post(tmpAddr, 32));\n+    __ ldpq(v2, v3, __ post(tmpAddr, 96));\n+    __ ldpq(v4, v5, __ post(tmpAddr, 32));\n+    __ ldpq(v6, v7, tmpAddr);\n+    kyber_load64zetas(zetas);\n+    kyber_montmul64(false);\n+    __ add(tmpAddr, coeffs, 0);\n+    __ ldpq(v0, v1, __ post(tmpAddr, 32));\n+    __ ldpq(v2, v3, __ post(tmpAddr, 96));\n+    __ ldpq(v4, v5, __ post(tmpAddr, 32));\n+    __ ldpq(v6, v7, tmpAddr);\n+    kyber_subv_addv64();\n+    __ add(tmpAddr, coeffs, 0);\n+    __ stpq(v0, v1, __ post(tmpAddr, 32));\n+    __ stpq(v2, v3, __ post(tmpAddr, 32));\n+    __ stpq(v24, v25, __ post(tmpAddr, 32));\n+    __ stpq(v26, v27, __ post(tmpAddr, 32));\n+    __ stpq(v4, v5, __ post(tmpAddr, 32));\n+    __ stpq(v6, v7, __ post(tmpAddr, 32));\n+    __ stpq(v28, v29, __ post(tmpAddr, 32));\n+    __ stpq(v30, v31, __ post(tmpAddr, 96));\n+    __ ldpq(v30, v31, kyberConsts);\n+    __ ldpq(v0, v1, __ post(tmpAddr, 32));\n+    __ ldpq(v2, v3, __ post(tmpAddr, 96));\n+    __ ldpq(v4, v5, __ post(tmpAddr, 32));\n+    __ ldpq(v6, v7, tmpAddr);\n+    kyber_load64zetas(zetas);\n+    kyber_montmul64(false);\n+    __ add(tmpAddr, coeffs, 256);\n+    __ ldpq(v0, v1, __ post(tmpAddr, 32));\n+    __ ldpq(v2, v3, __ post(tmpAddr, 96));\n+    __ ldpq(v4, v5, __ post(tmpAddr, 32));\n+    __ ldpq(v6, v7, tmpAddr);\n+    kyber_subv_addv64();\n+    __ add(tmpAddr, coeffs, 256);\n+    __ stpq(v0, v1, __ post(tmpAddr, 32));\n+    __ stpq(v2, v3, __ post(tmpAddr, 32));\n+    __ stpq(v24, v25, __ post(tmpAddr, 32));\n+    __ stpq(v26, v27, __ post(tmpAddr, 32));\n+    __ stpq(v4, v5, __ post(tmpAddr, 32));\n+    __ stpq(v6, v7, __ post(tmpAddr, 32));\n+    __ stpq(v28, v29, __ post(tmpAddr, 32));\n+    __ stpq(v30, v31, tmpAddr);\n+    \/\/ level 3\n+    __ ldpq(v30, v31, kyberConsts);\n+    __ add(tmpAddr, coeffs, 32);\n+    __ ldpq(v0, v1, __ post(tmpAddr, 64));\n+    __ ldpq(v2, v3, __ post(tmpAddr, 64));\n+    __ ldpq(v4, v5, __ post(tmpAddr, 64));\n+    __ ldpq(v6, v7, tmpAddr);\n+    kyber_load64zetas(zetas);\n+    kyber_montmul64(false);\n+    __ add(tmpAddr, coeffs, 0);\n+    __ ldpq(v0, v1, __ post(tmpAddr, 64));\n+    __ ldpq(v2, v3, __ post(tmpAddr, 64));\n+    __ ldpq(v4, v5, __ post(tmpAddr, 64));\n+    __ ldpq(v6, v7, tmpAddr);\n+    kyber_subv_addv64();\n+    __ add(tmpAddr, coeffs, 0);\n+    __ stpq(v0, v1, __ post(tmpAddr, 32));\n+    __ stpq(v24, v25, __ post(tmpAddr, 32));\n+    __ stpq(v2, v3, __ post(tmpAddr, 32));\n+    __ stpq(v26, v27, __ post(tmpAddr, 32));\n+    __ stpq(v4, v5, __ post(tmpAddr, 32));\n+    __ stpq(v28, v29, __ post(tmpAddr, 32));\n+    __ stpq(v6, v7, __ post(tmpAddr, 32));\n+    __ stpq(v30, v31, __ post(tmpAddr, 64));\n+\n+    __ ldpq(v30, v31, kyberConsts);\n+    __ ldpq(v0, v1, __ post(tmpAddr, 64));\n+    __ ldpq(v2, v3, __ post(tmpAddr, 64));\n+    __ ldpq(v4, v5, __ post(tmpAddr, 64));\n+    __ ldpq(v6, v7, tmpAddr);\n+    kyber_load64zetas(zetas);\n+    kyber_montmul64(false);\n+    __ add(tmpAddr, coeffs, 256);\n+    __ ldpq(v0, v1, __ post(tmpAddr, 64));\n+    __ ldpq(v2, v3, __ post(tmpAddr, 64));\n+    __ ldpq(v4, v5, __ post(tmpAddr, 64));\n+    __ ldpq(v6, v7, tmpAddr);\n+    kyber_subv_addv64();\n+    __ add(tmpAddr, coeffs, 256);\n+    __ stpq(v0, v1, __ post(tmpAddr, 32));\n+    __ stpq(v24, v25, __ post(tmpAddr, 32));\n+    __ stpq(v2, v3, __ post(tmpAddr, 32));\n+    __ stpq(v26, v27, __ post(tmpAddr, 32));\n+    __ stpq(v4, v5, __ post(tmpAddr, 32));\n+    __ stpq(v28, v29, __ post(tmpAddr, 32));\n+    __ stpq(v6, v7, __ post(tmpAddr, 32));\n+    __ stpq(v30, v31, tmpAddr);\n+    \/\/ level 4\n+    __ ldpq(v30, v31, kyberConsts);\n+    __ add(tmpAddr, coeffs, 16);\n+    __ ldr(v0, __ Q, __ post(tmpAddr, 32));\n+    __ ldr(v1, __ Q, __ post(tmpAddr, 32));\n+    __ ldr(v2, __ Q, __ post(tmpAddr, 32));\n+    __ ldr(v3, __ Q, __ post(tmpAddr, 32));\n+    __ ldr(v4, __ Q, __ post(tmpAddr, 32));\n+    __ ldr(v5, __ Q, __ post(tmpAddr, 32));\n+    __ ldr(v6, __ Q, __ post(tmpAddr, 32));\n+    __ ldr(v7, __ Q, tmpAddr);\n+    kyber_load64zetas(zetas);\n+    kyber_montmul64(false);\n+    __ add(tmpAddr, coeffs, 0);\n+    __ ldr(v0, __ Q, __ post(tmpAddr, 32));\n+    __ ldr(v1, __ Q, __ post(tmpAddr, 32));\n+    __ ldr(v2, __ Q, __ post(tmpAddr, 32));\n+    __ ldr(v3, __ Q, __ post(tmpAddr, 32));\n+    __ ldr(v4, __ Q, __ post(tmpAddr, 32));\n+    __ ldr(v5, __ Q, __ post(tmpAddr, 32));\n+    __ ldr(v6, __ Q, __ post(tmpAddr, 32));\n+    __ ldr(v7, __ Q, tmpAddr);\n+    kyber_subv_addv64();\n+    __ add(tmpAddr, coeffs, 0);\n+    __ str(v0, __ Q, __ post(tmpAddr, 16));\n+    __ str(v24, __ Q, __ post(tmpAddr, 16));\n+    __ str(v1, __ Q, __ post(tmpAddr, 16));\n+    __ str(v25, __ Q, __ post(tmpAddr, 16));\n+    __ str(v2, __ Q, __ post(tmpAddr, 16));\n+    __ str(v26, __ Q, __ post(tmpAddr, 16));\n+    __ str(v3, __ Q, __ post(tmpAddr, 16));\n+    __ str(v27, __ Q, __ post(tmpAddr, 16));\n+    __ str(v4, __ Q, __ post(tmpAddr, 16));\n+    __ str(v28, __ Q, __ post(tmpAddr, 16));\n+    __ str(v5, __ Q, __ post(tmpAddr, 16));\n+    __ str(v29, __ Q, __ post(tmpAddr, 16));\n+    __ str(v6, __ Q, __ post(tmpAddr, 16));\n+    __ str(v30, __ Q, __ post(tmpAddr, 16));\n+    __ str(v7, __ Q, __ post(tmpAddr, 16));\n+    __ str(v31, __ Q, __ post(tmpAddr, 32));\n+\n+    __ ldpq(v30, v31, kyberConsts);\n+    __ ldr(v0, __ Q, __ post(tmpAddr, 32));\n+    __ ldr(v1, __ Q, __ post(tmpAddr, 32));\n+    __ ldr(v2, __ Q, __ post(tmpAddr, 32));\n+    __ ldr(v3, __ Q, __ post(tmpAddr, 32));\n+    __ ldr(v4, __ Q, __ post(tmpAddr, 32));\n+    __ ldr(v5, __ Q, __ post(tmpAddr, 32));\n+    __ ldr(v6, __ Q, __ post(tmpAddr, 32));\n+    __ ldr(v7, __ Q, tmpAddr);\n+    kyber_load64zetas(zetas);\n+    kyber_montmul64(false);\n+    __ add(tmpAddr, coeffs, 256);\n+    __ ldr(v0, __ Q, __ post(tmpAddr, 32));\n+    __ ldr(v1, __ Q, __ post(tmpAddr, 32));\n+    __ ldr(v2, __ Q, __ post(tmpAddr, 32));\n+    __ ldr(v3, __ Q, __ post(tmpAddr, 32));\n+    __ ldr(v4, __ Q, __ post(tmpAddr, 32));\n+    __ ldr(v5, __ Q, __ post(tmpAddr, 32));\n+    __ ldr(v6, __ Q, __ post(tmpAddr, 32));\n+    __ ldr(v7, __ Q, tmpAddr);\n+    kyber_subv_addv64();\n+    __ add(tmpAddr, coeffs, 256);\n+    __ str(v0, __ Q, __ post(tmpAddr, 16));\n+    __ str(v24, __ Q, __ post(tmpAddr, 16));\n+    __ str(v1, __ Q, __ post(tmpAddr, 16));\n+    __ str(v25, __ Q, __ post(tmpAddr, 16));\n+    __ str(v2, __ Q, __ post(tmpAddr, 16));\n+    __ str(v26, __ Q, __ post(tmpAddr, 16));\n+    __ str(v3, __ Q, __ post(tmpAddr, 16));\n+    __ str(v27, __ Q, __ post(tmpAddr, 16));\n+    __ str(v4, __ Q, __ post(tmpAddr, 16));\n+    __ str(v28, __ Q, __ post(tmpAddr, 16));\n+    __ str(v5, __ Q, __ post(tmpAddr, 16));\n+    __ str(v29, __ Q, __ post(tmpAddr, 16));\n+    __ str(v6, __ Q, __ post(tmpAddr, 16));\n+    __ str(v30, __ Q, __ post(tmpAddr, 16));\n+    __ str(v7, __ Q, __ post(tmpAddr, 16));\n+    __ str(v31, __ Q, tmpAddr);\n+    \/\/ level 5\n+    __ ldpq(v30, v31, kyberConsts);\n+    __ add(tmpAddr, coeffs, 0);\n+    __ ld2(v0, v1, __ T2D, __ post(tmpAddr, 32));\n+    __ ld2(v2, v3, __ T2D, __ post(tmpAddr, 32));\n+    __ ld2(v4, v5, __ T2D, __ post(tmpAddr, 32));\n+    __ ld2(v6, v7, __ T2D, tmpAddr);\n+    __ ldpq(v16, v17, __ post(zetas, 32));\n+    __ ldpq(v18, v19, __ post(zetas, 32));\n+    kyber_montmul_subv_addv32();\n+    __ add(tmpAddr, coeffs, 0);\n+    __ st2(v0, v1, __ T2D, __ post(tmpAddr, 32));\n+    __ st2(v2, v3, __ T2D, __ post(tmpAddr, 32));\n+    __ st2(v4, v5, __ T2D, __ post(tmpAddr, 32));\n+    __ st2(v6, v7, __ T2D, __ post(tmpAddr, 32));\n+\n+    __ ld2(v0, v1, __ T2D, __ post(tmpAddr, 32));\n+    __ ld2(v2, v3, __ T2D, __ post(tmpAddr, 32));\n+    __ ld2(v4, v5, __ T2D, __ post(tmpAddr, 32));\n+    __ ld2(v6, v7, __ T2D, tmpAddr);\n+    __ ldpq(v16, v17, __ post(zetas, 32));\n+    __ ldpq(v18, v19, __ post(zetas, 32));\n+    kyber_montmul_subv_addv32();\n+    __ add(tmpAddr, coeffs, 128);\n+    __ st2(v0, v1, __ T2D, __ post(tmpAddr, 32));\n+    __ st2(v2, v3, __ T2D, __ post(tmpAddr, 32));\n+    __ st2(v4, v5, __ T2D, __ post(tmpAddr, 32));\n+    __ st2(v6, v7, __ T2D, __ post(tmpAddr, 32));\n+\n+    __ ld2(v0, v1, __ T2D, __ post(tmpAddr, 32));\n+    __ ld2(v2, v3, __ T2D, __ post(tmpAddr, 32));\n+    __ ld2(v4, v5, __ T2D, __ post(tmpAddr, 32));\n+    __ ld2(v6, v7, __ T2D, tmpAddr);\n+    __ ldpq(v16, v17, __ post(zetas, 32));\n+    __ ldpq(v18, v19, __ post(zetas, 32));\n+    kyber_montmul_subv_addv32();\n+    __ add(tmpAddr, coeffs, 256);\n+    __ st2(v0, v1, __ T2D, __ post(tmpAddr, 32));\n+    __ st2(v2, v3, __ T2D, __ post(tmpAddr, 32));\n+    __ st2(v4, v5, __ T2D, __ post(tmpAddr, 32));\n+    __ st2(v6, v7, __ T2D, __ post(tmpAddr, 32));\n+\n+    __ ld2(v0, v1, __ T2D, __ post(tmpAddr, 32));\n+    __ ld2(v2, v3, __ T2D, __ post(tmpAddr, 32));\n+    __ ld2(v4, v5, __ T2D, __ post(tmpAddr, 32));\n+    __ ld2(v6, v7, __ T2D, tmpAddr);\n+    __ ldpq(v16, v17, __ post(zetas, 32));\n+    __ ldpq(v18, v19, __ post(zetas, 32));\n+    kyber_montmul_subv_addv32();\n+    __ add(tmpAddr, coeffs, 384);\n+    __ st2(v0, v1, __ T2D, __ post(tmpAddr, 32));\n+    __ st2(v2, v3, __ T2D, __ post(tmpAddr, 32));\n+    __ st2(v4, v5, __ T2D, __ post(tmpAddr, 32));\n+    __ st2(v6, v7, __ T2D, tmpAddr);\n+    \/\/ level 6\n+    __ add(tmpAddr, coeffs, 0);\n+    __ ld2(v0, v1, __ T4S, __ post(tmpAddr, 32));\n+    __ ld2(v2, v3, __ T4S, __ post(tmpAddr, 32));\n+    __ ld2(v4, v5, __ T4S, __ post(tmpAddr, 32));\n+    __ ld2(v6, v7, __ T4S, tmpAddr);\n+    __ ldpq(v16, v17, __ post(zetas, 32));\n+    __ ldpq(v18, v19, __ post(zetas, 32));\n+    kyber_montmul_subv_addv32();\n+    __ add(tmpAddr, coeffs, 0);\n+    __ st2(v0, v1, __ T4S, __ post(tmpAddr, 32));\n+    __ st2(v2, v3, __ T4S, __ post(tmpAddr, 32));\n+    __ st2(v4, v5, __ T4S, __ post(tmpAddr, 32));\n+    __ st2(v6, v7, __ T4S, __ post(tmpAddr, 32));\n+\n+    __ ld2(v0, v1, __ T4S, __ post(tmpAddr, 32));\n+    __ ld2(v2, v3, __ T4S, __ post(tmpAddr, 32));\n+    __ ld2(v4, v5, __ T4S, __ post(tmpAddr, 32));\n+    __ ld2(v6, v7, __ T4S, tmpAddr);\n+    __ ldpq(v16, v17, __ post(zetas, 32));\n+    __ ldpq(v18, v19, __ post(zetas, 32));\n+    kyber_montmul_subv_addv32();\n+    __ add(tmpAddr, coeffs, 128);\n+    __ st2(v0, v1, __ T4S, __ post(tmpAddr, 32));\n+    __ st2(v2, v3, __ T4S, __ post(tmpAddr, 32));\n+    __ st2(v4, v5, __ T4S, __ post(tmpAddr, 32));\n+    __ st2(v6, v7, __ T4S, __ post(tmpAddr, 32));\n+\n+    __ ld2(v0, v1, __ T4S, __ post(tmpAddr, 32));\n+    __ ld2(v2, v3, __ T4S, __ post(tmpAddr, 32));\n+    __ ld2(v4, v5, __ T4S, __ post(tmpAddr, 32));\n+    __ ld2(v6, v7, __ T4S, tmpAddr);\n+    __ ldpq(v16, v17, __ post(zetas, 32));\n+    __ ldpq(v18, v19, __ post(zetas, 32));\n+    kyber_montmul_subv_addv32();\n+    __ add(tmpAddr, coeffs, 256);\n+    __ st2(v0, v1, __ T4S, __ post(tmpAddr, 32));\n+    __ st2(v2, v3, __ T4S, __ post(tmpAddr, 32));\n+    __ st2(v4, v5, __ T4S, __ post(tmpAddr, 32));\n+    __ st2(v6, v7, __ T4S, __ post(tmpAddr, 32));\n+\n+    __ ld2(v0, v1, __ T4S, __ post(tmpAddr, 32));\n+    __ ld2(v2, v3, __ T4S, __ post(tmpAddr, 32));\n+    __ ld2(v4, v5, __ T4S, __ post(tmpAddr, 32));\n+    __ ld2(v6, v7, __ T4S, tmpAddr);\n+    __ ldpq(v16, v17, __ post(zetas, 32));\n+    __ ldpq(v18, v19, zetas);\n+    kyber_montmul_subv_addv32();\n+    __ add(tmpAddr, coeffs, 384);\n+    __ st2(v0, v1, __ T4S, __ post(tmpAddr, 32));\n+    __ st2(v2, v3, __ T4S, __ post(tmpAddr, 32));\n+    __ st2(v4, v5, __ T4S, __ post(tmpAddr, 32));\n+    __ st2(v6, v7, __ T4S, tmpAddr);\n+\n+    __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n+    __ mov(r0, zr); \/\/ return 0\n+    __ ret(lr);\n+\n+    return start;\n+  }\n+\n+  void kyber_sqdmulh32(int i0) {\n+    __ sqdmulh(as_FloatRegister(i0 + 18), __ T8H, as_FloatRegister(i0), v31);\n+    __ sqdmulh(as_FloatRegister(i0 + 19), __ T8H, as_FloatRegister(i0 + 1), v31);\n+    __ sqdmulh(as_FloatRegister(i0 + 20), __ T8H, as_FloatRegister(i0 + 2), v31);\n+    __ sqdmulh(as_FloatRegister(i0 + 21), __ T8H, as_FloatRegister(i0 + 3), v31);\n+  }\n+\n+  void kyber_sshr32(int i0) {\n+    __ sshr(as_FloatRegister(i0), __ T8H, as_FloatRegister(i0), 11);\n+    __ sshr(as_FloatRegister(i0 + 1), __ T8H, as_FloatRegister(i0 + 1), 11);\n+    __ sshr(as_FloatRegister(i0 + 2), __ T8H, as_FloatRegister(i0 + 2), 11);\n+    __ sshr(as_FloatRegister(i0 + 3), __ T8H, as_FloatRegister(i0 + 3), 11);\n+  }\n+\n+  void kyber_mlsv32(int o0) {\n+    __ mlsv(as_FloatRegister(o0), __ T8H, as_FloatRegister(o0 + 18), v30);\n+    __ mlsv(as_FloatRegister(o0 + 1), __ T8H, as_FloatRegister(o0 + 19), v30);\n+    __ mlsv(as_FloatRegister(o0 + 2), __ T8H, as_FloatRegister(o0 + 20), v30);\n+    __ mlsv(as_FloatRegister(o0 + 3), __ T8H, as_FloatRegister(o0 + 21), v30);\n+  }\n+\n+  \/\/ Kyber Inverse NTT function\n+  \/\/ Implements\n+  \/\/ static int implKyberInverseNtt(short[] poly, short[] zetas) {}\n+  \/\/\n+  \/\/ coeffs (short[256]) = c_rarg0\n+  \/\/ ntt_zetas (short[256]) = c_rarg1\n+  address generate_kyberInverseNtt() {\n+\n+    __ align(CodeEntryAlignment);\n+    StubGenStubId stub_id = StubGenStubId::kyberInverseNtt_id;\n+    StubCodeMark mark(this, stub_id);\n+    address start = __ pc();\n+    __ enter();\n+\n+    const Register coeffs = c_rarg0;\n+    const Register zetas = c_rarg1;\n+\n+    const Register kyberConsts = r10;\n+    const Register tmpAddr = r11;\n+    const Register tmpAddr2 = c_rarg2;\n+\n+    __ lea(kyberConsts, ExternalAddress((address) StubRoutines::aarch64::_kyberConsts));\n+\n+    \/\/ level 0\n+    __ ldpq(v30, v31, kyberConsts);\n+    __ add(tmpAddr, coeffs, 0);\n+    __ ld2(v0, v1, __ T4S, __ post(tmpAddr, 32));\n+    __ ld2(v2, v3, __ T4S, __ post(tmpAddr, 32));\n+    __ ld2(v4, v5, __ T4S, __ post(tmpAddr, 32));\n+    __ ld2(v6, v7, __ T4S, tmpAddr);\n+    __ ldpq(v16, v17, __ post(zetas, 32));\n+    __ ldpq(v18, v19, __ post(zetas, 32));\n+    kyber_subv_addv_montmul32();\n+    __ add(tmpAddr, coeffs, 0);\n+    __ st2(v0, v1, __ T4S, __ post(tmpAddr, 32));\n+    __ st2(v2, v3, __ T4S, __ post(tmpAddr, 32));\n+    __ st2(v4, v5, __ T4S, __ post(tmpAddr, 32));\n+    __ st2(v6, v7, __ T4S, __ post(tmpAddr, 32));\n+    __ ld2(v0, v1, __ T4S, __ post(tmpAddr, 32));\n+    __ ld2(v2, v3, __ T4S, __ post(tmpAddr, 32));\n+    __ ld2(v4, v5, __ T4S, __ post(tmpAddr, 32));\n+    __ ld2(v6, v7, __ T4S, tmpAddr);\n+    __ ldpq(v16, v17, __ post(zetas, 32));\n+    __ ldpq(v18, v19, __ post(zetas, 32));\n+    kyber_subv_addv_montmul32();\n+    __ add(tmpAddr, coeffs, 128);\n+    __ st2(v0, v1, __ T4S, __ post(tmpAddr, 32));\n+    __ st2(v2, v3, __ T4S, __ post(tmpAddr, 32));\n+    __ st2(v4, v5, __ T4S, __ post(tmpAddr, 32));\n+    __ st2(v6, v7, __ T4S, __ post(tmpAddr, 32));\n+\n+    __ ld2(v0, v1, __ T4S, __ post(tmpAddr, 32));\n+    __ ld2(v2, v3, __ T4S, __ post(tmpAddr, 32));\n+    __ ld2(v4, v5, __ T4S, __ post(tmpAddr, 32));\n+    __ ld2(v6, v7, __ T4S, tmpAddr);\n+    __ ldpq(v16, v17, __ post(zetas, 32));\n+    __ ldpq(v18, v19, __ post(zetas, 32));\n+    kyber_subv_addv_montmul32();\n+    __ add(tmpAddr, coeffs, 256);\n+    __ st2(v0, v1, __ T4S, __ post(tmpAddr, 32));\n+    __ st2(v2, v3, __ T4S, __ post(tmpAddr, 32));\n+    __ st2(v4, v5, __ T4S, __ post(tmpAddr, 32));\n+    __ st2(v6, v7, __ T4S, __ post(tmpAddr, 32));\n+    __ ld2(v0, v1, __ T4S, __ post(tmpAddr, 32));\n+    __ ld2(v2, v3, __ T4S, __ post(tmpAddr, 32));\n+    __ ld2(v4, v5, __ T4S, __ post(tmpAddr, 32));\n+    __ ld2(v6, v7, __ T4S, tmpAddr);\n+    __ ldpq(v16, v17, __ post(zetas, 32));\n+    __ ldpq(v18, v19, __ post(zetas, 32));\n+    kyber_subv_addv_montmul32();\n+    __ add(tmpAddr, coeffs, 384);\n+    __ st2(v0, v1, __ T4S, __ post(tmpAddr, 32));\n+    __ st2(v2, v3, __ T4S, __ post(tmpAddr, 32));\n+    __ st2(v4, v5, __ T4S, __ post(tmpAddr, 32));\n+    __ st2(v6, v7, __ T4S, tmpAddr);\n+    \/\/ level 1\n+    __ add(tmpAddr, coeffs, 0);\n+    __ ld2(v0, v1, __ T2D, __ post(tmpAddr, 32));\n+    __ ld2(v2, v3, __ T2D, __ post(tmpAddr, 32));\n+    __ ld2(v4, v5, __ T2D, __ post(tmpAddr, 32));\n+    __ ld2(v6, v7, __ T2D, tmpAddr);\n+    __ ldpq(v16, v17, __ post(zetas, 32));\n+    __ ldpq(v18, v19, __ post(zetas, 32));\n+    kyber_subv_addv_montmul32();\n+    __ add(tmpAddr, coeffs, 0);\n+    __ st2(v0, v1, __ T2D, __ post(tmpAddr, 32));\n+    __ st2(v2, v3, __ T2D, __ post(tmpAddr, 32));\n+    __ st2(v4, v5, __ T2D, __ post(tmpAddr, 32));\n+    __ st2(v6, v7, __ T2D, __ post(tmpAddr, 32));\n+    __ ld2(v0, v1, __ T2D, __ post(tmpAddr, 32));\n+    __ ld2(v2, v3, __ T2D, __ post(tmpAddr, 32));\n+    __ ld2(v4, v5, __ T2D, __ post(tmpAddr, 32));\n+    __ ld2(v6, v7, __ T2D, tmpAddr);\n+    __ ldpq(v16, v17, __ post(zetas, 32));\n+    __ ldpq(v18, v19, __ post(zetas, 32));\n+    kyber_subv_addv_montmul32();\n+    __ add(tmpAddr, coeffs, 128);\n+    __ st2(v0, v1, __ T2D, __ post(tmpAddr, 32));\n+    __ st2(v2, v3, __ T2D, __ post(tmpAddr, 32));\n+    __ st2(v4, v5, __ T2D, __ post(tmpAddr, 32));\n+    __ st2(v6, v7, __ T2D, __ post(tmpAddr, 32));\n+\n+    __ ld2(v0, v1, __ T2D, __ post(tmpAddr, 32));\n+    __ ld2(v2, v3, __ T2D, __ post(tmpAddr, 32));\n+    __ ld2(v4, v5, __ T2D, __ post(tmpAddr, 32));\n+    __ ld2(v6, v7, __ T2D, tmpAddr);\n+    __ ldpq(v16, v17, __ post(zetas, 32));\n+    __ ldpq(v18, v19, __ post(zetas, 32));\n+    kyber_subv_addv_montmul32();\n+    __ add(tmpAddr, coeffs, 256);\n+    __ st2(v0, v1, __ T2D, __ post(tmpAddr, 32));\n+    __ st2(v2, v3, __ T2D, __ post(tmpAddr, 32));\n+    __ st2(v4, v5, __ T2D, __ post(tmpAddr, 32));\n+    __ st2(v6, v7, __ T2D, __ post(tmpAddr, 32));\n+    __ ld2(v0, v1, __ T2D, __ post(tmpAddr, 32));\n+    __ ld2(v2, v3, __ T2D, __ post(tmpAddr, 32));\n+    __ ld2(v4, v5, __ T2D, __ post(tmpAddr, 32));\n+    __ ld2(v6, v7, __ T2D, tmpAddr);\n+    __ ldpq(v16, v17, __ post(zetas, 32));\n+    __ ldpq(v18, v19, __ post(zetas, 32));\n+    kyber_subv_addv_montmul32();\n+    __ add(tmpAddr, coeffs, 384);\n+    __ st2(v0, v1, __ T2D, __ post(tmpAddr, 32));\n+    __ st2(v2, v3, __ T2D, __ post(tmpAddr, 32));\n+    __ st2(v4, v5, __ T2D, __ post(tmpAddr, 32));\n+    __ st2(v6, v7, __ T2D, tmpAddr);\n+    \/\/ level 2\n+    __ add(tmpAddr, coeffs, 0);\n+    __ ldr(v0, __ Q, __ post(tmpAddr, 16));\n+    __ ldr(v16, __ Q, __ post(tmpAddr, 16));\n+    __ ldr(v1, __ Q, __ post(tmpAddr, 16));\n+    __ ldr(v17, __ Q, __ post(tmpAddr, 16));\n+    __ ldr(v2, __ Q, __ post(tmpAddr, 16));\n+    __ ldr(v18, __ Q, __ post(tmpAddr, 16));\n+    __ ldr(v3, __ Q, __ post(tmpAddr, 16));\n+    __ ldr(v19, __ Q, __ post(tmpAddr, 16));\n+    __ ldr(v4, __ Q, __ post(tmpAddr, 16));\n+    __ ldr(v20, __ Q, __ post(tmpAddr, 16));\n+    __ ldr(v5, __ Q, __ post(tmpAddr, 16));\n+    __ ldr(v21, __ Q, __ post(tmpAddr, 16));\n+    __ ldr(v6, __ Q, __ post(tmpAddr, 16));\n+    __ ldr(v22, __ Q, __ post(tmpAddr, 16));\n+    __ ldr(v7, __ Q, __ post(tmpAddr, 16));\n+    __ ldr(v23, __ Q, tmpAddr);\n+    kyber_addv_subv64();\n+    __ add(tmpAddr, coeffs, 0);\n+    __ str(v24, __ Q, __ post(tmpAddr, 32));\n+    __ str(v25, __ Q, __ post(tmpAddr, 32));\n+    __ str(v26, __ Q, __ post(tmpAddr, 32));\n+    __ str(v27, __ Q, __ post(tmpAddr, 32));\n+    __ str(v28, __ Q, __ post(tmpAddr, 32));\n+    __ str(v29, __ Q, __ post(tmpAddr, 32));\n+    __ str(v30, __ Q, __ post(tmpAddr, 32));\n+    __ str(v31, __ Q, tmpAddr);\n+    kyber_load64zetas(zetas);\n+    __ ldpq(v30, v31, kyberConsts);\n+    kyber_montmul64(false);\n+    __ add(tmpAddr, coeffs, 16);\n+    __ str(v16, __ Q, __ post(tmpAddr, 32));\n+    __ str(v17, __ Q, __ post(tmpAddr, 32));\n+    __ str(v18, __ Q, __ post(tmpAddr, 32));\n+    __ str(v19, __ Q, __ post(tmpAddr, 32));\n+    __ str(v20, __ Q, __ post(tmpAddr, 32));\n+    __ str(v21, __ Q, __ post(tmpAddr, 32));\n+    __ str(v22, __ Q, __ post(tmpAddr, 32));\n+    __ str(v23, __ Q, __ post(tmpAddr, 16));\n+\n+    __ ldr(v0, __ Q, __ post(tmpAddr, 16));\n+    __ ldr(v16, __ Q, __ post(tmpAddr, 16));\n+    __ ldr(v1, __ Q, __ post(tmpAddr, 16));\n+    __ ldr(v17, __ Q, __ post(tmpAddr, 16));\n+    __ ldr(v2, __ Q, __ post(tmpAddr, 16));\n+    __ ldr(v18, __ Q, __ post(tmpAddr, 16));\n+    __ ldr(v3, __ Q, __ post(tmpAddr, 16));\n+    __ ldr(v19, __ Q, __ post(tmpAddr, 16));\n+    __ ldr(v4, __ Q, __ post(tmpAddr, 16));\n+    __ ldr(v20, __ Q, __ post(tmpAddr, 16));\n+    __ ldr(v5, __ Q, __ post(tmpAddr, 16));\n+    __ ldr(v21, __ Q, __ post(tmpAddr, 16));\n+    __ ldr(v6, __ Q, __ post(tmpAddr, 16));\n+    __ ldr(v22, __ Q, __ post(tmpAddr, 16));\n+    __ ldr(v7, __ Q, __ post(tmpAddr, 16));\n+    __ ldr(v23, __ Q, tmpAddr);\n+    kyber_addv_subv64();\n+    __ add(tmpAddr, coeffs, 256);\n+    __ str(v24, __ Q, __ post(tmpAddr, 32));\n+    __ str(v25, __ Q, __ post(tmpAddr, 32));\n+    __ str(v26, __ Q, __ post(tmpAddr, 32));\n+    __ str(v27, __ Q, __ post(tmpAddr, 32));\n+    __ str(v28, __ Q, __ post(tmpAddr, 32));\n+    __ str(v29, __ Q, __ post(tmpAddr, 32));\n+    __ str(v30, __ Q, __ post(tmpAddr, 32));\n+    __ str(v31, __ Q, tmpAddr);\n+    kyber_load64zetas(zetas);\n+    __ ldpq(v30, v31, kyberConsts);\n+    kyber_montmul64(false);\n+    __ add(tmpAddr, coeffs, 272);\n+    __ str(v16, __ Q, __ post(tmpAddr, 32));\n+    __ str(v17, __ Q, __ post(tmpAddr, 32));\n+    __ str(v18, __ Q, __ post(tmpAddr, 32));\n+    __ str(v19, __ Q, __ post(tmpAddr, 32));\n+    __ str(v20, __ Q, __ post(tmpAddr, 32));\n+    __ str(v21, __ Q, __ post(tmpAddr, 32));\n+    __ str(v22, __ Q, __ post(tmpAddr, 32));\n+    __ str(v23, __ Q, tmpAddr);\n+    \/\/ Barrett reduction at indexes where overflow may happen\n+    __ add(tmpAddr, kyberConsts, 16);\n+    __ ldpq(v30, v31, tmpAddr);\n+    __ add(tmpAddr, coeffs, 0);\n+    __ ldr(v0, __ Q, __ post(tmpAddr, 32));\n+    __ ldr(v1, __ Q, __ post(tmpAddr, 32));\n+    __ ldr(v2, __ Q, __ post(tmpAddr, 32));\n+    __ ldr(v3, __ Q, __ post(tmpAddr, 32));\n+    __ ldr(v4, __ Q, __ post(tmpAddr, 32));\n+    __ ldr(v5, __ Q, __ post(tmpAddr, 32));\n+    __ ldr(v6, __ Q, __ post(tmpAddr, 32));\n+    __ ldr(v7, __ Q, tmpAddr);\n+    kyber_sqdmulh32(0);\n+    kyber_sqdmulh32(4);\n+    kyber_sshr32(18);\n+    kyber_sshr32(22);\n+    kyber_mlsv32(0);\n+    kyber_mlsv32(4);\n+    __ add(tmpAddr, coeffs, 0);\n+    __ str(v0, __ Q, __ post(tmpAddr, 32));\n+    __ str(v1, __ Q, __ post(tmpAddr, 32));\n+    __ str(v2, __ Q, __ post(tmpAddr, 32));\n+    __ str(v3, __ Q, __ post(tmpAddr, 32));\n+    __ str(v4, __ Q, __ post(tmpAddr, 32));\n+    __ str(v5, __ Q, __ post(tmpAddr, 32));\n+    __ str(v6, __ Q, __ post(tmpAddr, 32));\n+    __ str(v7, __ Q, __ post(tmpAddr, 32));\n+    __ ldr(v0, __ Q, __ post(tmpAddr, 32));\n+    __ ldr(v1, __ Q, __ post(tmpAddr, 32));\n+    __ ldr(v2, __ Q, __ post(tmpAddr, 32));\n+    __ ldr(v3, __ Q, __ post(tmpAddr, 32));\n+    __ ldr(v4, __ Q, __ post(tmpAddr, 32));\n+    __ ldr(v5, __ Q, __ post(tmpAddr, 32));\n+    __ ldr(v6, __ Q, __ post(tmpAddr, 32));\n+    __ ldr(v7, __ Q, tmpAddr);\n+    kyber_sqdmulh32(0);\n+    kyber_sqdmulh32(4);\n+    kyber_sshr32(18);\n+    kyber_sshr32(22);\n+    kyber_mlsv32(0);\n+    kyber_mlsv32(4);\n+    __ add(tmpAddr, coeffs, 256);\n+    __ str(v0, __ Q, __ post(tmpAddr, 32));\n+    __ str(v1, __ Q, __ post(tmpAddr, 32));\n+    __ str(v2, __ Q, __ post(tmpAddr, 32));\n+    __ str(v3, __ Q, __ post(tmpAddr, 32));\n+    __ str(v4, __ Q, __ post(tmpAddr, 32));\n+    __ str(v5, __ Q, __ post(tmpAddr, 32));\n+    __ str(v6, __ Q, __ post(tmpAddr, 32));\n+    __ str(v7, __ Q, tmpAddr);\n+    \/\/ level 3\n+    __ add(tmpAddr, coeffs, 0);\n+    __ ldpq(v0, v1, __ post(tmpAddr, 32));\n+    __ ldpq(v16, v17, __ post(tmpAddr, 32));\n+    __ ldpq(v2, v3, __ post(tmpAddr, 32));\n+    __ ldpq(v18, v19, __ post(tmpAddr, 32));\n+    __ ldpq(v4, v5, __ post(tmpAddr, 32));\n+    __ ldpq(v20, v21, __ post(tmpAddr, 32));\n+    __ ldpq(v6, v7, __ post(tmpAddr, 32));\n+    __ ldpq(v22, v23, tmpAddr);\n+    kyber_addv_subv64();\n+    __ add(tmpAddr, coeffs, 0);\n+    __ stpq(v24, v25, __ post(tmpAddr, 64));\n+    __ stpq(v26, v27, __ post(tmpAddr, 64));\n+    __ stpq(v28, v29, __ post(tmpAddr, 64));\n+    __ stpq(v30, v31, tmpAddr);\n+    kyber_load64zetas(zetas);\n+    __ ldpq(v30, v31, kyberConsts);\n+    kyber_montmul64(false);\n+    __ add(tmpAddr, coeffs, 32);\n+    __ stpq(v16, v17, __ post(tmpAddr, 64));\n+    __ stpq(v18, v19, __ post(tmpAddr, 64));\n+    __ stpq(v20, v21, __ post(tmpAddr, 64));\n+    __ stpq(v22, v23, __ post(tmpAddr, 32));\n+\n+    __ ldpq(v0, v1, __ post(tmpAddr, 32));\n+    __ ldpq(v16, v17, __ post(tmpAddr, 32));\n+    __ ldpq(v2, v3, __ post(tmpAddr, 32));\n+    __ ldpq(v18, v19, __ post(tmpAddr, 32));\n+    __ ldpq(v4, v5, __ post(tmpAddr, 32));\n+    __ ldpq(v20, v21, __ post(tmpAddr, 32));\n+    __ ldpq(v6, v7, __ post(tmpAddr, 32));\n+    __ ldpq(v22, v23, tmpAddr);\n+    kyber_addv_subv64();\n+    __ add(tmpAddr, coeffs, 256);\n+    __ stpq(v24, v25, __ post(tmpAddr, 64));\n+    __ stpq(v26, v27, __ post(tmpAddr, 64));\n+    __ stpq(v28, v29, __ post(tmpAddr, 64));\n+    __ stpq(v30, v31, tmpAddr);\n+    kyber_load64zetas(zetas);\n+    __ ldpq(v30, v31, kyberConsts);\n+    kyber_montmul64(false);\n+    __ add(tmpAddr, coeffs, 288);\n+    __ stpq(v16, v17, __ post(tmpAddr, 64));\n+    __ stpq(v18, v19, __ post(tmpAddr, 64));\n+    __ stpq(v20, v21, __ post(tmpAddr, 64));\n+    __ stpq(v22, v23, tmpAddr);\n+    \/\/ level 4\n+    __ add(tmpAddr, coeffs, 0);\n+    __ ldpq(v0, v1, __ post(tmpAddr, 32));\n+    __ ldpq(v2, v3, __ post(tmpAddr, 32));\n+    __ ldpq(v16, v17, __ post(tmpAddr, 32));\n+    __ ldpq(v18, v19, __ post(tmpAddr, 32));\n+    __ ldpq(v4, v5, __ post(tmpAddr, 32));\n+    __ ldpq(v6, v7, __ post(tmpAddr, 32));\n+    __ ldpq(v20, v21, __ post(tmpAddr, 32));\n+    __ ldpq(v22, v23, tmpAddr);\n+    kyber_addv_subv64();\n+    __ add(tmpAddr, coeffs, 0);\n+    __ stpq(v24, v25, __ post(tmpAddr, 32));\n+    __ stpq(v26, v27, __ post(tmpAddr, 96));\n+    __ stpq(v28, v29, __ post(tmpAddr, 32));\n+    __ stpq(v30, v31, tmpAddr);\n+    kyber_load64zetas(zetas);\n+    __ ldpq(v30, v31, kyberConsts);\n+    kyber_montmul64(false);\n+    __ add(tmpAddr, coeffs, 64);\n+    __ stpq(v16, v17, __ post(tmpAddr, 32));\n+    __ stpq(v18, v19, __ post(tmpAddr, 96));\n+    __ stpq(v20, v21, __ post(tmpAddr, 32));\n+    __ stpq(v22, v23, __ post(tmpAddr, 32));\n+\n+    __ ldpq(v0, v1, __ post(tmpAddr, 32));\n+    __ ldpq(v2, v3, __ post(tmpAddr, 32));\n+    __ ldpq(v16, v17, __ post(tmpAddr, 32));\n+    __ ldpq(v18, v19, __ post(tmpAddr, 32));\n+    __ ldpq(v4, v5, __ post(tmpAddr, 32));\n+    __ ldpq(v6, v7, __ post(tmpAddr, 32));\n+    __ ldpq(v20, v21, __ post(tmpAddr, 32));\n+    __ ldpq(v22, v23, tmpAddr);\n+    kyber_addv_subv64();\n+    __ add(tmpAddr, coeffs, 256);\n+    __ stpq(v24, v25, __ post(tmpAddr, 32));\n+    __ stpq(v26, v27, __ post(tmpAddr, 96));\n+    __ stpq(v28, v29, __ post(tmpAddr, 32));\n+    __ stpq(v30, v31, tmpAddr);\n+    kyber_load64zetas(zetas);\n+    __ ldpq(v30, v31, kyberConsts);\n+    kyber_montmul64(false);\n+    __ add(tmpAddr, coeffs, 320);\n+    __ stpq(v16, v17, __ post(tmpAddr, 32));\n+    __ stpq(v18, v19, __ post(tmpAddr, 96));\n+    __ stpq(v20, v21, __ post(tmpAddr, 32));\n+    __ stpq(v22, v23, tmpAddr);\n+    \/\/ level 5\n+    __ add(tmpAddr, coeffs, 0);\n+    kyber_load64coeffs(0, tmpAddr);\n+    __ add(tmpAddr, coeffs, 128);\n+    kyber_load64coeffs(16, tmpAddr);\n+    kyber_addv_subv64();\n+    __ add(tmpAddr, coeffs, 0);\n+    kyber_store64coeffs(24, tmpAddr);\n+    kyber_load64zetas(zetas);\n+    __ ldpq(v30, v31, kyberConsts);\n+    kyber_montmul64(false);\n+    __ add(tmpAddr, coeffs, 128);\n+    kyber_store64coeffs(16, tmpAddr);\n+\n+    kyber_load64coeffs(0, tmpAddr);\n+    __ add(tmpAddr, coeffs, 384);\n+    kyber_load64coeffs(16, tmpAddr);\n+    kyber_addv_subv64();\n+    __ add(tmpAddr, coeffs, 256);\n+    kyber_store64coeffs(24, tmpAddr);\n+    kyber_load64zetas(zetas);\n+    __ ldpq(v30, v31, kyberConsts);\n+    kyber_montmul64(false);\n+    __ add(tmpAddr, coeffs, 384);\n+    kyber_store64coeffs(16, tmpAddr);\n+    \/\/ Barrett reduction at indexes where overflow may happen\n+    __ add(tmpAddr, kyberConsts, 16);\n+    __ ldpq(v30, v31, tmpAddr);\n+    __ add(tmpAddr, coeffs, 0);\n+    __ ldpq(v0, v1, __ post(tmpAddr, 256));\n+    __ ldpq(v2, v3, tmpAddr);\n+    kyber_sqdmulh32(0);\n+    kyber_sshr32(18);\n+    kyber_mlsv32(0);\n+    __ add(tmpAddr, coeffs, 0);\n+    __ stpq(v0, v1, __ post(tmpAddr, 256));\n+    __ stpq(v2, v3, tmpAddr);\n+    \/\/ level 6\n+    __ add(tmpAddr, coeffs, 0);\n+    kyber_load64coeffs(0, tmpAddr);\n+    __ add(tmpAddr, coeffs, 256);\n+    kyber_load64coeffs(16, tmpAddr);\n+    kyber_addv_subv64();\n+    __ add(tmpAddr, coeffs, 0);\n+    kyber_store64coeffs(24, tmpAddr);\n+    kyber_load64zetas(zetas);\n+    __ ldpq(v30, v31, kyberConsts);\n+    kyber_montmul64(false);\n+    __ add(tmpAddr, coeffs, 256);\n+    kyber_store64coeffs(16, tmpAddr);\n+\n+    __ add(tmpAddr, coeffs, 128);\n+    kyber_load64coeffs(0, tmpAddr);\n+    __ add(tmpAddr, coeffs, 384);\n+    kyber_load64coeffs(16, tmpAddr);\n+    kyber_addv_subv64();\n+    __ add(tmpAddr, coeffs, 128);\n+    kyber_store64coeffs(24, tmpAddr);\n+    kyber_load64zetas(zetas);\n+    __ ldpq(v30, v31, kyberConsts);\n+    kyber_montmul64(false);\n+    __ add(tmpAddr, coeffs, 384);\n+    kyber_store64coeffs(16, tmpAddr);\n+    \/\/ multiply by 2^-n\n+    __ add(tmpAddr, kyberConsts, 48);\n+    __ ldr(v29, __ Q, tmpAddr);\n+    __ ldpq(v30, v31, kyberConsts);\n+    __ add(tmpAddr, coeffs, 0);\n+    kyber_load64coeffs(0, tmpAddr);\n+    kyber_montmul64(true);\n+    __ add(tmpAddr, coeffs, 0);\n+    kyber_store64coeffs(16, tmpAddr);\n+\n+    kyber_load64coeffs(0, tmpAddr);\n+    kyber_montmul64(true);\n+    __ add(tmpAddr, coeffs, 128);\n+    kyber_store64coeffs(16, tmpAddr);\n+\n+    kyber_load64coeffs(0, tmpAddr);\n+    kyber_montmul64(true);\n+    __ add(tmpAddr, coeffs, 256);\n+    kyber_store64coeffs(16, tmpAddr);\n+\n+    kyber_load64coeffs(0, tmpAddr);\n+    kyber_montmul64(true);\n+   __ add(tmpAddr, coeffs, 384);\n+    kyber_store64coeffs(16, tmpAddr);\n+\n+    __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n+    __ mov(r0, zr); \/\/ return 0\n+    __ ret(lr);\n+\n+    return start;\n+  }\n+\n+  \/\/ Kyber multiply polynomials in the NTT domain.\n+  \/\/ Implements\n+  \/\/ static int implKyberNttMult(\n+  \/\/              short[] result, short[] ntta, short[] nttb, short[] zetas) {}\n+  \/\/\n+  \/\/ result (short[256]) = c_rarg0\n+  \/\/ ntta (short[256]) = c_rarg1\n+  \/\/ nttb (short[256]) = c_rarg2\n+  \/\/ zetas (short[128]) = c_rarg3\n+  address generate_kyberNttMult() {\n+\n+    __ align(CodeEntryAlignment);\n+    StubGenStubId stub_id = StubGenStubId::kyberNttMult_id;\n+    StubCodeMark mark(this, stub_id);\n+    address start = __ pc();\n+    __ enter();\n+\n+    const Register result = c_rarg0;\n+    const Register ntta = c_rarg1;\n+    const Register nttb = c_rarg2;\n+    const Register zetas = c_rarg3;\n+\n+    const Register kyberConsts = r10;\n+    const Register limit = r11;\n+\n+    __ lea(kyberConsts, ExternalAddress((address) StubRoutines::aarch64::_kyberConsts));\n+\n+    Label kyberNttMult_loop;\n+\n+    __ add(limit, result, 512);\n+\n+    __ ldpq(v30, v31, __ post(kyberConsts, 64));\n+    __ ldr(v27, __ Q, kyberConsts);\n+\n+    __ BIND(kyberNttMult_loop);\n+    __ ldpq(v28, v29, __ post(zetas, 32));\n+    __ ld2(v0, v1, __ T8H, __ post(ntta, 32));\n+    __ ld2(v2, v3, __ T8H, __ post(nttb, 32));\n+    __ ld2(v20, v21, __ T8H, __ post(ntta, 32));\n+    __ ld2(v22, v23, __ T8H, __ post(nttb, 32));\n+    \/\/ montmul\n+    __ sqdmulh(v5, __ T8H, v1, v3);\n+    __ mulv(v17, __ T8H, v1, v3);\n+    __ sqdmulh(v4, __ T8H, v0, v2);\n+    __ mulv(v16, __ T8H, v0, v2);\n+    __ sqdmulh(v6, __ T8H, v0, v3);\n+    __ mulv(v18, __ T8H, v0, v3);\n+    __ sqdmulh(v7, __ T8H, v1, v2);\n+    __ mulv(v19, __ T8H, v1, v2);\n+    __ mulv(v17, __ T8H, v17, v30);\n+    __ mulv(v16, __ T8H, v16, v30);\n+    __ mulv(v18, __ T8H, v18, v30);\n+    __ mulv(v19, __ T8H, v19, v30);\n+    __ sqdmulh(v17, __ T8H, v17, v31);\n+    __ sqdmulh(v16, __ T8H, v16, v31);\n+    __ sqdmulh(v18, __ T8H, v18, v31);\n+    __ sqdmulh(v19, __ T8H, v19, v31);\n+    __ shsubv(v17, __ T8H, v5, v17);\n+    __ shsubv(v16, __ T8H, v4, v16);\n+    __ shsubv(v18, __ T8H, v6, v18);\n+    __ shsubv(v19, __ T8H, v7, v19);\n+    __ sqdmulh(v5, __ T8H, v21, v23);\n+    __ mulv(v1, __ T8H, v21, v23);\n+    __ sqdmulh(v4, __ T8H, v20, v22);\n+    __ mulv(v0, __ T8H, v20, v22);\n+    __ sqdmulh(v6, __ T8H, v20, v23);\n+    __ mulv(v2, __ T8H, v20, v23);\n+    __ sqdmulh(v7, __ T8H, v21, v22);\n+    __ mulv(v3, __ T8H, v21, v22);\n+    __ mulv(v1, __ T8H, v1, v30);\n+    __ mulv(v0, __ T8H, v0, v30);\n+    __ mulv(v2, __ T8H, v2, v30);\n+    __ mulv(v3, __ T8H, v3, v30);\n+    __ sqdmulh(v1, __ T8H, v1, v31);\n+    __ sqdmulh(v0, __ T8H, v0, v31);\n+    __ sqdmulh(v2, __ T8H, v2, v31);\n+    __ sqdmulh(v3, __ T8H, v3, v31);\n+    __ shsubv(v1, __ T8H, v5, v1);\n+    __ shsubv(v0, __ T8H, v4, v0);\n+    __ shsubv(v2, __ T8H, v6, v2);\n+    __ shsubv(v3, __ T8H, v7, v3);\n+    __ sqdmulh(v5, __ T8H, v17, v28);\n+    __ mulv(v17, __ T8H, v17, v28);\n+    __ sqdmulh(v4, __ T8H, v1, v29);\n+    __ mulv(v1, __ T8H, v1, v29);\n+    __ mulv(v17, __ T8H, v17, v30);\n+    __ mulv(v1, __ T8H, v1, v30);\n+    __ sqdmulh(v17, __ T8H, v17, v31);\n+    __ sqdmulh(v1, __ T8H, v1, v31);\n+    __ shsubv(v17, __ T8H, v5, v17);\n+    __ shsubv(v1, __ T8H, v4, v1);\n+    __ addv(v16, __ T8H, v16, v17);\n+    __ addv(v17, __ T8H, v18, v19);\n+    __ addv(v18, __ T8H, v0, v1);\n+    __ addv(v19, __ T8H, v2, v3);\n+    __ sqdmulh(v5, __ T8H, v16, v27);\n+    __ mulv(v0, __ T8H, v16, v27);\n+    __ sqdmulh(v4, __ T8H, v17, v27);\n+    __ mulv(v1, __ T8H, v17, v27);\n+    __ sqdmulh(v6, __ T8H, v18, v27);\n+    __ mulv(v2, __ T8H, v18, v27);\n+    __ sqdmulh(v7, __ T8H, v19, v27);\n+    __ mulv(v3, __ T8H, v19, v27);\n+    __ mulv(v0, __ T8H, v0, v30);\n+    __ mulv(v1, __ T8H, v1, v30);\n+    __ mulv(v2, __ T8H, v2, v30);\n+    __ mulv(v3, __ T8H, v3, v30);\n+    __ sqdmulh(v0, __ T8H, v0, v31);\n+    __ sqdmulh(v1, __ T8H, v1, v31);\n+    __ sqdmulh(v2, __ T8H, v2, v31);\n+    __ sqdmulh(v3, __ T8H, v3, v31);\n+    __ shsubv(v0, __ T8H, v5, v0);\n+    __ shsubv(v1, __ T8H, v4, v1);\n+    __ shsubv(v2, __ T8H, v6, v2);\n+    __ shsubv(v3, __ T8H, v7, v3);\n+    __ st2(v0, v1, __ T8H, __ post(result, 32));\n+    __ st2(v2, v3, __ T8H, __ post(result, 32));\n+\n+    __ cmp(result, limit);\n+    __ br(Assembler::NE, kyberNttMult_loop);\n+\n+    __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n+    __ mov(r0, zr); \/\/ return 0\n+    __ ret(lr);\n+\n+    return start;\n+  }\n+\n+  void kyber_load80v0_v17(const Register a) {\n+    __ ldpq(v0, v1, __ post(a, 32));\n+    __ ldpq(v2, v3, __ post(a, 32));\n+    __ ldpq(v4, v5, __ post(a, 32));\n+    __ ldpq(v6, v7, __ post(a, 32));\n+    __ ldpq(v16, v17, __ post(a, 32));\n+  }\n+\n+  void kyber_load80v18_v27(const Register b) {\n+    __ ldpq(v18, v19, __ post(b, 32));\n+    __ ldpq(v20, v21, __ post(b, 32));\n+    __ ldpq(v22, v23, __ post(b, 32));\n+    __ ldpq(v24, v25, __ post(b, 32));\n+    __ ldpq(v26, v27, __ post(b, 32));\n+  }\n+\n+  void kyber_addv80() {\n+    __ addv(v0, __ T8H, v18, v0);\n+    __ addv(v1, __ T8H, v19, v1);\n+    __ addv(v2, __ T8H, v20, v2);\n+    __ addv(v3, __ T8H, v21, v3);\n+    __ addv(v4, __ T8H, v22, v4);\n+    __ addv(v5, __ T8H, v23, v5);\n+    __ addv(v6, __ T8H, v24, v6);\n+    __ addv(v7, __ T8H, v25, v7);\n+    __ addv(v16, __ T8H, v26, v16);\n+    __ addv(v17, __ T8H, v27, v17);\n+  }\n+\n+  void kyber_addv_v31_80() {\n+    __ addv(v0, __ T8H, v31, v0);\n+    __ addv(v1, __ T8H, v31, v1);\n+    __ addv(v2, __ T8H, v31, v2);\n+    __ addv(v3, __ T8H, v31, v3);\n+    __ addv(v4, __ T8H, v31, v4);\n+    __ addv(v5, __ T8H, v31, v5);\n+    __ addv(v6, __ T8H, v31, v6);\n+    __ addv(v7, __ T8H, v31, v7);\n+    __ addv(v16, __ T8H, v31, v16);\n+    __ addv(v17, __ T8H, v31, v17);\n+  }\n+\n+  void kyber_store80(const Register result) {\n+    __ stpq(v0, v1, __ post(result, 32));\n+    __ stpq(v2, v3, __ post(result, 32));\n+    __ stpq(v4, v5, __ post(result, 32));\n+    __ stpq(v6, v7, __ post(result, 32));\n+    __ stpq(v16, v17, __ post(result, 32));\n+  }\n+\n+  \/\/ Kyber add 2 polynomials.\n+  \/\/ Implements\n+  \/\/ static int implKyberAddPoly(short[] result, short[] a, short[] b) {}\n+  \/\/\n+  \/\/ result (short[256]) = c_rarg0\n+  \/\/ a (short[256]) = c_rarg1\n+  \/\/ b (short[256]) = c_rarg2\n+  address generate_kyberAddPoly_2() {\n+\n+    __ align(CodeEntryAlignment);\n+    StubGenStubId stub_id = StubGenStubId::kyberAddPoly_2_id;\n+    StubCodeMark mark(this, stub_id);\n+    address start = __ pc();\n+    __ enter();\n+\n+    const Register result = c_rarg0;\n+    const Register a = c_rarg1;\n+    const Register b = c_rarg2;\n+\n+    const Register kyberConsts = r11;\n+\n+    __ lea(kyberConsts, ExternalAddress((address) StubRoutines::aarch64::_kyberConsts));\n+\n+    __ add(kyberConsts, kyberConsts, 16);\n+    __ ldr(v31, __ Q, kyberConsts);\n+    kyber_load80v0_v17(a);\n+    __ ldr(v28, __ Q, __ post(a, 16));\n+    kyber_load80v18_v27(b);\n+    __ ldr(v29, __ Q, __ post(b, 16));\n+    kyber_addv80();\n+    __ addv(v28, __ T8H, v28, v29);\n+    kyber_addv_v31_80();\n+    __ addv(v28, __ T8H, v28, v31);\n+    kyber_store80(result);\n+    __ str(v28, __ Q, __ post(result, 16));\n+    kyber_load80v0_v17(a);\n+    __ ldr(v28, __ Q, __ post(a, 16));\n+    kyber_load80v18_v27(b);\n+    __ ldr(v29, __ Q, __ post(b, 16));\n+    kyber_addv80();\n+    __ addv(v28, __ T8H, v28, v29);\n+    kyber_addv_v31_80();\n+    __ addv(v28, __ T8H, v28, v31);\n+    kyber_store80(result);\n+    __ str(v28, __ Q, __ post(result, 16));\n+    kyber_load80v0_v17(a);\n+    kyber_load80v18_v27(b);\n+    kyber_addv80();\n+    kyber_addv_v31_80();\n+    kyber_store80(result);\n+\n+    __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n+    __ mov(r0, zr); \/\/ return 0\n+    __ ret(lr);\n+\n+    return start;\n+  }\n+\n+  \/\/ Kyber add 3 polynomials.\n+  \/\/ Implements\n+  \/\/ static int implKyberAddPoly(short[] result, short[] a, short[] b, short[] c) {}\n+  \/\/\n+  \/\/ result (short[256]) = c_rarg0\n+  \/\/ a (short[256]) = c_rarg1\n+  \/\/ b (short[256]) = c_rarg2\n+  \/\/ c (short[256]) = c_rarg3\n+  address generate_kyberAddPoly_3() {\n+\n+    __ align(CodeEntryAlignment);\n+    StubGenStubId stub_id = StubGenStubId::kyberAddPoly_3_id;\n+    StubCodeMark mark(this, stub_id);\n+    address start = __ pc();\n+    __ enter();\n+\n+    const Register result = c_rarg0;\n+    const Register a = c_rarg1;\n+    const Register b = c_rarg2;\n+    const Register c = c_rarg3;\n+\n+    const Register kyberConsts = r11;\n+\n+    __ lea(kyberConsts, ExternalAddress((address) StubRoutines::aarch64::_kyberConsts));\n+\n+    __ add(kyberConsts, kyberConsts, 16);\n+    __ ldr(v31, __ Q, kyberConsts);\n+    __ addv(v31, __ T8H, v31, v31);\n+    kyber_load80v0_v17(a);\n+    __ ldr(v28, __ Q, __ post(a, 16));\n+    kyber_load80v18_v27(b);\n+    __ ldr(v29, __ Q, __ post(b, 16));\n+    kyber_addv80();\n+    __ addv(v28, __ T8H, v28, v29);\n+    kyber_load80v18_v27(c);\n+    __ ldr(v29, __ Q, __ post(c, 16));\n+    kyber_addv80();\n+    __ addv(v28, __ T8H, v28, v29);\n+    kyber_addv_v31_80();\n+    __ addv(v28, __ T8H, v28, v31);\n+    kyber_store80(result);\n+    __ str(v28, __ Q, __ post(result, 16));\n+    kyber_load80v0_v17(a);\n+    __ ldr(v28, __ Q, __ post(a, 16));\n+    kyber_load80v18_v27(b);\n+    __ ldr(v29, __ Q, __ post(b, 16));\n+    kyber_addv80();\n+    __ addv(v28, __ T8H, v28, v29);\n+    kyber_load80v18_v27(c);\n+    __ ldr(v29, __ Q, __ post(c, 16));\n+    kyber_addv80();\n+    __ addv(v28, __ T8H, v28, v29);\n+    kyber_addv_v31_80();\n+    __ addv(v28, __ T8H, v28, v31);\n+    kyber_store80(result);\n+    __ str(v28, __ Q, __ post(result, 16));\n+    kyber_load80v0_v17(a);\n+    kyber_load80v18_v27(b);\n+    kyber_addv80();\n+    kyber_load80v18_v27(c);\n+    kyber_addv80();\n+    kyber_addv_v31_80();\n+    kyber_store80(result);\n+\n+    __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n+    __ mov(r0, zr); \/\/ return 0\n+    __ ret(lr);\n+\n+    return start;\n+  }\n+\n+\/\/ Kyber parse XOF output to polynomial coefficient candidates\n+\/\/ or decodePoly(12, ...).\n+\/\/ Implements\n+\/\/ static int implKyber12To16(\n+\/\/         byte[] condensed, int index, short[] parsed, int parsedLength) {}\n+\/\/\n+\/\/ (parsedLength or (parsedLength - 48) must be divisible by 64.)\n+\/\/\n+\/\/ condensed (byte[]) = c_rarg0\n+\/\/ condensedIndex = c_rarg1\n+\/\/ parsed (short[112 or 256]) = c_rarg2\n+\/\/ parsedLength (112 or 256) = c_rarg3\n+  address generate_kyber12To16() {\n+   Label L_F00, L_loop, L_end;\n+\n+   __ BIND(L_F00);\n+   __ emit_int64(0x0f000f000f000f00);\n+   __ emit_int64(0x0f000f000f000f00);\n+\n+    __ align(CodeEntryAlignment);\n+    StubGenStubId stub_id = StubGenStubId::kyber12To16_id;\n+    StubCodeMark mark(this, stub_id);\n+    address start = __ pc();\n+    __ enter();\n+\n+    const Register condensed = c_rarg0;\n+    const Register condensedOffs = c_rarg1;\n+    const Register parsed = c_rarg2;\n+    const Register parsedLength = c_rarg3;\n+\n+    const Register tmpAddr = r11;\n+\n+    __ adr(tmpAddr, L_F00);\n+    __ ldr(v31, __ Q, tmpAddr);\n+    __ add(condensed, condensed, condensedOffs);\n+\n+    __ BIND(L_loop);\n+    __ ld3(v24, v25, v26, __ T16B, __ post(condensed, 48));\n+    __ ld3(v27, v28, v29, __ T16B, __ post(condensed, 48));\n+\n+    __ ushll(v0, __ T8H, v24, __ T8B, 0);\n+    __ ushll2(v1, __ T8H, v24, __ T16B, 0);\n+    __ ushll(v2, __ T8H, v25, __ T8B, 0);\n+    __ ushll2(v3, __ T8H, v25, __ T16B, 0);\n+    __ ushll(v4, __ T8H, v25, __ T8B, 0);\n+    __ ushll2(v5, __ T8H, v25, __ T16B, 0);\n+    __ ushll(v16, __ T8H, v27, __ T8B, 0);\n+    __ ushll2(v17, __ T8H, v27, __ T16B, 0);\n+    __ ushll(v18, __ T8H, v28, __ T8B, 0);\n+    __ ushll2(v19, __ T8H, v28, __ T16B, 0);\n+    __ ushll(v20, __ T8H, v28, __ T8B, 0);\n+    __ ushll2(v21, __ T8H, v28, __ T16B, 0);\n+    __ shl(v2, __ T8H, v2, 8);\n+    __ shl(v3, __ T8H, v3, 8);\n+    __ shl(v18, __ T8H, v18, 8);\n+    __ shl(v19, __ T8H, v19, 8);\n+    __ ushll(v6, __ T8H, v26, __ T8B, 4);\n+    __ ushll2(v7, __ T8H, v26, __ T16B, 4);\n+    __ ushll(v22, __ T8H, v29, __ T8B, 4);\n+    __ ushll2(v23, __ T8H, v29, __ T16B, 4);\n+    __ andr(v2, __ T16B, v2, v31);\n+    __ andr(v3, __ T16B, v3, v31);\n+    __ ushr(v4, __ T8H, v4, 4);\n+    __ ushr(v5, __ T8H, v5, 4);\n+    __ andr(v18, __ T16B, v18, v31);\n+    __ andr(v19, __ T16B, v19, v31);\n+    __ ushr(v20, __ T8H, v20, 4);\n+    __ ushr(v21, __ T8H, v21, 4);\n+    __ addv(v0, __ T8H, v0, v2);\n+    __ addv(v2, __ T8H, v1, v3);\n+    __ addv(v1, __ T8H, v4, v6);\n+    __ addv(v3, __ T8H, v5, v7);\n+    __ addv(v16, __ T8H, v16, v18);\n+    __ addv(v18, __ T8H, v17, v19);\n+    __ addv(v17, __ T8H, v20, v22);\n+    __ addv(v19, __ T8H, v21, v23);\n+\n+    __ st2(v0, v1, __ T8H, __ post(parsed, 32));\n+    __ st2(v2, v3, __ T8H, __ post(parsed, 32));\n+    __ st2(v16, v17, __ T8H, __ post(parsed, 32));\n+    __ st2(v18, v19, __ T8H, __ post(parsed, 32));\n+\n+    __ sub(parsedLength, parsedLength, 64);\n+    __ cmp(parsedLength, (u1)64);\n+    __ br(Assembler::GE, L_loop);\n+    __ cbz(parsedLength, L_end);\n+\n+    __ ld3(v24, v25, v26, __ T16B, __ post(condensed, 48));\n+    __ ld3(v27, v28, v29, __ T8B, condensed);\n+\n+    __ ushll(v0, __ T8H, v24, __ T8B, 0);\n+    __ ushll2(v1, __ T8H, v24, __ T16B, 0);\n+    __ ushll(v2, __ T8H, v25, __ T8B, 0);\n+    __ ushll2(v3, __ T8H, v25, __ T16B, 0);\n+    __ ushll(v4, __ T8H, v25, __ T8B, 0);\n+    __ ushll2(v5, __ T8H, v25, __ T16B, 0);\n+    __ ushll(v16, __ T8H, v27, __ T8B, 0);\n+    __ ushll(v18, __ T8H, v28, __ T8B, 0);\n+    __ ushll(v20, __ T8H, v28, __ T8B, 0);\n+    __ shl(v2, __ T8H, v2, 8);\n+    __ shl(v3, __ T8H, v3, 8);\n+    __ shl(v18, __ T8H, v18, 8);\n+    __ ushll(v6, __ T8H, v26, __ T8B, 4);\n+    __ ushll2(v7, __ T8H, v26, __ T16B, 4);\n+    __ ushll(v22, __ T8H, v29, __ T8B, 4);\n+    __ andr(v2, __ T16B, v2, v31);\n+    __ andr(v3, __ T16B, v3, v31);\n+    __ ushr(v4, __ T8H, v4, 4);\n+    __ ushr(v5, __ T8H, v5, 4);\n+    __ andr(v18, __ T16B, v18, v31);\n+    __ ushr(v20, __ T8H, v20, 4);\n+    __ addv(v0, __ T8H, v0, v2);\n+    __ addv(v2, __ T8H, v1, v3);\n+    __ addv(v1, __ T8H, v4, v6);\n+    __ addv(v3, __ T8H, v5, v7);\n+    __ addv(v16, __ T8H, v16, v18);\n+    __ addv(v17, __ T8H, v20, v22);\n+\n+    __ st2(v0, v1, __ T8H, __ post(parsed, 32));\n+    __ st2(v2, v3, __ T8H, __ post(parsed, 32));\n+    __ st2(v16, v17, __ T8H, __ post(parsed, 32));\n+\n+    __ BIND(L_end);\n+\n+    __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n+    __ mov(r0, zr); \/\/ return 0\n+    __ ret(lr);\n+\n+    return start;\n+  }\n+\n+  void kyber_sqdmulh80() {\n+    kyber_sqdmulh32(0);\n+    kyber_sqdmulh32(4);\n+    __ sqdmulh(v26, __ T8H, v16, v31);\n+    __ sqdmulh(v27, __ T8H, v17, v31);\n+  }\n+\n+  void kyber_sshr80() {\n+    kyber_sshr32(18);\n+    kyber_sshr32(22);\n+    __ sshr(v26, __ T8H, v26, 11);\n+    __ sshr(v27, __ T8H, v27, 11);\n+  }\n+\n+  void kyber_mlsv80() {\n+    kyber_mlsv32(0);\n+    kyber_mlsv32(4);\n+    __ mlsv(v16, __ T8H, v26, v30);\n+    __ mlsv(v17, __ T8H, v27, v30);\n+  }\n+\n+  \/\/ Kyber barrett reduce function.\n+  \/\/ Implements\n+  \/\/ static int implKyberBarrettReduce(short[] coeffs) {}\n+  \/\/\n+  \/\/ coeffs (short[256]) = c_rarg0\n+  address generate_kyberBarrettReduce() {\n+\n+    __ align(CodeEntryAlignment);\n+    StubGenStubId stub_id = StubGenStubId::kyberBarrettReduce_id;\n+    StubCodeMark mark(this, stub_id);\n+    address start = __ pc();\n+    __ enter();\n+\n+    const Register coeffs = c_rarg0;\n+\n+    const Register kyberConsts = r10;\n+    const Register result = r11;\n+\n+    __ add(result, coeffs, 0);\n+    __ lea(kyberConsts, ExternalAddress((address) StubRoutines::aarch64::_kyberConsts));\n+\n+    __ add(kyberConsts, kyberConsts, 16);\n+    __ ldpq(v30, v31, kyberConsts);\n+\n+    kyber_load80v0_v17(coeffs);\n+    __ ldr(v28, __ Q, __ post(coeffs, 16));\n+    kyber_sqdmulh80();\n+    __ sqdmulh(v29, __ T8H, v28, v31);\n+    kyber_sshr80();\n+    __ sshr(v29, __ T8H, v29, 11);\n+    kyber_mlsv80();\n+    __ mlsv(v28, __ T8H, v29, v30);\n+    kyber_store80(result);\n+    __ str(v28, __ Q, __ post(result, 16));\n+\n+    kyber_load80v0_v17(coeffs);\n+    __ ldr(v28, __ Q, __ post(coeffs, 16));\n+    kyber_sqdmulh80();\n+    __ sqdmulh(v29, __ T8H, v28, v31);\n+    kyber_sshr80();\n+    __ sshr(v29, __ T8H, v29, 11);\n+    kyber_mlsv80();\n+    __ mlsv(v28, __ T8H, v29, v30);\n+    kyber_store80(result);\n+    __ str(v28, __ Q, __ post(result, 16));\n+\n+    kyber_load80v0_v17(coeffs);\n+    kyber_sqdmulh80();\n+    kyber_sshr80();\n+    kyber_mlsv80();\n+    kyber_store80(result);\n+\n+    __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n+    __ mov(r0, zr); \/\/ return 0\n+    __ ret(lr);\n+\n+    return start;\n+  }\n+\n@@ -8942,0 +10572,10 @@\n+    if (UseKyberIntrinsics) {\n+      StubRoutines::_kyberNtt = generate_kyberNtt();\n+      StubRoutines::_kyberInverseNtt = generate_kyberInverseNtt();\n+      StubRoutines::_kyberNttMult = generate_kyberNttMult();\n+      StubRoutines::_kyberAddPoly_2 = generate_kyberAddPoly_2();\n+      StubRoutines::_kyberAddPoly_3 = generate_kyberAddPoly_3();\n+      StubRoutines::_kyber12To16 = generate_kyber12To16();\n+      StubRoutines::_kyberBarrettReduce = generate_kyberBarrettReduce();\n+    }\n+\n@@ -8984,0 +10624,1 @@\n+      StubRoutines::_double_keccak         = generate_double_keccak();\n","filename":"src\/hotspot\/cpu\/aarch64\/stubGenerator_aarch64.cpp","additions":1718,"deletions":77,"binary":false,"changes":1795,"status":"modified"},{"patch":"@@ -51,0 +51,10 @@\n+ATTRIBUTE_ALIGNED(64) uint16_t StubRoutines::aarch64::_kyberConsts[] =\n+{\n+    0xF301, 0xF301, 0xF301, 0xF301, 0xF301, 0xF301, 0xF301, 0xF301,\n+    0x0D01, 0x0D01, 0x0D01, 0x0D01, 0x0D01, 0x0D01, 0x0D01, 0x0D01,\n+    0x4EBF, 0x4EBF, 0x4EBF, 0x4EBF, 0x4EBF, 0x4EBF, 0x4EBF, 0x4EBF,\n+    0x0200, 0x0200, 0x0200, 0x0200, 0x0200, 0x0200, 0x0200, 0x0200,\n+    0x0549, 0x0549, 0x0549, 0x0549, 0x0549, 0x0549, 0x0549, 0x0549,\n+    0x0F00, 0x0F00, 0x0F00, 0x0F00, 0x0F00, 0x0F00, 0x0F00, 0x0F00\n+};\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/stubRoutines_aarch64.cpp","additions":10,"deletions":0,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -113,0 +113,1 @@\n+  static uint16_t  _kyberConsts[];\n","filename":"src\/hotspot\/cpu\/aarch64\/stubRoutines_aarch64.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -420,0 +420,11 @@\n+  if (_features & CPU_ASIMD) {\n+      if (FLAG_IS_DEFAULT(UseKyberIntrinsics)) {\n+          UseKyberIntrinsics = true;\n+      }\n+  } else if (UseKyberIntrinsics) {\n+      if (!FLAG_IS_DEFAULT(UseKyberIntrinsics)) {\n+          warning(\"Kyber intrinsic requires ASIMD instructions\");\n+      }\n+      FLAG_SET_DEFAULT(UseKyberIntrinsics, false);\n+  }\n+\n@@ -686,0 +697,1 @@\n+    fprintf(stderr, \"_features_string = \\\"%s\\\"\", _features_string);\n@@ -698,0 +710,1 @@\n+  fprintf(stderr, \"_features_string = \\\"%s\\\"\", _features_string);\n","filename":"src\/hotspot\/cpu\/aarch64\/vm_version_aarch64.cpp","additions":13,"deletions":0,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -478,0 +478,1 @@\n+  case vmIntrinsics::_double_keccak:\n@@ -490,0 +491,9 @@\n+  case vmIntrinsics::_kyberNtt:\n+  case vmIntrinsics::_kyberInverseNtt:\n+  case vmIntrinsics::_kyberNttMult:\n+  case vmIntrinsics::_kyberAddPoly_2:\n+  case vmIntrinsics::_kyberAddPoly_3:\n+  case vmIntrinsics::_kyber12To16:\n+  case vmIntrinsics::_kyberBarrettReduce:\n+    if (!UseKyberIntrinsics) return true;\n+    break;\n","filename":"src\/hotspot\/share\/classfile\/vmIntrinsics.cpp","additions":10,"deletions":0,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -519,0 +519,6 @@\n+  \/* support for sun.security.provider.SHAKE128Parallel *\/                                                              \\\n+  do_class(sun_security_provider_sha3_parallel,                \"sun\/security\/provider\/SHA3Parallel\")                    \\\n+   do_intrinsic(_double_keccak, sun_security_provider_sha3_parallel, double_keccak_name, double_keccak_signature, F_S)   \\\n+   do_name(     double_keccak_name,                                 \"doubleKeccak\")                                     \\\n+   do_signature(double_keccak_signature,                            \"([J[J)I\")                                          \\\n+                                                                                                                        \\\n@@ -564,0 +570,21 @@\n+  \/* support for com.sun.crypto.provider.ML_KEM *\/                                                                      \\\n+  do_class(com_sun_crypto_provider_ML_KEM,      \"com\/sun\/crypto\/provider\/ML_KEM\")                                       \\\n+   do_signature(SaSaSaSaI_signature, \"([S[S[S[S)I\")                                                                     \\\n+   do_signature(BaISaII_signature, \"([BI[SI)I\")                                                                         \\\n+   do_signature(SaSaSaI_signature, \"([S[S[S)I\")                                                                         \\\n+   do_signature(SaSaI_signature, \"([S[S)I\")                                                                             \\\n+   do_signature(SaI_signature, \"([S)I\")                                                                                 \\\n+   do_name(kyberAddPoly_name,                             \"implKyberAddPoly\")                                           \\\n+  do_intrinsic(_kyberNtt, com_sun_crypto_provider_ML_KEM, kyberNtt_name, SaSaI_signature, F_S)                          \\\n+   do_name(kyberNtt_name,                                  \"implKyberNtt\")                                              \\\n+  do_intrinsic(_kyberInverseNtt, com_sun_crypto_provider_ML_KEM, kyberInverseNtt_name, SaSaI_signature, F_S)            \\\n+   do_name(kyberInverseNtt_name,                           \"implKyberInverseNtt\")                                       \\\n+  do_intrinsic(_kyberNttMult, com_sun_crypto_provider_ML_KEM, kyberNttMult_name, SaSaSaSaI_signature, F_S)              \\\n+   do_name(kyberNttMult_name,                              \"implKyberNttMult\")                                          \\\n+  do_intrinsic(_kyberAddPoly_2, com_sun_crypto_provider_ML_KEM, kyberAddPoly_name, SaSaSaI_signature, F_S)              \\\n+  do_intrinsic(_kyberAddPoly_3, com_sun_crypto_provider_ML_KEM, kyberAddPoly_name, SaSaSaSaI_signature, F_S)            \\\n+  do_intrinsic(_kyber12To16, com_sun_crypto_provider_ML_KEM, kyber12To16_name, BaISaII_signature, F_S)                  \\\n+   do_name(kyber12To16_name,                             \"implKyber12To16\")                                             \\\n+  do_intrinsic(_kyberBarrettReduce, com_sun_crypto_provider_ML_KEM, kyberBarrettReduce_name, SaI_signature, F_S)        \\\n+   do_name(kyberBarrettReduce_name,                        \"implKyberBarrettReduce\")                                    \\\n+                                                                                                                        \\\n","filename":"src\/hotspot\/share\/classfile\/vmIntrinsics.hpp","additions":27,"deletions":0,"binary":false,"changes":27,"status":"modified"},{"patch":"@@ -397,0 +397,1 @@\n+  static_field(StubRoutines,                _double_keccak,                                   address)                               \\\n@@ -398,0 +399,7 @@\n+  static_field(StubRoutines,                _kyberNtt,                                        address)                               \\\n+  static_field(StubRoutines,                _kyberInverseNtt,                                 address)                               \\\n+  static_field(StubRoutines,                _kyberNttMult,                                    address)                               \\\n+  static_field(StubRoutines,                _kyberAddPoly_2,                                  address)                               \\\n+  static_field(StubRoutines,                _kyberAddPoly_3,                                  address)                               \\\n+  static_field(StubRoutines,                _kyber12To16,                                     address)                               \\\n+  static_field(StubRoutines,                _kyberBarrettReduce,                              address)                               \\\n","filename":"src\/hotspot\/share\/jvmci\/vmStructs_jvmci.cpp","additions":8,"deletions":0,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -783,0 +783,1 @@\n+  case vmIntrinsics::_double_keccak:\n@@ -792,0 +793,7 @@\n+  case vmIntrinsics::_kyberNtt:\n+  case vmIntrinsics::_kyberInverseNtt:\n+  case vmIntrinsics::_kyberNttMult:\n+  case vmIntrinsics::_kyberAddPoly_2:\n+  case vmIntrinsics::_kyberAddPoly_3:\n+  case vmIntrinsics::_kyber12To16:\n+  case vmIntrinsics::_kyberBarrettReduce:\n","filename":"src\/hotspot\/share\/opto\/c2compiler.cpp","additions":8,"deletions":0,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -2195,0 +2195,7 @@\n+                  strcmp(call->as_CallLeaf()->_name, \"kyberNtt\") == 0 ||\n+                  strcmp(call->as_CallLeaf()->_name, \"kyberInverseNtt\") == 0 ||\n+                  strcmp(call->as_CallLeaf()->_name, \"kyberNttMult\") == 0 ||\n+                  strcmp(call->as_CallLeaf()->_name, \"kyberAddPoly_2\") == 0 ||\n+                  strcmp(call->as_CallLeaf()->_name, \"kyberAddPoly_3\") == 0 ||\n+                  strcmp(call->as_CallLeaf()->_name, \"kyber12To16\") == 0 ||\n+                  strcmp(call->as_CallLeaf()->_name, \"kyberBarrettReduce\") == 0 ||\n@@ -2206,0 +2213,1 @@\n+                  strcmp(call->as_CallLeaf()->_name, \"double_keccak\") == 0 ||\n","filename":"src\/hotspot\/share\/opto\/escape.cpp","additions":8,"deletions":0,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -236,0 +236,22 @@\n+  if(intrinsic_id() == vmIntrinsics::_digestBase_implCompressMB) {\n+      fprintf(stderr, \"try_to_inline call for _digestBase_implCompressMB\\n\");\n+  } else if(intrinsic_id() == vmIntrinsics::_sha3_implCompress) {\n+      fprintf(stderr, \"try_to_inline call for _sha3_implCompress\\n\");\n+  } else if(intrinsic_id() == vmIntrinsics::_double_keccak) {\n+      fprintf(stderr, \"try_to_inline call for _double_keccak\\n\");\n+  } else if(intrinsic_id() == vmIntrinsics::_kyberNtt) {\n+      fprintf(stderr, \"try_to_inline call for _kyberNtt\\n\");\n+  } else if(intrinsic_id() == vmIntrinsics::_kyberInverseNtt) {\n+      fprintf(stderr, \"try_to_inline call for _kyberInverseNtt\\n\");\n+  } else if(intrinsic_id() == vmIntrinsics::_kyberNttMult) {\n+      fprintf(stderr, \"try_to_inline call for _kyberNttMult\\n\");\n+  } else if(intrinsic_id() == vmIntrinsics::_kyberAddPoly_2) {\n+      fprintf(stderr, \"try_to_inline call for _kyberAddPoly_2\\n\");\n+  } else if(intrinsic_id() == vmIntrinsics::_kyberAddPoly_3) {\n+      fprintf(stderr, \"try_to_inline call for _kyberAddPoly_3\\n\");\n+  } else if(intrinsic_id() == vmIntrinsics::_kyber12To16) {\n+      fprintf(stderr, \"try_to_inline call for _kyber12To16\\n\");\n+  } else if(intrinsic_id() == vmIntrinsics::_kyberBarrettReduce) {\n+      fprintf(stderr, \"try_to_inline call for _kyberBarrettReduce\\n\");\n+  }\n+\n@@ -597,0 +619,2 @@\n+  case vmIntrinsics::_double_keccak:\n+    return inline_double_keccak();\n@@ -627,0 +651,14 @@\n+  case vmIntrinsics::_kyberNtt:\n+    return inline_kyberNtt();\n+  case vmIntrinsics::_kyberInverseNtt:\n+    return inline_kyberInverseNtt();\n+  case vmIntrinsics::_kyberNttMult:\n+    return inline_kyberNttMult();\n+  case vmIntrinsics::_kyberAddPoly_2:\n+    return inline_kyberAddPoly_2();\n+  case vmIntrinsics::_kyberAddPoly_3:\n+    return inline_kyberAddPoly_3();\n+  case vmIntrinsics::_kyber12To16:\n+    return inline_kyber12To16();\n+  case vmIntrinsics::_kyberBarrettReduce:\n+    return inline_kyberBarrettReduce();\n@@ -7591,0 +7629,240 @@\n+\/\/------------------------------inline_kyberNtt\n+bool LibraryCallKit::inline_kyberNtt() {\n+  address stubAddr;\n+  const char *stubName;\n+  assert(UseKyberIntrinsics, \"need Kyber intrinsics support\");\n+  assert(callee()->signature()->size() == 2, \"kyberNtt has 2 parameters\");\n+\n+  stubAddr = StubRoutines::kyberNtt();\n+  stubName = \"kyberNtt\";\n+  if (!stubAddr) return false;\n+\n+  Node* coeffs          = argument(0);\n+  Node* ntt_zetas        = argument(1);\n+\n+  coeffs = must_be_not_null(coeffs, true);\n+  ntt_zetas = must_be_not_null(ntt_zetas, true);\n+\n+  Node* coeffs_start  = array_element_address(coeffs, intcon(0), T_SHORT);\n+  assert(coeffs_start, \"coeffs is null\");\n+  Node* ntt_zetas_start  = array_element_address(ntt_zetas, intcon(0), T_SHORT);\n+  assert(ntt_zetas_start, \"ntt_zetas is null\");\n+  Node* kyberNtt = make_runtime_call(RC_LEAF|RC_NO_FP,\n+                                  OptoRuntime::kyberNtt_Type(),\n+                                  stubAddr, stubName, TypePtr::BOTTOM,\n+                                  coeffs_start, ntt_zetas_start);\n+  \/\/ return an int\n+  Node* retvalue = _gvn.transform(new ProjNode(kyberNtt, TypeFunc::Parms));\n+  set_result(retvalue);\n+  return true;\n+}\n+\n+\/\/------------------------------inline_kyberInverseNtt\n+bool LibraryCallKit::inline_kyberInverseNtt() {\n+  address stubAddr;\n+  const char *stubName;\n+  assert(UseKyberIntrinsics, \"need Kyber intrinsics support\");\n+  assert(callee()->signature()->size() == 2, \"kyberInverseNtt has 2 parameters\");\n+\n+  stubAddr = StubRoutines::kyberInverseNtt();\n+  stubName = \"kyberInverseNtt\";\n+  if (!stubAddr) return false;\n+\n+  Node* coeffs          = argument(0);\n+  Node* zetas           = argument(1);\n+\n+  coeffs = must_be_not_null(coeffs, true);\n+  zetas = must_be_not_null(zetas, true);\n+\n+  Node* coeffs_start  = array_element_address(coeffs, intcon(0), T_SHORT);\n+  assert(coeffs_start, \"coeffs is null\");\n+  Node* zetas_start  = array_element_address(zetas, intcon(0), T_SHORT);\n+  assert(zetas_start, \"inverseNtt_zetas is null\");\n+  Node* kyberInverseNtt = make_runtime_call(RC_LEAF|RC_NO_FP,\n+                                  OptoRuntime::kyberInverseNtt_Type(),\n+                                  stubAddr, stubName, TypePtr::BOTTOM,\n+                                  coeffs_start, zetas_start);\n+\n+  \/\/ return an int\n+  Node* retvalue = _gvn.transform(new ProjNode(kyberInverseNtt, TypeFunc::Parms));\n+  set_result(retvalue);\n+  return true;\n+}\n+\n+\/\/------------------------------inline_kyberNttMult\n+bool LibraryCallKit::inline_kyberNttMult() {\n+  address stubAddr;\n+  const char *stubName;\n+  assert(UseKyberIntrinsics, \"need Kyber intrinsics support\");\n+  assert(callee()->signature()->size() == 4, \"kyberNttMult has 4 parameters\");\n+\n+  stubAddr = StubRoutines::kyberNttMult();\n+  stubName = \"kyberNttMult\";\n+  if (!stubAddr) return false;\n+\n+  Node* result          = argument(0);\n+  Node* ntta            = argument(1);\n+  Node* nttb            = argument(2);\n+  Node* zetas           = argument(3);\n+\n+  result = must_be_not_null(result, true);\n+  ntta = must_be_not_null(ntta, true);\n+  nttb = must_be_not_null(nttb, true);\n+  zetas = must_be_not_null(zetas, true);\n+\n+  Node* result_start  = array_element_address(result, intcon(0), T_SHORT);\n+  assert(result_start, \"result is null\");\n+  Node* ntta_start  = array_element_address(ntta, intcon(0), T_SHORT);\n+  assert(ntta_start, \"ntta is null\");\n+  Node* nttb_start  = array_element_address(nttb, intcon(0), T_SHORT);\n+  assert(nttb_start, \"nttb is null\");\n+  Node* zetas_start  = array_element_address(zetas, intcon(0), T_SHORT);\n+  assert(zetas_start, \"nttMult_zetas is null\");\n+  Node* kyberNttMult = make_runtime_call(RC_LEAF|RC_NO_FP,\n+                                  OptoRuntime::kyberNttMult_Type(),\n+                                  stubAddr, stubName, TypePtr::BOTTOM,\n+                                  result_start, ntta_start, nttb_start,\n+                                  zetas_start);\n+\n+  \/\/ return an int\n+  Node* retvalue = _gvn.transform(new ProjNode(kyberNttMult, TypeFunc::Parms));\n+  set_result(retvalue);\n+\n+  return true;\n+}\n+\n+\/\/------------------------------inline_kyberAddPoly_2\n+bool LibraryCallKit::inline_kyberAddPoly_2() {\n+  address stubAddr;\n+  const char *stubName;\n+  assert(UseKyberIntrinsics, \"need Kyber intrinsics support\");\n+  assert(callee()->signature()->size() == 3, \"kyberAddPoly_2 has 3 parameters\");\n+\n+  stubAddr = StubRoutines::kyberAddPoly_2();\n+  stubName = \"kyberAddPoly_2\";\n+  if (!stubAddr) return false;\n+\n+  Node* result          = argument(0);\n+  Node* a               = argument(1);\n+  Node* b               = argument(2);\n+\n+  result = must_be_not_null(result, true);\n+  a = must_be_not_null(a, true);\n+  b = must_be_not_null(b, true);\n+\n+  Node* result_start  = array_element_address(result, intcon(0), T_SHORT);\n+  assert(result_start, \"result is null\");\n+  Node* a_start  = array_element_address(a, intcon(0), T_SHORT);\n+  assert(a_start, \"a is null\");\n+  Node* b_start  = array_element_address(b, intcon(0), T_SHORT);\n+  assert(b_start, \"b is null\");\n+  Node* kyberAddPoly_2 = make_runtime_call(RC_LEAF|RC_NO_FP,\n+                                  OptoRuntime::kyberAddPoly_2_Type(),\n+                                  stubAddr, stubName, TypePtr::BOTTOM,\n+                                  result_start, a_start, b_start);\n+  \/\/ return an int\n+  Node* retvalue = _gvn.transform(new ProjNode(kyberAddPoly_2, TypeFunc::Parms));\n+  set_result(retvalue);\n+  return true;\n+}\n+\n+\/\/------------------------------inline_kyberAddPoly_3\n+bool LibraryCallKit::inline_kyberAddPoly_3() {\n+  address stubAddr;\n+  const char *stubName;\n+  assert(UseKyberIntrinsics, \"need Kyber intrinsics support\");\n+  assert(callee()->signature()->size() == 4, \"kyberAddPoly_3 has 4 parameters\");\n+\n+  stubAddr = StubRoutines::kyberAddPoly_3();\n+  stubName = \"kyberAddPoly_3\";\n+  if (!stubAddr) return false;\n+\n+  Node* result          = argument(0);\n+  Node* a               = argument(1);\n+  Node* b               = argument(2);\n+  Node* c               = argument(3);\n+\n+  result = must_be_not_null(result, true);\n+  a = must_be_not_null(a, true);\n+  b = must_be_not_null(b, true);\n+  c = must_be_not_null(c, true);\n+\n+  Node* result_start  = array_element_address(result, intcon(0), T_SHORT);\n+  assert(result_start, \"result is null\");\n+  Node* a_start  = array_element_address(a, intcon(0), T_SHORT);\n+  assert(a_start, \"a is null\");\n+  Node* b_start  = array_element_address(b, intcon(0), T_SHORT);\n+  assert(b_start, \"b is null\");\n+  Node* c_start  = array_element_address(c, intcon(0), T_SHORT);\n+  assert(c_start, \"c is null\");\n+  Node* kyberAddPoly_3 = make_runtime_call(RC_LEAF|RC_NO_FP,\n+                                  OptoRuntime::kyberAddPoly_3_Type(),\n+                                  stubAddr, stubName, TypePtr::BOTTOM,\n+                                  result_start, a_start, b_start, c_start);\n+  \/\/ return an int\n+  Node* retvalue = _gvn.transform(new ProjNode(kyberAddPoly_3, TypeFunc::Parms));\n+  set_result(retvalue);\n+  return true;\n+}\n+\n+\/\/------------------------------inline_kyber12To16\n+bool LibraryCallKit::inline_kyber12To16() {\n+  address stubAddr;\n+  const char *stubName;\n+  assert(UseKyberIntrinsics, \"need Kyber intrinsics support\");\n+  assert(callee()->signature()->size() == 3, \"kyber12To16 has 3 parameters\");\n+\n+  stubAddr = StubRoutines::kyber12To16();\n+  stubName = \"kyber12To16\";\n+  if (!stubAddr) return false;\n+\n+  Node* condensed       = argument(0);\n+  Node* condensedOffs   = argument(1);\n+  Node* parsed          = argument(2);\n+  Node* parsedLength    = argument(3);\n+\n+  condensed = must_be_not_null(condensed, true);\n+  parsed = must_be_not_null(parsed, true);\n+\n+  Node* condensed_start  = array_element_address(condensed, intcon(0), T_BYTE);\n+  assert(condensed_start, \"condensed is null\");\n+  Node* parsed_start  = array_element_address(parsed, intcon(0), T_SHORT);\n+  assert(parsed_start, \"parsed is null\");\n+  Node* kyber12To16 = make_runtime_call(RC_LEAF|RC_NO_FP,\n+                                  OptoRuntime::kyber12To16_Type(),\n+                                  stubAddr, stubName, TypePtr::BOTTOM,\n+                                  condensed_start, condensedOffs, parsed_start, parsedLength);\n+  \/\/ return an int\n+  Node* retvalue = _gvn.transform(new ProjNode(kyber12To16, TypeFunc::Parms));\n+  set_result(retvalue);\n+  return true;\n+\n+}\n+\n+\/\/------------------------------inline_kyberBarrettReduce\n+bool LibraryCallKit::inline_kyberBarrettReduce() {\n+  address stubAddr;\n+  const char *stubName;\n+  assert(UseKyberIntrinsics, \"need Kyber intrinsics support\");\n+  assert(callee()->signature()->size() == 1, \"kyberBarrettReduce has 1 parameters\");\n+\n+  stubAddr = StubRoutines::kyberBarrettReduce();\n+  stubName = \"kyberBarrettReduce\";\n+  if (!stubAddr) return false;\n+\n+  Node* coeffs          = argument(0);\n+\n+  coeffs = must_be_not_null(coeffs, true);\n+\n+  Node* coeffs_start  = array_element_address(coeffs, intcon(0), T_SHORT);\n+  assert(coeffs_start, \"coeffs is null\");\n+  Node* kyberBarrettReduce = make_runtime_call(RC_LEAF|RC_NO_FP,\n+                                  OptoRuntime::kyberBarrettReduce_Type(),\n+                                  stubAddr, stubName, TypePtr::BOTTOM,\n+                                  coeffs_start);\n+  \/\/ return an int\n+  Node* retvalue = _gvn.transform(new ProjNode(kyberBarrettReduce, TypeFunc::Parms));\n+  set_result(retvalue);\n+  return true;\n+}\n+\n@@ -7854,0 +8132,32 @@\n+\/\/------------------------------inline_double_keccak\n+bool LibraryCallKit::inline_double_keccak() {\n+  address stubAddr;\n+  const char *stubName;\n+  assert(UseSHA3Intrinsics, \"need SHA3 intrinsics support\");\n+  assert(callee()->signature()->size() == 2, \"double_keccak has 2 parameters\");\n+\n+  stubAddr = StubRoutines::double_keccak();\n+  stubName = \"double_keccak\";\n+  if (!stubAddr) return false;\n+\n+  Node* status0        = argument(0);\n+  Node* status1        = argument(1);\n+\n+  status0 = must_be_not_null(status0, true);\n+  status1 = must_be_not_null(status1, true);\n+\n+  Node* status0_start  = array_element_address(status0, intcon(0), T_LONG);\n+  assert(status0_start, \"status0 is null\");\n+  Node* status1_start  = array_element_address(status1, intcon(0), T_LONG);\n+  assert(status1_start, \"status1 is null\");\n+  Node* double_keccak = make_runtime_call(RC_LEAF|RC_NO_FP,\n+                                  OptoRuntime::double_keccak_Type(),\n+                                  stubAddr, stubName, TypePtr::BOTTOM,\n+                                  status0_start, status1_start);\n+  \/\/ return an int\n+  Node* retvalue = _gvn.transform(new ProjNode(double_keccak, TypeFunc::Parms));\n+  set_result(retvalue);\n+  return true;\n+}\n+\n+\n","filename":"src\/hotspot\/share\/opto\/library_call.cpp","additions":310,"deletions":0,"binary":false,"changes":310,"status":"modified"},{"patch":"@@ -319,0 +319,7 @@\n+  bool inline_kyberNtt();\n+  bool inline_kyberInverseNtt();\n+  bool inline_kyberNttMult();\n+  bool inline_kyberAddPoly_2();\n+  bool inline_kyberAddPoly_3();\n+  bool inline_kyber12To16();\n+  bool inline_kyberBarrettReduce();\n@@ -325,0 +332,1 @@\n+  bool inline_double_keccak();\n","filename":"src\/hotspot\/share\/opto\/library_call.hpp","additions":8,"deletions":0,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -232,0 +232,1 @@\n+const TypeFunc* OptoRuntime::_double_keccak_Type                  = nullptr;\n@@ -241,0 +242,9 @@\n+\n+const TypeFunc* OptoRuntime::_kyberNtt_Type                             = nullptr;\n+const TypeFunc* OptoRuntime::_kyberInverseNtt_Type                      = nullptr;\n+const TypeFunc* OptoRuntime::_kyberNttMult_Type                         = nullptr;\n+const TypeFunc* OptoRuntime::_kyberAddPoly_2_Type                       = nullptr;\n+const TypeFunc* OptoRuntime::_kyberAddPoly_3_Type                       = nullptr;\n+const TypeFunc* OptoRuntime::_kyber12To16_Type                          = nullptr;\n+const TypeFunc* OptoRuntime::_kyberBarrettReduce_Type                   = nullptr;\n+\n@@ -1172,0 +1182,3 @@\n+\/*\n+ * int implCompressMultiBlock(byte[] b, int ofs, int limit)\n+ *\/\n@@ -1193,0 +1206,19 @@\n+\/\/ SHAKE128Parallel doubleKeccak function\n+static const TypeFunc* make_double_keccak_Type() {\n+    int argcnt = 2;\n+\n+    const Type** fields = TypeTuple::fields(argcnt);\n+    int argp = TypeFunc::Parms;\n+    fields[argp++] = TypePtr::NOTNULL;      \/\/ status0\n+    fields[argp++] = TypePtr::NOTNULL;      \/\/ status1\n+\n+    assert(argp == TypeFunc::Parms + argcnt, \"correct decoding\");\n+    const TypeTuple* domain = TypeTuple::make(TypeFunc::Parms + argcnt, fields);\n+\n+    \/\/ result type needed\n+    fields = TypeTuple::fields(1);\n+    fields[TypeFunc::Parms + 0] = TypeInt::INT;\n+    const TypeTuple* range = TypeTuple::make(TypeFunc::Parms + 1, fields);\n+    return TypeFunc::make(domain, range);\n+}\n+\n@@ -1378,0 +1410,142 @@\n+\/\/ Kyber NTT function\n+static const TypeFunc* make_kyberNtt_Type() {\n+    int argcnt = 2;\n+\n+    const Type** fields = TypeTuple::fields(argcnt);\n+    int argp = TypeFunc::Parms;\n+    fields[argp++] = TypePtr::NOTNULL;      \/\/ coeffs\n+    fields[argp++] = TypePtr::NOTNULL;      \/\/ NTT zetas\n+\n+    assert(argp == TypeFunc::Parms + argcnt, \"correct decoding\");\n+    const TypeTuple* domain = TypeTuple::make(TypeFunc::Parms + argcnt, fields);\n+\n+    \/\/ result type needed\n+    fields = TypeTuple::fields(1);\n+    fields[TypeFunc::Parms + 0] = TypeInt::INT;\n+    const TypeTuple* range = TypeTuple::make(TypeFunc::Parms + 1, fields);\n+    return TypeFunc::make(domain, range);\n+}\n+\n+\/\/ Kyber inverse NTT function\n+static const TypeFunc* make_kyberInverseNtt_Type() {\n+    int argcnt = 2;\n+\n+    const Type** fields = TypeTuple::fields(argcnt);\n+    int argp = TypeFunc::Parms;\n+    fields[argp++] = TypePtr::NOTNULL;      \/\/ coeffs\n+    fields[argp++] = TypePtr::NOTNULL;      \/\/ inverse NTT zetas\n+\n+    assert(argp == TypeFunc::Parms + argcnt, \"correct decoding\");\n+    const TypeTuple* domain = TypeTuple::make(TypeFunc::Parms + argcnt, fields);\n+\n+    \/\/ result type needed\n+    fields = TypeTuple::fields(1);\n+    fields[TypeFunc::Parms + 0] = TypeInt::INT;\n+    const TypeTuple* range = TypeTuple::make(TypeFunc::Parms + 1, fields);\n+    return TypeFunc::make(domain, range);\n+}\n+\n+\/\/ Kyber NTT multiply function\n+static const TypeFunc* make_kyberNttMult_Type() {\n+    int argcnt = 4;\n+\n+    const Type** fields = TypeTuple::fields(argcnt);\n+    int argp = TypeFunc::Parms;\n+    fields[argp++] = TypePtr::NOTNULL;      \/\/ result\n+    fields[argp++] = TypePtr::NOTNULL;      \/\/ ntta\n+    fields[argp++] = TypePtr::NOTNULL;      \/\/ nttb\n+    fields[argp++] = TypePtr::NOTNULL;      \/\/ NTT multiply zetas\n+\n+    assert(argp == TypeFunc::Parms + argcnt, \"correct decoding\");\n+    const TypeTuple* domain = TypeTuple::make(TypeFunc::Parms + argcnt, fields);\n+\n+    \/\/ result type needed\n+    fields = TypeTuple::fields(1);\n+    fields[TypeFunc::Parms + 0] = TypeInt::INT;\n+    const TypeTuple* range = TypeTuple::make(TypeFunc::Parms + 1, fields);\n+    return TypeFunc::make(domain, range);\n+}\n+\n+\/\/ Kyber add 2 polynomials function\n+static const TypeFunc* make_kyberAddPoly_2_Type() {\n+    int argcnt = 3;\n+\n+    const Type** fields = TypeTuple::fields(argcnt);\n+    int argp = TypeFunc::Parms;\n+    fields[argp++] = TypePtr::NOTNULL;      \/\/ result\n+    fields[argp++] = TypePtr::NOTNULL;      \/\/ a\n+    fields[argp++] = TypePtr::NOTNULL;      \/\/ b\n+\n+    assert(argp == TypeFunc::Parms + argcnt, \"correct decoding\");\n+    const TypeTuple* domain = TypeTuple::make(TypeFunc::Parms + argcnt, fields);\n+\n+    \/\/ result type needed\n+    fields = TypeTuple::fields(1);\n+    fields[TypeFunc::Parms + 0] = TypeInt::INT;\n+    const TypeTuple* range = TypeTuple::make(TypeFunc::Parms + 1, fields);\n+    return TypeFunc::make(domain, range);\n+}\n+\n+\n+\/\/ Kyber add 3 polynomials function\n+static const TypeFunc* make_kyberAddPoly_3_Type() {\n+    int argcnt = 4;\n+\n+    const Type** fields = TypeTuple::fields(argcnt);\n+    int argp = TypeFunc::Parms;\n+    fields[argp++] = TypePtr::NOTNULL;      \/\/ result\n+    fields[argp++] = TypePtr::NOTNULL;      \/\/ a\n+    fields[argp++] = TypePtr::NOTNULL;      \/\/ b\n+    fields[argp++] = TypePtr::NOTNULL;      \/\/ c\n+\n+    assert(argp == TypeFunc::Parms + argcnt, \"correct decoding\");\n+    const TypeTuple* domain = TypeTuple::make(TypeFunc::Parms + argcnt, fields);\n+\n+    \/\/ result type needed\n+    fields = TypeTuple::fields(1);\n+    fields[TypeFunc::Parms + 0] = TypeInt::INT;\n+    const TypeTuple* range = TypeTuple::make(TypeFunc::Parms + 1, fields);\n+    return TypeFunc::make(domain, range);\n+}\n+\n+\n+\/\/ Kyber XOF output parsing into polynomial coefficients candidates\n+\/\/ or decompress(12,...) function\n+static const TypeFunc* make_kyber12To16_Type() {\n+    int argcnt = 4;\n+\n+    const Type** fields = TypeTuple::fields(argcnt);\n+    int argp = TypeFunc::Parms;\n+    fields[argp++] = TypePtr::NOTNULL;      \/\/ condensed\n+    fields[argp++] = TypeInt::INT;          \/\/ condensedOffs\n+    fields[argp++] = TypePtr::NOTNULL;      \/\/ parsed\n+    fields[argp++] = TypeInt::INT;          \/\/ parsedLength\n+\n+    assert(argp == TypeFunc::Parms + argcnt, \"correct decoding\");\n+    const TypeTuple* domain = TypeTuple::make(TypeFunc::Parms + argcnt, fields);\n+\n+    \/\/ result type needed\n+    fields = TypeTuple::fields(1);\n+    fields[TypeFunc::Parms + 0] = TypeInt::INT;\n+    const TypeTuple* range = TypeTuple::make(TypeFunc::Parms + 1, fields);\n+    return TypeFunc::make(domain, range);\n+}\n+\n+\/\/ Kyber Barrett reduce function\n+static const TypeFunc* make_kyberBarrettReduce_Type() {\n+    int argcnt = 1;\n+\n+    const Type** fields = TypeTuple::fields(argcnt);\n+    int argp = TypeFunc::Parms;\n+    fields[argp++] = TypePtr::NOTNULL;      \/\/ coeffs\n+\n+    assert(argp == TypeFunc::Parms + argcnt, \"correct decoding\");\n+    const TypeTuple* domain = TypeTuple::make(TypeFunc::Parms + argcnt, fields);\n+\n+    \/\/ result type needed\n+    fields = TypeTuple::fields(1);\n+    fields[TypeFunc::Parms + 0] = TypeInt::INT;\n+    const TypeTuple* range = TypeTuple::make(TypeFunc::Parms + 1, fields);\n+    return TypeFunc::make(domain, range);\n+}\n+\n@@ -1981,0 +2155,1 @@\n+  _double_keccak_Type                 = make_double_keccak_Type();\n@@ -1989,0 +2164,9 @@\n+\n+  _kyberNtt_Type                      = make_kyberNtt_Type();\n+  _kyberInverseNtt_Type               = make_kyberInverseNtt_Type();\n+  _kyberNttMult_Type                  = make_kyberNttMult_Type();\n+  _kyberAddPoly_2_Type                = make_kyberAddPoly_2_Type();\n+  _kyberAddPoly_3_Type                = make_kyberAddPoly_3_Type();\n+  _kyber12To16_Type                   = make_kyber12To16_Type();\n+  _kyberBarrettReduce_Type            = make_kyberBarrettReduce_Type();\n+\n","filename":"src\/hotspot\/share\/opto\/runtime.cpp","additions":184,"deletions":0,"binary":false,"changes":184,"status":"modified"},{"patch":"@@ -173,0 +173,1 @@\n+  static const TypeFunc* _double_keccak_Type;\n@@ -182,0 +183,9 @@\n+\n+  static const TypeFunc* _kyberNtt_Type;\n+  static const TypeFunc* _kyberInverseNtt_Type;\n+  static const TypeFunc* _kyberNttMult_Type;\n+  static const TypeFunc* _kyberAddPoly_2_Type;\n+  static const TypeFunc* _kyberAddPoly_3_Type;\n+  static const TypeFunc* _kyber12To16_Type;\n+  static const TypeFunc* _kyberBarrettReduce_Type;\n+\n@@ -465,0 +475,4 @@\n+\/\/  static const TypeFunc* digestBase_implCompress_Type(bool is_sha3);\n+\/\/  static const TypeFunc* digestBase_implCompressMB_Type(bool is_sha3);\n+\/\/  static const TypeFunc* double_keccak_Type();\n+\n@@ -528,0 +542,5 @@\n+  static inline const TypeFunc* double_keccak_Type() {\n+    assert(_double_keccak_Type != nullptr, \"should be initialized\");\n+    return _double_keccak_Type;\n+  }\n+\n@@ -576,0 +595,35 @@\n+  static const TypeFunc* kyberNtt_Type() {\n+    assert(_kyberNtt_Type != nullptr, \"should be initialized\");\n+    return _kyberNtt_Type;\n+  }\n+\n+  static const TypeFunc* kyberInverseNtt_Type() {\n+    assert(_kyberInverseNtt_Type != nullptr, \"should be initialized\");\n+    return _kyberInverseNtt_Type;\n+  }\n+\n+  static const TypeFunc* kyberNttMult_Type() {\n+    assert(_kyberNttMult_Type != nullptr, \"should be initialized\");\n+    return _kyberNttMult_Type;\n+  }\n+\n+  static const TypeFunc* kyberAddPoly_2_Type() {\n+    assert(_kyberAddPoly_2_Type != nullptr, \"should be initialized\");\n+    return _kyberAddPoly_2_Type;\n+  }\n+\n+  static const TypeFunc* kyberAddPoly_3_Type() {\n+    assert(_kyberAddPoly_3_Type != nullptr, \"should be initialized\");\n+    return _kyberAddPoly_3_Type;\n+  }\n+\n+  static const TypeFunc* kyber12To16_Type() {\n+    assert(_kyber12To16_Type != nullptr, \"should be initialized\");\n+    return _kyber12To16_Type;\n+  }\n+\n+  static const TypeFunc* kyberBarrettReduce_Type() {\n+    assert(_kyberBarrettReduce_Type != nullptr, \"should be initialized\");\n+    return _kyberBarrettReduce_Type;\n+  }\n+\n","filename":"src\/hotspot\/share\/opto\/runtime.hpp","additions":54,"deletions":0,"binary":false,"changes":54,"status":"modified"},{"patch":"@@ -328,0 +328,3 @@\n+  product(bool, UseKyberIntrinsics, false, DIAGNOSTIC,                      \\\n+          \"Use intrinsics for the vectorized version of Kyber\")             \\\n+                                                                            \\\n","filename":"src\/hotspot\/share\/runtime\/globals.hpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -681,0 +681,15 @@\n+  do_stub(compiler, kyberNtt)                                           \\\n+  do_entry(compiler, kyberNtt, kyberNtt, kyberNtt)                      \\\n+  do_stub(compiler, kyberInverseNtt)                                    \\\n+  do_entry(compiler, kyberInverseNtt, kyberInverseNtt, kyberInverseNtt) \\\n+  do_stub(compiler, kyberNttMult)                                       \\\n+  do_entry(compiler, kyberNttMult, kyberNttMult, kyberNttMult)          \\\n+  do_stub(compiler, kyberAddPoly_2)                                     \\\n+  do_entry(compiler, kyberAddPoly_2, kyberAddPoly_2, kyberAddPoly_2)    \\\n+  do_stub(compiler, kyberAddPoly_3)                                     \\\n+  do_entry(compiler, kyberAddPoly_3, kyberAddPoly_3, kyberAddPoly_3)    \\\n+  do_stub(compiler, kyber12To16)                                        \\\n+  do_entry(compiler, kyber12To16, kyber12To16, kyber12To16)             \\\n+  do_stub(compiler, kyberBarrettReduce)                                 \\\n+  do_entry(compiler, kyberBarrettReduce, kyberBarrettReduce,            \\\n+           kyberBarrettReduce)                                          \\\n@@ -728,0 +743,2 @@\n+  do_stub(compiler, double_keccak)                                      \\\n+  do_entry(compiler, double_keccak, double_keccak, double_keccak)       \\\n","filename":"src\/hotspot\/share\/runtime\/stubDeclarations.hpp","additions":17,"deletions":0,"binary":false,"changes":17,"status":"modified"},{"patch":"@@ -31,0 +31,1 @@\n+import jdk.internal.vm.annotation.IntrinsicCandidate;\n@@ -74,0 +75,262 @@\n+    private static final short[] montZetasForVectorNttArr = new short[]{\n+            \/\/ level 0\n+            -758, -758, -758, -758, -758, -758, -758, -758,\n+            -758, -758, -758, -758, -758, -758, -758, -758,\n+            -758, -758, -758, -758, -758, -758, -758, -758,\n+            -758, -758, -758, -758, -758, -758, -758, -758,\n+            -758, -758, -758, -758, -758, -758, -758, -758,\n+            -758, -758, -758, -758, -758, -758, -758, -758,\n+            -758, -758, -758, -758, -758, -758, -758, -758,\n+            -758, -758, -758, -758, -758, -758, -758, -758,\n+            -758, -758, -758, -758, -758, -758, -758, -758,\n+            -758, -758, -758, -758, -758, -758, -758, -758,\n+            -758, -758, -758, -758, -758, -758, -758, -758,\n+            -758, -758, -758, -758, -758, -758, -758, -758,\n+            -758, -758, -758, -758, -758, -758, -758, -758,\n+            -758, -758, -758, -758, -758, -758, -758, -758,\n+            -758, -758, -758, -758, -758, -758, -758, -758,\n+            -758, -758, -758, -758, -758, -758, -758, -758,\n+            \/\/ level 1\n+            -359, -359, -359, -359, -359, -359, -359, -359,\n+            -359, -359, -359, -359, -359, -359, -359, -359,\n+            -359, -359, -359, -359, -359, -359, -359, -359,\n+            -359, -359, -359, -359, -359, -359, -359, -359,\n+            -359, -359, -359, -359, -359, -359, -359, -359,\n+            -359, -359, -359, -359, -359, -359, -359, -359,\n+            -359, -359, -359, -359, -359, -359, -359, -359,\n+            -359, -359, -359, -359, -359, -359, -359, -359,\n+            -1517, -1517, -1517, -1517, -1517, -1517, -1517, -1517,\n+            -1517, -1517, -1517, -1517, -1517, -1517, -1517, -1517,\n+            -1517, -1517, -1517, -1517, -1517, -1517, -1517, -1517,\n+            -1517, -1517, -1517, -1517, -1517, -1517, -1517, -1517,\n+            -1517, -1517, -1517, -1517, -1517, -1517, -1517, -1517,\n+            -1517, -1517, -1517, -1517, -1517, -1517, -1517, -1517,\n+            -1517, -1517, -1517, -1517, -1517, -1517, -1517, -1517,\n+            -1517, -1517, -1517, -1517, -1517, -1517, -1517, -1517,\n+            \/\/ level 2\n+            1493, 1493, 1493, 1493, 1493, 1493, 1493, 1493,\n+            1493, 1493, 1493, 1493, 1493, 1493, 1493, 1493,\n+            1493, 1493, 1493, 1493, 1493, 1493, 1493, 1493,\n+            1493, 1493, 1493, 1493, 1493, 1493, 1493, 1493,\n+            1422, 1422, 1422, 1422, 1422, 1422, 1422, 1422,\n+            1422, 1422, 1422, 1422, 1422, 1422, 1422, 1422,\n+            1422, 1422, 1422, 1422, 1422, 1422, 1422, 1422,\n+            1422, 1422, 1422, 1422, 1422, 1422, 1422, 1422,\n+            287, 287, 287, 287, 287, 287, 287, 287,\n+            287, 287, 287, 287, 287, 287, 287, 287,\n+            287, 287, 287, 287, 287, 287, 287, 287,\n+            287, 287, 287, 287, 287, 287, 287, 287,\n+            202, 202, 202, 202, 202, 202, 202, 202,\n+            202, 202, 202, 202, 202, 202, 202, 202,\n+            202, 202, 202, 202, 202, 202, 202, 202,\n+            202, 202, 202, 202, 202, 202, 202, 202,\n+            \/\/ level 3\n+            -171, -171, -171, -171, -171, -171, -171, -171,\n+            -171, -171, -171, -171, -171, -171, -171, -171,\n+            622, 622, 622, 622, 622, 622, 622, 622,\n+            622, 622, 622, 622, 622, 622, 622, 622,\n+            1577, 1577, 1577, 1577, 1577, 1577, 1577, 1577,\n+            1577, 1577, 1577, 1577, 1577, 1577, 1577, 1577,\n+            182, 182, 182, 182, 182, 182, 182, 182,\n+            182, 182, 182, 182, 182, 182, 182, 182,\n+            962, 962, 962, 962, 962, 962, 962, 962,\n+            962, 962, 962, 962, 962, 962, 962, 962,\n+            -1202, -1202, -1202, -1202, -1202, -1202, -1202, -1202,\n+            -1202, -1202, -1202, -1202, -1202, -1202, -1202, -1202,\n+            -1474, -1474, -1474, -1474, -1474, -1474, -1474, -1474,\n+            -1474, -1474, -1474, -1474, -1474, -1474, -1474, -1474,\n+            1468, 1468, 1468, 1468, 1468, 1468, 1468, 1468,\n+            1468, 1468, 1468, 1468, 1468, 1468, 1468, 1468,\n+            \/\/ level 4\n+            573, 573, 573, 573, 573, 573, 573, 573,\n+            -1325, -1325, -1325, -1325, -1325, -1325, -1325, -1325,\n+            264, 264, 264, 264, 264, 264, 264, 264,\n+            383, 383, 383, 383, 383, 383, 383, 383,\n+            -829, -829, -829, -829, -829, -829, -829, -829,\n+            1458, 1458, 1458, 1458, 1458, 1458, 1458, 1458,\n+            -1602, -1602, -1602, -1602, -1602, -1602, -1602, -1602,\n+            -130, -130, -130, -130, -130, -130, -130, -130,\n+            -681, -681, -681, -681, -681, -681, -681, -681,\n+            1017, 1017, 1017, 1017, 1017, 1017, 1017, 1017,\n+            732, 732, 732, 732, 732, 732, 732, 732,\n+            608, 608, 608, 608, 608, 608, 608, 608,\n+            -1542, -1542, -1542, -1542, -1542, -1542, -1542, -1542,\n+            411, 411, 411, 411, 411, 411, 411, 411,\n+            -205, -205, -205, -205, -205, -205, -205, -205,\n+            -1571, -1571, -1571, -1571, -1571, -1571, -1571, -1571,\n+            \/\/ level 5\n+            1223, 1223, 1223, 1223, 652, 652, 652, 652,\n+            -552, -552, -552, -552, 1015, 1015, 1015, 1015,\n+            -1293, -1293, -1293, -1293, 1491, 1491, 1491, 1491,\n+            -282, -282, -282, -282, -1544, -1544, -1544, -1544,\n+            516, 516, 516, 516, -8, -8, -8, -8,\n+            -320, -320, -320, -320, -666, -666, -666, -666,\n+            1711, 1711, 1711, 1711, -1162, -1162, -1162, -1162,\n+            126, 126, 126, 126, 1469, 1469, 1469, 1469,\n+            -853, -853, -853, -853, -90, -90, -90, -90,\n+            -271, -271, -271, -271, 830, 830, 830, 830,\n+            107, 107, 107, 107, -1421, -1421, -1421, -1421,\n+            -247, -247, -247, -247, -951, -951, -951, -951,\n+            -398, -398, -398, -398, 961, 961, 961, 961,\n+            -1508, -1508, -1508, -1508, -725, -725, -725, -725,\n+            448, 448, 448, 448, -1065, -1065, -1065, -1065,\n+            677, 677, 677, 677, -1275, -1275, -1275, -1275,\n+            \/\/ level 6\n+            -1103, -1103, 430, 430, 555, 555, 843, 843,\n+            -1251, -1251, 871, 871, 1550, 1550, 105, 105,\n+            422, 422, 587, 587, 177, 177, -235, -235,\n+            -291, -291, -460, -460, 1574, 1574, 1653, 1653,\n+            -246, -246, 778, 778, 1159, 1159, -147, -147,\n+            -777, -777, 1483, 1483, -602, -602, 1119, 1119,\n+            -1590, -1590, 644, 644, -872, -872, 349, 349,\n+            418, 418, 329, 329, -156, -156, -75, -75,\n+            817, 817, 1097, 1097, 603, 603, 610, 610,\n+            1322, 1322, -1285, -1285, -1465, -1465, 384, 384,\n+            -1215, -1215, -136, -136, 1218, 1218, -1335, -1335,\n+            -874, -874, 220, 220, -1187, -1187, 1670, 1670,\n+            -1185, -1185, -1530, -1530, -1278, -1278, 794, 794,\n+            -1510, -1510, -854, -854, -870, -870, 478, 478,\n+            -108, -108, -308, -308, 996, 996, 991, 991,\n+            958, 958, -1460, -1460, 1522, 1522, 1628, 1628\n+    };\n+    private static final int[] MONT_ZETAS_FOR_INVERSE_NTT = new int[]{\n+            584, -1049, 57, 1317, 789, 709, 1599, -1601,\n+            -990, 604, 348, 857, 612, 474, 1177, -1014,\n+            -88, -982, -191, 668, 1386, 486, -1153, -534,\n+            514, 137, 586, -1178, 227, 339, -907, 244,\n+            1200, -833, 1394, -30, 1074, 636, -317, -1192,\n+            -1259, -355, -425, -884, -977, 1430, 868, 607,\n+            184, 1448, 702, 1327, 431, 497, 595, -94,\n+            1649, -1497, -620, 42, -172, 1107, -222, 1003,\n+            426, -845, 395, -510, 1613, 825, 1269, -290,\n+            -1429, 623, -567, 1617, 36, 1007, 1440, 332,\n+            -201, 1313, -1382, -744, 669, -1538, 128, -1598,\n+            1401, 1183, -553, 714, 405, -1155, -445, 406,\n+            -1496, -49, 82, 1369, 259, 1604, 373, 909,\n+            -1249, -1000, -25, -52, 530, -895, 1226, 819,\n+            -185, 281, -742, 1253, 417, 1400, 35, -593,\n+            97, -1263, 551, -585, 969, -914, -1188\n+    };\n+\n+    private static final short[] montZetasForVectorInverseNttArr = new short[]{\n+            \/\/ level 0\n+            -1628, -1628, -1522, -1522, 1460, 1460, -958, -958,\n+            -991, -991, -996, -996, 308, 308, 108, 108,\n+            -478, -478, 870, 870, 854, 854, 1510, 1510,\n+            -794, -794, 1278, 1278, 1530, 1530, 1185, 1185,\n+            1659, 1659, 1187, 1187, -220, -220, 874, 874,\n+            1335, 1335, -1218, -1218, 136, 136, 1215, 1215,\n+            -384, -384, 1465, 1465, 1285, 1285, -1322, -1322,\n+            -610, -610, -603, -603, -1097, -1097, -817, -817,\n+            75, 75, 156, 156, -329, -329, -418, -418,\n+            -349, -349, 872, 872, -644, -644, 1590, 1590,\n+            -1119, -1119, 602, 602, -1483, -1483, 777, 777,\n+            147, 147, -1159, -1159, -778, -778, 246, 246,\n+            -1653, -1653, -1574, -1574, 460, 460, 291, 291,\n+            235, 235, -177, -177, -587, -587, -422, -422,\n+            -105, -105, -1550, -1550, -871, -871, 1251, 1251,\n+            -843, -843, -555, -555, -430, -430, 1103, 1103,\n+            \/\/ level 1\n+            1275, 1275, 1275, 1275, -677, -677, -677, -677,\n+            1065, 1065, 1065, 1065, -448, -448, -448, -448,\n+            725, 725, 725, 725, 1508, 1508, 1508, 1508,\n+            -961, -961, -961, -961, 398, 398, 398, 398,\n+            951, 951, 951, 951, 247, 247, 247, 247,\n+            1421, 1421, 1421, 1421, -107, -107, -107, -107,\n+            -830, -830, -830, -830, 271, 271, 271, 271,\n+            90, 90, 90, 90, 853, 853, 853, 853,\n+            -1469, -1469, -1469, -1469, -126, -126, -126, -126,\n+            1162, 1162, 1162, 1162, 1618, 1618, 1618, 1618,\n+            666, 666, 666, 666, 320, 320, 320, 320,\n+            8, 8, 8, 8, -516, -516, -516, -516,\n+            1544, 1544, 1544, 1544, 282, 282, 282, 282,\n+            -1491, -1491, -1491, -1491, 1293, 1293, 1293, 1293,\n+            -1015, -1015, -1015, -1015, 552, 552, 552, 552,\n+            -652, -652, -652, -652, -1223, -1223, -1223, -1223,\n+            \/\/ level 2\n+            1571, 1571, 1571, 1571, 1571, 1571, 1571, 1571,\n+            205, 205, 205, 205, 205, 205, 205, 205,\n+            -411, -411, -411, -411, -411, -411, -411, -411,\n+            1542, 1542, 1542, 1542, 1542, 1542, 1542, 1542,\n+            -608, -608, -608, -608, -608, -608, -608, -608,\n+            -732, -732, -732, -732, -732, -732, -732, -732,\n+            -1017, -1017, -1017, -1017, -1017, -1017, -1017, -1017,\n+            681, 681, 681, 681, 681, 681, 681, 681,\n+            130, 130, 130, 130, 130, 130, 130, 130,\n+            1602, 1602, 1602, 1602, 1602, 1602, 1602, 1602,\n+            -1458, -1458, -1458, -1458, -1458, -1458, -1458, -1458,\n+            829, 829, 829, 829, 829, 829, 829, 829,\n+            -383, -383, -383, -383, -383, -383, -383, -383,\n+            -264, -264, -264, -264, -264, -264, -264, -264,\n+            1325, 1325, 1325, 1325, 1325, 1325, 1325, 1325,\n+            -573, -573, -573, -573, -573, -573, -573, -573,\n+            \/\/ level 3\n+            -1468, -1468, -1468, -1468, -1468, -1468, -1468, -1468,\n+            -1468, -1468, -1468, -1468, -1468, -1468, -1468, -1468,\n+            1474, 1474, 1474, 1474, 1474, 1474, 1474, 1474,\n+            1474, 1474, 1474, 1474, 1474, 1474, 1474, 1474,\n+            1202, 1202, 1202, 1202, 1202, 1202, 1202, 1202,\n+            1202, 1202, 1202, 1202, 1202, 1202, 1202, 1202,\n+            -962, -962, -962, -962, -962, -962, -962, -962,\n+            -962, -962, -962, -962, -962, -962, -962, -962,\n+            -182, -182, -182, -182, -182, -182, -182, -182,\n+            -182, -182, -182, -182, -182, -182, -182, -182,\n+            -1577, -1577, -1577, -1577, -1577, -1577, -1577, -1577,\n+            -1577, -1577, -1577, -1577, -1577, -1577, -1577, -1577,\n+            -622, -622, -622, -622, -622, -622, -622, -622,\n+            -622, -622, -622, -622, -622, -622, -622, -622,\n+            171, 171, 171, 171, 171, 171, 171, 171,\n+            171, 171, 171, 171, 171, 171, 171, 171,\n+            \/\/ level 4\n+            -202, -202, -202, -202, -202, -202, -202, -202,\n+            -202, -202, -202, -202, -202, -202, -202, -202,\n+            -202, -202, -202, -202, -202, -202, -202, -202,\n+            -202, -202, -202, -202, -202, -202, -202, -202,\n+            -287, -287, -287, -287, -287, -287, -287, -287,\n+            -287, -287, -287, -287, -287, -287, -287, -287,\n+            -287, -287, -287, -287, -287, -287, -287, -287,\n+            -287, -287, -287, -287, -287, -287, -287, -287,\n+            -1422, -1422, -1422, -1422, -1422, -1422, -1422, -1422,\n+            -1422, -1422, -1422, -1422, -1422, -1422, -1422, -1422,\n+            -1422, -1422, -1422, -1422, -1422, -1422, -1422, -1422,\n+            -1422, -1422, -1422, -1422, -1422, -1422, -1422, -1422,\n+            -1493, -1493, -1493, -1493, -1493, -1493, -1493, -1493,\n+            -1493, -1493, -1493, -1493, -1493, -1493, -1493, -1493,\n+            -1493, -1493, -1493, -1493, -1493, -1493, -1493, -1493,\n+            -1493, -1493, -1493, -1493, -1493, -1493, -1493, -1493,\n+            \/\/ level 5\n+            1517, 1517, 1517, 1517, 1517, 1517, 1517, 1517,\n+            1517, 1517, 1517, 1517, 1517, 1517, 1517, 1517,\n+            1517, 1517, 1517, 1517, 1517, 1517, 1517, 1517,\n+            1517, 1517, 1517, 1517, 1517, 1517, 1517, 1517,\n+            1517, 1517, 1517, 1517, 1517, 1517, 1517, 1517,\n+            1517, 1517, 1517, 1517, 1517, 1517, 1517, 1517,\n+            1517, 1517, 1517, 1517, 1517, 1517, 1517, 1517,\n+            1517, 1517, 1517, 1517, 1517, 1517, 1517, 1517,\n+            359, 359, 359, 359, 359, 359, 359, 359,\n+            359, 359, 359, 359, 359, 359, 359, 359,\n+            359, 359, 359, 359, 359, 359, 359, 359,\n+            359, 359, 359, 359, 359, 359, 359, 359,\n+            359, 359, 359, 359, 359, 359, 359, 359,\n+            359, 359, 359, 359, 359, 359, 359, 359,\n+            359, 359, 359, 359, 359, 359, 359, 359,\n+            359, 359, 359, 359, 359, 359, 359, 359,\n+            \/\/ level 6\n+            758, 758, 758, 758, 758, 758, 758, 758,\n+            758, 758, 758, 758, 758, 758, 758, 758,\n+            758, 758, 758, 758, 758, 758, 758, 758,\n+            758, 758, 758, 758, 758, 758, 758, 758,\n+            758, 758, 758, 758, 758, 758, 758, 758,\n+            758, 758, 758, 758, 758, 758, 758, 758,\n+            758, 758, 758, 758, 758, 758, 758, 758,\n+            758, 758, 758, 758, 758, 758, 758, 758,\n+            758, 758, 758, 758, 758, 758, 758, 758,\n+            758, 758, 758, 758, 758, 758, 758, 758,\n+            758, 758, 758, 758, 758, 758, 758, 758,\n+            758, 758, 758, 758, 758, 758, 758, 758,\n+            758, 758, 758, 758, 758, 758, 758, 758,\n+            758, 758, 758, 758, 758, 758, 758, 758,\n+            758, 758, 758, 758, 758, 758, 758, 758,\n+            758, 758, 758, 758, 758, 758, 758, 758\n+    };\n+\n@@ -92,0 +355,18 @@\n+    private static final short[] montZetasForVectorNttMultArr = new short[]{\n+            -1103, 1103, 430, -430, 555, -555, 843, -843,\n+            -1251, 1251, 871, -871, 1550, -1550, 105, -105,\n+            422, -422, 587, -587, 177, -177, -235, 235,\n+            -291, 291, -460, 460, 1574, -1574, 1653, -1653,\n+            -246, 246, 778, -778, 1159, -1159, -147, 147,\n+            -777, 777, 1483, -1483, -602, 602, 1119, -1119,\n+            -1590, 1590, 644, -644, -872, 872, 349, -349,\n+            418, -418, 329, -329, -156, 156, -75, 75,\n+            817, -817, 1097, -1097, 603, -603, 610, -610,\n+            1322, -1322, -1285, 1285, -1465, 1465, 384, -384,\n+            -1215, 1215, -136, 136, 1218, -1218, -1335, 1335,\n+            -874, 874, 220, -220, -1187, 1187, 1670, 1659,\n+            -1185, 1185, -1530, 1530, -1278, 1278, 794, -794,\n+            -1510, 1510, -854, 854, -870, 870, 478, -478,\n+            -108, 108, -308, 308, 996, -996, 991, -991,\n+            958, -958, -1460, 1460, 1522, -1522, 1628, -1628\n+    };\n@@ -264,1 +545,1 @@\n-        } catch (NoSuchAlgorithmException e){\n+        } catch (NoSuchAlgorithmException e) {\n@@ -371,1 +652,1 @@\n-        mlKemG.update((byte)mlKem_k);\n+\/\/        mlKemG.update((byte)mlKem_k);\n@@ -530,1 +811,1 @@\n-                    xofBufArr[parInd] = seedBuf.clone();\n+                    System.arraycopy(seedBuf, 0, xofBufArr[parInd], 0, seedBuf.length);\n@@ -710,3 +991,7 @@\n-    \/\/ The elements of poly should be in the range [-ML_KEM_Q, ML_KEM_Q]\n-    \/\/ The elements of poly at return will be in the range of [0, ML_KEM_Q]\n-    private void mlKemNTT(short[] poly) {\n+    @IntrinsicCandidate\n+    static int implKyberNtt(short[] poly, short[] ntt_zetas) {\n+        implKyberNttJava(poly);\n+        return 1;\n+    }\n+\n+    static void implKyberNttJava(short[] poly) {\n@@ -721,0 +1006,6 @@\n+    }\n+\n+    \/\/ The elements of poly should be in the range [-mlKem_q, mlKem_q]\n+    \/\/ The elements of poly at return will be in the range of [0, mlKem_q]\n+    private void mlKemNTT(short[] poly) {\n+        implKyberNtt(poly, montZetasForVectorNttArr);\n@@ -724,3 +1015,7 @@\n-    \/\/ Works in place, but also returns its (modified) input so that it can\n-    \/\/ be used in expressions\n-    private short[] mlKemInverseNTT(short[] poly) {\n+    @IntrinsicCandidate\n+    static int implKyberInverseNtt(short[] poly, short[] zetas) {\n+        implKyberInverseNttJava(poly);\n+        return 1;\n+    }\n+\n+    static void implKyberInverseNttJava(short[] poly) {\n@@ -735,0 +1030,6 @@\n+    }\n+\n+    \/\/ Works in place, but also returns its (modified) input so that it can\n+    \/\/ be used in expressions\n+    private short[] mlKemInverseNTT(short[] poly) {\n+        implKyberInverseNtt(poly, montZetasForVectorInverseNttArr);\n@@ -825,4 +1126,8 @@\n-    \/\/ Multiplies two polynomials represented in the NTT domain.\n-    \/\/ The result is a representation of the product still in the NTT domain.\n-    \/\/ The coefficients in the result are in the range (-ML_KEM_Q, ML_KEM_Q).\n-    private void nttMult(short[] result, short[] ntta, short[] nttb) {\n+    @IntrinsicCandidate\n+    static int implKyberNttMult(short[] result, short[] ntta, short[] nttb,\n+                                short[] zetas) {\n+        implKyberNttMultJava(result, ntta, nttb);\n+        return 1;\n+    }\n+\n+    static void implKyberNttMultJava(short[] result, short[] ntta, short[] nttb) {\n@@ -830,0 +1135,1 @@\n+\n@@ -842,0 +1148,7 @@\n+    \/\/ Multiplies two polynomials represented in the NTT domain.\n+    \/\/ The result is a representation of the product still in the NTT domain.\n+    \/\/ The coefficients in the result are in the range (-mlKem_q, mlKem_q).\n+    private void nttMult(short[] result, short[] ntta, short[] nttb) {\n+        implKyberNttMult(result, ntta, nttb, montZetasForVectorNttMultArr);\n+    }\n+\n@@ -856,0 +1169,14 @@\n+    @IntrinsicCandidate\n+    static int implKyberAddPoly(short[] result, short[] a, short[] b) {\n+        implKyberAddPolyJava(result, a, b);\n+        return 1;\n+    }\n+\n+    static void implKyberAddPolyJava(short[] result, short[] a, short[] b) {\n+        for (int m = 0; m < ML_KEM_N; m++) {\n+            int r = a[m] + b[m] + ML_KEM_Q; \/\/ This makes r > - ML_KEM_Q\n+            a[m] = (short) r;\n+        }\n+        mlKemBarrettReduce(a);\n+    }\n+\n@@ -861,1 +1188,12 @@\n-    private void mlKemAddPoly(short[] a, short[] b) {\n+    private short[] mlKemAddPoly(short[] a, short[] b) {\n+                    implKyberAddPoly(a, a, b);\n+        return a;\n+    }\n+\n+    @IntrinsicCandidate\n+    static int implKyberAddPoly(short[] result, short[] a, short[] b, short[] c) {\n+        implKyberAddPolyJava(result, a, b, c);\n+        return 1;\n+    }\n+\n+    static void implKyberAddPolyJava(short[] result, short[] a, short[] b, short[] c) {\n@@ -863,2 +1201,2 @@\n-            int r = a[m] + b[m] + ML_KEM_Q; \/\/ This makes r > -ML_KEM_Q\n-            a[m] = (short) r;\n+            int r = a[m] + b[m] + c[m] + 2 * ML_KEM_Q; \/\/ This makes r > - ML_KEM_Q\n+            result[m] = (short) r;\n@@ -874,4 +1212,1 @@\n-        for (int m = 0; m < ML_KEM_N; m++) {\n-            int r = a[m] + b[m] + c[m] + 2 * ML_KEM_Q; \/\/ This makes r > - ML_KEM_Q\n-            a[m] = (short) r;\n-        }\n+        implKyberAddPoly(a, a, b, c);\n@@ -1000,8 +1335,5 @@\n-    \/\/ The intrinsic implementations assume that the input and output buffers\n-    \/\/ are such that condensed can be read in 192-byte chunks and\n-    \/\/ parsed can be written in 128 shorts chunks. In other words,\n-    \/\/ if (i - 1) * 128 < parsedLengths <= i * 128 then\n-    \/\/ parsed.size should be at least i * 128 and\n-    \/\/ condensed.size should be at least index + i * 192\n-    private void twelve2Sixteen(byte[] condensed, int index,\n-                                short[] parsed, int parsedLength) {\n+    @IntrinsicCandidate\n+    private static int implKyber12To16(byte[] condensed, int index, short[] parsed, int parsedLength) {\n+        implKyber12To16Java(condensed, index, parsed, parsedLength);\n+        return 1;\n+    }\n@@ -1009,0 +1341,1 @@\n+    private static void implKyber12To16Java(byte[] condensed, int index, short[] parsed, int parsedLength) {\n@@ -1017,0 +1350,22 @@\n+    \/\/ The intrinsic implementations assume that the input and output buffers\n+    \/\/ are such that condensed can be read in 96-byte chunks and\n+    \/\/ parsed can be written in 64 shorts chunks except for the last chunk\n+    \/\/ that can be either 48 or 64 shorts. In other words,\n+    \/\/ if (i - 1) * 64 < parsedLengths <= i * 64 then\n+    \/\/ parsed.length should be either i * 64 or (i-1) * 64 + 48 and\n+    \/\/ condensed.length should be at least index + i * 96.\n+    private void twelve2Sixteen(byte[] condensed, int index,\n+                                short[] parsed, int parsedLength) {\n+        int i = parsedLength \/ 64;\n+        int remainder = parsedLength - i * 64;\n+        if (remainder != 0) {\n+            i++;\n+        }\n+        if (((remainder != 0) && (remainder != 48)) ||\n+            index + i * 96 > condensed.length) {\n+            \/\/ this should never happen\n+            throw new ProviderException(\"Bad parameters\");\n+        }\n+        implKyber12To16(condensed, index, parsed, parsedLength);\n+    }\n+\n@@ -1155,0 +1510,13 @@\n+    @IntrinsicCandidate\n+    static int implKyberBarrettReduce(short[] coeffs) {\n+        implKyberBarrettReduceJava(coeffs);\n+        return 1;\n+    }\n+\n+    static void implKyberBarrettReduceJava(short[] poly) {\n+        for (int m = 0; m < ML_KEM_N; m++) {\n+            int tmp = ((int) poly[m] * BARRETT_MULTIPLIER) >> BARRETT_SHIFT;\n+            poly[m] = (short) (poly[m] - tmp * ML_KEM_Q);\n+        }\n+    }\n+\n@@ -1164,5 +1532,2 @@\n-    private void mlKemBarrettReduce(short[] poly) {\n-        for (int m = 0; m < ML_KEM_N; m++) {\n-            int tmp = ((int) poly[m] * BARRETT_MULTIPLIER) >> BARRETT_SHIFT;\n-            poly[m] = (short) (poly[m] - tmp * ML_KEM_Q);\n-        }\n+    private static void mlKemBarrettReduce(short[] poly) {\n+        implKyberBarrettReduce(poly);\n","filename":"src\/java.base\/share\/classes\/com\/sun\/crypto\/provider\/ML_KEM.java","additions":398,"deletions":33,"binary":false,"changes":431,"status":"modified"},{"patch":"@@ -1210,0 +1210,1 @@\n+    \/\/ See e.g. Algorithm 3 in https:\/\/eprint.iacr.org\/2018\/039.pdf\n","filename":"src\/java.base\/share\/classes\/sun\/security\/provider\/ML_DSA.java","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -38,0 +38,14 @@\n+\/*\n+ * This class is for making it possible that NRPAR (= 2) (rather restricted)\n+ * SHAKE computations execute in parallel.\n+ * The restrictions are:\n+ *  1. The messages processed should be such that the absorb phase should\n+ * execute a single keccak() call and the byte arrays passed to the constructor\n+ * (or reset() method) of this class should be the message padded with the\n+ * appropriate padding described in\n+ * https:\/\/nvlpubs.nist.gov\/nistpubs\/fips\/nist.fips.202.pdf.\n+ *  2. The only available way for extracting data is the squeeze() method\n+ * that extracts exactly 1 block of data of each computation, delivering it\n+ * in the arrays that were passed to the class in the constructor (or the\n+ * reset() call).\n+ *\/\n","filename":"src\/java.base\/share\/classes\/sun\/security\/provider\/SHA3Parallel.java","additions":14,"deletions":0,"binary":false,"changes":14,"status":"modified"}]}