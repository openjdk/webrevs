{"files":[{"patch":"@@ -46,1 +46,0 @@\n-#ifdef _LP64\n@@ -49,13 +48,0 @@\n-#else\n-  const Register tmp1 = rcx;\n-  const Register tmp2 = rdx;\n-  __ push(tmp1);\n-  __ push(tmp2);\n-\n-  __ lea(tmp1, safepoint_pc);\n-  __ get_thread(tmp2);\n-  __ movptr(Address(tmp2, JavaThread::saved_exception_pc_offset()), tmp1);\n-\n-  __ pop(tmp2);\n-  __ pop(tmp1);\n-#endif\n","filename":"src\/hotspot\/cpu\/x86\/c2_CodeStubs_x86.cpp","additions":0,"deletions":14,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -110,10 +110,0 @@\n-#ifndef _LP64\n-  \/\/ If method sets FPU control word do it now\n-  if (fp_mode_24b) {\n-    fldcw(ExternalAddress(StubRoutines::x86::addr_fpu_cntrl_wrd_24()));\n-  }\n-  if (UseSSE >= 2 && VerifyFPU) {\n-    verify_FPU(0, \"FPU stack must be clean on entry\");\n-  }\n-#endif\n-\n@@ -136,1 +126,0 @@\n- #ifdef _LP64\n@@ -150,4 +139,0 @@\n-#else\n-    \/\/ Don't bother with out-of-line nmethod entry barrier stub for x86_32.\n-    bs->nmethod_entry_barrier(this, nullptr \/* slow_path *\/, nullptr \/* continuation *\/);\n-#endif\n@@ -302,1 +287,1 @@\n-    andptr(tmpReg, (int32_t) (NOT_LP64(0xFFFFF003) LP64_ONLY(7 - (int)os::vm_page_size())) );\n+    andptr(tmpReg, (int32_t) (7 - (int)os::vm_page_size()) );\n@@ -310,4 +295,0 @@\n-#ifndef _LP64\n-  \/\/ Just take slow path to avoid dealing with 64 bit atomic instructions here.\n-  orl(boxReg, 1);  \/\/ set ICC.ZF=0 to indicate failure\n-#else\n@@ -332,1 +313,0 @@\n-#endif \/\/ _LP64\n@@ -341,1 +321,0 @@\n-#ifdef _LP64\n@@ -344,1 +323,0 @@\n-#endif\n@@ -407,5 +385,0 @@\n-#ifndef _LP64\n-  \/\/ Just take slow path to avoid dealing with 64 bit atomic instructions here.\n-  orl(boxReg, 1);  \/\/ set ICC.ZF=0 to indicate failure\n-  jmpb(DONE_LABEL);\n-#else\n@@ -465,1 +438,0 @@\n-#endif  \/\/ _LP64\n@@ -485,1 +457,0 @@\n-#ifdef _LP64\n@@ -487,1 +458,0 @@\n-#endif\n@@ -566,5 +536,0 @@\n-#ifndef _LP64\n-    \/\/ Just take slow path to avoid dealing with 64 bit atomic instructions here.\n-    orl(box, 1);  \/\/ set ICC.ZF=0 to indicate failure\n-    jmpb(slow_path);\n-#else\n@@ -636,1 +601,0 @@\n-#endif  \/\/ _LP64\n@@ -749,5 +713,0 @@\n-#ifndef _LP64\n-    \/\/ Just take slow path to avoid dealing with 64 bit atomic instructions here.\n-    orl(t, 1);  \/\/ set ICC.ZF=0 to indicate failure\n-    jmpb(slow_path);\n-#else\n@@ -803,1 +762,0 @@\n-#endif  \/\/ _LP64\n@@ -1525,1 +1483,0 @@\n-#ifdef _LP64\n@@ -1564,1 +1521,0 @@\n-#endif \/\/ _LP64\n@@ -1636,1 +1592,1 @@\n-      LP64_ONLY(vgather8b_masked_offset(elem_ty, temp_dst, base, idx_base, offset, mask, mask_idx, rtmp, vlen_enc));\n+      vgather8b_masked_offset(elem_ty, temp_dst, base, idx_base, offset, mask, mask_idx, rtmp, vlen_enc);\n@@ -2040,1 +1996,0 @@\n-#ifdef _LP64\n@@ -2052,1 +2007,0 @@\n-#endif \/\/ _LP64\n@@ -2302,1 +2256,0 @@\n-#ifdef _LP64\n@@ -2328,1 +2281,0 @@\n-#endif \/\/ _LP64\n@@ -2744,1 +2696,0 @@\n-#ifdef _LP64\n@@ -2773,1 +2724,0 @@\n-#endif\n@@ -3701,1 +3651,1 @@\n-  Label COMPARE_WIDE_VECTORS_LOOP_FAILED;  \/\/ used only _LP64 && AVX3\n+  Label COMPARE_WIDE_VECTORS_LOOP_FAILED;  \/\/ used only AVX3\n@@ -3771,1 +3721,1 @@\n-    Label COMPARE_WIDE_VECTORS_LOOP_AVX3;  \/\/ used only _LP64 && AVX3\n+    Label COMPARE_WIDE_VECTORS_LOOP_AVX3;  \/\/ used only AVX3\n@@ -3841,1 +3791,0 @@\n-#ifdef _LP64\n@@ -3865,2 +3814,0 @@\n-#endif \/\/ _LP64\n-\n@@ -4035,1 +3982,0 @@\n-#ifdef _LP64\n@@ -4061,1 +4007,0 @@\n-#endif \/\/ _LP64\n@@ -4136,1 +4081,0 @@\n-#ifdef _LP64\n@@ -4144,25 +4088,0 @@\n-#else\n-    Label k_init;\n-    jmp(k_init);\n-\n-    \/\/ We could not read 64-bits from a general purpose register thus we move\n-    \/\/ data required to compose 64 1's to the instruction stream\n-    \/\/ We emit 64 byte wide series of elements from 0..63 which later on would\n-    \/\/ be used as a compare targets with tail count contained in tmp1 register.\n-    \/\/ Result would be a k register having tmp1 consecutive number or 1\n-    \/\/ counting from least significant bit.\n-    address tmp = pc();\n-    emit_int64(0x0706050403020100);\n-    emit_int64(0x0F0E0D0C0B0A0908);\n-    emit_int64(0x1716151413121110);\n-    emit_int64(0x1F1E1D1C1B1A1918);\n-    emit_int64(0x2726252423222120);\n-    emit_int64(0x2F2E2D2C2B2A2928);\n-    emit_int64(0x3736353433323130);\n-    emit_int64(0x3F3E3D3C3B3A3938);\n-\n-    bind(k_init);\n-    lea(len, InternalAddress(tmp));\n-    \/\/ create mask to test for negative byte inside a vector\n-    evpbroadcastb(vec1, tmp1, Assembler::AVX_512bit);\n-    evpcmpgtb(mask2, vec1, Address(len, 0), Assembler::AVX_512bit);\n@@ -4170,1 +4089,0 @@\n-#endif\n@@ -4417,1 +4335,0 @@\n-#ifdef _LP64\n@@ -4454,1 +4371,1 @@\n-#endif \/\/_LP64\n+\n@@ -4621,2 +4538,0 @@\n-#ifdef _LP64\n-\n@@ -4669,2 +4584,0 @@\n-#endif \/\/ _LP64\n-\n@@ -5330,1 +5243,0 @@\n-#ifdef _LP64\n@@ -5382,1 +5294,0 @@\n-#endif \/\/ _LP64\n@@ -5513,1 +5424,0 @@\n-#ifdef _LP64\n@@ -5771,1 +5681,0 @@\n-#endif\n@@ -5836,4 +5745,2 @@\n-  bool is_LP64 = LP64_ONLY(true) NOT_LP64(false);\n-  if ((is_LP64 || lane_size < 8) &&\n-      ((is_non_subword_integral_type(bt) && VM_Version::supports_avx512vl()) ||\n-       (is_subword_type(bt) && VM_Version::supports_avx512vlbw()))) {\n+  if ((is_non_subword_integral_type(bt) && VM_Version::supports_avx512vl()) ||\n+      (is_subword_type(bt) && VM_Version::supports_avx512vlbw())) {\n@@ -5851,1 +5758,1 @@\n-    LP64_ONLY(movq(dst, rtmp)) NOT_LP64(movdl(dst, rtmp));\n+    movq(dst, rtmp);\n@@ -5986,8 +5893,0 @@\n-#ifndef _LP64\n-void C2_MacroAssembler::vector_maskall_operation32(KRegister dst, Register src, KRegister tmp, int mask_len) {\n-  assert(VM_Version::supports_avx512bw(), \"\");\n-  kmovdl(tmp, src);\n-  kunpckdql(dst, tmp, tmp);\n-}\n-#endif\n-\n@@ -6453,1 +6352,0 @@\n-#ifdef _LP64\n@@ -6617,1 +6515,0 @@\n-#endif\n","filename":"src\/hotspot\/cpu\/x86\/c2_MacroAssembler_x86.cpp","additions":8,"deletions":111,"binary":false,"changes":119,"status":"modified"},{"patch":"@@ -133,1 +133,0 @@\n-#ifdef _LP64\n@@ -135,1 +134,0 @@\n-#endif\n@@ -155,1 +153,0 @@\n-#ifdef _LP64\n@@ -158,1 +155,0 @@\n-#endif \/\/ _LP64\n@@ -205,1 +201,0 @@\n-#ifdef _LP64\n@@ -209,1 +204,0 @@\n-#endif \/\/ _LP64\n@@ -240,1 +234,0 @@\n-#ifdef _LP64\n@@ -249,1 +242,0 @@\n-#endif\n@@ -253,4 +245,0 @@\n-#ifndef _LP64\n-  void vector_maskall_operation32(KRegister dst, Register src, KRegister ktmp, int mask_len);\n-#endif\n-\n@@ -316,1 +304,0 @@\n-#ifdef _LP64\n@@ -318,1 +305,0 @@\n-#endif\n@@ -393,1 +379,0 @@\n-#ifdef _LP64\n@@ -406,1 +391,0 @@\n-#endif \/\/ _LP64\n@@ -412,1 +396,0 @@\n-#ifdef _LP64\n@@ -420,1 +403,0 @@\n-#endif\n@@ -514,1 +496,0 @@\n-#ifdef _LP64\n@@ -517,1 +498,1 @@\n-#endif\n+\n","filename":"src\/hotspot\/cpu\/x86\/c2_MacroAssembler_x86.hpp","additions":1,"deletions":20,"binary":false,"changes":21,"status":"modified"},{"patch":"@@ -62,1 +62,0 @@\n-\n@@ -64,1 +63,0 @@\n-#ifdef _LP64\n@@ -66,3 +64,0 @@\n-#else\n-    return false;\n-#endif\n@@ -71,1 +66,0 @@\n-#ifdef _LP64\n@@ -74,4 +68,0 @@\n-#else\n-  \/\/ Needs 2 CMOV's for longs.\n-  static constexpr int long_cmove_cost() { return 1; }\n-#endif\n@@ -79,1 +69,0 @@\n-#ifdef _LP64\n@@ -82,4 +71,0 @@\n-#else\n-  \/\/ No CMOVF\/CMOVD with SSE\/SSE2\n-  static int float_cmove_cost() { return (UseSSE>=1) ? ConditionalMoveLimit : 0; }\n-#endif\n@@ -88,1 +73,0 @@\n-    NOT_LP64(ShouldNotCallThis();)\n@@ -94,1 +78,0 @@\n-    NOT_LP64(ShouldNotCallThis();)\n@@ -101,1 +84,0 @@\n-    NOT_LP64(ShouldNotCallThis();)\n@@ -108,1 +90,0 @@\n-    NOT_LP64(ShouldNotCallThis();)\n@@ -126,2 +107,0 @@\n-  \/\/ On x32 it is stored with conversion only when FPU is used for floats.\n-#ifdef _LP64\n@@ -131,5 +110,0 @@\n-#else\n-  static bool float_in_double() {\n-    return (UseSSE == 0);\n-  }\n-#endif\n@@ -138,1 +112,0 @@\n-#ifdef _LP64\n@@ -140,4 +113,0 @@\n-#else\n-  static const bool int_in_long = false;\n-#endif\n-\n","filename":"src\/hotspot\/cpu\/x86\/matcher_x86.hpp","additions":0,"deletions":31,"binary":false,"changes":31,"status":"modified"},{"patch":"@@ -213,2 +213,0 @@\n-#ifdef _LP64\n-\n@@ -623,3 +621,0 @@\n-#endif \/\/ _LP64\n-\n-#ifdef _LP64\n@@ -627,3 +622,0 @@\n-#else\n-reg_def RFLAGS(SOC, SOC, 0, 8, VMRegImpl::Bad());\n-#endif \/\/ _LP64\n@@ -661,3 +653,2 @@\n-                   XMM7,  XMM7b,  XMM7c,  XMM7d,  XMM7e,  XMM7f,  XMM7g,  XMM7h,  XMM7i,  XMM7j,  XMM7k,  XMM7l,  XMM7m,  XMM7n,  XMM7o,  XMM7p\n-#ifdef _LP64\n-                  ,XMM8,  XMM8b,  XMM8c,  XMM8d,  XMM8e,  XMM8f,  XMM8g,  XMM8h,  XMM8i,  XMM8j,  XMM8k,  XMM8l,  XMM8m,  XMM8n,  XMM8o,  XMM8p,\n+                   XMM7,  XMM7b,  XMM7c,  XMM7d,  XMM7e,  XMM7f,  XMM7g,  XMM7h,  XMM7i,  XMM7j,  XMM7k,  XMM7l,  XMM7m,  XMM7n,  XMM7o,  XMM7p,\n+                   XMM8,  XMM8b,  XMM8c,  XMM8d,  XMM8e,  XMM8f,  XMM8g,  XMM8h,  XMM8i,  XMM8j,  XMM8k,  XMM8l,  XMM8m,  XMM8n,  XMM8o,  XMM8p,\n@@ -670,2 +661,2 @@\n-                   XMM15, XMM15b, XMM15c, XMM15d, XMM15e, XMM15f, XMM15g, XMM15h, XMM15i, XMM15j, XMM15k, XMM15l, XMM15m, XMM15n, XMM15o, XMM15p\n-                  ,XMM16, XMM16b, XMM16c, XMM16d, XMM16e, XMM16f, XMM16g, XMM16h, XMM16i, XMM16j, XMM16k, XMM16l, XMM16m, XMM16n, XMM16o, XMM16p,\n+                   XMM15, XMM15b, XMM15c, XMM15d, XMM15e, XMM15f, XMM15g, XMM15h, XMM15i, XMM15j, XMM15k, XMM15l, XMM15m, XMM15n, XMM15o, XMM15p,\n+                   XMM16, XMM16b, XMM16c, XMM16d, XMM16e, XMM16f, XMM16g, XMM16h, XMM16i, XMM16j, XMM16k, XMM16l, XMM16m, XMM16n, XMM16o, XMM16p,\n@@ -686,3 +677,1 @@\n-                   XMM31, XMM31b, XMM31c, XMM31d, XMM31e, XMM31f, XMM31g, XMM31h, XMM31i, XMM31j, XMM31k, XMM31l, XMM31m, XMM31n, XMM31o, XMM31p\n-#endif\n-                      );\n+                   XMM31, XMM31b, XMM31c, XMM31d, XMM31e, XMM31f, XMM31g, XMM31h, XMM31i, XMM31j, XMM31k, XMM31l, XMM31m, XMM31n, XMM31o, XMM31p);\n@@ -729,3 +718,2 @@\n-                    XMM7\n-#ifdef _LP64\n-                   ,XMM8,\n+                    XMM7,\n+                    XMM8,\n@@ -738,3 +726,1 @@\n-                    XMM15\n-#endif\n-                    );\n+                    XMM15);\n@@ -750,3 +736,2 @@\n-                    XMM7\n-#ifdef _LP64\n-                   ,XMM8,\n+                    XMM7,\n+                    XMM8,\n@@ -775,3 +760,1 @@\n-                    XMM31\n-#endif\n-                    );\n+                    XMM31);\n@@ -790,3 +773,2 @@\n-                     XMM7,  XMM7b\n-#ifdef _LP64\n-                    ,XMM8,  XMM8b,\n+                     XMM7,  XMM7b,\n+                     XMM8,  XMM8b,\n@@ -799,3 +781,1 @@\n-                     XMM15, XMM15b\n-#endif\n-                     );\n+                     XMM15, XMM15b);\n@@ -811,3 +791,2 @@\n-                     XMM7,  XMM7b\n-#ifdef _LP64\n-                    ,XMM8,  XMM8b,\n+                     XMM7,  XMM7b,\n+                     XMM8,  XMM8b,\n@@ -836,3 +815,1 @@\n-                     XMM31, XMM31b\n-#endif\n-                     );\n+                     XMM31, XMM31b);\n@@ -851,3 +828,2 @@\n-                      XMM7\n-#ifdef _LP64\n-                     ,XMM8,\n+                      XMM7,\n+                      XMM8,\n@@ -860,3 +836,1 @@\n-                      XMM15\n-#endif\n-                      );\n+                      XMM15);\n@@ -872,3 +846,2 @@\n-                      XMM7\n-#ifdef _LP64\n-                     ,XMM8,\n+                      XMM7,\n+                      XMM8,\n@@ -897,3 +870,1 @@\n-                      XMM31\n-#endif\n-                      );\n+                      XMM31);\n@@ -912,3 +883,2 @@\n-                      XMM7,  XMM7b\n-#ifdef _LP64\n-                     ,XMM8,  XMM8b,\n+                      XMM7,  XMM7b,\n+                      XMM8,  XMM8b,\n@@ -921,3 +891,1 @@\n-                      XMM15, XMM15b\n-#endif\n-                      );\n+                      XMM15, XMM15b);\n@@ -933,3 +901,2 @@\n-                      XMM7,  XMM7b\n-#ifdef _LP64\n-                     ,XMM8,  XMM8b,\n+                      XMM7,  XMM7b,\n+                      XMM8,  XMM8b,\n@@ -958,3 +925,1 @@\n-                      XMM31, XMM31b\n-#endif\n-                      );\n+                      XMM31, XMM31b);\n@@ -973,3 +938,2 @@\n-                      XMM7,  XMM7b,  XMM7c,  XMM7d\n-#ifdef _LP64\n-                     ,XMM8,  XMM8b,  XMM8c,  XMM8d,\n+                      XMM7,  XMM7b,  XMM7c,  XMM7d,\n+                      XMM8,  XMM8b,  XMM8c,  XMM8d,\n@@ -982,3 +946,1 @@\n-                      XMM15, XMM15b, XMM15c, XMM15d\n-#endif\n-                      );\n+                      XMM15, XMM15b, XMM15c, XMM15d);\n@@ -994,3 +956,2 @@\n-                      XMM7,  XMM7b,  XMM7c,  XMM7d\n-#ifdef _LP64\n-                     ,XMM8,  XMM8b,  XMM8c,  XMM8d,\n+                      XMM7,  XMM7b,  XMM7c,  XMM7d,\n+                      XMM8,  XMM8b,  XMM8c,  XMM8d,\n@@ -1019,3 +980,1 @@\n-                      XMM31, XMM31b, XMM31c, XMM31d\n-#endif\n-                      );\n+                      XMM31, XMM31b, XMM31c, XMM31d);\n@@ -1034,3 +993,2 @@\n-                      XMM7,  XMM7b,  XMM7c,  XMM7d,  XMM7e,  XMM7f,  XMM7g,  XMM7h\n-#ifdef _LP64\n-                     ,XMM8,  XMM8b,  XMM8c,  XMM8d,  XMM8e,  XMM8f,  XMM8g,  XMM8h,\n+                      XMM7,  XMM7b,  XMM7c,  XMM7d,  XMM7e,  XMM7f,  XMM7g,  XMM7h,\n+                      XMM8,  XMM8b,  XMM8c,  XMM8d,  XMM8e,  XMM8f,  XMM8g,  XMM8h,\n@@ -1043,3 +1001,1 @@\n-                      XMM15, XMM15b, XMM15c, XMM15d, XMM15e, XMM15f, XMM15g, XMM15h\n-#endif\n-                      );\n+                      XMM15, XMM15b, XMM15c, XMM15d, XMM15e, XMM15f, XMM15g, XMM15h);\n@@ -1055,3 +1011,2 @@\n-                      XMM7,  XMM7b,  XMM7c,  XMM7d,  XMM7e,  XMM7f,  XMM7g,  XMM7h\n-#ifdef _LP64\n-                     ,XMM8,  XMM8b,  XMM8c,  XMM8d,  XMM8e,  XMM8f,  XMM8g,  XMM8h,\n+                      XMM7,  XMM7b,  XMM7c,  XMM7d,  XMM7e,  XMM7f,  XMM7g,  XMM7h,\n+                      XMM8,  XMM8b,  XMM8c,  XMM8d,  XMM8e,  XMM8f,  XMM8g,  XMM8h,\n@@ -1080,3 +1035,1 @@\n-                      XMM31, XMM31b, XMM31c, XMM31d, XMM31e, XMM31f, XMM31g, XMM31h\n-#endif\n-                      );\n+                      XMM31, XMM31b, XMM31c, XMM31d, XMM31e, XMM31f, XMM31g, XMM31h);\n@@ -1095,3 +1048,2 @@\n-                      XMM7,  XMM7b,  XMM7c,  XMM7d,  XMM7e,  XMM7f,  XMM7g,  XMM7h,  XMM7i,  XMM7j,  XMM7k,  XMM7l,  XMM7m,  XMM7n,  XMM7o,  XMM7p\n-#ifdef _LP64\n-                     ,XMM8,  XMM8b,  XMM8c,  XMM8d,  XMM8e,  XMM8f,  XMM8g,  XMM8h,  XMM8i,  XMM8j,  XMM8k,  XMM8l,  XMM8m,  XMM8n,  XMM8o,  XMM8p,\n+                      XMM7,  XMM7b,  XMM7c,  XMM7d,  XMM7e,  XMM7f,  XMM7g,  XMM7h,  XMM7i,  XMM7j,  XMM7k,  XMM7l,  XMM7m,  XMM7n,  XMM7o,  XMM7p,\n+                      XMM8,  XMM8b,  XMM8c,  XMM8d,  XMM8e,  XMM8f,  XMM8g,  XMM8h,  XMM8i,  XMM8j,  XMM8k,  XMM8l,  XMM8m,  XMM8n,  XMM8o,  XMM8p,\n@@ -1104,2 +1056,2 @@\n-                      XMM15, XMM15b, XMM15c, XMM15d, XMM15e, XMM15f, XMM15g, XMM15h, XMM15i, XMM15j, XMM15k, XMM15l, XMM15m, XMM15n, XMM15o, XMM15p\n-                     ,XMM16, XMM16b, XMM16c, XMM16d, XMM16e, XMM16f, XMM16g, XMM16h, XMM16i, XMM16j, XMM16k, XMM16l, XMM16m, XMM16n, XMM16o, XMM16p,\n+                      XMM15, XMM15b, XMM15c, XMM15d, XMM15e, XMM15f, XMM15g, XMM15h, XMM15i, XMM15j, XMM15k, XMM15l, XMM15m, XMM15n, XMM15o, XMM15p,\n+                      XMM16, XMM16b, XMM16c, XMM16d, XMM16e, XMM16f, XMM16g, XMM16h, XMM16i, XMM16j, XMM16k, XMM16l, XMM16m, XMM16n, XMM16o, XMM16p,\n@@ -1120,3 +1072,1 @@\n-                      XMM31, XMM31b, XMM31c, XMM31d, XMM31e, XMM31f, XMM31g, XMM31h, XMM31i, XMM31j, XMM31k, XMM31l, XMM31m, XMM31n, XMM31o, XMM31p\n-#endif\n-                      );\n+                      XMM31, XMM31b, XMM31c, XMM31d, XMM31e, XMM31f, XMM31g, XMM31h, XMM31i, XMM31j, XMM31k, XMM31l, XMM31m, XMM31n, XMM31o, XMM31p);\n@@ -1132,3 +1082,2 @@\n-                      XMM7,  XMM7b,  XMM7c,  XMM7d,  XMM7e,  XMM7f,  XMM7g,  XMM7h,  XMM7i,  XMM7j,  XMM7k,  XMM7l,  XMM7m,  XMM7n,  XMM7o,  XMM7p\n-#ifdef _LP64\n-                     ,XMM8,  XMM8b,  XMM8c,  XMM8d,  XMM8e,  XMM8f,  XMM8g,  XMM8h,  XMM8i,  XMM8j,  XMM8k,  XMM8l,  XMM8m,  XMM8n,  XMM8o,  XMM8p,\n+                      XMM7,  XMM7b,  XMM7c,  XMM7d,  XMM7e,  XMM7f,  XMM7g,  XMM7h,  XMM7i,  XMM7j,  XMM7k,  XMM7l,  XMM7m,  XMM7n,  XMM7o,  XMM7p,\n+                      XMM8,  XMM8b,  XMM8c,  XMM8d,  XMM8e,  XMM8f,  XMM8g,  XMM8h,  XMM8i,  XMM8j,  XMM8k,  XMM8l,  XMM8m,  XMM8n,  XMM8o,  XMM8p,\n@@ -1141,3 +1090,1 @@\n-                      XMM15, XMM15b, XMM15c, XMM15d, XMM15e, XMM15f, XMM15g, XMM15h, XMM15i, XMM15j, XMM15k, XMM15l, XMM15m, XMM15n, XMM15o, XMM15p\n-#endif\n-                      );\n+                      XMM15, XMM15b, XMM15c, XMM15d, XMM15e, XMM15f, XMM15g, XMM15h, XMM15i, XMM15j, XMM15k, XMM15l, XMM15m, XMM15n, XMM15o, XMM15p);\n@@ -1202,1 +1149,0 @@\n-#ifdef _LP64\n@@ -1207,10 +1153,0 @@\n-#else\n-  static uint size_deopt_handler() {\n-    \/\/ NativeCall instruction size is the same as NativeJump.\n-    \/\/ exception handler starts out as jump and can be patched to\n-    \/\/ a call be deoptimization.  (4932387)\n-    \/\/ Note that this value is also credited (in output.cpp) to\n-    \/\/ the size of the code section.\n-    return 5 + NativeJump::instruction_size; \/\/ pushl(); jmp;\n-  }\n-#endif\n@@ -1337,1 +1273,0 @@\n-#ifdef _LP64\n@@ -1348,4 +1283,0 @@\n-#else\n-  InternalAddress here(__ pc());\n-  __ pushptr(here.addr(), noreg);\n-#endif\n@@ -1375,1 +1306,0 @@\n-#ifdef _LP64\n@@ -1380,6 +1310,0 @@\n-#else\n-  static address float_signmask()  { return (address)float_signmask_pool; }\n-  static address float_signflip()  { return (address)float_signflip_pool; }\n-  static address double_signmask() { return (address)double_signmask_pool; }\n-  static address double_signflip() { return (address)double_signflip_pool; }\n-#endif\n@@ -1407,1 +1331,0 @@\n-  const bool is_LP64 = LP64_ONLY(true) NOT_LP64(false);\n@@ -1509,1 +1432,1 @@\n-      if (!is_LP64 || (UseAVX < 2)) {\n+      if (UseAVX < 2) {\n@@ -1524,1 +1447,0 @@\n-#ifdef _LP64\n@@ -1526,1 +1448,0 @@\n-#endif\n@@ -1555,1 +1476,0 @@\n-#ifdef _LP64\n@@ -1564,1 +1484,0 @@\n-#endif\n@@ -1607,1 +1526,1 @@\n-      if (!is_LP64  || UseAVX < 3 || !VM_Version::supports_bmi2()) {\n+      if (UseAVX < 3 || !VM_Version::supports_bmi2()) {\n@@ -1615,1 +1534,1 @@\n-      if (!is_LP64 || UseAVX < 1) {\n+      if (UseAVX < 1) {\n@@ -1621,3 +1540,0 @@\n-      if (!is_LP64) {\n-        return false;\n-      }\n@@ -1627,1 +1543,1 @@\n-      if (UseAVX < 3 || !is_LP64)  {\n+      if (UseAVX < 3)  {\n@@ -1634,18 +1550,0 @@\n-#ifndef _LP64\n-    case Op_AddReductionVF:\n-    case Op_AddReductionVD:\n-    case Op_MulReductionVF:\n-    case Op_MulReductionVD:\n-      if (UseSSE < 1) { \/\/ requires at least SSE\n-        return false;\n-      }\n-      break;\n-    case Op_MulAddVS2VI:\n-    case Op_RShiftVL:\n-    case Op_AbsVD:\n-    case Op_NegVD:\n-      if (UseSSE < 2) {\n-        return false;\n-      }\n-      break;\n-#endif \/\/ !LP64\n@@ -1653,4 +1551,0 @@\n-      if (!VM_Version::supports_bmi2() || (!is_LP64 && UseSSE < 2)) {\n-        return false;\n-      }\n-      break;\n@@ -1658,1 +1552,1 @@\n-      if (!VM_Version::supports_bmi2() || (!is_LP64 && (UseSSE < 2 || !VM_Version::supports_bmi1()))) {\n+      if (!VM_Version::supports_bmi2()) {\n@@ -1683,1 +1577,0 @@\n-#ifdef _LP64\n@@ -1687,4 +1580,0 @@\n-#else\n-      \/\/ x86_32.ad has a special match rule for SqrtD.\n-      \/\/ Together with common x86 rules, this handles all UseSSE cases.\n-#endif\n@@ -1722,1 +1611,0 @@\n-  const bool is_LP64 = LP64_ONLY(true) NOT_LP64(false);\n@@ -1769,1 +1657,1 @@\n-      if (!is_LP64 || !VM_Version::supports_avx512bw()) {\n+      if (!VM_Version::supports_avx512bw()) {\n@@ -1819,11 +1707,0 @@\n-#ifndef _LP64\n-      if (bt == T_BYTE || bt == T_LONG) {\n-        return false;\n-      }\n-#endif\n-      break;\n-#ifndef _LP64\n-    case Op_VectorInsert:\n-      if (bt == T_LONG || bt == T_DOUBLE) {\n-        return false;\n-      }\n@@ -1831,1 +1708,0 @@\n-#endif\n@@ -1846,5 +1722,0 @@\n-#ifndef _LP64\n-      if (bt == T_BYTE || bt == T_LONG) {\n-        return false;\n-      }\n-#endif\n@@ -1935,4 +1806,3 @@\n-         (!is_LP64                                                ||\n-         (size_in_bits > 256 && !VM_Version::supports_avx512bw()) ||\n-         (size_in_bits < 64)                                      ||\n-         (bt == T_SHORT && !VM_Version::supports_bmi2()))) {\n+         ((size_in_bits > 256 && !VM_Version::supports_avx512bw()) ||\n+          (size_in_bits < 64)                                      ||\n+          (bt == T_SHORT && !VM_Version::supports_bmi2()))) {\n@@ -2007,3 +1877,0 @@\n-      if (!is_LP64 && !VM_Version::supports_avx512vl() && size_in_bits < 512) {\n-        return false;\n-      }\n@@ -2014,1 +1881,1 @@\n-      if (UseAVX < 1 || !is_LP64) {\n+      if (UseAVX < 1) {\n@@ -2062,1 +1929,0 @@\n-  const bool is_LP64 = LP64_ONLY(true) NOT_LP64(false);\n@@ -2398,1 +2264,0 @@\n-#ifdef _LP64\n@@ -2408,2 +2273,1 @@\n-    } else\n-#endif\n+    } else {\n@@ -2411,0 +2275,1 @@\n+    }\n@@ -2548,1 +2413,1 @@\n-        LP64_ONLY( off->get_long() == (int) (off->get_long()) && ) \/\/ immL32\n+        off->get_long() == (int) (off->get_long()) && \/\/ immL32\n@@ -2622,3 +2487,0 @@\n-#ifndef _LP64\n-      __ movdqu(as_XMMRegister(Matcher::_regEncode[dst_lo]), as_XMMRegister(Matcher::_regEncode[src_lo]));\n-#else\n@@ -2630,1 +2492,0 @@\n-#endif\n@@ -2633,3 +2494,0 @@\n-#ifndef _LP64\n-      __ vmovdqu(as_XMMRegister(Matcher::_regEncode[dst_lo]), as_XMMRegister(Matcher::_regEncode[src_lo]));\n-#else\n@@ -2641,1 +2499,0 @@\n-#endif\n@@ -2680,1 +2537,0 @@\n-#ifndef _LP64\n@@ -2682,8 +2538,0 @@\n-#else\n-        if ((UseAVX < 3) || VM_Version::supports_avx512vl()) {\n-          __ movdqu(as_XMMRegister(Matcher::_regEncode[reg]), Address(rsp, stack_offset));\n-        } else {\n-          __ vpxor(as_XMMRegister(Matcher::_regEncode[reg]), as_XMMRegister(Matcher::_regEncode[reg]), as_XMMRegister(Matcher::_regEncode[reg]), 2);\n-          __ vinsertf32x4(as_XMMRegister(Matcher::_regEncode[reg]), as_XMMRegister(Matcher::_regEncode[reg]), Address(rsp, stack_offset),0x0);\n-        }\n-#endif\n@@ -2692,1 +2540,0 @@\n-#ifndef _LP64\n@@ -2694,8 +2541,0 @@\n-#else\n-        if ((UseAVX < 3) || VM_Version::supports_avx512vl()) {\n-          __ vmovdqu(as_XMMRegister(Matcher::_regEncode[reg]), Address(rsp, stack_offset));\n-        } else {\n-          __ vpxor(as_XMMRegister(Matcher::_regEncode[reg]), as_XMMRegister(Matcher::_regEncode[reg]), as_XMMRegister(Matcher::_regEncode[reg]), 2);\n-          __ vinsertf64x4(as_XMMRegister(Matcher::_regEncode[reg]), as_XMMRegister(Matcher::_regEncode[reg]), Address(rsp, stack_offset),0x0);\n-        }\n-#endif\n@@ -2718,1 +2557,0 @@\n-#ifndef _LP64\n@@ -2720,8 +2558,0 @@\n-#else\n-        if ((UseAVX < 3) || VM_Version::supports_avx512vl()) {\n-          __ movdqu(Address(rsp, stack_offset), as_XMMRegister(Matcher::_regEncode[reg]));\n-        }\n-        else {\n-          __ vextractf32x4(Address(rsp, stack_offset), as_XMMRegister(Matcher::_regEncode[reg]), 0x0);\n-        }\n-#endif\n@@ -2730,1 +2560,0 @@\n-#ifndef _LP64\n@@ -2732,8 +2561,0 @@\n-#else\n-        if ((UseAVX < 3) || VM_Version::supports_avx512vl()) {\n-          __ vmovdqu(Address(rsp, stack_offset), as_XMMRegister(Matcher::_regEncode[reg]));\n-        }\n-        else {\n-          __ vextractf64x4(Address(rsp, stack_offset), as_XMMRegister(Matcher::_regEncode[reg]), 0x0);\n-        }\n-#endif\n@@ -3970,1 +3791,0 @@\n-#ifdef _LP64\n@@ -4041,1 +3861,0 @@\n-#endif \/\/ _LP64\n@@ -4259,1 +4078,0 @@\n-#ifdef _LP64\n@@ -4422,1 +4240,0 @@\n-#endif\n@@ -4538,1 +4355,0 @@\n-#ifdef _LP64\n@@ -4565,1 +4381,0 @@\n-#endif\n@@ -4662,1 +4477,0 @@\n-#ifdef _LP64\n@@ -4683,55 +4497,0 @@\n-#else \/\/ _LP64\n-\/\/ Replicate long (8 byte) scalar to be vector\n-instruct ReplL_reg(vec dst, eRegL src, vec tmp) %{\n-  predicate(Matcher::vector_length(n) <= 4 && Matcher::vector_element_basic_type(n) == T_LONG);\n-  match(Set dst (Replicate src));\n-  effect(TEMP dst, USE src, TEMP tmp);\n-  format %{ \"replicateL $dst,$src\" %}\n-  ins_encode %{\n-    uint vlen = Matcher::vector_length(this);\n-    if (vlen == 2) {\n-      __ movdl($dst$$XMMRegister, $src$$Register);\n-      __ movdl($tmp$$XMMRegister, HIGH_FROM_LOW($src$$Register));\n-      __ punpckldq($dst$$XMMRegister, $tmp$$XMMRegister);\n-      __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);\n-    } else if (VM_Version::supports_avx512vl()) { \/\/ AVX512VL for <512bit operands\n-      int vlen_enc = Assembler::AVX_256bit;\n-      __ movdl($dst$$XMMRegister, $src$$Register);\n-      __ movdl($tmp$$XMMRegister, HIGH_FROM_LOW($src$$Register));\n-      __ punpckldq($dst$$XMMRegister, $tmp$$XMMRegister);\n-      __ vpbroadcastq($dst$$XMMRegister, $dst$$XMMRegister, vlen_enc);\n-    } else {\n-      __ movdl($dst$$XMMRegister, $src$$Register);\n-      __ movdl($tmp$$XMMRegister, HIGH_FROM_LOW($src$$Register));\n-      __ punpckldq($dst$$XMMRegister, $tmp$$XMMRegister);\n-      __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);\n-      __ vinserti128_high($dst$$XMMRegister, $dst$$XMMRegister);\n-    }\n-  %}\n-  ins_pipe( pipe_slow );\n-%}\n-\n-instruct ReplL_reg_leg(legVec dst, eRegL src, legVec tmp) %{\n-  predicate(Matcher::vector_length(n) == 8 && Matcher::vector_element_basic_type(n) == T_LONG);\n-  match(Set dst (Replicate src));\n-  effect(TEMP dst, USE src, TEMP tmp);\n-  format %{ \"replicateL $dst,$src\" %}\n-  ins_encode %{\n-    if (VM_Version::supports_avx512vl()) {\n-      __ movdl($dst$$XMMRegister, $src$$Register);\n-      __ movdl($tmp$$XMMRegister, HIGH_FROM_LOW($src$$Register));\n-      __ punpckldq($dst$$XMMRegister, $tmp$$XMMRegister);\n-      __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);\n-      __ vinserti128_high($dst$$XMMRegister, $dst$$XMMRegister);\n-      __ vinserti64x4($dst$$XMMRegister, $dst$$XMMRegister, $dst$$XMMRegister, 0x1);\n-    } else {\n-      int vlen_enc = Assembler::AVX_512bit;\n-      __ movdl($dst$$XMMRegister, $src$$Register);\n-      __ movdl($tmp$$XMMRegister, HIGH_FROM_LOW($src$$Register));\n-      __ punpckldq($dst$$XMMRegister, $tmp$$XMMRegister);\n-      __ vpbroadcastq($dst$$XMMRegister, $dst$$XMMRegister, vlen_enc);\n-    }\n-  %}\n-  ins_pipe( pipe_slow );\n-%}\n-#endif \/\/ _LP64\n@@ -5011,1 +4770,0 @@\n-#ifdef _LP64\n@@ -5062,1 +4820,0 @@\n-#endif\n@@ -5108,1 +4865,0 @@\n-#ifdef _LP64\n@@ -5163,1 +4919,0 @@\n-#endif\n@@ -5190,1 +4945,0 @@\n-#ifdef _LP64\n@@ -5228,1 +4982,0 @@\n-#endif \/\/ _LP64\n@@ -5440,1 +5193,0 @@\n-#ifdef _LP64\n@@ -5476,1 +5228,0 @@\n-#endif\n@@ -6777,1 +6528,0 @@\n-#ifdef _LP64\n@@ -6803,2 +6553,0 @@\n-#endif \/\/ _LP64\n-\n@@ -7977,1 +7725,0 @@\n-#ifdef _LP64\n@@ -8027,2 +7774,0 @@\n-#endif \/\/ _LP64\n-\n@@ -8238,1 +7983,0 @@\n-#ifdef _LP64\n@@ -8240,1 +7984,0 @@\n-#endif\n@@ -8256,1 +7999,0 @@\n-#ifdef _LP64\n@@ -8258,1 +8000,0 @@\n-#endif\n@@ -8271,1 +8012,0 @@\n-#ifdef _LP64\n@@ -8299,1 +8039,0 @@\n-#endif\n@@ -8564,1 +8303,0 @@\n-#ifdef _LP64\n@@ -8632,1 +8370,0 @@\n-#endif\n@@ -8883,1 +8620,0 @@\n-#ifdef _LP64\n@@ -8915,1 +8651,1 @@\n-#endif\n+\n@@ -9496,1 +9232,0 @@\n-#ifdef _LP64\n@@ -9710,1 +9445,0 @@\n-#ifdef _LP64\n@@ -9726,1 +9460,0 @@\n-#endif\n@@ -9754,2 +9487,0 @@\n-#endif \/\/ _LP64\n-\n@@ -10476,1 +10207,0 @@\n-#ifdef _LP64\n@@ -10541,1 +10271,0 @@\n-#endif\n","filename":"src\/hotspot\/cpu\/x86\/x86.ad","additions":63,"deletions":334,"binary":false,"changes":397,"status":"modified"}]}