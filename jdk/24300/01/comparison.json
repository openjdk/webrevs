{"files":[{"patch":"@@ -46,1 +46,0 @@\n-#ifdef _LP64\n@@ -49,13 +48,0 @@\n-#else\n-  const Register tmp1 = rcx;\n-  const Register tmp2 = rdx;\n-  __ push(tmp1);\n-  __ push(tmp2);\n-\n-  __ lea(tmp1, safepoint_pc);\n-  __ get_thread(tmp2);\n-  __ movptr(Address(tmp2, JavaThread::saved_exception_pc_offset()), tmp1);\n-\n-  __ pop(tmp2);\n-  __ pop(tmp1);\n-#endif\n","filename":"src\/hotspot\/cpu\/x86\/c2_CodeStubs_x86.cpp","additions":0,"deletions":14,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -110,10 +110,0 @@\n-#ifndef _LP64\n-  \/\/ If method sets FPU control word do it now\n-  if (fp_mode_24b) {\n-    fldcw(ExternalAddress(StubRoutines::x86::addr_fpu_cntrl_wrd_24()));\n-  }\n-  if (UseSSE >= 2 && VerifyFPU) {\n-    verify_FPU(0, \"FPU stack must be clean on entry\");\n-  }\n-#endif\n-\n@@ -136,1 +126,0 @@\n- #ifdef _LP64\n@@ -150,4 +139,0 @@\n-#else\n-    \/\/ Don't bother with out-of-line nmethod entry barrier stub for x86_32.\n-    bs->nmethod_entry_barrier(this, nullptr \/* slow_path *\/, nullptr \/* continuation *\/);\n-#endif\n@@ -302,1 +287,1 @@\n-    andptr(tmpReg, (int32_t) (NOT_LP64(0xFFFFF003) LP64_ONLY(7 - (int)os::vm_page_size())) );\n+    andptr(tmpReg, (int32_t) (7 - (int)os::vm_page_size()) );\n@@ -310,4 +295,0 @@\n-#ifndef _LP64\n-  \/\/ Just take slow path to avoid dealing with 64 bit atomic instructions here.\n-  orl(boxReg, 1);  \/\/ set ICC.ZF=0 to indicate failure\n-#else\n@@ -332,1 +313,0 @@\n-#endif \/\/ _LP64\n@@ -341,1 +321,0 @@\n-#ifdef _LP64\n@@ -344,1 +323,0 @@\n-#endif\n@@ -407,5 +385,0 @@\n-#ifndef _LP64\n-  \/\/ Just take slow path to avoid dealing with 64 bit atomic instructions here.\n-  orl(boxReg, 1);  \/\/ set ICC.ZF=0 to indicate failure\n-  jmpb(DONE_LABEL);\n-#else\n@@ -465,1 +438,0 @@\n-#endif  \/\/ _LP64\n@@ -485,1 +457,0 @@\n-#ifdef _LP64\n@@ -487,1 +458,0 @@\n-#endif\n@@ -566,5 +536,0 @@\n-#ifndef _LP64\n-    \/\/ Just take slow path to avoid dealing with 64 bit atomic instructions here.\n-    orl(box, 1);  \/\/ set ICC.ZF=0 to indicate failure\n-    jmpb(slow_path);\n-#else\n@@ -636,1 +601,0 @@\n-#endif  \/\/ _LP64\n@@ -749,5 +713,0 @@\n-#ifndef _LP64\n-    \/\/ Just take slow path to avoid dealing with 64 bit atomic instructions here.\n-    orl(t, 1);  \/\/ set ICC.ZF=0 to indicate failure\n-    jmpb(slow_path);\n-#else\n@@ -803,1 +762,0 @@\n-#endif  \/\/ _LP64\n@@ -1525,1 +1483,0 @@\n-#ifdef _LP64\n@@ -1564,1 +1521,0 @@\n-#endif \/\/ _LP64\n@@ -1636,1 +1592,1 @@\n-      LP64_ONLY(vgather8b_masked_offset(elem_ty, temp_dst, base, idx_base, offset, mask, mask_idx, rtmp, vlen_enc));\n+      vgather8b_masked_offset(elem_ty, temp_dst, base, idx_base, offset, mask, mask_idx, rtmp, vlen_enc);\n@@ -2040,1 +1996,0 @@\n-#ifdef _LP64\n@@ -2052,1 +2007,0 @@\n-#endif \/\/ _LP64\n@@ -2302,1 +2256,0 @@\n-#ifdef _LP64\n@@ -2328,1 +2281,0 @@\n-#endif \/\/ _LP64\n@@ -2744,1 +2696,0 @@\n-#ifdef _LP64\n@@ -2773,1 +2724,0 @@\n-#endif\n@@ -3701,1 +3651,1 @@\n-  Label COMPARE_WIDE_VECTORS_LOOP_FAILED;  \/\/ used only _LP64 && AVX3\n+  Label COMPARE_WIDE_VECTORS_LOOP_FAILED;  \/\/ used only AVX3\n@@ -3771,1 +3721,1 @@\n-    Label COMPARE_WIDE_VECTORS_LOOP_AVX3;  \/\/ used only _LP64 && AVX3\n+    Label COMPARE_WIDE_VECTORS_LOOP_AVX3;  \/\/ used only AVX3\n@@ -3841,1 +3791,0 @@\n-#ifdef _LP64\n@@ -3865,2 +3814,0 @@\n-#endif \/\/ _LP64\n-\n@@ -4035,1 +3982,0 @@\n-#ifdef _LP64\n@@ -4061,1 +4007,0 @@\n-#endif \/\/ _LP64\n@@ -4136,1 +4081,0 @@\n-#ifdef _LP64\n@@ -4144,25 +4088,0 @@\n-#else\n-    Label k_init;\n-    jmp(k_init);\n-\n-    \/\/ We could not read 64-bits from a general purpose register thus we move\n-    \/\/ data required to compose 64 1's to the instruction stream\n-    \/\/ We emit 64 byte wide series of elements from 0..63 which later on would\n-    \/\/ be used as a compare targets with tail count contained in tmp1 register.\n-    \/\/ Result would be a k register having tmp1 consecutive number or 1\n-    \/\/ counting from least significant bit.\n-    address tmp = pc();\n-    emit_int64(0x0706050403020100);\n-    emit_int64(0x0F0E0D0C0B0A0908);\n-    emit_int64(0x1716151413121110);\n-    emit_int64(0x1F1E1D1C1B1A1918);\n-    emit_int64(0x2726252423222120);\n-    emit_int64(0x2F2E2D2C2B2A2928);\n-    emit_int64(0x3736353433323130);\n-    emit_int64(0x3F3E3D3C3B3A3938);\n-\n-    bind(k_init);\n-    lea(len, InternalAddress(tmp));\n-    \/\/ create mask to test for negative byte inside a vector\n-    evpbroadcastb(vec1, tmp1, Assembler::AVX_512bit);\n-    evpcmpgtb(mask2, vec1, Address(len, 0), Assembler::AVX_512bit);\n@@ -4170,1 +4089,0 @@\n-#endif\n@@ -4417,1 +4335,0 @@\n-#ifdef _LP64\n@@ -4454,1 +4371,1 @@\n-#endif \/\/_LP64\n+\n@@ -4621,2 +4538,0 @@\n-#ifdef _LP64\n-\n@@ -4669,2 +4584,0 @@\n-#endif \/\/ _LP64\n-\n@@ -5330,1 +5243,0 @@\n-#ifdef _LP64\n@@ -5382,1 +5294,0 @@\n-#endif \/\/ _LP64\n@@ -5513,1 +5424,0 @@\n-#ifdef _LP64\n@@ -5771,1 +5681,0 @@\n-#endif\n@@ -5836,4 +5745,2 @@\n-  bool is_LP64 = LP64_ONLY(true) NOT_LP64(false);\n-  if ((is_LP64 || lane_size < 8) &&\n-      ((is_non_subword_integral_type(bt) && VM_Version::supports_avx512vl()) ||\n-       (is_subword_type(bt) && VM_Version::supports_avx512vlbw()))) {\n+  if ((is_non_subword_integral_type(bt) && VM_Version::supports_avx512vl()) ||\n+      (is_subword_type(bt) && VM_Version::supports_avx512vlbw())) {\n@@ -5851,1 +5758,1 @@\n-    LP64_ONLY(movq(dst, rtmp)) NOT_LP64(movdl(dst, rtmp));\n+    movq(dst, rtmp);\n@@ -5986,8 +5893,0 @@\n-#ifndef _LP64\n-void C2_MacroAssembler::vector_maskall_operation32(KRegister dst, Register src, KRegister tmp, int mask_len) {\n-  assert(VM_Version::supports_avx512bw(), \"\");\n-  kmovdl(tmp, src);\n-  kunpckdql(dst, tmp, tmp);\n-}\n-#endif\n-\n@@ -6453,1 +6352,0 @@\n-#ifdef _LP64\n@@ -6617,1 +6515,0 @@\n-#endif\n","filename":"src\/hotspot\/cpu\/x86\/c2_MacroAssembler_x86.cpp","additions":8,"deletions":111,"binary":false,"changes":119,"status":"modified"},{"patch":"@@ -133,1 +133,0 @@\n-#ifdef _LP64\n@@ -135,1 +134,0 @@\n-#endif\n@@ -155,1 +153,0 @@\n-#ifdef _LP64\n@@ -158,1 +155,0 @@\n-#endif \/\/ _LP64\n@@ -205,1 +201,0 @@\n-#ifdef _LP64\n@@ -209,1 +204,0 @@\n-#endif \/\/ _LP64\n@@ -240,1 +234,0 @@\n-#ifdef _LP64\n@@ -249,1 +242,0 @@\n-#endif\n@@ -253,4 +245,0 @@\n-#ifndef _LP64\n-  void vector_maskall_operation32(KRegister dst, Register src, KRegister ktmp, int mask_len);\n-#endif\n-\n@@ -316,1 +304,0 @@\n-#ifdef _LP64\n@@ -318,1 +305,0 @@\n-#endif\n@@ -393,1 +379,0 @@\n-#ifdef _LP64\n@@ -406,1 +391,0 @@\n-#endif \/\/ _LP64\n@@ -412,1 +396,0 @@\n-#ifdef _LP64\n@@ -420,1 +403,0 @@\n-#endif\n@@ -514,1 +496,0 @@\n-#ifdef _LP64\n@@ -517,1 +498,1 @@\n-#endif\n+\n","filename":"src\/hotspot\/cpu\/x86\/c2_MacroAssembler_x86.hpp","additions":1,"deletions":20,"binary":false,"changes":21,"status":"modified"},{"patch":"@@ -62,1 +62,0 @@\n-\n@@ -64,1 +63,0 @@\n-#ifdef _LP64\n@@ -66,3 +64,0 @@\n-#else\n-    return false;\n-#endif\n@@ -71,1 +66,0 @@\n-#ifdef _LP64\n@@ -74,4 +68,0 @@\n-#else\n-  \/\/ Needs 2 CMOV's for longs.\n-  static constexpr int long_cmove_cost() { return 1; }\n-#endif\n@@ -79,1 +69,0 @@\n-#ifdef _LP64\n@@ -82,4 +71,0 @@\n-#else\n-  \/\/ No CMOVF\/CMOVD with SSE\/SSE2\n-  static int float_cmove_cost() { return (UseSSE>=1) ? ConditionalMoveLimit : 0; }\n-#endif\n@@ -88,1 +73,0 @@\n-    NOT_LP64(ShouldNotCallThis();)\n@@ -94,1 +78,0 @@\n-    NOT_LP64(ShouldNotCallThis();)\n@@ -101,1 +84,0 @@\n-    NOT_LP64(ShouldNotCallThis();)\n@@ -108,1 +90,0 @@\n-    NOT_LP64(ShouldNotCallThis();)\n@@ -126,2 +107,0 @@\n-  \/\/ On x32 it is stored with conversion only when FPU is used for floats.\n-#ifdef _LP64\n@@ -131,5 +110,0 @@\n-#else\n-  static bool float_in_double() {\n-    return (UseSSE == 0);\n-  }\n-#endif\n@@ -138,1 +112,0 @@\n-#ifdef _LP64\n@@ -140,4 +113,0 @@\n-#else\n-  static const bool int_in_long = false;\n-#endif\n-\n","filename":"src\/hotspot\/cpu\/x86\/matcher_x86.hpp","additions":0,"deletions":31,"binary":false,"changes":31,"status":"modified"},{"patch":"@@ -213,2 +213,0 @@\n-#ifdef _LP64\n-\n@@ -623,3 +621,0 @@\n-#endif \/\/ _LP64\n-\n-#ifdef _LP64\n@@ -627,3 +622,0 @@\n-#else\n-reg_def RFLAGS(SOC, SOC, 0, 8, VMRegImpl::Bad());\n-#endif \/\/ _LP64\n@@ -661,3 +653,2 @@\n-                   XMM7,  XMM7b,  XMM7c,  XMM7d,  XMM7e,  XMM7f,  XMM7g,  XMM7h,  XMM7i,  XMM7j,  XMM7k,  XMM7l,  XMM7m,  XMM7n,  XMM7o,  XMM7p\n-#ifdef _LP64\n-                  ,XMM8,  XMM8b,  XMM8c,  XMM8d,  XMM8e,  XMM8f,  XMM8g,  XMM8h,  XMM8i,  XMM8j,  XMM8k,  XMM8l,  XMM8m,  XMM8n,  XMM8o,  XMM8p,\n+                   XMM7,  XMM7b,  XMM7c,  XMM7d,  XMM7e,  XMM7f,  XMM7g,  XMM7h,  XMM7i,  XMM7j,  XMM7k,  XMM7l,  XMM7m,  XMM7n,  XMM7o,  XMM7p,\n+                   XMM8,  XMM8b,  XMM8c,  XMM8d,  XMM8e,  XMM8f,  XMM8g,  XMM8h,  XMM8i,  XMM8j,  XMM8k,  XMM8l,  XMM8m,  XMM8n,  XMM8o,  XMM8p,\n@@ -670,2 +661,2 @@\n-                   XMM15, XMM15b, XMM15c, XMM15d, XMM15e, XMM15f, XMM15g, XMM15h, XMM15i, XMM15j, XMM15k, XMM15l, XMM15m, XMM15n, XMM15o, XMM15p\n-                  ,XMM16, XMM16b, XMM16c, XMM16d, XMM16e, XMM16f, XMM16g, XMM16h, XMM16i, XMM16j, XMM16k, XMM16l, XMM16m, XMM16n, XMM16o, XMM16p,\n+                   XMM15, XMM15b, XMM15c, XMM15d, XMM15e, XMM15f, XMM15g, XMM15h, XMM15i, XMM15j, XMM15k, XMM15l, XMM15m, XMM15n, XMM15o, XMM15p,\n+                   XMM16, XMM16b, XMM16c, XMM16d, XMM16e, XMM16f, XMM16g, XMM16h, XMM16i, XMM16j, XMM16k, XMM16l, XMM16m, XMM16n, XMM16o, XMM16p,\n@@ -686,3 +677,1 @@\n-                   XMM31, XMM31b, XMM31c, XMM31d, XMM31e, XMM31f, XMM31g, XMM31h, XMM31i, XMM31j, XMM31k, XMM31l, XMM31m, XMM31n, XMM31o, XMM31p\n-#endif\n-                      );\n+                   XMM31, XMM31b, XMM31c, XMM31d, XMM31e, XMM31f, XMM31g, XMM31h, XMM31i, XMM31j, XMM31k, XMM31l, XMM31m, XMM31n, XMM31o, XMM31p);\n@@ -729,3 +718,2 @@\n-                    XMM7\n-#ifdef _LP64\n-                   ,XMM8,\n+                    XMM7,\n+                    XMM8,\n@@ -738,3 +726,1 @@\n-                    XMM15\n-#endif\n-                    );\n+                    XMM15);\n@@ -750,3 +736,2 @@\n-                    XMM7\n-#ifdef _LP64\n-                   ,XMM8,\n+                    XMM7,\n+                    XMM8,\n@@ -775,3 +760,1 @@\n-                    XMM31\n-#endif\n-                    );\n+                    XMM31);\n@@ -790,3 +773,2 @@\n-                     XMM7,  XMM7b\n-#ifdef _LP64\n-                    ,XMM8,  XMM8b,\n+                     XMM7,  XMM7b,\n+                     XMM8,  XMM8b,\n@@ -799,3 +781,1 @@\n-                     XMM15, XMM15b\n-#endif\n-                     );\n+                     XMM15, XMM15b);\n@@ -811,3 +791,2 @@\n-                     XMM7,  XMM7b\n-#ifdef _LP64\n-                    ,XMM8,  XMM8b,\n+                     XMM7,  XMM7b,\n+                     XMM8,  XMM8b,\n@@ -836,3 +815,1 @@\n-                     XMM31, XMM31b\n-#endif\n-                     );\n+                     XMM31, XMM31b);\n@@ -851,3 +828,2 @@\n-                      XMM7\n-#ifdef _LP64\n-                     ,XMM8,\n+                      XMM7,\n+                      XMM8,\n@@ -860,3 +836,1 @@\n-                      XMM15\n-#endif\n-                      );\n+                      XMM15);\n@@ -872,3 +846,2 @@\n-                      XMM7\n-#ifdef _LP64\n-                     ,XMM8,\n+                      XMM7,\n+                      XMM8,\n@@ -897,3 +870,1 @@\n-                      XMM31\n-#endif\n-                      );\n+                      XMM31);\n@@ -912,3 +883,2 @@\n-                      XMM7,  XMM7b\n-#ifdef _LP64\n-                     ,XMM8,  XMM8b,\n+                      XMM7,  XMM7b,\n+                      XMM8,  XMM8b,\n@@ -921,3 +891,1 @@\n-                      XMM15, XMM15b\n-#endif\n-                      );\n+                      XMM15, XMM15b);\n@@ -933,3 +901,2 @@\n-                      XMM7,  XMM7b\n-#ifdef _LP64\n-                     ,XMM8,  XMM8b,\n+                      XMM7,  XMM7b,\n+                      XMM8,  XMM8b,\n@@ -958,3 +925,1 @@\n-                      XMM31, XMM31b\n-#endif\n-                      );\n+                      XMM31, XMM31b);\n@@ -973,3 +938,2 @@\n-                      XMM7,  XMM7b,  XMM7c,  XMM7d\n-#ifdef _LP64\n-                     ,XMM8,  XMM8b,  XMM8c,  XMM8d,\n+                      XMM7,  XMM7b,  XMM7c,  XMM7d,\n+                      XMM8,  XMM8b,  XMM8c,  XMM8d,\n@@ -982,3 +946,1 @@\n-                      XMM15, XMM15b, XMM15c, XMM15d\n-#endif\n-                      );\n+                      XMM15, XMM15b, XMM15c, XMM15d);\n@@ -994,3 +956,2 @@\n-                      XMM7,  XMM7b,  XMM7c,  XMM7d\n-#ifdef _LP64\n-                     ,XMM8,  XMM8b,  XMM8c,  XMM8d,\n+                      XMM7,  XMM7b,  XMM7c,  XMM7d,\n+                      XMM8,  XMM8b,  XMM8c,  XMM8d,\n@@ -1019,3 +980,1 @@\n-                      XMM31, XMM31b, XMM31c, XMM31d\n-#endif\n-                      );\n+                      XMM31, XMM31b, XMM31c, XMM31d);\n@@ -1034,3 +993,2 @@\n-                      XMM7,  XMM7b,  XMM7c,  XMM7d,  XMM7e,  XMM7f,  XMM7g,  XMM7h\n-#ifdef _LP64\n-                     ,XMM8,  XMM8b,  XMM8c,  XMM8d,  XMM8e,  XMM8f,  XMM8g,  XMM8h,\n+                      XMM7,  XMM7b,  XMM7c,  XMM7d,  XMM7e,  XMM7f,  XMM7g,  XMM7h,\n+                      XMM8,  XMM8b,  XMM8c,  XMM8d,  XMM8e,  XMM8f,  XMM8g,  XMM8h,\n@@ -1043,3 +1001,1 @@\n-                      XMM15, XMM15b, XMM15c, XMM15d, XMM15e, XMM15f, XMM15g, XMM15h\n-#endif\n-                      );\n+                      XMM15, XMM15b, XMM15c, XMM15d, XMM15e, XMM15f, XMM15g, XMM15h);\n@@ -1055,3 +1011,2 @@\n-                      XMM7,  XMM7b,  XMM7c,  XMM7d,  XMM7e,  XMM7f,  XMM7g,  XMM7h\n-#ifdef _LP64\n-                     ,XMM8,  XMM8b,  XMM8c,  XMM8d,  XMM8e,  XMM8f,  XMM8g,  XMM8h,\n+                      XMM7,  XMM7b,  XMM7c,  XMM7d,  XMM7e,  XMM7f,  XMM7g,  XMM7h,\n+                      XMM8,  XMM8b,  XMM8c,  XMM8d,  XMM8e,  XMM8f,  XMM8g,  XMM8h,\n@@ -1080,3 +1035,1 @@\n-                      XMM31, XMM31b, XMM31c, XMM31d, XMM31e, XMM31f, XMM31g, XMM31h\n-#endif\n-                      );\n+                      XMM31, XMM31b, XMM31c, XMM31d, XMM31e, XMM31f, XMM31g, XMM31h);\n@@ -1095,3 +1048,2 @@\n-                      XMM7,  XMM7b,  XMM7c,  XMM7d,  XMM7e,  XMM7f,  XMM7g,  XMM7h,  XMM7i,  XMM7j,  XMM7k,  XMM7l,  XMM7m,  XMM7n,  XMM7o,  XMM7p\n-#ifdef _LP64\n-                     ,XMM8,  XMM8b,  XMM8c,  XMM8d,  XMM8e,  XMM8f,  XMM8g,  XMM8h,  XMM8i,  XMM8j,  XMM8k,  XMM8l,  XMM8m,  XMM8n,  XMM8o,  XMM8p,\n+                      XMM7,  XMM7b,  XMM7c,  XMM7d,  XMM7e,  XMM7f,  XMM7g,  XMM7h,  XMM7i,  XMM7j,  XMM7k,  XMM7l,  XMM7m,  XMM7n,  XMM7o,  XMM7p,\n+                      XMM8,  XMM8b,  XMM8c,  XMM8d,  XMM8e,  XMM8f,  XMM8g,  XMM8h,  XMM8i,  XMM8j,  XMM8k,  XMM8l,  XMM8m,  XMM8n,  XMM8o,  XMM8p,\n@@ -1104,2 +1056,2 @@\n-                      XMM15, XMM15b, XMM15c, XMM15d, XMM15e, XMM15f, XMM15g, XMM15h, XMM15i, XMM15j, XMM15k, XMM15l, XMM15m, XMM15n, XMM15o, XMM15p\n-                     ,XMM16, XMM16b, XMM16c, XMM16d, XMM16e, XMM16f, XMM16g, XMM16h, XMM16i, XMM16j, XMM16k, XMM16l, XMM16m, XMM16n, XMM16o, XMM16p,\n+                      XMM15, XMM15b, XMM15c, XMM15d, XMM15e, XMM15f, XMM15g, XMM15h, XMM15i, XMM15j, XMM15k, XMM15l, XMM15m, XMM15n, XMM15o, XMM15p,\n+                      XMM16, XMM16b, XMM16c, XMM16d, XMM16e, XMM16f, XMM16g, XMM16h, XMM16i, XMM16j, XMM16k, XMM16l, XMM16m, XMM16n, XMM16o, XMM16p,\n@@ -1120,3 +1072,1 @@\n-                      XMM31, XMM31b, XMM31c, XMM31d, XMM31e, XMM31f, XMM31g, XMM31h, XMM31i, XMM31j, XMM31k, XMM31l, XMM31m, XMM31n, XMM31o, XMM31p\n-#endif\n-                      );\n+                      XMM31, XMM31b, XMM31c, XMM31d, XMM31e, XMM31f, XMM31g, XMM31h, XMM31i, XMM31j, XMM31k, XMM31l, XMM31m, XMM31n, XMM31o, XMM31p);\n@@ -1132,3 +1082,2 @@\n-                      XMM7,  XMM7b,  XMM7c,  XMM7d,  XMM7e,  XMM7f,  XMM7g,  XMM7h,  XMM7i,  XMM7j,  XMM7k,  XMM7l,  XMM7m,  XMM7n,  XMM7o,  XMM7p\n-#ifdef _LP64\n-                     ,XMM8,  XMM8b,  XMM8c,  XMM8d,  XMM8e,  XMM8f,  XMM8g,  XMM8h,  XMM8i,  XMM8j,  XMM8k,  XMM8l,  XMM8m,  XMM8n,  XMM8o,  XMM8p,\n+                      XMM7,  XMM7b,  XMM7c,  XMM7d,  XMM7e,  XMM7f,  XMM7g,  XMM7h,  XMM7i,  XMM7j,  XMM7k,  XMM7l,  XMM7m,  XMM7n,  XMM7o,  XMM7p,\n+                      XMM8,  XMM8b,  XMM8c,  XMM8d,  XMM8e,  XMM8f,  XMM8g,  XMM8h,  XMM8i,  XMM8j,  XMM8k,  XMM8l,  XMM8m,  XMM8n,  XMM8o,  XMM8p,\n@@ -1141,3 +1090,1 @@\n-                      XMM15, XMM15b, XMM15c, XMM15d, XMM15e, XMM15f, XMM15g, XMM15h, XMM15i, XMM15j, XMM15k, XMM15l, XMM15m, XMM15n, XMM15o, XMM15p\n-#endif\n-                      );\n+                      XMM15, XMM15b, XMM15c, XMM15d, XMM15e, XMM15f, XMM15g, XMM15h, XMM15i, XMM15j, XMM15k, XMM15l, XMM15m, XMM15n, XMM15o, XMM15p);\n@@ -1202,1 +1149,0 @@\n-#ifdef _LP64\n@@ -1207,10 +1153,0 @@\n-#else\n-  static uint size_deopt_handler() {\n-    \/\/ NativeCall instruction size is the same as NativeJump.\n-    \/\/ exception handler starts out as jump and can be patched to\n-    \/\/ a call be deoptimization.  (4932387)\n-    \/\/ Note that this value is also credited (in output.cpp) to\n-    \/\/ the size of the code section.\n-    return 5 + NativeJump::instruction_size; \/\/ pushl(); jmp;\n-  }\n-#endif\n@@ -1337,1 +1273,0 @@\n-#ifdef _LP64\n@@ -1348,4 +1283,0 @@\n-#else\n-  InternalAddress here(__ pc());\n-  __ pushptr(here.addr(), noreg);\n-#endif\n@@ -1375,1 +1306,0 @@\n-#ifdef _LP64\n@@ -1380,6 +1310,0 @@\n-#else\n-  static address float_signmask()  { return (address)float_signmask_pool; }\n-  static address float_signflip()  { return (address)float_signflip_pool; }\n-  static address double_signmask() { return (address)double_signmask_pool; }\n-  static address double_signflip() { return (address)double_signflip_pool; }\n-#endif\n@@ -1407,1 +1331,0 @@\n-  const bool is_LP64 = LP64_ONLY(true) NOT_LP64(false);\n@@ -1512,1 +1435,1 @@\n-      if (!is_LP64 || (UseAVX < 2)) {\n+      if (UseAVX < 2) {\n@@ -1527,1 +1450,0 @@\n-#ifdef _LP64\n@@ -1529,1 +1451,0 @@\n-#endif\n@@ -1558,1 +1479,0 @@\n-#ifdef _LP64\n@@ -1567,1 +1487,0 @@\n-#endif\n@@ -1610,1 +1529,1 @@\n-      if (!is_LP64  || UseAVX < 3 || !VM_Version::supports_bmi2()) {\n+      if (UseAVX < 3 || !VM_Version::supports_bmi2()) {\n@@ -1618,1 +1537,1 @@\n-      if (!is_LP64 || UseAVX < 1) {\n+      if (UseAVX < 1) {\n@@ -1624,3 +1543,0 @@\n-      if (!is_LP64) {\n-        return false;\n-      }\n@@ -1630,1 +1546,1 @@\n-      if (UseAVX < 3 || !is_LP64)  {\n+      if (UseAVX < 3)  {\n@@ -1637,18 +1553,0 @@\n-#ifndef _LP64\n-    case Op_AddReductionVF:\n-    case Op_AddReductionVD:\n-    case Op_MulReductionVF:\n-    case Op_MulReductionVD:\n-      if (UseSSE < 1) { \/\/ requires at least SSE\n-        return false;\n-      }\n-      break;\n-    case Op_MulAddVS2VI:\n-    case Op_RShiftVL:\n-    case Op_AbsVD:\n-    case Op_NegVD:\n-      if (UseSSE < 2) {\n-        return false;\n-      }\n-      break;\n-#endif \/\/ !LP64\n@@ -1656,4 +1554,0 @@\n-      if (!VM_Version::supports_bmi2() || (!is_LP64 && UseSSE < 2)) {\n-        return false;\n-      }\n-      break;\n@@ -1661,1 +1555,1 @@\n-      if (!VM_Version::supports_bmi2() || (!is_LP64 && (UseSSE < 2 || !VM_Version::supports_bmi1()))) {\n+      if (!VM_Version::supports_bmi2()) {\n@@ -1686,1 +1580,0 @@\n-#ifdef _LP64\n@@ -1690,4 +1583,0 @@\n-#else\n-      \/\/ x86_32.ad has a special match rule for SqrtD.\n-      \/\/ Together with common x86 rules, this handles all UseSSE cases.\n-#endif\n@@ -1725,1 +1614,0 @@\n-  const bool is_LP64 = LP64_ONLY(true) NOT_LP64(false);\n@@ -1772,1 +1660,1 @@\n-      if (!is_LP64 || !VM_Version::supports_avx512bw()) {\n+      if (!VM_Version::supports_avx512bw()) {\n@@ -1822,5 +1710,0 @@\n-#ifndef _LP64\n-      if (bt == T_BYTE || bt == T_LONG) {\n-        return false;\n-      }\n-#endif\n@@ -1828,7 +1711,0 @@\n-#ifndef _LP64\n-    case Op_VectorInsert:\n-      if (bt == T_LONG || bt == T_DOUBLE) {\n-        return false;\n-      }\n-      break;\n-#endif\n@@ -1849,5 +1725,0 @@\n-#ifndef _LP64\n-      if (bt == T_BYTE || bt == T_LONG) {\n-        return false;\n-      }\n-#endif\n@@ -1938,4 +1809,3 @@\n-         (!is_LP64                                                ||\n-         (size_in_bits > 256 && !VM_Version::supports_avx512bw()) ||\n-         (size_in_bits < 64)                                      ||\n-         (bt == T_SHORT && !VM_Version::supports_bmi2()))) {\n+         ((size_in_bits > 256 && !VM_Version::supports_avx512bw()) ||\n+          (size_in_bits < 64)                                      ||\n+          (bt == T_SHORT && !VM_Version::supports_bmi2()))) {\n@@ -2010,3 +1880,0 @@\n-      if (!is_LP64 && !VM_Version::supports_avx512vl() && size_in_bits < 512) {\n-        return false;\n-      }\n@@ -2017,1 +1884,1 @@\n-      if (UseAVX < 1 || !is_LP64) {\n+      if (UseAVX < 1) {\n@@ -2065,1 +1932,0 @@\n-  const bool is_LP64 = LP64_ONLY(true) NOT_LP64(false);\n@@ -2401,1 +2267,0 @@\n-#ifdef _LP64\n@@ -2411,2 +2276,1 @@\n-    } else\n-#endif\n+    } else {\n@@ -2414,0 +2278,1 @@\n+    }\n@@ -2551,1 +2416,1 @@\n-        LP64_ONLY( off->get_long() == (int) (off->get_long()) && ) \/\/ immL32\n+        off->get_long() == (int) (off->get_long()) && \/\/ immL32\n@@ -2625,3 +2490,0 @@\n-#ifndef _LP64\n-      __ movdqu(as_XMMRegister(Matcher::_regEncode[dst_lo]), as_XMMRegister(Matcher::_regEncode[src_lo]));\n-#else\n@@ -2633,1 +2495,0 @@\n-#endif\n@@ -2636,3 +2497,0 @@\n-#ifndef _LP64\n-      __ vmovdqu(as_XMMRegister(Matcher::_regEncode[dst_lo]), as_XMMRegister(Matcher::_regEncode[src_lo]));\n-#else\n@@ -2644,1 +2502,0 @@\n-#endif\n@@ -2683,3 +2540,0 @@\n-#ifndef _LP64\n-        __ movdqu(as_XMMRegister(Matcher::_regEncode[reg]), Address(rsp, stack_offset));\n-#else\n@@ -2692,1 +2546,0 @@\n-#endif\n@@ -2695,3 +2548,0 @@\n-#ifndef _LP64\n-        __ vmovdqu(as_XMMRegister(Matcher::_regEncode[reg]), Address(rsp, stack_offset));\n-#else\n@@ -2704,1 +2554,0 @@\n-#endif\n@@ -2721,3 +2570,0 @@\n-#ifndef _LP64\n-        __ movdqu(Address(rsp, stack_offset), as_XMMRegister(Matcher::_regEncode[reg]));\n-#else\n@@ -2730,1 +2576,0 @@\n-#endif\n@@ -2733,3 +2578,0 @@\n-#ifndef _LP64\n-        __ vmovdqu(Address(rsp, stack_offset), as_XMMRegister(Matcher::_regEncode[reg]));\n-#else\n@@ -2742,1 +2584,0 @@\n-#endif\n@@ -3973,1 +3814,0 @@\n-#ifdef _LP64\n@@ -4044,1 +3884,0 @@\n-#endif \/\/ _LP64\n@@ -4262,1 +4101,0 @@\n-#ifdef _LP64\n@@ -4425,1 +4263,0 @@\n-#endif\n@@ -4541,1 +4378,0 @@\n-#ifdef _LP64\n@@ -4568,1 +4404,0 @@\n-#endif\n@@ -4665,1 +4500,0 @@\n-#ifdef _LP64\n@@ -4686,55 +4520,0 @@\n-#else \/\/ _LP64\n-\/\/ Replicate long (8 byte) scalar to be vector\n-instruct ReplL_reg(vec dst, eRegL src, vec tmp) %{\n-  predicate(Matcher::vector_length(n) <= 4 && Matcher::vector_element_basic_type(n) == T_LONG);\n-  match(Set dst (Replicate src));\n-  effect(TEMP dst, USE src, TEMP tmp);\n-  format %{ \"replicateL $dst,$src\" %}\n-  ins_encode %{\n-    uint vlen = Matcher::vector_length(this);\n-    if (vlen == 2) {\n-      __ movdl($dst$$XMMRegister, $src$$Register);\n-      __ movdl($tmp$$XMMRegister, HIGH_FROM_LOW($src$$Register));\n-      __ punpckldq($dst$$XMMRegister, $tmp$$XMMRegister);\n-      __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);\n-    } else if (VM_Version::supports_avx512vl()) { \/\/ AVX512VL for <512bit operands\n-      int vlen_enc = Assembler::AVX_256bit;\n-      __ movdl($dst$$XMMRegister, $src$$Register);\n-      __ movdl($tmp$$XMMRegister, HIGH_FROM_LOW($src$$Register));\n-      __ punpckldq($dst$$XMMRegister, $tmp$$XMMRegister);\n-      __ vpbroadcastq($dst$$XMMRegister, $dst$$XMMRegister, vlen_enc);\n-    } else {\n-      __ movdl($dst$$XMMRegister, $src$$Register);\n-      __ movdl($tmp$$XMMRegister, HIGH_FROM_LOW($src$$Register));\n-      __ punpckldq($dst$$XMMRegister, $tmp$$XMMRegister);\n-      __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);\n-      __ vinserti128_high($dst$$XMMRegister, $dst$$XMMRegister);\n-    }\n-  %}\n-  ins_pipe( pipe_slow );\n-%}\n-\n-instruct ReplL_reg_leg(legVec dst, eRegL src, legVec tmp) %{\n-  predicate(Matcher::vector_length(n) == 8 && Matcher::vector_element_basic_type(n) == T_LONG);\n-  match(Set dst (Replicate src));\n-  effect(TEMP dst, USE src, TEMP tmp);\n-  format %{ \"replicateL $dst,$src\" %}\n-  ins_encode %{\n-    if (VM_Version::supports_avx512vl()) {\n-      __ movdl($dst$$XMMRegister, $src$$Register);\n-      __ movdl($tmp$$XMMRegister, HIGH_FROM_LOW($src$$Register));\n-      __ punpckldq($dst$$XMMRegister, $tmp$$XMMRegister);\n-      __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);\n-      __ vinserti128_high($dst$$XMMRegister, $dst$$XMMRegister);\n-      __ vinserti64x4($dst$$XMMRegister, $dst$$XMMRegister, $dst$$XMMRegister, 0x1);\n-    } else {\n-      int vlen_enc = Assembler::AVX_512bit;\n-      __ movdl($dst$$XMMRegister, $src$$Register);\n-      __ movdl($tmp$$XMMRegister, HIGH_FROM_LOW($src$$Register));\n-      __ punpckldq($dst$$XMMRegister, $tmp$$XMMRegister);\n-      __ vpbroadcastq($dst$$XMMRegister, $dst$$XMMRegister, vlen_enc);\n-    }\n-  %}\n-  ins_pipe( pipe_slow );\n-%}\n-#endif \/\/ _LP64\n@@ -5014,1 +4793,0 @@\n-#ifdef _LP64\n@@ -5065,1 +4843,0 @@\n-#endif\n@@ -5111,1 +4888,0 @@\n-#ifdef _LP64\n@@ -5166,1 +4942,0 @@\n-#endif\n@@ -5193,1 +4968,0 @@\n-#ifdef _LP64\n@@ -5231,1 +5005,0 @@\n-#endif \/\/ _LP64\n@@ -5443,1 +5216,0 @@\n-#ifdef _LP64\n@@ -5479,1 +5251,0 @@\n-#endif\n@@ -6780,1 +6551,0 @@\n-#ifdef _LP64\n@@ -6806,2 +6576,0 @@\n-#endif \/\/ _LP64\n-\n@@ -7980,1 +7748,0 @@\n-#ifdef _LP64\n@@ -8030,2 +7797,0 @@\n-#endif \/\/ _LP64\n-\n@@ -8241,1 +8006,0 @@\n-#ifdef _LP64\n@@ -8243,1 +8007,0 @@\n-#endif\n@@ -8259,1 +8022,0 @@\n-#ifdef _LP64\n@@ -8261,1 +8023,0 @@\n-#endif\n@@ -8274,1 +8035,0 @@\n-#ifdef _LP64\n@@ -8302,1 +8062,0 @@\n-#endif\n@@ -8567,1 +8326,0 @@\n-#ifdef _LP64\n@@ -8635,1 +8393,0 @@\n-#endif\n@@ -8886,1 +8643,0 @@\n-#ifdef _LP64\n@@ -8918,1 +8674,1 @@\n-#endif\n+\n@@ -9499,1 +9255,0 @@\n-#ifdef _LP64\n@@ -9713,1 +9468,0 @@\n-#ifdef _LP64\n@@ -9729,1 +9483,0 @@\n-#endif\n@@ -9757,2 +9510,0 @@\n-#endif \/\/ _LP64\n-\n@@ -10479,1 +10230,0 @@\n-#ifdef _LP64\n@@ -10544,1 +10294,0 @@\n-#endif\n","filename":"src\/hotspot\/cpu\/x86\/x86.ad","additions":63,"deletions":314,"binary":false,"changes":377,"status":"modified"},{"patch":"@@ -754,1 +754,0 @@\n-  \/\/ x86_32 combine x86.ad and x86_32.ad, the vec*\/legVec* can not be cleaned from IA32\n@@ -759,1 +758,1 @@\n-#elif defined(IA32) || defined(AMD64)\n+#elif defined(AMD64)\n","filename":"src\/hotspot\/share\/adlc\/archDesc.cpp","additions":1,"deletions":2,"binary":false,"changes":3,"status":"modified"}]}