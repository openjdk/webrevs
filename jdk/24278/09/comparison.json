{"files":[{"patch":"@@ -359,0 +359,4 @@\n+  product(bool, LoopMultiversioningOptimizeSlowLoop, true, DIAGNOSTIC,      \\\n+          \"When using loop multiversioning, and a speculative runtime\"      \\\n+          \" check is added, resume optimization for the stalled slow_loop\") \\\n+                                                                            \\\n@@ -362,0 +366,6 @@\n+  product(bool, UseAutoVectorizationPredicate, true, DIAGNOSTIC,            \\\n+          \"Use AutoVectorization predicate (for speculative compilation)\")  \\\n+                                                                            \\\n+  product(bool, UseAutoVectorizationSpeculativeAliasingChecks, true, DIAGNOSTIC, \\\n+          \"Use Multiversioning or Predicate to add aliasing runtime checks\") \\\n+                                                                            \\\n","filename":"src\/hotspot\/share\/opto\/c2_globals.hpp","additions":10,"deletions":0,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -4077,1 +4077,3 @@\n-  add_parse_predicate(Deoptimization::Reason_auto_vectorization_check, nargs);\n+  if (UseAutoVectorizationPredicate) {\n+    add_parse_predicate(Deoptimization::Reason_auto_vectorization_check, nargs);\n+  }\n","filename":"src\/hotspot\/share\/opto\/graphKit.cpp","additions":3,"deletions":1,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -566,0 +566,13 @@\n+  \/\/ If explicitly asked for (e.g. for benchmarking) we do not continue with\n+  \/\/ optimizations. That means the slow_loop should still be correct, but\n+  \/\/ a bit slower, as there is no unrolling etc.\n+  if (!LoopMultiversioningOptimizeSlowLoop) {\n+#ifndef PRODUCT\n+    if (TraceLoopOpts) {\n+      tty->print(\"WARNING: Resumption of Optimization disabled (-XX:-LoopMultiversioningOptimizeSlowLoop)\");\n+      lpt->dump_head();\n+    }\n+#endif\n+    return false;\n+  }\n+\n","filename":"src\/hotspot\/share\/opto\/loopUnswitch.cpp","additions":13,"deletions":0,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -34,0 +34,1 @@\n+#include \"opto\/c2_globals.hpp\"\n@@ -1136,5 +1137,7 @@\n-    \/\/ We only want to use the auto-vectorization check as a trap once per bci. And\n-    \/\/ PhaseIdealLoop::add_parse_predicate only checks trap limits per method, so\n-    \/\/ we do a custom check here.\n-    if (!C->too_many_traps(cloned_sfpt->jvms()->method(), cloned_sfpt->jvms()->bci(), Deoptimization::Reason_auto_vectorization_check)) {\n-      add_parse_predicate(Deoptimization::Reason_auto_vectorization_check, inner_head, outer_ilt, cloned_sfpt);\n+    if (UseAutoVectorizationPredicate) {\n+      \/\/ We only want to use the auto-vectorization check as a trap once per bci. And\n+      \/\/ PhaseIdealLoop::add_parse_predicate only checks trap limits per method, so\n+      \/\/ we do a custom check here.\n+      if (!C->too_many_traps(cloned_sfpt->jvms()->method(), cloned_sfpt->jvms()->bci(), Deoptimization::Reason_auto_vectorization_check)) {\n+        add_parse_predicate(Deoptimization::Reason_auto_vectorization_check, inner_head, outer_ilt, cloned_sfpt);\n+      }\n@@ -4645,0 +4648,3 @@\n+  if (UseAutoVectorizationPredicate && predicates.auto_vectorization_check_block()->is_non_empty()) {\n+    tty->print(\" auto_vectorization_check_predicate\");\n+  }\n","filename":"src\/hotspot\/share\/opto\/loopnode.cpp","additions":11,"deletions":5,"binary":false,"changes":16,"status":"modified"},{"patch":"@@ -44,0 +44,1 @@\n+  assert(_raw_summands.is_empty(), \"no prior parsing\");\n@@ -49,0 +50,8 @@\n+#ifndef PRODUCT\n+  if (trace.is_trace_parsing()) {\n+    tty->print_cr(\"MemPointerParser::parse: size=%d\", size);\n+    tty->print(\"  mem:     \"); _mem->dump();\n+    tty->print(\"  pointer: \"); pointer->dump();\n+  }\n+#endif\n+\n@@ -50,1 +59,1 @@\n-  _worklist.push(MemPointerSummand(pointer, NoOverflowInt(1)));\n+  _worklist.push(MemPointerRawSummand::make_trivial(pointer));\n@@ -63,3 +72,61 @@\n-  \/\/ Bail out if there is a constant overflow.\n-  if (_con.is_NaN()) {\n-    return MemPointer::make_trivial(pointer, size NOT_PRODUCT(COMMA trace));\n+  NOT_PRODUCT( if (trace.is_trace_parsing()) { MemPointerRawSummand::print_on(tty, _raw_summands); } )\n+  canonicalize_raw_summands();\n+  NOT_PRODUCT( if (trace.is_trace_parsing()) { MemPointerRawSummand::print_on(tty, _raw_summands); } )\n+\n+  create_summands();\n+  NOT_PRODUCT( if (trace.is_trace_parsing()) { MemPointerSummand::print_on(tty, _con, _summands); } )\n+  canonicalize_summands();\n+  NOT_PRODUCT( if (trace.is_trace_parsing()) { MemPointerSummand::print_on(tty, _con, _summands); } )\n+\n+  return MemPointer::make(pointer, _raw_summands, _con, _summands, size NOT_PRODUCT(COMMA trace));\n+}\n+\n+void MemPointerParser::canonicalize_raw_summands() {\n+  \/\/ We sort by:\n+  \/\/  - int group id\n+  \/\/  - variable idx\n+  \/\/ This means that summands of the same int group with the same variable are consecutive.\n+  \/\/ This simplifies the combining of summands below.\n+  _raw_summands.sort(MemPointerRawSummand::cmp_by_int_group_and_variable_idx);\n+\n+  \/\/ Combine summands of the same int group with the same variable, adding up the scales.\n+  int pos_put = 0;\n+  int pos_get = 0;\n+  while (pos_get < _raw_summands.length()) {\n+    const MemPointerRawSummand& summand = _raw_summands.at(pos_get++);\n+    Node* variable      = summand.variable();\n+    NoOverflowInt scaleI = summand.scaleI();\n+    NoOverflowInt scaleL = summand.scaleL();\n+    int int_group = summand.int_group();\n+    \/\/ Add up scale of all summands with the same variable.\n+    while (pos_get < _raw_summands.length() &&\n+           _raw_summands.at(pos_get).int_group() == int_group &&\n+           _raw_summands.at(pos_get).variable() == variable) {\n+      MemPointerRawSummand s = _raw_summands.at(pos_get++);\n+      if (int_group == 0) {\n+        assert(scaleI.is_one() && s.scaleI().is_one(), \"no ConvI2L\");\n+        scaleL = scaleL + s.scaleL();\n+      } else {\n+        assert(scaleL.value() == s.scaleL().value(), \"same ConvI2L, same scaleL\");\n+        scaleI = scaleI + s.scaleI();\n+      }\n+    }\n+    \/\/ Keep summands with non-zero scale.\n+    if (!scaleI.is_zero() && !scaleL.is_NaN()) {\n+      _raw_summands.at_put(pos_put++, MemPointerRawSummand(variable, scaleI, scaleL, int_group));\n+    }\n+  }\n+  _raw_summands.trunc_to(pos_put);\n+}\n+\n+void MemPointerParser::create_summands() {\n+  assert(_con.is_zero(), \"no prior parsing\");\n+  assert(_summands.is_empty(), \"no prior parsing\");\n+\n+  for (int i = 0; i < _raw_summands.length(); i++) {\n+    const MemPointerRawSummand& raw_summand = _raw_summands.at(i);\n+    if (raw_summand.is_con()) {\n+      _con = _con + raw_summand.to_con();\n+    } else {\n+      _summands.push(raw_summand.to_summand());\n+    }\n@@ -67,0 +134,1 @@\n+}\n@@ -68,0 +136,1 @@\n+void MemPointerParser::canonicalize_summands() {\n@@ -84,4 +153,0 @@\n-    \/\/ Bail out if scale is NaN.\n-    if (scale.is_NaN()) {\n-      return MemPointer::make_trivial(pointer, size NOT_PRODUCT(COMMA trace));\n-    }\n@@ -94,2 +159,0 @@\n-\n-  return MemPointer::make(pointer, _summands, _con, size NOT_PRODUCT(COMMA trace));\n@@ -101,1 +164,1 @@\n-void MemPointerParser::parse_sub_expression(const MemPointerSummand& summand, MemPointerParserCallback& callback) {\n+void MemPointerParser::parse_sub_expression(const MemPointerRawSummand& summand, MemPointerParserCallback& callback) {\n@@ -103,1 +166,3 @@\n-  const NoOverflowInt scale = summand.scale();\n+  const NoOverflowInt scaleI = summand.scaleI();\n+  const NoOverflowInt scaleL = summand.scaleL();\n+  const int int_group = summand.int_group();\n@@ -107,1 +172,1 @@\n-  if (is_safe_to_decompose_op(opc, scale)) {\n+  if (is_safe_to_decompose_op(opc, scaleI * scaleL)) {\n@@ -112,1 +177,1 @@\n-        \/\/ Terminal: add to constant.\n+        \/\/ Terminal summand.\n@@ -115,1 +180,3 @@\n-        _con = _con + scale * con;\n+        NoOverflowInt conI = (int_group == 0) ? scaleI : scaleI * con;\n+        NoOverflowInt conL = (int_group == 0) ? scaleL * con : scaleL;\n+        _raw_summands.push(MemPointerRawSummand::make_con(conI, conL, int_group));\n@@ -125,2 +192,2 @@\n-        _worklist.push(MemPointerSummand(a, scale));\n-        _worklist.push(MemPointerSummand(b, scale));\n+        _worklist.push(MemPointerRawSummand(a, scaleI, scaleL, int_group));\n+        _worklist.push(MemPointerRawSummand(b, scaleI, scaleL, int_group));\n@@ -137,1 +204,3 @@\n-        NoOverflowInt sub_scale = NoOverflowInt(-1) * scale;\n+        \/\/                   int_group  x.scaleI  x.scaleL  y.scaleI  y.scaleL\n+        \/\/ 2L * (x - y)      0          1         2         1         -2\n+        \/\/ ConvI2L(x - y)    1          1         1         -1        1\n@@ -139,2 +208,5 @@\n-        _worklist.push(MemPointerSummand(a, scale));\n-        _worklist.push(MemPointerSummand(b, sub_scale));\n+        NoOverflowInt sub_scaleI = (int_group == 0) ? scaleI : scaleI * NoOverflowInt(-1);\n+        NoOverflowInt sub_scaleL = (int_group == 0) ? scaleL * NoOverflowInt(-1) : scaleL;\n+\n+        _worklist.push(MemPointerRawSummand(a,     scaleI,     scaleL, int_group));\n+        _worklist.push(MemPointerRawSummand(b, sub_scaleI, sub_scaleL, int_group));\n@@ -172,2 +244,6 @@\n-        \/\/ Accumulate scale.\n-        NoOverflowInt new_scale = scale * factor;\n+        \/\/                         int_group  x.scaleI  x.scaleL\n+        \/\/ 2L * (4L * x)           0          1         8\n+        \/\/ 2L * ConvI2L(4 * x)     1          4         2\n+\n+        NoOverflowInt mul_scaleI = (int_group == 0) ? scaleI : scaleI * factor;\n+        NoOverflowInt mul_scaleL = (int_group == 0) ? scaleL * factor : scaleL;\n@@ -175,1 +251,1 @@\n-        _worklist.push(MemPointerSummand(variable, new_scale));\n+        _worklist.push(MemPointerRawSummand(variable, mul_scaleI, mul_scaleL, int_group));\n@@ -203,1 +279,10 @@\n-          _worklist.push(MemPointerSummand(a, scale));\n+\n+          int cast_int_group = int_group;\n+#ifdef _LP64\n+          if (opc == Op_ConvI2L) {\n+            assert(int_group == 0, \"only find ConvI2L once\");\n+            \/\/ We just discovered a new ConvI2L, and this creates a new \"int group\".\n+            cast_int_group = _next_int_group++;\n+          }\n+#endif\n+          _worklist.push(MemPointerRawSummand(a, scaleI, scaleL, cast_int_group));\n@@ -215,1 +300,1 @@\n-  _summands.push(summand);\n+  _raw_summands.push(summand);\n@@ -621,0 +706,52 @@\n+\/\/ Examples:\n+\/\/   p1 = MemPointer[size=1, base + i + 16]\n+\/\/   p2 = MemPointer[size=1, base + i + 17]\n+\/\/   -> Always at distance 1\n+\/\/   -> Can never overlap -> return false\n+\/\/\n+\/\/   p1 = MemPointer[size=1, base + i + 16]\n+\/\/   p2 = MemPointer[size=1, base + i + 16]\n+\/\/   -> Always at distance 0\n+\/\/   -> Always have exact overlap -> return true\n+\/\/\n+\/\/   p1 = MemPointer[size=4, x + y + z + 4L * i + 16]\n+\/\/   p2 = MemPointer[size=4, x + y + z + 4L * i + 56]\n+\/\/   -> Always at distance 40\n+\/\/   -> Can never overlap -> return false\n+\/\/\n+\/\/   p1 = MemPointer[size=8, x + y + z + 4L * i + 16]\n+\/\/   p2 = MemPointer[size=8, x + y + z + 4L * i + 20]\n+\/\/   -> Always at distance 4\n+\/\/   -> Always have partial overlap -> return true\n+\/\/\n+\/\/   p1 = MemPointer[size=4, base1 + 4L * i1 + 16]\n+\/\/   p2 = MemPointer[size=4, base2 + 4L * i2 + 20]\n+\/\/   -> Have differing summands, distance is unknown\n+\/\/   -> Unknown if overlap at runtime -> return false\n+bool MemPointer::always_overlaps_with(const MemPointer& other) const {\n+  const MemPointerAliasing aliasing = get_aliasing_with(other NOT_PRODUCT( COMMA _trace ));\n+\n+  \/\/ The aliasing tries to compute:\n+  \/\/   distance = other - this\n+  \/\/\n+  \/\/ We know that we have an overlap if we can prove:\n+  \/\/   this < other + other.size       &&  this + this.size > other\n+  \/\/\n+  \/\/ Which we can restate as:\n+  \/\/   distance > -other.size          &&  this.size > distance\n+  \/\/\n+  const jint distance_lo = -other.size();\n+  const jint distance_hi = size();\n+  bool is_always_overlap = aliasing.is_always_in_distance_range(distance_lo, distance_hi);\n+\n+#ifndef PRODUCT\n+  if (_trace.is_trace_overlap()) {\n+    tty->print(\"Always Overlap: %s, distance_lo: %d, distance_hi: %d, aliasing: \",\n+               is_always_overlap ? \"true\" : \"false\", distance_lo, distance_hi);\n+    aliasing.print_on(tty);\n+    tty->cr();\n+  }\n+#endif\n+\n+  return is_always_overlap;\n+}\n","filename":"src\/hotspot\/share\/opto\/mempointer.cpp","additions":162,"deletions":25,"binary":false,"changes":187,"status":"modified"},{"patch":"@@ -44,0 +44,4 @@\n+\/\/ A more advanced use case of MemPointers is speculative aliasing analysis. If we can prove that\n+\/\/ the MemPointer has a linear form in the loop induction variable (iv), we can formulate runtime\n+\/\/ checks to establish that two MemPointer never overlap for all iterations, i.e. for all iv values.\n+\/\/\n@@ -161,1 +165,1 @@\n-\/\/   Where each summand_i in summands has the form:\n+\/\/   Where each summand_i in summands has the MemPointerSummand form:\n@@ -270,0 +274,4 @@\n+\/\/   Even further down, we prove the \"MemPointer Linearity Corrolary\", where we show that\n+\/\/   (under reasonable restrictions) both the MemPointer and the corresponding pointer\n+\/\/   can be considered linear functions.\n+\/\/\n@@ -388,1 +396,90 @@\n-\n+\/\/\n+\/\/\n+\/\/ Having proven the \"MemPointer Lemma\", we can now derive an interesting corrolary.\n+\/\/\n+\/\/ MemPointer Linearity Corrolary:\n+\/\/   Given:\n+\/\/     (C0) pointer p and its MemPointer mp, which is constructed with safe decompositions.\n+\/\/     (C1) a summand \"scale_v * v\" that occurs in mp.\n+\/\/     (C2) a strided range r = [lo, lo + stride_v, .. hi] for v.\n+\/\/     (C3) for all v in this strided range r we know that p is within bounds of its memory object.\n+\/\/     (C4) abs(scale_v * stride_v) < 2^31.\n+\/\/\n+\/\/   Then:\n+\/\/     Both p and mp have a linear form for v in r:\n+\/\/       p(v)  = p(lo)  - lo * scale_v + v * scale_v              (Corrolary P)\n+\/\/       mp(v) = mp(lo) - lo * scale_v + v * scale_v              (Corrolary MP)\n+\/\/\n+\/\/     Note: the calculations are done in long, and hence there can be no int overflow.\n+\/\/           Thus, p(v) and mp(v) can be considered linear functions for v in r.\n+\/\/\n+\/\/   It can be useful to \"anchor\" at hi instead of lo:\n+\/\/     p(hi) = p(lo) - lo * scale_v + hi * scale_v\n+\/\/\n+\/\/     p(v) = p(lo) - lo * scale_v + v * scale_v\n+\/\/            --------------------\n+\/\/          = p(hi) - hi * scale_v + v * scale_v             (Alternative Corrolary P)\n+\/\/\n+\/\/\n+\/\/ Proof of \"MemPointer Linearity Corrolary\":\n+\/\/   We state the form of mp:\n+\/\/\n+\/\/     mp = summand_rest + scale_v * v + con\n+\/\/\n+\/\/   We prove the Corrolary by induction over v:\n+\/\/   Base Case: v = lo\n+\/\/     p(lo)  = p(lo)  - lo * scale_v + lo * scale_v\n+\/\/     mp(lo) = mp(lo) - lo * scale_v + lo * scale_v\n+\/\/\n+\/\/   Step Case: v0 and v1 in r, v1 = v0 + stride_v\n+\/\/     Assume:\n+\/\/       p(v0)  = p(lo)  - lo * scale_v + v * scale_v          (Induction Hypothesis IH-P)\n+\/\/       mp(v0) = mp(lo) - lo * scale_v + v * scale_v          (Induction Hypothesis IH-MP)\n+\/\/\n+\/\/     We take the form of mp, and further apply SAFE1 decompositions, i.e. long addition,\n+\/\/     subtraction and multiplication:\n+\/\/       mp(v1) = summand_rest + scale_v * v1                                   + con\n+\/\/              = summand_rest + scale_v * (v0 + stride_v)                      + con\n+\/\/              = summand_rest + scale_v * v0              + scale_v * stride_v + con\n+\/\/              = summand_rest + scale_v * v0              + scale_v * stride_v + con\n+\/\/              = mp(v0)                                   + scale_v * stride_v\n+\/\/\n+\/\/     From this it follows that we can see mp(v0) and mp(v1) as two MemPointer with the\n+\/\/     same summands, and only their constants differ by exactly \"scale_v * stride_v\":\n+\/\/       mp(v0) = summand_rest + scale_v * v0 + con\n+\/\/       mp(v1) = summand_rest + scale_v * v0 + con + scale_v * stride_v            (MP-DIFF)\n+\/\/\n+\/\/     We continue by applying the Induction Hypothesis IH-MP\n+\/\/       mp(v1) = mp(v0)                                + scale_v * stride_v\n+\/\/                -------- apply (IH-MP) -------------\n+\/\/              = mp(lo) - lo * scale_v + v0 * scale_v  + scale_v * stride_v\n+\/\/              = mp(lo) - lo * scale_v + (v0 + stride_v) * scale_v\n+\/\/              = mp(lo) - lo * scale_v + v1 * scale_v\n+\/\/\n+\/\/     This proves the Corrolary MP.\n+\/\/\n+\/\/     To prove the Corrolary P, we now apply the MemPointer Lemma:\n+\/\/       (S0) Let p(v0) and p(v1) be the pointers corresponding to v0 and v1, and mp(v0) and mp(v1)\n+\/\/            their MemPointer. (C0) provides the safe deconstruction, and reformulation of terms\n+\/\/            happens with long addition, subtraction and multiplication only, and is hence SAFE\n+\/\/            as well.\n+\/\/       (S1) According to (C3), p is in bounds of its memory object for all v in r. Since v0 and\n+\/\/            v1 are in r, it follows that p(v0) and p(v1) are in bounds of the same memory object.\n+\/\/       (S2) The difference of constants of mp(v0) and mp(v1) is exactly \"scale_v * stride_v\" (MP-DIFF).\n+\/\/            Given (C4), this difference is not too large.\n+\/\/       (S3) All summands of mp0 and mp1 are the same (only the constants differ), given (MP-DIFF).\n+\/\/\n+\/\/     It follows:\n+\/\/       p(v1) - p(v0) = mp(v1) - mp(v0)\n+\/\/\n+\/\/     Reformulating and applying (MP-DIFF) and (IH-P):\n+\/\/       p(v1) = p(v0)                                  + mp(v1) - mp(v1)\n+\/\/                                                        apply (MP-DIFF)\n+\/\/             = p(v0)                                  + scale_v * stride_v\n+\/\/               ------------ apply (IH-P) ------------\n+\/\/             = p(lo) - lo * scale_v + v0 * scale_v    + scale_v * stride_v\n+\/\/             = p(lo) - lo * scale_v + (v0 + stride_v) * scale_v\n+\/\/             = p(lo) - lo * scale_v + v1 * scale_v\n+\/\/\n+\/\/     This proves Corrolary P.\n+\/\/\n@@ -469,0 +566,7 @@\n+  \/\/ Use case: overlap.\n+  \/\/ Note: the bounds are exclusive: lo < element < hi\n+  bool is_always_in_distance_range(const jint distance_lo, const jint distance_hi) const {\n+    return _aliasing == AlwaysAtDistance &&\n+           (distance_lo < _distance && _distance < distance_hi);\n+  }\n+\n@@ -543,1 +647,145 @@\n-    tty->print(\" * [%d %s]\", _variable->_idx, _variable->Name());\n+    st->print(\" * [%d %s]\", _variable->_idx, _variable->Name());\n+  }\n+\n+  static void print_on(outputStream* st, NoOverflowInt con, const GrowableArray<MemPointerSummand>& summands) {\n+    st->print(\"Summands (%d): con(\", summands.length());\n+    con.print_on(st);\n+    st->print(\")\");\n+    for (int i = 0; i < summands.length(); i++) {\n+      st->print(\" + \");\n+      summands.at(i).print_on(tty);\n+    }\n+    st->cr();\n+  }\n+#endif\n+};\n+\n+\/\/ We need two different ways of tracking the summands:\n+\/\/ - MemPointerRawSummand: designed to keep track of the original form of\n+\/\/                         the pointer, preserving its overflow behavior.\n+\/\/ - MemPointerSummand:    designed to allow simplification of the MemPointer\n+\/\/                         form, does not preserve the original form and\n+\/\/                         ignores overflow from ConvI2L.\n+\/\/\n+\/\/ The MemPointerSummand is designed to allow the simplification of\n+\/\/ the MemPointer form as much as possible, to allow aliasing checks\n+\/\/ to be as simple as possible. For example, the pointer:\n+\/\/\n+\/\/   pointer = base + 2L * ConvI2L(i + 4 * j + con1) + con2\n+\/\/\n+\/\/ is simplified to this MemPointer form:\n+\/\/\n+\/\/   pointer = base + 2L * ConvI2L(i) + 8L * ConvI2L(j) + con\n+\/\/   con = 2L * con1 + con2\n+\/\/\n+\/\/ This is really convenient, because this way we are able to ignore\n+\/\/ the ConvI2L in the aliasing anaylsis computation, and we can collect\n+\/\/ all constants to a single constant. Even with this simplicication,\n+\/\/ we are able to prove the correctness of the aliasing checks.\n+\/\/\n+\/\/ However, there is one thing we are not able to do with this simplification:\n+\/\/ we cannot reconstruct the original pointer expression, because the\n+\/\/ simplification ignores overflows that could happen inside the ConvI2L:\n+\/\/\n+\/\/   2L * ConvI2L(i + 4 * j + con1) != 2L * ConvI2L(i) + 8L * ConvI2L(j) + 2L * con1\n+\/\/\n+\/\/ The MemPointerRawSummand is designed to keep track of the original form\n+\/\/ of the pointer, preserving its overflow behaviour. We observe that the\n+\/\/ only critical point for overflows is at the ConvI2L. Thus, we give each\n+\/\/ ConvI2L a \"int group\" id > 0, and all raw summands belonging to that ConvI2L\n+\/\/ have that id. This allows us to reconstruct which raw summands need to\n+\/\/ be added together before the ConvI2L. Any raw summands that do not belong\n+\/\/ to a ConvI2L (i.e. the summands with long variables) have \"int group\"\n+\/\/ id = 0, since they do not belong to any such \"int group\" and can be\n+\/\/ directly added together. For raw summands belonging to an \"int group\",\n+\/\/ we need to track the scale inside (scaleI) and outside (scaleL) the\n+\/\/ ConvI2L. With the example from above:\n+\/\/\n+\/\/   pointer = base + 2L * ConvI2L(i + 4 * j + con1) + con2\n+\/\/\n+\/\/   _variable  = base  _variable  = i  _variable  = j  _variable  = null  _variable  = null\n+\/\/   _scaleI    = 1     _scaleI    = 1  _scaleI    = 4  _scaleI    = con1  _scaleI    = 1\n+\/\/   _scaleL    = 1     _scaleL    = 2  _scaleL    = 2  _scaleL    = 2     _scaleL    = con2\n+\/\/   _int_group = 0     _int_group = 1  _int_group = 1  _int_group = 1     _int_group = 0\n+\/\/\n+\/\/ Note: we also need to track constants as separate raw summands. For\n+\/\/       this, we say that a raw summand tracks a constant iff _variable == null,\n+\/\/       and we store the constant value in _scaleI (for int constant) and in\n+\/\/       _scaleL (for long constants).\n+\/\/\n+class MemPointerRawSummand : public StackObj {\n+private:\n+  Node* _variable;\n+  NoOverflowInt _scaleI;\n+  NoOverflowInt _scaleL;\n+  int _int_group;\n+\n+public:\n+  MemPointerRawSummand(Node* variable, NoOverflowInt scaleI, NoOverflowInt scaleL, int int_group) :\n+    _variable(variable), _scaleI(scaleI), _scaleL(scaleL), _int_group(int_group) {}\n+\n+  MemPointerRawSummand() :\n+    MemPointerRawSummand(nullptr, NoOverflowInt::make_NaN(), NoOverflowInt::make_NaN(), -1) {}\n+\n+  static MemPointerRawSummand make_trivial(Node* variable) {\n+    assert(variable != nullptr, \"must have variable\");\n+    return MemPointerRawSummand(variable, NoOverflowInt(1), NoOverflowInt(1), 0);\n+  }\n+\n+  static MemPointerRawSummand make_con(NoOverflowInt scaleI, NoOverflowInt scaleL, int int_group) {\n+    return MemPointerRawSummand(nullptr, scaleI, scaleL, int_group);\n+  }\n+\n+  bool is_valid() const { return _int_group >= 0; }\n+  bool is_con() const { assert(is_valid(), \"\"); return _variable == nullptr; }\n+  Node* variable() const { assert(is_valid(), \"\"); return _variable; }\n+  NoOverflowInt scaleI() const { assert(is_valid(), \"\"); return _scaleI; }\n+  NoOverflowInt scaleL() const { assert(is_valid(), \"\"); return _scaleL; }\n+  int int_group() const { assert(is_valid(), \"\"); return _int_group; }\n+\n+  MemPointerSummand to_summand() const {\n+    assert(!is_con(), \"must be variable\");\n+    return MemPointerSummand(variable(), scaleL() * scaleI());\n+  }\n+\n+  NoOverflowInt to_con() const {\n+    assert(is_con(), \"must be constant\");\n+    return scaleL() * scaleI();\n+  }\n+\n+  static int cmp_by_int_group_and_variable_idx(MemPointerRawSummand* p1, MemPointerRawSummand* p2) {\n+    int int_group_diff = p1->int_group() - p2->int_group();\n+    if (int_group_diff != 0) { return int_group_diff; }\n+\n+    if (p1->is_con()) {\n+      return p2->is_con() ? 0 : 1;\n+    }\n+    if (p2->is_con()) {\n+      return -1;\n+    }\n+    return p1->variable()->_idx - p2->variable()->_idx;\n+  }\n+\n+#ifndef PRODUCT\n+  void print_on(outputStream* st) const {\n+    if (!is_valid()) {\n+      st->print(\"<invalid>\");\n+    } else {\n+      st->print(\"<%d: \", _int_group);\n+      _scaleL.print_on(st);\n+      st->print(\" * \");\n+      _scaleI.print_on(st);\n+      if (!is_con()) {\n+        st->print(\" * [%d %s]\", _variable->_idx, _variable->Name());\n+      }\n+      st->print(\">\");\n+    }\n+  }\n+\n+  static void print_on(outputStream* st, const GrowableArray<MemPointerRawSummand>& summands) {\n+    st->print(\"Raw Summands (%d): \", summands.length());\n+    for (int i = 0; i < summands.length(); i++) {\n+      if (i > 0) { st->print(\" + \"); }\n+      summands.at(i).print_on(tty);\n+    }\n+    st->cr();\n@@ -585,4 +833,7 @@\n-  \/\/ We limit the number of summands to 10. This is just a best guess, and not at this\n-  \/\/ point supported by evidence. But I think it is reasonable: usually, a pointer\n-  \/\/ contains a base pointer (e.g. array pointer or null for native memory) and a few\n-  \/\/ variables. It should be rare that we have more than 9 variables.\n+  \/\/ We limit the number of summands to 10, and the raw summands to 16. This is just a\n+  \/\/ best guess, and not at this point supported by evidence. But I think it is reasonable:\n+  \/\/ usually, a pointer contains a base pointer (e.g. array pointer or null for native memory)\n+  \/\/ and a few variables. It should be rare that we have more than 9 variables. We need\n+  \/\/ a few more raw summands, especially because there can be multiple constants, one\n+  \/\/ per ConvI2L \"int group\".\n+  static const int RAW_SUMMANDS_SIZE = 16;\n@@ -640,0 +891,7 @@\n+  \/\/ Raw summands: represent the pointer form exactly, allowing the reconstruction of the\n+  \/\/               pointer expression. Overflows inside the \"int groups\" (i.e. ConvI2L)\n+  \/\/               are preserved, and there may be multiple constants.\n+  MemPointerRawSummand _raw_summands[RAW_SUMMANDS_SIZE];\n+\n+  \/\/ Summands:     Simplified form, with only a single constant. Makes aliasing analysis\n+  \/\/               much simpler.\n@@ -643,0 +901,2 @@\n+\n+  \/\/ Size in bytes for the referenced memory region: [pointer, pointer + size)\n@@ -644,0 +904,1 @@\n+\n@@ -662,0 +923,1 @@\n+             const GrowableArray<MemPointerRawSummand>& raw_summands,\n@@ -673,0 +935,1 @@\n+    assert(raw_summands.length() <= RAW_SUMMANDS_SIZE, \"raw summands must fit\");\n@@ -679,0 +942,5 @@\n+    for (int i = 0; i < raw_summands.length(); i++) {\n+      const MemPointerRawSummand& s = raw_summands.at(i);\n+      assert(!s.scaleI().is_NaN(), \"non-NaN scale\");\n+      assert(!s.scaleL().is_NaN(), \"non-NaN scale\");\n+    }\n@@ -681,0 +949,5 @@\n+    \/\/ Copy raw summands in the same order.\n+    for (int i = 0; i < raw_summands.length(); i++) {\n+      _raw_summands[i] = raw_summands.at(i);\n+    }\n+\n@@ -711,0 +984,5 @@\n+\n+    for (int i = 0; i < RAW_SUMMANDS_SIZE; i++) {\n+      _raw_summands[i] = old.raw_summands_at(i);\n+    }\n+\n@@ -736,0 +1014,2 @@\n+                         const GrowableArray<MemPointerRawSummand>& raw_summands,\n+                         const NoOverflowInt con,\n@@ -737,1 +1017,0 @@\n-                         const NoOverflowInt& con,\n@@ -740,2 +1019,4 @@\n-    if (summands.length() <= SUMMANDS_SIZE) {\n-      return MemPointer(pointer, summands, con, size NOT_PRODUCT(COMMA trace));\n+    if (raw_summands.length() <= RAW_SUMMANDS_SIZE &&\n+        summands.length() <= SUMMANDS_SIZE &&\n+        has_no_NaN_in_con_and_summands(con, summands)) {\n+      return MemPointer(pointer, raw_summands, summands, con, size NOT_PRODUCT(COMMA trace));\n@@ -747,0 +1028,9 @@\n+  static bool has_no_NaN_in_con_and_summands(const NoOverflowInt con,\n+                                             const GrowableArray<MemPointerSummand>& summands) {\n+    if (con.is_NaN()) { return false; }\n+    for (int i = 0; i < summands.length(); i++) {\n+      if (summands.at(i).scale().is_NaN()) { return false; }\n+    }\n+    return true;\n+  }\n+\n@@ -778,0 +1068,5 @@\n+  const MemPointerRawSummand& raw_summands_at(const uint i) const {\n+    assert(i < RAW_SUMMANDS_SIZE, \"in bounds\");\n+    return _raw_summands[i];\n+  }\n+\n@@ -782,0 +1077,19 @@\n+  int max_int_group() const {\n+    int n = 0;\n+    for (int i = 0; i < RAW_SUMMANDS_SIZE; i++) {\n+      const MemPointerRawSummand& s = _raw_summands[i];\n+      if (!s.is_valid()) { continue; }\n+      n = MAX2(n, s.int_group());\n+    }\n+    return n;\n+  }\n+\n+  template<typename Callback>\n+  void for_each_raw_summand_of_int_group(int int_group, Callback callback) const {\n+    for (int i = 0; i < RAW_SUMMANDS_SIZE; i++) {\n+      const MemPointerRawSummand& s = _raw_summands[i];\n+      if (!s.is_valid() || s.int_group() != int_group) { continue; }\n+      callback(s);\n+    }\n+  }\n+\n@@ -804,0 +1118,1 @@\n+  bool always_overlaps_with(const MemPointer& other) const;\n@@ -821,1 +1136,1 @@\n-  void print_on(outputStream* st, bool end_with_cr = true) const {\n+  void print_on(outputStream* st) const {\n@@ -827,1 +1142,42 @@\n-    if (end_with_cr) { st->cr(); }\n+    st->cr();\n+\n+    st->print(\"  raw: \");\n+\n+    int long_count = 0;\n+    for_each_raw_summand_of_int_group(0, [&] (const MemPointerRawSummand& s) {\n+      if (long_count > 0) { st->print(\" + \"); }\n+      long_count++;\n+      if (s.is_con()) {\n+        \/\/ Long constant.\n+        NoOverflowInt con = s.scaleI() * s.scaleL();\n+        con.print_on(st);\n+        st->print(\"L\");\n+      } else {\n+        \/\/ Long variable.\n+        assert(s.scaleI().is_one(), \"must be long variable\");\n+        s.scaleL().print_on(st);\n+        st->print(\"L * [%d %s]\", s.variable()->_idx, s.variable()->Name());\n+      }\n+    });\n+\n+    \/\/ Int groups, i.e. \"ConvI2L(...)\"\n+    for (int int_group = 1; int_group <= max_int_group(); int_group++) {\n+      if (long_count > 0) { st->print(\" + \"); }\n+      long_count++;\n+      int int_count = 0;\n+      for_each_raw_summand_of_int_group(int_group, [&] (const MemPointerRawSummand& s) {\n+        if (int_count == 0) {\n+          s.scaleL().print_on(st);\n+          st->print(\"L * ConvI2L(\");\n+        } else {\n+          st->print(\" + \");\n+        }\n+        int_count++;\n+        s.scaleI().print_on(st);\n+        if (!s.is_con()) {\n+          st->print(\" * [%d %s]\", s.variable()->_idx, s.variable()->Name());\n+        }\n+      });\n+      st->print(\")\");\n+    }\n+    st->cr();\n@@ -841,3 +1197,7 @@\n-  \/\/ Internal data-structures for parsing.\n-  NoOverflowInt _con;\n-  GrowableArray<MemPointerSummand> _worklist;\n+  \/\/ Internal data-structures for parsing raw summands.\n+  int _next_int_group = 1;\n+  GrowableArray<MemPointerRawSummand> _worklist;\n+  GrowableArray<MemPointerRawSummand> _raw_summands;\n+\n+  \/\/ Internal data-structures for parsing \"regular\" summands.\n+  NoOverflowInt _con = NoOverflowInt(0);\n@@ -853,1 +1213,0 @@\n-    _con(NoOverflowInt(0)),\n@@ -884,1 +1243,1 @@\n-  void parse_sub_expression(const MemPointerSummand& summand, MemPointerParserCallback& callback);\n+  void parse_sub_expression(const MemPointerRawSummand& summand, MemPointerParserCallback& callback);\n@@ -888,0 +1247,4 @@\n+\n+  void canonicalize_raw_summands();\n+  void create_summands();\n+  void canonicalize_summands();\n","filename":"src\/hotspot\/share\/opto\/mempointer.hpp","additions":380,"deletions":17,"binary":false,"changes":397,"status":"modified"},{"patch":"@@ -28,0 +28,1 @@\n+#include \"opto\/c2_globals.hpp\"\n@@ -47,4 +48,6 @@\n- *                    There are initially three Parse Predicates for each loop:\n- *                    - Loop Parse Predicate:             The Parse Predicate added for Loop Predicates.\n- *                    - Profiled Loop Parse Predicate:    The Parse Predicate added for Profiled Loop Predicates.\n- *                    - Loop Limit Check Parse Predicate: The Parse Predicate added for a Loop Limit Check Predicate.\n+ *                    There are initially four Parse Predicates for each loop:\n+ *                    - Loop Parse Predicate:               The Parse Predicate added for Loop Predicates.\n+ *                    - Profiled Loop Parse Predicate:      The Parse Predicate added for Profiled Loop Predicates.\n+ *                    - Loop Limit Check Parse Predicate:   The Parse Predicate added for a Loop Limit Check Predicate.\n+ *                    - Short Running Loop Parse Predicate: The Parse Predicate added for the short running long loop check.\n+ *                    - AutoVectorization Parse Predicate:  The Parse Predicate added for AutoVectorization runtime checks.\n@@ -52,1 +55,2 @@\n- *                      Loop Predicate) or a Loop Limit Check Predicate. These predicates will be checked at runtime while\n+ *                      Loop Predicate), a Loop Limit Check Predicate, a Short Running Long Loop Predicate, or a\n+ *                      AutoVectorization Runtime Check Predicate. These predicates will be checked at runtime while\n@@ -84,0 +88,15 @@\n+ *     - AutoVectorization:  This predicate is used for speculative runtime checks required for AutoVectorization.\n+ *       Runtime Check       There are multiple reasons why we need a runtime check to allow vectorization:\n+ *       Predicate           - Unknown aliasing:\n+ *                             An important compoinent of AutoVectorization is proving that memory addresses do not\n+ *                             alias, and can therefore be reordered. In some cases, this cannot be done statically\n+ *                             and a runtime check is necessary.\n+ *                           - Unknown alignment of native memory:\n+ *                             While heap objects have 8-byte alignment, off-heap (native) memory often has no alignment\n+ *                             guarantees. On platforms that require vectors to be aligned, we need to prove alignment.\n+ *                             We cannot do that statically with native memory, hence we need a runtime check.\n+ *                           The benefit of using a predicate is that we only have to compile the vectorized loop. If\n+ *                           the runtime check fails, we simply deoptimize. Should we eventually recompile, then the\n+ *                           predicate is not available any more, and we instead use a multiversioning approach with\n+ *                           both a vectorized and a scalar loop, where the runtime determines which loop is taken.\n+ *                           See: PhaseIdealLoop::maybe_multiversion_for_auto_vectorization_runtime_checks\n@@ -785,2 +804,4 @@\n-    PredicateBlockIterator auto_vectorization_check_iterator(current_node, Deoptimization::Reason_auto_vectorization_check);\n-    current_node = auto_vectorization_check_iterator.for_each(predicate_visitor);\n+    if (UseAutoVectorizationPredicate) {\n+      PredicateBlockIterator auto_vectorization_check_iterator(current_node, Deoptimization::Reason_auto_vectorization_check);\n+      current_node = auto_vectorization_check_iterator.for_each(predicate_visitor);\n+    }\n","filename":"src\/hotspot\/share\/opto\/predicates.hpp","additions":28,"deletions":7,"binary":false,"changes":35,"status":"modified"},{"patch":"@@ -835,0 +835,5 @@\n+  \/\/ If we can speculate (using the aliasing runtime check), we can drop the weak edges,\n+  \/\/ and later insert a runtime check.\n+  \/\/ If we cannot speculate (aliasing analysis runtime checks), we need to respect all edges.\n+  bool speculate_away_weak_edges = _vloop.use_speculative_aliasing_checks();\n+\n@@ -841,0 +846,1 @@\n+      if (speculate_away_weak_edges && preds.is_current_weak_memory_edge()) { continue; }\n@@ -872,0 +878,6 @@\n+\n+  \/\/ If we can speculate (using the aliasing runtime check), we can drop the weak edges,\n+  \/\/ and later insert a runtime check.\n+  \/\/ If we cannot speculate (aliasing analysis runtime checks), we need to respect all edges.\n+  bool speculate_away_weak_edges = _vloop.use_speculative_aliasing_checks();\n+\n@@ -875,0 +887,1 @@\n+      if (speculate_away_weak_edges && preds.is_current_weak_memory_edge()) { continue; }\n@@ -1939,0 +1952,1 @@\n+                        _vloop.is_trace_speculative_aliasing_analysis(),\n@@ -1992,1 +2006,2 @@\n-  apply_speculative_runtime_checks();\n+  apply_speculative_alignment_runtime_checks();\n+  apply_speculative_aliasing_runtime_checks();\n","filename":"src\/hotspot\/share\/opto\/superword.cpp","additions":16,"deletions":1,"binary":false,"changes":17,"status":"modified"},{"patch":"@@ -97,1 +97,1 @@\n-        VTransformBoolVectorNode* vtn_mask_cmp = vtn->in(1)->isa_BoolVector();\n+        VTransformBoolVectorNode* vtn_mask_cmp = vtn->in_req(1)->isa_BoolVector();\n@@ -302,1 +302,4 @@\n-void SuperWordVTransformBuilder::add_memory_dependencies_of_node_to_vtnode(Node*n, VTransformNode* vtn, VectorSet& vtn_memory_dependencies) {\n+void SuperWordVTransformBuilder::add_memory_dependencies_of_node_to_vtnode(Node* n, VTransformNode* vtn, VectorSet& vtn_memory_dependencies) {\n+  \/\/ If we cannot speculate, then all dependencies must be strong edges, i.e. scheduling must respect them.\n+  bool are_speculative_checks_possible = _vloop.are_speculative_checks_possible();\n+\n@@ -307,0 +310,1 @@\n+    assert(n->is_Mem() && pred->is_Mem(), \"only memory edges\");\n@@ -312,2 +316,5 @@\n-    assert(n->is_Mem() && pred->is_Mem(), \"only memory edges\");\n-    vtn->add_memory_dependency(dependency); \/\/ Add every dependency only once per vtn.\n+    if (are_speculative_checks_possible && preds.is_current_weak_memory_edge()) {\n+      vtn->add_weak_memory_edge(dependency);\n+    } else {\n+      vtn->add_strong_memory_edge(dependency);\n+    }\n","filename":"src\/hotspot\/share\/opto\/superwordVTransformBuilder.cpp","additions":11,"deletions":4,"binary":false,"changes":15,"status":"modified"},{"patch":"@@ -49,0 +49,1 @@\n+  flags(SPECULATIVE_ALIASING_ANALYSIS, \"Trace Speculative Aliasing Analysis\") \\\n","filename":"src\/hotspot\/share\/opto\/traceAutoVectorizationTag.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -26,0 +26,1 @@\n+#include \"opto\/castnode.hpp\"\n@@ -28,0 +29,2 @@\n+#include \"opto\/divnode.hpp\"\n+#include \"opto\/movenode.hpp\"\n@@ -29,0 +32,2 @@\n+#include \"opto\/noOverflowInt.hpp\"\n+#include \"opto\/phaseX.hpp\"\n@@ -260,0 +265,3 @@\n+\/\/    - Strong edge: must be respected.\n+\/\/    - Weak edge:   if we add a speculative aliasing check, we can violate\n+\/\/                   the edge, i.e. spaw the order.\n@@ -266,1 +274,2 @@\n-  GrowableArray<int> memory_pred_edges;\n+  GrowableArray<int> strong_memory_edges;\n+  GrowableArray<int> weak_memory_edges;\n@@ -278,1 +287,2 @@\n-      memory_pred_edges.clear();\n+      strong_memory_edges.clear();\n+      weak_memory_edges.clear();\n@@ -289,0 +299,2 @@\n+\n+        \/\/ If we can prove that they will never overlap -> drop edge.\n@@ -290,2 +302,5 @@\n-          \/\/ Possibly overlapping memory\n-          memory_pred_edges.append(_body.bb_idx(n2));\n+          if (p1.can_make_speculative_aliasing_check_with(p2)) {\n+            weak_memory_edges.append(_body.bb_idx(n2));\n+          } else {\n+            strong_memory_edges.append(_body.bb_idx(n2));\n+          }\n@@ -294,1 +309,1 @@\n-      if (memory_pred_edges.is_nonempty()) {\n+      if (strong_memory_edges.is_nonempty() || weak_memory_edges.is_nonempty()) {\n@@ -297,1 +312,1 @@\n-        add_node(n1, memory_pred_edges);\n+        add_node(n1, strong_memory_edges, weak_memory_edges);\n@@ -308,1 +323,1 @@\n-void VLoopDependencyGraph::add_node(MemNode* n, GrowableArray<int>& memory_pred_edges) {\n+void VLoopDependencyGraph::add_node(MemNode* n, GrowableArray<int>& strong_memory_edges, GrowableArray<int>& weak_memory_edges) {\n@@ -310,2 +325,1 @@\n-  assert(!memory_pred_edges.is_empty(), \"no need to create a node without edges\");\n-  DependencyNode* dn = new (_arena) DependencyNode(n, memory_pred_edges, _arena);\n+  DependencyNode* dn = new (_arena) DependencyNode(n, strong_memory_edges, weak_memory_edges, _arena);\n@@ -318,0 +332,3 @@\n+    \/\/ We must compute the dependence graph depth with all edges (including the weak edges), so that\n+    \/\/ the independence queries work correctly, no matter if we check independence with or without\n+    \/\/ weak edges.\n@@ -361,2 +378,7 @@\n-      for (uint j = 0; j < dn->memory_pred_edges_length(); j++) {\n-        Node* pred = _body.body().at(dn->memory_pred_edge(j));\n+      for (uint j = 0; j < dn->num_strong_memory_edges(); j++) {\n+        Node* pred = _body.body().at(dn->strong_memory_edge(j));\n+        tty->print(\"  %d %s\", pred->_idx, pred->Name());\n+      }\n+      tty->print(\" | weak:\");\n+      for (uint j = 0; j < dn->num_weak_memory_edges(); j++) {\n+        Node* pred = _body.body().at(dn->weak_memory_edge(j));\n@@ -370,1 +392,7 @@\n-  tty->print_cr(\" Complete dependency graph:\");\n+  \/\/ If we cannot speculate (aliasing analysis runtime checks), we need to respect all edges.\n+  bool with_weak_memory_edges = !_vloop.use_speculative_aliasing_checks();\n+  if (with_weak_memory_edges) {\n+    tty->print_cr(\" Complete dependency graph (with weak edges, because we cannot speculate):\");\n+  } else {\n+    tty->print_cr(\" Dependency graph without weak edges (because we can speculate):\");\n+  }\n@@ -375,0 +403,1 @@\n+      if (!with_weak_memory_edges && it.is_current_weak_memory_edge()) { continue; }\n@@ -384,1 +413,2 @@\n-                                                     GrowableArray<int>& memory_pred_edges,\n+                                                     GrowableArray<int>& strong_memory_edges,\n+                                                     GrowableArray<int>& weak_memory_edges,\n@@ -387,2 +417,3 @@\n-    _memory_pred_edges_length(memory_pred_edges.length()),\n-    _memory_pred_edges(nullptr)\n+    _num_strong_memory_edges(strong_memory_edges.length()),\n+    _num_weak_memory_edges(weak_memory_edges.length()),\n+    _memory_edges(nullptr)\n@@ -390,4 +421,11 @@\n-  assert(memory_pred_edges.is_nonempty(), \"not empty\");\n-  uint bytes = memory_pred_edges.length() * sizeof(int);\n-  _memory_pred_edges = (int*)arena->Amalloc(bytes);\n-  memcpy(_memory_pred_edges, memory_pred_edges.adr_at(0), bytes);\n+  assert(strong_memory_edges.is_nonempty() || weak_memory_edges.is_nonempty(), \"only generate DependencyNode if there are pred edges\");\n+  uint bytes_strong = strong_memory_edges.length() * sizeof(int);\n+  uint bytes_weak = weak_memory_edges.length() * sizeof(int);\n+  uint bytes_total = bytes_strong + bytes_weak;\n+  _memory_edges = (int*)arena->Amalloc(bytes_total);\n+  if (strong_memory_edges.length() > 0) {\n+    memcpy(_memory_edges, strong_memory_edges.adr_at(0), bytes_strong);\n+  }\n+  if (weak_memory_edges.length() > 0) {\n+    memcpy(_memory_edges + strong_memory_edges.length(), weak_memory_edges.adr_at(0), bytes_weak);\n+  }\n@@ -403,4 +441,7 @@\n-    _next_pred(0),\n-    _end_pred(node->req()),\n-    _next_memory_pred(0),\n-    _end_memory_pred((_dependency_node != nullptr) ? _dependency_node->memory_pred_edges_length() : 0)\n+    _is_current_weak_memory_edge(false),\n+    _next_data_edge(0),\n+    _end_data_edge(node->req()),\n+    _next_strong_memory_edge(0),\n+    _end_strong_memory_edge((_dependency_node != nullptr) ? _dependency_node->num_strong_memory_edges() : 0),\n+    _next_weak_memory_edge(0),\n+    _end_weak_memory_edge((_dependency_node != nullptr) ? _dependency_node->num_weak_memory_edges() : 0)\n@@ -409,3 +450,5 @@\n-    \/\/ Load: address\n-    \/\/ Store: address, value\n-    _next_pred = MemNode::Address;\n+    \/\/ Ignore ctrl and memory, only address and value are data dependencies.\n+    \/\/ Memory edges are already covered by the strong and weak memory edges.\n+    \/\/ Load:  [ctrl, memory] address\n+    \/\/ Store: [ctrl, memory] address, value\n+    _next_data_edge = MemNode::Address;\n@@ -414,1 +457,1 @@\n-    _next_pred = 1; \/\/ skip control\n+    _next_data_edge = 1; \/\/ skip control\n@@ -420,2 +463,2 @@\n-  if (_next_pred < _end_pred) {\n-    _current = _node->in(_next_pred++);\n+  if (_next_data_edge < _end_data_edge) {\n+    _current = _node->in(_next_data_edge++);\n@@ -423,2 +466,8 @@\n-  } else if (_next_memory_pred < _end_memory_pred) {\n-    int pred_bb_idx = _dependency_node->memory_pred_edge(_next_memory_pred++);\n+    _is_current_weak_memory_edge = false;\n+  } else if (_next_strong_memory_edge < _end_strong_memory_edge) {\n+    int pred_bb_idx = _dependency_node->strong_memory_edge(_next_strong_memory_edge++);\n+    _current = _dependency_graph._body.body().at(pred_bb_idx);\n+    _is_current_memory_edge = true;\n+    _is_current_weak_memory_edge = false;\n+  } else if (_next_weak_memory_edge < _end_weak_memory_edge) {\n+    int pred_bb_idx = _dependency_node->weak_memory_edge(_next_weak_memory_edge++);\n@@ -427,0 +476,1 @@\n+    _is_current_weak_memory_edge = true;\n@@ -430,0 +480,579 @@\n+    _is_current_weak_memory_edge = false;\n+  }\n+}\n+\n+\/\/ Computing aliasing runtime check using init and last of main-loop\n+\/\/ -----------------------------------------------------------------\n+\/\/\n+\/\/ We have two VPointer vp1 and vp2, and would like to create a runtime check that\n+\/\/ guarantees that the corresponding pointers p1 and p2 do not overlap (alias) for\n+\/\/ any iv value in the strided range r = [init, init + iv_stride, .. limit).\n+\/\/\n+\/\/   for all iv in r: p1(iv) + size1 <= p2(iv) OR p2(iv) + size2 <= p1(iv)\n+\/\/\n+\/\/ This would allow situations where for some iv p1 is lower than p2, and for\n+\/\/ other iv p1 is higher than p2. This is not very useful in practice. We can\n+\/\/ strengthen the condition, which will make the check simpler later:\n+\/\/\n+\/\/   for all iv in r: p1(iv) + size1 <= p2(iv)                    (P1-BEFORE-P2)\n+\/\/   OR\n+\/\/   for all iv in r: p2(iv) + size2 <= p1(iv)                    (P1-AFTER-P2)\n+\/\/\n+\/\/ Note: apart from this strengthening, the checks we derive below are byte accurate,\n+\/\/       i.e. they are equivalent to the conditions above. This means we have NO case\n+\/\/       where:\n+\/\/       1) The check passes (predicts no overlap) but the pointers do actually overlap.\n+\/\/          This would be bad because we would wrongly vectorize, possibly leading to\n+\/\/          wrong results.\n+\/\/       2) The check does not pass (predicts overlap) but the pointers do not overlap.\n+\/\/          This would be suboptimal, as we would not be able to vectorize, and either\n+\/\/          trap (with predicate), or go into the slow-loop (with multiversioning).\n+\/\/\n+\/\/\n+\/\/ We apply the \"MemPointer Linearity Corrolary\" to VPointer vp and the corresponding\n+\/\/ pointer p:\n+\/\/   (C0) is given by the construction of VPointer vp, which simply wraps a MemPointer mp.\n+\/\/   (c1) with v = iv and scale_v = iv_scale\n+\/\/   (C2) with r = [init, init + iv_stride, .. last - stride_v, last], which is the set\n+\/\/        of possible iv values in the loop, with \"init\" the first iv value, and \"last\"\n+\/\/        the last iv value which is closest to limit.\n+\/\/        Note: iv_stride > 0  ->  limit - iv_stride <= last < limit\n+\/\/              iv_stride < 0  ->  limit < last <= limit - iv_stride\n+\/\/        We have to be a little careful, and cannot just use \"limit\" instead of \"last\" as\n+\/\/        the last value in r, because the iv never reaches limit in the main-loop, and\n+\/\/        so we are not sure if the memory access at p(limit) is still in bounds.\n+\/\/        For now, we just assume that we can compute init and limit, and we will derive\n+\/\/        the computation of these values later on.\n+\/\/   (C3) the memory accesses for every iv value in the loop must be in bounds, otherwise\n+\/\/        the program has undefined behaviour already.\n+\/\/   (C4) abs(iv_scale * iv_stride) < 2^31 is given by the checks in\n+\/\/        VPointer::init_are_scale_and_stride_not_too_large.\n+\/\/\n+\/\/ Hence, it follows that we can see p and vp as linear functions of iv in r, i.e. for\n+\/\/ all iv values in the loop:\n+\/\/   p(iv)  = p(init)  - init * iv_scale + iv * iv_scale\n+\/\/   vp(iv) = vp(init) - init * iv_scale + iv * iv_scale\n+\/\/\n+\/\/ Hence, p1 and p2 have the linear form:\n+\/\/   p1(iv)  = p1(init) - init * iv_scale1 + iv * iv_scale1             (LINEAR-FORM-INIT)\n+\/\/   p2(iv)  = p2(init) - init * iv_scale2 + iv * iv_scale2\n+\/\/\n+\/\/ With the (Alternative Corrolary P) we get the alternative linar form:\n+\/\/   p1(iv)  = p1(last) - last * iv_scale1 + iv * iv_scale1             (LINEAR-FORM-LAST)\n+\/\/   p2(iv)  = p2(last) - last * iv_scale2 + iv * iv_scale2\n+\/\/\n+\/\/\n+\/\/ We can now use this linearity to construct aliasing runtime checks, depending on the\n+\/\/ different \"geometry\" of the two VPointer over their iv, i.e. the \"slopes\" of the linear\n+\/\/ functions. In the following graphs, the x-axis denotes the values of iv, from init to\n+\/\/ last. And the y-axis denotes the pointer position p(iv). Intuitively, this problem\n+\/\/ can be seen as having two bands that should not overlap.\n+\/\/\n+\/\/       Case 1                     Case 2                     Case 3\n+\/\/       parallel lines             same sign slope            different sign slope\n+\/\/                                  but not parallel\n+\/\/\n+\/\/       +---------+                +---------+                +---------+\n+\/\/       |         |                |        #|                |#        |\n+\/\/       |         |                |       # |                |  #      |\n+\/\/       |        #|                |      #  |                |    #    |\n+\/\/       |      #  |                |     #   |                |      #  |\n+\/\/       |    #    |                |    #    |                |        #|\n+\/\/       |  # ^    |                |   #     |                |        ^|\n+\/\/       |#   |   #|                |  #      |                |        ||\n+\/\/       |    v #  |                | #       |                |        v|\n+\/\/       |    #    |                |#       #|                |        #|\n+\/\/       |  #      |                |^     #  |                |      #  |\n+\/\/       |#        |                ||   #    |                |    #    |\n+\/\/       |         |                |v #      |                |  #      |\n+\/\/       |         |                |#        |                |#        |\n+\/\/       +---------+                +---------+                +---------+\n+\/\/\n+\/\/\n+\/\/ Case 1: parallel lines, i.e. iv_scale = iv_scale1 = iv_scale2\n+\/\/\n+\/\/   p1(iv)  = p1(init)  - init * iv_scale + iv * iv_scale\n+\/\/   p2(iv)  = p2(init)  - init * iv_scale + iv * iv_scale\n+\/\/\n+\/\/   Given this, it follows:\n+\/\/     p1(iv) + size1 <= p2(iv)      <==>      p1(init) + size1 <= p2(init)\n+\/\/     p2(iv) + size2 <= p1(iv)      <==>      p2(init) + size2 <= p1(init)\n+\/\/\n+\/\/   Hence, we do not have to check the condition for every iv, but only for init.\n+\/\/\n+\/\/   p1(init) + size1 <= p2(init)  OR  p2(init) + size2 <= p1(init)\n+\/\/   ----- is equivalent to -----      ---- is equivalent to ------\n+\/\/          (P1-BEFORE-P2)         OR         (P1-AFTER-P2)\n+\/\/\n+\/\/\n+\/\/ Case 2 and 3: different slopes, i.e. iv_scale1 != iv_scale2\n+\/\/\n+\/\/   Without loss of generality, we assume iv_scale1 < iv_scale2.\n+\/\/   (Otherwise, we just swap p1 and p2).\n+\/\/\n+\/\/   If iv_stride >= 0, i.e. init <= iv <= last:\n+\/\/     (iv - init) * iv_scale1 <= (iv - init) * iv_scale2\n+\/\/     (iv - last) * iv_scale1 >= (iv - last) * iv_scale2                 (POS-STRIDE)\n+\/\/   If iv_stride <= 0, i.e. last <= iv <= init:\n+\/\/     (iv - init) * iv_scale1 >= (iv - init) * iv_scale2\n+\/\/     (iv - last) * iv_scale1 <= (iv - last) * iv_scale2                 (NEG-STRIDE)\n+\/\/\n+\/\/   Below, we show that these conditions are equivalent:\n+\/\/\n+\/\/       p1(init) + size1 <= p2(init)       (if iv_stride >= 0)  |    p2(last) + size2 <= p1(last)      (if iv_stride >= 0)   |\n+\/\/       p1(last) + size1 <= p2(last)       (if iv_stride <= 0)  |    p2(init) + size2 <= p1(init)      (if iv_stride <= 0)   |\n+\/\/       ---- are equivalent to -----                            |    ---- are equivalent to -----                            |\n+\/\/              (P1-BEFORE-P2)                                   |           (P1-AFTER-P2)                                    |\n+\/\/                                                               |                                                            |\n+\/\/   Proof:                                                      |                                                            |\n+\/\/                                                               |                                                            |\n+\/\/     Assume: (P1-BEFORE-P2)                                    |  Assume: (P1-AFTER-P2)                                     |\n+\/\/       for all iv in r: p1(iv) + size1 <= p2(iv)               |    for all iv in r: p2(iv) + size2 <= p1(iv)               |\n+\/\/       => And since init and last in r =>                      |    => And since init and last in r =>                      |\n+\/\/       p1(init) + size1 <= p2(init)                            |    p2(init) + size2 <= p1(init)                            |\n+\/\/       p1(last) + size1 <= p2(last)                            |    p2(last) + size2 <= p1(last)                            |\n+\/\/                                                               |                                                            |\n+\/\/                                                               |                                                            |\n+\/\/     Assume: p1(init) + size1 <= p2(init)                      |  Assume: p2(last) + size2 <= p1(last)                      |\n+\/\/        and: iv_stride >= 0                                    |     and: iv_stride >= 0                                    |\n+\/\/                                                               |                                                            |\n+\/\/          size1 + p1(iv)                                       |       size2 + p2(iv)                                       |\n+\/\/                  --------- apply (LINEAR-FORM-INIT) --------- |               --------- apply (LINEAR-FORM-LAST) --------- |\n+\/\/        = size1 + p1(init) - init * iv_scale1 + iv * iv_scale1 |     = size2 + p2(last) - last * iv_scale2 + iv * iv_scale2 |\n+\/\/                           ------ apply (POS-STRIDE) --------- |                        ------ apply (POS-STRIDE) --------- |\n+\/\/       <= size1 + p1(init) - init * iv_scale2 + iv * iv_scale2 |    <= size2 + p2(last) - last * iv_scale1 + iv * iv_scale1 |\n+\/\/          -- assumption --                                     |       -- assumption --                                     |\n+\/\/       <=         p2(init) - init * iv_scale2 + iv * iv_scale2 |    <=         p1(last) - last * iv_scale1 + iv * iv_scale1 |\n+\/\/                  --------- apply (LINEAR-FORM-INIT) --------- |               --------- apply (LINEAR-FORM-LAST) --------- |\n+\/\/        =         p2(iv)                                       |     =         p1(iv)                                       |\n+\/\/                                                               |                                                            |\n+\/\/                                                               |                                                            |\n+\/\/     Assume: p1(last) + size1 <= p2(last)                      |  Assume: p2(init) + size2 <= p1(init)                      |\n+\/\/        and: iv_stride <= 0                                    |     and: iv_stride <= 0                                    |\n+\/\/                                                               |                                                            |\n+\/\/          size1 + p1(iv)                                       |       size2 + p2(iv)                                       |\n+\/\/                  --------- apply (LINEAR-FORM-LAST) --------- |               --------- apply (LINEAR-FORM-INIT) --------- |\n+\/\/        = size1 + p1(last) - last * iv_scale1 + iv * iv_scale1 |     = size2 + p2(init) - init * iv_scale2 + iv * iv_scale2 |\n+\/\/                           ------ apply (NEG-STRIDE) --------- |                        ------ apply (NEG-STRIDE) --------- |\n+\/\/       <= size1 + p1(last) - last * iv_scale2 + iv * iv_scale2 |    <= size2 + p2(init) - init * iv_scale1 + iv * iv_scale1 |\n+\/\/          -- assumption --                                     |       -- assumption --                                     |\n+\/\/       <=         p2(last) - last * iv_scale2 + iv * iv_scale2 |    <=         p1(init) - init * iv_scale1 + iv * iv_scale1 |\n+\/\/                  --------- apply (LINEAR-FORM-LAST) --------- |               --------- apply (LINEAR-FORM-INIT) --------- |\n+\/\/        =         p2(iv)                                       |     =         p1(iv)                                       |\n+\/\/                                                               |                                                            |\n+\/\/\n+\/\/   The obtained conditions already look very simple. However, we would like to avoid\n+\/\/   computing 4 addresses (p1(init), p1(last), p2(init), p2(last)), and would instead\n+\/\/   prefer to only compute 2 addresses, and derive the other two from the distance (span)\n+\/\/   between the pointers at init and last. Using (LINEAR-FORM-INIT), we get:\n+\/\/\n+\/\/     p1(last) = p1(init) - init * iv_scale1 + last * iv_scale1                 (SPAN-1)\n+\/\/                         --------------- defines -------------\n+\/\/                p1(init) + span1\n+\/\/\n+\/\/     p2(last) = p2(init) - init * iv_scale2 + last * iv_scale2                 (SPAN-2)\n+\/\/                         --------------- defines -------------\n+\/\/                p1(init) + span2\n+\/\/\n+\/\/     span1 = - init * iv_scale1 + last * iv_scale1 = (last - init) * iv_scale1\n+\/\/     span2 = - init * iv_scale2 + last * iv_scale2 = (last - init) * iv_scale2\n+\/\/\n+\/\/   Thus, we can use the conditions below:\n+\/\/     p1(init)         + size1 <= p2(init)          OR  p2(init) + span2 + size2 <= p1(init) + span1    (if iv_stride >= 0)\n+\/\/     p1(init) + span1 + size1 <= p2(init) + span2  OR  p2(init)         + size2 <= p1(init)            (if iv_stride <= 0)\n+\/\/\n+\/\/   Below, we visualize the conditions, so that the reader can gain an intuitiion.\n+\/\/   For simplicity, we only show the case with iv_stride > 0. Also, remember that\n+\/\/   iv_scale1 < iv_scale2.\n+\/\/\n+\/\/                             +---------+                     +---------+\n+\/\/                             |        #|                     |        #| <-- p1(init) + span1\n+\/\/                             |       # |  ^ span2    span1 ^ |      # ^|\n+\/\/                             |      #  |  |                | |    #   ||\n+\/\/                             |     #   |  |                | |  #     v| <-- p2(init) + span2 + size2\n+\/\/                             |    #    |  |                v |#       #|\n+\/\/                             |   #     |  |          span2 ^ |       # |\n+\/\/                             |  #      |  |                | |      #  |\n+\/\/                             | #       |  |                | |     #   |\n+\/\/        p2(init)         --> |#       #|  v                | |    #    |\n+\/\/                             |^     #  |  ^ span1          | |   #     |\n+\/\/                             ||   #    |  |                | |  #      |\n+\/\/        p1(init) + size1 --> |v #      |  |                | | #       |\n+\/\/                             |#        |  v                v |#        |\n+\/\/                             +---------+                     +---------+\n+\/\/\n+\/\/ -------------------------------------------------------------------------------------------------------------------------\n+\/\/\n+\/\/ Computing the last iv value in a loop\n+\/\/ -------------------------------------\n+\/\/\n+\/\/ Let us define a helper function, that computes the last iv value in a loop,\n+\/\/ given variable init and limit values, and a constant stride. If the loop\n+\/\/ is never entered, we just return the init value.\n+\/\/\n+\/\/   LAST(init, stride, limit), where stride > 0:   |  LAST(init, stride, limit), where stride < 0:\n+\/\/     last = init                                  |  last = init\n+\/\/     for (iv = init; iv < limit; iv += stride)    |  for (iv = init; iv > limit; iv += stride)\n+\/\/       last = iv                                  |    last = iv\n+\/\/\n+\/\/ It follows that for some k:\n+\/\/    last = init + k * stride\n+\/\/\n+\/\/ If the loop is not entered, we can set k=0.\n+\/\/\n+\/\/ If the loop is entered:\n+\/\/   last is very close to limit:\n+\/\/     stride > 0  ->  limit - stride <= last < limit\n+\/\/     stride < 0  ->  limit < last <= limit - stride\n+\/\/\n+\/\/     If stride > 0:\n+\/\/         limit        - stride                   <= last              <   limit\n+\/\/         limit        - stride                   <= init + k * stride <   limit\n+\/\/         limit - init - stride                   <=        k * stride <   limit - init\n+\/\/         limit - init - stride - 1               <         k * stride <=  limit - init - 1\n+\/\/        (limit - init - stride - 1) \/ stride     <         k          <= (limit - init - 1) \/ stride\n+\/\/        (limit - init          - 1) \/ stride - 1 <         k          <= (limit - init - 1) \/ stride\n+\/\/     -> k = (limit - init - 1) \/ stride\n+\/\/     -> dividend \"limit - init - 1\" is >=0. So a regular round to zero division can be used.\n+\/\/        Note: to incorporate the case where the loop is not entered (init >= limit), we see\n+\/\/              that the divident is zero or negative, and so the result will be zero or\n+\/\/              negative. Thus, we can just clamp k to zero, or last to init, so that we get\n+\/\/              a solution that also works when the loop is not entered:\n+\/\/\n+\/\/              k = (limit - init - 1) \/ abs(stride)\n+\/\/              last = MAX(init, init + k * stride)\n+\/\/\n+\/\/     If stride < 0:\n+\/\/         limit                               <  last              <=   limit        - stride\n+\/\/         limit                               <  init + k * stride <=   limit        - stride\n+\/\/         limit - init                        <         k * stride <=   limit - init - stride\n+\/\/         limit - init + 1                    <=        k * stride <    limit - init - stride + 1\n+\/\/        (limit - init + 1) \/     stride      >=        k          >   (limit - init - stride + 1) \/     stride\n+\/\/       -(limit - init + 1) \/ abs(stride)     >=        k          >  -(limit - init - stride + 1) \/ abs(stride)\n+\/\/       -(limit - init + 1) \/ abs(stride)     >=        k          >  -(limit - init          + 1) \/ abs(stride) - 1\n+\/\/        (init - limit - 1) \/ abs(stride)     >=        k          >   (init - limit          - 1) \/ abs(stride) - 1\n+\/\/        (init - limit - 1) \/ abs(stride)     >=        k          >   (init - limit          - 1) \/ abs(stride) - 1\n+\/\/     -> k = (init - limit - 1) \/ abs(stride)\n+\/\/     -> dividend \"init - limit\" is >=0. So a regular round to zero division can be used.\n+\/\/        Note: to incorporate the case where the loop is not entered (init <= limit), we see\n+\/\/              that the divident is zero or negative, and so the result will be zero or\n+\/\/              negative. Thus, we can just clamp k to zero, or last to init, so that we get\n+\/\/              a solution that also works when the loop is not entered:\n+\/\/\n+\/\/              k = (init - limit - 1) \/ abs(stride)\n+\/\/              last = MIN(init, init + k * stride)\n+\/\/\n+\/\/ Now we can put it all together:\n+\/\/   LAST(init, stride, limit)\n+\/\/     If stride > 0:\n+\/\/       k = (limit - init - 1) \/ abs(stride)\n+\/\/       last = MAX(init, init + k * stride)\n+\/\/     If stride < 0:\n+\/\/       k = (init - limit - 1) \/ abs(stride)\n+\/\/       last = MIN(init, init + k * stride)\n+\/\/\n+\/\/ We will have to consider the implications of clamping to init when the loop is not entered\n+\/\/ at the use of LAST further down.\n+\/\/\n+\/\/ -------------------------------------------------------------------------------------------------------------------------\n+\/\/\n+\/\/ Computing init and last for the main-loop\n+\/\/ -----------------------------------------\n+\/\/\n+\/\/ As we have seen above, we always need the \"init\" of the main-loop. And if \"iv_scale1 != iv_scale2\", then we\n+\/\/ also need the \"last\" of the main-loop. These values need to be pre-loop invariant, because the check is\n+\/\/ to be performed before the pre-loop (at the predicate or multiversioning selector_if). It will be helpful\n+\/\/ to recall the iv structure in the pre and main-loop:\n+\/\/\n+\/\/                  | iv = pre_init\n+\/\/                  |\n+\/\/   Pre-Loop       | +----------------+\n+\/\/                  phi                |\n+\/\/                   |                 |  -> pre_last: last iv value in pre-loop\n+\/\/                   + pre_iv_stride   |\n+\/\/                   |-----------------+\n+\/\/                   | exit check: < pre_limit\n+\/\/                   |\n+\/\/                   | iv = main_init = init\n+\/\/                   |\n+\/\/   Main-Loop       | +------------------------------+\n+\/\/                   phi                              |\n+\/\/                    |                               | -> last: last iv value in main-loop\n+\/\/                    + main_iv_stride = iv_stride    |\n+\/\/                    |-------------------------------+\n+\/\/                    | exit check: < main_limit = limit\n+\/\/\n+\/\/ Unfortunately, the init (aka. main_init) is not pre-loop invariant, rather it is only available\n+\/\/ after the pre-loop. We will have to compute:\n+\/\/\n+\/\/   pre_last = LAST(pre_init, pre_iv_stride, pre_limit)\n+\/\/   init = pre_last + pre_iv_stride\n+\/\/\n+\/\/ If we need \"last\", we unfortunately must compute it as well:\n+\/\/\n+\/\/   last = LAST(init, iv_stride, limit)\n+\/\/\n+\/\/\n+\/\/ These computations assume that we indeed do enter the main-loop - otherwise\n+\/\/ it does not make sense to talk about the \"last main iteration\". Of course\n+\/\/ entering the main-loop implies that we entered the pre-loop already. But\n+\/\/ what happens if we check the aliasing runtime check, but later would never\n+\/\/ enter the main-loop?\n+\/\/\n+\/\/ First: no matter if we pass or fail the aliasing runtime check, we will\n+\/\/ not get wrong results. If we fail the check, we end up in the less optimized\n+\/\/ slow-loop. If we pass the check, and we don't enter the main-loop, we\n+\/\/ never rely on the aliasing check, after all only the vectorized main-loop\n+\/\/ (and the vectorized post-loop) rely on the aliasing check.\n+\/\/\n+\/\/ But: The worry is that we may fail the aliasing runtime check \"spuriously\",\n+\/\/ i.e. even though we would never enter the main-loop, and that this could have\n+\/\/ unfortunate side-effects (for example deopting unnecessarily). Let's\n+\/\/ look at the two possible cases:\n+\/\/  1) We would never even enter the pre-loop.\n+\/\/     There are only predicates between the aliasing runtime check and the pre-loop,\n+\/\/     so a predicate would have to fail. These are rather rare cases. If we\n+\/\/     are using multiversioning for the aliasing runtime check, we would\n+\/\/     immediately fail the predicate in either the slow or fast loop, so\n+\/\/     the decision of the aliasing runtime check does not matter. But if\n+\/\/     we are using a predicate for the aliaing runtime check, then we may\n+\/\/     end up deopting twice: once for the aliasing runtime check, and then\n+\/\/     again for the other predicate. This would not be great, but again,\n+\/\/     failing predicates are rare in the first place.\n+\/\/\n+\/\/  2) We would enter the pre-loop, but not the main-loop.\n+\/\/     The pre_last must be accurate, because we are entering the pre-loop.\n+\/\/     But then we fail the zero-trip guard of the main-loop. Thus, for the\n+\/\/     main-loop, the init lies \"after\" the limit. Thus, the computed last\n+\/\/     for the main-loop equals the init. This means that span1 and span2\n+\/\/     are zero. Hence, p1(init) and p2(init) would have to alias for the\n+\/\/     aliasing runtime check to fail. Hence, it would not be surprising\n+\/\/     at all if we deopted because of the aliasing runtime check.\n+\/\/\n+bool VPointer::can_make_speculative_aliasing_check_with(const VPointer& other) const {\n+  const VPointer& vp1 = *this;\n+  const VPointer& vp2 = other;\n+\n+  if (!_vloop.use_speculative_aliasing_checks()) { return false; }\n+\n+  \/\/ Both pointers need a nice linear form, otherwise we cannot formulate the check.\n+  if (!vp1.is_valid() || !vp2.is_valid()) { return false; }\n+\n+  \/\/ The pointers always overlap -> a speculative check would always fail.\n+  if (vp1.always_overlaps_with(vp2)) { return false; }\n+\n+  \/\/ The pointers never overlap -> a speculative check would always succeed.\n+  assert(!vp1.never_overlaps_with(vp2), \"ensured by caller\");\n+\n+  \/\/ The speculative aliasing check happens either at the AutoVectorization predicate\n+  \/\/ or at the multiversion_if. That is before the pre-loop. From the construction of\n+  \/\/ VPointer, we already know that all its variables (except iv) are pre-loop invariant.\n+  \/\/\n+  \/\/ For the computation of main_init, we also need the pre_limit, and so we need\n+  \/\/ to check that this value is pre-loop invariant. In the case of non-equal iv_scales,\n+  \/\/ we also need the main_limit in the aliasing check, and so this value must then\n+  \/\/ also be pre-loop invariant.\n+  Opaque1Node* pre_limit_opaq = _vloop.pre_loop_end()->limit()->as_Opaque1();\n+  Node* pre_limit = pre_limit_opaq->in(1);\n+  Node* main_limit = _vloop.cl()->limit();\n+\n+  if (!_vloop.is_pre_loop_invariant(pre_limit)) {\n+#ifdef ASSERT\n+    if (_vloop.is_trace_speculative_aliasing_analysis()) {\n+      tty->print_cr(\"VPointer::can_make_speculative_aliasing_check_with: pre_limit is not pre-loop independent!\");\n+    }\n+#endif\n+    return false;\n+  }\n+\n+  if (vp1.iv_scale() != vp2.iv_scale() && !_vloop.is_pre_loop_invariant(main_limit)) {\n+#ifdef ASSERT\n+    if (_vloop.is_trace_speculative_aliasing_analysis()) {\n+      tty->print_cr(\"VPointer::can_make_speculative_aliasing_check_with: main_limit is not pre-loop independent!\");\n+    }\n+#endif\n+    return false;\n+  }\n+\n+  return true;\n+}\n+\n+\/\/ For description and derivation see \"Computing the last iv value in a loop\".\n+\/\/ Note: the iv computations here should not overflow. But out of an abundance\n+\/\/       of caution, we compute everything in long anyway.\n+Node* make_last(Node* initL, jint stride, Node* limitL, PhaseIdealLoop* phase) {\n+  PhaseIterGVN& igvn = phase->igvn();\n+\n+  Node* abs_strideL = igvn.longcon(abs(stride));\n+  Node* strideL = igvn.longcon(stride);\n+\n+  \/\/ If in some rare case the limit is \"before\" init, then\n+  \/\/ this subtraction could overflow. Doing the calculations\n+  \/\/ in long prevents this. Below, we clamp the \"last\" value\n+  \/\/ back to init, which gets us back into the safe int range.\n+  Node* diffL = (stride > 0) ? new SubLNode(limitL, initL)\n+                             : new SubLNode(initL, limitL);\n+  Node* diffL_m1 = new AddLNode(diffL, igvn.longcon(-1));\n+  Node* k = new DivLNode(nullptr, diffL_m1, abs_strideL);\n+\n+  \/\/ Compute last = init + k * iv_stride\n+  Node* k_mul_stride = new MulLNode(k, strideL);\n+  Node* last = new AddLNode(initL, k_mul_stride);\n+\n+  \/\/ Make sure that the last does not lie \"before\" init.\n+  Node* last_clamped = MaxNode::build_min_max_long(&igvn, initL, last, stride > 0);\n+\n+  phase->register_new_node_with_ctrl_of(diffL,        initL);\n+  phase->register_new_node_with_ctrl_of(diffL_m1,     initL);\n+  phase->register_new_node_with_ctrl_of(k,            initL);\n+  phase->register_new_node_with_ctrl_of(k_mul_stride, initL);\n+  phase->register_new_node_with_ctrl_of(last,         initL);\n+  phase->register_new_node_with_ctrl_of(last_clamped, initL);\n+\n+  return last_clamped;\n+}\n+\n+BoolNode* make_a_plus_b_leq_c(Node* a, Node* b, Node* c, PhaseIdealLoop* phase) {\n+  Node* a_plus_b = new AddLNode(a, b);\n+  Node* cmp = CmpNode::make(a_plus_b, c, T_LONG, true);\n+  BoolNode* bol = new BoolNode(cmp, BoolTest::le);\n+  phase->register_new_node_with_ctrl_of(a_plus_b, a);\n+  phase->register_new_node_with_ctrl_of(cmp, a);\n+  phase->register_new_node_with_ctrl_of(bol, a);\n+  return bol;\n+}\n+\n+BoolNode* VPointer::make_speculative_aliasing_check_with(const VPointer& other) const {\n+  \/\/ Ensure iv_scale1 <= iv_scale2.\n+  const VPointer& vp1 = (this->iv_scale() <= other.iv_scale()) ? *this : other;\n+  const VPointer& vp2 = (this->iv_scale() <= other.iv_scale()) ? other :*this ;\n+  assert(vp1.iv_scale() <= vp2.iv_scale(), \"ensured by swapping if necessary\");\n+\n+  assert(vp1.can_make_speculative_aliasing_check_with(vp2), \"sanity\");\n+\n+  PhaseIdealLoop* phase = _vloop.phase();\n+  PhaseIterGVN& igvn = phase->igvn();\n+\n+  \/\/ init (aka main_init): compute it from the the pre-loop structure.\n+  \/\/ As described above, we cannot just take the _vloop.cl().init_trip(), because that\n+  \/\/ value is pre-loop dependent, and we need a pre-loop independent value, so we can\n+  \/\/ have it available at the predicate \/ multiversioning selector_if.\n+  \/\/ For this, we need to be sure that the pre_limit is pre-loop independent as well,\n+  \/\/ see can_make_speculative_aliasing_check_with.\n+  Node* pre_init = _vloop.pre_loop_end()->init_trip();\n+  jint pre_iv_stride = _vloop.pre_loop_end()->stride_con();\n+  Opaque1Node* pre_limit_opaq = _vloop.pre_loop_end()->limit()->as_Opaque1();\n+  Node* pre_limit = pre_limit_opaq->in(1);\n+  assert(_vloop.is_pre_loop_invariant(pre_init),  \"needed for aliasing check before pre-loop\");\n+  assert(_vloop.is_pre_loop_invariant(pre_limit), \"needed for aliasing check before pre-loop\");\n+\n+  Node* pre_initL = new ConvI2LNode(pre_init);\n+  Node* pre_limitL = new ConvI2LNode(pre_limit);\n+  phase->register_new_node_with_ctrl_of(pre_initL, pre_init);\n+  phase->register_new_node_with_ctrl_of(pre_limitL, pre_init);\n+\n+  Node* pre_lastL = make_last(pre_initL, pre_iv_stride, pre_limitL, phase);\n+\n+  Node* main_initL = new AddLNode(pre_lastL, igvn.longcon(pre_iv_stride));\n+  phase->register_new_node_with_ctrl_of(main_initL, pre_init);\n+\n+  Node* main_init = new ConvL2INode(main_initL);\n+  phase->register_new_node_with_ctrl_of(main_init, pre_init);\n+\n+  Node* p1_init = vp1.make_pointer_expression(main_init);\n+  Node* p2_init = vp2.make_pointer_expression(main_init);\n+  Node* size1 = igvn.longcon(vp1.size());\n+  Node* size2 = igvn.longcon(vp2.size());\n+\n+#ifdef ASSERT\n+  if (_vloop.is_trace_speculative_aliasing_analysis() || _vloop.is_trace_speculative_runtime_checks()) {\n+    tty->print_cr(\"\\nVPointer::make_speculative_aliasing_check_with:\");\n+    tty->print(\"pre_init:  \"); pre_init->dump();\n+    tty->print(\"pre_limit: \"); pre_limit->dump();\n+    tty->print(\"pre_lastL: \"); pre_lastL->dump();\n+    tty->print(\"main_init: \"); main_init->dump();\n+    tty->print_cr(\"p1_init:\");\n+    p1_init->dump_bfs(5, nullptr, \"\");\n+    tty->print_cr(\"p2_init:\");\n+    p2_init->dump_bfs(5, nullptr, \"\");\n+  }\n+#endif\n+\n+  BoolNode* condition1 = nullptr;\n+  BoolNode* condition2 = nullptr;\n+  if (vp1.iv_scale() == vp2.iv_scale()) {\n+#ifdef ASSERT\n+    if (_vloop.is_trace_speculative_aliasing_analysis() || _vloop.is_trace_speculative_runtime_checks()) {\n+      tty->print_cr(\"  Same iv_scale(%d) -> parallel lines -> simple conditions:\", vp1.iv_scale());\n+      tty->print_cr(\"  p1(init) + size1 <= p2(init)  OR  p2(init) + size2 <= p1(init)\");\n+      tty->print_cr(\"  -------- condition1 --------      ------- condition2 ---------\");\n+    }\n+#endif\n+    condition1 = make_a_plus_b_leq_c(p1_init, size1, p2_init, phase);\n+    condition2 = make_a_plus_b_leq_c(p2_init, size2, p1_init, phase);\n+  } else {\n+    assert(vp1.iv_scale() < vp2.iv_scale(), \"assumed in proof, established above by swapping\");\n+\n+#ifdef ASSERT\n+    if (_vloop.is_trace_speculative_aliasing_analysis() || _vloop.is_trace_speculative_runtime_checks()) {\n+      tty->print_cr(\"  Different iv_scale -> lines with different slopes -> more complex conditions:\");\n+      tty->print_cr(\"  p1(init)         + size1 <= p2(init)          OR  p2(init) + span2 + size2 <= p1(init) + span1  (if iv_stride >= 0)\");\n+      tty->print_cr(\"  p1(init) + span1 + size1 <= p2(init) + span2  OR  p2(init)         + size2 <= p1(init)          (if iv_stride <= 0)\");\n+      tty->print_cr(\"  ---------------- condition1 ----------------      --------------- condition2 -----------------\");\n+    }\n+#endif\n+\n+    \/\/ last (aka main_last): compute from main-loop structure.\n+    jint main_iv_stride = _vloop.iv_stride();\n+    Node* main_limit = _vloop.cl()->limit();\n+    assert(_vloop.is_pre_loop_invariant(main_limit), \"needed for aliasing check before pre-loop\");\n+\n+    Node* main_limitL = new ConvI2LNode(main_limit);\n+    phase->register_new_node_with_ctrl_of(main_limitL, pre_init);\n+\n+    Node* main_lastL = make_last(main_initL, main_iv_stride, main_limitL, phase);\n+\n+    \/\/ Compute span1 = (last - init) * iv_scale1\n+    \/\/         span2 = (last - init) * iv_scale2\n+    Node* last_minus_init = new SubLNode(main_lastL, main_initL);\n+    Node* iv_scale1 = igvn.longcon(vp1.iv_scale());\n+    Node* iv_scale2 = igvn.longcon(vp2.iv_scale());\n+    Node* span1 = new MulLNode(last_minus_init, iv_scale1);\n+    Node* span2 = new MulLNode(last_minus_init, iv_scale2);\n+\n+    phase->register_new_node_with_ctrl_of(last_minus_init, pre_init);\n+    phase->register_new_node_with_ctrl_of(span1,           pre_init);\n+    phase->register_new_node_with_ctrl_of(span2,           pre_init);\n+\n+#ifdef ASSERT\n+    if (_vloop.is_trace_speculative_aliasing_analysis() || _vloop.is_trace_speculative_runtime_checks()) {\n+      tty->print(\"main_limitL: \"); main_limitL->dump();\n+      tty->print(\"main_lastL: \"); main_lastL->dump();\n+      tty->print(\"p1_init: \"); p1_init->dump();\n+      tty->print(\"p2_init: \"); p2_init->dump();\n+      tty->print(\"size1: \"); size1->dump();\n+      tty->print(\"size2: \"); size2->dump();\n+      tty->print_cr(\"span1: \"); span1->dump_bfs(5, nullptr, \"\");\n+      tty->print_cr(\"span2: \"); span2->dump_bfs(5, nullptr, \"\");\n+    }\n+#endif\n+\n+    Node* p1_init_plus_span1 = new AddLNode(p1_init, span1);\n+    Node* p2_init_plus_span2 = new AddLNode(p2_init, span2);\n+    phase->register_new_node_with_ctrl_of(p1_init_plus_span1, pre_init);\n+    phase->register_new_node_with_ctrl_of(p2_init_plus_span2, pre_init);\n+    if (_vloop.iv_stride() >= 0) {\n+      condition1 = make_a_plus_b_leq_c(p1_init,            size1, p2_init,            phase);\n+      condition2 = make_a_plus_b_leq_c(p2_init_plus_span2, size2, p1_init_plus_span1, phase);\n+    } else {\n+      condition1 = make_a_plus_b_leq_c(p1_init_plus_span1, size1, p2_init_plus_span2, phase);\n+      condition2 = make_a_plus_b_leq_c(p2_init,            size2, p1_init,            phase);\n+    }\n+  }\n+\n+#ifdef ASSERT\n+  if (_vloop.is_trace_speculative_aliasing_analysis() || _vloop.is_trace_speculative_runtime_checks()) {\n+    tty->print_cr(\"condition1:\");\n+    condition1->dump_bfs(5, nullptr, \"\");\n+    tty->print_cr(\"condition2:\");\n+    condition2->dump_bfs(5, nullptr, \"\");\n@@ -431,0 +1060,87 @@\n+#endif\n+\n+  \/\/ Construct \"condition1 OR condition2\". Convert the bol value back to an int value\n+  \/\/ that we can \"OR\" to create a single bol value. On x64, the two CMove are converted\n+  \/\/ to two setbe instructions which capture the condition bits to a register, meaning\n+  \/\/ we only have a single branch in the end.\n+  Node* zero = igvn.intcon(0);\n+  Node* one  = igvn.intcon(1);\n+  Node* cmov1 = new CMoveINode(condition1, zero, one, TypeInt::INT);\n+  Node* cmov2 = new CMoveINode(condition2, zero, one, TypeInt::INT);\n+  phase->register_new_node_with_ctrl_of(cmov1, main_initL);\n+  phase->register_new_node_with_ctrl_of(cmov2, main_initL);\n+\n+  Node* c1_or_c2 = new OrINode(cmov1, cmov2);\n+  Node* cmp = CmpNode::make(c1_or_c2, zero, T_INT);\n+  BoolNode* bol = new BoolNode(cmp, BoolTest::ne);\n+  phase->register_new_node_with_ctrl_of(c1_or_c2, main_initL);\n+  phase->register_new_node_with_ctrl_of(cmp, main_initL);\n+  phase->register_new_node_with_ctrl_of(bol, main_initL);\n+\n+  return bol;\n+}\n+\n+Node* VPointer::make_pointer_expression(Node* iv_value) const {\n+  assert(is_valid(), \"must be valid\");\n+\n+  PhaseIdealLoop* phase = _vloop.phase();\n+  PhaseIterGVN& igvn = phase->igvn();\n+  Node* iv = _vloop.iv();\n+  Node* ctrl = phase->get_ctrl(iv_value);\n+\n+  auto maybe_add = [&] (Node* n1, Node* n2, BasicType bt) {\n+    if (n1 == nullptr) { return n2; }\n+    Node* add = AddNode::make(n1, n2, bt);\n+    phase->register_new_node(add, ctrl);\n+    return add;\n+  };\n+\n+  Node* expression = nullptr;\n+  mem_pointer().for_each_raw_summand_of_int_group(0, [&] (const MemPointerRawSummand& s) {\n+    Node* node = nullptr;\n+    if (s.is_con()) {\n+      \/\/ Long constant.\n+      NoOverflowInt con = s.scaleI() * s.scaleL();\n+      node = igvn.longcon(con.value());\n+    } else {\n+      \/\/ Long variable.\n+      assert(s.scaleI().is_one(), \"must be long variable\");\n+      Node* scaleL = igvn.longcon(s.scaleL().value());\n+      Node* variable = (s.variable() == iv) ? iv_value : s.variable();\n+      if (variable->bottom_type()->isa_ptr() != nullptr) {\n+        variable = new CastP2XNode(nullptr, variable);\n+        phase->register_new_node(variable, ctrl);\n+      }\n+      node = new MulLNode(scaleL, variable);\n+      phase->register_new_node(node, ctrl);\n+    }\n+    expression = maybe_add(expression, node, T_LONG);\n+  });\n+\n+  int max_int_group = mem_pointer().max_int_group();\n+  for (int int_group = 1; int_group <= max_int_group; int_group++) {\n+    Node* int_expression = nullptr;\n+    NoOverflowInt int_group_scaleL;\n+    mem_pointer().for_each_raw_summand_of_int_group(int_group, [&] (const MemPointerRawSummand& s) {\n+      Node* node = nullptr;\n+      if (s.is_con()) {\n+        node = igvn.intcon(s.scaleI().value());\n+      } else {\n+        Node* scaleI = igvn.intcon(s.scaleI().value());\n+        Node* variable = (s.variable() == iv) ? iv_value : s.variable();\n+        node = new MulINode(scaleI, variable);\n+        phase->register_new_node(node, ctrl);\n+      }\n+      int_group_scaleL = s.scaleL(); \/\/ remember for multiplication after ConvI2L\n+      int_expression = maybe_add(int_expression, node, T_INT);\n+    });\n+    assert(int_expression != nullptr, \"no empty int group\");\n+    int_expression = new ConvI2LNode(int_expression);\n+    phase->register_new_node(int_expression, ctrl);\n+    Node* scaleL = igvn.longcon(int_group_scaleL.value());\n+    int_expression = new MulLNode(scaleL, int_expression);\n+    phase->register_new_node(int_expression, ctrl);\n+    expression = maybe_add(expression, int_expression, T_LONG);\n+  }\n+\n+  return expression;\n","filename":"src\/hotspot\/share\/opto\/vectorization.cpp","additions":747,"deletions":31,"binary":false,"changes":778,"status":"modified"},{"patch":"@@ -32,0 +32,1 @@\n+#include \"utilities\/growableArray.hpp\"\n@@ -164,0 +165,4 @@\n+  bool use_speculative_aliasing_checks() const {\n+    return are_speculative_checks_possible() && UseAutoVectorizationSpeculativeAliasingChecks;\n+  }\n+\n@@ -206,0 +211,4 @@\n+\n+  bool is_trace_speculative_aliasing_analysis() const {\n+    return _vtrace.is_trace(TraceAutoVectorizationTag::SPECULATIVE_ALIASING_ANALYSIS);\n+  }\n@@ -225,0 +234,1 @@\n+    \/\/ Usually the ctrl of n is already before the pre-loop.\n@@ -226,4 +236,2 @@\n-\n-    \/\/ Quick test: is it in the main-loop?\n-    if (lpt()->is_member(phase()->get_loop(ctrl))) {\n-      return false;\n+    if (is_before_pre_loop(ctrl)) {\n+      return true;\n@@ -232,2 +240,6 @@\n-    \/\/ Is it before the pre-loop?\n-    return phase()->is_dominator(ctrl, pre_loop_head());\n+    \/\/ But in some cases, the ctrl of n is between the pre and\n+    \/\/ main loop, but the early ctrl is before the pre-loop.\n+    \/\/ As long as the early ctrl is before the pre-loop, we can\n+    \/\/ compute n before the pre-loop.\n+    Node* early = phase()->compute_early_ctrl(n, ctrl);\n+    return is_before_pre_loop(early);\n@@ -242,0 +254,10 @@\n+\n+  bool is_before_pre_loop(Node* ctrl) const {\n+    \/\/ Quick test: is it in the main-loop?\n+    if (lpt()->is_member(phase()->get_loop(ctrl))) {\n+      return false;\n+    }\n+\n+    \/\/ Is it before the pre-loop?\n+    return phase()->is_dominator(ctrl, pre_loop_head());\n+  }\n@@ -572,0 +594,3 @@\n+\/\/    - Strong edge: must be respected.\n+\/\/    - Weak edge:   if we add a speculative aliasing check, we can violate\n+\/\/                   the edge, i.e. swap the order.\n@@ -614,1 +639,1 @@\n-  void add_node(MemNode* n, GrowableArray<int>& memory_pred_edges);\n+  void add_node(MemNode* n, GrowableArray<int>& strong_memory_edges, GrowableArray<int>& weak_memory_edges);\n@@ -628,2 +653,3 @@\n-    const uint _memory_pred_edges_length;\n-    int* _memory_pred_edges; \/\/ memory pred-edges, mapping to bb_idx\n+    const uint _num_strong_memory_edges;\n+    const uint _num_weak_memory_edges;\n+    int* _memory_edges; \/\/ memory pred-edges, mapping to bb_idx\n@@ -631,1 +657,1 @@\n-    DependencyNode(MemNode* n, GrowableArray<int>& memory_pred_edges, Arena* arena);\n+    DependencyNode(MemNode* n, GrowableArray<int>& strong_memory_edges, GrowableArray<int>& weak_memory_edges, Arena* arena);\n@@ -633,1 +659,7 @@\n-    uint memory_pred_edges_length() const { return _memory_pred_edges_length; }\n+    uint num_strong_memory_edges() const { return _num_strong_memory_edges; }\n+    uint num_weak_memory_edges() const { return _num_weak_memory_edges; }\n+\n+    int strong_memory_edge(uint i) const {\n+      assert(i < _num_strong_memory_edges, \"bounds check\");\n+      return _memory_edges[i];\n+    }\n@@ -635,3 +667,3 @@\n-    int memory_pred_edge(uint i) const {\n-      assert(i < _memory_pred_edges_length, \"bounds check\");\n-      return _memory_pred_edges[i];\n+    int weak_memory_edge(uint i) const {\n+      assert(i < _num_weak_memory_edges, \"bounds check\");\n+      return _memory_edges[_num_strong_memory_edges + i];\n@@ -652,0 +684,9 @@\n+    bool _is_current_weak_memory_edge;\n+\n+    \/\/ Iterate in data edges, i.e. iterate node->in(i), excluding control and memory edges.\n+    int _next_data_edge;\n+    int _end_data_edge;\n+\n+    \/\/ Iterate in dependency_node->strong_memory_edges()\n+    int _next_strong_memory_edge;\n+    int _end_strong_memory_edge;\n@@ -653,3 +694,3 @@\n-    \/\/ Iterate in node->in(i)\n-    int _next_pred;\n-    int _end_pred;\n+    \/\/ Iterate in dependency_node->weak_memory_edge()\n+    int _next_weak_memory_edge;\n+    int _end_weak_memory_edge;\n@@ -657,3 +698,0 @@\n-    \/\/ Iterate in dependency_node->memory_pred_edge(i)\n-    int _next_memory_pred;\n-    int _end_memory_pred;\n@@ -665,0 +703,1 @@\n+\n@@ -669,0 +708,1 @@\n+\n@@ -673,0 +713,5 @@\n+\n+    bool is_current_weak_memory_edge() const {\n+      assert(!done(), \"not done yet\");\n+      return _is_current_weak_memory_edge;\n+    }\n@@ -937,0 +982,37 @@\n+  \/\/ Delegate to MemPointer::always_overlaps_with, but guard for invalid cases\n+  \/\/ where we must return a conservative answer: unknown overlap, return false.\n+  bool always_overlaps_with(const VPointer& other) const {\n+    if (!is_valid() || !other.is_valid()) {\n+#ifndef PRODUCT\n+      if (_vloop.mptrace().is_trace_overlap()) {\n+        tty->print_cr(\"VPointer::always_overlaps_with: invalid VPointer, overlap unknown.\");\n+      }\n+#endif\n+      return false;\n+    }\n+    return mem_pointer().always_overlaps_with(other.mem_pointer());\n+  }\n+\n+  static int cmp_summands(const VPointer& vp1, const VPointer& vp2) {\n+    return MemPointer::cmp_summands(vp1.mem_pointer(), vp2.mem_pointer());\n+  }\n+\n+  static int cmp_con(const VPointer& vp1, const VPointer& vp2) {\n+    \/\/ We use two comparisons, because a subtraction could underflow.\n+    jint con1 = vp1.con();\n+    jint con2 = vp2.con();\n+    if (con1 < con2) { return -1; }\n+    if (con1 > con2) { return  1; }\n+    return 0;\n+  }\n+\n+  static int cmp_summands_and_con(const VPointer& vp1, const VPointer& vp2) {\n+    int cmp = cmp_summands(vp1, vp2);\n+    if (cmp != 0) { return cmp; }\n+    return cmp_con(vp1, vp2);\n+  }\n+\n+  bool can_make_speculative_aliasing_check_with(const VPointer& other) const;\n+  Node* make_pointer_expression(Node* iv_value) const;\n+  BoolNode* make_speculative_aliasing_check_with(const VPointer& other) const;\n+\n","filename":"src\/hotspot\/share\/opto\/vectorization.hpp","additions":102,"deletions":20,"binary":false,"changes":122,"status":"modified"},{"patch":"@@ -26,0 +26,1 @@\n+#include \"opto\/vectorization.hpp\"\n@@ -60,1 +61,1 @@\n-  collect_nodes_without_req_or_dependency(stack);\n+  collect_nodes_without_strong_in_edges(stack);\n@@ -75,2 +76,5 @@\n-      for (int i = 0; i < vtn->outs(); i++) {\n-        VTransformNode* use = vtn->out(i);\n+      \/\/ We only need to respect the strong edges (data edges and strong memory edges).\n+      \/\/ Violated weak memory edges are allowed, but require a speculative aliasing\n+      \/\/ runtime check, see VTransform::apply_speculative_aliasing_runtime_checks.\n+      for (uint i = 0; i < vtn->out_strong_edges(); i++) {\n+        VTransformNode* use = vtn->out_strong_edge(i);\n@@ -112,2 +116,2 @@\n-\/\/ Push all \"root\" nodes, i.e. those that have no inputs (req or dependency):\n-void VTransformGraph::collect_nodes_without_req_or_dependency(GrowableArray<VTransformNode*>& stack) const {\n+\/\/ Push all \"root\" nodes, i.e. those that have no strong input edges (data edges and strong memory edges):\n+void VTransformGraph::collect_nodes_without_strong_in_edges(GrowableArray<VTransformNode*>& stack) const {\n@@ -116,1 +120,1 @@\n-    if (!vtn->has_req_or_dependency()) {\n+    if (!vtn->has_strong_in_edge()) {\n@@ -147,1 +151,1 @@\n-void VTransform::apply_speculative_runtime_checks() {\n+void VTransform::apply_speculative_alignment_runtime_checks() {\n@@ -151,1 +155,1 @@\n-      tty->print_cr(\"\\nVTransform::apply_speculative_runtime_checks: native memory alignment\");\n+      tty->print_cr(\"\\nVTransform::apply_speculative_alignment_runtime_checks: native memory alignment\");\n@@ -219,0 +223,177 @@\n+class VPointerWeakAliasingPair : public StackObj {\n+private:\n+  \/\/ Using references instead of pointers would be preferrable, but GrowableArray\n+  \/\/ requires a default constructor, and we do not have a default constructor for\n+  \/\/ VPointer.\n+  const VPointer* _vp1 = nullptr;\n+  const VPointer* _vp2 = nullptr;\n+\n+  VPointerWeakAliasingPair(const VPointer& vp1, const VPointer& vp2) : _vp1(&vp1), _vp2(&vp2) {\n+    assert(vp1.is_valid(), \"sanity\");\n+    assert(vp2.is_valid(), \"sanity\");\n+    assert(!vp1.never_overlaps_with(vp2), \"otherwise no aliasing\");\n+    assert(!vp1.always_overlaps_with(vp2), \"otherwise must be strong\");\n+    assert(VPointer::cmp_summands_and_con(vp1, vp2) <= 0, \"must be sorted\");\n+  }\n+\n+public:\n+  \/\/ Default constructor to make GrowableArray happy.\n+  VPointerWeakAliasingPair() : _vp1(nullptr), _vp2(nullptr) {}\n+\n+  static VPointerWeakAliasingPair make(const VPointer& vp1, const VPointer& vp2) {\n+    if (VPointer::cmp_summands_and_con(vp1, vp2) <= 0) {\n+      return VPointerWeakAliasingPair(vp1, vp2);\n+    } else {\n+      return VPointerWeakAliasingPair(vp2, vp1);\n+    }\n+  }\n+\n+  const VPointer& vp1() const { return *_vp1; }\n+  const VPointer& vp2() const { return *_vp2; }\n+\n+  \/\/ Sort by summands, so that pairs with same summands (summand1, summands2) are adjacent.\n+  static int cmp_for_sort(VPointerWeakAliasingPair* pair1, VPointerWeakAliasingPair* pair2) {\n+    int cmp_summands1 = VPointer::cmp_summands(pair1->vp1(), pair2->vp1());\n+    if (cmp_summands1 != 0) { return cmp_summands1; }\n+    return VPointer::cmp_summands(pair1->vp2(), pair2->vp2());\n+  }\n+};\n+\n+void VTransform::apply_speculative_aliasing_runtime_checks() {\n+\n+  if (_vloop.use_speculative_aliasing_checks()) {\n+\n+#ifdef ASSERT\n+    if (_trace._speculative_aliasing_analysis || _trace._speculative_runtime_checks) {\n+      tty->print_cr(\"\\nVTransform::apply_speculative_aliasing_runtime_checks: speculative aliasing analysis runtime checks\");\n+    }\n+#endif\n+\n+    \/\/ It would be nice to add a ResourceMark here. But it would collide with resource allocation\n+    \/\/ in PhaseIdealLoop::set_idom for _idom and _dom_depth. See also JDK-8337015.\n+    VectorSet visited;\n+    GrowableArray<VPointerWeakAliasingPair> weak_aliasing_pairs;\n+\n+    const GrowableArray<VTransformNode*>& schedule = _graph.get_schedule();\n+    for (int i = 0; i < schedule.length(); i++) {\n+      VTransformNode* vtn = schedule.at(i);\n+      for (uint i = 0; i < vtn->out_weak_edges(); i++) {\n+        VTransformNode* use = vtn->out_weak_edge(i);\n+        if (visited.test(use->_idx)) {\n+          \/\/ The use node was already visited, i.e. is higher up in the schedule.\n+          \/\/ The \"out\" edge thus points backward, i.e. it is violated.\n+          const VPointer& vp1 = vtn->vpointer(_vloop_analyzer);\n+          const VPointer& vp2 = use->vpointer(_vloop_analyzer);\n+#ifdef ASSERT\n+          if (_trace._speculative_aliasing_analysis || _trace._speculative_runtime_checks) {\n+            tty->print_cr(\"\\nViolated Weak Edge:\");\n+            vtn->print();\n+            vp1.print_on(tty);\n+            use->print();\n+            vp2.print_on(tty);\n+          }\n+#endif\n+\n+          \/\/ We could generate checks for the pair (vp1, vp2) directly. But in\n+          \/\/ some graphs, this generates quadratically many checks. Example:\n+          \/\/\n+          \/\/   set1: a[i+0] a[i+1] a[i+2] a[i+3]\n+          \/\/   set2: b[i+0] b[i+1] b[i+2] b[i+3]\n+          \/\/\n+          \/\/ We may have a weak memory edge between every memory access from\n+          \/\/ set1 to every memory access from set2. In this example, this would\n+          \/\/ be 4 * 4 = 16 checks. But instead, we can create a union VPointer\n+          \/\/ for set1 and set2 each, and only create a single check.\n+          \/\/\n+          \/\/   set1: a[i+0, size = 4]\n+          \/\/   set1: b[i+0, size = 4]\n+          \/\/\n+          \/\/ For this, we add all pairs to an array, and process it below.\n+          weak_aliasing_pairs.push(VPointerWeakAliasingPair::make(vp1, vp2));\n+        }\n+      }\n+      visited.set(vtn->_idx);\n+    }\n+\n+    \/\/ Sort so that all pairs with the same summands (summands1, summands2)\n+    \/\/ are consecutive, i.e. in the same group. This allows us to do a linear\n+    \/\/ walk over all pairs of a group and create the union VPointers.\n+    weak_aliasing_pairs.sort(VPointerWeakAliasingPair::cmp_for_sort);\n+\n+    int group_start = 0;\n+    while (group_start < weak_aliasing_pairs.length()) {\n+      \/\/ New group: pick the first pair as the reference.\n+      const VPointer* vp1 = &weak_aliasing_pairs.at(group_start).vp1();\n+      const VPointer* vp2 = &weak_aliasing_pairs.at(group_start).vp2();\n+      jint size1 = vp1->size();\n+      jint size2 = vp2->size();\n+      int group_end = group_start + 1;\n+      while (group_end < weak_aliasing_pairs.length()) {\n+        const VPointer* vp1_next = &weak_aliasing_pairs.at(group_end).vp1();\n+        const VPointer* vp2_next = &weak_aliasing_pairs.at(group_end).vp2();\n+        jint size1_next = vp1_next->size();\n+        jint size2_next = vp2_next->size();\n+\n+        \/\/ Different summands -> different group.\n+        if (VPointer::cmp_summands(*vp1, *vp1_next) != 0) { break; }\n+        if (VPointer::cmp_summands(*vp2, *vp2_next) != 0) { break; }\n+\n+        \/\/ Pick the one with the lower con as the reference.\n+        if (vp1->con() > vp1_next->con()) {\n+          swap(vp1, vp1_next);\n+          swap(size1, size1_next);\n+        }\n+        if (vp2->con() > vp2_next->con()) {\n+          swap(vp2, vp2_next);\n+          swap(size2, size2_next);\n+        }\n+\n+        \/\/ Compute the distance from vp1 to vp1_next + size, to get a size that would include vp1_next.\n+        NoOverflowInt new_size1 = NoOverflowInt(vp1_next->con()) + NoOverflowInt(size1_next) - NoOverflowInt(vp1->con());\n+        NoOverflowInt new_size2 = NoOverflowInt(vp2_next->con()) + NoOverflowInt(size2_next) - NoOverflowInt(vp2->con());\n+        if (new_size1.is_NaN() || new_size2.is_NaN()) { break; \/* overflow -> new group *\/ }\n+\n+        \/\/ The \"next\" VPointer indeed belong to the group.\n+        \/\/\n+        \/\/ vp1:       |-------------->\n+        \/\/ vp1_next:            |---------------->\n+        \/\/ result:    |-------------------------->\n+        \/\/\n+        \/\/ vp1:       |-------------------------->\n+        \/\/ vp1_next:            |------->\n+        \/\/ result:    |-------------------------->\n+        \/\/\n+        size1 = MAX2(size1, new_size1.value());\n+        size2 = MAX2(size2, new_size2.value());\n+        group_end++;\n+      }\n+      \/\/ Create \"union\" VPointer that cover all VPointer from the group.\n+      const VPointer vp1_union = vp1->make_with_size(size1);\n+      const VPointer vp2_union = vp2->make_with_size(size2);\n+\n+#ifdef ASSERT\n+      if (_trace._speculative_aliasing_analysis || _trace._speculative_runtime_checks) {\n+        tty->print_cr(\"\\nUnion of %d weak aliasing edges:\", group_end - group_start);\n+        vp1_union.print_on(tty);\n+        vp2_union.print_on(tty);\n+      }\n+\n+      \/\/ Verification - union must contain all VPointer of the group.\n+      for (int i = group_start; i < group_end; i++) {\n+        const VPointer& vp1_i = weak_aliasing_pairs.at(i).vp1();\n+        const VPointer& vp2_i = weak_aliasing_pairs.at(i).vp2();\n+        assert(vp1_union.con() <= vp1_i.con(), \"must start before\");\n+        assert(vp2_union.con() <= vp2_i.con(), \"must start before\");\n+        assert(vp1_union.size() >= vp1_i.size(), \"must end after\");\n+        assert(vp2_union.size() >= vp2_i.size(), \"must end after\");\n+      }\n+#endif\n+\n+      BoolNode* bol = vp1_union.make_speculative_aliasing_check_with(vp2_union);\n+      add_speculative_check(bol);\n+\n+      group_start = group_end;\n+    }\n+  }\n+}\n+\n@@ -497,1 +678,1 @@\n-  Node* n = vnode_idx_to_transformed_node.at(in(i)->_idx);\n+  Node* n = vnode_idx_to_transformed_node.at(in_req(i)->_idx);\n@@ -619,1 +800,1 @@\n-  VTransformElementWiseVectorNode* vtn_cmp = in(1)->isa_ElementWiseVector();\n+  VTransformElementWiseVectorNode* vtn_cmp = in_req(1)->isa_ElementWiseVector();\n@@ -753,2 +934,8 @@\n-    tty->print(\" |\");\n-    for (int i = _req; i < _in.length(); i++) {\n+    tty->print(\" | strong:\");\n+    for (uint i = _req; i < _in_end_strong_memory_edges; i++) {\n+      print_node_idx(_in.at(i));\n+    }\n+  }\n+  if ((uint)_in.length() > _in_end_strong_memory_edges) {\n+    tty->print(\" | weak:\");\n+    for (uint i = _in_end_strong_memory_edges; i < (uint)_in.length(); i++) {\n@@ -759,1 +946,1 @@\n-  for (int i = 0; i < _out.length(); i++) {\n+  for (uint i = 0; i < _out_end_strong_edges; i++) {\n@@ -762,0 +949,6 @@\n+  if ((uint)_out.length() > _out_end_strong_edges) {\n+    tty->print(\" | weak:\");\n+    for (uint i = _out_end_strong_edges; i < (uint)_out.length(); i++) {\n+      print_node_idx(_out.at(i));\n+    }\n+  }\n","filename":"src\/hotspot\/share\/opto\/vtransform.cpp","additions":206,"deletions":13,"binary":false,"changes":219,"status":"modified"},{"patch":"@@ -112,0 +112,1 @@\n+  const bool _speculative_aliasing_analysis;\n@@ -118,0 +119,1 @@\n+                  const bool is_trace_speculative_aliasing_analysis,\n@@ -121,4 +123,5 @@\n-    _rejections                (_verbose | is_trace_vtransform(vtrace) | is_trace_rejections),\n-    _align_vector              (_verbose | is_trace_vtransform(vtrace) | is_trace_align_vector),\n-    _speculative_runtime_checks(_verbose | is_trace_vtransform(vtrace) | is_trace_speculative_runtime_checks),\n-    _info                      (_verbose | is_trace_vtransform(vtrace) | is_trace_info) {}\n+    _rejections                    (_verbose | is_trace_vtransform(vtrace) | is_trace_rejections),\n+    _align_vector                  (_verbose | is_trace_vtransform(vtrace) | is_trace_align_vector),\n+    _speculative_aliasing_analysis (_verbose | is_trace_vtransform(vtrace) | is_trace_speculative_aliasing_analysis),\n+    _speculative_runtime_checks    (_verbose | is_trace_vtransform(vtrace) | is_trace_speculative_runtime_checks),\n+    _info                          (_verbose | is_trace_vtransform(vtrace) | is_trace_info) {}\n@@ -164,0 +167,1 @@\n+  const GrowableArray<VTransformNode*>& get_schedule() const { return _schedule; }\n@@ -176,1 +180,1 @@\n-  void collect_nodes_without_req_or_dependency(GrowableArray<VTransformNode*>& stack) const;\n+  void collect_nodes_without_strong_in_edges(GrowableArray<VTransformNode*>& stack) const;\n@@ -251,1 +255,2 @@\n-  void apply_speculative_runtime_checks();\n+  void apply_speculative_alignment_runtime_checks();\n+  void apply_speculative_aliasing_runtime_checks();\n@@ -262,0 +267,20 @@\n+\/\/\n+\/\/ There are 3 tyes of edges:\n+\/\/ - data edges (req):           corresponding to C2 IR Node data edges, except control\n+\/\/                               and memory.\n+\/\/ - strong memory edges:        memory edges that must be respected when scheduling.\n+\/\/ - weak memory edges:          memory edges that can be violated, but if violated then\n+\/\/                               corresponding aliasing analysis runtime checks must be\n+\/\/                               inserted.\n+\/\/\n+\/\/ Strong edges: union of data edges and strong memory edges.\n+\/\/               These must be respected by scheduling in all cases.\n+\/\/\n+\/\/ The C2 IR Node memory edges essentially define a linear order of all memory operations\n+\/\/ (only Loads with the same memory input can be executed in an arbitrary order). This is\n+\/\/ efficient, because it means every Load and Store has exactly one input memory edge,\n+\/\/ which keeps the memory edge count linear. This is approach is too restrictive for\n+\/\/ vectorization, for example, we could never vectorize stores, since they are all in a\n+\/\/ dependency chain. Instead, we model the memory edges between all memory nodes, which\n+\/\/ could be quadratic in the worst case. For vectorization, we must essentially reorder the\n+\/\/ instructions in the graph. For this we must model all memory dependencies.\n@@ -267,2 +292,4 @@\n-  \/\/ _in is split into required inputs (_req, i.e. all data dependencies),\n-  \/\/ and memory dependencies.\n+  \/\/ We split _in into 3 sections:\n+  \/\/ - data edges (req):     _in[0                           .. _req-1]\n+  \/\/ - strong memory edges:  _in[_req                        .. _in_end_strong_memory_edges-1]\n+  \/\/ - weak memory edges:    _in[_in_end_strong_memory_edges .. ]\n@@ -270,0 +297,1 @@\n+  uint _in_end_strong_memory_edges;\n@@ -271,0 +299,5 @@\n+\n+  \/\/ We split _out into 2 sections:\n+  \/\/ - strong edges:         _out[0                     .. _out_end_strong_edges-1]\n+  \/\/ - weak memory edges:    _out[_out_end_strong_edges .. _len-1]\n+  uint _out_end_strong_edges;\n@@ -277,0 +310,1 @@\n+    _in_end_strong_memory_edges(req),\n@@ -278,0 +312,1 @@\n+    _out_end_strong_edges(0),\n@@ -287,1 +322,1 @@\n-    n->add_out(this);\n+    n->add_out_strong_edge(this);\n@@ -298,1 +333,16 @@\n-  void add_memory_dependency(VTransformNode* n) {\n+  void add_strong_memory_edge(VTransformNode* n) {\n+    assert(n != nullptr, \"no need to add nullptr\");\n+    if (_in_end_strong_memory_edges < (uint)_in.length()) {\n+      \/\/ Put n in place of first weak memory edge, and move\n+      \/\/ the weak memory edge to the end.\n+      VTransformNode* first_weak = _in.at(_in_end_strong_memory_edges);\n+      _in.at_put(_in_end_strong_memory_edges, n);\n+      _in.push(first_weak);\n+    } else {\n+      _in.push(n);\n+    }\n+    _in_end_strong_memory_edges++;\n+    n->add_out_strong_edge(this);\n+  }\n+\n+  void add_weak_memory_edge(VTransformNode* n) {\n@@ -301,1 +351,15 @@\n-    n->add_out(this);\n+    n->add_out_weak_memory_edge(this);\n+  }\n+\n+private:\n+  void add_out_strong_edge(VTransformNode* n) {\n+    if (_out_end_strong_edges < (uint)_out.length()) {\n+      \/\/ Put n in place of first weak memory edge, and move\n+      \/\/ the weak memory edge to the end.\n+      VTransformNode* first_weak = _out.at(_out_end_strong_edges);\n+      _out.at_put(_out_end_strong_edges, n);\n+      _out.push(first_weak);\n+    } else {\n+      _out.push(n);\n+    }\n+    _out_end_strong_edges++;\n@@ -304,1 +368,1 @@\n-  void add_out(VTransformNode* n) {\n+  void add_out_weak_memory_edge(VTransformNode* n) {\n@@ -308,0 +372,1 @@\n+public:\n@@ -309,3 +374,17 @@\n-  VTransformNode* in(int i) const { return _in.at(i); }\n-  int outs() const { return _out.length(); }\n-  VTransformNode* out(int i) const { return _out.at(i); }\n+  uint out_strong_edges() const { return _out_end_strong_edges; }\n+  uint out_weak_edges() const { return _out.length() - _out_end_strong_edges; }\n+\n+  VTransformNode* in_req(uint i) const {\n+    assert(i < _req, \"must be a req\");\n+    return _in.at(i);\n+  }\n+\n+  VTransformNode* out_strong_edge(uint i) const {\n+    assert(i < out_strong_edges(), \"must be a strong memory edge or data edge\");\n+    return _out.at(i);\n+  }\n+\n+  VTransformNode* out_weak_edge(uint i) const {\n+    assert(i < out_weak_edges(), \"must be a strong memory edge\");\n+    return _out.at(_out_end_strong_edges + i);\n+  }\n@@ -313,2 +392,2 @@\n-  bool has_req_or_dependency() const {\n-    for (int i = 0; i < _in.length(); i++) {\n+  bool has_strong_in_edge() const {\n+    for (uint i = 0; i < _in_end_strong_memory_edges; i++) {\n","filename":"src\/hotspot\/share\/opto\/vtransform.hpp","additions":96,"deletions":17,"binary":false,"changes":113,"status":"modified"},{"patch":"@@ -34,0 +34,1 @@\n+import java.util.List;\n@@ -53,13 +54,18 @@\n-        \/\/ Cross-product: +-AlignVector and +-UseCompactObjectHeaders\n-        TestFramework.runWithFlags(\"--add-modules\", \"java.base\", \"--add-exports\", \"java.base\/jdk.internal.misc=ALL-UNNAMED\",\n-                                   \"-XX:-UseCompactObjectHeaders\",\n-                                   \"-XX:-AlignVector\");\n-        TestFramework.runWithFlags(\"--add-modules\", \"java.base\", \"--add-exports\", \"java.base\/jdk.internal.misc=ALL-UNNAMED\",\n-                                   \"-XX:-UseCompactObjectHeaders\",\n-                                   \"-XX:+AlignVector\");\n-        TestFramework.runWithFlags(\"--add-modules\", \"java.base\", \"--add-exports\", \"java.base\/jdk.internal.misc=ALL-UNNAMED\",\n-                                   \"-XX:+UseCompactObjectHeaders\",\n-                                   \"-XX:-AlignVector\");\n-        TestFramework.runWithFlags(\"--add-modules\", \"java.base\", \"--add-exports\", \"java.base\/jdk.internal.misc=ALL-UNNAMED\",\n-                                   \"-XX:+UseCompactObjectHeaders\",\n-                                   \"-XX:+AlignVector\");\n+        TestFramework framework = new TestFramework();\n+        framework.addFlags(\"--add-modules\", \"java.base\", \"--add-exports\", \"java.base\/jdk.internal.misc=ALL-UNNAMED\",\n+                           \"-XX:+UnlockExperimentalVMOptions\");\n+\n+        \/\/ Cross-product:\n+        \/\/   +-AlignVector\n+        \/\/   +-UseCompactObjectHeaders\n+        \/\/   +-UseAutoVectorizationSpeculativeAliasingChecks\n+        int idx = 0;\n+        for (String av : List.of(\"-XX:-AlignVector\", \"-XX:+AlignVector\")) {\n+            for (String coh : List.of(\"-XX:-UseCompactObjectHeaders\", \"-XX:+UseCompactObjectHeaders\")) {\n+                for (String sac : List.of(\"-XX:-UseAutoVectorizationSpeculativeAliasingChecks\", \"-XX:+UseAutoVectorizationSpeculativeAliasingChecks\")) {\n+                    framework.addScenarios(new Scenario(idx++, av, coh, sac));\n+                }\n+            }\n+        }\n+\n+        framework.start();\n@@ -129,1 +135,1 @@\n-            int val = offset > 0 ? verifyByteArray[(i-offset) % 8] : verifyByteArray[i-offset];\n+            int val = offset >=1 ? verifyByteArray[(i-offset) % 8] : verifyByteArray[i-offset];\n@@ -482,1 +488,8 @@\n-    @IR(failOn = { IRNode.LOAD_VECTOR_L, IRNode.STORE_VECTOR })\n+    @IR(failOn = { IRNode.LOAD_VECTOR_L, IRNode.STORE_VECTOR },\n+        applyIf = {\"UseAutoVectorizationSpeculativeAliasingChecks\", \"false\"})\n+    @IR(counts = { IRNode.LOAD_VECTOR_L, \">=1\", IRNode.STORE_VECTOR, \">=1\", \".*multiversion.*\", \">=1\"},\n+        phase = CompilePhase.PRINT_IDEAL,\n+        applyIfCPUFeatureOr = {\"sse2\", \"true\", \"asimd\", \"true\", \"rvv\", \"true\"},\n+        applyIfAnd = {\"UseAutoVectorizationSpeculativeAliasingChecks\", \"true\", \"AlignVector\", \"false\"},\n+        applyIfPlatform = {\"64-bit\", \"true\"})\n+    \/\/ We have unknown aliasing. At runtime \"dest == src\", so the AutoVectorization Predicate fails, and recompiles with Multiversioning.\n@@ -490,1 +503,8 @@\n-    @IR(failOn = { IRNode.LOAD_VECTOR_L, IRNode.STORE_VECTOR })\n+    @IR(failOn = { IRNode.LOAD_VECTOR_L, IRNode.STORE_VECTOR },\n+        applyIf = {\"UseAutoVectorizationSpeculativeAliasingChecks\", \"false\"})\n+    @IR(counts = { IRNode.LOAD_VECTOR_L, \">=1\", IRNode.STORE_VECTOR, \">=1\", \".*multiversion.*\", \">=1\"},\n+        phase = CompilePhase.PRINT_IDEAL,\n+        applyIfCPUFeatureOr = {\"sse2\", \"true\", \"asimd\", \"true\", \"rvv\", \"true\"},\n+        applyIfAnd = {\"UseAutoVectorizationSpeculativeAliasingChecks\", \"true\", \"AlignVector\", \"false\"},\n+        applyIfPlatform = {\"64-bit\", \"true\"})\n+    \/\/ We have unknown aliasing. At runtime \"dest == src\", so the AutoVectorization Predicate fails, and recompiles with Multiversioning.\n@@ -504,1 +524,8 @@\n-    @IR(failOn = { IRNode.LOAD_VECTOR_L, IRNode.STORE_VECTOR })\n+    @IR(failOn = { IRNode.LOAD_VECTOR_L, IRNode.STORE_VECTOR },\n+        applyIf = {\"UseAutoVectorizationSpeculativeAliasingChecks\", \"false\"})\n+    @IR(counts = { IRNode.LOAD_VECTOR_L, \">=1\", IRNode.STORE_VECTOR, \">=1\", \".*multiversion.*\", \">=1\"},\n+        phase = CompilePhase.PRINT_IDEAL,\n+        applyIfCPUFeatureOr = {\"sse2\", \"true\", \"asimd\", \"true\", \"rvv\", \"true\"},\n+        applyIfAnd = {\"UseAutoVectorizationSpeculativeAliasingChecks\", \"true\", \"AlignVector\", \"false\"},\n+        applyIfPlatform = {\"64-bit\", \"true\"})\n+    \/\/ We have unknown aliasing. At runtime \"dest == src\", so the AutoVectorization Predicate fails, and recompiles with Multiversioning.\n@@ -512,1 +539,8 @@\n-    @IR(failOn = { IRNode.LOAD_VECTOR_L, IRNode.STORE_VECTOR })\n+    @IR(failOn = { IRNode.LOAD_VECTOR_L, IRNode.STORE_VECTOR },\n+        applyIf = {\"UseAutoVectorizationSpeculativeAliasingChecks\", \"false\"})\n+    @IR(counts = { IRNode.LOAD_VECTOR_L, \">=1\", IRNode.STORE_VECTOR, \">=1\", \".*multiversion.*\", \">=1\"},\n+        phase = CompilePhase.PRINT_IDEAL,\n+        applyIfCPUFeatureOr = {\"sse2\", \"true\", \"asimd\", \"true\", \"rvv\", \"true\"},\n+        applyIfAnd = {\"UseAutoVectorizationSpeculativeAliasingChecks\", \"true\", \"AlignVector\", \"false\"},\n+        applyIfPlatform = {\"64-bit\", \"true\"})\n+    \/\/ We have unknown aliasing. At runtime \"dest == src\", so the AutoVectorization Predicate fails, and recompiles with Multiversioning.\n@@ -527,1 +561,8 @@\n-    @IR(failOn = { IRNode.LOAD_VECTOR_L, IRNode.STORE_VECTOR })\n+    @IR(failOn = { IRNode.LOAD_VECTOR_L, IRNode.STORE_VECTOR },\n+        applyIf = {\"UseAutoVectorizationSpeculativeAliasingChecks\", \"false\"})\n+    @IR(counts = { IRNode.LOAD_VECTOR_L, \">=1\", IRNode.STORE_VECTOR, \">=1\", \".*multiversion.*\", \">=1\"},\n+        phase = CompilePhase.PRINT_IDEAL,\n+        applyIfCPUFeatureOr = {\"sse2\", \"true\", \"asimd\", \"true\", \"rvv\", \"true\"},\n+        applyIfAnd = {\"UseAutoVectorizationSpeculativeAliasingChecks\", \"true\", \"AlignVector\", \"false\"},\n+        applyIfPlatform = {\"64-bit\", \"true\"})\n+    \/\/ We have unknown aliasing. At runtime \"dest == src\", so the AutoVectorization Predicate fails, and recompiles with Multiversioning.\n@@ -535,1 +576,8 @@\n-    @IR(failOn = { IRNode.LOAD_VECTOR_L, IRNode.STORE_VECTOR })\n+    @IR(failOn = { IRNode.LOAD_VECTOR_L, IRNode.STORE_VECTOR },\n+        applyIf = {\"UseAutoVectorizationSpeculativeAliasingChecks\", \"false\"})\n+    @IR(counts = { IRNode.LOAD_VECTOR_L, \">=1\", IRNode.STORE_VECTOR, \">=1\", \".*multiversion.*\", \">=1\"},\n+        phase = CompilePhase.PRINT_IDEAL,\n+        applyIfCPUFeatureOr = {\"sse2\", \"true\", \"asimd\", \"true\", \"rvv\", \"true\"},\n+        applyIfAnd = {\"UseAutoVectorizationSpeculativeAliasingChecks\", \"true\", \"AlignVector\", \"false\"},\n+        applyIfPlatform = {\"64-bit\", \"true\"})\n+    \/\/ We have unknown aliasing. At runtime \"dest == src\", so the AutoVectorization Predicate fails, and recompiles with Multiversioning.\n","filename":"test\/hotspot\/jtreg\/compiler\/c2\/irTests\/TestVectorizationMismatchedAccess.java","additions":68,"deletions":20,"binary":false,"changes":88,"status":"modified"},{"patch":"@@ -0,0 +1,547 @@\n+\/*\n+ * Copyright (c) 2024, 2025, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+\/*\n+ * @test\n+ * @bug 8324751\n+ * @summary Test Speculative Aliasing checks in SuperWord\n+ * @library \/test\/lib \/\n+ * @run driver compiler.loopopts.superword.TestAliasing nCOH_nAV_ySAC\n+ * @run driver compiler.loopopts.superword.TestAliasing nCOH_yAV_ySAC\n+ * @run driver compiler.loopopts.superword.TestAliasing yCOH_nAV_ySAC\n+ * @run driver compiler.loopopts.superword.TestAliasing yCOH_yAV_ySAC\n+ * @run driver compiler.loopopts.superword.TestAliasing nCOH_nAV_nSAC\n+ * @run driver compiler.loopopts.superword.TestAliasing nCOH_yAV_nSAC\n+ * @run driver compiler.loopopts.superword.TestAliasing yCOH_nAV_nSAC\n+ * @run driver compiler.loopopts.superword.TestAliasing yCOH_yAV_nSAC\n+ * @run driver compiler.loopopts.superword.TestAliasing noSlowLoopOptimizations\n+ *\/\n+\n+package compiler.loopopts.superword;\n+\n+import jdk.test.lib.Utils;\n+import java.util.Map;\n+import java.util.HashMap;\n+import java.util.Random;\n+\n+import compiler.lib.ir_framework.*;\n+import compiler.lib.verify.*;\n+import static compiler.lib.generators.Generators.G;\n+import compiler.lib.generators.Generator;\n+\n+\/**\n+ * More complicated test cases can be found in {@link TestAliasingFuzzing}.\n+ *\/\n+public class TestAliasing {\n+    static int SIZE = 1024*8;\n+    private static final Random RANDOM = Utils.getRandomInstance();\n+    private static final Generator INT_GEN = G.ints();\n+\n+    \/\/ Invariants used in tests.\n+    public static int INVAR_ZERO = 0;\n+\n+    \/\/ Original data.\n+    public static byte[] ORIG_AB = fillRandom(new byte[SIZE]);\n+    public static byte[] ORIG_BB = fillRandom(new byte[SIZE]);\n+    public static int[]  ORIG_AI = fillRandom(new int[SIZE]);\n+    public static int[]  ORIG_BI = fillRandom(new int[SIZE]);\n+\n+    \/\/ The data we use in the tests. It is initialized from ORIG_* every time.\n+    public static byte[] AB = new byte[SIZE];\n+    public static byte[] BB = new byte[SIZE];\n+    public static int[]  AI = new int[SIZE];\n+    public static int[]  BI = new int[SIZE];\n+\n+    \/\/ Parallel to data above, but for use in reference methods.\n+    public static byte[] AB_REFERENCE = new byte[SIZE];\n+    public static byte[] BB_REFERENCE = new byte[SIZE];\n+    public static int[]  AI_REFERENCE = new int[SIZE];\n+    public static int[]  BI_REFERENCE = new int[SIZE];\n+\n+    interface TestFunction {\n+        void run();\n+    }\n+\n+    \/\/ Map of goldTests, i.e. tests that work with a golds value generated from the same test method,\n+    \/\/ at the beginning when we are still executing in the interpreter.\n+    Map<String,TestFunction> goldTests = new HashMap<String,TestFunction>();\n+\n+    \/\/ Map of gold, the results from the first run before compilation, one per goldTests entry.\n+    Map<String,Object> golds = new HashMap<String,Object>();\n+\n+    \/\/ Map of referenceTests, i.e. tests that have a reference implementation that is run with the interpreter.\n+    \/\/ The TestFunction must run both the test and reference methods.\n+    Map<String,TestFunction> referenceTests = new HashMap<String,TestFunction>();\n+\n+    public static void main(String[] args) {\n+        TestFramework framework = new TestFramework(TestAliasing.class);\n+        switch (args[0]) {\n+            case \"nCOH_nAV_ySAC\" -> { framework.addFlags(\"-XX:+UnlockExperimentalVMOptions\", \"-XX:-UseCompactObjectHeaders\", \"-XX:-AlignVector\", \"-XX:+UseAutoVectorizationSpeculativeAliasingChecks\"); }\n+            case \"nCOH_yAV_ySAC\" -> { framework.addFlags(\"-XX:+UnlockExperimentalVMOptions\", \"-XX:-UseCompactObjectHeaders\", \"-XX:+AlignVector\", \"-XX:+UseAutoVectorizationSpeculativeAliasingChecks\"); }\n+            case \"yCOH_nAV_ySAC\" -> { framework.addFlags(\"-XX:+UnlockExperimentalVMOptions\", \"-XX:+UseCompactObjectHeaders\", \"-XX:-AlignVector\", \"-XX:+UseAutoVectorizationSpeculativeAliasingChecks\"); }\n+            case \"yCOH_yAV_ySAC\" -> { framework.addFlags(\"-XX:+UnlockExperimentalVMOptions\", \"-XX:+UseCompactObjectHeaders\", \"-XX:+AlignVector\", \"-XX:+UseAutoVectorizationSpeculativeAliasingChecks\"); }\n+            case \"nCOH_nAV_nSAC\" -> { framework.addFlags(\"-XX:+UnlockExperimentalVMOptions\", \"-XX:-UseCompactObjectHeaders\", \"-XX:-AlignVector\", \"-XX:-UseAutoVectorizationSpeculativeAliasingChecks\"); }\n+            case \"nCOH_yAV_nSAC\" -> { framework.addFlags(\"-XX:+UnlockExperimentalVMOptions\", \"-XX:-UseCompactObjectHeaders\", \"-XX:+AlignVector\", \"-XX:-UseAutoVectorizationSpeculativeAliasingChecks\"); }\n+            case \"yCOH_nAV_nSAC\" -> { framework.addFlags(\"-XX:+UnlockExperimentalVMOptions\", \"-XX:+UseCompactObjectHeaders\", \"-XX:-AlignVector\", \"-XX:-UseAutoVectorizationSpeculativeAliasingChecks\"); }\n+            case \"yCOH_yAV_nSAC\" -> { framework.addFlags(\"-XX:+UnlockExperimentalVMOptions\", \"-XX:+UseCompactObjectHeaders\", \"-XX:+AlignVector\", \"-XX:-UseAutoVectorizationSpeculativeAliasingChecks\"); }\n+            case \"noSlowLoopOptimizations\" -> { framework.addFlags(\"-XX:+UnlockExperimentalVMOptions\", \"-XX:-LoopMultiversioningOptimizeSlowLoop\"); }\n+            default -> { throw new RuntimeException(\"Test argument not recognized: \" + args[0]); }\n+        };\n+        framework.start();\n+    }\n+\n+    public TestAliasing() {\n+        \/\/ Add all goldTests to list\n+        goldTests.put(\"copy_B_sameIndex_noalias\",         () -> { copy_B_sameIndex_noalias(AB, BB); });\n+        goldTests.put(\"copy_B_sameIndex_alias\",           () -> { copy_B_sameIndex_alias(AB, AB); });\n+        goldTests.put(\"copy_B_differentIndex_noalias\",    () -> { copy_B_differentIndex_noalias(AB, BB); });\n+        goldTests.put(\"copy_B_differentIndex_noalias_v2\", () -> { copy_B_differentIndex_noalias_v2(); });\n+        goldTests.put(\"copy_B_differentIndex_alias\",      () -> { copy_B_differentIndex_alias(AB, AB); });\n+\n+        goldTests.put(\"copy_I_sameIndex_noalias\",         () -> { copy_I_sameIndex_noalias(AI, BI); });\n+        goldTests.put(\"copy_I_sameIndex_alias\",           () -> { copy_I_sameIndex_alias(AI, AI); });\n+        goldTests.put(\"copy_I_differentIndex_noalias\",    () -> { copy_I_differentIndex_noalias(AI, BI); });\n+        goldTests.put(\"copy_I_differentIndex_alias\",      () -> { copy_I_differentIndex_alias(AI, AI); });\n+\n+        \/\/ Compute gold value for all test methods before compilation\n+        for (Map.Entry<String,TestFunction> entry : goldTests.entrySet()) {\n+            String name = entry.getKey();\n+            TestFunction test = entry.getValue();\n+            init();\n+            test.run();\n+            Object gold = snapshotCopy();\n+            golds.put(name, gold);\n+        }\n+\n+        referenceTests.put(\"fill_B_sameArray_alias\", () -> {\n+            int invar1 = RANDOM.nextInt(64);\n+            int invar2 = RANDOM.nextInt(64);\n+            test_fill_B_sameArray_alias(AB, AB, invar1, invar2);\n+            reference_fill_B_sameArray_alias(AB_REFERENCE, AB_REFERENCE, invar1, invar2);\n+        });\n+        referenceTests.put(\"fill_B_sameArray_noalias\", () -> {\n+            \/\/ The accesses either start at the middle and go out,\n+            \/\/ or start from opposite sides and meet in the middle.\n+            \/\/ But they never overlap.\n+            \/\/      <------|------>\n+            \/\/      ------>|<------\n+            \/\/\n+            \/\/ This tests that the checks we emit are not too relaxed.\n+            int middle = SIZE \/ 2 + RANDOM.nextInt(-256, 256);\n+            int limit = SIZE \/ 3 + RANDOM.nextInt(256);\n+            int invar1 = middle;\n+            int invar2 = middle;\n+            if (RANDOM.nextBoolean()) {\n+                invar1 -= limit;\n+                invar2 += limit;\n+            }\n+            test_fill_B_sameArray_noalias(AB, AB, invar1, invar2, limit);\n+            reference_fill_B_sameArray_noalias(AB_REFERENCE, AB_REFERENCE, invar1, invar2, limit);\n+        });\n+    }\n+\n+    public static void init() {\n+        System.arraycopy(ORIG_AB, 0, AB, 0, SIZE);\n+        System.arraycopy(ORIG_BB, 0, BB, 0, SIZE);\n+        System.arraycopy(ORIG_AI, 0, AI, 0, SIZE);\n+        System.arraycopy(ORIG_BI, 0, BI, 0, SIZE);\n+    }\n+\n+    public static void initReference() {\n+        System.arraycopy(ORIG_AB, 0, AB_REFERENCE, 0, SIZE);\n+        System.arraycopy(ORIG_BB, 0, BB_REFERENCE, 0, SIZE);\n+        System.arraycopy(ORIG_AI, 0, AI_REFERENCE, 0, SIZE);\n+        System.arraycopy(ORIG_BI, 0, BI_REFERENCE, 0, SIZE);\n+    }\n+\n+    public static Object snapshotCopy() {\n+        return new Object[] {\n+            AB.clone(), BB.clone(),\n+            AI.clone(), BI.clone()\n+        };\n+    }\n+\n+    public static Object snapshot() {\n+        return new Object[] {\n+            AB, BB,\n+            AI, BI\n+        };\n+    }\n+\n+    public static Object snapshotReference() {\n+        return new Object[] {\n+            AB_REFERENCE, BB_REFERENCE,\n+            AI_REFERENCE, BI_REFERENCE\n+        };\n+    }\n+\n+    @Warmup(100)\n+    @Run(test = {\"copy_B_sameIndex_noalias\",\n+                 \"copy_B_sameIndex_alias\",\n+                 \"copy_B_differentIndex_noalias\",\n+                 \"copy_B_differentIndex_noalias_v2\",\n+                 \"copy_B_differentIndex_alias\",\n+                 \"copy_I_sameIndex_noalias\",\n+                 \"copy_I_sameIndex_alias\",\n+                 \"copy_I_differentIndex_noalias\",\n+                 \"copy_I_differentIndex_alias\",\n+                 \"test_fill_B_sameArray_alias\",\n+                 \"test_fill_B_sameArray_noalias\"})\n+    public void runTests() {\n+        for (Map.Entry<String,TestFunction> entry : goldTests.entrySet()) {\n+            String name = entry.getKey();\n+            TestFunction test = entry.getValue();\n+            \/\/ Recall gold value from before compilation\n+            Object gold = golds.get(name);\n+            \/\/ Compute new result\n+            init();\n+            test.run();\n+            Object result = snapshot();\n+            \/\/ Compare gold and new result\n+            try {\n+                Verify.checkEQ(gold, result);\n+            } catch (VerifyException e) {\n+                throw new RuntimeException(\"Verify failed for \" + name, e);\n+            }\n+        }\n+\n+        for (Map.Entry<String,TestFunction> entry : referenceTests.entrySet()) {\n+            String name = entry.getKey();\n+            TestFunction test = entry.getValue();\n+            \/\/ Init data for test and reference\n+            init();\n+            initReference();\n+            \/\/ Run test and reference\n+            test.run();\n+            \/\/ Capture results from test and reference\n+            Object result = snapshot();\n+            Object expected = snapshotReference();\n+            \/\/ Compare expected and new result\n+            try {\n+                Verify.checkEQ(expected, result);\n+            } catch (VerifyException e) {\n+                throw new RuntimeException(\"Verify failed for \" + name, e);\n+            }\n+        }\n+    }\n+\n+    static byte[] fillRandom(byte[] a) {\n+        for (int i = 0; i < a.length; i++) {\n+            a[i] = (byte)(int)INT_GEN.next();\n+        }\n+        return a;\n+    }\n+\n+    static int[] fillRandom(int[] a) {\n+        G.fill(INT_GEN, a);\n+        return a;\n+    }\n+\n+    @Test\n+    @IR(counts = {IRNode.LOAD_VECTOR_B, \"> 0\",\n+                  IRNode.STORE_VECTOR, \"> 0\",\n+                  \".*multiversion.*\", \"= 0\"},\n+        phase = CompilePhase.PRINT_IDEAL,\n+        applyIfPlatform = {\"64-bit\", \"true\"},\n+        applyIfCPUFeatureOr = {\"sse4.1\", \"true\", \"asimd\", \"true\"})\n+    \/\/ Should always vectorize, no speculative runtime check required.\n+    static void copy_B_sameIndex_noalias(byte[] a, byte[] b) {\n+        for (int i = 0; i < a.length; i++) {\n+          b[i] = a[i];\n+        }\n+    }\n+\n+    @Test\n+    @IR(counts = {IRNode.LOAD_VECTOR_B, \"> 0\",\n+                  IRNode.STORE_VECTOR, \"> 0\",\n+                  \".*multiversion.*\", \"= 0\"},\n+        phase = CompilePhase.PRINT_IDEAL,\n+        applyIfPlatform = {\"64-bit\", \"true\"},\n+        applyIfCPUFeatureOr = {\"sse4.1\", \"true\", \"asimd\", \"true\"})\n+    \/\/ Should always vectorize, no speculative runtime check required.\n+    static void copy_B_sameIndex_alias(byte[] a, byte[] b) {\n+        for (int i = 0; i < a.length; i++) {\n+          b[i] = a[i];\n+        }\n+    }\n+\n+    @Test\n+    @IR(counts = {IRNode.LOAD_VECTOR_B, \"= 0\",\n+                  IRNode.STORE_VECTOR, \"= 0\",\n+                  \".*multiversion.*\", \"= 0\"},\n+        phase = CompilePhase.PRINT_IDEAL,\n+        applyIf = {\"UseAutoVectorizationSpeculativeAliasingChecks\", \"false\"},\n+        applyIfPlatform = {\"64-bit\", \"true\"},\n+        applyIfCPUFeatureOr = {\"sse4.1\", \"true\", \"asimd\", \"true\"})\n+    \/\/ Without speculative runtime check we cannot know that there is no aliasing.\n+    @IR(counts = {IRNode.LOAD_VECTOR_B, \"> 0\",\n+                  IRNode.STORE_VECTOR, \"> 0\",\n+                  \".*multiversion.*\", \"= 0\"},\n+        phase = CompilePhase.PRINT_IDEAL,\n+        applyIfAnd = {\"UseAutoVectorizationSpeculativeAliasingChecks\", \"true\", \"AlignVector\", \"false\"},\n+        applyIfPlatform = {\"64-bit\", \"true\"},\n+        applyIfCPUFeatureOr = {\"sse4.1\", \"true\", \"asimd\", \"true\"})\n+    \/\/ We use speculative runtime checks, they never fail, so no multiversioning required.\n+    \/\/ With AlignVector we cannot prove that both accesses are alignable.\n+    static void copy_B_differentIndex_noalias(byte[] a, byte[] b) {\n+        for (int i = 0; i < a.length; i++) {\n+          b[i] = a[i + INVAR_ZERO];\n+        }\n+    }\n+\n+    @Test\n+    @IR(counts = {IRNode.LOAD_VECTOR_B, \"= 0\",\n+                  IRNode.STORE_VECTOR, \"= 0\",\n+                  \".*multiversion.*\", \"= 0\"},\n+        phase = CompilePhase.PRINT_IDEAL,\n+        applyIfPlatform = {\"64-bit\", \"true\"},\n+        applyIfCPUFeatureOr = {\"sse4.1\", \"true\", \"asimd\", \"true\"})\n+    \/\/ Same as \"copy_B_differentIndex_noalias, but somehow loading from fields rather\n+    \/\/ than arguments does not lead to vectorization.\n+    \/\/ Probably related to JDK-8348096, issue with RangeCheck elimination.\n+    static void copy_B_differentIndex_noalias_v2() {\n+        for (int i = 0; i < AB.length; i++) {\n+            BB[i] = AB[i + INVAR_ZERO];\n+        }\n+    }\n+\n+    @Test\n+    @IR(counts = {IRNode.LOAD_VECTOR_B, \"= 0\",\n+                  IRNode.STORE_VECTOR, \"= 0\",\n+                  \".*multiversion.*\", \"= 0\"},\n+        phase = CompilePhase.PRINT_IDEAL,\n+        applyIf = {\"UseAutoVectorizationSpeculativeAliasingChecks\", \"false\"},\n+        applyIfPlatform = {\"64-bit\", \"true\"},\n+        applyIfCPUFeatureOr = {\"sse4.1\", \"true\", \"asimd\", \"true\"})\n+    \/\/ Without speculative runtime check we cannot know that there is no aliasing.\n+    @IR(counts = {IRNode.LOAD_VECTOR_B,            \"> 0\",\n+                  IRNode.STORE_VECTOR,             \"> 0\",\n+                  \".*pre .* multiversion_fast.*\",  \"= 1\",\n+                  \".*main .* multiversion_fast.*\", \"= 1\",\n+                  \".*post .* multiversion_fast.*\", \"= 2\", \/\/ vectorized and scalar versions\n+                  \".*multiversion_slow.*\",         \"= 2\", \/\/ main and post (pre-loop only has a single iteration)\n+                  \".*multiversion.*\",              \"= 6\"},\n+        phase = CompilePhase.PRINT_IDEAL,\n+        applyIfAnd = {\"UseAutoVectorizationSpeculativeAliasingChecks\", \"true\",\n+                      \"LoopMultiversioningOptimizeSlowLoop\", \"true\",\n+                      \"AlignVector\", \"false\"},\n+        applyIfPlatform = {\"64-bit\", \"true\"},\n+        applyIfCPUFeatureOr = {\"sse4.1\", \"true\", \"asimd\", \"true\"})\n+    \/\/ We use speculative runtime checks, it fails and so we do need multiversioning.\n+    \/\/ With AlignVector we cannot prove that both accesses are alignable.\n+    @IR(counts = {IRNode.LOAD_VECTOR_B,            \"> 0\",\n+                  IRNode.STORE_VECTOR,             \"> 0\",\n+                  \".*pre .* multiversion_fast.*\",  \"= 1\",\n+                  \".*main .* multiversion_fast.*\", \"= 1\",\n+                  \".*post .* multiversion_fast.*\", \"= 2\", \/\/ vectorized and scalar versions\n+                  \".*multiversion_delayed_slow.*\", \"= 1\", \/\/ effect from flag -> stays delayed\n+                  \".*multiversion.*\",              \"= 5\"},\n+        phase = CompilePhase.PRINT_IDEAL,\n+        applyIfAnd = {\"UseAutoVectorizationSpeculativeAliasingChecks\", \"true\",\n+                      \"LoopMultiversioningOptimizeSlowLoop\", \"false\", \/\/ slow_loop stays delayed\n+                      \"AlignVector\", \"false\"},\n+        applyIfPlatform = {\"64-bit\", \"true\"},\n+        applyIfCPUFeatureOr = {\"sse4.1\", \"true\", \"asimd\", \"true\"})\n+    static void copy_B_differentIndex_alias(byte[] a, byte[] b) {\n+        for (int i = 0; i < a.length; i++) {\n+          b[i] = a[i + INVAR_ZERO];\n+        }\n+    }\n+\n+    @Test\n+    @IR(counts = {IRNode.LOAD_VECTOR_I, \"> 0\",\n+                  IRNode.STORE_VECTOR, \"> 0\",\n+                  \".*multiversion.*\", \"= 0\"},\n+        phase = CompilePhase.PRINT_IDEAL,\n+        applyIfPlatform = {\"64-bit\", \"true\"},\n+        applyIfCPUFeatureOr = {\"sse4.1\", \"true\", \"asimd\", \"true\"})\n+    \/\/ Should always vectorize, no speculative runtime check required.\n+    static void copy_I_sameIndex_noalias(int[] a, int[] b) {\n+        for (int i = 0; i < a.length; i++) {\n+          b[i] = a[i];\n+        }\n+    }\n+\n+    @Test\n+    @IR(counts = {IRNode.LOAD_VECTOR_I, \"> 0\",\n+                  IRNode.STORE_VECTOR, \"> 0\",\n+                  \".*multiversion.*\", \"= 0\"},\n+        phase = CompilePhase.PRINT_IDEAL,\n+        applyIfPlatform = {\"64-bit\", \"true\"},\n+        applyIfCPUFeatureOr = {\"sse4.1\", \"true\", \"asimd\", \"true\"})\n+    \/\/ Should always vectorize, no speculative runtime check required.\n+    static void copy_I_sameIndex_alias(int[] a, int[] b) {\n+        for (int i = 0; i < a.length; i++) {\n+          b[i] = a[i];\n+        }\n+    }\n+\n+    @Test\n+    @IR(counts = {IRNode.LOAD_VECTOR_I, \"= 0\",\n+                  IRNode.STORE_VECTOR, \"= 0\",\n+                  \".*multiversion.*\", \"= 0\"},\n+        phase = CompilePhase.PRINT_IDEAL,\n+        applyIf = {\"UseAutoVectorizationSpeculativeAliasingChecks\", \"false\"},\n+        applyIfPlatform = {\"64-bit\", \"true\"},\n+        applyIfCPUFeatureOr = {\"sse4.1\", \"true\", \"asimd\", \"true\"})\n+    \/\/ Without speculative runtime check we cannot know that there is no aliasing.\n+    @IR(counts = {IRNode.LOAD_VECTOR_I, \"> 0\",\n+                  IRNode.STORE_VECTOR, \"> 0\",\n+                  \".*multiversion.*\", \"= 0\"},\n+        phase = CompilePhase.PRINT_IDEAL,\n+        applyIfAnd = {\"UseAutoVectorizationSpeculativeAliasingChecks\", \"true\", \"AlignVector\", \"false\"},\n+        applyIfPlatform = {\"64-bit\", \"true\"},\n+        applyIfCPUFeatureOr = {\"sse4.1\", \"true\", \"asimd\", \"true\"})\n+    \/\/ We use speculative runtime checks, they never fail, so no multiversioning required.\n+    \/\/ With AlignVector we cannot prove that both accesses are alignable.\n+    static void copy_I_differentIndex_noalias(int[] a, int[] b) {\n+        for (int i = 0; i < a.length; i++) {\n+          b[i] = a[i + INVAR_ZERO];\n+        }\n+    }\n+\n+    @Test\n+    @IR(counts = {IRNode.LOAD_VECTOR_I, \"= 0\",\n+                  IRNode.STORE_VECTOR, \"= 0\",\n+                  \".*multiversion.*\", \"= 0\"},\n+        phase = CompilePhase.PRINT_IDEAL,\n+        applyIf = {\"UseAutoVectorizationSpeculativeAliasingChecks\", \"false\"},\n+        applyIfPlatform = {\"64-bit\", \"true\"},\n+        applyIfCPUFeatureOr = {\"sse4.1\", \"true\", \"asimd\", \"true\"})\n+    \/\/ Without speculative runtime check we cannot know that there is no aliasing.\n+    @IR(counts = {IRNode.LOAD_VECTOR_I,            \"> 0\",\n+                  IRNode.STORE_VECTOR,             \"> 0\",\n+                  \".*pre .* multiversion_fast.*\",  \"= 1\",\n+                  \".*main .* multiversion_fast.*\", \"= 1\",\n+                  \".*post .* multiversion_fast.*\", \"= 2\", \/\/ vectorized and scalar versions\n+                  \".*multiversion_slow.*\",         \"= 2\", \/\/ main and post (pre-loop only has a single iteration)\n+                  \".*multiversion.*\",              \"= 6\"},\n+        phase = CompilePhase.PRINT_IDEAL,\n+        applyIfAnd = {\"UseAutoVectorizationSpeculativeAliasingChecks\", \"true\",\n+                      \"LoopMultiversioningOptimizeSlowLoop\", \"true\",\n+                      \"AlignVector\", \"false\"},\n+        applyIfPlatform = {\"64-bit\", \"true\"},\n+        applyIfCPUFeatureOr = {\"sse4.1\", \"true\", \"asimd\", \"true\"})\n+    \/\/ We use speculative runtime checks, it fails and so we do need multiversioning.\n+    \/\/ With AlignVector we cannot prove that both accesses are alignable.\n+    @IR(counts = {IRNode.LOAD_VECTOR_I,            \"> 0\",\n+                  IRNode.STORE_VECTOR,             \"> 0\",\n+                  \".*pre .* multiversion_fast.*\",  \"= 1\",\n+                  \".*main .* multiversion_fast.*\", \"= 1\",\n+                  \".*post .* multiversion_fast.*\", \"= 2\", \/\/ vectorized and scalar versions\n+                  \".*multiversion_delayed_slow.*\", \"= 1\", \/\/ effect from flag -> stays delayed\n+                  \".*multiversion.*\",              \"= 5\"},\n+        phase = CompilePhase.PRINT_IDEAL,\n+        applyIfAnd = {\"UseAutoVectorizationSpeculativeAliasingChecks\", \"true\",\n+                      \"LoopMultiversioningOptimizeSlowLoop\", \"false\", \/\/ slow_loop stays delayed\n+                      \"AlignVector\", \"false\"},\n+        applyIfPlatform = {\"64-bit\", \"true\"},\n+        applyIfCPUFeatureOr = {\"sse4.1\", \"true\", \"asimd\", \"true\"})\n+    static void copy_I_differentIndex_alias(int[] a, int[] b) {\n+        for (int i = 0; i < a.length; i++) {\n+            b[i] = a[i + INVAR_ZERO];\n+        }\n+    }\n+\n+    @Test\n+    @IR(counts = {IRNode.STORE_VECTOR, \"= 0\",\n+                  \".*multiversion.*\", \"= 0\"},\n+        phase = CompilePhase.PRINT_IDEAL,\n+        applyIf = {\"UseAutoVectorizationSpeculativeAliasingChecks\", \"false\"},\n+        applyIfPlatform = {\"64-bit\", \"true\"},\n+        applyIfCPUFeatureOr = {\"sse4.1\", \"true\", \"asimd\", \"true\"})\n+    \/\/ Without speculative runtime check we cannot know that there is no aliasing.\n+    @IR(counts = {IRNode.STORE_VECTOR,             \"> 0\",\n+                  \".*pre .* multiversion_fast.*\",  \"= 1\",\n+                  \".*main .* multiversion_fast.*\", \"= 1\",\n+                  \".*post .* multiversion_fast.*\", \"= 2\", \/\/ vectorized and scalar versions\n+                  \".*multiversion_slow.*\",         \"= 2\", \/\/ main and post (pre-loop only has a single iteration)\n+                  \".*multiversion.*\",              \"= 6\"},\n+        phase = CompilePhase.PRINT_IDEAL,\n+        applyIfAnd = {\"UseAutoVectorizationSpeculativeAliasingChecks\", \"true\",\n+                      \"LoopMultiversioningOptimizeSlowLoop\", \"true\",\n+                      \"AlignVector\", \"false\"},\n+        applyIfPlatform = {\"64-bit\", \"true\"},\n+        applyIfCPUFeatureOr = {\"sse4.1\", \"true\", \"asimd\", \"true\"})\n+    \/\/ We use speculative runtime checks, it fails and so we do need multiversioning.\n+    \/\/ With AlignVector we cannot prove that both accesses are alignable.\n+    @IR(counts = {IRNode.STORE_VECTOR,             \"> 0\",\n+                  \".*pre .* multiversion_fast.*\",  \"= 1\",\n+                  \".*main .* multiversion_fast.*\", \"= 1\",\n+                  \".*post .* multiversion_fast.*\", \"= 2\", \/\/ vectorized and scalar versions\n+                  \".*multiversion_delayed_slow.*\", \"= 1\", \/\/ effect from flag -> stays delayed\n+                  \".*multiversion.*\",              \"= 5\"},\n+        phase = CompilePhase.PRINT_IDEAL,\n+        applyIfAnd = {\"UseAutoVectorizationSpeculativeAliasingChecks\", \"true\",\n+                      \"LoopMultiversioningOptimizeSlowLoop\", \"false\", \/\/ slow_loop stays delayed\n+                      \"AlignVector\", \"false\"},\n+        applyIfPlatform = {\"64-bit\", \"true\"},\n+        applyIfCPUFeatureOr = {\"sse4.1\", \"true\", \"asimd\", \"true\"})\n+    \/\/ FYI: invar1 and invar2 are small values, only used to test that everything runs\n+    \/\/      correctly with at different offsets \/ with different alignment.\n+    static void test_fill_B_sameArray_alias(byte[] a, byte[] b, int invar1, int invar2) {\n+        for (int i = 0; i < a.length - 100; i++) {\n+            a[i + invar1] = (byte)0x0a;\n+            b[a.length - i - 1 - invar2] = (byte)0x0b;\n+        }\n+    }\n+\n+    @DontCompile\n+    static void reference_fill_B_sameArray_alias(byte[] a, byte[] b, int invar1, int invar2) {\n+        for (int i = 0; i < a.length - 100; i++) {\n+            a[i + invar1] = (byte)0x0a;\n+            b[a.length - i - 1 - invar2] = (byte)0x0b;\n+        }\n+    }\n+\n+    @Test\n+    @IR(counts = {IRNode.STORE_VECTOR, \"= 0\",\n+                  \".*multiversion.*\", \"= 0\"},\n+        phase = CompilePhase.PRINT_IDEAL,\n+        applyIf = {\"UseAutoVectorizationSpeculativeAliasingChecks\", \"false\"},\n+        applyIfPlatform = {\"64-bit\", \"true\"},\n+        applyIfCPUFeatureOr = {\"sse4.1\", \"true\", \"asimd\", \"true\"})\n+    \/\/ Without speculative runtime check we cannot know that there is no aliasing.\n+    @IR(counts = {IRNode.STORE_VECTOR,             \"> 0\",\n+                  \".*multiversion.*\",              \"= 0\"},\n+        phase = CompilePhase.PRINT_IDEAL,\n+        applyIfAnd = {\"UseAutoVectorizationSpeculativeAliasingChecks\", \"true\",\n+                      \"AlignVector\", \"false\"},\n+        applyIfPlatform = {\"64-bit\", \"true\"},\n+        applyIfCPUFeatureOr = {\"sse4.1\", \"true\", \"asimd\", \"true\"})\n+    \/\/ We use speculative runtime checks, and they should not fail, so no multiversioning.\n+    static void test_fill_B_sameArray_noalias(byte[] a, byte[] b, int invar1, int invar2, int limit) {\n+        for (int i = 0; i < limit; i++) {\n+            a[invar1 + i] = (byte)0x0a;\n+            b[invar2 - i] = (byte)0x0b;\n+        }\n+    }\n+\n+    @DontCompile\n+    static void reference_fill_B_sameArray_noalias(byte[] a, byte[] b, int invar1, int invar2, int limit) {\n+        for (int i = 0; i < limit; i++) {\n+            a[invar1 + i] = (byte)0x0a;\n+            b[invar2 - i] = (byte)0x0b;\n+        }\n+    }\n+}\n","filename":"test\/hotspot\/jtreg\/compiler\/loopopts\/superword\/TestAliasing.java","additions":547,"deletions":0,"binary":false,"changes":547,"status":"added"},{"patch":"@@ -0,0 +1,1271 @@\n+\/*\n+ * Copyright (c) 2024, 2025, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+\/*\n+ * @test id=vanilla\n+ * @bug 8324751\n+ * @summary Test Speculative Aliasing checks in SuperWord\n+ * @modules java.base\/jdk.internal.misc\n+ * @library \/test\/lib \/\n+ * @compile ..\/..\/..\/compiler\/lib\/ir_framework\/TestFramework.java\n+ * @compile ..\/..\/..\/compiler\/lib\/generators\/Generators.java\n+ * @compile ..\/..\/..\/compiler\/lib\/verify\/Verify.java\n+ * @run driver compiler.loopopts.superword.TestAliasingFuzzer vanilla\n+ *\/\n+\n+\/*\n+ * @test id=random-flags\n+ * @bug 8324751\n+ * @summary Test Speculative Aliasing checks in SuperWord\n+ * @modules java.base\/jdk.internal.misc\n+ * @library \/test\/lib \/\n+ * @compile ..\/..\/..\/compiler\/lib\/ir_framework\/TestFramework.java\n+ * @compile ..\/..\/..\/compiler\/lib\/generators\/Generators.java\n+ * @compile ..\/..\/..\/compiler\/lib\/verify\/Verify.java\n+ * @run driver compiler.loopopts.superword.TestAliasingFuzzer random-flags\n+ *\/\n+\n+package compiler.loopopts.superword;\n+\n+import java.util.Set;\n+import java.util.List;\n+import java.util.ArrayList;\n+import java.util.Random;\n+import java.util.Arrays;\n+import java.util.stream.Collectors;\n+import java.util.stream.IntStream;\n+\n+import jdk.test.lib.Utils;\n+\n+import compiler.lib.compile_framework.*;\n+import compiler.lib.generators.Generators;\n+import compiler.lib.template_framework.Template;\n+import compiler.lib.template_framework.TemplateToken;\n+import static compiler.lib.template_framework.Template.body;\n+import static compiler.lib.template_framework.Template.let;\n+import static compiler.lib.template_framework.Template.$;\n+\n+import compiler.lib.template_framework.library.TestFrameworkClass;\n+\n+\/**\n+ * Simpler test cases can be found in {@link TestAliasing}.\n+ *\n+ * We randomly generate tests to verify the behavior of the aliasing runtime checks. We feature:\n+ * - Different primitive types:\n+ *   - for access type (primitive, we can have multiple types in a single loop)\n+ *   - for backing type (primitive and additionally we have native memory)\n+ * - Different AccessScenarios:\n+ *   - copy (load and store)\n+ *   - fill (using two stores)\n+ * - Different Aliasing: in some cases we never alias at runtime, in other cases we might\n+ *   -> Should exercise both the predicate and the multiversioning approach with the\n+ *      aliasing runtime checks.\n+ * - Backing memory\n+ *   - Arrays: using int-index\n+ *   - MemorySegment (backed by primitive array or native memory):\n+ *     - Using long-index with MemorySegment::getAtIndex\n+ *     - Using byte-offset with MemorySegment::get\n+ * - Loop iv:\n+ *   - forward (counting up) and backward (counting down)\n+ *   - Different iv stride:\n+ *     - inc\/dec by one, and then scale with ivScale:   for (..; i++)  { access(i * 4); }\n+ *     - abs(ivScale) == 1, but use iv stride instead:  for (..; i+=4) { access(i); }\n+ *   - type of index, invars, and bounds (see isLongIvType)\n+ *     - int: for array and MemorySegment\n+ *     - long: for MemorySegment\n+ * - IR rules:\n+ *   - Verify that verification does (not) happen as expected.\n+ *   - Verify that we do not use multiversioning when no aliasing is expected at runtime.\n+ *     -> verify that the aliasing runtime check is not overly sensitive, so that the\n+ *        predicate does not fail unnecessarily and we have to recompile with multiversioning.\n+ *\n+ * Possible extensions (Future Work):\n+ * - Access with Unsafe\n+ * - Backing memory with Buffers\n+ * - AccessScenario:\n+ *   - More than two accesses\n+ * - Improve IR rules, once more cases vectorize (see e.g. JDK-8359688)\n+ * - Aliasing:\n+ *   - MemorySegment on same backing memory, creating different MemorySegments\n+ *     via slicing. Possibly overlapping MemorySegments.\n+ *   - CONTAINER_UNKNOWN_ALIASING_NEVER: currently always has different\n+ *     memory and split ranges. But we could alternate between same memory\n+ *     and split ranges, and then different memory but overlapping ranges.\n+ *     This would also be never aliasing.\n+ *\n+ *\/\n+public class TestAliasingFuzzer {\n+    private static final Random RANDOM = Utils.getRandomInstance();\n+\n+    public record MyType(String name, int byteSize, String con1, String con2, String layout) {\n+        @Override\n+        public String toString() { return name(); }\n+\n+        public String letter() { return name().substring(0, 1).toUpperCase(); }\n+    }\n+    public static final String con1 = \"0x0102030405060708L\";\n+    public static final String con2 = \"0x0910111213141516L\";\n+    public static final String con1F = \"Float.intBitsToFloat(0x01020304)\";\n+    public static final String con2F = \"Float.intBitsToFloat(0x09101112)\";\n+    public static final String con1D = \"Double.longBitsToDouble(\" + con1 + \")\";\n+    public static final String con2D = \"Double.longBitsToDouble(\" + con2 + \")\";\n+\n+    \/\/ List of primitive types for accesses and arrays.\n+    public static final MyType myByte   = new MyType(\"byte\",   1, con1, con2,   \"ValueLayout.JAVA_BYTE\");\n+    public static final MyType myChar   = new MyType(\"char\",   2, con1, con2,   \"ValueLayout.JAVA_CHAR_UNALIGNED\");\n+    public static final MyType myShort  = new MyType(\"short\",  2, con1, con2,   \"ValueLayout.JAVA_SHORT_UNALIGNED\");\n+    public static final MyType myInt    = new MyType(\"int\",    4, con1, con2,   \"ValueLayout.JAVA_INT_UNALIGNED\");\n+    public static final MyType myLong   = new MyType(\"long\",   8, con1, con2,   \"ValueLayout.JAVA_LONG_UNALIGNED\");\n+    public static final MyType myFloat  = new MyType(\"float\",  4, con1F, con2F, \"ValueLayout.JAVA_FLOAT_UNALIGNED\");\n+    public static final MyType myDouble = new MyType(\"double\", 8, con1D, con2D, \"ValueLayout.JAVA_DOUBLE_UNALIGNED\");\n+    public static final List<MyType> primitiveTypes\n+        = List.of(myByte, myChar, myShort, myInt, myLong, myFloat, myDouble);\n+\n+    \/\/ For native memory, we use this \"fake\" type. It has a byteSize of 1, since we measure the memory in bytes.\n+    public static final MyType myNative = new MyType(\"native\", 1, null, null,   null);\n+    public static final List<MyType> primitiveTypesAndNative\n+        = List.of(myByte, myChar, myShort, myInt, myLong, myFloat, myDouble, myNative);\n+\n+    \/\/ Do the containers (array, MemorySegment, etc) ever overlap?\n+    enum Aliasing {\n+        CONTAINER_DIFFERENT,\n+        CONTAINER_SAME_ALIASING_NEVER,\n+        CONTAINER_SAME_ALIASING_UNKNOWN,\n+        CONTAINER_UNKNOWN_ALIASING_NEVER,\n+        CONTAINER_UNKNOWN_ALIASING_UNKNOWN,\n+    }\n+\n+    enum AccessScenario {\n+        COPY_LOAD_STORE,  \/\/ a[i1] = b[i2];\n+        FILL_STORE_STORE, \/\/ a[i1] = x; b[i2] = y;\n+    }\n+\n+    enum ContainerKind {\n+        ARRAY,\n+        MEMORY_SEGMENT_LONG_ADR_SCALE,  \/\/ for (..; i++)  { access(i * 4); }\n+        MEMORY_SEGMENT_LONG_ADR_STRIDE, \/\/ for (..; i+=4) { access(i); }\n+        MEMORY_SEGMENT_AT_INDEX,\n+    }\n+\n+    public static void main(String[] args) {\n+        \/\/ Create a new CompileFramework instance.\n+        CompileFramework comp = new CompileFramework();\n+\n+        long t0 = System.nanoTime();\n+        \/\/ Add a java source file.\n+        comp.addJavaSourceCode(\"compiler.loopopts.superword.templated.AliasingFuzzer\", generate(comp));\n+\n+        long t1 = System.nanoTime();\n+        \/\/ Compile the source file.\n+        comp.compile();\n+\n+        long t2 = System.nanoTime();\n+\n+        String[] flags = switch(args[0]) {\n+            case \"vanilla\" -> new String[] {};\n+            case \"random-flags\" -> randomFlags();\n+            default -> throw new RuntimeException(\"unknown run id=\" + args[0]);\n+        };\n+        \/\/ Run the tests without any additional VM flags.\n+        \/\/ compiler.loopopts.superword.templated.AliasingFuzzer.main(new String[] {});\n+        comp.invoke(\"compiler.loopopts.superword.templated.AliasingFuzzer\", \"main\", new Object[] {flags});\n+        long t3 = System.nanoTime();\n+\n+        System.out.println(\"Code Generation:  \" + (t1-t0) * 1e-9f);\n+        System.out.println(\"Code Compilation: \" + (t2-t1) * 1e-9f);\n+        System.out.println(\"Running Tests:    \" + (t3-t2) * 1e-9f);\n+    }\n+\n+    public static String[] randomFlags() {\n+        \/\/ We don't want to always run with all flags, that is too expensive.\n+        \/\/ But let's make sure things don't completely, rot by running with some\n+        \/\/ random flags that are relevant.\n+        \/\/ We set the odds towards the \"default\" we are targetting.\n+        return new String[] {\n+            \/\/ Default disabled.\n+            \"-XX:\" + randomPlusMinus(1, 5) + \"AlignVector\",\n+            \/\/ Default enabled.\n+            \"-XX:\" + randomPlusMinus(5, 1) + \"UseAutoVectorizationSpeculativeAliasingChecks\",\n+            \"-XX:\" + randomPlusMinus(5, 1) + \"UseAutoVectorizationPredicate\",\n+            \"-XX:\" + randomPlusMinus(5, 1) + \"LoopMultiversioningOptimizeSlowLoop\",\n+            \/\/ Either way is ok.\n+            \"-XX:\" + randomPlusMinus(1, 1) + \"UseCompactObjectHeaders\",\n+            \"-XX:SuperWordAutomaticAlignment=\" + RANDOM.nextInt(0,3)\n+        };\n+    }\n+\n+    public static String randomPlusMinus(int plus, int minus) {\n+        return (RANDOM.nextInt(plus + minus) < plus) ? \"+\" : \"-\";\n+    }\n+\n+    public static <T> T sample(List<T> list) {\n+        int r = RANDOM.nextInt(list.size());\n+        return list.get(r);\n+    }\n+\n+    public static String generate(CompileFramework comp) {\n+        \/\/ Create a list to collect all tests.\n+        List<TemplateToken> testTemplateTokens = new ArrayList<>();\n+\n+        \/\/ Add some basic functionalities.\n+        testTemplateTokens.add(generateIndexForm());\n+\n+        \/\/ Array tests\n+        for (int i = 0; i < 20; i++) {\n+            testTemplateTokens.add(TestGenerator.makeArray().generate());\n+        }\n+\n+        \/\/ MemorySegment with getAtIndex \/ setAtIndex\n+        for (int i = 0; i < 40; i++) {\n+            testTemplateTokens.add(TestGenerator.makeMemorySegment().generate());\n+        }\n+\n+        \/\/ Create the test class, which runs all testTemplateTokens.\n+        return TestFrameworkClass.render(\n+            \/\/ package and class name.\n+            \"compiler.loopopts.superword.templated\", \"AliasingFuzzer\",\n+            \/\/ List of imports.\n+            Set.of(\"compiler.lib.generators.*\",\n+                   \"compiler.lib.verify.*\",\n+                   \"java.lang.foreign.*\",\n+                   \"java.util.Random\",\n+                   \"jdk.test.lib.Utils\"),\n+            \/\/ classpath, so the Test VM has access to the compiled class files.\n+            comp.getEscapedClassPathOfCompiledClasses(),\n+            \/\/ The list of tests.\n+            testTemplateTokens);\n+    }\n+\n+    \/\/ The IndexForm is used to model the index. We can use it for arrays, but also by\n+    \/\/ restricting the MemorySegment index to a simple index.\n+    \/\/\n+    \/\/ Form:\n+    \/\/   index = con + iv * ivScale + invar0 * invar0Scale + invarRest\n+    \/\/                                                       [err]\n+    \/\/\n+    \/\/ The index has a size >= 1, so that the index refers to a region:\n+    \/\/   [index, index + size]\n+    \/\/\n+    \/\/ The idea is that invarRest is always close to zero, with some small range [-err .. err].\n+    \/\/ The invar variables for invarRest must be in the range [-1, 0, 1], so that we can\n+    \/\/ estimate the error range from the invarRestScales.\n+    \/\/\n+    \/\/ At runtime, we will have to generate inputs for the iv.lo\/iv.hi, as well as the invar0,\n+    \/\/ so that the index range lays in some predetermined range [range.lo, range.hi] and the\n+    \/\/ ivStride:\n+    \/\/\n+    \/\/ for (int iv = iv.lo; iv < iv.hi; iv += ivStride) {\n+    \/\/     assert: range.lo <= index(iv)\n+    \/\/                         index(iv) + size <= range.hi\n+    \/\/ }\n+    \/\/\n+    \/\/ Since there are multiple memory accesses, we may have multiple indices to compute.\n+    \/\/ Since they are all in the same loop, the indices share the same iv.lo and iv.hi. Hence,\n+    \/\/ we fix either iv.lo or iv.hi, and compute the other via the constraints.\n+    \/\/\n+    \/\/ Fix iv.lo, assume ivScale > 0:\n+    \/\/   index(iv) is smallest for iv = iv.lo, so we must satisfy\n+    \/\/     range.lo <= con + iv.lo * ivScale + invar0 * invar0Scale + invarRest\n+    \/\/              <= con + iv.lo * ivScale + invar0 * invar0Scale - err\n+    \/\/   It follows:\n+    \/\/     invar0 * invar0Scale >= range.lo - con - iv.lo * ivScale + err\n+    \/\/   This allows us to pick a invar0.\n+    \/\/   Now, we can compute the largest iv.lo possible.\n+    \/\/   index(iv) is largest for iv = iv.hi, so we must satisfy:\n+    \/\/     range.hi >= con + iv.hi * ivScale + invar0 * invar0Scale + invarRest + size\n+    \/\/              >= con + iv.hi * ivScale + invar0 * invar0Scale + err       + size\n+    \/\/   It follows:\n+    \/\/     iv.hi * ivScale <= range.hi - con - invar0 * invar0Scale - err - size\n+    \/\/   This allows us to pick a iv.hi.\n+    \/\/\n+    \/\/ More details can be found in the implementation below.\n+    \/\/\n+    public static record IndexForm(int con, int ivScale, int invar0Scale, int[] invarRestScales, int size) {\n+        public static IndexForm random(int numInvarRest, int size, int ivStrideAbs) {\n+            int con = RANDOM.nextInt(-100_000, 100_000);\n+            int ivScale = randomScale(size \/ ivStrideAbs);\n+            int invar0Scale = randomScale(size);\n+            int[] invarRestScales = new int[numInvarRest];\n+            \/\/ Sample values [-1, 0, 1]\n+            for (int i = 0; i < invarRestScales.length; i++) {\n+                invarRestScales[i] = RANDOM.nextInt(-1, 2);\n+            }\n+            return new IndexForm(con, ivScale, invar0Scale, invarRestScales, size);\n+        }\n+\n+        public static int randomScale(int size) {\n+            int scale = switch(RANDOM.nextInt(10)) {\n+                case 0 -> RANDOM.nextInt(1, 4 * size + 1); \/\/ any strided access\n+                default -> size; \/\/ in most cases, we do not want it to be strided\n+            };\n+            return RANDOM.nextBoolean() ? scale : -scale;\n+        }\n+\n+        public String generate() {\n+            return \"new IndexForm(\" + con() + \", \" + ivScale() + \", \" + invar0Scale() + \", new int[] {\" +\n+                   Arrays.stream(invarRestScales)\n+                         .mapToObj(String::valueOf)\n+                         .collect(Collectors.joining(\", \")) +\n+                   \"}, \" + size() + \")\";\n+        }\n+\n+        public TemplateToken index(String invar0, String[] invarRest) {\n+            var template = Template.make(() -> body(\n+                let(\"con\", con),\n+                let(\"ivScale\", ivScale),\n+                let(\"invar0Scale\", invar0Scale),\n+                let(\"invar0\", invar0),\n+                \"#con + #ivScale * i + #invar0Scale * #invar0\",\n+                IntStream.range(0, invarRestScales.length).mapToObj(\n+                    i -> List.of(\" + \", invarRestScales[i], \" * \", invarRest[i])\n+                ).toList()\n+            ));\n+            return template.asToken();\n+        }\n+\n+        \/\/ MemorySegment need to be long-addressed, otherwise there can be int-overflow\n+        \/\/ in the index, and that prevents RangeCheck Elimination and Vectorization.\n+        public TemplateToken indexLong(String invar0, String[] invarRest) {\n+            var template = Template.make(() -> body(\n+                let(\"con\", con),\n+                let(\"ivScale\", ivScale),\n+                let(\"invar0Scale\", invar0Scale),\n+                let(\"invar0\", invar0),\n+                \"#{con}L + #{ivScale}L * i + #{invar0Scale}L * #invar0\",\n+                IntStream.range(0, invarRestScales.length).mapToObj(\n+                    i -> List.of(\" + \", invarRestScales[i], \"L * \", invarRest[i])\n+                ).toList()\n+            ));\n+            return template.asToken();\n+        }\n+    }\n+\n+    \/\/ Mirror the IndexForm from the generator to the test.\n+    public static TemplateToken generateIndexForm() {\n+        var template = Template.make(() -> body(\n+            \"\"\"\n+            private static final Random RANDOM = Utils.getRandomInstance();\n+\n+            public static record IndexForm(int con, int ivScale, int invar0Scale, int[] invarRestScales, int size) {\n+                public IndexForm {\n+                    if (ivScale == 0 || invar0Scale == 0) {\n+                        throw new RuntimeException(\"Bad scales: \" + ivScale + \" \" + invar0Scale);\n+                    }\n+                }\n+\n+                public static record Range(int lo, int hi) {\n+                    public Range {\n+                        if (lo >= hi) { throw new RuntimeException(\"Bad range: \" + lo + \" \" + hi); }\n+                    }\n+                }\n+\n+                public int err() {\n+                    int sum = 0;\n+                    for (int scale : invarRestScales) { sum += Math.abs(scale); }\n+                    return sum;\n+                }\n+\n+                public int invar0ForIvLo(Range range, int ivLo) {\n+                    if (ivScale > 0) {\n+                        \/\/ index(iv) is smallest for iv = ivLo, so we must satisfy:\n+                        \/\/   range.lo <= con + iv.lo * ivScale + invar0 * invar0Scale + invarRest\n+                        \/\/            <= con + iv.lo * ivScale + invar0 * invar0Scale - err\n+                        \/\/ It follows:\n+                        \/\/   invar0 * invar0Scale >= range.lo - con - iv.lo * ivScale + err\n+                        int rhs = range.lo() - con - ivLo * ivScale + err();\n+                        int invar0 = (invar0Scale > 0)\n+                        ?\n+                            \/\/ invar0 * invar0Scale >=  range.lo - con - iv.lo * ivScale + err\n+                            \/\/ invar0               >= (range.lo - con - iv.lo * ivScale + err) \/ invar0Scale\n+                            Math.floorDiv(rhs + invar0Scale - 1, invar0Scale) \/\/ round up division\n+                        :\n+                            \/\/ invar0 * invar0Scale >=  range.lo - con - iv.lo * ivScale + err\n+                            \/\/ invar0               <= (range.lo - con - iv.lo * ivScale + err) \/ invar0Scale\n+                            Math.floorDiv(rhs, invar0Scale); \/\/ round down division\n+                        if (range.lo() > con + ivLo * ivScale + invar0 * invar0Scale - err()) {\n+                            throw new RuntimeException(\"sanity check failed (1)\");\n+                        }\n+                        return invar0;\n+                    } else {\n+                        \/\/ index(iv) is largest for iv = ivLo, so we must satisfy:\n+                        \/\/   range.hi >= con + iv.lo * ivScale + invar0 * invar0Scale + invarRest + size\n+                        \/\/            >= con + iv.lo * ivScale + invar0 * invar0Scale + err       + size\n+                        \/\/ It follows:\n+                        \/\/   invar0 * invar0Scale <= range.hi - con - iv.lo * ivScale - err - size\n+                        int rhs = range.hi() - con - ivLo * ivScale - err() - size();\n+                        int invar0 = (invar0Scale > 0)\n+                        ?\n+                            \/\/ invar0 * invar0Scale <= rhs\n+                            \/\/ invar0               <= rhs \/ invar0Scale\n+                            Math.floorDiv(rhs, invar0Scale) \/\/ round down division\n+                        :\n+                            \/\/ invar0 * invar0Scale <= rhs\n+                            \/\/ invar0               >= rhs \/ invar0Scale\n+                            Math.floorDiv(rhs + invar0Scale + 1, invar0Scale); \/\/ round up division\n+                        if (range.hi() < con + ivLo * ivScale + invar0 * invar0Scale + err() + size()) {\n+                            throw new RuntimeException(\"sanity check failed (2)\");\n+                        }\n+                        return invar0;\n+\n+                    }\n+                }\n+\n+                public int ivHiForInvar0(Range range, int invar0) {\n+                    if (ivScale > 0) {\n+                        \/\/ index(iv) is largest for iv = ivHi, so we must satisfy:\n+                        \/\/   range.hi >= con + iv.hi * ivScale + invar0 * invar0Scale + invarRest + size\n+                        \/\/            >= con + iv.hi * ivScale + invar0 * invar0Scale + err       + size\n+                        \/\/ It follows:\n+                        \/\/   iv.hi * ivScale <=  range.hi - con - invar0 * invar0Scale - err - size\n+                        \/\/   iv.hi           <= (range.hi - con - invar0 * invar0Scale - err - size) \/ ivScale\n+                        int rhs = range.hi() - con - invar0 * invar0Scale - err() - size();\n+                        int ivHi = Math.floorDiv(rhs, ivScale); \/\/ round down division\n+                        if (range.hi() < con + ivHi * ivScale + invar0 * invar0Scale + err() + size()) {\n+                            throw new RuntimeException(\"sanity check failed (3)\");\n+                        }\n+                        return ivHi;\n+                    } else {\n+                        \/\/ index(iv) is smallest for iv = ivHi, so we must satisfy:\n+                        \/\/   range.lo <= con + iv.hi * ivScale + invar0 * invar0Scale + invarRest\n+                        \/\/            <= con + iv.hi * ivScale + invar0 * invar0Scale - err\n+                        \/\/ It follows:\n+                        \/\/   iv.hi * ivScale >=  range.lo - con - invar0 * invar0Scale + err\n+                        \/\/   iv.hi           <= (range.lo - con - invar0 * invar0Scale + err) \/ ivScale\n+                        int rhs = range.lo() - con - invar0 * invar0Scale + err();\n+                        int ivHi = Math.floorDiv(rhs, ivScale); \/\/ round down division\n+                        if (range.lo() > con + ivHi * ivScale + invar0 * invar0Scale - err()) {\n+                            throw new RuntimeException(\"sanity check failed (4)\");\n+                        }\n+                        return ivHi;\n+\n+                    }\n+                }\n+            }\n+            \"\"\"\n+        ));\n+        return template.asToken();\n+    }\n+\n+    public static record TestGenerator(\n+        \/\/ The containers.\n+        int numContainers,\n+        int containerByteSize,\n+        ContainerKind containerKind,\n+        MyType containerElementType,\n+\n+        \/\/ Do we count up or down, iterate over the containers forward or backward?\n+        boolean loopForward,\n+        int ivStrideAbs,\n+        boolean isLongIvType,\n+\n+        \/\/ For all index forms: number of invariants in the rest, i.e. the [err] term.\n+        int numInvarRest,\n+\n+        \/\/ Each access has an index form and a type.\n+        IndexForm[] accessIndexForm,\n+        MyType[] accessType,\n+\n+        \/\/ The scenario.\n+        Aliasing aliasing,\n+        AccessScenario accessScenario) {\n+\n+        public static TestGenerator makeArray() {\n+            \/\/ Sample some random parameters:\n+            Aliasing aliasing = sample(Arrays.asList(Aliasing.values()));\n+            AccessScenario accessScenario = sample(Arrays.asList(AccessScenario.values()));\n+            MyType type = sample(primitiveTypes);\n+\n+            \/\/ size must be large enough for:\n+            \/\/   - scale = 4\n+            \/\/   - range with size \/ 4\n+            \/\/ -> need at least size 16_000 to ensure we have 1000 iterations\n+            \/\/ We want there to be a little variation, so alignment is not always the same.\n+            int numElements = Generators.G.safeRestrict(Generators.G.ints(), 18_000, 20_000).next();\n+            int containerByteSize = numElements * type.byteSize();\n+            boolean loopForward = RANDOM.nextBoolean();\n+\n+            int numInvarRest = RANDOM.nextInt(5);\n+            int ivStrideAbs = 1;\n+            boolean isLongIvType = false; \/\/ int index\n+            var form0 = IndexForm.random(numInvarRest, 1, ivStrideAbs);\n+            var form1 = IndexForm.random(numInvarRest, 1, ivStrideAbs);\n+\n+            return new TestGenerator(\n+                2,\n+                containerByteSize,\n+                ContainerKind.ARRAY,\n+                type,\n+                loopForward,\n+                ivStrideAbs,\n+                isLongIvType,\n+                numInvarRest,\n+                new IndexForm[] {form0, form1},\n+                new MyType[]    {type,   type},\n+                aliasing,\n+                accessScenario);\n+        }\n+\n+        public static int alignUp(int value, int align) {\n+            return Math.ceilDiv(value, align) * align;\n+        }\n+\n+        public static TestGenerator makeMemorySegment() {\n+            \/\/ Sample some random parameters:\n+            Aliasing aliasing = sample(Arrays.asList(Aliasing.values()));\n+            AccessScenario accessScenario = sample(Arrays.asList(AccessScenario.values()));\n+            \/\/ Backing memory can be native, access must be primitive.\n+            MyType containerElementType = sample(primitiveTypesAndNative);\n+            MyType accessType0 = sample(primitiveTypes);\n+            MyType accessType1 = sample(primitiveTypes);\n+            ContainerKind containerKind = sample(List.of(\n+                ContainerKind.MEMORY_SEGMENT_AT_INDEX,\n+                ContainerKind.MEMORY_SEGMENT_LONG_ADR_SCALE,\n+                ContainerKind.MEMORY_SEGMENT_LONG_ADR_STRIDE\n+            ));\n+\n+            if (containerKind == ContainerKind.MEMORY_SEGMENT_AT_INDEX) {\n+                \/\/ The access types must be the same, it is a limitation of the index computation.\n+                accessType1 = accessType0;\n+            }\n+\n+            final int minAccessSize = Math.min(accessType0.byteSize(), accessType1.byteSize());\n+            final int maxAccessSize = Math.max(accessType0.byteSize(), accessType1.byteSize());\n+\n+            \/\/ size must be large enough for:\n+            \/\/   - scale = 4\n+            \/\/   - range with size \/ 4\n+            \/\/ -> need at least size 16_000 to ensure we have 1000 iterations\n+            \/\/ We want there to be a little variation, so alignment is not always the same.\n+            final int numAccessElements = Generators.G.safeRestrict(Generators.G.ints(), 18_000, 20_000).next();\n+            final int align = Math.max(maxAccessSize, containerElementType.byteSize());\n+            \/\/ We need to align up, so the size is divisible exactly by all involved type sizes.\n+            final int containerByteSize = alignUp(numAccessElements * maxAccessSize, align);\n+            final boolean loopForward = RANDOM.nextBoolean();\n+\n+            final int numInvarRest = RANDOM.nextInt(5);\n+            int indexSize0 = accessType0.byteSize();\n+            int indexSize1 = accessType1.byteSize();\n+            if (containerKind == ContainerKind.MEMORY_SEGMENT_AT_INDEX) {\n+                \/\/ These are int-indeces for getAtIndex, so we index by element and not bytes.\n+                indexSize0 = 1;\n+                indexSize1 = 1;\n+            }\n+\n+            boolean withAbsOneIvScale = containerKind == ContainerKind.MEMORY_SEGMENT_LONG_ADR_STRIDE;\n+            int ivStrideAbs = containerKind == ContainerKind.MEMORY_SEGMENT_LONG_ADR_STRIDE ? minAccessSize : 1;\n+            boolean isLongIvType = RANDOM.nextBoolean();\n+            var form0 = IndexForm.random(numInvarRest, indexSize0, ivStrideAbs);\n+            var form1 = IndexForm.random(numInvarRest, indexSize1, ivStrideAbs);\n+\n+            return new TestGenerator(\n+                2,\n+                containerByteSize,\n+                containerKind,\n+                containerElementType,\n+                loopForward,\n+                ivStrideAbs,\n+                isLongIvType,\n+                numInvarRest,\n+                new IndexForm[] {form0, form1},\n+                new MyType[]    {accessType0, accessType1},\n+                aliasing,\n+                accessScenario);\n+        }\n+\n+        public TemplateToken generate() {\n+            var testTemplate = Template.make(() -> {\n+                \/\/ Let's generate the variable names that are to be shared for the nested Templates.\n+                String[] invarRest = new String[numInvarRest];\n+                for (int i = 0; i < invarRest.length; i++) {\n+                    invarRest[i] = $(\"invar\" + i);\n+                }\n+                String[] containerNames = new String[numContainers];\n+                for (int i = 0; i < numContainers; i++) {\n+                    containerNames[i] = $(\"container\" + i);\n+                }\n+                String[] indexFormNames = new String[accessIndexForm.length];\n+                for (int i = 0; i < indexFormNames.length; i++) {\n+                    indexFormNames[i] = $(\"index\" + i);\n+                }\n+                return body(\n+                    \"\"\"\n+                    \/\/ --- $test start ---\n+                    \"\"\",\n+                    generateTestFields(invarRest, containerNames, indexFormNames),\n+                    \"\"\"\n+                    \/\/ Count the run invocations.\n+                    private static int $iterations = 0;\n+\n+                    @Run(test = \"$test\")\n+                    @Warmup(100)\n+                    public static void $run(RunInfo info) {\n+\n+                        \/\/ Once warmup is over (100x), repeat 10x to get reasonable coverage of the\n+                        \/\/ randomness in the tests.\n+                        int reps = info.isWarmUp() ? 10 : 1;\n+                        for (int r = 0; r < reps; r++) {\n+\n+                            $iterations++;\n+                    \"\"\",\n+                    generateContainerInit(containerNames),\n+                    generateContainerAliasing(containerNames, $(\"iterations\")),\n+                    generateRanges(),\n+                    generateBoundsAndInvariants(indexFormNames, invarRest),\n+                    \"\"\"\n+                            \/\/ Run test and compare with interpreter results.\n+                    \"\"\",\n+                    generateCallMethod(\"result\", $(\"test\"), \"test\"),\n+                    generateCallMethod(\"expected\", $(\"reference\"), \"reference\"),\n+                    \"\"\"\n+                            Verify.checkEQ(result, expected);\n+                        } \/\/ end reps\n+                    } \/\/ end $run\n+\n+                    @Test\n+                    \"\"\",\n+                    generateIRRules(),\n+                    generateTestMethod($(\"test\"), invarRest),\n+                    \"\"\"\n+                    @DontCompile\n+                    \"\"\",\n+                    generateTestMethod($(\"reference\"), invarRest),\n+                    \"\"\"\n+\n+                    \/\/ --- $test end ---\n+                    \"\"\"\n+                );\n+            });\n+            return testTemplate.asToken();\n+        }\n+\n+        private TemplateToken generateArrayField(String name, MyType type) {\n+            var template = Template.make(() -> body(\n+                let(\"size\", containerByteSize \/ type.byteSize()),\n+                let(\"name\", name),\n+                let(\"type\", type),\n+                \"\"\"\n+                private static #type[] original_#name  = new #type[#size];\n+                private static #type[] test_#name      = new #type[#size];\n+                private static #type[] reference_#name = new #type[#size];\n+                \"\"\"\n+            ));\n+            return template.asToken();\n+        }\n+\n+        private TemplateToken generateMemorySegmentField(String name, MyType type) {\n+            var template = Template.make(() -> body(\n+                let(\"size\", containerByteSize \/ type.byteSize()),\n+                let(\"byteSize\", containerByteSize),\n+                let(\"name\", name),\n+                let(\"type\", type),\n+                (type == myNative\n+                 ?  \"\"\"\n+                    private static MemorySegment original_#name  = Arena.ofAuto().allocate(#byteSize);\n+                    private static MemorySegment test_#name      = Arena.ofAuto().allocate(#byteSize);\n+                    private static MemorySegment reference_#name = Arena.ofAuto().allocate(#byteSize);\n+                    \"\"\"\n+                 :  \"\"\"\n+                    private static MemorySegment original_#name  = MemorySegment.ofArray(new #type[#size]);\n+                    private static MemorySegment test_#name      = MemorySegment.ofArray(new #type[#size]);\n+                    private static MemorySegment reference_#name = MemorySegment.ofArray(new #type[#size]);\n+                    \"\"\"\n+                )\n+            ));\n+            return template.asToken();\n+        }\n+\n+        private TemplateToken generateIndexField(String name, IndexForm form) {\n+            var template = Template.make(() -> body(\n+                let(\"name\", name),\n+                let(\"form\", form.generate()),\n+                \"\"\"\n+                private static IndexForm #name = #form;\n+                \"\"\"\n+            ));\n+            return template.asToken();\n+        }\n+\n+        private TemplateToken generateTestFields(String[] invarRest, String[] containerNames, String[] indexFormNames) {\n+            var template = Template.make(() -> body(\n+                let(\"ivType\", isLongIvType ? \"long\" : \"int\"),\n+                \"\"\"\n+                \/\/ invarRest fields:\n+                \"\"\",\n+                Arrays.stream(invarRest).map(invar ->\n+                    List.of(\"private static #ivType \", invar, \" = 0;\\n\")\n+                ).toList(),\n+                \"\"\"\n+                \/\/ Containers fields:\n+                \"\"\",\n+                Arrays.stream(containerNames).map(name ->\n+                    switch (containerKind) {\n+                        case ContainerKind.ARRAY ->\n+                            generateArrayField(name, containerElementType);\n+                        case ContainerKind.MEMORY_SEGMENT_AT_INDEX,\n+                             ContainerKind.MEMORY_SEGMENT_LONG_ADR_SCALE,\n+                             ContainerKind.MEMORY_SEGMENT_LONG_ADR_STRIDE ->\n+                            generateMemorySegmentField(name, containerElementType);\n+                    }\n+                ).toList(),\n+                \"\"\"\n+                \/\/ Index forms for the accesses:\n+                \"\"\",\n+                IntStream.range(0, indexFormNames.length).mapToObj(i ->\n+                    generateIndexField(indexFormNames[i], accessIndexForm[i])\n+                ).toList()\n+            ));\n+            return template.asToken();\n+        }\n+\n+        private TemplateToken generateContainerInitArray(String name) {\n+            var template = Template.make(() -> body(\n+                let(\"size\", containerByteSize \/ containerElementType.byteSize()),\n+                let(\"name\", name),\n+                \"\"\"\n+                System.arraycopy(original_#name, 0, test_#name, 0, #size);\n+                System.arraycopy(original_#name, 0, reference_#name, 0, #size);\n+                \"\"\"\n+            ));\n+            return template.asToken();\n+        }\n+\n+        private TemplateToken generateContainerInitMemorySegment(String name) {\n+            var template = Template.make(() -> body(\n+                let(\"size\", containerByteSize \/ containerElementType.byteSize()),\n+                let(\"name\", name),\n+                \"\"\"\n+                test_#name.copyFrom(original_#name);\n+                reference_#name.copyFrom(original_#name);\n+                \"\"\"\n+            ));\n+            return template.asToken();\n+        }\n+\n+        private TemplateToken generateContainerInit(String[] containerNames) {\n+            var template = Template.make(() -> body(\n+                \"\"\"\n+                \/\/ Init containers from original data:\n+                \"\"\",\n+                Arrays.stream(containerNames).map(name ->\n+                    switch (containerKind) {\n+                        case ContainerKind.ARRAY ->\n+                            generateContainerInitArray(name);\n+                        case ContainerKind.MEMORY_SEGMENT_AT_INDEX,\n+                             ContainerKind.MEMORY_SEGMENT_LONG_ADR_SCALE,\n+                             ContainerKind.MEMORY_SEGMENT_LONG_ADR_STRIDE ->\n+                            generateContainerInitMemorySegment(name);\n+                    }\n+                ).toList()\n+             ));\n+            return template.asToken();\n+        }\n+\n+        private TemplateToken generateContainerAliasingAssignment(int i, String name1, String name2, String iterations) {\n+            var template = Template.make(() -> body(\n+                let(\"i\", i),\n+                let(\"name1\", name1),\n+                let(\"name2\", name2),\n+                let(\"iterations\", iterations),\n+                \"\"\"\n+                var test_#i      = (#iterations % 2 == 0) ? test_#name1      : test_#name2;\n+                var reference_#i = (#iterations % 2 == 0) ? reference_#name1 : reference_#name2;\n+                \"\"\"\n+            ));\n+            return template.asToken();\n+        }\n+\n+        private TemplateToken generateContainerAliasing(String[] containerNames, String iterations) {\n+            var template = Template.make(() -> body(\n+                \"\"\"\n+                \/\/ Container aliasing:\n+                \"\"\",\n+                IntStream.range(0, containerNames.length).mapToObj(i ->\n+                    switch(aliasing) {\n+                        case Aliasing.CONTAINER_DIFFERENT ->\n+                            generateContainerAliasingAssignment(i, containerNames[i], containerNames[i], iterations);\n+                        case Aliasing.CONTAINER_SAME_ALIASING_NEVER,\n+                             Aliasing.CONTAINER_SAME_ALIASING_UNKNOWN ->\n+                            generateContainerAliasingAssignment(i, containerNames[0], containerNames[0], iterations);\n+                        case Aliasing.CONTAINER_UNKNOWN_ALIASING_NEVER,\n+                             Aliasing.CONTAINER_UNKNOWN_ALIASING_UNKNOWN ->\n+                            generateContainerAliasingAssignment(i, containerNames[i], containerNames[0], iterations);\n+                    }\n+                ).toList()\n+             ));\n+            return template.asToken();\n+        }\n+\n+        private TemplateToken generateRanges() {\n+            int size = switch (containerKind) {\n+                case ContainerKind.ARRAY,\n+                     ContainerKind.MEMORY_SEGMENT_AT_INDEX ->\n+                    \/\/ Access with element index\n+                    containerByteSize \/ accessType[0].byteSize();\n+                case ContainerKind.MEMORY_SEGMENT_LONG_ADR_SCALE,\n+                     ContainerKind.MEMORY_SEGMENT_LONG_ADR_STRIDE ->\n+                    \/\/ Access with byte offset\n+                    containerByteSize;\n+            };\n+\n+            if (accessIndexForm.length != 2) { throw new RuntimeException(\"not yet implemented\"); }\n+\n+            var templateSplitRanges = Template.make(() -> body(\n+                let(\"size\", size),\n+                \"\"\"\n+                int middle = RANDOM.nextInt(#size \/ 3, #size * 2 \/ 3);\n+                int rnd = Math.min(256, #size \/ 10);\n+                int range = #size \/ 3 - RANDOM.nextInt(rnd);\n+                \"\"\",\n+                (RANDOM.nextBoolean()\n+                 \/\/ Maximal range\n+                 ?  \"\"\"\n+                    var r0 = new IndexForm.Range(0, middle);\n+                    var r1 = new IndexForm.Range(middle, #size);\n+                    \"\"\"\n+                 \/\/ Same size range\n+                 \/\/ If the accesses run towards each other, and the runtime\n+                 \/\/ check is too relaxed, we may fail the checks even though\n+                 \/\/ there is no overlap. Having same size ranges makes this\n+                 \/\/ more likely, and we could detect it if we get multiversioning\n+                 \/\/ unexpectedly.\n+                 :  \"\"\"\n+                    var r0 = new IndexForm.Range(middle - range, middle);\n+                    var r1 = new IndexForm.Range(middle, middle + range);\n+                    \"\"\"\n+                ),\n+                \"\"\"\n+                if (RANDOM.nextBoolean()) {\n+                    var tmp = r0;\n+                    r0 = r1;\n+                    r1 = tmp;\n+                }\n+                \"\"\"\n+            ));\n+\n+            var templateWholeRanges = Template.make(() -> body(\n+                let(\"size\", size),\n+                \"\"\"\n+                var r0 = new IndexForm.Range(0, #size);\n+                var r1 = new IndexForm.Range(0, #size);\n+                \"\"\"\n+            ));\n+\n+            var templateRandomRanges = Template.make(() -> body(\n+                let(\"size\", size),\n+                \"\"\"\n+                int lo0 = RANDOM.nextInt(0, #size * 3 \/ 4);\n+                int lo1 = RANDOM.nextInt(0, #size * 3 \/ 4);\n+                var r0 = new IndexForm.Range(lo0, lo0 + #size \/ 4);\n+                var r1 = new IndexForm.Range(lo1, lo1 + #size \/ 4);\n+                \"\"\"\n+            ));\n+\n+            var templateSmallOverlapRanges = Template.make(() -> body(\n+                \/\/ Idea: same size ranges, with size \"range\". A small overlap,\n+                \/\/       so that bad runtime checks would create wrong results.\n+                let(\"size\", size),\n+                \"\"\"\n+                int rnd = Math.min(256, #size \/ 10);\n+                int middle = #size \/ 2 + RANDOM.nextInt(-rnd, rnd);\n+                int range = #size \/ 3 - RANDOM.nextInt(rnd);\n+                int overlap = RANDOM.nextInt(-rnd, rnd);\n+                var r0 = new IndexForm.Range(middle - range + overlap, middle + overlap);\n+                var r1 = new IndexForm.Range(middle, middle + range);\n+                if (RANDOM.nextBoolean()) {\n+                    var tmp = r0;\n+                    r0 = r1;\n+                    r1 = tmp;\n+                }\n+                \"\"\"\n+                \/\/ Can this go out of bounds? Assume worst case on lower end:\n+                \/\/   middle         - range          + overlap\n+                \/\/   (size\/2 - rnd) - (size\/3 - rnd) - rnd\n+                \/\/   size\/6 - rnd\n+                \/\/ -> safe with rnd = size\/10\n+            ));\n+\n+            var templateAnyRanges = Template.make(() -> body(\n+                switch(RANDOM.nextInt(4)) {\n+                    case 0 -> templateSplitRanges.asToken();\n+                    case 1 -> templateWholeRanges.asToken();\n+                    case 2 -> templateRandomRanges.asToken();\n+                    case 3 -> templateSmallOverlapRanges.asToken();\n+                    default -> throw new RuntimeException(\"impossible\");\n+                }\n+            ));\n+\n+            var template = Template.make(() -> body(\n+                \"\"\"\n+                \/\/ Generate ranges:\n+                \"\"\",\n+                switch(aliasing) {\n+                    case Aliasing.CONTAINER_DIFFERENT ->\n+                        templateAnyRanges.asToken();\n+                    case Aliasing.CONTAINER_SAME_ALIASING_NEVER ->\n+                        templateSplitRanges.asToken();\n+                    case Aliasing.CONTAINER_SAME_ALIASING_UNKNOWN ->\n+                        templateAnyRanges.asToken();\n+                    case Aliasing.CONTAINER_UNKNOWN_ALIASING_NEVER ->\n+                        templateSplitRanges.asToken();\n+                    case Aliasing.CONTAINER_UNKNOWN_ALIASING_UNKNOWN ->\n+                        templateAnyRanges.asToken();\n+                }\n+            ));\n+            return template.asToken();\n+        }\n+\n+        private TemplateToken generateBoundsAndInvariants(String[] indexFormNames, String[] invarRest) {\n+            \/\/ We want there to be at least 1000 iterations.\n+            final int minIvRange = ivStrideAbs * 1000;\n+\n+            var template = Template.make(() -> body(\n+                let(\"containerByteSize\", containerByteSize),\n+                \"\"\"\n+                \/\/ Compute loop bounds and loop invariants.\n+                int ivLo = RANDOM.nextInt(-1000, 1000);\n+                int ivHi = ivLo + #containerByteSize;\n+                \"\"\",\n+                IntStream.range(0, indexFormNames.length).mapToObj(i ->\n+                    Template.make(() -> body(\n+                        let(\"i\", i),\n+                        let(\"form\", indexFormNames[i]),\n+                        \"\"\"\n+                        int invar0_#i = #form.invar0ForIvLo(r#i, ivLo);\n+                        ivHi = Math.min(ivHi, #form.ivHiForInvar0(r#i, invar0_#i));\n+                        \"\"\"\n+                    )).asToken()\n+                ).toList(),\n+                let(\"minIvRange\", minIvRange),\n+                \"\"\"\n+                \/\/ Let's check that the range is large enough, so that the vectorized\n+                \/\/ main loop can even be entered.\n+                if (ivLo + #minIvRange > ivHi) { throw new RuntimeException(\"iv range too small: \" + ivLo + \" \" + ivHi); }\n+                \"\"\",\n+                Arrays.stream(invarRest).map(invar ->\n+                    List.of(invar, \" = RANDOM.nextInt(-1, 2);\\n\")\n+                ).toList(),\n+                \"\"\"\n+                \/\/ Verify the bounds we just created, just to be sure there is no unexpected aliasing!\n+                int i = ivLo;\n+                \"\"\",\n+                IntStream.range(0, indexFormNames.length).mapToObj(i ->\n+                    List.of(\"int lo_\", i, \" = (int)(\", accessIndexForm[i].index(\"invar0_\" + i, invarRest), \");\\n\")\n+                ).toList(),\n+                \"\"\"\n+                i = ivHi;\n+                \"\"\",\n+                IntStream.range(0, indexFormNames.length).mapToObj(i ->\n+                    List.of(\"int hi_\", i, \" =  (int)(\", accessIndexForm[i].index(\"invar0_\" + i, invarRest), \");\\n\")\n+                ).toList(),\n+                switch(aliasing) {\n+                    case Aliasing.CONTAINER_SAME_ALIASING_NEVER,\n+                         Aliasing.CONTAINER_UNKNOWN_ALIASING_NEVER -> \/\/ could fail in the future if we make it smarter\n+                        List.of(\n+                        \"\"\"\n+                        \/\/ Bounds should not overlap.\n+                        if (false\n+                        \"\"\",\n+                        IntStream.range(0, indexFormNames.length).mapToObj(i1 ->\n+                            IntStream.range(0, i1).mapToObj(i2 ->\n+                                Template.make(() -> body(\n+                                    let(\"i1\", i1),\n+                                    let(\"i2\", i2),\n+                                    \/\/ i1 < i2 or i1 > i2\n+                                    \"\"\"\n+                                    || (lo_#i1 < lo_#i2 && lo_#i1 < hi_#i2 && hi_#i1 < lo_#i2 && hi_#i1 < hi_#i2)\n+                                    || (lo_#i1 > lo_#i2 && lo_#i1 > hi_#i2 && hi_#i1 > lo_#i2 && hi_#i1 > hi_#i2)\n+                                    \"\"\"\n+                                )).asToken()\n+                            ).toList()\n+                        ).toList(),\n+                        \"\"\"\n+                        ) {\n+                            \/\/ pass\n+                        } else {\n+                            throw new RuntimeException(\"bounds overlap!\");\n+                        }\n+                        \"\"\");\n+                    case Aliasing.CONTAINER_DIFFERENT,\n+                         Aliasing.CONTAINER_SAME_ALIASING_UNKNOWN,\n+                         Aliasing.CONTAINER_UNKNOWN_ALIASING_UNKNOWN ->\n+                        \"\"\"\n+                        \/\/ Aliasing unknown, cannot verify bounds.\n+                        \"\"\";\n+                }\n+            ));\n+            return template.asToken();\n+        }\n+\n+\n+        private TemplateToken generateCallMethod(String output, String methodName, String containerPrefix) {\n+            var template = Template.make(() -> body(\n+                let(\"output\", output),\n+                let(\"methodName\", methodName),\n+                \"var #output = #methodName(\",\n+                IntStream.range(0, numContainers).mapToObj(i ->\n+                    List.of(containerPrefix, \"_\", i, \", invar0_\", i, \", \")\n+                ).toList(),\n+                \"ivLo, ivHi);\\n\"\n+            ));\n+            return template.asToken();\n+        }\n+\n+        private TemplateToken generateIRRules() {\n+            var template = Template.make(() -> body(\n+                switch (containerKind) {\n+                    case ContainerKind.ARRAY ->\n+                        generateIRRulesArray();\n+                    case ContainerKind.MEMORY_SEGMENT_AT_INDEX ->\n+                        generateIRRulesMemorySegmentAtIndex();\n+                    case ContainerKind.MEMORY_SEGMENT_LONG_ADR_SCALE ->\n+                        generateIRRulesMemorySegmentLongAdrScale();\n+                    case ContainerKind.MEMORY_SEGMENT_LONG_ADR_STRIDE ->\n+                        generateIRRulesMemorySegmentLongAdrStride();\n+                },\n+                \/\/ In same scnearios, we know that a aliasing runtime check will never fail.\n+                \/\/ That means if we have UseAutoVectorizationPredicate enabled, that predicate\n+                \/\/ will never fail, and we will not have to do multiversioning.\n+                switch(aliasing) {\n+                    case Aliasing.CONTAINER_DIFFERENT,\n+                         Aliasing.CONTAINER_SAME_ALIASING_NEVER,\n+                         Aliasing.CONTAINER_UNKNOWN_ALIASING_NEVER ->\n+                        (containerKind == ContainerKind.MEMORY_SEGMENT_AT_INDEX\n+                         ?  \"\"\"\n+                            \/\/ Due to cases like JDK-8360204, there can be issues with RCE leading\n+                            \/\/ cases where we remove predicates and then unroll again and then\n+                            \/\/ end up multiversioning. These cases seem relatively rare but prevent\n+                            \/\/ us from asserting that there is never multiversioning in these cases.\n+                            \"\"\"\n+                         :  \"\"\"\n+                            \/\/ Aliasing check should never fail at runtime, so the predicate\n+                            \/\/ should never fail, and we do not have to use multiversioning.\n+                            @IR(counts = {\".*multiversion.*\", \"= 0\"},\n+                                phase = CompilePhase.PRINT_IDEAL,\n+                                applyIf = {\"UseAutoVectorizationPredicate\", \"true\"},\n+                                applyIfPlatform = {\"64-bit\", \"true\"},\n+                                applyIfCPUFeatureOr = {\"sse4.1\", \"true\", \"asimd\", \"true\"})\n+                            \"\"\");\n+                    case Aliasing.CONTAINER_SAME_ALIASING_UNKNOWN,\n+                         Aliasing.CONTAINER_UNKNOWN_ALIASING_UNKNOWN ->\n+                            \"\"\"\n+                            \/\/ Aliasing unknown, we may use the predicate or multiversioning.\n+                            \"\"\";\n+                }\n+            ));\n+            return template.asToken();\n+        }\n+\n+        \/\/ Regular array-accesses are vectorized quite predictably, and we can create nice\n+        \/\/ IR rules - even for cases where we do not expect vectorization.\n+        private TemplateToken generateIRRulesArray() {\n+            var template = Template.make(() -> body(\n+                let(\"T\", containerElementType.letter()),\n+                switch (accessScenario) {\n+                    case COPY_LOAD_STORE ->\n+                        \/\/ Currently, we do not allow strided access or shuffle.\n+                        \/\/ Since the load and store are connected, we either vectorize both or none.\n+                        (accessIndexForm[0].ivScale() == accessIndexForm[1].ivScale() &&\n+                         Math.abs(accessIndexForm[0].ivScale()) == 1)\n+                        ?   \"\"\"\n+                            \/\/ Good ivScales, vectorization expected.\n+                            @IR(counts = {IRNode.LOAD_VECTOR_#T, \"> 0\",\n+                                          IRNode.STORE_VECTOR,   \"> 0\"},\n+                                applyIfAnd = {\"UseAutoVectorizationSpeculativeAliasingChecks\", \"true\",\n+                                              \"AlignVector\", \"false\"},\n+                                applyIfPlatform = {\"64-bit\", \"true\"},\n+                                applyIfCPUFeatureOr = {\"sse4.1\", \"true\", \"asimd\", \"true\"})\n+                            \"\"\"\n+                        :   \"\"\"\n+                            \/\/ Bad ivScales, no vectorization expected.\n+                            @IR(counts = {IRNode.LOAD_VECTOR_#T, \"= 0\",\n+                                          IRNode.STORE_VECTOR,   \"= 0\"},\n+                                applyIfPlatform = {\"64-bit\", \"true\"},\n+                                applyIfCPUFeatureOr = {\"sse4.1\", \"true\", \"asimd\", \"true\"})\n+                            \"\"\";\n+                    case FILL_STORE_STORE ->\n+                        \/\/ Currently, we do not allow strided access.\n+                        \/\/ We vectorize any contiguous pattern. Possibly only one is vectorized.\n+                        (Math.abs(accessIndexForm[0].ivScale()) == 1 ||\n+                         Math.abs(accessIndexForm[1].ivScale()) == 1)\n+                        ?   \"\"\"\n+                            \/\/ Good ivScales, vectorization expected.\n+                            @IR(counts = {IRNode.LOAD_VECTOR_#T, \"= 0\",\n+                                          IRNode.STORE_VECTOR,   \"> 0\"},\n+                                applyIfAnd = {\"UseAutoVectorizationSpeculativeAliasingChecks\", \"true\",\n+                                              \"AlignVector\", \"false\"},\n+                                applyIfPlatform = {\"64-bit\", \"true\"},\n+                                applyIfCPUFeatureOr = {\"sse4.1\", \"true\", \"asimd\", \"true\"})\n+                            \"\"\"\n+                        :   \"\"\"\n+                            \/\/ Bad ivScales, no vectorization expected.\n+                            @IR(counts = {IRNode.LOAD_VECTOR_#T, \"= 0\",\n+                                          IRNode.STORE_VECTOR,   \"= 0\"},\n+                                applyIfPlatform = {\"64-bit\", \"true\"},\n+                                applyIfCPUFeatureOr = {\"sse4.1\", \"true\", \"asimd\", \"true\"})\n+                            \"\"\";\n+                }\n+            ));\n+            return template.asToken();\n+        }\n+\n+        private TemplateToken generateIRRulesMemorySegmentAtIndex() {\n+           var template = Template.make(() -> body(\n+                \"\"\"\n+                \/\/ Unfortunately, there are some issues that prevent RangeCheck elimination.\n+                \/\/ The cases are currently quite unpredictable, so we cannot create any IR\n+                \/\/ rules - sometimes there are vectors sometimes not.\n+                \"\"\"\n+                \/\/ JDK-8359688: it seems we only vectorize with ivScale=1, and not ivScale=-1\n+                \/\/              The issue seems to be RangeCheck elimination\n+            ));\n+            return template.asToken();\n+        }\n+\n+        private TemplateToken generateIRRulesMemorySegmentLongAdrStride() {\n+           var template = Template.make(() -> body(\n+                \"\"\"\n+                \/\/ Unfortunately, there are some issues that prevent RangeCheck elimination.\n+                \/\/ The cases are currently quite unpredictable, so we cannot create any IR\n+                \/\/ rules - sometimes there are vectors sometimes not.\n+                \"\"\"\n+            ));\n+            return template.asToken();\n+        }\n+\n+        private TemplateToken generateIRRulesMemorySegmentLongAdrScale() {\n+           var template = Template.make(() -> body(\n+                \"\"\"\n+                \/\/ Unfortunately, there are some issues that prevent RangeCheck elimination.\n+                \/\/ The cases are currently quite unpredictable, so we cannot create any IR\n+                \/\/ rules - sometimes there are vectors sometimes not.\n+                \"\"\"\n+            ));\n+            return template.asToken();\n+        }\n+\n+        private TemplateToken generateTestMethod(String methodName, String[] invarRest) {\n+            var template = Template.make(() -> body(\n+                let(\"methodName\", methodName),\n+                let(\"containerElementType\", containerElementType),\n+                let(\"ivStrideAbs\", ivStrideAbs),\n+                let(\"ivType\", isLongIvType ? \"long\" : \"int\"),\n+                \/\/ Method head \/ signature.\n+                \"public static Object #methodName(\",\n+                IntStream.range(0, numContainers).mapToObj(i ->\n+                    switch (containerKind) {\n+                        case ContainerKind.ARRAY ->\n+                            List.of(\"#containerElementType[] container_\", i, \", #ivType invar0_\", i, \", \");\n+                        case ContainerKind.MEMORY_SEGMENT_AT_INDEX,\n+                             ContainerKind.MEMORY_SEGMENT_LONG_ADR_SCALE,\n+                             ContainerKind.MEMORY_SEGMENT_LONG_ADR_STRIDE ->\n+                            List.of(\"MemorySegment container_\", i, \", #ivType invar0_\", i, \", \");\n+                    }\n+                ).toList(),\n+                \"#ivType ivLo, #ivType ivHi) {\\n\",\n+                \/\/ Method loop body.\n+                (loopForward\n+                 ?  \"for (#ivType i = ivLo; i < ivHi; i+=#ivStrideAbs) {\\n\"\n+                 :  \"for (#ivType i = ivHi-#ivStrideAbs; i >= ivLo; i-=#ivStrideAbs) {\\n\"),\n+                \/\/ Loop iteration.\n+                switch (containerKind) {\n+                    case ContainerKind.ARRAY ->\n+                        generateTestLoopIterationArray(invarRest);\n+                    case ContainerKind.MEMORY_SEGMENT_AT_INDEX ->\n+                        generateTestLoopIterationMemorySegmentAtIndex(invarRest);\n+                    case ContainerKind.MEMORY_SEGMENT_LONG_ADR_SCALE,\n+                         ContainerKind.MEMORY_SEGMENT_LONG_ADR_STRIDE ->\n+                        generateTestLoopIterationMemorySegmentLongAdr(invarRest);\n+                },\n+                \"\"\"\n+                    }\n+                    return new Object[] {\n+                \"\"\",\n+                \/\/ Return a list of all containers that are involved in the test.\n+                \/\/ The caller can then compare the results of the test and reference method.\n+                IntStream.range(0, numContainers).mapToObj(i ->\n+                    \"container_\" + i\n+                ).collect(Collectors.joining(\", \")), \"\\n\",\n+                \"\"\"\n+                    };\n+                }\n+                \"\"\"\n+            ));\n+            return template.asToken();\n+        }\n+\n+        private TemplateToken generateTestLoopIterationArray(String[] invarRest) {\n+            var template = Template.make(() -> body(\n+                let(\"type\", containerElementType),\n+                switch (accessScenario) {\n+                    case COPY_LOAD_STORE ->\n+                        List.of(\"container_0[\", accessIndexForm[0].index(\"invar0_0\", invarRest), \"] = \",\n+                                \"container_1[\", accessIndexForm[1].index(\"invar0_1\", invarRest), \"];\\n\");\n+                    case FILL_STORE_STORE ->\n+                        List.of(\"container_0[\", accessIndexForm[0].index(\"invar0_0\", invarRest), \"] = (#type)0x0102030405060708L;\\n\",\n+                                \"container_1[\", accessIndexForm[1].index(\"invar0_1\", invarRest), \"] = (#type)0x1112131415161718L;\\n\");\n+                }\n+            ));\n+            return template.asToken();\n+        }\n+\n+        private TemplateToken generateTestLoopIterationMemorySegmentAtIndex(String[] invarRest) {\n+            var template = Template.make(() -> body(\n+                let(\"type0\", accessType[0]),\n+                let(\"type1\", accessType[1]),\n+                let(\"type0Layout\", accessType[0].layout()),\n+                let(\"type1Layout\", accessType[1].layout()),\n+                switch (accessScenario) {\n+                    case COPY_LOAD_STORE ->\n+                        \/\/ Conversion not implemented, index bound computation is too limited for this currently.\n+                        List.of(\"var v = \",\n+                                \"container_0.getAtIndex(#type0Layout, \", accessIndexForm[0].indexLong(\"invar0_0\", invarRest), \");\\n\",\n+                                \"container_1.setAtIndex(#type1Layout, \", accessIndexForm[1].indexLong(\"invar0_1\", invarRest), \", v);\\n\");\n+                    case FILL_STORE_STORE ->\n+                        List.of(\"container_0.setAtIndex(#type0Layout, \", accessIndexForm[0].indexLong(\"invar0_0\", invarRest), \", (#type0)0x0102030405060708L);\\n\",\n+                                \"container_1.setAtIndex(#type1Layout, \", accessIndexForm[1].indexLong(\"invar0_1\", invarRest), \", (#type1)0x1112131415161718L);\\n\");\n+                }\n+            ));\n+            return template.asToken();\n+        }\n+\n+        private TemplateToken generateTestLoopIterationMemorySegmentLongAdr(String[] invarRest) {\n+            var template = Template.make(() -> body(\n+                let(\"type0\", accessType[0]),\n+                let(\"type1\", accessType[1]),\n+                let(\"type0Layout\", accessType[0].layout()),\n+                let(\"type1Layout\", accessType[1].layout()),\n+                switch (accessScenario) {\n+                    case COPY_LOAD_STORE ->\n+                        \/\/ We allow conversions here.\n+                        List.of(\"#type1 v = (#type1)\",\n+                                \"container_0.get(#type0Layout, \", accessIndexForm[0].indexLong(\"invar0_0\", invarRest), \");\\n\",\n+                                \"container_1.set(#type1Layout, \", accessIndexForm[1].indexLong(\"invar0_1\", invarRest), \", v);\\n\");\n+                    case FILL_STORE_STORE ->\n+                        List.of(\"container_0.set(#type0Layout, \", accessIndexForm[0].indexLong(\"invar0_0\", invarRest), \", (#type0)0x0102030405060708L);\\n\",\n+                                \"container_1.set(#type1Layout, \", accessIndexForm[1].indexLong(\"invar0_1\", invarRest), \", (#type1)0x1112131415161718L);\\n\");\n+                }\n+            ));\n+            return template.asToken();\n+        }\n+    }\n+}\n","filename":"test\/hotspot\/jtreg\/compiler\/loopopts\/superword\/TestAliasingFuzzer.java","additions":1271,"deletions":0,"binary":false,"changes":1271,"status":"added"},{"patch":"@@ -34,0 +34,1 @@\n+import java.util.List;\n@@ -39,1 +40,0 @@\n-    static final int ITER  = 100;\n@@ -76,6 +76,22 @@\n-        TestFramework.runWithFlags(\"-XX:CompileCommand=compileonly,TestCyclicDependency::test*\",\n-                                   \"-XX:+IgnoreUnrecognizedVMOptions\", \"-XX:-AlignVector\", \"-XX:-VerifyAlignVector\");\n-        TestFramework.runWithFlags(\"-XX:CompileCommand=compileonly,TestCyclicDependency::test*\",\n-                                   \"-XX:+IgnoreUnrecognizedVMOptions\", \"-XX:+AlignVector\", \"-XX:-VerifyAlignVector\");\n-        TestFramework.runWithFlags(\"-XX:CompileCommand=compileonly,TestCyclicDependency::test*\",\n-                                   \"-XX:+IgnoreUnrecognizedVMOptions\", \"-XX:+AlignVector\", \"-XX:+VerifyAlignVector\");\n+        \/\/ Cross-product:\n+        \/\/ - AlignVector (+VerifyAlignVector)\n+        \/\/ - UseAutoVectorizationSpeculativeAliasingChecks\n+        List<String[]> avList = List.of(\n+            new String[] {\"-XX:-AlignVector\", \"-XX:-VerifyAlignVector\"},\n+            new String[] {\"-XX:+AlignVector\", \"-XX:-VerifyAlignVector\"},\n+            new String[] {\"-XX:+AlignVector\", \"-XX:+VerifyAlignVector\"}\n+        );\n+        List<String[]> sacList = List.of(\n+            new String[] {\"-XX:-UseAutoVectorizationSpeculativeAliasingChecks\"},\n+            new String[] {\"-XX:+UseAutoVectorizationSpeculativeAliasingChecks\"}\n+        );\n+        for (String[] av : avList) {\n+            for (String[] sac : sacList) {\n+                TestFramework framework = new TestFramework();\n+                framework.addFlags(\"-XX:CompileCommand=compileonly,TestCyclicDependency::test*\",\n+                                   \"-XX:+IgnoreUnrecognizedVMOptions\");\n+                framework.addFlags(av);\n+                framework.addFlags(sac);\n+                framework.start();\n+            }\n+        }\n@@ -137,1 +153,1 @@\n-    @Warmup(100)\n+    @Warmup(1000)\n@@ -148,1 +164,1 @@\n-    @Warmup(100)\n+    @Warmup(1000)\n@@ -159,1 +175,1 @@\n-    @Warmup(100)\n+    @Warmup(1000)\n@@ -170,1 +186,1 @@\n-    @Warmup(100)\n+    @Warmup(1000)\n@@ -181,1 +197,1 @@\n-    @Warmup(100)\n+    @Warmup(1000)\n@@ -192,1 +208,1 @@\n-    @Warmup(100)\n+    @Warmup(1000)\n@@ -203,1 +219,1 @@\n-    @Warmup(100)\n+    @Warmup(1000)\n@@ -214,1 +230,1 @@\n-    @Warmup(100)\n+    @Warmup(1000)\n@@ -225,1 +241,1 @@\n-    @Warmup(100)\n+    @Warmup(1000)\n@@ -236,1 +252,1 @@\n-    @Warmup(100)\n+    @Warmup(1000)\n@@ -247,1 +263,1 @@\n-    @Warmup(100)\n+    @Warmup(1000)\n@@ -260,1 +276,1 @@\n-    @Warmup(100)\n+    @Warmup(1000)\n@@ -271,1 +287,1 @@\n-    @Warmup(100)\n+    @Warmup(1000)\n@@ -282,1 +298,1 @@\n-    @Warmup(100)\n+    @Warmup(1000)\n@@ -295,1 +311,1 @@\n-    @Warmup(100)\n+    @Warmup(1000)\n@@ -306,1 +322,1 @@\n-    @Warmup(100)\n+    @Warmup(1000)\n@@ -433,2 +449,4 @@\n-                  IRNode.ADD_VF, IRNode.VECTOR_SIZE + \"2\", \"> 0\"},\n-        applyIf = {\"AlignVector\", \"false\"},\n+                  IRNode.ADD_VF, IRNode.VECTOR_SIZE + \"2\", \"> 0\",\n+                  \".*multiversion.*\", \"=0\"},\n+        phase = CompilePhase.PRINT_IDEAL,\n+        applyIfAnd = {\"AlignVector\", \"false\", \"UseAutoVectorizationSpeculativeAliasingChecks\", \"false\"},\n@@ -437,1 +455,9 @@\n-                  IRNode.ADD_VF, \"= 0\"},\n+                  IRNode.ADD_VF, IRNode.VECTOR_SIZE + \"min(max_int, max_float)\", \"> 0\",\n+                  \".*multiversion.*\", \"= 0\"},\n+        phase = CompilePhase.PRINT_IDEAL,\n+        applyIfAnd = {\"AlignVector\", \"false\", \"UseAutoVectorizationSpeculativeAliasingChecks\", \"true\"},\n+        applyIfCPUFeatureOr = {\"sse4.1\", \"true\", \"asimd\", \"true\"})\n+    @IR(counts = {IRNode.ADD_VI, \"> 0\",\n+                  IRNode.ADD_VF, \"= 0\",\n+                  \".*multiversion.*\", \"=0\"},\n+        phase = CompilePhase.PRINT_IDEAL,\n@@ -448,4 +474,8 @@\n-            \/\/   AlignVector=false -> vectorizes because we cannot prove store-to-load forwarding\n-            \/\/                        failure. But we can only have 2-element vectors in case\n-            \/\/                        the two float-arrays reference the same array.\n-            \/\/                        Note: at runtime the float-arrays are always different.\n+            \/\/   AlignVector=false\n+            \/\/     UseAutoVectorizationSpeculativeAliasingChecks=false\n+            \/\/       vectorizes because we cannot prove store-to-load forwarding\n+            \/\/       failure. But we can only have 2-element vectors in case\n+            \/\/       the two float-arrays reference the same array.\n+            \/\/     UseAutoVectorizationSpeculativeAliasingChecks=true\n+            \/\/       Speculate that dataF and dataF_2 do not alias -> full vectorization.\n+            \/\/ Note: at runtime the float-arrays are always different -> predicate suffices, no multiversioning.\n@@ -459,2 +489,11 @@\n-                  IRNode.ADD_VF, IRNode.VECTOR_SIZE + \"2\", \"> 0\"},\n-        applyIf = {\"AlignVector\", \"false\"},\n+                  IRNode.ADD_VF, IRNode.VECTOR_SIZE + \"2\", \"> 0\",\n+                  \".*multiversion.*\", \"=0\"},\n+        phase = CompilePhase.PRINT_IDEAL,\n+        applyIfAnd = {\"AlignVector\", \"false\", \"UseAutoVectorizationSpeculativeAliasingChecks\", \"false\"},\n+        applyIfCPUFeatureOr = {\"sse4.1\", \"true\", \"asimd\", \"true\"})\n+    @IR(counts = {IRNode.ADD_VI, \"> 0\",\n+                  IRNode.ADD_VF, IRNode.VECTOR_SIZE + \"min(max_int, max_float)\", \"> 0\",\n+                  IRNode.ADD_VF, IRNode.VECTOR_SIZE + \"2\", \"> 0\",\n+                  \".*multiversion.*\", \"> 0\"},\n+        phase = CompilePhase.PRINT_IDEAL,\n+        applyIfAnd = {\"AlignVector\", \"false\", \"UseAutoVectorizationSpeculativeAliasingChecks\", \"true\"},\n@@ -464,0 +503,1 @@\n+        phase = CompilePhase.PRINT_IDEAL,\n@@ -474,4 +514,9 @@\n-            \/\/   AlignVector=false -> vectorizes because we cannot prove store-to-load forwarding\n-            \/\/                        failure. But we can only have 2-element vectors in case\n-            \/\/                        the two float-arrays reference the same array.\n-            \/\/                        Note: at runtime the float-arrays are always the same.\n+            \/\/   AlignVector=false\n+            \/\/     UseAutoVectorizationSpeculativeAliasingChecks=false\n+            \/\/       vectorizes because we cannot prove store-to-load forwarding\n+            \/\/       failure. But we can only have 2-element vectors in case\n+            \/\/       the two float-arrays reference the same array.\n+            \/\/     UseAutoVectorizationSpeculativeAliasingChecks=true\n+            \/\/       Speculate that dataF and dataF_2 do not alias -> full vectorization.\n+            \/\/       multiversion_slow loop can still vectorize, but only with 2 elements.\n+            \/\/ Note: at runtime the float-arrays are always the same -> predicate fails -> multiversioning.\n@@ -511,2 +556,10 @@\n-                  IRNode.ADD_VF, IRNode.VECTOR_SIZE + \"min(max_int, max_float)\", \"> 0\"},\n-        applyIf = {\"AlignVector\", \"false\"},\n+                  IRNode.ADD_VF, IRNode.VECTOR_SIZE + \"min(max_int, max_float)\", \"> 0\",\n+                  \".*multiversion.*\", \"=0\"},\n+        phase = CompilePhase.PRINT_IDEAL,\n+        applyIfAnd = {\"AlignVector\", \"false\", \"UseAutoVectorizationSpeculativeAliasingChecks\", \"false\"},\n+        applyIfCPUFeatureOr = {\"sse4.1\", \"true\", \"asimd\", \"true\"})\n+    @IR(counts = {IRNode.ADD_VI, \"> 0\",\n+                  IRNode.ADD_VF, IRNode.VECTOR_SIZE + \"min(max_int, max_float)\", \"> 0\",\n+                  \".*multiversion.*\", \"=0\"},\n+        phase = CompilePhase.PRINT_IDEAL,\n+        applyIfAnd = {\"AlignVector\", \"false\", \"UseAutoVectorizationSpeculativeAliasingChecks\", \"true\"},\n@@ -515,1 +568,3 @@\n-                  IRNode.ADD_VF, IRNode.VECTOR_SIZE + \"min(max_int, max_float)\", \"> 0\"},\n+                  IRNode.ADD_VF, IRNode.VECTOR_SIZE + \"min(max_int, max_float)\", \"> 0\",\n+                  \".*multiversion.*\", \"=0\"},\n+        phase = CompilePhase.PRINT_IDEAL,\n@@ -523,4 +578,8 @@\n-            \/\/   AlignVector=false -> vectorizes because we cannot prove store-to-load forwarding\n-            \/\/                        failure. But we can only have 2-element vectors in case\n-            \/\/                        the two float-arrays reference the same array.\n-            \/\/                        Note: at runtime the float-arrays are always different.\n+            \/\/   AlignVector=false\n+            \/\/     UseAutoVectorizationSpeculativeAliasingChecks=false\n+            \/\/       vectorizes because we cannot prove store-to-load forwarding\n+            \/\/       failure. But we can only have 2-element vectors in case\n+            \/\/       the two int-arrays reference the same array.\n+            \/\/     UseAutoVectorizationSpeculativeAliasingChecks=true\n+            \/\/       Speculate that dataI and dataI_2 do not alias -> full vectorization.\n+            \/\/ Note: at runtime the int-arrays are always different -> predicate suffices, no multiversioning.\n@@ -537,2 +596,11 @@\n-                  IRNode.ADD_VF, IRNode.VECTOR_SIZE + \"min(max_int, max_float)\", \"> 0\"},\n-        applyIf = {\"AlignVector\", \"false\"},\n+                  IRNode.ADD_VF, IRNode.VECTOR_SIZE + \"min(max_int, max_float)\", \"> 0\",\n+                  \".*multiversion.*\", \"=0\"},\n+        phase = CompilePhase.PRINT_IDEAL,\n+        applyIfAnd = {\"AlignVector\", \"false\", \"UseAutoVectorizationSpeculativeAliasingChecks\", \"false\"},\n+        applyIfCPUFeatureOr = {\"sse4.1\", \"true\", \"asimd\", \"true\"})\n+    @IR(counts = {IRNode.ADD_VI, \"> 0\",\n+                  IRNode.ADD_VI, IRNode.VECTOR_SIZE + \"2\", \"> 0\",\n+                  IRNode.ADD_VF, IRNode.VECTOR_SIZE + \"min(max_int, max_float)\", \"> 0\",\n+                  \".*multiversion.*\", \">0\"},\n+        phase = CompilePhase.PRINT_IDEAL,\n+        applyIfAnd = {\"AlignVector\", \"false\", \"UseAutoVectorizationSpeculativeAliasingChecks\", \"true\"},\n@@ -542,0 +610,1 @@\n+        phase = CompilePhase.PRINT_IDEAL,\n@@ -549,4 +618,9 @@\n-            \/\/   AlignVector=false -> vectorizes because we cannot prove store-to-load forwarding\n-            \/\/                        failure. But we can only have 2-element vectors in case\n-            \/\/                        the two float-arrays reference the same array.\n-            \/\/                        Note: at runtime the float-arrays are always the same.\n+            \/\/   AlignVector=false\n+            \/\/     UseAutoVectorizationSpeculativeAliasingChecks=false\n+            \/\/       vectorizes because we cannot prove store-to-load forwarding\n+            \/\/       failure. But we can only have 2-element vectors in case\n+            \/\/       the two int-arrays reference the same array.\n+            \/\/     UseAutoVectorizationSpeculativeAliasingChecks=true\n+            \/\/       Speculate that dataF and dataF_2 do not alias -> full vectorization.\n+            \/\/       multiversion_slow loop can still vectorize, but only with 2 elements.\n+            \/\/ Note: at runtime the int-arrays are always the same -> predicate fails -> multiversioning.\n","filename":"test\/hotspot\/jtreg\/compiler\/loopopts\/superword\/TestCyclicDependency.java","additions":123,"deletions":49,"binary":false,"changes":172,"status":"modified"},{"patch":"@@ -754,0 +754,2 @@\n+                \/\/ If we have two array references, then we can speculate that they do not alias, and\n+                \/\/ still produce full vectorization.\n@@ -758,1 +760,1 @@\n-                if (0 < byteOffset && byteOffset < maxVectorWidth) {\n+                if (isSingleArray && 0 < byteOffset && byteOffset < maxVectorWidth) {\n","filename":"test\/hotspot\/jtreg\/compiler\/loopopts\/superword\/TestDependencyOffsets.java","additions":3,"deletions":1,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -50,0 +50,16 @@\n+\/*\n+ * @test id=byte-array-NoSpeculativeAliasingCheck\n+ * @bug 8329273 8348263 8324751\n+ * @summary Test vectorization of loops over MemorySegment\n+ * @library \/test\/lib \/\n+ * @run driver compiler.loopopts.superword.TestMemorySegment ByteArray NoSpeculativeAliasingCheck\n+ *\/\n+\n+\/*\n+ * @test id=byte-array-AlignVector-NoSpeculativeAliasingCheck\n+ * @bug 8329273 8348263 8324751\n+ * @summary Test vectorization of loops over MemorySegment\n+ * @library \/test\/lib \/\n+ * @run driver compiler.loopopts.superword.TestMemorySegment ByteArray AlignVector NoSpeculativeAliasingCheck\n+ *\/\n+\n@@ -66,1 +82,0 @@\n-\n@@ -196,0 +211,1 @@\n+                case \"NoSpeculativeAliasingCheck\" -> framework.addFlags(\"-XX:-UseAutoVectorizationSpeculativeAliasingChecks\");\n@@ -197,0 +213,1 @@\n+                default ->                           throw new RuntimeException(\"Bad tag: \" + tag);\n@@ -199,3 +216,0 @@\n-        if (args.length > 1 && args[1].equals(\"AlignVector\")) {\n-            framework.addFlags(\"-XX:+AlignVector\");\n-        }\n@@ -804,1 +818,2 @@\n-        applyIfAnd = { \"ShortRunningLongLoop\", \"false\", \"AlignVector\", \"false\" },\n+        applyIfAnd = {\"UseAutoVectorizationSpeculativeAliasingChecks\", \"false\",\n+                      \"ShortRunningLongLoop\", \"false\"},\n@@ -807,0 +822,2 @@\n+    \/\/ FAILS: invariants are sorted differently, because of differently inserted Cast.\n+    \/\/ See: JDK-8331659\n@@ -809,2 +826,6 @@\n-                  IRNode.STORE_VECTOR,  \"> 0\"},\n-        applyIfAnd = { \"ShortRunningLongLoop\", \"true\", \"AlignVector\", \"false\" },\n+                  IRNode.STORE_VECTOR,  \"> 0\",\n+                  \".*multiversion.*\", \"> 0\"},\n+        phase = CompilePhase.PRINT_IDEAL,\n+        applyIfAnd = {\"UseAutoVectorizationSpeculativeAliasingChecks\", \"true\",\n+                      \"ShortRunningLongLoop\", \"false\",\n+                      \"AlignVector\", \"false\"},\n@@ -813,2 +834,13 @@\n-    \/\/ FAILS: invariants are sorted differently, because of differently inserted Cast.\n-    \/\/ See: JDK-8331659\n+    \/\/ After JDK-8324751, we now insert a aliasing runtime check, but it will always fail, which is a suboptimal.\n+    @IR(counts = {IRNode.LOAD_VECTOR_I, \"> 0\",\n+                  IRNode.ADD_VI,        \"> 0\",\n+                  IRNode.STORE_VECTOR,  \"> 0\",\n+                  \".*multiversion.*\", \"= 0\"},\n+        phase = CompilePhase.PRINT_IDEAL,\n+        applyIfAnd = {\"ShortRunningLongLoop\", \"true\",\n+                      \"AlignVector\", \"false\"},\n+        applyIfPlatform = {\"64-bit\", \"true\"},\n+        applyIfCPUFeatureOr = {\"sse4.1\", \"true\", \"asimd\", \"true\", \"rvv\", \"true\"})\n+    \/\/ If we don't create the loop nest for the long loop, then the issue with different Casts\n+    \/\/ seems to disappear. We also don't need multiversioning because the pointers are seen\n+    \/\/ as identical.\n@@ -825,2 +857,2 @@\n-    @Test\n-    @IR(counts = {IRNode.LOAD_VECTOR_I, \"= 0\",\n+   @Test\n+   @IR(counts = {IRNode.LOAD_VECTOR_I, \"= 0\",\n@@ -829,1 +861,14 @@\n-        applyIfAnd = { \"ShortRunningLongLoop\", \"false\", \"AlignVector\", \"false\" },\n+        applyIfAnd = {\"UseAutoVectorizationSpeculativeAliasingChecks\", \"false\",\n+                      \"ShortRunningLongLoop\", \"false\"},\n+        applyIfPlatform = {\"64-bit\", \"true\"},\n+        applyIfCPUFeatureOr = {\"sse4.1\", \"true\", \"asimd\", \"true\", \"rvv\", \"true\"})\n+    \/\/ FAILS: invariants are sorted differently, because of differently inserted Cast.\n+    \/\/ See: JDK-8331659\n+    @IR(counts = {IRNode.LOAD_VECTOR_I, \"> 0\",\n+                  IRNode.ADD_VI,        \"> 0\",\n+                  IRNode.STORE_VECTOR,  \"> 0\",\n+                  \".*multiversion.*\", \"> 0\"},\n+        phase = CompilePhase.PRINT_IDEAL,\n+        applyIfAnd = {\"UseAutoVectorizationSpeculativeAliasingChecks\", \"true\",\n+                      \"ShortRunningLongLoop\", \"false\",\n+                      \"AlignVector\", \"false\"},\n@@ -832,0 +877,1 @@\n+    \/\/ After JDK-8324751, we now insert a aliasing runtime check, but it will always fail, which is a suboptimal.\n@@ -834,2 +880,5 @@\n-                  IRNode.STORE_VECTOR,  \"> 0\"},\n-        applyIfAnd = { \"ShortRunningLongLoop\", \"true\", \"AlignVector\", \"false\" },\n+                  IRNode.STORE_VECTOR,  \"> 0\",\n+                  \".*multiversion.*\", \"= 0\"},\n+        phase = CompilePhase.PRINT_IDEAL,\n+        applyIfAnd = {\"ShortRunningLongLoop\", \"true\",\n+                      \"AlignVector\", \"false\"},\n@@ -838,2 +887,3 @@\n-    \/\/ FAILS: invariants are sorted differently, because of differently inserted Cast.\n-    \/\/ See: JDK-8331659\n+    \/\/ If we don't create the loop nest for the long loop, then the issue with different Casts\n+    \/\/ seems to disappear. We also don't need multiversioning because the pointers are seen\n+    \/\/ as identical.\n","filename":"test\/hotspot\/jtreg\/compiler\/loopopts\/superword\/TestMemorySegment.java","additions":66,"deletions":16,"binary":false,"changes":82,"status":"modified"},{"patch":"@@ -0,0 +1,854 @@\n+\/*\n+ * Copyright (c) 2025, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+package compiler.loopopts.superword;\n+\n+import compiler.lib.ir_framework.*;\n+import compiler.lib.verify.*;\n+import jdk.test.lib.Utils;\n+import java.nio.ByteBuffer;\n+import java.util.Map;\n+import java.util.HashMap;\n+import java.util.Random;\n+import java.lang.foreign.*;\n+\n+\/*\n+ * @test id=byte-array\n+ * @bug 8324751\n+ * @summary Test vectorization of loops over MemorySegment\n+ * @library \/test\/lib \/\n+ * @run driver compiler.loopopts.superword.TestMemorySegmentAliasing ByteArray\n+ *\/\n+\n+\/*\n+ * @test id=byte-array-AlignVector\n+ * @bug 8324751\n+ * @summary Test vectorization of loops over MemorySegment\n+ * @library \/test\/lib \/\n+ * @run driver compiler.loopopts.superword.TestMemorySegmentAliasing ByteArray AlignVector\n+ *\/\n+\n+\/*\n+ * @test id=byte-array-NoSpeculativeAliasingCheck\n+ * @bug 8324751\n+ * @summary Test vectorization of loops over MemorySegment\n+ * @library \/test\/lib \/\n+ * @run driver compiler.loopopts.superword.TestMemorySegmentAliasing ByteArray NoSpeculativeAliasingCheck\n+ *\/\n+\n+\/*\n+ * @test id=byte-array-AlignVector-NoSpeculativeAliasingCheck\n+ * @bug 8324751\n+ * @summary Test vectorization of loops over MemorySegment\n+ * @library \/test\/lib \/\n+ * @run driver compiler.loopopts.superword.TestMemorySegmentAliasing ByteArray AlignVector NoSpeculativeAliasingCheck\n+ *\/\n+\n+\/*\n+ * @test id=byte-array-NoAutoAlignment\n+ * @bug 8324751\n+ * @summary Test vectorization of loops over MemorySegment\n+ * @library \/test\/lib \/\n+ * @run driver compiler.loopopts.superword.TestMemorySegmentAliasing ByteArray NoAutoAlignment\n+ *\/\n+\n+\/*\n+ * @test id=char-array\n+ * @bug 8324751\n+ * @summary Test vectorization of loops over MemorySegment\n+ * @library \/test\/lib \/\n+ * @run driver compiler.loopopts.superword.TestMemorySegmentAliasing CharArray\n+ *\/\n+\n+\/*\n+ * @test id=short-array\n+ * @bug 8324751\n+ * @summary Test vectorization of loops over MemorySegment\n+ * @library \/test\/lib \/\n+ * @run driver compiler.loopopts.superword.TestMemorySegmentAliasing ShortArray\n+ *\/\n+\n+\/*\n+ * @test id=int-array\n+ * @bug 8324751\n+ * @summary Test vectorization of loops over MemorySegment\n+ * @library \/test\/lib \/\n+ * @run driver compiler.loopopts.superword.TestMemorySegmentAliasing IntArray\n+ *\/\n+\n+\/*\n+ * @test id=int-array-AlignVector\n+ * @bug 8324751\n+ * @summary Test vectorization of loops over MemorySegment\n+ * @library \/test\/lib \/\n+ * @run driver compiler.loopopts.superword.TestMemorySegmentAliasing IntArray AlignVector\n+ *\/\n+\n+\/*\n+ * @test id=int-array-NoSpeculativeAliasingCheck\n+ * @bug 8324751\n+ * @summary Test vectorization of loops over MemorySegment\n+ * @library \/test\/lib \/\n+ * @run driver compiler.loopopts.superword.TestMemorySegmentAliasing IntArray NoSpeculativeAliasingCheck\n+ *\/\n+\n+\/*\n+ * @test id=int-array-AlignVector-NoSpeculativeAliasingCheck\n+ * @bug 8324751\n+ * @summary Test vectorization of loops over MemorySegment\n+ * @library \/test\/lib \/\n+ * @run driver compiler.loopopts.superword.TestMemorySegmentAliasing IntArray AlignVector NoSpeculativeAliasingCheck\n+ *\/\n+\n+\/*\n+ * @test id=int-array-NoAutoAlignment\n+ * @bug 8324751\n+ * @summary Test vectorization of loops over MemorySegment\n+ * @library \/test\/lib \/\n+ * @run driver compiler.loopopts.superword.TestMemorySegmentAliasing IntArray NoAutoAlignment\n+ *\/\n+\n+\/*\n+ * @test id=long-array\n+ * @bug 8324751\n+ * @summary Test vectorization of loops over MemorySegment\n+ * @library \/test\/lib \/\n+ * @run driver compiler.loopopts.superword.TestMemorySegmentAliasing LongArray\n+ *\/\n+\n+\/*\n+ * @test id=long-array-AlignVector\n+ * @bug 8324751\n+ * @summary Test vectorization of loops over MemorySegment\n+ * @library \/test\/lib \/\n+ * @run driver compiler.loopopts.superword.TestMemorySegmentAliasing LongArray AlignVector\n+ *\/\n+\n+\/*\n+ * @test id=long-array-NoSpeculativeAliasingCheck\n+ * @bug 8324751\n+ * @summary Test vectorization of loops over MemorySegment\n+ * @library \/test\/lib \/\n+ * @run driver compiler.loopopts.superword.TestMemorySegmentAliasing LongArray NoSpeculativeAliasingCheck\n+ *\/\n+\n+\/*\n+ * @test id=long-array-AlignVector-NoSpeculativeAliasingCheck\n+ * @bug 8324751\n+ * @summary Test vectorization of loops over MemorySegment\n+ * @library \/test\/lib \/\n+ * @run driver compiler.loopopts.superword.TestMemorySegmentAliasing LongArray AlignVector NoSpeculativeAliasingCheck\n+ *\/\n+\n+\/*\n+ * @test id=float-array\n+ * @bug 8324751\n+ * @summary Test vectorization of loops over MemorySegment\n+ * @library \/test\/lib \/\n+ * @run driver compiler.loopopts.superword.TestMemorySegmentAliasing FloatArray\n+ *\/\n+\n+\/*\n+ * @test id=double-array\n+ * @bug 8324751\n+ * @summary Test vectorization of loops over MemorySegment\n+ * @library \/test\/lib \/\n+ * @run driver compiler.loopopts.superword.TestMemorySegmentAliasing DoubleArray\n+ *\/\n+\n+\/*\n+ * @test id=byte-buffer\n+ * @bug 8324751\n+ * @summary Test vectorization of loops over MemorySegment\n+ * @library \/test\/lib \/\n+ * @run driver compiler.loopopts.superword.TestMemorySegmentAliasing ByteBuffer\n+ *\/\n+\n+\/*\n+ * @test id=byte-buffer-direct\n+ * @bug 8324751\n+ * @summary Test vectorization of loops over MemorySegment\n+ * @library \/test\/lib \/\n+ * @run driver compiler.loopopts.superword.TestMemorySegmentAliasing ByteBufferDirect\n+ *\/\n+\n+\/*\n+ * @test id=native\n+ * @bug 8324751\n+ * @summary Test vectorization of loops over MemorySegment\n+ * @library \/test\/lib \/\n+ * @run driver compiler.loopopts.superword.TestMemorySegmentAliasing Native\n+ *\/\n+\n+\/*\n+ * @test id=native-AlignVector\n+ * @bug 8324751\n+ * @summary Test vectorization of loops over MemorySegment\n+ * @library \/test\/lib \/\n+ * @run driver compiler.loopopts.superword.TestMemorySegmentAliasing Native AlignVector\n+ *\/\n+\n+\/*\n+ * @test id=native-NoSpeculativeAliasingCheck\n+ * @bug 8324751\n+ * @summary Test vectorization of loops over MemorySegment\n+ * @library \/test\/lib \/\n+ * @run driver compiler.loopopts.superword.TestMemorySegmentAliasing Native NoSpeculativeAliasingCheck\n+ *\/\n+\n+\/*\n+ * @test id=native-AlignVector-NoSpeculativeAliasingCheck\n+ * @bug 8324751\n+ * @summary Test vectorization of loops over MemorySegment\n+ * @library \/test\/lib \/\n+ * @run driver compiler.loopopts.superword.TestMemorySegmentAliasing Native AlignVector NoSpeculativeAliasingCheck\n+ *\/\n+\n+\/*\n+ * @test id=native-NoAutoAlignment\n+ * @bug 8324751\n+ * @summary Test vectorization of loops over MemorySegment\n+ * @library \/test\/lib \/\n+ * @run driver compiler.loopopts.superword.TestMemorySegmentAliasing Native NoAutoAlignment\n+ *\/\n+\n+public class TestMemorySegmentAliasing {\n+    public static void main(String[] args) {\n+        TestFramework framework = new TestFramework(TestMemorySegmentAliasingImpl.class);\n+        framework.addFlags(\"-DmemorySegmentProviderNameForTestVM=\" + args[0]);\n+        for (int i = 1; i < args.length; i++) {\n+            String tag = args[i];\n+            switch (tag) {\n+                case \"AlignVector\" ->                framework.addFlags(\"-XX:+AlignVector\");\n+                case \"NoSpeculativeAliasingCheck\" -> framework.addFlags(\"-XX:-UseAutoVectorizationSpeculativeAliasingChecks\");\n+                \/\/ automatic alignment has an impact on where the main-loop starts, and that affects init and limit\n+                \/\/ of the main loop.\n+                case \"NoAutoAlignment\" ->            framework.addFlags(\"-XX:SuperWordAutomaticAlignment=0\");\n+                default ->                           throw new RuntimeException(\"Bad tag: \" + tag);\n+            }\n+        }\n+        framework.setDefaultWarmup(100);\n+        framework.start();\n+    }\n+}\n+\n+class TestMemorySegmentAliasingImpl {\n+    static final int BACKING_SIZE = 1024 * 8;\n+    static final Random RANDOM = Utils.getRandomInstance();\n+\n+\n+    interface TestFunction {\n+        void run();\n+    }\n+\n+    interface MemorySegmentProvider {\n+        MemorySegment newMemorySegment();\n+    }\n+\n+    public static MemorySegmentProvider provider;\n+\n+    static {\n+        String providerName = System.getProperty(\"memorySegmentProviderNameForTestVM\");\n+        provider = switch (providerName) {\n+            case \"ByteArray\"        -> TestMemorySegmentAliasingImpl::newMemorySegmentOfByteArray;\n+            case \"CharArray\"        -> TestMemorySegmentAliasingImpl::newMemorySegmentOfCharArray;\n+            case \"ShortArray\"       -> TestMemorySegmentAliasingImpl::newMemorySegmentOfShortArray;\n+            case \"IntArray\"         -> TestMemorySegmentAliasingImpl::newMemorySegmentOfIntArray;\n+            case \"LongArray\"        -> TestMemorySegmentAliasingImpl::newMemorySegmentOfLongArray;\n+            case \"FloatArray\"       -> TestMemorySegmentAliasingImpl::newMemorySegmentOfFloatArray;\n+            case \"DoubleArray\"      -> TestMemorySegmentAliasingImpl::newMemorySegmentOfDoubleArray;\n+            case \"ByteBuffer\"       -> TestMemorySegmentAliasingImpl::newMemorySegmentOfByteBuffer;\n+            case \"ByteBufferDirect\" -> TestMemorySegmentAliasingImpl::newMemorySegmentOfByteBufferDirect;\n+            case \"Native\"           -> TestMemorySegmentAliasingImpl::newMemorySegmentOfNative;\n+            case \"MixedArray\"       -> TestMemorySegmentAliasingImpl::newMemorySegmentOfMixedArray;\n+            case \"MixedBuffer\"      -> TestMemorySegmentAliasingImpl::newMemorySegmentOfMixedBuffer;\n+            case \"Mixed\"            -> TestMemorySegmentAliasingImpl::newMemorySegmentOfMixed;\n+            default -> throw new RuntimeException(\"Test argument not recognized: \" + providerName);\n+        };\n+    }\n+\n+    \/\/ Map of goldTests\n+    public static Map<String, TestFunction> goldTests = new HashMap<>();\n+\n+    \/\/ Map of gold for the goldTests, the results from the first run before compilation\n+    public static Map<String, Object> golds = new HashMap<>();\n+\n+    \/\/ Map of referenceTests, i.e. tests that have a reference implementation that is run with the interpreter.\n+    \/\/ The TestFunction must run both the test and reference methods.\n+    public static Map<String, TestFunction> referenceTests = new HashMap<>();\n+\n+    \/\/ Original data.\n+    public static MemorySegment ORIG_A = fillRandom(newMemorySegment());\n+    public static MemorySegment ORIG_B = fillRandom(newMemorySegment());\n+    public static MemorySegment ORIG_C = fillRandom(newMemorySegment());\n+\n+    \/\/ The data we use in the tests. It is initialized from ORIG_* every time.\n+    public static MemorySegment A = newMemorySegment();\n+    public static MemorySegment B = newMemorySegment();\n+    public static MemorySegment C = newMemorySegment();\n+\n+    \/\/ Parallel to data above, but for use in reference methods.\n+    public static MemorySegment A_REFERENCE = newMemorySegment();\n+    public static MemorySegment B_REFERENCE = newMemorySegment();\n+    public static MemorySegment C_REFERENCE = newMemorySegment();\n+\n+    public TestMemorySegmentAliasingImpl () {\n+        \/\/ Add all goldTests to list\n+        goldTests.put(\"test_byte_incr_noaliasing\",     () -> test_byte_incr_noaliasing(A, B));\n+        goldTests.put(\"test_byte_incr_aliasing\",       () -> test_byte_incr_aliasing(A, A));\n+        goldTests.put(\"test_byte_incr_aliasing_fwd3\",  () -> {\n+            MemorySegment x = A.asSlice(0, BACKING_SIZE - 3);\n+            MemorySegment y = A.asSlice(3, BACKING_SIZE - 3);\n+            test_byte_incr_aliasing_fwd3(x, y);\n+        });\n+        goldTests.put(\"test_byte_incr_noaliasing_fwd128\",  () -> {\n+            MemorySegment x = A.asSlice(0,   BACKING_SIZE - 128);\n+            MemorySegment y = A.asSlice(120, BACKING_SIZE - 128);\n+            test_byte_incr_noaliasing_fwd128(x, y);\n+        });\n+\n+        goldTests.put(\"test_int_to_long_noaliasing\",   () -> test_int_to_long_noaliasing(A, B));\n+\n+        \/\/ Compute gold value for all test methods before compilation\n+        for (Map.Entry<String,TestFunction> entry : goldTests.entrySet()) {\n+            String name = entry.getKey();\n+            TestFunction test = entry.getValue();\n+            init();\n+            test.run();\n+            Object gold = snapshotCopy();\n+            golds.put(name, gold);\n+        }\n+\n+        referenceTests.put(\"test_fill_byte_sameMS_alias\", () -> {\n+            int invar1 = RANDOM.nextInt(64);\n+            int invar2 = RANDOM.nextInt(64);\n+            test_fill_byte_sameMS_alias(A, A, invar1, invar2);\n+            reference_fill_byte_sameMS_alias(A_REFERENCE, A_REFERENCE, invar1, invar2);\n+        });\n+        referenceTests.put(\"test_fill_byte_sameMS_noalias\", () -> {\n+            \/\/ The accesses either start at the middle and go out,\n+            \/\/ or start from opposite sides and meet in the middle.\n+            \/\/ But they never overlap.\n+            \/\/      <------|------>\n+            \/\/      ------>|<------\n+            \/\/\n+            \/\/ This tests that the checks we emit are not too relaxed.\n+            int middle = BACKING_SIZE \/ 2 + RANDOM.nextInt(-256, 256);\n+            int limit = BACKING_SIZE \/ 3 + RANDOM.nextInt(256);\n+            int invar1 = middle;\n+            int invar2 = middle;\n+            if (RANDOM.nextBoolean()) {\n+                invar1 -= limit;\n+                invar2 += limit;\n+            }\n+            test_fill_byte_sameMS_noalias(A, A, invar1, invar2, limit);\n+            reference_fill_byte_sameMS_noalias(A_REFERENCE, A_REFERENCE, invar1, invar2, limit);\n+        });\n+        referenceTests.put(\"test_fill_byte_sameMS_maybeAlias\", () -> {\n+            \/\/ The accesses either start at the middle and go out,\n+            \/\/ or start from opposite sides and meet in the middle.\n+            \/\/ In the middle, sometimes we overlap and sometimes not.\n+            \/\/      <------|------>\n+            \/\/      ------>|<------\n+            \/\/\n+            \/\/ This tests that the checks we emit are not too relaxed.\n+            int middle = BACKING_SIZE \/ 2 + RANDOM.nextInt(-256, 256);\n+            int limit = BACKING_SIZE \/ 3 + RANDOM.nextInt(256);\n+            int invar1 = middle + RANDOM.nextInt(-256, 256);\n+            int invar2 = middle + RANDOM.nextInt(-256, 256);\n+            \/\/ Are the bounds safe? Assume extreme values:\n+            \/\/ invar1 = 8k\/2 + 256 + 256\n+            \/\/ limit = 8k\/3 + 256\n+            \/\/ invar1 + limit = 8k * 5\/6 + 3 * 256\n+            \/\/                = 8k * 5\/6 + 3\/4 * 1k = 7.41k < 8k\n+            if (RANDOM.nextBoolean()) {\n+                invar1 -= limit;\n+                invar2 += limit;\n+            }\n+            test_fill_byte_sameMS_maybeAlias(A, A, invar1, invar2, limit);\n+            reference_fill_byte_sameMS_maybeAlias(A_REFERENCE, A_REFERENCE, invar1, invar2, limit);\n+        });\n+        referenceTests.put(\"test_fill_int_sameMS_alias\", () -> {\n+            int invar1 = RANDOM.nextInt(64);\n+            int invar2 = RANDOM.nextInt(64);\n+            test_fill_int_sameMS_alias(A, A, invar1, invar2);\n+            reference_fill_int_sameMS_alias(A_REFERENCE, A_REFERENCE, invar1, invar2);\n+        });\n+        referenceTests.put(\"test_fill_int_sameMS_noalias\", () -> {\n+            \/\/ The accesses either start at the middle and go out,\n+            \/\/ or start from opposite sides and meet in the middle.\n+            \/\/ But they never overlap.\n+            \/\/      <------|------>\n+            \/\/      ------>|<------\n+            \/\/\n+            \/\/ This tests that the checks we emit are not too relaxed.\n+            int middle = BACKING_SIZE \/ 2 + RANDOM.nextInt(-256, 256);\n+            int limit = BACKING_SIZE \/ 3 + RANDOM.nextInt(256);\n+            int invar1 = middle;\n+            int invar2 = middle;\n+            if (RANDOM.nextBoolean()) {\n+                invar1 -= limit;\n+                invar2 += limit;\n+            }\n+            test_fill_int_sameMS_noalias(A, A, invar1, invar2, limit);\n+            reference_fill_int_sameMS_noalias(A_REFERENCE, A_REFERENCE, invar1, invar2, limit);\n+        });\n+        referenceTests.put(\"test_fill_int_sameMS_maybeAlias\", () -> {\n+            \/\/ The accesses either start at the middle and go out,\n+            \/\/ or start from opposite sides and meet in the middle.\n+            \/\/ In the middle, sometimes we overlap and sometimes not.\n+            \/\/      <------|------>\n+            \/\/      ------>|<------\n+            \/\/\n+            \/\/ This tests that the checks we emit are not too relaxed.\n+            int middle = BACKING_SIZE \/ 2 + RANDOM.nextInt(-256, 256);\n+            int limit = BACKING_SIZE \/ 3 + RANDOM.nextInt(256);\n+            int invar1 = middle + RANDOM.nextInt(-256, 256);\n+            int invar2 = middle + RANDOM.nextInt(-256, 256);\n+            \/\/ Are the bounds safe? Assume extreme values:\n+            \/\/ invar1 = 8k\/2 + 256 + 256\n+            \/\/ limit = 8k\/3 + 256\n+            \/\/ invar1 + limit = 8k * 5\/6 + 3 * 256\n+            \/\/                = 8k * 5\/6 + 3\/4 * 1k = 7.41k < 8k\n+            if (RANDOM.nextBoolean()) {\n+                invar1 -= limit;\n+                invar2 += limit;\n+            }\n+            test_fill_int_sameMS_maybeAlias(A, A, invar1, invar2, limit);\n+            reference_fill_int_sameMS_maybeAlias(A_REFERENCE, A_REFERENCE, invar1, invar2, limit);\n+        });\n+    }\n+\n+    static MemorySegment newMemorySegment() {\n+        return provider.newMemorySegment();\n+    }\n+\n+    static MemorySegment copy(MemorySegment src) {\n+        MemorySegment dst = newMemorySegment();\n+        dst.copyFrom(src);\n+        return dst;\n+    }\n+\n+    static MemorySegment newMemorySegmentOfByteArray() {\n+        return MemorySegment.ofArray(new byte[BACKING_SIZE]);\n+    }\n+\n+    static MemorySegment newMemorySegmentOfCharArray() {\n+        return MemorySegment.ofArray(new char[BACKING_SIZE \/ 2]);\n+    }\n+\n+    static MemorySegment newMemorySegmentOfShortArray() {\n+        return MemorySegment.ofArray(new short[BACKING_SIZE \/ 2]);\n+    }\n+\n+    static MemorySegment newMemorySegmentOfIntArray() {\n+        return MemorySegment.ofArray(new int[BACKING_SIZE \/ 4]);\n+    }\n+\n+    static MemorySegment newMemorySegmentOfLongArray() {\n+        return MemorySegment.ofArray(new long[BACKING_SIZE \/ 8]);\n+    }\n+\n+    static MemorySegment newMemorySegmentOfFloatArray() {\n+        return MemorySegment.ofArray(new float[BACKING_SIZE \/ 4]);\n+    }\n+\n+    static MemorySegment newMemorySegmentOfDoubleArray() {\n+        return MemorySegment.ofArray(new double[BACKING_SIZE \/ 8]);\n+    }\n+\n+    static MemorySegment newMemorySegmentOfByteBuffer() {\n+        return MemorySegment.ofBuffer(ByteBuffer.allocate(BACKING_SIZE));\n+    }\n+\n+    static MemorySegment newMemorySegmentOfByteBufferDirect() {\n+        return MemorySegment.ofBuffer(ByteBuffer.allocateDirect(BACKING_SIZE));\n+    }\n+\n+    static MemorySegment newMemorySegmentOfNative() {\n+        \/\/ Auto arena: GC decides when there is no reference to the MemorySegment,\n+        \/\/ and then it deallocates the backing memory.\n+        return Arena.ofAuto().allocate(BACKING_SIZE, 1);\n+    }\n+\n+    static MemorySegment newMemorySegmentOfMixedArray() {\n+        switch(RANDOM.nextInt(7)) {\n+            case 0  -> { return newMemorySegmentOfByteArray(); }\n+            case 1  -> { return newMemorySegmentOfCharArray(); }\n+            case 2  -> { return newMemorySegmentOfShortArray(); }\n+            case 3  -> { return newMemorySegmentOfIntArray(); }\n+            case 4  -> { return newMemorySegmentOfLongArray(); }\n+            case 5  -> { return newMemorySegmentOfFloatArray(); }\n+            default -> { return newMemorySegmentOfDoubleArray(); }\n+        }\n+    }\n+\n+    static MemorySegment newMemorySegmentOfMixedBuffer() {\n+        switch (RANDOM.nextInt(2)) {\n+            case 0  -> { return newMemorySegmentOfByteBuffer(); }\n+            default -> { return newMemorySegmentOfByteBufferDirect(); }\n+        }\n+    }\n+\n+    static MemorySegment newMemorySegmentOfMixed() {\n+        switch (RANDOM.nextInt(3)) {\n+            case 0  -> { return newMemorySegmentOfMixedArray(); }\n+            case 1  -> { return newMemorySegmentOfMixedBuffer(); }\n+            default -> { return newMemorySegmentOfNative(); }\n+        }\n+    }\n+\n+    static MemorySegment fillRandom(MemorySegment data) {\n+        for (int i = 0; i < (int)data.byteSize(); i += 8) {\n+            data.set(ValueLayout.JAVA_LONG_UNALIGNED, i, RANDOM.nextLong());\n+        }\n+        return data;\n+    }\n+\n+    public static void init() {\n+        A.copyFrom(ORIG_A);\n+        B.copyFrom(ORIG_B);\n+        C.copyFrom(ORIG_C);\n+    }\n+\n+    public static void initReference() {\n+        A_REFERENCE.copyFrom(ORIG_A);\n+        B_REFERENCE.copyFrom(ORIG_B);\n+        C_REFERENCE.copyFrom(ORIG_C);\n+    }\n+\n+    public static Object snapshotCopy() {\n+        return new Object[]{copy(A), copy(B), copy(C)};\n+    }\n+\n+    public static Object snapshot() {\n+        return new Object[]{A, B, C};\n+    }\n+\n+    public static Object snapshotReference() {\n+        return new Object[]{A_REFERENCE, B_REFERENCE, C_REFERENCE};\n+    }\n+\n+    @Run(test = {\"test_byte_incr_noaliasing\",\n+                 \"test_byte_incr_aliasing\",\n+                 \"test_byte_incr_aliasing_fwd3\",\n+                 \"test_byte_incr_noaliasing_fwd128\",\n+                 \"test_int_to_long_noaliasing\",\n+                 \"test_fill_byte_sameMS_alias\",\n+                 \"test_fill_byte_sameMS_noalias\",\n+                 \"test_fill_byte_sameMS_maybeAlias\",\n+                 \"test_fill_int_sameMS_alias\",\n+                 \"test_fill_int_sameMS_noalias\",\n+                 \"test_fill_int_sameMS_maybeAlias\"})\n+    void runTests(RunInfo info) {\n+        for (Map.Entry<String,TestFunction> entry : goldTests.entrySet()) {\n+            String name = entry.getKey();\n+            TestFunction test = entry.getValue();\n+            \/\/ Recall gold value from before compilation\n+            Object gold = golds.get(name);\n+            \/\/ Compute new result\n+            init();\n+            test.run();\n+            Object result = snapshot();\n+            \/\/ Compare gold and new result\n+            try {\n+                Verify.checkEQ(gold, result);\n+            } catch (VerifyException e) {\n+                throw new RuntimeException(\"Verify failed for \" + name, e);\n+            }\n+        }\n+\n+        \/\/ Once warmup is over (100x), repeat 10x to get reasonable coverage of the\n+        \/\/ randomness in the tests.\n+        int reps = info.isWarmUp() ? 10 : 1;\n+        for (int r = 0; r < reps; r++) {\n+            for (Map.Entry<String,TestFunction> entry : referenceTests.entrySet()) {\n+                String name = entry.getKey();\n+                TestFunction test = entry.getValue();\n+                \/\/ Init data for test and reference\n+                init();\n+                initReference();\n+                \/\/ Run test and reference\n+                test.run();\n+                \/\/ Capture results from test and reference\n+                Object result = snapshot();\n+                Object expected = snapshotReference();\n+                \/\/ Compare expected and new result\n+                try {\n+                    Verify.checkEQ(expected, result);\n+                } catch (VerifyException e) {\n+                    throw new RuntimeException(\"Verify failed for \" + name, e);\n+                }\n+            }\n+        }\n+    }\n+\n+    @Test\n+    @IR(counts = {IRNode.LOAD_VECTOR_B, \"> 0\",\n+                  IRNode.ADD_VB,        \"> 0\",\n+                  IRNode.STORE_VECTOR,  \"> 0\",\n+                  \".*multiversion.*\",   \"= 0\"}, \/\/ AutoVectorization Predicate SUFFICES\n+        phase = CompilePhase.PRINT_IDEAL,\n+        applyIfPlatform = {\"64-bit\", \"true\"},\n+        applyIfAnd = {\"AlignVector\", \"false\", \"UseAutoVectorizationSpeculativeAliasingChecks\", \"true\"},\n+        applyIfCPUFeatureOr = {\"sse4.1\", \"true\", \"asimd\", \"true\"})\n+    static void test_byte_incr_noaliasing(MemorySegment a, MemorySegment b) {\n+        for (long i = 0; i < a.byteSize(); i++) {\n+            byte v = a.get(ValueLayout.JAVA_BYTE, i);\n+            b.set(ValueLayout.JAVA_BYTE, i, (byte)(v + 1));\n+        }\n+    }\n+\n+    @Test\n+    @IR(counts = {IRNode.LOAD_VECTOR_B, \"> 0\",\n+                  IRNode.ADD_VB,        \"> 0\",\n+                  IRNode.STORE_VECTOR,  \"> 0\",\n+                  \".*multiversion.*\",   \"> 0\"}, \/\/ AutoVectorization Predicate FAILS\n+        phase = CompilePhase.PRINT_IDEAL,\n+        applyIfAnd = {\"AlignVector\", \"false\", \"UseAutoVectorizationSpeculativeAliasingChecks\", \"true\"},\n+        applyIfCPUFeatureOr = {\"sse4.1\", \"true\", \"asimd\", \"true\"})\n+    static void test_byte_incr_aliasing(MemorySegment a, MemorySegment b) {\n+        for (long i = 0; i < a.byteSize(); i++) {\n+            byte v = a.get(ValueLayout.JAVA_BYTE, i);\n+            b.set(ValueLayout.JAVA_BYTE, i, (byte)(v + 1));\n+        }\n+    }\n+\n+    @Test\n+    @IR(counts = {IRNode.LOAD_VECTOR_B, \"> 0\",\n+                  IRNode.ADD_VB,        \"> 0\",\n+                  IRNode.STORE_VECTOR,  \"> 0\",\n+                  \".*multiversion.*\",   \"> 0\"}, \/\/ AutoVectorization Predicate FAILS\n+        phase = CompilePhase.PRINT_IDEAL,\n+        applyIfPlatform = {\"64-bit\", \"true\"},\n+        applyIfAnd = {\"AlignVector\", \"false\", \"UseAutoVectorizationSpeculativeAliasingChecks\", \"true\"},\n+        applyIfCPUFeatureOr = {\"sse4.1\", \"true\", \"asimd\", \"true\"})\n+    static void test_byte_incr_aliasing_fwd3(MemorySegment a, MemorySegment b) {\n+        for (long i = 0; i < a.byteSize(); i++) {\n+            byte v = a.get(ValueLayout.JAVA_BYTE, i);\n+            b.set(ValueLayout.JAVA_BYTE, i, (byte)(v + 1));\n+        }\n+    }\n+\n+    @Test\n+    @IR(counts = {IRNode.LOAD_VECTOR_B, \"> 0\",\n+                  IRNode.ADD_VB,        \"> 0\",\n+                  IRNode.STORE_VECTOR,  \"> 0\",\n+                  \".*multiversion.*\",   \"= 0\"}, \/\/ AutoVectorization Predicate SUFFICES\n+        phase = CompilePhase.PRINT_IDEAL,\n+        applyIfPlatform = {\"64-bit\", \"true\"},\n+        applyIfAnd = {\"AlignVector\", \"false\", \"UseAutoVectorizationSpeculativeAliasingChecks\", \"true\"},\n+        applyIfCPUFeatureOr = {\"sse4.1\", \"true\", \"asimd\", \"true\"})\n+    static void test_byte_incr_noaliasing_fwd128(MemorySegment a, MemorySegment b) {\n+        for (long i = 0; i < a.byteSize(); i++) {\n+            byte v = a.get(ValueLayout.JAVA_BYTE, i);\n+            b.set(ValueLayout.JAVA_BYTE, i, (byte)(v + 1));\n+        }\n+    }\n+\n+    @Test\n+    @IR(counts = {IRNode.LOAD_VECTOR_I,   IRNode.VECTOR_SIZE + \"min(max_int, max_long)\", \"> 0\",\n+                  IRNode.VECTOR_CAST_I2L, IRNode.VECTOR_SIZE + \"min(max_int, max_long)\", \"> 0\",\n+                  IRNode.STORE_VECTOR,                                                   \"> 0\",\n+                  \".*multiversion.*\",   \"= 0\"}, \/\/ AutoVectorization Predicate SUFFICES\n+        phase = CompilePhase.PRINT_IDEAL,\n+        applyIfPlatform = {\"64-bit\", \"true\"},\n+        applyIfAnd = {\"AlignVector\", \"false\", \"UseAutoVectorizationSpeculativeAliasingChecks\", \"true\"},\n+        applyIfCPUFeatureOr = {\"avx\", \"true\", \"asimd\", \"true\"})\n+    \/\/ In this case, the limit is pre-loop independent, but its assigned\n+    \/\/ ctrl sits between main and pre loop. Only the early ctrl is before\n+    \/\/ the pre loop.\n+    static void test_int_to_long_noaliasing(MemorySegment a, MemorySegment b) {\n+        long limit = a.byteSize() \/ 8L;\n+        for (long i = 0; i < limit; i++) {\n+            int v = a.get(ValueLayout.JAVA_INT_UNALIGNED, 4L * i);\n+            b.set(ValueLayout.JAVA_LONG_UNALIGNED, 8L * i, v);\n+        }\n+    }\n+\n+    @Test\n+    \/\/ @IR(counts = {IRNode.STORE_VECTOR,  \"> 0\",\n+    \/\/               \".*multiversion.*\",   \"> 0\"}, \/\/ AutoVectorization Predicate FAILS\n+    \/\/     phase = CompilePhase.PRINT_IDEAL,\n+    \/\/     applyIfPlatform = {\"64-bit\", \"true\"},\n+    \/\/     applyIfAnd = {\"AlignVector\", \"false\", \"UseAutoVectorizationSpeculativeAliasingChecks\", \"true\"},\n+    \/\/     applyIfCPUFeatureOr = {\"sse4.1\", \"true\", \"asimd\", \"true\"})\n+    \/\/\n+    \/\/ FAILS: but only on \"native\" and \"byte-buffer-direct\"\n+    \/\/        The issue is that one of the VPointers is invalid.\n+    static void test_fill_byte_sameMS_alias(MemorySegment a, MemorySegment b, long invar1, long invar2) {\n+        for (long i = 0; i < a.byteSize() - 100; i++) {\n+            a.set(ValueLayout.JAVA_BYTE, i + invar1, (byte)0x0a);\n+            b.set(ValueLayout.JAVA_BYTE, a.byteSize() - i - 1 - invar2, (byte)0x0b);\n+        }\n+    }\n+\n+    @DontCompile\n+    static void reference_fill_byte_sameMS_alias(MemorySegment a, MemorySegment b, long invar1, long invar2) {\n+        for (long i = 0; i < a.byteSize() - 100; i++) {\n+            a.set(ValueLayout.JAVA_BYTE, i + invar1, (byte)0x0a);\n+            b.set(ValueLayout.JAVA_BYTE, a.byteSize() - i - 1 - invar2, (byte)0x0b);\n+        }\n+    }\n+\n+    @Test\n+    \/\/ @IR(counts = {IRNode.STORE_VECTOR,  \"> 0\",\n+    \/\/               \".*multiversion.*\",   \"= 0\"}, \/\/ AutoVectorization Predicate SUFFICES\n+    \/\/     phase = CompilePhase.PRINT_IDEAL,\n+    \/\/     applyIfPlatform = {\"64-bit\", \"true\"},\n+    \/\/     applyIfAnd = {\"AlignVector\", \"false\", \"UseAutoVectorizationSpeculativeAliasingChecks\", \"true\"},\n+    \/\/     applyIfCPUFeatureOr = {\"sse4.1\", \"true\", \"asimd\", \"true\"})\n+    \/\/\n+    \/\/ FAILS: but only on \"native\" and \"byte-buffer-direct\"\n+    \/\/        The issue is that one of the VPointers is invalid.\n+    \/\/\n+    \/\/ For now, we just assert that there is never multiversioning, which holds with or without vectorization:\n+    @IR(counts = {\".*multiversion.*\",   \"= 0\"}, \/\/ AutoVectorization Predicate SUFFICES\n+        phase = CompilePhase.PRINT_IDEAL,\n+        applyIfPlatform = {\"64-bit\", \"true\"},\n+        applyIfAnd = {\"AlignVector\", \"false\", \"UseAutoVectorizationSpeculativeAliasingChecks\", \"true\"},\n+        applyIfCPUFeatureOr = {\"sse4.1\", \"true\", \"asimd\", \"true\"})\n+    static void test_fill_byte_sameMS_noalias(MemorySegment a, MemorySegment b, long invar1, long invar2, long limit) {\n+        for (long i = 0; i < limit; i++) {\n+            a.set(ValueLayout.JAVA_BYTE, invar1 + i, (byte)0xa);\n+            b.set(ValueLayout.JAVA_BYTE, invar2 - i, (byte)0xb);\n+        }\n+    }\n+\n+    @DontCompile\n+    static void reference_fill_byte_sameMS_noalias(MemorySegment a, MemorySegment b, long invar1, long invar2, long limit) {\n+        for (long i = 0; i < limit; i++) {\n+            a.set(ValueLayout.JAVA_BYTE, invar1 + i, (byte)0xa);\n+            b.set(ValueLayout.JAVA_BYTE, invar2 - i, (byte)0xb);\n+        }\n+    }\n+\n+    @Test\n+    \/\/ @IR(counts = {IRNode.STORE_VECTOR,  \"> 0\"},\n+    \/\/     phase = CompilePhase.PRINT_IDEAL,\n+    \/\/     applyIfPlatform = {\"64-bit\", \"true\"},\n+    \/\/     applyIfAnd = {\"AlignVector\", \"false\", \"UseAutoVectorizationSpeculativeAliasingChecks\", \"true\"},\n+    \/\/     applyIfCPUFeatureOr = {\"sse4.1\", \"true\", \"asimd\", \"true\"})\n+    \/\/\n+    \/\/ FAILS: but only on \"native\" and \"byte-buffer-direct\"\n+    \/\/        The issue is that one of the VPointers is invalid.\n+    \/\/\n+    \/\/ Note: we may or may not use multiversioning, depending if we alias or not at runtime.\n+    static void test_fill_byte_sameMS_maybeAlias(MemorySegment a, MemorySegment b, long invar1, long invar2, long limit) {\n+        for (long i = 0; i < limit; i++) {\n+            a.set(ValueLayout.JAVA_BYTE, invar1 + i, (byte)0xa);\n+            b.set(ValueLayout.JAVA_BYTE, invar2 - i, (byte)0xb);\n+        }\n+    }\n+\n+    @DontCompile\n+    static void reference_fill_byte_sameMS_maybeAlias(MemorySegment a, MemorySegment b, long invar1, long invar2, long limit) {\n+        for (long i = 0; i < limit; i++) {\n+            a.set(ValueLayout.JAVA_BYTE, invar1 + i, (byte)0xa);\n+            b.set(ValueLayout.JAVA_BYTE, invar2 - i, (byte)0xb);\n+        }\n+    }\n+\n+    @Test\n+    \/\/ @IR(counts = {IRNode.STORE_VECTOR,  \"> 0\",\n+    \/\/               \".*multiversion.*\",   \"> 0\"}, \/\/ AutoVectorization Predicate FAILS\n+    \/\/     phase = CompilePhase.PRINT_IDEAL,\n+    \/\/     applyIfPlatform = {\"64-bit\", \"true\"},\n+    \/\/     applyIfAnd = {\"AlignVector\", \"false\", \"UseAutoVectorizationSpeculativeAliasingChecks\", \"true\"},\n+    \/\/     applyIfCPUFeatureOr = {\"sse4.1\", \"true\", \"asimd\", \"true\"})\n+    \/\/\n+    \/\/ FAILS: but only on \"native\" and \"byte-buffer-direct\"\n+    \/\/        The issue is that one of the VPointers is invalid.\n+    static void test_fill_int_sameMS_alias(MemorySegment a, MemorySegment b, long invar1, long invar2) {\n+        for (long i = 0; i < a.byteSize() - 100; i+=4) {\n+            a.set(ValueLayout.JAVA_INT_UNALIGNED, i + invar1, 0x01020304);\n+            b.set(ValueLayout.JAVA_INT_UNALIGNED, a.byteSize() - i - 4 - invar2, 0x11121314);\n+        }\n+    }\n+\n+    @DontCompile\n+    static void reference_fill_int_sameMS_alias(MemorySegment a, MemorySegment b, long invar1, long invar2) {\n+        for (long i = 0; i < a.byteSize() - 100; i+=4) {\n+            a.set(ValueLayout.JAVA_INT_UNALIGNED, i + invar1, 0x01020304);\n+            b.set(ValueLayout.JAVA_INT_UNALIGNED, a.byteSize() - i - 4 - invar2, 0x11121314);\n+        }\n+    }\n+\n+    @Test\n+    \/\/ @IR(counts = {IRNode.STORE_VECTOR,  \"> 0\",\n+    \/\/               \".*multiversion.*\",   \"= 0\"}, \/\/ AutoVectorization Predicate SUFFICES\n+    \/\/     phase = CompilePhase.PRINT_IDEAL,\n+    \/\/     applyIfPlatform = {\"64-bit\", \"true\"},\n+    \/\/     applyIfAnd = {\"AlignVector\", \"false\", \"UseAutoVectorizationSpeculativeAliasingChecks\", \"true\"},\n+    \/\/     applyIfCPUFeatureOr = {\"sse4.1\", \"true\", \"asimd\", \"true\"})\n+    \/\/\n+    \/\/ FAILS: but only on \"native\" and \"byte-buffer-direct\"\n+    \/\/        The issue is that one of the VPointers is invalid.\n+    \/\/\n+    \/\/ For now, we just assert that there is never multiversioning, which holds with or without vectorization:\n+    @IR(counts = {\".*multiversion.*\",   \"= 0\"}, \/\/ AutoVectorization Predicate SUFFICES\n+        phase = CompilePhase.PRINT_IDEAL,\n+        applyIfPlatform = {\"64-bit\", \"true\"},\n+        applyIfAnd = {\"AlignVector\", \"false\", \"UseAutoVectorizationSpeculativeAliasingChecks\", \"true\"},\n+        applyIfCPUFeatureOr = {\"sse4.1\", \"true\", \"asimd\", \"true\"})\n+    static void test_fill_int_sameMS_noalias(MemorySegment a, MemorySegment b, long invar1, long invar2, long limit) {\n+        for (long i = 0; i <= limit - 4; i+=4) {\n+            a.set(ValueLayout.JAVA_INT_UNALIGNED, invar1 + i, 0x01020304);\n+            b.set(ValueLayout.JAVA_INT_UNALIGNED, invar2 - i, 0x11121314);\n+        }\n+    }\n+\n+    @DontCompile\n+    static void reference_fill_int_sameMS_noalias(MemorySegment a, MemorySegment b, long invar1, long invar2, long limit) {\n+        for (long i = 0; i <= limit - 4; i+=4) {\n+            a.set(ValueLayout.JAVA_INT_UNALIGNED, invar1 + i, 0x01020304);\n+            b.set(ValueLayout.JAVA_INT_UNALIGNED, invar2 - i, 0x11121314);\n+        }\n+    }\n+\n+    @Test\n+    \/\/ @IR(counts = {IRNode.STORE_VECTOR,  \"> 0\"},\n+    \/\/     phase = CompilePhase.PRINT_IDEAL,\n+    \/\/     applyIfPlatform = {\"64-bit\", \"true\"},\n+    \/\/     applyIfAnd = {\"AlignVector\", \"false\", \"UseAutoVectorizationSpeculativeAliasingChecks\", \"true\"},\n+    \/\/     applyIfCPUFeatureOr = {\"sse4.1\", \"true\", \"asimd\", \"true\"})\n+    \/\/\n+    \/\/ FAILS: but only on \"native\" and \"byte-buffer-direct\"\n+    \/\/        The issue is that one of the VPointers is invalid.\n+    \/\/\n+    \/\/ Note: we may or may not use multiversioning, depending if we alias or not at runtime.\n+    static void test_fill_int_sameMS_maybeAlias(MemorySegment a, MemorySegment b, long invar1, long invar2, long limit) {\n+        for (long i = 0; i <= limit - 4; i+=4) {\n+            a.set(ValueLayout.JAVA_INT_UNALIGNED, invar1 + i, 0x01020304);\n+            b.set(ValueLayout.JAVA_INT_UNALIGNED, invar2 - i, 0x11121314);\n+        }\n+    }\n+\n+    @DontCompile\n+    static void reference_fill_int_sameMS_maybeAlias(MemorySegment a, MemorySegment b, long invar1, long invar2, long limit) {\n+        for (long i = 0; i <= limit - 4; i+=4) {\n+            a.set(ValueLayout.JAVA_INT_UNALIGNED, invar1 + i, 0x01020304);\n+            b.set(ValueLayout.JAVA_INT_UNALIGNED, invar2 - i, 0x11121314);\n+        }\n+    }\n+}\n","filename":"test\/hotspot\/jtreg\/compiler\/loopopts\/superword\/TestMemorySegmentAliasing.java","additions":854,"deletions":0,"binary":false,"changes":854,"status":"added"},{"patch":"@@ -40,4 +40,8 @@\n- * @run driver compiler.loopopts.superword.TestSplitPacks nCOH_nAV\n- * @run driver compiler.loopopts.superword.TestSplitPacks nCOH_yAV\n- * @run driver compiler.loopopts.superword.TestSplitPacks yCOH_nAV\n- * @run driver compiler.loopopts.superword.TestSplitPacks yCOH_yAV\n+ * @run driver compiler.loopopts.superword.TestSplitPacks nCOH_nAV_ySAC\n+ * @run driver compiler.loopopts.superword.TestSplitPacks nCOH_yAV_ySAC\n+ * @run driver compiler.loopopts.superword.TestSplitPacks yCOH_nAV_ySAC\n+ * @run driver compiler.loopopts.superword.TestSplitPacks yCOH_yAV_ySAC\n+ * @run driver compiler.loopopts.superword.TestSplitPacks nCOH_nAV_nSAC\n+ * @run driver compiler.loopopts.superword.TestSplitPacks nCOH_yAV_nSAC\n+ * @run driver compiler.loopopts.superword.TestSplitPacks yCOH_nAV_nSAC\n+ * @run driver compiler.loopopts.superword.TestSplitPacks yCOH_yAV_nSAC\n@@ -79,4 +83,8 @@\n-            case \"nCOH_nAV\" -> { framework.addFlags(\"-XX:-UseCompactObjectHeaders\", \"-XX:-AlignVector\"); }\n-            case \"nCOH_yAV\" -> { framework.addFlags(\"-XX:-UseCompactObjectHeaders\", \"-XX:+AlignVector\"); }\n-            case \"yCOH_nAV\" -> { framework.addFlags(\"-XX:+UseCompactObjectHeaders\", \"-XX:-AlignVector\"); }\n-            case \"yCOH_yAV\" -> { framework.addFlags(\"-XX:+UseCompactObjectHeaders\", \"-XX:+AlignVector\"); }\n+            case \"nCOH_nAV_ySAC\" -> { framework.addFlags(\"-XX:+UnlockExperimentalVMOptions\", \"-XX:-UseCompactObjectHeaders\", \"-XX:-AlignVector\", \"-XX:+UseAutoVectorizationSpeculativeAliasingChecks\"); }\n+            case \"nCOH_yAV_ySAC\" -> { framework.addFlags(\"-XX:+UnlockExperimentalVMOptions\", \"-XX:-UseCompactObjectHeaders\", \"-XX:+AlignVector\", \"-XX:+UseAutoVectorizationSpeculativeAliasingChecks\"); }\n+            case \"yCOH_nAV_ySAC\" -> { framework.addFlags(\"-XX:+UnlockExperimentalVMOptions\", \"-XX:+UseCompactObjectHeaders\", \"-XX:-AlignVector\", \"-XX:+UseAutoVectorizationSpeculativeAliasingChecks\"); }\n+            case \"yCOH_yAV_ySAC\" -> { framework.addFlags(\"-XX:+UnlockExperimentalVMOptions\", \"-XX:+UseCompactObjectHeaders\", \"-XX:+AlignVector\", \"-XX:+UseAutoVectorizationSpeculativeAliasingChecks\"); }\n+            case \"nCOH_nAV_nSAC\" -> { framework.addFlags(\"-XX:+UnlockExperimentalVMOptions\", \"-XX:-UseCompactObjectHeaders\", \"-XX:-AlignVector\", \"-XX:-UseAutoVectorizationSpeculativeAliasingChecks\"); }\n+            case \"nCOH_yAV_nSAC\" -> { framework.addFlags(\"-XX:+UnlockExperimentalVMOptions\", \"-XX:-UseCompactObjectHeaders\", \"-XX:+AlignVector\", \"-XX:-UseAutoVectorizationSpeculativeAliasingChecks\"); }\n+            case \"yCOH_nAV_nSAC\" -> { framework.addFlags(\"-XX:+UnlockExperimentalVMOptions\", \"-XX:+UseCompactObjectHeaders\", \"-XX:-AlignVector\", \"-XX:-UseAutoVectorizationSpeculativeAliasingChecks\"); }\n+            case \"yCOH_yAV_nSAC\" -> { framework.addFlags(\"-XX:+UnlockExperimentalVMOptions\", \"-XX:+UseCompactObjectHeaders\", \"-XX:+AlignVector\", \"-XX:-UseAutoVectorizationSpeculativeAliasingChecks\"); }\n@@ -117,0 +125,7 @@\n+        tests.put(\"test4a_alias\",() -> { short[] x = aS.clone(); return test4a_alias(x, x); });\n+        tests.put(\"test4b_alias\",() -> { short[] x = aS.clone(); return test4b_alias(x, x); });\n+        tests.put(\"test4c_alias\",() -> { short[] x = aS.clone(); return test4c_alias(x, x); });\n+        tests.put(\"test4d_alias\",() -> { short[] x = aS.clone(); return test4d_alias(x, x); });\n+        tests.put(\"test4e_alias\",() -> { short[] x = aS.clone(); return test4e_alias(x, x); });\n+        tests.put(\"test4f_alias\",() -> { short[] x = aS.clone(); return test4f_alias(x, x); });\n+        tests.put(\"test4g_alias\",() -> { short[] x = aS.clone(); return test4g_alias(x, x); });\n@@ -148,0 +163,7 @@\n+                 \"test4a_alias\",\n+                 \"test4b_alias\",\n+                 \"test4c_alias\",\n+                 \"test4d_alias\",\n+                 \"test4e_alias\",\n+                 \"test4f_alias\",\n+                 \"test4g_alias\",\n@@ -714,1 +736,4 @@\n-                  IRNode.STORE_VECTOR, \"> 0\"},\n+                  IRNode.STORE_VECTOR, \"> 0\",\n+                  \".*multiversion.*\", \"= 0\"},\n+        phase = CompilePhase.PRINT_IDEAL,\n+        applyIf = {\"UseAutoVectorizationSpeculativeAliasingChecks\", \"false\"},\n@@ -718,0 +743,8 @@\n+    @IR(counts = {IRNode.LOAD_VECTOR_S, \"> 0\",\n+                  IRNode.STORE_VECTOR, \"> 0\",\n+                  \".*multiversion.*\", \"= 0\"},\n+        phase = CompilePhase.PRINT_IDEAL,\n+        applyIfAnd = {\"UseAutoVectorizationSpeculativeAliasingChecks\", \"true\", \"AlignVector\", \"false\"},\n+        applyIfPlatform = {\"64-bit\", \"true\"},\n+        applyIfCPUFeatureOr = {\"sse4.1\", \"true\"})\n+    \/\/ Speculative aliasing check -> full vectorization.\n@@ -727,2 +760,4 @@\n-                  IRNode.STORE_VECTOR, \"> 0\"},\n-        applyIf = {\"AlignVector\", \"false\"},\n+                  IRNode.STORE_VECTOR, \"> 0\",\n+                  \".*multiversion.*\", \"= 0\"},\n+        phase = CompilePhase.PRINT_IDEAL,\n+        applyIfAnd = {\"AlignVector\", \"false\", \"UseAutoVectorizationSpeculativeAliasingChecks\", \"false\"},\n@@ -732,0 +767,8 @@\n+    @IR(counts = {IRNode.LOAD_VECTOR_S, \"> 0\",\n+                  IRNode.STORE_VECTOR, \"> 0\",\n+                  \".*multiversion.*\", \"= 0\"},\n+        phase = CompilePhase.PRINT_IDEAL,\n+        applyIfAnd = {\"AlignVector\", \"false\", \"UseAutoVectorizationSpeculativeAliasingChecks\", \"true\"},\n+        applyIfPlatform = {\"64-bit\", \"true\"},\n+        applyIfCPUFeatureOr = {\"sse4.1\", \"true\"})\n+    \/\/ Speculative aliasing check -> full vectorization.\n@@ -741,2 +784,4 @@\n-                  IRNode.STORE_VECTOR, \"> 0\"},\n-        applyIf = {\"MaxVectorSize\", \">=8\"},\n+                  IRNode.STORE_VECTOR, \"> 0\",\n+                  \".*multiversion.*\", \"= 0\"},\n+        phase = CompilePhase.PRINT_IDEAL,\n+        applyIfAnd = {\"MaxVectorSize\", \">=8\", \"UseAutoVectorizationSpeculativeAliasingChecks\", \"false\"},\n@@ -746,0 +791,8 @@\n+    @IR(counts = {IRNode.LOAD_VECTOR_S, \"> 0\",\n+                  IRNode.STORE_VECTOR, \"> 0\",\n+                  \".*multiversion.*\", \"= 0\"},\n+        phase = CompilePhase.PRINT_IDEAL,\n+        applyIfAnd = {\"MaxVectorSize\", \">=8\", \"UseAutoVectorizationSpeculativeAliasingChecks\", \"true\"},\n+        applyIfPlatform = {\"64-bit\", \"true\"},\n+        applyIfCPUFeatureOr = {\"sse4.1\", \"true\", \"asimd\", \"true\", \"rvv\", \"true\"})\n+    \/\/ Speculative aliasing check -> full vectorization.\n@@ -755,2 +808,4 @@\n-                  IRNode.STORE_VECTOR, \"> 0\"},\n-        applyIfAnd = {\"MaxVectorSize\", \">=8\", \"AlignVector\", \"false\"},\n+                  IRNode.STORE_VECTOR, \"> 0\",\n+                  \".*multiversion.*\", \"= 0\"},\n+        phase = CompilePhase.PRINT_IDEAL,\n+        applyIfAnd = {\"MaxVectorSize\", \">=8\", \"AlignVector\", \"false\", \"UseAutoVectorizationSpeculativeAliasingChecks\", \"false\"},\n@@ -760,0 +815,8 @@\n+    @IR(counts = {IRNode.LOAD_VECTOR_S, \"> 0\",\n+                  IRNode.STORE_VECTOR, \"> 0\",\n+                  \".*multiversion.*\", \"= 0\"},\n+        phase = CompilePhase.PRINT_IDEAL,\n+        applyIfAnd = {\"MaxVectorSize\", \">=8\", \"AlignVector\", \"false\", \"UseAutoVectorizationSpeculativeAliasingChecks\", \"true\"},\n+        applyIfPlatform = {\"64-bit\", \"true\"},\n+        applyIfCPUFeatureOr = {\"sse4.1\", \"true\", \"asimd\", \"true\", \"rvv\", \"true\"})\n+    \/\/ Speculative aliasing check -> full vectorization.\n@@ -769,2 +832,4 @@\n-                  IRNode.STORE_VECTOR, \"> 0\"},\n-        applyIfAnd = {\"MaxVectorSize\", \">=8\", \"AlignVector\", \"false\"},\n+                  IRNode.STORE_VECTOR, \"> 0\",\n+                  \".*multiversion.*\", \"= 0\"},\n+        phase = CompilePhase.PRINT_IDEAL,\n+        applyIfAnd = {\"MaxVectorSize\", \">=8\", \"AlignVector\", \"false\", \"UseAutoVectorizationSpeculativeAliasingChecks\", \"false\"},\n@@ -774,0 +839,8 @@\n+    @IR(counts = {IRNode.LOAD_VECTOR_S, \"> 0\",\n+                  IRNode.STORE_VECTOR, \"> 0\",\n+                  \".*multiversion.*\", \"= 0\"},\n+        phase = CompilePhase.PRINT_IDEAL,\n+        applyIfAnd = {\"MaxVectorSize\", \">=8\", \"AlignVector\", \"false\", \"UseAutoVectorizationSpeculativeAliasingChecks\", \"true\"},\n+        applyIfPlatform = {\"64-bit\", \"true\"},\n+        applyIfCPUFeatureOr = {\"sse4.1\", \"true\", \"asimd\", \"true\", \"rvv\", \"true\"})\n+    \/\/ Speculative aliasing check -> full vectorization.\n@@ -783,2 +856,4 @@\n-                  IRNode.STORE_VECTOR, \"> 0\"},\n-        applyIfAnd = {\"MaxVectorSize\", \">=8\", \"AlignVector\", \"false\"},\n+                  IRNode.STORE_VECTOR, \"> 0\",\n+                  \".*multiversion.*\", \"= 0\"},\n+        phase = CompilePhase.PRINT_IDEAL,\n+        applyIfAnd = {\"MaxVectorSize\", \">=8\", \"AlignVector\", \"false\", \"UseAutoVectorizationSpeculativeAliasingChecks\", \"false\"},\n@@ -788,0 +863,8 @@\n+    @IR(counts = {IRNode.LOAD_VECTOR_S, \"> 0\",\n+                  IRNode.STORE_VECTOR, \"> 0\",\n+                  \".*multiversion.*\", \"= 0\"},\n+        phase = CompilePhase.PRINT_IDEAL,\n+        applyIfAnd = {\"MaxVectorSize\", \">=8\", \"AlignVector\", \"false\", \"UseAutoVectorizationSpeculativeAliasingChecks\", \"true\"},\n+        applyIfPlatform = {\"64-bit\", \"true\"},\n+        applyIfCPUFeatureOr = {\"sse4.1\", \"true\", \"asimd\", \"true\", \"rvv\", \"true\"})\n+    \/\/ Speculative aliasing check -> full vectorization.\n@@ -797,2 +880,4 @@\n-                  IRNode.STORE_VECTOR, \"> 0\"},\n-        applyIf = {\"MaxVectorSize\", \">=32\"},\n+                  IRNode.STORE_VECTOR, \"> 0\",\n+                  \".*multiversion.*\", \"= 0\"},\n+        phase = CompilePhase.PRINT_IDEAL,\n+        applyIfAnd = {\"MaxVectorSize\", \">=32\", \"UseAutoVectorizationSpeculativeAliasingChecks\", \"false\"},\n@@ -802,0 +887,8 @@\n+    @IR(counts = {IRNode.LOAD_VECTOR_S, \"> 0\",\n+                  IRNode.STORE_VECTOR, \"> 0\",\n+                  \".*multiversion.*\", \"= 0\"},\n+        phase = CompilePhase.PRINT_IDEAL,\n+        applyIfAnd = {\"MaxVectorSize\", \">=32\", \"UseAutoVectorizationSpeculativeAliasingChecks\", \"true\"},\n+        applyIfPlatform = {\"64-bit\", \"true\"},\n+        applyIfCPUFeatureOr = {\"sse4.1\", \"true\", \"asimd\", \"true\", \"rvv\", \"true\"})\n+    \/\/ Speculative aliasing check -> full vectorization.\n@@ -809,0 +902,175 @@\n+    @Test\n+    @IR(counts = {IRNode.LOAD_VECTOR_S, IRNode.VECTOR_SIZE_2, \"> 0\",\n+                  IRNode.STORE_VECTOR, \"> 0\",\n+                  \".*multiversion.*\", \"= 0\"},\n+        phase = CompilePhase.PRINT_IDEAL,\n+        applyIf = {\"UseAutoVectorizationSpeculativeAliasingChecks\", \"false\"},\n+        applyIfPlatform = {\"64-bit\", \"true\"},\n+        applyIfCPUFeatureOr = {\"sse4.1\", \"true\"})\n+    \/\/ Cyclic dependency with distance 2 -> split into 2-packs\n+    @IR(counts = {IRNode.LOAD_VECTOR_S, \"> 0\",\n+                  IRNode.LOAD_VECTOR_S, IRNode.VECTOR_SIZE_2, \"> 0\",\n+                  IRNode.STORE_VECTOR, \"> 0\",\n+                  \".*multiversion.*\", \"> 0\"},\n+        phase = CompilePhase.PRINT_IDEAL,\n+        applyIfAnd = {\"UseAutoVectorizationSpeculativeAliasingChecks\", \"true\", \"AlignVector\", \"false\"},\n+        applyIfPlatform = {\"64-bit\", \"true\"},\n+        applyIfCPUFeatureOr = {\"sse4.1\", \"true\"})\n+    \/\/ Speculative aliasing check with multiversioning -> full vectorization & split packs.\n+    static Object[] test4a_alias(short[] a, short[] b) {\n+        for (int i = 0; i < RANGE-64; i++) {\n+          b[i+2] = a[i+0];\n+        }\n+        return new Object[]{ a, b };\n+    }\n+\n+    @Test\n+    @IR(counts = {IRNode.LOAD_VECTOR_S, IRNode.VECTOR_SIZE_2, \"> 0\",\n+                  IRNode.STORE_VECTOR, \"> 0\",\n+                  \".*multiversion.*\", \"= 0\"},\n+        phase = CompilePhase.PRINT_IDEAL,\n+        applyIfAnd = {\"AlignVector\", \"false\", \"UseAutoVectorizationSpeculativeAliasingChecks\", \"false\"},\n+        applyIfPlatform = {\"64-bit\", \"true\"},\n+        applyIfCPUFeatureOr = {\"sse4.1\", \"true\"})\n+    \/\/ Cyclic dependency with distance 3 -> split into 2-packs\n+    @IR(counts = {IRNode.LOAD_VECTOR_S, \"> 0\",\n+                  IRNode.LOAD_VECTOR_S, IRNode.VECTOR_SIZE_2, \"> 0\",\n+                  IRNode.STORE_VECTOR, \"> 0\",\n+                  \".*multiversion.*\", \"> 0\"},\n+        phase = CompilePhase.PRINT_IDEAL,\n+        applyIfAnd = {\"AlignVector\", \"false\", \"UseAutoVectorizationSpeculativeAliasingChecks\", \"true\"},\n+        applyIfPlatform = {\"64-bit\", \"true\"},\n+        applyIfCPUFeatureOr = {\"sse4.1\", \"true\"})\n+    \/\/ Speculative aliasing check with multiversioning -> full vectorization & split packs.\n+    static Object[] test4b_alias(short[] a, short[] b) {\n+        for (int i = 0; i < RANGE-64; i++) {\n+          b[i+3] = a[i+0];\n+        }\n+        return new Object[]{ a, b };\n+    }\n+\n+    @Test\n+    @IR(counts = {IRNode.LOAD_VECTOR_S, IRNode.VECTOR_SIZE_4, \"> 0\",\n+                  IRNode.STORE_VECTOR, \"> 0\",\n+                  \".*multiversion.*\", \"= 0\"},\n+        phase = CompilePhase.PRINT_IDEAL,\n+        applyIfAnd = {\"MaxVectorSize\", \">=8\", \"UseAutoVectorizationSpeculativeAliasingChecks\", \"false\"},\n+        applyIfPlatform = {\"64-bit\", \"true\"},\n+        applyIfCPUFeatureOr = {\"sse4.1\", \"true\", \"asimd\", \"true\", \"rvv\", \"true\"})\n+    \/\/ Cyclic dependency with distance 4 -> split into 4-packs\n+    @IR(counts = {IRNode.LOAD_VECTOR_S, \"> 0\",\n+                  IRNode.LOAD_VECTOR_S, IRNode.VECTOR_SIZE_4, \"> 0\",\n+                  IRNode.STORE_VECTOR, \"> 0\",\n+                  \".*multiversion.*\", \"> 0\"},\n+        phase = CompilePhase.PRINT_IDEAL,\n+        applyIfAnd = {\"MaxVectorSize\", \">=8\", \"UseAutoVectorizationSpeculativeAliasingChecks\", \"true\"},\n+        applyIfPlatform = {\"64-bit\", \"true\"},\n+        applyIfCPUFeatureOr = {\"sse4.1\", \"true\", \"asimd\", \"true\", \"rvv\", \"true\"})\n+    \/\/ Speculative aliasing check with multiversioning -> full vectorization & split packs.\n+    static Object[] test4c_alias(short[] a, short[] b) {\n+        for (int i = 0; i < RANGE-64; i++) {\n+          b[i+4] = a[i+0];\n+        }\n+        return new Object[]{ a, b };\n+    }\n+\n+    @Test\n+    @IR(counts = {IRNode.LOAD_VECTOR_S, IRNode.VECTOR_SIZE_4, \"> 0\",\n+                  IRNode.STORE_VECTOR, \"> 0\",\n+                  \".*multiversion.*\", \"= 0\"},\n+        phase = CompilePhase.PRINT_IDEAL,\n+        applyIfAnd = {\"MaxVectorSize\", \">=8\", \"AlignVector\", \"false\", \"UseAutoVectorizationSpeculativeAliasingChecks\", \"false\"},\n+        applyIfPlatform = {\"64-bit\", \"true\"},\n+        applyIfCPUFeatureOr = {\"sse4.1\", \"true\", \"asimd\", \"true\", \"rvv\", \"true\"})\n+    \/\/ Cyclic dependency with distance 5 -> split into 4-packs\n+    @IR(counts = {IRNode.LOAD_VECTOR_S, \"> 0\",\n+                  IRNode.LOAD_VECTOR_S, IRNode.VECTOR_SIZE_4, \"> 0\",\n+                  IRNode.STORE_VECTOR, \"> 0\",\n+                  \".*multiversion.*\", \"> 0\"},\n+        phase = CompilePhase.PRINT_IDEAL,\n+        applyIfAnd = {\"MaxVectorSize\", \">=8\", \"AlignVector\", \"false\", \"UseAutoVectorizationSpeculativeAliasingChecks\", \"true\"},\n+        applyIfPlatform = {\"64-bit\", \"true\"},\n+        applyIfCPUFeatureOr = {\"sse4.1\", \"true\", \"asimd\", \"true\", \"rvv\", \"true\"})\n+    \/\/ Speculative aliasing check with multiversioning -> full vectorization & split packs.\n+    static Object[] test4d_alias(short[] a, short[] b) {\n+        for (int i = 0; i < RANGE-64; i++) {\n+          b[i+5] = a[i+0];\n+        }\n+        return new Object[]{ a, b };\n+    }\n+\n+    @Test\n+    @IR(counts = {IRNode.LOAD_VECTOR_S, IRNode.VECTOR_SIZE_4, \"> 0\",\n+                  IRNode.STORE_VECTOR, \"> 0\",\n+                  \".*multiversion.*\", \"= 0\"},\n+        phase = CompilePhase.PRINT_IDEAL,\n+        applyIfAnd = {\"MaxVectorSize\", \">=8\", \"AlignVector\", \"false\", \"UseAutoVectorizationSpeculativeAliasingChecks\", \"false\"},\n+        applyIfPlatform = {\"64-bit\", \"true\"},\n+        applyIfCPUFeatureOr = {\"sse4.1\", \"true\", \"asimd\", \"true\", \"rvv\", \"true\"})\n+    \/\/ Cyclic dependency with distance 6 -> split into 4-packs\n+    @IR(counts = {IRNode.LOAD_VECTOR_S, \"> 0\",\n+                  IRNode.LOAD_VECTOR_S, IRNode.VECTOR_SIZE_4, \"> 0\",\n+                  IRNode.STORE_VECTOR, \"> 0\",\n+                  \".*multiversion.*\", \"> 0\"},\n+        phase = CompilePhase.PRINT_IDEAL,\n+        applyIfAnd = {\"MaxVectorSize\", \">=8\", \"AlignVector\", \"false\", \"UseAutoVectorizationSpeculativeAliasingChecks\", \"true\"},\n+        applyIfPlatform = {\"64-bit\", \"true\"},\n+        applyIfCPUFeatureOr = {\"sse4.1\", \"true\", \"asimd\", \"true\", \"rvv\", \"true\"})\n+    \/\/ Speculative aliasing check with multiversioning -> full vectorization & split packs.\n+    static Object[] test4e_alias(short[] a, short[] b) {\n+        for (int i = 0; i < RANGE-64; i++) {\n+          b[i+6] = a[i+0];\n+        }\n+        return new Object[]{ a, b };\n+    }\n+\n+    @Test\n+    @IR(counts = {IRNode.LOAD_VECTOR_S, IRNode.VECTOR_SIZE_4, \"> 0\",\n+                  IRNode.STORE_VECTOR, \"> 0\",\n+                  \".*multiversion.*\", \"= 0\"},\n+        phase = CompilePhase.PRINT_IDEAL,\n+        applyIfAnd = {\"MaxVectorSize\", \">=8\", \"AlignVector\", \"false\", \"UseAutoVectorizationSpeculativeAliasingChecks\", \"false\"},\n+        applyIfPlatform = {\"64-bit\", \"true\"},\n+        applyIfCPUFeatureOr = {\"sse4.1\", \"true\", \"asimd\", \"true\", \"rvv\", \"true\"})\n+    \/\/ Cyclic dependency with distance 7 -> split into 4-packs\n+    @IR(counts = {IRNode.LOAD_VECTOR_S, \"> 0\",\n+                  IRNode.LOAD_VECTOR_S, IRNode.VECTOR_SIZE_4, \"> 0\",\n+                  IRNode.STORE_VECTOR, \"> 0\",\n+                  \".*multiversion.*\", \"> 0\"},\n+        phase = CompilePhase.PRINT_IDEAL,\n+        applyIfAnd = {\"MaxVectorSize\", \">=8\", \"AlignVector\", \"false\", \"UseAutoVectorizationSpeculativeAliasingChecks\", \"true\"},\n+        applyIfPlatform = {\"64-bit\", \"true\"},\n+        applyIfCPUFeatureOr = {\"sse4.1\", \"true\", \"asimd\", \"true\", \"rvv\", \"true\"})\n+    \/\/ Speculative aliasing check with multiversioning -> full vectorization & split packs.\n+    static Object[] test4f_alias(short[] a, short[] b) {\n+        for (int i = 0; i < RANGE-64; i++) {\n+          b[i+7] = a[i+0];\n+        }\n+        return new Object[]{ a, b };\n+    }\n+\n+    @Test\n+    @IR(counts = {IRNode.LOAD_VECTOR_S, IRNode.VECTOR_SIZE_8, \"> 0\",\n+                  IRNode.STORE_VECTOR, \"> 0\",\n+                  \".*multiversion.*\", \"= 0\"},\n+        phase = CompilePhase.PRINT_IDEAL,\n+        applyIfAnd = {\"MaxVectorSize\", \">=32\", \"UseAutoVectorizationSpeculativeAliasingChecks\", \"false\"},\n+        applyIfPlatform = {\"64-bit\", \"true\"},\n+        applyIfCPUFeatureOr = {\"sse4.1\", \"true\", \"asimd\", \"true\", \"rvv\", \"true\"})\n+    \/\/ Cyclic dependency with distance 8 -> split into 8-packs\n+    @IR(counts = {IRNode.LOAD_VECTOR_S, \"> 0\",\n+                  IRNode.LOAD_VECTOR_S, IRNode.VECTOR_SIZE_8, \"> 0\",\n+                  IRNode.STORE_VECTOR, \"> 0\",\n+                  \".*multiversion.*\", \"> 0\"},\n+        phase = CompilePhase.PRINT_IDEAL,\n+        applyIfAnd = {\"MaxVectorSize\", \">=32\", \"UseAutoVectorizationSpeculativeAliasingChecks\", \"true\"},\n+        applyIfPlatform = {\"64-bit\", \"true\"},\n+        applyIfCPUFeatureOr = {\"avx2\", \"true\", \"asimd\", \"true\", \"rvv\", \"true\"})\n+    \/\/ Speculative aliasing check with multiversioning -> full vectorization & split packs.\n+    static Object[] test4g_alias(short[] a, short[] b) {\n+        for (int i = 0; i < RANGE-64; i++) {\n+          b[i+8] = a[i+0];\n+        }\n+        return new Object[]{ a, b };\n+    }\n+\n","filename":"test\/hotspot\/jtreg\/compiler\/loopopts\/superword\/TestSplitPacks.java","additions":289,"deletions":21,"binary":false,"changes":310,"status":"modified"},{"patch":"@@ -34,4 +34,12 @@\n- * @run main\/othervm -Xbootclasspath\/a:.\n- *                   -XX:+UnlockDiagnosticVMOptions\n- *                   -XX:+WhiteBoxAPI\n- *                   compiler.vectorization.runner.LoopArrayIndexComputeTest\n+ *\n+ * @run main\/othervm -Xbootclasspath\/a:. -XX:+UnlockDiagnosticVMOptions -XX:+WhiteBoxAPI\n+ *                   compiler.vectorization.runner.LoopArrayIndexComputeTest nAV_ySAC\n+ *\n+ * @run main\/othervm -Xbootclasspath\/a:. -XX:+UnlockDiagnosticVMOptions -XX:+WhiteBoxAPI\n+ *                   compiler.vectorization.runner.LoopArrayIndexComputeTest yAV_ySAC\n+ *\n+ * @run main\/othervm -Xbootclasspath\/a:. -XX:+UnlockDiagnosticVMOptions -XX:+WhiteBoxAPI\n+ *                   compiler.vectorization.runner.LoopArrayIndexComputeTest nAV_nSAC\n+ *\n+ * @run main\/othervm -Xbootclasspath\/a:. -XX:+UnlockDiagnosticVMOptions -XX:+WhiteBoxAPI\n+ *                   compiler.vectorization.runner.LoopArrayIndexComputeTest yAV_nSAC\n@@ -51,0 +59,12 @@\n+    \/\/ We must pass the flags directly to the test-VM, and not the driver vm in the @run above.\n+    @Override\n+    protected String[] testVMFlags(String[] args) {\n+        return switch (args[0]) {\n+            case \"nAV_ySAC\" -> new String[]{\"-XX:-AlignVector\", \"-XX:+UseAutoVectorizationSpeculativeAliasingChecks\"};\n+            case \"yAV_ySAC\" -> new String[]{\"-XX:+AlignVector\", \"-XX:+UseAutoVectorizationSpeculativeAliasingChecks\"};\n+            case \"nAV_nSAC\" -> new String[]{\"-XX:-AlignVector\", \"-XX:-UseAutoVectorizationSpeculativeAliasingChecks\"};\n+            case \"yAV_nSAC\" -> new String[]{\"-XX:+AlignVector\", \"-XX:-UseAutoVectorizationSpeculativeAliasingChecks\"};\n+            default -> { throw new RuntimeException(\"Test argument not recognized: \" + args[0]); }\n+        };\n+    }\n+\n@@ -178,1 +198,10 @@\n-    @IR(failOn = {IRNode.STORE_VECTOR})\n+    @IR(failOn = {IRNode.STORE_VECTOR},\n+        applyIf = {\"UseAutoVectorizationSpeculativeAliasingChecks\", \"false\"})\n+     \/\/ Speculative aliasing check -> never fails -> only predicate, no multiversioning.\n+    @IR(applyIfCPUFeatureOr = {\"asimd\", \"true\", \"sse2\", \"true\"},\n+        applyIfAnd = {\"UseAutoVectorizationSpeculativeAliasingChecks\", \"true\", \"AlignVector\", \"false\"},\n+        phase = CompilePhase.PRINT_IDEAL,\n+        counts = {IRNode.STORE_VECTOR, \">0\",\n+                  IRNode.LOAD_VECTOR_I, \">0\", \/\/ full vectorization\n+                  \".*multiversion.*\", \"= 0\"})\n+    \/\/ JDK-8354303: could we prove statically that there is no aliasing?\n@@ -189,1 +218,10 @@\n-    @IR(failOn = {IRNode.STORE_VECTOR})\n+    @IR(failOn = {IRNode.STORE_VECTOR},\n+        applyIf = {\"UseAutoVectorizationSpeculativeAliasingChecks\", \"false\"})\n+    \/\/ Speculative aliasing check -> never fails -> only predicate, no multiversioning.\n+    @IR(applyIfCPUFeatureOr = {\"asimd\", \"true\", \"sse2\", \"true\"},\n+        applyIfAnd = {\"UseAutoVectorizationSpeculativeAliasingChecks\", \"true\", \"AlignVector\", \"false\"},\n+        phase = CompilePhase.PRINT_IDEAL,\n+        counts = {IRNode.STORE_VECTOR, \">0\",\n+                  IRNode.LOAD_VECTOR_I, \">0\", \/\/ full vectorization\n+                  \".*multiversion.*\", \"= 0\"})\n+    \/\/ JDK-8354303: could we prove statically that there is no aliasing?\n@@ -279,1 +317,11 @@\n-    @IR(failOn = {IRNode.STORE_VECTOR})\n+    @IR(failOn = {IRNode.STORE_VECTOR},\n+        applyIf = {\"UseAutoVectorizationSpeculativeAliasingChecks\", \"false\"})\n+    \/\/ Speculative aliasing check -> never fails -> only predicate, no multiversioning.\n+    @IR(applyIfCPUFeatureOr = {\"asimd\", \"true\", \"sse2\", \"true\"},\n+        applyIfAnd = {\"UseAutoVectorizationSpeculativeAliasingChecks\", \"true\", \"AlignVector\", \"false\"},\n+        phase = CompilePhase.PRINT_IDEAL,\n+        counts = {IRNode.STORE_VECTOR, \">0\",\n+                  IRNode.MUL_VS, \">0\",\n+                  IRNode.LOAD_VECTOR_S, \">0\", \/\/ full vectorization\n+                  \".*multiversion.*\", \"= 0\"})\n+    \/\/ JDK-8354303: could we prove statically that there is no aliasing?\n@@ -307,0 +355,2 @@\n+        applyIf = {\"UseAutoVectorizationSpeculativeAliasingChecks\", \"false\"},\n+        phase = CompilePhase.PRINT_IDEAL,\n@@ -308,1 +358,2 @@\n-                  IRNode.MUL_VS, IRNode.VECTOR_SIZE_2, \">0\"}) \/\/ size 2 only\n+                  IRNode.MUL_VS, IRNode.VECTOR_SIZE_2, \">0\", \/\/ size 2 only\n+                  \".*multiversion.*\", \"= 0\"})\n@@ -335,1 +386,11 @@\n-    @IR(failOn = {IRNode.STORE_VECTOR})\n+    @IR(applyIfCPUFeatureOr = {\"asimd\", \"true\", \"sse2\", \"true\"},\n+        applyIf = {\"UseAutoVectorizationSpeculativeAliasingChecks\", \"false\"},\n+        failOn = {IRNode.STORE_VECTOR})\n+    \/\/ Speculative aliasing check -> never fails -> only predicate, no multiversioning.\n+    @IR(applyIfCPUFeatureOr = {\"asimd\", \"true\", \"sse2\", \"true\"},\n+        applyIfAnd = {\"UseAutoVectorizationSpeculativeAliasingChecks\", \"true\", \"AlignVector\", \"false\"},\n+        phase = CompilePhase.PRINT_IDEAL,\n+        counts = {IRNode.STORE_VECTOR, \">0\",\n+                  IRNode.ADD_VB, \">0\", \/\/ full vectorization\n+                  \".*multiversion.*\", \"= 0\"})\n+    \/\/ JDK-8354303: could we prove statically that there is no aliasing?\n@@ -340,1 +401,1 @@\n-            res[i] *= bytes[i - 3];\n+            res[i] += bytes[i - 3];\n@@ -362,0 +423,9 @@\n+        applyIf = {\"UseAutoVectorizationSpeculativeAliasingChecks\", \"false\"},\n+        phase = CompilePhase.PRINT_IDEAL,\n+        counts = {IRNode.STORE_VECTOR, \">0\",\n+                  IRNode.OR_VB, IRNode.VECTOR_SIZE_4, \">0\", \/\/ size 4 only\n+                  \".*multiversion.*\", \"= 0\"})\n+    \/\/ Speculative aliasing check -> never fails -> only predicate, no multiversioning.\n+    @IR(applyIfCPUFeatureOr = {\"asimd\", \"true\", \"sse2\", \"true\"},\n+        applyIfAnd = {\"UseAutoVectorizationSpeculativeAliasingChecks\", \"true\", \"AlignVector\", \"false\"},\n+        phase = CompilePhase.PRINT_IDEAL,\n@@ -363,1 +433,3 @@\n-                  IRNode.OR_VB, IRNode.VECTOR_SIZE_4, \">0\"}) \/\/ size 4 only\n+                  IRNode.OR_VB, \">0\", \/\/ full vectorization\n+                  \".*multiversion.*\", \"= 0\"})\n+    \/\/ JDK-8354303: could we prove statically that there is no aliasing?\n@@ -389,1 +461,10 @@\n-    @IR(failOn = {IRNode.STORE_VECTOR})\n+    @IR(failOn = {IRNode.STORE_VECTOR},\n+        applyIf = {\"UseAutoVectorizationSpeculativeAliasingChecks\", \"false\"})\n+    \/\/ Speculative aliasing check -> never fails -> only predicate, no multiversioning.\n+    @IR(applyIfCPUFeatureOr = {\"asimd\", \"true\", \"sse2\", \"true\"},\n+        applyIf = {\"UseAutoVectorizationSpeculativeAliasingChecks\", \"true\"},\n+        phase = CompilePhase.PRINT_IDEAL,\n+        counts = {IRNode.STORE_VECTOR, \">0\",\n+                  IRNode.LOAD_VECTOR_I, \">0\", \/\/ full vectorization\n+                  \".*multiversion.*\", \"= 0\"})\n+    \/\/ JDK-8354303: could we prove statically that there is no aliasing?\n","filename":"test\/hotspot\/jtreg\/compiler\/vectorization\/runner\/LoopArrayIndexComputeTest.java","additions":93,"deletions":12,"binary":false,"changes":105,"status":"modified"},{"patch":"@@ -0,0 +1,248 @@\n+\/*\n+ * Copyright (c) 2023, 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+package org.openjdk.bench.vm.compiler;\n+\n+import org.openjdk.jmh.annotations.*;\n+import org.openjdk.jmh.infra.*;\n+\n+import java.util.concurrent.TimeUnit;\n+import java.util.Random;\n+\n+@BenchmarkMode(Mode.AverageTime)\n+@OutputTimeUnit(TimeUnit.NANOSECONDS)\n+@State(Scope.Thread)\n+@Warmup(iterations = 1, time = 1, timeUnit = TimeUnit.SECONDS)\n+@Measurement(iterations = 3, time = 1, timeUnit = TimeUnit.SECONDS)\n+@Fork(value = 1)\n+public abstract class VectorAliasing {\n+    @Param({\/*\"512\",  \"1024\", *\/  \"10000\"})\n+    public int SIZE;\n+\n+    public static int INVAR_ZERO = 0;\n+\n+    \/\/ For all types we have an \"a\" and \"b\" series. Each series is an alias to the same array.\n+    private byte[] aB;\n+    private byte[] bB;\n+\n+    private int[] aI;\n+    private int[] bI;\n+\n+    private long[] aL;\n+    private long[] bL;\n+\n+    @Param(\"0\")\n+    private int seed;\n+    private Random r = new Random(seed);\n+\n+    @Setup\n+    public void init() {\n+        aB = new byte[SIZE];\n+        bB = new byte[SIZE];\n+\n+        aI = new int[SIZE];\n+        bI = new int[SIZE];\n+\n+        aL = new long[SIZE];\n+        bL = new long[SIZE];\n+\n+        for (int i = 0; i < SIZE; i++) {\n+            aB[i] = (byte) r.nextInt();\n+            bB[i] = (byte) r.nextInt();\n+\n+            aI[i] = r.nextInt();\n+            bI[i] = r.nextInt();\n+\n+            aL[i] = r.nextLong();\n+            bL[i] = r.nextLong();\n+        }\n+    }\n+\n+    @CompilerControl(CompilerControl.Mode.DONT_INLINE)\n+    public void copy_B(byte[] a, byte b[]) {\n+        for (int i = 0; i < a.length; i++) {\n+            b[i] = a[i];\n+        }\n+    }\n+\n+    @CompilerControl(CompilerControl.Mode.DONT_INLINE)\n+    public void copy_B(byte[] a, byte b[], int aOffset, int bOffset, int size) {\n+        for (int i = 0; i < size; i++) {\n+            b[i + bOffset] = a[i + aOffset];\n+        }\n+    }\n+\n+    @CompilerControl(CompilerControl.Mode.DONT_INLINE)\n+    public void copy_I(int[] a, int[] b) {\n+        for (int i = 0; i < a.length; i++) {\n+            b[i] = a[i];\n+        }\n+    }\n+\n+    @CompilerControl(CompilerControl.Mode.DONT_INLINE)\n+    public void copy_I(int[] a, int[] b, int aOffset, int bOffset, int size) {\n+        for (int i = 0; i < size; i++) {\n+            b[i + bOffset] = a[i + aOffset];\n+        }\n+    }\n+\n+    @CompilerControl(CompilerControl.Mode.DONT_INLINE)\n+    public void copy_L(long[] a, long[] b) {\n+        for (int i = 0; i < a.length; i++) {\n+            b[i] = a[i];\n+        }\n+    }\n+\n+    @CompilerControl(CompilerControl.Mode.DONT_INLINE)\n+    public void copy_L(long[] a, long[] b, int aOffset, int bOffset, int size) {\n+        for (int i = 0; i < size; i++) {\n+            b[i + bOffset] = a[i + aOffset];\n+        }\n+    }\n+\n+    @Benchmark\n+    public void bench_copy_array_B_sameIndex_noalias() {\n+        copy_B(bB, aB);\n+    }\n+\n+    @Benchmark\n+    public void bench_copy_array_B_sameIndex_alias() {\n+        copy_B(aB, aB);\n+    }\n+\n+    @Benchmark\n+    public void bench_copy_array_B_differentIndex_noalias() {\n+        copy_B(bB, aB, 0, 0, aB.length);\n+    }\n+\n+    @Benchmark\n+    public void bench_copy_array_B_differentIndex_alias() {\n+        copy_B(aB, aB, 0, 0, aB.length);\n+    }\n+\n+    \/\/ No overlap -> expect vectoirzation.\n+    @Benchmark\n+    public void bench_copy_array_B_half() {\n+        copy_B(aB, aB, 0, aB.length \/ 2, aB.length \/ 2);\n+    }\n+\n+    \/\/ Overlap, but never alias -> expect vectorization.\n+    @Benchmark\n+    public void bench_copy_array_B_partial_overlap() {\n+        copy_B(aB, aB, 0, aB.length \/ 4, aB.length \/ 4 * 3);\n+    }\n+\n+    @Benchmark\n+    public void bench_copy_array_I_sameIndex_noalias() {\n+        copy_I(bI, aI);\n+    }\n+\n+    @Benchmark\n+    public void bench_copy_array_I_sameIndex_alias() {\n+        copy_I(aI, aI);\n+    }\n+\n+    @Benchmark\n+    public void bench_copy_array_I_differentIndex_noalias() {\n+        copy_I(bI, aI, 0, 0, aI.length);\n+    }\n+\n+    @Benchmark\n+    public void bench_copy_array_I_differentIndex_alias() {\n+        copy_I(aI, aI, 0, 0, aI.length);\n+    }\n+\n+    \/\/ No overlap -> expect vectoirzation.\n+    @Benchmark\n+    public void bench_copy_array_I_half() {\n+        copy_I(aI, aI, 0, aI.length \/ 2, aI.length \/ 2);\n+    }\n+\n+    \/\/ Overlap, but never alias -> expect vectorization.\n+    @Benchmark\n+    public void bench_copy_array_I_partial_overlap() {\n+        copy_I(aI, aI, 0, aI.length \/ 4, aI.length \/ 4 * 3);\n+    }\n+\n+    @Benchmark\n+    public void bench_copy_array_L_sameIndex_noalias() {\n+        copy_L(bL, aL);\n+    }\n+\n+    @Benchmark\n+    public void bench_copy_array_L_sameIndex_alias() {\n+        copy_L(aL, aL);\n+    }\n+\n+    @Benchmark\n+    public void bench_copy_array_L_differentIndex_noalias() {\n+        copy_L(bL, aL, 0, 0, aL.length);\n+    }\n+\n+    @Benchmark\n+    public void bench_copy_array_L_differentIndex_alias() {\n+        copy_L(aL, aL, 0, 0, aL.length);\n+    }\n+\n+    \/\/ No overlap -> expect vectoirzation.\n+    @Benchmark\n+    public void bench_copy_array_L_half() {\n+        copy_L(aL, aL, 0, aL.length \/ 2, aL.length \/ 2);\n+    }\n+\n+    \/\/ Overlap, but never alias -> expect vectorization.\n+    @Benchmark\n+    public void bench_copy_array_L_partial_overlap() {\n+        copy_L(aL, aL, 0, aL.length \/ 4, aL.length \/ 4 * 3);\n+    }\n+\n+    @Fork(value = 1, jvmArgs = {\n+        \"-XX:+UseSuperWord\",\n+        \"-XX:+UnlockDiagnosticVMOptions\",\n+        \"-XX:-UseAutoVectorizationSpeculativeAliasingChecks\"\n+    })\n+    public static class VectorAliasingSuperWordWithoutSpeculativeAliasingChecks extends VectorAliasing {}\n+\n+    @Fork(value = 1, jvmArgs = {\n+        \"-XX:+UseSuperWord\",\n+        \"-XX:+UnlockDiagnosticVMOptions\",\n+        \"-XX:-LoopMultiversioningOptimizeSlowLoop\"\n+    })\n+    public static class VectorAliasingSuperWordWithoutSlowLoopOptimizations extends VectorAliasing {}\n+\n+    @Fork(value = 1, jvmArgs = {\n+        \"-XX:+UseSuperWord\",\n+        \"-XX:+UnlockDiagnosticVMOptions\",\n+        \"-XX:AutoVectorizationOverrideProfitability=0\"\n+    })\n+    public static class VectorAliasingSuperWordPretendNotProfitable extends VectorAliasing {}\n+\n+    @Fork(value = 1, jvmArgs = {\n+        \"-XX:+UseSuperWord\"\n+    })\n+    public static class VectorAliasingSuperWord extends VectorAliasing {}\n+\n+    @Fork(value = 1, jvmArgs = {\n+        \"-XX:-UseSuperWord\"\n+    })\n+    public static class VectorAliasingNoSuperWord extends VectorAliasing {}\n+}\n","filename":"test\/micro\/org\/openjdk\/bench\/vm\/compiler\/VectorAliasing.java","additions":248,"deletions":0,"binary":false,"changes":248,"status":"added"}]}