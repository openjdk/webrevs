{"files":[{"patch":"@@ -38,0 +38,2 @@\n+import static jdk.incubator.vector.Float16Consts.EXP_BIAS;\n+import static jdk.incubator.vector.Float16Consts.SIGNIFICAND_WIDTH;\n@@ -430,1 +432,0 @@\n-        \/\/ Exponent bias adjust in the representation is equal to MAX_EXPONENT.\n@@ -432,1 +433,1 @@\n-                                   ( ((exp + MAX_EXPONENT) << (PRECISION - 1)) + signif_bits ) ));\n+                                   ( ((exp + EXP_BIAS) << (PRECISION - 1)) + signif_bits) ));\n@@ -1534,2 +1535,2 @@\n-        int bin16ExpBits     = 0x0000_7c00 & bits;     \/\/ Five exponent bits.\n-        return (bin16ExpBits >> (PRECISION - 1)) - 15;\n+        int bin16ExpBits = EXP_BIT_MASK & bits; \/\/ Five exponent bits.\n+        return (bin16ExpBits >> (PRECISION - 1)) - EXP_BIAS;\n@@ -1690,2 +1691,1 @@\n-        final int MAX_SCALE = Float16.MAX_EXPONENT + -Float16.MIN_EXPONENT +\n-                Float16Consts.SIGNIFICAND_WIDTH + 1;\n+        final int MAX_SCALE = MAX_EXPONENT + -MIN_EXPONENT + SIGNIFICAND_WIDTH + 1;\n@@ -1728,3 +1728,2 @@\n-        return shortBitsToFloat16((short) ((float16ToRawShortBits(sign) & SIGN_BIT_MASK) |\n-                                           (float16ToRawShortBits(magnitude) &\n-                                            (EXP_BIT_MASK | SIGNIF_BIT_MASK) )));\n+        return shortBitsToFloat16((short)((float16ToRawShortBits(sign)      & SIGN_BIT_MASK) |\n+                                          (float16ToRawShortBits(magnitude) & MAG_BIT_MASK)));\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/Float16.java","additions":8,"deletions":9,"binary":false,"changes":17,"status":"modified"}]}