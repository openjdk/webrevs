{"files":[{"patch":"@@ -37,0 +37,1 @@\n+#include \"runtime\/globals.hpp\"\n@@ -41,0 +42,1 @@\n+#include \"utilities\/globalDefinitions.hpp\"\n@@ -61,0 +63,3 @@\n+  } else if (LockingMode == LM_LIGHTWEIGHT) {\n+    \/\/ null check obj. load_klass performs load if DiagnoseSyncOnValueBasedClasses != 0.\n+    testptr(hdr, Address(obj));\n@@ -63,3 +68,0 @@\n-  \/\/ Load object header\n-  movptr(hdr, Address(obj, hdr_offset));\n-\n@@ -76,0 +78,2 @@\n+    \/\/ Load object header\n+    movptr(hdr, Address(obj, hdr_offset));\n@@ -137,3 +141,8 @@\n-    movptr(disp_hdr, Address(obj, hdr_offset));\n-    andptr(disp_hdr, ~(int32_t)markWord::lock_mask_in_place);\n-    lightweight_unlock(obj, disp_hdr, hdr, slow_case);\n+#ifdef _LP64\n+    lightweight_unlock(obj, disp_hdr, r15_thread, hdr, slow_case);\n+#else\n+    \/\/ This relies on the implementation of lightweight_unlock knowing that it\n+    \/\/ will clobber its thread when using EAX.\n+    get_thread(disp_hdr);\n+    lightweight_unlock(obj, disp_hdr, disp_hdr, hdr, slow_case);\n+#endif\n","filename":"src\/hotspot\/cpu\/x86\/c1_MacroAssembler_x86.cpp","additions":15,"deletions":6,"binary":false,"changes":21,"status":"modified"},{"patch":"@@ -76,0 +76,77 @@\n+int C2FastUnlockLightweightStub::max_size() const {\n+  return 128;\n+}\n+\n+void C2FastUnlockLightweightStub::emit(C2_MacroAssembler& masm) {\n+  assert(_t == rax, \"must be\");\n+\n+  Label restore_held_monitor_count_and_slow_path;\n+\n+  { \/\/ Restore lock-stack and handle the unlock in runtime.\n+\n+    __ bind(_push_and_slow_path);\n+#ifdef ASSERT\n+    \/\/ The obj was only cleared in debug.\n+    __ movl(_t, Address(_thread, JavaThread::lock_stack_top_offset()));\n+    __ movptr(Address(_thread, _t), _obj);\n+#endif\n+    __ addl(Address(_thread, JavaThread::lock_stack_top_offset()), oopSize);\n+  }\n+\n+  { \/\/ Restore held monitor and slow path.\n+\n+    __ bind(restore_held_monitor_count_and_slow_path);\n+    \/\/ Restore held monitor count.\n+    __ increment(Address(_thread, JavaThread::held_monitor_count_offset()));\n+    \/\/ increment will always result in ZF = 0 (no overflows).\n+    \/\/ continuation is the slow_path.\n+    __ jmp(continuation());\n+  }\n+\n+  { \/\/ Handle monitor medium path.\n+\n+    __ bind(_check_successor);\n+\n+    Label fix_zf_and_unlocked;\n+    const Register monitor = _mark;\n+\n+#ifndef _LP64\n+    \/\/ The owner may be anonymous, see comment in x86_64 section.\n+    __ movptr(Address(monitor, OM_OFFSET_NO_MONITOR_VALUE_TAG(owner)), _thread);\n+    __ jmpb(restore_held_monitor_count_and_slow_path);\n+#else \/\/ _LP64\n+    \/\/ The owner may be anonymous and we removed the last obj entry in\n+    \/\/ the lock-stack. This loses the information about the owner.\n+    \/\/ Write the thread to the owner field so the runtime knows the owner.\n+    __ movptr(Address(monitor, OM_OFFSET_NO_MONITOR_VALUE_TAG(owner)), _thread);\n+\n+    \/\/ successor null check.\n+    __ cmpptr(Address(monitor, OM_OFFSET_NO_MONITOR_VALUE_TAG(succ)), NULL_WORD);\n+    __ jccb(Assembler::equal, restore_held_monitor_count_and_slow_path);\n+\n+    \/\/ Release lock.\n+    __ movptr(Address(monitor, OM_OFFSET_NO_MONITOR_VALUE_TAG(owner)), NULL_WORD);\n+\n+    \/\/ Fence.\n+    __ lock(); __ addl(Address(rsp, 0), 0);\n+\n+    \/\/ Recheck successor.\n+    __ cmpptr(Address(monitor, OM_OFFSET_NO_MONITOR_VALUE_TAG(succ)), NULL_WORD);\n+    \/\/ Seen a successor after the release -> fence we have handed of the monitor\n+    __ jccb(Assembler::notEqual, fix_zf_and_unlocked);\n+\n+    \/\/ Try to relock, if it fail the monitor has been handed over\n+    \/\/ TODO: Caveat, this may fail due to deflation, which does\n+    \/\/       not handle the monitor handoff. Currently only works\n+    \/\/       due to the responsible thread.\n+    __ xorptr(rax, rax);\n+    __ lock(); __ cmpxchgptr(_thread, Address(monitor, OM_OFFSET_NO_MONITOR_VALUE_TAG(owner)));\n+    __ jccb  (Assembler::equal, restore_held_monitor_count_and_slow_path);\n+#endif\n+\n+    __ bind(fix_zf_and_unlocked);\n+    __ xorl(rax, rax);\n+    __ jmp(unlocked());\n+  }\n+}\n+\n","filename":"src\/hotspot\/cpu\/x86\/c2_CodeStubs_x86.cpp","additions":77,"deletions":0,"binary":false,"changes":77,"status":"modified"},{"patch":"@@ -36,0 +36,1 @@\n+#include \"runtime\/globals.hpp\"\n@@ -39,0 +40,3 @@\n+#include \"utilities\/globalDefinitions.hpp\"\n+#include \"utilities\/powerOfTwo.hpp\"\n+#include \"utilities\/sizes.hpp\"\n@@ -557,0 +561,1 @@\n+  assert(LockingMode != LM_LIGHTWEIGHT, \"uses fast_lock_lightweight\");\n@@ -608,1 +613,2 @@\n-  } else if (LockingMode == LM_LEGACY) {\n+  } else {\n+    assert(LockingMode == LM_LEGACY, \"\");\n@@ -623,4 +629,0 @@\n-  } else {\n-    assert(LockingMode == LM_LIGHTWEIGHT, \"\");\n-    lightweight_lock(objReg, tmpReg, thread, scrReg, NO_COUNT);\n-    jmp(COUNT);\n@@ -757,0 +759,1 @@\n+  assert(LockingMode != LM_LIGHTWEIGHT, \"uses fast_unlock_lightweight\");\n@@ -787,17 +790,0 @@\n-  if (LockingMode == LM_LIGHTWEIGHT) {\n-    \/\/ If the owner is ANONYMOUS, we need to fix it -  in an outline stub.\n-    testb(Address(tmpReg, OM_OFFSET_NO_MONITOR_VALUE_TAG(owner)), (int32_t) ObjectMonitor::ANONYMOUS_OWNER);\n-#ifdef _LP64\n-    if (!Compile::current()->output()->in_scratch_emit_size()) {\n-      C2HandleAnonOMOwnerStub* stub = new (Compile::current()->comp_arena()) C2HandleAnonOMOwnerStub(tmpReg, boxReg);\n-      Compile::current()->output()->add_stub(stub);\n-      jcc(Assembler::notEqual, stub->entry());\n-      bind(stub->continuation());\n-    } else\n-#endif\n-    {\n-      \/\/ We can't easily implement this optimization on 32 bit because we don't have a thread register.\n-      \/\/ Call the slow-path instead.\n-      jcc(Assembler::notEqual, NO_COUNT);\n-    }\n-  }\n@@ -925,1 +911,1 @@\n-  if (LockingMode != LM_MONITOR) {\n+  if (LockingMode == LM_LEGACY) {\n@@ -927,9 +913,3 @@\n-    if (LockingMode == LM_LIGHTWEIGHT) {\n-      mov(boxReg, tmpReg);\n-      lightweight_unlock(objReg, boxReg, tmpReg, NO_COUNT);\n-      jmp(COUNT);\n-    } else if (LockingMode == LM_LEGACY) {\n-      movptr(tmpReg, Address (boxReg, 0));      \/\/ re-fetch\n-      lock();\n-      cmpxchgptr(tmpReg, Address(objReg, oopDesc::mark_offset_in_bytes())); \/\/ Uses RAX which is box\n-    }\n+    movptr(tmpReg, Address (boxReg, 0));      \/\/ re-fetch\n+    lock();\n+    cmpxchgptr(tmpReg, Address(objReg, oopDesc::mark_offset_in_bytes())); \/\/ Uses RAX which is box\n@@ -938,0 +918,1 @@\n+\n@@ -958,0 +939,241 @@\n+void C2_MacroAssembler::fast_lock_lightweight(Register obj, Register box, Register rax_reg,\n+                                              Register t, Register thread) {\n+  assert(LockingMode == LM_LIGHTWEIGHT, \"must be\");\n+  assert(rax_reg == rax, \"Used for CAS\");\n+  assert_different_registers(obj, box, rax_reg, t, thread);\n+\n+  \/\/ Handle inflated monitor.\n+  Label inflated;\n+  \/\/ Finish fast lock successfully. ZF value is irrelevant.\n+  Label locked;\n+  \/\/ Finish fast lock unsuccessfully. MUST jump with ZF == 0\n+  Label slow_path;\n+\n+  if (DiagnoseSyncOnValueBasedClasses != 0) {\n+    load_klass(rax_reg, obj, t);\n+    movl(rax_reg, Address(rax_reg, Klass::access_flags_offset()));\n+    testl(rax_reg, JVM_ACC_IS_VALUE_BASED_CLASS);\n+    jcc(Assembler::notZero, slow_path);\n+  }\n+\n+  const Register mark = t;\n+\n+  { \/\/ Lightweight Lock\n+\n+    Label push;\n+\n+    const Register top = box;\n+\n+    \/\/ Load the mark.\n+    movptr(mark, Address(obj, oopDesc::mark_offset_in_bytes()));\n+\n+    \/\/ Prefetch top.\n+    movl(top, Address(thread, JavaThread::lock_stack_top_offset()));\n+\n+    \/\/ Check for monitor (0b10).\n+    testptr(mark, markWord::monitor_value);\n+    jcc(Assembler::notZero, inflated);\n+\n+    \/\/ Check if lock-stack is full.\n+    cmpl(top, LockStack::end_offset() - 1);\n+    jcc(Assembler::greater, slow_path);\n+\n+    \/\/ Check if recursive.\n+    cmpptr(obj, Address(thread, top, Address::times_1, -oopSize));\n+    jccb(Assembler::equal, push);\n+\n+    \/\/ Try to lock. Transition lock bits 0b01 => 0b00\n+    movptr(rax_reg, mark);\n+    orptr(rax_reg, markWord::unlocked_value);\n+    andptr(mark, ~(int32_t)markWord::unlocked_value);\n+    lock(); cmpxchgptr(mark, Address(obj, oopDesc::mark_offset_in_bytes()));\n+    jcc(Assembler::notEqual, slow_path);\n+\n+    bind(push);\n+    \/\/ After successful lock, push object on lock-stack.\n+    movptr(Address(thread, top), obj);\n+    addl(Address(thread, JavaThread::lock_stack_top_offset()), oopSize);\n+    jmpb(locked);\n+  }\n+\n+  { \/\/ Handle inflated monitor.\n+    bind(inflated);\n+\n+    const Register tagged_monitor = mark;\n+\n+    \/\/ CAS owner (null => current thread).\n+    xorptr(rax_reg, rax_reg);\n+    lock(); cmpxchgptr(thread, Address(tagged_monitor, OM_OFFSET_NO_MONITOR_VALUE_TAG(owner)));\n+    jccb(Assembler::equal, locked);\n+\n+    \/\/ Check if recursive.\n+    cmpptr(thread, rax_reg);\n+    jccb(Assembler::notEqual, slow_path);\n+\n+    \/\/ Recursive.\n+    increment(Address(tagged_monitor, OM_OFFSET_NO_MONITOR_VALUE_TAG(recursions)));\n+  }\n+\n+  bind(locked);\n+  increment(Address(thread, JavaThread::held_monitor_count_offset()));\n+  \/\/ Set ZF = 1\n+  xorl(rax_reg, rax_reg);\n+\n+#ifdef ASSERT\n+  \/\/ Check that locked label is reached with ZF set.\n+  Label zf_correct;\n+  jccb(Assembler::zero, zf_correct);\n+  stop(\"Fast Lock ZF != 1\");\n+#endif\n+\n+  bind(slow_path);\n+#ifdef ASSERT\n+  \/\/ Check that slow_path label is reached with ZF not set.\n+  jccb(Assembler::notZero, zf_correct);\n+  stop(\"Fast Lock ZF != 0\");\n+  bind(zf_correct);\n+#endif\n+  \/\/ C2 uses the value of ZF to determine the continuation.\n+}\n+\n+void C2_MacroAssembler::fast_unlock_lightweight(Register obj, Register reg_rax, Register t, Register thread) {\n+  assert(LockingMode == LM_LIGHTWEIGHT, \"must be\");\n+  assert(reg_rax == rax, \"Used for CAS\");\n+  assert_different_registers(obj, reg_rax, t);\n+\n+  \/\/ Handle inflated monitor.\n+  Label inflated, inflated_check_lock_stack;\n+  \/\/ Finish fast unlock successfully.  MUST jump with ZF == 1\n+  Label unlocked;\n+\n+  \/\/ Assume success.\n+  decrement(Address(thread, JavaThread::held_monitor_count_offset()));\n+\n+  const Register mark = t;\n+  const Register top = reg_rax;\n+\n+  Label dummy;\n+  C2FastUnlockLightweightStub* stub = nullptr;\n+\n+  if (!Compile::current()->output()->in_scratch_emit_size()) {\n+    stub = new (Compile::current()->comp_arena()) C2FastUnlockLightweightStub(obj, mark, reg_rax, thread);\n+    Compile::current()->output()->add_stub(stub);\n+  }\n+\n+  Label& push_and_slow_path = stub == nullptr ? dummy : stub->push_and_slow_path();\n+  Label& check_successor = stub == nullptr ? dummy : stub->check_successor();\n+\n+  { \/\/ Lightweight Unlock\n+\n+    \/\/ Load top.\n+    movl(top, Address(thread, JavaThread::lock_stack_top_offset()));\n+\n+    \/\/ Prefetch mark.\n+    movptr(mark, Address(obj, oopDesc::mark_offset_in_bytes()));\n+\n+    \/\/ Check if obj is top of lock-stack.\n+    cmpptr(obj, Address(thread, top, Address::times_1, -oopSize));\n+    \/\/ Top of lock stack was not obj. Must be monitor.\n+    jcc(Assembler::notEqual, inflated_check_lock_stack);\n+\n+    \/\/ Pop lock-stack.\n+    DEBUG_ONLY(movptr(Address(thread, top, Address::times_1, -oopSize), 0);)\n+    subl(Address(thread, JavaThread::lock_stack_top_offset()), oopSize);\n+\n+    \/\/ Check if recursive.\n+    cmpptr(obj, Address(thread, top, Address::times_1, -2 * oopSize));\n+    jcc(Assembler::equal, unlocked);\n+\n+    \/\/ We elide the monitor check, let the CAS fail instead.\n+\n+    \/\/ Try to unlock. Transition lock bits 0b00 => 0b01\n+    movptr(reg_rax, mark);\n+    andptr(reg_rax, ~(int32_t)markWord::lock_mask);\n+    orptr(mark, markWord::unlocked_value);\n+    lock(); cmpxchgptr(mark, Address(obj, oopDesc::mark_offset_in_bytes()));\n+    jcc(Assembler::notEqual, push_and_slow_path);\n+    jmp(unlocked);\n+  }\n+\n+\n+  { \/\/ Handle inflated monitor.\n+    bind(inflated_check_lock_stack);\n+#ifdef ASSERT\n+    Label check_done;\n+    subl(top, oopSize);\n+    cmpl(top, in_bytes(JavaThread::lock_stack_base_offset()));\n+    jcc(Assembler::below, check_done);\n+    cmpptr(obj, Address(thread, top));\n+    jccb(Assembler::notEqual, inflated_check_lock_stack);\n+    stop(\"Fast Unlock lock on stack\");\n+    bind(check_done);\n+    testptr(mark, markWord::monitor_value);\n+    jccb(Assembler::notZero, inflated);\n+    stop(\"Fast Unlock not monitor\");\n+#endif\n+\n+    bind(inflated);\n+\n+    \/\/ mark contains the tagged ObjectMonitor*.\n+    const Register monitor = mark;\n+\n+#ifndef _LP64\n+    \/\/ Check if recursive.\n+    xorptr(reg_rax, reg_rax);\n+    orptr(reg_rax, Address(monitor, OM_OFFSET_NO_MONITOR_VALUE_TAG(recursions)));\n+    jcc(Assembler::notZero, check_successor);\n+\n+    \/\/ Check if the entry lists are empty.\n+    movptr(reg_rax, Address(monitor, OM_OFFSET_NO_MONITOR_VALUE_TAG(EntryList)));\n+    orptr(reg_rax, Address(monitor, OM_OFFSET_NO_MONITOR_VALUE_TAG(cxq)));\n+    jcc(Assembler::notZero, check_successor);\n+\n+    \/\/ Release lock.\n+    movptr(Address(monitor, OM_OFFSET_NO_MONITOR_VALUE_TAG(owner)), NULL_WORD);\n+#else \/\/ _LP64\n+    Label recursive;\n+\n+    \/\/ Check if recursive.\n+    cmpptr(Address(monitor, OM_OFFSET_NO_MONITOR_VALUE_TAG(recursions)), 0);\n+    jccb(Assembler::notEqual, recursive);\n+\n+    \/\/ Check if the entry lists are empty.\n+    movptr(reg_rax, Address(monitor, OM_OFFSET_NO_MONITOR_VALUE_TAG(cxq)));\n+    orptr(reg_rax, Address(monitor, OM_OFFSET_NO_MONITOR_VALUE_TAG(EntryList)));\n+    jcc(Assembler::notZero, check_successor);\n+\n+    \/\/ Release lock.\n+    movptr(Address(monitor, OM_OFFSET_NO_MONITOR_VALUE_TAG(owner)), NULL_WORD);\n+    jmpb(unlocked);\n+\n+    \/\/ Recursive unlock.\n+    bind(recursive);\n+    decrement(Address(monitor, OM_OFFSET_NO_MONITOR_VALUE_TAG(recursions)));\n+    xorl(t, t);\n+#endif\n+  }\n+\n+  bind(unlocked);\n+  if (stub != nullptr) {\n+    bind(stub->unlocked());\n+  }\n+\n+#ifdef ASSERT\n+  \/\/ Check that unlocked label is reached with ZF set.\n+  Label zf_correct;\n+  jccb(Assembler::zero, zf_correct);\n+  stop(\"Fast Unlock ZF != 1\");\n+#endif\n+\n+  if (stub != nullptr) {\n+    bind(stub->continuation());\n+  }\n+#ifdef ASSERT\n+  \/\/ Check that stub->continuation() label is reached with ZF not set.\n+  jccb(Assembler::notZero, zf_correct);\n+  stop(\"Fast Unlock ZF != 0\");\n+  bind(zf_correct);\n+#endif\n+  \/\/ C2 uses the value of ZF to determine the continuation.\n+}\n+\n","filename":"src\/hotspot\/cpu\/x86\/c2_MacroAssembler_x86.cpp","additions":254,"deletions":32,"binary":false,"changes":286,"status":"modified"},{"patch":"@@ -46,0 +46,4 @@\n+  void fast_lock_lightweight(Register obj, Register box, Register rax_reg,\n+                             Register t, Register thread);\n+  void fast_unlock_lightweight(Register obj, Register reg_rax, Register t, Register thread);\n+\n","filename":"src\/hotspot\/cpu\/x86\/c2_MacroAssembler_x86.hpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -1239,2 +1239,0 @@\n-      \/\/ Load object header, prepare for CAS from unlocked to locked.\n-      movptr(swap_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n@@ -1358,1 +1356,1 @@\n-      const Register thread = r15_thread;\n+      lightweight_unlock(obj_reg, swap_reg, r15_thread, header_reg, slow_case);\n@@ -1360,2 +1358,4 @@\n-      const Register thread = header_reg;\n-      get_thread(thread);\n+      \/\/ This relies on the implementation of lightweight_unlock knowing that it\n+      \/\/ will clobber its thread when using EAX.\n+      get_thread(swap_reg);\n+      lightweight_unlock(obj_reg, swap_reg, swap_reg, header_reg, slow_case);\n@@ -1363,9 +1363,0 @@\n-      \/\/ Handle unstructured locking.\n-      Register tmp = swap_reg;\n-      movl(tmp, Address(thread, JavaThread::lock_stack_top_offset()));\n-      cmpptr(obj_reg, Address(thread, tmp, Address::times_1,  -oopSize));\n-      jcc(Assembler::notEqual, slow_case);\n-      \/\/ Try to swing header from locked to unlocked.\n-      movptr(swap_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n-      andptr(swap_reg, ~(int32_t)markWord::lock_mask_in_place);\n-      lightweight_unlock(obj_reg, swap_reg, header_reg, slow_case);\n","filename":"src\/hotspot\/cpu\/x86\/interp_masm_x86.cpp","additions":5,"deletions":14,"binary":false,"changes":19,"status":"modified"},{"patch":"@@ -9817,2 +9817,0 @@\n-\/\/ Branches to slow upon failure to lock the object, with ZF cleared.\n-\/\/ Falls through upon success with unspecified ZF.\n@@ -9821,1 +9819,1 @@\n-\/\/ hdr: the (pre-loaded) header of the object, must be rax\n+\/\/ reg_rax: rax\n@@ -9824,19 +9822,28 @@\n-void MacroAssembler::lightweight_lock(Register obj, Register hdr, Register thread, Register tmp, Label& slow) {\n-  assert(hdr == rax, \"header must be in rax for cmpxchg\");\n-  assert_different_registers(obj, hdr, thread, tmp);\n-\n-  \/\/ First we need to check if the lock-stack has room for pushing the object reference.\n-  \/\/ Note: we subtract 1 from the end-offset so that we can do a 'greater' comparison, instead\n-  \/\/ of 'greaterEqual' below, which readily clears the ZF. This makes C2 code a little simpler and\n-  \/\/ avoids one branch.\n-  cmpl(Address(thread, JavaThread::lock_stack_top_offset()), LockStack::end_offset() - 1);\n-  jcc(Assembler::greater, slow);\n-\n-  \/\/ Now we attempt to take the fast-lock.\n-  \/\/ Clear lock_mask bits (locked state).\n-  andptr(hdr, ~(int32_t)markWord::lock_mask_in_place);\n-  movptr(tmp, hdr);\n-  \/\/ Set unlocked_value bit.\n-  orptr(hdr, markWord::unlocked_value);\n-  lock();\n-  cmpxchgptr(tmp, Address(obj, oopDesc::mark_offset_in_bytes()));\n+void MacroAssembler::lightweight_lock(Register obj, Register reg_rax, Register thread, Register tmp, Label& slow) {\n+  assert(reg_rax == rax, \"\");\n+  assert_different_registers(obj, reg_rax, thread, tmp);\n+\n+  Label push;\n+  const Register top = tmp;\n+\n+  \/\/ Load top.\n+  movl(top, Address(thread, JavaThread::lock_stack_top_offset()));\n+\n+  \/\/ Check if the lock-stack is full.\n+  cmpl(top, LockStack::end_offset());\n+  jcc(Assembler::greaterEqual, slow);\n+\n+  \/\/ Check for recursion.\n+  cmpptr(obj, Address(thread, top, Address::times_1, -oopSize));\n+  jcc(Assembler::equal, push);\n+\n+  \/\/ Check header for monitor (0b10).\n+  movptr(reg_rax, Address(obj, oopDesc::mark_offset_in_bytes()));\n+  testptr(reg_rax, markWord::monitor_value);\n+  jcc(Assembler::notZero, slow);\n+\n+  \/\/ Try to lock. Transition lock bits 0b01 => 0b00\n+  movptr(tmp, reg_rax);\n+  andptr(tmp, ~(int32_t)markWord::unlocked_value);\n+  orptr(reg_rax, markWord::unlocked_value);\n+  lock(); cmpxchgptr(tmp, Address(obj, oopDesc::mark_offset_in_bytes()));\n@@ -9845,5 +9852,8 @@\n-  \/\/ If successful, push object to lock-stack.\n-  movl(tmp, Address(thread, JavaThread::lock_stack_top_offset()));\n-  movptr(Address(thread, tmp), obj);\n-  incrementl(tmp, oopSize);\n-  movl(Address(thread, JavaThread::lock_stack_top_offset()), tmp);\n+  \/\/ Restore top, CAS clobbers register.\n+  movl(top, Address(thread, JavaThread::lock_stack_top_offset()));\n+\n+  bind(push);\n+  \/\/ After successful lock, push object on lock-stack.\n+  movptr(Address(thread, top), obj);\n+  incrementl(top, oopSize);\n+  movl(Address(thread, JavaThread::lock_stack_top_offset()), top);\n@@ -9853,2 +9863,0 @@\n-\/\/ Branches to slow upon failure, with ZF cleared.\n-\/\/ Falls through upon success, with unspecified ZF.\n@@ -9857,1 +9865,2 @@\n-\/\/ hdr: the (pre-loaded) header of the object, must be rax\n+\/\/ reg_rax: rax\n+\/\/ thread: the thread, may be EAX on x86_32\n@@ -9859,3 +9868,4 @@\n-void MacroAssembler::lightweight_unlock(Register obj, Register hdr, Register tmp, Label& slow) {\n-  assert(hdr == rax, \"header must be in rax for cmpxchg\");\n-  assert_different_registers(obj, hdr, tmp);\n+void MacroAssembler::lightweight_unlock(Register obj, Register reg_rax, Register thread, Register tmp, Label& slow) {\n+  assert(reg_rax == rax, \"\");\n+  assert_different_registers(obj, reg_rax, tmp);\n+  LP64_ONLY(assert_different_registers(obj, reg_rax, thread, tmp);)\n@@ -9863,5 +9873,6 @@\n-  \/\/ Mark-word must be lock_mask now, try to swing it back to unlocked_value.\n-  movptr(tmp, hdr); \/\/ The expected old value\n-  orptr(tmp, markWord::unlocked_value);\n-  lock();\n-  cmpxchgptr(tmp, Address(obj, oopDesc::mark_offset_in_bytes()));\n+  Label unlocked, push_and_slow;\n+  const Register top = tmp;\n+\n+  \/\/ Check if obj is top of lock-stack.\n+  movl(top, Address(thread, JavaThread::lock_stack_top_offset()));\n+  cmpptr(obj, Address(thread, top, Address::times_1, -oopSize));\n@@ -9869,7 +9880,3 @@\n-  \/\/ Pop the lock object from the lock-stack.\n-#ifdef _LP64\n-  const Register thread = r15_thread;\n-#else\n-  const Register thread = rax;\n-  get_thread(thread);\n-#endif\n+\n+  \/\/ Pop lock-stack.\n+  DEBUG_ONLY(movptr(Address(thread, top, Address::times_1, -oopSize), 0);)\n@@ -9877,0 +9884,31 @@\n+\n+  \/\/ Check if recursive.\n+  cmpptr(obj, Address(thread, top, Address::times_1, -2 * oopSize));\n+  jcc(Assembler::equal, unlocked);\n+\n+  \/\/ Not recursive. Check header for monitor (0b10).\n+  movptr(reg_rax, Address(obj, oopDesc::mark_offset_in_bytes()));\n+  testptr(reg_rax, markWord::monitor_value);\n+  jcc(Assembler::notZero, push_and_slow);\n+\n+#ifdef ASSERT\n+  \/\/ Check header not unlocked (0b01).\n+  Label not_unlocked;\n+  testptr(reg_rax, markWord::unlocked_value);\n+  jcc(Assembler::zero, not_unlocked);\n+  stop(\"lightweight_unlock already unlocked\");\n+  bind(not_unlocked);\n+#endif\n+\n+  \/\/ Try to unlock. Transition lock bits 0b00 => 0b01\n+  movptr(tmp, reg_rax);\n+  orptr(tmp, markWord::unlocked_value);\n+  lock(); cmpxchgptr(tmp, Address(obj, oopDesc::mark_offset_in_bytes()));\n+  jcc(Assembler::equal, unlocked);\n+\n+  bind(push_and_slow);\n+  \/\/ Restore lock-stack and handle the unlock in runtime.\n+  if (thread == reg_rax) {\n+    \/\/ On x86_32 we may lose the thread.\n+    get_thread(thread);\n+  }\n@@ -9878,2 +9916,2 @@\n-  movl(tmp, Address(thread, JavaThread::lock_stack_top_offset()));\n-  movptr(Address(thread, tmp), 0);\n+  movl(top, Address(thread, JavaThread::lock_stack_top_offset()));\n+  movptr(Address(thread, top), obj);\n@@ -9881,0 +9919,4 @@\n+  addl(Address(thread, JavaThread::lock_stack_top_offset()), oopSize);\n+  jmp(slow);\n+\n+  bind(unlocked);\n","filename":"src\/hotspot\/cpu\/x86\/macroAssembler_x86.cpp","additions":89,"deletions":47,"binary":false,"changes":136,"status":"modified"},{"patch":"@@ -2029,2 +2029,2 @@\n-  void lightweight_lock(Register obj, Register hdr, Register thread, Register tmp, Label& slow);\n-  void lightweight_unlock(Register obj, Register hdr, Register tmp, Label& slow);\n+  void lightweight_lock(Register obj, Register reg_rax, Register thread, Register tmp, Label& slow);\n+  void lightweight_unlock(Register obj, Register reg_rax, Register thread, Register tmp, Label& slow);\n","filename":"src\/hotspot\/cpu\/x86\/macroAssembler_x86.hpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -1718,2 +1718,0 @@\n-      \/\/ Load object header\n-      __ movptr(swap_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n@@ -1877,3 +1875,1 @@\n-      __ movptr(swap_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n-      __ andptr(swap_reg, ~(int32_t)markWord::lock_mask_in_place);\n-      __ lightweight_unlock(obj_reg, swap_reg, lock_reg, slow_path_unlock);\n+      __ lightweight_unlock(obj_reg, swap_reg, thread, lock_reg, slow_path_unlock);\n","filename":"src\/hotspot\/cpu\/x86\/sharedRuntime_x86_32.cpp","additions":1,"deletions":5,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -2189,2 +2189,0 @@\n-      \/\/ Load object header\n-      __ movptr(swap_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n@@ -2333,3 +2331,1 @@\n-      __ movptr(swap_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n-      __ andptr(swap_reg, ~(int32_t)markWord::lock_mask_in_place);\n-      __ lightweight_unlock(obj_reg, swap_reg, lock_reg, slow_path_unlock);\n+      __ lightweight_unlock(obj_reg, swap_reg, r15_thread, lock_reg, slow_path_unlock);\n","filename":"src\/hotspot\/cpu\/x86\/sharedRuntime_x86_64.cpp","additions":1,"deletions":5,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -769,0 +769,4 @@\n+  constexpr static bool supports_recursive_lightweight_locking() {\n+    return true;\n+  }\n+\n","filename":"src\/hotspot\/cpu\/x86\/vm_version_x86.hpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -13781,1 +13781,1 @@\n-  predicate(!Compile::current()->use_rtm());\n+  predicate(LockingMode != LM_LIGHTWEIGHT && !Compile::current()->use_rtm());\n@@ -13795,0 +13795,1 @@\n+  predicate(LockingMode != LM_LIGHTWEIGHT);\n@@ -13805,0 +13806,26 @@\n+instruct cmpFastLockLightweight(eFlagsReg cr, eRegP object, eBXRegP box, eAXRegI tmp, eRegP scr, eRegP thread) %{\n+  predicate(LockingMode == LM_LIGHTWEIGHT);\n+  match(Set cr (FastLock object box));\n+  effect(TEMP tmp, TEMP scr, USE_KILL box, TEMP thread);\n+  ins_cost(300);\n+  format %{ \"FASTLOCK $object,$box\\t! kills $box,$tmp,$scr\" %}\n+  ins_encode %{\n+    __ get_thread($thread$$Register);\n+    __ fast_lock_lightweight($object$$Register, $box$$Register, $tmp$$Register, $scr$$Register, $thread$$Register);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct cmpFastUnlockLightweight(eFlagsReg cr, eRegP object, eAXRegP box, eRegP tmp, eRegP thread) %{\n+  predicate(LockingMode == LM_LIGHTWEIGHT);\n+  match(Set cr (FastUnlock object box));\n+  effect(TEMP tmp, USE_KILL box, TEMP thread);\n+  ins_cost(300);\n+  format %{ \"FASTUNLOCK $object,$box\\t! kills $box,$tmp\" %}\n+  ins_encode %{\n+    __ get_thread($thread$$Register);\n+    __ fast_unlock_lightweight($object$$Register, $box$$Register, $tmp$$Register, $thread$$Register);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n","filename":"src\/hotspot\/cpu\/x86\/x86_32.ad","additions":28,"deletions":1,"binary":false,"changes":29,"status":"modified"},{"patch":"@@ -13418,1 +13418,1 @@\n-  predicate(!Compile::current()->use_rtm());\n+  predicate(LockingMode != LM_LIGHTWEIGHT && !Compile::current()->use_rtm());\n@@ -13431,0 +13431,1 @@\n+  predicate(LockingMode != LM_LIGHTWEIGHT);\n@@ -13441,0 +13442,24 @@\n+instruct cmpFastLockLightweight(rFlagsReg cr, rRegP object, rbx_RegP box, rax_RegI tmp, rRegP scr) %{\n+  predicate(LockingMode == LM_LIGHTWEIGHT);\n+  match(Set cr (FastLock object box));\n+  effect(TEMP tmp, TEMP scr, USE_KILL box);\n+  ins_cost(300);\n+  format %{ \"fastlock $object,$box\\t! kills $box,$tmp,$scr\" %}\n+  ins_encode %{\n+    __ fast_lock_lightweight($object$$Register, $box$$Register, $tmp$$Register, $scr$$Register, r15_thread);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct cmpFastUnlockLightweight(rFlagsReg cr, rRegP object, rax_RegP box, rRegP tmp) %{\n+  predicate(LockingMode == LM_LIGHTWEIGHT);\n+  match(Set cr (FastUnlock object box));\n+  effect(TEMP tmp, USE_KILL box);\n+  ins_cost(300);\n+  format %{ \"fastunlock $object,$box\\t! kills $box,$tmp\" %}\n+  ins_encode %{\n+    __ fast_unlock_lightweight($object$$Register, $box$$Register, $tmp$$Register, r15_thread);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n","filename":"src\/hotspot\/cpu\/x86\/x86_64.ad","additions":26,"deletions":1,"binary":false,"changes":27,"status":"modified"},{"patch":"@@ -100,0 +100,19 @@\n+class C2FastUnlockLightweightStub : public C2CodeStub {\n+private:\n+  Register _obj;\n+  Register _mark;\n+  Register _t;\n+  Register _thread;\n+  Label _push_and_slow_path;\n+  Label _check_successor;\n+  Label _unlocked;\n+public:\n+  C2FastUnlockLightweightStub(Register obj, Register mark, Register t, Register thread) : C2CodeStub(),\n+    _obj(obj), _mark(mark), _t(t), _thread(thread) {}\n+  int max_size() const;\n+  void emit(C2_MacroAssembler& masm);\n+  Label& push_and_slow_path() { return _push_and_slow_path; }\n+  Label& check_successor() { return _check_successor; }\n+  Label& unlocked() { return _unlocked; }\n+};\n+\n","filename":"src\/hotspot\/share\/opto\/c2_CodeStubs.hpp","additions":19,"deletions":0,"binary":false,"changes":19,"status":"modified"}]}