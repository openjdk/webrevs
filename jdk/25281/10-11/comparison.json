{"files":[{"patch":"@@ -2610,0 +2610,17 @@\n+  void increase_counter_128(Register counter, Register tmp) {\n+    __ addi(t0, counter, 8);\n+    __ ld(tmp, Address(t0));\n+    __ rev8(tmp, tmp);\n+    __ addi(tmp, tmp, 1);\n+    __ rev8(tmp, tmp);\n+    __ sd(tmp, Address(t0));\n+    __ mv(t0, 0x0ul);\n+    __ sltu(tmp, t0, tmp);\n+    __ xori(t0, tmp, 1);\n+    __ ld(tmp, Address(counter));\n+    __ rev8(tmp, tmp);\n+    __ add(tmp, tmp, t0);\n+    __ rev8(tmp, tmp);\n+    __ sd(tmp, Address(counter));\n+  }\n+\n@@ -2662,0 +2679,34 @@\n+    \/\/ Algorithm:\n+    \/\/\n+    \/\/ if (len == 0) {\n+    \/\/   goto L_exit;\n+    \/\/ } else {\n+    \/\/   generate_aes_loadkeys();\n+    \/\/\n+    \/\/   L_encrypt_next:\n+    \/\/     while (used < block_size) {\n+    \/\/       if (len == 0) goto L_exit;\n+    \/\/       out[outOff++] = (byte)(in[inOff++] ^ saved_encrypted_ctr[used++]);\n+    \/\/       len--;\n+    \/\/     }\n+    \/\/\n+    \/\/   L_main:\n+    \/\/     saved_encrypted_ctr = aes_encrypt(counter);\n+    \/\/     increase_counter(counter);\n+    \/\/     if (len < block_size) {\n+    \/\/       used = 0;\n+    \/\/       goto L_encrypt_next;\n+    \/\/     }\n+    \/\/     v_in = load_16Byte(in);\n+    \/\/     v_out = load_16Byte(out);\n+    \/\/     v_saved_encrypted_ctr = load_16Byte(saved_encrypted_ctr);\n+    \/\/     v_out = v_in ^ v_saved_encrypted_ctr;\n+    \/\/     out += block_size;\n+    \/\/     in += block_size;\n+    \/\/     len -= block_size;\n+    \/\/     goto L_main;\n+    \/\/ }\n+    \/\/\n+    \/\/ L_exit:\n+    \/\/   return result;\n+\n@@ -2666,6 +2717,0 @@\n-    \/\/ Init 0b01010101... v0 mask for counter increase\n-    uint64_t maskIndex = 0xaaul;\n-    __ vsetvli(t0, x0, Assembler::e8, Assembler::m1);\n-    __ mv(t0, maskIndex);\n-    __ vmv_v_x(v0, t0);\n-\n@@ -2712,2 +2757,0 @@\n-    Label L_first_loop, L_loop, L_calculate_one_next;\n-\n@@ -2715,100 +2758,0 @@\n-    \/\/ Check whether the counter increases overflow 64 bit and changes to the scalar path,\n-    \/\/ most of time the counter may not increase over 64 bit,\n-    \/\/ but when counter starts with a large init number, we may encounter overflow one time\n-    \/\/ at most, just calculate it on the scalar path, and then go back to the large block\n-    __ addi(t0, counter, 8);\n-    __ ld(tmp, Address(t0));\n-    __ rev8(tmp, tmp);\n-    __ srli(t0, len, 4);\n-    __ addi(t0, t0, 1);\n-    __ add(t0, tmp, t0);\n-    __ bltu(t0, tmp, L_calculate_one_next);\n-\n-    \/\/ Calculate the number of 16 Bytes for CTR large block as t0.\n-    \/\/ Because of zvkned need sew as e32, so we save t0 * 4 into len32.\n-    \/\/ After that we save the data length < 16 back into len,\n-    \/\/ and calculate them one by one in L_next later.\n-    __ srli(t0, len, 4);\n-    __ slli(len32, t0, 2);\n-    __ slli(t0, len32, 2);\n-    __ sub(len, len, t0);\n-\n-    \/\/ We may still have fewer than 16 Bytes data at beginning.\n-    \/\/ So we need to calculate next counter and encryptedCounter\n-    __ beqz(len32, L_calculate_one_next);\n-\n-    \/\/ AES\/CTR large block loop\n-    \/\/ Init large block counter group\n-    __ vsetivli(x0, 2, Assembler::e64, Assembler::m1);\n-    __ vle64_v(v31, counter);\n-    \/\/ Convert the big-endian counter into little-endian for increment\n-    __ vrev8_v(v31, v31, Assembler::VectorMask::v0_t);\n-    __ vsetvli(x0, len32, Assembler::e32, Assembler::m4);\n-    __ vmv_v_i(v16, 0);\n-    __ vaesz_vs(v16, v31);\n-    __ srli(t0, len32, 1);\n-    __ vsetvli(x0, t0, Assembler::e64, Assembler::m4);\n-    __ viota_m(v20, v0, Assembler::VectorMask::v0_t);\n-    __ vadd_vv(v16, v16, v20, Assembler::VectorMask::v0_t);\n-    __ j(L_first_loop);\n-\n-    __ bind(L_loop);\n-    __ srli(t0, len32, 1);\n-    __ vsetvli(x0, t0, Assembler::e64, Assembler::m4);\n-    __ vadd_vx(v16, v16, ctr, Assembler::VectorMask::v0_t);\n-\n-    __ bind(L_first_loop);\n-    __ vmv_v_v(v24, v16);\n-    \/\/ convert the little-endian back to big-endian\n-    __ vrev8_v(v24, v24, Assembler::VectorMask::v0_t);\n-    __ vsetvli(vl, len32, Assembler::e32, Assembler::m4);\n-    __ vaesz_vs(v24, working_vregs[0]);\n-\n-    Label L_aes128_loop, L_aes192_loop, L_exit_aes_loop;\n-    __ mv(t0, 52);\n-    __ blt(keylen, t0, L_aes128_loop);\n-    __ beq(keylen, t0, L_aes192_loop);\n-\n-    \/\/ Encrypt the counters aes256\n-    for (int i = 1; i < 14; i++) {\n-      __ vaesem_vs(v24, working_vregs[i]);\n-    }\n-    __ vaesef_vs(v24, working_vregs[14]);\n-    __ j(L_exit_aes_loop);\n-\n-    \/\/ Encrypt the counters aes192\n-    __ bind(L_aes192_loop);\n-    for (int i = 1; i < 12; i++) {\n-      __ vaesem_vs(v24, working_vregs[i]);\n-    }\n-    __ vaesef_vs(v24, working_vregs[12]);\n-    __ j(L_exit_aes_loop);\n-\n-    \/\/ Encrypt the counters aes128\n-    __ bind(L_aes128_loop);\n-    for (int i = 1; i < 10; i++) {\n-      __ vaesem_vs(v24, working_vregs[i]);\n-    }\n-    __ vaesef_vs(v24, working_vregs[10]);\n-    __ bind(L_exit_aes_loop);\n-\n-    \/\/ XOR the encryptedCounter with the inputs\n-    __ vle32_v(v20, in);\n-    __ slli(t0, vl, 2);\n-    __ srli(ctr, vl, 2);\n-\n-    __ sub(len32, len32, vl);\n-    __ add(in, in, t0);\n-    __ vxor_vv(v24, v24, v20);\n-    __ vse32_v(v24, out);\n-    __ add(out, out, t0);\n-    __ bnez(len32, L_loop);\n-\n-    \/\/ Save the encryptedCounter and next counter according to ctr\n-    __ mv(used, block_size);\n-    __ vsetivli(x0, 2, Assembler::e64, Assembler::m1);\n-    __ vadd_vx(v16, v16, ctr, Assembler::VectorMask::v0_t);\n-    __ vrev8_v(v16, v16, Assembler::VectorMask::v0_t);\n-    __ vse64_v(v16, counter);\n-\n-    __ bind(L_calculate_one_next);\n@@ -2838,15 +2781,2 @@\n-    Label L_skip_next_inc;\n-    __ addi(t0, counter, 8);\n-    __ ld(tmp, Address(t0));\n-    __ rev8(tmp, tmp);\n-    __ addi(tmp, tmp, 1);\n-    __ rev8(tmp, tmp);\n-    __ sd(tmp, Address(t0));\n-    __ mv(t0, 0x0ul);\n-    __ bne(tmp, t0, L_skip_next_inc);\n-    __ ld(tmp, Address(counter));\n-    __ rev8(tmp, tmp);\n-    __ addi(tmp, tmp, 1);\n-    __ rev8(tmp, tmp);\n-    __ sd(tmp, Address(counter));\n-    __ bind(L_skip_next_inc);\n+    increase_counter_128(counter, tmp);\n+\n@@ -2854,1 +2784,11 @@\n-    __ j(L_encrypt_next);\n+\n+    __ mv(t0, block_size);\n+    __ blt(len, t0, L_encrypt_next);\n+\n+    __ vle32_v(v20, in);\n+    __ vxor_vv(v24, v24, v20);\n+    __ vse32_v(v24, out);\n+    __ add(out, out, t0);\n+    __ add(in, in, t0);\n+    __ sub(len, len, t0);\n+    __ j(L_main);\n","filename":"src\/hotspot\/cpu\/riscv\/stubGenerator_riscv.cpp","additions":64,"deletions":124,"binary":false,"changes":188,"status":"modified"}]}