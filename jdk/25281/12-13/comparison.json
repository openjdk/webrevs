{"files":[{"patch":"@@ -2610,15 +2610,13 @@\n-  void increase_counter_128(Register counter, Register tmp) {\n-    __ addi(t0, counter, 8);\n-    __ ld(tmp, Address(t0));\n-    __ rev8(tmp, tmp);\n-    __ addi(tmp, tmp, 1);\n-    __ rev8(tmp, tmp);\n-    __ sd(tmp, Address(t0));\n-    __ mv(t0, 0x0ul);\n-    __ sltu(tmp, t0, tmp);\n-    __ xori(t0, tmp, 1);\n-    __ ld(tmp, Address(counter));\n-    __ rev8(tmp, tmp);\n-    __ add(tmp, tmp, t0);\n-    __ rev8(tmp, tmp);\n-    __ sd(tmp, Address(counter));\n+  void increase_counter_128(Register counter, Register tmp1, Register tmp2) {\n+    __ ld(tmp1, Address(counter, 8));  \/\/ load low 64-bit from counter\n+    __ rev8(tmp1, tmp1);               \/\/ change to little endian for add\n+    __ addi(tmp1, tmp1, 1);\n+    __ rev8(tmp1, tmp1);               \/\/ change back to big endian\n+    __ sd(tmp1, Address(counter, 8));  \/\/ store the result back\n+    __ seqz(tmp2, tmp1);               \/\/ Check for result overflow,\n+                                       \/\/ set tmp2 = 1 if overflow, else tmp2 = 0\n+    __ ld(tmp1, Address(counter));     \/\/ load high 64-bit from counter\n+    __ rev8(tmp1, tmp1);\n+    __ add(tmp1, tmp1, tmp2);          \/\/ add 1 if overflow, else add 0\n+    __ rev8(tmp1, tmp1);\n+    __ sd(tmp1, Address(counter));\n@@ -2658,1 +2656,0 @@\n-    const Register tmp                 = c_rarg7;\n@@ -2663,0 +2660,3 @@\n+    const Register tmp1                = x28;\n+    const Register tmp2                = t1;\n+    const Register blk_size            = t2;\n@@ -2674,2 +2674,0 @@\n-    Label L_exit;\n-\n@@ -2710,0 +2708,1 @@\n+    Label L_exit;\n@@ -2713,0 +2712,1 @@\n+    __ mv(blk_size, block_size);\n@@ -2738,2 +2738,1 @@\n-    __ mv(t0, block_size);\n-    __ bge(used, t0, L_main);\n+    __ bge(used, blk_size, L_main);\n@@ -2742,4 +2741,4 @@\n-    __ add(tmp, saved_encrypted_ctr, used);\n-    __ lbu(t0, Address(tmp));\n-    __ lbu(tmp, Address(in));\n-    __ xorr(t0, t0, tmp);\n+    __ add(tmp1, saved_encrypted_ctr, used);\n+    __ lbu(t0, Address(tmp1));\n+    __ lbu(tmp1, Address(in));\n+    __ xorr(t0, t0, tmp1);\n@@ -2755,2 +2754,1 @@\n-    __ vsetivli(x0, 4, Assembler::e32, Assembler::m1);\n-    __ vle32_v(v24, counter);\n+    __ vle32_v(v16, counter);\n@@ -2763,1 +2761,1 @@\n-    generate_aes_encrypt(v24, working_vregs, 15);\n+    generate_aes_encrypt(v16, working_vregs, 15);\n@@ -2767,1 +2765,1 @@\n-    generate_aes_encrypt(v24, working_vregs, 13);\n+    generate_aes_encrypt(v16, working_vregs, 13);\n@@ -2771,1 +2769,1 @@\n-    generate_aes_encrypt(v24, working_vregs, 11);\n+    generate_aes_encrypt(v16, working_vregs, 11);\n@@ -2774,1 +2772,1 @@\n-    __ vse32_v(v24, saved_encrypted_ctr);\n+    __ vse32_v(v16, saved_encrypted_ctr);\n@@ -2778,1 +2776,1 @@\n-    increase_counter_128(counter, tmp);\n+    increase_counter_128(counter, tmp1, tmp2);\n@@ -2782,2 +2780,1 @@\n-    __ mv(t0, block_size);\n-    __ blt(len, t0, L_encrypt_next);\n+    __ blt(len, blk_size, L_encrypt_next);\n@@ -2785,6 +2782,6 @@\n-    __ vle32_v(v20, in);\n-    __ vxor_vv(v24, v24, v20);\n-    __ vse32_v(v24, out);\n-    __ add(out, out, t0);\n-    __ add(in, in, t0);\n-    __ sub(len, len, t0);\n+    __ vle32_v(v17, in);\n+    __ vxor_vv(v16, v16, v17);\n+    __ vse32_v(v16, out);\n+    __ add(out, out, blk_size);\n+    __ add(in, in, blk_size);\n+    __ sub(len, len, blk_size);\n","filename":"src\/hotspot\/cpu\/riscv\/stubGenerator_riscv.cpp","additions":36,"deletions":39,"binary":false,"changes":75,"status":"modified"}]}