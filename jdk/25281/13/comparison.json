{"files":[{"patch":"@@ -2650,0 +2650,8 @@\n+  INSN(vaesem_vs,   0b1110111, 0b010, 0b00010, 0b101001);\n+  INSN(vaesef_vs,   0b1110111, 0b010, 0b00011, 0b101001);\n+\n+  INSN(vaesdm_vs,   0b1110111, 0b010, 0b00000, 0b101001);\n+  INSN(vaesdf_vs,   0b1110111, 0b010, 0b00001, 0b101001);\n+\n+  INSN(vaesz_vs,    0b1110111, 0b010, 0b00111, 0b101001);\n+\n","filename":"src\/hotspot\/cpu\/riscv\/assembler_riscv.hpp","additions":8,"deletions":0,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -2610,0 +2610,189 @@\n+  void increase_counter_128(Register counter, Register tmp1, Register tmp2) {\n+    __ ld(tmp1, Address(counter, 8));  \/\/ load low 64-bit from counter\n+    __ rev8(tmp1, tmp1);               \/\/ change to little endian for add\n+    __ addi(tmp1, tmp1, 1);\n+    __ rev8(tmp1, tmp1);               \/\/ change back to big endian\n+    __ sd(tmp1, Address(counter, 8));  \/\/ store the result back\n+    __ seqz(tmp2, tmp1);               \/\/ Check for result overflow,\n+                                       \/\/ set tmp2 = 1 if overflow, else tmp2 = 0\n+    __ ld(tmp1, Address(counter));     \/\/ load high 64-bit from counter\n+    __ rev8(tmp1, tmp1);\n+    __ add(tmp1, tmp1, tmp2);          \/\/ add 1 if overflow, else add 0\n+    __ rev8(tmp1, tmp1);\n+    __ sd(tmp1, Address(counter));\n+  }\n+\n+  \/\/ CTR AES crypt.\n+  \/\/ Arguments:\n+  \/\/\n+  \/\/ Inputs:\n+  \/\/   c_rarg0   - source byte array address\n+  \/\/   c_rarg1   - destination byte array address\n+  \/\/   c_rarg2   - K (key) in little endian int array\n+  \/\/   c_rarg3   - counter vector byte array address\n+  \/\/   c_rarg4   - input length\n+  \/\/   c_rarg5   - saved encryptedCounter start\n+  \/\/   c_rarg6   - saved used length\n+  \/\/\n+  \/\/ Output:\n+  \/\/   x10       - input length\n+  \/\/\n+  address generate_counterMode_AESCrypt() {\n+    assert(UseAESCTRIntrinsics, \"need AES instructions (Zvkned extension) support\");\n+    assert(UseZvbb, \"need vector bit manipulation (Zvbb extension) support\");\n+    assert(UseZbb, \"need basic bit manipulation (Zbb extension) support\");\n+\n+    __ align(CodeEntryAlignment);\n+    StubId stub_id = StubId::stubgen_aescrypt_decryptBlock_id;\n+    StubCodeMark mark(this, stub_id);\n+\n+    const Register in                  = c_rarg0;\n+    const Register out                 = c_rarg1;\n+    const Register key                 = c_rarg2;\n+    const Register counter             = c_rarg3;\n+    const Register input_len           = c_rarg4;\n+    const Register saved_encrypted_ctr = c_rarg5;\n+    const Register used_ptr            = c_rarg6;\n+\n+    const Register keylen              = x31;\n+    const Register used                = x30;\n+    const Register len                 = x29;\n+    const Register tmp1                = x28;\n+    const Register tmp2                = t1;\n+    const Register blk_size            = t2;\n+\n+    const unsigned char block_size = 16;\n+\n+    VectorRegister working_vregs[] = {\n+      v1, v2, v3, v4, v5, v6, v7, v8,\n+      v9, v10, v11, v12, v13, v14, v15\n+    };\n+\n+    const address start = __ pc();\n+    __ enter();\n+\n+    \/\/ Algorithm:\n+    \/\/\n+    \/\/ if (len == 0) {\n+    \/\/   goto L_exit;\n+    \/\/ } else {\n+    \/\/   generate_aes_loadkeys();\n+    \/\/\n+    \/\/   L_encrypt_next:\n+    \/\/     while (used < block_size) {\n+    \/\/       if (len == 0) goto L_exit;\n+    \/\/       out[outOff++] = (byte)(in[inOff++] ^ saved_encrypted_ctr[used++]);\n+    \/\/       len--;\n+    \/\/     }\n+    \/\/\n+    \/\/   L_main:\n+    \/\/     saved_encrypted_ctr = aes_encrypt(counter);\n+    \/\/     increase_counter(counter);\n+    \/\/     if (len < block_size) {\n+    \/\/       used = 0;\n+    \/\/       goto L_encrypt_next;\n+    \/\/     }\n+    \/\/     v_in = load_16Byte(in);\n+    \/\/     v_out = load_16Byte(out);\n+    \/\/     v_saved_encrypted_ctr = load_16Byte(saved_encrypted_ctr);\n+    \/\/     v_out = v_in ^ v_saved_encrypted_ctr;\n+    \/\/     out += block_size;\n+    \/\/     in += block_size;\n+    \/\/     len -= block_size;\n+    \/\/     goto L_main;\n+    \/\/ }\n+    \/\/\n+    \/\/ L_exit:\n+    \/\/   return result;\n+\n+    Label L_exit;\n+    __ lw(used, Address(used_ptr));\n+    __ beqz(input_len, L_exit);\n+    __ mv(len, input_len);\n+    __ mv(blk_size, block_size);\n+\n+    Label L_aes128_loadkeys, L_aes192_loadkeys, L_exit_loadkeys;\n+    \/\/ Compute #rounds for AES based on the length of the key array\n+    __ lw(keylen, Address(key, arrayOopDesc::length_offset_in_bytes() - arrayOopDesc::base_offset_in_bytes(T_INT)));\n+    __ vsetivli(x0, 4, Assembler::e32, Assembler::m1);\n+    __ mv(t0, 52);\n+    __ blt(keylen, t0, L_aes128_loadkeys);\n+    __ beq(keylen, t0, L_aes192_loadkeys);\n+\n+    \/\/ Load aes keys into working_vregs according to the keylen\n+    generate_aes_loadkeys(key, working_vregs, 15);\n+    __ j(L_exit_loadkeys);\n+\n+    __ bind(L_aes192_loadkeys);\n+    generate_aes_loadkeys(key, working_vregs, 13);\n+    __ j(L_exit_loadkeys);\n+\n+    __ bind(L_aes128_loadkeys);\n+    generate_aes_loadkeys(key, working_vregs, 11);\n+    __ bind(L_exit_loadkeys);\n+\n+    Label L_next, L_encrypt_next, L_main;\n+\n+    \/\/ Encrypt bytes left with last encryptedCounter\n+    __ bind(L_next);\n+    __ bge(used, blk_size, L_main);\n+\n+    __ bind(L_encrypt_next);\n+    __ add(tmp1, saved_encrypted_ctr, used);\n+    __ lbu(t0, Address(tmp1));\n+    __ lbu(tmp1, Address(in));\n+    __ xorr(t0, t0, tmp1);\n+    __ sb(t0, Address(out));\n+    __ addi(in, in, 1);\n+    __ addi(out, out, 1);\n+    __ addi(used, used, 1);\n+    __ subi(len, len, 1);\n+    __ beqz(len, L_exit);\n+    __ j(L_next);\n+\n+    __ bind(L_main);\n+    __ vle32_v(v16, counter);\n+\n+    Label L_aes128_loop_next, L_aes192_loop_next, L_exit_aes_loop_next;\n+    __ mv(t0, 52);\n+    __ blt(keylen, t0, L_aes128_loop_next);\n+    __ beq(keylen, t0, L_aes192_loop_next);\n+\n+    generate_aes_encrypt(v16, working_vregs, 15);\n+    __ j(L_exit_aes_loop_next);\n+\n+    __ bind(L_aes192_loop_next);\n+    generate_aes_encrypt(v16, working_vregs, 13);\n+    __ j(L_exit_aes_loop_next);\n+\n+    __ bind(L_aes128_loop_next);\n+    generate_aes_encrypt(v16, working_vregs, 11);\n+    __ bind(L_exit_aes_loop_next);\n+\n+    __ vse32_v(v16, saved_encrypted_ctr);\n+    __ mv(used, 0);\n+\n+    \/\/ Increase counter\n+    increase_counter_128(counter, tmp1, tmp2);\n+\n+    __ beqz(len, L_exit);\n+\n+    __ blt(len, blk_size, L_encrypt_next);\n+\n+    __ vle32_v(v17, in);\n+    __ vxor_vv(v16, v16, v17);\n+    __ vse32_v(v16, out);\n+    __ add(out, out, blk_size);\n+    __ add(in, in, blk_size);\n+    __ sub(len, len, blk_size);\n+    __ j(L_main);\n+\n+    __ bind(L_exit);\n+    __ sw(used, Address(used_ptr));\n+    __ mv(x10, input_len);\n+    __ leave();\n+    __ ret();\n+\n+    return start;\n+  }\n+\n@@ -6830,0 +7019,4 @@\n+    if (UseAESCTRIntrinsics) {\n+      StubRoutines::_counterMode_AESCrypt = generate_counterMode_AESCrypt();\n+    }\n+\n","filename":"src\/hotspot\/cpu\/riscv\/stubGenerator_riscv.cpp","additions":193,"deletions":0,"binary":false,"changes":193,"status":"modified"},{"patch":"@@ -437,0 +437,11 @@\n+\n+    if (FLAG_IS_DEFAULT(UseAESCTRIntrinsics)) {\n+      FLAG_SET_DEFAULT(UseAESCTRIntrinsics, true);\n+    }\n+\n+    if (UseAESCTRIntrinsics) {\n+      if (!(UseZvbb && UseZbb)) {\n+        warning(\"Cannot enable UseAESCTRIntrinsics on cpu without UseZvbb and UseZbb support.\");\n+        FLAG_SET_DEFAULT(UseAESCTRIntrinsics, false);\n+      }\n+    }\n@@ -446,5 +457,4 @@\n-  }\n-\n-  if (UseAESCTRIntrinsics) {\n-    warning(\"AES\/CTR intrinsics are not available on this CPU\");\n-    FLAG_SET_DEFAULT(UseAESCTRIntrinsics, false);\n+    if (UseAESCTRIntrinsics) {\n+      warning(\"AES\/CTR intrinsics are not available on this CPU\");\n+      FLAG_SET_DEFAULT(UseAESCTRIntrinsics, false);\n+    }\n","filename":"src\/hotspot\/cpu\/riscv\/vm_version_riscv.cpp","additions":15,"deletions":5,"binary":false,"changes":20,"status":"modified"}]}