{"files":[{"patch":"@@ -66,1 +66,1 @@\n-VSeq<4> vs_front(VSeq<8> v) {\n+VSeq<4> vs_front(const VSeq<8>& v) {\n@@ -70,1 +70,1 @@\n-VSeq<4> vs_back(VSeq<8> v) {\n+VSeq<4> vs_back(const VSeq<8>& v) {\n@@ -74,1 +74,1 @@\n-VSeq<4> vs_even(VSeq<8> v) {\n+VSeq<4> vs_even(const VSeq<8>& v) {\n@@ -78,1 +78,1 @@\n-VSeq<4> vs_odd(VSeq<8> v) {\n+VSeq<4> vs_odd(const VSeq<8>& v) {\n","filename":"src\/hotspot\/cpu\/aarch64\/register_aarch64.cpp","additions":4,"deletions":4,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -458,1 +458,1 @@\n-  int mask() {\n+  int mask() const {\n@@ -466,2 +466,2 @@\n-  int base() { return _base; }\n-  int delta() { return _delta; }\n+  int base() const { return _base; }\n+  int delta() const { return _delta; }\n@@ -472,4 +472,4 @@\n-VSeq<4> vs_front(VSeq<8> v);\n-VSeq<4> vs_back(VSeq<8> v);\n-VSeq<4> vs_even(VSeq<8> v);\n-VSeq<4> vs_odd(VSeq<8> v);\n+VSeq<4> vs_front(const VSeq<8>& v);\n+VSeq<4> vs_back(const VSeq<8>& v);\n+VSeq<4> vs_even(const VSeq<8>& v);\n+VSeq<4> vs_odd(const VSeq<8>& v);\n@@ -480,2 +480,2 @@\n-template<int N, int M> bool vs_disjoint(VSeq<N> &n, VSeq<M> &m) { return (n.mask() & m.mask()) == 0; }\n-template<int N> bool vs_same(VSeq<N> &n, VSeq<N> &m) { return n.mask() == m.mask(); }\n+template<int N, int M> bool vs_disjoint(const VSeq<N>& n, const VSeq<M>& m) { return (n.mask() & m.mask()) == 0; }\n+template<int N> bool vs_same(const VSeq<N>& n, const VSeq<N>& m) { return n.mask() == m.mask(); }\n","filename":"src\/hotspot\/cpu\/aarch64\/register_aarch64.hpp","additions":9,"deletions":9,"binary":false,"changes":18,"status":"modified"},{"patch":"@@ -4652,2 +4652,2 @@\n-  void vs_addv(VSeq<N> v, Assembler::SIMD_Arrangement T,\n-               VSeq<N> v1, VSeq<N> v2) {\n+  void vs_addv(const VSeq<N>& v, Assembler::SIMD_Arrangement T,\n+               const VSeq<N>& v1, const VSeq<N>& v2) {\n@@ -4660,2 +4660,2 @@\n-  void vs_subv(VSeq<N> v, Assembler::SIMD_Arrangement T,\n-               VSeq<N> v1, VSeq<N> v2) {\n+  void vs_subv(const VSeq<N>& v, Assembler::SIMD_Arrangement T,\n+               const VSeq<N>& v1, const VSeq<N>& v2) {\n@@ -4668,2 +4668,2 @@\n-  void vs_mulv(VSeq<N> v, Assembler::SIMD_Arrangement T,\n-               VSeq<N> v1, VSeq<N> v2) {\n+  void vs_mulv(const VSeq<N>& v, Assembler::SIMD_Arrangement T,\n+               const VSeq<N>& v1, const VSeq<N>& v2) {\n@@ -4676,1 +4676,1 @@\n-  void vs_negr(VSeq<N> v, Assembler::SIMD_Arrangement T, VSeq<N> v1) {\n+  void vs_negr(const VSeq<N>& v, Assembler::SIMD_Arrangement T, const VSeq<N>& v1) {\n@@ -4683,2 +4683,2 @@\n-  void vs_sshr(VSeq<N> v, Assembler::SIMD_Arrangement T,\n-               VSeq<N> v1, int shift) {\n+  void vs_sshr(const VSeq<N>& v, Assembler::SIMD_Arrangement T,\n+               const VSeq<N>& v1, int shift) {\n@@ -4691,1 +4691,1 @@\n-  void vs_andr(VSeq<N> v, VSeq<N> v1, VSeq<N> v2) {\n+  void vs_andr(const VSeq<N>& v, const VSeq<N>& v1, const VSeq<N>& v2) {\n@@ -4698,1 +4698,1 @@\n-  void vs_orr(VSeq<N> v, VSeq<N> v1, VSeq<N> v2) {\n+  void vs_orr(const VSeq<N>& v, const VSeq<N>& v1, const VSeq<N>& v2) {\n@@ -4705,1 +4705,1 @@\n-    void vs_notr(VSeq<N> v, VSeq<N> v1) {\n+    void vs_notr(const VSeq<N>& v, const VSeq<N>& v1) {\n@@ -4715,1 +4715,1 @@\n-  void vs_ldpq(VSeq<N> v, Register base) {\n+  void vs_ldpq(const VSeq<N>& v, Register base) {\n@@ -4725,1 +4725,1 @@\n-  void vs_ldpq_post(VSeq<N> v, Register base) {\n+  void vs_ldpq_post(const VSeq<N>& v, Register base) {\n@@ -4735,1 +4735,1 @@\n-  void vs_stpq_post(VSeq<N> v, Register base) {\n+  void vs_stpq_post(const VSeq<N>& v, Register base) {\n@@ -4746,1 +4746,1 @@\n-  void vs_ldpq_indexed(VSeq<N> v, Register base, int start, int offsets[N\/2]) {\n+  void vs_ldpq_indexed(const VSeq<N>& v, Register base, int start, int (&offsets)[N\/2]) {\n@@ -4757,1 +4757,1 @@\n-  void vs_stpq_indexed(VSeq<N> v, Register base, int start, int offsets[N\/2]) {\n+  void vs_stpq_indexed(const VSeq<N>& v, Register base, int start, int offsets[N\/2]) {\n@@ -4768,2 +4768,2 @@\n-  void vs_ldr_indexed(VSeq<N> v, Assembler::SIMD_RegVariant T, Register base,\n-                      int start, int offsets[N]) {\n+  void vs_ldr_indexed(const VSeq<N>& v, Assembler::SIMD_RegVariant T, Register base,\n+                      int start, int (&offsets)[N]) {\n@@ -4780,2 +4780,2 @@\n-  void vs_str_indexed(VSeq<N> v, Assembler::SIMD_RegVariant T, Register base,\n-                      int start, int offsets[N]) {\n+  void vs_str_indexed(const VSeq<N>& v, Assembler::SIMD_RegVariant T, Register base,\n+                      int start, int (&offsets)[N]) {\n@@ -4792,2 +4792,2 @@\n-  void vs_ld2_indexed(VSeq<N> v, Assembler::SIMD_Arrangement T, Register base,\n-                      Register tmp, int start, int offsets[N\/2]) {\n+  void vs_ld2_indexed(const VSeq<N>& v, Assembler::SIMD_Arrangement T, Register base,\n+                      Register tmp, int start, int (&offsets)[N\/2]) {\n@@ -4805,2 +4805,2 @@\n-  void vs_st2_indexed(VSeq<N> v, Assembler::SIMD_Arrangement T, Register base,\n-                      Register tmp, int start, int offsets[N\/2]) {\n+  void vs_st2_indexed(const VSeq<N>& v, Assembler::SIMD_Arrangement T, Register base,\n+                      Register tmp, int start, int (&offsets)[N\/2]) {\n@@ -4828,2 +4828,2 @@\n-  void dilithium_montmul16(VSeq<4> va, VSeq<4> vb, VSeq<4> vc,\n-                    VSeq<4> vtmp, VSeq<2> vq) {\n+  void dilithium_montmul16(const VSeq<4>& va, const VSeq<4>& vb, const VSeq<4>& vc,\n+                    const VSeq<4>& vtmp, const VSeq<2>& vq) {\n@@ -4875,2 +4875,2 @@\n-  void vs_montmul32(VSeq<8> va, VSeq<8> vb, VSeq<8> vc,\n-                    VSeq<4> vtmp, VSeq<2> vq) {\n+  void vs_montmul32(const VSeq<8>& va, const VSeq<8>& vb, const VSeq<8>& vc,\n+                    const VSeq<4>& vtmp, const VSeq<2>& vq) {\n@@ -4910,2 +4910,2 @@\n-  void dilithium_montmul16_sub_add(VSeq<4> va0, VSeq<4> va1, VSeq<4> vc,\n-                            VSeq<4> vtmp, VSeq<2> vq) {\n+  void dilithium_montmul16_sub_add(const VSeq<4>& va0, const VSeq<4>& va1, const VSeq<4>& vc,\n+                                   const VSeq<4>& vtmp, const VSeq<2>& vq) {\n@@ -4922,2 +4922,2 @@\n-  void dilithium_sub_add_montmul16(VSeq<4> va0, VSeq<4> va1, VSeq<4> vb,\n-                            VSeq<4> vtmp1, VSeq<4> vtmp2, VSeq<2> vq) {\n+  void dilithium_sub_add_montmul16(const VSeq<4>& va0, const VSeq<4>& va1, const VSeq<4>& vb,\n+                                   const VSeq<4>& vtmp1, const VSeq<4>& vtmp2, const VSeq<2>& vq) {\n@@ -5272,1 +5272,1 @@\n-    \/\/ did for level 7 in the multiply code. So we load and store the\n+    \/\/ did for level 6 in the multiply code. So we load and store the\n","filename":"src\/hotspot\/cpu\/aarch64\/stubGenerator_aarch64.cpp","additions":34,"deletions":34,"binary":false,"changes":68,"status":"modified"}]}