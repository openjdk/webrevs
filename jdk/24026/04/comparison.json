{"files":[{"patch":"@@ -61,0 +61,20 @@\n+\n+\/\/ convenience methods for splitting 8-way vector register sequences\n+\/\/ in half -- needed because vector operations can normally only be\n+\/\/ benefit from 4-way instruction parallelism\n+\n+VSeq<4> vs_front(const VSeq<8>& v) {\n+  return VSeq<4>(v.base(), v.delta());\n+}\n+\n+VSeq<4> vs_back(const VSeq<8>& v) {\n+  return VSeq<4>(v.base() + 4 * v.delta(), v.delta());\n+}\n+\n+VSeq<4> vs_even(const VSeq<8>& v) {\n+  return VSeq<4>(v.base(), v.delta() * 2);\n+}\n+\n+VSeq<4> vs_odd(const VSeq<8>& v) {\n+  return VSeq<4>(v.base() + 1, v.delta() * 2);\n+}\n","filename":"src\/hotspot\/cpu\/aarch64\/register_aarch64.cpp","additions":20,"deletions":0,"binary":false,"changes":20,"status":"modified"},{"patch":"@@ -415,0 +415,68 @@\n+\/\/ AArch64 Vector Register Sequence management support\n+\/\/\n+\/\/ VSeq implements an indexable (by operator[]) vector register\n+\/\/ sequence starting from a fixed base register and with a fixed delta\n+\/\/ (defaulted to 1, but sometimes 0 or 2) e.g. VSeq<4>(16) will return\n+\/\/ registers v16, ... v19 for indices 0, ... 3.\n+\/\/\n+\/\/ Generator methods may iterate across sets of VSeq<4> to schedule an\n+\/\/ operation 4 times using distinct input and output registers,\n+\/\/ profiting from 4-way instruction parallelism.\n+\/\/\n+\/\/ A VSeq<2> can be used to specify registers loaded with special\n+\/\/ constants e.g. <v30, v31> --> <MONT_Q, MONT_Q_INV_MOD_R>.\n+\/\/\n+\/\/ A VSeq with base n and delta 0 can be used to generate code that\n+\/\/ combines values in another VSeq with the constant in register vn.\n+\/\/\n+\/\/ A VSeq with base n and delta 2 can be used to select an odd or even\n+\/\/ indexed set of registers.\n+\/\/\n+\/\/ Methods which accept arguments of type VSeq<8>, may split their\n+\/\/ inputs into front and back halves or odd and even halves (see\n+\/\/ convenience methods below).\n+\n+template<int N> class VSeq {\n+  static_assert(N >= 2, \"vector sequence length must be greater than 1\");\n+  static_assert(N <= 8, \"vector sequence length must not exceed 8\");\n+  static_assert((N & (N - 1)) == 0, \"vector sequence length must be power of two\");\n+private:\n+  int _base;  \/\/ index of first register in sequence\n+  int _delta; \/\/ increment to derive successive indices\n+public:\n+  VSeq(FloatRegister base_reg, int delta = 1) : VSeq(base_reg->encoding(), delta) { }\n+  VSeq(int base, int delta = 1) : _base(base), _delta(delta) {\n+    assert (_base >= 0, \"invalid base register\");\n+    assert (_delta >= 0, \"invalid register delta\");\n+    assert ((_base + (N - 1) * _delta) < 32, \"range exceeded\");\n+  }\n+  \/\/ indexed access to sequence\n+  FloatRegister operator [](int i) const {\n+    assert (0 <= i && i < N, \"index out of bounds\");\n+    return as_FloatRegister(_base + i * _delta);\n+  }\n+  int mask() const {\n+    int m = 0;\n+    int bit = 1 << _base;\n+    for (int i = 0; i < N; i++) {\n+      m |= bit << (i * _delta);\n+    }\n+    return m;\n+  }\n+  int base() const { return _base; }\n+  int delta() const { return _delta; }\n+};\n+\n+\/\/ declare convenience methods for splitting vector register sequences\n+\n+VSeq<4> vs_front(const VSeq<8>& v);\n+VSeq<4> vs_back(const VSeq<8>& v);\n+VSeq<4> vs_even(const VSeq<8>& v);\n+VSeq<4> vs_odd(const VSeq<8>& v);\n+\n+\/\/ methods for use in asserts to check VSeq inputs and oupts are\n+\/\/ either disjoint or equal\n+\n+template<int N, int M> bool vs_disjoint(const VSeq<N>& n, const VSeq<M>& m) { return (n.mask() & m.mask()) == 0; }\n+template<int N> bool vs_same(const VSeq<N>& n, const VSeq<N>& m) { return n.mask() == m.mask(); }\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/register_aarch64.hpp","additions":68,"deletions":0,"binary":false,"changes":68,"status":"modified"},{"patch":"@@ -4646,3 +4646,2 @@\n-  void dilithium_load16zetas(int o0, Register zetas) {\n-    __ ldpq(as_FloatRegister(o0), as_FloatRegister(o0 + 1), __ post (zetas, 32));\n-    __ ldpq(as_FloatRegister(o0 + 2), as_FloatRegister(o0 + 3), __ post (zetas, 32));\n+  \/\/ Helpers to schedule parallel operation bundles across vector\n+  \/\/ register sequences of size 2, 4 or 8.\n@@ -4650,0 +4649,162 @@\n+  \/\/ Implement various primitive computations across vector sequences\n+\n+  template<int N>\n+  void vs_addv(const VSeq<N>& v, Assembler::SIMD_Arrangement T,\n+               const VSeq<N>& v1, const VSeq<N>& v2) {\n+    for (int i = 0; i < N; i++) {\n+      __ addv(v[i], T, v1[i], v2[i]);\n+    }\n+  }\n+\n+  template<int N>\n+  void vs_subv(const VSeq<N>& v, Assembler::SIMD_Arrangement T,\n+               const VSeq<N>& v1, const VSeq<N>& v2) {\n+    for (int i = 0; i < N; i++) {\n+      __ subv(v[i], T, v1[i], v2[i]);\n+    }\n+  }\n+\n+  template<int N>\n+  void vs_mulv(const VSeq<N>& v, Assembler::SIMD_Arrangement T,\n+               const VSeq<N>& v1, const VSeq<N>& v2) {\n+    for (int i = 0; i < N; i++) {\n+      __ mulv(v[i], T, v1[i], v2[i]);\n+    }\n+  }\n+\n+  template<int N>\n+  void vs_negr(const VSeq<N>& v, Assembler::SIMD_Arrangement T, const VSeq<N>& v1) {\n+    for (int i = 0; i < N; i++) {\n+      __ negr(v[i], T, v1[i]);\n+    }\n+  }\n+\n+  template<int N>\n+  void vs_sshr(const VSeq<N>& v, Assembler::SIMD_Arrangement T,\n+               const VSeq<N>& v1, int shift) {\n+    for (int i = 0; i < N; i++) {\n+      __ sshr(v[i], T, v1[i], shift);\n+    }\n+  }\n+\n+  template<int N>\n+  void vs_andr(const VSeq<N>& v, const VSeq<N>& v1, const VSeq<N>& v2) {\n+    for (int i = 0; i < N; i++) {\n+      __ andr(v[i], __ T16B, v1[i], v2[i]);\n+    }\n+  }\n+\n+  template<int N>\n+  void vs_orr(const VSeq<N>& v, const VSeq<N>& v1, const VSeq<N>& v2) {\n+    for (int i = 0; i < N; i++) {\n+      __ orr(v[i], __ T16B, v1[i], v2[i]);\n+    }\n+  }\n+\n+  template<int N>\n+    void vs_notr(const VSeq<N>& v, const VSeq<N>& v1) {\n+    for (int i = 0; i < N; i++) {\n+      __ notr(v[i], __ T16B, v1[i]);\n+    }\n+  }\n+\n+  \/\/ load N\/2 successive pairs of quadword values from memory in order\n+  \/\/ into N successive vector registers of the sequence via the\n+  \/\/ address supplied in base.\n+  template<int N>\n+  void vs_ldpq(const VSeq<N>& v, Register base) {\n+    for (int i = 0; i < N; i += 2) {\n+      __ ldpq(v[i], v[i+1], Address(base, 32 * i));\n+    }\n+  }\n+\n+  \/\/ load N\/2 successive pairs of quadword values from memory in order\n+  \/\/ into N vector registers of the sequence via the address supplied\n+  \/\/ in base using post-increment addressing\n+  template<int N>\n+  void vs_ldpq_post(const VSeq<N>& v, Register base) {\n+    for (int i = 0; i < N; i += 2) {\n+      __ ldpq(v[i], v[i+1], __ post(base, 32));\n+    }\n+  }\n+\n+  \/\/ store N successive vector registers of the sequence into N\/2\n+  \/\/ successive pairs of quadword memory locations via the address\n+  \/\/ supplied in base using post-increment addressing\n+  template<int N>\n+  void vs_stpq_post(const VSeq<N>& v, Register base) {\n+    for (int i = 0; i < N; i += 2) {\n+      __ stpq(v[i], v[i+1], __ post(base, 32));\n+    }\n+  }\n+\n+  \/\/ load N\/2 pairs of quadword values from memory into N vector\n+  \/\/ registers via the address supplied in base with each pair indexed\n+  \/\/ using the the start offset plus the corresponding entry in the\n+  \/\/ offsets array\n+  template<int N>\n+  void vs_ldpq_indexed(const VSeq<N>& v, Register base, int start, int (&offsets)[N\/2]) {\n+    for (int i = 0; i < N\/2; i++) {\n+      __ ldpq(v[2*i], v[2*i+1], Address(base, start + offsets[i]));\n+    }\n+  }\n+\n+  \/\/ store N vector registers into N\/2 pairs of quadword memory\n+  \/\/ locations via the address supplied in base with each pair indexed\n+  \/\/ using the the start offset plus the corresponding entry in the\n+  \/\/ offsets array\n+  template<int N>\n+  void vs_stpq_indexed(const VSeq<N>& v, Register base, int start, int offsets[N\/2]) {\n+    for (int i = 0; i < N\/2; i++) {\n+      __ stpq(v[2*i], v[2*i+1], Address(base, start + offsets[i]));\n+    }\n+  }\n+\n+  \/\/ load N single quadword values from memory into N vector registers\n+  \/\/ via the address supplied in base with each value indexed using\n+  \/\/ the the start offset plus the corresponding entry in the offsets\n+  \/\/ array\n+  template<int N>\n+  void vs_ldr_indexed(const VSeq<N>& v, Assembler::SIMD_RegVariant T, Register base,\n+                      int start, int (&offsets)[N]) {\n+    for (int i = 0; i < N; i++) {\n+      __ ldr(v[i], T, Address(base, start + offsets[i]));\n+    }\n+  }\n+\n+  \/\/ store N vector registers into N single quadword memory locations\n+  \/\/ via the address supplied in base with each value indexed using\n+  \/\/ the the start offset plus the corresponding entry in the offsets\n+  \/\/ array\n+  template<int N>\n+  void vs_str_indexed(const VSeq<N>& v, Assembler::SIMD_RegVariant T, Register base,\n+                      int start, int (&offsets)[N]) {\n+    for (int i = 0; i < N; i++) {\n+      __ str(v[i], T, Address(base, start + offsets[i]));\n+    }\n+  }\n+\n+  \/\/ load N\/2 pairs of quadword values from memory de-interleaved into\n+  \/\/ N vector registers 2 at a time via the address supplied in base\n+  \/\/ with each pair indexed using the the start offset plus the\n+  \/\/ corresponding entry in the offsets array\n+  template<int N>\n+  void vs_ld2_indexed(const VSeq<N>& v, Assembler::SIMD_Arrangement T, Register base,\n+                      Register tmp, int start, int (&offsets)[N\/2]) {\n+    for (int i = 0; i < N\/2; i++) {\n+      __ add(tmp, base, start + offsets[i]);\n+      __ ld2(v[2*i], v[2*i+1], T, tmp);\n+    }\n+  }\n+\n+  \/\/ store N vector registers 2 at a time interleaved into N\/2 pairs\n+  \/\/ of quadword memory locations via the address supplied in base\n+  \/\/ with each pair indexed using the the start offset plus the\n+  \/\/ corresponding entry in the offsets array\n+  template<int N>\n+  void vs_st2_indexed(const VSeq<N>& v, Assembler::SIMD_Arrangement T, Register base,\n+                      Register tmp, int start, int (&offsets)[N\/2]) {\n+    for (int i = 0; i < N\/2; i++) {\n+      __ add(tmp, base, start + offsets[i]);\n+      __ st2(v[2*i], v[2*i+1], T, tmp);\n+    }\n@@ -4652,3 +4813,48 @@\n-  void dilithium_load32zetas(Register zetas) {\n-    dilithium_load16zetas(16, zetas);\n-    dilithium_load16zetas(20, zetas);\n+  \/\/ Helper routines for various flavours of dilithium montgomery\n+  \/\/ multiply\n+\n+  \/\/ Perform 16 32-bit Montgomery multiplications in parallel\n+  \/\/ See the montMul() method of the sun.security.provider.ML_DSA class.\n+  \/\/\n+  \/\/ Computes 4x4S results\n+  \/\/    a = b * c * 2^-32 mod MONT_Q\n+  \/\/ Inputs:  vb, vc - 4x4S vector register sequences\n+  \/\/          vq - 2x4S constants <MONT_Q, MONT_Q_INV_MOD_R>\n+  \/\/ Temps:   vtmp - 4x4S vector sequence trashed after call\n+  \/\/ Outputs: va - 4x4S vector register sequences\n+  \/\/ vb, vc, vtmp and vq must all be disjoint\n+  \/\/ va must be disjoint from all other inputs\/temps or must equal vc\n+  \/\/ n.b. MONT_R_BITS is 32, so the right shift by it is implicit.\n+  void dilithium_montmul16(const VSeq<4>& va, const VSeq<4>& vb, const VSeq<4>& vc,\n+                    const VSeq<4>& vtmp, const VSeq<2>& vq) {\n+    assert(vs_disjoint(vb, vc), \"vb and vc overlap\");\n+    assert(vs_disjoint(vb, vq), \"vb and vq overlap\");\n+    assert(vs_disjoint(vb, vtmp), \"vb and vtmp overlap\");\n+\n+    assert(vs_disjoint(vc, vq), \"vc and vq overlap\");\n+    assert(vs_disjoint(vc, vtmp), \"vc and vtmp overlap\");\n+\n+    assert(vs_disjoint(vq, vtmp), \"vq and vtmp overlap\");\n+\n+    assert(vs_disjoint(va, vc) || vs_same(va, vc), \"va and vc neither disjoint nor equal\");\n+    assert(vs_disjoint(va, vb), \"va and vb overlap\");\n+    assert(vs_disjoint(va, vq), \"va and vq overlap\");\n+    assert(vs_disjoint(va, vtmp), \"va and vtmp overlap\");\n+\n+    \/\/ schedule 4 streams of instructions across the vector sequences\n+    for (int i = 0; i < 4; i++) {\n+      __ sqdmulh(vtmp[i], __ T4S, vb[i], vc[i]); \/\/ aHigh = hi32(2 * b * c)\n+      __ mulv(va[i], __ T4S, vb[i], vc[i]);    \/\/ aLow = lo32(b * c)\n+    }\n+\n+    for (int i = 0; i < 4; i++) {\n+      __ mulv(va[i], __ T4S, va[i], vq[0]);     \/\/ m = aLow * qinv\n+    }\n+\n+    for (int i = 0; i < 4; i++) {\n+      __ sqdmulh(va[i], __ T4S, va[i], vq[1]);  \/\/ n = hi32(2 * m * q)\n+    }\n+\n+    for (int i = 0; i < 4; i++) {\n+      __ shsubv(va[i], __ T4S, vtmp[i], va[i]);   \/\/ a = (aHigh - n) \/ 2\n+    }\n@@ -4657,1 +4863,1 @@\n-  \/\/ 2x16 32-bit Montgomery multiplications in parallel\n+  \/\/ Perform 2x16 32-bit Montgomery multiplications in parallel\n@@ -4659,63 +4865,41 @@\n-  \/\/ Here MONT_R_BITS is 32, so the right shift by it is implicit.\n-  \/\/ The constants qInv = MONT_Q_INV_MOD_R and q = MONT_Q are loaded in\n-  \/\/ (all 32-bit chunks of) vector registers v30 and v31, resp.\n-  \/\/ The inputs are b[i]s in v0-v7 and c[i]s v16-v23 and\n-  \/\/ the results are a[i]s in v16-v23, four 32-bit values in each register\n-  \/\/ and we do a_i = b_i * c_i * 2^-32 mod MONT_Q for all\n-  void dilithium_montmul32(bool by_constant) {\n-    FloatRegister vr0 = by_constant ? v29 : v0;\n-    FloatRegister vr1 = by_constant ? v29 : v1;\n-    FloatRegister vr2 = by_constant ? v29 : v2;\n-    FloatRegister vr3 = by_constant ? v29 : v3;\n-    FloatRegister vr4 = by_constant ? v29 : v4;\n-    FloatRegister vr5 = by_constant ? v29 : v5;\n-    FloatRegister vr6 = by_constant ? v29 : v6;\n-    FloatRegister vr7 = by_constant ? v29 : v7;\n-\n-    __ sqdmulh(v24, __ T4S, vr0, v16); \/\/ aHigh = hi32(2 * b * c)\n-    __ mulv(v16, __ T4S, vr0, v16);    \/\/ aLow = lo32(b * c)\n-    __ sqdmulh(v25, __ T4S, vr1, v17);\n-    __ mulv(v17, __ T4S, vr1, v17);\n-    __ sqdmulh(v26, __ T4S, vr2, v18);\n-    __ mulv(v18, __ T4S, vr2, v18);\n-    __ sqdmulh(v27, __ T4S, vr3, v19);\n-    __ mulv(v19, __ T4S, vr3, v19);\n-\n-    __ mulv(v16, __ T4S, v16, v30);     \/\/ m = aLow * qinv\n-    __ mulv(v17, __ T4S, v17, v30);\n-    __ mulv(v18, __ T4S, v18, v30);\n-    __ mulv(v19, __ T4S, v19, v30);\n-\n-    __ sqdmulh(v16, __ T4S, v16, v31);  \/\/ n = hi32(2 * m * q)\n-    __ sqdmulh(v17, __ T4S, v17, v31);\n-    __ sqdmulh(v18, __ T4S, v18, v31);\n-    __ sqdmulh(v19, __ T4S, v19, v31);\n-\n-    __ shsubv(v16, __ T4S, v24, v16);   \/\/ a = (aHigh - n) \/ 2\n-    __ shsubv(v17, __ T4S, v25, v17);\n-    __ shsubv(v18, __ T4S, v26, v18);\n-    __ shsubv(v19, __ T4S, v27, v19);\n-\n-    __ sqdmulh(v24, __ T4S, vr4, v20);\n-    __ mulv(v20, __ T4S, vr4, v20);\n-    __ sqdmulh(v25, __ T4S, vr5, v21);\n-    __ mulv(v21, __ T4S, vr5, v21);\n-    __ sqdmulh(v26, __ T4S, vr6, v22);\n-    __ mulv(v22, __ T4S, vr6, v22);\n-    __ sqdmulh(v27, __ T4S, vr7, v23);\n-    __ mulv(v23, __ T4S, vr7, v23);\n-\n-    __ mulv(v20, __ T4S, v20, v30);\n-    __ mulv(v21, __ T4S, v21, v30);\n-    __ mulv(v22, __ T4S, v22, v30);\n-    __ mulv(v23, __ T4S, v23, v30);\n-\n-    __ sqdmulh(v20, __ T4S, v20, v31);\n-    __ sqdmulh(v21, __ T4S, v21, v31);\n-    __ sqdmulh(v22, __ T4S, v22, v31);\n-    __ sqdmulh(v23, __ T4S, v23, v31);\n-\n-    __ shsubv(v20, __ T4S, v24, v20);\n-    __ shsubv(v21, __ T4S, v25, v21);\n-    __ shsubv(v22, __ T4S, v26, v22);\n-    __ shsubv(v23, __ T4S, v27, v23);\n+  \/\/\n+  \/\/ Computes 8x4S results\n+  \/\/    a = b * c * 2^-32 mod MONT_Q\n+  \/\/ Inputs:  vb, vc - 8x4S vector register sequences\n+  \/\/          vq - 2x4S constants <MONT_Q, MONT_Q_INV_MOD_R>\n+  \/\/ Temps:   vtmp - 4x4S vector sequence trashed after call\n+  \/\/ Outputs: va - 8x4S vector register sequences\n+  \/\/ vb, vc, vtmp and vq must all be disjoint\n+  \/\/ va must be disjoint from all other inputs\/temps or must equal vc\n+  \/\/ n.b. MONT_R_BITS is 32, so the right shift by it is implicit.\n+  void vs_montmul32(const VSeq<8>& va, const VSeq<8>& vb, const VSeq<8>& vc,\n+                    const VSeq<4>& vtmp, const VSeq<2>& vq) {\n+    \/\/ vb, vc, vtmp and vq must be disjoint. va must either be\n+    \/\/ disjoint from all other registers or equal vc\n+\n+    assert(vs_disjoint(vb, vc), \"vb and vc overlap\");\n+    assert(vs_disjoint(vb, vq), \"vb and vq overlap\");\n+    assert(vs_disjoint(vb, vtmp), \"vb and vtmp overlap\");\n+\n+    assert(vs_disjoint(vc, vq), \"vc and vq overlap\");\n+    assert(vs_disjoint(vc, vtmp), \"vc and vtmp overlap\");\n+\n+    assert(vs_disjoint(vq, vtmp), \"vq and vtmp overlap\");\n+\n+    assert(vs_disjoint(va, vc) || vs_same(va, vc), \"va and vc neither disjoint nor equal\");\n+    assert(vs_disjoint(va, vb), \"va and vb overlap\");\n+    assert(vs_disjoint(va, vq), \"va and vq overlap\");\n+    assert(vs_disjoint(va, vtmp), \"va and vtmp overlap\");\n+\n+    \/\/ we need to multiply the front and back halves of each sequence\n+    \/\/ 4x4S at a time because\n+    \/\/\n+    \/\/ 1) we are currently only able to get 4-way instruction\n+    \/\/ parallelism at best\n+    \/\/\n+    \/\/ 2) we need registers for the constants in vq and temporary\n+    \/\/ scratch registers to hold intermediate results so vtmp can only\n+    \/\/ be a VSeq<4> which means we only have 4 scratch slots\n+\n+    dilithium_montmul16(vs_front(va), vs_front(vb), vs_front(vc), vtmp, vq);\n+    dilithium_montmul16(vs_back(va), vs_back(vb), vs_back(vc), vtmp, vq);\n@@ -4724,20 +4908,10 @@\n- \/\/ Do the addition and subtraction done in the ntt algorithm.\n- \/\/ See sun.security.provider.ML_DSA.implDilithiumAlmostNttJava()\n-  void dilithium_add_sub32() {\n-    __ addv(v24, __ T4S, v0, v16); \/\/ coeffs[j] = coeffs[j] + tmp;\n-    __ addv(v25, __ T4S, v1, v17);\n-    __ addv(v26, __ T4S, v2, v18);\n-    __ addv(v27, __ T4S, v3, v19);\n-    __ addv(v28, __ T4S, v4, v20);\n-    __ addv(v29, __ T4S, v5, v21);\n-    __ addv(v30, __ T4S, v6, v22);\n-    __ addv(v31, __ T4S, v7, v23);\n-\n-    __ subv(v0, __ T4S, v0, v16);  \/\/ coeffs[j + l] = coeffs[j] - tmp;\n-    __ subv(v1, __ T4S, v1, v17);\n-    __ subv(v2, __ T4S, v2, v18);\n-    __ subv(v3, __ T4S, v3, v19);\n-    __ subv(v4, __ T4S, v4, v20);\n-    __ subv(v5, __ T4S, v5, v21);\n-    __ subv(v6, __ T4S, v6, v22);\n-    __ subv(v7, __ T4S, v7, v23);\n+  \/\/ perform combined montmul then add\/sub on 4x4S vectors\n+\n+  void dilithium_montmul16_sub_add(const VSeq<4>& va0, const VSeq<4>& va1, const VSeq<4>& vc,\n+                                   const VSeq<4>& vtmp, const VSeq<2>& vq) {\n+    \/\/ compute a = montmul(a1, c)\n+    dilithium_montmul16(vc, va1, vc, vtmp, vq);\n+    \/\/ ouptut a1 = a0 - a\n+    vs_subv(va1, __ T4S, va0, vc);\n+    \/\/    and a0 = a0 + a\n+    vs_addv(va0, __ T4S, va0, vc);\n@@ -4746,38 +4920,10 @@\n-  \/\/ Do the same computation that\n-  \/\/ dilithium_montmul32() and dilithium_add_sub32() does,\n-  \/\/ except for only 4x4 32-bit vector elements and with\n-  \/\/ different register usage.\n-  void dilithium_montmul_sub_add16() {\n-    __ sqdmulh(v24, __ T4S, v1, v16);\n-    __ mulv(v16, __ T4S, v1, v16);\n-    __ sqdmulh(v25, __ T4S, v3, v17);\n-    __ mulv(v17, __ T4S, v3, v17);\n-    __ sqdmulh(v26, __ T4S, v5, v18);\n-    __ mulv(v18, __ T4S, v5, v18);\n-    __ sqdmulh(v27, __ T4S, v7, v19);\n-    __ mulv(v19, __ T4S, v7, v19);\n-\n-    __ mulv(v16, __ T4S, v16, v30);\n-    __ mulv(v17, __ T4S, v17, v30);\n-    __ mulv(v18, __ T4S, v18, v30);\n-    __ mulv(v19, __ T4S, v19, v30);\n-\n-    __ sqdmulh(v16, __ T4S, v16, v31);\n-    __ sqdmulh(v17, __ T4S, v17, v31);\n-    __ sqdmulh(v18, __ T4S, v18, v31);\n-    __ sqdmulh(v19, __ T4S, v19, v31);\n-\n-    __ shsubv(v16, __ T4S, v24, v16);\n-    __ shsubv(v17, __ T4S, v25, v17);\n-    __ shsubv(v18, __ T4S, v26, v18);\n-    __ shsubv(v19, __ T4S, v27, v19);\n-\n-    __ subv(v1, __ T4S, v0, v16);\n-    __ subv(v3, __ T4S, v2, v17);\n-    __ subv(v5, __ T4S, v4, v18);\n-    __ subv(v7, __ T4S, v6, v19);\n-\n-    __ addv(v0, __ T4S, v0, v16);\n-    __ addv(v2, __ T4S, v2, v17);\n-    __ addv(v4, __ T4S, v4, v18);\n-    __ addv(v6, __ T4S, v6, v19);\n+  \/\/ perform combined add\/sub then montul on 4x4S vectors\n+\n+  void dilithium_sub_add_montmul16(const VSeq<4>& va0, const VSeq<4>& va1, const VSeq<4>& vb,\n+                                   const VSeq<4>& vtmp1, const VSeq<4>& vtmp2, const VSeq<2>& vq) {\n+    \/\/ compute c = a0 - a1\n+    vs_subv(vtmp1, __ T4S, va0, va1);\n+    \/\/ output a0 = a0 + a1\n+    vs_addv(va0, __ T4S, va0, va1);\n+    \/\/ output a1 = b montmul c\n+    dilithium_montmul16(va1, vtmp1, vb, vtmp2, vq);\n@@ -4801,3 +4947,5 @@\n-    int incr1 = 32;\n-    int incr2 = 64;\n-    int incr3 = 96;\n+    \/\/ don't use callee save registers v8 - v15\n+    VSeq<8> vs1(0), vs2(16), vs3(24);  \/\/ 3 sets of 8x4s inputs\/outputs\n+    VSeq<4> vtmp = vs_front(vs3);         \/\/ n.b. tmp registers overlap vs3\n+    VSeq<2> vq(30);                    \/\/ n.b. constants overlap vs3\n+    int offsets[4] = { 0, 32, 64, 96 };\n@@ -4809,3 +4957,3 @@\n-        incr1 = 32;\n-        incr2 = 128;\n-        incr3 = 160;\n+        offsets[1] = 32;\n+        offsets[2] = 128;\n+        offsets[3] = 160;\n@@ -4813,3 +4961,3 @@\n-        incr1 = 64;\n-        incr2 = 128;\n-        incr3 = 192;\n+        offsets[1] = 64;\n+        offsets[2] = 128;\n+        offsets[3] = 192;\n@@ -4818,0 +4966,4 @@\n+      \/\/ for levels 1 - 4 we simply load 2 x 4 adjacent values at a\n+      \/\/ time at 4 different offsets and multiply them in order by the\n+      \/\/ next set of input values. So we employ indexed load and store\n+      \/\/ pair instructions with arrangement 4S\n@@ -4819,20 +4971,17 @@\n-        __ ldpq(v30, v31, Address(dilithiumConsts, 0)); \/\/ qInv, q\n-        __ ldpq(v0, v1, Address(coeffs, c2Start));\n-        __ ldpq(v2, v3, Address(coeffs, c2Start + incr1));\n-        __ ldpq(v4, v5, Address(coeffs, c2Start + incr2));\n-        __ ldpq(v6, v7, Address(coeffs, c2Start + incr3));\n-        dilithium_load32zetas(zetas);\n-        dilithium_montmul32(false);\n-        __ ldpq(v0, v1, Address(coeffs, c1Start));\n-        __ ldpq(v2, v3, Address(coeffs, c1Start + incr1));\n-        __ ldpq(v4, v5, Address(coeffs, c1Start + incr2));\n-        __ ldpq(v6, v7, Address(coeffs, c1Start + incr3));\n-        dilithium_add_sub32();\n-        __ stpq(v24, v25, Address(coeffs, c1Start));\n-        __ stpq(v26, v27, Address(coeffs, c1Start + incr1));\n-        __ stpq(v28, v29, Address(coeffs, c1Start + incr2));\n-        __ stpq(v30, v31, Address(coeffs, c1Start + incr3));\n-        __ stpq(v0, v1, Address(coeffs, c2Start));\n-        __ stpq(v2, v3, Address(coeffs, c2Start + incr1));\n-        __ stpq(v4, v5, Address(coeffs, c2Start + incr2));\n-        __ stpq(v6, v7, Address(coeffs, c2Start + incr3));\n+        \/\/ reload q and qinv\n+        vs_ldpq(vq, dilithiumConsts); \/\/ qInv, q\n+        \/\/ load 8x4S coefficients via second start pos == c2\n+        vs_ldpq_indexed(vs1, coeffs, c2Start, offsets);\n+        \/\/ load next 8x4S inputs == b\n+        vs_ldpq_post(vs2, zetas);\n+        \/\/ compute a == c2 * b mod MONT_Q\n+        vs_montmul32(vs2, vs1, vs2, vtmp, vq);\n+        \/\/ load 8x4s coefficients via first start pos == c1\n+        vs_ldpq_indexed(vs1, coeffs, c1Start, offsets);\n+        \/\/ compute a1 =  c1 + a\n+        vs_addv(vs3, __ T4S, vs1, vs2);\n+        \/\/ compute a2 =  c1 - a\n+        vs_subv(vs1, __ T4S, vs1, vs2);\n+        \/\/ output a1 and a2\n+        vs_stpq_indexed(vs3, coeffs, c1Start, offsets);\n+        vs_stpq_indexed(vs1, coeffs, c2Start, offsets);\n@@ -4879,1 +5028,7 @@\n-\n+    \/\/ don't use callee save registers v8 - v15\n+    VSeq<8> vs1(0), vs2(16), vs3(24);  \/\/ 3 sets of 8x4s inputs\/outputs\n+    VSeq<4> vtmp = vs_front(vs3);         \/\/ n.b. tmp registers overlap vs3\n+    VSeq<2> vq(30);                    \/\/ n.b. constants overlap vs3\n+    int offsets[4] = {0, 32, 64, 96};\n+    int offsets1[8] = {16, 48, 80, 112, 144, 176, 208, 240 };\n+    int offsets2[8] = { 0, 32, 64, 96, 128, 160, 192, 224 };\n@@ -4889,0 +5044,7 @@\n+\n+    \/\/ at level 5 the coefficients we need to combine with the zetas\n+    \/\/ are grouped in memory in blocks of size 4. So, for both sets of\n+    \/\/ coefficients we load 4 adjacent values at 8 different offsets\n+    \/\/ using an indexed ldr with register variant Q and multiply them\n+    \/\/ in sequence order by the next set of inputs. Likewise we store\n+    \/\/ the resuls using an indexed str with register variant Q.\n@@ -4890,36 +5052,16 @@\n-      __ ldpq(v30, v31, Address(dilithiumConsts, 0));  \/\/ qInv, q\n-      __ ldr(v0, __ Q, Address(coeffs, i + 16));\n-      __ ldr(v1, __ Q, Address(coeffs, i + 48));\n-      __ ldr(v2, __ Q, Address(coeffs, i + 80));\n-      __ ldr(v3, __ Q, Address(coeffs, i + 112));\n-      __ ldr(v4, __ Q, Address(coeffs, i + 144));\n-      __ ldr(v5, __ Q, Address(coeffs, i + 176));\n-      __ ldr(v6, __ Q, Address(coeffs, i + 208));\n-      __ ldr(v7, __ Q, Address(coeffs, i + 240));\n-      dilithium_load32zetas(zetas);\n-      dilithium_montmul32(false);\n-      __ ldr(v0, __ Q, Address(coeffs, i));\n-      __ ldr(v1, __ Q, Address(coeffs, i + 32));\n-      __ ldr(v2, __ Q, Address(coeffs, i + 64));\n-      __ ldr(v3, __ Q, Address(coeffs, i + 96));\n-      __ ldr(v4, __ Q, Address(coeffs, i + 128));\n-      __ ldr(v5, __ Q, Address(coeffs, i + 160));\n-      __ ldr(v6, __ Q, Address(coeffs, i + 192));\n-      __ ldr(v7, __ Q, Address(coeffs, i + 224));\n-      dilithium_add_sub32();\n-      __ str(v24, __ Q, Address(coeffs, i));\n-      __ str(v25, __ Q, Address(coeffs, i + 32));\n-      __ str(v26, __ Q, Address(coeffs, i + 64));\n-      __ str(v27, __ Q, Address(coeffs, i + 96));\n-      __ str(v28, __ Q, Address(coeffs, i + 128));\n-      __ str(v29, __ Q, Address(coeffs, i + 160));\n-      __ str(v30, __ Q, Address(coeffs, i + 192));\n-      __ str(v31, __ Q, Address(coeffs, i + 224));\n-      __ str(v0, __ Q, Address(coeffs, i + 16));\n-      __ str(v1, __ Q, Address(coeffs, i + 48));\n-      __ str(v2, __ Q, Address(coeffs, i + 80));\n-      __ str(v3, __ Q, Address(coeffs, i + 112));\n-      __ str(v4, __ Q, Address(coeffs, i + 144));\n-      __ str(v5, __ Q, Address(coeffs, i + 176));\n-      __ str(v6, __ Q, Address(coeffs, i + 208));\n-      __ str(v7, __ Q, Address(coeffs, i + 240));\n+      \/\/ reload constants q, qinv each iteration as they get clobbered later\n+      vs_ldpq(vq, dilithiumConsts); \/\/ qInv, q\n+      \/\/ load 32 (8x4S) coefficients via first offsets = c1\n+      vs_ldr_indexed(vs1, __ Q, coeffs, i, offsets1);\n+      \/\/ load next 32 (8x4S) inputs = b\n+      vs_ldpq_post(vs2, zetas);\n+      \/\/ a = b montul c1\n+      vs_montmul32(vs2, vs1, vs2, vtmp, vq);\n+      \/\/ load 32 (8x4S) coefficients via second offsets = c2\n+      vs_ldr_indexed(vs1, __ Q, coeffs, i, offsets2);\n+      \/\/ add\/sub with result of multiply\n+      vs_addv(vs3, __ T4S, vs1, vs2);     \/\/ a1 = a - c2\n+      vs_subv(vs1, __ T4S, vs1, vs2);     \/\/ a0 = a + c1\n+      \/\/ write back new coefficients using same offsets\n+      vs_str_indexed(vs3, __ Q, coeffs, i, offsets2);\n+      vs_str_indexed(vs1, __ Q, coeffs, i, offsets1);\n@@ -4929,0 +5071,13 @@\n+    \/\/ at level 6 the coefficients we need to combine with the zetas\n+    \/\/ are grouped in memory in pairs, the first two being montmul\n+    \/\/ inputs and the second add\/sub inputs. We can still implement\n+    \/\/ the montmul+sub+add using 4-way parallelism but only if we\n+    \/\/ combine the coefficients with the zetas 16 at a time. We load 8\n+    \/\/ adjacent values at 4 different offsets using an ld2 load with\n+    \/\/ arrangement 2D. That interleaves the lower and upper halves of\n+    \/\/ each pair of quadwords into successive vector registers. We\n+    \/\/ then need to montmul the 4 even elements of the coefficients\n+    \/\/ register sequence by the zetas in order and then add\/sub the 4\n+    \/\/ odd elements of the coefficients register sequence. We use an\n+    \/\/ equivalent st2 operation to store the results back into memory\n+    \/\/ de-interleaved.\n@@ -4930,19 +5085,11 @@\n-      __ ldpq(v30, v31, Address(dilithiumConsts, 0));  \/\/ qInv, q\n-      __ add(tmpAddr, coeffs, i);\n-      __ ld2(v0, v1, __ T2D, tmpAddr);\n-      __ add(tmpAddr, coeffs, i + 32);\n-      __ ld2(v2, v3, __ T2D, tmpAddr);\n-      __ add(tmpAddr, coeffs, i + 64);\n-      __ ld2(v4, v5, __ T2D, tmpAddr);\n-      __ add(tmpAddr, coeffs, i + 96);\n-      __ ld2(v6, v7, __ T2D, tmpAddr);\n-      dilithium_load16zetas(16, zetas);\n-      dilithium_montmul_sub_add16();\n-      __ add(tmpAddr, coeffs, i);\n-      __ st2(v0, v1, __ T2D, tmpAddr);\n-      __ add(tmpAddr, coeffs, i + 32);\n-      __ st2(v2, v3, __ T2D, tmpAddr);\n-      __ add(tmpAddr, coeffs, i + 64);\n-      __ st2(v4, v5, __ T2D, tmpAddr);\n-      __ add(tmpAddr, coeffs, i + 96);\n-      __ st2(v6, v7, __ T2D, tmpAddr);\n+      \/\/ reload constants q, qinv each iteration as they get clobbered later\n+      vs_ldpq(vq, dilithiumConsts); \/\/ qInv, q\n+      \/\/ load interleaved 16 (4x2D) coefficients via offsets\n+      vs_ld2_indexed(vs1, __ T2D, coeffs, tmpAddr, i, offsets);\n+      \/\/ load next 16 (4x4S) inputs\n+      vs_ldpq_post(vs_front(vs2), zetas);\n+      \/\/ mont multiply odd elements of vs1 by vs2 and add\/sub into odds\/evens\n+      dilithium_montmul16_sub_add(vs_even(vs1), vs_odd(vs1),\n+                                  vs_front(vs2), vtmp, vq);\n+      \/\/ store interleaved 16 (4x2D) coefficients via offsets\n+      vs_st2_indexed(vs1, __ T2D, coeffs, tmpAddr, i, offsets);\n@@ -4952,0 +5099,13 @@\n+    \/\/ at level 7 the coefficients we need to combine with the zetas\n+    \/\/ occur singly with montmul inputs alterating with add\/sub\n+    \/\/ inputs. Once again we can use 4-way parallelism to combine 16\n+    \/\/ zetas at a time. However, we have to load 8 adjacent values at\n+    \/\/ 4 different offsets using an ld2 load with arrangement 4S. That\n+    \/\/ interleaves the the odd words of each pair into one\n+    \/\/ coefficients vector register and the even words of the pair\n+    \/\/ into the next register. We then need to montmul the 4 even\n+    \/\/ elements of the coefficients register sequence by the zetas in\n+    \/\/ order and then add\/sub the 4 odd elements of the coefficients\n+    \/\/ register sequence. We use an equivalent st2 operation to store\n+    \/\/ the results back into memory de-interleaved.\n+\n@@ -4953,19 +5113,11 @@\n-      __ ldpq(v30, v31, Address(dilithiumConsts, 0));  \/\/ qInv, q\n-      __ add(tmpAddr, coeffs, i);\n-      __ ld2(v0, v1, __ T4S, tmpAddr);\n-      __ add(tmpAddr, coeffs, i + 32);\n-      __ ld2(v2, v3, __ T4S, tmpAddr);\n-      __ add(tmpAddr, coeffs, i + 64);\n-      __ ld2(v4, v5, __ T4S, tmpAddr);\n-      __ add(tmpAddr, coeffs, i + 96);\n-      __ ld2(v6, v7, __ T4S, tmpAddr);\n-      dilithium_load16zetas(16, zetas);\n-      dilithium_montmul_sub_add16();\n-      __ add(tmpAddr, coeffs, i);\n-      __ st2(v0, v1, __ T4S, tmpAddr);\n-      __ add(tmpAddr, coeffs, i + 32);\n-      __ st2(v2, v3, __ T4S, tmpAddr);\n-      __ add(tmpAddr, coeffs, i + 64);\n-      __ st2(v4, v5, __ T4S, tmpAddr);\n-      __ add(tmpAddr, coeffs, i + 96);\n-      __ st2(v6, v7, __ T4S, tmpAddr);\n+      \/\/ reload constants q, qinv each iteration as they get clobbered later\n+      vs_ldpq(vq, dilithiumConsts); \/\/ qInv, q\n+      \/\/ load interleaved 16 (4x4S) coefficients via offsets\n+      vs_ld2_indexed(vs1, __ T4S, coeffs, tmpAddr, i, offsets);\n+      \/\/ load next 16 (4x4S) inputs\n+      vs_ldpq_post(vs_front(vs2), zetas);\n+      \/\/ mont multiply odd elements of vs1 by vs2 and add\/sub into odds\/evens\n+      dilithium_montmul16_sub_add(vs_even(vs1), vs_odd(vs1),\n+                                  vs_front(vs2), vtmp, vq);\n+      \/\/ store interleaved 16 (4x4S) coefficients via offsets\n+      vs_st2_indexed(vs1, __ T4S, coeffs, tmpAddr, i, offsets);\n@@ -4978,46 +5130,0 @@\n-\n-  }\n-\n-  \/\/ Do the computations that can be found in the body of the loop in\n-  \/\/ sun.security.provider.ML_DSA.implDilithiumAlmostInverseNttJava()\n-  \/\/ for 16 coefficients in parallel:\n-  \/\/ tmp = coeffs[j];\n-  \/\/ coeffs[j] = (tmp + coeffs[j + l]);\n-  \/\/ coeffs[j + l] = montMul(tmp - coeffs[j + l], -MONT_ZETAS_FOR_NTT[m]);\n-  \/\/ coefss[j]s are loaded in v0, v2, v4 and v6,\n-  \/\/ coeffs[j + l]s in v1, v3, v5 and v7,\n-  \/\/ the corresponding zetas in v16, v17, v18 and v19.\n-  void dilithium_sub_add_montmul16() {\n-    __ subv(v20, __ T4S, v0, v1);\n-    __ subv(v21, __ T4S, v2, v3);\n-    __ subv(v22, __ T4S, v4, v5);\n-    __ subv(v23, __ T4S, v6, v7);\n-\n-    __ addv(v0, __ T4S, v0, v1);\n-    __ addv(v2, __ T4S, v2, v3);\n-    __ addv(v4, __ T4S, v4, v5);\n-    __ addv(v6, __ T4S, v6, v7);\n-\n-    __ sqdmulh(v24, __ T4S, v20, v16); \/\/ aHigh = hi32(2 * b * c)\n-    __ mulv(v1, __ T4S, v20, v16);     \/\/ aLow = lo32(b * c)\n-    __ sqdmulh(v25, __ T4S, v21, v17);\n-    __ mulv(v3, __ T4S, v21, v17);\n-    __ sqdmulh(v26, __ T4S, v22, v18);\n-    __ mulv(v5, __ T4S, v22, v18);\n-    __ sqdmulh(v27, __ T4S, v23, v19);\n-    __ mulv(v7, __ T4S, v23, v19);\n-\n-    __ mulv(v1, __ T4S, v1, v30);      \/\/ m = (aLow * q)\n-    __ mulv(v3, __ T4S, v3, v30);\n-    __ mulv(v5, __ T4S, v5, v30);\n-    __ mulv(v7, __ T4S, v7, v30);\n-\n-    __ sqdmulh(v1, __ T4S, v1, v31);  \/\/ n = hi32(2 * m * q)\n-    __ sqdmulh(v3, __ T4S, v3, v31);\n-    __ sqdmulh(v5, __ T4S, v5, v31);\n-    __ sqdmulh(v7, __ T4S, v7, v31);\n-\n-    __ shsubv(v1, __ T4S, v24, v1);  \/\/ a = (aHigh  - n) \/ 2\n-    __ shsubv(v3, __ T4S, v25, v3);\n-    __ shsubv(v5, __ T4S, v26, v5);\n-    __ shsubv(v7, __ T4S, v27, v7);\n@@ -5030,5 +5136,5 @@\n-  \/\/ We collect the coefficients that correspond to the 'j's into v0-v7\n-  \/\/ the coefficiets that correspond to the 'j+l's into v16-v23 then\n-  \/\/ do the additions into v24-v31 and the subtractions into v0-v7 then\n-  \/\/ save the result of the additions, load the zetas into v16-v23\n-  \/\/ do the (Montgomery) multiplications by zeta in parallel into v16-v23\n+  \/\/ We collect the coefficients that correspond to the 'j's into vs1\n+  \/\/ the coefficiets that correspond to the 'j+l's into vs2 then\n+  \/\/ do the additions into vs3 and the subtractions into vs1 then\n+  \/\/ save the result of the additions, load the zetas into vs2\n+  \/\/ do the (Montgomery) multiplications by zeta in parallel into vs2\n@@ -5041,3 +5147,6 @@\n-    int incr1;\n-    int incr2;\n-    int incr3;\n+    int offsets[4];\n+    VSeq<8> vs1(0), vs2(16), vs3(24);  \/\/ 3 sets of 8x4s inputs\/outputs\n+    VSeq<4> vtmp = vs_front(vs3);      \/\/ n.b. tmp registers overlap vs3\n+    VSeq<2> vq(30);                    \/\/ n.b. constants overlap vs3\n+\n+    offsets[0] = 0;\n@@ -5049,3 +5158,3 @@\n-        incr1 = 64;\n-        incr2 = 128;\n-        incr3 = 192;\n+        offsets[1] = 64;\n+        offsets[2] = 128;\n+        offsets[3] = 192;\n@@ -5053,3 +5162,3 @@\n-        incr1 = 32;\n-        incr2 = 128;\n-        incr3 = 160;\n+        offsets[1] = 32;\n+        offsets[2] = 128;\n+        offsets[3] = 160;\n@@ -5057,3 +5166,3 @@\n-        incr1 = 32;\n-        incr2 = 64;\n-        incr3 = 96;\n+        offsets[1] = 32;\n+        offsets[2] = 64;\n+        offsets[3] = 96;\n@@ -5062,0 +5171,4 @@\n+      \/\/ for levels 3 - 7 we simply load 2 x 4 adjacent values at a\n+      \/\/ time at 4 different offsets and multiply them in order by the\n+      \/\/ next set of input values. So we employ indexed load and store\n+      \/\/ pair instructions with arrangement 4S\n@@ -5063,20 +5176,18 @@\n-        __ ldpq(v0, v1, Address(coeffs, c1Start));\n-        __ ldpq(v2, v3, Address(coeffs, c1Start + incr1));\n-        __ ldpq(v4, v5, Address(coeffs, c1Start + incr2));\n-        __ ldpq(v6, v7, Address(coeffs, c1Start + incr3));\n-        __ ldpq(v16, v17, Address(coeffs, c2Start));\n-        __ ldpq(v18, v19, Address(coeffs, c2Start + incr1));\n-        __ ldpq(v20, v21, Address(coeffs, c2Start + incr2));\n-        __ ldpq(v22, v23, Address(coeffs, c2Start + incr3));\n-        dilithium_add_sub32();\n-        __ stpq(v24, v25, Address(coeffs, c1Start));\n-        __ stpq(v26, v27, Address(coeffs, c1Start + incr1));\n-        __ stpq(v28, v29, Address(coeffs, c1Start + incr2));\n-        __ stpq(v30, v31, Address(coeffs, c1Start + incr3));\n-        __ ldpq(v30, v31, Address(dilithiumConsts, 0));   \/\/ qInv, q\n-        dilithium_load32zetas(zetas);\n-        dilithium_montmul32(false);\n-        __ stpq(v16, v17, Address(coeffs, c2Start));\n-        __ stpq(v18, v19, Address(coeffs, c2Start + incr1));\n-        __ stpq(v20, v21, Address(coeffs, c2Start + incr2));\n-        __ stpq(v22, v23, Address(coeffs, c2Start + incr3));\n+        \/\/ load v1 32 (8x4S) coefficients relative to first start index\n+        vs_ldpq_indexed(vs1, coeffs, c1Start, offsets);\n+        \/\/ load v2 32 (8x4S) coefficients relative to second start index\n+        vs_ldpq_indexed(vs2, coeffs, c2Start, offsets);\n+        \/\/ a0 = v1 + v2 -- n.b. clobbers vqs\n+        vs_addv(vs3, __ T4S, vs1, vs2);\n+        \/\/ a1 = v1 - v2\n+        vs_subv(vs1, __ T4S, vs1, vs2);\n+        \/\/ save a1 relative to first start index\n+        vs_stpq_indexed(vs3, coeffs, c1Start, offsets);\n+        \/\/ load constants q, qinv each iteration as they get clobbered above\n+        vs_ldpq(vq, dilithiumConsts); \/\/ qInv, q\n+        \/\/ load b next 32 (8x4S) inputs\n+        vs_ldpq_post(vs2, zetas);\n+        \/\/ a = a1 montmul b\n+        vs_montmul32(vs2, vs1, vs2, vtmp, vq);\n+        \/\/ save a relative to second start index\n+        vs_stpq_indexed(vs2, coeffs, c2Start, offsets);\n@@ -5123,0 +5234,6 @@\n+    VSeq<8> vs1(0), vs2(16), vs3(24);  \/\/ 3 sets of 8x4s inputs\/outputs\n+    VSeq<4> vtmp = vs_front(vs3);     \/\/ n.b. tmp registers overlap vs3\n+    VSeq<2> vq(30);                    \/\/ n.b. constants overlap vs3\n+    int offsets[4] = { 0, 32, 64, 96 };\n+    int offsets1[8] = { 0, 32, 64, 96, 128, 160, 192, 224 };\n+    int offsets2[8] = { 16, 48, 80, 112, 144, 176, 208, 240 };\n@@ -5129,0 +5246,6 @@\n+\n+    \/\/ level 0\n+    \/\/ At level 0 we need to interleave adjacent quartets of\n+    \/\/ coefficients before we multiply and add\/sub by the next 16\n+    \/\/ zetas just as we did for level 7 in the multiply code. So we\n+    \/\/ load and store the values using an ld2\/st2 with arrangement 4S\n@@ -5130,19 +5253,14 @@\n-      __ ldpq(v30, v31, Address(dilithiumConsts, 0));  \/\/ qInv, q\n-      __ add(tmpAddr, coeffs, i);\n-      __ ld2(v0, v1, __ T4S, tmpAddr);\n-      __ add(tmpAddr, coeffs, i + 32);\n-      __ ld2(v2, v3, __ T4S, tmpAddr);\n-      __ add(tmpAddr, coeffs, i + 64);\n-      __ ld2(v4, v5, __ T4S, tmpAddr);\n-      __ add(tmpAddr, coeffs, i + 96);\n-      __ ld2(v6, v7, __ T4S, tmpAddr);\n-      dilithium_load16zetas(16, zetas);\n-      dilithium_sub_add_montmul16();\n-      __ add(tmpAddr, coeffs, i);\n-      __ st2(v0, v1, __ T4S, tmpAddr);\n-      __ add(tmpAddr, coeffs, i + 32);\n-      __ st2(v2, v3, __ T4S, tmpAddr);\n-      __ add(tmpAddr, coeffs, i + 64);\n-      __ st2(v4, v5, __ T4S, tmpAddr);\n-      __ add(tmpAddr, coeffs, i + 96);\n-      __ st2(v6, v7, __ T4S, tmpAddr);\n+      \/\/ load constants q, qinv\n+      \/\/ n.b. this can be moved out of the loop as they do not get\n+      \/\/ clobbered by first two loops\n+      vs_ldpq(vq, dilithiumConsts); \/\/ qInv, q\n+      \/\/ a0\/a1 load interleaved 32 (8x4S) coefficients\n+      vs_ld2_indexed(vs1, __ T4S, coeffs, tmpAddr, i, offsets);\n+      \/\/ b load next 32 (8x4S) inputs\n+      vs_ldpq_post(vs_front(vs2), zetas);\n+      \/\/ compute in parallel (a0, a1) = (a0 + a1, (a0 - a1) montmul b)\n+      \/\/ n.b. second half of vs2 provides temporary register storage\n+      dilithium_sub_add_montmul16(vs_even(vs1), vs_odd(vs1),\n+                                  vs_front(vs2), vs_back(vs2), vtmp, vq);\n+      \/\/ a0\/a1 store interleaved 32 (8x4S) coefficients\n+      vs_st2_indexed(vs1, __ T4S, coeffs, tmpAddr, i, offsets);\n@@ -5152,0 +5270,4 @@\n+    \/\/ At level 1 we need to interleave pairs of adjacent pairs of\n+    \/\/ coefficients before we multiply by the next 16 zetas just as we\n+    \/\/ did for level 6 in the multiply code. So we load and store the\n+    \/\/ values an ld2\/st2 with arrangement 2D\n@@ -5153,18 +5275,10 @@\n-      __ add(tmpAddr, coeffs, i);\n-      __ ld2(v0, v1, __ T2D, tmpAddr);\n-      __ add(tmpAddr, coeffs, i + 32);\n-      __ ld2(v2, v3, __ T2D, tmpAddr);\n-      __ add(tmpAddr, coeffs, i + 64);\n-      __ ld2(v4, v5, __ T2D, tmpAddr);\n-      __ add(tmpAddr, coeffs, i + 96);\n-      __ ld2(v6, v7, __ T2D, tmpAddr);\n-      dilithium_load16zetas(16, zetas);\n-      dilithium_sub_add_montmul16();\n-      __ add(tmpAddr, coeffs, i);\n-      __ st2(v0, v1, __ T2D, tmpAddr);\n-      __ add(tmpAddr, coeffs, i + 32);\n-      __ st2(v2, v3, __ T2D, tmpAddr);\n-      __ add(tmpAddr, coeffs, i + 64);\n-      __ st2(v4, v5, __ T2D, tmpAddr);\n-      __ add(tmpAddr, coeffs, i + 96);\n-      __ st2(v6, v7, __ T2D, tmpAddr);\n+      \/\/ a0\/a1 load interleaved 32 (8x2D) coefficients\n+      vs_ld2_indexed(vs1, __ T2D, coeffs, tmpAddr, i, offsets);\n+      \/\/ b load next 16 (4x4S) inputs\n+      vs_ldpq_post(vs_front(vs2), zetas);\n+      \/\/ compute in parallel (a0, a1) = (a0 + a1, (a0 - a1) montmul b)\n+      \/\/ n.b. second half of vs2 provides temporary register storage\n+      dilithium_sub_add_montmul16(vs_even(vs1), vs_odd(vs1),\n+                                  vs_front(vs2), vs_back(vs2), vtmp, vq);\n+      \/\/ a0\/a1 store interleaved 32 (8x2D) coefficients\n+      vs_st2_indexed(vs1, __ T2D, coeffs, tmpAddr, i, offsets);\n@@ -5173,1 +5287,6 @@\n-    \/\/level 2\n+    \/\/ level 2\n+    \/\/ At level 2 coefficients come in blocks of 4. So, we load 4\n+    \/\/ adjacent coefficients at 8 distinct offsets for both the first\n+    \/\/ and second coefficient sequences, using an ldr with register\n+    \/\/ variant Q then combine them with next set of 32 zetas. Likewise\n+    \/\/ we store the results using an str with register variant Q.\n@@ -5175,36 +5294,18 @@\n-      __ ldr(v0, __ Q, Address(coeffs, i));\n-      __ ldr(v1, __ Q, Address(coeffs, i + 32));\n-      __ ldr(v2, __ Q, Address(coeffs, i + 64));\n-      __ ldr(v3, __ Q, Address(coeffs, i + 96));\n-      __ ldr(v4, __ Q, Address(coeffs, i + 128));\n-      __ ldr(v5, __ Q, Address(coeffs, i + 160));\n-      __ ldr(v6, __ Q, Address(coeffs, i + 192));\n-      __ ldr(v7, __ Q, Address(coeffs, i + 224));\n-      __ ldr(v16, __ Q, Address(coeffs, i + 16));\n-      __ ldr(v17, __ Q, Address(coeffs, i + 48));\n-      __ ldr(v18, __ Q, Address(coeffs, i + 80));\n-      __ ldr(v19, __ Q, Address(coeffs, i + 112));\n-      __ ldr(v20, __ Q, Address(coeffs, i + 144));\n-      __ ldr(v21, __ Q, Address(coeffs, i + 176));\n-      __ ldr(v22, __ Q, Address(coeffs, i + 208));\n-      __ ldr(v23, __ Q, Address(coeffs, i + 240));\n-      dilithium_add_sub32();\n-      __ str(v24, __ Q, Address(coeffs, i));\n-      __ str(v25, __ Q, Address(coeffs, i + 32));\n-      __ str(v26, __ Q, Address(coeffs, i + 64));\n-      __ str(v27, __ Q, Address(coeffs, i + 96));\n-      __ str(v28, __ Q, Address(coeffs, i + 128));\n-      __ str(v29, __ Q, Address(coeffs, i + 160));\n-      __ str(v30, __ Q, Address(coeffs, i + 192));\n-      __ str(v31, __ Q, Address(coeffs, i + 224));\n-      dilithium_load32zetas(zetas);\n-      __ ldpq(v30, v31, Address(dilithiumConsts, 0));  \/\/ qInv, q\n-      dilithium_montmul32(false);\n-      __ str(v16, __ Q, Address(coeffs, i + 16));\n-      __ str(v17, __ Q, Address(coeffs, i + 48));\n-      __ str(v18, __ Q, Address(coeffs, i + 80));\n-      __ str(v19, __ Q, Address(coeffs, i + 112));\n-      __ str(v20, __ Q, Address(coeffs, i + 144));\n-      __ str(v21, __ Q, Address(coeffs, i + 176));\n-      __ str(v22, __ Q, Address(coeffs, i + 208));\n-      __ str(v23, __ Q, Address(coeffs, i + 240));\n+      \/\/ c0 load 32 (8x4S) coefficients via first offsets\n+      vs_ldr_indexed(vs1, __ Q, coeffs, i, offsets1);\n+      \/\/ c1 load 32 (8x4S) coefficients via second offsets\n+      vs_ldr_indexed(vs2, __ Q,coeffs, i, offsets2);\n+      \/\/ a0 = c0 + c1  n.b. clobbers vq which overlaps vs3\n+      vs_addv(vs3, __ T4S, vs1, vs2);\n+      \/\/ c = c0 - c1\n+      vs_subv(vs1, __ T4S, vs1, vs2);\n+      \/\/ store a0 32 (8x4S) coefficients via first offsets\n+      vs_str_indexed(vs3, __ Q, coeffs, i, offsets1);\n+      \/\/ b load 32 (8x4S) next inputs\n+      vs_ldpq_post(vs2, zetas);\n+      \/\/ reload constants q, qinv -- they were clobbered earlier\n+      vs_ldpq(vq, dilithiumConsts); \/\/ qInv, q\n+      \/\/ compute a1 = b montmul c\n+      vs_montmul32(vs2, vs1, vs2, vtmp, vq);\n+      \/\/ store a1 32 (8x4S) coefficients via second offsets\n+      vs_str_indexed(vs2, __ Q, coeffs, i, offsets2);\n@@ -5235,1 +5336,1 @@\n-    __ align(CodeEntryAlignment);\n+        __ align(CodeEntryAlignment);\n@@ -5250,0 +5351,5 @@\n+    VSeq<8> vs1(0), vs2(16), vs3(24);  \/\/ 3 sets of 8x4s inputs\/outputs\n+    VSeq<4> vtmp = vs_front(vs3);         \/\/ n.b. tmp registers overlap vs3\n+    VSeq<2> vq(30);                    \/\/ n.b. constants overlap vs3\n+    VSeq<8> vrsquare(29, 0);           \/\/ for montmul by constant RSQUARE\n+\n@@ -5252,1 +5358,3 @@\n-    __ ldpq(v30, v31, Address(dilithiumConsts, 0));   \/\/ qInv, q\n+    \/\/ load constants q, qinv\n+    vs_ldpq(vq, dilithiumConsts); \/\/ qInv, q\n+    \/\/ load constant rSquare into v29\n@@ -5260,14 +5368,10 @@\n-    __ ldpq(v0, v1, __ post(poly1, 32));\n-    __ ldpq(v2, v3, __ post(poly1, 32));\n-    __ ldpq(v4, v5, __ post(poly1, 32));\n-    __ ldpq(v6, v7, __ post(poly1, 32));\n-    __ ldpq(v16, v17, __ post(poly2, 32));\n-    __ ldpq(v18, v19, __ post(poly2, 32));\n-    __ ldpq(v20, v21, __ post(poly2, 32));\n-    __ ldpq(v22, v23, __ post(poly2, 32));\n-    dilithium_montmul32(false);\n-    dilithium_montmul32(true);\n-    __ stpq(v16, v17, __ post(result, 32));\n-    __ stpq(v18, v19, __ post(result, 32));\n-    __ stpq(v20, v21, __ post(result, 32));\n-    __ stpq(v22, v23, __ post(result, 32));\n+    \/\/ b load 32 (8x4S) next inputs from poly1\n+    vs_ldpq_post(vs1, poly1);\n+    \/\/ c load 32 (8x4S) next inputs from poly2\n+    vs_ldpq_post(vs2, poly2);\n+    \/\/ compute a = b montmul c\n+    vs_montmul32(vs2, vs1, vs2, vtmp, vq);\n+    \/\/ compute a = rsquare montmul a\n+    vs_montmul32(vs2, vrsquare, vs2, vtmp, vq);\n+    \/\/ save a 32 (8x4S) results\n+    vs_stpq_post(vs2, result);\n@@ -5311,0 +5415,6 @@\n+    VSeq<8> vs1(0), vs2(16), vs3(24);  \/\/ 3 sets of 8x4s inputs\/outputs\n+    VSeq<4> vtmp = vs_front(vs3);         \/\/ n.b. tmp registers overlap vs3\n+    VSeq<2> vq(30);                    \/\/ n.b. constants overlap vs3\n+    VSeq<8> vconst(29, 0);             \/\/ for montmul by constant\n+\n+    \/\/ results track inputs\n@@ -5314,2 +5424,4 @@\n-    __ ldpq(v30, v31, Address(dilithiumConsts, 0));   \/\/ qInv, q\n-    __ dup(v29, __ T4S, constant);\n+    \/\/ load constants q, qinv -- they do not get clobbered by first two loops\n+    vs_ldpq(vq, dilithiumConsts); \/\/ qInv, q\n+    \/\/ copy caller supplied constant across vconst\n+    __ dup(vconst[0], __ T4S, constant);\n@@ -5321,9 +5433,6 @@\n-    __ ldpq(v16, v17, __ post(coeffs, 32));\n-    __ ldpq(v18, v19, __ post(coeffs, 32));\n-    __ ldpq(v20, v21, __ post(coeffs, 32));\n-    __ ldpq(v22, v23, __ post(coeffs, 32));\n-    dilithium_montmul32(true);\n-    __ stpq(v16, v17, __ post(result, 32));\n-    __ stpq(v18, v19, __ post(result, 32));\n-    __ stpq(v20, v21, __ post(result, 32));\n-    __ stpq(v22, v23, __ post(result, 32));\n+    \/\/ load next 32 inputs\n+    vs_ldpq_post(vs2, coeffs);\n+    \/\/ mont mul by constant\n+    vs_montmul32(vs2, vconst, vs2, vtmp, vq);\n+    \/\/ write next 32 results\n+    vs_stpq_post(vs2, result);\n@@ -5340,0 +5449,1 @@\n+\n@@ -5358,2 +5468,0 @@\n-    __ enter();\n-\n@@ -5372,0 +5480,12 @@\n+    VSeq<4> vs1(0), vs2(4), vs3(8); \/\/ 6 independent sets of 4x4s values\n+    VSeq<4> vs4(12), vs5(16), vtmp(20);\n+    VSeq<4> one(25, 0);            \/\/ 7 constants for cross-multiplying\n+    VSeq<4> qminus1(26, 0);\n+    VSeq<4> g2(27, 0);\n+    VSeq<4> twog2(28, 0);\n+    VSeq<4> mult(29, 0);\n+    VSeq<4> q(30, 0);\n+    VSeq<4> qadd(31, 0);\n+\n+    __ enter();\n+\n@@ -5380,1 +5500,1 @@\n-\n+    \/\/ populate constant registers\n@@ -5383,7 +5503,7 @@\n-    __ dup(v25, __ T4S, tmp); \/\/ 1\n-    __ ldr(v30, __ Q, Address(dilithiumConsts, 16)); \/\/ q\n-    __ ldr(v31, __ Q, Address(dilithiumConsts, 64)); \/\/ addend for mod q reduce\n-    __ dup(v28, __ T4S, twoGamma2); \/\/ 2 * gamma2\n-    __ dup(v29, __ T4S, multiplier); \/\/ multiplier for mod 2 * gamma reduce\n-    __ subv(v26, __ T4S, v30, v25); \/\/ q - 1\n-    __ sshr(v27, __ T4S, v28, 1); \/\/ gamma2\n+    __ dup(one[0], __ T4S, tmp); \/\/ 1\n+    __ ldr(q[0], __ Q, Address(dilithiumConsts, 16)); \/\/ q\n+    __ ldr(qadd[0], __ Q, Address(dilithiumConsts, 64)); \/\/ addend for mod q reduce\n+    __ dup(twog2[0], __ T4S, twoGamma2); \/\/ 2 * gamma2\n+    __ dup(mult[0], __ T4S, multiplier); \/\/ multiplier for mod 2 * gamma reduce\n+    __ subv(qminus1[0], __ T4S, v30, v25); \/\/ q - 1\n+    __ sshr(g2[0], __ T4S, v28, 1); \/\/ gamma2\n@@ -5396,8 +5516,2 @@\n-    __ ld4(v0, v1, v2, v3, __ T4S, __ post(input, 64));\n-\n-    \/\/ rplus in v0\n-    \/\/  rplus = rplus - ((rplus + 5373807) >> 23) * dilithium_q;\n-    __ addv(v4, __ T4S, v0, v31);\n-    __ addv(v5, __ T4S, v1, v31);\n-    __ addv(v6, __ T4S, v2, v31);\n-    __ addv(v7, __ T4S, v3, v31);\n+    \/\/ load next 4x4S inputs interleaved: rplus --> vs1\n+    __ ld4(vs1[0], vs1[1], vs1[2], vs1[3], __ T4S, __ post(input, 64));\n@@ -5405,4 +5519,5 @@\n-    __ sshr(v4, __ T4S, v4, 23);\n-    __ sshr(v5, __ T4S, v5, 23);\n-    __ sshr(v6, __ T4S, v6, 23);\n-    __ sshr(v7, __ T4S, v7, 23);\n+    \/\/  rplus = rplus - ((rplus + qadd) >> 23) * q\n+    vs_addv(vtmp, __ T4S, vs1, qadd);\n+    vs_sshr(vtmp, __ T4S, vtmp, 23);\n+    vs_mulv(vtmp, __ T4S, vtmp, q);\n+    vs_subv(vs1, __ T4S, vs1, vtmp);\n@@ -5410,11 +5525,0 @@\n-    __ mulv(v4, __ T4S, v4, v30);\n-    __ mulv(v5, __ T4S, v5, v30);\n-    __ mulv(v6, __ T4S, v6, v30);\n-    __ mulv(v7, __ T4S, v7, v30);\n-\n-    __ subv(v0, __ T4S, v0, v4);\n-    __ subv(v1, __ T4S, v1, v5);\n-    __ subv(v2, __ T4S, v2, v6);\n-    __ subv(v3, __ T4S, v3, v7);\n-\n-    \/\/ rplus in v0\n@@ -5422,21 +5526,3 @@\n-    __ sshr(v4, __ T4S, v0, 31);\n-    __ sshr(v5, __ T4S, v1, 31);\n-    __ sshr(v6, __ T4S, v2, 31);\n-    __ sshr(v7, __ T4S, v3, 31);\n-\n-    __ andr(v4, __ T16B, v4, v30);\n-    __ andr(v5, __ T16B, v5, v30);\n-    __ andr(v6, __ T16B, v6, v30);\n-    __ andr(v7, __ T16B, v7, v30);\n-\n-    __ addv(v0, __ T4S, v0, v4);\n-    __ addv(v1, __ T4S, v1, v5);\n-    __ addv(v2, __ T4S, v2, v6);\n-    __ addv(v3, __ T4S, v3, v7);\n-\n-    \/\/ rplus in v0\n-    \/\/ int quotient = (rplus * multiplier) >> 22;\n-    __ mulv(v4, __ T4S, v0, v29);\n-    __ mulv(v5, __ T4S, v1, v29);\n-    __ mulv(v6, __ T4S, v2, v29);\n-    __ mulv(v7, __ T4S, v3, v29);\n+    vs_sshr(vtmp, __ T4S, vs1, 31);\n+    vs_andr(vtmp, vtmp, q);\n+    vs_addv(vs1, __ T4S, vs1, vtmp);\n@@ -5444,4 +5530,4 @@\n-    __ sshr(v4, __ T4S, v4, 22);\n-    __ sshr(v5, __ T4S, v5, 22);\n-    __ sshr(v6, __ T4S, v6, 22);\n-    __ sshr(v7, __ T4S, v7, 22);\n+    \/\/ quotient --> vs2\n+    \/\/ int quotient = (rplus * multiplier) >> 22;\n+    vs_mulv(vtmp, __ T4S, vs1, mult);\n+    vs_sshr(vs2, __ T4S, vtmp, 22);\n@@ -5449,1 +5535,1 @@\n-    \/\/ quotient in v4\n+    \/\/ r0 --> vs3\n@@ -5451,9 +5537,2 @@\n-    __ mulv(v8, __ T4S, v4, v28);\n-    __ mulv(v9, __ T4S, v5, v28);\n-    __ mulv(v10, __ T4S, v6, v28);\n-    __ mulv(v11, __ T4S, v7, v28);\n-\n-    __ subv(v8, __ T4S, v0, v8);\n-    __ subv(v9, __ T4S, v1, v9);\n-    __ subv(v10, __ T4S, v2, v10);\n-    __ subv(v11, __ T4S, v3, v11);\n+    vs_mulv(vtmp, __ T4S, vs2, twog2);\n+    vs_subv(vs3, __ T4S, vs1, vtmp);\n@@ -5461,1 +5540,1 @@\n-    \/\/ r0 in v8\n+    \/\/ mask --> vs4\n@@ -5463,9 +5542,2 @@\n-    __ subv(v12, __ T4S, v28, v8);\n-    __ subv(v13, __ T4S, v28, v9);\n-    __ subv(v14, __ T4S, v28, v10);\n-    __ subv(v15, __ T4S, v28, v11);\n-\n-    __ sshr(v12, __ T4S, v12, 22);\n-    __ sshr(v13, __ T4S, v13, 22);\n-    __ sshr(v14, __ T4S, v14, 22);\n-    __ sshr(v15, __ T4S, v15, 22);\n+    vs_subv(vtmp, __ T4S, twog2, vs3);\n+    vs_sshr(vs4, __ T4S, vtmp, 22);\n@@ -5473,1 +5545,0 @@\n-    \/\/ mask in v12\n@@ -5475,4 +5546,2 @@\n-    __ andr(v16, __ T16B, v12, v28);\n-    __ andr(v17, __ T16B, v13, v28);\n-    __ andr(v18, __ T16B, v14, v28);\n-    __ andr(v19, __ T16B, v15, v28);\n+    vs_andr(vtmp, vs4, twog2);\n+    vs_subv(vs3, __ T4S, vs3, vtmp);\n@@ -5480,6 +5549,0 @@\n-    __ subv(v8, __ T4S, v8, v16);\n-    __ subv(v9, __ T4S, v9, v17);\n-    __ subv(v10, __ T4S, v10, v18);\n-    __ subv(v11, __ T4S, v11, v19);\n-\n-    \/\/ r0 in v8\n@@ -5487,9 +5550,2 @@\n-    __ andr(v16, __ T16B, v12, v25);\n-    __ andr(v17, __ T16B, v13, v25);\n-    __ andr(v18, __ T16B, v14, v25);\n-    __ andr(v19, __ T16B, v15, v25);\n-\n-    __ addv(v4, __ T4S, v4, v16);\n-    __ addv(v5, __ T4S, v5, v17);\n-    __ addv(v6, __ T4S, v6, v18);\n-    __ addv(v7, __ T4S, v7, v19);\n+    vs_andr(vtmp, vs4, one);\n+    vs_addv(vs2, __ T4S, vs2, vtmp);\n@@ -5498,9 +5554,2 @@\n-    __ subv(v12, __ T4S, v27, v8);\n-    __ subv(v13, __ T4S, v27, v9);\n-    __ subv(v14, __ T4S, v27, v10);\n-    __ subv(v15, __ T4S, v27, v11);\n-\n-    __ sshr(v12, __ T4S, v12, 31);\n-    __ sshr(v13, __ T4S, v13, 31);\n-    __ sshr(v14, __ T4S, v14, 31);\n-    __ sshr(v15, __ T4S, v15, 31);\n+    vs_subv(vtmp, __ T4S, g2, vs3);\n+    vs_sshr(vs4, __ T4S, vtmp, 31);\n@@ -5509,9 +5558,2 @@\n-    __ andr(v16, __ T16B, v12, v28);\n-    __ andr(v17, __ T16B, v13, v28);\n-    __ andr(v18, __ T16B, v14, v28);\n-    __ andr(v19, __ T16B, v15, v28);\n-\n-    __ subv(v8, __ T4S, v8, v16);\n-    __ subv(v9, __ T4S, v9, v17);\n-    __ subv(v10, __ T4S, v10, v18);\n-    __ subv(v11, __ T4S, v11, v19);\n+    vs_andr(vtmp, vs4, twog2);\n+    vs_subv(vs3, __ T4S, vs3, vtmp);\n@@ -5520,9 +5562,2 @@\n-    __ andr(v16, __ T16B, v12, v25);\n-    __ andr(v17, __ T16B, v13, v25);\n-    __ andr(v18, __ T16B, v14, v25);\n-    __ andr(v19, __ T16B, v15, v25);\n-\n-    __ addv(v4, __ T4S, v4, v16);\n-    __ addv(v5, __ T4S, v5, v17);\n-    __ addv(v6, __ T4S, v6, v18);\n-    __ addv(v7, __ T4S, v7, v19);\n+    vs_andr(vtmp, vs4, one);\n+    vs_addv(vs2, __ T4S, vs2, vtmp);\n@@ -5530,0 +5565,1 @@\n+    \/\/ r1 --> vs5\n@@ -5531,9 +5567,2 @@\n-    __ subv(v16, __ T4S, v0, v8);\n-    __ subv(v17, __ T4S, v1, v9);\n-    __ subv(v18, __ T4S, v2, v10);\n-    __ subv(v19, __ T4S, v3, v11);\n-\n-    __ subv(v16, __ T4S, v16, v26);\n-    __ subv(v17, __ T4S, v17, v26);\n-    __ subv(v18, __ T4S, v18, v26);\n-    __ subv(v19, __ T4S, v19, v26);\n+    vs_subv(vtmp, __ T4S, vs1, vs3);\n+    vs_subv(vs5, __ T4S, vtmp, qminus1);\n@@ -5541,1 +5570,1 @@\n-    \/\/ r1 in v16\n+    \/\/ r1 --> vs1 (overwriting rplus)\n@@ -5543,21 +5572,3 @@\n-    __ negr(v20, __ T4S, v16);\n-    __ negr(v21, __ T4S, v17);\n-    __ negr(v22, __ T4S, v18);\n-    __ negr(v23, __ T4S, v19);\n-\n-    __ orr(v16, __ T16B, v16, v20);\n-    __ orr(v17, __ T16B, v17, v21);\n-    __ orr(v18, __ T16B, v18, v22);\n-    __ orr(v19, __ T16B, v19, v23);\n-\n-    __ sshr(v0, __ T4S, v16, 31);\n-    __ sshr(v1, __ T4S, v17, 31);\n-    __ sshr(v2, __ T4S, v18, 31);\n-    __ sshr(v3, __ T4S, v19, 31);\n-\n-    \/\/ r1 in v0\n-    \/\/ r0 += ~r1;\n-    __ notr(v20, __ T16B, v0);\n-    __ notr(v21, __ T16B, v1);\n-    __ notr(v22, __ T16B, v2);\n-    __ notr(v23, __ T16B, v3);\n+    vs_negr(vtmp, __ T4S, vs5);\n+    vs_orr(vtmp, vs5, vtmp);\n+    vs_sshr(vs1, __ T4S, vtmp, 31);\n@@ -5565,4 +5576,3 @@\n-    __ addv(v8, __ T4S, v8, v20);\n-    __ addv(v9, __ T4S, v9, v21);\n-    __ addv(v10, __ T4S, v10, v22);\n-    __ addv(v11, __ T4S, v11, v23);\n+    \/\/ r0 += ~r1;\n+    vs_notr(vtmp, vs1);\n+    vs_addv(vs3, __ T4S, vs3, vtmp);\n@@ -5570,1 +5580,0 @@\n-    \/\/ r0 in v8\n@@ -5572,4 +5581,1 @@\n-    __ andr(v0, __ T16B, v4, v0);\n-    __ andr(v1, __ T16B, v5, v1);\n-    __ andr(v2, __ T16B, v6, v2);\n-    __ andr(v3, __ T16B, v7, v3);\n+    vs_andr(vs1, vs2, vs1);\n@@ -5577,1 +5583,1 @@\n-    \/\/ r1 in v0\n+    \/\/ store results inteleaved\n@@ -5580,2 +5586,2 @@\n-    __ st4(v8, v9, v10, v11, __ T4S, __ post(lowPart, 64));\n-    __ st4(v0, v1, v2, v3, __ T4S, __ post(highPart, 64));\n+    __ st4(vs3[0], vs3[1], vs3[2], vs3[3], __ T4S, __ post(lowPart, 64));\n+    __ st4(vs1[0], vs1[1], vs1[2], vs1[3], __ T4S, __ post(highPart, 64));\n@@ -5599,0 +5605,1 @@\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/stubGenerator_aarch64.cpp","additions":591,"deletions":584,"binary":false,"changes":1175,"status":"modified"}]}