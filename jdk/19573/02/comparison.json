{"files":[{"patch":"@@ -71,3 +71,4 @@\n-  flags(SUPERWORD1_BEFORE_SCHEDULE,     \"Superword 1, Before Schedule\") \\\n-  flags(SUPERWORD2_BEFORE_OUTPUT,       \"Superword 2, Before Output\") \\\n-  flags(SUPERWORD3_AFTER_OUTPUT,        \"Superword 3, After Output\") \\\n+  flags(AUTO_VECTORIZATION1_BEFORE_APPLY,       \"AutoVectorization 1, Before Apply\") \\\n+  flags(AUTO_VECTORIZATION2_AFTER_REORDER,      \"AutoVectorization 2, After Apply Memop Reordering\") \\\n+  flags(AUTO_VECTORIZATION3_AFTER_ADJUST_LIMIT, \"AutoVectorization 3, After Adjusting Pre-Loop Limit\") \\\n+  flags(AUTO_VECTORIZATION4_AFTER_APPLY,        \"AutoVectorization 4, After Apply\") \\\n","filename":"src\/hotspot\/share\/opto\/phasetype.hpp","additions":4,"deletions":3,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -484,0 +484,1 @@\n+  DEBUG_ONLY(verify_no_extract());\n@@ -485,3 +486,1 @@\n-  schedule();\n-\n-  return output();\n+  return schedule_and_apply();\n@@ -1469,1 +1468,1 @@\n-  if (!vectors_should_be_aligned()) {\n+  if (!VLoop::vectors_should_be_aligned()) {\n@@ -1595,6 +1594,2 @@\n-    } else if (requires_long_to_int_conversion(opc)) {\n-      \/\/ Java API for Long.bitCount\/numberOfLeadingZeros\/numberOfTrailingZeros\n-      \/\/ returns int type, but Vector API for them returns long type. To unify\n-      \/\/ the implementation in backend, superword splits the vector implementation\n-      \/\/ for Java API into an execution node with long type plus another node\n-      \/\/ converting long to int.\n+    } else if (VectorNode::is_scalar_op_that_returns_int_but_vector_op_returns_long(opc)) {\n+      \/\/ Requires extra vector long -> int conversion.\n@@ -1604,5 +1599,1 @@\n-      \/\/ Vector unsigned right shift for signed subword types behaves differently\n-      \/\/ from Java Spec. But when the shift amount is a constant not greater than\n-      \/\/ the number of sign extended bits, the unsigned right shift can be\n-      \/\/ vectorized to a signed right shift.\n-      if (VectorNode::can_transform_shift_op(p0, velt_basic_type(p0))) {\n+      if (VectorNode::can_use_RShiftI_instead_of_URShiftI(p0, velt_basic_type(p0))) {\n@@ -1633,13 +1624,7 @@\n-\/\/ Java API for Long.bitCount\/numberOfLeadingZeros\/numberOfTrailingZeros\n-\/\/ returns int type, but Vector API for them returns long type. To unify\n-\/\/ the implementation in backend, superword splits the vector implementation\n-\/\/ for Java API into an execution node with long type plus another node\n-\/\/ converting long to int.\n-bool SuperWord::requires_long_to_int_conversion(int opc) {\n-  switch(opc) {\n-    case Op_PopCountL:\n-    case Op_CountLeadingZerosL:\n-    case Op_CountTrailingZerosL:\n-      return true;\n-    default:\n-      return false;\n+\/\/ If the j-th input for all nodes in the pack is the same unique input: return it, else nullptr.\n+Node* PackSet::same_inputs_at_index_or_null(const Node_List* pack, int j) const {\n+  Node* unique = pack->at(0)->in(j);\n+  for (uint i = 1; i < pack->size(); i++) {\n+    if (pack->at(i)->in(j) != unique) {\n+      return nullptr; \/\/ not unique\n+    }\n@@ -1647,0 +1632,1 @@\n+  return unique;\n@@ -1649,11 +1635,66 @@\n-\/\/------------------------------same_inputs--------------------------\n-\/\/ For pack p, are all idx operands the same?\n-bool SuperWord::same_inputs(const Node_List* p, int idx) const {\n-  Node* p0 = p->at(0);\n-  uint vlen = p->size();\n-  Node* p0_def = p0->in(idx);\n-  for (uint i = 1; i < vlen; i++) {\n-    Node* pi = p->at(i);\n-    Node* pi_def = pi->in(idx);\n-    if (p0_def != pi_def) {\n-      return false;\n+VTransformBoolTest PackSet::get_bool_test(const Node_List* bool_pack) const {\n+  BoolNode* bol = bool_pack->at(0)->as_Bool();\n+  BoolTest::mask mask = bol->_test._test;\n+  bool is_negated = false;\n+  assert(mask == BoolTest::eq ||\n+         mask == BoolTest::ne ||\n+         mask == BoolTest::ge ||\n+         mask == BoolTest::gt ||\n+         mask == BoolTest::lt ||\n+         mask == BoolTest::le,\n+         \"Bool should be one of: eq, ne, ge, gt, lt, le\");\n+\n+#ifdef ASSERT\n+  for (uint j = 0; j < bool_pack->size(); j++) {\n+    Node* m = bool_pack->at(j);\n+    assert(m->as_Bool()->_test._test == mask,\n+           \"all bool nodes must have same test\");\n+  }\n+#endif\n+\n+  CmpNode* cmp0 = bol->in(1)->as_Cmp();\n+  assert(get_pack(cmp0) != nullptr, \"Bool must have matching Cmp pack\");\n+\n+  if (cmp0->Opcode() == Op_CmpF || cmp0->Opcode() == Op_CmpD) {\n+    \/\/ If we have a Float or Double comparison, we must be careful with\n+    \/\/ handling NaN's correctly. CmpF and CmpD have a return code, as\n+    \/\/ they are based on the java bytecodes fcmpl\/dcmpl:\n+    \/\/ -1: cmp_in1 <  cmp_in2, or at least one of the two is a NaN\n+    \/\/  0: cmp_in1 == cmp_in2  (no NaN)\n+    \/\/  1: cmp_in1 >  cmp_in2  (no NaN)\n+    \/\/\n+    \/\/ The \"mask\" selects which of the [-1, 0, 1] cases lead to \"true\".\n+    \/\/\n+    \/\/ Note: ordered   (O) comparison returns \"false\" if either input is NaN.\n+    \/\/       unordered (U) comparison returns \"true\"  if either input is NaN.\n+    \/\/\n+    \/\/ The VectorMaskCmpNode does a comparison directly on in1 and in2, in the java\n+    \/\/ standard way (all comparisons are ordered, except NEQ is unordered).\n+    \/\/\n+    \/\/ In the following, \"mask\" already matches the cmp code for VectorMaskCmpNode:\n+    \/\/   BoolTest::eq:  Case 0     -> EQ_O\n+    \/\/   BoolTest::ne:  Case -1, 1 -> NEQ_U\n+    \/\/   BoolTest::ge:  Case 0, 1  -> GE_O\n+    \/\/   BoolTest::gt:  Case 1     -> GT_O\n+    \/\/\n+    \/\/ But the lt and le comparisons must be converted from unordered to ordered:\n+    \/\/   BoolTest::lt:  Case -1    -> LT_U -> VectorMaskCmp would interpret lt as LT_O\n+    \/\/   BoolTest::le:  Case -1, 0 -> LE_U -> VectorMaskCmp would interpret le as LE_O\n+    \/\/\n+    if (mask == BoolTest::lt || mask == BoolTest::le) {\n+      \/\/ Negating the mask gives us the negated result, since all non-NaN cases are\n+      \/\/ negated, and the unordered (U) comparisons are turned into ordered (O) comparisons.\n+      \/\/          VectorMaskCmp(LT_U, in1_cmp, in2_cmp)\n+      \/\/ <==> NOT VectorMaskCmp(GE_O, in1_cmp, in2_cmp)\n+      \/\/          VectorMaskCmp(LE_U, in1_cmp, in2_cmp)\n+      \/\/ <==> NOT VectorMaskCmp(GT_O, in1_cmp, in2_cmp)\n+      \/\/\n+      \/\/ When a VectorBlend uses the negated mask, it can simply swap its blend-inputs:\n+      \/\/      VectorBlend(    VectorMaskCmp(LT_U, in1_cmp, in2_cmp), in1_blend, in2_blend)\n+      \/\/ <==> VectorBlend(NOT VectorMaskCmp(GE_O, in1_cmp, in2_cmp), in1_blend, in2_blend)\n+      \/\/ <==> VectorBlend(    VectorMaskCmp(GE_O, in1_cmp, in2_cmp), in2_blend, in1_blend)\n+      \/\/      VectorBlend(    VectorMaskCmp(LE_U, in1_cmp, in2_cmp), in1_blend, in2_blend)\n+      \/\/ <==> VectorBlend(NOT VectorMaskCmp(GT_O, in1_cmp, in2_cmp), in1_blend, in2_blend)\n+      \/\/ <==> VectorBlend(    VectorMaskCmp(GT_O, in1_cmp, in2_cmp), in2_blend, in1_blend)\n+      mask = bol->_test.negate();\n+      is_negated = true;\n@@ -1662,1 +1703,2 @@\n-  return true;\n+\n+  return VTransformBoolTest(mask, is_negated);\n@@ -1699,3 +1741,1 @@\n-    if (cnt_pk != nullptr)\n-      return false;\n-    if (!same_inputs(p, 2))\n+    if (cnt_pk != nullptr || _packset.same_inputs_at_index_or_null(p, 2) == nullptr) {\n@@ -1703,0 +1743,1 @@\n+    }\n@@ -2045,1 +2086,3 @@\n-\/\/ The C2 graph (specifically the memory graph), needs to be re-ordered.\n+\/\/ We want to replace the packed scalars from the PackSet and replace them\n+\/\/ with vector operations. This requires scheduling and re-ordering the memory\n+\/\/ graph. We take these steps:\n@@ -2053,4 +2096,5 @@\n-\/\/ (4) Use the memops_schedule to re-order the memops in all slices.\n-void SuperWord::schedule() {\n-  if (_packset.length() == 0) {\n-    return; \/\/ empty packset\n+\/\/ (4) Apply the vectorization, including re-ordering the memops and replacing\n+\/\/     packed scalars with vector operations.\n+bool SuperWord::schedule_and_apply() {\n+  if (_packset.is_empty()) {\n+    return false;\n@@ -2082,1 +2126,1 @@\n-    return;\n+    return false;\n@@ -2085,6 +2129,3 @@\n-#ifndef PRODUCT\n-  if (is_trace_superword_info()) {\n-    tty->print_cr(\"SuperWord::schedule: memops_schedule:\");\n-    memops_schedule.dump();\n-  }\n-#endif\n+  \/\/ (4) Apply the vectorization, including re-ordering the memops.\n+  return apply(memops_schedule);\n+}\n@@ -2092,0 +2133,2 @@\n+bool SuperWord::apply(Node_List& memops_schedule) {\n+  Compile* C = phase()->C;\n@@ -2093,1 +2136,1 @@\n-  phase()->C->print_method(PHASE_SUPERWORD1_BEFORE_SCHEDULE, 4, cl);\n+  C->print_method(PHASE_AUTO_VECTORIZATION1_BEFORE_APPLY, 4, cl);\n@@ -2095,3 +2138,8 @@\n-  \/\/ (4) Use the memops_schedule to re-order the memops in all slices.\n-  schedule_reorder_memops(memops_schedule);\n-}\n+  apply_memops_reordering_with_schedule(memops_schedule);\n+  C->print_method(PHASE_AUTO_VECTORIZATION2_AFTER_REORDER, 4, cl);\n+\n+  adjust_pre_loop_limit_to_align_main_loop_vectors();\n+  C->print_method(PHASE_AUTO_VECTORIZATION3_AFTER_ADJUST_LIMIT, 4, cl);\n+\n+  bool is_success = apply_vectorization();\n+  C->print_method(PHASE_AUTO_VECTORIZATION4_AFTER_APPLY, 4, cl);\n@@ -2099,0 +2147,2 @@\n+  return is_success;\n+}\n@@ -2102,1 +2152,8 @@\n-void SuperWord::schedule_reorder_memops(Node_List &memops_schedule) {\n+void SuperWord::apply_memops_reordering_with_schedule(Node_List& memops_schedule) {\n+#ifndef PRODUCT\n+  if (is_trace_superword_info()) {\n+    tty->print_cr(\"\\nSuperWord::apply_memops_reordering_with_schedule:\");\n+    memops_schedule.dump();\n+  }\n+#endif\n+\n@@ -2183,1 +2240,0 @@\n-\/\/------------------------------output---------------------------\n@@ -2190,1 +2246,1 @@\n-bool SuperWord::output() {\n+bool SuperWord::apply_vectorization() {\n@@ -2194,3 +2250,1 @@\n-  if (_packset.is_empty()) {\n-    return false;\n-  }\n+  assert(!_packset.is_empty(), \"vectorization requires non-empty packset\");\n@@ -2200,1 +2254,1 @@\n-    tty->print(\"SuperWord::output    \");\n+    tty->print(\"SuperWord::apply_vectorization \");\n@@ -2204,5 +2258,0 @@\n-  phase()->C->print_method(PHASE_SUPERWORD2_BEFORE_OUTPUT, 4, cl);\n-\n-  adjust_pre_loop_limit_to_align_main_loop_vectors();\n-\n-  DEBUG_ONLY(verify_no_extract());\n@@ -2217,1 +2266,1 @@\n-      \/\/ After schedule_reorder_memops, we know that the memops have the same order in the pack\n+      \/\/ After apply_memops_reordering_with_schedule, we know that the memops have the same order in the pack\n@@ -2297,18 +2346,2 @@\n-        BoolTest::mask bol_test = bol->_test._test;\n-        assert(bol_test == BoolTest::eq ||\n-               bol_test == BoolTest::ne ||\n-               bol_test == BoolTest::ge ||\n-               bol_test == BoolTest::gt ||\n-               bol_test == BoolTest::lt ||\n-               bol_test == BoolTest::le,\n-               \"CMove bool should be one of: eq,ne,ge,ge,lt,le\");\n-        Node_List* p_bol = get_pack(bol);\n-        assert(p_bol != nullptr, \"CMove must have matching Bool pack\");\n-\n-#ifdef ASSERT\n-        for (uint j = 0; j < p_bol->size(); j++) {\n-          Node* m = p_bol->at(j);\n-          assert(m->as_Bool()->_test._test == bol_test,\n-                 \"all bool nodes must have same test\");\n-        }\n-#endif\n+        Node_List* bool_pack = get_pack(bol);\n+        assert(bool_pack != nullptr, \"CMove must have matching Bool pack\");\n@@ -2318,2 +2351,2 @@\n-        Node_List* p_cmp = get_pack(cmp);\n-        assert(p_cmp != nullptr, \"Bool must have matching Cmp pack\");\n+        Node_List* cmp_pack = get_pack(cmp);\n+        assert(cmp_pack != nullptr, \"Bool must have matching Cmp pack\");\n@@ -2321,2 +2354,2 @@\n-        Node* cmp_in1 = vector_opd(p_cmp, 1);\n-        Node* cmp_in2 = vector_opd(p_cmp, 2);\n+        Node* cmp_in1 = vector_opd(cmp_pack, 1);\n+        Node* cmp_in2 = vector_opd(cmp_pack, 2);\n@@ -2327,36 +2360,5 @@\n-        if (cmp->Opcode() == Op_CmpF || cmp->Opcode() == Op_CmpD) {\n-          \/\/ If we have a Float or Double comparison, we must be careful with\n-          \/\/ handling NaN's correctly. CmpF and CmpD have a return code, as\n-          \/\/ they are based on the java bytecodes fcmpl\/dcmpl:\n-          \/\/ -1: cmp_in1 <  cmp_in2, or at least one of the two is a NaN\n-          \/\/  0: cmp_in1 == cmp_in2  (no NaN)\n-          \/\/  1: cmp_in1 >  cmp_in2  (no NaN)\n-          \/\/\n-          \/\/ The \"bol_test\" selects which of the [-1, 0, 1] cases lead to \"true\".\n-          \/\/\n-          \/\/ Note: ordered   (O) comparison returns \"false\" if either input is NaN.\n-          \/\/       unordered (U) comparison returns \"true\"  if either input is NaN.\n-          \/\/\n-          \/\/ The VectorMaskCmpNode does a comparison directly on in1 and in2, in the java\n-          \/\/ standard way (all comparisons are ordered, except NEQ is unordered).\n-          \/\/\n-          \/\/ In the following, \"bol_test\" already matches the cmp code for VectorMaskCmpNode:\n-          \/\/   BoolTest::eq:  Case 0     -> EQ_O\n-          \/\/   BoolTest::ne:  Case -1, 1 -> NEQ_U\n-          \/\/   BoolTest::ge:  Case 0, 1  -> GE_O\n-          \/\/   BoolTest::gt:  Case 1     -> GT_O\n-          \/\/\n-          \/\/ But the lt and le comparisons must be converted from unordered to ordered:\n-          \/\/   BoolTest::lt:  Case -1    -> LT_U -> VectorMaskCmp would interpret lt as LT_O\n-          \/\/   BoolTest::le:  Case -1, 0 -> LE_U -> VectorMaskCmp would interpret le as LE_O\n-          \/\/\n-          if (bol_test == BoolTest::lt || bol_test == BoolTest::le) {\n-            \/\/ Negating the bol_test and swapping the blend-inputs leaves all non-NaN cases equal,\n-            \/\/ but converts the unordered (U) to an ordered (O) comparison.\n-            \/\/      VectorBlend(VectorMaskCmp(LT_U, in1_cmp, in2_cmp), in1_blend, in2_blend)\n-            \/\/ <==> VectorBlend(VectorMaskCmp(GE_O, in1_cmp, in2_cmp), in2_blend, in1_blend)\n-            \/\/      VectorBlend(VectorMaskCmp(LE_U, in1_cmp, in2_cmp), in1_blend, in2_blend)\n-            \/\/ <==> VectorBlend(VectorMaskCmp(GT_O, in1_cmp, in2_cmp), in2_blend, in1_blend)\n-            bol_test = bol->_test.negate();\n-            swap(blend_in1, blend_in2);\n-          }\n+        VTransformBoolTest bool_test = _packset.get_bool_test(bool_pack);\n+        BoolTest::mask test_mask = bool_test._mask;\n+        if (bool_test._is_negated) {\n+           \/\/ We can cancel out the negation by swapping the blend inputs.\n+           swap(blend_in1, blend_in2);\n@@ -2366,1 +2368,1 @@\n-        ConINode* bol_test_node  = igvn().intcon((int)bol_test);\n+        ConINode* test_mask_node  = igvn().intcon((int)test_mask);\n@@ -2369,1 +2371,1 @@\n-        VectorNode* mask = new VectorMaskCmpNode(bol_test, cmp_in1, cmp_in2, bol_test_node, vt);\n+        VectorNode* mask = new VectorMaskCmpNode(test_mask, cmp_in1, cmp_in2, test_mask_node, vt);\n@@ -2411,5 +2413,1 @@\n-          \/\/ Vector unsigned right shift for signed subword types behaves differently\n-          \/\/ from Java Spec. But when the shift amount is a constant not greater than\n-          \/\/ the number of sign extended bits, the unsigned right shift can be\n-          \/\/ vectorized to a signed right shift.\n-          if (VectorNode::can_transform_shift_op(n, velt_basic_type(n))) {\n+          if (VectorNode::can_use_RShiftI_instead_of_URShiftI(n, velt_basic_type(n))) {\n@@ -2421,10 +2419,1 @@\n-      } else if (opc == Op_SqrtF || opc == Op_SqrtD ||\n-                 opc == Op_AbsF || opc == Op_AbsD ||\n-                 opc == Op_AbsI || opc == Op_AbsL ||\n-                 opc == Op_NegF || opc == Op_NegD ||\n-                 opc == Op_RoundF || opc == Op_RoundD ||\n-                 opc == Op_ReverseBytesI || opc == Op_ReverseBytesL ||\n-                 opc == Op_ReverseBytesUS || opc == Op_ReverseBytesS ||\n-                 opc == Op_ReverseI || opc == Op_ReverseL ||\n-                 opc == Op_PopCountI || opc == Op_CountLeadingZerosI ||\n-                 opc == Op_CountTrailingZerosI) {\n+      } else if (VectorNode::is_scalar_unary_op_with_equal_input_and_output_types(opc)) {\n@@ -2435,6 +2424,1 @@\n-      } else if (requires_long_to_int_conversion(opc)) {\n-        \/\/ Java API for Long.bitCount\/numberOfLeadingZeros\/numberOfTrailingZeros\n-        \/\/ returns int type, but Vector API for them returns long type. To unify\n-        \/\/ the implementation in backend, superword splits the vector implementation\n-        \/\/ for Java API into an execution node with long type plus another node\n-        \/\/ converting long to int.\n+      } else if (VectorNode::is_scalar_op_that_returns_int_but_vector_op_returns_long(opc)) {\n@@ -2445,0 +2429,1 @@\n+        \/\/ Requires extra vector long -> int conversion.\n@@ -2528,2 +2513,0 @@\n-  phase()->C->print_method(PHASE_SUPERWORD3_AFTER_OUTPUT, 4, cl);\n-\n@@ -2540,1 +2523,1 @@\n-  bool have_same_inputs = same_inputs(p, opd_idx);\n+  Node* unique_input = _packset.same_inputs_at_index_or_null(p, opd_idx);\n@@ -2546,1 +2529,1 @@\n-  if (opd == iv() && !have_same_inputs) {\n+  if (opd == iv() && unique_input == nullptr) {\n@@ -2557,1 +2540,1 @@\n-  if (have_same_inputs) {\n+  if (unique_input != nullptr) {\n@@ -2852,1 +2835,1 @@\n-  if (requires_long_to_int_conversion(use->Opcode())) {\n+  if (VectorNode::is_scalar_op_that_returns_int_but_vector_op_returns_long(use->Opcode())) {\n@@ -2999,1 +2982,1 @@\n-        requires_long_to_int_conversion(n->Opcode())) ||\n+        VectorNode::is_scalar_op_that_returns_int_but_vector_op_returns_long(n->Opcode())) ||\n@@ -3176,1 +3159,1 @@\n-    assert(vectors_should_be_aligned(), \"mem_ref only set if filtered for alignment\");\n+    assert(VLoop::vectors_should_be_aligned(), \"mem_ref only set if filtered for alignment\");\n","filename":"src\/hotspot\/share\/opto\/superword.cpp","additions":144,"deletions":161,"binary":false,"changes":305,"status":"modified"},{"patch":"@@ -365,0 +365,4 @@\n+  Node* same_inputs_at_index_or_null(const Node_List* pack, int j) const;\n+\n+  VTransformBoolTest get_bool_test(const Node_List* bool_pack) const;\n+\n@@ -548,6 +552,0 @@\n-  \/\/ should we align vector memory references on this platform?\n-  bool vectors_should_be_aligned() { return !Matcher::misaligned_vectors_ok() || AlignVector; }\n-\n-  \/\/ For pack p, are all idx operands the same?\n-  bool same_inputs(const Node_List* p, int idx) const;\n-\n@@ -603,7 +601,4 @@\n-  \/\/ Adjust the memory graph for the packed operations\n-  void schedule();\n-  \/\/ Helper function for schedule, that reorders all memops, slice by slice, according to the schedule\n-  void schedule_reorder_memops(Node_List &memops_schedule);\n-\n-  \/\/ Convert packs into vector node operations\n-  bool output();\n+  bool schedule_and_apply();\n+  bool apply(Node_List& memops_schedule);\n+  void apply_memops_reordering_with_schedule(Node_List& memops_schedule);\n+  bool apply_vectorization();\n@@ -635,2 +630,0 @@\n-  static bool requires_long_to_int_conversion(int opc);\n-\n","filename":"src\/hotspot\/share\/opto\/superword.hpp","additions":8,"deletions":15,"binary":false,"changes":23,"status":"modified"},{"patch":"@@ -132,0 +132,3 @@\n+  \/\/ Should we align vector memory references on this platform?\n+  static bool vectors_should_be_aligned() { return !Matcher::misaligned_vectors_ok() || AlignVector; }\n+\n@@ -1323,0 +1326,8 @@\n+struct VTransformBoolTest {\n+  const BoolTest::mask _mask;\n+  const bool _is_negated;\n+\n+  VTransformBoolTest(const BoolTest::mask mask, bool is_negated) :\n+    _mask(mask), _is_negated(is_negated) {}\n+};\n+\n","filename":"src\/hotspot\/share\/opto\/vectorization.hpp","additions":11,"deletions":0,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -510,1 +510,5 @@\n-bool VectorNode::can_transform_shift_op(Node* n, BasicType bt) {\n+\/\/ Vector unsigned right shift for signed subword types behaves differently\n+\/\/ from Java Spec. But when the shift amount is a constant not greater than\n+\/\/ the number of sign extended bits, the unsigned right shift can be\n+\/\/ vectorized to a signed right shift.\n+bool VectorNode::can_use_RShiftI_instead_of_URShiftI(Node* n, BasicType bt) {\n@@ -923,0 +927,44 @@\n+bool VectorNode::is_scalar_unary_op_with_equal_input_and_output_types(int opc) {\n+  switch (opc) {\n+    case Op_SqrtF:\n+    case Op_SqrtD:\n+    case Op_AbsF:\n+    case Op_AbsD:\n+    case Op_AbsI:\n+    case Op_AbsL:\n+    case Op_NegF:\n+    case Op_NegD:\n+    case Op_RoundF:\n+    case Op_RoundD:\n+    case Op_ReverseBytesI:\n+    case Op_ReverseBytesL:\n+    case Op_ReverseBytesUS:\n+    case Op_ReverseBytesS:\n+    case Op_ReverseI:\n+    case Op_ReverseL:\n+    case Op_PopCountI:\n+    case Op_CountLeadingZerosI:\n+    case Op_CountTrailingZerosI:\n+      return true;\n+    default:\n+      return false;\n+  }\n+}\n+\n+\/\/ Java API for Long.bitCount\/numberOfLeadingZeros\/numberOfTrailingZeros\n+\/\/ returns int type, but Vector API for them returns long type. To unify\n+\/\/ the implementation in backend, AutoVectorization splits the vector\n+\/\/ implementation for Java API into an execution node with long type plus\n+\/\/ another node converting long to int.\n+bool VectorNode::is_scalar_op_that_returns_int_but_vector_op_returns_long(int opc) {\n+  switch (opc) {\n+    case Op_PopCountL:\n+    case Op_CountLeadingZerosL:\n+    case Op_CountTrailingZerosL:\n+      return true;\n+    default:\n+      return false;\n+  }\n+}\n+\n+\n","filename":"src\/hotspot\/share\/opto\/vectornode.cpp","additions":49,"deletions":1,"binary":false,"changes":50,"status":"modified"},{"patch":"@@ -87,1 +87,1 @@\n-  static bool can_transform_shift_op(Node* n, BasicType bt);\n+  static bool can_use_RShiftI_instead_of_URShiftI(Node* n, BasicType bt);\n@@ -133,0 +133,3 @@\n+  static bool is_scalar_unary_op_with_equal_input_and_output_types(int opc);\n+  static bool is_scalar_op_that_returns_int_but_vector_op_returns_long(int opc);\n+\n","filename":"src\/hotspot\/share\/opto\/vectornode.hpp","additions":4,"deletions":1,"binary":false,"changes":5,"status":"modified"}]}