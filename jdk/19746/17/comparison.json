{"files":[{"patch":"@@ -203,0 +203,7 @@\n+  ifeq ($(call check-jvm-feature, g1gc), true)\n+    AD_SRC_FILES += $(call uniq, $(wildcard $(foreach d, $(AD_SRC_ROOTS), \\\n+        $d\/cpu\/$(HOTSPOT_TARGET_CPU_ARCH)\/gc\/g1\/g1_$(HOTSPOT_TARGET_CPU).ad \\\n+        $d\/cpu\/$(HOTSPOT_TARGET_CPU_ARCH)\/gc\/g1\/g1_$(HOTSPOT_TARGET_CPU_ARCH).ad \\\n+      )))\n+  endif\n+\n","filename":"make\/hotspot\/gensrc\/GensrcAdlc.gmk","additions":7,"deletions":0,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -2623,1 +2623,2 @@\n-      is_valid_sve_arith_imm_pattern(n, m)) {\n+      is_valid_sve_arith_imm_pattern(n, m) ||\n+      is_encode_and_store_pattern(n, m)) {\n@@ -6413,1 +6414,1 @@\n-  predicate(!needs_acquiring_load(n));\n+  predicate(!needs_acquiring_load(n) && n->as_Load()->barrier_data() == 0);\n@@ -6842,1 +6843,1 @@\n-  predicate(!needs_releasing_store(n));\n+  predicate(!needs_releasing_store(n) && n->as_Store()->barrier_data() == 0);\n@@ -6855,1 +6856,1 @@\n-  predicate(!needs_releasing_store(n));\n+  predicate(!needs_releasing_store(n) && n->as_Store()->barrier_data() == 0);\n@@ -7089,0 +7090,1 @@\n+  predicate(n->as_Load()->barrier_data() == 0);\n@@ -7256,0 +7258,1 @@\n+  predicate(n->as_Store()->barrier_data() == 0);\n@@ -7268,0 +7271,1 @@\n+  predicate(n->as_Store()->barrier_data() == 0);\n@@ -8064,0 +8068,1 @@\n+  predicate(n->as_LoadStore()->barrier_data() == 0);\n@@ -8178,1 +8183,1 @@\n-  predicate(needs_acquiring_load_exclusive(n));\n+  predicate(needs_acquiring_load_exclusive(n) && n->as_LoadStore()->barrier_data() == 0);\n@@ -8283,0 +8288,1 @@\n+  predicate(n->as_LoadStore()->barrier_data() == 0);\n@@ -8392,1 +8398,1 @@\n-  predicate(needs_acquiring_load_exclusive(n));\n+  predicate(needs_acquiring_load_exclusive(n) && n->as_LoadStore()->barrier_data() == 0);\n@@ -8504,0 +8510,1 @@\n+  predicate(n->as_LoadStore()->barrier_data() == 0);\n@@ -8623,1 +8630,1 @@\n-  predicate(needs_acquiring_load_exclusive(n));\n+  predicate(needs_acquiring_load_exclusive(n) && n->as_LoadStore()->barrier_data() == 0);\n@@ -8684,0 +8691,1 @@\n+  predicate(n->as_LoadStore()->barrier_data() == 0);\n@@ -8727,1 +8735,1 @@\n-  predicate(needs_acquiring_load_exclusive(n));\n+  predicate(needs_acquiring_load_exclusive(n) && n->as_LoadStore()->barrier_data() == 0);\n","filename":"src\/hotspot\/cpu\/aarch64\/aarch64.ad","additions":16,"deletions":8,"binary":false,"changes":24,"status":"modified"},{"patch":"@@ -48,0 +48,1 @@\n+       $1$6,NAcq,INDENT(predicate(needs_acquiring_load_exclusive(n) && n->as_LoadStore()->barrier_data() == 0);),\n@@ -49,0 +50,1 @@\n+       $1,N,INDENT(predicate(n->as_LoadStore()->barrier_data() == 0);),\n@@ -125,0 +127,1 @@\n+       $1$6,NAcq,INDENT(predicate(needs_acquiring_load_exclusive(n) && n->as_LoadStore()->barrier_data() == 0);),\n@@ -126,0 +129,1 @@\n+       $1,N,INDENT(predicate(n->as_LoadStore()->barrier_data() == 0);),\n","filename":"src\/hotspot\/cpu\/aarch64\/cas.m4","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -41,1 +41,4 @@\n-#endif\n+#endif \/\/ COMPILER1\n+#ifdef COMPILER2\n+#include \"gc\/g1\/c2\/g1BarrierSetC2.hpp\"\n+#endif \/\/ COMPILER2\n@@ -98,0 +101,48 @@\n+static void generate_queue_test_and_insertion(MacroAssembler* masm, ByteSize index_offset, ByteSize buffer_offset, Label& runtime,\n+                                              const Register thread, const Register value, const Register temp1, const Register temp2) {\n+  \/\/ Can we store a value in the given thread's buffer?\n+  \/\/ (The index field is typed as size_t.)\n+  __ ldr(temp1, Address(thread, in_bytes(index_offset)));   \/\/ temp1 := *(index address)\n+  __ cbz(temp1, runtime);                                   \/\/ jump to runtime if index == 0 (full buffer)\n+  \/\/ The buffer is not full, store value into it.\n+  __ sub(temp1, temp1, wordSize);                           \/\/ temp1 := next index\n+  __ str(temp1, Address(thread, in_bytes(index_offset)));   \/\/ *(index address) := next index\n+  __ ldr(temp2, Address(thread, in_bytes(buffer_offset)));  \/\/ temp2 := buffer address\n+  __ str(value, Address(temp2, temp1));                     \/\/ *(buffer address + next index) := value\n+}\n+\n+static void generate_pre_barrier_fast_path(MacroAssembler* masm,\n+                                           const Register thread,\n+                                           const Register tmp1) {\n+  Address in_progress(thread, in_bytes(G1ThreadLocalData::satb_mark_queue_active_offset()));\n+  \/\/ Is marking active?\n+  if (in_bytes(SATBMarkQueue::byte_width_of_active()) == 4) {\n+    __ ldrw(tmp1, in_progress);\n+  } else {\n+    assert(in_bytes(SATBMarkQueue::byte_width_of_active()) == 1, \"Assumption\");\n+    __ ldrb(tmp1, in_progress);\n+  }\n+}\n+\n+static void generate_pre_barrier_slow_path(MacroAssembler* masm,\n+                                           const Register obj,\n+                                           const Register pre_val,\n+                                           const Register thread,\n+                                           const Register tmp1,\n+                                           const Register tmp2,\n+                                           Label& done,\n+                                           Label& runtime) {\n+  \/\/ Do we need to load the previous value?\n+  if (obj != noreg) {\n+    __ load_heap_oop(pre_val, Address(obj, 0), noreg, noreg, AS_RAW);\n+  }\n+  \/\/ Is the previous value null?\n+  __ cbz(pre_val, done);\n+  generate_queue_test_and_insertion(masm,\n+                                    G1ThreadLocalData::satb_mark_queue_index_offset(),\n+                                    G1ThreadLocalData::satb_mark_queue_buffer_offset(),\n+                                    runtime,\n+                                    thread, pre_val, tmp1, tmp2);\n+  __ b(done);\n+}\n+\n@@ -118,11 +169,2 @@\n-  Address in_progress(thread, in_bytes(G1ThreadLocalData::satb_mark_queue_active_offset()));\n-  Address index(thread, in_bytes(G1ThreadLocalData::satb_mark_queue_index_offset()));\n-  Address buffer(thread, in_bytes(G1ThreadLocalData::satb_mark_queue_buffer_offset()));\n-\n-  \/\/ Is marking active?\n-  if (in_bytes(SATBMarkQueue::byte_width_of_active()) == 4) {\n-    __ ldrw(tmp1, in_progress);\n-  } else {\n-    assert(in_bytes(SATBMarkQueue::byte_width_of_active()) == 1, \"Assumption\");\n-    __ ldrb(tmp1, in_progress);\n-  }\n+  generate_pre_barrier_fast_path(masm, thread, tmp1);\n+  \/\/ If marking is not active (*(mark queue active address) == 0), jump to done\n@@ -130,25 +172,1 @@\n-\n-  \/\/ Do we need to load the previous value?\n-  if (obj != noreg) {\n-    __ load_heap_oop(pre_val, Address(obj, 0), noreg, noreg, AS_RAW);\n-  }\n-\n-  \/\/ Is the previous value null?\n-  __ cbz(pre_val, done);\n-\n-  \/\/ Can we store original value in the thread's buffer?\n-  \/\/ Is index == 0?\n-  \/\/ (The index field is typed as size_t.)\n-\n-  __ ldr(tmp1, index);                      \/\/ tmp := *index_adr\n-  __ cbz(tmp1, runtime);                    \/\/ tmp == 0?\n-                                        \/\/ If yes, goto runtime\n-\n-  __ sub(tmp1, tmp1, wordSize);             \/\/ tmp := tmp - wordSize\n-  __ str(tmp1, index);                      \/\/ *index_adr := tmp\n-  __ ldr(tmp2, buffer);\n-  __ add(tmp1, tmp1, tmp2);                 \/\/ tmp := tmp + *buffer_adr\n-\n-  \/\/ Record the previous value\n-  __ str(pre_val, Address(tmp1, 0));\n-  __ b(done);\n+  generate_pre_barrier_slow_path(masm, obj, pre_val, thread, tmp1, tmp2, done, runtime);\n@@ -185,0 +203,44 @@\n+static void generate_post_barrier_fast_path(MacroAssembler* masm,\n+                                            const Register store_addr,\n+                                            const Register new_val,\n+                                            const Register tmp1,\n+                                            const Register tmp2,\n+                                            Label& done,\n+                                            bool new_val_may_be_null) {\n+    \/\/ Does store cross heap regions?\n+  __ eor(tmp1, store_addr, new_val);                     \/\/ tmp1 := store address ^ new value\n+  __ lsr(tmp1, tmp1, G1HeapRegion::LogOfHRGrainBytes);   \/\/ tmp1 := ((store address ^ new value) >> LogOfHRGrainBytes)\n+  __ cbz(tmp1, done);\n+  \/\/ Crosses regions, storing null?\n+  if (new_val_may_be_null) {\n+    __ cbz(new_val, done);\n+  }\n+  \/\/ Storing region crossing non-null, is card young?\n+  __ lsr(tmp1, store_addr, CardTable::card_shift());     \/\/ tmp1 := card address relative to card table base\n+  __ load_byte_map_base(tmp2);                           \/\/ tmp2 := card table base address\n+  __ add(tmp1, tmp1, tmp2);                              \/\/ tmp1 := card address\n+  __ ldrb(tmp2, Address(tmp1));                          \/\/ tmp2 := card\n+  __ cmpw(tmp2, (int)G1CardTable::g1_young_card_val());  \/\/ tmp2 := card == young_card_val?\n+}\n+\n+static void generate_post_barrier_slow_path(MacroAssembler* masm,\n+                                            const Register thread,\n+                                            const Register tmp1,\n+                                            const Register tmp2,\n+                                            Label& done,\n+                                            Label& runtime) {\n+  __ membar(Assembler::StoreLoad);  \/\/ StoreLoad membar\n+  __ ldrb(tmp2, Address(tmp1));     \/\/ tmp2 := card\n+  __ cbzw(tmp2, done);\n+  \/\/ Storing a region crossing, non-null oop, card is clean.\n+  \/\/ Dirty card and log.\n+  STATIC_ASSERT(CardTable::dirty_card_val() == 0);\n+  __ strb(zr, Address(tmp1));       \/\/ *(card address) := dirty_card_val\n+  generate_queue_test_and_insertion(masm,\n+                                    G1ThreadLocalData::dirty_card_queue_index_offset(),\n+                                    G1ThreadLocalData::dirty_card_queue_buffer_offset(),\n+                                    runtime,\n+                                    thread, tmp1, tmp2, rscratch1);\n+  __ b(done);\n+}\n+\n@@ -197,7 +259,0 @@\n-  Address queue_index(thread, in_bytes(G1ThreadLocalData::dirty_card_queue_index_offset()));\n-  Address buffer(thread, in_bytes(G1ThreadLocalData::dirty_card_queue_buffer_offset()));\n-\n-  BarrierSet* bs = BarrierSet::barrier_set();\n-  CardTableBarrierSet* ctbs = barrier_set_cast<CardTableBarrierSet>(bs);\n-  CardTable* ct = ctbs->card_table();\n-\n@@ -207,1 +262,4 @@\n-  \/\/ Does store cross heap regions?\n+  generate_post_barrier_fast_path(masm, store_addr, new_val, tmp1, tmp2, done, true \/* new_val_may_be_null *\/);\n+  \/\/ If card is young, jump to done\n+  __ br(Assembler::EQ, done);\n+  generate_post_barrier_slow_path(masm, thread, tmp1, tmp2, done, runtime);\n@@ -209,3 +267,6 @@\n-  __ eor(tmp1, store_addr, new_val);\n-  __ lsr(tmp1, tmp1, G1HeapRegion::LogOfHRGrainBytes);\n-  __ cbz(tmp1, done);\n+  __ bind(runtime);\n+  \/\/ save the live input values\n+  RegSet saved = RegSet::of(store_addr);\n+  __ push(saved, sp);\n+  __ call_VM_leaf(CAST_FROM_FN_PTR(address, G1BarrierSetRuntime::write_ref_field_post_entry), tmp1, thread);\n+  __ pop(saved, sp);\n@@ -213,1 +274,2 @@\n-  \/\/ crosses regions, storing null?\n+  __ bind(done);\n+}\n@@ -215,1 +277,1 @@\n-  __ cbz(new_val, done);\n+#if defined(COMPILER2)\n@@ -217,1 +279,9 @@\n-  \/\/ storing region crossing non-null, is card already dirty?\n+static void generate_c2_barrier_runtime_call(MacroAssembler* masm, G1BarrierStubC2* stub, const Register arg, const address runtime_path) {\n+  SaveLiveRegisters save_registers(masm, stub);\n+  if (c_rarg0 != arg) {\n+    __ mov(c_rarg0, arg);\n+  }\n+  __ mov(c_rarg1, rthread);\n+  __ mov(rscratch1, runtime_path);\n+  __ blr(rscratch1);\n+}\n@@ -219,1 +289,10 @@\n-  const Register card_addr = tmp1;\n+void G1BarrierSetAssembler::g1_write_barrier_pre_c2(MacroAssembler* masm,\n+                                                    Register obj,\n+                                                    Register pre_val,\n+                                                    Register thread,\n+                                                    Register tmp1,\n+                                                    Register tmp2,\n+                                                    G1PreBarrierStubC2* stub) {\n+  assert(thread == rthread, \"must be\");\n+  assert_different_registers(obj, pre_val, tmp1, tmp2);\n+  assert(pre_val != noreg && tmp1 != noreg && tmp2 != noreg, \"expecting a register\");\n@@ -221,1 +300,1 @@\n-  __ lsr(card_addr, store_addr, CardTable::card_shift());\n+  stub->initialize_registers(obj, pre_val, thread, tmp1, tmp2);\n@@ -223,6 +302,3 @@\n-  \/\/ get the address of the card\n-  __ load_byte_map_base(tmp2);\n-  __ add(card_addr, card_addr, tmp2);\n-  __ ldrb(tmp2, Address(card_addr));\n-  __ cmpw(tmp2, (int)G1CardTable::g1_young_card_val());\n-  __ br(Assembler::EQ, done);\n+  generate_pre_barrier_fast_path(masm, thread, tmp1);\n+  \/\/ If marking is active (*(mark queue active address) != 0), jump to stub (slow path)\n+  __ cbnzw(tmp1, *stub->entry());\n@@ -230,1 +306,2 @@\n-  assert((int)CardTable::dirty_card_val() == 0, \"must be 0\");\n+  __ bind(*stub->continuation());\n+}\n@@ -232,1 +309,9 @@\n-  __ membar(Assembler::StoreLoad);\n+void G1BarrierSetAssembler::generate_c2_pre_barrier_stub(MacroAssembler* masm,\n+                                                         G1PreBarrierStubC2* stub) const {\n+  Assembler::InlineSkippedInstructionsCounter skip_counter(masm);\n+  Label runtime;\n+  Register obj = stub->obj();\n+  Register pre_val = stub->pre_val();\n+  Register thread = stub->thread();\n+  Register tmp1 = stub->tmp1();\n+  Register tmp2 = stub->tmp2();\n@@ -234,2 +319,2 @@\n-  __ ldrb(tmp2, Address(card_addr));\n-  __ cbzw(tmp2, done);\n+  __ bind(*stub->entry());\n+  generate_pre_barrier_slow_path(masm, obj, pre_val, thread, tmp1, tmp2, *stub->continuation(), runtime);\n@@ -237,2 +322,4 @@\n-  \/\/ storing a region crossing, non-null oop, card is clean.\n-  \/\/ dirty card and log.\n+  __ bind(runtime);\n+  generate_c2_barrier_runtime_call(masm, stub, pre_val, CAST_FROM_FN_PTR(address, G1BarrierSetRuntime::write_ref_field_pre_entry));\n+  __ b(*stub->continuation());\n+}\n@@ -240,1 +327,12 @@\n-  __ strb(zr, Address(card_addr));\n+void G1BarrierSetAssembler::g1_write_barrier_post_c2(MacroAssembler* masm,\n+                                                     Register store_addr,\n+                                                     Register new_val,\n+                                                     Register thread,\n+                                                     Register tmp1,\n+                                                     Register tmp2,\n+                                                     G1PostBarrierStubC2* stub) {\n+  assert(thread == rthread, \"must be\");\n+  assert_different_registers(store_addr, new_val, thread, tmp1, tmp2,\n+                             rscratch1);\n+  assert(store_addr != noreg && new_val != noreg && tmp1 != noreg\n+         && tmp2 != noreg, \"expecting a register\");\n@@ -242,4 +340,1 @@\n-  __ ldr(rscratch1, queue_index);\n-  __ cbz(rscratch1, runtime);\n-  __ sub(rscratch1, rscratch1, wordSize);\n-  __ str(rscratch1, queue_index);\n+  stub->initialize_registers(thread, tmp1, tmp2);\n@@ -247,3 +342,4 @@\n-  __ ldr(tmp2, buffer);\n-  __ str(card_addr, Address(tmp2, rscratch1));\n-  __ b(done);\n+  bool new_val_may_be_null = (stub->barrier_data() & G1C2BarrierPostNotNull) == 0;\n+  generate_post_barrier_fast_path(masm, store_addr, new_val, tmp1, tmp2, *stub->continuation(), new_val_may_be_null);\n+  \/\/ If card is not young, jump to stub (slow path)\n+  __ br(Assembler::NE, *stub->entry());\n@@ -251,6 +347,2 @@\n-  __ bind(runtime);\n-  \/\/ save the live input values\n-  RegSet saved = RegSet::of(store_addr);\n-  __ push(saved, sp);\n-  __ call_VM_leaf(CAST_FROM_FN_PTR(address, G1BarrierSetRuntime::write_ref_field_post_entry), card_addr, thread);\n-  __ pop(saved, sp);\n+  __ bind(*stub->continuation());\n+}\n@@ -258,1 +350,14 @@\n-  __ bind(done);\n+void G1BarrierSetAssembler::generate_c2_post_barrier_stub(MacroAssembler* masm,\n+                                                          G1PostBarrierStubC2* stub) const {\n+  Assembler::InlineSkippedInstructionsCounter skip_counter(masm);\n+  Label runtime;\n+  Register thread = stub->thread();\n+  Register tmp1 = stub->tmp1(); \/\/ tmp1 holds the card address.\n+  Register tmp2 = stub->tmp2();\n+\n+  __ bind(*stub->entry());\n+  generate_post_barrier_slow_path(masm, thread, tmp1, tmp2, *stub->continuation(), runtime);\n+\n+  __ bind(runtime);\n+  generate_c2_barrier_runtime_call(masm, stub, tmp1, CAST_FROM_FN_PTR(address, G1BarrierSetRuntime::write_ref_field_post_entry));\n+  __ b(*stub->continuation());\n@@ -261,0 +366,2 @@\n+#endif \/\/ COMPILER2\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/gc\/g1\/g1BarrierSetAssembler_aarch64.cpp","additions":187,"deletions":80,"binary":false,"changes":267,"status":"modified"},{"patch":"@@ -36,0 +36,2 @@\n+class G1PreBarrierStubC2;\n+class G1PostBarrierStubC2;\n@@ -72,0 +74,21 @@\n+#ifdef COMPILER2\n+  void g1_write_barrier_pre_c2(MacroAssembler* masm,\n+                               Register obj,\n+                               Register pre_val,\n+                               Register thread,\n+                               Register tmp1,\n+                               Register tmp2,\n+                               G1PreBarrierStubC2* c2_stub);\n+  void generate_c2_pre_barrier_stub(MacroAssembler* masm,\n+                                    G1PreBarrierStubC2* stub) const;\n+  void g1_write_barrier_post_c2(MacroAssembler* masm,\n+                                Register store_addr,\n+                                Register new_val,\n+                                Register thread,\n+                                Register tmp1,\n+                                Register tmp2,\n+                                G1PostBarrierStubC2* c2_stub);\n+  void generate_c2_post_barrier_stub(MacroAssembler* masm,\n+                                     G1PostBarrierStubC2* stub) const;\n+#endif\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/gc\/g1\/g1BarrierSetAssembler_aarch64.hpp","additions":23,"deletions":0,"binary":false,"changes":23,"status":"modified"},{"patch":"@@ -0,0 +1,667 @@\n+\/\/\n+\/\/ Copyright (c) 2024, Oracle and\/or its affiliates. All rights reserved.\n+\/\/ DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+\/\/\n+\/\/ This code is free software; you can redistribute it and\/or modify it\n+\/\/ under the terms of the GNU General Public License version 2 only, as\n+\/\/ published by the Free Software Foundation.\n+\/\/\n+\/\/ This code is distributed in the hope that it will be useful, but WITHOUT\n+\/\/ ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+\/\/ FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+\/\/ version 2 for more details (a copy is included in the LICENSE file that\n+\/\/ accompanied this code).\n+\/\/\n+\/\/ You should have received a copy of the GNU General Public License version\n+\/\/ 2 along with this work; if not, write to the Free Software Foundation,\n+\/\/ Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+\/\/\n+\/\/ Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+\/\/ or visit www.oracle.com if you need additional information or have any\n+\/\/ questions.\n+\/\/\n+\n+source_hpp %{\n+\n+#include \"gc\/g1\/c2\/g1BarrierSetC2.hpp\"\n+#include \"gc\/shared\/gc_globals.hpp\"\n+\n+%}\n+\n+source %{\n+\n+#include \"gc\/g1\/g1BarrierSetAssembler_aarch64.hpp\"\n+#include \"gc\/g1\/g1BarrierSetRuntime.hpp\"\n+\n+static void write_barrier_pre(MacroAssembler* masm,\n+                              const MachNode* node,\n+                              Register obj,\n+                              Register pre_val,\n+                              Register tmp1,\n+                              Register tmp2,\n+                              RegSet preserve = RegSet(),\n+                              RegSet no_preserve = RegSet()) {\n+  if (!G1PreBarrierStubC2::needs_barrier(node)) {\n+    return;\n+  }\n+  Assembler::InlineSkippedInstructionsCounter skip_counter(masm);\n+  G1BarrierSetAssembler* g1_asm = static_cast<G1BarrierSetAssembler*>(BarrierSet::barrier_set()->barrier_set_assembler());\n+  G1PreBarrierStubC2* const stub = G1PreBarrierStubC2::create(node);\n+  for (RegSetIterator<Register> reg = preserve.begin(); *reg != noreg; ++reg) {\n+    stub->preserve(*reg);\n+  }\n+  for (RegSetIterator<Register> reg = no_preserve.begin(); *reg != noreg; ++reg) {\n+    stub->dont_preserve(*reg);\n+  }\n+  g1_asm->g1_write_barrier_pre_c2(masm, obj, pre_val, rthread, tmp1, tmp2, stub);\n+}\n+\n+static void write_barrier_post(MacroAssembler* masm,\n+                               const MachNode* node,\n+                               Register store_addr,\n+                               Register new_val,\n+                               Register tmp1,\n+                               Register tmp2) {\n+  if (!G1PostBarrierStubC2::needs_barrier(node)) {\n+    return;\n+  }\n+  Assembler::InlineSkippedInstructionsCounter skip_counter(masm);\n+  G1BarrierSetAssembler* g1_asm = static_cast<G1BarrierSetAssembler*>(BarrierSet::barrier_set()->barrier_set_assembler());\n+  G1PostBarrierStubC2* const stub = G1PostBarrierStubC2::create(node);\n+  g1_asm->g1_write_barrier_post_c2(masm, store_addr, new_val, rthread, tmp1, tmp2, stub);\n+}\n+\n+%}\n+\n+\/\/ BEGIN This section of the file is automatically generated. Do not edit --------------\n+\n+\/\/ This section is generated from g1_aarch64.m4\n+\n+\n+\/\/ This pattern is generated automatically from g1_aarch64.m4.\n+\/\/ DO NOT EDIT ANYTHING IN THIS SECTION OF THE FILE\n+instruct g1StoreP(indirect mem, iRegP src, iRegPNoSp tmp1, iRegPNoSp tmp2, iRegPNoSp tmp3, rFlagsReg cr)\n+%{\n+  predicate(UseG1GC && !needs_releasing_store(n) && n->as_Store()->barrier_data() != 0);\n+  match(Set mem (StoreP mem src));\n+  effect(TEMP tmp1, TEMP tmp2, TEMP tmp3, KILL cr);\n+  ins_cost(INSN_COST);\n+  format %{ \"str  $src, $mem\\t# ptr\" %}\n+  ins_encode %{\n+    write_barrier_pre(masm, this,\n+                      $mem$$Register \/* obj *\/,\n+                      $tmp1$$Register \/* pre_val *\/,\n+                      $tmp2$$Register \/* tmp1 *\/,\n+                      $tmp3$$Register \/* tmp2 *\/,\n+                      RegSet::of($mem$$Register, $src$$Register) \/* preserve *\/);\n+    __ str($src$$Register, $mem$$Register);\n+    write_barrier_post(masm, this,\n+                       $mem$$Register \/* store_addr *\/,\n+                       $src$$Register \/* new_val *\/,\n+                       $tmp2$$Register \/* tmp1 *\/,\n+                       $tmp3$$Register \/* tmp2 *\/);\n+  %}\n+  ins_pipe(istore_reg_mem);\n+%}\n+\n+\/\/ This pattern is generated automatically from g1_aarch64.m4.\n+\/\/ DO NOT EDIT ANYTHING IN THIS SECTION OF THE FILE\n+instruct g1StorePVolatile(indirect mem, iRegP src, iRegPNoSp tmp1, iRegPNoSp tmp2, iRegPNoSp tmp3, rFlagsReg cr)\n+%{\n+  predicate(UseG1GC && needs_releasing_store(n) && n->as_Store()->barrier_data() != 0);\n+  match(Set mem (StoreP mem src));\n+  effect(TEMP tmp1, TEMP tmp2, TEMP tmp3, KILL cr);\n+  ins_cost(VOLATILE_REF_COST);\n+  format %{ \"stlr  $src, $mem\\t# ptr\" %}\n+  ins_encode %{\n+    write_barrier_pre(masm, this,\n+                      $mem$$Register \/* obj *\/,\n+                      $tmp1$$Register \/* pre_val *\/,\n+                      $tmp2$$Register \/* tmp1 *\/,\n+                      $tmp3$$Register \/* tmp2 *\/,\n+                      RegSet::of($mem$$Register, $src$$Register) \/* preserve *\/);\n+    __ stlr($src$$Register, $mem$$Register);\n+    write_barrier_post(masm, this,\n+                       $mem$$Register \/* store_addr *\/,\n+                       $src$$Register \/* new_val *\/,\n+                       $tmp2$$Register \/* tmp1 *\/,\n+                       $tmp3$$Register \/* tmp2 *\/);\n+  %}\n+  ins_pipe(pipe_class_memory);\n+%}\n+\n+\/\/ This pattern is generated automatically from g1_aarch64.m4.\n+\/\/ DO NOT EDIT ANYTHING IN THIS SECTION OF THE FILE\n+instruct g1StoreN(indirect mem, iRegN src, iRegPNoSp tmp1, iRegPNoSp tmp2, iRegPNoSp tmp3, rFlagsReg cr)\n+%{\n+  predicate(UseG1GC && !needs_releasing_store(n) && n->as_Store()->barrier_data() != 0);\n+  match(Set mem (StoreN mem src));\n+  effect(TEMP tmp1, TEMP tmp2, TEMP tmp3, KILL cr);\n+  ins_cost(INSN_COST);\n+  format %{ \"strw  $src, $mem\\t# compressed ptr\" %}\n+  ins_encode %{\n+    write_barrier_pre(masm, this,\n+                      $mem$$Register \/* obj *\/,\n+                      $tmp1$$Register \/* pre_val *\/,\n+                      $tmp2$$Register \/* tmp1 *\/,\n+                      $tmp3$$Register \/* tmp2 *\/,\n+                      RegSet::of($mem$$Register, $src$$Register) \/* preserve *\/);\n+    __ strw($src$$Register, $mem$$Register);\n+    if ((barrier_data() & G1C2BarrierPost) != 0) {\n+      if ((barrier_data() & G1C2BarrierPostNotNull) == 0) {\n+        __ decode_heap_oop($tmp1$$Register, $src$$Register);\n+      } else {\n+        __ decode_heap_oop_not_null($tmp1$$Register, $src$$Register);\n+      }\n+    }\n+    write_barrier_post(masm, this,\n+                       $mem$$Register \/* store_addr *\/,\n+                       $tmp1$$Register \/* new_val *\/,\n+                       $tmp2$$Register \/* tmp1 *\/,\n+                       $tmp3$$Register \/* tmp2 *\/);\n+  %}\n+  ins_pipe(istore_reg_mem);\n+%}\n+\n+\/\/ This pattern is generated automatically from g1_aarch64.m4.\n+\/\/ DO NOT EDIT ANYTHING IN THIS SECTION OF THE FILE\n+instruct g1EncodePAndStoreN(indirect mem, iRegP src, iRegPNoSp tmp1, iRegPNoSp tmp2, iRegPNoSp tmp3, rFlagsReg cr)\n+%{\n+  predicate(UseG1GC && !needs_releasing_store(n) && n->as_Store()->barrier_data() != 0);\n+  match(Set mem (StoreN mem (EncodeP src)));\n+  effect(TEMP tmp1, TEMP tmp2, TEMP tmp3, KILL cr);\n+  ins_cost(INSN_COST);\n+  format %{ \"encode_heap_oop $tmp1, $src\\n\\t\"\n+            \"strw  $tmp1, $mem\\t# compressed ptr\" %}\n+  ins_encode %{\n+    write_barrier_pre(masm, this,\n+                      $mem$$Register \/* obj *\/,\n+                      $tmp1$$Register \/* pre_val *\/,\n+                      $tmp2$$Register \/* tmp1 *\/,\n+                      $tmp3$$Register \/* tmp2 *\/,\n+                      RegSet::of($mem$$Register, $src$$Register) \/* preserve *\/);\n+    if ((barrier_data() & G1C2BarrierPostNotNull) == 0) {\n+      __ encode_heap_oop($tmp1$$Register, $src$$Register);\n+    } else {\n+      __ encode_heap_oop_not_null($tmp1$$Register, $src$$Register);\n+    }\n+    __ strw($tmp1$$Register, $mem$$Register);\n+    write_barrier_post(masm, this,\n+                       $mem$$Register \/* store_addr *\/,\n+                       $src$$Register \/* new_val *\/,\n+                       $tmp2$$Register \/* tmp1 *\/,\n+                       $tmp3$$Register \/* tmp2 *\/);\n+  %}\n+  ins_pipe(istore_reg_mem);\n+%}\n+\n+\/\/ Very few of the total executed stores are volatile (less than 1% across\n+\/\/ multiple benchmark suites), no need to define an encode-and-store version.\n+\n+\/\/ This pattern is generated automatically from g1_aarch64.m4.\n+\/\/ DO NOT EDIT ANYTHING IN THIS SECTION OF THE FILE\n+instruct g1StoreNVolatile(indirect mem, iRegN src, iRegPNoSp tmp1, iRegPNoSp tmp2, iRegPNoSp tmp3, rFlagsReg cr)\n+%{\n+  predicate(UseG1GC && needs_releasing_store(n) && n->as_Store()->barrier_data() != 0);\n+  match(Set mem (StoreN mem src));\n+  effect(TEMP tmp1, TEMP tmp2, TEMP tmp3, KILL cr);\n+  ins_cost(VOLATILE_REF_COST);\n+  format %{ \"stlrw  $src, $mem\\t# compressed ptr\" %}\n+  ins_encode %{\n+    write_barrier_pre(masm, this,\n+                      $mem$$Register \/* obj *\/,\n+                      $tmp1$$Register \/* pre_val *\/,\n+                      $tmp2$$Register \/* tmp1 *\/,\n+                      $tmp3$$Register \/* tmp2 *\/,\n+                      RegSet::of($mem$$Register, $src$$Register) \/* preserve *\/);\n+    __ stlrw($src$$Register, $mem$$Register);\n+    if ((barrier_data() & G1C2BarrierPost) != 0) {\n+      if ((barrier_data() & G1C2BarrierPostNotNull) == 0) {\n+        __ decode_heap_oop($tmp1$$Register, $src$$Register);\n+      } else {\n+        __ decode_heap_oop_not_null($tmp1$$Register, $src$$Register);\n+      }\n+    }\n+    write_barrier_post(masm, this,\n+                       $mem$$Register \/* store_addr *\/,\n+                       $tmp1$$Register \/* new_val *\/,\n+                       $tmp2$$Register \/* tmp1 *\/,\n+                       $tmp3$$Register \/* tmp2 *\/);\n+  %}\n+  ins_pipe(pipe_class_memory);\n+%}\n+\n+\/\/ This pattern is generated automatically from g1_aarch64.m4.\n+\/\/ DO NOT EDIT ANYTHING IN THIS SECTION OF THE FILE\n+instruct g1CompareAndExchangeP(iRegPNoSp res, indirect mem, iRegP oldval, iRegP newval, iRegPNoSp tmp1, iRegPNoSp tmp2, iRegPNoSp tmp3, rFlagsReg cr)\n+%{\n+  predicate(UseG1GC && !needs_acquiring_load_exclusive(n) && n->as_LoadStore()->barrier_data() != 0);\n+  match(Set res (CompareAndExchangeP mem (Binary oldval newval)));\n+  effect(TEMP res, TEMP tmp1, TEMP tmp2, TEMP tmp3, KILL cr);\n+  ins_cost(2 * VOLATILE_REF_COST);\n+  format %{ \"cmpxchg $res = $mem, $oldval, $newval\\t# ptr\" %}\n+  ins_encode %{\n+    assert_different_registers($oldval$$Register, $mem$$Register);\n+    assert_different_registers($newval$$Register, $mem$$Register);\n+    \/\/ Pass $oldval to the pre-barrier (instead of loading from $mem), because\n+    \/\/ $oldval is the only value that can be overwritten.\n+    \/\/ The same holds for g1CompareAndSwapP and its Acq variant.\n+    write_barrier_pre(masm, this,\n+                      noreg \/* obj *\/,\n+                      $oldval$$Register \/* pre_val *\/,\n+                      $tmp2$$Register \/* tmp1 *\/,\n+                      $tmp3$$Register \/* tmp2 *\/,\n+                      RegSet::of($mem$$Register, $oldval$$Register, $newval$$Register) \/* preserve *\/,\n+                      RegSet::of($res$$Register) \/* no_preserve *\/);\n+    __ mov($tmp1$$Register, $oldval$$Register);\n+    __ mov($tmp2$$Register, $newval$$Register);\n+    __ cmpxchg($mem$$Register, $tmp1$$Register, $tmp2$$Register, Assembler::xword,\n+               false \/* acquire *\/, true \/* release *\/, false \/* weak *\/, $res$$Register);\n+    write_barrier_post(masm, this,\n+                       $mem$$Register \/* store_addr *\/,\n+                       $tmp2$$Register \/* new_val *\/,\n+                       $tmp1$$Register \/* tmp1 *\/,\n+                       $tmp3$$Register \/* tmp2 *\/);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ This pattern is generated automatically from g1_aarch64.m4.\n+\/\/ DO NOT EDIT ANYTHING IN THIS SECTION OF THE FILE\n+instruct g1CompareAndExchangePAcq(iRegPNoSp res, indirect mem, iRegP oldval, iRegP newval, iRegPNoSp tmp1, iRegPNoSp tmp2, iRegPNoSp tmp3, rFlagsReg cr)\n+%{\n+  predicate(UseG1GC && needs_acquiring_load_exclusive(n) && n->as_LoadStore()->barrier_data() != 0);\n+  match(Set res (CompareAndExchangeP mem (Binary oldval newval)));\n+  effect(TEMP res, TEMP tmp1, TEMP tmp2, TEMP tmp3, KILL cr);\n+  ins_cost(VOLATILE_REF_COST);\n+  format %{ \"cmpxchg_acq $res = $mem, $oldval, $newval\\t# ptr\" %}\n+  ins_encode %{\n+    assert_different_registers($oldval$$Register, $mem$$Register);\n+    assert_different_registers($newval$$Register, $mem$$Register);\n+    \/\/ Pass $oldval to the pre-barrier (instead of loading from $mem), because\n+    \/\/ $oldval is the only value that can be overwritten.\n+    \/\/ The same holds for g1CompareAndSwapP and its Acq variant.\n+    write_barrier_pre(masm, this,\n+                      noreg \/* obj *\/,\n+                      $oldval$$Register \/* pre_val *\/,\n+                      $tmp2$$Register \/* tmp1 *\/,\n+                      $tmp3$$Register \/* tmp2 *\/,\n+                      RegSet::of($mem$$Register, $oldval$$Register, $newval$$Register) \/* preserve *\/,\n+                      RegSet::of($res$$Register) \/* no_preserve *\/);\n+    __ mov($tmp1$$Register, $oldval$$Register);\n+    __ mov($tmp2$$Register, $newval$$Register);\n+    __ cmpxchg($mem$$Register, $tmp1$$Register, $tmp2$$Register, Assembler::xword,\n+               true \/* acquire *\/, true \/* release *\/, false \/* weak *\/, $res$$Register);\n+    write_barrier_post(masm, this,\n+                       $mem$$Register \/* store_addr *\/,\n+                       $tmp2$$Register \/* new_val *\/,\n+                       $tmp1$$Register \/* tmp1 *\/,\n+                       $tmp3$$Register \/* tmp2 *\/);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ This pattern is generated automatically from g1_aarch64.m4.\n+\/\/ DO NOT EDIT ANYTHING IN THIS SECTION OF THE FILE\n+instruct g1CompareAndExchangeN(iRegNNoSp res, indirect mem, iRegN oldval, iRegN newval, iRegPNoSp tmp1, iRegPNoSp tmp2, iRegPNoSp tmp3, rFlagsReg cr)\n+%{\n+  predicate(UseG1GC && !needs_acquiring_load_exclusive(n) && n->as_LoadStore()->barrier_data() != 0);\n+  match(Set res (CompareAndExchangeN mem (Binary oldval newval)));\n+  effect(TEMP res, TEMP tmp1, TEMP tmp2, TEMP tmp3, KILL cr);\n+  ins_cost(2 * VOLATILE_REF_COST);\n+  format %{ \"cmpxchg $res = $mem, $oldval, $newval\\t# narrow oop\" %}\n+  ins_encode %{\n+    assert_different_registers($oldval$$Register, $mem$$Register);\n+    assert_different_registers($newval$$Register, $mem$$Register);\n+    write_barrier_pre(masm, this,\n+                      $mem$$Register \/* obj *\/,\n+                      $tmp1$$Register \/* pre_val *\/,\n+                      $tmp2$$Register \/* tmp1 *\/,\n+                      $tmp3$$Register \/* tmp2 *\/,\n+                      RegSet::of($mem$$Register, $oldval$$Register, $newval$$Register) \/* preserve *\/,\n+                      RegSet::of($res$$Register) \/* no_preserve *\/);\n+    __ mov($tmp1$$Register, $oldval$$Register);\n+    __ mov($tmp2$$Register, $newval$$Register);\n+    __ cmpxchg($mem$$Register, $tmp1$$Register, $tmp2$$Register, Assembler::word,\n+               false \/* acquire *\/, true \/* release *\/, false \/* weak *\/, $res$$Register);\n+    __ decode_heap_oop($tmp2$$Register);\n+    write_barrier_post(masm, this,\n+                       $mem$$Register \/* store_addr *\/,\n+                       $tmp2$$Register \/* new_val *\/,\n+                       $tmp1$$Register \/* tmp1 *\/,\n+                       $tmp3$$Register \/* tmp2 *\/);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ This pattern is generated automatically from g1_aarch64.m4.\n+\/\/ DO NOT EDIT ANYTHING IN THIS SECTION OF THE FILE\n+instruct g1CompareAndExchangeNAcq(iRegNNoSp res, indirect mem, iRegN oldval, iRegN newval, iRegPNoSp tmp1, iRegPNoSp tmp2, iRegPNoSp tmp3, rFlagsReg cr)\n+%{\n+  predicate(UseG1GC && needs_acquiring_load_exclusive(n) && n->as_LoadStore()->barrier_data() != 0);\n+  match(Set res (CompareAndExchangeN mem (Binary oldval newval)));\n+  effect(TEMP res, TEMP tmp1, TEMP tmp2, TEMP tmp3, KILL cr);\n+  ins_cost(VOLATILE_REF_COST);\n+  format %{ \"cmpxchg_acq $res = $mem, $oldval, $newval\\t# narrow oop\" %}\n+  ins_encode %{\n+    assert_different_registers($oldval$$Register, $mem$$Register);\n+    assert_different_registers($newval$$Register, $mem$$Register);\n+    write_barrier_pre(masm, this,\n+                      $mem$$Register \/* obj *\/,\n+                      $tmp1$$Register \/* pre_val *\/,\n+                      $tmp2$$Register \/* tmp1 *\/,\n+                      $tmp3$$Register \/* tmp2 *\/,\n+                      RegSet::of($mem$$Register, $oldval$$Register, $newval$$Register) \/* preserve *\/,\n+                      RegSet::of($res$$Register) \/* no_preserve *\/);\n+    __ mov($tmp1$$Register, $oldval$$Register);\n+    __ mov($tmp2$$Register, $newval$$Register);\n+    __ cmpxchg($mem$$Register, $tmp1$$Register, $tmp2$$Register, Assembler::word,\n+               true \/* acquire *\/, true \/* release *\/, false \/* weak *\/, $res$$Register);\n+    __ decode_heap_oop($tmp2$$Register);\n+    write_barrier_post(masm, this,\n+                       $mem$$Register \/* store_addr *\/,\n+                       $tmp2$$Register \/* new_val *\/,\n+                       $tmp1$$Register \/* tmp1 *\/,\n+                       $tmp3$$Register \/* tmp2 *\/);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ This pattern is generated automatically from g1_aarch64.m4.\n+\/\/ DO NOT EDIT ANYTHING IN THIS SECTION OF THE FILE\n+instruct g1CompareAndSwapP(iRegINoSp res, indirect mem, iRegP newval, iRegPNoSp tmp1, iRegPNoSp tmp2, iRegPNoSp tmp3, iRegP oldval, rFlagsReg cr)\n+%{\n+  predicate(UseG1GC && !needs_acquiring_load_exclusive(n) && n->as_LoadStore()->barrier_data() != 0);\n+  match(Set res (CompareAndSwapP mem (Binary oldval newval)));\n+  match(Set res (WeakCompareAndSwapP mem (Binary oldval newval)));\n+  effect(TEMP res, TEMP tmp1, TEMP tmp2, TEMP tmp3, KILL cr);\n+  ins_cost(2 * VOLATILE_REF_COST);\n+  format %{ \"cmpxchg $mem, $oldval, $newval\\t# (ptr)\\n\\t\"\n+            \"cset $res, EQ\" %}\n+  ins_encode %{\n+    assert_different_registers($oldval$$Register, $mem$$Register);\n+    assert_different_registers($newval$$Register, $mem$$Register);\n+    write_barrier_pre(masm, this,\n+                      noreg \/* obj *\/,\n+                      $oldval$$Register \/* pre_val *\/,\n+                      $tmp2$$Register \/* tmp1 *\/,\n+                      $tmp3$$Register \/* tmp2 *\/,\n+                      RegSet::of($mem$$Register, $oldval$$Register, $newval$$Register) \/* preserve *\/,\n+                      RegSet::of($res$$Register) \/* no_preserve *\/);\n+    __ mov($tmp1$$Register, $oldval$$Register);\n+    __ mov($tmp2$$Register, $newval$$Register);\n+    __ cmpxchg($mem$$Register, $tmp1$$Register, $tmp2$$Register, Assembler::xword,\n+               false \/* acquire *\/, true \/* release *\/, false \/* weak *\/, noreg);\n+    __ cset($res$$Register, Assembler::EQ);\n+    write_barrier_post(masm, this,\n+                       $mem$$Register \/* store_addr *\/,\n+                       $tmp2$$Register \/* new_val *\/,\n+                       $tmp1$$Register \/* tmp1 *\/,\n+                       $tmp3$$Register \/* tmp2 *\/);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ This pattern is generated automatically from g1_aarch64.m4.\n+\/\/ DO NOT EDIT ANYTHING IN THIS SECTION OF THE FILE\n+instruct g1CompareAndSwapPAcq(iRegINoSp res, indirect mem, iRegP newval, iRegPNoSp tmp1, iRegPNoSp tmp2, iRegPNoSp tmp3, iRegP oldval, rFlagsReg cr)\n+%{\n+  predicate(UseG1GC && needs_acquiring_load_exclusive(n) && n->as_LoadStore()->barrier_data() != 0);\n+  match(Set res (CompareAndSwapP mem (Binary oldval newval)));\n+  match(Set res (WeakCompareAndSwapP mem (Binary oldval newval)));\n+  effect(TEMP res, TEMP tmp1, TEMP tmp2, TEMP tmp3, KILL cr);\n+  ins_cost(VOLATILE_REF_COST);\n+  format %{ \"cmpxchg_acq $mem, $oldval, $newval\\t# (ptr)\\n\\t\"\n+            \"cset $res, EQ\" %}\n+  ins_encode %{\n+    assert_different_registers($oldval$$Register, $mem$$Register);\n+    assert_different_registers($newval$$Register, $mem$$Register);\n+    write_barrier_pre(masm, this,\n+                      noreg \/* obj *\/,\n+                      $oldval$$Register \/* pre_val *\/,\n+                      $tmp2$$Register \/* tmp1 *\/,\n+                      $tmp3$$Register \/* tmp2 *\/,\n+                      RegSet::of($mem$$Register, $oldval$$Register, $newval$$Register) \/* preserve *\/,\n+                      RegSet::of($res$$Register) \/* no_preserve *\/);\n+    __ mov($tmp1$$Register, $oldval$$Register);\n+    __ mov($tmp2$$Register, $newval$$Register);\n+    __ cmpxchg($mem$$Register, $tmp1$$Register, $tmp2$$Register, Assembler::xword,\n+               true \/* acquire *\/, true \/* release *\/, false \/* weak *\/, noreg);\n+    __ cset($res$$Register, Assembler::EQ);\n+    write_barrier_post(masm, this,\n+                       $mem$$Register \/* store_addr *\/,\n+                       $tmp2$$Register \/* new_val *\/,\n+                       $tmp1$$Register \/* tmp1 *\/,\n+                       $tmp3$$Register \/* tmp2 *\/);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ This pattern is generated automatically from g1_aarch64.m4.\n+\/\/ DO NOT EDIT ANYTHING IN THIS SECTION OF THE FILE\n+instruct g1CompareAndSwapN(iRegINoSp res, indirect mem, iRegN newval, iRegPNoSp tmp1, iRegPNoSp tmp2, iRegPNoSp tmp3, iRegN oldval, rFlagsReg cr)\n+%{\n+  predicate(UseG1GC && !needs_acquiring_load_exclusive(n) && n->as_LoadStore()->barrier_data() != 0);\n+  match(Set res (CompareAndSwapN mem (Binary oldval newval)));\n+  match(Set res (WeakCompareAndSwapN mem (Binary oldval newval)));\n+  effect(TEMP res, TEMP tmp1, TEMP tmp2, TEMP tmp3, KILL cr);\n+  ins_cost(2 * VOLATILE_REF_COST);\n+  format %{ \"cmpxchg $mem, $oldval, $newval\\t# (narrow oop)\\n\\t\"\n+            \"cset $res, EQ\" %}\n+  ins_encode %{\n+    assert_different_registers($oldval$$Register, $mem$$Register);\n+    assert_different_registers($newval$$Register, $mem$$Register);\n+    write_barrier_pre(masm, this,\n+                      $mem$$Register \/* obj *\/,\n+                      $tmp1$$Register \/* pre_val *\/,\n+                      $tmp2$$Register \/* tmp1 *\/,\n+                      $tmp3$$Register \/* tmp2 *\/,\n+                      RegSet::of($mem$$Register, $oldval$$Register, $newval$$Register) \/* preserve *\/,\n+                      RegSet::of($res$$Register) \/* no_preserve *\/);\n+    __ mov($tmp1$$Register, $oldval$$Register);\n+    __ mov($tmp2$$Register, $newval$$Register);\n+    __ cmpxchg($mem$$Register, $tmp1$$Register, $tmp2$$Register, Assembler::word,\n+               false \/* acquire *\/, true \/* release *\/, false \/* weak *\/, noreg);\n+    __ cset($res$$Register, Assembler::EQ);\n+    __ decode_heap_oop($tmp2$$Register);\n+    write_barrier_post(masm, this,\n+                       $mem$$Register \/* store_addr *\/,\n+                       $tmp2$$Register \/* new_val *\/,\n+                       $tmp1$$Register \/* tmp1 *\/,\n+                       $tmp3$$Register \/* tmp2 *\/);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ This pattern is generated automatically from g1_aarch64.m4.\n+\/\/ DO NOT EDIT ANYTHING IN THIS SECTION OF THE FILE\n+instruct g1CompareAndSwapNAcq(iRegINoSp res, indirect mem, iRegN newval, iRegPNoSp tmp1, iRegPNoSp tmp2, iRegPNoSp tmp3, iRegN oldval, rFlagsReg cr)\n+%{\n+  predicate(UseG1GC && needs_acquiring_load_exclusive(n) && n->as_LoadStore()->barrier_data() != 0);\n+  match(Set res (CompareAndSwapN mem (Binary oldval newval)));\n+  match(Set res (WeakCompareAndSwapN mem (Binary oldval newval)));\n+  effect(TEMP res, TEMP tmp1, TEMP tmp2, TEMP tmp3, KILL cr);\n+  ins_cost(VOLATILE_REF_COST);\n+  format %{ \"cmpxchg_acq $mem, $oldval, $newval\\t# (narrow oop)\\n\\t\"\n+            \"cset $res, EQ\" %}\n+  ins_encode %{\n+    assert_different_registers($oldval$$Register, $mem$$Register);\n+    assert_different_registers($newval$$Register, $mem$$Register);\n+    write_barrier_pre(masm, this,\n+                      $mem$$Register \/* obj *\/,\n+                      $tmp1$$Register \/* pre_val *\/,\n+                      $tmp2$$Register \/* tmp1 *\/,\n+                      $tmp3$$Register \/* tmp2 *\/,\n+                      RegSet::of($mem$$Register, $oldval$$Register, $newval$$Register) \/* preserve *\/,\n+                      RegSet::of($res$$Register) \/* no_preserve *\/);\n+    __ mov($tmp1$$Register, $oldval$$Register);\n+    __ mov($tmp2$$Register, $newval$$Register);\n+    __ cmpxchg($mem$$Register, $tmp1$$Register, $tmp2$$Register, Assembler::word,\n+               true \/* acquire *\/, true \/* release *\/, false \/* weak *\/, noreg);\n+    __ cset($res$$Register, Assembler::EQ);\n+    __ decode_heap_oop($tmp2$$Register);\n+    write_barrier_post(masm, this,\n+                       $mem$$Register \/* store_addr *\/,\n+                       $tmp2$$Register \/* new_val *\/,\n+                       $tmp1$$Register \/* tmp1 *\/,\n+                       $tmp3$$Register \/* tmp2 *\/);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ This pattern is generated automatically from g1_aarch64.m4.\n+\/\/ DO NOT EDIT ANYTHING IN THIS SECTION OF THE FILE\n+instruct g1GetAndSetP(indirect mem, iRegP newval, iRegPNoSp tmp1, iRegPNoSp tmp2, iRegPNoSp preval, rFlagsReg cr)\n+%{\n+  predicate(UseG1GC && !needs_acquiring_load_exclusive(n) && n->as_LoadStore()->barrier_data() != 0);\n+  match(Set preval (GetAndSetP mem newval));\n+  effect(TEMP preval, TEMP tmp1, TEMP tmp2, KILL cr);\n+  ins_cost(2 * VOLATILE_REF_COST);\n+  format %{ \"atomic_xchg  $preval, $newval, [$mem]\" %}\n+  ins_encode %{\n+    assert_different_registers($mem$$Register, $newval$$Register);\n+    write_barrier_pre(masm, this,\n+                      $mem$$Register \/* obj *\/,\n+                      $preval$$Register \/* pre_val (as a temporary register) *\/,\n+                      $tmp1$$Register \/* tmp1 *\/,\n+                      $tmp2$$Register \/* tmp2 *\/,\n+                      RegSet::of($mem$$Register, $preval$$Register, $newval$$Register) \/* preserve *\/);\n+    __ atomic_xchg($preval$$Register, $newval$$Register, $mem$$Register);\n+    write_barrier_post(masm, this,\n+                       $mem$$Register \/* store_addr *\/,\n+                       $newval$$Register \/* new_val *\/,\n+                       $tmp1$$Register \/* tmp1 *\/,\n+                       $tmp2$$Register \/* tmp2 *\/);\n+  %}\n+  ins_pipe(pipe_serial);\n+%}\n+\n+\/\/ This pattern is generated automatically from g1_aarch64.m4.\n+\/\/ DO NOT EDIT ANYTHING IN THIS SECTION OF THE FILE\n+instruct g1GetAndSetPAcq(indirect mem, iRegP newval, iRegPNoSp tmp1, iRegPNoSp tmp2, iRegPNoSp preval, rFlagsReg cr)\n+%{\n+  predicate(UseG1GC && needs_acquiring_load_exclusive(n) && n->as_LoadStore()->barrier_data() != 0);\n+  match(Set preval (GetAndSetP mem newval));\n+  effect(TEMP preval, TEMP tmp1, TEMP tmp2, KILL cr);\n+  ins_cost(VOLATILE_REF_COST);\n+  format %{ \"atomic_xchg_acq  $preval, $newval, [$mem]\" %}\n+  ins_encode %{\n+    assert_different_registers($mem$$Register, $newval$$Register);\n+    write_barrier_pre(masm, this,\n+                      $mem$$Register \/* obj *\/,\n+                      $preval$$Register \/* pre_val (as a temporary register) *\/,\n+                      $tmp1$$Register \/* tmp1 *\/,\n+                      $tmp2$$Register \/* tmp2 *\/,\n+                      RegSet::of($mem$$Register, $preval$$Register, $newval$$Register) \/* preserve *\/);\n+    __ atomic_xchgal($preval$$Register, $newval$$Register, $mem$$Register);\n+    write_barrier_post(masm, this,\n+                       $mem$$Register \/* store_addr *\/,\n+                       $newval$$Register \/* new_val *\/,\n+                       $tmp1$$Register \/* tmp1 *\/,\n+                       $tmp2$$Register \/* tmp2 *\/);\n+  %}\n+  ins_pipe(pipe_serial);\n+%}\n+\n+\/\/ This pattern is generated automatically from g1_aarch64.m4.\n+\/\/ DO NOT EDIT ANYTHING IN THIS SECTION OF THE FILE\n+instruct g1GetAndSetN(indirect mem, iRegN newval, iRegPNoSp tmp1, iRegPNoSp tmp2, iRegPNoSp tmp3, iRegNNoSp preval, rFlagsReg cr)\n+%{\n+  predicate(UseG1GC && !needs_acquiring_load_exclusive(n) && n->as_LoadStore()->barrier_data() != 0);\n+  match(Set preval (GetAndSetN mem newval));\n+  effect(TEMP preval, TEMP tmp1, TEMP tmp2, TEMP tmp3, KILL cr);\n+  ins_cost(2 * VOLATILE_REF_COST);\n+  format %{ \"atomic_xchgw $preval, $newval, [$mem]\" %}\n+  ins_encode %{\n+    assert_different_registers($mem$$Register, $newval$$Register);\n+    write_barrier_pre(masm, this,\n+                      $mem$$Register \/* obj *\/,\n+                      $tmp1$$Register \/* pre_val *\/,\n+                      $tmp2$$Register \/* tmp1 *\/,\n+                      $tmp3$$Register \/* tmp2 *\/,\n+                      RegSet::of($mem$$Register, $preval$$Register, $newval$$Register) \/* preserve *\/);\n+    __ atomic_xchgw($preval$$Register, $newval$$Register, $mem$$Register);\n+    __ decode_heap_oop($tmp1$$Register, $newval$$Register);\n+    write_barrier_post(masm, this,\n+                       $mem$$Register \/* store_addr *\/,\n+                       $tmp1$$Register \/* new_val *\/,\n+                       $tmp2$$Register \/* tmp1 *\/,\n+                       $tmp3$$Register \/* tmp2 *\/);\n+  %}\n+  ins_pipe(pipe_serial);\n+%}\n+\n+\/\/ This pattern is generated automatically from g1_aarch64.m4.\n+\/\/ DO NOT EDIT ANYTHING IN THIS SECTION OF THE FILE\n+instruct g1GetAndSetNAcq(indirect mem, iRegN newval, iRegPNoSp tmp1, iRegPNoSp tmp2, iRegPNoSp tmp3, iRegNNoSp preval, rFlagsReg cr)\n+%{\n+  predicate(UseG1GC && needs_acquiring_load_exclusive(n) && n->as_LoadStore()->barrier_data() != 0);\n+  match(Set preval (GetAndSetN mem newval));\n+  effect(TEMP preval, TEMP tmp1, TEMP tmp2, TEMP tmp3, KILL cr);\n+  ins_cost(VOLATILE_REF_COST);\n+  format %{ \"atomic_xchgw_acq $preval, $newval, [$mem]\" %}\n+  ins_encode %{\n+    assert_different_registers($mem$$Register, $newval$$Register);\n+    write_barrier_pre(masm, this,\n+                      $mem$$Register \/* obj *\/,\n+                      $tmp1$$Register \/* pre_val *\/,\n+                      $tmp2$$Register \/* tmp1 *\/,\n+                      $tmp3$$Register \/* tmp2 *\/,\n+                      RegSet::of($mem$$Register, $preval$$Register, $newval$$Register) \/* preserve *\/);\n+    __ atomic_xchgalw($preval$$Register, $newval$$Register, $mem$$Register);\n+    __ decode_heap_oop($tmp1$$Register, $newval$$Register);\n+    write_barrier_post(masm, this,\n+                       $mem$$Register \/* store_addr *\/,\n+                       $tmp1$$Register \/* new_val *\/,\n+                       $tmp2$$Register \/* tmp1 *\/,\n+                       $tmp3$$Register \/* tmp2 *\/);\n+  %}\n+  ins_pipe(pipe_serial);\n+%}\n+\n+\/\/ This pattern is generated automatically from g1_aarch64.m4.\n+\/\/ DO NOT EDIT ANYTHING IN THIS SECTION OF THE FILE\n+instruct g1LoadP(iRegPNoSp dst, indirect mem, iRegPNoSp tmp1, iRegPNoSp tmp2, rFlagsReg cr)\n+%{\n+  \/\/ This instruction does not need an acquiring counterpart because it is only\n+  \/\/ used for reference loading (Reference::get()). The same holds for g1LoadN.\n+  predicate(UseG1GC && !needs_acquiring_load(n) && n->as_Load()->barrier_data() != 0);\n+  match(Set dst (LoadP mem));\n+  effect(TEMP dst, TEMP tmp1, TEMP tmp2, KILL cr);\n+  ins_cost(4 * INSN_COST);\n+  format %{ \"ldr  $dst, $mem\\t# ptr\" %}\n+  ins_encode %{\n+    __ ldr($dst$$Register, $mem$$Register);\n+    write_barrier_pre(masm, this,\n+                      noreg \/* obj *\/,\n+                      $dst$$Register \/* pre_val *\/,\n+                      $tmp1$$Register \/* tmp1 *\/,\n+                      $tmp2$$Register \/* tmp2 *\/);\n+  %}\n+  ins_pipe(iload_reg_mem);\n+%}\n+\n+\/\/ This pattern is generated automatically from g1_aarch64.m4.\n+\/\/ DO NOT EDIT ANYTHING IN THIS SECTION OF THE FILE\n+instruct g1LoadN(iRegNNoSp dst, indirect mem, iRegPNoSp tmp1, iRegPNoSp tmp2, iRegPNoSp tmp3, rFlagsReg cr)\n+%{\n+  predicate(UseG1GC && !needs_acquiring_load(n) && n->as_Load()->barrier_data() != 0);\n+  match(Set dst (LoadN mem));\n+  effect(TEMP dst, TEMP tmp1, TEMP tmp2, TEMP tmp3, KILL cr);\n+  ins_cost(4 * INSN_COST);\n+  format %{ \"ldrw  $dst, $mem\\t# compressed ptr\" %}\n+  ins_encode %{\n+    __ ldrw($dst$$Register, $mem$$Register);\n+    if ((barrier_data() & G1C2BarrierPre) != 0) {\n+      __ decode_heap_oop($tmp1$$Register, $dst$$Register);\n+      write_barrier_pre(masm, this,\n+                        noreg \/* obj *\/,\n+                        $tmp1$$Register \/* pre_val *\/,\n+                        $tmp2$$Register \/* tmp1 *\/,\n+                        $tmp3$$Register \/* tmp2 *\/);\n+    }\n+  %}\n+  ins_pipe(iload_reg_mem);\n+%}\n+\n+\/\/ END This section of the file is automatically generated. Do not edit --------------\n","filename":"src\/hotspot\/cpu\/aarch64\/gc\/g1\/g1_aarch64.ad","additions":667,"deletions":0,"binary":false,"changes":667,"status":"added"},{"patch":"@@ -0,0 +1,391 @@\n+dnl Copyright (c) 2024, Oracle and\/or its affiliates. All rights reserved.\n+dnl DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+dnl\n+dnl This code is free software; you can redistribute it and\/or modify it\n+dnl under the terms of the GNU General Public License version 2 only, as\n+dnl published by the Free Software Foundation.\n+dnl\n+dnl This code is distributed in the hope that it will be useful, but WITHOUT\n+dnl ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+dnl FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+dnl version 2 for more details (a copy is included in the LICENSE file that\n+dnl accompanied this code).\n+dnl\n+dnl You should have received a copy of the GNU General Public License version\n+dnl 2 along with this work; if not, write to the Free Software Foundation,\n+dnl Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+dnl\n+dnl Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+dnl or visit www.oracle.com if you need additional information or have any\n+dnl questions.\n+dnl\n+\/\/ BEGIN This section of the file is automatically generated. Do not edit --------------\n+\n+\/\/ This section is generated from g1_aarch64.m4\n+\n+define(`STOREP_INSN',\n+`\n+\/\/ This pattern is generated automatically from g1_aarch64.m4.\n+\/\/ DO NOT EDIT ANYTHING IN THIS SECTION OF THE FILE\n+instruct g1StoreP$1(indirect mem, iRegP src, iRegPNoSp tmp1, iRegPNoSp tmp2, iRegPNoSp tmp3, rFlagsReg cr)\n+%{\n+  predicate(UseG1GC && ifelse($1,Volatile,'needs_releasing_store(n)`,'!needs_releasing_store(n)`) && n->as_Store()->barrier_data() != 0);\n+  match(Set mem (StoreP mem src));\n+  effect(TEMP tmp1, TEMP tmp2, TEMP tmp3, KILL cr);\n+  ins_cost(ifelse($1,Volatile,VOLATILE_REF_COST,INSN_COST));\n+  format %{ \"$2  $src, $mem\\t# ptr\" %}\n+  ins_encode %{\n+    write_barrier_pre(masm, this,\n+                      $mem$$Register \/* obj *\/,\n+                      $tmp1$$Register \/* pre_val *\/,\n+                      $tmp2$$Register \/* tmp1 *\/,\n+                      $tmp3$$Register \/* tmp2 *\/,\n+                      RegSet::of($mem$$Register, $src$$Register) \/* preserve *\/);\n+    __ $2($src$$Register, $mem$$Register);\n+    write_barrier_post(masm, this,\n+                       $mem$$Register \/* store_addr *\/,\n+                       $src$$Register \/* new_val *\/,\n+                       $tmp2$$Register \/* tmp1 *\/,\n+                       $tmp3$$Register \/* tmp2 *\/);\n+  %}\n+  ins_pipe(ifelse($1,Volatile,pipe_class_memory,istore_reg_mem));\n+%}')dnl\n+STOREP_INSN(,str)\n+STOREP_INSN(Volatile,stlr)\n+dnl\n+define(`STOREN_INSN',\n+`\n+\/\/ This pattern is generated automatically from g1_aarch64.m4.\n+\/\/ DO NOT EDIT ANYTHING IN THIS SECTION OF THE FILE\n+instruct g1StoreN$1(indirect mem, iRegN src, iRegPNoSp tmp1, iRegPNoSp tmp2, iRegPNoSp tmp3, rFlagsReg cr)\n+%{\n+  predicate(UseG1GC && ifelse($1,Volatile,'needs_releasing_store(n)`,'!needs_releasing_store(n)`) && n->as_Store()->barrier_data() != 0);\n+  match(Set mem (StoreN mem src));\n+  effect(TEMP tmp1, TEMP tmp2, TEMP tmp3, KILL cr);\n+  ins_cost(ifelse($1,Volatile,VOLATILE_REF_COST,INSN_COST));\n+  format %{ \"$2  $src, $mem\\t# compressed ptr\" %}\n+  ins_encode %{\n+    write_barrier_pre(masm, this,\n+                      $mem$$Register \/* obj *\/,\n+                      $tmp1$$Register \/* pre_val *\/,\n+                      $tmp2$$Register \/* tmp1 *\/,\n+                      $tmp3$$Register \/* tmp2 *\/,\n+                      RegSet::of($mem$$Register, $src$$Register) \/* preserve *\/);\n+    __ $2($src$$Register, $mem$$Register);\n+    if ((barrier_data() & G1C2BarrierPost) != 0) {\n+      if ((barrier_data() & G1C2BarrierPostNotNull) == 0) {\n+        __ decode_heap_oop($tmp1$$Register, $src$$Register);\n+      } else {\n+        __ decode_heap_oop_not_null($tmp1$$Register, $src$$Register);\n+      }\n+    }\n+    write_barrier_post(masm, this,\n+                       $mem$$Register \/* store_addr *\/,\n+                       $tmp1$$Register \/* new_val *\/,\n+                       $tmp2$$Register \/* tmp1 *\/,\n+                       $tmp3$$Register \/* tmp2 *\/);\n+  %}\n+  ins_pipe(ifelse($1,Volatile,pipe_class_memory,istore_reg_mem));\n+%}')dnl\n+STOREN_INSN(,strw)\n+\n+\/\/ This pattern is generated automatically from g1_aarch64.m4.\n+\/\/ DO NOT EDIT ANYTHING IN THIS SECTION OF THE FILE\n+instruct g1EncodePAndStoreN(indirect mem, iRegP src, iRegPNoSp tmp1, iRegPNoSp tmp2, iRegPNoSp tmp3, rFlagsReg cr)\n+%{\n+  predicate(UseG1GC && !needs_releasing_store(n) && n->as_Store()->barrier_data() != 0);\n+  match(Set mem (StoreN mem (EncodeP src)));\n+  effect(TEMP tmp1, TEMP tmp2, TEMP tmp3, KILL cr);\n+  ins_cost(INSN_COST);\n+  format %{ \"encode_heap_oop $tmp1, $src\\n\\t\"\n+            \"strw  $tmp1, $mem\\t# compressed ptr\" %}\n+  ins_encode %{\n+    write_barrier_pre(masm, this,\n+                      $mem$$Register \/* obj *\/,\n+                      $tmp1$$Register \/* pre_val *\/,\n+                      $tmp2$$Register \/* tmp1 *\/,\n+                      $tmp3$$Register \/* tmp2 *\/,\n+                      RegSet::of($mem$$Register, $src$$Register) \/* preserve *\/);\n+    if ((barrier_data() & G1C2BarrierPostNotNull) == 0) {\n+      __ encode_heap_oop($tmp1$$Register, $src$$Register);\n+    } else {\n+      __ encode_heap_oop_not_null($tmp1$$Register, $src$$Register);\n+    }\n+    __ strw($tmp1$$Register, $mem$$Register);\n+    write_barrier_post(masm, this,\n+                       $mem$$Register \/* store_addr *\/,\n+                       $src$$Register \/* new_val *\/,\n+                       $tmp2$$Register \/* tmp1 *\/,\n+                       $tmp3$$Register \/* tmp2 *\/);\n+  %}\n+  ins_pipe(istore_reg_mem);\n+%}\n+\n+\/\/ Very few of the total executed stores are volatile (less than 1% across\n+\/\/ multiple benchmark suites), no need to define an encode-and-store version.\n+STOREN_INSN(Volatile,stlrw)\n+dnl\n+define(`CAEP_INSN',\n+`\n+\/\/ This pattern is generated automatically from g1_aarch64.m4.\n+\/\/ DO NOT EDIT ANYTHING IN THIS SECTION OF THE FILE\n+instruct g1CompareAndExchangeP$1(iRegPNoSp res, indirect mem, iRegP oldval, iRegP newval, iRegPNoSp tmp1, iRegPNoSp tmp2, iRegPNoSp tmp3, rFlagsReg cr)\n+%{\n+  predicate(UseG1GC && ifelse($1,Acq,'needs_acquiring_load_exclusive(n)`,'!needs_acquiring_load_exclusive(n)`) && n->as_LoadStore()->barrier_data() != 0);\n+  match(Set res (CompareAndExchangeP mem (Binary oldval newval)));\n+  effect(TEMP res, TEMP tmp1, TEMP tmp2, TEMP tmp3, KILL cr);\n+  ins_cost(ifelse($1,Acq,VOLATILE_REF_COST,2 * VOLATILE_REF_COST));\n+  format %{ \"cmpxchg$2 $res = $mem, $oldval, $newval\\t# ptr\" %}\n+  ins_encode %{\n+    assert_different_registers($oldval$$Register, $mem$$Register);\n+    assert_different_registers($newval$$Register, $mem$$Register);\n+    \/\/ Pass $oldval to the pre-barrier (instead of loading from $mem), because\n+    \/\/ $oldval is the only value that can be overwritten.\n+    \/\/ The same holds for g1CompareAndSwapP and its Acq variant.\n+    write_barrier_pre(masm, this,\n+                      noreg \/* obj *\/,\n+                      $oldval$$Register \/* pre_val *\/,\n+                      $tmp2$$Register \/* tmp1 *\/,\n+                      $tmp3$$Register \/* tmp2 *\/,\n+                      RegSet::of($mem$$Register, $oldval$$Register, $newval$$Register) \/* preserve *\/,\n+                      RegSet::of($res$$Register) \/* no_preserve *\/);\n+    __ mov($tmp1$$Register, $oldval$$Register);\n+    __ mov($tmp2$$Register, $newval$$Register);\n+    __ cmpxchg($mem$$Register, $tmp1$$Register, $tmp2$$Register, Assembler::xword,\n+               $3 \/* acquire *\/, true \/* release *\/, false \/* weak *\/, $res$$Register);\n+    write_barrier_post(masm, this,\n+                       $mem$$Register \/* store_addr *\/,\n+                       $tmp2$$Register \/* new_val *\/,\n+                       $tmp1$$Register \/* tmp1 *\/,\n+                       $tmp3$$Register \/* tmp2 *\/);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}')dnl\n+CAEP_INSN(,,false)\n+CAEP_INSN(Acq,_acq,true)\n+dnl\n+define(`CAEN_INSN',\n+`\n+\/\/ This pattern is generated automatically from g1_aarch64.m4.\n+\/\/ DO NOT EDIT ANYTHING IN THIS SECTION OF THE FILE\n+instruct g1CompareAndExchangeN$1(iRegNNoSp res, indirect mem, iRegN oldval, iRegN newval, iRegPNoSp tmp1, iRegPNoSp tmp2, iRegPNoSp tmp3, rFlagsReg cr)\n+%{\n+  predicate(UseG1GC && ifelse($1,Acq,'needs_acquiring_load_exclusive(n)`,'!needs_acquiring_load_exclusive(n)`) && n->as_LoadStore()->barrier_data() != 0);\n+  match(Set res (CompareAndExchangeN mem (Binary oldval newval)));\n+  effect(TEMP res, TEMP tmp1, TEMP tmp2, TEMP tmp3, KILL cr);\n+  ins_cost(ifelse($1,Acq,VOLATILE_REF_COST,2 * VOLATILE_REF_COST));\n+  format %{ \"cmpxchg$2 $res = $mem, $oldval, $newval\\t# narrow oop\" %}\n+  ins_encode %{\n+    assert_different_registers($oldval$$Register, $mem$$Register);\n+    assert_different_registers($newval$$Register, $mem$$Register);\n+    write_barrier_pre(masm, this,\n+                      $mem$$Register \/* obj *\/,\n+                      $tmp1$$Register \/* pre_val *\/,\n+                      $tmp2$$Register \/* tmp1 *\/,\n+                      $tmp3$$Register \/* tmp2 *\/,\n+                      RegSet::of($mem$$Register, $oldval$$Register, $newval$$Register) \/* preserve *\/,\n+                      RegSet::of($res$$Register) \/* no_preserve *\/);\n+    __ mov($tmp1$$Register, $oldval$$Register);\n+    __ mov($tmp2$$Register, $newval$$Register);\n+    __ cmpxchg($mem$$Register, $tmp1$$Register, $tmp2$$Register, Assembler::word,\n+               $3 \/* acquire *\/, true \/* release *\/, false \/* weak *\/, $res$$Register);\n+    __ decode_heap_oop($tmp2$$Register);\n+    write_barrier_post(masm, this,\n+                       $mem$$Register \/* store_addr *\/,\n+                       $tmp2$$Register \/* new_val *\/,\n+                       $tmp1$$Register \/* tmp1 *\/,\n+                       $tmp3$$Register \/* tmp2 *\/);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}')dnl\n+CAEN_INSN(,,false)\n+CAEN_INSN(Acq,_acq,true)\n+dnl\n+define(`CASP_INSN',\n+`\n+\/\/ This pattern is generated automatically from g1_aarch64.m4.\n+\/\/ DO NOT EDIT ANYTHING IN THIS SECTION OF THE FILE\n+instruct g1CompareAndSwapP$1(iRegINoSp res, indirect mem, iRegP newval, iRegPNoSp tmp1, iRegPNoSp tmp2, iRegPNoSp tmp3, iRegP oldval, rFlagsReg cr)\n+%{\n+  predicate(UseG1GC && ifelse($1,Acq,'needs_acquiring_load_exclusive(n)`,'!needs_acquiring_load_exclusive(n)`) && n->as_LoadStore()->barrier_data() != 0);\n+  match(Set res (CompareAndSwapP mem (Binary oldval newval)));\n+  match(Set res (WeakCompareAndSwapP mem (Binary oldval newval)));\n+  effect(TEMP res, TEMP tmp1, TEMP tmp2, TEMP tmp3, KILL cr);\n+  ins_cost(ifelse($1,Acq,VOLATILE_REF_COST,2 * VOLATILE_REF_COST));\n+  format %{ \"cmpxchg$2 $mem, $oldval, $newval\\t# (ptr)\\n\\t\"\n+            \"cset $res, EQ\" %}\n+  ins_encode %{\n+    assert_different_registers($oldval$$Register, $mem$$Register);\n+    assert_different_registers($newval$$Register, $mem$$Register);\n+    write_barrier_pre(masm, this,\n+                      noreg \/* obj *\/,\n+                      $oldval$$Register \/* pre_val *\/,\n+                      $tmp2$$Register \/* tmp1 *\/,\n+                      $tmp3$$Register \/* tmp2 *\/,\n+                      RegSet::of($mem$$Register, $oldval$$Register, $newval$$Register) \/* preserve *\/,\n+                      RegSet::of($res$$Register) \/* no_preserve *\/);\n+    __ mov($tmp1$$Register, $oldval$$Register);\n+    __ mov($tmp2$$Register, $newval$$Register);\n+    __ cmpxchg($mem$$Register, $tmp1$$Register, $tmp2$$Register, Assembler::xword,\n+               $3 \/* acquire *\/, true \/* release *\/, false \/* weak *\/, noreg);\n+    __ cset($res$$Register, Assembler::EQ);\n+    write_barrier_post(masm, this,\n+                       $mem$$Register \/* store_addr *\/,\n+                       $tmp2$$Register \/* new_val *\/,\n+                       $tmp1$$Register \/* tmp1 *\/,\n+                       $tmp3$$Register \/* tmp2 *\/);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}')dnl\n+CASP_INSN(,,false)\n+CASP_INSN(Acq,_acq,true)\n+dnl\n+define(`CASN_INSN',\n+`\n+\/\/ This pattern is generated automatically from g1_aarch64.m4.\n+\/\/ DO NOT EDIT ANYTHING IN THIS SECTION OF THE FILE\n+instruct g1CompareAndSwapN$1(iRegINoSp res, indirect mem, iRegN newval, iRegPNoSp tmp1, iRegPNoSp tmp2, iRegPNoSp tmp3, iRegN oldval, rFlagsReg cr)\n+%{\n+  predicate(UseG1GC && ifelse($1,Acq,'needs_acquiring_load_exclusive(n)`,'!needs_acquiring_load_exclusive(n)`) && n->as_LoadStore()->barrier_data() != 0);\n+  match(Set res (CompareAndSwapN mem (Binary oldval newval)));\n+  match(Set res (WeakCompareAndSwapN mem (Binary oldval newval)));\n+  effect(TEMP res, TEMP tmp1, TEMP tmp2, TEMP tmp3, KILL cr);\n+  ins_cost(ifelse($1,Acq,VOLATILE_REF_COST,2 * VOLATILE_REF_COST));\n+  format %{ \"cmpxchg$2 $mem, $oldval, $newval\\t# (narrow oop)\\n\\t\"\n+            \"cset $res, EQ\" %}\n+  ins_encode %{\n+    assert_different_registers($oldval$$Register, $mem$$Register);\n+    assert_different_registers($newval$$Register, $mem$$Register);\n+    write_barrier_pre(masm, this,\n+                      $mem$$Register \/* obj *\/,\n+                      $tmp1$$Register \/* pre_val *\/,\n+                      $tmp2$$Register \/* tmp1 *\/,\n+                      $tmp3$$Register \/* tmp2 *\/,\n+                      RegSet::of($mem$$Register, $oldval$$Register, $newval$$Register) \/* preserve *\/,\n+                      RegSet::of($res$$Register) \/* no_preserve *\/);\n+    __ mov($tmp1$$Register, $oldval$$Register);\n+    __ mov($tmp2$$Register, $newval$$Register);\n+    __ cmpxchg($mem$$Register, $tmp1$$Register, $tmp2$$Register, Assembler::word,\n+               $3 \/* acquire *\/, true \/* release *\/, false \/* weak *\/, noreg);\n+    __ cset($res$$Register, Assembler::EQ);\n+    __ decode_heap_oop($tmp2$$Register);\n+    write_barrier_post(masm, this,\n+                       $mem$$Register \/* store_addr *\/,\n+                       $tmp2$$Register \/* new_val *\/,\n+                       $tmp1$$Register \/* tmp1 *\/,\n+                       $tmp3$$Register \/* tmp2 *\/);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}')dnl\n+CASN_INSN(,,false)\n+CASN_INSN(Acq,_acq,true)\n+dnl\n+define(`XCHGP_INSN',\n+`\n+\/\/ This pattern is generated automatically from g1_aarch64.m4.\n+\/\/ DO NOT EDIT ANYTHING IN THIS SECTION OF THE FILE\n+instruct g1GetAndSetP$1(indirect mem, iRegP newval, iRegPNoSp tmp1, iRegPNoSp tmp2, iRegPNoSp preval, rFlagsReg cr)\n+%{\n+  predicate(UseG1GC && ifelse($1,Acq,'needs_acquiring_load_exclusive(n)`,'!needs_acquiring_load_exclusive(n)`) && n->as_LoadStore()->barrier_data() != 0);\n+  match(Set preval (GetAndSetP mem newval));\n+  effect(TEMP preval, TEMP tmp1, TEMP tmp2, KILL cr);\n+  ins_cost(ifelse($1,Acq,VOLATILE_REF_COST,2 * VOLATILE_REF_COST));\n+  format %{ \"atomic_xchg$2  $preval, $newval, [$mem]\" %}\n+  ins_encode %{\n+    assert_different_registers($mem$$Register, $newval$$Register);\n+    write_barrier_pre(masm, this,\n+                      $mem$$Register \/* obj *\/,\n+                      $preval$$Register \/* pre_val (as a temporary register) *\/,\n+                      $tmp1$$Register \/* tmp1 *\/,\n+                      $tmp2$$Register \/* tmp2 *\/,\n+                      RegSet::of($mem$$Register, $preval$$Register, $newval$$Register) \/* preserve *\/);\n+    __ $3($preval$$Register, $newval$$Register, $mem$$Register);\n+    write_barrier_post(masm, this,\n+                       $mem$$Register \/* store_addr *\/,\n+                       $newval$$Register \/* new_val *\/,\n+                       $tmp1$$Register \/* tmp1 *\/,\n+                       $tmp2$$Register \/* tmp2 *\/);\n+  %}\n+  ins_pipe(pipe_serial);\n+%}')dnl\n+XCHGP_INSN(,,atomic_xchg)\n+XCHGP_INSN(Acq,_acq,atomic_xchgal)\n+dnl\n+define(`XCHGN_INSN',\n+`\n+\/\/ This pattern is generated automatically from g1_aarch64.m4.\n+\/\/ DO NOT EDIT ANYTHING IN THIS SECTION OF THE FILE\n+instruct g1GetAndSetN$1(indirect mem, iRegN newval, iRegPNoSp tmp1, iRegPNoSp tmp2, iRegPNoSp tmp3, iRegNNoSp preval, rFlagsReg cr)\n+%{\n+  predicate(UseG1GC && ifelse($1,Acq,'needs_acquiring_load_exclusive(n)`,'!needs_acquiring_load_exclusive(n)`) && n->as_LoadStore()->barrier_data() != 0);\n+  match(Set preval (GetAndSetN mem newval));\n+  effect(TEMP preval, TEMP tmp1, TEMP tmp2, TEMP tmp3, KILL cr);\n+  ins_cost(ifelse($1,Acq,VOLATILE_REF_COST,2 * VOLATILE_REF_COST));\n+  format %{ \"$2 $preval, $newval, [$mem]\" %}\n+  ins_encode %{\n+    assert_different_registers($mem$$Register, $newval$$Register);\n+    write_barrier_pre(masm, this,\n+                      $mem$$Register \/* obj *\/,\n+                      $tmp1$$Register \/* pre_val *\/,\n+                      $tmp2$$Register \/* tmp1 *\/,\n+                      $tmp3$$Register \/* tmp2 *\/,\n+                      RegSet::of($mem$$Register, $preval$$Register, $newval$$Register) \/* preserve *\/);\n+    __ $3($preval$$Register, $newval$$Register, $mem$$Register);\n+    __ decode_heap_oop($tmp1$$Register, $newval$$Register);\n+    write_barrier_post(masm, this,\n+                       $mem$$Register \/* store_addr *\/,\n+                       $tmp1$$Register \/* new_val *\/,\n+                       $tmp2$$Register \/* tmp1 *\/,\n+                       $tmp3$$Register \/* tmp2 *\/);\n+  %}\n+  ins_pipe(pipe_serial);\n+%}')dnl\n+XCHGN_INSN(,atomic_xchgw,atomic_xchgw)\n+XCHGN_INSN(Acq,atomic_xchgw_acq,atomic_xchgalw)\n+\n+\/\/ This pattern is generated automatically from g1_aarch64.m4.\n+\/\/ DO NOT EDIT ANYTHING IN THIS SECTION OF THE FILE\n+instruct g1LoadP(iRegPNoSp dst, indirect mem, iRegPNoSp tmp1, iRegPNoSp tmp2, rFlagsReg cr)\n+%{\n+  \/\/ This instruction does not need an acquiring counterpart because it is only\n+  \/\/ used for reference loading (Reference::get()). The same holds for g1LoadN.\n+  predicate(UseG1GC && !needs_acquiring_load(n) && n->as_Load()->barrier_data() != 0);\n+  match(Set dst (LoadP mem));\n+  effect(TEMP dst, TEMP tmp1, TEMP tmp2, KILL cr);\n+  ins_cost(4 * INSN_COST);\n+  format %{ \"ldr  $dst, $mem\\t# ptr\" %}\n+  ins_encode %{\n+    __ ldr($dst$$Register, $mem$$Register);\n+    write_barrier_pre(masm, this,\n+                      noreg \/* obj *\/,\n+                      $dst$$Register \/* pre_val *\/,\n+                      $tmp1$$Register \/* tmp1 *\/,\n+                      $tmp2$$Register \/* tmp2 *\/);\n+  %}\n+  ins_pipe(iload_reg_mem);\n+%}\n+\n+\/\/ This pattern is generated automatically from g1_aarch64.m4.\n+\/\/ DO NOT EDIT ANYTHING IN THIS SECTION OF THE FILE\n+instruct g1LoadN(iRegNNoSp dst, indirect mem, iRegPNoSp tmp1, iRegPNoSp tmp2, iRegPNoSp tmp3, rFlagsReg cr)\n+%{\n+  predicate(UseG1GC && !needs_acquiring_load(n) && n->as_Load()->barrier_data() != 0);\n+  match(Set dst (LoadN mem));\n+  effect(TEMP dst, TEMP tmp1, TEMP tmp2, TEMP tmp3, KILL cr);\n+  ins_cost(4 * INSN_COST);\n+  format %{ \"ldrw  $dst, $mem\\t# compressed ptr\" %}\n+  ins_encode %{\n+    __ ldrw($dst$$Register, $mem$$Register);\n+    if ((barrier_data() & G1C2BarrierPre) != 0) {\n+      __ decode_heap_oop($tmp1$$Register, $dst$$Register);\n+      write_barrier_pre(masm, this,\n+                        noreg \/* obj *\/,\n+                        $tmp1$$Register \/* pre_val *\/,\n+                        $tmp2$$Register \/* tmp1 *\/,\n+                        $tmp3$$Register \/* tmp2 *\/);\n+    }\n+  %}\n+  ins_pipe(iload_reg_mem);\n+%}\n+\n+\/\/ END This section of the file is automatically generated. Do not edit --------------\n","filename":"src\/hotspot\/cpu\/aarch64\/gc\/g1\/g1_aarch64.m4","additions":391,"deletions":0,"binary":false,"changes":391,"status":"added"},{"patch":"@@ -44,1 +44,4 @@\n-#endif\n+#endif \/\/ COMPILER1\n+#ifdef COMPILER2\n+#include \"gc\/g1\/c2\/g1BarrierSetC2.hpp\"\n+#endif \/\/ COMPILER2\n@@ -48,0 +51,7 @@\n+static void generate_marking_inactive_test(MacroAssembler* masm) {\n+  int active_offset = in_bytes(G1ThreadLocalData::satb_mark_queue_active_offset());\n+  assert(in_bytes(SATBMarkQueue::byte_width_of_active()) == 1, \"Assumption\");\n+  __ lbz(R0, active_offset, R16_thread);  \/\/ tmp1 := *(mark queue active address)\n+  __ cmpwi(CCR0, R0, 0);\n+}\n+\n@@ -61,7 +71,1 @@\n-    if (in_bytes(SATBMarkQueue::byte_width_of_active()) == 4) {\n-      __ lwz(R0, in_bytes(G1ThreadLocalData::satb_mark_queue_active_offset()), R16_thread);\n-    } else {\n-      guarantee(in_bytes(SATBMarkQueue::byte_width_of_active()) == 1, \"Assumption\");\n-      __ lbz(R0, in_bytes(G1ThreadLocalData::satb_mark_queue_active_offset()), R16_thread);\n-    }\n-    __ cmpdi(CCR0, R0, 0);\n+    generate_marking_inactive_test(masm);\n@@ -112,0 +116,15 @@\n+static void generate_queue_insertion(MacroAssembler* masm, ByteSize index_offset, ByteSize buffer_offset, Label& runtime,\n+                                     const Register value, const Register temp) {\n+  assert_different_registers(value, temp);\n+  \/\/ Can we store a value in the given thread's buffer?\n+  \/\/ (The index field is typed as size_t.)\n+  __ ld(temp, in_bytes(index_offset), R16_thread);  \/\/ temp := *(index address)\n+  __ cmpdi(CCR0, temp, 0);                          \/\/ jump to runtime if index == 0 (full buffer)\n+  __ beq(CCR0, runtime);\n+  \/\/ The buffer is not full, store value into it.\n+  __ ld(R0, in_bytes(buffer_offset), R16_thread);   \/\/ R0 := buffer address\n+  __ addi(temp, temp, -wordSize);                   \/\/ temp := next index\n+  __ std(temp, in_bytes(index_offset), R16_thread); \/\/ *(index address) := next index\n+  __ stdx(value, temp, R0);                         \/\/ *(buffer address + next index) := value\n+}\n+\n@@ -116,0 +135,2 @@\n+  assert_different_registers(pre_val, tmp1, tmp2);\n+\n@@ -120,1 +141,7 @@\n-  if (preloaded) {\n+  \/\/ Determine necessary runtime invocation preservation measures\n+  const bool needs_frame = preservation_level >= MacroAssembler::PRESERVATION_FRAME_LR;\n+  const bool preserve_gp_registers = preservation_level >= MacroAssembler::PRESERVATION_FRAME_LR_GP_REGS;\n+  const bool preserve_fp_registers = preservation_level >= MacroAssembler::PRESERVATION_FRAME_LR_GP_FP_REGS;\n+  int nbytes_save = 0;\n+\n+  if (pre_val->is_volatile() && preloaded && !preserve_gp_registers) {\n@@ -124,5 +151,2 @@\n-    assert_different_registers(pre_val, tmp1, tmp2);\n-    if (pre_val->is_volatile()) {\n-      nv_save = !tmp1->is_volatile() ? tmp1 : tmp2;\n-      assert(!nv_save->is_volatile(), \"need one nv temp register if pre_val lives in volatile register\");\n-    }\n+    nv_save = !tmp1->is_volatile() ? tmp1 : tmp2;\n+    assert(!nv_save->is_volatile(), \"need one nv temp register if pre_val lives in volatile register\");\n@@ -133,8 +157,1 @@\n-  \/\/ Is marking active?\n-  if (in_bytes(SATBMarkQueue::byte_width_of_active()) == 4) {\n-    __ lwz(tmp1, in_bytes(G1ThreadLocalData::satb_mark_queue_active_offset()), R16_thread);\n-  } else {\n-    guarantee(in_bytes(SATBMarkQueue::byte_width_of_active()) == 1, \"Assumption\");\n-    __ lbz(tmp1, in_bytes(G1ThreadLocalData::satb_mark_queue_active_offset()), R16_thread);\n-  }\n-  __ cmpdi(CCR0, tmp1, 0);\n+  generate_marking_inactive_test(masm);\n@@ -178,12 +195,2 @@\n-  const Register Rbuffer = tmp1, Rindex = tmp2;\n-\n-  __ ld(Rindex, in_bytes(G1ThreadLocalData::satb_mark_queue_index_offset()), R16_thread);\n-  __ cmpdi(CCR0, Rindex, 0);\n-  __ beq(CCR0, runtime); \/\/ If index == 0, goto runtime.\n-  __ ld(Rbuffer, in_bytes(G1ThreadLocalData::satb_mark_queue_buffer_offset()), R16_thread);\n-\n-  __ addi(Rindex, Rindex, -wordSize); \/\/ Decrement index.\n-  __ std(Rindex, in_bytes(G1ThreadLocalData::satb_mark_queue_index_offset()), R16_thread);\n-\n-  \/\/ Record the previous value.\n-  __ stdx(pre_val, Rbuffer, Rindex);\n+  generate_queue_insertion(masm, G1ThreadLocalData::satb_mark_queue_index_offset(), G1ThreadLocalData::satb_mark_queue_buffer_offset(),\n+                           runtime, pre_val, tmp1);\n@@ -194,6 +201,0 @@\n-  \/\/ Determine necessary runtime invocation preservation measures\n-  const bool needs_frame = preservation_level >= MacroAssembler::PRESERVATION_FRAME_LR;\n-  const bool preserve_gp_registers = preservation_level >= MacroAssembler::PRESERVATION_FRAME_LR_GP_REGS;\n-  const bool preserve_fp_registers = preservation_level >= MacroAssembler::PRESERVATION_FRAME_LR_GP_FP_REGS;\n-  int nbytes_save = 0;\n-\n@@ -213,1 +214,1 @@\n-  if (pre_val->is_volatile() && preloaded && !preserve_gp_registers) {\n+  if (nv_save != noreg) {\n@@ -217,1 +218,1 @@\n-  if (pre_val->is_volatile() && preloaded && !preserve_gp_registers) {\n+  if (nv_save != noreg) {\n@@ -233,0 +234,20 @@\n+static void generate_region_crossing_test(MacroAssembler* masm, const Register store_addr, const Register new_val) {\n+  __ xorr(R0, store_addr, new_val);                  \/\/ tmp1 := store address ^ new value\n+  __ srdi_(R0, R0, G1HeapRegion::LogOfHRGrainBytes); \/\/ tmp1 := ((store address ^ new value) >> LogOfHRGrainBytes)\n+}\n+\n+static Address generate_card_young_test(MacroAssembler* masm, const Register store_addr, const Register tmp1, const Register tmp2) {\n+  CardTableBarrierSet* ct = barrier_set_cast<CardTableBarrierSet>(BarrierSet::barrier_set());\n+  __ load_const_optimized(tmp1, (address)(ct->card_table()->byte_map_base()), tmp2);\n+  __ srdi(tmp2, store_addr, CardTable::card_shift());        \/\/ tmp1 := card address relative to card table base\n+  __ lbzx(R0, tmp1, tmp2);                                   \/\/ tmp1 := card address\n+  __ cmpwi(CCR0, R0, (int)G1CardTable::g1_young_card_val());\n+  return Address(tmp1, tmp2); \/\/ return card address\n+}\n+\n+static void generate_card_dirty_test(MacroAssembler* masm, Address card_addr) {\n+  __ membar(Assembler::StoreLoad);                        \/\/ Must reload after StoreLoad membar due to concurrent refinement\n+  __ lbzx(R0, card_addr.base(), card_addr.index());       \/\/ tmp2 := card\n+  __ cmpwi(CCR0, R0, (int)G1CardTable::dirty_card_val()); \/\/ tmp2 := card == dirty_card_val?\n+}\n+\n@@ -244,3 +265,1 @@\n-  \/\/ Does store cross heap regions?\n-  __ xorr(tmp1, store_addr, new_val);\n-  __ srdi_(tmp1, tmp1, G1HeapRegion::LogOfHRGrainBytes);\n+  generate_region_crossing_test(masm, store_addr, new_val);\n@@ -260,10 +279,1 @@\n-  \/\/ Storing region crossing non-null, is card already dirty?\n-  const Register Rcard_addr = tmp1;\n-  Register Rbase = tmp2;\n-  __ load_const_optimized(Rbase, (address)(ct->card_table()->byte_map_base()), \/*temp*\/ tmp3);\n-\n-  __ srdi(Rcard_addr, store_addr, CardTable::card_shift());\n-\n-  \/\/ Get the address of the card.\n-  __ lbzx(\/*card value*\/ tmp3, Rbase, Rcard_addr);\n-  __ cmpwi(CCR0, tmp3, (int)G1CardTable::g1_young_card_val());\n+  Address card_addr = generate_card_young_test(masm, store_addr, tmp1, tmp2);\n@@ -272,3 +282,1 @@\n-  __ membar(Assembler::StoreLoad);\n-  __ lbzx(\/*card value*\/ tmp3, Rbase, Rcard_addr);  \/\/ Reload after membar.\n-  __ cmpwi(CCR0, tmp3 \/* card value *\/, (int)G1CardTable::dirty_card_val());\n+  generate_card_dirty_test(masm, card_addr);\n@@ -277,15 +285,2 @@\n-  \/\/ Storing a region crossing, non-null oop, card is clean.\n-  \/\/ Dirty card and log.\n-  __ li(tmp3, (int)G1CardTable::dirty_card_val());\n-  \/\/release(); \/\/ G1: oops are allowed to get visible after dirty marking.\n-  __ stbx(tmp3, Rbase, Rcard_addr);\n-\n-  __ add(Rcard_addr, Rbase, Rcard_addr); \/\/ This is the address which needs to get enqueued.\n-  Rbase = noreg; \/\/ end of lifetime\n-\n-  const Register Rqueue_index = tmp2,\n-                 Rqueue_buf   = tmp3;\n-  __ ld(Rqueue_index, in_bytes(G1ThreadLocalData::dirty_card_queue_index_offset()), R16_thread);\n-  __ cmpdi(CCR0, Rqueue_index, 0);\n-  __ beq(CCR0, runtime); \/\/ index == 0 then jump to runtime\n-  __ ld(Rqueue_buf, in_bytes(G1ThreadLocalData::dirty_card_queue_buffer_offset()), R16_thread);\n+  __ li(R0, (int)G1CardTable::dirty_card_val());\n+  __ stbx(R0, card_addr.base(), card_addr.index()); \/\/ *(card address) := dirty_card_val\n@@ -293,2 +288,2 @@\n-  __ addi(Rqueue_index, Rqueue_index, -wordSize); \/\/ decrement index\n-  __ std(Rqueue_index, in_bytes(G1ThreadLocalData::dirty_card_queue_index_offset()), R16_thread);\n+  Register Rcard_addr = tmp3;\n+  __ add(Rcard_addr, card_addr.base(), card_addr.index()); \/\/ This is the address which needs to get enqueued.\n@@ -296,1 +291,4 @@\n-  __ stdx(Rcard_addr, Rqueue_buf, Rqueue_index); \/\/ store card\n+  generate_queue_insertion(masm,\n+                           G1ThreadLocalData::dirty_card_queue_index_offset(),\n+                           G1ThreadLocalData::dirty_card_queue_buffer_offset(),\n+                           runtime, Rcard_addr, tmp1);\n@@ -395,0 +393,136 @@\n+#ifdef COMPILER2\n+\n+static void generate_c2_barrier_runtime_call(MacroAssembler* masm, G1BarrierStubC2* stub, const Register arg, const address runtime_path) {\n+  SaveLiveRegisters save_registers(masm, stub);\n+  __ call_VM_leaf(runtime_path, arg, R16_thread);\n+}\n+\n+void G1BarrierSetAssembler::g1_write_barrier_pre_c2(MacroAssembler* masm,\n+                                                    Register obj,\n+                                                    Register pre_val,\n+                                                    Register tmp1,\n+                                                    Register tmp2,\n+                                                    G1PreBarrierStubC2* stub) {\n+  assert_different_registers(obj, tmp1, tmp2, R0);\n+  assert_different_registers(pre_val, tmp1, R0);\n+  assert(!UseCompressedOops || tmp2 != noreg, \"tmp2 needed with CompressedOops\");\n+\n+  stub->initialize_registers(obj, pre_val, R16_thread, tmp1, tmp2);\n+\n+  generate_marking_inactive_test(masm);\n+  __ bc_far_optimized(Assembler::bcondCRbiIs0, __ bi0(CCR0, Assembler::equal), *stub->entry());\n+\n+  __ bind(*stub->continuation());\n+}\n+\n+void G1BarrierSetAssembler::generate_c2_pre_barrier_stub(MacroAssembler* masm,\n+                                                         G1PreBarrierStubC2* stub) const {\n+  Assembler::InlineSkippedInstructionsCounter skip_counter(masm);\n+  Label runtime;\n+  Register obj = stub->obj();\n+  Register pre_val = stub->pre_val();\n+  Register tmp1 = stub->tmp1();\n+\n+  __ bind(*stub->entry());\n+\n+  if (obj != noreg) {\n+    \/\/ Note: C2 currently doesn't use implicit null checks with barriers.\n+    \/\/ Otherwise, obj could be null and the following instruction would raise a SIGSEGV.\n+    if (UseCompressedOops) {\n+      __ lwz(pre_val, 0, obj);\n+    } else {\n+      __ ld(pre_val, 0, obj);\n+    }\n+  }\n+  __ cmpdi(CCR0, pre_val, 0);\n+  __ bc_far_optimized(Assembler::bcondCRbiIs1, __ bi0(CCR0, Assembler::equal), *stub->continuation());\n+\n+  Register pre_val_decoded = pre_val;\n+  if (UseCompressedOops) {\n+    pre_val_decoded = __ decode_heap_oop_not_null(stub->tmp2(), pre_val);\n+  }\n+\n+  generate_queue_insertion(masm,\n+                           G1ThreadLocalData::satb_mark_queue_index_offset(),\n+                           G1ThreadLocalData::satb_mark_queue_buffer_offset(),\n+                           runtime, pre_val_decoded, tmp1);\n+  __ b(*stub->continuation());\n+\n+  __ bind(runtime);\n+  generate_c2_barrier_runtime_call(masm, stub, pre_val_decoded, CAST_FROM_FN_PTR(address, G1BarrierSetRuntime::write_ref_field_pre_entry));\n+  __ b(*stub->continuation());\n+}\n+\n+void G1BarrierSetAssembler::g1_write_barrier_post_c2(MacroAssembler* masm,\n+                                                     Register store_addr,\n+                                                     Register new_val,\n+                                                     Register tmp1,\n+                                                     Register tmp2,\n+                                                     G1PostBarrierStubC2* stub,\n+                                                     bool decode_new_val) {\n+  assert_different_registers(store_addr, new_val, tmp1, R0);\n+  assert_different_registers(store_addr, tmp1, tmp2, R0);\n+\n+  stub->initialize_registers(R16_thread, tmp1, tmp2);\n+\n+  bool null_check_required = (stub->barrier_data() & G1C2BarrierPostNotNull) == 0;\n+  Register new_val_decoded = new_val;\n+\n+  if (decode_new_val) {\n+    assert(UseCompressedOops, \"or should not be here\");\n+    if (null_check_required && CompressedOops::base() != nullptr) {\n+      \/\/ We prefer doing the null check after the region crossing check.\n+      \/\/ Only compressed oop modes with base != null require a null check here.\n+      __ cmpwi(CCR0, new_val, 0);\n+      __ beq(CCR0, *stub->continuation());\n+      null_check_required = false;\n+    }\n+    new_val_decoded = __ decode_heap_oop_not_null(tmp2, new_val);\n+  }\n+\n+  generate_region_crossing_test(masm, store_addr, new_val_decoded);\n+  __ beq(CCR0, *stub->continuation());\n+\n+  \/\/ crosses regions, storing null?\n+  if (null_check_required) {\n+    __ cmpdi(CCR0, new_val_decoded, 0);\n+    __ beq(CCR0, *stub->continuation());\n+  }\n+\n+  Address card_addr = generate_card_young_test(masm, store_addr, tmp1, tmp2);\n+  assert(card_addr.base() == tmp1 && card_addr.index() == tmp2, \"needed by post barrier stub\");\n+  __ bc_far_optimized(Assembler::bcondCRbiIs0, __ bi0(CCR0, Assembler::equal), *stub->entry());\n+\n+  __ bind(*stub->continuation());\n+}\n+\n+void G1BarrierSetAssembler::generate_c2_post_barrier_stub(MacroAssembler* masm,\n+                                                          G1PostBarrierStubC2* stub) const {\n+  Assembler::InlineSkippedInstructionsCounter skip_counter(masm);\n+  Label runtime;\n+  Address card_addr(stub->tmp1(), stub->tmp2()); \/\/ See above.\n+\n+  __ bind(*stub->entry());\n+\n+  generate_card_dirty_test(masm, card_addr);\n+  __ bc_far_optimized(Assembler::bcondCRbiIs1, __ bi0(CCR0, Assembler::equal), *stub->continuation());\n+\n+  __ li(R0, (int)G1CardTable::dirty_card_val());\n+  __ stbx(R0, card_addr.base(), card_addr.index()); \/\/ *(card address) := dirty_card_val\n+\n+  Register Rcard_addr = stub->tmp1();\n+  __ add(Rcard_addr, card_addr.base(), card_addr.index()); \/\/ This is the address which needs to get enqueued.\n+\n+  generate_queue_insertion(masm,\n+                           G1ThreadLocalData::dirty_card_queue_index_offset(),\n+                           G1ThreadLocalData::dirty_card_queue_buffer_offset(),\n+                           runtime, Rcard_addr, stub->tmp2());\n+  __ b(*stub->continuation());\n+\n+  __ bind(runtime);\n+  generate_c2_barrier_runtime_call(masm, stub, Rcard_addr, CAST_FROM_FN_PTR(address, G1BarrierSetRuntime::write_ref_field_post_entry));\n+  __ b(*stub->continuation());\n+}\n+\n+#endif \/\/ COMPILER2\n+\n@@ -473,7 +607,1 @@\n-  if (in_bytes(SATBMarkQueue::byte_width_of_active()) == 4) {\n-    __ lwz(tmp, satb_q_active_byte_offset, R16_thread);\n-  } else {\n-    assert(in_bytes(SATBMarkQueue::byte_width_of_active()) == 1, \"Assumption\");\n-    __ lbz(tmp, satb_q_active_byte_offset, R16_thread);\n-  }\n-  __ cmpdi(CCR0, tmp, 0);\n+  generate_marking_inactive_test(sasm);\n","filename":"src\/hotspot\/cpu\/ppc\/gc\/g1\/g1BarrierSetAssembler_ppc.cpp","additions":211,"deletions":83,"binary":false,"changes":294,"status":"modified"},{"patch":"@@ -33,0 +33,4 @@\n+#ifdef COMPILER2\n+#include \"gc\/g1\/c2\/g1BarrierSetC2.hpp\"\n+#endif\n+\n@@ -37,0 +41,2 @@\n+class G1PreBarrierStubC2;\n+class G1PostBarrierStubC2;\n@@ -62,0 +68,19 @@\n+#ifdef COMPILER2\n+  void g1_write_barrier_pre_c2(MacroAssembler* masm,\n+                               Register obj,\n+                               Register pre_val,\n+                               Register tmp1,\n+                               Register tmp2,\n+                               G1PreBarrierStubC2* c2_stub);\n+  void generate_c2_pre_barrier_stub(MacroAssembler* masm,\n+                                    G1PreBarrierStubC2* stub) const;\n+  void g1_write_barrier_post_c2(MacroAssembler* masm,\n+                                Register store_addr,\n+                                Register new_val,\n+                                Register tmp1,\n+                                Register tmp2,\n+                                G1PostBarrierStubC2* c2_stub,\n+                                bool decode_new_val);\n+  void generate_c2_post_barrier_stub(MacroAssembler* masm,\n+                                     G1PostBarrierStubC2* stub) const;\n+#endif\n","filename":"src\/hotspot\/cpu\/ppc\/gc\/g1\/g1BarrierSetAssembler_ppc.hpp","additions":25,"deletions":0,"binary":false,"changes":25,"status":"modified"},{"patch":"@@ -0,0 +1,687 @@\n+\/\/\n+\/\/ Copyright (c) 2024, Oracle and\/or its affiliates. All rights reserved.\n+\/\/ Copyright (c) 2024 SAP SE. All rights reserved.\n+\/\/ DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+\/\/\n+\/\/ This code is free software; you can redistribute it and\/or modify it\n+\/\/ under the terms of the GNU General Public License version 2 only, as\n+\/\/ published by the Free Software Foundation.\n+\/\/\n+\/\/ This code is distributed in the hope that it will be useful, but WITHOUT\n+\/\/ ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+\/\/ FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+\/\/ version 2 for more details (a copy is included in the LICENSE file that\n+\/\/ accompanied this code).\n+\/\/\n+\/\/ You should have received a copy of the GNU General Public License version\n+\/\/ 2 along with this work; if not, write to the Free Software Foundation,\n+\/\/ Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+\/\/\n+\/\/ Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+\/\/ or visit www.oracle.com if you need additional information or have any\n+\/\/ questions.\n+\/\/\n+\n+source_hpp %{\n+\n+#include \"gc\/g1\/c2\/g1BarrierSetC2.hpp\"\n+#include \"gc\/shared\/gc_globals.hpp\"\n+\n+%}\n+\n+source %{\n+\n+#include \"gc\/g1\/g1BarrierSetAssembler_ppc.hpp\"\n+#include \"gc\/g1\/g1BarrierSetRuntime.hpp\"\n+\n+static void pre_write_barrier(MacroAssembler* masm,\n+                              const MachNode* node,\n+                              Register obj,\n+                              Register pre_val,\n+                              Register tmp1,\n+                              Register tmp2 = noreg, \/\/ only needed with CompressedOops when pre_val needs to be preserved\n+                              RegSet preserve = RegSet(),\n+                              RegSet no_preserve = RegSet()) {\n+  if (!G1PreBarrierStubC2::needs_barrier(node)) {\n+    return;\n+  }\n+  Assembler::InlineSkippedInstructionsCounter skip_counter(masm);\n+  G1BarrierSetAssembler* g1_asm = static_cast<G1BarrierSetAssembler*>(BarrierSet::barrier_set()->barrier_set_assembler());\n+  G1PreBarrierStubC2* const stub = G1PreBarrierStubC2::create(node);\n+  for (RegSetIterator<Register> reg = preserve.begin(); *reg != noreg; ++reg) {\n+    stub->preserve(*reg);\n+  }\n+  for (RegSetIterator<Register> reg = no_preserve.begin(); *reg != noreg; ++reg) {\n+    stub->dont_preserve(*reg);\n+  }\n+  g1_asm->g1_write_barrier_pre_c2(masm, obj, pre_val, tmp1, (tmp2 != noreg) ? tmp2 : pre_val, stub);\n+}\n+\n+static void post_write_barrier(MacroAssembler* masm,\n+                               const MachNode* node,\n+                               Register store_addr,\n+                               Register new_val,\n+                               Register tmp1,\n+                               Register tmp2,\n+                               bool decode_new_val = false) {\n+  if (!G1PostBarrierStubC2::needs_barrier(node)) {\n+    return;\n+  }\n+  Assembler::InlineSkippedInstructionsCounter skip_counter(masm);\n+  G1BarrierSetAssembler* g1_asm = static_cast<G1BarrierSetAssembler*>(BarrierSet::barrier_set()->barrier_set_assembler());\n+  G1PostBarrierStubC2* const stub = G1PostBarrierStubC2::create(node);\n+  g1_asm->g1_write_barrier_post_c2(masm, store_addr, new_val, tmp1, tmp2, stub, decode_new_val);\n+}\n+\n+%}\n+\n+instruct g1StoreP(indirect mem, iRegPsrc src, iRegPdst tmp1, iRegPdst tmp2, flagsRegCR0 cr0)\n+%{\n+  predicate(UseG1GC && n->as_Store()->barrier_data() != 0);\n+  match(Set mem (StoreP mem src));\n+  effect(TEMP tmp1, TEMP tmp2, KILL cr0);\n+  ins_cost(2 * MEMORY_REF_COST);\n+  format %{ \"std    $mem, $src\\t# ptr\" %}\n+  ins_encode %{\n+    pre_write_barrier(masm, this,\n+                      $mem$$Register,\n+                      $tmp1$$Register,\n+                      $tmp2$$Register,\n+                      noreg,\n+                      RegSet::of($mem$$Register, $src$$Register) \/* preserve *\/);\n+    __ std($src$$Register, 0, $mem$$Register);\n+    post_write_barrier(masm, this,\n+                       $mem$$Register,\n+                       $src$$Register \/* new_val *\/,\n+                       $tmp1$$Register,\n+                       $tmp2$$Register);\n+  %}\n+  ins_pipe(pipe_class_default);\n+%}\n+\n+instruct g1StoreN(indirect mem, iRegNsrc src, iRegPdst tmp1, iRegPdst tmp2, flagsRegCR0 cr0)\n+%{\n+  predicate(UseG1GC && n->as_Store()->barrier_data() != 0);\n+  match(Set mem (StoreN mem src));\n+  effect(TEMP tmp1, TEMP tmp2, KILL cr0);\n+  ins_cost(2 * MEMORY_REF_COST);\n+  format %{ \"stw    $mem, $src\\t# ptr\" %}\n+  ins_encode %{\n+    assert(!in(operand_index(2))->is_Mach() ||\n+           (in(operand_index(2))->as_Mach()->ideal_Opcode() != Op_EncodeP),\n+           \"EncodeP src nodes should be matched with their corresponding StoreN nodes\");\n+    pre_write_barrier(masm, this,\n+                      $mem$$Register,\n+                      $tmp1$$Register,\n+                      $tmp2$$Register,\n+                      noreg,\n+                      RegSet::of($mem$$Register, $src$$Register) \/* preserve *\/);\n+    __ stw($src$$Register, 0, $mem$$Register);\n+    post_write_barrier(masm, this,\n+                       $mem$$Register,\n+                       $src$$Register \/* new_val *\/,\n+                       $tmp1$$Register,\n+                       $tmp2$$Register,\n+                       true \/* decode_new_val *\/);\n+  %}\n+  ins_pipe(pipe_class_default);\n+%}\n+\n+instruct g1EncodePAndStoreN(indirect mem, iRegPsrc src, iRegPdst tmp1, iRegPdst tmp2, flagsRegCR0 cr0)\n+%{\n+  predicate(UseG1GC && n->as_Store()->barrier_data() != 0);\n+  match(Set mem (StoreN mem (EncodeP src)));\n+  effect(TEMP tmp1, TEMP tmp2, KILL cr0);\n+  ins_cost(2 * MEMORY_REF_COST);\n+  format %{ \"encode_heap_oop $src\\n\\t\"\n+            \"stw   $mem, $src\\t# ptr\" %}\n+  ins_encode %{\n+    pre_write_barrier(masm, this,\n+                      $mem$$Register,\n+                      $tmp1$$Register,\n+                      $tmp2$$Register,\n+                      noreg,\n+                      RegSet::of($mem$$Register, $src$$Register) \/* preserve *\/);\n+    Register encoded_oop = noreg;\n+    if ((barrier_data() & G1C2BarrierPostNotNull) == 0) {\n+      encoded_oop = __ encode_heap_oop($tmp2$$Register, $src$$Register);\n+    } else {\n+      encoded_oop = __ encode_heap_oop_not_null($tmp2$$Register, $src$$Register);\n+    }\n+    __ stw(encoded_oop, 0, $mem$$Register);\n+    post_write_barrier(masm, this,\n+                       $mem$$Register,\n+                       $src$$Register \/* new_val *\/,\n+                       $tmp1$$Register,\n+                       $tmp2$$Register);\n+  %}\n+  ins_pipe(pipe_class_default);\n+%}\n+\n+instruct g1CompareAndExchangeP(iRegPdst res, indirect mem, iRegPsrc oldval, iRegPsrc newval, iRegPdst tmp1, iRegPdst tmp2, flagsRegCR0 cr0)\n+%{\n+  predicate(UseG1GC && n->as_LoadStore()->barrier_data() != 0 &&\n+            (((CompareAndExchangeNode*)n)->order() != MemNode::acquire && ((CompareAndExchangeNode*)n)->order() != MemNode::seqcst));\n+  match(Set res (CompareAndExchangeP mem (Binary oldval newval)));\n+  effect(TEMP_DEF res, TEMP tmp1, TEMP tmp2, KILL cr0);\n+  format %{ \"cmpxchgd $newval, $mem\" %}\n+  ins_encode %{\n+    Label no_update;\n+    __ cmpxchgd(CCR0, $res$$Register, $oldval$$Register, $newval$$Register, $mem$$Register,\n+                MacroAssembler::MemBarNone, MacroAssembler::cmpxchgx_hint_atomic_update(),\n+                noreg, &no_update, true);\n+    \/\/ Pass oldval to SATB which is the only value which can get overwritten.\n+    \/\/ Can be done after cmpxchg because there's no safepoint here.\n+    pre_write_barrier(masm, this,\n+                      noreg,\n+                      $oldval$$Register,\n+                      $tmp1$$Register,\n+                      $tmp2$$Register,\n+                      RegSet::of($mem$$Register, $newval$$Register) \/* preserve *\/);\n+    post_write_barrier(masm, this,\n+                       $mem$$Register,\n+                       $newval$$Register,\n+                       $tmp1$$Register,\n+                       $tmp2$$Register);\n+    __ bind(no_update);\n+  %}\n+  ins_pipe(pipe_class_default);\n+%}\n+\n+instruct g1CompareAndExchangeP_acq(iRegPdst res, indirect mem, iRegPsrc oldval, iRegPsrc newval, iRegPdst tmp1, iRegPdst tmp2, flagsRegCR0 cr0)\n+%{\n+  predicate(UseG1GC && n->as_LoadStore()->barrier_data() != 0 &&\n+            (((CompareAndExchangeNode*)n)->order() == MemNode::acquire || ((CompareAndExchangeNode*)n)->order() == MemNode::seqcst));\n+  match(Set res (CompareAndExchangeP mem (Binary oldval newval)));\n+  effect(TEMP_DEF res, TEMP tmp1, TEMP tmp2, KILL cr0);\n+  format %{ \"cmpxchgd acq $newval, $mem\" %}\n+  ins_encode %{\n+    Label no_update;\n+    __ cmpxchgd(CCR0, $res$$Register, $oldval$$Register, $newval$$Register, $mem$$Register,\n+                MacroAssembler::MemBarNone, MacroAssembler::cmpxchgx_hint_atomic_update(),\n+                noreg, &no_update, true);\n+    \/\/ Pass oldval to SATB which is the only value which can get overwritten.\n+    \/\/ Can be done after cmpxchg because there's no safepoint here.\n+    pre_write_barrier(masm, this,\n+                      noreg,\n+                      $oldval$$Register,\n+                      $tmp1$$Register,\n+                      $tmp2$$Register,\n+                      RegSet::of($mem$$Register, $newval$$Register) \/* preserve *\/);\n+    post_write_barrier(masm, this,\n+                       $mem$$Register,\n+                       $newval$$Register,\n+                       $tmp1$$Register,\n+                       $tmp2$$Register);\n+    __ bind(no_update);\n+    if (support_IRIW_for_not_multiple_copy_atomic_cpu) {\n+      __ isync();\n+    } else {\n+      \/\/ isync would be sufficient in case of CompareAndExchangeAcquire, but we currently don't optimize for that.\n+      __ sync();\n+    }\n+  %}\n+  ins_pipe(pipe_class_default);\n+%}\n+\n+instruct g1CompareAndExchangeN(iRegNdst res, indirect mem, iRegNsrc oldval, iRegNsrc newval, iRegPdst tmp1, iRegPdst tmp2, flagsRegCR0 cr0)\n+%{\n+  predicate(UseG1GC && n->as_LoadStore()->barrier_data() != 0 &&\n+            (((CompareAndExchangeNode*)n)->order() != MemNode::acquire && ((CompareAndExchangeNode*)n)->order() != MemNode::seqcst));\n+  match(Set res (CompareAndExchangeN mem (Binary oldval newval)));\n+  effect(TEMP_DEF res, TEMP tmp1, TEMP tmp2, KILL cr0);\n+  format %{ \"cmpxchgw $newval, $mem\" %}\n+  ins_encode %{\n+    Label no_update;\n+    __ cmpxchgw(CCR0, $res$$Register, $oldval$$Register, $newval$$Register, $mem$$Register,\n+                MacroAssembler::MemBarNone, MacroAssembler::cmpxchgx_hint_atomic_update(),\n+                noreg, &no_update, true);\n+    \/\/ Pass oldval to SATB which is the only value which can get overwritten.\n+    \/\/ Can be done after cmpxchg because there's no safepoint here.\n+    pre_write_barrier(masm, this,\n+                      noreg,\n+                      $oldval$$Register,\n+                      $tmp1$$Register,\n+                      $tmp2$$Register,\n+                      RegSet::of($mem$$Register, $newval$$Register) \/* preserve *\/);\n+    post_write_barrier(masm, this,\n+                       $mem$$Register,\n+                       $newval$$Register,\n+                       $tmp1$$Register,\n+                       $tmp2$$Register,\n+                       true \/* decode_new_val *\/);\n+    __ bind(no_update);\n+  %}\n+  ins_pipe(pipe_class_default);\n+%}\n+\n+instruct g1CompareAndExchangeN_acq(iRegNdst res, indirect mem, iRegNsrc oldval, iRegNsrc newval, iRegPdst tmp1, iRegPdst tmp2, flagsRegCR0 cr0)\n+%{\n+  predicate(UseG1GC && n->as_LoadStore()->barrier_data() != 0 &&\n+            (((CompareAndExchangeNode*)n)->order() == MemNode::acquire || ((CompareAndExchangeNode*)n)->order() == MemNode::seqcst));\n+  match(Set res (CompareAndExchangeN mem (Binary oldval newval)));\n+  effect(TEMP_DEF res, TEMP tmp1, TEMP tmp2, KILL cr0);\n+  format %{ \"cmpxchgw acq $newval, $mem\" %}\n+  ins_encode %{\n+    Label no_update;\n+    __ cmpxchgw(CCR0, $res$$Register, $oldval$$Register, $newval$$Register, $mem$$Register,\n+                MacroAssembler::MemBarNone, MacroAssembler::cmpxchgx_hint_atomic_update(),\n+                noreg, &no_update, true);\n+    \/\/ Pass oldval to SATB which is the only value which can get overwritten.\n+    \/\/ Can be done after cmpxchg because there's no safepoint here.\n+    pre_write_barrier(masm, this,\n+                      noreg,\n+                      $oldval$$Register,\n+                      $tmp1$$Register,\n+                      $tmp2$$Register,\n+                      RegSet::of($mem$$Register, $newval$$Register) \/* preserve *\/);\n+    post_write_barrier(masm, this,\n+                       $mem$$Register,\n+                       $newval$$Register,\n+                       $tmp1$$Register,\n+                       $tmp2$$Register,\n+                       true \/* decode_new_val *\/);\n+    __ bind(no_update);\n+    if (support_IRIW_for_not_multiple_copy_atomic_cpu) {\n+      __ isync();\n+    } else {\n+      \/\/ isync would be sufficient in case of CompareAndExchangeAcquire, but we currently don't optimize for that.\n+      __ sync();\n+    }\n+  %}\n+  ins_pipe(pipe_class_default);\n+%}\n+\n+instruct g1CompareAndSwapP(iRegIdst res, indirect mem, iRegPsrc oldval, iRegPsrc newval, iRegPdst tmp, flagsRegCR0 cr0)\n+%{\n+  predicate(UseG1GC && n->as_LoadStore()->barrier_data() != 0 &&\n+            (((CompareAndSwapNode*)n)->order() != MemNode::acquire && ((CompareAndSwapNode*)n)->order() != MemNode::seqcst));\n+  match(Set res (CompareAndSwapP mem (Binary oldval newval)));\n+  effect(TEMP_DEF res, TEMP tmp, KILL cr0);\n+  format %{ \"CMPXCHGD $res, $mem, $oldval, $newval; as bool; ptr\" %}\n+  ins_encode %{\n+    Label no_update;\n+    __ li($res$$Register, 0);\n+    __ cmpxchgd(CCR0, R0, $oldval$$Register, $newval$$Register, $mem$$Register,\n+                MacroAssembler::MemBarNone, MacroAssembler::cmpxchgx_hint_atomic_update(),\n+                noreg, &no_update, true);\n+    \/\/ Pass oldval to SATB which is the only value which can get overwritten.\n+    \/\/ Can be done after cmpxchg because there's no safepoint here.\n+    pre_write_barrier(masm, this,\n+                      noreg,\n+                      $oldval$$Register \/* pre_val *\/,\n+                      $tmp$$Register,\n+                      $res$$Register \/* temp *\/,\n+                      RegSet::of($mem$$Register, $newval$$Register) \/* preserve *\/,\n+                      RegSet::of($res$$Register) \/* no_preserve *\/);\n+    post_write_barrier(masm, this,\n+                       $mem$$Register,\n+                       $newval$$Register,\n+                       $tmp$$Register,\n+                       $res$$Register \/* temp *\/);\n+    __ li($res$$Register, 1);\n+    __ bind(no_update);\n+  %}\n+  ins_pipe(pipe_class_default);\n+%}\n+\n+instruct g1CompareAndSwapP_acq(iRegIdst res, indirect mem, iRegPsrc oldval, iRegPsrc newval, iRegPdst tmp, flagsRegCR0 cr0)\n+%{\n+  predicate(UseG1GC && n->as_LoadStore()->barrier_data() != 0 &&\n+            (((CompareAndSwapNode*)n)->order() == MemNode::acquire || ((CompareAndSwapNode*)n)->order() == MemNode::seqcst));\n+  match(Set res (CompareAndSwapP mem (Binary oldval newval)));\n+  effect(TEMP_DEF res, TEMP tmp, KILL cr0);\n+  format %{ \"CMPXCHGD acq $res, $mem, $oldval, $newval; as bool; ptr\" %}\n+  ins_encode %{\n+    Label no_update;\n+    __ li($res$$Register, 0);\n+    __ cmpxchgd(CCR0, R0, $oldval$$Register, $newval$$Register, $mem$$Register,\n+                MacroAssembler::MemBarNone, MacroAssembler::cmpxchgx_hint_atomic_update(),\n+                noreg, &no_update, true);\n+    \/\/ Pass oldval to SATB which is the only value which can get overwritten.\n+    \/\/ Can be done after cmpxchg because there's no safepoint here.\n+    pre_write_barrier(masm, this,\n+                      noreg,\n+                      $oldval$$Register \/* pre_val *\/,\n+                      $tmp$$Register,\n+                      $res$$Register \/* temp *\/,\n+                      RegSet::of($mem$$Register, $newval$$Register) \/* preserve *\/,\n+                      RegSet::of($res$$Register) \/* no_preserve *\/);\n+    post_write_barrier(masm, this,\n+                       $mem$$Register,\n+                       $newval$$Register,\n+                       $tmp$$Register,\n+                       $res$$Register \/* temp *\/);\n+    __ li($res$$Register, 1);\n+    __ bind(no_update);\n+    if (support_IRIW_for_not_multiple_copy_atomic_cpu) {\n+      __ isync();\n+    } else {\n+      \/\/ isync would be sufficient in case of CompareAndExchangeAcquire, but we currently don't optimize for that.\n+      __ sync();\n+    }\n+  %}\n+  ins_pipe(pipe_class_default);\n+%}\n+\n+instruct g1CompareAndSwapN(iRegIdst res, indirect mem, iRegNsrc oldval, iRegNsrc newval, iRegPdst tmp, flagsRegCR0 cr0)\n+%{\n+  predicate(UseG1GC && n->as_LoadStore()->barrier_data() != 0 &&\n+            (((CompareAndSwapNode*)n)->order() != MemNode::acquire && ((CompareAndSwapNode*)n)->order() != MemNode::seqcst));\n+  match(Set res (CompareAndSwapN mem (Binary oldval newval)));\n+  effect(TEMP_DEF res, TEMP tmp, KILL cr0);\n+  format %{ \"CMPXCHGW $res, $mem, $oldval, $newval; as bool; ptr\" %}\n+  ins_encode %{\n+    Label no_update;\n+    __ li($res$$Register, 0);\n+    __ cmpxchgw(CCR0, R0, $oldval$$Register, $newval$$Register, $mem$$Register,\n+                MacroAssembler::MemBarNone, MacroAssembler::cmpxchgx_hint_atomic_update(),\n+                noreg, &no_update, true);\n+    \/\/ Pass oldval to SATB which is the only value which can get overwritten.\n+    \/\/ Can be done after cmpxchg because there's no safepoint here.\n+    pre_write_barrier(masm, this,\n+                      noreg,\n+                      $oldval$$Register \/* pre_val *\/,\n+                      $tmp$$Register,\n+                      $res$$Register \/* temp *\/,\n+                      RegSet::of($mem$$Register, $newval$$Register) \/* preserve *\/,\n+                      RegSet::of($res$$Register) \/* no_preserve *\/);\n+    post_write_barrier(masm, this,\n+                       $mem$$Register,\n+                       $newval$$Register,\n+                       $tmp$$Register,\n+                       $res$$Register \/* temp *\/,\n+                       true \/* decode_new_val *\/);\n+    __ li($res$$Register, 1);\n+    __ bind(no_update);\n+  %}\n+  ins_pipe(pipe_class_default);\n+%}\n+\n+instruct g1CompareAndSwapN_acq(iRegIdst res, indirect mem, iRegNsrc oldval, iRegNsrc newval, iRegPdst tmp, flagsRegCR0 cr0)\n+%{\n+  predicate(UseG1GC && n->as_LoadStore()->barrier_data() != 0 &&\n+            (((CompareAndSwapNode*)n)->order() == MemNode::acquire || ((CompareAndSwapNode*)n)->order() == MemNode::seqcst));\n+  match(Set res (CompareAndSwapN mem (Binary oldval newval)));\n+  effect(TEMP_DEF res, TEMP tmp, KILL cr0);\n+  format %{ \"CMPXCHGW acq $res, $mem, $oldval, $newval; as bool; ptr\" %}\n+  ins_encode %{\n+    Label no_update;\n+    __ li($res$$Register, 0);\n+    __ cmpxchgw(CCR0, R0, $oldval$$Register, $newval$$Register, $mem$$Register,\n+                MacroAssembler::MemBarNone, MacroAssembler::cmpxchgx_hint_atomic_update(),\n+                noreg, &no_update, true);\n+    \/\/ Pass oldval to SATB which is the only value which can get overwritten.\n+    \/\/ Can be done after cmpxchg because there's no safepoint here.\n+    pre_write_barrier(masm, this,\n+                      noreg,\n+                      $oldval$$Register \/* pre_val *\/,\n+                      $tmp$$Register,\n+                      $res$$Register \/* temp *\/,\n+                      RegSet::of($mem$$Register, $newval$$Register) \/* preserve *\/,\n+                      RegSet::of($res$$Register) \/* no_preserve *\/);\n+    post_write_barrier(masm, this,\n+                       $mem$$Register,\n+                       $newval$$Register,\n+                       $tmp$$Register,\n+                       $res$$Register \/* temp *\/,\n+                       true \/* decode_new_val *\/);\n+    __ li($res$$Register, 1);\n+    __ bind(no_update);\n+    if (support_IRIW_for_not_multiple_copy_atomic_cpu) {\n+      __ isync();\n+    } else {\n+      \/\/ isync would be sufficient in case of CompareAndExchangeAcquire, but we currently don't optimize for that.\n+      __ sync();\n+    }\n+  %}\n+  ins_pipe(pipe_class_default);\n+%}\n+\n+instruct weakG1CompareAndSwapP(iRegIdst res, indirect mem, iRegPsrc oldval, iRegPsrc newval, iRegPdst tmp, flagsRegCR0 cr0)\n+%{\n+  predicate(UseG1GC && n->as_LoadStore()->barrier_data() != 0 &&\n+            (((CompareAndSwapNode*)n)->order() != MemNode::acquire && ((CompareAndSwapNode*)n)->order() != MemNode::seqcst));\n+  match(Set res (WeakCompareAndSwapP mem (Binary oldval newval)));\n+  effect(TEMP_DEF res, TEMP tmp, KILL cr0);\n+  format %{ \"weak CMPXCHGD $res, $mem, $oldval, $newval; as bool; ptr\" %}\n+  ins_encode %{\n+    Label no_update;\n+    __ li($res$$Register, 0);\n+    __ cmpxchgd(CCR0, R0, $oldval$$Register, $newval$$Register, $mem$$Register,\n+                MacroAssembler::MemBarNone, MacroAssembler::cmpxchgx_hint_atomic_update(),\n+                noreg, &no_update, true, true);\n+    \/\/ Pass oldval to SATB which is the only value which can get overwritten.\n+    \/\/ Can be done after cmpxchg because there's no safepoint here.\n+    pre_write_barrier(masm, this,\n+                      noreg,\n+                      $oldval$$Register \/* pre_val *\/,\n+                      $tmp$$Register,\n+                      $res$$Register \/* temp *\/,\n+                      RegSet::of($mem$$Register, $newval$$Register) \/* preserve *\/,\n+                      RegSet::of($res$$Register) \/* no_preserve *\/);\n+    post_write_barrier(masm, this,\n+                       $mem$$Register,\n+                       $newval$$Register,\n+                       $tmp$$Register,\n+                       $res$$Register \/* temp *\/);\n+    __ li($res$$Register, 1);\n+    __ bind(no_update);\n+  %}\n+  ins_pipe(pipe_class_default);\n+%}\n+\n+instruct weakG1CompareAndSwapP_acq(iRegIdst res, indirect mem, iRegPsrc oldval, iRegPsrc newval, iRegPdst tmp, flagsRegCR0 cr0)\n+%{\n+  predicate(UseG1GC && n->as_LoadStore()->barrier_data() != 0 &&\n+            (((CompareAndSwapNode*)n)->order() == MemNode::acquire || ((CompareAndSwapNode*)n)->order() == MemNode::seqcst));\n+  match(Set res (WeakCompareAndSwapP mem (Binary oldval newval)));\n+  effect(TEMP_DEF res, TEMP tmp, KILL cr0);\n+  format %{ \"weak CMPXCHGD acq $res, $mem, $oldval, $newval; as bool; ptr\" %}\n+  ins_encode %{\n+    Label no_update;\n+    __ li($res$$Register, 0);\n+    __ cmpxchgd(CCR0, R0, $oldval$$Register, $newval$$Register, $mem$$Register,\n+                MacroAssembler::MemBarNone, MacroAssembler::cmpxchgx_hint_atomic_update(),\n+                noreg, &no_update, true, true);\n+    \/\/ Pass oldval to SATB which is the only value which can get overwritten.\n+    \/\/ Can be done after cmpxchg because there's no safepoint here.\n+    pre_write_barrier(masm, this,\n+                      noreg,\n+                      $oldval$$Register \/* pre_val *\/,\n+                      $tmp$$Register,\n+                      $res$$Register \/* temp *\/,\n+                      RegSet::of($mem$$Register, $newval$$Register) \/* preserve *\/,\n+                      RegSet::of($res$$Register) \/* no_preserve *\/);\n+    post_write_barrier(masm, this,\n+                       $mem$$Register,\n+                       $newval$$Register,\n+                       $tmp$$Register,\n+                       $res$$Register \/* temp *\/);\n+    __ li($res$$Register, 1);\n+    if (support_IRIW_for_not_multiple_copy_atomic_cpu) {\n+      __ isync();\n+    } else {\n+      \/\/ isync would be sufficient in case of CompareAndExchangeAcquire, but we currently don't optimize for that.\n+      __ sync();\n+    }\n+    __ bind(no_update); \/\/ weak version requires no memory barrier on failure\n+  %}\n+  ins_pipe(pipe_class_default);\n+%}\n+\n+instruct weakG1CompareAndSwapN(iRegIdst res, indirect mem, iRegNsrc oldval, iRegNsrc newval, iRegPdst tmp, flagsRegCR0 cr0)\n+%{\n+  predicate(UseG1GC && n->as_LoadStore()->barrier_data() != 0 &&\n+            (((CompareAndSwapNode*)n)->order() != MemNode::acquire && ((CompareAndSwapNode*)n)->order() != MemNode::seqcst));\n+  match(Set res (WeakCompareAndSwapN mem (Binary oldval newval)));\n+  effect(TEMP_DEF res, TEMP tmp, KILL cr0);\n+  format %{ \"weak CMPXCHGW $res, $mem, $oldval, $newval; as bool; ptr\" %}\n+  ins_encode %{\n+    Label no_update;\n+    __ li($res$$Register, 0);\n+    __ cmpxchgw(CCR0, R0, $oldval$$Register, $newval$$Register, $mem$$Register,\n+                MacroAssembler::MemBarNone, MacroAssembler::cmpxchgx_hint_atomic_update(),\n+                noreg, &no_update, true, true);\n+    \/\/ Pass oldval to SATB which is the only value which can get overwritten.\n+    \/\/ Can be done after cmpxchg because there's no safepoint here.\n+    pre_write_barrier(masm, this,\n+                      noreg,\n+                      $oldval$$Register \/* pre_val *\/,\n+                      $tmp$$Register,\n+                      $res$$Register \/* temp *\/,\n+                      RegSet::of($mem$$Register, $newval$$Register) \/* preserve *\/,\n+                      RegSet::of($res$$Register) \/* no_preserve *\/);\n+    post_write_barrier(masm, this,\n+                       $mem$$Register,\n+                       $newval$$Register,\n+                       $tmp$$Register,\n+                       $res$$Register \/* temp *\/,\n+                       true \/* decode_new_val *\/);\n+    __ li($res$$Register, 1);\n+    __ bind(no_update);\n+  %}\n+  ins_pipe(pipe_class_default);\n+%}\n+\n+instruct weakG1CompareAndSwapN_acq(iRegIdst res, indirect mem, iRegNsrc oldval, iRegNsrc newval, iRegPdst tmp, flagsRegCR0 cr0)\n+%{\n+  predicate(UseG1GC && n->as_LoadStore()->barrier_data() != 0 &&\n+            (((CompareAndSwapNode*)n)->order() == MemNode::acquire || ((CompareAndSwapNode*)n)->order() == MemNode::seqcst));\n+  match(Set res (WeakCompareAndSwapN mem (Binary oldval newval)));\n+  effect(TEMP_DEF res, TEMP tmp, KILL cr0);\n+  format %{ \"weak CMPXCHGW acq $res, $mem, $oldval, $newval; as bool; ptr\" %}\n+  ins_encode %{\n+    Label no_update;\n+    __ li($res$$Register, 0);\n+    __ cmpxchgw(CCR0, R0, $oldval$$Register, $newval$$Register, $mem$$Register,\n+                MacroAssembler::MemBarNone, MacroAssembler::cmpxchgx_hint_atomic_update(),\n+                noreg, &no_update, true, true);\n+    \/\/ Pass oldval to SATB which is the only value which can get overwritten.\n+    \/\/ Can be done after cmpxchg because there's no safepoint here.\n+    pre_write_barrier(masm, this,\n+                      noreg,\n+                      $oldval$$Register \/* pre_val *\/,\n+                      $tmp$$Register,\n+                      $res$$Register \/* temp *\/,\n+                      RegSet::of($mem$$Register, $newval$$Register) \/* preserve *\/,\n+                      RegSet::of($res$$Register) \/* no_preserve *\/);\n+    post_write_barrier(masm, this,\n+                       $mem$$Register,\n+                       $newval$$Register,\n+                       $tmp$$Register,\n+                       $res$$Register \/* temp *\/,\n+                       true \/* decode_new_val *\/);\n+    __ li($res$$Register, 1);\n+    if (support_IRIW_for_not_multiple_copy_atomic_cpu) {\n+      __ isync();\n+    } else {\n+      \/\/ isync would be sufficient in case of CompareAndExchangeAcquire, but we currently don't optimize for that.\n+      __ sync();\n+    }\n+    __ bind(no_update); \/\/ weak version requires no memory barrier on failure\n+  %}\n+  ins_pipe(pipe_class_default);\n+%}\n+\n+instruct g1GetAndSetP(iRegPdst res, indirect mem, iRegPsrc newval, iRegPdst tmp1, iRegPdst tmp2, flagsRegCR0 cr0)\n+%{\n+  predicate(UseG1GC && n->as_LoadStore()->barrier_data() != 0);\n+  match(Set res (GetAndSetP mem newval));\n+  effect(TEMP_DEF res, TEMP tmp1, TEMP tmp2, KILL cr0);\n+  format %{ \"GetAndSetP    $newval, $mem\" %}\n+  ins_encode %{\n+    assert_different_registers($mem$$Register, $newval$$Register);\n+    __ getandsetd($res$$Register, $newval$$Register, $mem$$Register,\n+                  MacroAssembler::cmpxchgx_hint_atomic_update());\n+    \/\/ Can be done after cmpxchg because there's no safepoint here.\n+    pre_write_barrier(masm, this,\n+                      noreg \/* obj *\/,\n+                      $res$$Register \/* res *\/,\n+                      $tmp1$$Register,\n+                      $tmp2$$Register,\n+                      RegSet::of($mem$$Register, $newval$$Register) \/* preserve *\/);\n+    post_write_barrier(masm, this,\n+                       $mem$$Register,\n+                       $newval$$Register,\n+                       $tmp1$$Register,\n+                       $tmp2$$Register);\n+    if (support_IRIW_for_not_multiple_copy_atomic_cpu) {\n+      __ isync();\n+    } else {\n+      __ sync();\n+    }\n+  %}\n+  ins_pipe(pipe_class_default);\n+%}\n+\n+instruct g1GetAndSetN(iRegNdst res, indirect mem, iRegNsrc newval, iRegPdst tmp1, iRegPdst tmp2, flagsRegCR0 cr0)\n+%{\n+  predicate(UseG1GC && n->as_LoadStore()->barrier_data() != 0);\n+  match(Set res (GetAndSetN mem newval));\n+  effect(TEMP_DEF res, TEMP tmp1, TEMP tmp2, KILL cr0);\n+  format %{ \"GetAndSetN    $newval, $mem\" %}\n+  ins_encode %{\n+    assert_different_registers($mem$$Register, $newval$$Register);\n+    __ getandsetw($res$$Register, $newval$$Register, $mem$$Register,\n+                  MacroAssembler::cmpxchgx_hint_atomic_update());\n+    \/\/ Can be done after cmpxchg because there's no safepoint here.\n+    pre_write_barrier(masm, this,\n+                      noreg \/* obj *\/,\n+                      $res$$Register \/* res *\/,\n+                      $tmp1$$Register,\n+                      $tmp2$$Register,\n+                      RegSet::of($mem$$Register, $newval$$Register) \/* preserve *\/);\n+    post_write_barrier(masm, this,\n+                       $mem$$Register,\n+                       $newval$$Register,\n+                       $tmp1$$Register,\n+                       $tmp2$$Register,\n+                       true \/* decode_new_val *\/);\n+    if (support_IRIW_for_not_multiple_copy_atomic_cpu) {\n+      __ isync();\n+    } else {\n+      __ sync();\n+    }\n+  %}\n+  ins_pipe(pipe_class_default);\n+%}\n+\n+instruct g1LoadP(iRegPdst dst, memoryAlg4 mem, iRegPdst tmp, flagsRegCR0 cr0)\n+%{\n+  predicate(UseG1GC && n->as_Load()->is_unordered() && n->as_Load()->barrier_data() != 0);\n+  \/\/ This instruction does not need an acquiring counterpart because it is only\n+  \/\/ used for reference loading (Reference::get()).\n+  match(Set dst (LoadP mem));\n+  effect(TEMP_DEF dst, TEMP tmp, KILL cr0);\n+  ins_cost(2 * MEMORY_REF_COST);\n+  format %{ \"ld    $dst, $mem\\t# ptr\" %}\n+  ins_encode %{\n+    __ ld($dst$$Register, $mem$$disp, $mem$$base$$Register);\n+    pre_write_barrier(masm, this,\n+                      noreg \/* obj *\/,\n+                      $dst$$Register \/* pre_val *\/,\n+                      $tmp$$Register);\n+  %}\n+  ins_pipe(pipe_class_default);\n+%}\n+\n+instruct g1LoadN(iRegNdst dst, memoryAlg4 mem, iRegPdst tmp1, iRegPdst tmp2, flagsRegCR0 cr0)\n+%{\n+  predicate(UseG1GC && n->as_Load()->is_unordered() && n->as_Load()->barrier_data() != 0);\n+  \/\/ This instruction does not need an acquiring counterpart because it is only\n+  \/\/ used for reference loading (Reference::get()).\n+  match(Set dst (LoadN mem));\n+  effect(TEMP_DEF dst, TEMP tmp1, TEMP tmp2, KILL cr0);\n+  ins_cost(2 * MEMORY_REF_COST);\n+  format %{ \"lwz    $dst, $mem\\t# ptr\" %}\n+  ins_encode %{\n+    __ lwz($dst$$Register, $mem$$disp, $mem$$base$$Register);\n+    pre_write_barrier(masm, this,\n+                      noreg \/* obj *\/,\n+                      $dst$$Register,\n+                      $tmp1$$Register,\n+                      $tmp2$$Register);\n+  %}\n+  ins_pipe(pipe_class_default);\n+%}\n","filename":"src\/hotspot\/cpu\/ppc\/gc\/g1\/g1_ppc.ad","additions":687,"deletions":0,"binary":false,"changes":687,"status":"added"},{"patch":"@@ -1003,0 +1003,4 @@\n+  if (is_encode_and_store_pattern(n, m)) {\n+    mstack.push(m, Visit);\n+    return true;\n+  }\n@@ -5410,1 +5414,1 @@\n-  predicate(n->as_Load()->is_unordered() || followed_by_acquire(n));\n+  predicate((n->as_Load()->is_unordered() || followed_by_acquire(n)) && n->as_Load()->barrier_data() == 0);\n@@ -5422,0 +5426,1 @@\n+  predicate(n->as_Load()->barrier_data() == 0);\n@@ -5435,1 +5440,1 @@\n-  predicate(_kids[0]->_leaf->as_Load()->is_unordered() && CompressedOops::shift() == 0);\n+  predicate(_kids[0]->_leaf->as_Load()->is_unordered() && CompressedOops::shift() == 0 && _kids[0]->_leaf->as_Load()->barrier_data() == 0);\n@@ -6426,0 +6431,1 @@\n+  predicate(n->as_Store()->barrier_data() == 0);\n@@ -7480,0 +7486,1 @@\n+  predicate(n->as_LoadStore()->barrier_data() == 0);\n@@ -7679,1 +7686,1 @@\n-  predicate(((CompareAndSwapNode*)n)->order() != MemNode::acquire && ((CompareAndSwapNode*)n)->order() != MemNode::seqcst);\n+  predicate(((CompareAndSwapNode*)n)->order() != MemNode::acquire && ((CompareAndSwapNode*)n)->order() != MemNode::seqcst && n->as_LoadStore()->barrier_data() == 0);\n@@ -7693,1 +7700,1 @@\n-  predicate(((CompareAndSwapNode*)n)->order() == MemNode::acquire || ((CompareAndSwapNode*)n)->order() == MemNode::seqcst);\n+  predicate((((CompareAndSwapNode*)n)->order() == MemNode::acquire || ((CompareAndSwapNode*)n)->order() == MemNode::seqcst) && n->as_LoadStore()->barrier_data() == 0);\n@@ -7942,1 +7949,1 @@\n-  predicate(((CompareAndSwapNode*)n)->order() != MemNode::acquire && ((CompareAndSwapNode*)n)->order() != MemNode::seqcst);\n+  predicate(((CompareAndSwapNode*)n)->order() != MemNode::acquire && ((CompareAndSwapNode*)n)->order() != MemNode::seqcst && n->as_LoadStore()->barrier_data() == 0);\n@@ -7956,1 +7963,1 @@\n-  predicate(((CompareAndSwapNode*)n)->order() == MemNode::acquire || ((CompareAndSwapNode*)n)->order() == MemNode::seqcst);\n+  predicate((((CompareAndSwapNode*)n)->order() == MemNode::acquire || ((CompareAndSwapNode*)n)->order() == MemNode::seqcst) && n->as_LoadStore()->barrier_data() == 0);\n@@ -8265,0 +8272,1 @@\n+  predicate(n->as_LoadStore()->barrier_data() == 0);\n","filename":"src\/hotspot\/cpu\/ppc\/ppc.ad","additions":14,"deletions":6,"binary":false,"changes":20,"status":"modified"},{"patch":"@@ -30,0 +30,1 @@\n+#include \"utilities\/count_trailing_zeros.hpp\"\n@@ -558,0 +559,8 @@\n+template <>\n+inline Register AbstractRegSet<Register>::first() {\n+  if (_bitset == 0) { return noreg; }\n+  return as_Register(count_trailing_zeros(_bitset));\n+}\n+\n+typedef AbstractRegSet<Register> RegSet;\n+\n","filename":"src\/hotspot\/cpu\/ppc\/register_ppc.hpp","additions":9,"deletions":0,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -3,1 +3,1 @@\n- * Copyright (c) 2020, 2023, Huawei Technologies Co., Ltd. All rights reserved.\n+ * Copyright (c) 2020, 2024, Huawei Technologies Co., Ltd. All rights reserved.\n@@ -42,1 +42,4 @@\n-#endif\n+#endif \/\/ COMPILER1\n+#ifdef COMPILER2\n+#include \"gc\/g1\/c2\/g1BarrierSetC2.hpp\"\n+#endif \/\/ COMPILER2\n@@ -99,0 +102,49 @@\n+static void generate_queue_test_and_insertion(MacroAssembler* masm, ByteSize index_offset, ByteSize buffer_offset, Label& runtime,\n+                                              const Register thread, const Register value, const Register tmp1, const Register tmp2) {\n+  \/\/ Can we store a value in the given thread's buffer?\n+  \/\/ (The index field is typed as size_t.)\n+  __ ld(tmp1, Address(thread, in_bytes(index_offset)));   \/\/ tmp1 := *(index address)\n+  __ beqz(tmp1, runtime);                                 \/\/ jump to runtime if index == 0 (full buffer)\n+  \/\/ The buffer is not full, store value into it.\n+  __ sub(tmp1, tmp1, wordSize);                           \/\/ tmp1 := next index\n+  __ sd(tmp1, Address(thread, in_bytes(index_offset)));   \/\/ *(index address) := next index\n+  __ ld(tmp2, Address(thread, in_bytes(buffer_offset)));  \/\/ tmp2 := buffer address\n+  __ add(tmp2, tmp2, tmp1);\n+  __ sd(value, Address(tmp2));                            \/\/ *(buffer address + next index) := value\n+}\n+\n+static void generate_pre_barrier_fast_path(MacroAssembler* masm,\n+                                           const Register thread,\n+                                           const Register tmp1) {\n+  Address in_progress(thread, in_bytes(G1ThreadLocalData::satb_mark_queue_active_offset()));\n+  \/\/ Is marking active?\n+  if (in_bytes(SATBMarkQueue::byte_width_of_active()) == 4) {\n+    __ lwu(tmp1, in_progress);\n+  } else {\n+    assert(in_bytes(SATBMarkQueue::byte_width_of_active()) == 1, \"Assumption\");\n+    __ lbu(tmp1, in_progress);\n+  }\n+}\n+\n+static void generate_pre_barrier_slow_path(MacroAssembler* masm,\n+                                           const Register obj,\n+                                           const Register pre_val,\n+                                           const Register thread,\n+                                           const Register tmp1,\n+                                           const Register tmp2,\n+                                           Label& done,\n+                                           Label& runtime) {\n+  \/\/ Do we need to load the previous value?\n+  if (obj != noreg) {\n+    __ load_heap_oop(pre_val, Address(obj, 0), noreg, noreg, AS_RAW);\n+  }\n+  \/\/ Is the previous value null?\n+  __ beqz(pre_val, done, true);\n+  generate_queue_test_and_insertion(masm,\n+                                    G1ThreadLocalData::satb_mark_queue_index_offset(),\n+                                    G1ThreadLocalData::satb_mark_queue_buffer_offset(),\n+                                    runtime,\n+                                    thread, pre_val, tmp1, tmp2);\n+  __ j(done);\n+}\n+\n@@ -119,11 +171,2 @@\n-  Address in_progress(thread, in_bytes(G1ThreadLocalData::satb_mark_queue_active_offset()));\n-  Address index(thread, in_bytes(G1ThreadLocalData::satb_mark_queue_index_offset()));\n-  Address buffer(thread, in_bytes(G1ThreadLocalData::satb_mark_queue_buffer_offset()));\n-\n-  \/\/ Is marking active?\n-  if (in_bytes(SATBMarkQueue::byte_width_of_active()) == 4) { \/\/ 4-byte width\n-    __ lwu(tmp1, in_progress);\n-  } else {\n-    assert(in_bytes(SATBMarkQueue::byte_width_of_active()) == 1, \"Assumption\");\n-    __ lbu(tmp1, in_progress);\n-  }\n+  generate_pre_barrier_fast_path(masm, thread, tmp1);\n+  \/\/ If marking is not active (*(mark queue active address) == 0), jump to done\n@@ -131,25 +174,1 @@\n-\n-  \/\/ Do we need to load the previous value?\n-  if (obj != noreg) {\n-    __ load_heap_oop(pre_val, Address(obj, 0), noreg, noreg, AS_RAW);\n-  }\n-\n-  \/\/ Is the previous value null?\n-  __ beqz(pre_val, done);\n-\n-  \/\/ Can we store original value in the thread's buffer?\n-  \/\/ Is index == 0?\n-  \/\/ (The index field is typed as size_t.)\n-\n-  __ ld(tmp1, index);                  \/\/ tmp := *index_adr\n-  __ beqz(tmp1, runtime);              \/\/ tmp == 0?\n-                                       \/\/ If yes, goto runtime\n-\n-  __ sub(tmp1, tmp1, wordSize);        \/\/ tmp := tmp - wordSize\n-  __ sd(tmp1, index);                  \/\/ *index_adr := tmp\n-  __ ld(tmp2, buffer);\n-  __ add(tmp1, tmp1, tmp2);            \/\/ tmp := tmp + *buffer_adr\n-\n-  \/\/ Record the previous value\n-  __ sd(pre_val, Address(tmp1, 0));\n-  __ j(done);\n+  generate_pre_barrier_slow_path(masm, obj, pre_val, thread, tmp1, tmp2, done, runtime);\n@@ -174,0 +193,43 @@\n+static void generate_post_barrier_fast_path(MacroAssembler* masm,\n+                                            const Register store_addr,\n+                                            const Register new_val,\n+                                            const Register tmp1,\n+                                            const Register tmp2,\n+                                            Label& done,\n+                                            bool new_val_may_be_null) {\n+  \/\/ Does store cross heap regions?\n+  __ xorr(tmp1, store_addr, new_val);                    \/\/ tmp1 := store address ^ new value\n+  __ srli(tmp1, tmp1, G1HeapRegion::LogOfHRGrainBytes);  \/\/ tmp1 := ((store address ^ new value) >> LogOfHRGrainBytes)\n+  __ beqz(tmp1, done);\n+  \/\/ Crosses regions, storing null?\n+  if (new_val_may_be_null) {\n+    __ beqz(new_val, done);\n+  }\n+  \/\/ Storing region crossing non-null, is card young?\n+  __ srli(tmp1, store_addr, CardTable::card_shift());    \/\/ tmp1 := card address relative to card table base\n+  __ load_byte_map_base(tmp2);                           \/\/ tmp2 := card table base address\n+  __ add(tmp1, tmp1, tmp2);                              \/\/ tmp1 := card address\n+  __ lbu(tmp2, Address(tmp1));                           \/\/ tmp2 := card\n+}\n+\n+static void generate_post_barrier_slow_path(MacroAssembler* masm,\n+                                            const Register thread,\n+                                            const Register tmp1,\n+                                            const Register tmp2,\n+                                            Label& done,\n+                                            Label& runtime) {\n+  __ membar(MacroAssembler::StoreLoad);  \/\/ StoreLoad membar\n+  __ lbu(tmp2, Address(tmp1));           \/\/ tmp2 := card\n+  __ beqz(tmp2, done, true);\n+  \/\/ Storing a region crossing, non-null oop, card is clean.\n+  \/\/ Dirty card and log.\n+  STATIC_ASSERT(CardTable::dirty_card_val() == 0);\n+  __ sb(zr, Address(tmp1));       \/\/ *(card address) := dirty_card_val\n+  generate_queue_test_and_insertion(masm,\n+                                    G1ThreadLocalData::dirty_card_queue_index_offset(),\n+                                    G1ThreadLocalData::dirty_card_queue_buffer_offset(),\n+                                    runtime,\n+                                    thread, tmp1, tmp2, t0);\n+  __ j(done);\n+}\n+\n@@ -182,8 +244,2 @@\n-  assert(store_addr != noreg && new_val != noreg && tmp1 != noreg &&\n-         tmp2 != noreg, \"expecting a register\");\n-\n-  Address queue_index(thread, in_bytes(G1ThreadLocalData::dirty_card_queue_index_offset()));\n-  Address buffer(thread, in_bytes(G1ThreadLocalData::dirty_card_queue_buffer_offset()));\n-\n-  BarrierSet* bs = BarrierSet::barrier_set();\n-  CardTableBarrierSet* ctbs = barrier_set_cast<CardTableBarrierSet>(bs);\n+  assert(store_addr != noreg && new_val != noreg && tmp1 != noreg && tmp2 != noreg,\n+         \"expecting a register\");\n@@ -194,1 +250,5 @@\n-  \/\/ Does store cross heap regions?\n+  generate_post_barrier_fast_path(masm, store_addr, new_val, tmp1, tmp2, done, true \/* new_val_may_be_null *\/);\n+  \/\/ If card is young, jump to done (tmp2 holds the card value)\n+  __ mv(t0, (int)G1CardTable::g1_young_card_val());\n+  __ beq(tmp2, t0, done);   \/\/ card == young_card_val?\n+  generate_post_barrier_slow_path(masm, thread, tmp1, tmp2, done, runtime);\n@@ -196,3 +256,6 @@\n-  __ xorr(tmp1, store_addr, new_val);\n-  __ srli(tmp1, tmp1, G1HeapRegion::LogOfHRGrainBytes);\n-  __ beqz(tmp1, done);\n+  __ bind(runtime);\n+  \/\/ save the live input values\n+  RegSet saved = RegSet::of(store_addr);\n+  __ push_reg(saved, sp);\n+  __ call_VM_leaf(CAST_FROM_FN_PTR(address, G1BarrierSetRuntime::write_ref_field_post_entry), tmp1, thread);\n+  __ pop_reg(saved, sp);\n@@ -200,1 +263,2 @@\n-  \/\/ crosses regions, storing null?\n+  __ bind(done);\n+}\n@@ -202,1 +266,1 @@\n-  __ beqz(new_val, done);\n+#if defined(COMPILER2)\n@@ -204,1 +268,9 @@\n-  \/\/ storing region crossing non-null, is card already dirty?\n+static void generate_c2_barrier_runtime_call(MacroAssembler* masm, G1BarrierStubC2* stub, const Register arg, const address runtime_path) {\n+  SaveLiveRegisters save_registers(masm, stub);\n+  if (c_rarg0 != arg) {\n+    __ mv(c_rarg0, arg);\n+  }\n+  __ mv(c_rarg1, xthread);\n+  __ mv(t0, runtime_path);\n+  __ jalr(t0);\n+}\n@@ -206,1 +278,10 @@\n-  const Register card_addr = tmp1;\n+void G1BarrierSetAssembler::g1_write_barrier_pre_c2(MacroAssembler* masm,\n+                                                    Register obj,\n+                                                    Register pre_val,\n+                                                    Register thread,\n+                                                    Register tmp1,\n+                                                    Register tmp2,\n+                                                    G1PreBarrierStubC2* stub) {\n+  assert(thread == xthread, \"must be\");\n+  assert_different_registers(obj, pre_val, tmp1, tmp2);\n+  assert(pre_val != noreg && tmp1 != noreg && tmp2 != noreg, \"expecting a register\");\n@@ -208,1 +289,1 @@\n-  __ srli(card_addr, store_addr, CardTable::card_shift());\n+  stub->initialize_registers(obj, pre_val, thread, tmp1, tmp2);\n@@ -210,6 +291,3 @@\n-  \/\/ get the address of the card\n-  __ load_byte_map_base(tmp2);\n-  __ add(card_addr, card_addr, tmp2);\n-  __ lbu(tmp2, Address(card_addr));\n-  __ mv(t0, (int)G1CardTable::g1_young_card_val());\n-  __ beq(tmp2, t0, done);\n+  generate_pre_barrier_fast_path(masm, thread, tmp1);\n+  \/\/ If marking is active (*(mark queue active address) != 0), jump to stub (slow path)\n+  __ bnez(tmp1, *stub->entry(), true);\n@@ -217,1 +295,2 @@\n-  assert((int)CardTable::dirty_card_val() == 0, \"must be 0\");\n+  __ bind(*stub->continuation());\n+}\n@@ -219,1 +298,9 @@\n-  __ membar(MacroAssembler::StoreLoad);\n+void G1BarrierSetAssembler::generate_c2_pre_barrier_stub(MacroAssembler* masm,\n+                                                         G1PreBarrierStubC2* stub) const {\n+  Assembler::InlineSkippedInstructionsCounter skip_counter(masm);\n+  Label runtime;\n+  Register obj = stub->obj();\n+  Register pre_val = stub->pre_val();\n+  Register thread = stub->thread();\n+  Register tmp1 = stub->tmp1();\n+  Register tmp2 = stub->tmp2();\n@@ -221,2 +308,2 @@\n-  __ lbu(tmp2, Address(card_addr));\n-  __ beqz(tmp2, done);\n+  __ bind(*stub->entry());\n+  generate_pre_barrier_slow_path(masm, obj, pre_val, thread, tmp1, tmp2, *stub->continuation(), runtime);\n@@ -224,2 +311,4 @@\n-  \/\/ storing a region crossing, non-null oop, card is clean.\n-  \/\/ dirty card and log.\n+  __ bind(runtime);\n+  generate_c2_barrier_runtime_call(masm, stub, pre_val, CAST_FROM_FN_PTR(address, G1BarrierSetRuntime::write_ref_field_pre_entry));\n+  __ j(*stub->continuation());\n+}\n@@ -227,1 +316,11 @@\n-  __ sb(zr, Address(card_addr));\n+void G1BarrierSetAssembler::g1_write_barrier_post_c2(MacroAssembler* masm,\n+                                                     Register store_addr,\n+                                                     Register new_val,\n+                                                     Register thread,\n+                                                     Register tmp1,\n+                                                     Register tmp2,\n+                                                     G1PostBarrierStubC2* stub) {\n+  assert(thread == xthread, \"must be\");\n+  assert_different_registers(store_addr, new_val, thread, tmp1, tmp2, t0);\n+  assert(store_addr != noreg && new_val != noreg && tmp1 != noreg && tmp2 != noreg,\n+         \"expecting a register\");\n@@ -229,4 +328,1 @@\n-  __ ld(t0, queue_index);\n-  __ beqz(t0, runtime);\n-  __ sub(t0, t0, wordSize);\n-  __ sd(t0, queue_index);\n+  stub->initialize_registers(thread, tmp1, tmp2);\n@@ -234,4 +330,5 @@\n-  __ ld(tmp2, buffer);\n-  __ add(t0, tmp2, t0);\n-  __ sd(card_addr, Address(t0, 0));\n-  __ j(done);\n+  bool new_val_may_be_null = (stub->barrier_data() & G1C2BarrierPostNotNull) == 0;\n+  generate_post_barrier_fast_path(masm, store_addr, new_val, tmp1, tmp2, *stub->continuation(), new_val_may_be_null);\n+  \/\/ If card is not young, jump to stub (slow path) (tmp2 holds the card value)\n+  __ mv(t0, (int)G1CardTable::g1_young_card_val());\n+  __ bne(tmp2, t0, *stub->entry(), true);\n@@ -239,6 +336,2 @@\n-  __ bind(runtime);\n-  \/\/ save the live input values\n-  RegSet saved = RegSet::of(store_addr);\n-  __ push_reg(saved, sp);\n-  __ call_VM_leaf(CAST_FROM_FN_PTR(address, G1BarrierSetRuntime::write_ref_field_post_entry), card_addr, thread);\n-  __ pop_reg(saved, sp);\n+  __ bind(*stub->continuation());\n+}\n@@ -246,1 +339,14 @@\n-  __ bind(done);\n+void G1BarrierSetAssembler::generate_c2_post_barrier_stub(MacroAssembler* masm,\n+                                                          G1PostBarrierStubC2* stub) const {\n+  Assembler::InlineSkippedInstructionsCounter skip_counter(masm);\n+  Label runtime;\n+  Register thread = stub->thread();\n+  Register tmp1 = stub->tmp1(); \/\/ tmp1 holds the card address.\n+  Register tmp2 = stub->tmp2();\n+\n+  __ bind(*stub->entry());\n+  generate_post_barrier_slow_path(masm, thread, tmp1, tmp2, *stub->continuation(), runtime);\n+\n+  __ bind(runtime);\n+  generate_c2_barrier_runtime_call(masm, stub, tmp1, CAST_FROM_FN_PTR(address, G1BarrierSetRuntime::write_ref_field_post_entry));\n+  __ j(*stub->continuation());\n@@ -249,0 +355,2 @@\n+#endif \/\/ COMPILER2\n+\n","filename":"src\/hotspot\/cpu\/riscv\/gc\/g1\/g1BarrierSetAssembler_riscv.cpp","additions":191,"deletions":83,"binary":false,"changes":274,"status":"modified"},{"patch":"@@ -3,1 +3,1 @@\n- * Copyright (c) 2020, 2021, Huawei Technologies Co., Ltd. All rights reserved.\n+ * Copyright (c) 2020, 2024, Huawei Technologies Co., Ltd. All rights reserved.\n@@ -39,0 +39,2 @@\n+class G1PreBarrierStubC2;\n+class G1PostBarrierStubC2;\n@@ -75,0 +77,21 @@\n+#ifdef COMPILER2\n+  void g1_write_barrier_pre_c2(MacroAssembler* masm,\n+                               Register obj,\n+                               Register pre_val,\n+                               Register thread,\n+                               Register tmp1,\n+                               Register tmp2,\n+                               G1PreBarrierStubC2* c2_stub);\n+  void generate_c2_pre_barrier_stub(MacroAssembler* masm,\n+                                    G1PreBarrierStubC2* stub) const;\n+  void g1_write_barrier_post_c2(MacroAssembler* masm,\n+                                Register store_addr,\n+                                Register new_val,\n+                                Register thread,\n+                                Register tmp1,\n+                                Register tmp2,\n+                                G1PostBarrierStubC2* c2_stub);\n+  void generate_c2_post_barrier_stub(MacroAssembler* masm,\n+                                     G1PostBarrierStubC2* stub) const;\n+#endif\n+\n","filename":"src\/hotspot\/cpu\/riscv\/gc\/g1\/g1BarrierSetAssembler_riscv.hpp","additions":24,"deletions":1,"binary":false,"changes":25,"status":"modified"},{"patch":"@@ -0,0 +1,582 @@\n+\/\/\n+\/\/ Copyright (c) 2024, Oracle and\/or its affiliates. All rights reserved.\n+\/\/ Copyright (c) 2024, Huawei Technologies Co., Ltd. All rights reserved.\n+\/\/ DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+\/\/\n+\/\/ This code is free software; you can redistribute it and\/or modify it\n+\/\/ under the terms of the GNU General Public License version 2 only, as\n+\/\/ published by the Free Software Foundation.\n+\/\/\n+\/\/ This code is distributed in the hope that it will be useful, but WITHOUT\n+\/\/ ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+\/\/ FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+\/\/ version 2 for more details (a copy is included in the LICENSE file that\n+\/\/ accompanied this code).\n+\/\/\n+\/\/ You should have received a copy of the GNU General Public License version\n+\/\/ 2 along with this work; if not, write to the Free Software Foundation,\n+\/\/ Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+\/\/\n+\/\/ Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+\/\/ or visit www.oracle.com if you need additional information or have any\n+\/\/ questions.\n+\/\/\n+\n+source_hpp %{\n+\n+#include \"gc\/g1\/c2\/g1BarrierSetC2.hpp\"\n+#include \"gc\/shared\/gc_globals.hpp\"\n+\n+%}\n+\n+source %{\n+\n+#include \"gc\/g1\/g1BarrierSetAssembler_riscv.hpp\"\n+#include \"gc\/g1\/g1BarrierSetRuntime.hpp\"\n+\n+static void write_barrier_pre(MacroAssembler* masm,\n+                              const MachNode* node,\n+                              Register obj,\n+                              Register pre_val,\n+                              Register tmp1,\n+                              Register tmp2,\n+                              RegSet preserve = RegSet(),\n+                              RegSet no_preserve = RegSet()) {\n+  if (!G1PreBarrierStubC2::needs_barrier(node)) {\n+    return;\n+  }\n+  Assembler::InlineSkippedInstructionsCounter skip_counter(masm);\n+  G1BarrierSetAssembler* g1_asm = static_cast<G1BarrierSetAssembler*>(BarrierSet::barrier_set()->barrier_set_assembler());\n+  G1PreBarrierStubC2* const stub = G1PreBarrierStubC2::create(node);\n+  for (RegSetIterator<Register> reg = preserve.begin(); *reg != noreg; ++reg) {\n+    stub->preserve(*reg);\n+  }\n+  for (RegSetIterator<Register> reg = no_preserve.begin(); *reg != noreg; ++reg) {\n+    stub->dont_preserve(*reg);\n+  }\n+  g1_asm->g1_write_barrier_pre_c2(masm, obj, pre_val, xthread, tmp1, tmp2, stub);\n+}\n+\n+static void write_barrier_post(MacroAssembler* masm,\n+                               const MachNode* node,\n+                               Register store_addr,\n+                               Register new_val,\n+                               Register tmp1,\n+                               Register tmp2) {\n+  if (!G1PostBarrierStubC2::needs_barrier(node)) {\n+    return;\n+  }\n+  Assembler::InlineSkippedInstructionsCounter skip_counter(masm);\n+  G1BarrierSetAssembler* g1_asm = static_cast<G1BarrierSetAssembler*>(BarrierSet::barrier_set()->barrier_set_assembler());\n+  G1PostBarrierStubC2* const stub = G1PostBarrierStubC2::create(node);\n+  g1_asm->g1_write_barrier_post_c2(masm, store_addr, new_val, xthread, tmp1, tmp2, stub);\n+}\n+\n+%}\n+\n+instruct g1StoreP(indirect mem, iRegP src, iRegPNoSp tmp1, iRegPNoSp tmp2, iRegPNoSp tmp3)\n+%{\n+  predicate(UseG1GC && n->as_Store()->barrier_data() != 0);\n+  match(Set mem (StoreP mem src));\n+  effect(TEMP tmp1, TEMP tmp2, TEMP tmp3);\n+  ins_cost(STORE_COST);\n+  format %{ \"sd  $src, $mem\\t# ptr\" %}\n+  ins_encode %{\n+    guarantee($mem$$disp == 0, \"impossible encoding\");\n+    write_barrier_pre(masm, this,\n+                      $mem$$Register  \/* obj *\/,\n+                      $tmp1$$Register \/* pre_val *\/,\n+                      $tmp2$$Register \/* tmp1 *\/,\n+                      $tmp3$$Register \/* tmp2 *\/,\n+                      RegSet::of($mem$$Register, $src$$Register) \/* preserve *\/);\n+    __ sd($src$$Register, Address($mem$$Register));\n+    write_barrier_post(masm, this,\n+                       $mem$$Register  \/* store_addr *\/,\n+                       $src$$Register  \/* new_val *\/,\n+                       $tmp2$$Register \/* tmp1 *\/,\n+                       $tmp3$$Register \/* tmp2 *\/);\n+  %}\n+  ins_pipe(istore_reg_mem);\n+%}\n+\n+instruct g1StoreN(indirect mem, iRegN src, iRegPNoSp tmp1, iRegPNoSp tmp2, iRegPNoSp tmp3)\n+%{\n+  predicate(UseG1GC && n->as_Store()->barrier_data() != 0);\n+  match(Set mem (StoreN mem src));\n+  effect(TEMP tmp1, TEMP tmp2, TEMP tmp3);\n+  ins_cost(STORE_COST);\n+  format %{ \"sw  $src, $mem\\t# compressed ptr\" %}\n+  ins_encode %{\n+    guarantee($mem$$disp == 0, \"impossible encoding\");\n+    write_barrier_pre(masm, this,\n+                      $mem$$Register  \/* obj *\/,\n+                      $tmp1$$Register \/* pre_val *\/,\n+                      $tmp2$$Register \/* tmp1 *\/,\n+                      $tmp3$$Register \/* tmp2 *\/,\n+                      RegSet::of($mem$$Register, $src$$Register) \/* preserve *\/);\n+    __ sw($src$$Register, Address($mem$$Register));\n+    if ((barrier_data() & G1C2BarrierPost) != 0) {\n+      if ((barrier_data() & G1C2BarrierPostNotNull) == 0) {\n+        __ decode_heap_oop($tmp1$$Register, $src$$Register);\n+      } else {\n+        __ decode_heap_oop_not_null($tmp1$$Register, $src$$Register);\n+      }\n+    }\n+    write_barrier_post(masm, this,\n+                       $mem$$Register  \/* store_addr *\/,\n+                       $tmp1$$Register \/* new_val *\/,\n+                       $tmp2$$Register \/* tmp1 *\/,\n+                       $tmp3$$Register \/* tmp2 *\/);\n+  %}\n+  ins_pipe(istore_reg_mem);\n+%}\n+\n+instruct g1EncodePAndStoreN(indirect mem, iRegP src, iRegPNoSp tmp1, iRegPNoSp tmp2, iRegPNoSp tmp3)\n+%{\n+  predicate(UseG1GC && n->as_Store()->barrier_data() != 0);\n+  match(Set mem (StoreN mem (EncodeP src)));\n+  effect(TEMP tmp1, TEMP tmp2, TEMP tmp3);\n+  ins_cost(STORE_COST);\n+  format %{ \"encode_heap_oop $tmp1, $src\\n\\t\"\n+            \"sw  $tmp1, $mem\\t# compressed ptr\" %}\n+  ins_encode %{\n+    guarantee($mem$$disp == 0, \"impossible encoding\");\n+    write_barrier_pre(masm, this,\n+                      $mem$$Register  \/* obj *\/,\n+                      $tmp1$$Register \/* pre_val *\/,\n+                      $tmp2$$Register \/* tmp1 *\/,\n+                      $tmp3$$Register \/* tmp2 *\/,\n+                      RegSet::of($mem$$Register, $src$$Register) \/* preserve *\/);\n+    if ((barrier_data() & G1C2BarrierPostNotNull) == 0) {\n+      __ encode_heap_oop($tmp1$$Register, $src$$Register);\n+    } else {\n+      __ encode_heap_oop_not_null($tmp1$$Register, $src$$Register);\n+    }\n+    __ sw($tmp1$$Register, Address($mem$$Register));\n+    write_barrier_post(masm, this,\n+                       $mem$$Register  \/* store_addr *\/,\n+                       $src$$Register  \/* new_val *\/,\n+                       $tmp2$$Register \/* tmp1 *\/,\n+                       $tmp3$$Register \/* tmp2 *\/);\n+  %}\n+  ins_pipe(istore_reg_mem);\n+%}\n+\n+instruct g1CompareAndExchangeP(iRegPNoSp res, indirect mem, iRegP oldval, iRegP newval, iRegPNoSp tmp1, iRegPNoSp tmp2, iRegPNoSp tmp3)\n+%{\n+  predicate(UseG1GC && n->as_LoadStore()->barrier_data() != 0);\n+  match(Set res (CompareAndExchangeP mem (Binary oldval newval)));\n+  effect(TEMP res, TEMP tmp1, TEMP tmp2, TEMP tmp3);\n+  ins_cost(2 * VOLATILE_REF_COST);\n+  format %{ \"cmpxchg $res = $mem, $oldval, $newval\\t# ptr\" %}\n+  ins_encode %{\n+    guarantee($mem$$disp == 0, \"impossible encoding\");\n+    assert_different_registers($oldval$$Register, $mem$$Register);\n+    assert_different_registers($newval$$Register, $mem$$Register);\n+    \/\/ Pass $oldval to the pre-barrier (instead of loading from $mem), because\n+    \/\/ $oldval is the only value that can be overwritten.\n+    \/\/ The same holds for g1CompareAndSwapP and its Acq variant.\n+    write_barrier_pre(masm, this,\n+                      noreg             \/* obj *\/,\n+                      $oldval$$Register \/* pre_val *\/,\n+                      $tmp2$$Register   \/* tmp1 *\/,\n+                      $tmp3$$Register   \/* tmp2 *\/,\n+                      RegSet::of($mem$$Register, $oldval$$Register, $newval$$Register) \/* preserve *\/,\n+                      RegSet::of($res$$Register) \/* no_preserve *\/);\n+    __ mv($tmp1$$Register, $oldval$$Register);\n+    __ mv($tmp2$$Register, $newval$$Register);\n+    __ cmpxchg($mem$$Register, $oldval$$Register, $newval$$Register, Assembler::int64,\n+               \/*acquire*\/ Assembler::relaxed, \/*release*\/ Assembler::rl, $res$$Register);\n+    write_barrier_post(masm, this,\n+                       $mem$$Register  \/* store_addr *\/,\n+                       $tmp2$$Register \/* new_val *\/,\n+                       $tmp1$$Register \/* tmp1 *\/,\n+                       $tmp3$$Register \/* tmp2 *\/);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct g1CompareAndExchangePAcq(iRegPNoSp res, indirect mem, iRegP oldval, iRegP newval, iRegPNoSp tmp1, iRegPNoSp tmp2, iRegPNoSp tmp3)\n+%{\n+  predicate(UseG1GC && needs_acquiring_load_reserved(n) && n->as_LoadStore()->barrier_data() != 0);\n+  match(Set res (CompareAndExchangeP mem (Binary oldval newval)));\n+  effect(TEMP res, TEMP tmp1, TEMP tmp2, TEMP tmp3);\n+  ins_cost(VOLATILE_REF_COST);\n+  format %{ \"cmpxchg_acq $res = $mem, $oldval, $newval\\t# ptr\" %}\n+  ins_encode %{\n+    guarantee($mem$$disp == 0, \"impossible encoding\");\n+    assert_different_registers($oldval$$Register, $mem$$Register);\n+    assert_different_registers($newval$$Register, $mem$$Register);\n+    \/\/ Pass $oldval to the pre-barrier (instead of loading from $mem), because\n+    \/\/ $oldval is the only value that can be overwritten.\n+    \/\/ The same holds for g1CompareAndSwapP and its Acq variant.\n+    write_barrier_pre(masm, this,\n+                      noreg             \/* obj *\/,\n+                      $oldval$$Register \/* pre_val *\/,\n+                      $tmp2$$Register   \/* tmp1 *\/,\n+                      $tmp3$$Register   \/* tmp2 *\/,\n+                      RegSet::of($mem$$Register, $oldval$$Register, $newval$$Register) \/* preserve *\/,\n+                      RegSet::of($res$$Register) \/* no_preserve *\/);\n+    __ mv($tmp1$$Register, $oldval$$Register);\n+    __ mv($tmp2$$Register, $newval$$Register);\n+    __ cmpxchg($mem$$Register, $oldval$$Register, $newval$$Register, Assembler::int64,\n+               \/*acquire*\/ Assembler::aq, \/*release*\/ Assembler::rl, $res$$Register);\n+    write_barrier_post(masm, this,\n+                       $mem$$Register  \/* store_addr *\/,\n+                       $tmp2$$Register \/* new_val *\/,\n+                       $tmp1$$Register \/* tmp1 *\/,\n+                       $tmp3$$Register \/* tmp2 *\/);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct g1CompareAndExchangeN(iRegNNoSp res, indirect mem, iRegN oldval, iRegN newval, iRegPNoSp tmp1, iRegPNoSp tmp2, iRegPNoSp tmp3)\n+%{\n+  predicate(UseG1GC && n->as_LoadStore()->barrier_data() != 0);\n+  match(Set res (CompareAndExchangeN mem (Binary oldval newval)));\n+  effect(TEMP res, TEMP tmp1, TEMP tmp2, TEMP tmp3);\n+  ins_cost(2 * VOLATILE_REF_COST);\n+  format %{ \"cmpxchg $res = $mem, $oldval, $newval\\t# narrow oop\" %}\n+  ins_encode %{\n+    guarantee($mem$$disp == 0, \"impossible encoding\");\n+    assert_different_registers($oldval$$Register, $mem$$Register);\n+    assert_different_registers($newval$$Register, $mem$$Register);\n+    write_barrier_pre(masm, this,\n+                      $mem$$Register  \/* obj *\/,\n+                      $tmp1$$Register \/* pre_val *\/,\n+                      $tmp2$$Register \/* tmp1 *\/,\n+                      $tmp3$$Register \/* tmp2 *\/,\n+                      RegSet::of($mem$$Register, $oldval$$Register, $newval$$Register) \/* preserve *\/,\n+                      RegSet::of($res$$Register) \/* no_preserve *\/);\n+    __ mv($tmp1$$Register, $oldval$$Register);\n+    __ mv($tmp2$$Register, $newval$$Register);\n+    __ cmpxchg($mem$$Register, $oldval$$Register, $newval$$Register, Assembler::uint32,\n+               \/*acquire*\/ Assembler::relaxed, \/*release*\/ Assembler::rl, $res$$Register);\n+    __ decode_heap_oop($tmp2$$Register);\n+    write_barrier_post(masm, this,\n+                       $mem$$Register  \/* store_addr *\/,\n+                       $tmp2$$Register \/* new_val *\/,\n+                       $tmp1$$Register \/* tmp1 *\/,\n+                       $tmp3$$Register \/* tmp2 *\/);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct g1CompareAndExchangeNAcq(iRegNNoSp res, indirect mem, iRegN oldval, iRegN newval, iRegPNoSp tmp1, iRegPNoSp tmp2, iRegPNoSp tmp3)\n+%{\n+  predicate(UseG1GC && needs_acquiring_load_reserved(n) && n->as_LoadStore()->barrier_data() != 0);\n+  match(Set res (CompareAndExchangeN mem (Binary oldval newval)));\n+  effect(TEMP res, TEMP tmp1, TEMP tmp2, TEMP tmp3);\n+  ins_cost(VOLATILE_REF_COST);\n+  format %{ \"cmpxchg_acq $res = $mem, $oldval, $newval\\t# narrow oop\" %}\n+  ins_encode %{\n+    guarantee($mem$$disp == 0, \"impossible encoding\");\n+    assert_different_registers($oldval$$Register, $mem$$Register);\n+    assert_different_registers($newval$$Register, $mem$$Register);\n+    write_barrier_pre(masm, this,\n+                      $mem$$Register  \/* obj *\/,\n+                      $tmp1$$Register \/* pre_val *\/,\n+                      $tmp2$$Register \/* tmp1 *\/,\n+                      $tmp3$$Register \/* tmp2 *\/,\n+                      RegSet::of($mem$$Register, $oldval$$Register, $newval$$Register) \/* preserve *\/,\n+                      RegSet::of($res$$Register) \/* no_preserve *\/);\n+    __ mv($tmp1$$Register, $oldval$$Register);\n+    __ mv($tmp2$$Register, $newval$$Register);\n+    __ cmpxchg($mem$$Register, $oldval$$Register, $newval$$Register, Assembler::uint32,\n+               \/*acquire*\/ Assembler::aq, \/*release*\/ Assembler::rl, $res$$Register);\n+    __ decode_heap_oop($tmp2$$Register);\n+    write_barrier_post(masm, this,\n+                       $mem$$Register  \/* store_addr *\/,\n+                       $tmp2$$Register \/* new_val *\/,\n+                       $tmp1$$Register \/* tmp1 *\/,\n+                       $tmp3$$Register \/* tmp2 *\/);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct g1CompareAndSwapP(iRegINoSp res, indirect mem, iRegP newval, iRegPNoSp tmp1, iRegPNoSp tmp2, iRegPNoSp tmp3, iRegP oldval)\n+%{\n+  predicate(UseG1GC && n->as_LoadStore()->barrier_data() != 0);\n+  match(Set res (CompareAndSwapP mem (Binary oldval newval)));\n+  match(Set res (WeakCompareAndSwapP mem (Binary oldval newval)));\n+  effect(TEMP res, TEMP tmp1, TEMP tmp2, TEMP tmp3);\n+  ins_cost(2 * VOLATILE_REF_COST);\n+  format %{ \"cmpxchg $mem, $oldval, $newval\\t# (ptr)\\n\\t\"\n+            \"mv $res, $res == $oldval\" %}\n+  ins_encode %{\n+    guarantee($mem$$disp == 0, \"impossible encoding\");\n+    assert_different_registers($oldval$$Register, $mem$$Register);\n+    assert_different_registers($newval$$Register, $mem$$Register);\n+    write_barrier_pre(masm, this,\n+                      noreg             \/* obj *\/,\n+                      $oldval$$Register \/* pre_val *\/,\n+                      $tmp2$$Register   \/* tmp1 *\/,\n+                      $tmp3$$Register   \/* tmp2 *\/,\n+                      RegSet::of($mem$$Register, $oldval$$Register, $newval$$Register) \/* preserve *\/,\n+                      RegSet::of($res$$Register) \/* no_preserve *\/);\n+    __ mv($tmp1$$Register, $oldval$$Register);\n+    __ mv($tmp2$$Register, $newval$$Register);\n+    __ cmpxchg($mem$$Register, $oldval$$Register, $newval$$Register, Assembler::int64,\n+               \/*acquire*\/ Assembler::relaxed, \/*release*\/ Assembler::rl, $res$$Register,\n+               \/*result as bool*\/ true);\n+    write_barrier_post(masm, this,\n+                       $mem$$Register  \/* store_addr *\/,\n+                       $tmp2$$Register \/* new_val *\/,\n+                       $tmp1$$Register \/* tmp1 *\/,\n+                       $tmp3$$Register \/* tmp2 *\/);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct g1CompareAndSwapPAcq(iRegINoSp res, indirect mem, iRegP newval, iRegPNoSp tmp1, iRegPNoSp tmp2, iRegPNoSp tmp3, iRegP oldval)\n+%{\n+  predicate(UseG1GC && needs_acquiring_load_reserved(n) && n->as_LoadStore()->barrier_data() != 0);\n+  match(Set res (CompareAndSwapP mem (Binary oldval newval)));\n+  match(Set res (WeakCompareAndSwapP mem (Binary oldval newval)));\n+  effect(TEMP res, TEMP tmp1, TEMP tmp2, TEMP tmp3);\n+  ins_cost(VOLATILE_REF_COST);\n+  format %{ \"cmpxchg_acq $mem, $oldval, $newval\\t# (ptr)\\n\\t\"\n+            \"mv $res, $res == $oldval\" %}\n+  ins_encode %{\n+    guarantee($mem$$disp == 0, \"impossible encoding\");\n+    assert_different_registers($oldval$$Register, $mem$$Register);\n+    assert_different_registers($newval$$Register, $mem$$Register);\n+    write_barrier_pre(masm, this,\n+                      noreg             \/* obj *\/,\n+                      $oldval$$Register \/* pre_val *\/,\n+                      $tmp2$$Register   \/* tmp1 *\/,\n+                      $tmp3$$Register   \/* tmp2 *\/,\n+                      RegSet::of($mem$$Register, $oldval$$Register, $newval$$Register) \/* preserve *\/,\n+                      RegSet::of($res$$Register) \/* no_preserve *\/);\n+    __ mv($tmp1$$Register, $oldval$$Register);\n+    __ mv($tmp2$$Register, $newval$$Register);\n+    __ cmpxchg($mem$$Register, $oldval$$Register, $newval$$Register, Assembler::int64,\n+               \/*acquire*\/ Assembler::aq, \/*release*\/ Assembler::rl, $res$$Register,\n+               \/*result as bool*\/ true);\n+    write_barrier_post(masm, this,\n+                       $mem$$Register  \/* store_addr *\/,\n+                       $tmp2$$Register \/* new_val *\/,\n+                       $tmp1$$Register \/* tmp1 *\/,\n+                       $tmp3$$Register \/* tmp2 *\/);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct g1CompareAndSwapN(iRegINoSp res, indirect mem, iRegN newval, iRegPNoSp tmp1, iRegPNoSp tmp2, iRegPNoSp tmp3, iRegN oldval)\n+%{\n+  predicate(UseG1GC && n->as_LoadStore()->barrier_data() != 0);\n+  match(Set res (CompareAndSwapN mem (Binary oldval newval)));\n+  match(Set res (WeakCompareAndSwapN mem (Binary oldval newval)));\n+  effect(TEMP res, TEMP tmp1, TEMP tmp2, TEMP tmp3);\n+  ins_cost(2 * VOLATILE_REF_COST);\n+  format %{ \"cmpxchg $mem, $oldval, $newval\\t# (narrow oop)\\n\\t\"\n+            \"mv $res, $res == $oldval\" %}\n+  ins_encode %{\n+    guarantee($mem$$disp == 0, \"impossible encoding\");\n+    assert_different_registers($oldval$$Register, $mem$$Register);\n+    assert_different_registers($newval$$Register, $mem$$Register);\n+    write_barrier_pre(masm, this,\n+                      $mem$$Register  \/* obj *\/,\n+                      $tmp1$$Register \/* pre_val *\/,\n+                      $tmp2$$Register \/* tmp1 *\/,\n+                      $tmp3$$Register \/* tmp2 *\/,\n+                      RegSet::of($mem$$Register, $oldval$$Register, $newval$$Register) \/* preserve *\/,\n+                      RegSet::of($res$$Register) \/* no_preserve *\/);\n+    __ mv($tmp1$$Register, $oldval$$Register);\n+    __ mv($tmp2$$Register, $newval$$Register);\n+    __ cmpxchg($mem$$Register, $oldval$$Register, $newval$$Register, Assembler::uint32,\n+               \/*acquire*\/ Assembler::relaxed, \/*release*\/ Assembler::rl, $res$$Register,\n+               \/*result as bool*\/ true);\n+    __ decode_heap_oop($tmp2$$Register);\n+    write_barrier_post(masm, this,\n+                       $mem$$Register  \/* store_addr *\/,\n+                       $tmp2$$Register \/* new_val *\/,\n+                       $tmp1$$Register \/* tmp1 *\/,\n+                       $tmp3$$Register \/* tmp2 *\/);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct g1CompareAndSwapNAcq(iRegINoSp res, indirect mem, iRegN newval, iRegPNoSp tmp1, iRegPNoSp tmp2, iRegPNoSp tmp3, iRegN oldval)\n+%{\n+  predicate(UseG1GC && needs_acquiring_load_reserved(n) && n->as_LoadStore()->barrier_data() != 0);\n+  match(Set res (CompareAndSwapN mem (Binary oldval newval)));\n+  match(Set res (WeakCompareAndSwapN mem (Binary oldval newval)));\n+  effect(TEMP res, TEMP tmp1, TEMP tmp2, TEMP tmp3);\n+  ins_cost(VOLATILE_REF_COST);\n+  format %{ \"cmpxchg_acq $mem, $oldval, $newval\\t# (narrow oop)\\n\\t\"\n+            \"mv $res, $res == $oldval\" %}\n+  ins_encode %{\n+    guarantee($mem$$disp == 0, \"impossible encoding\");\n+    assert_different_registers($oldval$$Register, $mem$$Register);\n+    assert_different_registers($newval$$Register, $mem$$Register);\n+    write_barrier_pre(masm, this,\n+                      $mem$$Register  \/* obj *\/,\n+                      $tmp1$$Register \/* pre_val *\/,\n+                      $tmp2$$Register \/* tmp1 *\/,\n+                      $tmp3$$Register \/* tmp2 *\/,\n+                      RegSet::of($mem$$Register, $oldval$$Register, $newval$$Register) \/* preserve *\/,\n+                      RegSet::of($res$$Register) \/* no_preserve *\/);\n+    __ mv($tmp1$$Register, $oldval$$Register);\n+    __ mv($tmp2$$Register, $newval$$Register);\n+    __ cmpxchg($mem$$Register, $oldval$$Register, $newval$$Register, Assembler::uint32,\n+              \/*acquire*\/ Assembler::aq, \/*release*\/ Assembler::rl, $res$$Register,\n+              \/*result as bool*\/ true);\n+    __ decode_heap_oop($tmp2$$Register);\n+    write_barrier_post(masm, this,\n+                       $mem$$Register  \/* store_addr *\/,\n+                       $tmp2$$Register \/* new_val *\/,\n+                       $tmp1$$Register \/* tmp1 *\/,\n+                       $tmp3$$Register \/* tmp2 *\/);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct g1GetAndSetP(indirect mem, iRegP newval, iRegPNoSp tmp1, iRegPNoSp tmp2, iRegPNoSp preval)\n+%{\n+  predicate(UseG1GC && n->as_LoadStore()->barrier_data() != 0);\n+  match(Set preval (GetAndSetP mem newval));\n+  effect(TEMP preval, TEMP tmp1, TEMP tmp2);\n+  ins_cost(2 * VOLATILE_REF_COST);\n+  format %{ \"atomic_xchg  $preval, $newval, [$mem]\" %}\n+  ins_encode %{\n+    guarantee($mem$$disp == 0, \"impossible encoding\");\n+    assert_different_registers($mem$$Register, $newval$$Register);\n+    write_barrier_pre(masm, this,\n+                      $mem$$Register    \/* obj *\/,\n+                      $preval$$Register \/* pre_val (as a temporary register) *\/,\n+                      $tmp1$$Register   \/* tmp1 *\/,\n+                      $tmp2$$Register   \/* tmp2 *\/,\n+                      RegSet::of($mem$$Register, $preval$$Register, $newval$$Register) \/* preserve *\/);\n+    __ atomic_xchg($preval$$Register, $newval$$Register, $mem$$Register);\n+    write_barrier_post(masm, this,\n+                       $mem$$Register    \/* store_addr *\/,\n+                       $newval$$Register \/* new_val *\/,\n+                       $tmp1$$Register   \/* tmp1 *\/,\n+                       $tmp2$$Register   \/* tmp2 *\/);\n+  %}\n+  ins_pipe(pipe_serial);\n+%}\n+\n+instruct g1GetAndSetPAcq(indirect mem, iRegP newval, iRegPNoSp tmp1, iRegPNoSp tmp2, iRegPNoSp preval)\n+%{\n+  predicate(UseG1GC && needs_acquiring_load_reserved(n) && n->as_LoadStore()->barrier_data() != 0);\n+  match(Set preval (GetAndSetP mem newval));\n+  effect(TEMP preval, TEMP tmp1, TEMP tmp2);\n+  ins_cost(VOLATILE_REF_COST);\n+  format %{ \"atomic_xchg_acq  $preval, $newval, [$mem]\" %}\n+  ins_encode %{\n+    guarantee($mem$$disp == 0, \"impossible encoding\");\n+    assert_different_registers($mem$$Register, $newval$$Register);\n+    write_barrier_pre(masm, this,\n+                      $mem$$Register    \/* obj *\/,\n+                      $preval$$Register \/* pre_val (as a temporary register) *\/,\n+                      $tmp1$$Register   \/* tmp1 *\/,\n+                      $tmp2$$Register   \/* tmp2 *\/,\n+                      RegSet::of($mem$$Register, $preval$$Register, $newval$$Register) \/* preserve *\/);\n+    __ atomic_xchgal($preval$$Register, $newval$$Register, $mem$$Register);\n+    write_barrier_post(masm, this,\n+                       $mem$$Register    \/* store_addr *\/,\n+                       $newval$$Register \/* new_val *\/,\n+                       $tmp1$$Register   \/* tmp1 *\/,\n+                       $tmp2$$Register   \/* tmp2 *\/);\n+  %}\n+  ins_pipe(pipe_serial);\n+%}\n+\n+instruct g1GetAndSetN(indirect mem, iRegN newval, iRegPNoSp tmp1, iRegPNoSp tmp2, iRegPNoSp tmp3, iRegNNoSp preval)\n+%{\n+  predicate(UseG1GC && n->as_LoadStore()->barrier_data() != 0);\n+  match(Set preval (GetAndSetN mem newval));\n+  effect(TEMP preval, TEMP tmp1, TEMP tmp2, TEMP tmp3);\n+  ins_cost(2 * VOLATILE_REF_COST);\n+  format %{ \"atomic_xchgwu $preval, $newval, [$mem]\" %}\n+  ins_encode %{\n+    guarantee($mem$$disp == 0, \"impossible encoding\");\n+    assert_different_registers($mem$$Register, $newval$$Register);\n+    write_barrier_pre(masm, this,\n+                      $mem$$Register  \/* obj *\/,\n+                      $tmp1$$Register \/* pre_val *\/,\n+                      $tmp2$$Register \/* tmp1 *\/,\n+                      $tmp3$$Register \/* tmp2 *\/,\n+                      RegSet::of($mem$$Register, $preval$$Register, $newval$$Register) \/* preserve *\/);\n+    __ atomic_xchgwu($preval$$Register, $newval$$Register, $mem$$Register);\n+    __ decode_heap_oop($tmp1$$Register, $newval$$Register);\n+    write_barrier_post(masm, this,\n+                       $mem$$Register  \/* store_addr *\/,\n+                       $tmp1$$Register \/* new_val *\/,\n+                       $tmp2$$Register \/* tmp1 *\/,\n+                       $tmp3$$Register \/* tmp2 *\/);\n+  %}\n+  ins_pipe(pipe_serial);\n+%}\n+\n+instruct g1GetAndSetNAcq(indirect mem, iRegN newval, iRegPNoSp tmp1, iRegPNoSp tmp2, iRegPNoSp tmp3, iRegNNoSp preval)\n+%{\n+  predicate(UseG1GC && needs_acquiring_load_reserved(n) && n->as_LoadStore()->barrier_data() != 0);\n+  match(Set preval (GetAndSetN mem newval));\n+  effect(TEMP preval, TEMP tmp1, TEMP tmp2, TEMP tmp3);\n+  ins_cost(VOLATILE_REF_COST);\n+  format %{ \"atomic_xchgwu_acq $preval, $newval, [$mem]\" %}\n+  ins_encode %{\n+    guarantee($mem$$disp == 0, \"impossible encoding\");\n+    assert_different_registers($mem$$Register, $newval$$Register);\n+    write_barrier_pre(masm, this,\n+                      $mem$$Register  \/* obj *\/,\n+                      $tmp1$$Register \/* pre_val *\/,\n+                      $tmp2$$Register \/* tmp1 *\/,\n+                      $tmp3$$Register \/* tmp2 *\/,\n+                      RegSet::of($mem$$Register, $preval$$Register, $newval$$Register) \/* preserve *\/);\n+    __ atomic_xchgalwu($preval$$Register, $newval$$Register, $mem$$Register);\n+    __ decode_heap_oop($tmp1$$Register, $newval$$Register);\n+    write_barrier_post(masm, this,\n+                       $mem$$Register  \/* store_addr *\/,\n+                       $tmp1$$Register \/* new_val *\/,\n+                       $tmp2$$Register \/* tmp1 *\/,\n+                       $tmp3$$Register \/* tmp2 *\/);\n+  %}\n+  ins_pipe(pipe_serial);\n+%}\n+\n+instruct g1LoadP(iRegPNoSp dst, indirect mem, iRegPNoSp tmp1, iRegPNoSp tmp2)\n+%{\n+  \/\/ This instruction does not need an acquiring counterpart because it is only\n+  \/\/ used for reference loading (Reference::get()). The same holds for g1LoadN.\n+  predicate(UseG1GC && n->as_Load()->barrier_data() != 0);\n+  match(Set dst (LoadP mem));\n+  effect(TEMP dst, TEMP tmp1, TEMP tmp2);\n+  ins_cost(LOAD_COST + BRANCH_COST);\n+  format %{ \"ld  $dst, $mem\\t# ptr\" %}\n+  ins_encode %{\n+    guarantee($mem$$disp == 0, \"impossible encoding\");\n+    __ ld($dst$$Register, Address($mem$$Register));\n+    write_barrier_pre(masm, this,\n+                      noreg \/* obj *\/,\n+                      $dst$$Register \/* pre_val *\/,\n+                      $tmp1$$Register \/* tmp1 *\/,\n+                      $tmp2$$Register \/* tmp2 *\/);\n+  %}\n+  ins_pipe(iload_reg_mem);\n+%}\n+\n+instruct g1LoadN(iRegNNoSp dst, indirect mem, iRegPNoSp tmp1, iRegPNoSp tmp2, iRegPNoSp tmp3)\n+%{\n+  predicate(UseG1GC && n->as_Load()->barrier_data() != 0);\n+  match(Set dst (LoadN mem));\n+  effect(TEMP dst, TEMP tmp1, TEMP tmp2, TEMP tmp3);\n+  ins_cost(VOLATILE_REF_COST);\n+  format %{ \"lwu  $dst, $mem\\t# compressed ptr\" %}\n+  ins_encode %{\n+    guarantee($mem$$disp == 0, \"impossible encoding\");\n+    __ lwu($dst$$Register, Address($mem$$Register));\n+    if ((barrier_data() & G1C2BarrierPre) != 0) {\n+      __ decode_heap_oop($tmp1$$Register, $dst$$Register);\n+      write_barrier_pre(masm, this,\n+                        noreg \/* obj *\/,\n+                        $tmp1$$Register \/* pre_val *\/,\n+                        $tmp2$$Register \/* tmp1 *\/,\n+                        $tmp3$$Register \/* tmp2 *\/);\n+    }\n+  %}\n+  ins_pipe(iload_reg_mem);\n+%}\n","filename":"src\/hotspot\/cpu\/riscv\/gc\/g1\/g1_riscv.ad","additions":582,"deletions":0,"binary":false,"changes":582,"status":"added"},{"patch":"@@ -2215,1 +2215,2 @@\n-      is_vector_scalar_bitwise_pattern(n, m)) {\n+      is_vector_scalar_bitwise_pattern(n, m) ||\n+      is_encode_and_store_pattern(n, m)) {\n@@ -4776,0 +4777,1 @@\n+  predicate(n->as_Load()->barrier_data() == 0);\n@@ -5211,0 +5213,1 @@\n+  predicate(n->as_Store()->barrier_data() == 0);\n@@ -5225,0 +5228,1 @@\n+  predicate(n->as_Store()->barrier_data() == 0);\n@@ -5415,0 +5419,1 @@\n+  predicate(n->as_LoadStore()->barrier_data() == 0);\n@@ -5536,1 +5541,1 @@\n-  predicate(needs_acquiring_load_reserved(n));\n+  predicate(needs_acquiring_load_reserved(n) && n->as_LoadStore()->barrier_data() == 0);\n@@ -5644,0 +5649,1 @@\n+  predicate(n->as_LoadStore()->barrier_data() == 0);\n@@ -5777,1 +5783,1 @@\n-  predicate(needs_acquiring_load_reserved(n));\n+  predicate(needs_acquiring_load_reserved(n) && n->as_LoadStore()->barrier_data() == 0);\n@@ -5905,0 +5911,1 @@\n+  predicate(n->as_LoadStore()->barrier_data() == 0);\n@@ -6036,1 +6043,1 @@\n-  predicate(needs_acquiring_load_reserved(n));\n+  predicate(needs_acquiring_load_reserved(n) && n->as_LoadStore()->barrier_data() == 0);\n@@ -6108,0 +6115,2 @@\n+  predicate(n->as_LoadStore()->barrier_data() == 0);\n+\n@@ -6173,1 +6182,1 @@\n-  predicate(needs_acquiring_load_reserved(n));\n+  predicate(needs_acquiring_load_reserved(n) && n->as_LoadStore()->barrier_data() == 0);\n","filename":"src\/hotspot\/cpu\/riscv\/riscv.ad","additions":14,"deletions":5,"binary":false,"changes":19,"status":"modified"},{"patch":"@@ -2,2 +2,2 @@\n- * Copyright (c) 2019, 2023, Oracle and\/or its affiliates. All rights reserved.\n- * Copyright (c) 2018, 2023 SAP SE. All rights reserved.\n+ * Copyright (c) 2019, 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2018, 2024 SAP SE. All rights reserved.\n@@ -45,1 +45,4 @@\n-#endif\n+#endif \/\/ COMPILER1\n+#ifdef COMPILER2\n+#include \"gc\/g1\/c2\/g1BarrierSetC2.hpp\"\n+#endif \/\/ COMPILER2\n@@ -49,1 +52,34 @@\n-#define BLOCK_COMMENT(str) if (PrintAssembly) __ block_comment(str)\n+#define BLOCK_COMMENT(str) __ block_comment(str)\n+\n+static void generate_pre_barrier_fast_path(MacroAssembler* masm,\n+                                           const Register thread,\n+                                           const Register tmp1) {\n+  Address in_progress(thread, in_bytes(G1ThreadLocalData::satb_mark_queue_active_offset()));\n+  \/\/ Is marking active?\n+  if (in_bytes(SATBMarkQueue::byte_width_of_active()) == 4) {\n+    __ load_and_test_int(tmp1, in_progress);\n+  } else {\n+    assert(in_bytes(SATBMarkQueue::byte_width_of_active()) == 1, \"Assumption\");\n+    __ load_and_test_byte(tmp1, in_progress);\n+  }\n+}\n+\n+static void generate_queue_test_and_insertion(MacroAssembler* masm, ByteSize index_offset, ByteSize buffer_offset, Label& runtime,\n+                                              const Register Z_thread, const Register value, const Register temp) {\n+  BLOCK_COMMENT(\"generate_queue_test_and_insertion {\");\n+\n+  assert_different_registers(temp, value);\n+  \/\/ Can we store a value in the given thread's buffer?\n+  \/\/ (The index field is typed as size_t.)\n+\n+  __ load_and_test_long(temp, Address(Z_thread, in_bytes(index_offset))); \/\/ temp := *(index address)\n+  __ branch_optimized(Assembler::bcondEqual, runtime);                    \/\/ jump to runtime if index == 0 (full buffer)\n+\n+  \/\/ The buffer is not full, store value into it.\n+  __ add2reg(temp, -wordSize);                                            \/\/ temp := next index\n+  __ z_stg(temp, in_bytes(index_offset), Z_thread);                       \/\/ *(index address) := next index\n+\n+  __ z_ag(temp, Address(Z_thread, in_bytes(buffer_offset)));              \/\/ temp := buffer address + next index\n+  __ z_stg(value, 0, temp);                                               \/\/ *(buffer address + next index) := value\n+  BLOCK_COMMENT(\"} generate_queue_test_and_insertion\");\n+}\n@@ -62,7 +98,2 @@\n-    const int active_offset = in_bytes(G1ThreadLocalData::satb_mark_queue_active_offset());\n-    if (in_bytes(SATBMarkQueue::byte_width_of_active()) == 4) {\n-      __ load_and_test_int(Rtmp1, Address(Z_thread, active_offset));\n-    } else {\n-      guarantee(in_bytes(SATBMarkQueue::byte_width_of_active()) == 1, \"Assumption\");\n-      __ load_and_test_byte(Rtmp1, Address(Z_thread, active_offset));\n-    }\n+\n+    generate_pre_barrier_fast_path(masm, Z_thread, Rtmp1);\n@@ -103,0 +134,175 @@\n+#if defined(COMPILER2)\n+\n+#undef __\n+#define __ masm->\n+\n+static void generate_c2_barrier_runtime_call(MacroAssembler* masm, G1BarrierStubC2* stub, const Register pre_val, const address runtime_path) {\n+  BLOCK_COMMENT(\"generate_c2_barrier_runtime_call {\");\n+  SaveLiveRegisters save_registers(masm, stub);\n+  __ call_VM_leaf(runtime_path, pre_val, Z_thread);\n+  BLOCK_COMMENT(\"} generate_c2_barrier_runtime_call\");\n+}\n+\n+void G1BarrierSetAssembler::g1_write_barrier_pre_c2(MacroAssembler* masm,\n+                                                    Register obj,\n+                                                    Register pre_val,\n+                                                    Register thread,\n+                                                    Register tmp1,\n+                                                    G1PreBarrierStubC2* stub) {\n+\n+  BLOCK_COMMENT(\"g1_write_barrier_pre_c2 {\");\n+\n+  assert(thread == Z_thread, \"must be\");\n+  assert_different_registers(obj, pre_val, tmp1);\n+  assert(pre_val != noreg && tmp1 != noreg, \"expecting a register\");\n+\n+  stub->initialize_registers(obj, pre_val, thread, tmp1, noreg);\n+\n+  generate_pre_barrier_fast_path(masm, thread, tmp1);\n+  __ branch_optimized(Assembler::bcondNotEqual, *stub->entry()); \/\/ Activity indicator is zero, so there is no marking going on currently.\n+\n+  __ bind(*stub->continuation());\n+\n+  BLOCK_COMMENT(\"} g1_write_barrier_pre_c2\");\n+}\n+\n+void G1BarrierSetAssembler::generate_c2_pre_barrier_stub(MacroAssembler* masm,\n+                                                         G1PreBarrierStubC2* stub) const {\n+\n+  BLOCK_COMMENT(\"generate_c2_pre_barrier_stub {\");\n+\n+  Assembler::InlineSkippedInstructionsCounter skip_counter(masm);\n+\n+  Label runtime;\n+  Register obj     = stub->obj();\n+  Register pre_val = stub->pre_val();\n+  Register thread  = stub->thread();\n+  Register tmp1    = stub->tmp1();\n+\n+  __ bind(*stub->entry());\n+\n+  BLOCK_COMMENT(\"generate_pre_val_not_null_test {\");\n+  if (obj != noreg) {\n+    __ load_heap_oop(pre_val, Address(obj), noreg, noreg, AS_RAW);\n+  }\n+  __ z_ltgr(pre_val, pre_val);\n+  __ branch_optimized(Assembler::bcondEqual, *stub->continuation());\n+  BLOCK_COMMENT(\"} generate_pre_val_not_null_test\");\n+\n+  generate_queue_test_and_insertion(masm,\n+                                    G1ThreadLocalData::satb_mark_queue_index_offset(),\n+                                    G1ThreadLocalData::satb_mark_queue_buffer_offset(),\n+                                    runtime,\n+                                    Z_thread, pre_val, tmp1);\n+\n+  __ branch_optimized(Assembler::bcondAlways, *stub->continuation());\n+\n+  __ bind(runtime);\n+\n+  generate_c2_barrier_runtime_call(masm, stub, pre_val, CAST_FROM_FN_PTR(address, G1BarrierSetRuntime::write_ref_field_pre_entry));\n+\n+  __ branch_optimized(Assembler::bcondAlways, *stub->continuation());\n+\n+  BLOCK_COMMENT(\"} generate_c2_pre_barrier_stub\");\n+}\n+\n+void G1BarrierSetAssembler::g1_write_barrier_post_c2(MacroAssembler* masm,\n+                                                     Register store_addr,\n+                                                     Register new_val,\n+                                                     Register thread,\n+                                                     Register tmp1,\n+                                                     Register tmp2,\n+                                                     G1PostBarrierStubC2* stub) {\n+  BLOCK_COMMENT(\"g1_write_barrier_post_c2 {\");\n+\n+  assert(thread == Z_thread, \"must be\");\n+  assert_different_registers(store_addr, new_val, thread, tmp1, tmp2, Z_R1_scratch);\n+\n+  assert(store_addr != noreg && new_val != noreg && tmp1 != noreg && tmp2 != noreg, \"expecting a register\");\n+\n+  stub->initialize_registers(thread, tmp1, tmp2);\n+\n+  BLOCK_COMMENT(\"generate_region_crossing_test {\");\n+  if (VM_Version::has_DistinctOpnds()) {\n+    __ z_xgrk(tmp1, store_addr, new_val);\n+  } else {\n+    __ z_lgr(tmp1, store_addr);\n+    __ z_xgr(tmp1, new_val);\n+  }\n+  __ z_srag(tmp1, tmp1, G1HeapRegion::LogOfHRGrainBytes);\n+  __ branch_optimized(Assembler::bcondEqual, *stub->continuation());\n+  BLOCK_COMMENT(\"} generate_region_crossing_test\");\n+\n+  \/\/ crosses regions, storing null?\n+  if ((stub->barrier_data() & G1C2BarrierPostNotNull) == 0) {\n+    __ z_ltgr(new_val, new_val);\n+    __ branch_optimized(Assembler::bcondEqual, *stub->continuation());\n+  }\n+\n+  BLOCK_COMMENT(\"generate_card_young_test {\");\n+  CardTableBarrierSet* ct = barrier_set_cast<CardTableBarrierSet>(BarrierSet::barrier_set());\n+  \/\/ calculate address of card\n+  __ load_const_optimized(tmp2, (address)ct->card_table()->byte_map_base());      \/\/ Card table base.\n+  __ z_srlg(tmp1, store_addr, CardTable::card_shift());         \/\/ Index into card table.\n+  __ z_algr(tmp1, tmp2);                                      \/\/ Explicit calculation needed for cli.\n+\n+  \/\/ Filter young.\n+  __ z_cli(0, tmp1, G1CardTable::g1_young_card_val());\n+\n+  BLOCK_COMMENT(\"} generate_card_young_test\");\n+\n+  \/\/ From here on, tmp1 holds the card address.\n+  __ branch_optimized(Assembler::bcondNotEqual, *stub->entry());\n+\n+  __ bind(*stub->continuation());\n+\n+  BLOCK_COMMENT(\"} g1_write_barrier_post_c2\");\n+}\n+\n+void G1BarrierSetAssembler::generate_c2_post_barrier_stub(MacroAssembler* masm,\n+                                                          G1PostBarrierStubC2* stub) const {\n+\n+  BLOCK_COMMENT(\"generate_c2_post_barrier_stub {\");\n+\n+  Assembler::InlineSkippedInstructionsCounter skip_counter(masm);\n+  Label runtime;\n+\n+  Register thread     = stub->thread();\n+  Register tmp1       = stub->tmp1(); \/\/ tmp1 holds the card address.\n+  Register tmp2       = stub->tmp2();\n+  Register Rcard_addr = tmp1;\n+\n+  __ bind(*stub->entry());\n+\n+  BLOCK_COMMENT(\"generate_card_clean_test {\");\n+  __ z_sync(); \/\/ Required to support concurrent cleaning.\n+  __ z_cli(0, Rcard_addr, 0); \/\/ Reload after membar.\n+  __ branch_optimized(Assembler::bcondEqual, *stub->continuation());\n+  BLOCK_COMMENT(\"} generate_card_clean_test\");\n+\n+  BLOCK_COMMENT(\"generate_dirty_card {\");\n+  \/\/ Storing a region crossing, non-null oop, card is clean.\n+  \/\/ Dirty card and log.\n+  STATIC_ASSERT(CardTable::dirty_card_val() == 0);\n+  __ z_mvi(0, Rcard_addr, CardTable::dirty_card_val());\n+  BLOCK_COMMENT(\"} generate_dirty_card\");\n+\n+  generate_queue_test_and_insertion(masm,\n+                                    G1ThreadLocalData::dirty_card_queue_index_offset(),\n+                                    G1ThreadLocalData::dirty_card_queue_buffer_offset(),\n+                                    runtime,\n+                                    Z_thread, tmp1, tmp2);\n+\n+  __ branch_optimized(Assembler::bcondAlways, *stub->continuation());\n+\n+  __ bind(runtime);\n+\n+  generate_c2_barrier_runtime_call(masm, stub, tmp1, CAST_FROM_FN_PTR(address, G1BarrierSetRuntime::write_ref_field_post_entry));\n+\n+  __ branch_optimized(Assembler::bcondAlways, *stub->continuation());\n+\n+  BLOCK_COMMENT(\"} generate_c2_post_barrier_stub\");\n+}\n+\n+#endif \/\/COMPILER2\n+\n@@ -139,3 +345,0 @@\n-  const int active_offset = in_bytes(G1ThreadLocalData::satb_mark_queue_active_offset());\n-  const int buffer_offset = in_bytes(G1ThreadLocalData::satb_mark_queue_buffer_offset());\n-  const int index_offset  = in_bytes(G1ThreadLocalData::satb_mark_queue_index_offset());\n@@ -150,8 +353,1 @@\n-  \/\/ Is marking active?\n-  \/\/ Note: value is loaded for test purposes only. No further use here.\n-  if (in_bytes(SATBMarkQueue::byte_width_of_active()) == 4) {\n-    __ load_and_test_int(Rtmp1, Address(Z_thread, active_offset));\n-  } else {\n-    guarantee(in_bytes(SATBMarkQueue::byte_width_of_active()) == 1, \"Assumption\");\n-    __ load_and_test_byte(Rtmp1, Address(Z_thread, active_offset));\n-  }\n+  generate_pre_barrier_fast_path(masm, Z_thread, Rtmp1);\n@@ -197,7 +393,0 @@\n-  Register Rbuffer = Rtmp1, Rindex = Rtmp2;\n-  assert_different_registers(Rbuffer, Rindex, Rpre_val);\n-\n-  __ z_lg(Rbuffer, buffer_offset, Z_thread);\n-\n-  __ load_and_test_long(Rindex, Address(Z_thread, index_offset));\n-  __ z_bre(callRuntime); \/\/ If index == 0, goto runtime.\n@@ -205,5 +394,5 @@\n-  __ add2reg(Rindex, -wordSize); \/\/ Decrement index.\n-  __ z_stg(Rindex, index_offset, Z_thread);\n-\n-  \/\/ Record the previous value.\n-  __ z_stg(Rpre_val, 0, Rbuffer, Rindex);\n+  generate_queue_test_and_insertion(masm,\n+                                    G1ThreadLocalData::satb_mark_queue_index_offset(),\n+                                    G1ThreadLocalData::satb_mark_queue_buffer_offset(),\n+                                    callRuntime,\n+                                    Z_thread, Rpre_val, Rtmp2);\n@@ -212,3 +401,0 @@\n-  Rbuffer = noreg;  \/\/ end of life\n-  Rindex  = noreg;  \/\/ end of life\n-\n@@ -329,4 +515,1 @@\n-  Register Rqueue_buf   = (Rtmp3 != Z_R0_scratch) ? Rtmp3 : Rtmp1;\n-  const int qidx_off    = in_bytes(G1ThreadLocalData::dirty_card_queue_index_offset());\n-  const int qbuf_off    = in_bytes(G1ThreadLocalData::dirty_card_queue_buffer_offset());\n-  if ((Rcard_addr == Rqueue_buf) || (Rcard_addr == Rqueue_index)) {\n+  if (Rcard_addr == Rqueue_index) {\n@@ -337,9 +520,5 @@\n-  __ load_and_test_long(Rqueue_index, Address(Z_thread, qidx_off));\n-  __ z_bre(callRuntime); \/\/ Index == 0 then jump to runtime.\n-\n-  __ z_lg(Rqueue_buf, qbuf_off, Z_thread);\n-\n-  __ add2reg(Rqueue_index, -wordSize); \/\/ Decrement index.\n-  __ z_stg(Rqueue_index, qidx_off, Z_thread);\n-\n-  __ z_stg(Rcard_addr_x, 0, Rqueue_index, Rqueue_buf); \/\/ Store card.\n+  generate_queue_test_and_insertion(masm,\n+                                    G1ThreadLocalData::dirty_card_queue_index_offset(),\n+                                    G1ThreadLocalData::dirty_card_queue_buffer_offset(),\n+                                    callRuntime,\n+                                    Z_thread, Rcard_addr_x, Rqueue_index);\n","filename":"src\/hotspot\/cpu\/s390\/gc\/g1\/g1BarrierSetAssembler_s390.cpp","additions":229,"deletions":50,"binary":false,"changes":279,"status":"modified"},{"patch":"@@ -2,2 +2,2 @@\n- * Copyright (c) 2018, 2023, Oracle and\/or its affiliates. All rights reserved.\n- * Copyright (c) 2018 SAP SE. All rights reserved.\n+ * Copyright (c) 2018, 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2018, 2024 SAP SE. All rights reserved.\n@@ -37,0 +37,2 @@\n+class G1PreBarrierStubC2;\n+class G1PostBarrierStubC2;\n@@ -65,1 +67,21 @@\n-#endif\n+#endif \/\/ COMPILER1\n+\n+#ifdef COMPILER2\n+  void g1_write_barrier_pre_c2(MacroAssembler* masm,\n+                               Register obj,\n+                               Register pre_val,\n+                               Register thread,\n+                               Register tmp1,\n+                               G1PreBarrierStubC2* c2_stub);\n+  void generate_c2_pre_barrier_stub(MacroAssembler* masm,\n+                                    G1PreBarrierStubC2* stub) const;\n+  void g1_write_barrier_post_c2(MacroAssembler* masm,\n+                                Register store_addr,\n+                                Register new_val,\n+                                Register thread,\n+                                Register tmp1,\n+                                Register tmp2,\n+                                G1PostBarrierStubC2* c2_stub);\n+  void generate_c2_post_barrier_stub(MacroAssembler* masm,\n+                                     G1PostBarrierStubC2* stub) const;\n+#endif \/\/ COMPILER2\n","filename":"src\/hotspot\/cpu\/s390\/gc\/g1\/g1BarrierSetAssembler_s390.hpp","additions":25,"deletions":3,"binary":false,"changes":28,"status":"modified"},{"patch":"@@ -0,0 +1,457 @@\n+\/\/\n+\/\/ Copyright (c) 2024, Oracle and\/or its affiliates. All rights reserved.\n+\/\/ Copyright 2024 IBM Corporation. All rights reserved.\n+\/\/ DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+\/\/\n+\/\/ This code is free software; you can redistribute it and\/or modify it\n+\/\/ under the terms of the GNU General Public License version 2 only, as\n+\/\/ published by the Free Software Foundation.\n+\/\/\n+\/\/ This code is distributed in the hope that it will be useful, but WITHOUT\n+\/\/ ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+\/\/ FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+\/\/ version 2 for more details (a copy is included in the LICENSE file that\n+\/\/ accompanied this code).\n+\/\/\n+\/\/ You should have received a copy of the GNU General Public License version\n+\/\/ 2 along with this work; if not, write to the Free Software Foundation,\n+\/\/ Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+\/\/\n+\/\/ Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+\/\/ or visit www.oracle.com if you need additional information or have any\n+\/\/ questions.\n+\/\/\n+\n+source_hpp %{\n+\n+#include \"gc\/g1\/c2\/g1BarrierSetC2.hpp\"\n+#include \"gc\/shared\/gc_globals.hpp\"\n+\n+%}\n+\n+source %{\n+\n+#include \"gc\/g1\/g1BarrierSetAssembler_s390.hpp\"\n+#include \"gc\/g1\/g1BarrierSetRuntime.hpp\"\n+\n+static void write_barrier_pre(MacroAssembler* masm,\n+                              const MachNode* node,\n+                              Register obj,\n+                              Register pre_val,\n+                              Register tmp1,\n+                              RegSet preserve = RegSet(),\n+                              RegSet no_preserve = RegSet()) {\n+  if (!G1PreBarrierStubC2::needs_barrier(node)) {\n+    return;\n+  }\n+  Assembler::InlineSkippedInstructionsCounter skip_counter(masm);\n+  G1BarrierSetAssembler* g1_asm = static_cast<G1BarrierSetAssembler*>(BarrierSet::barrier_set()->barrier_set_assembler());\n+  G1PreBarrierStubC2* const stub = G1PreBarrierStubC2::create(node);\n+  for (RegSetIterator<Register> reg = preserve.begin(); *reg != noreg; ++reg) {\n+    stub->preserve(*reg);\n+  }\n+  for (RegSetIterator<Register> reg = no_preserve.begin(); *reg != noreg; ++reg) {\n+    stub->dont_preserve(*reg);\n+  }\n+  g1_asm->g1_write_barrier_pre_c2(masm, obj, pre_val, Z_thread, tmp1, stub);\n+}\n+\n+static void write_barrier_post(MacroAssembler* masm,\n+                               const MachNode* node,\n+                               Register store_addr,\n+                               Register new_val,\n+                               Register tmp1,\n+                               Register tmp2) {\n+  if (!G1PostBarrierStubC2::needs_barrier(node)) {\n+    return;\n+  }\n+  Assembler::InlineSkippedInstructionsCounter skip_counter(masm);\n+  G1BarrierSetAssembler* g1_asm = static_cast<G1BarrierSetAssembler*>(BarrierSet::barrier_set()->barrier_set_assembler());\n+  G1PostBarrierStubC2* const stub = G1PostBarrierStubC2::create(node);\n+  g1_asm->g1_write_barrier_post_c2(masm, store_addr, new_val, Z_thread, tmp1, tmp2, stub);\n+}\n+\n+%} \/\/ source\n+\n+\/\/ store pointer\n+instruct g1StoreP(indirect dst, memoryRegP src, iRegL tmp1, iRegL tmp2, flagsReg cr) %{\n+  predicate(UseG1GC && n->as_Store()->barrier_data() != 0);\n+  match(Set dst (StoreP dst src));\n+  effect(TEMP tmp1, TEMP tmp2, KILL cr);\n+  ins_cost(MEMORY_REF_COST);\n+  format %{ \"STG     $src,$dst\\t # ptr\" %}\n+  ins_encode %{\n+    __ block_comment(\"g1StoreP {\");\n+    write_barrier_pre(masm, this,\n+                      $dst$$Register  \/* obj     *\/,\n+                      $tmp1$$Register \/* pre_val *\/,\n+                      $tmp2$$Register \/* tmp1    *\/,\n+                      RegSet::of($dst$$Register, $src$$Register) \/* preserve *\/);\n+\n+    __ z_stg($src$$Register, Address($dst$$Register));\n+\n+    write_barrier_post(masm, this,\n+                       $dst$$Register, \/* store_addr *\/\n+                       $src$$Register  \/* new_val    *\/,\n+                       $tmp1$$Register \/* tmp1       *\/,\n+                       $tmp2$$Register \/* tmp2       *\/);\n+    __ block_comment(\"} g1StoreP\");\n+  %}\n+  ins_pipe(pipe_class_dummy);\n+%}\n+\n+\/\/ Store Compressed Pointer\n+instruct g1StoreN(indirect mem, iRegN_P2N src, iRegL tmp1, iRegL tmp2, iRegL tmp3, flagsReg cr) %{\n+  predicate(UseG1GC && n->as_Store()->barrier_data() != 0);\n+  match(Set mem (StoreN mem src));\n+  effect(TEMP tmp1, TEMP tmp2, TEMP tmp3, KILL cr);\n+  ins_cost(MEMORY_REF_COST);\n+  format %{ \"STY     $src,$mem\\t # (cOop)\" %}\n+  ins_encode %{\n+    __ block_comment(\"g1StoreN {\");\n+    write_barrier_pre(masm, this,\n+                      $mem$$Register  \/* obj     *\/,\n+                      $tmp1$$Register \/* pre_val *\/,\n+                      $tmp2$$Register \/* tmp1    *\/,\n+                      RegSet::of($mem$$Register, $src$$Register) \/* preserve *\/);\n+\n+    __ z_sty($src$$Register, Address($mem$$Register));\n+\n+    if ((barrier_data() & G1C2BarrierPost) != 0) {\n+      if ((barrier_data() & G1C2BarrierPostNotNull) == 0) {\n+        __ oop_decoder($tmp1$$Register, $src$$Register, true \/* maybe_null *\/);\n+      } else {\n+        __ oop_decoder($tmp1$$Register, $src$$Register, false \/* maybe_null *\/);\n+      }\n+    }\n+\n+    write_barrier_post(masm, this,\n+                       $mem$$Register  \/* store_addr *\/,\n+                       $tmp1$$Register \/* new_val    *\/,\n+                       $tmp2$$Register \/* tmp1       *\/,\n+                       $tmp3$$Register \/* tmp2       *\/);\n+    __ block_comment(\"} g1StoreN\");\n+  %}\n+\n+  ins_pipe(pipe_class_dummy);\n+%}\n+\n+instruct g1CompareAndSwapN(indirect mem_ptr, rarg5RegN oldval, iRegN_P2N newval, iRegI res, iRegL tmp1, iRegL tmp2, iRegL tmp3, flagsReg cr) %{\n+  predicate(UseG1GC && n->as_LoadStore()->barrier_data() != 0);\n+  match(Set res (CompareAndSwapN mem_ptr (Binary oldval newval)));\n+  match(Set res (WeakCompareAndSwapN mem_ptr (Binary oldval newval)));\n+  effect(USE mem_ptr, TEMP res, TEMP tmp1, TEMP tmp2, TEMP tmp3, USE_KILL oldval, KILL cr);\n+  format %{ \"$res = CompareAndSwapN $oldval,$newval,$mem_ptr\" %}\n+  ins_encode %{\n+    assert_different_registers($oldval$$Register, $mem_ptr$$Register);\n+    assert_different_registers($newval$$Register, $mem_ptr$$Register);\n+    __ block_comment(\"g1compareAndSwapN {\");\n+\n+    Register Rcomp = reg_to_register_object($oldval$$reg);\n+    Register Rnew  = reg_to_register_object($newval$$reg);\n+    Register Raddr = reg_to_register_object($mem_ptr$$reg);\n+    Register Rres  = reg_to_register_object($res$$reg);\n+\n+    write_barrier_pre(masm, this,\n+                      Raddr           \/* obj     *\/,\n+                      $tmp1$$Register \/* pre_val *\/,\n+                      $tmp2$$Register \/* tmp1    *\/,\n+                      RegSet::of(Raddr, Rcomp, Rnew) \/* preserve *\/,\n+                      RegSet::of(Rres) \/* no_preserve *\/);\n+\n+    __ z_cs(Rcomp, Rnew, 0, Raddr);\n+\n+    assert_different_registers(Rres, Raddr);\n+    if (VM_Version::has_LoadStoreConditional()) {\n+      __ load_const_optimized(Z_R0_scratch, 0L); \/\/ false (failed)\n+      __ load_const_optimized(Rres, 1L);         \/\/ true  (succeed)\n+      __ z_locgr(Rres, Z_R0_scratch, Assembler::bcondNotEqual);\n+    } else {\n+      Label done;\n+      __ load_const_optimized(Rres, 0L); \/\/ false (failed)\n+      __ z_brne(done);                   \/\/ Assume true to be the common case.\n+      __ load_const_optimized(Rres, 1L); \/\/ true  (succeed)\n+      __ bind(done);\n+    }\n+\n+    __ oop_decoder($tmp3$$Register, Rnew, true \/* maybe_null *\/);\n+\n+    write_barrier_post(masm, this,\n+                       Raddr            \/* store_addr *\/,\n+                       $tmp3$$Register  \/* new_val    *\/,\n+                       $tmp1$$Register  \/* tmp1       *\/,\n+                       $tmp2$$Register  \/* tmp2       *\/);\n+    __ block_comment(\"} g1compareAndSwapN\");\n+  %}\n+  ins_pipe(pipe_class_dummy);\n+%}\n+\n+instruct g1CompareAndExchangeN(iRegP mem_ptr, rarg5RegN oldval, iRegN_P2N newval, iRegN res, iRegL tmp1, iRegL tmp2, iRegL tmp3, flagsReg cr) %{\n+  predicate(UseG1GC && n->as_LoadStore()->barrier_data() != 0);\n+  match(Set res (CompareAndExchangeN mem_ptr (Binary oldval newval)));\n+  effect(USE mem_ptr, TEMP res, TEMP tmp1, TEMP tmp2, TEMP tmp3, USE_KILL oldval, KILL cr);\n+  format %{ \"$res = CompareAndExchangeN $oldval,$newval,$mem_ptr\" %}\n+  ins_encode %{\n+    assert_different_registers($oldval$$Register, $mem_ptr$$Register);\n+    assert_different_registers($newval$$Register, $mem_ptr$$Register);\n+    __ block_comment(\"g1CompareAndExchangeN {\");\n+    write_barrier_pre(masm, this,\n+                      $mem_ptr$$Register \/* obj     *\/,\n+                      $tmp1$$Register    \/* pre_val *\/,\n+                      $tmp2$$Register    \/* tmp1    *\/,\n+                      RegSet::of($mem_ptr$$Register, $oldval$$Register, $newval$$Register) \/* preserve *\/,\n+                      RegSet::of($res$$Register) \/* no_preserve *\/);\n+\n+    Register Rcomp = reg_to_register_object($oldval$$reg);\n+    Register Rnew  = reg_to_register_object($newval$$reg);\n+    Register Raddr = reg_to_register_object($mem_ptr$$reg);\n+\n+    Register Rres = reg_to_register_object($res$$reg);\n+    assert_different_registers(Rres, Raddr);\n+\n+    __ z_lgr(Rres, Rcomp);  \/\/ previous contents\n+    __ z_csy(Rres, Rnew, 0, Raddr); \/\/ Try to store new value.\n+\n+    __ oop_decoder($tmp1$$Register, Rnew, true \/* maybe_null *\/);\n+\n+    write_barrier_post(masm, this,\n+                       Raddr           \/* store_addr *\/,\n+                       $tmp1$$Register \/* new_val    *\/,\n+                       $tmp2$$Register \/* tmp1       *\/,\n+                       $tmp3$$Register \/* tmp2       *\/);\n+    __ block_comment(\"} g1CompareAndExchangeN\");\n+  %}\n+  ins_pipe(pipe_class_dummy);\n+%}\n+\n+\/\/ Load narrow oop\n+instruct g1LoadN(iRegN dst, indirect mem, iRegP tmp1, iRegP tmp2, flagsReg cr) %{\n+  predicate(UseG1GC && n->as_Load()->barrier_data() != 0);\n+  match(Set dst (LoadN mem));\n+  effect(TEMP dst, TEMP tmp1, TEMP tmp2, KILL cr);\n+  ins_cost(MEMORY_REF_COST);\n+  format %{ \"LoadN   $dst,$mem\\t # (cOop)\" %}\n+  ins_encode %{\n+    __ block_comment(\"g1LoadN {\");\n+    __ z_llgf($dst$$Register, Address($mem$$Register));\n+    if ((barrier_data() & G1C2BarrierPre) != 0) {\n+      __ oop_decoder($tmp1$$Register, $dst$$Register, true);\n+      write_barrier_pre(masm, this,\n+                        noreg           \/* obj     *\/,\n+                        $tmp1$$Register \/* pre_val *\/,\n+                        $tmp2$$Register );\n+    }\n+    __ block_comment(\"} g1LoadN\");\n+  %}\n+  ins_pipe(pipe_class_dummy);\n+%}\n+\n+instruct g1GetAndSetN(indirect mem, iRegN dst, iRegI tmp, iRegL tmp1, iRegL tmp2, iRegL tmp3, flagsReg cr) %{\n+  predicate(UseG1GC && n->as_LoadStore()->barrier_data() != 0);\n+  match(Set dst (GetAndSetN mem dst));\n+  effect(KILL cr, TEMP tmp, TEMP tmp1, TEMP tmp2, TEMP tmp3); \/\/ USE_DEF dst by match rule.\n+  format %{ \"XCHGN   $dst,[$mem]\\t # EXCHANGE (coop, atomic), temp $tmp\" %}\n+  ins_encode %{\n+    __ block_comment(\"g1GetAndSetN {\");\n+    assert_different_registers($mem$$Register, $dst$$Register);\n+    write_barrier_pre(masm, this,\n+                      $mem$$Register  \/* obj     *\/,\n+                      $tmp1$$Register \/* pre_val *\/,\n+                      $tmp2$$Register \/* tmp1    *\/,\n+                      RegSet::of($mem$$Register, $dst$$Register) \/* preserve *\/);\n+\n+    Register Rdst = reg_to_register_object($dst$$reg);\n+    Register Rtmp = reg_to_register_object($tmp$$reg);\n+    guarantee(Rdst != Rtmp, \"Fix match rule to use TEMP_DEF\");\n+    Label    retry;\n+\n+    \/\/ Iterate until swap succeeds.\n+    __ z_llgf(Rtmp, Address($mem$$Register)); \/\/ current contents\n+    __ bind(retry);\n+    \/\/ Calculate incremented value.\n+    __ z_csy(Rtmp, Rdst, Address($mem$$Register)); \/\/ Try to store new value.\n+    __ z_brne(retry); \/\/ Yikes, concurrent update, need to retry.\n+\n+    __ oop_decoder($tmp1$$Register, $dst$$Register, true \/* maybe_null *\/);\n+\n+    __ z_lgr(Rdst, Rtmp);  \/\/ Exchanged value from memory is return value.\n+\n+    write_barrier_post(masm, this,\n+                       $mem$$Register  \/* store_addr *\/,\n+                       $tmp1$$Register \/* new_val    *\/,\n+                       $tmp2$$Register \/* tmp1       *\/,\n+                       $tmp3$$Register \/* tmp2       *\/);\n+\n+    __ block_comment(\"} g1GetAndSetN\");\n+  %}\n+  ins_pipe(pipe_class_dummy);\n+%}\n+\n+instruct g1CompareAndSwapP(iRegP mem_ptr, rarg5RegP oldval, iRegP_N2P newval, iRegI res, iRegL tmp1, iRegL tmp2, flagsReg cr) %{\n+  predicate(UseG1GC && n->as_LoadStore()->barrier_data() != 0);\n+  match(Set res (CompareAndSwapP mem_ptr (Binary oldval newval)));\n+  match(Set res (WeakCompareAndSwapP mem_ptr (Binary oldval newval)));\n+  effect(TEMP res, TEMP tmp1, TEMP tmp2, USE mem_ptr, USE_KILL oldval, KILL cr);\n+  format %{ \"$res = CompareAndSwapP $oldval,$newval,$mem_ptr\" %}\n+  ins_encode %{\n+    __ block_comment(\"g1CompareAndSwapP {\");\n+    assert_different_registers($oldval$$Register, $mem_ptr$$Register);\n+    assert_different_registers($newval$$Register, $mem_ptr$$Register);\n+\n+    Register Rcomp = reg_to_register_object($oldval$$reg);\n+    Register Rnew  = reg_to_register_object($newval$$reg);\n+    Register Raddr = reg_to_register_object($mem_ptr$$reg);\n+    Register Rres  = reg_to_register_object($res$$reg);\n+\n+    write_barrier_pre(masm, this,\n+                      noreg           \/* obj     *\/,\n+                      Rcomp           \/* pre_val *\/,\n+                      $tmp1$$Register \/* tmp1    *\/,\n+                      RegSet::of(Raddr, Rcomp, Rnew) \/* preserve *\/,\n+                      RegSet::of(Rres) \/* no_preserve *\/);\n+\n+    __ z_csg(Rcomp, Rnew, 0, Raddr);\n+\n+    if (VM_Version::has_LoadStoreConditional()) {\n+      __ load_const_optimized(Z_R0_scratch, 0L); \/\/ false (failed)\n+      __ load_const_optimized(Rres, 1L);         \/\/ true  (succeed)\n+      __ z_locgr(Rres, Z_R0_scratch, Assembler::bcondNotEqual);\n+    } else {\n+      Label done;\n+      __ load_const_optimized(Rres, 0L); \/\/ false (failed)\n+      __ z_brne(done);                   \/\/ Assume true to be the common case.\n+      __ load_const_optimized(Rres, 1L); \/\/ true  (succeed)\n+      __ bind(done);\n+    }\n+\n+    write_barrier_post(masm, this,\n+                       Raddr           \/* store_addr *\/,\n+                       Rnew            \/* new_val    *\/,\n+                       $tmp1$$Register \/* tmp1       *\/,\n+                       $tmp2$$Register \/* tmp2       *\/);\n+    __ block_comment(\"} g1CompareAndSwapP\");\n+  %}\n+  ins_pipe(pipe_class_dummy);\n+%}\n+\n+instruct g1CompareAndExchangeP(iRegP mem_ptr, rarg5RegP oldval, iRegP_N2P newval, iRegP res, iRegL tmp1, iRegL tmp2, flagsReg cr) %{\n+  predicate(UseG1GC && n->as_LoadStore()->barrier_data() != 0);\n+  match(Set res (CompareAndExchangeP mem_ptr (Binary oldval newval)));\n+  effect(TEMP res, TEMP tmp1, TEMP tmp2, USE mem_ptr, USE_KILL oldval, KILL cr);\n+  format %{ \"$res = CompareAndExchangeP $oldval,$newval,$mem_ptr\" %}\n+  ins_encode %{\n+    __ block_comment(\"g1CompareAndExchangeP {\");\n+    assert_different_registers($oldval$$Register, $mem_ptr$$Register);\n+    assert_different_registers($newval$$Register, $mem_ptr$$Register);\n+\n+    \/\/ Pass $oldval to the pre-barrier (instead of loading from $mem), because\n+    \/\/ $oldval is the only value that can be overwritten.\n+    \/\/ The same holds for g1CompareAndSwapP.\n+    write_barrier_pre(masm, this,\n+                      noreg             \/* obj     *\/,\n+                      $oldval$$Register \/* pre_val *\/,\n+                      $tmp2$$Register   \/* tmp1    *\/,\n+                      RegSet::of($mem_ptr$$Register, $oldval$$Register, $newval$$Register) \/* preserve *\/,\n+                      RegSet::of($res$$Register) \/* no_preserve *\/);\n+\n+    __ z_lgr($res$$Register, $oldval$$Register); \/\/ previous content\n+\n+    __ z_csg($oldval$$Register, $newval$$Register, 0, $mem_ptr$$reg);\n+\n+    write_barrier_post(masm, this,\n+                       $mem_ptr$$Register \/* store_addr *\/,\n+                       $newval$$Register  \/* new_val    *\/,\n+                       $tmp1$$Register    \/* tmp1       *\/,\n+                       $tmp2$$Register    \/* tmp2       *\/);\n+    __ block_comment(\"} g1CompareAndExchangeP\");\n+  %}\n+  ins_pipe(pipe_class_dummy);\n+%}\n+\n+\/\/ Load Pointer\n+instruct g1LoadP(iRegP dst, memory mem, iRegL tmp1, flagsReg cr) %{\n+  predicate(UseG1GC && n->as_Load()->barrier_data() != 0);\n+  match(Set dst (LoadP mem));\n+  effect(TEMP dst, TEMP tmp1, KILL cr);\n+  ins_cost(MEMORY_REF_COST);\n+  format %{ \"LG      $dst,$mem\\t # ptr\" %}\n+  ins_encode %{\n+    __ block_comment(\"g1LoadP {\");\n+    __ z_lg($dst$$Register, $mem$$Address);\n+    write_barrier_pre(masm, this,\n+                      noreg \/* obj *\/,\n+                      $dst$$Register \/* pre_val *\/,\n+                      $tmp1$$Register );\n+    __ block_comment(\"} g1LoadP\");\n+  %}\n+  ins_pipe(pipe_class_dummy);\n+%}\n+\n+instruct g1GetAndSetP(indirect mem, iRegP dst, iRegL tmp, iRegL tmp1, iRegL tmp2, flagsReg cr) %{\n+  predicate(UseG1GC && n->as_LoadStore()->barrier_data() != 0);\n+  match(Set dst (GetAndSetP mem dst));\n+  effect(KILL cr, TEMP tmp, TEMP tmp1, TEMP tmp2); \/\/ USE_DEF dst by match rule.\n+  format %{ \"XCHGP   $dst,[$mem]\\t # EXCHANGE (oop, atomic), temp $tmp\" %}\n+  ins_encode %{\n+    __ block_comment(\"g1GetAndSetP {\");\n+\n+    write_barrier_pre(masm, this,\n+                      $mem$$Register  \/* obj  *\/,\n+                      $tmp$$Register  \/* pre_val (as a temporary register) *\/,\n+                      $tmp1$$Register \/* tmp1 *\/,\n+                      RegSet::of($mem$$Register, $dst$$Register) \/* preserve *\/);\n+\n+    __ z_lgr($tmp1$$Register, $dst$$Register);\n+    Register Rdst = reg_to_register_object($dst$$reg);\n+    Register Rtmp = reg_to_register_object($tmp$$reg);\n+    guarantee(Rdst != Rtmp, \"Fix match rule to use TEMP_DEF\");\n+    Label    retry;\n+\n+    \/\/ Iterate until swap succeeds.\n+    __ z_lg(Rtmp, Address($mem$$Register));  \/\/ current contents\n+    __ bind(retry);\n+    \/\/ Calculate incremented value.\n+    __ z_csg(Rtmp, Rdst, Address($mem$$Register)); \/\/ Try to store new value.\n+    __ z_brne(retry);                              \/\/ Yikes, concurrent update, need to retry.\n+    __ z_lgr(Rdst, Rtmp);                          \/\/ Exchanged value from memory is return value.\n+\n+    write_barrier_post(masm, this,\n+                       $mem$$Register  \/* store_addr *\/,\n+                       $tmp1$$Register \/* new_val    *\/,\n+                       $tmp2$$Register \/* tmp1       *\/,\n+                       $tmp$$Register  \/* tmp2       *\/);\n+    __ block_comment(\"} g1GetAndSetP\");\n+  %}\n+  ins_pipe(pipe_class_dummy);\n+%}\n+\n+instruct g1EncodePAndStoreN(indirect mem, iRegP src, iRegL tmp1, iRegL tmp2, flagsReg cr)\n+%{\n+  predicate(UseG1GC && n->as_Store()->barrier_data() != 0);\n+  match(Set mem (StoreN mem (EncodeP src)));\n+  effect(TEMP tmp1, TEMP tmp2, KILL cr);\n+  \/\/ ins_cost(INSN_COST);\n+  format %{ \"encode_heap_oop $tmp1, $src\\n\\t\"\n+            \"st  $tmp1, $mem\\t# compressed ptr\" %}\n+  ins_encode %{\n+    __ block_comment(\"g1EncodePAndStoreN {\");\n+    write_barrier_pre(masm, this,\n+                      $mem$$Register  \/* obj     *\/,\n+                      $tmp1$$Register \/* pre_val *\/,\n+                      $tmp2$$Register \/* tmp1    *\/,\n+                      RegSet::of($mem$$Register, $src$$Register) \/* preserve *\/);\n+    if ((barrier_data() & G1C2BarrierPostNotNull) == 0) {\n+      __ oop_encoder($tmp1$$Register, $src$$Register, true \/* maybe_null *\/);\n+    } else {\n+      __ oop_encoder($tmp1$$Register, $src$$Register, false \/* maybe_null *\/);\n+    }\n+    __ z_st($tmp1$$Register, Address($mem$$Register));\n+    write_barrier_post(masm, this,\n+                       $mem$$Register  \/* store_addr *\/,\n+                       $src$$Register  \/* new_val    *\/,\n+                       $tmp1$$Register \/* tmp1       *\/,\n+                       $tmp2$$Register \/* tmp2       *\/);\n+    __ block_comment(\"} g1EncodePAndStoreN\");\n+  %}\n+  ins_pipe(pipe_class_dummy);\n+%}\n","filename":"src\/hotspot\/cpu\/s390\/gc\/g1\/g1_s390.ad","additions":457,"deletions":0,"binary":false,"changes":457,"status":"added"},{"patch":"@@ -36,0 +36,3 @@\n+#ifdef COMPILER2\n+#include \"gc\/shared\/c2\/barrierSetC2.hpp\"\n+#endif \/\/ COMPILER2\n@@ -153,2 +156,87 @@\n-OptoReg::Name BarrierSetAssembler::refine_register(const Node* node, OptoReg::Name opto_reg) {\n-  Unimplemented(); \/\/ This must be implemented to support late barrier expansion.\n+OptoReg::Name BarrierSetAssembler::refine_register(const Node* node, OptoReg::Name opto_reg) const {\n+  if (!OptoReg::is_reg(opto_reg)) {\n+    return OptoReg::Bad;\n+  }\n+\n+  VMReg vm_reg = OptoReg::as_VMReg(opto_reg);\n+  if ((vm_reg->is_Register() || vm_reg ->is_FloatRegister()) && (opto_reg & 1) != 0) {\n+    return OptoReg::Bad;\n+  }\n+\n+  return opto_reg;\n+}\n+\n+#undef __\n+#define __ _masm->\n+\n+SaveLiveRegisters::SaveLiveRegisters(MacroAssembler *masm, BarrierStubC2 *stub)\n+  : _masm(masm), _reg_mask(stub->preserve_set()) {\n+\n+  const int register_save_size = iterate_over_register_mask(ACTION_COUNT_ONLY) * BytesPerWord;\n+\n+  _frame_size = align_up(register_save_size, frame::alignment_in_bytes) + frame::z_abi_160_size; \/\/ FIXME: this could be restricted to argument only\n+\n+  __ save_return_pc();\n+  __ push_frame(_frame_size, Z_R14); \/\/ FIXME: check if Z_R1_scaratch can do a job here;\n+\n+  __ z_lg(Z_R14, _z_common_abi(return_pc) + _frame_size, Z_SP);\n+\n+  iterate_over_register_mask(ACTION_SAVE, _frame_size);\n+}\n+\n+SaveLiveRegisters::~SaveLiveRegisters() {\n+  iterate_over_register_mask(ACTION_RESTORE, _frame_size);\n+\n+  __ pop_frame();\n+\n+  __ restore_return_pc();\n+}\n+\n+int SaveLiveRegisters::iterate_over_register_mask(IterationAction action, int offset) {\n+  int reg_save_index = 0;\n+  RegMaskIterator live_regs_iterator(_reg_mask);\n+\n+  while(live_regs_iterator.has_next()) {\n+    const OptoReg::Name opto_reg = live_regs_iterator.next();\n+\n+    \/\/ Filter out stack slots (spilled registers, i.e., stack-allocated registers).\n+    if (!OptoReg::is_reg(opto_reg)) {\n+      continue;\n+    }\n+\n+    const VMReg vm_reg = OptoReg::as_VMReg(opto_reg);\n+    if (vm_reg->is_Register()) {\n+      Register std_reg = vm_reg->as_Register();\n+\n+      if (std_reg->encoding() >= Z_R2->encoding() && std_reg->encoding() <= Z_R15->encoding()) {\n+        reg_save_index++;\n+\n+        if (action == ACTION_SAVE) {\n+          __ z_stg(std_reg, offset - reg_save_index * BytesPerWord, Z_SP);\n+        } else if (action == ACTION_RESTORE) {\n+          __ z_lg(std_reg, offset - reg_save_index * BytesPerWord, Z_SP);\n+        } else {\n+          assert(action == ACTION_COUNT_ONLY, \"Sanity\");\n+        }\n+      }\n+    } else if (vm_reg->is_FloatRegister()) {\n+      FloatRegister fp_reg = vm_reg->as_FloatRegister();\n+      if (fp_reg->encoding() >= Z_F0->encoding() && fp_reg->encoding() <= Z_F15->encoding()\n+          && fp_reg->encoding() != Z_F1->encoding()) {\n+        reg_save_index++;\n+\n+        if (action == ACTION_SAVE) {\n+          __ z_std(fp_reg, offset - reg_save_index * BytesPerWord, Z_SP);\n+        } else if (action == ACTION_RESTORE) {\n+          __ z_ld(fp_reg, offset - reg_save_index * BytesPerWord, Z_SP);\n+        } else {\n+          assert(action == ACTION_COUNT_ONLY, \"Sanity\");\n+        }\n+      }\n+    } else if (false \/* vm_reg->is_VectorRegister() *\/){\n+      fatal(\"Vector register support is not there yet!\");\n+    } else {\n+      fatal(\"Register type is not known\");\n+    }\n+  }\n+  return reg_save_index;\n","filename":"src\/hotspot\/cpu\/s390\/gc\/shared\/barrierSetAssembler_s390.cpp","additions":90,"deletions":2,"binary":false,"changes":92,"status":"modified"},{"patch":"@@ -35,0 +35,1 @@\n+#include \"opto\/regmask.hpp\"\n@@ -36,0 +37,1 @@\n+class BarrierStubC2;\n@@ -64,1 +66,1 @@\n-                                OptoReg::Name opto_reg);\n+                                OptoReg::Name opto_reg) const;\n@@ -68,0 +70,34 @@\n+#ifdef COMPILER2\n+\n+\/\/ This class saves and restores the registers that need to be preserved across\n+\/\/ the runtime call represented by a given C2 barrier stub. Use as follows:\n+\/\/ {\n+\/\/   SaveLiveRegisters save(masm, stub);\n+\/\/   ..\n+\/\/   __ call_VM_leaf(...);\n+\/\/   ..\n+\/\/ }\n+\n+class SaveLiveRegisters {\n+  MacroAssembler* _masm;\n+  RegMask _reg_mask;\n+  Register _result_reg;\n+  int _frame_size;\n+\n+ public:\n+  SaveLiveRegisters(MacroAssembler *masm, BarrierStubC2 *stub);\n+\n+  ~SaveLiveRegisters();\n+\n+ private:\n+  enum IterationAction : int {\n+    ACTION_SAVE,\n+    ACTION_RESTORE,\n+    ACTION_COUNT_ONLY\n+  };\n+\n+  int iterate_over_register_mask(IterationAction action, int offset = 0);\n+};\n+\n+#endif \/\/ COMPILER2\n+\n","filename":"src\/hotspot\/cpu\/s390\/gc\/shared\/barrierSetAssembler_s390.hpp","additions":37,"deletions":1,"binary":false,"changes":38,"status":"modified"},{"patch":"@@ -2130,1 +2130,1 @@\n-  BLOCK_COMMENT(\"pop_frame:\");\n+  BLOCK_COMMENT(\"pop_frame {\");\n@@ -2132,0 +2132,1 @@\n+  BLOCK_COMMENT(\"} pop_frame\");\n","filename":"src\/hotspot\/cpu\/s390\/macroAssembler_s390.cpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -451,0 +451,8 @@\n+typedef AbstractRegSet<Register> RegSet;\n+\n+template <>\n+inline Register AbstractRegSet<Register>::first() {\n+  if (_bitset == 0) { return noreg; }\n+  return as_Register(count_trailing_zeros(_bitset));\n+}\n+\n","filename":"src\/hotspot\/cpu\/s390\/register_s390.hpp","additions":8,"deletions":0,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -1647,0 +1647,4 @@\n+  if (is_encode_and_store_pattern(n, m)) {\n+    mstack.push(m, Visit);\n+    return true;\n+  }\n@@ -3916,0 +3920,1 @@\n+  predicate(n->as_Load()->barrier_data() == 0);\n@@ -3927,0 +3932,1 @@\n+  predicate(n->as_Load()->barrier_data() == 0);\n@@ -4289,0 +4295,1 @@\n+  predicate(n->as_Store()->barrier_data() == 0);\n@@ -4391,0 +4398,1 @@\n+  predicate(n->as_Store()->barrier_data() == 0);\n@@ -4420,0 +4428,1 @@\n+  predicate(n->as_Load()->barrier_data() == 0);\n@@ -4483,1 +4492,1 @@\n-  predicate(false && (CompressedOops::base()==nullptr)&&(CompressedOops::shift()==0));\n+  predicate(false && (CompressedOops::base()==nullptr) && (CompressedOops::shift()==0));\n@@ -4738,0 +4747,1 @@\n+  predicate(n->as_Store()->barrier_data() == 0);\n@@ -5149,0 +5159,1 @@\n+  predicate(n->as_LoadStore()->barrier_data() == 0);\n@@ -5159,0 +5170,1 @@\n+  predicate(n->as_LoadStore()->barrier_data() == 0);\n@@ -5446,0 +5458,1 @@\n+  predicate(n->as_LoadStore()->barrier_data() == 0);\n@@ -5455,0 +5468,1 @@\n+  predicate(n->as_LoadStore()->barrier_data() == 0);\n@@ -5929,1 +5943,1 @@\n-  predicate(VM_Version::has_MemWithImmALUOps());\n+  predicate(VM_Version::has_MemWithImmALUOps() && n->as_LoadStore()->barrier_data() == 0);\n","filename":"src\/hotspot\/cpu\/s390\/s390.ad","additions":16,"deletions":2,"binary":false,"changes":18,"status":"modified"},{"patch":"@@ -41,1 +41,4 @@\n-#endif\n+#endif \/\/ COMPILER1\n+#ifdef COMPILER2\n+#include \"gc\/g1\/c2\/g1BarrierSetC2.hpp\"\n+#endif \/\/ COMPILER2\n@@ -163,0 +166,50 @@\n+static void generate_queue_insertion(MacroAssembler* masm, ByteSize index_offset, ByteSize buffer_offset, Label& runtime,\n+                                     const Register thread, const Register value, const Register temp) {\n+  \/\/ This code assumes that buffer index is pointer sized.\n+  STATIC_ASSERT(in_bytes(SATBMarkQueue::byte_width_of_index()) == sizeof(intptr_t));\n+  \/\/ Can we store a value in the given thread's buffer?\n+  \/\/ (The index field is typed as size_t.)\n+  __ movptr(temp, Address(thread, in_bytes(index_offset)));   \/\/ temp := *(index address)\n+  __ testptr(temp, temp);                                     \/\/ index == 0?\n+  __ jcc(Assembler::zero, runtime);                           \/\/ jump to runtime if index == 0 (full buffer)\n+  \/\/ The buffer is not full, store value into it.\n+  __ subptr(temp, wordSize);                                  \/\/ temp := next index\n+  __ movptr(Address(thread, in_bytes(index_offset)), temp);   \/\/ *(index address) := next index\n+  __ addptr(temp, Address(thread, in_bytes(buffer_offset)));  \/\/ temp := buffer address + next index\n+  __ movptr(Address(temp, 0), value);                         \/\/ *(buffer address + next index) := value\n+}\n+\n+static void generate_pre_barrier_fast_path(MacroAssembler* masm,\n+                                           const Register thread) {\n+  Address in_progress(thread, in_bytes(G1ThreadLocalData::satb_mark_queue_active_offset()));\n+  \/\/ Is marking active?\n+  if (in_bytes(SATBMarkQueue::byte_width_of_active()) == 4) {\n+    __ cmpl(in_progress, 0);\n+  } else {\n+    assert(in_bytes(SATBMarkQueue::byte_width_of_active()) == 1, \"Assumption\");\n+    __ cmpb(in_progress, 0);\n+  }\n+}\n+\n+static void generate_pre_barrier_slow_path(MacroAssembler* masm,\n+                                           const Register obj,\n+                                           const Register pre_val,\n+                                           const Register thread,\n+                                           const Register tmp,\n+                                           Label& done,\n+                                           Label& runtime) {\n+  \/\/ Do we need to load the previous value?\n+  if (obj != noreg) {\n+    __ load_heap_oop(pre_val, Address(obj, 0), noreg, noreg, AS_RAW);\n+  }\n+  \/\/ Is the previous value null?\n+  __ cmpptr(pre_val, NULL_WORD);\n+  __ jcc(Assembler::equal, done);\n+  generate_queue_insertion(masm,\n+                           G1ThreadLocalData::satb_mark_queue_index_offset(),\n+                           G1ThreadLocalData::satb_mark_queue_buffer_offset(),\n+                           runtime,\n+                           thread, pre_val, tmp);\n+  __ jmp(done);\n+}\n+\n@@ -188,20 +241,2 @@\n-  Address in_progress(thread, in_bytes(G1ThreadLocalData::satb_mark_queue_active_offset()));\n-  Address index(thread, in_bytes(G1ThreadLocalData::satb_mark_queue_index_offset()));\n-  Address buffer(thread, in_bytes(G1ThreadLocalData::satb_mark_queue_buffer_offset()));\n-\n-  \/\/ Is marking active?\n-  if (in_bytes(SATBMarkQueue::byte_width_of_active()) == 4) {\n-    __ cmpl(in_progress, 0);\n-  } else {\n-    assert(in_bytes(SATBMarkQueue::byte_width_of_active()) == 1, \"Assumption\");\n-    __ cmpb(in_progress, 0);\n-  }\n-  __ jcc(Assembler::equal, done);\n-\n-  \/\/ Do we need to load the previous value?\n-  if (obj != noreg) {\n-    __ load_heap_oop(pre_val, Address(obj, 0), noreg, noreg, AS_RAW);\n-  }\n-\n-  \/\/ Is the previous value null?\n-  __ cmpptr(pre_val, NULL_WORD);\n+  generate_pre_barrier_fast_path(masm, thread);\n+  \/\/ If marking is not active (*(mark queue active address) == 0), jump to done\n@@ -209,16 +244,1 @@\n-\n-  \/\/ Can we store original value in the thread's buffer?\n-  \/\/ Is index == 0?\n-  \/\/ (The index field is typed as size_t.)\n-\n-  __ movptr(tmp, index);                   \/\/ tmp := *index_adr\n-  __ cmpptr(tmp, 0);                       \/\/ tmp == 0?\n-  __ jcc(Assembler::equal, runtime);       \/\/ If yes, goto runtime\n-\n-  __ subptr(tmp, wordSize);                \/\/ tmp := tmp - wordSize\n-  __ movptr(index, tmp);                   \/\/ *index_adr := tmp\n-  __ addptr(tmp, buffer);                  \/\/ tmp := tmp + *buffer_adr\n-\n-  \/\/ Record the previous value\n-  __ movptr(Address(tmp, 0), pre_val);\n-  __ jmp(done);\n+  generate_pre_barrier_slow_path(masm, obj, pre_val, thread, tmp, done, runtime);\n@@ -266,0 +286,48 @@\n+static void generate_post_barrier_fast_path(MacroAssembler* masm,\n+                                            const Register store_addr,\n+                                            const Register new_val,\n+                                            const Register tmp,\n+                                            const Register tmp2,\n+                                            Label& done,\n+                                            bool new_val_may_be_null) {\n+  CardTableBarrierSet* ct = barrier_set_cast<CardTableBarrierSet>(BarrierSet::barrier_set());\n+    \/\/ Does store cross heap regions?\n+  __ movptr(tmp, store_addr);                                    \/\/ tmp := store address\n+  __ xorptr(tmp, new_val);                                       \/\/ tmp := store address ^ new value\n+  __ shrptr(tmp, G1HeapRegion::LogOfHRGrainBytes);               \/\/ ((store address ^ new value) >> LogOfHRGrainBytes) == 0?\n+  __ jcc(Assembler::equal, done);\n+  \/\/ Crosses regions, storing null?\n+  if (new_val_may_be_null) {\n+    __ cmpptr(new_val, NULL_WORD);                               \/\/ new value == null?\n+    __ jcc(Assembler::equal, done);\n+  }\n+  \/\/ Storing region crossing non-null, is card young?\n+  __ movptr(tmp, store_addr);                                    \/\/ tmp := store address\n+  __ shrptr(tmp, CardTable::card_shift());                       \/\/ tmp := card address relative to card table base\n+  \/\/ Do not use ExternalAddress to load 'byte_map_base', since 'byte_map_base' is NOT\n+  \/\/ a valid address and therefore is not properly handled by the relocation code.\n+  __ movptr(tmp2, (intptr_t)ct->card_table()->byte_map_base());  \/\/ tmp2 := card table base address\n+  __ addptr(tmp, tmp2);                                          \/\/ tmp := card address\n+  __ cmpb(Address(tmp, 0), G1CardTable::g1_young_card_val());    \/\/ *(card address) == young_card_val?\n+}\n+\n+static void generate_post_barrier_slow_path(MacroAssembler* masm,\n+                                            const Register thread,\n+                                            const Register tmp,\n+                                            const Register tmp2,\n+                                            Label& done,\n+                                            Label& runtime) {\n+  __ membar(Assembler::Membar_mask_bits(Assembler::StoreLoad));  \/\/ StoreLoad membar\n+  __ cmpb(Address(tmp, 0), G1CardTable::dirty_card_val());       \/\/ *(card address) == dirty_card_val?\n+  __ jcc(Assembler::equal, done);\n+  \/\/ Storing a region crossing, non-null oop, card is clean.\n+  \/\/ Dirty card and log.\n+  __ movb(Address(tmp, 0), G1CardTable::dirty_card_val());       \/\/ *(card address) := dirty_card_val\n+  generate_queue_insertion(masm,\n+                           G1ThreadLocalData::dirty_card_queue_index_offset(),\n+                           G1ThreadLocalData::dirty_card_queue_buffer_offset(),\n+                           runtime,\n+                           thread, tmp, tmp2);\n+  __ jmp(done);\n+}\n+\n@@ -276,6 +344,0 @@\n-  Address queue_index(thread, in_bytes(G1ThreadLocalData::dirty_card_queue_index_offset()));\n-  Address buffer(thread, in_bytes(G1ThreadLocalData::dirty_card_queue_buffer_offset()));\n-\n-  CardTableBarrierSet* ct =\n-    barrier_set_cast<CardTableBarrierSet>(BarrierSet::barrier_set());\n-\n@@ -285,5 +347,2 @@\n-  \/\/ Does store cross heap regions?\n-\n-  __ movptr(tmp, store_addr);\n-  __ xorptr(tmp, new_val);\n-  __ shrptr(tmp, G1HeapRegion::LogOfHRGrainBytes);\n+  generate_post_barrier_fast_path(masm, store_addr, new_val, tmp, tmp2, done, true \/* new_val_may_be_null *\/);\n+  \/\/ If card is young, jump to done\n@@ -291,0 +350,1 @@\n+  generate_post_barrier_slow_path(masm, thread, tmp, tmp2, done, runtime);\n@@ -292,1 +352,6 @@\n-  \/\/ crosses regions, storing null?\n+  __ bind(runtime);\n+  \/\/ save the live input values\n+  RegSet saved = RegSet::of(store_addr NOT_LP64(COMMA thread));\n+  __ push_set(saved);\n+  __ call_VM_leaf(CAST_FROM_FN_PTR(address, G1BarrierSetRuntime::write_ref_field_post_entry), tmp, thread);\n+  __ pop_set(saved);\n@@ -294,2 +359,2 @@\n-  __ cmpptr(new_val, NULL_WORD);\n-  __ jcc(Assembler::equal, done);\n+  __ bind(done);\n+}\n@@ -297,1 +362,1 @@\n-  \/\/ storing region crossing non-null, is card already dirty?\n+#if defined(COMPILER2)\n@@ -299,2 +364,17 @@\n-  const Register card_addr = tmp;\n-  const Register cardtable = tmp2;\n+static void generate_c2_barrier_runtime_call(MacroAssembler* masm, G1BarrierStubC2* stub, const Register arg, const address runtime_path) {\n+#ifdef _LP64\n+  SaveLiveRegisters save_registers(masm, stub);\n+  if (c_rarg0 != arg) {\n+    __ mov(c_rarg0, arg);\n+  }\n+  __ mov(c_rarg1, r15_thread);\n+  \/\/ rax is a caller-saved, non-argument-passing register, so it does not\n+  \/\/ interfere with c_rarg0 or c_rarg1. If it contained any live value before\n+  \/\/ entering this stub, it is saved at this point, and restored after the\n+  \/\/ call. If it did not contain any live value, it is free to be used. In\n+  \/\/ either case, it is safe to use it here as a call scratch register.\n+  __ call(RuntimeAddress(runtime_path), rax);\n+#else\n+  Unimplemented();\n+#endif \/\/ _LP64\n+}\n@@ -302,6 +382,13 @@\n-  __ movptr(card_addr, store_addr);\n-  __ shrptr(card_addr, CardTable::card_shift());\n-  \/\/ Do not use ExternalAddress to load 'byte_map_base', since 'byte_map_base' is NOT\n-  \/\/ a valid address and therefore is not properly handled by the relocation code.\n-  __ movptr(cardtable, (intptr_t)ct->card_table()->byte_map_base());\n-  __ addptr(card_addr, cardtable);\n+void G1BarrierSetAssembler::g1_write_barrier_pre_c2(MacroAssembler* masm,\n+                                                    Register obj,\n+                                                    Register pre_val,\n+                                                    Register thread,\n+                                                    Register tmp,\n+                                                    G1PreBarrierStubC2* stub) {\n+#ifdef _LP64\n+  assert(thread == r15_thread, \"must be\");\n+#endif \/\/ _LP64\n+  assert(pre_val != noreg, \"check this code\");\n+  if (obj != noreg) {\n+    assert_different_registers(obj, pre_val, tmp);\n+  }\n@@ -309,2 +396,1 @@\n-  __ cmpb(Address(card_addr, 0), G1CardTable::g1_young_card_val());\n-  __ jcc(Assembler::equal, done);\n+  stub->initialize_registers(obj, pre_val, thread, tmp, noreg);\n@@ -312,3 +398,3 @@\n-  __ membar(Assembler::Membar_mask_bits(Assembler::StoreLoad));\n-  __ cmpb(Address(card_addr, 0), G1CardTable::dirty_card_val());\n-  __ jcc(Assembler::equal, done);\n+  generate_pre_barrier_fast_path(masm, thread);\n+  \/\/ If marking is active (*(mark queue active address) != 0), jump to stub (slow path)\n+  __ jcc(Assembler::notEqual, *stub->entry());\n@@ -316,0 +402,2 @@\n+  __ bind(*stub->continuation());\n+}\n@@ -317,2 +405,8 @@\n-  \/\/ storing a region crossing, non-null oop, card is clean.\n-  \/\/ dirty card and log.\n+void G1BarrierSetAssembler::generate_c2_pre_barrier_stub(MacroAssembler* masm,\n+                                                         G1PreBarrierStubC2* stub) const {\n+  Assembler::InlineSkippedInstructionsCounter skip_counter(masm);\n+  Label runtime;\n+  Register obj = stub->obj();\n+  Register pre_val = stub->pre_val();\n+  Register thread = stub->thread();\n+  Register tmp = stub->tmp1();\n@@ -320,1 +414,2 @@\n-  __ movb(Address(card_addr, 0), G1CardTable::dirty_card_val());\n+  __ bind(*stub->entry());\n+  generate_pre_barrier_slow_path(masm, obj, pre_val, thread, tmp, *stub->continuation(), runtime);\n@@ -322,2 +417,4 @@\n-  \/\/ The code below assumes that buffer index is pointer sized.\n-  STATIC_ASSERT(in_bytes(G1DirtyCardQueue::byte_width_of_index()) == sizeof(intptr_t));\n+  __ bind(runtime);\n+  generate_c2_barrier_runtime_call(masm, stub, pre_val, CAST_FROM_FN_PTR(address, G1BarrierSetRuntime::write_ref_field_pre_entry));\n+  __ jmp(*stub->continuation());\n+}\n@@ -325,8 +422,10 @@\n-  __ movptr(tmp2, queue_index);\n-  __ testptr(tmp2, tmp2);\n-  __ jcc(Assembler::zero, runtime);\n-  __ subptr(tmp2, wordSize);\n-  __ movptr(queue_index, tmp2);\n-  __ addptr(tmp2, buffer);\n-  __ movptr(Address(tmp2, 0), card_addr);\n-  __ jmp(done);\n+void G1BarrierSetAssembler::g1_write_barrier_post_c2(MacroAssembler* masm,\n+                                                     Register store_addr,\n+                                                     Register new_val,\n+                                                     Register thread,\n+                                                     Register tmp,\n+                                                     Register tmp2,\n+                                                     G1PostBarrierStubC2* stub) {\n+#ifdef _LP64\n+  assert(thread == r15_thread, \"must be\");\n+#endif \/\/ _LP64\n@@ -334,6 +433,1 @@\n-  __ bind(runtime);\n-  \/\/ save the live input values\n-  RegSet saved = RegSet::of(store_addr NOT_LP64(COMMA thread));\n-  __ push_set(saved);\n-  __ call_VM_leaf(CAST_FROM_FN_PTR(address, G1BarrierSetRuntime::write_ref_field_post_entry), card_addr, thread);\n-  __ pop_set(saved);\n+  stub->initialize_registers(thread, tmp, tmp2);\n@@ -341,1 +435,6 @@\n-  __ bind(done);\n+  bool new_val_may_be_null = (stub->barrier_data() & G1C2BarrierPostNotNull) == 0;\n+  generate_post_barrier_fast_path(masm, store_addr, new_val, tmp, tmp2, *stub->continuation(), new_val_may_be_null);\n+  \/\/ If card is not young, jump to stub (slow path)\n+  __ jcc(Assembler::notEqual, *stub->entry());\n+\n+  __ bind(*stub->continuation());\n@@ -344,0 +443,18 @@\n+void G1BarrierSetAssembler::generate_c2_post_barrier_stub(MacroAssembler* masm,\n+                                                          G1PostBarrierStubC2* stub) const {\n+  Assembler::InlineSkippedInstructionsCounter skip_counter(masm);\n+  Label runtime;\n+  Register thread = stub->thread();\n+  Register tmp = stub->tmp1(); \/\/ tmp holds the card address.\n+  Register tmp2 = stub->tmp2();\n+\n+  __ bind(*stub->entry());\n+  generate_post_barrier_slow_path(masm, thread, tmp, tmp2, *stub->continuation(), runtime);\n+\n+  __ bind(runtime);\n+  generate_c2_barrier_runtime_call(masm, stub, tmp, CAST_FROM_FN_PTR(address, G1BarrierSetRuntime::write_ref_field_post_entry));\n+  __ jmp(*stub->continuation());\n+}\n+\n+#endif \/\/ COMPILER2\n+\n","filename":"src\/hotspot\/cpu\/x86\/gc\/g1\/g1BarrierSetAssembler_x86.cpp","additions":202,"deletions":85,"binary":false,"changes":287,"status":"modified"},{"patch":"@@ -35,0 +35,3 @@\n+class G1BarrierStubC2;\n+class G1PreBarrierStubC2;\n+class G1PostBarrierStubC2;\n@@ -68,0 +71,20 @@\n+\n+#ifdef COMPILER2\n+  void g1_write_barrier_pre_c2(MacroAssembler* masm,\n+                               Register obj,\n+                               Register pre_val,\n+                               Register thread,\n+                               Register tmp,\n+                               G1PreBarrierStubC2* c2_stub);\n+  void generate_c2_pre_barrier_stub(MacroAssembler* masm,\n+                                    G1PreBarrierStubC2* stub) const;\n+  void g1_write_barrier_post_c2(MacroAssembler* masm,\n+                                Register store_addr,\n+                                Register new_val,\n+                                Register thread,\n+                                Register tmp,\n+                                Register tmp2,\n+                                G1PostBarrierStubC2* c2_stub);\n+  void generate_c2_post_barrier_stub(MacroAssembler* masm,\n+                                     G1PostBarrierStubC2* stub) const;\n+#endif \/\/ COMPILER2\n","filename":"src\/hotspot\/cpu\/x86\/gc\/g1\/g1BarrierSetAssembler_x86.hpp","additions":23,"deletions":0,"binary":false,"changes":23,"status":"modified"},{"patch":"@@ -0,0 +1,374 @@\n+\/\/\n+\/\/ Copyright (c) 2024, Oracle and\/or its affiliates. All rights reserved.\n+\/\/ DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+\/\/\n+\/\/ This code is free software; you can redistribute it and\/or modify it\n+\/\/ under the terms of the GNU General Public License version 2 only, as\n+\/\/ published by the Free Software Foundation.\n+\/\/\n+\/\/ This code is distributed in the hope that it will be useful, but WITHOUT\n+\/\/ ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+\/\/ FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+\/\/ version 2 for more details (a copy is included in the LICENSE file that\n+\/\/ accompanied this code).\n+\/\/\n+\/\/ You should have received a copy of the GNU General Public License version\n+\/\/ 2 along with this work; if not, write to the Free Software Foundation,\n+\/\/ Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+\/\/\n+\/\/ Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+\/\/ or visit www.oracle.com if you need additional information or have any\n+\/\/ questions.\n+\/\/\n+\n+source_hpp %{\n+\n+#include \"gc\/g1\/c2\/g1BarrierSetC2.hpp\"\n+#include \"gc\/shared\/gc_globals.hpp\"\n+\n+%}\n+\n+source %{\n+\n+#include \"gc\/g1\/g1BarrierSetAssembler_x86.hpp\"\n+#include \"gc\/g1\/g1BarrierSetRuntime.hpp\"\n+\n+static void write_barrier_pre(MacroAssembler* masm,\n+                              const MachNode* node,\n+                              Register obj,\n+                              Register pre_val,\n+                              Register tmp,\n+                              RegSet preserve = RegSet(),\n+                              RegSet no_preserve = RegSet()) {\n+  if (!G1PreBarrierStubC2::needs_barrier(node)) {\n+    return;\n+  }\n+  Assembler::InlineSkippedInstructionsCounter skip_counter(masm);\n+  G1BarrierSetAssembler* g1_asm = static_cast<G1BarrierSetAssembler*>(BarrierSet::barrier_set()->barrier_set_assembler());\n+  G1PreBarrierStubC2* const stub = G1PreBarrierStubC2::create(node);\n+  for (RegSetIterator<Register> reg = preserve.begin(); *reg != noreg; ++reg) {\n+    stub->preserve(*reg);\n+  }\n+  for (RegSetIterator<Register> reg = no_preserve.begin(); *reg != noreg; ++reg) {\n+    stub->dont_preserve(*reg);\n+  }\n+  g1_asm->g1_write_barrier_pre_c2(masm, obj, pre_val, r15_thread, tmp, stub);\n+}\n+\n+static void write_barrier_post(MacroAssembler* masm,\n+                               const MachNode* node,\n+                               Register store_addr,\n+                               Register new_val,\n+                               Register tmp1,\n+                               Register tmp2) {\n+  if (!G1PostBarrierStubC2::needs_barrier(node)) {\n+    return;\n+  }\n+  Assembler::InlineSkippedInstructionsCounter skip_counter(masm);\n+  G1BarrierSetAssembler* g1_asm = static_cast<G1BarrierSetAssembler*>(BarrierSet::barrier_set()->barrier_set_assembler());\n+  G1PostBarrierStubC2* const stub = G1PostBarrierStubC2::create(node);\n+  g1_asm->g1_write_barrier_post_c2(masm, store_addr, new_val, r15_thread, tmp1, tmp2, stub);\n+}\n+\n+%}\n+\n+instruct g1StoreP(memory mem, any_RegP src, rRegP tmp1, rRegP tmp2, rRegP tmp3, rFlagsReg cr)\n+%{\n+  predicate(UseG1GC && n->as_Store()->barrier_data() != 0);\n+  match(Set mem (StoreP mem src));\n+  effect(TEMP tmp1, TEMP tmp2, TEMP tmp3, KILL cr);\n+  ins_cost(125); \/\/ XXX\n+  format %{ \"movq    $mem, $src\\t# ptr\" %}\n+  ins_encode %{\n+    \/\/ Materialize the store address internally (as opposed to defining 'mem' as\n+    \/\/ an indirect memory operand) to reduce the overhead of LCM when processing\n+    \/\/ large basic blocks with many stores. Such basic blocks arise, for\n+    \/\/ instance, from static initializations of large String arrays.\n+    \/\/ The same holds for g1StoreN and g1EncodePAndStoreN.\n+    __ lea($tmp1$$Register, $mem$$Address);\n+    write_barrier_pre(masm, this,\n+                      $tmp1$$Register \/* obj *\/,\n+                      $tmp2$$Register \/* pre_val *\/,\n+                      $tmp3$$Register \/* tmp *\/,\n+                      RegSet::of($tmp1$$Register, $src$$Register) \/* preserve *\/);\n+    __ movq(Address($tmp1$$Register, 0), $src$$Register);\n+    write_barrier_post(masm, this,\n+                       $tmp1$$Register \/* store_addr *\/,\n+                       $src$$Register \/* new_val *\/,\n+                       $tmp3$$Register \/* tmp1 *\/,\n+                       $tmp2$$Register \/* tmp2 *\/);\n+  %}\n+  ins_pipe(ialu_mem_reg);\n+%}\n+\n+instruct g1StoreN(memory mem, rRegN src, rRegP tmp1, rRegP tmp2, rRegP tmp3, rFlagsReg cr)\n+%{\n+  predicate(UseG1GC && n->as_Store()->barrier_data() != 0);\n+  match(Set mem (StoreN mem src));\n+  effect(TEMP tmp1, TEMP tmp2, TEMP tmp3, KILL cr);\n+  ins_cost(125); \/\/ XXX\n+  format %{ \"movl    $mem, $src\\t# ptr\" %}\n+  ins_encode %{\n+    assert(!in(operand_index(2))->is_Mach() ||\n+           (in(operand_index(2))->as_Mach()->ideal_Opcode() != Op_EncodeP),\n+           \"EncodeP src nodes should be matched with their corresponding StoreN nodes\");\n+    __ lea($tmp1$$Register, $mem$$Address);\n+    write_barrier_pre(masm, this,\n+                      $tmp1$$Register \/* obj *\/,\n+                      $tmp2$$Register \/* pre_val *\/,\n+                      $tmp3$$Register \/* tmp *\/,\n+                      RegSet::of($tmp1$$Register, $src$$Register) \/* preserve *\/);\n+    __ movl(Address($tmp1$$Register, 0), $src$$Register);\n+    if ((barrier_data() & G1C2BarrierPost) != 0) {\n+      __ movl($tmp2$$Register, $src$$Register);\n+      if ((barrier_data() & G1C2BarrierPostNotNull) == 0) {\n+        __ decode_heap_oop($tmp2$$Register);\n+      } else {\n+        __ decode_heap_oop_not_null($tmp2$$Register);\n+      }\n+    }\n+    write_barrier_post(masm, this,\n+                       $tmp1$$Register \/* store_addr *\/,\n+                       $tmp2$$Register \/* new_val *\/,\n+                       $tmp3$$Register \/* tmp1 *\/,\n+                       $tmp2$$Register \/* tmp2 *\/);\n+  %}\n+  ins_pipe(ialu_mem_reg);\n+%}\n+\n+instruct g1EncodePAndStoreN(memory mem, any_RegP src, rRegP tmp1, rRegP tmp2, rRegP tmp3, rFlagsReg cr)\n+%{\n+  predicate(UseG1GC && n->as_Store()->barrier_data() != 0);\n+  match(Set mem (StoreN mem (EncodeP src)));\n+  effect(TEMP tmp1, TEMP tmp2, TEMP tmp3, KILL cr);\n+  ins_cost(125); \/\/ XXX\n+  format %{ \"encode_heap_oop $src\\n\\t\"\n+            \"movl   $mem, $src\\t# ptr\" %}\n+  ins_encode %{\n+    __ lea($tmp1$$Register, $mem$$Address);\n+    write_barrier_pre(masm, this,\n+                      $tmp1$$Register \/* obj *\/,\n+                      $tmp2$$Register \/* pre_val *\/,\n+                      $tmp3$$Register \/* tmp *\/,\n+                      RegSet::of($tmp1$$Register, $src$$Register) \/* preserve *\/);\n+    __ movq($tmp2$$Register, $src$$Register);\n+    if ((barrier_data() & G1C2BarrierPostNotNull) == 0) {\n+      __ encode_heap_oop($tmp2$$Register);\n+    } else {\n+      __ encode_heap_oop_not_null($tmp2$$Register);\n+    }\n+    __ movl(Address($tmp1$$Register, 0), $tmp2$$Register);\n+    write_barrier_post(masm, this,\n+                       $tmp1$$Register \/* store_addr *\/,\n+                       $src$$Register \/* new_val *\/,\n+                       $tmp3$$Register \/* tmp1 *\/,\n+                       $tmp2$$Register \/* tmp2 *\/);\n+  %}\n+  ins_pipe(ialu_mem_reg);\n+%}\n+\n+instruct g1CompareAndExchangeP(indirect mem, rRegP newval, rRegP tmp1, rRegP tmp2, rRegP tmp3, rax_RegP oldval, rFlagsReg cr)\n+%{\n+  predicate(UseG1GC && n->as_LoadStore()->barrier_data() != 0);\n+  match(Set oldval (CompareAndExchangeP mem (Binary oldval newval)));\n+  effect(TEMP tmp1, TEMP tmp2, TEMP tmp3, KILL cr);\n+  format %{ \"lock\\n\\t\"\n+            \"cmpxchgq $newval, $mem\" %}\n+  ins_encode %{\n+    assert_different_registers($oldval$$Register, $mem$$Register);\n+    \/\/ Pass $oldval to the pre-barrier (instead of loading from $mem), because\n+    \/\/ $oldval is the only value that can be overwritten.\n+    \/\/ The same holds for g1CompareAndSwapP.\n+    write_barrier_pre(masm, this,\n+                      noreg \/* obj *\/,\n+                      $oldval$$Register \/* pre_val *\/,\n+                      $tmp3$$Register \/* tmp *\/,\n+                      RegSet::of($mem$$Register, $newval$$Register, $oldval$$Register) \/* preserve *\/);\n+    __ movq($tmp1$$Register, $newval$$Register);\n+    __ lock();\n+    __ cmpxchgq($tmp1$$Register, Address($mem$$Register, 0));\n+    write_barrier_post(masm, this,\n+                       $mem$$Register \/* store_addr *\/,\n+                       $tmp1$$Register \/* new_val *\/,\n+                       $tmp2$$Register \/* tmp1 *\/,\n+                       $tmp3$$Register \/* tmp2 *\/);\n+  %}\n+  ins_pipe(pipe_cmpxchg);\n+%}\n+\n+instruct g1CompareAndExchangeN(indirect mem, rRegN newval, rRegP tmp1, rRegP tmp2, rRegP tmp3, rax_RegN oldval, rFlagsReg cr)\n+%{\n+  predicate(UseG1GC && n->as_LoadStore()->barrier_data() != 0);\n+  match(Set oldval (CompareAndExchangeN mem (Binary oldval newval)));\n+  effect(TEMP tmp1, TEMP tmp2, TEMP tmp3, KILL cr);\n+  format %{ \"lock\\n\\t\"\n+            \"cmpxchgq $newval, $mem\" %}\n+  ins_encode %{\n+    assert_different_registers($oldval$$Register, $mem$$Register);\n+    write_barrier_pre(masm, this,\n+                      $mem$$Register \/* obj *\/,\n+                      $tmp2$$Register \/* pre_val *\/,\n+                      $tmp3$$Register \/* tmp *\/,\n+                      RegSet::of($mem$$Register, $newval$$Register, $oldval$$Register) \/* preserve *\/);\n+    __ movl($tmp1$$Register, $newval$$Register);\n+    __ lock();\n+    __ cmpxchgl($tmp1$$Register, Address($mem$$Register, 0));\n+    __ decode_heap_oop($tmp1$$Register);\n+    write_barrier_post(masm, this,\n+                       $mem$$Register \/* store_addr *\/,\n+                       $tmp1$$Register \/* new_val *\/,\n+                       $tmp2$$Register \/* tmp1 *\/,\n+                       $tmp3$$Register \/* tmp2 *\/);\n+  %}\n+  ins_pipe(pipe_cmpxchg);\n+%}\n+\n+instruct g1CompareAndSwapP(rRegI res, indirect mem, rRegP newval, rRegP tmp1, rRegP tmp2, rRegP tmp3, rax_RegP oldval, rFlagsReg cr)\n+%{\n+  predicate(UseG1GC && n->as_LoadStore()->barrier_data() != 0);\n+  match(Set res (CompareAndSwapP mem (Binary oldval newval)));\n+  match(Set res (WeakCompareAndSwapP mem (Binary oldval newval)));\n+  effect(TEMP res, TEMP tmp1, TEMP tmp2, TEMP tmp3, KILL oldval, KILL cr);\n+  format %{ \"lock\\n\\t\"\n+            \"cmpxchgq $newval, $mem\\n\\t\"\n+            \"sete     $res\\n\\t\"\n+            \"movzbl   $res, $res\" %}\n+  ins_encode %{\n+    assert_different_registers($oldval$$Register, $mem$$Register);\n+    write_barrier_pre(masm, this,\n+                      noreg \/* obj *\/,\n+                      $oldval$$Register \/* pre_val *\/,\n+                      $tmp3$$Register \/* tmp *\/,\n+                      RegSet::of($mem$$Register, $newval$$Register, $oldval$$Register) \/* preserve *\/,\n+                      RegSet::of($res$$Register) \/* no_preserve *\/);\n+    __ movq($tmp1$$Register, $newval$$Register);\n+    __ lock();\n+    __ cmpxchgq($tmp1$$Register, Address($mem$$Register, 0));\n+    __ setb(Assembler::equal, $res$$Register);\n+    __ movzbl($res$$Register, $res$$Register);\n+    write_barrier_post(masm, this,\n+                       $mem$$Register \/* store_addr *\/,\n+                       $tmp1$$Register \/* new_val *\/,\n+                       $tmp2$$Register \/* tmp1 *\/,\n+                       $tmp3$$Register \/* tmp2 *\/);\n+  %}\n+  ins_pipe(pipe_cmpxchg);\n+%}\n+\n+instruct g1CompareAndSwapN(rRegI res, indirect mem, rRegN newval, rRegP tmp1, rRegP tmp2, rRegP tmp3, rax_RegN oldval, rFlagsReg cr)\n+%{\n+  predicate(UseG1GC && n->as_LoadStore()->barrier_data() != 0);\n+  match(Set res (CompareAndSwapN mem (Binary oldval newval)));\n+  match(Set res (WeakCompareAndSwapN mem (Binary oldval newval)));\n+  effect(TEMP res, TEMP tmp1, TEMP tmp2, TEMP tmp3, KILL oldval, KILL cr);\n+  format %{ \"lock\\n\\t\"\n+            \"cmpxchgq $newval, $mem\\n\\t\"\n+            \"sete     $res\\n\\t\"\n+            \"movzbl   $res, $res\" %}\n+  ins_encode %{\n+    assert_different_registers($oldval$$Register, $mem$$Register);\n+    write_barrier_pre(masm, this,\n+                      $mem$$Register \/* obj *\/,\n+                      $tmp2$$Register \/* pre_val *\/,\n+                      $tmp3$$Register \/* tmp *\/,\n+                      RegSet::of($mem$$Register, $newval$$Register, $oldval$$Register) \/* preserve *\/,\n+                      RegSet::of($res$$Register) \/* no_preserve *\/);\n+    __ movl($tmp1$$Register, $newval$$Register);\n+    __ lock();\n+    __ cmpxchgl($tmp1$$Register, Address($mem$$Register, 0));\n+    __ setb(Assembler::equal, $res$$Register);\n+    __ movzbl($res$$Register, $res$$Register);\n+    __ decode_heap_oop($tmp1$$Register);\n+    write_barrier_post(masm, this,\n+                       $mem$$Register \/* store_addr *\/,\n+                       $tmp1$$Register \/* new_val *\/,\n+                       $tmp2$$Register \/* tmp1 *\/,\n+                       $tmp3$$Register \/* tmp2 *\/);\n+  %}\n+  ins_pipe(pipe_cmpxchg);\n+%}\n+\n+instruct g1GetAndSetP(indirect mem, rRegP newval, rRegP tmp1, rRegP tmp2, rRegP tmp3, rFlagsReg cr)\n+%{\n+  predicate(UseG1GC && n->as_LoadStore()->barrier_data() != 0);\n+  match(Set newval (GetAndSetP mem newval));\n+  effect(TEMP tmp1, TEMP tmp2, TEMP tmp3, KILL cr);\n+  format %{ \"xchgq    $newval, $mem\" %}\n+  ins_encode %{\n+    assert_different_registers($mem$$Register, $newval$$Register);\n+    write_barrier_pre(masm, this,\n+                      $mem$$Register \/* obj *\/,\n+                      $tmp2$$Register \/* pre_val *\/,\n+                      $tmp3$$Register \/* tmp *\/,\n+                      RegSet::of($mem$$Register, $newval$$Register) \/* preserve *\/);\n+    __ movq($tmp1$$Register, $newval$$Register);\n+    __ xchgq($newval$$Register, Address($mem$$Register, 0));\n+    write_barrier_post(masm, this,\n+                       $mem$$Register \/* store_addr *\/,\n+                       $tmp1$$Register \/* new_val *\/,\n+                       $tmp2$$Register \/* tmp1 *\/,\n+                       $tmp3$$Register \/* tmp2 *\/);\n+  %}\n+  ins_pipe(pipe_cmpxchg);\n+%}\n+\n+instruct g1GetAndSetN(indirect mem, rRegN newval, rRegP tmp1, rRegP tmp2, rRegP tmp3, rFlagsReg cr)\n+%{\n+  predicate(UseG1GC && n->as_LoadStore()->barrier_data() != 0);\n+  match(Set newval (GetAndSetN mem newval));\n+  effect(TEMP tmp1, TEMP tmp2, TEMP tmp3, KILL cr);\n+  format %{ \"xchgq    $newval, $mem\" %}\n+  ins_encode %{\n+    assert_different_registers($mem$$Register, $newval$$Register);\n+    write_barrier_pre(masm, this,\n+                      $mem$$Register \/* obj *\/,\n+                      $tmp2$$Register \/* pre_val *\/,\n+                      $tmp3$$Register \/* tmp *\/,\n+                      RegSet::of($mem$$Register, $newval$$Register) \/* preserve *\/);\n+    __ movl($tmp1$$Register, $newval$$Register);\n+    __ decode_heap_oop($tmp1$$Register);\n+    __ xchgl($newval$$Register, Address($mem$$Register, 0));\n+    write_barrier_post(masm, this,\n+                       $mem$$Register \/* store_addr *\/,\n+                       $tmp1$$Register \/* new_val *\/,\n+                       $tmp2$$Register \/* tmp1 *\/,\n+                       $tmp3$$Register \/* tmp2 *\/);\n+  %}\n+  ins_pipe(pipe_cmpxchg);\n+%}\n+\n+instruct g1LoadP(rRegP dst, memory mem, rRegP tmp, rFlagsReg cr)\n+%{\n+  predicate(UseG1GC && n->as_Load()->barrier_data() != 0);\n+  match(Set dst (LoadP mem));\n+  effect(TEMP dst, TEMP tmp, KILL cr);\n+  ins_cost(125); \/\/ XXX\n+  format %{ \"movq    $dst, $mem\\t# ptr\" %}\n+  ins_encode %{\n+    __ movq($dst$$Register, $mem$$Address);\n+    write_barrier_pre(masm, this,\n+                      noreg \/* obj *\/,\n+                      $dst$$Register \/* pre_val *\/,\n+                      $tmp$$Register \/* tmp *\/);\n+  %}\n+  ins_pipe(ialu_reg_mem); \/\/ XXX\n+%}\n+\n+instruct g1LoadN(rRegN dst, memory mem, rRegP tmp1, rRegP tmp2, rFlagsReg cr)\n+%{\n+  predicate(UseG1GC && n->as_Load()->barrier_data() != 0);\n+  match(Set dst (LoadN mem));\n+  effect(TEMP dst, TEMP tmp1, TEMP tmp2, KILL cr);\n+  ins_cost(125); \/\/ XXX\n+  format %{ \"movl    $dst, $mem\\t# compressed ptr\" %}\n+  ins_encode %{\n+    __ movl($dst$$Register, $mem$$Address);\n+    __ movl($tmp1$$Register, $dst$$Register);\n+    __ decode_heap_oop($tmp1$$Register);\n+    write_barrier_pre(masm, this,\n+                      noreg \/* obj *\/,\n+                      $tmp1$$Register \/* pre_val *\/,\n+                      $tmp2$$Register \/* tmp *\/);\n+  %}\n+  ins_pipe(ialu_reg_mem); \/\/ XXX\n+%}\n","filename":"src\/hotspot\/cpu\/x86\/gc\/g1\/g1_x86_64.ad","additions":374,"deletions":0,"binary":false,"changes":374,"status":"added"},{"patch":"@@ -2460,0 +2460,4 @@\n+  if (is_encode_and_store_pattern(n, m)) {\n+    mstack.push(m, Visit);\n+    return true;\n+  }\n","filename":"src\/hotspot\/cpu\/x86\/x86.ad","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -4345,0 +4345,1 @@\n+   predicate(n->as_Load()->barrier_data() == 0);\n@@ -5130,0 +5131,1 @@\n+  predicate(n->as_Store()->barrier_data() == 0);\n@@ -5154,1 +5156,1 @@\n-  predicate(CompressedOops::base() == nullptr);\n+  predicate(CompressedOops::base() == nullptr && n->as_Store()->barrier_data() == 0);\n@@ -5167,0 +5169,1 @@\n+  predicate(n->as_Store()->barrier_data() == 0);\n@@ -7176,0 +7179,1 @@\n+  predicate(n->as_LoadStore()->barrier_data() == 0);\n@@ -7265,0 +7269,1 @@\n+  predicate(n->as_LoadStore()->barrier_data() == 0);\n@@ -7486,0 +7491,1 @@\n+  predicate(n->as_LoadStore()->barrier_data() == 0);\n@@ -11677,0 +11683,1 @@\n+  predicate(n->in(2)->as_Load()->barrier_data() == 0);\n@@ -11698,0 +11705,1 @@\n+  predicate(n->in(2)->as_Load()->barrier_data() == 0);\n@@ -11738,1 +11746,2 @@\n-  predicate(CompressedOops::base() != nullptr);\n+  predicate(CompressedOops::base() != nullptr &&\n+            n->in(1)->as_Load()->barrier_data() == 0);\n@@ -11751,1 +11760,2 @@\n-  predicate(CompressedOops::base() == nullptr);\n+  predicate(CompressedOops::base() == nullptr &&\n+            n->in(1)->as_Load()->barrier_data() == 0);\n","filename":"src\/hotspot\/cpu\/x86\/x86_64.ad","additions":13,"deletions":3,"binary":false,"changes":16,"status":"modified"},{"patch":"@@ -27,0 +27,1 @@\n+#include \"code\/vmreg.inline.hpp\"\n@@ -29,0 +30,1 @@\n+#include \"gc\/g1\/g1BarrierSetAssembler.hpp\"\n@@ -34,0 +36,1 @@\n+#include \"opto\/block.hpp\"\n@@ -38,0 +41,1 @@\n+#include \"opto\/machnode.hpp\"\n@@ -39,0 +43,4 @@\n+#include \"opto\/memnode.hpp\"\n+#include \"opto\/node.hpp\"\n+#include \"opto\/output.hpp\"\n+#include \"opto\/regalloc.hpp\"\n@@ -40,0 +48,1 @@\n+#include \"opto\/runtime.hpp\"\n@@ -41,0 +50,1 @@\n+#include \"utilities\/growableArray.hpp\"\n@@ -43,1 +53,494 @@\n-const TypeFunc *G1BarrierSetC2::write_ref_field_pre_entry_Type() {\n+\/*\n+ * Determine if the G1 pre-barrier can be removed. The pre-barrier is\n+ * required by SATB to make sure all objects live at the start of the\n+ * marking are kept alive, all reference updates need to any previous\n+ * reference stored before writing.\n+ *\n+ * If the previous value is null there is no need to save the old value.\n+ * References that are null are filtered during runtime by the barrier\n+ * code to avoid unnecessary queuing.\n+ *\n+ * However in the case of newly allocated objects it might be possible to\n+ * prove that the reference about to be overwritten is null during compile\n+ * time and avoid adding the barrier code completely.\n+ *\n+ * The compiler needs to determine that the object in which a field is about\n+ * to be written is newly allocated, and that no prior store to the same field\n+ * has happened since the allocation.\n+ *\/\n+bool G1BarrierSetC2::g1_can_remove_pre_barrier(GraphKit* kit,\n+                                               PhaseValues* phase,\n+                                               Node* adr,\n+                                               BasicType bt,\n+                                               uint adr_idx) const {\n+  intptr_t offset = 0;\n+  Node* base = AddPNode::Ideal_base_and_offset(adr, phase, offset);\n+  AllocateNode* alloc = AllocateNode::Ideal_allocation(base);\n+\n+  if (offset == Type::OffsetBot) {\n+    return false; \/\/ Cannot unalias unless there are precise offsets.\n+  }\n+  if (alloc == nullptr) {\n+    return false; \/\/ No allocation found.\n+  }\n+\n+  intptr_t size_in_bytes = type2aelembytes(bt);\n+  Node* mem = kit->memory(adr_idx); \/\/ Start searching here.\n+\n+  for (int cnt = 0; cnt < 50; cnt++) {\n+    if (mem->is_Store()) {\n+      Node* st_adr = mem->in(MemNode::Address);\n+      intptr_t st_offset = 0;\n+      Node* st_base = AddPNode::Ideal_base_and_offset(st_adr, phase, st_offset);\n+\n+      if (st_base == nullptr) {\n+        break; \/\/ Inscrutable pointer.\n+      }\n+      if (st_base == base && st_offset == offset) {\n+        \/\/ We have found a store with same base and offset as ours.\n+        break;\n+      }\n+      if (st_offset != offset && st_offset != Type::OffsetBot) {\n+        const int MAX_STORE = BytesPerLong;\n+        if (st_offset >= offset + size_in_bytes ||\n+            st_offset <= offset - MAX_STORE ||\n+            st_offset <= offset - mem->as_Store()->memory_size()) {\n+          \/\/ Success:  The offsets are provably independent.\n+          \/\/ (You may ask, why not just test st_offset != offset and be done?\n+          \/\/ The answer is that stores of different sizes can co-exist\n+          \/\/ in the same sequence of RawMem effects.  We sometimes initialize\n+          \/\/ a whole 'tile' of array elements with a single jint or jlong.)\n+          mem = mem->in(MemNode::Memory);\n+          continue; \/\/ Advance through independent store memory.\n+        }\n+      }\n+      if (st_base != base\n+          && MemNode::detect_ptr_independence(base, alloc, st_base,\n+                                              AllocateNode::Ideal_allocation(st_base),\n+                                              phase)) {\n+        \/\/ Success: the bases are provably independent.\n+        mem = mem->in(MemNode::Memory);\n+        continue; \/\/ Advance through independent store memory.\n+      }\n+    } else if (mem->is_Proj() && mem->in(0)->is_Initialize()) {\n+      InitializeNode* st_init = mem->in(0)->as_Initialize();\n+      AllocateNode* st_alloc = st_init->allocation();\n+\n+      \/\/ Make sure that we are looking at the same allocation site.\n+      \/\/ The alloc variable is guaranteed to not be null here from earlier check.\n+      if (alloc == st_alloc) {\n+        \/\/ Check that the initialization is storing null so that no previous store\n+        \/\/ has been moved up and directly write a reference.\n+        Node* captured_store = st_init->find_captured_store(offset,\n+                                                            type2aelembytes(T_OBJECT),\n+                                                            phase);\n+        if (captured_store == nullptr || captured_store == st_init->zero_memory()) {\n+          return true;\n+        }\n+      }\n+    }\n+    \/\/ Unless there is an explicit 'continue', we must bail out here,\n+    \/\/ because 'mem' is an inscrutable memory state (e.g., a call).\n+    break;\n+  }\n+  return false;\n+}\n+\n+\/*\n+ * G1, similar to any GC with a Young Generation, requires a way to keep track\n+ * of references from Old Generation to Young Generation to make sure all live\n+ * objects are found. G1 also requires to keep track of object references\n+ * between different regions to enable evacuation of old regions, which is done\n+ * as part of mixed collections. References are tracked in remembered sets,\n+ * which are continuously updated as references are written to with the help of\n+ * the post-barrier.\n+ *\n+ * To reduce the number of updates to the remembered set, the post-barrier\n+ * filters out updates to fields in objects located in the Young Generation, the\n+ * same region as the reference, when null is being written, or if the card is\n+ * already marked as dirty by an earlier write.\n+ *\n+ * Under certain circumstances it is possible to avoid generating the\n+ * post-barrier completely, if it is possible during compile time to prove the\n+ * object is newly allocated and that no safepoint exists between the allocation\n+ * and the store. This can be seen as a compile-time version of the\n+ * above-mentioned Young Generation filter.\n+ *\n+ * In the case of a slow allocation, the allocation code must handle the barrier\n+ * as part of the allocation if the allocated object is not located in the\n+ * nursery; this would happen for humongous objects.\n+ *\/\n+bool G1BarrierSetC2::g1_can_remove_post_barrier(GraphKit* kit,\n+                                                PhaseValues* phase, Node* store_ctrl,\n+                                                Node* adr) const {\n+  intptr_t      offset = 0;\n+  Node*         base   = AddPNode::Ideal_base_and_offset(adr, phase, offset);\n+  AllocateNode* alloc  = AllocateNode::Ideal_allocation(base);\n+\n+  if (offset == Type::OffsetBot) {\n+    return false; \/\/ Cannot unalias unless there are precise offsets.\n+  }\n+  if (alloc == nullptr) {\n+     return false; \/\/ No allocation found.\n+  }\n+\n+  Node* mem = store_ctrl;   \/\/ Start search from Store node.\n+  if (mem->is_Proj() && mem->in(0)->is_Initialize()) {\n+    InitializeNode* st_init = mem->in(0)->as_Initialize();\n+    AllocateNode*  st_alloc = st_init->allocation();\n+    \/\/ Make sure we are looking at the same allocation\n+    if (alloc == st_alloc) {\n+      return true;\n+    }\n+  }\n+\n+  return false;\n+}\n+\n+Node* G1BarrierSetC2::load_at_resolved(C2Access& access, const Type* val_type) const {\n+  DecoratorSet decorators = access.decorators();\n+  bool on_weak = (decorators & ON_WEAK_OOP_REF) != 0;\n+  bool on_phantom = (decorators & ON_PHANTOM_OOP_REF) != 0;\n+  bool no_keepalive = (decorators & AS_NO_KEEPALIVE) != 0;\n+  \/\/ If we are reading the value of the referent field of a Reference object, we\n+  \/\/ need to record the referent in an SATB log buffer using the pre-barrier\n+  \/\/ mechanism. Also we need to add a memory barrier to prevent commoning reads\n+  \/\/ from this field across safepoints, since GC can change its value.\n+  bool need_read_barrier = ((on_weak || on_phantom) && !no_keepalive);\n+  if (access.is_oop() && need_read_barrier) {\n+    access.set_barrier_data(G1C2BarrierPre);\n+  }\n+  return CardTableBarrierSetC2::load_at_resolved(access, val_type);\n+}\n+\n+void G1BarrierSetC2::eliminate_gc_barrier(PhaseMacroExpand* macro, Node* node) const {\n+  eliminate_gc_barrier_data(node);\n+}\n+\n+void G1BarrierSetC2::eliminate_gc_barrier_data(Node* node) const {\n+  if (node->is_LoadStore()) {\n+    LoadStoreNode* loadstore = node->as_LoadStore();\n+    loadstore->set_barrier_data(0);\n+  } else if (node->is_Mem()) {\n+    MemNode* mem = node->as_Mem();\n+    mem->set_barrier_data(0);\n+  }\n+}\n+\n+static void refine_barrier_by_new_val_type(const Node* n) {\n+  if (n->Opcode() != Op_StoreP &&\n+      n->Opcode() != Op_StoreN) {\n+    return;\n+  }\n+  MemNode* store = n->as_Mem();\n+  const Node* newval = n->in(MemNode::ValueIn);\n+  assert(newval != nullptr, \"\");\n+  const Type* newval_bottom = newval->bottom_type();\n+  assert(newval_bottom->isa_ptr() || newval_bottom->isa_narrowoop(), \"newval should be an OOP\");\n+  TypePtr::PTR newval_type = newval_bottom->make_ptr()->ptr();\n+  uint8_t barrier_data = store->barrier_data();\n+  if (newval_type == TypePtr::Null) {\n+    \/\/ Simply elide post-barrier if writing null.\n+    barrier_data &= ~G1C2BarrierPost;\n+    barrier_data &= ~G1C2BarrierPostNotNull;\n+  } else if (((barrier_data & G1C2BarrierPost) != 0) &&\n+             newval_type == TypePtr::NotNull) {\n+    \/\/ If the post-barrier has not been elided yet (e.g. due to newval being\n+    \/\/ freshly allocated), mark it as not-null (simplifies barrier tests and\n+    \/\/ compressed OOPs logic).\n+    barrier_data |= G1C2BarrierPostNotNull;\n+  }\n+  store->set_barrier_data(barrier_data);\n+  return;\n+}\n+\n+\/\/ Refine (not really expand) G1 barriers by looking at the new value type\n+\/\/ (whether it is necessarily null or necessarily non-null).\n+bool G1BarrierSetC2::expand_barriers(Compile* C, PhaseIterGVN& igvn) const {\n+  ResourceMark rm;\n+  VectorSet visited;\n+  Node_List worklist;\n+  worklist.push(C->root());\n+  while (worklist.size() > 0) {\n+    Node* n = worklist.pop();\n+    if (visited.test_set(n->_idx)) {\n+      continue;\n+    }\n+    refine_barrier_by_new_val_type(n);\n+    for (uint j = 0; j < n->req(); j++) {\n+      Node* in = n->in(j);\n+      if (in != nullptr) {\n+        worklist.push(in);\n+      }\n+    }\n+  }\n+  return false;\n+}\n+\n+uint G1BarrierSetC2::estimated_barrier_size(const Node* node) const {\n+  \/\/ These Ideal node counts are extracted from the pre-matching Ideal graph\n+  \/\/ generated when compiling the following method with early barrier expansion:\n+  \/\/   static void write(MyObject obj1, Object o) {\n+  \/\/     obj1.o1 = o;\n+  \/\/   }\n+  uint8_t barrier_data = MemNode::barrier_data(node);\n+  uint nodes = 0;\n+  if ((barrier_data & G1C2BarrierPre) != 0) {\n+    nodes += 50;\n+  }\n+  if ((barrier_data & G1C2BarrierPost) != 0) {\n+    nodes += 60;\n+  }\n+  return nodes;\n+}\n+\n+bool G1BarrierSetC2::can_initialize_object(const StoreNode* store) const {\n+  assert(store->Opcode() == Op_StoreP || store->Opcode() == Op_StoreN, \"OOP store expected\");\n+  \/\/ It is OK to move the store across the object initialization boundary only\n+  \/\/ if it does not have any barrier, or if it has barriers that can be safely\n+  \/\/ elided (because of the compensation steps taken on the allocation slow path\n+  \/\/ when ReduceInitialCardMarks is enabled).\n+  return (MemNode::barrier_data(store) == 0) || use_ReduceInitialCardMarks();\n+}\n+\n+void G1BarrierSetC2::clone_at_expansion(PhaseMacroExpand* phase, ArrayCopyNode* ac) const {\n+  if (ac->is_clone_inst() && !use_ReduceInitialCardMarks()) {\n+    clone_in_runtime(phase, ac, G1BarrierSetRuntime::clone_addr(), \"G1BarrierSetRuntime::clone\");\n+    return;\n+  }\n+  BarrierSetC2::clone_at_expansion(phase, ac);\n+}\n+\n+Node* G1BarrierSetC2::store_at_resolved(C2Access& access, C2AccessValue& val) const {\n+  DecoratorSet decorators = access.decorators();\n+  bool anonymous = (decorators & ON_UNKNOWN_OOP_REF) != 0;\n+  bool in_heap = (decorators & IN_HEAP) != 0;\n+  bool tightly_coupled_alloc = (decorators & C2_TIGHTLY_COUPLED_ALLOC) != 0;\n+  bool need_store_barrier = !(tightly_coupled_alloc && use_ReduceInitialCardMarks()) && (in_heap || anonymous);\n+  if (access.is_oop() && need_store_barrier) {\n+    access.set_barrier_data(get_store_barrier(access));\n+    if (tightly_coupled_alloc) {\n+      assert(!use_ReduceInitialCardMarks(),\n+             \"post-barriers are only needed for tightly-coupled initialization stores when ReduceInitialCardMarks is disabled\");\n+      access.set_barrier_data(access.barrier_data() ^ G1C2BarrierPre);\n+    }\n+  }\n+  return BarrierSetC2::store_at_resolved(access, val);\n+}\n+\n+Node* G1BarrierSetC2::atomic_cmpxchg_val_at_resolved(C2AtomicParseAccess& access, Node* expected_val,\n+                                                         Node* new_val, const Type* value_type) const {\n+  GraphKit* kit = access.kit();\n+  if (!access.is_oop()) {\n+    return BarrierSetC2::atomic_cmpxchg_val_at_resolved(access, expected_val, new_val, value_type);\n+  }\n+  access.set_barrier_data(G1C2BarrierPre | G1C2BarrierPost);\n+  return BarrierSetC2::atomic_cmpxchg_val_at_resolved(access, expected_val, new_val, value_type);\n+}\n+\n+Node* G1BarrierSetC2::atomic_cmpxchg_bool_at_resolved(C2AtomicParseAccess& access, Node* expected_val,\n+                                                          Node* new_val, const Type* value_type) const {\n+  GraphKit* kit = access.kit();\n+  if (!access.is_oop()) {\n+    return BarrierSetC2::atomic_cmpxchg_bool_at_resolved(access, expected_val, new_val, value_type);\n+  }\n+  access.set_barrier_data(G1C2BarrierPre | G1C2BarrierPost);\n+  return BarrierSetC2::atomic_cmpxchg_bool_at_resolved(access, expected_val, new_val, value_type);\n+}\n+\n+Node* G1BarrierSetC2::atomic_xchg_at_resolved(C2AtomicParseAccess& access, Node* new_val, const Type* value_type) const {\n+  GraphKit* kit = access.kit();\n+  if (!access.is_oop()) {\n+    return BarrierSetC2::atomic_xchg_at_resolved(access, new_val, value_type);\n+  }\n+  access.set_barrier_data(G1C2BarrierPre | G1C2BarrierPost);\n+  return BarrierSetC2::atomic_xchg_at_resolved(access, new_val, value_type);\n+}\n+\n+class G1BarrierSetC2State : public BarrierSetC2State {\n+private:\n+  GrowableArray<G1BarrierStubC2*>* _stubs;\n+\n+public:\n+  G1BarrierSetC2State(Arena* arena)\n+    : BarrierSetC2State(arena),\n+      _stubs(new (arena) GrowableArray<G1BarrierStubC2*>(arena, 8,  0, nullptr)) {}\n+\n+  GrowableArray<G1BarrierStubC2*>* stubs() {\n+    return _stubs;\n+  }\n+\n+  bool needs_liveness_data(const MachNode* mach) const {\n+    return G1PreBarrierStubC2::needs_barrier(mach) ||\n+           G1PostBarrierStubC2::needs_barrier(mach);\n+  }\n+\n+  bool needs_livein_data() const {\n+    return false;\n+  }\n+};\n+\n+static G1BarrierSetC2State* barrier_set_state() {\n+  return reinterpret_cast<G1BarrierSetC2State*>(Compile::current()->barrier_set_state());\n+}\n+\n+G1BarrierStubC2::G1BarrierStubC2(const MachNode* node) : BarrierStubC2(node) {}\n+\n+G1PreBarrierStubC2::G1PreBarrierStubC2(const MachNode* node) : G1BarrierStubC2(node) {}\n+\n+bool G1PreBarrierStubC2::needs_barrier(const MachNode* node) {\n+  return (node->barrier_data() & G1C2BarrierPre) != 0;\n+}\n+\n+G1PreBarrierStubC2* G1PreBarrierStubC2::create(const MachNode* node) {\n+  G1PreBarrierStubC2* const stub = new (Compile::current()->comp_arena()) G1PreBarrierStubC2(node);\n+  if (!Compile::current()->output()->in_scratch_emit_size()) {\n+    barrier_set_state()->stubs()->append(stub);\n+  }\n+  return stub;\n+}\n+\n+void G1PreBarrierStubC2::initialize_registers(Register obj, Register pre_val, Register thread, Register tmp1, Register tmp2) {\n+  _obj = obj;\n+  _pre_val = pre_val;\n+  _thread = thread;\n+  _tmp1 = tmp1;\n+  _tmp2 = tmp2;\n+}\n+\n+Register G1PreBarrierStubC2::obj() const {\n+  return _obj;\n+}\n+\n+Register G1PreBarrierStubC2::pre_val() const {\n+  return _pre_val;\n+}\n+\n+Register G1PreBarrierStubC2::thread() const {\n+  return _thread;\n+}\n+\n+Register G1PreBarrierStubC2::tmp1() const {\n+  return _tmp1;\n+}\n+\n+Register G1PreBarrierStubC2::tmp2() const {\n+  return _tmp2;\n+}\n+\n+void G1PreBarrierStubC2::emit_code(MacroAssembler& masm) {\n+  G1BarrierSetAssembler* bs = static_cast<G1BarrierSetAssembler*>(BarrierSet::barrier_set()->barrier_set_assembler());\n+  bs->generate_c2_pre_barrier_stub(&masm, this);\n+}\n+\n+G1PostBarrierStubC2::G1PostBarrierStubC2(const MachNode* node) : G1BarrierStubC2(node) {}\n+\n+bool G1PostBarrierStubC2::needs_barrier(const MachNode* node) {\n+  return (node->barrier_data() & G1C2BarrierPost) != 0;\n+}\n+\n+G1PostBarrierStubC2* G1PostBarrierStubC2::create(const MachNode* node) {\n+  G1PostBarrierStubC2* const stub = new (Compile::current()->comp_arena()) G1PostBarrierStubC2(node);\n+  if (!Compile::current()->output()->in_scratch_emit_size()) {\n+    barrier_set_state()->stubs()->append(stub);\n+  }\n+  return stub;\n+}\n+\n+void G1PostBarrierStubC2::initialize_registers(Register thread, Register tmp1, Register tmp2) {\n+  _thread = thread;\n+  _tmp1 = tmp1;\n+  _tmp2 = tmp2;\n+}\n+\n+Register G1PostBarrierStubC2::thread() const {\n+  return _thread;\n+}\n+\n+Register G1PostBarrierStubC2::tmp1() const {\n+  return _tmp1;\n+}\n+\n+Register G1PostBarrierStubC2::tmp2() const {\n+  return _tmp2;\n+}\n+\n+void G1PostBarrierStubC2::emit_code(MacroAssembler& masm) {\n+  G1BarrierSetAssembler* bs = static_cast<G1BarrierSetAssembler*>(BarrierSet::barrier_set()->barrier_set_assembler());\n+  bs->generate_c2_post_barrier_stub(&masm, this);\n+}\n+\n+void* G1BarrierSetC2::create_barrier_state(Arena* comp_arena) const {\n+  return new (comp_arena) G1BarrierSetC2State(comp_arena);\n+}\n+\n+int G1BarrierSetC2::get_store_barrier(C2Access& access) const {\n+  if (!access.is_parse_access()) {\n+    \/\/ Only support for eliding barriers at parse time for now.\n+    return G1C2BarrierPre | G1C2BarrierPost;\n+  }\n+  GraphKit* kit = (static_cast<C2ParseAccess&>(access)).kit();\n+  Node * ctl = kit->control();\n+  Node* adr = access.addr().node();\n+  uint adr_idx = kit->C->get_alias_index(access.addr().type());\n+  assert(adr_idx != Compile::AliasIdxTop, \"use other store_to_memory factory\" );\n+\n+  bool can_remove_pre_barrier = g1_can_remove_pre_barrier(kit, &kit->gvn(), adr, access.type(), adr_idx);\n+\n+  \/\/ We can skip marks on a freshly-allocated object in Eden. Keep this code in\n+  \/\/ sync with CardTableBarrierSet::on_slowpath_allocation_exit. That routine\n+  \/\/ informs GC to take appropriate compensating steps, upon a slow-path\n+  \/\/ allocation, so as to make this card-mark elision safe.\n+  \/\/ The post-barrier can also be removed if null is written. This case is\n+  \/\/ handled by G1BarrierSetC2::expand_barriers, which runs at the end of C2's\n+  \/\/ platform-independent optimizations to exploit stronger type information.\n+  bool can_remove_post_barrier = use_ReduceInitialCardMarks() &&\n+    ((access.base() == kit->just_allocated_object(ctl)) ||\n+     g1_can_remove_post_barrier(kit, &kit->gvn(), ctl, adr));\n+\n+  int barriers = 0;\n+  if (!can_remove_pre_barrier) {\n+    barriers |= G1C2BarrierPre;\n+  }\n+  if (!can_remove_post_barrier) {\n+    barriers |= G1C2BarrierPost;\n+  }\n+\n+  return barriers;\n+}\n+\n+void G1BarrierSetC2::late_barrier_analysis() const {\n+  compute_liveness_at_stubs();\n+}\n+\n+void G1BarrierSetC2::emit_stubs(CodeBuffer& cb) const {\n+  MacroAssembler masm(&cb);\n+  GrowableArray<G1BarrierStubC2*>* const stubs = barrier_set_state()->stubs();\n+  for (int i = 0; i < stubs->length(); i++) {\n+    \/\/ Make sure there is enough space in the code buffer\n+    if (cb.insts()->maybe_expand_to_ensure_remaining(PhaseOutput::MAX_inst_size) && cb.blob() == nullptr) {\n+      ciEnv::current()->record_failure(\"CodeCache is full\");\n+      return;\n+    }\n+    stubs->at(i)->emit_code(masm);\n+  }\n+  masm.flush();\n+}\n+\n+#ifndef PRODUCT\n+void G1BarrierSetC2::dump_barrier_data(const MachNode* mach, outputStream* st) const {\n+  if ((mach->barrier_data() & G1C2BarrierPre) != 0) {\n+    st->print(\"pre \");\n+  }\n+  if ((mach->barrier_data() & G1C2BarrierPost) != 0) {\n+    st->print(\"post \");\n+  }\n+  if ((mach->barrier_data() & G1C2BarrierPostNotNull) != 0) {\n+    st->print(\"notnull \");\n+  }\n+}\n+#endif \/\/ !PRODUCT\n+\n+#if G1_LATE_BARRIER_MIGRATION_SUPPORT\n+\n+const TypeFunc *G1BarrierSetC2Early::write_ref_field_pre_entry_Type() {\n@@ -56,1 +559,1 @@\n-const TypeFunc *G1BarrierSetC2::write_ref_field_post_entry_Type() {\n+const TypeFunc *G1BarrierSetC2Early::write_ref_field_post_entry_Type() {\n@@ -90,5 +593,5 @@\n-bool G1BarrierSetC2::g1_can_remove_pre_barrier(GraphKit* kit,\n-                                               PhaseValues* phase,\n-                                               Node* adr,\n-                                               BasicType bt,\n-                                               uint adr_idx) const {\n+bool G1BarrierSetC2Early::g1_can_remove_pre_barrier(GraphKit* kit,\n+                                                    PhaseValues* phase,\n+                                                    Node* adr,\n+                                                    BasicType bt,\n+                                                    uint adr_idx) const {\n@@ -179,10 +682,10 @@\n-void G1BarrierSetC2::pre_barrier(GraphKit* kit,\n-                                 bool do_load,\n-                                 Node* ctl,\n-                                 Node* obj,\n-                                 Node* adr,\n-                                 uint alias_idx,\n-                                 Node* val,\n-                                 const TypeOopPtr* val_type,\n-                                 Node* pre_val,\n-                                 BasicType bt) const {\n+void G1BarrierSetC2Early::pre_barrier(GraphKit* kit,\n+                                      bool do_load,\n+                                      Node* ctl,\n+                                      Node* obj,\n+                                      Node* adr,\n+                                      uint alias_idx,\n+                                      Node* val,\n+                                      const TypeOopPtr* val_type,\n+                                      Node* pre_val,\n+                                      BasicType bt) const {\n@@ -305,3 +808,3 @@\n-bool G1BarrierSetC2::g1_can_remove_post_barrier(GraphKit* kit,\n-                                                PhaseValues* phase, Node* store,\n-                                                Node* adr) const {\n+bool G1BarrierSetC2Early::g1_can_remove_post_barrier(GraphKit* kit,\n+                                                     PhaseValues* phase, Node* store,\n+                                                     Node* adr) const {\n@@ -339,9 +842,9 @@\n-void G1BarrierSetC2::g1_mark_card(GraphKit* kit,\n-                                  IdealKit& ideal,\n-                                  Node* card_adr,\n-                                  Node* oop_store,\n-                                  uint oop_alias_idx,\n-                                  Node* index,\n-                                  Node* index_adr,\n-                                  Node* buffer,\n-                                  const TypeFunc* tf) const {\n+void G1BarrierSetC2Early::g1_mark_card(GraphKit* kit,\n+                                       IdealKit& ideal,\n+                                       Node* card_adr,\n+                                       Node* oop_store,\n+                                       uint oop_alias_idx,\n+                                       Node* index,\n+                                       Node* index_adr,\n+                                       Node* buffer,\n+                                       const TypeFunc* tf) const {\n@@ -371,9 +874,9 @@\n-void G1BarrierSetC2::post_barrier(GraphKit* kit,\n-                                  Node* ctl,\n-                                  Node* oop_store,\n-                                  Node* obj,\n-                                  Node* adr,\n-                                  uint alias_idx,\n-                                  Node* val,\n-                                  BasicType bt,\n-                                  bool use_precise) const {\n+void G1BarrierSetC2Early::post_barrier(GraphKit* kit,\n+                                       Node* ctl,\n+                                       Node* oop_store,\n+                                       Node* obj,\n+                                       Node* adr,\n+                                       uint alias_idx,\n+                                       Node* val,\n+                                       BasicType bt,\n+                                       bool use_precise) const {\n@@ -499,2 +1002,2 @@\n-void G1BarrierSetC2::insert_pre_barrier(GraphKit* kit, Node* base_oop, Node* offset,\n-                                        Node* pre_val, bool need_mem_bar) const {\n+void G1BarrierSetC2Early::insert_pre_barrier(GraphKit* kit, Node* base_oop, Node* offset,\n+                                             Node* pre_val, bool need_mem_bar) const {\n@@ -595,1 +1098,1 @@\n-Node* G1BarrierSetC2::load_at_resolved(C2Access& access, const Type* val_type) const {\n+Node* G1BarrierSetC2Early::load_at_resolved(C2Access& access, const Type* val_type) const {\n@@ -664,2 +1167,2 @@\n-bool G1BarrierSetC2::is_gc_barrier_node(Node* node) const {\n-  if (CardTableBarrierSetC2::is_gc_barrier_node(node)) {\n+bool G1BarrierSetC2Early::is_gc_barrier_node(Node* node) const {\n+  if (node->Opcode() == Op_StoreCM) {\n@@ -679,1 +1182,1 @@\n-bool G1BarrierSetC2::is_g1_pre_val_load(Node* n) {\n+bool G1BarrierSetC2Early::is_g1_pre_val_load(Node* n) {\n@@ -709,1 +1212,26 @@\n-bool G1BarrierSetC2::is_gc_pre_barrier_node(Node *node) const {\n+void G1BarrierSetC2Early::clone(GraphKit* kit, Node* src, Node* dst, Node* size, bool is_array) const {\n+  BarrierSetC2::clone(kit, src, dst, size, is_array);\n+  const TypePtr* raw_adr_type = TypeRawPtr::BOTTOM;\n+\n+  \/\/ If necessary, emit some card marks afterwards.  (Non-arrays only.)\n+  bool card_mark = !is_array && !use_ReduceInitialCardMarks();\n+  if (card_mark) {\n+    assert(!is_array, \"\");\n+    \/\/ Put in store barrier for any and all oops we are sticking\n+    \/\/ into this object.  (We could avoid this if we could prove\n+    \/\/ that the object type contains no oop fields at all.)\n+    Node* no_particular_value = nullptr;\n+    Node* no_particular_field = nullptr;\n+    int raw_adr_idx = Compile::AliasIdxRaw;\n+    post_barrier(kit, kit->control(),\n+                 kit->memory(raw_adr_type),\n+                 dst,\n+                 no_particular_field,\n+                 raw_adr_idx,\n+                 no_particular_value,\n+                 T_OBJECT,\n+                 false);\n+  }\n+}\n+\n+bool G1BarrierSetC2Early::is_gc_pre_barrier_node(Node *node) const {\n@@ -713,1 +1241,1 @@\n-void G1BarrierSetC2::eliminate_gc_barrier(PhaseMacroExpand* macro, Node* node) const {\n+void G1BarrierSetC2Early::eliminate_gc_barrier(PhaseMacroExpand* macro, Node* node) const {\n@@ -798,1 +1326,1 @@\n-Node* G1BarrierSetC2::step_over_gc_barrier(Node* c) const {\n+Node* G1BarrierSetC2Early::step_over_gc_barrier(Node* c) const {\n@@ -834,1 +1362,1 @@\n-bool G1BarrierSetC2::has_cas_in_use_chain(Node *n) const {\n+bool G1BarrierSetC2Early::has_cas_in_use_chain(Node *n) const {\n@@ -864,1 +1392,1 @@\n-void G1BarrierSetC2::verify_pre_load(Node* marking_if, Unique_Node_List& loads \/*output*\/) const {\n+void G1BarrierSetC2Early::verify_pre_load(Node* marking_if, Unique_Node_List& loads \/*output*\/) const {\n@@ -909,1 +1437,1 @@\n-void G1BarrierSetC2::verify_no_safepoints(Compile* compile, Node* marking_check_if, const Unique_Node_List& loads) const {\n+void G1BarrierSetC2Early::verify_no_safepoints(Compile* compile, Node* marking_check_if, const Unique_Node_List& loads) const {\n@@ -963,1 +1491,1 @@\n-void G1BarrierSetC2::verify_gc_barriers(Compile* compile, CompilePhase phase) const {\n+void G1BarrierSetC2Early::verify_gc_barriers(Compile* compile, CompilePhase phase) const {\n@@ -1030,1 +1558,1 @@\n-bool G1BarrierSetC2::escape_add_to_con_graph(ConnectionGraph* conn_graph, PhaseGVN* gvn, Unique_Node_List* delayed_worklist, Node* n, uint opcode) const {\n+bool G1BarrierSetC2Early::escape_add_to_con_graph(ConnectionGraph* conn_graph, PhaseGVN* gvn, Unique_Node_List* delayed_worklist, Node* n, uint opcode) const {\n@@ -1058,0 +1586,2 @@\n+\n+#endif \/\/ G1_LATE_BARRIER_MIGRATION_SUPPORT\n","filename":"src\/hotspot\/share\/gc\/g1\/c2\/g1BarrierSetC2.cpp","additions":582,"deletions":52,"binary":false,"changes":634,"status":"modified"},{"patch":"@@ -34,0 +34,52 @@\n+const int G1C2BarrierPre         = 1;\n+const int G1C2BarrierPost        = 2;\n+const int G1C2BarrierPostNotNull = 4;\n+\n+class G1BarrierStubC2 : public BarrierStubC2 {\n+public:\n+  G1BarrierStubC2(const MachNode* node);\n+  virtual void emit_code(MacroAssembler& masm) = 0;\n+};\n+\n+class G1PreBarrierStubC2 : public G1BarrierStubC2 {\n+private:\n+  Register _obj;\n+  Register _pre_val;\n+  Register _thread;\n+  Register _tmp1;\n+  Register _tmp2;\n+\n+protected:\n+  G1PreBarrierStubC2(const MachNode* node);\n+\n+public:\n+  static bool needs_barrier(const MachNode* node);\n+  static G1PreBarrierStubC2* create(const MachNode* node);\n+  void initialize_registers(Register obj, Register pre_val, Register thread, Register tmp1, Register tmp2);\n+  Register obj() const;\n+  Register pre_val() const;\n+  Register thread() const;\n+  Register tmp1() const;\n+  Register tmp2() const;\n+  virtual void emit_code(MacroAssembler& masm);\n+};\n+\n+class G1PostBarrierStubC2 : public G1BarrierStubC2 {\n+private:\n+  Register _thread;\n+  Register _tmp1;\n+  Register _tmp2;\n+\n+protected:\n+  G1PostBarrierStubC2(const MachNode* node);\n+\n+public:\n+  static bool needs_barrier(const MachNode* node);\n+  static G1PostBarrierStubC2* create(const MachNode* node);\n+  void initialize_registers(Register thread, Register tmp1, Register tmp2);\n+  Register thread() const;\n+  Register tmp1() const;\n+  Register tmp2() const;\n+  virtual void emit_code(MacroAssembler& masm);\n+};\n+\n@@ -35,0 +87,40 @@\n+protected:\n+  bool g1_can_remove_pre_barrier(GraphKit* kit,\n+                                 PhaseValues* phase,\n+                                 Node* adr,\n+                                 BasicType bt,\n+                                 uint adr_idx) const;\n+\n+  bool g1_can_remove_post_barrier(GraphKit* kit,\n+                                  PhaseValues* phase, Node* store,\n+                                  Node* adr) const;\n+\n+  int get_store_barrier(C2Access& access) const;\n+\n+  virtual Node* load_at_resolved(C2Access& access, const Type* val_type) const;\n+  virtual Node* store_at_resolved(C2Access& access, C2AccessValue& val) const;\n+  virtual Node* atomic_cmpxchg_val_at_resolved(C2AtomicParseAccess& access, Node* expected_val,\n+                                                         Node* new_val, const Type* value_type) const;\n+  virtual Node* atomic_cmpxchg_bool_at_resolved(C2AtomicParseAccess& access, Node* expected_val,\n+                                                Node* new_val, const Type* value_type) const;\n+  virtual Node* atomic_xchg_at_resolved(C2AtomicParseAccess& access, Node* new_val, const Type* value_type) const;\n+\n+public:\n+  virtual void eliminate_gc_barrier(PhaseMacroExpand* macro, Node* node) const;\n+  virtual void eliminate_gc_barrier_data(Node* node) const;\n+  virtual bool expand_barriers(Compile* C, PhaseIterGVN& igvn) const;\n+  virtual uint estimated_barrier_size(const Node* node) const;\n+  virtual bool can_initialize_object(const StoreNode* store) const;\n+  virtual void clone_at_expansion(PhaseMacroExpand* phase,\n+                                  ArrayCopyNode* ac) const;\n+  virtual void* create_barrier_state(Arena* comp_arena) const;\n+  virtual void emit_stubs(CodeBuffer& cb) const;\n+  virtual void late_barrier_analysis() const;\n+\n+#ifndef PRODUCT\n+  virtual void dump_barrier_data(const MachNode* mach, outputStream* st) const;\n+#endif\n+};\n+\n+#if G1_LATE_BARRIER_MIGRATION_SUPPORT\n+class G1BarrierSetC2Early : public CardTableBarrierSetC2 {\n@@ -95,0 +187,1 @@\n+  virtual void clone(GraphKit* kit, Node* src, Node* dst, Node* size, bool is_array) const;\n@@ -106,0 +199,1 @@\n+#endif \/\/ G1_LATE_BARRIER_MIGRATION_SUPPORT\n","filename":"src\/hotspot\/share\/gc\/g1\/c2\/g1BarrierSetC2.hpp","additions":94,"deletions":0,"binary":false,"changes":94,"status":"modified"},{"patch":"@@ -246,0 +246,33 @@\n+\n+#if G1_LATE_BARRIER_MIGRATION_SUPPORT\n+  if (G1StressBarriers) {\n+    \/\/ Increase the frequency of concurrent marking (so that the pre-barrier is\n+    \/\/ not trivially skipped).\n+    if (FLAG_IS_DEFAULT(G1UseAdaptiveIHOP)) {\n+      FLAG_SET_ERGO(G1UseAdaptiveIHOP, false);\n+    }\n+    if (FLAG_IS_DEFAULT(InitiatingHeapOccupancyPercent)) {\n+      FLAG_SET_ERGO(InitiatingHeapOccupancyPercent, 0);\n+    }\n+    \/\/ Exercise both pre-barrier enqueue paths (inline and runtime) equally.\n+    if (FLAG_IS_DEFAULT(G1SATBBufferSize)) {\n+      FLAG_SET_ERGO(G1SATBBufferSize, 2);\n+    }\n+    \/\/ Increase the frequency of inter-regional objects in the post-barrier.\n+    if (FLAG_IS_DEFAULT(G1HeapRegionSize)) {\n+      FLAG_SET_ERGO(G1HeapRegionSize, G1HeapRegionBounds::min_size());\n+    }\n+    \/\/ Increase the frequency with which the post-barrier sees a clean card and\n+    \/\/ has to dirty it.\n+    if (FLAG_IS_DEFAULT(GCCardSizeInBytes)) {\n+      FLAG_SET_ERGO(GCCardSizeInBytes, MAX2(ObjectAlignmentInBytes, 128));\n+    }\n+    if (FLAG_IS_DEFAULT(G1RSetUpdatingPauseTimePercent)) {\n+      FLAG_SET_ERGO(G1RSetUpdatingPauseTimePercent, 0);\n+    }\n+    \/\/ Exercise both post-barrier dirtying paths (inline and runtime) equally.\n+    if (FLAG_IS_DEFAULT(G1UpdateBufferSize)) {\n+      FLAG_SET_ERGO(G1UpdateBufferSize, 2);\n+    }\n+  }\n+#endif \/\/ G1_LATE_BARRIER_MIGRATION_SUPPORT\n","filename":"src\/hotspot\/share\/gc\/g1\/g1Arguments.cpp","additions":33,"deletions":0,"binary":false,"changes":33,"status":"modified"},{"patch":"@@ -52,0 +52,3 @@\n+#if G1_LATE_BARRIER_MIGRATION_SUPPORT\n+class G1BarrierSetC2Early;\n+#endif\n@@ -56,0 +59,5 @@\n+#if G1_LATE_BARRIER_MIGRATION_SUPPORT\n+                      G1UseLateBarrierExpansion ?\n+                      make_barrier_set_c2<G1BarrierSetC2>() :\n+                      make_barrier_set_c2<G1BarrierSetC2Early>(),\n+#else\n@@ -57,0 +65,1 @@\n+#endif\n","filename":"src\/hotspot\/share\/gc\/g1\/g1BarrierSet.cpp","additions":9,"deletions":0,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -64,0 +64,8 @@\n+\n+JRT_LEAF(void, G1BarrierSetRuntime::clone(oopDesc* src, oopDesc* dst, size_t size))\n+  HeapAccess<>::clone(src, dst, size);\n+JRT_END\n+\n+address G1BarrierSetRuntime::clone_addr() {\n+  return reinterpret_cast<address>(clone);\n+}\n","filename":"src\/hotspot\/share\/gc\/g1\/g1BarrierSetRuntime.cpp","additions":8,"deletions":0,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -38,0 +38,2 @@\n+private:\n+  static void clone(oopDesc* src, oopDesc* dst, size_t size);\n@@ -49,0 +51,2 @@\n+\n+  static address clone_addr();\n","filename":"src\/hotspot\/share\/gc\/g1\/g1BarrierSetRuntime.hpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -90,0 +90,16 @@\n+\n+\/\/ Temporary flags to support the migration from early to late barrier expansion\n+\/\/ (see JEP 475) for all platforms. These flags are not intended to be\n+\/\/ integrated in the main JDK repository.\n+#if G1_LATE_BARRIER_MIGRATION_SUPPORT\n+#define G1_LATE_BARRIER_MIGRATION_SUPPORT_FLAGS(product)                    \\\n+                                                                            \\\n+  product(bool, G1UseLateBarrierExpansion, true,                            \\\n+          \"Expand G1 barriers late during C2 compilation\")                  \\\n+                                                                            \\\n+  product(bool, G1StressBarriers, false,                                    \\\n+          \"Configure G1 to exercise cold barrier paths\")\n+#else\n+#define G1_LATE_BARRIER_MIGRATION_SUPPORT_FLAGS(product)\n+#endif\n+\n@@ -342,1 +358,3 @@\n-                    constraint)\n+                    constraint)                                             \\\n+                                                                            \\\n+  G1_LATE_BARRIER_MIGRATION_SUPPORT_FLAGS(product)\n","filename":"src\/hotspot\/share\/gc\/g1\/g1_globals.hpp","additions":19,"deletions":1,"binary":false,"changes":20,"status":"modified"},{"patch":"@@ -112,0 +112,4 @@\n+uint8_t BarrierStubC2::barrier_data() const {\n+  return _node->barrier_data();\n+}\n+\n","filename":"src\/hotspot\/share\/gc\/shared\/c2\/barrierSetC2.cpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -257,0 +257,2 @@\n+  \/\/ High-level, GC-specific barrier flags.\n+  uint8_t barrier_data() const;\n@@ -343,0 +345,2 @@\n+  \/\/ Whether the given store can be used to initialize a newly allocated object.\n+  virtual bool can_initialize_object(const StoreNode* store) const { return true; }\n","filename":"src\/hotspot\/share\/gc\/shared\/c2\/barrierSetC2.hpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -128,25 +128,0 @@\n-void CardTableBarrierSetC2::clone(GraphKit* kit, Node* src, Node* dst, Node* size, bool is_array) const {\n-  BarrierSetC2::clone(kit, src, dst, size, is_array);\n-  const TypePtr* raw_adr_type = TypeRawPtr::BOTTOM;\n-\n-  \/\/ If necessary, emit some card marks afterwards.  (Non-arrays only.)\n-  bool card_mark = !is_array && !use_ReduceInitialCardMarks();\n-  if (card_mark) {\n-    assert(!is_array, \"\");\n-    \/\/ Put in store barrier for any and all oops we are sticking\n-    \/\/ into this object.  (We could avoid this if we could prove\n-    \/\/ that the object type contains no oop fields at all.)\n-    Node* no_particular_value = nullptr;\n-    Node* no_particular_field = nullptr;\n-    int raw_adr_idx = Compile::AliasIdxRaw;\n-    post_barrier(kit, kit->control(),\n-                 kit->memory(raw_adr_type),\n-                 dst,\n-                 no_particular_field,\n-                 raw_adr_idx,\n-                 no_particular_value,\n-                 T_OBJECT,\n-                 false);\n-  }\n-}\n-\n@@ -157,4 +132,0 @@\n-bool CardTableBarrierSetC2::is_gc_barrier_node(Node* node) const {\n-  return ModRefBarrierSetC2::is_gc_barrier_node(node) || node->Opcode() == Op_StoreCM;\n-}\n-\n","filename":"src\/hotspot\/share\/gc\/shared\/c2\/cardTableBarrierSetC2.cpp","additions":0,"deletions":29,"binary":false,"changes":29,"status":"modified"},{"patch":"@@ -45,2 +45,0 @@\n-  virtual void clone(GraphKit* kit, Node* src, Node* dst, Node* size, bool is_array) const;\n-  virtual bool is_gc_barrier_node(Node* node) const;\n","filename":"src\/hotspot\/share\/gc\/shared\/c2\/cardTableBarrierSetC2.hpp","additions":0,"deletions":2,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -238,0 +238,7 @@\n+    if (def->is_MachTemp()) {\n+      assert(!def->bottom_type()->isa_oop_ptr(),\n+             \"ADLC only assigns OOP types to MachTemp defs corresponding to xRegN operands\");\n+      \/\/ Exclude MachTemp definitions even if they are typed as oops.\n+      continue;\n+    }\n+\n","filename":"src\/hotspot\/share\/opto\/buildOopMap.cpp","additions":7,"deletions":0,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -1597,0 +1597,6 @@\n+    if (Matcher::is_encode_and_store_pattern(n, m)) {\n+      \/\/ Make it possible to match \"encode and store\" patterns, regardless of\n+      \/\/ whether the encode operation is pinned to a control node (e.g. by\n+      \/\/ CastPP node removal in final graph reshaping).\n+      return false;\n+    }\n@@ -1820,2 +1826,0 @@\n-    assert(C->node_arena()->contains(s->_leaf) || !has_new_node(s->_leaf),\n-           \"duplicating node that's already been matched\");\n@@ -2836,0 +2840,7 @@\n+bool Matcher::is_encode_and_store_pattern(const Node* n, const Node* m) {\n+  return n != nullptr &&\n+    m != nullptr &&\n+    n->Opcode() == Op_StoreN &&\n+    m->is_EncodeP();\n+}\n+\n","filename":"src\/hotspot\/share\/opto\/matcher.cpp","additions":13,"deletions":2,"binary":false,"changes":15,"status":"modified"},{"patch":"@@ -388,0 +388,2 @@\n+  static bool is_encode_and_store_pattern(const Node* n, const Node* m);\n+\n","filename":"src\/hotspot\/share\/opto\/matcher.hpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -4641,0 +4641,5 @@\n+  BarrierSetC2* bs = BarrierSet::barrier_set()->barrier_set_c2();\n+  if ((st->Opcode() == Op_StoreP || st->Opcode() == Op_StoreN) &&\n+      !bs->can_initialize_object(st)) {\n+    return FAIL;\n+  }\n","filename":"src\/hotspot\/share\/opto\/memnode.cpp","additions":5,"deletions":0,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -2025,0 +2025,2 @@\n+      assert(n->in(1)->as_Mach()->barrier_data() == 0,\n+             \"Implicit null checks on memory accesses with barriers are not yet supported\");\n","filename":"src\/hotspot\/share\/opto\/output.cpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -34,0 +34,8 @@\n+\/\/ G1_LATE_BARRIER_MIGRATION_SUPPORT enables temporary support for migrating\n+\/\/ from early to late barrier expansion (see JEP 475) for all platforms.\n+\/\/ This support is not intended to be integrated in the main JDK repository.\n+#ifdef G1_LATE_BARRIER_MIGRATION_SUPPORT\n+#error \"G1_LATE_BARRIER_MIGRATION_SUPPORT already defined\"\n+#endif\n+#define G1_LATE_BARRIER_MIGRATION_SUPPORT 0\n+\n","filename":"src\/hotspot\/share\/runtime\/globals_shared.hpp","additions":8,"deletions":0,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -264,6 +264,0 @@\n-            \/\/ a card mark volatile barrier should be generated\n-            \/\/ before the card mark strb\n-            \/\/\n-            \/\/ following the fix for 8225776 the G1 barrier is now\n-            \/\/ scheduled out of line after the membar volatile and\n-            \/\/ and subsequent return\n@@ -274,4 +268,1 @@\n-                \"ret\",\n-                \"membar_volatile\",\n-                \"dmb ish\",\n-                \"strb\"\n+                \"ret\"\n@@ -335,6 +326,0 @@\n-            \/\/ a card mark volatile barrier should be generated\n-            \/\/ before the card mark strb\n-            \/\/\n-            \/\/ following the fix for 8225776 the G1 barrier is now\n-            \/\/ scheduled out of line after the membar acquire and\n-            \/\/ and subsequent return\n@@ -345,4 +330,1 @@\n-                \"ret\",\n-                \"membar_volatile\",\n-                \"dmb ish\",\n-                \"strb\"\n+                \"ret\"\n@@ -421,6 +403,0 @@\n-            \/\/ a card mark volatile barrier should be generated\n-            \/\/ before the card mark strb\n-            \/\/\n-            \/\/ following the fix for 8225776 the G1 barrier is now\n-            \/\/ scheduled out of line after the membar acquire and\n-            \/\/ and subsequent return\n@@ -431,4 +407,1 @@\n-                \"ret\",\n-                \"membar_volatile\",\n-                \"dmb ish\",\n-                \"strb\"\n+                \"ret\"\n@@ -487,6 +460,0 @@\n-            \/\/ a card mark volatile barrier should be generated\n-            \/\/ before the card mark strb\n-            \/\/\n-            \/\/ following the fix for 8225776 the G1 barrier is now\n-            \/\/ scheduled out of line after the membar acquire and\n-            \/\/ and subsequent return\n@@ -497,4 +464,1 @@\n-                \"ret\",\n-                \"membar_volatile\",\n-                \"dmb ish\",\n-                \"strb\"\n+                \"ret\"\n","filename":"test\/hotspot\/jtreg\/compiler\/c2\/aarch64\/TestVolatiles.java","additions":4,"deletions":40,"binary":false,"changes":44,"status":"modified"},{"patch":"@@ -1358,3 +1358,6 @@\n-    @IR(counts = { IRNode.ALLOC, \"1\" })\n-    \/\/ The last allocation won't be reduced because it would cause the creation\n-    \/\/ of a nested SafePointScalarMergeNode.\n+    \/\/ Using G1, all allocations are reduced.\n+    @IR(applyIf = {\"UseG1GC\", \"true\"}, failOn = { IRNode.ALLOC })\n+    \/\/ Otherwise, the last allocation won't be reduced because it would cause\n+    \/\/ the creation of a nested SafePointScalarMergeNode. This is caused by the\n+    \/\/ store barrier corresponding to 'C.other = B'.\n+    @IR(applyIf = {\"UseG1GC\", \"false\"}, counts = { IRNode.ALLOC, \"1\" })\n","filename":"test\/hotspot\/jtreg\/compiler\/c2\/irTests\/scalarReplacement\/AllocationMergesTests.java","additions":6,"deletions":3,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -0,0 +1,584 @@\n+\/*\n+ * Copyright (c) 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+package compiler.gcbarriers;\n+\n+import compiler.lib.ir_framework.*;\n+import java.lang.invoke.VarHandle;\n+import java.lang.invoke.MethodHandles;\n+import java.lang.ref.Reference;\n+import java.lang.ref.ReferenceQueue;\n+import java.lang.ref.SoftReference;\n+import java.lang.ref.WeakReference;\n+import jdk.test.lib.Asserts;\n+\n+\/**\n+ * @test\n+ * @summary Test that G1 barriers are generated and optimized as expected.\n+ * @library \/test\/lib \/\n+ * @requires vm.gc.G1\n+ * @run driver compiler.gcbarriers.TestG1BarrierGeneration\n+ *\/\n+\n+public class TestG1BarrierGeneration {\n+    static final String PRE_ONLY = \"pre\";\n+    static final String POST_ONLY = \"post\";\n+    static final String POST_ONLY_NOT_NULL = \"post notnull\";\n+    static final String PRE_AND_POST = \"pre post\";\n+    static final String PRE_AND_POST_NOT_NULL = \"pre post notnull\";\n+\n+    static class Outer {\n+        Object f;\n+    }\n+\n+    static class OuterWithFewFields implements Cloneable {\n+        Object f1;\n+        Object f2;\n+        public Object clone() throws CloneNotSupportedException {\n+            return super.clone();\n+        }\n+    }\n+\n+    static class OuterWithManyFields implements Cloneable {\n+        Object f1;\n+        Object f2;\n+        Object f3;\n+        Object f4;\n+        Object f5;\n+        Object f6;\n+        Object f7;\n+        Object f8;\n+        Object f9;\n+        Object f10;\n+        public Object clone() throws CloneNotSupportedException {\n+            return super.clone();\n+        }\n+    }\n+\n+    static final VarHandle fVarHandle;\n+    static {\n+        MethodHandles.Lookup l = MethodHandles.lookup();\n+        try {\n+            fVarHandle = l.findVarHandle(Outer.class, \"f\", Object.class);\n+        } catch (Exception e) {\n+            throw new Error(e);\n+        }\n+    }\n+\n+    public static void main(String[] args) {\n+        TestFramework framework = new TestFramework();\n+        Scenario[] scenarios = new Scenario[2*2];\n+        int scenarioIndex = 0;\n+        for (int i = 0; i < 2; i++) {\n+            for (int j = 0; j < 2; j++) {\n+                scenarios[scenarioIndex] =\n+                    new Scenario(scenarioIndex,\n+                                 \"-XX:CompileCommand=inline,java.lang.ref.*::*\",\n+                                 \"-XX:\" + (i == 0 ? \"-\" : \"+\") + \"UseCompressedOops\",\n+                                 \"-XX:\" + (j == 0 ? \"-\" : \"+\") + \"ReduceInitialCardMarks\");\n+                scenarioIndex++;\n+            }\n+        }\n+        framework.addScenarios(scenarios);\n+        framework.start();\n+    }\n+\n+    @Test\n+    @IR(applyIf = {\"UseCompressedOops\", \"false\"},\n+        counts = {IRNode.G1_STORE_P_WITH_BARRIER_FLAG, PRE_AND_POST, \"1\"},\n+        phase = CompilePhase.FINAL_CODE)\n+    @IR(applyIf = {\"UseCompressedOops\", \"true\"},\n+        counts = {IRNode.G1_ENCODE_P_AND_STORE_N_WITH_BARRIER_FLAG, PRE_AND_POST, \"1\"},\n+        phase = CompilePhase.FINAL_CODE)\n+    public static void testStore(Outer o, Object o1) {\n+        o.f = o1;\n+    }\n+\n+    @Test\n+    @IR(applyIf = {\"UseCompressedOops\", \"false\"},\n+        counts = {IRNode.G1_STORE_P_WITH_BARRIER_FLAG, PRE_ONLY, \"1\"},\n+        phase = CompilePhase.FINAL_CODE)\n+    @IR(applyIf = {\"UseCompressedOops\", \"true\"},\n+        counts = {IRNode.G1_STORE_N_WITH_BARRIER_FLAG, PRE_ONLY, \"1\"},\n+        phase = CompilePhase.FINAL_CODE)\n+    public static void testStoreNull(Outer o) {\n+        o.f = null;\n+    }\n+\n+    @Test\n+    @IR(applyIf = {\"UseCompressedOops\", \"false\"},\n+        counts = {IRNode.G1_STORE_P_WITH_BARRIER_FLAG, PRE_ONLY, \"1\"},\n+        phase = CompilePhase.FINAL_CODE)\n+    @IR(applyIf = {\"UseCompressedOops\", \"true\"},\n+        counts = {IRNode.G1_STORE_N_WITH_BARRIER_FLAG, PRE_ONLY, \"1\"},\n+        phase = CompilePhase.FINAL_CODE)\n+    public static void testStoreObfuscatedNull(Outer o, Object o1) {\n+        Object o2 = o1;\n+        for (int i = 0; i < 4; i++) {\n+            if ((i % 2) == 0) {\n+                o2 = null;\n+            }\n+        }\n+        \/\/ o2 is null here, but this is only known to C2 after applying some\n+        \/\/ optimizations (loop unrolling, IGVN).\n+        o.f = o2;\n+    }\n+\n+    @Test\n+    @IR(applyIf = {\"UseCompressedOops\", \"false\"},\n+        counts = {IRNode.G1_STORE_P_WITH_BARRIER_FLAG, PRE_AND_POST_NOT_NULL, \"1\"},\n+        phase = CompilePhase.FINAL_CODE)\n+    @IR(applyIf = {\"UseCompressedOops\", \"true\"},\n+        counts = {IRNode.G1_ENCODE_P_AND_STORE_N_WITH_BARRIER_FLAG, PRE_AND_POST_NOT_NULL, \"1\"},\n+        phase = CompilePhase.FINAL_CODE)\n+    public static void testStoreNotNull(Outer o, Object o1) {\n+        if (o1.hashCode() == 42) {\n+            return;\n+        }\n+        o.f = o1;\n+    }\n+\n+    @Test\n+    @IR(applyIf = {\"UseCompressedOops\", \"false\"},\n+        counts = {IRNode.G1_STORE_P_WITH_BARRIER_FLAG, PRE_AND_POST, \"2\"},\n+        phase = CompilePhase.FINAL_CODE)\n+    @IR(applyIf = {\"UseCompressedOops\", \"true\"},\n+        counts = {IRNode.G1_ENCODE_P_AND_STORE_N_WITH_BARRIER_FLAG, PRE_AND_POST, \"2\"},\n+        phase = CompilePhase.FINAL_CODE)\n+    public static void testStoreTwice(Outer o, Outer p, Object o1) {\n+        o.f = o1;\n+        p.f = o1;\n+    }\n+\n+    @Test\n+    @IR(applyIfAnd = {\"UseCompressedOops\", \"false\", \"ReduceInitialCardMarks\", \"false\"},\n+        counts = {IRNode.G1_STORE_P_WITH_BARRIER_FLAG, POST_ONLY, \"1\"},\n+        phase = CompilePhase.FINAL_CODE)\n+    @IR(applyIfAnd = {\"UseCompressedOops\", \"true\", \"ReduceInitialCardMarks\", \"false\"},\n+        counts = {IRNode.G1_ENCODE_P_AND_STORE_N_WITH_BARRIER_FLAG, POST_ONLY, \"1\"},\n+        phase = CompilePhase.FINAL_CODE)\n+    @IR(applyIfAnd = {\"UseCompressedOops\", \"false\", \"ReduceInitialCardMarks\", \"true\"},\n+        failOn = {IRNode.G1_STORE_P},\n+        phase = CompilePhase.FINAL_CODE)\n+    @IR(applyIfAnd = {\"UseCompressedOops\", \"true\", \"ReduceInitialCardMarks\", \"true\"},\n+        failOn = {IRNode.G1_STORE_N, IRNode.G1_ENCODE_P_AND_STORE_N},\n+        phase = CompilePhase.FINAL_CODE)\n+    public static Outer testStoreOnNewObject(Object o1) {\n+        Outer o = new Outer();\n+        o.f = o1;\n+        return o;\n+    }\n+\n+    @Test\n+    @IR(failOn = {IRNode.STORE_P, IRNode.STORE_N},\n+        phase = CompilePhase.BEFORE_MACRO_EXPANSION)\n+    public static Outer testStoreNullOnNewObject() {\n+        Outer o = new Outer();\n+        o.f = null;\n+        return o;\n+    }\n+\n+    @Test\n+    @IR(applyIfAnd = {\"UseCompressedOops\", \"false\", \"ReduceInitialCardMarks\", \"false\"},\n+        counts = {IRNode.G1_STORE_P_WITH_BARRIER_FLAG, POST_ONLY_NOT_NULL, \"1\"},\n+        phase = CompilePhase.FINAL_CODE)\n+    @IR(applyIfAnd = {\"UseCompressedOops\", \"true\", \"ReduceInitialCardMarks\", \"false\"},\n+        counts = {IRNode.G1_ENCODE_P_AND_STORE_N_WITH_BARRIER_FLAG, POST_ONLY_NOT_NULL, \"1\"},\n+        phase = CompilePhase.FINAL_CODE)\n+    @IR(applyIfAnd = {\"UseCompressedOops\", \"false\", \"ReduceInitialCardMarks\", \"true\"},\n+        failOn = {IRNode.G1_STORE_P},\n+        phase = CompilePhase.FINAL_CODE)\n+    @IR(applyIfAnd = {\"UseCompressedOops\", \"true\", \"ReduceInitialCardMarks\", \"true\"},\n+        failOn = {IRNode.G1_STORE_N, IRNode.G1_ENCODE_P_AND_STORE_N},\n+        phase = CompilePhase.FINAL_CODE)\n+    public static Outer testStoreNotNullOnNewObject(Object o1) {\n+        if (o1.hashCode() == 42) {\n+            return null;\n+        }\n+        Outer o = new Outer();\n+        o.f = o1;\n+        return o;\n+    }\n+\n+    @Run(test = {\"testStore\",\n+                 \"testStoreNull\",\n+                 \"testStoreObfuscatedNull\",\n+                 \"testStoreNotNull\",\n+                 \"testStoreTwice\",\n+                 \"testStoreOnNewObject\",\n+                 \"testStoreNotNullOnNewObject\"})\n+    public void runStoreTests() {\n+        {\n+            Outer o = new Outer();\n+            Object o1 = new Object();\n+            testStore(o, o1);\n+            Asserts.assertEquals(o1, o.f);\n+        }\n+        {\n+            Outer o = new Outer();\n+            testStoreNull(o);\n+            Asserts.assertNull(o.f);\n+        }\n+        {\n+            Outer o = new Outer();\n+            Object o1 = new Object();\n+            testStoreObfuscatedNull(o, o1);\n+            Asserts.assertNull(o.f);\n+        }\n+        {\n+            Outer o = new Outer();\n+            Object o1 = new Object();\n+            testStoreNotNull(o, o1);\n+            Asserts.assertEquals(o1, o.f);\n+        }\n+        {\n+            Outer o = new Outer();\n+            Outer p = new Outer();\n+            Object o1 = new Object();\n+            testStoreTwice(o, p, o1);\n+            Asserts.assertEquals(o1, o.f);\n+            Asserts.assertEquals(o1, p.f);\n+        }\n+        {\n+            Object o1 = new Object();\n+            Outer o = testStoreOnNewObject(o1);\n+            Asserts.assertEquals(o1, o.f);\n+        }\n+        {\n+            Outer o = testStoreNullOnNewObject();\n+            Asserts.assertNull(o.f);\n+        }\n+        {\n+            Object o1 = new Object();\n+            Outer o = testStoreNotNullOnNewObject(o1);\n+            Asserts.assertEquals(o1, o.f);\n+        }\n+    }\n+\n+    @Test\n+    @IR(applyIf = {\"UseCompressedOops\", \"false\"},\n+        counts = {IRNode.G1_STORE_P_WITH_BARRIER_FLAG, PRE_AND_POST, \"1\"},\n+        phase = CompilePhase.FINAL_CODE)\n+    @IR(applyIf = {\"UseCompressedOops\", \"true\"},\n+        counts = {IRNode.G1_ENCODE_P_AND_STORE_N_WITH_BARRIER_FLAG, PRE_AND_POST, \"1\"},\n+        phase = CompilePhase.FINAL_CODE)\n+    public static void testArrayStore(Object[] a, int index, Object o1) {\n+        a[index] = o1;\n+    }\n+\n+    @Test\n+    @IR(applyIf = {\"UseCompressedOops\", \"false\"},\n+        counts = {IRNode.G1_STORE_P_WITH_BARRIER_FLAG, PRE_ONLY, \"1\"},\n+        phase = CompilePhase.FINAL_CODE)\n+    @IR(applyIf = {\"UseCompressedOops\", \"true\"},\n+        counts = {IRNode.G1_STORE_N_WITH_BARRIER_FLAG, PRE_ONLY, \"1\"},\n+        phase = CompilePhase.FINAL_CODE)\n+    public static void testArrayStoreNull(Object[] a, int index) {\n+        a[index] = null;\n+    }\n+\n+    @Test\n+    @IR(applyIf = {\"UseCompressedOops\", \"false\"},\n+        counts = {IRNode.G1_STORE_P_WITH_BARRIER_FLAG, PRE_AND_POST_NOT_NULL, \"1\"},\n+        phase = CompilePhase.FINAL_CODE)\n+    @IR(applyIf = {\"UseCompressedOops\", \"true\"},\n+        counts = {IRNode.G1_ENCODE_P_AND_STORE_N_WITH_BARRIER_FLAG, PRE_AND_POST_NOT_NULL, \"1\"},\n+        phase = CompilePhase.FINAL_CODE)\n+    public static void testArrayStoreNotNull(Object[] a, int index, Object o1) {\n+        if (o1.hashCode() == 42) {\n+            return;\n+        }\n+        a[index] = o1;\n+    }\n+\n+    @Test\n+    @IR(applyIf = {\"UseCompressedOops\", \"false\"},\n+        counts = {IRNode.G1_STORE_P_WITH_BARRIER_FLAG, PRE_AND_POST, \"2\"},\n+        phase = CompilePhase.FINAL_CODE)\n+    @IR(applyIf = {\"UseCompressedOops\", \"true\"},\n+        counts = {IRNode.G1_ENCODE_P_AND_STORE_N_WITH_BARRIER_FLAG, PRE_AND_POST, \"2\"},\n+        phase = CompilePhase.FINAL_CODE)\n+    public static void testArrayStoreTwice(Object[] a, Object[] b, int index, Object o1) {\n+        a[index] = o1;\n+        b[index] = o1;\n+    }\n+\n+    @Test\n+    @IR(applyIfAnd = {\"UseCompressedOops\", \"false\", \"ReduceInitialCardMarks\", \"true\"},\n+        failOn = {IRNode.G1_STORE_P},\n+        phase = CompilePhase.FINAL_CODE)\n+    @IR(applyIfAnd = {\"UseCompressedOops\", \"true\", \"ReduceInitialCardMarks\", \"true\"},\n+        failOn = {IRNode.G1_STORE_N, IRNode.G1_ENCODE_P_AND_STORE_N},\n+        phase = CompilePhase.FINAL_CODE)\n+    public static Object[] testStoreOnNewArray(Object o1) {\n+        Object[] a = new Object[10];\n+        \/\/ The index needs to be concrete for C2 to detect that it is safe to\n+        \/\/ remove the pre-barrier.\n+        a[4] = o1;\n+        return a;\n+    }\n+\n+    @Run(test = {\"testArrayStore\",\n+                 \"testArrayStoreNull\",\n+                 \"testArrayStoreNotNull\",\n+                 \"testArrayStoreTwice\",\n+                 \"testStoreOnNewArray\"})\n+    public void runArrayStoreTests() {\n+        {\n+            Object[] a = new Object[10];\n+            Object o1 = new Object();\n+            testArrayStore(a, 4, o1);\n+            Asserts.assertEquals(o1, a[4]);\n+        }\n+        {\n+            Object[] a = new Object[10];\n+            testArrayStoreNull(a, 4);\n+            Asserts.assertNull(a[4]);\n+        }\n+        {\n+            Object[] a = new Object[10];\n+            Object o1 = new Object();\n+            testArrayStoreNotNull(a, 4, o1);\n+            Asserts.assertEquals(o1, a[4]);\n+        }\n+        {\n+            Object[] a = new Object[10];\n+            Object[] b = new Object[10];\n+            Object o1 = new Object();\n+            testArrayStoreTwice(a, b, 4, o1);\n+            Asserts.assertEquals(o1, a[4]);\n+            Asserts.assertEquals(o1, b[4]);\n+        }\n+        {\n+            Object o1 = new Object();\n+            Object[] a = testStoreOnNewArray(o1);\n+            Asserts.assertEquals(o1, a[4]);\n+        }\n+    }\n+\n+    @Test\n+    public static Object[] testCloneArrayOfObjects(Object[] a) {\n+        Object[] a1 = null;\n+        try {\n+            a1 = a.clone();\n+        } catch (Exception e) {}\n+        return a1;\n+    }\n+\n+    @Test\n+    @IR(applyIf = {\"ReduceInitialCardMarks\", \"true\"},\n+        failOn = {IRNode.G1_STORE_P, IRNode.G1_STORE_N, IRNode.G1_ENCODE_P_AND_STORE_N},\n+        phase = CompilePhase.FINAL_CODE)\n+    @IR(applyIfAnd = {\"ReduceInitialCardMarks\", \"false\", \"UseCompressedOops\", \"false\"},\n+        counts = {IRNode.G1_STORE_P_WITH_BARRIER_FLAG, POST_ONLY, \"2\"},\n+        phase = CompilePhase.FINAL_CODE)\n+    @IR(applyIfAnd = {\"ReduceInitialCardMarks\", \"false\", \"UseCompressedOops\", \"true\"},\n+        counts = {IRNode.G1_STORE_N_WITH_BARRIER_FLAG, POST_ONLY, \"2\"},\n+        phase = CompilePhase.FINAL_CODE)\n+    public static OuterWithFewFields testCloneObjectWithFewFields(OuterWithFewFields o) {\n+        Object o1 = null;\n+        try {\n+            o1 = o.clone();\n+        } catch (Exception e) {}\n+        return (OuterWithFewFields)o1;\n+    }\n+\n+    @Test\n+    @IR(applyIf = {\"ReduceInitialCardMarks\", \"true\"},\n+        counts = {IRNode.CALL_OF, \"jlong_disjoint_arraycopy\", \"1\"})\n+    @IR(applyIf = {\"ReduceInitialCardMarks\", \"false\"},\n+        counts = {IRNode.CALL_OF, \"G1BarrierSetRuntime::clone\", \"1\"})\n+    public static OuterWithManyFields testCloneObjectWithManyFields(OuterWithManyFields o) {\n+        Object o1 = null;\n+        try {\n+            o1 = o.clone();\n+        } catch (Exception e) {}\n+        return (OuterWithManyFields)o1;\n+    }\n+\n+    @Run(test = {\"testCloneArrayOfObjects\",\n+                 \"testCloneObjectWithFewFields\",\n+                 \"testCloneObjectWithManyFields\"})\n+    public void runCloneTests() {\n+        {\n+            Object o1 = new Object();\n+            Object[] a = new Object[4];\n+            for (int i = 0; i < 4; i++) {\n+                a[i] = o1;\n+            }\n+            Object[] a1 = testCloneArrayOfObjects(a);\n+            for (int i = 0; i < 4; i++) {\n+                Asserts.assertEquals(o1, a1[i]);\n+            }\n+        }\n+        {\n+            Object a = new Object();\n+            Object b = new Object();\n+            OuterWithFewFields o = new OuterWithFewFields();\n+            o.f1 = a;\n+            o.f2 = b;\n+            OuterWithFewFields o1 = testCloneObjectWithFewFields(o);\n+            Asserts.assertEquals(a, o1.f1);\n+            Asserts.assertEquals(b, o1.f2);\n+        }\n+        {\n+            Object a = new Object();\n+            Object b = new Object();\n+            Object c = new Object();\n+            Object d = new Object();\n+            Object e = new Object();\n+            Object f = new Object();\n+            Object g = new Object();\n+            Object h = new Object();\n+            Object i = new Object();\n+            Object j = new Object();\n+            OuterWithManyFields o = new OuterWithManyFields();\n+            o.f1 = a;\n+            o.f2 = b;\n+            o.f3 = c;\n+            o.f4 = d;\n+            o.f5 = e;\n+            o.f6 = f;\n+            o.f7 = g;\n+            o.f8 = h;\n+            o.f9 = i;\n+            o.f10 = j;\n+            OuterWithManyFields o1 = testCloneObjectWithManyFields(o);\n+            Asserts.assertEquals(a, o1.f1);\n+            Asserts.assertEquals(b, o1.f2);\n+            Asserts.assertEquals(c, o1.f3);\n+            Asserts.assertEquals(d, o1.f4);\n+            Asserts.assertEquals(e, o1.f5);\n+            Asserts.assertEquals(f, o1.f6);\n+            Asserts.assertEquals(g, o1.f7);\n+            Asserts.assertEquals(h, o1.f8);\n+            Asserts.assertEquals(i, o1.f9);\n+            Asserts.assertEquals(j, o1.f10);\n+        }\n+    }\n+\n+    @Test\n+    @IR(applyIf = {\"UseCompressedOops\", \"false\"},\n+        counts = {IRNode.G1_COMPARE_AND_EXCHANGE_P_WITH_BARRIER_FLAG, PRE_AND_POST, \"1\"},\n+        phase = CompilePhase.FINAL_CODE)\n+    @IR(applyIf = {\"UseCompressedOops\", \"true\"},\n+        counts = {IRNode.G1_COMPARE_AND_EXCHANGE_N_WITH_BARRIER_FLAG, PRE_AND_POST, \"1\"},\n+        phase = CompilePhase.FINAL_CODE)\n+    static Object testCompareAndExchange(Outer o, Object oldVal, Object newVal) {\n+        return fVarHandle.compareAndExchange(o, oldVal, newVal);\n+    }\n+\n+    @Test\n+    @IR(applyIf = {\"UseCompressedOops\", \"false\"},\n+        counts = {IRNode.G1_COMPARE_AND_SWAP_P_WITH_BARRIER_FLAG, PRE_AND_POST, \"1\"},\n+        phase = CompilePhase.FINAL_CODE)\n+    @IR(applyIf = {\"UseCompressedOops\", \"true\"},\n+        counts = {IRNode.G1_COMPARE_AND_SWAP_N_WITH_BARRIER_FLAG, PRE_AND_POST, \"1\"},\n+        phase = CompilePhase.FINAL_CODE)\n+    static boolean testCompareAndSwap(Outer o, Object oldVal, Object newVal) {\n+        return fVarHandle.compareAndSet(o, oldVal, newVal);\n+    }\n+\n+    @Test\n+    @IR(applyIf = {\"UseCompressedOops\", \"false\"},\n+        counts = {IRNode.G1_GET_AND_SET_P_WITH_BARRIER_FLAG, PRE_AND_POST, \"1\"},\n+        phase = CompilePhase.FINAL_CODE)\n+    @IR(applyIf = {\"UseCompressedOops\", \"true\"},\n+        counts = {IRNode.G1_GET_AND_SET_N_WITH_BARRIER_FLAG, PRE_AND_POST, \"1\"},\n+        phase = CompilePhase.FINAL_CODE)\n+    static Object testGetAndSet(Outer o, Object newVal) {\n+        return fVarHandle.getAndSet(o, newVal);\n+    }\n+\n+    @Run(test = {\"testCompareAndExchange\",\n+                 \"testCompareAndSwap\",\n+                 \"testGetAndSet\"})\n+    public void runAtomicTests() {\n+        {\n+            Outer o = new Outer();\n+            Object oldVal = new Object();\n+            o.f = oldVal;\n+            Object newVal = new Object();\n+            Object oldVal2 = testCompareAndExchange(o, oldVal, newVal);\n+            Asserts.assertEquals(oldVal, oldVal2);\n+            Asserts.assertEquals(o.f, newVal);\n+        }\n+        {\n+            Outer o = new Outer();\n+            Object oldVal = new Object();\n+            o.f = oldVal;\n+            Object newVal = new Object();\n+            boolean b = testCompareAndSwap(o, oldVal, newVal);\n+            Asserts.assertTrue(b);\n+            Asserts.assertEquals(o.f, newVal);\n+        }\n+        {\n+            Outer o = new Outer();\n+            Object oldVal = new Object();\n+            o.f = oldVal;\n+            Object newVal = new Object();\n+            Object oldVal2 = testGetAndSet(o, newVal);\n+            Asserts.assertEquals(oldVal, oldVal2);\n+            Asserts.assertEquals(o.f, newVal);\n+        }\n+    }\n+\n+    @Test\n+    @IR(applyIf = {\"UseCompressedOops\", \"false\"},\n+        counts = {IRNode.G1_LOAD_P_WITH_BARRIER_FLAG, PRE_ONLY, \"1\"},\n+        phase = CompilePhase.FINAL_CODE)\n+    @IR(applyIf = {\"UseCompressedOops\", \"true\"},\n+        counts = {IRNode.G1_LOAD_N_WITH_BARRIER_FLAG, PRE_ONLY, \"1\"},\n+        phase = CompilePhase.FINAL_CODE)\n+    static Object testLoadSoftReference(SoftReference<Object> ref) {\n+        return ref.get();\n+    }\n+\n+    @Test\n+    @IR(applyIf = {\"UseCompressedOops\", \"false\"},\n+        counts = {IRNode.G1_LOAD_P_WITH_BARRIER_FLAG, PRE_ONLY, \"1\"},\n+        phase = CompilePhase.FINAL_CODE)\n+    @IR(applyIf = {\"UseCompressedOops\", \"true\"},\n+        counts = {IRNode.G1_LOAD_N_WITH_BARRIER_FLAG, PRE_ONLY, \"1\"},\n+        phase = CompilePhase.FINAL_CODE)\n+    static Object testLoadWeakReference(WeakReference<Object> ref) {\n+        return ref.get();\n+    }\n+\n+    @Run(test = {\"testLoadSoftReference\",\n+                 \"testLoadWeakReference\"})\n+    public void runReferenceTests() {\n+        {\n+            Object o1 = new Object();\n+            SoftReference<Object> sref = new SoftReference<Object>(o1);\n+            Object o2 = testLoadSoftReference(sref);\n+            Asserts.assertTrue(o2 == o1 || o2 == null);\n+        }\n+        {\n+            Object o1 = new Object();\n+            WeakReference<Object> wref = new WeakReference<Object>(o1);\n+            Object o2 = testLoadWeakReference(wref);\n+            Asserts.assertTrue(o2 == o1 || o2 == null);\n+        }\n+    }\n+}\n","filename":"test\/hotspot\/jtreg\/compiler\/gcbarriers\/TestG1BarrierGeneration.java","additions":584,"deletions":0,"binary":false,"changes":584,"status":"added"},{"patch":"@@ -361,0 +361,5 @@\n+    public static final String CALL_OF = COMPOSITE_PREFIX + \"CALL_OF\" + POSTFIX;\n+    static {\n+        callOfNodes(CALL_OF, \"Call.*\");\n+    }\n+\n@@ -569,0 +574,86 @@\n+    public static final String G1_COMPARE_AND_EXCHANGE_N_WITH_BARRIER_FLAG = COMPOSITE_PREFIX + \"G1_COMPARE_AND_EXCHANGE_N_WITH_BARRIER_FLAG\" + POSTFIX;\n+    static {\n+        String regex = START + \"g1CompareAndExchangeN\\\\S*\" + MID + \"barrier\\\\(\\\\s*\" + IS_REPLACED + \"\\\\s*\\\\)\" + END;\n+        machOnly(G1_COMPARE_AND_EXCHANGE_N_WITH_BARRIER_FLAG, regex);\n+    }\n+\n+    public static final String G1_COMPARE_AND_EXCHANGE_P_WITH_BARRIER_FLAG = COMPOSITE_PREFIX + \"G1_COMPARE_AND_EXCHANGE_P_WITH_BARRIER_FLAG\" + POSTFIX;\n+    static {\n+        String regex = START + \"g1CompareAndExchangeP\\\\S*\" + MID + \"barrier\\\\(\\\\s*\" + IS_REPLACED + \"\\\\s*\\\\)\" + END;\n+        machOnly(G1_COMPARE_AND_EXCHANGE_P_WITH_BARRIER_FLAG, regex);\n+    }\n+\n+    public static final String G1_COMPARE_AND_SWAP_N_WITH_BARRIER_FLAG = COMPOSITE_PREFIX + \"G1_COMPARE_AND_SWAP_N_WITH_BARRIER_FLAG\" + POSTFIX;\n+    static {\n+        String regex = START + \"g1CompareAndSwapN\\\\S*\" + MID + \"barrier\\\\(\\\\s*\" + IS_REPLACED + \"\\\\s*\\\\)\" + END;\n+        machOnly(G1_COMPARE_AND_SWAP_N_WITH_BARRIER_FLAG, regex);\n+    }\n+\n+    public static final String G1_COMPARE_AND_SWAP_P_WITH_BARRIER_FLAG = COMPOSITE_PREFIX + \"G1_COMPARE_AND_SWAP_P_WITH_BARRIER_FLAG\" + POSTFIX;\n+    static {\n+        String regex = START + \"g1CompareAndSwapP\\\\S*\" + MID + \"barrier\\\\(\\\\s*\" + IS_REPLACED + \"\\\\s*\\\\)\" + END;\n+        machOnly(G1_COMPARE_AND_SWAP_P_WITH_BARRIER_FLAG, regex);\n+    }\n+\n+    public static final String G1_ENCODE_P_AND_STORE_N = PREFIX + \"G1_ENCODE_P_AND_STORE_N\" + POSTFIX;\n+    static {\n+        machOnlyNameRegex(G1_ENCODE_P_AND_STORE_N, \"g1EncodePAndStoreN\");\n+    }\n+\n+    public static final String G1_ENCODE_P_AND_STORE_N_WITH_BARRIER_FLAG = COMPOSITE_PREFIX + \"G1_ENCODE_P_AND_STORE_N_WITH_BARRIER_FLAG\" + POSTFIX;\n+    static {\n+        String regex = START + \"g1EncodePAndStoreN\\\\S*\" + MID + \"barrier\\\\(\\\\s*\" + IS_REPLACED + \"\\\\s*\\\\)\" + END;\n+        machOnly(G1_ENCODE_P_AND_STORE_N_WITH_BARRIER_FLAG, regex);\n+    }\n+\n+    public static final String G1_GET_AND_SET_N_WITH_BARRIER_FLAG = COMPOSITE_PREFIX + \"G1_GET_AND_SET_N_WITH_BARRIER_FLAG\" + POSTFIX;\n+    static {\n+        String regex = START + \"g1GetAndSetN\\\\S*\" + MID + \"barrier\\\\(\\\\s*\" + IS_REPLACED + \"\\\\s*\\\\)\" + END;\n+        machOnly(G1_GET_AND_SET_N_WITH_BARRIER_FLAG, regex);\n+    }\n+\n+    public static final String G1_GET_AND_SET_P_WITH_BARRIER_FLAG = COMPOSITE_PREFIX + \"G1_GET_AND_SET_P_WITH_BARRIER_FLAG\" + POSTFIX;\n+    static {\n+        String regex = START + \"g1GetAndSetP\\\\S*\" + MID + \"barrier\\\\(\\\\s*\" + IS_REPLACED + \"\\\\s*\\\\)\" + END;\n+        machOnly(G1_GET_AND_SET_P_WITH_BARRIER_FLAG, regex);\n+    }\n+\n+    public static final String G1_LOAD_N = PREFIX + \"G1_LOAD_N\" + POSTFIX;\n+    static {\n+        machOnlyNameRegex(G1_LOAD_N, \"g1LoadN\");\n+    }\n+\n+    public static final String G1_LOAD_N_WITH_BARRIER_FLAG = COMPOSITE_PREFIX + \"G1_LOAD_N_WITH_BARRIER_FLAG\" + POSTFIX;\n+    static {\n+        String regex = START + \"g1LoadN\\\\S*\" + MID + \"barrier\\\\(\\\\s*\" + IS_REPLACED + \"\\\\s*\\\\)\" + END;\n+        machOnly(G1_LOAD_N_WITH_BARRIER_FLAG, regex);\n+    }\n+\n+    public static final String G1_LOAD_P_WITH_BARRIER_FLAG = COMPOSITE_PREFIX + \"G1_LOAD_P_WITH_BARRIER_FLAG\" + POSTFIX;\n+    static {\n+        String regex = START + \"g1LoadP\\\\S*\" + MID + \"barrier\\\\(\\\\s*\" + IS_REPLACED + \"\\\\s*\\\\)\" + END;\n+        machOnly(G1_LOAD_P_WITH_BARRIER_FLAG, regex);\n+    }\n+\n+    public static final String G1_STORE_N = PREFIX + \"G1_STORE_N\" + POSTFIX;\n+    static {\n+        machOnlyNameRegex(G1_STORE_N, \"g1StoreN\");\n+    }\n+\n+    public static final String G1_STORE_N_WITH_BARRIER_FLAG = COMPOSITE_PREFIX + \"G1_STORE_N_WITH_BARRIER_FLAG\" + POSTFIX;\n+    static {\n+        String regex = START + \"g1StoreN\\\\S*\" + MID + \"barrier\\\\(\\\\s*\" + IS_REPLACED + \"\\\\s*\\\\)\" + END;\n+        machOnly(G1_STORE_N_WITH_BARRIER_FLAG, regex);\n+    }\n+\n+    public static final String G1_STORE_P = PREFIX + \"G1_STORE_P\" + POSTFIX;\n+    static {\n+        machOnlyNameRegex(G1_STORE_P, \"g1StoreP\");\n+    }\n+\n+    public static final String G1_STORE_P_WITH_BARRIER_FLAG = COMPOSITE_PREFIX + \"G1_STORE_P_WITH_BARRIER_FLAG\" + POSTFIX;\n+    static {\n+        String regex = START + \"g1StoreP\\\\S*\" + MID + \"barrier\\\\(\\\\s*\" + IS_REPLACED + \"\\\\s*\\\\)\" + END;\n+        machOnly(G1_STORE_P_WITH_BARRIER_FLAG, regex);\n+    }\n+\n@@ -840,0 +931,5 @@\n+    public static final String MACH_TEMP = PREFIX + \"MACH_TEMP\" + POSTFIX;\n+    static {\n+        machOnlyNameRegex(MACH_TEMP, \"MachTemp\");\n+    }\n+\n@@ -1126,0 +1222,6 @@\n+    public static final String OOPMAP_WITH = COMPOSITE_PREFIX + \"OOPMAP_WITH\" + POSTFIX;\n+    static {\n+        String regex = \"(#\\\\s*OopMap\\\\s*\\\\{.*\" + IS_REPLACED + \".*\\\\})\";\n+        optoOnly(OOPMAP_WITH, regex);\n+    }\n+\n","filename":"test\/hotspot\/jtreg\/compiler\/lib\/ir_framework\/IRNode.java","additions":102,"deletions":0,"binary":false,"changes":102,"status":"modified"},{"patch":"@@ -0,0 +1,98 @@\n+\/*\n+ * Copyright (c) 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+package compiler.runtime.safepoints;\n+\n+import compiler.lib.ir_framework.*;\n+import java.lang.ref.SoftReference;\n+\n+\/**\n+ * @test\n+ * @summary Test that undefined values generated by MachTemp nodes (in this\n+ *          case, derived from G1 barriers) are not included in OopMaps.\n+ *          Extracted from java.lang.invoke.LambdaFormEditor::getInCache.\n+ * @key randomness\n+ * @library \/test\/lib \/\n+ * @requires vm.gc.G1 & vm.bits == 64 & vm.opt.final.UseCompressedOops == true\n+ * @run driver compiler.runtime.safepoints.TestMachTempsAcrossSafepoints\n+ *\/\n+\n+public class TestMachTempsAcrossSafepoints {\n+\n+    static class RefWithKey extends SoftReference<Object> {\n+        final int key;\n+\n+        public RefWithKey(int key) {\n+            super(new Object());\n+            this.key = key;\n+        }\n+\n+        @DontInline\n+        @Override\n+        public boolean equals(Object obj) {\n+            return obj instanceof RefWithKey that && this.key == that.key;\n+        }\n+    }\n+\n+    public static void main(String[] args) throws Exception {\n+        String inlineCmd = \"-XX:CompileCommand=inline,java.lang.ref.SoftReference::get\";\n+        TestFramework.runWithFlags(inlineCmd, \"-XX:+StressGCM\", \"-XX:+StressLCM\", \"-XX:StressSeed=1\");\n+        TestFramework.runWithFlags(inlineCmd, \"-XX:+StressGCM\", \"-XX:+StressLCM\");\n+    }\n+\n+    @Test\n+    @IR(counts = {IRNode.G1_LOAD_N, \"1\"}, phase = CompilePhase.FINAL_CODE)\n+    @IR(counts = {IRNode.MACH_TEMP, \">= 1\"}, phase = CompilePhase.FINAL_CODE)\n+    @IR(counts = {IRNode.STATIC_CALL_OF_METHOD, \"equals\", \"2\"})\n+    @IR(failOn = {IRNode.OOPMAP_WITH, \"NarrowOop\"})\n+    static private Object test(RefWithKey key, RefWithKey[] refs) {\n+        RefWithKey k = null;\n+        \/\/ This loop causes the register allocator to not \"rematerialize\" all\n+        \/\/ MachTemp nodes generated for the reference g1LoadN instruction below.\n+        for (int i = 0; i < refs.length; i++) {\n+            RefWithKey k0 = refs[0];\n+            if (k0.equals(key)) {\n+                k = k0;\n+            }\n+        }\n+        if (k != null && !key.equals(k)) {\n+            return null;\n+        }\n+        \/\/ The MachTemp node implementing the dst TEMP operand in the g1LoadN\n+        \/\/ instruction corresponding to k.get() can be scheduled across the\n+        \/\/ above call to RefWithKey::equals(), due to an unfortunate interaction\n+        \/\/ of inaccurate basic block frequency estimation (emulated in this test\n+        \/\/ by randomizing the GCM and LCM heuristics) and call-catch cleanup.\n+        \/\/ Since narrow pointer MachTemp nodes are typed as narrow OOPs, this\n+        \/\/ causes the oopmap builder to include the MachTemp node definition in\n+        \/\/ the RefWithKey::equals() return oopmap.\n+        return (k != null) ? k.get() : null;\n+    }\n+\n+    @Run(test = \"test\")\n+    @Warmup(0)\n+    public void run() {\n+        RefWithKey ref = new RefWithKey(42);\n+        test(ref, new RefWithKey[]{ref});\n+    }\n+}\n","filename":"test\/hotspot\/jtreg\/compiler\/runtime\/safepoints\/TestMachTempsAcrossSafepoints.java","additions":98,"deletions":0,"binary":false,"changes":98,"status":"added"},{"patch":"@@ -307,1 +307,4 @@\n-                \"-XX:StressSeed=\" + rng.nextInt(Integer.MAX_VALUE)));\n+                \"-XX:StressSeed=\" + rng.nextInt(Integer.MAX_VALUE),\n+                \/\/ Do not fail on huge methods where StressGCM makes register\n+                \/\/ allocation allocate lots of memory\n+                \"-XX:CompileCommand=memlimit,*.*,0\"));\n","filename":"test\/hotspot\/jtreg\/testlibrary\/ctw\/src\/sun\/hotspot\/tools\/ctw\/CtwRunner.java","additions":4,"deletions":1,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -27,1 +27,1 @@\n- * @run junit\/othervm\/timeout=2500 -XX:+IgnoreUnrecognizedVMOptions -XX:-VerifyDependencies -esa -DBigArityTest.ITERATION_COUNT=1 test.java.lang.invoke.BigArityTest\n+ * @run junit\/othervm\/timeout=2500 -XX:+IgnoreUnrecognizedVMOptions -XX:-VerifyDependencies -XX:CompileCommand=memlimit,*.*,0 -esa -DBigArityTest.ITERATION_COUNT=1 test.java.lang.invoke.BigArityTest\n","filename":"test\/jdk\/java\/lang\/invoke\/BigArityTest.java","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"}]}