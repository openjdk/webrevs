{"files":[{"patch":"@@ -203,0 +203,6 @@\n+  ifeq ($(call check-jvm-feature, g1gc), true)\n+    AD_SRC_FILES += $(call uniq, $(wildcard $(foreach d, $(AD_SRC_ROOTS), \\\n+        $d\/cpu\/$(HOTSPOT_TARGET_CPU_ARCH)\/gc\/g1\/g1_$(HOTSPOT_TARGET_CPU).ad \\\n+      )))\n+  endif\n+\n","filename":"make\/hotspot\/gensrc\/GensrcAdlc.gmk","additions":6,"deletions":0,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -2623,1 +2623,2 @@\n-      is_valid_sve_arith_imm_pattern(n, m)) {\n+      is_valid_sve_arith_imm_pattern(n, m) ||\n+      is_encode_and_store_pattern(n, m)) {\n@@ -6597,1 +6598,1 @@\n-  predicate(!needs_acquiring_load(n));\n+  predicate(!needs_acquiring_load(n) && n->as_Load()->barrier_data() == 0);\n@@ -7026,1 +7027,1 @@\n-  predicate(!needs_releasing_store(n));\n+  predicate(!needs_releasing_store(n) && n->as_Store()->barrier_data() == 0);\n@@ -7039,1 +7040,1 @@\n-  predicate(!needs_releasing_store(n));\n+  predicate(!needs_releasing_store(n) && n->as_Store()->barrier_data() == 0);\n@@ -7273,0 +7274,1 @@\n+  predicate(n->as_Load()->barrier_data() == 0);\n@@ -7440,0 +7442,1 @@\n+  predicate(n->as_Store()->barrier_data() == 0);\n@@ -7452,0 +7455,1 @@\n+  predicate(n->as_Store()->barrier_data() == 0);\n@@ -8246,0 +8250,1 @@\n+  predicate(n->as_LoadStore()->barrier_data() == 0);\n@@ -8360,1 +8365,1 @@\n-  predicate(needs_acquiring_load_exclusive(n));\n+  predicate(needs_acquiring_load_exclusive(n) && n->as_LoadStore()->barrier_data() == 0);\n@@ -8465,0 +8470,1 @@\n+  predicate(n->as_LoadStore()->barrier_data() == 0);\n@@ -8574,1 +8580,1 @@\n-  predicate(needs_acquiring_load_exclusive(n));\n+  predicate(needs_acquiring_load_exclusive(n) && n->as_LoadStore()->barrier_data() == 0);\n@@ -8686,0 +8692,1 @@\n+  predicate(n->as_LoadStore()->barrier_data() == 0);\n@@ -8805,1 +8812,1 @@\n-  predicate(needs_acquiring_load_exclusive(n));\n+  predicate(needs_acquiring_load_exclusive(n) && n->as_LoadStore()->barrier_data() == 0);\n@@ -8866,0 +8873,1 @@\n+  predicate(n->as_LoadStore()->barrier_data() == 0);\n@@ -8909,1 +8917,1 @@\n-  predicate(needs_acquiring_load_exclusive(n));\n+  predicate(needs_acquiring_load_exclusive(n) && n->as_LoadStore()->barrier_data() == 0);\n","filename":"src\/hotspot\/cpu\/aarch64\/aarch64.ad","additions":16,"deletions":8,"binary":false,"changes":24,"status":"modified"},{"patch":"@@ -48,0 +48,1 @@\n+       $1$6,NAcq,INDENT(predicate(needs_acquiring_load_exclusive(n) && n->as_LoadStore()->barrier_data() == 0);),\n@@ -49,0 +50,1 @@\n+       $1,N,INDENT(predicate(n->as_LoadStore()->barrier_data() == 0);),\n@@ -125,0 +127,1 @@\n+       $1$6,NAcq,INDENT(predicate(needs_acquiring_load_exclusive(n) && n->as_LoadStore()->barrier_data() == 0);),\n@@ -126,0 +129,1 @@\n+       $1,N,INDENT(predicate(n->as_LoadStore()->barrier_data() == 0);),\n","filename":"src\/hotspot\/cpu\/aarch64\/cas.m4","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -41,1 +41,4 @@\n-#endif\n+#endif \/\/ COMPILER1\n+#ifdef COMPILER2\n+#include \"gc\/g1\/c2\/g1BarrierSetC2.hpp\"\n+#endif \/\/ COMPILER2\n@@ -98,0 +101,48 @@\n+static void generate_queue_test_and_insertion(MacroAssembler* masm, ByteSize index_offset, ByteSize buffer_offset, Label& runtime,\n+                                              const Register thread, const Register value, const Register temp1, const Register temp2) {\n+  \/\/ Can we store a value in the given thread's buffer?\n+  \/\/ (The index field is typed as size_t.)\n+  __ ldr(temp1, Address(thread, in_bytes(index_offset)));   \/\/ temp1 := *(index address)\n+  __ cbz(temp1, runtime);                                   \/\/ jump to runtime if index == 0 (full buffer)\n+  \/\/ The buffer is not full, store value into it.\n+  __ sub(temp1, temp1, wordSize);                           \/\/ temp1 := next index\n+  __ str(temp1, Address(thread, in_bytes(index_offset)));   \/\/ *(index address) := next index\n+  __ ldr(temp2, Address(thread, in_bytes(buffer_offset)));  \/\/ temp2 := buffer address\n+  __ str(value, Address(temp2, temp1));                     \/\/ *(buffer address + next index) := value\n+}\n+\n+static void generate_pre_barrier_fast_path(MacroAssembler* masm,\n+                                           const Register thread,\n+                                           const Register tmp1) {\n+  Address in_progress(thread, in_bytes(G1ThreadLocalData::satb_mark_queue_active_offset()));\n+  \/\/ Is marking active?\n+  if (in_bytes(SATBMarkQueue::byte_width_of_active()) == 4) {\n+    __ ldrw(tmp1, in_progress);\n+  } else {\n+    assert(in_bytes(SATBMarkQueue::byte_width_of_active()) == 1, \"Assumption\");\n+    __ ldrb(tmp1, in_progress);\n+  }\n+}\n+\n+static void generate_pre_barrier_slow_path(MacroAssembler* masm,\n+                                           const Register obj,\n+                                           const Register pre_val,\n+                                           const Register thread,\n+                                           const Register tmp1,\n+                                           const Register tmp2,\n+                                           Label& done,\n+                                           Label& runtime) {\n+  \/\/ Do we need to load the previous value?\n+  if (obj != noreg) {\n+    __ load_heap_oop(pre_val, Address(obj, 0), noreg, noreg, AS_RAW);\n+  }\n+  \/\/ Is the previous value null?\n+  __ cbz(pre_val, done);\n+  generate_queue_test_and_insertion(masm,\n+                                    G1ThreadLocalData::satb_mark_queue_index_offset(),\n+                                    G1ThreadLocalData::satb_mark_queue_buffer_offset(),\n+                                    runtime,\n+                                    thread, pre_val, tmp1, tmp2);\n+  __ b(done);\n+}\n+\n@@ -118,11 +169,2 @@\n-  Address in_progress(thread, in_bytes(G1ThreadLocalData::satb_mark_queue_active_offset()));\n-  Address index(thread, in_bytes(G1ThreadLocalData::satb_mark_queue_index_offset()));\n-  Address buffer(thread, in_bytes(G1ThreadLocalData::satb_mark_queue_buffer_offset()));\n-\n-  \/\/ Is marking active?\n-  if (in_bytes(SATBMarkQueue::byte_width_of_active()) == 4) {\n-    __ ldrw(tmp1, in_progress);\n-  } else {\n-    assert(in_bytes(SATBMarkQueue::byte_width_of_active()) == 1, \"Assumption\");\n-    __ ldrb(tmp1, in_progress);\n-  }\n+  generate_pre_barrier_fast_path(masm, thread, tmp1);\n+  \/\/ If marking is not active (*(mark queue active address) == 0), jump to done\n@@ -130,25 +172,1 @@\n-\n-  \/\/ Do we need to load the previous value?\n-  if (obj != noreg) {\n-    __ load_heap_oop(pre_val, Address(obj, 0), noreg, noreg, AS_RAW);\n-  }\n-\n-  \/\/ Is the previous value null?\n-  __ cbz(pre_val, done);\n-\n-  \/\/ Can we store original value in the thread's buffer?\n-  \/\/ Is index == 0?\n-  \/\/ (The index field is typed as size_t.)\n-\n-  __ ldr(tmp1, index);                      \/\/ tmp := *index_adr\n-  __ cbz(tmp1, runtime);                    \/\/ tmp == 0?\n-                                        \/\/ If yes, goto runtime\n-\n-  __ sub(tmp1, tmp1, wordSize);             \/\/ tmp := tmp - wordSize\n-  __ str(tmp1, index);                      \/\/ *index_adr := tmp\n-  __ ldr(tmp2, buffer);\n-  __ add(tmp1, tmp1, tmp2);                 \/\/ tmp := tmp + *buffer_adr\n-\n-  \/\/ Record the previous value\n-  __ str(pre_val, Address(tmp1, 0));\n-  __ b(done);\n+  generate_pre_barrier_slow_path(masm, obj, pre_val, thread, tmp1, tmp2, done, runtime);\n@@ -185,0 +203,44 @@\n+static void generate_post_barrier_fast_path(MacroAssembler* masm,\n+                                            const Register store_addr,\n+                                            const Register new_val,\n+                                            const Register tmp1,\n+                                            const Register tmp2,\n+                                            Label& done,\n+                                            bool new_val_may_be_null) {\n+    \/\/ Does store cross heap regions?\n+  __ eor(tmp1, store_addr, new_val);                     \/\/ tmp1 := store address ^ new value\n+  __ lsr(tmp1, tmp1, G1HeapRegion::LogOfHRGrainBytes);   \/\/ tmp1 := ((store address ^ new value) >> LogOfHRGrainBytes)\n+  __ cbz(tmp1, done);\n+  \/\/ Crosses regions, storing null?\n+  if (new_val_may_be_null) {\n+    __ cbz(new_val, done);\n+  }\n+  \/\/ Storing region crossing non-null, is card already dirty?\n+  __ lsr(tmp1, store_addr, CardTable::card_shift());     \/\/ tmp1 := card address relative to card table base\n+  __ load_byte_map_base(tmp2);                           \/\/ tmp2 := card table base address\n+  __ add(tmp1, tmp1, tmp2);                              \/\/ tmp1 := card address\n+  __ ldrb(tmp2, Address(tmp1));                          \/\/ tmp2 := card\n+  __ cmpw(tmp2, (int)G1CardTable::g1_young_card_val());  \/\/ tmp2 := card == young_card_val?\n+}\n+\n+static void generate_post_barrier_slow_path(MacroAssembler* masm,\n+                                            const Register thread,\n+                                            const Register tmp1,\n+                                            const Register tmp2,\n+                                            Label& done,\n+                                            Label& runtime) {\n+  __ membar(Assembler::StoreLoad);  \/\/ StoreLoad membar\n+  __ ldrb(tmp2, Address(tmp1));     \/\/ tmp2 := card\n+  __ cbzw(tmp2, done);\n+  \/\/ Storing a region crossing, non-null oop, card is clean.\n+  \/\/ Dirty card and log.\n+  STATIC_ASSERT(CardTable::dirty_card_val() == 0);\n+  __ strb(zr, Address(tmp1));       \/\/ *(card address) := dirty_card_val\n+  generate_queue_test_and_insertion(masm,\n+                                    G1ThreadLocalData::dirty_card_queue_index_offset(),\n+                                    G1ThreadLocalData::dirty_card_queue_buffer_offset(),\n+                                    runtime,\n+                                    thread, tmp1, tmp2, rscratch1);\n+  __ b(done);\n+}\n+\n@@ -197,7 +259,0 @@\n-  Address queue_index(thread, in_bytes(G1ThreadLocalData::dirty_card_queue_index_offset()));\n-  Address buffer(thread, in_bytes(G1ThreadLocalData::dirty_card_queue_buffer_offset()));\n-\n-  BarrierSet* bs = BarrierSet::barrier_set();\n-  CardTableBarrierSet* ctbs = barrier_set_cast<CardTableBarrierSet>(bs);\n-  CardTable* ct = ctbs->card_table();\n-\n@@ -207,1 +262,4 @@\n-  \/\/ Does store cross heap regions?\n+  generate_post_barrier_fast_path(masm, store_addr, new_val, tmp1, tmp2, done, true \/* new_val_may_be_null *\/);\n+  \/\/ If card is young, jump to done\n+  __ br(Assembler::EQ, done);\n+  generate_post_barrier_slow_path(masm, thread, tmp1, tmp2, done, runtime);\n@@ -209,3 +267,6 @@\n-  __ eor(tmp1, store_addr, new_val);\n-  __ lsr(tmp1, tmp1, G1HeapRegion::LogOfHRGrainBytes);\n-  __ cbz(tmp1, done);\n+  __ bind(runtime);\n+  \/\/ save the live input values\n+  RegSet saved = RegSet::of(store_addr);\n+  __ push(saved, sp);\n+  __ call_VM_leaf(CAST_FROM_FN_PTR(address, G1BarrierSetRuntime::write_ref_field_post_entry), tmp1, thread);\n+  __ pop(saved, sp);\n@@ -213,1 +274,2 @@\n-  \/\/ crosses regions, storing null?\n+  __ bind(done);\n+}\n@@ -215,1 +277,1 @@\n-  __ cbz(new_val, done);\n+#if defined(COMPILER2)\n@@ -217,1 +279,2 @@\n-  \/\/ storing region crossing non-null, is card already dirty?\n+#undef __\n+#define __ masm->\n@@ -219,1 +282,9 @@\n-  const Register card_addr = tmp1;\n+static void generate_c2_barrier_runtime_call(MacroAssembler* masm, G1BarrierStubC2* stub, const Register arg, const address runtime_path) {\n+  SaveLiveRegisters save_registers(masm, stub);\n+  if (c_rarg0 != arg) {\n+    __ mov(c_rarg0, arg);\n+  }\n+  __ mov(c_rarg1, rthread);\n+  __ mov(rscratch1, runtime_path);\n+  __ blr(rscratch1);\n+}\n@@ -221,1 +292,10 @@\n-  __ lsr(card_addr, store_addr, CardTable::card_shift());\n+void G1BarrierSetAssembler::g1_write_barrier_pre_c2(MacroAssembler* masm,\n+                                                    Register obj,\n+                                                    Register pre_val,\n+                                                    Register thread,\n+                                                    Register tmp1,\n+                                                    Register tmp2,\n+                                                    G1PreBarrierStubC2* stub) {\n+  assert(thread == rthread, \"must be\");\n+  assert_different_registers(obj, pre_val, tmp1, tmp2);\n+  assert(pre_val != noreg && tmp1 != noreg && tmp2 != noreg, \"expecting a register\");\n@@ -223,6 +303,1 @@\n-  \/\/ get the address of the card\n-  __ load_byte_map_base(tmp2);\n-  __ add(card_addr, card_addr, tmp2);\n-  __ ldrb(tmp2, Address(card_addr));\n-  __ cmpw(tmp2, (int)G1CardTable::g1_young_card_val());\n-  __ br(Assembler::EQ, done);\n+  stub->initialize_registers(obj, pre_val, thread, tmp1, tmp2);\n@@ -230,1 +305,3 @@\n-  assert((int)CardTable::dirty_card_val() == 0, \"must be 0\");\n+  generate_pre_barrier_fast_path(masm, thread, tmp1);\n+  \/\/ If marking is active (*(mark queue active address) != 0), jump to stub (slow path)\n+  __ cbnzw(tmp1, *stub->entry());\n@@ -232,1 +309,2 @@\n-  __ membar(Assembler::StoreLoad);\n+  __ bind(*stub->continuation());\n+}\n@@ -234,2 +312,9 @@\n-  __ ldrb(tmp2, Address(card_addr));\n-  __ cbzw(tmp2, done);\n+void G1BarrierSetAssembler::generate_c2_pre_barrier_stub(MacroAssembler* masm,\n+                                                         G1PreBarrierStubC2* stub) const {\n+  Assembler::InlineSkippedInstructionsCounter skip_counter(masm);\n+  Label runtime;\n+  Register obj = stub->obj();\n+  Register pre_val = stub->pre_val();\n+  Register thread = stub->thread();\n+  Register tmp1 = stub->tmp1();\n+  Register tmp2 = stub->tmp2();\n@@ -237,2 +322,2 @@\n-  \/\/ storing a region crossing, non-null oop, card is clean.\n-  \/\/ dirty card and log.\n+  __ bind(*stub->entry());\n+  generate_pre_barrier_slow_path(masm, obj, pre_val, thread, tmp1, tmp2, *stub->continuation(), runtime);\n@@ -240,1 +325,4 @@\n-  __ strb(zr, Address(card_addr));\n+  __ bind(runtime);\n+  generate_c2_barrier_runtime_call(masm, stub, pre_val, CAST_FROM_FN_PTR(address, G1BarrierSetRuntime::write_ref_field_pre_entry));\n+  __ b(*stub->continuation());\n+}\n@@ -242,4 +330,12 @@\n-  __ ldr(rscratch1, queue_index);\n-  __ cbz(rscratch1, runtime);\n-  __ sub(rscratch1, rscratch1, wordSize);\n-  __ str(rscratch1, queue_index);\n+void G1BarrierSetAssembler::g1_write_barrier_post_c2(MacroAssembler* masm,\n+                                                     Register store_addr,\n+                                                     Register new_val,\n+                                                     Register thread,\n+                                                     Register tmp1,\n+                                                     Register tmp2,\n+                                                     G1PostBarrierStubC2* stub) {\n+  assert(thread == rthread, \"must be\");\n+  assert_different_registers(store_addr, new_val, thread, tmp1, tmp2,\n+                             rscratch1);\n+  assert(store_addr != noreg && new_val != noreg && tmp1 != noreg\n+         && tmp2 != noreg, \"expecting a register\");\n@@ -247,3 +343,1 @@\n-  __ ldr(tmp2, buffer);\n-  __ str(card_addr, Address(tmp2, rscratch1));\n-  __ b(done);\n+  stub->initialize_registers(thread, tmp1, tmp2);\n@@ -251,6 +345,4 @@\n-  __ bind(runtime);\n-  \/\/ save the live input values\n-  RegSet saved = RegSet::of(store_addr);\n-  __ push(saved, sp);\n-  __ call_VM_leaf(CAST_FROM_FN_PTR(address, G1BarrierSetRuntime::write_ref_field_post_entry), card_addr, thread);\n-  __ pop(saved, sp);\n+  bool new_val_may_be_null = (stub->barrier_data() & G1C2BarrierPostNotNull) == 0;\n+  generate_post_barrier_fast_path(masm, store_addr, new_val, tmp1, tmp2, *stub->continuation(), new_val_may_be_null);\n+  \/\/ If card is not young, jump to stub (slow path)\n+  __ br(Assembler::NE, *stub->entry());\n@@ -258,1 +350,1 @@\n-  __ bind(done);\n+  __ bind(*stub->continuation());\n@@ -261,0 +353,18 @@\n+void G1BarrierSetAssembler::generate_c2_post_barrier_stub(MacroAssembler* masm,\n+                                                          G1PostBarrierStubC2* stub) const {\n+  Assembler::InlineSkippedInstructionsCounter skip_counter(masm);\n+  Label runtime;\n+  Register thread = stub->thread();\n+  Register tmp1 = stub->tmp1(); \/\/ tmp1 holds the card address.\n+  Register tmp2 = stub->tmp2();\n+\n+  __ bind(*stub->entry());\n+  generate_post_barrier_slow_path(masm, thread, tmp1, tmp2, *stub->continuation(), runtime);\n+\n+  __ bind(runtime);\n+  generate_c2_barrier_runtime_call(masm, stub, tmp1, CAST_FROM_FN_PTR(address, G1BarrierSetRuntime::write_ref_field_post_entry));\n+  __ b(*stub->continuation());\n+}\n+\n+#endif \/\/ COMPILER2\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/gc\/g1\/g1BarrierSetAssembler_aarch64.cpp","additions":190,"deletions":80,"binary":false,"changes":270,"status":"modified"},{"patch":"@@ -36,0 +36,2 @@\n+class G1PreBarrierStubC2;\n+class G1PostBarrierStubC2;\n@@ -72,0 +74,21 @@\n+#ifdef COMPILER2\n+  void g1_write_barrier_pre_c2(MacroAssembler* masm,\n+                               Register obj,\n+                               Register pre_val,\n+                               Register thread,\n+                               Register tmp1,\n+                               Register tmp2,\n+                               G1PreBarrierStubC2* c2_stub);\n+  void generate_c2_pre_barrier_stub(MacroAssembler* masm,\n+                                    G1PreBarrierStubC2* stub) const;\n+  void g1_write_barrier_post_c2(MacroAssembler* masm,\n+                                Register store_addr,\n+                                Register new_val,\n+                                Register thread,\n+                                Register tmp1,\n+                                Register tmp2,\n+                                G1PostBarrierStubC2* c2_stub);\n+  void generate_c2_post_barrier_stub(MacroAssembler* masm,\n+                                     G1PostBarrierStubC2* stub) const;\n+#endif\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/gc\/g1\/g1BarrierSetAssembler_aarch64.hpp","additions":23,"deletions":0,"binary":false,"changes":23,"status":"modified"},{"patch":"@@ -0,0 +1,702 @@\n+\/\/\n+\/\/ Copyright (c) 2024, Oracle and\/or its affiliates. All rights reserved.\n+\/\/ DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+\/\/\n+\/\/ This code is free software; you can redistribute it and\/or modify it\n+\/\/ under the terms of the GNU General Public License version 2 only, as\n+\/\/ published by the Free Software Foundation.\n+\/\/\n+\/\/ This code is distributed in the hope that it will be useful, but WITHOUT\n+\/\/ ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+\/\/ FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+\/\/ version 2 for more details (a copy is included in the LICENSE file that\n+\/\/ accompanied this code).\n+\/\/\n+\/\/ You should have received a copy of the GNU General Public License version\n+\/\/ 2 along with this work; if not, write to the Free Software Foundation,\n+\/\/ Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+\/\/\n+\/\/ Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+\/\/ or visit www.oracle.com if you need additional information or have any\n+\/\/ questions.\n+\/\/\n+\n+source_hpp %{\n+\n+#include \"gc\/g1\/c2\/g1BarrierSetC2.hpp\"\n+#include \"gc\/shared\/gc_globals.hpp\"\n+\n+%}\n+\n+source %{\n+\n+#include \"gc\/g1\/g1BarrierSetAssembler_aarch64.hpp\"\n+#include \"gc\/g1\/g1BarrierSetRuntime.hpp\"\n+\n+static void g1_pre_write_barrier(MacroAssembler* masm,\n+                                 const MachNode* node,\n+                                 Register obj,\n+                                 Register pre_val,\n+                                 Register tmp1,\n+                                 Register tmp2,\n+                                 RegSet preserve = RegSet(),\n+                                 RegSet no_preserve = RegSet()) {\n+  if (!G1PreBarrierStubC2::needs_barrier(node)) {\n+    return;\n+  }\n+  Assembler::InlineSkippedInstructionsCounter skip_counter(masm);\n+  G1BarrierSetAssembler* g1_asm = static_cast<G1BarrierSetAssembler*>(BarrierSet::barrier_set()->barrier_set_assembler());\n+  G1PreBarrierStubC2* const stub = G1PreBarrierStubC2::create(node);\n+  for (RegSetIterator<Register> reg = preserve.begin(); *reg != noreg; ++reg) {\n+    stub->preserve(*reg);\n+  }\n+  for (RegSetIterator<Register> reg = no_preserve.begin(); *reg != noreg; ++reg) {\n+    stub->dont_preserve(*reg);\n+  }\n+  g1_asm->g1_write_barrier_pre_c2(masm, obj, pre_val, rthread, tmp1, tmp2, stub);\n+}\n+\n+static void g1_post_write_barrier(MacroAssembler* masm,\n+                                  const MachNode* node,\n+                                  Register store_addr,\n+                                  Register new_val,\n+                                  Register tmp1,\n+                                  Register tmp2) {\n+  if (!G1PostBarrierStubC2::needs_barrier(node)) {\n+    return;\n+  }\n+  Assembler::InlineSkippedInstructionsCounter skip_counter(masm);\n+  G1BarrierSetAssembler* g1_asm = static_cast<G1BarrierSetAssembler*>(BarrierSet::barrier_set()->barrier_set_assembler());\n+  G1PostBarrierStubC2* const stub = G1PostBarrierStubC2::create(node);\n+  g1_asm->g1_write_barrier_post_c2(masm, store_addr, new_val, rthread, tmp1, tmp2, stub);\n+}\n+\n+%}\n+\n+\/\/ BEGIN This section of the file is automatically generated. Do not edit --------------\n+\n+\/\/ This section is generated from g1_aarch64.m4\n+\n+\n+\/\/ This pattern is generated automatically from g1_aarch64.m4.\n+\/\/ DO NOT EDIT ANYTHING IN THIS SECTION OF THE FILE\n+instruct g1StoreP(indirect mem, iRegP src, iRegPNoSp tmp1, iRegPNoSp tmp2, iRegPNoSp tmp3, rFlagsReg cr)\n+%{\n+  predicate(UseG1GC && !needs_releasing_store(n) && n->as_Store()->barrier_data() != 0);\n+  match(Set mem (StoreP mem src));\n+  effect(TEMP tmp1, TEMP tmp2, TEMP tmp3, KILL cr);\n+  ins_cost(INSN_COST);\n+  format %{ \"str  $src, $mem\\t# ptr\" %}\n+  ins_encode %{\n+    g1_pre_write_barrier(masm, this,\n+                         $mem$$Register \/* obj *\/,\n+                         $tmp1$$Register \/* pre_val *\/,\n+                         $tmp2$$Register \/* tmp1 *\/,\n+                         $tmp3$$Register \/* tmp2 *\/,\n+                         RegSet::of($mem$$Register, $src$$Register) \/* preserve *\/);\n+    __ str($src$$Register, $mem$$Register);\n+    g1_post_write_barrier(masm, this,\n+                          $mem$$Register \/* store_addr *\/,\n+                          $src$$Register \/* new_val *\/,\n+                          $tmp2$$Register \/* tmp1 *\/,\n+                          $tmp3$$Register \/* tmp2 *\/);\n+  %}\n+  ins_pipe(istore_reg_mem);\n+%}\n+\n+\/\/ This pattern is generated automatically from g1_aarch64.m4.\n+\/\/ DO NOT EDIT ANYTHING IN THIS SECTION OF THE FILE\n+instruct g1StorePVolatile(indirect mem, iRegP src, iRegPNoSp tmp1, iRegPNoSp tmp2, iRegPNoSp tmp3, rFlagsReg cr)\n+%{\n+  predicate(UseG1GC && needs_releasing_store(n) && n->as_Store()->barrier_data() != 0);\n+  match(Set mem (StoreP mem src));\n+  effect(TEMP tmp1, TEMP tmp2, TEMP tmp3, KILL cr);\n+  ins_cost(VOLATILE_REF_COST);\n+  format %{ \"stlr  $src, $mem\\t# ptr\" %}\n+  ins_encode %{\n+    g1_pre_write_barrier(masm, this,\n+                         $mem$$Register \/* obj *\/,\n+                         $tmp1$$Register \/* pre_val *\/,\n+                         $tmp2$$Register \/* tmp1 *\/,\n+                         $tmp3$$Register \/* tmp2 *\/,\n+                         RegSet::of($mem$$Register, $src$$Register) \/* preserve *\/);\n+    __ stlr($src$$Register, $mem$$Register);\n+    g1_post_write_barrier(masm, this,\n+                          $mem$$Register \/* store_addr *\/,\n+                          $src$$Register \/* new_val *\/,\n+                          $tmp2$$Register \/* tmp1 *\/,\n+                          $tmp3$$Register \/* tmp2 *\/);\n+  %}\n+  ins_pipe(pipe_class_memory);\n+%}\n+\n+\/\/ This pattern is generated automatically from g1_aarch64.m4.\n+\/\/ DO NOT EDIT ANYTHING IN THIS SECTION OF THE FILE\n+instruct g1StoreN(indirect mem, iRegN src, iRegPNoSp tmp1, iRegPNoSp tmp2, iRegPNoSp tmp3, rFlagsReg cr)\n+%{\n+  predicate(UseG1GC && !needs_releasing_store(n) && n->as_Store()->barrier_data() != 0);\n+  match(Set mem (StoreN mem src));\n+  effect(TEMP tmp1, TEMP tmp2, TEMP tmp3, KILL cr);\n+  ins_cost(INSN_COST);\n+  format %{ \"strw  $src, $mem\\t# compressed ptr\" %}\n+  ins_encode %{\n+    g1_pre_write_barrier(masm, this,\n+                         $mem$$Register \/* obj *\/,\n+                         $tmp1$$Register \/* pre_val *\/,\n+                         $tmp2$$Register \/* tmp1 *\/,\n+                         $tmp3$$Register \/* tmp2 *\/,\n+                         RegSet::of($mem$$Register, $src$$Register) \/* preserve *\/);\n+    __ strw($src$$Register, $mem$$Register);\n+    if ((barrier_data() & G1C2BarrierPost) != 0) {\n+      if ((barrier_data() & G1C2BarrierPostNotNull) == 0) {\n+        __ decode_heap_oop($tmp1$$Register, $src$$Register);\n+      } else {\n+        __ decode_heap_oop_not_null($tmp1$$Register, $src$$Register);\n+      }\n+    }\n+    g1_post_write_barrier(masm, this,\n+                          $mem$$Register \/* store_addr *\/,\n+                          $tmp1$$Register \/* new_val *\/,\n+                          $tmp2$$Register \/* tmp1 *\/,\n+                          $tmp3$$Register \/* tmp2 *\/);\n+  %}\n+  ins_pipe(istore_reg_mem);\n+%}\n+\n+\/\/ This pattern is generated automatically from g1_aarch64.m4.\n+\/\/ DO NOT EDIT ANYTHING IN THIS SECTION OF THE FILE\n+instruct g1EncodePAndStoreN(indirect mem, iRegP src, iRegPNoSp tmp1, iRegPNoSp tmp2, iRegPNoSp tmp3, rFlagsReg cr)\n+%{\n+  predicate(UseG1GC && !needs_releasing_store(n) && n->as_Store()->barrier_data() != 0);\n+  match(Set mem (StoreN mem (EncodeP src)));\n+  effect(TEMP tmp1, TEMP tmp2, TEMP tmp3, KILL cr);\n+  ins_cost(INSN_COST);\n+  format %{ \"encode_heap_oop $tmp1, $src\\n\\t\"\n+            \"strw  $tmp1, $mem\\t# compressed ptr\" %}\n+  ins_encode %{\n+    g1_pre_write_barrier(masm, this,\n+                         $mem$$Register \/* obj *\/,\n+                         $tmp1$$Register \/* pre_val *\/,\n+                         $tmp2$$Register \/* tmp1 *\/,\n+                         $tmp3$$Register \/* tmp2 *\/,\n+                         RegSet::of($mem$$Register, $src$$Register) \/* preserve *\/);\n+    if ((barrier_data() & G1C2BarrierPostNotNull) == 0) {\n+      __ encode_heap_oop($tmp1$$Register, $src$$Register);\n+    } else {\n+      __ encode_heap_oop_not_null($tmp1$$Register, $src$$Register);\n+    }\n+    __ strw($tmp1$$Register, $mem$$Register);\n+    g1_post_write_barrier(masm, this,\n+                          $mem$$Register \/* store_addr *\/,\n+                          $src$$Register \/* new_val *\/,\n+                          $tmp2$$Register \/* tmp1 *\/,\n+                          $tmp3$$Register \/* tmp2 *\/);\n+  %}\n+  ins_pipe(istore_reg_mem);\n+%}\n+\n+\/\/ Very few of the total executed stores are volatile (less than 1% across\n+\/\/ multiple benchmark suites), no need to define an encode-and-store version.\n+\n+\/\/ This pattern is generated automatically from g1_aarch64.m4.\n+\/\/ DO NOT EDIT ANYTHING IN THIS SECTION OF THE FILE\n+instruct g1StoreNVolatile(indirect mem, iRegN src, iRegPNoSp tmp1, iRegPNoSp tmp2, iRegPNoSp tmp3, rFlagsReg cr)\n+%{\n+  predicate(UseG1GC && needs_releasing_store(n) && n->as_Store()->barrier_data() != 0);\n+  match(Set mem (StoreN mem src));\n+  effect(TEMP tmp1, TEMP tmp2, TEMP tmp3, KILL cr);\n+  ins_cost(VOLATILE_REF_COST);\n+  format %{ \"stlrw  $src, $mem\\t# compressed ptr\" %}\n+  ins_encode %{\n+    g1_pre_write_barrier(masm, this,\n+                         $mem$$Register \/* obj *\/,\n+                         $tmp1$$Register \/* pre_val *\/,\n+                         $tmp2$$Register \/* tmp1 *\/,\n+                         $tmp3$$Register \/* tmp2 *\/,\n+                         RegSet::of($mem$$Register, $src$$Register) \/* preserve *\/);\n+    __ stlrw($src$$Register, $mem$$Register);\n+    if ((barrier_data() & G1C2BarrierPost) != 0) {\n+      if ((barrier_data() & G1C2BarrierPostNotNull) == 0) {\n+        __ decode_heap_oop($tmp1$$Register, $src$$Register);\n+      } else {\n+        __ decode_heap_oop_not_null($tmp1$$Register, $src$$Register);\n+      }\n+    }\n+    g1_post_write_barrier(masm, this,\n+                          $mem$$Register \/* store_addr *\/,\n+                          $tmp1$$Register \/* new_val *\/,\n+                          $tmp2$$Register \/* tmp1 *\/,\n+                          $tmp3$$Register \/* tmp2 *\/);\n+  %}\n+  ins_pipe(pipe_class_memory);\n+%}\n+\n+\/\/ This pattern is generated automatically from g1_aarch64.m4.\n+\/\/ DO NOT EDIT ANYTHING IN THIS SECTION OF THE FILE\n+instruct g1CompareAndExchangeP(iRegPNoSp res, indirect mem, iRegP oldval, iRegP newval, iRegPNoSp tmp1, iRegPNoSp tmp2, iRegPNoSp tmp3, rFlagsReg cr)\n+%{\n+  predicate(UseG1GC && !needs_acquiring_load_exclusive(n) && n->as_LoadStore()->barrier_data() != 0);\n+  match(Set res (CompareAndExchangeP mem (Binary oldval newval)));\n+  effect(TEMP res, TEMP tmp1, TEMP tmp2, TEMP tmp3, KILL cr);\n+  ins_cost(2 * VOLATILE_REF_COST);\n+  format %{ \"cmpxchg $res = $mem, $oldval, $newval\\t# ptr\" %}\n+  ins_encode %{\n+    assert_different_registers($oldval$$Register, $mem$$Register);\n+    assert_different_registers($newval$$Register, $mem$$Register);\n+    g1_pre_write_barrier(masm, this,\n+                         $mem$$Register \/* obj *\/,\n+                         $tmp1$$Register \/* pre_val *\/,\n+                         $tmp2$$Register \/* tmp1 *\/,\n+                         $tmp3$$Register \/* tmp2 *\/,\n+                         RegSet::of($mem$$Register, $oldval$$Register, $newval$$Register) \/* preserve *\/,\n+                         RegSet::of($res$$Register) \/* no_preserve *\/);\n+    __ mov($tmp1$$Register, $oldval$$Register);\n+    __ mov($tmp2$$Register, $newval$$Register);\n+    __ cmpxchg($mem$$Register, $tmp1$$Register, $tmp2$$Register, Assembler::xword,\n+               false \/* acquire *\/, true \/* release *\/, false \/* weak *\/, $res$$Register);\n+    g1_post_write_barrier(masm, this,\n+                          $mem$$Register \/* store_addr *\/,\n+                          $tmp2$$Register \/* new_val *\/,\n+                          $tmp1$$Register \/* tmp1 *\/,\n+                          $tmp3$$Register \/* tmp2 *\/);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ This pattern is generated automatically from g1_aarch64.m4.\n+\/\/ DO NOT EDIT ANYTHING IN THIS SECTION OF THE FILE\n+instruct g1CompareAndExchangePAcq(iRegPNoSp res, indirect mem, iRegP oldval, iRegP newval, iRegPNoSp tmp1, iRegPNoSp tmp2, iRegPNoSp tmp3, rFlagsReg cr)\n+%{\n+  predicate(UseG1GC && needs_acquiring_load_exclusive(n) && n->as_LoadStore()->barrier_data() != 0);\n+  match(Set res (CompareAndExchangeP mem (Binary oldval newval)));\n+  effect(TEMP res, TEMP tmp1, TEMP tmp2, TEMP tmp3, KILL cr);\n+  ins_cost(VOLATILE_REF_COST);\n+  format %{ \"cmpxchg_acq $res = $mem, $oldval, $newval\\t# ptr\" %}\n+  ins_encode %{\n+    assert_different_registers($oldval$$Register, $mem$$Register);\n+    assert_different_registers($newval$$Register, $mem$$Register);\n+    g1_pre_write_barrier(masm, this,\n+                         $mem$$Register \/* obj *\/,\n+                         $tmp1$$Register \/* pre_val *\/,\n+                         $tmp2$$Register \/* tmp1 *\/,\n+                         $tmp3$$Register \/* tmp2 *\/,\n+                         RegSet::of($mem$$Register, $oldval$$Register, $newval$$Register) \/* preserve *\/,\n+                         RegSet::of($res$$Register) \/* no_preserve *\/);\n+    __ mov($tmp1$$Register, $oldval$$Register);\n+    __ mov($tmp2$$Register, $newval$$Register);\n+    __ cmpxchg($mem$$Register, $tmp1$$Register, $tmp2$$Register, Assembler::xword,\n+               true \/* acquire *\/, true \/* release *\/, false \/* weak *\/, $res$$Register);\n+    g1_post_write_barrier(masm, this,\n+                          $mem$$Register \/* store_addr *\/,\n+                          $tmp2$$Register \/* new_val *\/,\n+                          $tmp1$$Register \/* tmp1 *\/,\n+                          $tmp3$$Register \/* tmp2 *\/);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ This pattern is generated automatically from g1_aarch64.m4.\n+\/\/ DO NOT EDIT ANYTHING IN THIS SECTION OF THE FILE\n+instruct g1CompareAndExchangeN(iRegNNoSp res, indirect mem, iRegN oldval, iRegN newval, iRegPNoSp tmp1, iRegPNoSp tmp2, iRegPNoSp tmp3, rFlagsReg cr)\n+%{\n+  predicate(UseG1GC && !needs_acquiring_load_exclusive(n) && n->as_LoadStore()->barrier_data() != 0);\n+  match(Set res (CompareAndExchangeN mem (Binary oldval newval)));\n+  effect(TEMP res, TEMP tmp1, TEMP tmp2, TEMP tmp3, KILL cr);\n+  ins_cost(2 * VOLATILE_REF_COST);\n+  format %{ \"cmpxchg $res = $mem, $oldval, $newval\\t# narrow oop\" %}\n+  ins_encode %{\n+    assert_different_registers($oldval$$Register, $mem$$Register);\n+    assert_different_registers($newval$$Register, $mem$$Register);\n+    g1_pre_write_barrier(masm, this,\n+                         $mem$$Register \/* obj *\/,\n+                         $tmp1$$Register \/* pre_val *\/,\n+                         $tmp2$$Register \/* tmp1 *\/,\n+                         $tmp3$$Register \/* tmp2 *\/,\n+                         RegSet::of($mem$$Register, $oldval$$Register, $newval$$Register) \/* preserve *\/,\n+                         RegSet::of($res$$Register) \/* no_preserve *\/);\n+    __ mov($tmp1$$Register, $oldval$$Register);\n+    __ mov($tmp2$$Register, $newval$$Register);\n+    __ cmpxchg($mem$$Register, $tmp1$$Register, $tmp2$$Register, Assembler::word,\n+               false \/* acquire *\/, true \/* release *\/, false \/* weak *\/, $res$$Register);\n+    __ decode_heap_oop($tmp2$$Register);\n+    g1_post_write_barrier(masm, this,\n+                          $mem$$Register \/* store_addr *\/,\n+                          $tmp2$$Register \/* new_val *\/,\n+                          $tmp1$$Register \/* tmp1 *\/,\n+                          $tmp3$$Register \/* tmp2 *\/);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ This pattern is generated automatically from g1_aarch64.m4.\n+\/\/ DO NOT EDIT ANYTHING IN THIS SECTION OF THE FILE\n+instruct g1CompareAndExchangeNAcq(iRegNNoSp res, indirect mem, iRegN oldval, iRegN newval, iRegPNoSp tmp1, iRegPNoSp tmp2, iRegPNoSp tmp3, rFlagsReg cr)\n+%{\n+  predicate(UseG1GC && needs_acquiring_load_exclusive(n) && n->as_LoadStore()->barrier_data() != 0);\n+  match(Set res (CompareAndExchangeN mem (Binary oldval newval)));\n+  effect(TEMP res, TEMP tmp1, TEMP tmp2, TEMP tmp3, KILL cr);\n+  ins_cost(VOLATILE_REF_COST);\n+  format %{ \"cmpxchg_acq $res = $mem, $oldval, $newval\\t# narrow oop\" %}\n+  ins_encode %{\n+    assert_different_registers($oldval$$Register, $mem$$Register);\n+    assert_different_registers($newval$$Register, $mem$$Register);\n+    g1_pre_write_barrier(masm, this,\n+                         $mem$$Register \/* obj *\/,\n+                         $tmp1$$Register \/* pre_val *\/,\n+                         $tmp2$$Register \/* tmp1 *\/,\n+                         $tmp3$$Register \/* tmp2 *\/,\n+                         RegSet::of($mem$$Register, $oldval$$Register, $newval$$Register) \/* preserve *\/,\n+                         RegSet::of($res$$Register) \/* no_preserve *\/);\n+    __ mov($tmp1$$Register, $oldval$$Register);\n+    __ mov($tmp2$$Register, $newval$$Register);\n+    __ cmpxchg($mem$$Register, $tmp1$$Register, $tmp2$$Register, Assembler::word,\n+               true \/* acquire *\/, true \/* release *\/, false \/* weak *\/, $res$$Register);\n+    __ decode_heap_oop($tmp2$$Register);\n+    g1_post_write_barrier(masm, this,\n+                          $mem$$Register \/* store_addr *\/,\n+                          $tmp2$$Register \/* new_val *\/,\n+                          $tmp1$$Register \/* tmp1 *\/,\n+                          $tmp3$$Register \/* tmp2 *\/);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ This pattern is generated automatically from g1_aarch64.m4.\n+\/\/ DO NOT EDIT ANYTHING IN THIS SECTION OF THE FILE\n+instruct g1CompareAndSwapP(iRegINoSp res, indirect mem, iRegP newval, iRegPNoSp tmp1, iRegPNoSp tmp2, iRegPNoSp tmp3, iRegP oldval, rFlagsReg cr)\n+%{\n+  predicate(UseG1GC && !needs_acquiring_load_exclusive(n) && n->as_LoadStore()->barrier_data() != 0);\n+  match(Set res (CompareAndSwapP mem (Binary oldval newval)));\n+  match(Set res (WeakCompareAndSwapP mem (Binary oldval newval)));\n+  effect(TEMP res, TEMP tmp1, TEMP tmp2, TEMP tmp3, KILL cr);\n+  ins_cost(2 * VOLATILE_REF_COST);\n+  format %{ \"cmpxchg $mem, $oldval, $newval\\t# (ptr)\\n\\t\"\n+            \"cset $res, EQ\" %}\n+  ins_encode %{\n+    assert_different_registers($oldval$$Register, $mem$$Register);\n+    assert_different_registers($newval$$Register, $mem$$Register);\n+    g1_pre_write_barrier(masm, this,\n+                         $mem$$Register \/* obj *\/,\n+                         $tmp1$$Register \/* pre_val *\/,\n+                         $tmp2$$Register \/* tmp1 *\/,\n+                         $tmp3$$Register \/* tmp2 *\/,\n+                         RegSet::of($mem$$Register, $oldval$$Register, $newval$$Register) \/* preserve *\/,\n+                         RegSet::of($res$$Register) \/* no_preserve *\/);\n+    __ mov($tmp1$$Register, $oldval$$Register);\n+    __ mov($tmp2$$Register, $newval$$Register);\n+    __ cmpxchg($mem$$Register, $tmp1$$Register, $tmp2$$Register, Assembler::xword,\n+               false \/* acquire *\/, true \/* release *\/, false \/* weak *\/, noreg);\n+    __ cset($res$$Register, Assembler::EQ);\n+    g1_post_write_barrier(masm, this,\n+                          $mem$$Register \/* store_addr *\/,\n+                          $tmp2$$Register \/* new_val *\/,\n+                          $tmp1$$Register \/* tmp1 *\/,\n+                          $tmp3$$Register \/* tmp2 *\/);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ This pattern is generated automatically from g1_aarch64.m4.\n+\/\/ DO NOT EDIT ANYTHING IN THIS SECTION OF THE FILE\n+instruct g1CompareAndSwapPAcq(iRegINoSp res, indirect mem, iRegP newval, iRegPNoSp tmp1, iRegPNoSp tmp2, iRegPNoSp tmp3, iRegP oldval, rFlagsReg cr)\n+%{\n+  predicate(UseG1GC && needs_acquiring_load_exclusive(n) && n->as_LoadStore()->barrier_data() != 0);\n+  match(Set res (CompareAndSwapP mem (Binary oldval newval)));\n+  match(Set res (WeakCompareAndSwapP mem (Binary oldval newval)));\n+  effect(TEMP res, TEMP tmp1, TEMP tmp2, TEMP tmp3, KILL cr);\n+  ins_cost(VOLATILE_REF_COST);\n+  format %{ \"cmpxchg_acq $mem, $oldval, $newval\\t# (ptr)\\n\\t\"\n+            \"cset $res, EQ\" %}\n+  ins_encode %{\n+    assert_different_registers($oldval$$Register, $mem$$Register);\n+    assert_different_registers($newval$$Register, $mem$$Register);\n+    g1_pre_write_barrier(masm, this,\n+                         $mem$$Register \/* obj *\/,\n+                         $tmp1$$Register \/* pre_val *\/,\n+                         $tmp2$$Register \/* tmp1 *\/,\n+                         $tmp3$$Register \/* tmp2 *\/,\n+                         RegSet::of($mem$$Register, $oldval$$Register, $newval$$Register) \/* preserve *\/,\n+                         RegSet::of($res$$Register) \/* no_preserve *\/);\n+    __ mov($tmp1$$Register, $oldval$$Register);\n+    __ mov($tmp2$$Register, $newval$$Register);\n+    __ cmpxchg($mem$$Register, $tmp1$$Register, $tmp2$$Register, Assembler::xword,\n+               true \/* acquire *\/, true \/* release *\/, false \/* weak *\/, noreg);\n+    __ cset($res$$Register, Assembler::EQ);\n+    g1_post_write_barrier(masm, this,\n+                          $mem$$Register \/* store_addr *\/,\n+                          $tmp2$$Register \/* new_val *\/,\n+                          $tmp1$$Register \/* tmp1 *\/,\n+                          $tmp3$$Register \/* tmp2 *\/);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ This pattern is generated automatically from g1_aarch64.m4.\n+\/\/ DO NOT EDIT ANYTHING IN THIS SECTION OF THE FILE\n+instruct g1CompareAndSwapN(iRegINoSp res, indirect mem, iRegN newval, iRegPNoSp tmp1, iRegPNoSp tmp2, iRegPNoSp tmp3, iRegN oldval, rFlagsReg cr)\n+%{\n+  predicate(UseG1GC && !needs_acquiring_load_exclusive(n) && n->as_LoadStore()->barrier_data() != 0);\n+  match(Set res (CompareAndSwapN mem (Binary oldval newval)));\n+  match(Set res (WeakCompareAndSwapN mem (Binary oldval newval)));\n+  effect(TEMP res, TEMP tmp1, TEMP tmp2, TEMP tmp3, KILL cr);\n+  ins_cost(2 * VOLATILE_REF_COST);\n+  format %{ \"cmpxchg $mem, $oldval, $newval\\t# (narrow oop)\\n\\t\"\n+            \"cset $res, EQ\" %}\n+  ins_encode %{\n+    assert_different_registers($oldval$$Register, $mem$$Register);\n+    assert_different_registers($newval$$Register, $mem$$Register);\n+    g1_pre_write_barrier(masm, this,\n+                         $mem$$Register \/* obj *\/,\n+                         $tmp1$$Register \/* pre_val *\/,\n+                         $tmp2$$Register \/* tmp1 *\/,\n+                         $tmp3$$Register \/* tmp2 *\/,\n+                         RegSet::of($mem$$Register, $oldval$$Register, $newval$$Register) \/* preserve *\/,\n+                         RegSet::of($res$$Register) \/* no_preserve *\/);\n+    __ mov($tmp1$$Register, $oldval$$Register);\n+    __ mov($tmp2$$Register, $newval$$Register);\n+    __ cmpxchg($mem$$Register, $tmp1$$Register, $tmp2$$Register, Assembler::word,\n+               false \/* acquire *\/, true \/* release *\/, false \/* weak *\/, noreg);\n+    __ cset($res$$Register, Assembler::EQ);\n+    __ decode_heap_oop($tmp2$$Register);\n+    g1_post_write_barrier(masm, this,\n+                          $mem$$Register \/* store_addr *\/,\n+                          $tmp2$$Register \/* new_val *\/,\n+                          $tmp1$$Register \/* tmp1 *\/,\n+                          $tmp3$$Register \/* tmp2 *\/);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ This pattern is generated automatically from g1_aarch64.m4.\n+\/\/ DO NOT EDIT ANYTHING IN THIS SECTION OF THE FILE\n+instruct g1CompareAndSwapNAcq(iRegINoSp res, indirect mem, iRegN newval, iRegPNoSp tmp1, iRegPNoSp tmp2, iRegPNoSp tmp3, iRegN oldval, rFlagsReg cr)\n+%{\n+  predicate(UseG1GC && needs_acquiring_load_exclusive(n) && n->as_LoadStore()->barrier_data() != 0);\n+  match(Set res (CompareAndSwapN mem (Binary oldval newval)));\n+  match(Set res (WeakCompareAndSwapN mem (Binary oldval newval)));\n+  effect(TEMP res, TEMP tmp1, TEMP tmp2, TEMP tmp3, KILL cr);\n+  ins_cost(VOLATILE_REF_COST);\n+  format %{ \"cmpxchg_acq $mem, $oldval, $newval\\t# (narrow oop)\\n\\t\"\n+            \"cset $res, EQ\" %}\n+  ins_encode %{\n+    assert_different_registers($oldval$$Register, $mem$$Register);\n+    assert_different_registers($newval$$Register, $mem$$Register);\n+    g1_pre_write_barrier(masm, this,\n+                         $mem$$Register \/* obj *\/,\n+                         $tmp1$$Register \/* pre_val *\/,\n+                         $tmp2$$Register \/* tmp1 *\/,\n+                         $tmp3$$Register \/* tmp2 *\/,\n+                         RegSet::of($mem$$Register, $oldval$$Register, $newval$$Register) \/* preserve *\/,\n+                         RegSet::of($res$$Register) \/* no_preserve *\/);\n+    __ mov($tmp1$$Register, $oldval$$Register);\n+    __ mov($tmp2$$Register, $newval$$Register);\n+    __ cmpxchg($mem$$Register, $tmp1$$Register, $tmp2$$Register, Assembler::word,\n+               true \/* acquire *\/, true \/* release *\/, false \/* weak *\/, noreg);\n+    __ cset($res$$Register, Assembler::EQ);\n+    __ decode_heap_oop($tmp2$$Register);\n+    g1_post_write_barrier(masm, this,\n+                          $mem$$Register \/* store_addr *\/,\n+                          $tmp2$$Register \/* new_val *\/,\n+                          $tmp1$$Register \/* tmp1 *\/,\n+                          $tmp3$$Register \/* tmp2 *\/);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ This pattern is generated automatically from g1_aarch64.m4.\n+\/\/ DO NOT EDIT ANYTHING IN THIS SECTION OF THE FILE\n+instruct g1XChgP(indirect mem, iRegP newval, iRegPNoSp tmp1, iRegPNoSp tmp2, iRegPNoSp preval, rFlagsReg cr)\n+%{\n+  predicate(UseG1GC && !needs_acquiring_load_exclusive(n) && n->as_LoadStore()->barrier_data() != 0);\n+  match(Set preval (GetAndSetP mem newval));\n+  effect(TEMP preval, TEMP tmp1, TEMP tmp2, KILL cr);\n+  ins_cost(2 * VOLATILE_REF_COST);\n+  format %{ \"atomic_xchg  $preval, $newval, [$mem]\" %}\n+  ins_encode %{\n+    assert_different_registers($mem$$Register, $newval$$Register);\n+    g1_pre_write_barrier(masm, this,\n+                         $mem$$Register \/* obj *\/,\n+                         $preval$$Register \/* pre_val (as a temporary register) *\/,\n+                         $tmp1$$Register \/* tmp1 *\/,\n+                         $tmp2$$Register \/* tmp2 *\/,\n+                         RegSet::of($mem$$Register, $preval$$Register, $newval$$Register) \/* preserve *\/);\n+    __ atomic_xchg($preval$$Register, $newval$$Register, $mem$$Register);\n+    g1_post_write_barrier(masm, this,\n+                          $mem$$Register \/* store_addr *\/,\n+                          $newval$$Register \/* new_val *\/,\n+                          $tmp1$$Register \/* tmp1 *\/,\n+                          $tmp2$$Register \/* tmp2 *\/);\n+  %}\n+  ins_pipe(pipe_serial);\n+%}\n+\n+\/\/ This pattern is generated automatically from g1_aarch64.m4.\n+\/\/ DO NOT EDIT ANYTHING IN THIS SECTION OF THE FILE\n+instruct g1XChgPAcq(indirect mem, iRegP newval, iRegPNoSp tmp1, iRegPNoSp tmp2, iRegPNoSp preval, rFlagsReg cr)\n+%{\n+  predicate(UseG1GC && needs_acquiring_load_exclusive(n) && n->as_LoadStore()->barrier_data() != 0);\n+  match(Set preval (GetAndSetP mem newval));\n+  effect(TEMP preval, TEMP tmp1, TEMP tmp2, KILL cr);\n+  ins_cost(VOLATILE_REF_COST);\n+  format %{ \"atomic_xchg_acq  $preval, $newval, [$mem]\" %}\n+  ins_encode %{\n+    assert_different_registers($mem$$Register, $newval$$Register);\n+    g1_pre_write_barrier(masm, this,\n+                         $mem$$Register \/* obj *\/,\n+                         $preval$$Register \/* pre_val (as a temporary register) *\/,\n+                         $tmp1$$Register \/* tmp1 *\/,\n+                         $tmp2$$Register \/* tmp2 *\/,\n+                         RegSet::of($mem$$Register, $preval$$Register, $newval$$Register) \/* preserve *\/);\n+    __ atomic_xchgal($preval$$Register, $newval$$Register, $mem$$Register);\n+    g1_post_write_barrier(masm, this,\n+                          $mem$$Register \/* store_addr *\/,\n+                          $newval$$Register \/* new_val *\/,\n+                          $tmp1$$Register \/* tmp1 *\/,\n+                          $tmp2$$Register \/* tmp2 *\/);\n+  %}\n+  ins_pipe(pipe_serial);\n+%}\n+\n+\/\/ This pattern is generated automatically from g1_aarch64.m4.\n+\/\/ DO NOT EDIT ANYTHING IN THIS SECTION OF THE FILE\n+instruct g1XChgN(indirect mem, iRegN newval, iRegPNoSp tmp1, iRegPNoSp tmp2, iRegPNoSp tmp3, iRegNNoSp preval, rFlagsReg cr)\n+%{\n+  predicate(UseG1GC && !needs_acquiring_load_exclusive(n) && n->as_LoadStore()->barrier_data() != 0);\n+  match(Set preval (GetAndSetN mem newval));\n+  effect(TEMP preval, TEMP tmp1, TEMP tmp2, TEMP tmp3, KILL cr);\n+  ins_cost(2 * VOLATILE_REF_COST);\n+  format %{ \"atomic_xchgw $preval, $newval, [$mem]\" %}\n+  ins_encode %{\n+    assert_different_registers($mem$$Register, $newval$$Register);\n+    g1_pre_write_barrier(masm, this,\n+                         $mem$$Register \/* obj *\/,\n+                         $tmp1$$Register \/* pre_val *\/,\n+                         $tmp2$$Register \/* tmp1 *\/,\n+                         $tmp3$$Register \/* tmp2 *\/,\n+                         RegSet::of($mem$$Register, $preval$$Register, $newval$$Register) \/* preserve *\/);\n+    __ atomic_xchgw($preval$$Register, $newval$$Register, $mem$$Register);\n+    __ decode_heap_oop($tmp1$$Register, $newval$$Register);\n+    g1_post_write_barrier(masm, this,\n+                          $mem$$Register \/* store_addr *\/,\n+                          $tmp1$$Register \/* new_val *\/,\n+                          $tmp2$$Register \/* tmp1 *\/,\n+                          $tmp3$$Register \/* tmp2 *\/);\n+  %}\n+  ins_pipe(pipe_serial);\n+%}\n+\n+\/\/ This pattern is generated automatically from g1_aarch64.m4.\n+\/\/ DO NOT EDIT ANYTHING IN THIS SECTION OF THE FILE\n+instruct g1XChgNAcq(indirect mem, iRegN newval, iRegPNoSp tmp1, iRegPNoSp tmp2, iRegPNoSp tmp3, iRegNNoSp preval, rFlagsReg cr)\n+%{\n+  predicate(UseG1GC && needs_acquiring_load_exclusive(n) && n->as_LoadStore()->barrier_data() != 0);\n+  match(Set preval (GetAndSetN mem newval));\n+  effect(TEMP preval, TEMP tmp1, TEMP tmp2, TEMP tmp3, KILL cr);\n+  ins_cost(VOLATILE_REF_COST);\n+  format %{ \"atomic_xchgw_acq $preval, $newval, [$mem]\" %}\n+  ins_encode %{\n+    assert_different_registers($mem$$Register, $newval$$Register);\n+    g1_pre_write_barrier(masm, this,\n+                         $mem$$Register \/* obj *\/,\n+                         $tmp1$$Register \/* pre_val *\/,\n+                         $tmp2$$Register \/* tmp1 *\/,\n+                         $tmp3$$Register \/* tmp2 *\/,\n+                         RegSet::of($mem$$Register, $preval$$Register, $newval$$Register) \/* preserve *\/);\n+    __ atomic_xchgalw($preval$$Register, $newval$$Register, $mem$$Register);\n+    __ decode_heap_oop($tmp1$$Register, $newval$$Register);\n+    g1_post_write_barrier(masm, this,\n+                          $mem$$Register \/* store_addr *\/,\n+                          $tmp1$$Register \/* new_val *\/,\n+                          $tmp2$$Register \/* tmp1 *\/,\n+                          $tmp3$$Register \/* tmp2 *\/);\n+  %}\n+  ins_pipe(pipe_serial);\n+%}\n+\n+\/\/ This pattern is generated automatically from g1_aarch64.m4.\n+\/\/ DO NOT EDIT ANYTHING IN THIS SECTION OF THE FILE\n+instruct g1LoadP(iRegPNoSp dst, indirect mem, iRegPNoSp tmp1, iRegPNoSp tmp2, rFlagsReg cr)\n+%{\n+  predicate(UseG1GC && !needs_acquiring_load(n) && n->as_Load()->barrier_data() != 0);\n+  match(Set dst (LoadP mem));\n+  effect(TEMP dst, TEMP tmp1, TEMP tmp2, KILL cr);\n+  ins_cost(4 * INSN_COST);\n+  format %{ \"ldr  $dst, $mem\\t# ptr\" %}\n+  ins_encode %{\n+    __ ldr($dst$$Register, $mem$$Register);\n+    g1_pre_write_barrier(masm, this,\n+                         noreg \/* obj *\/,\n+                         $dst$$Register \/* pre_val *\/,\n+                         $tmp1$$Register \/* tmp1 *\/,\n+                         $tmp2$$Register \/* tmp2 *\/);\n+  %}\n+  ins_pipe(iload_reg_mem);\n+%}\n+\n+\/\/ This pattern is generated automatically from g1_aarch64.m4.\n+\/\/ DO NOT EDIT ANYTHING IN THIS SECTION OF THE FILE\n+instruct g1LoadPVolatile(iRegPNoSp dst, indirect mem, iRegPNoSp tmp1, iRegPNoSp tmp2, rFlagsReg cr)\n+%{\n+  predicate(UseG1GC && needs_acquiring_load(n) && n->as_Load()->barrier_data() != 0);\n+  match(Set dst (LoadP mem));\n+  effect(TEMP dst, TEMP tmp1, TEMP tmp2, KILL cr);\n+  ins_cost(VOLATILE_REF_COST);\n+  format %{ \"ldar  $dst, $mem\\t# ptr\" %}\n+  ins_encode %{\n+    __ ldar($dst$$Register, $mem$$Register);\n+    g1_pre_write_barrier(masm, this,\n+                         noreg \/* obj *\/,\n+                         $dst$$Register \/* pre_val *\/,\n+                         $tmp1$$Register \/* tmp1 *\/,\n+                         $tmp2$$Register \/* tmp2 *\/);\n+  %}\n+  ins_pipe(pipe_serial);\n+%}\n+\n+\/\/ This pattern is generated automatically from g1_aarch64.m4.\n+\/\/ DO NOT EDIT ANYTHING IN THIS SECTION OF THE FILE\n+instruct g1LoadN(iRegNNoSp dst, indirect mem, iRegPNoSp tmp1, iRegPNoSp tmp2, iRegPNoSp tmp3, rFlagsReg cr)\n+%{\n+  predicate(UseG1GC && !needs_acquiring_load(n) && n->as_Load()->barrier_data() != 0);\n+  match(Set dst (LoadN mem));\n+  effect(TEMP dst, TEMP tmp1, TEMP tmp2, TEMP tmp3, KILL cr);\n+  ins_cost(4 * INSN_COST);\n+  format %{ \"ldrw  $dst, $mem\\t# compressed ptr\" %}\n+  ins_encode %{\n+    __ ldrw($dst$$Register, $mem$$Register);\n+    if ((barrier_data() & G1C2BarrierPre) != 0) {\n+      __ decode_heap_oop($tmp1$$Register, $dst$$Register);\n+      g1_pre_write_barrier(masm, this,\n+                           noreg \/* obj *\/,\n+                           $tmp1$$Register \/* pre_val *\/,\n+                           $tmp2$$Register \/* tmp1 *\/,\n+                           $tmp3$$Register \/* tmp2 *\/);\n+    }\n+  %}\n+  ins_pipe(iload_reg_mem);\n+%}\n+\n+\/\/ This pattern is generated automatically from g1_aarch64.m4.\n+\/\/ DO NOT EDIT ANYTHING IN THIS SECTION OF THE FILE\n+instruct g1LoadNVolatile(iRegNNoSp dst, indirect mem, iRegPNoSp tmp1, iRegPNoSp tmp2, iRegPNoSp tmp3, rFlagsReg cr)\n+%{\n+  predicate(UseG1GC && needs_acquiring_load(n) && n->as_Load()->barrier_data() != 0);\n+  match(Set dst (LoadN mem));\n+  effect(TEMP dst, TEMP tmp1, TEMP tmp2, TEMP tmp3, KILL cr);\n+  ins_cost(VOLATILE_REF_COST);\n+  format %{ \"ldarw  $dst, $mem\\t# compressed ptr\" %}\n+  ins_encode %{\n+    __ ldarw($dst$$Register, $mem$$Register);\n+    if ((barrier_data() & G1C2BarrierPre) != 0) {\n+      __ decode_heap_oop($tmp1$$Register, $dst$$Register);\n+      g1_pre_write_barrier(masm, this,\n+                           noreg \/* obj *\/,\n+                           $tmp1$$Register \/* pre_val *\/,\n+                           $tmp2$$Register \/* tmp1 *\/,\n+                           $tmp3$$Register \/* tmp2 *\/);\n+    }\n+  %}\n+  ins_pipe(pipe_serial);\n+%}\n+\n+\/\/ END This section of the file is automatically generated. Do not edit --------------\n","filename":"src\/hotspot\/cpu\/aarch64\/gc\/g1\/g1_aarch64.ad","additions":702,"deletions":0,"binary":false,"changes":702,"status":"added"},{"patch":"@@ -0,0 +1,394 @@\n+dnl Copyright (c) 2024, Oracle and\/or its affiliates. All rights reserved.\n+dnl DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+dnl\n+dnl This code is free software; you can redistribute it and\/or modify it\n+dnl under the terms of the GNU General Public License version 2 only, as\n+dnl published by the Free Software Foundation.\n+dnl\n+dnl This code is distributed in the hope that it will be useful, but WITHOUT\n+dnl ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+dnl FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+dnl version 2 for more details (a copy is included in the LICENSE file that\n+dnl accompanied this code).\n+dnl\n+dnl You should have received a copy of the GNU General Public License version\n+dnl 2 along with this work; if not, write to the Free Software Foundation,\n+dnl Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+dnl\n+dnl Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+dnl or visit www.oracle.com if you need additional information or have any\n+dnl questions.\n+dnl\n+\/\/ BEGIN This section of the file is automatically generated. Do not edit --------------\n+\n+\/\/ This section is generated from g1_aarch64.m4\n+\n+define(`STOREP_INSN',\n+`\n+\/\/ This pattern is generated automatically from g1_aarch64.m4.\n+\/\/ DO NOT EDIT ANYTHING IN THIS SECTION OF THE FILE\n+instruct g1StoreP$1(indirect mem, iRegP src, iRegPNoSp tmp1, iRegPNoSp tmp2, iRegPNoSp tmp3, rFlagsReg cr)\n+%{\n+  predicate(UseG1GC && ifelse($1,Volatile,'needs_releasing_store(n)`,'!needs_releasing_store(n)`) && n->as_Store()->barrier_data() != 0);\n+  match(Set mem (StoreP mem src));\n+  effect(TEMP tmp1, TEMP tmp2, TEMP tmp3, KILL cr);\n+  ins_cost(ifelse($1,Volatile,VOLATILE_REF_COST,INSN_COST));\n+  format %{ \"$2  $src, $mem\\t# ptr\" %}\n+  ins_encode %{\n+    g1_pre_write_barrier(masm, this,\n+                         $mem$$Register \/* obj *\/,\n+                         $tmp1$$Register \/* pre_val *\/,\n+                         $tmp2$$Register \/* tmp1 *\/,\n+                         $tmp3$$Register \/* tmp2 *\/,\n+                         RegSet::of($mem$$Register, $src$$Register) \/* preserve *\/);\n+    __ $2($src$$Register, $mem$$Register);\n+    g1_post_write_barrier(masm, this,\n+                          $mem$$Register \/* store_addr *\/,\n+                          $src$$Register \/* new_val *\/,\n+                          $tmp2$$Register \/* tmp1 *\/,\n+                          $tmp3$$Register \/* tmp2 *\/);\n+  %}\n+  ins_pipe(ifelse($1,Volatile,pipe_class_memory,istore_reg_mem));\n+%}')dnl\n+STOREP_INSN(,str)\n+STOREP_INSN(Volatile,stlr)\n+dnl\n+define(`STOREN_INSN',\n+`\n+\/\/ This pattern is generated automatically from g1_aarch64.m4.\n+\/\/ DO NOT EDIT ANYTHING IN THIS SECTION OF THE FILE\n+instruct g1StoreN$1(indirect mem, iRegN src, iRegPNoSp tmp1, iRegPNoSp tmp2, iRegPNoSp tmp3, rFlagsReg cr)\n+%{\n+  predicate(UseG1GC && ifelse($1,Volatile,'needs_releasing_store(n)`,'!needs_releasing_store(n)`) && n->as_Store()->barrier_data() != 0);\n+  match(Set mem (StoreN mem src));\n+  effect(TEMP tmp1, TEMP tmp2, TEMP tmp3, KILL cr);\n+  ins_cost(ifelse($1,Volatile,VOLATILE_REF_COST,INSN_COST));\n+  format %{ \"$2  $src, $mem\\t# compressed ptr\" %}\n+  ins_encode %{\n+    g1_pre_write_barrier(masm, this,\n+                         $mem$$Register \/* obj *\/,\n+                         $tmp1$$Register \/* pre_val *\/,\n+                         $tmp2$$Register \/* tmp1 *\/,\n+                         $tmp3$$Register \/* tmp2 *\/,\n+                         RegSet::of($mem$$Register, $src$$Register) \/* preserve *\/);\n+    __ $2($src$$Register, $mem$$Register);\n+    if ((barrier_data() & G1C2BarrierPost) != 0) {\n+      if ((barrier_data() & G1C2BarrierPostNotNull) == 0) {\n+        __ decode_heap_oop($tmp1$$Register, $src$$Register);\n+      } else {\n+        __ decode_heap_oop_not_null($tmp1$$Register, $src$$Register);\n+      }\n+    }\n+    g1_post_write_barrier(masm, this,\n+                          $mem$$Register \/* store_addr *\/,\n+                          $tmp1$$Register \/* new_val *\/,\n+                          $tmp2$$Register \/* tmp1 *\/,\n+                          $tmp3$$Register \/* tmp2 *\/);\n+  %}\n+  ins_pipe(ifelse($1,Volatile,pipe_class_memory,istore_reg_mem));\n+%}')dnl\n+STOREN_INSN(,strw)\n+\n+\/\/ This pattern is generated automatically from g1_aarch64.m4.\n+\/\/ DO NOT EDIT ANYTHING IN THIS SECTION OF THE FILE\n+instruct g1EncodePAndStoreN(indirect mem, iRegP src, iRegPNoSp tmp1, iRegPNoSp tmp2, iRegPNoSp tmp3, rFlagsReg cr)\n+%{\n+  predicate(UseG1GC && !needs_releasing_store(n) && n->as_Store()->barrier_data() != 0);\n+  match(Set mem (StoreN mem (EncodeP src)));\n+  effect(TEMP tmp1, TEMP tmp2, TEMP tmp3, KILL cr);\n+  ins_cost(INSN_COST);\n+  format %{ \"encode_heap_oop $tmp1, $src\\n\\t\"\n+            \"strw  $tmp1, $mem\\t# compressed ptr\" %}\n+  ins_encode %{\n+    g1_pre_write_barrier(masm, this,\n+                         $mem$$Register \/* obj *\/,\n+                         $tmp1$$Register \/* pre_val *\/,\n+                         $tmp2$$Register \/* tmp1 *\/,\n+                         $tmp3$$Register \/* tmp2 *\/,\n+                         RegSet::of($mem$$Register, $src$$Register) \/* preserve *\/);\n+    if ((barrier_data() & G1C2BarrierPostNotNull) == 0) {\n+      __ encode_heap_oop($tmp1$$Register, $src$$Register);\n+    } else {\n+      __ encode_heap_oop_not_null($tmp1$$Register, $src$$Register);\n+    }\n+    __ strw($tmp1$$Register, $mem$$Register);\n+    g1_post_write_barrier(masm, this,\n+                          $mem$$Register \/* store_addr *\/,\n+                          $src$$Register \/* new_val *\/,\n+                          $tmp2$$Register \/* tmp1 *\/,\n+                          $tmp3$$Register \/* tmp2 *\/);\n+  %}\n+  ins_pipe(istore_reg_mem);\n+%}\n+\n+\/\/ Very few of the total executed stores are volatile (less than 1% across\n+\/\/ multiple benchmark suites), no need to define an encode-and-store version.\n+STOREN_INSN(Volatile,stlrw)\n+dnl\n+define(`CAEP_INSN',\n+`\n+\/\/ This pattern is generated automatically from g1_aarch64.m4.\n+\/\/ DO NOT EDIT ANYTHING IN THIS SECTION OF THE FILE\n+instruct g1CompareAndExchangeP$1(iRegPNoSp res, indirect mem, iRegP oldval, iRegP newval, iRegPNoSp tmp1, iRegPNoSp tmp2, iRegPNoSp tmp3, rFlagsReg cr)\n+%{\n+  predicate(UseG1GC && ifelse($1,Acq,'needs_acquiring_load_exclusive(n)`,'!needs_acquiring_load_exclusive(n)`) && n->as_LoadStore()->barrier_data() != 0);\n+  match(Set res (CompareAndExchangeP mem (Binary oldval newval)));\n+  effect(TEMP res, TEMP tmp1, TEMP tmp2, TEMP tmp3, KILL cr);\n+  ins_cost(ifelse($1,Acq,VOLATILE_REF_COST,2 * VOLATILE_REF_COST));\n+  format %{ \"cmpxchg$2 $res = $mem, $oldval, $newval\\t# ptr\" %}\n+  ins_encode %{\n+    assert_different_registers($oldval$$Register, $mem$$Register);\n+    assert_different_registers($newval$$Register, $mem$$Register);\n+    g1_pre_write_barrier(masm, this,\n+                         $mem$$Register \/* obj *\/,\n+                         $tmp1$$Register \/* pre_val *\/,\n+                         $tmp2$$Register \/* tmp1 *\/,\n+                         $tmp3$$Register \/* tmp2 *\/,\n+                         RegSet::of($mem$$Register, $oldval$$Register, $newval$$Register) \/* preserve *\/,\n+                         RegSet::of($res$$Register) \/* no_preserve *\/);\n+    __ mov($tmp1$$Register, $oldval$$Register);\n+    __ mov($tmp2$$Register, $newval$$Register);\n+    __ cmpxchg($mem$$Register, $tmp1$$Register, $tmp2$$Register, Assembler::xword,\n+               $3 \/* acquire *\/, true \/* release *\/, false \/* weak *\/, $res$$Register);\n+    g1_post_write_barrier(masm, this,\n+                          $mem$$Register \/* store_addr *\/,\n+                          $tmp2$$Register \/* new_val *\/,\n+                          $tmp1$$Register \/* tmp1 *\/,\n+                          $tmp3$$Register \/* tmp2 *\/);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}')dnl\n+CAEP_INSN(,,false)\n+CAEP_INSN(Acq,_acq,true)\n+dnl\n+define(`CAEN_INSN',\n+`\n+\/\/ This pattern is generated automatically from g1_aarch64.m4.\n+\/\/ DO NOT EDIT ANYTHING IN THIS SECTION OF THE FILE\n+instruct g1CompareAndExchangeN$1(iRegNNoSp res, indirect mem, iRegN oldval, iRegN newval, iRegPNoSp tmp1, iRegPNoSp tmp2, iRegPNoSp tmp3, rFlagsReg cr)\n+%{\n+  predicate(UseG1GC && ifelse($1,Acq,'needs_acquiring_load_exclusive(n)`,'!needs_acquiring_load_exclusive(n)`) && n->as_LoadStore()->barrier_data() != 0);\n+  match(Set res (CompareAndExchangeN mem (Binary oldval newval)));\n+  effect(TEMP res, TEMP tmp1, TEMP tmp2, TEMP tmp3, KILL cr);\n+  ins_cost(ifelse($1,Acq,VOLATILE_REF_COST,2 * VOLATILE_REF_COST));\n+  format %{ \"cmpxchg$2 $res = $mem, $oldval, $newval\\t# narrow oop\" %}\n+  ins_encode %{\n+    assert_different_registers($oldval$$Register, $mem$$Register);\n+    assert_different_registers($newval$$Register, $mem$$Register);\n+    g1_pre_write_barrier(masm, this,\n+                         $mem$$Register \/* obj *\/,\n+                         $tmp1$$Register \/* pre_val *\/,\n+                         $tmp2$$Register \/* tmp1 *\/,\n+                         $tmp3$$Register \/* tmp2 *\/,\n+                         RegSet::of($mem$$Register, $oldval$$Register, $newval$$Register) \/* preserve *\/,\n+                         RegSet::of($res$$Register) \/* no_preserve *\/);\n+    __ mov($tmp1$$Register, $oldval$$Register);\n+    __ mov($tmp2$$Register, $newval$$Register);\n+    __ cmpxchg($mem$$Register, $tmp1$$Register, $tmp2$$Register, Assembler::word,\n+               $3 \/* acquire *\/, true \/* release *\/, false \/* weak *\/, $res$$Register);\n+    __ decode_heap_oop($tmp2$$Register);\n+    g1_post_write_barrier(masm, this,\n+                          $mem$$Register \/* store_addr *\/,\n+                          $tmp2$$Register \/* new_val *\/,\n+                          $tmp1$$Register \/* tmp1 *\/,\n+                          $tmp3$$Register \/* tmp2 *\/);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}')dnl\n+CAEN_INSN(,,false)\n+CAEN_INSN(Acq,_acq,true)\n+dnl\n+define(`CASP_INSN',\n+`\n+\/\/ This pattern is generated automatically from g1_aarch64.m4.\n+\/\/ DO NOT EDIT ANYTHING IN THIS SECTION OF THE FILE\n+instruct g1CompareAndSwapP$1(iRegINoSp res, indirect mem, iRegP newval, iRegPNoSp tmp1, iRegPNoSp tmp2, iRegPNoSp tmp3, iRegP oldval, rFlagsReg cr)\n+%{\n+  predicate(UseG1GC && ifelse($1,Acq,'needs_acquiring_load_exclusive(n)`,'!needs_acquiring_load_exclusive(n)`) && n->as_LoadStore()->barrier_data() != 0);\n+  match(Set res (CompareAndSwapP mem (Binary oldval newval)));\n+  match(Set res (WeakCompareAndSwapP mem (Binary oldval newval)));\n+  effect(TEMP res, TEMP tmp1, TEMP tmp2, TEMP tmp3, KILL cr);\n+  ins_cost(ifelse($1,Acq,VOLATILE_REF_COST,2 * VOLATILE_REF_COST));\n+  format %{ \"cmpxchg$2 $mem, $oldval, $newval\\t# (ptr)\\n\\t\"\n+            \"cset $res, EQ\" %}\n+  ins_encode %{\n+    assert_different_registers($oldval$$Register, $mem$$Register);\n+    assert_different_registers($newval$$Register, $mem$$Register);\n+    g1_pre_write_barrier(masm, this,\n+                         $mem$$Register \/* obj *\/,\n+                         $tmp1$$Register \/* pre_val *\/,\n+                         $tmp2$$Register \/* tmp1 *\/,\n+                         $tmp3$$Register \/* tmp2 *\/,\n+                         RegSet::of($mem$$Register, $oldval$$Register, $newval$$Register) \/* preserve *\/,\n+                         RegSet::of($res$$Register) \/* no_preserve *\/);\n+    __ mov($tmp1$$Register, $oldval$$Register);\n+    __ mov($tmp2$$Register, $newval$$Register);\n+    __ cmpxchg($mem$$Register, $tmp1$$Register, $tmp2$$Register, Assembler::xword,\n+               $3 \/* acquire *\/, true \/* release *\/, false \/* weak *\/, noreg);\n+    __ cset($res$$Register, Assembler::EQ);\n+    g1_post_write_barrier(masm, this,\n+                          $mem$$Register \/* store_addr *\/,\n+                          $tmp2$$Register \/* new_val *\/,\n+                          $tmp1$$Register \/* tmp1 *\/,\n+                          $tmp3$$Register \/* tmp2 *\/);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}')dnl\n+CASP_INSN(,,false)\n+CASP_INSN(Acq,_acq,true)\n+dnl\n+define(`CASN_INSN',\n+`\n+\/\/ This pattern is generated automatically from g1_aarch64.m4.\n+\/\/ DO NOT EDIT ANYTHING IN THIS SECTION OF THE FILE\n+instruct g1CompareAndSwapN$1(iRegINoSp res, indirect mem, iRegN newval, iRegPNoSp tmp1, iRegPNoSp tmp2, iRegPNoSp tmp3, iRegN oldval, rFlagsReg cr)\n+%{\n+  predicate(UseG1GC && ifelse($1,Acq,'needs_acquiring_load_exclusive(n)`,'!needs_acquiring_load_exclusive(n)`) && n->as_LoadStore()->barrier_data() != 0);\n+  match(Set res (CompareAndSwapN mem (Binary oldval newval)));\n+  match(Set res (WeakCompareAndSwapN mem (Binary oldval newval)));\n+  effect(TEMP res, TEMP tmp1, TEMP tmp2, TEMP tmp3, KILL cr);\n+  ins_cost(ifelse($1,Acq,VOLATILE_REF_COST,2 * VOLATILE_REF_COST));\n+  format %{ \"cmpxchg$2 $mem, $oldval, $newval\\t# (narrow oop)\\n\\t\"\n+            \"cset $res, EQ\" %}\n+  ins_encode %{\n+    assert_different_registers($oldval$$Register, $mem$$Register);\n+    assert_different_registers($newval$$Register, $mem$$Register);\n+    g1_pre_write_barrier(masm, this,\n+                         $mem$$Register \/* obj *\/,\n+                         $tmp1$$Register \/* pre_val *\/,\n+                         $tmp2$$Register \/* tmp1 *\/,\n+                         $tmp3$$Register \/* tmp2 *\/,\n+                         RegSet::of($mem$$Register, $oldval$$Register, $newval$$Register) \/* preserve *\/,\n+                         RegSet::of($res$$Register) \/* no_preserve *\/);\n+    __ mov($tmp1$$Register, $oldval$$Register);\n+    __ mov($tmp2$$Register, $newval$$Register);\n+    __ cmpxchg($mem$$Register, $tmp1$$Register, $tmp2$$Register, Assembler::word,\n+               $3 \/* acquire *\/, true \/* release *\/, false \/* weak *\/, noreg);\n+    __ cset($res$$Register, Assembler::EQ);\n+    __ decode_heap_oop($tmp2$$Register);\n+    g1_post_write_barrier(masm, this,\n+                          $mem$$Register \/* store_addr *\/,\n+                          $tmp2$$Register \/* new_val *\/,\n+                          $tmp1$$Register \/* tmp1 *\/,\n+                          $tmp3$$Register \/* tmp2 *\/);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}')dnl\n+CASN_INSN(,,false)\n+CASN_INSN(Acq,_acq,true)\n+dnl\n+define(`XCHGP_INSN',\n+`\n+\/\/ This pattern is generated automatically from g1_aarch64.m4.\n+\/\/ DO NOT EDIT ANYTHING IN THIS SECTION OF THE FILE\n+instruct g1XChgP$1(indirect mem, iRegP newval, iRegPNoSp tmp1, iRegPNoSp tmp2, iRegPNoSp preval, rFlagsReg cr)\n+%{\n+  predicate(UseG1GC && ifelse($1,Acq,'needs_acquiring_load_exclusive(n)`,'!needs_acquiring_load_exclusive(n)`) && n->as_LoadStore()->barrier_data() != 0);\n+  match(Set preval (GetAndSetP mem newval));\n+  effect(TEMP preval, TEMP tmp1, TEMP tmp2, KILL cr);\n+  ins_cost(ifelse($1,Acq,VOLATILE_REF_COST,2 * VOLATILE_REF_COST));\n+  format %{ \"atomic_xchg$2  $preval, $newval, [$mem]\" %}\n+  ins_encode %{\n+    assert_different_registers($mem$$Register, $newval$$Register);\n+    g1_pre_write_barrier(masm, this,\n+                         $mem$$Register \/* obj *\/,\n+                         $preval$$Register \/* pre_val (as a temporary register) *\/,\n+                         $tmp1$$Register \/* tmp1 *\/,\n+                         $tmp2$$Register \/* tmp2 *\/,\n+                         RegSet::of($mem$$Register, $preval$$Register, $newval$$Register) \/* preserve *\/);\n+    __ $3($preval$$Register, $newval$$Register, $mem$$Register);\n+    g1_post_write_barrier(masm, this,\n+                          $mem$$Register \/* store_addr *\/,\n+                          $newval$$Register \/* new_val *\/,\n+                          $tmp1$$Register \/* tmp1 *\/,\n+                          $tmp2$$Register \/* tmp2 *\/);\n+  %}\n+  ins_pipe(pipe_serial);\n+%}')dnl\n+XCHGP_INSN(,,atomic_xchg)\n+XCHGP_INSN(Acq,_acq,atomic_xchgal)\n+dnl\n+define(`XCHGN_INSN',\n+`\n+\/\/ This pattern is generated automatically from g1_aarch64.m4.\n+\/\/ DO NOT EDIT ANYTHING IN THIS SECTION OF THE FILE\n+instruct g1XChgN$1(indirect mem, iRegN newval, iRegPNoSp tmp1, iRegPNoSp tmp2, iRegPNoSp tmp3, iRegNNoSp preval, rFlagsReg cr)\n+%{\n+  predicate(UseG1GC && ifelse($1,Acq,'needs_acquiring_load_exclusive(n)`,'!needs_acquiring_load_exclusive(n)`) && n->as_LoadStore()->barrier_data() != 0);\n+  match(Set preval (GetAndSetN mem newval));\n+  effect(TEMP preval, TEMP tmp1, TEMP tmp2, TEMP tmp3, KILL cr);\n+  ins_cost(ifelse($1,Acq,VOLATILE_REF_COST,2 * VOLATILE_REF_COST));\n+  format %{ \"$2 $preval, $newval, [$mem]\" %}\n+  ins_encode %{\n+    assert_different_registers($mem$$Register, $newval$$Register);\n+    g1_pre_write_barrier(masm, this,\n+                         $mem$$Register \/* obj *\/,\n+                         $tmp1$$Register \/* pre_val *\/,\n+                         $tmp2$$Register \/* tmp1 *\/,\n+                         $tmp3$$Register \/* tmp2 *\/,\n+                         RegSet::of($mem$$Register, $preval$$Register, $newval$$Register) \/* preserve *\/);\n+    __ $3($preval$$Register, $newval$$Register, $mem$$Register);\n+    __ decode_heap_oop($tmp1$$Register, $newval$$Register);\n+    g1_post_write_barrier(masm, this,\n+                          $mem$$Register \/* store_addr *\/,\n+                          $tmp1$$Register \/* new_val *\/,\n+                          $tmp2$$Register \/* tmp1 *\/,\n+                          $tmp3$$Register \/* tmp2 *\/);\n+  %}\n+  ins_pipe(pipe_serial);\n+%}')dnl\n+XCHGN_INSN(,atomic_xchgw,atomic_xchgw)\n+XCHGN_INSN(Acq,atomic_xchgw_acq,atomic_xchgalw)\n+dnl\n+define(`LOADP_INSN',\n+`\n+\/\/ This pattern is generated automatically from g1_aarch64.m4.\n+\/\/ DO NOT EDIT ANYTHING IN THIS SECTION OF THE FILE\n+instruct g1LoadP$1(iRegPNoSp dst, indirect mem, iRegPNoSp tmp1, iRegPNoSp tmp2, rFlagsReg cr)\n+%{\n+  predicate(UseG1GC && ifelse($1,Volatile,'needs_acquiring_load(n)`,'!needs_acquiring_load(n)`) && n->as_Load()->barrier_data() != 0);\n+  match(Set dst (LoadP mem));\n+  effect(TEMP dst, TEMP tmp1, TEMP tmp2, KILL cr);\n+  ins_cost(ifelse($1,Volatile,VOLATILE_REF_COST,4 * INSN_COST));\n+  format %{ \"$2  $dst, $mem\\t# ptr\" %}\n+  ins_encode %{\n+    __ $2($dst$$Register, $mem$$Register);\n+    g1_pre_write_barrier(masm, this,\n+                         noreg \/* obj *\/,\n+                         $dst$$Register \/* pre_val *\/,\n+                         $tmp1$$Register \/* tmp1 *\/,\n+                         $tmp2$$Register \/* tmp2 *\/);\n+  %}\n+  ins_pipe(ifelse($1,Volatile,pipe_serial,iload_reg_mem));\n+%}')dnl\n+LOADP_INSN(,ldr)\n+LOADP_INSN(Volatile,ldar)\n+dnl\n+define(`LOADN_INSN',\n+`\n+\/\/ This pattern is generated automatically from g1_aarch64.m4.\n+\/\/ DO NOT EDIT ANYTHING IN THIS SECTION OF THE FILE\n+instruct g1LoadN$1(iRegNNoSp dst, indirect mem, iRegPNoSp tmp1, iRegPNoSp tmp2, iRegPNoSp tmp3, rFlagsReg cr)\n+%{\n+  predicate(UseG1GC && ifelse($1,Volatile,'needs_acquiring_load(n)`,'!needs_acquiring_load(n)`) && n->as_Load()->barrier_data() != 0);\n+  match(Set dst (LoadN mem));\n+  effect(TEMP dst, TEMP tmp1, TEMP tmp2, TEMP tmp3, KILL cr);\n+  ins_cost(ifelse($1,Volatile,VOLATILE_REF_COST,4 * INSN_COST));\n+  format %{ \"$2  $dst, $mem\\t# compressed ptr\" %}\n+  ins_encode %{\n+    __ $2($dst$$Register, $mem$$Register);\n+    if ((barrier_data() & G1C2BarrierPre) != 0) {\n+      __ decode_heap_oop($tmp1$$Register, $dst$$Register);\n+      g1_pre_write_barrier(masm, this,\n+                           noreg \/* obj *\/,\n+                           $tmp1$$Register \/* pre_val *\/,\n+                           $tmp2$$Register \/* tmp1 *\/,\n+                           $tmp3$$Register \/* tmp2 *\/);\n+    }\n+  %}\n+  ins_pipe(ifelse($1,Volatile,pipe_serial,iload_reg_mem));\n+%}')dnl\n+LOADN_INSN(,ldrw)\n+LOADN_INSN(Volatile,ldarw)\n+\n+\/\/ END This section of the file is automatically generated. Do not edit --------------\n","filename":"src\/hotspot\/cpu\/aarch64\/gc\/g1\/g1_aarch64.m4","additions":394,"deletions":0,"binary":false,"changes":394,"status":"added"},{"patch":"@@ -41,1 +41,4 @@\n-#endif\n+#endif \/\/ COMPILER1\n+#ifdef COMPILER2\n+#include \"gc\/g1\/c2\/g1BarrierSetC2.hpp\"\n+#endif \/\/ COMPILER2\n@@ -163,0 +166,50 @@\n+static void generate_queue_insertion(MacroAssembler* masm, ByteSize index_offset, ByteSize buffer_offset, Label& runtime,\n+                                     const Register thread, const Register value, const Register temp) {\n+  \/\/ This code assumes that buffer index is pointer sized.\n+  STATIC_ASSERT(in_bytes(SATBMarkQueue::byte_width_of_index()) == sizeof(intptr_t));\n+  \/\/ Can we store a value in the given thread's buffer?\n+  \/\/ (The index field is typed as size_t.)\n+  __ movptr(temp, Address(thread, in_bytes(index_offset)));   \/\/ temp := *(index address)\n+  __ testptr(temp, temp);                                     \/\/ index == 0?\n+  __ jcc(Assembler::zero, runtime);                           \/\/ jump to runtime if index == 0 (full buffer)\n+  \/\/ The buffer is not full, store value into it.\n+  __ subptr(temp, wordSize);                                  \/\/ temp := next index\n+  __ movptr(Address(thread, in_bytes(index_offset)), temp);   \/\/ *(index address) := next index\n+  __ addptr(temp, Address(thread, in_bytes(buffer_offset)));  \/\/ temp := buffer address + next index\n+  __ movptr(Address(temp, 0), value);                         \/\/ *(buffer address + next index) := value\n+}\n+\n+static void generate_pre_barrier_fast_path(MacroAssembler* masm,\n+                                           const Register thread) {\n+  Address in_progress(thread, in_bytes(G1ThreadLocalData::satb_mark_queue_active_offset()));\n+  \/\/ Is marking active?\n+  if (in_bytes(SATBMarkQueue::byte_width_of_active()) == 4) {\n+    __ cmpl(in_progress, 0);\n+  } else {\n+    assert(in_bytes(SATBMarkQueue::byte_width_of_active()) == 1, \"Assumption\");\n+    __ cmpb(in_progress, 0);\n+  }\n+}\n+\n+static void generate_pre_barrier_slow_path(MacroAssembler* masm,\n+                                           const Register obj,\n+                                           const Register pre_val,\n+                                           const Register thread,\n+                                           const Register tmp,\n+                                           Label& done,\n+                                           Label& runtime) {\n+  \/\/ Do we need to load the previous value?\n+  if (obj != noreg) {\n+    __ load_heap_oop(pre_val, Address(obj, 0), noreg, noreg, AS_RAW);\n+  }\n+  \/\/ Is the previous value null?\n+  __ cmpptr(pre_val, NULL_WORD);\n+  __ jcc(Assembler::equal, done);\n+  generate_queue_insertion(masm,\n+                           G1ThreadLocalData::satb_mark_queue_index_offset(),\n+                           G1ThreadLocalData::satb_mark_queue_buffer_offset(),\n+                           runtime,\n+                           thread, pre_val, tmp);\n+  __ jmp(done);\n+}\n+\n@@ -188,20 +241,2 @@\n-  Address in_progress(thread, in_bytes(G1ThreadLocalData::satb_mark_queue_active_offset()));\n-  Address index(thread, in_bytes(G1ThreadLocalData::satb_mark_queue_index_offset()));\n-  Address buffer(thread, in_bytes(G1ThreadLocalData::satb_mark_queue_buffer_offset()));\n-\n-  \/\/ Is marking active?\n-  if (in_bytes(SATBMarkQueue::byte_width_of_active()) == 4) {\n-    __ cmpl(in_progress, 0);\n-  } else {\n-    assert(in_bytes(SATBMarkQueue::byte_width_of_active()) == 1, \"Assumption\");\n-    __ cmpb(in_progress, 0);\n-  }\n-  __ jcc(Assembler::equal, done);\n-\n-  \/\/ Do we need to load the previous value?\n-  if (obj != noreg) {\n-    __ load_heap_oop(pre_val, Address(obj, 0), noreg, noreg, AS_RAW);\n-  }\n-\n-  \/\/ Is the previous value null?\n-  __ cmpptr(pre_val, NULL_WORD);\n+  generate_pre_barrier_fast_path(masm, thread);\n+  \/\/ If marking is not active (*(mark queue active address) == 0), jump to done\n@@ -209,16 +244,1 @@\n-\n-  \/\/ Can we store original value in the thread's buffer?\n-  \/\/ Is index == 0?\n-  \/\/ (The index field is typed as size_t.)\n-\n-  __ movptr(tmp, index);                   \/\/ tmp := *index_adr\n-  __ cmpptr(tmp, 0);                       \/\/ tmp == 0?\n-  __ jcc(Assembler::equal, runtime);       \/\/ If yes, goto runtime\n-\n-  __ subptr(tmp, wordSize);                \/\/ tmp := tmp - wordSize\n-  __ movptr(index, tmp);                   \/\/ *index_adr := tmp\n-  __ addptr(tmp, buffer);                  \/\/ tmp := tmp + *buffer_adr\n-\n-  \/\/ Record the previous value\n-  __ movptr(Address(tmp, 0), pre_val);\n-  __ jmp(done);\n+  generate_pre_barrier_slow_path(masm, obj, pre_val, thread, tmp, done, runtime);\n@@ -266,0 +286,48 @@\n+static void generate_post_barrier_fast_path(MacroAssembler* masm,\n+                                            const Register store_addr,\n+                                            const Register new_val,\n+                                            const Register tmp,\n+                                            const Register tmp2,\n+                                            Label& done,\n+                                            bool new_val_may_be_null) {\n+  CardTableBarrierSet* ct = barrier_set_cast<CardTableBarrierSet>(BarrierSet::barrier_set());\n+    \/\/ Does store cross heap regions?\n+  __ movptr(tmp, store_addr);                                    \/\/ tmp := store address\n+  __ xorptr(tmp, new_val);                                       \/\/ tmp := store address ^ new value\n+  __ shrptr(tmp, G1HeapRegion::LogOfHRGrainBytes);               \/\/ ((store address ^ new value) >> LogOfHRGrainBytes) == 0?\n+  __ jcc(Assembler::equal, done);\n+  \/\/ Crosses regions, storing null?\n+  if (new_val_may_be_null) {\n+    __ cmpptr(new_val, NULL_WORD);                               \/\/ new value == null?\n+    __ jcc(Assembler::equal, done);\n+  }\n+  \/\/ Storing region crossing non-null, is card already dirty?\n+  __ movptr(tmp, store_addr);                                    \/\/ tmp := store address\n+  __ shrptr(tmp, CardTable::card_shift());                       \/\/ tmp := card address relative to card table base\n+  \/\/ Do not use ExternalAddress to load 'byte_map_base', since 'byte_map_base' is NOT\n+  \/\/ a valid address and therefore is not properly handled by the relocation code.\n+  __ movptr(tmp2, (intptr_t)ct->card_table()->byte_map_base());  \/\/ tmp2 := card table base address\n+  __ addptr(tmp, tmp2);                                          \/\/ tmp := card address\n+  __ cmpb(Address(tmp, 0), G1CardTable::g1_young_card_val());    \/\/ *(card address) == young_card_val?\n+}\n+\n+static void generate_post_barrier_slow_path(MacroAssembler* masm,\n+                                            const Register thread,\n+                                            const Register tmp,\n+                                            const Register tmp2,\n+                                            Label& done,\n+                                            Label& runtime) {\n+  __ membar(Assembler::Membar_mask_bits(Assembler::StoreLoad));  \/\/ StoreLoad membar\n+  __ cmpb(Address(tmp, 0), G1CardTable::dirty_card_val());       \/\/ *(card address) == dirty_card_val?\n+  __ jcc(Assembler::equal, done);\n+  \/\/ Storing a region crossing, non-null oop, card is clean.\n+  \/\/ Dirty card and log.\n+  __ movb(Address(tmp, 0), G1CardTable::dirty_card_val());       \/\/ *(card address) := dirty_card_val\n+  generate_queue_insertion(masm,\n+                           G1ThreadLocalData::dirty_card_queue_index_offset(),\n+                           G1ThreadLocalData::dirty_card_queue_buffer_offset(),\n+                           runtime,\n+                           thread, tmp, tmp2);\n+  __ jmp(done);\n+}\n+\n@@ -276,6 +344,0 @@\n-  Address queue_index(thread, in_bytes(G1ThreadLocalData::dirty_card_queue_index_offset()));\n-  Address buffer(thread, in_bytes(G1ThreadLocalData::dirty_card_queue_buffer_offset()));\n-\n-  CardTableBarrierSet* ct =\n-    barrier_set_cast<CardTableBarrierSet>(BarrierSet::barrier_set());\n-\n@@ -285,5 +347,2 @@\n-  \/\/ Does store cross heap regions?\n-\n-  __ movptr(tmp, store_addr);\n-  __ xorptr(tmp, new_val);\n-  __ shrptr(tmp, G1HeapRegion::LogOfHRGrainBytes);\n+  generate_post_barrier_fast_path(masm, store_addr, new_val, tmp, tmp2, done, true \/* new_val_may_be_null *\/);\n+  \/\/ If card is young, jump to done\n@@ -291,0 +350,1 @@\n+  generate_post_barrier_slow_path(masm, thread, tmp, tmp2, done, runtime);\n@@ -292,4 +352,6 @@\n-  \/\/ crosses regions, storing null?\n-\n-  __ cmpptr(new_val, NULL_WORD);\n-  __ jcc(Assembler::equal, done);\n+  __ bind(runtime);\n+  \/\/ save the live input values\n+  RegSet saved = RegSet::of(store_addr NOT_LP64(COMMA thread));\n+  __ push_set(saved);\n+  __ call_VM_leaf(CAST_FROM_FN_PTR(address, G1BarrierSetRuntime::write_ref_field_post_entry), tmp, thread);\n+  __ pop_set(saved);\n@@ -297,1 +359,2 @@\n-  \/\/ storing region crossing non-null, is card already dirty?\n+  __ bind(done);\n+}\n@@ -299,2 +362,1 @@\n-  const Register card_addr = tmp;\n-  const Register cardtable = tmp2;\n+#if defined(COMPILER2)\n@@ -302,6 +364,2 @@\n-  __ movptr(card_addr, store_addr);\n-  __ shrptr(card_addr, CardTable::card_shift());\n-  \/\/ Do not use ExternalAddress to load 'byte_map_base', since 'byte_map_base' is NOT\n-  \/\/ a valid address and therefore is not properly handled by the relocation code.\n-  __ movptr(cardtable, (intptr_t)ct->card_table()->byte_map_base());\n-  __ addptr(card_addr, cardtable);\n+#undef __\n+#define __ masm->\n@@ -309,2 +367,17 @@\n-  __ cmpb(Address(card_addr, 0), G1CardTable::g1_young_card_val());\n-  __ jcc(Assembler::equal, done);\n+static void generate_c2_barrier_runtime_call(MacroAssembler* masm, G1BarrierStubC2* stub, const Register arg, const address runtime_path) {\n+#ifdef _LP64\n+  SaveLiveRegisters save_registers(masm, stub);\n+  if (c_rarg0 != arg) {\n+    __ mov(c_rarg0, arg);\n+  }\n+  __ mov(c_rarg1, r15_thread);\n+  \/\/ rax is a caller-saved, non-argument-passing register, so it does not\n+  \/\/ interfere with c_rarg0 or c_rarg1. If it contained any live value before\n+  \/\/ entering this stub, it is saved at this point, and restored after the\n+  \/\/ call. If it did not contain any live value, it is free to be used. In\n+  \/\/ either case, it is safe to use it here as a call scratch register.\n+  __ call(RuntimeAddress(runtime_path), rax);\n+#else\n+  Unimplemented();\n+#endif \/\/ _LP64\n+}\n@@ -312,3 +385,13 @@\n-  __ membar(Assembler::Membar_mask_bits(Assembler::StoreLoad));\n-  __ cmpb(Address(card_addr, 0), G1CardTable::dirty_card_val());\n-  __ jcc(Assembler::equal, done);\n+void G1BarrierSetAssembler::g1_write_barrier_pre_c2(MacroAssembler* masm,\n+                                                    Register obj,\n+                                                    Register pre_val,\n+                                                    Register thread,\n+                                                    Register tmp,\n+                                                    G1PreBarrierStubC2* stub) {\n+#ifdef _LP64\n+  assert(thread == r15_thread, \"must be\");\n+#endif \/\/ _LP64\n+  assert(pre_val != noreg, \"check this code\");\n+  if (obj != noreg) {\n+    assert_different_registers(obj, pre_val, tmp);\n+  }\n@@ -316,0 +399,1 @@\n+  stub->initialize_registers(obj, pre_val, thread, tmp, noreg);\n@@ -317,2 +401,3 @@\n-  \/\/ storing a region crossing, non-null oop, card is clean.\n-  \/\/ dirty card and log.\n+  generate_pre_barrier_fast_path(masm, thread);\n+  \/\/ If marking is active (*(mark queue active address) != 0), jump to stub (slow path)\n+  __ jcc(Assembler::notEqual, *stub->entry());\n@@ -320,1 +405,2 @@\n-  __ movb(Address(card_addr, 0), G1CardTable::dirty_card_val());\n+  __ bind(*stub->continuation());\n+}\n@@ -322,2 +408,8 @@\n-  \/\/ The code below assumes that buffer index is pointer sized.\n-  STATIC_ASSERT(in_bytes(G1DirtyCardQueue::byte_width_of_index()) == sizeof(intptr_t));\n+void G1BarrierSetAssembler::generate_c2_pre_barrier_stub(MacroAssembler* masm,\n+                                                         G1PreBarrierStubC2* stub) const {\n+  Assembler::InlineSkippedInstructionsCounter skip_counter(masm);\n+  Label runtime;\n+  Register obj = stub->obj();\n+  Register pre_val = stub->pre_val();\n+  Register thread = stub->thread();\n+  Register tmp = stub->tmp1();\n@@ -325,8 +417,2 @@\n-  __ movptr(tmp2, queue_index);\n-  __ testptr(tmp2, tmp2);\n-  __ jcc(Assembler::zero, runtime);\n-  __ subptr(tmp2, wordSize);\n-  __ movptr(queue_index, tmp2);\n-  __ addptr(tmp2, buffer);\n-  __ movptr(Address(tmp2, 0), card_addr);\n-  __ jmp(done);\n+  __ bind(*stub->entry());\n+  generate_pre_barrier_slow_path(masm, obj, pre_val, thread, tmp, *stub->continuation(), runtime);\n@@ -335,5 +421,3 @@\n-  \/\/ save the live input values\n-  RegSet saved = RegSet::of(store_addr NOT_LP64(COMMA thread));\n-  __ push_set(saved);\n-  __ call_VM_leaf(CAST_FROM_FN_PTR(address, G1BarrierSetRuntime::write_ref_field_post_entry), card_addr, thread);\n-  __ pop_set(saved);\n+  generate_c2_barrier_runtime_call(masm, stub, pre_val, CAST_FROM_FN_PTR(address, G1BarrierSetRuntime::write_ref_field_pre_entry));\n+  __ jmp(*stub->continuation());\n+}\n@@ -341,1 +425,19 @@\n-  __ bind(done);\n+void G1BarrierSetAssembler::g1_write_barrier_post_c2(MacroAssembler* masm,\n+                                                     Register store_addr,\n+                                                     Register new_val,\n+                                                     Register thread,\n+                                                     Register tmp,\n+                                                     Register tmp2,\n+                                                     G1PostBarrierStubC2* stub) {\n+#ifdef _LP64\n+  assert(thread == r15_thread, \"must be\");\n+#endif \/\/ _LP64\n+\n+  stub->initialize_registers(thread, tmp, tmp2);\n+\n+  bool new_val_may_be_null = (stub->barrier_data() & G1C2BarrierPostNotNull) == 0;\n+  generate_post_barrier_fast_path(masm, store_addr, new_val, tmp, tmp2, *stub->continuation(), new_val_may_be_null);\n+  \/\/ If card is not young, jump to stub (slow path)\n+  __ jcc(Assembler::notEqual, *stub->entry());\n+\n+  __ bind(*stub->continuation());\n@@ -344,0 +446,18 @@\n+void G1BarrierSetAssembler::generate_c2_post_barrier_stub(MacroAssembler* masm,\n+                                                          G1PostBarrierStubC2* stub) const {\n+  Assembler::InlineSkippedInstructionsCounter skip_counter(masm);\n+  Label runtime;\n+  Register thread = stub->thread();\n+  Register tmp = stub->tmp1(); \/\/ tmp holds the card address.\n+  Register tmp2 = stub->tmp2();\n+\n+  __ bind(*stub->entry());\n+  generate_post_barrier_slow_path(masm, thread, tmp, tmp2, *stub->continuation(), runtime);\n+\n+  __ bind(runtime);\n+  generate_c2_barrier_runtime_call(masm, stub, tmp, CAST_FROM_FN_PTR(address, G1BarrierSetRuntime::write_ref_field_post_entry));\n+  __ jmp(*stub->continuation());\n+}\n+\n+#endif \/\/ COMPILER2\n+\n","filename":"src\/hotspot\/cpu\/x86\/gc\/g1\/g1BarrierSetAssembler_x86.cpp","additions":205,"deletions":85,"binary":false,"changes":290,"status":"modified"},{"patch":"@@ -35,0 +35,3 @@\n+class G1BarrierStubC2;\n+class G1PreBarrierStubC2;\n+class G1PostBarrierStubC2;\n@@ -68,0 +71,20 @@\n+\n+#ifdef COMPILER2\n+  void g1_write_barrier_pre_c2(MacroAssembler* masm,\n+                               Register obj,\n+                               Register pre_val,\n+                               Register thread,\n+                               Register tmp,\n+                               G1PreBarrierStubC2* c2_stub);\n+  void generate_c2_pre_barrier_stub(MacroAssembler* masm,\n+                                    G1PreBarrierStubC2* stub) const;\n+  void g1_write_barrier_post_c2(MacroAssembler* masm,\n+                                Register store_addr,\n+                                Register new_val,\n+                                Register thread,\n+                                Register tmp,\n+                                Register tmp2,\n+                                G1PostBarrierStubC2* c2_stub);\n+  void generate_c2_post_barrier_stub(MacroAssembler* masm,\n+                                     G1PostBarrierStubC2* stub) const;\n+#endif \/\/ COMPILER2\n","filename":"src\/hotspot\/cpu\/x86\/gc\/g1\/g1BarrierSetAssembler_x86.hpp","additions":23,"deletions":0,"binary":false,"changes":23,"status":"modified"},{"patch":"@@ -0,0 +1,370 @@\n+\/\/\n+\/\/ Copyright (c) 2024, Oracle and\/or its affiliates. All rights reserved.\n+\/\/ DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+\/\/\n+\/\/ This code is free software; you can redistribute it and\/or modify it\n+\/\/ under the terms of the GNU General Public License version 2 only, as\n+\/\/ published by the Free Software Foundation.\n+\/\/\n+\/\/ This code is distributed in the hope that it will be useful, but WITHOUT\n+\/\/ ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+\/\/ FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+\/\/ version 2 for more details (a copy is included in the LICENSE file that\n+\/\/ accompanied this code).\n+\/\/\n+\/\/ You should have received a copy of the GNU General Public License version\n+\/\/ 2 along with this work; if not, write to the Free Software Foundation,\n+\/\/ Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+\/\/\n+\/\/ Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+\/\/ or visit www.oracle.com if you need additional information or have any\n+\/\/ questions.\n+\/\/\n+\n+source_hpp %{\n+\n+#include \"gc\/g1\/c2\/g1BarrierSetC2.hpp\"\n+#include \"gc\/shared\/gc_globals.hpp\"\n+\n+%}\n+\n+source %{\n+\n+#include \"gc\/g1\/g1BarrierSetAssembler_x86.hpp\"\n+#include \"gc\/g1\/g1BarrierSetRuntime.hpp\"\n+\n+static void g1_pre_write_barrier(MacroAssembler* masm,\n+                                 const MachNode* node,\n+                                 Register obj,\n+                                 Register pre_val,\n+                                 Register tmp,\n+                                 RegSet preserve = RegSet(),\n+                                 RegSet no_preserve = RegSet()) {\n+  if (!G1PreBarrierStubC2::needs_barrier(node)) {\n+    return;\n+  }\n+  Assembler::InlineSkippedInstructionsCounter skip_counter(masm);\n+  G1BarrierSetAssembler* g1_asm = static_cast<G1BarrierSetAssembler*>(BarrierSet::barrier_set()->barrier_set_assembler());\n+  G1PreBarrierStubC2* const stub = G1PreBarrierStubC2::create(node);\n+  for (RegSetIterator<Register> reg = preserve.begin(); *reg != noreg; ++reg) {\n+    stub->preserve(*reg);\n+  }\n+  for (RegSetIterator<Register> reg = no_preserve.begin(); *reg != noreg; ++reg) {\n+    stub->dont_preserve(*reg);\n+  }\n+  g1_asm->g1_write_barrier_pre_c2(masm, obj, pre_val, r15_thread, tmp, stub);\n+}\n+\n+static void g1_post_write_barrier(MacroAssembler* masm,\n+                                  const MachNode* node,\n+                                  Register store_addr,\n+                                  Register new_val,\n+                                  Register tmp1,\n+                                  Register tmp2) {\n+  if (!G1PostBarrierStubC2::needs_barrier(node)) {\n+    return;\n+  }\n+  Assembler::InlineSkippedInstructionsCounter skip_counter(masm);\n+  G1BarrierSetAssembler* g1_asm = static_cast<G1BarrierSetAssembler*>(BarrierSet::barrier_set()->barrier_set_assembler());\n+  G1PostBarrierStubC2* const stub = G1PostBarrierStubC2::create(node);\n+  g1_asm->g1_write_barrier_post_c2(masm, store_addr, new_val, r15_thread, tmp1, tmp2, stub);\n+}\n+\n+%}\n+\n+instruct g1StoreP(memory mem, any_RegP src, rRegP tmp1, rRegP tmp2, rRegP tmp3, rFlagsReg cr)\n+%{\n+  predicate(UseG1GC && n->as_Store()->barrier_data() != 0);\n+  match(Set mem (StoreP mem src));\n+  effect(TEMP tmp1, TEMP tmp2, TEMP tmp3, KILL cr);\n+  ins_cost(125); \/\/ XXX\n+  format %{ \"movq    $mem, $src\\t# ptr\" %}\n+  ins_encode %{\n+    \/\/ Materialize the store address internally (as opposed to defining 'mem' as\n+    \/\/ an indirect memory operand) to reduce C2's scheduling and register\n+    \/\/ allocation pressure (fewer Mach nodes). The same holds for g1StoreN and\n+    \/\/ g1EncodePAndStoreN.\n+    __ lea($tmp1$$Register, $mem$$Address);\n+    g1_pre_write_barrier(masm, this,\n+                         $tmp1$$Register \/* obj *\/,\n+                         $tmp2$$Register \/* pre_val *\/,\n+                         $tmp3$$Register \/* tmp *\/,\n+                         RegSet::of($tmp1$$Register, $src$$Register) \/* preserve *\/);\n+    __ movq(Address($tmp1$$Register, 0), $src$$Register);\n+    g1_post_write_barrier(masm, this,\n+                          $tmp1$$Register \/* store_addr *\/,\n+                          $src$$Register \/* new_val *\/,\n+                          $tmp3$$Register \/* tmp1 *\/,\n+                          $tmp2$$Register \/* tmp2 *\/);\n+  %}\n+  ins_pipe(ialu_mem_reg);\n+%}\n+\n+instruct g1StoreN(memory mem, rRegN src, rRegP tmp1, rRegP tmp2, rRegP tmp3, rFlagsReg cr)\n+%{\n+  predicate(UseG1GC && n->as_Store()->barrier_data() != 0);\n+  match(Set mem (StoreN mem src));\n+  effect(TEMP tmp1, TEMP tmp2, TEMP tmp3, KILL cr);\n+  ins_cost(125); \/\/ XXX\n+  format %{ \"movl    $mem, $src\\t# ptr\" %}\n+  ins_encode %{\n+    assert(!in(operand_index(2))->is_Mach() ||\n+           (in(operand_index(2))->as_Mach()->ideal_Opcode() != Op_EncodeP),\n+           \"EncodeP src nodes should be matched with their corresponding StoreN nodes\");\n+    __ lea($tmp1$$Register, $mem$$Address);\n+    g1_pre_write_barrier(masm, this,\n+                         $tmp1$$Register \/* obj *\/,\n+                         $tmp2$$Register \/* pre_val *\/,\n+                         $tmp3$$Register \/* tmp *\/,\n+                         RegSet::of($tmp1$$Register, $src$$Register) \/* preserve *\/);\n+    __ movl(Address($tmp1$$Register, 0), $src$$Register);\n+    if ((barrier_data() & G1C2BarrierPost) != 0) {\n+      __ movl($tmp2$$Register, $src$$Register);\n+      if ((barrier_data() & G1C2BarrierPostNotNull) == 0) {\n+        __ decode_heap_oop($tmp2$$Register);\n+      } else {\n+        __ decode_heap_oop_not_null($tmp2$$Register);\n+      }\n+    }\n+    g1_post_write_barrier(masm, this,\n+                          $tmp1$$Register \/* store_addr *\/,\n+                          $tmp2$$Register \/* new_val *\/,\n+                          $tmp3$$Register \/* tmp1 *\/,\n+                          $tmp2$$Register \/* tmp2 *\/);\n+  %}\n+  ins_pipe(ialu_mem_reg);\n+%}\n+\n+instruct g1EncodePAndStoreN(memory mem, any_RegP src, rRegP tmp1, rRegP tmp2, rRegP tmp3, rFlagsReg cr)\n+%{\n+  predicate(UseG1GC && n->as_Store()->barrier_data() != 0);\n+  match(Set mem (StoreN mem (EncodeP src)));\n+  effect(TEMP tmp1, TEMP tmp2, TEMP tmp3, KILL cr);\n+  ins_cost(125); \/\/ XXX\n+  format %{ \"encode_heap_oop $src\\n\\t\"\n+            \"movl   $mem, $src\\t# ptr\" %}\n+  ins_encode %{\n+    __ lea($tmp1$$Register, $mem$$Address);\n+    g1_pre_write_barrier(masm, this,\n+                         $tmp1$$Register \/* obj *\/,\n+                         $tmp2$$Register \/* pre_val *\/,\n+                         $tmp3$$Register \/* tmp *\/,\n+                         RegSet::of($tmp1$$Register, $src$$Register) \/* preserve *\/);\n+    __ movq($tmp2$$Register, $src$$Register);\n+    if ((barrier_data() & G1C2BarrierPostNotNull) == 0) {\n+      __ encode_heap_oop($tmp2$$Register);\n+    } else {\n+      __ encode_heap_oop_not_null($tmp2$$Register);\n+    }\n+    __ movl(Address($tmp1$$Register, 0), $tmp2$$Register);\n+    g1_post_write_barrier(masm, this,\n+                          $tmp1$$Register \/* store_addr *\/,\n+                          $src$$Register \/* new_val *\/,\n+                          $tmp3$$Register \/* tmp1 *\/,\n+                          $tmp2$$Register \/* tmp2 *\/);\n+  %}\n+  ins_pipe(ialu_mem_reg);\n+%}\n+\n+instruct g1CompareAndExchangeP(indirect mem, rRegP newval, rRegP tmp1, rRegP tmp2, rRegP tmp3, rax_RegP oldval, rFlagsReg cr)\n+%{\n+  predicate(UseG1GC && n->as_LoadStore()->barrier_data() != 0);\n+  match(Set oldval (CompareAndExchangeP mem (Binary oldval newval)));\n+  effect(TEMP tmp1, TEMP tmp2, TEMP tmp3, KILL cr);\n+  format %{ \"lock\\n\\t\"\n+            \"cmpxchgq $newval, $mem\" %}\n+  ins_encode %{\n+    assert_different_registers($oldval$$Register, $mem$$Register);\n+    g1_pre_write_barrier(masm, this,\n+                         $mem$$Register \/* obj *\/,\n+                         $tmp2$$Register \/* pre_val *\/,\n+                         $tmp3$$Register \/* tmp *\/,\n+                         RegSet::of($mem$$Register, $newval$$Register, $oldval$$Register) \/* preserve *\/);\n+    __ movq($tmp1$$Register, $newval$$Register);\n+    __ lock();\n+    __ cmpxchgq($tmp1$$Register, Address($mem$$Register, 0));\n+    g1_post_write_barrier(masm, this,\n+                          $mem$$Register \/* store_addr *\/,\n+                          $tmp1$$Register \/* new_val *\/,\n+                          $tmp2$$Register \/* tmp1 *\/,\n+                          $tmp3$$Register \/* tmp2 *\/);\n+  %}\n+  ins_pipe(pipe_cmpxchg);\n+%}\n+\n+instruct g1CompareAndExchangeN(indirect mem, rRegN newval, rRegP tmp1, rRegP tmp2, rRegP tmp3, rax_RegN oldval, rFlagsReg cr)\n+%{\n+  predicate(UseG1GC && n->as_LoadStore()->barrier_data() != 0);\n+  match(Set oldval (CompareAndExchangeN mem (Binary oldval newval)));\n+  effect(TEMP tmp1, TEMP tmp2, TEMP tmp3, KILL cr);\n+  format %{ \"lock\\n\\t\"\n+            \"cmpxchgq $newval, $mem\" %}\n+  ins_encode %{\n+    assert_different_registers($oldval$$Register, $mem$$Register);\n+    g1_pre_write_barrier(masm, this,\n+                         $mem$$Register \/* obj *\/,\n+                         $tmp2$$Register \/* pre_val *\/,\n+                         $tmp3$$Register \/* tmp *\/,\n+                         RegSet::of($mem$$Register, $newval$$Register, $oldval$$Register) \/* preserve *\/);\n+    __ movl($tmp1$$Register, $newval$$Register);\n+    __ lock();\n+    __ cmpxchgl($tmp1$$Register, Address($mem$$Register, 0));\n+    __ decode_heap_oop($tmp1$$Register);\n+    g1_post_write_barrier(masm, this,\n+                          $mem$$Register \/* store_addr *\/,\n+                          $tmp1$$Register \/* new_val *\/,\n+                          $tmp2$$Register \/* tmp1 *\/,\n+                          $tmp3$$Register \/* tmp2 *\/);\n+  %}\n+  ins_pipe(pipe_cmpxchg);\n+%}\n+\n+instruct g1CompareAndSwapP(rRegI res, indirect mem, rRegP newval, rRegP tmp1, rRegP tmp2, rRegP tmp3, rax_RegP oldval, rFlagsReg cr)\n+%{\n+  predicate(UseG1GC && n->as_LoadStore()->barrier_data() != 0);\n+  match(Set res (CompareAndSwapP mem (Binary oldval newval)));\n+  match(Set res (WeakCompareAndSwapP mem (Binary oldval newval)));\n+  effect(TEMP res, TEMP tmp1, TEMP tmp2, TEMP tmp3, KILL oldval, KILL cr);\n+  format %{ \"lock\\n\\t\"\n+            \"cmpxchgq $newval, $mem\\n\\t\"\n+            \"sete     $res\\n\\t\"\n+            \"movzbl   $res, $res\" %}\n+  ins_encode %{\n+    assert_different_registers($oldval$$Register, $mem$$Register);\n+    g1_pre_write_barrier(masm, this,\n+                         $mem$$Register \/* obj *\/,\n+                         $tmp2$$Register \/* pre_val *\/,\n+                         $tmp3$$Register \/* tmp *\/,\n+                         RegSet::of($mem$$Register, $newval$$Register, $oldval$$Register) \/* preserve *\/,\n+                         RegSet::of($res$$Register) \/* no_preserve *\/);\n+    __ movq($tmp1$$Register, $newval$$Register);\n+    __ lock();\n+    __ cmpxchgq($tmp1$$Register, Address($mem$$Register, 0));\n+    __ setb(Assembler::equal, $res$$Register);\n+    __ movzbl($res$$Register, $res$$Register);\n+    g1_post_write_barrier(masm, this,\n+                          $mem$$Register \/* store_addr *\/,\n+                          $tmp1$$Register \/* new_val *\/,\n+                          $tmp2$$Register \/* tmp1 *\/,\n+                          $tmp3$$Register \/* tmp2 *\/);\n+  %}\n+  ins_pipe(pipe_cmpxchg);\n+%}\n+\n+instruct g1CompareAndSwapN(rRegI res, indirect mem, rRegN newval, rRegP tmp1, rRegP tmp2, rRegP tmp3, rax_RegN oldval, rFlagsReg cr)\n+%{\n+  predicate(UseG1GC && n->as_LoadStore()->barrier_data() != 0);\n+  match(Set res (CompareAndSwapN mem (Binary oldval newval)));\n+  match(Set res (WeakCompareAndSwapN mem (Binary oldval newval)));\n+  effect(TEMP res, TEMP tmp1, TEMP tmp2, TEMP tmp3, KILL oldval, KILL cr);\n+  format %{ \"lock\\n\\t\"\n+            \"cmpxchgq $newval, $mem\\n\\t\"\n+            \"sete     $res\\n\\t\"\n+            \"movzbl   $res, $res\" %}\n+  ins_encode %{\n+    assert_different_registers($oldval$$Register, $mem$$Register);\n+    g1_pre_write_barrier(masm, this,\n+                         $mem$$Register \/* obj *\/,\n+                         $tmp2$$Register \/* pre_val *\/,\n+                         $tmp3$$Register \/* tmp *\/,\n+                         RegSet::of($mem$$Register, $newval$$Register, $oldval$$Register) \/* preserve *\/,\n+                         RegSet::of($res$$Register) \/* no_preserve *\/);\n+    __ movl($tmp1$$Register, $newval$$Register);\n+    __ lock();\n+    __ cmpxchgl($tmp1$$Register, Address($mem$$Register, 0));\n+    __ setb(Assembler::equal, $res$$Register);\n+    __ movzbl($res$$Register, $res$$Register);\n+    __ decode_heap_oop($tmp1$$Register);\n+    g1_post_write_barrier(masm, this,\n+                          $mem$$Register \/* store_addr *\/,\n+                          $tmp1$$Register \/* new_val *\/,\n+                          $tmp2$$Register \/* tmp1 *\/,\n+                          $tmp3$$Register \/* tmp2 *\/);\n+  %}\n+  ins_pipe(pipe_cmpxchg);\n+%}\n+\n+instruct g1XChgP(indirect mem, rRegP newval, rRegP tmp1, rRegP tmp2, rRegP tmp3, rFlagsReg cr)\n+%{\n+  predicate(UseG1GC && n->as_LoadStore()->barrier_data() != 0);\n+  match(Set newval (GetAndSetP mem newval));\n+  effect(TEMP tmp1, TEMP tmp2, TEMP tmp3, KILL cr);\n+  format %{ \"xchgq    $newval, $mem\" %}\n+  ins_encode %{\n+    assert_different_registers($mem$$Register, $newval$$Register);\n+    g1_pre_write_barrier(masm, this,\n+                         $mem$$Register \/* obj *\/,\n+                         $tmp2$$Register \/* pre_val *\/,\n+                         $tmp3$$Register \/* tmp *\/,\n+                         RegSet::of($mem$$Register, $newval$$Register) \/* preserve *\/);\n+    __ movq($tmp1$$Register, $newval$$Register);\n+    __ xchgq($newval$$Register, Address($mem$$Register, 0));\n+    g1_post_write_barrier(masm, this,\n+                          $mem$$Register \/* store_addr *\/,\n+                          $tmp1$$Register \/* new_val *\/,\n+                          $tmp2$$Register \/* tmp1 *\/,\n+                          $tmp3$$Register \/* tmp2 *\/);\n+  %}\n+  ins_pipe(pipe_cmpxchg);\n+%}\n+\n+instruct g1XChgN(indirect mem, rRegN newval, rRegP tmp1, rRegP tmp2, rRegP tmp3, rFlagsReg cr)\n+%{\n+  predicate(UseG1GC && n->as_LoadStore()->barrier_data() != 0);\n+  match(Set newval (GetAndSetN mem newval));\n+  effect(TEMP tmp1, TEMP tmp2, TEMP tmp3, KILL cr);\n+  format %{ \"xchgq    $newval, $mem\" %}\n+  ins_encode %{\n+    assert_different_registers($mem$$Register, $newval$$Register);\n+    g1_pre_write_barrier(masm, this,\n+                         $mem$$Register \/* obj *\/,\n+                         $tmp2$$Register \/* pre_val *\/,\n+                         $tmp3$$Register \/* tmp *\/,\n+                         RegSet::of($mem$$Register, $newval$$Register) \/* preserve *\/);\n+    __ movl($tmp1$$Register, $newval$$Register);\n+    __ decode_heap_oop($tmp1$$Register);\n+    __ xchgl($newval$$Register, Address($mem$$Register, 0));\n+    g1_post_write_barrier(masm, this,\n+                          $mem$$Register \/* store_addr *\/,\n+                          $tmp1$$Register \/* new_val *\/,\n+                          $tmp2$$Register \/* tmp1 *\/,\n+                          $tmp3$$Register \/* tmp2 *\/);\n+  %}\n+  ins_pipe(pipe_cmpxchg);\n+%}\n+\n+instruct g1LoadP(rRegP dst, memory mem, rRegP tmp, rFlagsReg cr)\n+%{\n+  predicate(UseG1GC && n->as_Load()->barrier_data() != 0);\n+  match(Set dst (LoadP mem));\n+  effect(TEMP dst, TEMP tmp, KILL cr);\n+  ins_cost(125); \/\/ XXX\n+  format %{ \"movq    $dst, $mem\\t# ptr\" %}\n+  ins_encode %{\n+    __ movq($dst$$Register, $mem$$Address);\n+    g1_pre_write_barrier(masm, this,\n+                         noreg \/* obj *\/,\n+                         $dst$$Register \/* pre_val *\/,\n+                         $tmp$$Register \/* tmp *\/);\n+  %}\n+  ins_pipe(ialu_reg_mem); \/\/ XXX\n+%}\n+\n+instruct g1LoadN(rRegN dst, memory mem, rRegP tmp1, rRegP tmp2, rFlagsReg cr)\n+%{\n+  predicate(UseG1GC && n->as_Load()->barrier_data() != 0);\n+  match(Set dst (LoadN mem));\n+  effect(TEMP dst, TEMP tmp1, TEMP tmp2, KILL cr);\n+  ins_cost(125); \/\/ XXX\n+  format %{ \"movl    $dst, $mem\\t# compressed ptr\" %}\n+  ins_encode %{\n+    __ movl($dst$$Register, $mem$$Address);\n+    __ movl($tmp1$$Register, $dst$$Register);\n+    __ decode_heap_oop($tmp1$$Register);\n+    g1_pre_write_barrier(masm, this,\n+                         noreg \/* obj *\/,\n+                         $tmp1$$Register \/* pre_val *\/,\n+                         $tmp2$$Register \/* tmp *\/);\n+  %}\n+  ins_pipe(ialu_reg_mem); \/\/ XXX\n+%}\n","filename":"src\/hotspot\/cpu\/x86\/gc\/g1\/g1_x86_64.ad","additions":370,"deletions":0,"binary":false,"changes":370,"status":"added"},{"patch":"@@ -2460,0 +2460,4 @@\n+  if (is_encode_and_store_pattern(n, m)) {\n+    mstack.push(m, Visit);\n+    return true;\n+  }\n","filename":"src\/hotspot\/cpu\/x86\/x86.ad","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -4235,0 +4235,1 @@\n+   predicate(n->as_Load()->barrier_data() == 0);\n@@ -5020,0 +5021,1 @@\n+  predicate(n->as_Store()->barrier_data() == 0);\n@@ -5044,1 +5046,1 @@\n-  predicate(CompressedOops::base() == nullptr);\n+  predicate(CompressedOops::base() == nullptr && n->as_Store()->barrier_data() == 0);\n@@ -5057,0 +5059,1 @@\n+  predicate(n->as_Store()->barrier_data() == 0);\n@@ -7066,0 +7069,1 @@\n+  predicate(n->as_LoadStore()->barrier_data() == 0);\n@@ -7155,0 +7159,1 @@\n+  predicate(n->as_LoadStore()->barrier_data() == 0);\n@@ -7376,0 +7381,1 @@\n+  predicate(n->as_LoadStore()->barrier_data() == 0);\n@@ -11567,0 +11573,1 @@\n+  predicate(n->in(2)->as_Load()->barrier_data() == 0);\n@@ -11588,0 +11595,1 @@\n+  predicate(n->in(2)->as_Load()->barrier_data() == 0);\n@@ -11628,1 +11636,2 @@\n-  predicate(CompressedOops::base() != nullptr);\n+  predicate(CompressedOops::base() != nullptr &&\n+            n->in(1)->as_Load()->barrier_data() == 0);\n@@ -11641,1 +11650,2 @@\n-  predicate(CompressedOops::base() == nullptr);\n+  predicate(CompressedOops::base() == nullptr &&\n+            n->in(1)->as_Load()->barrier_data() == 0);\n","filename":"src\/hotspot\/cpu\/x86\/x86_64.ad","additions":13,"deletions":3,"binary":false,"changes":16,"status":"modified"},{"patch":"@@ -27,0 +27,1 @@\n+#include \"code\/vmreg.inline.hpp\"\n@@ -29,0 +30,1 @@\n+#include \"gc\/g1\/g1BarrierSetAssembler.hpp\"\n@@ -34,0 +36,1 @@\n+#include \"opto\/block.hpp\"\n@@ -38,0 +41,1 @@\n+#include \"opto\/machnode.hpp\"\n@@ -39,0 +43,4 @@\n+#include \"opto\/memnode.hpp\"\n+#include \"opto\/node.hpp\"\n+#include \"opto\/output.hpp\"\n+#include \"opto\/regalloc.hpp\"\n@@ -40,0 +48,1 @@\n+#include \"opto\/runtime.hpp\"\n@@ -41,0 +50,1 @@\n+#include \"utilities\/growableArray.hpp\"\n@@ -43,1 +53,490 @@\n-const TypeFunc *G1BarrierSetC2::write_ref_field_pre_entry_Type() {\n+\/*\n+ * Determine if the G1 pre-barrier can be removed. The pre-barrier is\n+ * required by SATB to make sure all objects live at the start of the\n+ * marking are kept alive, all reference updates need to any previous\n+ * reference stored before writing.\n+ *\n+ * If the previous value is null there is no need to save the old value.\n+ * References that are null are filtered during runtime by the barrier\n+ * code to avoid unnecessary queuing.\n+ *\n+ * However in the case of newly allocated objects it might be possible to\n+ * prove that the reference about to be overwritten is null during compile\n+ * time and avoid adding the barrier code completely.\n+ *\n+ * The compiler needs to determine that the object in which a field is about\n+ * to be written is newly allocated, and that no prior store to the same field\n+ * has happened since the allocation.\n+ *\/\n+bool G1BarrierSetC2::g1_can_remove_pre_barrier(GraphKit* kit,\n+                                               PhaseValues* phase,\n+                                               Node* adr,\n+                                               BasicType bt,\n+                                               uint adr_idx) const {\n+  intptr_t offset = 0;\n+  Node* base = AddPNode::Ideal_base_and_offset(adr, phase, offset);\n+  AllocateNode* alloc = AllocateNode::Ideal_allocation(base);\n+\n+  if (offset == Type::OffsetBot) {\n+    return false; \/\/ Cannot unalias unless there are precise offsets.\n+  }\n+  if (alloc == nullptr) {\n+    return false; \/\/ No allocation found.\n+  }\n+\n+  intptr_t size_in_bytes = type2aelembytes(bt);\n+  Node* mem = kit->memory(adr_idx); \/\/ Start searching here.\n+\n+  for (int cnt = 0; cnt < 50; cnt++) {\n+    if (mem->is_Store()) {\n+      Node* st_adr = mem->in(MemNode::Address);\n+      intptr_t st_offset = 0;\n+      Node* st_base = AddPNode::Ideal_base_and_offset(st_adr, phase, st_offset);\n+\n+      if (st_base == nullptr) {\n+        break; \/\/ Inscrutable pointer.\n+      }\n+      if (st_base == base && st_offset == offset) {\n+        \/\/ We have found a store with same base and offset as ours.\n+        break;\n+      }\n+      if (st_offset != offset && st_offset != Type::OffsetBot) {\n+        const int MAX_STORE = BytesPerLong;\n+        if (st_offset >= offset + size_in_bytes ||\n+            st_offset <= offset - MAX_STORE ||\n+            st_offset <= offset - mem->as_Store()->memory_size()) {\n+          \/\/ Success:  The offsets are provably independent.\n+          \/\/ (You may ask, why not just test st_offset != offset and be done?\n+          \/\/ The answer is that stores of different sizes can co-exist\n+          \/\/ in the same sequence of RawMem effects.  We sometimes initialize\n+          \/\/ a whole 'tile' of array elements with a single jint or jlong.)\n+          mem = mem->in(MemNode::Memory);\n+          continue; \/\/ Advance through independent store memory.\n+        }\n+      }\n+      if (st_base != base\n+          && MemNode::detect_ptr_independence(base, alloc, st_base,\n+                                              AllocateNode::Ideal_allocation(st_base),\n+                                              phase)) {\n+        \/\/ Success: the bases are provably independent.\n+        mem = mem->in(MemNode::Memory);\n+        continue; \/\/ Advance through independent store memory.\n+      }\n+    } else if (mem->is_Proj() && mem->in(0)->is_Initialize()) {\n+      InitializeNode* st_init = mem->in(0)->as_Initialize();\n+      AllocateNode* st_alloc = st_init->allocation();\n+\n+      \/\/ Make sure that we are looking at the same allocation site.\n+      \/\/ The alloc variable is guaranteed to not be null here from earlier check.\n+      if (alloc == st_alloc) {\n+        \/\/ Check that the initialization is storing null so that no previous store\n+        \/\/ has been moved up and directly write a reference.\n+        Node* captured_store = st_init->find_captured_store(offset,\n+                                                            type2aelembytes(T_OBJECT),\n+                                                            phase);\n+        if (captured_store == nullptr || captured_store == st_init->zero_memory()) {\n+          return true;\n+        }\n+      }\n+    }\n+    \/\/ Unless there is an explicit 'continue', we must bail out here,\n+    \/\/ because 'mem' is an inscrutable memory state (e.g., a call).\n+    break;\n+  }\n+  return false;\n+}\n+\n+\/*\n+ * G1, similar to any GC with a Young Generation, requires a way to keep track\n+ * of references from Old Generation to Young Generation to make sure all live\n+ * objects are found. G1 also requires to keep track of object references\n+ * between different regions to enable evacuation of old regions, which is done\n+ * as part of mixed collections. References are tracked in remembered sets,\n+ * which are continuously updated as references are written to with the help of\n+ * the post-barrier.\n+ *\n+ * To reduce the number of updates to the remembered set, the post-barrier\n+ * filters out updates to fields in objects located in the Young Generation, the\n+ * same region as the reference, when the null is being written, or if the card\n+ * is already marked as dirty by an earlier write.\n+ *\n+ * Under certain circumstances it is possible to avoid generating the\n+ * post-barrier completely, if it is possible during compile time to prove the\n+ * object is newly allocated and that no safepoint exists between the allocation\n+ * and the store.\n+ *\n+ * In the case of a slow allocation, the allocation code must handle the barrier\n+ * as part of the allocation if the allocated object is not located in the\n+ * nursery; this would happen for humongous objects.\n+ *\/\n+bool G1BarrierSetC2::g1_can_remove_post_barrier(GraphKit* kit,\n+                                                PhaseValues* phase, Node* store_ctrl,\n+                                                Node* adr) const {\n+  intptr_t      offset = 0;\n+  Node*         base   = AddPNode::Ideal_base_and_offset(adr, phase, offset);\n+  AllocateNode* alloc  = AllocateNode::Ideal_allocation(base);\n+\n+  if (offset == Type::OffsetBot) {\n+    return false; \/\/ Cannot unalias unless there are precise offsets.\n+  }\n+  if (alloc == nullptr) {\n+     return false; \/\/ No allocation found.\n+  }\n+\n+  Node* mem = store_ctrl;   \/\/ Start search from Store node.\n+  if (mem->is_Proj() && mem->in(0)->is_Initialize()) {\n+    InitializeNode* st_init = mem->in(0)->as_Initialize();\n+    AllocateNode*  st_alloc = st_init->allocation();\n+    \/\/ Make sure we are looking at the same allocation\n+    if (alloc == st_alloc) {\n+      return true;\n+    }\n+  }\n+\n+  return false;\n+}\n+\n+Node* G1BarrierSetC2::load_at_resolved(C2Access& access, const Type* val_type) const {\n+  DecoratorSet decorators = access.decorators();\n+  bool on_weak = (decorators & ON_WEAK_OOP_REF) != 0;\n+  bool on_phantom = (decorators & ON_PHANTOM_OOP_REF) != 0;\n+  bool no_keepalive = (decorators & AS_NO_KEEPALIVE) != 0;\n+  \/\/ If we are reading the value of the referent field of a Reference object, we\n+  \/\/ need to record the referent in an SATB log buffer using the pre-barrier\n+  \/\/ mechanism. Also we need to add a memory barrier to prevent commoning reads\n+  \/\/ from this field across safepoints, since GC can change its value.\n+  bool need_read_barrier = ((on_weak || on_phantom) && !no_keepalive);\n+  if (access.is_oop() && need_read_barrier) {\n+    access.set_barrier_data(G1C2BarrierPre);\n+  }\n+  return CardTableBarrierSetC2::load_at_resolved(access, val_type);\n+}\n+\n+void G1BarrierSetC2::eliminate_gc_barrier(PhaseMacroExpand* macro, Node* node) const {\n+  eliminate_gc_barrier_data(node);\n+}\n+\n+void G1BarrierSetC2::eliminate_gc_barrier_data(Node* node) const {\n+  if (node->is_LoadStore()) {\n+    LoadStoreNode* loadstore = node->as_LoadStore();\n+    loadstore->set_barrier_data(0);\n+  } else if (node->is_Mem()) {\n+    MemNode* mem = node->as_Mem();\n+    mem->set_barrier_data(0);\n+  }\n+}\n+\n+void refine_barrier_by_new_val_type(Node* n) {\n+  if (n->Opcode() != Op_StoreP &&\n+      n->Opcode() != Op_StoreN) {\n+    return;\n+  }\n+  MemNode* store = n->as_Mem();\n+  const Node* newval = n->in(MemNode::ValueIn);\n+  assert(newval != nullptr, \"\");\n+  const Type* newval_bottom = newval->bottom_type();\n+  assert(newval_bottom->isa_ptr() || newval_bottom->isa_narrowoop(), \"newval should be an OOP\");\n+  TypePtr::PTR newval_type = newval_bottom->make_ptr()->ptr();\n+  uint8_t barrier_data = store->barrier_data();\n+  if (newval_type == TypePtr::Null) {\n+    \/\/ Simply elide post-barrier if writing null.\n+    barrier_data &= ~G1C2BarrierPost;\n+    barrier_data &= ~G1C2BarrierPostNotNull;\n+  } else if (((barrier_data & G1C2BarrierPost) != 0) &&\n+             newval_type == TypePtr::NotNull) {\n+    \/\/ If the post-barrier has not been elided yet (e.g. due to newval being\n+    \/\/ freshly allocated), mark it as not-null (simplifies barrier tests and\n+    \/\/ compressed OOPs logic).\n+    barrier_data |= G1C2BarrierPostNotNull;\n+  }\n+  store->set_barrier_data(barrier_data);\n+  return;\n+}\n+\n+\/\/ Refine (not really expand) G1 barriers by looking at the new value type\n+\/\/ (whether it is necessarily null or necessarily non-null).\n+bool G1BarrierSetC2::expand_barriers(Compile* C, PhaseIterGVN& igvn) const {\n+  ResourceMark rm;\n+  VectorSet visited;\n+  Node_List worklist;\n+  worklist.push(C->root());\n+  while (worklist.size() > 0) {\n+    Node* n = worklist.pop();\n+    if (visited.test_set(n->_idx)) {\n+      continue;\n+    }\n+    refine_barrier_by_new_val_type(n);\n+    for (uint j = 0; j < n->req(); j++) {\n+      Node* in = n->in(j);\n+      if (in != nullptr) {\n+        worklist.push(in);\n+      }\n+    }\n+  }\n+  return false;\n+}\n+\n+uint G1BarrierSetC2::estimated_barrier_size(const Node* node) const {\n+  \/\/ These Ideal node counts are extracted from the pre-matching Ideal graph\n+  \/\/ generated when compiling the following method with early barrier expansion:\n+  \/\/   static void write(MyObject obj1, Object o) {\n+  \/\/     obj1.o1 = o;\n+  \/\/   }\n+  uint8_t barrier_data = MemNode::barrier_data(node);\n+  uint nodes = 0;\n+  if ((barrier_data & G1C2BarrierPre) != 0) {\n+    nodes += 50;\n+  }\n+  if ((barrier_data & G1C2BarrierPost) != 0) {\n+    nodes += 60;\n+  }\n+  return nodes;\n+}\n+\n+void G1BarrierSetC2::clone_at_expansion(PhaseMacroExpand* phase, ArrayCopyNode* ac) const {\n+  if (ac->is_clone_inst() && !use_ReduceInitialCardMarks()) {\n+    clone_in_runtime(phase, ac, G1BarrierSetRuntime::clone_addr(), \"G1BarrierSetRuntime::clone\");\n+    return;\n+  }\n+  BarrierSetC2::clone_at_expansion(phase, ac);\n+}\n+\n+Node* G1BarrierSetC2::store_at_resolved(C2Access& access, C2AccessValue& val) const {\n+  DecoratorSet decorators = access.decorators();\n+  bool anonymous = (decorators & ON_UNKNOWN_OOP_REF) != 0;\n+  bool in_heap = (decorators & IN_HEAP) != 0;\n+  bool tightly_coupled_alloc = (decorators & C2_TIGHTLY_COUPLED_ALLOC) != 0;\n+  bool need_store_barrier = !(tightly_coupled_alloc && use_ReduceInitialCardMarks()) && (in_heap || anonymous);\n+  if (access.is_oop() && need_store_barrier) {\n+    access.set_barrier_data(get_store_barrier(access, val));\n+    if (tightly_coupled_alloc) {\n+      assert(!use_ReduceInitialCardMarks(),\n+             \"post-barriers are only needed for tightly-coupled initialization stores when ReduceInitialCardMarks is disabled\");\n+      access.set_barrier_data(access.barrier_data() ^ G1C2BarrierPre);\n+    }\n+  }\n+  return BarrierSetC2::store_at_resolved(access, val);\n+}\n+\n+Node* G1BarrierSetC2::atomic_cmpxchg_val_at_resolved(C2AtomicParseAccess& access, Node* expected_val,\n+                                                         Node* new_val, const Type* value_type) const {\n+  GraphKit* kit = access.kit();\n+  if (!access.is_oop()) {\n+    return BarrierSetC2::atomic_cmpxchg_val_at_resolved(access, expected_val, new_val, value_type);\n+  }\n+  access.set_barrier_data(G1C2BarrierPre | G1C2BarrierPost);\n+  return BarrierSetC2::atomic_cmpxchg_val_at_resolved(access, expected_val, new_val, value_type);\n+}\n+\n+Node* G1BarrierSetC2::atomic_cmpxchg_bool_at_resolved(C2AtomicParseAccess& access, Node* expected_val,\n+                                                          Node* new_val, const Type* value_type) const {\n+  GraphKit* kit = access.kit();\n+  if (!access.is_oop()) {\n+    return BarrierSetC2::atomic_cmpxchg_bool_at_resolved(access, expected_val, new_val, value_type);\n+  }\n+  access.set_barrier_data(G1C2BarrierPre | G1C2BarrierPost);\n+  return BarrierSetC2::atomic_cmpxchg_bool_at_resolved(access, expected_val, new_val, value_type);\n+}\n+\n+Node* G1BarrierSetC2::atomic_xchg_at_resolved(C2AtomicParseAccess& access, Node* new_val, const Type* value_type) const {\n+  GraphKit* kit = access.kit();\n+  if (!access.is_oop()) {\n+    return BarrierSetC2::atomic_xchg_at_resolved(access, new_val, value_type);\n+  }\n+  access.set_barrier_data(G1C2BarrierPre | G1C2BarrierPost);\n+  return BarrierSetC2::atomic_xchg_at_resolved(access, new_val, value_type);\n+}\n+\n+class G1BarrierSetC2State : public BarrierSetC2State {\n+private:\n+  GrowableArray<G1BarrierStubC2*>* _stubs;\n+\n+public:\n+  G1BarrierSetC2State(Arena* arena)\n+    : BarrierSetC2State(arena),\n+      _stubs(new (arena) GrowableArray<G1BarrierStubC2*>(arena, 8,  0, nullptr)) {}\n+\n+  GrowableArray<G1BarrierStubC2*>* stubs() {\n+    return _stubs;\n+  }\n+\n+  bool needs_liveness_data(const MachNode* mach) const {\n+    return G1PreBarrierStubC2::needs_barrier(mach) ||\n+           G1PostBarrierStubC2::needs_barrier(mach);\n+  }\n+\n+  bool needs_livein_data() const {\n+    return false;\n+  }\n+};\n+\n+static G1BarrierSetC2State* barrier_set_state() {\n+  return reinterpret_cast<G1BarrierSetC2State*>(Compile::current()->barrier_set_state());\n+}\n+\n+G1BarrierStubC2::G1BarrierStubC2(const MachNode* node) : BarrierStubC2(node) {}\n+\n+G1PreBarrierStubC2::G1PreBarrierStubC2(const MachNode* node) : G1BarrierStubC2(node) {}\n+\n+bool G1PreBarrierStubC2::needs_barrier(const MachNode* node) {\n+  return (node->barrier_data() & G1C2BarrierPre) != 0;\n+}\n+\n+G1PreBarrierStubC2* G1PreBarrierStubC2::create(const MachNode* node) {\n+  G1PreBarrierStubC2* const stub = new (Compile::current()->comp_arena()) G1PreBarrierStubC2(node);\n+  if (!Compile::current()->output()->in_scratch_emit_size()) {\n+    barrier_set_state()->stubs()->append(stub);\n+  }\n+  return stub;\n+}\n+\n+void G1PreBarrierStubC2::initialize_registers(Register obj, Register pre_val, Register thread, Register tmp1, Register tmp2) {\n+  _obj = obj;\n+  _pre_val = pre_val;\n+  _thread = thread;\n+  _tmp1 = tmp1;\n+  _tmp2 = tmp2;\n+}\n+\n+Register G1PreBarrierStubC2::obj() const {\n+  return _obj;\n+}\n+\n+Register G1PreBarrierStubC2::pre_val() const {\n+  return _pre_val;\n+}\n+\n+Register G1PreBarrierStubC2::thread() const {\n+  return _thread;\n+}\n+\n+Register G1PreBarrierStubC2::tmp1() const {\n+  return _tmp1;\n+}\n+\n+Register G1PreBarrierStubC2::tmp2() const {\n+  return _tmp2;\n+}\n+\n+void G1PreBarrierStubC2::emit_code(MacroAssembler& masm) {\n+  G1BarrierSetAssembler* bs = static_cast<G1BarrierSetAssembler*>(BarrierSet::barrier_set()->barrier_set_assembler());\n+  bs->generate_c2_pre_barrier_stub(&masm, this);\n+}\n+\n+G1PostBarrierStubC2::G1PostBarrierStubC2(const MachNode* node) : G1BarrierStubC2(node) {}\n+\n+bool G1PostBarrierStubC2::needs_barrier(const MachNode* node) {\n+  return (node->barrier_data() & G1C2BarrierPost) != 0;\n+}\n+\n+G1PostBarrierStubC2* G1PostBarrierStubC2::create(const MachNode* node) {\n+  G1PostBarrierStubC2* const stub = new (Compile::current()->comp_arena()) G1PostBarrierStubC2(node);\n+  if (!Compile::current()->output()->in_scratch_emit_size()) {\n+    barrier_set_state()->stubs()->append(stub);\n+  }\n+  return stub;\n+}\n+\n+void G1PostBarrierStubC2::initialize_registers(Register thread, Register tmp1, Register tmp2) {\n+  _thread = thread;\n+  _tmp1 = tmp1;\n+  _tmp2 = tmp2;\n+}\n+\n+Register G1PostBarrierStubC2::thread() const {\n+  return _thread;\n+}\n+\n+Register G1PostBarrierStubC2::tmp1() const {\n+  return _tmp1;\n+}\n+\n+Register G1PostBarrierStubC2::tmp2() const {\n+  return _tmp2;\n+}\n+\n+void G1PostBarrierStubC2::emit_code(MacroAssembler& masm) {\n+  G1BarrierSetAssembler* bs = static_cast<G1BarrierSetAssembler*>(BarrierSet::barrier_set()->barrier_set_assembler());\n+  bs->generate_c2_post_barrier_stub(&masm, this);\n+}\n+\n+void* G1BarrierSetC2::create_barrier_state(Arena* comp_arena) const {\n+  return new (comp_arena) G1BarrierSetC2State(comp_arena);\n+}\n+\n+int G1BarrierSetC2::get_store_barrier(C2Access& access, C2AccessValue& val) const {\n+  if (!access.is_parse_access()) {\n+    \/\/ Only support for eliding barriers at parse time for now.\n+    return G1C2BarrierPre | G1C2BarrierPost;\n+  }\n+  GraphKit* kit = (static_cast<C2ParseAccess&>(access)).kit();\n+  Node* adr = access.addr().node();\n+  uint adr_idx = kit->C->get_alias_index(access.addr().type());\n+  assert(adr_idx != Compile::AliasIdxTop, \"use other store_to_memory factory\" );\n+\n+  bool can_remove_pre_barrier = g1_can_remove_pre_barrier(kit, &kit->gvn(), adr, access.type(), adr_idx);\n+\n+  bool can_remove_post_barrier = false;\n+  if (val.node() != nullptr && val.node()->is_Con() && val.node()->bottom_type() == TypePtr::NULL_PTR) {\n+    \/\/ Must be null.\n+    const Type* t = val.node()->bottom_type();\n+    assert(t == Type::TOP || t == TypePtr::NULL_PTR, \"must be NULL\");\n+    \/\/ No post barrier if writing null.\n+    can_remove_post_barrier = true;\n+  } else if (use_ReduceInitialCardMarks() && access.base() == kit->just_allocated_object(kit->control())) {\n+    \/\/ We can skip marks on a freshly-allocated object in Eden. Keep this code\n+    \/\/ in sync with CardTableBarrierSet::on_slowpath_allocation_exit. That\n+    \/\/ routine informs GC to take appropriate compensating steps, upon a\n+    \/\/ slow-path allocation, so as to make this card-mark elision safe.\n+    can_remove_post_barrier = true;\n+  } else if (use_ReduceInitialCardMarks()\n+             && g1_can_remove_post_barrier(kit, &kit->gvn(), kit->control(), adr)) {\n+    can_remove_post_barrier = true;\n+  }\n+\n+  int barriers = 0;\n+  if (!can_remove_pre_barrier) {\n+    barriers |= G1C2BarrierPre;\n+  }\n+  if (!can_remove_post_barrier) {\n+    barriers |= G1C2BarrierPost;\n+  }\n+\n+  return barriers;\n+}\n+\n+void G1BarrierSetC2::late_barrier_analysis() const {\n+  compute_liveness_at_stubs();\n+}\n+\n+void G1BarrierSetC2::emit_stubs(CodeBuffer& cb) const {\n+  MacroAssembler masm(&cb);\n+  GrowableArray<G1BarrierStubC2*>* const stubs = barrier_set_state()->stubs();\n+  for (int i = 0; i < stubs->length(); i++) {\n+    \/\/ Make sure there is enough space in the code buffer\n+    if (cb.insts()->maybe_expand_to_ensure_remaining(PhaseOutput::MAX_inst_size) && cb.blob() == nullptr) {\n+      ciEnv::current()->record_failure(\"CodeCache is full\");\n+      return;\n+    }\n+    stubs->at(i)->emit_code(masm);\n+  }\n+  masm.flush();\n+}\n+\n+#ifndef PRODUCT\n+void G1BarrierSetC2::dump_barrier_data(const MachNode* mach, outputStream* st) const {\n+  if ((mach->barrier_data() & G1C2BarrierPre) != 0) {\n+    st->print(\"pre \");\n+  }\n+  if ((mach->barrier_data() & G1C2BarrierPost) != 0) {\n+    st->print(\"post \");\n+  }\n+  if ((mach->barrier_data() & G1C2BarrierPostNotNull) != 0) {\n+    st->print(\"notnull \");\n+  }\n+}\n+#endif \/\/ !PRODUCT\n+\n+#if G1_LATE_BARRIER_MIGRATION_SUPPORT\n+\n+const TypeFunc *G1BarrierSetC2Early::write_ref_field_pre_entry_Type() {\n@@ -56,1 +555,1 @@\n-const TypeFunc *G1BarrierSetC2::write_ref_field_post_entry_Type() {\n+const TypeFunc *G1BarrierSetC2Early::write_ref_field_post_entry_Type() {\n@@ -90,5 +589,5 @@\n-bool G1BarrierSetC2::g1_can_remove_pre_barrier(GraphKit* kit,\n-                                               PhaseValues* phase,\n-                                               Node* adr,\n-                                               BasicType bt,\n-                                               uint adr_idx) const {\n+bool G1BarrierSetC2Early::g1_can_remove_pre_barrier(GraphKit* kit,\n+                                                    PhaseValues* phase,\n+                                                    Node* adr,\n+                                                    BasicType bt,\n+                                                    uint adr_idx) const {\n@@ -179,10 +678,10 @@\n-void G1BarrierSetC2::pre_barrier(GraphKit* kit,\n-                                 bool do_load,\n-                                 Node* ctl,\n-                                 Node* obj,\n-                                 Node* adr,\n-                                 uint alias_idx,\n-                                 Node* val,\n-                                 const TypeOopPtr* val_type,\n-                                 Node* pre_val,\n-                                 BasicType bt) const {\n+void G1BarrierSetC2Early::pre_barrier(GraphKit* kit,\n+                                      bool do_load,\n+                                      Node* ctl,\n+                                      Node* obj,\n+                                      Node* adr,\n+                                      uint alias_idx,\n+                                      Node* val,\n+                                      const TypeOopPtr* val_type,\n+                                      Node* pre_val,\n+                                      BasicType bt) const {\n@@ -305,3 +804,3 @@\n-bool G1BarrierSetC2::g1_can_remove_post_barrier(GraphKit* kit,\n-                                                PhaseValues* phase, Node* store,\n-                                                Node* adr) const {\n+bool G1BarrierSetC2Early::g1_can_remove_post_barrier(GraphKit* kit,\n+                                                     PhaseValues* phase, Node* store,\n+                                                     Node* adr) const {\n@@ -339,9 +838,9 @@\n-void G1BarrierSetC2::g1_mark_card(GraphKit* kit,\n-                                  IdealKit& ideal,\n-                                  Node* card_adr,\n-                                  Node* oop_store,\n-                                  uint oop_alias_idx,\n-                                  Node* index,\n-                                  Node* index_adr,\n-                                  Node* buffer,\n-                                  const TypeFunc* tf) const {\n+void G1BarrierSetC2Early::g1_mark_card(GraphKit* kit,\n+                                       IdealKit& ideal,\n+                                       Node* card_adr,\n+                                       Node* oop_store,\n+                                       uint oop_alias_idx,\n+                                       Node* index,\n+                                       Node* index_adr,\n+                                       Node* buffer,\n+                                       const TypeFunc* tf) const {\n@@ -371,9 +870,9 @@\n-void G1BarrierSetC2::post_barrier(GraphKit* kit,\n-                                  Node* ctl,\n-                                  Node* oop_store,\n-                                  Node* obj,\n-                                  Node* adr,\n-                                  uint alias_idx,\n-                                  Node* val,\n-                                  BasicType bt,\n-                                  bool use_precise) const {\n+void G1BarrierSetC2Early::post_barrier(GraphKit* kit,\n+                                       Node* ctl,\n+                                       Node* oop_store,\n+                                       Node* obj,\n+                                       Node* adr,\n+                                       uint alias_idx,\n+                                       Node* val,\n+                                       BasicType bt,\n+                                       bool use_precise) const {\n@@ -499,2 +998,2 @@\n-void G1BarrierSetC2::insert_pre_barrier(GraphKit* kit, Node* base_oop, Node* offset,\n-                                        Node* pre_val, bool need_mem_bar) const {\n+void G1BarrierSetC2Early::insert_pre_barrier(GraphKit* kit, Node* base_oop, Node* offset,\n+                                             Node* pre_val, bool need_mem_bar) const {\n@@ -595,1 +1094,1 @@\n-Node* G1BarrierSetC2::load_at_resolved(C2Access& access, const Type* val_type) const {\n+Node* G1BarrierSetC2Early::load_at_resolved(C2Access& access, const Type* val_type) const {\n@@ -664,2 +1163,2 @@\n-bool G1BarrierSetC2::is_gc_barrier_node(Node* node) const {\n-  if (CardTableBarrierSetC2::is_gc_barrier_node(node)) {\n+bool G1BarrierSetC2Early::is_gc_barrier_node(Node* node) const {\n+  if (node->Opcode() == Op_StoreCM) {\n@@ -679,1 +1178,1 @@\n-bool G1BarrierSetC2::is_g1_pre_val_load(Node* n) {\n+bool G1BarrierSetC2Early::is_g1_pre_val_load(Node* n) {\n@@ -709,1 +1208,26 @@\n-bool G1BarrierSetC2::is_gc_pre_barrier_node(Node *node) const {\n+void G1BarrierSetC2Early::clone(GraphKit* kit, Node* src, Node* dst, Node* size, bool is_array) const {\n+  BarrierSetC2::clone(kit, src, dst, size, is_array);\n+  const TypePtr* raw_adr_type = TypeRawPtr::BOTTOM;\n+\n+  \/\/ If necessary, emit some card marks afterwards.  (Non-arrays only.)\n+  bool card_mark = !is_array && !use_ReduceInitialCardMarks();\n+  if (card_mark) {\n+    assert(!is_array, \"\");\n+    \/\/ Put in store barrier for any and all oops we are sticking\n+    \/\/ into this object.  (We could avoid this if we could prove\n+    \/\/ that the object type contains no oop fields at all.)\n+    Node* no_particular_value = nullptr;\n+    Node* no_particular_field = nullptr;\n+    int raw_adr_idx = Compile::AliasIdxRaw;\n+    post_barrier(kit, kit->control(),\n+                 kit->memory(raw_adr_type),\n+                 dst,\n+                 no_particular_field,\n+                 raw_adr_idx,\n+                 no_particular_value,\n+                 T_OBJECT,\n+                 false);\n+  }\n+}\n+\n+bool G1BarrierSetC2Early::is_gc_pre_barrier_node(Node *node) const {\n@@ -713,1 +1237,1 @@\n-void G1BarrierSetC2::eliminate_gc_barrier(PhaseMacroExpand* macro, Node* node) const {\n+void G1BarrierSetC2Early::eliminate_gc_barrier(PhaseMacroExpand* macro, Node* node) const {\n@@ -798,1 +1322,1 @@\n-Node* G1BarrierSetC2::step_over_gc_barrier(Node* c) const {\n+Node* G1BarrierSetC2Early::step_over_gc_barrier(Node* c) const {\n@@ -834,1 +1358,1 @@\n-bool G1BarrierSetC2::has_cas_in_use_chain(Node *n) const {\n+bool G1BarrierSetC2Early::has_cas_in_use_chain(Node *n) const {\n@@ -864,1 +1388,1 @@\n-void G1BarrierSetC2::verify_pre_load(Node* marking_if, Unique_Node_List& loads \/*output*\/) const {\n+void G1BarrierSetC2Early::verify_pre_load(Node* marking_if, Unique_Node_List& loads \/*output*\/) const {\n@@ -909,1 +1433,1 @@\n-void G1BarrierSetC2::verify_no_safepoints(Compile* compile, Node* marking_check_if, const Unique_Node_List& loads) const {\n+void G1BarrierSetC2Early::verify_no_safepoints(Compile* compile, Node* marking_check_if, const Unique_Node_List& loads) const {\n@@ -963,1 +1487,1 @@\n-void G1BarrierSetC2::verify_gc_barriers(Compile* compile, CompilePhase phase) const {\n+void G1BarrierSetC2Early::verify_gc_barriers(Compile* compile, CompilePhase phase) const {\n@@ -1030,1 +1554,1 @@\n-bool G1BarrierSetC2::escape_add_to_con_graph(ConnectionGraph* conn_graph, PhaseGVN* gvn, Unique_Node_List* delayed_worklist, Node* n, uint opcode) const {\n+bool G1BarrierSetC2Early::escape_add_to_con_graph(ConnectionGraph* conn_graph, PhaseGVN* gvn, Unique_Node_List* delayed_worklist, Node* n, uint opcode) const {\n@@ -1058,0 +1582,2 @@\n+\n+#endif \/\/ G1_LATE_BARRIER_MIGRATION_SUPPORT\n","filename":"src\/hotspot\/share\/gc\/g1\/c2\/g1BarrierSetC2.cpp","additions":578,"deletions":52,"binary":false,"changes":630,"status":"modified"},{"patch":"@@ -34,0 +34,52 @@\n+const int G1C2BarrierPre         = 1;\n+const int G1C2BarrierPost        = 2;\n+const int G1C2BarrierPostNotNull = 4;\n+\n+class G1BarrierStubC2 : public BarrierStubC2 {\n+public:\n+  G1BarrierStubC2(const MachNode* node);\n+  virtual void emit_code(MacroAssembler& masm) = 0;\n+};\n+\n+class G1PreBarrierStubC2 : public G1BarrierStubC2 {\n+private:\n+  Register _obj;\n+  Register _pre_val;\n+  Register _thread;\n+  Register _tmp1;\n+  Register _tmp2;\n+\n+protected:\n+  G1PreBarrierStubC2(const MachNode* node);\n+\n+public:\n+  static bool needs_barrier(const MachNode* node);\n+  static G1PreBarrierStubC2* create(const MachNode* node);\n+  void initialize_registers(Register obj, Register pre_val, Register thread, Register tmp1, Register tmp2);\n+  Register obj() const;\n+  Register pre_val() const;\n+  Register thread() const;\n+  Register tmp1() const;\n+  Register tmp2() const;\n+  virtual void emit_code(MacroAssembler& masm);\n+};\n+\n+class G1PostBarrierStubC2 : public G1BarrierStubC2 {\n+private:\n+  Register _thread;\n+  Register _tmp1;\n+  Register _tmp2;\n+\n+protected:\n+  G1PostBarrierStubC2(const MachNode* node);\n+\n+public:\n+  static bool needs_barrier(const MachNode* node);\n+  static G1PostBarrierStubC2* create(const MachNode* node);\n+  void initialize_registers(Register thread, Register tmp1, Register tmp2);\n+  Register thread() const;\n+  Register tmp1() const;\n+  Register tmp2() const;\n+  virtual void emit_code(MacroAssembler& masm);\n+};\n+\n@@ -35,0 +87,39 @@\n+protected:\n+  bool g1_can_remove_pre_barrier(GraphKit* kit,\n+                                 PhaseValues* phase,\n+                                 Node* adr,\n+                                 BasicType bt,\n+                                 uint adr_idx) const;\n+\n+  bool g1_can_remove_post_barrier(GraphKit* kit,\n+                                  PhaseValues* phase, Node* store,\n+                                  Node* adr) const;\n+\n+  int get_store_barrier(C2Access& access, C2AccessValue& val) const;\n+\n+  virtual Node* load_at_resolved(C2Access& access, const Type* val_type) const;\n+  virtual Node* store_at_resolved(C2Access& access, C2AccessValue& val) const;\n+  virtual Node* atomic_cmpxchg_val_at_resolved(C2AtomicParseAccess& access, Node* expected_val,\n+                                                         Node* new_val, const Type* value_type) const;\n+  virtual Node* atomic_cmpxchg_bool_at_resolved(C2AtomicParseAccess& access, Node* expected_val,\n+                                                Node* new_val, const Type* value_type) const;\n+  virtual Node* atomic_xchg_at_resolved(C2AtomicParseAccess& access, Node* new_val, const Type* value_type) const;\n+\n+public:\n+  virtual void eliminate_gc_barrier(PhaseMacroExpand* macro, Node* node) const;\n+  virtual void eliminate_gc_barrier_data(Node* node) const;\n+  virtual bool expand_barriers(Compile* C, PhaseIterGVN& igvn) const;\n+  virtual uint estimated_barrier_size(const Node* node) const;\n+  virtual void clone_at_expansion(PhaseMacroExpand* phase,\n+                                  ArrayCopyNode* ac) const;\n+  virtual void* create_barrier_state(Arena* comp_arena) const;\n+  virtual void emit_stubs(CodeBuffer& cb) const;\n+  virtual void late_barrier_analysis() const;\n+\n+#ifndef PRODUCT\n+  virtual void dump_barrier_data(const MachNode* mach, outputStream* st) const;\n+#endif\n+};\n+\n+#if G1_LATE_BARRIER_MIGRATION_SUPPORT\n+class G1BarrierSetC2Early : public CardTableBarrierSetC2 {\n@@ -95,0 +186,1 @@\n+  virtual void clone(GraphKit* kit, Node* src, Node* dst, Node* size, bool is_array) const;\n@@ -106,0 +198,1 @@\n+#endif \/\/ G1_LATE_BARRIER_MIGRATION_SUPPORT\n","filename":"src\/hotspot\/share\/gc\/g1\/c2\/g1BarrierSetC2.hpp","additions":93,"deletions":0,"binary":false,"changes":93,"status":"modified"},{"patch":"@@ -248,0 +248,33 @@\n+\n+#if G1_LATE_BARRIER_MIGRATION_SUPPORT\n+  if (G1StressBarriers) {\n+    \/\/ Increase the frequency of concurrent marking (so that the pre-barrier is\n+    \/\/ not trivially skipped).\n+    if (FLAG_IS_DEFAULT(G1UseAdaptiveIHOP)) {\n+      FLAG_SET_ERGO(G1UseAdaptiveIHOP, false);\n+    }\n+    if (FLAG_IS_DEFAULT(InitiatingHeapOccupancyPercent)) {\n+      FLAG_SET_ERGO(InitiatingHeapOccupancyPercent, 0);\n+    }\n+    \/\/ Exercise both pre-barrier enqueue paths (inline and runtime) equally.\n+    if (FLAG_IS_DEFAULT(G1SATBBufferSize)) {\n+      FLAG_SET_ERGO(G1SATBBufferSize, 2);\n+    }\n+    \/\/ Increase the frequency of inter-regional objects in the post-barrier.\n+    if (FLAG_IS_DEFAULT(G1HeapRegionSize)) {\n+      FLAG_SET_ERGO(G1HeapRegionSize, HeapRegionBounds::min_size());\n+    }\n+    \/\/ Increase the frequency with which the post-barrier sees a clean card and\n+    \/\/ has to dirty it.\n+    if (FLAG_IS_DEFAULT(GCCardSizeInBytes)) {\n+      FLAG_SET_ERGO(GCCardSizeInBytes, MAX2(ObjectAlignmentInBytes, 128));\n+    }\n+    if (FLAG_IS_DEFAULT(G1RSetUpdatingPauseTimePercent)) {\n+      FLAG_SET_ERGO(G1RSetUpdatingPauseTimePercent, 0);\n+    }\n+    \/\/ Exercise both post-barrier dirtying paths (inline and runtime) equally.\n+    if (FLAG_IS_DEFAULT(G1UpdateBufferSize)) {\n+      FLAG_SET_ERGO(G1UpdateBufferSize, 2);\n+    }\n+  }\n+#endif \/\/ G1_LATE_BARRIER_MIGRATION_SUPPORT\n","filename":"src\/hotspot\/share\/gc\/g1\/g1Arguments.cpp","additions":33,"deletions":0,"binary":false,"changes":33,"status":"modified"},{"patch":"@@ -52,0 +52,3 @@\n+#if G1_LATE_BARRIER_MIGRATION_SUPPORT\n+class G1BarrierSetC2Early;\n+#endif\n@@ -56,0 +59,5 @@\n+#if G1_LATE_BARRIER_MIGRATION_SUPPORT\n+                      G1UseLateBarrierExpansion ?\n+                      make_barrier_set_c2<G1BarrierSetC2>() :\n+                      make_barrier_set_c2<G1BarrierSetC2Early>(),\n+#else\n@@ -57,0 +65,1 @@\n+#endif\n","filename":"src\/hotspot\/share\/gc\/g1\/g1BarrierSet.cpp","additions":9,"deletions":0,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -64,0 +64,8 @@\n+\n+JRT_LEAF(void, G1BarrierSetRuntime::clone(oopDesc* src, oopDesc* dst, size_t size))\n+  HeapAccess<>::clone(src, dst, size);\n+JRT_END\n+\n+address G1BarrierSetRuntime::clone_addr() {\n+  return reinterpret_cast<address>(clone);\n+}\n","filename":"src\/hotspot\/share\/gc\/g1\/g1BarrierSetRuntime.cpp","additions":8,"deletions":0,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -38,0 +38,2 @@\n+private:\n+  static void clone(oopDesc* src, oopDesc* dst, size_t size);\n@@ -49,0 +51,2 @@\n+\n+  static address clone_addr();\n","filename":"src\/hotspot\/share\/gc\/g1\/g1BarrierSetRuntime.hpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -90,0 +90,16 @@\n+\n+\/\/ Temporary flags to support the migration from early to late barrier expansion\n+\/\/ (see JEP 475) for all platforms. These flags are not intended to be\n+\/\/ integrated in the main JDK repository.\n+#if G1_LATE_BARRIER_MIGRATION_SUPPORT\n+#define G1_LATE_BARRIER_MIGRATION_SUPPORT_FLAGS(product)                    \\\n+                                                                            \\\n+  product(bool, G1UseLateBarrierExpansion, true,                            \\\n+          \"Expand G1 barriers late during C2 compilation\")                  \\\n+                                                                            \\\n+  product(bool, G1StressBarriers, false,                                    \\\n+          \"Configure G1 to exercise cold barrier paths\")\n+#else\n+#define G1_LATE_BARRIER_MIGRATION_SUPPORT_FLAGS(product)\n+#endif\n+\n@@ -342,1 +358,3 @@\n-                    constraint)\n+                    constraint)                                             \\\n+                                                                            \\\n+  G1_LATE_BARRIER_MIGRATION_SUPPORT_FLAGS(product)\n","filename":"src\/hotspot\/share\/gc\/g1\/g1_globals.hpp","additions":19,"deletions":1,"binary":false,"changes":20,"status":"modified"},{"patch":"@@ -112,0 +112,4 @@\n+uint8_t BarrierStubC2::barrier_data() const {\n+  return _node->barrier_data();\n+}\n+\n","filename":"src\/hotspot\/share\/gc\/shared\/c2\/barrierSetC2.cpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -257,0 +257,2 @@\n+  \/\/ High-level, GC-specific barrier flags.\n+  uint8_t barrier_data() const;\n","filename":"src\/hotspot\/share\/gc\/shared\/c2\/barrierSetC2.hpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -128,25 +128,0 @@\n-void CardTableBarrierSetC2::clone(GraphKit* kit, Node* src, Node* dst, Node* size, bool is_array) const {\n-  BarrierSetC2::clone(kit, src, dst, size, is_array);\n-  const TypePtr* raw_adr_type = TypeRawPtr::BOTTOM;\n-\n-  \/\/ If necessary, emit some card marks afterwards.  (Non-arrays only.)\n-  bool card_mark = !is_array && !use_ReduceInitialCardMarks();\n-  if (card_mark) {\n-    assert(!is_array, \"\");\n-    \/\/ Put in store barrier for any and all oops we are sticking\n-    \/\/ into this object.  (We could avoid this if we could prove\n-    \/\/ that the object type contains no oop fields at all.)\n-    Node* no_particular_value = nullptr;\n-    Node* no_particular_field = nullptr;\n-    int raw_adr_idx = Compile::AliasIdxRaw;\n-    post_barrier(kit, kit->control(),\n-                 kit->memory(raw_adr_type),\n-                 dst,\n-                 no_particular_field,\n-                 raw_adr_idx,\n-                 no_particular_value,\n-                 T_OBJECT,\n-                 false);\n-  }\n-}\n-\n@@ -157,4 +132,0 @@\n-bool CardTableBarrierSetC2::is_gc_barrier_node(Node* node) const {\n-  return ModRefBarrierSetC2::is_gc_barrier_node(node) || node->Opcode() == Op_StoreCM;\n-}\n-\n","filename":"src\/hotspot\/share\/gc\/shared\/c2\/cardTableBarrierSetC2.cpp","additions":0,"deletions":29,"binary":false,"changes":29,"status":"modified"},{"patch":"@@ -45,2 +45,0 @@\n-  virtual void clone(GraphKit* kit, Node* src, Node* dst, Node* size, bool is_array) const;\n-  virtual bool is_gc_barrier_node(Node* node) const;\n","filename":"src\/hotspot\/share\/gc\/shared\/c2\/cardTableBarrierSetC2.hpp","additions":0,"deletions":2,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -238,0 +238,7 @@\n+    if (def->is_MachTemp()) {\n+      assert(!def->bottom_type()->isa_oop_ptr(),\n+             \"ADLC only assigns OOP types to MachTemp defs corresponding to xRegN operands\");\n+      \/\/ Exclude MachTemp definitions even if they are typed as oops.\n+      continue;\n+    }\n+\n","filename":"src\/hotspot\/share\/opto\/buildOopMap.cpp","additions":7,"deletions":0,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -1584,0 +1584,6 @@\n+    if (Matcher::is_encode_and_store_pattern(n, m)) {\n+      \/\/ Make it possible to match \"encode and store\" patterns, regardless of\n+      \/\/ whether the encode operation is pinned to a control node (e.g. by\n+      \/\/ CastPP node removal in final graph reshaping).\n+      return false;\n+    }\n@@ -1807,2 +1813,0 @@\n-    assert(C->node_arena()->contains(s->_leaf) || !has_new_node(s->_leaf),\n-           \"duplicating node that's already been matched\");\n@@ -2823,0 +2827,7 @@\n+bool Matcher::is_encode_and_store_pattern(const Node* n, const Node* m) {\n+  return n != nullptr &&\n+    m != nullptr &&\n+    n->Opcode() == Op_StoreN &&\n+    m->is_EncodeP();\n+}\n+\n","filename":"src\/hotspot\/share\/opto\/matcher.cpp","additions":13,"deletions":2,"binary":false,"changes":15,"status":"modified"},{"patch":"@@ -388,0 +388,2 @@\n+  static bool is_encode_and_store_pattern(const Node* n, const Node* m);\n+\n","filename":"src\/hotspot\/share\/opto\/matcher.hpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -3447,1 +3447,1 @@\n-  if (ReduceFieldZeroing && \/*can_reshape &&*\/\n+  if (ReduceFieldZeroing && ReduceInitialCardMarks && \/*can_reshape &&*\/\n","filename":"src\/hotspot\/share\/opto\/memnode.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -34,0 +34,8 @@\n+\/\/ G1_LATE_BARRIER_MIGRATION_SUPPORT enables temporary support for migrating\n+\/\/ from early to late barrier expansion (see JEP 475) for all platforms.\n+\/\/ This support is not intended to be integrated in the main JDK repository.\n+#ifdef G1_LATE_BARRIER_MIGRATION_SUPPORT\n+#error \"G1_LATE_BARRIER_MIGRATION_SUPPORT already defined\"\n+#endif\n+#define G1_LATE_BARRIER_MIGRATION_SUPPORT 0\n+\n","filename":"src\/hotspot\/share\/runtime\/globals_shared.hpp","additions":8,"deletions":0,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -272,6 +272,0 @@\n-            \/\/ a card mark volatile barrier should be generated\n-            \/\/ before the card mark strb\n-            \/\/\n-            \/\/ following the fix for 8225776 the G1 barrier is now\n-            \/\/ scheduled out of line after the membar volatile and\n-            \/\/ and subsequent return\n@@ -282,4 +276,1 @@\n-                \"ret\",\n-                \"membar_volatile\",\n-                \"dmb ish\",\n-                \"strb\"\n+                \"ret\"\n@@ -344,6 +335,0 @@\n-            \/\/ a card mark volatile barrier should be generated\n-            \/\/ before the card mark strb\n-            \/\/\n-            \/\/ following the fix for 8225776 the G1 barrier is now\n-            \/\/ scheduled out of line after the membar acquire and\n-            \/\/ and subsequent return\n@@ -354,4 +339,1 @@\n-                \"ret\",\n-                \"membar_volatile\",\n-                \"dmb ish\",\n-                \"strb\"\n+                \"ret\"\n@@ -431,6 +413,0 @@\n-            \/\/ a card mark volatile barrier should be generated\n-            \/\/ before the card mark strb\n-            \/\/\n-            \/\/ following the fix for 8225776 the G1 barrier is now\n-            \/\/ scheduled out of line after the membar acquire and\n-            \/\/ and subsequent return\n@@ -441,4 +417,1 @@\n-                \"ret\",\n-                \"membar_volatile\",\n-                \"dmb ish\",\n-                \"strb\"\n+                \"ret\"\n@@ -498,6 +471,0 @@\n-            \/\/ a card mark volatile barrier should be generated\n-            \/\/ before the card mark strb\n-            \/\/\n-            \/\/ following the fix for 8225776 the G1 barrier is now\n-            \/\/ scheduled out of line after the membar acquire and\n-            \/\/ and subsequent return\n@@ -508,4 +475,1 @@\n-                \"ret\",\n-                \"membar_volatile\",\n-                \"dmb ish\",\n-                \"strb\"\n+                \"ret\"\n","filename":"test\/hotspot\/jtreg\/compiler\/c2\/aarch64\/TestVolatiles.java","additions":4,"deletions":40,"binary":false,"changes":44,"status":"modified"},{"patch":"@@ -1358,3 +1358,6 @@\n-    @IR(counts = { IRNode.ALLOC, \"1\" })\n-    \/\/ The last allocation won't be reduced because it would cause the creation\n-    \/\/ of a nested SafePointScalarMergeNode.\n+    \/\/ Using G1, all allocations are reduced.\n+    @IR(applyIf = {\"UseG1GC\", \"true\"}, failOn = { IRNode.ALLOC })\n+    \/\/ Otherwise, the last allocation won't be reduced because it would cause\n+    \/\/ the creation of a nested SafePointScalarMergeNode. This is caused by the\n+    \/\/ store barrier corresponding to 'C.other = B'.\n+    @IR(applyIf = {\"UseG1GC\", \"false\"}, counts = { IRNode.ALLOC, \"1\" })\n","filename":"test\/hotspot\/jtreg\/compiler\/c2\/irTests\/scalarReplacement\/AllocationMergesTests.java","additions":6,"deletions":3,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -0,0 +1,510 @@\n+\/*\n+ * Copyright (c) 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+package compiler.gcbarriers;\n+\n+import compiler.lib.ir_framework.*;\n+import java.lang.invoke.VarHandle;\n+import java.lang.invoke.MethodHandles;\n+import java.lang.ref.Reference;\n+import java.lang.ref.ReferenceQueue;\n+import java.lang.ref.SoftReference;\n+import java.lang.ref.WeakReference;\n+import jdk.test.lib.Asserts;\n+\n+\/**\n+ * @test\n+ * @summary Test that G1 barriers are generated and optimized as expected.\n+ * @library \/test\/lib \/\n+ * @requires vm.gc.G1\n+ * @run driver compiler.gcbarriers.TestG1BarrierGeneration\n+ *\/\n+\n+public class TestG1BarrierGeneration {\n+    static final String PRE_ONLY = \"pre\";\n+    static final String POST_ONLY = \"post\";\n+    static final String PRE_AND_POST = \"pre post\";\n+    static final String PRE_AND_POST_NOT_NULL = \"pre post notnull\";\n+\n+    static class Outer {\n+        Object f;\n+    }\n+\n+    static class OuterWithFewFields implements Cloneable {\n+        Object f1;\n+        Object f2;\n+        public Object clone() throws CloneNotSupportedException {\n+            return super.clone();\n+        }\n+    }\n+\n+    static class OuterWithManyFields implements Cloneable {\n+        Object f1;\n+        Object f2;\n+        Object f3;\n+        Object f4;\n+        Object f5;\n+        Object f6;\n+        Object f7;\n+        Object f8;\n+        Object f9;\n+        Object f10;\n+        public Object clone() throws CloneNotSupportedException {\n+            return super.clone();\n+        }\n+    }\n+\n+    static final VarHandle fVarHandle;\n+    static {\n+        MethodHandles.Lookup l = MethodHandles.lookup();\n+        try {\n+            fVarHandle = l.findVarHandle(Outer.class, \"f\", Object.class);\n+        } catch (Exception e) {\n+            throw new Error(e);\n+        }\n+    }\n+\n+    public static void main(String[] args) {\n+        TestFramework framework = new TestFramework();\n+        Scenario[] scenarios = new Scenario[2*2];\n+        int scenarioIndex = 0;\n+        for (int i = 0; i < 2; i++) {\n+            for (int j = 0; j < 2; j++) {\n+                scenarios[scenarioIndex] =\n+                    new Scenario(scenarioIndex,\n+                                 \"-XX:CompileCommand=inline,java.lang.ref.*::*\",\n+                                 \"-XX:\" + (i == 0 ? \"-\" : \"+\") + \"UseCompressedOops\",\n+                                 \"-XX:\" + (j == 0 ? \"-\" : \"+\") + \"ReduceInitialCardMarks\");\n+                scenarioIndex++;\n+            }\n+        }\n+        framework.addScenarios(scenarios);\n+        framework.start();\n+    }\n+\n+    @Test\n+    @IR(applyIf = {\"UseCompressedOops\", \"false\"},\n+        counts = {IRNode.G1_STORE_P_WITH_BARRIER_FLAG, PRE_AND_POST, \"1\"},\n+        phase = CompilePhase.FINAL_CODE)\n+    @IR(applyIf = {\"UseCompressedOops\", \"true\"},\n+        counts = {IRNode.G1_ENCODE_P_AND_STORE_N_WITH_BARRIER_FLAG, PRE_AND_POST, \"1\"},\n+        phase = CompilePhase.FINAL_CODE)\n+    public static void testStore(Outer o, Object o1) {\n+        o.f = o1;\n+    }\n+\n+    @Test\n+    @IR(applyIf = {\"UseCompressedOops\", \"false\"},\n+        counts = {IRNode.G1_STORE_P_WITH_BARRIER_FLAG, PRE_ONLY, \"1\"},\n+        phase = CompilePhase.FINAL_CODE)\n+    @IR(applyIf = {\"UseCompressedOops\", \"true\"},\n+        counts = {IRNode.G1_STORE_N_WITH_BARRIER_FLAG, PRE_ONLY, \"1\"},\n+        phase = CompilePhase.FINAL_CODE)\n+    public static void testStoreNull(Outer o) {\n+        o.f = null;\n+    }\n+\n+    @Test\n+    @IR(applyIf = {\"UseCompressedOops\", \"false\"},\n+        counts = {IRNode.G1_STORE_P_WITH_BARRIER_FLAG, PRE_AND_POST_NOT_NULL, \"1\"},\n+        phase = CompilePhase.FINAL_CODE)\n+    @IR(applyIf = {\"UseCompressedOops\", \"true\"},\n+        counts = {IRNode.G1_ENCODE_P_AND_STORE_N_WITH_BARRIER_FLAG, PRE_AND_POST_NOT_NULL, \"1\"},\n+        phase = CompilePhase.FINAL_CODE)\n+    public static void testStoreNotNull(Outer o, Object o1) {\n+        if (o1.hashCode() == 42) {\n+            return;\n+        }\n+        o.f = o1;\n+    }\n+\n+    @Test\n+    @IR(applyIf = {\"UseCompressedOops\", \"false\"},\n+        counts = {IRNode.G1_STORE_P_WITH_BARRIER_FLAG, PRE_AND_POST, \"2\"},\n+        phase = CompilePhase.FINAL_CODE)\n+    @IR(applyIf = {\"UseCompressedOops\", \"true\"},\n+        counts = {IRNode.G1_ENCODE_P_AND_STORE_N_WITH_BARRIER_FLAG, PRE_AND_POST, \"2\"},\n+        phase = CompilePhase.FINAL_CODE)\n+    public static void testStoreTwice(Outer o, Outer p, Object o1) {\n+        o.f = o1;\n+        p.f = o1;\n+    }\n+\n+    @Test\n+    @IR(applyIfAnd = {\"UseCompressedOops\", \"false\", \"ReduceInitialCardMarks\", \"true\"},\n+        failOn = {IRNode.G1_STORE_P},\n+        phase = CompilePhase.FINAL_CODE)\n+    @IR(applyIfAnd = {\"UseCompressedOops\", \"true\", \"ReduceInitialCardMarks\", \"true\"},\n+        failOn = {IRNode.G1_STORE_N, IRNode.G1_ENCODE_P_AND_STORE_N},\n+        phase = CompilePhase.FINAL_CODE)\n+    public static Outer testStoreOnNewObject(Object o1) {\n+        Outer o = new Outer();\n+        o.f = o1;\n+        return o;\n+    }\n+\n+    @Run(test = {\"testStore\",\n+                 \"testStoreNull\",\n+                 \"testStoreNotNull\",\n+                 \"testStoreTwice\",\n+                 \"testStoreOnNewObject\"})\n+    public void runStoreTests() {\n+        {\n+            Outer o = new Outer();\n+            Object o1 = new Object();\n+            testStore(o, o1);\n+            Asserts.assertEquals(o1, o.f);\n+        }\n+        {\n+            Outer o = new Outer();\n+            testStoreNull(o);\n+            Asserts.assertNull(o.f);\n+        }\n+        {\n+            Outer o = new Outer();\n+            Object o1 = new Object();\n+            testStoreNotNull(o, o1);\n+            Asserts.assertEquals(o1, o.f);\n+        }\n+        {\n+            Outer o = new Outer();\n+            Outer p = new Outer();\n+            Object o1 = new Object();\n+            testStoreTwice(o, p, o1);\n+            Asserts.assertEquals(o1, o.f);\n+            Asserts.assertEquals(o1, p.f);\n+        }\n+        {\n+            Object o1 = new Object();\n+            Outer o = testStoreOnNewObject(o1);\n+            Asserts.assertEquals(o1, o.f);\n+        }\n+    }\n+\n+    @Test\n+    @IR(applyIf = {\"UseCompressedOops\", \"false\"},\n+        counts = {IRNode.G1_STORE_P_WITH_BARRIER_FLAG, PRE_AND_POST, \"1\"},\n+        phase = CompilePhase.FINAL_CODE)\n+    @IR(applyIf = {\"UseCompressedOops\", \"true\"},\n+        counts = {IRNode.G1_ENCODE_P_AND_STORE_N_WITH_BARRIER_FLAG, PRE_AND_POST, \"1\"},\n+        phase = CompilePhase.FINAL_CODE)\n+    public static void testArrayStore(Object[] a, int index, Object o1) {\n+        a[index] = o1;\n+    }\n+\n+    @Test\n+    @IR(applyIf = {\"UseCompressedOops\", \"false\"},\n+        counts = {IRNode.G1_STORE_P_WITH_BARRIER_FLAG, PRE_ONLY, \"1\"},\n+        phase = CompilePhase.FINAL_CODE)\n+    @IR(applyIf = {\"UseCompressedOops\", \"true\"},\n+        counts = {IRNode.G1_STORE_N_WITH_BARRIER_FLAG, PRE_ONLY, \"1\"},\n+        phase = CompilePhase.FINAL_CODE)\n+    public static void testArrayStoreNull(Object[] a, int index) {\n+        a[index] = null;\n+    }\n+\n+    @Test\n+    @IR(applyIf = {\"UseCompressedOops\", \"false\"},\n+        counts = {IRNode.G1_STORE_P_WITH_BARRIER_FLAG, PRE_AND_POST_NOT_NULL, \"1\"},\n+        phase = CompilePhase.FINAL_CODE)\n+    @IR(applyIf = {\"UseCompressedOops\", \"true\"},\n+        counts = {IRNode.G1_ENCODE_P_AND_STORE_N_WITH_BARRIER_FLAG, PRE_AND_POST_NOT_NULL, \"1\"},\n+        phase = CompilePhase.FINAL_CODE)\n+    public static void testArrayStoreNotNull(Object[] a, int index, Object o1) {\n+        if (o1.hashCode() == 42) {\n+            return;\n+        }\n+        a[index] = o1;\n+    }\n+\n+    @Test\n+    @IR(applyIf = {\"UseCompressedOops\", \"false\"},\n+        counts = {IRNode.G1_STORE_P_WITH_BARRIER_FLAG, PRE_AND_POST, \"2\"},\n+        phase = CompilePhase.FINAL_CODE)\n+    @IR(applyIf = {\"UseCompressedOops\", \"true\"},\n+        counts = {IRNode.G1_ENCODE_P_AND_STORE_N_WITH_BARRIER_FLAG, PRE_AND_POST, \"2\"},\n+        phase = CompilePhase.FINAL_CODE)\n+    public static void testArrayStoreTwice(Object[] a, Object[] b, int index, Object o1) {\n+        a[index] = o1;\n+        b[index] = o1;\n+    }\n+\n+    @Test\n+    @IR(applyIfAnd = {\"UseCompressedOops\", \"false\", \"ReduceInitialCardMarks\", \"true\"},\n+        failOn = {IRNode.G1_STORE_P},\n+        phase = CompilePhase.FINAL_CODE)\n+    @IR(applyIfAnd = {\"UseCompressedOops\", \"true\", \"ReduceInitialCardMarks\", \"true\"},\n+        failOn = {IRNode.G1_STORE_N, IRNode.G1_ENCODE_P_AND_STORE_N},\n+        phase = CompilePhase.FINAL_CODE)\n+    public static Object[] testStoreOnNewArray(Object o1) {\n+        Object[] a = new Object[10];\n+        \/\/ The index needs to be concrete for C2 to detect that it is safe to\n+        \/\/ remove the pre-barrier.\n+        a[4] = o1;\n+        return a;\n+    }\n+\n+    @Run(test = {\"testArrayStore\",\n+                 \"testArrayStoreNull\",\n+                 \"testArrayStoreNotNull\",\n+                 \"testArrayStoreTwice\",\n+                 \"testStoreOnNewArray\"})\n+    public void runArrayStoreTests() {\n+        {\n+            Object[] a = new Object[10];\n+            Object o1 = new Object();\n+            testArrayStore(a, 4, o1);\n+            Asserts.assertEquals(o1, a[4]);\n+        }\n+        {\n+            Object[] a = new Object[10];\n+            testArrayStoreNull(a, 4);\n+            Asserts.assertNull(a[4]);\n+        }\n+        {\n+            Object[] a = new Object[10];\n+            Object o1 = new Object();\n+            testArrayStoreNotNull(a, 4, o1);\n+            Asserts.assertEquals(o1, a[4]);\n+        }\n+        {\n+            Object[] a = new Object[10];\n+            Object[] b = new Object[10];\n+            Object o1 = new Object();\n+            testArrayStoreTwice(a, b, 4, o1);\n+            Asserts.assertEquals(o1, a[4]);\n+            Asserts.assertEquals(o1, b[4]);\n+        }\n+        {\n+            Object o1 = new Object();\n+            Object[] a = testStoreOnNewArray(o1);\n+            Asserts.assertEquals(o1, a[4]);\n+        }\n+    }\n+\n+    @Test\n+    public static Object[] testCloneArrayOfObjects(Object[] a) {\n+        Object[] a1 = null;\n+        try {\n+            a1 = a.clone();\n+        } catch (Exception e) {}\n+        return a1;\n+    }\n+\n+    @Test\n+    @IR(applyIf = {\"ReduceInitialCardMarks\", \"true\"},\n+        failOn = {IRNode.G1_STORE_P, IRNode.G1_STORE_N, IRNode.G1_ENCODE_P_AND_STORE_N},\n+        phase = CompilePhase.FINAL_CODE)\n+    @IR(applyIfAnd = {\"ReduceInitialCardMarks\", \"false\", \"UseCompressedOops\", \"false\"},\n+        counts = {IRNode.G1_STORE_P_WITH_BARRIER_FLAG, POST_ONLY, \"2\"},\n+        phase = CompilePhase.FINAL_CODE)\n+    @IR(applyIfAnd = {\"ReduceInitialCardMarks\", \"false\", \"UseCompressedOops\", \"true\"},\n+        counts = {IRNode.G1_STORE_N_WITH_BARRIER_FLAG, POST_ONLY, \"2\"},\n+        phase = CompilePhase.FINAL_CODE)\n+    public static OuterWithFewFields testCloneObjectWithFewFields(OuterWithFewFields o) {\n+        Object o1 = null;\n+        try {\n+            o1 = o.clone();\n+        } catch (Exception e) {}\n+        return (OuterWithFewFields)o1;\n+    }\n+\n+    @Test\n+    @IR(applyIf = {\"ReduceInitialCardMarks\", \"true\"},\n+        counts = {IRNode.CALL_OF, \"jlong_disjoint_arraycopy\", \"1\"})\n+    @IR(applyIf = {\"ReduceInitialCardMarks\", \"false\"},\n+        counts = {IRNode.CALL_OF, \"G1BarrierSetRuntime::clone\", \"1\"})\n+    public static OuterWithManyFields testCloneObjectWithManyFields(OuterWithManyFields o) {\n+        Object o1 = null;\n+        try {\n+            o1 = o.clone();\n+        } catch (Exception e) {}\n+        return (OuterWithManyFields)o1;\n+    }\n+\n+    @Run(test = {\"testCloneArrayOfObjects\",\n+                 \"testCloneObjectWithFewFields\",\n+                 \"testCloneObjectWithManyFields\"})\n+    public void runCloneTests() {\n+        {\n+            Object o1 = new Object();\n+            Object[] a = new Object[4];\n+            for (int i = 0; i < 4; i++) {\n+                a[i] = o1;\n+            }\n+            Object[] a1 = testCloneArrayOfObjects(a);\n+            for (int i = 0; i < 4; i++) {\n+                Asserts.assertEquals(o1, a1[i]);\n+            }\n+        }\n+        {\n+            Object a = new Object();\n+            Object b = new Object();\n+            OuterWithFewFields o = new OuterWithFewFields();\n+            o.f1 = a;\n+            o.f2 = b;\n+            OuterWithFewFields o1 = testCloneObjectWithFewFields(o);\n+            Asserts.assertEquals(a, o1.f1);\n+            Asserts.assertEquals(b, o1.f2);\n+        }\n+        {\n+            Object a = new Object();\n+            Object b = new Object();\n+            Object c = new Object();\n+            Object d = new Object();\n+            Object e = new Object();\n+            Object f = new Object();\n+            Object g = new Object();\n+            Object h = new Object();\n+            Object i = new Object();\n+            Object j = new Object();\n+            OuterWithManyFields o = new OuterWithManyFields();\n+            o.f1 = a;\n+            o.f2 = b;\n+            o.f3 = c;\n+            o.f4 = d;\n+            o.f5 = e;\n+            o.f6 = f;\n+            o.f7 = g;\n+            o.f8 = h;\n+            o.f9 = i;\n+            o.f10 = j;\n+            OuterWithManyFields o1 = testCloneObjectWithManyFields(o);\n+            Asserts.assertEquals(a, o1.f1);\n+            Asserts.assertEquals(b, o1.f2);\n+            Asserts.assertEquals(c, o1.f3);\n+            Asserts.assertEquals(d, o1.f4);\n+            Asserts.assertEquals(e, o1.f5);\n+            Asserts.assertEquals(f, o1.f6);\n+            Asserts.assertEquals(g, o1.f7);\n+            Asserts.assertEquals(h, o1.f8);\n+            Asserts.assertEquals(i, o1.f9);\n+            Asserts.assertEquals(j, o1.f10);\n+        }\n+    }\n+\n+    @Test\n+    @IR(applyIf = {\"UseCompressedOops\", \"false\"},\n+        counts = {IRNode.G1_COMPARE_AND_EXCHANGE_P_WITH_BARRIER_FLAG, PRE_AND_POST, \"1\"},\n+        phase = CompilePhase.FINAL_CODE)\n+    @IR(applyIf = {\"UseCompressedOops\", \"true\"},\n+        counts = {IRNode.G1_COMPARE_AND_EXCHANGE_N_WITH_BARRIER_FLAG, PRE_AND_POST, \"1\"},\n+        phase = CompilePhase.FINAL_CODE)\n+    static Object testCompareAndExchange(Outer o, Object oldVal, Object newVal) {\n+        return fVarHandle.compareAndExchange(o, oldVal, newVal);\n+    }\n+\n+    @Test\n+    @IR(applyIf = {\"UseCompressedOops\", \"false\"},\n+        counts = {IRNode.G1_COMPARE_AND_SWAP_P_WITH_BARRIER_FLAG, PRE_AND_POST, \"1\"},\n+        phase = CompilePhase.FINAL_CODE)\n+    @IR(applyIf = {\"UseCompressedOops\", \"true\"},\n+        counts = {IRNode.G1_COMPARE_AND_SWAP_N_WITH_BARRIER_FLAG, PRE_AND_POST, \"1\"},\n+        phase = CompilePhase.FINAL_CODE)\n+    static boolean testCompareAndSwap(Outer o, Object oldVal, Object newVal) {\n+        return fVarHandle.compareAndSet(o, oldVal, newVal);\n+    }\n+\n+    @Test\n+    @IR(applyIf = {\"UseCompressedOops\", \"false\"},\n+        counts = {IRNode.G1_GET_AND_SET_P_WITH_BARRIER_FLAG, PRE_AND_POST, \"1\"},\n+        phase = CompilePhase.FINAL_CODE)\n+    @IR(applyIf = {\"UseCompressedOops\", \"true\"},\n+        counts = {IRNode.G1_GET_AND_SET_N_WITH_BARRIER_FLAG, PRE_AND_POST, \"1\"},\n+        phase = CompilePhase.FINAL_CODE)\n+    static Object testGetAndSet(Outer o, Object newVal) {\n+        return fVarHandle.getAndSet(o, newVal);\n+    }\n+\n+    @Run(test = {\"testCompareAndExchange\",\n+                 \"testCompareAndSwap\",\n+                 \"testGetAndSet\"})\n+    public void runAtomicTests() {\n+        {\n+            Outer o = new Outer();\n+            Object oldVal = new Object();\n+            o.f = oldVal;\n+            Object newVal = new Object();\n+            Object oldVal2 = testCompareAndExchange(o, oldVal, newVal);\n+            Asserts.assertEquals(oldVal, oldVal2);\n+            Asserts.assertEquals(o.f, newVal);\n+        }\n+        {\n+            Outer o = new Outer();\n+            Object oldVal = new Object();\n+            o.f = oldVal;\n+            Object newVal = new Object();\n+            boolean b = testCompareAndSwap(o, oldVal, newVal);\n+            Asserts.assertTrue(b);\n+            Asserts.assertEquals(o.f, newVal);\n+        }\n+        {\n+            Outer o = new Outer();\n+            Object oldVal = new Object();\n+            o.f = oldVal;\n+            Object newVal = new Object();\n+            Object oldVal2 = testGetAndSet(o, newVal);\n+            Asserts.assertEquals(oldVal, oldVal2);\n+            Asserts.assertEquals(o.f, newVal);\n+        }\n+    }\n+\n+    @Test\n+    @IR(applyIf = {\"UseCompressedOops\", \"false\"},\n+        counts = {IRNode.G1_LOAD_P_WITH_BARRIER_FLAG, PRE_ONLY, \"1\"},\n+        phase = CompilePhase.FINAL_CODE)\n+    @IR(applyIf = {\"UseCompressedOops\", \"true\"},\n+        counts = {IRNode.G1_LOAD_N_WITH_BARRIER_FLAG, PRE_ONLY, \"1\"},\n+        phase = CompilePhase.FINAL_CODE)\n+    static Object testLoadSoftReference(SoftReference<Object> ref) {\n+        return ref.get();\n+    }\n+\n+    @Test\n+    @IR(applyIf = {\"UseCompressedOops\", \"false\"},\n+        counts = {IRNode.G1_LOAD_P_WITH_BARRIER_FLAG, PRE_ONLY, \"1\"},\n+        phase = CompilePhase.FINAL_CODE)\n+    @IR(applyIf = {\"UseCompressedOops\", \"true\"},\n+        counts = {IRNode.G1_LOAD_N_WITH_BARRIER_FLAG, PRE_ONLY, \"1\"},\n+        phase = CompilePhase.FINAL_CODE)\n+    static Object testLoadWeakReference(WeakReference<Object> ref) {\n+        return ref.get();\n+    }\n+\n+    @Run(test = {\"testLoadSoftReference\",\n+                 \"testLoadWeakReference\"})\n+    public void runReferenceTests() {\n+        {\n+            Object o1 = new Object();\n+            SoftReference<Object> sref = new SoftReference<Object>(o1);\n+            Object o2 = testLoadSoftReference(sref);\n+            Asserts.assertTrue(o2 == o1 || o2 == null);\n+        }\n+        {\n+            Object o1 = new Object();\n+            WeakReference<Object> wref = new WeakReference<Object>(o1);\n+            Object o2 = testLoadWeakReference(wref);\n+            Asserts.assertTrue(o2 == o1 || o2 == null);\n+        }\n+    }\n+}\n","filename":"test\/hotspot\/jtreg\/compiler\/gcbarriers\/TestG1BarrierGeneration.java","additions":510,"deletions":0,"binary":false,"changes":510,"status":"added"},{"patch":"@@ -361,0 +361,5 @@\n+    public static final String CALL_OF = COMPOSITE_PREFIX + \"CALL_OF\" + POSTFIX;\n+    static {\n+        callOfNodes(CALL_OF, \"Call.*\");\n+    }\n+\n@@ -549,0 +554,86 @@\n+    public static final String G1_COMPARE_AND_EXCHANGE_N_WITH_BARRIER_FLAG = COMPOSITE_PREFIX + \"G1_COMPARE_AND_EXCHANGE_N_WITH_BARRIER_FLAG\" + POSTFIX;\n+    static {\n+        String regex = START + \"g1CompareAndExchangeN\\\\S*\" + MID + \"barrier\\\\(\\\\s*\" + IS_REPLACED + \"\\\\s*\\\\)\" + END;\n+        machOnly(G1_COMPARE_AND_EXCHANGE_N_WITH_BARRIER_FLAG, regex);\n+    }\n+\n+    public static final String G1_COMPARE_AND_EXCHANGE_P_WITH_BARRIER_FLAG = COMPOSITE_PREFIX + \"G1_COMPARE_AND_EXCHANGE_P_WITH_BARRIER_FLAG\" + POSTFIX;\n+    static {\n+        String regex = START + \"g1CompareAndExchangeP\\\\S*\" + MID + \"barrier\\\\(\\\\s*\" + IS_REPLACED + \"\\\\s*\\\\)\" + END;\n+        machOnly(G1_COMPARE_AND_EXCHANGE_P_WITH_BARRIER_FLAG, regex);\n+    }\n+\n+    public static final String G1_COMPARE_AND_SWAP_N_WITH_BARRIER_FLAG = COMPOSITE_PREFIX + \"G1_COMPARE_AND_SWAP_N_WITH_BARRIER_FLAG\" + POSTFIX;\n+    static {\n+        String regex = START + \"g1CompareAndSwapN\\\\S*\" + MID + \"barrier\\\\(\\\\s*\" + IS_REPLACED + \"\\\\s*\\\\)\" + END;\n+        machOnly(G1_COMPARE_AND_SWAP_N_WITH_BARRIER_FLAG, regex);\n+    }\n+\n+    public static final String G1_COMPARE_AND_SWAP_P_WITH_BARRIER_FLAG = COMPOSITE_PREFIX + \"G1_COMPARE_AND_SWAP_P_WITH_BARRIER_FLAG\" + POSTFIX;\n+    static {\n+        String regex = START + \"g1CompareAndSwapP\\\\S*\" + MID + \"barrier\\\\(\\\\s*\" + IS_REPLACED + \"\\\\s*\\\\)\" + END;\n+        machOnly(G1_COMPARE_AND_SWAP_P_WITH_BARRIER_FLAG, regex);\n+    }\n+\n+    public static final String G1_ENCODE_P_AND_STORE_N = PREFIX + \"G1_ENCODE_P_AND_STORE_N\" + POSTFIX;\n+    static {\n+        machOnlyNameRegex(G1_ENCODE_P_AND_STORE_N, \"g1EncodePAndStoreN\");\n+    }\n+\n+    public static final String G1_ENCODE_P_AND_STORE_N_WITH_BARRIER_FLAG = COMPOSITE_PREFIX + \"G1_ENCODE_P_AND_STORE_N_WITH_BARRIER_FLAG\" + POSTFIX;\n+    static {\n+        String regex = START + \"g1EncodePAndStoreN\\\\S*\" + MID + \"barrier\\\\(\\\\s*\" + IS_REPLACED + \"\\\\s*\\\\)\" + END;\n+        machOnly(G1_ENCODE_P_AND_STORE_N_WITH_BARRIER_FLAG, regex);\n+    }\n+\n+    public static final String G1_GET_AND_SET_N_WITH_BARRIER_FLAG = COMPOSITE_PREFIX + \"G1_GET_AND_SET_N_WITH_BARRIER_FLAG\" + POSTFIX;\n+    static {\n+        String regex = START + \"g1XChgN\\\\S*\" + MID + \"barrier\\\\(\\\\s*\" + IS_REPLACED + \"\\\\s*\\\\)\" + END;\n+        machOnly(G1_GET_AND_SET_N_WITH_BARRIER_FLAG, regex);\n+    }\n+\n+    public static final String G1_GET_AND_SET_P_WITH_BARRIER_FLAG = COMPOSITE_PREFIX + \"G1_GET_AND_SET_P_WITH_BARRIER_FLAG\" + POSTFIX;\n+    static {\n+        String regex = START + \"g1XChgP\\\\S*\" + MID + \"barrier\\\\(\\\\s*\" + IS_REPLACED + \"\\\\s*\\\\)\" + END;\n+        machOnly(G1_GET_AND_SET_P_WITH_BARRIER_FLAG, regex);\n+    }\n+\n+    public static final String G1_LOAD_N = PREFIX + \"G1_LOAD_N\" + POSTFIX;\n+    static {\n+        machOnlyNameRegex(G1_LOAD_N, \"g1LoadN\");\n+    }\n+\n+    public static final String G1_LOAD_N_WITH_BARRIER_FLAG = COMPOSITE_PREFIX + \"G1_LOAD_N_WITH_BARRIER_FLAG\" + POSTFIX;\n+    static {\n+        String regex = START + \"g1LoadN\\\\S*\" + MID + \"barrier\\\\(\\\\s*\" + IS_REPLACED + \"\\\\s*\\\\)\" + END;\n+        machOnly(G1_LOAD_N_WITH_BARRIER_FLAG, regex);\n+    }\n+\n+    public static final String G1_LOAD_P_WITH_BARRIER_FLAG = COMPOSITE_PREFIX + \"G1_LOAD_P_WITH_BARRIER_FLAG\" + POSTFIX;\n+    static {\n+        String regex = START + \"g1LoadP\\\\S*\" + MID + \"barrier\\\\(\\\\s*\" + IS_REPLACED + \"\\\\s*\\\\)\" + END;\n+        machOnly(G1_LOAD_P_WITH_BARRIER_FLAG, regex);\n+    }\n+\n+    public static final String G1_STORE_N = PREFIX + \"G1_STORE_N\" + POSTFIX;\n+    static {\n+        machOnlyNameRegex(G1_STORE_N, \"g1StoreN\");\n+    }\n+\n+    public static final String G1_STORE_N_WITH_BARRIER_FLAG = COMPOSITE_PREFIX + \"G1_STORE_N_WITH_BARRIER_FLAG\" + POSTFIX;\n+    static {\n+        String regex = START + \"g1StoreN\\\\S*\" + MID + \"barrier\\\\(\\\\s*\" + IS_REPLACED + \"\\\\s*\\\\)\" + END;\n+        machOnly(G1_STORE_N_WITH_BARRIER_FLAG, regex);\n+    }\n+\n+    public static final String G1_STORE_P = PREFIX + \"G1_STORE_P\" + POSTFIX;\n+    static {\n+        machOnlyNameRegex(G1_STORE_P, \"g1StoreP\");\n+    }\n+\n+    public static final String G1_STORE_P_WITH_BARRIER_FLAG = COMPOSITE_PREFIX + \"G1_STORE_P_WITH_BARRIER_FLAG\" + POSTFIX;\n+    static {\n+        String regex = START + \"g1StoreP\\\\S*\" + MID + \"barrier\\\\(\\\\s*\" + IS_REPLACED + \"\\\\s*\\\\)\" + END;\n+        machOnly(G1_STORE_P_WITH_BARRIER_FLAG, regex);\n+    }\n+\n@@ -820,0 +911,5 @@\n+    public static final String MACH_TEMP = PREFIX + \"MACH_TEMP\" + POSTFIX;\n+    static {\n+        machOnlyNameRegex(MACH_TEMP, \"MachTemp\");\n+    }\n+\n@@ -1106,0 +1202,6 @@\n+    public static final String OOPMAP_WITH = COMPOSITE_PREFIX + \"OOPMAP_WITH\" + POSTFIX;\n+    static {\n+        String regex = \"(#\\\\s*OopMap\\\\s*\\\\{.*\" + IS_REPLACED + \".*\\\\})\";\n+        optoOnly(OOPMAP_WITH, regex);\n+    }\n+\n","filename":"test\/hotspot\/jtreg\/compiler\/lib\/ir_framework\/IRNode.java","additions":102,"deletions":0,"binary":false,"changes":102,"status":"modified"},{"patch":"@@ -0,0 +1,98 @@\n+\/*\n+ * Copyright (c) 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+package compiler.runtime.safepoints;\n+\n+import compiler.lib.ir_framework.*;\n+import java.lang.ref.SoftReference;\n+\n+\/**\n+ * @test\n+ * @summary Test that undefined values generated by MachTemp nodes (in this\n+ *          case, derived from G1 barriers) are not included in OopMaps.\n+ *          Extracted from java.lang.invoke.LambdaFormEditor::getInCache.\n+ * @key randomness\n+ * @library \/test\/lib \/\n+ * @requires vm.gc.G1 & vm.bits == 64 & vm.opt.final.UseCompressedOops == true\n+ * @run driver compiler.runtime.safepoints.TestMachTempsAcrossSafepoints\n+ *\/\n+\n+public class TestMachTempsAcrossSafepoints {\n+\n+    static class RefWithKey extends SoftReference<Object> {\n+        final int key;\n+\n+        public RefWithKey(int key) {\n+            super(new Object());\n+            this.key = key;\n+        }\n+\n+        @DontInline\n+        @Override\n+        public boolean equals(Object obj) {\n+            return obj instanceof RefWithKey that && this.key == that.key;\n+        }\n+    }\n+\n+    public static void main(String[] args) throws Exception {\n+        String inlineCmd = \"-XX:CompileCommand=inline,java.lang.ref.SoftReference::get\";\n+        TestFramework.runWithFlags(inlineCmd, \"-XX:+StressGCM\", \"-XX:+StressLCM\", \"-XX:StressSeed=1\");\n+        TestFramework.runWithFlags(inlineCmd, \"-XX:+StressGCM\", \"-XX:+StressLCM\");\n+    }\n+\n+    @Test\n+    @IR(counts = {IRNode.G1_LOAD_N, \"1\"}, phase = CompilePhase.FINAL_CODE)\n+    @IR(counts = {IRNode.MACH_TEMP, \">= 1\"}, phase = CompilePhase.FINAL_CODE)\n+    @IR(counts = {IRNode.STATIC_CALL_OF_METHOD, \"equals\", \"2\"})\n+    @IR(failOn = {IRNode.OOPMAP_WITH, \"NarrowOop\"})\n+    static private Object test(RefWithKey key, RefWithKey[] refs) {\n+        RefWithKey k = null;\n+        \/\/ This loop causes the register allocator to not \"rematerialize\" all\n+        \/\/ MachTemp nodes generated for the reference g1LoadN instruction below.\n+        for (int i = 0; i < refs.length; i++) {\n+            RefWithKey k0 = refs[0];\n+            if (k0.equals(key)) {\n+                k = k0;\n+            }\n+        }\n+        if (k != null && !key.equals(k)) {\n+            return null;\n+        }\n+        \/\/ The MachTemp node implementing the dst TEMP operand in the g1LoadN\n+        \/\/ instruction corresponding to k.get() can be scheduled across the\n+        \/\/ above call to RefWithKey::equals(), due to an unfortunate interaction\n+        \/\/ of inaccurate basic block frequency estimation (emulated in this test\n+        \/\/ by randomizing the GCM and LCM heuristics) and call-catch cleanup.\n+        \/\/ Since narrow pointer MachTemp nodes are typed as narrow OOPs, this\n+        \/\/ causes the oopmap builder to include the MachTemp node definition in\n+        \/\/ the RefWithKey::equals() return oopmap.\n+        return (k != null) ? k.get() : null;\n+    }\n+\n+    @Run(test = \"test\")\n+    @Warmup(0)\n+    public void run() {\n+        RefWithKey ref = new RefWithKey(42);\n+        test(ref, new RefWithKey[]{ref});\n+    }\n+}\n","filename":"test\/hotspot\/jtreg\/compiler\/runtime\/safepoints\/TestMachTempsAcrossSafepoints.java","additions":98,"deletions":0,"binary":false,"changes":98,"status":"added"},{"patch":"@@ -307,1 +307,4 @@\n-                \"-XX:StressSeed=\" + rng.nextInt(Integer.MAX_VALUE)));\n+                \"-XX:StressSeed=\" + rng.nextInt(Integer.MAX_VALUE),\n+                \/\/ Do not fail on huge methods where StressGCM makes register\n+                \/\/ allocation allocate lots of memory\n+                \"-XX:CompileCommand=memlimit,*.*,0\"));\n","filename":"test\/hotspot\/jtreg\/testlibrary\/ctw\/src\/sun\/hotspot\/tools\/ctw\/CtwRunner.java","additions":4,"deletions":1,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -27,1 +27,1 @@\n- * @run junit\/othervm\/timeout=2500 -XX:+IgnoreUnrecognizedVMOptions -XX:-VerifyDependencies -esa -DBigArityTest.ITERATION_COUNT=1 test.java.lang.invoke.BigArityTest\n+ * @run junit\/othervm\/timeout=2500 -XX:+IgnoreUnrecognizedVMOptions -XX:-VerifyDependencies -XX:CompileCommand=memlimit,*.*,0 -esa -DBigArityTest.ITERATION_COUNT=1 test.java.lang.invoke.BigArityTest\n","filename":"test\/jdk\/java\/lang\/invoke\/BigArityTest.java","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"}]}