{"files":[{"patch":"@@ -165,1 +165,1 @@\n-instruct g1CompareAndExchangeP(iRegPNoSp res, indirect mem, iRegP oldval, iRegP newval, iRegPNoSp tmp1, iRegPNoSp tmp2, iRegPNoSp tmp3)\n+instruct g1CompareAndExchangeP(iRegPNoSp res, indirect mem, iRegP oldval, iRegP newval, iRegPNoSp tmp1, iRegPNoSp tmp2)\n@@ -169,1 +169,1 @@\n-  effect(TEMP res, TEMP tmp1, TEMP tmp2, TEMP tmp3);\n+  effect(TEMP res, TEMP tmp1, TEMP tmp2);\n@@ -182,2 +182,2 @@\n-                      $tmp2$$Register   \/* tmp1 *\/,\n-                      $tmp3$$Register   \/* tmp2 *\/,\n+                      $tmp1$$Register   \/* tmp1 *\/,\n+                      $tmp2$$Register   \/* tmp2 *\/,\n@@ -186,2 +186,0 @@\n-    __ mv($tmp1$$Register, $oldval$$Register);\n-    __ mv($tmp2$$Register, $newval$$Register);\n@@ -191,4 +189,4 @@\n-                       $mem$$Register  \/* store_addr *\/,\n-                       $tmp2$$Register \/* new_val *\/,\n-                       $tmp1$$Register \/* tmp1 *\/,\n-                       $tmp3$$Register \/* tmp2 *\/);\n+                       $mem$$Register    \/* store_addr *\/,\n+                       $newval$$Register \/* new_val *\/,\n+                       $tmp1$$Register   \/* tmp1 *\/,\n+                       $tmp2$$Register   \/* tmp2 *\/);\n@@ -199,1 +197,1 @@\n-instruct g1CompareAndExchangePAcq(iRegPNoSp res, indirect mem, iRegP oldval, iRegP newval, iRegPNoSp tmp1, iRegPNoSp tmp2, iRegPNoSp tmp3)\n+instruct g1CompareAndExchangePAcq(iRegPNoSp res, indirect mem, iRegP oldval, iRegP newval, iRegPNoSp tmp1, iRegPNoSp tmp2)\n@@ -203,1 +201,1 @@\n-  effect(TEMP res, TEMP tmp1, TEMP tmp2, TEMP tmp3);\n+  effect(TEMP res, TEMP tmp1, TEMP tmp2);\n@@ -216,2 +214,2 @@\n-                      $tmp2$$Register   \/* tmp1 *\/,\n-                      $tmp3$$Register   \/* tmp2 *\/,\n+                      $tmp1$$Register   \/* tmp1 *\/,\n+                      $tmp2$$Register   \/* tmp2 *\/,\n@@ -220,2 +218,0 @@\n-    __ mv($tmp1$$Register, $oldval$$Register);\n-    __ mv($tmp2$$Register, $newval$$Register);\n@@ -225,4 +221,4 @@\n-                       $mem$$Register  \/* store_addr *\/,\n-                       $tmp2$$Register \/* new_val *\/,\n-                       $tmp1$$Register \/* tmp1 *\/,\n-                       $tmp3$$Register \/* tmp2 *\/);\n+                       $mem$$Register    \/* store_addr *\/,\n+                       $newval$$Register \/* new_val *\/,\n+                       $tmp1$$Register   \/* tmp1 *\/,\n+                       $tmp2$$Register   \/* tmp2 *\/);\n@@ -251,2 +247,0 @@\n-    __ mv($tmp1$$Register, $oldval$$Register);\n-    __ mv($tmp2$$Register, $newval$$Register);\n@@ -255,1 +249,1 @@\n-    __ decode_heap_oop($tmp2$$Register);\n+    __ decode_heap_oop($tmp1$$Register, $newval$$Register);\n@@ -258,2 +252,2 @@\n-                       $tmp2$$Register \/* new_val *\/,\n-                       $tmp1$$Register \/* tmp1 *\/,\n+                       $tmp1$$Register \/* new_val *\/,\n+                       $tmp2$$Register \/* tmp1 *\/,\n@@ -283,2 +277,0 @@\n-    __ mv($tmp1$$Register, $oldval$$Register);\n-    __ mv($tmp2$$Register, $newval$$Register);\n@@ -287,1 +279,1 @@\n-    __ decode_heap_oop($tmp2$$Register);\n+    __ decode_heap_oop($tmp1$$Register, $newval$$Register);\n@@ -290,2 +282,2 @@\n-                       $tmp2$$Register \/* new_val *\/,\n-                       $tmp1$$Register \/* tmp1 *\/,\n+                       $tmp1$$Register \/* new_val *\/,\n+                       $tmp2$$Register \/* tmp1 *\/,\n@@ -297,1 +289,1 @@\n-instruct g1CompareAndSwapP(iRegINoSp res, indirect mem, iRegP newval, iRegPNoSp tmp1, iRegPNoSp tmp2, iRegPNoSp tmp3, iRegP oldval)\n+instruct g1CompareAndSwapP(iRegINoSp res, indirect mem, iRegP newval, iRegPNoSp tmp1, iRegPNoSp tmp2, iRegP oldval)\n@@ -302,1 +294,1 @@\n-  effect(TEMP res, TEMP tmp1, TEMP tmp2, TEMP tmp3);\n+  effect(TEMP res, TEMP tmp1, TEMP tmp2);\n@@ -313,2 +305,2 @@\n-                      $tmp2$$Register   \/* tmp1 *\/,\n-                      $tmp3$$Register   \/* tmp2 *\/,\n+                      $tmp1$$Register   \/* tmp1 *\/,\n+                      $tmp2$$Register   \/* tmp2 *\/,\n@@ -317,2 +309,0 @@\n-    __ mv($tmp1$$Register, $oldval$$Register);\n-    __ mv($tmp2$$Register, $newval$$Register);\n@@ -323,4 +313,4 @@\n-                       $mem$$Register  \/* store_addr *\/,\n-                       $tmp2$$Register \/* new_val *\/,\n-                       $tmp1$$Register \/* tmp1 *\/,\n-                       $tmp3$$Register \/* tmp2 *\/);\n+                       $mem$$Register    \/* store_addr *\/,\n+                       $newval$$Register \/* new_val *\/,\n+                       $tmp1$$Register   \/* tmp1 *\/,\n+                       $tmp2$$Register   \/* tmp2 *\/);\n@@ -331,1 +321,1 @@\n-instruct g1CompareAndSwapPAcq(iRegINoSp res, indirect mem, iRegP newval, iRegPNoSp tmp1, iRegPNoSp tmp2, iRegPNoSp tmp3, iRegP oldval)\n+instruct g1CompareAndSwapPAcq(iRegINoSp res, indirect mem, iRegP newval, iRegPNoSp tmp1, iRegPNoSp tmp2, iRegP oldval)\n@@ -336,1 +326,1 @@\n-  effect(TEMP res, TEMP tmp1, TEMP tmp2, TEMP tmp3);\n+  effect(TEMP res, TEMP tmp1, TEMP tmp2);\n@@ -347,2 +337,2 @@\n-                      $tmp2$$Register   \/* tmp1 *\/,\n-                      $tmp3$$Register   \/* tmp2 *\/,\n+                      $tmp1$$Register   \/* tmp1 *\/,\n+                      $tmp2$$Register   \/* tmp2 *\/,\n@@ -351,2 +341,0 @@\n-    __ mv($tmp1$$Register, $oldval$$Register);\n-    __ mv($tmp2$$Register, $newval$$Register);\n@@ -357,4 +345,4 @@\n-                       $mem$$Register  \/* store_addr *\/,\n-                       $tmp2$$Register \/* new_val *\/,\n-                       $tmp1$$Register \/* tmp1 *\/,\n-                       $tmp3$$Register \/* tmp2 *\/);\n+                       $mem$$Register    \/* store_addr *\/,\n+                       $newval$$Register \/* new_val *\/,\n+                       $tmp1$$Register   \/* tmp1 *\/,\n+                       $tmp2$$Register   \/* tmp2 *\/);\n@@ -385,2 +373,0 @@\n-    __ mv($tmp1$$Register, $oldval$$Register);\n-    __ mv($tmp2$$Register, $newval$$Register);\n@@ -390,1 +376,1 @@\n-    __ decode_heap_oop($tmp2$$Register);\n+    __ decode_heap_oop($tmp1$$Register, $newval$$Register);\n@@ -393,2 +379,2 @@\n-                       $tmp2$$Register \/* new_val *\/,\n-                       $tmp1$$Register \/* tmp1 *\/,\n+                       $tmp1$$Register \/* new_val *\/,\n+                       $tmp2$$Register \/* tmp1 *\/,\n@@ -420,2 +406,0 @@\n-    __ mv($tmp1$$Register, $oldval$$Register);\n-    __ mv($tmp2$$Register, $newval$$Register);\n@@ -425,1 +409,1 @@\n-    __ decode_heap_oop($tmp2$$Register);\n+    __ decode_heap_oop($tmp1$$Register, $newval$$Register);\n@@ -428,2 +412,2 @@\n-                       $tmp2$$Register \/* new_val *\/,\n-                       $tmp1$$Register \/* tmp1 *\/,\n+                       $tmp1$$Register \/* new_val *\/,\n+                       $tmp2$$Register \/* tmp1 *\/,\n@@ -543,2 +527,0 @@\n-  \/\/ This instruction does not need an acquiring counterpart because it is only\n-  \/\/ used for reference loading (Reference::get()). The same holds for g1LoadN.\n@@ -567,1 +549,1 @@\n-  ins_cost(VOLATILE_REF_COST);\n+  ins_cost(LOAD_COST + BRANCH_COST);\n","filename":"src\/hotspot\/cpu\/riscv\/gc\/g1\/g1_riscv.ad","additions":45,"deletions":63,"binary":false,"changes":108,"status":"modified"},{"patch":"@@ -558,1045 +558,0 @@\n-\n-#if G1_LATE_BARRIER_MIGRATION_SUPPORT\n-\n-const TypeFunc *G1BarrierSetC2Early::write_ref_field_pre_entry_Type() {\n-  const Type **fields = TypeTuple::fields(2);\n-  fields[TypeFunc::Parms+0] = TypeInstPtr::NOTNULL; \/\/ original field value\n-  fields[TypeFunc::Parms+1] = TypeRawPtr::NOTNULL; \/\/ thread\n-  const TypeTuple *domain = TypeTuple::make(TypeFunc::Parms+2, fields);\n-\n-  \/\/ create result type (range)\n-  fields = TypeTuple::fields(0);\n-  const TypeTuple *range = TypeTuple::make(TypeFunc::Parms+0, fields);\n-\n-  return TypeFunc::make(domain, range);\n-}\n-\n-const TypeFunc *G1BarrierSetC2Early::write_ref_field_post_entry_Type() {\n-  const Type **fields = TypeTuple::fields(2);\n-  fields[TypeFunc::Parms+0] = TypeRawPtr::NOTNULL;  \/\/ Card addr\n-  fields[TypeFunc::Parms+1] = TypeRawPtr::NOTNULL;  \/\/ thread\n-  const TypeTuple *domain = TypeTuple::make(TypeFunc::Parms+2, fields);\n-\n-  \/\/ create result type (range)\n-  fields = TypeTuple::fields(0);\n-  const TypeTuple *range = TypeTuple::make(TypeFunc::Parms, fields);\n-\n-  return TypeFunc::make(domain, range);\n-}\n-\n-#define __ ideal.\n-\/*\n- * Determine if the G1 pre-barrier can be removed. The pre-barrier is\n- * required by SATB to make sure all objects live at the start of the\n- * marking are kept alive, all reference updates need to any previous\n- * reference stored before writing.\n- *\n- * If the previous value is null there is no need to save the old value.\n- * References that are null are filtered during runtime by the barrier\n- * code to avoid unnecessary queuing.\n- *\n- * However in the case of newly allocated objects it might be possible to\n- * prove that the reference about to be overwritten is null during compile\n- * time and avoid adding the barrier code completely.\n- *\n- * The compiler needs to determine that the object in which a field is about\n- * to be written is newly allocated, and that no prior store to the same field\n- * has happened since the allocation.\n- *\n- * Returns true if the pre-barrier can be removed\n- *\/\n-bool G1BarrierSetC2Early::g1_can_remove_pre_barrier(GraphKit* kit,\n-                                                    PhaseValues* phase,\n-                                                    Node* adr,\n-                                                    BasicType bt,\n-                                                    uint adr_idx) const {\n-  intptr_t offset = 0;\n-  Node* base = AddPNode::Ideal_base_and_offset(adr, phase, offset);\n-  AllocateNode* alloc = AllocateNode::Ideal_allocation(base);\n-\n-  if (offset == Type::OffsetBot) {\n-    return false; \/\/ cannot unalias unless there are precise offsets\n-  }\n-\n-  if (alloc == nullptr) {\n-    return false; \/\/ No allocation found\n-  }\n-\n-  intptr_t size_in_bytes = type2aelembytes(bt);\n-\n-  Node* mem = kit->memory(adr_idx); \/\/ start searching here...\n-\n-  for (int cnt = 0; cnt < 50; cnt++) {\n-\n-    if (mem->is_Store()) {\n-\n-      Node* st_adr = mem->in(MemNode::Address);\n-      intptr_t st_offset = 0;\n-      Node* st_base = AddPNode::Ideal_base_and_offset(st_adr, phase, st_offset);\n-\n-      if (st_base == nullptr) {\n-        break; \/\/ inscrutable pointer\n-      }\n-\n-      \/\/ Break we have found a store with same base and offset as ours so break\n-      if (st_base == base && st_offset == offset) {\n-        break;\n-      }\n-\n-      if (st_offset != offset && st_offset != Type::OffsetBot) {\n-        const int MAX_STORE = BytesPerLong;\n-        if (st_offset >= offset + size_in_bytes ||\n-            st_offset <= offset - MAX_STORE ||\n-            st_offset <= offset - mem->as_Store()->memory_size()) {\n-          \/\/ Success:  The offsets are provably independent.\n-          \/\/ (You may ask, why not just test st_offset != offset and be done?\n-          \/\/ The answer is that stores of different sizes can co-exist\n-          \/\/ in the same sequence of RawMem effects.  We sometimes initialize\n-          \/\/ a whole 'tile' of array elements with a single jint or jlong.)\n-          mem = mem->in(MemNode::Memory);\n-          continue; \/\/ advance through independent store memory\n-        }\n-      }\n-\n-      if (st_base != base\n-          && MemNode::detect_ptr_independence(base, alloc, st_base,\n-                                              AllocateNode::Ideal_allocation(st_base),\n-                                              phase)) {\n-        \/\/ Success:  The bases are provably independent.\n-        mem = mem->in(MemNode::Memory);\n-        continue; \/\/ advance through independent store memory\n-      }\n-    } else if (mem->is_Proj() && mem->in(0)->is_Initialize()) {\n-\n-      InitializeNode* st_init = mem->in(0)->as_Initialize();\n-      AllocateNode* st_alloc = st_init->allocation();\n-\n-      \/\/ Make sure that we are looking at the same allocation site.\n-      \/\/ The alloc variable is guaranteed to not be null here from earlier check.\n-      if (alloc == st_alloc) {\n-        \/\/ Check that the initialization is storing null so that no previous store\n-        \/\/ has been moved up and directly write a reference\n-        Node* captured_store = st_init->find_captured_store(offset,\n-                                                            type2aelembytes(T_OBJECT),\n-                                                            phase);\n-        if (captured_store == nullptr || captured_store == st_init->zero_memory()) {\n-          return true;\n-        }\n-      }\n-    }\n-\n-    \/\/ Unless there is an explicit 'continue', we must bail out here,\n-    \/\/ because 'mem' is an inscrutable memory state (e.g., a call).\n-    break;\n-  }\n-\n-  return false;\n-}\n-\n-\/\/ G1 pre\/post barriers\n-void G1BarrierSetC2Early::pre_barrier(GraphKit* kit,\n-                                      bool do_load,\n-                                      Node* ctl,\n-                                      Node* obj,\n-                                      Node* adr,\n-                                      uint alias_idx,\n-                                      Node* val,\n-                                      const TypeOopPtr* val_type,\n-                                      Node* pre_val,\n-                                      BasicType bt) const {\n-  \/\/ Some sanity checks\n-  \/\/ Note: val is unused in this routine.\n-\n-  if (do_load) {\n-    \/\/ We need to generate the load of the previous value\n-    assert(obj != nullptr, \"must have a base\");\n-    assert(adr != nullptr, \"where are loading from?\");\n-    assert(pre_val == nullptr, \"loaded already?\");\n-    assert(val_type != nullptr, \"need a type\");\n-\n-    if (use_ReduceInitialCardMarks()\n-        && g1_can_remove_pre_barrier(kit, &kit->gvn(), adr, bt, alias_idx)) {\n-      return;\n-    }\n-\n-  } else {\n-    \/\/ In this case both val_type and alias_idx are unused.\n-    assert(pre_val != nullptr, \"must be loaded already\");\n-    \/\/ Nothing to be done if pre_val is null.\n-    if (pre_val->bottom_type() == TypePtr::NULL_PTR) return;\n-    assert(pre_val->bottom_type()->basic_type() == T_OBJECT, \"or we shouldn't be here\");\n-  }\n-  assert(bt == T_OBJECT, \"or we shouldn't be here\");\n-\n-  IdealKit ideal(kit, true);\n-\n-  Node* tls = __ thread(); \/\/ ThreadLocalStorage\n-\n-  Node* no_base = __ top();\n-  Node* zero  = __ ConI(0);\n-  Node* zeroX = __ ConX(0);\n-\n-  float likely  = PROB_LIKELY(0.999);\n-  float unlikely  = PROB_UNLIKELY(0.999);\n-\n-  BasicType active_type = in_bytes(SATBMarkQueue::byte_width_of_active()) == 4 ? T_INT : T_BYTE;\n-  assert(in_bytes(SATBMarkQueue::byte_width_of_active()) == 4 || in_bytes(SATBMarkQueue::byte_width_of_active()) == 1, \"flag width\");\n-\n-  \/\/ Offsets into the thread\n-  const int marking_offset = in_bytes(G1ThreadLocalData::satb_mark_queue_active_offset());\n-  const int index_offset   = in_bytes(G1ThreadLocalData::satb_mark_queue_index_offset());\n-  const int buffer_offset  = in_bytes(G1ThreadLocalData::satb_mark_queue_buffer_offset());\n-\n-  \/\/ Now the actual pointers into the thread\n-  Node* marking_adr = __ AddP(no_base, tls, __ ConX(marking_offset));\n-  Node* buffer_adr  = __ AddP(no_base, tls, __ ConX(buffer_offset));\n-  Node* index_adr   = __ AddP(no_base, tls, __ ConX(index_offset));\n-\n-  \/\/ Now some of the values\n-  Node* marking = __ load(__ ctrl(), marking_adr, TypeInt::INT, active_type, Compile::AliasIdxRaw);\n-\n-  \/\/ if (!marking)\n-  __ if_then(marking, BoolTest::ne, zero, unlikely); {\n-    BasicType index_bt = TypeX_X->basic_type();\n-    assert(sizeof(size_t) == type2aelembytes(index_bt), \"Loading G1 SATBMarkQueue::_index with wrong size.\");\n-    Node* index   = __ load(__ ctrl(), index_adr, TypeX_X, index_bt, Compile::AliasIdxRaw);\n-\n-    if (do_load) {\n-      \/\/ load original value\n-      pre_val = __ load(__ ctrl(), adr, val_type, bt, alias_idx, false, MemNode::unordered, LoadNode::Pinned);\n-    }\n-\n-    \/\/ if (pre_val != nullptr)\n-    __ if_then(pre_val, BoolTest::ne, kit->null()); {\n-      Node* buffer  = __ load(__ ctrl(), buffer_adr, TypeRawPtr::NOTNULL, T_ADDRESS, Compile::AliasIdxRaw);\n-\n-      \/\/ is the queue for this thread full?\n-      __ if_then(index, BoolTest::ne, zeroX, likely); {\n-\n-        \/\/ decrement the index\n-        Node* next_index = kit->gvn().transform(new SubXNode(index, __ ConX(sizeof(intptr_t))));\n-\n-        \/\/ Now get the buffer location we will log the previous value into and store it\n-        Node *log_addr = __ AddP(no_base, buffer, next_index);\n-        __ store(__ ctrl(), log_addr, pre_val, T_OBJECT, Compile::AliasIdxRaw, MemNode::unordered);\n-        \/\/ update the index\n-        __ store(__ ctrl(), index_adr, next_index, index_bt, Compile::AliasIdxRaw, MemNode::unordered);\n-\n-      } __ else_(); {\n-\n-        \/\/ logging buffer is full, call the runtime\n-        const TypeFunc *tf = write_ref_field_pre_entry_Type();\n-        __ make_leaf_call(tf, CAST_FROM_FN_PTR(address, G1BarrierSetRuntime::write_ref_field_pre_entry), \"write_ref_field_pre_entry\", pre_val, tls);\n-      } __ end_if();  \/\/ (!index)\n-    } __ end_if();  \/\/ (pre_val != nullptr)\n-  } __ end_if();  \/\/ (!marking)\n-\n-  \/\/ Final sync IdealKit and GraphKit.\n-  kit->final_sync(ideal);\n-}\n-\n-\/*\n- * G1 similar to any GC with a Young Generation requires a way to keep track of\n- * references from Old Generation to Young Generation to make sure all live\n- * objects are found. G1 also requires to keep track of object references\n- * between different regions to enable evacuation of old regions, which is done\n- * as part of mixed collections. References are tracked in remembered sets and\n- * is continuously updated as reference are written to with the help of the\n- * post-barrier.\n- *\n- * To reduce the number of updates to the remembered set the post-barrier\n- * filters updates to fields in objects located in the Young Generation,\n- * the same region as the reference, when the null is being written or\n- * if the card is already marked as dirty by an earlier write.\n- *\n- * Under certain circumstances it is possible to avoid generating the\n- * post-barrier completely if it is possible during compile time to prove\n- * the object is newly allocated and that no safepoint exists between the\n- * allocation and the store.\n- *\n- * In the case of slow allocation the allocation code must handle the barrier\n- * as part of the allocation in the case the allocated object is not located\n- * in the nursery; this would happen for humongous objects.\n- *\n- * Returns true if the post barrier can be removed\n- *\/\n-bool G1BarrierSetC2Early::g1_can_remove_post_barrier(GraphKit* kit,\n-                                                     PhaseValues* phase, Node* store,\n-                                                     Node* adr) const {\n-  intptr_t      offset = 0;\n-  Node*         base   = AddPNode::Ideal_base_and_offset(adr, phase, offset);\n-  AllocateNode* alloc  = AllocateNode::Ideal_allocation(base);\n-\n-  if (offset == Type::OffsetBot) {\n-    return false; \/\/ cannot unalias unless there are precise offsets\n-  }\n-\n-  if (alloc == nullptr) {\n-     return false; \/\/ No allocation found\n-  }\n-\n-  \/\/ Start search from Store node\n-  Node* mem = store->in(MemNode::Control);\n-  if (mem->is_Proj() && mem->in(0)->is_Initialize()) {\n-\n-    InitializeNode* st_init = mem->in(0)->as_Initialize();\n-    AllocateNode*  st_alloc = st_init->allocation();\n-\n-    \/\/ Make sure we are looking at the same allocation\n-    if (alloc == st_alloc) {\n-      return true;\n-    }\n-  }\n-\n-  return false;\n-}\n-\n-\/\/\n-\/\/ Update the card table and add card address to the queue\n-\/\/\n-void G1BarrierSetC2Early::g1_mark_card(GraphKit* kit,\n-                                       IdealKit& ideal,\n-                                       Node* card_adr,\n-                                       Node* oop_store,\n-                                       uint oop_alias_idx,\n-                                       Node* index,\n-                                       Node* index_adr,\n-                                       Node* buffer,\n-                                       const TypeFunc* tf) const {\n-  Node* zero  = __ ConI(0);\n-  Node* zeroX = __ ConX(0);\n-  Node* no_base = __ top();\n-  BasicType card_bt = T_BYTE;\n-  \/\/ Smash zero into card. MUST BE ORDERED WRT TO STORE\n-  __ storeCM(__ ctrl(), card_adr, zero, oop_store, oop_alias_idx, card_bt, Compile::AliasIdxRaw);\n-\n-  \/\/  Now do the queue work\n-  __ if_then(index, BoolTest::ne, zeroX); {\n-\n-    Node* next_index = kit->gvn().transform(new SubXNode(index, __ ConX(sizeof(intptr_t))));\n-    Node* log_addr = __ AddP(no_base, buffer, next_index);\n-\n-    \/\/ Order, see storeCM.\n-    __ store(__ ctrl(), log_addr, card_adr, T_ADDRESS, Compile::AliasIdxRaw, MemNode::unordered);\n-    __ store(__ ctrl(), index_adr, next_index, TypeX_X->basic_type(), Compile::AliasIdxRaw, MemNode::unordered);\n-\n-  } __ else_(); {\n-    __ make_leaf_call(tf, CAST_FROM_FN_PTR(address, G1BarrierSetRuntime::write_ref_field_post_entry), \"write_ref_field_post_entry\", card_adr, __ thread());\n-  } __ end_if();\n-\n-}\n-\n-void G1BarrierSetC2Early::post_barrier(GraphKit* kit,\n-                                       Node* ctl,\n-                                       Node* oop_store,\n-                                       Node* obj,\n-                                       Node* adr,\n-                                       uint alias_idx,\n-                                       Node* val,\n-                                       BasicType bt,\n-                                       bool use_precise) const {\n-  \/\/ If we are writing a null then we need no post barrier\n-\n-  if (val != nullptr && val->is_Con() && val->bottom_type() == TypePtr::NULL_PTR) {\n-    \/\/ Must be null\n-    const Type* t = val->bottom_type();\n-    assert(t == Type::TOP || t == TypePtr::NULL_PTR, \"must be null\");\n-    \/\/ No post barrier if writing null\n-    return;\n-  }\n-\n-  if (use_ReduceInitialCardMarks() && obj == kit->just_allocated_object(kit->control())) {\n-    \/\/ We can skip marks on a freshly-allocated object in Eden.\n-    \/\/ Keep this code in sync with CardTableBarrierSet::on_slowpath_allocation_exit.\n-    \/\/ That routine informs GC to take appropriate compensating steps,\n-    \/\/ upon a slow-path allocation, so as to make this card-mark\n-    \/\/ elision safe.\n-    return;\n-  }\n-\n-  if (use_ReduceInitialCardMarks()\n-      && g1_can_remove_post_barrier(kit, &kit->gvn(), oop_store, adr)) {\n-    return;\n-  }\n-\n-  if (!use_precise) {\n-    \/\/ All card marks for a (non-array) instance are in one place:\n-    adr = obj;\n-  }\n-  \/\/ (Else it's an array (or unknown), and we want more precise card marks.)\n-  assert(adr != nullptr, \"\");\n-\n-  IdealKit ideal(kit, true);\n-\n-  Node* tls = __ thread(); \/\/ ThreadLocalStorage\n-\n-  Node* no_base = __ top();\n-  float likely = PROB_LIKELY_MAG(3);\n-  float unlikely = PROB_UNLIKELY_MAG(3);\n-  Node* young_card = __ ConI((jint)G1CardTable::g1_young_card_val());\n-  Node* dirty_card = __ ConI((jint)G1CardTable::dirty_card_val());\n-  Node* zeroX = __ ConX(0);\n-\n-  const TypeFunc *tf = write_ref_field_post_entry_Type();\n-\n-  \/\/ Offsets into the thread\n-  const int index_offset  = in_bytes(G1ThreadLocalData::dirty_card_queue_index_offset());\n-  const int buffer_offset = in_bytes(G1ThreadLocalData::dirty_card_queue_buffer_offset());\n-\n-  \/\/ Pointers into the thread\n-\n-  Node* buffer_adr = __ AddP(no_base, tls, __ ConX(buffer_offset));\n-  Node* index_adr =  __ AddP(no_base, tls, __ ConX(index_offset));\n-\n-  \/\/ Now some values\n-  \/\/ Use ctrl to avoid hoisting these values past a safepoint, which could\n-  \/\/ potentially reset these fields in the JavaThread.\n-  Node* index  = __ load(__ ctrl(), index_adr, TypeX_X, TypeX_X->basic_type(), Compile::AliasIdxRaw);\n-  Node* buffer = __ load(__ ctrl(), buffer_adr, TypeRawPtr::NOTNULL, T_ADDRESS, Compile::AliasIdxRaw);\n-\n-  \/\/ Convert the store obj pointer to an int prior to doing math on it\n-  \/\/ Must use ctrl to prevent \"integerized oop\" existing across safepoint\n-  Node* cast =  __ CastPX(__ ctrl(), adr);\n-\n-  \/\/ Divide pointer by card size\n-  Node* card_offset = __ URShiftX( cast, __ ConI(CardTable::card_shift()) );\n-\n-  \/\/ Combine card table base and card offset\n-  Node* card_adr = __ AddP(no_base, byte_map_base_node(kit), card_offset );\n-\n-  \/\/ If we know the value being stored does it cross regions?\n-\n-  if (val != nullptr) {\n-    \/\/ Does the store cause us to cross regions?\n-\n-    \/\/ Should be able to do an unsigned compare of region_size instead of\n-    \/\/ and extra shift. Do we have an unsigned compare??\n-    \/\/ Node* region_size = __ ConI(1 << G1HeapRegion::LogOfHRGrainBytes);\n-    Node* xor_res =  __ URShiftX ( __ XorX( cast,  __ CastPX(__ ctrl(), val)), __ ConI(checked_cast<jint>(G1HeapRegion::LogOfHRGrainBytes)));\n-\n-    \/\/ if (xor_res == 0) same region so skip\n-    __ if_then(xor_res, BoolTest::ne, zeroX, likely); {\n-\n-      \/\/ No barrier if we are storing a null.\n-      __ if_then(val, BoolTest::ne, kit->null(), likely); {\n-\n-        \/\/ Ok must mark the card if not already dirty\n-\n-        \/\/ load the original value of the card\n-        Node* card_val = __ load(__ ctrl(), card_adr, TypeInt::INT, T_BYTE, Compile::AliasIdxRaw);\n-\n-        __ if_then(card_val, BoolTest::ne, young_card, unlikely); {\n-          kit->sync_kit(ideal);\n-          kit->insert_mem_bar(Op_MemBarVolatile, oop_store);\n-          __ sync_kit(kit);\n-\n-          Node* card_val_reload = __ load(__ ctrl(), card_adr, TypeInt::INT, T_BYTE, Compile::AliasIdxRaw);\n-          __ if_then(card_val_reload, BoolTest::ne, dirty_card); {\n-            g1_mark_card(kit, ideal, card_adr, oop_store, alias_idx, index, index_adr, buffer, tf);\n-          } __ end_if();\n-        } __ end_if();\n-      } __ end_if();\n-    } __ end_if();\n-  } else {\n-    \/\/ The Object.clone() intrinsic uses this path if !ReduceInitialCardMarks.\n-    \/\/ We don't need a barrier here if the destination is a newly allocated object\n-    \/\/ in Eden. Otherwise, GC verification breaks because we assume that cards in Eden\n-    \/\/ are set to 'g1_young_gen' (see G1CardTable::verify_g1_young_region()).\n-    assert(!use_ReduceInitialCardMarks(), \"can only happen with card marking\");\n-    Node* card_val = __ load(__ ctrl(), card_adr, TypeInt::INT, T_BYTE, Compile::AliasIdxRaw);\n-    __ if_then(card_val, BoolTest::ne, young_card); {\n-      g1_mark_card(kit, ideal, card_adr, oop_store, alias_idx, index, index_adr, buffer, tf);\n-    } __ end_if();\n-  }\n-\n-  \/\/ Final sync IdealKit and GraphKit.\n-  kit->final_sync(ideal);\n-}\n-\n-\/\/ Helper that guards and inserts a pre-barrier.\n-void G1BarrierSetC2Early::insert_pre_barrier(GraphKit* kit, Node* base_oop, Node* offset,\n-                                             Node* pre_val, bool need_mem_bar) const {\n-  \/\/ We could be accessing the referent field of a reference object. If so, when G1\n-  \/\/ is enabled, we need to log the value in the referent field in an SATB buffer.\n-  \/\/ This routine performs some compile time filters and generates suitable\n-  \/\/ runtime filters that guard the pre-barrier code.\n-  \/\/ Also add memory barrier for non volatile load from the referent field\n-  \/\/ to prevent commoning of loads across safepoint.\n-\n-  \/\/ Some compile time checks.\n-\n-  \/\/ If offset is a constant, is it java_lang_ref_Reference::_reference_offset?\n-  const TypeX* otype = offset->find_intptr_t_type();\n-  if (otype != nullptr && otype->is_con() &&\n-      otype->get_con() != java_lang_ref_Reference::referent_offset()) {\n-    \/\/ Constant offset but not the reference_offset so just return\n-    return;\n-  }\n-\n-  \/\/ We only need to generate the runtime guards for instances.\n-  const TypeOopPtr* btype = base_oop->bottom_type()->isa_oopptr();\n-  if (btype != nullptr) {\n-    if (btype->isa_aryptr()) {\n-      \/\/ Array type so nothing to do\n-      return;\n-    }\n-\n-    const TypeInstPtr* itype = btype->isa_instptr();\n-    if (itype != nullptr) {\n-      \/\/ Can the klass of base_oop be statically determined to be\n-      \/\/ _not_ a sub-class of Reference and _not_ Object?\n-      ciKlass* klass = itype->instance_klass();\n-      if (klass->is_loaded() &&\n-          !klass->is_subtype_of(kit->env()->Reference_klass()) &&\n-          !kit->env()->Object_klass()->is_subtype_of(klass)) {\n-        return;\n-      }\n-    }\n-  }\n-\n-  \/\/ The compile time filters did not reject base_oop\/offset so\n-  \/\/ we need to generate the following runtime filters\n-  \/\/\n-  \/\/ if (offset == java_lang_ref_Reference::_reference_offset) {\n-  \/\/   if (instance_of(base, java.lang.ref.Reference)) {\n-  \/\/     pre_barrier(_, pre_val, ...);\n-  \/\/   }\n-  \/\/ }\n-\n-  float likely   = PROB_LIKELY(  0.999);\n-  float unlikely = PROB_UNLIKELY(0.999);\n-\n-  IdealKit ideal(kit);\n-\n-  Node* referent_off = __ ConX(java_lang_ref_Reference::referent_offset());\n-\n-  __ if_then(offset, BoolTest::eq, referent_off, unlikely); {\n-      \/\/ Update graphKit memory and control from IdealKit.\n-      kit->sync_kit(ideal);\n-\n-      Node* ref_klass_con = kit->makecon(TypeKlassPtr::make(kit->env()->Reference_klass()));\n-      Node* is_instof = kit->gen_instanceof(base_oop, ref_klass_con);\n-\n-      \/\/ Update IdealKit memory and control from graphKit.\n-      __ sync_kit(kit);\n-\n-      Node* one = __ ConI(1);\n-      \/\/ is_instof == 0 if base_oop == nullptr\n-      __ if_then(is_instof, BoolTest::eq, one, unlikely); {\n-\n-        \/\/ Update graphKit from IdeakKit.\n-        kit->sync_kit(ideal);\n-\n-        \/\/ Use the pre-barrier to record the value in the referent field\n-        pre_barrier(kit, false \/* do_load *\/,\n-                    __ ctrl(),\n-                    nullptr \/* obj *\/, nullptr \/* adr *\/, max_juint \/* alias_idx *\/, nullptr \/* val *\/, nullptr \/* val_type *\/,\n-                    pre_val \/* pre_val *\/,\n-                    T_OBJECT);\n-        if (need_mem_bar) {\n-          \/\/ Add memory barrier to prevent commoning reads from this field\n-          \/\/ across safepoint since GC can change its value.\n-          kit->insert_mem_bar(Op_MemBarCPUOrder);\n-        }\n-        \/\/ Update IdealKit from graphKit.\n-        __ sync_kit(kit);\n-\n-      } __ end_if(); \/\/ _ref_type != ref_none\n-  } __ end_if(); \/\/ offset == referent_offset\n-\n-  \/\/ Final sync IdealKit and GraphKit.\n-  kit->final_sync(ideal);\n-}\n-\n-#undef __\n-\n-Node* G1BarrierSetC2Early::load_at_resolved(C2Access& access, const Type* val_type) const {\n-  DecoratorSet decorators = access.decorators();\n-  Node* adr = access.addr().node();\n-  Node* obj = access.base();\n-\n-  bool anonymous = (decorators & C2_UNSAFE_ACCESS) != 0;\n-  bool mismatched = (decorators & C2_MISMATCHED) != 0;\n-  bool unknown = (decorators & ON_UNKNOWN_OOP_REF) != 0;\n-  bool in_heap = (decorators & IN_HEAP) != 0;\n-  bool in_native = (decorators & IN_NATIVE) != 0;\n-  bool on_weak = (decorators & ON_WEAK_OOP_REF) != 0;\n-  bool on_phantom = (decorators & ON_PHANTOM_OOP_REF) != 0;\n-  bool is_unordered = (decorators & MO_UNORDERED) != 0;\n-  bool no_keepalive = (decorators & AS_NO_KEEPALIVE) != 0;\n-  bool is_mixed = !in_heap && !in_native;\n-  bool need_cpu_mem_bar = !is_unordered || mismatched || is_mixed;\n-\n-  Node* top = Compile::current()->top();\n-  Node* offset = adr->is_AddP() ? adr->in(AddPNode::Offset) : top;\n-\n-  \/\/ If we are reading the value of the referent field of a Reference\n-  \/\/ object (either by using Unsafe directly or through reflection)\n-  \/\/ then, if G1 is enabled, we need to record the referent in an\n-  \/\/ SATB log buffer using the pre-barrier mechanism.\n-  \/\/ Also we need to add memory barrier to prevent commoning reads\n-  \/\/ from this field across safepoint since GC can change its value.\n-  bool need_read_barrier = (((on_weak || on_phantom) && !no_keepalive) ||\n-                            (in_heap && unknown && offset != top && obj != top));\n-\n-  if (!access.is_oop() || !need_read_barrier) {\n-    return CardTableBarrierSetC2::load_at_resolved(access, val_type);\n-  }\n-\n-  assert(access.is_parse_access(), \"entry not supported at optimization time\");\n-\n-  C2ParseAccess& parse_access = static_cast<C2ParseAccess&>(access);\n-  GraphKit* kit = parse_access.kit();\n-  Node* load;\n-\n-  Node* control =  kit->control();\n-  const TypePtr* adr_type = access.addr().type();\n-  MemNode::MemOrd mo = access.mem_node_mo();\n-  bool requires_atomic_access = (decorators & MO_UNORDERED) == 0;\n-  bool unaligned = (decorators & C2_UNALIGNED) != 0;\n-  bool unsafe = (decorators & C2_UNSAFE_ACCESS) != 0;\n-  \/\/ Pinned control dependency is the strictest. So it's ok to substitute it for any other.\n-  load = kit->make_load(control, adr, val_type, access.type(), adr_type, mo,\n-      LoadNode::Pinned, requires_atomic_access, unaligned, mismatched, unsafe,\n-      access.barrier_data());\n-\n-\n-  if (on_weak || on_phantom) {\n-    \/\/ Use the pre-barrier to record the value in the referent field\n-    pre_barrier(kit, false \/* do_load *\/,\n-                kit->control(),\n-                nullptr \/* obj *\/, nullptr \/* adr *\/, max_juint \/* alias_idx *\/, nullptr \/* val *\/, nullptr \/* val_type *\/,\n-                load \/* pre_val *\/, T_OBJECT);\n-    \/\/ Add memory barrier to prevent commoning reads from this field\n-    \/\/ across safepoint since GC can change its value.\n-    kit->insert_mem_bar(Op_MemBarCPUOrder);\n-  } else if (unknown) {\n-    \/\/ We do not require a mem bar inside pre_barrier if need_mem_bar\n-    \/\/ is set: the barriers would be emitted by us.\n-    insert_pre_barrier(kit, obj, offset, load, !need_cpu_mem_bar);\n-  }\n-\n-  return load;\n-}\n-\n-bool G1BarrierSetC2Early::is_gc_barrier_node(Node* node) const {\n-  if (node->Opcode() == Op_StoreCM) {\n-    return true;\n-  }\n-  if (node->Opcode() != Op_CallLeaf) {\n-    return false;\n-  }\n-  CallLeafNode *call = node->as_CallLeaf();\n-  if (call->_name == nullptr) {\n-    return false;\n-  }\n-\n-  return strcmp(call->_name, \"write_ref_field_pre_entry\") == 0 || strcmp(call->_name, \"write_ref_field_post_entry\") == 0;\n-}\n-\n-bool G1BarrierSetC2Early::is_g1_pre_val_load(Node* n) {\n-  if (n->is_Load() && n->as_Load()->has_pinned_control_dependency()) {\n-    \/\/ Make sure the only users of it are: CmpP, StoreP, and a call to write_ref_field_pre_entry\n-\n-    \/\/ Skip possible decode\n-    if (n->outcnt() == 1 && n->unique_out()->is_DecodeN()) {\n-      n = n->unique_out();\n-    }\n-\n-    if (n->outcnt() == 3) {\n-      int found = 0;\n-      for (SimpleDUIterator iter(n); iter.has_next(); iter.next()) {\n-        Node* use = iter.get();\n-        if (use->is_Cmp() || use->is_Store()) {\n-          ++found;\n-        } else if (use->is_CallLeaf()) {\n-          CallLeafNode* call = use->as_CallLeaf();\n-          if (strcmp(call->_name, \"write_ref_field_pre_entry\") == 0) {\n-            ++found;\n-          }\n-        }\n-      }\n-      if (found == 3) {\n-        return true;\n-      }\n-    }\n-  }\n-  return false;\n-}\n-\n-void G1BarrierSetC2Early::clone(GraphKit* kit, Node* src, Node* dst, Node* size, bool is_array) const {\n-  BarrierSetC2::clone(kit, src, dst, size, is_array);\n-  const TypePtr* raw_adr_type = TypeRawPtr::BOTTOM;\n-\n-  \/\/ If necessary, emit some card marks afterwards.  (Non-arrays only.)\n-  bool card_mark = !is_array && !use_ReduceInitialCardMarks();\n-  if (card_mark) {\n-    assert(!is_array, \"\");\n-    \/\/ Put in store barrier for any and all oops we are sticking\n-    \/\/ into this object.  (We could avoid this if we could prove\n-    \/\/ that the object type contains no oop fields at all.)\n-    Node* no_particular_value = nullptr;\n-    Node* no_particular_field = nullptr;\n-    int raw_adr_idx = Compile::AliasIdxRaw;\n-    post_barrier(kit, kit->control(),\n-                 kit->memory(raw_adr_type),\n-                 dst,\n-                 no_particular_field,\n-                 raw_adr_idx,\n-                 no_particular_value,\n-                 T_OBJECT,\n-                 false);\n-  }\n-}\n-\n-bool G1BarrierSetC2Early::is_gc_pre_barrier_node(Node *node) const {\n-  return is_g1_pre_val_load(node);\n-}\n-\n-void G1BarrierSetC2Early::eliminate_gc_barrier(PhaseMacroExpand* macro, Node* node) const {\n-  if (is_g1_pre_val_load(node)) {\n-    macro->replace_node(node, macro->zerocon(node->as_Load()->bottom_type()->basic_type()));\n-  } else {\n-    assert(node->Opcode() == Op_CastP2X, \"ConvP2XNode required\");\n-    assert(node->outcnt() <= 2, \"expects 1 or 2 users: Xor and URShift nodes\");\n-    \/\/ It could be only one user, URShift node, in Object.clone() intrinsic\n-    \/\/ but the new allocation is passed to arraycopy stub and it could not\n-    \/\/ be scalar replaced. So we don't check the case.\n-\n-    \/\/ An other case of only one user (Xor) is when the value check for null\n-    \/\/ in G1 post barrier is folded after CCP so the code which used URShift\n-    \/\/ is removed.\n-\n-    \/\/ Take Region node before eliminating post barrier since it also\n-    \/\/ eliminates CastP2X node when it has only one user.\n-    Node* this_region = node->in(0);\n-    assert(this_region != nullptr, \"\");\n-\n-    \/\/ Remove G1 post barrier.\n-\n-    \/\/ Search for CastP2X->Xor->URShift->Cmp path which\n-    \/\/ checks if the store done to a different from the value's region.\n-    \/\/ And replace Cmp with #0 (false) to collapse G1 post barrier.\n-    Node* xorx = node->find_out_with(Op_XorX);\n-    if (xorx != nullptr) {\n-      Node* shift = xorx->unique_out();\n-      Node* cmpx = shift->unique_out();\n-      assert(cmpx->is_Cmp() && cmpx->unique_out()->is_Bool() &&\n-          cmpx->unique_out()->as_Bool()->_test._test == BoolTest::ne,\n-          \"missing region check in G1 post barrier\");\n-      macro->replace_node(cmpx, macro->makecon(TypeInt::CC_EQ));\n-\n-      \/\/ Remove G1 pre barrier.\n-\n-      \/\/ Search \"if (marking != 0)\" check and set it to \"false\".\n-      \/\/ There is no G1 pre barrier if previous stored value is null\n-      \/\/ (for example, after initialization).\n-      if (this_region->is_Region() && this_region->req() == 3) {\n-        int ind = 1;\n-        if (!this_region->in(ind)->is_IfFalse()) {\n-          ind = 2;\n-        }\n-        if (this_region->in(ind)->is_IfFalse() &&\n-            this_region->in(ind)->in(0)->Opcode() == Op_If) {\n-          Node* bol = this_region->in(ind)->in(0)->in(1);\n-          assert(bol->is_Bool(), \"\");\n-          cmpx = bol->in(1);\n-          if (bol->as_Bool()->_test._test == BoolTest::ne &&\n-              cmpx->is_Cmp() && cmpx->in(2) == macro->intcon(0) &&\n-              cmpx->in(1)->is_Load()) {\n-            Node* adr = cmpx->in(1)->as_Load()->in(MemNode::Address);\n-            const int marking_offset = in_bytes(G1ThreadLocalData::satb_mark_queue_active_offset());\n-            if (adr->is_AddP() && adr->in(AddPNode::Base) == macro->top() &&\n-                adr->in(AddPNode::Address)->Opcode() == Op_ThreadLocal &&\n-                adr->in(AddPNode::Offset) == macro->MakeConX(marking_offset)) {\n-              macro->replace_node(cmpx, macro->makecon(TypeInt::CC_EQ));\n-            }\n-          }\n-        }\n-      }\n-    } else {\n-      assert(!use_ReduceInitialCardMarks(), \"can only happen with card marking\");\n-      \/\/ This is a G1 post barrier emitted by the Object.clone() intrinsic.\n-      \/\/ Search for the CastP2X->URShiftX->AddP->LoadB->Cmp path which checks if the card\n-      \/\/ is marked as young_gen and replace the Cmp with 0 (false) to collapse the barrier.\n-      Node* shift = node->find_out_with(Op_URShiftX);\n-      assert(shift != nullptr, \"missing G1 post barrier\");\n-      Node* addp = shift->unique_out();\n-      Node* load = addp->find_out_with(Op_LoadB);\n-      assert(load != nullptr, \"missing G1 post barrier\");\n-      Node* cmpx = load->unique_out();\n-      assert(cmpx->is_Cmp() && cmpx->unique_out()->is_Bool() &&\n-          cmpx->unique_out()->as_Bool()->_test._test == BoolTest::ne,\n-          \"missing card value check in G1 post barrier\");\n-      macro->replace_node(cmpx, macro->makecon(TypeInt::CC_EQ));\n-      \/\/ There is no G1 pre barrier in this case\n-    }\n-    \/\/ Now CastP2X can be removed since it is used only on dead path\n-    \/\/ which currently still alive until igvn optimize it.\n-    assert(node->outcnt() == 0 || node->unique_out()->Opcode() == Op_URShiftX, \"\");\n-    macro->replace_node(node, macro->top());\n-  }\n-}\n-\n-Node* G1BarrierSetC2Early::step_over_gc_barrier(Node* c) const {\n-  if (!use_ReduceInitialCardMarks() &&\n-      c != nullptr && c->is_Region() && c->req() == 3) {\n-    for (uint i = 1; i < c->req(); i++) {\n-      if (c->in(i) != nullptr && c->in(i)->is_Region() &&\n-          c->in(i)->req() == 3) {\n-        Node* r = c->in(i);\n-        for (uint j = 1; j < r->req(); j++) {\n-          if (r->in(j) != nullptr && r->in(j)->is_Proj() &&\n-              r->in(j)->in(0) != nullptr &&\n-              r->in(j)->in(0)->Opcode() == Op_CallLeaf &&\n-              r->in(j)->in(0)->as_Call()->entry_point() == CAST_FROM_FN_PTR(address, G1BarrierSetRuntime::write_ref_field_post_entry)) {\n-            Node* call = r->in(j)->in(0);\n-            c = c->in(i == 1 ? 2 : 1);\n-            if (c != nullptr && c->Opcode() != Op_Parm) {\n-              c = c->in(0);\n-              if (c != nullptr) {\n-                c = c->in(0);\n-                assert(call->in(0) == nullptr ||\n-                       call->in(0)->in(0) == nullptr ||\n-                       call->in(0)->in(0)->in(0) == nullptr ||\n-                       call->in(0)->in(0)->in(0)->in(0) == nullptr ||\n-                       call->in(0)->in(0)->in(0)->in(0)->in(0) == nullptr ||\n-                       c == call->in(0)->in(0)->in(0)->in(0)->in(0), \"bad barrier shape\");\n-                return c;\n-              }\n-            }\n-          }\n-        }\n-      }\n-    }\n-  }\n-  return c;\n-}\n-\n-#ifdef ASSERT\n-bool G1BarrierSetC2Early::has_cas_in_use_chain(Node *n) const {\n-  Unique_Node_List visited;\n-  Node_List worklist;\n-  worklist.push(n);\n-  while (worklist.size() > 0) {\n-    Node* x = worklist.pop();\n-    if (visited.member(x)) {\n-      continue;\n-    } else {\n-      visited.push(x);\n-    }\n-\n-    if (x->is_LoadStore()) {\n-      int op = x->Opcode();\n-      if (op == Op_CompareAndExchangeP || op == Op_CompareAndExchangeN ||\n-          op == Op_CompareAndSwapP     || op == Op_CompareAndSwapN     ||\n-          op == Op_WeakCompareAndSwapP || op == Op_WeakCompareAndSwapN) {\n-        return true;\n-      }\n-    }\n-    if (!x->is_CFG()) {\n-      for (SimpleDUIterator iter(x); iter.has_next(); iter.next()) {\n-        Node* use = iter.get();\n-        worklist.push(use);\n-      }\n-    }\n-  }\n-  return false;\n-}\n-\n-void G1BarrierSetC2Early::verify_pre_load(Node* marking_if, Unique_Node_List& loads \/*output*\/) const {\n-  assert(loads.size() == 0, \"Loads list should be empty\");\n-  Node* pre_val_if = marking_if->find_out_with(Op_IfTrue)->find_out_with(Op_If);\n-  if (pre_val_if != nullptr) {\n-    Unique_Node_List visited;\n-    Node_List worklist;\n-    Node* pre_val = pre_val_if->in(1)->in(1)->in(1);\n-\n-    worklist.push(pre_val);\n-    while (worklist.size() > 0) {\n-      Node* x = worklist.pop();\n-      if (visited.member(x)) {\n-        continue;\n-      } else {\n-        visited.push(x);\n-      }\n-\n-      if (has_cas_in_use_chain(x)) {\n-        loads.clear();\n-        return;\n-      }\n-\n-      if (x->is_Con()) {\n-        continue;\n-      }\n-      if (x->is_EncodeP() || x->is_DecodeN()) {\n-        worklist.push(x->in(1));\n-        continue;\n-      }\n-      if (x->is_Load() || x->is_LoadStore()) {\n-        assert(x->in(0) != nullptr, \"Pre-val load has to have a control\");\n-        loads.push(x);\n-        continue;\n-      }\n-      if (x->is_Phi()) {\n-        for (uint i = 1; i < x->req(); i++) {\n-          worklist.push(x->in(i));\n-        }\n-        continue;\n-      }\n-      assert(false, \"Pre-val anomaly\");\n-    }\n-  }\n-}\n-\n-void G1BarrierSetC2Early::verify_no_safepoints(Compile* compile, Node* marking_check_if, const Unique_Node_List& loads) const {\n-  if (loads.size() == 0) {\n-    return;\n-  }\n-\n-  if (loads.size() == 1) { \/\/ Handle the typical situation when there a single pre-value load\n-                           \/\/ that is dominated by the marking_check_if, that's true when the\n-                           \/\/ barrier itself does the pre-val load.\n-    Node *pre_val = loads.at(0);\n-    if (pre_val->in(0)->in(0) == marking_check_if) { \/\/ IfTrue->If\n-      return;\n-    }\n-  }\n-\n-  \/\/ All other cases are when pre-value loads dominate the marking check.\n-  Unique_Node_List controls;\n-  for (uint i = 0; i < loads.size(); i++) {\n-    Node *c = loads.at(i)->in(0);\n-    controls.push(c);\n-  }\n-\n-  Unique_Node_List visited;\n-  Unique_Node_List safepoints;\n-  Node_List worklist;\n-  uint found = 0;\n-\n-  worklist.push(marking_check_if);\n-  while (worklist.size() > 0 && found < controls.size()) {\n-    Node* x = worklist.pop();\n-    if (x == nullptr || x == compile->top()) continue;\n-    if (visited.member(x)) {\n-      continue;\n-    } else {\n-      visited.push(x);\n-    }\n-\n-    if (controls.member(x)) {\n-      found++;\n-    }\n-    if (x->is_Region()) {\n-      for (uint i = 1; i < x->req(); i++) {\n-        worklist.push(x->in(i));\n-      }\n-    } else {\n-      if (!x->is_SafePoint()) {\n-        worklist.push(x->in(0));\n-      } else {\n-        safepoints.push(x);\n-      }\n-    }\n-  }\n-  assert(found == controls.size(), \"Pre-barrier structure anomaly or possible safepoint\");\n-}\n-\n-void G1BarrierSetC2Early::verify_gc_barriers(Compile* compile, CompilePhase phase) const {\n-  if (phase != BarrierSetC2::BeforeCodeGen) {\n-    return;\n-  }\n-  \/\/ Verify G1 pre-barriers\n-  const int marking_offset = in_bytes(G1ThreadLocalData::satb_mark_queue_active_offset());\n-\n-  Unique_Node_List visited;\n-  Node_List worklist;\n-  \/\/ We're going to walk control flow backwards starting from the Root\n-  worklist.push(compile->root());\n-  while (worklist.size() > 0) {\n-    Node* x = worklist.pop();\n-    if (x == nullptr || x == compile->top()) continue;\n-    if (visited.member(x)) {\n-      continue;\n-    } else {\n-      visited.push(x);\n-    }\n-\n-    if (x->is_Region()) {\n-      for (uint i = 1; i < x->req(); i++) {\n-        worklist.push(x->in(i));\n-      }\n-    } else {\n-      worklist.push(x->in(0));\n-      \/\/ We are looking for the pattern:\n-      \/\/                            \/->ThreadLocal\n-      \/\/ If->Bool->CmpI->LoadB->AddP->ConL(marking_offset)\n-      \/\/              \\->ConI(0)\n-      \/\/ We want to verify that the If and the LoadB have the same control\n-      \/\/ See GraphKit::g1_write_barrier_pre()\n-      if (x->is_If()) {\n-        IfNode *iff = x->as_If();\n-        if (iff->in(1)->is_Bool() && iff->in(1)->in(1)->is_Cmp()) {\n-          CmpNode *cmp = iff->in(1)->in(1)->as_Cmp();\n-          if (cmp->Opcode() == Op_CmpI && cmp->in(2)->is_Con() && cmp->in(2)->bottom_type()->is_int()->get_con() == 0\n-              && cmp->in(1)->is_Load()) {\n-            LoadNode* load = cmp->in(1)->as_Load();\n-            if (load->Opcode() == Op_LoadB && load->in(2)->is_AddP() && load->in(2)->in(2)->Opcode() == Op_ThreadLocal\n-                && load->in(2)->in(3)->is_Con()\n-                && load->in(2)->in(3)->bottom_type()->is_intptr_t()->get_con() == marking_offset) {\n-\n-              Node* if_ctrl = iff->in(0);\n-              Node* load_ctrl = load->in(0);\n-\n-              if (if_ctrl != load_ctrl) {\n-                \/\/ Skip possible CProj->NeverBranch in infinite loops\n-                if ((if_ctrl->is_Proj() && if_ctrl->Opcode() == Op_CProj)\n-                    && if_ctrl->in(0)->is_NeverBranch()) {\n-                  if_ctrl = if_ctrl->in(0)->in(0);\n-                }\n-              }\n-              assert(load_ctrl != nullptr && if_ctrl == load_ctrl, \"controls must match\");\n-\n-              Unique_Node_List loads;\n-              verify_pre_load(iff, loads);\n-              verify_no_safepoints(compile, iff, loads);\n-            }\n-          }\n-        }\n-      }\n-    }\n-  }\n-}\n-#endif\n-\n-bool G1BarrierSetC2Early::escape_add_to_con_graph(ConnectionGraph* conn_graph, PhaseGVN* gvn, Unique_Node_List* delayed_worklist, Node* n, uint opcode) const {\n-  if (opcode == Op_StoreP) {\n-    Node* adr = n->in(MemNode::Address);\n-    const Type* adr_type = gvn->type(adr);\n-    \/\/ Pointer stores in G1 barriers looks like unsafe access.\n-    \/\/ Ignore such stores to be able scalar replace non-escaping\n-    \/\/ allocations.\n-    if (adr_type->isa_rawptr() && adr->is_AddP()) {\n-      Node* base = conn_graph->get_addp_base(adr);\n-      if (base->Opcode() == Op_LoadP &&\n-          base->in(MemNode::Address)->is_AddP()) {\n-        adr = base->in(MemNode::Address);\n-        Node* tls = conn_graph->get_addp_base(adr);\n-        if (tls->Opcode() == Op_ThreadLocal) {\n-          int offs = (int) gvn->find_intptr_t_con(adr->in(AddPNode::Offset), Type::OffsetBot);\n-          const int buf_offset = in_bytes(G1ThreadLocalData::satb_mark_queue_buffer_offset());\n-          if (offs == buf_offset) {\n-            return true; \/\/ G1 pre barrier previous oop value store.\n-          }\n-          if (offs == in_bytes(G1ThreadLocalData::dirty_card_queue_buffer_offset())) {\n-            return true; \/\/ G1 post barrier card address store.\n-          }\n-        }\n-      }\n-    }\n-  }\n-  return false;\n-}\n-\n-#endif \/\/ G1_LATE_BARRIER_MIGRATION_SUPPORT\n","filename":"src\/hotspot\/share\/gc\/g1\/c2\/g1BarrierSetC2.cpp","additions":0,"deletions":1045,"binary":false,"changes":1045,"status":"modified"},{"patch":"@@ -127,76 +127,0 @@\n-#if G1_LATE_BARRIER_MIGRATION_SUPPORT\n-class G1BarrierSetC2Early : public CardTableBarrierSetC2 {\n-protected:\n-  virtual void pre_barrier(GraphKit* kit,\n-                           bool do_load,\n-                           Node* ctl,\n-                           Node* obj,\n-                           Node* adr,\n-                           uint adr_idx,\n-                           Node* val,\n-                           const TypeOopPtr* val_type,\n-                           Node* pre_val,\n-                           BasicType bt) const;\n-\n-  virtual void post_barrier(GraphKit* kit,\n-                            Node* ctl,\n-                            Node* store,\n-                            Node* obj,\n-                            Node* adr,\n-                            uint adr_idx,\n-                            Node* val,\n-                            BasicType bt,\n-                            bool use_precise) const;\n-\n-  bool g1_can_remove_pre_barrier(GraphKit* kit,\n-                                 PhaseValues* phase,\n-                                 Node* adr,\n-                                 BasicType bt,\n-                                 uint adr_idx) const;\n-\n-  bool g1_can_remove_post_barrier(GraphKit* kit,\n-                                  PhaseValues* phase, Node* store,\n-                                  Node* adr) const;\n-\n-  void g1_mark_card(GraphKit* kit,\n-                    IdealKit& ideal,\n-                    Node* card_adr,\n-                    Node* oop_store,\n-                    uint oop_alias_idx,\n-                    Node* index,\n-                    Node* index_adr,\n-                    Node* buffer,\n-                    const TypeFunc* tf) const;\n-\n-  \/\/ Helper for unsafe accesses, that may or may not be on the referent field.\n-  \/\/ Generates the guards that check whether the result of\n-  \/\/ Unsafe.getReference should be recorded in an SATB log buffer.\n-  void insert_pre_barrier(GraphKit* kit, Node* base_oop, Node* offset, Node* pre_val, bool need_mem_bar) const;\n-\n-  static const TypeFunc* write_ref_field_pre_entry_Type();\n-  static const TypeFunc* write_ref_field_post_entry_Type();\n-\n-  virtual Node* load_at_resolved(C2Access& access, const Type* val_type) const;\n-\n-#ifdef ASSERT\n-  bool has_cas_in_use_chain(Node* x) const;\n-  void verify_pre_load(Node* marking_check_if, Unique_Node_List& loads \/*output*\/) const;\n-  void verify_no_safepoints(Compile* compile, Node* marking_load, const Unique_Node_List& loads) const;\n-#endif\n-\n-  static bool is_g1_pre_val_load(Node* n);\n-public:\n-  virtual void clone(GraphKit* kit, Node* src, Node* dst, Node* size, bool is_array) const;\n-  virtual bool is_gc_pre_barrier_node(Node* node) const;\n-  virtual bool is_gc_barrier_node(Node* node) const;\n-  virtual void eliminate_gc_barrier(PhaseMacroExpand* macro, Node* node) const;\n-  virtual Node* step_over_gc_barrier(Node* c) const;\n-\n-#ifdef ASSERT\n-  virtual void verify_gc_barriers(Compile* compile, CompilePhase phase) const;\n-#endif\n-\n-  virtual bool escape_add_to_con_graph(ConnectionGraph* conn_graph, PhaseGVN* gvn, Unique_Node_List* delayed_worklist, Node* n, uint opcode) const;\n-};\n-#endif \/\/ G1_LATE_BARRIER_MIGRATION_SUPPORT\n-\n","filename":"src\/hotspot\/share\/gc\/g1\/c2\/g1BarrierSetC2.hpp","additions":0,"deletions":76,"binary":false,"changes":76,"status":"modified"},{"patch":"@@ -246,33 +246,0 @@\n-\n-#if G1_LATE_BARRIER_MIGRATION_SUPPORT\n-  if (G1StressBarriers) {\n-    \/\/ Increase the frequency of concurrent marking (so that the pre-barrier is\n-    \/\/ not trivially skipped).\n-    if (FLAG_IS_DEFAULT(G1UseAdaptiveIHOP)) {\n-      FLAG_SET_ERGO(G1UseAdaptiveIHOP, false);\n-    }\n-    if (FLAG_IS_DEFAULT(InitiatingHeapOccupancyPercent)) {\n-      FLAG_SET_ERGO(InitiatingHeapOccupancyPercent, 0);\n-    }\n-    \/\/ Exercise both pre-barrier enqueue paths (inline and runtime) equally.\n-    if (FLAG_IS_DEFAULT(G1SATBBufferSize)) {\n-      FLAG_SET_ERGO(G1SATBBufferSize, 2);\n-    }\n-    \/\/ Increase the frequency of inter-regional objects in the post-barrier.\n-    if (FLAG_IS_DEFAULT(G1HeapRegionSize)) {\n-      FLAG_SET_ERGO(G1HeapRegionSize, G1HeapRegionBounds::min_size());\n-    }\n-    \/\/ Increase the frequency with which the post-barrier sees a clean card and\n-    \/\/ has to dirty it.\n-    if (FLAG_IS_DEFAULT(GCCardSizeInBytes)) {\n-      FLAG_SET_ERGO(GCCardSizeInBytes, MAX2(ObjectAlignmentInBytes, 128));\n-    }\n-    if (FLAG_IS_DEFAULT(G1RSetUpdatingPauseTimePercent)) {\n-      FLAG_SET_ERGO(G1RSetUpdatingPauseTimePercent, 0);\n-    }\n-    \/\/ Exercise both post-barrier dirtying paths (inline and runtime) equally.\n-    if (FLAG_IS_DEFAULT(G1UpdateBufferSize)) {\n-      FLAG_SET_ERGO(G1UpdateBufferSize, 2);\n-    }\n-  }\n-#endif \/\/ G1_LATE_BARRIER_MIGRATION_SUPPORT\n","filename":"src\/hotspot\/share\/gc\/g1\/g1Arguments.cpp","additions":0,"deletions":33,"binary":false,"changes":33,"status":"modified"},{"patch":"@@ -52,3 +52,0 @@\n-#if G1_LATE_BARRIER_MIGRATION_SUPPORT\n-class G1BarrierSetC2Early;\n-#endif\n@@ -59,5 +56,0 @@\n-#if G1_LATE_BARRIER_MIGRATION_SUPPORT\n-                      G1UseLateBarrierExpansion ?\n-                      make_barrier_set_c2<G1BarrierSetC2>() :\n-                      make_barrier_set_c2<G1BarrierSetC2Early>(),\n-#else\n@@ -65,1 +57,0 @@\n-#endif\n","filename":"src\/hotspot\/share\/gc\/g1\/g1BarrierSet.cpp","additions":0,"deletions":9,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -90,16 +90,0 @@\n-\n-\/\/ Temporary flags to support the migration from early to late barrier expansion\n-\/\/ (see JEP 475) for all platforms. These flags are not intended to be\n-\/\/ integrated in the main JDK repository.\n-#if G1_LATE_BARRIER_MIGRATION_SUPPORT\n-#define G1_LATE_BARRIER_MIGRATION_SUPPORT_FLAGS(product)                    \\\n-                                                                            \\\n-  product(bool, G1UseLateBarrierExpansion, true,                            \\\n-          \"Expand G1 barriers late during C2 compilation\")                  \\\n-                                                                            \\\n-  product(bool, G1StressBarriers, false,                                    \\\n-          \"Configure G1 to exercise cold barrier paths\")\n-#else\n-#define G1_LATE_BARRIER_MIGRATION_SUPPORT_FLAGS(product)\n-#endif\n-\n@@ -358,3 +342,1 @@\n-                    constraint)                                             \\\n-                                                                            \\\n-  G1_LATE_BARRIER_MIGRATION_SUPPORT_FLAGS(product)\n+                    constraint)\n","filename":"src\/hotspot\/share\/gc\/g1\/g1_globals.hpp","additions":1,"deletions":19,"binary":false,"changes":20,"status":"modified"},{"patch":"@@ -34,8 +34,0 @@\n-\/\/ G1_LATE_BARRIER_MIGRATION_SUPPORT enables temporary support for migrating\n-\/\/ from early to late barrier expansion (see JEP 475) for all platforms.\n-\/\/ This support is not intended to be integrated in the main JDK repository.\n-#ifdef G1_LATE_BARRIER_MIGRATION_SUPPORT\n-#error \"G1_LATE_BARRIER_MIGRATION_SUPPORT already defined\"\n-#endif\n-#define G1_LATE_BARRIER_MIGRATION_SUPPORT 0\n-\n","filename":"src\/hotspot\/share\/runtime\/globals_shared.hpp","additions":0,"deletions":8,"binary":false,"changes":8,"status":"modified"}]}