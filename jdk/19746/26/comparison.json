{"files":[{"patch":"@@ -203,0 +203,7 @@\n+  ifeq ($(call check-jvm-feature, g1gc), true)\n+    AD_SRC_FILES += $(call uniq, $(wildcard $(foreach d, $(AD_SRC_ROOTS), \\\n+        $d\/cpu\/$(HOTSPOT_TARGET_CPU_ARCH)\/gc\/g1\/g1_$(HOTSPOT_TARGET_CPU).ad \\\n+        $d\/cpu\/$(HOTSPOT_TARGET_CPU_ARCH)\/gc\/g1\/g1_$(HOTSPOT_TARGET_CPU_ARCH).ad \\\n+      )))\n+  endif\n+\n","filename":"make\/hotspot\/gensrc\/GensrcAdlc.gmk","additions":7,"deletions":0,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -2623,1 +2623,2 @@\n-      is_valid_sve_arith_imm_pattern(n, m)) {\n+      is_valid_sve_arith_imm_pattern(n, m) ||\n+      is_encode_and_store_pattern(n, m)) {\n@@ -6413,1 +6414,1 @@\n-  predicate(!needs_acquiring_load(n));\n+  predicate(!needs_acquiring_load(n) && n->as_Load()->barrier_data() == 0);\n@@ -6842,1 +6843,1 @@\n-  predicate(!needs_releasing_store(n));\n+  predicate(!needs_releasing_store(n) && n->as_Store()->barrier_data() == 0);\n@@ -6855,1 +6856,1 @@\n-  predicate(!needs_releasing_store(n));\n+  predicate(!needs_releasing_store(n) && n->as_Store()->barrier_data() == 0);\n@@ -7089,0 +7090,1 @@\n+  predicate(n->as_Load()->barrier_data() == 0);\n@@ -7256,0 +7258,1 @@\n+  predicate(n->as_Store()->barrier_data() == 0);\n@@ -7268,0 +7271,1 @@\n+  predicate(n->as_Store()->barrier_data() == 0);\n@@ -8064,0 +8068,1 @@\n+  predicate(n->as_LoadStore()->barrier_data() == 0);\n@@ -8178,1 +8183,1 @@\n-  predicate(needs_acquiring_load_exclusive(n));\n+  predicate(needs_acquiring_load_exclusive(n) && n->as_LoadStore()->barrier_data() == 0);\n@@ -8283,0 +8288,1 @@\n+  predicate(n->as_LoadStore()->barrier_data() == 0);\n@@ -8392,1 +8398,1 @@\n-  predicate(needs_acquiring_load_exclusive(n));\n+  predicate(needs_acquiring_load_exclusive(n) && n->as_LoadStore()->barrier_data() == 0);\n@@ -8504,0 +8510,1 @@\n+  predicate(n->as_LoadStore()->barrier_data() == 0);\n@@ -8623,1 +8630,1 @@\n-  predicate(needs_acquiring_load_exclusive(n));\n+  predicate(needs_acquiring_load_exclusive(n) && n->as_LoadStore()->barrier_data() == 0);\n@@ -8684,0 +8691,1 @@\n+  predicate(n->as_LoadStore()->barrier_data() == 0);\n@@ -8727,1 +8735,1 @@\n-  predicate(needs_acquiring_load_exclusive(n));\n+  predicate(needs_acquiring_load_exclusive(n) && n->as_LoadStore()->barrier_data() == 0);\n","filename":"src\/hotspot\/cpu\/aarch64\/aarch64.ad","additions":16,"deletions":8,"binary":false,"changes":24,"status":"modified"},{"patch":"@@ -48,0 +48,1 @@\n+       $1$6,NAcq,INDENT(predicate(needs_acquiring_load_exclusive(n) && n->as_LoadStore()->barrier_data() == 0);),\n@@ -49,0 +50,1 @@\n+       $1,N,INDENT(predicate(n->as_LoadStore()->barrier_data() == 0);),\n@@ -125,0 +127,1 @@\n+       $1$6,NAcq,INDENT(predicate(needs_acquiring_load_exclusive(n) && n->as_LoadStore()->barrier_data() == 0);),\n@@ -126,0 +129,1 @@\n+       $1,N,INDENT(predicate(n->as_LoadStore()->barrier_data() == 0);),\n","filename":"src\/hotspot\/cpu\/aarch64\/cas.m4","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -41,1 +41,4 @@\n-#endif\n+#endif \/\/ COMPILER1\n+#ifdef COMPILER2\n+#include \"gc\/g1\/c2\/g1BarrierSetC2.hpp\"\n+#endif \/\/ COMPILER2\n@@ -98,0 +101,48 @@\n+static void generate_queue_test_and_insertion(MacroAssembler* masm, ByteSize index_offset, ByteSize buffer_offset, Label& runtime,\n+                                              const Register thread, const Register value, const Register temp1, const Register temp2) {\n+  \/\/ Can we store a value in the given thread's buffer?\n+  \/\/ (The index field is typed as size_t.)\n+  __ ldr(temp1, Address(thread, in_bytes(index_offset)));   \/\/ temp1 := *(index address)\n+  __ cbz(temp1, runtime);                                   \/\/ jump to runtime if index == 0 (full buffer)\n+  \/\/ The buffer is not full, store value into it.\n+  __ sub(temp1, temp1, wordSize);                           \/\/ temp1 := next index\n+  __ str(temp1, Address(thread, in_bytes(index_offset)));   \/\/ *(index address) := next index\n+  __ ldr(temp2, Address(thread, in_bytes(buffer_offset)));  \/\/ temp2 := buffer address\n+  __ str(value, Address(temp2, temp1));                     \/\/ *(buffer address + next index) := value\n+}\n+\n+static void generate_pre_barrier_fast_path(MacroAssembler* masm,\n+                                           const Register thread,\n+                                           const Register tmp1) {\n+  Address in_progress(thread, in_bytes(G1ThreadLocalData::satb_mark_queue_active_offset()));\n+  \/\/ Is marking active?\n+  if (in_bytes(SATBMarkQueue::byte_width_of_active()) == 4) {\n+    __ ldrw(tmp1, in_progress);\n+  } else {\n+    assert(in_bytes(SATBMarkQueue::byte_width_of_active()) == 1, \"Assumption\");\n+    __ ldrb(tmp1, in_progress);\n+  }\n+}\n+\n+static void generate_pre_barrier_slow_path(MacroAssembler* masm,\n+                                           const Register obj,\n+                                           const Register pre_val,\n+                                           const Register thread,\n+                                           const Register tmp1,\n+                                           const Register tmp2,\n+                                           Label& done,\n+                                           Label& runtime) {\n+  \/\/ Do we need to load the previous value?\n+  if (obj != noreg) {\n+    __ load_heap_oop(pre_val, Address(obj, 0), noreg, noreg, AS_RAW);\n+  }\n+  \/\/ Is the previous value null?\n+  __ cbz(pre_val, done);\n+  generate_queue_test_and_insertion(masm,\n+                                    G1ThreadLocalData::satb_mark_queue_index_offset(),\n+                                    G1ThreadLocalData::satb_mark_queue_buffer_offset(),\n+                                    runtime,\n+                                    thread, pre_val, tmp1, tmp2);\n+  __ b(done);\n+}\n+\n@@ -118,11 +169,2 @@\n-  Address in_progress(thread, in_bytes(G1ThreadLocalData::satb_mark_queue_active_offset()));\n-  Address index(thread, in_bytes(G1ThreadLocalData::satb_mark_queue_index_offset()));\n-  Address buffer(thread, in_bytes(G1ThreadLocalData::satb_mark_queue_buffer_offset()));\n-\n-  \/\/ Is marking active?\n-  if (in_bytes(SATBMarkQueue::byte_width_of_active()) == 4) {\n-    __ ldrw(tmp1, in_progress);\n-  } else {\n-    assert(in_bytes(SATBMarkQueue::byte_width_of_active()) == 1, \"Assumption\");\n-    __ ldrb(tmp1, in_progress);\n-  }\n+  generate_pre_barrier_fast_path(masm, thread, tmp1);\n+  \/\/ If marking is not active (*(mark queue active address) == 0), jump to done\n@@ -130,25 +172,1 @@\n-\n-  \/\/ Do we need to load the previous value?\n-  if (obj != noreg) {\n-    __ load_heap_oop(pre_val, Address(obj, 0), noreg, noreg, AS_RAW);\n-  }\n-\n-  \/\/ Is the previous value null?\n-  __ cbz(pre_val, done);\n-\n-  \/\/ Can we store original value in the thread's buffer?\n-  \/\/ Is index == 0?\n-  \/\/ (The index field is typed as size_t.)\n-\n-  __ ldr(tmp1, index);                      \/\/ tmp := *index_adr\n-  __ cbz(tmp1, runtime);                    \/\/ tmp == 0?\n-                                        \/\/ If yes, goto runtime\n-\n-  __ sub(tmp1, tmp1, wordSize);             \/\/ tmp := tmp - wordSize\n-  __ str(tmp1, index);                      \/\/ *index_adr := tmp\n-  __ ldr(tmp2, buffer);\n-  __ add(tmp1, tmp1, tmp2);                 \/\/ tmp := tmp + *buffer_adr\n-\n-  \/\/ Record the previous value\n-  __ str(pre_val, Address(tmp1, 0));\n-  __ b(done);\n+  generate_pre_barrier_slow_path(masm, obj, pre_val, thread, tmp1, tmp2, done, runtime);\n@@ -185,0 +203,44 @@\n+static void generate_post_barrier_fast_path(MacroAssembler* masm,\n+                                            const Register store_addr,\n+                                            const Register new_val,\n+                                            const Register tmp1,\n+                                            const Register tmp2,\n+                                            Label& done,\n+                                            bool new_val_may_be_null) {\n+  \/\/ Does store cross heap regions?\n+  __ eor(tmp1, store_addr, new_val);                     \/\/ tmp1 := store address ^ new value\n+  __ lsr(tmp1, tmp1, G1HeapRegion::LogOfHRGrainBytes);   \/\/ tmp1 := ((store address ^ new value) >> LogOfHRGrainBytes)\n+  __ cbz(tmp1, done);\n+  \/\/ Crosses regions, storing null?\n+  if (new_val_may_be_null) {\n+    __ cbz(new_val, done);\n+  }\n+  \/\/ Storing region crossing non-null, is card young?\n+  __ lsr(tmp1, store_addr, CardTable::card_shift());     \/\/ tmp1 := card address relative to card table base\n+  __ load_byte_map_base(tmp2);                           \/\/ tmp2 := card table base address\n+  __ add(tmp1, tmp1, tmp2);                              \/\/ tmp1 := card address\n+  __ ldrb(tmp2, Address(tmp1));                          \/\/ tmp2 := card\n+  __ cmpw(tmp2, (int)G1CardTable::g1_young_card_val());  \/\/ tmp2 := card == young_card_val?\n+}\n+\n+static void generate_post_barrier_slow_path(MacroAssembler* masm,\n+                                            const Register thread,\n+                                            const Register tmp1,\n+                                            const Register tmp2,\n+                                            Label& done,\n+                                            Label& runtime) {\n+  __ membar(Assembler::StoreLoad);  \/\/ StoreLoad membar\n+  __ ldrb(tmp2, Address(tmp1));     \/\/ tmp2 := card\n+  __ cbzw(tmp2, done);\n+  \/\/ Storing a region crossing, non-null oop, card is clean.\n+  \/\/ Dirty card and log.\n+  STATIC_ASSERT(CardTable::dirty_card_val() == 0);\n+  __ strb(zr, Address(tmp1));       \/\/ *(card address) := dirty_card_val\n+  generate_queue_test_and_insertion(masm,\n+                                    G1ThreadLocalData::dirty_card_queue_index_offset(),\n+                                    G1ThreadLocalData::dirty_card_queue_buffer_offset(),\n+                                    runtime,\n+                                    thread, tmp1, tmp2, rscratch1);\n+  __ b(done);\n+}\n+\n@@ -197,7 +259,0 @@\n-  Address queue_index(thread, in_bytes(G1ThreadLocalData::dirty_card_queue_index_offset()));\n-  Address buffer(thread, in_bytes(G1ThreadLocalData::dirty_card_queue_buffer_offset()));\n-\n-  BarrierSet* bs = BarrierSet::barrier_set();\n-  CardTableBarrierSet* ctbs = barrier_set_cast<CardTableBarrierSet>(bs);\n-  CardTable* ct = ctbs->card_table();\n-\n@@ -207,1 +262,4 @@\n-  \/\/ Does store cross heap regions?\n+  generate_post_barrier_fast_path(masm, store_addr, new_val, tmp1, tmp2, done, true \/* new_val_may_be_null *\/);\n+  \/\/ If card is young, jump to done\n+  __ br(Assembler::EQ, done);\n+  generate_post_barrier_slow_path(masm, thread, tmp1, tmp2, done, runtime);\n@@ -209,3 +267,6 @@\n-  __ eor(tmp1, store_addr, new_val);\n-  __ lsr(tmp1, tmp1, G1HeapRegion::LogOfHRGrainBytes);\n-  __ cbz(tmp1, done);\n+  __ bind(runtime);\n+  \/\/ save the live input values\n+  RegSet saved = RegSet::of(store_addr);\n+  __ push(saved, sp);\n+  __ call_VM_leaf(CAST_FROM_FN_PTR(address, G1BarrierSetRuntime::write_ref_field_post_entry), tmp1, thread);\n+  __ pop(saved, sp);\n@@ -213,1 +274,2 @@\n-  \/\/ crosses regions, storing null?\n+  __ bind(done);\n+}\n@@ -215,1 +277,1 @@\n-  __ cbz(new_val, done);\n+#if defined(COMPILER2)\n@@ -217,1 +279,9 @@\n-  \/\/ storing region crossing non-null, is card already dirty?\n+static void generate_c2_barrier_runtime_call(MacroAssembler* masm, G1BarrierStubC2* stub, const Register arg, const address runtime_path) {\n+  SaveLiveRegisters save_registers(masm, stub);\n+  if (c_rarg0 != arg) {\n+    __ mov(c_rarg0, arg);\n+  }\n+  __ mov(c_rarg1, rthread);\n+  __ mov(rscratch1, runtime_path);\n+  __ blr(rscratch1);\n+}\n@@ -219,1 +289,10 @@\n-  const Register card_addr = tmp1;\n+void G1BarrierSetAssembler::g1_write_barrier_pre_c2(MacroAssembler* masm,\n+                                                    Register obj,\n+                                                    Register pre_val,\n+                                                    Register thread,\n+                                                    Register tmp1,\n+                                                    Register tmp2,\n+                                                    G1PreBarrierStubC2* stub) {\n+  assert(thread == rthread, \"must be\");\n+  assert_different_registers(obj, pre_val, tmp1, tmp2);\n+  assert(pre_val != noreg && tmp1 != noreg && tmp2 != noreg, \"expecting a register\");\n@@ -221,1 +300,1 @@\n-  __ lsr(card_addr, store_addr, CardTable::card_shift());\n+  stub->initialize_registers(obj, pre_val, thread, tmp1, tmp2);\n@@ -223,6 +302,3 @@\n-  \/\/ get the address of the card\n-  __ load_byte_map_base(tmp2);\n-  __ add(card_addr, card_addr, tmp2);\n-  __ ldrb(tmp2, Address(card_addr));\n-  __ cmpw(tmp2, (int)G1CardTable::g1_young_card_val());\n-  __ br(Assembler::EQ, done);\n+  generate_pre_barrier_fast_path(masm, thread, tmp1);\n+  \/\/ If marking is active (*(mark queue active address) != 0), jump to stub (slow path)\n+  __ cbnzw(tmp1, *stub->entry());\n@@ -230,1 +306,2 @@\n-  assert((int)CardTable::dirty_card_val() == 0, \"must be 0\");\n+  __ bind(*stub->continuation());\n+}\n@@ -232,1 +309,9 @@\n-  __ membar(Assembler::StoreLoad);\n+void G1BarrierSetAssembler::generate_c2_pre_barrier_stub(MacroAssembler* masm,\n+                                                         G1PreBarrierStubC2* stub) const {\n+  Assembler::InlineSkippedInstructionsCounter skip_counter(masm);\n+  Label runtime;\n+  Register obj = stub->obj();\n+  Register pre_val = stub->pre_val();\n+  Register thread = stub->thread();\n+  Register tmp1 = stub->tmp1();\n+  Register tmp2 = stub->tmp2();\n@@ -234,2 +319,2 @@\n-  __ ldrb(tmp2, Address(card_addr));\n-  __ cbzw(tmp2, done);\n+  __ bind(*stub->entry());\n+  generate_pre_barrier_slow_path(masm, obj, pre_val, thread, tmp1, tmp2, *stub->continuation(), runtime);\n@@ -237,2 +322,4 @@\n-  \/\/ storing a region crossing, non-null oop, card is clean.\n-  \/\/ dirty card and log.\n+  __ bind(runtime);\n+  generate_c2_barrier_runtime_call(masm, stub, pre_val, CAST_FROM_FN_PTR(address, G1BarrierSetRuntime::write_ref_field_pre_entry));\n+  __ b(*stub->continuation());\n+}\n@@ -240,1 +327,12 @@\n-  __ strb(zr, Address(card_addr));\n+void G1BarrierSetAssembler::g1_write_barrier_post_c2(MacroAssembler* masm,\n+                                                     Register store_addr,\n+                                                     Register new_val,\n+                                                     Register thread,\n+                                                     Register tmp1,\n+                                                     Register tmp2,\n+                                                     G1PostBarrierStubC2* stub) {\n+  assert(thread == rthread, \"must be\");\n+  assert_different_registers(store_addr, new_val, thread, tmp1, tmp2,\n+                             rscratch1);\n+  assert(store_addr != noreg && new_val != noreg && tmp1 != noreg\n+         && tmp2 != noreg, \"expecting a register\");\n@@ -242,4 +340,1 @@\n-  __ ldr(rscratch1, queue_index);\n-  __ cbz(rscratch1, runtime);\n-  __ sub(rscratch1, rscratch1, wordSize);\n-  __ str(rscratch1, queue_index);\n+  stub->initialize_registers(thread, tmp1, tmp2);\n@@ -247,3 +342,4 @@\n-  __ ldr(tmp2, buffer);\n-  __ str(card_addr, Address(tmp2, rscratch1));\n-  __ b(done);\n+  bool new_val_may_be_null = (stub->barrier_data() & G1C2BarrierPostNotNull) == 0;\n+  generate_post_barrier_fast_path(masm, store_addr, new_val, tmp1, tmp2, *stub->continuation(), new_val_may_be_null);\n+  \/\/ If card is not young, jump to stub (slow path)\n+  __ br(Assembler::NE, *stub->entry());\n@@ -251,6 +347,2 @@\n-  __ bind(runtime);\n-  \/\/ save the live input values\n-  RegSet saved = RegSet::of(store_addr);\n-  __ push(saved, sp);\n-  __ call_VM_leaf(CAST_FROM_FN_PTR(address, G1BarrierSetRuntime::write_ref_field_post_entry), card_addr, thread);\n-  __ pop(saved, sp);\n+  __ bind(*stub->continuation());\n+}\n@@ -258,1 +350,15 @@\n-  __ bind(done);\n+void G1BarrierSetAssembler::generate_c2_post_barrier_stub(MacroAssembler* masm,\n+                                                          G1PostBarrierStubC2* stub) const {\n+  Assembler::InlineSkippedInstructionsCounter skip_counter(masm);\n+  Label runtime;\n+  Register thread = stub->thread();\n+  Register tmp1 = stub->tmp1(); \/\/ tmp1 holds the card address.\n+  Register tmp2 = stub->tmp2();\n+  assert(stub->tmp3() == noreg, \"not needed in this platform\");\n+\n+  __ bind(*stub->entry());\n+  generate_post_barrier_slow_path(masm, thread, tmp1, tmp2, *stub->continuation(), runtime);\n+\n+  __ bind(runtime);\n+  generate_c2_barrier_runtime_call(masm, stub, tmp1, CAST_FROM_FN_PTR(address, G1BarrierSetRuntime::write_ref_field_post_entry));\n+  __ b(*stub->continuation());\n@@ -261,0 +367,2 @@\n+#endif \/\/ COMPILER2\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/gc\/g1\/g1BarrierSetAssembler_aarch64.cpp","additions":188,"deletions":80,"binary":false,"changes":268,"status":"modified"},{"patch":"@@ -36,0 +36,2 @@\n+class G1PreBarrierStubC2;\n+class G1PostBarrierStubC2;\n@@ -72,0 +74,21 @@\n+#ifdef COMPILER2\n+  void g1_write_barrier_pre_c2(MacroAssembler* masm,\n+                               Register obj,\n+                               Register pre_val,\n+                               Register thread,\n+                               Register tmp1,\n+                               Register tmp2,\n+                               G1PreBarrierStubC2* c2_stub);\n+  void generate_c2_pre_barrier_stub(MacroAssembler* masm,\n+                                    G1PreBarrierStubC2* stub) const;\n+  void g1_write_barrier_post_c2(MacroAssembler* masm,\n+                                Register store_addr,\n+                                Register new_val,\n+                                Register thread,\n+                                Register tmp1,\n+                                Register tmp2,\n+                                G1PostBarrierStubC2* c2_stub);\n+  void generate_c2_post_barrier_stub(MacroAssembler* masm,\n+                                     G1PostBarrierStubC2* stub) const;\n+#endif\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/gc\/g1\/g1BarrierSetAssembler_aarch64.hpp","additions":23,"deletions":0,"binary":false,"changes":23,"status":"modified"},{"patch":"@@ -0,0 +1,680 @@\n+\/\/\n+\/\/ Copyright (c) 2024, Oracle and\/or its affiliates. All rights reserved.\n+\/\/ DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+\/\/\n+\/\/ This code is free software; you can redistribute it and\/or modify it\n+\/\/ under the terms of the GNU General Public License version 2 only, as\n+\/\/ published by the Free Software Foundation.\n+\/\/\n+\/\/ This code is distributed in the hope that it will be useful, but WITHOUT\n+\/\/ ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+\/\/ FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+\/\/ version 2 for more details (a copy is included in the LICENSE file that\n+\/\/ accompanied this code).\n+\/\/\n+\/\/ You should have received a copy of the GNU General Public License version\n+\/\/ 2 along with this work; if not, write to the Free Software Foundation,\n+\/\/ Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+\/\/\n+\/\/ Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+\/\/ or visit www.oracle.com if you need additional information or have any\n+\/\/ questions.\n+\/\/\n+\n+source_hpp %{\n+\n+#include \"gc\/g1\/c2\/g1BarrierSetC2.hpp\"\n+#include \"gc\/shared\/gc_globals.hpp\"\n+\n+%}\n+\n+source %{\n+\n+#include \"gc\/g1\/g1BarrierSetAssembler_aarch64.hpp\"\n+#include \"gc\/g1\/g1BarrierSetRuntime.hpp\"\n+\n+static void write_barrier_pre(MacroAssembler* masm,\n+                              const MachNode* node,\n+                              Register obj,\n+                              Register pre_val,\n+                              Register tmp1,\n+                              Register tmp2,\n+                              RegSet preserve = RegSet(),\n+                              RegSet no_preserve = RegSet()) {\n+  if (!G1PreBarrierStubC2::needs_barrier(node)) {\n+    return;\n+  }\n+  Assembler::InlineSkippedInstructionsCounter skip_counter(masm);\n+  G1BarrierSetAssembler* g1_asm = static_cast<G1BarrierSetAssembler*>(BarrierSet::barrier_set()->barrier_set_assembler());\n+  G1PreBarrierStubC2* const stub = G1PreBarrierStubC2::create(node);\n+  for (RegSetIterator<Register> reg = preserve.begin(); *reg != noreg; ++reg) {\n+    stub->preserve(*reg);\n+  }\n+  for (RegSetIterator<Register> reg = no_preserve.begin(); *reg != noreg; ++reg) {\n+    stub->dont_preserve(*reg);\n+  }\n+  g1_asm->g1_write_barrier_pre_c2(masm, obj, pre_val, rthread, tmp1, tmp2, stub);\n+}\n+\n+static void write_barrier_post(MacroAssembler* masm,\n+                               const MachNode* node,\n+                               Register store_addr,\n+                               Register new_val,\n+                               Register tmp1,\n+                               Register tmp2) {\n+  if (!G1PostBarrierStubC2::needs_barrier(node)) {\n+    return;\n+  }\n+  Assembler::InlineSkippedInstructionsCounter skip_counter(masm);\n+  G1BarrierSetAssembler* g1_asm = static_cast<G1BarrierSetAssembler*>(BarrierSet::barrier_set()->barrier_set_assembler());\n+  G1PostBarrierStubC2* const stub = G1PostBarrierStubC2::create(node);\n+  g1_asm->g1_write_barrier_post_c2(masm, store_addr, new_val, rthread, tmp1, tmp2, stub);\n+}\n+\n+%}\n+\n+\/\/ BEGIN This section of the file is automatically generated. Do not edit --------------\n+\n+\/\/ This section is generated from g1_aarch64.m4\n+\n+\n+\/\/ This pattern is generated automatically from g1_aarch64.m4.\n+\/\/ DO NOT EDIT ANYTHING IN THIS SECTION OF THE FILE\n+instruct g1StoreP(indirect mem, iRegP src, iRegPNoSp tmp1, iRegPNoSp tmp2, iRegPNoSp tmp3, rFlagsReg cr)\n+%{\n+  predicate(UseG1GC && !needs_releasing_store(n) && n->as_Store()->barrier_data() != 0);\n+  match(Set mem (StoreP mem src));\n+  effect(TEMP tmp1, TEMP tmp2, TEMP tmp3, KILL cr);\n+  ins_cost(INSN_COST);\n+  format %{ \"str  $src, $mem\\t# ptr\" %}\n+  ins_encode %{\n+    write_barrier_pre(masm, this,\n+                      $mem$$Register \/* obj *\/,\n+                      $tmp1$$Register \/* pre_val *\/,\n+                      $tmp2$$Register \/* tmp1 *\/,\n+                      $tmp3$$Register \/* tmp2 *\/,\n+                      RegSet::of($mem$$Register, $src$$Register) \/* preserve *\/);\n+    __ str($src$$Register, $mem$$Register);\n+    write_barrier_post(masm, this,\n+                       $mem$$Register \/* store_addr *\/,\n+                       $src$$Register \/* new_val *\/,\n+                       $tmp2$$Register \/* tmp1 *\/,\n+                       $tmp3$$Register \/* tmp2 *\/);\n+  %}\n+  ins_pipe(istore_reg_mem);\n+%}\n+\n+\/\/ This pattern is generated automatically from g1_aarch64.m4.\n+\/\/ DO NOT EDIT ANYTHING IN THIS SECTION OF THE FILE\n+instruct g1StorePVolatile(indirect mem, iRegP src, iRegPNoSp tmp1, iRegPNoSp tmp2, iRegPNoSp tmp3, rFlagsReg cr)\n+%{\n+  predicate(UseG1GC && needs_releasing_store(n) && n->as_Store()->barrier_data() != 0);\n+  match(Set mem (StoreP mem src));\n+  effect(TEMP tmp1, TEMP tmp2, TEMP tmp3, KILL cr);\n+  ins_cost(VOLATILE_REF_COST);\n+  format %{ \"stlr  $src, $mem\\t# ptr\" %}\n+  ins_encode %{\n+    write_barrier_pre(masm, this,\n+                      $mem$$Register \/* obj *\/,\n+                      $tmp1$$Register \/* pre_val *\/,\n+                      $tmp2$$Register \/* tmp1 *\/,\n+                      $tmp3$$Register \/* tmp2 *\/,\n+                      RegSet::of($mem$$Register, $src$$Register) \/* preserve *\/);\n+    __ stlr($src$$Register, $mem$$Register);\n+    write_barrier_post(masm, this,\n+                       $mem$$Register \/* store_addr *\/,\n+                       $src$$Register \/* new_val *\/,\n+                       $tmp2$$Register \/* tmp1 *\/,\n+                       $tmp3$$Register \/* tmp2 *\/);\n+  %}\n+  ins_pipe(pipe_class_memory);\n+%}\n+\n+\/\/ This pattern is generated automatically from g1_aarch64.m4.\n+\/\/ DO NOT EDIT ANYTHING IN THIS SECTION OF THE FILE\n+instruct g1StoreN(indirect mem, iRegN src, iRegPNoSp tmp1, iRegPNoSp tmp2, iRegPNoSp tmp3, rFlagsReg cr)\n+%{\n+  predicate(UseG1GC && !needs_releasing_store(n) && n->as_Store()->barrier_data() != 0);\n+  match(Set mem (StoreN mem src));\n+  effect(TEMP tmp1, TEMP tmp2, TEMP tmp3, KILL cr);\n+  ins_cost(INSN_COST);\n+  format %{ \"strw  $src, $mem\\t# compressed ptr\" %}\n+  ins_encode %{\n+    write_barrier_pre(masm, this,\n+                      $mem$$Register \/* obj *\/,\n+                      $tmp1$$Register \/* pre_val *\/,\n+                      $tmp2$$Register \/* tmp1 *\/,\n+                      $tmp3$$Register \/* tmp2 *\/,\n+                      RegSet::of($mem$$Register, $src$$Register) \/* preserve *\/);\n+    __ strw($src$$Register, $mem$$Register);\n+    if ((barrier_data() & G1C2BarrierPost) != 0) {\n+      if ((barrier_data() & G1C2BarrierPostNotNull) == 0) {\n+        __ decode_heap_oop($tmp1$$Register, $src$$Register);\n+      } else {\n+        __ decode_heap_oop_not_null($tmp1$$Register, $src$$Register);\n+      }\n+    }\n+    write_barrier_post(masm, this,\n+                       $mem$$Register \/* store_addr *\/,\n+                       $tmp1$$Register \/* new_val *\/,\n+                       $tmp2$$Register \/* tmp1 *\/,\n+                       $tmp3$$Register \/* tmp2 *\/);\n+  %}\n+  ins_pipe(istore_reg_mem);\n+%}\n+\n+\/\/ This pattern is generated automatically from g1_aarch64.m4.\n+\/\/ DO NOT EDIT ANYTHING IN THIS SECTION OF THE FILE\n+instruct g1StoreNVolatile(indirect mem, iRegN src, iRegPNoSp tmp1, iRegPNoSp tmp2, iRegPNoSp tmp3, rFlagsReg cr)\n+%{\n+  predicate(UseG1GC && needs_releasing_store(n) && n->as_Store()->barrier_data() != 0);\n+  match(Set mem (StoreN mem src));\n+  effect(TEMP tmp1, TEMP tmp2, TEMP tmp3, KILL cr);\n+  ins_cost(VOLATILE_REF_COST);\n+  format %{ \"stlrw  $src, $mem\\t# compressed ptr\" %}\n+  ins_encode %{\n+    write_barrier_pre(masm, this,\n+                      $mem$$Register \/* obj *\/,\n+                      $tmp1$$Register \/* pre_val *\/,\n+                      $tmp2$$Register \/* tmp1 *\/,\n+                      $tmp3$$Register \/* tmp2 *\/,\n+                      RegSet::of($mem$$Register, $src$$Register) \/* preserve *\/);\n+    __ stlrw($src$$Register, $mem$$Register);\n+    if ((barrier_data() & G1C2BarrierPost) != 0) {\n+      if ((barrier_data() & G1C2BarrierPostNotNull) == 0) {\n+        __ decode_heap_oop($tmp1$$Register, $src$$Register);\n+      } else {\n+        __ decode_heap_oop_not_null($tmp1$$Register, $src$$Register);\n+      }\n+    }\n+    write_barrier_post(masm, this,\n+                       $mem$$Register \/* store_addr *\/,\n+                       $tmp1$$Register \/* new_val *\/,\n+                       $tmp2$$Register \/* tmp1 *\/,\n+                       $tmp3$$Register \/* tmp2 *\/);\n+  %}\n+  ins_pipe(pipe_class_memory);\n+%}\n+\n+\/\/ This pattern is generated automatically from g1_aarch64.m4.\n+\/\/ DO NOT EDIT ANYTHING IN THIS SECTION OF THE FILE\n+instruct g1EncodePAndStoreN(indirect mem, iRegP src, iRegPNoSp tmp1, iRegPNoSp tmp2, iRegPNoSp tmp3, rFlagsReg cr)\n+%{\n+  predicate(UseG1GC && !needs_releasing_store(n) && n->as_Store()->barrier_data() != 0);\n+  match(Set mem (StoreN mem (EncodeP src)));\n+  effect(TEMP tmp1, TEMP tmp2, TEMP tmp3, KILL cr);\n+  ins_cost(INSN_COST);\n+  format %{ \"encode_heap_oop $tmp1, $src\\n\\t\"\n+            \"strw  $tmp1, $mem\\t# compressed ptr\" %}\n+  ins_encode %{\n+    write_barrier_pre(masm, this,\n+                      $mem$$Register \/* obj *\/,\n+                      $tmp1$$Register \/* pre_val *\/,\n+                      $tmp2$$Register \/* tmp1 *\/,\n+                      $tmp3$$Register \/* tmp2 *\/,\n+                      RegSet::of($mem$$Register, $src$$Register) \/* preserve *\/);\n+    if ((barrier_data() & G1C2BarrierPostNotNull) == 0) {\n+      __ encode_heap_oop($tmp1$$Register, $src$$Register);\n+    } else {\n+      __ encode_heap_oop_not_null($tmp1$$Register, $src$$Register);\n+    }\n+    __ strw($tmp1$$Register, $mem$$Register);\n+    write_barrier_post(masm, this,\n+                       $mem$$Register \/* store_addr *\/,\n+                       $src$$Register \/* new_val *\/,\n+                       $tmp2$$Register \/* tmp1 *\/,\n+                       $tmp3$$Register \/* tmp2 *\/);\n+  %}\n+  ins_pipe(istore_reg_mem);\n+%}\n+\n+\/\/ This pattern is generated automatically from g1_aarch64.m4.\n+\/\/ DO NOT EDIT ANYTHING IN THIS SECTION OF THE FILE\n+instruct g1EncodePAndStoreNVolatile(indirect mem, iRegP src, iRegPNoSp tmp1, iRegPNoSp tmp2, iRegPNoSp tmp3, rFlagsReg cr)\n+%{\n+  predicate(UseG1GC && needs_releasing_store(n) && n->as_Store()->barrier_data() != 0);\n+  match(Set mem (StoreN mem (EncodeP src)));\n+  effect(TEMP tmp1, TEMP tmp2, TEMP tmp3, KILL cr);\n+  ins_cost(VOLATILE_REF_COST);\n+  format %{ \"encode_heap_oop $tmp1, $src\\n\\t\"\n+            \"stlrw  $tmp1, $mem\\t# compressed ptr\" %}\n+  ins_encode %{\n+    write_barrier_pre(masm, this,\n+                      $mem$$Register \/* obj *\/,\n+                      $tmp1$$Register \/* pre_val *\/,\n+                      $tmp2$$Register \/* tmp1 *\/,\n+                      $tmp3$$Register \/* tmp2 *\/,\n+                      RegSet::of($mem$$Register, $src$$Register) \/* preserve *\/);\n+    if ((barrier_data() & G1C2BarrierPostNotNull) == 0) {\n+      __ encode_heap_oop($tmp1$$Register, $src$$Register);\n+    } else {\n+      __ encode_heap_oop_not_null($tmp1$$Register, $src$$Register);\n+    }\n+    __ stlrw($tmp1$$Register, $mem$$Register);\n+    write_barrier_post(masm, this,\n+                       $mem$$Register \/* store_addr *\/,\n+                       $src$$Register \/* new_val *\/,\n+                       $tmp2$$Register \/* tmp1 *\/,\n+                       $tmp3$$Register \/* tmp2 *\/);\n+  %}\n+  ins_pipe(pipe_class_memory);\n+%}\n+\n+\/\/ This pattern is generated automatically from g1_aarch64.m4.\n+\/\/ DO NOT EDIT ANYTHING IN THIS SECTION OF THE FILE\n+instruct g1CompareAndExchangeP(iRegPNoSp res, indirect mem, iRegP oldval, iRegP newval, iRegPNoSp tmp1, iRegPNoSp tmp2, rFlagsReg cr)\n+%{\n+  predicate(UseG1GC && !needs_acquiring_load_exclusive(n) && n->as_LoadStore()->barrier_data() != 0);\n+  match(Set res (CompareAndExchangeP mem (Binary oldval newval)));\n+  effect(TEMP res, TEMP tmp1, TEMP tmp2, KILL cr);\n+  ins_cost(2 * VOLATILE_REF_COST);\n+  format %{ \"cmpxchg $res = $mem, $oldval, $newval\\t# ptr\" %}\n+  ins_encode %{\n+    assert_different_registers($oldval$$Register, $mem$$Register);\n+    assert_different_registers($newval$$Register, $mem$$Register);\n+    \/\/ Pass $oldval to the pre-barrier (instead of loading from $mem), because\n+    \/\/ $oldval is the only value that can be overwritten.\n+    \/\/ The same holds for g1CompareAndSwapP and its Acq variant.\n+    write_barrier_pre(masm, this,\n+                      noreg \/* obj *\/,\n+                      $oldval$$Register \/* pre_val *\/,\n+                      $tmp1$$Register \/* tmp1 *\/,\n+                      $tmp2$$Register \/* tmp2 *\/,\n+                      RegSet::of($mem$$Register, $oldval$$Register, $newval$$Register) \/* preserve *\/,\n+                      RegSet::of($res$$Register) \/* no_preserve *\/);\n+    __ cmpxchg($mem$$Register, $oldval$$Register, $newval$$Register, Assembler::xword,\n+               false \/* acquire *\/, true \/* release *\/, false \/* weak *\/, $res$$Register);\n+    write_barrier_post(masm, this,\n+                       $mem$$Register \/* store_addr *\/,\n+                       $newval$$Register \/* new_val *\/,\n+                       $tmp1$$Register \/* tmp1 *\/,\n+                       $tmp2$$Register \/* tmp2 *\/);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ This pattern is generated automatically from g1_aarch64.m4.\n+\/\/ DO NOT EDIT ANYTHING IN THIS SECTION OF THE FILE\n+instruct g1CompareAndExchangePAcq(iRegPNoSp res, indirect mem, iRegP oldval, iRegP newval, iRegPNoSp tmp1, iRegPNoSp tmp2, rFlagsReg cr)\n+%{\n+  predicate(UseG1GC && needs_acquiring_load_exclusive(n) && n->as_LoadStore()->barrier_data() != 0);\n+  match(Set res (CompareAndExchangeP mem (Binary oldval newval)));\n+  effect(TEMP res, TEMP tmp1, TEMP tmp2, KILL cr);\n+  ins_cost(VOLATILE_REF_COST);\n+  format %{ \"cmpxchg_acq $res = $mem, $oldval, $newval\\t# ptr\" %}\n+  ins_encode %{\n+    assert_different_registers($oldval$$Register, $mem$$Register);\n+    assert_different_registers($newval$$Register, $mem$$Register);\n+    \/\/ Pass $oldval to the pre-barrier (instead of loading from $mem), because\n+    \/\/ $oldval is the only value that can be overwritten.\n+    \/\/ The same holds for g1CompareAndSwapP and its Acq variant.\n+    write_barrier_pre(masm, this,\n+                      noreg \/* obj *\/,\n+                      $oldval$$Register \/* pre_val *\/,\n+                      $tmp1$$Register \/* tmp1 *\/,\n+                      $tmp2$$Register \/* tmp2 *\/,\n+                      RegSet::of($mem$$Register, $oldval$$Register, $newval$$Register) \/* preserve *\/,\n+                      RegSet::of($res$$Register) \/* no_preserve *\/);\n+    __ cmpxchg($mem$$Register, $oldval$$Register, $newval$$Register, Assembler::xword,\n+               true \/* acquire *\/, true \/* release *\/, false \/* weak *\/, $res$$Register);\n+    write_barrier_post(masm, this,\n+                       $mem$$Register \/* store_addr *\/,\n+                       $newval$$Register \/* new_val *\/,\n+                       $tmp1$$Register \/* tmp1 *\/,\n+                       $tmp2$$Register \/* tmp2 *\/);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ This pattern is generated automatically from g1_aarch64.m4.\n+\/\/ DO NOT EDIT ANYTHING IN THIS SECTION OF THE FILE\n+instruct g1CompareAndExchangeN(iRegNNoSp res, indirect mem, iRegN oldval, iRegN newval, iRegPNoSp tmp1, iRegPNoSp tmp2, iRegPNoSp tmp3, rFlagsReg cr)\n+%{\n+  predicate(UseG1GC && !needs_acquiring_load_exclusive(n) && n->as_LoadStore()->barrier_data() != 0);\n+  match(Set res (CompareAndExchangeN mem (Binary oldval newval)));\n+  effect(TEMP res, TEMP tmp1, TEMP tmp2, TEMP tmp3, KILL cr);\n+  ins_cost(2 * VOLATILE_REF_COST);\n+  format %{ \"cmpxchg $res = $mem, $oldval, $newval\\t# narrow oop\" %}\n+  ins_encode %{\n+    assert_different_registers($oldval$$Register, $mem$$Register);\n+    assert_different_registers($newval$$Register, $mem$$Register);\n+    write_barrier_pre(masm, this,\n+                      $mem$$Register \/* obj *\/,\n+                      $tmp1$$Register \/* pre_val *\/,\n+                      $tmp2$$Register \/* tmp1 *\/,\n+                      $tmp3$$Register \/* tmp2 *\/,\n+                      RegSet::of($mem$$Register, $oldval$$Register, $newval$$Register) \/* preserve *\/,\n+                      RegSet::of($res$$Register) \/* no_preserve *\/);\n+    __ cmpxchg($mem$$Register, $oldval$$Register, $newval$$Register, Assembler::word,\n+               false \/* acquire *\/, true \/* release *\/, false \/* weak *\/, $res$$Register);\n+    __ decode_heap_oop($tmp1$$Register, $newval$$Register);\n+    write_barrier_post(masm, this,\n+                       $mem$$Register \/* store_addr *\/,\n+                       $tmp1$$Register \/* new_val *\/,\n+                       $tmp2$$Register \/* tmp1 *\/,\n+                       $tmp3$$Register \/* tmp2 *\/);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ This pattern is generated automatically from g1_aarch64.m4.\n+\/\/ DO NOT EDIT ANYTHING IN THIS SECTION OF THE FILE\n+instruct g1CompareAndExchangeNAcq(iRegNNoSp res, indirect mem, iRegN oldval, iRegN newval, iRegPNoSp tmp1, iRegPNoSp tmp2, iRegPNoSp tmp3, rFlagsReg cr)\n+%{\n+  predicate(UseG1GC && needs_acquiring_load_exclusive(n) && n->as_LoadStore()->barrier_data() != 0);\n+  match(Set res (CompareAndExchangeN mem (Binary oldval newval)));\n+  effect(TEMP res, TEMP tmp1, TEMP tmp2, TEMP tmp3, KILL cr);\n+  ins_cost(VOLATILE_REF_COST);\n+  format %{ \"cmpxchg_acq $res = $mem, $oldval, $newval\\t# narrow oop\" %}\n+  ins_encode %{\n+    assert_different_registers($oldval$$Register, $mem$$Register);\n+    assert_different_registers($newval$$Register, $mem$$Register);\n+    write_barrier_pre(masm, this,\n+                      $mem$$Register \/* obj *\/,\n+                      $tmp1$$Register \/* pre_val *\/,\n+                      $tmp2$$Register \/* tmp1 *\/,\n+                      $tmp3$$Register \/* tmp2 *\/,\n+                      RegSet::of($mem$$Register, $oldval$$Register, $newval$$Register) \/* preserve *\/,\n+                      RegSet::of($res$$Register) \/* no_preserve *\/);\n+    __ cmpxchg($mem$$Register, $oldval$$Register, $newval$$Register, Assembler::word,\n+               true \/* acquire *\/, true \/* release *\/, false \/* weak *\/, $res$$Register);\n+    __ decode_heap_oop($tmp1$$Register, $newval$$Register);\n+    write_barrier_post(masm, this,\n+                       $mem$$Register \/* store_addr *\/,\n+                       $tmp1$$Register \/* new_val *\/,\n+                       $tmp2$$Register \/* tmp1 *\/,\n+                       $tmp3$$Register \/* tmp2 *\/);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ This pattern is generated automatically from g1_aarch64.m4.\n+\/\/ DO NOT EDIT ANYTHING IN THIS SECTION OF THE FILE\n+instruct g1CompareAndSwapP(iRegINoSp res, indirect mem, iRegP newval, iRegPNoSp tmp1, iRegPNoSp tmp2, iRegP oldval, rFlagsReg cr)\n+%{\n+  predicate(UseG1GC && !needs_acquiring_load_exclusive(n) && n->as_LoadStore()->barrier_data() != 0);\n+  match(Set res (CompareAndSwapP mem (Binary oldval newval)));\n+  match(Set res (WeakCompareAndSwapP mem (Binary oldval newval)));\n+  effect(TEMP res, TEMP tmp1, TEMP tmp2, KILL cr);\n+  ins_cost(2 * VOLATILE_REF_COST);\n+  format %{ \"cmpxchg $mem, $oldval, $newval\\t# (ptr)\\n\\t\"\n+            \"cset $res, EQ\" %}\n+  ins_encode %{\n+    assert_different_registers($oldval$$Register, $mem$$Register);\n+    assert_different_registers($newval$$Register, $mem$$Register);\n+    write_barrier_pre(masm, this,\n+                      noreg \/* obj *\/,\n+                      $oldval$$Register \/* pre_val *\/,\n+                      $tmp1$$Register \/* tmp1 *\/,\n+                      $tmp2$$Register \/* tmp2 *\/,\n+                      RegSet::of($mem$$Register, $oldval$$Register, $newval$$Register) \/* preserve *\/,\n+                      RegSet::of($res$$Register) \/* no_preserve *\/);\n+    __ cmpxchg($mem$$Register, $oldval$$Register, $newval$$Register, Assembler::xword,\n+               false \/* acquire *\/, true \/* release *\/, false \/* weak *\/, noreg);\n+    __ cset($res$$Register, Assembler::EQ);\n+    write_barrier_post(masm, this,\n+                       $mem$$Register \/* store_addr *\/,\n+                       $newval$$Register \/* new_val *\/,\n+                       $tmp1$$Register \/* tmp1 *\/,\n+                       $tmp2$$Register \/* tmp2 *\/);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ This pattern is generated automatically from g1_aarch64.m4.\n+\/\/ DO NOT EDIT ANYTHING IN THIS SECTION OF THE FILE\n+instruct g1CompareAndSwapPAcq(iRegINoSp res, indirect mem, iRegP newval, iRegPNoSp tmp1, iRegPNoSp tmp2, iRegP oldval, rFlagsReg cr)\n+%{\n+  predicate(UseG1GC && needs_acquiring_load_exclusive(n) && n->as_LoadStore()->barrier_data() != 0);\n+  match(Set res (CompareAndSwapP mem (Binary oldval newval)));\n+  match(Set res (WeakCompareAndSwapP mem (Binary oldval newval)));\n+  effect(TEMP res, TEMP tmp1, TEMP tmp2, KILL cr);\n+  ins_cost(VOLATILE_REF_COST);\n+  format %{ \"cmpxchg_acq $mem, $oldval, $newval\\t# (ptr)\\n\\t\"\n+            \"cset $res, EQ\" %}\n+  ins_encode %{\n+    assert_different_registers($oldval$$Register, $mem$$Register);\n+    assert_different_registers($newval$$Register, $mem$$Register);\n+    write_barrier_pre(masm, this,\n+                      noreg \/* obj *\/,\n+                      $oldval$$Register \/* pre_val *\/,\n+                      $tmp1$$Register \/* tmp1 *\/,\n+                      $tmp2$$Register \/* tmp2 *\/,\n+                      RegSet::of($mem$$Register, $oldval$$Register, $newval$$Register) \/* preserve *\/,\n+                      RegSet::of($res$$Register) \/* no_preserve *\/);\n+    __ cmpxchg($mem$$Register, $oldval$$Register, $newval$$Register, Assembler::xword,\n+               true \/* acquire *\/, true \/* release *\/, false \/* weak *\/, noreg);\n+    __ cset($res$$Register, Assembler::EQ);\n+    write_barrier_post(masm, this,\n+                       $mem$$Register \/* store_addr *\/,\n+                       $newval$$Register \/* new_val *\/,\n+                       $tmp1$$Register \/* tmp1 *\/,\n+                       $tmp2$$Register \/* tmp2 *\/);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ This pattern is generated automatically from g1_aarch64.m4.\n+\/\/ DO NOT EDIT ANYTHING IN THIS SECTION OF THE FILE\n+instruct g1CompareAndSwapN(iRegINoSp res, indirect mem, iRegN newval, iRegPNoSp tmp1, iRegPNoSp tmp2, iRegPNoSp tmp3, iRegN oldval, rFlagsReg cr)\n+%{\n+  predicate(UseG1GC && !needs_acquiring_load_exclusive(n) && n->as_LoadStore()->barrier_data() != 0);\n+  match(Set res (CompareAndSwapN mem (Binary oldval newval)));\n+  match(Set res (WeakCompareAndSwapN mem (Binary oldval newval)));\n+  effect(TEMP res, TEMP tmp1, TEMP tmp2, TEMP tmp3, KILL cr);\n+  ins_cost(2 * VOLATILE_REF_COST);\n+  format %{ \"cmpxchg $mem, $oldval, $newval\\t# (narrow oop)\\n\\t\"\n+            \"cset $res, EQ\" %}\n+  ins_encode %{\n+    assert_different_registers($oldval$$Register, $mem$$Register);\n+    assert_different_registers($newval$$Register, $mem$$Register);\n+    write_barrier_pre(masm, this,\n+                      $mem$$Register \/* obj *\/,\n+                      $tmp1$$Register \/* pre_val *\/,\n+                      $tmp2$$Register \/* tmp1 *\/,\n+                      $tmp3$$Register \/* tmp2 *\/,\n+                      RegSet::of($mem$$Register, $oldval$$Register, $newval$$Register) \/* preserve *\/,\n+                      RegSet::of($res$$Register) \/* no_preserve *\/);\n+    __ cmpxchg($mem$$Register, $oldval$$Register, $newval$$Register, Assembler::word,\n+               false \/* acquire *\/, true \/* release *\/, false \/* weak *\/, noreg);\n+    __ cset($res$$Register, Assembler::EQ);\n+    __ decode_heap_oop($tmp1$$Register, $newval$$Register);\n+    write_barrier_post(masm, this,\n+                       $mem$$Register \/* store_addr *\/,\n+                       $tmp1$$Register \/* new_val *\/,\n+                       $tmp2$$Register \/* tmp1 *\/,\n+                       $tmp3$$Register \/* tmp2 *\/);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ This pattern is generated automatically from g1_aarch64.m4.\n+\/\/ DO NOT EDIT ANYTHING IN THIS SECTION OF THE FILE\n+instruct g1CompareAndSwapNAcq(iRegINoSp res, indirect mem, iRegN newval, iRegPNoSp tmp1, iRegPNoSp tmp2, iRegPNoSp tmp3, iRegN oldval, rFlagsReg cr)\n+%{\n+  predicate(UseG1GC && needs_acquiring_load_exclusive(n) && n->as_LoadStore()->barrier_data() != 0);\n+  match(Set res (CompareAndSwapN mem (Binary oldval newval)));\n+  match(Set res (WeakCompareAndSwapN mem (Binary oldval newval)));\n+  effect(TEMP res, TEMP tmp1, TEMP tmp2, TEMP tmp3, KILL cr);\n+  ins_cost(VOLATILE_REF_COST);\n+  format %{ \"cmpxchg_acq $mem, $oldval, $newval\\t# (narrow oop)\\n\\t\"\n+            \"cset $res, EQ\" %}\n+  ins_encode %{\n+    assert_different_registers($oldval$$Register, $mem$$Register);\n+    assert_different_registers($newval$$Register, $mem$$Register);\n+    write_barrier_pre(masm, this,\n+                      $mem$$Register \/* obj *\/,\n+                      $tmp1$$Register \/* pre_val *\/,\n+                      $tmp2$$Register \/* tmp1 *\/,\n+                      $tmp3$$Register \/* tmp2 *\/,\n+                      RegSet::of($mem$$Register, $oldval$$Register, $newval$$Register) \/* preserve *\/,\n+                      RegSet::of($res$$Register) \/* no_preserve *\/);\n+    __ cmpxchg($mem$$Register, $oldval$$Register, $newval$$Register, Assembler::word,\n+               true \/* acquire *\/, true \/* release *\/, false \/* weak *\/, noreg);\n+    __ cset($res$$Register, Assembler::EQ);\n+    __ decode_heap_oop($tmp1$$Register, $newval$$Register);\n+    write_barrier_post(masm, this,\n+                       $mem$$Register \/* store_addr *\/,\n+                       $tmp1$$Register \/* new_val *\/,\n+                       $tmp2$$Register \/* tmp1 *\/,\n+                       $tmp3$$Register \/* tmp2 *\/);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ This pattern is generated automatically from g1_aarch64.m4.\n+\/\/ DO NOT EDIT ANYTHING IN THIS SECTION OF THE FILE\n+instruct g1GetAndSetP(indirect mem, iRegP newval, iRegPNoSp tmp1, iRegPNoSp tmp2, iRegPNoSp preval, rFlagsReg cr)\n+%{\n+  predicate(UseG1GC && !needs_acquiring_load_exclusive(n) && n->as_LoadStore()->barrier_data() != 0);\n+  match(Set preval (GetAndSetP mem newval));\n+  effect(TEMP preval, TEMP tmp1, TEMP tmp2, KILL cr);\n+  ins_cost(2 * VOLATILE_REF_COST);\n+  format %{ \"atomic_xchg  $preval, $newval, [$mem]\" %}\n+  ins_encode %{\n+    assert_different_registers($mem$$Register, $newval$$Register);\n+    write_barrier_pre(masm, this,\n+                      $mem$$Register \/* obj *\/,\n+                      $preval$$Register \/* pre_val (as a temporary register) *\/,\n+                      $tmp1$$Register \/* tmp1 *\/,\n+                      $tmp2$$Register \/* tmp2 *\/,\n+                      RegSet::of($mem$$Register, $preval$$Register, $newval$$Register) \/* preserve *\/);\n+    __ atomic_xchg($preval$$Register, $newval$$Register, $mem$$Register);\n+    write_barrier_post(masm, this,\n+                       $mem$$Register \/* store_addr *\/,\n+                       $newval$$Register \/* new_val *\/,\n+                       $tmp1$$Register \/* tmp1 *\/,\n+                       $tmp2$$Register \/* tmp2 *\/);\n+  %}\n+  ins_pipe(pipe_serial);\n+%}\n+\n+\/\/ This pattern is generated automatically from g1_aarch64.m4.\n+\/\/ DO NOT EDIT ANYTHING IN THIS SECTION OF THE FILE\n+instruct g1GetAndSetPAcq(indirect mem, iRegP newval, iRegPNoSp tmp1, iRegPNoSp tmp2, iRegPNoSp preval, rFlagsReg cr)\n+%{\n+  predicate(UseG1GC && needs_acquiring_load_exclusive(n) && n->as_LoadStore()->barrier_data() != 0);\n+  match(Set preval (GetAndSetP mem newval));\n+  effect(TEMP preval, TEMP tmp1, TEMP tmp2, KILL cr);\n+  ins_cost(VOLATILE_REF_COST);\n+  format %{ \"atomic_xchg_acq  $preval, $newval, [$mem]\" %}\n+  ins_encode %{\n+    assert_different_registers($mem$$Register, $newval$$Register);\n+    write_barrier_pre(masm, this,\n+                      $mem$$Register \/* obj *\/,\n+                      $preval$$Register \/* pre_val (as a temporary register) *\/,\n+                      $tmp1$$Register \/* tmp1 *\/,\n+                      $tmp2$$Register \/* tmp2 *\/,\n+                      RegSet::of($mem$$Register, $preval$$Register, $newval$$Register) \/* preserve *\/);\n+    __ atomic_xchgal($preval$$Register, $newval$$Register, $mem$$Register);\n+    write_barrier_post(masm, this,\n+                       $mem$$Register \/* store_addr *\/,\n+                       $newval$$Register \/* new_val *\/,\n+                       $tmp1$$Register \/* tmp1 *\/,\n+                       $tmp2$$Register \/* tmp2 *\/);\n+  %}\n+  ins_pipe(pipe_serial);\n+%}\n+\n+\/\/ This pattern is generated automatically from g1_aarch64.m4.\n+\/\/ DO NOT EDIT ANYTHING IN THIS SECTION OF THE FILE\n+instruct g1GetAndSetN(indirect mem, iRegN newval, iRegPNoSp tmp1, iRegPNoSp tmp2, iRegPNoSp tmp3, iRegNNoSp preval, rFlagsReg cr)\n+%{\n+  predicate(UseG1GC && !needs_acquiring_load_exclusive(n) && n->as_LoadStore()->barrier_data() != 0);\n+  match(Set preval (GetAndSetN mem newval));\n+  effect(TEMP preval, TEMP tmp1, TEMP tmp2, TEMP tmp3, KILL cr);\n+  ins_cost(2 * VOLATILE_REF_COST);\n+  format %{ \"atomic_xchgw $preval, $newval, [$mem]\" %}\n+  ins_encode %{\n+    assert_different_registers($mem$$Register, $newval$$Register);\n+    write_barrier_pre(masm, this,\n+                      $mem$$Register \/* obj *\/,\n+                      $tmp1$$Register \/* pre_val *\/,\n+                      $tmp2$$Register \/* tmp1 *\/,\n+                      $tmp3$$Register \/* tmp2 *\/,\n+                      RegSet::of($mem$$Register, $preval$$Register, $newval$$Register) \/* preserve *\/);\n+    __ atomic_xchgw($preval$$Register, $newval$$Register, $mem$$Register);\n+    __ decode_heap_oop($tmp1$$Register, $newval$$Register);\n+    write_barrier_post(masm, this,\n+                       $mem$$Register \/* store_addr *\/,\n+                       $tmp1$$Register \/* new_val *\/,\n+                       $tmp2$$Register \/* tmp1 *\/,\n+                       $tmp3$$Register \/* tmp2 *\/);\n+  %}\n+  ins_pipe(pipe_serial);\n+%}\n+\n+\/\/ This pattern is generated automatically from g1_aarch64.m4.\n+\/\/ DO NOT EDIT ANYTHING IN THIS SECTION OF THE FILE\n+instruct g1GetAndSetNAcq(indirect mem, iRegN newval, iRegPNoSp tmp1, iRegPNoSp tmp2, iRegPNoSp tmp3, iRegNNoSp preval, rFlagsReg cr)\n+%{\n+  predicate(UseG1GC && needs_acquiring_load_exclusive(n) && n->as_LoadStore()->barrier_data() != 0);\n+  match(Set preval (GetAndSetN mem newval));\n+  effect(TEMP preval, TEMP tmp1, TEMP tmp2, TEMP tmp3, KILL cr);\n+  ins_cost(VOLATILE_REF_COST);\n+  format %{ \"atomic_xchgw_acq $preval, $newval, [$mem]\" %}\n+  ins_encode %{\n+    assert_different_registers($mem$$Register, $newval$$Register);\n+    write_barrier_pre(masm, this,\n+                      $mem$$Register \/* obj *\/,\n+                      $tmp1$$Register \/* pre_val *\/,\n+                      $tmp2$$Register \/* tmp1 *\/,\n+                      $tmp3$$Register \/* tmp2 *\/,\n+                      RegSet::of($mem$$Register, $preval$$Register, $newval$$Register) \/* preserve *\/);\n+    __ atomic_xchgalw($preval$$Register, $newval$$Register, $mem$$Register);\n+    __ decode_heap_oop($tmp1$$Register, $newval$$Register);\n+    write_barrier_post(masm, this,\n+                       $mem$$Register \/* store_addr *\/,\n+                       $tmp1$$Register \/* new_val *\/,\n+                       $tmp2$$Register \/* tmp1 *\/,\n+                       $tmp3$$Register \/* tmp2 *\/);\n+  %}\n+  ins_pipe(pipe_serial);\n+%}\n+\n+\/\/ This pattern is generated automatically from g1_aarch64.m4.\n+\/\/ DO NOT EDIT ANYTHING IN THIS SECTION OF THE FILE\n+instruct g1LoadP(iRegPNoSp dst, indirect mem, iRegPNoSp tmp1, iRegPNoSp tmp2, rFlagsReg cr)\n+%{\n+  \/\/ This instruction does not need an acquiring counterpart because it is only\n+  \/\/ used for reference loading (Reference::get()). The same holds for g1LoadN.\n+  predicate(UseG1GC && !needs_acquiring_load(n) && n->as_Load()->barrier_data() != 0);\n+  match(Set dst (LoadP mem));\n+  effect(TEMP dst, TEMP tmp1, TEMP tmp2, KILL cr);\n+  ins_cost(4 * INSN_COST);\n+  format %{ \"ldr  $dst, $mem\\t# ptr\" %}\n+  ins_encode %{\n+    __ ldr($dst$$Register, $mem$$Register);\n+    write_barrier_pre(masm, this,\n+                      noreg \/* obj *\/,\n+                      $dst$$Register \/* pre_val *\/,\n+                      $tmp1$$Register \/* tmp1 *\/,\n+                      $tmp2$$Register \/* tmp2 *\/);\n+  %}\n+  ins_pipe(iload_reg_mem);\n+%}\n+\n+\/\/ This pattern is generated automatically from g1_aarch64.m4.\n+\/\/ DO NOT EDIT ANYTHING IN THIS SECTION OF THE FILE\n+instruct g1LoadN(iRegNNoSp dst, indirect mem, iRegPNoSp tmp1, iRegPNoSp tmp2, iRegPNoSp tmp3, rFlagsReg cr)\n+%{\n+  predicate(UseG1GC && !needs_acquiring_load(n) && n->as_Load()->barrier_data() != 0);\n+  match(Set dst (LoadN mem));\n+  effect(TEMP dst, TEMP tmp1, TEMP tmp2, TEMP tmp3, KILL cr);\n+  ins_cost(4 * INSN_COST);\n+  format %{ \"ldrw  $dst, $mem\\t# compressed ptr\" %}\n+  ins_encode %{\n+    __ ldrw($dst$$Register, $mem$$Register);\n+    if ((barrier_data() & G1C2BarrierPre) != 0) {\n+      __ decode_heap_oop($tmp1$$Register, $dst$$Register);\n+      write_barrier_pre(masm, this,\n+                        noreg \/* obj *\/,\n+                        $tmp1$$Register \/* pre_val *\/,\n+                        $tmp2$$Register \/* tmp1 *\/,\n+                        $tmp3$$Register \/* tmp2 *\/);\n+    }\n+  %}\n+  ins_pipe(iload_reg_mem);\n+%}\n+\n+\/\/ END This section of the file is automatically generated. Do not edit --------------\n","filename":"src\/hotspot\/cpu\/aarch64\/gc\/g1\/g1_aarch64.ad","additions":680,"deletions":0,"binary":false,"changes":680,"status":"added"},{"patch":"@@ -0,0 +1,384 @@\n+dnl Copyright (c) 2024, Oracle and\/or its affiliates. All rights reserved.\n+dnl DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+dnl\n+dnl This code is free software; you can redistribute it and\/or modify it\n+dnl under the terms of the GNU General Public License version 2 only, as\n+dnl published by the Free Software Foundation.\n+dnl\n+dnl This code is distributed in the hope that it will be useful, but WITHOUT\n+dnl ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+dnl FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+dnl version 2 for more details (a copy is included in the LICENSE file that\n+dnl accompanied this code).\n+dnl\n+dnl You should have received a copy of the GNU General Public License version\n+dnl 2 along with this work; if not, write to the Free Software Foundation,\n+dnl Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+dnl\n+dnl Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+dnl or visit www.oracle.com if you need additional information or have any\n+dnl questions.\n+dnl\n+\/\/ BEGIN This section of the file is automatically generated. Do not edit --------------\n+\n+\/\/ This section is generated from g1_aarch64.m4\n+\n+define(`STOREP_INSN',\n+`\n+\/\/ This pattern is generated automatically from g1_aarch64.m4.\n+\/\/ DO NOT EDIT ANYTHING IN THIS SECTION OF THE FILE\n+instruct g1StoreP$1(indirect mem, iRegP src, iRegPNoSp tmp1, iRegPNoSp tmp2, iRegPNoSp tmp3, rFlagsReg cr)\n+%{\n+  predicate(UseG1GC && ifelse($1,Volatile,'needs_releasing_store(n)`,'!needs_releasing_store(n)`) && n->as_Store()->barrier_data() != 0);\n+  match(Set mem (StoreP mem src));\n+  effect(TEMP tmp1, TEMP tmp2, TEMP tmp3, KILL cr);\n+  ins_cost(ifelse($1,Volatile,VOLATILE_REF_COST,INSN_COST));\n+  format %{ \"$2  $src, $mem\\t# ptr\" %}\n+  ins_encode %{\n+    write_barrier_pre(masm, this,\n+                      $mem$$Register \/* obj *\/,\n+                      $tmp1$$Register \/* pre_val *\/,\n+                      $tmp2$$Register \/* tmp1 *\/,\n+                      $tmp3$$Register \/* tmp2 *\/,\n+                      RegSet::of($mem$$Register, $src$$Register) \/* preserve *\/);\n+    __ $2($src$$Register, $mem$$Register);\n+    write_barrier_post(masm, this,\n+                       $mem$$Register \/* store_addr *\/,\n+                       $src$$Register \/* new_val *\/,\n+                       $tmp2$$Register \/* tmp1 *\/,\n+                       $tmp3$$Register \/* tmp2 *\/);\n+  %}\n+  ins_pipe(ifelse($1,Volatile,pipe_class_memory,istore_reg_mem));\n+%}')dnl\n+STOREP_INSN(,str)\n+STOREP_INSN(Volatile,stlr)\n+dnl\n+define(`STOREN_INSN',\n+`\n+\/\/ This pattern is generated automatically from g1_aarch64.m4.\n+\/\/ DO NOT EDIT ANYTHING IN THIS SECTION OF THE FILE\n+instruct g1StoreN$1(indirect mem, iRegN src, iRegPNoSp tmp1, iRegPNoSp tmp2, iRegPNoSp tmp3, rFlagsReg cr)\n+%{\n+  predicate(UseG1GC && ifelse($1,Volatile,'needs_releasing_store(n)`,'!needs_releasing_store(n)`) && n->as_Store()->barrier_data() != 0);\n+  match(Set mem (StoreN mem src));\n+  effect(TEMP tmp1, TEMP tmp2, TEMP tmp3, KILL cr);\n+  ins_cost(ifelse($1,Volatile,VOLATILE_REF_COST,INSN_COST));\n+  format %{ \"$2  $src, $mem\\t# compressed ptr\" %}\n+  ins_encode %{\n+    write_barrier_pre(masm, this,\n+                      $mem$$Register \/* obj *\/,\n+                      $tmp1$$Register \/* pre_val *\/,\n+                      $tmp2$$Register \/* tmp1 *\/,\n+                      $tmp3$$Register \/* tmp2 *\/,\n+                      RegSet::of($mem$$Register, $src$$Register) \/* preserve *\/);\n+    __ $2($src$$Register, $mem$$Register);\n+    if ((barrier_data() & G1C2BarrierPost) != 0) {\n+      if ((barrier_data() & G1C2BarrierPostNotNull) == 0) {\n+        __ decode_heap_oop($tmp1$$Register, $src$$Register);\n+      } else {\n+        __ decode_heap_oop_not_null($tmp1$$Register, $src$$Register);\n+      }\n+    }\n+    write_barrier_post(masm, this,\n+                       $mem$$Register \/* store_addr *\/,\n+                       $tmp1$$Register \/* new_val *\/,\n+                       $tmp2$$Register \/* tmp1 *\/,\n+                       $tmp3$$Register \/* tmp2 *\/);\n+  %}\n+  ins_pipe(ifelse($1,Volatile,pipe_class_memory,istore_reg_mem));\n+%}')dnl\n+STOREN_INSN(,strw)\n+STOREN_INSN(Volatile,stlrw)\n+dnl\n+define(`ENCODESTOREN_INSN',\n+`\n+\/\/ This pattern is generated automatically from g1_aarch64.m4.\n+\/\/ DO NOT EDIT ANYTHING IN THIS SECTION OF THE FILE\n+instruct g1EncodePAndStoreN$1(indirect mem, iRegP src, iRegPNoSp tmp1, iRegPNoSp tmp2, iRegPNoSp tmp3, rFlagsReg cr)\n+%{\n+  predicate(UseG1GC && ifelse($1,Volatile,'needs_releasing_store(n)`,'!needs_releasing_store(n)`) && n->as_Store()->barrier_data() != 0);\n+  match(Set mem (StoreN mem (EncodeP src)));\n+  effect(TEMP tmp1, TEMP tmp2, TEMP tmp3, KILL cr);\n+  ins_cost(ifelse($1,Volatile,VOLATILE_REF_COST,INSN_COST));\n+  format %{ \"encode_heap_oop $tmp1, $src\\n\\t\"\n+            \"$2  $tmp1, $mem\\t# compressed ptr\" %}\n+  ins_encode %{\n+    write_barrier_pre(masm, this,\n+                      $mem$$Register \/* obj *\/,\n+                      $tmp1$$Register \/* pre_val *\/,\n+                      $tmp2$$Register \/* tmp1 *\/,\n+                      $tmp3$$Register \/* tmp2 *\/,\n+                      RegSet::of($mem$$Register, $src$$Register) \/* preserve *\/);\n+    if ((barrier_data() & G1C2BarrierPostNotNull) == 0) {\n+      __ encode_heap_oop($tmp1$$Register, $src$$Register);\n+    } else {\n+      __ encode_heap_oop_not_null($tmp1$$Register, $src$$Register);\n+    }\n+    __ $2($tmp1$$Register, $mem$$Register);\n+    write_barrier_post(masm, this,\n+                       $mem$$Register \/* store_addr *\/,\n+                       $src$$Register \/* new_val *\/,\n+                       $tmp2$$Register \/* tmp1 *\/,\n+                       $tmp3$$Register \/* tmp2 *\/);\n+  %}\n+  ins_pipe(ifelse($1,Volatile,pipe_class_memory,istore_reg_mem));\n+%}')dnl\n+ENCODESTOREN_INSN(,strw)\n+ENCODESTOREN_INSN(Volatile,stlrw)\n+dnl\n+define(`CAEP_INSN',\n+`\n+\/\/ This pattern is generated automatically from g1_aarch64.m4.\n+\/\/ DO NOT EDIT ANYTHING IN THIS SECTION OF THE FILE\n+instruct g1CompareAndExchangeP$1(iRegPNoSp res, indirect mem, iRegP oldval, iRegP newval, iRegPNoSp tmp1, iRegPNoSp tmp2, rFlagsReg cr)\n+%{\n+  predicate(UseG1GC && ifelse($1,Acq,'needs_acquiring_load_exclusive(n)`,'!needs_acquiring_load_exclusive(n)`) && n->as_LoadStore()->barrier_data() != 0);\n+  match(Set res (CompareAndExchangeP mem (Binary oldval newval)));\n+  effect(TEMP res, TEMP tmp1, TEMP tmp2, KILL cr);\n+  ins_cost(ifelse($1,Acq,VOLATILE_REF_COST,2 * VOLATILE_REF_COST));\n+  format %{ \"cmpxchg$2 $res = $mem, $oldval, $newval\\t# ptr\" %}\n+  ins_encode %{\n+    assert_different_registers($oldval$$Register, $mem$$Register);\n+    assert_different_registers($newval$$Register, $mem$$Register);\n+    \/\/ Pass $oldval to the pre-barrier (instead of loading from $mem), because\n+    \/\/ $oldval is the only value that can be overwritten.\n+    \/\/ The same holds for g1CompareAndSwapP and its Acq variant.\n+    write_barrier_pre(masm, this,\n+                      noreg \/* obj *\/,\n+                      $oldval$$Register \/* pre_val *\/,\n+                      $tmp1$$Register \/* tmp1 *\/,\n+                      $tmp2$$Register \/* tmp2 *\/,\n+                      RegSet::of($mem$$Register, $oldval$$Register, $newval$$Register) \/* preserve *\/,\n+                      RegSet::of($res$$Register) \/* no_preserve *\/);\n+    __ cmpxchg($mem$$Register, $oldval$$Register, $newval$$Register, Assembler::xword,\n+               $3 \/* acquire *\/, true \/* release *\/, false \/* weak *\/, $res$$Register);\n+    write_barrier_post(masm, this,\n+                       $mem$$Register \/* store_addr *\/,\n+                       $newval$$Register \/* new_val *\/,\n+                       $tmp1$$Register \/* tmp1 *\/,\n+                       $tmp2$$Register \/* tmp2 *\/);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}')dnl\n+CAEP_INSN(,,false)\n+CAEP_INSN(Acq,_acq,true)\n+dnl\n+define(`CAEN_INSN',\n+`\n+\/\/ This pattern is generated automatically from g1_aarch64.m4.\n+\/\/ DO NOT EDIT ANYTHING IN THIS SECTION OF THE FILE\n+instruct g1CompareAndExchangeN$1(iRegNNoSp res, indirect mem, iRegN oldval, iRegN newval, iRegPNoSp tmp1, iRegPNoSp tmp2, iRegPNoSp tmp3, rFlagsReg cr)\n+%{\n+  predicate(UseG1GC && ifelse($1,Acq,'needs_acquiring_load_exclusive(n)`,'!needs_acquiring_load_exclusive(n)`) && n->as_LoadStore()->barrier_data() != 0);\n+  match(Set res (CompareAndExchangeN mem (Binary oldval newval)));\n+  effect(TEMP res, TEMP tmp1, TEMP tmp2, TEMP tmp3, KILL cr);\n+  ins_cost(ifelse($1,Acq,VOLATILE_REF_COST,2 * VOLATILE_REF_COST));\n+  format %{ \"cmpxchg$2 $res = $mem, $oldval, $newval\\t# narrow oop\" %}\n+  ins_encode %{\n+    assert_different_registers($oldval$$Register, $mem$$Register);\n+    assert_different_registers($newval$$Register, $mem$$Register);\n+    write_barrier_pre(masm, this,\n+                      $mem$$Register \/* obj *\/,\n+                      $tmp1$$Register \/* pre_val *\/,\n+                      $tmp2$$Register \/* tmp1 *\/,\n+                      $tmp3$$Register \/* tmp2 *\/,\n+                      RegSet::of($mem$$Register, $oldval$$Register, $newval$$Register) \/* preserve *\/,\n+                      RegSet::of($res$$Register) \/* no_preserve *\/);\n+    __ cmpxchg($mem$$Register, $oldval$$Register, $newval$$Register, Assembler::word,\n+               $3 \/* acquire *\/, true \/* release *\/, false \/* weak *\/, $res$$Register);\n+    __ decode_heap_oop($tmp1$$Register, $newval$$Register);\n+    write_barrier_post(masm, this,\n+                       $mem$$Register \/* store_addr *\/,\n+                       $tmp1$$Register \/* new_val *\/,\n+                       $tmp2$$Register \/* tmp1 *\/,\n+                       $tmp3$$Register \/* tmp2 *\/);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}')dnl\n+CAEN_INSN(,,false)\n+CAEN_INSN(Acq,_acq,true)\n+dnl\n+define(`CASP_INSN',\n+`\n+\/\/ This pattern is generated automatically from g1_aarch64.m4.\n+\/\/ DO NOT EDIT ANYTHING IN THIS SECTION OF THE FILE\n+instruct g1CompareAndSwapP$1(iRegINoSp res, indirect mem, iRegP newval, iRegPNoSp tmp1, iRegPNoSp tmp2, iRegP oldval, rFlagsReg cr)\n+%{\n+  predicate(UseG1GC && ifelse($1,Acq,'needs_acquiring_load_exclusive(n)`,'!needs_acquiring_load_exclusive(n)`) && n->as_LoadStore()->barrier_data() != 0);\n+  match(Set res (CompareAndSwapP mem (Binary oldval newval)));\n+  match(Set res (WeakCompareAndSwapP mem (Binary oldval newval)));\n+  effect(TEMP res, TEMP tmp1, TEMP tmp2, KILL cr);\n+  ins_cost(ifelse($1,Acq,VOLATILE_REF_COST,2 * VOLATILE_REF_COST));\n+  format %{ \"cmpxchg$2 $mem, $oldval, $newval\\t# (ptr)\\n\\t\"\n+            \"cset $res, EQ\" %}\n+  ins_encode %{\n+    assert_different_registers($oldval$$Register, $mem$$Register);\n+    assert_different_registers($newval$$Register, $mem$$Register);\n+    write_barrier_pre(masm, this,\n+                      noreg \/* obj *\/,\n+                      $oldval$$Register \/* pre_val *\/,\n+                      $tmp1$$Register \/* tmp1 *\/,\n+                      $tmp2$$Register \/* tmp2 *\/,\n+                      RegSet::of($mem$$Register, $oldval$$Register, $newval$$Register) \/* preserve *\/,\n+                      RegSet::of($res$$Register) \/* no_preserve *\/);\n+    __ cmpxchg($mem$$Register, $oldval$$Register, $newval$$Register, Assembler::xword,\n+               $3 \/* acquire *\/, true \/* release *\/, false \/* weak *\/, noreg);\n+    __ cset($res$$Register, Assembler::EQ);\n+    write_barrier_post(masm, this,\n+                       $mem$$Register \/* store_addr *\/,\n+                       $newval$$Register \/* new_val *\/,\n+                       $tmp1$$Register \/* tmp1 *\/,\n+                       $tmp2$$Register \/* tmp2 *\/);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}')dnl\n+CASP_INSN(,,false)\n+CASP_INSN(Acq,_acq,true)\n+dnl\n+define(`CASN_INSN',\n+`\n+\/\/ This pattern is generated automatically from g1_aarch64.m4.\n+\/\/ DO NOT EDIT ANYTHING IN THIS SECTION OF THE FILE\n+instruct g1CompareAndSwapN$1(iRegINoSp res, indirect mem, iRegN newval, iRegPNoSp tmp1, iRegPNoSp tmp2, iRegPNoSp tmp3, iRegN oldval, rFlagsReg cr)\n+%{\n+  predicate(UseG1GC && ifelse($1,Acq,'needs_acquiring_load_exclusive(n)`,'!needs_acquiring_load_exclusive(n)`) && n->as_LoadStore()->barrier_data() != 0);\n+  match(Set res (CompareAndSwapN mem (Binary oldval newval)));\n+  match(Set res (WeakCompareAndSwapN mem (Binary oldval newval)));\n+  effect(TEMP res, TEMP tmp1, TEMP tmp2, TEMP tmp3, KILL cr);\n+  ins_cost(ifelse($1,Acq,VOLATILE_REF_COST,2 * VOLATILE_REF_COST));\n+  format %{ \"cmpxchg$2 $mem, $oldval, $newval\\t# (narrow oop)\\n\\t\"\n+            \"cset $res, EQ\" %}\n+  ins_encode %{\n+    assert_different_registers($oldval$$Register, $mem$$Register);\n+    assert_different_registers($newval$$Register, $mem$$Register);\n+    write_barrier_pre(masm, this,\n+                      $mem$$Register \/* obj *\/,\n+                      $tmp1$$Register \/* pre_val *\/,\n+                      $tmp2$$Register \/* tmp1 *\/,\n+                      $tmp3$$Register \/* tmp2 *\/,\n+                      RegSet::of($mem$$Register, $oldval$$Register, $newval$$Register) \/* preserve *\/,\n+                      RegSet::of($res$$Register) \/* no_preserve *\/);\n+    __ cmpxchg($mem$$Register, $oldval$$Register, $newval$$Register, Assembler::word,\n+               $3 \/* acquire *\/, true \/* release *\/, false \/* weak *\/, noreg);\n+    __ cset($res$$Register, Assembler::EQ);\n+    __ decode_heap_oop($tmp1$$Register, $newval$$Register);\n+    write_barrier_post(masm, this,\n+                       $mem$$Register \/* store_addr *\/,\n+                       $tmp1$$Register \/* new_val *\/,\n+                       $tmp2$$Register \/* tmp1 *\/,\n+                       $tmp3$$Register \/* tmp2 *\/);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}')dnl\n+CASN_INSN(,,false)\n+CASN_INSN(Acq,_acq,true)\n+dnl\n+define(`XCHGP_INSN',\n+`\n+\/\/ This pattern is generated automatically from g1_aarch64.m4.\n+\/\/ DO NOT EDIT ANYTHING IN THIS SECTION OF THE FILE\n+instruct g1GetAndSetP$1(indirect mem, iRegP newval, iRegPNoSp tmp1, iRegPNoSp tmp2, iRegPNoSp preval, rFlagsReg cr)\n+%{\n+  predicate(UseG1GC && ifelse($1,Acq,'needs_acquiring_load_exclusive(n)`,'!needs_acquiring_load_exclusive(n)`) && n->as_LoadStore()->barrier_data() != 0);\n+  match(Set preval (GetAndSetP mem newval));\n+  effect(TEMP preval, TEMP tmp1, TEMP tmp2, KILL cr);\n+  ins_cost(ifelse($1,Acq,VOLATILE_REF_COST,2 * VOLATILE_REF_COST));\n+  format %{ \"atomic_xchg$2  $preval, $newval, [$mem]\" %}\n+  ins_encode %{\n+    assert_different_registers($mem$$Register, $newval$$Register);\n+    write_barrier_pre(masm, this,\n+                      $mem$$Register \/* obj *\/,\n+                      $preval$$Register \/* pre_val (as a temporary register) *\/,\n+                      $tmp1$$Register \/* tmp1 *\/,\n+                      $tmp2$$Register \/* tmp2 *\/,\n+                      RegSet::of($mem$$Register, $preval$$Register, $newval$$Register) \/* preserve *\/);\n+    __ $3($preval$$Register, $newval$$Register, $mem$$Register);\n+    write_barrier_post(masm, this,\n+                       $mem$$Register \/* store_addr *\/,\n+                       $newval$$Register \/* new_val *\/,\n+                       $tmp1$$Register \/* tmp1 *\/,\n+                       $tmp2$$Register \/* tmp2 *\/);\n+  %}\n+  ins_pipe(pipe_serial);\n+%}')dnl\n+XCHGP_INSN(,,atomic_xchg)\n+XCHGP_INSN(Acq,_acq,atomic_xchgal)\n+dnl\n+define(`XCHGN_INSN',\n+`\n+\/\/ This pattern is generated automatically from g1_aarch64.m4.\n+\/\/ DO NOT EDIT ANYTHING IN THIS SECTION OF THE FILE\n+instruct g1GetAndSetN$1(indirect mem, iRegN newval, iRegPNoSp tmp1, iRegPNoSp tmp2, iRegPNoSp tmp3, iRegNNoSp preval, rFlagsReg cr)\n+%{\n+  predicate(UseG1GC && ifelse($1,Acq,'needs_acquiring_load_exclusive(n)`,'!needs_acquiring_load_exclusive(n)`) && n->as_LoadStore()->barrier_data() != 0);\n+  match(Set preval (GetAndSetN mem newval));\n+  effect(TEMP preval, TEMP tmp1, TEMP tmp2, TEMP tmp3, KILL cr);\n+  ins_cost(ifelse($1,Acq,VOLATILE_REF_COST,2 * VOLATILE_REF_COST));\n+  format %{ \"$2 $preval, $newval, [$mem]\" %}\n+  ins_encode %{\n+    assert_different_registers($mem$$Register, $newval$$Register);\n+    write_barrier_pre(masm, this,\n+                      $mem$$Register \/* obj *\/,\n+                      $tmp1$$Register \/* pre_val *\/,\n+                      $tmp2$$Register \/* tmp1 *\/,\n+                      $tmp3$$Register \/* tmp2 *\/,\n+                      RegSet::of($mem$$Register, $preval$$Register, $newval$$Register) \/* preserve *\/);\n+    __ $3($preval$$Register, $newval$$Register, $mem$$Register);\n+    __ decode_heap_oop($tmp1$$Register, $newval$$Register);\n+    write_barrier_post(masm, this,\n+                       $mem$$Register \/* store_addr *\/,\n+                       $tmp1$$Register \/* new_val *\/,\n+                       $tmp2$$Register \/* tmp1 *\/,\n+                       $tmp3$$Register \/* tmp2 *\/);\n+  %}\n+  ins_pipe(pipe_serial);\n+%}')dnl\n+XCHGN_INSN(,atomic_xchgw,atomic_xchgw)\n+XCHGN_INSN(Acq,atomic_xchgw_acq,atomic_xchgalw)\n+\n+\/\/ This pattern is generated automatically from g1_aarch64.m4.\n+\/\/ DO NOT EDIT ANYTHING IN THIS SECTION OF THE FILE\n+instruct g1LoadP(iRegPNoSp dst, indirect mem, iRegPNoSp tmp1, iRegPNoSp tmp2, rFlagsReg cr)\n+%{\n+  \/\/ This instruction does not need an acquiring counterpart because it is only\n+  \/\/ used for reference loading (Reference::get()). The same holds for g1LoadN.\n+  predicate(UseG1GC && !needs_acquiring_load(n) && n->as_Load()->barrier_data() != 0);\n+  match(Set dst (LoadP mem));\n+  effect(TEMP dst, TEMP tmp1, TEMP tmp2, KILL cr);\n+  ins_cost(4 * INSN_COST);\n+  format %{ \"ldr  $dst, $mem\\t# ptr\" %}\n+  ins_encode %{\n+    __ ldr($dst$$Register, $mem$$Register);\n+    write_barrier_pre(masm, this,\n+                      noreg \/* obj *\/,\n+                      $dst$$Register \/* pre_val *\/,\n+                      $tmp1$$Register \/* tmp1 *\/,\n+                      $tmp2$$Register \/* tmp2 *\/);\n+  %}\n+  ins_pipe(iload_reg_mem);\n+%}\n+\n+\/\/ This pattern is generated automatically from g1_aarch64.m4.\n+\/\/ DO NOT EDIT ANYTHING IN THIS SECTION OF THE FILE\n+instruct g1LoadN(iRegNNoSp dst, indirect mem, iRegPNoSp tmp1, iRegPNoSp tmp2, iRegPNoSp tmp3, rFlagsReg cr)\n+%{\n+  predicate(UseG1GC && !needs_acquiring_load(n) && n->as_Load()->barrier_data() != 0);\n+  match(Set dst (LoadN mem));\n+  effect(TEMP dst, TEMP tmp1, TEMP tmp2, TEMP tmp3, KILL cr);\n+  ins_cost(4 * INSN_COST);\n+  format %{ \"ldrw  $dst, $mem\\t# compressed ptr\" %}\n+  ins_encode %{\n+    __ ldrw($dst$$Register, $mem$$Register);\n+    if ((barrier_data() & G1C2BarrierPre) != 0) {\n+      __ decode_heap_oop($tmp1$$Register, $dst$$Register);\n+      write_barrier_pre(masm, this,\n+                        noreg \/* obj *\/,\n+                        $tmp1$$Register \/* pre_val *\/,\n+                        $tmp2$$Register \/* tmp1 *\/,\n+                        $tmp3$$Register \/* tmp2 *\/);\n+    }\n+  %}\n+  ins_pipe(iload_reg_mem);\n+%}\n+\n+\/\/ END This section of the file is automatically generated. Do not edit --------------\n","filename":"src\/hotspot\/cpu\/aarch64\/gc\/g1\/g1_aarch64.m4","additions":384,"deletions":0,"binary":false,"changes":384,"status":"added"},{"patch":"@@ -3893,0 +3893,1 @@\n+  predicate(!(UseG1GC && n->as_Load()->barrier_data() != 0));\n@@ -4359,0 +4360,1 @@\n+  predicate(!(UseG1GC && n->as_Store()->barrier_data() != 0));\n@@ -5393,0 +5395,1 @@\n+  predicate(!(UseG1GC && n->as_LoadStore()->barrier_data() != 0));\n@@ -5662,0 +5665,1 @@\n+  predicate(!(UseG1GC && n->as_LoadStore()->barrier_data() != 0));\n","filename":"src\/hotspot\/cpu\/arm\/arm.ad","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -122,2 +122,3 @@\n-    assert((set1._encoding & set2._encoding) == 0,\n-           \"encoding constraint\");\n+\/\/    why so strong constraint?\n+\/\/    assert((set1._encoding & set2._encoding) == 0,\n+\/\/           \"encoding constraint\");\n@@ -145,0 +146,5 @@\n+\n+  static RegisterSet from(RegSet set) {\n+    assert(set.size(), \"RegSet must not be empty\");\n+    return RegisterSet(set.bits());\n+  }\n@@ -160,0 +166,4 @@\n+  FloatRegisterSet() {\n+    _encoding = 0;\n+  }\n+\n@@ -188,0 +198,9 @@\n+  static FloatRegisterSet from(FloatRegSet set) {\n+    assert(set.size(), \"FloatRegSet must not be empty\");\n+    \/\/ the vector load\/store instructions operate on a set of consecutive registers.\n+    \/\/ for the sake of simplicity, write all registers between the first and last in the set\n+    size_t range =  (*set.rbegin())->encoding() - (*set.begin())->encoding() + 1;\n+    \/\/ push_float stores float regisgters by pairs\n+    return  FloatRegisterSet(*set.begin(), (range+1)\/2);\n+  }\n+\n","filename":"src\/hotspot\/cpu\/arm\/assembler_arm_32.hpp","additions":21,"deletions":2,"binary":false,"changes":23,"status":"modified"},{"patch":"@@ -42,2 +42,4 @@\n-#endif\n-\n+#endif \/\/ COMPILER1\n+#ifdef COMPILER2\n+#include \"gc\/g1\/c2\/g1BarrierSetC2.hpp\"\n+#endif \/\/ COMPILER2\n@@ -109,24 +111,14 @@\n-\/\/ G1 pre-barrier.\n-\/\/ Blows all volatile registers R0-R3, Rtemp, LR).\n-\/\/ If store_addr != noreg, then previous value is loaded from [store_addr];\n-\/\/ in such case store_addr and new_val registers are preserved;\n-\/\/ otherwise pre_val register is preserved.\n-void G1BarrierSetAssembler::g1_write_barrier_pre(MacroAssembler* masm,\n-                                          Register store_addr,\n-                                          Register new_val,\n-                                          Register pre_val,\n-                                          Register tmp1,\n-                                          Register tmp2) {\n-  Label done;\n-  Label runtime;\n-\n-  if (store_addr != noreg) {\n-    assert_different_registers(store_addr, new_val, pre_val, tmp1, tmp2, noreg);\n-  } else {\n-    assert (new_val == noreg, \"should be\");\n-    assert_different_registers(pre_val, tmp1, tmp2, noreg);\n-  }\n-\n-  Address in_progress(Rthread, in_bytes(G1ThreadLocalData::satb_mark_queue_active_offset()));\n-  Address index(Rthread, in_bytes(G1ThreadLocalData::satb_mark_queue_index_offset()));\n-  Address buffer(Rthread, in_bytes(G1ThreadLocalData::satb_mark_queue_buffer_offset()));\n+static void generate_queue_test_and_insertion(MacroAssembler* masm, ByteSize index_offset, ByteSize buffer_offset, Label& runtime,\n+                                              const Register thread, const Register value, const Register temp1, const Register temp2) {\n+  assert_different_registers(value, temp1, temp2);\n+  \/\/ Can we store original value in the thread's buffer?\n+  \/\/ (The index field is typed as size_t.)\n+  __ ldr(temp1, Address(thread, in_bytes(index_offset)));  \/\/ temp1 := *(index address)\n+  __ cbz(temp1, runtime);                                  \/\/ jump to runtime if index == 0 (full buffer)\n+  \/\/ The buffer is not full, store value into it.\n+  __ sub(temp1, temp1, wordSize);                          \/\/ temp1 := next index\n+  __ str(temp1, Address(thread, in_bytes(index_offset)));  \/\/ *(index address) := next index\n+  __ ldr(temp2, Address(thread, in_bytes(buffer_offset))); \/\/ temp2 := buffer address\n+  \/\/ Record the previous value\n+  __ str(value, Address(temp2, temp1));                    \/\/ *(buffer address + next index) := value\n+ }\n@@ -134,0 +126,4 @@\n+static void generate_pre_barrier_fast_path(MacroAssembler* masm,\n+                                           const Register thread,\n+                                           const Register tmp1) {\n+  Address in_progress(thread, in_bytes(G1ThreadLocalData::satb_mark_queue_active_offset()));\n@@ -137,1 +133,1 @@\n-  __ cbz(tmp1, done);\n+}\n@@ -139,0 +135,8 @@\n+static void generate_pre_barrier_slow_path(MacroAssembler* masm,\n+                                           const Register obj,\n+                                           const Register pre_val,\n+                                           const Register thread,\n+                                           const Register tmp1,\n+                                           const Register tmp2,\n+                                           Label& done,\n+                                           Label& runtime) {\n@@ -140,2 +144,2 @@\n-  if (store_addr != noreg) {\n-    __ load_heap_oop(pre_val, Address(store_addr, 0));\n+  if (obj != noreg) {\n+    __ load_heap_oop(pre_val, Address(obj, 0));\n@@ -147,3 +151,7 @@\n-  \/\/ Can we store original value in the thread's buffer?\n-  \/\/ Is index == 0?\n-  \/\/ (The index field is typed as size_t.)\n+  generate_queue_test_and_insertion(masm,\n+                                    G1ThreadLocalData::satb_mark_queue_index_offset(),\n+                                    G1ThreadLocalData::satb_mark_queue_buffer_offset(),\n+                                    runtime,\n+                                    thread, pre_val, tmp1, tmp2);\n+  __ b(done);\n+}\n@@ -151,2 +159,12 @@\n-  __ ldr(tmp1, index);           \/\/ tmp1 := *index_adr\n-  __ ldr(tmp2, buffer);\n+\/\/ G1 pre-barrier.\n+\/\/ Blows all volatile registers R0-R3, LR).\n+\/\/ If obj != noreg, then previous value is loaded from [obj];\n+\/\/ in such case obj and pre_val registers is preserved;\n+\/\/ otherwise pre_val register is preserved.\n+void G1BarrierSetAssembler::g1_write_barrier_pre(MacroAssembler* masm,\n+                                          Register obj,\n+                                          Register pre_val,\n+                                          Register tmp1,\n+                                          Register tmp2) {\n+  Label done;\n+  Label runtime;\n@@ -154,2 +172,1 @@\n-  __ subs(tmp1, tmp1, wordSize); \/\/ tmp1 := tmp1 - wordSize\n-  __ b(runtime, lt);             \/\/ If negative, goto runtime\n+  assert_different_registers(obj, pre_val, tmp1, tmp2, noreg);\n@@ -157,1 +174,3 @@\n-  __ str(tmp1, index);           \/\/ *index_adr := tmp1\n+  generate_pre_barrier_fast_path(masm, Rthread, tmp1);\n+  \/\/ If marking is not active (*(mark queue active address) == 0), jump to done\n+  __ cbz(tmp1, done);\n@@ -159,3 +178,1 @@\n-  \/\/ Record the previous value\n-  __ str(pre_val, Address(tmp2, tmp1));\n-  __ b(done);\n+   generate_pre_barrier_slow_path(masm, obj, pre_val, Rthread, tmp1, tmp2, done, runtime);\n@@ -166,5 +183,5 @@\n-  if (store_addr != noreg) {\n-    \/\/ avoid raw_push to support any ordering of store_addr and new_val\n-    __ push(RegisterSet(store_addr) | RegisterSet(new_val));\n-  } else {\n-    __ push(pre_val);\n+  RegisterSet set = RegisterSet(pre_val) | RegisterSet(R0, R3) | RegisterSet(R12);\n+  \/\/ save the live input values\n+  if (obj != noreg) {\n+    \/\/ avoid raw_push to support any ordering of store_addr and pre_val\n+    set = set | RegisterSet(obj);\n@@ -173,0 +190,2 @@\n+  __ push(set);\n+\n@@ -180,6 +199,1 @@\n-  if (store_addr != noreg) {\n-    __ pop(RegisterSet(store_addr) | RegisterSet(new_val));\n-  } else {\n-    __ pop(pre_val);\n-  }\n-\n+  __ pop(set);\n@@ -189,18 +203,7 @@\n-\/\/ G1 post-barrier.\n-\/\/ Blows all volatile registers R0-R3, Rtemp, LR).\n-void G1BarrierSetAssembler::g1_write_barrier_post(MacroAssembler* masm,\n-                                           Register store_addr,\n-                                           Register new_val,\n-                                           Register tmp1,\n-                                           Register tmp2,\n-                                           Register tmp3) {\n-\n-  Address queue_index(Rthread, in_bytes(G1ThreadLocalData::dirty_card_queue_index_offset()));\n-  Address buffer(Rthread, in_bytes(G1ThreadLocalData::dirty_card_queue_buffer_offset()));\n-\n-  BarrierSet* bs = BarrierSet::barrier_set();\n-  CardTableBarrierSet* ctbs = barrier_set_cast<CardTableBarrierSet>(bs);\n-  CardTable* ct = ctbs->card_table();\n-  Label done;\n-  Label runtime;\n-\n+static void generate_post_barrier_fast_path(MacroAssembler* masm,\n+                                            const Register store_addr,\n+                                            const Register new_val,\n+                                            const Register tmp1,\n+                                            const Register tmp2,\n+                                            Label& done,\n+                                            bool new_val_may_be_null) {\n@@ -214,3 +217,3 @@\n-\n-  __ cbz(new_val, done);\n-\n+  if (new_val_may_be_null) {\n+    __ cbz(new_val, done);\n+  }\n@@ -220,1 +223,2 @@\n-  __ mov_address(tmp2, (address)ct->byte_map_base());\n+  CardTableBarrierSet* ct = barrier_set_cast<CardTableBarrierSet>(BarrierSet::barrier_set());\n+  __ mov_address(tmp2, (address)ct->card_table()->byte_map_base());\n@@ -225,1 +229,1 @@\n-  __ b(done, eq);\n+}\n@@ -227,0 +231,7 @@\n+static void generate_post_barrier_slow_path(MacroAssembler* masm,\n+                                            const Register thread,\n+                                            const Register tmp1,\n+                                            const Register tmp2,\n+                                            const Register tmp3,\n+                                            Label& done,\n+                                            Label& runtime) {\n@@ -228,1 +239,0 @@\n-\n@@ -230,0 +240,2 @@\n+  \/\/ card_addr is loaded by generate_post_barrier_fast_path\n+  const Register card_addr = tmp1;\n@@ -237,0 +249,7 @@\n+  generate_queue_test_and_insertion(masm,\n+                                    G1ThreadLocalData::dirty_card_queue_index_offset(),\n+                                    G1ThreadLocalData::dirty_card_queue_buffer_offset(),\n+                                    runtime,\n+                                    thread, card_addr, tmp2, tmp3);\n+  __ b(done);\n+}\n@@ -238,5 +257,0 @@\n-  __ ldr(tmp2, queue_index);\n-  __ ldr(tmp3, buffer);\n-\n-  __ subs(tmp2, tmp2, wordSize);\n-  __ b(runtime, lt); \/\/ go to runtime if now negative\n@@ -244,1 +258,10 @@\n-  __ str(tmp2, queue_index);\n+\/\/ G1 post-barrier.\n+\/\/ Blows all volatile registers R0-R3,  LR).\n+void G1BarrierSetAssembler::g1_write_barrier_post(MacroAssembler* masm,\n+                                           Register store_addr,\n+                                           Register new_val,\n+                                           Register tmp1,\n+                                           Register tmp2,\n+                                           Register tmp3) {\n+  Label done;\n+  Label runtime;\n@@ -246,2 +269,7 @@\n-  __ str(card_addr, Address(tmp3, tmp2));\n-  __ b(done);\n+  generate_post_barrier_fast_path(masm, store_addr, new_val, tmp1, tmp2, done, true \/* new_val_may_be_null *\/);\n+  \/\/ If card is young, jump to done\n+  \/\/ card_addr and card are loaded by generate_post_barrier_fast_path\n+  const Register card      = tmp2;\n+  const Register card_addr = tmp1;\n+   __ b(done, eq);\n+  generate_post_barrier_slow_path(masm, Rthread, card_addr, tmp2, tmp3, done, runtime);\n@@ -251,0 +279,3 @@\n+  RegisterSet set = RegisterSet(store_addr) | RegisterSet(R0, R3) | RegisterSet(R12);\n+  __ push(set);\n+\n@@ -257,0 +288,2 @@\n+  __ pop(set);\n+\n@@ -260,0 +293,89 @@\n+#if defined(COMPILER2)\n+\n+static void generate_c2_barrier_runtime_call(MacroAssembler* masm, G1BarrierStubC2* stub, const Register arg, const address runtime_path, Register tmp1) {\n+  SaveLiveRegisters save_registers(masm, stub);\n+  if (c_rarg0 != arg) {\n+    __ mov(c_rarg0, arg);\n+  }\n+  __ mov(c_rarg1, Rthread);\n+  __ call_VM_leaf(runtime_path, R0, R1);\n+}\n+\n+void G1BarrierSetAssembler::g1_write_barrier_pre_c2(MacroAssembler* masm,\n+                                                    Register obj,\n+                                                    Register pre_val,\n+                                                    Register thread,\n+                                                    Register tmp1,\n+                                                    Register tmp2,\n+                                                    G1PreBarrierStubC2* stub) {\n+  assert(thread == Rthread, \"must be\");\n+  assert_different_registers(obj, pre_val, tmp1, tmp2);\n+  assert(pre_val != noreg && tmp1 != noreg && tmp2 != noreg, \"expecting a register\");\n+\n+  stub->initialize_registers(obj, pre_val, thread, tmp1, tmp2);\n+\n+  generate_pre_barrier_fast_path(masm, thread, tmp1);\n+  \/\/ If marking is active (*(mark queue active address) != 0), jump to stub (slow path)\n+  __ cbnz(tmp1, *stub->entry());\n+\n+  __ bind(*stub->continuation());\n+}\n+\n+void G1BarrierSetAssembler::generate_c2_pre_barrier_stub(MacroAssembler* masm,\n+                                                         G1PreBarrierStubC2* stub) const {\n+  Assembler::InlineSkippedInstructionsCounter skip_counter(masm);\n+  Label runtime;\n+  Register obj = stub->obj();\n+  Register pre_val = stub->pre_val();\n+  Register thread = stub->thread();\n+  Register tmp1 = stub->tmp1();\n+  Register tmp2 = stub->tmp2();\n+\n+  __ bind(*stub->entry());\n+  generate_pre_barrier_slow_path(masm, obj, pre_val, thread, tmp1, tmp2, *stub->continuation(), runtime);\n+\n+  __ bind(runtime);\n+  generate_c2_barrier_runtime_call(masm, stub, pre_val, CAST_FROM_FN_PTR(address, G1BarrierSetRuntime::write_ref_field_pre_entry), tmp1);\n+  __ b(*stub->continuation());\n+}\n+\n+void G1BarrierSetAssembler::g1_write_barrier_post_c2(MacroAssembler* masm,\n+                                                     Register store_addr,\n+                                                     Register new_val,\n+                                                     Register thread,\n+                                                     Register tmp1,\n+                                                     Register tmp2,\n+                                                     Register tmp3,\n+                                                     G1PostBarrierStubC2* stub) {\n+  assert(thread == Rthread, \"must be\");\n+  assert_different_registers(store_addr, new_val, thread, tmp1, tmp2, noreg);\n+\n+  stub->initialize_registers(thread, tmp1, tmp2, tmp3);\n+\n+  bool new_val_may_be_null = (stub->barrier_data() & G1C2BarrierPostNotNull) == 0;\n+  generate_post_barrier_fast_path(masm, store_addr, new_val, tmp1, tmp2, *stub->continuation(), new_val_may_be_null);\n+  \/\/ If card is not young, jump to stub (slow path)\n+  __ b(*stub->entry(), ne);\n+\n+  __ bind(*stub->continuation());\n+}\n+\n+void G1BarrierSetAssembler::generate_c2_post_barrier_stub(MacroAssembler* masm,\n+                                                          G1PostBarrierStubC2* stub) const {\n+  Assembler::InlineSkippedInstructionsCounter skip_counter(masm);\n+  Label runtime;\n+  Register thread = stub->thread();\n+  Register tmp1 = stub->tmp1(); \/\/ tmp1 holds the card address.\n+  Register tmp2 = stub->tmp2();\n+  Register tmp3 = stub->tmp3();\n+\n+  __ bind(*stub->entry());\n+  generate_post_barrier_slow_path(masm, thread, tmp1, tmp2, tmp3,  *stub->continuation(), runtime);\n+\n+  __ bind(runtime);\n+  generate_c2_barrier_runtime_call(masm, stub, tmp1, CAST_FROM_FN_PTR(address, G1BarrierSetRuntime::write_ref_field_post_entry), tmp2);\n+  __ b(*stub->continuation());\n+}\n+\n+#endif \/\/ COMPILER2\n+\n@@ -271,1 +393,1 @@\n-    g1_write_barrier_pre(masm, noreg, noreg, dst, tmp1, tmp2);\n+    g1_write_barrier_pre(masm, noreg, dst, tmp1, tmp2);\n@@ -298,1 +420,1 @@\n-    g1_write_barrier_pre(masm, store_addr, new_val, tmp1, tmp2, tmp3);\n+    g1_write_barrier_pre(masm, store_addr, tmp3 \/*pre_val*\/, tmp1, tmp2);\n","filename":"src\/hotspot\/cpu\/arm\/gc\/g1\/g1BarrierSetAssembler_arm.cpp","additions":207,"deletions":85,"binary":false,"changes":292,"status":"modified"},{"patch":"@@ -36,0 +36,2 @@\n+class G1PreBarrierStubC2;\n+class G1PostBarrierStubC2;\n@@ -46,1 +48,0 @@\n-                            Register new_val,\n@@ -73,0 +74,23 @@\n+\n+#ifdef COMPILER2\n+  void g1_write_barrier_pre_c2(MacroAssembler* masm,\n+                               Register obj,\n+                               Register pre_val,\n+                               Register thread,\n+                               Register tmp1,\n+                               Register tmp2,\n+                               G1PreBarrierStubC2* c2_stub);\n+  void generate_c2_pre_barrier_stub(MacroAssembler* masm,\n+                                    G1PreBarrierStubC2* stub) const;\n+  void g1_write_barrier_post_c2(MacroAssembler* masm,\n+                                Register store_addr,\n+                                Register new_val,\n+                                Register thread,\n+                                Register tmp1,\n+                                Register tmp2,\n+                                Register tmp3,\n+                                G1PostBarrierStubC2* c2_stub);\n+  void generate_c2_post_barrier_stub(MacroAssembler* masm,\n+                                     G1PostBarrierStubC2* stub) const;\n+#endif\n+\n","filename":"src\/hotspot\/cpu\/arm\/gc\/g1\/g1BarrierSetAssembler_arm.hpp","additions":25,"deletions":1,"binary":false,"changes":26,"status":"modified"},{"patch":"@@ -0,0 +1,201 @@\n+\/\/\n+\/\/ Copyright (c) 2024, Oracle and\/or its affiliates. All rights reserved.\n+\/\/ DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+\/\/\n+\/\/ This code is free software; you can redistribute it and\/or modify it\n+\/\/ under the terms of the GNU General Public License version 2 only, as\n+\/\/ published by the Free Software Foundation.\n+\/\/\n+\/\/ This code is distributed in the hope that it will be useful, but WITHOUT\n+\/\/ ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+\/\/ FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+\/\/ version 2 for more details (a copy is included in the LICENSE file that\n+\/\/ accompanied this code).\n+\/\/\n+\/\/ You should have received a copy of the GNU General Public License version\n+\/\/ 2 along with this work; if not, write to the Free Software Foundation,\n+\/\/ Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+\/\/\n+\/\/ Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+\/\/ or visit www.oracle.com if you need additional information or have any\n+\/\/ questions.\n+\/\/\n+\n+source_hpp %{\n+\n+#include \"gc\/g1\/c2\/g1BarrierSetC2.hpp\"\n+#include \"gc\/shared\/gc_globals.hpp\"\n+\n+%}\n+\n+source %{\n+\n+#include \"gc\/g1\/g1BarrierSetAssembler_arm.hpp\"\n+#include \"gc\/g1\/g1BarrierSetRuntime.hpp\"\n+\n+static void write_barrier_pre(MacroAssembler* masm,\n+                              const MachNode* node,\n+                              Register obj,\n+                              Register pre_val,\n+                              Register tmp1,\n+                              Register tmp2,\n+                              RegSet preserve = RegSet(),\n+                              RegSet no_preserve = RegSet()) {\n+  if (!G1PreBarrierStubC2::needs_barrier(node)) {\n+    return;\n+  }\n+  Assembler::InlineSkippedInstructionsCounter skip_counter(masm);\n+  G1BarrierSetAssembler* g1_asm = static_cast<G1BarrierSetAssembler*>(BarrierSet::barrier_set()->barrier_set_assembler());\n+  G1PreBarrierStubC2* const stub = G1PreBarrierStubC2::create(node);\n+  for (RegSetIterator<Register> reg = preserve.begin(); *reg != noreg; ++reg) {\n+    stub->preserve(*reg);\n+  }\n+  for (RegSetIterator<Register> reg = no_preserve.begin(); *reg != noreg; ++reg) {\n+    stub->dont_preserve(*reg);\n+  }\n+  g1_asm->g1_write_barrier_pre_c2(masm, obj, pre_val, Rthread, tmp1, tmp2, stub);\n+}\n+\n+static void write_barrier_post(MacroAssembler* masm,\n+                               const MachNode* node,\n+                               Register store_addr,\n+                               Register new_val,\n+                               Register tmp1,\n+                               Register tmp2,\n+                               Register tmp3) {\n+  if (!G1PostBarrierStubC2::needs_barrier(node)) {\n+    return;\n+  }\n+  Assembler::InlineSkippedInstructionsCounter skip_counter(masm);\n+  G1BarrierSetAssembler* g1_asm = static_cast<G1BarrierSetAssembler*>(BarrierSet::barrier_set()->barrier_set_assembler());\n+  G1PostBarrierStubC2* const stub = G1PostBarrierStubC2::create(node);\n+  g1_asm->g1_write_barrier_post_c2(masm, store_addr, new_val, Rthread, tmp1, tmp2, tmp3, stub);\n+}\n+\n+%}\n+\n+instruct g1StoreP(indirect mem, iRegP src, iRegP tmp1, iRegP tmp2, iRegP tmp3, flagsReg icc)\n+%{\n+  predicate(UseG1GC && n->as_Store()->barrier_data() != 0);\n+  match(Set mem (StoreP mem src));\n+  effect(TEMP tmp1, TEMP tmp2, TEMP tmp3, KILL icc);\n+  ins_cost(2 * (MEMORY_REF_COST + BRANCH_COST));\n+  format %{ \"sd  $src, $mem\\t# ptr\" %}\n+  ins_encode %{\n+    guarantee($mem$$disp == 0, \"impossible encoding\");\n+    write_barrier_pre(masm, this,\n+                      $mem$$Register  \/* obj *\/,\n+                      $tmp1$$Register \/* pre_val *\/,\n+                      $tmp2$$Register \/* tmp1 *\/,\n+                      $tmp3$$Register \/* tmp2 *\/,\n+                      RegSet::of($mem$$Register, $src$$Register) \/* preserve *\/);\n+    __ str($src$$Register, Address($mem$$Register));\n+    write_barrier_post(masm, this,\n+                       $mem$$Register  \/* store_addr *\/,\n+                       $src$$Register  \/* new_val *\/,\n+                       $tmp1$$Register \/* tmp1 *\/,\n+                       $tmp2$$Register \/* tmp2 *\/,\n+                       $tmp3$$Register \/* tmp3 *\/);\n+  %}\n+  ins_pipe(istore_mem_reg);\n+%}\n+\n+instruct g1CompareAndSwapP(iRegI res, indirect mem, iRegP newval, iRegP tmp1, iRegP tmp2, iRegP tmp3, iRegP oldval, flagsReg ccr )\n+%{\n+  predicate(UseG1GC && n->as_LoadStore()->barrier_data() != 0);\n+  match(Set res (CompareAndSwapP mem (Binary oldval newval)));\n+  effect(KILL ccr, TEMP res, TEMP tmp1, TEMP tmp2, TEMP tmp3);\n+  ins_cost(4 * (MEMORY_REF_COST + BRANCH_COST));\n+  format %{ \"loop: \\n\\t\"\n+            \"LDREX    $tmp1, $mem\\t! If $oldval==[$mem] Then store $newval into [$mem]\\n\\t\"\n+            \"CMP      $tmp1, $oldval\\n\\t\"\n+            \"STREX.eq $tmp1, $newval, $mem\\n\\t\"\n+            \"MOV.ne   $tmp1, 0 \\n\\t\"\n+            \"EORS.eq  $tmp1,$tmp1, 1 \\n\\t\"\n+            \"B.eq     loop \\n\\t\"\n+            \"MOV      $res, $tmp1\" %}\n+  ins_encode %{\n+    guarantee($mem$$disp == 0, \"impossible encoding\");\n+    assert_different_registers($oldval$$Register, $mem$$Register);\n+    assert_different_registers($newval$$Register, $mem$$Register);\n+    write_barrier_pre(masm, this,\n+                      noreg             \/* obj *\/,\n+                      $oldval$$Register \/* pre_val *\/,\n+                      $tmp2$$Register   \/* tmp1 *\/,\n+                      $tmp3$$Register   \/* tmp2 *\/,\n+                      RegSet::of($mem$$Register, $oldval$$Register, $newval$$Register) \/* preserve *\/,\n+                      RegSet::of($res$$Register) \/* no_preserve *\/);\n+    Label loop;\n+    __ bind(loop);\n+    __ ldrex($tmp1$$Register,$mem$$Address);\n+    __ cmp($tmp1$$Register, $oldval$$Register);\n+    __ strex($tmp1$$Register, $newval$$Register, $mem$$Address, eq);\n+    __ mov($tmp1$$Register, 0, ne);\n+    __ eors($tmp1$$Register, $tmp1$$Register, 1, eq);\n+    __ b(loop, eq);\n+    __ mov($res$$Register, $tmp1$$Register);\n+    write_barrier_post(masm, this,\n+                       $mem$$Register  \/* store_addr *\/,\n+                       $newval$$Register \/* new_val *\/,\n+                       $tmp1$$Register \/* tmp1 *\/,\n+                       $tmp2$$Register \/* tmp2 *\/,\n+                       $tmp3$$Register \/* tmp3 *\/);\n+  %}\n+  ins_pipe(long_memory_op);\n+%}\n+\n+\n+instruct g1GetAndSetP(indirect mem, iRegP newval, iRegP tmp1, iRegP tmp2, iRegP tmp3, iRegP preval, flagsReg ccr)\n+%{\n+  predicate(UseG1GC && n->as_LoadStore()->barrier_data() != 0);\n+  match(Set preval (GetAndSetP mem newval));\n+  effect(KILL ccr, TEMP preval, TEMP tmp1, TEMP tmp2, TEMP tmp3);\n+  ins_cost(4 * (MEMORY_REF_COST + BRANCH_COST));\n+  format %{ \"loop: \\n\\t\"\n+            \"LDREX    $preval, $mem\\n\\t\"\n+            \"STREX    $tmp1, $newval, $mem\\n\\t\"\n+            \"CMP      $tmp1, 0 \\n\\t\"\n+            \"B.ne     loop \\n\\t\" %}\n+  ins_encode %{\n+    guarantee($mem$$disp == 0, \"impossible encoding\");\n+    assert_different_registers($mem$$Register, $newval$$Register);\n+    write_barrier_pre(masm, this,\n+                      $mem$$Register    \/* obj *\/,\n+                      $preval$$Register \/* pre_val (as a temporary register) *\/,\n+                      $tmp1$$Register   \/* tmp1 *\/,\n+                      $tmp2$$Register   \/* tmp2 *\/,\n+                      RegSet::of($mem$$Register, $preval$$Register, $newval$$Register) \/* preserve *\/);\n+    Label loop;\n+    __ bind(loop);\n+    __ ldrex($preval$$Register,$mem$$Address);\n+    __ strex($tmp1$$Register, $newval$$Register, $mem$$Address);\n+    __ cmp($tmp1$$Register, 0);\n+    __ b(loop, ne);\n+    write_barrier_post(masm, this,\n+                       $mem$$Register    \/* store_addr *\/,\n+                       $newval$$Register \/* new_val *\/,\n+                       $tmp1$$Register   \/* tmp1 *\/,\n+                       $tmp2$$Register   \/* tmp2 *\/,\n+                       $tmp3$$Register   \/* tmp3 *\/);\n+  %}\n+  ins_pipe(long_memory_op);\n+%}\n+\n+instruct g1LoadP(iRegP dst, indirect mem, iRegP tmp1, iRegP tmp2, flagsReg icc)\n+%{\n+  predicate(UseG1GC && n->as_Load()->barrier_data() != 0);\n+  match(Set dst (LoadP mem));\n+  effect(TEMP dst, TEMP tmp1, TEMP tmp2, KILL icc);\n+  ins_cost(MEMORY_REF_COST + BRANCH_COST);\n+  format %{ \"ld  $dst, $mem\\t# ptr\" %}\n+  ins_encode %{\n+    guarantee($mem$$disp == 0, \"impossible encoding\");\n+    __ ldr($dst$$Register, Address($mem$$Register));\n+    write_barrier_pre(masm, this,\n+                      noreg \/* obj *\/,\n+                      $dst$$Register \/* pre_val *\/,\n+                      $tmp1$$Register \/* tmp1 *\/,\n+                      $tmp2$$Register \/* tmp2 *\/);\n+  %}\n+  ins_pipe(iload_mem);\n+%}\n","filename":"src\/hotspot\/cpu\/arm\/gc\/g1\/g1_arm.ad","additions":201,"deletions":0,"binary":false,"changes":201,"status":"added"},{"patch":"@@ -34,0 +34,4 @@\n+#ifdef COMPILER2\n+#include \"gc\/shared\/c2\/barrierSetC2.hpp\"\n+#endif \/\/ COMPILER2\n+\n@@ -209,1 +213,11 @@\n-  Unimplemented(); \/\/ This must be implemented to support late barrier expansion.\n+  if (!OptoReg::is_reg(opto_reg)) {\n+    return OptoReg::Bad;\n+  }\n+\n+  const VMReg vm_reg = OptoReg::as_VMReg(opto_reg);\n+  if (!vm_reg->is_valid()){\n+    \/\/ skip APSR and FPSCR\n+    return OptoReg::Bad;\n+  }\n+\n+  return opto_reg;\n@@ -212,0 +226,40 @@\n+void SaveLiveRegisters::initialize(BarrierStubC2* stub) {\n+  \/\/ Record registers that needs to be saved\/restored\n+  RegMaskIterator rmi(stub->preserve_set());\n+  while (rmi.has_next()) {\n+    const OptoReg::Name opto_reg = rmi.next();\n+    if (OptoReg::is_reg(opto_reg)) {\n+      const VMReg vm_reg = OptoReg::as_VMReg(opto_reg);\n+      if (vm_reg->is_Register()) {\n+        gp_regs += RegSet::of(vm_reg->as_Register());\n+      } else if (vm_reg->is_FloatRegister()) {\n+        fp_regs += FloatRegSet::of(vm_reg->as_FloatRegister());\n+      } else {\n+        fatal(\"Unknown register type\");\n+      }\n+    }\n+  }\n+  \/\/ Remove C-ABI SOE registers that will be updated\n+  gp_regs -= RegSet::range(R4, R11) + RegSet::of(R13, R15);\n+\n+  \/\/ Remove C-ABI SOE fp registers\n+  fp_regs -= FloatRegSet::range(S16, S31);\n+}\n+\n+SaveLiveRegisters::SaveLiveRegisters(MacroAssembler* masm, BarrierStubC2* stub)\n+  : masm(masm),\n+    gp_regs(),\n+    fp_regs() {\n+  \/\/ Figure out what registers to save\/restore\n+  initialize(stub);\n+\n+  \/\/ Save registers\n+  if (gp_regs.size() > 0) __ push(RegisterSet::from(gp_regs));\n+  if (fp_regs.size() > 0) __ fpush(FloatRegisterSet::from(fp_regs));\n+}\n+\n+SaveLiveRegisters::~SaveLiveRegisters() {\n+  \/\/ Restore registers\n+  if (fp_regs.size() > 0) __ fpop(FloatRegisterSet::from(fp_regs));\n+  if (gp_regs.size() > 0) __ pop(RegisterSet::from(gp_regs));\n+}\n","filename":"src\/hotspot\/cpu\/arm\/gc\/shared\/barrierSetAssembler_arm.cpp","additions":55,"deletions":1,"binary":false,"changes":56,"status":"modified"},{"patch":"@@ -34,0 +34,1 @@\n+#include \"opto\/regmask.hpp\"\n@@ -35,0 +36,1 @@\n+class BarrierStubC2;\n@@ -72,0 +74,22 @@\n+#ifdef COMPILER2\n+\/\/ This class saves and restores the registers that need to be preserved across\n+\/\/ the runtime call represented by a given C2 barrier stub. Use as follows:\n+\/\/ {\n+\/\/   SaveLiveRegisters save(masm, stub);\n+\/\/   ..\n+\/\/   __ bl(...);\n+\/\/   ..\n+\/\/ }\n+class SaveLiveRegisters {\n+private:\n+  MacroAssembler* const masm;\n+  RegSet                gp_regs;\n+  FloatRegSet           fp_regs;\n+\n+public:\n+  void initialize(BarrierStubC2* stub);\n+  SaveLiveRegisters(MacroAssembler* masm, BarrierStubC2* stub);\n+  ~SaveLiveRegisters();\n+};\n+\n+#endif \/\/ COMPILER2\n","filename":"src\/hotspot\/cpu\/arm\/gc\/shared\/barrierSetAssembler_arm.hpp","additions":24,"deletions":0,"binary":false,"changes":24,"status":"modified"},{"patch":"@@ -306,0 +306,25 @@\n+typedef AbstractRegSet<Register> RegSet;\n+typedef AbstractRegSet<FloatRegister> FloatRegSet;\n+\n+template <>\n+inline Register AbstractRegSet<Register>::first() {\n+  if (_bitset == 0) { return noreg; }\n+  return as_Register(count_trailing_zeros(_bitset));\n+}\n+\n+\n+template <>\n+inline FloatRegister AbstractRegSet<FloatRegister>::first() {\n+  uint32_t first = _bitset & -_bitset;\n+  return first ? as_FloatRegister(exact_log2(first)) : fnoreg;\n+}\n+\n+template <>\n+inline FloatRegister AbstractRegSet<FloatRegister>::last() {\n+  if (_bitset == 0) { return fnoreg; }\n+  int last = max_size() - 1 - count_leading_zeros(_bitset);\n+  return as_FloatRegister(last);\n+}\n+\n+\n+\n","filename":"src\/hotspot\/cpu\/arm\/register_arm.hpp","additions":25,"deletions":0,"binary":false,"changes":25,"status":"modified"},{"patch":"@@ -44,1 +44,4 @@\n-#endif\n+#endif \/\/ COMPILER1\n+#ifdef COMPILER2\n+#include \"gc\/g1\/c2\/g1BarrierSetC2.hpp\"\n+#endif \/\/ COMPILER2\n@@ -48,0 +51,7 @@\n+static void generate_marking_inactive_test(MacroAssembler* masm) {\n+  int active_offset = in_bytes(G1ThreadLocalData::satb_mark_queue_active_offset());\n+  assert(in_bytes(SATBMarkQueue::byte_width_of_active()) == 1, \"Assumption\");\n+  __ lbz(R0, active_offset, R16_thread);  \/\/ tmp1 := *(mark queue active address)\n+  __ cmpwi(CCR0, R0, 0);\n+}\n+\n@@ -61,7 +71,1 @@\n-    if (in_bytes(SATBMarkQueue::byte_width_of_active()) == 4) {\n-      __ lwz(R0, in_bytes(G1ThreadLocalData::satb_mark_queue_active_offset()), R16_thread);\n-    } else {\n-      guarantee(in_bytes(SATBMarkQueue::byte_width_of_active()) == 1, \"Assumption\");\n-      __ lbz(R0, in_bytes(G1ThreadLocalData::satb_mark_queue_active_offset()), R16_thread);\n-    }\n-    __ cmpdi(CCR0, R0, 0);\n+    generate_marking_inactive_test(masm);\n@@ -112,0 +116,15 @@\n+static void generate_queue_insertion(MacroAssembler* masm, ByteSize index_offset, ByteSize buffer_offset, Label& runtime,\n+                                     const Register value, const Register temp) {\n+  assert_different_registers(value, temp);\n+  \/\/ Can we store a value in the given thread's buffer?\n+  \/\/ (The index field is typed as size_t.)\n+  __ ld(temp, in_bytes(index_offset), R16_thread);  \/\/ temp := *(index address)\n+  __ cmpdi(CCR0, temp, 0);                          \/\/ jump to runtime if index == 0 (full buffer)\n+  __ beq(CCR0, runtime);\n+  \/\/ The buffer is not full, store value into it.\n+  __ ld(R0, in_bytes(buffer_offset), R16_thread);   \/\/ R0 := buffer address\n+  __ addi(temp, temp, -wordSize);                   \/\/ temp := next index\n+  __ std(temp, in_bytes(index_offset), R16_thread); \/\/ *(index address) := next index\n+  __ stdx(value, temp, R0);                         \/\/ *(buffer address + next index) := value\n+}\n+\n@@ -116,0 +135,2 @@\n+  assert_different_registers(pre_val, tmp1, tmp2);\n+\n@@ -120,1 +141,7 @@\n-  if (preloaded) {\n+  \/\/ Determine necessary runtime invocation preservation measures\n+  const bool needs_frame = preservation_level >= MacroAssembler::PRESERVATION_FRAME_LR;\n+  const bool preserve_gp_registers = preservation_level >= MacroAssembler::PRESERVATION_FRAME_LR_GP_REGS;\n+  const bool preserve_fp_registers = preservation_level >= MacroAssembler::PRESERVATION_FRAME_LR_GP_FP_REGS;\n+  int nbytes_save = 0;\n+\n+  if (pre_val->is_volatile() && preloaded && !preserve_gp_registers) {\n@@ -124,5 +151,2 @@\n-    assert_different_registers(pre_val, tmp1, tmp2);\n-    if (pre_val->is_volatile()) {\n-      nv_save = !tmp1->is_volatile() ? tmp1 : tmp2;\n-      assert(!nv_save->is_volatile(), \"need one nv temp register if pre_val lives in volatile register\");\n-    }\n+    nv_save = !tmp1->is_volatile() ? tmp1 : tmp2;\n+    assert(!nv_save->is_volatile(), \"need one nv temp register if pre_val lives in volatile register\");\n@@ -133,8 +157,1 @@\n-  \/\/ Is marking active?\n-  if (in_bytes(SATBMarkQueue::byte_width_of_active()) == 4) {\n-    __ lwz(tmp1, in_bytes(G1ThreadLocalData::satb_mark_queue_active_offset()), R16_thread);\n-  } else {\n-    guarantee(in_bytes(SATBMarkQueue::byte_width_of_active()) == 1, \"Assumption\");\n-    __ lbz(tmp1, in_bytes(G1ThreadLocalData::satb_mark_queue_active_offset()), R16_thread);\n-  }\n-  __ cmpdi(CCR0, tmp1, 0);\n+  generate_marking_inactive_test(masm);\n@@ -178,12 +195,2 @@\n-  const Register Rbuffer = tmp1, Rindex = tmp2;\n-\n-  __ ld(Rindex, in_bytes(G1ThreadLocalData::satb_mark_queue_index_offset()), R16_thread);\n-  __ cmpdi(CCR0, Rindex, 0);\n-  __ beq(CCR0, runtime); \/\/ If index == 0, goto runtime.\n-  __ ld(Rbuffer, in_bytes(G1ThreadLocalData::satb_mark_queue_buffer_offset()), R16_thread);\n-\n-  __ addi(Rindex, Rindex, -wordSize); \/\/ Decrement index.\n-  __ std(Rindex, in_bytes(G1ThreadLocalData::satb_mark_queue_index_offset()), R16_thread);\n-\n-  \/\/ Record the previous value.\n-  __ stdx(pre_val, Rbuffer, Rindex);\n+  generate_queue_insertion(masm, G1ThreadLocalData::satb_mark_queue_index_offset(), G1ThreadLocalData::satb_mark_queue_buffer_offset(),\n+                           runtime, pre_val, tmp1);\n@@ -194,6 +201,0 @@\n-  \/\/ Determine necessary runtime invocation preservation measures\n-  const bool needs_frame = preservation_level >= MacroAssembler::PRESERVATION_FRAME_LR;\n-  const bool preserve_gp_registers = preservation_level >= MacroAssembler::PRESERVATION_FRAME_LR_GP_REGS;\n-  const bool preserve_fp_registers = preservation_level >= MacroAssembler::PRESERVATION_FRAME_LR_GP_FP_REGS;\n-  int nbytes_save = 0;\n-\n@@ -213,1 +214,1 @@\n-  if (pre_val->is_volatile() && preloaded && !preserve_gp_registers) {\n+  if (nv_save != noreg) {\n@@ -217,1 +218,1 @@\n-  if (pre_val->is_volatile() && preloaded && !preserve_gp_registers) {\n+  if (nv_save != noreg) {\n@@ -233,0 +234,20 @@\n+static void generate_region_crossing_test(MacroAssembler* masm, const Register store_addr, const Register new_val) {\n+  __ xorr(R0, store_addr, new_val);                  \/\/ tmp1 := store address ^ new value\n+  __ srdi_(R0, R0, G1HeapRegion::LogOfHRGrainBytes); \/\/ tmp1 := ((store address ^ new value) >> LogOfHRGrainBytes)\n+}\n+\n+static Address generate_card_young_test(MacroAssembler* masm, const Register store_addr, const Register tmp1, const Register tmp2) {\n+  CardTableBarrierSet* ct = barrier_set_cast<CardTableBarrierSet>(BarrierSet::barrier_set());\n+  __ load_const_optimized(tmp1, (address)(ct->card_table()->byte_map_base()), tmp2);\n+  __ srdi(tmp2, store_addr, CardTable::card_shift());        \/\/ tmp1 := card address relative to card table base\n+  __ lbzx(R0, tmp1, tmp2);                                   \/\/ tmp1 := card address\n+  __ cmpwi(CCR0, R0, (int)G1CardTable::g1_young_card_val());\n+  return Address(tmp1, tmp2); \/\/ return card address\n+}\n+\n+static void generate_card_dirty_test(MacroAssembler* masm, Address card_addr) {\n+  __ membar(Assembler::StoreLoad);                        \/\/ Must reload after StoreLoad membar due to concurrent refinement\n+  __ lbzx(R0, card_addr.base(), card_addr.index());       \/\/ tmp2 := card\n+  __ cmpwi(CCR0, R0, (int)G1CardTable::dirty_card_val()); \/\/ tmp2 := card == dirty_card_val?\n+}\n+\n@@ -244,3 +265,1 @@\n-  \/\/ Does store cross heap regions?\n-  __ xorr(tmp1, store_addr, new_val);\n-  __ srdi_(tmp1, tmp1, G1HeapRegion::LogOfHRGrainBytes);\n+  generate_region_crossing_test(masm, store_addr, new_val);\n@@ -260,10 +279,1 @@\n-  \/\/ Storing region crossing non-null, is card already dirty?\n-  const Register Rcard_addr = tmp1;\n-  Register Rbase = tmp2;\n-  __ load_const_optimized(Rbase, (address)(ct->card_table()->byte_map_base()), \/*temp*\/ tmp3);\n-\n-  __ srdi(Rcard_addr, store_addr, CardTable::card_shift());\n-\n-  \/\/ Get the address of the card.\n-  __ lbzx(\/*card value*\/ tmp3, Rbase, Rcard_addr);\n-  __ cmpwi(CCR0, tmp3, (int)G1CardTable::g1_young_card_val());\n+  Address card_addr = generate_card_young_test(masm, store_addr, tmp1, tmp2);\n@@ -272,3 +282,1 @@\n-  __ membar(Assembler::StoreLoad);\n-  __ lbzx(\/*card value*\/ tmp3, Rbase, Rcard_addr);  \/\/ Reload after membar.\n-  __ cmpwi(CCR0, tmp3 \/* card value *\/, (int)G1CardTable::dirty_card_val());\n+  generate_card_dirty_test(masm, card_addr);\n@@ -277,15 +285,2 @@\n-  \/\/ Storing a region crossing, non-null oop, card is clean.\n-  \/\/ Dirty card and log.\n-  __ li(tmp3, (int)G1CardTable::dirty_card_val());\n-  \/\/release(); \/\/ G1: oops are allowed to get visible after dirty marking.\n-  __ stbx(tmp3, Rbase, Rcard_addr);\n-\n-  __ add(Rcard_addr, Rbase, Rcard_addr); \/\/ This is the address which needs to get enqueued.\n-  Rbase = noreg; \/\/ end of lifetime\n-\n-  const Register Rqueue_index = tmp2,\n-                 Rqueue_buf   = tmp3;\n-  __ ld(Rqueue_index, in_bytes(G1ThreadLocalData::dirty_card_queue_index_offset()), R16_thread);\n-  __ cmpdi(CCR0, Rqueue_index, 0);\n-  __ beq(CCR0, runtime); \/\/ index == 0 then jump to runtime\n-  __ ld(Rqueue_buf, in_bytes(G1ThreadLocalData::dirty_card_queue_buffer_offset()), R16_thread);\n+  __ li(R0, (int)G1CardTable::dirty_card_val());\n+  __ stbx(R0, card_addr.base(), card_addr.index()); \/\/ *(card address) := dirty_card_val\n@@ -293,2 +288,2 @@\n-  __ addi(Rqueue_index, Rqueue_index, -wordSize); \/\/ decrement index\n-  __ std(Rqueue_index, in_bytes(G1ThreadLocalData::dirty_card_queue_index_offset()), R16_thread);\n+  Register Rcard_addr = tmp3;\n+  __ add(Rcard_addr, card_addr.base(), card_addr.index()); \/\/ This is the address which needs to get enqueued.\n@@ -296,1 +291,4 @@\n-  __ stdx(Rcard_addr, Rqueue_buf, Rqueue_index); \/\/ store card\n+  generate_queue_insertion(masm,\n+                           G1ThreadLocalData::dirty_card_queue_index_offset(),\n+                           G1ThreadLocalData::dirty_card_queue_buffer_offset(),\n+                           runtime, Rcard_addr, tmp1);\n@@ -395,0 +393,136 @@\n+#ifdef COMPILER2\n+\n+static void generate_c2_barrier_runtime_call(MacroAssembler* masm, G1BarrierStubC2* stub, const Register arg, const address runtime_path) {\n+  SaveLiveRegisters save_registers(masm, stub);\n+  __ call_VM_leaf(runtime_path, arg, R16_thread);\n+}\n+\n+void G1BarrierSetAssembler::g1_write_barrier_pre_c2(MacroAssembler* masm,\n+                                                    Register obj,\n+                                                    Register pre_val,\n+                                                    Register tmp1,\n+                                                    Register tmp2,\n+                                                    G1PreBarrierStubC2* stub) {\n+  assert_different_registers(obj, tmp1, tmp2, R0);\n+  assert_different_registers(pre_val, tmp1, R0);\n+  assert(!UseCompressedOops || tmp2 != noreg, \"tmp2 needed with CompressedOops\");\n+\n+  stub->initialize_registers(obj, pre_val, R16_thread, tmp1, tmp2);\n+\n+  generate_marking_inactive_test(masm);\n+  __ bc_far_optimized(Assembler::bcondCRbiIs0, __ bi0(CCR0, Assembler::equal), *stub->entry());\n+\n+  __ bind(*stub->continuation());\n+}\n+\n+void G1BarrierSetAssembler::generate_c2_pre_barrier_stub(MacroAssembler* masm,\n+                                                         G1PreBarrierStubC2* stub) const {\n+  Assembler::InlineSkippedInstructionsCounter skip_counter(masm);\n+  Label runtime;\n+  Register obj = stub->obj();\n+  Register pre_val = stub->pre_val();\n+  Register tmp1 = stub->tmp1();\n+\n+  __ bind(*stub->entry());\n+\n+  if (obj != noreg) {\n+    \/\/ Note: C2 currently doesn't use implicit null checks with barriers.\n+    \/\/ Otherwise, obj could be null and the following instruction would raise a SIGSEGV.\n+    if (UseCompressedOops) {\n+      __ lwz(pre_val, 0, obj);\n+    } else {\n+      __ ld(pre_val, 0, obj);\n+    }\n+  }\n+  __ cmpdi(CCR0, pre_val, 0);\n+  __ bc_far_optimized(Assembler::bcondCRbiIs1, __ bi0(CCR0, Assembler::equal), *stub->continuation());\n+\n+  Register pre_val_decoded = pre_val;\n+  if (UseCompressedOops) {\n+    pre_val_decoded = __ decode_heap_oop_not_null(stub->tmp2(), pre_val);\n+  }\n+\n+  generate_queue_insertion(masm,\n+                           G1ThreadLocalData::satb_mark_queue_index_offset(),\n+                           G1ThreadLocalData::satb_mark_queue_buffer_offset(),\n+                           runtime, pre_val_decoded, tmp1);\n+  __ b(*stub->continuation());\n+\n+  __ bind(runtime);\n+  generate_c2_barrier_runtime_call(masm, stub, pre_val_decoded, CAST_FROM_FN_PTR(address, G1BarrierSetRuntime::write_ref_field_pre_entry));\n+  __ b(*stub->continuation());\n+}\n+\n+void G1BarrierSetAssembler::g1_write_barrier_post_c2(MacroAssembler* masm,\n+                                                     Register store_addr,\n+                                                     Register new_val,\n+                                                     Register tmp1,\n+                                                     Register tmp2,\n+                                                     G1PostBarrierStubC2* stub,\n+                                                     bool decode_new_val) {\n+  assert_different_registers(store_addr, new_val, tmp1, R0);\n+  assert_different_registers(store_addr, tmp1, tmp2, R0);\n+\n+  stub->initialize_registers(R16_thread, tmp1, tmp2);\n+\n+  bool null_check_required = (stub->barrier_data() & G1C2BarrierPostNotNull) == 0;\n+  Register new_val_decoded = new_val;\n+\n+  if (decode_new_val) {\n+    assert(UseCompressedOops, \"or should not be here\");\n+    if (null_check_required && CompressedOops::base() != nullptr) {\n+      \/\/ We prefer doing the null check after the region crossing check.\n+      \/\/ Only compressed oop modes with base != null require a null check here.\n+      __ cmpwi(CCR0, new_val, 0);\n+      __ beq(CCR0, *stub->continuation());\n+      null_check_required = false;\n+    }\n+    new_val_decoded = __ decode_heap_oop_not_null(tmp2, new_val);\n+  }\n+\n+  generate_region_crossing_test(masm, store_addr, new_val_decoded);\n+  __ beq(CCR0, *stub->continuation());\n+\n+  \/\/ crosses regions, storing null?\n+  if (null_check_required) {\n+    __ cmpdi(CCR0, new_val_decoded, 0);\n+    __ beq(CCR0, *stub->continuation());\n+  }\n+\n+  Address card_addr = generate_card_young_test(masm, store_addr, tmp1, tmp2);\n+  assert(card_addr.base() == tmp1 && card_addr.index() == tmp2, \"needed by post barrier stub\");\n+  __ bc_far_optimized(Assembler::bcondCRbiIs0, __ bi0(CCR0, Assembler::equal), *stub->entry());\n+\n+  __ bind(*stub->continuation());\n+}\n+\n+void G1BarrierSetAssembler::generate_c2_post_barrier_stub(MacroAssembler* masm,\n+                                                          G1PostBarrierStubC2* stub) const {\n+  Assembler::InlineSkippedInstructionsCounter skip_counter(masm);\n+  Label runtime;\n+  Address card_addr(stub->tmp1(), stub->tmp2()); \/\/ See above.\n+\n+  __ bind(*stub->entry());\n+\n+  generate_card_dirty_test(masm, card_addr);\n+  __ bc_far_optimized(Assembler::bcondCRbiIs1, __ bi0(CCR0, Assembler::equal), *stub->continuation());\n+\n+  __ li(R0, (int)G1CardTable::dirty_card_val());\n+  __ stbx(R0, card_addr.base(), card_addr.index()); \/\/ *(card address) := dirty_card_val\n+\n+  Register Rcard_addr = stub->tmp1();\n+  __ add(Rcard_addr, card_addr.base(), card_addr.index()); \/\/ This is the address which needs to get enqueued.\n+\n+  generate_queue_insertion(masm,\n+                           G1ThreadLocalData::dirty_card_queue_index_offset(),\n+                           G1ThreadLocalData::dirty_card_queue_buffer_offset(),\n+                           runtime, Rcard_addr, stub->tmp2());\n+  __ b(*stub->continuation());\n+\n+  __ bind(runtime);\n+  generate_c2_barrier_runtime_call(masm, stub, Rcard_addr, CAST_FROM_FN_PTR(address, G1BarrierSetRuntime::write_ref_field_post_entry));\n+  __ b(*stub->continuation());\n+}\n+\n+#endif \/\/ COMPILER2\n+\n@@ -473,7 +607,1 @@\n-  if (in_bytes(SATBMarkQueue::byte_width_of_active()) == 4) {\n-    __ lwz(tmp, satb_q_active_byte_offset, R16_thread);\n-  } else {\n-    assert(in_bytes(SATBMarkQueue::byte_width_of_active()) == 1, \"Assumption\");\n-    __ lbz(tmp, satb_q_active_byte_offset, R16_thread);\n-  }\n-  __ cmpdi(CCR0, tmp, 0);\n+  generate_marking_inactive_test(sasm);\n","filename":"src\/hotspot\/cpu\/ppc\/gc\/g1\/g1BarrierSetAssembler_ppc.cpp","additions":211,"deletions":83,"binary":false,"changes":294,"status":"modified"},{"patch":"@@ -33,0 +33,4 @@\n+#ifdef COMPILER2\n+#include \"gc\/g1\/c2\/g1BarrierSetC2.hpp\"\n+#endif\n+\n@@ -37,0 +41,2 @@\n+class G1PreBarrierStubC2;\n+class G1PostBarrierStubC2;\n@@ -62,0 +68,19 @@\n+#ifdef COMPILER2\n+  void g1_write_barrier_pre_c2(MacroAssembler* masm,\n+                               Register obj,\n+                               Register pre_val,\n+                               Register tmp1,\n+                               Register tmp2,\n+                               G1PreBarrierStubC2* c2_stub);\n+  void generate_c2_pre_barrier_stub(MacroAssembler* masm,\n+                                    G1PreBarrierStubC2* stub) const;\n+  void g1_write_barrier_post_c2(MacroAssembler* masm,\n+                                Register store_addr,\n+                                Register new_val,\n+                                Register tmp1,\n+                                Register tmp2,\n+                                G1PostBarrierStubC2* c2_stub,\n+                                bool decode_new_val);\n+  void generate_c2_post_barrier_stub(MacroAssembler* masm,\n+                                     G1PostBarrierStubC2* stub) const;\n+#endif\n","filename":"src\/hotspot\/cpu\/ppc\/gc\/g1\/g1BarrierSetAssembler_ppc.hpp","additions":25,"deletions":0,"binary":false,"changes":25,"status":"modified"},{"patch":"@@ -0,0 +1,684 @@\n+\/\/\n+\/\/ Copyright (c) 2024, Oracle and\/or its affiliates. All rights reserved.\n+\/\/ Copyright (c) 2024 SAP SE. All rights reserved.\n+\/\/ DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+\/\/\n+\/\/ This code is free software; you can redistribute it and\/or modify it\n+\/\/ under the terms of the GNU General Public License version 2 only, as\n+\/\/ published by the Free Software Foundation.\n+\/\/\n+\/\/ This code is distributed in the hope that it will be useful, but WITHOUT\n+\/\/ ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+\/\/ FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+\/\/ version 2 for more details (a copy is included in the LICENSE file that\n+\/\/ accompanied this code).\n+\/\/\n+\/\/ You should have received a copy of the GNU General Public License version\n+\/\/ 2 along with this work; if not, write to the Free Software Foundation,\n+\/\/ Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+\/\/\n+\/\/ Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+\/\/ or visit www.oracle.com if you need additional information or have any\n+\/\/ questions.\n+\/\/\n+\n+source_hpp %{\n+\n+#include \"gc\/g1\/c2\/g1BarrierSetC2.hpp\"\n+#include \"gc\/shared\/gc_globals.hpp\"\n+\n+%}\n+\n+source %{\n+\n+#include \"gc\/g1\/g1BarrierSetAssembler_ppc.hpp\"\n+#include \"gc\/g1\/g1BarrierSetRuntime.hpp\"\n+\n+static void pre_write_barrier(MacroAssembler* masm,\n+                              const MachNode* node,\n+                              Register obj,\n+                              Register pre_val,\n+                              Register tmp1,\n+                              Register tmp2 = noreg, \/\/ only needed with CompressedOops when pre_val needs to be preserved\n+                              RegSet preserve = RegSet(),\n+                              RegSet no_preserve = RegSet()) {\n+  if (!G1PreBarrierStubC2::needs_barrier(node)) {\n+    return;\n+  }\n+  Assembler::InlineSkippedInstructionsCounter skip_counter(masm);\n+  G1BarrierSetAssembler* g1_asm = static_cast<G1BarrierSetAssembler*>(BarrierSet::barrier_set()->barrier_set_assembler());\n+  G1PreBarrierStubC2* const stub = G1PreBarrierStubC2::create(node);\n+  for (RegSetIterator<Register> reg = preserve.begin(); *reg != noreg; ++reg) {\n+    stub->preserve(*reg);\n+  }\n+  for (RegSetIterator<Register> reg = no_preserve.begin(); *reg != noreg; ++reg) {\n+    stub->dont_preserve(*reg);\n+  }\n+  g1_asm->g1_write_barrier_pre_c2(masm, obj, pre_val, tmp1, (tmp2 != noreg) ? tmp2 : pre_val, stub);\n+}\n+\n+static void post_write_barrier(MacroAssembler* masm,\n+                               const MachNode* node,\n+                               Register store_addr,\n+                               Register new_val,\n+                               Register tmp1,\n+                               Register tmp2,\n+                               bool decode_new_val = false) {\n+  if (!G1PostBarrierStubC2::needs_barrier(node)) {\n+    return;\n+  }\n+  Assembler::InlineSkippedInstructionsCounter skip_counter(masm);\n+  G1BarrierSetAssembler* g1_asm = static_cast<G1BarrierSetAssembler*>(BarrierSet::barrier_set()->barrier_set_assembler());\n+  G1PostBarrierStubC2* const stub = G1PostBarrierStubC2::create(node);\n+  g1_asm->g1_write_barrier_post_c2(masm, store_addr, new_val, tmp1, tmp2, stub, decode_new_val);\n+}\n+\n+%}\n+\n+instruct g1StoreP(indirect mem, iRegPsrc src, iRegPdst tmp1, iRegPdst tmp2, flagsRegCR0 cr0)\n+%{\n+  predicate(UseG1GC && n->as_Store()->barrier_data() != 0);\n+  match(Set mem (StoreP mem src));\n+  effect(TEMP tmp1, TEMP tmp2, KILL cr0);\n+  ins_cost(2 * MEMORY_REF_COST);\n+  format %{ \"std    $mem, $src\\t# ptr\" %}\n+  ins_encode %{\n+    pre_write_barrier(masm, this,\n+                      $mem$$Register,\n+                      $tmp1$$Register,\n+                      $tmp2$$Register,\n+                      noreg,\n+                      RegSet::of($mem$$Register, $src$$Register) \/* preserve *\/);\n+    __ std($src$$Register, 0, $mem$$Register);\n+    post_write_barrier(masm, this,\n+                       $mem$$Register,\n+                       $src$$Register \/* new_val *\/,\n+                       $tmp1$$Register,\n+                       $tmp2$$Register);\n+  %}\n+  ins_pipe(pipe_class_default);\n+%}\n+\n+instruct g1StoreN(indirect mem, iRegNsrc src, iRegPdst tmp1, iRegPdst tmp2, flagsRegCR0 cr0)\n+%{\n+  predicate(UseG1GC && n->as_Store()->barrier_data() != 0);\n+  match(Set mem (StoreN mem src));\n+  effect(TEMP tmp1, TEMP tmp2, KILL cr0);\n+  ins_cost(2 * MEMORY_REF_COST);\n+  format %{ \"stw    $mem, $src\\t# ptr\" %}\n+  ins_encode %{\n+    pre_write_barrier(masm, this,\n+                      $mem$$Register,\n+                      $tmp1$$Register,\n+                      $tmp2$$Register,\n+                      noreg,\n+                      RegSet::of($mem$$Register, $src$$Register) \/* preserve *\/);\n+    __ stw($src$$Register, 0, $mem$$Register);\n+    post_write_barrier(masm, this,\n+                       $mem$$Register,\n+                       $src$$Register \/* new_val *\/,\n+                       $tmp1$$Register,\n+                       $tmp2$$Register,\n+                       true \/* decode_new_val *\/);\n+  %}\n+  ins_pipe(pipe_class_default);\n+%}\n+\n+instruct g1EncodePAndStoreN(indirect mem, iRegPsrc src, iRegPdst tmp1, iRegPdst tmp2, flagsRegCR0 cr0)\n+%{\n+  predicate(UseG1GC && n->as_Store()->barrier_data() != 0);\n+  match(Set mem (StoreN mem (EncodeP src)));\n+  effect(TEMP tmp1, TEMP tmp2, KILL cr0);\n+  ins_cost(2 * MEMORY_REF_COST);\n+  format %{ \"encode_heap_oop $src\\n\\t\"\n+            \"stw   $mem, $src\\t# ptr\" %}\n+  ins_encode %{\n+    pre_write_barrier(masm, this,\n+                      $mem$$Register,\n+                      $tmp1$$Register,\n+                      $tmp2$$Register,\n+                      noreg,\n+                      RegSet::of($mem$$Register, $src$$Register) \/* preserve *\/);\n+    Register encoded_oop = noreg;\n+    if ((barrier_data() & G1C2BarrierPostNotNull) == 0) {\n+      encoded_oop = __ encode_heap_oop($tmp2$$Register, $src$$Register);\n+    } else {\n+      encoded_oop = __ encode_heap_oop_not_null($tmp2$$Register, $src$$Register);\n+    }\n+    __ stw(encoded_oop, 0, $mem$$Register);\n+    post_write_barrier(masm, this,\n+                       $mem$$Register,\n+                       $src$$Register \/* new_val *\/,\n+                       $tmp1$$Register,\n+                       $tmp2$$Register);\n+  %}\n+  ins_pipe(pipe_class_default);\n+%}\n+\n+instruct g1CompareAndExchangeP(iRegPdst res, indirect mem, iRegPsrc oldval, iRegPsrc newval, iRegPdst tmp1, iRegPdst tmp2, flagsRegCR0 cr0)\n+%{\n+  predicate(UseG1GC && n->as_LoadStore()->barrier_data() != 0 &&\n+            (((CompareAndExchangeNode*)n)->order() != MemNode::acquire && ((CompareAndExchangeNode*)n)->order() != MemNode::seqcst));\n+  match(Set res (CompareAndExchangeP mem (Binary oldval newval)));\n+  effect(TEMP_DEF res, TEMP tmp1, TEMP tmp2, KILL cr0);\n+  format %{ \"cmpxchgd $newval, $mem\" %}\n+  ins_encode %{\n+    Label no_update;\n+    __ cmpxchgd(CCR0, $res$$Register, $oldval$$Register, $newval$$Register, $mem$$Register,\n+                MacroAssembler::MemBarNone, MacroAssembler::cmpxchgx_hint_atomic_update(),\n+                noreg, &no_update, true);\n+    \/\/ Pass oldval to SATB which is the only value which can get overwritten.\n+    \/\/ Can be done after cmpxchg because there's no safepoint here.\n+    pre_write_barrier(masm, this,\n+                      noreg,\n+                      $oldval$$Register,\n+                      $tmp1$$Register,\n+                      $tmp2$$Register,\n+                      RegSet::of($mem$$Register, $newval$$Register) \/* preserve *\/);\n+    post_write_barrier(masm, this,\n+                       $mem$$Register,\n+                       $newval$$Register,\n+                       $tmp1$$Register,\n+                       $tmp2$$Register);\n+    __ bind(no_update);\n+  %}\n+  ins_pipe(pipe_class_default);\n+%}\n+\n+instruct g1CompareAndExchangeP_acq(iRegPdst res, indirect mem, iRegPsrc oldval, iRegPsrc newval, iRegPdst tmp1, iRegPdst tmp2, flagsRegCR0 cr0)\n+%{\n+  predicate(UseG1GC && n->as_LoadStore()->barrier_data() != 0 &&\n+            (((CompareAndExchangeNode*)n)->order() == MemNode::acquire || ((CompareAndExchangeNode*)n)->order() == MemNode::seqcst));\n+  match(Set res (CompareAndExchangeP mem (Binary oldval newval)));\n+  effect(TEMP_DEF res, TEMP tmp1, TEMP tmp2, KILL cr0);\n+  format %{ \"cmpxchgd acq $newval, $mem\" %}\n+  ins_encode %{\n+    Label no_update;\n+    __ cmpxchgd(CCR0, $res$$Register, $oldval$$Register, $newval$$Register, $mem$$Register,\n+                MacroAssembler::MemBarNone, MacroAssembler::cmpxchgx_hint_atomic_update(),\n+                noreg, &no_update, true);\n+    \/\/ Pass oldval to SATB which is the only value which can get overwritten.\n+    \/\/ Can be done after cmpxchg because there's no safepoint here.\n+    pre_write_barrier(masm, this,\n+                      noreg,\n+                      $oldval$$Register,\n+                      $tmp1$$Register,\n+                      $tmp2$$Register,\n+                      RegSet::of($mem$$Register, $newval$$Register) \/* preserve *\/);\n+    post_write_barrier(masm, this,\n+                       $mem$$Register,\n+                       $newval$$Register,\n+                       $tmp1$$Register,\n+                       $tmp2$$Register);\n+    __ bind(no_update);\n+    if (support_IRIW_for_not_multiple_copy_atomic_cpu) {\n+      __ isync();\n+    } else {\n+      \/\/ isync would be sufficient in case of CompareAndExchangeAcquire, but we currently don't optimize for that.\n+      __ sync();\n+    }\n+  %}\n+  ins_pipe(pipe_class_default);\n+%}\n+\n+instruct g1CompareAndExchangeN(iRegNdst res, indirect mem, iRegNsrc oldval, iRegNsrc newval, iRegPdst tmp1, iRegPdst tmp2, flagsRegCR0 cr0)\n+%{\n+  predicate(UseG1GC && n->as_LoadStore()->barrier_data() != 0 &&\n+            (((CompareAndExchangeNode*)n)->order() != MemNode::acquire && ((CompareAndExchangeNode*)n)->order() != MemNode::seqcst));\n+  match(Set res (CompareAndExchangeN mem (Binary oldval newval)));\n+  effect(TEMP_DEF res, TEMP tmp1, TEMP tmp2, KILL cr0);\n+  format %{ \"cmpxchgw $newval, $mem\" %}\n+  ins_encode %{\n+    Label no_update;\n+    __ cmpxchgw(CCR0, $res$$Register, $oldval$$Register, $newval$$Register, $mem$$Register,\n+                MacroAssembler::MemBarNone, MacroAssembler::cmpxchgx_hint_atomic_update(),\n+                noreg, &no_update, true);\n+    \/\/ Pass oldval to SATB which is the only value which can get overwritten.\n+    \/\/ Can be done after cmpxchg because there's no safepoint here.\n+    pre_write_barrier(masm, this,\n+                      noreg,\n+                      $oldval$$Register,\n+                      $tmp1$$Register,\n+                      $tmp2$$Register,\n+                      RegSet::of($mem$$Register, $newval$$Register) \/* preserve *\/);\n+    post_write_barrier(masm, this,\n+                       $mem$$Register,\n+                       $newval$$Register,\n+                       $tmp1$$Register,\n+                       $tmp2$$Register,\n+                       true \/* decode_new_val *\/);\n+    __ bind(no_update);\n+  %}\n+  ins_pipe(pipe_class_default);\n+%}\n+\n+instruct g1CompareAndExchangeN_acq(iRegNdst res, indirect mem, iRegNsrc oldval, iRegNsrc newval, iRegPdst tmp1, iRegPdst tmp2, flagsRegCR0 cr0)\n+%{\n+  predicate(UseG1GC && n->as_LoadStore()->barrier_data() != 0 &&\n+            (((CompareAndExchangeNode*)n)->order() == MemNode::acquire || ((CompareAndExchangeNode*)n)->order() == MemNode::seqcst));\n+  match(Set res (CompareAndExchangeN mem (Binary oldval newval)));\n+  effect(TEMP_DEF res, TEMP tmp1, TEMP tmp2, KILL cr0);\n+  format %{ \"cmpxchgw acq $newval, $mem\" %}\n+  ins_encode %{\n+    Label no_update;\n+    __ cmpxchgw(CCR0, $res$$Register, $oldval$$Register, $newval$$Register, $mem$$Register,\n+                MacroAssembler::MemBarNone, MacroAssembler::cmpxchgx_hint_atomic_update(),\n+                noreg, &no_update, true);\n+    \/\/ Pass oldval to SATB which is the only value which can get overwritten.\n+    \/\/ Can be done after cmpxchg because there's no safepoint here.\n+    pre_write_barrier(masm, this,\n+                      noreg,\n+                      $oldval$$Register,\n+                      $tmp1$$Register,\n+                      $tmp2$$Register,\n+                      RegSet::of($mem$$Register, $newval$$Register) \/* preserve *\/);\n+    post_write_barrier(masm, this,\n+                       $mem$$Register,\n+                       $newval$$Register,\n+                       $tmp1$$Register,\n+                       $tmp2$$Register,\n+                       true \/* decode_new_val *\/);\n+    __ bind(no_update);\n+    if (support_IRIW_for_not_multiple_copy_atomic_cpu) {\n+      __ isync();\n+    } else {\n+      \/\/ isync would be sufficient in case of CompareAndExchangeAcquire, but we currently don't optimize for that.\n+      __ sync();\n+    }\n+  %}\n+  ins_pipe(pipe_class_default);\n+%}\n+\n+instruct g1CompareAndSwapP(iRegIdst res, indirect mem, iRegPsrc oldval, iRegPsrc newval, iRegPdst tmp, flagsRegCR0 cr0)\n+%{\n+  predicate(UseG1GC && n->as_LoadStore()->barrier_data() != 0 &&\n+            (((CompareAndSwapNode*)n)->order() != MemNode::acquire && ((CompareAndSwapNode*)n)->order() != MemNode::seqcst));\n+  match(Set res (CompareAndSwapP mem (Binary oldval newval)));\n+  effect(TEMP_DEF res, TEMP tmp, KILL cr0);\n+  format %{ \"CMPXCHGD $res, $mem, $oldval, $newval; as bool; ptr\" %}\n+  ins_encode %{\n+    Label no_update;\n+    __ li($res$$Register, 0);\n+    __ cmpxchgd(CCR0, R0, $oldval$$Register, $newval$$Register, $mem$$Register,\n+                MacroAssembler::MemBarNone, MacroAssembler::cmpxchgx_hint_atomic_update(),\n+                noreg, &no_update, true);\n+    \/\/ Pass oldval to SATB which is the only value which can get overwritten.\n+    \/\/ Can be done after cmpxchg because there's no safepoint here.\n+    pre_write_barrier(masm, this,\n+                      noreg,\n+                      $oldval$$Register \/* pre_val *\/,\n+                      $tmp$$Register,\n+                      $res$$Register \/* temp *\/,\n+                      RegSet::of($mem$$Register, $newval$$Register) \/* preserve *\/,\n+                      RegSet::of($res$$Register) \/* no_preserve *\/);\n+    post_write_barrier(masm, this,\n+                       $mem$$Register,\n+                       $newval$$Register,\n+                       $tmp$$Register,\n+                       $res$$Register \/* temp *\/);\n+    __ li($res$$Register, 1);\n+    __ bind(no_update);\n+  %}\n+  ins_pipe(pipe_class_default);\n+%}\n+\n+instruct g1CompareAndSwapP_acq(iRegIdst res, indirect mem, iRegPsrc oldval, iRegPsrc newval, iRegPdst tmp, flagsRegCR0 cr0)\n+%{\n+  predicate(UseG1GC && n->as_LoadStore()->barrier_data() != 0 &&\n+            (((CompareAndSwapNode*)n)->order() == MemNode::acquire || ((CompareAndSwapNode*)n)->order() == MemNode::seqcst));\n+  match(Set res (CompareAndSwapP mem (Binary oldval newval)));\n+  effect(TEMP_DEF res, TEMP tmp, KILL cr0);\n+  format %{ \"CMPXCHGD acq $res, $mem, $oldval, $newval; as bool; ptr\" %}\n+  ins_encode %{\n+    Label no_update;\n+    __ li($res$$Register, 0);\n+    __ cmpxchgd(CCR0, R0, $oldval$$Register, $newval$$Register, $mem$$Register,\n+                MacroAssembler::MemBarNone, MacroAssembler::cmpxchgx_hint_atomic_update(),\n+                noreg, &no_update, true);\n+    \/\/ Pass oldval to SATB which is the only value which can get overwritten.\n+    \/\/ Can be done after cmpxchg because there's no safepoint here.\n+    pre_write_barrier(masm, this,\n+                      noreg,\n+                      $oldval$$Register \/* pre_val *\/,\n+                      $tmp$$Register,\n+                      $res$$Register \/* temp *\/,\n+                      RegSet::of($mem$$Register, $newval$$Register) \/* preserve *\/,\n+                      RegSet::of($res$$Register) \/* no_preserve *\/);\n+    post_write_barrier(masm, this,\n+                       $mem$$Register,\n+                       $newval$$Register,\n+                       $tmp$$Register,\n+                       $res$$Register \/* temp *\/);\n+    __ li($res$$Register, 1);\n+    __ bind(no_update);\n+    if (support_IRIW_for_not_multiple_copy_atomic_cpu) {\n+      __ isync();\n+    } else {\n+      \/\/ isync would be sufficient in case of CompareAndExchangeAcquire, but we currently don't optimize for that.\n+      __ sync();\n+    }\n+  %}\n+  ins_pipe(pipe_class_default);\n+%}\n+\n+instruct g1CompareAndSwapN(iRegIdst res, indirect mem, iRegNsrc oldval, iRegNsrc newval, iRegPdst tmp, flagsRegCR0 cr0)\n+%{\n+  predicate(UseG1GC && n->as_LoadStore()->barrier_data() != 0 &&\n+            (((CompareAndSwapNode*)n)->order() != MemNode::acquire && ((CompareAndSwapNode*)n)->order() != MemNode::seqcst));\n+  match(Set res (CompareAndSwapN mem (Binary oldval newval)));\n+  effect(TEMP_DEF res, TEMP tmp, KILL cr0);\n+  format %{ \"CMPXCHGW $res, $mem, $oldval, $newval; as bool; ptr\" %}\n+  ins_encode %{\n+    Label no_update;\n+    __ li($res$$Register, 0);\n+    __ cmpxchgw(CCR0, R0, $oldval$$Register, $newval$$Register, $mem$$Register,\n+                MacroAssembler::MemBarNone, MacroAssembler::cmpxchgx_hint_atomic_update(),\n+                noreg, &no_update, true);\n+    \/\/ Pass oldval to SATB which is the only value which can get overwritten.\n+    \/\/ Can be done after cmpxchg because there's no safepoint here.\n+    pre_write_barrier(masm, this,\n+                      noreg,\n+                      $oldval$$Register \/* pre_val *\/,\n+                      $tmp$$Register,\n+                      $res$$Register \/* temp *\/,\n+                      RegSet::of($mem$$Register, $newval$$Register) \/* preserve *\/,\n+                      RegSet::of($res$$Register) \/* no_preserve *\/);\n+    post_write_barrier(masm, this,\n+                       $mem$$Register,\n+                       $newval$$Register,\n+                       $tmp$$Register,\n+                       $res$$Register \/* temp *\/,\n+                       true \/* decode_new_val *\/);\n+    __ li($res$$Register, 1);\n+    __ bind(no_update);\n+  %}\n+  ins_pipe(pipe_class_default);\n+%}\n+\n+instruct g1CompareAndSwapN_acq(iRegIdst res, indirect mem, iRegNsrc oldval, iRegNsrc newval, iRegPdst tmp, flagsRegCR0 cr0)\n+%{\n+  predicate(UseG1GC && n->as_LoadStore()->barrier_data() != 0 &&\n+            (((CompareAndSwapNode*)n)->order() == MemNode::acquire || ((CompareAndSwapNode*)n)->order() == MemNode::seqcst));\n+  match(Set res (CompareAndSwapN mem (Binary oldval newval)));\n+  effect(TEMP_DEF res, TEMP tmp, KILL cr0);\n+  format %{ \"CMPXCHGW acq $res, $mem, $oldval, $newval; as bool; ptr\" %}\n+  ins_encode %{\n+    Label no_update;\n+    __ li($res$$Register, 0);\n+    __ cmpxchgw(CCR0, R0, $oldval$$Register, $newval$$Register, $mem$$Register,\n+                MacroAssembler::MemBarNone, MacroAssembler::cmpxchgx_hint_atomic_update(),\n+                noreg, &no_update, true);\n+    \/\/ Pass oldval to SATB which is the only value which can get overwritten.\n+    \/\/ Can be done after cmpxchg because there's no safepoint here.\n+    pre_write_barrier(masm, this,\n+                      noreg,\n+                      $oldval$$Register \/* pre_val *\/,\n+                      $tmp$$Register,\n+                      $res$$Register \/* temp *\/,\n+                      RegSet::of($mem$$Register, $newval$$Register) \/* preserve *\/,\n+                      RegSet::of($res$$Register) \/* no_preserve *\/);\n+    post_write_barrier(masm, this,\n+                       $mem$$Register,\n+                       $newval$$Register,\n+                       $tmp$$Register,\n+                       $res$$Register \/* temp *\/,\n+                       true \/* decode_new_val *\/);\n+    __ li($res$$Register, 1);\n+    __ bind(no_update);\n+    if (support_IRIW_for_not_multiple_copy_atomic_cpu) {\n+      __ isync();\n+    } else {\n+      \/\/ isync would be sufficient in case of CompareAndExchangeAcquire, but we currently don't optimize for that.\n+      __ sync();\n+    }\n+  %}\n+  ins_pipe(pipe_class_default);\n+%}\n+\n+instruct weakG1CompareAndSwapP(iRegIdst res, indirect mem, iRegPsrc oldval, iRegPsrc newval, iRegPdst tmp, flagsRegCR0 cr0)\n+%{\n+  predicate(UseG1GC && n->as_LoadStore()->barrier_data() != 0 &&\n+            (((CompareAndSwapNode*)n)->order() != MemNode::acquire && ((CompareAndSwapNode*)n)->order() != MemNode::seqcst));\n+  match(Set res (WeakCompareAndSwapP mem (Binary oldval newval)));\n+  effect(TEMP_DEF res, TEMP tmp, KILL cr0);\n+  format %{ \"weak CMPXCHGD $res, $mem, $oldval, $newval; as bool; ptr\" %}\n+  ins_encode %{\n+    Label no_update;\n+    __ li($res$$Register, 0);\n+    __ cmpxchgd(CCR0, R0, $oldval$$Register, $newval$$Register, $mem$$Register,\n+                MacroAssembler::MemBarNone, MacroAssembler::cmpxchgx_hint_atomic_update(),\n+                noreg, &no_update, true, true);\n+    \/\/ Pass oldval to SATB which is the only value which can get overwritten.\n+    \/\/ Can be done after cmpxchg because there's no safepoint here.\n+    pre_write_barrier(masm, this,\n+                      noreg,\n+                      $oldval$$Register \/* pre_val *\/,\n+                      $tmp$$Register,\n+                      $res$$Register \/* temp *\/,\n+                      RegSet::of($mem$$Register, $newval$$Register) \/* preserve *\/,\n+                      RegSet::of($res$$Register) \/* no_preserve *\/);\n+    post_write_barrier(masm, this,\n+                       $mem$$Register,\n+                       $newval$$Register,\n+                       $tmp$$Register,\n+                       $res$$Register \/* temp *\/);\n+    __ li($res$$Register, 1);\n+    __ bind(no_update);\n+  %}\n+  ins_pipe(pipe_class_default);\n+%}\n+\n+instruct weakG1CompareAndSwapP_acq(iRegIdst res, indirect mem, iRegPsrc oldval, iRegPsrc newval, iRegPdst tmp, flagsRegCR0 cr0)\n+%{\n+  predicate(UseG1GC && n->as_LoadStore()->barrier_data() != 0 &&\n+            (((CompareAndSwapNode*)n)->order() == MemNode::acquire || ((CompareAndSwapNode*)n)->order() == MemNode::seqcst));\n+  match(Set res (WeakCompareAndSwapP mem (Binary oldval newval)));\n+  effect(TEMP_DEF res, TEMP tmp, KILL cr0);\n+  format %{ \"weak CMPXCHGD acq $res, $mem, $oldval, $newval; as bool; ptr\" %}\n+  ins_encode %{\n+    Label no_update;\n+    __ li($res$$Register, 0);\n+    __ cmpxchgd(CCR0, R0, $oldval$$Register, $newval$$Register, $mem$$Register,\n+                MacroAssembler::MemBarNone, MacroAssembler::cmpxchgx_hint_atomic_update(),\n+                noreg, &no_update, true, true);\n+    \/\/ Pass oldval to SATB which is the only value which can get overwritten.\n+    \/\/ Can be done after cmpxchg because there's no safepoint here.\n+    pre_write_barrier(masm, this,\n+                      noreg,\n+                      $oldval$$Register \/* pre_val *\/,\n+                      $tmp$$Register,\n+                      $res$$Register \/* temp *\/,\n+                      RegSet::of($mem$$Register, $newval$$Register) \/* preserve *\/,\n+                      RegSet::of($res$$Register) \/* no_preserve *\/);\n+    post_write_barrier(masm, this,\n+                       $mem$$Register,\n+                       $newval$$Register,\n+                       $tmp$$Register,\n+                       $res$$Register \/* temp *\/);\n+    __ li($res$$Register, 1);\n+    if (support_IRIW_for_not_multiple_copy_atomic_cpu) {\n+      __ isync();\n+    } else {\n+      \/\/ isync would be sufficient in case of CompareAndExchangeAcquire, but we currently don't optimize for that.\n+      __ sync();\n+    }\n+    __ bind(no_update); \/\/ weak version requires no memory barrier on failure\n+  %}\n+  ins_pipe(pipe_class_default);\n+%}\n+\n+instruct weakG1CompareAndSwapN(iRegIdst res, indirect mem, iRegNsrc oldval, iRegNsrc newval, iRegPdst tmp, flagsRegCR0 cr0)\n+%{\n+  predicate(UseG1GC && n->as_LoadStore()->barrier_data() != 0 &&\n+            (((CompareAndSwapNode*)n)->order() != MemNode::acquire && ((CompareAndSwapNode*)n)->order() != MemNode::seqcst));\n+  match(Set res (WeakCompareAndSwapN mem (Binary oldval newval)));\n+  effect(TEMP_DEF res, TEMP tmp, KILL cr0);\n+  format %{ \"weak CMPXCHGW $res, $mem, $oldval, $newval; as bool; ptr\" %}\n+  ins_encode %{\n+    Label no_update;\n+    __ li($res$$Register, 0);\n+    __ cmpxchgw(CCR0, R0, $oldval$$Register, $newval$$Register, $mem$$Register,\n+                MacroAssembler::MemBarNone, MacroAssembler::cmpxchgx_hint_atomic_update(),\n+                noreg, &no_update, true, true);\n+    \/\/ Pass oldval to SATB which is the only value which can get overwritten.\n+    \/\/ Can be done after cmpxchg because there's no safepoint here.\n+    pre_write_barrier(masm, this,\n+                      noreg,\n+                      $oldval$$Register \/* pre_val *\/,\n+                      $tmp$$Register,\n+                      $res$$Register \/* temp *\/,\n+                      RegSet::of($mem$$Register, $newval$$Register) \/* preserve *\/,\n+                      RegSet::of($res$$Register) \/* no_preserve *\/);\n+    post_write_barrier(masm, this,\n+                       $mem$$Register,\n+                       $newval$$Register,\n+                       $tmp$$Register,\n+                       $res$$Register \/* temp *\/,\n+                       true \/* decode_new_val *\/);\n+    __ li($res$$Register, 1);\n+    __ bind(no_update);\n+  %}\n+  ins_pipe(pipe_class_default);\n+%}\n+\n+instruct weakG1CompareAndSwapN_acq(iRegIdst res, indirect mem, iRegNsrc oldval, iRegNsrc newval, iRegPdst tmp, flagsRegCR0 cr0)\n+%{\n+  predicate(UseG1GC && n->as_LoadStore()->barrier_data() != 0 &&\n+            (((CompareAndSwapNode*)n)->order() == MemNode::acquire || ((CompareAndSwapNode*)n)->order() == MemNode::seqcst));\n+  match(Set res (WeakCompareAndSwapN mem (Binary oldval newval)));\n+  effect(TEMP_DEF res, TEMP tmp, KILL cr0);\n+  format %{ \"weak CMPXCHGW acq $res, $mem, $oldval, $newval; as bool; ptr\" %}\n+  ins_encode %{\n+    Label no_update;\n+    __ li($res$$Register, 0);\n+    __ cmpxchgw(CCR0, R0, $oldval$$Register, $newval$$Register, $mem$$Register,\n+                MacroAssembler::MemBarNone, MacroAssembler::cmpxchgx_hint_atomic_update(),\n+                noreg, &no_update, true, true);\n+    \/\/ Pass oldval to SATB which is the only value which can get overwritten.\n+    \/\/ Can be done after cmpxchg because there's no safepoint here.\n+    pre_write_barrier(masm, this,\n+                      noreg,\n+                      $oldval$$Register \/* pre_val *\/,\n+                      $tmp$$Register,\n+                      $res$$Register \/* temp *\/,\n+                      RegSet::of($mem$$Register, $newval$$Register) \/* preserve *\/,\n+                      RegSet::of($res$$Register) \/* no_preserve *\/);\n+    post_write_barrier(masm, this,\n+                       $mem$$Register,\n+                       $newval$$Register,\n+                       $tmp$$Register,\n+                       $res$$Register \/* temp *\/,\n+                       true \/* decode_new_val *\/);\n+    __ li($res$$Register, 1);\n+    if (support_IRIW_for_not_multiple_copy_atomic_cpu) {\n+      __ isync();\n+    } else {\n+      \/\/ isync would be sufficient in case of CompareAndExchangeAcquire, but we currently don't optimize for that.\n+      __ sync();\n+    }\n+    __ bind(no_update); \/\/ weak version requires no memory barrier on failure\n+  %}\n+  ins_pipe(pipe_class_default);\n+%}\n+\n+instruct g1GetAndSetP(iRegPdst res, indirect mem, iRegPsrc newval, iRegPdst tmp1, iRegPdst tmp2, flagsRegCR0 cr0)\n+%{\n+  predicate(UseG1GC && n->as_LoadStore()->barrier_data() != 0);\n+  match(Set res (GetAndSetP mem newval));\n+  effect(TEMP_DEF res, TEMP tmp1, TEMP tmp2, KILL cr0);\n+  format %{ \"GetAndSetP    $newval, $mem\" %}\n+  ins_encode %{\n+    assert_different_registers($mem$$Register, $newval$$Register);\n+    __ getandsetd($res$$Register, $newval$$Register, $mem$$Register,\n+                  MacroAssembler::cmpxchgx_hint_atomic_update());\n+    \/\/ Can be done after cmpxchg because there's no safepoint here.\n+    pre_write_barrier(masm, this,\n+                      noreg \/* obj *\/,\n+                      $res$$Register \/* res *\/,\n+                      $tmp1$$Register,\n+                      $tmp2$$Register,\n+                      RegSet::of($mem$$Register, $newval$$Register) \/* preserve *\/);\n+    post_write_barrier(masm, this,\n+                       $mem$$Register,\n+                       $newval$$Register,\n+                       $tmp1$$Register,\n+                       $tmp2$$Register);\n+    if (support_IRIW_for_not_multiple_copy_atomic_cpu) {\n+      __ isync();\n+    } else {\n+      __ sync();\n+    }\n+  %}\n+  ins_pipe(pipe_class_default);\n+%}\n+\n+instruct g1GetAndSetN(iRegNdst res, indirect mem, iRegNsrc newval, iRegPdst tmp1, iRegPdst tmp2, flagsRegCR0 cr0)\n+%{\n+  predicate(UseG1GC && n->as_LoadStore()->barrier_data() != 0);\n+  match(Set res (GetAndSetN mem newval));\n+  effect(TEMP_DEF res, TEMP tmp1, TEMP tmp2, KILL cr0);\n+  format %{ \"GetAndSetN    $newval, $mem\" %}\n+  ins_encode %{\n+    assert_different_registers($mem$$Register, $newval$$Register);\n+    __ getandsetw($res$$Register, $newval$$Register, $mem$$Register,\n+                  MacroAssembler::cmpxchgx_hint_atomic_update());\n+    \/\/ Can be done after cmpxchg because there's no safepoint here.\n+    pre_write_barrier(masm, this,\n+                      noreg \/* obj *\/,\n+                      $res$$Register \/* res *\/,\n+                      $tmp1$$Register,\n+                      $tmp2$$Register,\n+                      RegSet::of($mem$$Register, $newval$$Register) \/* preserve *\/);\n+    post_write_barrier(masm, this,\n+                       $mem$$Register,\n+                       $newval$$Register,\n+                       $tmp1$$Register,\n+                       $tmp2$$Register,\n+                       true \/* decode_new_val *\/);\n+    if (support_IRIW_for_not_multiple_copy_atomic_cpu) {\n+      __ isync();\n+    } else {\n+      __ sync();\n+    }\n+  %}\n+  ins_pipe(pipe_class_default);\n+%}\n+\n+instruct g1LoadP(iRegPdst dst, memoryAlg4 mem, iRegPdst tmp, flagsRegCR0 cr0)\n+%{\n+  predicate(UseG1GC && n->as_Load()->is_unordered() && n->as_Load()->barrier_data() != 0);\n+  \/\/ This instruction does not need an acquiring counterpart because it is only\n+  \/\/ used for reference loading (Reference::get()).\n+  match(Set dst (LoadP mem));\n+  effect(TEMP_DEF dst, TEMP tmp, KILL cr0);\n+  ins_cost(2 * MEMORY_REF_COST);\n+  format %{ \"ld    $dst, $mem\\t# ptr\" %}\n+  ins_encode %{\n+    __ ld($dst$$Register, $mem$$disp, $mem$$base$$Register);\n+    pre_write_barrier(masm, this,\n+                      noreg \/* obj *\/,\n+                      $dst$$Register \/* pre_val *\/,\n+                      $tmp$$Register);\n+  %}\n+  ins_pipe(pipe_class_default);\n+%}\n+\n+instruct g1LoadN(iRegNdst dst, memoryAlg4 mem, iRegPdst tmp1, iRegPdst tmp2, flagsRegCR0 cr0)\n+%{\n+  predicate(UseG1GC && n->as_Load()->is_unordered() && n->as_Load()->barrier_data() != 0);\n+  \/\/ This instruction does not need an acquiring counterpart because it is only\n+  \/\/ used for reference loading (Reference::get()).\n+  match(Set dst (LoadN mem));\n+  effect(TEMP_DEF dst, TEMP tmp1, TEMP tmp2, KILL cr0);\n+  ins_cost(2 * MEMORY_REF_COST);\n+  format %{ \"lwz    $dst, $mem\\t# ptr\" %}\n+  ins_encode %{\n+    __ lwz($dst$$Register, $mem$$disp, $mem$$base$$Register);\n+    pre_write_barrier(masm, this,\n+                      noreg \/* obj *\/,\n+                      $dst$$Register,\n+                      $tmp1$$Register,\n+                      $tmp2$$Register);\n+  %}\n+  ins_pipe(pipe_class_default);\n+%}\n","filename":"src\/hotspot\/cpu\/ppc\/gc\/g1\/g1_ppc.ad","additions":684,"deletions":0,"binary":false,"changes":684,"status":"added"},{"patch":"@@ -1003,0 +1003,4 @@\n+  if (is_encode_and_store_pattern(n, m)) {\n+    mstack.push(m, Visit);\n+    return true;\n+  }\n@@ -5410,1 +5414,1 @@\n-  predicate(n->as_Load()->is_unordered() || followed_by_acquire(n));\n+  predicate((n->as_Load()->is_unordered() || followed_by_acquire(n)) && n->as_Load()->barrier_data() == 0);\n@@ -5422,0 +5426,1 @@\n+  predicate(n->as_Load()->barrier_data() == 0);\n@@ -5435,1 +5440,1 @@\n-  predicate(_kids[0]->_leaf->as_Load()->is_unordered() && CompressedOops::shift() == 0);\n+  predicate(_kids[0]->_leaf->as_Load()->is_unordered() && CompressedOops::shift() == 0 && _kids[0]->_leaf->as_Load()->barrier_data() == 0);\n@@ -6426,0 +6431,1 @@\n+  predicate(n->as_Store()->barrier_data() == 0);\n@@ -7480,0 +7486,1 @@\n+  predicate(n->as_LoadStore()->barrier_data() == 0);\n@@ -7679,1 +7686,1 @@\n-  predicate(((CompareAndSwapNode*)n)->order() != MemNode::acquire && ((CompareAndSwapNode*)n)->order() != MemNode::seqcst);\n+  predicate(((CompareAndSwapNode*)n)->order() != MemNode::acquire && ((CompareAndSwapNode*)n)->order() != MemNode::seqcst && n->as_LoadStore()->barrier_data() == 0);\n@@ -7693,1 +7700,1 @@\n-  predicate(((CompareAndSwapNode*)n)->order() == MemNode::acquire || ((CompareAndSwapNode*)n)->order() == MemNode::seqcst);\n+  predicate((((CompareAndSwapNode*)n)->order() == MemNode::acquire || ((CompareAndSwapNode*)n)->order() == MemNode::seqcst) && n->as_LoadStore()->barrier_data() == 0);\n@@ -7942,1 +7949,1 @@\n-  predicate(((CompareAndSwapNode*)n)->order() != MemNode::acquire && ((CompareAndSwapNode*)n)->order() != MemNode::seqcst);\n+  predicate(((CompareAndSwapNode*)n)->order() != MemNode::acquire && ((CompareAndSwapNode*)n)->order() != MemNode::seqcst && n->as_LoadStore()->barrier_data() == 0);\n@@ -7956,1 +7963,1 @@\n-  predicate(((CompareAndSwapNode*)n)->order() == MemNode::acquire || ((CompareAndSwapNode*)n)->order() == MemNode::seqcst);\n+  predicate((((CompareAndSwapNode*)n)->order() == MemNode::acquire || ((CompareAndSwapNode*)n)->order() == MemNode::seqcst) && n->as_LoadStore()->barrier_data() == 0);\n@@ -8265,0 +8272,1 @@\n+  predicate(n->as_LoadStore()->barrier_data() == 0);\n","filename":"src\/hotspot\/cpu\/ppc\/ppc.ad","additions":14,"deletions":6,"binary":false,"changes":20,"status":"modified"},{"patch":"@@ -30,0 +30,1 @@\n+#include \"utilities\/count_trailing_zeros.hpp\"\n@@ -558,0 +559,8 @@\n+template <>\n+inline Register AbstractRegSet<Register>::first() {\n+  if (_bitset == 0) { return noreg; }\n+  return as_Register(count_trailing_zeros(_bitset));\n+}\n+\n+typedef AbstractRegSet<Register> RegSet;\n+\n","filename":"src\/hotspot\/cpu\/ppc\/register_ppc.hpp","additions":9,"deletions":0,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -3,1 +3,1 @@\n- * Copyright (c) 2020, 2023, Huawei Technologies Co., Ltd. All rights reserved.\n+ * Copyright (c) 2020, 2024, Huawei Technologies Co., Ltd. All rights reserved.\n@@ -42,1 +42,4 @@\n-#endif\n+#endif \/\/ COMPILER1\n+#ifdef COMPILER2\n+#include \"gc\/g1\/c2\/g1BarrierSetC2.hpp\"\n+#endif \/\/ COMPILER2\n@@ -99,0 +102,49 @@\n+static void generate_queue_test_and_insertion(MacroAssembler* masm, ByteSize index_offset, ByteSize buffer_offset, Label& runtime,\n+                                              const Register thread, const Register value, const Register tmp1, const Register tmp2) {\n+  \/\/ Can we store a value in the given thread's buffer?\n+  \/\/ (The index field is typed as size_t.)\n+  __ ld(tmp1, Address(thread, in_bytes(index_offset)));   \/\/ tmp1 := *(index address)\n+  __ beqz(tmp1, runtime);                                 \/\/ jump to runtime if index == 0 (full buffer)\n+  \/\/ The buffer is not full, store value into it.\n+  __ sub(tmp1, tmp1, wordSize);                           \/\/ tmp1 := next index\n+  __ sd(tmp1, Address(thread, in_bytes(index_offset)));   \/\/ *(index address) := next index\n+  __ ld(tmp2, Address(thread, in_bytes(buffer_offset)));  \/\/ tmp2 := buffer address\n+  __ add(tmp2, tmp2, tmp1);\n+  __ sd(value, Address(tmp2));                            \/\/ *(buffer address + next index) := value\n+}\n+\n+static void generate_pre_barrier_fast_path(MacroAssembler* masm,\n+                                           const Register thread,\n+                                           const Register tmp1) {\n+  Address in_progress(thread, in_bytes(G1ThreadLocalData::satb_mark_queue_active_offset()));\n+  \/\/ Is marking active?\n+  if (in_bytes(SATBMarkQueue::byte_width_of_active()) == 4) {\n+    __ lwu(tmp1, in_progress);\n+  } else {\n+    assert(in_bytes(SATBMarkQueue::byte_width_of_active()) == 1, \"Assumption\");\n+    __ lbu(tmp1, in_progress);\n+  }\n+}\n+\n+static void generate_pre_barrier_slow_path(MacroAssembler* masm,\n+                                           const Register obj,\n+                                           const Register pre_val,\n+                                           const Register thread,\n+                                           const Register tmp1,\n+                                           const Register tmp2,\n+                                           Label& done,\n+                                           Label& runtime) {\n+  \/\/ Do we need to load the previous value?\n+  if (obj != noreg) {\n+    __ load_heap_oop(pre_val, Address(obj, 0), noreg, noreg, AS_RAW);\n+  }\n+  \/\/ Is the previous value null?\n+  __ beqz(pre_val, done, true);\n+  generate_queue_test_and_insertion(masm,\n+                                    G1ThreadLocalData::satb_mark_queue_index_offset(),\n+                                    G1ThreadLocalData::satb_mark_queue_buffer_offset(),\n+                                    runtime,\n+                                    thread, pre_val, tmp1, tmp2);\n+  __ j(done);\n+}\n+\n@@ -119,11 +171,2 @@\n-  Address in_progress(thread, in_bytes(G1ThreadLocalData::satb_mark_queue_active_offset()));\n-  Address index(thread, in_bytes(G1ThreadLocalData::satb_mark_queue_index_offset()));\n-  Address buffer(thread, in_bytes(G1ThreadLocalData::satb_mark_queue_buffer_offset()));\n-\n-  \/\/ Is marking active?\n-  if (in_bytes(SATBMarkQueue::byte_width_of_active()) == 4) { \/\/ 4-byte width\n-    __ lwu(tmp1, in_progress);\n-  } else {\n-    assert(in_bytes(SATBMarkQueue::byte_width_of_active()) == 1, \"Assumption\");\n-    __ lbu(tmp1, in_progress);\n-  }\n+  generate_pre_barrier_fast_path(masm, thread, tmp1);\n+  \/\/ If marking is not active (*(mark queue active address) == 0), jump to done\n@@ -131,25 +174,1 @@\n-\n-  \/\/ Do we need to load the previous value?\n-  if (obj != noreg) {\n-    __ load_heap_oop(pre_val, Address(obj, 0), noreg, noreg, AS_RAW);\n-  }\n-\n-  \/\/ Is the previous value null?\n-  __ beqz(pre_val, done);\n-\n-  \/\/ Can we store original value in the thread's buffer?\n-  \/\/ Is index == 0?\n-  \/\/ (The index field is typed as size_t.)\n-\n-  __ ld(tmp1, index);                  \/\/ tmp := *index_adr\n-  __ beqz(tmp1, runtime);              \/\/ tmp == 0?\n-                                       \/\/ If yes, goto runtime\n-\n-  __ sub(tmp1, tmp1, wordSize);        \/\/ tmp := tmp - wordSize\n-  __ sd(tmp1, index);                  \/\/ *index_adr := tmp\n-  __ ld(tmp2, buffer);\n-  __ add(tmp1, tmp1, tmp2);            \/\/ tmp := tmp + *buffer_adr\n-\n-  \/\/ Record the previous value\n-  __ sd(pre_val, Address(tmp1, 0));\n-  __ j(done);\n+  generate_pre_barrier_slow_path(masm, obj, pre_val, thread, tmp1, tmp2, done, runtime);\n@@ -174,0 +193,43 @@\n+static void generate_post_barrier_fast_path(MacroAssembler* masm,\n+                                            const Register store_addr,\n+                                            const Register new_val,\n+                                            const Register tmp1,\n+                                            const Register tmp2,\n+                                            Label& done,\n+                                            bool new_val_may_be_null) {\n+  \/\/ Does store cross heap regions?\n+  __ xorr(tmp1, store_addr, new_val);                    \/\/ tmp1 := store address ^ new value\n+  __ srli(tmp1, tmp1, G1HeapRegion::LogOfHRGrainBytes);  \/\/ tmp1 := ((store address ^ new value) >> LogOfHRGrainBytes)\n+  __ beqz(tmp1, done);\n+  \/\/ Crosses regions, storing null?\n+  if (new_val_may_be_null) {\n+    __ beqz(new_val, done);\n+  }\n+  \/\/ Storing region crossing non-null, is card young?\n+  __ srli(tmp1, store_addr, CardTable::card_shift());    \/\/ tmp1 := card address relative to card table base\n+  __ load_byte_map_base(tmp2);                           \/\/ tmp2 := card table base address\n+  __ add(tmp1, tmp1, tmp2);                              \/\/ tmp1 := card address\n+  __ lbu(tmp2, Address(tmp1));                           \/\/ tmp2 := card\n+}\n+\n+static void generate_post_barrier_slow_path(MacroAssembler* masm,\n+                                            const Register thread,\n+                                            const Register tmp1,\n+                                            const Register tmp2,\n+                                            Label& done,\n+                                            Label& runtime) {\n+  __ membar(MacroAssembler::StoreLoad);  \/\/ StoreLoad membar\n+  __ lbu(tmp2, Address(tmp1));           \/\/ tmp2 := card\n+  __ beqz(tmp2, done, true);\n+  \/\/ Storing a region crossing, non-null oop, card is clean.\n+  \/\/ Dirty card and log.\n+  STATIC_ASSERT(CardTable::dirty_card_val() == 0);\n+  __ sb(zr, Address(tmp1));       \/\/ *(card address) := dirty_card_val\n+  generate_queue_test_and_insertion(masm,\n+                                    G1ThreadLocalData::dirty_card_queue_index_offset(),\n+                                    G1ThreadLocalData::dirty_card_queue_buffer_offset(),\n+                                    runtime,\n+                                    thread, tmp1, tmp2, t0);\n+  __ j(done);\n+}\n+\n@@ -182,8 +244,2 @@\n-  assert(store_addr != noreg && new_val != noreg && tmp1 != noreg &&\n-         tmp2 != noreg, \"expecting a register\");\n-\n-  Address queue_index(thread, in_bytes(G1ThreadLocalData::dirty_card_queue_index_offset()));\n-  Address buffer(thread, in_bytes(G1ThreadLocalData::dirty_card_queue_buffer_offset()));\n-\n-  BarrierSet* bs = BarrierSet::barrier_set();\n-  CardTableBarrierSet* ctbs = barrier_set_cast<CardTableBarrierSet>(bs);\n+  assert(store_addr != noreg && new_val != noreg && tmp1 != noreg && tmp2 != noreg,\n+         \"expecting a register\");\n@@ -194,1 +250,5 @@\n-  \/\/ Does store cross heap regions?\n+  generate_post_barrier_fast_path(masm, store_addr, new_val, tmp1, tmp2, done, true \/* new_val_may_be_null *\/);\n+  \/\/ If card is young, jump to done (tmp2 holds the card value)\n+  __ mv(t0, (int)G1CardTable::g1_young_card_val());\n+  __ beq(tmp2, t0, done);   \/\/ card == young_card_val?\n+  generate_post_barrier_slow_path(masm, thread, tmp1, tmp2, done, runtime);\n@@ -196,3 +256,6 @@\n-  __ xorr(tmp1, store_addr, new_val);\n-  __ srli(tmp1, tmp1, G1HeapRegion::LogOfHRGrainBytes);\n-  __ beqz(tmp1, done);\n+  __ bind(runtime);\n+  \/\/ save the live input values\n+  RegSet saved = RegSet::of(store_addr);\n+  __ push_reg(saved, sp);\n+  __ call_VM_leaf(CAST_FROM_FN_PTR(address, G1BarrierSetRuntime::write_ref_field_post_entry), tmp1, thread);\n+  __ pop_reg(saved, sp);\n@@ -200,1 +263,2 @@\n-  \/\/ crosses regions, storing null?\n+  __ bind(done);\n+}\n@@ -202,1 +266,1 @@\n-  __ beqz(new_val, done);\n+#if defined(COMPILER2)\n@@ -204,1 +268,9 @@\n-  \/\/ storing region crossing non-null, is card already dirty?\n+static void generate_c2_barrier_runtime_call(MacroAssembler* masm, G1BarrierStubC2* stub, const Register arg, const address runtime_path) {\n+  SaveLiveRegisters save_registers(masm, stub);\n+  if (c_rarg0 != arg) {\n+    __ mv(c_rarg0, arg);\n+  }\n+  __ mv(c_rarg1, xthread);\n+  __ mv(t0, runtime_path);\n+  __ jalr(t0);\n+}\n@@ -206,1 +278,10 @@\n-  const Register card_addr = tmp1;\n+void G1BarrierSetAssembler::g1_write_barrier_pre_c2(MacroAssembler* masm,\n+                                                    Register obj,\n+                                                    Register pre_val,\n+                                                    Register thread,\n+                                                    Register tmp1,\n+                                                    Register tmp2,\n+                                                    G1PreBarrierStubC2* stub) {\n+  assert(thread == xthread, \"must be\");\n+  assert_different_registers(obj, pre_val, tmp1, tmp2);\n+  assert(pre_val != noreg && tmp1 != noreg && tmp2 != noreg, \"expecting a register\");\n@@ -208,1 +289,1 @@\n-  __ srli(card_addr, store_addr, CardTable::card_shift());\n+  stub->initialize_registers(obj, pre_val, thread, tmp1, tmp2);\n@@ -210,6 +291,3 @@\n-  \/\/ get the address of the card\n-  __ load_byte_map_base(tmp2);\n-  __ add(card_addr, card_addr, tmp2);\n-  __ lbu(tmp2, Address(card_addr));\n-  __ mv(t0, (int)G1CardTable::g1_young_card_val());\n-  __ beq(tmp2, t0, done);\n+  generate_pre_barrier_fast_path(masm, thread, tmp1);\n+  \/\/ If marking is active (*(mark queue active address) != 0), jump to stub (slow path)\n+  __ bnez(tmp1, *stub->entry(), true);\n@@ -217,1 +295,2 @@\n-  assert((int)CardTable::dirty_card_val() == 0, \"must be 0\");\n+  __ bind(*stub->continuation());\n+}\n@@ -219,1 +298,9 @@\n-  __ membar(MacroAssembler::StoreLoad);\n+void G1BarrierSetAssembler::generate_c2_pre_barrier_stub(MacroAssembler* masm,\n+                                                         G1PreBarrierStubC2* stub) const {\n+  Assembler::InlineSkippedInstructionsCounter skip_counter(masm);\n+  Label runtime;\n+  Register obj = stub->obj();\n+  Register pre_val = stub->pre_val();\n+  Register thread = stub->thread();\n+  Register tmp1 = stub->tmp1();\n+  Register tmp2 = stub->tmp2();\n@@ -221,2 +308,2 @@\n-  __ lbu(tmp2, Address(card_addr));\n-  __ beqz(tmp2, done);\n+  __ bind(*stub->entry());\n+  generate_pre_barrier_slow_path(masm, obj, pre_val, thread, tmp1, tmp2, *stub->continuation(), runtime);\n@@ -224,2 +311,4 @@\n-  \/\/ storing a region crossing, non-null oop, card is clean.\n-  \/\/ dirty card and log.\n+  __ bind(runtime);\n+  generate_c2_barrier_runtime_call(masm, stub, pre_val, CAST_FROM_FN_PTR(address, G1BarrierSetRuntime::write_ref_field_pre_entry));\n+  __ j(*stub->continuation());\n+}\n@@ -227,1 +316,11 @@\n-  __ sb(zr, Address(card_addr));\n+void G1BarrierSetAssembler::g1_write_barrier_post_c2(MacroAssembler* masm,\n+                                                     Register store_addr,\n+                                                     Register new_val,\n+                                                     Register thread,\n+                                                     Register tmp1,\n+                                                     Register tmp2,\n+                                                     G1PostBarrierStubC2* stub) {\n+  assert(thread == xthread, \"must be\");\n+  assert_different_registers(store_addr, new_val, thread, tmp1, tmp2, t0);\n+  assert(store_addr != noreg && new_val != noreg && tmp1 != noreg && tmp2 != noreg,\n+         \"expecting a register\");\n@@ -229,4 +328,1 @@\n-  __ ld(t0, queue_index);\n-  __ beqz(t0, runtime);\n-  __ sub(t0, t0, wordSize);\n-  __ sd(t0, queue_index);\n+  stub->initialize_registers(thread, tmp1, tmp2);\n@@ -234,4 +330,5 @@\n-  __ ld(tmp2, buffer);\n-  __ add(t0, tmp2, t0);\n-  __ sd(card_addr, Address(t0, 0));\n-  __ j(done);\n+  bool new_val_may_be_null = (stub->barrier_data() & G1C2BarrierPostNotNull) == 0;\n+  generate_post_barrier_fast_path(masm, store_addr, new_val, tmp1, tmp2, *stub->continuation(), new_val_may_be_null);\n+  \/\/ If card is not young, jump to stub (slow path) (tmp2 holds the card value)\n+  __ mv(t0, (int)G1CardTable::g1_young_card_val());\n+  __ bne(tmp2, t0, *stub->entry(), true);\n@@ -239,6 +336,2 @@\n-  __ bind(runtime);\n-  \/\/ save the live input values\n-  RegSet saved = RegSet::of(store_addr);\n-  __ push_reg(saved, sp);\n-  __ call_VM_leaf(CAST_FROM_FN_PTR(address, G1BarrierSetRuntime::write_ref_field_post_entry), card_addr, thread);\n-  __ pop_reg(saved, sp);\n+  __ bind(*stub->continuation());\n+}\n@@ -246,1 +339,14 @@\n-  __ bind(done);\n+void G1BarrierSetAssembler::generate_c2_post_barrier_stub(MacroAssembler* masm,\n+                                                          G1PostBarrierStubC2* stub) const {\n+  Assembler::InlineSkippedInstructionsCounter skip_counter(masm);\n+  Label runtime;\n+  Register thread = stub->thread();\n+  Register tmp1 = stub->tmp1(); \/\/ tmp1 holds the card address.\n+  Register tmp2 = stub->tmp2();\n+\n+  __ bind(*stub->entry());\n+  generate_post_barrier_slow_path(masm, thread, tmp1, tmp2, *stub->continuation(), runtime);\n+\n+  __ bind(runtime);\n+  generate_c2_barrier_runtime_call(masm, stub, tmp1, CAST_FROM_FN_PTR(address, G1BarrierSetRuntime::write_ref_field_post_entry));\n+  __ j(*stub->continuation());\n@@ -249,0 +355,2 @@\n+#endif \/\/ COMPILER2\n+\n","filename":"src\/hotspot\/cpu\/riscv\/gc\/g1\/g1BarrierSetAssembler_riscv.cpp","additions":191,"deletions":83,"binary":false,"changes":274,"status":"modified"},{"patch":"@@ -3,1 +3,1 @@\n- * Copyright (c) 2020, 2021, Huawei Technologies Co., Ltd. All rights reserved.\n+ * Copyright (c) 2020, 2024, Huawei Technologies Co., Ltd. All rights reserved.\n@@ -39,0 +39,2 @@\n+class G1PreBarrierStubC2;\n+class G1PostBarrierStubC2;\n@@ -75,0 +77,21 @@\n+#ifdef COMPILER2\n+  void g1_write_barrier_pre_c2(MacroAssembler* masm,\n+                               Register obj,\n+                               Register pre_val,\n+                               Register thread,\n+                               Register tmp1,\n+                               Register tmp2,\n+                               G1PreBarrierStubC2* c2_stub);\n+  void generate_c2_pre_barrier_stub(MacroAssembler* masm,\n+                                    G1PreBarrierStubC2* stub) const;\n+  void g1_write_barrier_post_c2(MacroAssembler* masm,\n+                                Register store_addr,\n+                                Register new_val,\n+                                Register thread,\n+                                Register tmp1,\n+                                Register tmp2,\n+                                G1PostBarrierStubC2* c2_stub);\n+  void generate_c2_post_barrier_stub(MacroAssembler* masm,\n+                                     G1PostBarrierStubC2* stub) const;\n+#endif\n+\n","filename":"src\/hotspot\/cpu\/riscv\/gc\/g1\/g1BarrierSetAssembler_riscv.hpp","additions":24,"deletions":1,"binary":false,"changes":25,"status":"modified"},{"patch":"@@ -0,0 +1,564 @@\n+\/\/\n+\/\/ Copyright (c) 2024, Oracle and\/or its affiliates. All rights reserved.\n+\/\/ Copyright (c) 2024, Huawei Technologies Co., Ltd. All rights reserved.\n+\/\/ DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+\/\/\n+\/\/ This code is free software; you can redistribute it and\/or modify it\n+\/\/ under the terms of the GNU General Public License version 2 only, as\n+\/\/ published by the Free Software Foundation.\n+\/\/\n+\/\/ This code is distributed in the hope that it will be useful, but WITHOUT\n+\/\/ ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+\/\/ FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+\/\/ version 2 for more details (a copy is included in the LICENSE file that\n+\/\/ accompanied this code).\n+\/\/\n+\/\/ You should have received a copy of the GNU General Public License version\n+\/\/ 2 along with this work; if not, write to the Free Software Foundation,\n+\/\/ Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+\/\/\n+\/\/ Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+\/\/ or visit www.oracle.com if you need additional information or have any\n+\/\/ questions.\n+\/\/\n+\n+source_hpp %{\n+\n+#include \"gc\/g1\/c2\/g1BarrierSetC2.hpp\"\n+#include \"gc\/shared\/gc_globals.hpp\"\n+\n+%}\n+\n+source %{\n+\n+#include \"gc\/g1\/g1BarrierSetAssembler_riscv.hpp\"\n+#include \"gc\/g1\/g1BarrierSetRuntime.hpp\"\n+\n+static void write_barrier_pre(MacroAssembler* masm,\n+                              const MachNode* node,\n+                              Register obj,\n+                              Register pre_val,\n+                              Register tmp1,\n+                              Register tmp2,\n+                              RegSet preserve = RegSet(),\n+                              RegSet no_preserve = RegSet()) {\n+  if (!G1PreBarrierStubC2::needs_barrier(node)) {\n+    return;\n+  }\n+  Assembler::InlineSkippedInstructionsCounter skip_counter(masm);\n+  G1BarrierSetAssembler* g1_asm = static_cast<G1BarrierSetAssembler*>(BarrierSet::barrier_set()->barrier_set_assembler());\n+  G1PreBarrierStubC2* const stub = G1PreBarrierStubC2::create(node);\n+  for (RegSetIterator<Register> reg = preserve.begin(); *reg != noreg; ++reg) {\n+    stub->preserve(*reg);\n+  }\n+  for (RegSetIterator<Register> reg = no_preserve.begin(); *reg != noreg; ++reg) {\n+    stub->dont_preserve(*reg);\n+  }\n+  g1_asm->g1_write_barrier_pre_c2(masm, obj, pre_val, xthread, tmp1, tmp2, stub);\n+}\n+\n+static void write_barrier_post(MacroAssembler* masm,\n+                               const MachNode* node,\n+                               Register store_addr,\n+                               Register new_val,\n+                               Register tmp1,\n+                               Register tmp2) {\n+  if (!G1PostBarrierStubC2::needs_barrier(node)) {\n+    return;\n+  }\n+  Assembler::InlineSkippedInstructionsCounter skip_counter(masm);\n+  G1BarrierSetAssembler* g1_asm = static_cast<G1BarrierSetAssembler*>(BarrierSet::barrier_set()->barrier_set_assembler());\n+  G1PostBarrierStubC2* const stub = G1PostBarrierStubC2::create(node);\n+  g1_asm->g1_write_barrier_post_c2(masm, store_addr, new_val, xthread, tmp1, tmp2, stub);\n+}\n+\n+%}\n+\n+instruct g1StoreP(indirect mem, iRegP src, iRegPNoSp tmp1, iRegPNoSp tmp2, iRegPNoSp tmp3)\n+%{\n+  predicate(UseG1GC && n->as_Store()->barrier_data() != 0);\n+  match(Set mem (StoreP mem src));\n+  effect(TEMP tmp1, TEMP tmp2, TEMP tmp3);\n+  ins_cost(STORE_COST);\n+  format %{ \"sd  $src, $mem\\t# ptr\" %}\n+  ins_encode %{\n+    guarantee($mem$$disp == 0, \"impossible encoding\");\n+    write_barrier_pre(masm, this,\n+                      $mem$$Register  \/* obj *\/,\n+                      $tmp1$$Register \/* pre_val *\/,\n+                      $tmp2$$Register \/* tmp1 *\/,\n+                      $tmp3$$Register \/* tmp2 *\/,\n+                      RegSet::of($mem$$Register, $src$$Register) \/* preserve *\/);\n+    __ sd($src$$Register, Address($mem$$Register));\n+    write_barrier_post(masm, this,\n+                       $mem$$Register  \/* store_addr *\/,\n+                       $src$$Register  \/* new_val *\/,\n+                       $tmp2$$Register \/* tmp1 *\/,\n+                       $tmp3$$Register \/* tmp2 *\/);\n+  %}\n+  ins_pipe(istore_reg_mem);\n+%}\n+\n+instruct g1StoreN(indirect mem, iRegN src, iRegPNoSp tmp1, iRegPNoSp tmp2, iRegPNoSp tmp3)\n+%{\n+  predicate(UseG1GC && n->as_Store()->barrier_data() != 0);\n+  match(Set mem (StoreN mem src));\n+  effect(TEMP tmp1, TEMP tmp2, TEMP tmp3);\n+  ins_cost(STORE_COST);\n+  format %{ \"sw  $src, $mem\\t# compressed ptr\" %}\n+  ins_encode %{\n+    guarantee($mem$$disp == 0, \"impossible encoding\");\n+    write_barrier_pre(masm, this,\n+                      $mem$$Register  \/* obj *\/,\n+                      $tmp1$$Register \/* pre_val *\/,\n+                      $tmp2$$Register \/* tmp1 *\/,\n+                      $tmp3$$Register \/* tmp2 *\/,\n+                      RegSet::of($mem$$Register, $src$$Register) \/* preserve *\/);\n+    __ sw($src$$Register, Address($mem$$Register));\n+    if ((barrier_data() & G1C2BarrierPost) != 0) {\n+      if ((barrier_data() & G1C2BarrierPostNotNull) == 0) {\n+        __ decode_heap_oop($tmp1$$Register, $src$$Register);\n+      } else {\n+        __ decode_heap_oop_not_null($tmp1$$Register, $src$$Register);\n+      }\n+    }\n+    write_barrier_post(masm, this,\n+                       $mem$$Register  \/* store_addr *\/,\n+                       $tmp1$$Register \/* new_val *\/,\n+                       $tmp2$$Register \/* tmp1 *\/,\n+                       $tmp3$$Register \/* tmp2 *\/);\n+  %}\n+  ins_pipe(istore_reg_mem);\n+%}\n+\n+instruct g1EncodePAndStoreN(indirect mem, iRegP src, iRegPNoSp tmp1, iRegPNoSp tmp2, iRegPNoSp tmp3)\n+%{\n+  predicate(UseG1GC && n->as_Store()->barrier_data() != 0);\n+  match(Set mem (StoreN mem (EncodeP src)));\n+  effect(TEMP tmp1, TEMP tmp2, TEMP tmp3);\n+  ins_cost(STORE_COST);\n+  format %{ \"encode_heap_oop $tmp1, $src\\n\\t\"\n+            \"sw  $tmp1, $mem\\t# compressed ptr\" %}\n+  ins_encode %{\n+    guarantee($mem$$disp == 0, \"impossible encoding\");\n+    write_barrier_pre(masm, this,\n+                      $mem$$Register  \/* obj *\/,\n+                      $tmp1$$Register \/* pre_val *\/,\n+                      $tmp2$$Register \/* tmp1 *\/,\n+                      $tmp3$$Register \/* tmp2 *\/,\n+                      RegSet::of($mem$$Register, $src$$Register) \/* preserve *\/);\n+    if ((barrier_data() & G1C2BarrierPostNotNull) == 0) {\n+      __ encode_heap_oop($tmp1$$Register, $src$$Register);\n+    } else {\n+      __ encode_heap_oop_not_null($tmp1$$Register, $src$$Register);\n+    }\n+    __ sw($tmp1$$Register, Address($mem$$Register));\n+    write_barrier_post(masm, this,\n+                       $mem$$Register  \/* store_addr *\/,\n+                       $src$$Register  \/* new_val *\/,\n+                       $tmp2$$Register \/* tmp1 *\/,\n+                       $tmp3$$Register \/* tmp2 *\/);\n+  %}\n+  ins_pipe(istore_reg_mem);\n+%}\n+\n+instruct g1CompareAndExchangeP(iRegPNoSp res, indirect mem, iRegP oldval, iRegP newval, iRegPNoSp tmp1, iRegPNoSp tmp2)\n+%{\n+  predicate(UseG1GC && n->as_LoadStore()->barrier_data() != 0);\n+  match(Set res (CompareAndExchangeP mem (Binary oldval newval)));\n+  effect(TEMP res, TEMP tmp1, TEMP tmp2);\n+  ins_cost(2 * VOLATILE_REF_COST);\n+  format %{ \"cmpxchg $res = $mem, $oldval, $newval\\t# ptr\" %}\n+  ins_encode %{\n+    guarantee($mem$$disp == 0, \"impossible encoding\");\n+    assert_different_registers($oldval$$Register, $mem$$Register);\n+    assert_different_registers($newval$$Register, $mem$$Register);\n+    \/\/ Pass $oldval to the pre-barrier (instead of loading from $mem), because\n+    \/\/ $oldval is the only value that can be overwritten.\n+    \/\/ The same holds for g1CompareAndSwapP and its Acq variant.\n+    write_barrier_pre(masm, this,\n+                      noreg             \/* obj *\/,\n+                      $oldval$$Register \/* pre_val *\/,\n+                      $tmp1$$Register   \/* tmp1 *\/,\n+                      $tmp2$$Register   \/* tmp2 *\/,\n+                      RegSet::of($mem$$Register, $oldval$$Register, $newval$$Register) \/* preserve *\/,\n+                      RegSet::of($res$$Register) \/* no_preserve *\/);\n+    __ cmpxchg($mem$$Register, $oldval$$Register, $newval$$Register, Assembler::int64,\n+               \/*acquire*\/ Assembler::relaxed, \/*release*\/ Assembler::rl, $res$$Register);\n+    write_barrier_post(masm, this,\n+                       $mem$$Register    \/* store_addr *\/,\n+                       $newval$$Register \/* new_val *\/,\n+                       $tmp1$$Register   \/* tmp1 *\/,\n+                       $tmp2$$Register   \/* tmp2 *\/);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct g1CompareAndExchangePAcq(iRegPNoSp res, indirect mem, iRegP oldval, iRegP newval, iRegPNoSp tmp1, iRegPNoSp tmp2)\n+%{\n+  predicate(UseG1GC && needs_acquiring_load_reserved(n) && n->as_LoadStore()->barrier_data() != 0);\n+  match(Set res (CompareAndExchangeP mem (Binary oldval newval)));\n+  effect(TEMP res, TEMP tmp1, TEMP tmp2);\n+  ins_cost(VOLATILE_REF_COST);\n+  format %{ \"cmpxchg_acq $res = $mem, $oldval, $newval\\t# ptr\" %}\n+  ins_encode %{\n+    guarantee($mem$$disp == 0, \"impossible encoding\");\n+    assert_different_registers($oldval$$Register, $mem$$Register);\n+    assert_different_registers($newval$$Register, $mem$$Register);\n+    \/\/ Pass $oldval to the pre-barrier (instead of loading from $mem), because\n+    \/\/ $oldval is the only value that can be overwritten.\n+    \/\/ The same holds for g1CompareAndSwapP and its Acq variant.\n+    write_barrier_pre(masm, this,\n+                      noreg             \/* obj *\/,\n+                      $oldval$$Register \/* pre_val *\/,\n+                      $tmp1$$Register   \/* tmp1 *\/,\n+                      $tmp2$$Register   \/* tmp2 *\/,\n+                      RegSet::of($mem$$Register, $oldval$$Register, $newval$$Register) \/* preserve *\/,\n+                      RegSet::of($res$$Register) \/* no_preserve *\/);\n+    __ cmpxchg($mem$$Register, $oldval$$Register, $newval$$Register, Assembler::int64,\n+               \/*acquire*\/ Assembler::aq, \/*release*\/ Assembler::rl, $res$$Register);\n+    write_barrier_post(masm, this,\n+                       $mem$$Register    \/* store_addr *\/,\n+                       $newval$$Register \/* new_val *\/,\n+                       $tmp1$$Register   \/* tmp1 *\/,\n+                       $tmp2$$Register   \/* tmp2 *\/);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct g1CompareAndExchangeN(iRegNNoSp res, indirect mem, iRegN oldval, iRegN newval, iRegPNoSp tmp1, iRegPNoSp tmp2, iRegPNoSp tmp3)\n+%{\n+  predicate(UseG1GC && n->as_LoadStore()->barrier_data() != 0);\n+  match(Set res (CompareAndExchangeN mem (Binary oldval newval)));\n+  effect(TEMP res, TEMP tmp1, TEMP tmp2, TEMP tmp3);\n+  ins_cost(2 * VOLATILE_REF_COST);\n+  format %{ \"cmpxchg $res = $mem, $oldval, $newval\\t# narrow oop\" %}\n+  ins_encode %{\n+    guarantee($mem$$disp == 0, \"impossible encoding\");\n+    assert_different_registers($oldval$$Register, $mem$$Register);\n+    assert_different_registers($newval$$Register, $mem$$Register);\n+    write_barrier_pre(masm, this,\n+                      $mem$$Register  \/* obj *\/,\n+                      $tmp1$$Register \/* pre_val *\/,\n+                      $tmp2$$Register \/* tmp1 *\/,\n+                      $tmp3$$Register \/* tmp2 *\/,\n+                      RegSet::of($mem$$Register, $oldval$$Register, $newval$$Register) \/* preserve *\/,\n+                      RegSet::of($res$$Register) \/* no_preserve *\/);\n+    __ cmpxchg($mem$$Register, $oldval$$Register, $newval$$Register, Assembler::uint32,\n+               \/*acquire*\/ Assembler::relaxed, \/*release*\/ Assembler::rl, $res$$Register);\n+    __ decode_heap_oop($tmp1$$Register, $newval$$Register);\n+    write_barrier_post(masm, this,\n+                       $mem$$Register  \/* store_addr *\/,\n+                       $tmp1$$Register \/* new_val *\/,\n+                       $tmp2$$Register \/* tmp1 *\/,\n+                       $tmp3$$Register \/* tmp2 *\/);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct g1CompareAndExchangeNAcq(iRegNNoSp res, indirect mem, iRegN oldval, iRegN newval, iRegPNoSp tmp1, iRegPNoSp tmp2, iRegPNoSp tmp3)\n+%{\n+  predicate(UseG1GC && needs_acquiring_load_reserved(n) && n->as_LoadStore()->barrier_data() != 0);\n+  match(Set res (CompareAndExchangeN mem (Binary oldval newval)));\n+  effect(TEMP res, TEMP tmp1, TEMP tmp2, TEMP tmp3);\n+  ins_cost(VOLATILE_REF_COST);\n+  format %{ \"cmpxchg_acq $res = $mem, $oldval, $newval\\t# narrow oop\" %}\n+  ins_encode %{\n+    guarantee($mem$$disp == 0, \"impossible encoding\");\n+    assert_different_registers($oldval$$Register, $mem$$Register);\n+    assert_different_registers($newval$$Register, $mem$$Register);\n+    write_barrier_pre(masm, this,\n+                      $mem$$Register  \/* obj *\/,\n+                      $tmp1$$Register \/* pre_val *\/,\n+                      $tmp2$$Register \/* tmp1 *\/,\n+                      $tmp3$$Register \/* tmp2 *\/,\n+                      RegSet::of($mem$$Register, $oldval$$Register, $newval$$Register) \/* preserve *\/,\n+                      RegSet::of($res$$Register) \/* no_preserve *\/);\n+    __ cmpxchg($mem$$Register, $oldval$$Register, $newval$$Register, Assembler::uint32,\n+               \/*acquire*\/ Assembler::aq, \/*release*\/ Assembler::rl, $res$$Register);\n+    __ decode_heap_oop($tmp1$$Register, $newval$$Register);\n+    write_barrier_post(masm, this,\n+                       $mem$$Register  \/* store_addr *\/,\n+                       $tmp1$$Register \/* new_val *\/,\n+                       $tmp2$$Register \/* tmp1 *\/,\n+                       $tmp3$$Register \/* tmp2 *\/);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct g1CompareAndSwapP(iRegINoSp res, indirect mem, iRegP newval, iRegPNoSp tmp1, iRegPNoSp tmp2, iRegP oldval)\n+%{\n+  predicate(UseG1GC && n->as_LoadStore()->barrier_data() != 0);\n+  match(Set res (CompareAndSwapP mem (Binary oldval newval)));\n+  match(Set res (WeakCompareAndSwapP mem (Binary oldval newval)));\n+  effect(TEMP res, TEMP tmp1, TEMP tmp2);\n+  ins_cost(2 * VOLATILE_REF_COST);\n+  format %{ \"cmpxchg $mem, $oldval, $newval\\t# (ptr)\\n\\t\"\n+            \"mv $res, $res == $oldval\" %}\n+  ins_encode %{\n+    guarantee($mem$$disp == 0, \"impossible encoding\");\n+    assert_different_registers($oldval$$Register, $mem$$Register);\n+    assert_different_registers($newval$$Register, $mem$$Register);\n+    write_barrier_pre(masm, this,\n+                      noreg             \/* obj *\/,\n+                      $oldval$$Register \/* pre_val *\/,\n+                      $tmp1$$Register   \/* tmp1 *\/,\n+                      $tmp2$$Register   \/* tmp2 *\/,\n+                      RegSet::of($mem$$Register, $oldval$$Register, $newval$$Register) \/* preserve *\/,\n+                      RegSet::of($res$$Register) \/* no_preserve *\/);\n+    __ cmpxchg($mem$$Register, $oldval$$Register, $newval$$Register, Assembler::int64,\n+               \/*acquire*\/ Assembler::relaxed, \/*release*\/ Assembler::rl, $res$$Register,\n+               \/*result as bool*\/ true);\n+    write_barrier_post(masm, this,\n+                       $mem$$Register    \/* store_addr *\/,\n+                       $newval$$Register \/* new_val *\/,\n+                       $tmp1$$Register   \/* tmp1 *\/,\n+                       $tmp2$$Register   \/* tmp2 *\/);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct g1CompareAndSwapPAcq(iRegINoSp res, indirect mem, iRegP newval, iRegPNoSp tmp1, iRegPNoSp tmp2, iRegP oldval)\n+%{\n+  predicate(UseG1GC && needs_acquiring_load_reserved(n) && n->as_LoadStore()->barrier_data() != 0);\n+  match(Set res (CompareAndSwapP mem (Binary oldval newval)));\n+  match(Set res (WeakCompareAndSwapP mem (Binary oldval newval)));\n+  effect(TEMP res, TEMP tmp1, TEMP tmp2);\n+  ins_cost(VOLATILE_REF_COST);\n+  format %{ \"cmpxchg_acq $mem, $oldval, $newval\\t# (ptr)\\n\\t\"\n+            \"mv $res, $res == $oldval\" %}\n+  ins_encode %{\n+    guarantee($mem$$disp == 0, \"impossible encoding\");\n+    assert_different_registers($oldval$$Register, $mem$$Register);\n+    assert_different_registers($newval$$Register, $mem$$Register);\n+    write_barrier_pre(masm, this,\n+                      noreg             \/* obj *\/,\n+                      $oldval$$Register \/* pre_val *\/,\n+                      $tmp1$$Register   \/* tmp1 *\/,\n+                      $tmp2$$Register   \/* tmp2 *\/,\n+                      RegSet::of($mem$$Register, $oldval$$Register, $newval$$Register) \/* preserve *\/,\n+                      RegSet::of($res$$Register) \/* no_preserve *\/);\n+    __ cmpxchg($mem$$Register, $oldval$$Register, $newval$$Register, Assembler::int64,\n+               \/*acquire*\/ Assembler::aq, \/*release*\/ Assembler::rl, $res$$Register,\n+               \/*result as bool*\/ true);\n+    write_barrier_post(masm, this,\n+                       $mem$$Register    \/* store_addr *\/,\n+                       $newval$$Register \/* new_val *\/,\n+                       $tmp1$$Register   \/* tmp1 *\/,\n+                       $tmp2$$Register   \/* tmp2 *\/);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct g1CompareAndSwapN(iRegINoSp res, indirect mem, iRegN newval, iRegPNoSp tmp1, iRegPNoSp tmp2, iRegPNoSp tmp3, iRegN oldval)\n+%{\n+  predicate(UseG1GC && n->as_LoadStore()->barrier_data() != 0);\n+  match(Set res (CompareAndSwapN mem (Binary oldval newval)));\n+  match(Set res (WeakCompareAndSwapN mem (Binary oldval newval)));\n+  effect(TEMP res, TEMP tmp1, TEMP tmp2, TEMP tmp3);\n+  ins_cost(2 * VOLATILE_REF_COST);\n+  format %{ \"cmpxchg $mem, $oldval, $newval\\t# (narrow oop)\\n\\t\"\n+            \"mv $res, $res == $oldval\" %}\n+  ins_encode %{\n+    guarantee($mem$$disp == 0, \"impossible encoding\");\n+    assert_different_registers($oldval$$Register, $mem$$Register);\n+    assert_different_registers($newval$$Register, $mem$$Register);\n+    write_barrier_pre(masm, this,\n+                      $mem$$Register  \/* obj *\/,\n+                      $tmp1$$Register \/* pre_val *\/,\n+                      $tmp2$$Register \/* tmp1 *\/,\n+                      $tmp3$$Register \/* tmp2 *\/,\n+                      RegSet::of($mem$$Register, $oldval$$Register, $newval$$Register) \/* preserve *\/,\n+                      RegSet::of($res$$Register) \/* no_preserve *\/);\n+    __ cmpxchg($mem$$Register, $oldval$$Register, $newval$$Register, Assembler::uint32,\n+               \/*acquire*\/ Assembler::relaxed, \/*release*\/ Assembler::rl, $res$$Register,\n+               \/*result as bool*\/ true);\n+    __ decode_heap_oop($tmp1$$Register, $newval$$Register);\n+    write_barrier_post(masm, this,\n+                       $mem$$Register  \/* store_addr *\/,\n+                       $tmp1$$Register \/* new_val *\/,\n+                       $tmp2$$Register \/* tmp1 *\/,\n+                       $tmp3$$Register \/* tmp2 *\/);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct g1CompareAndSwapNAcq(iRegINoSp res, indirect mem, iRegN newval, iRegPNoSp tmp1, iRegPNoSp tmp2, iRegPNoSp tmp3, iRegN oldval)\n+%{\n+  predicate(UseG1GC && needs_acquiring_load_reserved(n) && n->as_LoadStore()->barrier_data() != 0);\n+  match(Set res (CompareAndSwapN mem (Binary oldval newval)));\n+  match(Set res (WeakCompareAndSwapN mem (Binary oldval newval)));\n+  effect(TEMP res, TEMP tmp1, TEMP tmp2, TEMP tmp3);\n+  ins_cost(VOLATILE_REF_COST);\n+  format %{ \"cmpxchg_acq $mem, $oldval, $newval\\t# (narrow oop)\\n\\t\"\n+            \"mv $res, $res == $oldval\" %}\n+  ins_encode %{\n+    guarantee($mem$$disp == 0, \"impossible encoding\");\n+    assert_different_registers($oldval$$Register, $mem$$Register);\n+    assert_different_registers($newval$$Register, $mem$$Register);\n+    write_barrier_pre(masm, this,\n+                      $mem$$Register  \/* obj *\/,\n+                      $tmp1$$Register \/* pre_val *\/,\n+                      $tmp2$$Register \/* tmp1 *\/,\n+                      $tmp3$$Register \/* tmp2 *\/,\n+                      RegSet::of($mem$$Register, $oldval$$Register, $newval$$Register) \/* preserve *\/,\n+                      RegSet::of($res$$Register) \/* no_preserve *\/);\n+    __ cmpxchg($mem$$Register, $oldval$$Register, $newval$$Register, Assembler::uint32,\n+              \/*acquire*\/ Assembler::aq, \/*release*\/ Assembler::rl, $res$$Register,\n+              \/*result as bool*\/ true);\n+    __ decode_heap_oop($tmp1$$Register, $newval$$Register);\n+    write_barrier_post(masm, this,\n+                       $mem$$Register  \/* store_addr *\/,\n+                       $tmp1$$Register \/* new_val *\/,\n+                       $tmp2$$Register \/* tmp1 *\/,\n+                       $tmp3$$Register \/* tmp2 *\/);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct g1GetAndSetP(indirect mem, iRegP newval, iRegPNoSp tmp1, iRegPNoSp tmp2, iRegPNoSp preval)\n+%{\n+  predicate(UseG1GC && n->as_LoadStore()->barrier_data() != 0);\n+  match(Set preval (GetAndSetP mem newval));\n+  effect(TEMP preval, TEMP tmp1, TEMP tmp2);\n+  ins_cost(2 * VOLATILE_REF_COST);\n+  format %{ \"atomic_xchg  $preval, $newval, [$mem]\" %}\n+  ins_encode %{\n+    guarantee($mem$$disp == 0, \"impossible encoding\");\n+    assert_different_registers($mem$$Register, $newval$$Register);\n+    write_barrier_pre(masm, this,\n+                      $mem$$Register    \/* obj *\/,\n+                      $preval$$Register \/* pre_val (as a temporary register) *\/,\n+                      $tmp1$$Register   \/* tmp1 *\/,\n+                      $tmp2$$Register   \/* tmp2 *\/,\n+                      RegSet::of($mem$$Register, $preval$$Register, $newval$$Register) \/* preserve *\/);\n+    __ atomic_xchg($preval$$Register, $newval$$Register, $mem$$Register);\n+    write_barrier_post(masm, this,\n+                       $mem$$Register    \/* store_addr *\/,\n+                       $newval$$Register \/* new_val *\/,\n+                       $tmp1$$Register   \/* tmp1 *\/,\n+                       $tmp2$$Register   \/* tmp2 *\/);\n+  %}\n+  ins_pipe(pipe_serial);\n+%}\n+\n+instruct g1GetAndSetPAcq(indirect mem, iRegP newval, iRegPNoSp tmp1, iRegPNoSp tmp2, iRegPNoSp preval)\n+%{\n+  predicate(UseG1GC && needs_acquiring_load_reserved(n) && n->as_LoadStore()->barrier_data() != 0);\n+  match(Set preval (GetAndSetP mem newval));\n+  effect(TEMP preval, TEMP tmp1, TEMP tmp2);\n+  ins_cost(VOLATILE_REF_COST);\n+  format %{ \"atomic_xchg_acq  $preval, $newval, [$mem]\" %}\n+  ins_encode %{\n+    guarantee($mem$$disp == 0, \"impossible encoding\");\n+    assert_different_registers($mem$$Register, $newval$$Register);\n+    write_barrier_pre(masm, this,\n+                      $mem$$Register    \/* obj *\/,\n+                      $preval$$Register \/* pre_val (as a temporary register) *\/,\n+                      $tmp1$$Register   \/* tmp1 *\/,\n+                      $tmp2$$Register   \/* tmp2 *\/,\n+                      RegSet::of($mem$$Register, $preval$$Register, $newval$$Register) \/* preserve *\/);\n+    __ atomic_xchgal($preval$$Register, $newval$$Register, $mem$$Register);\n+    write_barrier_post(masm, this,\n+                       $mem$$Register    \/* store_addr *\/,\n+                       $newval$$Register \/* new_val *\/,\n+                       $tmp1$$Register   \/* tmp1 *\/,\n+                       $tmp2$$Register   \/* tmp2 *\/);\n+  %}\n+  ins_pipe(pipe_serial);\n+%}\n+\n+instruct g1GetAndSetN(indirect mem, iRegN newval, iRegPNoSp tmp1, iRegPNoSp tmp2, iRegPNoSp tmp3, iRegNNoSp preval)\n+%{\n+  predicate(UseG1GC && n->as_LoadStore()->barrier_data() != 0);\n+  match(Set preval (GetAndSetN mem newval));\n+  effect(TEMP preval, TEMP tmp1, TEMP tmp2, TEMP tmp3);\n+  ins_cost(2 * VOLATILE_REF_COST);\n+  format %{ \"atomic_xchgwu $preval, $newval, [$mem]\" %}\n+  ins_encode %{\n+    guarantee($mem$$disp == 0, \"impossible encoding\");\n+    assert_different_registers($mem$$Register, $newval$$Register);\n+    write_barrier_pre(masm, this,\n+                      $mem$$Register  \/* obj *\/,\n+                      $tmp1$$Register \/* pre_val *\/,\n+                      $tmp2$$Register \/* tmp1 *\/,\n+                      $tmp3$$Register \/* tmp2 *\/,\n+                      RegSet::of($mem$$Register, $preval$$Register, $newval$$Register) \/* preserve *\/);\n+    __ atomic_xchgwu($preval$$Register, $newval$$Register, $mem$$Register);\n+    __ decode_heap_oop($tmp1$$Register, $newval$$Register);\n+    write_barrier_post(masm, this,\n+                       $mem$$Register  \/* store_addr *\/,\n+                       $tmp1$$Register \/* new_val *\/,\n+                       $tmp2$$Register \/* tmp1 *\/,\n+                       $tmp3$$Register \/* tmp2 *\/);\n+  %}\n+  ins_pipe(pipe_serial);\n+%}\n+\n+instruct g1GetAndSetNAcq(indirect mem, iRegN newval, iRegPNoSp tmp1, iRegPNoSp tmp2, iRegPNoSp tmp3, iRegNNoSp preval)\n+%{\n+  predicate(UseG1GC && needs_acquiring_load_reserved(n) && n->as_LoadStore()->barrier_data() != 0);\n+  match(Set preval (GetAndSetN mem newval));\n+  effect(TEMP preval, TEMP tmp1, TEMP tmp2, TEMP tmp3);\n+  ins_cost(VOLATILE_REF_COST);\n+  format %{ \"atomic_xchgwu_acq $preval, $newval, [$mem]\" %}\n+  ins_encode %{\n+    guarantee($mem$$disp == 0, \"impossible encoding\");\n+    assert_different_registers($mem$$Register, $newval$$Register);\n+    write_barrier_pre(masm, this,\n+                      $mem$$Register  \/* obj *\/,\n+                      $tmp1$$Register \/* pre_val *\/,\n+                      $tmp2$$Register \/* tmp1 *\/,\n+                      $tmp3$$Register \/* tmp2 *\/,\n+                      RegSet::of($mem$$Register, $preval$$Register, $newval$$Register) \/* preserve *\/);\n+    __ atomic_xchgalwu($preval$$Register, $newval$$Register, $mem$$Register);\n+    __ decode_heap_oop($tmp1$$Register, $newval$$Register);\n+    write_barrier_post(masm, this,\n+                       $mem$$Register  \/* store_addr *\/,\n+                       $tmp1$$Register \/* new_val *\/,\n+                       $tmp2$$Register \/* tmp1 *\/,\n+                       $tmp3$$Register \/* tmp2 *\/);\n+  %}\n+  ins_pipe(pipe_serial);\n+%}\n+\n+instruct g1LoadP(iRegPNoSp dst, indirect mem, iRegPNoSp tmp1, iRegPNoSp tmp2)\n+%{\n+  predicate(UseG1GC && n->as_Load()->barrier_data() != 0);\n+  match(Set dst (LoadP mem));\n+  effect(TEMP dst, TEMP tmp1, TEMP tmp2);\n+  ins_cost(LOAD_COST + BRANCH_COST);\n+  format %{ \"ld  $dst, $mem\\t# ptr\" %}\n+  ins_encode %{\n+    guarantee($mem$$disp == 0, \"impossible encoding\");\n+    __ ld($dst$$Register, Address($mem$$Register));\n+    write_barrier_pre(masm, this,\n+                      noreg \/* obj *\/,\n+                      $dst$$Register \/* pre_val *\/,\n+                      $tmp1$$Register \/* tmp1 *\/,\n+                      $tmp2$$Register \/* tmp2 *\/);\n+  %}\n+  ins_pipe(iload_reg_mem);\n+%}\n+\n+instruct g1LoadN(iRegNNoSp dst, indirect mem, iRegPNoSp tmp1, iRegPNoSp tmp2, iRegPNoSp tmp3)\n+%{\n+  predicate(UseG1GC && n->as_Load()->barrier_data() != 0);\n+  match(Set dst (LoadN mem));\n+  effect(TEMP dst, TEMP tmp1, TEMP tmp2, TEMP tmp3);\n+  ins_cost(LOAD_COST + BRANCH_COST);\n+  format %{ \"lwu  $dst, $mem\\t# compressed ptr\" %}\n+  ins_encode %{\n+    guarantee($mem$$disp == 0, \"impossible encoding\");\n+    __ lwu($dst$$Register, Address($mem$$Register));\n+    if ((barrier_data() & G1C2BarrierPre) != 0) {\n+      __ decode_heap_oop($tmp1$$Register, $dst$$Register);\n+      write_barrier_pre(masm, this,\n+                        noreg \/* obj *\/,\n+                        $tmp1$$Register \/* pre_val *\/,\n+                        $tmp2$$Register \/* tmp1 *\/,\n+                        $tmp3$$Register \/* tmp2 *\/);\n+    }\n+  %}\n+  ins_pipe(iload_reg_mem);\n+%}\n","filename":"src\/hotspot\/cpu\/riscv\/gc\/g1\/g1_riscv.ad","additions":564,"deletions":0,"binary":false,"changes":564,"status":"added"},{"patch":"@@ -2227,1 +2227,2 @@\n-      is_vector_scalar_bitwise_pattern(n, m)) {\n+      is_vector_scalar_bitwise_pattern(n, m) ||\n+      is_encode_and_store_pattern(n, m)) {\n@@ -4788,0 +4789,1 @@\n+  predicate(n->as_Load()->barrier_data() == 0);\n@@ -5223,0 +5225,1 @@\n+  predicate(n->as_Store()->barrier_data() == 0);\n@@ -5237,0 +5240,1 @@\n+  predicate(n->as_Store()->barrier_data() == 0);\n@@ -5427,0 +5431,1 @@\n+  predicate(n->as_LoadStore()->barrier_data() == 0);\n@@ -5548,1 +5553,1 @@\n-  predicate(needs_acquiring_load_reserved(n));\n+  predicate(needs_acquiring_load_reserved(n) && n->as_LoadStore()->barrier_data() == 0);\n@@ -5656,0 +5661,1 @@\n+  predicate(n->as_LoadStore()->barrier_data() == 0);\n@@ -5789,1 +5795,1 @@\n-  predicate(needs_acquiring_load_reserved(n));\n+  predicate(needs_acquiring_load_reserved(n) && n->as_LoadStore()->barrier_data() == 0);\n@@ -5917,0 +5923,1 @@\n+  predicate(n->as_LoadStore()->barrier_data() == 0);\n@@ -6048,1 +6055,1 @@\n-  predicate(needs_acquiring_load_reserved(n));\n+  predicate(needs_acquiring_load_reserved(n) && n->as_LoadStore()->barrier_data() == 0);\n@@ -6120,0 +6127,2 @@\n+  predicate(n->as_LoadStore()->barrier_data() == 0);\n+\n@@ -6185,1 +6194,1 @@\n-  predicate(needs_acquiring_load_reserved(n));\n+  predicate(needs_acquiring_load_reserved(n) && n->as_LoadStore()->barrier_data() == 0);\n","filename":"src\/hotspot\/cpu\/riscv\/riscv.ad","additions":14,"deletions":5,"binary":false,"changes":19,"status":"modified"},{"patch":"@@ -2,2 +2,2 @@\n- * Copyright (c) 2019, 2023, Oracle and\/or its affiliates. All rights reserved.\n- * Copyright (c) 2018, 2023 SAP SE. All rights reserved.\n+ * Copyright (c) 2019, 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2018, 2024 SAP SE. All rights reserved.\n@@ -45,1 +45,4 @@\n-#endif\n+#endif \/\/ COMPILER1\n+#ifdef COMPILER2\n+#include \"gc\/g1\/c2\/g1BarrierSetC2.hpp\"\n+#endif \/\/ COMPILER2\n@@ -49,1 +52,34 @@\n-#define BLOCK_COMMENT(str) if (PrintAssembly) __ block_comment(str)\n+#define BLOCK_COMMENT(str) __ block_comment(str)\n+\n+static void generate_pre_barrier_fast_path(MacroAssembler* masm,\n+                                           const Register thread,\n+                                           const Register tmp1) {\n+  Address in_progress(thread, in_bytes(G1ThreadLocalData::satb_mark_queue_active_offset()));\n+  \/\/ Is marking active?\n+  if (in_bytes(SATBMarkQueue::byte_width_of_active()) == 4) {\n+    __ load_and_test_int(tmp1, in_progress);\n+  } else {\n+    assert(in_bytes(SATBMarkQueue::byte_width_of_active()) == 1, \"Assumption\");\n+    __ load_and_test_byte(tmp1, in_progress);\n+  }\n+}\n+\n+static void generate_queue_test_and_insertion(MacroAssembler* masm, ByteSize index_offset, ByteSize buffer_offset, Label& runtime,\n+                                              const Register Z_thread, const Register value, const Register temp) {\n+  BLOCK_COMMENT(\"generate_queue_test_and_insertion {\");\n+\n+  assert_different_registers(temp, value);\n+  \/\/ Can we store a value in the given thread's buffer?\n+  \/\/ (The index field is typed as size_t.)\n+\n+  __ load_and_test_long(temp, Address(Z_thread, in_bytes(index_offset))); \/\/ temp := *(index address)\n+  __ branch_optimized(Assembler::bcondEqual, runtime);                    \/\/ jump to runtime if index == 0 (full buffer)\n+\n+  \/\/ The buffer is not full, store value into it.\n+  __ add2reg(temp, -wordSize);                                            \/\/ temp := next index\n+  __ z_stg(temp, in_bytes(index_offset), Z_thread);                       \/\/ *(index address) := next index\n+\n+  __ z_ag(temp, Address(Z_thread, in_bytes(buffer_offset)));              \/\/ temp := buffer address + next index\n+  __ z_stg(value, 0, temp);                                               \/\/ *(buffer address + next index) := value\n+  BLOCK_COMMENT(\"} generate_queue_test_and_insertion\");\n+}\n@@ -62,7 +98,2 @@\n-    const int active_offset = in_bytes(G1ThreadLocalData::satb_mark_queue_active_offset());\n-    if (in_bytes(SATBMarkQueue::byte_width_of_active()) == 4) {\n-      __ load_and_test_int(Rtmp1, Address(Z_thread, active_offset));\n-    } else {\n-      guarantee(in_bytes(SATBMarkQueue::byte_width_of_active()) == 1, \"Assumption\");\n-      __ load_and_test_byte(Rtmp1, Address(Z_thread, active_offset));\n-    }\n+\n+    generate_pre_barrier_fast_path(masm, Z_thread, Rtmp1);\n@@ -103,0 +134,175 @@\n+#if defined(COMPILER2)\n+\n+#undef __\n+#define __ masm->\n+\n+static void generate_c2_barrier_runtime_call(MacroAssembler* masm, G1BarrierStubC2* stub, const Register pre_val, const address runtime_path) {\n+  BLOCK_COMMENT(\"generate_c2_barrier_runtime_call {\");\n+  SaveLiveRegisters save_registers(masm, stub);\n+  __ call_VM_leaf(runtime_path, pre_val, Z_thread);\n+  BLOCK_COMMENT(\"} generate_c2_barrier_runtime_call\");\n+}\n+\n+void G1BarrierSetAssembler::g1_write_barrier_pre_c2(MacroAssembler* masm,\n+                                                    Register obj,\n+                                                    Register pre_val,\n+                                                    Register thread,\n+                                                    Register tmp1,\n+                                                    G1PreBarrierStubC2* stub) {\n+\n+  BLOCK_COMMENT(\"g1_write_barrier_pre_c2 {\");\n+\n+  assert(thread == Z_thread, \"must be\");\n+  assert_different_registers(obj, pre_val, tmp1);\n+  assert(pre_val != noreg && tmp1 != noreg, \"expecting a register\");\n+\n+  stub->initialize_registers(obj, pre_val, thread, tmp1, noreg);\n+\n+  generate_pre_barrier_fast_path(masm, thread, tmp1);\n+  __ branch_optimized(Assembler::bcondNotEqual, *stub->entry()); \/\/ Activity indicator is zero, so there is no marking going on currently.\n+\n+  __ bind(*stub->continuation());\n+\n+  BLOCK_COMMENT(\"} g1_write_barrier_pre_c2\");\n+}\n+\n+void G1BarrierSetAssembler::generate_c2_pre_barrier_stub(MacroAssembler* masm,\n+                                                         G1PreBarrierStubC2* stub) const {\n+\n+  BLOCK_COMMENT(\"generate_c2_pre_barrier_stub {\");\n+\n+  Assembler::InlineSkippedInstructionsCounter skip_counter(masm);\n+\n+  Label runtime;\n+  Register obj     = stub->obj();\n+  Register pre_val = stub->pre_val();\n+  Register thread  = stub->thread();\n+  Register tmp1    = stub->tmp1();\n+\n+  __ bind(*stub->entry());\n+\n+  BLOCK_COMMENT(\"generate_pre_val_not_null_test {\");\n+  if (obj != noreg) {\n+    __ load_heap_oop(pre_val, Address(obj), noreg, noreg, AS_RAW);\n+  }\n+  __ z_ltgr(pre_val, pre_val);\n+  __ branch_optimized(Assembler::bcondEqual, *stub->continuation());\n+  BLOCK_COMMENT(\"} generate_pre_val_not_null_test\");\n+\n+  generate_queue_test_and_insertion(masm,\n+                                    G1ThreadLocalData::satb_mark_queue_index_offset(),\n+                                    G1ThreadLocalData::satb_mark_queue_buffer_offset(),\n+                                    runtime,\n+                                    Z_thread, pre_val, tmp1);\n+\n+  __ branch_optimized(Assembler::bcondAlways, *stub->continuation());\n+\n+  __ bind(runtime);\n+\n+  generate_c2_barrier_runtime_call(masm, stub, pre_val, CAST_FROM_FN_PTR(address, G1BarrierSetRuntime::write_ref_field_pre_entry));\n+\n+  __ branch_optimized(Assembler::bcondAlways, *stub->continuation());\n+\n+  BLOCK_COMMENT(\"} generate_c2_pre_barrier_stub\");\n+}\n+\n+void G1BarrierSetAssembler::g1_write_barrier_post_c2(MacroAssembler* masm,\n+                                                     Register store_addr,\n+                                                     Register new_val,\n+                                                     Register thread,\n+                                                     Register tmp1,\n+                                                     Register tmp2,\n+                                                     G1PostBarrierStubC2* stub) {\n+  BLOCK_COMMENT(\"g1_write_barrier_post_c2 {\");\n+\n+  assert(thread == Z_thread, \"must be\");\n+  assert_different_registers(store_addr, new_val, thread, tmp1, tmp2, Z_R1_scratch);\n+\n+  assert(store_addr != noreg && new_val != noreg && tmp1 != noreg && tmp2 != noreg, \"expecting a register\");\n+\n+  stub->initialize_registers(thread, tmp1, tmp2);\n+\n+  BLOCK_COMMENT(\"generate_region_crossing_test {\");\n+  if (VM_Version::has_DistinctOpnds()) {\n+    __ z_xgrk(tmp1, store_addr, new_val);\n+  } else {\n+    __ z_lgr(tmp1, store_addr);\n+    __ z_xgr(tmp1, new_val);\n+  }\n+  __ z_srag(tmp1, tmp1, G1HeapRegion::LogOfHRGrainBytes);\n+  __ branch_optimized(Assembler::bcondEqual, *stub->continuation());\n+  BLOCK_COMMENT(\"} generate_region_crossing_test\");\n+\n+  \/\/ crosses regions, storing null?\n+  if ((stub->barrier_data() & G1C2BarrierPostNotNull) == 0) {\n+    __ z_ltgr(new_val, new_val);\n+    __ branch_optimized(Assembler::bcondEqual, *stub->continuation());\n+  }\n+\n+  BLOCK_COMMENT(\"generate_card_young_test {\");\n+  CardTableBarrierSet* ct = barrier_set_cast<CardTableBarrierSet>(BarrierSet::barrier_set());\n+  \/\/ calculate address of card\n+  __ load_const_optimized(tmp2, (address)ct->card_table()->byte_map_base());      \/\/ Card table base.\n+  __ z_srlg(tmp1, store_addr, CardTable::card_shift());         \/\/ Index into card table.\n+  __ z_algr(tmp1, tmp2);                                      \/\/ Explicit calculation needed for cli.\n+\n+  \/\/ Filter young.\n+  __ z_cli(0, tmp1, G1CardTable::g1_young_card_val());\n+\n+  BLOCK_COMMENT(\"} generate_card_young_test\");\n+\n+  \/\/ From here on, tmp1 holds the card address.\n+  __ branch_optimized(Assembler::bcondNotEqual, *stub->entry());\n+\n+  __ bind(*stub->continuation());\n+\n+  BLOCK_COMMENT(\"} g1_write_barrier_post_c2\");\n+}\n+\n+void G1BarrierSetAssembler::generate_c2_post_barrier_stub(MacroAssembler* masm,\n+                                                          G1PostBarrierStubC2* stub) const {\n+\n+  BLOCK_COMMENT(\"generate_c2_post_barrier_stub {\");\n+\n+  Assembler::InlineSkippedInstructionsCounter skip_counter(masm);\n+  Label runtime;\n+\n+  Register thread     = stub->thread();\n+  Register tmp1       = stub->tmp1(); \/\/ tmp1 holds the card address.\n+  Register tmp2       = stub->tmp2();\n+  Register Rcard_addr = tmp1;\n+\n+  __ bind(*stub->entry());\n+\n+  BLOCK_COMMENT(\"generate_card_clean_test {\");\n+  __ z_sync(); \/\/ Required to support concurrent cleaning.\n+  __ z_cli(0, Rcard_addr, 0); \/\/ Reload after membar.\n+  __ branch_optimized(Assembler::bcondEqual, *stub->continuation());\n+  BLOCK_COMMENT(\"} generate_card_clean_test\");\n+\n+  BLOCK_COMMENT(\"generate_dirty_card {\");\n+  \/\/ Storing a region crossing, non-null oop, card is clean.\n+  \/\/ Dirty card and log.\n+  STATIC_ASSERT(CardTable::dirty_card_val() == 0);\n+  __ z_mvi(0, Rcard_addr, CardTable::dirty_card_val());\n+  BLOCK_COMMENT(\"} generate_dirty_card\");\n+\n+  generate_queue_test_and_insertion(masm,\n+                                    G1ThreadLocalData::dirty_card_queue_index_offset(),\n+                                    G1ThreadLocalData::dirty_card_queue_buffer_offset(),\n+                                    runtime,\n+                                    Z_thread, tmp1, tmp2);\n+\n+  __ branch_optimized(Assembler::bcondAlways, *stub->continuation());\n+\n+  __ bind(runtime);\n+\n+  generate_c2_barrier_runtime_call(masm, stub, tmp1, CAST_FROM_FN_PTR(address, G1BarrierSetRuntime::write_ref_field_post_entry));\n+\n+  __ branch_optimized(Assembler::bcondAlways, *stub->continuation());\n+\n+  BLOCK_COMMENT(\"} generate_c2_post_barrier_stub\");\n+}\n+\n+#endif \/\/COMPILER2\n+\n@@ -139,3 +345,0 @@\n-  const int active_offset = in_bytes(G1ThreadLocalData::satb_mark_queue_active_offset());\n-  const int buffer_offset = in_bytes(G1ThreadLocalData::satb_mark_queue_buffer_offset());\n-  const int index_offset  = in_bytes(G1ThreadLocalData::satb_mark_queue_index_offset());\n@@ -150,8 +353,1 @@\n-  \/\/ Is marking active?\n-  \/\/ Note: value is loaded for test purposes only. No further use here.\n-  if (in_bytes(SATBMarkQueue::byte_width_of_active()) == 4) {\n-    __ load_and_test_int(Rtmp1, Address(Z_thread, active_offset));\n-  } else {\n-    guarantee(in_bytes(SATBMarkQueue::byte_width_of_active()) == 1, \"Assumption\");\n-    __ load_and_test_byte(Rtmp1, Address(Z_thread, active_offset));\n-  }\n+  generate_pre_barrier_fast_path(masm, Z_thread, Rtmp1);\n@@ -197,7 +393,0 @@\n-  Register Rbuffer = Rtmp1, Rindex = Rtmp2;\n-  assert_different_registers(Rbuffer, Rindex, Rpre_val);\n-\n-  __ z_lg(Rbuffer, buffer_offset, Z_thread);\n-\n-  __ load_and_test_long(Rindex, Address(Z_thread, index_offset));\n-  __ z_bre(callRuntime); \/\/ If index == 0, goto runtime.\n@@ -205,5 +394,5 @@\n-  __ add2reg(Rindex, -wordSize); \/\/ Decrement index.\n-  __ z_stg(Rindex, index_offset, Z_thread);\n-\n-  \/\/ Record the previous value.\n-  __ z_stg(Rpre_val, 0, Rbuffer, Rindex);\n+  generate_queue_test_and_insertion(masm,\n+                                    G1ThreadLocalData::satb_mark_queue_index_offset(),\n+                                    G1ThreadLocalData::satb_mark_queue_buffer_offset(),\n+                                    callRuntime,\n+                                    Z_thread, Rpre_val, Rtmp2);\n@@ -212,3 +401,0 @@\n-  Rbuffer = noreg;  \/\/ end of life\n-  Rindex  = noreg;  \/\/ end of life\n-\n@@ -329,4 +515,1 @@\n-  Register Rqueue_buf   = (Rtmp3 != Z_R0_scratch) ? Rtmp3 : Rtmp1;\n-  const int qidx_off    = in_bytes(G1ThreadLocalData::dirty_card_queue_index_offset());\n-  const int qbuf_off    = in_bytes(G1ThreadLocalData::dirty_card_queue_buffer_offset());\n-  if ((Rcard_addr == Rqueue_buf) || (Rcard_addr == Rqueue_index)) {\n+  if (Rcard_addr == Rqueue_index) {\n@@ -337,9 +520,5 @@\n-  __ load_and_test_long(Rqueue_index, Address(Z_thread, qidx_off));\n-  __ z_bre(callRuntime); \/\/ Index == 0 then jump to runtime.\n-\n-  __ z_lg(Rqueue_buf, qbuf_off, Z_thread);\n-\n-  __ add2reg(Rqueue_index, -wordSize); \/\/ Decrement index.\n-  __ z_stg(Rqueue_index, qidx_off, Z_thread);\n-\n-  __ z_stg(Rcard_addr_x, 0, Rqueue_index, Rqueue_buf); \/\/ Store card.\n+  generate_queue_test_and_insertion(masm,\n+                                    G1ThreadLocalData::dirty_card_queue_index_offset(),\n+                                    G1ThreadLocalData::dirty_card_queue_buffer_offset(),\n+                                    callRuntime,\n+                                    Z_thread, Rcard_addr_x, Rqueue_index);\n","filename":"src\/hotspot\/cpu\/s390\/gc\/g1\/g1BarrierSetAssembler_s390.cpp","additions":229,"deletions":50,"binary":false,"changes":279,"status":"modified"},{"patch":"@@ -2,2 +2,2 @@\n- * Copyright (c) 2018, 2023, Oracle and\/or its affiliates. All rights reserved.\n- * Copyright (c) 2018 SAP SE. All rights reserved.\n+ * Copyright (c) 2018, 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2018, 2024 SAP SE. All rights reserved.\n@@ -37,0 +37,2 @@\n+class G1PreBarrierStubC2;\n+class G1PostBarrierStubC2;\n@@ -65,1 +67,21 @@\n-#endif\n+#endif \/\/ COMPILER1\n+\n+#ifdef COMPILER2\n+  void g1_write_barrier_pre_c2(MacroAssembler* masm,\n+                               Register obj,\n+                               Register pre_val,\n+                               Register thread,\n+                               Register tmp1,\n+                               G1PreBarrierStubC2* c2_stub);\n+  void generate_c2_pre_barrier_stub(MacroAssembler* masm,\n+                                    G1PreBarrierStubC2* stub) const;\n+  void g1_write_barrier_post_c2(MacroAssembler* masm,\n+                                Register store_addr,\n+                                Register new_val,\n+                                Register thread,\n+                                Register tmp1,\n+                                Register tmp2,\n+                                G1PostBarrierStubC2* c2_stub);\n+  void generate_c2_post_barrier_stub(MacroAssembler* masm,\n+                                     G1PostBarrierStubC2* stub) const;\n+#endif \/\/ COMPILER2\n","filename":"src\/hotspot\/cpu\/s390\/gc\/g1\/g1BarrierSetAssembler_s390.hpp","additions":25,"deletions":3,"binary":false,"changes":28,"status":"modified"},{"patch":"@@ -0,0 +1,457 @@\n+\/\/\n+\/\/ Copyright (c) 2024, Oracle and\/or its affiliates. All rights reserved.\n+\/\/ Copyright 2024 IBM Corporation. All rights reserved.\n+\/\/ DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+\/\/\n+\/\/ This code is free software; you can redistribute it and\/or modify it\n+\/\/ under the terms of the GNU General Public License version 2 only, as\n+\/\/ published by the Free Software Foundation.\n+\/\/\n+\/\/ This code is distributed in the hope that it will be useful, but WITHOUT\n+\/\/ ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+\/\/ FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+\/\/ version 2 for more details (a copy is included in the LICENSE file that\n+\/\/ accompanied this code).\n+\/\/\n+\/\/ You should have received a copy of the GNU General Public License version\n+\/\/ 2 along with this work; if not, write to the Free Software Foundation,\n+\/\/ Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+\/\/\n+\/\/ Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+\/\/ or visit www.oracle.com if you need additional information or have any\n+\/\/ questions.\n+\/\/\n+\n+source_hpp %{\n+\n+#include \"gc\/g1\/c2\/g1BarrierSetC2.hpp\"\n+#include \"gc\/shared\/gc_globals.hpp\"\n+\n+%}\n+\n+source %{\n+\n+#include \"gc\/g1\/g1BarrierSetAssembler_s390.hpp\"\n+#include \"gc\/g1\/g1BarrierSetRuntime.hpp\"\n+\n+static void write_barrier_pre(MacroAssembler* masm,\n+                              const MachNode* node,\n+                              Register obj,\n+                              Register pre_val,\n+                              Register tmp1,\n+                              RegSet preserve = RegSet(),\n+                              RegSet no_preserve = RegSet()) {\n+  if (!G1PreBarrierStubC2::needs_barrier(node)) {\n+    return;\n+  }\n+  Assembler::InlineSkippedInstructionsCounter skip_counter(masm);\n+  G1BarrierSetAssembler* g1_asm = static_cast<G1BarrierSetAssembler*>(BarrierSet::barrier_set()->barrier_set_assembler());\n+  G1PreBarrierStubC2* const stub = G1PreBarrierStubC2::create(node);\n+  for (RegSetIterator<Register> reg = preserve.begin(); *reg != noreg; ++reg) {\n+    stub->preserve(*reg);\n+  }\n+  for (RegSetIterator<Register> reg = no_preserve.begin(); *reg != noreg; ++reg) {\n+    stub->dont_preserve(*reg);\n+  }\n+  g1_asm->g1_write_barrier_pre_c2(masm, obj, pre_val, Z_thread, tmp1, stub);\n+}\n+\n+static void write_barrier_post(MacroAssembler* masm,\n+                               const MachNode* node,\n+                               Register store_addr,\n+                               Register new_val,\n+                               Register tmp1,\n+                               Register tmp2) {\n+  if (!G1PostBarrierStubC2::needs_barrier(node)) {\n+    return;\n+  }\n+  Assembler::InlineSkippedInstructionsCounter skip_counter(masm);\n+  G1BarrierSetAssembler* g1_asm = static_cast<G1BarrierSetAssembler*>(BarrierSet::barrier_set()->barrier_set_assembler());\n+  G1PostBarrierStubC2* const stub = G1PostBarrierStubC2::create(node);\n+  g1_asm->g1_write_barrier_post_c2(masm, store_addr, new_val, Z_thread, tmp1, tmp2, stub);\n+}\n+\n+%} \/\/ source\n+\n+\/\/ store pointer\n+instruct g1StoreP(indirect dst, memoryRegP src, iRegL tmp1, iRegL tmp2, flagsReg cr) %{\n+  predicate(UseG1GC && n->as_Store()->barrier_data() != 0);\n+  match(Set dst (StoreP dst src));\n+  effect(TEMP tmp1, TEMP tmp2, KILL cr);\n+  ins_cost(MEMORY_REF_COST);\n+  format %{ \"STG     $src,$dst\\t # ptr\" %}\n+  ins_encode %{\n+    __ block_comment(\"g1StoreP {\");\n+    write_barrier_pre(masm, this,\n+                      $dst$$Register  \/* obj     *\/,\n+                      $tmp1$$Register \/* pre_val *\/,\n+                      $tmp2$$Register \/* tmp1    *\/,\n+                      RegSet::of($dst$$Register, $src$$Register) \/* preserve *\/);\n+\n+    __ z_stg($src$$Register, Address($dst$$Register));\n+\n+    write_barrier_post(masm, this,\n+                       $dst$$Register, \/* store_addr *\/\n+                       $src$$Register  \/* new_val    *\/,\n+                       $tmp1$$Register \/* tmp1       *\/,\n+                       $tmp2$$Register \/* tmp2       *\/);\n+    __ block_comment(\"} g1StoreP\");\n+  %}\n+  ins_pipe(pipe_class_dummy);\n+%}\n+\n+\/\/ Store Compressed Pointer\n+instruct g1StoreN(indirect mem, iRegN_P2N src, iRegL tmp1, iRegL tmp2, iRegL tmp3, flagsReg cr) %{\n+  predicate(UseG1GC && n->as_Store()->barrier_data() != 0);\n+  match(Set mem (StoreN mem src));\n+  effect(TEMP tmp1, TEMP tmp2, TEMP tmp3, KILL cr);\n+  ins_cost(MEMORY_REF_COST);\n+  format %{ \"STY     $src,$mem\\t # (cOop)\" %}\n+  ins_encode %{\n+    __ block_comment(\"g1StoreN {\");\n+    write_barrier_pre(masm, this,\n+                      $mem$$Register  \/* obj     *\/,\n+                      $tmp1$$Register \/* pre_val *\/,\n+                      $tmp2$$Register \/* tmp1    *\/,\n+                      RegSet::of($mem$$Register, $src$$Register) \/* preserve *\/);\n+\n+    __ z_sty($src$$Register, Address($mem$$Register));\n+\n+    if ((barrier_data() & G1C2BarrierPost) != 0) {\n+      if ((barrier_data() & G1C2BarrierPostNotNull) == 0) {\n+        __ oop_decoder($tmp1$$Register, $src$$Register, true \/* maybe_null *\/);\n+      } else {\n+        __ oop_decoder($tmp1$$Register, $src$$Register, false \/* maybe_null *\/);\n+      }\n+    }\n+\n+    write_barrier_post(masm, this,\n+                       $mem$$Register  \/* store_addr *\/,\n+                       $tmp1$$Register \/* new_val    *\/,\n+                       $tmp2$$Register \/* tmp1       *\/,\n+                       $tmp3$$Register \/* tmp2       *\/);\n+    __ block_comment(\"} g1StoreN\");\n+  %}\n+\n+  ins_pipe(pipe_class_dummy);\n+%}\n+\n+instruct g1CompareAndSwapN(indirect mem_ptr, rarg5RegN oldval, iRegN_P2N newval, iRegI res, iRegL tmp1, iRegL tmp2, iRegL tmp3, flagsReg cr) %{\n+  predicate(UseG1GC && n->as_LoadStore()->barrier_data() != 0);\n+  match(Set res (CompareAndSwapN mem_ptr (Binary oldval newval)));\n+  match(Set res (WeakCompareAndSwapN mem_ptr (Binary oldval newval)));\n+  effect(USE mem_ptr, TEMP res, TEMP tmp1, TEMP tmp2, TEMP tmp3, USE_KILL oldval, KILL cr);\n+  format %{ \"$res = CompareAndSwapN $oldval,$newval,$mem_ptr\" %}\n+  ins_encode %{\n+    assert_different_registers($oldval$$Register, $mem_ptr$$Register);\n+    assert_different_registers($newval$$Register, $mem_ptr$$Register);\n+    __ block_comment(\"g1compareAndSwapN {\");\n+\n+    Register Rcomp = reg_to_register_object($oldval$$reg);\n+    Register Rnew  = reg_to_register_object($newval$$reg);\n+    Register Raddr = reg_to_register_object($mem_ptr$$reg);\n+    Register Rres  = reg_to_register_object($res$$reg);\n+\n+    write_barrier_pre(masm, this,\n+                      Raddr           \/* obj     *\/,\n+                      $tmp1$$Register \/* pre_val *\/,\n+                      $tmp2$$Register \/* tmp1    *\/,\n+                      RegSet::of(Raddr, Rcomp, Rnew) \/* preserve *\/,\n+                      RegSet::of(Rres) \/* no_preserve *\/);\n+\n+    __ z_cs(Rcomp, Rnew, 0, Raddr);\n+\n+    assert_different_registers(Rres, Raddr);\n+    if (VM_Version::has_LoadStoreConditional()) {\n+      __ load_const_optimized(Z_R0_scratch, 0L); \/\/ false (failed)\n+      __ load_const_optimized(Rres, 1L);         \/\/ true  (succeed)\n+      __ z_locgr(Rres, Z_R0_scratch, Assembler::bcondNotEqual);\n+    } else {\n+      Label done;\n+      __ load_const_optimized(Rres, 0L); \/\/ false (failed)\n+      __ z_brne(done);                   \/\/ Assume true to be the common case.\n+      __ load_const_optimized(Rres, 1L); \/\/ true  (succeed)\n+      __ bind(done);\n+    }\n+\n+    __ oop_decoder($tmp3$$Register, Rnew, true \/* maybe_null *\/);\n+\n+    write_barrier_post(masm, this,\n+                       Raddr            \/* store_addr *\/,\n+                       $tmp3$$Register  \/* new_val    *\/,\n+                       $tmp1$$Register  \/* tmp1       *\/,\n+                       $tmp2$$Register  \/* tmp2       *\/);\n+    __ block_comment(\"} g1compareAndSwapN\");\n+  %}\n+  ins_pipe(pipe_class_dummy);\n+%}\n+\n+instruct g1CompareAndExchangeN(iRegP mem_ptr, rarg5RegN oldval, iRegN_P2N newval, iRegN res, iRegL tmp1, iRegL tmp2, iRegL tmp3, flagsReg cr) %{\n+  predicate(UseG1GC && n->as_LoadStore()->barrier_data() != 0);\n+  match(Set res (CompareAndExchangeN mem_ptr (Binary oldval newval)));\n+  effect(USE mem_ptr, TEMP res, TEMP tmp1, TEMP tmp2, TEMP tmp3, USE_KILL oldval, KILL cr);\n+  format %{ \"$res = CompareAndExchangeN $oldval,$newval,$mem_ptr\" %}\n+  ins_encode %{\n+    assert_different_registers($oldval$$Register, $mem_ptr$$Register);\n+    assert_different_registers($newval$$Register, $mem_ptr$$Register);\n+    __ block_comment(\"g1CompareAndExchangeN {\");\n+    write_barrier_pre(masm, this,\n+                      $mem_ptr$$Register \/* obj     *\/,\n+                      $tmp1$$Register    \/* pre_val *\/,\n+                      $tmp2$$Register    \/* tmp1    *\/,\n+                      RegSet::of($mem_ptr$$Register, $oldval$$Register, $newval$$Register) \/* preserve *\/,\n+                      RegSet::of($res$$Register) \/* no_preserve *\/);\n+\n+    Register Rcomp = reg_to_register_object($oldval$$reg);\n+    Register Rnew  = reg_to_register_object($newval$$reg);\n+    Register Raddr = reg_to_register_object($mem_ptr$$reg);\n+\n+    Register Rres = reg_to_register_object($res$$reg);\n+    assert_different_registers(Rres, Raddr);\n+\n+    __ z_lgr(Rres, Rcomp);  \/\/ previous contents\n+    __ z_csy(Rres, Rnew, 0, Raddr); \/\/ Try to store new value.\n+\n+    __ oop_decoder($tmp1$$Register, Rnew, true \/* maybe_null *\/);\n+\n+    write_barrier_post(masm, this,\n+                       Raddr           \/* store_addr *\/,\n+                       $tmp1$$Register \/* new_val    *\/,\n+                       $tmp2$$Register \/* tmp1       *\/,\n+                       $tmp3$$Register \/* tmp2       *\/);\n+    __ block_comment(\"} g1CompareAndExchangeN\");\n+  %}\n+  ins_pipe(pipe_class_dummy);\n+%}\n+\n+\/\/ Load narrow oop\n+instruct g1LoadN(iRegN dst, indirect mem, iRegP tmp1, iRegP tmp2, flagsReg cr) %{\n+  predicate(UseG1GC && n->as_Load()->barrier_data() != 0);\n+  match(Set dst (LoadN mem));\n+  effect(TEMP dst, TEMP tmp1, TEMP tmp2, KILL cr);\n+  ins_cost(MEMORY_REF_COST);\n+  format %{ \"LoadN   $dst,$mem\\t # (cOop)\" %}\n+  ins_encode %{\n+    __ block_comment(\"g1LoadN {\");\n+    __ z_llgf($dst$$Register, Address($mem$$Register));\n+    if ((barrier_data() & G1C2BarrierPre) != 0) {\n+      __ oop_decoder($tmp1$$Register, $dst$$Register, true);\n+      write_barrier_pre(masm, this,\n+                        noreg           \/* obj     *\/,\n+                        $tmp1$$Register \/* pre_val *\/,\n+                        $tmp2$$Register );\n+    }\n+    __ block_comment(\"} g1LoadN\");\n+  %}\n+  ins_pipe(pipe_class_dummy);\n+%}\n+\n+instruct g1GetAndSetN(indirect mem, iRegN dst, iRegI tmp, iRegL tmp1, iRegL tmp2, iRegL tmp3, flagsReg cr) %{\n+  predicate(UseG1GC && n->as_LoadStore()->barrier_data() != 0);\n+  match(Set dst (GetAndSetN mem dst));\n+  effect(KILL cr, TEMP tmp, TEMP tmp1, TEMP tmp2, TEMP tmp3); \/\/ USE_DEF dst by match rule.\n+  format %{ \"XCHGN   $dst,[$mem]\\t # EXCHANGE (coop, atomic), temp $tmp\" %}\n+  ins_encode %{\n+    __ block_comment(\"g1GetAndSetN {\");\n+    assert_different_registers($mem$$Register, $dst$$Register);\n+    write_barrier_pre(masm, this,\n+                      $mem$$Register  \/* obj     *\/,\n+                      $tmp1$$Register \/* pre_val *\/,\n+                      $tmp2$$Register \/* tmp1    *\/,\n+                      RegSet::of($mem$$Register, $dst$$Register) \/* preserve *\/);\n+\n+    Register Rdst = reg_to_register_object($dst$$reg);\n+    Register Rtmp = reg_to_register_object($tmp$$reg);\n+    guarantee(Rdst != Rtmp, \"Fix match rule to use TEMP_DEF\");\n+    Label    retry;\n+\n+    \/\/ Iterate until swap succeeds.\n+    __ z_llgf(Rtmp, Address($mem$$Register)); \/\/ current contents\n+    __ bind(retry);\n+    \/\/ Calculate incremented value.\n+    __ z_csy(Rtmp, Rdst, Address($mem$$Register)); \/\/ Try to store new value.\n+    __ z_brne(retry); \/\/ Yikes, concurrent update, need to retry.\n+\n+    __ oop_decoder($tmp1$$Register, $dst$$Register, true \/* maybe_null *\/);\n+\n+    __ z_lgr(Rdst, Rtmp);  \/\/ Exchanged value from memory is return value.\n+\n+    write_barrier_post(masm, this,\n+                       $mem$$Register  \/* store_addr *\/,\n+                       $tmp1$$Register \/* new_val    *\/,\n+                       $tmp2$$Register \/* tmp1       *\/,\n+                       $tmp3$$Register \/* tmp2       *\/);\n+\n+    __ block_comment(\"} g1GetAndSetN\");\n+  %}\n+  ins_pipe(pipe_class_dummy);\n+%}\n+\n+instruct g1CompareAndSwapP(iRegP mem_ptr, rarg5RegP oldval, iRegP_N2P newval, iRegI res, iRegL tmp1, iRegL tmp2, flagsReg cr) %{\n+  predicate(UseG1GC && n->as_LoadStore()->barrier_data() != 0);\n+  match(Set res (CompareAndSwapP mem_ptr (Binary oldval newval)));\n+  match(Set res (WeakCompareAndSwapP mem_ptr (Binary oldval newval)));\n+  effect(TEMP res, TEMP tmp1, TEMP tmp2, USE mem_ptr, USE_KILL oldval, KILL cr);\n+  format %{ \"$res = CompareAndSwapP $oldval,$newval,$mem_ptr\" %}\n+  ins_encode %{\n+    __ block_comment(\"g1CompareAndSwapP {\");\n+    assert_different_registers($oldval$$Register, $mem_ptr$$Register);\n+    assert_different_registers($newval$$Register, $mem_ptr$$Register);\n+\n+    Register Rcomp = reg_to_register_object($oldval$$reg);\n+    Register Rnew  = reg_to_register_object($newval$$reg);\n+    Register Raddr = reg_to_register_object($mem_ptr$$reg);\n+    Register Rres  = reg_to_register_object($res$$reg);\n+\n+    write_barrier_pre(masm, this,\n+                      noreg           \/* obj     *\/,\n+                      Rcomp           \/* pre_val *\/,\n+                      $tmp1$$Register \/* tmp1    *\/,\n+                      RegSet::of(Raddr, Rcomp, Rnew) \/* preserve *\/,\n+                      RegSet::of(Rres) \/* no_preserve *\/);\n+\n+    __ z_csg(Rcomp, Rnew, 0, Raddr);\n+\n+    if (VM_Version::has_LoadStoreConditional()) {\n+      __ load_const_optimized(Z_R0_scratch, 0L); \/\/ false (failed)\n+      __ load_const_optimized(Rres, 1L);         \/\/ true  (succeed)\n+      __ z_locgr(Rres, Z_R0_scratch, Assembler::bcondNotEqual);\n+    } else {\n+      Label done;\n+      __ load_const_optimized(Rres, 0L); \/\/ false (failed)\n+      __ z_brne(done);                   \/\/ Assume true to be the common case.\n+      __ load_const_optimized(Rres, 1L); \/\/ true  (succeed)\n+      __ bind(done);\n+    }\n+\n+    write_barrier_post(masm, this,\n+                       Raddr           \/* store_addr *\/,\n+                       Rnew            \/* new_val    *\/,\n+                       $tmp1$$Register \/* tmp1       *\/,\n+                       $tmp2$$Register \/* tmp2       *\/);\n+    __ block_comment(\"} g1CompareAndSwapP\");\n+  %}\n+  ins_pipe(pipe_class_dummy);\n+%}\n+\n+instruct g1CompareAndExchangeP(iRegP mem_ptr, rarg5RegP oldval, iRegP_N2P newval, iRegP res, iRegL tmp1, iRegL tmp2, flagsReg cr) %{\n+  predicate(UseG1GC && n->as_LoadStore()->barrier_data() != 0);\n+  match(Set res (CompareAndExchangeP mem_ptr (Binary oldval newval)));\n+  effect(TEMP res, TEMP tmp1, TEMP tmp2, USE mem_ptr, USE_KILL oldval, KILL cr);\n+  format %{ \"$res = CompareAndExchangeP $oldval,$newval,$mem_ptr\" %}\n+  ins_encode %{\n+    __ block_comment(\"g1CompareAndExchangeP {\");\n+    assert_different_registers($oldval$$Register, $mem_ptr$$Register);\n+    assert_different_registers($newval$$Register, $mem_ptr$$Register);\n+\n+    \/\/ Pass $oldval to the pre-barrier (instead of loading from $mem), because\n+    \/\/ $oldval is the only value that can be overwritten.\n+    \/\/ The same holds for g1CompareAndSwapP.\n+    write_barrier_pre(masm, this,\n+                      noreg             \/* obj     *\/,\n+                      $oldval$$Register \/* pre_val *\/,\n+                      $tmp2$$Register   \/* tmp1    *\/,\n+                      RegSet::of($mem_ptr$$Register, $oldval$$Register, $newval$$Register) \/* preserve *\/,\n+                      RegSet::of($res$$Register) \/* no_preserve *\/);\n+\n+    __ z_lgr($res$$Register, $oldval$$Register); \/\/ previous content\n+\n+    __ z_csg($oldval$$Register, $newval$$Register, 0, $mem_ptr$$reg);\n+\n+    write_barrier_post(masm, this,\n+                       $mem_ptr$$Register \/* store_addr *\/,\n+                       $newval$$Register  \/* new_val    *\/,\n+                       $tmp1$$Register    \/* tmp1       *\/,\n+                       $tmp2$$Register    \/* tmp2       *\/);\n+    __ block_comment(\"} g1CompareAndExchangeP\");\n+  %}\n+  ins_pipe(pipe_class_dummy);\n+%}\n+\n+\/\/ Load Pointer\n+instruct g1LoadP(iRegP dst, memory mem, iRegL tmp1, flagsReg cr) %{\n+  predicate(UseG1GC && n->as_Load()->barrier_data() != 0);\n+  match(Set dst (LoadP mem));\n+  effect(TEMP dst, TEMP tmp1, KILL cr);\n+  ins_cost(MEMORY_REF_COST);\n+  format %{ \"LG      $dst,$mem\\t # ptr\" %}\n+  ins_encode %{\n+    __ block_comment(\"g1LoadP {\");\n+    __ z_lg($dst$$Register, $mem$$Address);\n+    write_barrier_pre(masm, this,\n+                      noreg \/* obj *\/,\n+                      $dst$$Register \/* pre_val *\/,\n+                      $tmp1$$Register );\n+    __ block_comment(\"} g1LoadP\");\n+  %}\n+  ins_pipe(pipe_class_dummy);\n+%}\n+\n+instruct g1GetAndSetP(indirect mem, iRegP dst, iRegL tmp, iRegL tmp1, iRegL tmp2, flagsReg cr) %{\n+  predicate(UseG1GC && n->as_LoadStore()->barrier_data() != 0);\n+  match(Set dst (GetAndSetP mem dst));\n+  effect(KILL cr, TEMP tmp, TEMP tmp1, TEMP tmp2); \/\/ USE_DEF dst by match rule.\n+  format %{ \"XCHGP   $dst,[$mem]\\t # EXCHANGE (oop, atomic), temp $tmp\" %}\n+  ins_encode %{\n+    __ block_comment(\"g1GetAndSetP {\");\n+\n+    write_barrier_pre(masm, this,\n+                      $mem$$Register  \/* obj  *\/,\n+                      $tmp$$Register  \/* pre_val (as a temporary register) *\/,\n+                      $tmp1$$Register \/* tmp1 *\/,\n+                      RegSet::of($mem$$Register, $dst$$Register) \/* preserve *\/);\n+\n+    __ z_lgr($tmp1$$Register, $dst$$Register);\n+    Register Rdst = reg_to_register_object($dst$$reg);\n+    Register Rtmp = reg_to_register_object($tmp$$reg);\n+    guarantee(Rdst != Rtmp, \"Fix match rule to use TEMP_DEF\");\n+    Label    retry;\n+\n+    \/\/ Iterate until swap succeeds.\n+    __ z_lg(Rtmp, Address($mem$$Register));  \/\/ current contents\n+    __ bind(retry);\n+    \/\/ Calculate incremented value.\n+    __ z_csg(Rtmp, Rdst, Address($mem$$Register)); \/\/ Try to store new value.\n+    __ z_brne(retry);                              \/\/ Yikes, concurrent update, need to retry.\n+    __ z_lgr(Rdst, Rtmp);                          \/\/ Exchanged value from memory is return value.\n+\n+    write_barrier_post(masm, this,\n+                       $mem$$Register  \/* store_addr *\/,\n+                       $tmp1$$Register \/* new_val    *\/,\n+                       $tmp2$$Register \/* tmp1       *\/,\n+                       $tmp$$Register  \/* tmp2       *\/);\n+    __ block_comment(\"} g1GetAndSetP\");\n+  %}\n+  ins_pipe(pipe_class_dummy);\n+%}\n+\n+instruct g1EncodePAndStoreN(indirect mem, iRegP src, iRegL tmp1, iRegL tmp2, flagsReg cr)\n+%{\n+  predicate(UseG1GC && n->as_Store()->barrier_data() != 0);\n+  match(Set mem (StoreN mem (EncodeP src)));\n+  effect(TEMP tmp1, TEMP tmp2, KILL cr);\n+  \/\/ ins_cost(INSN_COST);\n+  format %{ \"encode_heap_oop $tmp1, $src\\n\\t\"\n+            \"st  $tmp1, $mem\\t# compressed ptr\" %}\n+  ins_encode %{\n+    __ block_comment(\"g1EncodePAndStoreN {\");\n+    write_barrier_pre(masm, this,\n+                      $mem$$Register  \/* obj     *\/,\n+                      $tmp1$$Register \/* pre_val *\/,\n+                      $tmp2$$Register \/* tmp1    *\/,\n+                      RegSet::of($mem$$Register, $src$$Register) \/* preserve *\/);\n+    if ((barrier_data() & G1C2BarrierPostNotNull) == 0) {\n+      __ oop_encoder($tmp1$$Register, $src$$Register, true \/* maybe_null *\/);\n+    } else {\n+      __ oop_encoder($tmp1$$Register, $src$$Register, false \/* maybe_null *\/);\n+    }\n+    __ z_st($tmp1$$Register, Address($mem$$Register));\n+    write_barrier_post(masm, this,\n+                       $mem$$Register  \/* store_addr *\/,\n+                       $src$$Register  \/* new_val    *\/,\n+                       $tmp1$$Register \/* tmp1       *\/,\n+                       $tmp2$$Register \/* tmp2       *\/);\n+    __ block_comment(\"} g1EncodePAndStoreN\");\n+  %}\n+  ins_pipe(pipe_class_dummy);\n+%}\n","filename":"src\/hotspot\/cpu\/s390\/gc\/g1\/g1_s390.ad","additions":457,"deletions":0,"binary":false,"changes":457,"status":"added"},{"patch":"@@ -36,0 +36,3 @@\n+#ifdef COMPILER2\n+#include \"gc\/shared\/c2\/barrierSetC2.hpp\"\n+#endif \/\/ COMPILER2\n@@ -197,2 +200,87 @@\n-OptoReg::Name BarrierSetAssembler::refine_register(const Node* node, OptoReg::Name opto_reg) {\n-  Unimplemented(); \/\/ This must be implemented to support late barrier expansion.\n+OptoReg::Name BarrierSetAssembler::refine_register(const Node* node, OptoReg::Name opto_reg) const {\n+  if (!OptoReg::is_reg(opto_reg)) {\n+    return OptoReg::Bad;\n+  }\n+\n+  VMReg vm_reg = OptoReg::as_VMReg(opto_reg);\n+  if ((vm_reg->is_Register() || vm_reg ->is_FloatRegister()) && (opto_reg & 1) != 0) {\n+    return OptoReg::Bad;\n+  }\n+\n+  return opto_reg;\n+}\n+\n+#undef __\n+#define __ _masm->\n+\n+SaveLiveRegisters::SaveLiveRegisters(MacroAssembler *masm, BarrierStubC2 *stub)\n+  : _masm(masm), _reg_mask(stub->preserve_set()) {\n+\n+  const int register_save_size = iterate_over_register_mask(ACTION_COUNT_ONLY) * BytesPerWord;\n+\n+  _frame_size = align_up(register_save_size, frame::alignment_in_bytes) + frame::z_abi_160_size; \/\/ FIXME: this could be restricted to argument only\n+\n+  __ save_return_pc();\n+  __ push_frame(_frame_size, Z_R14); \/\/ FIXME: check if Z_R1_scaratch can do a job here;\n+\n+  __ z_lg(Z_R14, _z_common_abi(return_pc) + _frame_size, Z_SP);\n+\n+  iterate_over_register_mask(ACTION_SAVE, _frame_size);\n+}\n+\n+SaveLiveRegisters::~SaveLiveRegisters() {\n+  iterate_over_register_mask(ACTION_RESTORE, _frame_size);\n+\n+  __ pop_frame();\n+\n+  __ restore_return_pc();\n+}\n+\n+int SaveLiveRegisters::iterate_over_register_mask(IterationAction action, int offset) {\n+  int reg_save_index = 0;\n+  RegMaskIterator live_regs_iterator(_reg_mask);\n+\n+  while(live_regs_iterator.has_next()) {\n+    const OptoReg::Name opto_reg = live_regs_iterator.next();\n+\n+    \/\/ Filter out stack slots (spilled registers, i.e., stack-allocated registers).\n+    if (!OptoReg::is_reg(opto_reg)) {\n+      continue;\n+    }\n+\n+    const VMReg vm_reg = OptoReg::as_VMReg(opto_reg);\n+    if (vm_reg->is_Register()) {\n+      Register std_reg = vm_reg->as_Register();\n+\n+      if (std_reg->encoding() >= Z_R2->encoding() && std_reg->encoding() <= Z_R15->encoding()) {\n+        reg_save_index++;\n+\n+        if (action == ACTION_SAVE) {\n+          __ z_stg(std_reg, offset - reg_save_index * BytesPerWord, Z_SP);\n+        } else if (action == ACTION_RESTORE) {\n+          __ z_lg(std_reg, offset - reg_save_index * BytesPerWord, Z_SP);\n+        } else {\n+          assert(action == ACTION_COUNT_ONLY, \"Sanity\");\n+        }\n+      }\n+    } else if (vm_reg->is_FloatRegister()) {\n+      FloatRegister fp_reg = vm_reg->as_FloatRegister();\n+      if (fp_reg->encoding() >= Z_F0->encoding() && fp_reg->encoding() <= Z_F15->encoding()\n+          && fp_reg->encoding() != Z_F1->encoding()) {\n+        reg_save_index++;\n+\n+        if (action == ACTION_SAVE) {\n+          __ z_std(fp_reg, offset - reg_save_index * BytesPerWord, Z_SP);\n+        } else if (action == ACTION_RESTORE) {\n+          __ z_ld(fp_reg, offset - reg_save_index * BytesPerWord, Z_SP);\n+        } else {\n+          assert(action == ACTION_COUNT_ONLY, \"Sanity\");\n+        }\n+      }\n+    } else if (false \/* vm_reg->is_VectorRegister() *\/){\n+      fatal(\"Vector register support is not there yet!\");\n+    } else {\n+      fatal(\"Register type is not known\");\n+    }\n+  }\n+  return reg_save_index;\n","filename":"src\/hotspot\/cpu\/s390\/gc\/shared\/barrierSetAssembler_s390.cpp","additions":90,"deletions":2,"binary":false,"changes":92,"status":"modified"},{"patch":"@@ -35,0 +35,1 @@\n+#include \"opto\/regmask.hpp\"\n@@ -36,0 +37,1 @@\n+class BarrierStubC2;\n@@ -65,1 +67,1 @@\n-                                OptoReg::Name opto_reg);\n+                                OptoReg::Name opto_reg) const;\n@@ -69,0 +71,34 @@\n+#ifdef COMPILER2\n+\n+\/\/ This class saves and restores the registers that need to be preserved across\n+\/\/ the runtime call represented by a given C2 barrier stub. Use as follows:\n+\/\/ {\n+\/\/   SaveLiveRegisters save(masm, stub);\n+\/\/   ..\n+\/\/   __ call_VM_leaf(...);\n+\/\/   ..\n+\/\/ }\n+\n+class SaveLiveRegisters {\n+  MacroAssembler* _masm;\n+  RegMask _reg_mask;\n+  Register _result_reg;\n+  int _frame_size;\n+\n+ public:\n+  SaveLiveRegisters(MacroAssembler *masm, BarrierStubC2 *stub);\n+\n+  ~SaveLiveRegisters();\n+\n+ private:\n+  enum IterationAction : int {\n+    ACTION_SAVE,\n+    ACTION_RESTORE,\n+    ACTION_COUNT_ONLY\n+  };\n+\n+  int iterate_over_register_mask(IterationAction action, int offset = 0);\n+};\n+\n+#endif \/\/ COMPILER2\n+\n","filename":"src\/hotspot\/cpu\/s390\/gc\/shared\/barrierSetAssembler_s390.hpp","additions":37,"deletions":1,"binary":false,"changes":38,"status":"modified"},{"patch":"@@ -2130,1 +2130,1 @@\n-  BLOCK_COMMENT(\"pop_frame:\");\n+  BLOCK_COMMENT(\"pop_frame {\");\n@@ -2132,0 +2132,1 @@\n+  BLOCK_COMMENT(\"} pop_frame\");\n","filename":"src\/hotspot\/cpu\/s390\/macroAssembler_s390.cpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -451,0 +451,8 @@\n+typedef AbstractRegSet<Register> RegSet;\n+\n+template <>\n+inline Register AbstractRegSet<Register>::first() {\n+  if (_bitset == 0) { return noreg; }\n+  return as_Register(count_trailing_zeros(_bitset));\n+}\n+\n","filename":"src\/hotspot\/cpu\/s390\/register_s390.hpp","additions":8,"deletions":0,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -1647,0 +1647,4 @@\n+  if (is_encode_and_store_pattern(n, m)) {\n+    mstack.push(m, Visit);\n+    return true;\n+  }\n@@ -3916,0 +3920,1 @@\n+  predicate(n->as_Load()->barrier_data() == 0);\n@@ -3927,0 +3932,1 @@\n+  predicate(n->as_Load()->barrier_data() == 0);\n@@ -4289,0 +4295,1 @@\n+  predicate(n->as_Store()->barrier_data() == 0);\n@@ -4391,0 +4398,1 @@\n+  predicate(n->as_Store()->barrier_data() == 0);\n@@ -4420,0 +4428,1 @@\n+  predicate(n->as_Load()->barrier_data() == 0);\n@@ -4483,1 +4492,1 @@\n-  predicate(false && (CompressedOops::base()==nullptr)&&(CompressedOops::shift()==0));\n+  predicate(false && (CompressedOops::base()==nullptr) && (CompressedOops::shift()==0));\n@@ -4738,0 +4747,1 @@\n+  predicate(n->as_Store()->barrier_data() == 0);\n@@ -5149,0 +5159,1 @@\n+  predicate(n->as_LoadStore()->barrier_data() == 0);\n@@ -5159,0 +5170,1 @@\n+  predicate(n->as_LoadStore()->barrier_data() == 0);\n@@ -5446,0 +5458,1 @@\n+  predicate(n->as_LoadStore()->barrier_data() == 0);\n@@ -5455,0 +5468,1 @@\n+  predicate(n->as_LoadStore()->barrier_data() == 0);\n@@ -5929,1 +5943,1 @@\n-  predicate(VM_Version::has_MemWithImmALUOps());\n+  predicate(VM_Version::has_MemWithImmALUOps() && n->as_LoadStore()->barrier_data() == 0);\n","filename":"src\/hotspot\/cpu\/s390\/s390.ad","additions":16,"deletions":2,"binary":false,"changes":18,"status":"modified"},{"patch":"@@ -41,1 +41,4 @@\n-#endif\n+#endif \/\/ COMPILER1\n+#ifdef COMPILER2\n+#include \"gc\/g1\/c2\/g1BarrierSetC2.hpp\"\n+#endif \/\/ COMPILER2\n@@ -163,0 +166,50 @@\n+static void generate_queue_insertion(MacroAssembler* masm, ByteSize index_offset, ByteSize buffer_offset, Label& runtime,\n+                                     const Register thread, const Register value, const Register temp) {\n+  \/\/ This code assumes that buffer index is pointer sized.\n+  STATIC_ASSERT(in_bytes(SATBMarkQueue::byte_width_of_index()) == sizeof(intptr_t));\n+  \/\/ Can we store a value in the given thread's buffer?\n+  \/\/ (The index field is typed as size_t.)\n+  __ movptr(temp, Address(thread, in_bytes(index_offset)));   \/\/ temp := *(index address)\n+  __ testptr(temp, temp);                                     \/\/ index == 0?\n+  __ jcc(Assembler::zero, runtime);                           \/\/ jump to runtime if index == 0 (full buffer)\n+  \/\/ The buffer is not full, store value into it.\n+  __ subptr(temp, wordSize);                                  \/\/ temp := next index\n+  __ movptr(Address(thread, in_bytes(index_offset)), temp);   \/\/ *(index address) := next index\n+  __ addptr(temp, Address(thread, in_bytes(buffer_offset)));  \/\/ temp := buffer address + next index\n+  __ movptr(Address(temp, 0), value);                         \/\/ *(buffer address + next index) := value\n+}\n+\n+static void generate_pre_barrier_fast_path(MacroAssembler* masm,\n+                                           const Register thread) {\n+  Address in_progress(thread, in_bytes(G1ThreadLocalData::satb_mark_queue_active_offset()));\n+  \/\/ Is marking active?\n+  if (in_bytes(SATBMarkQueue::byte_width_of_active()) == 4) {\n+    __ cmpl(in_progress, 0);\n+  } else {\n+    assert(in_bytes(SATBMarkQueue::byte_width_of_active()) == 1, \"Assumption\");\n+    __ cmpb(in_progress, 0);\n+  }\n+}\n+\n+static void generate_pre_barrier_slow_path(MacroAssembler* masm,\n+                                           const Register obj,\n+                                           const Register pre_val,\n+                                           const Register thread,\n+                                           const Register tmp,\n+                                           Label& done,\n+                                           Label& runtime) {\n+  \/\/ Do we need to load the previous value?\n+  if (obj != noreg) {\n+    __ load_heap_oop(pre_val, Address(obj, 0), noreg, noreg, AS_RAW);\n+  }\n+  \/\/ Is the previous value null?\n+  __ cmpptr(pre_val, NULL_WORD);\n+  __ jcc(Assembler::equal, done);\n+  generate_queue_insertion(masm,\n+                           G1ThreadLocalData::satb_mark_queue_index_offset(),\n+                           G1ThreadLocalData::satb_mark_queue_buffer_offset(),\n+                           runtime,\n+                           thread, pre_val, tmp);\n+  __ jmp(done);\n+}\n+\n@@ -188,20 +241,2 @@\n-  Address in_progress(thread, in_bytes(G1ThreadLocalData::satb_mark_queue_active_offset()));\n-  Address index(thread, in_bytes(G1ThreadLocalData::satb_mark_queue_index_offset()));\n-  Address buffer(thread, in_bytes(G1ThreadLocalData::satb_mark_queue_buffer_offset()));\n-\n-  \/\/ Is marking active?\n-  if (in_bytes(SATBMarkQueue::byte_width_of_active()) == 4) {\n-    __ cmpl(in_progress, 0);\n-  } else {\n-    assert(in_bytes(SATBMarkQueue::byte_width_of_active()) == 1, \"Assumption\");\n-    __ cmpb(in_progress, 0);\n-  }\n-  __ jcc(Assembler::equal, done);\n-\n-  \/\/ Do we need to load the previous value?\n-  if (obj != noreg) {\n-    __ load_heap_oop(pre_val, Address(obj, 0), noreg, noreg, AS_RAW);\n-  }\n-\n-  \/\/ Is the previous value null?\n-  __ cmpptr(pre_val, NULL_WORD);\n+  generate_pre_barrier_fast_path(masm, thread);\n+  \/\/ If marking is not active (*(mark queue active address) == 0), jump to done\n@@ -209,16 +244,1 @@\n-\n-  \/\/ Can we store original value in the thread's buffer?\n-  \/\/ Is index == 0?\n-  \/\/ (The index field is typed as size_t.)\n-\n-  __ movptr(tmp, index);                   \/\/ tmp := *index_adr\n-  __ cmpptr(tmp, 0);                       \/\/ tmp == 0?\n-  __ jcc(Assembler::equal, runtime);       \/\/ If yes, goto runtime\n-\n-  __ subptr(tmp, wordSize);                \/\/ tmp := tmp - wordSize\n-  __ movptr(index, tmp);                   \/\/ *index_adr := tmp\n-  __ addptr(tmp, buffer);                  \/\/ tmp := tmp + *buffer_adr\n-\n-  \/\/ Record the previous value\n-  __ movptr(Address(tmp, 0), pre_val);\n-  __ jmp(done);\n+  generate_pre_barrier_slow_path(masm, obj, pre_val, thread, tmp, done, runtime);\n@@ -266,0 +286,48 @@\n+static void generate_post_barrier_fast_path(MacroAssembler* masm,\n+                                            const Register store_addr,\n+                                            const Register new_val,\n+                                            const Register tmp,\n+                                            const Register tmp2,\n+                                            Label& done,\n+                                            bool new_val_may_be_null) {\n+  CardTableBarrierSet* ct = barrier_set_cast<CardTableBarrierSet>(BarrierSet::barrier_set());\n+  \/\/ Does store cross heap regions?\n+  __ movptr(tmp, store_addr);                                    \/\/ tmp := store address\n+  __ xorptr(tmp, new_val);                                       \/\/ tmp := store address ^ new value\n+  __ shrptr(tmp, G1HeapRegion::LogOfHRGrainBytes);               \/\/ ((store address ^ new value) >> LogOfHRGrainBytes) == 0?\n+  __ jcc(Assembler::equal, done);\n+  \/\/ Crosses regions, storing null?\n+  if (new_val_may_be_null) {\n+    __ cmpptr(new_val, NULL_WORD);                               \/\/ new value == null?\n+    __ jcc(Assembler::equal, done);\n+  }\n+  \/\/ Storing region crossing non-null, is card young?\n+  __ movptr(tmp, store_addr);                                    \/\/ tmp := store address\n+  __ shrptr(tmp, CardTable::card_shift());                       \/\/ tmp := card address relative to card table base\n+  \/\/ Do not use ExternalAddress to load 'byte_map_base', since 'byte_map_base' is NOT\n+  \/\/ a valid address and therefore is not properly handled by the relocation code.\n+  __ movptr(tmp2, (intptr_t)ct->card_table()->byte_map_base());  \/\/ tmp2 := card table base address\n+  __ addptr(tmp, tmp2);                                          \/\/ tmp := card address\n+  __ cmpb(Address(tmp, 0), G1CardTable::g1_young_card_val());    \/\/ *(card address) == young_card_val?\n+}\n+\n+static void generate_post_barrier_slow_path(MacroAssembler* masm,\n+                                            const Register thread,\n+                                            const Register tmp,\n+                                            const Register tmp2,\n+                                            Label& done,\n+                                            Label& runtime) {\n+  __ membar(Assembler::Membar_mask_bits(Assembler::StoreLoad));  \/\/ StoreLoad membar\n+  __ cmpb(Address(tmp, 0), G1CardTable::dirty_card_val());       \/\/ *(card address) == dirty_card_val?\n+  __ jcc(Assembler::equal, done);\n+  \/\/ Storing a region crossing, non-null oop, card is clean.\n+  \/\/ Dirty card and log.\n+  __ movb(Address(tmp, 0), G1CardTable::dirty_card_val());       \/\/ *(card address) := dirty_card_val\n+  generate_queue_insertion(masm,\n+                           G1ThreadLocalData::dirty_card_queue_index_offset(),\n+                           G1ThreadLocalData::dirty_card_queue_buffer_offset(),\n+                           runtime,\n+                           thread, tmp, tmp2);\n+  __ jmp(done);\n+}\n+\n@@ -276,6 +344,0 @@\n-  Address queue_index(thread, in_bytes(G1ThreadLocalData::dirty_card_queue_index_offset()));\n-  Address buffer(thread, in_bytes(G1ThreadLocalData::dirty_card_queue_buffer_offset()));\n-\n-  CardTableBarrierSet* ct =\n-    barrier_set_cast<CardTableBarrierSet>(BarrierSet::barrier_set());\n-\n@@ -285,5 +347,2 @@\n-  \/\/ Does store cross heap regions?\n-\n-  __ movptr(tmp, store_addr);\n-  __ xorptr(tmp, new_val);\n-  __ shrptr(tmp, G1HeapRegion::LogOfHRGrainBytes);\n+  generate_post_barrier_fast_path(masm, store_addr, new_val, tmp, tmp2, done, true \/* new_val_may_be_null *\/);\n+  \/\/ If card is young, jump to done\n@@ -291,0 +350,1 @@\n+  generate_post_barrier_slow_path(masm, thread, tmp, tmp2, done, runtime);\n@@ -292,1 +352,6 @@\n-  \/\/ crosses regions, storing null?\n+  __ bind(runtime);\n+  \/\/ save the live input values\n+  RegSet saved = RegSet::of(store_addr NOT_LP64(COMMA thread));\n+  __ push_set(saved);\n+  __ call_VM_leaf(CAST_FROM_FN_PTR(address, G1BarrierSetRuntime::write_ref_field_post_entry), tmp, thread);\n+  __ pop_set(saved);\n@@ -294,2 +359,2 @@\n-  __ cmpptr(new_val, NULL_WORD);\n-  __ jcc(Assembler::equal, done);\n+  __ bind(done);\n+}\n@@ -297,1 +362,1 @@\n-  \/\/ storing region crossing non-null, is card already dirty?\n+#if defined(COMPILER2)\n@@ -299,2 +364,17 @@\n-  const Register card_addr = tmp;\n-  const Register cardtable = tmp2;\n+static void generate_c2_barrier_runtime_call(MacroAssembler* masm, G1BarrierStubC2* stub, const Register arg, const address runtime_path) {\n+#ifdef _LP64\n+  SaveLiveRegisters save_registers(masm, stub);\n+  if (c_rarg0 != arg) {\n+    __ mov(c_rarg0, arg);\n+  }\n+  __ mov(c_rarg1, r15_thread);\n+  \/\/ rax is a caller-saved, non-argument-passing register, so it does not\n+  \/\/ interfere with c_rarg0 or c_rarg1. If it contained any live value before\n+  \/\/ entering this stub, it is saved at this point, and restored after the\n+  \/\/ call. If it did not contain any live value, it is free to be used. In\n+  \/\/ either case, it is safe to use it here as a call scratch register.\n+  __ call(RuntimeAddress(runtime_path), rax);\n+#else\n+  Unimplemented();\n+#endif \/\/ _LP64\n+}\n@@ -302,6 +382,13 @@\n-  __ movptr(card_addr, store_addr);\n-  __ shrptr(card_addr, CardTable::card_shift());\n-  \/\/ Do not use ExternalAddress to load 'byte_map_base', since 'byte_map_base' is NOT\n-  \/\/ a valid address and therefore is not properly handled by the relocation code.\n-  __ movptr(cardtable, (intptr_t)ct->card_table()->byte_map_base());\n-  __ addptr(card_addr, cardtable);\n+void G1BarrierSetAssembler::g1_write_barrier_pre_c2(MacroAssembler* masm,\n+                                                    Register obj,\n+                                                    Register pre_val,\n+                                                    Register thread,\n+                                                    Register tmp,\n+                                                    G1PreBarrierStubC2* stub) {\n+#ifdef _LP64\n+  assert(thread == r15_thread, \"must be\");\n+#endif \/\/ _LP64\n+  assert(pre_val != noreg, \"check this code\");\n+  if (obj != noreg) {\n+    assert_different_registers(obj, pre_val, tmp);\n+  }\n@@ -309,2 +396,1 @@\n-  __ cmpb(Address(card_addr, 0), G1CardTable::g1_young_card_val());\n-  __ jcc(Assembler::equal, done);\n+  stub->initialize_registers(obj, pre_val, thread, tmp);\n@@ -312,3 +398,3 @@\n-  __ membar(Assembler::Membar_mask_bits(Assembler::StoreLoad));\n-  __ cmpb(Address(card_addr, 0), G1CardTable::dirty_card_val());\n-  __ jcc(Assembler::equal, done);\n+  generate_pre_barrier_fast_path(masm, thread);\n+  \/\/ If marking is active (*(mark queue active address) != 0), jump to stub (slow path)\n+  __ jcc(Assembler::notEqual, *stub->entry());\n@@ -316,0 +402,2 @@\n+  __ bind(*stub->continuation());\n+}\n@@ -317,2 +405,9 @@\n-  \/\/ storing a region crossing, non-null oop, card is clean.\n-  \/\/ dirty card and log.\n+void G1BarrierSetAssembler::generate_c2_pre_barrier_stub(MacroAssembler* masm,\n+                                                         G1PreBarrierStubC2* stub) const {\n+  Assembler::InlineSkippedInstructionsCounter skip_counter(masm);\n+  Label runtime;\n+  Register obj = stub->obj();\n+  Register pre_val = stub->pre_val();\n+  Register thread = stub->thread();\n+  Register tmp = stub->tmp1();\n+  assert(stub->tmp2() == noreg, \"not needed in this platform\");\n@@ -320,1 +415,2 @@\n-  __ movb(Address(card_addr, 0), G1CardTable::dirty_card_val());\n+  __ bind(*stub->entry());\n+  generate_pre_barrier_slow_path(masm, obj, pre_val, thread, tmp, *stub->continuation(), runtime);\n@@ -322,2 +418,4 @@\n-  \/\/ The code below assumes that buffer index is pointer sized.\n-  STATIC_ASSERT(in_bytes(G1DirtyCardQueue::byte_width_of_index()) == sizeof(intptr_t));\n+  __ bind(runtime);\n+  generate_c2_barrier_runtime_call(masm, stub, pre_val, CAST_FROM_FN_PTR(address, G1BarrierSetRuntime::write_ref_field_pre_entry));\n+  __ jmp(*stub->continuation());\n+}\n@@ -325,8 +423,10 @@\n-  __ movptr(tmp2, queue_index);\n-  __ testptr(tmp2, tmp2);\n-  __ jcc(Assembler::zero, runtime);\n-  __ subptr(tmp2, wordSize);\n-  __ movptr(queue_index, tmp2);\n-  __ addptr(tmp2, buffer);\n-  __ movptr(Address(tmp2, 0), card_addr);\n-  __ jmp(done);\n+void G1BarrierSetAssembler::g1_write_barrier_post_c2(MacroAssembler* masm,\n+                                                     Register store_addr,\n+                                                     Register new_val,\n+                                                     Register thread,\n+                                                     Register tmp,\n+                                                     Register tmp2,\n+                                                     G1PostBarrierStubC2* stub) {\n+#ifdef _LP64\n+  assert(thread == r15_thread, \"must be\");\n+#endif \/\/ _LP64\n@@ -334,6 +434,1 @@\n-  __ bind(runtime);\n-  \/\/ save the live input values\n-  RegSet saved = RegSet::of(store_addr NOT_LP64(COMMA thread));\n-  __ push_set(saved);\n-  __ call_VM_leaf(CAST_FROM_FN_PTR(address, G1BarrierSetRuntime::write_ref_field_post_entry), card_addr, thread);\n-  __ pop_set(saved);\n+  stub->initialize_registers(thread, tmp, tmp2);\n@@ -341,1 +436,6 @@\n-  __ bind(done);\n+  bool new_val_may_be_null = (stub->barrier_data() & G1C2BarrierPostNotNull) == 0;\n+  generate_post_barrier_fast_path(masm, store_addr, new_val, tmp, tmp2, *stub->continuation(), new_val_may_be_null);\n+  \/\/ If card is not young, jump to stub (slow path)\n+  __ jcc(Assembler::notEqual, *stub->entry());\n+\n+  __ bind(*stub->continuation());\n@@ -344,0 +444,19 @@\n+void G1BarrierSetAssembler::generate_c2_post_barrier_stub(MacroAssembler* masm,\n+                                                          G1PostBarrierStubC2* stub) const {\n+  Assembler::InlineSkippedInstructionsCounter skip_counter(masm);\n+  Label runtime;\n+  Register thread = stub->thread();\n+  Register tmp = stub->tmp1(); \/\/ tmp holds the card address.\n+  Register tmp2 = stub->tmp2();\n+  assert(stub->tmp3() == noreg, \"not needed in this platform\");\n+\n+  __ bind(*stub->entry());\n+  generate_post_barrier_slow_path(masm, thread, tmp, tmp2, *stub->continuation(), runtime);\n+\n+  __ bind(runtime);\n+  generate_c2_barrier_runtime_call(masm, stub, tmp, CAST_FROM_FN_PTR(address, G1BarrierSetRuntime::write_ref_field_post_entry));\n+  __ jmp(*stub->continuation());\n+}\n+\n+#endif \/\/ COMPILER2\n+\n","filename":"src\/hotspot\/cpu\/x86\/gc\/g1\/g1BarrierSetAssembler_x86.cpp","additions":204,"deletions":85,"binary":false,"changes":289,"status":"modified"},{"patch":"@@ -35,0 +35,3 @@\n+class G1BarrierStubC2;\n+class G1PreBarrierStubC2;\n+class G1PostBarrierStubC2;\n@@ -68,0 +71,20 @@\n+\n+#ifdef COMPILER2\n+  void g1_write_barrier_pre_c2(MacroAssembler* masm,\n+                               Register obj,\n+                               Register pre_val,\n+                               Register thread,\n+                               Register tmp,\n+                               G1PreBarrierStubC2* c2_stub);\n+  void generate_c2_pre_barrier_stub(MacroAssembler* masm,\n+                                    G1PreBarrierStubC2* stub) const;\n+  void g1_write_barrier_post_c2(MacroAssembler* masm,\n+                                Register store_addr,\n+                                Register new_val,\n+                                Register thread,\n+                                Register tmp,\n+                                Register tmp2,\n+                                G1PostBarrierStubC2* c2_stub);\n+  void generate_c2_post_barrier_stub(MacroAssembler* masm,\n+                                     G1PostBarrierStubC2* stub) const;\n+#endif \/\/ COMPILER2\n","filename":"src\/hotspot\/cpu\/x86\/gc\/g1\/g1BarrierSetAssembler_x86.hpp","additions":23,"deletions":0,"binary":false,"changes":23,"status":"modified"},{"patch":"@@ -0,0 +1,371 @@\n+\/\/\n+\/\/ Copyright (c) 2024, Oracle and\/or its affiliates. All rights reserved.\n+\/\/ DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+\/\/\n+\/\/ This code is free software; you can redistribute it and\/or modify it\n+\/\/ under the terms of the GNU General Public License version 2 only, as\n+\/\/ published by the Free Software Foundation.\n+\/\/\n+\/\/ This code is distributed in the hope that it will be useful, but WITHOUT\n+\/\/ ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+\/\/ FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+\/\/ version 2 for more details (a copy is included in the LICENSE file that\n+\/\/ accompanied this code).\n+\/\/\n+\/\/ You should have received a copy of the GNU General Public License version\n+\/\/ 2 along with this work; if not, write to the Free Software Foundation,\n+\/\/ Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+\/\/\n+\/\/ Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+\/\/ or visit www.oracle.com if you need additional information or have any\n+\/\/ questions.\n+\/\/\n+\n+source_hpp %{\n+\n+#include \"gc\/g1\/c2\/g1BarrierSetC2.hpp\"\n+#include \"gc\/shared\/gc_globals.hpp\"\n+\n+%}\n+\n+source %{\n+\n+#include \"gc\/g1\/g1BarrierSetAssembler_x86.hpp\"\n+#include \"gc\/g1\/g1BarrierSetRuntime.hpp\"\n+\n+static void write_barrier_pre(MacroAssembler* masm,\n+                              const MachNode* node,\n+                              Register obj,\n+                              Register pre_val,\n+                              Register tmp,\n+                              RegSet preserve = RegSet(),\n+                              RegSet no_preserve = RegSet()) {\n+  if (!G1PreBarrierStubC2::needs_barrier(node)) {\n+    return;\n+  }\n+  Assembler::InlineSkippedInstructionsCounter skip_counter(masm);\n+  G1BarrierSetAssembler* g1_asm = static_cast<G1BarrierSetAssembler*>(BarrierSet::barrier_set()->barrier_set_assembler());\n+  G1PreBarrierStubC2* const stub = G1PreBarrierStubC2::create(node);\n+  for (RegSetIterator<Register> reg = preserve.begin(); *reg != noreg; ++reg) {\n+    stub->preserve(*reg);\n+  }\n+  for (RegSetIterator<Register> reg = no_preserve.begin(); *reg != noreg; ++reg) {\n+    stub->dont_preserve(*reg);\n+  }\n+  g1_asm->g1_write_barrier_pre_c2(masm, obj, pre_val, r15_thread, tmp, stub);\n+}\n+\n+static void write_barrier_post(MacroAssembler* masm,\n+                               const MachNode* node,\n+                               Register store_addr,\n+                               Register new_val,\n+                               Register tmp1,\n+                               Register tmp2) {\n+  if (!G1PostBarrierStubC2::needs_barrier(node)) {\n+    return;\n+  }\n+  Assembler::InlineSkippedInstructionsCounter skip_counter(masm);\n+  G1BarrierSetAssembler* g1_asm = static_cast<G1BarrierSetAssembler*>(BarrierSet::barrier_set()->barrier_set_assembler());\n+  G1PostBarrierStubC2* const stub = G1PostBarrierStubC2::create(node);\n+  g1_asm->g1_write_barrier_post_c2(masm, store_addr, new_val, r15_thread, tmp1, tmp2, stub);\n+}\n+\n+%}\n+\n+instruct g1StoreP(memory mem, any_RegP src, rRegP tmp1, rRegP tmp2, rRegP tmp3, rFlagsReg cr)\n+%{\n+  predicate(UseG1GC && n->as_Store()->barrier_data() != 0);\n+  match(Set mem (StoreP mem src));\n+  effect(TEMP tmp1, TEMP tmp2, TEMP tmp3, KILL cr);\n+  ins_cost(125); \/\/ XXX\n+  format %{ \"movq    $mem, $src\\t# ptr\" %}\n+  ins_encode %{\n+    \/\/ Materialize the store address internally (as opposed to defining 'mem' as\n+    \/\/ an indirect memory operand) to reduce the overhead of LCM when processing\n+    \/\/ large basic blocks with many stores. Such basic blocks arise, for\n+    \/\/ instance, from static initializations of large String arrays.\n+    \/\/ The same holds for g1StoreN and g1EncodePAndStoreN.\n+    __ lea($tmp1$$Register, $mem$$Address);\n+    write_barrier_pre(masm, this,\n+                      $tmp1$$Register \/* obj *\/,\n+                      $tmp2$$Register \/* pre_val *\/,\n+                      $tmp3$$Register \/* tmp *\/,\n+                      RegSet::of($tmp1$$Register, $src$$Register) \/* preserve *\/);\n+    __ movq(Address($tmp1$$Register, 0), $src$$Register);\n+    write_barrier_post(masm, this,\n+                       $tmp1$$Register \/* store_addr *\/,\n+                       $src$$Register \/* new_val *\/,\n+                       $tmp3$$Register \/* tmp1 *\/,\n+                       $tmp2$$Register \/* tmp2 *\/);\n+  %}\n+  ins_pipe(ialu_mem_reg);\n+%}\n+\n+instruct g1StoreN(memory mem, rRegN src, rRegP tmp1, rRegP tmp2, rRegP tmp3, rFlagsReg cr)\n+%{\n+  predicate(UseG1GC && n->as_Store()->barrier_data() != 0);\n+  match(Set mem (StoreN mem src));\n+  effect(TEMP tmp1, TEMP tmp2, TEMP tmp3, KILL cr);\n+  ins_cost(125); \/\/ XXX\n+  format %{ \"movl    $mem, $src\\t# ptr\" %}\n+  ins_encode %{\n+    __ lea($tmp1$$Register, $mem$$Address);\n+    write_barrier_pre(masm, this,\n+                      $tmp1$$Register \/* obj *\/,\n+                      $tmp2$$Register \/* pre_val *\/,\n+                      $tmp3$$Register \/* tmp *\/,\n+                      RegSet::of($tmp1$$Register, $src$$Register) \/* preserve *\/);\n+    __ movl(Address($tmp1$$Register, 0), $src$$Register);\n+    if ((barrier_data() & G1C2BarrierPost) != 0) {\n+      __ movl($tmp2$$Register, $src$$Register);\n+      if ((barrier_data() & G1C2BarrierPostNotNull) == 0) {\n+        __ decode_heap_oop($tmp2$$Register);\n+      } else {\n+        __ decode_heap_oop_not_null($tmp2$$Register);\n+      }\n+    }\n+    write_barrier_post(masm, this,\n+                       $tmp1$$Register \/* store_addr *\/,\n+                       $tmp2$$Register \/* new_val *\/,\n+                       $tmp3$$Register \/* tmp1 *\/,\n+                       $tmp2$$Register \/* tmp2 *\/);\n+  %}\n+  ins_pipe(ialu_mem_reg);\n+%}\n+\n+instruct g1EncodePAndStoreN(memory mem, any_RegP src, rRegP tmp1, rRegP tmp2, rRegP tmp3, rFlagsReg cr)\n+%{\n+  predicate(UseG1GC && n->as_Store()->barrier_data() != 0);\n+  match(Set mem (StoreN mem (EncodeP src)));\n+  effect(TEMP tmp1, TEMP tmp2, TEMP tmp3, KILL cr);\n+  ins_cost(125); \/\/ XXX\n+  format %{ \"encode_heap_oop $src\\n\\t\"\n+            \"movl   $mem, $src\\t# ptr\" %}\n+  ins_encode %{\n+    __ lea($tmp1$$Register, $mem$$Address);\n+    write_barrier_pre(masm, this,\n+                      $tmp1$$Register \/* obj *\/,\n+                      $tmp2$$Register \/* pre_val *\/,\n+                      $tmp3$$Register \/* tmp *\/,\n+                      RegSet::of($tmp1$$Register, $src$$Register) \/* preserve *\/);\n+    __ movq($tmp2$$Register, $src$$Register);\n+    if ((barrier_data() & G1C2BarrierPostNotNull) == 0) {\n+      __ encode_heap_oop($tmp2$$Register);\n+    } else {\n+      __ encode_heap_oop_not_null($tmp2$$Register);\n+    }\n+    __ movl(Address($tmp1$$Register, 0), $tmp2$$Register);\n+    write_barrier_post(masm, this,\n+                       $tmp1$$Register \/* store_addr *\/,\n+                       $src$$Register \/* new_val *\/,\n+                       $tmp3$$Register \/* tmp1 *\/,\n+                       $tmp2$$Register \/* tmp2 *\/);\n+  %}\n+  ins_pipe(ialu_mem_reg);\n+%}\n+\n+instruct g1CompareAndExchangeP(indirect mem, rRegP newval, rRegP tmp1, rRegP tmp2, rRegP tmp3, rax_RegP oldval, rFlagsReg cr)\n+%{\n+  predicate(UseG1GC && n->as_LoadStore()->barrier_data() != 0);\n+  match(Set oldval (CompareAndExchangeP mem (Binary oldval newval)));\n+  effect(TEMP tmp1, TEMP tmp2, TEMP tmp3, KILL cr);\n+  format %{ \"lock\\n\\t\"\n+            \"cmpxchgq $newval, $mem\" %}\n+  ins_encode %{\n+    assert_different_registers($oldval$$Register, $mem$$Register);\n+    \/\/ Pass $oldval to the pre-barrier (instead of loading from $mem), because\n+    \/\/ $oldval is the only value that can be overwritten.\n+    \/\/ The same holds for g1CompareAndSwapP.\n+    write_barrier_pre(masm, this,\n+                      noreg \/* obj *\/,\n+                      $oldval$$Register \/* pre_val *\/,\n+                      $tmp3$$Register \/* tmp *\/,\n+                      RegSet::of($mem$$Register, $newval$$Register, $oldval$$Register) \/* preserve *\/);\n+    __ movq($tmp1$$Register, $newval$$Register);\n+    __ lock();\n+    __ cmpxchgq($tmp1$$Register, Address($mem$$Register, 0));\n+    write_barrier_post(masm, this,\n+                       $mem$$Register \/* store_addr *\/,\n+                       $tmp1$$Register \/* new_val *\/,\n+                       $tmp2$$Register \/* tmp1 *\/,\n+                       $tmp3$$Register \/* tmp2 *\/);\n+  %}\n+  ins_pipe(pipe_cmpxchg);\n+%}\n+\n+instruct g1CompareAndExchangeN(indirect mem, rRegN newval, rRegP tmp1, rRegP tmp2, rRegP tmp3, rax_RegN oldval, rFlagsReg cr)\n+%{\n+  predicate(UseG1GC && n->as_LoadStore()->barrier_data() != 0);\n+  match(Set oldval (CompareAndExchangeN mem (Binary oldval newval)));\n+  effect(TEMP tmp1, TEMP tmp2, TEMP tmp3, KILL cr);\n+  format %{ \"lock\\n\\t\"\n+            \"cmpxchgq $newval, $mem\" %}\n+  ins_encode %{\n+    assert_different_registers($oldval$$Register, $mem$$Register);\n+    write_barrier_pre(masm, this,\n+                      $mem$$Register \/* obj *\/,\n+                      $tmp2$$Register \/* pre_val *\/,\n+                      $tmp3$$Register \/* tmp *\/,\n+                      RegSet::of($mem$$Register, $newval$$Register, $oldval$$Register) \/* preserve *\/);\n+    __ movl($tmp1$$Register, $newval$$Register);\n+    __ lock();\n+    __ cmpxchgl($tmp1$$Register, Address($mem$$Register, 0));\n+    __ decode_heap_oop($tmp1$$Register);\n+    write_barrier_post(masm, this,\n+                       $mem$$Register \/* store_addr *\/,\n+                       $tmp1$$Register \/* new_val *\/,\n+                       $tmp2$$Register \/* tmp1 *\/,\n+                       $tmp3$$Register \/* tmp2 *\/);\n+  %}\n+  ins_pipe(pipe_cmpxchg);\n+%}\n+\n+instruct g1CompareAndSwapP(rRegI res, indirect mem, rRegP newval, rRegP tmp1, rRegP tmp2, rRegP tmp3, rax_RegP oldval, rFlagsReg cr)\n+%{\n+  predicate(UseG1GC && n->as_LoadStore()->barrier_data() != 0);\n+  match(Set res (CompareAndSwapP mem (Binary oldval newval)));\n+  match(Set res (WeakCompareAndSwapP mem (Binary oldval newval)));\n+  effect(TEMP res, TEMP tmp1, TEMP tmp2, TEMP tmp3, KILL oldval, KILL cr);\n+  format %{ \"lock\\n\\t\"\n+            \"cmpxchgq $newval, $mem\\n\\t\"\n+            \"sete     $res\\n\\t\"\n+            \"movzbl   $res, $res\" %}\n+  ins_encode %{\n+    assert_different_registers($oldval$$Register, $mem$$Register);\n+    write_barrier_pre(masm, this,\n+                      noreg \/* obj *\/,\n+                      $oldval$$Register \/* pre_val *\/,\n+                      $tmp3$$Register \/* tmp *\/,\n+                      RegSet::of($mem$$Register, $newval$$Register, $oldval$$Register) \/* preserve *\/,\n+                      RegSet::of($res$$Register) \/* no_preserve *\/);\n+    __ movq($tmp1$$Register, $newval$$Register);\n+    __ lock();\n+    __ cmpxchgq($tmp1$$Register, Address($mem$$Register, 0));\n+    __ setb(Assembler::equal, $res$$Register);\n+    __ movzbl($res$$Register, $res$$Register);\n+    write_barrier_post(masm, this,\n+                       $mem$$Register \/* store_addr *\/,\n+                       $tmp1$$Register \/* new_val *\/,\n+                       $tmp2$$Register \/* tmp1 *\/,\n+                       $tmp3$$Register \/* tmp2 *\/);\n+  %}\n+  ins_pipe(pipe_cmpxchg);\n+%}\n+\n+instruct g1CompareAndSwapN(rRegI res, indirect mem, rRegN newval, rRegP tmp1, rRegP tmp2, rRegP tmp3, rax_RegN oldval, rFlagsReg cr)\n+%{\n+  predicate(UseG1GC && n->as_LoadStore()->barrier_data() != 0);\n+  match(Set res (CompareAndSwapN mem (Binary oldval newval)));\n+  match(Set res (WeakCompareAndSwapN mem (Binary oldval newval)));\n+  effect(TEMP res, TEMP tmp1, TEMP tmp2, TEMP tmp3, KILL oldval, KILL cr);\n+  format %{ \"lock\\n\\t\"\n+            \"cmpxchgq $newval, $mem\\n\\t\"\n+            \"sete     $res\\n\\t\"\n+            \"movzbl   $res, $res\" %}\n+  ins_encode %{\n+    assert_different_registers($oldval$$Register, $mem$$Register);\n+    write_barrier_pre(masm, this,\n+                      $mem$$Register \/* obj *\/,\n+                      $tmp2$$Register \/* pre_val *\/,\n+                      $tmp3$$Register \/* tmp *\/,\n+                      RegSet::of($mem$$Register, $newval$$Register, $oldval$$Register) \/* preserve *\/,\n+                      RegSet::of($res$$Register) \/* no_preserve *\/);\n+    __ movl($tmp1$$Register, $newval$$Register);\n+    __ lock();\n+    __ cmpxchgl($tmp1$$Register, Address($mem$$Register, 0));\n+    __ setb(Assembler::equal, $res$$Register);\n+    __ movzbl($res$$Register, $res$$Register);\n+    __ decode_heap_oop($tmp1$$Register);\n+    write_barrier_post(masm, this,\n+                       $mem$$Register \/* store_addr *\/,\n+                       $tmp1$$Register \/* new_val *\/,\n+                       $tmp2$$Register \/* tmp1 *\/,\n+                       $tmp3$$Register \/* tmp2 *\/);\n+  %}\n+  ins_pipe(pipe_cmpxchg);\n+%}\n+\n+instruct g1GetAndSetP(indirect mem, rRegP newval, rRegP tmp1, rRegP tmp2, rRegP tmp3, rFlagsReg cr)\n+%{\n+  predicate(UseG1GC && n->as_LoadStore()->barrier_data() != 0);\n+  match(Set newval (GetAndSetP mem newval));\n+  effect(TEMP tmp1, TEMP tmp2, TEMP tmp3, KILL cr);\n+  format %{ \"xchgq    $newval, $mem\" %}\n+  ins_encode %{\n+    assert_different_registers($mem$$Register, $newval$$Register);\n+    write_barrier_pre(masm, this,\n+                      $mem$$Register \/* obj *\/,\n+                      $tmp2$$Register \/* pre_val *\/,\n+                      $tmp3$$Register \/* tmp *\/,\n+                      RegSet::of($mem$$Register, $newval$$Register) \/* preserve *\/);\n+    __ movq($tmp1$$Register, $newval$$Register);\n+    __ xchgq($newval$$Register, Address($mem$$Register, 0));\n+    write_barrier_post(masm, this,\n+                       $mem$$Register \/* store_addr *\/,\n+                       $tmp1$$Register \/* new_val *\/,\n+                       $tmp2$$Register \/* tmp1 *\/,\n+                       $tmp3$$Register \/* tmp2 *\/);\n+  %}\n+  ins_pipe(pipe_cmpxchg);\n+%}\n+\n+instruct g1GetAndSetN(indirect mem, rRegN newval, rRegP tmp1, rRegP tmp2, rRegP tmp3, rFlagsReg cr)\n+%{\n+  predicate(UseG1GC && n->as_LoadStore()->barrier_data() != 0);\n+  match(Set newval (GetAndSetN mem newval));\n+  effect(TEMP tmp1, TEMP tmp2, TEMP tmp3, KILL cr);\n+  format %{ \"xchgq    $newval, $mem\" %}\n+  ins_encode %{\n+    assert_different_registers($mem$$Register, $newval$$Register);\n+    write_barrier_pre(masm, this,\n+                      $mem$$Register \/* obj *\/,\n+                      $tmp2$$Register \/* pre_val *\/,\n+                      $tmp3$$Register \/* tmp *\/,\n+                      RegSet::of($mem$$Register, $newval$$Register) \/* preserve *\/);\n+    __ movl($tmp1$$Register, $newval$$Register);\n+    __ decode_heap_oop($tmp1$$Register);\n+    __ xchgl($newval$$Register, Address($mem$$Register, 0));\n+    write_barrier_post(masm, this,\n+                       $mem$$Register \/* store_addr *\/,\n+                       $tmp1$$Register \/* new_val *\/,\n+                       $tmp2$$Register \/* tmp1 *\/,\n+                       $tmp3$$Register \/* tmp2 *\/);\n+  %}\n+  ins_pipe(pipe_cmpxchg);\n+%}\n+\n+instruct g1LoadP(rRegP dst, memory mem, rRegP tmp, rFlagsReg cr)\n+%{\n+  predicate(UseG1GC && n->as_Load()->barrier_data() != 0);\n+  match(Set dst (LoadP mem));\n+  effect(TEMP dst, TEMP tmp, KILL cr);\n+  ins_cost(125); \/\/ XXX\n+  format %{ \"movq    $dst, $mem\\t# ptr\" %}\n+  ins_encode %{\n+    __ movq($dst$$Register, $mem$$Address);\n+    write_barrier_pre(masm, this,\n+                      noreg \/* obj *\/,\n+                      $dst$$Register \/* pre_val *\/,\n+                      $tmp$$Register \/* tmp *\/);\n+  %}\n+  ins_pipe(ialu_reg_mem); \/\/ XXX\n+%}\n+\n+instruct g1LoadN(rRegN dst, memory mem, rRegP tmp1, rRegP tmp2, rFlagsReg cr)\n+%{\n+  predicate(UseG1GC && n->as_Load()->barrier_data() != 0);\n+  match(Set dst (LoadN mem));\n+  effect(TEMP dst, TEMP tmp1, TEMP tmp2, KILL cr);\n+  ins_cost(125); \/\/ XXX\n+  format %{ \"movl    $dst, $mem\\t# compressed ptr\" %}\n+  ins_encode %{\n+    __ movl($dst$$Register, $mem$$Address);\n+    __ movl($tmp1$$Register, $dst$$Register);\n+    __ decode_heap_oop($tmp1$$Register);\n+    write_barrier_pre(masm, this,\n+                      noreg \/* obj *\/,\n+                      $tmp1$$Register \/* pre_val *\/,\n+                      $tmp2$$Register \/* tmp *\/);\n+  %}\n+  ins_pipe(ialu_reg_mem); \/\/ XXX\n+%}\n","filename":"src\/hotspot\/cpu\/x86\/gc\/g1\/g1_x86_64.ad","additions":371,"deletions":0,"binary":false,"changes":371,"status":"added"},{"patch":"@@ -2460,0 +2460,4 @@\n+  if (is_encode_and_store_pattern(n, m)) {\n+    mstack.push(m, Visit);\n+    return true;\n+  }\n","filename":"src\/hotspot\/cpu\/x86\/x86.ad","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -4344,0 +4344,1 @@\n+   predicate(n->as_Load()->barrier_data() == 0);\n@@ -5129,0 +5130,1 @@\n+  predicate(n->as_Store()->barrier_data() == 0);\n@@ -5153,1 +5155,1 @@\n-  predicate(CompressedOops::base() == nullptr);\n+  predicate(CompressedOops::base() == nullptr && n->as_Store()->barrier_data() == 0);\n@@ -5166,0 +5168,1 @@\n+  predicate(n->as_Store()->barrier_data() == 0);\n@@ -7165,0 +7168,1 @@\n+  predicate(n->as_LoadStore()->barrier_data() == 0);\n@@ -7252,0 +7256,1 @@\n+  predicate(n->as_LoadStore()->barrier_data() == 0);\n@@ -7473,0 +7478,1 @@\n+  predicate(n->as_LoadStore()->barrier_data() == 0);\n@@ -11662,0 +11668,1 @@\n+  predicate(n->in(2)->as_Load()->barrier_data() == 0);\n@@ -11683,0 +11690,1 @@\n+  predicate(n->in(2)->as_Load()->barrier_data() == 0);\n@@ -11723,1 +11731,2 @@\n-  predicate(CompressedOops::base() != nullptr);\n+  predicate(CompressedOops::base() != nullptr &&\n+            n->in(1)->as_Load()->barrier_data() == 0);\n@@ -11736,1 +11745,2 @@\n-  predicate(CompressedOops::base() == nullptr);\n+  predicate(CompressedOops::base() == nullptr &&\n+            n->in(1)->as_Load()->barrier_data() == 0);\n","filename":"src\/hotspot\/cpu\/x86\/x86_64.ad","additions":13,"deletions":3,"binary":false,"changes":16,"status":"modified"},{"patch":"@@ -27,0 +27,1 @@\n+#include \"code\/vmreg.inline.hpp\"\n@@ -29,0 +30,1 @@\n+#include \"gc\/g1\/g1BarrierSetAssembler.hpp\"\n@@ -34,0 +36,1 @@\n+#include \"opto\/block.hpp\"\n@@ -38,0 +41,1 @@\n+#include \"opto\/machnode.hpp\"\n@@ -39,0 +43,4 @@\n+#include \"opto\/memnode.hpp\"\n+#include \"opto\/node.hpp\"\n+#include \"opto\/output.hpp\"\n+#include \"opto\/regalloc.hpp\"\n@@ -40,0 +48,1 @@\n+#include \"opto\/runtime.hpp\"\n@@ -41,0 +50,1 @@\n+#include \"utilities\/growableArray.hpp\"\n@@ -43,27 +53,0 @@\n-const TypeFunc *G1BarrierSetC2::write_ref_field_pre_entry_Type() {\n-  const Type **fields = TypeTuple::fields(2);\n-  fields[TypeFunc::Parms+0] = TypeInstPtr::NOTNULL; \/\/ original field value\n-  fields[TypeFunc::Parms+1] = TypeRawPtr::NOTNULL; \/\/ thread\n-  const TypeTuple *domain = TypeTuple::make(TypeFunc::Parms+2, fields);\n-\n-  \/\/ create result type (range)\n-  fields = TypeTuple::fields(0);\n-  const TypeTuple *range = TypeTuple::make(TypeFunc::Parms+0, fields);\n-\n-  return TypeFunc::make(domain, range);\n-}\n-\n-const TypeFunc *G1BarrierSetC2::write_ref_field_post_entry_Type() {\n-  const Type **fields = TypeTuple::fields(2);\n-  fields[TypeFunc::Parms+0] = TypeRawPtr::NOTNULL;  \/\/ Card addr\n-  fields[TypeFunc::Parms+1] = TypeRawPtr::NOTNULL;  \/\/ thread\n-  const TypeTuple *domain = TypeTuple::make(TypeFunc::Parms+2, fields);\n-\n-  \/\/ create result type (range)\n-  fields = TypeTuple::fields(0);\n-  const TypeTuple *range = TypeTuple::make(TypeFunc::Parms, fields);\n-\n-  return TypeFunc::make(domain, range);\n-}\n-\n-#define __ ideal.\n@@ -87,2 +70,0 @@\n- *\n- * Returns true if the pre-barrier can be removed\n@@ -100,1 +81,1 @@\n-    return false; \/\/ cannot unalias unless there are precise offsets\n+    return false; \/\/ Cannot unalias unless there are precise offsets.\n@@ -102,1 +83,0 @@\n-\n@@ -104,1 +84,1 @@\n-    return false; \/\/ No allocation found\n+    return false; \/\/ No allocation found.\n@@ -108,2 +88,1 @@\n-\n-  Node* mem = kit->memory(adr_idx); \/\/ start searching here...\n+  Node* mem = kit->memory(adr_idx); \/\/ Start searching here.\n@@ -112,1 +91,0 @@\n-\n@@ -114,1 +92,0 @@\n-\n@@ -120,1 +97,1 @@\n-        break; \/\/ inscrutable pointer\n+        break; \/\/ Inscrutable pointer.\n@@ -122,2 +99,0 @@\n-\n-      \/\/ Break we have found a store with same base and offset as ours so break\n@@ -125,0 +100,1 @@\n+        \/\/ We have found a store with same base and offset as ours.\n@@ -127,1 +103,0 @@\n-\n@@ -139,1 +114,1 @@\n-          continue; \/\/ advance through independent store memory\n+          continue; \/\/ Advance through independent store memory.\n@@ -142,1 +117,0 @@\n-\n@@ -147,1 +121,1 @@\n-        \/\/ Success:  The bases are provably independent.\n+        \/\/ Success: the bases are provably independent.\n@@ -149,1 +123,1 @@\n-        continue; \/\/ advance through independent store memory\n+        continue; \/\/ Advance through independent store memory.\n@@ -152,1 +126,0 @@\n-\n@@ -160,1 +133,1 @@\n-        \/\/ has been moved up and directly write a reference\n+        \/\/ has been moved up and directly write a reference.\n@@ -169,1 +142,0 @@\n-\n@@ -174,1 +146,0 @@\n-\n@@ -178,102 +149,0 @@\n-\/\/ G1 pre\/post barriers\n-void G1BarrierSetC2::pre_barrier(GraphKit* kit,\n-                                 bool do_load,\n-                                 Node* ctl,\n-                                 Node* obj,\n-                                 Node* adr,\n-                                 uint alias_idx,\n-                                 Node* val,\n-                                 const TypeOopPtr* val_type,\n-                                 Node* pre_val,\n-                                 BasicType bt) const {\n-  \/\/ Some sanity checks\n-  \/\/ Note: val is unused in this routine.\n-\n-  if (do_load) {\n-    \/\/ We need to generate the load of the previous value\n-    assert(obj != nullptr, \"must have a base\");\n-    assert(adr != nullptr, \"where are loading from?\");\n-    assert(pre_val == nullptr, \"loaded already?\");\n-    assert(val_type != nullptr, \"need a type\");\n-\n-    if (use_ReduceInitialCardMarks()\n-        && g1_can_remove_pre_barrier(kit, &kit->gvn(), adr, bt, alias_idx)) {\n-      return;\n-    }\n-\n-  } else {\n-    \/\/ In this case both val_type and alias_idx are unused.\n-    assert(pre_val != nullptr, \"must be loaded already\");\n-    \/\/ Nothing to be done if pre_val is null.\n-    if (pre_val->bottom_type() == TypePtr::NULL_PTR) return;\n-    assert(pre_val->bottom_type()->basic_type() == T_OBJECT, \"or we shouldn't be here\");\n-  }\n-  assert(bt == T_OBJECT, \"or we shouldn't be here\");\n-\n-  IdealKit ideal(kit, true);\n-\n-  Node* tls = __ thread(); \/\/ ThreadLocalStorage\n-\n-  Node* no_base = __ top();\n-  Node* zero  = __ ConI(0);\n-  Node* zeroX = __ ConX(0);\n-\n-  float likely  = PROB_LIKELY(0.999);\n-  float unlikely  = PROB_UNLIKELY(0.999);\n-\n-  BasicType active_type = in_bytes(SATBMarkQueue::byte_width_of_active()) == 4 ? T_INT : T_BYTE;\n-  assert(in_bytes(SATBMarkQueue::byte_width_of_active()) == 4 || in_bytes(SATBMarkQueue::byte_width_of_active()) == 1, \"flag width\");\n-\n-  \/\/ Offsets into the thread\n-  const int marking_offset = in_bytes(G1ThreadLocalData::satb_mark_queue_active_offset());\n-  const int index_offset   = in_bytes(G1ThreadLocalData::satb_mark_queue_index_offset());\n-  const int buffer_offset  = in_bytes(G1ThreadLocalData::satb_mark_queue_buffer_offset());\n-\n-  \/\/ Now the actual pointers into the thread\n-  Node* marking_adr = __ AddP(no_base, tls, __ ConX(marking_offset));\n-  Node* buffer_adr  = __ AddP(no_base, tls, __ ConX(buffer_offset));\n-  Node* index_adr   = __ AddP(no_base, tls, __ ConX(index_offset));\n-\n-  \/\/ Now some of the values\n-  Node* marking = __ load(__ ctrl(), marking_adr, TypeInt::INT, active_type, Compile::AliasIdxRaw);\n-\n-  \/\/ if (!marking)\n-  __ if_then(marking, BoolTest::ne, zero, unlikely); {\n-    BasicType index_bt = TypeX_X->basic_type();\n-    assert(sizeof(size_t) == type2aelembytes(index_bt), \"Loading G1 SATBMarkQueue::_index with wrong size.\");\n-    Node* index   = __ load(__ ctrl(), index_adr, TypeX_X, index_bt, Compile::AliasIdxRaw);\n-\n-    if (do_load) {\n-      \/\/ load original value\n-      pre_val = __ load(__ ctrl(), adr, val_type, bt, alias_idx, false, MemNode::unordered, LoadNode::Pinned);\n-    }\n-\n-    \/\/ if (pre_val != nullptr)\n-    __ if_then(pre_val, BoolTest::ne, kit->null()); {\n-      Node* buffer  = __ load(__ ctrl(), buffer_adr, TypeRawPtr::NOTNULL, T_ADDRESS, Compile::AliasIdxRaw);\n-\n-      \/\/ is the queue for this thread full?\n-      __ if_then(index, BoolTest::ne, zeroX, likely); {\n-\n-        \/\/ decrement the index\n-        Node* next_index = kit->gvn().transform(new SubXNode(index, __ ConX(sizeof(intptr_t))));\n-\n-        \/\/ Now get the buffer location we will log the previous value into and store it\n-        Node *log_addr = __ AddP(no_base, buffer, next_index);\n-        __ store(__ ctrl(), log_addr, pre_val, T_OBJECT, Compile::AliasIdxRaw, MemNode::unordered);\n-        \/\/ update the index\n-        __ store(__ ctrl(), index_adr, next_index, index_bt, Compile::AliasIdxRaw, MemNode::unordered);\n-\n-      } __ else_(); {\n-\n-        \/\/ logging buffer is full, call the runtime\n-        const TypeFunc *tf = write_ref_field_pre_entry_Type();\n-        __ make_leaf_call(tf, CAST_FROM_FN_PTR(address, G1BarrierSetRuntime::write_ref_field_pre_entry), \"write_ref_field_pre_entry\", pre_val, tls);\n-      } __ end_if();  \/\/ (!index)\n-    } __ end_if();  \/\/ (pre_val != nullptr)\n-  } __ end_if();  \/\/ (!marking)\n-\n-  \/\/ Final sync IdealKit and GraphKit.\n-  kit->final_sync(ideal);\n-}\n-\n@@ -281,2 +150,2 @@\n- * G1 similar to any GC with a Young Generation requires a way to keep track of\n- * references from Old Generation to Young Generation to make sure all live\n+ * G1, similar to any GC with a Young Generation, requires a way to keep track\n+ * of references from Old Generation to Young Generation to make sure all live\n@@ -285,3 +154,3 @@\n- * as part of mixed collections. References are tracked in remembered sets and\n- * is continuously updated as reference are written to with the help of the\n- * post-barrier.\n+ * as part of mixed collections. References are tracked in remembered sets,\n+ * which are continuously updated as references are written to with the help of\n+ * the post-barrier.\n@@ -289,4 +158,4 @@\n- * To reduce the number of updates to the remembered set the post-barrier\n- * filters updates to fields in objects located in the Young Generation,\n- * the same region as the reference, when the null is being written or\n- * if the card is already marked as dirty by an earlier write.\n+ * To reduce the number of updates to the remembered set, the post-barrier\n+ * filters out updates to fields in objects located in the Young Generation, the\n+ * same region as the reference, when null is being written, or if the card is\n+ * already marked as dirty by an earlier write.\n@@ -295,7 +164,4 @@\n- * post-barrier completely if it is possible during compile time to prove\n- * the object is newly allocated and that no safepoint exists between the\n- * allocation and the store.\n- *\n- * In the case of slow allocation the allocation code must handle the barrier\n- * as part of the allocation in the case the allocated object is not located\n- * in the nursery; this would happen for humongous objects.\n+ * post-barrier completely, if it is possible during compile time to prove the\n+ * object is newly allocated and that no safepoint exists between the allocation\n+ * and the store. This can be seen as a compile-time version of the\n+ * above-mentioned Young Generation filter.\n@@ -303,1 +169,3 @@\n- * Returns true if the post barrier can be removed\n+ * In the case of a slow allocation, the allocation code must handle the barrier\n+ * as part of the allocation if the allocated object is not located in the\n+ * nursery; this would happen for humongous objects.\n@@ -306,1 +174,1 @@\n-                                                PhaseValues* phase, Node* store,\n+                                                PhaseValues* phase, Node* store_ctrl,\n@@ -313,1 +181,1 @@\n-    return false; \/\/ cannot unalias unless there are precise offsets\n+    return false; \/\/ Cannot unalias unless there are precise offsets.\n@@ -315,1 +183,0 @@\n-\n@@ -317,1 +184,1 @@\n-     return false; \/\/ No allocation found\n+    return false; \/\/ No allocation found.\n@@ -320,2 +187,1 @@\n-  \/\/ Start search from Store node\n-  Node* mem = store->in(MemNode::Control);\n+  Node* mem = store_ctrl;   \/\/ Start search from Store node.\n@@ -323,1 +189,0 @@\n-\n@@ -326,1 +191,0 @@\n-\n@@ -336,33 +200,14 @@\n-\/\/\n-\/\/ Update the card table and add card address to the queue\n-\/\/\n-void G1BarrierSetC2::g1_mark_card(GraphKit* kit,\n-                                  IdealKit& ideal,\n-                                  Node* card_adr,\n-                                  Node* oop_store,\n-                                  uint oop_alias_idx,\n-                                  Node* index,\n-                                  Node* index_adr,\n-                                  Node* buffer,\n-                                  const TypeFunc* tf) const {\n-  Node* zero  = __ ConI(0);\n-  Node* zeroX = __ ConX(0);\n-  Node* no_base = __ top();\n-  BasicType card_bt = T_BYTE;\n-  \/\/ Smash zero into card. MUST BE ORDERED WRT TO STORE\n-  __ storeCM(__ ctrl(), card_adr, zero, oop_store, oop_alias_idx, card_bt, Compile::AliasIdxRaw);\n-\n-  \/\/  Now do the queue work\n-  __ if_then(index, BoolTest::ne, zeroX); {\n-\n-    Node* next_index = kit->gvn().transform(new SubXNode(index, __ ConX(sizeof(intptr_t))));\n-    Node* log_addr = __ AddP(no_base, buffer, next_index);\n-\n-    \/\/ Order, see storeCM.\n-    __ store(__ ctrl(), log_addr, card_adr, T_ADDRESS, Compile::AliasIdxRaw, MemNode::unordered);\n-    __ store(__ ctrl(), index_adr, next_index, TypeX_X->basic_type(), Compile::AliasIdxRaw, MemNode::unordered);\n-\n-  } __ else_(); {\n-    __ make_leaf_call(tf, CAST_FROM_FN_PTR(address, G1BarrierSetRuntime::write_ref_field_post_entry), \"write_ref_field_post_entry\", card_adr, __ thread());\n-  } __ end_if();\n-\n+Node* G1BarrierSetC2::load_at_resolved(C2Access& access, const Type* val_type) const {\n+  DecoratorSet decorators = access.decorators();\n+  bool on_weak = (decorators & ON_WEAK_OOP_REF) != 0;\n+  bool on_phantom = (decorators & ON_PHANTOM_OOP_REF) != 0;\n+  bool no_keepalive = (decorators & AS_NO_KEEPALIVE) != 0;\n+  \/\/ If we are reading the value of the referent field of a Reference object, we\n+  \/\/ need to record the referent in an SATB log buffer using the pre-barrier\n+  \/\/ mechanism. Also we need to add a memory barrier to prevent commoning reads\n+  \/\/ from this field across safepoints, since GC can change its value.\n+  bool need_read_barrier = ((on_weak || on_phantom) && !no_keepalive);\n+  if (access.is_oop() && need_read_barrier) {\n+    access.set_barrier_data(G1C2BarrierPre);\n+  }\n+  return CardTableBarrierSetC2::load_at_resolved(access, val_type);\n@@ -371,10 +216,3 @@\n-void G1BarrierSetC2::post_barrier(GraphKit* kit,\n-                                  Node* ctl,\n-                                  Node* oop_store,\n-                                  Node* obj,\n-                                  Node* adr,\n-                                  uint alias_idx,\n-                                  Node* val,\n-                                  BasicType bt,\n-                                  bool use_precise) const {\n-  \/\/ If we are writing a null then we need no post barrier\n+void G1BarrierSetC2::eliminate_gc_barrier(PhaseMacroExpand* macro, Node* node) const {\n+  eliminate_gc_barrier_data(node);\n+}\n@@ -382,6 +220,7 @@\n-  if (val != nullptr && val->is_Con() && val->bottom_type() == TypePtr::NULL_PTR) {\n-    \/\/ Must be null\n-    const Type* t = val->bottom_type();\n-    assert(t == Type::TOP || t == TypePtr::NULL_PTR, \"must be null\");\n-    \/\/ No post barrier if writing null\n-    return;\n+void G1BarrierSetC2::eliminate_gc_barrier_data(Node* node) const {\n+  if (node->is_LoadStore()) {\n+    LoadStoreNode* loadstore = node->as_LoadStore();\n+    loadstore->set_barrier_data(0);\n+  } else if (node->is_Mem()) {\n+    MemNode* mem = node->as_Mem();\n+    mem->set_barrier_data(0);\n@@ -389,0 +228,1 @@\n+}\n@@ -390,6 +230,3 @@\n-  if (use_ReduceInitialCardMarks() && obj == kit->just_allocated_object(kit->control())) {\n-    \/\/ We can skip marks on a freshly-allocated object in Eden.\n-    \/\/ Keep this code in sync with CardTableBarrierSet::on_slowpath_allocation_exit.\n-    \/\/ That routine informs GC to take appropriate compensating steps,\n-    \/\/ upon a slow-path allocation, so as to make this card-mark\n-    \/\/ elision safe.\n+static void refine_barrier_by_new_val_type(const Node* n) {\n+  if (n->Opcode() != Op_StoreP &&\n+      n->Opcode() != Op_StoreN) {\n@@ -398,3 +235,11 @@\n-\n-  if (use_ReduceInitialCardMarks()\n-      && g1_can_remove_post_barrier(kit, &kit->gvn(), oop_store, adr)) {\n+  MemNode* store = n->as_Mem();\n+  const Node* newval = n->in(MemNode::ValueIn);\n+  assert(newval != nullptr, \"\");\n+  const Type* newval_bottom = newval->bottom_type();\n+  TypePtr::PTR newval_type = newval_bottom->make_ptr()->ptr();\n+  uint8_t barrier_data = store->barrier_data();\n+  if (!newval_bottom->isa_oopptr() &&\n+      !newval_bottom->isa_narrowoop() &&\n+      newval_type != TypePtr::Null) {\n+    \/\/ newval is neither an OOP nor null, so there is no barrier to refine.\n+    assert(barrier_data == 0, \"non-OOP stores should have no barrier data\");\n@@ -403,4 +248,3 @@\n-\n-  if (!use_precise) {\n-    \/\/ All card marks for a (non-array) instance are in one place:\n-    adr = obj;\n+  if (barrier_data == 0) {\n+    \/\/ No barrier to refine.\n+    return;\n@@ -408,84 +252,10 @@\n-  \/\/ (Else it's an array (or unknown), and we want more precise card marks.)\n-  assert(adr != nullptr, \"\");\n-\n-  IdealKit ideal(kit, true);\n-\n-  Node* tls = __ thread(); \/\/ ThreadLocalStorage\n-\n-  Node* no_base = __ top();\n-  float likely = PROB_LIKELY_MAG(3);\n-  float unlikely = PROB_UNLIKELY_MAG(3);\n-  Node* young_card = __ ConI((jint)G1CardTable::g1_young_card_val());\n-  Node* dirty_card = __ ConI((jint)G1CardTable::dirty_card_val());\n-  Node* zeroX = __ ConX(0);\n-\n-  const TypeFunc *tf = write_ref_field_post_entry_Type();\n-\n-  \/\/ Offsets into the thread\n-  const int index_offset  = in_bytes(G1ThreadLocalData::dirty_card_queue_index_offset());\n-  const int buffer_offset = in_bytes(G1ThreadLocalData::dirty_card_queue_buffer_offset());\n-\n-  \/\/ Pointers into the thread\n-\n-  Node* buffer_adr = __ AddP(no_base, tls, __ ConX(buffer_offset));\n-  Node* index_adr =  __ AddP(no_base, tls, __ ConX(index_offset));\n-\n-  \/\/ Now some values\n-  \/\/ Use ctrl to avoid hoisting these values past a safepoint, which could\n-  \/\/ potentially reset these fields in the JavaThread.\n-  Node* index  = __ load(__ ctrl(), index_adr, TypeX_X, TypeX_X->basic_type(), Compile::AliasIdxRaw);\n-  Node* buffer = __ load(__ ctrl(), buffer_adr, TypeRawPtr::NOTNULL, T_ADDRESS, Compile::AliasIdxRaw);\n-\n-  \/\/ Convert the store obj pointer to an int prior to doing math on it\n-  \/\/ Must use ctrl to prevent \"integerized oop\" existing across safepoint\n-  Node* cast =  __ CastPX(__ ctrl(), adr);\n-\n-  \/\/ Divide pointer by card size\n-  Node* card_offset = __ URShiftX( cast, __ ConI(CardTable::card_shift()) );\n-\n-  \/\/ Combine card table base and card offset\n-  Node* card_adr = __ AddP(no_base, byte_map_base_node(kit), card_offset );\n-\n-  \/\/ If we know the value being stored does it cross regions?\n-\n-  if (val != nullptr) {\n-    \/\/ Does the store cause us to cross regions?\n-\n-    \/\/ Should be able to do an unsigned compare of region_size instead of\n-    \/\/ and extra shift. Do we have an unsigned compare??\n-    \/\/ Node* region_size = __ ConI(1 << G1HeapRegion::LogOfHRGrainBytes);\n-    Node* xor_res =  __ URShiftX ( __ XorX( cast,  __ CastPX(__ ctrl(), val)), __ ConI(checked_cast<jint>(G1HeapRegion::LogOfHRGrainBytes)));\n-\n-    \/\/ if (xor_res == 0) same region so skip\n-    __ if_then(xor_res, BoolTest::ne, zeroX, likely); {\n-\n-      \/\/ No barrier if we are storing a null.\n-      __ if_then(val, BoolTest::ne, kit->null(), likely); {\n-\n-        \/\/ Ok must mark the card if not already dirty\n-\n-        \/\/ load the original value of the card\n-        Node* card_val = __ load(__ ctrl(), card_adr, TypeInt::INT, T_BYTE, Compile::AliasIdxRaw);\n-\n-        __ if_then(card_val, BoolTest::ne, young_card, unlikely); {\n-          kit->sync_kit(ideal);\n-          kit->insert_mem_bar(Op_MemBarVolatile, oop_store);\n-          __ sync_kit(kit);\n-\n-          Node* card_val_reload = __ load(__ ctrl(), card_adr, TypeInt::INT, T_BYTE, Compile::AliasIdxRaw);\n-          __ if_then(card_val_reload, BoolTest::ne, dirty_card); {\n-            g1_mark_card(kit, ideal, card_adr, oop_store, alias_idx, index, index_adr, buffer, tf);\n-          } __ end_if();\n-        } __ end_if();\n-      } __ end_if();\n-    } __ end_if();\n-  } else {\n-    \/\/ The Object.clone() intrinsic uses this path if !ReduceInitialCardMarks.\n-    \/\/ We don't need a barrier here if the destination is a newly allocated object\n-    \/\/ in Eden. Otherwise, GC verification breaks because we assume that cards in Eden\n-    \/\/ are set to 'g1_young_gen' (see G1CardTable::verify_g1_young_region()).\n-    assert(!use_ReduceInitialCardMarks(), \"can only happen with card marking\");\n-    Node* card_val = __ load(__ ctrl(), card_adr, TypeInt::INT, T_BYTE, Compile::AliasIdxRaw);\n-    __ if_then(card_val, BoolTest::ne, young_card); {\n-      g1_mark_card(kit, ideal, card_adr, oop_store, alias_idx, index, index_adr, buffer, tf);\n-    } __ end_if();\n+  if (newval_type == TypePtr::Null) {\n+    \/\/ Simply elide post-barrier if writing null.\n+    barrier_data &= ~G1C2BarrierPost;\n+    barrier_data &= ~G1C2BarrierPostNotNull;\n+  } else if (((barrier_data & G1C2BarrierPost) != 0) &&\n+             newval_type == TypePtr::NotNull) {\n+    \/\/ If the post-barrier has not been elided yet (e.g. due to newval being\n+    \/\/ freshly allocated), mark it as not-null (simplifies barrier tests and\n+    \/\/ compressed OOPs logic).\n+    barrier_data |= G1C2BarrierPostNotNull;\n@@ -493,3 +263,2 @@\n-\n-  \/\/ Final sync IdealKit and GraphKit.\n-  kit->final_sync(ideal);\n+  store->set_barrier_data(barrier_data);\n+  return;\n@@ -498,26 +267,11 @@\n-\/\/ Helper that guards and inserts a pre-barrier.\n-void G1BarrierSetC2::insert_pre_barrier(GraphKit* kit, Node* base_oop, Node* offset,\n-                                        Node* pre_val, bool need_mem_bar) const {\n-  \/\/ We could be accessing the referent field of a reference object. If so, when G1\n-  \/\/ is enabled, we need to log the value in the referent field in an SATB buffer.\n-  \/\/ This routine performs some compile time filters and generates suitable\n-  \/\/ runtime filters that guard the pre-barrier code.\n-  \/\/ Also add memory barrier for non volatile load from the referent field\n-  \/\/ to prevent commoning of loads across safepoint.\n-\n-  \/\/ Some compile time checks.\n-\n-  \/\/ If offset is a constant, is it java_lang_ref_Reference::_reference_offset?\n-  const TypeX* otype = offset->find_intptr_t_type();\n-  if (otype != nullptr && otype->is_con() &&\n-      otype->get_con() != java_lang_ref_Reference::referent_offset()) {\n-    \/\/ Constant offset but not the reference_offset so just return\n-    return;\n-  }\n-\n-  \/\/ We only need to generate the runtime guards for instances.\n-  const TypeOopPtr* btype = base_oop->bottom_type()->isa_oopptr();\n-  if (btype != nullptr) {\n-    if (btype->isa_aryptr()) {\n-      \/\/ Array type so nothing to do\n-      return;\n+\/\/ Refine (not really expand) G1 barriers by looking at the new value type\n+\/\/ (whether it is necessarily null or necessarily non-null).\n+bool G1BarrierSetC2::expand_barriers(Compile* C, PhaseIterGVN& igvn) const {\n+  ResourceMark rm;\n+  VectorSet visited;\n+  Node_List worklist;\n+  worklist.push(C->root());\n+  while (worklist.size() > 0) {\n+    Node* n = worklist.pop();\n+    if (visited.test_set(n->_idx)) {\n+      continue;\n@@ -525,10 +279,5 @@\n-\n-    const TypeInstPtr* itype = btype->isa_instptr();\n-    if (itype != nullptr) {\n-      \/\/ Can the klass of base_oop be statically determined to be\n-      \/\/ _not_ a sub-class of Reference and _not_ Object?\n-      ciKlass* klass = itype->instance_klass();\n-      if (klass->is_loaded() &&\n-          !klass->is_subtype_of(kit->env()->Reference_klass()) &&\n-          !kit->env()->Object_klass()->is_subtype_of(klass)) {\n-        return;\n+    refine_barrier_by_new_val_type(n);\n+    for (uint j = 0; j < n->req(); j++) {\n+      Node* in = n->in(j);\n+      if (in != nullptr) {\n+        worklist.push(in);\n@@ -538,0 +287,2 @@\n+  return false;\n+}\n@@ -539,6 +290,5 @@\n-  \/\/ The compile time filters did not reject base_oop\/offset so\n-  \/\/ we need to generate the following runtime filters\n-  \/\/\n-  \/\/ if (offset == java_lang_ref_Reference::_reference_offset) {\n-  \/\/   if (instance_of(base, java.lang.ref.Reference)) {\n-  \/\/     pre_barrier(_, pre_val, ...);\n+uint G1BarrierSetC2::estimated_barrier_size(const Node* node) const {\n+  \/\/ These Ideal node counts are extracted from the pre-matching Ideal graph\n+  \/\/ generated when compiling the following method with early barrier expansion:\n+  \/\/   static void write(MyObject obj1, Object o) {\n+  \/\/     obj1.o1 = o;\n@@ -546,42 +296,10 @@\n-  \/\/ }\n-\n-  float likely   = PROB_LIKELY(  0.999);\n-  float unlikely = PROB_UNLIKELY(0.999);\n-\n-  IdealKit ideal(kit);\n-\n-  Node* referent_off = __ ConX(java_lang_ref_Reference::referent_offset());\n-\n-  __ if_then(offset, BoolTest::eq, referent_off, unlikely); {\n-      \/\/ Update graphKit memory and control from IdealKit.\n-      kit->sync_kit(ideal);\n-\n-      Node* ref_klass_con = kit->makecon(TypeKlassPtr::make(kit->env()->Reference_klass()));\n-      Node* is_instof = kit->gen_instanceof(base_oop, ref_klass_con);\n-\n-      \/\/ Update IdealKit memory and control from graphKit.\n-      __ sync_kit(kit);\n-\n-      Node* one = __ ConI(1);\n-      \/\/ is_instof == 0 if base_oop == nullptr\n-      __ if_then(is_instof, BoolTest::eq, one, unlikely); {\n-\n-        \/\/ Update graphKit from IdeakKit.\n-        kit->sync_kit(ideal);\n-\n-        \/\/ Use the pre-barrier to record the value in the referent field\n-        pre_barrier(kit, false \/* do_load *\/,\n-                    __ ctrl(),\n-                    nullptr \/* obj *\/, nullptr \/* adr *\/, max_juint \/* alias_idx *\/, nullptr \/* val *\/, nullptr \/* val_type *\/,\n-                    pre_val \/* pre_val *\/,\n-                    T_OBJECT);\n-        if (need_mem_bar) {\n-          \/\/ Add memory barrier to prevent commoning reads from this field\n-          \/\/ across safepoint since GC can change its value.\n-          kit->insert_mem_bar(Op_MemBarCPUOrder);\n-        }\n-        \/\/ Update IdealKit from graphKit.\n-        __ sync_kit(kit);\n-\n-      } __ end_if(); \/\/ _ref_type != ref_none\n-  } __ end_if(); \/\/ offset == referent_offset\n+  uint8_t barrier_data = MemNode::barrier_data(node);\n+  uint nodes = 0;\n+  if ((barrier_data & G1C2BarrierPre) != 0) {\n+    nodes += 50;\n+  }\n+  if ((barrier_data & G1C2BarrierPost) != 0) {\n+    nodes += 60;\n+  }\n+  return nodes;\n+}\n@@ -589,2 +307,7 @@\n-  \/\/ Final sync IdealKit and GraphKit.\n-  kit->final_sync(ideal);\n+bool G1BarrierSetC2::can_initialize_object(const StoreNode* store) const {\n+  assert(store->Opcode() == Op_StoreP || store->Opcode() == Op_StoreN, \"OOP store expected\");\n+  \/\/ It is OK to move the store across the object initialization boundary only\n+  \/\/ if it does not have any barrier, or if it has barriers that can be safely\n+  \/\/ elided (because of the compensation steps taken on the allocation slow path\n+  \/\/ when ReduceInitialCardMarks is enabled).\n+  return (MemNode::barrier_data(store) == 0) || use_ReduceInitialCardMarks();\n@@ -593,1 +316,7 @@\n-#undef __\n+void G1BarrierSetC2::clone_at_expansion(PhaseMacroExpand* phase, ArrayCopyNode* ac) const {\n+  if (ac->is_clone_inst() && !use_ReduceInitialCardMarks()) {\n+    clone_in_runtime(phase, ac, G1BarrierSetRuntime::clone_addr(), \"G1BarrierSetRuntime::clone\");\n+    return;\n+  }\n+  BarrierSetC2::clone_at_expansion(phase, ac);\n+}\n@@ -595,1 +324,1 @@\n-Node* G1BarrierSetC2::load_at_resolved(C2Access& access, const Type* val_type) const {\n+Node* G1BarrierSetC2::store_at_resolved(C2Access& access, C2AccessValue& val) const {\n@@ -597,6 +326,1 @@\n-  Node* adr = access.addr().node();\n-  Node* obj = access.base();\n-\n-  bool anonymous = (decorators & C2_UNSAFE_ACCESS) != 0;\n-  bool mismatched = (decorators & C2_MISMATCHED) != 0;\n-  bool unknown = (decorators & ON_UNKNOWN_OOP_REF) != 0;\n+  bool anonymous = (decorators & ON_UNKNOWN_OOP_REF) != 0;\n@@ -604,19 +328,12 @@\n-  bool in_native = (decorators & IN_NATIVE) != 0;\n-  bool on_weak = (decorators & ON_WEAK_OOP_REF) != 0;\n-  bool on_phantom = (decorators & ON_PHANTOM_OOP_REF) != 0;\n-  bool is_unordered = (decorators & MO_UNORDERED) != 0;\n-  bool no_keepalive = (decorators & AS_NO_KEEPALIVE) != 0;\n-  bool is_mixed = !in_heap && !in_native;\n-  bool need_cpu_mem_bar = !is_unordered || mismatched || is_mixed;\n-\n-  Node* top = Compile::current()->top();\n-  Node* offset = adr->is_AddP() ? adr->in(AddPNode::Offset) : top;\n-\n-  \/\/ If we are reading the value of the referent field of a Reference\n-  \/\/ object (either by using Unsafe directly or through reflection)\n-  \/\/ then, if G1 is enabled, we need to record the referent in an\n-  \/\/ SATB log buffer using the pre-barrier mechanism.\n-  \/\/ Also we need to add memory barrier to prevent commoning reads\n-  \/\/ from this field across safepoint since GC can change its value.\n-  bool need_read_barrier = (((on_weak || on_phantom) && !no_keepalive) ||\n-                            (in_heap && unknown && offset != top && obj != top));\n+  bool tightly_coupled_alloc = (decorators & C2_TIGHTLY_COUPLED_ALLOC) != 0;\n+  bool need_store_barrier = !(tightly_coupled_alloc && use_ReduceInitialCardMarks()) && (in_heap || anonymous);\n+  if (access.is_oop() && need_store_barrier) {\n+    access.set_barrier_data(get_store_barrier(access));\n+    if (tightly_coupled_alloc) {\n+      assert(!use_ReduceInitialCardMarks(),\n+             \"post-barriers are only needed for tightly-coupled initialization stores when ReduceInitialCardMarks is disabled\");\n+      access.set_barrier_data(access.barrier_data() ^ G1C2BarrierPre);\n+    }\n+  }\n+  return BarrierSetC2::store_at_resolved(access, val);\n+}\n@@ -624,2 +341,5 @@\n-  if (!access.is_oop() || !need_read_barrier) {\n-    return CardTableBarrierSetC2::load_at_resolved(access, val_type);\n+Node* G1BarrierSetC2::atomic_cmpxchg_val_at_resolved(C2AtomicParseAccess& access, Node* expected_val,\n+                                                     Node* new_val, const Type* value_type) const {\n+  GraphKit* kit = access.kit();\n+  if (!access.is_oop()) {\n+    return BarrierSetC2::atomic_cmpxchg_val_at_resolved(access, expected_val, new_val, value_type);\n@@ -627,0 +347,3 @@\n+  access.set_barrier_data(G1C2BarrierPre | G1C2BarrierPost);\n+  return BarrierSetC2::atomic_cmpxchg_val_at_resolved(access, expected_val, new_val, value_type);\n+}\n@@ -628,1 +351,9 @@\n-  assert(access.is_parse_access(), \"entry not supported at optimization time\");\n+Node* G1BarrierSetC2::atomic_cmpxchg_bool_at_resolved(C2AtomicParseAccess& access, Node* expected_val,\n+                                                      Node* new_val, const Type* value_type) const {\n+  GraphKit* kit = access.kit();\n+  if (!access.is_oop()) {\n+    return BarrierSetC2::atomic_cmpxchg_bool_at_resolved(access, expected_val, new_val, value_type);\n+  }\n+  access.set_barrier_data(G1C2BarrierPre | G1C2BarrierPost);\n+  return BarrierSetC2::atomic_cmpxchg_bool_at_resolved(access, expected_val, new_val, value_type);\n+}\n@@ -630,3 +361,8 @@\n-  C2ParseAccess& parse_access = static_cast<C2ParseAccess&>(access);\n-  GraphKit* kit = parse_access.kit();\n-  Node* load;\n+Node* G1BarrierSetC2::atomic_xchg_at_resolved(C2AtomicParseAccess& access, Node* new_val, const Type* value_type) const {\n+  GraphKit* kit = access.kit();\n+  if (!access.is_oop()) {\n+    return BarrierSetC2::atomic_xchg_at_resolved(access, new_val, value_type);\n+  }\n+  access.set_barrier_data(G1C2BarrierPre | G1C2BarrierPost);\n+  return BarrierSetC2::atomic_xchg_at_resolved(access, new_val, value_type);\n+}\n@@ -634,10 +370,3 @@\n-  Node* control =  kit->control();\n-  const TypePtr* adr_type = access.addr().type();\n-  MemNode::MemOrd mo = access.mem_node_mo();\n-  bool requires_atomic_access = (decorators & MO_UNORDERED) == 0;\n-  bool unaligned = (decorators & C2_UNALIGNED) != 0;\n-  bool unsafe = (decorators & C2_UNSAFE_ACCESS) != 0;\n-  \/\/ Pinned control dependency is the strictest. So it's ok to substitute it for any other.\n-  load = kit->make_load(control, adr, val_type, access.type(), adr_type, mo,\n-      LoadNode::Pinned, requires_atomic_access, unaligned, mismatched, unsafe,\n-      access.barrier_data());\n+class G1BarrierSetC2State : public BarrierSetC2State {\n+private:\n+  GrowableArray<G1BarrierStubC2*>* _stubs;\n@@ -645,0 +374,4 @@\n+public:\n+  G1BarrierSetC2State(Arena* arena)\n+    : BarrierSetC2State(arena),\n+      _stubs(new (arena) GrowableArray<G1BarrierStubC2*>(arena, 8,  0, nullptr)) {}\n@@ -646,13 +379,2 @@\n-  if (on_weak || on_phantom) {\n-    \/\/ Use the pre-barrier to record the value in the referent field\n-    pre_barrier(kit, false \/* do_load *\/,\n-                kit->control(),\n-                nullptr \/* obj *\/, nullptr \/* adr *\/, max_juint \/* alias_idx *\/, nullptr \/* val *\/, nullptr \/* val_type *\/,\n-                load \/* pre_val *\/, T_OBJECT);\n-    \/\/ Add memory barrier to prevent commoning reads from this field\n-    \/\/ across safepoint since GC can change its value.\n-    kit->insert_mem_bar(Op_MemBarCPUOrder);\n-  } else if (unknown) {\n-    \/\/ We do not require a mem bar inside pre_barrier if need_mem_bar\n-    \/\/ is set: the barriers would be emitted by us.\n-    insert_pre_barrier(kit, obj, offset, load, !need_cpu_mem_bar);\n+  GrowableArray<G1BarrierStubC2*>* stubs() {\n+    return _stubs;\n@@ -661,6 +383,3 @@\n-  return load;\n-}\n-\n-bool G1BarrierSetC2::is_gc_barrier_node(Node* node) const {\n-  if (CardTableBarrierSetC2::is_gc_barrier_node(node)) {\n-    return true;\n+  bool needs_liveness_data(const MachNode* mach) const {\n+    return G1PreBarrierStubC2::needs_barrier(mach) ||\n+           G1PostBarrierStubC2::needs_barrier(mach);\n@@ -668,5 +387,2 @@\n-  if (node->Opcode() != Op_CallLeaf) {\n-    return false;\n-  }\n-  CallLeafNode *call = node->as_CallLeaf();\n-  if (call->_name == nullptr) {\n+\n+  bool needs_livein_data() const {\n@@ -675,0 +391,1 @@\n+};\n@@ -676,1 +393,2 @@\n-  return strcmp(call->_name, \"write_ref_field_pre_entry\") == 0 || strcmp(call->_name, \"write_ref_field_post_entry\") == 0;\n+static G1BarrierSetC2State* barrier_set_state() {\n+  return reinterpret_cast<G1BarrierSetC2State*>(Compile::current()->barrier_set_state());\n@@ -679,3 +397,1 @@\n-bool G1BarrierSetC2::is_g1_pre_val_load(Node* n) {\n-  if (n->is_Load() && n->as_Load()->has_pinned_control_dependency()) {\n-    \/\/ Make sure the only users of it are: CmpP, StoreP, and a call to write_ref_field_pre_entry\n+G1BarrierStubC2::G1BarrierStubC2(const MachNode* node) : BarrierStubC2(node) {}\n@@ -683,4 +399,1 @@\n-    \/\/ Skip possible decode\n-    if (n->outcnt() == 1 && n->unique_out()->is_DecodeN()) {\n-      n = n->unique_out();\n-    }\n+G1PreBarrierStubC2::G1PreBarrierStubC2(const MachNode* node) : G1BarrierStubC2(node) {}\n@@ -688,17 +401,8 @@\n-    if (n->outcnt() == 3) {\n-      int found = 0;\n-      for (SimpleDUIterator iter(n); iter.has_next(); iter.next()) {\n-        Node* use = iter.get();\n-        if (use->is_Cmp() || use->is_Store()) {\n-          ++found;\n-        } else if (use->is_CallLeaf()) {\n-          CallLeafNode* call = use->as_CallLeaf();\n-          if (strcmp(call->_name, \"write_ref_field_pre_entry\") == 0) {\n-            ++found;\n-          }\n-        }\n-      }\n-      if (found == 3) {\n-        return true;\n-      }\n-    }\n+bool G1PreBarrierStubC2::needs_barrier(const MachNode* node) {\n+  return (node->barrier_data() & G1C2BarrierPre) != 0;\n+}\n+\n+G1PreBarrierStubC2* G1PreBarrierStubC2::create(const MachNode* node) {\n+  G1PreBarrierStubC2* const stub = new (Compile::current()->comp_arena()) G1PreBarrierStubC2(node);\n+  if (!Compile::current()->output()->in_scratch_emit_size()) {\n+    barrier_set_state()->stubs()->append(stub);\n@@ -706,1 +410,1 @@\n-  return false;\n+  return stub;\n@@ -709,2 +413,6 @@\n-bool G1BarrierSetC2::is_gc_pre_barrier_node(Node *node) const {\n-  return is_g1_pre_val_load(node);\n+void G1PreBarrierStubC2::initialize_registers(Register obj, Register pre_val, Register thread, Register tmp1, Register tmp2) {\n+  _obj = obj;\n+  _pre_val = pre_val;\n+  _thread = thread;\n+  _tmp1 = tmp1;\n+  _tmp2 = tmp2;\n@@ -713,9 +421,3 @@\n-void G1BarrierSetC2::eliminate_gc_barrier(PhaseMacroExpand* macro, Node* node) const {\n-  if (is_g1_pre_val_load(node)) {\n-    macro->replace_node(node, macro->zerocon(node->as_Load()->bottom_type()->basic_type()));\n-  } else {\n-    assert(node->Opcode() == Op_CastP2X, \"ConvP2XNode required\");\n-    assert(node->outcnt() <= 2, \"expects 1 or 2 users: Xor and URShift nodes\");\n-    \/\/ It could be only one user, URShift node, in Object.clone() intrinsic\n-    \/\/ but the new allocation is passed to arraycopy stub and it could not\n-    \/\/ be scalar replaced. So we don't check the case.\n+Register G1PreBarrierStubC2::obj() const {\n+  return _obj;\n+}\n@@ -723,3 +425,3 @@\n-    \/\/ An other case of only one user (Xor) is when the value check for null\n-    \/\/ in G1 post barrier is folded after CCP so the code which used URShift\n-    \/\/ is removed.\n+Register G1PreBarrierStubC2::pre_val() const {\n+  return _pre_val;\n+}\n@@ -727,4 +429,3 @@\n-    \/\/ Take Region node before eliminating post barrier since it also\n-    \/\/ eliminates CastP2X node when it has only one user.\n-    Node* this_region = node->in(0);\n-    assert(this_region != nullptr, \"\");\n+Register G1PreBarrierStubC2::thread() const {\n+  return _thread;\n+}\n@@ -732,1 +433,7 @@\n-    \/\/ Remove G1 post barrier.\n+Register G1PreBarrierStubC2::tmp1() const {\n+  return _tmp1;\n+}\n+\n+Register G1PreBarrierStubC2::tmp2() const {\n+  return _tmp2;\n+}\n@@ -734,11 +441,4 @@\n-    \/\/ Search for CastP2X->Xor->URShift->Cmp path which\n-    \/\/ checks if the store done to a different from the value's region.\n-    \/\/ And replace Cmp with #0 (false) to collapse G1 post barrier.\n-    Node* xorx = node->find_out_with(Op_XorX);\n-    if (xorx != nullptr) {\n-      Node* shift = xorx->unique_out();\n-      Node* cmpx = shift->unique_out();\n-      assert(cmpx->is_Cmp() && cmpx->unique_out()->is_Bool() &&\n-          cmpx->unique_out()->as_Bool()->_test._test == BoolTest::ne,\n-          \"missing region check in G1 post barrier\");\n-      macro->replace_node(cmpx, macro->makecon(TypeInt::CC_EQ));\n+void G1PreBarrierStubC2::emit_code(MacroAssembler& masm) {\n+  G1BarrierSetAssembler* bs = static_cast<G1BarrierSetAssembler*>(BarrierSet::barrier_set()->barrier_set_assembler());\n+  bs->generate_c2_pre_barrier_stub(&masm, this);\n+}\n@@ -746,1 +446,1 @@\n-      \/\/ Remove G1 pre barrier.\n+G1PostBarrierStubC2::G1PostBarrierStubC2(const MachNode* node) : G1BarrierStubC2(node) {}\n@@ -748,48 +448,2 @@\n-      \/\/ Search \"if (marking != 0)\" check and set it to \"false\".\n-      \/\/ There is no G1 pre barrier if previous stored value is null\n-      \/\/ (for example, after initialization).\n-      if (this_region->is_Region() && this_region->req() == 3) {\n-        int ind = 1;\n-        if (!this_region->in(ind)->is_IfFalse()) {\n-          ind = 2;\n-        }\n-        if (this_region->in(ind)->is_IfFalse() &&\n-            this_region->in(ind)->in(0)->Opcode() == Op_If) {\n-          Node* bol = this_region->in(ind)->in(0)->in(1);\n-          assert(bol->is_Bool(), \"\");\n-          cmpx = bol->in(1);\n-          if (bol->as_Bool()->_test._test == BoolTest::ne &&\n-              cmpx->is_Cmp() && cmpx->in(2) == macro->intcon(0) &&\n-              cmpx->in(1)->is_Load()) {\n-            Node* adr = cmpx->in(1)->as_Load()->in(MemNode::Address);\n-            const int marking_offset = in_bytes(G1ThreadLocalData::satb_mark_queue_active_offset());\n-            if (adr->is_AddP() && adr->in(AddPNode::Base) == macro->top() &&\n-                adr->in(AddPNode::Address)->Opcode() == Op_ThreadLocal &&\n-                adr->in(AddPNode::Offset) == macro->MakeConX(marking_offset)) {\n-              macro->replace_node(cmpx, macro->makecon(TypeInt::CC_EQ));\n-            }\n-          }\n-        }\n-      }\n-    } else {\n-      assert(!use_ReduceInitialCardMarks(), \"can only happen with card marking\");\n-      \/\/ This is a G1 post barrier emitted by the Object.clone() intrinsic.\n-      \/\/ Search for the CastP2X->URShiftX->AddP->LoadB->Cmp path which checks if the card\n-      \/\/ is marked as young_gen and replace the Cmp with 0 (false) to collapse the barrier.\n-      Node* shift = node->find_out_with(Op_URShiftX);\n-      assert(shift != nullptr, \"missing G1 post barrier\");\n-      Node* addp = shift->unique_out();\n-      Node* load = addp->find_out_with(Op_LoadB);\n-      assert(load != nullptr, \"missing G1 post barrier\");\n-      Node* cmpx = load->unique_out();\n-      assert(cmpx->is_Cmp() && cmpx->unique_out()->is_Bool() &&\n-          cmpx->unique_out()->as_Bool()->_test._test == BoolTest::ne,\n-          \"missing card value check in G1 post barrier\");\n-      macro->replace_node(cmpx, macro->makecon(TypeInt::CC_EQ));\n-      \/\/ There is no G1 pre barrier in this case\n-    }\n-    \/\/ Now CastP2X can be removed since it is used only on dead path\n-    \/\/ which currently still alive until igvn optimize it.\n-    assert(node->outcnt() == 0 || node->unique_out()->Opcode() == Op_URShiftX, \"\");\n-    macro->replace_node(node, macro->top());\n-  }\n+bool G1PostBarrierStubC2::needs_barrier(const MachNode* node) {\n+  return (node->barrier_data() & G1C2BarrierPost) != 0;\n@@ -798,31 +452,4 @@\n-Node* G1BarrierSetC2::step_over_gc_barrier(Node* c) const {\n-  if (!use_ReduceInitialCardMarks() &&\n-      c != nullptr && c->is_Region() && c->req() == 3) {\n-    for (uint i = 1; i < c->req(); i++) {\n-      if (c->in(i) != nullptr && c->in(i)->is_Region() &&\n-          c->in(i)->req() == 3) {\n-        Node* r = c->in(i);\n-        for (uint j = 1; j < r->req(); j++) {\n-          if (r->in(j) != nullptr && r->in(j)->is_Proj() &&\n-              r->in(j)->in(0) != nullptr &&\n-              r->in(j)->in(0)->Opcode() == Op_CallLeaf &&\n-              r->in(j)->in(0)->as_Call()->entry_point() == CAST_FROM_FN_PTR(address, G1BarrierSetRuntime::write_ref_field_post_entry)) {\n-            Node* call = r->in(j)->in(0);\n-            c = c->in(i == 1 ? 2 : 1);\n-            if (c != nullptr && c->Opcode() != Op_Parm) {\n-              c = c->in(0);\n-              if (c != nullptr) {\n-                c = c->in(0);\n-                assert(call->in(0) == nullptr ||\n-                       call->in(0)->in(0) == nullptr ||\n-                       call->in(0)->in(0)->in(0) == nullptr ||\n-                       call->in(0)->in(0)->in(0)->in(0) == nullptr ||\n-                       call->in(0)->in(0)->in(0)->in(0)->in(0) == nullptr ||\n-                       c == call->in(0)->in(0)->in(0)->in(0)->in(0), \"bad barrier shape\");\n-                return c;\n-              }\n-            }\n-          }\n-        }\n-      }\n-    }\n+G1PostBarrierStubC2* G1PostBarrierStubC2::create(const MachNode* node) {\n+  G1PostBarrierStubC2* const stub = new (Compile::current()->comp_arena()) G1PostBarrierStubC2(node);\n+  if (!Compile::current()->output()->in_scratch_emit_size()) {\n+    barrier_set_state()->stubs()->append(stub);\n@@ -830,1 +457,1 @@\n-  return c;\n+  return stub;\n@@ -833,12 +460,6 @@\n-#ifdef ASSERT\n-bool G1BarrierSetC2::has_cas_in_use_chain(Node *n) const {\n-  Unique_Node_List visited;\n-  Node_List worklist;\n-  worklist.push(n);\n-  while (worklist.size() > 0) {\n-    Node* x = worklist.pop();\n-    if (visited.member(x)) {\n-      continue;\n-    } else {\n-      visited.push(x);\n-    }\n+void G1PostBarrierStubC2::initialize_registers(Register thread, Register tmp1, Register tmp2, Register tmp3) {\n+  _thread = thread;\n+  _tmp1 = tmp1;\n+  _tmp2 = tmp2;\n+  _tmp3 = tmp3;\n+}\n@@ -846,16 +467,2 @@\n-    if (x->is_LoadStore()) {\n-      int op = x->Opcode();\n-      if (op == Op_CompareAndExchangeP || op == Op_CompareAndExchangeN ||\n-          op == Op_CompareAndSwapP     || op == Op_CompareAndSwapN     ||\n-          op == Op_WeakCompareAndSwapP || op == Op_WeakCompareAndSwapN) {\n-        return true;\n-      }\n-    }\n-    if (!x->is_CFG()) {\n-      for (SimpleDUIterator iter(x); iter.has_next(); iter.next()) {\n-        Node* use = iter.get();\n-        worklist.push(use);\n-      }\n-    }\n-  }\n-  return false;\n+Register G1PostBarrierStubC2::thread() const {\n+  return _thread;\n@@ -864,7 +471,3 @@\n-void G1BarrierSetC2::verify_pre_load(Node* marking_if, Unique_Node_List& loads \/*output*\/) const {\n-  assert(loads.size() == 0, \"Loads list should be empty\");\n-  Node* pre_val_if = marking_if->find_out_with(Op_IfTrue)->find_out_with(Op_If);\n-  if (pre_val_if != nullptr) {\n-    Unique_Node_List visited;\n-    Node_List worklist;\n-    Node* pre_val = pre_val_if->in(1)->in(1)->in(1);\n+Register G1PostBarrierStubC2::tmp1() const {\n+  return _tmp1;\n+}\n@@ -872,8 +475,3 @@\n-    worklist.push(pre_val);\n-    while (worklist.size() > 0) {\n-      Node* x = worklist.pop();\n-      if (visited.member(x)) {\n-        continue;\n-      } else {\n-        visited.push(x);\n-      }\n+Register G1PostBarrierStubC2::tmp2() const {\n+  return _tmp2;\n+}\n@@ -881,4 +479,3 @@\n-      if (has_cas_in_use_chain(x)) {\n-        loads.clear();\n-        return;\n-      }\n+Register G1PostBarrierStubC2::tmp3() const {\n+  return _tmp3;\n+}\n@@ -886,21 +483,3 @@\n-      if (x->is_Con()) {\n-        continue;\n-      }\n-      if (x->is_EncodeP() || x->is_DecodeN()) {\n-        worklist.push(x->in(1));\n-        continue;\n-      }\n-      if (x->is_Load() || x->is_LoadStore()) {\n-        assert(x->in(0) != nullptr, \"Pre-val load has to have a control\");\n-        loads.push(x);\n-        continue;\n-      }\n-      if (x->is_Phi()) {\n-        for (uint i = 1; i < x->req(); i++) {\n-          worklist.push(x->in(i));\n-        }\n-        continue;\n-      }\n-      assert(false, \"Pre-val anomaly\");\n-    }\n-  }\n+void G1PostBarrierStubC2::emit_code(MacroAssembler& masm) {\n+  G1BarrierSetAssembler* bs = static_cast<G1BarrierSetAssembler*>(BarrierSet::barrier_set()->barrier_set_assembler());\n+  bs->generate_c2_post_barrier_stub(&masm, this);\n@@ -909,4 +488,3 @@\n-void G1BarrierSetC2::verify_no_safepoints(Compile* compile, Node* marking_check_if, const Unique_Node_List& loads) const {\n-  if (loads.size() == 0) {\n-    return;\n-  }\n+void* G1BarrierSetC2::create_barrier_state(Arena* comp_arena) const {\n+  return new (comp_arena) G1BarrierSetC2State(comp_arena);\n+}\n@@ -914,7 +492,4 @@\n-  if (loads.size() == 1) { \/\/ Handle the typical situation when there a single pre-value load\n-                           \/\/ that is dominated by the marking_check_if, that's true when the\n-                           \/\/ barrier itself does the pre-val load.\n-    Node *pre_val = loads.at(0);\n-    if (pre_val->in(0)->in(0) == marking_check_if) { \/\/ IfTrue->If\n-      return;\n-    }\n+int G1BarrierSetC2::get_store_barrier(C2Access& access) const {\n+  if (!access.is_parse_access()) {\n+    \/\/ Only support for eliding barriers at parse time for now.\n+    return G1C2BarrierPre | G1C2BarrierPost;\n@@ -922,6 +497,25 @@\n-\n-  \/\/ All other cases are when pre-value loads dominate the marking check.\n-  Unique_Node_List controls;\n-  for (uint i = 0; i < loads.size(); i++) {\n-    Node *c = loads.at(i)->in(0);\n-    controls.push(c);\n+  GraphKit* kit = (static_cast<C2ParseAccess&>(access)).kit();\n+  Node* ctl = kit->control();\n+  Node* adr = access.addr().node();\n+  uint adr_idx = kit->C->get_alias_index(access.addr().type());\n+  assert(adr_idx != Compile::AliasIdxTop, \"use other store_to_memory factory\");\n+\n+  bool can_remove_pre_barrier = g1_can_remove_pre_barrier(kit, &kit->gvn(), adr, access.type(), adr_idx);\n+\n+  \/\/ We can skip marks on a freshly-allocated object in Eden. Keep this code in\n+  \/\/ sync with CardTableBarrierSet::on_slowpath_allocation_exit. That routine\n+  \/\/ informs GC to take appropriate compensating steps, upon a slow-path\n+  \/\/ allocation, so as to make this card-mark elision safe.\n+  \/\/ The post-barrier can also be removed if null is written. This case is\n+  \/\/ handled by G1BarrierSetC2::expand_barriers, which runs at the end of C2's\n+  \/\/ platform-independent optimizations to exploit stronger type information.\n+  bool can_remove_post_barrier = use_ReduceInitialCardMarks() &&\n+    ((access.base() == kit->just_allocated_object(ctl)) ||\n+     g1_can_remove_post_barrier(kit, &kit->gvn(), ctl, adr));\n+\n+  int barriers = 0;\n+  if (!can_remove_pre_barrier) {\n+    barriers |= G1C2BarrierPre;\n+  }\n+  if (!can_remove_post_barrier) {\n+    barriers |= G1C2BarrierPost;\n@@ -930,4 +524,2 @@\n-  Unique_Node_List visited;\n-  Unique_Node_List safepoints;\n-  Node_List worklist;\n-  uint found = 0;\n+  return barriers;\n+}\n@@ -935,9 +527,3 @@\n-  worklist.push(marking_check_if);\n-  while (worklist.size() > 0 && found < controls.size()) {\n-    Node* x = worklist.pop();\n-    if (x == nullptr || x == compile->top()) continue;\n-    if (visited.member(x)) {\n-      continue;\n-    } else {\n-      visited.push(x);\n-    }\n+void G1BarrierSetC2::late_barrier_analysis() const {\n+  compute_liveness_at_stubs();\n+}\n@@ -945,13 +531,8 @@\n-    if (controls.member(x)) {\n-      found++;\n-    }\n-    if (x->is_Region()) {\n-      for (uint i = 1; i < x->req(); i++) {\n-        worklist.push(x->in(i));\n-      }\n-    } else {\n-      if (!x->is_SafePoint()) {\n-        worklist.push(x->in(0));\n-      } else {\n-        safepoints.push(x);\n-      }\n+void G1BarrierSetC2::emit_stubs(CodeBuffer& cb) const {\n+  MacroAssembler masm(&cb);\n+  GrowableArray<G1BarrierStubC2*>* const stubs = barrier_set_state()->stubs();\n+  for (int i = 0; i < stubs->length(); i++) {\n+    \/\/ Make sure there is enough space in the code buffer\n+    if (cb.insts()->maybe_expand_to_ensure_remaining(PhaseOutput::MAX_inst_size) && cb.blob() == nullptr) {\n+      ciEnv::current()->record_failure(\"CodeCache is full\");\n+      return;\n@@ -959,0 +540,1 @@\n+    stubs->at(i)->emit_code(masm);\n@@ -960,1 +542,1 @@\n-  assert(found == controls.size(), \"Pre-barrier structure anomaly or possible safepoint\");\n+  masm.flush();\n@@ -963,3 +545,4 @@\n-void G1BarrierSetC2::verify_gc_barriers(Compile* compile, CompilePhase phase) const {\n-  if (phase != BarrierSetC2::BeforeCodeGen) {\n-    return;\n+#ifndef PRODUCT\n+void G1BarrierSetC2::dump_barrier_data(const MachNode* mach, outputStream* st) const {\n+  if ((mach->barrier_data() & G1C2BarrierPre) != 0) {\n+    st->print(\"pre \");\n@@ -967,59 +550,2 @@\n-  \/\/ Verify G1 pre-barriers\n-  const int marking_offset = in_bytes(G1ThreadLocalData::satb_mark_queue_active_offset());\n-\n-  Unique_Node_List visited;\n-  Node_List worklist;\n-  \/\/ We're going to walk control flow backwards starting from the Root\n-  worklist.push(compile->root());\n-  while (worklist.size() > 0) {\n-    Node* x = worklist.pop();\n-    if (x == nullptr || x == compile->top()) continue;\n-    if (visited.member(x)) {\n-      continue;\n-    } else {\n-      visited.push(x);\n-    }\n-\n-    if (x->is_Region()) {\n-      for (uint i = 1; i < x->req(); i++) {\n-        worklist.push(x->in(i));\n-      }\n-    } else {\n-      worklist.push(x->in(0));\n-      \/\/ We are looking for the pattern:\n-      \/\/                            \/->ThreadLocal\n-      \/\/ If->Bool->CmpI->LoadB->AddP->ConL(marking_offset)\n-      \/\/              \\->ConI(0)\n-      \/\/ We want to verify that the If and the LoadB have the same control\n-      \/\/ See GraphKit::g1_write_barrier_pre()\n-      if (x->is_If()) {\n-        IfNode *iff = x->as_If();\n-        if (iff->in(1)->is_Bool() && iff->in(1)->in(1)->is_Cmp()) {\n-          CmpNode *cmp = iff->in(1)->in(1)->as_Cmp();\n-          if (cmp->Opcode() == Op_CmpI && cmp->in(2)->is_Con() && cmp->in(2)->bottom_type()->is_int()->get_con() == 0\n-              && cmp->in(1)->is_Load()) {\n-            LoadNode* load = cmp->in(1)->as_Load();\n-            if (load->Opcode() == Op_LoadB && load->in(2)->is_AddP() && load->in(2)->in(2)->Opcode() == Op_ThreadLocal\n-                && load->in(2)->in(3)->is_Con()\n-                && load->in(2)->in(3)->bottom_type()->is_intptr_t()->get_con() == marking_offset) {\n-\n-              Node* if_ctrl = iff->in(0);\n-              Node* load_ctrl = load->in(0);\n-\n-              if (if_ctrl != load_ctrl) {\n-                \/\/ Skip possible CProj->NeverBranch in infinite loops\n-                if ((if_ctrl->is_Proj() && if_ctrl->Opcode() == Op_CProj)\n-                    && if_ctrl->in(0)->is_NeverBranch()) {\n-                  if_ctrl = if_ctrl->in(0)->in(0);\n-                }\n-              }\n-              assert(load_ctrl != nullptr && if_ctrl == load_ctrl, \"controls must match\");\n-\n-              Unique_Node_List loads;\n-              verify_pre_load(iff, loads);\n-              verify_no_safepoints(compile, iff, loads);\n-            }\n-          }\n-        }\n-      }\n-    }\n+  if ((mach->barrier_data() & G1C2BarrierPost) != 0) {\n+    st->print(\"post \");\n@@ -1027,28 +553,2 @@\n-}\n-#endif\n-\n-bool G1BarrierSetC2::escape_add_to_con_graph(ConnectionGraph* conn_graph, PhaseGVN* gvn, Unique_Node_List* delayed_worklist, Node* n, uint opcode) const {\n-  if (opcode == Op_StoreP) {\n-    Node* adr = n->in(MemNode::Address);\n-    const Type* adr_type = gvn->type(adr);\n-    \/\/ Pointer stores in G1 barriers looks like unsafe access.\n-    \/\/ Ignore such stores to be able scalar replace non-escaping\n-    \/\/ allocations.\n-    if (adr_type->isa_rawptr() && adr->is_AddP()) {\n-      Node* base = conn_graph->get_addp_base(adr);\n-      if (base->Opcode() == Op_LoadP &&\n-          base->in(MemNode::Address)->is_AddP()) {\n-        adr = base->in(MemNode::Address);\n-        Node* tls = conn_graph->get_addp_base(adr);\n-        if (tls->Opcode() == Op_ThreadLocal) {\n-          int offs = (int) gvn->find_intptr_t_con(adr->in(AddPNode::Offset), Type::OffsetBot);\n-          const int buf_offset = in_bytes(G1ThreadLocalData::satb_mark_queue_buffer_offset());\n-          if (offs == buf_offset) {\n-            return true; \/\/ G1 pre barrier previous oop value store.\n-          }\n-          if (offs == in_bytes(G1ThreadLocalData::dirty_card_queue_buffer_offset())) {\n-            return true; \/\/ G1 post barrier card address store.\n-          }\n-        }\n-      }\n-    }\n+  if ((mach->barrier_data() & G1C2BarrierPostNotNull) != 0) {\n+    st->print(\"notnull \");\n@@ -1056,1 +556,0 @@\n-  return false;\n@@ -1058,0 +557,1 @@\n+#endif \/\/ !PRODUCT\n","filename":"src\/hotspot\/share\/gc\/g1\/c2\/g1BarrierSetC2.cpp","additions":317,"deletions":817,"binary":false,"changes":1134,"status":"modified"},{"patch":"@@ -34,1 +34,40 @@\n-class G1BarrierSetC2: public CardTableBarrierSetC2 {\n+const int G1C2BarrierPre         = 1;\n+const int G1C2BarrierPost        = 2;\n+const int G1C2BarrierPostNotNull = 4;\n+\n+class G1BarrierStubC2 : public BarrierStubC2 {\n+public:\n+  G1BarrierStubC2(const MachNode* node);\n+  virtual void emit_code(MacroAssembler& masm) = 0;\n+};\n+\n+class G1PreBarrierStubC2 : public G1BarrierStubC2 {\n+private:\n+  Register _obj;\n+  Register _pre_val;\n+  Register _thread;\n+  Register _tmp1;\n+  Register _tmp2;\n+\n+protected:\n+  G1PreBarrierStubC2(const MachNode* node);\n+\n+public:\n+  static bool needs_barrier(const MachNode* node);\n+  static G1PreBarrierStubC2* create(const MachNode* node);\n+  void initialize_registers(Register obj, Register pre_val, Register thread, Register tmp1 = noreg, Register tmp2 = noreg);\n+  Register obj() const;\n+  Register pre_val() const;\n+  Register thread() const;\n+  Register tmp1() const;\n+  Register tmp2() const;\n+  virtual void emit_code(MacroAssembler& masm);\n+};\n+\n+class G1PostBarrierStubC2 : public G1BarrierStubC2 {\n+private:\n+  Register _thread;\n+  Register _tmp1;\n+  Register _tmp2;\n+  Register _tmp3;\n+\n@@ -36,20 +75,1 @@\n-  virtual void pre_barrier(GraphKit* kit,\n-                           bool do_load,\n-                           Node* ctl,\n-                           Node* obj,\n-                           Node* adr,\n-                           uint adr_idx,\n-                           Node* val,\n-                           const TypeOopPtr* val_type,\n-                           Node* pre_val,\n-                           BasicType bt) const;\n-\n-  virtual void post_barrier(GraphKit* kit,\n-                            Node* ctl,\n-                            Node* store,\n-                            Node* obj,\n-                            Node* adr,\n-                            uint adr_idx,\n-                            Node* val,\n-                            BasicType bt,\n-                            bool use_precise) const;\n+  G1PostBarrierStubC2(const MachNode* node);\n@@ -57,0 +77,13 @@\n+public:\n+  static bool needs_barrier(const MachNode* node);\n+  static G1PostBarrierStubC2* create(const MachNode* node);\n+  void initialize_registers(Register thread, Register tmp1 = noreg, Register tmp2 = noreg, Register tmp3 = noreg);\n+  Register thread() const;\n+  Register tmp1() const;\n+  Register tmp2() const;\n+  Register tmp3() const;\n+  virtual void emit_code(MacroAssembler& masm);\n+};\n+\n+class G1BarrierSetC2: public CardTableBarrierSetC2 {\n+protected:\n@@ -67,17 +100,1 @@\n-  void g1_mark_card(GraphKit* kit,\n-                    IdealKit& ideal,\n-                    Node* card_adr,\n-                    Node* oop_store,\n-                    uint oop_alias_idx,\n-                    Node* index,\n-                    Node* index_adr,\n-                    Node* buffer,\n-                    const TypeFunc* tf) const;\n-\n-  \/\/ Helper for unsafe accesses, that may or may not be on the referent field.\n-  \/\/ Generates the guards that check whether the result of\n-  \/\/ Unsafe.getReference should be recorded in an SATB log buffer.\n-  void insert_pre_barrier(GraphKit* kit, Node* base_oop, Node* offset, Node* pre_val, bool need_mem_bar) const;\n-\n-  static const TypeFunc* write_ref_field_pre_entry_Type();\n-  static const TypeFunc* write_ref_field_post_entry_Type();\n+  int get_store_barrier(C2Access& access) const;\n@@ -86,0 +103,6 @@\n+  virtual Node* store_at_resolved(C2Access& access, C2AccessValue& val) const;\n+  virtual Node* atomic_cmpxchg_val_at_resolved(C2AtomicParseAccess& access, Node* expected_val,\n+                                               Node* new_val, const Type* value_type) const;\n+  virtual Node* atomic_cmpxchg_bool_at_resolved(C2AtomicParseAccess& access, Node* expected_val,\n+                                                Node* new_val, const Type* value_type) const;\n+  virtual Node* atomic_xchg_at_resolved(C2AtomicParseAccess& access, Node* new_val, const Type* value_type) const;\n@@ -87,7 +110,0 @@\n-#ifdef ASSERT\n-  bool has_cas_in_use_chain(Node* x) const;\n-  void verify_pre_load(Node* marking_check_if, Unique_Node_List& loads \/*output*\/) const;\n-  void verify_no_safepoints(Compile* compile, Node* marking_load, const Unique_Node_List& loads) const;\n-#endif\n-\n-  static bool is_g1_pre_val_load(Node* n);\n@@ -95,2 +111,0 @@\n-  virtual bool is_gc_pre_barrier_node(Node* node) const;\n-  virtual bool is_gc_barrier_node(Node* node) const;\n@@ -98,4 +112,12 @@\n-  virtual Node* step_over_gc_barrier(Node* c) const;\n-\n-#ifdef ASSERT\n-  virtual void verify_gc_barriers(Compile* compile, CompilePhase phase) const;\n+  virtual void eliminate_gc_barrier_data(Node* node) const;\n+  virtual bool expand_barriers(Compile* C, PhaseIterGVN& igvn) const;\n+  virtual uint estimated_barrier_size(const Node* node) const;\n+  virtual bool can_initialize_object(const StoreNode* store) const;\n+  virtual void clone_at_expansion(PhaseMacroExpand* phase,\n+                                  ArrayCopyNode* ac) const;\n+  virtual void* create_barrier_state(Arena* comp_arena) const;\n+  virtual void emit_stubs(CodeBuffer& cb) const;\n+  virtual void late_barrier_analysis() const;\n+\n+#ifndef PRODUCT\n+  virtual void dump_barrier_data(const MachNode* mach, outputStream* st) const;\n@@ -103,2 +125,0 @@\n-\n-  virtual bool escape_add_to_con_graph(ConnectionGraph* conn_graph, PhaseGVN* gvn, Unique_Node_List* delayed_worklist, Node* n, uint opcode) const;\n","filename":"src\/hotspot\/share\/gc\/g1\/c2\/g1BarrierSetC2.hpp","additions":73,"deletions":53,"binary":false,"changes":126,"status":"modified"},{"patch":"@@ -64,0 +64,8 @@\n+\n+JRT_LEAF(void, G1BarrierSetRuntime::clone(oopDesc* src, oopDesc* dst, size_t size))\n+  HeapAccess<>::clone(src, dst, size);\n+JRT_END\n+\n+address G1BarrierSetRuntime::clone_addr() {\n+  return reinterpret_cast<address>(clone);\n+}\n","filename":"src\/hotspot\/share\/gc\/g1\/g1BarrierSetRuntime.cpp","additions":8,"deletions":0,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -38,0 +38,2 @@\n+private:\n+  static void clone(oopDesc* src, oopDesc* dst, size_t size);\n@@ -49,0 +51,2 @@\n+\n+  static address clone_addr();\n","filename":"src\/hotspot\/share\/gc\/g1\/g1BarrierSetRuntime.hpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -112,0 +112,4 @@\n+uint8_t BarrierStubC2::barrier_data() const {\n+  return _node->barrier_data();\n+}\n+\n","filename":"src\/hotspot\/share\/gc\/shared\/c2\/barrierSetC2.cpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -257,0 +257,2 @@\n+  \/\/ High-level, GC-specific barrier flags.\n+  uint8_t barrier_data() const;\n@@ -343,0 +345,2 @@\n+  \/\/ Whether the given store can be used to initialize a newly allocated object.\n+  virtual bool can_initialize_object(const StoreNode* store) const { return true; }\n","filename":"src\/hotspot\/share\/gc\/shared\/c2\/barrierSetC2.hpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -128,25 +128,0 @@\n-void CardTableBarrierSetC2::clone(GraphKit* kit, Node* src, Node* dst, Node* size, bool is_array) const {\n-  BarrierSetC2::clone(kit, src, dst, size, is_array);\n-  const TypePtr* raw_adr_type = TypeRawPtr::BOTTOM;\n-\n-  \/\/ If necessary, emit some card marks afterwards.  (Non-arrays only.)\n-  bool card_mark = !is_array && !use_ReduceInitialCardMarks();\n-  if (card_mark) {\n-    assert(!is_array, \"\");\n-    \/\/ Put in store barrier for any and all oops we are sticking\n-    \/\/ into this object.  (We could avoid this if we could prove\n-    \/\/ that the object type contains no oop fields at all.)\n-    Node* no_particular_value = nullptr;\n-    Node* no_particular_field = nullptr;\n-    int raw_adr_idx = Compile::AliasIdxRaw;\n-    post_barrier(kit, kit->control(),\n-                 kit->memory(raw_adr_type),\n-                 dst,\n-                 no_particular_field,\n-                 raw_adr_idx,\n-                 no_particular_value,\n-                 T_OBJECT,\n-                 false);\n-  }\n-}\n-\n@@ -157,4 +132,0 @@\n-bool CardTableBarrierSetC2::is_gc_barrier_node(Node* node) const {\n-  return ModRefBarrierSetC2::is_gc_barrier_node(node) || node->Opcode() == Op_StoreCM;\n-}\n-\n","filename":"src\/hotspot\/share\/gc\/shared\/c2\/cardTableBarrierSetC2.cpp","additions":0,"deletions":29,"binary":false,"changes":29,"status":"modified"},{"patch":"@@ -45,2 +45,0 @@\n-  virtual void clone(GraphKit* kit, Node* src, Node* dst, Node* size, bool is_array) const;\n-  virtual bool is_gc_barrier_node(Node* node) const;\n","filename":"src\/hotspot\/share\/gc\/shared\/c2\/cardTableBarrierSetC2.hpp","additions":0,"deletions":2,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -238,0 +238,7 @@\n+    if (def->is_MachTemp()) {\n+      assert(!def->bottom_type()->isa_oop_ptr(),\n+             \"ADLC only assigns OOP types to MachTemp defs corresponding to xRegN operands\");\n+      \/\/ Exclude MachTemp definitions even if they are typed as oops.\n+      continue;\n+    }\n+\n","filename":"src\/hotspot\/share\/opto\/buildOopMap.cpp","additions":7,"deletions":0,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -164,0 +164,8 @@\n+    if (mach->barrier_data() != 0) {\n+      \/\/ Using memory accesses with barriers to perform implicit null checks is\n+      \/\/ not supported. These operations might expand into multiple assembly\n+      \/\/ instructions during code emission, including new memory accesses (e.g.\n+      \/\/ in G1's pre-barrier), which would invalidate the implicit null\n+      \/\/ exception table.\n+      continue;\n+    }\n","filename":"src\/hotspot\/share\/opto\/lcm.cpp","additions":8,"deletions":0,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -1597,0 +1597,8 @@\n+    if (!shared && Matcher::is_encode_and_store_pattern(n, m)) {\n+      \/\/ Make it possible to match \"encode and store\" patterns with non-shared\n+      \/\/ encode operations that are pinned to a control node (e.g. by CastPP\n+      \/\/ node removal in final graph reshaping). The matched instruction cannot\n+      \/\/ float above the encode's control node because it is pinned to the\n+      \/\/ store's control node.\n+      return false;\n+    }\n@@ -2836,0 +2844,12 @@\n+bool Matcher::is_encode_and_store_pattern(const Node* n, const Node* m) {\n+  if (n == nullptr ||\n+      m == nullptr ||\n+      n->Opcode() != Op_StoreN ||\n+      !m->is_EncodeP() ||\n+      n->as_Store()->barrier_data() == 0) {\n+    return false;\n+  }\n+  assert(m == n->in(MemNode::ValueIn), \"m should be input to n\");\n+  return true;\n+}\n+\n","filename":"src\/hotspot\/share\/opto\/matcher.cpp","additions":20,"deletions":0,"binary":false,"changes":20,"status":"modified"},{"patch":"@@ -388,0 +388,2 @@\n+  static bool is_encode_and_store_pattern(const Node* n, const Node* m);\n+\n","filename":"src\/hotspot\/share\/opto\/matcher.hpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -4647,0 +4647,5 @@\n+  BarrierSetC2* bs = BarrierSet::barrier_set()->barrier_set_c2();\n+  if ((st->Opcode() == Op_StoreP || st->Opcode() == Op_StoreN) &&\n+      !bs->can_initialize_object(st)) {\n+    return FAIL;\n+  }\n","filename":"src\/hotspot\/share\/opto\/memnode.cpp","additions":5,"deletions":0,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -2025,0 +2025,2 @@\n+      assert(n->in(1)->as_Mach()->barrier_data() == 0,\n+             \"Implicit null checks on memory accesses with barriers are not yet supported\");\n","filename":"src\/hotspot\/share\/opto\/output.cpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -264,6 +264,0 @@\n-            \/\/ a card mark volatile barrier should be generated\n-            \/\/ before the card mark strb\n-            \/\/\n-            \/\/ following the fix for 8225776 the G1 barrier is now\n-            \/\/ scheduled out of line after the membar volatile and\n-            \/\/ and subsequent return\n@@ -274,4 +268,1 @@\n-                \"ret\",\n-                \"membar_volatile\",\n-                \"dmb ish\",\n-                \"strb\"\n+                \"ret\"\n@@ -335,6 +326,0 @@\n-            \/\/ a card mark volatile barrier should be generated\n-            \/\/ before the card mark strb\n-            \/\/\n-            \/\/ following the fix for 8225776 the G1 barrier is now\n-            \/\/ scheduled out of line after the membar acquire and\n-            \/\/ and subsequent return\n@@ -345,4 +330,1 @@\n-                \"ret\",\n-                \"membar_volatile\",\n-                \"dmb ish\",\n-                \"strb\"\n+                \"ret\"\n@@ -421,6 +403,0 @@\n-            \/\/ a card mark volatile barrier should be generated\n-            \/\/ before the card mark strb\n-            \/\/\n-            \/\/ following the fix for 8225776 the G1 barrier is now\n-            \/\/ scheduled out of line after the membar acquire and\n-            \/\/ and subsequent return\n@@ -431,4 +407,1 @@\n-                \"ret\",\n-                \"membar_volatile\",\n-                \"dmb ish\",\n-                \"strb\"\n+                \"ret\"\n@@ -487,6 +460,0 @@\n-            \/\/ a card mark volatile barrier should be generated\n-            \/\/ before the card mark strb\n-            \/\/\n-            \/\/ following the fix for 8225776 the G1 barrier is now\n-            \/\/ scheduled out of line after the membar acquire and\n-            \/\/ and subsequent return\n@@ -497,4 +464,1 @@\n-                \"ret\",\n-                \"membar_volatile\",\n-                \"dmb ish\",\n-                \"strb\"\n+                \"ret\"\n","filename":"test\/hotspot\/jtreg\/compiler\/c2\/aarch64\/TestVolatiles.java","additions":4,"deletions":40,"binary":false,"changes":44,"status":"modified"},{"patch":"@@ -1358,3 +1358,6 @@\n-    @IR(counts = { IRNode.ALLOC, \"1\" })\n-    \/\/ The last allocation won't be reduced because it would cause the creation\n-    \/\/ of a nested SafePointScalarMergeNode.\n+    \/\/ Using G1, all allocations are reduced.\n+    @IR(applyIf = {\"UseG1GC\", \"true\"}, failOn = { IRNode.ALLOC })\n+    \/\/ Otherwise, the last allocation won't be reduced because it would cause\n+    \/\/ the creation of a nested SafePointScalarMergeNode. This is caused by the\n+    \/\/ store barrier corresponding to 'C.other = B'.\n+    @IR(applyIf = {\"UseG1GC\", \"false\"}, counts = { IRNode.ALLOC, \"1\" })\n","filename":"test\/hotspot\/jtreg\/compiler\/c2\/irTests\/scalarReplacement\/AllocationMergesTests.java","additions":6,"deletions":3,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -0,0 +1,639 @@\n+\/*\n+ * Copyright (c) 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+package compiler.gcbarriers;\n+\n+import compiler.lib.ir_framework.*;\n+import java.lang.invoke.VarHandle;\n+import java.lang.invoke.MethodHandles;\n+import java.lang.ref.Reference;\n+import java.lang.ref.ReferenceQueue;\n+import java.lang.ref.SoftReference;\n+import java.lang.ref.WeakReference;\n+import java.util.concurrent.ThreadLocalRandom;\n+import jdk.test.lib.Asserts;\n+\n+\/**\n+ * @test\n+ * @summary Test that G1 barriers are generated and optimized as expected.\n+ * @library \/test\/lib \/\n+ * @requires vm.gc.G1\n+ * @run driver compiler.gcbarriers.TestG1BarrierGeneration\n+ *\/\n+\n+public class TestG1BarrierGeneration {\n+    static final String PRE_ONLY = \"pre\";\n+    static final String POST_ONLY = \"post\";\n+    static final String POST_ONLY_NOT_NULL = \"post notnull\";\n+    static final String PRE_AND_POST = \"pre post\";\n+    static final String PRE_AND_POST_NOT_NULL = \"pre post notnull\";\n+\n+    static class Outer {\n+        Object f;\n+    }\n+\n+    static class OuterWithVolatileField {\n+        volatile Object f;\n+    }\n+\n+    static class OuterWithFewFields implements Cloneable {\n+        Object f1;\n+        Object f2;\n+        public Object clone() throws CloneNotSupportedException {\n+            return super.clone();\n+        }\n+    }\n+\n+    static class OuterWithManyFields implements Cloneable {\n+        Object f1;\n+        Object f2;\n+        Object f3;\n+        Object f4;\n+        Object f5;\n+        Object f6;\n+        Object f7;\n+        Object f8;\n+        Object f9;\n+        Object f10;\n+        public Object clone() throws CloneNotSupportedException {\n+            return super.clone();\n+        }\n+    }\n+\n+    static final VarHandle fVarHandle;\n+    static {\n+        MethodHandles.Lookup l = MethodHandles.lookup();\n+        try {\n+            fVarHandle = l.findVarHandle(Outer.class, \"f\", Object.class);\n+        } catch (Exception e) {\n+            throw new Error(e);\n+        }\n+    }\n+\n+    public static void main(String[] args) {\n+        TestFramework framework = new TestFramework();\n+        Scenario[] scenarios = new Scenario[2*2];\n+        int scenarioIndex = 0;\n+        for (int i = 0; i < 2; i++) {\n+            for (int j = 0; j < 2; j++) {\n+                scenarios[scenarioIndex] =\n+                    new Scenario(scenarioIndex,\n+                                 \"-XX:CompileCommand=inline,java.lang.ref.*::*\",\n+                                 \"-XX:\" + (i == 0 ? \"-\" : \"+\") + \"UseCompressedOops\",\n+                                 \"-XX:\" + (j == 0 ? \"-\" : \"+\") + \"ReduceInitialCardMarks\");\n+                scenarioIndex++;\n+            }\n+        }\n+        framework.addScenarios(scenarios);\n+        framework.start();\n+    }\n+\n+    @Test\n+    @IR(applyIf = {\"UseCompressedOops\", \"false\"},\n+        counts = {IRNode.G1_STORE_P_WITH_BARRIER_FLAG, PRE_AND_POST, \"1\"},\n+        phase = CompilePhase.FINAL_CODE)\n+    @IR(applyIf = {\"UseCompressedOops\", \"true\"},\n+        counts = {IRNode.G1_ENCODE_P_AND_STORE_N_WITH_BARRIER_FLAG, PRE_AND_POST, \"1\"},\n+        phase = CompilePhase.FINAL_CODE)\n+    public static void testStore(Outer o, Object o1) {\n+        o.f = o1;\n+    }\n+\n+    @Test\n+    @IR(applyIf = {\"UseCompressedOops\", \"false\"},\n+        counts = {IRNode.G1_STORE_P_WITH_BARRIER_FLAG, PRE_ONLY, \"1\"},\n+        phase = CompilePhase.FINAL_CODE)\n+    @IR(applyIf = {\"UseCompressedOops\", \"true\"},\n+        counts = {IRNode.G1_STORE_N_WITH_BARRIER_FLAG, PRE_ONLY, \"1\"},\n+        phase = CompilePhase.FINAL_CODE)\n+    public static void testStoreNull(Outer o) {\n+        o.f = null;\n+    }\n+\n+    @Test\n+    @IR(applyIf = {\"UseCompressedOops\", \"false\"},\n+        counts = {IRNode.G1_STORE_P_WITH_BARRIER_FLAG, PRE_ONLY, \"1\"},\n+        phase = CompilePhase.FINAL_CODE)\n+    @IR(applyIf = {\"UseCompressedOops\", \"true\"},\n+        counts = {IRNode.G1_STORE_N_WITH_BARRIER_FLAG, PRE_ONLY, \"1\"},\n+        phase = CompilePhase.FINAL_CODE)\n+    public static void testStoreObfuscatedNull(Outer o, Object o1) {\n+        Object o2 = o1;\n+        for (int i = 0; i < 4; i++) {\n+            if ((i % 2) == 0) {\n+                o2 = null;\n+            }\n+        }\n+        \/\/ o2 is null here, but this is only known to C2 after applying some\n+        \/\/ optimizations (loop unrolling, IGVN).\n+        o.f = o2;\n+    }\n+\n+    @Test\n+    @IR(applyIf = {\"UseCompressedOops\", \"false\"},\n+        counts = {IRNode.G1_STORE_P_WITH_BARRIER_FLAG, PRE_AND_POST_NOT_NULL, \"1\"},\n+        phase = CompilePhase.FINAL_CODE)\n+    @IR(applyIf = {\"UseCompressedOops\", \"true\"},\n+        counts = {IRNode.G1_ENCODE_P_AND_STORE_N_WITH_BARRIER_FLAG, PRE_AND_POST_NOT_NULL, \"1\"},\n+        phase = CompilePhase.FINAL_CODE)\n+    public static void testStoreNotNull(Outer o, Object o1) {\n+        if (o1.hashCode() == 42) {\n+            return;\n+        }\n+        o.f = o1;\n+    }\n+\n+    @Test\n+    @IR(applyIf = {\"UseCompressedOops\", \"false\"},\n+        counts = {IRNode.G1_STORE_P_WITH_BARRIER_FLAG, PRE_AND_POST, \"2\"},\n+        phase = CompilePhase.FINAL_CODE)\n+    @IR(applyIf = {\"UseCompressedOops\", \"true\"},\n+        counts = {IRNode.G1_ENCODE_P_AND_STORE_N_WITH_BARRIER_FLAG, PRE_AND_POST, \"2\"},\n+        phase = CompilePhase.FINAL_CODE)\n+    public static void testStoreTwice(Outer o, Outer p, Object o1) {\n+        o.f = o1;\n+        p.f = o1;\n+    }\n+\n+    @Test\n+    @IR(applyIf = {\"UseCompressedOops\", \"false\"},\n+        counts = {IRNode.G1_STORE_P_WITH_BARRIER_FLAG, PRE_AND_POST, \"1\"},\n+        phase = CompilePhase.FINAL_CODE)\n+    @IR(applyIf = {\"UseCompressedOops\", \"true\"},\n+        counts = {IRNode.G1_ENCODE_P_AND_STORE_N_WITH_BARRIER_FLAG, PRE_AND_POST, \"1\"},\n+        phase = CompilePhase.FINAL_CODE)\n+    public static void testStoreVolatile(OuterWithVolatileField o, Object o1) {\n+        o.f = o1;\n+    }\n+\n+    @Test\n+    @IR(applyIfAnd = {\"UseCompressedOops\", \"false\", \"ReduceInitialCardMarks\", \"false\"},\n+        counts = {IRNode.G1_STORE_P_WITH_BARRIER_FLAG, POST_ONLY, \"1\"},\n+        phase = CompilePhase.FINAL_CODE)\n+    @IR(applyIfAnd = {\"UseCompressedOops\", \"true\", \"ReduceInitialCardMarks\", \"false\"},\n+        counts = {IRNode.G1_ENCODE_P_AND_STORE_N_WITH_BARRIER_FLAG, POST_ONLY, \"1\"},\n+        phase = CompilePhase.FINAL_CODE)\n+    @IR(applyIfAnd = {\"UseCompressedOops\", \"false\", \"ReduceInitialCardMarks\", \"true\"},\n+        failOn = {IRNode.G1_STORE_P},\n+        phase = CompilePhase.FINAL_CODE)\n+    @IR(applyIfAnd = {\"UseCompressedOops\", \"true\", \"ReduceInitialCardMarks\", \"true\"},\n+        failOn = {IRNode.G1_STORE_N, IRNode.G1_ENCODE_P_AND_STORE_N},\n+        phase = CompilePhase.FINAL_CODE)\n+    public static Outer testStoreOnNewObject(Object o1) {\n+        Outer o = new Outer();\n+        o.f = o1;\n+        return o;\n+    }\n+\n+    @Test\n+    @IR(failOn = {IRNode.STORE_P, IRNode.STORE_N},\n+        phase = CompilePhase.BEFORE_MACRO_EXPANSION)\n+    public static Outer testStoreNullOnNewObject() {\n+        Outer o = new Outer();\n+        o.f = null;\n+        return o;\n+    }\n+\n+    @Test\n+    @IR(applyIfAnd = {\"UseCompressedOops\", \"false\", \"ReduceInitialCardMarks\", \"false\"},\n+        counts = {IRNode.G1_STORE_P_WITH_BARRIER_FLAG, POST_ONLY_NOT_NULL, \"1\"},\n+        phase = CompilePhase.FINAL_CODE)\n+    @IR(applyIfAnd = {\"UseCompressedOops\", \"true\", \"ReduceInitialCardMarks\", \"false\"},\n+        counts = {IRNode.G1_ENCODE_P_AND_STORE_N_WITH_BARRIER_FLAG, POST_ONLY_NOT_NULL, \"1\"},\n+        phase = CompilePhase.FINAL_CODE)\n+    @IR(applyIfAnd = {\"UseCompressedOops\", \"false\", \"ReduceInitialCardMarks\", \"true\"},\n+        failOn = {IRNode.G1_STORE_P},\n+        phase = CompilePhase.FINAL_CODE)\n+    @IR(applyIfAnd = {\"UseCompressedOops\", \"true\", \"ReduceInitialCardMarks\", \"true\"},\n+        failOn = {IRNode.G1_STORE_N, IRNode.G1_ENCODE_P_AND_STORE_N},\n+        phase = CompilePhase.FINAL_CODE)\n+    public static Outer testStoreNotNullOnNewObject(Object o1) {\n+        if (o1.hashCode() == 42) {\n+            return null;\n+        }\n+        Outer o = new Outer();\n+        o.f = o1;\n+        return o;\n+    }\n+\n+    @Test\n+    @IR(applyIfAnd = {\"UseCompressedOops\", \"false\", \"ReduceInitialCardMarks\", \"false\"},\n+        counts = {IRNode.G1_STORE_P_WITH_BARRIER_FLAG, POST_ONLY, \"2\"},\n+        phase = CompilePhase.FINAL_CODE)\n+    @IR(applyIfAnd = {\"UseCompressedOops\", \"true\", \"ReduceInitialCardMarks\", \"false\"},\n+        counts = {IRNode.G1_ENCODE_P_AND_STORE_N_WITH_BARRIER_FLAG, POST_ONLY, \"2\"},\n+        phase = CompilePhase.FINAL_CODE)\n+    @IR(applyIfAnd = {\"UseCompressedOops\", \"false\", \"ReduceInitialCardMarks\", \"true\"},\n+        failOn = {IRNode.G1_STORE_P},\n+        phase = CompilePhase.FINAL_CODE)\n+    @IR(applyIfAnd = {\"UseCompressedOops\", \"true\", \"ReduceInitialCardMarks\", \"true\"},\n+        failOn = {IRNode.G1_STORE_N, IRNode.G1_ENCODE_P_AND_STORE_N},\n+        phase = CompilePhase.FINAL_CODE)\n+    public static Outer testStoreOnNewObjectInTwoPaths(Object o1, boolean c) {\n+        Outer o;\n+        if (c) {\n+            o = new Outer();\n+            o.f = o1;\n+        } else {\n+            o = new Outer();\n+            o.f = o1;\n+        }\n+        return o;\n+    }\n+\n+    @Run(test = {\"testStore\",\n+                 \"testStoreNull\",\n+                 \"testStoreObfuscatedNull\",\n+                 \"testStoreNotNull\",\n+                 \"testStoreTwice\",\n+                 \"testStoreVolatile\",\n+                 \"testStoreOnNewObject\",\n+                 \"testStoreNullOnNewObject\",\n+                 \"testStoreNotNullOnNewObject\",\n+                 \"testStoreOnNewObjectInTwoPaths\"})\n+    public void runStoreTests() {\n+        {\n+            Outer o = new Outer();\n+            Object o1 = new Object();\n+            testStore(o, o1);\n+            Asserts.assertEquals(o1, o.f);\n+        }\n+        {\n+            Outer o = new Outer();\n+            testStoreNull(o);\n+            Asserts.assertNull(o.f);\n+        }\n+        {\n+            Outer o = new Outer();\n+            Object o1 = new Object();\n+            testStoreObfuscatedNull(o, o1);\n+            Asserts.assertNull(o.f);\n+        }\n+        {\n+            Outer o = new Outer();\n+            Object o1 = new Object();\n+            testStoreNotNull(o, o1);\n+            Asserts.assertEquals(o1, o.f);\n+        }\n+        {\n+            Outer o = new Outer();\n+            Outer p = new Outer();\n+            Object o1 = new Object();\n+            testStoreTwice(o, p, o1);\n+            Asserts.assertEquals(o1, o.f);\n+            Asserts.assertEquals(o1, p.f);\n+        }\n+        {\n+            OuterWithVolatileField o = new OuterWithVolatileField();\n+            Object o1 = new Object();\n+            testStoreVolatile(o, o1);\n+            Asserts.assertEquals(o1, o.f);\n+        }\n+        {\n+            Object o1 = new Object();\n+            Outer o = testStoreOnNewObject(o1);\n+            Asserts.assertEquals(o1, o.f);\n+        }\n+        {\n+            Outer o = testStoreNullOnNewObject();\n+            Asserts.assertNull(o.f);\n+        }\n+        {\n+            Object o1 = new Object();\n+            Outer o = testStoreNotNullOnNewObject(o1);\n+            Asserts.assertEquals(o1, o.f);\n+        }\n+        {\n+            Object o1 = new Object();\n+            Outer o = testStoreOnNewObjectInTwoPaths(o1, ThreadLocalRandom.current().nextBoolean());\n+            Asserts.assertEquals(o1, o.f);\n+        }\n+    }\n+\n+    @Test\n+    @IR(applyIf = {\"UseCompressedOops\", \"false\"},\n+        counts = {IRNode.G1_STORE_P_WITH_BARRIER_FLAG, PRE_AND_POST, \"1\"},\n+        phase = CompilePhase.FINAL_CODE)\n+    @IR(applyIf = {\"UseCompressedOops\", \"true\"},\n+        counts = {IRNode.G1_ENCODE_P_AND_STORE_N_WITH_BARRIER_FLAG, PRE_AND_POST, \"1\"},\n+        phase = CompilePhase.FINAL_CODE)\n+    public static void testArrayStore(Object[] a, int index, Object o1) {\n+        a[index] = o1;\n+    }\n+\n+    @Test\n+    @IR(applyIf = {\"UseCompressedOops\", \"false\"},\n+        counts = {IRNode.G1_STORE_P_WITH_BARRIER_FLAG, PRE_ONLY, \"1\"},\n+        phase = CompilePhase.FINAL_CODE)\n+    @IR(applyIf = {\"UseCompressedOops\", \"true\"},\n+        counts = {IRNode.G1_STORE_N_WITH_BARRIER_FLAG, PRE_ONLY, \"1\"},\n+        phase = CompilePhase.FINAL_CODE)\n+    public static void testArrayStoreNull(Object[] a, int index) {\n+        a[index] = null;\n+    }\n+\n+    @Test\n+    @IR(applyIf = {\"UseCompressedOops\", \"false\"},\n+        counts = {IRNode.G1_STORE_P_WITH_BARRIER_FLAG, PRE_AND_POST_NOT_NULL, \"1\"},\n+        phase = CompilePhase.FINAL_CODE)\n+    @IR(applyIf = {\"UseCompressedOops\", \"true\"},\n+        counts = {IRNode.G1_ENCODE_P_AND_STORE_N_WITH_BARRIER_FLAG, PRE_AND_POST_NOT_NULL, \"1\"},\n+        phase = CompilePhase.FINAL_CODE)\n+    public static void testArrayStoreNotNull(Object[] a, int index, Object o1) {\n+        if (o1.hashCode() == 42) {\n+            return;\n+        }\n+        a[index] = o1;\n+    }\n+\n+    @Test\n+    @IR(applyIf = {\"UseCompressedOops\", \"false\"},\n+        counts = {IRNode.G1_STORE_P_WITH_BARRIER_FLAG, PRE_AND_POST, \"2\"},\n+        phase = CompilePhase.FINAL_CODE)\n+    @IR(applyIf = {\"UseCompressedOops\", \"true\"},\n+        counts = {IRNode.G1_ENCODE_P_AND_STORE_N_WITH_BARRIER_FLAG, PRE_AND_POST, \"2\"},\n+        phase = CompilePhase.FINAL_CODE)\n+    public static void testArrayStoreTwice(Object[] a, Object[] b, int index, Object o1) {\n+        a[index] = o1;\n+        b[index] = o1;\n+    }\n+\n+    @Test\n+    @IR(applyIfAnd = {\"UseCompressedOops\", \"false\", \"ReduceInitialCardMarks\", \"true\"},\n+        failOn = {IRNode.G1_STORE_P},\n+        phase = CompilePhase.FINAL_CODE)\n+    @IR(applyIfAnd = {\"UseCompressedOops\", \"true\", \"ReduceInitialCardMarks\", \"true\"},\n+        failOn = {IRNode.G1_STORE_N, IRNode.G1_ENCODE_P_AND_STORE_N},\n+        phase = CompilePhase.FINAL_CODE)\n+    public static Object[] testStoreOnNewArray(Object o1) {\n+        Object[] a = new Object[10];\n+        \/\/ The index needs to be concrete for C2 to detect that it is safe to\n+        \/\/ remove the pre-barrier.\n+        a[4] = o1;\n+        return a;\n+    }\n+\n+    @Run(test = {\"testArrayStore\",\n+                 \"testArrayStoreNull\",\n+                 \"testArrayStoreNotNull\",\n+                 \"testArrayStoreTwice\",\n+                 \"testStoreOnNewArray\"})\n+    public void runArrayStoreTests() {\n+        {\n+            Object[] a = new Object[10];\n+            Object o1 = new Object();\n+            testArrayStore(a, 4, o1);\n+            Asserts.assertEquals(o1, a[4]);\n+        }\n+        {\n+            Object[] a = new Object[10];\n+            testArrayStoreNull(a, 4);\n+            Asserts.assertNull(a[4]);\n+        }\n+        {\n+            Object[] a = new Object[10];\n+            Object o1 = new Object();\n+            testArrayStoreNotNull(a, 4, o1);\n+            Asserts.assertEquals(o1, a[4]);\n+        }\n+        {\n+            Object[] a = new Object[10];\n+            Object[] b = new Object[10];\n+            Object o1 = new Object();\n+            testArrayStoreTwice(a, b, 4, o1);\n+            Asserts.assertEquals(o1, a[4]);\n+            Asserts.assertEquals(o1, b[4]);\n+        }\n+        {\n+            Object o1 = new Object();\n+            Object[] a = testStoreOnNewArray(o1);\n+            Asserts.assertEquals(o1, a[4]);\n+        }\n+    }\n+\n+    @Test\n+    public static Object[] testCloneArrayOfObjects(Object[] a) {\n+        Object[] a1 = null;\n+        try {\n+            a1 = a.clone();\n+        } catch (Exception e) {}\n+        return a1;\n+    }\n+\n+    @Test\n+    @IR(applyIf = {\"ReduceInitialCardMarks\", \"true\"},\n+        failOn = {IRNode.G1_STORE_P, IRNode.G1_STORE_N, IRNode.G1_ENCODE_P_AND_STORE_N},\n+        phase = CompilePhase.FINAL_CODE)\n+    @IR(applyIfAnd = {\"ReduceInitialCardMarks\", \"false\", \"UseCompressedOops\", \"false\"},\n+        counts = {IRNode.G1_STORE_P_WITH_BARRIER_FLAG, POST_ONLY, \"2\"},\n+        phase = CompilePhase.FINAL_CODE)\n+    @IR(applyIfAnd = {\"ReduceInitialCardMarks\", \"false\", \"UseCompressedOops\", \"true\"},\n+        counts = {IRNode.G1_STORE_N_WITH_BARRIER_FLAG, POST_ONLY, \"2\"},\n+        phase = CompilePhase.FINAL_CODE)\n+    public static OuterWithFewFields testCloneObjectWithFewFields(OuterWithFewFields o) {\n+        Object o1 = null;\n+        try {\n+            o1 = o.clone();\n+        } catch (Exception e) {}\n+        return (OuterWithFewFields)o1;\n+    }\n+\n+    @Test\n+    @IR(applyIf = {\"ReduceInitialCardMarks\", \"true\"},\n+        counts = {IRNode.CALL_OF, \"jlong_disjoint_arraycopy\", \"1\"})\n+    @IR(applyIf = {\"ReduceInitialCardMarks\", \"false\"},\n+        counts = {IRNode.CALL_OF, \"G1BarrierSetRuntime::clone\", \"1\"})\n+    public static OuterWithManyFields testCloneObjectWithManyFields(OuterWithManyFields o) {\n+        Object o1 = null;\n+        try {\n+            o1 = o.clone();\n+        } catch (Exception e) {}\n+        return (OuterWithManyFields)o1;\n+    }\n+\n+    @Run(test = {\"testCloneArrayOfObjects\",\n+                 \"testCloneObjectWithFewFields\",\n+                 \"testCloneObjectWithManyFields\"})\n+    public void runCloneTests() {\n+        {\n+            Object o1 = new Object();\n+            Object[] a = new Object[4];\n+            for (int i = 0; i < 4; i++) {\n+                a[i] = o1;\n+            }\n+            Object[] a1 = testCloneArrayOfObjects(a);\n+            for (int i = 0; i < 4; i++) {\n+                Asserts.assertEquals(o1, a1[i]);\n+            }\n+        }\n+        {\n+            Object a = new Object();\n+            Object b = new Object();\n+            OuterWithFewFields o = new OuterWithFewFields();\n+            o.f1 = a;\n+            o.f2 = b;\n+            OuterWithFewFields o1 = testCloneObjectWithFewFields(o);\n+            Asserts.assertEquals(a, o1.f1);\n+            Asserts.assertEquals(b, o1.f2);\n+        }\n+        {\n+            Object a = new Object();\n+            Object b = new Object();\n+            Object c = new Object();\n+            Object d = new Object();\n+            Object e = new Object();\n+            Object f = new Object();\n+            Object g = new Object();\n+            Object h = new Object();\n+            Object i = new Object();\n+            Object j = new Object();\n+            OuterWithManyFields o = new OuterWithManyFields();\n+            o.f1 = a;\n+            o.f2 = b;\n+            o.f3 = c;\n+            o.f4 = d;\n+            o.f5 = e;\n+            o.f6 = f;\n+            o.f7 = g;\n+            o.f8 = h;\n+            o.f9 = i;\n+            o.f10 = j;\n+            OuterWithManyFields o1 = testCloneObjectWithManyFields(o);\n+            Asserts.assertEquals(a, o1.f1);\n+            Asserts.assertEquals(b, o1.f2);\n+            Asserts.assertEquals(c, o1.f3);\n+            Asserts.assertEquals(d, o1.f4);\n+            Asserts.assertEquals(e, o1.f5);\n+            Asserts.assertEquals(f, o1.f6);\n+            Asserts.assertEquals(g, o1.f7);\n+            Asserts.assertEquals(h, o1.f8);\n+            Asserts.assertEquals(i, o1.f9);\n+            Asserts.assertEquals(j, o1.f10);\n+        }\n+    }\n+\n+    @Test\n+    @IR(applyIf = {\"UseCompressedOops\", \"false\"},\n+        counts = {IRNode.G1_COMPARE_AND_EXCHANGE_P_WITH_BARRIER_FLAG, PRE_AND_POST, \"1\"},\n+        phase = CompilePhase.FINAL_CODE)\n+    @IR(applyIf = {\"UseCompressedOops\", \"true\"},\n+        counts = {IRNode.G1_COMPARE_AND_EXCHANGE_N_WITH_BARRIER_FLAG, PRE_AND_POST, \"1\"},\n+        phase = CompilePhase.FINAL_CODE)\n+    static Object testCompareAndExchange(Outer o, Object oldVal, Object newVal) {\n+        return fVarHandle.compareAndExchange(o, oldVal, newVal);\n+    }\n+\n+    @Test\n+    @IR(applyIf = {\"UseCompressedOops\", \"false\"},\n+        counts = {IRNode.G1_COMPARE_AND_SWAP_P_WITH_BARRIER_FLAG, PRE_AND_POST, \"1\"},\n+        phase = CompilePhase.FINAL_CODE)\n+    @IR(applyIf = {\"UseCompressedOops\", \"true\"},\n+        counts = {IRNode.G1_COMPARE_AND_SWAP_N_WITH_BARRIER_FLAG, PRE_AND_POST, \"1\"},\n+        phase = CompilePhase.FINAL_CODE)\n+    static boolean testCompareAndSwap(Outer o, Object oldVal, Object newVal) {\n+        return fVarHandle.compareAndSet(o, oldVal, newVal);\n+    }\n+\n+    @Test\n+    @IR(applyIf = {\"UseCompressedOops\", \"false\"},\n+        counts = {IRNode.G1_GET_AND_SET_P_WITH_BARRIER_FLAG, PRE_AND_POST, \"1\"},\n+        phase = CompilePhase.FINAL_CODE)\n+    @IR(applyIf = {\"UseCompressedOops\", \"true\"},\n+        counts = {IRNode.G1_GET_AND_SET_N_WITH_BARRIER_FLAG, PRE_AND_POST, \"1\"},\n+        phase = CompilePhase.FINAL_CODE)\n+    static Object testGetAndSet(Outer o, Object newVal) {\n+        return fVarHandle.getAndSet(o, newVal);\n+    }\n+\n+    @Run(test = {\"testCompareAndExchange\",\n+                 \"testCompareAndSwap\",\n+                 \"testGetAndSet\"})\n+    public void runAtomicTests() {\n+        {\n+            Outer o = new Outer();\n+            Object oldVal = new Object();\n+            o.f = oldVal;\n+            Object newVal = new Object();\n+            Object oldVal2 = testCompareAndExchange(o, oldVal, newVal);\n+            Asserts.assertEquals(oldVal, oldVal2);\n+            Asserts.assertEquals(o.f, newVal);\n+        }\n+        {\n+            Outer o = new Outer();\n+            Object oldVal = new Object();\n+            o.f = oldVal;\n+            Object newVal = new Object();\n+            boolean b = testCompareAndSwap(o, oldVal, newVal);\n+            Asserts.assertTrue(b);\n+            Asserts.assertEquals(o.f, newVal);\n+        }\n+        {\n+            Outer o = new Outer();\n+            Object oldVal = new Object();\n+            o.f = oldVal;\n+            Object newVal = new Object();\n+            Object oldVal2 = testGetAndSet(o, newVal);\n+            Asserts.assertEquals(oldVal, oldVal2);\n+            Asserts.assertEquals(o.f, newVal);\n+        }\n+    }\n+\n+    @Test\n+    @IR(applyIf = {\"UseCompressedOops\", \"false\"},\n+        counts = {IRNode.G1_LOAD_P_WITH_BARRIER_FLAG, PRE_ONLY, \"1\"},\n+        phase = CompilePhase.FINAL_CODE)\n+    @IR(applyIf = {\"UseCompressedOops\", \"true\"},\n+        counts = {IRNode.G1_LOAD_N_WITH_BARRIER_FLAG, PRE_ONLY, \"1\"},\n+        phase = CompilePhase.FINAL_CODE)\n+    static Object testLoadSoftReference(SoftReference<Object> ref) {\n+        return ref.get();\n+    }\n+\n+    @Test\n+    @IR(applyIf = {\"UseCompressedOops\", \"false\"},\n+        counts = {IRNode.G1_LOAD_P_WITH_BARRIER_FLAG, PRE_ONLY, \"1\"},\n+        phase = CompilePhase.FINAL_CODE)\n+    @IR(applyIf = {\"UseCompressedOops\", \"true\"},\n+        counts = {IRNode.G1_LOAD_N_WITH_BARRIER_FLAG, PRE_ONLY, \"1\"},\n+        phase = CompilePhase.FINAL_CODE)\n+    static Object testLoadWeakReference(WeakReference<Object> ref) {\n+        return ref.get();\n+    }\n+\n+    @Run(test = {\"testLoadSoftReference\",\n+                 \"testLoadWeakReference\"})\n+    public void runReferenceTests() {\n+        {\n+            Object o1 = new Object();\n+            SoftReference<Object> sref = new SoftReference<Object>(o1);\n+            Object o2 = testLoadSoftReference(sref);\n+            Asserts.assertTrue(o2 == o1 || o2 == null);\n+        }\n+        {\n+            Object o1 = new Object();\n+            WeakReference<Object> wref = new WeakReference<Object>(o1);\n+            Object o2 = testLoadWeakReference(wref);\n+            Asserts.assertTrue(o2 == o1 || o2 == null);\n+        }\n+    }\n+}\n","filename":"test\/hotspot\/jtreg\/compiler\/gcbarriers\/TestG1BarrierGeneration.java","additions":639,"deletions":0,"binary":false,"changes":639,"status":"added"},{"patch":"@@ -361,0 +361,5 @@\n+    public static final String CALL_OF = COMPOSITE_PREFIX + \"CALL_OF\" + POSTFIX;\n+    static {\n+        callOfNodes(CALL_OF, \"Call.*\");\n+    }\n+\n@@ -584,0 +589,86 @@\n+    public static final String G1_COMPARE_AND_EXCHANGE_N_WITH_BARRIER_FLAG = COMPOSITE_PREFIX + \"G1_COMPARE_AND_EXCHANGE_N_WITH_BARRIER_FLAG\" + POSTFIX;\n+    static {\n+        String regex = START + \"g1CompareAndExchangeN\\\\S*\" + MID + \"barrier\\\\(\\\\s*\" + IS_REPLACED + \"\\\\s*\\\\)\" + END;\n+        machOnly(G1_COMPARE_AND_EXCHANGE_N_WITH_BARRIER_FLAG, regex);\n+    }\n+\n+    public static final String G1_COMPARE_AND_EXCHANGE_P_WITH_BARRIER_FLAG = COMPOSITE_PREFIX + \"G1_COMPARE_AND_EXCHANGE_P_WITH_BARRIER_FLAG\" + POSTFIX;\n+    static {\n+        String regex = START + \"g1CompareAndExchangeP\\\\S*\" + MID + \"barrier\\\\(\\\\s*\" + IS_REPLACED + \"\\\\s*\\\\)\" + END;\n+        machOnly(G1_COMPARE_AND_EXCHANGE_P_WITH_BARRIER_FLAG, regex);\n+    }\n+\n+    public static final String G1_COMPARE_AND_SWAP_N_WITH_BARRIER_FLAG = COMPOSITE_PREFIX + \"G1_COMPARE_AND_SWAP_N_WITH_BARRIER_FLAG\" + POSTFIX;\n+    static {\n+        String regex = START + \"g1CompareAndSwapN\\\\S*\" + MID + \"barrier\\\\(\\\\s*\" + IS_REPLACED + \"\\\\s*\\\\)\" + END;\n+        machOnly(G1_COMPARE_AND_SWAP_N_WITH_BARRIER_FLAG, regex);\n+    }\n+\n+    public static final String G1_COMPARE_AND_SWAP_P_WITH_BARRIER_FLAG = COMPOSITE_PREFIX + \"G1_COMPARE_AND_SWAP_P_WITH_BARRIER_FLAG\" + POSTFIX;\n+    static {\n+        String regex = START + \"g1CompareAndSwapP\\\\S*\" + MID + \"barrier\\\\(\\\\s*\" + IS_REPLACED + \"\\\\s*\\\\)\" + END;\n+        machOnly(G1_COMPARE_AND_SWAP_P_WITH_BARRIER_FLAG, regex);\n+    }\n+\n+    public static final String G1_ENCODE_P_AND_STORE_N = PREFIX + \"G1_ENCODE_P_AND_STORE_N\" + POSTFIX;\n+    static {\n+        machOnlyNameRegex(G1_ENCODE_P_AND_STORE_N, \"g1EncodePAndStoreN\");\n+    }\n+\n+    public static final String G1_ENCODE_P_AND_STORE_N_WITH_BARRIER_FLAG = COMPOSITE_PREFIX + \"G1_ENCODE_P_AND_STORE_N_WITH_BARRIER_FLAG\" + POSTFIX;\n+    static {\n+        String regex = START + \"g1EncodePAndStoreN\\\\S*\" + MID + \"barrier\\\\(\\\\s*\" + IS_REPLACED + \"\\\\s*\\\\)\" + END;\n+        machOnly(G1_ENCODE_P_AND_STORE_N_WITH_BARRIER_FLAG, regex);\n+    }\n+\n+    public static final String G1_GET_AND_SET_N_WITH_BARRIER_FLAG = COMPOSITE_PREFIX + \"G1_GET_AND_SET_N_WITH_BARRIER_FLAG\" + POSTFIX;\n+    static {\n+        String regex = START + \"g1GetAndSetN\\\\S*\" + MID + \"barrier\\\\(\\\\s*\" + IS_REPLACED + \"\\\\s*\\\\)\" + END;\n+        machOnly(G1_GET_AND_SET_N_WITH_BARRIER_FLAG, regex);\n+    }\n+\n+    public static final String G1_GET_AND_SET_P_WITH_BARRIER_FLAG = COMPOSITE_PREFIX + \"G1_GET_AND_SET_P_WITH_BARRIER_FLAG\" + POSTFIX;\n+    static {\n+        String regex = START + \"g1GetAndSetP\\\\S*\" + MID + \"barrier\\\\(\\\\s*\" + IS_REPLACED + \"\\\\s*\\\\)\" + END;\n+        machOnly(G1_GET_AND_SET_P_WITH_BARRIER_FLAG, regex);\n+    }\n+\n+    public static final String G1_LOAD_N = PREFIX + \"G1_LOAD_N\" + POSTFIX;\n+    static {\n+        machOnlyNameRegex(G1_LOAD_N, \"g1LoadN\");\n+    }\n+\n+    public static final String G1_LOAD_N_WITH_BARRIER_FLAG = COMPOSITE_PREFIX + \"G1_LOAD_N_WITH_BARRIER_FLAG\" + POSTFIX;\n+    static {\n+        String regex = START + \"g1LoadN\\\\S*\" + MID + \"barrier\\\\(\\\\s*\" + IS_REPLACED + \"\\\\s*\\\\)\" + END;\n+        machOnly(G1_LOAD_N_WITH_BARRIER_FLAG, regex);\n+    }\n+\n+    public static final String G1_LOAD_P_WITH_BARRIER_FLAG = COMPOSITE_PREFIX + \"G1_LOAD_P_WITH_BARRIER_FLAG\" + POSTFIX;\n+    static {\n+        String regex = START + \"g1LoadP\\\\S*\" + MID + \"barrier\\\\(\\\\s*\" + IS_REPLACED + \"\\\\s*\\\\)\" + END;\n+        machOnly(G1_LOAD_P_WITH_BARRIER_FLAG, regex);\n+    }\n+\n+    public static final String G1_STORE_N = PREFIX + \"G1_STORE_N\" + POSTFIX;\n+    static {\n+        machOnlyNameRegex(G1_STORE_N, \"g1StoreN\");\n+    }\n+\n+    public static final String G1_STORE_N_WITH_BARRIER_FLAG = COMPOSITE_PREFIX + \"G1_STORE_N_WITH_BARRIER_FLAG\" + POSTFIX;\n+    static {\n+        String regex = START + \"g1StoreN\\\\S*\" + MID + \"barrier\\\\(\\\\s*\" + IS_REPLACED + \"\\\\s*\\\\)\" + END;\n+        machOnly(G1_STORE_N_WITH_BARRIER_FLAG, regex);\n+    }\n+\n+    public static final String G1_STORE_P = PREFIX + \"G1_STORE_P\" + POSTFIX;\n+    static {\n+        machOnlyNameRegex(G1_STORE_P, \"g1StoreP\");\n+    }\n+\n+    public static final String G1_STORE_P_WITH_BARRIER_FLAG = COMPOSITE_PREFIX + \"G1_STORE_P_WITH_BARRIER_FLAG\" + POSTFIX;\n+    static {\n+        String regex = START + \"g1StoreP\\\\S*\" + MID + \"barrier\\\\(\\\\s*\" + IS_REPLACED + \"\\\\s*\\\\)\" + END;\n+        machOnly(G1_STORE_P_WITH_BARRIER_FLAG, regex);\n+    }\n+\n@@ -855,0 +946,5 @@\n+    public static final String MACH_TEMP = PREFIX + \"MACH_TEMP\" + POSTFIX;\n+    static {\n+        machOnlyNameRegex(MACH_TEMP, \"MachTemp\");\n+    }\n+\n@@ -1151,0 +1247,6 @@\n+    public static final String OOPMAP_WITH = COMPOSITE_PREFIX + \"OOPMAP_WITH\" + POSTFIX;\n+    static {\n+        String regex = \"(#\\\\s*OopMap\\\\s*\\\\{.*\" + IS_REPLACED + \".*\\\\})\";\n+        optoOnly(OOPMAP_WITH, regex);\n+    }\n+\n","filename":"test\/hotspot\/jtreg\/compiler\/lib\/ir_framework\/IRNode.java","additions":102,"deletions":0,"binary":false,"changes":102,"status":"modified"},{"patch":"@@ -0,0 +1,98 @@\n+\/*\n+ * Copyright (c) 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+package compiler.runtime.safepoints;\n+\n+import compiler.lib.ir_framework.*;\n+import java.lang.ref.SoftReference;\n+\n+\/**\n+ * @test\n+ * @summary Test that undefined values generated by MachTemp nodes (in this\n+ *          case, derived from G1 barriers) are not included in OopMaps.\n+ *          Extracted from java.lang.invoke.LambdaFormEditor::getInCache.\n+ * @key randomness\n+ * @library \/test\/lib \/\n+ * @requires vm.gc.G1 & vm.bits == 64 & vm.opt.final.UseCompressedOops == true\n+ * @run driver compiler.runtime.safepoints.TestMachTempsAcrossSafepoints\n+ *\/\n+\n+public class TestMachTempsAcrossSafepoints {\n+\n+    static class RefWithKey extends SoftReference<Object> {\n+        final int key;\n+\n+        public RefWithKey(int key) {\n+            super(new Object());\n+            this.key = key;\n+        }\n+\n+        @DontInline\n+        @Override\n+        public boolean equals(Object obj) {\n+            return obj instanceof RefWithKey that && this.key == that.key;\n+        }\n+    }\n+\n+    public static void main(String[] args) throws Exception {\n+        String inlineCmd = \"-XX:CompileCommand=inline,java.lang.ref.SoftReference::get\";\n+        TestFramework.runWithFlags(inlineCmd, \"-XX:+StressGCM\", \"-XX:+StressLCM\", \"-XX:StressSeed=1\");\n+        TestFramework.runWithFlags(inlineCmd, \"-XX:+StressGCM\", \"-XX:+StressLCM\");\n+    }\n+\n+    @Test\n+    @IR(counts = {IRNode.G1_LOAD_N, \"1\"}, phase = CompilePhase.FINAL_CODE)\n+    @IR(counts = {IRNode.MACH_TEMP, \">= 1\"}, phase = CompilePhase.FINAL_CODE)\n+    @IR(counts = {IRNode.STATIC_CALL_OF_METHOD, \"equals\", \"2\"})\n+    @IR(failOn = {IRNode.OOPMAP_WITH, \"NarrowOop\"})\n+    static private Object test(RefWithKey key, RefWithKey[] refs) {\n+        RefWithKey k = null;\n+        \/\/ This loop causes the register allocator to not \"rematerialize\" all\n+        \/\/ MachTemp nodes generated for the reference g1LoadN instruction below.\n+        for (int i = 0; i < refs.length; i++) {\n+            RefWithKey k0 = refs[0];\n+            if (k0.equals(key)) {\n+                k = k0;\n+            }\n+        }\n+        if (k != null && !key.equals(k)) {\n+            return null;\n+        }\n+        \/\/ The MachTemp node implementing the dst TEMP operand in the g1LoadN\n+        \/\/ instruction corresponding to k.get() can be scheduled across the\n+        \/\/ above call to RefWithKey::equals(), due to an unfortunate interaction\n+        \/\/ of inaccurate basic block frequency estimation (emulated in this test\n+        \/\/ by randomizing the GCM and LCM heuristics) and call-catch cleanup.\n+        \/\/ Since narrow pointer MachTemp nodes are typed as narrow OOPs, this\n+        \/\/ causes the oopmap builder to include the MachTemp node definition in\n+        \/\/ the RefWithKey::equals() return oopmap.\n+        return (k != null) ? k.get() : null;\n+    }\n+\n+    @Run(test = \"test\")\n+    @Warmup(0)\n+    public void run() {\n+        RefWithKey ref = new RefWithKey(42);\n+        test(ref, new RefWithKey[]{ref});\n+    }\n+}\n","filename":"test\/hotspot\/jtreg\/compiler\/runtime\/safepoints\/TestMachTempsAcrossSafepoints.java","additions":98,"deletions":0,"binary":false,"changes":98,"status":"added"},{"patch":"@@ -307,1 +307,4 @@\n-                \"-XX:StressSeed=\" + rng.nextInt(Integer.MAX_VALUE)));\n+                \"-XX:StressSeed=\" + rng.nextInt(Integer.MAX_VALUE),\n+                \/\/ Do not fail on huge methods where StressGCM makes register\n+                \/\/ allocation allocate lots of memory\n+                \"-XX:CompileCommand=memlimit,*.*,0\"));\n","filename":"test\/hotspot\/jtreg\/testlibrary\/ctw\/src\/sun\/hotspot\/tools\/ctw\/CtwRunner.java","additions":4,"deletions":1,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -27,1 +27,1 @@\n- * @run junit\/othervm\/timeout=2500 -XX:+IgnoreUnrecognizedVMOptions -XX:-VerifyDependencies -esa -DBigArityTest.ITERATION_COUNT=1 test.java.lang.invoke.BigArityTest\n+ * @run junit\/othervm\/timeout=2500 -XX:+IgnoreUnrecognizedVMOptions -XX:-VerifyDependencies -XX:CompileCommand=memlimit,*.*,0 -esa -DBigArityTest.ITERATION_COUNT=1 test.java.lang.invoke.BigArityTest\n","filename":"test\/jdk\/java\/lang\/invoke\/BigArityTest.java","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"}]}