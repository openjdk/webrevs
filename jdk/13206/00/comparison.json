{"files":[{"patch":"@@ -58,1 +58,0 @@\n-  _cur_covered_regions = 1;\n","filename":"src\/hotspot\/share\/gc\/g1\/g1CardTable.cpp","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -118,1 +118,0 @@\n-  void initialize() override {}\n@@ -121,2 +120,0 @@\n-  void resize_covered_region(MemRegion new_region) override { ShouldNotReachHere(); }\n-\n","filename":"src\/hotspot\/share\/gc\/g1\/g1CardTable.hpp","additions":0,"deletions":3,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -1542,1 +1542,0 @@\n-  ct->initialize();\n","filename":"src\/hotspot\/share\/gc\/g1\/g1CollectedHeap.cpp","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -72,0 +72,4 @@\n+  \/\/ Layout the reserved space for the generations.\n+  ReservedSpace old_rs   = heap_rs.first_part(MaxOldSize);\n+  ReservedSpace young_rs = heap_rs.last_part(MaxOldSize);\n+  assert(young_rs.size() == MaxNewSize, \"Didn't reserve all of the heap\");\n@@ -74,1 +78,2 @@\n-  card_table->initialize();\n+  card_table->initialize(old_rs.base(), young_rs.base());\n+\n@@ -79,9 +84,0 @@\n-  \/\/ Make up the generations\n-  assert(MinOldSize <= OldSize && OldSize <= MaxOldSize, \"Parameter check\");\n-  assert(MinNewSize <= NewSize && NewSize <= MaxNewSize, \"Parameter check\");\n-\n-  \/\/ Layout the reserved space for the generations.\n-  ReservedSpace old_rs   = heap_rs.first_part(MaxOldSize);\n-  ReservedSpace young_rs = heap_rs.last_part(MaxOldSize);\n-  assert(young_rs.size() == MaxNewSize, \"Didn't reserve all of the heap\");\n-\n","filename":"src\/hotspot\/share\/gc\/parallel\/parallelScavengeHeap.cpp","additions":6,"deletions":10,"binary":false,"changes":16,"status":"modified"},{"patch":"@@ -76,1 +76,0 @@\n-  _cur_covered_regions(0),\n@@ -90,1 +89,1 @@\n-void CardTable::initialize() {\n+void CardTable::initialize(void* region0_start, void* region1_start) {\n@@ -100,2 +99,0 @@\n-  _cur_covered_regions = 0;\n-\n@@ -128,0 +125,2 @@\n+  initialize_covered_region(region0_start, region1_start);\n+\n@@ -134,24 +133,3 @@\n-int CardTable::find_covering_region_by_base(HeapWord* base) {\n-  int i;\n-  for (i = 0; i < _cur_covered_regions; i++) {\n-    if (_covered[i].start() == base) return i;\n-    if (_covered[i].start() > base) break;\n-  }\n-  \/\/ If we didn't find it, create a new one.\n-  assert(_cur_covered_regions < _max_covered_regions,\n-         \"too many covered regions\");\n-  \/\/ Move the ones above up, to maintain sorted order.\n-  for (int j = _cur_covered_regions; j > i; j--) {\n-    _covered[j] = _covered[j-1];\n-    _committed[j] = _committed[j-1];\n-  }\n-  int res = i;\n-  _cur_covered_regions++;\n-  _covered[res].set_start(base);\n-  _covered[res].set_word_size(0);\n-  CardValue* ct_start = byte_for(base);\n-  HeapWord* ct_start_aligned = align_down((HeapWord*)ct_start, _page_size);\n-  _committed[res].set_start(ct_start_aligned);\n-  _committed[res].set_word_size(0);\n-  return res;\n-}\n+void CardTable::initialize_covered_region(void* region0_start, void* region1_start) {\n+  assert(_whole_heap.start() == region0_start, \"precondition\");\n+  assert(region0_start < region1_start, \"precondition\");\n@@ -159,8 +137,2 @@\n-HeapWord* CardTable::largest_prev_committed_end(int ind) const {\n-  HeapWord* max_end = nullptr;\n-  for (int j = 0; j < ind; j++) {\n-    HeapWord* this_end = _committed[j].end();\n-    if (this_end > max_end) max_end = this_end;\n-  }\n-  return max_end;\n-}\n+  assert(_covered[0].start() == nullptr, \"precondition\");\n+  assert(_committed[0].start() == nullptr, \"precondition\");\n@@ -168,9 +140,8 @@\n-MemRegion CardTable::committed_unique_to_self(int self, MemRegion mr) const {\n-  assert(mr.intersection(_guard_region).is_empty(), \"precondition\");\n-  MemRegion result = mr;\n-  for (int r = 0; r < _cur_covered_regions; r += 1) {\n-    if (r != self) {\n-      result = result.minus(_committed[r]);\n-    }\n-  }\n-  return result;\n+  assert(_covered[1].start() == nullptr, \"precondition\");\n+  assert(_committed[1].start() == nullptr, \"precondition\");\n+\n+  _covered[0] = MemRegion((HeapWord*)region0_start, (size_t)0);\n+  _committed[0] = MemRegion((HeapWord*)align_down(byte_for(region0_start), _page_size), (size_t)0);\n+\n+  _covered[1] = MemRegion((HeapWord*)region1_start, (size_t)0);\n+  _committed[1] = MemRegion((HeapWord*)align_down(byte_for(region1_start), _page_size), (size_t)0);\n@@ -180,1 +151,1 @@\n-  \/\/ We don't change the start of a region, only the end.\n+  assert(UseSerialGC || UseParallelGC, \"only these two collectors\");\n@@ -183,105 +154,1 @@\n-  \/\/ collided is true if the expansion would push into another committed region\n-  debug_only(bool collided = false;)\n-  int const ind = find_covering_region_by_base(new_region.start());\n-  MemRegion const old_region = _covered[ind];\n-  assert(old_region.start() == new_region.start(), \"just checking\");\n-  if (new_region.word_size() != old_region.word_size()) {\n-    \/\/ Commit new or uncommit old pages, if necessary.\n-    MemRegion cur_committed = _committed[ind];\n-    \/\/ Extend the end of this _committed region\n-    \/\/ to cover the end of any lower _committed regions.\n-    \/\/ This forms overlapping regions, but never interior regions.\n-    HeapWord* const max_prev_end = largest_prev_committed_end(ind);\n-    if (max_prev_end > cur_committed.end()) {\n-      cur_committed.set_end(max_prev_end);\n-    }\n-    \/\/ Align the end up to a page size (starts are already aligned).\n-    HeapWord* new_end = (HeapWord*) byte_after(new_region.last());\n-    HeapWord* new_end_aligned = align_up(new_end, _page_size);\n-    assert(new_end_aligned >= new_end, \"align up, but less\");\n-    \/\/ Check the other regions (excludes \"ind\") to ensure that\n-    \/\/ the new_end_aligned does not intrude onto the committed\n-    \/\/ space of another region.\n-    int ri = 0;\n-    for (ri = ind + 1; ri < _cur_covered_regions; ri++) {\n-      if (new_end_aligned > _committed[ri].start()) {\n-        assert(new_end_aligned <= _committed[ri].end(),\n-               \"An earlier committed region can't cover a later committed region\");\n-        \/\/ Any region containing the new end\n-        \/\/ should start at or beyond the region found (ind)\n-        \/\/ for the new end (committed regions are not expected to\n-        \/\/ be proper subsets of other committed regions).\n-        assert(_committed[ri].start() >= _committed[ind].start(),\n-               \"New end of committed region is inconsistent\");\n-        new_end_aligned = _committed[ri].start();\n-        \/\/ new_end_aligned can be equal to the start of its\n-        \/\/ committed region (i.e., of \"ind\") if a second\n-        \/\/ region following \"ind\" also start at the same location\n-        \/\/ as \"ind\".\n-        assert(new_end_aligned >= _committed[ind].start(),\n-          \"New end of committed region is before start\");\n-        debug_only(collided = true;)\n-        \/\/ Should only collide with 1 region\n-        break;\n-      }\n-    }\n-#ifdef ASSERT\n-    for (++ri; ri < _cur_covered_regions; ri++) {\n-      assert(!_committed[ri].contains(new_end_aligned),\n-        \"New end of committed region is in a second committed region\");\n-    }\n-#endif\n-    \/\/ The guard page is always committed and should not be committed over.\n-    \/\/ \"guarded\" is used for assertion checking below and recalls the fact\n-    \/\/ that the would-be end of the new committed region would have\n-    \/\/ penetrated the guard page.\n-    HeapWord* new_end_for_commit = new_end_aligned;\n-\n-    DEBUG_ONLY(bool guarded = false;)\n-    if (new_end_for_commit > _guard_region.start()) {\n-      new_end_for_commit = _guard_region.start();\n-      DEBUG_ONLY(guarded = true;)\n-    }\n-\n-    if (new_end_for_commit > cur_committed.end()) {\n-      \/\/ Must commit new pages.\n-      MemRegion const new_committed =\n-        MemRegion(cur_committed.end(), new_end_for_commit);\n-\n-      assert(!new_committed.is_empty(), \"Region should not be empty here\");\n-      os::commit_memory_or_exit((char*)new_committed.start(),\n-                                new_committed.byte_size(), _page_size,\n-                                !ExecMem, \"card table expansion\");\n-    \/\/ Use new_end_aligned (as opposed to new_end_for_commit) because\n-    \/\/ the cur_committed region may include the guard region.\n-    } else if (new_end_aligned < cur_committed.end()) {\n-      \/\/ Must uncommit pages.\n-      MemRegion const uncommit_region =\n-        committed_unique_to_self(ind, MemRegion(new_end_aligned,\n-                                                cur_committed.end()));\n-      if (!uncommit_region.is_empty()) {\n-        if (!os::uncommit_memory((char*)uncommit_region.start(),\n-                                 uncommit_region.byte_size())) {\n-          assert(false, \"Card table contraction failed\");\n-          \/\/ The call failed so don't change the end of the\n-          \/\/ committed region.  This is better than taking the\n-          \/\/ VM down.\n-          new_end_aligned = _committed[ind].end();\n-        }\n-      }\n-    }\n-    \/\/ In any case, we can reset the end of the current committed entry.\n-    _committed[ind].set_end(new_end_aligned);\n-\n-#ifdef ASSERT\n-    \/\/ Check that the last card in the new region is committed according\n-    \/\/ to the tables.\n-    bool covered = false;\n-    for (int cr = 0; cr < _cur_covered_regions; cr++) {\n-      if (_committed[cr].contains(new_end - 1)) {\n-        covered = true;\n-        break;\n-      }\n-    }\n-    assert(covered, \"Card for end of new region not committed\");\n-#endif\n+  int idx = new_region.start() == _whole_heap.start() ? 0 : 1;\n@@ -289,31 +156,21 @@\n-    \/\/ The default of 0 is not necessarily clean cards.\n-    CardValue* entry;\n-    if (old_region.last() < _whole_heap.start()) {\n-      entry = byte_for(_whole_heap.start());\n-    } else {\n-      entry = byte_after(old_region.last());\n-    }\n-    assert(index_for(new_region.last()) <=  last_valid_index(),\n-      \"The guard card will be overwritten\");\n-    \/\/ This line commented out cleans the newly expanded region and\n-    \/\/ not the aligned up expanded region.\n-    \/\/ CardValue* const end = byte_after(new_region.last());\n-    CardValue* const end = (CardValue*) new_end_for_commit;\n-    assert((end >= byte_after(new_region.last())) || collided || guarded,\n-      \"Expect to be beyond new region unless impacting another region\");\n-    \/\/ do nothing if we resized downward.\n-#ifdef ASSERT\n-    for (int ri = 0; ri < _cur_covered_regions; ri++) {\n-      if (ri != ind) {\n-        \/\/ The end of the new committed region should not\n-        \/\/ be in any existing region unless it matches\n-        \/\/ the start of the next region.\n-        assert(!_committed[ri].contains(end) ||\n-               (_committed[ri].start() == (HeapWord*) end),\n-               \"Overlapping committed regions\");\n-      }\n-    }\n-#endif\n-    if (entry < end) {\n-      memset(entry, clean_card, pointer_delta(end, entry, sizeof(CardValue)));\n-    }\n+  assert(_covered[idx].start() != nullptr, \"precondition\");\n+  assert(_committed[idx].start() != nullptr, \"precondition\");\n+\n+  \/\/ We don't change the start of a region, only the end.\n+  assert(_covered[idx].start() == new_region.start(), \"inv\");\n+  assert(_committed[idx].start() == (HeapWord*)align_down(byte_for(new_region.start()), _page_size), \"inv\");\n+\n+  _covered[idx] = new_region;\n+\n+  HeapWord* addr_l = (HeapWord*)align_down(byte_for(new_region.start()), _page_size);\n+  HeapWord* addr_r = (HeapWord*)align_up(byte_after(new_region.last()), _page_size);\n+\n+  if (idx == 0) {\n+    \/\/ in case the card for gen-boundary is not page-size aligned\n+    addr_r = MIN2(addr_r, _committed[1].start());\n+  }\n+\n+  MemRegion new_committed = MemRegion(addr_l, addr_r);\n+\n+  if (new_committed.word_size() == _committed[idx].word_size()) {\n+    return;\n@@ -321,2 +178,23 @@\n-  \/\/ In any case, the covered size changes.\n-  _covered[ind].set_word_size(new_region.word_size());\n+\n+  if (new_committed.word_size() > _committed[idx].word_size()) {\n+    \/\/ expand\n+    MemRegion delta = MemRegion(_committed[idx].end(),\n+                                new_committed.word_size() - _committed[idx].word_size());\n+\n+    os::commit_memory_or_exit((char*)delta.start(),\n+                              delta.byte_size(),\n+                              _page_size,\n+                              !ExecMem,\n+                              \"card table expansion\");\n+\n+    memset(delta.start(), clean_card, delta.byte_size());\n+  } else {\n+    \/\/ shrink\n+    MemRegion delta = MemRegion(new_committed.end(),\n+                                _committed[idx].word_size() - new_committed.word_size());\n+    bool res = os::uncommit_memory((char*)delta.start(),\n+                                   delta.byte_size());\n+    assert(res, \"uncommit should succeed\");\n+  }\n+\n+  _committed[idx] = new_committed;\n@@ -326,1 +204,1 @@\n-                         ind, p2i(_covered[ind].start()), ind, p2i(_covered[ind].last()));\n+                         idx, p2i(_covered[idx].start()), idx, p2i(_covered[idx].last()));\n@@ -328,1 +206,1 @@\n-                         ind, p2i(_committed[ind].start()), ind, p2i(_committed[ind].last()));\n+                         idx, p2i(_committed[idx].start()), idx, p2i(_committed[idx].last()));\n@@ -330,1 +208,1 @@\n-                         p2i(byte_for(_covered[ind].start())),  p2i(byte_for(_covered[ind].last())));\n+                         p2i(byte_for(_covered[idx].start())),  p2i(byte_for(_covered[idx].last())));\n@@ -332,1 +210,1 @@\n-                         p2i(addr_for((CardValue*) _committed[ind].start())),  p2i(addr_for((CardValue*) _committed[ind].last())));\n+                         p2i(addr_for((CardValue*) _committed[idx].start())),  p2i(addr_for((CardValue*) _committed[idx].last())));\n@@ -336,1 +214,1 @@\n-  debug_only((void) (*byte_for(_covered[ind].last()));)\n+  debug_only((void) (*byte_for(_covered[idx].last()));)\n@@ -374,1 +252,1 @@\n-  for (int i = 0; i < _cur_covered_regions; i++) {\n+  for (int i = 0; i < _max_covered_regions; i++) {\n","filename":"src\/hotspot\/share\/gc\/shared\/cardTable.cpp","additions":68,"deletions":190,"binary":false,"changes":258,"status":"modified"},{"patch":"@@ -52,1 +52,6 @@\n-  int _cur_covered_regions;\n+  \/\/ Some barrier sets create tables whose elements correspond to parts of\n+  \/\/ the heap; the CardTableBarrierSet is an example.  Such barrier sets will\n+  \/\/ normally reserve space for such tables, and commit parts of the table\n+  \/\/ \"covering\" parts of the heap that are committed. At most one covered\n+  \/\/ region per generation is needed.\n+  static constexpr int _max_covered_regions = 2;\n@@ -69,24 +74,0 @@\n-  \/\/ Finds and return the index of the region, if any, to which the given\n-  \/\/ region would be contiguous.  If none exists, assign a new region and\n-  \/\/ returns its index.  Requires that no more than the maximum number of\n-  \/\/ covered regions defined in the constructor are ever in use.\n-  int find_covering_region_by_base(HeapWord* base);\n-\n-  \/\/ Returns the leftmost end of a committed region corresponding to a\n-  \/\/ covered region before covered region \"ind\", or else \"null\" if \"ind\" is\n-  \/\/ the first covered region.\n-  HeapWord* largest_prev_committed_end(int ind) const;\n-\n-  \/\/ Returns the part of the region mr that doesn't intersect with\n-  \/\/ any committed region other than self.  Used to prevent uncommitting\n-  \/\/ regions that are also committed by other regions.  Also protects\n-  \/\/ against uncommitting the guard region.\n-  MemRegion committed_unique_to_self(int self, MemRegion mr) const;\n-\n-  \/\/ Some barrier sets create tables whose elements correspond to parts of\n-  \/\/ the heap; the CardTableBarrierSet is an example.  Such barrier sets will\n-  \/\/ normally reserve space for such tables, and commit parts of the table\n-  \/\/ \"covering\" parts of the heap that are committed. At most one covered\n-  \/\/ region per generation is needed.\n-  static const int _max_covered_regions = 2;\n-\n@@ -111,0 +92,3 @@\n+private:\n+  void initialize_covered_region(void* region0_start, void* region1_start);\n+\n@@ -114,1 +98,1 @@\n-  virtual void initialize();\n+  void initialize(void* region0_start, void* region1_start);\n@@ -199,1 +183,1 @@\n-  virtual void resize_covered_region(MemRegion new_region);\n+  void resize_covered_region(MemRegion new_region);\n","filename":"src\/hotspot\/share\/gc\/shared\/cardTable.hpp","additions":11,"deletions":27,"binary":false,"changes":38,"status":"modified"},{"patch":"@@ -120,0 +120,3 @@\n+  ReservedSpace young_rs = heap_rs.first_part(_young_gen_spec->max_size());\n+  ReservedSpace old_rs = heap_rs.last_part(_young_gen_spec->max_size());\n+\n@@ -121,1 +124,2 @@\n-  _rem_set->initialize();\n+  _rem_set->initialize(young_rs.base(), old_rs.base());\n+\n@@ -126,1 +130,0 @@\n-  ReservedSpace young_rs = heap_rs.first_part(_young_gen_spec->max_size());\n@@ -128,3 +131,0 @@\n-  ReservedSpace old_rs = heap_rs.last_part(_young_gen_spec->max_size());\n-\n-  old_rs = old_rs.first_part(_old_gen_spec->max_size());\n","filename":"src\/hotspot\/share\/gc\/shared\/genCollectedHeap.cpp","additions":5,"deletions":5,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -93,1 +93,0 @@\n-  nonstatic_field(CardTable,                   _cur_covered_regions,                          int)                                   \\\n","filename":"src\/hotspot\/share\/gc\/shared\/vmStructs_gc.hpp","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"}]}