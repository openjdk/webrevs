{"files":[{"patch":"@@ -36,0 +36,1 @@\n+#include \"opto\/traceMergeStoresTag.hpp\"\n@@ -305,1 +306,2 @@\n-  _trace_auto_vectorization_tags(TRACE_AUTO_VECTORIZATION_TAG_NUM, mtCompiler)\n+  _trace_auto_vectorization_tags(TRACE_AUTO_VECTORIZATION_TAG_NUM, mtCompiler),\n+  _trace_merge_stores_tags(TraceMergeStores::TAG_NUM, mtCompiler)\n@@ -435,1 +437,0 @@\n-    \/\/ Parse PrintIdealPhaseName and create a lookup set\n@@ -448,0 +449,11 @@\n+    if (!_modified[TraceMergeStoresIndex]) {\n+      \/\/ Parse ccstr and create mask\n+      ccstrlist option;\n+      if (CompilerOracle::has_option_value(method, CompileCommandEnum::TraceMergeStores, option)) {\n+        TraceMergeStores::TagValidator validator(option, false);\n+        if (validator.is_valid()) {\n+          set.cloned()->set_trace_merge_stores_tags(validator.tags());\n+        }\n+      }\n+    }\n+    \/\/ Parse PrintIdealPhaseName and create a lookup set\n","filename":"src\/hotspot\/share\/compiler\/compilerDirectives.cpp","additions":14,"deletions":2,"binary":false,"changes":16,"status":"modified"},{"patch":"@@ -93,0 +93,1 @@\n+NOT_PRODUCT(cflags(TraceMergeStores, ccstrlist, \"\", TraceMergeStores)) \\\n@@ -134,0 +135,1 @@\n+  CHeapBitMap _trace_merge_stores_tags;\n@@ -214,0 +216,6 @@\n+  void set_trace_merge_stores_tags(const CHeapBitMap& tags) {\n+    _trace_merge_stores_tags.set_from(tags);\n+  };\n+  const CHeapBitMap& trace_merge_stores_tags() {\n+    return _trace_merge_stores_tags;\n+  };\n","filename":"src\/hotspot\/share\/compiler\/compilerDirectives.hpp","additions":8,"deletions":0,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -39,0 +39,1 @@\n+#include \"opto\/traceMergeStoresTag.hpp\"\n@@ -805,0 +806,6 @@\n+        if (!validator.is_valid()) {\n+          jio_snprintf(errorbuf, buf_size, \"Unrecognized tag name in %s: %s\", option2name(option), validator.what());\n+        }\n+      } else if (option == CompileCommandEnum::TraceMergeStores) {\n+        TraceMergeStores::TagValidator validator(value, true);\n+\n","filename":"src\/hotspot\/share\/compiler\/compilerOracle.cpp","additions":7,"deletions":0,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -89,0 +89,1 @@\n+NOT_PRODUCT(option(TraceMergeStores, \"TraceMergeStores\", Ccstrlist)) \\\n","filename":"src\/hotspot\/share\/compiler\/compilerOracle.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -32,0 +32,1 @@\n+#include \"opto\/traceMergeStoresTag.hpp\"\n@@ -350,0 +351,9 @@\n+        } else if (strncmp(option_key->name, \"TraceMergeStores\", 16) == 0) {\n+          TraceMergeStores::TagValidator validator(s, false);\n+\n+          valid = validator.is_valid();\n+          if (valid) {\n+            set->set_trace_merge_stores_tags(validator.tags());\n+          } else {\n+            error(VALUE_ERROR, \"Unrecognized tag name detected in TraceMergeStores: %s\", validator.what());\n+          }\n","filename":"src\/hotspot\/share\/compiler\/directivesParser.cpp","additions":10,"deletions":0,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -370,3 +370,0 @@\n-  develop(bool, TraceMergeStores, false,                                    \\\n-          \"Trace creation of merged stores\")                                \\\n-                                                                            \\\n","filename":"src\/hotspot\/share\/opto\/c2_globals.hpp","additions":0,"deletions":3,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -46,0 +46,1 @@\n+#include \"opto\/mempointer.hpp\"\n@@ -51,0 +52,1 @@\n+#include \"opto\/traceMergeStoresTag.hpp\"\n@@ -2741,178 +2743,0 @@\n-\/\/ Class to parse array pointers, and determine if they are adjacent. We parse the form:\n-\/\/\n-\/\/   pointer =   base\n-\/\/             + constant_offset\n-\/\/             + LShiftL( ConvI2L(int_offset + int_con), int_offset_shift)\n-\/\/             + sum(other_offsets)\n-\/\/\n-\/\/\n-\/\/ Note: we accumulate all constant offsets into constant_offset, even the int constant behind\n-\/\/       the \"LShiftL(ConvI2L(...))\" pattern. We convert \"ConvI2L(int_offset + int_con)\" to\n-\/\/       \"ConvI2L(int_offset) + int_con\", which is only safe if we can assume that either all\n-\/\/       compared addresses have an overflow for \"int_offset + int_con\" or none.\n-\/\/       For loads and stores on arrays, we know that if one overflows and the other not, then\n-\/\/       the two addresses lay almost max_int indices apart, but the maximal array size is\n-\/\/       only about half of that. Therefore, the RangeCheck on at least one of them must have\n-\/\/       failed.\n-\/\/\n-\/\/   constant_offset += LShiftL( ConvI2L(int_con), int_offset_shift)\n-\/\/\n-\/\/   pointer =   base\n-\/\/             + constant_offset\n-\/\/             + LShiftL( ConvI2L(int_offset), int_offset_shift)\n-\/\/             + sum(other_offsets)\n-\/\/\n-class ArrayPointer {\n-private:\n-  const Node* _pointer;          \/\/ The final pointer to the position in the array\n-  const Node* _base;             \/\/ Base address of the array\n-  const jlong _constant_offset;  \/\/ Sum of collected constant offsets\n-  const Node* _int_offset;       \/\/ (optional) Offset behind LShiftL and ConvI2L\n-  const GrowableArray<Node*>* _other_offsets; \/\/ List of other AddP offsets\n-  const jint _int_offset_shift; \/\/ (optional) Shift value for int_offset\n-  const bool _is_valid;          \/\/ The parsing succeeded\n-\n-  ArrayPointer(const bool is_valid,\n-               const Node* pointer,\n-               const Node* base,\n-               const jlong constant_offset,\n-               const Node* int_offset,\n-               const jint int_offset_shift,\n-               const GrowableArray<Node*>* other_offsets) :\n-      _pointer(pointer),\n-      _base(base),\n-      _constant_offset(constant_offset),\n-      _int_offset(int_offset),\n-      _other_offsets(other_offsets),\n-      _int_offset_shift(int_offset_shift),\n-      _is_valid(is_valid)\n-  {\n-    assert(_pointer != nullptr, \"must always have pointer\");\n-    assert(is_valid == (_base != nullptr), \"have base exactly if valid\");\n-    assert(is_valid == (_other_offsets != nullptr), \"have other_offsets exactly if valid\");\n-  }\n-\n-  static ArrayPointer make_invalid(const Node* pointer) {\n-    return ArrayPointer(false, pointer, nullptr, 0, nullptr, 0, nullptr);\n-  }\n-\n-  static bool parse_int_offset(Node* offset, Node*& int_offset, jint& int_offset_shift) {\n-    \/\/ offset = LShiftL( ConvI2L(int_offset), int_offset_shift)\n-    if (offset->Opcode() == Op_LShiftL &&\n-        offset->in(1)->Opcode() == Op_ConvI2L &&\n-        offset->in(2)->Opcode() == Op_ConI) {\n-      int_offset = offset->in(1)->in(1); \/\/ LShiftL -> ConvI2L -> int_offset\n-      int_offset_shift = offset->in(2)->get_int(); \/\/ LShiftL -> int_offset_shift\n-      return true;\n-    }\n-\n-    \/\/ offset = ConvI2L(int_offset) = LShiftL( ConvI2L(int_offset), 0)\n-    if (offset->Opcode() == Op_ConvI2L) {\n-      int_offset = offset->in(1);\n-      int_offset_shift = 0;\n-      return true;\n-    }\n-\n-    \/\/ parse failed\n-    return false;\n-  }\n-\n-public:\n-  \/\/ Parse the structure above the pointer\n-  static ArrayPointer make(PhaseGVN* phase, const Node* pointer) {\n-    assert(phase->type(pointer)->isa_aryptr() != nullptr, \"must be array pointer\");\n-    if (!pointer->is_AddP()) { return ArrayPointer::make_invalid(pointer); }\n-\n-    const Node* base = pointer->in(AddPNode::Base);\n-    if (base == nullptr) { return ArrayPointer::make_invalid(pointer); }\n-\n-    const int search_depth = 5;\n-    Node* offsets[search_depth];\n-    int count = pointer->as_AddP()->unpack_offsets(offsets, search_depth);\n-\n-    \/\/ We expect at least a constant each\n-    if (count <= 0) { return ArrayPointer::make_invalid(pointer); }\n-\n-    \/\/ We extract the form:\n-    \/\/\n-    \/\/   pointer =   base\n-    \/\/             + constant_offset\n-    \/\/             + LShiftL( ConvI2L(int_offset + int_con), int_offset_shift)\n-    \/\/             + sum(other_offsets)\n-    \/\/\n-    jlong constant_offset = 0;\n-    Node* int_offset = nullptr;\n-    jint int_offset_shift = 0;\n-    GrowableArray<Node*>* other_offsets = new GrowableArray<Node*>(count);\n-\n-    for (int i = 0; i < count; i++) {\n-      Node* offset = offsets[i];\n-      if (offset->Opcode() == Op_ConI) {\n-        \/\/ Constant int offset\n-        constant_offset += offset->get_int();\n-      } else if (offset->Opcode() == Op_ConL) {\n-        \/\/ Constant long offset\n-        constant_offset += offset->get_long();\n-      } else if(int_offset == nullptr && parse_int_offset(offset, int_offset, int_offset_shift)) {\n-        \/\/ LShiftL( ConvI2L(int_offset), int_offset_shift)\n-        int_offset = int_offset->uncast();\n-        if (int_offset->Opcode() == Op_AddI && int_offset->in(2)->Opcode() == Op_ConI) {\n-          \/\/ LShiftL( ConvI2L(int_offset + int_con), int_offset_shift)\n-          constant_offset += ((jlong)int_offset->in(2)->get_int()) << int_offset_shift;\n-          int_offset = int_offset->in(1);\n-        }\n-      } else {\n-        \/\/ All others\n-        other_offsets->append(offset);\n-      }\n-    }\n-\n-    return ArrayPointer(true, pointer, base, constant_offset, int_offset, int_offset_shift, other_offsets);\n-  }\n-\n-  bool is_adjacent_to_and_before(const ArrayPointer& other, const jlong data_size) const {\n-    if (!_is_valid || !other._is_valid) { return false; }\n-\n-    \/\/ Offset adjacent?\n-    if (this->_constant_offset + data_size != other._constant_offset) { return false; }\n-\n-    \/\/ All other components identical?\n-    if (this->_base != other._base ||\n-        this->_int_offset != other._int_offset ||\n-        this->_int_offset_shift != other._int_offset_shift ||\n-        this->_other_offsets->length() != other._other_offsets->length()) {\n-      return false;\n-    }\n-\n-    for (int i = 0; i < this->_other_offsets->length(); i++) {\n-      Node* o1 = this->_other_offsets->at(i);\n-      Node* o2 = other._other_offsets->at(i);\n-      if (o1 != o2) { return false; }\n-    }\n-\n-    return true;\n-  }\n-\n-#ifndef PRODUCT\n-  void dump() {\n-    if (!_is_valid) {\n-      tty->print(\"ArrayPointer[%d %s, invalid]\", _pointer->_idx, _pointer->Name());\n-      return;\n-    }\n-    tty->print(\"ArrayPointer[%d %s, base[%d %s] + %lld\",\n-               _pointer->_idx, _pointer->Name(),\n-               _base->_idx, _base->Name(),\n-               (long long)_constant_offset);\n-    if (_int_offset != nullptr) {\n-      tty->print(\" + I2L[%d %s] << %d\",\n-                 _int_offset->_idx, _int_offset->Name(), _int_offset_shift);\n-    }\n-    for (int i = 0; i < _other_offsets->length(); i++) {\n-      Node* n = _other_offsets->at(i);\n-      tty->print(\" + [%d %s]\", n->_idx, n->Name());\n-    }\n-    tty->print_cr(\"]\");\n-  }\n-#endif\n-};\n-\n@@ -2954,1 +2778,1 @@\n-class MergePrimitiveArrayStores : public StackObj {\n+class MergePrimitiveStores : public StackObj {\n@@ -2959,0 +2783,2 @@\n+  NOT_PRODUCT( const CHeapBitMap &_trace_tags; )\n+\n@@ -2960,1 +2786,4 @@\n-  MergePrimitiveArrayStores(PhaseGVN* phase, StoreNode* store) : _phase(phase), _store(store) {}\n+  MergePrimitiveStores(PhaseGVN* phase, StoreNode* store) :\n+    _phase(phase), _store(store)\n+    NOT_PRODUCT( COMMA _trace_tags(Compile::current()->directive()->trace_merge_stores_tags()) )\n+    {}\n@@ -2991,0 +2820,11 @@\n+\n+#ifndef PRODUCT\n+    void print_on(outputStream* st) const {\n+      if (_found_store == nullptr) {\n+        st->print_cr(\"None\");\n+      } else {\n+        st->print_cr(\"Found[%d %s, %s]\", _found_store->_idx, _found_store->Name(),\n+                                         _found_range_check ? \"RC\" : \"no-RC\");\n+      }\n+    }\n+#endif\n@@ -3004,2 +2844,5 @@\n-  DEBUG_ONLY( void trace(const Node_List& merge_list, const Node* merged_input_value, const StoreNode* merged_store) const; )\n-};\n+#ifndef PRODUCT\n+  \/\/ Access to TraceMergeStores tags\n+  bool is_trace(TraceMergeStores::Tag tag) const {\n+    return _trace_tags.at(tag);\n+  }\n@@ -3007,5 +2850,2 @@\n-StoreNode* MergePrimitiveArrayStores::run() {\n-  \/\/ Check for B\/S\/C\/I\n-  int opc = _store->Opcode();\n-  if (opc != Op_StoreB && opc != Op_StoreC && opc != Op_StoreI) {\n-    return nullptr;\n+  bool is_trace_basic() const {\n+    return is_trace(TraceMergeStores::Tag::BASIC);\n@@ -3014,4 +2854,2 @@\n-  \/\/ Only merge stores on arrays, and the stores must have the same size as the elements.\n-  const TypePtr* ptr_t = _store->adr_type();\n-  if (ptr_t == nullptr) {\n-    return nullptr;\n+  bool is_trace_pointer() const {\n+    return is_trace(TraceMergeStores::Tag::POINTER);\n@@ -3019,3 +2857,3 @@\n-  const TypeAryPtr* aryptr_t = ptr_t->isa_aryptr();\n-  if (aryptr_t == nullptr) {\n-    return nullptr;\n+\n+  bool is_trace_aliasing() const {\n+    return is_trace(TraceMergeStores::Tag::ALIASING);\n@@ -3023,4 +2861,7 @@\n-  BasicType bt = aryptr_t->elem()->array_element_basic_type();\n-  if (!is_java_primitive(bt) ||\n-      type2aelembytes(bt) != _store->memory_size()) {\n-    return nullptr;\n+\n+  bool is_trace_adjacency() const {\n+    return is_trace(TraceMergeStores::Tag::ADJACENCY);\n+  }\n+\n+  bool is_trace_success() const {\n+    return is_trace(TraceMergeStores::Tag::SUCCESS);\n@@ -3028,1 +2869,9 @@\n-  if (_store->is_unsafe_access()) {\n+#endif\n+\n+  NOT_PRODUCT( void trace(const Node_List& merge_list, const Node* merged_input_value, const StoreNode* merged_store) const; )\n+};\n+\n+StoreNode* MergePrimitiveStores::run() {\n+  \/\/ Check for B\/S\/C\/I\n+  int opc = _store->Opcode();\n+  if (opc != Op_StoreB && opc != Op_StoreC && opc != Op_StoreI) {\n@@ -3032,0 +2881,2 @@\n+  NOT_PRODUCT( if(is_trace_basic()) { tty->print(\"[TraceMergeStores] MergePrimitiveStores::run: \"); _store->dump(); })\n+\n@@ -3035,0 +2886,1 @@\n+  NOT_PRODUCT( if(is_trace_basic()) { tty->print(\"[TraceMergeStores] expect no use: \"); status_use.print_on(tty); })\n@@ -3041,0 +2893,1 @@\n+  NOT_PRODUCT( if(is_trace_basic()) { tty->print(\"[TraceMergeStores] expect def: \"); status_def.print_on(tty); })\n@@ -3054,1 +2907,1 @@\n-  DEBUG_ONLY( if(TraceMergeStores) { trace(merge_list, merged_input_value, merged_store); } )\n+  NOT_PRODUCT( if(is_trace_success()) { trace(merge_list, merged_input_value, merged_store); } )\n@@ -3060,1 +2913,1 @@\n-bool MergePrimitiveArrayStores::is_compatible_store(const StoreNode* other_store) const {\n+bool MergePrimitiveStores::is_compatible_store(const StoreNode* other_store) const {\n@@ -3063,2 +2916,0 @@\n-  assert(_store->adr_type()->isa_aryptr() != nullptr, \"must be array store\");\n-  assert(!_store->is_unsafe_access(), \"no unsafe accesses\");\n@@ -3067,4 +2918,1 @@\n-      _store->Opcode() != other_store->Opcode() ||\n-      other_store->adr_type() == nullptr ||\n-      other_store->adr_type()->isa_aryptr() == nullptr ||\n-      other_store->is_unsafe_access()) {\n+      _store->Opcode() != other_store->Opcode()) {\n@@ -3074,15 +2922,0 @@\n-  \/\/ Check that the size of the stores, and the array elements are all the same.\n-  const TypeAryPtr* aryptr_t1 = _store->adr_type()->is_aryptr();\n-  const TypeAryPtr* aryptr_t2 = other_store->adr_type()->is_aryptr();\n-  BasicType aryptr_bt1 = aryptr_t1->elem()->array_element_basic_type();\n-  BasicType aryptr_bt2 = aryptr_t2->elem()->array_element_basic_type();\n-  if (!is_java_primitive(aryptr_bt1) || !is_java_primitive(aryptr_bt2)) {\n-    return false;\n-  }\n-  int size1 = type2aelembytes(aryptr_bt1);\n-  int size2 = type2aelembytes(aryptr_bt2);\n-  if (size1 != size2 ||\n-      size1 != _store->memory_size() ||\n-      _store->memory_size() != other_store->memory_size()) {\n-    return false;\n-  }\n@@ -3092,1 +2925,1 @@\n-bool MergePrimitiveArrayStores::is_adjacent_pair(const StoreNode* use_store, const StoreNode* def_store) const {\n+bool MergePrimitiveStores::is_adjacent_pair(const StoreNode* use_store, const StoreNode* def_store) const {\n@@ -3100,3 +2933,8 @@\n-  ArrayPointer array_pointer_use = ArrayPointer::make(_phase, use_store->in(MemNode::Address));\n-  ArrayPointer array_pointer_def = ArrayPointer::make(_phase, def_store->in(MemNode::Address));\n-  if (!array_pointer_def.is_adjacent_to_and_before(array_pointer_use, use_store->memory_size())) {\n+#ifndef PRODUCT\n+  const TraceMemPointer trace(is_trace_pointer(),\n+                              is_trace_aliasing(),\n+                              is_trace_adjacency());\n+#endif\n+  const MemPointer pointer_use(use_store NOT_PRODUCT( COMMA trace ));\n+  const MemPointer pointer_def(def_store NOT_PRODUCT( COMMA trace ));\n+  if (!pointer_def.is_adjacent_to_and_before(pointer_use)) {\n@@ -3105,1 +2943,0 @@\n-\n@@ -3109,1 +2946,1 @@\n-bool MergePrimitiveArrayStores::is_adjacent_input_pair(const Node* n1, const Node* n2, const int memory_size) const {\n+bool MergePrimitiveStores::is_adjacent_input_pair(const Node* n1, const Node* n2, const int memory_size) const {\n@@ -3151,1 +2988,1 @@\n-bool MergePrimitiveArrayStores::is_con_RShift(const Node* n, Node const*& base_out, jint& shift_out) {\n+bool MergePrimitiveStores::is_con_RShift(const Node* n, Node const*& base_out, jint& shift_out) {\n@@ -3174,1 +3011,1 @@\n-MergePrimitiveArrayStores::CFGStatus MergePrimitiveArrayStores::cfg_status_for_pair(const StoreNode* use_store, const StoreNode* def_store) {\n+MergePrimitiveStores::CFGStatus MergePrimitiveStores::cfg_status_for_pair(const StoreNode* use_store, const StoreNode* def_store) {\n@@ -3219,1 +3056,1 @@\n-MergePrimitiveArrayStores::Status MergePrimitiveArrayStores::find_adjacent_use_store(const StoreNode* def_store) const {\n+MergePrimitiveStores::Status MergePrimitiveStores::find_adjacent_use_store(const StoreNode* def_store) const {\n@@ -3228,1 +3065,1 @@\n-MergePrimitiveArrayStores::Status MergePrimitiveArrayStores::find_adjacent_def_store(const StoreNode* use_store) const {\n+MergePrimitiveStores::Status MergePrimitiveStores::find_adjacent_def_store(const StoreNode* use_store) const {\n@@ -3237,1 +3074,1 @@\n-MergePrimitiveArrayStores::Status MergePrimitiveArrayStores::find_use_store(const StoreNode* def_store) const {\n+MergePrimitiveStores::Status MergePrimitiveStores::find_use_store(const StoreNode* def_store) const {\n@@ -3253,1 +3090,1 @@\n-MergePrimitiveArrayStores::Status MergePrimitiveArrayStores::find_def_store(const StoreNode* use_store) const {\n+MergePrimitiveStores::Status MergePrimitiveStores::find_def_store(const StoreNode* use_store) const {\n@@ -3269,1 +3106,1 @@\n-MergePrimitiveArrayStores::Status MergePrimitiveArrayStores::find_use_store_unidirectional(const StoreNode* def_store) const {\n+MergePrimitiveStores::Status MergePrimitiveStores::find_use_store_unidirectional(const StoreNode* def_store) const {\n@@ -3282,1 +3119,1 @@\n-MergePrimitiveArrayStores::Status MergePrimitiveArrayStores::find_def_store_unidirectional(const StoreNode* use_store) const {\n+MergePrimitiveStores::Status MergePrimitiveStores::find_def_store_unidirectional(const StoreNode* use_store) const {\n@@ -3293,1 +3130,1 @@\n-void MergePrimitiveArrayStores::collect_merge_list(Node_List& merge_list) const {\n+void MergePrimitiveStores::collect_merge_list(Node_List& merge_list) const {\n@@ -3306,0 +3143,2 @@\n+    NOT_PRODUCT( if(is_trace_basic()) { tty->print(\"[TraceMergeStores] find def: \"); status.print_on(tty); })\n+\n@@ -3312,0 +3151,1 @@\n+        NOT_PRODUCT( if(is_trace_basic()) { tty->print_cr(\"[TraceMergeStores] found RangeCheck, stop traversal.\"); })\n@@ -3317,0 +3157,2 @@\n+  NOT_PRODUCT( if(is_trace_basic()) { tty->print_cr(\"[TraceMergeStores] found:\"); merge_list.dump(); })\n+\n@@ -3321,0 +3163,2 @@\n+\n+  NOT_PRODUCT( if(is_trace_basic()) { tty->print_cr(\"[TraceMergeStores] truncated:\"); merge_list.dump(); })\n@@ -3324,1 +3168,1 @@\n-Node* MergePrimitiveArrayStores::make_merged_input_value(const Node_List& merge_list) {\n+Node* MergePrimitiveStores::make_merged_input_value(const Node_List& merge_list) {\n@@ -3410,1 +3254,1 @@\n-StoreNode* MergePrimitiveArrayStores::make_merged_store(const Node_List& merge_list, Node* merged_input_value) {\n+StoreNode* MergePrimitiveStores::make_merged_store(const Node_List& merge_list, Node* merged_input_value) {\n@@ -3439,2 +3283,2 @@\n-#ifdef ASSERT\n-void MergePrimitiveArrayStores::trace(const Node_List& merge_list, const Node* merged_input_value, const StoreNode* merged_store) const {\n+#ifndef PRODUCT\n+void MergePrimitiveStores::trace(const Node_List& merge_list, const Node* merged_input_value, const StoreNode* merged_store) const {\n@@ -3538,1 +3382,1 @@\n-      MergePrimitiveArrayStores merge(phase, this);\n+      MergePrimitiveStores merge(phase, this);\n","filename":"src\/hotspot\/share\/opto\/memnode.cpp","additions":86,"deletions":242,"binary":false,"changes":328,"status":"modified"},{"patch":"@@ -0,0 +1,379 @@\n+\/*\n+ * Copyright (c) 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"opto\/mempointer.hpp\"\n+#include \"utilities\/resourceHash.hpp\"\n+\n+\/\/ Recursively parse the pointer expression with a DFS all-path traversal\n+\/\/ (i.e. with node repetitions), starting at the pointer.\n+MemPointerDecomposedForm MemPointerDecomposedFormParser::parse_decomposed_form() {\n+  assert(_worklist.is_empty(), \"no prior parsing\");\n+  assert(_summands.is_empty(), \"no prior parsing\");\n+\n+  Node* pointer = _mem->in(MemNode::Address);\n+\n+  \/\/ Start with the trivial summand.\n+  _worklist.push(MemPointerSummand(pointer, NoOverflowInt(1)));\n+\n+  \/\/ Decompose the summands until only terminal summands remain. This effectively\n+  \/\/ parses the pointer expression recursively.\n+  int traversal_count = 0;\n+  while (_worklist.is_nonempty()) {\n+    if (traversal_count++ > 1000) { return MemPointerDecomposedForm(pointer); }\n+    parse_sub_expression(_worklist.pop());\n+  }\n+\n+  \/\/ Check for constant overflow.\n+  if (_con.is_NaN()) { return MemPointerDecomposedForm(pointer); }\n+\n+  \/\/ Sort summands by variable->_idx\n+  _summands.sort(MemPointerSummand::cmp_for_sort);\n+\n+  \/\/ Combine summands for the same variable, adding up the scales.\n+  int pos_put = 0;\n+  int pos_get = 0;\n+  while (pos_get < _summands.length()) {\n+    MemPointerSummand summand = _summands.at(pos_get++);\n+    Node* variable      = summand.variable();\n+    NoOverflowInt scale = summand.scale();\n+    \/\/ Add up scale of all summands with the same variable.\n+    while (pos_get < _summands.length() && _summands.at(pos_get).variable() == variable) {\n+      MemPointerSummand s = _summands.at(pos_get++);\n+      scale = scale + s.scale();\n+    }\n+    \/\/ Bail out if scale is NaN.\n+    if (scale.is_NaN()) {\n+      return MemPointerDecomposedForm(pointer);\n+    }\n+    \/\/ Keep summands with non-zero scale.\n+    if (!scale.is_zero()) {\n+      _summands.at_put(pos_put++, MemPointerSummand(variable, scale));\n+    }\n+  }\n+  _summands.trunc_to(pos_put);\n+\n+  return MemPointerDecomposedForm::make(pointer, _summands, _con);\n+}\n+\n+\/\/ Parse a sub-expression of the pointer, starting at the current summand. We parse the\n+\/\/ current node, and see if it can be decomposed into further summands, or if the current\n+\/\/ summand is terminal.\n+void MemPointerDecomposedFormParser::parse_sub_expression(const MemPointerSummand summand) {\n+  Node* n = summand.variable();\n+  const NoOverflowInt scale = summand.scale();\n+  const NoOverflowInt one(1);\n+\n+  int opc = n->Opcode();\n+  if (is_safe_to_decompose_op(opc, scale)) {\n+    switch (opc) {\n+      case Op_ConI:\n+      case Op_ConL:\n+      {\n+        \/\/ Terminal: add to constant.\n+        NoOverflowInt con = (opc == Op_ConI) ? NoOverflowInt(n->get_int())\n+                                             : NoOverflowInt(n->get_long());\n+        _con = _con + scale * con;\n+        return;\n+      }\n+      case Op_AddP:\n+      case Op_AddL:\n+      case Op_AddI:\n+      {\n+        \/\/ Decompose addition.\n+        Node* a = n->in((opc == Op_AddP) ? 2 : 1);\n+        Node* b = n->in((opc == Op_AddP) ? 3 : 2);\n+        _worklist.push(MemPointerSummand(a, scale));\n+        _worklist.push(MemPointerSummand(b, scale));\n+        return;\n+      }\n+      case Op_SubL:\n+      case Op_SubI:\n+      {\n+        \/\/ Decompose subtraction.\n+        Node* a = n->in(1);\n+        Node* b = n->in(2);\n+\n+        NoOverflowInt sub_scale = NoOverflowInt(-1) * scale;\n+\n+        _worklist.push(MemPointerSummand(a, scale));\n+        _worklist.push(MemPointerSummand(b, sub_scale));\n+        return;\n+      }\n+      case Op_MulL:\n+      case Op_MulI:\n+      case Op_LShiftL:\n+      case Op_LShiftI:\n+      {\n+        \/\/ Only multiplication with constants is allowed: factor * variable\n+        \/\/ IGVN already folds constants to in(2). If we find a variable there\n+        \/\/ instead, we cannot further decompose this summand, and have to add\n+        \/\/ it to the terminal summands.\n+        Node* variable = n->in(1);\n+        Node* con      = n->in(2);\n+        if (!con->is_Con()) { break; }\n+        NoOverflowInt factor;\n+        switch (opc) {\n+          case Op_MulL:    \/\/ variable * con\n+            factor = NoOverflowInt(con->get_long());\n+            break;\n+          case Op_MulI:    \/\/ variable * con\n+            factor = NoOverflowInt(con->get_int());\n+            break;\n+          case Op_LShiftL: \/\/ variable << con = variable * (1 << con)\n+            factor = one << NoOverflowInt(con->get_int());\n+            break;\n+          case Op_LShiftI: \/\/ variable << con = variable * (1 << con)\n+            factor = one << NoOverflowInt(con->get_int());\n+            break;\n+        }\n+\n+        \/\/ Accumulate scale.\n+        NoOverflowInt new_scale = scale * factor;\n+\n+        _worklist.push(MemPointerSummand(variable, new_scale));\n+        return;\n+      }\n+      case Op_CastII:\n+      case Op_CastLL:\n+      case Op_CastX2P:\n+      case Op_ConvI2L:\n+      \/\/ On 32bit systems we can also look through ConvL2I, since the final result will always\n+      \/\/ be truncated back with ConvL2I. On 64bit systems we cannot decompose ConvL2I because\n+      \/\/ such int values will eventually be expanded to long with a ConvI2L:\n+      \/\/\n+      \/\/   valL = max_jint + 1\n+      \/\/   ConvI2L(ConvL2I(valL)) = ConvI2L(min_jint) = min_jint != max_jint + 1 = valL\n+      \/\/\n+      NOT_LP64( case Op_ConvL2I: )\n+      {\n+        \/\/ Decompose: look through.\n+        Node* a = n->in(1);\n+        _worklist.push(MemPointerSummand(a, scale));\n+        return;\n+      }\n+      default:\n+        \/\/ All other operations cannot be further decomposed. We just add them to the\n+        \/\/ terminal summands below.\n+        break;\n+    }\n+  }\n+\n+  \/\/ Default: we could not parse the \"summand\" further, i.e. it is terminal.\n+  _summands.push(summand);\n+}\n+\n+\/\/ Check if the decomposition of operation opc is guaranteed to be safe.\n+\/\/ Please refer to the definition of \"safe decomposition\" in mempointer.hpp\n+bool MemPointerDecomposedFormParser::is_safe_to_decompose_op(const int opc, const NoOverflowInt scale) const {\n+#ifndef _LP64\n+  \/\/ On 32-bit platforms, the pointer has 32bits, and thus any higher bits will always\n+  \/\/ be truncated. Thus, it does not matter if we have int or long overflows.\n+  \/\/ Simply put: all decompositions are (SAFE1).\n+  return true;\n+#else\n+\n+  switch(opc) {\n+    \/\/ These operations are always safe to decompose, i.e. (SAFE1):\n+    case Op_ConI:\n+    case Op_ConL:\n+    case Op_AddP:\n+    case Op_AddL:\n+    case Op_SubL:\n+    case Op_MulL:\n+    case Op_LShiftL:\n+    case Op_CastII:\n+    case Op_CastLL:\n+    case Op_CastX2P:\n+    case Op_CastPP:\n+    case Op_ConvI2L:\n+      return true;\n+\n+    \/\/ But on 64-bit platforms, these operations are not trivially safe to decompose:\n+    case Op_AddI:    \/\/ ConvI2L(a +  b)    != ConvI2L(a) +  ConvI2L(b)\n+    case Op_SubI:    \/\/ ConvI2L(a -  b)    != ConvI2L(a) -  ConvI2L(b)\n+    case Op_MulI:    \/\/ ConvI2L(a *  conI) != ConvI2L(a) *  ConvI2L(conI)\n+    case Op_LShiftI: \/\/ ConvI2L(a << conI) != ConvI2L(a) << ConvI2L(conI)\n+      break; \/\/ Analysis below.\n+\n+    \/\/ All other operations are assumed not safe to decompose, or simply cannot be decomposed\n+    default:\n+      return false;\n+  }\n+\n+  const TypeAryPtr* ary_ptr_t = _mem->adr_type()->isa_aryptr();\n+  if (ary_ptr_t != nullptr) {\n+    \/\/ Array accesses that are not Unsafe always have a RangeCheck which ensures\n+    \/\/ that there is no int overflow. And without overflows, all decompositions\n+    \/\/ are (SAFE1).\n+    if (!_mem->is_unsafe_access()) {\n+      return true;\n+    }\n+\n+    \/\/ Intuition: In general, the decomposition of AddI, SubI, MulI or LShiftI is not safe,\n+    \/\/            because of overflows. But under some conditions, we can prove that such a\n+    \/\/            decomposition is (SAFE2). Intuitively, we want to prove that an overflow\n+    \/\/            would mean that the pointers have such a large distance, that at least one\n+    \/\/            must lie out of bounds. In the proof of the \"MemPointer Lemma\", we thus\n+    \/\/            get a contradiction with the condition that both pointers are in bounds.\n+    \/\/\n+    \/\/ We prove that the decomposition of AddI, SubI, MulI (with constant) and ShiftI (with\n+    \/\/ constant) is (SAFE2), under the condition:\n+    \/\/\n+    \/\/   abs(scale) % array_element_size_in_bytes = 0\n+    \/\/\n+    \/\/ First, we describe how the decomposition works:\n+    \/\/\n+    \/\/   mp_i = con + sum(other_summands) + summand\n+    \/\/          -------------------------   -------\n+    \/\/          rest                        scale * ConvI2L(op)\n+    \/\/\n+    \/\/  We decompose the summand depending on the op, where we know that there is some\n+    \/\/  integer y, such that:\n+    \/\/\n+    \/\/    scale * ConvI2L(a + b)     =  scale * ConvI2L(a) + scale * ConvI2L(b)  +  scale * y * 2^32\n+    \/\/    scale * ConvI2L(a - b)     =  scale * ConvI2L(a) - scale * ConvI2L(b)  +  scale * y * 2^32\n+    \/\/    scale * ConvI2L(a * con)   =  scale * con * ConvI2L(a)                 +  scale * y * 2^32\n+    \/\/    scale * ConvI2L(a << con)  =  scale * (1 << con) * ConvI2L(a)          +  scale * y * 2^32\n+    \/\/    \\_______________________\/     \\_____________________________________\/     \\______________\/\n+    \/\/      before decomposition               after decomposition                 overflow correction\n+    \/\/\n+    \/\/  Thus, for AddI and SubI, we get:\n+    \/\/    summand = new_summand1 + new_summand2 + scale * y * 2^32\n+    \/\/\n+    \/\/    mp_{i+1} = con + sum(other_summands) + new_summand1 + new_summand2\n+    \/\/             = con + sum(other_summands) + summand - scale * y * 2^32\n+    \/\/             = mp_i                                - scale * y * 2^32\n+    \/\/\n+    \/\/  And for MulI and ShiftI we get:\n+    \/\/    summand = new_summand + scale * y * 2^32\n+    \/\/\n+    \/\/    mp_{i+1} = con + sum(other_summands) + new_summand\n+    \/\/             = con + sum(other_summands) + summand - scale * y * 2^32\n+    \/\/             = mp_i                                - scale * y * 2^32\n+    \/\/\n+    \/\/  Further:\n+    \/\/    abs(scale) % array_element_size_in_bytes = 0\n+    \/\/  implies that there is some integer z, such that:\n+    \/\/    z * array_element_size_in_bytes = scale\n+    \/\/\n+    \/\/  And hence, with \"x = y * z\":\n+    \/\/    mp_i = mp_{i+1} + scale                           * y * 2^32\n+    \/\/         = mp_{i+1} + z * array_element_size_in_bytes * y * 2^32\n+    \/\/         = mp_{i+1} + x * array_element_size_in_bytes     * 2^32\n+    \/\/\n+    BasicType array_element_bt = ary_ptr_t->elem()->array_element_basic_type();\n+    if (is_java_primitive(array_element_bt)) {\n+      NoOverflowInt array_element_size_in_bytes = NoOverflowInt(type2aelembytes(array_element_bt));\n+      if (scale.is_multiple_of(array_element_size_in_bytes)) {\n+        return true;\n+      }\n+    }\n+  }\n+\n+  return false;\n+#endif\n+}\n+\n+\/\/ Compute the aliasing between two MemPointerDecomposedForm. We use the \"MemPointer Lemma\" to\n+\/\/ prove that the computed aliasing also applies for the underlying pointers.\n+\/\/\n+\/\/ Pre-Condition:\n+\/\/   We assume that both pointers are in-bounds of their respective memory object.\n+\/\/\n+MemPointerAliasing MemPointerDecomposedForm::get_aliasing_with(const MemPointerDecomposedForm& other\n+                                                               NOT_PRODUCT( COMMA const TraceMemPointer& trace) ) const {\n+#ifndef PRODUCT\n+  if (trace.is_trace_aliasing()) {\n+    tty->print_cr(\"MemPointerDecomposedForm::get_aliasing_with:\");\n+    print_on(tty);\n+    other.print_on(tty);\n+  }\n+#endif\n+\n+  \/\/ \"MemPointer Lemma\" condition S2: check if all summands are the same:\n+  for (uint i = 0; i < SUMMANDS_SIZE; i++) {\n+    const MemPointerSummand s1 = summands_at(i);\n+    const MemPointerSummand s2 = other.summands_at(i);\n+    if (s1 != s2) {\n+#ifndef PRODUCT\n+      if (trace.is_trace_aliasing()) {\n+        tty->print_cr(\"  -> Aliasing unknown, differ on summand %d.\", i);\n+      }\n+#endif\n+      return MemPointerAliasing::make_unknown();\n+    }\n+  }\n+\n+  \/\/ \"MemPointer Lemma\" condition S3: check that the constants do not differ too much:\n+  const NoOverflowInt distance = other.con() - con();\n+  \/\/ We must check that: abs(distance) < 2^32\n+  \/\/ However, this is only false if: distance = min_jint\n+  if (distance.is_NaN() || distance.value() == min_jint) {\n+#ifndef PRODUCT\n+    if (trace.is_trace_aliasing()) {\n+      tty->print(\"  -> Aliasing unknown, bad distance: \");\n+      distance.print_on(tty);\n+      tty->cr();\n+    }\n+#endif\n+    return MemPointerAliasing::make_unknown();\n+  }\n+\n+  \/\/ \"MemPointer Lemma\" condition S1:\n+  \/\/   Given that all summands are the same, we know that both pointers point into the\n+  \/\/   same memory object. With the Pre-Condition, we know that both pointers are in\n+  \/\/   bounds of that same memory object.\n+\n+  \/\/ Hence, all 3 conditions of the \"MemoryPointer Lemma\" are established, and hence\n+  \/\/ we know that the distance between the underlying pointers is equal to the distance\n+  \/\/ we computed for the MemPointers:\n+  \/\/   p_other - p_this = distance = other.con - this.con\n+#ifndef PRODUCT\n+    if (trace.is_trace_aliasing()) {\n+      tty->print_cr(\"  -> Aliasing always, distance = %d.\", distance.value());\n+    }\n+#endif\n+  return MemPointerAliasing::make_always(distance.value());\n+}\n+\n+bool MemPointer::is_adjacent_to_and_before(const MemPointer& other) const {\n+  const MemPointerDecomposedForm& s1 = decomposed_form();\n+  const MemPointerDecomposedForm& s2 = other.decomposed_form();\n+  const MemPointerAliasing aliasing = s1.get_aliasing_with(s2 NOT_PRODUCT( COMMA _trace ));\n+  const jint size = mem()->memory_size();\n+  const bool is_adjacent = aliasing.is_always_at_distance(size);\n+\n+#ifndef PRODUCT\n+  if (_trace.is_trace_adjacency()) {\n+    tty->print(\"Adjacent: %s, because size = %d and aliasing = \",\n+               is_adjacent ? \"true\" : \"false\", size);\n+    aliasing.print_on(tty);\n+    tty->cr();\n+  }\n+#endif\n+\n+  return is_adjacent;\n+}\n+\n","filename":"src\/hotspot\/share\/opto\/mempointer.cpp","additions":379,"deletions":0,"binary":false,"changes":379,"status":"added"},{"patch":"@@ -0,0 +1,591 @@\n+\/*\n+ * Copyright (c) 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef SHARE_OPTO_MEMPOINTER_HPP\n+#define SHARE_OPTO_MEMPOINTER_HPP\n+\n+#include \"opto\/memnode.hpp\"\n+#include \"opto\/noOverflowInt.hpp\"\n+\n+\/\/ The MemPointer is a shared facility to parse pointers and check the aliasing of pointers,\n+\/\/ e.g. checking if two stores are adjacent.\n+\/\/\n+\/\/ -----------------------------------------------------------------------------------------\n+\/\/\n+\/\/ Intuition and Examples:\n+\/\/   We parse \/ decompose pointers into a linear form:\n+\/\/\n+\/\/     pointer = sum_i(scale_i * variable_i) + con\n+\/\/\n+\/\/   The con and scale_i are compile-time constants (NoOverflowInt), and the variable_i are\n+\/\/   compile-time variables (C2 nodes).\n+\/\/\n+\/\/   For the MemPointer, we do not explicitly track base address. For Java heap pointers, the\n+\/\/   base address is just a variable in a summand with scale == 1. For native memory (C heap)\n+\/\/   pointers, the base address is null, and is hence implicitly a zero constant.\n+\/\/\n+\/\/\n+\/\/   Example1: byte array access:\n+\/\/\n+\/\/     array[i]\n+\/\/\n+\/\/     pointer =           array_base + ARRAY_BYTE_BASE_OFFSET + 1       * i\n+\/\/             = 1       * array_base + ARRAY_BYTE_BASE_OFFSET + 1       * i\n+\/\/               --------------------   ----------------------   --------------------\n+\/\/             = scale_0 * variable_0 + con                    + scale_1 * variable_1\n+\/\/\n+\/\/\n+\/\/   Example2: int array access\n+\/\/\n+\/\/     array[5 + i + 3 * j]\n+\/\/\n+\/\/     pointer =           array_base + ARRAY_INT_BASE_OFFSET + 4 * 5 + 4       * j          + 4       * 3 * j\n+\/\/             = 1       * array_base + ARRAY_INT_BASE_OFFSET + 20    + 4       * j          + 12      * j\n+\/\/               --------------------   -----------------------------   --------------------   --------------------\n+\/\/             = scale_0 * variable_0 + con                           + scale_1 * variable_1 + scale_2 * variable_2\n+\/\/\n+\/\/\n+\/\/   Example3: Unsafe with int array\n+\/\/\n+\/\/     UNSAFE.getInt(array, ARRAY_INT_BASE_OFFSET + 4 * i);\n+\/\/\n+\/\/     pointer =           array_base + ARRAY_INT_BASE_OFFSET + 4       * i\n+\/\/             = 1       * array_base + ARRAY_INT_BASE_OFFSET + 4       * i\n+\/\/             = scale_0 * variable_0 + con                   + scale_1 * variable_1\n+\/\/\n+\/\/\n+\/\/   Example4: Unsafe with native memory address\n+\/\/\n+\/\/     long address;\n+\/\/     UNSAFE.getInt(null, address + 4 * i);\n+\/\/\n+\/\/     pointer =           address          + 4       * i\n+\/\/             = 1       * address    + 0   + 4       * i\n+\/\/             = scale_0 * variable_0 + con + scale_1 * variable_1\n+\/\/\n+\/\/\n+\/\/   Example5: MemorySegment with byte array as backing type\n+\/\/\n+\/\/     byte[] array = new byte[1000];\n+\/\/     MemorySegment ms = MemorySegment.ofArray(array);\n+\/\/     assert ms.heapBase().get() == array: \"array is base\";\n+\/\/     assert ms.address() == 0: \"zero offset from base\";\n+\/\/     byte val = ms.get(ValueLayout.JAVA_BYTE, i);\n+\/\/\n+\/\/     pointer =           ms.heapBase() + ARRAY_BYTE_BASE_OFFSET + ms.address() +           i\n+\/\/             = 1       * array_base    + ARRAY_BYTE_BASE_OFFSET + 0            + 1       * i\n+\/\/               -----------------------   -------------------------------------   --------------------\n+\/\/             = scale_0 * variable_0    + con                                   + scale_1 * variable_1\n+\/\/\n+\/\/\n+\/\/   Example6: MemorySegment with native memory\n+\/\/\n+\/\/     MemorySegment ms = Arena.ofAuto().allocate(1000, 1);\n+\/\/     assert ms.heapBase().isEmpty(): \"null base\";\n+\/\/     assert ms.address() != 0: \"non-zero native memory address\";\n+\/\/     byte val2 = ms.get(ValueLayout.JAVA_BYTE, i);\n+\/\/\n+\/\/     pointer = ms.heapBase() +           ms.address() +           i\n+\/\/             = 0             + 1       * ms.address() + 1       * i\n+\/\/               ------------    ----------------------   --------------------\n+\/\/             = con             scale_0 * variable_0   + scale_1 * variable_1\n+\/\/\n+\/\/\n+\/\/   Example7: Non-linear access to int array\n+\/\/\n+\/\/     array[5 + i + j * k]\n+\/\/\n+\/\/     pointer =           array_base + ARRAY_INT_BASE_OFFSET + 4 * 5 + 4       * j          + 4       * j * k\n+\/\/             = 1       * array_base + ARRAY_INT_BASE_OFFSET + 20    + 4       * j          + 4       * j * k\n+\/\/               --------------------   -----------------------------   --------------------   --------------------\n+\/\/             = scale_0 * variable_0 + con                           + scale_1 * variable_1 + scale_2 * variable_2\n+\/\/\n+\/\/     Note: we simply stop parsing once a term is not linear. We keep \"j * k\" as its own variable.\n+\/\/\n+\/\/\n+\/\/   Example8: Unsafe with native memory address, non-linear access\n+\/\/\n+\/\/     UNSAFE.getInt(null, i * j);\n+\/\/\n+\/\/     pointer =                 i * j\n+\/\/             = 0   + 1       * i * j\n+\/\/               ---   --------------------\n+\/\/             = con + scale_0 * variable_0\n+\/\/\n+\/\/     Note: we can always parse a pointer into its trivial linear form:\n+\/\/\n+\/\/             pointer = 0 + 1 * pointer.\n+\/\/\n+\/\/ -----------------------------------------------------------------------------------------\n+\/\/\n+\/\/ MemPointerDecomposedForm:\n+\/\/   When the pointer is parsed, it is decomposed into sum of summands plus a constant:\n+\/\/\n+\/\/     pointer = sum(summands) + con\n+\/\/\n+\/\/   Where each summand_i in summands has the form:\n+\/\/\n+\/\/     summand_i = scale_i * variable_i\n+\/\/\n+\/\/   Hence, the full decomposed form is:\n+\/\/\n+\/\/     pointer = sum_i(scale_i * variable_i) + con\n+\/\/\n+\/\/   Note: the scale_i are compile-time constants (NoOverflowInt), and the variable_i are\n+\/\/         compile-time variables (C2 nodes).\n+\/\/   On 64bit systems, this decomposed form is computed with long-add\/mul, on 32bit systems\n+\/\/   it is computed with int-add\/mul.\n+\/\/\n+\/\/ MemPointerAliasing:\n+\/\/   The decomposed form allows us to determine the aliasing between two pointers easily. For\n+\/\/   example, if two pointers are identical, except for their constant:\n+\/\/\n+\/\/     pointer1 = sum(summands) + con1\n+\/\/     pointer2 = sum(summands) + con2\n+\/\/\n+\/\/   then we can easily compute the distance between the pointers (distance = con2 - con1),\n+\/\/   and determine if they are adjacent.\n+\/\/\n+\/\/ MemPointerDecomposedFormParser:\n+\/\/   Any pointer can be parsed into this (default \/ trivial) decomposed form:\n+\/\/\n+\/\/     pointer = 1       * pointer    + 0\n+\/\/               scale_0 * variable_0 + con\n+\/\/\n+\/\/   However, this is not particularly useful to compute aliasing. We would like to decompose\n+\/\/   the pointer as far as possible, i.e. extract as many summands and add up the constants to\n+\/\/   a single constant.\n+\/\/\n+\/\/   Example (normal int-array access):\n+\/\/     pointer1 = array[i + 0] = array_base + array_int_base_offset + 4L * ConvI2L(i + 0)\n+\/\/     pointer2 = array[i + 1] = array_base + array_int_base_offset + 4L * ConvI2L(i + 1)\n+\/\/\n+\/\/     At first, computing aliasing is difficult because the distance is hidden inside the\n+\/\/     ConvI2L. we can convert this (with array_int_base_offset = 16) into these decomposed forms:\n+\/\/\n+\/\/     pointer1 = 1L * array_base + 4L * i + 16L\n+\/\/     pointer2 = 1L * array_base + 4L * i + 20L\n+\/\/\n+\/\/     This allows us to easily see that these two pointers are adjacent (distance = 4).\n+\/\/\n+\/\/   Hence, in MemPointerDecomposedFormParser::parse_decomposed_form, we start with the pointer as\n+\/\/   a trivial summand. A summand can either be decomposed further or it is terminal (cannot\n+\/\/   be decomposed further). We decompose the summands recursively until all remaining summands\n+\/\/   are terminal, see MemPointerDecomposedFormParser::parse_sub_expression. This effectively parses\n+\/\/   the pointer expression recursively.\n+\/\/\n+\/\/ -----------------------------------------------------------------------------------------\n+\/\/\n+\/\/   We have to be careful on 64bit systems with ConvI2L: decomposing its input is not\n+\/\/   correct in general, overflows may not be preserved in the decomposed form:\n+\/\/\n+\/\/     AddI:     ConvI2L(a +  b)    != ConvI2L(a) +  ConvI2L(b)\n+\/\/     SubI:     ConvI2L(a -  b)    != ConvI2L(a) -  ConvI2L(b)\n+\/\/     MulI:     ConvI2L(a *  conI) != ConvI2L(a) *  ConvI2L(conI)\n+\/\/     LShiftI:  ConvI2L(a << conI) != ConvI2L(a) << ConvI2L(conI)\n+\/\/\n+\/\/   If we want to prove the correctness of MemPointerAliasing, we need some guarantees,\n+\/\/   that the MemPointers adequately represent the underlying pointers, such that we can\n+\/\/   compute the aliasing based on the summands and constants.\n+\/\/\n+\/\/ -----------------------------------------------------------------------------------------\n+\/\/\n+\/\/   Below, we will formulate a \"MemPointer Lemma\" that helps us to prove the correctness of\n+\/\/   the MemPointerAliasing computations. To prove the \"MemPointer Lemma\", we need to define\n+\/\/   the idea of a \"safe decomposition\", and then prove that all the decompositions we apply\n+\/\/   are such \"safe decompositions\".\n+\/\/\n+\/\/\n+\/\/  Definition: Safe decomposition (from some mp_i to mp_{i+1})\n+\/\/    We decompose summand in:\n+\/\/      mp_i     = con + summand                     + sum(other_summands)\n+\/\/    Resulting in:      +-------------------------+\n+\/\/      mp_{i+1} = con + dec_con + sum(dec_summands) + sum(other_summands)\n+\/\/               = new_con + sum(new_summands)\n+\/\/\n+\/\/    We call a decomposition safe if either:\n+\/\/      SAFE1) No matter the values of the summand variables:\n+\/\/               mp_i = mp_{i+1}\n+\/\/\n+\/\/      SAFE2) The pointer is on an array with a known array_element_size_in_bytes,\n+\/\/             and there is an integer x, such that:\n+\/\/               mp_i = mp_{i+1} + x * array_element_size_in_bytes * 2^32\n+\/\/\n+\/\/             Note: if \"x = 0\", we have \"mp1 = mp2\", and if \"x != 0\", then mp1 and mp2\n+\/\/                   have a distance at least twice as large as the array size, and so\n+\/\/                   at least one of mp1 or mp2 must be out of bounds of the array.\n+\/\/\n+\/\/    Note: MemPointerDecomposedFormParser::is_safe_to_decompose_op checks that all\n+\/\/          decompositions we apply are safe.\n+\/\/\n+\/\/\n+\/\/  MemPointer Lemma:\n+\/\/    Given two pointers p1 and p2, and their respective MemPointers mp1 and mp2.\n+\/\/    If these conditions hold:\n+\/\/      S1) Both p1 and p2 are within the bounds of the same memory object.\n+\/\/      S2) The constants do not differ too much: abs(mp1.con - mp2.con) < 2^31\n+\/\/      S3) All summands of mp1 and mp2 are identical.\n+\/\/\n+\/\/    Then the ponter difference between p1 and p2 is identical to the difference between\n+\/\/    mp1 and mp2:\n+\/\/      p1 - p2 = mp1 - mp2\n+\/\/\n+\/\/    Note: MemPointerDecomposedForm::get_aliasing_with relies on this MemPointer Lemma to\n+\/\/          prove the correctness of its aliasing computation between two MemPointers.\n+\/\/\n+\/\/\n+\/\/  Proof of the \"MemPointer Lemma\":\n+\/\/    Case 0: no decompositions were used:\n+\/\/      mp1 = 0 + 1 * p1 = p1\n+\/\/      mp2 = 0 + 1 * p2 = p2\n+\/\/      =>\n+\/\/      p1 - p2 = mp1 - mp2\n+\/\/\n+\/\/    Case 1: only decompositions of type (SAFE1) were used:\n+\/\/      We make an induction proof over the decompositions from p1 to mp1, starting with\n+\/\/      the trivial decompoisition:\n+\/\/        mp1_0 = 0 + 1 * p1 = p1\n+\/\/      and then for the i'th decomposition, we know that\n+\/\/        mp1_i = mp1_{i+1}\n+\/\/      and hence, if mp1 was decomposed with n decompositions from p1:\n+\/\/        p1 = mp1_0 = mp1_i = mp1_n = mp1\n+\/\/      The analogue can be proven for p2 and mp2:\n+\/\/        p2 = mp2\n+\/\/\n+\/\/      p1 = mp1\n+\/\/      p2 = mp2\n+\/\/      =>\n+\/\/      p1 - p2 = mp1 - mp2\n+\/\/\n+\/\/    Case 2: decompositions of type (SAFE2) were used, and possibly also decompositions of\n+\/\/            type (SAFE1).\n+\/\/       Given we have (SAFE2) decompositions, we know that we are operating on an array of\n+\/\/       known array_element_size_in_bytes. We can weaken the guarantees from (SAFE1)\n+\/\/       decompositions to the same guarantee as (SAFE2) decompositions, hence all applied\n+\/\/       decompositions satisfy:\n+\/\/         mp1_i = mp1_{i+1} + x1_i * array_element_size_in_bytes * 2^32\n+\/\/       where x_i = 0 for (SAFE1) decompositions.\n+\/\/\n+\/\/      We make an induction proof over the decompositions from p1 to mp1, starting with\n+\/\/      the trivial decompoisition:\n+\/\/        mp1_0 = 0 + 1 * p1 = p1\n+\/\/      and then for the i'th decomposition, we know that\n+\/\/        mp1_i = mp1_{i+1} + x1_i * array_element_size_in_bytes * 2^32\n+\/\/      and hence, if mp1 was decomposed with n decompositions from p1:\n+\/\/        p1 = mp1 + x1 * array_element_size_in_bytes * 2^32\n+\/\/      where x1 = sum(x1_i).\n+\/\/      The analogue can be proven for p2 and mp2:\n+\/\/        p2 = mp2 + x2 * array_element_size_in_bytes * 2^32\n+\/\/\n+\/\/      And hence, there must be an x, such that:\n+\/\/        p1 - p2 = mp1 - mp2 + x * array_element_size_in_bytes * 2^32\n+\/\/\n+\/\/      If \"x = 0\", then it follows:\n+\/\/        p1 - p2 = mp1 - mp2\n+\/\/\n+\/\/      If \"x != 0\", then:\n+\/\/        abs(p1 - p2) =  abs(mp1 - mp2 + x * array_element_size_in_bytes * 2^32)\n+\/\/                     >= abs(x * array_element_size_in_bytes * 2^32) - abs(mp1 - mp2)\n+\/\/                            -- apply x != 0 --\n+\/\/                     >= array_element_size_in_bytes * 2^32          - abs(mp1 - mp2)\n+\/\/                                                               -- apply S2 and S3 --\n+\/\/                     >  array_element_size_in_bytes * 2^32          - 2^31\n+\/\/                     >= array_element_size_in_bytes * 2^31\n+\/\/                     >= max_possible_array_size_in_bytes\n+\/\/                     >= array_size_in_bytes\n+\/\/\n+\/\/        Thus we get a contradiction: p1 and p2 have a distance greater than the array\n+\/\/        size, and hence at least one of the two must be out of bounds. But condition S1\n+\/\/        of the MemPointer Lemma requires that both p1 and p2 are both in bounds of the\n+\/\/        same memory object.\n+\n+#ifndef PRODUCT\n+class TraceMemPointer : public StackObj {\n+private:\n+  const bool _is_trace_pointer;\n+  const bool _is_trace_aliasing;\n+  const bool _is_trace_adjacency;\n+\n+public:\n+  TraceMemPointer(const bool is_trace_pointer,\n+                  const bool is_trace_aliasing,\n+                  const bool is_trace_adjacency) :\n+    _is_trace_pointer(  is_trace_pointer),\n+    _is_trace_aliasing( is_trace_aliasing),\n+    _is_trace_adjacency(is_trace_adjacency)\n+    {}\n+\n+  bool is_trace_pointer()   const { return _is_trace_pointer; }\n+  bool is_trace_aliasing()  const { return _is_trace_aliasing; }\n+  bool is_trace_adjacency() const { return _is_trace_adjacency; }\n+};\n+#endif\n+\n+\/\/ Class to represent aliasing between two MemPointer.\n+class MemPointerAliasing {\n+public:\n+  enum Aliasing {\n+    Unknown, \/\/ Distance unknown.\n+             \/\/   Example: two \"int[]\" with different variable index offsets.\n+             \/\/            e.g. \"array[i]  vs  array[j]\".\n+             \/\/            e.g. \"array1[i] vs  array2[j]\".\n+    Always}; \/\/ Constant distance = p1 - p2.\n+             \/\/   Example: The same address expression, except for a constant offset\n+             \/\/            e.g. \"array[i]  vs  array[i+1]\".\n+private:\n+  const Aliasing _aliasing;\n+  const jint _distance;\n+\n+  MemPointerAliasing(const Aliasing aliasing, const jint distance) :\n+    _aliasing(aliasing),\n+    _distance(distance)\n+  {\n+    const jint max_distance = 1 << 30;\n+    assert(_distance < max_distance && _distance > -max_distance, \"safe distance\");\n+  }\n+\n+public:\n+  MemPointerAliasing() : MemPointerAliasing(Unknown, 0) {}\n+\n+  static MemPointerAliasing make_unknown() {\n+    return MemPointerAliasing();\n+  }\n+\n+  static MemPointerAliasing make_always(const jint distance) {\n+    return MemPointerAliasing(Always, distance);\n+  }\n+\n+  \/\/ Use case: exact aliasing and adjacency.\n+  bool is_always_at_distance(const jint distance) const {\n+    return _aliasing == Always && _distance == distance;\n+  }\n+\n+#ifndef PRODUCT\n+  void print_on(outputStream* st) const {\n+    switch(_aliasing) {\n+      case Unknown: st->print(\"Unknown\");               break;\n+      case Always:  st->print(\"Always(%d)\", _distance); break;\n+      default: ShouldNotReachHere();\n+    }\n+  }\n+#endif\n+};\n+\n+\/\/ Summand of a MemPointerDecomposedForm:\n+\/\/\n+\/\/   summand = scale * variable\n+\/\/\n+class MemPointerSummand : public StackObj {\n+private:\n+  Node* _variable;\n+  NoOverflowInt _scale;\n+\n+public:\n+  MemPointerSummand() :\n+      _variable(nullptr),\n+      _scale(NoOverflowInt::make_NaN()) {}\n+  MemPointerSummand(Node* variable, const NoOverflowInt scale) :\n+      _variable(variable),\n+      _scale(scale)\n+  {\n+    assert(_variable != nullptr, \"must have variable\");\n+    assert(!_scale.is_zero(), \"non-zero scale\");\n+  }\n+\n+  Node* variable() const { return _variable; }\n+  NoOverflowInt scale() const { return _scale; }\n+\n+  static int cmp_for_sort(MemPointerSummand* p1, MemPointerSummand* p2) {\n+    if (p1->variable() == nullptr) {\n+      return (p2->variable() == nullptr) ? 0 : 1;\n+    } else if (p2->variable() == nullptr) {\n+      return -1;\n+    }\n+\n+    return p1->variable()->_idx - p2->variable()->_idx;\n+  }\n+\n+  friend bool operator==(const MemPointerSummand a, const MemPointerSummand b) {\n+    \/\/ Both \"null\" -> equal.\n+    if (a.variable() == nullptr && b.variable() == nullptr) { return true; }\n+\n+    \/\/ Same variable and scale?\n+    if (a.variable() != b.variable()) { return false; }\n+    return a.scale() == b.scale();\n+  }\n+\n+  friend bool operator!=(const MemPointerSummand a, const MemPointerSummand b) {\n+    return !(a == b);\n+  }\n+\n+#ifndef PRODUCT\n+  void print_on(outputStream* st) const {\n+    st->print(\"Summand[\");\n+    _scale.print_on(st);\n+    tty->print(\" * [%d %s]]\", _variable->_idx, _variable->Name());\n+  }\n+#endif\n+};\n+\n+\/\/ Decomposed form of the pointer sub-expression of \"pointer\".\n+\/\/\n+\/\/   pointer = sum(summands) + con\n+\/\/\n+class MemPointerDecomposedForm : public StackObj {\n+private:\n+  \/\/ We limit the number of summands to 10. Usually, a pointer contains a base pointer\n+  \/\/ (e.g. array pointer or null for native memory) and a few variables.\n+  static const int SUMMANDS_SIZE = 10;\n+\n+  Node* _pointer; \/\/ pointer node associated with this (sub)pointer\n+\n+  MemPointerSummand _summands[SUMMANDS_SIZE];\n+  NoOverflowInt _con;\n+\n+public:\n+  \/\/ Empty\n+  MemPointerDecomposedForm() : _pointer(nullptr), _con(NoOverflowInt::make_NaN()) {}\n+  \/\/ Default \/ trivial: pointer = 0 + 1 * pointer\n+  MemPointerDecomposedForm(Node* pointer) : _pointer(pointer), _con(NoOverflowInt(0)) {\n+    assert(pointer != nullptr, \"pointer must be non-null\");\n+    _summands[0] = MemPointerSummand(pointer, NoOverflowInt(1));\n+  }\n+\n+private:\n+  MemPointerDecomposedForm(Node* pointer, const GrowableArray<MemPointerSummand>& summands, const NoOverflowInt con)\n+    :_pointer(pointer), _con(con) {\n+    assert(!_con.is_NaN(), \"non-NaN constant\");\n+    assert(summands.length() <= SUMMANDS_SIZE, \"summands must fit\");\n+    for (int i = 0; i < summands.length(); i++) {\n+      MemPointerSummand s = summands.at(i);\n+      assert(s.variable() != nullptr, \"variable cannot be null\");\n+      assert(!s.scale().is_NaN(), \"non-NaN scale\");\n+      _summands[i] = s;\n+    }\n+  }\n+\n+public:\n+  static MemPointerDecomposedForm make(Node* pointer, const GrowableArray<MemPointerSummand>& summands, const NoOverflowInt con) {\n+    if (summands.length() <= SUMMANDS_SIZE) {\n+      return MemPointerDecomposedForm(pointer, summands, con);\n+    } else {\n+      return MemPointerDecomposedForm(pointer);\n+    }\n+  }\n+\n+  MemPointerAliasing get_aliasing_with(const MemPointerDecomposedForm& other\n+                                       NOT_PRODUCT( COMMA const TraceMemPointer& trace) ) const;\n+\n+  const MemPointerSummand summands_at(const uint i) const {\n+    assert(i < SUMMANDS_SIZE, \"in bounds\");\n+    return _summands[i];\n+  }\n+\n+  const NoOverflowInt con() const { return _con; }\n+\n+#ifndef PRODUCT\n+  void print_on(outputStream* st) const {\n+    if (_pointer == nullptr) {\n+      st->print_cr(\"MemPointerDecomposedForm empty.\");\n+      return;\n+    }\n+    st->print(\"MemPointerDecomposedForm[%d %s:  con = \", _pointer->_idx, _pointer->Name());\n+    _con.print_on(st);\n+    for (int i = 0; i < SUMMANDS_SIZE; i++) {\n+      const MemPointerSummand& summand = _summands[i];\n+      if (summand.variable() != nullptr) {\n+        st->print(\", \");\n+        summand.print_on(st);\n+      }\n+    }\n+    st->print_cr(\"]\");\n+  }\n+#endif\n+};\n+\n+class MemPointerDecomposedFormParser : public StackObj {\n+private:\n+  const MemNode* _mem;\n+\n+  \/\/ Internal data-structures for parsing.\n+  NoOverflowInt _con;\n+  GrowableArray<MemPointerSummand> _worklist;\n+  GrowableArray<MemPointerSummand> _summands;\n+\n+  \/\/ Resulting decomposed-form.\n+  MemPointerDecomposedForm _decomposed_form;\n+\n+public:\n+  MemPointerDecomposedFormParser(const MemNode* mem) : _mem(mem), _con(NoOverflowInt(0)) {\n+    _decomposed_form = parse_decomposed_form();\n+  }\n+\n+  const MemPointerDecomposedForm decomposed_form() const { return _decomposed_form; }\n+\n+private:\n+  MemPointerDecomposedForm parse_decomposed_form();\n+  void parse_sub_expression(const MemPointerSummand summand);\n+\n+  bool is_safe_to_decompose_op(const int opc, const NoOverflowInt scale) const;\n+};\n+\n+\/\/ Facility to parse the pointer of a Load or Store, so that aliasing between two such\n+\/\/ memory operations can be determined (e.g. adjacency).\n+class MemPointer : public StackObj {\n+private:\n+  const MemNode* _mem;\n+  const MemPointerDecomposedForm _decomposed_form;\n+\n+  NOT_PRODUCT( const TraceMemPointer& _trace; )\n+\n+public:\n+  MemPointer(const MemNode* mem NOT_PRODUCT( COMMA const TraceMemPointer& trace)) :\n+    _mem(mem),\n+    _decomposed_form(init_decomposed_form(_mem))\n+    NOT_PRODUCT( COMMA _trace(trace) )\n+  {\n+#ifndef PRODUCT\n+    if (_trace.is_trace_pointer()) {\n+      tty->print_cr(\"MemPointer::MemPointer:\");\n+      tty->print(\"mem: \"); mem->dump();\n+      _mem->in(MemNode::Address)->dump_bfs(5, 0, \"d\");\n+      _decomposed_form.print_on(tty);\n+    }\n+#endif\n+  }\n+\n+  const MemNode* mem() const { return _mem; }\n+  const MemPointerDecomposedForm decomposed_form() const { return _decomposed_form; }\n+  bool is_adjacent_to_and_before(const MemPointer& other) const;\n+\n+private:\n+  static const MemPointerDecomposedForm init_decomposed_form(const MemNode* mem) {\n+    assert(mem->is_Store(), \"only stores are supported\");\n+    ResourceMark rm;\n+    MemPointerDecomposedFormParser parser(mem);\n+    return parser.decomposed_form();\n+  }\n+};\n+\n+#endif \/\/ SHARE_OPTO_MEMPOINTER_HPP\n","filename":"src\/hotspot\/share\/opto\/mempointer.hpp","additions":591,"deletions":0,"binary":false,"changes":591,"status":"added"},{"patch":"@@ -0,0 +1,115 @@\n+\/*\n+ * Copyright (c) 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef SHARE_OPTO_NOOVERFLOWINT_HPP\n+#define SHARE_OPTO_NOOVERFLOWINT_HPP\n+\n+#include \"utilities\/globalDefinitions.hpp\"\n+#include \"utilities\/ostream.hpp\"\n+\n+\/\/ Wrapper around jint, which detects overflow.\n+\/\/ If any operation overflows, then it returns a NaN.\n+class NoOverflowInt {\n+private:\n+  bool _is_NaN; \/\/ overflow, uninitialized, etc.\n+  jint _value;\n+\n+public:\n+  \/\/ Default: NaN.\n+  constexpr NoOverflowInt() : _is_NaN(true), _value(0) {}\n+\n+  \/\/ Create from jlong (or jint) -> NaN if overflows jint.\n+  constexpr explicit NoOverflowInt(jlong value) : _is_NaN(true), _value(0) {\n+    jint trunc = (jint)value;\n+    if ((jlong)trunc == value) {\n+      _is_NaN = false;\n+      _value = trunc;\n+    }\n+  }\n+\n+  static constexpr NoOverflowInt make_NaN() { return NoOverflowInt(); }\n+\n+  bool is_NaN() const { return _is_NaN; }\n+  jint value() const { assert(!is_NaN(), \"NaN not allowed\"); return _value; }\n+  bool is_zero() const { return !is_NaN() && value() == 0; }\n+\n+  friend NoOverflowInt operator+(const NoOverflowInt a, const NoOverflowInt b) {\n+    if (a.is_NaN()) { return a; }\n+    if (b.is_NaN()) { return b; }\n+    return NoOverflowInt((jlong)a.value() + (jlong)b.value());\n+  }\n+\n+  friend NoOverflowInt operator-(const NoOverflowInt a, const NoOverflowInt b) {\n+    if (a.is_NaN()) { return a; }\n+    if (b.is_NaN()) { return b; }\n+    return NoOverflowInt((jlong)a.value() - (jlong)b.value());\n+  }\n+\n+  friend NoOverflowInt operator*(const NoOverflowInt a, const NoOverflowInt b) {\n+    if (a.is_NaN()) { return a; }\n+    if (b.is_NaN()) { return b; }\n+    return NoOverflowInt((jlong)a.value() * (jlong)b.value());\n+  }\n+\n+  friend NoOverflowInt operator<<(const NoOverflowInt a, const NoOverflowInt b) {\n+    if (a.is_NaN()) { return a; }\n+    if (b.is_NaN()) { return b; }\n+    jint shift = b.value();\n+    if (shift < 0 || shift > 31) { return make_NaN(); }\n+    return NoOverflowInt((jlong)a.value() << shift);\n+  }\n+\n+  friend bool operator==(const NoOverflowInt a, const NoOverflowInt b) {\n+    if (a.is_NaN()) { return false; }\n+    if (b.is_NaN()) { return false; }\n+    return a.value() == b.value();\n+  }\n+\n+  NoOverflowInt abs() const {\n+    if (is_NaN()) { return make_NaN(); }\n+    if (value() >= 0) { return *this; }\n+    return NoOverflowInt(0) - *this;\n+  }\n+\n+  bool is_multiple_of(const NoOverflowInt other) const {\n+    NoOverflowInt a = this->abs();\n+    NoOverflowInt b = other.abs();\n+    if (a.is_NaN()) { return false; }\n+    if (b.is_NaN()) { return false; }\n+    if (b.is_zero()) { return false; }\n+    return a.value() % b.value() == 0;\n+  }\n+\n+#ifndef PRODUCT\n+  void print_on(outputStream* st) const {\n+    if (is_NaN()) {\n+      st->print(\"NaN\");\n+    } else {\n+      st->print(\"%d\", value());\n+    }\n+  }\n+#endif\n+};\n+\n+#endif \/\/ SHARE_OPTO_NOOVERFLOWINT_HPP\n","filename":"src\/hotspot\/share\/opto\/noOverflowInt.hpp","additions":115,"deletions":0,"binary":false,"changes":115,"status":"added"},{"patch":"@@ -0,0 +1,138 @@\n+\/*\n+ * Copyright (c) 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef SHARE_OPTO_TRACEMERGESTORESTAG_HPP\n+#define SHARE_OPTO_TRACEMERGESTORESTAG_HPP\n+\n+#include \"utilities\/bitMap.inline.hpp\"\n+#include \"utilities\/stringUtils.hpp\"\n+\n+namespace TraceMergeStores {\n+  #define COMPILER_TAG(flags) \\\n+    flags(BASIC,                \"Trace basic analysis steps\") \\\n+    flags(POINTER,              \"Trace pointer IR\") \\\n+    flags(ALIASING,             \"Trace MemPointerSimpleForm::get_aliasing_with\") \\\n+    flags(ADJACENCY,            \"Trace adjacency\") \\\n+    flags(SUCCESS,              \"Trace successful merges\") \\\n+\n+  #define table_entry(name, description) name,\n+  enum Tag {\n+    COMPILER_TAG(table_entry)\n+    TAG_NUM,\n+    TAG_NONE\n+  };\n+  #undef table_entry\n+\n+  static const char* tag_descriptions[] = {\n+  #define array_of_labels(name, description) description,\n+         COMPILER_TAG(array_of_labels)\n+  #undef array_of_labels\n+  };\n+\n+  static const char* tag_names[] = {\n+  #define array_of_labels(name, description) #name,\n+         COMPILER_TAG(array_of_labels)\n+  #undef array_of_labels\n+  };\n+\n+  static Tag find_tag(const char* str) {\n+    for (int i = 0; i < TAG_NUM; i++) {\n+      if (strcmp(tag_names[i], str) == 0) {\n+        return (Tag)i;\n+      }\n+    }\n+    return TAG_NONE;\n+  }\n+\n+  class TagValidator {\n+   private:\n+    CHeapBitMap _tags;\n+    bool _valid;\n+    char* _bad;\n+    bool _is_print_usage;\n+\n+   public:\n+    TagValidator(ccstrlist option, bool is_print_usage) :\n+      _tags(TAG_NUM, mtCompiler),\n+      _valid(true),\n+      _bad(nullptr),\n+      _is_print_usage(is_print_usage)\n+    {\n+      for (StringUtils::CommaSeparatedStringIterator iter(option); *iter != nullptr && _valid; ++iter) {\n+        char const* tag_name = *iter;\n+        if (strcmp(\"help\", tag_name) == 0) {\n+          if (_is_print_usage) {\n+            print_help();\n+          }\n+          continue;\n+        }\n+        bool set_bit = true;\n+        \/\/ Check for \"TAG\" or \"-TAG\"\n+        if (strncmp(\"-\", tag_name, strlen(\"-\")) == 0) {\n+          tag_name++;\n+          set_bit = false;\n+        }\n+        Tag tag = find_tag(tag_name);\n+        if (TAG_NONE == tag) {\n+          \/\/ cap len to a value we know is enough for all tags\n+          const size_t len = MIN2<size_t>(strlen(*iter), 63) + 1;\n+          _bad = NEW_C_HEAP_ARRAY(char, len, mtCompiler);\n+          \/\/ strncpy always writes len characters. If the source string is\n+          \/\/ shorter, the function fills the remaining bytes with nulls.\n+          strncpy(_bad, *iter, len);\n+          _valid = false;\n+        } else {\n+          assert(tag < TAG_NUM, \"out of bounds\");\n+          _tags.at_put(tag, set_bit);\n+        }\n+      }\n+    }\n+\n+    ~TagValidator() {\n+      if (_bad != nullptr) {\n+        FREE_C_HEAP_ARRAY(char, _bad);\n+      }\n+    }\n+\n+    bool is_valid() const { return _valid; }\n+    const char* what() const { return _bad; }\n+    const CHeapBitMap& tags() const {\n+      assert(is_valid(), \"only read tags when valid\");\n+      return _tags;\n+    }\n+\n+    static void print_help() {\n+      tty->cr();\n+      tty->print_cr(\"Usage for CompileCommand TraceMergeStores:\");\n+      tty->print_cr(\"  -XX:CompileCommand=TraceMergeStores,<package.class::method>,<tags>\");\n+      tty->print_cr(\"  %-22s %s\", \"tags\", \"descriptions\");\n+      for (int i = 0; i < TAG_NUM; i++) {\n+        tty->print_cr(\"  %-22s %s\", tag_names[i], tag_descriptions[i]);\n+      }\n+      tty->cr();\n+    }\n+  };\n+}\n+\n+#endif \/\/ SHARE_OPTO_TRACEMERGESTORESTAG_HPP\n","filename":"src\/hotspot\/share\/opto\/traceMergeStoresTag.hpp","additions":138,"deletions":0,"binary":false,"changes":138,"status":"added"},{"patch":"@@ -0,0 +1,175 @@\n+\/*\n+ * Copyright (c) 2018, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"opto\/noOverflowInt.hpp\"\n+#include \"unittest.hpp\"\n+\n+static void check_jlong(const jlong val) {\n+  const NoOverflowInt x(val);\n+\n+  if (val > max_jint || min_jint > val) {\n+    ASSERT_TRUE(x.is_NaN());\n+  } else {\n+    ASSERT_FALSE(x.is_NaN());\n+    ASSERT_EQ(x.value(), val);\n+  }\n+}\n+\n+TEST_VM(opto, NoOverflowInt_check_jlong) {\n+  jlong start = (jlong)min_jint - 10000LL;\n+  jlong end   = (jlong)max_jint + 10000LL;\n+  for (jlong i = start; i < end; i+= 1000LL) {\n+    check_jlong(i);\n+  }\n+\n+  check_jlong((jlong)min_jint - 1LL);\n+  check_jlong((jlong)min_jint);\n+  check_jlong((jlong)min_jint + 1LL);\n+  check_jlong((jlong)max_jint - 1LL);\n+  check_jlong((jlong)max_jint);\n+  check_jlong((jlong)max_jint + 1LL);\n+\n+  const NoOverflowInt nan;\n+  ASSERT_TRUE(nan.is_NaN());\n+}\n+\n+TEST_VM(opto, NoOverflowInt_add_sub) {\n+  const NoOverflowInt nan;\n+  const NoOverflowInt zero(0);\n+  const NoOverflowInt one(1);\n+  const NoOverflowInt two(2);\n+  const NoOverflowInt big(1 << 30);\n+\n+  ASSERT_EQ((one + two).value(), 3);\n+  ASSERT_EQ((one - two).value(), -1);\n+  ASSERT_TRUE((nan + one).is_NaN());\n+  ASSERT_TRUE((one + nan).is_NaN());\n+  ASSERT_TRUE((nan + nan).is_NaN());\n+  ASSERT_TRUE((nan - one).is_NaN());\n+  ASSERT_TRUE((one - nan).is_NaN());\n+  ASSERT_TRUE((nan - nan).is_NaN());\n+\n+  ASSERT_EQ((big + one).value(), (1 << 30) + 1);\n+  ASSERT_TRUE((big + big).is_NaN());\n+  ASSERT_EQ((big - one).value(), (1 << 30) - 1);\n+  ASSERT_EQ((big - big).value(), 0);\n+\n+  ASSERT_EQ((big - one + big).value(), max_jint);\n+  ASSERT_EQ((zero - big - big).value(), min_jint);\n+  ASSERT_TRUE((zero - big - big - one).is_NaN());\n+}\n+\n+TEST_VM(opto, NoOverflowInt_mul) {\n+  const NoOverflowInt nan;\n+  const NoOverflowInt zero(0);\n+  const NoOverflowInt one(1);\n+  const NoOverflowInt two(2);\n+  const NoOverflowInt big(1 << 30);\n+\n+  ASSERT_EQ((one * two).value(), 2);\n+  ASSERT_TRUE((nan * one).is_NaN());\n+  ASSERT_TRUE((one * nan).is_NaN());\n+  ASSERT_TRUE((nan * nan).is_NaN());\n+\n+  ASSERT_EQ((big * one).value(), (1 << 30));\n+  ASSERT_EQ((one * big).value(), (1 << 30));\n+  ASSERT_EQ((big * zero).value(), 0);\n+  ASSERT_EQ((zero * big).value(), 0);\n+  ASSERT_TRUE((big * big).is_NaN());\n+  ASSERT_TRUE((big * two).is_NaN());\n+\n+  ASSERT_EQ(((big - one) * two).value(), max_jint - 1);\n+  ASSERT_EQ(((one - big) * two).value(), min_jint + 2);\n+  ASSERT_EQ(((zero - big) * two).value(), min_jint);\n+  ASSERT_TRUE(((big + one) * two).is_NaN());\n+  ASSERT_TRUE(((zero - big - one) * two).is_NaN());\n+}\n+\n+TEST_VM(opto, NoOverflowInt_lshift) {\n+  const NoOverflowInt nan;\n+  const NoOverflowInt zero(0);\n+  const NoOverflowInt one(1);\n+  const NoOverflowInt two(2);\n+  const NoOverflowInt big(1 << 30);\n+\n+  for (int i = 0; i < 31; i++) {\n+    ASSERT_EQ((one << NoOverflowInt(i)).value(), 1LL << i);\n+  }\n+  for (int i = 31; i < 1000; i++) {\n+    ASSERT_TRUE((one << NoOverflowInt(i)).is_NaN());\n+  }\n+  for (int i = -1000; i < 0; i++) {\n+    ASSERT_TRUE((one << NoOverflowInt(i)).is_NaN());\n+  }\n+\n+  ASSERT_EQ((NoOverflowInt(3) << NoOverflowInt(2)).value(), 3 * 4);\n+  ASSERT_EQ((NoOverflowInt(11) << NoOverflowInt(5)).value(), 11 * 32);\n+  ASSERT_EQ((NoOverflowInt(-13) << NoOverflowInt(4)).value(), -13 * 16);\n+}\n+\n+TEST_VM(opto, NoOverflowInt_misc) {\n+  const NoOverflowInt nan;\n+  const NoOverflowInt zero(0);\n+  const NoOverflowInt one(1);\n+  const NoOverflowInt two(2);\n+  const NoOverflowInt big(1 << 30);\n+\n+  \/\/ operator==\n+  ASSERT_FALSE(nan == nan);\n+  ASSERT_FALSE(nan == zero);\n+  ASSERT_FALSE(zero == nan);\n+  ASSERT_TRUE(zero == zero);\n+  ASSERT_TRUE(one == one);\n+  ASSERT_TRUE((one + two) == (two + one));\n+  ASSERT_TRUE((big + two) == (two + big));\n+  ASSERT_FALSE((big + big) == (big + big));\n+  ASSERT_TRUE((big - one + big) == (big - one + big));\n+\n+  \/\/ abs\n+  for (int i = 0; i < (1 << 31); i += 1024) {\n+    ASSERT_EQ(NoOverflowInt(i).abs().value(), i);\n+    ASSERT_EQ(NoOverflowInt(-i).abs().value(), i);\n+  }\n+  ASSERT_EQ(NoOverflowInt(max_jint).abs().value(), max_jint);\n+  ASSERT_EQ(NoOverflowInt(min_jint + 1).abs().value(), max_jint);\n+  ASSERT_TRUE(NoOverflowInt(min_jint).abs().is_NaN());\n+  ASSERT_TRUE(NoOverflowInt(nan).abs().is_NaN());\n+\n+  \/\/ is_multiple_of\n+  ASSERT_TRUE(one.is_multiple_of(one));\n+  ASSERT_FALSE(one.is_multiple_of(nan));\n+  ASSERT_FALSE(nan.is_multiple_of(one));\n+  ASSERT_FALSE(nan.is_multiple_of(nan));\n+  for (int i = 0; i < (1 << 31); i += 1023) {\n+    ASSERT_TRUE(NoOverflowInt(i).is_multiple_of(one));\n+    ASSERT_TRUE(NoOverflowInt(-i).is_multiple_of(one));\n+    ASSERT_FALSE(NoOverflowInt(i).is_multiple_of(zero));\n+    ASSERT_FALSE(NoOverflowInt(-i).is_multiple_of(zero));\n+  }\n+  ASSERT_TRUE(NoOverflowInt(33 * 7).is_multiple_of(NoOverflowInt(33)));\n+  ASSERT_TRUE(NoOverflowInt(13 * 5).is_multiple_of(NoOverflowInt(5)));\n+  ASSERT_FALSE(NoOverflowInt(7).is_multiple_of(NoOverflowInt(5)));\n+}\n+\n","filename":"test\/hotspot\/gtest\/opto\/test_no_overflow_int.cpp","additions":175,"deletions":0,"binary":false,"changes":175,"status":"added"},{"patch":"@@ -36,1 +36,1 @@\n- * @bug 8318446 8331054 8331311\n+ * @bug 8318446 8331054 8331311 8335392\n@@ -45,1 +45,1 @@\n- * @bug 8318446 8331054 8331311\n+ * @bug 8318446 8331054 8331311 8335392\n@@ -78,0 +78,11 @@\n+    static int zero0 = 0;\n+    static int zero1 = 0;\n+    static int zero2 = 0;\n+    static int zero3 = 0;\n+    static int zero4 = 0;\n+    static int zero5 = 0;\n+    static int zero6 = 0;\n+    static int zero7 = 0;\n+    static int zero8 = 0;\n+    static int zero9 = 0;\n+\n@@ -157,0 +168,9 @@\n+        testGroups.put(\"test10\", new HashMap<String,TestFunction>());\n+        testGroups.get(\"test10\").put(\"test10R\", (_,_) -> { return test10R(aB.clone()); });\n+        testGroups.get(\"test10\").put(\"test10a\", (_,_) -> { return test10a(aB.clone()); });\n+        testGroups.get(\"test10\").put(\"test10b\", (_,_) -> { return test10b(aB.clone()); });\n+        testGroups.get(\"test10\").put(\"test10c\", (_,_) -> { return test10c(aB.clone()); });\n+        testGroups.get(\"test10\").put(\"test10d\", (_,_) -> { return test10d(aB.clone()); });\n+        testGroups.get(\"test10\").put(\"test10e\", (_,_) -> { return test10e(aB.clone()); });\n+        testGroups.get(\"test10\").put(\"test10f\", (_,_) -> { return test10f(aB.clone()); });\n+\n@@ -237,0 +257,4 @@\n+        testGroups.put(\"test601\", new HashMap<String,TestFunction>());\n+        testGroups.get(\"test601\").put(\"test601R\", (_,i) -> { return test601R(aB.clone(), aI.clone(), i, offset1); });\n+        testGroups.get(\"test601\").put(\"test601a\", (_,i) -> { return test601a(aB.clone(), aI.clone(), i, offset1); });\n+\n@@ -277,0 +301,6 @@\n+                 \"test10a\",\n+                 \"test10b\",\n+                 \"test10c\",\n+                 \"test10d\",\n+                 \"test10e\",\n+                 \"test10f\",\n@@ -295,0 +325,1 @@\n+                 \"test601a\",\n@@ -614,3 +645,2 @@\n-    \/\/ Disabled by JDK-8335390, to be enabled again by JDK-8335392.\n-    \/\/ @IR(counts = {IRNode.STORE_L_OF_CLASS, \"byte\\\\\\\\[int:>=0] \\\\\\\\(java\/lang\/Cloneable,java\/io\/Serializable\\\\\\\\)\", \"1\"},\n-    \/\/     applyIf = {\"UseUnalignedAccesses\", \"true\"})\n+    @IR(counts = {IRNode.STORE_L_OF_CLASS, \"byte\\\\\\\\[int:>=0] \\\\\\\\(java\/lang\/Cloneable,java\/io\/Serializable\\\\\\\\)\", \"1\"},\n+        applyIf = {\"UseUnalignedAccesses\", \"true\"})\n@@ -1127,0 +1157,139 @@\n+    @DontCompile\n+    static Object[] test10R(byte[] a) {\n+        int zero = zero0 + zero1 + zero2 + zero3 + zero4\n+                 + zero5 + zero6 + zero7 + zero8 + zero9;\n+        a[zero + 0] = 'h';\n+        a[zero + 1] = 'e';\n+        a[zero + 2] = 'l';\n+        a[zero + 3] = 'l';\n+        a[zero + 4] = 'o';\n+        a[zero + 5] = ' ';\n+        a[zero + 6] = ':';\n+        a[zero + 7] = ')';\n+        return new Object[]{ a };\n+    }\n+\n+    @Test\n+    @IR(counts = {IRNode.STORE_B_OF_CLASS, \"byte\\\\\\\\[int:>=0] \\\\\\\\(java\/lang\/Cloneable,java\/io\/Serializable\\\\\\\\)\", \"8\", \/\/ no merge\n+                  IRNode.STORE_C_OF_CLASS, \"byte\\\\\\\\[int:>=0] \\\\\\\\(java\/lang\/Cloneable,java\/io\/Serializable\\\\\\\\)\", \"0\",\n+                  IRNode.STORE_I_OF_CLASS, \"byte\\\\\\\\[int:>=0] \\\\\\\\(java\/lang\/Cloneable,java\/io\/Serializable\\\\\\\\)\", \"0\",\n+                  IRNode.STORE_L_OF_CLASS, \"byte\\\\\\\\[int:>=0] \\\\\\\\(java\/lang\/Cloneable,java\/io\/Serializable\\\\\\\\)\", \"0\"})\n+    static Object[] test10a(byte[] a) {\n+        \/\/ We have 11 summands: 10x zero variable + 1x array base.\n+        \/\/ Parsing only allows 10 summands -> does not merge the stores.\n+        int zero = zero0 + zero1 + zero2 + zero3 + zero4\n+                 + zero5 + zero6 + zero7 + zero8 + zero9;\n+        a[zero + 0] = 'h';\n+        a[zero + 1] = 'e';\n+        a[zero + 2] = 'l';\n+        a[zero + 3] = 'l';\n+        a[zero + 4] = 'o';\n+        a[zero + 5] = ' ';\n+        a[zero + 6] = ':';\n+        a[zero + 7] = ')';\n+        return new Object[]{ a };\n+    }\n+\n+    @Test\n+    @IR(counts = {IRNode.STORE_B_OF_CLASS, \"byte\\\\\\\\[int:>=0] \\\\\\\\(java\/lang\/Cloneable,java\/io\/Serializable\\\\\\\\)\", \"1\", \/\/ 1 left in uncommon trap path of RangeCheck\n+                  IRNode.STORE_C_OF_CLASS, \"byte\\\\\\\\[int:>=0] \\\\\\\\(java\/lang\/Cloneable,java\/io\/Serializable\\\\\\\\)\", \"0\",\n+                  IRNode.STORE_I_OF_CLASS, \"byte\\\\\\\\[int:>=0] \\\\\\\\(java\/lang\/Cloneable,java\/io\/Serializable\\\\\\\\)\", \"0\",\n+                  IRNode.STORE_L_OF_CLASS, \"byte\\\\\\\\[int:>=0] \\\\\\\\(java\/lang\/Cloneable,java\/io\/Serializable\\\\\\\\)\", \"1\"}, \/\/ all merged\n+        applyIf = {\"UseUnalignedAccesses\", \"true\"})\n+    static Object[] test10b(byte[] a) {\n+        int zero = zero0 + zero1 + zero2 + zero3 + zero4\n+                 + zero5 + zero6 + zero7 + zero8;\n+        \/\/ We have 10 summands: 9x zero variable + 1x array base.\n+        \/\/ Parsing allows 10 summands, so this should merge the stores.\n+        a[zero + 0] = 'h';\n+        a[zero + 1] = 'e';\n+        a[zero + 2] = 'l';\n+        a[zero + 3] = 'l';\n+        a[zero + 4] = 'o';\n+        a[zero + 5] = ' ';\n+        a[zero + 6] = ':';\n+        a[zero + 7] = ')';\n+        return new Object[]{ a };\n+    }\n+\n+    @Test\n+    @IR(counts = {IRNode.STORE_B_OF_CLASS, \"byte\\\\\\\\[int:>=0] \\\\\\\\(java\/lang\/Cloneable,java\/io\/Serializable\\\\\\\\)\", \"1\", \/\/ 1 left in uncommon trap path of RangeCheck\n+                  IRNode.STORE_C_OF_CLASS, \"byte\\\\\\\\[int:>=0] \\\\\\\\(java\/lang\/Cloneable,java\/io\/Serializable\\\\\\\\)\", \"0\",\n+                  IRNode.STORE_I_OF_CLASS, \"byte\\\\\\\\[int:>=0] \\\\\\\\(java\/lang\/Cloneable,java\/io\/Serializable\\\\\\\\)\", \"0\",\n+                  IRNode.STORE_L_OF_CLASS, \"byte\\\\\\\\[int:>=0] \\\\\\\\(java\/lang\/Cloneable,java\/io\/Serializable\\\\\\\\)\", \"1\"}, \/\/ all merged\n+        applyIf = {\"UseUnalignedAccesses\", \"true\"})\n+    static Object[] test10c(byte[] a) {\n+        int zero = 7 * zero0 + 7 * zero1 + 7 * zero2 + 7 * zero3 + 7 * zero4\n+                 + 7 * zero5 + 7 * zero6 + 7 * zero7 + 7 * zero8;\n+        \/\/ The \"7 * zero\" is split into \"zero << 3 - zero\". But the parsing combines it again, lowering the summand count.\n+        \/\/ We have 10 summands: 9x zero variable + 1x array base.\n+        \/\/ Parsing allows 10 summands, so this should merge the stores.\n+        a[zero + 0] = 'h';\n+        a[zero + 1] = 'e';\n+        a[zero + 2] = 'l';\n+        a[zero + 3] = 'l';\n+        a[zero + 4] = 'o';\n+        a[zero + 5] = ' ';\n+        a[zero + 6] = ':';\n+        a[zero + 7] = ')';\n+        return new Object[]{ a };\n+    }\n+\n+    @Test\n+    @IR(counts = {IRNode.STORE_B_OF_CLASS, \"byte\\\\\\\\[int:>=0] \\\\\\\\(java\/lang\/Cloneable,java\/io\/Serializable\\\\\\\\)\", \"0\",\n+                  IRNode.STORE_C_OF_CLASS, \"byte\\\\\\\\[int:>=0] \\\\\\\\(java\/lang\/Cloneable,java\/io\/Serializable\\\\\\\\)\", \"0\",\n+                  IRNode.STORE_I_OF_CLASS, \"byte\\\\\\\\[int:>=0] \\\\\\\\(java\/lang\/Cloneable,java\/io\/Serializable\\\\\\\\)\", \"0\",\n+                  IRNode.STORE_L_OF_CLASS, \"byte\\\\\\\\[int:>=0] \\\\\\\\(java\/lang\/Cloneable,java\/io\/Serializable\\\\\\\\)\", \"1\"}, \/\/ all merged\n+        applyIf = {\"UseUnalignedAccesses\", \"true\"})\n+    static Object[] test10d(byte[] a) {\n+        \/\/ Summand is subtracted from itself -> scale = 0 -> should be removed from list.\n+        UNSAFE.putByte(a, UNSAFE.ARRAY_BYTE_BASE_OFFSET + (long)(zero0 + 0) - zero0, (byte)'h');\n+        UNSAFE.putByte(a, UNSAFE.ARRAY_BYTE_BASE_OFFSET + (long)(zero0 + 1) - zero0, (byte)'e');\n+        UNSAFE.putByte(a, UNSAFE.ARRAY_BYTE_BASE_OFFSET + (long)(zero0 + 2) - zero0, (byte)'l');\n+        UNSAFE.putByte(a, UNSAFE.ARRAY_BYTE_BASE_OFFSET + (long)(zero0 + 3) - zero0, (byte)'l');\n+        UNSAFE.putByte(a, UNSAFE.ARRAY_BYTE_BASE_OFFSET + (long)(zero0 + 4) - zero0, (byte)'o');\n+        UNSAFE.putByte(a, UNSAFE.ARRAY_BYTE_BASE_OFFSET + (long)(zero0 + 5) - zero0, (byte)' ');\n+        UNSAFE.putByte(a, UNSAFE.ARRAY_BYTE_BASE_OFFSET + (long)(zero0 + 6) - zero0, (byte)':');\n+        UNSAFE.putByte(a, UNSAFE.ARRAY_BYTE_BASE_OFFSET + (long)(zero0 + 7) - zero0, (byte)')');\n+        return new Object[]{ a };\n+    }\n+\n+    @Test\n+    @IR(counts = {IRNode.STORE_B_OF_CLASS, \"byte\\\\\\\\[int:>=0] \\\\\\\\(java\/lang\/Cloneable,java\/io\/Serializable\\\\\\\\)\", \"0\",\n+                  IRNode.STORE_C_OF_CLASS, \"byte\\\\\\\\[int:>=0] \\\\\\\\(java\/lang\/Cloneable,java\/io\/Serializable\\\\\\\\)\", \"0\",\n+                  IRNode.STORE_I_OF_CLASS, \"byte\\\\\\\\[int:>=0] \\\\\\\\(java\/lang\/Cloneable,java\/io\/Serializable\\\\\\\\)\", \"0\",\n+                  IRNode.STORE_L_OF_CLASS, \"byte\\\\\\\\[int:>=0] \\\\\\\\(java\/lang\/Cloneable,java\/io\/Serializable\\\\\\\\)\", \"1\"}, \/\/ all merged\n+        applyIf = {\"UseUnalignedAccesses\", \"true\"})\n+    static Object[] test10e(byte[] a) {\n+        \/\/ Summand is subtracted from itself -> scale = 0 -> should be removed from list. Thus equal to if not present at all.\n+        UNSAFE.putByte(a, UNSAFE.ARRAY_BYTE_BASE_OFFSET + (long)(zero0 + 0) - zero0, (byte)'h');\n+        UNSAFE.putByte(a, UNSAFE.ARRAY_BYTE_BASE_OFFSET + (long)(zero0 + 1) - zero0, (byte)'e');\n+        UNSAFE.putByte(a, UNSAFE.ARRAY_BYTE_BASE_OFFSET + (long)(zero0 + 2) - zero0, (byte)'l');\n+        UNSAFE.putByte(a, UNSAFE.ARRAY_BYTE_BASE_OFFSET + (long)(zero0 + 3) - zero0, (byte)'l');\n+        UNSAFE.putByte(a, UNSAFE.ARRAY_BYTE_BASE_OFFSET +                4,          (byte)'o');\n+        UNSAFE.putByte(a, UNSAFE.ARRAY_BYTE_BASE_OFFSET +                5,          (byte)' ');\n+        UNSAFE.putByte(a, UNSAFE.ARRAY_BYTE_BASE_OFFSET +                6,          (byte)':');\n+        UNSAFE.putByte(a, UNSAFE.ARRAY_BYTE_BASE_OFFSET +                7,          (byte)')');\n+        return new Object[]{ a };\n+    }\n+\n+    @Test\n+    @IR(counts = {IRNode.STORE_B_OF_CLASS, \"byte\\\\\\\\[int:>=0] \\\\\\\\(java\/lang\/Cloneable,java\/io\/Serializable\\\\\\\\)\", \"8\", \/\/ no merge\n+                  IRNode.STORE_C_OF_CLASS, \"byte\\\\\\\\[int:>=0] \\\\\\\\(java\/lang\/Cloneable,java\/io\/Serializable\\\\\\\\)\", \"0\",\n+                  IRNode.STORE_I_OF_CLASS, \"byte\\\\\\\\[int:>=0] \\\\\\\\(java\/lang\/Cloneable,java\/io\/Serializable\\\\\\\\)\", \"0\",\n+                  IRNode.STORE_L_OF_CLASS, \"byte\\\\\\\\[int:>=0] \\\\\\\\(java\/lang\/Cloneable,java\/io\/Serializable\\\\\\\\)\", \"0\"})\n+    static Object[] test10f(byte[] a) {\n+        int big = 1 << 29;\n+        \/\/ Adding up the scales overflows -> no merge.\n+        long offset = zero9 * big + zero9 * big + zero9 * big + zero9 * big;\n+        UNSAFE.putByte(a, UNSAFE.ARRAY_BYTE_BASE_OFFSET + offset + 0, (byte)'h');\n+        UNSAFE.putByte(a, UNSAFE.ARRAY_BYTE_BASE_OFFSET + offset + 1, (byte)'e');\n+        UNSAFE.putByte(a, UNSAFE.ARRAY_BYTE_BASE_OFFSET + offset + 2, (byte)'l');\n+        UNSAFE.putByte(a, UNSAFE.ARRAY_BYTE_BASE_OFFSET + offset + 3, (byte)'l');\n+        UNSAFE.putByte(a, UNSAFE.ARRAY_BYTE_BASE_OFFSET + offset + 4, (byte)'o');\n+        UNSAFE.putByte(a, UNSAFE.ARRAY_BYTE_BASE_OFFSET + offset + 5, (byte)' ');\n+        UNSAFE.putByte(a, UNSAFE.ARRAY_BYTE_BASE_OFFSET + offset + 6, (byte)':');\n+        UNSAFE.putByte(a, UNSAFE.ARRAY_BYTE_BASE_OFFSET + offset + 7, (byte)')');\n+        return new Object[]{ a };\n+    }\n+\n@@ -1563,6 +1732,2 @@\n-    \/\/ We must be careful with mismatched accesses on arrays:\n-    \/\/ An int-array can have about 2x max_int size, and hence if we address bytes in it, we can have int-overflows.\n-    \/\/ We might consider addresses (x + 0) and (x + 1) as adjacent, even if x = max_int, and therefore the second\n-    \/\/ address overflows and is not adjacent at all.\n-    \/\/ Therefore, we should only consider stores that have the same size as the element type of the array.\n-    @IR(counts = {IRNode.STORE_B_OF_CLASS, \"int\\\\\\\\[int:>=0] \\\\\\\\(java\/lang\/Cloneable,java\/io\/Serializable\\\\\\\\)\", \"8\", \/\/ no merging\n+    \/\/ All constants are known, and AddI can be converted to AddL safely, hence the stores can be merged.\n+    @IR(counts = {IRNode.STORE_B_OF_CLASS, \"int\\\\\\\\[int:>=0] \\\\\\\\(java\/lang\/Cloneable,java\/io\/Serializable\\\\\\\\)\", \"0\",\n@@ -1571,1 +1736,2 @@\n-                  IRNode.STORE_L_OF_CLASS, \"int\\\\\\\\[int:>=0] \\\\\\\\(java\/lang\/Cloneable,java\/io\/Serializable\\\\\\\\)\", \"0\"})\n+                  IRNode.STORE_L_OF_CLASS, \"int\\\\\\\\[int:>=0] \\\\\\\\(java\/lang\/Cloneable,java\/io\/Serializable\\\\\\\\)\", \"1\"}, \/\/ all merged\n+        applyIf = {\"UseUnalignedAccesses\", \"true\"})\n@@ -1861,1 +2027,5 @@\n-    @IR(counts = {IRNode.STORE_B_OF_CLASS, \"bottom\\\\\\\\[int:>=0] \\\\\\\\(java\/lang\/Cloneable,java\/io\/Serializable\\\\\\\\)\", \"8\"}) \/\/ note: bottom type\n+    @IR(counts = {IRNode.STORE_B_OF_CLASS, \"bottom\\\\\\\\[int:>=0] \\\\\\\\(java\/lang\/Cloneable,java\/io\/Serializable\\\\\\\\)\", \"0\",\n+                  IRNode.STORE_C_OF_CLASS, \"bottom\\\\\\\\[int:>=0] \\\\\\\\(java\/lang\/Cloneable,java\/io\/Serializable\\\\\\\\)\", \"0\",\n+                  IRNode.STORE_I_OF_CLASS, \"bottom\\\\\\\\[int:>=0] \\\\\\\\(java\/lang\/Cloneable,java\/io\/Serializable\\\\\\\\)\", \"0\",\n+                  IRNode.STORE_L_OF_CLASS, \"bottom\\\\\\\\[int:>=0] \\\\\\\\(java\/lang\/Cloneable,java\/io\/Serializable\\\\\\\\)\", \"1\"}, \/\/ all merged\n+        applyIf = {\"UseUnalignedAccesses\", \"true\"})\n@@ -1872,1 +2042,1 @@\n-        \/\/ array a is an aryptr, but its element type is unknown, i.e. bottom.\n+        \/\/ Array type is unknown, i.e. bottom[]. But all AddI can be safely converted to AddL -> safe to merge.\n@@ -1884,0 +2054,57 @@\n+    @DontCompile\n+    static Object[] test601R(byte[] aB, int[] aI, int i, int offset1) {\n+        Object a = null;\n+        long base = 0;\n+        if (i % 2 == 0) {\n+            a = aB;\n+            base = UNSAFE.ARRAY_BYTE_BASE_OFFSET;\n+        } else {\n+            a = aI;\n+            base = UNSAFE.ARRAY_INT_BASE_OFFSET;\n+        }\n+        UNSAFE.putByte(a, base + (offset1 + 0), (byte)0xbe);\n+        UNSAFE.putByte(a, base + (offset1 + 1), (byte)0xba);\n+        UNSAFE.putByte(a, base + (offset1 + 2), (byte)0xad);\n+        UNSAFE.putByte(a, base + (offset1 + 3), (byte)0xba);\n+        UNSAFE.putByte(a, base + (offset1 + 4), (byte)0xef);\n+        UNSAFE.putByte(a, base + (offset1 + 5), (byte)0xbe);\n+        UNSAFE.putByte(a, base + (offset1 + 6), (byte)0xad);\n+        UNSAFE.putByte(a, base + (offset1 + 7), (byte)0xde);\n+        return new Object[]{ aB, aI };\n+    }\n+\n+    @Test\n+    @IR(counts = {IRNode.STORE_B_OF_CLASS, \"bottom\\\\\\\\[int:>=0] \\\\\\\\(java\/lang\/Cloneable,java\/io\/Serializable\\\\\\\\)\", \"8\",  \/\/ nothing merged\n+                  IRNode.STORE_C_OF_CLASS, \"bottom\\\\\\\\[int:>=0] \\\\\\\\(java\/lang\/Cloneable,java\/io\/Serializable\\\\\\\\)\", \"0\",\n+                  IRNode.STORE_I_OF_CLASS, \"bottom\\\\\\\\[int:>=0] \\\\\\\\(java\/lang\/Cloneable,java\/io\/Serializable\\\\\\\\)\", \"0\",\n+                  IRNode.STORE_L_OF_CLASS, \"bottom\\\\\\\\[int:>=0] \\\\\\\\(java\/lang\/Cloneable,java\/io\/Serializable\\\\\\\\)\", \"0\"},\n+        applyIfPlatform = {\"64-bit\", \"true\"})\n+    @IR(counts = {IRNode.STORE_B_OF_CLASS, \"bottom\\\\\\\\[int:>=0] \\\\\\\\(java\/lang\/Cloneable,java\/io\/Serializable\\\\\\\\)\", \"0\",\n+                  IRNode.STORE_C_OF_CLASS, \"bottom\\\\\\\\[int:>=0] \\\\\\\\(java\/lang\/Cloneable,java\/io\/Serializable\\\\\\\\)\", \"0\",\n+                  IRNode.STORE_I_OF_CLASS, \"bottom\\\\\\\\[int:>=0] \\\\\\\\(java\/lang\/Cloneable,java\/io\/Serializable\\\\\\\\)\", \"0\",\n+                  IRNode.STORE_L_OF_CLASS, \"bottom\\\\\\\\[int:>=0] \\\\\\\\(java\/lang\/Cloneable,java\/io\/Serializable\\\\\\\\)\", \"1\"}, \/\/ all merged\n+        applyIf = {\"UseUnalignedAccesses\", \"true\"},\n+        applyIfPlatform = {\"32-bit\", \"true\"})\n+    static Object[] test601a(byte[] aB, int[] aI, int i, int offset1) {\n+        Object a = null;\n+        long base = 0;\n+        if (i % 2 == 0) {\n+            a = aB;\n+            base = UNSAFE.ARRAY_BYTE_BASE_OFFSET;\n+        } else {\n+            a = aI;\n+            base = UNSAFE.ARRAY_INT_BASE_OFFSET;\n+        }\n+        \/\/ Array type is unknown, i.e. bottom[]. Hence we do not know the element size of the array.\n+        \/\/ Thus, on 64-bits systems merging is not safe, there could be overflows.\n+        UNSAFE.putByte(a, base + (offset1 + 0), (byte)0xbe);\n+        UNSAFE.putByte(a, base + (offset1 + 1), (byte)0xba);\n+        UNSAFE.putByte(a, base + (offset1 + 2), (byte)0xad);\n+        UNSAFE.putByte(a, base + (offset1 + 3), (byte)0xba);\n+        UNSAFE.putByte(a, base + (offset1 + 4), (byte)0xef);\n+        UNSAFE.putByte(a, base + (offset1 + 5), (byte)0xbe);\n+        UNSAFE.putByte(a, base + (offset1 + 6), (byte)0xad);\n+        UNSAFE.putByte(a, base + (offset1 + 7), (byte)0xde);\n+        return new Object[]{ aB, aI };\n+    }\n+\n","filename":"test\/hotspot\/jtreg\/compiler\/c2\/TestMergeStores.java","additions":241,"deletions":14,"binary":false,"changes":255,"status":"modified"},{"patch":"@@ -0,0 +1,426 @@\n+\/*\n+ * Copyright (c) 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+package compiler.c2;\n+\n+import compiler.lib.ir_framework.*;\n+import jdk.test.lib.Utils;\n+import java.nio.ByteBuffer;\n+import java.util.Map;\n+import java.util.HashMap;\n+import java.util.Random;\n+import java.lang.foreign.*;\n+\n+\/*\n+ * @test id=byte-array\n+ * @bug 8335392\n+ * @summary Test MergeStores optimization for MemorySegment\n+ * @library \/test\/lib \/\n+ * @run driver compiler.c2.TestMergeStoresMemorySegment ByteArray\n+ *\/\n+\n+\/*\n+ * @test id=char-array\n+ * @bug 8335392\n+ * @summary Test MergeStores optimization for MemorySegment\n+ * @library \/test\/lib \/\n+ * @run driver compiler.c2.TestMergeStoresMemorySegment CharArray\n+ *\/\n+\n+\/*\n+ * @test id=short-array\n+ * @bug 8335392\n+ * @summary Test MergeStores optimization for MemorySegment\n+ * @library \/test\/lib \/\n+ * @run driver compiler.c2.TestMergeStoresMemorySegment ShortArray\n+ *\/\n+\n+\/*\n+ * @test id=int-array\n+ * @bug 8335392\n+ * @summary Test MergeStores optimization for MemorySegment\n+ * @library \/test\/lib \/\n+ * @run driver compiler.c2.TestMergeStoresMemorySegment IntArray\n+ *\/\n+\n+\/*\n+ * @test id=long-array\n+ * @bug 8335392\n+ * @summary Test MergeStores optimization for MemorySegment\n+ * @library \/test\/lib \/\n+ * @run driver compiler.c2.TestMergeStoresMemorySegment LongArray\n+ *\/\n+\n+\/*\n+ * @test id=float-array\n+ * @bug 8335392\n+ * @summary Test MergeStores optimization for MemorySegment\n+ * @library \/test\/lib \/\n+ * @run driver compiler.c2.TestMergeStoresMemorySegment FloatArray\n+ *\/\n+\n+\/*\n+ * @test id=double-array\n+ * @bug 8335392\n+ * @summary Test MergeStores optimization for MemorySegment\n+ * @library \/test\/lib \/\n+ * @run driver compiler.c2.TestMergeStoresMemorySegment DoubleArray\n+ *\/\n+\n+\/*\n+ * @test id=byte-buffer\n+ * @bug 8335392\n+ * @summary Test MergeStores optimization for MemorySegment\n+ * @library \/test\/lib \/\n+ * @run driver compiler.c2.TestMergeStoresMemorySegment ByteBuffer\n+ *\/\n+\n+\/*\n+ * @test id=byte-buffer-direct\n+ * @bug 8335392\n+ * @summary Test MergeStores optimization for MemorySegment\n+ * @library \/test\/lib \/\n+ * @run driver compiler.c2.TestMergeStoresMemorySegment ByteBufferDirect\n+ *\/\n+\n+\/*\n+ * @test id=native\n+ * @bug 8335392\n+ * @summary Test MergeStores optimization for MemorySegment\n+ * @library \/test\/lib \/\n+ * @run driver compiler.c2.TestMergeStoresMemorySegment Native\n+ *\/\n+\n+\/\/ FAILS: mixed providers currently do not merge stores. Maybe there is some inlining issue.\n+\/\/ \/*\n+\/\/  * @test id=mixed-array\n+\/\/  * @bug 8335392\n+\/\/  * @summary Test MergeStores optimization for MemorySegment\n+\/\/  * @library \/test\/lib \/\n+\/\/  * @run driver compiler.c2.TestMergeStoresMemorySegment MixedArray\n+\/\/  *\/\n+\/\/\n+\/\/ \/*\n+\/\/  * @test id=MixedBuffer\n+\/\/  * @bug 8335392\n+\/\/  * @summary Test MergeStores optimization for MemorySegment\n+\/\/  * @library \/test\/lib \/\n+\/\/  * @run driver compiler.c2.TestMergeStoresMemorySegment MixedBuffer\n+\/\/  *\/\n+\/\/\n+\/\/ \/*\n+\/\/  * @test id=mixed\n+\/\/  * @bug 8335392\n+\/\/  * @summary Test MergeStores optimization for MemorySegment\n+\/\/  * @library \/test\/lib \/\n+\/\/  * @run driver compiler.c2.TestMergeStoresMemorySegment Mixed\n+\/\/  *\/\n+\n+public class TestMergeStoresMemorySegment {\n+    public static void main(String[] args) {\n+        for (String unaligned : new String[]{\"-XX:-UseUnalignedAccesses\", \"-XX:+UseUnalignedAccesses\"}) {\n+            TestFramework framework = new TestFramework(TestMergeStoresMemorySegmentImpl.class);\n+            framework.addFlags(\"-DmemorySegmentProviderNameForTestVM=\" + args[0], unaligned);\n+            framework.start();\n+        }\n+    }\n+}\n+\n+class TestMergeStoresMemorySegmentImpl {\n+    static final int BACKING_SIZE = 1024 * 8;\n+    static final Random RANDOM = Utils.getRandomInstance();\n+\n+    private static final String START = \"(\\\\d+(\\\\s){2}(\";\n+    private static final String MID = \".*)+(\\\\s){2}===.*\";\n+    private static final String END = \")\";\n+\n+    \/\/ Custom Regex: allows us to only match Store that come from MemorySegment internals.\n+    private static final String REGEX_STORE_B_TO_MS_FROM_B = START + \"StoreB\" + MID + END + \"ScopedMemoryAccess::putByteInternal\";\n+    private static final String REGEX_STORE_C_TO_MS_FROM_B = START + \"StoreC\" + MID + END + \"ScopedMemoryAccess::putByteInternal\";\n+    private static final String REGEX_STORE_I_TO_MS_FROM_B = START + \"StoreI\" + MID + END + \"ScopedMemoryAccess::putByteInternal\";\n+    private static final String REGEX_STORE_L_TO_MS_FROM_B = START + \"StoreL\" + MID + END + \"ScopedMemoryAccess::putByteInternal\";\n+\n+    interface TestFunction {\n+        Object[] run();\n+    }\n+\n+    interface MemorySegmentProvider {\n+        MemorySegment newMemorySegment();\n+    }\n+\n+    static MemorySegmentProvider provider;\n+\n+    static {\n+        String providerName = System.getProperty(\"memorySegmentProviderNameForTestVM\");\n+        provider = switch (providerName) {\n+            case \"ByteArray\"        -> TestMergeStoresMemorySegmentImpl::newMemorySegmentOfByteArray;\n+            case \"CharArray\"        -> TestMergeStoresMemorySegmentImpl::newMemorySegmentOfCharArray;\n+            case \"ShortArray\"       -> TestMergeStoresMemorySegmentImpl::newMemorySegmentOfShortArray;\n+            case \"IntArray\"         -> TestMergeStoresMemorySegmentImpl::newMemorySegmentOfIntArray;\n+            case \"LongArray\"        -> TestMergeStoresMemorySegmentImpl::newMemorySegmentOfLongArray;\n+            case \"FloatArray\"       -> TestMergeStoresMemorySegmentImpl::newMemorySegmentOfFloatArray;\n+            case \"DoubleArray\"      -> TestMergeStoresMemorySegmentImpl::newMemorySegmentOfDoubleArray;\n+            case \"ByteBuffer\"       -> TestMergeStoresMemorySegmentImpl::newMemorySegmentOfByteBuffer;\n+            case \"ByteBufferDirect\" -> TestMergeStoresMemorySegmentImpl::newMemorySegmentOfByteBufferDirect;\n+            case \"Native\"           -> TestMergeStoresMemorySegmentImpl::newMemorySegmentOfNative;\n+            case \"MixedArray\"       -> TestMergeStoresMemorySegmentImpl::newMemorySegmentOfMixedArray;\n+            case \"MixedBuffer\"      -> TestMergeStoresMemorySegmentImpl::newMemorySegmentOfMixedBuffer;\n+            case \"Mixed\"            -> TestMergeStoresMemorySegmentImpl::newMemorySegmentOfMixed;\n+            default -> throw new RuntimeException(\"Test argument not recognized: \" + providerName);\n+        };\n+    }\n+\n+    \/\/ List of tests\n+    Map<String, TestFunction> tests = new HashMap<>();\n+\n+    \/\/ List of gold, the results from the first run before compilation\n+    Map<String, Object[]> golds = new HashMap<>();\n+\n+    public TestMergeStoresMemorySegmentImpl () {\n+        \/\/ Generate two MemorySegments as inputs\n+        MemorySegment a = newMemorySegment();\n+        MemorySegment b = newMemorySegment();\n+        fillRandom(a);\n+        fillRandom(b);\n+\n+        \/\/ Future Work: add more test cases. For now, the issue seems to be that\n+        \/\/              RangeCheck smearing does not remove the RangeChecks, thus\n+        \/\/              we can only ever merge two stores.\n+        \/\/\n+        \/\/ Ideas for more test cases, once they are better optimized:\n+        \/\/\n+        \/\/   Have about 3 variables, each either int or long. Add all in int or\n+        \/\/   long. Give them different scales. Compute the address in the same\n+        \/\/   expression or separately. Use different element store sizes (BCIL).\n+        \/\/\n+        tests.put(\"test_xxx\",       () -> test_xxx(copy(a), 5, 11, 31));\n+        tests.put(\"test_yyy\",       () -> test_yyy(copy(a), 5, 11, 31));\n+        tests.put(\"test_zzz\",       () -> test_zzz(copy(a), 5, 11, 31));\n+\n+        \/\/ Compute gold value for all test methods before compilation\n+        for (Map.Entry<String,TestFunction> entry : tests.entrySet()) {\n+            String name = entry.getKey();\n+            TestFunction test = entry.getValue();\n+            Object[] gold = test.run();\n+            golds.put(name, gold);\n+        }\n+    }\n+\n+    MemorySegment newMemorySegment() {\n+        return provider.newMemorySegment();\n+    }\n+\n+    MemorySegment copy(MemorySegment src) {\n+        MemorySegment dst = newMemorySegment();\n+        MemorySegment.copy(src, 0, dst, 0, src.byteSize());\n+        return dst;\n+    }\n+\n+    static MemorySegment newMemorySegmentOfByteArray() {\n+        return MemorySegment.ofArray(new byte[BACKING_SIZE]);\n+    }\n+\n+    static MemorySegment newMemorySegmentOfCharArray() {\n+        return MemorySegment.ofArray(new char[BACKING_SIZE \/ 2]);\n+    }\n+\n+    static MemorySegment newMemorySegmentOfShortArray() {\n+        return MemorySegment.ofArray(new short[BACKING_SIZE \/ 2]);\n+    }\n+\n+    static MemorySegment newMemorySegmentOfIntArray() {\n+        return MemorySegment.ofArray(new int[BACKING_SIZE \/ 4]);\n+    }\n+\n+    static MemorySegment newMemorySegmentOfLongArray() {\n+        return MemorySegment.ofArray(new long[BACKING_SIZE \/ 8]);\n+    }\n+\n+    static MemorySegment newMemorySegmentOfFloatArray() {\n+        return MemorySegment.ofArray(new float[BACKING_SIZE \/ 4]);\n+    }\n+\n+    static MemorySegment newMemorySegmentOfDoubleArray() {\n+        return MemorySegment.ofArray(new double[BACKING_SIZE \/ 8]);\n+    }\n+\n+    static MemorySegment newMemorySegmentOfByteBuffer() {\n+        return MemorySegment.ofBuffer(ByteBuffer.allocate(BACKING_SIZE));\n+    }\n+\n+    static MemorySegment newMemorySegmentOfByteBufferDirect() {\n+        return MemorySegment.ofBuffer(ByteBuffer.allocateDirect(BACKING_SIZE));\n+    }\n+\n+    static MemorySegment newMemorySegmentOfNative() {\n+        \/\/ Auto arena: GC decides when there is no reference to the MemorySegment,\n+        \/\/ and then it deallocates the backing memory.\n+        return Arena.ofAuto().allocate(BACKING_SIZE, 1);\n+    }\n+\n+    static MemorySegment newMemorySegmentOfMixedArray() {\n+        switch(RANDOM.nextInt(7)) {\n+            case 0  -> { return newMemorySegmentOfByteArray(); }\n+            case 1  -> { return newMemorySegmentOfCharArray(); }\n+            case 2  -> { return newMemorySegmentOfShortArray(); }\n+            case 3  -> { return newMemorySegmentOfIntArray(); }\n+            case 4  -> { return newMemorySegmentOfLongArray(); }\n+            case 5  -> { return newMemorySegmentOfFloatArray(); }\n+            default -> { return newMemorySegmentOfDoubleArray(); }\n+        }\n+    }\n+\n+    static MemorySegment newMemorySegmentOfMixedBuffer() {\n+        switch (RANDOM.nextInt(2)) {\n+            case 0  -> { return newMemorySegmentOfByteBuffer(); }\n+            default -> { return newMemorySegmentOfByteBufferDirect(); }\n+        }\n+    }\n+\n+    static MemorySegment newMemorySegmentOfMixed() {\n+        switch (RANDOM.nextInt(3)) {\n+            case 0  -> { return newMemorySegmentOfMixedArray(); }\n+            case 1  -> { return newMemorySegmentOfMixedBuffer(); }\n+            default -> { return newMemorySegmentOfNative(); }\n+        }\n+    }\n+\n+    static void fillRandom(MemorySegment data) {\n+        for (int i = 0; i < (int)data.byteSize(); i += 8) {\n+            data.set(ValueLayout.JAVA_LONG_UNALIGNED, i, RANDOM.nextLong());\n+        }\n+    }\n+\n+\n+    static void verify(String name, Object[] gold, Object[] result) {\n+        if (gold.length != result.length) {\n+            throw new RuntimeException(\"verify \" + name + \": not the same number of outputs: gold.length = \" +\n+                                       gold.length + \", result.length = \" + result.length);\n+        }\n+        for (int i = 0; i < gold.length; i++) {\n+            Object g = gold[i];\n+            Object r = result[i];\n+            if (g == r) {\n+                throw new RuntimeException(\"verify \" + name + \": should be two separate objects (with identical content):\" +\n+                                           \" gold[\" + i + \"] == result[\" + i + \"]\");\n+            }\n+\n+            if (!(g instanceof MemorySegment && r instanceof MemorySegment)) {\n+                throw new RuntimeException(\"verify \" + name + \": only MemorySegment supported, i=\" + i);\n+            }\n+\n+            MemorySegment mg = (MemorySegment)g;\n+            MemorySegment mr = (MemorySegment)r;\n+\n+            if (mg.byteSize() != mr.byteSize()) {\n+                throw new RuntimeException(\"verify \" + name + \": MemorySegment must have same byteSize:\" +\n+                                       \" gold[\" + i + \"].byteSize = \" + mg.byteSize() +\n+                                       \" result[\" + i + \"].byteSize = \" + mr.byteSize());\n+            }\n+\n+            for (int j = 0; j < (int)mg.byteSize(); j++) {\n+                byte vg = mg.get(ValueLayout.JAVA_BYTE, j);\n+                byte vr = mr.get(ValueLayout.JAVA_BYTE, j);\n+                if (vg != vr) {\n+                    throw new RuntimeException(\"verify \" + name + \": MemorySegment must have same content:\" +\n+                                               \" gold[\" + i + \"][\" + j + \"] = \" + vg +\n+                                               \" result[\" + i + \"][\" + j + \"] = \" + vr);\n+                }\n+            }\n+        }\n+    }\n+\n+    @Run(test = { \"test_xxx\", \"test_yyy\", \"test_zzz\" })\n+    void runTests() {\n+        for (Map.Entry<String,TestFunction> entry : tests.entrySet()) {\n+            String name = entry.getKey();\n+            TestFunction test = entry.getValue();\n+            \/\/ Recall gold value from before compilation\n+            Object[] gold = golds.get(name);\n+            \/\/ Compute new result\n+            Object[] result = test.run();\n+            \/\/ Compare gold and new result\n+            verify(name, gold, result);\n+        }\n+    }\n+\n+    @Test\n+    @IR(counts = {REGEX_STORE_B_TO_MS_FROM_B, \"<=5\", \/\/ 4x RC\n+                  REGEX_STORE_C_TO_MS_FROM_B, \">=3\", \/\/ 4x merged\n+                  REGEX_STORE_I_TO_MS_FROM_B, \"0\",\n+                  REGEX_STORE_L_TO_MS_FROM_B, \"0\"},\n+        phase = CompilePhase.PRINT_IDEAL,\n+        applyIf = {\"UseUnalignedAccesses\", \"true\"})\n+    static Object[] test_xxx(MemorySegment a, int xI, int yI, int zI) {\n+         \/\/ All RangeChecks remain -> RC smearing not good enough?\n+         a.set(ValueLayout.JAVA_BYTE, (long)(xI + yI + zI + 0), (byte)'h');\n+         a.set(ValueLayout.JAVA_BYTE, (long)(xI + yI + zI + 1), (byte)'e');\n+         a.set(ValueLayout.JAVA_BYTE, (long)(xI + yI + zI + 2), (byte)'l');\n+         a.set(ValueLayout.JAVA_BYTE, (long)(xI + yI + zI + 3), (byte)'l');\n+         a.set(ValueLayout.JAVA_BYTE, (long)(xI + yI + zI + 4), (byte)'o');\n+         a.set(ValueLayout.JAVA_BYTE, (long)(xI + yI + zI + 5), (byte)' ');\n+         a.set(ValueLayout.JAVA_BYTE, (long)(xI + yI + zI + 6), (byte)':');\n+         a.set(ValueLayout.JAVA_BYTE, (long)(xI + yI + zI + 7), (byte)')');\n+         return new Object[]{ a };\n+    }\n+\n+    @Test\n+    @IR(counts = {REGEX_STORE_B_TO_MS_FROM_B, \"<=5\", \/\/ 4x RC\n+                  REGEX_STORE_C_TO_MS_FROM_B, \">=3\", \/\/ 4x merged\n+                  REGEX_STORE_I_TO_MS_FROM_B, \"0\",\n+                  REGEX_STORE_L_TO_MS_FROM_B, \"0\"},\n+        phase = CompilePhase.PRINT_IDEAL,\n+        applyIf = {\"UseUnalignedAccesses\", \"true\"})\n+    static Object[] test_yyy(MemorySegment a, int xI, int yI, int zI) {\n+         \/\/ All RangeChecks remain -> RC smearing not good enough?\n+         a.set(ValueLayout.JAVA_BYTE, (long)(xI) + (long)(yI) + (long)(zI) + 0L, (byte)'h');\n+         a.set(ValueLayout.JAVA_BYTE, (long)(xI) + (long)(yI) + (long)(zI) + 1L, (byte)'e');\n+         a.set(ValueLayout.JAVA_BYTE, (long)(xI) + (long)(yI) + (long)(zI) + 2L, (byte)'l');\n+         a.set(ValueLayout.JAVA_BYTE, (long)(xI) + (long)(yI) + (long)(zI) + 3L, (byte)'l');\n+         a.set(ValueLayout.JAVA_BYTE, (long)(xI) + (long)(yI) + (long)(zI) + 4L, (byte)'o');\n+         a.set(ValueLayout.JAVA_BYTE, (long)(xI) + (long)(yI) + (long)(zI) + 5L, (byte)' ');\n+         a.set(ValueLayout.JAVA_BYTE, (long)(xI) + (long)(yI) + (long)(zI) + 6L, (byte)':');\n+         a.set(ValueLayout.JAVA_BYTE, (long)(xI) + (long)(yI) + (long)(zI) + 7L, (byte)')');\n+         return new Object[]{ a };\n+    }\n+\n+    @Test\n+    @IR(counts = {REGEX_STORE_B_TO_MS_FROM_B, \"<=5\", \/\/ 4x RC\n+                  REGEX_STORE_C_TO_MS_FROM_B, \">=3\", \/\/ 4x merged\n+                  REGEX_STORE_I_TO_MS_FROM_B, \"0\",\n+                  REGEX_STORE_L_TO_MS_FROM_B, \"0\"},\n+        phase = CompilePhase.PRINT_IDEAL,\n+        applyIf = {\"UseUnalignedAccesses\", \"true\"})\n+    static Object[] test_zzz(MemorySegment a, long xL, long yL, long zL) {\n+         \/\/ All RangeChecks remain -> RC smearing not good enough?\n+         a.set(ValueLayout.JAVA_BYTE, xL + yL + zL + 0L, (byte)'h');\n+         a.set(ValueLayout.JAVA_BYTE, xL + yL + zL + 1L, (byte)'e');\n+         a.set(ValueLayout.JAVA_BYTE, xL + yL + zL + 2L, (byte)'l');\n+         a.set(ValueLayout.JAVA_BYTE, xL + yL + zL + 3L, (byte)'l');\n+         a.set(ValueLayout.JAVA_BYTE, xL + yL + zL + 4L, (byte)'o');\n+         a.set(ValueLayout.JAVA_BYTE, xL + yL + zL + 5L, (byte)' ');\n+         a.set(ValueLayout.JAVA_BYTE, xL + yL + zL + 6L, (byte)':');\n+         a.set(ValueLayout.JAVA_BYTE, xL + yL + zL + 7L, (byte)')');\n+         return new Object[]{ a };\n+    }\n+}\n","filename":"test\/hotspot\/jtreg\/compiler\/c2\/TestMergeStoresMemorySegment.java","additions":426,"deletions":0,"binary":false,"changes":426,"status":"added"},{"patch":"@@ -55,0 +55,4 @@\n+    static int max_int = Integer.MAX_VALUE;\n+    static int min_int = Integer.MIN_VALUE;\n+    static int val_2_to_30 = (1 << 30);\n+    static int large_by_53 = (int)((1L << 31) \/ 53L + 1L);\n@@ -98,0 +102,85 @@\n+        val = 0;\n+        System.out.println(\"test3\");\n+        for (int i = 0; i < 100_000; i++) {\n+            testClear(big);\n+            test3(big, ANCHOR);\n+            long sum = testSum(big);\n+            if (i == 0) {\n+                val = sum;\n+            } else {\n+                if (sum != val) {\n+                    System.out.println(\"ERROR: test3 had wrong value: \" + val + \" != \" + sum);\n+                    errors++;\n+                    break;\n+                }\n+            }\n+        }\n+\n+        val = 0;\n+        System.out.println(\"test4\");\n+        for (int i = 0; i < 100_000; i++) {\n+            testClear(big);\n+            test4(big, ANCHOR);\n+            long sum = testSum(big);\n+            if (i == 0) {\n+                val = sum;\n+            } else {\n+                if (sum != val) {\n+                    System.out.println(\"ERROR: test4 had wrong value: \" + val + \" != \" + sum);\n+                    errors++;\n+                    break;\n+                }\n+            }\n+        }\n+\n+        val = 0;\n+        System.out.println(\"test5\");\n+        for (int i = 0; i < 100_000; i++) {\n+            testClear(big);\n+            test5(big, ANCHOR);\n+            long sum = testSum(big);\n+            if (i == 0) {\n+                val = sum;\n+            } else {\n+                if (sum != val) {\n+                    System.out.println(\"ERROR: test5 had wrong value: \" + val + \" != \" + sum);\n+                    errors++;\n+                    break;\n+                }\n+            }\n+        }\n+\n+        val = 0;\n+        System.out.println(\"test6\");\n+        for (int i = 0; i < 100_000; i++) {\n+            testClear(big);\n+            test6(big, ANCHOR);\n+            long sum = testSum(big);\n+            if (i == 0) {\n+                val = sum;\n+            } else {\n+                if (sum != val) {\n+                    System.out.println(\"ERROR: test6 had wrong value: \" + val + \" != \" + sum);\n+                    errors++;\n+                    break;\n+                }\n+            }\n+        }\n+\n+        val = 0;\n+        System.out.println(\"test7\");\n+        for (int i = 0; i < 100_000; i++) {\n+            testClear(big);\n+            test7(big, ANCHOR);\n+            long sum = testSum(big);\n+            if (i == 0) {\n+                val = sum;\n+            } else {\n+                if (sum != val) {\n+                    System.out.println(\"ERROR: test7 had wrong value: \" + val + \" != \" + sum);\n+                    errors++;\n+                    break;\n+                }\n+            }\n+        }\n+\n@@ -132,0 +221,39 @@\n+\n+    \/\/ Test: if MergeStores is applied this can lead to wrong results\n+    \/\/  -> AddI needs overflow check.\n+    static void test3(int[] a, long anchor) {\n+        long base = UNSAFE.ARRAY_INT_BASE_OFFSET + anchor;\n+        UNSAFE.putInt(a, base + (long)(max_int + 0), 0x42424242);\n+        UNSAFE.putInt(a, base + (long)(max_int + 4), 0x66666666);\n+    }\n+\n+    \/\/ Test: \"max_int - four\" cannot be parsed further, but would not make a difference here.\n+    static void test4(int[] a, long anchor) {\n+        long base = UNSAFE.ARRAY_INT_BASE_OFFSET + anchor;\n+        UNSAFE.putInt(a, base + (long)(min_int - four) + 0, 0x42424242);\n+        UNSAFE.putInt(a, base + (long)(min_int - four) + 4, 0x66666666);\n+    }\n+\n+    \/\/ Test: if MergeStores is applied this can lead to wrong results\n+    \/\/  -> SubI needs overflow check.\n+    static void test5(int[] a, long anchor) {\n+        long base = UNSAFE.ARRAY_INT_BASE_OFFSET + anchor;\n+        UNSAFE.putInt(a, base + (long)(min_int) - (long)(four) + 0, 0x42424242); \/\/ no overflow\n+        UNSAFE.putInt(a, base + (long)(min_int - four)         + 4, 0x66666666); \/\/ overflow\n+    }\n+\n+    \/\/ Test: if MergeStores is applied this can lead to wrong results\n+    \/\/  -> LShiftI needs overflow check.\n+    static void test6(int[] a, long anchor) {\n+        long base = UNSAFE.ARRAY_INT_BASE_OFFSET + anchor;\n+        UNSAFE.putInt(a, base +  (long)(2 * val_2_to_30) + 0, 0x42424242); \/\/ overflow\n+        UNSAFE.putInt(a, base + 2L * (long)(val_2_to_30) + 4, 0x66666666); \/\/ no overflow\n+    }\n+\n+    \/\/ Test: if MergeStores is applied this can lead to wrong results\n+    \/\/  -> MulI needs overflow check.\n+    static void test7(int[] a, long anchor) {\n+        long base = UNSAFE.ARRAY_INT_BASE_OFFSET + anchor;\n+        UNSAFE.putInt(a, base +  (long)(53 * large_by_53) + 0, 0x42424242); \/\/ overflow\n+        UNSAFE.putInt(a, base + 53L * (long)(large_by_53) + 4, 0x66666666); \/\/ no overflow\n+    }\n","filename":"test\/hotspot\/jtreg\/compiler\/c2\/TestMergeStoresUnsafeArrayPointer.java","additions":128,"deletions":0,"binary":false,"changes":128,"status":"modified"},{"patch":"@@ -44,3 +44,3 @@\n-@Warmup(iterations = 3, time = 3)\n-@Measurement(iterations = 3, time = 3)\n-@Fork(value = 3, jvmArgs = {\n+@Warmup(iterations = 2, time = 1)\n+@Measurement(iterations = 3, time = 1)\n+@Fork(value = 1, jvmArgs = {\n@@ -49,1 +49,1 @@\n-@State(Scope.Benchmark)\n+@State(Scope.Thread)\n@@ -69,0 +69,1 @@\n+    public static long native_adr = UNSAFE.allocateMemory(RANGE * 8);\n@@ -694,0 +695,55 @@\n+\n+    @Benchmark\n+    public void store_unsafe_B8_L_offs_noalloc_direct() {\n+        UNSAFE.putByte(aB, Unsafe.ARRAY_BYTE_BASE_OFFSET + offset + 0, (byte)(vL >> 0 ));\n+        UNSAFE.putByte(aB, Unsafe.ARRAY_BYTE_BASE_OFFSET + offset + 1, (byte)(vL >> 8 ));\n+        UNSAFE.putByte(aB, Unsafe.ARRAY_BYTE_BASE_OFFSET + offset + 2, (byte)(vL >> 16));\n+        UNSAFE.putByte(aB, Unsafe.ARRAY_BYTE_BASE_OFFSET + offset + 3, (byte)(vL >> 24));\n+        UNSAFE.putByte(aB, Unsafe.ARRAY_BYTE_BASE_OFFSET + offset + 4, (byte)(vL >> 32));\n+        UNSAFE.putByte(aB, Unsafe.ARRAY_BYTE_BASE_OFFSET + offset + 5, (byte)(vL >> 40));\n+        UNSAFE.putByte(aB, Unsafe.ARRAY_BYTE_BASE_OFFSET + offset + 6, (byte)(vL >> 48));\n+        UNSAFE.putByte(aB, Unsafe.ARRAY_BYTE_BASE_OFFSET + offset + 7, (byte)(vL >> 56));\n+    }\n+\n+    @Benchmark\n+    public void store_unsafe_B8_L_offs_noalloc_unsafe() {\n+        UNSAFE.putLongUnaligned(aB, Unsafe.ARRAY_BYTE_BASE_OFFSET + offset + 0, vL);\n+    }\n+\n+    @Benchmark\n+    public void store_unsafe_C4_L_offs_noalloc_direct() {\n+        UNSAFE.putChar(aB, Unsafe.ARRAY_BYTE_BASE_OFFSET + offset + 0, (char)(vL >> 0 ));\n+        UNSAFE.putChar(aB, Unsafe.ARRAY_BYTE_BASE_OFFSET + offset + 2, (char)(vL >> 16));\n+        UNSAFE.putChar(aB, Unsafe.ARRAY_BYTE_BASE_OFFSET + offset + 4, (char)(vL >> 32));\n+        UNSAFE.putChar(aB, Unsafe.ARRAY_BYTE_BASE_OFFSET + offset + 6, (char)(vL >> 48));\n+    }\n+\n+    @Benchmark\n+    public void store_unsafe_native_B8_L_offs_noalloc_direct() {\n+        UNSAFE.putByte(null, native_adr + offset + 0, (byte)(vL >> 0 ));\n+        UNSAFE.putByte(null, native_adr + offset + 1, (byte)(vL >> 8 ));\n+        UNSAFE.putByte(null, native_adr + offset + 2, (byte)(vL >> 16));\n+        UNSAFE.putByte(null, native_adr + offset + 3, (byte)(vL >> 24));\n+        UNSAFE.putByte(null, native_adr + offset + 4, (byte)(vL >> 32));\n+        UNSAFE.putByte(null, native_adr + offset + 5, (byte)(vL >> 40));\n+        UNSAFE.putByte(null, native_adr + offset + 6, (byte)(vL >> 48));\n+        UNSAFE.putByte(null, native_adr + offset + 7, (byte)(vL >> 56));\n+    }\n+\n+    @Benchmark\n+    public void store_unsafe_native_C4_L_offs_noalloc_direct() {\n+        UNSAFE.putChar(null, native_adr + offset + 0, (char)(vL >> 0 ));\n+        UNSAFE.putChar(null, native_adr + offset + 2, (char)(vL >> 16));\n+        UNSAFE.putChar(null, native_adr + offset + 4, (char)(vL >> 32));\n+        UNSAFE.putChar(null, native_adr + offset + 6, (char)(vL >> 48));\n+    }\n+\n+    @Benchmark\n+    public void store_unsafe_native_B8_L_offs_noalloc_unsafe() {\n+        UNSAFE.putLongUnaligned(null, native_adr + offset + 0, vL);\n+    }\n+\n+    @Fork(value = 1, jvmArgsPrepend = {\n+        \"-XX:+UnlockDiagnosticVMOptions\", \"-XX:-MergeStores\"\n+    })\n+    public static class MergeStoresDisabled extends MergeStores {}\n","filename":"test\/micro\/org\/openjdk\/bench\/vm\/compiler\/MergeStores.java","additions":60,"deletions":4,"binary":false,"changes":64,"status":"modified"}]}