{"files":[{"patch":"@@ -35,0 +35,1 @@\n+#include \"gc\/z\/zNMT.hpp\"\n@@ -49,0 +50,1 @@\n+  ZNMT::initialize();\n","filename":"src\/hotspot\/share\/gc\/z\/zInitialize.cpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -31,0 +31,1 @@\n+#include \"nmt\/nmtMemoryFileTracker.hpp\"\n@@ -33,2 +34,1 @@\n-ZNMT::Reservation ZNMT::_reservations[ZMaxVirtualReservations] = {};\n-size_t ZNMT::_num_reservations = 0;\n+MemoryFileTracker::MemoryFile* ZNMT::_device = nullptr;\n@@ -36,54 +36,2 @@\n-size_t ZNMT::reservation_index(zoffset offset, size_t* offset_in_reservation) {\n-  assert(_num_reservations > 0, \"at least one reservation must exist\");\n-\n-  size_t index = 0;\n-  *offset_in_reservation = untype(offset);\n-  for (; index < _num_reservations; ++index) {\n-    const size_t reservation_size = _reservations[index]._size;\n-    if (*offset_in_reservation < reservation_size) {\n-      break;\n-    }\n-    *offset_in_reservation -= reservation_size;\n-  }\n-\n-  assert(index != _num_reservations, \"failed to find reservation index\");\n-  return index;\n-}\n-\n-void ZNMT::process_fake_mapping(zoffset offset, size_t size, bool commit) {\n-  \/\/ In order to satisfy NTM's requirement of an 1:1 mapping between committed\n-  \/\/ and reserved addresses, a fake mapping from the offset into the reservation\n-  \/\/ is used.\n-  \/\/\n-  \/\/ These mappings from\n-  \/\/   [offset, offset + size) -> {[virtual address range], ...}\n-  \/\/ are stable after the heap has been reserved. No commits proceed any\n-  \/\/ reservations. Committing and uncommitting the same [offset, offset + size)\n-  \/\/ range will result in same virtual memory ranges.\n-\n-  size_t left_to_process = size;\n-  size_t offset_in_reservation;\n-  for (size_t i = reservation_index(offset, &offset_in_reservation); i < _num_reservations; ++i) {\n-    const zaddress_unsafe reservation_start = _reservations[i]._start;\n-    const size_t reservation_size = _reservations[i]._size;\n-    const size_t sub_range_size = MIN2(left_to_process, reservation_size - offset_in_reservation);\n-    const uintptr_t sub_range_addr = untype(reservation_start) + offset_in_reservation;\n-\n-    \/\/ commit \/ uncommit memory\n-    if (commit) {\n-      MemTracker::record_virtual_memory_commit((void*)sub_range_addr, sub_range_size, CALLER_PC);\n-    } else {\n-      ThreadCritical tc;\n-      MemTracker::record_virtual_memory_uncommit((address)sub_range_addr, sub_range_size);\n-    }\n-\n-    left_to_process -= sub_range_size;\n-    if (left_to_process == 0) {\n-      \/\/ Processed all nmt registrations\n-      return;\n-    }\n-\n-    offset_in_reservation = 0;\n-  }\n-\n-  assert(left_to_process == 0, \"everything was not commited\");\n+void ZNMT::initialize() {\n+  _device = MemTracker::register_device(\"ZGC heap backing file\");\n@@ -93,7 +41,1 @@\n-  assert(_num_reservations < ZMaxVirtualReservations, \"too many reservations\");\n-  \/\/ Keep track of the reservations made in order to create fake mappings\n-  \/\/ between the reserved and commited memory.\n-  \/\/ See details in ZNMT::process_fake_mapping\n-  _reservations[_num_reservations++] = {start, size};\n-\n-  MemTracker::record_virtual_memory_reserve((void*)untype(start), size, CALLER_PC, mtJavaHeap);\n+  MemTracker::record_virtual_memory_reserve((address)untype(start), size, CALLER_PC, mtJavaHeap);\n@@ -103,9 +45,1 @@\n-  \/\/ NMT expects a 1-to-1 mapping between virtual and physical memory.\n-  \/\/ ZGC can temporarily have multiple virtual addresses pointing to\n-  \/\/ the same physical memory.\n-  \/\/\n-  \/\/ When this function is called we don't know where in the virtual memory\n-  \/\/ this physical memory will be mapped. So we fake the virtual memory\n-  \/\/ address by mapping the physical offset into offsets in the reserved\n-  \/\/ memory space.\n-  process_fake_mapping(offset, size, true);\n+  MemTracker::allocate_memory_in(ZNMT::_device, untype(offset), size, mtJavaHeap, CALLER_PC);\n@@ -115,4 +49,9 @@\n-  \/\/ We fake the virtual memory address by mapping the physical offset\n-  \/\/ into offsets in the reserved memory space.\n-  \/\/ See comment in ZNMT::commit\n-  process_fake_mapping(offset, size, false);\n+  MemTracker::free_memory_in(ZNMT::_device, untype(offset), size);\n+}\n+\n+void ZNMT::map(zaddress_unsafe addr, size_t size, zoffset offset) {\n+  \/\/ NMT doesn't track mappings at the moment.\n+}\n+\n+void ZNMT::unmap(zaddress_unsafe addr, size_t size) {\n+  \/\/ NMT doesn't track mappings at the moment.\n","filename":"src\/hotspot\/share\/gc\/z\/zNMT.cpp","additions":15,"deletions":76,"binary":false,"changes":91,"status":"modified"},{"patch":"@@ -32,0 +32,2 @@\n+#include \"nmt\/memTracker.hpp\"\n+#include \"nmt\/nmtMemoryFileTracker.hpp\"\n@@ -37,9 +39,1 @@\n-  struct Reservation {\n-    zaddress_unsafe _start;\n-    size_t          _size;\n-  };\n-  static Reservation _reservations[ZMaxVirtualReservations];\n-  static size_t      _num_reservations;\n-\n-  static size_t reservation_index(zoffset offset, size_t* offset_in_reservation);\n-  static void process_fake_mapping(zoffset offset, size_t size, bool commit);\n+  static MemoryFileTracker::MemoryFile* _device;\n@@ -48,0 +42,1 @@\n+  static void initialize();\n@@ -51,0 +46,2 @@\n+  static void map(zaddress_unsafe addr, size_t size, zoffset offset);\n+  static void unmap(zaddress_unsafe addr, size_t size);\n","filename":"src\/hotspot\/share\/gc\/z\/zNMT.hpp","additions":6,"deletions":9,"binary":false,"changes":15,"status":"modified"},{"patch":"@@ -140,1 +140,0 @@\n-\n@@ -144,0 +143,5 @@\n+  {\n+    MemoryFileTracker::Instance::Locker lock;\n+    MemoryFileTracker::Instance::summary_snapshot(&_virtual_memory_snapshot);\n+  }\n+\n@@ -197,1 +201,0 @@\n-\n","filename":"src\/hotspot\/share\/nmt\/memBaseline.cpp","additions":5,"deletions":2,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -901,1 +901,14 @@\n- }\n+}\n+\n+void MemDetailReporter::report_physical_devices() {\n+  MemoryFileTracker::Instance::Locker lock;\n+  const GrowableArrayCHeap<MemoryFileTracker::MemoryFile*, mtNMT>& devices =\n+      MemoryFileTracker::Instance::devices();\n+  this->output()->cr();\n+  this->output()->print_cr(\"Memory file details\");\n+  this->output()->cr();\n+  for (int i = 0; i < devices.length(); i++) {\n+    MemoryFileTracker::MemoryFile* dev = devices.at(i);\n+    MemoryFileTracker::Instance::print_report_on(dev, this->output(), scale());\n+  }\n+}\n","filename":"src\/hotspot\/share\/nmt\/memReporter.cpp","additions":14,"deletions":1,"binary":false,"changes":15,"status":"modified"},{"patch":"@@ -33,0 +33,1 @@\n+#include \"nmt\/nmtMemoryFileTracker.hpp\"\n@@ -34,0 +35,1 @@\n+#include \"runtime\/mutexLocker.hpp\"\n@@ -168,0 +170,1 @@\n+    report_physical_devices();\n@@ -176,0 +179,2 @@\n+  \/\/ Report all physical devices\n+  void report_physical_devices();\n","filename":"src\/hotspot\/share\/nmt\/memReporter.hpp","additions":5,"deletions":0,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -71,0 +71,1 @@\n+        !MemoryFileTracker::Instance::initialize(level) ||\n","filename":"src\/hotspot\/share\/nmt\/memTracker.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -31,0 +31,1 @@\n+#include \"nmt\/nmtMemoryFileTracker.hpp\"\n@@ -169,0 +170,29 @@\n+  static inline MemoryFileTracker::MemoryFile* register_device(const char* descriptive_name) {\n+    assert_post_init();\n+    if (!enabled()) return nullptr;\n+    MemoryFileTracker::Instance::Locker lock;\n+    return MemoryFileTracker::Instance::make_device(descriptive_name);\n+  }\n+\n+  static inline void remove_device(MemoryFileTracker::MemoryFile* device) {\n+    assert_post_init();\n+    if (!enabled()) return;\n+    MemoryFileTracker::Instance::Locker lock;\n+    MemoryFileTracker::Instance::free_device(device);\n+  }\n+\n+  static inline void allocate_memory_in(MemoryFileTracker::MemoryFile* device, size_t offset, size_t size,\n+                                        MEMFLAGS flag, const NativeCallStack& stack) {\n+    assert_post_init();\n+    if (!enabled()) return;\n+    MemoryFileTracker::Instance::Locker lock;\n+    MemoryFileTracker::Instance::allocate_memory(device, offset, size, flag, stack);\n+  }\n+  static inline void free_memory_in(MemoryFileTracker::MemoryFile* device,\n+                                        size_t offset, size_t size) {\n+    assert_post_init();\n+    if (!enabled()) return;\n+    MemoryFileTracker::Instance::Locker lock;\n+    MemoryFileTracker::Instance::free_memory(device, offset, size);\n+  }\n+\n","filename":"src\/hotspot\/share\/nmt\/memTracker.hpp","additions":30,"deletions":0,"binary":false,"changes":30,"status":"modified"},{"patch":"@@ -0,0 +1,143 @@\n+#include \"precompiled.hpp\"\n+\n+#include \"memory\/allocation.hpp\"\n+#include \"nmt\/nmtCommon.hpp\"\n+#include \"nmt\/nmtMemoryFileTracker.hpp\"\n+#include \"nmt\/nmtNativeCallStackStorage.hpp\"\n+#include \"nmt\/vmatree.hpp\"\n+#include \"runtime\/mutex.hpp\"\n+#include \"utilities\/growableArray.hpp\"\n+#include \"utilities\/nativeCallStack.hpp\"\n+#include \"utilities\/ostream.hpp\"\n+\n+MemoryFileTracker* MemoryFileTracker::Instance::_tracker = nullptr;\n+PlatformMutex* MemoryFileTracker::Instance::_mutex = nullptr;\n+\n+MemoryFileTracker::MemoryFileTracker(bool is_detailed_mode)\n+: _stack_storage(is_detailed_mode), _devices() {\n+}\n+\n+void MemoryFileTracker::allocate_memory(MemoryFile* device, size_t offset,\n+                                            size_t size, MEMFLAGS flag,\n+                                            const NativeCallStack& stack) {\n+  NativeCallStackStorage::StackIndex sidx = _stack_storage.push(stack);\n+  DeviceSpace::Metadata metadata(sidx, flag);\n+  DeviceSpace::SummaryDiff diff = device->_tree.reserve_mapping(offset, size, metadata);\n+  for (int i = 0; i < mt_number_of_types; i++) {\n+    const VMATree::SingleDiff& rescom = diff.flag[i];\n+    VirtualMemory* summary = device->_summary.by_type(NMTUtil::index_to_flag(i));\n+    summary->reserve_memory(rescom.reserve);\n+  }\n+}\n+\n+void MemoryFileTracker::free_memory(MemoryFile* device, size_t offset, size_t size) {\n+  DeviceSpace::SummaryDiff diff = device->_tree.release_mapping(offset, size);\n+  for (int i = 0; i < mt_number_of_types; i++) {\n+    const VMATree::SingleDiff& rescom = diff.flag[i];\n+    VirtualMemory* summary = device->_summary.by_type(NMTUtil::index_to_flag(i));\n+    summary->reserve_memory(rescom.reserve);\n+  }\n+}\n+\n+void MemoryFileTracker::print_report_on(const MemoryFile* device, outputStream* stream, size_t scale) {\n+  stream->print_cr(\"Memory map of %s\", device->_descriptive_name);\n+  stream->cr();\n+  VMATree::VTreap* prev = nullptr;\n+  device->_tree.in_order_traversal([&](VMATree::VTreap* current) {\n+    if (prev == nullptr) {\n+      \/\/ Must be first node.\n+      prev = current;\n+      return;\n+    }\n+    const VMATree::NodeState& pval = prev->val();\n+    const VMATree::NodeState& cval = current->val();\n+    assert(pval.out.type == cval.in.type, \"must be\");\n+    if (pval.out.type == VMATree::StateType::Reserved) {\n+      const auto& start_addr = prev->key();\n+      const auto& end_addr = current->key();\n+      stream->print_cr(\"[\" PTR_FORMAT \" - \" PTR_FORMAT \"] allocated \" SIZE_FORMAT \"%s\" \" for %s\", start_addr, end_addr,\n+                       NMTUtil::amount_in_scale(end_addr - start_addr, scale),\n+                       NMTUtil::scale_name(scale),\n+                       NMTUtil::flag_to_name(pval.out.data.flag));\n+      pval.out.data.stack_idx.stack().print_on(stream, 4);\n+      stream->cr();\n+    }\n+    prev = current;\n+  });\n+}\n+\n+MemoryFileTracker::MemoryFile* MemoryFileTracker::make_device(const char* descriptive_name) {\n+  MemoryFile* device_place = new MemoryFile{descriptive_name};\n+  _devices.push(device_place);\n+  return device_place;\n+}\n+\n+void MemoryFileTracker::free_device(MemoryFile* device) {\n+  _devices.remove(device);\n+  delete device;\n+}\n+\n+const GrowableArrayCHeap<MemoryFileTracker::MemoryFile*, mtNMT>& MemoryFileTracker::devices() {\n+  return _devices;\n+}\n+const VirtualMemorySnapshot& MemoryFileTracker::summary_for(const MemoryFile* device) {\n+  return device->_summary;\n+}\n+\n+\n+bool MemoryFileTracker::Instance::initialize(NMT_TrackingLevel tracking_level) {\n+  if (tracking_level == NMT_TrackingLevel::NMT_off) return true;\n+  _tracker = static_cast<MemoryFileTracker*>(os::malloc(sizeof(MemoryFileTracker), mtNMT));\n+  if (_tracker == nullptr) return false;\n+  new (_tracker) MemoryFileTracker(tracking_level == NMT_TrackingLevel::NMT_detail);\n+  _mutex = new PlatformMutex();\n+  return true;\n+}\n+void MemoryFileTracker::Instance::allocate_memory(MemoryFile* device, size_t offset,\n+                                                      size_t size, MEMFLAGS flag,\n+                                                      const NativeCallStack& stack) {\n+  _tracker->allocate_memory(device, offset, size, flag, stack);\n+}\n+\n+void MemoryFileTracker::Instance::free_memory(MemoryFile* device, size_t offset,\n+                                                  size_t size) {\n+  _tracker->free_memory(device, offset, size);\n+}\n+\n+MemoryFileTracker::MemoryFile*\n+MemoryFileTracker::Instance::make_device(const char* descriptive_name) {\n+  return _tracker->make_device(descriptive_name);\n+}\n+\n+void MemoryFileTracker::Instance::print_report_on(const MemoryFile* device,\n+                                                      outputStream* stream, size_t scale) {\n+  _tracker->print_report_on(device, stream, scale);\n+}\n+\n+const GrowableArrayCHeap<MemoryFileTracker::MemoryFile*, mtNMT>& MemoryFileTracker::Instance::devices() {\n+  return _tracker->devices();\n+};\n+\n+void MemoryFileTracker::summary_snapshot(VirtualMemorySnapshot* snapshot) const {\n+  for (int d = 0; d < _devices.length(); d++) {\n+    auto& device = _devices.at(d);\n+    for (int i = 0; i < mt_number_of_types; i++) {\n+      auto snap = snapshot->by_type(NMTUtil::index_to_flag(i));\n+      auto current = device->_summary.by_type(NMTUtil::index_to_flag(i));\n+      \/\/ PDT stores the memory as reserved but it's accounted as committed.\n+      snap->commit_memory(current->reserved());\n+    }\n+  }\n+}\n+\n+void MemoryFileTracker::Instance::summary_snapshot(VirtualMemorySnapshot* snapshot) {\n+  _tracker->summary_snapshot(snapshot);\n+}\n+\n+MemoryFileTracker::Instance::Locker::Locker() {\n+  MemoryFileTracker::Instance::_mutex->lock();\n+}\n+\n+MemoryFileTracker::Instance::Locker::~Locker() {\n+  MemoryFileTracker::Instance::_mutex->unlock();\n+}\n","filename":"src\/hotspot\/share\/nmt\/nmtMemoryFileTracker.cpp","additions":143,"deletions":0,"binary":false,"changes":143,"status":"added"},{"patch":"@@ -0,0 +1,116 @@\n+\/*\n+ * Copyright (c) 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef SHARE_NMT_NMTMEMORYFILETRACKER_HPP\n+#define SHARE_NMT_NMTMEMORYFILETRACKER_HPP\n+\n+#include \"memory\/allocation.hpp\"\n+#include \"nmt\/nmtCommon.hpp\"\n+#include \"nmt\/nmtNativeCallStackStorage.hpp\"\n+#include \"nmt\/virtualMemoryTracker.hpp\"\n+#include \"nmt\/vmatree.hpp\"\n+#include \"runtime\/mutex.hpp\"\n+#include \"runtime\/os.inline.hpp\"\n+#include \"utilities\/growableArray.hpp\"\n+#include \"utilities\/nativeCallStack.hpp\"\n+#include \"utilities\/ostream.hpp\"\n+\n+\/\/ The PhysicalDeviceTracker tracks memory of 'physical devices',\n+\/\/ storage with its own memory space separate from the process.\n+\/\/ A typical example of such a device is a memory mapped file.\n+class MemoryFileTracker {\n+  friend class MemoryFileTrackerTest;\n+  \/\/ Provide caching of stacks.\n+  NativeCallStackStorage _stack_storage;\n+\n+  \/\/ Each device has its own memory space.\n+  using DeviceSpace = VMATree;\n+public:\n+  class MemoryFile : public CHeapObj<mtNMT> {\n+    friend MemoryFileTracker;\n+    friend class MemoryFileTrackerTest;\n+    const char* _descriptive_name;\n+    VirtualMemorySnapshot _summary;\n+    DeviceSpace _tree;\n+  public:\n+    NONCOPYABLE(MemoryFile);\n+    MemoryFile(const char* descriptive_name)\n+      : _descriptive_name(descriptive_name) {\n+    }\n+  };\n+\n+private:\n+  \/\/ We need pointers to each allocated device.\n+  GrowableArrayCHeap<MemoryFile*, mtNMT> _devices;\n+\n+public:\n+  MemoryFileTracker(bool is_detailed_mode);\n+\n+  void allocate_memory(MemoryFile* device, size_t offset, size_t size, MEMFLAGS flag,\n+                       const NativeCallStack& stack);\n+  void free_memory(MemoryFile* device, size_t offset, size_t size);\n+\n+  MemoryFile* make_device(const char* descriptive_name);\n+  void free_device(MemoryFile* device);\n+\n+  const VirtualMemorySnapshot& summary_for(const MemoryFile* device);\n+\n+  void summary_snapshot(VirtualMemorySnapshot* snapshot) const;\n+\n+  \/\/ Print detailed report of device\n+  void print_report_on(const MemoryFile* device, outputStream* stream, size_t scale);\n+\n+  const GrowableArrayCHeap<MemoryFile*, mtNMT>& devices();\n+\n+  class Instance : public AllStatic {\n+    static MemoryFileTracker* _tracker;\n+    static PlatformMutex* _mutex;\n+\n+  public:\n+    class Locker : public StackObj {\n+    public:\n+      Locker();\n+      ~Locker();\n+    };\n+\n+    static bool initialize(NMT_TrackingLevel tracking_level);\n+\n+    static MemoryFile* make_device(const char* descriptive_name);\n+    static void free_device(MemoryFile* device);\n+\n+    static void allocate_memory(MemoryFile* device, size_t offset, size_t size,\n+                                MEMFLAGS flag, const NativeCallStack& stack);\n+    static void free_memory(MemoryFile* device, size_t offset, size_t size);\n+\n+    static const VirtualMemorySnapshot& summary_for(const MemoryFile* device);\n+\n+    static void summary_snapshot(VirtualMemorySnapshot* snapshot);\n+\n+    static void print_report_on(const MemoryFile* device, outputStream* stream, size_t scale);\n+\n+    static const GrowableArrayCHeap<MemoryFile*, mtNMT>& devices();\n+  };\n+};\n+\n+#endif \/\/ SHARE_NMT_NMTMEMORYFILETRACKER_HPP\n","filename":"src\/hotspot\/share\/nmt\/nmtMemoryFileTracker.hpp","additions":116,"deletions":0,"binary":false,"changes":116,"status":"added"},{"patch":"@@ -0,0 +1,106 @@\n+\/*\n+ * Copyright (c) 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).    EXPECT_EQ(dev->_summary.by_type(mtTest)->reserved(), 0);\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef SHARE_NMT_NMTNATIVECALLSTACKSTORAGE_HPP\n+#define SHARE_NMT_NMTNATIVECALLSTACKSTORAGE_HPP\n+\n+#include \"utilities\/growableArray.hpp\"\n+#include \"utilities\/nativeCallStack.hpp\"\n+\n+\/\/ Virtual memory regions that are tracked by NMT also have their NativeCallStack (NCS) tracked.\n+\/\/ NCS:s are:\n+\/\/ - Fairly large\n+\/\/ - Regularly compared for equality\n+\/\/ - Read a lot when a detailed report is printed\n+\/\/ Therefore we'd like:\n+\/\/ - To not store duplicates\n+\/\/ - Have fast comparisons\n+\/\/ - Have constant time access\n+\/\/ We achieve this by storing them in a bog-standard closed addressing hashtable and never removing any elements.\n+class NativeCallStackStorage : public CHeapObj<mtNMT> {\n+private:\n+  struct Link : public CHeapObj<mtNMT> {\n+    Link* next;\n+    NativeCallStack stack;\n+    Link(Link* next, NativeCallStack v)\n+      : next(next),\n+        stack(v) {\n+    }\n+  };\n+  NativeCallStack* put(const NativeCallStack& value) {\n+    int bucket = value.calculate_hash() % nr_buckets;\n+    Link* link = buckets.at(bucket);\n+    while (link != nullptr) {\n+      if (value.equals(link->stack)) {\n+        return &link->stack;\n+      }\n+      link = link->next;\n+    }\n+    Link* new_link = new Link(buckets.at(bucket), value);\n+    buckets.at_put(bucket, new_link);\n+    return &new_link->stack;\n+  }\n+\n+  \/\/ 4096 buckets ensures that probability of collision is 50% at approximately 64\n+  \/\/ different call stacks.\n+  static const constexpr int nr_buckets = 4096;\n+  GrowableArrayCHeap<Link*, mtNMT> buckets;\n+  bool is_detailed_mode;\n+public:\n+  struct StackIndex {\n+    friend NativeCallStackStorage;\n+  private:\n+    NativeCallStack* _stack;\n+    StackIndex(NativeCallStack* stack) : _stack(stack) {}\n+  public:\n+    static bool equals(const StackIndex& a, const StackIndex& b) {\n+      return a._stack == b._stack;\n+    }\n+    StackIndex() : _stack(nullptr) {}\n+    const NativeCallStack& stack() const {\n+      return *_stack;\n+    }\n+  };\n+\n+  StackIndex push(const NativeCallStack& stack) {\n+    \/\/ Not in detailed mode, so not tracking stacks.\n+    if (!is_detailed_mode) {\n+      return StackIndex(nullptr);\n+    }\n+    return put(stack);\n+  }\n+\n+  const inline NativeCallStack& get(StackIndex si) {\n+    return *si._stack;\n+  }\n+\n+  NativeCallStackStorage(bool is_detailed_mode)\n+  :  buckets(), is_detailed_mode(is_detailed_mode) {\n+    if (is_detailed_mode) {\n+      buckets.at_grow(nr_buckets, nullptr);\n+    }\n+  }\n+};\n+\n+#endif \/\/ SHARE_NMT_NMTNATIVECALLSTACKSTORAGE_HPP\n","filename":"src\/hotspot\/share\/nmt\/nmtNativeCallStackStorage.hpp","additions":106,"deletions":0,"binary":false,"changes":106,"status":"added"},{"patch":"@@ -0,0 +1,282 @@\n+\/*\n+ * Copyright (c) 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef SHARE_NMT_TREAP_HPP\n+#define SHARE_NMT_TREAP_HPP\n+\n+#include \"utilities\/globalDefinitions.hpp\"\n+#include \"memory\/allocation.hpp\"\n+#include \"runtime\/os.hpp\"\n+#include \"utilities\/growableArray.hpp\"\n+#include <stddef.h>\n+#include <stdint.h>\n+\n+\/\/ A Treap is a self-balanced binary tree where each node is equipped with a\n+\/\/ priority. It adds the invariant that the priority of a parent P is strictly larger\n+\/\/ larger than the priority of its children. When priorities are randomly\n+\/\/ assigned the tree is balanced.\n+\/\/ All operations are defined through merge and split, which are each other's inverse.\n+\/\/ merge(left_treap, right_treap) => treap where left_treap <= right_treap\n+\/\/ split(treap, key) => (left_treap, right_treap)  where left_treap <= right_treap\n+\/\/ Recursion is used in these, but the depth of the call stack is the depth of\n+\/\/ the tree which is O(log n) so we are safe from stack overflow.\n+\n+\/\/ TreapNode has LEQ nodes on the left, GT nodes on the right.\n+template<typename K, typename V, int(*CMP)(K,K)>\n+class TreapNode {\n+  template<typename InnerK, typename InnerV, int(*CMPP)(InnerK,InnerK)>\n+  friend class TreapCHeap;\n+\n+  uint64_t _priority;\n+  const K _key;\n+  V _value;\n+  using Nd = TreapNode<K,V,CMP>;\n+  Nd* _left;\n+  Nd* _right;\n+\n+  struct nd_pair {\n+    Nd* left;\n+    Nd* right;\n+  };\n+\n+  enum SplitMode {\n+    LT, \/\/ <\n+    LEQ \/\/ <=\n+  };\n+\n+  \/\/ Split tree at head into two trees, SplitMode decides where EQ values go.\n+  \/\/ We have SplitMode because it makes remove() trivial to implement.\n+  static nd_pair split(Nd* head, const K& key, SplitMode mode = LEQ) {\n+    if (head == nullptr) {\n+      return {nullptr, nullptr};\n+    }\n+    if ( (CMP(head->_key, key) <= 0 && mode == LEQ) ||\n+         (CMP(head->_key, key) < 0 && mode == LT) ) {\n+      nd_pair p = split(head->_right, key, mode);\n+      head->_right = p.left;\n+      return {head, p.right};\n+    } else {\n+      nd_pair p = split(head->_left, key, mode);\n+      head->_left = p.right;\n+      return {p.left, head};\n+    }\n+  }\n+\n+  \/\/ Invariant: left is a treap whose keys are LEQ to the keys in right.\n+  static Nd* merge(Nd* left, Nd* right) {\n+    if (left == nullptr) return right;\n+    if (right == nullptr) return left;\n+\n+    if (left->_priority > right->_priority) {\n+      \/\/ We need\n+      \/\/      LEFT\n+      \/\/         |\n+      \/\/         RIGHT\n+      \/\/ For the invariant re: priorities to hold.\n+      left->_right = merge(left->_right, right);\n+      return left;\n+    } else {\n+      \/\/ We need\n+      \/\/         RIGHT\n+      \/\/         |\n+      \/\/      LEFT\n+      \/\/ For the invariant re: priorities to hold.\n+      right->_left = merge(left, right->_left);\n+      return right;\n+    }\n+  }\n+\n+public:\n+  TreapNode(const K& k, const V& v, uint64_t p)\n+  : _priority(p), _key(k), _value(v), _left(nullptr), _right(nullptr) {\n+  }\n+\n+  const K& key() const {\n+    return _key;\n+  }\n+  V& val() {\n+    return _value;\n+  }\n+\n+  Nd* left() const {\n+    return _left;\n+  }\n+  Nd* right() const {\n+    return _right;\n+  }\n+\n+  static Nd* find(Nd* node, const K& k) {\n+    if (node == nullptr) {\n+      return nullptr;\n+    }\n+    if (CMP(node->_key, k) == 0) { \/\/ EQ\n+      return node;\n+    }\n+\n+    if (CMP(node->_key, k) <= 0) { \/\/ LEQ\n+      return find(node->_left, k);\n+    } else {\n+      return find(node->_right, k);\n+    }\n+  }\n+\n+  template<typename MakeNode>\n+  static Nd* upsert(Nd* head, const K& k, const V& v, MakeNode make_node) {\n+    \/\/ (LEQ_k, GT_k)\n+    nd_pair split = Nd::split(head, k);\n+    Nd* found = find(split.left, k);\n+    if (found != nullptr) {\n+      \/\/ Already exists, update value.\n+      found->_value = v;\n+      return merge(split.left, split.right);\n+    }\n+    \/\/ Doesn't exist, make node\n+    Nd* node = make_node(k, v);\n+    \/\/ merge(merge(LEQ_k, EQ_k), GT_k)\n+    return merge(merge(split.left, node), split.right);\n+  }\n+\n+  template<typename Free>\n+  static Nd* remove(Nd *head, const K& k, Free free) {\n+    \/\/ (LEQ_k, GT_k)\n+    nd_pair fst_split = split(head, k, LEQ);\n+    \/\/ (LT_k, GEQ_k) == (LT_k, EQ_k) since it's from LEQ_k and keys are unique.\n+    nd_pair snd_split = split(fst_split.left, k, LT);\n+\n+    if (snd_split.right != nullptr) {\n+      \/\/ The key k existed, we delete it.\n+      free(snd_split.right);\n+    }\n+    \/\/ Merge together everything\n+    return merge(snd_split.left, fst_split.right);\n+  }\n+\n+  \/\/ Delete all nodes.\n+  template<typename Free>\n+  static Nd* delete_all(Nd* tree, Free free) {\n+    GrowableArrayCHeap<Nd*, mtNMT> to_delete;\n+    to_delete.push(tree);\n+\n+    while (!to_delete.is_empty()) {\n+      Nd* head = to_delete.pop();\n+      if (head == nullptr) continue;\n+      to_delete.push(head->_left);\n+      to_delete.push(head->_right);\n+      free(head);\n+    }\n+    return nullptr;\n+  }\n+};\n+\n+template<typename K, typename V, int(*CMP)(K,K)>\n+class TreapCHeap {\n+  friend class VMATree;\n+  friend class VMATreeTest;\n+  using CTreap = TreapNode<K, V, CMP>;\n+  CTreap* tree;\n+  uint64_t prng_seed;\n+public:\n+  TreapCHeap(uint64_t seed = 1234) : tree(nullptr), prng_seed(seed) {\n+  }\n+  ~TreapCHeap() {\n+    this->remove_all();\n+  }\n+\n+  uint64_t prng_next() {\n+    \/\/ Taken directly off of JFRPrng\n+    static const uint64_t PrngMult = 0x5DEECE66DLL;\n+    static const uint64_t PrngAdd = 0xB;\n+    static const uint64_t PrngModPower = 48;\n+    static const uint64_t PrngModMask = (static_cast<uint64_t>(1) << PrngModPower) - 1;\n+    prng_seed = (PrngMult * prng_seed + PrngAdd) & PrngModMask;\n+    return prng_seed;\n+  }\n+\n+  void upsert(const K& k, const V& v) {\n+    tree = CTreap::upsert(tree, k, v, [&](const K& k, const V& v) {\n+      uint64_t rand = this->prng_next();\n+      void* place = os::malloc(sizeof(CTreap), mtNMT);\n+      new (place) CTreap(k, v, rand);\n+      return (CTreap*)place;\n+    });\n+  }\n+\n+  void remove(const K& k) {\n+    tree = CTreap::remove(tree, k, [](void* ptr) {\n+      os::free(ptr);\n+    });\n+  }\n+\n+  void remove_all() {\n+    tree = CTreap::delete_all(tree, [](void* ptr){\n+      os::free(ptr);\n+    });\n+  }\n+\n+  CTreap* closest_geq(const K& key) {\n+    \/\/ Need to go \"left-ward\" for EQ node, so do a leq search first.\n+    CTreap* leqB = closest_leq(key);\n+    if (leqB != nullptr && leqB->key() == key) {\n+      return leqB;\n+    }\n+    CTreap* gtB = nullptr;\n+    CTreap* head = tree;\n+    while (head != nullptr) {\n+      int cmp_r = CMP(head->key(), key);\n+      if (cmp_r == 0) { \/\/ Exact match\n+        gtB = head;\n+        break; \/\/ Can't become better than that.\n+      }\n+      if (cmp_r > 0) {\n+        \/\/ Found a match, try to find a better one.\n+        gtB = head;\n+        head = head->_left;\n+      } else if (cmp_r < 0) {\n+        head = head->_right;\n+      }\n+    }\n+    return gtB;\n+  }\n+  CTreap* closest_leq(const K& key) {\n+    CTreap* leqA_n = nullptr;\n+    CTreap* head = tree;\n+    while (head != nullptr) {\n+      int cmp_r = CMP(head->key(), key);\n+      if (cmp_r == 0) { \/\/ Exact match\n+        leqA_n = head;\n+        break; \/\/ Can't become better than that.\n+      }\n+      if (cmp_r < 0) {\n+        \/\/ Found a match, try to find a better one.\n+        leqA_n = head;\n+        head = head->_right;\n+      } else if (cmp_r > 0) {\n+        head = head->_left;\n+      }\n+    }\n+    return leqA_n;\n+  }\n+};\n+\n+#endif \/\/SHARE_NMT_TREAP_HPP\n","filename":"src\/hotspot\/share\/nmt\/nmtTreap.hpp","additions":282,"deletions":0,"binary":false,"changes":282,"status":"added"},{"patch":"@@ -33,0 +33,1 @@\n+#include \"runtime\/atomic.hpp\"\n","filename":"src\/hotspot\/share\/nmt\/virtualMemoryTracker.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -0,0 +1,238 @@\n+\/*\n+ * Copyright (c) 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#include \"nmt\/vmatree.hpp\"\n+\n+VMATree::SummaryDiff VMATree::register_mapping(size_t A, size_t B, StateType state,\n+                                               Metadata& metadata) {\n+  \/\/ AddressState saves the necessary information for performing online summary accounting.\n+  struct AddressState {\n+    size_t address;\n+    NodeState state;\n+    MEMFLAGS flag_out() const {\n+      return state.out.data.flag;\n+    }\n+  };\n+\n+  \/\/ We will need the nodes which are closest to A from the left side and closest to B from the right side.\n+  \/\/ Motivating example: reserve(0,100, mtNMT); reserve(50,75, mtTest);\n+  \/\/ This will require the 2nd call to know which region the second reserve 'smashes' a hole into for proper summary accounting.\n+  \/\/ LEQ_A is figured out a bit later on, as we need to find it for other purposes anyway.\n+  bool LEQ_A_found = false;\n+  AddressState LEQ_A;\n+  bool GEQ_B_found = false;\n+  AddressState GEQ_B;\n+  VTreap* geqB_n = tree.closest_geq(B);\n+  if (geqB_n != nullptr) {\n+    GEQ_B = {geqB_n->key(), geqB_n->val()};\n+    GEQ_B_found = true;\n+  }\n+\n+  SummaryDiff diff;\n+  NodeState stA{\n+      Arrow{StateType::Released, Metadata{}},\n+      Arrow{              state,   metadata}\n+  };\n+  NodeState stB{\n+      Arrow{              state,   metadata},\n+      Arrow{StateType::Released, Metadata{}}\n+  };\n+  \/\/ First handle A.\n+  \/\/ Find closest node that is LEQ A\n+  VTreap* leqA_n = tree.closest_leq(A);\n+  if (leqA_n == nullptr) {\n+    \/\/ No match.\n+    if (stA.is_noop()) {\n+      \/\/ nothing to do.\n+    } else {\n+      \/\/ Add new node.\n+      tree.upsert(A, stA);\n+    }\n+  } else {\n+    LEQ_A_found = true;\n+    LEQ_A = AddressState{leqA_n->key(), leqA_n->val()};\n+    \/\/ Unless we know better, let B's outgoing state be the outgoing state of the node at or preceding A.\n+    \/\/ Consider the case where the found node is the start of a region enclosing [A,B)\n+    stB.out = leqA_n->val().out;\n+\n+    \/\/ Direct address match.\n+    if (leqA_n->key() == A) {\n+      \/\/ Take over in state from old address.\n+      stA.in = leqA_n->val().in;\n+\n+      \/\/ We may now be able to merge two regions:\n+      \/\/ If the node's old state matches the new, it becomes a noop. That happens, for example,\n+      \/\/ when expanding a committed area: commit [x1, A); ... commit [A, x3)\n+      \/\/ and the result should be a larger area, [x1, x3). In that case, the middle node (A and le_n)\n+      \/\/ is not needed anymore. So we just remove the old node.\n+      \/\/ We can only do this merge if the metadata is considered equivalent.\n+      stA.out.merge(leqA_n->val().out);\n+      stB.in = stA.out;\n+      if (stA.is_noop()) {\n+        \/\/ invalidates leqA_n\n+        tree.remove(leqA_n->key());\n+        \/\/ Summary accounting: Not needed, we are only expanding\n+      } else {\n+        \/\/ If the state is not matching then we have different operations, such as:\n+        \/\/ reserve [x1, A); ... commit [A, x2); or\n+        \/\/ reserve [x1, A), flag1; ... reserve [A, x2), flag2; or\n+        \/\/ reserve [A, x1), flag1; ... reserve [A, x2), flag2;\n+        \/\/ then we re-use the existing out node, overwriting its old metadata.\n+        leqA_n->val() = stA;\n+      }\n+    } else {\n+      \/\/ The address must be smaller.\n+      assert(A > leqA_n->key(), \"must be\");\n+\n+      \/\/ We add a new node, but only if there would be a state change. If there would not be a\n+      \/\/ state change, we just omit the node.\n+      \/\/ That happens, for example, when reserving within an already reserved region with identical metadata.\n+      stA.in = leqA_n->val().out; \/\/ .. and the region's prior state is the incoming state\n+      if (stA.is_noop()) {\n+        \/\/ Nothing to do.\n+      } else {\n+        \/\/ Add new node.\n+        tree.upsert(A, stA);\n+      }\n+    }\n+  }\n+\n+  \/\/ Now we handle B.\n+  \/\/ We first search all nodes that are (A, B]. All of these nodes\n+  \/\/ need to be deleted and summary accounted for. The last node before B determines B's outgoing state.\n+  \/\/ If there is no node between A and B, its A's incoming state.\n+  GrowableArrayCHeap<AddressState, mtNMT> to_be_deleted_inbetween_a_b;\n+  bool B_needs_insert = true;\n+\n+  \/\/ Find all nodes between (A, B] and record their addresses. Also update B's\n+  \/\/ outgoing state.\n+  { \/\/ Iterate over each node which is larger than A\n+    GrowableArrayCHeap<VTreap*, mtNMT> to_visit;\n+    to_visit.push(tree.tree);\n+    VTreap* head = nullptr;\n+    while (!to_visit.is_empty()) {\n+      head = to_visit.pop();\n+      if (head == nullptr) continue;\n+\n+      int cmp_A = addr_cmp(head->key(), A);\n+      int cmp_B = addr_cmp(head->key(), B);\n+      if (cmp_B > 0) {\n+        \/\/ head > B\n+        to_visit.push(head->left());\n+      } else if (cmp_A <= 0) {\n+        \/\/ head <= A\n+        to_visit.push(head->right());\n+      } else if (cmp_A > 0 && cmp_B <= 0) {\n+        \/\/ A < head <= B\n+                to_visit.push(head->left());\n+        to_visit.push(head->right());\n+\n+        stB.out = head->val().out;\n+        if (cmp_B < 0) {\n+          \/\/ Record all nodes preceding B.\n+          to_be_deleted_inbetween_a_b.push({head->key(), head->val()});\n+        } else if (cmp_B == 0) {\n+          \/\/ Re-purpose B node, unless it would result in a noop node, in\n+          \/\/ which case record old node at B for deletion and summary accounting.\n+          stB.out.merge(head->val().out);\n+          if (stB.is_noop()) {\n+            to_be_deleted_inbetween_a_b.push(AddressState{B, head->val()});\n+          } else {\n+            head->val() = stB;\n+          }\n+          B_needs_insert = false;\n+        } else { \/* Unreachable *\/\n+        }\n+      } else {\n+        \/\/ Impossible.\n+        assert(false, \"cannot happen.\");\n+      }\n+    }\n+  }\n+  \/\/ Insert B node if needed\n+  if (B_needs_insert && \/\/ Was not already inserted\n+      (!stB.is_noop() || \/\/ The operation is differing Or\n+       !Metadata::equals(stB.out.data, Metadata{})) \/\/ The metadata was changed from empty earlier\n+  ) {\n+    tree.upsert(B, stB);\n+  }\n+\n+  \/\/ Finally, we need to:\n+  \/\/ 1. Perform summary accounting.\n+  \/\/ 2. Delete all nodes between (A, B]. Including B in the case of a noop.\n+\n+  if (to_be_deleted_inbetween_a_b.length() == 0 && LEQ_A_found && GEQ_B_found &&\n+      GEQ_B.address >= B) {\n+    \/\/ We have smashed a hole in an existing region (or replaced it entirely).\n+    \/\/ LEQ_A - A - B - GEQ_B\n+    auto& rescom = diff.flag[NMTUtil::flag_to_index(LEQ_A.flag_out())];\n+    if (LEQ_A.state.out.type == StateType::Reserved) {\n+      rescom.reserve -= B - A;\n+    } else if (LEQ_A.state.out.type == StateType::Committed) {\n+      rescom.commit -= B - A;\n+      rescom.reserve -= B - A;\n+    }\n+  }\n+\n+  AddressState prev = {A, stA}; \/\/ stA is just filler\n+  MEMFLAGS flag_in = LEQ_A.flag_out();\n+  while (to_be_deleted_inbetween_a_b.length() > 0) {\n+    const AddressState delete_me = to_be_deleted_inbetween_a_b.top();\n+    to_be_deleted_inbetween_a_b.pop();\n+    tree.remove(delete_me.address);\n+    auto& rescom = diff.flag[NMTUtil::flag_to_index(flag_in)];\n+    if (delete_me.state.in.type == StateType::Reserved) {\n+      rescom.reserve -= delete_me.address - prev.address;\n+    } else if (delete_me.state.in.type == StateType::Committed) {\n+      rescom.commit -= delete_me.address - prev.address;\n+      rescom.reserve -= delete_me.address - prev.address;\n+    }\n+    prev = delete_me;\n+    flag_in = delete_me.flag_out();\n+  }\n+  if (prev.address != A && prev.state.out.type != StateType::Released &&\n+      GEQ_B.state.in.type != StateType::Released) {\n+    \/\/ There was some node inside of (A, B) and it is connected to GEQ_B\n+    \/\/ A - prev - B - GEQ_B\n+    \/\/ It might be that prev.address == B == GEQ_B.address, this is fine.\n+    if (prev.state.out.type == StateType::Reserved) {\n+      auto& rescom = diff.flag[NMTUtil::flag_to_index(prev.flag_out())];\n+      rescom.reserve -= B - prev.address;\n+    } else if (prev.state.out.type == StateType::Committed) {\n+      auto& rescom = diff.flag[NMTUtil::flag_to_index(prev.flag_out())];\n+      rescom.commit -= B - prev.address;\n+      rescom.reserve -= B - prev.address;\n+    }\n+  }\n+\n+  \/\/ Finally, we can register the new region [A, B)'s summary data.\n+  auto& rescom = diff.flag[NMTUtil::flag_to_index(metadata.flag)];\n+  if (state == StateType::Reserved) {\n+    rescom.reserve += B - A;\n+  } else if (state == StateType::Committed) {\n+    rescom.commit += B - A;\n+    rescom.reserve += B - A;\n+  }\n+  return diff;\n+}\n","filename":"src\/hotspot\/share\/nmt\/vmatree.cpp","additions":238,"deletions":0,"binary":false,"changes":238,"status":"added"},{"patch":"@@ -0,0 +1,171 @@\n+\/*\n+ * Copyright (c) 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2024, Red Hat Inc. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef SHARE_NMT_VMATREE_HPP\n+#define SHARE_NMT_VMATREE_HPP\n+\n+#include \"memory\/resourceArea.hpp\"\n+#include \"nmt\/nmtNativeCallStackStorage.hpp\"\n+#include \"nmt\/nmtTreap.hpp\"\n+#include \"runtime\/os.hpp\"\n+#include \"utilities\/globalDefinitions.hpp\"\n+#include \"utilities\/growableArray.hpp\"\n+\n+class VMATree {\n+  static int addr_cmp(size_t a, size_t b) {\n+    if (a < b) return -1;\n+    if (a == b) return 0;\n+    if (a > b) return 1;\n+    ShouldNotReachHere();\n+  }\n+\n+public:\n+  enum class StateType : uint8_t { Reserved, Committed, Released };\n+\n+  \/\/ Each node has some stack and a flag associated with it.\n+  struct Metadata {\n+    NativeCallStackStorage::StackIndex stack_idx;\n+    MEMFLAGS flag;\n+\n+    Metadata()\n+      : stack_idx(),\n+        flag(mtNone) {\n+    }\n+    Metadata(NativeCallStackStorage::StackIndex stack_idx, MEMFLAGS flag)\n+      : stack_idx(stack_idx),\n+        flag(flag) {\n+    }\n+    static bool equals(const Metadata& a, const Metadata& b) {\n+      return NativeCallStackStorage::StackIndex::equals(a.stack_idx, b.stack_idx) &&\n+             a.flag == b.flag;\n+    }\n+  };\n+\n+  struct Arrow {\n+    StateType type;\n+    Metadata data;\n+\n+    void merge(const Arrow& b) {\n+      if (this->type == StateType::Released) {\n+        this->data.flag = b.data.flag;\n+        this->data.stack_idx = b.data.stack_idx;\n+      } else if (this->type == StateType::Committed) {\n+        if (this->type == StateType::Committed) {\n+          this->data.flag = b.data.flag;\n+        }\n+      }\n+    }\n+  };\n+\n+  \/\/ A node has an arrow going into it and an arrow going out of it.\n+  struct NodeState {\n+    Arrow in;\n+    Arrow out;\n+\n+    bool is_noop() {\n+      return (in.type == StateType::Released && out.type == StateType::Released) ||\n+             (in.type == out.type && Metadata::equals(in.data, out.data));\n+    }\n+  };\n+\n+  using VTreap = TreapNode<size_t, NodeState, addr_cmp>;\n+  TreapCHeap<size_t, NodeState, addr_cmp> tree;\n+  VMATree()\n+    : tree() {\n+  }\n+\n+  struct SingleDiff {\n+    int64_t reserve;\n+    int64_t commit;\n+  };\n+  struct SummaryDiff {\n+    SingleDiff flag[mt_number_of_types];\n+    SummaryDiff() {\n+      for (int i = 0; i < mt_number_of_types; i++) {\n+        flag[i] = {0, 0};\n+      }\n+    }\n+  };\n+\n+  SummaryDiff register_mapping(size_t A, size_t B, StateType state, Metadata& metadata);\n+\n+  SummaryDiff reserve_mapping(size_t from, size_t sz, Metadata& metadata) {\n+    return register_mapping(from, from + sz, StateType::Reserved, metadata);\n+  }\n+\n+  SummaryDiff commit_mapping(size_t from, size_t sz, Metadata& metadata) {\n+    return register_mapping(from, from + sz, StateType::Committed, metadata);\n+  }\n+\n+  SummaryDiff release_mapping(size_t from, size_t sz) {\n+    Metadata empty;\n+    return register_mapping(from, from + sz, StateType::Released, empty);\n+  }\n+\n+  \/\/ Visit all nodes between [from, to) and call f on them.\n+  template<typename F>\n+  void visit(size_t from, size_t to, F f) {\n+    ResourceArea area(mtNMT);\n+    ResourceMark rm(&area);\n+    GrowableArray<VTreap*> to_visit(&area, 16, 0, nullptr);\n+    to_visit.push(tree.tree);\n+    VTreap* head = nullptr;\n+    while (!to_visit.is_empty()) {\n+      head = to_visit.pop();\n+      if (head == nullptr) continue;\n+\n+      int cmp_from = addr_cmp(head->key(), from);\n+      int cmp_to = addr_cmp(head->key(), to);\n+      if (cmp_from >= 0 && cmp_to < 0) {\n+        f(head);\n+      }\n+      if (cmp_to >= 0) {\n+        to_visit.push(head->left());\n+      } else if (cmp_from >= 0) {\n+        to_visit.push(head->left());\n+        to_visit.push(head->right());\n+      } else {\n+        to_visit.push(head->right());\n+      }\n+    }\n+  }\n+\n+private:\n+  template<typename F>\n+  void in_order_traversal_doer(F f, VTreap* node) const {\n+    if (node == nullptr) return;\n+    in_order_traversal_doer(f, node->left());\n+    f(node);\n+    in_order_traversal_doer(f, node->right());\n+  }\n+\n+public:\n+  template<typename F>\n+  void in_order_traversal(F f) const {\n+    in_order_traversal_doer(f, tree.tree);\n+  }\n+};\n+\n+#endif\n","filename":"src\/hotspot\/share\/nmt\/vmatree.hpp","additions":171,"deletions":0,"binary":false,"changes":171,"status":"added"},{"patch":"@@ -57,0 +57,1 @@\n+  friend class VMATreeTest;\n","filename":"src\/hotspot\/share\/utilities\/nativeCallStack.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -0,0 +1,31 @@\n+#include \"precompiled.hpp\"\n+#include \"memory\/allocation.hpp\"\n+#include \"nmt\/vmatree.hpp\"\n+#include \"nmt\/memTracker.hpp\"\n+#include \"runtime\/os.hpp\"\n+#include \"unittest.hpp\"\n+\n+class MemoryFileTrackerTest : public testing::Test {\n+public:\n+  size_t sz(int x) { return (size_t) x; }\n+  void basics() {\n+    MemoryFileTracker tracker(false);\n+    MemoryFileTracker::MemoryFile* dev = tracker.make_device(\"test\");\n+    tracker.allocate_memory(dev, 0, 100, mtTest, CALLER_PC);\n+    EXPECT_EQ(dev->_summary.by_type(mtTest)->reserved(), sz(100));\n+    tracker.allocate_memory(dev, 100, 100, mtTest, CALLER_PC);\n+    EXPECT_EQ(dev->_summary.by_type(mtTest)->reserved(), sz(200));\n+    tracker.allocate_memory(dev, 200, 100, mtTest, CALLER_PC);\n+    EXPECT_EQ(dev->_summary.by_type(mtTest)->reserved(), sz(300));\n+    tracker.free_memory(dev, 0, 300);\n+    EXPECT_EQ(dev->_summary.by_type(mtTest)->reserved(), sz(0));\n+    tracker.allocate_memory(dev, 0, 100, mtTest, CALLER_PC);\n+    EXPECT_EQ(dev->_summary.by_type(mtTest)->reserved(), sz(100));\n+    tracker.free_memory(dev, 50, 10);\n+    EXPECT_EQ(dev->_summary.by_type(mtTest)->reserved(), sz(90));\n+  };\n+};\n+\n+TEST_VM_F(MemoryFileTrackerTest, Basics) {\n+  this->basics();\n+}\n","filename":"test\/hotspot\/gtest\/nmt\/test_nmt_physicaldevicetracker.cpp","additions":31,"deletions":0,"binary":false,"changes":31,"status":"added"},{"patch":"@@ -0,0 +1,273 @@\n+#include \"precompiled.hpp\"\n+#include \"memory\/allocation.hpp\"\n+#include \"nmt\/vmatree.hpp\"\n+#include \"nmt\/memTracker.hpp\"\n+#include \"runtime\/os.hpp\"\n+#include \"unittest.hpp\"\n+\n+class VMATreeTest : public testing::Test {\n+public:\n+  VMATree::VTreap* treap_of(VMATree& tree) {\n+    return tree.tree.tree;\n+  }\n+  NativeCallStack make_stack(size_t a, size_t b, size_t c, size_t d) {\n+    NativeCallStack stack;\n+    stack._stack[0] = (address)a;\n+    stack._stack[1] = (address)b;\n+    stack._stack[2] = (address)c;\n+    stack._stack[3] = (address)d;\n+    return stack;\n+  }\n+  NativeCallStack stack1 = make_stack(size_t{0x00007bece59b89ac},\n+                                      size_t{0x00007bece59b1fdd},\n+                                      size_t{0x00007bece59b2997},\n+                                      size_t{0x00007bece59b2add});\n+  NativeCallStack stack2 = make_stack(0x123, 0x456,0x789, 0xAAAA);\n+};\n+\n+\/\/ Low-level tests inspecting the state of the tree.\n+TEST_VM_F(VMATreeTest, LowLevel) {\n+  using Tree = VMATree;\n+  using Node = Tree::VTreap;\n+  using NCS = NativeCallStackStorage;\n+  NativeCallStackStorage ncs(true);\n+  NativeCallStackStorage::StackIndex si1 = ncs.push(stack1);\n+  NativeCallStackStorage::StackIndex si2 = ncs.push(stack2);\n+\n+  \/\/ Adjacent reservations should result in exactly 2 nodes\n+  auto adjacent_2_nodes = [&](VMATree::Metadata& md) {\n+    Tree tree;\n+    for (int i = 0; i < 100; i++) {\n+      tree.reserve_mapping(i * 100, 100, md);\n+    }\n+    int found_nodes = 0;\n+    tree.visit(0, 999999, [&](Node* x) {\n+      found_nodes++;\n+    });\n+    EXPECT_EQ(2, found_nodes) << \"Adjacent reservations should result in exactly 2 nodes\";\n+  };\n+\n+  \/\/ After removing all ranges we should be left with an entirely empty tree\n+  auto remove_all_leaves_empty_tree = [&](VMATree::Metadata& md) {\n+    Tree tree;\n+    tree.reserve_mapping(0, 100*100, md);\n+    for (int i = 0; i < 100; i++) {\n+      tree.release_mapping(i*100, 100);\n+    }\n+    EXPECT_EQ(nullptr, treap_of(tree)) << \"Releasing all memory should result in an empty tree\";\n+\n+    \/\/ Other way around\n+    tree.reserve_mapping(0, 100*100, md);\n+    for (int i = 99; i >= 0; i--) {\n+      tree.release_mapping(i*100, 100);\n+    }\n+    EXPECT_EQ(nullptr, treap_of(tree)) << \"Releasing all memory should result in an empty tree\";\n+  };\n+\n+  \/\/ Committing in middle works as expected\n+  auto commit_middle = [&](VMATree::Metadata& md) {\n+    Tree tree;\n+    tree.reserve_mapping(0, 100, md);\n+    tree.commit_mapping(0, 50, md);\n+\n+    size_t found[16];\n+    size_t wanted[3] = {0, 50, 100};\n+    auto exists = [&](size_t x) {\n+      for (int i = 0; i < 3; i++) {\n+        if (wanted[i] == x) return true;\n+      }\n+      return false;\n+    };\n+    int i = 0;\n+    tree.visit(0, 300, [&](Node* x) {\n+      if (i < 16) {\n+        found[i] = x->key();\n+      }\n+      i++;\n+    });\n+    ASSERT_EQ(3, i) << \"0 - 50 - 100 nodes expected\";\n+    EXPECT_TRUE(exists(found[0]));\n+    EXPECT_TRUE(exists(found[1]));\n+    EXPECT_TRUE(exists(found[2]));\n+  };\n+\n+  auto commit_whole = [&](VMATree::Metadata& md) { \/\/ Committing in a whole reserved range results in 2 nodes\n+    Tree tree;\n+    tree.reserve_mapping(0, 100*100, md);\n+    for (int i = 0; i < 100; i++) {\n+      tree.commit_mapping(i*100, 100, md);\n+    }\n+    int found_nodes = 0;\n+    tree.visit(0, 999999, [&](Node* x) {\n+      found_nodes++;\n+      VMATree::NodeState& v = x->val();\n+      EXPECT_TRUE((v.in.type == VMATree::StateType::Released && v.out.type == VMATree::StateType::Committed) ||\n+                  (v.in.type == VMATree::StateType::Committed && v.out.type == VMATree::StateType::Released));\n+    });\n+    EXPECT_EQ(2, found_nodes);\n+  };\n+  VMATree::Metadata nothing;\n+  adjacent_2_nodes(nothing);\n+  remove_all_leaves_empty_tree(nothing);\n+  commit_middle(nothing);\n+  commit_whole(nothing);\n+\n+  VMATree::Metadata md{si1, mtTest };\n+  adjacent_2_nodes(md);\n+  remove_all_leaves_empty_tree(md);\n+  commit_middle(md);\n+  commit_whole(md);\n+\n+  { \/\/ Identical operation but different metadata should store both\n+    Tree tree;\n+    VMATree::Metadata md{si1, mtTest };\n+    VMATree::Metadata md2{si2, mtNMT };\n+    tree.reserve_mapping(0, 100, md);\n+    tree.reserve_mapping(100, 100, md2);\n+    int found_nodes = 0;\n+    tree.visit(0, 99999, [&](Node* x) {\n+      found_nodes++;\n+    });\n+    EXPECT_EQ(3, found_nodes);\n+  }\n+\n+  { \/\/ Reserving should overwrite commit\n+    Tree tree;\n+    VMATree::Metadata md{si1, mtTest };\n+    VMATree::Metadata md2{si2, mtNMT };\n+    tree.commit_mapping(50, 50, md2);\n+    tree.reserve_mapping(0, 100, md);\n+    int found_nodes = 0;\n+    tree.visit(0, 99999, [&](Node* x) {\n+      EXPECT_TRUE(x->key() == 0 || x->key() == 100);\n+      if (x->key() == 0) {\n+        EXPECT_EQ(x->val().out.data.flag, mtTest);\n+      }\n+      found_nodes++;\n+    });\n+    EXPECT_EQ(2, found_nodes);\n+  }\n+\n+  { \/\/ Split a reserved region into two different reserved regions\n+    Tree tree;\n+    VMATree::Metadata md{si1, mtTest };\n+    VMATree::Metadata md2{si2, mtNMT };\n+    VMATree::Metadata md3{si1, mtNone };\n+    tree.reserve_mapping(0, 100, md);\n+    tree.reserve_mapping(0, 50, md2);\n+    tree.reserve_mapping(50, 50, md3);\n+    int found_nodes = 0;\n+    tree.visit(0, 99999, [&](Node* x) {\n+      found_nodes++;\n+    });\n+    EXPECT_EQ(3, found_nodes);\n+  }\n+  { \/\/ One big reserve + release leaves an empty tree\n+    Tree::Metadata md{si1, mtNMT};\n+    Tree tree;\n+    tree.reserve_mapping(0, 500000, md);\n+    tree.release_mapping(0, 500000);\n+    EXPECT_EQ(nullptr, treap_of(tree));\n+  }\n+  { \/\/ A committed region inside of\/replacing a reserved region\n+    \/\/ should inherit the reserved region\n+    Tree::Metadata md{si1, mtNMT};\n+    VMATree::Metadata md2{si2, mtNone};\n+    Tree tree;\n+    tree.reserve_mapping(0, 100, md);\n+    tree.commit_mapping(0, 100, md2);\n+    tree.visit(0, 99999, [&](Node* x) {\n+      if (x->key() == 0) {\n+        EXPECT_EQ(mtNMT, x->val().out.data.flag);\n+      }\n+      if (x->key() == 100) {\n+        EXPECT_EQ(mtNMT, x->val().in.data.flag);\n+      }\n+    });\n+  }\n+  { \/\/ Merging prioritises the first region when committing over two reserved regions.\n+    Tree tree;\n+    VMATree::Metadata md{si1, mtTest };\n+    VMATree::Metadata md2{si2, mtNMT };\n+    VMATree::Metadata md3{si1, mtNone };\n+    tree.reserve_mapping(0, 50, md);\n+    tree.reserve_mapping(50, 50, md2);\n+    tree.commit_mapping(0, 100, md3);\n+    int found_nodes = 0;\n+    tree.visit(0, 99999, [&](Node* x) {\n+      EXPECT_TRUE(x->key() == 0 || x->key() == 100);\n+      if (x->key() == 0) {\n+        EXPECT_EQ(x->val().out.data.flag, mtTest);\n+      }\n+      if (x->key() == 100) {\n+        EXPECT_EQ(x->val().in.data.flag, mtTest);\n+      }\n+      found_nodes++;\n+    });\n+    EXPECT_EQ(2, found_nodes);\n+  }\n+}\n+\n+\/\/ Tests for summary accounting\n+TEST_VM_F(VMATreeTest, SummaryAccounting) {\n+  using Tree = VMATree;\n+  using Node = Tree::VTreap;\n+  using NCS = NativeCallStackStorage;\n+  { \/\/ Fully enclosed re-reserving works correctly.\n+    Tree::Metadata md(NCS::StackIndex(), mtTest);\n+    Tree::Metadata md2(NCS::StackIndex(), mtNMT);\n+    Tree tree;\n+    auto all_diff = tree.reserve_mapping(0, 100, md);\n+    auto diff = all_diff.flag[NMTUtil::flag_to_index(mtTest)];\n+    EXPECT_EQ(100, diff.reserve);\n+    all_diff = tree.reserve_mapping(50, 25, md2);\n+    diff = all_diff.flag[NMTUtil::flag_to_index(mtTest)];\n+    auto diff2 = all_diff.flag[NMTUtil::flag_to_index(mtNMT)];\n+    EXPECT_EQ(-25, diff.reserve);\n+    EXPECT_EQ(25, diff2.reserve);\n+  }\n+  { \/\/ Fully release reserved mapping\n+    Tree::Metadata md(NCS::StackIndex(), mtTest);\n+    Tree tree;\n+    auto all_diff = tree.reserve_mapping(0, 100, md);\n+    auto diff = all_diff.flag[NMTUtil::flag_to_index(mtTest)];\n+    EXPECT_EQ(100, diff.reserve);\n+    all_diff = tree.release_mapping(0, 100);\n+    diff = all_diff.flag[NMTUtil::flag_to_index(mtTest)];\n+    EXPECT_EQ(-100, diff.reserve);\n+  }\n+  { \/\/ Convert some of a released mapping to a committed one\n+    Tree::Metadata md(NCS::StackIndex(), mtTest);\n+    Tree tree;\n+    auto all_diff = tree.reserve_mapping(0, 100, md);\n+    auto diff = all_diff.flag[NMTUtil::flag_to_index(mtTest)];\n+    EXPECT_EQ(diff.reserve, 100);\n+    all_diff = tree.commit_mapping(0, 100, md);\n+    diff = all_diff.flag[NMTUtil::flag_to_index(mtTest)];\n+    EXPECT_EQ(0, diff.reserve);\n+    EXPECT_EQ(100, diff.commit);\n+  }\n+  { \/\/ Adjacent reserved mappings with same flag\n+    Tree::Metadata md(NCS::StackIndex(), mtTest);\n+    Tree tree;\n+    auto all_diff = tree.reserve_mapping(0, 100, md);\n+    auto diff = all_diff.flag[NMTUtil::flag_to_index(mtTest)];\n+    EXPECT_EQ(diff.reserve, 100);\n+    all_diff = tree.reserve_mapping(100, 100, md);\n+    diff = all_diff.flag[NMTUtil::flag_to_index(mtTest)];\n+    EXPECT_EQ(100, diff.reserve);\n+  }\n+  { \/\/ Adjacent reserved mappings with different flags\n+    Tree::Metadata md(NCS::StackIndex(), mtTest);\n+    Tree::Metadata md2(NCS::StackIndex(), mtNMT);\n+    Tree tree;\n+    auto all_diff = tree.reserve_mapping(0, 100, md);\n+    auto diff = all_diff.flag[NMTUtil::flag_to_index(mtTest)];\n+    EXPECT_EQ(diff.reserve, 100);\n+    all_diff = tree.reserve_mapping(100, 100, md2);\n+    diff = all_diff.flag[NMTUtil::flag_to_index(mtTest)];\n+    EXPECT_EQ(0, diff.reserve);\n+    diff = all_diff.flag[NMTUtil::flag_to_index(mtNMT)];\n+    EXPECT_EQ(100, diff.reserve);\n+  }\n+}\n","filename":"test\/hotspot\/gtest\/nmt\/test_vmatree.cpp","additions":273,"deletions":0,"binary":false,"changes":273,"status":"added"},{"patch":"@@ -89,8 +89,3 @@\n-\n-        if (XmsInM < XmxInM) {\n-            \/\/ There will be reservations which are smaller than the total\n-            \/\/ memory allocated in TestZNMT.Test.main. This means that some\n-            \/\/ reservation will be completely committed and print the following\n-            \/\/ in the NMT statistics.\n-            oa.shouldMatch(\"reserved and committed \\\\d+ for Java Heap\");\n-        }\n+        \/\/ We expect to have a report of this type.\n+        oa.shouldMatch(\"ZGC heap backing file\");\n+        oa.shouldMatch(\"allocated \\\\d+ for Java Heap\");\n@@ -100,3 +95,0 @@\n-        testValue(0);\n-        testValue(1);\n-        testValue(2);\n","filename":"test\/hotspot\/jtreg\/gc\/z\/TestZNMT.java","additions":3,"deletions":11,"binary":false,"changes":14,"status":"modified"}]}