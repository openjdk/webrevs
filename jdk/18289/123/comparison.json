{"files":[{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2016, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2016, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -35,0 +35,1 @@\n+#include \"gc\/z\/zNMT.hpp\"\n@@ -49,0 +50,1 @@\n+  ZNMT::initialize();\n","filename":"src\/hotspot\/share\/gc\/z\/zInitialize.cpp","additions":3,"deletions":1,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -31,0 +31,1 @@\n+#include \"nmt\/memoryFileTracker.hpp\"\n@@ -33,2 +34,1 @@\n-ZNMT::Reservation ZNMT::_reservations[ZMaxVirtualReservations] = {};\n-size_t ZNMT::_num_reservations = 0;\n+MemoryFileTracker::MemoryFile* ZNMT::_device = nullptr;\n@@ -36,54 +36,3 @@\n-size_t ZNMT::reservation_index(zoffset offset, size_t* offset_in_reservation) {\n-  assert(_num_reservations > 0, \"at least one reservation must exist\");\n-\n-  size_t index = 0;\n-  *offset_in_reservation = untype(offset);\n-  for (; index < _num_reservations; ++index) {\n-    const size_t reservation_size = _reservations[index]._size;\n-    if (*offset_in_reservation < reservation_size) {\n-      break;\n-    }\n-    *offset_in_reservation -= reservation_size;\n-  }\n-\n-  assert(index != _num_reservations, \"failed to find reservation index\");\n-  return index;\n-}\n-\n-void ZNMT::process_fake_mapping(zoffset offset, size_t size, bool commit) {\n-  \/\/ In order to satisfy NTM's requirement of an 1:1 mapping between committed\n-  \/\/ and reserved addresses, a fake mapping from the offset into the reservation\n-  \/\/ is used.\n-  \/\/\n-  \/\/ These mappings from\n-  \/\/   [offset, offset + size) -> {[virtual address range], ...}\n-  \/\/ are stable after the heap has been reserved. No commits proceed any\n-  \/\/ reservations. Committing and uncommitting the same [offset, offset + size)\n-  \/\/ range will result in same virtual memory ranges.\n-\n-  size_t left_to_process = size;\n-  size_t offset_in_reservation;\n-  for (size_t i = reservation_index(offset, &offset_in_reservation); i < _num_reservations; ++i) {\n-    const zaddress_unsafe reservation_start = _reservations[i]._start;\n-    const size_t reservation_size = _reservations[i]._size;\n-    const size_t sub_range_size = MIN2(left_to_process, reservation_size - offset_in_reservation);\n-    const uintptr_t sub_range_addr = untype(reservation_start) + offset_in_reservation;\n-\n-    \/\/ commit \/ uncommit memory\n-    if (commit) {\n-      MemTracker::record_virtual_memory_commit((void*)sub_range_addr, sub_range_size, CALLER_PC);\n-    } else {\n-      ThreadCritical tc;\n-      MemTracker::record_virtual_memory_uncommit((address)sub_range_addr, sub_range_size);\n-    }\n-\n-    left_to_process -= sub_range_size;\n-    if (left_to_process == 0) {\n-      \/\/ Processed all nmt registrations\n-      return;\n-    }\n-\n-    offset_in_reservation = 0;\n-  }\n-\n-  assert(left_to_process == 0, \"everything was not commited\");\n+void ZNMT::initialize() {\n+  _device = MemTracker::register_file(\"ZGC heap backing file\");\n+  assert(_device != nullptr, \"\");\n@@ -93,7 +42,1 @@\n-  assert(_num_reservations < ZMaxVirtualReservations, \"too many reservations\");\n-  \/\/ Keep track of the reservations made in order to create fake mappings\n-  \/\/ between the reserved and commited memory.\n-  \/\/ See details in ZNMT::process_fake_mapping\n-  _reservations[_num_reservations++] = {start, size};\n-\n-  MemTracker::record_virtual_memory_reserve((void*)untype(start), size, CALLER_PC, mtJavaHeap);\n+  MemTracker::record_virtual_memory_reserve((address)untype(start), size, CALLER_PC, mtJavaHeap);\n@@ -103,9 +46,1 @@\n-  \/\/ NMT expects a 1-to-1 mapping between virtual and physical memory.\n-  \/\/ ZGC can temporarily have multiple virtual addresses pointing to\n-  \/\/ the same physical memory.\n-  \/\/\n-  \/\/ When this function is called we don't know where in the virtual memory\n-  \/\/ this physical memory will be mapped. So we fake the virtual memory\n-  \/\/ address by mapping the physical offset into offsets in the reserved\n-  \/\/ memory space.\n-  process_fake_mapping(offset, size, true);\n+  MemTracker::allocate_memory_in(ZNMT::_device, untype(offset), size, CALLER_PC, mtJavaHeap);\n@@ -115,4 +50,9 @@\n-  \/\/ We fake the virtual memory address by mapping the physical offset\n-  \/\/ into offsets in the reserved memory space.\n-  \/\/ See comment in ZNMT::commit\n-  process_fake_mapping(offset, size, false);\n+  MemTracker::free_memory_in(ZNMT::_device, untype(offset), size);\n+}\n+\n+void ZNMT::map(zaddress_unsafe addr, size_t size, zoffset offset) {\n+  \/\/ NMT doesn't track mappings at the moment.\n+}\n+\n+void ZNMT::unmap(zaddress_unsafe addr, size_t size) {\n+  \/\/ NMT doesn't track mappings at the moment.\n","filename":"src\/hotspot\/share\/gc\/z\/zNMT.cpp","additions":16,"deletions":76,"binary":false,"changes":92,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2023, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -32,0 +32,2 @@\n+#include \"nmt\/memTracker.hpp\"\n+#include \"nmt\/memoryFileTracker.hpp\"\n@@ -37,9 +39,1 @@\n-  struct Reservation {\n-    zaddress_unsafe _start;\n-    size_t          _size;\n-  };\n-  static Reservation _reservations[ZMaxVirtualReservations];\n-  static size_t      _num_reservations;\n-\n-  static size_t reservation_index(zoffset offset, size_t* offset_in_reservation);\n-  static void process_fake_mapping(zoffset offset, size_t size, bool commit);\n+  static MemoryFileTracker::MemoryFile* _device;\n@@ -48,0 +42,2 @@\n+  static void initialize();\n+\n@@ -51,0 +47,3 @@\n+\n+  static void map(zaddress_unsafe addr, size_t size, zoffset offset);\n+  static void unmap(zaddress_unsafe addr, size_t size);\n","filename":"src\/hotspot\/share\/gc\/z\/zNMT.hpp","additions":9,"deletions":10,"binary":false,"changes":19,"status":"modified"},{"patch":"@@ -140,1 +140,0 @@\n-\n@@ -144,0 +143,5 @@\n+  {\n+    MemoryFileTracker::Instance::Locker lock;\n+    MemoryFileTracker::Instance::summary_snapshot(&_virtual_memory_snapshot);\n+  }\n+\n@@ -192,1 +196,0 @@\n-\n","filename":"src\/hotspot\/share\/nmt\/memBaseline.cpp","additions":5,"deletions":2,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -31,0 +31,1 @@\n+#include \"nmt\/memoryFileTracker.hpp\"\n@@ -878,1 +879,10 @@\n- }\n+}\n+\n+void MemDetailReporter::report_memory_file_allocations() {\n+  stringStream st;\n+  {\n+    MemoryFileTracker::Instance::Locker lock;\n+    MemoryFileTracker::Instance::print_all_reports_on(&st, scale());\n+  }\n+  output()->print_raw(st.freeze());\n+}\n","filename":"src\/hotspot\/share\/nmt\/memReporter.cpp","additions":11,"deletions":1,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2012, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2012, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -33,1 +33,0 @@\n-#include \"oops\/instanceKlass.hpp\"\n@@ -168,0 +167,1 @@\n+    report_memory_file_allocations();\n@@ -176,0 +176,2 @@\n+  \/\/ Report all physical devices\n+  void report_memory_file_allocations();\n","filename":"src\/hotspot\/share\/nmt\/memReporter.hpp","additions":4,"deletions":2,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -70,0 +70,1 @@\n+        !MemoryFileTracker::Instance::initialize(level) ||\n","filename":"src\/hotspot\/share\/nmt\/memTracker.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -30,0 +30,1 @@\n+#include \"nmt\/memoryFileTracker.hpp\"\n@@ -169,0 +170,33 @@\n+  static inline MemoryFileTracker::MemoryFile* register_file(const char* descriptive_name) {\n+    assert_post_init();\n+    if (!enabled()) return nullptr;\n+    MemoryFileTracker::Instance::Locker lock;\n+    return MemoryFileTracker::Instance::make_file(descriptive_name);\n+  }\n+\n+  static inline void remove_file(MemoryFileTracker::MemoryFile* file) {\n+    assert_post_init();\n+    if (!enabled()) return;\n+    assert(file != nullptr, \"must be\");\n+    MemoryFileTracker::Instance::Locker lock;\n+    MemoryFileTracker::Instance::free_file(file);\n+  }\n+\n+  static inline void allocate_memory_in(MemoryFileTracker::MemoryFile* file, size_t offset, size_t size,\n+                                       const NativeCallStack& stack, MEMFLAGS flag) {\n+    assert_post_init();\n+    if (!enabled()) return;\n+    assert(file != nullptr, \"must be\");\n+    MemoryFileTracker::Instance::Locker lock;\n+    MemoryFileTracker::Instance::allocate_memory(file, offset, size, stack, flag);\n+  }\n+\n+  static inline void free_memory_in(MemoryFileTracker::MemoryFile* file,\n+                                        size_t offset, size_t size) {\n+    assert_post_init();\n+    if (!enabled()) return;\n+    assert(file != nullptr, \"must be\");\n+    MemoryFileTracker::Instance::Locker lock;\n+    MemoryFileTracker::Instance::free_memory(file, offset, size);\n+  }\n+\n","filename":"src\/hotspot\/share\/nmt\/memTracker.hpp","additions":34,"deletions":0,"binary":false,"changes":34,"status":"modified"},{"patch":"@@ -0,0 +1,195 @@\n+\/*\n+ * Copyright (c) 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"memory\/allocation.hpp\"\n+#include \"nmt\/memTracker.hpp\"\n+#include \"nmt\/memoryFileTracker.hpp\"\n+#include \"nmt\/nmtCommon.hpp\"\n+#include \"nmt\/nmtNativeCallStackStorage.hpp\"\n+#include \"nmt\/vmatree.hpp\"\n+#include \"runtime\/mutex.hpp\"\n+#include \"utilities\/growableArray.hpp\"\n+#include \"utilities\/nativeCallStack.hpp\"\n+#include \"utilities\/ostream.hpp\"\n+\n+MemoryFileTracker* MemoryFileTracker::Instance::_tracker = nullptr;\n+PlatformMutex* MemoryFileTracker::Instance::_mutex = nullptr;\n+\n+MemoryFileTracker::MemoryFileTracker(bool is_detailed_mode)\n+  : _stack_storage(is_detailed_mode), _files() {}\n+\n+void MemoryFileTracker::allocate_memory(MemoryFile* file, size_t offset,\n+                                        size_t size, const NativeCallStack& stack,\n+                                        MEMFLAGS flag) {\n+  NativeCallStackStorage::StackIndex sidx = _stack_storage.push(stack);\n+  VMATree::RegionData regiondata(sidx, flag);\n+  VMATree::SummaryDiff diff = file->_tree.commit_mapping(offset, size, regiondata);\n+  for (int i = 0; i < mt_number_of_types; i++) {\n+    VirtualMemory* summary = file->_summary.by_type(NMTUtil::index_to_flag(i));\n+    summary->reserve_memory(diff.flag[i].commit);\n+    summary->commit_memory(diff.flag[i].commit);\n+  }\n+}\n+\n+void MemoryFileTracker::free_memory(MemoryFile* file, size_t offset, size_t size) {\n+  VMATree::SummaryDiff diff = file->_tree.release_mapping(offset, size);\n+  for (int i = 0; i < mt_number_of_types; i++) {\n+    VirtualMemory* summary = file->_summary.by_type(NMTUtil::index_to_flag(i));\n+    summary->reserve_memory(diff.flag[i].commit);\n+    summary->commit_memory(diff.flag[i].commit);\n+  }\n+}\n+\n+void MemoryFileTracker::print_report_on(const MemoryFile* file, outputStream* stream, size_t scale) {\n+  assert(MemTracker::tracking_level() == NMT_detail, \"must\");\n+\n+  stream->print_cr(\"Memory map of %s\", file->_descriptive_name);\n+  stream->cr();\n+  VMATree::TreapNode* prev = nullptr;\n+#ifdef ASSERT\n+  VMATree::TreapNode* broken_start = nullptr;\n+  VMATree::TreapNode* broken_end = nullptr;\n+#endif\n+  file->_tree.visit_in_order([&](VMATree::TreapNode* current) {\n+    if (prev == nullptr) {\n+      \/\/ Must be first node.\n+      prev = current;\n+      return;\n+    }\n+#ifdef ASSERT\n+    if (broken_start != nullptr && prev->val().out.type() != current->val().in.type()) {\n+      broken_start = prev;\n+      broken_end = current;\n+    }\n+#endif\n+    if (prev->val().out.type() == VMATree::StateType::Committed) {\n+      const VMATree::position& start_addr = prev->key();\n+      const VMATree::position& end_addr = current->key();\n+      stream->print_cr(\"[\" PTR_FORMAT \" - \" PTR_FORMAT \"] allocated \" SIZE_FORMAT \"%s\" \" for %s\",\n+                       start_addr, end_addr,\n+                       NMTUtil::amount_in_scale(end_addr - start_addr, scale),\n+                       NMTUtil::scale_name(scale),\n+                       NMTUtil::flag_to_name(prev->val().out.flag()));\n+      _stack_storage.get(prev->val().out.stack()).print_on(stream, 4);\n+      stream->cr();\n+    }\n+    prev = current;\n+  });\n+#ifdef ASSERT\n+  if (broken_start != nullptr) {\n+    tty->print_cr(\"Broken tree found with first occurrence at nodes %zu, %zu\",\n+                  broken_start->key(), broken_end->key());\n+    tty->print_cr(\"Expected start out to have same type as end in, but was: %s, %s\",\n+                  VMATree::statetype_to_string(broken_start->val().out.type()),\n+                  VMATree::statetype_to_string(broken_end->val().in.type()));\n+  }\n+#endif\n+}\n+\n+MemoryFileTracker::MemoryFile* MemoryFileTracker::make_file(const char* descriptive_name) {\n+  MemoryFile* file_place = new MemoryFile{descriptive_name};\n+  _files.push(file_place);\n+  return file_place;\n+}\n+\n+void MemoryFileTracker::free_file(MemoryFile* file) {\n+  _files.remove(file);\n+  delete file;\n+}\n+\n+const GrowableArrayCHeap<MemoryFileTracker::MemoryFile*, mtNMT>& MemoryFileTracker::files() {\n+  return _files;\n+}\n+\n+bool MemoryFileTracker::Instance::initialize(NMT_TrackingLevel tracking_level) {\n+  if (tracking_level == NMT_TrackingLevel::NMT_off) return true;\n+  _tracker = static_cast<MemoryFileTracker*>(os::malloc(sizeof(MemoryFileTracker), mtNMT));\n+  if (_tracker == nullptr) return false;\n+  new (_tracker) MemoryFileTracker(tracking_level == NMT_TrackingLevel::NMT_detail);\n+  _mutex = new PlatformMutex();\n+  return true;\n+}\n+\n+void MemoryFileTracker::Instance::allocate_memory(MemoryFile* file, size_t offset,\n+                                                  size_t size, const NativeCallStack& stack,\n+                                                  MEMFLAGS flag) {\n+  _tracker->allocate_memory(file, offset, size, stack, flag);\n+}\n+\n+void MemoryFileTracker::Instance::free_memory(MemoryFile* file, size_t offset, size_t size) {\n+  _tracker->free_memory(file, offset, size);\n+}\n+\n+MemoryFileTracker::MemoryFile*\n+MemoryFileTracker::Instance::make_file(const char* descriptive_name) {\n+  return _tracker->make_file(descriptive_name);\n+}\n+\n+void MemoryFileTracker::Instance::print_report_on(const MemoryFile* file,\n+                                                  outputStream* stream, size_t scale) {\n+  assert(file != nullptr, \"must be\");\n+  assert(stream != nullptr, \"must be\");\n+  _tracker->print_report_on(file, stream, scale);\n+}\n+\n+void MemoryFileTracker::Instance::print_all_reports_on(outputStream* stream, size_t scale) {\n+  const GrowableArrayCHeap<MemoryFileTracker::MemoryFile*, mtNMT>& files =\n+      MemoryFileTracker::Instance::files();\n+  stream->cr();\n+  stream->print_cr(\"Memory file details\");\n+  stream->cr();\n+  for (int i = 0; i < files.length(); i++) {\n+    MemoryFileTracker::MemoryFile* file = files.at(i);\n+    MemoryFileTracker::Instance::print_report_on(file, stream, scale);\n+  }\n+}\n+\n+const GrowableArrayCHeap<MemoryFileTracker::MemoryFile*, mtNMT>& MemoryFileTracker::Instance::files() {\n+  return _tracker->files();\n+};\n+\n+void MemoryFileTracker::summary_snapshot(VirtualMemorySnapshot* snapshot) const {\n+  for (int d = 0; d < _files.length(); d++) {\n+    const MemoryFile* file = _files.at(d);\n+    for (int i = 0; i < mt_number_of_types; i++) {\n+      VirtualMemory* snap = snapshot->by_type(NMTUtil::index_to_flag(i));\n+      const VirtualMemory* current = file->_summary.by_type(NMTUtil::index_to_flag(i));\n+      \/\/ Only account the committed memory.\n+      snap->commit_memory(current->committed());\n+    }\n+  }\n+}\n+\n+void MemoryFileTracker::Instance::summary_snapshot(VirtualMemorySnapshot* snapshot) {\n+  _tracker->summary_snapshot(snapshot);\n+}\n+\n+MemoryFileTracker::Instance::Locker::Locker() {\n+  MemoryFileTracker::Instance::_mutex->lock();\n+}\n+\n+MemoryFileTracker::Instance::Locker::~Locker() {\n+  MemoryFileTracker::Instance::_mutex->unlock();\n+}\n","filename":"src\/hotspot\/share\/nmt\/memoryFileTracker.cpp","additions":195,"deletions":0,"binary":false,"changes":195,"status":"added"},{"patch":"@@ -0,0 +1,111 @@\n+\/*\n+ * Copyright (c) 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef SHARE_NMT_MEMORYFILETRACKER_HPP\n+#define SHARE_NMT_MEMORYFILETRACKER_HPP\n+\n+#include \"memory\/allocation.hpp\"\n+#include \"nmt\/nmtCommon.hpp\"\n+#include \"nmt\/nmtNativeCallStackStorage.hpp\"\n+#include \"nmt\/virtualMemoryTracker.hpp\"\n+#include \"nmt\/vmatree.hpp\"\n+#include \"runtime\/mutex.hpp\"\n+#include \"runtime\/os.inline.hpp\"\n+#include \"utilities\/growableArray.hpp\"\n+#include \"utilities\/nativeCallStack.hpp\"\n+#include \"utilities\/ostream.hpp\"\n+\n+\/\/ The MemoryFileTracker tracks memory of 'memory files',\n+\/\/ storage with its own memory space separate from the process.\n+\/\/ A typical example of such a file is a memory mapped file.\n+class MemoryFileTracker {\n+  friend class MemoryFileTrackerTest;\n+\n+  \/\/ Provide caching of stacks.\n+  NativeCallStackStorage _stack_storage;\n+\n+public:\n+  class MemoryFile : public CHeapObj<mtNMT> {\n+    friend MemoryFileTracker;\n+    friend class MemoryFileTrackerTest;\n+    const char* _descriptive_name;\n+    VirtualMemorySnapshot _summary;\n+    VMATree _tree;\n+  public:\n+    NONCOPYABLE(MemoryFile);\n+    MemoryFile(const char* descriptive_name)\n+      : _descriptive_name(descriptive_name) {}\n+  };\n+\n+private:\n+  \/\/ We need pointers to each allocated file.\n+  GrowableArrayCHeap<MemoryFile*, mtNMT> _files;\n+\n+public:\n+  MemoryFileTracker(bool is_detailed_mode);\n+\n+  void allocate_memory(MemoryFile* file, size_t offset, size_t size, const NativeCallStack& stack,\n+                       MEMFLAGS flag);\n+  void free_memory(MemoryFile* file, size_t offset, size_t size);\n+\n+  MemoryFile* make_file(const char* descriptive_name);\n+  void free_file(MemoryFile* file);\n+\n+  void summary_snapshot(VirtualMemorySnapshot* snapshot) const;\n+\n+  \/\/ Print detailed report of file\n+  void print_report_on(const MemoryFile* file, outputStream* stream, size_t scale);\n+\n+  const GrowableArrayCHeap<MemoryFile*, mtNMT>& files();\n+\n+  class Instance : public AllStatic {\n+    static MemoryFileTracker* _tracker;\n+    static PlatformMutex* _mutex;\n+\n+  public:\n+    class Locker : public StackObj {\n+    public:\n+      Locker();\n+      ~Locker();\n+    };\n+\n+    static bool initialize(NMT_TrackingLevel tracking_level);\n+\n+    static MemoryFile* make_file(const char* descriptive_name);\n+    static void free_file(MemoryFile* device);\n+\n+    static void allocate_memory(MemoryFile* device, size_t offset, size_t size,\n+                                const NativeCallStack& stack, MEMFLAGS flag);\n+    static void free_memory(MemoryFile* device, size_t offset, size_t size);\n+\n+    static void summary_snapshot(VirtualMemorySnapshot* snapshot);\n+\n+    static void print_report_on(const MemoryFile* device, outputStream* stream, size_t scale);\n+    static void print_all_reports_on(outputStream* stream, size_t scale);\n+\n+    static const GrowableArrayCHeap<MemoryFile*, mtNMT>& files();\n+  };\n+};\n+\n+#endif \/\/ SHARE_NMT_MEMORYFILETRACKER_HPP\n","filename":"src\/hotspot\/share\/nmt\/memoryFileTracker.hpp","additions":111,"deletions":0,"binary":false,"changes":111,"status":"added"},{"patch":"@@ -0,0 +1,134 @@\n+\/*\n+ * Copyright (c) 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef SHARE_NMT_NMTNATIVECALLSTACKSTORAGE_HPP\n+#define SHARE_NMT_NMTNATIVECALLSTACKSTORAGE_HPP\n+\n+#include \"memory\/allocation.hpp\"\n+#include \"memory\/arena.hpp\"\n+#include \"utilities\/growableArray.hpp\"\n+#include \"utilities\/nativeCallStack.hpp\"\n+\n+\/\/ Virtual memory regions that are tracked by NMT also have their NativeCallStack (NCS) tracked.\n+\/\/ NCS:s are:\n+\/\/ - Fairly large\n+\/\/ - Regularly compared for equality\n+\/\/ - Read a lot when a detailed report is printed\n+\/\/ Therefore we'd like:\n+\/\/ - To not store duplicates\n+\/\/ - Have fast comparisons\n+\/\/ - Have constant time access\n+\/\/ We achieve this by using a closed hashtable for finding previously existing NCS:s and referring to them by an index that's smaller than a pointer.\n+class NativeCallStackStorage : public CHeapObj<mtNMT> {\n+public:\n+  struct StackIndex {\n+    friend NativeCallStackStorage;\n+\n+  private:\n+    static constexpr const int32_t _invalid = -1;\n+\n+    int32_t _stack_index;\n+    StackIndex(int32_t stack_index)\n+      : _stack_index(stack_index) {\n+    }\n+\n+  public:\n+    static bool equals(const StackIndex& a, const StackIndex& b) {\n+      return a._stack_index == b._stack_index;\n+    }\n+\n+    bool is_invalid() {\n+      return _stack_index == _invalid;\n+    }\n+\n+    StackIndex()\n+      : _stack_index(_invalid) {\n+    }\n+  };\n+\n+private:\n+  struct Link : public ArenaObj {\n+    Link* next;\n+    StackIndex stack;\n+    Link(Link* next, StackIndex v)\n+      : next(next),\n+        stack(v) {\n+    }\n+  };\n+  StackIndex put(const NativeCallStack& value) {\n+    int bucket = value.calculate_hash() % _table_size;\n+    Link* link = _table[bucket];\n+    while (link != nullptr) {\n+      if (value.equals(get(link->stack))) {\n+        return link->stack;\n+      }\n+      link = link->next;\n+    }\n+    int idx = _stacks.append(value);\n+    Link* new_link = new (&_arena) Link(_table[bucket], StackIndex(idx));\n+    _table[bucket] = new_link;\n+    return new_link->stack;\n+  }\n+\n+  \/\/ For storage of the Links\n+  Arena _arena;\n+  \/\/ Pick a prime number of buckets.\n+  \/\/ 4099 gives a 50% probability of collisions at 76 stacks (as per birthday problem).\n+  static const constexpr int default_table_size = 4099;\n+  int _table_size;\n+  Link** _table;\n+  GrowableArrayCHeap<NativeCallStack, mtNMT> _stacks;\n+  const bool _is_detailed_mode;\n+\n+  const NativeCallStack _fake_stack;\n+public:\n+\n+  StackIndex push(const NativeCallStack& stack) {\n+    \/\/ Not in detailed mode, so not tracking stacks.\n+    if (!_is_detailed_mode) {\n+      return StackIndex();\n+    }\n+    return put(stack);\n+  }\n+\n+  const inline NativeCallStack& get(StackIndex si) {\n+    if (si._stack_index == -1) {\n+      return _fake_stack;\n+    }\n+    return _stacks.at(si._stack_index);\n+  }\n+\n+  NativeCallStackStorage(bool is_detailed_mode, int table_size = default_table_size)\n+  : _arena(mtNMT), _table_size(table_size), _table(nullptr), _stacks(),\n+    _is_detailed_mode(is_detailed_mode), _fake_stack() {\n+    if (_is_detailed_mode) {\n+      _table = NEW_ARENA_ARRAY(&_arena, Link*, _table_size);\n+      for (int i = 0; i < _table_size; i++) {\n+        _table[i] = nullptr;\n+      }\n+    }\n+  }\n+};\n+\n+#endif \/\/ SHARE_NMT_NMTNATIVECALLSTACKSTORAGE_HPP\n","filename":"src\/hotspot\/share\/nmt\/nmtNativeCallStackStorage.hpp","additions":134,"deletions":0,"binary":false,"changes":134,"status":"added"},{"patch":"@@ -0,0 +1,376 @@\n+\/*\n+ * Copyright (c) 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef SHARE_NMT_NMTTREAP_HPP\n+#define SHARE_NMT_NMTTREAP_HPP\n+\n+#include \"memory\/allocation.hpp\"\n+#include \"runtime\/os.hpp\"\n+#include \"utilities\/debug.hpp\"\n+#include \"utilities\/globalDefinitions.hpp\"\n+#include \"utilities\/growableArray.hpp\"\n+#include \"utilities\/macros.hpp\"\n+#include <stdint.h>\n+\n+\/\/ A Treap is a self-balanced binary tree where each node is equipped with a\n+\/\/ priority. It adds the invariant that the priority of a parent P is strictly larger\n+\/\/ larger than the priority of its children. When priorities are randomly\n+\/\/ assigned the tree is balanced.\n+\/\/ All operations are defined through merge and split, which are each other's inverse.\n+\/\/ merge(left_treap, right_treap) => treap where left_treap <= right_treap\n+\/\/ split(treap, key) => (left_treap, right_treap)  where left_treap <= right_treap\n+\/\/ Recursion is used in these, but the depth of the call stack is the depth of\n+\/\/ the tree which is O(log n) so we are safe from stack overflow.\n+\/\/ TreapNode has LEQ nodes on the left, GT nodes on the right.\n+\/\/\n+\/\/ COMPARATOR must have a static function `cmp(a,b)` which returns:\n+\/\/     - an int < 0 when a < b\n+\/\/     - an int == 0 when a == b\n+\/\/     - an int > 0 when a > b\n+\/\/ ALLOCATOR must check for oom and exit, as Treap currently does not handle the allocation\n+\/\/ failing.\n+\n+template<typename K, typename V, typename COMPARATOR, typename ALLOCATOR>\n+class Treap {\n+  friend class VMATreeTest;\n+  friend class TreapTest;\n+public:\n+  class TreapNode {\n+    friend Treap;\n+    uint64_t _priority;\n+    const K _key;\n+    V _value;\n+\n+    TreapNode* _left;\n+    TreapNode* _right;\n+\n+  public:\n+    TreapNode(const K& k, const V& v, uint64_t p)\n+      : _priority(p),\n+        _key(k),\n+        _value(v),\n+        _left(nullptr),\n+        _right(nullptr) {\n+    }\n+\n+    const K& key() const { return _key; }\n+    V& val() { return _value; }\n+\n+    TreapNode* left() const { return _left; }\n+    TreapNode* right() const { return _right; }\n+  };\n+\n+private:\n+  ALLOCATOR _allocator;\n+  TreapNode* _root;\n+  uint64_t _prng_seed;\n+  int _node_count;\n+\n+  uint64_t prng_next() {\n+    \/\/ Taken directly off of JFRPrng\n+    static const constexpr uint64_t PrngMult = 0x5DEECE66DLL;\n+    static const constexpr uint64_t PrngAdd = 0xB;\n+    static const constexpr uint64_t PrngModPower = 48;\n+    static const constexpr uint64_t PrngModMask = (static_cast<uint64_t>(1) << PrngModPower) - 1;\n+    _prng_seed = (PrngMult * _prng_seed + PrngAdd) & PrngModMask;\n+    return _prng_seed;\n+  }\n+\n+  struct node_pair {\n+    TreapNode* left;\n+    TreapNode* right;\n+  };\n+\n+  enum SplitMode {\n+    LT, \/\/ <\n+    LEQ \/\/ <=\n+  };\n+\n+  \/\/ Split tree at head into two trees, SplitMode decides where EQ values go.\n+  \/\/ We have SplitMode because it makes remove() trivial to implement.\n+  static node_pair split(TreapNode* head, const K& key, SplitMode mode = LEQ DEBUG_ONLY(COMMA int recur_count = 0)) {\n+    assert(recur_count < 200, \"Call-stack depth should never exceed 200\");\n+\n+    if (head == nullptr) {\n+      return {nullptr, nullptr};\n+    }\n+    if ((COMPARATOR::cmp(head->_key, key) <= 0 && mode == LEQ) || (COMPARATOR::cmp(head->_key, key) < 0 && mode == LT)) {\n+      node_pair p = split(head->_right, key, mode DEBUG_ONLY(COMMA recur_count + 1));\n+      head->_right = p.left;\n+      return node_pair{head, p.right};\n+    } else {\n+      node_pair p = split(head->_left, key, mode DEBUG_ONLY(COMMA recur_count + 1));\n+      head->_left = p.right;\n+      return node_pair{p.left, head};\n+    }\n+  }\n+\n+  \/\/ Invariant: left is a treap whose keys are LEQ to the keys in right.\n+  static TreapNode* merge(TreapNode* left, TreapNode* right DEBUG_ONLY(COMMA int recur_count = 0)) {\n+    assert(recur_count < 200, \"Call-stack depth should never exceed 200\");\n+\n+    if (left == nullptr) return right;\n+    if (right == nullptr) return left;\n+\n+    if (left->_priority > right->_priority) {\n+      \/\/ We need\n+      \/\/      LEFT\n+      \/\/         |\n+      \/\/         RIGHT\n+      \/\/ for the invariant re: priorities to hold.\n+      left->_right = merge(left->_right, right DEBUG_ONLY(COMMA recur_count + 1));\n+      return left;\n+    } else {\n+      \/\/ We need\n+      \/\/         RIGHT\n+      \/\/         |\n+      \/\/      LEFT\n+      \/\/ for the invariant re: priorities to hold.\n+      right->_left = merge(left, right->_left DEBUG_ONLY(COMMA recur_count + 1));\n+      return right;\n+    }\n+  }\n+\n+  static TreapNode* find(TreapNode* node, const K& k DEBUG_ONLY(COMMA int recur_count = 0)) {\n+    if (node == nullptr) {\n+      return nullptr;\n+    }\n+\n+    int key_cmp_k = COMPARATOR::cmp(node->key(), k);\n+\n+    if (key_cmp_k == 0) { \/\/ key EQ k\n+      return node;\n+    }\n+\n+    if (key_cmp_k < 0) { \/\/ key LT k\n+      return find(node->right(), k DEBUG_ONLY(COMMA recur_count + 1));\n+    } else { \/\/ key GT k\n+      return find(node->left(), k DEBUG_ONLY(COMMA recur_count + 1));\n+    }\n+  }\n+\n+#ifdef ASSERT\n+  void verify_self() {\n+    \/\/ A balanced binary search tree should have a depth on the order of log(N).\n+    \/\/ We take the ceiling of log_2(N + 1) * 2.5 as our maximum bound.\n+    \/\/ For comparison, a RB-tree has a proven max depth of log_2(N + 1) * 2.\n+    const int expected_maximum_depth = ceil((log(this->_node_count+1) \/ log(2)) * 2.5);\n+    \/\/ Find the maximum depth through DFS and ensure that the priority invariant holds.\n+    int maximum_depth_found = 0;\n+\n+    struct DFS {\n+      int depth;\n+      uint64_t parent_prio;\n+      TreapNode* n;\n+    };\n+    GrowableArrayCHeap<DFS, mtNMT> to_visit;\n+    constexpr const uint64_t positive_infinity = 0xFFFFFFFFFFFFFFFF;\n+\n+    to_visit.push({0, positive_infinity, this->_root});\n+    while (!to_visit.is_empty()) {\n+      DFS head = to_visit.pop();\n+      if (head.n == nullptr) continue;\n+      maximum_depth_found = MAX2(maximum_depth_found, head.depth);\n+\n+      assert(head.parent_prio >= head.n->_priority, \"broken priority invariant\");\n+\n+      to_visit.push({head.depth + 1, head.n->_priority, head.n->left()});\n+      to_visit.push({head.depth + 1, head.n->_priority, head.n->right()});\n+    }\n+    assert(maximum_depth_found - expected_maximum_depth <= 3,\n+           \"depth unexpectedly large for treap of node count %d, was: %d, expected between %d and %d\",\n+           _node_count, maximum_depth_found, expected_maximum_depth - 3, expected_maximum_depth);\n+\n+    \/\/ Visit everything in order, see that the key ordering is monotonically increasing.\n+    TreapNode* last_seen = nullptr;\n+    bool failed = false;\n+    int seen_count = 0;\n+    this->visit_in_order([&](TreapNode* node) {\n+      seen_count++;\n+      if (last_seen == nullptr) {\n+        last_seen = node;\n+        return;\n+      }\n+      if (COMPARATOR::cmp(last_seen->key(), node->key()) > 0) {\n+        failed = false;\n+      }\n+      last_seen = node;\n+    });\n+    assert(seen_count == _node_count, \"the number of visited nodes do not match with the number of stored nodes\");\n+    assert(!failed, \"keys was not monotonically strongly increasing when visiting in order\");\n+  }\n+#endif \/\/ ASSERT\n+\n+public:\n+  NONCOPYABLE(Treap);\n+\n+  Treap(uint64_t seed = static_cast<uint64_t>(os::random())\n+                        | (static_cast<uint64_t>(os::random()) << 32))\n+  : _allocator(),\n+    _root(nullptr),\n+    _prng_seed(seed),\n+    _node_count(0) {}\n+\n+  ~Treap() {\n+    this->remove_all();\n+  }\n+\n+  void upsert(const K& k, const V& v) {\n+    TreapNode* found = find(_root, k);\n+    if (found != nullptr) {\n+      \/\/ Already exists, update value.\n+      found->_value = v;\n+      return;\n+    }\n+    _node_count++;\n+    \/\/ Doesn't exist, make node\n+    void* node_place = _allocator.allocate(sizeof(TreapNode));\n+    uint64_t prio = prng_next();\n+    TreapNode* node = new (node_place) TreapNode(k, v, prio);\n+\n+    \/\/ (LEQ_k, GT_k)\n+    node_pair split_up = split(this->_root, k);\n+    \/\/ merge(merge(LEQ_k, EQ_k), GT_k)\n+    this->_root = merge(merge(split_up.left, node), split_up.right);\n+  }\n+\n+  void remove(const K& k) {\n+    \/\/ (LEQ_k, GT_k)\n+    node_pair first_split = split(this->_root, k, LEQ);\n+    \/\/ (LT_k, GEQ_k) == (LT_k, EQ_k) since it's from LEQ_k and keys are unique.\n+    node_pair second_split = split(first_split.left, k, LT);\n+\n+    if (second_split.right != nullptr) {\n+      \/\/ The key k existed, we delete it.\n+      _node_count--;\n+      _allocator.free(second_split.right);\n+    }\n+    \/\/ Merge together everything\n+    _root = merge(second_split.left, first_split.right);\n+  }\n+\n+  \/\/ Delete all nodes.\n+  void remove_all() {\n+    _node_count = 0;\n+    GrowableArrayCHeap<TreapNode*, mtNMT> to_delete;\n+    to_delete.push(_root);\n+\n+    while (!to_delete.is_empty()) {\n+      TreapNode* head = to_delete.pop();\n+      if (head == nullptr) continue;\n+      to_delete.push(head->_left);\n+      to_delete.push(head->_right);\n+      _allocator.free(head);\n+    }\n+    _root = nullptr;\n+  }\n+\n+  TreapNode* closest_leq(const K& key) {\n+    TreapNode* candidate = nullptr;\n+    TreapNode* pos = _root;\n+    while (pos != nullptr) {\n+      int cmp_r = COMPARATOR::cmp(pos->key(), key);\n+      if (cmp_r == 0) { \/\/ Exact match\n+        candidate = pos;\n+        break; \/\/ Can't become better than that.\n+      }\n+      if (cmp_r < 0) {\n+        \/\/ Found a match, try to find a better one.\n+        candidate = pos;\n+        pos = pos->_right;\n+      } else if (cmp_r > 0) {\n+        pos = pos->_left;\n+      }\n+    }\n+    return candidate;\n+  }\n+\n+  \/\/ Visit all TreapNodes in ascending key order.\n+  template<typename F>\n+  void visit_in_order(F f) const {\n+    GrowableArrayCHeap<TreapNode*, mtNMT> to_visit;\n+    TreapNode* head = _root;\n+    while (!to_visit.is_empty() || head != nullptr) {\n+      while (head != nullptr) {\n+        to_visit.push(head);\n+        head = head->left();\n+      }\n+      head = to_visit.pop();\n+      f(head);\n+      head = head->right();\n+    }\n+  }\n+\n+  \/\/ Visit all TreapNodes in ascending order whose keys are in range [from, to).\n+  template<typename F>\n+  void visit_range_in_order(const K& from, const K& to, F f) {\n+    assert(COMPARATOR::cmp(from, to) <= 0, \"from must be less or equal to to\");\n+    GrowableArrayCHeap<TreapNode*, mtNMT> to_visit;\n+    TreapNode* head = _root;\n+    while (!to_visit.is_empty() || head != nullptr) {\n+      while (head != nullptr) {\n+        int cmp_from = COMPARATOR::cmp(head->key(), from);\n+        to_visit.push(head);\n+        if (cmp_from >= 0) {\n+          head = head->left();\n+        } else {\n+          \/\/ We've reached a node which is strictly less than from\n+          \/\/ We don't need to visit any further to the left.\n+          break;\n+        }\n+      }\n+      head = to_visit.pop();\n+      const int cmp_from = COMPARATOR::cmp(head->key(), from);\n+      const int cmp_to = COMPARATOR::cmp(head->key(), to);\n+      if (cmp_from >= 0 && cmp_to < 0) {\n+        f(head);\n+      }\n+      if (cmp_to < 0) {\n+        head = head->right();\n+      } else {\n+        head = nullptr;\n+      }\n+    }\n+  }\n+};\n+\n+class TreapCHeapAllocator {\n+public:\n+  void* allocate(size_t sz) {\n+    void* allocation = os::malloc(sz, mtNMT);\n+    if (allocation == nullptr) {\n+      vm_exit_out_of_memory(sz, OOM_MALLOC_ERROR, \"treap failed allocation\");\n+    }\n+    return allocation;\n+  }\n+\n+  void free(void* ptr) {\n+    os::free(ptr);\n+  }\n+};\n+\n+template<typename K, typename V, typename COMPARATOR>\n+using TreapCHeap = Treap<K, V, COMPARATOR, TreapCHeapAllocator>;\n+\n+#endif \/\/SHARE_NMT_NMTTREAP_HPP\n","filename":"src\/hotspot\/share\/nmt\/nmtTreap.hpp","additions":376,"deletions":0,"binary":false,"changes":376,"status":"added"},{"patch":"@@ -33,0 +33,1 @@\n+#include \"runtime\/atomic.hpp\"\n","filename":"src\/hotspot\/share\/nmt\/virtualMemoryTracker.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -0,0 +1,199 @@\n+\/*\n+ * Copyright (c) 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2024, Red Hat Inc.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"nmt\/vmatree.hpp\"\n+#include \"utilities\/growableArray.hpp\"\n+\n+const VMATree::RegionData VMATree::empty_regiondata{NativeCallStackStorage::StackIndex{}, mtNone};\n+\n+const char* VMATree::statetype_strings[3] = {\n+  \"reserved\", \"committed\", \"released\",\n+};\n+\n+VMATree::SummaryDiff VMATree::register_mapping(position A, position B, StateType state,\n+                                               const RegionData& metadata) {\n+  if (A == B) {\n+    \/\/ A 0-sized mapping isn't worth recording.\n+    return SummaryDiff();\n+  }\n+\n+  IntervalChange stA{\n+      IntervalState{StateType::Released, empty_regiondata},\n+      IntervalState{              state,   metadata}\n+  };\n+  IntervalChange stB{\n+      IntervalState{              state,   metadata},\n+      IntervalState{StateType::Released, empty_regiondata}\n+  };\n+\n+  \/\/ First handle A.\n+  \/\/ Find closest node that is LEQ A\n+  bool LEQ_A_found = false;\n+  AddressState LEQ_A;\n+  TreapNode* leqA_n = _tree.closest_leq(A);\n+  if (leqA_n == nullptr) {\n+    \/\/ No match. We add the A node directly, unless it would have no effect.\n+    if (!stA.is_noop()) {\n+      _tree.upsert(A, stA);\n+    }\n+  } else {\n+    LEQ_A_found = true;\n+    LEQ_A = AddressState{leqA_n->key(), leqA_n->val()};\n+    \/\/ Unless we know better, let B's outgoing state be the outgoing state of the node at or preceding A.\n+    \/\/ Consider the case where the found node is the start of a region enclosing [A,B)\n+    stB.out = leqA_n->val().out;\n+\n+    \/\/ Direct address match.\n+    if (leqA_n->key() == A) {\n+      \/\/ Take over in state from old address.\n+      stA.in = leqA_n->val().in;\n+\n+      \/\/ We may now be able to merge two regions:\n+      \/\/ If the node's old state matches the new, it becomes a noop. That happens, for example,\n+      \/\/ when expanding a committed area: commit [x1, A); ... commit [A, x3)\n+      \/\/ and the result should be a larger area, [x1, x3). In that case, the middle node (A and le_n)\n+      \/\/ is not needed anymore. So we just remove the old node.\n+      stB.in = stA.out;\n+      if (stA.is_noop()) {\n+        \/\/ invalidates leqA_n\n+        _tree.remove(leqA_n->key());\n+      } else {\n+        \/\/ If the state is not matching then we have different operations, such as:\n+        \/\/ reserve [x1, A); ... commit [A, x2); or\n+        \/\/ reserve [x1, A), flag1; ... reserve [A, x2), flag2; or\n+        \/\/ reserve [A, x1), flag1; ... reserve [A, x2), flag2;\n+        \/\/ then we re-use the existing out node, overwriting its old metadata.\n+        leqA_n->val() = stA;\n+      }\n+    } else {\n+      \/\/ The address must be smaller.\n+      assert(A > leqA_n->key(), \"must be\");\n+\n+      \/\/ We add a new node, but only if there would be a state change. If there would not be a\n+      \/\/ state change, we just omit the node.\n+      \/\/ That happens, for example, when reserving within an already reserved region with identical metadata.\n+      stA.in = leqA_n->val().out; \/\/ .. and the region's prior state is the incoming state\n+      if (stA.is_noop()) {\n+        \/\/ Nothing to do.\n+      } else {\n+        \/\/ Add new node.\n+        _tree.upsert(A, stA);\n+      }\n+    }\n+  }\n+\n+  \/\/ Now we handle B.\n+  \/\/ We first search all nodes that are (A, B]. All of these nodes\n+  \/\/ need to be deleted and summary accounted for. The last node before B determines B's outgoing state.\n+  \/\/ If there is no node between A and B, its A's incoming state.\n+  GrowableArrayCHeap<AddressState, mtNMT> to_be_deleted_inbetween_a_b;\n+  bool B_needs_insert = true;\n+\n+  \/\/ Find all nodes between (A, B] and record their addresses and values. Also update B's\n+  \/\/ outgoing state.\n+  _tree.visit_range_in_order(A + 1, B + 1, [&](TreapNode* head) {\n+    int cmp_B = PositionComparator::cmp(head->key(), B);\n+    stB.out = head->val().out;\n+    if (cmp_B < 0) {\n+      \/\/ Record all nodes preceding B.\n+      to_be_deleted_inbetween_a_b.push({head->key(), head->val()});\n+    } else if (cmp_B == 0) {\n+      \/\/ Re-purpose B node, unless it would result in a noop node, in\n+      \/\/ which case record old node at B for deletion and summary accounting.\n+      if (stB.is_noop()) {\n+        to_be_deleted_inbetween_a_b.push(AddressState{B, head->val()});\n+      } else {\n+        head->val() = stB;\n+      }\n+      B_needs_insert = false;\n+    }\n+  });\n+\n+  \/\/ Insert B node if needed\n+  if (B_needs_insert && \/\/ Was not already inserted\n+      !stB.is_noop())   \/\/ The operation is differing\n+    {\n+    _tree.upsert(B, stB);\n+  }\n+\n+  \/\/ We now need to:\n+  \/\/ a) Delete all nodes between (A, B]. Including B in the case of a noop.\n+  \/\/ b) Perform summary accounting\n+  SummaryDiff diff;\n+\n+  if (to_be_deleted_inbetween_a_b.length() == 0 && LEQ_A_found) {\n+    \/\/ We must have smashed a hole in an existing region (or replaced it entirely).\n+    \/\/ LEQ_A < A < B <= C\n+    SingleDiff& rescom = diff.flag[NMTUtil::flag_to_index(LEQ_A.out().flag())];\n+    if (LEQ_A.out().type() == StateType::Reserved) {\n+      rescom.reserve -= B - A;\n+    } else if (LEQ_A.out().type() == StateType::Committed) {\n+      rescom.commit -= B - A;\n+      rescom.reserve -= B - A;\n+    }\n+  }\n+\n+  \/\/ Track the previous node.\n+  AddressState prev{A, stA};\n+  for (int i = 0; i < to_be_deleted_inbetween_a_b.length(); i++) {\n+    const AddressState delete_me = to_be_deleted_inbetween_a_b.at(i);\n+    _tree.remove(delete_me.address);\n+\n+    \/\/ Perform summary accounting\n+    SingleDiff& rescom = diff.flag[NMTUtil::flag_to_index(delete_me.in().flag())];\n+    if (delete_me.in().type() == StateType::Reserved) {\n+      rescom.reserve -= delete_me.address - prev.address;\n+    } else if (delete_me.in().type() == StateType::Committed) {\n+      rescom.commit -= delete_me.address - prev.address;\n+      rescom.reserve -= delete_me.address - prev.address;\n+    }\n+    prev = delete_me;\n+  }\n+\n+  if (prev.address != A && prev.out().type() != StateType::Released) {\n+    \/\/ The last node wasn't released, so it must be connected to a node outside of (A, B)\n+    \/\/ A - prev - B - (some node >= B)\n+    \/\/ It might be that prev.address == B == (some node >= B), this is fine.\n+    if (prev.out().type() == StateType::Reserved) {\n+      SingleDiff& rescom = diff.flag[NMTUtil::flag_to_index(prev.out().flag())];\n+      rescom.reserve -= B - prev.address;\n+    } else if (prev.out().type() == StateType::Committed) {\n+      SingleDiff& rescom = diff.flag[NMTUtil::flag_to_index(prev.out().flag())];\n+      rescom.commit -= B - prev.address;\n+      rescom.reserve -= B - prev.address;\n+    }\n+  }\n+\n+  \/\/ Finally, we can register the new region [A, B)'s summary data.\n+  SingleDiff& rescom = diff.flag[NMTUtil::flag_to_index(metadata.flag)];\n+  if (state == StateType::Reserved) {\n+    rescom.reserve += B - A;\n+  } else if (state == StateType::Committed) {\n+    rescom.commit += B - A;\n+    rescom.reserve += B - A;\n+  }\n+  return diff;\n+}\n","filename":"src\/hotspot\/share\/nmt\/vmatree.cpp","additions":199,"deletions":0,"binary":false,"changes":199,"status":"added"},{"patch":"@@ -0,0 +1,191 @@\n+\/*\n+ * Copyright (c) 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2024, Red Hat Inc. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef SHARE_NMT_VMATREE_HPP\n+#define SHARE_NMT_VMATREE_HPP\n+\n+#include \"nmt\/nmtNativeCallStackStorage.hpp\"\n+#include \"nmt\/nmtTreap.hpp\"\n+#include \"runtime\/os.hpp\"\n+#include \"utilities\/globalDefinitions.hpp\"\n+#include <cstdint>\n+\n+\/\/ A VMATree stores a sequence of points on the natural number line.\n+\/\/ Each of these points stores information about a state change.\n+\/\/ For example, the state may go from released memory to committed memory,\n+\/\/ or from committed memory of a certain MEMFLAGS to committed memory of a different MEMFLAGS.\n+\/\/ The set of points is stored in a balanced binary tree for efficient querying and updating.\n+class VMATree {\n+  friend class VMATreeTest;\n+  \/\/ A position in memory.\n+public:\n+  using position = size_t;\n+\n+  class PositionComparator {\n+  public:\n+    static int cmp(position a, position b) {\n+      if (a < b) return -1;\n+      if (a == b) return 0;\n+      if (a > b) return 1;\n+      ShouldNotReachHere();\n+    }\n+  };\n+\n+  enum class StateType : uint8_t { Reserved, Committed, Released, LAST };\n+\n+private:\n+  static const char* statetype_strings[static_cast<uint8_t>(StateType::LAST)];\n+\n+public:\n+  NONCOPYABLE(VMATree);\n+\n+  static const char* statetype_to_string(StateType type) {\n+    assert(type != StateType::LAST, \"must be\");\n+    return statetype_strings[static_cast<uint8_t>(type)];\n+  }\n+\n+  \/\/ Each point has some stack and a flag associated with it.\n+  struct RegionData {\n+    const NativeCallStackStorage::StackIndex stack_idx;\n+    const MEMFLAGS flag;\n+\n+    RegionData() : stack_idx(), flag(mtNone) {}\n+\n+    RegionData(NativeCallStackStorage::StackIndex stack_idx, MEMFLAGS flag)\n+    : stack_idx(stack_idx), flag(flag) {}\n+\n+    static bool equals(const RegionData& a, const RegionData& b) {\n+      return a.flag == b.flag &&\n+             NativeCallStackStorage::StackIndex::equals(a.stack_idx, b.stack_idx);\n+    }\n+  };\n+\n+  static const RegionData empty_regiondata;\n+\n+private:\n+  struct IntervalState {\n+  private:\n+    \/\/ Store the type and flag as two bytes\n+    uint8_t type_flag[2];\n+    NativeCallStackStorage::StackIndex sidx;\n+\n+  public:\n+    IntervalState() : type_flag{0,0}, sidx() {}\n+    IntervalState(const StateType type, const RegionData data) {\n+      assert(!(type == StateType::Released) || data.flag == mtNone, \"Released type must have flag mtNone\");\n+      type_flag[0] = static_cast<uint8_t>(type);\n+      type_flag[1] = static_cast<uint8_t>(data.flag);\n+      sidx = data.stack_idx;\n+    }\n+\n+    StateType type() const {\n+      return static_cast<StateType>(type_flag[0]);\n+    }\n+\n+    MEMFLAGS flag() const {\n+      return static_cast<MEMFLAGS>(type_flag[1]);\n+    }\n+\n+    RegionData regiondata() const {\n+      return RegionData{sidx, flag()};\n+    }\n+\n+    const NativeCallStackStorage::StackIndex stack() const {\n+     return sidx;\n+    }\n+  };\n+\n+  \/\/ An IntervalChange indicates a change in state between two intervals. The incoming state\n+  \/\/ is denoted by in, and the outgoing state is denoted by out.\n+  struct IntervalChange {\n+    IntervalState in;\n+    IntervalState out;\n+\n+    bool is_noop() {\n+      return in.type() == out.type() &&\n+             RegionData::equals(in.regiondata(), out.regiondata());\n+    }\n+  };\n+\n+public:\n+  using VMATreap = TreapCHeap<position, IntervalChange, PositionComparator>;\n+  using TreapNode = VMATreap::TreapNode;\n+\n+private:\n+  VMATreap _tree;\n+\n+  \/\/ AddressState saves the necessary information for performing online summary accounting.\n+  struct AddressState {\n+    position address;\n+    IntervalChange state;\n+\n+    const IntervalState& out() const {\n+      return state.out;\n+    }\n+\n+    const IntervalState& in() const {\n+      return state.in;\n+    }\n+  };\n+\n+public:\n+  VMATree() : _tree() {}\n+\n+  struct SingleDiff {\n+    using delta = int64_t;\n+    delta reserve;\n+    delta commit;\n+  };\n+  struct SummaryDiff {\n+    SingleDiff flag[mt_number_of_types];\n+    SummaryDiff() {\n+      for (int i = 0; i < mt_number_of_types; i++) {\n+        flag[i] = SingleDiff{0, 0};\n+      }\n+    }\n+  };\n+\n+  SummaryDiff register_mapping(position A, position B, StateType state, const RegionData& metadata);\n+\n+  SummaryDiff reserve_mapping(position from, position sz, const RegionData& metadata) {\n+    return register_mapping(from, from + sz, StateType::Reserved, metadata);\n+  }\n+\n+  SummaryDiff commit_mapping(position from, position sz, const RegionData& metadata) {\n+    return register_mapping(from, from + sz, StateType::Committed, metadata);\n+  }\n+\n+  SummaryDiff release_mapping(position from, position sz) {\n+    return register_mapping(from, from + sz, StateType::Released, VMATree::empty_regiondata);\n+  }\n+\n+public:\n+  template<typename F>\n+  void visit_in_order(F f) const {\n+    _tree.visit_in_order(f);\n+  }\n+};\n+\n+#endif\n","filename":"src\/hotspot\/share\/nmt\/vmatree.hpp","additions":191,"deletions":0,"binary":false,"changes":191,"status":"added"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2014, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2014, 2024, Oracle and\/or its affiliates. All rights reserved.\n","filename":"src\/hotspot\/share\/utilities\/nativeCallStack.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -0,0 +1,53 @@\n+\/*\n+ * Copyright (c) 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"memory\/allocation.hpp\"\n+#include \"nmt\/memTracker.hpp\"\n+#include \"unittest.hpp\"\n+\n+class MemoryFileTrackerTest : public testing::Test {\n+public:\n+  size_t sz(int x) { return (size_t) x; }\n+  void basics() {\n+    MemoryFileTracker tracker(false);\n+    MemoryFileTracker::MemoryFile* file = tracker.make_file(\"test\");\n+    tracker.allocate_memory(file, 0, 100, CALLER_PC, mtTest);\n+    EXPECT_EQ(file->_summary.by_type(mtTest)->committed(), sz(100));\n+    tracker.allocate_memory(file, 100, 100, CALLER_PC, mtTest);\n+    EXPECT_EQ(file->_summary.by_type(mtTest)->committed(), sz(200));\n+    tracker.allocate_memory(file, 200, 100, CALLER_PC, mtTest);\n+    EXPECT_EQ(file->_summary.by_type(mtTest)->committed(), sz(300));\n+    tracker.free_memory(file, 0, 300);\n+    EXPECT_EQ(file->_summary.by_type(mtTest)->committed(), sz(0));\n+    tracker.allocate_memory(file, 0, 100, CALLER_PC, mtTest);\n+    EXPECT_EQ(file->_summary.by_type(mtTest)->committed(), sz(100));\n+    tracker.free_memory(file, 50, 10);\n+    EXPECT_EQ(file->_summary.by_type(mtTest)->committed(), sz(90));\n+  };\n+};\n+\n+TEST_VM_F(MemoryFileTrackerTest, Basics) {\n+  this->basics();\n+}\n","filename":"test\/hotspot\/gtest\/nmt\/test_nmt_memoryfiletracker.cpp","additions":53,"deletions":0,"binary":false,"changes":53,"status":"added"},{"patch":"@@ -0,0 +1,63 @@\n+\/*\n+ * Copyright (c) 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"nmt\/nmtNativeCallStackStorage.hpp\"\n+#include \"runtime\/os.hpp\"\n+#include \"unittest.hpp\"\n+\n+using NCSS = NativeCallStackStorage;\n+\n+class NativeCallStackStorageTest : public testing::Test {};\n+\n+TEST_VM_F(NativeCallStackStorageTest, DoNotStoreStackIfNotDetailed) {\n+  NativeCallStack ncs{};\n+  NCSS ncss(false);\n+  NCSS::StackIndex si = ncss.push(ncs);\n+  EXPECT_TRUE(si.is_invalid());\n+  NativeCallStack ncs_received = ncss.get(si);\n+  EXPECT_TRUE(ncs_received.is_empty());\n+}\n+\n+TEST_VM_F(NativeCallStackStorageTest, CollisionsReceiveDifferentIndexes) {\n+  constexpr const int nr_of_stacks = 10;\n+  NativeCallStack ncs_arr[nr_of_stacks];\n+  for (int i = 0; i < nr_of_stacks; i++) {\n+    ncs_arr[i] = NativeCallStack((address*)(&i), 1);\n+  }\n+\n+  NCSS ncss(true, 1);\n+  NCSS::StackIndex si_arr[nr_of_stacks];\n+  for (int i = 0; i < nr_of_stacks; i++) {\n+    si_arr[i] = ncss.push(ncs_arr[i]);\n+  }\n+\n+  \/\/ Every SI should be different as every sack is different\n+  for (int i = 0; i < nr_of_stacks; i++) {\n+    for (int j = 0; j < nr_of_stacks; j++) {\n+      if (i == j) continue;\n+      EXPECT_FALSE(NCSS::StackIndex::equals(si_arr[i],si_arr[j]));\n+    }\n+  }\n+}\n","filename":"test\/hotspot\/gtest\/nmt\/test_nmt_nativecallstackstorage.cpp","additions":63,"deletions":0,"binary":false,"changes":63,"status":"added"},{"patch":"@@ -0,0 +1,326 @@\n+\/*\n+ * Copyright (c) 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"memory\/resourceArea.hpp\"\n+#include \"nmt\/nmtTreap.hpp\"\n+#include \"runtime\/os.hpp\"\n+#include \"unittest.hpp\"\n+\n+class TreapTest : public testing::Test {\n+public:\n+  struct Cmp {\n+    static int cmp(int a, int b) {\n+      return a - b;\n+    }\n+  };\n+\n+  struct CmpInverse {\n+    static int cmp(int a, int b) {\n+      return b - a;\n+    }\n+  };\n+\n+  struct FCmp {\n+    static int cmp(float a, float b) {\n+      if (a < b) return -1;\n+      if (a == b) return 0;\n+      return 1;\n+    }\n+  };\n+\n+#ifdef ASSERT\n+  template<typename K, typename V, typename CMP, typename ALLOC>\n+  void verify_it(Treap<K, V, CMP, ALLOC>& t) {\n+    t.verify_self();\n+  }\n+#endif \/\/ ASSERT\n+\n+public:\n+  void inserting_duplicates_results_in_one_value() {\n+    constexpr const int up_to = 10;\n+    GrowableArrayCHeap<int, mtTest> nums_seen(up_to, up_to, 0);\n+    TreapCHeap<int, int, Cmp> treap;\n+\n+    for (int i = 0; i < up_to; i++) {\n+      treap.upsert(i, i);\n+      treap.upsert(i, i);\n+      treap.upsert(i, i);\n+      treap.upsert(i, i);\n+      treap.upsert(i, i);\n+    }\n+\n+    treap.visit_in_order([&](TreapCHeap<int, int, Cmp>::TreapNode* node) {\n+      nums_seen.at(node->key())++;\n+    });\n+    for (int i = 0; i < up_to; i++) {\n+      EXPECT_EQ(1, nums_seen.at(i));\n+    }\n+  }\n+\n+  void treap_ought_not_leak() {\n+    struct LeakCheckedAllocator {\n+      int allocations;\n+\n+      LeakCheckedAllocator()\n+        : allocations(0) {\n+      }\n+\n+      void* allocate(size_t sz) {\n+        void* allocation = os::malloc(sz, mtTest);\n+        if (allocation == nullptr) {\n+          vm_exit_out_of_memory(sz, OOM_MALLOC_ERROR, \"treap failed allocation\");\n+        }\n+        ++allocations;\n+        return allocation;\n+      }\n+\n+      void free(void* ptr) {\n+        --allocations;\n+        os::free(ptr);\n+      }\n+    };\n+\n+    constexpr const int up_to = 10;\n+    {\n+      Treap<int, int, Cmp, LeakCheckedAllocator> treap;\n+      for (int i = 0; i < up_to; i++) {\n+        treap.upsert(i, i);\n+      }\n+      EXPECT_EQ(up_to, treap._allocator.allocations);\n+      for (int i = 0; i < up_to; i++) {\n+        treap.remove(i);\n+      }\n+      EXPECT_EQ(0, treap._allocator.allocations);\n+      EXPECT_EQ(nullptr, treap._root);\n+    }\n+\n+    {\n+      Treap<int, int, Cmp, LeakCheckedAllocator> treap;\n+      for (int i = 0; i < up_to; i++) {\n+        treap.upsert(i, i);\n+      }\n+      treap.remove_all();\n+      EXPECT_EQ(0, treap._allocator.allocations);\n+      EXPECT_EQ(nullptr, treap._root);\n+    }\n+  }\n+\n+  void test_find() {\n+    struct Empty {};\n+    TreapCHeap<float, Empty, FCmp> treap;\n+    using Node = TreapCHeap<float, Empty, FCmp>::TreapNode;\n+\n+    Node* n = nullptr;\n+    auto test = [&](float f) {\n+      EXPECT_EQ(nullptr, treap.find(treap._root, f));\n+      treap.upsert(f, Empty{});\n+      Node* n = treap.find(treap._root, f);\n+      EXPECT_NE(nullptr, n);\n+      EXPECT_EQ(f, n->key());\n+    };\n+\n+    test(1.0f);\n+    test(5.0f);\n+    test(0.0f);\n+  }\n+};\n+\n+TEST_VM_F(TreapTest, InsertingDuplicatesResultsInOneValue) {\n+  this->inserting_duplicates_results_in_one_value();\n+}\n+\n+TEST_VM_F(TreapTest, TreapOughtNotLeak) {\n+  this->treap_ought_not_leak();\n+}\n+\n+TEST_VM_F(TreapTest, TestVisitors) {\n+  { \/\/ Tests with 'default' ordering (ascending)\n+    TreapCHeap<int, int, Cmp> treap;\n+    using Node = TreapCHeap<int, int, Cmp>::TreapNode;\n+\n+    treap.visit_range_in_order(0, 100, [&](Node* x) {\n+      EXPECT_TRUE(false) << \"Empty treap has no nodes to visit\";\n+    });\n+\n+    \/\/ Single-element set\n+    treap.upsert(1, 0);\n+    int count = 0;\n+    treap.visit_range_in_order(0, 100, [&](Node* x) {\n+      count++;\n+    });\n+    EXPECT_EQ(1, count);\n+\n+    count = 0;\n+    treap.visit_in_order([&](Node* x) {\n+      count++;\n+    });\n+    EXPECT_EQ(1, count);\n+\n+    \/\/ Add an element outside of the range that should not be visited on the right side and\n+    \/\/ one on the left side.\n+    treap.upsert(101, 0);\n+    treap.upsert(-1, 0);\n+    count = 0;\n+    treap.visit_range_in_order(0, 100, [&](Node* x) {\n+      count++;\n+    });\n+    EXPECT_EQ(1, count);\n+\n+    count = 0;\n+    treap.visit_in_order([&](Node* x) {\n+      count++;\n+    });\n+    EXPECT_EQ(3, count);\n+\n+    \/\/ Visiting empty range [0, 0) == {}\n+    treap.upsert(0, 0); \/\/ This node should not be visited.\n+    treap.visit_range_in_order(0, 0, [&](Node* x) {\n+      EXPECT_TRUE(false) << \"Empty visiting range should not visit any node\";\n+    });\n+\n+    treap.remove_all();\n+    for (int i = 0; i < 11; i++) {\n+      treap.upsert(i, 0);\n+    }\n+\n+    ResourceMark rm;\n+    GrowableArray<int> seen;\n+    treap.visit_range_in_order(0, 10, [&](Node* x) {\n+      seen.push(x->key());\n+    });\n+    EXPECT_EQ(10, seen.length());\n+    for (int i = 0; i < 10; i++) {\n+      EXPECT_EQ(i, seen.at(i));\n+    }\n+\n+    seen.clear();\n+    treap.visit_in_order([&](Node* x) {\n+      seen.push(x->key());\n+    });\n+    EXPECT_EQ(11, seen.length());\n+    for (int i = 0; i < 10; i++) {\n+      EXPECT_EQ(i, seen.at(i));\n+    }\n+\n+    seen.clear();\n+    treap.visit_range_in_order(10, 12, [&](Node* x) {\n+      seen.push(x->key());\n+    });\n+    EXPECT_EQ(1, seen.length());\n+    EXPECT_EQ(10, seen.at(0));\n+  }\n+  { \/\/ Test with descending ordering\n+    TreapCHeap<int, int, CmpInverse> treap;\n+    using Node = TreapCHeap<int, int, CmpInverse>::TreapNode;\n+\n+    for (int i = 0; i < 10; i++) {\n+      treap.upsert(i, 0);\n+    }\n+    ResourceMark rm;\n+    GrowableArray<int> seen;\n+    treap.visit_range_in_order(9, -1, [&](Node* x) {\n+      seen.push(x->key());\n+    });\n+    EXPECT_EQ(10, seen.length());\n+    for (int i = 0; i < 10; i++) {\n+      EXPECT_EQ(10-i-1, seen.at(i));\n+    }\n+    seen.clear();\n+\n+    treap.visit_in_order([&](Node* x) {\n+      seen.push(x->key());\n+    });\n+    EXPECT_EQ(10, seen.length());\n+    for (int i = 0; i < 10; i++) {\n+      EXPECT_EQ(10 - i - 1, seen.at(i));\n+    }\n+  }\n+}\n+\n+TEST_VM_F(TreapTest, TestFind) {\n+  test_find();\n+}\n+\n+TEST_VM_F(TreapTest, TestClosestLeq) {\n+  using Node = TreapCHeap<int, int, Cmp>::TreapNode;\n+  {\n+    TreapCHeap<int, int, Cmp> treap;\n+    Node* n = treap.closest_leq(0);\n+    EXPECT_EQ(nullptr, n);\n+\n+    treap.upsert(0, 0);\n+    n = treap.closest_leq(0);\n+    EXPECT_EQ(0, n->key());\n+\n+    treap.upsert(-1, -1);\n+    n = treap.closest_leq(0);\n+    EXPECT_EQ(0, n->key());\n+\n+    treap.upsert(6, 0);\n+    n = treap.closest_leq(6);\n+    EXPECT_EQ(6, n->key());\n+\n+    n = treap.closest_leq(-2);\n+    EXPECT_EQ(nullptr, n);\n+  }\n+}\n+\n+#ifdef ASSERT\n+\n+TEST_VM_F(TreapTest, VerifyItThroughStressTest) {\n+  { \/\/ Repeatedly verify a treap of moderate size\n+    TreapCHeap<int, int, Cmp> treap;\n+    constexpr const int ten_thousand = 10000;\n+    for (int i = 0; i < ten_thousand; i++) {\n+      int r = os::random();\n+      if (r % 2 == 0) {\n+        treap.upsert(i, i);\n+      } else {\n+        treap.remove(i);\n+      }\n+      verify_it(treap);\n+    }\n+    for (int i = 0; i < ten_thousand; i++) {\n+      int r = os::random();\n+      if (r % 2 == 0) {\n+        treap.upsert(i, i);\n+      } else {\n+        treap.remove(i);\n+      }\n+      verify_it(treap);\n+    }\n+  }\n+  { \/\/ Make a very large treap and verify at the end\n+  struct Nothing {};\n+    TreapCHeap<int, Nothing, Cmp> treap;\n+    constexpr const int five_million = 5000000;\n+    for (int i = 0; i < five_million; i++) {\n+      treap.upsert(i, Nothing());\n+    }\n+    verify_it(treap);\n+  }\n+}\n+\n+#endif \/\/ ASSERT\n","filename":"test\/hotspot\/gtest\/nmt\/test_nmt_treap.cpp","additions":326,"deletions":0,"binary":false,"changes":326,"status":"added"},{"patch":"@@ -0,0 +1,530 @@\n+\/*\n+ * Copyright (c) 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"memory\/allocation.hpp\"\n+#include \"nmt\/memflags.hpp\"\n+#include \"nmt\/nmtNativeCallStackStorage.hpp\"\n+#include \"nmt\/vmatree.hpp\"\n+#include \"runtime\/os.hpp\"\n+#include \"unittest.hpp\"\n+\n+using Tree = VMATree;\n+using Node = Tree::TreapNode;\n+using NCS = NativeCallStackStorage;\n+\n+class VMATreeTest : public testing::Test {\n+public:\n+  NCS ncs;\n+  constexpr static const int si_len = 2;\n+  NCS::StackIndex si[si_len];\n+  NativeCallStack stacks[si_len];\n+\n+  VMATreeTest() : ncs(true) {\n+    stacks[0] = make_stack(0xA);\n+    stacks[1] = make_stack(0xB);\n+    si[0] = ncs.push(stacks[0]);\n+    si[1] = ncs.push(stacks[0]);\n+  }\n+\n+  \/\/ Utilities\n+\n+  VMATree::TreapNode* treap_root(VMATree& tree) {\n+    return tree._tree._root;\n+  }\n+\n+  VMATree::VMATreap& treap(VMATree& tree) {\n+    return tree._tree;\n+  }\n+\n+  VMATree::TreapNode* find(VMATree::VMATreap& treap, const VMATree::position key) {\n+    return treap.find(treap._root, key);\n+  }\n+\n+  NativeCallStack make_stack(size_t a) {\n+    NativeCallStack stack((address*)&a, 1);\n+    return stack;\n+  }\n+\n+  VMATree::StateType in_type_of(VMATree::TreapNode* x) {\n+    return x->val().in.type();\n+  }\n+\n+  VMATree::StateType out_type_of(VMATree::TreapNode* x) {\n+    return x->val().out.type();\n+  }\n+\n+  int count_nodes(Tree& tree) {\n+    int count = 0;\n+    treap(tree).visit_in_order([&](Node* x) {\n+      ++count;\n+    });\n+    return count;\n+  }\n+\n+  \/\/ Tests\n+  \/\/ Adjacent reservations are merged if the properties match.\n+  void adjacent_2_nodes(const VMATree::RegionData& rd) {\n+    Tree tree;\n+    for (int i = 0; i < 10; i++) {\n+      tree.reserve_mapping(i * 100, 100, rd);\n+    }\n+    EXPECT_EQ(2, count_nodes(tree));\n+\n+    \/\/ Reserving the exact same space again should result in still having only 2 nodes\n+    for (int i = 0; i < 10; i++) {\n+      tree.reserve_mapping(i * 100, 100, rd);\n+    }\n+    EXPECT_EQ(2, count_nodes(tree));\n+\n+    \/\/ Do it backwards instead.\n+    Tree tree2;\n+    for (int i = 9; i >= 0; i--) {\n+      tree2.reserve_mapping(i * 100, 100, rd);\n+    }\n+    EXPECT_EQ(2, count_nodes(tree2));\n+  }\n+\n+  \/\/ After removing all ranges we should be left with an entirely empty tree\n+  void remove_all_leaves_empty_tree(const VMATree::RegionData& rd) {\n+    Tree tree;\n+    tree.reserve_mapping(0, 100 * 10, rd);\n+    for (int i = 0; i < 10; i++) {\n+      tree.release_mapping(i * 100, 100);\n+    }\n+    EXPECT_EQ(nullptr, treap_root(tree));\n+\n+    \/\/ Other way around\n+    tree.reserve_mapping(0, 100 * 10, rd);\n+    for (int i = 9; i >= 0; i--) {\n+      tree.release_mapping(i * 100, 100);\n+    }\n+    EXPECT_EQ(nullptr, treap_root(tree));\n+  }\n+\n+  \/\/ Committing in a whole reserved range results in 2 nodes\n+  void commit_whole(const VMATree::RegionData& rd) {\n+    Tree tree;\n+    tree.reserve_mapping(0, 100 * 10, rd);\n+    for (int i = 0; i < 10; i++) {\n+      tree.commit_mapping(i * 100, 100, rd);\n+    }\n+    treap(tree).visit_in_order([&](Node* x) {\n+      VMATree::StateType in = in_type_of(x);\n+      VMATree::StateType out = out_type_of(x);\n+      EXPECT_TRUE((in == VMATree::StateType::Released && out == VMATree::StateType::Committed) ||\n+                  (in == VMATree::StateType::Committed && out == VMATree::StateType::Released));\n+    });\n+    EXPECT_EQ(2, count_nodes(tree));\n+  }\n+\n+  \/\/ Committing in middle of reservation ends with a sequence of 4 nodes\n+  void commit_middle(const VMATree::RegionData& rd) {\n+    Tree tree;\n+    tree.reserve_mapping(0, 100, rd);\n+    tree.commit_mapping(50, 25, rd);\n+\n+    size_t found[16];\n+    size_t wanted[4] = {0, 50, 75, 100};\n+    auto exists = [&](size_t x) {\n+      for (int i = 0; i < 4; i++) {\n+        if (wanted[i] == x) return true;\n+      }\n+      return false;\n+    };\n+\n+    int i = 0;\n+    treap(tree).visit_in_order([&](Node* x) {\n+      if (i < 16) {\n+        found[i] = x->key();\n+      }\n+      i++;\n+    });\n+\n+    ASSERT_EQ(4, i) << \"0 - 50 - 75 - 100 nodes expected\";\n+    EXPECT_TRUE(exists(found[0]));\n+    EXPECT_TRUE(exists(found[1]));\n+    EXPECT_TRUE(exists(found[2]));\n+    EXPECT_TRUE(exists(found[3]));\n+  };\n+};\n+\n+\n+\n+TEST_VM_F(VMATreeTest, OverlappingReservationsResultInTwoNodes) {\n+  VMATree::RegionData rd{si[0], mtTest};\n+  Tree tree;\n+  for (int i = 99; i >= 0; i--) {\n+    tree.reserve_mapping(i * 100, 101, rd);\n+  }\n+  EXPECT_EQ(2, count_nodes(tree));\n+}\n+\n+\/\/ Low-level tests inspecting the state of the tree.\n+TEST_VM_F(VMATreeTest, LowLevel) {\n+  adjacent_2_nodes(VMATree::empty_regiondata);\n+  remove_all_leaves_empty_tree(VMATree::empty_regiondata);\n+  commit_middle(VMATree::empty_regiondata);\n+  commit_whole(VMATree::empty_regiondata);\n+\n+  VMATree::RegionData rd{si[0], mtTest };\n+  adjacent_2_nodes(rd);\n+  remove_all_leaves_empty_tree(rd);\n+  commit_middle(rd);\n+  commit_whole(rd);\n+\n+  { \/\/ Identical operation but different metadata should not merge\n+    Tree tree;\n+    VMATree::RegionData rd{si[0], mtTest };\n+    VMATree::RegionData rd2{si[1], mtNMT };\n+    tree.reserve_mapping(0, 100, rd);\n+    tree.reserve_mapping(100, 100, rd2);\n+\n+    EXPECT_EQ(3, count_nodes(tree));\n+    int found_nodes = 0;\n+  }\n+\n+  { \/\/ Reserving after commit should overwrite commit\n+    Tree tree;\n+    VMATree::RegionData rd{si[0], mtTest };\n+    VMATree::RegionData rd2{si[1], mtNMT };\n+    tree.commit_mapping(50, 50, rd2);\n+    tree.reserve_mapping(0, 100, rd);\n+    treap(tree).visit_in_order([&](Node* x) {\n+      EXPECT_TRUE(x->key() == 0 || x->key() == 100);\n+      if (x->key() == 0) {\n+        EXPECT_EQ(x->val().out.regiondata().flag, mtTest);\n+      }\n+    });\n+\n+    EXPECT_EQ(2, count_nodes(tree));\n+  }\n+\n+  { \/\/ Split a reserved region into two different reserved regions\n+    Tree tree;\n+    VMATree::RegionData rd{si[0], mtTest };\n+    VMATree::RegionData rd2{si[1], mtNMT };\n+    VMATree::RegionData rd3{si[0], mtNone };\n+    tree.reserve_mapping(0, 100, rd);\n+    tree.reserve_mapping(0, 50, rd2);\n+    tree.reserve_mapping(50, 50, rd3);\n+\n+    EXPECT_EQ(3, count_nodes(tree));\n+  }\n+  { \/\/ One big reserve + release leaves an empty tree\n+    Tree::RegionData rd{si[0], mtNMT};\n+    Tree tree;\n+    tree.reserve_mapping(0, 500000, rd);\n+    tree.release_mapping(0, 500000);\n+\n+    EXPECT_EQ(nullptr, treap_root(tree));\n+  }\n+  { \/\/ A committed region inside of\/replacing a reserved region\n+    \/\/ should replace the reserved region's metadata.\n+    Tree::RegionData rd{si[0], mtNMT};\n+    VMATree::RegionData rd2{si[1], mtTest};\n+    Tree tree;\n+    tree.reserve_mapping(0, 100, rd);\n+    tree.commit_mapping(0, 100, rd2);\n+    treap(tree).visit_range_in_order(0, 99999, [&](Node* x) {\n+      if (x->key() == 0) {\n+        EXPECT_EQ(mtTest, x->val().out.regiondata().flag);\n+      }\n+      if (x->key() == 100) {\n+        EXPECT_EQ(mtTest, x->val().in.regiondata().flag);\n+      }\n+    });\n+  }\n+\n+  { \/\/ Attempting to reserve or commit an empty region should not change the tree.\n+    Tree tree;\n+    Tree::RegionData rd{si[0], mtNMT};\n+    tree.reserve_mapping(0, 0, rd);\n+    EXPECT_EQ(nullptr, treap_root(tree));\n+    tree.commit_mapping(0, 0, rd);\n+    EXPECT_EQ(nullptr, treap_root(tree));\n+  }\n+}\n+\n+\/\/ Tests for summary accounting\n+TEST_VM_F(VMATreeTest, SummaryAccounting) {\n+  { \/\/ Fully enclosed re-reserving works correctly.\n+    Tree::RegionData rd(NCS::StackIndex(), mtTest);\n+    Tree::RegionData rd2(NCS::StackIndex(), mtNMT);\n+    Tree tree;\n+    VMATree::SummaryDiff all_diff = tree.reserve_mapping(0, 100, rd);\n+    VMATree::SingleDiff diff = all_diff.flag[NMTUtil::flag_to_index(mtTest)];\n+    EXPECT_EQ(100, diff.reserve);\n+    all_diff = tree.reserve_mapping(50, 25, rd2);\n+    diff = all_diff.flag[NMTUtil::flag_to_index(mtTest)];\n+    VMATree::SingleDiff diff2 = all_diff.flag[NMTUtil::flag_to_index(mtNMT)];\n+    EXPECT_EQ(-25, diff.reserve);\n+    EXPECT_EQ(25, diff2.reserve);\n+  }\n+  { \/\/ Fully release reserved mapping\n+    Tree::RegionData rd(NCS::StackIndex(), mtTest);\n+    Tree tree;\n+    VMATree::SummaryDiff all_diff = tree.reserve_mapping(0, 100, rd);\n+    VMATree::SingleDiff diff = all_diff.flag[NMTUtil::flag_to_index(mtTest)];\n+    EXPECT_EQ(100, diff.reserve);\n+    all_diff = tree.release_mapping(0, 100);\n+    diff = all_diff.flag[NMTUtil::flag_to_index(mtTest)];\n+    EXPECT_EQ(-100, diff.reserve);\n+  }\n+  { \/\/ Convert some of a released mapping to a committed one\n+    Tree::RegionData rd(NCS::StackIndex(), mtTest);\n+    Tree tree;\n+    VMATree::SummaryDiff all_diff = tree.reserve_mapping(0, 100, rd);\n+    VMATree::SingleDiff diff = all_diff.flag[NMTUtil::flag_to_index(mtTest)];\n+    EXPECT_EQ(diff.reserve, 100);\n+    all_diff = tree.commit_mapping(0, 100, rd);\n+    diff = all_diff.flag[NMTUtil::flag_to_index(mtTest)];\n+    EXPECT_EQ(0, diff.reserve);\n+    EXPECT_EQ(100, diff.commit);\n+  }\n+  { \/\/ Adjacent reserved mappings with same flag\n+    Tree::RegionData rd(NCS::StackIndex(), mtTest);\n+    Tree tree;\n+    VMATree::SummaryDiff all_diff = tree.reserve_mapping(0, 100, rd);\n+    VMATree::SingleDiff diff = all_diff.flag[NMTUtil::flag_to_index(mtTest)];\n+    EXPECT_EQ(diff.reserve, 100);\n+    all_diff = tree.reserve_mapping(100, 100, rd);\n+    diff = all_diff.flag[NMTUtil::flag_to_index(mtTest)];\n+    EXPECT_EQ(100, diff.reserve);\n+  }\n+  { \/\/ Adjacent reserved mappings with different flags\n+  Tree::RegionData rd(NCS::StackIndex(), mtTest);\n+    Tree::RegionData rd2(NCS::StackIndex(), mtNMT);\n+    Tree tree;\n+    VMATree::SummaryDiff all_diff = tree.reserve_mapping(0, 100, rd);\n+    VMATree::SingleDiff diff = all_diff.flag[NMTUtil::flag_to_index(mtTest)];\n+    EXPECT_EQ(diff.reserve, 100);\n+    all_diff = tree.reserve_mapping(100, 100, rd2);\n+    diff = all_diff.flag[NMTUtil::flag_to_index(mtTest)];\n+    EXPECT_EQ(0, diff.reserve);\n+    diff = all_diff.flag[NMTUtil::flag_to_index(mtNMT)];\n+    EXPECT_EQ(100, diff.reserve);\n+  }\n+\n+  { \/\/ A commit with two previous commits inside of it should only register\n+    \/\/ the new memory in the commit diff.\n+    Tree tree;\n+    Tree::RegionData rd(NCS::StackIndex(), mtTest);\n+    tree.commit_mapping(128, 128, rd);\n+    tree.commit_mapping(512, 128, rd);\n+    VMATree::SummaryDiff diff = tree.commit_mapping(0, 1024, rd);\n+    EXPECT_EQ(768, diff.flag[NMTUtil::flag_to_index(mtTest)].commit);\n+    EXPECT_EQ(768, diff.flag[NMTUtil::flag_to_index(mtTest)].reserve);\n+  }\n+}\n+\n+\/\/ Exceedingly simple tracker for page-granular allocations\n+\/\/ Use it for testing consistency with VMATree.\n+struct SimpleVMATracker : public CHeapObj<mtTest> {\n+  const size_t page_size = 4096;\n+  enum Type { Reserved, Committed, Free };\n+  struct Info {\n+    Type type;\n+    MEMFLAGS flag;\n+    NativeCallStack stack;\n+    Info() : type(Free), flag(mtNone), stack() {}\n+\n+    Info(Type type, NativeCallStack stack, MEMFLAGS flag)\n+    : type(type), flag(flag), stack(stack) {}\n+\n+    bool eq(Info other) {\n+      return flag == other.flag && stack.equals(other.stack);\n+    }\n+  };\n+  \/\/ Page (4KiB) granular array\n+  static constexpr const size_t num_pages = 1024 * 512;\n+  Info pages[num_pages];\n+\n+  SimpleVMATracker()\n+  : pages() {\n+    for (size_t i = 0; i < num_pages; i++) {\n+      pages[i] = Info();\n+    }\n+  }\n+\n+  VMATree::SummaryDiff do_it(Type type, size_t start, size_t size, NativeCallStack stack, MEMFLAGS flag) {\n+    assert(is_aligned(size, page_size) && is_aligned(start, page_size), \"page alignment\");\n+\n+    VMATree::SummaryDiff diff;\n+    const size_t page_count = size \/ page_size;\n+    const size_t start_idx = start \/ page_size;\n+    const size_t end_idx = start_idx + page_count;\n+    assert(end_idx < SimpleVMATracker::num_pages, \"\");\n+\n+    Info new_info(type, stack, flag);\n+    for (size_t i = start_idx; i < end_idx; i++) {\n+      Info& old_info = pages[i];\n+\n+      \/\/ Register diff\n+      if (old_info.type == Reserved) {\n+        diff.flag[(int)old_info.flag].reserve -= page_size;\n+      } else if (old_info.type == Committed) {\n+        diff.flag[(int)old_info.flag].reserve -= page_size;\n+        diff.flag[(int)old_info.flag].commit -= page_size;\n+      }\n+\n+      if (type == Reserved) {\n+        diff.flag[(int)new_info.flag].reserve += page_size;\n+      } else if(type == Committed) {\n+        diff.flag[(int)new_info.flag].reserve += page_size;\n+        diff.flag[(int)new_info.flag].commit += page_size;\n+      }\n+      \/\/ Overwrite old one with new\n+      pages[i] = new_info;\n+    }\n+    return diff;\n+  }\n+\n+  VMATree::SummaryDiff reserve(size_t start, size_t size, NativeCallStack stack, MEMFLAGS flag) {\n+    return do_it(Reserved, start, size, stack, flag);\n+  }\n+\n+  VMATree::SummaryDiff commit(size_t start, size_t size, NativeCallStack stack, MEMFLAGS flag) {\n+    return do_it(Committed, start, size, stack, flag);\n+  }\n+\n+  VMATree::SummaryDiff release(size_t start, size_t size) {\n+    return do_it(Free, start, size, NativeCallStack(), mtNone);\n+  }\n+};\n+\n+constexpr const size_t SimpleVMATracker::num_pages;\n+\n+TEST_VM_F(VMATreeTest, TestConsistencyWithSimpleTracker) {\n+  \/\/ In this test we use ASSERT macros from gtest instead of EXPECT\n+  \/\/ as any error will propagate and become larger as the test progresses.\n+  SimpleVMATracker* tr = new SimpleVMATracker();\n+  const size_t page_size = tr->page_size;\n+  VMATree tree;\n+  NCS ncss(true);\n+  constexpr const int candidates_len_flags = 4;\n+  constexpr const int candidates_len_stacks = 2;\n+\n+  NativeCallStack candidate_stacks[candidates_len_stacks] = {\n+    make_stack(0xA),\n+    make_stack(0xB),\n+  };\n+\n+  const MEMFLAGS candidate_flags[candidates_len_flags] = {\n+    mtNMT,\n+    mtTest,\n+    mtGC,\n+    mtCompiler\n+  };\n+\n+  const int operation_count = 100000; \/\/ One hundred thousand\n+  for (int i = 0; i < operation_count; i++) {\n+    size_t page_start = (size_t)(os::random() % SimpleVMATracker::num_pages);\n+    size_t page_end = (size_t)(os::random() % (SimpleVMATracker::num_pages));\n+\n+    if (page_end < page_start) {\n+      const size_t temp = page_start;\n+      page_start = page_end;\n+      page_end = page_start;\n+    }\n+    const size_t num_pages = page_end - page_start;\n+\n+    if (num_pages == 0) {\n+      i--; continue;\n+    }\n+\n+    const size_t start = page_start * page_size;\n+    const size_t size = num_pages * page_size;\n+\n+    const MEMFLAGS flag = candidate_flags[os::random() % candidates_len_flags];\n+    const NativeCallStack stack = candidate_stacks[os::random() % candidates_len_stacks];\n+\n+    const NCS::StackIndex si = ncss.push(stack);\n+    VMATree::RegionData data(si, flag);\n+\n+    const SimpleVMATracker::Type type = (SimpleVMATracker::Type)(os::random() % 3);\n+\n+    VMATree::SummaryDiff tree_diff;\n+    VMATree::SummaryDiff simple_diff;\n+    if (type == SimpleVMATracker::Reserved) {\n+      simple_diff = tr->reserve(start, size, stack, flag);\n+      tree_diff = tree.reserve_mapping(start, size, data);\n+    } else if (type == SimpleVMATracker::Committed) {\n+      simple_diff = tr->commit(start, size, stack, flag);\n+      tree_diff = tree.commit_mapping(start, size, data);\n+    } else {\n+      simple_diff = tr->release(start, size);\n+      tree_diff = tree.release_mapping(start, size);\n+    }\n+\n+    for (int j = 0; j < mt_number_of_types; j++) {\n+      VMATree::SingleDiff td = tree_diff.flag[j];\n+      VMATree::SingleDiff sd = simple_diff.flag[j];\n+      ASSERT_EQ(td.reserve, sd.reserve);\n+      ASSERT_EQ(td.commit, sd.commit);\n+    }\n+\n+\n+    \/\/ Do an in-depth check every 25 000 iterations.\n+    if (i % 25000 == 0) {\n+      size_t j = 0;\n+      while (j < SimpleVMATracker::num_pages) {\n+        while (j < SimpleVMATracker::num_pages &&\n+               tr->pages[j].type == SimpleVMATracker::Free) {\n+          j++;\n+        }\n+\n+        if (j == SimpleVMATracker::num_pages) {\n+          break;\n+        }\n+\n+        size_t start = j;\n+        SimpleVMATracker::Info starti = tr->pages[start];\n+\n+        while (j < SimpleVMATracker::num_pages &&\n+               tr->pages[j].eq(starti)) {\n+          j++;\n+        }\n+\n+        size_t end = j-1;\n+        ASSERT_LE(end, SimpleVMATracker::num_pages);\n+        SimpleVMATracker::Info endi = tr->pages[end];\n+\n+        VMATree::VMATreap& treap = this->treap(tree);\n+        VMATree::TreapNode* startn = find(treap, start * page_size);\n+        ASSERT_NE(nullptr, startn);\n+        VMATree::TreapNode* endn = find(treap, (end * page_size) + page_size);\n+        ASSERT_NE(nullptr, endn);\n+\n+        const NativeCallStack& start_stack = ncss.get(startn->val().out.stack());\n+        const NativeCallStack& end_stack = ncss.get(endn->val().in.stack());\n+        ASSERT_TRUE(starti.stack.equals(start_stack));\n+        ASSERT_TRUE(endi.stack.equals(end_stack));\n+\n+        ASSERT_EQ(starti.flag, startn->val().out.flag());\n+        ASSERT_EQ(endi.flag, endn->val().in.flag());\n+      }\n+    }\n+  }\n+}\n","filename":"test\/hotspot\/gtest\/nmt\/test_vmatree.cpp","additions":530,"deletions":0,"binary":false,"changes":530,"status":"added"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2023, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -89,8 +89,3 @@\n-\n-        if (XmsInM < XmxInM) {\n-            \/\/ There will be reservations which are smaller than the total\n-            \/\/ memory allocated in TestZNMT.Test.main. This means that some\n-            \/\/ reservation will be completely committed and print the following\n-            \/\/ in the NMT statistics.\n-            oa.shouldMatch(\"reserved and committed \\\\d+ for Java Heap\");\n-        }\n+        \/\/ We expect to have a report of this type.\n+        oa.shouldMatch(\"ZGC heap backing file\");\n+        oa.shouldMatch(\"allocated \\\\d+ for Java Heap\");\n@@ -100,3 +95,0 @@\n-        testValue(0);\n-        testValue(1);\n-        testValue(2);\n","filename":"test\/hotspot\/jtreg\/gc\/z\/TestZNMT.java","additions":4,"deletions":12,"binary":false,"changes":16,"status":"modified"}]}