{"files":[{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2016, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2016, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -35,0 +35,1 @@\n+#include \"gc\/z\/zNMT.hpp\"\n@@ -49,0 +50,1 @@\n+  ZNMT::initialize();\n","filename":"src\/hotspot\/share\/gc\/z\/zInitialize.cpp","additions":3,"deletions":1,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2023, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -31,0 +31,1 @@\n+#include \"nmt\/memoryFileTracker.hpp\"\n@@ -33,2 +34,1 @@\n-ZNMT::Reservation ZNMT::_reservations[ZMaxVirtualReservations] = {};\n-size_t ZNMT::_num_reservations = 0;\n+MemoryFileTracker::MemoryFile* ZNMT::_device = nullptr;\n@@ -36,54 +36,2 @@\n-size_t ZNMT::reservation_index(zoffset offset, size_t* offset_in_reservation) {\n-  assert(_num_reservations > 0, \"at least one reservation must exist\");\n-\n-  size_t index = 0;\n-  *offset_in_reservation = untype(offset);\n-  for (; index < _num_reservations; ++index) {\n-    const size_t reservation_size = _reservations[index]._size;\n-    if (*offset_in_reservation < reservation_size) {\n-      break;\n-    }\n-    *offset_in_reservation -= reservation_size;\n-  }\n-\n-  assert(index != _num_reservations, \"failed to find reservation index\");\n-  return index;\n-}\n-\n-void ZNMT::process_fake_mapping(zoffset offset, size_t size, bool commit) {\n-  \/\/ In order to satisfy NTM's requirement of an 1:1 mapping between committed\n-  \/\/ and reserved addresses, a fake mapping from the offset into the reservation\n-  \/\/ is used.\n-  \/\/\n-  \/\/ These mappings from\n-  \/\/   [offset, offset + size) -> {[virtual address range], ...}\n-  \/\/ are stable after the heap has been reserved. No commits proceed any\n-  \/\/ reservations. Committing and uncommitting the same [offset, offset + size)\n-  \/\/ range will result in same virtual memory ranges.\n-\n-  size_t left_to_process = size;\n-  size_t offset_in_reservation;\n-  for (size_t i = reservation_index(offset, &offset_in_reservation); i < _num_reservations; ++i) {\n-    const zaddress_unsafe reservation_start = _reservations[i]._start;\n-    const size_t reservation_size = _reservations[i]._size;\n-    const size_t sub_range_size = MIN2(left_to_process, reservation_size - offset_in_reservation);\n-    const uintptr_t sub_range_addr = untype(reservation_start) + offset_in_reservation;\n-\n-    \/\/ commit \/ uncommit memory\n-    if (commit) {\n-      MemTracker::record_virtual_memory_commit((void*)sub_range_addr, sub_range_size, CALLER_PC);\n-    } else {\n-      ThreadCritical tc;\n-      MemTracker::record_virtual_memory_uncommit((address)sub_range_addr, sub_range_size);\n-    }\n-\n-    left_to_process -= sub_range_size;\n-    if (left_to_process == 0) {\n-      \/\/ Processed all nmt registrations\n-      return;\n-    }\n-\n-    offset_in_reservation = 0;\n-  }\n-\n-  assert(left_to_process == 0, \"everything was not commited\");\n+void ZNMT::initialize() {\n+  _device = MemTracker::register_device(\"ZGC heap backing file\");\n@@ -93,7 +41,1 @@\n-  assert(_num_reservations < ZMaxVirtualReservations, \"too many reservations\");\n-  \/\/ Keep track of the reservations made in order to create fake mappings\n-  \/\/ between the reserved and commited memory.\n-  \/\/ See details in ZNMT::process_fake_mapping\n-  _reservations[_num_reservations++] = {start, size};\n-\n-  MemTracker::record_virtual_memory_reserve((void*)untype(start), size, CALLER_PC, mtJavaHeap);\n+  MemTracker::record_virtual_memory_reserve((address)untype(start), size, CALLER_PC, mtJavaHeap);\n@@ -103,9 +45,1 @@\n-  \/\/ NMT expects a 1-to-1 mapping between virtual and physical memory.\n-  \/\/ ZGC can temporarily have multiple virtual addresses pointing to\n-  \/\/ the same physical memory.\n-  \/\/\n-  \/\/ When this function is called we don't know where in the virtual memory\n-  \/\/ this physical memory will be mapped. So we fake the virtual memory\n-  \/\/ address by mapping the physical offset into offsets in the reserved\n-  \/\/ memory space.\n-  process_fake_mapping(offset, size, true);\n+  MemTracker::allocate_memory_in(ZNMT::_device, untype(offset), size, CALLER_PC, mtJavaHeap);\n@@ -115,4 +49,9 @@\n-  \/\/ We fake the virtual memory address by mapping the physical offset\n-  \/\/ into offsets in the reserved memory space.\n-  \/\/ See comment in ZNMT::commit\n-  process_fake_mapping(offset, size, false);\n+  MemTracker::free_memory_in(ZNMT::_device, untype(offset), size);\n+}\n+\n+void ZNMT::map(zaddress_unsafe addr, size_t size, zoffset offset) {\n+  \/\/ NMT doesn't track mappings at the moment.\n+}\n+\n+void ZNMT::unmap(zaddress_unsafe addr, size_t size) {\n+  \/\/ NMT doesn't track mappings at the moment.\n","filename":"src\/hotspot\/share\/gc\/z\/zNMT.cpp","additions":16,"deletions":77,"binary":false,"changes":93,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2023, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -32,0 +32,2 @@\n+#include \"nmt\/memTracker.hpp\"\n+#include \"nmt\/memoryFileTracker.hpp\"\n@@ -37,9 +39,1 @@\n-  struct Reservation {\n-    zaddress_unsafe _start;\n-    size_t          _size;\n-  };\n-  static Reservation _reservations[ZMaxVirtualReservations];\n-  static size_t      _num_reservations;\n-\n-  static size_t reservation_index(zoffset offset, size_t* offset_in_reservation);\n-  static void process_fake_mapping(zoffset offset, size_t size, bool commit);\n+  static MemoryFileTracker::MemoryFile* _device;\n@@ -48,0 +42,2 @@\n+  static void initialize();\n+\n@@ -51,0 +47,3 @@\n+\n+  static void map(zaddress_unsafe addr, size_t size, zoffset offset);\n+  static void unmap(zaddress_unsafe addr, size_t size);\n","filename":"src\/hotspot\/share\/gc\/z\/zNMT.hpp","additions":9,"deletions":10,"binary":false,"changes":19,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2012, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2012, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -140,1 +140,0 @@\n-\n@@ -144,0 +143,5 @@\n+  {\n+    MemoryFileTracker::Instance::Locker lock;\n+    MemoryFileTracker::Instance::summary_snapshot(&_virtual_memory_snapshot);\n+  }\n+\n@@ -197,1 +201,0 @@\n-\n","filename":"src\/hotspot\/share\/nmt\/memBaseline.cpp","additions":6,"deletions":3,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2012, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2012, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -31,0 +31,1 @@\n+#include \"nmt\/memoryFileTracker.hpp\"\n@@ -901,1 +902,6 @@\n- }\n+}\n+\n+void MemDetailReporter::report_physical_devices() {\n+  MemoryFileTracker::Instance::Locker lock;\n+  MemoryFileTracker::Instance::print_all_reports_on(output(), scale());\n+}\n","filename":"src\/hotspot\/share\/nmt\/memReporter.cpp","additions":8,"deletions":2,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2012, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2012, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -33,1 +33,0 @@\n-#include \"oops\/instanceKlass.hpp\"\n@@ -168,0 +167,1 @@\n+    report_physical_devices();\n@@ -176,0 +176,2 @@\n+  \/\/ Report all physical devices\n+  void report_physical_devices();\n","filename":"src\/hotspot\/share\/nmt\/memReporter.hpp","additions":4,"deletions":2,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2012, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2012, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -71,0 +71,1 @@\n+        !MemoryFileTracker::Instance::initialize(level) ||\n","filename":"src\/hotspot\/share\/nmt\/memTracker.cpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -30,0 +30,1 @@\n+#include \"nmt\/memoryFileTracker.hpp\"\n@@ -169,0 +170,33 @@\n+  static inline MemoryFileTracker::MemoryFile* register_device(const char* descriptive_name) {\n+    assert_post_init();\n+    if (!enabled()) return nullptr;\n+    MemoryFileTracker::Instance::Locker lock;\n+    return MemoryFileTracker::Instance::make_device(descriptive_name);\n+  }\n+\n+  static inline void remove_device(MemoryFileTracker::MemoryFile* device) {\n+    assert(device != nullptr, \"must be\");\n+    assert_post_init();\n+    if (!enabled()) return;\n+    MemoryFileTracker::Instance::Locker lock;\n+    MemoryFileTracker::Instance::free_device(device);\n+  }\n+\n+  static inline void allocate_memory_in(MemoryFileTracker::MemoryFile* device, size_t offset, size_t size,\n+                                       const NativeCallStack& stack, MEMFLAGS flag) {\n+    assert(device != nullptr, \"must be\");\n+    assert_post_init();\n+    if (!enabled()) return;\n+    MemoryFileTracker::Instance::Locker lock;\n+    MemoryFileTracker::Instance::allocate_memory(device, offset, size, stack, flag);\n+  }\n+\n+  static inline void free_memory_in(MemoryFileTracker::MemoryFile* device,\n+                                        size_t offset, size_t size) {\n+    assert(device != nullptr, \"must be\");\n+    assert_post_init();\n+    if (!enabled()) return;\n+    MemoryFileTracker::Instance::Locker lock;\n+    MemoryFileTracker::Instance::free_memory(device, offset, size);\n+  }\n+\n","filename":"src\/hotspot\/share\/nmt\/memTracker.hpp","additions":34,"deletions":0,"binary":false,"changes":34,"status":"modified"},{"patch":"@@ -0,0 +1,177 @@\n+\/*\n+ * Copyright (c) 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"memory\/allocation.hpp\"\n+#include \"nmt\/nmtCommon.hpp\"\n+#include \"nmt\/memoryFileTracker.hpp\"\n+#include \"nmt\/nmtNativeCallStackStorage.hpp\"\n+#include \"nmt\/vmatree.hpp\"\n+#include \"runtime\/mutex.hpp\"\n+#include \"utilities\/growableArray.hpp\"\n+#include \"utilities\/nativeCallStack.hpp\"\n+#include \"utilities\/ostream.hpp\"\n+\n+MemoryFileTracker* MemoryFileTracker::Instance::_tracker = nullptr;\n+PlatformMutex* MemoryFileTracker::Instance::_mutex = nullptr;\n+\n+MemoryFileTracker::MemoryFileTracker(bool is_detailed_mode)\n+  : _stack_storage(is_detailed_mode), _devices() {}\n+\n+void MemoryFileTracker::allocate_memory(MemoryFile* device, size_t offset,\n+                                        size_t size, const NativeCallStack& stack,\n+                                        MEMFLAGS flag) {\n+  NativeCallStackStorage::StackIndex sidx = _stack_storage.push(stack);\n+  VMATree::RegionData regiondata(sidx, flag);\n+  VMATree::SummaryDiff diff = device->_tree.reserve_mapping(offset, size, regiondata);\n+  for (int i = 0; i < mt_number_of_types; i++) {\n+    VirtualMemory* summary = device->_summary.by_type(NMTUtil::index_to_flag(i));\n+    summary->reserve_memory(diff.flag[i].reserve);\n+  }\n+}\n+\n+void MemoryFileTracker::free_memory(MemoryFile* device, size_t offset, size_t size) {\n+  VMATree::SummaryDiff diff = device->_tree.release_mapping(offset, size);\n+  for (int i = 0; i < mt_number_of_types; i++) {\n+    VirtualMemory* summary = device->_summary.by_type(NMTUtil::index_to_flag(i));\n+    summary->reserve_memory(diff.flag[i].reserve);\n+  }\n+}\n+\n+void MemoryFileTracker::print_report_on(const MemoryFile* device, outputStream* stream, size_t scale) {\n+  stream->print_cr(\"Memory map of %s\", device->_descriptive_name);\n+  stream->cr();\n+  VMATree::TreapNode* prev = nullptr;\n+  device->_tree.visit_in_order([&](VMATree::TreapNode* current) {\n+    if (prev == nullptr) {\n+      \/\/ Must be first node.\n+      prev = current;\n+      return;\n+    }\n+    assert(prev->val().out.type() == current->val().in.type(), \"must be\");\n+    if (prev->val().out.type() == VMATree::StateType::Reserved) {\n+      const auto& start_addr = prev->key();\n+      const auto& end_addr = current->key();\n+      stream->print_cr(\"[\" PTR_FORMAT \" - \" PTR_FORMAT \"] allocated \" SIZE_FORMAT \"%s\" \" for %s\",\n+                       start_addr, end_addr,\n+                       NMTUtil::amount_in_scale(end_addr - start_addr, scale),\n+                       NMTUtil::scale_name(scale),\n+                       NMTUtil::flag_to_name(prev->val().out.flag()));\n+      _stack_storage.get(prev->val().out.stack()).print_on(stream, 4);\n+      stream->cr();\n+    }\n+    prev = current;\n+  });\n+}\n+\n+MemoryFileTracker::MemoryFile* MemoryFileTracker::make_device(const char* descriptive_name) {\n+  MemoryFile* device_place = new MemoryFile{descriptive_name};\n+  _devices.push(device_place);\n+  return device_place;\n+}\n+\n+void MemoryFileTracker::free_device(MemoryFile* device) {\n+  _devices.remove(device);\n+  delete device;\n+}\n+\n+const GrowableArrayCHeap<MemoryFileTracker::MemoryFile*, mtNMT>& MemoryFileTracker::devices() {\n+  return _devices;\n+}\n+\n+const VirtualMemorySnapshot& MemoryFileTracker::summary_for(const MemoryFile* device) {\n+  return device->_summary;\n+}\n+\n+\n+bool MemoryFileTracker::Instance::initialize(NMT_TrackingLevel tracking_level) {\n+  if (tracking_level == NMT_TrackingLevel::NMT_off) return true;\n+  _tracker = static_cast<MemoryFileTracker*>(os::malloc(sizeof(MemoryFileTracker), mtNMT));\n+  if (_tracker == nullptr) return false;\n+  new (_tracker) MemoryFileTracker(tracking_level == NMT_TrackingLevel::NMT_detail);\n+  _mutex = new PlatformMutex();\n+  return true;\n+}\n+\n+void MemoryFileTracker::Instance::allocate_memory(MemoryFile* device, size_t offset,\n+                                                  size_t size, const NativeCallStack& stack,\n+                                                  MEMFLAGS flag) {\n+  _tracker->allocate_memory(device, offset, size, stack, flag);\n+}\n+\n+void MemoryFileTracker::Instance::free_memory(MemoryFile* device, size_t offset, size_t size) {\n+  _tracker->free_memory(device, offset, size);\n+}\n+\n+MemoryFileTracker::MemoryFile*\n+MemoryFileTracker::Instance::make_device(const char* descriptive_name) {\n+  return _tracker->make_device(descriptive_name);\n+}\n+\n+void MemoryFileTracker::Instance::print_report_on(const MemoryFile* device,\n+                                                  outputStream* stream, size_t scale) {\n+  assert(device != nullptr, \"must be\");\n+  assert(stream != nullptr, \"must be\");\n+  _tracker->print_report_on(device, stream, scale);\n+}\n+\n+void MemoryFileTracker::Instance::print_all_reports_on(outputStream* stream, size_t scale) {\n+  const GrowableArrayCHeap<MemoryFileTracker::MemoryFile*, mtNMT>& devices =\n+      MemoryFileTracker::Instance::devices();\n+  stream->cr();\n+  stream->print_cr(\"Memory file details\");\n+  stream->cr();\n+  for (int i = 0; i < devices.length(); i++) {\n+    MemoryFileTracker::MemoryFile* dev = devices.at(i);\n+    MemoryFileTracker::Instance::print_report_on(dev, stream, scale);\n+  }\n+}\n+\n+const GrowableArrayCHeap<MemoryFileTracker::MemoryFile*, mtNMT>& MemoryFileTracker::Instance::devices() {\n+  return _tracker->devices();\n+};\n+\n+void MemoryFileTracker::summary_snapshot(VirtualMemorySnapshot* snapshot) const {\n+  for (int d = 0; d < _devices.length(); d++) {\n+    auto& device = _devices.at(d);\n+    for (int i = 0; i < mt_number_of_types; i++) {\n+      auto snap = snapshot->by_type(NMTUtil::index_to_flag(i));\n+      auto current = device->_summary.by_type(NMTUtil::index_to_flag(i));\n+      \/\/ The MemoryFileTracker stores the memory as reserved but it's accounted as committed.\n+      snap->commit_memory(current->reserved());\n+    }\n+  }\n+}\n+\n+void MemoryFileTracker::Instance::summary_snapshot(VirtualMemorySnapshot* snapshot) {\n+  _tracker->summary_snapshot(snapshot);\n+}\n+\n+MemoryFileTracker::Instance::Locker::Locker() {\n+  MemoryFileTracker::Instance::_mutex->lock();\n+}\n+\n+MemoryFileTracker::Instance::Locker::~Locker() {\n+  MemoryFileTracker::Instance::_mutex->unlock();\n+}\n","filename":"src\/hotspot\/share\/nmt\/memoryFileTracker.cpp","additions":177,"deletions":0,"binary":false,"changes":177,"status":"added"},{"patch":"@@ -0,0 +1,115 @@\n+\/*\n+ * Copyright (c) 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef SHARE_NMT_MEMORYFILETRACKER_HPP\n+#define SHARE_NMT_MEMORYFILETRACKER_HPP\n+\n+#include \"memory\/allocation.hpp\"\n+#include \"nmt\/nmtCommon.hpp\"\n+#include \"nmt\/nmtNativeCallStackStorage.hpp\"\n+#include \"nmt\/virtualMemoryTracker.hpp\"\n+#include \"nmt\/vmatree.hpp\"\n+#include \"runtime\/mutex.hpp\"\n+#include \"runtime\/os.inline.hpp\"\n+#include \"utilities\/growableArray.hpp\"\n+#include \"utilities\/nativeCallStack.hpp\"\n+#include \"utilities\/ostream.hpp\"\n+\n+\/\/ The PhysicalDeviceTracker tracks memory of 'physical devices',\n+\/\/ storage with its own memory space separate from the process.\n+\/\/ A typical example of such a device is a memory mapped file.\n+class MemoryFileTracker {\n+  friend class MemoryFileTrackerTest;\n+\n+  \/\/ Provide caching of stacks.\n+  NativeCallStackStorage _stack_storage;\n+\n+public:\n+  class MemoryFile : public CHeapObj<mtNMT> {\n+    friend MemoryFileTracker;\n+    friend class MemoryFileTrackerTest;\n+    const char* _descriptive_name;\n+    VirtualMemorySnapshot _summary;\n+    VMATree _tree;\n+  public:\n+    NONCOPYABLE(MemoryFile);\n+    MemoryFile(const char* descriptive_name)\n+      : _descriptive_name(descriptive_name) {}\n+  };\n+\n+private:\n+  \/\/ We need pointers to each allocated device.\n+  GrowableArrayCHeap<MemoryFile*, mtNMT> _devices;\n+\n+public:\n+  MemoryFileTracker(bool is_detailed_mode);\n+\n+  void allocate_memory(MemoryFile* device, size_t offset, size_t size, const NativeCallStack& stack,\n+                       MEMFLAGS flag);\n+  void free_memory(MemoryFile* device, size_t offset, size_t size);\n+\n+  MemoryFile* make_device(const char* descriptive_name);\n+  void free_device(MemoryFile* device);\n+\n+  const VirtualMemorySnapshot& summary_for(const MemoryFile* device);\n+\n+  void summary_snapshot(VirtualMemorySnapshot* snapshot) const;\n+\n+  \/\/ Print detailed report of device\n+  void print_report_on(const MemoryFile* device, outputStream* stream, size_t scale);\n+\n+  const GrowableArrayCHeap<MemoryFile*, mtNMT>& devices();\n+\n+  class Instance : public AllStatic {\n+    static MemoryFileTracker* _tracker;\n+    static PlatformMutex* _mutex;\n+\n+  public:\n+    class Locker : public StackObj {\n+    public:\n+      Locker();\n+      ~Locker();\n+    };\n+\n+    static bool initialize(NMT_TrackingLevel tracking_level);\n+\n+    static MemoryFile* make_device(const char* descriptive_name);\n+    static void free_device(MemoryFile* device);\n+\n+    static void allocate_memory(MemoryFile* device, size_t offset, size_t size,\n+                                const NativeCallStack& stack, MEMFLAGS flag);\n+    static void free_memory(MemoryFile* device, size_t offset, size_t size);\n+\n+    static const VirtualMemorySnapshot& summary_for(const MemoryFile* device);\n+\n+    static void summary_snapshot(VirtualMemorySnapshot* snapshot);\n+\n+    static void print_report_on(const MemoryFile* device, outputStream* stream, size_t scale);\n+    static void print_all_reports_on(outputStream* stream, size_t scale);\n+\n+    static const GrowableArrayCHeap<MemoryFile*, mtNMT>& devices();\n+  };\n+};\n+\n+#endif \/\/ SHARE_NMT_MEMORYFILETRACKER_HPP\n","filename":"src\/hotspot\/share\/nmt\/memoryFileTracker.hpp","additions":115,"deletions":0,"binary":false,"changes":115,"status":"added"},{"patch":"@@ -0,0 +1,121 @@\n+\/*\n+ * Copyright (c) 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).    EXPECT_EQ(dev->_summary.by_type(mtTest)->reserved(), 0);\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef SHARE_NMT_NMTNATIVECALLSTACKSTORAGE_HPP\n+#define SHARE_NMT_NMTNATIVECALLSTACKSTORAGE_HPP\n+\n+#include \"memory\/allocation.hpp\"\n+#include \"utilities\/growableArray.hpp\"\n+#include \"utilities\/nativeCallStack.hpp\"\n+\n+\/\/ Virtual memory regions that are tracked by NMT also have their NativeCallStack (NCS) tracked.\n+\/\/ NCS:s are:\n+\/\/ - Fairly large\n+\/\/ - Regularly compared for equality\n+\/\/ - Read a lot when a detailed report is printed\n+\/\/ Therefore we'd like:\n+\/\/ - To not store duplicates\n+\/\/ - Have fast comparisons\n+\/\/ - Have constant time access\n+\/\/ We achieve this by using a closed hashtable for finding previously existing NCS:s and referring to them by an index that's smaller than a pointer.\n+class NativeCallStackStorage : public CHeapObj<mtNMT> {\n+public:\n+  struct StackIndex {\n+    friend NativeCallStackStorage;\n+\n+  private:\n+    int32_t _stack_index;\n+    StackIndex(int32_t stack_index)\n+      : _stack_index(stack_index) {\n+    }\n+\n+  public:\n+    static bool equals(const StackIndex& a, const StackIndex& b) {\n+      return a._stack_index == b._stack_index;\n+    }\n+    StackIndex()\n+      : _stack_index(-1) {\n+    }\n+  };\n+\n+private:\n+  struct Link : public CHeapObj<mtNMT> {\n+    Link* next;\n+    StackIndex stack;\n+    Link(Link* next, StackIndex v)\n+      : next(next),\n+        stack(v) {\n+    }\n+  };\n+  StackIndex put(const NativeCallStack& value) {\n+    int bucket = value.calculate_hash() % _nr_buckets;\n+    Link* link = _buckets[bucket];\n+    while (link != nullptr) {\n+      if (value.equals(get(link->stack))) {\n+        return link->stack;\n+      }\n+      link = link->next;\n+    }\n+    int idx = _stacks.append(value);\n+    Link* new_link = new Link(_buckets[bucket], StackIndex(idx));\n+    _buckets[bucket] = new_link;\n+    return new_link->stack;\n+  }\n+\n+  \/\/ Pick a prime number of buckets.\n+  \/\/ 4099 gives a 50% probability of collisions at 76 stacks (as per birthday problem).    os::\n+  static const constexpr int _nr_buckets = 4099;\n+  Link** _buckets;\n+  GrowableArrayCHeap<NativeCallStack, mtNMT> _stacks;\n+  bool _is_detailed_mode;\n+public:\n+\n+  StackIndex push(const NativeCallStack& stack) {\n+    \/\/ Not in detailed mode, so not tracking stacks.\n+    if (!_is_detailed_mode) {\n+      return StackIndex();\n+    }\n+    return put(stack);\n+  }\n+\n+  const inline NativeCallStack& get(StackIndex si) {\n+    return _stacks.at(si._stack_index);\n+  }\n+\n+  NativeCallStackStorage(bool is_detailed_mode)\n+  :  _buckets(nullptr), _stacks(), _is_detailed_mode(is_detailed_mode) {\n+    if (_is_detailed_mode) {\n+      _buckets = NEW_C_HEAP_ARRAY(Link*, _nr_buckets, mtNMT);\n+      for (int i = 0; i < _nr_buckets; i++) {\n+        _buckets[i] = nullptr;\n+      }\n+    }\n+  }\n+\n+  ~NativeCallStackStorage() {\n+    FREE_C_HEAP_ARRAY(Link*, _buckets);\n+  }\n+};\n+\n+#endif \/\/ SHARE_NMT_NMTNATIVECALLSTACKSTORAGE_HPP\n","filename":"src\/hotspot\/share\/nmt\/nmtNativeCallStackStorage.hpp","additions":121,"deletions":0,"binary":false,"changes":121,"status":"added"},{"patch":"@@ -0,0 +1,367 @@\n+\/*\n+ * Copyright (c) 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef SHARE_NMT_NMTTREAP_HPP\n+#define SHARE_NMT_NMTTREAP_HPP\n+\n+#include \"memory\/allocation.hpp\"\n+#include \"runtime\/os.hpp\"\n+#include \"utilities\/debug.hpp\"\n+#include \"utilities\/globalDefinitions.hpp\"\n+#include \"utilities\/growableArray.hpp\"\n+#include \"utilities\/macros.hpp\"\n+#include <stdint.h>\n+\n+\/\/ A Treap is a self-balanced binary tree where each node is equipped with a\n+\/\/ priority. It adds the invariant that the priority of a parent P is strictly larger\n+\/\/ larger than the priority of its children. When priorities are randomly\n+\/\/ assigned the tree is balanced.\n+\/\/ All operations are defined through merge and split, which are each other's inverse.\n+\/\/ merge(left_treap, right_treap) => treap where left_treap <= right_treap\n+\/\/ split(treap, key) => (left_treap, right_treap)  where left_treap <= right_treap\n+\/\/ Recursion is used in these, but the depth of the call stack is the depth of\n+\/\/ the tree which is O(log n) so we are safe from stack overflow.\n+\/\/ TreapNode has LEQ nodes on the left, GT nodes on the right.\n+\/\/\n+\/\/ COMPARATOR must have a static function `cmp(a,b)` which returns:\n+\/\/     - an int < 0 when a < b\n+\/\/     - an int == 0 when a == b\n+\/\/     - an int > 0 when a > b\n+\/\/ ALLOCATOR must check for oom and exit, as Treap currently does not handle the allocation\n+\/\/ failing.\n+\n+template<typename K, typename V, typename COMPARATOR, typename ALLOCATOR>\n+class Treap {\n+  friend class VMATreeTest;\n+  friend class TreapTest;\n+public:\n+  class TreapNode {\n+    friend Treap;\n+    uint64_t _priority;\n+    const K _key;\n+    V _value;\n+\n+    TreapNode* _left;\n+    TreapNode* _right;\n+\n+  public:\n+    TreapNode(const K& k, const V& v, uint64_t p)\n+      : _priority(p),\n+        _key(k),\n+        _value(v),\n+        _left(nullptr),\n+        _right(nullptr) {\n+    }\n+\n+    const K& key() const { return _key; }\n+    V& val() { return _value; }\n+\n+    TreapNode* left() const { return _left; }\n+    TreapNode* right() const { return _right; }\n+  };\n+\n+private:\n+  ALLOCATOR _allocator;\n+  TreapNode* _root;\n+  uint64_t _prng_seed;\n+  int _node_count;\n+\n+  uint64_t prng_next() {\n+    \/\/ Taken directly off of JFRPrng\n+    static const constexpr uint64_t PrngMult = 0x5DEECE66DLL;\n+    static const constexpr uint64_t PrngAdd = 0xB;\n+    static const constexpr uint64_t PrngModPower = 48;\n+    static const constexpr uint64_t PrngModMask = (static_cast<uint64_t>(1) << PrngModPower) - 1;\n+    _prng_seed = (PrngMult * _prng_seed + PrngAdd) & PrngModMask;\n+    return _prng_seed;\n+  }\n+\n+  struct node_pair {\n+    TreapNode* left;\n+    TreapNode* right;\n+  };\n+\n+  enum SplitMode {\n+    LT, \/\/ <\n+    LEQ \/\/ <=\n+  };\n+\n+  \/\/ Split tree at head into two trees, SplitMode decides where EQ values go.\n+  \/\/ We have SplitMode because it makes remove() trivial to implement.\n+  static node_pair split(TreapNode* head, const K& key, SplitMode mode = LEQ DEBUG_ONLY(COMMA int recur_count = 0)) {\n+    assert(recur_count < 200, \"Call-stack depth should never exceed 200\");\n+\n+    if (head == nullptr) {\n+      return {nullptr, nullptr};\n+    }\n+    if ((COMPARATOR::cmp(head->_key, key) <= 0 && mode == LEQ) || (COMPARATOR::cmp(head->_key, key) < 0 && mode == LT)) {\n+      node_pair p = split(head->_right, key, mode DEBUG_ONLY(COMMA recur_count + 1));\n+      head->_right = p.left;\n+      return node_pair{head, p.right};\n+    } else {\n+      node_pair p = split(head->_left, key, mode DEBUG_ONLY(COMMA recur_count + 1));\n+      head->_left = p.right;\n+      return node_pair{p.left, head};\n+    }\n+  }\n+\n+  \/\/ Invariant: left is a treap whose keys are LEQ to the keys in right.\n+  static TreapNode* merge(TreapNode* left, TreapNode* right DEBUG_ONLY(COMMA int recur_count = 0)) {\n+    assert(recur_count < 200, \"Call-stack depth should never exceed 200\");\n+\n+    if (left == nullptr) return right;\n+    if (right == nullptr) return left;\n+\n+    if (left->_priority > right->_priority) {\n+      \/\/ We need\n+      \/\/      LEFT\n+      \/\/         |\n+      \/\/         RIGHT\n+      \/\/ for the invariant re: priorities to hold.\n+      left->_right = merge(left->_right, right DEBUG_ONLY(COMMA recur_count + 1));\n+      return left;\n+    } else {\n+      \/\/ We need\n+      \/\/         RIGHT\n+      \/\/         |\n+      \/\/      LEFT\n+      \/\/ for the invariant re: priorities to hold.\n+      right->_left = merge(left, right->_left DEBUG_ONLY(COMMA recur_count + 1));\n+      return right;\n+    }\n+  }\n+\n+  static TreapNode* find(TreapNode* node, const K& k DEBUG_ONLY(COMMA int recur_count = 0)) {\n+    if (node == nullptr) {\n+      return nullptr;\n+    }\n+\n+    int key_cmp_k = COMPARATOR::cmp(node->key(), k);\n+\n+    if (key_cmp_k == 0) { \/\/ key EQ k\n+      return node;\n+    }\n+\n+    if (key_cmp_k < 0) { \/\/ key LT k\n+      return find(node->right(), k DEBUG_ONLY(COMMA recur_count + 1));\n+    } else { \/\/ key GT k\n+      return find(node->left(), k DEBUG_ONLY(COMMA recur_count + 1));\n+    }\n+  }\n+\n+#ifdef ASSERT\n+  void verify_self() {\n+    const double expected_maximum_depth = log(this->_node_count+1) * 5;\n+    \/\/ Find the maximum depth through DFS and ensure that the priority invariant holds.\n+    int maximum_depth_found = 0;\n+\n+    struct DFS {\n+      int depth;\n+      uint64_t parent_prio;\n+      TreapNode* n;\n+    };\n+    GrowableArrayCHeap<DFS, mtNMT> to_visit;\n+    constexpr const uint64_t positive_infinity = 0xFFFFFFFFFFFFFFFF;\n+\n+    to_visit.push({0, positive_infinity, this->_root});\n+    while (!to_visit.is_empty()) {\n+      DFS head = to_visit.pop();\n+      if (head.n == nullptr) continue;\n+      maximum_depth_found = MAX2(maximum_depth_found, head.depth);\n+\n+      assert(head.parent_prio >= head.n->_priority, \"broken priority invariant\");\n+\n+      to_visit.push({head.depth + 1, head.n->_priority, head.n->left()});\n+      to_visit.push({head.depth + 1, head.n->_priority, head.n->right()});\n+    }\n+    assert(maximum_depth_found <= (int)expected_maximum_depth, \"depth unexpectedly large\");\n+\n+    \/\/ Visit everything in order, see that the key ordering is monotonically increasing.\n+    TreapNode* last_seen = nullptr;\n+    bool failed = false;\n+    int seen_count = 0;\n+    this->visit_in_order([&](TreapNode* node) {\n+      seen_count++;\n+      if (last_seen == nullptr) {\n+        last_seen = node;\n+        return;\n+      }\n+      if (COMPARATOR::cmp(last_seen->key(), node->key()) > 0) {\n+        failed = false;\n+      }\n+      last_seen = node;\n+    });\n+    assert(seen_count == _node_count, \"the number of visited nodes do not match with the number of stored nodes\");\n+    assert(!failed, \"keys was not monotonically strongly increasing when visiting in order\");\n+  }\n+#endif \/\/ ASSERT\n+\n+public:\n+  Treap(uint64_t seed = static_cast<uint64_t>(os::random()))\n+  : _allocator(),\n+    _root(nullptr),\n+    _prng_seed(seed),\n+    _node_count(0) {}\n+\n+  ~Treap() {\n+    this->remove_all();\n+  }\n+\n+  void upsert(const K& k, const V& v) {\n+    TreapNode* found = find(_root, k);\n+    if (found != nullptr) {\n+      \/\/ Already exists, update value.\n+      found->_value = v;\n+      return;\n+    }\n+    _node_count++;\n+    \/\/ Doesn't exist, make node\n+    void* node_place = _allocator.allocate(sizeof(TreapNode));\n+    uint64_t prio = prng_next();\n+    TreapNode* node = new (node_place) TreapNode(k, v, prio);\n+\n+    \/\/ (LEQ_k, GT_k)\n+    node_pair split_up = split(this->_root, k);\n+    \/\/ merge(merge(LEQ_k, EQ_k), GT_k)\n+    this->_root = merge(merge(split_up.left, node), split_up.right);\n+  }\n+\n+  void remove(const K& k) {\n+    \/\/ (LEQ_k, GT_k)\n+    node_pair first_split = split(this->_root, k, LEQ);\n+    \/\/ (LT_k, GEQ_k) == (LT_k, EQ_k) since it's from LEQ_k and keys are unique.\n+    node_pair second_split = split(first_split.left, k, LT);\n+\n+    if (second_split.right != nullptr) {\n+      \/\/ The key k existed, we delete it.\n+      _node_count--;\n+      _allocator.free(second_split.right);\n+    }\n+    \/\/ Merge together everything\n+    _root = merge(second_split.left, first_split.right);\n+  }\n+\n+  \/\/ Delete all nodes.\n+  void remove_all() {\n+    _node_count = 0;\n+    GrowableArrayCHeap<TreapNode*, mtNMT> to_delete;\n+    to_delete.push(_root);\n+\n+    while (!to_delete.is_empty()) {\n+      TreapNode* head = to_delete.pop();\n+      if (head == nullptr) continue;\n+      to_delete.push(head->_left);\n+      to_delete.push(head->_right);\n+      _allocator.free(head);\n+    }\n+    _root = nullptr;\n+  }\n+\n+  TreapNode* closest_leq(const K& key) {\n+    TreapNode* candidate = nullptr;\n+    TreapNode* pos = _root;\n+    while (pos != nullptr) {\n+      int cmp_r = COMPARATOR::cmp(pos->key(), key);\n+      if (cmp_r == 0) { \/\/ Exact match\n+        candidate = pos;\n+        break; \/\/ Can't become better than that.\n+      }\n+      if (cmp_r < 0) {\n+        \/\/ Found a match, try to find a better one.\n+        candidate = pos;\n+        pos = pos->_right;\n+      } else if (cmp_r > 0) {\n+        pos = pos->_left;\n+      }\n+    }\n+    return candidate;\n+  }\n+\n+  \/\/ Visit all TreapNodes in ascending key order.\n+  template<typename F>\n+  void visit_in_order(F f) const {\n+    GrowableArrayCHeap<TreapNode*, mtNMT> to_visit;\n+    TreapNode* head = _root;\n+    while (!to_visit.is_empty() || head != nullptr) {\n+      while (head != nullptr) {\n+        to_visit.push(head);\n+        head = head->left();\n+      }\n+      head = to_visit.pop();\n+      f(head);\n+      head = head->right();\n+    }\n+  }\n+\n+  \/\/ Visit all TreapNodes in ascending order whose keys are in range [from, to).\n+  template<typename F>\n+  void visit_range_in_order(const K& from, const K& to, F f) {\n+    GrowableArrayCHeap<TreapNode*, mtNMT> to_visit;\n+    TreapNode* head = _root;\n+    while (!to_visit.is_empty() || head != nullptr) {\n+      while (head != nullptr) {\n+        int cmp_from = COMPARATOR::cmp(head->key(), from);\n+        to_visit.push(head);\n+        if (cmp_from >= 0) {\n+          head = head->left();\n+        } else {\n+          \/\/ We've reached a node which is strictly less than from\n+          \/\/ We don't need to visit any further to the left.\n+          break;\n+        }\n+      }\n+      head = to_visit.pop();\n+      const int cmp_from = COMPARATOR::cmp(head->key(), from);\n+      const int cmp_to = COMPARATOR::cmp(head->key(), to);\n+      if (cmp_from >= 0 && cmp_to < 0) {\n+        f(head);\n+      }\n+      if (cmp_to < 0) {\n+        head = head->right();\n+      } else {\n+        head = nullptr;\n+      }\n+    }\n+  }\n+};\n+\n+class TreapCHeapAllocator {\n+public:\n+  void* allocate(size_t sz) {\n+    void* allocation = os::malloc(sz, mtNMT);\n+    if (allocation == nullptr) {\n+      vm_exit_out_of_memory(sz, OOM_MALLOC_ERROR, \"treap failed allocation\");\n+    }\n+    return allocation;\n+  }\n+\n+  void free(void* ptr) {\n+    os::free(ptr);\n+  }\n+};\n+\n+template<typename K, typename V, typename COMPARATOR>\n+using TreapCHeap = Treap<K, V, COMPARATOR, TreapCHeapAllocator>;\n+\n+#endif \/\/SHARE_NMT_NMTTREAP_HPP\n","filename":"src\/hotspot\/share\/nmt\/nmtTreap.hpp","additions":367,"deletions":0,"binary":false,"changes":367,"status":"added"},{"patch":"@@ -33,0 +33,1 @@\n+#include \"runtime\/atomic.hpp\"\n","filename":"src\/hotspot\/share\/nmt\/virtualMemoryTracker.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -0,0 +1,209 @@\n+\/*\n+ * Copyright (c) 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2024, Red Hat, Inc. and\/or its affiliates.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, writ constexpre to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"nmt\/vmatree.hpp\"\n+#include \"utilities\/growableArray.hpp\"\n+\n+const VMATree::RegionData VMATree::empty_regiondata{NativeCallStackStorage::StackIndex{}, mtNone};\n+\n+VMATree::SummaryDiff VMATree::register_mapping(position A, position B, StateType state,\n+                                               const RegionData& metadata) {\n+  if (A == B) {\n+    \/\/ A 0-sized mapping isn't worth recording.\n+    return SummaryDiff();\n+  }\n+\n+  \/\/ AddressState saves the necessary information for performing online summary accounting.\n+  struct AddressState {\n+    position address;\n+    IntervalChange state;\n+\n+    const IntervalState& out() const {\n+      return state.out;\n+    }\n+\n+    const IntervalState& in() const {\n+      return state.in;\n+    }\n+  };\n+\n+  IntervalChange stA{\n+      IntervalState{StateType::Released, empty_regiondata},\n+      IntervalState{              state,   metadata}\n+  };\n+  IntervalChange stB{\n+      IntervalState{              state,   metadata},\n+      IntervalState{StateType::Released, empty_regiondata}\n+  };\n+\n+  \/\/ First handle A.\n+  \/\/ Find closest node that is LEQ A\n+  bool LEQ_A_found = false;\n+  AddressState LEQ_A;\n+  TreapNode* leqA_n = _tree.closest_leq(A);\n+  if (leqA_n == nullptr) {\n+    \/\/ No match. We add the A node directly, unless it would have no effect.\n+    if (!stA.is_noop()) {\n+      _tree.upsert(A, stA);\n+    }\n+  } else {\n+    LEQ_A_found = true;\n+    LEQ_A = AddressState{leqA_n->key(), leqA_n->val()};\n+    \/\/ Unless we know better, let B's outgoing state be the outgoing state of the node at or preceding A.\n+    \/\/ Consider the case where the found node is the start of a region enclosing [A,B)\n+    stB.out = leqA_n->val().out;\n+\n+    \/\/ Direct address match.\n+    if (leqA_n->key() == A) {\n+      \/\/ Take over in state from old address.\n+      stA.in = leqA_n->val().in;\n+\n+      \/\/ We may now be able to merge two regions:\n+      \/\/ If the node's old state matches the new, it becomes a noop. That happens, for example,\n+      \/\/ when expanding a committed area: commit [x1, A); ... commit [A, x3)\n+      \/\/ and the result should be a larger area, [x1, x3). In that case, the middle node (A and le_n)\n+      \/\/ is not needed anymore. So we just remove the old node.\n+      stB.in = stA.out;\n+      if (stA.is_noop()) {\n+        \/\/ invalidates leqA_n\n+        _tree.remove(leqA_n->key());\n+      } else {\n+        \/\/ If the state is not matching then we have different operations, such as:\n+        \/\/ reserve [x1, A); ... commit [A, x2); or\n+        \/\/ reserve [x1, A), flag1; ... reserve [A, x2), flag2; or\n+        \/\/ reserve [A, x1), flag1; ... reserve [A, x2), flag2;\n+        \/\/ then we re-use the existing out node, overwriting its old metadata.\n+        leqA_n->val() = stA;\n+      }\n+    } else {\n+      \/\/ The address must be smaller.\n+      assert(A > leqA_n->key(), \"must be\");\n+\n+      \/\/ We add a new node, but only if there would be a state change. If there would not be a\n+      \/\/ state change, we just omit the node.\n+      \/\/ That happens, for example, when reserving within an already reserved region with identical metadata.\n+      stA.in = leqA_n->val().out; \/\/ .. and the region's prior state is the incoming state\n+      if (stA.is_noop()) {\n+        \/\/ Nothing to do.\n+      } else {\n+        \/\/ Add new node.\n+        _tree.upsert(A, stA);\n+      }\n+    }\n+  }\n+\n+  \/\/ Now we handle B.\n+  \/\/ We first search all nodes that are (A, B]. All of these nodes\n+  \/\/ need to be deleted and summary accounted for. The last node before B determines B's outgoing state.\n+  \/\/ If there is no node between A and B, its A's incoming state.\n+  GrowableArrayCHeap<AddressState, mtNMT> to_be_deleted_inbetween_a_b;\n+  bool B_needs_insert = true;\n+\n+  \/\/ Find all nodes between (A, B] and record their addresses and values. Also update B's\n+  \/\/ outgoing state.\n+  _tree.visit_range_in_order(A + 1, B + 1, [&](TreapNode* head) {\n+    int cmp_B = AddressComparator::cmp(head->key(), B);\n+    stB.out = head->val().out;\n+    if (cmp_B < 0) {\n+      \/\/ Record all nodes preceding B.\n+      to_be_deleted_inbetween_a_b.push({head->key(), head->val()});\n+    } else if (cmp_B == 0) {\n+      \/\/ Re-purpose B node, unless it would result in a noop node, in\n+      \/\/ which case record old node at B for deletion and summary accounting.\n+      if (stB.is_noop()) {\n+        to_be_deleted_inbetween_a_b.push(AddressState{B, head->val()});\n+      } else {\n+        head->val() = stB;\n+      }\n+      B_needs_insert = false;\n+    }\n+  });\n+\n+  \/\/ Insert B node if needed\n+  if (B_needs_insert && \/\/ Was not already inserted\n+      !stB.is_noop())   \/\/ The operation is differing\n+    {\n+    _tree.upsert(B, stB);\n+  }\n+\n+  \/\/ We now need to:\n+  \/\/ a) Delete all nodes between (A, B]. Including B in the case of a noop.\n+  \/\/ b) Perform summary accounting\n+  SummaryDiff diff;\n+\n+  if (to_be_deleted_inbetween_a_b.length() == 0 && LEQ_A_found) {\n+    \/\/ We must have smashed a hole in an existing region (or replaced it entirely).\n+    \/\/ LEQ_A < A < B <= C\n+    SingleDiff& rescom = diff.flag[NMTUtil::flag_to_index(LEQ_A.out().flag())];\n+    if (LEQ_A.out().type() == StateType::Reserved) {\n+      rescom.reserve -= B - A;\n+    } else if (LEQ_A.out().type() == StateType::Committed) {\n+      rescom.commit -= B - A;\n+      rescom.reserve -= B - A;\n+    }\n+  }\n+\n+  \/\/ Track the previous node.\n+  AddressState prev{A, stA};\n+  for (int i = 0; i < to_be_deleted_inbetween_a_b.length(); i++) {\n+    const AddressState delete_me = to_be_deleted_inbetween_a_b.at(i);\n+    _tree.remove(delete_me.address);\n+\n+    \/\/ Perform summary accounting\n+    SingleDiff& rescom = diff.flag[NMTUtil::flag_to_index(delete_me.in().flag())];\n+    if (delete_me.in().type() == StateType::Reserved) {\n+      rescom.reserve -= delete_me.address - prev.address;\n+    } else if (delete_me.in().type() == StateType::Committed) {\n+      rescom.commit -= delete_me.address - prev.address;\n+      rescom.reserve -= delete_me.address - prev.address;\n+    }\n+    prev = delete_me;\n+  }\n+\n+  if (prev.address != A && prev.out().type() != StateType::Released) {\n+    \/\/ The last node wasn't released, so it must be connected to a node outside of (A, B)\n+    \/\/ A - prev - B - (some node >= B)\n+    \/\/ It might be that prev.address == B == (some node >= B), this is fine.\n+    if (prev.out().type() == StateType::Reserved) {\n+      SingleDiff& rescom = diff.flag[NMTUtil::flag_to_index(prev.out().flag())];\n+      rescom.reserve -= B - prev.address;\n+    } else if (prev.out().type() == StateType::Committed) {\n+      SingleDiff& rescom = diff.flag[NMTUtil::flag_to_index(prev.out().flag())];\n+      rescom.commit -= B - prev.address;\n+      rescom.reserve -= B - prev.address;\n+    }\n+  }\n+\n+  \/\/ Finally, we can register the new region [A, B)'s summary data.\n+  SingleDiff& rescom = diff.flag[NMTUtil::flag_to_index(metadata.flag)];\n+  if (state == StateType::Reserved) {\n+    rescom.reserve += B - A;\n+  } else if (state == StateType::Committed) {\n+    rescom.commit += B - A;\n+    rescom.reserve += B - A;\n+  }\n+  return diff;\n+}\n","filename":"src\/hotspot\/share\/nmt\/vmatree.cpp","additions":209,"deletions":0,"binary":false,"changes":209,"status":"added"},{"patch":"@@ -0,0 +1,164 @@\n+\/*\n+ * Copyright (c) 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2024, Red Hat Inc. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef SHARE_NMT_VMATREE_HPP\n+#define SHARE_NMT_VMATREE_HPP\n+\n+#include \"nmt\/nmtNativeCallStackStorage.hpp\"\n+#include \"nmt\/nmtTreap.hpp\"\n+#include \"runtime\/os.hpp\"\n+#include \"utilities\/globalDefinitions.hpp\"\n+\n+\/\/ A VMATree stores a sequence of points on the natural number line.\n+\/\/ Each of these points stores information about a state change.\n+\/\/ For example, the state may go from released memory to committed memory,\n+\/\/ or from committed memory of a certain MEMFLAGS to committed memory of a different MEMFLAGS.\n+\/\/ The set of points is stored in a balanced binary tree for efficient querying and updating.\n+class VMATree {\n+  friend class VMATreeTest;\n+  \/\/ A position in memory.\n+  using position = size_t;\n+\n+  class AddressComparator {\n+  public:\n+    static int cmp(position a, position b) {\n+      if (a < b) return -1;\n+      if (a == b) return 0;\n+      if (a > b) return 1;\n+      ShouldNotReachHere();\n+    }\n+  };\n+\n+public:\n+  enum class StateType : uint8_t { Reserved, Committed, Released };\n+\n+  \/\/ Each point has some stack and a flag associated with it.\n+  struct RegionData {\n+    const NativeCallStackStorage::StackIndex stack_idx;\n+    const MEMFLAGS flag;\n+\n+    RegionData() : stack_idx(), flag(mtNone) {}\n+\n+    RegionData(NativeCallStackStorage::StackIndex stack_idx, MEMFLAGS flag)\n+    : stack_idx(stack_idx), flag(flag) {}\n+\n+    static bool equals(const RegionData& a, const RegionData& b) {\n+      return a.flag == b.flag &&\n+             NativeCallStackStorage::StackIndex::equals(a.stack_idx, b.stack_idx);\n+    }\n+  };\n+\n+  static const RegionData empty_regiondata;\n+\n+private:\n+  struct IntervalState {\n+  private:\n+    \/\/ Store the type and flag as two bytes\n+    uint8_t type_flag[2];\n+    NativeCallStackStorage::StackIndex sidx;\n+\n+  public:\n+    IntervalState() : type_flag{0,0}, sidx() {}\n+    IntervalState(const StateType type, const RegionData data) {\n+      assert(!(type == StateType::Released) || data.flag == mtNone, \"Released type must have flag mtNone\");\n+      type_flag[0] = static_cast<uint8_t>(type);\n+      type_flag[1] = static_cast<uint8_t>(data.flag);\n+      sidx = data.stack_idx;\n+    }\n+\n+    StateType type() const {\n+      return static_cast<StateType>(type_flag[0]);\n+    }\n+\n+    MEMFLAGS flag() const {\n+      return static_cast<MEMFLAGS>(type_flag[1]);\n+    }\n+\n+    RegionData regiondata() const {\n+      return RegionData{sidx, flag()};\n+    }\n+\n+    const NativeCallStackStorage::StackIndex stack() const {\n+     return sidx;\n+    }\n+  };\n+\n+  \/\/ An IntervalChange indicates a change in state between two intervals. The incoming state\n+  \/\/ is denoted by in, and the outgoing state is denoted by out.\n+  struct IntervalChange {\n+    IntervalState in;\n+    IntervalState out;\n+\n+    bool is_noop() {\n+      return (in.type() == StateType::Released && out.type() == StateType::Released) ||\n+             (in.type() == out.type() && RegionData::equals(in.regiondata(), out.regiondata()));\n+    }\n+  };\n+\n+public:\n+  using VMATreap = TreapCHeap<position, IntervalChange, AddressComparator>;\n+  using TreapNode = VMATreap::TreapNode;\n+\n+private:\n+  VMATreap _tree;\n+\n+public:\n+  VMATree() : _tree() {}\n+\n+  struct SingleDiff {\n+    int64_t reserve;\n+    int64_t commit;\n+  };\n+  struct SummaryDiff {\n+    SingleDiff flag[mt_number_of_types];\n+    SummaryDiff() {\n+      for (int i = 0; i < mt_number_of_types; i++) {\n+        flag[i] = SingleDiff{0, 0};\n+      }\n+    }\n+  };\n+\n+  SummaryDiff register_mapping(position A, position B, StateType state, const RegionData& metadata);\n+\n+  SummaryDiff reserve_mapping(position from, position sz, const RegionData& metadata) {\n+    return register_mapping(from, from + sz, StateType::Reserved, metadata);\n+  }\n+\n+  SummaryDiff commit_mapping(position from, position sz, const RegionData& metadata) {\n+    return register_mapping(from, from + sz, StateType::Committed, metadata);\n+  }\n+\n+  SummaryDiff release_mapping(position from, position sz) {\n+    return register_mapping(from, from + sz, StateType::Released, VMATree::empty_regiondata);\n+  }\n+\n+public:\n+  template<typename F>\n+  void visit_in_order(F f) const {\n+    _tree.visit_in_order(f);\n+  }\n+};\n+\n+#endif\n","filename":"src\/hotspot\/share\/nmt\/vmatree.hpp","additions":164,"deletions":0,"binary":false,"changes":164,"status":"added"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2014, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2014, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -57,0 +57,1 @@\n+  friend class VMATreeTest;\n","filename":"src\/hotspot\/share\/utilities\/nativeCallStack.hpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -0,0 +1,53 @@\n+\/*\n+ * Copyright (c) 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"memory\/allocation.hpp\"\n+#include \"nmt\/memTracker.hpp\"\n+#include \"unittest.hpp\"\n+\n+class MemoryFileTrackerTest : public testing::Test {\n+public:\n+  size_t sz(int x) { return (size_t) x; }\n+  void basics() {\n+    MemoryFileTracker tracker(false);\n+    MemoryFileTracker::MemoryFile* dev = tracker.make_device(\"test\");\n+    tracker.allocate_memory(dev, 0, 100, CALLER_PC, mtTest);\n+    EXPECT_EQ(dev->_summary.by_type(mtTest)->reserved(), sz(100));\n+    tracker.allocate_memory(dev, 100, 100, CALLER_PC, mtTest);\n+    EXPECT_EQ(dev->_summary.by_type(mtTest)->reserved(), sz(200));\n+    tracker.allocate_memory(dev, 200, 100, CALLER_PC, mtTest);\n+    EXPECT_EQ(dev->_summary.by_type(mtTest)->reserved(), sz(300));\n+    tracker.free_memory(dev, 0, 300);\n+    EXPECT_EQ(dev->_summary.by_type(mtTest)->reserved(), sz(0));\n+    tracker.allocate_memory(dev, 0, 100, CALLER_PC, mtTest);\n+    EXPECT_EQ(dev->_summary.by_type(mtTest)->reserved(), sz(100));\n+    tracker.free_memory(dev, 50, 10);\n+    EXPECT_EQ(dev->_summary.by_type(mtTest)->reserved(), sz(90));\n+  };\n+};\n+\n+TEST_VM_F(MemoryFileTrackerTest, Basics) {\n+  this->basics();\n+}\n","filename":"test\/hotspot\/gtest\/nmt\/test_nmt_memoryfiletracker.cpp","additions":53,"deletions":0,"binary":false,"changes":53,"status":"added"},{"patch":"@@ -0,0 +1,258 @@\n+\/*\n+ * Copyright (c) 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"memory\/resourceArea.hpp\"\n+#include \"nmt\/nmtTreap.hpp\"\n+#include \"runtime\/os.hpp\"\n+#include \"unittest.hpp\"\n+\n+class TreapTest : public testing::Test {\n+public:\n+  struct Cmp {\n+    static int cmp(int a, int b) {\n+      return a - b;\n+    }\n+  };\n+\n+  struct CmpInverse {\n+    static int cmp(int a, int b) {\n+      return b - a;\n+    }\n+  };\n+\n+#ifdef ASSERT\n+  template<typename K, typename V, typename CMP, typename ALLOC>\n+  void verify_it(Treap<K, V, CMP, ALLOC>& t) {\n+    t.verify_self();\n+  }\n+#endif \/\/ ASSERT\n+\n+public:\n+  void inserting_duplicates_results_in_one_value() {\n+    constexpr const int up_to = 10;\n+    GrowableArrayCHeap<int, mtTest> nums_seen(up_to, up_to, 0);\n+    TreapCHeap<int, int, Cmp> treap;\n+\n+    for (int i = 0; i < up_to; i++) {\n+      treap.upsert(i, i);\n+      treap.upsert(i, i);\n+      treap.upsert(i, i);\n+      treap.upsert(i, i);\n+      treap.upsert(i, i);\n+    }\n+\n+    treap.visit_in_order([&](TreapCHeap<int, int, Cmp>::TreapNode* node) {\n+      nums_seen.at(node->key())++;\n+    });\n+    for (int i = 0; i < up_to; i++) {\n+      EXPECT_EQ(1, nums_seen.at(i));\n+    }\n+  }\n+\n+  void treap_ought_not_leak() {\n+    struct LeakCheckedAllocator {\n+      struct Check {\n+        void* ptr;\n+        bool released;\n+\n+        Check(void* ptr)\n+          : ptr(ptr),\n+            released(false) {\n+        }\n+\n+        Check()\n+          : ptr(nullptr),\n+            released(false) {}\n+\n+        void release() {\n+          released = true;\n+        }\n+      };\n+      GrowableArrayCHeap<Check, mtTest> allocations;\n+\n+      LeakCheckedAllocator()\n+        : allocations() {\n+      }\n+\n+      void* allocate(size_t sz) {\n+        void* allocation = os::malloc(sz, mtTest);\n+        if (allocation == nullptr) {\n+          vm_exit_out_of_memory(sz, OOM_MALLOC_ERROR, \"treap failed allocation\");\n+        }\n+        allocations.push(Check(allocation));\n+        return allocation;\n+      }\n+\n+      void free(void* ptr) {\n+        for (int i = 0; i < allocations.length(); i++) {\n+          Check& c = allocations.at(i);\n+          EXPECT_NE(nullptr, c.ptr);\n+          if (c.ptr == ptr) {\n+            c.release();\n+          }\n+        }\n+        os::free(ptr);\n+      }\n+    };\n+\n+    constexpr const int up_to = 10;\n+    {\n+      Treap<int, int, Cmp, LeakCheckedAllocator> treap;\n+      for (int i = 0; i < 10; i++) {\n+        treap.upsert(i, i);\n+      }\n+      for (int i = 0; i < 10; i++) {\n+        treap.remove(i);\n+      }\n+      EXPECT_EQ(10, treap._allocator.allocations.length());\n+      for (int i = 0; i < 10; i++) {\n+        EXPECT_TRUE(treap._allocator.allocations.at(i).released);\n+      }\n+      EXPECT_EQ(nullptr, treap._root);\n+    }\n+\n+    {\n+      Treap<int, int, Cmp, LeakCheckedAllocator> treap;\n+      for (int i = 0; i < 10; i++) {\n+        treap.upsert(i, i);\n+      }\n+      treap.remove_all();\n+      EXPECT_EQ(10, treap._allocator.allocations.length());\n+      for (int i = 0; i < 10; i++) {\n+        EXPECT_TRUE(treap._allocator.allocations.at(i).released);\n+      }\n+      EXPECT_EQ(nullptr, treap._root);\n+    }\n+  }\n+};\n+\n+TEST_VM_F(TreapTest, InsertingDuplicatesResultsInOneValue) {\n+  this->inserting_duplicates_results_in_one_value();\n+}\n+\n+TEST_VM_F(TreapTest, TreapOughtNotLeak) {\n+  this->treap_ought_not_leak();\n+}\n+\n+TEST_VM_F(TreapTest, TestVisitInRange) {\n+  { \/\/ Tests with 'default' ordering (ascending)\n+    TreapCHeap<int, int, Cmp> treap;\n+    using Node = TreapCHeap<int, int, Cmp>::TreapNode;\n+\n+    treap.visit_range_in_order(0, 100, [&](Node* x) {\n+      EXPECT_TRUE(false) << \"Empty treap has no nodes to visit\";\n+    });\n+\n+    \/\/ Single-element set\n+    treap.upsert(1, 0);\n+    int count = 0;\n+    treap.visit_range_in_order(0, 100, [&](Node* x) {\n+      count++;\n+    });\n+    EXPECT_EQ(1, count);\n+\n+    \/\/ Add an element outside of the range that should not be visited on the right side and\n+    \/\/ one on the left side.\n+    treap.upsert(101, 0);\n+    treap.upsert(-1, 0);\n+    count = 0;\n+    treap.visit_range_in_order(0, 100, [&](Node* x) {\n+      count++;\n+    });\n+    EXPECT_EQ(1, count);\n+\n+    \/\/ Visiting empty range [0, 0) == {}\n+    treap.upsert(0, 0); \/\/ This node should not be visited.\n+    treap.visit_range_in_order(0, 0, [&](Node* x) {\n+      EXPECT_TRUE(false) << \"Empty visiting range should not visit any node\";\n+    });\n+\n+    treap.remove_all();\n+    for (int i = 0; i < 11; i++) {\n+      treap.upsert(i, 0);\n+    }\n+\n+    ResourceMark rm;\n+    GrowableArray<int> seen;\n+    treap.visit_range_in_order(0, 10, [&](Node* x) {\n+      seen.push(x->key());\n+    });\n+    EXPECT_EQ(10, seen.length());\n+    for (int i = 0; i < 10; i++) {\n+      EXPECT_EQ(i, seen.at(i));\n+    }\n+    GrowableArray<int> seen2;\n+    treap.visit_range_in_order(10, 12, [&](Node* x) {\n+      seen2.push(x->key());\n+    });\n+    EXPECT_EQ(1, seen.length());\n+    EXPECT_EQ(10, seen.at(0));\n+  }\n+  { \/\/ Test with descending ordering\n+    TreapCHeap<int, int, CmpInverse> treap;\n+    using Node = TreapCHeap<int, int, CmpInverse>::TreapNode;\n+\n+    for (int i = 0; i < 10; i++) {\n+      treap.upsert(i, 0);\n+    }\n+    ResourceMark rm;\n+    GrowableArray<int> seen;\n+    treap.visit_range_in_order(0, 10, [&](Node* x) {\n+      seen.push(x->key());\n+    });\n+    EXPECT_EQ(10, seen.length());\n+    for (int i = 0; i < 10; i++) {\n+      EXPECT_EQ(9-i, seen.at(i));\n+    }\n+  }\n+}\n+\n+#ifdef ASSERT\n+\n+TEST_VM_F(TreapTest, VerifyItThroughStressTest) {\n+  TreapCHeap<int, int,Cmp> treap;\n+  \/\/ Really hammer a Treap\n+  int ten_thousand = 10000;\n+  for (int i = 0; i < ten_thousand; i++) {\n+    int r = os::random();\n+    if (r >= 0) {\n+      treap.upsert(i, i);\n+    } else {\n+      treap.remove(i);\n+    }\n+    verify_it(treap);\n+  }\n+  for (int i = 0; i < ten_thousand; i++) {\n+    int r = os::random();\n+    if (r >= 0) {\n+      treap.upsert(i, i);\n+    } else {\n+      treap.remove(i);\n+    }\n+    verify_it(treap);\n+  }\n+}\n+\n+#endif \/\/ ASSERT\n","filename":"test\/hotspot\/gtest\/nmt\/test_nmt_treap.cpp","additions":258,"deletions":0,"binary":false,"changes":258,"status":"added"},{"patch":"@@ -0,0 +1,346 @@\n+\/*\n+ * Copyright (c) 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"memory\/allocation.hpp\"\n+#include \"nmt\/vmatree.hpp\"\n+#include \"runtime\/os.hpp\"\n+#include \"unittest.hpp\"\n+\n+class VMATreeTest : public testing::Test {\n+public:\n+  VMATree::TreapNode* treap_root(VMATree& tree) {\n+    return tree._tree._root;\n+  }\n+\n+  VMATree::VMATreap& treap(VMATree& tree) {\n+    return tree._tree;\n+  }\n+\n+  NativeCallStack make_stack(size_t a, size_t b, size_t c, size_t d) {\n+    NativeCallStack stack;\n+    stack._stack[0] = (address)a;\n+    stack._stack[1] = (address)b;\n+    stack._stack[2] = (address)c;\n+    stack._stack[3] = (address)d;\n+    return stack;\n+  }\n+\n+  NativeCallStack stack1 = make_stack(size_t{0x89ac},\n+                                      size_t{0x1fdd},\n+                                      size_t{0x2997},\n+                                      size_t{0x2add});\n+  NativeCallStack stack2 = make_stack(0x123, 0x456,0x789, 0xAAAA);\n+\n+  VMATree::StateType in_type_of(VMATree::TreapNode* x) {\n+    return x->val().in.type();\n+  }\n+  VMATree::StateType out_type_of(VMATree::TreapNode* x) {\n+    return x->val().out.type();\n+  }\n+};\n+\n+\/\/ Low-level tests inspecting the state of the tree.\n+TEST_VM_F(VMATreeTest, LowLevel) {\n+  using Tree = VMATree;\n+  using Node = Tree::TreapNode;\n+  using NCS = NativeCallStackStorage;\n+  NativeCallStackStorage ncs(true);\n+  NativeCallStackStorage::StackIndex si1 = ncs.push(stack1);\n+  NativeCallStackStorage::StackIndex si2 = ncs.push(stack2);\n+\n+  \/\/ Adjacent reservations should result in exactly 2 nodes\n+  auto adjacent_2_nodes = [&](const VMATree::RegionData& rd) {\n+    Tree tree;\n+    for (int i = 0; i < 100; i++) {\n+      tree.reserve_mapping(i * 100, 100, rd);\n+    }\n+    int found_nodes = 0;\n+    treap(tree).visit_range_in_order(0, 999999, [&](Node* x) {\n+      found_nodes++;\n+    });\n+    EXPECT_EQ(2, found_nodes) << \"Adjacent reservations should result in exactly 2 nodes\";\n+\n+    \/\/ Reserving the exact same space again should result in still having only 2 nodes\n+    for (int i = 0; i < 100; i++) {\n+      tree.reserve_mapping(i * 100, 100, rd);\n+    }\n+    found_nodes = 0;\n+    treap(tree).visit_range_in_order(0, 999999, [&](Node* x) {\n+      found_nodes++;\n+    });\n+    EXPECT_EQ(2, found_nodes) << \"Adjacent reservations should result in exactly 2 nodes\";\n+\n+    \/\/ Do it backwards instead.\n+    Tree tree2;\n+    for (int i = 99; i >= 0; i--) {\n+      tree2.reserve_mapping(i * 100, 100, rd);\n+    }\n+    found_nodes = 0;\n+    treap(tree2).visit_range_in_order(0, 999999, [&](Node* x) {\n+      found_nodes++;\n+    });\n+    EXPECT_EQ(2, found_nodes) << \"Adjacent reservations should result in exactly 2 nodes\";\n+  };\n+\n+  { \/\/ Overlapping reservations should also only result in 2 nodes.\n+    VMATree::RegionData rd{si1, mtTest};\n+    Tree tree2;\n+    for (int i = 99; i >= 0; i--) {\n+      tree2.reserve_mapping(i * 100, 101, rd);\n+    }\n+    int found_nodes = 0;\n+    treap(tree2).visit_range_in_order(0, 999999, [&](Node* x) {\n+      found_nodes++;\n+    });\n+    EXPECT_EQ(2, found_nodes) << \"Adjacent reservations should result in exactly 2 nodes\";\n+  }\n+\n+  \/\/ After removing all ranges we should be left with an entirely empty tree\n+  auto remove_all_leaves_empty_tree = [&](const VMATree::RegionData& rd) {\n+    Tree tree;\n+    tree.reserve_mapping(0, 100*100, rd);\n+    for (int i = 0; i < 100; i++) {\n+      tree.release_mapping(i*100, 100);\n+    }\n+    EXPECT_EQ(nullptr, treap_root(tree)) << \"Releasing all memory should result in an empty tree\";\n+\n+    \/\/ Other way around\n+    tree.reserve_mapping(0, 100*100, rd);\n+    for (int i = 99; i >= 0; i--) {\n+      tree.release_mapping(i*100, 100);\n+    }\n+    EXPECT_EQ(nullptr, treap_root(tree)) << \"Releasing all memory should result in an empty tree\";\n+  };\n+\n+  \/\/ Committing in middle of reservation ends with a sequence of 4 nodes\n+  auto commit_middle = [&](const VMATree::RegionData& rd) {\n+    Tree tree;\n+    tree.reserve_mapping(0, 100, rd);\n+    tree.commit_mapping(50, 25, rd);\n+\n+    size_t found[16];\n+    size_t wanted[4] = {0, 50, 75, 100};\n+    auto exists = [&](size_t x) {\n+      for (int i = 0; i < 4; i++) {\n+        if (wanted[i] == x) return true;\n+      }\n+      return false;\n+    };\n+    int i = 0;\n+    treap(tree).visit_range_in_order(0, 300, [&](Node* x) {\n+      if (i < 16) {\n+        found[i] = x->key();\n+      }\n+      i++;\n+    });\n+    ASSERT_EQ(4, i) << \"0 - 50 - 75 - 100 nodes expected\";\n+    EXPECT_TRUE(exists(found[0]));\n+    EXPECT_TRUE(exists(found[1]));\n+    EXPECT_TRUE(exists(found[2]));\n+    EXPECT_TRUE(exists(found[3]));\n+  };\n+\n+  \/\/ Committing in a whole reserved range results in 2 nodes\n+  auto commit_whole = [&](const VMATree::RegionData& rd) {\n+    Tree tree;\n+    tree.reserve_mapping(0, 100*100, rd);\n+    for (int i = 0; i < 100; i++) {\n+      tree.commit_mapping(i*100, 100, rd);\n+    }\n+    int found_nodes = 0;\n+    treap(tree).visit_range_in_order(0, 999999, [&](Node* x) {\n+      found_nodes++;\n+      VMATree::StateType in = in_type_of(x);\n+      VMATree::StateType out = out_type_of(x);\n+      EXPECT_TRUE((in == VMATree::StateType::Released && out == VMATree::StateType::Committed) ||\n+                  (in == VMATree::StateType::Committed && out == VMATree::StateType::Released));\n+    });\n+    EXPECT_EQ(2, found_nodes);\n+  };\n+\n+\n+  adjacent_2_nodes(VMATree::empty_regiondata);\n+  remove_all_leaves_empty_tree(VMATree::empty_regiondata);\n+  commit_middle(VMATree::empty_regiondata);\n+  commit_whole(VMATree::empty_regiondata);\n+\n+  VMATree::RegionData rd{si1, mtTest };\n+  adjacent_2_nodes(rd);\n+  remove_all_leaves_empty_tree(rd);\n+  commit_middle(rd);\n+  commit_whole(rd);\n+\n+  { \/\/ Identical operation but different metadata should not merge\n+    Tree tree;\n+    VMATree::RegionData rd{si1, mtTest };\n+    VMATree::RegionData rd2{si2, mtNMT };\n+    tree.reserve_mapping(0, 100, rd);\n+    tree.reserve_mapping(100, 100, rd2);\n+    int found_nodes = 0;\n+    treap(tree).visit_range_in_order(0, 99999, [&](Node* x) {\n+      found_nodes++;\n+    });\n+    EXPECT_EQ(3, found_nodes);\n+  }\n+\n+  { \/\/ Reserving after commit should overwrite commit\n+    Tree tree;\n+    VMATree::RegionData rd{si1, mtTest };\n+    VMATree::RegionData rd2{si2, mtNMT };\n+    tree.commit_mapping(50, 50, rd2);\n+    tree.reserve_mapping(0, 100, rd);\n+    int found_nodes = 0;\n+    treap(tree).visit_range_in_order(0, 99999, [&](Node* x) {\n+      EXPECT_TRUE(x->key() == 0 || x->key() == 100);\n+      if (x->key() == 0) {\n+        EXPECT_EQ(x->val().out.regiondata().flag, mtTest);\n+      }\n+      found_nodes++;\n+    });\n+    EXPECT_EQ(2, found_nodes);\n+  }\n+\n+  { \/\/ Split a reserved region into two different reserved regions\n+    Tree tree;\n+    VMATree::RegionData rd{si1, mtTest };\n+    VMATree::RegionData rd2{si2, mtNMT };\n+    VMATree::RegionData rd3{si1, mtNone };\n+    tree.reserve_mapping(0, 100, rd);\n+    tree.reserve_mapping(0, 50, rd2);\n+    tree.reserve_mapping(50, 50, rd3);\n+    int found_nodes = 0;\n+    treap(tree).visit_range_in_order(0, 99999, [&](Node* x) {\n+      found_nodes++;\n+    });\n+    EXPECT_EQ(3, found_nodes);\n+  }\n+  { \/\/ One big reserve + release leaves an empty tree\n+    Tree::RegionData rd{si1, mtNMT};\n+    Tree tree;\n+    tree.reserve_mapping(0, 500000, rd);\n+    tree.release_mapping(0, 500000);\n+    EXPECT_EQ(nullptr, treap_root(tree));\n+  }\n+  { \/\/ A committed region inside of\/replacing a reserved region\n+    \/\/ should replace the reserved region's metadata.\n+    Tree::RegionData rd{si1, mtNMT};\n+    VMATree::RegionData rd2{si2, mtTest};\n+    Tree tree;\n+    tree.reserve_mapping(0, 100, rd);\n+    tree.commit_mapping(0, 100, rd2);\n+    treap(tree).visit_range_in_order(0, 99999, [&](Node* x) {\n+      if (x->key() == 0) {\n+        EXPECT_EQ(mtTest, x->val().out.regiondata().flag);\n+      }\n+      if (x->key() == 100) {\n+        EXPECT_EQ(mtTest, x->val().in.regiondata().flag);\n+      }\n+    });\n+  }\n+\n+  { \/\/ Attempting to reserve or commit an empty region should not change the tree.\n+    Tree tree;\n+    Tree::RegionData rd{si1, mtNMT};\n+    tree.reserve_mapping(0, 0, rd);\n+    EXPECT_EQ(nullptr, treap_root(tree));\n+    tree.commit_mapping(0, 0, rd);\n+    EXPECT_EQ(nullptr, treap_root(tree));\n+  }\n+}\n+\n+\/\/ Tests for summary accounting\n+TEST_VM_F(VMATreeTest, SummaryAccounting) {\n+  using Tree = VMATree;\n+  using Node = Tree::TreapNode;\n+  using NCS = NativeCallStackStorage;\n+  { \/\/ Fully enclosed re-reserving works correctly.\n+    Tree::RegionData rd(NCS::StackIndex(), mtTest);\n+    Tree::RegionData rd2(NCS::StackIndex(), mtNMT);\n+    Tree tree;\n+    auto all_diff = tree.reserve_mapping(0, 100, rd);\n+    auto diff = all_diff.flag[NMTUtil::flag_to_index(mtTest)];\n+    EXPECT_EQ(100, diff.reserve);\n+    all_diff = tree.reserve_mapping(50, 25, rd2);\n+    diff = all_diff.flag[NMTUtil::flag_to_index(mtTest)];\n+    auto diff2 = all_diff.flag[NMTUtil::flag_to_index(mtNMT)];\n+    EXPECT_EQ(-25, diff.reserve);\n+    EXPECT_EQ(25, diff2.reserve);\n+  }\n+  { \/\/ Fully release reserved mapping\n+    Tree::RegionData rd(NCS::StackIndex(), mtTest);\n+    Tree tree;\n+    auto all_diff = tree.reserve_mapping(0, 100, rd);\n+    auto diff = all_diff.flag[NMTUtil::flag_to_index(mtTest)];\n+    EXPECT_EQ(100, diff.reserve);\n+    all_diff = tree.release_mapping(0, 100);\n+    diff = all_diff.flag[NMTUtil::flag_to_index(mtTest)];\n+    EXPECT_EQ(-100, diff.reserve);\n+  }\n+  { \/\/ Convert some of a released mapping to a committed one\n+    Tree::RegionData rd(NCS::StackIndex(), mtTest);\n+    Tree tree;\n+    auto all_diff = tree.reserve_mapping(0, 100, rd);\n+    auto diff = all_diff.flag[NMTUtil::flag_to_index(mtTest)];\n+    EXPECT_EQ(diff.reserve, 100);\n+    all_diff = tree.commit_mapping(0, 100, rd);\n+    diff = all_diff.flag[NMTUtil::flag_to_index(mtTest)];\n+    EXPECT_EQ(0, diff.reserve);\n+    EXPECT_EQ(100, diff.commit);\n+  }\n+  { \/\/ Adjacent reserved mappings with same flag\n+    Tree::RegionData rd(NCS::StackIndex(), mtTest);\n+    Tree tree;\n+    auto all_diff = tree.reserve_mapping(0, 100, rd);\n+    auto diff = all_diff.flag[NMTUtil::flag_to_index(mtTest)];\n+    EXPECT_EQ(diff.reserve, 100);\n+    all_diff = tree.reserve_mapping(100, 100, rd);\n+    diff = all_diff.flag[NMTUtil::flag_to_index(mtTest)];\n+    EXPECT_EQ(100, diff.reserve);\n+  }\n+  { \/\/ Adjacent reserved mappings with different flags\n+  Tree::RegionData rd(NCS::StackIndex(), mtTest);\n+    Tree::RegionData rd2(NCS::StackIndex(), mtNMT);\n+    Tree tree;\n+    auto all_diff = tree.reserve_mapping(0, 100, rd);\n+    auto diff = all_diff.flag[NMTUtil::flag_to_index(mtTest)];\n+    EXPECT_EQ(diff.reserve, 100);\n+    all_diff = tree.reserve_mapping(100, 100, rd2);\n+    diff = all_diff.flag[NMTUtil::flag_to_index(mtTest)];\n+    EXPECT_EQ(0, diff.reserve);\n+    diff = all_diff.flag[NMTUtil::flag_to_index(mtNMT)];\n+    EXPECT_EQ(100, diff.reserve);\n+  }\n+\n+  { \/\/ A commit with two previous commits inside of it should only register\n+    \/\/ the new memory in the commit diff.\n+    Tree tree;\n+    Tree::RegionData rd(NCS::StackIndex(), mtTest);\n+    tree.commit_mapping(128, 128, rd);\n+    tree.commit_mapping(512, 128, rd);\n+    auto diff = tree.commit_mapping(0, 1024, rd);\n+    EXPECT_EQ(768, diff.flag[NMTUtil::flag_to_index(mtTest)].commit);\n+    EXPECT_EQ(768, diff.flag[NMTUtil::flag_to_index(mtTest)].reserve);\n+  }\n+}\n","filename":"test\/hotspot\/gtest\/nmt\/test_vmatree.cpp","additions":346,"deletions":0,"binary":false,"changes":346,"status":"added"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2023, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -89,8 +89,3 @@\n-\n-        if (XmsInM < XmxInM) {\n-            \/\/ There will be reservations which are smaller than the total\n-            \/\/ memory allocated in TestZNMT.Test.main. This means that some\n-            \/\/ reservation will be completely committed and print the following\n-            \/\/ in the NMT statistics.\n-            oa.shouldMatch(\"reserved and committed \\\\d+ for Java Heap\");\n-        }\n+        \/\/ We expect to have a report of this type.\n+        oa.shouldMatch(\"ZGC heap backing file\");\n+        oa.shouldMatch(\"allocated \\\\d+ for Java Heap\");\n@@ -100,3 +95,0 @@\n-        testValue(0);\n-        testValue(1);\n-        testValue(2);\n","filename":"test\/hotspot\/jtreg\/gc\/z\/TestZNMT.java","additions":4,"deletions":12,"binary":false,"changes":16,"status":"modified"}]}