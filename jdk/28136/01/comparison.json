{"files":[{"patch":"@@ -3863,0 +3863,40 @@\n+void Assembler::vmovsldup(XMMRegister dst, XMMRegister src, int vector_len) {\n+  assert(vector_len == AVX_512bit ? VM_Version::supports_evex() : VM_Version::supports_avx(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ true);\n+  int encode = vex_prefix_and_encode(dst->encoding(), 0, src->encoding(), VEX_SIMD_F3, VEX_OPCODE_0F, &attributes);\n+  emit_int16(0x12, (0xC0 | encode));\n+}\n+\n+void Assembler::vmovshdup(XMMRegister dst, XMMRegister src, int vector_len) {\n+  assert(vector_len == AVX_512bit ? VM_Version::supports_evex() : VM_Version::supports_avx(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ true);\n+  int encode = vex_prefix_and_encode(dst->encoding(), 0, src->encoding(), VEX_SIMD_F3, VEX_OPCODE_0F, &attributes);\n+  emit_int16(0x16, (0xC0 | encode));\n+}\n+\n+void Assembler::evmovsldup(XMMRegister dst, KRegister mask, XMMRegister src, bool merge, int vector_len) {\n+  assert(VM_Version::supports_evex(), \"\");\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  attributes.set_is_evex_instruction();\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), 0, src->encoding(), VEX_SIMD_F3, VEX_OPCODE_0F, &attributes);\n+  emit_int16(0x12, (0xC0 | encode));\n+}\n+\n+void Assembler::evmovshdup(XMMRegister dst, KRegister mask, XMMRegister src, bool merge, int vector_len) {\n+  assert(VM_Version::supports_evex(), \"\");\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  attributes.set_is_evex_instruction();\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), 0, src->encoding(), VEX_SIMD_F3, VEX_OPCODE_0F, &attributes);\n+  emit_int16(0x16, (0xC0 | encode));\n+}\n+\n","filename":"src\/hotspot\/cpu\/x86\/assembler_x86.cpp","additions":40,"deletions":0,"binary":false,"changes":40,"status":"modified"},{"patch":"@@ -1667,0 +1667,5 @@\n+  void vmovsldup(XMMRegister dst, XMMRegister src, int vector_len);\n+  void vmovshdup(XMMRegister dst, XMMRegister src, int vector_len);\n+  void evmovsldup(XMMRegister dst, KRegister mask, XMMRegister src, bool merge, int vector_len);\n+  void evmovshdup(XMMRegister dst, KRegister mask, XMMRegister src, bool merge, int vector_len);\n+\n","filename":"src\/hotspot\/cpu\/x86\/assembler_x86.hpp","additions":5,"deletions":0,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -1371,0 +1371,1 @@\n+  using Assembler::evpcmpeqd;\n","filename":"src\/hotspot\/cpu\/x86\/macroAssembler_x86.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -3,0 +3,1 @@\n+ * Copyright (c) 2025, Intel Corporation. All rights reserved.\n@@ -33,2 +34,0 @@\n-#define xmm(i) as_XMMRegister(i)\n-\n@@ -43,2 +42,0 @@\n-#define XMMBYTES 64\n-\n@@ -49,3 +46,3 @@\n-    8380417, \/\/ dilithium_q\n-    2365951, \/\/ montRSquareModQ\n-    5373807 \/\/ Barrett addend for modular reduction\n+    8380417,  \/\/ dilithium_q\n+    2365951,  \/\/ montRSquareModQ\n+    5373807   \/\/ Barrett addend for modular reduction\n@@ -63,4 +60,4 @@\n-const Register scratch = r10;\n-const XMMRegister montMulPerm = xmm28;\n-const XMMRegister montQInvModR = xmm30;\n-const XMMRegister dilithium_q = xmm31;\n+ATTRIBUTE_ALIGNED(64) static const uint32_t unshufflePerms[] = {\n+  \/\/ Shuffle for the 128-bit element swap (uint64_t)\n+  0, 0, 1,  0, 8,  0, 9, 0, 4, 0, 5, 0, 12, 0, 13, 0,\n+  10, 0, 11, 0, 2, 0, 3, 0, 14, 0, 15, 0, 6, 0, 7, 0,\n@@ -68,0 +65,3 @@\n+  \/\/ Final shuffle for AlmostNtt\n+  0, 16, 1, 17, 2, 18, 3, 19, 4, 20, 5, 21, 6, 22, 7, 23,\n+  24, 8, 25, 9, 26, 10, 27, 11, 28, 12, 29, 13, 30, 14, 31, 15,\n@@ -69,35 +69,3 @@\n-ATTRIBUTE_ALIGNED(64) static const uint32_t dilithiumAvx512Perms[] = {\n-     \/\/ collect montmul results into the destination register\n-    17, 1, 19, 3, 21, 5, 23, 7, 25, 9, 27, 11, 29, 13, 31, 15,\n-    \/\/ ntt\n-    \/\/ level 4\n-    0, 1, 2, 3, 4, 5, 6, 7, 16, 17, 18, 19, 20, 21, 22, 23,\n-    8, 9, 10, 11, 12, 13, 14, 15, 24, 25, 26, 27, 28, 29, 30, 31,\n-    \/\/ level 5\n-    0, 1, 2, 3, 16, 17, 18, 19, 8, 9, 10, 11, 24, 25, 26, 27,\n-    4, 5, 6, 7, 20, 21, 22, 23, 12, 13, 14, 15, 28, 29, 30, 31,\n-    \/\/ level 6\n-    0, 1, 16, 17, 4, 5, 20, 21, 8, 9, 24, 25, 12, 13, 28, 29,\n-    2, 3, 18, 19, 6, 7, 22, 23, 10, 11, 26, 27, 14, 15, 30, 31,\n-    \/\/ level 7\n-    0, 16, 2, 18, 4, 20, 6, 22, 8, 24, 10, 26, 12, 28, 14, 30,\n-    1, 17, 3, 19, 5, 21, 7, 23, 9, 25, 11, 27, 13, 29, 15, 31,\n-    0, 16, 1, 17, 2, 18, 3, 19, 4, 20, 5, 21, 6, 22, 7, 23,\n-    8, 24, 9, 25, 10, 26, 11, 27, 12, 28, 13, 29, 14, 30, 15, 31,\n-\n-    \/\/ ntt inverse\n-    \/\/ level 0\n-    0, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30,\n-    1, 3, 5, 7, 9, 11, 13, 15, 17, 19, 21, 23, 25, 27, 29, 31,\n-    \/\/ level 1\n-    0, 16, 2, 18, 4, 20, 6, 22, 8, 24, 10, 26, 12, 28, 14, 30,\n-    1, 17, 3, 19, 5, 21, 7, 23, 9, 25, 11, 27, 13, 29, 15, 31,\n-    \/\/ level 2\n-    0, 1, 16, 17, 4, 5, 20, 21, 8, 9, 24, 25, 12, 13, 28, 29,\n-    2, 3, 18, 19, 6, 7, 22, 23, 10, 11, 26, 27, 14, 15, 30, 31,\n-    \/\/ level 3\n-    0, 1, 2, 3, 16, 17, 18, 19, 8, 9, 10, 11, 24, 25, 26, 27,\n-    4, 5, 6, 7, 20, 21, 22, 23, 12, 13, 14, 15, 28, 29, 30, 31,\n-    \/\/ level 4\n-    0, 1, 2, 3, 4, 5, 6, 7, 16, 17, 18, 19, 20, 21, 22, 23,\n-    8, 9, 10, 11, 12, 13, 14, 15, 24, 25, 26, 27, 28, 29, 30, 31\n+  \/\/ Initial shuffle for AlmostInverseNtt\n+  0, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30,\n+  17, 19, 21, 23, 25, 27, 29, 31, 1, 3, 5, 7, 9, 11, 13, 15\n@@ -106,13 +74,170 @@\n-const int montMulPermsIdx = 0;\n-const int nttL4PermsIdx = 64;\n-const int nttL5PermsIdx = 192;\n-const int nttL6PermsIdx = 320;\n-const int nttL7PermsIdx = 448;\n-const int nttInvL0PermsIdx = 704;\n-const int nttInvL1PermsIdx = 832;\n-const int nttInvL2PermsIdx = 960;\n-const int nttInvL3PermsIdx = 1088;\n-const int nttInvL4PermsIdx = 1216;\n-\n-static address dilithiumAvx512PermsAddr() {\n-  return (address) dilithiumAvx512Perms;\n+static address unshufflePermsAddr(int offset) {\n+  return ((address) unshufflePerms) + offset*64;\n+}\n+\n+\/\/ The following function swaps elements A<->B, C<->D, and so forth.\n+\/\/ input1[] is shuffled in place; shuffle of input2[] is copied to output2[].\n+\/\/ Element size (in bits) is specified by size parameter.\n+\/\/ +-----+-----+-----+-----+-----\n+\/\/ |     |  A  |     |  C  | ...\n+\/\/ +-----+-----+-----+-----+-----\n+\/\/ +-----+-----+-----+-----+-----\n+\/\/ |  B  |     |  D  |     | ...\n+\/\/ +-----+-----+-----+-----+-----\n+\/\/\n+\/\/ NOTE: size 0 and 1 are used for initial and final shuffles respectivelly of\n+\/\/ dilithiumAlmostInverseNtt and dilithiumAlmostNtt. For size 0 and 1, input1[]\n+\/\/ and input2[] are modified in-place (and output2 is used as a temporary)\n+\/\/\n+\/\/ Using C++ lambdas for improved readability (to hide parameters that always repeat)\n+static auto whole_shuffle(Register scratch, KRegister mergeMask1, KRegister mergeMask2,\n+  const XMMRegister unshuffle1, const XMMRegister unshuffle2, int vector_len, MacroAssembler *_masm) {\n+\n+  int regCnt = 4;\n+  if (vector_len == Assembler::AVX_256bit) {\n+    regCnt = 2;\n+  }\n+\n+  return [=](const XMMRegister output2[], const XMMRegister input1[],\n+    const XMMRegister input2[], int size) {\n+    if (vector_len == Assembler::AVX_256bit) {\n+      switch (size) {\n+        case 128:\n+          for (int i = 0; i < regCnt; i++) {\n+            __ vperm2i128(output2[i], input1[i], input2[i], 0b110001);\n+          }\n+          for (int i = 0; i < regCnt; i++) {\n+            __ vinserti128(input1[i], input1[i], input2[i], 1);\n+          }\n+          break;\n+        case 64:\n+          for (int i = 0; i < regCnt; i++) {\n+            __ vshufpd(output2[i], input1[i], input2[i], 0b11111111, vector_len);\n+          }\n+          for (int i = 0; i < regCnt; i++) {\n+            __ vshufpd(input1[i], input1[i], input2[i], 0b00000000, vector_len);\n+          }\n+          break;\n+        case 32:\n+          for (int i = 0; i < regCnt; i++) {\n+            __ vmovshdup(output2[i], input1[i], vector_len);\n+          }\n+          for (int i = 0; i < regCnt; i++) {\n+            __ vpblendd(output2[i], output2[i], input2[i], 0b10101010, vector_len);\n+          }\n+          for (int i = 0; i < regCnt; i++) {\n+            __ vmovsldup(input2[i], input2[i], vector_len);\n+          }\n+          for (int i = 0; i < regCnt; i++) {\n+            __ vpblendd(input1[i], input1[i], input2[i], 0b10101010, vector_len);\n+          }\n+          break;\n+        \/\/ Special cases\n+        case 1: \/\/ initial shuffle for dilithiumAlmostInverseNtt\n+          \/\/ shuffle all even 32bit columns to input1, and odd to input2\n+          for (int i = 0; i < regCnt; i++) {\n+            \/\/ 0b-3-1-3-1\n+            __ vshufps(output2[i], input1[i], input2[i], 0b11011101, vector_len);\n+          }\n+          for (int i = 0; i < regCnt; i++) {\n+            \/\/ 0b-2-0-2-0\n+            __ vshufps(input1[i], input1[i], input2[i], 0b10001000, vector_len);\n+          }\n+          for (int i = 0; i < regCnt; i++) {\n+            __ vpermq(input2[i], output2[i], 0b11011000, vector_len);\n+          }\n+          for (int i = 0; i < regCnt; i++) {\n+            \/\/ 0b-3-1-2-0\n+            __ vpermq(input1[i], input1[i], 0b11011000, vector_len);\n+          }\n+          break;\n+        case 0: \/\/ final unshuffle for dilithiumAlmostNtt\n+          \/\/ reverse case 1: all even are in input1 and odd in input2, put back\n+          for (int i = 0; i < regCnt; i++) {\n+            __ vpunpckhdq(output2[i], input1[i], input2[i], vector_len);\n+          }\n+          for (int i = 0; i < regCnt; i++) {\n+            __ vpunpckldq(input1[i], input1[i], input2[i], vector_len);\n+          }\n+          for (int i = 0; i < regCnt; i++) {\n+            __ vperm2i128(input2[i], input1[i], output2[i], 0b110001);\n+          }\n+          for (int i = 0; i < regCnt; i++) {\n+            __ vinserti128(input1[i], input1[i], output2[i], 1);\n+          }\n+          break;\n+        default:\n+          assert(false, \"Don't call here\");\n+      }\n+    } else {\n+      switch (size) {\n+        case 256:\n+          for (int i = 0; i < regCnt; i++) {\n+            \/\/ 0b-3-2-3-2\n+            __ evshufi64x2(output2[i], input1[i], input2[i], 0b11101110, vector_len);\n+          }\n+          for (int i = 0; i < regCnt; i++) {\n+            __ vinserti64x4(input1[i], input1[i], input2[i], 1);\n+          }\n+          break;\n+        case 128:\n+          for (int i = 0; i < regCnt; i++) {\n+            __ vmovdqu(output2[i], input2[i], vector_len);\n+          }\n+          for (int i = 0; i < regCnt; i++) {\n+            __ evpermt2q(output2[i], unshuffle2, input1[i], vector_len);\n+          }\n+          for (int i = 0; i < regCnt; i++) {\n+            __ evpermt2q(input1[i], unshuffle1, input2[i], vector_len);\n+          }\n+\n+          break;\n+        case 64:\n+          for (int i = 0; i < regCnt; i++) {\n+            __ vshufpd(output2[i], input1[i], input2[i], 0b11111111, vector_len);\n+          }\n+          for (int i = 0; i < regCnt; i++) {\n+            __ vshufpd(input1[i], input1[i], input2[i], 0b00000000, vector_len);\n+          }\n+          break;\n+        case 32:\n+          for (int i = 0; i < regCnt; i++) {\n+            __ vmovdqu(output2[i], input2[i], vector_len);\n+          }\n+          for (int i = 0; i < regCnt; i++) {\n+            __ evmovshdup(output2[i], mergeMask2, input1[i], true, vector_len);\n+          }\n+          for (int i = 0; i < regCnt; i++) {\n+            __ evmovsldup(input1[i], mergeMask1, input2[i], true, vector_len);\n+          }\n+          break;\n+        \/\/ Special cases\n+        case 1: \/\/ initial shuffle for dilithiumAlmostInverseNtt\n+          \/\/ shuffle all even 32bit columns to input1, and odd to input2\n+          for (int i = 0; i < regCnt; i++) {\n+            __ vmovdqu(output2[i], input2[i], vector_len);\n+          }\n+          for (int i = 0; i < regCnt; i++) {\n+            __ evpermt2d(input2[i], unshuffle2, input1[i], vector_len);\n+          }\n+          for (int i = 0; i < regCnt; i++) {\n+            __ evpermt2d(input1[i], unshuffle1, output2[i], vector_len);\n+          }\n+          break;\n+        case 0: \/\/ final unshuffle for dilithiumAlmostNtt\n+          \/\/ reverse case 1: all even are in input1 and odd in input2, put back\n+          for (int i = 0; i < regCnt; i++) {\n+            __ vmovdqu(output2[i], input2[i], vector_len);\n+          }\n+          for (int i = 0; i < regCnt; i++) {\n+            __ evpermt2d(input2[i], unshuffle2, input1[i], vector_len);\n+          }\n+          for (int i = 0; i < regCnt; i++) {\n+            __ evpermt2d(input1[i], unshuffle1, output2[i], vector_len);\n+          }\n+          break;\n+        default:\n+          assert(false, \"Don't call here\");\n+      }\n+    }\n+  }; \/\/ return\n@@ -121,1 +246,1 @@\n-\/\/ We do Montgomery multiplications of two vectors of 16 ints each in 4 steps:\n+\/\/ We do Montgomery multiplications of two AVX registers in 4 steps:\n@@ -123,3 +248,3 @@\n-\/\/    the odd numbered slots of a third register.\n-\/\/ 2. Swap the even and odd numbered slots of the original input registers.\n-\/\/ 3. Similar to step 1, but into a different output register.\n+\/\/    the odd numbered slots of a scratch2 register.\n+\/\/ 2. Swap the even and odd numbered slots of the original input registers.*\n+\/\/ 3. Similar to step 1, but into output register.\n@@ -128,15 +253,3 @@\n-\/\/ (For levels 0-6 in the Ntt and levels 1-7 of the inverse Ntt we only swap the\n-\/\/ odd-even slots of the first multiplicand as in the second (zetas) the\n-\/\/ odd slots contain the same number as the corresponding even one.)\n-\/\/ The indexes of the registers to be multiplied\n-\/\/ are in inputRegs1[] and inputRegs[2].\n-\/\/ The results go to the registers whose indexes are in outputRegs.\n-\/\/ scratchRegs should contain 12 different register indexes.\n-\/\/ The set in outputRegs should not overlap with the set of the middle four\n-\/\/ scratch registers.\n-\/\/ The sets in inputRegs1 and inputRegs2 cannot overlap with the set of the\n-\/\/ first eight scratch registers.\n-\/\/ In most of the cases, the odd and the corresponding even slices of the\n-\/\/ registers indexed by the numbers in inputRegs2 will contain the same number,\n-\/\/ this should be indicated by calling this function with\n-\/\/ input2NeedsShuffle=false .\n+\/\/ (*For levels 0-6 in the Ntt and levels 1-7 of the inverse Ntt, need NOT swap\n+\/\/ the second operand (zetas) since the odd slots contain the same number\n+\/\/ as the corresponding even one. This is indicated by input2NeedsShuffle=false)\n@@ -144,19 +257,16 @@\n-static void montMul64(int outputRegs[], int inputRegs1[], int inputRegs2[],\n-                      int scratchRegs[], bool input2NeedsShuffle,\n-                      MacroAssembler *_masm) {\n-\n-  for (int i = 0; i < 4; i++) {\n-    __ vpmuldq(xmm(scratchRegs[i]), xmm(inputRegs1[i]), xmm(inputRegs2[i]),\n-               Assembler::AVX_512bit);\n-  }\n-  for (int i = 0; i < 4; i++) {\n-    __ vpmulld(xmm(scratchRegs[i + 4]), xmm(scratchRegs[i]), montQInvModR,\n-               Assembler::AVX_512bit);\n-  }\n-  for (int i = 0; i < 4; i++) {\n-    __ vpmuldq(xmm(scratchRegs[i + 4]), xmm(scratchRegs[i + 4]), dilithium_q,\n-               Assembler::AVX_512bit);\n-  }\n-  for (int i = 0; i < 4; i++) {\n-    __ evpsubd(xmm(scratchRegs[i + 4]), k0, xmm(scratchRegs[i]),\n-               xmm(scratchRegs[i + 4]), false, Assembler::AVX_512bit);\n+\/\/ The registers to be multiplied are in input1[] and inputs2[]. The results go\n+\/\/ into output[]. Two scratch[] register arrays are expected. input1[] can\n+\/\/ overlap with either output[] or scratch1[]\n+\/\/ - If AVX512, all register arrays are of length 4\n+\/\/ - If AVX2, first two registers of each array are in xmm0-xmm15 range\n+\/\/ Constants montQInvModR, dilithium_q and mergeMask expected to have already\n+\/\/ been loaded.\n+\/\/\n+\/\/ Using C++ lambdas for improved readability (to hide parameters that always repeat)\n+static auto whole_montMul(XMMRegister montQInvModR, XMMRegister dilithium_q,\n+    KRegister mergeMask, int vector_len, MacroAssembler *_masm) {\n+  int regCnt = 4;\n+  int regSize = 64;\n+  if (vector_len == Assembler::AVX_256bit) {\n+    regCnt = 2;\n+    regSize = 32;\n@@ -165,6 +275,10 @@\n-  for (int i = 0; i < 4; i++) {\n-    __ vpshufd(xmm(inputRegs1[i]), xmm(inputRegs1[i]), 0xB1,\n-               Assembler::AVX_512bit);\n-    if (input2NeedsShuffle) {\n-       __ vpshufd(xmm(inputRegs2[i]), xmm(inputRegs2[i]), 0xB1,\n-                  Assembler::AVX_512bit);\n+  return [=](const XMMRegister output[], const XMMRegister input1[],\n+    const XMMRegister input2[], const XMMRegister scratch1[],\n+    const XMMRegister scratch2[], bool input2NeedsShuffle = false) {\n+    \/\/ (Register overloading) Can't always use scratch1 (could override input1).\n+    \/\/ If so, use output:\n+    const XMMRegister* scratch = scratch1 == input1 ? output: scratch1;\n+\n+    \/\/ scratch = input1_even*intput2_even\n+    for (int i = 0; i < regCnt; i++) {\n+      __ vpmuldq(scratch[i], input1[i], input2[i], vector_len);\n@@ -172,1 +286,0 @@\n-  }\n@@ -174,16 +287,4 @@\n-  for (int i = 0; i < 4; i++) {\n-    __ vpmuldq(xmm(scratchRegs[i]), xmm(inputRegs1[i]), xmm(inputRegs2[i]),\n-               Assembler::AVX_512bit);\n-  }\n-  for (int i = 0; i < 4; i++) {\n-    __ vpmulld(xmm(scratchRegs[i + 8]), xmm(scratchRegs[i]), montQInvModR,\n-               Assembler::AVX_512bit);\n-  }\n-  for (int i = 0; i < 4; i++) {\n-    __ vpmuldq(xmm(scratchRegs[i + 8]), xmm(scratchRegs[i + 8]), dilithium_q,\n-               Assembler::AVX_512bit);\n-  }\n-  for (int i = 0; i < 4; i++) {\n-    __ evpsubd(xmm(outputRegs[i]), k0, xmm(scratchRegs[i]),\n-               xmm(scratchRegs[i + 8]), false, Assembler::AVX_512bit);\n-  }\n+    \/\/ scratch2_low = scratch_low * montQInvModR\n+    for (int i = 0; i < regCnt; i++) {\n+      __ vpmuldq(scratch2[i], scratch[i], montQInvModR, vector_len);\n+    }\n@@ -191,5 +292,38 @@\n-  for (int i = 0; i < 4; i++) {\n-    __ evpermt2d(xmm(outputRegs[i]), montMulPerm, xmm(scratchRegs[i + 4]),\n-                 Assembler::AVX_512bit);\n-  }\n-}\n+    \/\/ scratch2 = scratch2_low * dilithium_q\n+    for (int i = 0; i < regCnt; i++) {\n+      __ vpmuldq(scratch2[i], scratch2[i], dilithium_q, vector_len);\n+    }\n+\n+    \/\/ scratch2_high = scratch2_high - scratch_high\n+    for (int i = 0; i < regCnt; i++) {\n+      __ vpsubd(scratch2[i], scratch[i], scratch2[i], vector_len);\n+    }\n+\n+    \/\/ input1_even = input1_odd\n+    \/\/ input2_even = input2_odd\n+    for (int i = 0; i < regCnt; i++) {\n+      __ vpshufd(input1[i], input1[i], 0xB1, vector_len);\n+      if (input2NeedsShuffle) {\n+        __ vpshufd(input2[i], input2[i], 0xB1, vector_len);\n+      }\n+    }\n+\n+    \/\/ scratch1 = input1_even*intput2_even\n+    for (int i = 0; i < regCnt; i++) {\n+      __ vpmuldq(scratch1[i], input1[i], input2[i], vector_len);\n+    }\n+\n+    \/\/ output = scratch1_low * montQInvModR\n+    for (int i = 0; i < regCnt; i++) {\n+      __ vpmuldq(output[i], scratch1[i], montQInvModR, vector_len);\n+    }\n+\n+    \/\/ output = output * dilithium_q\n+    for (int i = 0; i < regCnt; i++) {\n+      __ vpmuldq(output[i], output[i], dilithium_q, vector_len);\n+    }\n+\n+    \/\/ output_high = scratch1_high - output_high\n+    for (int i = 0; i < regCnt; i++) {\n+      __ vpsubd(output[i], scratch1[i], output[i], vector_len);\n+    }\n@@ -197,3 +331,14 @@\n-static void montMul64(int outputRegs[], int inputRegs1[], int inputRegs2[],\n-                       int scratchRegs[], MacroAssembler *_masm) {\n-   montMul64(outputRegs, inputRegs1, inputRegs2, scratchRegs, false, _masm);\n+    \/\/ output = select(output_high, scratch2_high)\n+    if (vector_len == Assembler::AVX_256bit) {\n+      for (int i = 0; i < regCnt; i++) {\n+        __ vmovshdup(scratch2[i], scratch2[i], vector_len);\n+      }\n+      for (int i = 0; i < regCnt; i++) {\n+        __ vpblendd(output[i], output[i], scratch2[i], 0b01010101, vector_len);\n+      }\n+    } else {\n+      for (int i = 0; i < regCnt; i++) {\n+        __ evmovshdup(output[i], mergeMask, scratch2[i], true, vector_len);\n+      }\n+    }\n+  }; \/\/ return\n@@ -202,2 +347,7 @@\n-static void sub_add(int subResult[], int addResult[],\n-                    int input1[], int input2[], MacroAssembler *_masm) {\n+static void sub_add(const XMMRegister subResult[], const XMMRegister addResult[],\n+                    const XMMRegister input1[], const XMMRegister input2[],\n+                    int vector_len, MacroAssembler *_masm) {\n+  int regCnt = 4;\n+  if (vector_len == Assembler::AVX_256bit) {\n+    regCnt = 2;\n+  }\n@@ -205,3 +355,2 @@\n-  for (int i = 0; i < 4; i++) {\n-    __ evpsubd(xmm(subResult[i]), k0, xmm(input1[i]), xmm(input2[i]), false,\n-               Assembler::AVX_512bit);\n+  for (int i = 0; i < regCnt; i++) {\n+    __ vpsubd(subResult[i], input1[i], input2[i], vector_len);\n@@ -210,3 +359,2 @@\n-  for (int i = 0; i < 4; i++) {\n-    __ evpaddd(xmm(addResult[i]), k0, xmm(input1[i]), xmm(input2[i]), false,\n-               Assembler::AVX_512bit);\n+  for (int i = 0; i < regCnt; i++) {\n+    __ vpaddd(addResult[i], input1[i], input2[i], vector_len);\n@@ -216,9 +364,2 @@\n-static void loadPerm(int destinationRegs[], Register perms,\n-                      int offset, MacroAssembler *_masm) {\n-  __ evmovdqul(xmm(destinationRegs[0]), Address(perms, offset),\n-                 Assembler::AVX_512bit);\n-  for (int i = 1; i < 4; i++) {\n-      __ evmovdqul(xmm(destinationRegs[i]), xmm(destinationRegs[0]),\n-                   Assembler::AVX_512bit);\n-    }\n-}\n+static void loadXmms(const XMMRegister destinationRegs[], Register source, int offset,\n+                       int vector_len, MacroAssembler *_masm, int regCnt = -1, int memStep = -1) {\n@@ -226,5 +367,6 @@\n-static void load4Xmms(int destinationRegs[], Register source, int offset,\n-                       MacroAssembler *_masm) {\n-  for (int i = 0; i < 4; i++) {\n-    __ evmovdqul(xmm(destinationRegs[i]), Address(source, offset + i * XMMBYTES),\n-                 Assembler::AVX_512bit);\n+  if (vector_len == Assembler::AVX_256bit) {\n+    regCnt = regCnt == -1 ? 2 : regCnt;\n+    memStep = memStep == -1 ? 32 : memStep;\n+  } else {\n+    regCnt = 4;\n+    memStep = 64;\n@@ -232,1 +374,0 @@\n-}\n@@ -234,2 +375,3 @@\n-static void loadXmm29(Register source, int offset, MacroAssembler *_masm) {\n-    __ evmovdqul(xmm29, Address(source, offset), Assembler::AVX_512bit);\n+  for (int i = 0; i < regCnt; i++) {\n+    __ vmovdqu(destinationRegs[i], Address(source, offset + i * memStep), vector_len);\n+  }\n@@ -238,5 +380,8 @@\n-static void store4Xmms(Register destination, int offset, int xmmRegs[],\n-                       MacroAssembler *_masm) {\n-  for (int i = 0; i < 4; i++) {\n-    __ evmovdqul(Address(destination, offset + i * XMMBYTES), xmm(xmmRegs[i]),\n-                 Assembler::AVX_512bit);\n+static void storeXmms(Register destination, int offset, const XMMRegister xmmRegs[],\n+                       int vector_len, MacroAssembler *_masm, int regCnt = -1, int memStep = -1) {\n+  if (vector_len == Assembler::AVX_256bit) {\n+    regCnt = regCnt == -1 ? 2 : regCnt;\n+    memStep = memStep == -1 ? 32 : memStep;\n+  } else {\n+    regCnt = 4;\n+    memStep = 64;\n@@ -244,1 +389,0 @@\n-}\n@@ -246,18 +390,4 @@\n-static int xmm0_3[] = {0, 1, 2, 3};\n-static int xmm0145[] = {0, 1, 4, 5};\n-static int xmm0246[] = {0, 2, 4, 6};\n-static int xmm0426[] = {0, 4, 2, 6};\n-static int xmm1357[] = {1, 3, 5, 7};\n-static int xmm1537[] = {1, 5, 3, 7};\n-static int xmm2367[] = {2, 3, 6, 7};\n-static int xmm4_7[] = {4, 5, 6, 7};\n-static int xmm8_11[] = {8, 9, 10, 11};\n-static int xmm12_15[] = {12, 13, 14, 15};\n-static int xmm16_19[] = {16, 17, 18, 19};\n-static int xmm20_23[] = {20, 21, 22, 23};\n-static int xmm20222426[] = {20, 22, 24, 26};\n-static int xmm21232527[] = {21, 23, 25, 27};\n-static int xmm24_27[] = {24, 25, 26, 27};\n-static int xmm4_20_24[] = {4, 5, 6, 7, 20, 21, 22, 23, 24, 25, 26, 27};\n-static int xmm16_27[] = {16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27};\n-static int xmm29_29[] = {29, 29, 29, 29};\n+  for (int i = 0; i < regCnt; i++) {\n+    __ vmovdqu(Address(destination, offset + i * memStep), xmmRegs[i], vector_len);\n+  }\n+}\n@@ -270,2 +400,1 @@\n-\/\/ zetas (int[256]) = c_rarg1\n-\/\/\n+\/\/ zetas (int[128*8]) = c_rarg1\n@@ -273,3 +402,2 @@\n-static address generate_dilithiumAlmostNtt_avx512(StubGenerator *stubgen,\n-                                                  MacroAssembler *_masm) {\n-\n+static address generate_dilithiumAlmostNtt_avx(StubGenerator *stubgen,\n+                                               int vector_len, MacroAssembler *_masm) {\n@@ -282,2 +410,0 @@\n-  Label L_loop, L_end;\n-\n@@ -286,7 +412,1 @@\n-  const Register iterations = c_rarg2;\n-\n-  const Register perms = r11;\n-\n-  __ lea(perms, ExternalAddress(dilithiumAvx512PermsAddr()));\n-\n-  __ evmovdqul(montMulPerm, Address(perms, montMulPermsIdx), Assembler::AVX_512bit);\n+  const Register scratch = r10;\n@@ -298,2 +418,1 @@\n-  \/\/ coefficients. In each level we just collect the coefficients (using\n-  \/\/ evpermi2d() instructions where necessary, i.e. in levels 4-7) that need to\n+  \/\/ coefficients. In each level we just shuffle the coefficients that need to\n@@ -304,2 +423,22 @@\n-  \/\/ that we would use for them, so we use only one, xmm29.\n-  loadXmm29(zetas, 0, _masm);\n+  \/\/ that we would use for them, so we use only one register.\n+\n+  \/\/ AVX2 version uses the first half of these arrays\n+  const XMMRegister Coeffs1[] = {xmm0, xmm1, xmm16, xmm17};\n+  const XMMRegister Coeffs2[] = {xmm2, xmm3, xmm18, xmm19};\n+  const XMMRegister Coeffs3[] = {xmm4, xmm5, xmm20, xmm21};\n+  const XMMRegister Coeffs4[] = {xmm6, xmm7, xmm22, xmm23};\n+  const XMMRegister Scratch1[] = {xmm8, xmm9, xmm24, xmm25};\n+  const XMMRegister Scratch2[] = {xmm10, xmm11, xmm26, xmm27};\n+  const XMMRegister Zetas1[] = {xmm12, xmm12, xmm12, xmm12};\n+  const XMMRegister Zetas2[] = {xmm12, xmm12, xmm13, xmm13};\n+  const XMMRegister Zetas3[] = {xmm12, xmm13, xmm28, xmm29};\n+  const XMMRegister montQInvModR = xmm14;\n+  const XMMRegister dilithium_q = xmm15;\n+  const XMMRegister unshuffle1 = xmm30;\n+  const XMMRegister unshuffle2 = xmm31;\n+  KRegister mergeMask1 = k1;\n+  KRegister mergeMask2 = k2;\n+  \/\/ lambdas to hide repeated parameters\n+  auto shuffle = whole_shuffle(scratch, mergeMask1, mergeMask2, unshuffle1, unshuffle2, vector_len, _masm);\n+  auto montMul64 = whole_montMul(montQInvModR, dilithium_q, mergeMask2, vector_len, _masm);\n+\n@@ -308,1 +447,1 @@\n-                  Assembler::AVX_512bit, scratch); \/\/ q^-1 mod 2^32\n+                  vector_len, scratch); \/\/ q^-1 mod 2^32\n@@ -311,110 +450,83 @@\n-                  Assembler::AVX_512bit, scratch); \/\/ q\n-\n-  \/\/ load all coefficients into the vector registers Zmm_0-Zmm_15,\n-  \/\/ 16 coefficients into each\n-  load4Xmms(xmm0_3, coeffs, 0, _masm);\n-  load4Xmms(xmm4_7, coeffs, 4 * XMMBYTES, _masm);\n-  load4Xmms(xmm8_11, coeffs, 8 * XMMBYTES, _masm);\n-  load4Xmms(xmm12_15, coeffs, 12 * XMMBYTES, _masm);\n-\n-  \/\/ level 0 and 1 can be done entirely in registers as the zetas on these\n-  \/\/ levels are the same for all the montmuls that we can do in parallel\n-\n-  \/\/ level 0\n-  montMul64(xmm16_19, xmm8_11, xmm29_29, xmm16_27, _masm);\n-  sub_add(xmm8_11, xmm0_3, xmm0_3, xmm16_19, _masm);\n-  montMul64(xmm16_19, xmm12_15, xmm29_29, xmm16_27, _masm);\n-  loadXmm29(zetas, 512, _masm); \/\/ for level 1\n-  sub_add(xmm12_15, xmm4_7, xmm4_7, xmm16_19, _masm);\n-\n-  \/\/ level 1\n-\n-  montMul64(xmm16_19, xmm4_7, xmm29_29, xmm16_27, _masm);\n-  loadXmm29(zetas, 768, _masm);\n-  sub_add(xmm4_7, xmm0_3, xmm0_3, xmm16_19, _masm);\n-  montMul64(xmm16_19, xmm12_15, xmm29_29, xmm16_27, _masm);\n-  sub_add(xmm12_15, xmm8_11, xmm8_11, xmm16_19, _masm);\n-\n-  \/\/ levels 2 to 7 are done in 2 batches, by first saving half of the coefficients\n-  \/\/ from level 1 into memory, doing all the level 2 to level 7 computations\n-  \/\/ on the remaining half in the vector registers, saving the result to\n-  \/\/ memory after level 7, then loading back the coefficients that we saved after\n-  \/\/ level 1 and do the same computation with those\n-\n-  store4Xmms(coeffs, 8 * XMMBYTES, xmm8_11, _masm);\n-  store4Xmms(coeffs, 12 * XMMBYTES, xmm12_15, _masm);\n-\n-  __ movl(iterations, 2);\n-\n-  __ align(OptoLoopAlignment);\n-  __ BIND(L_loop);\n-\n-  __ subl(iterations, 1);\n-\n-  \/\/ level 2\n-  load4Xmms(xmm12_15, zetas, 2 * 512, _masm);\n-  montMul64(xmm16_19, xmm2367, xmm12_15, xmm16_27, _masm);\n-  load4Xmms(xmm12_15, zetas, 3 * 512, _masm); \/\/ for level 3\n-  sub_add(xmm2367, xmm0145, xmm0145, xmm16_19, _masm);\n-\n-  \/\/ level 3\n-\n-  montMul64(xmm16_19, xmm1357, xmm12_15, xmm16_27, _masm);\n-  sub_add(xmm1357, xmm0246, xmm0246, xmm16_19, _masm);\n-\n-  \/\/ level 4\n-  loadPerm(xmm16_19, perms, nttL4PermsIdx, _masm);\n-  loadPerm(xmm12_15, perms, nttL4PermsIdx + 64, _masm);\n-  load4Xmms(xmm24_27, zetas, 4 * 512, _masm);\n-\n-  for (int i = 0; i < 8; i += 2) {\n-    __ evpermi2d(xmm(i\/2 + 16), xmm(i), xmm(i + 1), Assembler::AVX_512bit);\n-  }\n-  for (int i = 0; i < 8; i += 2) {\n-    __ evpermi2d(xmm(i \/ 2 + 12), xmm(i), xmm(i + 1), Assembler::AVX_512bit);\n-  }\n-\n-  montMul64(xmm12_15, xmm12_15, xmm24_27, xmm4_20_24, _masm);\n-  sub_add(xmm1357, xmm0246, xmm16_19, xmm12_15, _masm);\n-\n-  \/\/ level 5\n-  loadPerm(xmm16_19, perms, nttL5PermsIdx, _masm);\n-  loadPerm(xmm12_15, perms, nttL5PermsIdx + 64, _masm);\n-  load4Xmms(xmm24_27, zetas, 5 * 512, _masm);\n-\n-  for (int i = 0; i < 8; i += 2) {\n-    __ evpermi2d(xmm(i\/2 + 16), xmm(i), xmm(i + 1), Assembler::AVX_512bit);\n-  }\n-  for (int i = 0; i < 8; i += 2) {\n-    __ evpermi2d(xmm(i \/ 2 + 12), xmm(i), xmm(i + 1), Assembler::AVX_512bit);\n-  }\n-\n-  montMul64(xmm12_15, xmm12_15, xmm24_27, xmm4_20_24, _masm);\n-  sub_add(xmm1357, xmm0246, xmm16_19, xmm12_15, _masm);\n-\n-  \/\/ level 6\n-  loadPerm(xmm16_19, perms, nttL6PermsIdx, _masm);\n-  loadPerm(xmm12_15, perms, nttL6PermsIdx + 64, _masm);\n-  load4Xmms(xmm24_27, zetas, 6 * 512, _masm);\n-\n-  for (int i = 0; i < 8; i += 2) {\n-    __ evpermi2d(xmm(i\/2 + 16), xmm(i), xmm(i + 1), Assembler::AVX_512bit);\n-  }\n-  for (int i = 0; i < 8; i += 2) {\n-    __ evpermi2d(xmm(i \/ 2 + 12), xmm(i), xmm(i + 1), Assembler::AVX_512bit);\n-  }\n-\n-  montMul64(xmm12_15, xmm12_15, xmm24_27, xmm4_20_24, _masm);\n-  sub_add(xmm1357, xmm0246, xmm16_19, xmm12_15, _masm);\n-\n-  \/\/ level 7\n-  loadPerm(xmm16_19, perms, nttL7PermsIdx, _masm);\n-  loadPerm(xmm12_15, perms, nttL7PermsIdx + 64, _masm);\n-  load4Xmms(xmm24_27, zetas, 7 * 512, _masm);\n-\n-  for (int i = 0; i < 8; i += 2) {\n-    __ evpermi2d(xmm(i \/ 2 + 16), xmm(i), xmm(i + 1), Assembler::AVX_512bit);\n-  }\n-  for (int i = 0; i < 8; i += 2) {\n-    __ evpermi2d(xmm(i \/ 2 + 12), xmm(i), xmm(i + 1), Assembler::AVX_512bit);\n-  }\n+                  vector_len, scratch); \/\/ q\n+\n+  if (vector_len == Assembler::AVX_512bit) {\n+    \/\/ levels 0-3, register shuffles:\n+    const XMMRegister Coeffs1_1[] = {xmm0, xmm1, xmm2, xmm3};\n+    const XMMRegister Coeffs2_1[] = {xmm16, xmm17, xmm18, xmm19};\n+    const XMMRegister Coeffs3_1[] = {xmm4, xmm5, xmm6, xmm7};\n+    const XMMRegister Coeffs4_1[] = {xmm20, xmm21, xmm22, xmm23};\n+    const XMMRegister Coeffs1_2[] = {xmm0, xmm16, xmm2, xmm18};\n+    const XMMRegister Coeffs2_2[] = {xmm1, xmm17, xmm3, xmm19};\n+    const XMMRegister Coeffs3_2[] = {xmm4, xmm20, xmm6, xmm22};\n+    const XMMRegister Coeffs4_2[] = {xmm5, xmm21, xmm7, xmm23};\n+\n+    \/\/ Constants for shuffle and montMul64\n+    __ mov64(scratch, 0b1010101010101010);\n+    __ kmovwl(mergeMask1, scratch);\n+    __ knotwl(mergeMask2, mergeMask1);\n+    __ vmovdqu(unshuffle1, ExternalAddress(unshufflePermsAddr(0)), vector_len, scratch);\n+    __ vmovdqu(unshuffle2, ExternalAddress(unshufflePermsAddr(1)), vector_len, scratch);\n+\n+    int memStep = 4 * 64; \/\/ 4*64-byte registers\n+    loadXmms(Coeffs1, coeffs, 0*memStep, vector_len, _masm);\n+    loadXmms(Coeffs2, coeffs, 1*memStep, vector_len, _masm);\n+    loadXmms(Coeffs3, coeffs, 2*memStep, vector_len, _masm);\n+    loadXmms(Coeffs4, coeffs, 3*memStep, vector_len, _masm);\n+\n+    \/\/ level 0-3 can be done by shuffling registers (also notice fewer zetas loads, they repeat)\n+    \/\/ level 0 - 128\n+    \/\/ scratch1 = coeffs3 * zetas1\n+    \/\/ coeffs3, coeffs1 = coeffs1±scratch1\n+    \/\/ scratch1 = coeffs4 * zetas1\n+    \/\/ coeffs4, coeffs2 = coeffs2 ± scratch1\n+    __ vmovdqu(Zetas1[0], Address(zetas, 0), vector_len);\n+    montMul64(Scratch1, Coeffs3, Zetas1, Coeffs3, Scratch2);\n+    sub_add(Coeffs3, Coeffs1, Coeffs1, Scratch1, vector_len, _masm);\n+    montMul64(Scratch1, Coeffs4, Zetas1, Coeffs4, Scratch2);\n+    sub_add(Coeffs4, Coeffs2, Coeffs2, Scratch1, vector_len, _masm);\n+\n+    \/\/ level 1 - 64\n+    __ vmovdqu(Zetas1[0], Address(zetas,        512), vector_len);\n+    montMul64(Scratch1, Coeffs2, Zetas1, Coeffs2, Scratch2);\n+    sub_add(Coeffs2, Coeffs1, Coeffs1, Scratch1, vector_len, _masm);\n+\n+    __ vmovdqu(Zetas1[0], Address(zetas, 4*64 + 512), vector_len);\n+    montMul64(Scratch1, Coeffs4, Zetas1, Coeffs4, Scratch2);\n+    sub_add(Coeffs4, Coeffs3, Coeffs3, Scratch1, vector_len, _masm);\n+\n+    \/\/ level 2 - 32\n+    __ vmovdqu(Zetas2[0], Address(zetas,        2 * 512), vector_len);\n+    __ vmovdqu(Zetas2[2], Address(zetas, 2*64 + 2 * 512), vector_len);\n+    montMul64(Scratch1, Coeffs2_1, Zetas2, Coeffs2_1, Scratch2);\n+    sub_add(Coeffs2_1, Coeffs1_1, Coeffs1_1, Scratch1, vector_len, _masm);\n+\n+    __ vmovdqu(Zetas2[0], Address(zetas, 4*64 + 2 * 512), vector_len);\n+    __ vmovdqu(Zetas2[2], Address(zetas, 6*64 + 2 * 512), vector_len);\n+    montMul64(Scratch1, Coeffs4_1, Zetas2, Coeffs4_1, Scratch2);\n+    sub_add(Coeffs4_1, Coeffs3_1, Coeffs3_1, Scratch1, vector_len, _masm);\n+\n+    \/\/ level 3 - 16\n+    loadXmms(Zetas3, zetas, 3 * 512, vector_len, _masm);\n+    montMul64(Scratch1, Coeffs2_2, Zetas3, Coeffs2_2, Scratch2);\n+    sub_add(Coeffs2_2, Coeffs1_2, Coeffs1_2, Scratch1, vector_len, _masm);\n+\n+    loadXmms(Zetas3, zetas, 4*64 + 3 * 512, vector_len, _masm);\n+    montMul64(Scratch1, Coeffs4_2, Zetas3, Coeffs4_2, Scratch2);\n+    sub_add(Coeffs4_2, Coeffs3_2, Coeffs3_2, Scratch1, vector_len, _masm);\n+\n+    for (int level = 4, distance = 8; level<8; level++, distance \/= 2) {\n+      \/\/ zetas = load(level * 512)\n+      \/\/ coeffs1_2, scratch1 = shuffle(coeffs1_2, coeffs2_2)\n+      \/\/ scratch1 = scratch1 * zetas\n+      \/\/ coeffs2_2 = coeffs1_2 - scratch1\n+      \/\/ coeffs1_2 = coeffs1_2 + scratch1\n+      loadXmms(Zetas3, zetas, level * 512, vector_len, _masm);\n+      shuffle(Scratch1, Coeffs1_2, Coeffs2_2, distance * 32); \/\/Coeffs2_2 freed\n+      montMul64(Scratch1, Scratch1, Zetas3, Coeffs2_2, Scratch2, level==7);\n+      sub_add(Coeffs2_2, Coeffs1_2, Coeffs1_2, Scratch1, vector_len, _masm);\n+\n+      loadXmms(Zetas3, zetas, 4*64 + level * 512, vector_len, _masm);\n+      shuffle(Scratch1, Coeffs3_2, Coeffs4_2, distance * 32); \/\/Coeffs4_2 freed\n+      montMul64(Scratch1, Scratch1, Zetas3, Coeffs4_2, Scratch2, level==7);\n+      sub_add(Coeffs4_2, Coeffs3_2, Coeffs3_2, Scratch1, vector_len, _masm);\n+    }\n@@ -422,4 +534,62 @@\n-  montMul64(xmm12_15, xmm12_15, xmm24_27, xmm4_20_24, true, _masm);\n-  loadPerm(xmm0246, perms, nttL7PermsIdx + 2 * XMMBYTES, _masm);\n-  loadPerm(xmm1357, perms, nttL7PermsIdx + 3 * XMMBYTES, _masm);\n-  sub_add(xmm21232527, xmm20222426, xmm16_19, xmm12_15, _masm);\n+    \/\/ Constants for final unshuffle\n+    __ vmovdqu(unshuffle1, ExternalAddress(unshufflePermsAddr(2)), vector_len, scratch);\n+    __ vmovdqu(unshuffle2, ExternalAddress(unshufflePermsAddr(3)), vector_len, scratch);\n+    shuffle(Scratch1, Coeffs1_2, Coeffs2_2, 0);\n+    shuffle(Scratch1, Coeffs3_2, Coeffs4_2, 0);\n+\n+    storeXmms(coeffs, 0*memStep, Coeffs1, vector_len, _masm);\n+    storeXmms(coeffs, 1*memStep, Coeffs2, vector_len, _masm);\n+    storeXmms(coeffs, 2*memStep, Coeffs3, vector_len, _masm);\n+    storeXmms(coeffs, 3*memStep, Coeffs4, vector_len, _masm);\n+  } else { \/\/ Assembler::AVX_256bit\n+    \/\/ levels 0-4, register shuffles:\n+    const XMMRegister Coeffs1_1[] = {xmm0, xmm2};\n+    const XMMRegister Coeffs2_1[] = {xmm1, xmm3};\n+    const XMMRegister Coeffs3_1[] = {xmm4, xmm6};\n+    const XMMRegister Coeffs4_1[] = {xmm5, xmm7};\n+\n+    const XMMRegister Coeffs1_2[] = {xmm0, xmm1, xmm2, xmm3};\n+    const XMMRegister Coeffs2_2[] = {xmm4, xmm5, xmm6, xmm7};\n+\n+    \/\/ Since we cannot fit the entire payload into registers, we process\n+    \/\/ input in two stages. First half, load 8 registers 32 integers each apart.\n+    \/\/ With one load, we can process level 0-2 (128-, 64- and 32-integers apart)\n+    \/\/ Remaining levels, load 8 registers from consecutive memory (16-, 8-, 4-,\n+    \/\/ 2-, 1-integer appart)\n+    \/\/ Levels 5, 6, 7 (4-, 2-, 1-integer appart) require shuffles within registers\n+    \/\/ Other levels, shuffles can be done by re-aranging register order\n+\n+    \/\/ Four batches of 8 registers each, 128 bytes appart\n+    for (int i=0; i<4; i++) {\n+      loadXmms(Coeffs1_2, coeffs, i*32 + 0*128, vector_len, _masm, 4, 128);\n+      loadXmms(Coeffs2_2, coeffs, i*32 + 4*128, vector_len, _masm, 4, 128);\n+\n+      \/\/ level 0-2 can be done by shuffling registers (also notice fewer zetas loads, they repeat)\n+      \/\/ level 0 - 128\n+      __ vmovdqu(Zetas1[0], Address(zetas, 0), vector_len);\n+      montMul64(Scratch1, Coeffs3, Zetas1, Coeffs3, Scratch2);\n+      sub_add(Coeffs3, Coeffs1, Coeffs1, Scratch1, vector_len, _masm);\n+      montMul64(Scratch1, Coeffs4, Zetas1, Coeffs4, Scratch2);\n+      sub_add(Coeffs4, Coeffs2, Coeffs2, Scratch1, vector_len, _masm);\n+\n+      \/\/ level 1 - 64\n+      __ vmovdqu(Zetas1[0], Address(zetas,        512), vector_len);\n+      montMul64(Scratch1, Coeffs2, Zetas1, Coeffs2, Scratch2);\n+      sub_add(Coeffs2, Coeffs1, Coeffs1, Scratch1, vector_len, _masm);\n+\n+      __ vmovdqu(Zetas1[0], Address(zetas, 4*64 + 512), vector_len);\n+      montMul64(Scratch1, Coeffs4, Zetas1, Coeffs4, Scratch2);\n+      sub_add(Coeffs4, Coeffs3, Coeffs3, Scratch1, vector_len, _masm);\n+\n+      \/\/ level 2 - 32\n+      loadXmms(Zetas3, zetas, 2 * 512, vector_len, _masm, 2, 128);\n+      montMul64(Scratch1, Coeffs2_1, Zetas3, Coeffs2_1, Scratch2);\n+      sub_add(Coeffs2_1, Coeffs1_1, Coeffs1_1, Scratch1, vector_len, _masm);\n+\n+      loadXmms(Zetas3, zetas, 4*64 + 2 * 512, vector_len, _masm, 2, 128);\n+      montMul64(Scratch1, Coeffs4_1, Zetas3, Coeffs4_1, Scratch2);\n+      sub_add(Coeffs4_1, Coeffs3_1, Coeffs3_1, Scratch1, vector_len, _masm);\n+\n+      storeXmms(coeffs, i*32 + 0*128, Coeffs1_2, vector_len, _masm, 4, 128);\n+      storeXmms(coeffs, i*32 + 4*128, Coeffs2_2, vector_len, _masm, 4, 128);\n+    }\n@@ -427,3 +597,46 @@\n-  for (int i = 0; i < 8; i += 2) {\n-    __ evpermi2d(xmm(i), xmm(i + 20), xmm(i + 21), Assembler::AVX_512bit);\n-    __ evpermi2d(xmm(i + 1), xmm(i + 20), xmm(i + 21), Assembler::AVX_512bit);\n+    \/\/ Four batches of 8 registers, consecutive loads\n+    for (int i=0; i<4; i++) {\n+      loadXmms(Coeffs1_2, coeffs,       i*256, vector_len, _masm, 4);\n+      loadXmms(Coeffs2_2, coeffs, 128 + i*256, vector_len, _masm, 4);\n+\n+      \/\/ level 3 - 16\n+      __ vmovdqu(Zetas1[0], Address(zetas, i*128 + 3 * 512), vector_len);\n+      montMul64(Scratch1, Coeffs2, Zetas1, Coeffs2, Scratch2);\n+      sub_add(Coeffs2, Coeffs1, Coeffs1, Scratch1, vector_len, _masm);\n+\n+      __ vmovdqu(Zetas1[0], Address(zetas, i*128 + 64 + 3 * 512), vector_len);\n+      montMul64(Scratch1, Coeffs4, Zetas1, Coeffs4, Scratch2);\n+      sub_add(Coeffs4, Coeffs3, Coeffs3, Scratch1, vector_len, _masm);\n+\n+      \/\/ level 4 - 8\n+      loadXmms(Zetas3, zetas, i*128 + 4 * 512, vector_len, _masm);\n+      montMul64(Scratch1, Coeffs2_1, Zetas3, Coeffs2_1, Scratch2);\n+      sub_add(Coeffs2_1, Coeffs1_1, Coeffs1_1, Scratch1, vector_len, _masm);\n+\n+      loadXmms(Zetas3, zetas, i*128 + 64 + 4 * 512, vector_len, _masm);\n+      montMul64(Scratch1, Coeffs4_1, Zetas3, Coeffs4_1, Scratch2);\n+      sub_add(Coeffs4_1, Coeffs3_1, Coeffs3_1, Scratch1, vector_len, _masm);\n+\n+      for (int level = 5, distance = 4; level<8; level++, distance \/= 2) {\n+        \/\/ zetas = load(level * 512)\n+        \/\/ coeffs1_2, scratch1 = shuffle(coeffs1_2, coeffs2_2)\n+        \/\/ scratch1 = scratch1 * zetas\n+        \/\/ coeffs2_2 = coeffs1_2 - scratch1\n+        \/\/ coeffs1_2 = coeffs1_2 + scratch1\n+        loadXmms(Zetas3, zetas, i*128 + level * 512, vector_len, _masm);\n+        shuffle(Scratch1, Coeffs1_1, Coeffs2_1, distance * 32); \/\/Coeffs2_2 freed\n+        montMul64(Scratch1, Scratch1, Zetas3, Coeffs2_1, Scratch2, level==7);\n+        sub_add(Coeffs2_1, Coeffs1_1, Coeffs1_1, Scratch1, vector_len, _masm);\n+\n+        loadXmms(Zetas3, zetas, i*128 + 64 + level * 512, vector_len, _masm);\n+        shuffle(Scratch1, Coeffs3_1, Coeffs4_1, distance * 32); \/\/Coeffs4_2 freed\n+        montMul64(Scratch1, Scratch1, Zetas3, Coeffs4_1, Scratch2, level==7);\n+        sub_add(Coeffs4_1, Coeffs3_1, Coeffs3_1, Scratch1, vector_len, _masm);\n+      }\n+\n+      shuffle(Scratch1, Coeffs1_1, Coeffs2_1, 0);\n+      shuffle(Scratch1, Coeffs3_1, Coeffs4_1, 0);\n+\n+      storeXmms(coeffs,       i*256, Coeffs1_2, vector_len, _masm, 4);\n+      storeXmms(coeffs, 128 + i*256, Coeffs2_2, vector_len, _masm, 4);\n+    }\n@@ -432,18 +645,0 @@\n-  __ cmpl(iterations, 0);\n-  __ jcc(Assembler::equal, L_end);\n-\n-  store4Xmms(coeffs, 0, xmm0_3, _masm);\n-  store4Xmms(coeffs, 4 * XMMBYTES, xmm4_7, _masm);\n-\n-  load4Xmms(xmm0_3, coeffs, 8 * XMMBYTES, _masm);\n-  load4Xmms(xmm4_7, coeffs, 12 * XMMBYTES, _masm);\n-\n-  __ addptr(zetas, 4 * XMMBYTES);\n-\n-  __ jmp(L_loop);\n-\n-  __ BIND(L_end);\n-\n-  store4Xmms(coeffs, 8 * XMMBYTES, xmm0_3, _masm);\n-  store4Xmms(coeffs, 12 * XMMBYTES, xmm4_7, _masm);\n-\n@@ -462,4 +657,3 @@\n-\/\/ zetas (int[256]) = c_rarg1\n-static address generate_dilithiumAlmostInverseNtt_avx512(StubGenerator *stubgen,\n-                                                         MacroAssembler *_masm) {\n-\n+\/\/ zetas (int[128*8]) = c_rarg1\n+static address generate_dilithiumAlmostInverseNtt_avx(StubGenerator *stubgen,\n+                                         int vector_len,MacroAssembler *_masm) {\n@@ -472,2 +666,0 @@\n-  Label L_loop, L_end;\n-\n@@ -476,0 +668,21 @@\n+  const Register scratch = r10;\n+\n+  \/\/ AVX2 version uses the first half of these arrays\n+  const XMMRegister Coeffs1[] = {xmm0, xmm1, xmm16, xmm17};\n+  const XMMRegister Coeffs2[] = {xmm2, xmm3, xmm18, xmm19};\n+  const XMMRegister Coeffs3[] = {xmm4, xmm5, xmm20, xmm21};\n+  const XMMRegister Coeffs4[] = {xmm6, xmm7, xmm22, xmm23};\n+  const XMMRegister Scratch1[] = {xmm8, xmm9, xmm24, xmm25};\n+  const XMMRegister Scratch2[] = {xmm10, xmm11, xmm26, xmm27};\n+  const XMMRegister Zetas1[] = {xmm12, xmm12, xmm12, xmm12};\n+  const XMMRegister Zetas2[] = {xmm12, xmm12, xmm13, xmm13};\n+  const XMMRegister Zetas3[] = {xmm12, xmm13, xmm28, xmm29};\n+  const XMMRegister montQInvModR = xmm14;\n+  const XMMRegister dilithium_q = xmm15;\n+  const XMMRegister unshuffle1 = xmm30;\n+  const XMMRegister unshuffle2 = xmm31;\n+  KRegister mergeMask1 = k1;\n+  KRegister mergeMask2 = k2;\n+  \/\/ lambdas to hide repeated parameters\n+  auto shuffle = whole_shuffle(scratch, mergeMask1, mergeMask2, unshuffle1, unshuffle2, vector_len, _masm);\n+  auto montMul64 = whole_montMul(montQInvModR, dilithium_q, mergeMask2, vector_len, _masm);\n@@ -477,7 +690,0 @@\n-  const Register iterations = c_rarg2;\n-\n-  const Register perms = r11;\n-\n-  __ lea(perms, ExternalAddress(dilithiumAvx512PermsAddr()));\n-\n-  __ evmovdqul(montMulPerm, Address(perms, montMulPermsIdx), Assembler::AVX_512bit);\n@@ -486,1 +692,1 @@\n-                  Assembler::AVX_512bit, scratch); \/\/ q^-1 mod 2^32\n+                  vector_len, scratch); \/\/ q^-1 mod 2^32\n@@ -489,1 +695,1 @@\n-                  Assembler::AVX_512bit, scratch); \/\/ q\n+                  vector_len, scratch); \/\/ q\n@@ -495,3 +701,2 @@\n-  \/\/ the substartion is (Montgomery) multiplied by the corresponding zetas.\n-  \/\/ In each level we just collect the coefficients (using evpermi2d()\n-  \/\/ instructions where necessary, i.e. on levels 0-4) so that the results of\n+  \/\/ the substration is (Montgomery) multiplied by the corresponding zetas.\n+  \/\/ In each level we just shuffle the coefficients so that the results of\n@@ -501,95 +706,46 @@\n-  \/\/ We do levels 0-6 in two batches, each batch entirely in the vector registers\n-  load4Xmms(xmm0_3, coeffs, 0, _masm);\n-  load4Xmms(xmm4_7, coeffs, 4 * XMMBYTES, _masm);\n-\n-  __ movl(iterations, 2);\n-\n-  __ align(OptoLoopAlignment);\n-  __ BIND(L_loop);\n-\n-  __ subl(iterations, 1);\n-\n-  \/\/ level 0\n-  loadPerm(xmm8_11, perms, nttInvL0PermsIdx, _masm);\n-  loadPerm(xmm12_15, perms, nttInvL0PermsIdx + 64, _masm);\n-\n-  for (int i = 0; i < 8; i += 2) {\n-    __ evpermi2d(xmm(i \/ 2 + 8), xmm(i), xmm(i + 1), Assembler::AVX_512bit);\n-    __ evpermi2d(xmm(i \/ 2 + 12), xmm(i), xmm(i + 1), Assembler::AVX_512bit);\n-  }\n-\n-  load4Xmms(xmm4_7, zetas, 0, _masm);\n-  sub_add(xmm24_27, xmm0_3, xmm8_11, xmm12_15, _masm);\n-  montMul64(xmm4_7, xmm4_7, xmm24_27, xmm16_27, true, _masm);\n-\n-  \/\/ level 1\n-  loadPerm(xmm8_11, perms, nttInvL1PermsIdx, _masm);\n-  loadPerm(xmm12_15, perms, nttInvL1PermsIdx + 64, _masm);\n-\n-  for (int i = 0; i < 4; i++) {\n-    __ evpermi2d(xmm(i + 8), xmm(i), xmm(i + 4), Assembler::AVX_512bit);\n-    __ evpermi2d(xmm(i + 12), xmm(i), xmm(i + 4), Assembler::AVX_512bit);\n-  }\n-\n-  load4Xmms(xmm4_7, zetas, 512, _masm);\n-  sub_add(xmm24_27, xmm0_3, xmm8_11, xmm12_15, _masm);\n-  montMul64(xmm4_7, xmm24_27, xmm4_7, xmm16_27, _masm);\n-\n-  \/\/ level 2\n-  loadPerm(xmm8_11, perms, nttInvL2PermsIdx, _masm);\n-  loadPerm(xmm12_15, perms, nttInvL2PermsIdx + 64, _masm);\n-\n-  for (int i = 0; i < 4; i++) {\n-    __ evpermi2d(xmm(i + 8), xmm(i), xmm(i + 4), Assembler::AVX_512bit);\n-    __ evpermi2d(xmm(i + 12), xmm(i), xmm(i + 4), Assembler::AVX_512bit);\n-  }\n-\n-  load4Xmms(xmm4_7, zetas, 2 * 512, _masm);\n-  sub_add(xmm24_27, xmm0_3, xmm8_11, xmm12_15, _masm);\n-  montMul64(xmm4_7, xmm24_27, xmm4_7, xmm16_27, _masm);\n-\n-  \/\/ level 3\n-  loadPerm(xmm8_11, perms, nttInvL3PermsIdx, _masm);\n-  loadPerm(xmm12_15, perms, nttInvL3PermsIdx + 64, _masm);\n-\n-  for (int i = 0; i < 4; i++) {\n-    __ evpermi2d(xmm(i + 8), xmm(i), xmm(i + 4), Assembler::AVX_512bit);\n-    __ evpermi2d(xmm(i + 12), xmm(i), xmm(i + 4), Assembler::AVX_512bit);\n-  }\n-\n-  load4Xmms(xmm4_7, zetas, 3 * 512, _masm);\n-  sub_add(xmm24_27, xmm0_3, xmm8_11, xmm12_15, _masm);\n-  montMul64(xmm4_7, xmm24_27, xmm4_7, xmm16_27, _masm);\n-\n-  \/\/ level 4\n-  loadPerm(xmm8_11, perms, nttInvL4PermsIdx, _masm);\n-  loadPerm(xmm12_15, perms, nttInvL4PermsIdx + 64, _masm);\n-\n-  for (int i = 0; i < 4; i++) {\n-    __ evpermi2d(xmm(i + 8), xmm(i), xmm(i + 4), Assembler::AVX_512bit);\n-    __ evpermi2d(xmm(i + 12), xmm(i), xmm(i + 4), Assembler::AVX_512bit);\n-  }\n-\n-  load4Xmms(xmm4_7, zetas, 4 * 512, _masm);\n-  sub_add(xmm24_27, xmm0_3, xmm8_11, xmm12_15, _masm);\n-  montMul64(xmm4_7, xmm24_27, xmm4_7, xmm16_27, _masm);\n-\n-  \/\/ level 5\n-  load4Xmms(xmm12_15, zetas, 5 * 512, _masm);\n-  sub_add(xmm8_11, xmm0_3, xmm0426, xmm1537, _masm);\n-  montMul64(xmm4_7, xmm8_11, xmm12_15, xmm16_27, _masm);\n-\n-  \/\/ level 6\n-  load4Xmms(xmm12_15, zetas, 6 * 512, _masm);\n-  sub_add(xmm8_11, xmm0_3, xmm0145, xmm2367, _masm);\n-  montMul64(xmm4_7, xmm8_11, xmm12_15, xmm16_27, _masm);\n-\n-  __ cmpl(iterations, 0);\n-  __ jcc(Assembler::equal, L_end);\n-\n-  \/\/ save the coefficients of the first batch, adjust the zetas\n-  \/\/ and load the second batch of coefficients\n-  store4Xmms(coeffs, 0, xmm0_3, _masm);\n-  store4Xmms(coeffs, 4 * XMMBYTES, xmm4_7, _masm);\n-\n-  __ addptr(zetas, 4 * XMMBYTES);\n+  if (vector_len == Assembler::AVX_512bit) {\n+    \/\/ levels 4-7, register shuffles:\n+    const XMMRegister Coeffs1_1[] = {xmm0, xmm1, xmm2, xmm3};\n+    const XMMRegister Coeffs2_1[] = {xmm16, xmm17, xmm18, xmm19};\n+    const XMMRegister Coeffs3_1[] = {xmm4, xmm5, xmm6, xmm7};\n+    const XMMRegister Coeffs4_1[] = {xmm20, xmm21, xmm22, xmm23};\n+    const XMMRegister Coeffs1_2[] = {xmm0, xmm16, xmm2, xmm18};\n+    const XMMRegister Coeffs2_2[] = {xmm1, xmm17, xmm3, xmm19};\n+    const XMMRegister Coeffs3_2[] = {xmm4, xmm20, xmm6, xmm22};\n+    const XMMRegister Coeffs4_2[] = {xmm5, xmm21, xmm7, xmm23};\n+\n+    \/\/ Constants for shuffle and montMul64\n+    __ mov64(scratch, 0b1010101010101010);\n+    __ kmovwl(mergeMask1, scratch);\n+    __ knotwl(mergeMask2, mergeMask1);\n+    __ vmovdqu(unshuffle1, ExternalAddress(unshufflePermsAddr(4)), vector_len, scratch);\n+    __ vmovdqu(unshuffle2, ExternalAddress(unshufflePermsAddr(5)), vector_len, scratch);\n+\n+    int memStep = 4 * 64;\n+    loadXmms(Coeffs1, coeffs, 0*memStep, vector_len, _masm);\n+    loadXmms(Coeffs2, coeffs, 1*memStep, vector_len, _masm);\n+    loadXmms(Coeffs3, coeffs, 2*memStep, vector_len, _masm);\n+    loadXmms(Coeffs4, coeffs, 3*memStep, vector_len, _masm);\n+\n+    shuffle(Scratch1, Coeffs1_2, Coeffs2_2, 1);\n+    shuffle(Scratch1, Coeffs3_2, Coeffs4_2, 1);\n+\n+    \/\/ Constants for shuffle(128)\n+    __ vmovdqu(unshuffle1, ExternalAddress(unshufflePermsAddr(0)), vector_len, scratch);\n+    __ vmovdqu(unshuffle2, ExternalAddress(unshufflePermsAddr(1)), vector_len, scratch);\n+    for (int level = 0, distance = 1; level<4; level++, distance *= 2) {\n+      \/\/ zetas = load(level * 512)\n+      \/\/ coeffs1_2 = coeffs1_2 + coeffs2_2\n+      \/\/ scratch1 = coeffs1_2 - coeffs2_2\n+      \/\/ scratch1 = scratch1 * zetas\n+      \/\/ coeffs1_2, coeffs2_2 = shuffle(coeffs1_2, scratch1)\n+      loadXmms(Zetas3, zetas, level * 512, vector_len, _masm);\n+      sub_add(Scratch1, Coeffs1_2, Coeffs1_2, Coeffs2_2, vector_len, _masm); \/\/ Coeffs2_2 freed\n+      montMul64(Scratch1, Scratch1, Zetas3, Coeffs2_2, Scratch2, level==0);\n+      shuffle(Coeffs2_2, Coeffs1_2, Scratch1, distance * 32);\n+\n+      loadXmms(Zetas3, zetas, 4*64 + level * 512, vector_len, _masm);\n+      sub_add(Scratch1, Coeffs3_2, Coeffs3_2, Coeffs4_2, vector_len, _masm); \/\/ Coeffs4_2 freed\n+      montMul64(Scratch1, Scratch1, Zetas3, Coeffs4_2, Scratch2, level==0);\n+      shuffle(Coeffs4_2, Coeffs3_2, Scratch1, distance * 32);\n+    }\n@@ -597,2 +753,4 @@\n-  load4Xmms(xmm0_3, coeffs, 8 * XMMBYTES, _masm);\n-  load4Xmms(xmm4_7, coeffs, 12 * XMMBYTES, _masm);\n+    \/\/ level 4\n+    loadXmms(Zetas3, zetas, 4 * 512, vector_len, _masm);\n+    sub_add(Scratch1, Coeffs1_2, Coeffs1_2, Coeffs2_2, vector_len, _masm); \/\/ Coeffs2_2 freed\n+    montMul64(Coeffs2_2, Scratch1, Zetas3, Scratch1, Scratch2);\n@@ -600,1 +758,3 @@\n-  __ jmp(L_loop);\n+    loadXmms(Zetas3, zetas, 4*64 + 4 * 512, vector_len, _masm);\n+    sub_add(Scratch1, Coeffs3_2, Coeffs3_2, Coeffs4_2, vector_len, _masm); \/\/ Coeffs4_2 freed\n+    montMul64(Coeffs4_2, Scratch1, Zetas3, Scratch1, Scratch2);\n@@ -602,1 +762,5 @@\n-  __ BIND(L_end);\n+    \/\/ level 5\n+    __ vmovdqu(Zetas2[0], Address(zetas,        5 * 512), vector_len);\n+    __ vmovdqu(Zetas2[2], Address(zetas, 2*64 + 5 * 512), vector_len);\n+    sub_add(Scratch1, Coeffs1_1, Coeffs1_1, Coeffs2_1, vector_len, _masm); \/\/ Coeffs2_1 freed\n+    montMul64(Coeffs2_1, Scratch1, Zetas2, Scratch1, Scratch2);\n@@ -604,5 +768,4 @@\n-  \/\/ load the coeffs of the first batch of coefficients that were saved after\n-  \/\/ level 6 into Zmm_8-Zmm_15 and do the last level entirely in the vector\n-  \/\/ registers\n-  load4Xmms(xmm8_11, coeffs, 0, _masm);\n-  load4Xmms(xmm12_15, coeffs, 4 * XMMBYTES, _masm);\n+    __ vmovdqu(Zetas2[0], Address(zetas, 4*64 + 5 * 512), vector_len);\n+    __ vmovdqu(Zetas2[2], Address(zetas, 6*64 + 5 * 512), vector_len);\n+    sub_add(Scratch1, Coeffs3_1, Coeffs3_1, Coeffs4_1, vector_len, _masm); \/\/ Coeffs4_1 freed\n+    montMul64(Coeffs4_1, Scratch1, Zetas2, Scratch1, Scratch2);\n@@ -610,1 +773,4 @@\n-  \/\/ level 7\n+    \/\/ level 6\n+    __ vmovdqu(Zetas1[0], Address(zetas,        6 * 512), vector_len);\n+    sub_add(Scratch1, Coeffs1, Coeffs1, Coeffs2, vector_len, _masm); \/\/ Coeffs2 freed\n+    montMul64(Coeffs2, Scratch1, Zetas1, Scratch1, Scratch2);\n@@ -612,1 +778,3 @@\n-  loadXmm29(zetas, 7 * 512, _masm);\n+    __ vmovdqu(Zetas1[0], Address(zetas, 4*64 + 6 * 512), vector_len);\n+    sub_add(Scratch1, Coeffs3, Coeffs3, Coeffs4, vector_len, _masm); \/\/ Coeffs4 freed\n+    montMul64(Coeffs4, Scratch1, Zetas1, Scratch1, Scratch2);\n@@ -614,3 +782,67 @@\n-  for (int i = 0; i < 8; i++) {\n-    __ evpaddd(xmm(i + 16), k0, xmm(i), xmm(i + 8), false, Assembler::AVX_512bit);\n-  }\n+    \/\/ level 7\n+    __ vmovdqu(Zetas1[0], Address(zetas, 7 * 512), vector_len);\n+    sub_add(Scratch1, Coeffs1, Coeffs1, Coeffs3, vector_len, _masm); \/\/ Coeffs3 freed\n+    montMul64(Coeffs3, Scratch1, Zetas1, Scratch1, Scratch2);\n+    sub_add(Scratch1, Coeffs2, Coeffs2, Coeffs4, vector_len, _masm); \/\/ Coeffs4 freed\n+    montMul64(Coeffs4, Scratch1, Zetas1, Scratch1, Scratch2);\n+\n+    storeXmms(coeffs, 0*memStep, Coeffs1, vector_len, _masm);\n+    storeXmms(coeffs, 1*memStep, Coeffs2, vector_len, _masm);\n+    storeXmms(coeffs, 2*memStep, Coeffs3, vector_len, _masm);\n+    storeXmms(coeffs, 3*memStep, Coeffs4, vector_len, _masm);\n+  } else { \/\/ Assembler::AVX_256bit\n+    \/\/ Permutations of Coeffs1, Coeffs2, Coeffs3 and Coeffs4\n+    const XMMRegister Coeffs1_1[] = {xmm0, xmm2};\n+    const XMMRegister Coeffs2_1[] = {xmm1, xmm3};\n+    const XMMRegister Coeffs3_1[] = {xmm4, xmm6};\n+    const XMMRegister Coeffs4_1[] = {xmm5, xmm7};\n+\n+    const XMMRegister Coeffs1_2[] = {xmm0, xmm1, xmm2, xmm3};\n+    const XMMRegister Coeffs2_2[] = {xmm4, xmm5, xmm6, xmm7};\n+\n+    \/\/ Four batches of 8 registers, consecutive loads\n+    for (int i=0; i<4; i++) {\n+      loadXmms(Coeffs1_2, coeffs,       i*256, vector_len, _masm, 4);\n+      loadXmms(Coeffs2_2, coeffs, 128 + i*256, vector_len, _masm, 4);\n+\n+      shuffle(Scratch1, Coeffs1_1, Coeffs2_1, 1);\n+      shuffle(Scratch1, Coeffs3_1, Coeffs4_1, 1);\n+\n+      for (int level = 0, distance = 1; level <= 2; level++, distance *= 2) {\n+        \/\/ zetas = load(level * 512)\n+        \/\/ coeffs1_2 = coeffs1_2 + coeffs2_2\n+        \/\/ scratch1 = coeffs1_2 - coeffs2_2\n+        \/\/ scratch1 = scratch1 * zetas\n+        \/\/ coeffs1_2, coeffs2_2 = shuffle(coeffs1_2, scratch1)\n+        loadXmms(Zetas3, zetas, i*128 + level * 512, vector_len, _masm);\n+        sub_add(Scratch1, Coeffs1_1, Coeffs1_1, Coeffs2_1, vector_len, _masm); \/\/ Coeffs2_1 freed\n+        montMul64(Scratch1, Scratch1, Zetas3, Coeffs2_1, Scratch2, level==0);\n+        shuffle(Coeffs2_1, Coeffs1_1, Scratch1, distance * 32);\n+\n+        loadXmms(Zetas3, zetas, i*128 + 64 + level * 512, vector_len, _masm);\n+        sub_add(Scratch1, Coeffs3_1, Coeffs3_1, Coeffs4_1, vector_len, _masm); \/\/ Coeffs4_1 freed\n+        montMul64(Scratch1, Scratch1, Zetas3, Coeffs4_1, Scratch2, level==0);\n+        shuffle(Coeffs4_1, Coeffs3_1, Scratch1, distance * 32);\n+      }\n+\n+      \/\/ level 3\n+      loadXmms(Zetas3, zetas, i*128 + 3 * 512, vector_len, _masm);\n+      sub_add(Scratch1, Coeffs1_1, Coeffs1_1, Coeffs2_1, vector_len, _masm); \/\/ Coeffs2_1 freed\n+      montMul64(Coeffs2_1, Scratch1, Zetas3, Scratch1, Scratch2);\n+\n+      loadXmms(Zetas3, zetas, i*128 + 64 + 3 * 512, vector_len, _masm);\n+      sub_add(Scratch1, Coeffs3_1, Coeffs3_1, Coeffs4_1, vector_len, _masm); \/\/ Coeffs4_1 freed\n+      montMul64(Coeffs4_1, Scratch1, Zetas3, Scratch1, Scratch2);\n+\n+      \/\/ level 4\n+      __ vmovdqu(Zetas1[0], Address(zetas, i*128 + 4 * 512), vector_len);\n+      sub_add(Scratch1, Coeffs1, Coeffs1, Coeffs2, vector_len, _masm); \/\/ Coeffs2 freed\n+      montMul64(Coeffs2, Scratch1, Zetas1, Scratch1, Scratch2);\n+\n+      __ vmovdqu(Zetas1[0], Address(zetas, i*128 + 64 + 4 * 512), vector_len);\n+      sub_add(Scratch1, Coeffs3, Coeffs3, Coeffs4, vector_len, _masm); \/\/ Coeffs4 freed\n+      montMul64(Coeffs4, Scratch1, Zetas1, Scratch1, Scratch2);\n+\n+      storeXmms(coeffs,       i*256, Coeffs1_2, vector_len, _masm, 4);\n+      storeXmms(coeffs, 128 + i*256, Coeffs2_2, vector_len, _masm, 4);\n+    }\n@@ -618,2 +850,33 @@\n-  for (int i = 0; i < 8; i++) {\n-    __ evpsubd(xmm(i), k0, xmm(i + 8), xmm(i), false, Assembler::AVX_512bit);\n+    \/\/ Four batches of 8 registers each, 128 bytes appart\n+    for (int i=0; i<4; i++) {\n+      loadXmms(Coeffs1_2, coeffs, i*32 + 0*128, vector_len, _masm, 4, 128);\n+      loadXmms(Coeffs2_2, coeffs, i*32 + 4*128, vector_len, _masm, 4, 128);\n+\n+      \/\/ level 5\n+      loadXmms(Zetas3, zetas, 5 * 512, vector_len, _masm, 2, 128);\n+      sub_add(Scratch1, Coeffs1_1, Coeffs1_1, Coeffs2_1, vector_len, _masm); \/\/ Coeffs2_1 freed\n+      montMul64(Coeffs2_1, Scratch1, Zetas3, Scratch1, Scratch2);\n+\n+      loadXmms(Zetas3, zetas, 4*64 + 5 * 512, vector_len, _masm, 2, 128);\n+      sub_add(Scratch1, Coeffs3_1, Coeffs3_1, Coeffs4_1, vector_len, _masm); \/\/ Coeffs4_1 freed\n+      montMul64(Coeffs4_1, Scratch1, Zetas3, Scratch1, Scratch2);\n+\n+      \/\/ level 6\n+      __ vmovdqu(Zetas1[0], Address(zetas,        6 * 512), vector_len);\n+      sub_add(Scratch1, Coeffs1, Coeffs1, Coeffs2, vector_len, _masm); \/\/ Coeffs2 freed\n+      montMul64(Coeffs2, Scratch1, Zetas1, Scratch1, Scratch2);\n+\n+      __ vmovdqu(Zetas1[0], Address(zetas, 4*64 + 6 * 512), vector_len);\n+      sub_add(Scratch1, Coeffs3, Coeffs3, Coeffs4, vector_len, _masm); \/\/ Coeffs4 freed\n+      montMul64(Coeffs4, Scratch1, Zetas1, Scratch1, Scratch2);\n+\n+      \/\/ level 7\n+      __ vmovdqu(Zetas1[0], Address(zetas, 7 * 512), vector_len);\n+      sub_add(Scratch1, Coeffs1, Coeffs1, Coeffs3, vector_len, _masm); \/\/ Coeffs3 freed\n+      montMul64(Coeffs3, Scratch1, Zetas1, Scratch1, Scratch2);\n+      sub_add(Scratch1, Coeffs2, Coeffs2, Coeffs4, vector_len, _masm); \/\/ Coeffs4 freed\n+      montMul64(Coeffs4, Scratch1, Zetas1, Scratch1, Scratch2);\n+\n+      storeXmms(coeffs, i*32 + 0*128, Coeffs1_2, vector_len, _masm, 4, 128);\n+      storeXmms(coeffs, i*32 + 4*128, Coeffs2_2, vector_len, _masm, 4, 128);\n+    }\n@@ -622,7 +885,0 @@\n-  store4Xmms(coeffs, 0, xmm16_19, _masm);\n-  store4Xmms(coeffs, 4 * XMMBYTES, xmm20_23, _masm);\n-  montMul64(xmm0_3, xmm0_3, xmm29_29, xmm16_27, _masm);\n-  montMul64(xmm4_7, xmm4_7, xmm29_29, xmm16_27, _masm);\n-  store4Xmms(coeffs, 8 * XMMBYTES, xmm0_3, _masm);\n-  store4Xmms(coeffs, 12 * XMMBYTES, xmm4_7, _masm);\n-\n@@ -644,2 +900,2 @@\n-static address generate_dilithiumNttMult_avx512(StubGenerator *stubgen,\n-                                                MacroAssembler *_masm) {\n+static address generate_dilithiumNttMult_avx(StubGenerator *stubgen,\n+                                     int vector_len, MacroAssembler *_masm) {\n@@ -658,2 +914,1 @@\n-\n-  const Register perms = r10; \/\/ scratch reused after not needed any more\n+  const Register scratch = r10;\n@@ -662,1 +917,11 @@\n-  const XMMRegister montRSquareModQ = xmm29;\n+  const XMMRegister montQInvModR = xmm8;\n+  const XMMRegister dilithium_q = xmm9;\n+\n+  const XMMRegister Poly1[] = {xmm0, xmm1, xmm16, xmm17};\n+  const XMMRegister Poly2[] = {xmm2, xmm3, xmm18, xmm19};\n+  const XMMRegister Scratch1[] = {xmm4, xmm5, xmm20, xmm21};\n+  const XMMRegister Scratch2[] = {xmm6, xmm7, xmm22, xmm23};\n+  const XMMRegister MontRSquareModQ[] = {xmm10, xmm10, xmm10, xmm10};\n+  KRegister mergeMask = k1;\n+  \/\/ lambda to hide repeated parameters\n+  auto montMul64 = whole_montMul(montQInvModR, dilithium_q, mergeMask, vector_len, _masm);\n@@ -666,1 +931,1 @@\n-                  Assembler::AVX_512bit, scratch); \/\/ q^-1 mod 2^32\n+                  vector_len, scratch); \/\/ q^-1 mod 2^32\n@@ -669,2 +934,2 @@\n-                  Assembler::AVX_512bit, scratch); \/\/ q\n-  __ vpbroadcastd(montRSquareModQ,\n+                  vector_len, scratch); \/\/ q\n+  __ vpbroadcastd(MontRSquareModQ[0],\n@@ -672,1 +937,5 @@\n-                  Assembler::AVX_512bit, scratch); \/\/ 2^64 mod q\n+                  vector_len, scratch); \/\/ 2^64 mod q\n+  if (vector_len == Assembler::AVX_512bit) {\n+    __ mov64(scratch, 0b0101010101010101);\n+    __ kmovwl(mergeMask, scratch);\n+  }\n@@ -674,2 +943,9 @@\n-  __ lea(perms, ExternalAddress(dilithiumAvx512PermsAddr()));\n-  __ evmovdqul(montMulPerm, Address(perms, montMulPermsIdx), Assembler::AVX_512bit);\n+  \/\/ Total payload is 256*int32s.\n+  \/\/ - memStep is number of bytes one iteration processes.\n+  \/\/ - loopCnt is number of iterations it will take to process entire payload.\n+  int loopCnt = 4;\n+  int memStep = 4 * 64;\n+  if (vector_len == Assembler::AVX_256bit) {\n+    loopCnt = 16;\n+    memStep = 2 * 32;\n+  }\n@@ -677,1 +953,1 @@\n-  __ movl(len, 4);\n+  __ movl(len, loopCnt);\n@@ -682,5 +958,5 @@\n-  load4Xmms(xmm4_7, poly2, 0, _masm);\n-  load4Xmms(xmm0_3, poly1, 0, _masm);\n-  montMul64(xmm4_7, xmm4_7, xmm29_29, xmm16_27, _masm);\n-  montMul64(xmm0_3, xmm0_3, xmm4_7, xmm16_27, true, _masm);\n-  store4Xmms(result, 0, xmm0_3, _masm);\n+  loadXmms(Poly2, poly2, 0, vector_len, _masm);\n+  loadXmms(Poly1, poly1, 0, vector_len, _masm);\n+  montMul64(Poly2, Poly2, MontRSquareModQ, Scratch1, Scratch2);\n+  montMul64(Poly1, Poly1, Poly2,           Scratch1, Scratch2, true);\n+  storeXmms(result, 0, Poly1, vector_len, _masm);\n@@ -689,3 +965,3 @@\n-  __ addptr(poly1, 4 * XMMBYTES);\n-  __ addptr(poly2, 4 * XMMBYTES);\n-  __ addptr(result, 4 * XMMBYTES);\n+  __ addptr(poly1, memStep);\n+  __ addptr(poly2, memStep);\n+  __ addptr(result, memStep);\n@@ -708,2 +984,2 @@\n-static address generate_dilithiumMontMulByConstant_avx512(StubGenerator *stubgen,\n-                                                          MacroAssembler *_masm) {\n+static address generate_dilithiumMontMulByConstant_avx(StubGenerator *stubgen,\n+                                        int vector_len, MacroAssembler *_masm) {\n@@ -721,2 +997,1 @@\n-\n-  const Register perms = c_rarg2; \/\/ not used for argument\n+  const Register scratch = r10;\n@@ -725,1 +1000,2 @@\n-  const XMMRegister constant = xmm29;\n+  const XMMRegister montQInvModR = xmm8;\n+  const XMMRegister dilithium_q = xmm9;\n@@ -727,1 +1003,9 @@\n-  __ lea(perms, ExternalAddress(dilithiumAvx512PermsAddr()));\n+  const XMMRegister Coeffs1[] = {xmm0, xmm1, xmm16, xmm17};\n+  const XMMRegister Coeffs2[] = {xmm2, xmm3, xmm18, xmm19};\n+  const XMMRegister Scratch1[] = {xmm4, xmm5, xmm20, xmm21};\n+  const XMMRegister Scratch2[] = {xmm6, xmm7, xmm22, xmm23};\n+  const XMMRegister Constant[] = {xmm10, xmm10, xmm10, xmm10};\n+  XMMRegister constant = Constant[0];\n+  KRegister mergeMask = k1;\n+  \/\/ lambda to hide repeated parameters\n+  auto montMul64 = whole_montMul(montQInvModR, dilithium_q, mergeMask, vector_len, _masm);\n@@ -729,1 +1013,1 @@\n-  \/\/ the following four vector registers are used in montMul64\n+  \/\/ load constants for montMul64\n@@ -732,1 +1016,1 @@\n-                  Assembler::AVX_512bit, scratch); \/\/ q^-1 mod 2^32\n+                  vector_len, scratch); \/\/ q^-1 mod 2^32\n@@ -735,3 +1019,21 @@\n-                  Assembler::AVX_512bit, scratch); \/\/ q\n-  __ evmovdqul(montMulPerm, Address(perms, montMulPermsIdx), Assembler::AVX_512bit);\n-  __ evpbroadcastd(constant, rConstant, Assembler::AVX_512bit); \/\/ constant multiplier\n+                  vector_len, scratch); \/\/ q\n+  if (vector_len == Assembler::AVX_256bit) {\n+    __ movdl(constant, rConstant);\n+    __ vpbroadcastd(constant, constant, vector_len); \/\/ constant multiplier\n+  } else {\n+    __ evpbroadcastd(constant, rConstant, Assembler::AVX_512bit); \/\/ constant multiplier\n+\n+    __ mov64(scratch, 0b0101010101010101); \/\/dw-mask\n+    __ kmovwl(mergeMask, scratch);\n+  }\n+\n+  \/\/ Total payload is 256*int32s.\n+  \/\/ - memStep is number of bytes one montMul64 processes.\n+  \/\/ - loopCnt is number of iterations it will take to process entire payload.\n+  \/\/ - (two memSteps per loop)\n+  int memStep = 4 * 64;\n+  int loopCnt = 2;\n+  if (vector_len == Assembler::AVX_256bit) {\n+    memStep = 2 * 32;\n+    loopCnt = 8;\n+  }\n@@ -739,1 +1041,1 @@\n-  __ movl(len, 2);\n+  __ movl(len, loopCnt);\n@@ -744,6 +1046,6 @@\n-  load4Xmms(xmm0_3, coeffs, 0, _masm);\n-  load4Xmms(xmm4_7, coeffs, 4 * XMMBYTES, _masm);\n-  montMul64(xmm0_3, xmm0_3, xmm29_29, xmm16_27, _masm);\n-  montMul64(xmm4_7, xmm4_7, xmm29_29, xmm16_27, _masm);\n-  store4Xmms(coeffs, 0, xmm0_3, _masm);\n-  store4Xmms(coeffs, 4 * XMMBYTES, xmm4_7, _masm);\n+  loadXmms(Coeffs1, coeffs, 0,       vector_len, _masm);\n+  loadXmms(Coeffs2, coeffs, memStep, vector_len, _masm);\n+  montMul64(Coeffs1, Coeffs1, Constant, Scratch1, Scratch2);\n+  montMul64(Coeffs2, Coeffs2, Constant, Scratch1, Scratch2);\n+  storeXmms(coeffs, 0,       Coeffs1, vector_len, _masm);\n+  storeXmms(coeffs, memStep, Coeffs2, vector_len, _masm);\n@@ -752,1 +1054,1 @@\n-  __ addptr(coeffs, 512);\n+  __ addptr(coeffs, 2 * memStep);\n@@ -772,3 +1074,2 @@\n-static address generate_dilithiumDecomposePoly_avx512(StubGenerator *stubgen,\n-                                                      MacroAssembler *_masm) {\n-\n+static address generate_dilithiumDecomposePoly_avx(StubGenerator *stubgen,\n+                                      int vector_len, MacroAssembler *_masm) {\n@@ -788,0 +1089,1 @@\n+  const Register scratch = r10;\n@@ -789,11 +1091,17 @@\n-  const XMMRegister zero = xmm24;\n-  const XMMRegister one = xmm25;\n-  const XMMRegister qMinus1 = xmm26;\n-  const XMMRegister gamma2 = xmm27;\n-  const XMMRegister twoGamma2 = xmm28;\n-  const XMMRegister barrettMultiplier = xmm29;\n-  const XMMRegister barrettAddend = xmm30;\n-\n-  __ vpxor(zero, zero, zero, Assembler::AVX_512bit); \/\/ 0\n-  __ vpternlogd(xmm0, 0xff, xmm0, xmm0, Assembler::AVX_512bit); \/\/ -1\n-  __ vpsubd(one, zero, xmm0, Assembler::AVX_512bit); \/\/ 1\n+\n+  const XMMRegister one = xmm0;\n+  const XMMRegister gamma2 = xmm1;\n+  const XMMRegister twoGamma2 = xmm2;\n+  const XMMRegister barrettMultiplier = xmm3;\n+  const XMMRegister barrettAddend = xmm4;\n+  const XMMRegister dilithium_q = xmm5;\n+  const XMMRegister zero = xmm29;     \/\/ AVX512-only\n+  const XMMRegister minusOne = xmm30; \/\/ AVX512-only\n+  const XMMRegister qMinus1 = xmm31;  \/\/ AVX512-only\n+\n+  XMMRegister RPlus[] = {xmm6, xmm7, xmm16, xmm17};\n+  XMMRegister Quotient[] = {xmm8, xmm9, xmm18, xmm19};\n+  XMMRegister R0[] = {xmm10, xmm11, xmm20, xmm21};\n+  XMMRegister Mask[] = {xmm12, xmm13, xmm22, xmm23};\n+  XMMRegister Tmp1[] = {xmm14, xmm15, xmm24, xmm25};\n+\n@@ -802,1 +1110,1 @@\n-                  Assembler::AVX_512bit, scratch); \/\/ q\n+                  vector_len, scratch); \/\/ q\n@@ -805,1 +1113,13 @@\n-                  Assembler::AVX_512bit, scratch); \/\/ addend for Barrett reduction\n+                  vector_len, scratch); \/\/ addend for Barrett reduction\n+  if (vector_len == Assembler::AVX_512bit) {\n+    __ vpxor(zero, zero, zero, vector_len); \/\/ 0\n+    __ vpternlogd(minusOne, 0xff, minusOne, minusOne, vector_len); \/\/ -1\n+    __ vpsrld(one, minusOne, 31, vector_len);\n+    __ vpsubd(qMinus1, dilithium_q, one, vector_len); \/\/ q - 1\n+    __ evpbroadcastd(twoGamma2, rTwoGamma2, vector_len); \/\/ 2 * gamma2\n+  } else {\n+    __ vpcmpeqd(one, one, one, vector_len);\n+    __ vpsrld(one, one, 31, vector_len);\n+    __ movdl(twoGamma2, rTwoGamma2);\n+    __ vpbroadcastd(twoGamma2, twoGamma2, vector_len); \/\/ 2 * gamma2\n+  }\n@@ -807,1 +1127,1 @@\n-  __ evpbroadcastd(twoGamma2, rTwoGamma2, Assembler::AVX_512bit); \/\/ 2 * gamma2\n+  __ vpsrad(gamma2, twoGamma2, 1, vector_len); \/\/ gamma2\n@@ -816,2 +1136,7 @@\n-  __ evpbroadcastd(barrettMultiplier, rMultiplier,\n-                   Assembler::AVX_512bit); \/\/ multiplier for mod 2 * gamma2 reduce\n+  if (vector_len == Assembler::AVX_512bit) {\n+    __ evpbroadcastd(barrettMultiplier, rMultiplier,\n+                  vector_len); \/\/ multiplier for mod 2 * gamma2 reduce\n+  } else {\n+    __ movdl(barrettMultiplier, rMultiplier);\n+    __ vpbroadcastd(barrettMultiplier, barrettMultiplier, vector_len);\n+  }\n@@ -819,2 +1144,7 @@\n-  __ evpsubd(qMinus1, k0, dilithium_q, one, false, Assembler::AVX_512bit); \/\/ q - 1\n-  __ evpsrad(gamma2, k0, twoGamma2, 1, false, Assembler::AVX_512bit); \/\/ gamma2\n+  \/\/ Total payload is 1024 bytes\n+  int memStep = 4 * 64; \/\/ Number of bytes per loop iteration\n+  int regCnt = 4; \/\/ Register array length\n+  if (vector_len == Assembler::AVX_256bit) {\n+    memStep = 2 * 32;\n+    regCnt = 2;\n+  }\n@@ -827,1 +1157,1 @@\n-  load4Xmms(xmm0_3, input, 0, _masm);\n+  loadXmms(RPlus, input, 0, vector_len, _masm);\n@@ -829,1 +1159,1 @@\n-  __ addptr(input, 4 * XMMBYTES);\n+  __ addptr(input, memStep);\n@@ -831,1 +1161,0 @@\n-  \/\/ rplus in xmm0\n@@ -833,20 +1162,16 @@\n-  __ evpaddd(xmm4, k0, xmm0, barrettAddend, false, Assembler::AVX_512bit);\n-  __ evpaddd(xmm5, k0, xmm1, barrettAddend, false, Assembler::AVX_512bit);\n-  __ evpaddd(xmm6, k0, xmm2, barrettAddend, false, Assembler::AVX_512bit);\n-  __ evpaddd(xmm7, k0, xmm3, barrettAddend, false, Assembler::AVX_512bit);\n-\n-  __ evpsrad(xmm4, k0, xmm4, 23, false, Assembler::AVX_512bit);\n-  __ evpsrad(xmm5, k0, xmm5, 23, false, Assembler::AVX_512bit);\n-  __ evpsrad(xmm6, k0, xmm6, 23, false, Assembler::AVX_512bit);\n-  __ evpsrad(xmm7, k0, xmm7, 23, false, Assembler::AVX_512bit);\n-\n-  __ evpmulld(xmm4, k0, xmm4, dilithium_q, false, Assembler::AVX_512bit);\n-  __ evpmulld(xmm5, k0, xmm5, dilithium_q, false, Assembler::AVX_512bit);\n-  __ evpmulld(xmm6, k0, xmm6, dilithium_q, false, Assembler::AVX_512bit);\n-  __ evpmulld(xmm7, k0, xmm7, dilithium_q, false, Assembler::AVX_512bit);\n-\n-  __ evpsubd(xmm0, k0, xmm0, xmm4, false, Assembler::AVX_512bit);\n-  __ evpsubd(xmm1, k0, xmm1, xmm5, false, Assembler::AVX_512bit);\n-  __ evpsubd(xmm2, k0, xmm2, xmm6, false, Assembler::AVX_512bit);\n-  __ evpsubd(xmm3, k0, xmm3, xmm7, false, Assembler::AVX_512bit);\n-  \/\/ rplus in xmm0\n+  for (int i = 0; i < regCnt; i++) {\n+    __ vpaddd(Tmp1[i], RPlus[i], barrettAddend, vector_len);\n+  }\n+\n+  for (int i = 0; i < regCnt; i++) {\n+    __ vpsrad(Tmp1[i], Tmp1[i], 23, vector_len);\n+  }\n+\n+  for (int i = 0; i < regCnt; i++) {\n+    __ vpmulld(Tmp1[i], Tmp1[i], dilithium_q, vector_len);\n+  }\n+\n+  for (int i = 0; i < regCnt; i++) {\n+    __ vpsubd(RPlus[i], RPlus[i], Tmp1[i], vector_len);\n+  }\n+\n@@ -854,15 +1179,12 @@\n-  __ evpsrad(xmm4, k0, xmm0, 31, false, Assembler::AVX_512bit);\n-  __ evpsrad(xmm5, k0, xmm1, 31, false, Assembler::AVX_512bit);\n-  __ evpsrad(xmm6, k0, xmm2, 31, false, Assembler::AVX_512bit);\n-  __ evpsrad(xmm7, k0, xmm3, 31, false, Assembler::AVX_512bit);\n-\n-  __ evpandd(xmm4, k0, xmm4, dilithium_q, false, Assembler::AVX_512bit);\n-  __ evpandd(xmm5, k0, xmm5, dilithium_q, false, Assembler::AVX_512bit);\n-  __ evpandd(xmm6, k0, xmm6, dilithium_q, false, Assembler::AVX_512bit);\n-  __ evpandd(xmm7, k0, xmm7, dilithium_q, false, Assembler::AVX_512bit);\n-\n-  __ evpaddd(xmm0, k0, xmm0, xmm4, false, Assembler::AVX_512bit);\n-  __ evpaddd(xmm1, k0, xmm1, xmm5, false, Assembler::AVX_512bit);\n-  __ evpaddd(xmm2, k0, xmm2, xmm6, false, Assembler::AVX_512bit);\n-  __ evpaddd(xmm3, k0, xmm3, xmm7, false, Assembler::AVX_512bit);\n-  \/\/ rplus in xmm0\n+  for (int i = 0; i < regCnt; i++) {\n+    __ vpsrad(Tmp1[i], RPlus[i], 31, vector_len);\n+  }\n+\n+  for (int i = 0; i < regCnt; i++) {\n+    __ vpand(Tmp1[i], Tmp1[i], dilithium_q, vector_len);\n+  }\n+\n+  for (int i = 0; i < regCnt; i++) {\n+    __ vpaddd(RPlus[i], RPlus[i], Tmp1[i], vector_len);\n+  }\n+\n@@ -870,10 +1192,8 @@\n-  __ evpmulld(xmm4, k0, xmm0, barrettMultiplier, false, Assembler::AVX_512bit);\n-  __ evpmulld(xmm5, k0, xmm1, barrettMultiplier, false, Assembler::AVX_512bit);\n-  __ evpmulld(xmm6, k0, xmm2, barrettMultiplier, false, Assembler::AVX_512bit);\n-  __ evpmulld(xmm7, k0, xmm3, barrettMultiplier, false, Assembler::AVX_512bit);\n-\n-  __ evpsrad(xmm4, k0, xmm4, 22, false, Assembler::AVX_512bit);\n-  __ evpsrad(xmm5, k0, xmm5, 22, false, Assembler::AVX_512bit);\n-  __ evpsrad(xmm6, k0, xmm6, 22, false, Assembler::AVX_512bit);\n-  __ evpsrad(xmm7, k0, xmm7, 22, false, Assembler::AVX_512bit);\n-  \/\/ quotient in xmm4\n+  for (int i = 0; i < regCnt; i++) {\n+    __ vpmulld(Quotient[i], RPlus[i], barrettMultiplier, vector_len);\n+  }\n+\n+  for (int i = 0; i < regCnt; i++) {\n+    __ vpsrad(Quotient[i], Quotient[i], 22, vector_len);\n+  }\n+\n@@ -881,10 +1201,8 @@\n-  __ evpmulld(xmm8, k0, xmm4, twoGamma2, false, Assembler::AVX_512bit);\n-  __ evpmulld(xmm9, k0, xmm5, twoGamma2, false, Assembler::AVX_512bit);\n-  __ evpmulld(xmm10, k0, xmm6, twoGamma2, false, Assembler::AVX_512bit);\n-  __ evpmulld(xmm11, k0, xmm7, twoGamma2, false, Assembler::AVX_512bit);\n-\n-  __ evpsubd(xmm8, k0, xmm0, xmm8, false, Assembler::AVX_512bit);\n-  __ evpsubd(xmm9, k0, xmm1, xmm9, false, Assembler::AVX_512bit);\n-  __ evpsubd(xmm10, k0, xmm2, xmm10, false, Assembler::AVX_512bit);\n-  __ evpsubd(xmm11, k0, xmm3, xmm11, false, Assembler::AVX_512bit);\n-  \/\/ r0 in xmm8\n+  for (int i = 0; i < regCnt; i++) {\n+    __ vpmulld(R0[i], Quotient[i], twoGamma2, vector_len);\n+  }\n+\n+  for (int i = 0; i < regCnt; i++) {\n+    __ vpsubd(R0[i], RPlus[i], R0[i], vector_len);\n+  }\n+\n@@ -892,10 +1210,8 @@\n-  __ evpsubd(xmm12, k0, twoGamma2, xmm8, false, Assembler::AVX_512bit);\n-  __ evpsubd(xmm13, k0, twoGamma2, xmm9, false, Assembler::AVX_512bit);\n-  __ evpsubd(xmm14, k0, twoGamma2, xmm10, false, Assembler::AVX_512bit);\n-  __ evpsubd(xmm15, k0, twoGamma2, xmm11, false, Assembler::AVX_512bit);\n-\n-  __ evpsrad(xmm12, k0, xmm12, 22, false, Assembler::AVX_512bit);\n-  __ evpsrad(xmm13, k0, xmm13, 22, false, Assembler::AVX_512bit);\n-  __ evpsrad(xmm14, k0, xmm14, 22, false, Assembler::AVX_512bit);\n-  __ evpsrad(xmm15, k0, xmm15, 22, false, Assembler::AVX_512bit);\n-  \/\/ mask in xmm12\n+  for (int i = 0; i < regCnt; i++) {\n+    __ vpsubd(Mask[i], twoGamma2, R0[i], vector_len);\n+  }\n+\n+  for (int i = 0; i < regCnt; i++) {\n+    __ vpsrad(Mask[i], Mask[i], 22, vector_len);\n+  }\n+\n@@ -903,10 +1219,8 @@\n-  __ evpandd(xmm16, k0, xmm12, twoGamma2, false, Assembler::AVX_512bit);\n-  __ evpandd(xmm17, k0, xmm13, twoGamma2, false, Assembler::AVX_512bit);\n-  __ evpandd(xmm18, k0, xmm14, twoGamma2, false, Assembler::AVX_512bit);\n-  __ evpandd(xmm19, k0, xmm15, twoGamma2, false, Assembler::AVX_512bit);\n-\n-  __ evpsubd(xmm8, k0, xmm8, xmm16, false, Assembler::AVX_512bit);\n-  __ evpsubd(xmm9, k0, xmm9, xmm17, false, Assembler::AVX_512bit);\n-  __ evpsubd(xmm10, k0, xmm10, xmm18, false, Assembler::AVX_512bit);\n-  __ evpsubd(xmm11, k0, xmm11, xmm19, false, Assembler::AVX_512bit);\n-  \/\/ r0 in xmm8\n+  for (int i = 0; i < regCnt; i++) {\n+    __ vpand(Tmp1[i], Mask[i], twoGamma2, vector_len);\n+  }\n+\n+  for (int i = 0; i < regCnt; i++) {\n+    __ vpsubd(R0[i], R0[i], Tmp1[i], vector_len);\n+  }\n+\n@@ -914,4 +1228,3 @@\n-  __ evpandd(xmm16, k0, xmm12, one, false, Assembler::AVX_512bit);\n-  __ evpandd(xmm17, k0, xmm13, one, false, Assembler::AVX_512bit);\n-  __ evpandd(xmm18, k0, xmm14, one, false, Assembler::AVX_512bit);\n-  __ evpandd(xmm19, k0, xmm15, one, false, Assembler::AVX_512bit);\n+  for (int i = 0; i < regCnt; i++) {\n+    __ vpand(Tmp1[i], Mask[i], one, vector_len);\n+  }\n@@ -919,4 +1232,3 @@\n-  __ evpaddd(xmm4, k0, xmm4, xmm16, false, Assembler::AVX_512bit);\n-  __ evpaddd(xmm5, k0, xmm5, xmm17, false, Assembler::AVX_512bit);\n-  __ evpaddd(xmm6, k0, xmm6, xmm18, false, Assembler::AVX_512bit);\n-  __ evpaddd(xmm7, k0, xmm7, xmm19, false, Assembler::AVX_512bit);\n+  for (int i = 0; i < regCnt; i++) {\n+    __ vpaddd(Quotient[i], Quotient[i], Tmp1[i], vector_len);\n+  }\n@@ -925,4 +1237,3 @@\n-  __ evpsubd(xmm12, k0, gamma2, xmm8, false, Assembler::AVX_512bit);\n-  __ evpsubd(xmm13, k0, gamma2, xmm9, false, Assembler::AVX_512bit);\n-  __ evpsubd(xmm14, k0, gamma2, xmm10, false, Assembler::AVX_512bit);\n-  __ evpsubd(xmm15, k0, gamma2, xmm11, false, Assembler::AVX_512bit);\n+  for (int i = 0; i < regCnt; i++) {\n+    __ vpsubd(Mask[i], gamma2, R0[i], vector_len);\n+  }\n@@ -930,4 +1241,3 @@\n-  __ evpsrad(xmm12, k0, xmm12, 31, false, Assembler::AVX_512bit);\n-  __ evpsrad(xmm13, k0, xmm13, 31, false, Assembler::AVX_512bit);\n-  __ evpsrad(xmm14, k0, xmm14, 31, false, Assembler::AVX_512bit);\n-  __ evpsrad(xmm15, k0, xmm15, 31, false, Assembler::AVX_512bit);\n+  for (int i = 0; i < regCnt; i++) {\n+    __ vpsrad(Mask[i], Mask[i], 31, vector_len);\n+  }\n@@ -936,10 +1246,8 @@\n-  __ evpandd(xmm16, k0, xmm12, twoGamma2, false, Assembler::AVX_512bit);\n-  __ evpandd(xmm17, k0, xmm13, twoGamma2, false, Assembler::AVX_512bit);\n-  __ evpandd(xmm18, k0, xmm14, twoGamma2, false, Assembler::AVX_512bit);\n-  __ evpandd(xmm19, k0, xmm15, twoGamma2, false, Assembler::AVX_512bit);\n-\n-  __ evpsubd(xmm8, k0, xmm8, xmm16, false, Assembler::AVX_512bit);\n-  __ evpsubd(xmm9, k0, xmm9, xmm17, false, Assembler::AVX_512bit);\n-  __ evpsubd(xmm10, k0, xmm10, xmm18, false, Assembler::AVX_512bit);\n-  __ evpsubd(xmm11, k0, xmm11, xmm19, false, Assembler::AVX_512bit);\n-  \/\/ r0 in xmm8\n+  for (int i = 0; i < regCnt; i++) {\n+    __ vpand(Tmp1[i], Mask[i], twoGamma2, vector_len);\n+  }\n+\n+  for (int i = 0; i < regCnt; i++) {\n+    __ vpsubd(R0[i], R0[i], Tmp1[i], vector_len);\n+  }\n+\n@@ -947,10 +1255,8 @@\n-  __ evpandd(xmm16, k0, xmm12, one, false, Assembler::AVX_512bit);\n-  __ evpandd(xmm17, k0, xmm13, one, false, Assembler::AVX_512bit);\n-  __ evpandd(xmm18, k0, xmm14, one, false, Assembler::AVX_512bit);\n-  __ evpandd(xmm19, k0, xmm15, one, false, Assembler::AVX_512bit);\n-\n-  __ evpaddd(xmm4, k0, xmm4, xmm16, false, Assembler::AVX_512bit);\n-  __ evpaddd(xmm5, k0, xmm5, xmm17, false, Assembler::AVX_512bit);\n-  __ evpaddd(xmm6, k0, xmm6, xmm18, false, Assembler::AVX_512bit);\n-  __ evpaddd(xmm7, k0, xmm7, xmm19, false, Assembler::AVX_512bit);\n-  \/\/ quotient in xmm4\n+  for (int i = 0; i < regCnt; i++) {\n+    __ vpand(Tmp1[i], Mask[i], one, vector_len);\n+  }\n+\n+  for (int i = 0; i < regCnt; i++) {\n+    __ vpaddd(Quotient[i], Quotient[i], Tmp1[i], vector_len);\n+  }\n+  \/\/ r1 in RPlus\n@@ -958,10 +1264,4 @@\n-  __ evpsubd(xmm16, k0, xmm0, xmm8, false, Assembler::AVX_512bit);\n-  __ evpsubd(xmm17, k0, xmm1, xmm9, false, Assembler::AVX_512bit);\n-  __ evpsubd(xmm18, k0, xmm2, xmm10, false, Assembler::AVX_512bit);\n-  __ evpsubd(xmm19, k0, xmm3, xmm11, false, Assembler::AVX_512bit);\n-\n-  __ evpsubd(xmm16, k0, xmm16, xmm26, false, Assembler::AVX_512bit);\n-  __ evpsubd(xmm17, k0, xmm17, xmm26, false, Assembler::AVX_512bit);\n-  __ evpsubd(xmm18, k0, xmm18, xmm26, false, Assembler::AVX_512bit);\n-  __ evpsubd(xmm19, k0, xmm19, xmm26, false, Assembler::AVX_512bit);\n-  \/\/ r1 in xmm16\n+  for (int i = 0; i < regCnt; i++) {\n+    __ vpsubd(RPlus[i], RPlus[i], R0[i], vector_len);\n+  }\n+\n@@ -969,34 +1269,39 @@\n-  __ evpsubd(xmm20, k0, zero, xmm16, false, Assembler::AVX_512bit);\n-  __ evpsubd(xmm21, k0, zero, xmm17, false, Assembler::AVX_512bit);\n-  __ evpsubd(xmm22, k0, zero, xmm18, false, Assembler::AVX_512bit);\n-  __ evpsubd(xmm23, k0, zero, xmm19, false, Assembler::AVX_512bit);\n-\n-  __ evporq(xmm16, k0, xmm16, xmm20, false, Assembler::AVX_512bit);\n-  __ evporq(xmm17, k0, xmm17, xmm21, false, Assembler::AVX_512bit);\n-  __ evporq(xmm18, k0, xmm18, xmm22, false, Assembler::AVX_512bit);\n-  __ evporq(xmm19, k0, xmm19, xmm23, false, Assembler::AVX_512bit);\n-\n-  __ evpsubd(xmm12, k0, zero, one, false, Assembler::AVX_512bit); \/\/ -1\n-\n-  __ evpsrad(xmm0, k0, xmm16, 31, false, Assembler::AVX_512bit);\n-  __ evpsrad(xmm1, k0, xmm17, 31, false, Assembler::AVX_512bit);\n-  __ evpsrad(xmm2, k0, xmm18, 31, false, Assembler::AVX_512bit);\n-  __ evpsrad(xmm3, k0, xmm19, 31, false, Assembler::AVX_512bit);\n-  \/\/ r1 in xmm0\n-  \/\/ r0 += ~r1;\n-  __ evpxorq(xmm20, k0, xmm0, xmm12, false, Assembler::AVX_512bit);\n-  __ evpxorq(xmm21, k0, xmm1, xmm12, false, Assembler::AVX_512bit);\n-  __ evpxorq(xmm22, k0, xmm2, xmm12, false, Assembler::AVX_512bit);\n-  __ evpxorq(xmm23, k0, xmm3, xmm12, false, Assembler::AVX_512bit);\n-\n-  __ evpaddd(xmm8, k0, xmm8, xmm20, false, Assembler::AVX_512bit);\n-  __ evpaddd(xmm9, k0, xmm9, xmm21, false, Assembler::AVX_512bit);\n-  __ evpaddd(xmm10, k0, xmm10, xmm22, false, Assembler::AVX_512bit);\n-  __ evpaddd(xmm11, k0, xmm11, xmm23, false, Assembler::AVX_512bit);\n-  \/\/ r0 in xmm8\n-  \/\/ r1 = r1 & quotient;\n-  __ evpandd(xmm0, k0, xmm4, xmm0, false, Assembler::AVX_512bit);\n-  __ evpandd(xmm1, k0, xmm5, xmm1, false, Assembler::AVX_512bit);\n-  __ evpandd(xmm2, k0, xmm6, xmm2, false, Assembler::AVX_512bit);\n-  __ evpandd(xmm3, k0, xmm7, xmm3, false, Assembler::AVX_512bit);\n-  \/\/ r1 in xmm0\n+  if (vector_len == Assembler::AVX_512bit) {\n+    KRegister EqMsk[] = {k1, k2, k3, k4};\n+    for (int i = 0; i < regCnt; i++) {\n+      __ evpcmpeqd(EqMsk[i], k0, RPlus[i], qMinus1, vector_len);\n+    }\n+\n+    \/\/ r0 += ~r1; \/\/ add -1 or keep as is, using EqMsk as filter\n+    for (int i = 0; i < regCnt; i++) {\n+      __ evpaddd(R0[i], EqMsk[i], R0[i], minusOne, true, vector_len);\n+    }\n+\n+    \/\/ r1 in Quotient\n+    \/\/ r1 = r1 & quotient; \/\/ copy 0 or keep as is, using EqMsk as filter\n+    for (int i = 0; i < regCnt; i++) {\n+      \/\/ FIXME: replace with void evmovdqul(Address dst, KRegister mask, XMMRegister src, bool merge, int vector_len);?\n+      __ evpandd(Quotient[i], EqMsk[i], Quotient[i], zero, true, vector_len);\n+    }\n+  } else {\n+    const XMMRegister qMinus1 = Tmp1[0];\n+    __ vpsubd(qMinus1, dilithium_q, one, vector_len); \/\/ q - 1\n+\n+    for (int i = 0; i < regCnt; i++) {\n+      __ vpcmpeqd(Mask[i], RPlus[i], qMinus1, vector_len);\n+    }\n+\n+    \/\/ r0 += ~r1;\n+    \/\/ Mask already negated\n+    for (int i = 0; i < regCnt; i++) {\n+      __ vpaddd(R0[i], R0[i], Mask[i], vector_len);\n+    }\n+\n+    \/\/ r1 in Quotient\n+    \/\/ r1 = r1 & quotient;\n+    for (int i = 0; i < regCnt; i++) {\n+      __ vpandn(Quotient[i], Mask[i], Quotient[i], vector_len);\n+    }\n+  }\n+\n+  \/\/ r1 in Quotient\n@@ -1005,2 +1310,2 @@\n-  store4Xmms(highPart, 0, xmm0_3, _masm);\n-  store4Xmms(lowPart, 0, xmm8_11, _masm);\n+  storeXmms(highPart, 0, Quotient, vector_len, _masm);\n+  storeXmms(lowPart, 0, R0, vector_len, _masm);\n@@ -1008,3 +1313,3 @@\n-  __ addptr(highPart, 4 * XMMBYTES);\n-  __ addptr(lowPart, 4 * XMMBYTES);\n-  __ subl(len, 4 * XMMBYTES);\n+  __ addptr(highPart, memStep);\n+  __ addptr(lowPart, memStep);\n+  __ subl(len, memStep);\n@@ -1021,0 +1326,4 @@\n+  int vector_len = Assembler::AVX_256bit;\n+  if (VM_Version::supports_evex() && VM_Version::supports_avx512bw()) {\n+    vector_len = Assembler::AVX_512bit;\n+  }\n@@ -1023,10 +1332,10 @@\n-      StubRoutines::_dilithiumAlmostNtt =\n-        generate_dilithiumAlmostNtt_avx512(this, _masm);\n-      StubRoutines::_dilithiumAlmostInverseNtt =\n-        generate_dilithiumAlmostInverseNtt_avx512(this, _masm);\n-      StubRoutines::_dilithiumNttMult =\n-        generate_dilithiumNttMult_avx512(this, _masm);\n-      StubRoutines::_dilithiumMontMulByConstant =\n-        generate_dilithiumMontMulByConstant_avx512(this, _masm);\n-      StubRoutines::_dilithiumDecomposePoly =\n-        generate_dilithiumDecomposePoly_avx512(this, _masm);\n+    StubRoutines::_dilithiumAlmostNtt =\n+        generate_dilithiumAlmostNtt_avx(this, vector_len, _masm);\n+    StubRoutines::_dilithiumAlmostInverseNtt =\n+        generate_dilithiumAlmostInverseNtt_avx(this, vector_len, _masm);\n+    StubRoutines::_dilithiumNttMult =\n+        generate_dilithiumNttMult_avx(this, vector_len, _masm);\n+    StubRoutines::_dilithiumMontMulByConstant =\n+        generate_dilithiumMontMulByConstant_avx(this, vector_len, _masm);\n+    StubRoutines::_dilithiumDecomposePoly =\n+        generate_dilithiumDecomposePoly_avx(this, vector_len, _masm);\n","filename":"src\/hotspot\/cpu\/x86\/stubGenerator_x86_64_dilithium.cpp","additions":1009,"deletions":700,"binary":false,"changes":1709,"status":"modified"},{"patch":"@@ -1274,2 +1274,1 @@\n-  \/\/ Currently we only have them for AVX512\n-  if (supports_evex() && supports_avx512bw()) {\n+  if (UseAVX > 1) {\n","filename":"src\/hotspot\/cpu\/x86\/vm_version_x86.cpp","additions":1,"deletions":2,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -0,0 +1,516 @@\n+\/*\n+ * Copyright (c) 2025, Intel Corporation. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+import java.util.Arrays;\n+import java.util.Random;\n+import java.lang.invoke.MethodHandle;\n+import java.lang.invoke.MethodHandles;\n+import java.lang.reflect.Method;\n+import java.util.HexFormat;\n+\n+\/*\n+ * @test\n+ * @library \/test\/lib\n+ * @key randomness\n+ * @modules java.base\/sun.security.provider:+open\n+ * @run main\/othervm ML_DSA_Intrinsic_Test -XX:+UnlockDiagnosticVMOptions -XX:-UseDilithiumIntrinsics\n+ *\/\n+\/*\n+ * @test\n+ * @library \/test\/lib\n+ * @key randomness\n+ * @modules java.base\/sun.security.provider:+open\n+ * @run main\/othervm -XX:UseAVX=2 ML_DSA_Intrinsic_Test\n+ *\/\n+\/*\n+ * @test\n+ * @library \/test\/lib\n+ * @key randomness\n+ * @modules java.base\/sun.security.provider:+open\n+ * @run main ML_DSA_Intrinsic_Test\n+ *\/\n+\n+\/\/ To run manually: java --add-opens java.base\/sun.security.provider=ALL-UNNAMED --add-exports java.base\/sun.security.provider=ALL-UNNAMED\n+\/\/  -XX:+UnlockDiagnosticVMOptions -XX:+UseDilithiumIntrinsics test\/jdk\/sun\/security\/provider\/acvp\/ML_DSA_Intrinsic_Test.java\n+\n+public class ML_DSA_Intrinsic_Test {\n+    public static void main(String[] args) throws Exception {\n+        MethodHandles.Lookup lookup = MethodHandles.lookup();\n+        Class<?> kClazz = sun.security.provider.ML_DSA.class;\n+\n+        Method m = kClazz.getDeclaredMethod(\"implDilithiumNttMult\",\n+                int[].class, int[].class, int[].class);\n+        m.setAccessible(true);\n+        MethodHandle mult = lookup.unreflect(m);\n+\n+        m = kClazz.getDeclaredMethod(\"implDilithiumNttMultJava\",\n+                int[].class, int[].class, int[].class);\n+        m.setAccessible(true);\n+        MethodHandle multJava = lookup.unreflect(m);\n+\n+        m = kClazz.getDeclaredMethod(\"implDilithiumMontMulByConstant\",\n+                int[].class, int.class);\n+        m.setAccessible(true);\n+        MethodHandle multConst = lookup.unreflect(m);\n+\n+        m = kClazz.getDeclaredMethod(\"implDilithiumMontMulByConstantJava\",\n+                int[].class, int.class);\n+        m.setAccessible(true);\n+        MethodHandle multConstJava = lookup.unreflect(m);\n+\n+        m = kClazz.getDeclaredMethod(\"implDilithiumDecomposePoly\",\n+                int[].class, int[].class, int[].class, int.class, int.class);\n+        m.setAccessible(true);\n+        MethodHandle decompose = lookup.unreflect(m);\n+\n+        m = kClazz.getDeclaredMethod(\"decomposePolyJava\",\n+                int[].class, int[].class, int[].class, int.class, int.class);\n+        m.setAccessible(true);\n+        MethodHandle decomposeJava = lookup.unreflect(m);\n+\n+        m = kClazz.getDeclaredMethod(\"implDilithiumAlmostNtt\",\n+        int[].class, int[].class);\n+        m.setAccessible(true);\n+        MethodHandle almostNtt = lookup.unreflect(m);\n+\n+        m = kClazz.getDeclaredMethod(\"implDilithiumAlmostNttJava\",\n+                int[].class);\n+        m.setAccessible(true);\n+        MethodHandle almostNttJava = lookup.unreflect(m);\n+\n+        m = kClazz.getDeclaredMethod(\"implDilithiumAlmostInverseNtt\",\n+        int[].class, int[].class);\n+        m.setAccessible(true);\n+        MethodHandle inverseNtt = lookup.unreflect(m);\n+\n+        m = kClazz.getDeclaredMethod(\"implDilithiumAlmostInverseNttJava\",\n+                int[].class);\n+        m.setAccessible(true);\n+        MethodHandle inverseNttJava = lookup.unreflect(m);\n+\n+        \/\/ Hint: if test fails, you can hardcode the seed to make the test more reproducible\n+        Random rnd = new Random();\n+        long seed = rnd.nextLong();\n+        rnd.setSeed(seed);\n+        \/\/Note: it might be useful to increase this number during development of new intrinsics\n+        final int repeat = 10000000;\n+        int[] coeffs1 = new int[ML_DSA_N];\n+        int[] coeffs2 = new int[ML_DSA_N];\n+        int[] prod1 = new int[ML_DSA_N];\n+        int[] prod2 = new int[ML_DSA_N];\n+        int[] prod3 = new int[ML_DSA_N];\n+        int[] prod4 = new int[ML_DSA_N];\n+        try {\n+            for (int i = 0; i < repeat; i++) {\n+                \/\/ Hint: if test fails, you can hardcode the seed to make the test more reproducible:\n+                \/\/ rnd.setSeed(seed);\n+                testMult(prod1, prod2, coeffs1, coeffs2, mult, multJava, rnd, seed, i);\n+                testMultConst(prod1, prod2, multConst, multConstJava, rnd, seed, i);\n+                testDecompose(prod1, prod2, prod3, prod4, coeffs1, coeffs2, decompose, decomposeJava, rnd, seed, i);\n+                testAlmostNtt(coeffs1, coeffs2, almostNtt, almostNttJava, rnd, seed, i);\n+                testInverseNtt(coeffs1, coeffs2, inverseNtt, inverseNttJava, rnd, seed, i);\n+            }\n+            System.out.println(\"Fuzz Success\");\n+        } catch (Throwable e) {\n+            System.out.println(\"Fuzz Failed: \" + e);\n+        }\n+    }\n+\n+    private static final int ML_DSA_N = 256;\n+    public static void testMult(int[] prod1, int[] prod2, int[] coeffs1, int[] coeffs2,\n+        MethodHandle mult, MethodHandle multJava, Random rnd,\n+        long seed, int i) throws Exception, Throwable {\n+\n+        for (int j = 0; j<ML_DSA_N; j++) {\n+            coeffs1[j] = rnd.nextInt();\n+            coeffs2[j] = rnd.nextInt();\n+        }\n+\n+        mult.invoke(prod1, coeffs1, coeffs2);\n+        multJava.invoke(prod2, coeffs1, coeffs2);\n+\n+        if (!Arrays.equals(prod1, prod2)) {\n+                throw new RuntimeException(\"[Seed \"+seed+\"@\"+i+\"] Result mult mismatch: \" + formatOf(prod1) + \" != \" + formatOf(prod2));\n+        }\n+    }\n+\n+    public static void testMultConst(int[] prod1, int[] prod2,\n+        MethodHandle multConst, MethodHandle multConstJava, Random rnd,\n+        long seed, int i) throws Exception, Throwable {\n+\n+        for (int j = 0; j<ML_DSA_N; j++) {\n+            prod1[j] = prod2[j] = rnd.nextInt();\n+        }\n+        \/\/ Per Algorithm 3 in https:\/\/eprint.iacr.org\/2018\/039.pdf, one of the inputs is bound, which prevents overflows\n+        int dilithium_q = 8380417;\n+        int c = rnd.nextInt(dilithium_q);\n+\n+        multConst.invoke(prod1, c);\n+        multConstJava.invoke(prod2, c);\n+\n+        if (!Arrays.equals(prod1, prod2)) {\n+                throw new RuntimeException(\"[Seed \"+seed+\"@\"+i+\"] Result multConst mismatch: \" + formatOf(prod1) + \" != \" + formatOf(prod2));\n+        }\n+    }\n+\n+    public static void testDecompose(int[] low1, int[] high1, int[] low2, int[] high2, int[] coeffs1, int[] coeffs2,\n+        MethodHandle decompose, MethodHandle decomposeJava, Random rnd,\n+        long seed, int i) throws Exception, Throwable {\n+\n+        for (int j = 0; j<ML_DSA_N; j++) {\n+            coeffs1[j] = coeffs2[j] = rnd.nextInt();\n+        }\n+        int gamma2 = 95232;\n+        if (rnd.nextBoolean()) {\n+            gamma2 = rnd.nextInt();\n+        }\n+        int multiplier = (gamma2 == 95232 ? 22 : 8);\n+\n+        decompose.invoke(coeffs1, low1, high1, 2 * gamma2, multiplier);\n+        decomposeJava.invoke(coeffs2, low2, high2, 2 * gamma2, multiplier);\n+\n+        if (!Arrays.equals(low1, low2)) {\n+                throw new RuntimeException(\"[Seed \"+seed+\"@\"+i+\"] Result low mismatch: \" + formatOf(low1) + \" != \" + formatOf(low2));\n+        }\n+\n+        if (!Arrays.equals(high1, high2)) {\n+                throw new RuntimeException(\"[Seed \"+seed+\"@\"+i+\"] Result high mismatch: \" + formatOf(high1) + \" != \" + formatOf(high2));\n+        }\n+    }\n+\n+    public static void testAlmostNtt(int[] coeffs1, int[] coeffs2,\n+        MethodHandle almostNtt, MethodHandle almostNttJava, Random rnd,\n+        long seed, int i) throws Exception, Throwable {\n+        for (int j = 0; j<ML_DSA_N; j++) {\n+            coeffs1[j] = coeffs2[j] = rnd.nextInt();\n+        }\n+\n+        almostNtt.invoke(coeffs1, MONT_ZETAS_FOR_VECTOR_NTT);\n+        almostNttJava.invoke(coeffs2);\n+\n+        if (!Arrays.equals(coeffs1, coeffs2)) {\n+            throw new RuntimeException(\"[Seed \"+seed+\"@\"+i+\"] Result AlmostNtt mismatch: \" + formatOf(coeffs1) + \" != \" + formatOf(coeffs2));\n+        }\n+    }\n+\n+    public static void testInverseNtt(int[] coeffs1, int[] coeffs2,\n+        MethodHandle inverseNtt, MethodHandle inverseNttJava, Random rnd,\n+        long seed, int i) throws Exception, Throwable {\n+        for (int j = 0; j<ML_DSA_N; j++) {\n+            coeffs1[j] = coeffs2[j] = rnd.nextInt();\n+        }\n+\n+        inverseNtt.invoke(coeffs1, MONT_ZETAS_FOR_VECTOR_INVERSE_NTT);\n+        inverseNttJava.invoke(coeffs2);\n+\n+        if (!Arrays.equals(coeffs1, coeffs2)) {\n+            throw new RuntimeException(\"[Seed \"+seed+\"@\"+i+\"] Result InverseNtt mismatch: \" + formatOf(coeffs1) + \" != \" + formatOf(coeffs2));\n+        }\n+    }\n+\n+    private static CharSequence formatOf(int[] arr) {\n+        StringBuilder b = new StringBuilder(arr.length*8);\n+        HexFormat hex = HexFormat.of();\n+        for (int j = 0; j<arr.length; j++) {\n+            b.append(hex.toHexDigits(arr[j]));\n+        }\n+        return b.toString();\n+    }\n+\n+    \/\/ Copied constants from sun.security.provider.ML_DSA\n+    private static final int[] MONT_ZETAS_FOR_VECTOR_INVERSE_NTT = new int[]{\n+            -1976782, 846154, -1400424, -3937738, 1362209, 48306, -3919660, 554416,\n+            3545687, -1612842, 976891, -183443, 2286327, 420899, 2235985, 2939036,\n+            3833893, 260646, 1104333, 1667432, -1910376, 1803090, -1723600, 426683,\n+            -472078, -1717735, 975884, -2213111, -269760, -3866901, -3523897, 3038916,\n+            1799107, 3694233, -1652634, -810149, -3014001, -1616392, -162844, 3183426,\n+            1207385, -185531, -3369112, -1957272, 164721, -2454455, -2432395, 2013608,\n+            3776993, -594136, 3724270, 2584293, 1846953, 1671176, 2831860, 542412,\n+            -3406031, -2235880, -777191, -1500165, 1374803, 2546312, -1917081, 1279661,\n+            1962642, -3306115, -1312455, 451100, 1430225, 3318210, -1237275, 1333058,\n+            1050970, -1903435, -1869119, 2994039, 3548272, -2635921, -1250494, 3767016,\n+            -1595974, -2486353, -1247620, -4055324, -1265009, 2590150, -2691481, -2842341,\n+            -203044, -1735879, 3342277, -3437287, -4108315, 2437823, -286988, -342297,\n+            3595838, 768622, 525098, 3556995, -3207046, -2031748, 3122442, 655327,\n+            522500, 43260, 1613174, -495491, -819034, -909542, -1859098, -900702,\n+            3193378, 1197226, 3759364, 3520352, -3513181, 1235728, -2434439, -266997,\n+            3562462, 2446433, -2244091, 3342478, -3817976, -2316500, -3407706, -2091667,\n+\n+            -3839961, -3839961, 3628969, 3628969, 3881060, 3881060, 3019102, 3019102,\n+            1439742, 1439742, 812732, 812732, 1584928, 1584928, -1285669, -1285669,\n+            -1341330, - 1341330, -1315589, -1315589, 177440, 177440, 2409325, 2409325,\n+            1851402, 1851402, -3159746, -3159746, 3553272, 3553272, -189548, -189548,\n+            1316856, 1316856, -759969, -759969, 210977, 210977, -2389356, -2389356,\n+            3249728, 3249728, -1653064, -1653064, 8578, 8578, 3724342, 3724342,\n+            -3958618, -3958618, -904516, -904516, 1100098, 1100098, -44288, -44288,\n+            -3097992, -3097992, -508951, -508951, -264944, -264944, 3343383, 3343383,\n+            1430430, 1430430, -1852771, -1852771, -1349076, -1349076, 381987, 381987,\n+            1308169, 1308169, 22981, 22981, 1228525, 1228525, 671102, 671102,\n+            2477047, 2477047, 411027, 411027, 3693493, 3693493, 2967645, 2967645,\n+            -2715295, -2715295, -2147896, -2147896, 983419, 983419, -3412210, -3412210,\n+            -126922, -126922, 3632928, 3632928, 3157330, 3157330, 3190144, 3190144,\n+            1000202, 1000202, 4083598, 4083598, -1939314, -1939314, 1257611, 1257611,\n+            1585221, 1585221, -2176455, -2176455, -3475950, -3475950, 1452451, 1452451,\n+            3041255, 3041255, 3677745, 3677745, 1528703, 1528703, 3930395, 3930395,\n+\n+            2797779, 2797779, 2797779, 2797779, -2071892, -2071892, -2071892, -2071892,\n+            2556880, 2556880, 2556880, 2556880, -3900724, -3900724, -3900724, -3900724,\n+            -3881043, -3881043, -3881043, -3881043, -954230, -954230, -954230, -954230,\n+            -531354, -531354, -531354, -531354, -811944, -811944, -811944, -811944,\n+            -3699596, -3699596, -3699596, -3699596, 1600420, 1600420, 1600420, 1600420,\n+            2140649, 2140649, 2140649, 2140649, -3507263, -3507263, -3507263, -3507263,\n+            3821735, 3821735, 3821735, 3821735, -3505694, -3505694, -3505694, -3505694,\n+            1643818, 1643818, 1643818, 1643818, 1699267, 1699267, 1699267, 1699267,\n+            539299, 539299, 539299, 539299, -2348700, -2348700, -2348700, -2348700,\n+            300467, 300467, 300467, 300467, -3539968, -3539968, -3539968, -3539968,\n+            2867647, 2867647, 2867647, 2867647, -3574422, -3574422, -3574422, -3574422,\n+            3043716, 3043716, 3043716, 3043716, 3861115, 3861115, 3861115, 3861115,\n+            -3915439, -3915439, -3915439, -3915439, 2537516, 2537516, 2537516, 2537516,\n+            3592148, 3592148, 3592148, 3592148, 1661693, 1661693, 1661693, 1661693,\n+            -3530437, -3530437, -3530437, -3530437, -3077325, -3077325, -3077325, -3077325,\n+            -95776, -95776, -95776, -95776, -2706023, -2706023, -2706023, -2706023,\n+\n+            -280005, -280005, -280005, -280005, -280005, -280005, -280005, -280005,\n+            -4010497, -4010497, -4010497, -4010497, -4010497, -4010497, -4010497, -4010497,\n+            19422, 19422, 19422, 19422, 19422, 19422, 19422, 19422,\n+            -1757237, -1757237, -1757237, -1757237, -1757237, -1757237, -1757237, -1757237,\n+            3277672, 3277672, 3277672, 3277672, 3277672, 3277672, 3277672, 3277672,\n+            1399561, 1399561, 1399561, 1399561, 1399561, 1399561, 1399561, 1399561,\n+            3859737, 3859737, 3859737, 3859737, 3859737, 3859737, 3859737, 3859737,\n+            2118186, 2118186, 2118186, 2118186, 2118186, 2118186, 2118186, 2118186,\n+            2108549, 2108549, 2108549, 2108549, 2108549, 2108549, 2108549, 2108549,\n+            -2619752, -2619752, -2619752, -2619752, -2619752, -2619752, -2619752, -2619752,\n+            1119584, 1119584, 1119584, 1119584, 1119584, 1119584, 1119584, 1119584,\n+            549488, 549488, 549488, 549488, 549488, 549488, 549488, 549488,\n+            -3585928, -3585928, -3585928, -3585928, -3585928, -3585928, -3585928, -3585928,\n+            1079900, 1079900, 1079900, 1079900, 1079900, 1079900, 1079900, 1079900,\n+            -1024112, -1024112, -1024112, -1024112, -1024112, -1024112, -1024112, -1024112,\n+            -2725464, -2725464, -2725464, -2725464, -2725464, -2725464, -2725464, -2725464,\n+\n+            -2680103, -2680103, -2680103, -2680103, -2680103, -2680103, -2680103, -2680103,\n+            -2680103, -2680103, -2680103, -2680103, -2680103, -2680103, -2680103, -2680103,\n+            -3111497, -3111497, -3111497, -3111497, -3111497, -3111497, -3111497, -3111497,\n+            -3111497, -3111497, -3111497, -3111497, -3111497, -3111497, -3111497, -3111497,\n+            2884855, 2884855, 2884855, 2884855, 2884855, 2884855, 2884855, 2884855,\n+            2884855, 2884855, 2884855, 2884855, 2884855, 2884855, 2884855, 2884855,\n+            -3119733, -3119733, -3119733, -3119733, -3119733, -3119733, -3119733, -3119733,\n+            -3119733, -3119733, -3119733, -3119733, -3119733, -3119733, -3119733, -3119733,\n+            2091905, 2091905, 2091905, 2091905, 2091905, 2091905, 2091905, 2091905,\n+            2091905, 2091905, 2091905, 2091905, 2091905, 2091905, 2091905, 2091905,\n+            359251, 359251, 359251, 359251, 359251, 359251, 359251, 359251,\n+            359251, 359251, 359251, 359251, 359251, 359251, 359251, 359251,\n+            -2353451, -2353451, -2353451, -2353451, -2353451, -2353451, -2353451, -2353451,\n+            -2353451, -2353451, -2353451, -2353451, -2353451, -2353451, -2353451, -2353451,\n+            -1826347, -1826347, -1826347, -1826347, -1826347, -1826347, -1826347, -1826347,\n+            -1826347, -1826347, -1826347, -1826347, -1826347, -1826347, -1826347, -1826347,\n+\n+            -466468, -466468, -466468, -466468, -466468, -466468, -466468, -466468,\n+            -466468, -466468, -466468, -466468, -466468, -466468, -466468, -466468,\n+            -466468, -466468, -466468, -466468, -466468, -466468, -466468, -466468,\n+            -466468, -466468, -466468, -466468, -466468, -466468, -466468, -466468,\n+            876248, 876248, 876248, 876248, 876248, 876248, 876248, 876248,\n+            876248, 876248, 876248, 876248, 876248, 876248, 876248, 876248,\n+            876248, 876248, 876248, 876248, 876248, 876248, 876248, 876248,\n+            876248, 876248, 876248, 876248, 876248, 876248, 876248, 876248,\n+            777960, 777960, 777960, 777960, 777960, 777960, 777960, 777960,\n+            777960, 777960, 777960, 777960, 777960, 777960, 777960, 777960,\n+            777960, 777960, 777960, 777960, 777960, 777960, 777960, 777960,\n+            777960, 777960, 777960, 777960, 777960, 777960, 777960, 777960,\n+            -237124, -237124, -237124, -237124, -237124, -237124, -237124, -237124,\n+            -237124, -237124, -237124, -237124, -237124, -237124, -237124, -237124,\n+            -237124, -237124, -237124, -237124, -237124, -237124, -237124, -237124,\n+            -237124, -237124, -237124, -237124, -237124, -237124, -237124, -237124,\n+\n+            518909, 518909, 518909, 518909, 518909, 518909, 518909, 518909,\n+            518909, 518909, 518909, 518909, 518909, 518909, 518909, 518909,\n+            518909, 518909, 518909, 518909, 518909, 518909, 518909, 518909,\n+            518909, 518909, 518909, 518909, 518909, 518909, 518909, 518909,\n+            518909, 518909, 518909, 518909, 518909, 518909, 518909, 518909,\n+            518909, 518909, 518909, 518909, 518909, 518909, 518909, 518909,\n+            518909, 518909, 518909, 518909, 518909, 518909, 518909, 518909,\n+            518909, 518909, 518909, 518909, 518909, 518909, 518909, 518909,\n+            2608894, 2608894, 2608894, 2608894, 2608894, 2608894, 2608894, 2608894,\n+            2608894, 2608894, 2608894, 2608894, 2608894, 2608894, 2608894, 2608894,\n+            2608894, 2608894, 2608894, 2608894, 2608894, 2608894, 2608894, 2608894,\n+            2608894, 2608894, 2608894, 2608894, 2608894, 2608894, 2608894, 2608894,\n+            2608894, 2608894, 2608894, 2608894, 2608894, 2608894, 2608894, 2608894,\n+            2608894, 2608894, 2608894, 2608894, 2608894, 2608894, 2608894, 2608894,\n+            2608894, 2608894, 2608894, 2608894, 2608894, 2608894, 2608894, 2608894,\n+            2608894, 2608894, 2608894, 2608894, 2608894, 2608894, 2608894, 2608894,\n+\n+            -25847, -25847, -25847, -25847, -25847, -25847, -25847, -25847,\n+            -25847, -25847, -25847, -25847, -25847, -25847, -25847, -25847,\n+            -25847, -25847, -25847, -25847, -25847, -25847, -25847, -25847,\n+            -25847, -25847, -25847, -25847, -25847, -25847, -25847, -25847,\n+            -25847, -25847, -25847, -25847, -25847, -25847, -25847, -25847,\n+            -25847, -25847, -25847, -25847, -25847, -25847, -25847, -25847,\n+            -25847, -25847, -25847, -25847, -25847, -25847, -25847, -25847,\n+            -25847, -25847, -25847, -25847, -25847, -25847, -25847, -25847,\n+            -25847, -25847, -25847, -25847, -25847, -25847, -25847, -25847,\n+            -25847, -25847, -25847, -25847, -25847, -25847, -25847, -25847,\n+            -25847, -25847, -25847, -25847, -25847, -25847, -25847, -25847,\n+            -25847, -25847, -25847, -25847, -25847, -25847, -25847, -25847,\n+            -25847, -25847, -25847, -25847, -25847, -25847, -25847, -25847,\n+            -25847, -25847, -25847, -25847, -25847, -25847, -25847, -25847,\n+            -25847, -25847, -25847, -25847, -25847, -25847, -25847, -25847,\n+            -25847, -25847, -25847, -25847, -25847, -25847, -25847, -25847\n+    };\n+\n+    private static final int[] MONT_ZETAS_FOR_VECTOR_NTT = new int[]{\n+            25847, 25847, 25847, 25847, 25847, 25847, 25847, 25847,\n+            25847, 25847, 25847, 25847, 25847, 25847, 25847, 25847,\n+            25847, 25847, 25847, 25847, 25847, 25847, 25847, 25847,\n+            25847, 25847, 25847, 25847, 25847, 25847, 25847, 25847,\n+            25847, 25847, 25847, 25847, 25847, 25847, 25847, 25847,\n+            25847, 25847, 25847, 25847, 25847, 25847, 25847, 25847,\n+            25847, 25847, 25847, 25847, 25847, 25847, 25847, 25847,\n+            25847, 25847, 25847, 25847, 25847, 25847, 25847, 25847,\n+            25847, 25847, 25847, 25847, 25847, 25847, 25847, 25847,\n+            25847, 25847, 25847, 25847, 25847, 25847, 25847, 25847,\n+            25847, 25847, 25847, 25847, 25847, 25847, 25847, 25847,\n+            25847, 25847, 25847, 25847, 25847, 25847, 25847, 25847,\n+            25847, 25847, 25847, 25847, 25847, 25847, 25847, 25847,\n+            25847, 25847, 25847, 25847, 25847, 25847, 25847, 25847,\n+            25847, 25847, 25847, 25847, 25847, 25847, 25847, 25847,\n+            25847, 25847, 25847, 25847, 25847, 25847, 25847, 25847,\n+\n+            -2608894, -2608894, -2608894, -2608894, -2608894, -2608894, -2608894, -2608894,\n+            -2608894, -2608894, -2608894, -2608894, -2608894, -2608894, -2608894, -2608894,\n+            -2608894, -2608894, -2608894, -2608894, -2608894, -2608894, -2608894, -2608894,\n+            -2608894, -2608894, -2608894, -2608894, -2608894, -2608894, -2608894, -2608894,\n+            -2608894, -2608894, -2608894, -2608894, -2608894, -2608894, -2608894, -2608894,\n+            -2608894, -2608894, -2608894, -2608894, -2608894, -2608894, -2608894, -2608894,\n+            -2608894, -2608894, -2608894, -2608894, -2608894, -2608894, -2608894, -2608894,\n+            -2608894, -2608894, -2608894, -2608894, -2608894, -2608894, -2608894, -2608894,\n+            -518909, -518909, -518909, -518909, -518909, -518909, -518909, -518909,\n+            -518909, -518909, -518909, -518909, -518909, -518909, -518909, -518909,\n+            -518909, -518909, -518909, -518909, -518909, -518909, -518909, -518909,\n+            -518909, -518909, -518909, -518909, -518909, -518909, -518909, -518909,\n+            -518909, -518909, -518909, -518909, -518909, -518909, -518909, -518909,\n+            -518909, -518909, -518909, -518909, -518909, -518909, -518909, -518909,\n+            -518909, -518909, -518909, -518909, -518909, -518909, -518909, -518909,\n+            -518909, -518909, -518909, -518909, -518909, -518909, -518909, -518909,\n+\n+            237124, 237124, 237124, 237124, 237124, 237124, 237124, 237124,\n+            237124, 237124, 237124, 237124, 237124, 237124, 237124, 237124,\n+            237124, 237124, 237124, 237124, 237124, 237124, 237124, 237124,\n+            237124, 237124, 237124, 237124, 237124, 237124, 237124, 237124,\n+            -777960, -777960, -777960, -777960, -777960, -777960, -777960, -777960,\n+            -777960, -777960, -777960, -777960, -777960, -777960, -777960, -777960,\n+            -777960, -777960, -777960, -777960, -777960, -777960, -777960, -777960,\n+            -777960, -777960, -777960, -777960, -777960, -777960, -777960, -777960,\n+            -876248, -876248, -876248, -876248, -876248, -876248, -876248, -876248,\n+            -876248, -876248, -876248, -876248, -876248, -876248, -876248, -876248,\n+            -876248, -876248, -876248, -876248, -876248, -876248, -876248, -876248,\n+            -876248, -876248, -876248, -876248, -876248, -876248, -876248, -876248,\n+            466468, 466468, 466468, 466468, 466468, 466468, 466468, 466468,\n+            466468, 466468, 466468, 466468, 466468, 466468, 466468, 466468,\n+            466468, 466468, 466468, 466468, 466468, 466468, 466468, 466468,\n+            466468, 466468, 466468, 466468, 466468, 466468, 466468, 466468,\n+\n+            1826347, 1826347, 1826347, 1826347, 1826347, 1826347, 1826347, 1826347,\n+            1826347, 1826347, 1826347, 1826347, 1826347, 1826347, 1826347, 1826347,\n+            2353451, 2353451, 2353451, 2353451, 2353451, 2353451, 2353451, 2353451,\n+            2353451, 2353451, 2353451, 2353451, 2353451, 2353451, 2353451, 2353451,\n+            -359251, -359251, -359251, -359251, -359251, -359251, -359251, -359251,\n+            -359251, -359251, -359251, -359251, -359251, -359251, -359251, -359251,\n+            -2091905, -2091905, -2091905, -2091905, -2091905, -2091905, -2091905, -2091905,\n+            -2091905, -2091905, -2091905, -2091905, -2091905, -2091905, -2091905, -2091905,\n+            3119733, 3119733, 3119733, 3119733, 3119733, 3119733, 3119733, 3119733,\n+            3119733, 3119733, 3119733, 3119733, 3119733, 3119733, 3119733, 3119733,\n+            -2884855, -2884855, -2884855, -2884855, -2884855, -2884855, -2884855, -2884855,\n+            -2884855, -2884855, -2884855, -2884855, -2884855, -2884855, -2884855, -2884855,\n+            3111497, 3111497, 3111497, 3111497, 3111497, 3111497, 3111497, 3111497,\n+            3111497, 3111497, 3111497, 3111497, 3111497, 3111497, 3111497, 3111497,\n+            2680103, 2680103, 2680103, 2680103, 2680103, 2680103, 2680103, 2680103,\n+            2680103, 2680103, 2680103, 2680103, 2680103, 2680103, 2680103, 2680103,\n+\n+            2725464, 2725464, 2725464, 2725464, 2725464, 2725464, 2725464, 2725464,\n+            1024112, 1024112, 1024112, 1024112, 1024112, 1024112, 1024112, 1024112,\n+            -1079900, -1079900, -1079900, -1079900, -1079900, -1079900, -1079900, -1079900,\n+            3585928, 3585928, 3585928, 3585928, 3585928, 3585928, 3585928, 3585928,\n+            -549488, -549488, -549488, -549488, -549488, -549488, -549488, -549488,\n+            -1119584, -1119584, -1119584, -1119584, -1119584, -1119584, -1119584, -1119584,\n+            2619752, 2619752, 2619752, 2619752, 2619752, 2619752, 2619752, 2619752,\n+            -2108549, -2108549, -2108549, -2108549, -2108549, -2108549, -2108549, -2108549,\n+            -2118186, -2118186, -2118186, -2118186, -2118186, -2118186, -2118186, -2118186,\n+            -3859737, -3859737, -3859737, -3859737, -3859737, -3859737, -3859737, -3859737,\n+            -1399561, -1399561, -1399561, -1399561, -1399561, -1399561, -1399561, -1399561,\n+            -3277672, -3277672, -3277672, -3277672, -3277672, -3277672, -3277672, -3277672,\n+            1757237, 1757237, 1757237, 1757237, 1757237, 1757237, 1757237, 1757237,\n+            -19422, -19422, -19422, -19422, -19422, -19422, -19422, -19422,\n+            4010497, 4010497, 4010497, 4010497, 4010497, 4010497, 4010497, 4010497,\n+            280005, 280005, 280005, 280005, 280005, 280005, 280005, 280005,\n+\n+            2706023, 2706023, 2706023, 2706023, 95776, 95776, 95776, 95776,\n+            3077325, 3077325, 3077325, 3077325, 3530437, 3530437, 3530437, 3530437,\n+            -1661693, -1661693, -1661693, -1661693, -3592148, -3592148, -3592148, -3592148,\n+            -2537516, -2537516, -2537516, -2537516, 3915439, 3915439, 3915439, 3915439,\n+            -3861115, -3861115, -3861115, -3861115, -3043716, -3043716, -3043716, -3043716,\n+            3574422, 3574422, 3574422, 3574422, -2867647, -2867647, -2867647, -2867647,\n+            3539968, 3539968, 3539968, 3539968, -300467, -300467, -300467, -300467,\n+            2348700, 2348700, 2348700, 2348700, -539299, -539299, -539299, -539299,\n+            -1699267, -1699267, -1699267, -1699267, -1643818, -1643818, -1643818, -1643818,\n+            3505694, 3505694, 3505694, 3505694, -3821735, -3821735, -3821735, -3821735,\n+            3507263, 3507263, 3507263, 3507263, -2140649, -2140649, -2140649, -2140649,\n+            -1600420, -1600420, -1600420, -1600420, 3699596, 3699596, 3699596, 3699596,\n+            811944, 811944, 811944, 811944, 531354, 531354, 531354, 531354,\n+            954230, 954230, 954230, 954230, 3881043, 3881043, 3881043, 3881043,\n+            3900724, 3900724, 3900724, 3900724, -2556880, -2556880, -2556880, -2556880,\n+            2071892, 2071892, 2071892, 2071892, -2797779, -2797779, -2797779, -2797779,\n+\n+            -3930395, -3930395, -1528703, -1528703, -3677745, -3677745, -3041255, -3041255,\n+            -1452451, -1452451, 3475950, 3475950, 2176455, 2176455, -1585221, -1585221,\n+            -1257611, -1257611, 1939314, 1939314, -4083598, -4083598, -1000202, -1000202,\n+            -3190144, -3190144, -3157330, -3157330, -3632928, -3632928, 126922, 126922,\n+            3412210, 3412210, -983419, -983419, 2147896, 2147896, 2715295, 2715295,\n+            -2967645, -2967645, -3693493, -3693493, -411027, -411027, -2477047, -2477047,\n+            -671102, -671102, -1228525, -1228525, -22981, -22981, -1308169, -1308169,\n+            -381987, -381987, 1349076, 1349076, 1852771, 1852771, -1430430, -1430430,\n+            -3343383, -3343383, 264944, 264944, 508951, 508951, 3097992, 3097992,\n+            44288, 44288, -1100098, -1100098, 904516, 904516, 3958618, 3958618,\n+            -3724342, -3724342, -8578, -8578, 1653064, 1653064, -3249728, -3249728,\n+            2389356, 2389356, -210977, -210977, 759969, 759969, -1316856, -1316856,\n+            189548, 189548, -3553272, -3553272, 3159746, 3159746, -1851402, -1851402,\n+            -2409325, -2409325, -177440, -177440, 1315589, 1315589, 1341330, 1341330,\n+            1285669, 1285669, -1584928, -1584928, -812732, -812732, -1439742, -1439742,\n+            -3019102, -3019102, -3881060, -3881060, -3628969, -3628969, 3839961, 3839961,\n+\n+            2091667, 3407706, 2316500, 3817976, -3342478, 2244091, -2446433, -3562462,\n+            266997, 2434439, -1235728, 3513181, -3520352, -3759364, -1197226, -3193378,\n+            900702, 1859098, 909542, 819034, 495491, -1613174, -43260, -522500,\n+            -655327, -3122442, 2031748, 3207046, -3556995, -525098, -768622, -3595838,\n+            342297, 286988, -2437823, 4108315, 3437287, -3342277, 1735879, 203044,\n+            2842341, 2691481, -2590150, 1265009, 4055324, 1247620, 2486353, 1595974,\n+            -3767016, 1250494, 2635921, -3548272, -2994039, 1869119, 1903435, -1050970,\n+            -1333058, 1237275, -3318210, -1430225, -451100, 1312455, 3306115, -1962642,\n+            -1279661, 1917081, -2546312, -1374803, 1500165, 777191, 2235880, 3406031,\n+            -542412, -2831860, -1671176, -1846953, -2584293, -3724270, 594136, -3776993,\n+            -2013608, 2432395, 2454455, -164721, 1957272, 3369112, 185531, -1207385,\n+            -3183426, 162844, 1616392, 3014001, 810149, 1652634, -3694233, -1799107,\n+            -3038916, 3523897, 3866901, 269760, 2213111, -975884, 1717735, 472078,\n+            -426683, 1723600, -1803090, 1910376, -1667432, -1104333, -260646, -3833893,\n+            -2939036, -2235985, -420899, -2286327, 183443, -976891, 1612842, -3545687,\n+            -554416, 3919660, -48306, -1362209, 3937738, 1400424, -846154, 1976782\n+    };\n+}\n","filename":"test\/jdk\/sun\/security\/provider\/acvp\/ML_DSA_Intrinsic_Test.java","additions":516,"deletions":0,"binary":false,"changes":516,"status":"added"},{"patch":"@@ -0,0 +1,421 @@\n+\/*\n+ * Copyright (c) 2025, Intel Corporation. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+package org.openjdk.bench.javax.crypto.full;\n+\n+import org.openjdk.jmh.annotations.Benchmark;\n+import org.openjdk.jmh.annotations.Param;\n+import org.openjdk.jmh.annotations.Setup;\n+import org.openjdk.jmh.annotations.Fork;\n+import org.openjdk.jmh.annotations.Warmup;\n+import org.openjdk.jmh.annotations.Measurement;\n+import org.openjdk.jmh.annotations.OutputTimeUnit;\n+\n+import java.lang.invoke.MethodHandle;\n+import java.lang.invoke.MethodHandles;\n+import java.lang.reflect.Field;\n+import java.lang.reflect.Method;\n+import java.lang.reflect.Constructor;\n+import java.util.Random;\n+import java.util.concurrent.TimeUnit;\n+\n+import javax.crypto.KeyGenerator;\n+import javax.crypto.Mac;\n+import java.security.InvalidKeyException;\n+import java.security.NoSuchAlgorithmException;\n+\n+@Measurement(iterations = 3, time = 10)\n+@Warmup(iterations = 3, time = 10)\n+@OutputTimeUnit(TimeUnit.MILLISECONDS)\n+@Fork(value = 1, jvmArgs = {\"--add-opens\", \"java.base\/sun.security.provider=ALL-UNNAMED\"})\n+public class MLDSABench extends CryptoBase {\n+    public static final int SET_SIZE = 128;\n+\n+    private int[][] coeffs1;\n+    private int[][] coeffs2;\n+    private int[][] prod1;\n+    private int[][] prod2;\n+    private MethodHandle mult, multConst, decompose, almostNtt, inverseNtt;\n+    int index = 0;\n+\n+    public static int[][] fillRandom(int[][] data) {\n+        Random rnd = new Random();\n+        for (int[] d : data) {\n+            for (int j = 0; j<d.length; j++) {\n+                d[j] = rnd.nextInt();\n+            }\n+        }\n+        return data;\n+    }\n+\n+    @Setup\n+    public void setup() throws Exception {\n+        coeffs1 = fillRandom(new int[SET_SIZE][ML_DSA_N]);\n+        coeffs2 = fillRandom(new int[SET_SIZE][ML_DSA_N]);\n+        prod1 = fillRandom(new int[SET_SIZE][ML_DSA_N]);\n+        prod2 = fillRandom(new int[SET_SIZE][ML_DSA_N]);\n+\n+        MethodHandles.Lookup lookup = MethodHandles.lookup();\n+        Class<?> kClazz = Class.forName(\"sun.security.provider.ML_DSA\");\n+        Constructor<?> constructor = kClazz.getDeclaredConstructor(\n+                int.class);\n+        constructor.setAccessible(true);\n+\n+        Method m = kClazz.getDeclaredMethod(\"implDilithiumNttMult\",\n+                int[].class, int[].class, int[].class);\n+        m.setAccessible(true);\n+        mult = lookup.unreflect(m);\n+\n+        m = kClazz.getDeclaredMethod(\"implDilithiumMontMulByConstant\",\n+                int[].class, int.class);\n+        m.setAccessible(true);\n+        multConst = lookup.unreflect(m);\n+\n+        m = kClazz.getDeclaredMethod(\"implDilithiumDecomposePoly\",\n+                int[].class, int[].class, int[].class, int.class, int.class);\n+        m.setAccessible(true);\n+        decompose = lookup.unreflect(m);\n+\n+        m = kClazz.getDeclaredMethod(\"implDilithiumAlmostNtt\",\n+        int[].class, int[].class);\n+        m.setAccessible(true);\n+        almostNtt = lookup.unreflect(m);\n+\n+        m = kClazz.getDeclaredMethod(\"implDilithiumAlmostInverseNtt\",\n+        int[].class, int[].class);\n+        m.setAccessible(true);\n+        inverseNtt = lookup.unreflect(m);\n+    }\n+\n+    @Benchmark\n+    public void mult() throws Exception, Throwable {\n+        mult.invoke(prod1[index], coeffs1[index], coeffs2[index]);\n+        index = (index + 1) % SET_SIZE;\n+    }\n+\n+    @Benchmark\n+    public void multConst() throws Exception, Throwable {\n+        multConst.invoke(prod1[index], coeffs1[index][index]);\n+        index = (index + 1) % SET_SIZE;\n+    }\n+\n+    @Benchmark\n+    public void multDecompose() throws Exception, Throwable {\n+        int gamma2 = 95232;\n+        if (coeffs1[index][0]%2==1) {\n+            gamma2 = coeffs1[index][1];\n+        }\n+        int multiplier = (gamma2 == 95232 ? 22 : 8);\n+        decompose.invoke(coeffs1[index], prod1[index], prod2[index], 2 * gamma2, multiplier);\n+        index = (index + 1) % SET_SIZE;\n+    }\n+\n+    @Benchmark\n+    public void multAlmostNtt() throws Exception, Throwable {\n+        almostNtt.invoke(coeffs1[index], MONT_ZETAS_FOR_VECTOR_NTT);\n+        index = (index + 1) % SET_SIZE;\n+    }\n+\n+    @Benchmark\n+    public void multInverseNtt() throws Exception, Throwable {\n+        inverseNtt.invoke(coeffs2[index], MONT_ZETAS_FOR_VECTOR_INVERSE_NTT);\n+        index = (index + 1) % SET_SIZE;\n+    }\n+\n+    \/\/ Copied constants from sun.security.provider.ML_DSA\n+    private static final int ML_DSA_N = 256;\n+    private static final int[] MONT_ZETAS_FOR_VECTOR_INVERSE_NTT = new int[]{\n+            -1976782, 846154, -1400424, -3937738, 1362209, 48306, -3919660, 554416,\n+            3545687, -1612842, 976891, -183443, 2286327, 420899, 2235985, 2939036,\n+            3833893, 260646, 1104333, 1667432, -1910376, 1803090, -1723600, 426683,\n+            -472078, -1717735, 975884, -2213111, -269760, -3866901, -3523897, 3038916,\n+            1799107, 3694233, -1652634, -810149, -3014001, -1616392, -162844, 3183426,\n+            1207385, -185531, -3369112, -1957272, 164721, -2454455, -2432395, 2013608,\n+            3776993, -594136, 3724270, 2584293, 1846953, 1671176, 2831860, 542412,\n+            -3406031, -2235880, -777191, -1500165, 1374803, 2546312, -1917081, 1279661,\n+            1962642, -3306115, -1312455, 451100, 1430225, 3318210, -1237275, 1333058,\n+            1050970, -1903435, -1869119, 2994039, 3548272, -2635921, -1250494, 3767016,\n+            -1595974, -2486353, -1247620, -4055324, -1265009, 2590150, -2691481, -2842341,\n+            -203044, -1735879, 3342277, -3437287, -4108315, 2437823, -286988, -342297,\n+            3595838, 768622, 525098, 3556995, -3207046, -2031748, 3122442, 655327,\n+            522500, 43260, 1613174, -495491, -819034, -909542, -1859098, -900702,\n+            3193378, 1197226, 3759364, 3520352, -3513181, 1235728, -2434439, -266997,\n+            3562462, 2446433, -2244091, 3342478, -3817976, -2316500, -3407706, -2091667,\n+\n+            -3839961, -3839961, 3628969, 3628969, 3881060, 3881060, 3019102, 3019102,\n+            1439742, 1439742, 812732, 812732, 1584928, 1584928, -1285669, -1285669,\n+            -1341330, - 1341330, -1315589, -1315589, 177440, 177440, 2409325, 2409325,\n+            1851402, 1851402, -3159746, -3159746, 3553272, 3553272, -189548, -189548,\n+            1316856, 1316856, -759969, -759969, 210977, 210977, -2389356, -2389356,\n+            3249728, 3249728, -1653064, -1653064, 8578, 8578, 3724342, 3724342,\n+            -3958618, -3958618, -904516, -904516, 1100098, 1100098, -44288, -44288,\n+            -3097992, -3097992, -508951, -508951, -264944, -264944, 3343383, 3343383,\n+            1430430, 1430430, -1852771, -1852771, -1349076, -1349076, 381987, 381987,\n+            1308169, 1308169, 22981, 22981, 1228525, 1228525, 671102, 671102,\n+            2477047, 2477047, 411027, 411027, 3693493, 3693493, 2967645, 2967645,\n+            -2715295, -2715295, -2147896, -2147896, 983419, 983419, -3412210, -3412210,\n+            -126922, -126922, 3632928, 3632928, 3157330, 3157330, 3190144, 3190144,\n+            1000202, 1000202, 4083598, 4083598, -1939314, -1939314, 1257611, 1257611,\n+            1585221, 1585221, -2176455, -2176455, -3475950, -3475950, 1452451, 1452451,\n+            3041255, 3041255, 3677745, 3677745, 1528703, 1528703, 3930395, 3930395,\n+\n+            2797779, 2797779, 2797779, 2797779, -2071892, -2071892, -2071892, -2071892,\n+            2556880, 2556880, 2556880, 2556880, -3900724, -3900724, -3900724, -3900724,\n+            -3881043, -3881043, -3881043, -3881043, -954230, -954230, -954230, -954230,\n+            -531354, -531354, -531354, -531354, -811944, -811944, -811944, -811944,\n+            -3699596, -3699596, -3699596, -3699596, 1600420, 1600420, 1600420, 1600420,\n+            2140649, 2140649, 2140649, 2140649, -3507263, -3507263, -3507263, -3507263,\n+            3821735, 3821735, 3821735, 3821735, -3505694, -3505694, -3505694, -3505694,\n+            1643818, 1643818, 1643818, 1643818, 1699267, 1699267, 1699267, 1699267,\n+            539299, 539299, 539299, 539299, -2348700, -2348700, -2348700, -2348700,\n+            300467, 300467, 300467, 300467, -3539968, -3539968, -3539968, -3539968,\n+            2867647, 2867647, 2867647, 2867647, -3574422, -3574422, -3574422, -3574422,\n+            3043716, 3043716, 3043716, 3043716, 3861115, 3861115, 3861115, 3861115,\n+            -3915439, -3915439, -3915439, -3915439, 2537516, 2537516, 2537516, 2537516,\n+            3592148, 3592148, 3592148, 3592148, 1661693, 1661693, 1661693, 1661693,\n+            -3530437, -3530437, -3530437, -3530437, -3077325, -3077325, -3077325, -3077325,\n+            -95776, -95776, -95776, -95776, -2706023, -2706023, -2706023, -2706023,\n+\n+            -280005, -280005, -280005, -280005, -280005, -280005, -280005, -280005,\n+            -4010497, -4010497, -4010497, -4010497, -4010497, -4010497, -4010497, -4010497,\n+            19422, 19422, 19422, 19422, 19422, 19422, 19422, 19422,\n+            -1757237, -1757237, -1757237, -1757237, -1757237, -1757237, -1757237, -1757237,\n+            3277672, 3277672, 3277672, 3277672, 3277672, 3277672, 3277672, 3277672,\n+            1399561, 1399561, 1399561, 1399561, 1399561, 1399561, 1399561, 1399561,\n+            3859737, 3859737, 3859737, 3859737, 3859737, 3859737, 3859737, 3859737,\n+            2118186, 2118186, 2118186, 2118186, 2118186, 2118186, 2118186, 2118186,\n+            2108549, 2108549, 2108549, 2108549, 2108549, 2108549, 2108549, 2108549,\n+            -2619752, -2619752, -2619752, -2619752, -2619752, -2619752, -2619752, -2619752,\n+            1119584, 1119584, 1119584, 1119584, 1119584, 1119584, 1119584, 1119584,\n+            549488, 549488, 549488, 549488, 549488, 549488, 549488, 549488,\n+            -3585928, -3585928, -3585928, -3585928, -3585928, -3585928, -3585928, -3585928,\n+            1079900, 1079900, 1079900, 1079900, 1079900, 1079900, 1079900, 1079900,\n+            -1024112, -1024112, -1024112, -1024112, -1024112, -1024112, -1024112, -1024112,\n+            -2725464, -2725464, -2725464, -2725464, -2725464, -2725464, -2725464, -2725464,\n+\n+            -2680103, -2680103, -2680103, -2680103, -2680103, -2680103, -2680103, -2680103,\n+            -2680103, -2680103, -2680103, -2680103, -2680103, -2680103, -2680103, -2680103,\n+            -3111497, -3111497, -3111497, -3111497, -3111497, -3111497, -3111497, -3111497,\n+            -3111497, -3111497, -3111497, -3111497, -3111497, -3111497, -3111497, -3111497,\n+            2884855, 2884855, 2884855, 2884855, 2884855, 2884855, 2884855, 2884855,\n+            2884855, 2884855, 2884855, 2884855, 2884855, 2884855, 2884855, 2884855,\n+            -3119733, -3119733, -3119733, -3119733, -3119733, -3119733, -3119733, -3119733,\n+            -3119733, -3119733, -3119733, -3119733, -3119733, -3119733, -3119733, -3119733,\n+            2091905, 2091905, 2091905, 2091905, 2091905, 2091905, 2091905, 2091905,\n+            2091905, 2091905, 2091905, 2091905, 2091905, 2091905, 2091905, 2091905,\n+            359251, 359251, 359251, 359251, 359251, 359251, 359251, 359251,\n+            359251, 359251, 359251, 359251, 359251, 359251, 359251, 359251,\n+            -2353451, -2353451, -2353451, -2353451, -2353451, -2353451, -2353451, -2353451,\n+            -2353451, -2353451, -2353451, -2353451, -2353451, -2353451, -2353451, -2353451,\n+            -1826347, -1826347, -1826347, -1826347, -1826347, -1826347, -1826347, -1826347,\n+            -1826347, -1826347, -1826347, -1826347, -1826347, -1826347, -1826347, -1826347,\n+\n+            -466468, -466468, -466468, -466468, -466468, -466468, -466468, -466468,\n+            -466468, -466468, -466468, -466468, -466468, -466468, -466468, -466468,\n+            -466468, -466468, -466468, -466468, -466468, -466468, -466468, -466468,\n+            -466468, -466468, -466468, -466468, -466468, -466468, -466468, -466468,\n+            876248, 876248, 876248, 876248, 876248, 876248, 876248, 876248,\n+            876248, 876248, 876248, 876248, 876248, 876248, 876248, 876248,\n+            876248, 876248, 876248, 876248, 876248, 876248, 876248, 876248,\n+            876248, 876248, 876248, 876248, 876248, 876248, 876248, 876248,\n+            777960, 777960, 777960, 777960, 777960, 777960, 777960, 777960,\n+            777960, 777960, 777960, 777960, 777960, 777960, 777960, 777960,\n+            777960, 777960, 777960, 777960, 777960, 777960, 777960, 777960,\n+            777960, 777960, 777960, 777960, 777960, 777960, 777960, 777960,\n+            -237124, -237124, -237124, -237124, -237124, -237124, -237124, -237124,\n+            -237124, -237124, -237124, -237124, -237124, -237124, -237124, -237124,\n+            -237124, -237124, -237124, -237124, -237124, -237124, -237124, -237124,\n+            -237124, -237124, -237124, -237124, -237124, -237124, -237124, -237124,\n+\n+            518909, 518909, 518909, 518909, 518909, 518909, 518909, 518909,\n+            518909, 518909, 518909, 518909, 518909, 518909, 518909, 518909,\n+            518909, 518909, 518909, 518909, 518909, 518909, 518909, 518909,\n+            518909, 518909, 518909, 518909, 518909, 518909, 518909, 518909,\n+            518909, 518909, 518909, 518909, 518909, 518909, 518909, 518909,\n+            518909, 518909, 518909, 518909, 518909, 518909, 518909, 518909,\n+            518909, 518909, 518909, 518909, 518909, 518909, 518909, 518909,\n+            518909, 518909, 518909, 518909, 518909, 518909, 518909, 518909,\n+            2608894, 2608894, 2608894, 2608894, 2608894, 2608894, 2608894, 2608894,\n+            2608894, 2608894, 2608894, 2608894, 2608894, 2608894, 2608894, 2608894,\n+            2608894, 2608894, 2608894, 2608894, 2608894, 2608894, 2608894, 2608894,\n+            2608894, 2608894, 2608894, 2608894, 2608894, 2608894, 2608894, 2608894,\n+            2608894, 2608894, 2608894, 2608894, 2608894, 2608894, 2608894, 2608894,\n+            2608894, 2608894, 2608894, 2608894, 2608894, 2608894, 2608894, 2608894,\n+            2608894, 2608894, 2608894, 2608894, 2608894, 2608894, 2608894, 2608894,\n+            2608894, 2608894, 2608894, 2608894, 2608894, 2608894, 2608894, 2608894,\n+\n+            -25847, -25847, -25847, -25847, -25847, -25847, -25847, -25847,\n+            -25847, -25847, -25847, -25847, -25847, -25847, -25847, -25847,\n+            -25847, -25847, -25847, -25847, -25847, -25847, -25847, -25847,\n+            -25847, -25847, -25847, -25847, -25847, -25847, -25847, -25847,\n+            -25847, -25847, -25847, -25847, -25847, -25847, -25847, -25847,\n+            -25847, -25847, -25847, -25847, -25847, -25847, -25847, -25847,\n+            -25847, -25847, -25847, -25847, -25847, -25847, -25847, -25847,\n+            -25847, -25847, -25847, -25847, -25847, -25847, -25847, -25847,\n+            -25847, -25847, -25847, -25847, -25847, -25847, -25847, -25847,\n+            -25847, -25847, -25847, -25847, -25847, -25847, -25847, -25847,\n+            -25847, -25847, -25847, -25847, -25847, -25847, -25847, -25847,\n+            -25847, -25847, -25847, -25847, -25847, -25847, -25847, -25847,\n+            -25847, -25847, -25847, -25847, -25847, -25847, -25847, -25847,\n+            -25847, -25847, -25847, -25847, -25847, -25847, -25847, -25847,\n+            -25847, -25847, -25847, -25847, -25847, -25847, -25847, -25847,\n+            -25847, -25847, -25847, -25847, -25847, -25847, -25847, -25847\n+    };\n+\n+    private static final int[] MONT_ZETAS_FOR_VECTOR_NTT = new int[]{\n+            25847, 25847, 25847, 25847, 25847, 25847, 25847, 25847,\n+            25847, 25847, 25847, 25847, 25847, 25847, 25847, 25847,\n+            25847, 25847, 25847, 25847, 25847, 25847, 25847, 25847,\n+            25847, 25847, 25847, 25847, 25847, 25847, 25847, 25847,\n+            25847, 25847, 25847, 25847, 25847, 25847, 25847, 25847,\n+            25847, 25847, 25847, 25847, 25847, 25847, 25847, 25847,\n+            25847, 25847, 25847, 25847, 25847, 25847, 25847, 25847,\n+            25847, 25847, 25847, 25847, 25847, 25847, 25847, 25847,\n+            25847, 25847, 25847, 25847, 25847, 25847, 25847, 25847,\n+            25847, 25847, 25847, 25847, 25847, 25847, 25847, 25847,\n+            25847, 25847, 25847, 25847, 25847, 25847, 25847, 25847,\n+            25847, 25847, 25847, 25847, 25847, 25847, 25847, 25847,\n+            25847, 25847, 25847, 25847, 25847, 25847, 25847, 25847,\n+            25847, 25847, 25847, 25847, 25847, 25847, 25847, 25847,\n+            25847, 25847, 25847, 25847, 25847, 25847, 25847, 25847,\n+            25847, 25847, 25847, 25847, 25847, 25847, 25847, 25847,\n+\n+            -2608894, -2608894, -2608894, -2608894, -2608894, -2608894, -2608894, -2608894,\n+            -2608894, -2608894, -2608894, -2608894, -2608894, -2608894, -2608894, -2608894,\n+            -2608894, -2608894, -2608894, -2608894, -2608894, -2608894, -2608894, -2608894,\n+            -2608894, -2608894, -2608894, -2608894, -2608894, -2608894, -2608894, -2608894,\n+            -2608894, -2608894, -2608894, -2608894, -2608894, -2608894, -2608894, -2608894,\n+            -2608894, -2608894, -2608894, -2608894, -2608894, -2608894, -2608894, -2608894,\n+            -2608894, -2608894, -2608894, -2608894, -2608894, -2608894, -2608894, -2608894,\n+            -2608894, -2608894, -2608894, -2608894, -2608894, -2608894, -2608894, -2608894,\n+            -518909, -518909, -518909, -518909, -518909, -518909, -518909, -518909,\n+            -518909, -518909, -518909, -518909, -518909, -518909, -518909, -518909,\n+            -518909, -518909, -518909, -518909, -518909, -518909, -518909, -518909,\n+            -518909, -518909, -518909, -518909, -518909, -518909, -518909, -518909,\n+            -518909, -518909, -518909, -518909, -518909, -518909, -518909, -518909,\n+            -518909, -518909, -518909, -518909, -518909, -518909, -518909, -518909,\n+            -518909, -518909, -518909, -518909, -518909, -518909, -518909, -518909,\n+            -518909, -518909, -518909, -518909, -518909, -518909, -518909, -518909,\n+\n+            237124, 237124, 237124, 237124, 237124, 237124, 237124, 237124,\n+            237124, 237124, 237124, 237124, 237124, 237124, 237124, 237124,\n+            237124, 237124, 237124, 237124, 237124, 237124, 237124, 237124,\n+            237124, 237124, 237124, 237124, 237124, 237124, 237124, 237124,\n+            -777960, -777960, -777960, -777960, -777960, -777960, -777960, -777960,\n+            -777960, -777960, -777960, -777960, -777960, -777960, -777960, -777960,\n+            -777960, -777960, -777960, -777960, -777960, -777960, -777960, -777960,\n+            -777960, -777960, -777960, -777960, -777960, -777960, -777960, -777960,\n+            -876248, -876248, -876248, -876248, -876248, -876248, -876248, -876248,\n+            -876248, -876248, -876248, -876248, -876248, -876248, -876248, -876248,\n+            -876248, -876248, -876248, -876248, -876248, -876248, -876248, -876248,\n+            -876248, -876248, -876248, -876248, -876248, -876248, -876248, -876248,\n+            466468, 466468, 466468, 466468, 466468, 466468, 466468, 466468,\n+            466468, 466468, 466468, 466468, 466468, 466468, 466468, 466468,\n+            466468, 466468, 466468, 466468, 466468, 466468, 466468, 466468,\n+            466468, 466468, 466468, 466468, 466468, 466468, 466468, 466468,\n+\n+            1826347, 1826347, 1826347, 1826347, 1826347, 1826347, 1826347, 1826347,\n+            1826347, 1826347, 1826347, 1826347, 1826347, 1826347, 1826347, 1826347,\n+            2353451, 2353451, 2353451, 2353451, 2353451, 2353451, 2353451, 2353451,\n+            2353451, 2353451, 2353451, 2353451, 2353451, 2353451, 2353451, 2353451,\n+            -359251, -359251, -359251, -359251, -359251, -359251, -359251, -359251,\n+            -359251, -359251, -359251, -359251, -359251, -359251, -359251, -359251,\n+            -2091905, -2091905, -2091905, -2091905, -2091905, -2091905, -2091905, -2091905,\n+            -2091905, -2091905, -2091905, -2091905, -2091905, -2091905, -2091905, -2091905,\n+            3119733, 3119733, 3119733, 3119733, 3119733, 3119733, 3119733, 3119733,\n+            3119733, 3119733, 3119733, 3119733, 3119733, 3119733, 3119733, 3119733,\n+            -2884855, -2884855, -2884855, -2884855, -2884855, -2884855, -2884855, -2884855,\n+            -2884855, -2884855, -2884855, -2884855, -2884855, -2884855, -2884855, -2884855,\n+            3111497, 3111497, 3111497, 3111497, 3111497, 3111497, 3111497, 3111497,\n+            3111497, 3111497, 3111497, 3111497, 3111497, 3111497, 3111497, 3111497,\n+            2680103, 2680103, 2680103, 2680103, 2680103, 2680103, 2680103, 2680103,\n+            2680103, 2680103, 2680103, 2680103, 2680103, 2680103, 2680103, 2680103,\n+\n+            2725464, 2725464, 2725464, 2725464, 2725464, 2725464, 2725464, 2725464,\n+            1024112, 1024112, 1024112, 1024112, 1024112, 1024112, 1024112, 1024112,\n+            -1079900, -1079900, -1079900, -1079900, -1079900, -1079900, -1079900, -1079900,\n+            3585928, 3585928, 3585928, 3585928, 3585928, 3585928, 3585928, 3585928,\n+            -549488, -549488, -549488, -549488, -549488, -549488, -549488, -549488,\n+            -1119584, -1119584, -1119584, -1119584, -1119584, -1119584, -1119584, -1119584,\n+            2619752, 2619752, 2619752, 2619752, 2619752, 2619752, 2619752, 2619752,\n+            -2108549, -2108549, -2108549, -2108549, -2108549, -2108549, -2108549, -2108549,\n+            -2118186, -2118186, -2118186, -2118186, -2118186, -2118186, -2118186, -2118186,\n+            -3859737, -3859737, -3859737, -3859737, -3859737, -3859737, -3859737, -3859737,\n+            -1399561, -1399561, -1399561, -1399561, -1399561, -1399561, -1399561, -1399561,\n+            -3277672, -3277672, -3277672, -3277672, -3277672, -3277672, -3277672, -3277672,\n+            1757237, 1757237, 1757237, 1757237, 1757237, 1757237, 1757237, 1757237,\n+            -19422, -19422, -19422, -19422, -19422, -19422, -19422, -19422,\n+            4010497, 4010497, 4010497, 4010497, 4010497, 4010497, 4010497, 4010497,\n+            280005, 280005, 280005, 280005, 280005, 280005, 280005, 280005,\n+\n+            2706023, 2706023, 2706023, 2706023, 95776, 95776, 95776, 95776,\n+            3077325, 3077325, 3077325, 3077325, 3530437, 3530437, 3530437, 3530437,\n+            -1661693, -1661693, -1661693, -1661693, -3592148, -3592148, -3592148, -3592148,\n+            -2537516, -2537516, -2537516, -2537516, 3915439, 3915439, 3915439, 3915439,\n+            -3861115, -3861115, -3861115, -3861115, -3043716, -3043716, -3043716, -3043716,\n+            3574422, 3574422, 3574422, 3574422, -2867647, -2867647, -2867647, -2867647,\n+            3539968, 3539968, 3539968, 3539968, -300467, -300467, -300467, -300467,\n+            2348700, 2348700, 2348700, 2348700, -539299, -539299, -539299, -539299,\n+            -1699267, -1699267, -1699267, -1699267, -1643818, -1643818, -1643818, -1643818,\n+            3505694, 3505694, 3505694, 3505694, -3821735, -3821735, -3821735, -3821735,\n+            3507263, 3507263, 3507263, 3507263, -2140649, -2140649, -2140649, -2140649,\n+            -1600420, -1600420, -1600420, -1600420, 3699596, 3699596, 3699596, 3699596,\n+            811944, 811944, 811944, 811944, 531354, 531354, 531354, 531354,\n+            954230, 954230, 954230, 954230, 3881043, 3881043, 3881043, 3881043,\n+            3900724, 3900724, 3900724, 3900724, -2556880, -2556880, -2556880, -2556880,\n+            2071892, 2071892, 2071892, 2071892, -2797779, -2797779, -2797779, -2797779,\n+\n+            -3930395, -3930395, -1528703, -1528703, -3677745, -3677745, -3041255, -3041255,\n+            -1452451, -1452451, 3475950, 3475950, 2176455, 2176455, -1585221, -1585221,\n+            -1257611, -1257611, 1939314, 1939314, -4083598, -4083598, -1000202, -1000202,\n+            -3190144, -3190144, -3157330, -3157330, -3632928, -3632928, 126922, 126922,\n+            3412210, 3412210, -983419, -983419, 2147896, 2147896, 2715295, 2715295,\n+            -2967645, -2967645, -3693493, -3693493, -411027, -411027, -2477047, -2477047,\n+            -671102, -671102, -1228525, -1228525, -22981, -22981, -1308169, -1308169,\n+            -381987, -381987, 1349076, 1349076, 1852771, 1852771, -1430430, -1430430,\n+            -3343383, -3343383, 264944, 264944, 508951, 508951, 3097992, 3097992,\n+            44288, 44288, -1100098, -1100098, 904516, 904516, 3958618, 3958618,\n+            -3724342, -3724342, -8578, -8578, 1653064, 1653064, -3249728, -3249728,\n+            2389356, 2389356, -210977, -210977, 759969, 759969, -1316856, -1316856,\n+            189548, 189548, -3553272, -3553272, 3159746, 3159746, -1851402, -1851402,\n+            -2409325, -2409325, -177440, -177440, 1315589, 1315589, 1341330, 1341330,\n+            1285669, 1285669, -1584928, -1584928, -812732, -812732, -1439742, -1439742,\n+            -3019102, -3019102, -3881060, -3881060, -3628969, -3628969, 3839961, 3839961,\n+\n+            2091667, 3407706, 2316500, 3817976, -3342478, 2244091, -2446433, -3562462,\n+            266997, 2434439, -1235728, 3513181, -3520352, -3759364, -1197226, -3193378,\n+            900702, 1859098, 909542, 819034, 495491, -1613174, -43260, -522500,\n+            -655327, -3122442, 2031748, 3207046, -3556995, -525098, -768622, -3595838,\n+            342297, 286988, -2437823, 4108315, 3437287, -3342277, 1735879, 203044,\n+            2842341, 2691481, -2590150, 1265009, 4055324, 1247620, 2486353, 1595974,\n+            -3767016, 1250494, 2635921, -3548272, -2994039, 1869119, 1903435, -1050970,\n+            -1333058, 1237275, -3318210, -1430225, -451100, 1312455, 3306115, -1962642,\n+            -1279661, 1917081, -2546312, -1374803, 1500165, 777191, 2235880, 3406031,\n+            -542412, -2831860, -1671176, -1846953, -2584293, -3724270, 594136, -3776993,\n+            -2013608, 2432395, 2454455, -164721, 1957272, 3369112, 185531, -1207385,\n+            -3183426, 162844, 1616392, 3014001, 810149, 1652634, -3694233, -1799107,\n+            -3038916, 3523897, 3866901, 269760, 2213111, -975884, 1717735, 472078,\n+            -426683, 1723600, -1803090, 1910376, -1667432, -1104333, -260646, -3833893,\n+            -2939036, -2235985, -420899, -2286327, 183443, -976891, 1612842, -3545687,\n+            -554416, 3919660, -48306, -1362209, 3937738, 1400424, -846154, 1976782\n+    };\n+}\n","filename":"test\/micro\/org\/openjdk\/bench\/javax\/crypto\/full\/MLDSABench.java","additions":421,"deletions":0,"binary":false,"changes":421,"status":"added"}]}