{"files":[{"patch":"@@ -38,0 +38,1 @@\n+#include \"runtime\/safepoint.hpp\"\n@@ -205,1 +206,2 @@\n-  assert(Heap_lock->owner() != nullptr, \"Should be owned on this thread's behalf.\");\n+  assert(Heap_lock->owner() != nullptr || SafepointSynchronize::is_at_safepoint(),\n+         \"Should be owned on this thread's behalf or at safepoint.\");\n","filename":"src\/hotspot\/share\/gc\/g1\/g1Allocator.cpp","additions":3,"deletions":1,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -35,0 +35,1 @@\n+class G1HeapSizingPolicy;\n","filename":"src\/hotspot\/share\/gc\/g1\/g1Allocator.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -57,2 +57,2 @@\n-  if (result != nullptr) {\n-    return result;\n+  if (result == nullptr) {\n+    result = mutator_alloc_region(node_index)->attempt_allocation(min_word_size, desired_word_size, actual_word_size);\n@@ -61,1 +61,1 @@\n-  return mutator_alloc_region(node_index)->attempt_allocation(min_word_size, desired_word_size, actual_word_size);\n+  return result;\n","filename":"src\/hotspot\/share\/gc\/g1\/g1Allocator.inline.hpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -1,3110 +1,1 @@\n-\/*\n- * Copyright (c) 2001, 2025, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\n- *\/\n-\n-#include \"classfile\/classLoaderDataGraph.hpp\"\n-#include \"classfile\/metadataOnStackMark.hpp\"\n-#include \"classfile\/systemDictionary.hpp\"\n-#include \"code\/codeCache.hpp\"\n-#include \"compiler\/oopMap.hpp\"\n-#include \"gc\/g1\/g1Allocator.inline.hpp\"\n-#include \"gc\/g1\/g1Arguments.hpp\"\n-#include \"gc\/g1\/g1BarrierSet.hpp\"\n-#include \"gc\/g1\/g1BatchedTask.hpp\"\n-#include \"gc\/g1\/g1CollectedHeap.inline.hpp\"\n-#include \"gc\/g1\/g1CollectionSet.hpp\"\n-#include \"gc\/g1\/g1CollectionSetCandidates.hpp\"\n-#include \"gc\/g1\/g1CollectorState.hpp\"\n-#include \"gc\/g1\/g1ConcurrentMarkThread.inline.hpp\"\n-#include \"gc\/g1\/g1ConcurrentRefine.hpp\"\n-#include \"gc\/g1\/g1ConcurrentRefineThread.hpp\"\n-#include \"gc\/g1\/g1DirtyCardQueue.hpp\"\n-#include \"gc\/g1\/g1EvacStats.inline.hpp\"\n-#include \"gc\/g1\/g1FullCollector.hpp\"\n-#include \"gc\/g1\/g1GCCounters.hpp\"\n-#include \"gc\/g1\/g1GCParPhaseTimesTracker.hpp\"\n-#include \"gc\/g1\/g1GCPauseType.hpp\"\n-#include \"gc\/g1\/g1GCPhaseTimes.hpp\"\n-#include \"gc\/g1\/g1HeapRegion.inline.hpp\"\n-#include \"gc\/g1\/g1HeapRegionPrinter.hpp\"\n-#include \"gc\/g1\/g1HeapRegionRemSet.inline.hpp\"\n-#include \"gc\/g1\/g1HeapRegionSet.inline.hpp\"\n-#include \"gc\/g1\/g1HeapSizingPolicy.hpp\"\n-#include \"gc\/g1\/g1HeapTransition.hpp\"\n-#include \"gc\/g1\/g1HeapVerifier.hpp\"\n-#include \"gc\/g1\/g1InitLogger.hpp\"\n-#include \"gc\/g1\/g1MemoryPool.hpp\"\n-#include \"gc\/g1\/g1MonotonicArenaFreeMemoryTask.hpp\"\n-#include \"gc\/g1\/g1OopClosures.inline.hpp\"\n-#include \"gc\/g1\/g1ParallelCleaning.hpp\"\n-#include \"gc\/g1\/g1ParScanThreadState.inline.hpp\"\n-#include \"gc\/g1\/g1PeriodicGCTask.hpp\"\n-#include \"gc\/g1\/g1Policy.hpp\"\n-#include \"gc\/g1\/g1RedirtyCardsQueue.hpp\"\n-#include \"gc\/g1\/g1RegionPinCache.inline.hpp\"\n-#include \"gc\/g1\/g1RegionToSpaceMapper.hpp\"\n-#include \"gc\/g1\/g1RemSet.hpp\"\n-#include \"gc\/g1\/g1RootClosures.hpp\"\n-#include \"gc\/g1\/g1RootProcessor.hpp\"\n-#include \"gc\/g1\/g1SATBMarkQueueSet.hpp\"\n-#include \"gc\/g1\/g1ServiceThread.hpp\"\n-#include \"gc\/g1\/g1ThreadLocalData.hpp\"\n-#include \"gc\/g1\/g1Trace.hpp\"\n-#include \"gc\/g1\/g1UncommitRegionTask.hpp\"\n-#include \"gc\/g1\/g1VMOperations.hpp\"\n-#include \"gc\/g1\/g1YoungCollector.hpp\"\n-#include \"gc\/g1\/g1YoungGCAllocationFailureInjector.hpp\"\n-#include \"gc\/shared\/classUnloadingContext.hpp\"\n-#include \"gc\/shared\/concurrentGCBreakpoints.hpp\"\n-#include \"gc\/shared\/fullGCForwarding.hpp\"\n-#include \"gc\/shared\/gcBehaviours.hpp\"\n-#include \"gc\/shared\/gcHeapSummary.hpp\"\n-#include \"gc\/shared\/gcId.hpp\"\n-#include \"gc\/shared\/gcTimer.hpp\"\n-#include \"gc\/shared\/gcTraceTime.inline.hpp\"\n-#include \"gc\/shared\/isGCActiveMark.hpp\"\n-#include \"gc\/shared\/locationPrinter.inline.hpp\"\n-#include \"gc\/shared\/oopStorageParState.hpp\"\n-#include \"gc\/shared\/partialArrayState.hpp\"\n-#include \"gc\/shared\/referenceProcessor.inline.hpp\"\n-#include \"gc\/shared\/suspendibleThreadSet.hpp\"\n-#include \"gc\/shared\/taskqueue.inline.hpp\"\n-#include \"gc\/shared\/taskTerminator.hpp\"\n-#include \"gc\/shared\/tlab_globals.hpp\"\n-#include \"gc\/shared\/weakProcessor.inline.hpp\"\n-#include \"gc\/shared\/workerPolicy.hpp\"\n-#include \"logging\/log.hpp\"\n-#include \"memory\/allocation.hpp\"\n-#include \"memory\/heapInspection.hpp\"\n-#include \"memory\/iterator.hpp\"\n-#include \"memory\/memoryReserver.hpp\"\n-#include \"memory\/metaspaceUtils.hpp\"\n-#include \"memory\/resourceArea.hpp\"\n-#include \"memory\/universe.hpp\"\n-#include \"oops\/access.inline.hpp\"\n-#include \"oops\/compressedOops.inline.hpp\"\n-#include \"oops\/oop.inline.hpp\"\n-#include \"runtime\/atomic.hpp\"\n-#include \"runtime\/cpuTimeCounters.hpp\"\n-#include \"runtime\/handles.inline.hpp\"\n-#include \"runtime\/init.hpp\"\n-#include \"runtime\/java.hpp\"\n-#include \"runtime\/orderAccess.hpp\"\n-#include \"runtime\/threadSMR.hpp\"\n-#include \"runtime\/vmThread.hpp\"\n-#include \"utilities\/align.hpp\"\n-#include \"utilities\/autoRestore.hpp\"\n-#include \"utilities\/bitMap.inline.hpp\"\n-#include \"utilities\/globalDefinitions.hpp\"\n-#include \"utilities\/stack.inline.hpp\"\n-\n-size_t G1CollectedHeap::_humongous_object_threshold_in_words = 0;\n-\n-\/\/ INVARIANTS\/NOTES\n-\/\/\n-\/\/ All allocation activity covered by the G1CollectedHeap interface is\n-\/\/ serialized by acquiring the HeapLock.  This happens in mem_allocate\n-\/\/ and allocate_new_tlab, which are the \"entry\" points to the\n-\/\/ allocation code from the rest of the JVM.  (Note that this does not\n-\/\/ apply to TLAB allocation, which is not part of this interface: it\n-\/\/ is done by clients of this interface.)\n-\n-void G1RegionMappingChangedListener::reset_from_card_cache(uint start_idx, size_t num_regions) {\n-  G1HeapRegionRemSet::invalidate_from_card_cache(start_idx, num_regions);\n-}\n-\n-void G1RegionMappingChangedListener::on_commit(uint start_idx, size_t num_regions, bool zero_filled) {\n-  \/\/ The from card cache is not the memory that is actually committed. So we cannot\n-  \/\/ take advantage of the zero_filled parameter.\n-  reset_from_card_cache(start_idx, num_regions);\n-}\n-\n-void G1CollectedHeap::run_batch_task(G1BatchedTask* cl) {\n-  uint num_workers = MAX2(1u, MIN2(cl->num_workers_estimate(), workers()->active_workers()));\n-  cl->set_max_workers(num_workers);\n-  workers()->run_task(cl, num_workers);\n-}\n-\n-uint G1CollectedHeap::get_chunks_per_region() {\n-  uint log_region_size = G1HeapRegion::LogOfHRGrainBytes;\n-  \/\/ Limit the expected input values to current known possible values of the\n-  \/\/ (log) region size. Adjust as necessary after testing if changing the permissible\n-  \/\/ values for region size.\n-  assert(log_region_size >= 20 && log_region_size <= 29,\n-         \"expected value in [20,29], but got %u\", log_region_size);\n-  return 1u << (log_region_size \/ 2 - 4);\n-}\n-\n-G1HeapRegion* G1CollectedHeap::new_heap_region(uint hrs_index,\n-                                               MemRegion mr) {\n-  return new G1HeapRegion(hrs_index, bot(), mr, &_card_set_config);\n-}\n-\n-\/\/ Private methods.\n-\n-G1HeapRegion* G1CollectedHeap::new_region(size_t word_size,\n-                                          G1HeapRegionType type,\n-                                          bool do_expand,\n-                                          uint node_index) {\n-  assert(!is_humongous(word_size) || word_size <= G1HeapRegion::GrainWords,\n-         \"the only time we use this to allocate a humongous region is \"\n-         \"when we are allocating a single humongous region\");\n-\n-  G1HeapRegion* res = _hrm.allocate_free_region(type, node_index);\n-\n-  if (res == nullptr && do_expand) {\n-    \/\/ Currently, only attempts to allocate GC alloc regions set\n-    \/\/ do_expand to true. So, we should only reach here during a\n-    \/\/ safepoint.\n-    assert(SafepointSynchronize::is_at_safepoint(), \"invariant\");\n-\n-    log_debug(gc, ergo, heap)(\"Attempt heap expansion (region allocation request failed). Allocation request: %zuB\",\n-                              word_size * HeapWordSize);\n-\n-    assert(word_size * HeapWordSize < G1HeapRegion::GrainBytes,\n-           \"This kind of expansion should never be more than one region. Size: %zu\",\n-           word_size * HeapWordSize);\n-    if (expand_single_region(node_index)) {\n-      \/\/ Given that expand_single_region() succeeded in expanding the heap, and we\n-      \/\/ always expand the heap by an amount aligned to the heap\n-      \/\/ region size, the free list should in theory not be empty.\n-      \/\/ In either case allocate_free_region() will check for null.\n-      res = _hrm.allocate_free_region(type, node_index);\n-    }\n-  }\n-  return res;\n-}\n-\n-void G1CollectedHeap::set_humongous_metadata(G1HeapRegion* first_hr,\n-                                             uint num_regions,\n-                                             size_t word_size,\n-                                             bool update_remsets) {\n-  \/\/ Calculate the new top of the humongous object.\n-  HeapWord* obj_top = first_hr->bottom() + word_size;\n-  \/\/ The word size sum of all the regions used\n-  size_t word_size_sum = num_regions * G1HeapRegion::GrainWords;\n-  assert(word_size <= word_size_sum, \"sanity\");\n-\n-  \/\/ How many words memory we \"waste\" which cannot hold a filler object.\n-  size_t words_not_fillable = 0;\n-\n-  \/\/ Pad out the unused tail of the last region with filler\n-  \/\/ objects, for improved usage accounting.\n-\n-  \/\/ How many words can we use for filler objects.\n-  size_t words_fillable = word_size_sum - word_size;\n-\n-  if (words_fillable >= G1CollectedHeap::min_fill_size()) {\n-    G1CollectedHeap::fill_with_objects(obj_top, words_fillable);\n-  } else {\n-    \/\/ We have space to fill, but we cannot fit an object there.\n-    words_not_fillable = words_fillable;\n-    words_fillable = 0;\n-  }\n-\n-  \/\/ We will set up the first region as \"starts humongous\". This\n-  \/\/ will also update the BOT covering all the regions to reflect\n-  \/\/ that there is a single object that starts at the bottom of the\n-  \/\/ first region.\n-  first_hr->hr_clear(false \/* clear_space *\/);\n-  first_hr->set_starts_humongous(obj_top, words_fillable);\n-\n-  if (update_remsets) {\n-    _policy->remset_tracker()->update_at_allocate(first_hr);\n-  }\n-\n-  \/\/ Indices of first and last regions in the series.\n-  uint first = first_hr->hrm_index();\n-  uint last = first + num_regions - 1;\n-\n-  G1HeapRegion* hr = nullptr;\n-  for (uint i = first + 1; i <= last; ++i) {\n-    hr = region_at(i);\n-    hr->hr_clear(false \/* clear_space *\/);\n-    hr->set_continues_humongous(first_hr);\n-    if (update_remsets) {\n-      _policy->remset_tracker()->update_at_allocate(hr);\n-    }\n-  }\n-\n-  \/\/ Up to this point no concurrent thread would have been able to\n-  \/\/ do any scanning on any region in this series. All the top\n-  \/\/ fields still point to bottom, so the intersection between\n-  \/\/ [bottom,top] and [card_start,card_end] will be empty. Before we\n-  \/\/ update the top fields, we'll do a storestore to make sure that\n-  \/\/ no thread sees the update to top before the zeroing of the\n-  \/\/ object header and the BOT initialization.\n-  OrderAccess::storestore();\n-\n-  \/\/ Now, we will update the top fields of the \"continues humongous\"\n-  \/\/ regions except the last one.\n-  for (uint i = first; i < last; ++i) {\n-    hr = region_at(i);\n-    hr->set_top(hr->end());\n-  }\n-\n-  hr = region_at(last);\n-  \/\/ If we cannot fit a filler object, we must set top to the end\n-  \/\/ of the humongous object, otherwise we cannot iterate the heap\n-  \/\/ and the BOT will not be complete.\n-  hr->set_top(hr->end() - words_not_fillable);\n-\n-  assert(hr->bottom() < obj_top && obj_top <= hr->end(),\n-         \"obj_top should be in last region\");\n-\n-  assert(words_not_fillable == 0 ||\n-         first_hr->bottom() + word_size_sum - words_not_fillable == hr->top(),\n-         \"Miscalculation in humongous allocation\");\n-}\n-\n-HeapWord*\n-G1CollectedHeap::humongous_obj_allocate_initialize_regions(G1HeapRegion* first_hr,\n-                                                           uint num_regions,\n-                                                           size_t word_size) {\n-  assert(first_hr != nullptr, \"pre-condition\");\n-  assert(is_humongous(word_size), \"word_size should be humongous\");\n-  assert(num_regions * G1HeapRegion::GrainWords >= word_size, \"pre-condition\");\n-\n-  \/\/ Index of last region in the series.\n-  uint first = first_hr->hrm_index();\n-  uint last = first + num_regions - 1;\n-\n-  \/\/ We need to initialize the region(s) we just discovered. This is\n-  \/\/ a bit tricky given that it can happen concurrently with\n-  \/\/ refinement threads refining cards on these regions and\n-  \/\/ potentially wanting to refine the BOT as they are scanning\n-  \/\/ those cards (this can happen shortly after a cleanup; see CR\n-  \/\/ 6991377). So we have to set up the region(s) carefully and in\n-  \/\/ a specific order.\n-\n-  \/\/ The passed in hr will be the \"starts humongous\" region. The header\n-  \/\/ of the new object will be placed at the bottom of this region.\n-  HeapWord* new_obj = first_hr->bottom();\n-\n-  \/\/ First, we need to zero the header of the space that we will be\n-  \/\/ allocating. When we update top further down, some refinement\n-  \/\/ threads might try to scan the region. By zeroing the header we\n-  \/\/ ensure that any thread that will try to scan the region will\n-  \/\/ come across the zero klass word and bail out.\n-  \/\/\n-  \/\/ NOTE: It would not have been correct to have used\n-  \/\/ CollectedHeap::fill_with_object() and make the space look like\n-  \/\/ an int array. The thread that is doing the allocation will\n-  \/\/ later update the object header to a potentially different array\n-  \/\/ type and, for a very short period of time, the klass and length\n-  \/\/ fields will be inconsistent. This could cause a refinement\n-  \/\/ thread to calculate the object size incorrectly.\n-  Copy::fill_to_words(new_obj, oopDesc::header_size(), 0);\n-\n-  \/\/ Next, update the metadata for the regions.\n-  set_humongous_metadata(first_hr, num_regions, word_size, true);\n-\n-  G1HeapRegion* last_hr = region_at(last);\n-  size_t used = byte_size(first_hr->bottom(), last_hr->top());\n-\n-  increase_used(used);\n-\n-  for (uint i = first; i <= last; ++i) {\n-    G1HeapRegion *hr = region_at(i);\n-    _humongous_set.add(hr);\n-    G1HeapRegionPrinter::alloc(hr);\n-  }\n-\n-  return new_obj;\n-}\n-\n-size_t G1CollectedHeap::humongous_obj_size_in_regions(size_t word_size) {\n-  assert(is_humongous(word_size), \"Object of size %zu must be humongous here\", word_size);\n-  return align_up(word_size, G1HeapRegion::GrainWords) \/ G1HeapRegion::GrainWords;\n-}\n-\n-\/\/ If could fit into free regions w\/o expansion, try.\n-\/\/ Otherwise, if can expand, do so.\n-\/\/ Otherwise, if using ex regions might help, try with ex given back.\n-HeapWord* G1CollectedHeap::humongous_obj_allocate(size_t word_size) {\n-  assert_heap_locked_or_at_safepoint(true \/* should_be_vm_thread *\/);\n-\n-  _verifier->verify_region_sets_optional();\n-\n-  uint obj_regions = (uint) humongous_obj_size_in_regions(word_size);\n-  if (obj_regions > num_available_regions()) {\n-    \/\/ Can't satisfy this allocation; early-return.\n-    return nullptr;\n-  }\n-\n-  \/\/ Policy: First try to allocate a humongous object in the free list.\n-  G1HeapRegion* humongous_start = _hrm.allocate_humongous(obj_regions);\n-  if (humongous_start == nullptr) {\n-    \/\/ Policy: We could not find enough regions for the humongous object in the\n-    \/\/ free list. Look through the heap to find a mix of free and uncommitted regions.\n-    \/\/ If so, expand the heap and allocate the humongous object.\n-    humongous_start = _hrm.expand_and_allocate_humongous(obj_regions);\n-    if (humongous_start != nullptr) {\n-      \/\/ We managed to find a region by expanding the heap.\n-      log_debug(gc, ergo, heap)(\"Heap expansion (humongous allocation request). Allocation request: %zuB\",\n-                                word_size * HeapWordSize);\n-      policy()->record_new_heap_size(num_committed_regions());\n-    } else {\n-      \/\/ Policy: Potentially trigger a defragmentation GC.\n-    }\n-  }\n-\n-  HeapWord* result = nullptr;\n-  if (humongous_start != nullptr) {\n-    result = humongous_obj_allocate_initialize_regions(humongous_start, obj_regions, word_size);\n-    assert(result != nullptr, \"it should always return a valid result\");\n-\n-    \/\/ A successful humongous object allocation changes the used space\n-    \/\/ information of the old generation so we need to recalculate the\n-    \/\/ sizes and update the jstat counters here.\n-    monitoring_support()->update_sizes();\n-  }\n-\n-  _verifier->verify_region_sets_optional();\n-\n-  return result;\n-}\n-\n-HeapWord* G1CollectedHeap::allocate_new_tlab(size_t min_size,\n-                                             size_t requested_size,\n-                                             size_t* actual_size) {\n-  assert_heap_not_locked_and_not_at_safepoint();\n-  assert(!is_humongous(requested_size), \"we do not allow humongous TLABs\");\n-\n-  return attempt_allocation(min_size, requested_size, actual_size);\n-}\n-\n-HeapWord*\n-G1CollectedHeap::mem_allocate(size_t word_size,\n-                              bool*  gc_overhead_limit_was_exceeded) {\n-  assert_heap_not_locked_and_not_at_safepoint();\n-\n-  if (is_humongous(word_size)) {\n-    return attempt_allocation_humongous(word_size);\n-  }\n-  size_t dummy = 0;\n-  return attempt_allocation(word_size, word_size, &dummy);\n-}\n-\n-HeapWord* G1CollectedHeap::attempt_allocation_slow(uint node_index, size_t word_size) {\n-  ResourceMark rm; \/\/ For retrieving the thread names in log messages.\n-\n-  \/\/ Make sure you read the note in attempt_allocation_humongous().\n-\n-  assert_heap_not_locked_and_not_at_safepoint();\n-  assert(!is_humongous(word_size), \"attempt_allocation_slow() should not \"\n-         \"be called for humongous allocation requests\");\n-\n-  \/\/ We should only get here after the first-level allocation attempt\n-  \/\/ (attempt_allocation()) failed to allocate.\n-\n-  \/\/ We will loop until a) we manage to successfully perform the allocation or b)\n-  \/\/ successfully schedule a collection which fails to perform the allocation.\n-  \/\/ Case b) is the only case when we'll return null.\n-  HeapWord* result = nullptr;\n-  for (uint try_count = 1; \/* we'll return *\/; try_count++) {\n-    uint gc_count_before;\n-\n-    {\n-      MutexLocker x(Heap_lock);\n-\n-      \/\/ Now that we have the lock, we first retry the allocation in case another\n-      \/\/ thread changed the region while we were waiting to acquire the lock.\n-      result = _allocator->attempt_allocation_locked(node_index, word_size);\n-      if (result != nullptr) {\n-        return result;\n-      }\n-\n-      \/\/ Read the GC count while still holding the Heap_lock.\n-      gc_count_before = total_collections();\n-    }\n-\n-    bool succeeded;\n-    result = do_collection_pause(word_size, gc_count_before, &succeeded, GCCause::_g1_inc_collection_pause);\n-    if (succeeded) {\n-      log_trace(gc, alloc)(\"%s: Successfully scheduled collection returning \" PTR_FORMAT,\n-                           Thread::current()->name(), p2i(result));\n-      return result;\n-    }\n-\n-    log_trace(gc, alloc)(\"%s: Unsuccessfully scheduled collection allocating %zu words\",\n-                         Thread::current()->name(), word_size);\n-\n-    \/\/ We can reach here if we were unsuccessful in scheduling a collection (because\n-    \/\/ another thread beat us to it). In this case immeditealy retry the allocation\n-    \/\/ attempt because another thread successfully performed a collection and possibly\n-    \/\/ reclaimed enough space. The first attempt (without holding the Heap_lock) is\n-    \/\/ here and the follow-on attempt will be at the start of the next loop\n-    \/\/ iteration (after taking the Heap_lock).\n-    size_t dummy = 0;\n-    result = _allocator->attempt_allocation(node_index, word_size, word_size, &dummy);\n-    if (result != nullptr) {\n-      return result;\n-    }\n-\n-    \/\/ Give a warning if we seem to be looping forever.\n-    if ((QueuedAllocationWarningCount > 0) &&\n-        (try_count % QueuedAllocationWarningCount == 0)) {\n-      log_warning(gc, alloc)(\"%s:  Retried allocation %u times for %zu words\",\n-                             Thread::current()->name(), try_count, word_size);\n-    }\n-  }\n-\n-  ShouldNotReachHere();\n-  return nullptr;\n-}\n-\n-template <typename Func>\n-void G1CollectedHeap::iterate_regions_in_range(MemRegion range, const Func& func) {\n-  \/\/ Mark each G1 region touched by the range as old, add it to\n-  \/\/ the old set, and set top.\n-  G1HeapRegion* curr_region = _hrm.addr_to_region(range.start());\n-  G1HeapRegion* end_region = _hrm.addr_to_region(range.last());\n-\n-  while (curr_region != nullptr) {\n-    bool is_last = curr_region == end_region;\n-    G1HeapRegion* next_region = is_last ? nullptr : _hrm.next_region_in_heap(curr_region);\n-\n-    func(curr_region, is_last);\n-\n-    curr_region = next_region;\n-  }\n-}\n-\n-HeapWord* G1CollectedHeap::alloc_archive_region(size_t word_size, HeapWord* preferred_addr) {\n-  assert(!is_init_completed(), \"Expect to be called at JVM init time\");\n-  MutexLocker x(Heap_lock);\n-\n-  MemRegion reserved = _hrm.reserved();\n-\n-  if (reserved.word_size() <= word_size) {\n-    log_info(gc, heap)(\"Unable to allocate regions as archive heap is too large; size requested = %zu\"\n-                       \" bytes, heap = %zu bytes\", word_size * HeapWordSize, reserved.byte_size());\n-    return nullptr;\n-  }\n-\n-  \/\/ Temporarily disable pretouching of heap pages. This interface is used\n-  \/\/ when mmap'ing archived heap data in, so pre-touching is wasted.\n-  FlagSetting fs(AlwaysPreTouch, false);\n-\n-  size_t commits = 0;\n-  \/\/ Attempt to allocate towards the end of the heap.\n-  HeapWord* start_addr = reserved.end() - align_up(word_size, G1HeapRegion::GrainWords);\n-  MemRegion range = MemRegion(start_addr, word_size);\n-  HeapWord* last_address = range.last();\n-  if (!_hrm.allocate_containing_regions(range, &commits, workers())) {\n-    return nullptr;\n-  }\n-  increase_used(word_size * HeapWordSize);\n-  if (commits != 0) {\n-    log_debug(gc, ergo, heap)(\"Attempt heap expansion (allocate archive regions). Total size: %zuB\",\n-                              G1HeapRegion::GrainWords * HeapWordSize * commits);\n-  }\n-\n-  \/\/ Mark each G1 region touched by the range as old, add it to\n-  \/\/ the old set, and set top.\n-  auto set_region_to_old = [&] (G1HeapRegion* r, bool is_last) {\n-    assert(r->is_empty(), \"Region already in use (%u)\", r->hrm_index());\n-\n-    HeapWord* top = is_last ? last_address + 1 : r->end();\n-    r->set_top(top);\n-\n-    r->set_old();\n-    G1HeapRegionPrinter::alloc(r);\n-    _old_set.add(r);\n-  };\n-\n-  iterate_regions_in_range(range, set_region_to_old);\n-  return start_addr;\n-}\n-\n-void G1CollectedHeap::populate_archive_regions_bot(MemRegion range) {\n-  assert(!is_init_completed(), \"Expect to be called at JVM init time\");\n-\n-  iterate_regions_in_range(range,\n-                           [&] (G1HeapRegion* r, bool is_last) {\n-                             r->update_bot();\n-                           });\n-}\n-\n-void G1CollectedHeap::dealloc_archive_regions(MemRegion range) {\n-  assert(!is_init_completed(), \"Expect to be called at JVM init time\");\n-  MemRegion reserved = _hrm.reserved();\n-  size_t size_used = 0;\n-\n-  \/\/ Free the G1 regions that are within the specified range.\n-  MutexLocker x(Heap_lock);\n-  HeapWord* start_address = range.start();\n-  HeapWord* last_address = range.last();\n-\n-  assert(reserved.contains(start_address) && reserved.contains(last_address),\n-         \"MemRegion outside of heap [\" PTR_FORMAT \", \" PTR_FORMAT \"]\",\n-         p2i(start_address), p2i(last_address));\n-  size_used += range.byte_size();\n-\n-  uint max_shrink_count = 0;\n-  if (capacity() > MinHeapSize) {\n-    size_t max_shrink_bytes = capacity() - MinHeapSize;\n-    max_shrink_count = (uint)(max_shrink_bytes \/ G1HeapRegion::GrainBytes);\n-  }\n-\n-  uint shrink_count = 0;\n-  \/\/ Free, empty and uncommit regions with CDS archive content.\n-  auto dealloc_archive_region = [&] (G1HeapRegion* r, bool is_last) {\n-    guarantee(r->is_old(), \"Expected old region at index %u\", r->hrm_index());\n-    _old_set.remove(r);\n-    r->set_free();\n-    r->set_top(r->bottom());\n-    if (shrink_count < max_shrink_count) {\n-      _hrm.shrink_at(r->hrm_index(), 1);\n-      shrink_count++;\n-    } else {\n-      _hrm.insert_into_free_list(r);\n-    }\n-  };\n-\n-  iterate_regions_in_range(range, dealloc_archive_region);\n-\n-  if (shrink_count != 0) {\n-    log_debug(gc, ergo, heap)(\"Attempt heap shrinking (CDS archive regions). Total size: %zuB (%u Regions)\",\n-                              G1HeapRegion::GrainWords * HeapWordSize * shrink_count, shrink_count);\n-    \/\/ Explicit uncommit.\n-    uncommit_regions(shrink_count);\n-  }\n-  decrease_used(size_used);\n-}\n-\n-inline HeapWord* G1CollectedHeap::attempt_allocation(size_t min_word_size,\n-                                                     size_t desired_word_size,\n-                                                     size_t* actual_word_size) {\n-  assert_heap_not_locked_and_not_at_safepoint();\n-  assert(!is_humongous(desired_word_size), \"attempt_allocation() should not \"\n-         \"be called for humongous allocation requests\");\n-\n-  \/\/ Fix NUMA node association for the duration of this allocation\n-  const uint node_index = _allocator->current_node_index();\n-\n-  HeapWord* result = _allocator->attempt_allocation(node_index, min_word_size, desired_word_size, actual_word_size);\n-\n-  if (result == nullptr) {\n-    *actual_word_size = desired_word_size;\n-    result = attempt_allocation_slow(node_index, desired_word_size);\n-  }\n-\n-  assert_heap_not_locked();\n-  if (result != nullptr) {\n-    assert(*actual_word_size != 0, \"Actual size must have been set here\");\n-    dirty_young_block(result, *actual_word_size);\n-  } else {\n-    *actual_word_size = 0;\n-  }\n-\n-  return result;\n-}\n-\n-HeapWord* G1CollectedHeap::attempt_allocation_humongous(size_t word_size) {\n-  ResourceMark rm; \/\/ For retrieving the thread names in log messages.\n-\n-  \/\/ The structure of this method has a lot of similarities to\n-  \/\/ attempt_allocation_slow(). The reason these two were not merged\n-  \/\/ into a single one is that such a method would require several \"if\n-  \/\/ allocation is not humongous do this, otherwise do that\"\n-  \/\/ conditional paths which would obscure its flow. In fact, an early\n-  \/\/ version of this code did use a unified method which was harder to\n-  \/\/ follow and, as a result, it had subtle bugs that were hard to\n-  \/\/ track down. So keeping these two methods separate allows each to\n-  \/\/ be more readable. It will be good to keep these two in sync as\n-  \/\/ much as possible.\n-\n-  assert_heap_not_locked_and_not_at_safepoint();\n-  assert(is_humongous(word_size), \"attempt_allocation_humongous() \"\n-         \"should only be called for humongous allocations\");\n-\n-  \/\/ Humongous objects can exhaust the heap quickly, so we should check if we\n-  \/\/ need to start a marking cycle at each humongous object allocation. We do\n-  \/\/ the check before we do the actual allocation. The reason for doing it\n-  \/\/ before the allocation is that we avoid having to keep track of the newly\n-  \/\/ allocated memory while we do a GC.\n-  if (policy()->need_to_start_conc_mark(\"concurrent humongous allocation\",\n-                                        word_size)) {\n-    collect(GCCause::_g1_humongous_allocation);\n-  }\n-\n-  \/\/ We will loop until a) we manage to successfully perform the allocation or b)\n-  \/\/ successfully schedule a collection which fails to perform the allocation.\n-  \/\/ Case b) is the only case when we'll return null.\n-  HeapWord* result = nullptr;\n-  for (uint try_count = 1; \/* we'll return *\/; try_count++) {\n-    uint gc_count_before;\n-\n-\n-    {\n-      MutexLocker x(Heap_lock);\n-\n-      size_t size_in_regions = humongous_obj_size_in_regions(word_size);\n-      \/\/ Given that humongous objects are not allocated in young\n-      \/\/ regions, we'll first try to do the allocation without doing a\n-      \/\/ collection hoping that there's enough space in the heap.\n-      result = humongous_obj_allocate(word_size);\n-      if (result != nullptr) {\n-        policy()->old_gen_alloc_tracker()->\n-          add_allocated_humongous_bytes_since_last_gc(size_in_regions * G1HeapRegion::GrainBytes);\n-        return result;\n-      }\n-\n-      \/\/ Read the GC count while still holding the Heap_lock.\n-      gc_count_before = total_collections();\n-    }\n-\n-    bool succeeded;\n-    result = do_collection_pause(word_size, gc_count_before, &succeeded, GCCause::_g1_humongous_allocation);\n-    if (succeeded) {\n-      log_trace(gc, alloc)(\"%s: Successfully scheduled collection returning \" PTR_FORMAT,\n-                           Thread::current()->name(), p2i(result));\n-      if (result != nullptr) {\n-        size_t size_in_regions = humongous_obj_size_in_regions(word_size);\n-        policy()->old_gen_alloc_tracker()->\n-          record_collection_pause_humongous_allocation(size_in_regions * G1HeapRegion::GrainBytes);\n-      }\n-      return result;\n-    }\n-\n-    log_trace(gc, alloc)(\"%s: Unsuccessfully scheduled collection allocating %zu\",\n-                         Thread::current()->name(), word_size);\n-\n-    \/\/ We can reach here if we were unsuccessful in scheduling a collection (because\n-    \/\/ another thread beat us to it).\n-    \/\/ Humongous object allocation always needs a lock, so we wait for the retry\n-    \/\/ in the next iteration of the loop, unlike for the regular iteration case.\n-    \/\/ Give a warning if we seem to be looping forever.\n-\n-    if ((QueuedAllocationWarningCount > 0) &&\n-        (try_count % QueuedAllocationWarningCount == 0)) {\n-      log_warning(gc, alloc)(\"%s: Retried allocation %u times for %zu words\",\n-                             Thread::current()->name(), try_count, word_size);\n-    }\n-  }\n-\n-  ShouldNotReachHere();\n-  return nullptr;\n-}\n-\n-HeapWord* G1CollectedHeap::attempt_allocation_at_safepoint(size_t word_size,\n-                                                           bool expect_null_mutator_alloc_region) {\n-  assert_at_safepoint_on_vm_thread();\n-  assert(!_allocator->has_mutator_alloc_region() || !expect_null_mutator_alloc_region,\n-         \"the current alloc region was unexpectedly found to be non-null\");\n-\n-  \/\/ Fix NUMA node association for the duration of this allocation\n-  const uint node_index = _allocator->current_node_index();\n-\n-  if (!is_humongous(word_size)) {\n-    return _allocator->attempt_allocation_locked(node_index, word_size);\n-  } else {\n-    HeapWord* result = humongous_obj_allocate(word_size);\n-    if (result != nullptr && policy()->need_to_start_conc_mark(\"STW humongous allocation\")) {\n-      collector_state()->set_initiate_conc_mark_if_possible(true);\n-    }\n-    return result;\n-  }\n-\n-  ShouldNotReachHere();\n-}\n-\n-class PostCompactionPrinterClosure: public G1HeapRegionClosure {\n-public:\n-  bool do_heap_region(G1HeapRegion* hr) {\n-    assert(!hr->is_young(), \"not expecting to find young regions\");\n-    G1HeapRegionPrinter::post_compaction(hr);\n-    return false;\n-  }\n-};\n-\n-void G1CollectedHeap::print_heap_after_full_collection() {\n-  \/\/ Post collection region logging.\n-  \/\/ We should do this after we potentially resize the heap so\n-  \/\/ that all the COMMIT \/ UNCOMMIT events are generated before\n-  \/\/ the compaction events.\n-  if (G1HeapRegionPrinter::is_active()) {\n-    PostCompactionPrinterClosure cl;\n-    heap_region_iterate(&cl);\n-  }\n-}\n-\n-bool G1CollectedHeap::abort_concurrent_cycle() {\n-  \/\/ Disable discovery and empty the discovered lists\n-  \/\/ for the CM ref processor.\n-  _ref_processor_cm->disable_discovery();\n-  _ref_processor_cm->abandon_partial_discovery();\n-  _ref_processor_cm->verify_no_references_recorded();\n-\n-  \/\/ Abandon current iterations of concurrent marking and concurrent\n-  \/\/ refinement, if any are in progress.\n-  return concurrent_mark()->concurrent_cycle_abort();\n-}\n-\n-void G1CollectedHeap::prepare_heap_for_full_collection() {\n-  \/\/ Make sure we'll choose a new allocation region afterwards.\n-  _allocator->release_mutator_alloc_regions();\n-  _allocator->abandon_gc_alloc_regions();\n-\n-  \/\/ We may have added regions to the current incremental collection\n-  \/\/ set between the last GC or pause and now. We need to clear the\n-  \/\/ incremental collection set and then start rebuilding it afresh\n-  \/\/ after this full GC.\n-  abandon_collection_set(collection_set());\n-\n-  _hrm.remove_all_free_regions();\n-}\n-\n-void G1CollectedHeap::verify_before_full_collection() {\n-  assert_used_and_recalculate_used_equal(this);\n-  if (!VerifyBeforeGC) {\n-    return;\n-  }\n-  if (!G1HeapVerifier::should_verify(G1HeapVerifier::G1VerifyFull)) {\n-    return;\n-  }\n-  _verifier->verify_region_sets_optional();\n-  _verifier->verify_before_gc();\n-  _verifier->verify_bitmap_clear(true \/* above_tams_only *\/);\n-}\n-\n-void G1CollectedHeap::prepare_for_mutator_after_full_collection(size_t allocation_word_size) {\n-  \/\/ Prepare heap for normal collections.\n-  assert(num_free_regions() == 0, \"we should not have added any free regions\");\n-  rebuild_region_sets(false \/* free_list_only *\/);\n-  abort_refinement();\n-  resize_heap_after_full_collection(allocation_word_size);\n-\n-  \/\/ Rebuild the code root lists for each region\n-  rebuild_code_roots();\n-\n-  start_new_collection_set();\n-  _allocator->init_mutator_alloc_regions();\n-\n-  \/\/ Post collection state updates.\n-  MetaspaceGC::compute_new_size();\n-}\n-\n-void G1CollectedHeap::abort_refinement() {\n-  \/\/ Discard all remembered set updates and reset refinement statistics.\n-  G1BarrierSet::dirty_card_queue_set().abandon_logs_and_stats();\n-  assert(G1BarrierSet::dirty_card_queue_set().num_cards() == 0,\n-         \"DCQS should be empty\");\n-  concurrent_refine()->get_and_reset_refinement_stats();\n-}\n-\n-void G1CollectedHeap::verify_after_full_collection() {\n-  if (!VerifyAfterGC) {\n-    return;\n-  }\n-  if (!G1HeapVerifier::should_verify(G1HeapVerifier::G1VerifyFull)) {\n-    return;\n-  }\n-  _hrm.verify_optional();\n-  _verifier->verify_region_sets_optional();\n-  _verifier->verify_after_gc();\n-  _verifier->verify_bitmap_clear(false \/* above_tams_only *\/);\n-\n-  \/\/ At this point there should be no regions in the\n-  \/\/ entire heap tagged as young.\n-  assert(check_young_list_empty(), \"young list should be empty at this point\");\n-\n-  \/\/ Note: since we've just done a full GC, concurrent\n-  \/\/ marking is no longer active. Therefore we need not\n-  \/\/ re-enable reference discovery for the CM ref processor.\n-  \/\/ That will be done at the start of the next marking cycle.\n-  \/\/ We also know that the STW processor should no longer\n-  \/\/ discover any new references.\n-  assert(!_ref_processor_stw->discovery_enabled(), \"Postcondition\");\n-  assert(!_ref_processor_cm->discovery_enabled(), \"Postcondition\");\n-  _ref_processor_stw->verify_no_references_recorded();\n-  _ref_processor_cm->verify_no_references_recorded();\n-}\n-\n-void G1CollectedHeap::do_full_collection(bool clear_all_soft_refs,\n-                                         bool do_maximal_compaction,\n-                                         size_t allocation_word_size) {\n-  assert_at_safepoint_on_vm_thread();\n-\n-  const bool do_clear_all_soft_refs = clear_all_soft_refs ||\n-      soft_ref_policy()->should_clear_all_soft_refs();\n-\n-  G1FullGCMark gc_mark;\n-  GCTraceTime(Info, gc) tm(\"Pause Full\", nullptr, gc_cause(), true);\n-  G1FullCollector collector(this, do_clear_all_soft_refs, do_maximal_compaction, gc_mark.tracer());\n-\n-  collector.prepare_collection();\n-  collector.collect();\n-  collector.complete_collection(allocation_word_size);\n-}\n-\n-void G1CollectedHeap::do_full_collection(bool clear_all_soft_refs) {\n-  \/\/ Currently, there is no facility in the do_full_collection(bool) API to notify\n-  \/\/ the caller that the collection did not succeed (e.g., because it was locked\n-  \/\/ out by the GC locker). So, right now, we'll ignore the return value.\n-\n-  do_full_collection(clear_all_soft_refs,\n-                     false \/* do_maximal_compaction *\/,\n-                     size_t(0) \/* allocation_word_size *\/);\n-}\n-\n-void G1CollectedHeap::upgrade_to_full_collection() {\n-  GCCauseSetter compaction(this, GCCause::_g1_compaction_pause);\n-  log_info(gc, ergo)(\"Attempting full compaction clearing soft references\");\n-  do_full_collection(true  \/* clear_all_soft_refs *\/,\n-                     false \/* do_maximal_compaction *\/,\n-                     size_t(0) \/* allocation_word_size *\/);\n-}\n-\n-\n-void G1CollectedHeap::resize_heap(size_t resize_bytes, bool should_expand) {\n-  if (should_expand) {\n-    expand(resize_bytes, _workers);\n-  } else {\n-    shrink(resize_bytes);\n-    uncommit_regions_if_necessary();\n-  }\n-}\n-\n-void G1CollectedHeap::resize_heap_after_full_collection(size_t allocation_word_size) {\n-  assert_at_safepoint_on_vm_thread();\n-\n-  bool should_expand;\n-  size_t resize_bytes = _heap_sizing_policy->full_collection_resize_amount(should_expand, allocation_word_size);\n-\n-  if (resize_bytes != 0) {\n-    resize_heap(resize_bytes, should_expand);\n-  }\n-}\n-\n-void G1CollectedHeap::resize_heap_after_young_collection(size_t allocation_word_size) {\n-  Ticks start = Ticks::now();\n-\n-  bool should_expand;\n-\n-  size_t resize_bytes = _heap_sizing_policy->young_collection_resize_amount(should_expand, allocation_word_size);\n-\n-  if (resize_bytes != 0) {\n-    resize_heap(resize_bytes, should_expand);\n-  }\n-\n-  phase_times()->record_resize_heap_time((Ticks::now() - start).seconds() * 1000.0);\n-}\n-\n-HeapWord* G1CollectedHeap::satisfy_failed_allocation_helper(size_t word_size,\n-                                                            bool do_gc,\n-                                                            bool maximal_compaction,\n-                                                            bool expect_null_mutator_alloc_region) {\n-  \/\/ Let's attempt the allocation first.\n-  HeapWord* result =\n-    attempt_allocation_at_safepoint(word_size,\n-                                    expect_null_mutator_alloc_region);\n-  if (result != nullptr) {\n-    return result;\n-  }\n-\n-  \/\/ In a G1 heap, we're supposed to keep allocation from failing by\n-  \/\/ incremental pauses.  Therefore, at least for now, we'll favor\n-  \/\/ expansion over collection.  (This might change in the future if we can\n-  \/\/ do something smarter than full collection to satisfy a failed alloc.)\n-  result = expand_and_allocate(word_size);\n-  if (result != nullptr) {\n-    return result;\n-  }\n-\n-  if (do_gc) {\n-    GCCauseSetter compaction(this, GCCause::_g1_compaction_pause);\n-    \/\/ Expansion didn't work, we'll try to do a Full GC.\n-    \/\/ If maximal_compaction is set we clear all soft references and don't\n-    \/\/ allow any dead wood to be left on the heap.\n-    if (maximal_compaction) {\n-      log_info(gc, ergo)(\"Attempting maximal full compaction clearing soft references\");\n-    } else {\n-      log_info(gc, ergo)(\"Attempting full compaction\");\n-    }\n-    do_full_collection(maximal_compaction \/* clear_all_soft_refs *\/,\n-                       maximal_compaction \/* do_maximal_compaction *\/,\n-                       word_size \/* allocation_word_size *\/);\n-  }\n-\n-  return nullptr;\n-}\n-\n-HeapWord* G1CollectedHeap::satisfy_failed_allocation(size_t word_size) {\n-  assert_at_safepoint_on_vm_thread();\n-\n-  \/\/ Attempts to allocate followed by Full GC.\n-  HeapWord* result =\n-    satisfy_failed_allocation_helper(word_size,\n-                                     true,  \/* do_gc *\/\n-                                     false, \/* maximum_collection *\/\n-                                     false \/* expect_null_mutator_alloc_region *\/);\n-\n-  if (result != nullptr) {\n-    return result;\n-  }\n-\n-  \/\/ Attempts to allocate followed by Full GC that will collect all soft references.\n-  result = satisfy_failed_allocation_helper(word_size,\n-                                            true, \/* do_gc *\/\n-                                            true, \/* maximum_collection *\/\n-                                            true \/* expect_null_mutator_alloc_region *\/);\n-\n-  if (result != nullptr) {\n-    return result;\n-  }\n-\n-  \/\/ Attempts to allocate, no GC\n-  result = satisfy_failed_allocation_helper(word_size,\n-                                            false, \/* do_gc *\/\n-                                            false, \/* maximum_collection *\/\n-                                            true  \/* expect_null_mutator_alloc_region *\/);\n-\n-  if (result != nullptr) {\n-    return result;\n-  }\n-\n-  assert(!soft_ref_policy()->should_clear_all_soft_refs(),\n-         \"Flag should have been handled and cleared prior to this point\");\n-\n-  \/\/ What else?  We might try synchronous finalization later.  If the total\n-  \/\/ space available is large enough for the allocation, then a more\n-  \/\/ complete compaction phase than we've tried so far might be\n-  \/\/ appropriate.\n-  return nullptr;\n-}\n-\n-\/\/ Attempting to expand the heap sufficiently\n-\/\/ to support an allocation of the given \"word_size\".  If\n-\/\/ successful, perform the allocation and return the address of the\n-\/\/ allocated block, or else null.\n-\n-HeapWord* G1CollectedHeap::expand_and_allocate(size_t word_size) {\n-  assert_at_safepoint_on_vm_thread();\n-\n-  _verifier->verify_region_sets_optional();\n-\n-  size_t expand_bytes = MAX2(word_size * HeapWordSize, MinHeapDeltaBytes);\n-  log_debug(gc, ergo, heap)(\"Attempt heap expansion (allocation request failed). Allocation request: %zuB\",\n-                            word_size * HeapWordSize);\n-\n-\n-  if (expand(expand_bytes, _workers)) {\n-    _hrm.verify_optional();\n-    _verifier->verify_region_sets_optional();\n-    return attempt_allocation_at_safepoint(word_size,\n-                                           false \/* expect_null_mutator_alloc_region *\/);\n-  }\n-  return nullptr;\n-}\n-\n-bool G1CollectedHeap::expand(size_t expand_bytes, WorkerThreads* pretouch_workers) {\n-  assert(expand_bytes > 0, \"precondition\");\n-\n-  size_t aligned_expand_bytes = os::align_up_vm_page_size(expand_bytes);\n-  aligned_expand_bytes = align_up(aligned_expand_bytes, G1HeapRegion::GrainBytes);\n-\n-  uint num_regions_to_expand = (uint)(aligned_expand_bytes \/ G1HeapRegion::GrainBytes);\n-\n-  log_debug(gc, ergo, heap)(\"Heap resize. Requested expansion amount: %zuB aligned expansion amount: %zuB (%u regions)\",\n-                            expand_bytes, aligned_expand_bytes, num_regions_to_expand);\n-\n-  if (num_inactive_regions() == 0) {\n-    log_debug(gc, ergo, heap)(\"Heap resize. Did not expand the heap (heap already fully expanded)\");\n-    return false;\n-  }\n-\n-  uint expanded_by = _hrm.expand_by(num_regions_to_expand, pretouch_workers);\n-\n-  size_t actual_expand_bytes = expanded_by * G1HeapRegion::GrainBytes;\n-  assert(actual_expand_bytes <= aligned_expand_bytes, \"post-condition\");\n-  policy()->record_new_heap_size(num_committed_regions());\n-\n-  return true;\n-}\n-\n-bool G1CollectedHeap::expand_single_region(uint node_index) {\n-  uint expanded_by = _hrm.expand_on_preferred_node(node_index);\n-\n-  if (expanded_by == 0) {\n-    assert(num_inactive_regions() == 0, \"Should be no regions left, available: %u\", num_inactive_regions());\n-    log_debug(gc, ergo, heap)(\"Did not expand the heap (heap already fully expanded)\");\n-    return false;\n-  }\n-\n-  policy()->record_new_heap_size(num_committed_regions());\n-  return true;\n-}\n-\n-void G1CollectedHeap::shrink_helper(size_t shrink_bytes) {\n-  assert(shrink_bytes > 0, \"must be\");\n-  assert(is_aligned(shrink_bytes, G1HeapRegion::GrainBytes),\n-         \"Shrink request for %zuB not aligned to heap region size %zuB\",\n-         shrink_bytes, G1HeapRegion::GrainBytes);\n-\n-  uint num_regions_to_remove = (uint)(shrink_bytes \/ G1HeapRegion::GrainBytes);\n-\n-  uint num_regions_removed = _hrm.shrink_by(num_regions_to_remove);\n-  size_t shrunk_bytes = num_regions_removed * G1HeapRegion::GrainBytes;\n-\n-  log_debug(gc, ergo, heap)(\"Heap resize. Requested shrinking amount: %zuB actual shrinking amount: %zuB (%u regions)\",\n-                            shrink_bytes, shrunk_bytes, num_regions_removed);\n-  if (num_regions_removed > 0) {\n-    policy()->record_new_heap_size(num_committed_regions());\n-  } else {\n-    log_debug(gc, ergo, heap)(\"Heap resize. Did not shrink the heap (heap shrinking operation failed)\");\n-  }\n-}\n-\n-void G1CollectedHeap::shrink(size_t shrink_bytes) {\n-  if (capacity() == min_capacity()) {\n-    log_debug(gc, ergo, heap)(\"Heap resize. Did not shrink the heap (heap already at minimum)\");\n-    return;\n-  }\n-\n-  size_t aligned_shrink_bytes = os::align_down_vm_page_size(shrink_bytes);\n-  aligned_shrink_bytes = align_down(aligned_shrink_bytes, G1HeapRegion::GrainBytes);\n-\n-  aligned_shrink_bytes = capacity() - MAX2(capacity() - aligned_shrink_bytes, min_capacity());\n-  assert(is_aligned(aligned_shrink_bytes, G1HeapRegion::GrainBytes), \"Bytes to shrink %zuB not aligned\", aligned_shrink_bytes);\n-\n-  log_debug(gc, ergo, heap)(\"Heap resize. Requested shrink amount: %zuB aligned shrink amount: %zuB\",\n-                            shrink_bytes, aligned_shrink_bytes);\n-\n-  if (aligned_shrink_bytes == 0) {\n-    log_debug(gc, ergo, heap)(\"Heap resize. Did not shrink the heap (shrink request too small)\");\n-    return;\n-  }\n-\n-  _verifier->verify_region_sets_optional();\n-\n-  \/\/ We should only reach here at the end of a Full GC or during Remark which\n-  \/\/ means we should not not be holding to any GC alloc regions. The method\n-  \/\/ below will make sure of that and do any remaining clean up.\n-  _allocator->abandon_gc_alloc_regions();\n-\n-  \/\/ Instead of tearing down \/ rebuilding the free lists here, we\n-  \/\/ could instead use the remove_all_pending() method on free_list to\n-  \/\/ remove only the ones that we need to remove.\n-  _hrm.remove_all_free_regions();\n-  shrink_helper(aligned_shrink_bytes);\n-  rebuild_region_sets(true \/* free_list_only *\/);\n-\n-  _hrm.verify_optional();\n-  _verifier->verify_region_sets_optional();\n-}\n-\n-class OldRegionSetChecker : public G1HeapRegionSetChecker {\n-public:\n-  void check_mt_safety() {\n-    \/\/ Master Old Set MT safety protocol:\n-    \/\/ (a) If we're at a safepoint, operations on the master old set\n-    \/\/ should be invoked:\n-    \/\/ - by the VM thread (which will serialize them), or\n-    \/\/ - by the GC workers while holding the FreeList_lock, if we're\n-    \/\/   at a safepoint for an evacuation pause (this lock is taken\n-    \/\/   anyway when an GC alloc region is retired so that a new one\n-    \/\/   is allocated from the free list), or\n-    \/\/ - by the GC workers while holding the OldSets_lock, if we're at a\n-    \/\/   safepoint for a cleanup pause.\n-    \/\/ (b) If we're not at a safepoint, operations on the master old set\n-    \/\/ should be invoked while holding the Heap_lock.\n-\n-    if (SafepointSynchronize::is_at_safepoint()) {\n-      guarantee(Thread::current()->is_VM_thread() ||\n-                FreeList_lock->owned_by_self() || OldSets_lock->owned_by_self(),\n-                \"master old set MT safety protocol at a safepoint\");\n-    } else {\n-      guarantee(Heap_lock->owned_by_self(), \"master old set MT safety protocol outside a safepoint\");\n-    }\n-  }\n-  bool is_correct_type(G1HeapRegion* hr) { return hr->is_old(); }\n-  const char* get_description() { return \"Old Regions\"; }\n-};\n-\n-class HumongousRegionSetChecker : public G1HeapRegionSetChecker {\n-public:\n-  void check_mt_safety() {\n-    \/\/ Humongous Set MT safety protocol:\n-    \/\/ (a) If we're at a safepoint, operations on the master humongous\n-    \/\/ set should be invoked by either the VM thread (which will\n-    \/\/ serialize them) or by the GC workers while holding the\n-    \/\/ OldSets_lock.\n-    \/\/ (b) If we're not at a safepoint, operations on the master\n-    \/\/ humongous set should be invoked while holding the Heap_lock.\n-\n-    if (SafepointSynchronize::is_at_safepoint()) {\n-      guarantee(Thread::current()->is_VM_thread() ||\n-                OldSets_lock->owned_by_self(),\n-                \"master humongous set MT safety protocol at a safepoint\");\n-    } else {\n-      guarantee(Heap_lock->owned_by_self(),\n-                \"master humongous set MT safety protocol outside a safepoint\");\n-    }\n-  }\n-  bool is_correct_type(G1HeapRegion* hr) { return hr->is_humongous(); }\n-  const char* get_description() { return \"Humongous Regions\"; }\n-};\n-\n-G1CollectedHeap::G1CollectedHeap() :\n-  CollectedHeap(),\n-  _service_thread(nullptr),\n-  _periodic_gc_task(nullptr),\n-  _free_arena_memory_task(nullptr),\n-  _workers(nullptr),\n-  _card_table(nullptr),\n-  _collection_pause_end(Ticks::now()),\n-  _old_set(\"Old Region Set\", new OldRegionSetChecker()),\n-  _humongous_set(\"Humongous Region Set\", new HumongousRegionSetChecker()),\n-  _bot(nullptr),\n-  _listener(),\n-  _numa(G1NUMA::create()),\n-  _hrm(),\n-  _allocator(nullptr),\n-  _allocation_failure_injector(),\n-  _verifier(nullptr),\n-  _summary_bytes_used(0),\n-  _bytes_used_during_gc(0),\n-  _survivor_evac_stats(\"Young\", YoungPLABSize, PLABWeight),\n-  _old_evac_stats(\"Old\", OldPLABSize, PLABWeight),\n-  _monitoring_support(nullptr),\n-  _num_humongous_objects(0),\n-  _num_humongous_reclaim_candidates(0),\n-  _collector_state(),\n-  _old_marking_cycles_started(0),\n-  _old_marking_cycles_completed(0),\n-  _eden(),\n-  _survivor(),\n-  _gc_timer_stw(new STWGCTimer()),\n-  _gc_tracer_stw(new G1NewTracer()),\n-  _policy(new G1Policy(_gc_timer_stw)),\n-  _heap_sizing_policy(nullptr),\n-  _collection_set(this, _policy),\n-  _rem_set(nullptr),\n-  _card_set_config(),\n-  _card_set_freelist_pool(G1CardSetConfiguration::num_mem_object_types()),\n-  _young_regions_cset_group(card_set_config(), &_card_set_freelist_pool, 1u \/* group_id *\/),\n-  _cm(nullptr),\n-  _cm_thread(nullptr),\n-  _cr(nullptr),\n-  _task_queues(nullptr),\n-  _partial_array_state_manager(nullptr),\n-  _ref_processor_stw(nullptr),\n-  _is_alive_closure_stw(this),\n-  _is_subject_to_discovery_stw(this),\n-  _ref_processor_cm(nullptr),\n-  _is_alive_closure_cm(),\n-  _is_subject_to_discovery_cm(this),\n-  _region_attr() {\n-\n-  _verifier = new G1HeapVerifier(this);\n-\n-  _allocator = new G1Allocator(this);\n-\n-  _heap_sizing_policy = G1HeapSizingPolicy::create(this, _policy->analytics());\n-\n-  _humongous_object_threshold_in_words = humongous_threshold_for(G1HeapRegion::GrainWords);\n-\n-  \/\/ Since filler arrays are never referenced, we can make them region sized.\n-  \/\/ This simplifies filling up the region in case we have some potentially\n-  \/\/ unreferenced (by Java code, but still in use by native code) pinned objects\n-  \/\/ in there.\n-  _filler_array_max_size = G1HeapRegion::GrainWords;\n-\n-  \/\/ Override the default _stack_chunk_max_size so that no humongous stack chunks are created\n-  _stack_chunk_max_size = _humongous_object_threshold_in_words;\n-\n-  uint n_queues = ParallelGCThreads;\n-  _task_queues = new G1ScannerTasksQueueSet(n_queues);\n-\n-  for (uint i = 0; i < n_queues; i++) {\n-    G1ScannerTasksQueue* q = new G1ScannerTasksQueue();\n-    _task_queues->register_queue(i, q);\n-  }\n-\n-  _partial_array_state_manager = new PartialArrayStateManager(n_queues);\n-\n-  _gc_tracer_stw->initialize();\n-}\n-\n-PartialArrayStateManager* G1CollectedHeap::partial_array_state_manager() const {\n-  return _partial_array_state_manager;\n-}\n-\n-G1RegionToSpaceMapper* G1CollectedHeap::create_aux_memory_mapper(const char* description,\n-                                                                 size_t size,\n-                                                                 size_t translation_factor) {\n-  size_t preferred_page_size = os::page_size_for_region_unaligned(size, 1);\n-\n-  \/\/ When a page size is given we don't want to mix large\n-  \/\/ and normal pages. If the size is not a multiple of the\n-  \/\/ page size it will be aligned up to achieve this.\n-  size_t alignment = os::vm_allocation_granularity();\n-  if (preferred_page_size != os::vm_page_size()) {\n-    alignment = MAX2(preferred_page_size, alignment);\n-    size = align_up(size, alignment);\n-  }\n-\n-  \/\/ Allocate a new reserved space, preferring to use large pages.\n-  ReservedSpace rs = MemoryReserver::reserve(size,\n-                                             alignment,\n-                                             preferred_page_size,\n-                                             mtGC);\n-\n-  size_t page_size = rs.page_size();\n-  G1RegionToSpaceMapper* result  =\n-    G1RegionToSpaceMapper::create_mapper(rs,\n-                                         size,\n-                                         page_size,\n-                                         G1HeapRegion::GrainBytes,\n-                                         translation_factor,\n-                                         mtGC);\n-\n-  os::trace_page_sizes_for_requested_size(description,\n-                                          size,\n-                                          preferred_page_size,\n-                                          rs.base(),\n-                                          rs.size(),\n-                                          page_size);\n-\n-  return result;\n-}\n-\n-jint G1CollectedHeap::initialize_concurrent_refinement() {\n-  jint ecode = JNI_OK;\n-  _cr = G1ConcurrentRefine::create(policy(), &ecode);\n-  return ecode;\n-}\n-\n-jint G1CollectedHeap::initialize_service_thread() {\n-  _service_thread = new G1ServiceThread();\n-  if (_service_thread->osthread() == nullptr) {\n-    vm_shutdown_during_initialization(\"Could not create G1ServiceThread\");\n-    return JNI_ENOMEM;\n-  }\n-  return JNI_OK;\n-}\n-\n-jint G1CollectedHeap::initialize() {\n-\n-  if (!os::is_thread_cpu_time_supported()) {\n-    vm_exit_during_initialization(\"G1 requires cpu time gathering support\");\n-  }\n-  \/\/ Necessary to satisfy locking discipline assertions.\n-\n-  MutexLocker x(Heap_lock);\n-\n-  \/\/ While there are no constraints in the GC code that HeapWordSize\n-  \/\/ be any particular value, there are multiple other areas in the\n-  \/\/ system which believe this to be true (e.g. oop->object_size in some\n-  \/\/ cases incorrectly returns the size in wordSize units rather than\n-  \/\/ HeapWordSize).\n-  guarantee(HeapWordSize == wordSize, \"HeapWordSize must equal wordSize\");\n-\n-  size_t init_byte_size = InitialHeapSize;\n-  size_t reserved_byte_size = G1Arguments::heap_reserved_size_bytes();\n-\n-  \/\/ Ensure that the sizes are properly aligned.\n-  Universe::check_alignment(init_byte_size, G1HeapRegion::GrainBytes, \"g1 heap\");\n-  Universe::check_alignment(reserved_byte_size, G1HeapRegion::GrainBytes, \"g1 heap\");\n-  Universe::check_alignment(reserved_byte_size, HeapAlignment, \"g1 heap\");\n-\n-  \/\/ Reserve the maximum.\n-\n-  \/\/ When compressed oops are enabled, the preferred heap base\n-  \/\/ is calculated by subtracting the requested size from the\n-  \/\/ 32Gb boundary and using the result as the base address for\n-  \/\/ heap reservation. If the requested size is not aligned to\n-  \/\/ G1HeapRegion::GrainBytes (i.e. the alignment that is passed\n-  \/\/ into the ReservedHeapSpace constructor) then the actual\n-  \/\/ base of the reserved heap may end up differing from the\n-  \/\/ address that was requested (i.e. the preferred heap base).\n-  \/\/ If this happens then we could end up using a non-optimal\n-  \/\/ compressed oops mode.\n-\n-  ReservedHeapSpace heap_rs = Universe::reserve_heap(reserved_byte_size,\n-                                                     HeapAlignment);\n-\n-  initialize_reserved_region(heap_rs);\n-\n-  \/\/ Create the barrier set for the entire reserved region.\n-  G1CardTable* ct = new G1CardTable(_reserved);\n-  G1BarrierSet* bs = new G1BarrierSet(ct);\n-  bs->initialize();\n-  assert(bs->is_a(BarrierSet::G1BarrierSet), \"sanity\");\n-  BarrierSet::set_barrier_set(bs);\n-  _card_table = ct;\n-\n-  {\n-    G1SATBMarkQueueSet& satbqs = bs->satb_mark_queue_set();\n-    satbqs.set_process_completed_buffers_threshold(G1SATBProcessCompletedThreshold);\n-    satbqs.set_buffer_enqueue_threshold_percentage(G1SATBBufferEnqueueingThresholdPercent);\n-  }\n-\n-  \/\/ Create space mappers.\n-  size_t page_size = heap_rs.page_size();\n-  G1RegionToSpaceMapper* heap_storage =\n-    G1RegionToSpaceMapper::create_mapper(heap_rs,\n-                                         heap_rs.size(),\n-                                         page_size,\n-                                         G1HeapRegion::GrainBytes,\n-                                         1,\n-                                         mtJavaHeap);\n-  if(heap_storage == nullptr) {\n-    vm_shutdown_during_initialization(\"Could not initialize G1 heap\");\n-    return JNI_ERR;\n-  }\n-\n-  os::trace_page_sizes(\"Heap\",\n-                       min_capacity(),\n-                       reserved_byte_size,\n-                       heap_rs.base(),\n-                       heap_rs.size(),\n-                       page_size);\n-  heap_storage->set_mapping_changed_listener(&_listener);\n-\n-  \/\/ Create storage for the BOT, card table and the bitmap.\n-  G1RegionToSpaceMapper* bot_storage =\n-    create_aux_memory_mapper(\"Block Offset Table\",\n-                             G1BlockOffsetTable::compute_size(heap_rs.size() \/ HeapWordSize),\n-                             G1BlockOffsetTable::heap_map_factor());\n-\n-  G1RegionToSpaceMapper* cardtable_storage =\n-    create_aux_memory_mapper(\"Card Table\",\n-                             G1CardTable::compute_size(heap_rs.size() \/ HeapWordSize),\n-                             G1CardTable::heap_map_factor());\n-\n-  size_t bitmap_size = G1CMBitMap::compute_size(heap_rs.size());\n-  G1RegionToSpaceMapper* bitmap_storage =\n-    create_aux_memory_mapper(\"Mark Bitmap\", bitmap_size, G1CMBitMap::heap_map_factor());\n-\n-  _hrm.initialize(heap_storage, bitmap_storage, bot_storage, cardtable_storage);\n-  _card_table->initialize(cardtable_storage);\n-\n-  \/\/ 6843694 - ensure that the maximum region index can fit\n-  \/\/ in the remembered set structures.\n-  const uint max_region_idx = (1U << (sizeof(RegionIdx_t)*BitsPerByte-1)) - 1;\n-  guarantee((max_num_regions() - 1) <= max_region_idx, \"too many regions\");\n-\n-  \/\/ The G1FromCardCache reserves card with value 0 as \"invalid\", so the heap must not\n-  \/\/ start within the first card.\n-  guarantee((uintptr_t)(heap_rs.base()) >= G1CardTable::card_size(), \"Java heap must not start within the first card.\");\n-  G1FromCardCache::initialize(max_num_regions());\n-  \/\/ Also create a G1 rem set.\n-  _rem_set = new G1RemSet(this, _card_table);\n-  _rem_set->initialize(max_num_regions());\n-\n-  size_t max_cards_per_region = ((size_t)1 << (sizeof(CardIdx_t)*BitsPerByte-1)) - 1;\n-  guarantee(G1HeapRegion::CardsPerRegion > 0, \"make sure it's initialized\");\n-  guarantee(G1HeapRegion::CardsPerRegion < max_cards_per_region,\n-            \"too many cards per region\");\n-\n-  G1HeapRegionRemSet::initialize(_reserved);\n-\n-  G1FreeRegionList::set_unrealistically_long_length(max_num_regions() + 1);\n-\n-  _bot = new G1BlockOffsetTable(reserved(), bot_storage);\n-\n-  {\n-    size_t granularity = G1HeapRegion::GrainBytes;\n-\n-    _region_attr.initialize(reserved(), granularity);\n-  }\n-\n-  _workers = new WorkerThreads(\"GC Thread\", ParallelGCThreads);\n-  if (_workers == nullptr) {\n-    return JNI_ENOMEM;\n-  }\n-  _workers->initialize_workers();\n-\n-  _numa->set_region_info(G1HeapRegion::GrainBytes, page_size);\n-\n-  \/\/ Create the G1ConcurrentMark data structure and thread.\n-  \/\/ (Must do this late, so that \"max_[reserved_]regions\" is defined.)\n-  _cm = new G1ConcurrentMark(this, bitmap_storage);\n-  _cm_thread = _cm->cm_thread();\n-\n-  \/\/ Now expand into the initial heap size.\n-  if (!expand(init_byte_size, _workers)) {\n-    vm_shutdown_during_initialization(\"Failed to allocate initial heap.\");\n-    return JNI_ENOMEM;\n-  }\n-\n-  \/\/ Perform any initialization actions delegated to the policy.\n-  policy()->init(this, &_collection_set);\n-\n-  jint ecode = initialize_concurrent_refinement();\n-  if (ecode != JNI_OK) {\n-    return ecode;\n-  }\n-\n-  ecode = initialize_service_thread();\n-  if (ecode != JNI_OK) {\n-    return ecode;\n-  }\n-\n-  \/\/ Create and schedule the periodic gc task on the service thread.\n-  _periodic_gc_task = new G1PeriodicGCTask(\"Periodic GC Task\");\n-  _service_thread->register_task(_periodic_gc_task);\n-\n-  _free_arena_memory_task = new G1MonotonicArenaFreeMemoryTask(\"Card Set Free Memory Task\");\n-  _service_thread->register_task(_free_arena_memory_task);\n-\n-  \/\/ Here we allocate the dummy G1HeapRegion that is required by the\n-  \/\/ G1AllocRegion class.\n-  G1HeapRegion* dummy_region = _hrm.get_dummy_region();\n-\n-  \/\/ We'll re-use the same region whether the alloc region will\n-  \/\/ require BOT updates or not and, if it doesn't, then a non-young\n-  \/\/ region will complain that it cannot support allocations without\n-  \/\/ BOT updates. So we'll tag the dummy region as eden to avoid that.\n-  dummy_region->set_eden();\n-  \/\/ Make sure it's full.\n-  dummy_region->set_top(dummy_region->end());\n-  G1AllocRegion::setup(this, dummy_region);\n-\n-  _allocator->init_mutator_alloc_regions();\n-\n-  \/\/ Do create of the monitoring and management support so that\n-  \/\/ values in the heap have been properly initialized.\n-  _monitoring_support = new G1MonitoringSupport(this);\n-\n-  _collection_set.initialize(max_num_regions());\n-\n-  allocation_failure_injector()->reset();\n-\n-  CPUTimeCounters::create_counter(CPUTimeGroups::CPUTimeType::gc_parallel_workers);\n-  CPUTimeCounters::create_counter(CPUTimeGroups::CPUTimeType::gc_conc_mark);\n-  CPUTimeCounters::create_counter(CPUTimeGroups::CPUTimeType::gc_conc_refine);\n-  CPUTimeCounters::create_counter(CPUTimeGroups::CPUTimeType::gc_service);\n-\n-  G1InitLogger::print();\n-\n-  FullGCForwarding::initialize(_reserved);\n-\n-  return JNI_OK;\n-}\n-\n-bool G1CollectedHeap::concurrent_mark_is_terminating() const {\n-  return _cm_thread->should_terminate();\n-}\n-\n-void G1CollectedHeap::stop() {\n-  \/\/ Stop all concurrent threads. We do this to make sure these threads\n-  \/\/ do not continue to execute and access resources (e.g. logging)\n-  \/\/ that are destroyed during shutdown.\n-  _cr->stop();\n-  _service_thread->stop();\n-  _cm_thread->stop();\n-}\n-\n-void G1CollectedHeap::safepoint_synchronize_begin() {\n-  SuspendibleThreadSet::synchronize();\n-}\n-\n-void G1CollectedHeap::safepoint_synchronize_end() {\n-  SuspendibleThreadSet::desynchronize();\n-}\n-\n-void G1CollectedHeap::post_initialize() {\n-  CollectedHeap::post_initialize();\n-  ref_processing_init();\n-}\n-\n-void G1CollectedHeap::ref_processing_init() {\n-  \/\/ Reference processing in G1 currently works as follows:\n-  \/\/\n-  \/\/ * There are two reference processor instances. One is\n-  \/\/   used to record and process discovered references\n-  \/\/   during concurrent marking; the other is used to\n-  \/\/   record and process references during STW pauses\n-  \/\/   (both full and incremental).\n-  \/\/ * Both ref processors need to 'span' the entire heap as\n-  \/\/   the regions in the collection set may be dotted around.\n-  \/\/\n-  \/\/ * For the concurrent marking ref processor:\n-  \/\/   * Reference discovery is enabled at concurrent start.\n-  \/\/   * Reference discovery is disabled and the discovered\n-  \/\/     references processed etc during remarking.\n-  \/\/   * Reference discovery is MT (see below).\n-  \/\/   * Reference discovery requires a barrier (see below).\n-  \/\/   * Reference processing may or may not be MT\n-  \/\/     (depending on the value of ParallelRefProcEnabled\n-  \/\/     and ParallelGCThreads).\n-  \/\/   * A full GC disables reference discovery by the CM\n-  \/\/     ref processor and abandons any entries on it's\n-  \/\/     discovered lists.\n-  \/\/\n-  \/\/ * For the STW processor:\n-  \/\/   * Non MT discovery is enabled at the start of a full GC.\n-  \/\/   * Processing and enqueueing during a full GC is non-MT.\n-  \/\/   * During a full GC, references are processed after marking.\n-  \/\/\n-  \/\/   * Discovery (may or may not be MT) is enabled at the start\n-  \/\/     of an incremental evacuation pause.\n-  \/\/   * References are processed near the end of a STW evacuation pause.\n-  \/\/   * For both types of GC:\n-  \/\/     * Discovery is atomic - i.e. not concurrent.\n-  \/\/     * Reference discovery will not need a barrier.\n-\n-  _is_alive_closure_cm.initialize(concurrent_mark());\n-  \/\/ Concurrent Mark ref processor\n-  _ref_processor_cm =\n-    new ReferenceProcessor(&_is_subject_to_discovery_cm,\n-                           ParallelGCThreads,                              \/\/ degree of mt processing\n-                           \/\/ We discover with the gc worker threads during Remark, so both\n-                           \/\/ thread counts must be considered for discovery.\n-                           MAX2(ParallelGCThreads, ConcGCThreads),         \/\/ degree of mt discovery\n-                           true,                                           \/\/ Reference discovery is concurrent\n-                           &_is_alive_closure_cm);                         \/\/ is alive closure\n-\n-  \/\/ STW ref processor\n-  _ref_processor_stw =\n-    new ReferenceProcessor(&_is_subject_to_discovery_stw,\n-                           ParallelGCThreads,                    \/\/ degree of mt processing\n-                           ParallelGCThreads,                    \/\/ degree of mt discovery\n-                           false,                                \/\/ Reference discovery is not concurrent\n-                           &_is_alive_closure_stw);              \/\/ is alive closure\n-}\n-\n-size_t G1CollectedHeap::capacity() const {\n-  return _hrm.num_committed_regions() * G1HeapRegion::GrainBytes;\n-}\n-\n-size_t G1CollectedHeap::unused_committed_regions_in_bytes() const {\n-  return _hrm.total_free_bytes();\n-}\n-\n-\/\/ Computes the sum of the storage used by the various regions.\n-size_t G1CollectedHeap::used() const {\n-  size_t result = _summary_bytes_used + _allocator->used_in_alloc_regions();\n-  return result;\n-}\n-\n-size_t G1CollectedHeap::used_unlocked() const {\n-  return _summary_bytes_used;\n-}\n-\n-class SumUsedClosure: public G1HeapRegionClosure {\n-  size_t _used;\n-public:\n-  SumUsedClosure() : _used(0) {}\n-  bool do_heap_region(G1HeapRegion* r) {\n-    _used += r->used();\n-    return false;\n-  }\n-  size_t result() { return _used; }\n-};\n-\n-size_t G1CollectedHeap::recalculate_used() const {\n-  SumUsedClosure blk;\n-  heap_region_iterate(&blk);\n-  return blk.result();\n-}\n-\n-bool  G1CollectedHeap::is_user_requested_concurrent_full_gc(GCCause::Cause cause) {\n-  return GCCause::is_user_requested_gc(cause) && ExplicitGCInvokesConcurrent;\n-}\n-\n-bool G1CollectedHeap::should_do_concurrent_full_gc(GCCause::Cause cause) {\n-  switch (cause) {\n-    case GCCause::_g1_humongous_allocation: return true;\n-    case GCCause::_g1_periodic_collection:  return G1PeriodicGCInvokesConcurrent;\n-    case GCCause::_wb_breakpoint:           return true;\n-    case GCCause::_codecache_GC_aggressive: return true;\n-    case GCCause::_codecache_GC_threshold:  return true;\n-    default:                                return is_user_requested_concurrent_full_gc(cause);\n-  }\n-}\n-\n-void G1CollectedHeap::increment_old_marking_cycles_started() {\n-  assert(_old_marking_cycles_started == _old_marking_cycles_completed ||\n-         _old_marking_cycles_started == _old_marking_cycles_completed + 1,\n-         \"Wrong marking cycle count (started: %d, completed: %d)\",\n-         _old_marking_cycles_started, _old_marking_cycles_completed);\n-\n-  _old_marking_cycles_started++;\n-}\n-\n-void G1CollectedHeap::increment_old_marking_cycles_completed(bool concurrent,\n-                                                             bool whole_heap_examined) {\n-  MonitorLocker ml(G1OldGCCount_lock, Mutex::_no_safepoint_check_flag);\n-\n-  \/\/ We assume that if concurrent == true, then the caller is a\n-  \/\/ concurrent thread that was joined the Suspendible Thread\n-  \/\/ Set. If there's ever a cheap way to check this, we should add an\n-  \/\/ assert here.\n-\n-  \/\/ Given that this method is called at the end of a Full GC or of a\n-  \/\/ concurrent cycle, and those can be nested (i.e., a Full GC can\n-  \/\/ interrupt a concurrent cycle), the number of full collections\n-  \/\/ completed should be either one (in the case where there was no\n-  \/\/ nesting) or two (when a Full GC interrupted a concurrent cycle)\n-  \/\/ behind the number of full collections started.\n-\n-  \/\/ This is the case for the inner caller, i.e. a Full GC.\n-  assert(concurrent ||\n-         (_old_marking_cycles_started == _old_marking_cycles_completed + 1) ||\n-         (_old_marking_cycles_started == _old_marking_cycles_completed + 2),\n-         \"for inner caller (Full GC): _old_marking_cycles_started = %u \"\n-         \"is inconsistent with _old_marking_cycles_completed = %u\",\n-         _old_marking_cycles_started, _old_marking_cycles_completed);\n-\n-  \/\/ This is the case for the outer caller, i.e. the concurrent cycle.\n-  assert(!concurrent ||\n-         (_old_marking_cycles_started == _old_marking_cycles_completed + 1),\n-         \"for outer caller (concurrent cycle): \"\n-         \"_old_marking_cycles_started = %u \"\n-         \"is inconsistent with _old_marking_cycles_completed = %u\",\n-         _old_marking_cycles_started, _old_marking_cycles_completed);\n-\n-  _old_marking_cycles_completed += 1;\n-  if (whole_heap_examined) {\n-    \/\/ Signal that we have completed a visit to all live objects.\n-    record_whole_heap_examined_timestamp();\n-  }\n-\n-  \/\/ We need to clear the \"in_progress\" flag in the CM thread before\n-  \/\/ we wake up any waiters (especially when ExplicitInvokesConcurrent\n-  \/\/ is set) so that if a waiter requests another System.gc() it doesn't\n-  \/\/ incorrectly see that a marking cycle is still in progress.\n-  if (concurrent) {\n-    _cm_thread->set_idle();\n-  }\n-\n-  \/\/ Notify threads waiting in System.gc() (with ExplicitGCInvokesConcurrent)\n-  \/\/ for a full GC to finish that their wait is over.\n-  ml.notify_all();\n-}\n-\n-\/\/ Helper for collect().\n-static G1GCCounters collection_counters(G1CollectedHeap* g1h) {\n-  MutexLocker ml(Heap_lock);\n-  return G1GCCounters(g1h);\n-}\n-\n-void G1CollectedHeap::collect(GCCause::Cause cause) {\n-  try_collect(cause, collection_counters(this));\n-}\n-\n-\/\/ Return true if (x < y) with allowance for wraparound.\n-static bool gc_counter_less_than(uint x, uint y) {\n-  return (x - y) > (UINT_MAX\/2);\n-}\n-\n-\/\/ LOG_COLLECT_CONCURRENTLY(cause, msg, args...)\n-\/\/ Macro so msg printing is format-checked.\n-#define LOG_COLLECT_CONCURRENTLY(cause, ...)                            \\\n-  do {                                                                  \\\n-    LogTarget(Trace, gc) LOG_COLLECT_CONCURRENTLY_lt;                   \\\n-    if (LOG_COLLECT_CONCURRENTLY_lt.is_enabled()) {                     \\\n-      ResourceMark rm; \/* For thread name. *\/                           \\\n-      LogStream LOG_COLLECT_CONCURRENTLY_s(&LOG_COLLECT_CONCURRENTLY_lt); \\\n-      LOG_COLLECT_CONCURRENTLY_s.print(\"%s: Try Collect Concurrently (%s): \", \\\n-                                       Thread::current()->name(),       \\\n-                                       GCCause::to_string(cause));      \\\n-      LOG_COLLECT_CONCURRENTLY_s.print(__VA_ARGS__);                    \\\n-    }                                                                   \\\n-  } while (0)\n-\n-#define LOG_COLLECT_CONCURRENTLY_COMPLETE(cause, result) \\\n-  LOG_COLLECT_CONCURRENTLY(cause, \"complete %s\", BOOL_TO_STR(result))\n-\n-bool G1CollectedHeap::try_collect_concurrently(GCCause::Cause cause,\n-                                               uint gc_counter,\n-                                               uint old_marking_started_before) {\n-  assert_heap_not_locked();\n-  assert(should_do_concurrent_full_gc(cause),\n-         \"Non-concurrent cause %s\", GCCause::to_string(cause));\n-\n-  for (uint i = 1; true; ++i) {\n-    \/\/ Try to schedule concurrent start evacuation pause that will\n-    \/\/ start a concurrent cycle.\n-    LOG_COLLECT_CONCURRENTLY(cause, \"attempt %u\", i);\n-    VM_G1TryInitiateConcMark op(gc_counter, cause);\n-    VMThread::execute(&op);\n-\n-    \/\/ Request is trivially finished.\n-    if (cause == GCCause::_g1_periodic_collection) {\n-      LOG_COLLECT_CONCURRENTLY_COMPLETE(cause, op.gc_succeeded());\n-      return op.gc_succeeded();\n-    }\n-\n-    \/\/ If VMOp skipped initiating concurrent marking cycle because\n-    \/\/ we're terminating, then we're done.\n-    if (op.terminating()) {\n-      LOG_COLLECT_CONCURRENTLY(cause, \"skipped: terminating\");\n-      return false;\n-    }\n-\n-    \/\/ Lock to get consistent set of values.\n-    uint old_marking_started_after;\n-    uint old_marking_completed_after;\n-    {\n-      MutexLocker ml(Heap_lock);\n-      \/\/ Update gc_counter for retrying VMOp if needed. Captured here to be\n-      \/\/ consistent with the values we use below for termination tests.  If\n-      \/\/ a retry is needed after a possible wait, and another collection\n-      \/\/ occurs in the meantime, it will cause our retry to be skipped and\n-      \/\/ we'll recheck for termination with updated conditions from that\n-      \/\/ more recent collection.  That's what we want, rather than having\n-      \/\/ our retry possibly perform an unnecessary collection.\n-      gc_counter = total_collections();\n-      old_marking_started_after = _old_marking_cycles_started;\n-      old_marking_completed_after = _old_marking_cycles_completed;\n-    }\n-\n-    if (cause == GCCause::_wb_breakpoint) {\n-      if (op.gc_succeeded()) {\n-        LOG_COLLECT_CONCURRENTLY_COMPLETE(cause, true);\n-        return true;\n-      }\n-      \/\/ When _wb_breakpoint there can't be another cycle or deferred.\n-      assert(!op.cycle_already_in_progress(), \"invariant\");\n-      assert(!op.whitebox_attached(), \"invariant\");\n-      \/\/ Concurrent cycle attempt might have been cancelled by some other\n-      \/\/ collection, so retry.  Unlike other cases below, we want to retry\n-      \/\/ even if cancelled by a STW full collection, because we really want\n-      \/\/ to start a concurrent cycle.\n-      if (old_marking_started_before != old_marking_started_after) {\n-        LOG_COLLECT_CONCURRENTLY(cause, \"ignoring STW full GC\");\n-        old_marking_started_before = old_marking_started_after;\n-      }\n-    } else if (!GCCause::is_user_requested_gc(cause)) {\n-      \/\/ For an \"automatic\" (not user-requested) collection, we just need to\n-      \/\/ ensure that progress is made.\n-      \/\/\n-      \/\/ Request is finished if any of\n-      \/\/ (1) the VMOp successfully performed a GC,\n-      \/\/ (2) a concurrent cycle was already in progress,\n-      \/\/ (3) whitebox is controlling concurrent cycles,\n-      \/\/ (4) a new cycle was started (by this thread or some other), or\n-      \/\/ (5) a Full GC was performed.\n-      \/\/ Cases (4) and (5) are detected together by a change to\n-      \/\/ _old_marking_cycles_started.\n-      \/\/\n-      \/\/ Note that (1) does not imply (4).  If we're still in the mixed\n-      \/\/ phase of an earlier concurrent collection, the request to make the\n-      \/\/ collection a concurrent start won't be honored.  If we don't check for\n-      \/\/ both conditions we'll spin doing back-to-back collections.\n-      if (op.gc_succeeded() ||\n-          op.cycle_already_in_progress() ||\n-          op.whitebox_attached() ||\n-          (old_marking_started_before != old_marking_started_after)) {\n-        LOG_COLLECT_CONCURRENTLY_COMPLETE(cause, true);\n-        return true;\n-      }\n-    } else {                    \/\/ User-requested GC.\n-      \/\/ For a user-requested collection, we want to ensure that a complete\n-      \/\/ full collection has been performed before returning, but without\n-      \/\/ waiting for more than needed.\n-\n-      \/\/ For user-requested GCs (unlike non-UR), a successful VMOp implies a\n-      \/\/ new cycle was started.  That's good, because it's not clear what we\n-      \/\/ should do otherwise.  Trying again just does back to back GCs.\n-      \/\/ Can't wait for someone else to start a cycle.  And returning fails\n-      \/\/ to meet the goal of ensuring a full collection was performed.\n-      assert(!op.gc_succeeded() ||\n-             (old_marking_started_before != old_marking_started_after),\n-             \"invariant: succeeded %s, started before %u, started after %u\",\n-             BOOL_TO_STR(op.gc_succeeded()),\n-             old_marking_started_before, old_marking_started_after);\n-\n-      \/\/ Request is finished if a full collection (concurrent or stw)\n-      \/\/ was started after this request and has completed, e.g.\n-      \/\/ started_before < completed_after.\n-      if (gc_counter_less_than(old_marking_started_before,\n-                               old_marking_completed_after)) {\n-        LOG_COLLECT_CONCURRENTLY_COMPLETE(cause, true);\n-        return true;\n-      }\n-\n-      if (old_marking_started_after != old_marking_completed_after) {\n-        \/\/ If there is an in-progress cycle (possibly started by us), then\n-        \/\/ wait for that cycle to complete, e.g.\n-        \/\/ while completed_now < started_after.\n-        LOG_COLLECT_CONCURRENTLY(cause, \"wait\");\n-        MonitorLocker ml(G1OldGCCount_lock);\n-        while (gc_counter_less_than(_old_marking_cycles_completed,\n-                                    old_marking_started_after)) {\n-          ml.wait();\n-        }\n-        \/\/ Request is finished if the collection we just waited for was\n-        \/\/ started after this request.\n-        if (old_marking_started_before != old_marking_started_after) {\n-          LOG_COLLECT_CONCURRENTLY(cause, \"complete after wait\");\n-          return true;\n-        }\n-      }\n-\n-      \/\/ If VMOp was successful then it started a new cycle that the above\n-      \/\/ wait &etc should have recognized as finishing this request.  This\n-      \/\/ differs from a non-user-request, where gc_succeeded does not imply\n-      \/\/ a new cycle was started.\n-      assert(!op.gc_succeeded(), \"invariant\");\n-\n-      if (op.cycle_already_in_progress()) {\n-        \/\/ If VMOp failed because a cycle was already in progress, it\n-        \/\/ is now complete.  But it didn't finish this user-requested\n-        \/\/ GC, so try again.\n-        LOG_COLLECT_CONCURRENTLY(cause, \"retry after in-progress\");\n-        continue;\n-      } else if (op.whitebox_attached()) {\n-        \/\/ If WhiteBox wants control, wait for notification of a state\n-        \/\/ change in the controller, then try again.  Don't wait for\n-        \/\/ release of control, since collections may complete while in\n-        \/\/ control.  Note: This won't recognize a STW full collection\n-        \/\/ while waiting; we can't wait on multiple monitors.\n-        LOG_COLLECT_CONCURRENTLY(cause, \"whitebox control stall\");\n-        MonitorLocker ml(ConcurrentGCBreakpoints::monitor());\n-        if (ConcurrentGCBreakpoints::is_controlled()) {\n-          ml.wait();\n-        }\n-        continue;\n-      }\n-    }\n-\n-    \/\/ Collection failed and should be retried.\n-    assert(op.transient_failure(), \"invariant\");\n-\n-    LOG_COLLECT_CONCURRENTLY(cause, \"retry\");\n-  }\n-}\n-\n-bool G1CollectedHeap::try_collect(GCCause::Cause cause,\n-                                  const G1GCCounters& counters_before) {\n-  if (should_do_concurrent_full_gc(cause)) {\n-    return try_collect_concurrently(cause,\n-                                    counters_before.total_collections(),\n-                                    counters_before.old_marking_cycles_started());\n-  } else if (cause == GCCause::_wb_young_gc\n-             DEBUG_ONLY(|| cause == GCCause::_scavenge_alot)) {\n-\n-    \/\/ Schedule a standard evacuation pause. We're setting word_size\n-    \/\/ to 0 which means that we are not requesting a post-GC allocation.\n-    VM_G1CollectForAllocation op(0,     \/* word_size *\/\n-                                 counters_before.total_collections(),\n-                                 cause);\n-    VMThread::execute(&op);\n-    return op.gc_succeeded();\n-  } else {\n-    \/\/ Schedule a Full GC.\n-    VM_G1CollectFull op(counters_before.total_collections(),\n-                        counters_before.total_full_collections(),\n-                        cause);\n-    VMThread::execute(&op);\n-    return op.gc_succeeded();\n-  }\n-}\n-\n-void G1CollectedHeap::start_concurrent_gc_for_metadata_allocation(GCCause::Cause gc_cause) {\n-  GCCauseSetter x(this, gc_cause);\n-\n-  \/\/ At this point we are supposed to start a concurrent cycle. We\n-  \/\/ will do so if one is not already in progress.\n-  bool should_start = policy()->force_concurrent_start_if_outside_cycle(gc_cause);\n-  if (should_start) {\n-    do_collection_pause_at_safepoint();\n-  }\n-}\n-\n-bool G1CollectedHeap::is_in(const void* p) const {\n-  return is_in_reserved(p) && _hrm.is_available(addr_to_region(p));\n-}\n-\n-\/\/ Iteration functions.\n-\n-\/\/ Iterates an ObjectClosure over all objects within a G1HeapRegion.\n-\n-class IterateObjectClosureRegionClosure: public G1HeapRegionClosure {\n-  ObjectClosure* _cl;\n-public:\n-  IterateObjectClosureRegionClosure(ObjectClosure* cl) : _cl(cl) {}\n-  bool do_heap_region(G1HeapRegion* r) {\n-    if (!r->is_continues_humongous()) {\n-      r->object_iterate(_cl);\n-    }\n-    return false;\n-  }\n-};\n-\n-void G1CollectedHeap::object_iterate(ObjectClosure* cl) {\n-  IterateObjectClosureRegionClosure blk(cl);\n-  heap_region_iterate(&blk);\n-}\n-\n-class G1ParallelObjectIterator : public ParallelObjectIteratorImpl {\n-private:\n-  G1CollectedHeap*  _heap;\n-  G1HeapRegionClaimer _claimer;\n-\n-public:\n-  G1ParallelObjectIterator(uint thread_num) :\n-      _heap(G1CollectedHeap::heap()),\n-      _claimer(thread_num == 0 ? G1CollectedHeap::heap()->workers()->active_workers() : thread_num) {}\n-\n-  virtual void object_iterate(ObjectClosure* cl, uint worker_id) {\n-    _heap->object_iterate_parallel(cl, worker_id, &_claimer);\n-  }\n-};\n-\n-ParallelObjectIteratorImpl* G1CollectedHeap::parallel_object_iterator(uint thread_num) {\n-  return new G1ParallelObjectIterator(thread_num);\n-}\n-\n-void G1CollectedHeap::object_iterate_parallel(ObjectClosure* cl, uint worker_id, G1HeapRegionClaimer* claimer) {\n-  IterateObjectClosureRegionClosure blk(cl);\n-  heap_region_par_iterate_from_worker_offset(&blk, claimer, worker_id);\n-}\n-\n-void G1CollectedHeap::keep_alive(oop obj) {\n-  G1BarrierSet::enqueue_preloaded(obj);\n-}\n-\n-void G1CollectedHeap::heap_region_iterate(G1HeapRegionClosure* cl) const {\n-  _hrm.iterate(cl);\n-}\n-\n-void G1CollectedHeap::heap_region_iterate(G1HeapRegionIndexClosure* cl) const {\n-  _hrm.iterate(cl);\n-}\n-\n-void G1CollectedHeap::heap_region_par_iterate_from_worker_offset(G1HeapRegionClosure* cl,\n-                                                                 G1HeapRegionClaimer *hrclaimer,\n-                                                                 uint worker_id) const {\n-  _hrm.par_iterate(cl, hrclaimer, hrclaimer->offset_for_worker(worker_id));\n-}\n-\n-void G1CollectedHeap::heap_region_par_iterate_from_start(G1HeapRegionClosure* cl,\n-                                                         G1HeapRegionClaimer *hrclaimer) const {\n-  _hrm.par_iterate(cl, hrclaimer, 0);\n-}\n-\n-void G1CollectedHeap::collection_set_iterate_all(G1HeapRegionClosure* cl) {\n-  _collection_set.iterate(cl);\n-}\n-\n-void G1CollectedHeap::collection_set_par_iterate_all(G1HeapRegionClosure* cl,\n-                                                     G1HeapRegionClaimer* hr_claimer,\n-                                                     uint worker_id) {\n-  _collection_set.par_iterate(cl, hr_claimer, worker_id);\n-}\n-\n-void G1CollectedHeap::collection_set_iterate_increment_from(G1HeapRegionClosure *cl,\n-                                                            G1HeapRegionClaimer* hr_claimer,\n-                                                            uint worker_id) {\n-  _collection_set.iterate_incremental_part_from(cl, hr_claimer, worker_id);\n-}\n-\n-void G1CollectedHeap::par_iterate_regions_array(G1HeapRegionClosure* cl,\n-                                                G1HeapRegionClaimer* hr_claimer,\n-                                                const uint regions[],\n-                                                size_t length,\n-                                                uint worker_id) const {\n-  assert_at_safepoint();\n-  if (length == 0) {\n-    return;\n-  }\n-  uint total_workers = workers()->active_workers();\n-\n-  size_t start_pos = (worker_id * length) \/ total_workers;\n-  size_t cur_pos = start_pos;\n-\n-  do {\n-    uint region_idx = regions[cur_pos];\n-    if (hr_claimer == nullptr || hr_claimer->claim_region(region_idx)) {\n-      G1HeapRegion* r = region_at(region_idx);\n-      bool result = cl->do_heap_region(r);\n-      guarantee(!result, \"Must not cancel iteration\");\n-    }\n-\n-    cur_pos++;\n-    if (cur_pos == length) {\n-      cur_pos = 0;\n-    }\n-  } while (cur_pos != start_pos);\n-}\n-\n-HeapWord* G1CollectedHeap::block_start(const void* addr) const {\n-  G1HeapRegion* hr = heap_region_containing(addr);\n-  \/\/ The CollectedHeap API requires us to not fail for any given address within\n-  \/\/ the heap. G1HeapRegion::block_start() has been optimized to not accept addresses\n-  \/\/ outside of the allocated area.\n-  if (addr >= hr->top()) {\n-    return nullptr;\n-  }\n-  return hr->block_start(addr);\n-}\n-\n-bool G1CollectedHeap::block_is_obj(const HeapWord* addr) const {\n-  G1HeapRegion* hr = heap_region_containing(addr);\n-  return hr->block_is_obj(addr, hr->parsable_bottom_acquire());\n-}\n-\n-size_t G1CollectedHeap::tlab_capacity(Thread* ignored) const {\n-  return eden_target_length() * G1HeapRegion::GrainBytes;\n-}\n-\n-size_t G1CollectedHeap::tlab_used(Thread* ignored) const {\n-  return _eden.length() * G1HeapRegion::GrainBytes;\n-}\n-\n-\/\/ For G1 TLABs should not contain humongous objects, so the maximum TLAB size\n-\/\/ must be equal to the humongous object limit.\n-size_t G1CollectedHeap::max_tlab_size() const {\n-  return align_down(_humongous_object_threshold_in_words, MinObjAlignment);\n-}\n-\n-size_t G1CollectedHeap::unsafe_max_tlab_alloc(Thread* ignored) const {\n-  return _allocator->unsafe_max_tlab_alloc();\n-}\n-\n-size_t G1CollectedHeap::max_capacity() const {\n-  return max_num_regions() * G1HeapRegion::GrainBytes;\n-}\n-\n-size_t G1CollectedHeap::min_capacity() const {\n-  return MinHeapSize;\n-}\n-\n-void G1CollectedHeap::prepare_for_verify() {\n-  _verifier->prepare_for_verify();\n-}\n-\n-void G1CollectedHeap::verify(VerifyOption vo) {\n-  _verifier->verify(vo);\n-}\n-\n-bool G1CollectedHeap::supports_concurrent_gc_breakpoints() const {\n-  return true;\n-}\n-\n-class G1PrintRegionClosure: public G1HeapRegionClosure {\n-  outputStream* _st;\n-public:\n-  G1PrintRegionClosure(outputStream* st) : _st(st) {}\n-  bool do_heap_region(G1HeapRegion* r) {\n-    r->print_on(_st);\n-    return false;\n-  }\n-};\n-\n-bool G1CollectedHeap::is_obj_dead_cond(const oop obj,\n-                                       const G1HeapRegion* hr,\n-                                       const VerifyOption vo) const {\n-  switch (vo) {\n-    case VerifyOption::G1UseConcMarking: return is_obj_dead(obj, hr);\n-    case VerifyOption::G1UseFullMarking: return is_obj_dead_full(obj, hr);\n-    default:                             ShouldNotReachHere();\n-  }\n-  return false; \/\/ keep some compilers happy\n-}\n-\n-bool G1CollectedHeap::is_obj_dead_cond(const oop obj,\n-                                       const VerifyOption vo) const {\n-  switch (vo) {\n-    case VerifyOption::G1UseConcMarking: return is_obj_dead(obj);\n-    case VerifyOption::G1UseFullMarking: return is_obj_dead_full(obj);\n-    default:                             ShouldNotReachHere();\n-  }\n-  return false; \/\/ keep some compilers happy\n-}\n-\n-void G1CollectedHeap::print_heap_regions() const {\n-  LogTarget(Trace, gc, heap, region) lt;\n-  if (lt.is_enabled()) {\n-    LogStream ls(lt);\n-    print_regions_on(&ls);\n-  }\n-}\n-\n-void G1CollectedHeap::print_heap_on(outputStream* st) const {\n-  size_t heap_used = Heap_lock->owned_by_self() ? used() : used_unlocked();\n-  st->print(\"%-20s\", \"garbage-first heap\");\n-  st->print(\" total reserved %zuK, committed %zuK, used %zuK\",\n-            _hrm.reserved().byte_size()\/K, capacity()\/K, heap_used\/K);\n-  st->print(\" [\" PTR_FORMAT \", \" PTR_FORMAT \")\",\n-            p2i(_hrm.reserved().start()),\n-            p2i(_hrm.reserved().end()));\n-  st->cr();\n-\n-  StreamIndentor si(st, 1);\n-  st->print(\"region size %zuK, \", G1HeapRegion::GrainBytes \/ K);\n-  uint young_regions = young_regions_count();\n-  st->print(\"%u young (%zuK), \", young_regions,\n-            (size_t) young_regions * G1HeapRegion::GrainBytes \/ K);\n-  uint survivor_regions = survivor_regions_count();\n-  st->print(\"%u survivors (%zuK)\", survivor_regions,\n-            (size_t) survivor_regions * G1HeapRegion::GrainBytes \/ K);\n-  st->cr();\n-  if (_numa->is_enabled()) {\n-    uint num_nodes = _numa->num_active_nodes();\n-    st->print(\"remaining free region(s) on each NUMA node: \");\n-    const uint* node_ids = _numa->node_ids();\n-    for (uint node_index = 0; node_index < num_nodes; node_index++) {\n-      uint num_free_regions = _hrm.num_free_regions(node_index);\n-      st->print(\"%u=%u \", node_ids[node_index], num_free_regions);\n-    }\n-    st->cr();\n-  }\n-}\n-\n-void G1CollectedHeap::print_regions_on(outputStream* st) const {\n-  st->print_cr(\"Heap Regions: E=young(eden), S=young(survivor), O=old, \"\n-               \"HS=humongous(starts), HC=humongous(continues), \"\n-               \"CS=collection set, F=free, \"\n-               \"TAMS=top-at-mark-start, \"\n-               \"PB=parsable bottom\");\n-  G1PrintRegionClosure blk(st);\n-  heap_region_iterate(&blk);\n-}\n-\n-void G1CollectedHeap::print_extended_on(outputStream* st) const {\n-  print_heap_on(st);\n-\n-  \/\/ Print the per-region information.\n-  st->cr();\n-  print_regions_on(st);\n-}\n-\n-void G1CollectedHeap::print_gc_on(outputStream* st) const {\n-  \/\/ Print the per-region information.\n-  print_regions_on(st);\n-  st->cr();\n-\n-  BarrierSet* bs = BarrierSet::barrier_set();\n-  if (bs != nullptr) {\n-    bs->print_on(st);\n-  }\n-\n-  if (_cm != nullptr) {\n-    st->cr();\n-    _cm->print_on(st);\n-  }\n-}\n-\n-void G1CollectedHeap::gc_threads_do(ThreadClosure* tc) const {\n-  workers()->threads_do(tc);\n-  tc->do_thread(_cm_thread);\n-  _cm->threads_do(tc);\n-  _cr->threads_do(tc);\n-  tc->do_thread(_service_thread);\n-}\n-\n-void G1CollectedHeap::print_tracing_info() const {\n-  rem_set()->print_summary_info();\n-  concurrent_mark()->print_summary_info();\n-}\n-\n-bool G1CollectedHeap::print_location(outputStream* st, void* addr) const {\n-  return BlockLocationPrinter<G1CollectedHeap>::print_location(st, addr);\n-}\n-\n-G1HeapSummary G1CollectedHeap::create_g1_heap_summary() {\n-\n-  size_t eden_used_bytes = _monitoring_support->eden_space_used();\n-  size_t survivor_used_bytes = _monitoring_support->survivor_space_used();\n-  size_t old_gen_used_bytes = _monitoring_support->old_gen_used();\n-  size_t heap_used = Heap_lock->owned_by_self() ? used() : used_unlocked();\n-\n-  size_t eden_capacity_bytes =\n-    (policy()->young_list_target_length() * G1HeapRegion::GrainBytes) - survivor_used_bytes;\n-\n-  VirtualSpaceSummary heap_summary = create_heap_space_summary();\n-  return G1HeapSummary(heap_summary, heap_used, eden_used_bytes, eden_capacity_bytes,\n-                       survivor_used_bytes, old_gen_used_bytes, num_committed_regions());\n-}\n-\n-G1EvacSummary G1CollectedHeap::create_g1_evac_summary(G1EvacStats* stats) {\n-  return G1EvacSummary(stats->allocated(), stats->wasted(), stats->undo_wasted(),\n-                       stats->unused(), stats->used(), stats->region_end_waste(),\n-                       stats->regions_filled(), stats->num_plab_filled(),\n-                       stats->direct_allocated(), stats->num_direct_allocated(),\n-                       stats->failure_used(), stats->failure_waste());\n-}\n-\n-void G1CollectedHeap::trace_heap(GCWhen::Type when, const GCTracer* gc_tracer) {\n-  const G1HeapSummary& heap_summary = create_g1_heap_summary();\n-  gc_tracer->report_gc_heap_summary(when, heap_summary);\n-\n-  const MetaspaceSummary& metaspace_summary = create_metaspace_summary();\n-  gc_tracer->report_metaspace_summary(when, metaspace_summary);\n-}\n-\n-void G1CollectedHeap::gc_prologue(bool full) {\n-  \/\/ Update common counters.\n-  increment_total_collections(full \/* full gc *\/);\n-  if (full || collector_state()->in_concurrent_start_gc()) {\n-    increment_old_marking_cycles_started();\n-  }\n-}\n-\n-void G1CollectedHeap::gc_epilogue(bool full) {\n-  \/\/ Update common counters.\n-  if (full) {\n-    \/\/ Update the number of full collections that have been completed.\n-    increment_old_marking_cycles_completed(false \/* concurrent *\/, true \/* liveness_completed *\/);\n-  }\n-\n-#if COMPILER2_OR_JVMCI\n-  assert(DerivedPointerTable::is_empty(), \"derived pointer present\");\n-#endif\n-\n-  \/\/ We have just completed a GC. Update the soft reference\n-  \/\/ policy with the new heap occupancy\n-  Universe::heap()->update_capacity_and_used_at_gc();\n-\n-  _collection_pause_end = Ticks::now();\n-\n-  _free_arena_memory_task->notify_new_stats(&_young_gen_card_set_stats,\n-                                            &_collection_set_candidates_card_set_stats);\n-\n-  update_perf_counter_cpu_time();\n-}\n-\n-uint G1CollectedHeap::uncommit_regions(uint region_limit) {\n-  return _hrm.uncommit_inactive_regions(region_limit);\n-}\n-\n-bool G1CollectedHeap::has_uncommittable_regions() {\n-  return _hrm.has_inactive_regions();\n-}\n-\n-void G1CollectedHeap::uncommit_regions_if_necessary() {\n-  if (has_uncommittable_regions()) {\n-    G1UncommitRegionTask::enqueue();\n-  }\n-}\n-\n-void G1CollectedHeap::verify_numa_regions(const char* desc) {\n-  LogTarget(Trace, gc, heap, verify) lt;\n-\n-  if (lt.is_enabled()) {\n-    LogStream ls(lt);\n-    \/\/ Iterate all heap regions to print matching between preferred numa id and actual numa id.\n-    G1NodeIndexCheckClosure cl(desc, _numa, &ls);\n-    heap_region_iterate(&cl);\n-  }\n-}\n-\n-HeapWord* G1CollectedHeap::do_collection_pause(size_t word_size,\n-                                               uint gc_count_before,\n-                                               bool* succeeded,\n-                                               GCCause::Cause gc_cause) {\n-  assert_heap_not_locked_and_not_at_safepoint();\n-  VM_G1CollectForAllocation op(word_size, gc_count_before, gc_cause);\n-  VMThread::execute(&op);\n-\n-  HeapWord* result = op.result();\n-  *succeeded = op.gc_succeeded();\n-  assert(result == nullptr || *succeeded,\n-         \"the result should be null if the VM did not succeed\");\n-\n-  assert_heap_not_locked();\n-  return result;\n-}\n-\n-void G1CollectedHeap::start_concurrent_cycle(bool concurrent_operation_is_full_mark) {\n-  assert(!_cm_thread->in_progress(), \"Can not start concurrent operation while in progress\");\n-\n-  MutexLocker x(CGC_lock, Mutex::_no_safepoint_check_flag);\n-  if (concurrent_operation_is_full_mark) {\n-    _cm->post_concurrent_mark_start();\n-    _cm_thread->start_full_mark();\n-  } else {\n-    _cm->post_concurrent_undo_start();\n-    _cm_thread->start_undo_mark();\n-  }\n-  CGC_lock->notify();\n-}\n-\n-bool G1CollectedHeap::is_potential_eager_reclaim_candidate(G1HeapRegion* r) const {\n-  \/\/ We don't nominate objects with many remembered set entries, on\n-  \/\/ the assumption that such objects are likely still live.\n-  G1HeapRegionRemSet* rem_set = r->rem_set();\n-\n-  return rem_set->occupancy_less_or_equal_than(G1EagerReclaimRemSetThreshold);\n-}\n-\n-#ifndef PRODUCT\n-void G1CollectedHeap::verify_region_attr_remset_is_tracked() {\n-  class VerifyRegionAttrRemSet : public G1HeapRegionClosure {\n-  public:\n-    virtual bool do_heap_region(G1HeapRegion* r) {\n-      G1CollectedHeap* g1h = G1CollectedHeap::heap();\n-      bool const remset_is_tracked = g1h->region_attr(r->bottom()).remset_is_tracked();\n-      assert(r->rem_set()->is_tracked() == remset_is_tracked,\n-             \"Region %u remset tracking status (%s) different to region attribute (%s)\",\n-             r->hrm_index(), BOOL_TO_STR(r->rem_set()->is_tracked()), BOOL_TO_STR(remset_is_tracked));\n-      return false;\n-    }\n-  } cl;\n-  heap_region_iterate(&cl);\n-}\n-#endif\n-\n-void G1CollectedHeap::update_perf_counter_cpu_time() {\n-  assert(Thread::current()->is_VM_thread(),\n-         \"Must be called from VM thread to avoid races\");\n-  if (!UsePerfData) {\n-    return;\n-  }\n-\n-  \/\/ Ensure ThreadTotalCPUTimeClosure destructor is called before publishing gc\n-  \/\/ time.\n-  {\n-    ThreadTotalCPUTimeClosure tttc(CPUTimeGroups::CPUTimeType::gc_parallel_workers);\n-    \/\/ Currently parallel worker threads never terminate (JDK-8081682), so it is\n-    \/\/ safe for VMThread to read their CPU times. However, if JDK-8087340 is\n-    \/\/ resolved so they terminate, we should rethink if it is still safe.\n-    workers()->threads_do(&tttc);\n-  }\n-\n-  CPUTimeCounters::publish_gc_total_cpu_time();\n-}\n-\n-void G1CollectedHeap::start_new_collection_set() {\n-  collection_set()->start_incremental_building();\n-\n-  clear_region_attr();\n-\n-  guarantee(_eden.length() == 0, \"eden should have been cleared\");\n-  policy()->transfer_survivors_to_cset(survivor());\n-\n-  \/\/ We redo the verification but now wrt to the new CSet which\n-  \/\/ has just got initialized after the previous CSet was freed.\n-  _cm->verify_no_collection_set_oops();\n-}\n-\n-void G1CollectedHeap::verify_before_young_collection(G1HeapVerifier::G1VerifyType type) {\n-  if (!VerifyBeforeGC) {\n-    return;\n-  }\n-  if (!G1HeapVerifier::should_verify(type)) {\n-    return;\n-  }\n-  Ticks start = Ticks::now();\n-  _verifier->prepare_for_verify();\n-  _verifier->verify_region_sets_optional();\n-  _verifier->verify_dirty_young_regions();\n-  _verifier->verify_before_gc();\n-  verify_numa_regions(\"GC Start\");\n-  phase_times()->record_verify_before_time_ms((Ticks::now() - start).seconds() * MILLIUNITS);\n-}\n-\n-void G1CollectedHeap::verify_after_young_collection(G1HeapVerifier::G1VerifyType type) {\n-  if (!VerifyAfterGC) {\n-    return;\n-  }\n-  if (!G1HeapVerifier::should_verify(type)) {\n-    return;\n-  }\n-  Ticks start = Ticks::now();\n-  _verifier->verify_after_gc();\n-  verify_numa_regions(\"GC End\");\n-  _verifier->verify_region_sets_optional();\n-\n-  if (collector_state()->in_concurrent_start_gc()) {\n-    log_debug(gc, verify)(\"Marking state\");\n-    _verifier->verify_marking_state();\n-  }\n-\n-  phase_times()->record_verify_after_time_ms((Ticks::now() - start).seconds() * MILLIUNITS);\n-}\n-\n-void G1CollectedHeap::do_collection_pause_at_safepoint(size_t allocation_word_size) {\n-  assert_at_safepoint_on_vm_thread();\n-  guarantee(!is_stw_gc_active(), \"collection is not reentrant\");\n-\n-  do_collection_pause_at_safepoint_helper(allocation_word_size);\n-}\n-\n-G1HeapPrinterMark::G1HeapPrinterMark(G1CollectedHeap* g1h) : _g1h(g1h), _heap_transition(g1h) {\n-  \/\/ This summary needs to be printed before incrementing total collections.\n-  _g1h->rem_set()->print_periodic_summary_info(\"Before GC RS summary\",\n-                                               _g1h->total_collections(),\n-                                               true \/* show_thread_times *\/);\n-  _g1h->print_before_gc();\n-  _g1h->print_heap_regions();\n-}\n-\n-G1HeapPrinterMark::~G1HeapPrinterMark() {\n-  _g1h->policy()->print_age_table();\n-  _g1h->rem_set()->print_coarsen_stats();\n-  \/\/ We are at the end of the GC. Total collections has already been increased.\n-  _g1h->rem_set()->print_periodic_summary_info(\"After GC RS summary\",\n-                                               _g1h->total_collections() - 1,\n-                                               false \/* show_thread_times *\/);\n-\n-  _heap_transition.print();\n-  _g1h->print_heap_regions();\n-  _g1h->print_after_gc();\n-  \/\/ Print NUMA statistics.\n-  _g1h->numa()->print_statistics();\n-}\n-\n-G1JFRTracerMark::G1JFRTracerMark(STWGCTimer* timer, GCTracer* tracer) :\n-  _timer(timer), _tracer(tracer) {\n-\n-  _timer->register_gc_start();\n-  _tracer->report_gc_start(G1CollectedHeap::heap()->gc_cause(), _timer->gc_start());\n-  G1CollectedHeap::heap()->trace_heap_before_gc(_tracer);\n-}\n-\n-G1JFRTracerMark::~G1JFRTracerMark() {\n-  G1CollectedHeap::heap()->trace_heap_after_gc(_tracer);\n-  _timer->register_gc_end();\n-  _tracer->report_gc_end(_timer->gc_end(), _timer->time_partitions());\n-}\n-\n-void G1CollectedHeap::prepare_for_mutator_after_young_collection() {\n-  Ticks start = Ticks::now();\n-\n-  _survivor_evac_stats.adjust_desired_plab_size();\n-  _old_evac_stats.adjust_desired_plab_size();\n-\n-  \/\/ Start a new incremental collection set for the mutator phase.\n-  start_new_collection_set();\n-  _allocator->init_mutator_alloc_regions();\n-\n-  phase_times()->record_prepare_for_mutator_time_ms((Ticks::now() - start).seconds() * 1000.0);\n-}\n-\n-void G1CollectedHeap::retire_tlabs() {\n-  ensure_parsability(true);\n-}\n-\n-void G1CollectedHeap::flush_region_pin_cache() {\n-  for (JavaThreadIteratorWithHandle jtiwh; JavaThread *thread = jtiwh.next(); ) {\n-    G1ThreadLocalData::pin_count_cache(thread).flush();\n-  }\n-}\n-\n-void G1CollectedHeap::do_collection_pause_at_safepoint_helper(size_t allocation_word_size) {\n-  ResourceMark rm;\n-\n-  IsSTWGCActiveMark active_gc_mark;\n-  GCIdMark gc_id_mark;\n-  SvcGCMarker sgcm(SvcGCMarker::MINOR);\n-\n-  GCTraceCPUTime tcpu(_gc_tracer_stw);\n-\n-  _bytes_used_during_gc = 0;\n-\n-  policy()->decide_on_concurrent_start_pause();\n-  \/\/ Record whether this pause may need to trigger a concurrent operation. Later,\n-  \/\/ when we signal the G1ConcurrentMarkThread, the collector state has already\n-  \/\/ been reset for the next pause.\n-  bool should_start_concurrent_mark_operation = collector_state()->in_concurrent_start_gc();\n-\n-  \/\/ Perform the collection.\n-  G1YoungCollector collector(gc_cause(), allocation_word_size);\n-  collector.collect();\n-\n-  \/\/ It should now be safe to tell the concurrent mark thread to start\n-  \/\/ without its logging output interfering with the logging output\n-  \/\/ that came from the pause.\n-  if (should_start_concurrent_mark_operation) {\n-    verifier()->verify_bitmap_clear(true \/* above_tams_only *\/);\n-    \/\/ CAUTION: after the start_concurrent_cycle() call below, the concurrent marking\n-    \/\/ thread(s) could be running concurrently with us. Make sure that anything\n-    \/\/ after this point does not assume that we are the only GC thread running.\n-    \/\/ Note: of course, the actual marking work will not start until the safepoint\n-    \/\/ itself is released in SuspendibleThreadSet::desynchronize().\n-    start_concurrent_cycle(collector.concurrent_operation_is_full_mark());\n-    ConcurrentGCBreakpoints::notify_idle_to_active();\n-  }\n-}\n-\n-void G1CollectedHeap::complete_cleaning(bool class_unloading_occurred) {\n-  uint num_workers = workers()->active_workers();\n-  G1ParallelCleaningTask unlink_task(num_workers, class_unloading_occurred);\n-  workers()->run_task(&unlink_task);\n-}\n-\n-void G1CollectedHeap::unload_classes_and_code(const char* description, BoolObjectClosure* is_alive, GCTimer* timer) {\n-  GCTraceTime(Debug, gc, phases) debug(description, timer);\n-\n-  ClassUnloadingContext ctx(workers()->active_workers(),\n-                            false \/* unregister_nmethods_during_purge *\/,\n-                            false \/* lock_nmethod_free_separately *\/);\n-  {\n-    CodeCache::UnlinkingScope scope(is_alive);\n-    bool unloading_occurred = SystemDictionary::do_unloading(timer);\n-    GCTraceTime(Debug, gc, phases) t(\"G1 Complete Cleaning\", timer);\n-    complete_cleaning(unloading_occurred);\n-  }\n-  {\n-    GCTraceTime(Debug, gc, phases) t(\"Purge Unlinked NMethods\", timer);\n-    ctx.purge_nmethods();\n-  }\n-  {\n-    GCTraceTime(Debug, gc, phases) ur(\"Unregister NMethods\", timer);\n-    G1CollectedHeap::heap()->bulk_unregister_nmethods();\n-  }\n-  {\n-    GCTraceTime(Debug, gc, phases) t(\"Free Code Blobs\", timer);\n-    ctx.free_nmethods();\n-  }\n-  {\n-    GCTraceTime(Debug, gc, phases) t(\"Purge Class Loader Data\", timer);\n-    ClassLoaderDataGraph::purge(true \/* at_safepoint *\/);\n-    DEBUG_ONLY(MetaspaceUtils::verify();)\n-  }\n-}\n-\n-class G1BulkUnregisterNMethodTask : public WorkerTask {\n-  G1HeapRegionClaimer _hrclaimer;\n-\n-  class UnregisterNMethodsHeapRegionClosure : public G1HeapRegionClosure {\n-  public:\n-\n-    bool do_heap_region(G1HeapRegion* hr) {\n-      hr->rem_set()->bulk_remove_code_roots();\n-      return false;\n-    }\n-  } _cl;\n-\n-public:\n-  G1BulkUnregisterNMethodTask(uint num_workers)\n-  : WorkerTask(\"G1 Remove Unlinked NMethods From Code Root Set Task\"),\n-    _hrclaimer(num_workers) { }\n-\n-  void work(uint worker_id) {\n-    G1CollectedHeap::heap()->heap_region_par_iterate_from_worker_offset(&_cl, &_hrclaimer, worker_id);\n-  }\n-};\n-\n-void G1CollectedHeap::bulk_unregister_nmethods() {\n-  uint num_workers = workers()->active_workers();\n-  G1BulkUnregisterNMethodTask t(num_workers);\n-  workers()->run_task(&t);\n-}\n-\n-bool G1STWSubjectToDiscoveryClosure::do_object_b(oop obj) {\n-  assert(obj != nullptr, \"must not be null\");\n-  assert(_g1h->is_in_reserved(obj), \"Trying to discover obj \" PTR_FORMAT \" not in heap\", p2i(obj));\n-  \/\/ The areas the CM and STW ref processor manage must be disjoint. The is_in_cset() below\n-  \/\/ may falsely indicate that this is not the case here: however the collection set only\n-  \/\/ contains old regions when concurrent mark is not running.\n-  return _g1h->is_in_cset(obj) || _g1h->heap_region_containing(obj)->is_survivor();\n-}\n-\n-void G1CollectedHeap::make_pending_list_reachable() {\n-  if (collector_state()->in_concurrent_start_gc()) {\n-    oop pll_head = Universe::reference_pending_list();\n-    if (pll_head != nullptr) {\n-      \/\/ Any valid worker id is fine here as we are in the VM thread and single-threaded.\n-      _cm->mark_in_bitmap(0 \/* worker_id *\/, pll_head);\n-    }\n-  }\n-}\n-\n-void G1CollectedHeap::set_humongous_stats(uint num_humongous_total, uint num_humongous_candidates) {\n-  _num_humongous_objects = num_humongous_total;\n-  _num_humongous_reclaim_candidates = num_humongous_candidates;\n-}\n-\n-bool G1CollectedHeap::should_sample_collection_set_candidates() const {\n-  const G1CollectionSetCandidates* candidates = collection_set()->candidates();\n-  return !candidates->is_empty();\n-}\n-\n-void G1CollectedHeap::set_collection_set_candidates_stats(G1MonotonicArenaMemoryStats& stats) {\n-  _collection_set_candidates_card_set_stats = stats;\n-}\n-\n-void G1CollectedHeap::set_young_gen_card_set_stats(const G1MonotonicArenaMemoryStats& stats) {\n-  _young_gen_card_set_stats = stats;\n-}\n-\n-void G1CollectedHeap::record_obj_copy_mem_stats() {\n-  size_t total_old_allocated = _old_evac_stats.allocated() + _old_evac_stats.direct_allocated();\n-  uint total_allocated = _survivor_evac_stats.regions_filled() + _old_evac_stats.regions_filled();\n-\n-  log_debug(gc)(\"Allocated %u survivor %u old percent total %1.2f%% (%u%%)\",\n-                _survivor_evac_stats.regions_filled(), _old_evac_stats.regions_filled(),\n-                percent_of(total_allocated, num_committed_regions() - total_allocated),\n-                G1ReservePercent);\n-\n-  policy()->old_gen_alloc_tracker()->\n-    add_allocated_bytes_since_last_gc(total_old_allocated * HeapWordSize);\n-\n-  _gc_tracer_stw->report_evacuation_statistics(create_g1_evac_summary(&_survivor_evac_stats),\n-                                               create_g1_evac_summary(&_old_evac_stats));\n-}\n-\n-void G1CollectedHeap::clear_bitmap_for_region(G1HeapRegion* hr) {\n-  concurrent_mark()->clear_bitmap_for_region(hr);\n-}\n-\n-void G1CollectedHeap::free_region(G1HeapRegion* hr, G1FreeRegionList* free_list) {\n-  assert(!hr->is_free(), \"the region should not be free\");\n-  assert(!hr->is_empty(), \"the region should not be empty\");\n-  assert(_hrm.is_available(hr->hrm_index()), \"region should be committed\");\n-  assert(!hr->has_pinned_objects(),\n-         \"must not free a region which contains pinned objects\");\n-\n-  \/\/ Reset region metadata to allow reuse.\n-  hr->hr_clear(true \/* clear_space *\/);\n-  _policy->remset_tracker()->update_at_free(hr);\n-\n-  if (free_list != nullptr) {\n-    free_list->add_ordered(hr);\n-  }\n-}\n-\n-void G1CollectedHeap::retain_region(G1HeapRegion* hr) {\n-  MutexLocker x(G1RareEvent_lock, Mutex::_no_safepoint_check_flag);\n-  collection_set()->candidates()->add_retained_region_unsorted(hr);\n-}\n-\n-void G1CollectedHeap::free_humongous_region(G1HeapRegion* hr,\n-                                            G1FreeRegionList* free_list) {\n-  assert(hr->is_humongous(), \"this is only for humongous regions\");\n-  hr->clear_humongous();\n-  free_region(hr, free_list);\n-}\n-\n-void G1CollectedHeap::remove_from_old_gen_sets(const uint old_regions_removed,\n-                                               const uint humongous_regions_removed) {\n-  if (old_regions_removed > 0 || humongous_regions_removed > 0) {\n-    MutexLocker x(OldSets_lock, Mutex::_no_safepoint_check_flag);\n-    _old_set.bulk_remove(old_regions_removed);\n-    _humongous_set.bulk_remove(humongous_regions_removed);\n-  }\n-\n-}\n-\n-void G1CollectedHeap::prepend_to_freelist(G1FreeRegionList* list) {\n-  assert(list != nullptr, \"list can't be null\");\n-  if (!list->is_empty()) {\n-    MutexLocker x(FreeList_lock, Mutex::_no_safepoint_check_flag);\n-    _hrm.insert_list_into_free_list(list);\n-  }\n-}\n-\n-void G1CollectedHeap::decrement_summary_bytes(size_t bytes) {\n-  decrease_used(bytes);\n-}\n-\n-void G1CollectedHeap::clear_eden() {\n-  _eden.clear();\n-}\n-\n-void G1CollectedHeap::clear_collection_set() {\n-  collection_set()->clear();\n-}\n-\n-void G1CollectedHeap::rebuild_free_region_list() {\n-  Ticks start = Ticks::now();\n-  _hrm.rebuild_free_list(workers());\n-  phase_times()->record_total_rebuild_freelist_time_ms((Ticks::now() - start).seconds() * 1000.0);\n-}\n-\n-class G1AbandonCollectionSetClosure : public G1HeapRegionClosure {\n-public:\n-  virtual bool do_heap_region(G1HeapRegion* r) {\n-    assert(r->in_collection_set(), \"Region %u must have been in collection set\", r->hrm_index());\n-    G1CollectedHeap::heap()->clear_region_attr(r);\n-    r->clear_young_index_in_cset();\n-    return false;\n-  }\n-};\n-\n-void G1CollectedHeap::abandon_collection_set(G1CollectionSet* collection_set) {\n-  G1AbandonCollectionSetClosure cl;\n-  collection_set_iterate_all(&cl);\n-\n-  collection_set->clear();\n-  collection_set->stop_incremental_building();\n-}\n-\n-bool G1CollectedHeap::is_old_gc_alloc_region(G1HeapRegion* hr) {\n-  return _allocator->is_retained_old_region(hr);\n-}\n-\n-void G1CollectedHeap::set_region_short_lived_locked(G1HeapRegion* hr) {\n-  _eden.add(hr);\n-  _policy->set_region_eden(hr);\n-  young_regions_cset_group()->add(hr);\n-}\n-\n-#ifdef ASSERT\n-\n-class NoYoungRegionsClosure: public G1HeapRegionClosure {\n-private:\n-  bool _success;\n-public:\n-  NoYoungRegionsClosure() : _success(true) { }\n-  bool do_heap_region(G1HeapRegion* r) {\n-    if (r->is_young()) {\n-      log_error(gc, verify)(\"Region [\" PTR_FORMAT \", \" PTR_FORMAT \") tagged as young\",\n-                            p2i(r->bottom()), p2i(r->end()));\n-      _success = false;\n-    }\n-    return false;\n-  }\n-  bool success() { return _success; }\n-};\n-\n-bool G1CollectedHeap::check_young_list_empty() {\n-  bool ret = (young_regions_count() == 0);\n-\n-  NoYoungRegionsClosure closure;\n-  heap_region_iterate(&closure);\n-  ret = ret && closure.success();\n-\n-  return ret;\n-}\n-\n-#endif \/\/ ASSERT\n-\n-\/\/ Remove the given G1HeapRegion from the appropriate region set.\n-void G1CollectedHeap::prepare_region_for_full_compaction(G1HeapRegion* hr) {\n-  if (hr->is_humongous()) {\n-    _humongous_set.remove(hr);\n-  } else if (hr->is_old()) {\n-    _old_set.remove(hr);\n-  } else if (hr->is_young()) {\n-    \/\/ Note that emptying the eden and survivor lists is postponed and instead\n-    \/\/ done as the first step when rebuilding the regions sets again. The reason\n-    \/\/ for this is that during a full GC string deduplication needs to know if\n-    \/\/ a collected region was young or old when the full GC was initiated.\n-    hr->uninstall_surv_rate_group();\n-  } else {\n-    \/\/ We ignore free regions, we'll empty the free list afterwards.\n-    assert(hr->is_free(), \"it cannot be another type\");\n-  }\n-}\n-\n-void G1CollectedHeap::increase_used(size_t bytes) {\n-  _summary_bytes_used += bytes;\n-}\n-\n-void G1CollectedHeap::decrease_used(size_t bytes) {\n-  assert(_summary_bytes_used >= bytes,\n-         \"invariant: _summary_bytes_used: %zu should be >= bytes: %zu\",\n-         _summary_bytes_used, bytes);\n-  _summary_bytes_used -= bytes;\n-}\n-\n-void G1CollectedHeap::set_used(size_t bytes) {\n-  _summary_bytes_used = bytes;\n-}\n-\n-class RebuildRegionSetsClosure : public G1HeapRegionClosure {\n-private:\n-  bool _free_list_only;\n-\n-  G1HeapRegionSet* _old_set;\n-  G1HeapRegionSet* _humongous_set;\n-\n-  G1HeapRegionManager* _hrm;\n-\n-  size_t _total_used;\n-\n-public:\n-  RebuildRegionSetsClosure(bool free_list_only,\n-                           G1HeapRegionSet* old_set,\n-                           G1HeapRegionSet* humongous_set,\n-                           G1HeapRegionManager* hrm) :\n-    _free_list_only(free_list_only), _old_set(old_set),\n-    _humongous_set(humongous_set), _hrm(hrm), _total_used(0) {\n-    assert(_hrm->num_free_regions() == 0, \"pre-condition\");\n-    if (!free_list_only) {\n-      assert(_old_set->is_empty(), \"pre-condition\");\n-      assert(_humongous_set->is_empty(), \"pre-condition\");\n-    }\n-  }\n-\n-  bool do_heap_region(G1HeapRegion* r) {\n-    if (r->is_empty()) {\n-      assert(r->rem_set()->is_empty(), \"Empty regions should have empty remembered sets.\");\n-      \/\/ Add free regions to the free list\n-      r->set_free();\n-      _hrm->insert_into_free_list(r);\n-    } else if (!_free_list_only) {\n-      assert(r->rem_set()->is_empty(), \"At this point remembered sets must have been cleared.\");\n-\n-      if (r->is_humongous()) {\n-        _humongous_set->add(r);\n-      } else {\n-        assert(r->is_young() || r->is_free() || r->is_old(), \"invariant\");\n-        \/\/ We now move all (non-humongous, non-old) regions to old gen,\n-        \/\/ and register them as such.\n-        r->move_to_old();\n-        _old_set->add(r);\n-      }\n-      _total_used += r->used();\n-    }\n-\n-    return false;\n-  }\n-\n-  size_t total_used() {\n-    return _total_used;\n-  }\n-};\n-\n-void G1CollectedHeap::rebuild_region_sets(bool free_list_only) {\n-  assert_at_safepoint_on_vm_thread();\n-\n-  if (!free_list_only) {\n-    _eden.clear();\n-    _survivor.clear();\n-  }\n-\n-  RebuildRegionSetsClosure cl(free_list_only,\n-                              &_old_set, &_humongous_set,\n-                              &_hrm);\n-  heap_region_iterate(&cl);\n-\n-  if (!free_list_only) {\n-    set_used(cl.total_used());\n-  }\n-  assert_used_and_recalculate_used_equal(this);\n-}\n-\n-\/\/ Methods for the mutator alloc region\n-\n-G1HeapRegion* G1CollectedHeap::new_mutator_alloc_region(size_t word_size,\n-                                                      uint node_index) {\n-  assert_heap_locked_or_at_safepoint(true \/* should_be_vm_thread *\/);\n-  bool should_allocate = policy()->should_allocate_mutator_region();\n-  if (should_allocate) {\n-    G1HeapRegion* new_alloc_region = new_region(word_size,\n-                                                G1HeapRegionType::Eden,\n-                                                false \/* do_expand *\/,\n-                                                node_index);\n-    if (new_alloc_region != nullptr) {\n-      set_region_short_lived_locked(new_alloc_region);\n-      G1HeapRegionPrinter::alloc(new_alloc_region);\n-      _policy->remset_tracker()->update_at_allocate(new_alloc_region);\n-      return new_alloc_region;\n-    }\n-  }\n-  return nullptr;\n-}\n-\n-void G1CollectedHeap::retire_mutator_alloc_region(G1HeapRegion* alloc_region,\n-                                                  size_t allocated_bytes) {\n-  assert_heap_locked_or_at_safepoint(true \/* should_be_vm_thread *\/);\n-  assert(alloc_region->is_eden(), \"all mutator alloc regions should be eden\");\n-\n-  collection_set()->add_eden_region(alloc_region);\n-  increase_used(allocated_bytes);\n-  _eden.add_used_bytes(allocated_bytes);\n-  G1HeapRegionPrinter::retire(alloc_region);\n-\n-  \/\/ We update the eden sizes here, when the region is retired,\n-  \/\/ instead of when it's allocated, since this is the point that its\n-  \/\/ used space has been recorded in _summary_bytes_used.\n-  monitoring_support()->update_eden_size();\n-}\n-\n-\/\/ Methods for the GC alloc regions\n-\n-bool G1CollectedHeap::has_more_regions(G1HeapRegionAttr dest) {\n-  if (dest.is_old()) {\n-    return true;\n-  } else {\n-    return survivor_regions_count() < policy()->max_survivor_regions();\n-  }\n-}\n-\n-G1HeapRegion* G1CollectedHeap::new_gc_alloc_region(size_t word_size, G1HeapRegionAttr dest, uint node_index) {\n-  assert(FreeList_lock->owned_by_self(), \"pre-condition\");\n-\n-  if (!has_more_regions(dest)) {\n-    return nullptr;\n-  }\n-\n-  G1HeapRegionType type;\n-  if (dest.is_young()) {\n-    type = G1HeapRegionType::Survivor;\n-  } else {\n-    type = G1HeapRegionType::Old;\n-  }\n-\n-  G1HeapRegion* new_alloc_region = new_region(word_size,\n-                                              type,\n-                                              true \/* do_expand *\/,\n-                                              node_index);\n-\n-  if (new_alloc_region != nullptr) {\n-    if (type.is_survivor()) {\n-      new_alloc_region->set_survivor();\n-      _survivor.add(new_alloc_region);\n-      register_new_survivor_region_with_region_attr(new_alloc_region);\n-      \/\/ Install the group cardset.\n-      young_regions_cset_group()->add(new_alloc_region);\n-    } else {\n-      new_alloc_region->set_old();\n-    }\n-    _policy->remset_tracker()->update_at_allocate(new_alloc_region);\n-    register_region_with_region_attr(new_alloc_region);\n-    G1HeapRegionPrinter::alloc(new_alloc_region);\n-    return new_alloc_region;\n-  }\n-  return nullptr;\n-}\n-\n-void G1CollectedHeap::retire_gc_alloc_region(G1HeapRegion* alloc_region,\n-                                             size_t allocated_bytes,\n-                                             G1HeapRegionAttr dest) {\n-  _bytes_used_during_gc += allocated_bytes;\n-  if (dest.is_old()) {\n-    old_set_add(alloc_region);\n-  } else {\n-    assert(dest.is_young(), \"Retiring alloc region should be young (%d)\", dest.type());\n-    _survivor.add_used_bytes(allocated_bytes);\n-  }\n-\n-  bool const during_im = collector_state()->in_concurrent_start_gc();\n-  if (during_im && allocated_bytes > 0) {\n-    _cm->add_root_region(alloc_region);\n-  }\n-  G1HeapRegionPrinter::retire(alloc_region);\n-}\n-\n-void G1CollectedHeap::mark_evac_failure_object(uint worker_id, const oop obj, size_t obj_size) const {\n-  assert(!_cm->is_marked_in_bitmap(obj), \"must be\");\n-\n-  _cm->raw_mark_in_bitmap(obj);\n-}\n-\n-\/\/ Optimized nmethod scanning\n-class RegisterNMethodOopClosure: public OopClosure {\n-  G1CollectedHeap* _g1h;\n-  nmethod* _nm;\n-\n-public:\n-  RegisterNMethodOopClosure(G1CollectedHeap* g1h, nmethod* nm) :\n-    _g1h(g1h), _nm(nm) {}\n-\n-  void do_oop(oop* p) {\n-    oop heap_oop = RawAccess<>::oop_load(p);\n-    if (!CompressedOops::is_null(heap_oop)) {\n-      oop obj = CompressedOops::decode_not_null(heap_oop);\n-      G1HeapRegion* hr = _g1h->heap_region_containing(obj);\n-      assert(!hr->is_continues_humongous(),\n-             \"trying to add code root \" PTR_FORMAT \" in continuation of humongous region \" HR_FORMAT\n-             \" starting at \" HR_FORMAT,\n-             p2i(_nm), HR_FORMAT_PARAMS(hr), HR_FORMAT_PARAMS(hr->humongous_start_region()));\n-\n-      hr->add_code_root(_nm);\n-    }\n-  }\n-\n-  void do_oop(narrowOop* p) { ShouldNotReachHere(); }\n-};\n-\n-void G1CollectedHeap::register_nmethod(nmethod* nm) {\n-  guarantee(nm != nullptr, \"sanity\");\n-  RegisterNMethodOopClosure reg_cl(this, nm);\n-  nm->oops_do(&reg_cl);\n-}\n-\n-void G1CollectedHeap::unregister_nmethod(nmethod* nm) {\n-  \/\/ We always unregister nmethods in bulk during code unloading only.\n-  ShouldNotReachHere();\n-}\n-\n-void G1CollectedHeap::update_used_after_gc(bool evacuation_failed) {\n-  if (evacuation_failed) {\n-    set_used(recalculate_used());\n-  } else {\n-    \/\/ The \"used\" of the collection set have already been subtracted\n-    \/\/ when they were freed.  Add in the bytes used.\n-    increase_used(_bytes_used_during_gc);\n-  }\n-}\n-\n-class RebuildCodeRootClosure: public NMethodClosure {\n-  G1CollectedHeap* _g1h;\n-\n-public:\n-  RebuildCodeRootClosure(G1CollectedHeap* g1h) :\n-    _g1h(g1h) {}\n-\n-  void do_nmethod(nmethod* nm) {\n-    assert(nm != nullptr, \"Sanity\");\n-    _g1h->register_nmethod(nm);\n-  }\n-};\n-\n-void G1CollectedHeap::rebuild_code_roots() {\n-  RebuildCodeRootClosure nmethod_cl(this);\n-  CodeCache::nmethods_do(&nmethod_cl);\n-}\n-\n-void G1CollectedHeap::initialize_serviceability() {\n-  _monitoring_support->initialize_serviceability();\n-}\n-\n-MemoryUsage G1CollectedHeap::memory_usage() {\n-  return _monitoring_support->memory_usage();\n-}\n-\n-GrowableArray<GCMemoryManager*> G1CollectedHeap::memory_managers() {\n-  return _monitoring_support->memory_managers();\n-}\n-\n-GrowableArray<MemoryPool*> G1CollectedHeap::memory_pools() {\n-  return _monitoring_support->memory_pools();\n-}\n-\n-void G1CollectedHeap::fill_with_dummy_object(HeapWord* start, HeapWord* end, bool zap) {\n-  G1HeapRegion* region = heap_region_containing(start);\n-  region->fill_with_dummy_object(start, pointer_delta(end, start), zap);\n-}\n-\n-void G1CollectedHeap::start_codecache_marking_cycle_if_inactive(bool concurrent_mark_start) {\n-  \/\/ We can reach here with an active code cache marking cycle either because the\n-  \/\/ previous G1 concurrent marking cycle was undone (if heap occupancy after the\n-  \/\/ concurrent start young collection was below the threshold) or aborted. See\n-  \/\/ CodeCache::on_gc_marking_cycle_finish() why this is.  We must not start a new code\n-  \/\/ cache cycle then. If we are about to start a new g1 concurrent marking cycle we\n-  \/\/ still have to arm all nmethod entry barriers. They are needed for adding oop\n-  \/\/ constants to the SATB snapshot. Full GC does not need nmethods to be armed.\n-  if (!CodeCache::is_gc_marking_cycle_active()) {\n-    CodeCache::on_gc_marking_cycle_start();\n-  }\n-  if (concurrent_mark_start) {\n-    CodeCache::arm_all_nmethods();\n-  }\n-}\n-\n-void G1CollectedHeap::finish_codecache_marking_cycle() {\n-  CodeCache::on_gc_marking_cycle_finish();\n-  CodeCache::arm_all_nmethods();\n-}\n-\n-void G1CollectedHeap::prepare_group_cardsets_for_scan() {\n-  young_regions_cardset()->reset_table_scanner_for_groups();\n-\n-  collection_set()->prepare_groups_for_scan();\n-}\n+\/* * Copyright (c) 2001, 2025, Oracle and\/or its affiliates. All rights reserved. * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER. * * This code is free software; you can redistribute it and\/or modify it * under the terms of the GNU General Public License version 2 only, as * published by the Free Software Foundation. * * This code is distributed in the hope that it will be useful, but WITHOUT * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License * version 2 for more details (a copy is included in the LICENSE file that * accompanied this code). * * You should have received a copy of the GNU General Public License version * 2 along with this work; if not, write to the Free Software Foundation, * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA. * * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA * or visit www.oracle.com if you need additional information or have any * questions. * *\/#include \"classfile\/classLoaderDataGraph.hpp\"#include \"classfile\/metadataOnStackMark.hpp\"#include \"classfile\/systemDictionary.hpp\"#include \"code\/codeCache.hpp\"#include \"compiler\/oopMap.hpp\"#include \"gc\/g1\/g1Allocator.inline.hpp\"#include \"gc\/g1\/g1Arguments.hpp\"#include \"gc\/g1\/g1BarrierSet.hpp\"#include \"gc\/g1\/g1BatchedTask.hpp\"#include \"gc\/g1\/g1CollectedHeap.inline.hpp\"#include \"gc\/g1\/g1CollectionSet.hpp\"#include \"gc\/g1\/g1CollectionSetCandidates.hpp\"#include \"gc\/g1\/g1CollectorState.hpp\"#include \"gc\/g1\/g1ConcurrentMarkThread.inline.hpp\"#include \"gc\/g1\/g1ConcurrentRefine.hpp\"#include \"gc\/g1\/g1ConcurrentRefineThread.hpp\"#include \"gc\/g1\/g1HeapSizingPolicy.hpp\"  \/\/ Include this first to avoid include cycle#include \"gc\/g1\/g1HeapEvaluationTask.hpp\"#include \"gc\/g1\/g1DirtyCardQueue.hpp\"#include \"gc\/g1\/g1EvacStats.inline.hpp\"#include \"gc\/g1\/g1FullCollector.hpp\"#include \"gc\/g1\/g1GCCounters.hpp\"#include \"gc\/g1\/g1GCParPhaseTimesTracker.hpp\"#include \"gc\/g1\/g1GCPauseType.hpp\"#include \"gc\/g1\/g1GCPhaseTimes.hpp\"#include \"gc\/g1\/g1HeapRegion.inline.hpp\"#include \"gc\/g1\/g1HeapRegionPrinter.hpp\"#include \"gc\/g1\/g1HeapRegionRemSet.inline.hpp\"#include \"gc\/g1\/g1HeapRegionSet.inline.hpp\"#include \"gc\/g1\/g1HeapTransition.hpp\"#include \"gc\/g1\/g1HeapVerifier.hpp\"#include \"gc\/g1\/g1InitLogger.hpp\"#include \"gc\/g1\/g1MemoryPool.hpp\"#include \"gc\/g1\/g1MonotonicArenaFreeMemoryTask.hpp\"#include \"gc\/g1\/g1OopClosures.inline.hpp\"#include \"gc\/g1\/g1ParallelCleaning.hpp\"#include \"gc\/g1\/g1ParScanThreadState.inline.hpp\"#include \"gc\/g1\/g1PeriodicGCTask.hpp\"#include \"gc\/g1\/g1Policy.hpp\"#include \"gc\/g1\/g1RedirtyCardsQueue.hpp\"#include \"gc\/g1\/g1RegionPinCache.inline.hpp\"#include \"gc\/g1\/g1RegionToSpaceMapper.hpp\"#include \"gc\/g1\/g1RemSet.hpp\"#include \"gc\/g1\/g1RootClosures.hpp\"#include \"gc\/g1\/g1RootProcessor.hpp\"#include \"gc\/g1\/g1SATBMarkQueueSet.hpp\"#include \"gc\/g1\/g1ServiceThread.hpp\"#include \"gc\/g1\/g1ThreadLocalData.hpp\"#include \"gc\/g1\/g1Trace.hpp\"#include \"gc\/g1\/g1UncommitRegionTask.hpp\"#include \"gc\/g1\/g1VMOperations.hpp\"#include \"gc\/g1\/g1YoungCollector.hpp\"#include \"gc\/g1\/g1YoungGCAllocationFailureInjector.hpp\"#include \"gc\/shared\/classUnloadingContext.hpp\"#include \"gc\/shared\/concurrentGCBreakpoints.hpp\"#include \"gc\/shared\/fullGCForwarding.hpp\"#include \"gc\/shared\/gcBehaviours.hpp\"#include \"gc\/shared\/gcHeapSummary.hpp\"#include \"gc\/shared\/gcId.hpp\"#include \"gc\/shared\/gcTimer.hpp\"#include \"gc\/shared\/gcTraceTime.inline.hpp\"#include \"gc\/shared\/isGCActiveMark.hpp\"#include \"gc\/shared\/locationPrinter.inline.hpp\"#include \"gc\/shared\/oopStorageParState.hpp\"#include \"gc\/shared\/partialArrayState.hpp\"#include \"gc\/shared\/referenceProcessor.inline.hpp\"#include \"gc\/shared\/suspendibleThreadSet.hpp\"#include \"gc\/shared\/taskqueue.inline.hpp\"#include \"gc\/shared\/taskTerminator.hpp\"#include \"gc\/shared\/tlab_globals.hpp\"#include \"gc\/shared\/weakProcessor.inline.hpp\"#include \"gc\/shared\/workerPolicy.hpp\"#include \"logging\/log.hpp\"#include \"memory\/allocation.hpp\"#include \"memory\/heapInspection.hpp\"#include \"memory\/iterator.hpp\"#include \"memory\/memoryReserver.hpp\"#include \"memory\/metaspaceUtils.hpp\"#include \"memory\/resourceArea.hpp\"#include \"memory\/universe.hpp\"#include \"oops\/access.inline.hpp\"#include \"oops\/compressedOops.inline.hpp\"#include \"oops\/oop.inline.hpp\"#include \"runtime\/atomic.hpp\"#include \"runtime\/cpuTimeCounters.hpp\"#include \"runtime\/handles.inline.hpp\"#include \"runtime\/init.hpp\"#include \"runtime\/java.hpp\"#include \"runtime\/orderAccess.hpp\"#include \"runtime\/threadSMR.hpp\"#include \"runtime\/vmOperations.hpp\"#include \"runtime\/vmThread.hpp\"#include \"utilities\/align.hpp\"#include \"utilities\/autoRestore.hpp\"#include \"utilities\/bitMap.inline.hpp\"#include \"utilities\/globalDefinitions.hpp\"#include \"utilities\/stack.inline.hpp\"size_t G1CollectedHeap::_humongous_object_threshold_in_words = 0;\/\/ INVARIANTS\/NOTES\/\/\/\/ All allocation activity covered by the G1CollectedHeap interface is\/\/ serialized by acquiring the HeapLock.  This happens in mem_allocate\/\/ and allocate_new_tlab, which are the \"entry\" points to the\/\/ allocation code from the rest of the JVM.  (Note that this does not\/\/ apply to TLAB allocation, which is not part of this interface: it\/\/ is done by clients of this interface.)void G1RegionMappingChangedListener::reset_from_card_cache(uint start_idx, size_t num_regions) {  G1HeapRegionRemSet::invalidate_from_card_cache(start_idx, num_regions);}void G1RegionMappingChangedListener::on_commit(uint start_idx, size_t num_regions, bool zero_filled) {  \/\/ The from card cache is not the memory that is actually committed. So we cannot  \/\/ take advantage of the zero_filled parameter.  reset_from_card_cache(start_idx, num_regions);}void G1CollectedHeap::run_batch_task(G1BatchedTask* cl) {  uint num_workers = MAX2(1u, MIN2(cl->num_workers_estimate(), workers()->active_workers()));  cl->set_max_workers(num_workers);  workers()->run_task(cl, num_workers);}uint G1CollectedHeap::get_chunks_per_region() {  uint log_region_size = G1HeapRegion::LogOfHRGrainBytes;  \/\/ Limit the expected input values to current known possible values of the  \/\/ (log) region size. Adjust as necessary after testing if changing the permissible  \/\/ values for region size.  assert(log_region_size >= 20 && log_region_size <= 29,         \"expected value in [20,29], but got %u\", log_region_size);  return 1u << (log_region_size \/ 2 - 4);}G1HeapRegion* G1CollectedHeap::new_heap_region(uint hrs_index,                                               MemRegion mr) {  return new G1HeapRegion(hrs_index, bot(), mr, &_card_set_config);}\/\/ Private methods.G1HeapRegion* G1CollectedHeap::new_region(size_t word_size,                                          G1HeapRegionType type,                                          bool do_expand,                                          uint node_index) {  assert(!is_humongous(word_size) || word_size <= G1HeapRegion::GrainWords,         \"the only time we use this to allocate a humongous region is \"         \"when we are allocating a single humongous region\");  G1HeapRegion* res = _hrm.allocate_free_region(type, node_index);  if (res == nullptr && do_expand) {    \/\/ Currently, only attempts to allocate GC alloc regions set    \/\/ do_expand to true. So, we should only reach here during a    \/\/ safepoint.    assert(SafepointSynchronize::is_at_safepoint(), \"invariant\");    log_debug(gc, ergo, heap)(\"Attempt heap expansion (region allocation request failed). Allocation request: %zuB\",                              word_size * HeapWordSize);    assert(word_size * HeapWordSize < G1HeapRegion::GrainBytes,           \"This kind of expansion should never be more than one region. Size: %zu\",           word_size * HeapWordSize);    if (expand_single_region(node_index)) {      \/\/ Given that expand_single_region() succeeded in expanding the heap, and we      \/\/ always expand the heap by an amount aligned to the heap      \/\/ region size, the free list should in theory not be empty.      \/\/ In either case allocate_free_region() will check for null.      res = _hrm.allocate_free_region(type, node_index);    }  }  return res;}void G1CollectedHeap::set_humongous_metadata(G1HeapRegion* first_hr,                                             uint num_regions,                                             size_t word_size,                                             bool update_remsets) {  \/\/ Calculate the new top of the humongous object.  HeapWord* obj_top = first_hr->bottom() + word_size;  \/\/ The word size sum of all the regions used  size_t word_size_sum = num_regions * G1HeapRegion::GrainWords;  assert(word_size <= word_size_sum, \"sanity\");  \/\/ How many words memory we \"waste\" which cannot hold a filler object.  size_t words_not_fillable = 0;  \/\/ Pad out the unused tail of the last region with filler  \/\/ objects, for improved usage accounting.  \/\/ How many words can we use for filler objects.  size_t words_fillable = word_size_sum - word_size;  if (words_fillable >= G1CollectedHeap::min_fill_size()) {    G1CollectedHeap::fill_with_objects(obj_top, words_fillable);  } else {    \/\/ We have space to fill, but we cannot fit an object there.    words_not_fillable = words_fillable;    words_fillable = 0;  }  \/\/ We will set up the first region as \"starts humongous\". This  \/\/ will also update the BOT covering all the regions to reflect  \/\/ that there is a single object that starts at the bottom of the  \/\/ first region.  first_hr->hr_clear(false \/* clear_space *\/);  first_hr->set_starts_humongous(obj_top, words_fillable);  if (update_remsets) {    _policy->remset_tracker()->update_at_allocate(first_hr);  }  \/\/ Indices of first and last regions in the series.  uint first = first_hr->hrm_index();  uint last = first + num_regions - 1;  G1HeapRegion* hr = nullptr;  for (uint i = first + 1; i <= last; ++i) {    hr = region_at(i);    hr->hr_clear(false \/* clear_space *\/);    hr->set_continues_humongous(first_hr);    if (update_remsets) {      _policy->remset_tracker()->update_at_allocate(hr);    }  }  \/\/ Up to this point no concurrent thread would have been able to  \/\/ do any scanning on any region in this series. All the top  \/\/ fields still point to bottom, so the intersection between  \/\/ [bottom,top] and [card_start,card_end] will be empty. Before we  \/\/ update the top fields, we'll do a storestore to make sure that  \/\/ no thread sees the update to top before the zeroing of the  \/\/ object header and the BOT initialization.  OrderAccess::storestore();  \/\/ Now, we will update the top fields of the \"continues humongous\"  \/\/ regions except the last one.  for (uint i = first; i < last; ++i) {    hr = region_at(i);    hr->set_top(hr->end());  }  hr = region_at(last);  \/\/ If we cannot fit a filler object, we must set top to the end  \/\/ of the humongous object, otherwise we cannot iterate the heap  \/\/ and the BOT will not be complete.  hr->set_top(hr->end() - words_not_fillable);  assert(hr->bottom() < obj_top && obj_top <= hr->end(),         \"obj_top should be in last region\");  assert(words_not_fillable == 0 ||         first_hr->bottom() + word_size_sum - words_not_fillable == hr->top(),         \"Miscalculation in humongous allocation\");}HeapWord*G1CollectedHeap::humongous_obj_allocate_initialize_regions(G1HeapRegion* first_hr,                                                           uint num_regions,                                                           size_t word_size) {  assert(first_hr != nullptr, \"pre-condition\");  assert(is_humongous(word_size), \"word_size should be humongous\");  assert(num_regions * G1HeapRegion::GrainWords >= word_size, \"pre-condition\");  \/\/ Index of last region in the series.  uint first = first_hr->hrm_index();  uint last = first + num_regions - 1;  \/\/ We need to initialize the region(s) we just discovered. This is  \/\/ a bit tricky given that it can happen concurrently with  \/\/ refinement threads refining cards on these regions and  \/\/ potentially wanting to refine the BOT as they are scanning  \/\/ those cards (this can happen shortly after a cleanup; see CR  \/\/ 6991377). So we have to set up the region(s) carefully and in  \/\/ a specific order.  \/\/ The passed in hr will be the \"starts humongous\" region. The header  \/\/ of the new object will be placed at the bottom of this region.  HeapWord* new_obj = first_hr->bottom();  \/\/ First, we need to zero the header of the space that we will be  \/\/ allocating. When we update top further down, some refinement  \/\/ threads might try to scan the region. By zeroing the header we  \/\/ ensure that any thread that will try to scan the region will  \/\/ come across the zero klass word and bail out.  \/\/  \/\/ NOTE: It would not have been correct to have used  \/\/ CollectedHeap::fill_with_object() and make the space look like  \/\/ an int array. The thread that is doing the allocation will  \/\/ later update the object header to a potentially different array  \/\/ type and, for a very short period of time, the klass and length  \/\/ fields will be inconsistent. This could cause a refinement  \/\/ thread to calculate the object size incorrectly.  Copy::fill_to_words(new_obj, oopDesc::header_size(), 0);  \/\/ Next, update the metadata for the regions.  set_humongous_metadata(first_hr, num_regions, word_size, true);  G1HeapRegion* last_hr = region_at(last);  size_t used = byte_size(first_hr->bottom(), last_hr->top());  increase_used(used);  for (uint i = first; i <= last; ++i) {    G1HeapRegion *hr = region_at(i);    _humongous_set.add(hr);    G1HeapRegionPrinter::alloc(hr);  }  return new_obj;}size_t G1CollectedHeap::humongous_obj_size_in_regions(size_t word_size) {  assert(is_humongous(word_size), \"Object of size %zu must be humongous here\", word_size);  return align_up(word_size, G1HeapRegion::GrainWords) \/ G1HeapRegion::GrainWords;}\/\/ If could fit into free regions w\/o expansion, try.\/\/ Otherwise, if can expand, do so.\/\/ Otherwise, if using ex regions might help, try with ex given back.HeapWord* G1CollectedHeap::humongous_obj_allocate(size_t word_size) {  assert_heap_locked_or_at_safepoint(true \/* should_be_vm_thread *\/);  _verifier->verify_region_sets_optional();  uint obj_regions = (uint) humongous_obj_size_in_regions(word_size);  if (obj_regions > num_available_regions()) {    \/\/ Can't satisfy this allocation; early-return.    return nullptr;  }  \/\/ Policy: First try to allocate a humongous object in the free list.  G1HeapRegion* humongous_start = _hrm.allocate_humongous(obj_regions);  if (humongous_start == nullptr) {    \/\/ Policy: We could not find enough regions for the humongous object in the    \/\/ free list. Look through the heap to find a mix of free and uncommitted regions.    \/\/ If so, expand the heap and allocate the humongous object.    humongous_start = _hrm.expand_and_allocate_humongous(obj_regions);    if (humongous_start != nullptr) {      \/\/ We managed to find a region by expanding the heap.      log_debug(gc, ergo, heap)(\"Heap expansion (humongous allocation request). Allocation request: %zuB\",                                word_size * HeapWordSize);      policy()->record_new_heap_size(num_committed_regions());    } else {      \/\/ Policy: Potentially trigger a defragmentation GC.    }  }  HeapWord* result = nullptr;  if (humongous_start != nullptr) {    result = humongous_obj_allocate_initialize_regions(humongous_start, obj_regions, word_size);    assert(result != nullptr, \"it should always return a valid result\");    \/\/ A successful humongous object allocation changes the used space    \/\/ information of the old generation so we need to recalculate the    \/\/ sizes and update the jstat counters here.    monitoring_support()->update_sizes();  }  _verifier->verify_region_sets_optional();  return result;}HeapWord* G1CollectedHeap::allocate_new_tlab(size_t min_size,                                             size_t requested_size,                                             size_t* actual_size) {  assert_heap_not_locked_and_not_at_safepoint();  assert(!is_humongous(requested_size), \"we do not allow humongous TLABs\");  return attempt_allocation(min_size, requested_size, actual_size);}HeapWord*G1CollectedHeap::mem_allocate(size_t word_size,                              bool*  gc_overhead_limit_was_exceeded) {  assert_heap_not_locked_and_not_at_safepoint();  if (is_humongous(word_size)) {    return attempt_allocation_humongous(word_size);  }  size_t dummy = 0;  return attempt_allocation(word_size, word_size, &dummy);}HeapWord* G1CollectedHeap::attempt_allocation_slow(uint node_index, size_t word_size) {  ResourceMark rm; \/\/ For retrieving the thread names in log messages.  \/\/ Make sure you read the note in attempt_allocation_humongous().  assert_heap_not_locked_and_not_at_safepoint();  assert(!is_humongous(word_size), \"attempt_allocation_slow() should not \"         \"be called for humongous allocation requests\");  \/\/ We should only get here after the first-level allocation attempt  \/\/ (attempt_allocation()) failed to allocate.  \/\/ We will loop until a) we manage to successfully perform the allocation or b)  \/\/ successfully schedule a collection which fails to perform the allocation.  \/\/ Case b) is the only case when we'll return null.  HeapWord* result = nullptr;  for (uint try_count = 1; \/* we'll return *\/; try_count++) {    uint gc_count_before;    {      MutexLocker x(Heap_lock);      \/\/ Now that we have the lock, we first retry the allocation in case another      \/\/ thread changed the region while we were waiting to acquire the lock.      result = _allocator->attempt_allocation_locked(node_index, word_size);      if (result != nullptr) {        return result;      }      \/\/ Read the GC count while still holding the Heap_lock.      gc_count_before = total_collections();    }    bool succeeded;    result = do_collection_pause(word_size, gc_count_before, &succeeded, GCCause::_g1_inc_collection_pause);    if (succeeded) {      log_trace(gc, alloc)(\"%s: Successfully scheduled collection returning \" PTR_FORMAT,                           Thread::current()->name(), p2i(result));      return result;    }    log_trace(gc, alloc)(\"%s: Unsuccessfully scheduled collection allocating %zu words\",                         Thread::current()->name(), word_size);    \/\/ We can reach here if we were unsuccessful in scheduling a collection (because    \/\/ another thread beat us to it). In this case immeditealy retry the allocation    \/\/ attempt because another thread successfully performed a collection and possibly    \/\/ reclaimed enough space. The first attempt (without holding the Heap_lock) is    \/\/ here and the follow-on attempt will be at the start of the next loop    \/\/ iteration (after taking the Heap_lock).    size_t dummy = 0;    result = _allocator->attempt_allocation(node_index, word_size, word_size, &dummy);    if (result != nullptr) {      return result;    }    \/\/ Give a warning if we seem to be looping forever.    if ((QueuedAllocationWarningCount > 0) &&        (try_count % QueuedAllocationWarningCount == 0)) {      log_warning(gc, alloc)(\"%s:  Retried allocation %u times for %zu words\",                             Thread::current()->name(), try_count, word_size);    }  }  ShouldNotReachHere();  return nullptr;}template <typename Func>void G1CollectedHeap::iterate_regions_in_range(MemRegion range, const Func& func) {  \/\/ Mark each G1 region touched by the range as old, add it to  \/\/ the old set, and set top.  G1HeapRegion* curr_region = _hrm.addr_to_region(range.start());  G1HeapRegion* end_region = _hrm.addr_to_region(range.last());  while (curr_region != nullptr) {    bool is_last = curr_region == end_region;    G1HeapRegion* next_region = is_last ? nullptr : _hrm.next_region_in_heap(curr_region);    func(curr_region, is_last);    curr_region = next_region;  }}HeapWord* G1CollectedHeap::alloc_archive_region(size_t word_size, HeapWord* preferred_addr) {  assert(!is_init_completed(), \"Expect to be called at JVM init time\");  MutexLocker x(Heap_lock);  MemRegion reserved = _hrm.reserved();  if (reserved.word_size() <= word_size) {    log_info(gc, heap)(\"Unable to allocate regions as archive heap is too large; size requested = %zu\"                       \" bytes, heap = %zu bytes\", word_size * HeapWordSize, reserved.byte_size());    return nullptr;  }  \/\/ Temporarily disable pretouching of heap pages. This interface is used  \/\/ when mmap'ing archived heap data in, so pre-touching is wasted.  FlagSetting fs(AlwaysPreTouch, false);  size_t commits = 0;  \/\/ Attempt to allocate towards the end of the heap.  HeapWord* start_addr = reserved.end() - align_up(word_size, G1HeapRegion::GrainWords);  MemRegion range = MemRegion(start_addr, word_size);  HeapWord* last_address = range.last();  if (!_hrm.allocate_containing_regions(range, &commits, workers())) {    return nullptr;  }  increase_used(word_size * HeapWordSize);  if (commits != 0) {    log_debug(gc, ergo, heap)(\"Attempt heap expansion (allocate archive regions). Total size: %zuB\",                              G1HeapRegion::GrainWords * HeapWordSize * commits);  }  \/\/ Mark each G1 region touched by the range as old, add it to  \/\/ the old set, and set top.  auto set_region_to_old = [&] (G1HeapRegion* r, bool is_last) {    assert(r->is_empty(), \"Region already in use (%u)\", r->hrm_index());    HeapWord* top = is_last ? last_address + 1 : r->end();    r->set_top(top);    r->set_old();    G1HeapRegionPrinter::alloc(r);    _old_set.add(r);  };  iterate_regions_in_range(range, set_region_to_old);  return start_addr;}void G1CollectedHeap::populate_archive_regions_bot(MemRegion range) {  assert(!is_init_completed(), \"Expect to be called at JVM init time\");  iterate_regions_in_range(range,                           [&] (G1HeapRegion* r, bool is_last) {                             r->update_bot();                           });}void G1CollectedHeap::dealloc_archive_regions(MemRegion range) {  assert(!is_init_completed(), \"Expect to be called at JVM init time\");  MemRegion reserved = _hrm.reserved();  size_t size_used = 0;  \/\/ Free the G1 regions that are within the specified range.  MutexLocker x(Heap_lock);  HeapWord* start_address = range.start();  HeapWord* last_address = range.last();  assert(reserved.contains(start_address) && reserved.contains(last_address),         \"MemRegion outside of heap [\" PTR_FORMAT \", \" PTR_FORMAT \"]\",         p2i(start_address), p2i(last_address));  size_used += range.byte_size();  uint max_shrink_count = 0;  if (capacity() > MinHeapSize) {    size_t max_shrink_bytes = capacity() - MinHeapSize;    max_shrink_count = (uint)(max_shrink_bytes \/ G1HeapRegion::GrainBytes);  }  uint shrink_count = 0;  \/\/ Free, empty and uncommit regions with CDS archive content.  auto dealloc_archive_region = [&] (G1HeapRegion* r, bool is_last) {    guarantee(r->is_old(), \"Expected old region at index %u\", r->hrm_index());    _old_set.remove(r);    r->set_free();    r->set_top(r->bottom());    if (shrink_count < max_shrink_count) {      _hrm.shrink_at(r->hrm_index(), 1);      shrink_count++;    } else {      _hrm.insert_into_free_list(r);    }  };  iterate_regions_in_range(range, dealloc_archive_region);  if (shrink_count != 0) {    log_debug(gc, ergo, heap)(\"Attempt heap shrinking (CDS archive regions). Total size: %zuB (%u Regions)\",                              G1HeapRegion::GrainWords * HeapWordSize * shrink_count, shrink_count);    \/\/ Explicit uncommit.    uncommit_regions(shrink_count);  }  decrease_used(size_used);}inline HeapWord* G1CollectedHeap::attempt_allocation(size_t min_word_size,                                                     size_t desired_word_size,                                                     size_t* actual_word_size) {  assert_heap_not_locked_and_not_at_safepoint();  assert(!is_humongous(desired_word_size), \"attempt_allocation() should not \"         \"be called for humongous allocation requests\");  \/\/ Fix NUMA node association for the duration of this allocation  const uint node_index = _allocator->current_node_index();  HeapWord* result = _allocator->attempt_allocation(node_index, min_word_size, desired_word_size, actual_word_size);  if (result == nullptr) {    *actual_word_size = desired_word_size;    result = attempt_allocation_slow(node_index, desired_word_size);  }  assert_heap_not_locked();  if (result != nullptr) {    assert(*actual_word_size != 0, \"Actual size must have been set here\");    dirty_young_block(result, *actual_word_size);  } else {    *actual_word_size = 0;  }  return result;}HeapWord* G1CollectedHeap::attempt_allocation_humongous(size_t word_size) {  ResourceMark rm; \/\/ For retrieving the thread names in log messages.  \/\/ The structure of this method has a lot of similarities to  \/\/ attempt_allocation_slow(). The reason these two were not merged  \/\/ into a single one is that such a method would require several \"if  \/\/ allocation is not humongous do this, otherwise do that\"  \/\/ conditional paths which would obscure its flow. In fact, an early  \/\/ version of this code did use a unified method which was harder to  \/\/ follow and, as a result, it had subtle bugs that were hard to  \/\/ track down. So keeping these two methods separate allows each to  \/\/ be more readable. It will be good to keep these two in sync as  \/\/ much as possible.  assert_heap_not_locked_and_not_at_safepoint();  assert(is_humongous(word_size), \"attempt_allocation_humongous() \"         \"should only be called for humongous allocations\");  \/\/ Humongous objects can exhaust the heap quickly, so we should check if we  \/\/ need to start a marking cycle at each humongous object allocation. We do  \/\/ the check before we do the actual allocation. The reason for doing it  \/\/ before the allocation is that we avoid having to keep track of the newly  \/\/ allocated memory while we do a GC.  if (policy()->need_to_start_conc_mark(\"concurrent humongous allocation\",                                        word_size)) {    collect(GCCause::_g1_humongous_allocation);  }  \/\/ We will loop until a) we manage to successfully perform the allocation or b)  \/\/ successfully schedule a collection which fails to perform the allocation.  \/\/ Case b) is the only case when we'll return null.  HeapWord* result = nullptr;  for (uint try_count = 1; \/* we'll return *\/; try_count++) {    uint gc_count_before;    {      MutexLocker x(Heap_lock);      size_t size_in_regions = humongous_obj_size_in_regions(word_size);      \/\/ Given that humongous objects are not allocated in young      \/\/ regions, we'll first try to do the allocation without doing a      \/\/ collection hoping that there's enough space in the heap.      result = humongous_obj_allocate(word_size);      if (result != nullptr) {        policy()->old_gen_alloc_tracker()->          add_allocated_humongous_bytes_since_last_gc(size_in_regions * G1HeapRegion::GrainBytes);        return result;      }      \/\/ Read the GC count while still holding the Heap_lock.      gc_count_before = total_collections();    }    bool succeeded;    result = do_collection_pause(word_size, gc_count_before, &succeeded, GCCause::_g1_humongous_allocation);    if (succeeded) {      log_trace(gc, alloc)(\"%s: Successfully scheduled collection returning \" PTR_FORMAT,                           Thread::current()->name(), p2i(result));      if (result != nullptr) {        size_t size_in_regions = humongous_obj_size_in_regions(word_size);        policy()->old_gen_alloc_tracker()->          record_collection_pause_humongous_allocation(size_in_regions * G1HeapRegion::GrainBytes);      }      return result;    }    log_trace(gc, alloc)(\"%s: Unsuccessfully scheduled collection allocating %zu\",                         Thread::current()->name(), word_size);    \/\/ We can reach here if we were unsuccessful in scheduling a collection (because    \/\/ another thread beat us to it).    \/\/ Humongous object allocation always needs a lock, so we wait for the retry    \/\/ in the next iteration of the loop, unlike for the regular iteration case.    \/\/ Give a warning if we seem to be looping forever.    if ((QueuedAllocationWarningCount > 0) &&        (try_count % QueuedAllocationWarningCount == 0)) {      log_warning(gc, alloc)(\"%s: Retried allocation %u times for %zu words\",                             Thread::current()->name(), try_count, word_size);    }  }  ShouldNotReachHere();  return nullptr;}HeapWord* G1CollectedHeap::attempt_allocation_at_safepoint(size_t word_size,                                                           bool expect_null_mutator_alloc_region) {  assert_at_safepoint_on_vm_thread();  assert(!_allocator->has_mutator_alloc_region() || !expect_null_mutator_alloc_region,         \"the current alloc region was unexpectedly found to be non-null\");  \/\/ Fix NUMA node association for the duration of this allocation  const uint node_index = _allocator->current_node_index();  if (!is_humongous(word_size)) {    return _allocator->attempt_allocation_locked(node_index, word_size);  } else {    HeapWord* result = humongous_obj_allocate(word_size);    if (result != nullptr && policy()->need_to_start_conc_mark(\"STW humongous allocation\")) {      collector_state()->set_initiate_conc_mark_if_possible(true);    }    return result;  }  ShouldNotReachHere();}class PostCompactionPrinterClosure: public G1HeapRegionClosure {public:  bool do_heap_region(G1HeapRegion* hr) {    assert(!hr->is_young(), \"not expecting to find young regions\");    G1HeapRegionPrinter::post_compaction(hr);    return false;  }};void G1CollectedHeap::print_heap_after_full_collection() {  \/\/ Post collection region logging.  \/\/ We should do this after we potentially resize the heap so  \/\/ that all the COMMIT \/ UNCOMMIT events are generated before  \/\/ the compaction events.  if (G1HeapRegionPrinter::is_active()) {    PostCompactionPrinterClosure cl;    heap_region_iterate(&cl);  }}bool G1CollectedHeap::abort_concurrent_cycle() {  \/\/ Disable discovery and empty the discovered lists  \/\/ for the CM ref processor.  _ref_processor_cm->disable_discovery();  _ref_processor_cm->abandon_partial_discovery();  _ref_processor_cm->verify_no_references_recorded();  \/\/ Abandon current iterations of concurrent marking and concurrent  \/\/ refinement, if any are in progress.  return concurrent_mark()->concurrent_cycle_abort();}void G1CollectedHeap::prepare_heap_for_full_collection() {  \/\/ Make sure we'll choose a new allocation region afterwards.  _allocator->release_mutator_alloc_regions();  _allocator->abandon_gc_alloc_regions();  \/\/ We may have added regions to the current incremental collection  \/\/ set between the last GC or pause and now. We need to clear the  \/\/ incremental collection set and then start rebuilding it afresh  \/\/ after this full GC.  abandon_collection_set(collection_set());  _hrm.remove_all_free_regions();}void G1CollectedHeap::verify_before_full_collection() {  assert_used_and_recalculate_used_equal(this);  if (!VerifyBeforeGC) {    return;  }  if (!G1HeapVerifier::should_verify(G1HeapVerifier::G1VerifyFull)) {    return;  }  _verifier->verify_region_sets_optional();  _verifier->verify_before_gc();  _verifier->verify_bitmap_clear(true \/* above_tams_only *\/);}void G1CollectedHeap::prepare_for_mutator_after_full_collection(size_t allocation_word_size) {  \/\/ Prepare heap for normal collections.  assert(num_free_regions() == 0, \"we should not have added any free regions\");  rebuild_region_sets(false \/* free_list_only *\/);  abort_refinement();  resize_heap_after_full_collection(allocation_word_size);  \/\/ Rebuild the code root lists for each region  rebuild_code_roots();  start_new_collection_set();  _allocator->init_mutator_alloc_regions();  \/\/ Post collection state updates.  MetaspaceGC::compute_new_size();}void G1CollectedHeap::abort_refinement() {  \/\/ Discard all remembered set updates and reset refinement statistics.  G1BarrierSet::dirty_card_queue_set().abandon_logs_and_stats();  assert(G1BarrierSet::dirty_card_queue_set().num_cards() == 0,         \"DCQS should be empty\");  concurrent_refine()->get_and_reset_refinement_stats();}void G1CollectedHeap::verify_after_full_collection() {  if (!VerifyAfterGC) {    return;  }  if (!G1HeapVerifier::should_verify(G1HeapVerifier::G1VerifyFull)) {    return;  }  _hrm.verify_optional();  _verifier->verify_region_sets_optional();  _verifier->verify_after_gc();  _verifier->verify_bitmap_clear(false \/* above_tams_only *\/);  \/\/ At this point there should be no regions in the  \/\/ entire heap tagged as young.  assert(check_young_list_empty(), \"young list should be empty at this point\");  \/\/ Note: since we've just done a full GC, concurrent  \/\/ marking is no longer active. Therefore we need not  \/\/ re-enable reference discovery for the CM ref processor.  \/\/ That will be done at the start of the next marking cycle.  \/\/ We also know that the STW processor should no longer  \/\/ discover any new references.  assert(!_ref_processor_stw->discovery_enabled(), \"Postcondition\");  assert(!_ref_processor_cm->discovery_enabled(), \"Postcondition\");  _ref_processor_stw->verify_no_references_recorded();  _ref_processor_cm->verify_no_references_recorded();}void G1CollectedHeap::do_full_collection(bool clear_all_soft_refs,                                         bool do_maximal_compaction,                                         size_t allocation_word_size) {  assert_at_safepoint_on_vm_thread();  const bool do_clear_all_soft_refs = clear_all_soft_refs ||      soft_ref_policy()->should_clear_all_soft_refs();  G1FullGCMark gc_mark;  GCTraceTime(Info, gc) tm(\"Pause Full\", nullptr, gc_cause(), true);  G1FullCollector collector(this, do_clear_all_soft_refs, do_maximal_compaction, gc_mark.tracer());  collector.prepare_collection();  collector.collect();  collector.complete_collection(allocation_word_size);}void G1CollectedHeap::do_full_collection(bool clear_all_soft_refs) {  \/\/ Currently, there is no facility in the do_full_collection(bool) API to notify  \/\/ the caller that the collection did not succeed (e.g., because it was locked  \/\/ out by the GC locker). So, right now, we'll ignore the return value.  do_full_collection(clear_all_soft_refs,                     false \/* do_maximal_compaction *\/,                     size_t(0) \/* allocation_word_size *\/);}void G1CollectedHeap::upgrade_to_full_collection() {  GCCauseSetter compaction(this, GCCause::_g1_compaction_pause);  log_info(gc, ergo)(\"Attempting full compaction clearing soft references\");  do_full_collection(true  \/* clear_all_soft_refs *\/,                     false \/* do_maximal_compaction *\/,                     size_t(0) \/* allocation_word_size *\/);}void G1CollectedHeap::resize_heap(size_t resize_bytes, bool should_expand) {  if (should_expand) {    expand(resize_bytes, _workers);  } else {    shrink(resize_bytes);    uncommit_regions_if_necessary();  }}void G1CollectedHeap::resize_heap_after_full_collection(size_t allocation_word_size) {  assert_at_safepoint_on_vm_thread();  bool should_expand;  size_t resize_bytes = _heap_sizing_policy->full_collection_resize_amount(should_expand, allocation_word_size);  if (resize_bytes != 0) {    resize_heap(resize_bytes, should_expand);  }}void G1CollectedHeap::resize_heap_after_young_collection(size_t allocation_word_size) {  Ticks start = Ticks::now();  bool should_expand;  size_t resize_bytes = _heap_sizing_policy->young_collection_resize_amount(should_expand, allocation_word_size);  if (resize_bytes != 0) {    resize_heap(resize_bytes, should_expand);  }  phase_times()->record_resize_heap_time((Ticks::now() - start).seconds() * 1000.0);}HeapWord* G1CollectedHeap::satisfy_failed_allocation_helper(size_t word_size,                                                            bool do_gc,                                                            bool maximal_compaction,                                                            bool expect_null_mutator_alloc_region) {  \/\/ Let's attempt the allocation first.  HeapWord* result =    attempt_allocation_at_safepoint(word_size,                                    expect_null_mutator_alloc_region);  if (result != nullptr) {    return result;  }  \/\/ In a G1 heap, we're supposed to keep allocation from failing by  \/\/ incremental pauses.  Therefore, at least for now, we'll favor  \/\/ expansion over collection.  (This might change in the future if we can  \/\/ do something smarter than full collection to satisfy a failed alloc.)  result = expand_and_allocate(word_size);  if (result != nullptr) {    return result;  }  if (do_gc) {    GCCauseSetter compaction(this, GCCause::_g1_compaction_pause);    \/\/ Expansion didn't work, we'll try to do a Full GC.    \/\/ If maximal_compaction is set we clear all soft references and don't    \/\/ allow any dead wood to be left on the heap.    if (maximal_compaction) {      log_info(gc, ergo)(\"Attempting maximal full compaction clearing soft references\");    } else {      log_info(gc, ergo)(\"Attempting full compaction\");    }    do_full_collection(maximal_compaction \/* clear_all_soft_refs *\/,                       maximal_compaction \/* do_maximal_compaction *\/,                       word_size \/* allocation_word_size *\/);  }  return nullptr;}HeapWord* G1CollectedHeap::satisfy_failed_allocation(size_t word_size) {  assert_at_safepoint_on_vm_thread();  \/\/ Attempts to allocate followed by Full GC.  HeapWord* result =    satisfy_failed_allocation_helper(word_size,                                     true,  \/* do_gc *\/                                     false, \/* maximum_collection *\/                                     false \/* expect_null_mutator_alloc_region *\/);  if (result != nullptr) {    return result;  }  \/\/ Attempts to allocate followed by Full GC that will collect all soft references.  result = satisfy_failed_allocation_helper(word_size,                                            true, \/* do_gc *\/                                            true, \/* maximum_collection *\/                                            true \/* expect_null_mutator_alloc_region *\/);  if (result != nullptr) {    return result;  }  \/\/ Attempts to allocate, no GC  result = satisfy_failed_allocation_helper(word_size,                                            false, \/* do_gc *\/                                            false, \/* maximum_collection *\/                                            true  \/* expect_null_mutator_alloc_region *\/);  if (result != nullptr) {    return result;  }  assert(!soft_ref_policy()->should_clear_all_soft_refs(),         \"Flag should have been handled and cleared prior to this point\");  \/\/ What else?  We might try synchronous finalization later.  If the total  \/\/ space available is large enough for the allocation, then a more  \/\/ complete compaction phase than we've tried so far might be  \/\/ appropriate.  return nullptr;}\/\/ Attempting to expand the heap sufficiently\/\/ to support an allocation of the given \"word_size\".  If\/\/ successful, perform the allocation and return the address of the\/\/ allocated block, or else null.HeapWord* G1CollectedHeap::expand_and_allocate(size_t word_size) {  assert_at_safepoint_on_vm_thread();  _verifier->verify_region_sets_optional();  size_t expand_bytes = MAX2(word_size * HeapWordSize, MinHeapDeltaBytes);  log_debug(gc, ergo, heap)(\"Attempt heap expansion (allocation request failed). Allocation request: %zuB\",                            word_size * HeapWordSize);  if (expand(expand_bytes, _workers)) {    _hrm.verify_optional();    _verifier->verify_region_sets_optional();    return attempt_allocation_at_safepoint(word_size,                                           false \/* expect_null_mutator_alloc_region *\/);  }  return nullptr;}bool G1CollectedHeap::expand(size_t expand_bytes, WorkerThreads* pretouch_workers) {  assert(expand_bytes > 0, \"precondition\");  size_t aligned_expand_bytes = os::align_up_vm_page_size(expand_bytes);  aligned_expand_bytes = align_up(aligned_expand_bytes, G1HeapRegion::GrainBytes);  uint num_regions_to_expand = (uint)(aligned_expand_bytes \/ G1HeapRegion::GrainBytes);  log_debug(gc, ergo, heap)(\"Heap resize. Requested expansion amount: %zuB aligned expansion amount: %zuB (%u regions)\",                            expand_bytes, aligned_expand_bytes, num_regions_to_expand);  if (num_inactive_regions() == 0) {    log_debug(gc, ergo, heap)(\"Heap resize. Did not expand the heap (heap already fully expanded)\");    return false;  }  uint expanded_by = _hrm.expand_by(num_regions_to_expand, pretouch_workers);  size_t actual_expand_bytes = expanded_by * G1HeapRegion::GrainBytes;  assert(actual_expand_bytes <= aligned_expand_bytes, \"post-condition\");  policy()->record_new_heap_size(num_committed_regions());  return true;}bool G1CollectedHeap::expand_single_region(uint node_index) {  uint expanded_by = _hrm.expand_on_preferred_node(node_index);  if (expanded_by == 0) {    assert(num_inactive_regions() == 0, \"Should be no regions left, available: %u\", num_inactive_regions());    log_debug(gc, ergo, heap)(\"Did not expand the heap (heap already fully expanded)\");    return false;  }  policy()->record_new_heap_size(num_committed_regions());  return true;}void G1CollectedHeap::shrink_helper(size_t shrink_bytes) {  assert(shrink_bytes > 0, \"must be\");  assert(is_aligned(shrink_bytes, G1HeapRegion::GrainBytes),         \"Shrink request for %zuB not aligned to heap region size %zuB\",         shrink_bytes, G1HeapRegion::GrainBytes);  uint num_regions_to_remove = (uint)(shrink_bytes \/ G1HeapRegion::GrainBytes);  uint num_regions_removed = _hrm.shrink_by(num_regions_to_remove);  size_t shrunk_bytes = num_regions_removed * G1HeapRegion::GrainBytes;  log_debug(gc, ergo, heap)(\"Shrink the heap. requested shrinking amount: %zuB aligned shrinking amount: %zuB attempted shrinking amount: %zuB\",                           shrink_bytes, aligned_shrink_bytes, shrunk_bytes);  if (num_regions_removed > 0) {    log_info(gc, heap)(\"Heap shrink completed: uncommitted %u regions (%zuMB), heap size now %zuMB\",                       num_regions_removed, shrunk_bytes \/ M, capacity() \/ M);    log_debug(gc, heap)(\"Heap shrink details: requested=%zuB aligned=%zuB attempted=%zuB actual=%zuB \"                        \"regions_removed=%u heap_capacity=%zuB\",                        shrink_bytes, aligned_shrink_bytes, num_regions_to_remove * G1HeapRegion::GrainBytes,                        shrunk_bytes, num_regions_removed, capacity());    policy()->record_new_heap_size(num_committed_regions());  } else {    log_debug(gc, ergo, heap)(\"Heap resize. Did not shrink the heap (heap shrinking operation failed)\");  }}void G1CollectedHeap::shrink(size_t shrink_bytes) {  if (capacity() == min_capacity()) {    log_debug(gc, ergo, heap)(\"Heap resize. Did not shrink the heap (heap already at minimum)\");    return;  }  size_t aligned_shrink_bytes = os::align_down_vm_page_size(shrink_bytes);  aligned_shrink_bytes = align_down(aligned_shrink_bytes, G1HeapRegion::GrainBytes);  aligned_shrink_bytes = capacity() - MAX2(capacity() - aligned_shrink_bytes, min_capacity());  assert(is_aligned(aligned_shrink_bytes, G1HeapRegion::GrainBytes), \"Bytes to shrink %zuB not aligned\", aligned_shrink_bytes);  log_debug(gc, ergo, heap)(\"Heap resize. Requested shrink amount: %zuB aligned shrink amount: %zuB\",                            shrink_bytes, aligned_shrink_bytes);  if (aligned_shrink_bytes == 0) {    log_debug(gc, ergo, heap)(\"Heap resize. Did not shrink the heap (shrink request too small)\");    return;  }  _verifier->verify_region_sets_optional();  \/\/ We should only reach here at the end of a Full GC or during Remark which  \/\/ means we should not not be holding to any GC alloc regions. The method  \/\/ below will make sure of that and do any remaining clean up.  _allocator->abandon_gc_alloc_regions();  \/\/ Instead of tearing down \/ rebuilding the free lists here, we  \/\/ could instead use the remove_all_pending() method on free_list to  \/\/ remove only the ones that we need to remove.  _hrm.remove_all_free_regions();  shrink_helper(aligned_shrink_bytes);  rebuild_region_sets(true \/* free_list_only *\/);  _hrm.verify_optional();  _verifier->verify_region_sets_optional();}bool G1CollectedHeap::request_heap_shrink(size_t shrink_bytes) {  if (shrink_bytes == 0) {    return false;  }  \/\/ Fast path: if we are already at a safepoint (e.g. called from the  \/\/ GC service thread) just do the work directly.  if (SafepointSynchronize::is_at_safepoint()) {    shrink(shrink_bytes);    return true;                     \/\/ we *did* something  }  \/\/ Schedule a small VM-op so the work is done at the next safepoint  VM_G1ShrinkHeap op(this, shrink_bytes);  VMThread::execute(&op);  return true;                       \/\/ pages were at least *requested* to be released}class OldRegionSetChecker : public G1HeapRegionSetChecker {public:  void check_mt_safety() {    \/\/ Master Old Set MT safety protocol:    \/\/ (a) If we're at a safepoint, operations on the master old set    \/\/ should be invoked:    \/\/ - by the VM thread (which will serialize them), or    \/\/ - by the GC workers while holding the FreeList_lock, if we're    \/\/   at a safepoint for an evacuation pause (this lock is taken    \/\/   anyway when an GC alloc region is retired so that a new one    \/\/   is allocated from the free list), or    \/\/ - by the GC workers while holding the OldSets_lock, if we're at a    \/\/   safepoint for a cleanup pause.    \/\/ (b) If we're not at a safepoint, operations on the master old set    \/\/ should be invoked while holding the Heap_lock.    if (SafepointSynchronize::is_at_safepoint()) {      guarantee(Thread::current()->is_VM_thread() ||                FreeList_lock->owned_by_self() || OldSets_lock->owned_by_self(),                \"master old set MT safety protocol at a safepoint\");    } else {      guarantee(Heap_lock->owned_by_self(), \"master old set MT safety protocol outside a safepoint\");    }  }  bool is_correct_type(G1HeapRegion* hr) { return hr->is_old(); }  const char* get_description() { return \"Old Regions\"; }};class HumongousRegionSetChecker : public G1HeapRegionSetChecker {public:  void check_mt_safety() {    \/\/ Humongous Set MT safety protocol:    \/\/ (a) If we're at a safepoint, operations on the master humongous    \/\/ set should be invoked by either the VM thread (which will    \/\/ serialize them) or by the GC workers while holding the    \/\/ OldSets_lock.    \/\/ (b) If we're not at a safepoint, operations on the master    \/\/ humongous set should be invoked while holding the Heap_lock.    if (SafepointSynchronize::is_at_safepoint()) {      guarantee(Thread::current()->is_VM_thread() ||                OldSets_lock->owned_by_self(),                \"master humongous set MT safety protocol at a safepoint\");    } else {      guarantee(Heap_lock->owned_by_self(),                \"master humongous set MT safety protocol outside a safepoint\");    }  }  bool is_correct_type(G1HeapRegion* hr) { return hr->is_humongous(); }  const char* get_description() { return \"Humongous Regions\"; }};G1CollectedHeap::G1CollectedHeap() :  CollectedHeap(),  _service_thread(nullptr),  _periodic_gc_task(nullptr),  _free_arena_memory_task(nullptr),  _workers(nullptr),  _card_table(nullptr),  _collection_pause_end(Ticks::now()),  _old_set(\"Old Region Set\", new OldRegionSetChecker()),  _humongous_set(\"Humongous Region Set\", new HumongousRegionSetChecker()),  _bot(nullptr),  _listener(),  _numa(G1NUMA::create()),  _hrm(),  _allocator(nullptr),  _allocation_failure_injector(),  _verifier(nullptr),  _summary_bytes_used(0),  _bytes_used_during_gc(0),  _survivor_evac_stats(\"Young\", YoungPLABSize, PLABWeight),  _old_evac_stats(\"Old\", OldPLABSize, PLABWeight),  _monitoring_support(nullptr),  _num_humongous_objects(0),  _num_humongous_reclaim_candidates(0),  _collector_state(),  _old_marking_cycles_started(0),  _old_marking_cycles_completed(0),  _eden(),  _survivor(),  _gc_timer_stw(new STWGCTimer()),  _gc_tracer_stw(new G1NewTracer()),  _policy(new G1Policy(_gc_timer_stw)),  _heap_sizing_policy(nullptr),  _collection_set(this, _policy),  _rem_set(nullptr),  _card_set_config(),  _card_set_freelist_pool(G1CardSetConfiguration::num_mem_object_types()),  _young_regions_cset_group(card_set_config(), &_card_set_freelist_pool, 1u \/* group_id *\/),  _cm(nullptr),  _cm_thread(nullptr),  _cr(nullptr),  _task_queues(nullptr),  _partial_array_state_manager(nullptr),  _ref_processor_stw(nullptr),  _is_alive_closure_stw(this),  _is_subject_to_discovery_stw(this),  _ref_processor_cm(nullptr),  _is_alive_closure_cm(),  _is_subject_to_discovery_cm(this),  _region_attr() {  _heap_evaluation_task = nullptr;  _verifier = new G1HeapVerifier(this);  _allocator = new G1Allocator(this);  _heap_sizing_policy = G1HeapSizingPolicy::create(this, _policy->analytics());  _heap_sizing_policy->initialize();  _humongous_object_threshold_in_words = humongous_threshold_for(G1HeapRegion::GrainWords);  \/\/ Since filler arrays are never referenced, we can make them region sized.  \/\/ This simplifies filling up the region in case we have some potentially  \/\/ unreferenced (by Java code, but still in use by native code) pinned objects  \/\/ in there.  _filler_array_max_size = G1HeapRegion::GrainWords;  \/\/ Override the default _stack_chunk_max_size so that no humongous stack chunks are created  _stack_chunk_max_size = _humongous_object_threshold_in_words;  uint n_queues = ParallelGCThreads;  _task_queues = new G1ScannerTasksQueueSet(n_queues);  for (uint i = 0; i < n_queues; i++) {    G1ScannerTasksQueue* q = new G1ScannerTasksQueue();    _task_queues->register_queue(i, q);  }  _partial_array_state_manager = new PartialArrayStateManager(n_queues);  _gc_tracer_stw->initialize();}PartialArrayStateManager* G1CollectedHeap::partial_array_state_manager() const {  return _partial_array_state_manager;}G1RegionToSpaceMapper* G1CollectedHeap::create_aux_memory_mapper(const char* description,                                                                 size_t size,                                                                 size_t translation_factor) {  size_t preferred_page_size = os::page_size_for_region_unaligned(size, 1);  \/\/ When a page size is given we don't want to mix large  \/\/ and normal pages. If the size is not a multiple of the  \/\/ page size it will be aligned up to achieve this.  size_t alignment = os::vm_allocation_granularity();  if (preferred_page_size != os::vm_page_size()) {    alignment = MAX2(preferred_page_size, alignment);    size = align_up(size, alignment);  }  \/\/ Allocate a new reserved space, preferring to use large pages.  ReservedSpace rs = MemoryReserver::reserve(size,                                             alignment,                                             preferred_page_size,                                             mtGC);  size_t page_size = rs.page_size();  G1RegionToSpaceMapper* result  =    G1RegionToSpaceMapper::create_mapper(rs,                                         size,                                         page_size,                                         G1HeapRegion::GrainBytes,                                         translation_factor,                                         mtGC);  os::trace_page_sizes_for_requested_size(description,                                          size,                                          preferred_page_size,                                          rs.base(),                                          rs.size(),                                          page_size);  return result;}jint G1CollectedHeap::initialize_concurrent_refinement() {  jint ecode = JNI_OK;  _cr = G1ConcurrentRefine::create(policy(), &ecode);  return ecode;}jint G1CollectedHeap::initialize_service_thread() {  _service_thread = new G1ServiceThread();  if (_service_thread->osthread() == nullptr) {    vm_shutdown_during_initialization(\"Could not create G1ServiceThread\");    return JNI_ENOMEM;  }  return JNI_OK;}jint G1CollectedHeap::initialize() {  if (!os::is_thread_cpu_time_supported()) {    vm_exit_during_initialization(\"G1 requires cpu time gathering support\");  }  \/\/ Necessary to satisfy locking discipline assertions.  MutexLocker x(Heap_lock);  \/\/ While there are no constraints in the GC code that HeapWordSize  \/\/ be any particular value, there are multiple other areas in the  \/\/ system which believe this to be true (e.g. oop->object_size in some  \/\/ cases incorrectly returns the size in wordSize units rather than  \/\/ HeapWordSize).  guarantee(HeapWordSize == wordSize, \"HeapWordSize must equal wordSize\");  size_t init_byte_size = InitialHeapSize;  size_t reserved_byte_size = G1Arguments::heap_reserved_size_bytes();  \/\/ Ensure that the sizes are properly aligned.  Universe::check_alignment(init_byte_size, G1HeapRegion::GrainBytes, \"g1 heap\");  Universe::check_alignment(reserved_byte_size, G1HeapRegion::GrainBytes, \"g1 heap\");  Universe::check_alignment(reserved_byte_size, HeapAlignment, \"g1 heap\");  \/\/ Reserve the maximum.  \/\/ When compressed oops are enabled, the preferred heap base  \/\/ is calculated by subtracting the requested size from the  \/\/ 32Gb boundary and using the result as the base address for  \/\/ heap reservation. If the requested size is not aligned to  \/\/ G1HeapRegion::GrainBytes (i.e. the alignment that is passed  \/\/ into the ReservedHeapSpace constructor) then the actual  \/\/ base of the reserved heap may end up differing from the  \/\/ address that was requested (i.e. the preferred heap base).  \/\/ If this happens then we could end up using a non-optimal  \/\/ compressed oops mode.  ReservedHeapSpace heap_rs = Universe::reserve_heap(reserved_byte_size,                                                     HeapAlignment);  initialize_reserved_region(heap_rs);  \/\/ Create the barrier set for the entire reserved region.  G1CardTable* ct = new G1CardTable(_reserved);  G1BarrierSet* bs = new G1BarrierSet(ct);  bs->initialize();  assert(bs->is_a(BarrierSet::G1BarrierSet), \"sanity\");  BarrierSet::set_barrier_set(bs);  _card_table = ct;  {    G1SATBMarkQueueSet& satbqs = bs->satb_mark_queue_set();    satbqs.set_process_completed_buffers_threshold(G1SATBProcessCompletedThreshold);    satbqs.set_buffer_enqueue_threshold_percentage(G1SATBBufferEnqueueingThresholdPercent);  }  \/\/ Create space mappers.  size_t page_size = heap_rs.page_size();  G1RegionToSpaceMapper* heap_storage =    G1RegionToSpaceMapper::create_mapper(heap_rs,                                         heap_rs.size(),                                         page_size,                                         G1HeapRegion::GrainBytes,                                         1,                                         mtJavaHeap);  if(heap_storage == nullptr) {    vm_shutdown_during_initialization(\"Could not initialize G1 heap\");    return JNI_ERR;  }  os::trace_page_sizes(\"Heap\",                       min_capacity(),                       reserved_byte_size,                       heap_rs.base(),                       heap_rs.size(),                       page_size);  heap_storage->set_mapping_changed_listener(&_listener);  \/\/ Create storage for the BOT, card table and the bitmap.  G1RegionToSpaceMapper* bot_storage =    create_aux_memory_mapper(\"Block Offset Table\",                             G1BlockOffsetTable::compute_size(heap_rs.size() \/ HeapWordSize),                             G1BlockOffsetTable::heap_map_factor());  G1RegionToSpaceMapper* cardtable_storage =    create_aux_memory_mapper(\"Card Table\",                             G1CardTable::compute_size(heap_rs.size() \/ HeapWordSize),                             G1CardTable::heap_map_factor());  size_t bitmap_size = G1CMBitMap::compute_size(heap_rs.size());  G1RegionToSpaceMapper* bitmap_storage =    create_aux_memory_mapper(\"Mark Bitmap\", bitmap_size, G1CMBitMap::heap_map_factor());  _hrm.initialize(heap_storage, bitmap_storage, bot_storage, cardtable_storage);  _card_table->initialize(cardtable_storage);  \/\/ 6843694 - ensure that the maximum region index can fit  \/\/ in the remembered set structures.  const uint max_region_idx = (1U << (sizeof(RegionIdx_t)*BitsPerByte-1)) - 1;  guarantee((max_num_regions() - 1) <= max_region_idx, \"too many regions\");  \/\/ The G1FromCardCache reserves card with value 0 as \"invalid\", so the heap must not  \/\/ start within the first card.  guarantee((uintptr_t)(heap_rs.base()) >= G1CardTable::card_size(), \"Java heap must not start within the first card.\");  G1FromCardCache::initialize(max_num_regions());  \/\/ Also create a G1 rem set.  _rem_set = new G1RemSet(this, _card_table);  _rem_set->initialize(max_num_regions());  size_t max_cards_per_region = ((size_t)1 << (sizeof(CardIdx_t)*BitsPerByte-1)) - 1;  guarantee(G1HeapRegion::CardsPerRegion > 0, \"make sure it's initialized\");  guarantee(G1HeapRegion::CardsPerRegion < max_cards_per_region,            \"too many cards per region\");  G1HeapRegionRemSet::initialize(_reserved);  G1FreeRegionList::set_unrealistically_long_length(max_num_regions() + 1);  _bot = new G1BlockOffsetTable(reserved(), bot_storage);  {    size_t granularity = G1HeapRegion::GrainBytes;    _region_attr.initialize(reserved(), granularity);  }  _workers = new WorkerThreads(\"GC Thread\", ParallelGCThreads);  if (_workers == nullptr) {    return JNI_ENOMEM;  }  _workers->initialize_workers();  _numa->set_region_info(G1HeapRegion::GrainBytes, page_size);  \/\/ Create the G1ConcurrentMark data structure and thread.  \/\/ (Must do this late, so that \"max_[reserved_]regions\" is defined.)  _cm = new G1ConcurrentMark(this, bitmap_storage);  _cm_thread = _cm->cm_thread();  \/\/ Now expand into the initial heap size.  if (!expand(init_byte_size, _workers)) {    vm_shutdown_during_initialization(\"Failed to allocate initial heap.\");    return JNI_ENOMEM;  }  \/\/ Perform any initialization actions delegated to the policy.  policy()->init(this, &_collection_set);  jint ecode = initialize_concurrent_refinement();  if (ecode != JNI_OK) {    return ecode;  }  ecode = initialize_service_thread();  if (ecode != JNI_OK) {    return ecode;  }  \/\/ Create and schedule the periodic gc task on the service thread.  _periodic_gc_task = new G1PeriodicGCTask(\"Periodic GC Task\");  _service_thread->register_task(_periodic_gc_task);  _free_arena_memory_task = new G1MonotonicArenaFreeMemoryTask(\"Card Set Free Memory Task\");  _service_thread->register_task(_free_arena_memory_task);  \/\/ Create the heap evaluation task using PeriodicTask  if (G1UseTimeBasedHeapSizing) {    _heap_evaluation_task = new G1HeapEvaluationTask(this, _heap_sizing_policy);    \/\/ PeriodicTask will be enrolled after G1 is fully initialized in post_initialize()    log_debug(gc, init)(\"G1 Time-Based Heap Evaluation task created (PeriodicTask)\");  } else {    _heap_evaluation_task = nullptr;  }  \/\/ Here we allocate the dummy G1HeapRegion that is required by the  \/\/ G1AllocRegion class.  G1HeapRegion* dummy_region = _hrm.get_dummy_region();  \/\/ We'll re-use the same region whether the alloc region will  \/\/ require BOT updates or not and, if it doesn't, then a non-young  \/\/ region will complain that it cannot support allocations without  \/\/ BOT updates. So we'll tag the dummy region as eden to avoid that.  dummy_region->set_eden();  \/\/ Make sure it's full.  dummy_region->set_top(dummy_region->end());  G1AllocRegion::setup(this, dummy_region);  _allocator->init_mutator_alloc_regions();  \/\/ Do create of the monitoring and management support so that  \/\/ values in the heap have been properly initialized.  _monitoring_support = new G1MonitoringSupport(this);  _collection_set.initialize(max_num_regions());  allocation_failure_injector()->reset();  CPUTimeCounters::create_counter(CPUTimeGroups::CPUTimeType::gc_parallel_workers);  CPUTimeCounters::create_counter(CPUTimeGroups::CPUTimeType::gc_conc_mark);  CPUTimeCounters::create_counter(CPUTimeGroups::CPUTimeType::gc_conc_refine);  CPUTimeCounters::create_counter(CPUTimeGroups::CPUTimeType::gc_service);  G1InitLogger::print();  FullGCForwarding::initialize(_reserved);  return JNI_OK;}bool G1CollectedHeap::concurrent_mark_is_terminating() const {  return _cm_thread->should_terminate();}void G1CollectedHeap::stop() {  \/\/ Stop all concurrent threads. We do this to make sure these threads  \/\/ do not continue to execute and access resources (e.g. logging)  \/\/ that are destroyed during shutdown.  _cr->stop();  _service_thread->stop();  _cm_thread->stop();}void G1CollectedHeap::safepoint_synchronize_begin() {  SuspendibleThreadSet::synchronize();}void G1CollectedHeap::safepoint_synchronize_end() {  SuspendibleThreadSet::desynchronize();}void G1CollectedHeap::post_initialize() {  CollectedHeap::post_initialize();  ref_processing_init();  \/\/ Enroll the heap evaluation task after G1 is fully initialized  if (G1UseTimeBasedHeapSizing && _heap_evaluation_task != nullptr) {    _heap_evaluation_task->enroll();  \/\/ PeriodicTask enroll() starts the task    log_debug(gc, init)(\"G1 Time-Based Heap Evaluation task enrolled (PeriodicTask)\");  }}void G1CollectedHeap::ref_processing_init() {  \/\/ Reference processing in G1 currently works as follows:  \/\/  \/\/ * There are two reference processor instances. One is  \/\/   used to record and process discovered references  \/\/   during concurrent marking; the other is used to  \/\/   record and process references during STW pauses  \/\/   (both full and incremental).  \/\/ * Both ref processors need to 'span' the entire heap as  \/\/   the regions in the collection set may be dotted around.  \/\/  \/\/ * For the concurrent marking ref processor:  \/\/   * Reference discovery is enabled at concurrent start.  \/\/   * Reference discovery is disabled and the discovered  \/\/     references processed etc during remarking.  \/\/   * Reference discovery is MT (see below).  \/\/   * Reference discovery requires a barrier (see below).  \/\/   * Reference processing may or may not be MT  \/\/     (depending on the value of ParallelRefProcEnabled  \/\/     and ParallelGCThreads).  \/\/   * A full GC disables reference discovery by the CM  \/\/     ref processor and abandons any entries on it's  \/\/     discovered lists.  \/\/  \/\/ * For the STW processor:  \/\/   * Non MT discovery is enabled at the start of a full GC.  \/\/   * Processing and enqueueing during a full GC is non-MT.  \/\/   * During a full GC, references are processed after marking.  \/\/  \/\/   * Discovery (may or may not be MT) is enabled at the start  \/\/     of an incremental evacuation pause.  \/\/   * References are processed near the end of a STW evacuation pause.  \/\/   * For both types of GC:  \/\/     * Discovery is atomic - i.e. not concurrent.  \/\/     * Reference discovery will not need a barrier.  _is_alive_closure_cm.initialize(concurrent_mark());  \/\/ Concurrent Mark ref processor  _ref_processor_cm =    new ReferenceProcessor(&_is_subject_to_discovery_cm,                           ParallelGCThreads,                              \/\/ degree of mt processing                           \/\/ We discover with the gc worker threads during Remark, so both                           \/\/ thread counts must be considered for discovery.                           MAX2(ParallelGCThreads, ConcGCThreads),         \/\/ degree of mt discovery                           true,                                           \/\/ Reference discovery is concurrent                           &_is_alive_closure_cm);                         \/\/ is alive closure  \/\/ STW ref processor  _ref_processor_stw =    new ReferenceProcessor(&_is_subject_to_discovery_stw,                           ParallelGCThreads,                    \/\/ degree of mt processing                           ParallelGCThreads,                    \/\/ degree of mt discovery                           false,                                \/\/ Reference discovery is not concurrent                           &_is_alive_closure_stw);              \/\/ is alive closure}size_t G1CollectedHeap::capacity() const {  return _hrm.num_committed_regions() * G1HeapRegion::GrainBytes;}size_t G1CollectedHeap::unused_committed_regions_in_bytes() const {  return _hrm.total_free_bytes();}\/\/ Computes the sum of the storage used by the various regions.size_t G1CollectedHeap::used() const {  size_t result = _summary_bytes_used + _allocator->used_in_alloc_regions();  return result;}size_t G1CollectedHeap::used_unlocked() const {  return _summary_bytes_used;}class SumUsedClosure: public G1HeapRegionClosure {  size_t _used;public:  SumUsedClosure() : _used(0) {}  bool do_heap_region(G1HeapRegion* r) {    _used += r->used();    return false;  }  size_t result() { return _used; }};size_t G1CollectedHeap::recalculate_used() const {  SumUsedClosure blk;  heap_region_iterate(&blk);  return blk.result();}bool  G1CollectedHeap::is_user_requested_concurrent_full_gc(GCCause::Cause cause) {  return GCCause::is_user_requested_gc(cause) && ExplicitGCInvokesConcurrent;}bool G1CollectedHeap::should_do_concurrent_full_gc(GCCause::Cause cause) {  switch (cause) {    case GCCause::_g1_humongous_allocation: return true;    case GCCause::_g1_periodic_collection:  return G1PeriodicGCInvokesConcurrent;    case GCCause::_wb_breakpoint:           return true;    case GCCause::_codecache_GC_aggressive: return true;    case GCCause::_codecache_GC_threshold:  return true;    default:                                return is_user_requested_concurrent_full_gc(cause);  }}void G1CollectedHeap::increment_old_marking_cycles_started() {  assert(_old_marking_cycles_started == _old_marking_cycles_completed ||         _old_marking_cycles_started == _old_marking_cycles_completed + 1,         \"Wrong marking cycle count (started: %d, completed: %d)\",         _old_marking_cycles_started, _old_marking_cycles_completed);  _old_marking_cycles_started++;}void G1CollectedHeap::increment_old_marking_cycles_completed(bool concurrent,                                                             bool whole_heap_examined) {  MonitorLocker ml(G1OldGCCount_lock, Mutex::_no_safepoint_check_flag);  \/\/ We assume that if concurrent == true, then the caller is a  \/\/ concurrent thread that was joined the Suspendible Thread  \/\/ Set. If there's ever a cheap way to check this, we should add an  \/\/ assert here.  \/\/ Given that this method is called at the end of a Full GC or of a  \/\/ concurrent cycle, and those can be nested (i.e., a Full GC can  \/\/ interrupt a concurrent cycle), the number of full collections  \/\/ completed should be either one (in the case where there was no  \/\/ nesting) or two (when a Full GC interrupted a concurrent cycle)  \/\/ behind the number of full collections started.  \/\/ This is the case for the inner caller, i.e. a Full GC.  assert(concurrent ||         (_old_marking_cycles_started == _old_marking_cycles_completed + 1) ||         (_old_marking_cycles_started == _old_marking_cycles_completed + 2),         \"for inner caller (Full GC): _old_marking_cycles_started = %u \"         \"is inconsistent with _old_marking_cycles_completed = %u\",         _old_marking_cycles_started, _old_marking_cycles_completed);  \/\/ This is the case for the outer caller, i.e. the concurrent cycle.  assert(!concurrent ||         (_old_marking_cycles_started == _old_marking_cycles_completed + 1),         \"for outer caller (concurrent cycle): \"         \"_old_marking_cycles_started = %u \"         \"is inconsistent with _old_marking_cycles_completed = %u\",         _old_marking_cycles_started, _old_marking_cycles_completed);  _old_marking_cycles_completed += 1;  if (whole_heap_examined) {    \/\/ Signal that we have completed a visit to all live objects.    record_whole_heap_examined_timestamp();  }  \/\/ We need to clear the \"in_progress\" flag in the CM thread before  \/\/ we wake up any waiters (especially when ExplicitInvokesConcurrent  \/\/ is set) so that if a waiter requests another System.gc() it doesn't  \/\/ incorrectly see that a marking cycle is still in progress.  if (concurrent) {    _cm_thread->set_idle();  }  \/\/ Notify threads waiting in System.gc() (with ExplicitGCInvokesConcurrent)  \/\/ for a full GC to finish that their wait is over.  ml.notify_all();}\/\/ Helper for collect().static G1GCCounters collection_counters(G1CollectedHeap* g1h) {  MutexLocker ml(Heap_lock);  return G1GCCounters(g1h);}void G1CollectedHeap::collect(GCCause::Cause cause) {  try_collect(cause, collection_counters(this));}\/\/ Return true if (x < y) with allowance for wraparound.static bool gc_counter_less_than(uint x, uint y) {  return (x - y) > (UINT_MAX\/2);}\/\/ LOG_COLLECT_CONCURRENTLY(cause, msg, args...)\/\/ Macro so msg printing is format-checked.#define LOG_COLLECT_CONCURRENTLY(cause, ...)                            \\  do {                                                                  \\    LogTarget(Trace, gc) LOG_COLLECT_CONCURRENTLY_lt;                   \\    if (LOG_COLLECT_CONCURRENTLY_lt.is_enabled()) {                     \\      ResourceMark rm; \/* For thread name. *\/                           \\      LogStream LOG_COLLECT_CONCURRENTLY_s(&LOG_COLLECT_CONCURRENTLY_lt); \\      LOG_COLLECT_CONCURRENTLY_s.print(\"%s: Try Collect Concurrently (%s): \", \\                                       Thread::current()->name(),       \\                                       GCCause::to_string(cause));      \\      LOG_COLLECT_CONCURRENTLY_s.print(__VA_ARGS__);                    \\    }                                                                   \\  } while (0)#define LOG_COLLECT_CONCURRENTLY_COMPLETE(cause, result) \\  LOG_COLLECT_CONCURRENTLY(cause, \"complete %s\", BOOL_TO_STR(result))bool G1CollectedHeap::try_collect_concurrently(GCCause::Cause cause,                                               uint gc_counter,                                               uint old_marking_started_before) {  assert_heap_not_locked();  assert(should_do_concurrent_full_gc(cause),         \"Non-concurrent cause %s\", GCCause::to_string(cause));  for (uint i = 1; true; ++i) {    \/\/ Try to schedule concurrent start evacuation pause that will    \/\/ start a concurrent cycle.    LOG_COLLECT_CONCURRENTLY(cause, \"attempt %u\", i);    VM_G1TryInitiateConcMark op(gc_counter, cause);    VMThread::execute(&op);    \/\/ Request is trivially finished.    if (cause == GCCause::_g1_periodic_collection) {      LOG_COLLECT_CONCURRENTLY_COMPLETE(cause, op.gc_succeeded());      return op.gc_succeeded();    }    \/\/ If VMOp skipped initiating concurrent marking cycle because    \/\/ we're terminating, then we're done.    if (op.terminating()) {      LOG_COLLECT_CONCURRENTLY(cause, \"skipped: terminating\");      return false;    }    \/\/ Lock to get consistent set of values.    uint old_marking_started_after;    uint old_marking_completed_after;    {      MutexLocker ml(Heap_lock);      \/\/ Update gc_counter for retrying VMOp if needed. Captured here to be      \/\/ consistent with the values we use below for termination tests.  If      \/\/ a retry is needed after a possible wait, and another collection      \/\/ occurs in the meantime, it will cause our retry to be skipped and      \/\/ we'll recheck for termination with updated conditions from that      \/\/ more recent collection.  That's what we want, rather than having      \/\/ our retry possibly perform an unnecessary collection.      gc_counter = total_collections();      old_marking_started_after = _old_marking_cycles_started;      old_marking_completed_after = _old_marking_cycles_completed;    }    if (cause == GCCause::_wb_breakpoint) {      if (op.gc_succeeded()) {        LOG_COLLECT_CONCURRENTLY_COMPLETE(cause, true);        return true;      }      \/\/ When _wb_breakpoint there can't be another cycle or deferred.      assert(!op.cycle_already_in_progress(), \"invariant\");      assert(!op.whitebox_attached(), \"invariant\");      \/\/ Concurrent cycle attempt might have been cancelled by some other      \/\/ collection, so retry.  Unlike other cases below, we want to retry      \/\/ even if cancelled by a STW full collection, because we really want      \/\/ to start a concurrent cycle.      if (old_marking_started_before != old_marking_started_after) {        LOG_COLLECT_CONCURRENTLY(cause, \"ignoring STW full GC\");        old_marking_started_before = old_marking_started_after;      }    } else if (!GCCause::is_user_requested_gc(cause)) {      \/\/ For an \"automatic\" (not user-requested) collection, we just need to      \/\/ ensure that progress is made.      \/\/      \/\/ Request is finished if any of      \/\/ (1) the VMOp successfully performed a GC,      \/\/ (2) a concurrent cycle was already in progress,      \/\/ (3) whitebox is controlling concurrent cycles,      \/\/ (4) a new cycle was started (by this thread or some other), or      \/\/ (5) a Full GC was performed.      \/\/ Cases (4) and (5) are detected together by a change to      \/\/ _old_marking_cycles_started.      \/\/      \/\/ Note that (1) does not imply (4).  If we're still in the mixed      \/\/ phase of an earlier concurrent collection, the request to make the      \/\/ collection a concurrent start won't be honored.  If we don't check for      \/\/ both conditions we'll spin doing back-to-back collections.      if (op.gc_succeeded() ||          op.cycle_already_in_progress() ||          op.whitebox_attached() ||          (old_marking_started_before != old_marking_started_after)) {        LOG_COLLECT_CONCURRENTLY_COMPLETE(cause, true);        return true;      }    } else {                    \/\/ User-requested GC.      \/\/ For a user-requested collection, we want to ensure that a complete      \/\/ full collection has been performed before returning, but without      \/\/ waiting for more than needed.      \/\/ For user-requested GCs (unlike non-UR), a successful VMOp implies a      \/\/ new cycle was started.  That's good, because it's not clear what we      \/\/ should do otherwise.  Trying again just does back to back GCs.      \/\/ Can't wait for someone else to start a cycle.  And returning fails      \/\/ to meet the goal of ensuring a full collection was performed.      assert(!op.gc_succeeded() ||             (old_marking_started_before != old_marking_started_after),             \"invariant: succeeded %s, started before %u, started after %u\",             BOOL_TO_STR(op.gc_succeeded()),             old_marking_started_before, old_marking_started_after);      \/\/ Request is finished if a full collection (concurrent or stw)      \/\/ was started after this request and has completed, e.g.      \/\/ started_before < completed_after.      if (gc_counter_less_than(old_marking_started_before,                               old_marking_completed_after)) {        LOG_COLLECT_CONCURRENTLY_COMPLETE(cause, true);        return true;      }      if (old_marking_started_after != old_marking_completed_after) {        \/\/ If there is an in-progress cycle (possibly started by us), then        \/\/ wait for that cycle to complete, e.g.        \/\/ while completed_now < started_after.        LOG_COLLECT_CONCURRENTLY(cause, \"wait\");        MonitorLocker ml(G1OldGCCount_lock);        while (gc_counter_less_than(_old_marking_cycles_completed,                                    old_marking_started_after)) {          ml.wait();        }        \/\/ Request is finished if the collection we just waited for was        \/\/ started after this request.        if (old_marking_started_before != old_marking_started_after) {          LOG_COLLECT_CONCURRENTLY(cause, \"complete after wait\");          return true;        }      }      \/\/ If VMOp was successful then it started a new cycle that the above      \/\/ wait &etc should have recognized as finishing this request.  This      \/\/ differs from a non-user-request, where gc_succeeded does not imply      \/\/ a new cycle was started.      assert(!op.gc_succeeded(), \"invariant\");      if (op.cycle_already_in_progress()) {        \/\/ If VMOp failed because a cycle was already in progress, it        \/\/ is now complete.  But it didn't finish this user-requested        \/\/ GC, so try again.        LOG_COLLECT_CONCURRENTLY(cause, \"retry after in-progress\");        continue;      } else if (op.whitebox_attached()) {        \/\/ If WhiteBox wants control, wait for notification of a state        \/\/ change in the controller, then try again.  Don't wait for        \/\/ release of control, since collections may complete while in        \/\/ control.  Note: This won't recognize a STW full collection        \/\/ while waiting; we can't wait on multiple monitors.        LOG_COLLECT_CONCURRENTLY(cause, \"whitebox control stall\");        MonitorLocker ml(ConcurrentGCBreakpoints::monitor());        if (ConcurrentGCBreakpoints::is_controlled()) {          ml.wait();        }        continue;      }    }    \/\/ Collection failed and should be retried.    assert(op.transient_failure(), \"invariant\");    LOG_COLLECT_CONCURRENTLY(cause, \"retry\");  }}bool G1CollectedHeap::try_collect(GCCause::Cause cause,                                  const G1GCCounters& counters_before) {  if (should_do_concurrent_full_gc(cause)) {    return try_collect_concurrently(cause,                                    counters_before.total_collections(),                                    counters_before.old_marking_cycles_started());  } else if (cause == GCCause::_wb_young_gc             DEBUG_ONLY(|| cause == GCCause::_scavenge_alot)) {    \/\/ Schedule a standard evacuation pause. We're setting word_size    \/\/ to 0 which means that we are not requesting a post-GC allocation.    VM_G1CollectForAllocation op(0,     \/* word_size *\/                                 counters_before.total_collections(),                                 cause);    VMThread::execute(&op);    return op.gc_succeeded();  } else {    \/\/ Schedule a Full GC.    VM_G1CollectFull op(counters_before.total_collections(),                        counters_before.total_full_collections(),                        cause);    VMThread::execute(&op);    return op.gc_succeeded();  }}void G1CollectedHeap::start_concurrent_gc_for_metadata_allocation(GCCause::Cause gc_cause) {  GCCauseSetter x(this, gc_cause);  \/\/ At this point we are supposed to start a concurrent cycle. We  \/\/ will do so if one is not already in progress.  bool should_start = policy()->force_concurrent_start_if_outside_cycle(gc_cause);  if (should_start) {    do_collection_pause_at_safepoint();  }}bool G1CollectedHeap::is_in(const void* p) const {  return is_in_reserved(p) && _hrm.is_available(addr_to_region(p));}\/\/ Iteration functions.\/\/ Iterates an ObjectClosure over all objects within a G1HeapRegion.class IterateObjectClosureRegionClosure: public G1HeapRegionClosure {  ObjectClosure* _cl;public:  IterateObjectClosureRegionClosure(ObjectClosure* cl) : _cl(cl) {}  bool do_heap_region(G1HeapRegion* r) {    if (!r->is_continues_humongous()) {      r->object_iterate(_cl);    }    return false;  }};void G1CollectedHeap::object_iterate(ObjectClosure* cl) {  IterateObjectClosureRegionClosure blk(cl);  heap_region_iterate(&blk);}class G1ParallelObjectIterator : public ParallelObjectIteratorImpl {private:  G1CollectedHeap*  _heap;  G1HeapRegionClaimer _claimer;public:  G1ParallelObjectIterator(uint thread_num) :      _heap(G1CollectedHeap::heap()),      _claimer(thread_num == 0 ? G1CollectedHeap::heap()->workers()->active_workers() : thread_num) {}  virtual void object_iterate(ObjectClosure* cl, uint worker_id) {    _heap->object_iterate_parallel(cl, worker_id, &_claimer);  }};ParallelObjectIteratorImpl* G1CollectedHeap::parallel_object_iterator(uint thread_num) {  return new G1ParallelObjectIterator(thread_num);}void G1CollectedHeap::object_iterate_parallel(ObjectClosure* cl, uint worker_id, G1HeapRegionClaimer* claimer) {  IterateObjectClosureRegionClosure blk(cl);  heap_region_par_iterate_from_worker_offset(&blk, claimer, worker_id);}void G1CollectedHeap::keep_alive(oop obj) {  G1BarrierSet::enqueue_preloaded(obj);}void G1CollectedHeap::heap_region_iterate(G1HeapRegionClosure* cl) const {  _hrm.iterate(cl);}void G1CollectedHeap::heap_region_iterate(G1HeapRegionIndexClosure* cl) const {  _hrm.iterate(cl);}void G1CollectedHeap::heap_region_par_iterate_from_worker_offset(G1HeapRegionClosure* cl,                                                                 G1HeapRegionClaimer *hrclaimer,                                                                 uint worker_id) const {  _hrm.par_iterate(cl, hrclaimer, hrclaimer->offset_for_worker(worker_id));}void G1CollectedHeap::heap_region_par_iterate_from_start(G1HeapRegionClosure* cl,                                                         G1HeapRegionClaimer *hrclaimer) const {  _hrm.par_iterate(cl, hrclaimer, 0);}void G1CollectedHeap::collection_set_iterate_all(G1HeapRegionClosure* cl) {  _collection_set.iterate(cl);}void G1CollectedHeap::collection_set_par_iterate_all(G1HeapRegionClosure* cl,                                                     G1HeapRegionClaimer* hr_claimer,                                                     uint worker_id) {  _collection_set.par_iterate(cl, hr_claimer, worker_id);}void G1CollectedHeap::collection_set_iterate_increment_from(G1HeapRegionClosure *cl,                                                            G1HeapRegionClaimer* hr_claimer,                                                            uint worker_id) {  _collection_set.iterate_incremental_part_from(cl, hr_claimer, worker_id);}void G1CollectedHeap::par_iterate_regions_array(G1HeapRegionClosure* cl,                                                G1HeapRegionClaimer* hr_claimer,                                                const uint regions[],                                                size_t length,                                                uint worker_id) const {  assert_at_safepoint();  if (length == 0) {    return;  }  uint total_workers = workers()->active_workers();  size_t start_pos = (worker_id * length) \/ total_workers;  size_t cur_pos = start_pos;  do {    uint region_idx = regions[cur_pos];    if (hr_claimer == nullptr || hr_claimer->claim_region(region_idx)) {      G1HeapRegion* r = region_at(region_idx);      bool result = cl->do_heap_region(r);      guarantee(!result, \"Must not cancel iteration\");    }    cur_pos++;    if (cur_pos == length) {      cur_pos = 0;    }  } while (cur_pos != start_pos);}HeapWord* G1CollectedHeap::block_start(const void* addr) const {  G1HeapRegion* hr = heap_region_containing(addr);  \/\/ The CollectedHeap API requires us to not fail for any given address within  \/\/ the heap. G1HeapRegion::block_start() has been optimized to not accept addresses  \/\/ outside of the allocated area.  if (addr >= hr->top()) {    return nullptr;  }  return hr->block_start(addr);}bool G1CollectedHeap::block_is_obj(const HeapWord* addr) const {  G1HeapRegion* hr = heap_region_containing(addr);  return hr->block_is_obj(addr, hr->parsable_bottom_acquire());}size_t G1CollectedHeap::tlab_capacity(Thread* ignored) const {  return eden_target_length() * G1HeapRegion::GrainBytes;}size_t G1CollectedHeap::tlab_used(Thread* ignored) const {  return _eden.length() * G1HeapRegion::GrainBytes;}\/\/ For G1 TLABs should not contain humongous objects, so the maximum TLAB size\/\/ must be equal to the humongous object limit.size_t G1CollectedHeap::max_tlab_size() const {  return align_down(_humongous_object_threshold_in_words, MinObjAlignment);}size_t G1CollectedHeap::unsafe_max_tlab_alloc(Thread* ignored) const {  return _allocator->unsafe_max_tlab_alloc();}size_t G1CollectedHeap::max_capacity() const {  return max_num_regions() * G1HeapRegion::GrainBytes;}size_t G1CollectedHeap::min_capacity() const {  return MinHeapSize;}void G1CollectedHeap::prepare_for_verify() {  _verifier->prepare_for_verify();}void G1CollectedHeap::verify(VerifyOption vo) {  _verifier->verify(vo);}bool G1CollectedHeap::supports_concurrent_gc_breakpoints() const {  return true;}class G1PrintRegionClosure: public G1HeapRegionClosure {  outputStream* _st;public:  G1PrintRegionClosure(outputStream* st) : _st(st) {}  bool do_heap_region(G1HeapRegion* r) {    r->print_on(_st);    return false;  }};bool G1CollectedHeap::is_obj_dead_cond(const oop obj,                                       const G1HeapRegion* hr,                                       const VerifyOption vo) const {  switch (vo) {    case VerifyOption::G1UseConcMarking: return is_obj_dead(obj, hr);    case VerifyOption::G1UseFullMarking: return is_obj_dead_full(obj, hr);    default:                             ShouldNotReachHere();  }  return false; \/\/ keep some compilers happy}bool G1CollectedHeap::is_obj_dead_cond(const oop obj,                                       const VerifyOption vo) const {  switch (vo) {    case VerifyOption::G1UseConcMarking: return is_obj_dead(obj);    case VerifyOption::G1UseFullMarking: return is_obj_dead_full(obj);    default:                             ShouldNotReachHere();  }  return false; \/\/ keep some compilers happy}void G1CollectedHeap::print_heap_regions() const {  LogTarget(Trace, gc, heap, region) lt;  if (lt.is_enabled()) {    LogStream ls(lt);    print_regions_on(&ls);  }}void G1CollectedHeap::print_heap_on(outputStream* st) const {  size_t heap_used = Heap_lock->owned_by_self() ? used() : used_unlocked();  st->print(\"%-20s\", \"garbage-first heap\");  st->print(\" total reserved %zuK, committed %zuK, used %zuK\",            _hrm.reserved().byte_size()\/K, capacity()\/K, heap_used\/K);  st->print(\" [\" PTR_FORMAT \", \" PTR_FORMAT \")\",            p2i(_hrm.reserved().start()),            p2i(_hrm.reserved().end()));  st->cr();  StreamIndentor si(st, 1);  st->print(\"region size %zuK, \", G1HeapRegion::GrainBytes \/ K);  uint young_regions = young_regions_count();  st->print(\"%u young (%zuK), \", young_regions,            (size_t) young_regions * G1HeapRegion::GrainBytes \/ K);  uint survivor_regions = survivor_regions_count();  st->print(\"%u survivors (%zuK)\", survivor_regions,            (size_t) survivor_regions * G1HeapRegion::GrainBytes \/ K);  st->cr();  if (_numa->is_enabled()) {    uint num_nodes = _numa->num_active_nodes();    st->print(\"remaining free region(s) on each NUMA node: \");    const uint* node_ids = _numa->node_ids();    for (uint node_index = 0; node_index < num_nodes; node_index++) {      uint num_free_regions = _hrm.num_free_regions(node_index);      st->print(\"%u=%u \", node_ids[node_index], num_free_regions);    }    st->cr();  }}void G1CollectedHeap::print_regions_on(outputStream* st) const {  st->print_cr(\"Heap Regions: E=young(eden), S=young(survivor), O=old, \"               \"HS=humongous(starts), HC=humongous(continues), \"               \"CS=collection set, F=free, \"               \"TAMS=top-at-mark-start, \"               \"PB=parsable bottom\");  G1PrintRegionClosure blk(st);  heap_region_iterate(&blk);}void G1CollectedHeap::print_extended_on(outputStream* st) const {  print_heap_on(st);  \/\/ Print the per-region information.  st->cr();  print_regions_on(st);}void G1CollectedHeap::print_gc_on(outputStream* st) const {  \/\/ Print the per-region information.  print_regions_on(st);  st->cr();  BarrierSet* bs = BarrierSet::barrier_set();  if (bs != nullptr) {    bs->print_on(st);  }  if (_cm != nullptr) {    st->cr();    _cm->print_on(st);  }}void G1CollectedHeap::gc_threads_do(ThreadClosure* tc) const {  workers()->threads_do(tc);  tc->do_thread(_cm_thread);  _cm->threads_do(tc);  _cr->threads_do(tc);  tc->do_thread(_service_thread);}void G1CollectedHeap::print_tracing_info() const {  rem_set()->print_summary_info();  concurrent_mark()->print_summary_info();}bool G1CollectedHeap::print_location(outputStream* st, void* addr) const {  return BlockLocationPrinter<G1CollectedHeap>::print_location(st, addr);}G1HeapSummary G1CollectedHeap::create_g1_heap_summary() {  size_t eden_used_bytes = _monitoring_support->eden_space_used();  size_t survivor_used_bytes = _monitoring_support->survivor_space_used();  size_t old_gen_used_bytes = _monitoring_support->old_gen_used();  size_t heap_used = Heap_lock->owned_by_self() ? used() : used_unlocked();  size_t eden_capacity_bytes =    (policy()->young_list_target_length() * G1HeapRegion::GrainBytes) - survivor_used_bytes;  VirtualSpaceSummary heap_summary = create_heap_space_summary();  return G1HeapSummary(heap_summary, heap_used, eden_used_bytes, eden_capacity_bytes,                       survivor_used_bytes, old_gen_used_bytes, num_committed_regions());}G1EvacSummary G1CollectedHeap::create_g1_evac_summary(G1EvacStats* stats) {  return G1EvacSummary(stats->allocated(), stats->wasted(), stats->undo_wasted(),                       stats->unused(), stats->used(), stats->region_end_waste(),                       stats->regions_filled(), stats->num_plab_filled(),                       stats->direct_allocated(), stats->num_direct_allocated(),                       stats->failure_used(), stats->failure_waste());}void G1CollectedHeap::trace_heap(GCWhen::Type when, const GCTracer* gc_tracer) {  const G1HeapSummary& heap_summary = create_g1_heap_summary();  gc_tracer->report_gc_heap_summary(when, heap_summary);  const MetaspaceSummary& metaspace_summary = create_metaspace_summary();  gc_tracer->report_metaspace_summary(when, metaspace_summary);}void G1CollectedHeap::gc_prologue(bool full) {  \/\/ Update common counters.  increment_total_collections(full \/* full gc *\/);  if (full || collector_state()->in_concurrent_start_gc()) {    increment_old_marking_cycles_started();  }}void G1CollectedHeap::gc_epilogue(bool full) {  \/\/ Update common counters.  if (full) {    \/\/ Update the number of full collections that have been completed.    increment_old_marking_cycles_completed(false \/* concurrent *\/, true \/* liveness_completed *\/);  }#if COMPILER2_OR_JVMCI  assert(DerivedPointerTable::is_empty(), \"derived pointer present\");#endif  \/\/ We have just completed a GC. Update the soft reference  \/\/ policy with the new heap occupancy  Universe::heap()->update_capacity_and_used_at_gc();  _collection_pause_end = Ticks::now();  _free_arena_memory_task->notify_new_stats(&_young_gen_card_set_stats,                                            &_collection_set_candidates_card_set_stats);  update_perf_counter_cpu_time();}uint G1CollectedHeap::uncommit_regions(uint region_limit) {  return _hrm.uncommit_inactive_regions(region_limit);}bool G1CollectedHeap::has_uncommittable_regions() {  return _hrm.has_inactive_regions();}void G1CollectedHeap::uncommit_regions_if_necessary() {  if (has_uncommittable_regions()) {    G1UncommitRegionTask::enqueue();  }}void G1CollectedHeap::verify_numa_regions(const char* desc) {  LogTarget(Trace, gc, heap, verify) lt;  if (lt.is_enabled()) {    LogStream ls(lt);    \/\/ Iterate all heap regions to print matching between preferred numa id and actual numa id.    G1NodeIndexCheckClosure cl(desc, _numa, &ls);    heap_region_iterate(&cl);  }}HeapWord* G1CollectedHeap::do_collection_pause(size_t word_size,                                               uint gc_count_before,                                               bool* succeeded,                                               GCCause::Cause gc_cause) {  assert_heap_not_locked_and_not_at_safepoint();  VM_G1CollectForAllocation op(word_size, gc_count_before, gc_cause);  VMThread::execute(&op);  HeapWord* result = op.result();  *succeeded = op.gc_succeeded();  assert(result == nullptr || *succeeded,         \"the result should be null if the VM did not succeed\");  assert_heap_not_locked();  return result;}void G1CollectedHeap::start_concurrent_cycle(bool concurrent_operation_is_full_mark) {  assert(!_cm_thread->in_progress(), \"Can not start concurrent operation while in progress\");  MutexLocker x(CGC_lock, Mutex::_no_safepoint_check_flag);  if (concurrent_operation_is_full_mark) {    _cm->post_concurrent_mark_start();    _cm_thread->start_full_mark();  } else {    _cm->post_concurrent_undo_start();    _cm_thread->start_undo_mark();  }  CGC_lock->notify();}bool G1CollectedHeap::is_potential_eager_reclaim_candidate(G1HeapRegion* r) const {  \/\/ We don't nominate objects with many remembered set entries, on  \/\/ the assumption that such objects are likely still live.  G1HeapRegionRemSet* rem_set = r->rem_set();  return rem_set->occupancy_less_or_equal_than(G1EagerReclaimRemSetThreshold);}#ifndef PRODUCTvoid G1CollectedHeap::verify_region_attr_remset_is_tracked() {  class VerifyRegionAttrRemSet : public G1HeapRegionClosure {  public:    virtual bool do_heap_region(G1HeapRegion* r) {      G1CollectedHeap* g1h = G1CollectedHeap::heap();      bool const remset_is_tracked = g1h->region_attr(r->bottom()).remset_is_tracked();      assert(r->rem_set()->is_tracked() == remset_is_tracked,             \"Region %u remset tracking status (%s) different to region attribute (%s)\",             r->hrm_index(), BOOL_TO_STR(r->rem_set()->is_tracked()), BOOL_TO_STR(remset_is_tracked));      return false;    }  } cl;  heap_region_iterate(&cl);}#endifvoid G1CollectedHeap::update_perf_counter_cpu_time() {  assert(Thread::current()->is_VM_thread(),         \"Must be called from VM thread to avoid races\");  if (!UsePerfData) {    return;  }  \/\/ Ensure ThreadTotalCPUTimeClosure destructor is called before publishing gc  \/\/ time.  {    ThreadTotalCPUTimeClosure tttc(CPUTimeGroups::CPUTimeType::gc_parallel_workers);    \/\/ Currently parallel worker threads never terminate (JDK-8081682), so it is    \/\/ safe for VMThread to read their CPU times. However, if JDK-8087340 is    \/\/ resolved so they terminate, we should rethink if it is still safe.    workers()->threads_do(&tttc);  }  CPUTimeCounters::publish_gc_total_cpu_time();}void G1CollectedHeap::start_new_collection_set() {  collection_set()->start_incremental_building();  clear_region_attr();  guarantee(_eden.length() == 0, \"eden should have been cleared\");  policy()->transfer_survivors_to_cset(survivor());  \/\/ We redo the verification but now wrt to the new CSet which  \/\/ has just got initialized after the previous CSet was freed.  _cm->verify_no_collection_set_oops();}void G1CollectedHeap::verify_before_young_collection(G1HeapVerifier::G1VerifyType type) {  if (!VerifyBeforeGC) {    return;  }  if (!G1HeapVerifier::should_verify(type)) {    return;  }  Ticks start = Ticks::now();  _verifier->prepare_for_verify();  _verifier->verify_region_sets_optional();  _verifier->verify_dirty_young_regions();  _verifier->verify_before_gc();  verify_numa_regions(\"GC Start\");  phase_times()->record_verify_before_time_ms((Ticks::now() - start).seconds() * MILLIUNITS);}void G1CollectedHeap::verify_after_young_collection(G1HeapVerifier::G1VerifyType type) {  if (!VerifyAfterGC) {    return;  }  if (!G1HeapVerifier::should_verify(type)) {    return;  }  Ticks start = Ticks::now();  _verifier->verify_after_gc();  verify_numa_regions(\"GC End\");  _verifier->verify_region_sets_optional();  if (collector_state()->in_concurrent_start_gc()) {    log_debug(gc, verify)(\"Marking state\");    _verifier->verify_marking_state();  }  phase_times()->record_verify_after_time_ms((Ticks::now() - start).seconds() * MILLIUNITS);}void G1CollectedHeap::do_collection_pause_at_safepoint(size_t allocation_word_size) {  assert_at_safepoint_on_vm_thread();  guarantee(!is_stw_gc_active(), \"collection is not reentrant\");  do_collection_pause_at_safepoint_helper(allocation_word_size);}G1HeapPrinterMark::G1HeapPrinterMark(G1CollectedHeap* g1h) : _g1h(g1h), _heap_transition(g1h) {  \/\/ This summary needs to be printed before incrementing total collections.  _g1h->rem_set()->print_periodic_summary_info(\"Before GC RS summary\",                                               _g1h->total_collections(),                                               true \/* show_thread_times *\/);  _g1h->print_before_gc();  _g1h->print_heap_regions();}G1HeapPrinterMark::~G1HeapPrinterMark() {  _g1h->policy()->print_age_table();  _g1h->rem_set()->print_coarsen_stats();  \/\/ We are at the end of the GC. Total collections has already been increased.  _g1h->rem_set()->print_periodic_summary_info(\"After GC RS summary\",                                               _g1h->total_collections() - 1,                                               false \/* show_thread_times *\/);  _heap_transition.print();  _g1h->print_heap_regions();  _g1h->print_after_gc();  \/\/ Print NUMA statistics.  _g1h->numa()->print_statistics();}G1JFRTracerMark::G1JFRTracerMark(STWGCTimer* timer, GCTracer* tracer) :  _timer(timer), _tracer(tracer) {  _timer->register_gc_start();  _tracer->report_gc_start(G1CollectedHeap::heap()->gc_cause(), _timer->gc_start());  G1CollectedHeap::heap()->trace_heap_before_gc(_tracer);}G1JFRTracerMark::~G1JFRTracerMark() {  G1CollectedHeap::heap()->trace_heap_after_gc(_tracer);  _timer->register_gc_end();  _tracer->report_gc_end(_timer->gc_end(), _timer->time_partitions());}void G1CollectedHeap::prepare_for_mutator_after_young_collection() {  Ticks start = Ticks::now();  _survivor_evac_stats.adjust_desired_plab_size();  _old_evac_stats.adjust_desired_plab_size();  \/\/ Start a new incremental collection set for the mutator phase.  start_new_collection_set();  _allocator->init_mutator_alloc_regions();  phase_times()->record_prepare_for_mutator_time_ms((Ticks::now() - start).seconds() * 1000.0);}void G1CollectedHeap::retire_tlabs() {  ensure_parsability(true);}void G1CollectedHeap::flush_region_pin_cache() {  for (JavaThreadIteratorWithHandle jtiwh; JavaThread *thread = jtiwh.next(); ) {    G1ThreadLocalData::pin_count_cache(thread).flush();  }}void G1CollectedHeap::do_collection_pause_at_safepoint_helper(size_t allocation_word_size) {  ResourceMark rm;  IsSTWGCActiveMark active_gc_mark;  GCIdMark gc_id_mark;  SvcGCMarker sgcm(SvcGCMarker::MINOR);  GCTraceCPUTime tcpu(_gc_tracer_stw);  _bytes_used_during_gc = 0;  policy()->decide_on_concurrent_start_pause();  \/\/ Record whether this pause may need to trigger a concurrent operation. Later,  \/\/ when we signal the G1ConcurrentMarkThread, the collector state has already  \/\/ been reset for the next pause.  bool should_start_concurrent_mark_operation = collector_state()->in_concurrent_start_gc();  \/\/ Perform the collection.  G1YoungCollector collector(gc_cause(), allocation_word_size);  collector.collect();  \/\/ It should now be safe to tell the concurrent mark thread to start  \/\/ without its logging output interfering with the logging output  \/\/ that came from the pause.  if (should_start_concurrent_mark_operation) {    verifier()->verify_bitmap_clear(true \/* above_tams_only *\/);    \/\/ CAUTION: after the start_concurrent_cycle() call below, the concurrent marking    \/\/ thread(s) could be running concurrently with us. Make sure that anything    \/\/ after this point does not assume that we are the only GC thread running.    \/\/ Note: of course, the actual marking work will not start until the safepoint    \/\/ itself is released in SuspendibleThreadSet::desynchronize().    start_concurrent_cycle(collector.concurrent_operation_is_full_mark());    ConcurrentGCBreakpoints::notify_idle_to_active();  }}void G1CollectedHeap::complete_cleaning(bool class_unloading_occurred) {  uint num_workers = workers()->active_workers();  G1ParallelCleaningTask unlink_task(num_workers, class_unloading_occurred);  workers()->run_task(&unlink_task);}void G1CollectedHeap::unload_classes_and_code(const char* description, BoolObjectClosure* is_alive, GCTimer* timer) {  GCTraceTime(Debug, gc, phases) debug(description, timer);  ClassUnloadingContext ctx(workers()->active_workers(),                            false \/* unregister_nmethods_during_purge *\/,                            false \/* lock_nmethod_free_separately *\/);  {    CodeCache::UnlinkingScope scope(is_alive);    bool unloading_occurred = SystemDictionary::do_unloading(timer);    GCTraceTime(Debug, gc, phases) t(\"G1 Complete Cleaning\", timer);    complete_cleaning(unloading_occurred);  }  {    GCTraceTime(Debug, gc, phases) t(\"Purge Unlinked NMethods\", timer);    ctx.purge_nmethods();  }  {    GCTraceTime(Debug, gc, phases) ur(\"Unregister NMethods\", timer);    G1CollectedHeap::heap()->bulk_unregister_nmethods();  }  {    GCTraceTime(Debug, gc, phases) t(\"Free Code Blobs\", timer);    ctx.free_nmethods();  }  {    GCTraceTime(Debug, gc, phases) t(\"Purge Class Loader Data\", timer);    ClassLoaderDataGraph::purge(true \/* at_safepoint *\/);    DEBUG_ONLY(MetaspaceUtils::verify();)  }}class G1BulkUnregisterNMethodTask : public WorkerTask {  G1HeapRegionClaimer _hrclaimer;  class UnregisterNMethodsHeapRegionClosure : public G1HeapRegionClosure {  public:    bool do_heap_region(G1HeapRegion* hr) {      hr->rem_set()->bulk_remove_code_roots();      return false;    }  } _cl;public:  G1BulkUnregisterNMethodTask(uint num_workers)  : WorkerTask(\"G1 Remove Unlinked NMethods From Code Root Set Task\"),    _hrclaimer(num_workers) { }  void work(uint worker_id) {    G1CollectedHeap::heap()->heap_region_par_iterate_from_worker_offset(&_cl, &_hrclaimer, worker_id);  }};void G1CollectedHeap::bulk_unregister_nmethods() {  uint num_workers = workers()->active_workers();  G1BulkUnregisterNMethodTask t(num_workers);  workers()->run_task(&t);}bool G1STWSubjectToDiscoveryClosure::do_object_b(oop obj) {  assert(obj != nullptr, \"must not be null\");  assert(_g1h->is_in_reserved(obj), \"Trying to discover obj \" PTR_FORMAT \" not in heap\", p2i(obj));  \/\/ The areas the CM and STW ref processor manage must be disjoint. The is_in_cset() below  \/\/ may falsely indicate that this is not the case here: however the collection set only  \/\/ contains old regions when concurrent mark is not running.  return _g1h->is_in_cset(obj) || _g1h->heap_region_containing(obj)->is_survivor();}void G1CollectedHeap::make_pending_list_reachable() {  if (collector_state()->in_concurrent_start_gc()) {    oop pll_head = Universe::reference_pending_list();    if (pll_head != nullptr) {      \/\/ Any valid worker id is fine here as we are in the VM thread and single-threaded.      _cm->mark_in_bitmap(0 \/* worker_id *\/, pll_head);    }  }}void G1CollectedHeap::set_humongous_stats(uint num_humongous_total, uint num_humongous_candidates) {  _num_humongous_objects = num_humongous_total;  _num_humongous_reclaim_candidates = num_humongous_candidates;}bool G1CollectedHeap::should_sample_collection_set_candidates() const {  const G1CollectionSetCandidates* candidates = collection_set()->candidates();  return !candidates->is_empty();}void G1CollectedHeap::set_collection_set_candidates_stats(G1MonotonicArenaMemoryStats& stats) {  _collection_set_candidates_card_set_stats = stats;}void G1CollectedHeap::set_young_gen_card_set_stats(const G1MonotonicArenaMemoryStats& stats) {  _young_gen_card_set_stats = stats;}void G1CollectedHeap::record_obj_copy_mem_stats() {  size_t total_old_allocated = _old_evac_stats.allocated() + _old_evac_stats.direct_allocated();  uint total_allocated = _survivor_evac_stats.regions_filled() + _old_evac_stats.regions_filled();  log_debug(gc)(\"Allocated %u survivor %u old percent total %1.2f%% (%u%%)\",                _survivor_evac_stats.regions_filled(), _old_evac_stats.regions_filled(),                percent_of(total_allocated, num_committed_regions() - total_allocated),                G1ReservePercent);  policy()->old_gen_alloc_tracker()->    add_allocated_bytes_since_last_gc(total_old_allocated * HeapWordSize);  _gc_tracer_stw->report_evacuation_statistics(create_g1_evac_summary(&_survivor_evac_stats),                                               create_g1_evac_summary(&_old_evac_stats));}void G1CollectedHeap::clear_bitmap_for_region(G1HeapRegion* hr) {  concurrent_mark()->clear_bitmap_for_region(hr);}void G1CollectedHeap::free_region(G1HeapRegion* hr, G1FreeRegionList* free_list) {  assert(!hr->is_free(), \"the region should not be free\");  assert(!hr->is_empty(), \"the region should not be empty\");  assert(_hrm.is_available(hr->hrm_index()), \"region should be committed\");  assert(!hr->has_pinned_objects(),         \"must not free a region which contains pinned objects\");  \/\/ Reset region metadata to allow reuse.  hr->hr_clear(true \/* clear_space *\/);  _policy->remset_tracker()->update_at_free(hr);  if (free_list != nullptr) {    free_list->add_ordered(hr);  }}void G1CollectedHeap::retain_region(G1HeapRegion* hr) {  MutexLocker x(G1RareEvent_lock, Mutex::_no_safepoint_check_flag);  collection_set()->candidates()->add_retained_region_unsorted(hr);}void G1CollectedHeap::free_humongous_region(G1HeapRegion* hr,                                            G1FreeRegionList* free_list) {  assert(hr->is_humongous(), \"this is only for humongous regions\");  hr->clear_humongous();  free_region(hr, free_list);}void G1CollectedHeap::remove_from_old_gen_sets(const uint old_regions_removed,                                               const uint humongous_regions_removed) {  if (old_regions_removed > 0 || humongous_regions_removed > 0) {    MutexLocker x(OldSets_lock, Mutex::_no_safepoint_check_flag);    _old_set.bulk_remove(old_regions_removed);    _humongous_set.bulk_remove(humongous_regions_removed);  }}void G1CollectedHeap::prepend_to_freelist(G1FreeRegionList* list) {  assert(list != nullptr, \"list can't be null\");  if (!list->is_empty()) {    MutexLocker x(FreeList_lock, Mutex::_no_safepoint_check_flag);    _hrm.insert_list_into_free_list(list);  }}void G1CollectedHeap::decrement_summary_bytes(size_t bytes) {  decrease_used(bytes);}void G1CollectedHeap::clear_eden() {  _eden.clear();}void G1CollectedHeap::clear_collection_set() {  collection_set()->clear();}void G1CollectedHeap::rebuild_free_region_list() {  Ticks start = Ticks::now();  _hrm.rebuild_free_list(workers());  phase_times()->record_total_rebuild_freelist_time_ms((Ticks::now() - start).seconds() * 1000.0);}class G1AbandonCollectionSetClosure : public G1HeapRegionClosure {public:  virtual bool do_heap_region(G1HeapRegion* r) {    assert(r->in_collection_set(), \"Region %u must have been in collection set\", r->hrm_index());    G1CollectedHeap::heap()->clear_region_attr(r);    r->clear_young_index_in_cset();    return false;  }};void G1CollectedHeap::abandon_collection_set(G1CollectionSet* collection_set) {  G1AbandonCollectionSetClosure cl;  collection_set_iterate_all(&cl);  collection_set->clear();  collection_set->stop_incremental_building();}bool G1CollectedHeap::is_old_gc_alloc_region(G1HeapRegion* hr) {  return _allocator->is_retained_old_region(hr);}void G1CollectedHeap::set_region_short_lived_locked(G1HeapRegion* hr) {  _eden.add(hr);  _policy->set_region_eden(hr);  young_regions_cset_group()->add(hr);}#ifdef ASSERTclass NoYoungRegionsClosure: public G1HeapRegionClosure {private:  bool _success;public:  NoYoungRegionsClosure() : _success(true) { }  bool do_heap_region(G1HeapRegion* r) {    if (r->is_young()) {      log_error(gc, verify)(\"Region [\" PTR_FORMAT \", \" PTR_FORMAT \") tagged as young\",                            p2i(r->bottom()), p2i(r->end()));      _success = false;    }    return false;  }  bool success() { return _success; }};bool G1CollectedHeap::check_young_list_empty() {  bool ret = (young_regions_count() == 0);  NoYoungRegionsClosure closure;  heap_region_iterate(&closure);  ret = ret && closure.success();  return ret;}#endif \/\/ ASSERT\/\/ Remove the given G1HeapRegion from the appropriate region set.void G1CollectedHeap::prepare_region_for_full_compaction(G1HeapRegion* hr) {  if (hr->is_humongous()) {    _humongous_set.remove(hr);  } else if (hr->is_old()) {    _old_set.remove(hr);  } else if (hr->is_young()) {    \/\/ Note that emptying the eden and survivor lists is postponed and instead    \/\/ done as the first step when rebuilding the regions sets again. The reason    \/\/ for this is that during a full GC string deduplication needs to know if    \/\/ a collected region was young or old when the full GC was initiated.    hr->uninstall_surv_rate_group();  } else {    \/\/ We ignore free regions, we'll empty the free list afterwards.    assert(hr->is_free(), \"it cannot be another type\");  }}void G1CollectedHeap::increase_used(size_t bytes) {  _summary_bytes_used += bytes;}void G1CollectedHeap::decrease_used(size_t bytes) {  assert(_summary_bytes_used >= bytes,         \"invariant: _summary_bytes_used: %zu should be >= bytes: %zu\",         _summary_bytes_used, bytes);  _summary_bytes_used -= bytes;}void G1CollectedHeap::set_used(size_t bytes) {  _summary_bytes_used = bytes;}class RebuildRegionSetsClosure : public G1HeapRegionClosure {private:  bool _free_list_only;  G1HeapRegionSet* _old_set;  G1HeapRegionSet* _humongous_set;  G1HeapRegionManager* _hrm;  size_t _total_used;public:  RebuildRegionSetsClosure(bool free_list_only,                           G1HeapRegionSet* old_set,                           G1HeapRegionSet* humongous_set,                           G1HeapRegionManager* hrm) :    _free_list_only(free_list_only), _old_set(old_set),    _humongous_set(humongous_set), _hrm(hrm), _total_used(0) {    assert(_hrm->num_free_regions() == 0, \"pre-condition\");    if (!free_list_only) {      assert(_old_set->is_empty(), \"pre-condition\");      assert(_humongous_set->is_empty(), \"pre-condition\");    }  }  bool do_heap_region(G1HeapRegion* r) {    if (r->is_empty()) {      assert(r->rem_set()->is_empty(), \"Empty regions should have empty remembered sets.\");      \/\/ Add free regions to the free list      r->set_free();      _hrm->insert_into_free_list(r);    } else if (!_free_list_only) {      assert(r->rem_set()->is_empty(), \"At this point remembered sets must have been cleared.\");      if (r->is_humongous()) {        _humongous_set->add(r);      } else {        assert(r->is_young() || r->is_free() || r->is_old(), \"invariant\");        \/\/ We now move all (non-humongous, non-old) regions to old gen,        \/\/ and register them as such.        r->move_to_old();        _old_set->add(r);      }      _total_used += r->used();    }    return false;  }  size_t total_used() {    return _total_used;  }};void G1CollectedHeap::rebuild_region_sets(bool free_list_only) {  assert_at_safepoint_on_vm_thread();  if (!free_list_only) {    _eden.clear();    _survivor.clear();  }  RebuildRegionSetsClosure cl(free_list_only,                              &_old_set, &_humongous_set,                              &_hrm);  heap_region_iterate(&cl);  if (!free_list_only) {    set_used(cl.total_used());  }  assert_used_and_recalculate_used_equal(this);}\/\/ Methods for the mutator alloc regionG1HeapRegion* G1CollectedHeap::new_mutator_alloc_region(size_t word_size,                                                      uint node_index) {  assert_heap_locked_or_at_safepoint(true \/* should_be_vm_thread *\/);  bool should_allocate = policy()->should_allocate_mutator_region();  if (should_allocate) {    G1HeapRegion* new_alloc_region = new_region(word_size,                                                G1HeapRegionType::Eden,                                                false \/* do_expand *\/,                                                node_index);    if (new_alloc_region != nullptr) {      set_region_short_lived_locked(new_alloc_region);      G1HeapRegionPrinter::alloc(new_alloc_region);      _policy->remset_tracker()->update_at_allocate(new_alloc_region);      return new_alloc_region;    }  }  return nullptr;}void G1CollectedHeap::retire_mutator_alloc_region(G1HeapRegion* alloc_region,                                                  size_t allocated_bytes) {  assert_heap_locked_or_at_safepoint(true \/* should_be_vm_thread *\/);  assert(alloc_region->is_eden(), \"all mutator alloc regions should be eden\");  alloc_region->record_activity();  \/\/ Record the activity of the alloc region  collection_set()->add_eden_region(alloc_region);  increase_used(allocated_bytes);  _eden.add_used_bytes(allocated_bytes);  G1HeapRegionPrinter::retire(alloc_region);  \/\/ We update the eden sizes here, when the region is retired,  \/\/ instead of when it's allocated, since this is the point that its  \/\/ used space has been recorded in _summary_bytes_used.  monitoring_support()->update_eden_size();}\/\/ Methods for the GC alloc regionsbool G1CollectedHeap::has_more_regions(G1HeapRegionAttr dest) {  if (dest.is_old()) {    return true;  } else {    return survivor_regions_count() < policy()->max_survivor_regions();  }}G1HeapRegion* G1CollectedHeap::new_gc_alloc_region(size_t word_size, G1HeapRegionAttr dest, uint node_index) {  assert(FreeList_lock->owned_by_self(), \"pre-condition\");  if (!has_more_regions(dest)) {    return nullptr;  }  G1HeapRegionType type;  if (dest.is_young()) {    type = G1HeapRegionType::Survivor;  } else {    type = G1HeapRegionType::Old;  }  G1HeapRegion* new_alloc_region = new_region(word_size,                                              type,                                              true \/* do_expand *\/,                                              node_index);  if (new_alloc_region != nullptr) {    if (type.is_survivor()) {      new_alloc_region->set_survivor();      _survivor.add(new_alloc_region);      register_new_survivor_region_with_region_attr(new_alloc_region);      \/\/ Install the group cardset.      young_regions_cset_group()->add(new_alloc_region);    } else {      new_alloc_region->set_old();    }    _policy->remset_tracker()->update_at_allocate(new_alloc_region);    register_region_with_region_attr(new_alloc_region);    G1HeapRegionPrinter::alloc(new_alloc_region);    return new_alloc_region;  }  return nullptr;}void G1CollectedHeap::retire_gc_alloc_region(G1HeapRegion* alloc_region,                                             size_t allocated_bytes,                                             G1HeapRegionAttr dest) {  alloc_region->record_activity();  \/\/ Record the activity of the alloc region  _bytes_used_during_gc += allocated_bytes;  if (dest.is_old()) {    old_set_add(alloc_region);  } else {    assert(dest.is_young(), \"Retiring alloc region should be young (%d)\", dest.type());    _survivor.add_used_bytes(allocated_bytes);  }  bool const during_im = collector_state()->in_concurrent_start_gc();  if (during_im && allocated_bytes > 0) {    _cm->add_root_region(alloc_region);  }  G1HeapRegionPrinter::retire(alloc_region);}void G1CollectedHeap::mark_evac_failure_object(uint worker_id, const oop obj, size_t obj_size) const {  assert(!_cm->is_marked_in_bitmap(obj), \"must be\");  _cm->raw_mark_in_bitmap(obj);}\/\/ Optimized nmethod scanningclass RegisterNMethodOopClosure: public OopClosure {  G1CollectedHeap* _g1h;  nmethod* _nm;public:  RegisterNMethodOopClosure(G1CollectedHeap* g1h, nmethod* nm) :    _g1h(g1h), _nm(nm) {}  void do_oop(oop* p) {    oop heap_oop = RawAccess<>::oop_load(p);    if (!CompressedOops::is_null(heap_oop)) {      oop obj = CompressedOops::decode_not_null(heap_oop);      G1HeapRegion* hr = _g1h->heap_region_containing(obj);      assert(!hr->is_continues_humongous(),             \"trying to add code root \" PTR_FORMAT \" in continuation of humongous region \" HR_FORMAT             \" starting at \" HR_FORMAT,             p2i(_nm), HR_FORMAT_PARAMS(hr), HR_FORMAT_PARAMS(hr->humongous_start_region()));      hr->add_code_root(_nm);    }  }  void do_oop(narrowOop* p) { ShouldNotReachHere(); }};void G1CollectedHeap::register_nmethod(nmethod* nm) {  guarantee(nm != nullptr, \"sanity\");  RegisterNMethodOopClosure reg_cl(this, nm);  nm->oops_do(&reg_cl);}void G1CollectedHeap::unregister_nmethod(nmethod* nm) {  \/\/ We always unregister nmethods in bulk during code unloading only.  ShouldNotReachHere();}void G1CollectedHeap::update_used_after_gc(bool evacuation_failed) {  if (evacuation_failed) {    set_used(recalculate_used());  } else {    \/\/ The \"used\" of the collection set have already been subtracted    \/\/ when they were freed.  Add in the bytes used.    increase_used(_bytes_used_during_gc);  }}class RebuildCodeRootClosure: public NMethodClosure {  G1CollectedHeap* _g1h;public:  RebuildCodeRootClosure(G1CollectedHeap* g1h) :    _g1h(g1h) {}  void do_nmethod(nmethod* nm) {    assert(nm != nullptr, \"Sanity\");    _g1h->register_nmethod(nm);  }};void G1CollectedHeap::rebuild_code_roots() {  RebuildCodeRootClosure nmethod_cl(this);  CodeCache::nmethods_do(&nmethod_cl);}void G1CollectedHeap::initialize_serviceability() {  _monitoring_support->initialize_serviceability();}MemoryUsage G1CollectedHeap::memory_usage() {  return _monitoring_support->memory_usage();}GrowableArray<GCMemoryManager*> G1CollectedHeap::memory_managers() {  return _monitoring_support->memory_managers();}GrowableArray<MemoryPool*> G1CollectedHeap::memory_pools() {  return _monitoring_support->memory_pools();}void G1CollectedHeap::fill_with_dummy_object(HeapWord* start, HeapWord* end, bool zap) {  G1HeapRegion* region = heap_region_containing(start);  region->fill_with_dummy_object(start, pointer_delta(end, start), zap);}void G1CollectedHeap::start_codecache_marking_cycle_if_inactive(bool concurrent_mark_start) {  \/\/ We can reach here with an active code cache marking cycle either because the  \/\/ previous G1 concurrent marking cycle was undone (if heap occupancy after the  \/\/ concurrent start young collection was below the threshold) or aborted. See  \/\/ CodeCache::on_gc_marking_cycle_finish() why this is.  We must not start a new code  \/\/ cache cycle then. If we are about to start a new g1 concurrent marking cycle we  \/\/ still have to arm all nmethod entry barriers. They are needed for adding oop  \/\/ constants to the SATB snapshot. Full GC does not need nmethods to be armed.  if (!CodeCache::is_gc_marking_cycle_active()) {    CodeCache::on_gc_marking_cycle_start();  }  if (concurrent_mark_start) {    CodeCache::arm_all_nmethods();  }}void G1CollectedHeap::finish_codecache_marking_cycle() {  CodeCache::on_gc_marking_cycle_finish();  CodeCache::arm_all_nmethods();}void G1CollectedHeap::prepare_group_cardsets_for_scan() {  young_regions_cardset()->reset_table_scanner_for_groups();  collection_set()->prepare_groups_for_scan();} * or visit www.oracle.com if you need additional information or have any * questions. * *\/#include \"classfile\/classLoaderDataGraph.hpp\"#include \"classfile\/metadataOnStackMark.hpp\"#include \"classfile\/systemDictionary.hpp\"#include \"code\/codeCache.hpp\"#include \"compiler\/oopMap.hpp\"#include \"gc\/g1\/g1Allocator.inline.hpp\"#include \"gc\/g1\/g1Arguments.hpp\"#include \"gc\/g1\/g1BarrierSet.hpp\"#include \"gc\/g1\/g1BatchedTask.hpp\"#include \"gc\/g1\/g1CollectedHeap.inline.hpp\"#include \"gc\/g1\/g1CollectionSet.hpp\"#include \"gc\/g1\/g1CollectionSetCandidates.hpp\"#include \"gc\/g1\/g1CollectorState.hpp\"#include \"gc\/g1\/g1ConcurrentMarkThread.inline.hpp\"#include \"gc\/g1\/g1ConcurrentRefine.hpp\"#include \"gc\/g1\/g1ConcurrentRefineThread.hpp\"#include \"gc\/g1\/g1HeapSizingPolicy.hpp\"  \/\/ Include this first to avoid include cycle#include \"gc\/g1\/g1HeapEvaluationTask.hpp\"#include \"gc\/g1\/g1DirtyCardQueue.hpp\"#include \"gc\/g1\/g1EvacStats.inline.hpp\"#include \"gc\/g1\/g1FullCollector.hpp\"#include \"gc\/g1\/g1GCCounters.hpp\"#include \"gc\/g1\/g1GCParPhaseTimesTracker.hpp\"#include \"gc\/g1\/g1GCPauseType.hpp\"#include \"gc\/g1\/g1GCPhaseTimes.hpp\"#include \"gc\/g1\/g1HeapRegion.inline.hpp\"#include \"gc\/g1\/g1HeapRegionPrinter.hpp\"#include \"gc\/g1\/g1HeapRegionRemSet.inline.hpp\"#include \"gc\/g1\/g1HeapRegionSet.inline.hpp\"#include \"gc\/g1\/g1HeapTransition.hpp\"#include \"gc\/g1\/g1HeapVerifier.hpp\"#include \"gc\/g1\/g1InitLogger.hpp\"#include \"gc\/g1\/g1MemoryPool.hpp\"#include \"gc\/g1\/g1MonotonicArenaFreeMemoryTask.hpp\"#include \"gc\/g1\/g1OopClosures.inline.hpp\"#include \"gc\/g1\/g1ParallelCleaning.hpp\"#include \"gc\/g1\/g1ParScanThreadState.inline.hpp\"#include \"gc\/g1\/g1PeriodicGCTask.hpp\"#include \"gc\/g1\/g1Policy.hpp\"#include \"gc\/g1\/g1RedirtyCardsQueue.hpp\"#include \"gc\/g1\/g1RegionPinCache.inline.hpp\"#include \"gc\/g1\/g1RegionToSpaceMapper.hpp\"#include \"gc\/g1\/g1RemSet.hpp\"#include \"gc\/g1\/g1RootClosures.hpp\"#include \"gc\/g1\/g1RootProcessor.hpp\"#include \"gc\/g1\/g1SATBMarkQueueSet.hpp\"#include \"gc\/g1\/g1ServiceThread.hpp\"#include \"gc\/g1\/g1ThreadLocalData.hpp\"#include \"gc\/g1\/g1Trace.hpp\"#include \"gc\/g1\/g1UncommitRegionTask.hpp\"#include \"gc\/g1\/g1VMOperations.hpp\"#include \"gc\/g1\/g1YoungCollector.hpp\"#include \"gc\/g1\/g1YoungGCAllocationFailureInjector.hpp\"#include \"gc\/shared\/classUnloadingContext.hpp\"#include \"gc\/shared\/concurrentGCBreakpoints.hpp\"#include \"gc\/shared\/fullGCForwarding.hpp\"#include \"gc\/shared\/gcBehaviours.hpp\"#include \"gc\/shared\/gcHeapSummary.hpp\"#include \"gc\/shared\/gcId.hpp\"#include \"gc\/shared\/gcTimer.hpp\"#include \"gc\/shared\/gcTraceTime.inline.hpp\"#include \"gc\/shared\/isGCActiveMark.hpp\"#include \"gc\/shared\/locationPrinter.inline.hpp\"#include \"gc\/shared\/oopStorageParState.hpp\"#include \"gc\/shared\/partialArrayState.hpp\"#include \"gc\/shared\/referenceProcessor.inline.hpp\"#include \"gc\/shared\/suspendibleThreadSet.hpp\"#include \"gc\/shared\/taskqueue.inline.hpp\"#include \"gc\/shared\/taskTerminator.hpp\"#include \"gc\/shared\/tlab_globals.hpp\"#include \"gc\/shared\/weakProcessor.inline.hpp\"#include \"gc\/shared\/workerPolicy.hpp\"#include \"logging\/log.hpp\"#include \"memory\/allocation.hpp\"#include \"memory\/heapInspection.hpp\"#include \"memory\/iterator.hpp\"#include \"memory\/memoryReserver.hpp\"#include \"memory\/metaspaceUtils.hpp\"#include \"memory\/resourceArea.hpp\"#include \"memory\/universe.hpp\"#include \"oops\/access.inline.hpp\"#include \"oops\/compressedOops.inline.hpp\"#include \"oops\/oop.inline.hpp\"#include \"runtime\/atomic.hpp\"#include \"runtime\/cpuTimeCounters.hpp\"#include \"runtime\/handles.inline.hpp\"#include \"runtime\/init.hpp\"#include \"runtime\/java.hpp\"#include \"runtime\/orderAccess.hpp\"#include \"runtime\/threadSMR.hpp\"#include \"runtime\/vmOperations.hpp\"#include \"runtime\/vmThread.hpp\"#include \"utilities\/align.hpp\"#include \"utilities\/autoRestore.hpp\"#include \"utilities\/bitMap.inline.hpp\"#include \"utilities\/globalDefinitions.hpp\"#include \"utilities\/stack.inline.hpp\"size_t G1CollectedHeap::_humongous_object_threshold_in_words = 0;\/\/ INVARIANTS\/NOTES\/\/\/\/ All allocation activity covered by the G1CollectedHeap interface is\/\/ serialized by acquiring the HeapLock.  This happens in mem_allocate\/\/ and allocate_new_tlab, which are the \"entry\" points to the\/\/ allocation code from the rest of the JVM.  (Note that this does not\/\/ apply to TLAB allocation, which is not part of this interface: it\/\/ is done by clients of this interface.)void G1RegionMappingChangedListener::reset_from_card_cache(uint start_idx, size_t num_regions) {  G1HeapRegionRemSet::invalidate_from_card_cache(start_idx, num_regions);}void G1RegionMappingChangedListener::on_commit(uint start_idx, size_t num_regions, bool zero_filled) {  \/\/ The from card cache is not the memory that is actually committed. So we cannot  \/\/ take advantage of the zero_filled parameter.  reset_from_card_cache(start_idx, num_regions);}void G1CollectedHeap::run_batch_task(G1BatchedTask* cl) {  uint num_workers = MAX2(1u, MIN2(cl->num_workers_estimate(), workers()->active_workers()));  cl->set_max_workers(num_workers);  workers()->run_task(cl, num_workers);}uint G1CollectedHeap::get_chunks_per_region() {  uint log_region_size = G1HeapRegion::LogOfHRGrainBytes;  \/\/ Limit the expected input values to current known possible values of the  \/\/ (log) region size. Adjust as necessary after testing if changing the permissible  \/\/ values for region size.  assert(log_region_size >= 20 && log_region_size <= 29,         \"expected value in [20,29], but got %u\", log_region_size);  return 1u << (log_region_size \/ 2 - 4);}G1HeapRegion* G1CollectedHeap::new_heap_region(uint hrs_index,                                               MemRegion mr) {  return new G1HeapRegion(hrs_index, bot(), mr, &_card_set_config);}\/\/ Private methods.G1HeapRegion* G1CollectedHeap::new_region(size_t word_size,                                          G1HeapRegionType type,                                          bool do_expand,                                          uint node_index) {  assert(!is_humongous(word_size) || word_size <= G1HeapRegion::GrainWords,         \"the only time we use this to allocate a humongous region is \"         \"when we are allocating a single humongous region\");  G1HeapRegion* res = _hrm.allocate_free_region(type, node_index);  if (res == nullptr && do_expand) {    \/\/ Currently, only attempts to allocate GC alloc regions set    \/\/ do_expand to true. So, we should only reach here during a    \/\/ safepoint.    assert(SafepointSynchronize::is_at_safepoint(), \"invariant\");    log_debug(gc, ergo, heap)(\"Attempt heap expansion (region allocation request failed). Allocation request: %zuB\",                              word_size * HeapWordSize);    assert(word_size * HeapWordSize < G1HeapRegion::GrainBytes,           \"This kind of expansion should never be more than one region. Size: %zu\",           word_size * HeapWordSize);    if (expand_single_region(node_index)) {      \/\/ Given that expand_single_region() succeeded in expanding the heap, and we      \/\/ always expand the heap by an amount aligned to the heap      \/\/ region size, the free list should in theory not be empty.      \/\/ In either case allocate_free_region() will check for null.      res = _hrm.allocate_free_region(type, node_index);    }  }  return res;}void G1CollectedHeap::set_humongous_metadata(G1HeapRegion* first_hr,                                             uint num_regions,                                             size_t word_size,                                             bool update_remsets) {  \/\/ Calculate the new top of the humongous object.  HeapWord* obj_top = first_hr->bottom() + word_size;  \/\/ The word size sum of all the regions used  size_t word_size_sum = num_regions * G1HeapRegion::GrainWords;  assert(word_size <= word_size_sum, \"sanity\");  \/\/ How many words memory we \"waste\" which cannot hold a filler object.  size_t words_not_fillable = 0;  \/\/ Pad out the unused tail of the last region with filler  \/\/ objects, for improved usage accounting.  \/\/ How many words can we use for filler objects.  size_t words_fillable = word_size_sum - word_size;  if (words_fillable >= G1CollectedHeap::min_fill_size()) {    G1CollectedHeap::fill_with_objects(obj_top, words_fillable);  } else {    \/\/ We have space to fill, but we cannot fit an object there.    words_not_fillable = words_fillable;    words_fillable = 0;  }  \/\/ We will set up the first region as \"starts humongous\". This  \/\/ will also update the BOT covering all the regions to reflect  \/\/ that there is a single object that starts at the bottom of the  \/\/ first region.  first_hr->hr_clear(false \/* clear_space *\/);  first_hr->set_starts_humongous(obj_top, words_fillable);  if (update_remsets) {    _policy->remset_tracker()->update_at_allocate(first_hr);  }  \/\/ Indices of first and last regions in the series.  uint first = first_hr->hrm_index();  uint last = first + num_regions - 1;  G1HeapRegion* hr = nullptr;  for (uint i = first + 1; i <= last; ++i) {    hr = region_at(i);    hr->hr_clear(false \/* clear_space *\/);    hr->set_continues_humongous(first_hr);    if (update_remsets) {      _policy->remset_tracker()->update_at_allocate(hr);    }  }  \/\/ Up to this point no concurrent thread would have been able to  \/\/ do any scanning on any region in this series. All the top  \/\/ fields still point to bottom, so the intersection between  \/\/ [bottom,top] and [card_start,card_end] will be empty. Before we  \/\/ update the top fields, we'll do a storestore to make sure that  \/\/ no thread sees the update to top before the zeroing of the  \/\/ object header and the BOT initialization.  OrderAccess::storestore();  \/\/ Now, we will update the top fields of the \"continues humongous\"  \/\/ regions except the last one.  for (uint i = first; i < last; ++i) {    hr = region_at(i);    hr->set_top(hr->end());  }  hr = region_at(last);  \/\/ If we cannot fit a filler object, we must set top to the end  \/\/ of the humongous object, otherwise we cannot iterate the heap  \/\/ and the BOT will not be complete.  hr->set_top(hr->end() - words_not_fillable);  assert(hr->bottom() < obj_top && obj_top <= hr->end(),         \"obj_top should be in last region\");  assert(words_not_fillable == 0 ||         first_hr->bottom() + word_size_sum - words_not_fillable == hr->top(),         \"Miscalculation in humongous allocation\");}HeapWord*G1CollectedHeap::humongous_obj_allocate_initialize_regions(G1HeapRegion* first_hr,                                                           uint num_regions,                                                           size_t word_size) {  assert(first_hr != nullptr, \"pre-condition\");  assert(is_humongous(word_size), \"word_size should be humongous\");  assert(num_regions * G1HeapRegion::GrainWords >= word_size, \"pre-condition\");  \/\/ Index of last region in the series.  uint first = first_hr->hrm_index();  uint last = first + num_regions - 1;  \/\/ We need to initialize the region(s) we just discovered. This is  \/\/ a bit tricky given that it can happen concurrently with  \/\/ refinement threads refining cards on these regions and  \/\/ potentially wanting to refine the BOT as they are scanning  \/\/ those cards (this can happen shortly after a cleanup; see CR  \/\/ 6991377). So we have to set up the region(s) carefully and in  \/\/ a specific order.  \/\/ The passed in hr will be the \"starts humongous\" region. The header  \/\/ of the new object will be placed at the bottom of this region.  HeapWord* new_obj = first_hr->bottom();  \/\/ First, we need to zero the header of the space that we will be  \/\/ allocating. When we update top further down, some refinement  \/\/ threads might try to scan the region. By zeroing the header we  \/\/ ensure that any thread that will try to scan the region will  \/\/ come across the zero klass word and bail out.  \/\/  \/\/ NOTE: It would not have been correct to have used  \/\/ CollectedHeap::fill_with_object() and make the space look like  \/\/ an int array. The thread that is doing the allocation will  \/\/ later update the object header to a potentially different array  \/\/ type and, for a very short period of time, the klass and length  \/\/ fields will be inconsistent. This could cause a refinement  \/\/ thread to calculate the object size incorrectly.  Copy::fill_to_words(new_obj, oopDesc::header_size(), 0);  \/\/ Next, update the metadata for the regions.  set_humongous_metadata(first_hr, num_regions, word_size, true);  G1HeapRegion* last_hr = region_at(last);  size_t used = byte_size(first_hr->bottom(), last_hr->top());  increase_used(used);  for (uint i = first; i <= last; ++i) {    G1HeapRegion *hr = region_at(i);    _humongous_set.add(hr);    G1HeapRegionPrinter::alloc(hr);  }  return new_obj;}size_t G1CollectedHeap::humongous_obj_size_in_regions(size_t word_size) {  assert(is_humongous(word_size), \"Object of size %zu must be humongous here\", word_size);  return align_up(word_size, G1HeapRegion::GrainWords) \/ G1HeapRegion::GrainWords;}\/\/ If could fit into free regions w\/o expansion, try.\/\/ Otherwise, if can expand, do so.\/\/ Otherwise, if using ex regions might help, try with ex given back.HeapWord* G1CollectedHeap::humongous_obj_allocate(size_t word_size) {  assert_heap_locked_or_at_safepoint(true \/* should_be_vm_thread *\/);  _verifier->verify_region_sets_optional();  uint obj_regions = (uint) humongous_obj_size_in_regions(word_size);  if (obj_regions > num_available_regions()) {    \/\/ Can't satisfy this allocation; early-return.    return nullptr;  }  \/\/ Policy: First try to allocate a humongous object in the free list.  G1HeapRegion* humongous_start = _hrm.allocate_humongous(obj_regions);  if (humongous_start == nullptr) {    \/\/ Policy: We could not find enough regions for the humongous object in the    \/\/ free list. Look through the heap to find a mix of free and uncommitted regions.    \/\/ If so, expand the heap and allocate the humongous object.    humongous_start = _hrm.expand_and_allocate_humongous(obj_regions);    if (humongous_start != nullptr) {      \/\/ We managed to find a region by expanding the heap.      log_debug(gc, ergo, heap)(\"Heap expansion (humongous allocation request). Allocation request: %zuB\",                                word_size * HeapWordSize);      policy()->record_new_heap_size(num_committed_regions());    } else {      \/\/ Policy: Potentially trigger a defragmentation GC.    }  }  HeapWord* result = nullptr;  if (humongous_start != nullptr) {    result = humongous_obj_allocate_initialize_regions(humongous_start, obj_regions, word_size);    assert(result != nullptr, \"it should always return a valid result\");    \/\/ A successful humongous object allocation changes the used space    \/\/ information of the old generation so we need to recalculate the    \/\/ sizes and update the jstat counters here.    monitoring_support()->update_sizes();  }  _verifier->verify_region_sets_optional();  return result;}HeapWord* G1CollectedHeap::allocate_new_tlab(size_t min_size,                                             size_t requested_size,                                             size_t* actual_size) {  assert_heap_not_locked_and_not_at_safepoint();  assert(!is_humongous(requested_size), \"we do not allow humongous TLABs\");  return attempt_allocation(min_size, requested_size, actual_size);}HeapWord*G1CollectedHeap::mem_allocate(size_t word_size,                              bool*  gc_overhead_limit_was_exceeded) {  assert_heap_not_locked_and_not_at_safepoint();  if (is_humongous(word_size)) {    return attempt_allocation_humongous(word_size);  }  size_t dummy = 0;  return attempt_allocation(word_size, word_size, &dummy);}HeapWord* G1CollectedHeap::attempt_allocation_slow(uint node_index, size_t word_size) {  ResourceMark rm; \/\/ For retrieving the thread names in log messages.  \/\/ Make sure you read the note in attempt_allocation_humongous().  assert_heap_not_locked_and_not_at_safepoint();  assert(!is_humongous(word_size), \"attempt_allocation_slow() should not \"         \"be called for humongous allocation requests\");  \/\/ We should only get here after the first-level allocation attempt  \/\/ (attempt_allocation()) failed to allocate.  \/\/ We will loop until a) we manage to successfully perform the allocation or b)  \/\/ successfully schedule a collection which fails to perform the allocation.  \/\/ Case b) is the only case when we'll return null.  HeapWord* result = nullptr;  for (uint try_count = 1; \/* we'll return *\/; try_count++) {    uint gc_count_before;    {      MutexLocker x(Heap_lock);      \/\/ Now that we have the lock, we first retry the allocation in case another      \/\/ thread changed the region while we were waiting to acquire the lock.      result = _allocator->attempt_allocation_locked(node_index, word_size);      if (result != nullptr) {        return result;      }      \/\/ Read the GC count while still holding the Heap_lock.      gc_count_before = total_collections();    }    bool succeeded;    result = do_collection_pause(word_size, gc_count_before, &succeeded, GCCause::_g1_inc_collection_pause);    if (succeeded) {      log_trace(gc, alloc)(\"%s: Successfully scheduled collection returning \" PTR_FORMAT,                           Thread::current()->name(), p2i(result));      return result;    }    log_trace(gc, alloc)(\"%s: Unsuccessfully scheduled collection allocating %zu words\",                         Thread::current()->name(), word_size);    \/\/ We can reach here if we were unsuccessful in scheduling a collection (because    \/\/ another thread beat us to it). In this case immeditealy retry the allocation    \/\/ attempt because another thread successfully performed a collection and possibly    \/\/ reclaimed enough space. The first attempt (without holding the Heap_lock) is    \/\/ here and the follow-on attempt will be at the start of the next loop    \/\/ iteration (after taking the Heap_lock).    size_t dummy = 0;    result = _allocator->attempt_allocation(node_index, word_size, word_size, &dummy);    if (result != nullptr) {      return result;    }    \/\/ Give a warning if we seem to be looping forever.    if ((QueuedAllocationWarningCount > 0) &&        (try_count % QueuedAllocationWarningCount == 0)) {      log_warning(gc, alloc)(\"%s:  Retried allocation %u times for %zu words\",                             Thread::current()->name(), try_count, word_size);    }  }  ShouldNotReachHere();  return nullptr;}template <typename Func>void G1CollectedHeap::iterate_regions_in_range(MemRegion range, const Func& func) {  \/\/ Mark each G1 region touched by the range as old, add it to  \/\/ the old set, and set top.  G1HeapRegion* curr_region = _hrm.addr_to_region(range.start());  G1HeapRegion* end_region = _hrm.addr_to_region(range.last());  while (curr_region != nullptr) {    bool is_last = curr_region == end_region;    G1HeapRegion* next_region = is_last ? nullptr : _hrm.next_region_in_heap(curr_region);    func(curr_region, is_last);    curr_region = next_region;  }}HeapWord* G1CollectedHeap::alloc_archive_region(size_t word_size, HeapWord* preferred_addr) {  assert(!is_init_completed(), \"Expect to be called at JVM init time\");  MutexLocker x(Heap_lock);  MemRegion reserved = _hrm.reserved();  if (reserved.word_size() <= word_size) {    log_info(gc, heap)(\"Unable to allocate regions as archive heap is too large; size requested = %zu\"                       \" bytes, heap = %zu bytes\", word_size * HeapWordSize, reserved.byte_size());    return nullptr;  }  \/\/ Temporarily disable pretouching of heap pages. This interface is used  \/\/ when mmap'ing archived heap data in, so pre-touching is wasted.  FlagSetting fs(AlwaysPreTouch, false);  size_t commits = 0;  \/\/ Attempt to allocate towards the end of the heap.  HeapWord* start_addr = reserved.end() - align_up(word_size, G1HeapRegion::GrainWords);  MemRegion range = MemRegion(start_addr, word_size);  HeapWord* last_address = range.last();  if (!_hrm.allocate_containing_regions(range, &commits, workers())) {    return nullptr;  }  increase_used(word_size * HeapWordSize);  if (commits != 0) {    log_debug(gc, ergo, heap)(\"Attempt heap expansion (allocate archive regions). Total size: %zuB\",                              G1HeapRegion::GrainWords * HeapWordSize * commits);  }  \/\/ Mark each G1 region touched by the range as old, add it to  \/\/ the old set, and set top.  auto set_region_to_old = [&] (G1HeapRegion* r, bool is_last) {    assert(r->is_empty(), \"Region already in use (%u)\", r->hrm_index());    HeapWord* top = is_last ? last_address + 1 : r->end();    r->set_top(top);    r->set_old();    G1HeapRegionPrinter::alloc(r);    _old_set.add(r);  };  iterate_regions_in_range(range, set_region_to_old);  return start_addr;}void G1CollectedHeap::populate_archive_regions_bot(MemRegion range) {  assert(!is_init_completed(), \"Expect to be called at JVM init time\");  iterate_regions_in_range(range,                           [&] (G1HeapRegion* r, bool is_last) {                             r->update_bot();                           });}void G1CollectedHeap::dealloc_archive_regions(MemRegion range) {  assert(!is_init_completed(), \"Expect to be called at JVM init time\");  MemRegion reserved = _hrm.reserved();  size_t size_used = 0;  \/\/ Free the G1 regions that are within the specified range.  MutexLocker x(Heap_lock);  HeapWord* start_address = range.start();  HeapWord* last_address = range.last();  assert(reserved.contains(start_address) && reserved.contains(last_address),         \"MemRegion outside of heap [\" PTR_FORMAT \", \" PTR_FORMAT \"]\",         p2i(start_address), p2i(last_address));  size_used += range.byte_size();  uint max_shrink_count = 0;  if (capacity() > MinHeapSize) {    size_t max_shrink_bytes = capacity() - MinHeapSize;    max_shrink_count = (uint)(max_shrink_bytes \/ G1HeapRegion::GrainBytes);  }  uint shrink_count = 0;  \/\/ Free, empty and uncommit regions with CDS archive content.  auto dealloc_archive_region = [&] (G1HeapRegion* r, bool is_last) {    guarantee(r->is_old(), \"Expected old region at index %u\", r->hrm_index());    _old_set.remove(r);    r->set_free();    r->set_top(r->bottom());    if (shrink_count < max_shrink_count) {      _hrm.shrink_at(r->hrm_index(), 1);      shrink_count++;    } else {      _hrm.insert_into_free_list(r);    }  };  iterate_regions_in_range(range, dealloc_archive_region);  if (shrink_count != 0) {    log_debug(gc, ergo, heap)(\"Attempt heap shrinking (CDS archive regions). Total size: %zuB (%u Regions)\",                              G1HeapRegion::GrainWords * HeapWordSize * shrink_count, shrink_count);    \/\/ Explicit uncommit.    uncommit_regions(shrink_count);  }  decrease_used(size_used);}inline HeapWord* G1CollectedHeap::attempt_allocation(size_t min_word_size,                                                     size_t desired_word_size,                                                     size_t* actual_word_size) {  assert_heap_not_locked_and_not_at_safepoint();  assert(!is_humongous(desired_word_size), \"attempt_allocation() should not \"         \"be called for humongous allocation requests\");  \/\/ Fix NUMA node association for the duration of this allocation  const uint node_index = _allocator->current_node_index();  HeapWord* result = _allocator->attempt_allocation(node_index, min_word_size, desired_word_size, actual_word_size);  if (result == nullptr) {    *actual_word_size = desired_word_size;    result = attempt_allocation_slow(node_index, desired_word_size);  }  assert_heap_not_locked();  if (result != nullptr) {    assert(*actual_word_size != 0, \"Actual size must have been set here\");    dirty_young_block(result, *actual_word_size);  } else {    *actual_word_size = 0;  }  return result;}HeapWord* G1CollectedHeap::attempt_allocation_humongous(size_t word_size) {  ResourceMark rm; \/\/ For retrieving the thread names in log messages.  \/\/ The structure of this method has a lot of similarities to  \/\/ attempt_allocation_slow(). The reason these two were not merged  \/\/ into a single one is that such a method would require several \"if  \/\/ allocation is not humongous do this, otherwise do that\"  \/\/ conditional paths which would obscure its flow. In fact, an early  \/\/ version of this code did use a unified method which was harder to  \/\/ follow and, as a result, it had subtle bugs that were hard to  \/\/ track down. So keeping these two methods separate allows each to  \/\/ be more readable. It will be good to keep these two in sync as  \/\/ much as possible.  assert_heap_not_locked_and_not_at_safepoint();  assert(is_humongous(word_size), \"attempt_allocation_humongous() \"         \"should only be called for humongous allocations\");  \/\/ Humongous objects can exhaust the heap quickly, so we should check if we  \/\/ need to start a marking cycle at each humongous object allocation. We do  \/\/ the check before we do the actual allocation. The reason for doing it  \/\/ before the allocation is that we avoid having to keep track of the newly  \/\/ allocated memory while we do a GC.  if (policy()->need_to_start_conc_mark(\"concurrent humongous allocation\",                                        word_size)) {    collect(GCCause::_g1_humongous_allocation);  }  \/\/ We will loop until a) we manage to successfully perform the allocation or b)  \/\/ successfully schedule a collection which fails to perform the allocation.  \/\/ Case b) is the only case when we'll return null.  HeapWord* result = nullptr;  for (uint try_count = 1; \/* we'll return *\/; try_count++) {    uint gc_count_before;    {      MutexLocker x(Heap_lock);      size_t size_in_regions = humongous_obj_size_in_regions(word_size);      \/\/ Given that humongous objects are not allocated in young      \/\/ regions, we'll first try to do the allocation without doing a      \/\/ collection hoping that there's enough space in the heap.      result = humongous_obj_allocate(word_size);      if (result != nullptr) {        policy()->old_gen_alloc_tracker()->          add_allocated_humongous_bytes_since_last_gc(size_in_regions * G1HeapRegion::GrainBytes);        return result;      }      \/\/ Read the GC count while still holding the Heap_lock.      gc_count_before = total_collections();    }    bool succeeded;    result = do_collection_pause(word_size, gc_count_before, &succeeded, GCCause::_g1_humongous_allocation);    if (succeeded) {      log_trace(gc, alloc)(\"%s: Successfully scheduled collection returning \" PTR_FORMAT,                           Thread::current()->name(), p2i(result));      if (result != nullptr) {        size_t size_in_regions = humongous_obj_size_in_regions(word_size);        policy()->old_gen_alloc_tracker()->          record_collection_pause_humongous_allocation(size_in_regions * G1HeapRegion::GrainBytes);      }      return result;    }    log_trace(gc, alloc)(\"%s: Unsuccessfully scheduled collection allocating %zu\",                         Thread::current()->name(), word_size);    \/\/ We can reach here if we were unsuccessful in scheduling a collection (because    \/\/ another thread beat us to it).    \/\/ Humongous object allocation always needs a lock, so we wait for the retry    \/\/ in the next iteration of the loop, unlike for the regular iteration case.    \/\/ Give a warning if we seem to be looping forever.    if ((QueuedAllocationWarningCount > 0) &&        (try_count % QueuedAllocationWarningCount == 0)) {      log_warning(gc, alloc)(\"%s: Retried allocation %u times for %zu words\",                             Thread::current()->name(), try_count, word_size);    }  }  ShouldNotReachHere();  return nullptr;}HeapWord* G1CollectedHeap::attempt_allocation_at_safepoint(size_t word_size,                                                           bool expect_null_mutator_alloc_region) {  assert_at_safepoint_on_vm_thread();  assert(!_allocator->has_mutator_alloc_region() || !expect_null_mutator_alloc_region,         \"the current alloc region was unexpectedly found to be non-null\");  \/\/ Fix NUMA node association for the duration of this allocation  const uint node_index = _allocator->current_node_index();  if (!is_humongous(word_size)) {    return _allocator->attempt_allocation_locked(node_index, word_size);  } else {    HeapWord* result = humongous_obj_allocate(word_size);    if (result != nullptr && policy()->need_to_start_conc_mark(\"STW humongous allocation\")) {      collector_state()->set_initiate_conc_mark_if_possible(true);    }    return result;  }  ShouldNotReachHere();}class PostCompactionPrinterClosure: public G1HeapRegionClosure {public:  bool do_heap_region(G1HeapRegion* hr) {    assert(!hr->is_young(), \"not expecting to find young regions\");    G1HeapRegionPrinter::post_compaction(hr);    return false;  }};void G1CollectedHeap::print_heap_after_full_collection() {  \/\/ Post collection region logging.  \/\/ We should do this after we potentially resize the heap so  \/\/ that all the COMMIT \/ UNCOMMIT events are generated before  \/\/ the compaction events.  if (G1HeapRegionPrinter::is_active()) {    PostCompactionPrinterClosure cl;    heap_region_iterate(&cl);  }}bool G1CollectedHeap::abort_concurrent_cycle() {  \/\/ Disable discovery and empty the discovered lists  \/\/ for the CM ref processor.  _ref_processor_cm->disable_discovery();  _ref_processor_cm->abandon_partial_discovery();  _ref_processor_cm->verify_no_references_recorded();  \/\/ Abandon current iterations of concurrent marking and concurrent  \/\/ refinement, if any are in progress.  return concurrent_mark()->concurrent_cycle_abort();}void G1CollectedHeap::prepare_heap_for_full_collection() {  \/\/ Make sure we'll choose a new allocation region afterwards.  _allocator->release_mutator_alloc_regions();  _allocator->abandon_gc_alloc_regions();  \/\/ We may have added regions to the current incremental collection  \/\/ set between the last GC or pause and now. We need to clear the  \/\/ incremental collection set and then start rebuilding it afresh  \/\/ after this full GC.  abandon_collection_set(collection_set());  _hrm.remove_all_free_regions();}void G1CollectedHeap::verify_before_full_collection() {  assert_used_and_recalculate_used_equal(this);  if (!VerifyBeforeGC) {    return;  }  if (!G1HeapVerifier::should_verify(G1HeapVerifier::G1VerifyFull)) {    return;  }  _verifier->verify_region_sets_optional();  _verifier->verify_before_gc();  _verifier->verify_bitmap_clear(true \/* above_tams_only *\/);}void G1CollectedHeap::prepare_for_mutator_after_full_collection(size_t allocation_word_size) {  \/\/ Prepare heap for normal collections.  assert(num_free_regions() == 0, \"we should not have added any free regions\");  rebuild_region_sets(false \/* free_list_only *\/);  abort_refinement();  resize_heap_if_necessary(allocation_word_size);  uncommit_regions_if_necessary();  \/\/ Rebuild the code root lists for each region  rebuild_code_roots();  start_new_collection_set();  _allocator->init_mutator_alloc_regions();  \/\/ Post collection state updates.  MetaspaceGC::compute_new_size();}void G1CollectedHeap::abort_refinement() {  \/\/ Discard all remembered set updates and reset refinement statistics.  G1BarrierSet::dirty_card_queue_set().abandon_logs_and_stats();  assert(G1BarrierSet::dirty_card_queue_set().num_cards() == 0,         \"DCQS should be empty\");  concurrent_refine()->get_and_reset_refinement_stats();}void G1CollectedHeap::verify_after_full_collection() {  if (!VerifyAfterGC) {    return;  }  if (!G1HeapVerifier::should_verify(G1HeapVerifier::G1VerifyFull)) {    return;  }  _hrm.verify_optional();  _verifier->verify_region_sets_optional();  _verifier->verify_after_gc();  _verifier->verify_bitmap_clear(false \/* above_tams_only *\/);  \/\/ At this point there should be no regions in the  \/\/ entire heap tagged as young.  assert(check_young_list_empty(), \"young list should be empty at this point\");  \/\/ Note: since we've just done a full GC, concurrent  \/\/ marking is no longer active. Therefore we need not  \/\/ re-enable reference discovery for the CM ref processor.  \/\/ That will be done at the start of the next marking cycle.  \/\/ We also know that the STW processor should no longer  \/\/ discover any new references.  assert(!_ref_processor_stw->discovery_enabled(), \"Postcondition\");  assert(!_ref_processor_cm->discovery_enabled(), \"Postcondition\");  _ref_processor_stw->verify_no_references_recorded();  _ref_processor_cm->verify_no_references_recorded();}void G1CollectedHeap::do_full_collection(bool clear_all_soft_refs,                                         bool do_maximal_compaction,                                         size_t allocation_word_size) {  assert_at_safepoint_on_vm_thread();  const bool do_clear_all_soft_refs = clear_all_soft_refs ||      soft_ref_policy()->should_clear_all_soft_refs();  G1FullGCMark gc_mark;  GCTraceTime(Info, gc) tm(\"Pause Full\", nullptr, gc_cause(), true);  G1FullCollector collector(this, do_clear_all_soft_refs, do_maximal_compaction, gc_mark.tracer());  collector.prepare_collection();  collector.collect();  collector.complete_collection(allocation_word_size);}void G1CollectedHeap::do_full_collection(bool clear_all_soft_refs) {  \/\/ Currently, there is no facility in the do_full_collection(bool) API to notify  \/\/ the caller that the collection did not succeed (e.g., because it was locked  \/\/ out by the GC locker). So, right now, we'll ignore the return value.  do_full_collection(clear_all_soft_refs,                     false \/* do_maximal_compaction *\/,                     size_t(0) \/* allocation_word_size *\/);}void G1CollectedHeap::upgrade_to_full_collection() {  GCCauseSetter compaction(this, GCCause::_g1_compaction_pause);  log_info(gc, ergo)(\"Attempting full compaction clearing soft references\");  do_full_collection(true  \/* clear_all_soft_refs *\/,                     false \/* do_maximal_compaction *\/,                     size_t(0) \/* allocation_word_size *\/);}void G1CollectedHeap::resize_heap_if_necessary(size_t allocation_word_size) {  assert_at_safepoint_on_vm_thread();  bool should_expand;  size_t resize_amount = _heap_sizing_policy->full_collection_resize_amount(should_expand, allocation_word_size);  if (resize_amount == 0) {    return;  } else if (should_expand) {    expand(resize_amount, _workers);  } else {    shrink(resize_amount);  }}HeapWord* G1CollectedHeap::satisfy_failed_allocation_helper(size_t word_size,                                                            bool do_gc,                                                            bool maximal_compaction,                                                            bool expect_null_mutator_alloc_region) {  \/\/ Let's attempt the allocation first.  HeapWord* result =    attempt_allocation_at_safepoint(word_size,                                    expect_null_mutator_alloc_region);  if (result != nullptr) {    return result;  }  \/\/ In a G1 heap, we're supposed to keep allocation from failing by  \/\/ incremental pauses.  Therefore, at least for now, we'll favor  \/\/ expansion over collection.  (This might change in the future if we can  \/\/ do something smarter than full collection to satisfy a failed alloc.)  result = expand_and_allocate(word_size);  if (result != nullptr) {    return result;  }  if (do_gc) {    GCCauseSetter compaction(this, GCCause::_g1_compaction_pause);    \/\/ Expansion didn't work, we'll try to do a Full GC.    \/\/ If maximal_compaction is set we clear all soft references and don't    \/\/ allow any dead wood to be left on the heap.    if (maximal_compaction) {      log_info(gc, ergo)(\"Attempting maximal full compaction clearing soft references\");    } else {      log_info(gc, ergo)(\"Attempting full compaction\");    }    do_full_collection(maximal_compaction \/* clear_all_soft_refs *\/,                       maximal_compaction \/* do_maximal_compaction *\/,                       word_size \/* allocation_word_size *\/);  }  return nullptr;}HeapWord* G1CollectedHeap::satisfy_failed_allocation(size_t word_size) {  assert_at_safepoint_on_vm_thread();  \/\/ Attempts to allocate followed by Full GC.  HeapWord* result =    satisfy_failed_allocation_helper(word_size,                                     true,  \/* do_gc *\/                                     false, \/* maximum_collection *\/                                     false \/* expect_null_mutator_alloc_region *\/);  if (result != nullptr) {    return result;  }  \/\/ Attempts to allocate followed by Full GC that will collect all soft references.  result = satisfy_failed_allocation_helper(word_size,                                            true, \/* do_gc *\/                                            true, \/* maximum_collection *\/                                            true \/* expect_null_mutator_alloc_region *\/);  if (result != nullptr) {    return result;  }  \/\/ Attempts to allocate, no GC  result = satisfy_failed_allocation_helper(word_size,                                            false, \/* do_gc *\/                                            false, \/* maximum_collection *\/                                            true  \/* expect_null_mutator_alloc_region *\/);  if (result != nullptr) {    return result;  }  assert(!soft_ref_policy()->should_clear_all_soft_refs(),         \"Flag should have been handled and cleared prior to this point\");  \/\/ What else?  We might try synchronous finalization later.  If the total  \/\/ space available is large enough for the allocation, then a more  \/\/ complete compaction phase than we've tried so far might be  \/\/ appropriate.  return nullptr;}\/\/ Attempting to expand the heap sufficiently\/\/ to support an allocation of the given \"word_size\".  If\/\/ successful, perform the allocation and return the address of the\/\/ allocated block, or else null.HeapWord* G1CollectedHeap::expand_and_allocate(size_t word_size) {  assert_at_safepoint_on_vm_thread();  _verifier->verify_region_sets_optional();  size_t expand_bytes = MAX2(word_size * HeapWordSize, MinHeapDeltaBytes);  log_debug(gc, ergo, heap)(\"Attempt heap expansion (allocation request failed). Allocation request: %zuB\",                            word_size * HeapWordSize);  if (expand(expand_bytes, _workers)) {    _hrm.verify_optional();    _verifier->verify_region_sets_optional();    return attempt_allocation_at_safepoint(word_size,                                           false \/* expect_null_mutator_alloc_region *\/);  }  return nullptr;}bool G1CollectedHeap::expand(size_t expand_bytes, WorkerThreads* pretouch_workers) {  size_t aligned_expand_bytes = os::align_up_vm_page_size(expand_bytes);  aligned_expand_bytes = align_up(aligned_expand_bytes, G1HeapRegion::GrainBytes);  log_debug(gc, ergo, heap)(\"Expand the heap. requested expansion amount: %zuB expansion amount: %zuB\",                            expand_bytes, aligned_expand_bytes);  if (num_inactive_regions() == 0) {    log_debug(gc, ergo, heap)(\"Did not expand the heap (heap already fully expanded)\");    return false;  }  uint regions_to_expand = (uint)(aligned_expand_bytes \/ G1HeapRegion::GrainBytes);  assert(regions_to_expand > 0, \"Must expand by at least one region\");  uint expanded_by = _hrm.expand_by(regions_to_expand, pretouch_workers);  assert(expanded_by > 0, \"must have failed during commit.\");  size_t actual_expand_bytes = expanded_by * G1HeapRegion::GrainBytes;  assert(actual_expand_bytes <= aligned_expand_bytes, \"post-condition\");  policy()->record_new_heap_size(num_committed_regions());  return true;}bool G1CollectedHeap::expand_single_region(uint node_index) {  uint expanded_by = _hrm.expand_on_preferred_node(node_index);  if (expanded_by == 0) {    assert(num_inactive_regions() == 0, \"Should be no regions left, available: %u\", num_inactive_regions());    log_debug(gc, ergo, heap)(\"Did not expand the heap (heap already fully expanded)\");    return false;  }  policy()->record_new_heap_size(num_committed_regions());  return true;}void G1CollectedHeap::shrink_helper(size_t shrink_bytes) {  size_t aligned_shrink_bytes = os::align_down_vm_page_size(shrink_bytes);  aligned_shrink_bytes = align_down(aligned_shrink_bytes, G1HeapRegion::GrainBytes);  uint num_regions_to_remove = (uint)(shrink_bytes \/ G1HeapRegion::GrainBytes);  uint num_regions_removed = _hrm.shrink_by(num_regions_to_remove);  size_t shrunk_bytes = num_regions_removed * G1HeapRegion::GrainBytes;  log_debug(gc, ergo, heap)(\"Shrink the heap. requested shrinking amount: %zuB aligned shrinking amount: %zuB attempted shrinking amount: %zuB\",                           shrink_bytes, aligned_shrink_bytes, shrunk_bytes);  if (num_regions_removed > 0) {    log_info(gc, heap)(\"Heap shrink completed: uncommitted %u regions (%zuMB), heap size now %zuMB\",                       num_regions_removed, shrunk_bytes \/ M, capacity() \/ M);    log_debug(gc, heap)(\"Heap shrink details: requested=%zuB aligned=%zuB attempted=%zuB actual=%zuB \"                        \"regions_removed=%u heap_capacity=%zuB\",                        shrink_bytes, aligned_shrink_bytes, num_regions_to_remove * G1HeapRegion::GrainBytes,                        shrunk_bytes, num_regions_removed, capacity());    policy()->record_new_heap_size(num_committed_regions());  } else {    log_debug(gc, ergo, heap)(\"Did not shrink the heap (heap shrinking operation failed)\");  }}void G1CollectedHeap::shrink(size_t shrink_bytes) {  _verifier->verify_region_sets_optional();  \/\/ We should only reach here at the end of a Full GC or during Remark which  \/\/ means we should not not be holding to any GC alloc regions. The method  \/\/ below will make sure of that and do any remaining clean up.  _allocator->abandon_gc_alloc_regions();  \/\/ Instead of tearing down \/ rebuilding the free lists here, we  \/\/ could instead use the remove_all_pending() method on free_list to  \/\/ remove only the ones that we need to remove.  _hrm.remove_all_free_regions();  shrink_helper(shrink_bytes);  rebuild_region_sets(true \/* free_list_only *\/);  _hrm.verify_optional();  _verifier->verify_region_sets_optional();}bool G1CollectedHeap::request_heap_shrink(size_t shrink_bytes) {  if (shrink_bytes == 0) {    return false;  }  \/\/ Fast path: if we are already at a safepoint (e.g. called from the  \/\/ GC service thread) just do the work directly.  if (SafepointSynchronize::is_at_safepoint()) {    shrink(shrink_bytes);    return true;                     \/\/ we *did* something  }  \/\/ Schedule a small VM-op so the work is done at the next safepoint  VM_G1ShrinkHeap op(this, shrink_bytes);  VMThread::execute(&op);  return true;                       \/\/ pages were at least *requested* to be released}class OldRegionSetChecker : public G1HeapRegionSetChecker {public:  void check_mt_safety() {    \/\/ Master Old Set MT safety protocol:    \/\/ (a) If we're at a safepoint, operations on the master old set    \/\/ should be invoked:    \/\/ - by the VM thread (which will serialize them), or    \/\/ - by the GC workers while holding the FreeList_lock, if we're    \/\/   at a safepoint for an evacuation pause (this lock is taken    \/\/   anyway when an GC alloc region is retired so that a new one    \/\/   is allocated from the free list), or    \/\/ - by the GC workers while holding the OldSets_lock, if we're at a    \/\/   safepoint for a cleanup pause.    \/\/ (b) If we're not at a safepoint, operations on the master old set    \/\/ should be invoked while holding the Heap_lock.    if (SafepointSynchronize::is_at_safepoint()) {      guarantee(Thread::current()->is_VM_thread() ||                FreeList_lock->owned_by_self() || OldSets_lock->owned_by_self(),                \"master old set MT safety protocol at a safepoint\");    } else {      guarantee(Heap_lock->owned_by_self(), \"master old set MT safety protocol outside a safepoint\");    }  }  bool is_correct_type(G1HeapRegion* hr) { return hr->is_old(); }  const char* get_description() { return \"Old Regions\"; }};class HumongousRegionSetChecker : public G1HeapRegionSetChecker {public:  void check_mt_safety() {    \/\/ Humongous Set MT safety protocol:    \/\/ (a) If we're at a safepoint, operations on the master humongous    \/\/ set should be invoked by either the VM thread (which will    \/\/ serialize them) or by the GC workers while holding the    \/\/ OldSets_lock.    \/\/ (b) If we're not at a safepoint, operations on the master    \/\/ humongous set should be invoked while holding the Heap_lock.    if (SafepointSynchronize::is_at_safepoint()) {      guarantee(Thread::current()->is_VM_thread() ||                OldSets_lock->owned_by_self(),                \"master humongous set MT safety protocol at a safepoint\");    } else {      guarantee(Heap_lock->owned_by_self(),                \"master humongous set MT safety protocol outside a safepoint\");    }  }  bool is_correct_type(G1HeapRegion* hr) { return hr->is_humongous(); }  const char* get_description() { return \"Humongous Regions\"; }};G1CollectedHeap::G1CollectedHeap() :  CollectedHeap(),  _service_thread(nullptr),  _periodic_gc_task(nullptr),  _free_arena_memory_task(nullptr),  _workers(nullptr),  _card_table(nullptr),  _collection_pause_end(Ticks::now()),  _old_set(\"Old Region Set\", new OldRegionSetChecker()),  _humongous_set(\"Humongous Region Set\", new HumongousRegionSetChecker()),  _bot(nullptr),  _listener(),  _numa(G1NUMA::create()),  _hrm(),  _allocator(nullptr),  _allocation_failure_injector(),  _verifier(nullptr),  _summary_bytes_used(0),  _bytes_used_during_gc(0),  _survivor_evac_stats(\"Young\", YoungPLABSize, PLABWeight),  _old_evac_stats(\"Old\", OldPLABSize, PLABWeight),  _monitoring_support(nullptr),  _num_humongous_objects(0),  _num_humongous_reclaim_candidates(0),  _collector_state(),  _old_marking_cycles_started(0),  _old_marking_cycles_completed(0),  _eden(),  _survivor(),  _gc_timer_stw(new STWGCTimer()),  _gc_tracer_stw(new G1NewTracer()),  _policy(new G1Policy(_gc_timer_stw)),  _heap_sizing_policy(nullptr),  _collection_set(this, _policy),  _rem_set(nullptr),  _card_set_config(),  _card_set_freelist_pool(G1CardSetConfiguration::num_mem_object_types()),  _young_regions_cset_group(card_set_config(), &_card_set_freelist_pool, 1u \/* group_id *\/),  _cm(nullptr),  _cm_thread(nullptr),  _cr(nullptr),  _task_queues(nullptr),  _partial_array_state_manager(nullptr),  _ref_processor_stw(nullptr),  _is_alive_closure_stw(this),  _is_subject_to_discovery_stw(this),  _ref_processor_cm(nullptr),  _is_alive_closure_cm(),  _is_subject_to_discovery_cm(this),  _region_attr() {  _heap_evaluation_task = nullptr;  _verifier = new G1HeapVerifier(this);  _allocator = new G1Allocator(this);  _heap_sizing_policy = G1HeapSizingPolicy::create(this, _policy->analytics());  _heap_sizing_policy->initialize();  _humongous_object_threshold_in_words = humongous_threshold_for(G1HeapRegion::GrainWords);  \/\/ Since filler arrays are never referenced, we can make them region sized.  \/\/ This simplifies filling up the region in case we have some potentially  \/\/ unreferenced (by Java code, but still in use by native code) pinned objects  \/\/ in there.  _filler_array_max_size = G1HeapRegion::GrainWords;  \/\/ Override the default _stack_chunk_max_size so that no humongous stack chunks are created  _stack_chunk_max_size = _humongous_object_threshold_in_words;  uint n_queues = ParallelGCThreads;  _task_queues = new G1ScannerTasksQueueSet(n_queues);  for (uint i = 0; i < n_queues; i++) {    G1ScannerTasksQueue* q = new G1ScannerTasksQueue();    _task_queues->register_queue(i, q);  }  _partial_array_state_manager = new PartialArrayStateManager(n_queues);  _gc_tracer_stw->initialize();}PartialArrayStateManager* G1CollectedHeap::partial_array_state_manager() const {  return _partial_array_state_manager;}G1RegionToSpaceMapper* G1CollectedHeap::create_aux_memory_mapper(const char* description,                                                                 size_t size,                                                                 size_t translation_factor) {  size_t preferred_page_size = os::page_size_for_region_unaligned(size, 1);  \/\/ When a page size is given we don't want to mix large  \/\/ and normal pages. If the size is not a multiple of the  \/\/ page size it will be aligned up to achieve this.  size_t alignment = os::vm_allocation_granularity();  if (preferred_page_size != os::vm_page_size()) {    alignment = MAX2(preferred_page_size, alignment);    size = align_up(size, alignment);  }  \/\/ Allocate a new reserved space, preferring to use large pages.  ReservedSpace rs = MemoryReserver::reserve(size,                                             alignment,                                             preferred_page_size,                                             mtGC);  size_t page_size = rs.page_size();  G1RegionToSpaceMapper* result  =    G1RegionToSpaceMapper::create_mapper(rs,                                         size,                                         page_size,                                         G1HeapRegion::GrainBytes,                                         translation_factor,                                         mtGC);  os::trace_page_sizes_for_requested_size(description,                                          size,                                          preferred_page_size,                                          rs.base(),                                          rs.size(),                                          page_size);  return result;}jint G1CollectedHeap::initialize_concurrent_refinement() {  jint ecode = JNI_OK;  _cr = G1ConcurrentRefine::create(policy(), &ecode);  return ecode;}jint G1CollectedHeap::initialize_service_thread() {  _service_thread = new G1ServiceThread();  if (_service_thread->osthread() == nullptr) {    vm_shutdown_during_initialization(\"Could not create G1ServiceThread\");    return JNI_ENOMEM;  }  return JNI_OK;}jint G1CollectedHeap::initialize() {  if (!os::is_thread_cpu_time_supported()) {    vm_exit_during_initialization(\"G1 requires cpu time gathering support\");  }  \/\/ Necessary to satisfy locking discipline assertions.  MutexLocker x(Heap_lock);  \/\/ While there are no constraints in the GC code that HeapWordSize  \/\/ be any particular value, there are multiple other areas in the  \/\/ system which believe this to be true (e.g. oop->object_size in some  \/\/ cases incorrectly returns the size in wordSize units rather than  \/\/ HeapWordSize).  guarantee(HeapWordSize == wordSize, \"HeapWordSize must equal wordSize\");  size_t init_byte_size = InitialHeapSize;  size_t reserved_byte_size = G1Arguments::heap_reserved_size_bytes();  \/\/ Ensure that the sizes are properly aligned.  Universe::check_alignment(init_byte_size, G1HeapRegion::GrainBytes, \"g1 heap\");  Universe::check_alignment(reserved_byte_size, G1HeapRegion::GrainBytes, \"g1 heap\");  Universe::check_alignment(reserved_byte_size, HeapAlignment, \"g1 heap\");  \/\/ Reserve the maximum.  \/\/ When compressed oops are enabled, the preferred heap base  \/\/ is calculated by subtracting the requested size from the  \/\/ 32Gb boundary and using the result as the base address for  \/\/ heap reservation. If the requested size is not aligned to  \/\/ G1HeapRegion::GrainBytes (i.e. the alignment that is passed  \/\/ into the ReservedHeapSpace constructor) then the actual  \/\/ base of the reserved heap may end up differing from the  \/\/ address that was requested (i.e. the preferred heap base).  \/\/ If this happens then we could end up using a non-optimal  \/\/ compressed oops mode.  ReservedHeapSpace heap_rs = Universe::reserve_heap(reserved_byte_size,                                                     HeapAlignment);  initialize_reserved_region(heap_rs);  \/\/ Create the barrier set for the entire reserved region.  G1CardTable* ct = new G1CardTable(_reserved);  G1BarrierSet* bs = new G1BarrierSet(ct);  bs->initialize();  assert(bs->is_a(BarrierSet::G1BarrierSet), \"sanity\");  BarrierSet::set_barrier_set(bs);  _card_table = ct;  {    G1SATBMarkQueueSet& satbqs = bs->satb_mark_queue_set();    satbqs.set_process_completed_buffers_threshold(G1SATBProcessCompletedThreshold);    satbqs.set_buffer_enqueue_threshold_percentage(G1SATBBufferEnqueueingThresholdPercent);  }  \/\/ Create space mappers.  size_t page_size = heap_rs.page_size();  G1RegionToSpaceMapper* heap_storage =    G1RegionToSpaceMapper::create_mapper(heap_rs,                                         heap_rs.size(),                                         page_size,                                         G1HeapRegion::GrainBytes,                                         1,                                         mtJavaHeap);  if(heap_storage == nullptr) {    vm_shutdown_during_initialization(\"Could not initialize G1 heap\");    return JNI_ERR;  }  os::trace_page_sizes(\"Heap\",                       MinHeapSize,                       reserved_byte_size,                       heap_rs.base(),                       heap_rs.size(),                       page_size);  heap_storage->set_mapping_changed_listener(&_listener);  \/\/ Create storage for the BOT, card table and the bitmap.  G1RegionToSpaceMapper* bot_storage =    create_aux_memory_mapper(\"Block Offset Table\",                             G1BlockOffsetTable::compute_size(heap_rs.size() \/ HeapWordSize),                             G1BlockOffsetTable::heap_map_factor());  G1RegionToSpaceMapper* cardtable_storage =    create_aux_memory_mapper(\"Card Table\",                             G1CardTable::compute_size(heap_rs.size() \/ HeapWordSize),                             G1CardTable::heap_map_factor());  size_t bitmap_size = G1CMBitMap::compute_size(heap_rs.size());  G1RegionToSpaceMapper* bitmap_storage =    create_aux_memory_mapper(\"Mark Bitmap\", bitmap_size, G1CMBitMap::heap_map_factor());  _hrm.initialize(heap_storage, bitmap_storage, bot_storage, cardtable_storage);  _card_table->initialize(cardtable_storage);  \/\/ 6843694 - ensure that the maximum region index can fit  \/\/ in the remembered set structures.  const uint max_region_idx = (1U << (sizeof(RegionIdx_t)*BitsPerByte-1)) - 1;  guarantee((max_num_regions() - 1) <= max_region_idx, \"too many regions\");  \/\/ The G1FromCardCache reserves card with value 0 as \"invalid\", so the heap must not  \/\/ start within the first card.  guarantee((uintptr_t)(heap_rs.base()) >= G1CardTable::card_size(), \"Java heap must not start within the first card.\");  G1FromCardCache::initialize(max_num_regions());  \/\/ Also create a G1 rem set.  _rem_set = new G1RemSet(this, _card_table);  _rem_set->initialize(max_num_regions());  size_t max_cards_per_region = ((size_t)1 << (sizeof(CardIdx_t)*BitsPerByte-1)) - 1;  guarantee(G1HeapRegion::CardsPerRegion > 0, \"make sure it's initialized\");  guarantee(G1HeapRegion::CardsPerRegion < max_cards_per_region,            \"too many cards per region\");  G1HeapRegionRemSet::initialize(_reserved);  G1FreeRegionList::set_unrealistically_long_length(max_num_regions() + 1);  _bot = new G1BlockOffsetTable(reserved(), bot_storage);  {    size_t granularity = G1HeapRegion::GrainBytes;    _region_attr.initialize(reserved(), granularity);  }  _workers = new WorkerThreads(\"GC Thread\", ParallelGCThreads);  if (_workers == nullptr) {    return JNI_ENOMEM;  }  _workers->initialize_workers();  _numa->set_region_info(G1HeapRegion::GrainBytes, page_size);  \/\/ Create the G1ConcurrentMark data structure and thread.  \/\/ (Must do this late, so that \"max_[reserved_]regions\" is defined.)  _cm = new G1ConcurrentMark(this, bitmap_storage);  _cm_thread = _cm->cm_thread();  \/\/ Now expand into the initial heap size.  if (!expand(init_byte_size, _workers)) {    vm_shutdown_during_initialization(\"Failed to allocate initial heap.\");    return JNI_ENOMEM;  }  \/\/ Perform any initialization actions delegated to the policy.  policy()->init(this, &_collection_set);  jint ecode = initialize_concurrent_refinement();  if (ecode != JNI_OK) {    return ecode;  }  ecode = initialize_service_thread();  if (ecode != JNI_OK) {    return ecode;  }  \/\/ Create and schedule the periodic gc task on the service thread.  _periodic_gc_task = new G1PeriodicGCTask(\"Periodic GC Task\");  _service_thread->register_task(_periodic_gc_task);  _free_arena_memory_task = new G1MonotonicArenaFreeMemoryTask(\"Card Set Free Memory Task\");  _service_thread->register_task(_free_arena_memory_task);  \/\/ Create the heap evaluation task using PeriodicTask  if (G1UseTimeBasedHeapSizing) {    _heap_evaluation_task = new G1HeapEvaluationTask(this, _heap_sizing_policy);    \/\/ PeriodicTask will be enrolled after G1 is fully initialized in post_initialize()    log_debug(gc, init)(\"G1 Time-Based Heap Evaluation task created (PeriodicTask)\");  } else {    _heap_evaluation_task = nullptr;  }  \/\/ Here we allocate the dummy G1HeapRegion that is required by the  \/\/ G1AllocRegion class.  G1HeapRegion* dummy_region = _hrm.get_dummy_region();  \/\/ We'll re-use the same region whether the alloc region will  \/\/ require BOT updates or not and, if it doesn't, then a non-young  \/\/ region will complain that it cannot support allocations without  \/\/ BOT updates. So we'll tag the dummy region as eden to avoid that.  dummy_region->set_eden();  \/\/ Make sure it's full.  dummy_region->set_top(dummy_region->end());  G1AllocRegion::setup(this, dummy_region);  _allocator->init_mutator_alloc_regions();  \/\/ Do create of the monitoring and management support so that  \/\/ values in the heap have been properly initialized.  _monitoring_support = new G1MonitoringSupport(this);  _collection_set.initialize(max_num_regions());  allocation_failure_injector()->reset();  CPUTimeCounters::create_counter(CPUTimeGroups::CPUTimeType::gc_parallel_workers);  CPUTimeCounters::create_counter(CPUTimeGroups::CPUTimeType::gc_conc_mark);  CPUTimeCounters::create_counter(CPUTimeGroups::CPUTimeType::gc_conc_refine);  CPUTimeCounters::create_counter(CPUTimeGroups::CPUTimeType::gc_service);  G1InitLogger::print();  FullGCForwarding::initialize(_reserved);  return JNI_OK;}bool G1CollectedHeap::concurrent_mark_is_terminating() const {  return _cm_thread->should_terminate();}void G1CollectedHeap::stop() {  \/\/ Stop all concurrent threads. We do this to make sure these threads  \/\/ do not continue to execute and access resources (e.g. logging)  \/\/ that are destroyed during shutdown.  _cr->stop();  _service_thread->stop();  _cm_thread->stop();}void G1CollectedHeap::safepoint_synchronize_begin() {  SuspendibleThreadSet::synchronize();}void G1CollectedHeap::safepoint_synchronize_end() {  SuspendibleThreadSet::desynchronize();}void G1CollectedHeap::post_initialize() {  CollectedHeap::post_initialize();  ref_processing_init();  \/\/ Enroll the heap evaluation task after G1 is fully initialized  if (G1UseTimeBasedHeapSizing && _heap_evaluation_task != nullptr) {    _heap_evaluation_task->enroll();  \/\/ PeriodicTask enroll() starts the task    log_debug(gc, init)(\"G1 Time-Based Heap Evaluation task enrolled (PeriodicTask)\");  }}void G1CollectedHeap::ref_processing_init() {  \/\/ Reference processing in G1 currently works as follows:  \/\/  \/\/ * There are two reference processor instances. One is  \/\/   used to record and process discovered references  \/\/   during concurrent marking; the other is used to  \/\/   record and process references during STW pauses  \/\/   (both full and incremental).  \/\/ * Both ref processors need to 'span' the entire heap as  \/\/   the regions in the collection set may be dotted around.  \/\/  \/\/ * For the concurrent marking ref processor:  \/\/   * Reference discovery is enabled at concurrent start.  \/\/   * Reference discovery is disabled and the discovered  \/\/     references processed etc during remarking.  \/\/   * Reference discovery is MT (see below).  \/\/   * Reference discovery requires a barrier (see below).  \/\/   * Reference processing may or may not be MT  \/\/     (depending on the value of ParallelRefProcEnabled  \/\/     and ParallelGCThreads).  \/\/   * A full GC disables reference discovery by the CM  \/\/     ref processor and abandons any entries on it's  \/\/     discovered lists.  \/\/  \/\/ * For the STW processor:  \/\/   * Non MT discovery is enabled at the start of a full GC.  \/\/   * Processing and enqueueing during a full GC is non-MT.  \/\/   * During a full GC, references are processed after marking.  \/\/  \/\/   * Discovery (may or may not be MT) is enabled at the start  \/\/     of an incremental evacuation pause.  \/\/   * References are processed near the end of a STW evacuation pause.  \/\/   * For both types of GC:  \/\/     * Discovery is atomic - i.e. not concurrent.  \/\/     * Reference discovery will not need a barrier.  _is_alive_closure_cm.initialize(concurrent_mark());  \/\/ Concurrent Mark ref processor  _ref_processor_cm =    new ReferenceProcessor(&_is_subject_to_discovery_cm,                           ParallelGCThreads,                              \/\/ degree of mt processing                           \/\/ We discover with the gc worker threads during Remark, so both                           \/\/ thread counts must be considered for discovery.                           MAX2(ParallelGCThreads, ConcGCThreads),         \/\/ degree of mt discovery                           true,                                           \/\/ Reference discovery is concurrent                           &_is_alive_closure_cm);                         \/\/ is alive closure  \/\/ STW ref processor  _ref_processor_stw =    new ReferenceProcessor(&_is_subject_to_discovery_stw,                           ParallelGCThreads,                    \/\/ degree of mt processing                           ParallelGCThreads,                    \/\/ degree of mt discovery                           false,                                \/\/ Reference discovery is not concurrent                           &_is_alive_closure_stw);              \/\/ is alive closure}size_t G1CollectedHeap::capacity() const {  return _hrm.num_committed_regions() * G1HeapRegion::GrainBytes;}size_t G1CollectedHeap::unused_committed_regions_in_bytes() const {  return _hrm.total_free_bytes();}\/\/ Computes the sum of the storage used by the various regions.size_t G1CollectedHeap::used() const {  size_t result = _summary_bytes_used + _allocator->used_in_alloc_regions();  return result;}size_t G1CollectedHeap::used_unlocked() const {  return _summary_bytes_used;}class SumUsedClosure: public G1HeapRegionClosure {  size_t _used;public:  SumUsedClosure() : _used(0) {}  bool do_heap_region(G1HeapRegion* r) {    _used += r->used();    return false;  }  size_t result() { return _used; }};size_t G1CollectedHeap::recalculate_used() const {  SumUsedClosure blk;  heap_region_iterate(&blk);  return blk.result();}bool  G1CollectedHeap::is_user_requested_concurrent_full_gc(GCCause::Cause cause) {  return GCCause::is_user_requested_gc(cause) && ExplicitGCInvokesConcurrent;}bool G1CollectedHeap::should_do_concurrent_full_gc(GCCause::Cause cause) {  switch (cause) {    case GCCause::_g1_humongous_allocation: return true;    case GCCause::_g1_periodic_collection:  return G1PeriodicGCInvokesConcurrent;    case GCCause::_wb_breakpoint:           return true;    case GCCause::_codecache_GC_aggressive: return true;    case GCCause::_codecache_GC_threshold:  return true;    default:                                return is_user_requested_concurrent_full_gc(cause);  }}void G1CollectedHeap::increment_old_marking_cycles_started() {  assert(_old_marking_cycles_started == _old_marking_cycles_completed ||         _old_marking_cycles_started == _old_marking_cycles_completed + 1,         \"Wrong marking cycle count (started: %d, completed: %d)\",         _old_marking_cycles_started, _old_marking_cycles_completed);  _old_marking_cycles_started++;}void G1CollectedHeap::increment_old_marking_cycles_completed(bool concurrent,                                                             bool whole_heap_examined) {  MonitorLocker ml(G1OldGCCount_lock, Mutex::_no_safepoint_check_flag);  \/\/ We assume that if concurrent == true, then the caller is a  \/\/ concurrent thread that was joined the Suspendible Thread  \/\/ Set. If there's ever a cheap way to check this, we should add an  \/\/ assert here.  \/\/ Given that this method is called at the end of a Full GC or of a  \/\/ concurrent cycle, and those can be nested (i.e., a Full GC can  \/\/ interrupt a concurrent cycle), the number of full collections  \/\/ completed should be either one (in the case where there was no  \/\/ nesting) or two (when a Full GC interrupted a concurrent cycle)  \/\/ behind the number of full collections started.  \/\/ This is the case for the inner caller, i.e. a Full GC.  assert(concurrent ||         (_old_marking_cycles_started == _old_marking_cycles_completed + 1) ||         (_old_marking_cycles_started == _old_marking_cycles_completed + 2),         \"for inner caller (Full GC): _old_marking_cycles_started = %u \"         \"is inconsistent with _old_marking_cycles_completed = %u\",         _old_marking_cycles_started, _old_marking_cycles_completed);  \/\/ This is the case for the outer caller, i.e. the concurrent cycle.  assert(!concurrent ||         (_old_marking_cycles_started == _old_marking_cycles_completed + 1),         \"for outer caller (concurrent cycle): \"         \"_old_marking_cycles_started = %u \"         \"is inconsistent with _old_marking_cycles_completed = %u\",         _old_marking_cycles_started, _old_marking_cycles_completed);  _old_marking_cycles_completed += 1;  if (whole_heap_examined) {    \/\/ Signal that we have completed a visit to all live objects.    record_whole_heap_examined_timestamp();  }  \/\/ We need to clear the \"in_progress\" flag in the CM thread before  \/\/ we wake up any waiters (especially when ExplicitInvokesConcurrent  \/\/ is set) so that if a waiter requests another System.gc() it doesn't  \/\/ incorrectly see that a marking cycle is still in progress.  if (concurrent) {    _cm_thread->set_idle();  }  \/\/ Notify threads waiting in System.gc() (with ExplicitGCInvokesConcurrent)  \/\/ for a full GC to finish that their wait is over.  ml.notify_all();}\/\/ Helper for collect().static G1GCCounters collection_counters(G1CollectedHeap* g1h) {  MutexLocker ml(Heap_lock);  return G1GCCounters(g1h);}void G1CollectedHeap::collect(GCCause::Cause cause) {  try_collect(cause, collection_counters(this));}\/\/ Return true if (x < y) with allowance for wraparound.static bool gc_counter_less_than(uint x, uint y) {  return (x - y) > (UINT_MAX\/2);}\/\/ LOG_COLLECT_CONCURRENTLY(cause, msg, args...)\/\/ Macro so msg printing is format-checked.#define LOG_COLLECT_CONCURRENTLY(cause, ...)                            \\  do {                                                                  \\    LogTarget(Trace, gc) LOG_COLLECT_CONCURRENTLY_lt;                   \\    if (LOG_COLLECT_CONCURRENTLY_lt.is_enabled()) {                     \\      ResourceMark rm; \/* For thread name. *\/                           \\      LogStream LOG_COLLECT_CONCURRENTLY_s(&LOG_COLLECT_CONCURRENTLY_lt); \\      LOG_COLLECT_CONCURRENTLY_s.print(\"%s: Try Collect Concurrently (%s): \", \\                                       Thread::current()->name(),       \\                                       GCCause::to_string(cause));      \\      LOG_COLLECT_CONCURRENTLY_s.print(__VA_ARGS__);                    \\    }                                                                   \\  } while (0)#define LOG_COLLECT_CONCURRENTLY_COMPLETE(cause, result) \\  LOG_COLLECT_CONCURRENTLY(cause, \"complete %s\", BOOL_TO_STR(result))bool G1CollectedHeap::try_collect_concurrently(GCCause::Cause cause,                                               uint gc_counter,                                               uint old_marking_started_before) {  assert_heap_not_locked();  assert(should_do_concurrent_full_gc(cause),         \"Non-concurrent cause %s\", GCCause::to_string(cause));  for (uint i = 1; true; ++i) {    \/\/ Try to schedule concurrent start evacuation pause that will    \/\/ start a concurrent cycle.    LOG_COLLECT_CONCURRENTLY(cause, \"attempt %u\", i);    VM_G1TryInitiateConcMark op(gc_counter, cause);    VMThread::execute(&op);    \/\/ Request is trivially finished.    if (cause == GCCause::_g1_periodic_collection) {      LOG_COLLECT_CONCURRENTLY_COMPLETE(cause, op.gc_succeeded());      return op.gc_succeeded();    }    \/\/ If VMOp skipped initiating concurrent marking cycle because    \/\/ we're terminating, then we're done.    if (op.terminating()) {      LOG_COLLECT_CONCURRENTLY(cause, \"skipped: terminating\");      return false;    }    \/\/ Lock to get consistent set of values.    uint old_marking_started_after;    uint old_marking_completed_after;    {      MutexLocker ml(Heap_lock);      \/\/ Update gc_counter for retrying VMOp if needed. Captured here to be      \/\/ consistent with the values we use below for termination tests.  If      \/\/ a retry is needed after a possible wait, and another collection      \/\/ occurs in the meantime, it will cause our retry to be skipped and      \/\/ we'll recheck for termination with updated conditions from that      \/\/ more recent collection.  That's what we want, rather than having      \/\/ our retry possibly perform an unnecessary collection.      gc_counter = total_collections();      old_marking_started_after = _old_marking_cycles_started;      old_marking_completed_after = _old_marking_cycles_completed;    }    if (cause == GCCause::_wb_breakpoint) {      if (op.gc_succeeded()) {        LOG_COLLECT_CONCURRENTLY_COMPLETE(cause, true);        return true;      }      \/\/ When _wb_breakpoint there can't be another cycle or deferred.      assert(!op.cycle_already_in_progress(), \"invariant\");      assert(!op.whitebox_attached(), \"invariant\");      \/\/ Concurrent cycle attempt might have been cancelled by some other      \/\/ collection, so retry.  Unlike other cases below, we want to retry      \/\/ even if cancelled by a STW full collection, because we really want      \/\/ to start a concurrent cycle.      if (old_marking_started_before != old_marking_started_after) {        LOG_COLLECT_CONCURRENTLY(cause, \"ignoring STW full GC\");        old_marking_started_before = old_marking_started_after;      }    } else if (!GCCause::is_user_requested_gc(cause)) {      \/\/ For an \"automatic\" (not user-requested) collection, we just need to      \/\/ ensure that progress is made.      \/\/      \/\/ Request is finished if any of      \/\/ (1) the VMOp successfully performed a GC,      \/\/ (2) a concurrent cycle was already in progress,      \/\/ (3) whitebox is controlling concurrent cycles,      \/\/ (4) a new cycle was started (by this thread or some other), or      \/\/ (5) a Full GC was performed.      \/\/ Cases (4) and (5) are detected together by a change to      \/\/ _old_marking_cycles_started.      \/\/      \/\/ Note that (1) does not imply (4).  If we're still in the mixed      \/\/ phase of an earlier concurrent collection, the request to make the      \/\/ collection a concurrent start won't be honored.  If we don't check for      \/\/ both conditions we'll spin doing back-to-back collections.      if (op.gc_succeeded() ||          op.cycle_already_in_progress() ||          op.whitebox_attached() ||          (old_marking_started_before != old_marking_started_after)) {        LOG_COLLECT_CONCURRENTLY_COMPLETE(cause, true);        return true;      }    } else {                    \/\/ User-requested GC.      \/\/ For a user-requested collection, we want to ensure that a complete      \/\/ full collection has been performed before returning, but without      \/\/ waiting for more than needed.      \/\/ For user-requested GCs (unlike non-UR), a successful VMOp implies a      \/\/ new cycle was started.  That's good, because it's not clear what we      \/\/ should do otherwise.  Trying again just does back to back GCs.      \/\/ Can't wait for someone else to start a cycle.  And returning fails      \/\/ to meet the goal of ensuring a full collection was performed.      assert(!op.gc_succeeded() ||             (old_marking_started_before != old_marking_started_after),             \"invariant: succeeded %s, started before %u, started after %u\",             BOOL_TO_STR(op.gc_succeeded()),             old_marking_started_before, old_marking_started_after);      \/\/ Request is finished if a full collection (concurrent or stw)      \/\/ was started after this request and has completed, e.g.      \/\/ started_before < completed_after.      if (gc_counter_less_than(old_marking_started_before,                               old_marking_completed_after)) {        LOG_COLLECT_CONCURRENTLY_COMPLETE(cause, true);        return true;      }      if (old_marking_started_after != old_marking_completed_after) {        \/\/ If there is an in-progress cycle (possibly started by us), then        \/\/ wait for that cycle to complete, e.g.        \/\/ while completed_now < started_after.        LOG_COLLECT_CONCURRENTLY(cause, \"wait\");        MonitorLocker ml(G1OldGCCount_lock);        while (gc_counter_less_than(_old_marking_cycles_completed,                                    old_marking_started_after)) {          ml.wait();        }        \/\/ Request is finished if the collection we just waited for was        \/\/ started after this request.        if (old_marking_started_before != old_marking_started_after) {          LOG_COLLECT_CONCURRENTLY(cause, \"complete after wait\");          return true;        }      }      \/\/ If VMOp was successful then it started a new cycle that the above      \/\/ wait &etc should have recognized as finishing this request.  This      \/\/ differs from a non-user-request, where gc_succeeded does not imply      \/\/ a new cycle was started.      assert(!op.gc_succeeded(), \"invariant\");      if (op.cycle_already_in_progress()) {        \/\/ If VMOp failed because a cycle was already in progress, it        \/\/ is now complete.  But it didn't finish this user-requested        \/\/ GC, so try again.        LOG_COLLECT_CONCURRENTLY(cause, \"retry after in-progress\");        continue;      } else if (op.whitebox_attached()) {        \/\/ If WhiteBox wants control, wait for notification of a state        \/\/ change in the controller, then try again.  Don't wait for        \/\/ release of control, since collections may complete while in        \/\/ control.  Note: This won't recognize a STW full collection        \/\/ while waiting; we can't wait on multiple monitors.        LOG_COLLECT_CONCURRENTLY(cause, \"whitebox control stall\");        MonitorLocker ml(ConcurrentGCBreakpoints::monitor());        if (ConcurrentGCBreakpoints::is_controlled()) {          ml.wait();        }        continue;      }    }    \/\/ Collection failed and should be retried.    assert(op.transient_failure(), \"invariant\");    LOG_COLLECT_CONCURRENTLY(cause, \"retry\");  }}bool G1CollectedHeap::try_collect(GCCause::Cause cause,                                  const G1GCCounters& counters_before) {  if (should_do_concurrent_full_gc(cause)) {    return try_collect_concurrently(cause,                                    counters_before.total_collections(),                                    counters_before.old_marking_cycles_started());  } else if (cause == GCCause::_wb_young_gc             DEBUG_ONLY(|| cause == GCCause::_scavenge_alot)) {    \/\/ Schedule a standard evacuation pause. We're setting word_size    \/\/ to 0 which means that we are not requesting a post-GC allocation.    VM_G1CollectForAllocation op(0,     \/* word_size *\/                                 counters_before.total_collections(),                                 cause);    VMThread::execute(&op);    return op.gc_succeeded();  } else {    \/\/ Schedule a Full GC.    VM_G1CollectFull op(counters_before.total_collections(),                        counters_before.total_full_collections(),                        cause);    VMThread::execute(&op);    return op.gc_succeeded();  }}void G1CollectedHeap::start_concurrent_gc_for_metadata_allocation(GCCause::Cause gc_cause) {  GCCauseSetter x(this, gc_cause);  \/\/ At this point we are supposed to start a concurrent cycle. We  \/\/ will do so if one is not already in progress.  bool should_start = policy()->force_concurrent_start_if_outside_cycle(gc_cause);  if (should_start) {    do_collection_pause_at_safepoint();  }}bool G1CollectedHeap::is_in(const void* p) const {  return is_in_reserved(p) && _hrm.is_available(addr_to_region(p));}\/\/ Iteration functions.\/\/ Iterates an ObjectClosure over all objects within a G1HeapRegion.class IterateObjectClosureRegionClosure: public G1HeapRegionClosure {  ObjectClosure* _cl;public:  IterateObjectClosureRegionClosure(ObjectClosure* cl) : _cl(cl) {}  bool do_heap_region(G1HeapRegion* r) {    if (!r->is_continues_humongous()) {      r->object_iterate(_cl);    }    return false;  }};void G1CollectedHeap::object_iterate(ObjectClosure* cl) {  IterateObjectClosureRegionClosure blk(cl);  heap_region_iterate(&blk);}class G1ParallelObjectIterator : public ParallelObjectIteratorImpl {private:  G1CollectedHeap*  _heap;  G1HeapRegionClaimer _claimer;public:  G1ParallelObjectIterator(uint thread_num) :      _heap(G1CollectedHeap::heap()),      _claimer(thread_num == 0 ? G1CollectedHeap::heap()->workers()->active_workers() : thread_num) {}  virtual void object_iterate(ObjectClosure* cl, uint worker_id) {    _heap->object_iterate_parallel(cl, worker_id, &_claimer);  }};ParallelObjectIteratorImpl* G1CollectedHeap::parallel_object_iterator(uint thread_num) {  return new G1ParallelObjectIterator(thread_num);}void G1CollectedHeap::object_iterate_parallel(ObjectClosure* cl, uint worker_id, G1HeapRegionClaimer* claimer) {  IterateObjectClosureRegionClosure blk(cl);  heap_region_par_iterate_from_worker_offset(&blk, claimer, worker_id);}void G1CollectedHeap::keep_alive(oop obj) {  G1BarrierSet::enqueue_preloaded(obj);}void G1CollectedHeap::heap_region_iterate(G1HeapRegionClosure* cl) const {  _hrm.iterate(cl);}void G1CollectedHeap::heap_region_iterate(G1HeapRegionIndexClosure* cl) const {  _hrm.iterate(cl);}void G1CollectedHeap::heap_region_par_iterate_from_worker_offset(G1HeapRegionClosure* cl,                                                                 G1HeapRegionClaimer *hrclaimer,                                                                 uint worker_id) const {  _hrm.par_iterate(cl, hrclaimer, hrclaimer->offset_for_worker(worker_id));}void G1CollectedHeap::heap_region_par_iterate_from_start(G1HeapRegionClosure* cl,                                                         G1HeapRegionClaimer *hrclaimer) const {  _hrm.par_iterate(cl, hrclaimer, 0);}void G1CollectedHeap::collection_set_iterate_all(G1HeapRegionClosure* cl) {  _collection_set.iterate(cl);}void G1CollectedHeap::collection_set_par_iterate_all(G1HeapRegionClosure* cl,                                                     G1HeapRegionClaimer* hr_claimer,                                                     uint worker_id) {  _collection_set.par_iterate(cl, hr_claimer, worker_id);}void G1CollectedHeap::collection_set_iterate_increment_from(G1HeapRegionClosure *cl,                                                            G1HeapRegionClaimer* hr_claimer,                                                            uint worker_id) {  _collection_set.iterate_incremental_part_from(cl, hr_claimer, worker_id);}void G1CollectedHeap::par_iterate_regions_array(G1HeapRegionClosure* cl,                                                G1HeapRegionClaimer* hr_claimer,                                                const uint regions[],                                                size_t length,                                                uint worker_id) const {  assert_at_safepoint();  if (length == 0) {    return;  }  uint total_workers = workers()->active_workers();  size_t start_pos = (worker_id * length) \/ total_workers;  size_t cur_pos = start_pos;  do {    uint region_idx = regions[cur_pos];    if (hr_claimer == nullptr || hr_claimer->claim_region(region_idx)) {      G1HeapRegion* r = region_at(region_idx);      bool result = cl->do_heap_region(r);      guarantee(!result, \"Must not cancel iteration\");    }    cur_pos++;    if (cur_pos == length) {      cur_pos = 0;    }  } while (cur_pos != start_pos);}HeapWord* G1CollectedHeap::block_start(const void* addr) const {  G1HeapRegion* hr = heap_region_containing(addr);  \/\/ The CollectedHeap API requires us to not fail for any given address within  \/\/ the heap. G1HeapRegion::block_start() has been optimized to not accept addresses  \/\/ outside of the allocated area.  if (addr >= hr->top()) {    return nullptr;  }  return hr->block_start(addr);}bool G1CollectedHeap::block_is_obj(const HeapWord* addr) const {  G1HeapRegion* hr = heap_region_containing(addr);  return hr->block_is_obj(addr, hr->parsable_bottom_acquire());}size_t G1CollectedHeap::tlab_capacity(Thread* ignored) const {  return (_policy->young_list_target_length() - _survivor.length()) * G1HeapRegion::GrainBytes;}size_t G1CollectedHeap::tlab_used(Thread* ignored) const {  return _eden.length() * G1HeapRegion::GrainBytes;}\/\/ For G1 TLABs should not contain humongous objects, so the maximum TLAB size\/\/ must be equal to the humongous object limit.size_t G1CollectedHeap::max_tlab_size() const {  return align_down(_humongous_object_threshold_in_words, MinObjAlignment);}size_t G1CollectedHeap::unsafe_max_tlab_alloc(Thread* ignored) const {  return _allocator->unsafe_max_tlab_alloc();}size_t G1CollectedHeap::max_capacity() const {  return max_num_regions() * G1HeapRegion::GrainBytes;}void G1CollectedHeap::prepare_for_verify() {  _verifier->prepare_for_verify();}void G1CollectedHeap::verify(VerifyOption vo) {  _verifier->verify(vo);}bool G1CollectedHeap::supports_concurrent_gc_breakpoints() const {  return true;}class G1PrintRegionClosure: public G1HeapRegionClosure {  outputStream* _st;public:  G1PrintRegionClosure(outputStream* st) : _st(st) {}  bool do_heap_region(G1HeapRegion* r) {    r->print_on(_st);    return false;  }};bool G1CollectedHeap::is_obj_dead_cond(const oop obj,                                       const G1HeapRegion* hr,                                       const VerifyOption vo) const {  switch (vo) {    case VerifyOption::G1UseConcMarking: return is_obj_dead(obj, hr);    case VerifyOption::G1UseFullMarking: return is_obj_dead_full(obj, hr);    default:                             ShouldNotReachHere();  }  return false; \/\/ keep some compilers happy}bool G1CollectedHeap::is_obj_dead_cond(const oop obj,                                       const VerifyOption vo) const {  switch (vo) {    case VerifyOption::G1UseConcMarking: return is_obj_dead(obj);    case VerifyOption::G1UseFullMarking: return is_obj_dead_full(obj);    default:                             ShouldNotReachHere();  }  return false; \/\/ keep some compilers happy}void G1CollectedHeap::print_heap_regions() const {  LogTarget(Trace, gc, heap, region) lt;  if (lt.is_enabled()) {    LogStream ls(lt);    print_regions_on(&ls);  }}void G1CollectedHeap::print_heap_on(outputStream* st) const {  size_t heap_used = Heap_lock->owned_by_self() ? used() : used_unlocked();  st->print(\"%-20s\", \"garbage-first heap\");  st->print(\" total reserved %zuK, committed %zuK, used %zuK\",            _hrm.reserved().byte_size()\/K, capacity()\/K, heap_used\/K);  st->print(\" [\" PTR_FORMAT \", \" PTR_FORMAT \")\",            p2i(_hrm.reserved().start()),            p2i(_hrm.reserved().end()));  st->cr();  StreamIndentor si(st, 1);  st->print(\"region size %zuK, \", G1HeapRegion::GrainBytes \/ K);  uint young_regions = young_regions_count();  st->print(\"%u young (%zuK), \", young_regions,            (size_t) young_regions * G1HeapRegion::GrainBytes \/ K);  uint survivor_regions = survivor_regions_count();  st->print(\"%u survivors (%zuK)\", survivor_regions,            (size_t) survivor_regions * G1HeapRegion::GrainBytes \/ K);  st->cr();  if (_numa->is_enabled()) {    uint num_nodes = _numa->num_active_nodes();    st->print(\"remaining free region(s) on each NUMA node: \");    const uint* node_ids = _numa->node_ids();    for (uint node_index = 0; node_index < num_nodes; node_index++) {      uint num_free_regions = _hrm.num_free_regions(node_index);      st->print(\"%u=%u \", node_ids[node_index], num_free_regions);    }    st->cr();  }}void G1CollectedHeap::print_regions_on(outputStream* st) const {  st->print_cr(\"Heap Regions: E=young(eden), S=young(survivor), O=old, \"               \"HS=humongous(starts), HC=humongous(continues), \"               \"CS=collection set, F=free, \"               \"TAMS=top-at-mark-start, \"               \"PB=parsable bottom\");  G1PrintRegionClosure blk(st);  heap_region_iterate(&blk);}void G1CollectedHeap::print_extended_on(outputStream* st) const {  print_heap_on(st);  \/\/ Print the per-region information.  st->cr();  print_regions_on(st);}void G1CollectedHeap::print_gc_on(outputStream* st) const {  \/\/ Print the per-region information.  print_regions_on(st);  st->cr();  BarrierSet* bs = BarrierSet::barrier_set();  if (bs != nullptr) {    bs->print_on(st);  }  if (_cm != nullptr) {    st->cr();    _cm->print_on(st);  }}void G1CollectedHeap::gc_threads_do(ThreadClosure* tc) const {  workers()->threads_do(tc);  tc->do_thread(_cm_thread);  _cm->threads_do(tc);  _cr->threads_do(tc);  tc->do_thread(_service_thread);}void G1CollectedHeap::print_tracing_info() const {  rem_set()->print_summary_info();  concurrent_mark()->print_summary_info();}bool G1CollectedHeap::print_location(outputStream* st, void* addr) const {  return BlockLocationPrinter<G1CollectedHeap>::print_location(st, addr);}G1HeapSummary G1CollectedHeap::create_g1_heap_summary() {  size_t eden_used_bytes = _monitoring_support->eden_space_used();  size_t survivor_used_bytes = _monitoring_support->survivor_space_used();  size_t old_gen_used_bytes = _monitoring_support->old_gen_used();  size_t heap_used = Heap_lock->owned_by_self() ? used() : used_unlocked();  size_t eden_capacity_bytes =    (policy()->young_list_target_length() * G1HeapRegion::GrainBytes) - survivor_used_bytes;  VirtualSpaceSummary heap_summary = create_heap_space_summary();  return G1HeapSummary(heap_summary, heap_used, eden_used_bytes, eden_capacity_bytes,                       survivor_used_bytes, old_gen_used_bytes, num_committed_regions());}G1EvacSummary G1CollectedHeap::create_g1_evac_summary(G1EvacStats* stats) {  return G1EvacSummary(stats->allocated(), stats->wasted(), stats->undo_wasted(),                       stats->unused(), stats->used(), stats->region_end_waste(),                       stats->regions_filled(), stats->num_plab_filled(),                       stats->direct_allocated(), stats->num_direct_allocated(),                       stats->failure_used(), stats->failure_waste());}void G1CollectedHeap::trace_heap(GCWhen::Type when, const GCTracer* gc_tracer) {  const G1HeapSummary& heap_summary = create_g1_heap_summary();  gc_tracer->report_gc_heap_summary(when, heap_summary);  const MetaspaceSummary& metaspace_summary = create_metaspace_summary();  gc_tracer->report_metaspace_summary(when, metaspace_summary);}void G1CollectedHeap::gc_prologue(bool full) {  \/\/ Update common counters.  increment_total_collections(full \/* full gc *\/);  if (full || collector_state()->in_concurrent_start_gc()) {    increment_old_marking_cycles_started();  }}void G1CollectedHeap::gc_epilogue(bool full) {  \/\/ Update common counters.  if (full) {    \/\/ Update the number of full collections that have been completed.    increment_old_marking_cycles_completed(false \/* concurrent *\/, true \/* liveness_completed *\/);  }#if COMPILER2_OR_JVMCI  assert(DerivedPointerTable::is_empty(), \"derived pointer present\");#endif  \/\/ We have just completed a GC. Update the soft reference  \/\/ policy with the new heap occupancy  Universe::heap()->update_capacity_and_used_at_gc();  _collection_pause_end = Ticks::now();  _free_arena_memory_task->notify_new_stats(&_young_gen_card_set_stats,                                            &_collection_set_candidates_card_set_stats);  update_perf_counter_cpu_time();}uint G1CollectedHeap::uncommit_regions(uint region_limit) {  return _hrm.uncommit_inactive_regions(region_limit);}bool G1CollectedHeap::has_uncommittable_regions() {  return _hrm.has_inactive_regions();}void G1CollectedHeap::uncommit_regions_if_necessary() {  if (has_uncommittable_regions()) {    G1UncommitRegionTask::enqueue();  }}void G1CollectedHeap::verify_numa_regions(const char* desc) {  LogTarget(Trace, gc, heap, verify) lt;  if (lt.is_enabled()) {    LogStream ls(lt);    \/\/ Iterate all heap regions to print matching between preferred numa id and actual numa id.    G1NodeIndexCheckClosure cl(desc, _numa, &ls);    heap_region_iterate(&cl);  }}HeapWord* G1CollectedHeap::do_collection_pause(size_t word_size,                                               uint gc_count_before,                                               bool* succeeded,                                               GCCause::Cause gc_cause) {  assert_heap_not_locked_and_not_at_safepoint();  VM_G1CollectForAllocation op(word_size, gc_count_before, gc_cause);  VMThread::execute(&op);  HeapWord* result = op.result();  *succeeded = op.gc_succeeded();  assert(result == nullptr || *succeeded,         \"the result should be null if the VM did not succeed\");  assert_heap_not_locked();  return result;}void G1CollectedHeap::start_concurrent_cycle(bool concurrent_operation_is_full_mark) {  assert(!_cm_thread->in_progress(), \"Can not start concurrent operation while in progress\");  MutexLocker x(CGC_lock, Mutex::_no_safepoint_check_flag);  if (concurrent_operation_is_full_mark) {    _cm->post_concurrent_mark_start();    _cm_thread->start_full_mark();  } else {    _cm->post_concurrent_undo_start();    _cm_thread->start_undo_mark();  }  CGC_lock->notify();}bool G1CollectedHeap::is_potential_eager_reclaim_candidate(G1HeapRegion* r) const {  \/\/ We don't nominate objects with many remembered set entries, on  \/\/ the assumption that such objects are likely still live.  G1HeapRegionRemSet* rem_set = r->rem_set();  return rem_set->occupancy_less_or_equal_than(G1EagerReclaimRemSetThreshold);}#ifndef PRODUCTvoid G1CollectedHeap::verify_region_attr_remset_is_tracked() {  class VerifyRegionAttrRemSet : public G1HeapRegionClosure {  public:    virtual bool do_heap_region(G1HeapRegion* r) {      G1CollectedHeap* g1h = G1CollectedHeap::heap();      bool const remset_is_tracked = g1h->region_attr(r->bottom()).remset_is_tracked();      assert(r->rem_set()->is_tracked() == remset_is_tracked,             \"Region %u remset tracking status (%s) different to region attribute (%s)\",             r->hrm_index(), BOOL_TO_STR(r->rem_set()->is_tracked()), BOOL_TO_STR(remset_is_tracked));      return false;    }  } cl;  heap_region_iterate(&cl);}#endifvoid G1CollectedHeap::update_perf_counter_cpu_time() {  assert(Thread::current()->is_VM_thread(),         \"Must be called from VM thread to avoid races\");  if (!UsePerfData) {    return;  }  \/\/ Ensure ThreadTotalCPUTimeClosure destructor is called before publishing gc  \/\/ time.  {    ThreadTotalCPUTimeClosure tttc(CPUTimeGroups::CPUTimeType::gc_parallel_workers);    \/\/ Currently parallel worker threads never terminate (JDK-8081682), so it is    \/\/ safe for VMThread to read their CPU times. However, if JDK-8087340 is    \/\/ resolved so they terminate, we should rethink if it is still safe.    workers()->threads_do(&tttc);  }  CPUTimeCounters::publish_gc_total_cpu_time();}void G1CollectedHeap::start_new_collection_set() {  collection_set()->start_incremental_building();  clear_region_attr();  guarantee(_eden.length() == 0, \"eden should have been cleared\");  policy()->transfer_survivors_to_cset(survivor());  \/\/ We redo the verification but now wrt to the new CSet which  \/\/ has just got initialized after the previous CSet was freed.  _cm->verify_no_collection_set_oops();}void G1CollectedHeap::verify_before_young_collection(G1HeapVerifier::G1VerifyType type) {  if (!VerifyBeforeGC) {    return;  }  if (!G1HeapVerifier::should_verify(type)) {    return;  }  Ticks start = Ticks::now();  _verifier->prepare_for_verify();  _verifier->verify_region_sets_optional();  _verifier->verify_dirty_young_regions();  _verifier->verify_before_gc();  verify_numa_regions(\"GC Start\");  phase_times()->record_verify_before_time_ms((Ticks::now() - start).seconds() * MILLIUNITS);}void G1CollectedHeap::verify_after_young_collection(G1HeapVerifier::G1VerifyType type) {  if (!VerifyAfterGC) {    return;  }  if (!G1HeapVerifier::should_verify(type)) {    return;  }  Ticks start = Ticks::now();  _verifier->verify_after_gc();  verify_numa_regions(\"GC End\");  _verifier->verify_region_sets_optional();  if (collector_state()->in_concurrent_start_gc()) {    log_debug(gc, verify)(\"Marking state\");    _verifier->verify_marking_state();  }  phase_times()->record_verify_after_time_ms((Ticks::now() - start).seconds() * MILLIUNITS);}void G1CollectedHeap::expand_heap_after_young_collection(){  size_t expand_bytes = _heap_sizing_policy->young_collection_expansion_amount();  if (expand_bytes > 0) {    \/\/ No need for an ergo logging here,    \/\/ expansion_amount() does this when it returns a value > 0.    Ticks expand_start = Ticks::now();    if (expand(expand_bytes, _workers)) {      double expand_ms = (Ticks::now() - expand_start).seconds() * MILLIUNITS;      phase_times()->record_expand_heap_time(expand_ms);    }  }}void G1CollectedHeap::do_collection_pause_at_safepoint() {  assert_at_safepoint_on_vm_thread();  guarantee(!is_stw_gc_active(), \"collection is not reentrant\");  do_collection_pause_at_safepoint_helper();}G1HeapPrinterMark::G1HeapPrinterMark(G1CollectedHeap* g1h) : _g1h(g1h), _heap_transition(g1h) {  \/\/ This summary needs to be printed before incrementing total collections.  _g1h->rem_set()->print_periodic_summary_info(\"Before GC RS summary\",                                               _g1h->total_collections(),                                               true \/* show_thread_times *\/);  _g1h->print_before_gc();  _g1h->print_heap_regions();}G1HeapPrinterMark::~G1HeapPrinterMark() {  _g1h->policy()->print_age_table();  _g1h->rem_set()->print_coarsen_stats();  \/\/ We are at the end of the GC. Total collections has already been increased.  _g1h->rem_set()->print_periodic_summary_info(\"After GC RS summary\",                                               _g1h->total_collections() - 1,                                               false \/* show_thread_times *\/);  _heap_transition.print();  _g1h->print_heap_regions();  _g1h->print_after_gc();  \/\/ Print NUMA statistics.  _g1h->numa()->print_statistics();}G1JFRTracerMark::G1JFRTracerMark(STWGCTimer* timer, GCTracer* tracer) :  _timer(timer), _tracer(tracer) {  _timer->register_gc_start();  _tracer->report_gc_start(G1CollectedHeap::heap()->gc_cause(), _timer->gc_start());  G1CollectedHeap::heap()->trace_heap_before_gc(_tracer);}G1JFRTracerMark::~G1JFRTracerMark() {  G1CollectedHeap::heap()->trace_heap_after_gc(_tracer);  _timer->register_gc_end();  _tracer->report_gc_end(_timer->gc_end(), _timer->time_partitions());}void G1CollectedHeap::prepare_for_mutator_after_young_collection() {  Ticks start = Ticks::now();  _survivor_evac_stats.adjust_desired_plab_size();  _old_evac_stats.adjust_desired_plab_size();  \/\/ Start a new incremental collection set for the mutator phase.  start_new_collection_set();  _allocator->init_mutator_alloc_regions();  phase_times()->record_prepare_for_mutator_time_ms((Ticks::now() - start).seconds() * 1000.0);}void G1CollectedHeap::retire_tlabs() {  ensure_parsability(true);}void G1CollectedHeap::flush_region_pin_cache() {  for (JavaThreadIteratorWithHandle jtiwh; JavaThread *thread = jtiwh.next(); ) {    G1ThreadLocalData::pin_count_cache(thread).flush();  }}void G1CollectedHeap::do_collection_pause_at_safepoint_helper() {  ResourceMark rm;  IsSTWGCActiveMark active_gc_mark;  GCIdMark gc_id_mark;  SvcGCMarker sgcm(SvcGCMarker::MINOR);  GCTraceCPUTime tcpu(_gc_tracer_stw);  _bytes_used_during_gc = 0;  policy()->decide_on_concurrent_start_pause();  \/\/ Record whether this pause may need to trigger a concurrent operation. Later,  \/\/ when we signal the G1ConcurrentMarkThread, the collector state has already  \/\/ been reset for the next pause.  bool should_start_concurrent_mark_operation = collector_state()->in_concurrent_start_gc();  \/\/ Perform the collection.  G1YoungCollector collector(gc_cause());  collector.collect();  \/\/ It should now be safe to tell the concurrent mark thread to start  \/\/ without its logging output interfering with the logging output  \/\/ that came from the pause.  if (should_start_concurrent_mark_operation) {    verifier()->verify_bitmap_clear(true \/* above_tams_only *\/);    \/\/ CAUTION: after the start_concurrent_cycle() call below, the concurrent marking    \/\/ thread(s) could be running concurrently with us. Make sure that anything    \/\/ after this point does not assume that we are the only GC thread running.    \/\/ Note: of course, the actual marking work will not start until the safepoint    \/\/ itself is released in SuspendibleThreadSet::desynchronize().    start_concurrent_cycle(collector.concurrent_operation_is_full_mark());    ConcurrentGCBreakpoints::notify_idle_to_active();  }}void G1CollectedHeap::complete_cleaning(bool class_unloading_occurred) {  uint num_workers = workers()->active_workers();  G1ParallelCleaningTask unlink_task(num_workers, class_unloading_occurred);  workers()->run_task(&unlink_task);}void G1CollectedHeap::unload_classes_and_code(const char* description, BoolObjectClosure* is_alive, GCTimer* timer) {  GCTraceTime(Debug, gc, phases) debug(description, timer);  ClassUnloadingContext ctx(workers()->active_workers(),                            false \/* unregister_nmethods_during_purge *\/,                            false \/* lock_nmethod_free_separately *\/);  {    CodeCache::UnlinkingScope scope(is_alive);    bool unloading_occurred = SystemDictionary::do_unloading(timer);    GCTraceTime(Debug, gc, phases) t(\"G1 Complete Cleaning\", timer);    complete_cleaning(unloading_occurred);  }  {    GCTraceTime(Debug, gc, phases) t(\"Purge Unlinked NMethods\", timer);    ctx.purge_nmethods();  }  {    GCTraceTime(Debug, gc, phases) ur(\"Unregister NMethods\", timer);    G1CollectedHeap::heap()->bulk_unregister_nmethods();  }  {    GCTraceTime(Debug, gc, phases) t(\"Free Code Blobs\", timer);    ctx.free_nmethods();  }  {    GCTraceTime(Debug, gc, phases) t(\"Purge Class Loader Data\", timer);    ClassLoaderDataGraph::purge(true \/* at_safepoint *\/);    DEBUG_ONLY(MetaspaceUtils::verify();)  }}class G1BulkUnregisterNMethodTask : public WorkerTask {  G1HeapRegionClaimer _hrclaimer;  class UnregisterNMethodsHeapRegionClosure : public G1HeapRegionClosure {  public:    bool do_heap_region(G1HeapRegion* hr) {      hr->rem_set()->bulk_remove_code_roots();      return false;    }  } _cl;public:  G1BulkUnregisterNMethodTask(uint num_workers)  : WorkerTask(\"G1 Remove Unlinked NMethods From Code Root Set Task\"),    _hrclaimer(num_workers) { }  void work(uint worker_id) {    G1CollectedHeap::heap()->heap_region_par_iterate_from_worker_offset(&_cl, &_hrclaimer, worker_id);  }};void G1CollectedHeap::bulk_unregister_nmethods() {  uint num_workers = workers()->active_workers();  G1BulkUnregisterNMethodTask t(num_workers);  workers()->run_task(&t);}bool G1STWSubjectToDiscoveryClosure::do_object_b(oop obj) {  assert(obj != nullptr, \"must not be null\");  assert(_g1h->is_in_reserved(obj), \"Trying to discover obj \" PTR_FORMAT \" not in heap\", p2i(obj));  \/\/ The areas the CM and STW ref processor manage must be disjoint. The is_in_cset() below  \/\/ may falsely indicate that this is not the case here: however the collection set only  \/\/ contains old regions when concurrent mark is not running.  return _g1h->is_in_cset(obj) || _g1h->heap_region_containing(obj)->is_survivor();}void G1CollectedHeap::make_pending_list_reachable() {  if (collector_state()->in_concurrent_start_gc()) {    oop pll_head = Universe::reference_pending_list();    if (pll_head != nullptr) {      \/\/ Any valid worker id is fine here as we are in the VM thread and single-threaded.      _cm->mark_in_bitmap(0 \/* worker_id *\/, pll_head);    }  }}void G1CollectedHeap::set_humongous_stats(uint num_humongous_total, uint num_humongous_candidates) {  _num_humongous_objects = num_humongous_total;  _num_humongous_reclaim_candidates = num_humongous_candidates;}bool G1CollectedHeap::should_sample_collection_set_candidates() const {  const G1CollectionSetCandidates* candidates = collection_set()->candidates();  return !candidates->is_empty();}void G1CollectedHeap::set_collection_set_candidates_stats(G1MonotonicArenaMemoryStats& stats) {  _collection_set_candidates_card_set_stats = stats;}void G1CollectedHeap::set_young_gen_card_set_stats(const G1MonotonicArenaMemoryStats& stats) {  _young_gen_card_set_stats = stats;}void G1CollectedHeap::record_obj_copy_mem_stats() {  size_t total_old_allocated = _old_evac_stats.allocated() + _old_evac_stats.direct_allocated();  policy()->old_gen_alloc_tracker()->    add_allocated_bytes_since_last_gc(total_old_allocated * HeapWordSize);  _gc_tracer_stw->report_evacuation_statistics(create_g1_evac_summary(&_survivor_evac_stats),                                               create_g1_evac_summary(&_old_evac_stats));}void G1CollectedHeap::clear_bitmap_for_region(G1HeapRegion* hr) {  concurrent_mark()->clear_bitmap_for_region(hr);}void G1CollectedHeap::free_region(G1HeapRegion* hr, G1FreeRegionList* free_list) {  assert(!hr->is_free(), \"the region should not be free\");  assert(!hr->is_empty(), \"the region should not be empty\");  assert(_hrm.is_available(hr->hrm_index()), \"region should be committed\");  assert(!hr->has_pinned_objects(),         \"must not free a region which contains pinned objects\");  \/\/ Reset region metadata to allow reuse.  hr->hr_clear(true \/* clear_space *\/);  _policy->remset_tracker()->update_at_free(hr);  if (free_list != nullptr) {    free_list->add_ordered(hr);  }}void G1CollectedHeap::retain_region(G1HeapRegion* hr) {  MutexLocker x(G1RareEvent_lock, Mutex::_no_safepoint_check_flag);  collection_set()->candidates()->add_retained_region_unsorted(hr);}void G1CollectedHeap::free_humongous_region(G1HeapRegion* hr,                                            G1FreeRegionList* free_list) {  assert(hr->is_humongous(), \"this is only for humongous regions\");  hr->clear_humongous();  free_region(hr, free_list);}void G1CollectedHeap::remove_from_old_gen_sets(const uint old_regions_removed,                                               const uint humongous_regions_removed) {  if (old_regions_removed > 0 || humongous_regions_removed > 0) {    MutexLocker x(OldSets_lock, Mutex::_no_safepoint_check_flag);    _old_set.bulk_remove(old_regions_removed);    _humongous_set.bulk_remove(humongous_regions_removed);  }}void G1CollectedHeap::prepend_to_freelist(G1FreeRegionList* list) {  assert(list != nullptr, \"list can't be null\");  if (!list->is_empty()) {    MutexLocker x(FreeList_lock, Mutex::_no_safepoint_check_flag);    _hrm.insert_list_into_free_list(list);  }}void G1CollectedHeap::decrement_summary_bytes(size_t bytes) {  decrease_used(bytes);}void G1CollectedHeap::clear_eden() {  _eden.clear();}void G1CollectedHeap::clear_collection_set() {  collection_set()->clear();}void G1CollectedHeap::rebuild_free_region_list() {  Ticks start = Ticks::now();  _hrm.rebuild_free_list(workers());  phase_times()->record_total_rebuild_freelist_time_ms((Ticks::now() - start).seconds() * 1000.0);}class G1AbandonCollectionSetClosure : public G1HeapRegionClosure {public:  virtual bool do_heap_region(G1HeapRegion* r) {    assert(r->in_collection_set(), \"Region %u must have been in collection set\", r->hrm_index());    G1CollectedHeap::heap()->clear_region_attr(r);    r->clear_young_index_in_cset();    return false;  }};void G1CollectedHeap::abandon_collection_set(G1CollectionSet* collection_set) {  G1AbandonCollectionSetClosure cl;  collection_set_iterate_all(&cl);  collection_set->clear();  collection_set->stop_incremental_building();}bool G1CollectedHeap::is_old_gc_alloc_region(G1HeapRegion* hr) {  return _allocator->is_retained_old_region(hr);}void G1CollectedHeap::set_region_short_lived_locked(G1HeapRegion* hr) {  _eden.add(hr);  _policy->set_region_eden(hr);  young_regions_cset_group()->add(hr);}#ifdef ASSERTclass NoYoungRegionsClosure: public G1HeapRegionClosure {private:  bool _success;public:  NoYoungRegionsClosure() : _success(true) { }  bool do_heap_region(G1HeapRegion* r) {    if (r->is_young()) {      log_error(gc, verify)(\"Region [\" PTR_FORMAT \", \" PTR_FORMAT \") tagged as young\",                            p2i(r->bottom()), p2i(r->end()));      _success = false;    }    return false;  }  bool success() { return _success; }};bool G1CollectedHeap::check_young_list_empty() {  bool ret = (young_regions_count() == 0);  NoYoungRegionsClosure closure;  heap_region_iterate(&closure);  ret = ret && closure.success();  return ret;}#endif \/\/ ASSERT\/\/ Remove the given G1HeapRegion from the appropriate region set.void G1CollectedHeap::prepare_region_for_full_compaction(G1HeapRegion* hr) {  if (hr->is_humongous()) {    _humongous_set.remove(hr);  } else if (hr->is_old()) {    _old_set.remove(hr);  } else if (hr->is_young()) {    \/\/ Note that emptying the eden and survivor lists is postponed and instead    \/\/ done as the first step when rebuilding the regions sets again. The reason    \/\/ for this is that during a full GC string deduplication needs to know if    \/\/ a collected region was young or old when the full GC was initiated.    hr->uninstall_surv_rate_group();  } else {    \/\/ We ignore free regions, we'll empty the free list afterwards.    assert(hr->is_free(), \"it cannot be another type\");  }}void G1CollectedHeap::increase_used(size_t bytes) {  _summary_bytes_used += bytes;}void G1CollectedHeap::decrease_used(size_t bytes) {  assert(_summary_bytes_used >= bytes,         \"invariant: _summary_bytes_used: %zu should be >= bytes: %zu\",         _summary_bytes_used, bytes);  _summary_bytes_used -= bytes;}void G1CollectedHeap::set_used(size_t bytes) {  _summary_bytes_used = bytes;}class RebuildRegionSetsClosure : public G1HeapRegionClosure {private:  bool _free_list_only;  G1HeapRegionSet* _old_set;  G1HeapRegionSet* _humongous_set;  G1HeapRegionManager* _hrm;  size_t _total_used;public:  RebuildRegionSetsClosure(bool free_list_only,                           G1HeapRegionSet* old_set,                           G1HeapRegionSet* humongous_set,                           G1HeapRegionManager* hrm) :    _free_list_only(free_list_only), _old_set(old_set),    _humongous_set(humongous_set), _hrm(hrm), _total_used(0) {    assert(_hrm->num_free_regions() == 0, \"pre-condition\");    if (!free_list_only) {      assert(_old_set->is_empty(), \"pre-condition\");      assert(_humongous_set->is_empty(), \"pre-condition\");    }  }  bool do_heap_region(G1HeapRegion* r) {    if (r->is_empty()) {      assert(r->rem_set()->is_empty(), \"Empty regions should have empty remembered sets.\");      \/\/ Add free regions to the free list      r->set_free();      _hrm->insert_into_free_list(r);    } else if (!_free_list_only) {      assert(r->rem_set()->is_empty(), \"At this point remembered sets must have been cleared.\");      if (r->is_humongous()) {        _humongous_set->add(r);      } else {        assert(r->is_young() || r->is_free() || r->is_old(), \"invariant\");        \/\/ We now move all (non-humongous, non-old) regions to old gen,        \/\/ and register them as such.        r->move_to_old();        _old_set->add(r);      }      _total_used += r->used();    }    return false;  }  size_t total_used() {    return _total_used;  }};void G1CollectedHeap::rebuild_region_sets(bool free_list_only) {  assert_at_safepoint_on_vm_thread();  if (!free_list_only) {    _eden.clear();    _survivor.clear();  }  RebuildRegionSetsClosure cl(free_list_only,                              &_old_set, &_humongous_set,                              &_hrm);  heap_region_iterate(&cl);  if (!free_list_only) {    set_used(cl.total_used());  }  assert_used_and_recalculate_used_equal(this);}\/\/ Methods for the mutator alloc regionG1HeapRegion* G1CollectedHeap::new_mutator_alloc_region(size_t word_size,                                                      uint node_index) {  assert_heap_locked_or_at_safepoint(true \/* should_be_vm_thread *\/);  bool should_allocate = policy()->should_allocate_mutator_region();  if (should_allocate) {    G1HeapRegion* new_alloc_region = new_region(word_size,                                                G1HeapRegionType::Eden,                                                false \/* do_expand *\/,                                                node_index);    if (new_alloc_region != nullptr) {      set_region_short_lived_locked(new_alloc_region);      G1HeapRegionPrinter::alloc(new_alloc_region);      _policy->remset_tracker()->update_at_allocate(new_alloc_region);      return new_alloc_region;    }  }  return nullptr;}void G1CollectedHeap::retire_mutator_alloc_region(G1HeapRegion* alloc_region,                                                  size_t allocated_bytes) {  assert_heap_locked_or_at_safepoint(true \/* should_be_vm_thread *\/);  assert(alloc_region->is_eden(), \"all mutator alloc regions should be eden\");  alloc_region->record_activity();  \/\/ Record the activity of the alloc region  collection_set()->add_eden_region(alloc_region);  increase_used(allocated_bytes);  _eden.add_used_bytes(allocated_bytes);  G1HeapRegionPrinter::retire(alloc_region);  \/\/ We update the eden sizes here, when the region is retired,  \/\/ instead of when it's allocated, since this is the point that its  \/\/ used space has been recorded in _summary_bytes_used.  monitoring_support()->update_eden_size();}\/\/ Methods for the GC alloc regionsbool G1CollectedHeap::has_more_regions(G1HeapRegionAttr dest) {  if (dest.is_old()) {    return true;  } else {    return survivor_regions_count() < policy()->max_survivor_regions();  }}G1HeapRegion* G1CollectedHeap::new_gc_alloc_region(size_t word_size, G1HeapRegionAttr dest, uint node_index) {  assert(FreeList_lock->owned_by_self(), \"pre-condition\");  if (!has_more_regions(dest)) {    return nullptr;  }  G1HeapRegionType type;  if (dest.is_young()) {    type = G1HeapRegionType::Survivor;  } else {    type = G1HeapRegionType::Old;  }  G1HeapRegion* new_alloc_region = new_region(word_size,                                              type,                                              true \/* do_expand *\/,                                              node_index);  if (new_alloc_region != nullptr) {    if (type.is_survivor()) {      new_alloc_region->set_survivor();      _survivor.add(new_alloc_region);      register_new_survivor_region_with_region_attr(new_alloc_region);      \/\/ Install the group cardset.      young_regions_cset_group()->add(new_alloc_region);    } else {      new_alloc_region->set_old();    }    _policy->remset_tracker()->update_at_allocate(new_alloc_region);    register_region_with_region_attr(new_alloc_region);    G1HeapRegionPrinter::alloc(new_alloc_region);    return new_alloc_region;  }  return nullptr;}void G1CollectedHeap::retire_gc_alloc_region(G1HeapRegion* alloc_region,                                             size_t allocated_bytes,                                             G1HeapRegionAttr dest) {  alloc_region->record_activity();  \/\/ Record the activity of the alloc region  _bytes_used_during_gc += allocated_bytes;  if (dest.is_old()) {    old_set_add(alloc_region);  } else {    assert(dest.is_young(), \"Retiring alloc region should be young (%d)\", dest.type());    _survivor.add_used_bytes(allocated_bytes);  }  bool const during_im = collector_state()->in_concurrent_start_gc();  if (during_im && allocated_bytes > 0) {    _cm->add_root_region(alloc_region);  }  G1HeapRegionPrinter::retire(alloc_region);}void G1CollectedHeap::mark_evac_failure_object(uint worker_id, const oop obj, size_t obj_size) const {  assert(!_cm->is_marked_in_bitmap(obj), \"must be\");  _cm->raw_mark_in_bitmap(obj);}\/\/ Optimized nmethod scanningclass RegisterNMethodOopClosure: public OopClosure {  G1CollectedHeap* _g1h;  nmethod* _nm;public:  RegisterNMethodOopClosure(G1CollectedHeap* g1h, nmethod* nm) :    _g1h(g1h), _nm(nm) {}  void do_oop(oop* p) {    oop heap_oop = RawAccess<>::oop_load(p);    if (!CompressedOops::is_null(heap_oop)) {      oop obj = CompressedOops::decode_not_null(heap_oop);      G1HeapRegion* hr = _g1h->heap_region_containing(obj);      assert(!hr->is_continues_humongous(),             \"trying to add code root \" PTR_FORMAT \" in continuation of humongous region \" HR_FORMAT             \" starting at \" HR_FORMAT,             p2i(_nm), HR_FORMAT_PARAMS(hr), HR_FORMAT_PARAMS(hr->humongous_start_region()));      hr->add_code_root(_nm);    }  }  void do_oop(narrowOop* p) { ShouldNotReachHere(); }};void G1CollectedHeap::register_nmethod(nmethod* nm) {  guarantee(nm != nullptr, \"sanity\");  RegisterNMethodOopClosure reg_cl(this, nm);  nm->oops_do(&reg_cl);}void G1CollectedHeap::unregister_nmethod(nmethod* nm) {  \/\/ We always unregister nmethods in bulk during code unloading only.  ShouldNotReachHere();}void G1CollectedHeap::update_used_after_gc(bool evacuation_failed) {  if (evacuation_failed) {    set_used(recalculate_used());  } else {    \/\/ The \"used\" of the collection set have already been subtracted    \/\/ when they were freed.  Add in the bytes used.    increase_used(_bytes_used_during_gc);  }}class RebuildCodeRootClosure: public NMethodClosure {  G1CollectedHeap* _g1h;public:  RebuildCodeRootClosure(G1CollectedHeap* g1h) :    _g1h(g1h) {}  void do_nmethod(nmethod* nm) {    assert(nm != nullptr, \"Sanity\");    _g1h->register_nmethod(nm);  }};void G1CollectedHeap::rebuild_code_roots() {  RebuildCodeRootClosure nmethod_cl(this);  CodeCache::nmethods_do(&nmethod_cl);}void G1CollectedHeap::initialize_serviceability() {  _monitoring_support->initialize_serviceability();}MemoryUsage G1CollectedHeap::memory_usage() {  return _monitoring_support->memory_usage();}GrowableArray<GCMemoryManager*> G1CollectedHeap::memory_managers() {  return _monitoring_support->memory_managers();}GrowableArray<MemoryPool*> G1CollectedHeap::memory_pools() {  return _monitoring_support->memory_pools();}void G1CollectedHeap::fill_with_dummy_object(HeapWord* start, HeapWord* end, bool zap) {  G1HeapRegion* region = heap_region_containing(start);  region->fill_with_dummy_object(start, pointer_delta(end, start), zap);}void G1CollectedHeap::start_codecache_marking_cycle_if_inactive(bool concurrent_mark_start) {  \/\/ We can reach here with an active code cache marking cycle either because the  \/\/ previous G1 concurrent marking cycle was undone (if heap occupancy after the  \/\/ concurrent start young collection was below the threshold) or aborted. See  \/\/ CodeCache::on_gc_marking_cycle_finish() why this is.  We must not start a new code  \/\/ cache cycle then. If we are about to start a new g1 concurrent marking cycle we  \/\/ still have to arm all nmethod entry barriers. They are needed for adding oop  \/\/ constants to the SATB snapshot. Full GC does not need nmethods to be armed.  if (!CodeCache::is_gc_marking_cycle_active()) {    CodeCache::on_gc_marking_cycle_start();  }  if (concurrent_mark_start) {    CodeCache::arm_all_nmethods();  }}void G1CollectedHeap::finish_codecache_marking_cycle() {  CodeCache::on_gc_marking_cycle_finish();  CodeCache::arm_all_nmethods();}void G1CollectedHeap::prepare_group_cardsets_for_scan() {  young_regions_cardset()->reset_table_scanner_for_groups();  collection_set()->prepare_groups_for_scan();}\n\\ No newline at end of file\n","filename":"src\/hotspot\/share\/gc\/g1\/g1CollectedHeap.cpp","additions":1,"deletions":3110,"binary":false,"changes":3111,"status":"modified"},{"patch":"@@ -1,1346 +1,1 @@\n-\/*\n- * Copyright (c) 2001, 2025, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\n- *\/\n-\n-#ifndef SHARE_GC_G1_G1COLLECTEDHEAP_HPP\n-#define SHARE_GC_G1_G1COLLECTEDHEAP_HPP\n-\n-#include \"gc\/g1\/g1BarrierSet.hpp\"\n-#include \"gc\/g1\/g1BiasedArray.hpp\"\n-#include \"gc\/g1\/g1CardSet.hpp\"\n-#include \"gc\/g1\/g1CardTable.hpp\"\n-#include \"gc\/g1\/g1CollectionSet.hpp\"\n-#include \"gc\/g1\/g1CollectorState.hpp\"\n-#include \"gc\/g1\/g1ConcurrentMark.hpp\"\n-#include \"gc\/g1\/g1EdenRegions.hpp\"\n-#include \"gc\/g1\/g1EvacStats.hpp\"\n-#include \"gc\/g1\/g1GCPauseType.hpp\"\n-#include \"gc\/g1\/g1HeapRegionAttr.hpp\"\n-#include \"gc\/g1\/g1HeapRegionManager.hpp\"\n-#include \"gc\/g1\/g1HeapRegionSet.hpp\"\n-#include \"gc\/g1\/g1HeapTransition.hpp\"\n-#include \"gc\/g1\/g1HeapVerifier.hpp\"\n-#include \"gc\/g1\/g1MonitoringSupport.hpp\"\n-#include \"gc\/g1\/g1MonotonicArenaFreeMemoryTask.hpp\"\n-#include \"gc\/g1\/g1MonotonicArenaFreePool.hpp\"\n-#include \"gc\/g1\/g1NUMA.hpp\"\n-#include \"gc\/g1\/g1SurvivorRegions.hpp\"\n-#include \"gc\/g1\/g1YoungGCAllocationFailureInjector.hpp\"\n-#include \"gc\/shared\/barrierSet.hpp\"\n-#include \"gc\/shared\/collectedHeap.hpp\"\n-#include \"gc\/shared\/gcHeapSummary.hpp\"\n-#include \"gc\/shared\/plab.hpp\"\n-#include \"gc\/shared\/softRefPolicy.hpp\"\n-#include \"gc\/shared\/taskqueue.hpp\"\n-#include \"memory\/allocation.hpp\"\n-#include \"memory\/iterator.hpp\"\n-#include \"memory\/memRegion.hpp\"\n-#include \"runtime\/mutexLocker.hpp\"\n-#include \"runtime\/threadSMR.hpp\"\n-#include \"utilities\/bitMap.hpp\"\n-\n-\/\/ A \"G1CollectedHeap\" is an implementation of a java heap for HotSpot.\n-\/\/ It uses the \"Garbage First\" heap organization and algorithm, which\n-\/\/ may combine concurrent marking with parallel, incremental compaction of\n-\/\/ heap subsets that will yield large amounts of garbage.\n-\n-\/\/ Forward declarations\n-class G1Allocator;\n-class G1BatchedTask;\n-class G1CardTableEntryClosure;\n-class G1ConcurrentMark;\n-class G1ConcurrentMarkThread;\n-class G1ConcurrentRefine;\n-class G1GCCounters;\n-class G1GCPhaseTimes;\n-class G1HeapSizingPolicy;\n-class G1NewTracer;\n-class G1RemSet;\n-class G1ServiceTask;\n-class G1ServiceThread;\n-class GCMemoryManager;\n-class G1HeapRegion;\n-class MemoryPool;\n-class nmethod;\n-class PartialArrayStateManager;\n-class ReferenceProcessor;\n-class STWGCTimer;\n-class WorkerThreads;\n-\n-typedef OverflowTaskQueue<ScannerTask, mtGC>           G1ScannerTasksQueue;\n-typedef GenericTaskQueueSet<G1ScannerTasksQueue, mtGC> G1ScannerTasksQueueSet;\n-\n-typedef int RegionIdx_t;   \/\/ needs to hold [ 0..max_num_regions() )\n-typedef int CardIdx_t;     \/\/ needs to hold [ 0..CardsPerRegion )\n-\n-\/\/ The G1 STW is alive closure.\n-\/\/ An instance is embedded into the G1CH and used as the\n-\/\/ (optional) _is_alive_non_header closure in the STW\n-\/\/ reference processor. It is also extensively used during\n-\/\/ reference processing during STW evacuation pauses.\n-class G1STWIsAliveClosure : public BoolObjectClosure {\n-  G1CollectedHeap* _g1h;\n-public:\n-  G1STWIsAliveClosure(G1CollectedHeap* g1h) : _g1h(g1h) {}\n-  bool do_object_b(oop p) override;\n-};\n-\n-class G1STWSubjectToDiscoveryClosure : public BoolObjectClosure {\n-  G1CollectedHeap* _g1h;\n-public:\n-  G1STWSubjectToDiscoveryClosure(G1CollectedHeap* g1h) : _g1h(g1h) {}\n-  bool do_object_b(oop p) override;\n-};\n-\n-class G1RegionMappingChangedListener : public G1MappingChangedListener {\n- private:\n-  void reset_from_card_cache(uint start_idx, size_t num_regions);\n- public:\n-  void on_commit(uint start_idx, size_t num_regions, bool zero_filled) override;\n-};\n-\n-\/\/ Helper to claim contiguous sets of JavaThread for processing by multiple threads.\n-class G1JavaThreadsListClaimer : public StackObj {\n-  ThreadsListHandle _list;\n-  uint _claim_step;\n-\n-  volatile uint _cur_claim;\n-\n-  \/\/ Attempts to claim _claim_step JavaThreads, returning an array of claimed\n-  \/\/ JavaThread* with count elements. Returns null (and a zero count) if there\n-  \/\/ are no more threads to claim.\n-  JavaThread* const* claim(uint& count);\n-\n-public:\n-  G1JavaThreadsListClaimer(uint claim_step) : _list(), _claim_step(claim_step), _cur_claim(0) {\n-    assert(claim_step > 0, \"must be\");\n-  }\n-\n-  \/\/ Executes the given closure on the elements of the JavaThread list, chunking the\n-  \/\/ JavaThread set in claim_step chunks for each caller to reduce parallelization\n-  \/\/ overhead.\n-  void apply(ThreadClosure* cl);\n-\n-  \/\/ Total number of JavaThreads that can be claimed.\n-  uint length() const { return _list.length(); }\n-};\n-\n-class G1CollectedHeap : public CollectedHeap {\n-  friend class VM_G1CollectForAllocation;\n-  friend class VM_G1CollectFull;\n-  friend class VM_G1TryInitiateConcMark;\n-  friend class VMStructs;\n-  friend class MutatorAllocRegion;\n-  friend class G1FullCollector;\n-  friend class G1GCAllocRegion;\n-  friend class G1HeapVerifier;\n-\n-  friend class G1YoungGCVerifierMark;\n-\n-  \/\/ Closures used in implementation.\n-  friend class G1EvacuateRegionsTask;\n-  friend class G1PLABAllocator;\n-\n-  \/\/ Other related classes.\n-  friend class G1HeapPrinterMark;\n-  friend class G1HeapRegionClaimer;\n-\n-  \/\/ Testing classes.\n-  friend class G1CheckRegionAttrTableClosure;\n-\n-private:\n-  G1ServiceThread* _service_thread;\n-  G1ServiceTask* _periodic_gc_task;\n-  G1MonotonicArenaFreeMemoryTask* _free_arena_memory_task;\n-\n-  WorkerThreads* _workers;\n-  G1CardTable* _card_table;\n-\n-  Ticks _collection_pause_end;\n-\n-  static size_t _humongous_object_threshold_in_words;\n-\n-  \/\/ These sets keep track of old and humongous regions respectively.\n-  G1HeapRegionSet _old_set;\n-  G1HeapRegionSet _humongous_set;\n-\n-  \/\/ Young gen memory statistics before GC.\n-  G1MonotonicArenaMemoryStats _young_gen_card_set_stats;\n-  \/\/ Collection set candidates memory statistics after GC.\n-  G1MonotonicArenaMemoryStats _collection_set_candidates_card_set_stats;\n-\n-  \/\/ The block offset table for the G1 heap.\n-  G1BlockOffsetTable* _bot;\n-\n-public:\n-  void rebuild_free_region_list();\n-  \/\/ Start a new incremental collection set for the next pause.\n-  void start_new_collection_set();\n-\n-  void prepare_region_for_full_compaction(G1HeapRegion* hr);\n-\n-private:\n-  \/\/ Rebuilds the region sets \/ lists so that they are repopulated to\n-  \/\/ reflect the contents of the heap. The only exception is the\n-  \/\/ humongous set which was not torn down in the first place. If\n-  \/\/ free_list_only is true, it will only rebuild the free list.\n-  void rebuild_region_sets(bool free_list_only);\n-\n-  \/\/ Callback for region mapping changed events.\n-  G1RegionMappingChangedListener _listener;\n-\n-  \/\/ Handle G1 NUMA support.\n-  G1NUMA* _numa;\n-\n-  \/\/ The sequence of all heap regions in the heap.\n-  G1HeapRegionManager _hrm;\n-\n-  \/\/ Manages all allocations with regions except humongous object allocations.\n-  G1Allocator* _allocator;\n-\n-  G1YoungGCAllocationFailureInjector _allocation_failure_injector;\n-\n-  \/\/ Manages all heap verification.\n-  G1HeapVerifier* _verifier;\n-\n-  \/\/ Outside of GC pauses, the number of bytes used in all regions other\n-  \/\/ than the current allocation region(s).\n-  volatile size_t _summary_bytes_used;\n-\n-  void increase_used(size_t bytes);\n-  void decrease_used(size_t bytes);\n-\n-  void set_used(size_t bytes);\n-\n-  \/\/ Number of bytes used in all regions during GC. Typically changed when\n-  \/\/ retiring a GC alloc region.\n-  size_t _bytes_used_during_gc;\n-\n-public:\n-  size_t bytes_used_during_gc() const { return _bytes_used_during_gc; }\n-\n-private:\n-  \/\/ GC allocation statistics policy for survivors.\n-  G1EvacStats _survivor_evac_stats;\n-\n-  \/\/ GC allocation statistics policy for tenured objects.\n-  G1EvacStats _old_evac_stats;\n-\n-  \/\/ Helper for monitoring and management support.\n-  G1MonitoringSupport* _monitoring_support;\n-\n-  uint _num_humongous_objects; \/\/ Current amount of (all) humongous objects found in the heap.\n-  uint _num_humongous_reclaim_candidates; \/\/ Number of humongous object eager reclaim candidates.\n-public:\n-  uint num_humongous_objects() const { return _num_humongous_objects; }\n-  uint num_humongous_reclaim_candidates() const { return _num_humongous_reclaim_candidates; }\n-  bool has_humongous_reclaim_candidates() const { return _num_humongous_reclaim_candidates > 0; }\n-\n-  void set_humongous_stats(uint num_humongous_total, uint num_humongous_candidates);\n-\n-  bool should_sample_collection_set_candidates() const;\n-  void set_collection_set_candidates_stats(G1MonotonicArenaMemoryStats& stats);\n-  void set_young_gen_card_set_stats(const G1MonotonicArenaMemoryStats& stats);\n-\n-  void update_perf_counter_cpu_time();\n-private:\n-\n-  \/\/ Return true if an explicit GC should start a concurrent cycle instead\n-  \/\/ of doing a STW full GC. A concurrent cycle should be started if:\n-  \/\/ (a) cause == _g1_humongous_allocation,\n-  \/\/ (b) cause == _java_lang_system_gc and +ExplicitGCInvokesConcurrent,\n-  \/\/ (c) cause == _dcmd_gc_run and +ExplicitGCInvokesConcurrent,\n-  \/\/ (d) cause == _wb_breakpoint,\n-  \/\/ (e) cause == _g1_periodic_collection and +G1PeriodicGCInvokesConcurrent.\n-  bool should_do_concurrent_full_gc(GCCause::Cause cause);\n-\n-  \/\/ Attempt to start a concurrent cycle with the indicated cause.\n-  \/\/ precondition: should_do_concurrent_full_gc(cause)\n-  bool try_collect_concurrently(GCCause::Cause cause,\n-                                uint gc_counter,\n-                                uint old_marking_started_before);\n-\n-  \/\/ indicates whether we are in young or mixed GC mode\n-  G1CollectorState _collector_state;\n-\n-  \/\/ Keeps track of how many \"old marking cycles\" (i.e., Full GCs or\n-  \/\/ concurrent cycles) we have started.\n-  volatile uint _old_marking_cycles_started;\n-\n-  \/\/ Keeps track of how many \"old marking cycles\" (i.e., Full GCs or\n-  \/\/ concurrent cycles) we have completed.\n-  volatile uint _old_marking_cycles_completed;\n-\n-  \/\/ Create a memory mapper for auxiliary data structures of the given size and\n-  \/\/ translation factor.\n-  static G1RegionToSpaceMapper* create_aux_memory_mapper(const char* description,\n-                                                         size_t size,\n-                                                         size_t translation_factor);\n-\n-  void trace_heap(GCWhen::Type when, const GCTracer* tracer) override;\n-\n-  \/\/ These are macros so that, if the assert fires, we get the correct\n-  \/\/ line number, file, etc.\n-\n-#define heap_locking_asserts_params(_extra_message_)                          \\\n-  \"%s : Heap_lock locked: %s, at safepoint: %s, is VM thread: %s\",            \\\n-  (_extra_message_),                                                          \\\n-  BOOL_TO_STR(Heap_lock->owned_by_self()),                                    \\\n-  BOOL_TO_STR(SafepointSynchronize::is_at_safepoint()),                       \\\n-  BOOL_TO_STR(Thread::current()->is_VM_thread())\n-\n-#define assert_heap_locked()                                                  \\\n-  do {                                                                        \\\n-    assert(Heap_lock->owned_by_self(),                                        \\\n-           heap_locking_asserts_params(\"should be holding the Heap_lock\"));   \\\n-  } while (0)\n-\n-#define assert_heap_locked_or_at_safepoint(_should_be_vm_thread_)             \\\n-  do {                                                                        \\\n-    assert(Heap_lock->owned_by_self() ||                                      \\\n-           (SafepointSynchronize::is_at_safepoint() &&                        \\\n-             ((_should_be_vm_thread_) == Thread::current()->is_VM_thread())), \\\n-           heap_locking_asserts_params(\"should be holding the Heap_lock or \"  \\\n-                                        \"should be at a safepoint\"));         \\\n-  } while (0)\n-\n-#define assert_heap_locked_and_not_at_safepoint()                             \\\n-  do {                                                                        \\\n-    assert(Heap_lock->owned_by_self() &&                                      \\\n-                                    !SafepointSynchronize::is_at_safepoint(), \\\n-          heap_locking_asserts_params(\"should be holding the Heap_lock and \"  \\\n-                                       \"should not be at a safepoint\"));      \\\n-  } while (0)\n-\n-#define assert_heap_not_locked()                                              \\\n-  do {                                                                        \\\n-    assert(!Heap_lock->owned_by_self(),                                       \\\n-        heap_locking_asserts_params(\"should not be holding the Heap_lock\"));  \\\n-  } while (0)\n-\n-#define assert_heap_not_locked_and_not_at_safepoint()                         \\\n-  do {                                                                        \\\n-    assert(!Heap_lock->owned_by_self() &&                                     \\\n-                                    !SafepointSynchronize::is_at_safepoint(), \\\n-      heap_locking_asserts_params(\"should not be holding the Heap_lock and \"  \\\n-                                   \"should not be at a safepoint\"));          \\\n-  } while (0)\n-\n-#define assert_at_safepoint_on_vm_thread()                                        \\\n-  do {                                                                            \\\n-    assert_at_safepoint();                                                        \\\n-    assert(Thread::current_or_null() != nullptr, \"no current thread\");            \\\n-    assert(Thread::current()->is_VM_thread(), \"current thread is not VM thread\"); \\\n-  } while (0)\n-\n-#ifdef ASSERT\n-#define assert_used_and_recalculate_used_equal(g1h)                           \\\n-  do {                                                                        \\\n-    size_t cur_used_bytes = g1h->used();                                      \\\n-    size_t recal_used_bytes = g1h->recalculate_used();                        \\\n-    assert(cur_used_bytes == recal_used_bytes, \"Used(%zu) is not\" \\\n-           \" same as recalculated used(%zu).\",                    \\\n-           cur_used_bytes, recal_used_bytes);                                 \\\n-  } while (0)\n-#else\n-#define assert_used_and_recalculate_used_equal(g1h) do {} while(0)\n-#endif\n-\n-  \/\/ The young region list.\n-  G1EdenRegions _eden;\n-  G1SurvivorRegions _survivor;\n-\n-  STWGCTimer* _gc_timer_stw;\n-\n-  G1NewTracer* _gc_tracer_stw;\n-\n-  \/\/ The current policy object for the collector.\n-  G1Policy* _policy;\n-  G1HeapSizingPolicy* _heap_sizing_policy;\n-\n-  G1CollectionSet _collection_set;\n-\n-  \/\/ Try to allocate a single non-humongous G1HeapRegion sufficient for\n-  \/\/ an allocation of the given word_size. If do_expand is true,\n-  \/\/ attempt to expand the heap if necessary to satisfy the allocation\n-  \/\/ request. 'type' takes the type of region to be allocated. (Use constants\n-  \/\/ Old, Eden, Humongous, Survivor defined in G1HeapRegionType.)\n-  G1HeapRegion* new_region(size_t word_size,\n-                           G1HeapRegionType type,\n-                           bool do_expand,\n-                           uint node_index = G1NUMA::AnyNodeIndex);\n-\n-  \/\/ Initialize a contiguous set of free regions of length num_regions\n-  \/\/ and starting at index first so that they appear as a single\n-  \/\/ humongous region.\n-  HeapWord* humongous_obj_allocate_initialize_regions(G1HeapRegion* first_hr,\n-                                                      uint num_regions,\n-                                                      size_t word_size);\n-\n-  \/\/ Attempt to allocate a humongous object of the given size. Return\n-  \/\/ null if unsuccessful.\n-  HeapWord* humongous_obj_allocate(size_t word_size);\n-\n-  \/\/ The following two methods, allocate_new_tlab() and\n-  \/\/ mem_allocate(), are the two main entry points from the runtime\n-  \/\/ into the G1's allocation routines. They have the following\n-  \/\/ assumptions:\n-  \/\/\n-  \/\/ * They should both be called outside safepoints.\n-  \/\/\n-  \/\/ * They should both be called without holding the Heap_lock.\n-  \/\/\n-  \/\/ * All allocation requests for new TLABs should go to\n-  \/\/   allocate_new_tlab().\n-  \/\/\n-  \/\/ * All non-TLAB allocation requests should go to mem_allocate().\n-  \/\/\n-  \/\/ * If either call cannot satisfy the allocation request using the\n-  \/\/   current allocating region, they will try to get a new one. If\n-  \/\/   this fails, they will attempt to do an evacuation pause and\n-  \/\/   retry the allocation.\n-  \/\/\n-  \/\/ * If all allocation attempts fail, even after trying to schedule\n-  \/\/   an evacuation pause, allocate_new_tlab() will return null,\n-  \/\/   whereas mem_allocate() will attempt a heap expansion and\/or\n-  \/\/   schedule a Full GC.\n-  \/\/\n-  \/\/ * We do not allow humongous-sized TLABs. So, allocate_new_tlab\n-  \/\/   should never be called with word_size being humongous. All\n-  \/\/   humongous allocation requests should go to mem_allocate() which\n-  \/\/   will satisfy them with a special path.\n-\n-  HeapWord* allocate_new_tlab(size_t min_size,\n-                              size_t requested_size,\n-                              size_t* actual_size) override;\n-\n-  HeapWord* mem_allocate(size_t word_size,\n-                         bool*  gc_overhead_limit_was_exceeded) override;\n-\n-  \/\/ First-level mutator allocation attempt: try to allocate out of\n-  \/\/ the mutator alloc region without taking the Heap_lock. This\n-  \/\/ should only be used for non-humongous allocations.\n-  inline HeapWord* attempt_allocation(size_t min_word_size,\n-                                      size_t desired_word_size,\n-                                      size_t* actual_word_size);\n-\n-  \/\/ Second-level mutator allocation attempt: take the Heap_lock and\n-  \/\/ retry the allocation attempt, potentially scheduling a GC\n-  \/\/ pause. This should only be used for non-humongous allocations.\n-  HeapWord* attempt_allocation_slow(uint node_index, size_t word_size);\n-\n-  \/\/ Takes the Heap_lock and attempts a humongous allocation. It can\n-  \/\/ potentially schedule a GC pause.\n-  HeapWord* attempt_allocation_humongous(size_t word_size);\n-\n-  \/\/ Allocation attempt that should be called during safepoints (e.g.,\n-  \/\/ at the end of a successful GC). expect_null_mutator_alloc_region\n-  \/\/ specifies whether the mutator alloc region is expected to be null\n-  \/\/ or not.\n-  HeapWord* attempt_allocation_at_safepoint(size_t word_size,\n-                                            bool expect_null_mutator_alloc_region);\n-\n-  \/\/ These methods are the \"callbacks\" from the G1AllocRegion class.\n-\n-  \/\/ For mutator alloc regions.\n-  G1HeapRegion* new_mutator_alloc_region(size_t word_size, uint node_index);\n-  void retire_mutator_alloc_region(G1HeapRegion* alloc_region,\n-                                   size_t allocated_bytes);\n-\n-  \/\/ For GC alloc regions.\n-  bool has_more_regions(G1HeapRegionAttr dest);\n-  G1HeapRegion* new_gc_alloc_region(size_t word_size, G1HeapRegionAttr dest, uint node_index);\n-  void retire_gc_alloc_region(G1HeapRegion* alloc_region,\n-                              size_t allocated_bytes, G1HeapRegionAttr dest);\n-\n-  void resize_heap(size_t resize_bytes, bool should_expand);\n-\n-  \/\/ - if clear_all_soft_refs is true, all soft references should be\n-  \/\/   cleared during the GC.\n-  \/\/ - if do_maximal_compaction is true, full gc will do a maximally\n-  \/\/   compacting collection, leaving no dead wood.\n-  \/\/ - if allocation_word_size is set, then this allocation size will\n-  \/\/    be accounted for in case shrinking of the heap happens.\n-  \/\/ - it returns false if it is unable to do the collection due to the\n-  \/\/   GC locker being active, true otherwise.\n-  void do_full_collection(bool clear_all_soft_refs,\n-                          bool do_maximal_compaction,\n-                          size_t allocation_word_size);\n-\n-  \/\/ Callback from VM_G1CollectFull operation, or collect_as_vm_thread.\n-  void do_full_collection(bool clear_all_soft_refs) override;\n-\n-  \/\/ Helper to do a full collection that clears soft references.\n-  void upgrade_to_full_collection();\n-\n-  \/\/ Callback from VM_G1CollectForAllocation operation.\n-  \/\/ This function does everything necessary\/possible to satisfy a\n-  \/\/ failed allocation request (including collection, expansion, etc.)\n-  HeapWord* satisfy_failed_allocation(size_t word_size);\n-  \/\/ Internal helpers used during full GC to split it up to\n-  \/\/ increase readability.\n-  bool abort_concurrent_cycle();\n-  void verify_before_full_collection();\n-  void prepare_heap_for_full_collection();\n-  void prepare_for_mutator_after_full_collection(size_t allocation_word_size);\n-  void abort_refinement();\n-  void verify_after_full_collection();\n-  void print_heap_after_full_collection();\n-\n-  \/\/ Helper method for satisfy_failed_allocation()\n-  HeapWord* satisfy_failed_allocation_helper(size_t word_size,\n-                                             bool do_gc,\n-                                             bool maximal_compaction,\n-                                             bool expect_null_mutator_alloc_region);\n-\n-  \/\/ Attempting to expand the heap sufficiently\n-  \/\/ to support an allocation of the given \"word_size\".  If\n-  \/\/ successful, perform the allocation and return the address of the\n-  \/\/ allocated block, or else null.\n-  HeapWord* expand_and_allocate(size_t word_size);\n-\n-  void verify_numa_regions(const char* desc);\n-\n-public:\n-  \/\/ If during a concurrent start pause we may install a pending list head which is not\n-  \/\/ otherwise reachable, ensure that it is marked in the bitmap for concurrent marking\n-  \/\/ to discover.\n-  void make_pending_list_reachable();\n-\n-  G1ServiceThread* service_thread() const { return _service_thread; }\n-\n-  WorkerThreads* workers() const { return _workers; }\n-\n-  \/\/ Run the given batch task using the workers.\n-  void run_batch_task(G1BatchedTask* cl);\n-\n-  \/\/ Return \"optimal\" number of chunks per region we want to use for claiming areas\n-  \/\/ within a region to claim.\n-  \/\/ The returned value is a trade-off between granularity of work distribution and\n-  \/\/ memory usage and maintenance costs of that table.\n-  \/\/ Testing showed that 64 for 1M\/2M region, 128 for 4M\/8M regions, 256 for 16\/32M regions,\n-  \/\/ and so on seems to be such a good trade-off.\n-  static uint get_chunks_per_region();\n-\n-  G1Allocator* allocator() {\n-    return _allocator;\n-  }\n-\n-  G1YoungGCAllocationFailureInjector* allocation_failure_injector() { return &_allocation_failure_injector; }\n-\n-  G1HeapVerifier* verifier() {\n-    return _verifier;\n-  }\n-\n-  G1MonitoringSupport* monitoring_support() {\n-    assert(_monitoring_support != nullptr, \"should have been initialized\");\n-    return _monitoring_support;\n-  }\n-\n-  void pin_object(JavaThread* thread, oop obj) override;\n-  void unpin_object(JavaThread* thread, oop obj) override;\n-\n-  void resize_heap_after_young_collection(size_t allocation_word_size);\n-  void resize_heap_after_full_collection(size_t allocation_word_size);\n-\n-  \/\/ Check if there is memory to uncommit and if so schedule a task to do it.\n-  void uncommit_regions_if_necessary();\n-  \/\/ Immediately uncommit uncommittable regions.\n-  uint uncommit_regions(uint region_limit);\n-  bool has_uncommittable_regions();\n-\n-  G1NUMA* numa() const { return _numa; }\n-\n-  \/\/ Expand the garbage-first heap by at least the given size (in bytes!).\n-  \/\/ Returns true if the heap was expanded by the requested amount;\n-  \/\/ false otherwise.\n-  \/\/ (Rounds up to a G1HeapRegion boundary.)\n-  bool expand(size_t expand_bytes, WorkerThreads* pretouch_workers);\n-  bool expand_single_region(uint node_index);\n-\n-  \/\/ Returns the PLAB statistics for a given destination.\n-  inline G1EvacStats* alloc_buffer_stats(G1HeapRegionAttr dest);\n-\n-  \/\/ Determines PLAB size for a given destination.\n-  inline size_t desired_plab_sz(G1HeapRegionAttr dest);\n-  \/\/ Clamp the given PLAB word size to allowed values. Prevents humongous PLAB sizes\n-  \/\/ for two reasons:\n-  \/\/ * PLABs are allocated using a similar paths as oops, but should\n-  \/\/   never be in a humongous region\n-  \/\/ * Allowing humongous PLABs needlessly churns the region free lists\n-  inline size_t clamp_plab_size(size_t value) const;\n-\n-  \/\/ Do anything common to GC's.\n-  void gc_prologue(bool full);\n-  void gc_epilogue(bool full);\n-\n-  \/\/ Does the given region fulfill remembered set based eager reclaim candidate requirements?\n-  bool is_potential_eager_reclaim_candidate(G1HeapRegion* r) const;\n-\n-  inline bool is_humongous_reclaim_candidate(uint region);\n-\n-  \/\/ Remove from the reclaim candidate set.  Also remove from the\n-  \/\/ collection set so that later encounters avoid the slow path.\n-  inline void set_humongous_is_live(oop obj);\n-\n-  \/\/ Register the given region to be part of the collection set.\n-  inline void register_humongous_candidate_region_with_region_attr(uint index);\n-\n-  void set_humongous_metadata(G1HeapRegion* first_hr,\n-                              uint num_regions,\n-                              size_t word_size,\n-                              bool update_remsets);\n-\n-  \/\/ We register a region with the fast \"in collection set\" test. We\n-  \/\/ simply set to true the array slot corresponding to this region.\n-  void register_young_region_with_region_attr(G1HeapRegion* r) {\n-    _region_attr.set_in_young(r->hrm_index(), r->has_pinned_objects());\n-  }\n-  inline void register_new_survivor_region_with_region_attr(G1HeapRegion* r);\n-  inline void register_region_with_region_attr(G1HeapRegion* r);\n-  inline void register_old_region_with_region_attr(G1HeapRegion* r);\n-  inline void register_optional_region_with_region_attr(G1HeapRegion* r);\n-\n-  void clear_region_attr(const G1HeapRegion* hr) {\n-    _region_attr.clear(hr);\n-  }\n-\n-  void clear_region_attr() {\n-    _region_attr.clear();\n-  }\n-\n-  \/\/ Verify that the G1RegionAttr remset tracking corresponds to actual remset tracking\n-  \/\/ for all regions.\n-  void verify_region_attr_remset_is_tracked() PRODUCT_RETURN;\n-\n-  void clear_bitmap_for_region(G1HeapRegion* hr);\n-\n-  bool is_user_requested_concurrent_full_gc(GCCause::Cause cause);\n-\n-  \/\/ This is called at the start of either a concurrent cycle or a Full\n-  \/\/ GC to update the number of old marking cycles started.\n-  void increment_old_marking_cycles_started();\n-\n-  \/\/ This is called at the end of either a concurrent cycle or a Full\n-  \/\/ GC to update the number of old marking cycles completed. Those two\n-  \/\/ can happen in a nested fashion, i.e., we start a concurrent\n-  \/\/ cycle, a Full GC happens half-way through it which ends first,\n-  \/\/ and then the cycle notices that a Full GC happened and ends\n-  \/\/ too. The concurrent parameter is a boolean to help us do a bit\n-  \/\/ tighter consistency checking in the method. If concurrent is\n-  \/\/ false, the caller is the inner caller in the nesting (i.e., the\n-  \/\/ Full GC). If concurrent is true, the caller is the outer caller\n-  \/\/ in this nesting (i.e., the concurrent cycle). Further nesting is\n-  \/\/ not currently supported. The end of this call also notifies\n-  \/\/ the G1OldGCCount_lock in case a Java thread is waiting for a full\n-  \/\/ GC to happen (e.g., it called System.gc() with\n-  \/\/ +ExplicitGCInvokesConcurrent).\n-  \/\/ whole_heap_examined should indicate that during that old marking\n-  \/\/ cycle the whole heap has been examined for live objects (as opposed\n-  \/\/ to only parts, or aborted before completion).\n-  void increment_old_marking_cycles_completed(bool concurrent, bool whole_heap_examined);\n-\n-  uint old_marking_cycles_started() const {\n-    return _old_marking_cycles_started;\n-  }\n-\n-  uint old_marking_cycles_completed() const {\n-    return _old_marking_cycles_completed;\n-  }\n-\n-  \/\/ Allocates a new heap region instance.\n-  G1HeapRegion* new_heap_region(uint hrs_index, MemRegion mr);\n-\n-  \/\/ Frees a region by resetting its metadata and adding it to the free list\n-  \/\/ passed as a parameter (this is usually a local list which will be appended\n-  \/\/ to the master free list later or null if free list management is handled\n-  \/\/ in another way).\n-  \/\/ Callers must ensure they are the only one calling free on the given region\n-  \/\/ at the same time.\n-  void free_region(G1HeapRegion* hr, G1FreeRegionList* free_list);\n-\n-  \/\/ Add the given region to the retained regions collection set candidates.\n-  void retain_region(G1HeapRegion* hr);\n-  \/\/ It dirties the cards that cover the block so that the post\n-  \/\/ write barrier never queues anything when updating objects on this\n-  \/\/ block. It is assumed (and in fact we assert) that the block\n-  \/\/ belongs to a young region.\n-  inline void dirty_young_block(HeapWord* start, size_t word_size);\n-\n-  \/\/ Frees a humongous region by collapsing it into individual regions\n-  \/\/ and calling free_region() for each of them. The freed regions\n-  \/\/ will be added to the free list that's passed as a parameter (this\n-  \/\/ is usually a local list which will be appended to the master free\n-  \/\/ list later).\n-  \/\/ The method assumes that only a single thread is ever calling\n-  \/\/ this for a particular region at once.\n-  void free_humongous_region(G1HeapRegion* hr,\n-                             G1FreeRegionList* free_list);\n-\n-  \/\/ Execute func(G1HeapRegion* r, bool is_last) on every region covered by the\n-  \/\/ given range.\n-  template <typename Func>\n-  void iterate_regions_in_range(MemRegion range, const Func& func);\n-\n-  \/\/ Commit the required number of G1 region(s) according to the size requested\n-  \/\/ and mark them as 'old' region(s). Preferred address is treated as a hint for\n-  \/\/ the location of the archive space in the heap. The returned address may or may\n-  \/\/ not be same as the preferred address.\n-  \/\/ This API is only used for allocating heap space for the archived heap objects\n-  \/\/ in the CDS archive.\n-  HeapWord* alloc_archive_region(size_t word_size, HeapWord* preferred_addr);\n-\n-  \/\/ Populate the G1BlockOffsetTable for archived regions with the given\n-  \/\/ memory range.\n-  void populate_archive_regions_bot(MemRegion range);\n-\n-  \/\/ For the specified range, uncommit the containing G1 regions\n-  \/\/ which had been allocated by alloc_archive_regions. This should be called\n-  \/\/ at JVM init time if the archive heap's contents cannot be used (e.g., if\n-  \/\/ CRC check fails).\n-  void dealloc_archive_regions(MemRegion range);\n-\n-private:\n-\n-  \/\/ Shrink the garbage-first heap by at most the given size (in bytes!).\n-  \/\/ (Rounds down to a G1HeapRegion boundary.)\n-  void shrink(size_t shrink_bytes);\n-  void shrink_helper(size_t expand_bytes);\n-\n-  \/\/ Schedule the VM operation that will do an evacuation pause to\n-  \/\/ satisfy an allocation request of word_size. *succeeded will\n-  \/\/ return whether the VM operation was successful (it did do an\n-  \/\/ evacuation pause) or not (another thread beat us to it or the GC\n-  \/\/ locker was active). Given that we should not be holding the\n-  \/\/ Heap_lock when we enter this method, we will pass the\n-  \/\/ gc_count_before (i.e., total_collections()) as a parameter since\n-  \/\/ it has to be read while holding the Heap_lock. Currently, both\n-  \/\/ methods that call do_collection_pause() release the Heap_lock\n-  \/\/ before the call, so it's easy to read gc_count_before just before.\n-  HeapWord* do_collection_pause(size_t         word_size,\n-                                uint           gc_count_before,\n-                                bool*          succeeded,\n-                                GCCause::Cause gc_cause);\n-\n-  \/\/ Perform an incremental collection at a safepoint, possibly\n-  \/\/ followed by a by-policy upgrade to a full collection.\n-  \/\/ precondition: at safepoint on VM thread\n-  \/\/ precondition: !is_stw_gc_active()\n-  void do_collection_pause_at_safepoint(size_t allocation_word_size = 0);\n-\n-  \/\/ Helper for do_collection_pause_at_safepoint, containing the guts\n-  \/\/ of the incremental collection pause, executed by the vm thread.\n-  void do_collection_pause_at_safepoint_helper(size_t allocation_word_size);\n-\n-  void verify_before_young_collection(G1HeapVerifier::G1VerifyType type);\n-  void verify_after_young_collection(G1HeapVerifier::G1VerifyType type);\n-\n-public:\n-  \/\/ Start a concurrent cycle.\n-  void start_concurrent_cycle(bool concurrent_operation_is_full_mark);\n-\n-  void prepare_for_mutator_after_young_collection();\n-\n-  void retire_tlabs();\n-\n-  \/\/ Update all region's pin counts from the per-thread caches and resets them.\n-  \/\/ Must be called before any decision based on pin counts.\n-  void flush_region_pin_cache();\n-\n-  void record_obj_copy_mem_stats();\n-\n-private:\n-  \/\/ The g1 remembered set of the heap.\n-  G1RemSet* _rem_set;\n-  \/\/ Global card set configuration\n-  G1CardSetConfiguration _card_set_config;\n-\n-  G1MonotonicArenaFreePool _card_set_freelist_pool;\n-\n-  \/\/ Group cardsets\n-  G1CSetCandidateGroup _young_regions_cset_group;\n-\n-public:\n-  G1CardSetConfiguration* card_set_config() { return &_card_set_config; }\n-\n-  G1CSetCandidateGroup* young_regions_cset_group() { return &_young_regions_cset_group; }\n-  G1CardSet* young_regions_cardset() { return _young_regions_cset_group.card_set(); };\n-\n-  G1MonotonicArenaMemoryStats young_regions_card_set_memory_stats() { return _young_regions_cset_group.card_set_memory_stats(); }\n-\n-  void prepare_group_cardsets_for_scan();\n-\n-  \/\/ After a collection pause, reset eden and the collection set.\n-  void clear_eden();\n-  void clear_collection_set();\n-\n-  \/\/ Abandon the current collection set without recording policy\n-  \/\/ statistics or updating free lists.\n-  void abandon_collection_set(G1CollectionSet* collection_set);\n-\n-  \/\/ The concurrent marker (and the thread it runs in.)\n-  G1ConcurrentMark* _cm;\n-  G1ConcurrentMarkThread* _cm_thread;\n-\n-  \/\/ The concurrent refiner.\n-  G1ConcurrentRefine* _cr;\n-\n-  \/\/ Reusable parallel task queues and partial array manager.\n-  G1ScannerTasksQueueSet* _task_queues;\n-  PartialArrayStateManager* _partial_array_state_manager;\n-\n-  \/\/ (\"Weak\") Reference processing support.\n-  \/\/\n-  \/\/ G1 has 2 instances of the reference processor class.\n-  \/\/\n-  \/\/ One (_ref_processor_cm) handles reference object discovery and subsequent\n-  \/\/ processing during concurrent marking cycles. Discovery is enabled\/disabled\n-  \/\/ at the start\/end of a concurrent marking cycle.\n-  \/\/\n-  \/\/ The other (_ref_processor_stw) handles reference object discovery and\n-  \/\/ processing during incremental evacuation pauses and full GC pauses.\n-  \/\/\n-  \/\/ ## Incremental evacuation pauses\n-  \/\/\n-  \/\/ STW ref processor discovery is enabled\/disabled at the start\/end of an\n-  \/\/ incremental evacuation pause. No particular handling of the CM ref\n-  \/\/ processor is needed, apart from treating the discovered references as\n-  \/\/ roots; CM discovery does not need to be temporarily disabled as all\n-  \/\/ marking threads are paused during incremental evacuation pauses.\n-  \/\/\n-  \/\/ ## Full GC pauses\n-  \/\/\n-  \/\/ We abort any ongoing concurrent marking cycle, disable CM discovery, and\n-  \/\/ temporarily substitute a new closure for the STW ref processor's\n-  \/\/ _is_alive_non_header field (old value is restored after the full GC). Then\n-  \/\/ STW ref processor discovery is enabled, and marking & compaction\n-  \/\/ commences.\n-\n-  \/\/ The (stw) reference processor...\n-  ReferenceProcessor* _ref_processor_stw;\n-\n-  \/\/ During reference object discovery, the _is_alive_non_header\n-  \/\/ closure (if non-null) is applied to the referent object to\n-  \/\/ determine whether the referent is live. If so then the\n-  \/\/ reference object does not need to be 'discovered' and can\n-  \/\/ be treated as a regular oop. This has the benefit of reducing\n-  \/\/ the number of 'discovered' reference objects that need to\n-  \/\/ be processed.\n-  \/\/\n-  \/\/ Instance of the is_alive closure for embedding into the\n-  \/\/ STW reference processor as the _is_alive_non_header field.\n-  \/\/ Supplying a value for the _is_alive_non_header field is\n-  \/\/ optional but doing so prevents unnecessary additions to\n-  \/\/ the discovered lists during reference discovery.\n-  G1STWIsAliveClosure _is_alive_closure_stw;\n-\n-  G1STWSubjectToDiscoveryClosure _is_subject_to_discovery_stw;\n-\n-  \/\/ The (concurrent marking) reference processor...\n-  ReferenceProcessor* _ref_processor_cm;\n-\n-  \/\/ Instance of the concurrent mark is_alive closure for embedding\n-  \/\/ into the Concurrent Marking reference processor as the\n-  \/\/ _is_alive_non_header field. Supplying a value for the\n-  \/\/ _is_alive_non_header field is optional but doing so prevents\n-  \/\/ unnecessary additions to the discovered lists during reference\n-  \/\/ discovery.\n-  G1CMIsAliveClosure _is_alive_closure_cm;\n-\n-  G1CMSubjectToDiscoveryClosure _is_subject_to_discovery_cm;\n-public:\n-\n-  G1ScannerTasksQueueSet* task_queues() const;\n-  G1ScannerTasksQueue* task_queue(uint i) const;\n-\n-  PartialArrayStateManager* partial_array_state_manager() const;\n-\n-  \/\/ Create a G1CollectedHeap.\n-  \/\/ Must call the initialize method afterwards.\n-  \/\/ May not return if something goes wrong.\n-  G1CollectedHeap();\n-\n-private:\n-  jint initialize_concurrent_refinement();\n-  jint initialize_service_thread();\n-\n-  void print_tracing_info() const override;\n-  void stop() override;\n-\n-public:\n-  \/\/ Initialize the G1CollectedHeap to have the initial and\n-  \/\/ maximum sizes and remembered and barrier sets\n-  \/\/ specified by the policy object.\n-  jint initialize() override;\n-\n-  \/\/ Returns whether concurrent mark threads (and the VM) are about to terminate.\n-  bool concurrent_mark_is_terminating() const;\n-\n-  void safepoint_synchronize_begin() override;\n-  void safepoint_synchronize_end() override;\n-\n-  \/\/ Does operations required after initialization has been done.\n-  void post_initialize() override;\n-\n-  \/\/ Initialize weak reference processing.\n-  void ref_processing_init();\n-\n-  Name kind() const override {\n-    return CollectedHeap::G1;\n-  }\n-\n-  const char* name() const override {\n-    return \"G1\";\n-  }\n-\n-  const G1CollectorState* collector_state() const { return &_collector_state; }\n-  G1CollectorState* collector_state() { return &_collector_state; }\n-\n-  \/\/ The current policy object for the collector.\n-  G1Policy* policy() const { return _policy; }\n-  \/\/ The remembered set.\n-  G1RemSet* rem_set() const { return _rem_set; }\n-\n-  const G1MonotonicArenaFreePool* card_set_freelist_pool() const { return &_card_set_freelist_pool; }\n-  G1MonotonicArenaFreePool* card_set_freelist_pool() { return &_card_set_freelist_pool; }\n-\n-  inline G1GCPhaseTimes* phase_times() const;\n-\n-  const G1CollectionSet* collection_set() const { return &_collection_set; }\n-  G1CollectionSet* collection_set() { return &_collection_set; }\n-\n-  inline bool is_collection_set_candidate(const G1HeapRegion* r) const;\n-\n-  void initialize_serviceability() override;\n-  MemoryUsage memory_usage() override;\n-  GrowableArray<GCMemoryManager*> memory_managers() override;\n-  GrowableArray<MemoryPool*> memory_pools() override;\n-\n-  void fill_with_dummy_object(HeapWord* start, HeapWord* end, bool zap) override;\n-\n-  static void start_codecache_marking_cycle_if_inactive(bool concurrent_mark_start);\n-  static void finish_codecache_marking_cycle();\n-\n-  \/\/ The shared block offset table array.\n-  G1BlockOffsetTable* bot() const { return _bot; }\n-\n-  \/\/ Reference Processing accessors\n-\n-  \/\/ The STW reference processor....\n-  ReferenceProcessor* ref_processor_stw() const { return _ref_processor_stw; }\n-\n-  G1NewTracer* gc_tracer_stw() const { return _gc_tracer_stw; }\n-  STWGCTimer* gc_timer_stw() const { return _gc_timer_stw; }\n-\n-  \/\/ The Concurrent Marking reference processor...\n-  ReferenceProcessor* ref_processor_cm() const { return _ref_processor_cm; }\n-\n-  size_t unused_committed_regions_in_bytes() const;\n-\n-  size_t capacity() const override;\n-  size_t used() const override;\n-  \/\/ This should be called when we're not holding the heap lock. The\n-  \/\/ result might be a bit inaccurate.\n-  size_t used_unlocked() const;\n-  size_t recalculate_used() const;\n-\n-  \/\/ These virtual functions do the actual allocation.\n-  \/\/ Some heaps may offer a contiguous region for shared non-blocking\n-  \/\/ allocation, via inlined code (by exporting the address of the top and\n-  \/\/ end fields defining the extent of the contiguous allocation region.)\n-  \/\/ But G1CollectedHeap doesn't yet support this.\n-\n-  \/\/ Returns true if an incremental GC should be upgrade to a full gc. This\n-  \/\/ is done when there are no free regions and the heap can't be expanded.\n-  bool should_upgrade_to_full_gc() const {\n-    return num_available_regions() == 0;\n-  }\n-\n-  \/\/ The number of inactive regions.\n-  uint num_inactive_regions() const { return _hrm.num_inactive_regions(); }\n-\n-  \/\/ The current number of regions in the heap.\n-  uint num_committed_regions() const { return _hrm.num_committed_regions(); }\n-\n-  \/\/ The max number of regions reserved for the heap.\n-  uint max_num_regions() const { return _hrm.max_num_regions(); }\n-\n-  \/\/ The number of regions that are completely free.\n-  uint num_free_regions() const { return _hrm.num_free_regions(); }\n-\n-  \/\/ The number of regions that are not completely free.\n-  uint num_used_regions() const { return _hrm.num_used_regions(); }\n-\n-  \/\/ The number of regions that can be allocated into.\n-  uint num_available_regions() const { return num_free_regions() + num_inactive_regions(); }\n-\n-  MemoryUsage get_auxiliary_data_memory_usage() const {\n-    return _hrm.get_auxiliary_data_memory_usage();\n-  }\n-\n-#ifdef ASSERT\n-  bool is_on_master_free_list(G1HeapRegion* hr) {\n-    return _hrm.is_free(hr);\n-  }\n-#endif \/\/ ASSERT\n-\n-  inline void old_set_add(G1HeapRegion* hr);\n-  inline void old_set_remove(G1HeapRegion* hr);\n-\n-  size_t non_young_capacity_bytes() {\n-    return (old_regions_count() + humongous_regions_count()) * G1HeapRegion::GrainBytes;\n-  }\n-\n-  \/\/ Determine whether the given region is one that we are using as an\n-  \/\/ old GC alloc region.\n-  bool is_old_gc_alloc_region(G1HeapRegion* hr);\n-\n-  \/\/ Perform a collection of the heap; intended for use in implementing\n-  \/\/ \"System.gc\".  This probably implies as full a collection as the\n-  \/\/ \"CollectedHeap\" supports.\n-  void collect(GCCause::Cause cause) override;\n-\n-  \/\/ Perform a collection of the heap with the given cause.\n-  \/\/ Returns whether this collection actually executed.\n-  bool try_collect(GCCause::Cause cause, const G1GCCounters& counters_before);\n-\n-  void start_concurrent_gc_for_metadata_allocation(GCCause::Cause gc_cause);\n-\n-  bool last_gc_was_periodic() { return _gc_lastcause == GCCause::_g1_periodic_collection; }\n-\n-  void remove_from_old_gen_sets(const uint old_regions_removed,\n-                                const uint humongous_regions_removed);\n-  void prepend_to_freelist(G1FreeRegionList* list);\n-  void decrement_summary_bytes(size_t bytes);\n-\n-  bool is_in(const void* p) const override;\n-\n-  \/\/ Return \"TRUE\" iff the given object address is within the collection\n-  \/\/ set. Assumes that the reference points into the heap.\n-  inline bool is_in_cset(const G1HeapRegion* hr) const;\n-  inline bool is_in_cset(oop obj) const;\n-  inline bool is_in_cset(HeapWord* addr) const;\n-\n-  inline bool is_in_cset_or_humongous_candidate(const oop obj);\n-\n- private:\n-  \/\/ This array is used for a quick test on whether a reference points into\n-  \/\/ the collection set or not. Each of the array's elements denotes whether the\n-  \/\/ corresponding region is in the collection set or not.\n-  G1HeapRegionAttrBiasedMappedArray _region_attr;\n-\n- public:\n-\n-  inline G1HeapRegionAttr region_attr(const void* obj) const;\n-  inline G1HeapRegionAttr region_attr(uint idx) const;\n-\n-  MemRegion reserved() const {\n-    return _hrm.reserved();\n-  }\n-\n-  bool is_in_reserved(const void* addr) const {\n-    return reserved().contains(addr);\n-  }\n-\n-  G1CardTable* card_table() const {\n-    return _card_table;\n-  }\n-\n-  \/\/ Iteration functions.\n-\n-  void object_iterate_parallel(ObjectClosure* cl, uint worker_id, G1HeapRegionClaimer* claimer);\n-\n-  \/\/ Iterate over all objects, calling \"cl.do_object\" on each.\n-  void object_iterate(ObjectClosure* cl) override;\n-\n-  ParallelObjectIteratorImpl* parallel_object_iterator(uint thread_num) override;\n-\n-  \/\/ Keep alive an object that was loaded with AS_NO_KEEPALIVE.\n-  void keep_alive(oop obj) override;\n-\n-  \/\/ Iterate over heap regions, in address order, terminating the\n-  \/\/ iteration early if the \"do_heap_region\" method returns \"true\".\n-  void heap_region_iterate(G1HeapRegionClosure* blk) const;\n-  void heap_region_iterate(G1HeapRegionIndexClosure* blk) const;\n-\n-  \/\/ Return the region with the given index. It assumes the index is valid.\n-  inline G1HeapRegion* region_at(uint index) const;\n-  inline G1HeapRegion* region_at_or_null(uint index) const;\n-\n-  \/\/ Iterate over the regions that the humongous object starting at the given\n-  \/\/ region and apply the given method with the signature f(G1HeapRegion*) on them.\n-  template <typename Func>\n-  void humongous_obj_regions_iterate(G1HeapRegion* start, const Func& f);\n-\n-  \/\/ Calculate the region index of the given address. Given address must be\n-  \/\/ within the heap.\n-  inline uint addr_to_region(const void* addr) const;\n-\n-  inline HeapWord* bottom_addr_for_region(uint index) const;\n-\n-  \/\/ Two functions to iterate over the heap regions in parallel. Threads\n-  \/\/ compete using the G1HeapRegionClaimer to claim the regions before\n-  \/\/ applying the closure on them.\n-  \/\/ The _from_worker_offset version uses the G1HeapRegionClaimer and\n-  \/\/ the worker id to calculate a start offset to prevent all workers to\n-  \/\/ start from the point.\n-  void heap_region_par_iterate_from_worker_offset(G1HeapRegionClosure* cl,\n-                                                  G1HeapRegionClaimer* hrclaimer,\n-                                                  uint worker_id) const;\n-\n-  void heap_region_par_iterate_from_start(G1HeapRegionClosure* cl,\n-                                          G1HeapRegionClaimer* hrclaimer) const;\n-\n-  \/\/ Iterate over all regions in the collection set in parallel.\n-  void collection_set_par_iterate_all(G1HeapRegionClosure* cl,\n-                                      G1HeapRegionClaimer* hr_claimer,\n-                                      uint worker_id);\n-\n-  \/\/ Iterate over all regions currently in the current collection set.\n-  void collection_set_iterate_all(G1HeapRegionClosure* blk);\n-\n-  \/\/ Iterate over the regions in the current increment of the collection set.\n-  \/\/ Starts the iteration so that the start regions of a given worker id over the\n-  \/\/ set active_workers are evenly spread across the set of collection set regions\n-  \/\/ to be iterated.\n-  \/\/ The variant with the G1HeapRegionClaimer guarantees that the closure will be\n-  \/\/ applied to a particular region exactly once.\n-  void collection_set_iterate_increment_from(G1HeapRegionClosure *blk, uint worker_id) {\n-    collection_set_iterate_increment_from(blk, nullptr, worker_id);\n-  }\n-  void collection_set_iterate_increment_from(G1HeapRegionClosure *blk, G1HeapRegionClaimer* hr_claimer, uint worker_id);\n-  \/\/ Iterate over the array of region indexes, uint regions[length], applying\n-  \/\/ the given G1HeapRegionClosure on each region. The worker_id will determine where\n-  \/\/ to start the iteration to allow for more efficient parallel iteration.\n-  void par_iterate_regions_array(G1HeapRegionClosure* cl,\n-                                 G1HeapRegionClaimer* hr_claimer,\n-                                 const uint regions[],\n-                                 size_t length,\n-                                 uint worker_id) const;\n-\n-  \/\/ Returns the G1HeapRegion that contains addr. addr must not be null.\n-  inline G1HeapRegion* heap_region_containing(const void* addr) const;\n-\n-  \/\/ Returns the G1HeapRegion that contains addr, or null if that is an uncommitted\n-  \/\/ region. addr must not be null.\n-  inline G1HeapRegion* heap_region_containing_or_null(const void* addr) const;\n-\n-  \/\/ A CollectedHeap is divided into a dense sequence of \"blocks\"; that is,\n-  \/\/ each address in the (reserved) heap is a member of exactly\n-  \/\/ one block.  The defining characteristic of a block is that it is\n-  \/\/ possible to find its size, and thus to progress forward to the next\n-  \/\/ block.  (Blocks may be of different sizes.)  Thus, blocks may\n-  \/\/ represent Java objects, or they might be free blocks in a\n-  \/\/ free-list-based heap (or subheap), as long as the two kinds are\n-  \/\/ distinguishable and the size of each is determinable.\n-\n-  \/\/ Returns the address of the start of the \"block\" that contains the\n-  \/\/ address \"addr\".  We say \"blocks\" instead of \"object\" since some heaps\n-  \/\/ may not pack objects densely; a chunk may either be an object or a\n-  \/\/ non-object.\n-  HeapWord* block_start(const void* addr) const;\n-\n-  \/\/ Requires \"addr\" to be the start of a block, and returns \"TRUE\" iff\n-  \/\/ the block is an object.\n-  bool block_is_obj(const HeapWord* addr) const;\n-\n-  \/\/ Section on thread-local allocation buffers (TLABs)\n-  \/\/ See CollectedHeap for semantics.\n-\n-  size_t tlab_capacity(Thread* ignored) const override;\n-  size_t tlab_used(Thread* ignored) const override;\n-  size_t max_tlab_size() const override;\n-  size_t unsafe_max_tlab_alloc(Thread* ignored) const override;\n-\n-  inline bool is_in_young(const oop obj) const;\n-  inline bool requires_barriers(stackChunkOop obj) const override;\n-\n-  \/\/ Returns \"true\" iff the given word_size is \"very large\".\n-  static bool is_humongous(size_t word_size) {\n-    \/\/ Note this has to be strictly greater-than as the TLABs\n-    \/\/ are capped at the humongous threshold and we want to\n-    \/\/ ensure that we don't try to allocate a TLAB as\n-    \/\/ humongous and that we don't allocate a humongous\n-    \/\/ object in a TLAB.\n-    return word_size > _humongous_object_threshold_in_words;\n-  }\n-\n-  \/\/ Returns the humongous threshold for a specific region size\n-  static size_t humongous_threshold_for(size_t region_size) {\n-    return (region_size \/ 2);\n-  }\n-\n-  \/\/ Returns the number of regions the humongous object of the given word size\n-  \/\/ requires.\n-  static size_t humongous_obj_size_in_regions(size_t word_size);\n-\n-  \/\/ Print the maximum heap capacity.\n-  size_t max_capacity() const override;\n-  size_t min_capacity() const;\n-\n-  Tickspan time_since_last_collection() const { return Ticks::now() - _collection_pause_end; }\n-\n-  \/\/ Convenience function to be used in situations where the heap type can be\n-  \/\/ asserted to be this type.\n-  static G1CollectedHeap* heap() {\n-    return named_heap<G1CollectedHeap>(CollectedHeap::G1);\n-  }\n-\n-  void set_region_short_lived_locked(G1HeapRegion* hr);\n-  \/\/ add appropriate methods for any other surv rate groups\n-\n-  G1SurvivorRegions* survivor() { return &_survivor; }\n-\n-  inline uint eden_target_length() const;\n-  uint eden_regions_count() const { return _eden.length(); }\n-  uint eden_regions_count(uint node_index) const { return _eden.regions_on_node(node_index); }\n-  uint survivor_regions_count() const { return _survivor.length(); }\n-  uint survivor_regions_count(uint node_index) const { return _survivor.regions_on_node(node_index); }\n-  size_t eden_regions_used_bytes() const { return _eden.used_bytes(); }\n-  size_t survivor_regions_used_bytes() const { return _survivor.used_bytes(); }\n-  uint young_regions_count() const { return _eden.length() + _survivor.length(); }\n-  uint old_regions_count() const { return _old_set.length(); }\n-  uint humongous_regions_count() const { return _humongous_set.length(); }\n-\n-#ifdef ASSERT\n-  bool check_young_list_empty();\n-#endif\n-\n-  bool is_marked(oop obj) const;\n-\n-  inline static bool is_obj_filler(const oop obj);\n-  \/\/ Determine if an object is dead, given the object and also\n-  \/\/ the region to which the object belongs.\n-  inline bool is_obj_dead(const oop obj, const G1HeapRegion* hr) const;\n-\n-  \/\/ Determine if an object is dead, given only the object itself.\n-  \/\/ This will find the region to which the object belongs and\n-  \/\/ then call the region version of the same function.\n-  \/\/ If obj is null it is not dead.\n-  inline bool is_obj_dead(const oop obj) const;\n-\n-  inline bool is_obj_dead_full(const oop obj, const G1HeapRegion* hr) const;\n-  inline bool is_obj_dead_full(const oop obj) const;\n-\n-  \/\/ Mark the live object that failed evacuation in the bitmap.\n-  void mark_evac_failure_object(uint worker_id, oop obj, size_t obj_size) const;\n-\n-  G1ConcurrentMark* concurrent_mark() const { return _cm; }\n-\n-  \/\/ Refinement\n-\n-  G1ConcurrentRefine* concurrent_refine() const { return _cr; }\n-\n-  \/\/ Optimized nmethod scanning support routines\n-\n-  \/\/ Register the given nmethod with the G1 heap.\n-  void register_nmethod(nmethod* nm) override;\n-\n-  \/\/ Unregister the given nmethod from the G1 heap.\n-  void unregister_nmethod(nmethod* nm) override;\n-\n-  \/\/ No nmethod verification implemented.\n-  void verify_nmethod(nmethod* nm) override {}\n-\n-  \/\/ Recalculate amount of used memory after GC. Must be called after all allocation\n-  \/\/ has finished.\n-  void update_used_after_gc(bool evacuation_failed);\n-\n-  \/\/ Rebuild the code root lists for each region\n-  \/\/ after a full GC.\n-  void rebuild_code_roots();\n-\n-  \/\/ Performs cleaning of data structures after class unloading.\n-  void complete_cleaning(bool class_unloading_occurred);\n-\n-  void unload_classes_and_code(const char* description, BoolObjectClosure* cl, GCTimer* timer);\n-\n-  void bulk_unregister_nmethods();\n-\n-  \/\/ Verification\n-\n-  \/\/ Perform any cleanup actions necessary before allowing a verification.\n-  void prepare_for_verify() override;\n-\n-  \/\/ Perform verification.\n-  void verify(VerifyOption vo) override;\n-\n-  \/\/ WhiteBox testing support.\n-  bool supports_concurrent_gc_breakpoints() const override;\n-\n-  WorkerThreads* safepoint_workers() override { return _workers; }\n-\n-  \/\/ The methods below are here for convenience and dispatch the\n-  \/\/ appropriate method depending on value of the given VerifyOption\n-  \/\/ parameter. The values for that parameter, and their meanings,\n-  \/\/ are the same as those above.\n-\n-  bool is_obj_dead_cond(const oop obj,\n-                        const G1HeapRegion* hr,\n-                        const VerifyOption vo) const;\n-\n-  bool is_obj_dead_cond(const oop obj,\n-                        const VerifyOption vo) const;\n-\n-  G1HeapSummary create_g1_heap_summary();\n-  G1EvacSummary create_g1_evac_summary(G1EvacStats* stats);\n-\n-  \/\/ Printing\n-private:\n-  void print_heap_regions() const;\n-  void print_regions_on(outputStream* st) const;\n-\n-public:\n-  void print_heap_on(outputStream* st) const override;\n-  void print_extended_on(outputStream* st) const;\n-  void print_gc_on(outputStream* st) const override;\n-\n-  void gc_threads_do(ThreadClosure* tc) const override;\n-\n-  \/\/ Used to print information about locations in the hs_err file.\n-  bool print_location(outputStream* st, void* addr) const override;\n-};\n-\n-\/\/ Scoped object that performs common pre- and post-gc heap printing operations.\n-class G1HeapPrinterMark : public StackObj {\n-  G1CollectedHeap* _g1h;\n-  G1HeapTransition _heap_transition;\n-\n-public:\n-  G1HeapPrinterMark(G1CollectedHeap* g1h);\n-  ~G1HeapPrinterMark();\n-};\n-\n-\/\/ Scoped object that performs common pre- and post-gc operations related to\n-\/\/ JFR events.\n-class G1JFRTracerMark : public StackObj {\n-protected:\n-  STWGCTimer* _timer;\n-  GCTracer* _tracer;\n-\n-public:\n-  G1JFRTracerMark(STWGCTimer* timer, GCTracer* tracer);\n-  ~G1JFRTracerMark();\n-};\n-\n-#endif \/\/ SHARE_GC_G1_G1COLLECTEDHEAP_HPP\n+\/* * Copyright (c) 2001, 2025, Oracle and\/or its affiliates. All rights reserved. * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER. * * This code is free software; you can redistribute it and\/or modify it * under the terms of the GNU General Public License version 2 only, as * published by the Free Software Foundation. * * This code is distributed in the hope that it will be useful, but WITHOUT * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License * version 2 for more details (a copy is included in the LICENSE file that * accompanied this code). * * You should have received a copy of the GNU General Public License version * 2 along with this work; if not, write to the Free Software Foundation, * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA. * * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA * or visit www.oracle.com if you need additional information or have any * questions. * *\/#ifndef SHARE_GC_G1_G1COLLECTEDHEAP_HPP#define SHARE_GC_G1_G1COLLECTEDHEAP_HPP#include \"gc\/g1\/g1BarrierSet.hpp\"#include \"gc\/g1\/g1BiasedArray.hpp\"#include \"gc\/g1\/g1CardSet.hpp\"#include \"gc\/g1\/g1CardTable.hpp\"#include \"gc\/g1\/g1CollectionSet.hpp\"#include \"gc\/g1\/g1CollectorState.hpp\"#include \"gc\/g1\/g1ConcurrentMark.hpp\"#include \"gc\/g1\/g1EdenRegions.hpp\"#include \"gc\/g1\/g1EvacStats.hpp\"#include \"gc\/g1\/g1GCPauseType.hpp\"#include \"gc\/g1\/g1HeapEvaluationTask.hpp\"#include \"gc\/g1\/g1HeapRegionAttr.hpp\"#include \"gc\/g1\/g1HeapRegionManager.hpp\"#include \"gc\/g1\/g1HeapRegionSet.hpp\"#include \"gc\/g1\/g1HeapTransition.hpp\"#include \"gc\/g1\/g1HeapVerifier.hpp\"#include \"gc\/g1\/g1MonitoringSupport.hpp\"#include \"gc\/g1\/g1MonotonicArenaFreeMemoryTask.hpp\"#include \"gc\/g1\/g1MonotonicArenaFreePool.hpp\"#include \"gc\/g1\/g1NUMA.hpp\"#include \"gc\/g1\/g1SurvivorRegions.hpp\"#include \"gc\/g1\/g1YoungGCAllocationFailureInjector.hpp\"#include \"gc\/shared\/barrierSet.hpp\"#include \"gc\/shared\/collectedHeap.hpp\"#include \"gc\/shared\/gcHeapSummary.hpp\"#include \"gc\/shared\/plab.hpp\"#include \"gc\/shared\/softRefPolicy.hpp\"#include \"gc\/shared\/taskqueue.hpp\"#include \"memory\/allocation.hpp\"#include \"memory\/iterator.hpp\"#include \"memory\/memRegion.hpp\"#include \"runtime\/mutexLocker.hpp\"#include \"runtime\/threadSMR.hpp\"#include \"utilities\/bitMap.hpp\"\/\/ A \"G1CollectedHeap\" is an implementation of a java heap for HotSpot.\/\/ It uses the \"Garbage First\" heap organization and algorithm, which\/\/ may combine concurrent marking with parallel, incremental compaction of\/\/ heap subsets that will yield large amounts of garbage.\/\/ Forward declarationsclass G1Allocator;class G1BatchedTask;class G1CardTableEntryClosure;class G1ConcurrentMark;class G1ConcurrentMarkThread;class G1ConcurrentRefine;class G1GCCounters;class G1GCPhaseTimes;class G1HeapSizingPolicy;class G1NewTracer;class G1RemSet;class G1ServiceTask;class G1ServiceThread;class GCMemoryManager;class G1HeapRegion;class MemoryPool;class nmethod;class PartialArrayStateManager;class ReferenceProcessor;class STWGCTimer;class WorkerThreads;typedef OverflowTaskQueue<ScannerTask, mtGC>           G1ScannerTasksQueue;typedef GenericTaskQueueSet<G1ScannerTasksQueue, mtGC> G1ScannerTasksQueueSet;typedef int RegionIdx_t;   \/\/ needs to hold [ 0..max_num_regions() )typedef int CardIdx_t;     \/\/ needs to hold [ 0..CardsPerRegion )\/\/ The G1 STW is alive closure.\/\/ An instance is embedded into the G1CH and used as the\/\/ (optional) _is_alive_non_header closure in the STW\/\/ reference processor. It is also extensively used during\/\/ reference processing during STW evacuation pauses.class G1STWIsAliveClosure : public BoolObjectClosure {  G1CollectedHeap* _g1h;public:  G1STWIsAliveClosure(G1CollectedHeap* g1h) : _g1h(g1h) {}  bool do_object_b(oop p) override;};class G1STWSubjectToDiscoveryClosure : public BoolObjectClosure {  G1CollectedHeap* _g1h;public:  G1STWSubjectToDiscoveryClosure(G1CollectedHeap* g1h) : _g1h(g1h) {}  bool do_object_b(oop p) override;};class G1RegionMappingChangedListener : public G1MappingChangedListener { private:  void reset_from_card_cache(uint start_idx, size_t num_regions); public:  void on_commit(uint start_idx, size_t num_regions, bool zero_filled) override;};\/\/ Helper to claim contiguous sets of JavaThread for processing by multiple threads.class G1JavaThreadsListClaimer : public StackObj {  ThreadsListHandle _list;  uint _claim_step;  volatile uint _cur_claim;  \/\/ Attempts to claim _claim_step JavaThreads, returning an array of claimed  \/\/ JavaThread* with count elements. Returns null (and a zero count) if there  \/\/ are no more threads to claim.  JavaThread* const* claim(uint& count);public:  G1JavaThreadsListClaimer(uint claim_step) : _list(), _claim_step(claim_step), _cur_claim(0) {    assert(claim_step > 0, \"must be\");  }  \/\/ Executes the given closure on the elements of the JavaThread list, chunking the  \/\/ JavaThread set in claim_step chunks for each caller to reduce parallelization  \/\/ overhead.  void apply(ThreadClosure* cl);  \/\/ Total number of JavaThreads that can be claimed.  uint length() const { return _list.length(); }};class G1CollectedHeap : public CollectedHeap {  friend class VM_G1CollectForAllocation;  friend class VM_G1CollectFull;  friend class VM_G1TryInitiateConcMark;  friend class VM_G1ShrinkHeap;  friend class VMStructs;  friend class MutatorAllocRegion;  friend class G1FullCollector;  friend class G1GCAllocRegion;  friend class G1HeapVerifier;  friend class G1YoungGCVerifierMark;  \/\/ Closures used in implementation.  friend class G1EvacuateRegionsTask;  friend class G1PLABAllocator;  \/\/ Other related classes.  friend class G1HeapPrinterMark;  friend class G1HeapRegionClaimer;  \/\/ Testing classes.  friend class G1CheckRegionAttrTableClosure;private:  G1ServiceThread* _service_thread;  G1ServiceTask* _periodic_gc_task;  G1MonotonicArenaFreeMemoryTask* _free_arena_memory_task;  WorkerThreads* _workers;  G1CardTable* _card_table;  Ticks _collection_pause_end;  static size_t _humongous_object_threshold_in_words;  \/\/ These sets keep track of old and humongous regions respectively.  G1HeapRegionSet _old_set;  G1HeapRegionSet _humongous_set;  \/\/ Young gen memory statistics before GC.  G1MonotonicArenaMemoryStats _young_gen_card_set_stats;  \/\/ Collection set candidates memory statistics after GC.  G1MonotonicArenaMemoryStats _collection_set_candidates_card_set_stats;  \/\/ The block offset table for the G1 heap.  G1BlockOffsetTable* _bot;  G1HeapEvaluationTask* _heap_evaluation_task;public:  void rebuild_free_region_list();  \/\/ Start a new incremental collection set for the next pause.  void start_new_collection_set();  void prepare_region_for_full_compaction(G1HeapRegion* hr);private:  \/\/ Rebuilds the region sets \/ lists so that they are repopulated to  \/\/ reflect the contents of the heap. The only exception is the  \/\/ humongous set which was not torn down in the first place. If  \/\/ free_list_only is true, it will only rebuild the free list.  void rebuild_region_sets(bool free_list_only);  \/\/ Callback for region mapping changed events.  G1RegionMappingChangedListener _listener;  \/\/ Handle G1 NUMA support.  G1NUMA* _numa;  \/\/ The sequence of all heap regions in the heap.  G1HeapRegionManager _hrm;  \/\/ Manages all allocations with regions except humongous object allocations.  G1Allocator* _allocator;  G1YoungGCAllocationFailureInjector _allocation_failure_injector;  \/\/ Manages all heap verification.  G1HeapVerifier* _verifier;  \/\/ Outside of GC pauses, the number of bytes used in all regions other  \/\/ than the current allocation region(s).  volatile size_t _summary_bytes_used;  void increase_used(size_t bytes);  void decrease_used(size_t bytes);  void set_used(size_t bytes);  \/\/ Number of bytes used in all regions during GC. Typically changed when  \/\/ retiring a GC alloc region.  size_t _bytes_used_during_gc;public:  size_t bytes_used_during_gc() const { return _bytes_used_during_gc; }private:  \/\/ GC allocation statistics policy for survivors.  G1EvacStats _survivor_evac_stats;  \/\/ GC allocation statistics policy for tenured objects.  G1EvacStats _old_evac_stats;  \/\/ Helper for monitoring and management support.  G1MonitoringSupport* _monitoring_support;  uint _num_humongous_objects; \/\/ Current amount of (all) humongous objects found in the heap.  uint _num_humongous_reclaim_candidates; \/\/ Number of humongous object eager reclaim candidates.public:  uint num_humongous_objects() const { return _num_humongous_objects; }  uint num_humongous_reclaim_candidates() const { return _num_humongous_reclaim_candidates; }  bool has_humongous_reclaim_candidates() const { return _num_humongous_reclaim_candidates > 0; }  void set_humongous_stats(uint num_humongous_total, uint num_humongous_candidates);  bool should_sample_collection_set_candidates() const;  void set_collection_set_candidates_stats(G1MonotonicArenaMemoryStats& stats);  void set_young_gen_card_set_stats(const G1MonotonicArenaMemoryStats& stats);  void update_perf_counter_cpu_time();private:  \/\/ Return true if an explicit GC should start a concurrent cycle instead  \/\/ of doing a STW full GC. A concurrent cycle should be started if:  \/\/ (a) cause == _g1_humongous_allocation,  \/\/ (b) cause == _java_lang_system_gc and +ExplicitGCInvokesConcurrent,  \/\/ (c) cause == _dcmd_gc_run and +ExplicitGCInvokesConcurrent,  \/\/ (d) cause == _wb_breakpoint,  \/\/ (e) cause == _g1_periodic_collection and +G1PeriodicGCInvokesConcurrent.  bool should_do_concurrent_full_gc(GCCause::Cause cause);  \/\/ Attempt to start a concurrent cycle with the indicated cause.  \/\/ precondition: should_do_concurrent_full_gc(cause)  bool try_collect_concurrently(GCCause::Cause cause,                                uint gc_counter,                                uint old_marking_started_before);  \/\/ indicates whether we are in young or mixed GC mode  G1CollectorState _collector_state;  \/\/ Keeps track of how many \"old marking cycles\" (i.e., Full GCs or  \/\/ concurrent cycles) we have started.  volatile uint _old_marking_cycles_started;  \/\/ Keeps track of how many \"old marking cycles\" (i.e., Full GCs or  \/\/ concurrent cycles) we have completed.  volatile uint _old_marking_cycles_completed;  \/\/ Create a memory mapper for auxiliary data structures of the given size and  \/\/ translation factor.  static G1RegionToSpaceMapper* create_aux_memory_mapper(const char* description,                                                         size_t size,                                                         size_t translation_factor);  void trace_heap(GCWhen::Type when, const GCTracer* tracer) override;  \/\/ These are macros so that, if the assert fires, we get the correct  \/\/ line number, file, etc.#define heap_locking_asserts_params(_extra_message_)                          \\  \"%s : Heap_lock locked: %s, at safepoint: %s, is VM thread: %s\",            \\  (_extra_message_),                                                          \\  BOOL_TO_STR(Heap_lock->owned_by_self()),                                    \\  BOOL_TO_STR(SafepointSynchronize::is_at_safepoint()),                       \\  BOOL_TO_STR(Thread::current()->is_VM_thread())#define assert_heap_locked()                                                  \\  do {                                                                        \\    assert(Heap_lock->owned_by_self(),                                        \\           heap_locking_asserts_params(\"should be holding the Heap_lock\"));   \\  } while (0)#define assert_heap_locked_or_at_safepoint(_should_be_vm_thread_)             \\  do {                                                                        \\    assert(Heap_lock->owned_by_self() ||                                      \\           (SafepointSynchronize::is_at_safepoint() &&                        \\             ((_should_be_vm_thread_) == Thread::current()->is_VM_thread())), \\           heap_locking_asserts_params(\"should be holding the Heap_lock or \"  \\                                        \"should be at a safepoint\"));         \\  } while (0)#define assert_heap_locked_and_not_at_safepoint()                             \\  do {                                                                        \\    assert(Heap_lock->owned_by_self() &&                                      \\                                    !SafepointSynchronize::is_at_safepoint(), \\          heap_locking_asserts_params(\"should be holding the Heap_lock and \"  \\                                       \"should not be at a safepoint\"));      \\  } while (0)#define assert_heap_not_locked()                                              \\  do {                                                                        \\    assert(!Heap_lock->owned_by_self(),                                       \\        heap_locking_asserts_params(\"should not be holding the Heap_lock\"));  \\  } while (0)#define assert_heap_not_locked_and_not_at_safepoint()                         \\  do {                                                                        \\    assert(!Heap_lock->owned_by_self() &&                                     \\                                    !SafepointSynchronize::is_at_safepoint(), \\      heap_locking_asserts_params(\"should not be holding the Heap_lock and \"  \\                                   \"should not be at a safepoint\"));          \\  } while (0)#define assert_at_safepoint_on_vm_thread()                                        \\  do {                                                                            \\    assert_at_safepoint();                                                        \\    assert(Thread::current_or_null() != nullptr, \"no current thread\");            \\    assert(Thread::current()->is_VM_thread(), \"current thread is not VM thread\"); \\  } while (0)#ifdef ASSERT#define assert_used_and_recalculate_used_equal(g1h)                           \\  do {                                                                        \\    size_t cur_used_bytes = g1h->used();                                      \\    size_t recal_used_bytes = g1h->recalculate_used();                        \\    assert(cur_used_bytes == recal_used_bytes, \"Used(%zu) is not\" \\           \" same as recalculated used(%zu).\",                    \\           cur_used_bytes, recal_used_bytes);                                 \\  } while (0)#else#define assert_used_and_recalculate_used_equal(g1h) do {} while(0)#endif  \/\/ The young region list.  G1EdenRegions _eden;  G1SurvivorRegions _survivor;  STWGCTimer* _gc_timer_stw;  G1NewTracer* _gc_tracer_stw;  \/\/ The current policy object for the collector.  G1Policy* _policy;  G1HeapSizingPolicy* _heap_sizing_policy;  G1CollectionSet _collection_set;  \/\/ Try to allocate a single non-humongous G1HeapRegion sufficient for  \/\/ an allocation of the given word_size. If do_expand is true,  \/\/ attempt to expand the heap if necessary to satisfy the allocation  \/\/ request. 'type' takes the type of region to be allocated. (Use constants  \/\/ Old, Eden, Humongous, Survivor defined in G1HeapRegionType.)  G1HeapRegion* new_region(size_t word_size,                           G1HeapRegionType type,                           bool do_expand,                           uint node_index = G1NUMA::AnyNodeIndex);  \/\/ Initialize a contiguous set of free regions of length num_regions  \/\/ and starting at index first so that they appear as a single  \/\/ humongous region.  HeapWord* humongous_obj_allocate_initialize_regions(G1HeapRegion* first_hr,                                                      uint num_regions,                                                      size_t word_size);  \/\/ Attempt to allocate a humongous object of the given size. Return  \/\/ null if unsuccessful.  HeapWord* humongous_obj_allocate(size_t word_size);  \/\/ The following two methods, allocate_new_tlab() and  \/\/ mem_allocate(), are the two main entry points from the runtime  \/\/ into the G1's allocation routines. They have the following  \/\/ assumptions:  \/\/  \/\/ * They should both be called outside safepoints.  \/\/  \/\/ * They should both be called without holding the Heap_lock.  \/\/  \/\/ * All allocation requests for new TLABs should go to  \/\/   allocate_new_tlab().  \/\/  \/\/ * All non-TLAB allocation requests should go to mem_allocate().  \/\/  \/\/ * If either call cannot satisfy the allocation request using the  \/\/   current allocating region, they will try to get a new one. If  \/\/   this fails, they will attempt to do an evacuation pause and  \/\/   retry the allocation.  \/\/  \/\/ * If all allocation attempts fail, even after trying to schedule  \/\/   an evacuation pause, allocate_new_tlab() will return null,  \/\/   whereas mem_allocate() will attempt a heap expansion and\/or  \/\/   schedule a Full GC.  \/\/  \/\/ * We do not allow humongous-sized TLABs. So, allocate_new_tlab  \/\/   should never be called with word_size being humongous. All  \/\/   humongous allocation requests should go to mem_allocate() which  \/\/   will satisfy them with a special path.  HeapWord* allocate_new_tlab(size_t min_size,                              size_t requested_size,                              size_t* actual_size) override;  HeapWord* mem_allocate(size_t word_size,                         bool*  gc_overhead_limit_was_exceeded) override;  \/\/ First-level mutator allocation attempt: try to allocate out of  \/\/ the mutator alloc region without taking the Heap_lock. This  \/\/ should only be used for non-humongous allocations.  inline HeapWord* attempt_allocation(size_t min_word_size,                                      size_t desired_word_size,                                      size_t* actual_word_size);  \/\/ Second-level mutator allocation attempt: take the Heap_lock and  \/\/ retry the allocation attempt, potentially scheduling a GC  \/\/ pause. This should only be used for non-humongous allocations.  HeapWord* attempt_allocation_slow(uint node_index, size_t word_size);  \/\/ Takes the Heap_lock and attempts a humongous allocation. It can  \/\/ potentially schedule a GC pause.  HeapWord* attempt_allocation_humongous(size_t word_size);  \/\/ Allocation attempt that should be called during safepoints (e.g.,  \/\/ at the end of a successful GC). expect_null_mutator_alloc_region  \/\/ specifies whether the mutator alloc region is expected to be null  \/\/ or not.  HeapWord* attempt_allocation_at_safepoint(size_t word_size,                                            bool expect_null_mutator_alloc_region);  \/\/ These methods are the \"callbacks\" from the G1AllocRegion class.  \/\/ For mutator alloc regions.  G1HeapRegion* new_mutator_alloc_region(size_t word_size, uint node_index);  void retire_mutator_alloc_region(G1HeapRegion* alloc_region,                                   size_t allocated_bytes);  \/\/ For GC alloc regions.  bool has_more_regions(G1HeapRegionAttr dest);  G1HeapRegion* new_gc_alloc_region(size_t word_size, G1HeapRegionAttr dest, uint node_index);  void retire_gc_alloc_region(G1HeapRegion* alloc_region,                              size_t allocated_bytes, G1HeapRegionAttr dest);  void resize_heap(size_t resize_bytes, bool should_expand);  \/\/ - if clear_all_soft_refs is true, all soft references should be  \/\/   cleared during the GC.  \/\/ - if do_maximal_compaction is true, full gc will do a maximally  \/\/   compacting collection, leaving no dead wood.  \/\/ - if allocation_word_size is set, then this allocation size will  \/\/    be accounted for in case shrinking of the heap happens.  \/\/ - it returns false if it is unable to do the collection due to the  \/\/   GC locker being active, true otherwise.  void do_full_collection(bool clear_all_soft_refs,                          bool do_maximal_compaction,                          size_t allocation_word_size);  \/\/ Callback from VM_G1CollectFull operation, or collect_as_vm_thread.  void do_full_collection(bool clear_all_soft_refs) override;  \/\/ Helper to do a full collection that clears soft references.  void upgrade_to_full_collection();  \/\/ Callback from VM_G1CollectForAllocation operation.  \/\/ This function does everything necessary\/possible to satisfy a  \/\/ failed allocation request (including collection, expansion, etc.)  HeapWord* satisfy_failed_allocation(size_t word_size);  \/\/ Internal helpers used during full GC to split it up to  \/\/ increase readability.  bool abort_concurrent_cycle();  void verify_before_full_collection();  void prepare_heap_for_full_collection();  void prepare_for_mutator_after_full_collection(size_t allocation_word_size);  void abort_refinement();  void verify_after_full_collection();  void print_heap_after_full_collection();  \/\/ Helper method for satisfy_failed_allocation()  HeapWord* satisfy_failed_allocation_helper(size_t word_size,                                             bool do_gc,                                             bool maximal_compaction,                                             bool expect_null_mutator_alloc_region);  \/\/ Attempting to expand the heap sufficiently  \/\/ to support an allocation of the given \"word_size\".  If  \/\/ successful, perform the allocation and return the address of the  \/\/ allocated block, or else null.  HeapWord* expand_and_allocate(size_t word_size);  void verify_numa_regions(const char* desc);public:  \/\/ If during a concurrent start pause we may install a pending list head which is not  \/\/ otherwise reachable, ensure that it is marked in the bitmap for concurrent marking  \/\/ to discover.  void make_pending_list_reachable();  G1ServiceThread* service_thread() const { return _service_thread; }  WorkerThreads* workers() const { return _workers; }  \/\/ Run the given batch task using the workers.  void run_batch_task(G1BatchedTask* cl);  \/\/ Return \"optimal\" number of chunks per region we want to use for claiming areas  \/\/ within a region to claim.  \/\/ The returned value is a trade-off between granularity of work distribution and  \/\/ memory usage and maintenance costs of that table.  \/\/ Testing showed that 64 for 1M\/2M region, 128 for 4M\/8M regions, 256 for 16\/32M regions,  \/\/ and so on seems to be such a good trade-off.  static uint get_chunks_per_region();  G1Allocator* allocator() {    return _allocator;  }  G1YoungGCAllocationFailureInjector* allocation_failure_injector() { return &_allocation_failure_injector; }  G1HeapVerifier* verifier() {    return _verifier;  }  G1MonitoringSupport* monitoring_support() {    assert(_monitoring_support != nullptr, \"should have been initialized\");    return _monitoring_support;  }  void pin_object(JavaThread* thread, oop obj) override;  void unpin_object(JavaThread* thread, oop obj) override;  void resize_heap_after_young_collection(size_t allocation_word_size);  void resize_heap_after_full_collection(size_t allocation_word_size);  \/\/ Check if there is memory to uncommit and if so schedule a task to do it.  void uncommit_regions_if_necessary();  \/\/ Immediately uncommit uncommittable regions.  uint uncommit_regions(uint region_limit);  bool has_uncommittable_regions();  G1NUMA* numa() const { return _numa; }  \/\/ Expand the garbage-first heap by at least the given size (in bytes!).  \/\/ Returns true if the heap was expanded by the requested amount;  \/\/ false otherwise.  \/\/ (Rounds up to a G1HeapRegion boundary.)  bool expand(size_t expand_bytes, WorkerThreads* pretouch_workers);  bool expand_single_region(uint node_index);  \/\/ Request an immediate heap contraction of (at most) the given number of bytes.  \/\/ Returns true if any pages were actually uncommitted.  bool request_heap_shrink(size_t shrink_bytes);  \/\/ Returns the PLAB statistics for a given destination.  inline G1EvacStats* alloc_buffer_stats(G1HeapRegionAttr dest);  \/\/ Determines PLAB size for a given destination.  inline size_t desired_plab_sz(G1HeapRegionAttr dest);  \/\/ Clamp the given PLAB word size to allowed values. Prevents humongous PLAB sizes  \/\/ for two reasons:  \/\/ * PLABs are allocated using a similar paths as oops, but should  \/\/   never be in a humongous region  \/\/ * Allowing humongous PLABs needlessly churns the region free lists  inline size_t clamp_plab_size(size_t value) const;  \/\/ Do anything common to GC's.  void gc_prologue(bool full);  void gc_epilogue(bool full);  \/\/ Does the given region fulfill remembered set based eager reclaim candidate requirements?  bool is_potential_eager_reclaim_candidate(G1HeapRegion* r) const;  inline bool is_humongous_reclaim_candidate(uint region);  \/\/ Remove from the reclaim candidate set.  Also remove from the  \/\/ collection set so that later encounters avoid the slow path.  inline void set_humongous_is_live(oop obj);  \/\/ Register the given region to be part of the collection set.  inline void register_humongous_candidate_region_with_region_attr(uint index);  void set_humongous_metadata(G1HeapRegion* first_hr,                              uint num_regions,                              size_t word_size,                              bool update_remsets);  \/\/ We register a region with the fast \"in collection set\" test. We  \/\/ simply set to true the array slot corresponding to this region.  void register_young_region_with_region_attr(G1HeapRegion* r) {    _region_attr.set_in_young(r->hrm_index(), r->has_pinned_objects());  }  inline void register_new_survivor_region_with_region_attr(G1HeapRegion* r);  inline void register_region_with_region_attr(G1HeapRegion* r);  inline void register_old_region_with_region_attr(G1HeapRegion* r);  inline void register_optional_region_with_region_attr(G1HeapRegion* r);  void clear_region_attr(const G1HeapRegion* hr) {    _region_attr.clear(hr);  }  void clear_region_attr() {    _region_attr.clear();  }  \/\/ Verify that the G1RegionAttr remset tracking corresponds to actual remset tracking  \/\/ for all regions.  void verify_region_attr_remset_is_tracked() PRODUCT_RETURN;  void clear_bitmap_for_region(G1HeapRegion* hr);  bool is_user_requested_concurrent_full_gc(GCCause::Cause cause);  \/\/ This is called at the start of either a concurrent cycle or a Full  \/\/ GC to update the number of old marking cycles started.  void increment_old_marking_cycles_started();  \/\/ This is called at the end of either a concurrent cycle or a Full  \/\/ GC to update the number of old marking cycles completed. Those two  \/\/ can happen in a nested fashion, i.e., we start a concurrent  \/\/ cycle, a Full GC happens half-way through it which ends first,  \/\/ and then the cycle notices that a Full GC happened and ends  \/\/ too. The concurrent parameter is a boolean to help us do a bit  \/\/ tighter consistency checking in the method. If concurrent is  \/\/ false, the caller is the inner caller in the nesting (i.e., the  \/\/ Full GC). If concurrent is true, the caller is the outer caller  \/\/ in this nesting (i.e., the concurrent cycle). Further nesting is  \/\/ not currently supported. The end of this call also notifies  \/\/ the G1OldGCCount_lock in case a Java thread is waiting for a full  \/\/ GC to happen (e.g., it called System.gc() with  \/\/ +ExplicitGCInvokesConcurrent).  \/\/ whole_heap_examined should indicate that during that old marking  \/\/ cycle the whole heap has been examined for live objects (as opposed  \/\/ to only parts, or aborted before completion).  void increment_old_marking_cycles_completed(bool concurrent, bool whole_heap_examined);  uint old_marking_cycles_started() const {    return _old_marking_cycles_started;  }  uint old_marking_cycles_completed() const {    return _old_marking_cycles_completed;  }  \/\/ Allocates a new heap region instance.  G1HeapRegion* new_heap_region(uint hrs_index, MemRegion mr);  \/\/ Frees a region by resetting its metadata and adding it to the free list  \/\/ passed as a parameter (this is usually a local list which will be appended  \/\/ to the master free list later or null if free list management is handled  \/\/ in another way).  \/\/ Callers must ensure they are the only one calling free on the given region  \/\/ at the same time.  void free_region(G1HeapRegion* hr, G1FreeRegionList* free_list);  \/\/ Add the given region to the retained regions collection set candidates.  void retain_region(G1HeapRegion* hr);  \/\/ It dirties the cards that cover the block so that the post  \/\/ write barrier never queues anything when updating objects on this  \/\/ block. It is assumed (and in fact we assert) that the block  \/\/ belongs to a young region.  inline void dirty_young_block(HeapWord* start, size_t word_size);  \/\/ Frees a humongous region by collapsing it into individual regions  \/\/ and calling free_region() for each of them. The freed regions  \/\/ will be added to the free list that's passed as a parameter (this  \/\/ is usually a local list which will be appended to the master free  \/\/ list later).  \/\/ The method assumes that only a single thread is ever calling  \/\/ this for a particular region at once.  void free_humongous_region(G1HeapRegion* hr,                             G1FreeRegionList* free_list);  \/\/ Execute func(G1HeapRegion* r, bool is_last) on every region covered by the  \/\/ given range.  template <typename Func>  void iterate_regions_in_range(MemRegion range, const Func& func);  \/\/ Commit the required number of G1 region(s) according to the size requested  \/\/ and mark them as 'old' region(s). Preferred address is treated as a hint for  \/\/ the location of the archive space in the heap. The returned address may or may  \/\/ not be same as the preferred address.  \/\/ This API is only used for allocating heap space for the archived heap objects  \/\/ in the CDS archive.  HeapWord* alloc_archive_region(size_t word_size, HeapWord* preferred_addr);  \/\/ Populate the G1BlockOffsetTable for archived regions with the given  \/\/ memory range.  void populate_archive_regions_bot(MemRegion range);  \/\/ For the specified range, uncommit the containing G1 regions  \/\/ which had been allocated by alloc_archive_regions. This should be called  \/\/ at JVM init time if the archive heap's contents cannot be used (e.g., if  \/\/ CRC check fails).  void dealloc_archive_regions(MemRegion range);private:  \/\/ Shrink the garbage-first heap by at most the given size (in bytes!).  \/\/ (Rounds down to a G1HeapRegion boundary.)  void shrink(size_t shrink_bytes);  void shrink_helper(size_t expand_bytes);  \/\/ Schedule the VM operation that will do an evacuation pause to  \/\/ satisfy an allocation request of word_size. *succeeded will  \/\/ return whether the VM operation was successful (it did do an  \/\/ evacuation pause) or not (another thread beat us to it or the GC  \/\/ locker was active). Given that we should not be holding the  \/\/ Heap_lock when we enter this method, we will pass the  \/\/ gc_count_before (i.e., total_collections()) as a parameter since  \/\/ it has to be read while holding the Heap_lock. Currently, both  \/\/ methods that call do_collection_pause() release the Heap_lock  \/\/ before the call, so it's easy to read gc_count_before just before.  HeapWord* do_collection_pause(size_t         word_size,                                uint           gc_count_before,                                bool*          succeeded,                                GCCause::Cause gc_cause);  \/\/ Perform an incremental collection at a safepoint, possibly  \/\/ followed by a by-policy upgrade to a full collection.  \/\/ precondition: at safepoint on VM thread  \/\/ precondition: !is_stw_gc_active()  void do_collection_pause_at_safepoint(size_t allocation_word_size = 0);  \/\/ Helper for do_collection_pause_at_safepoint, containing the guts  \/\/ of the incremental collection pause, executed by the vm thread.  void do_collection_pause_at_safepoint_helper(size_t allocation_word_size);  void verify_before_young_collection(G1HeapVerifier::G1VerifyType type);  void verify_after_young_collection(G1HeapVerifier::G1VerifyType type);public:  \/\/ Start a concurrent cycle.  void start_concurrent_cycle(bool concurrent_operation_is_full_mark);  void prepare_for_mutator_after_young_collection();  void retire_tlabs();  \/\/ Update all region's pin counts from the per-thread caches and resets them.  \/\/ Must be called before any decision based on pin counts.  void flush_region_pin_cache();  void record_obj_copy_mem_stats();private:  \/\/ The g1 remembered set of the heap.  G1RemSet* _rem_set;  \/\/ Global card set configuration  G1CardSetConfiguration _card_set_config;  G1MonotonicArenaFreePool _card_set_freelist_pool;  \/\/ Group cardsets  G1CSetCandidateGroup _young_regions_cset_group;public:  G1CardSetConfiguration* card_set_config() { return &_card_set_config; }  G1CSetCandidateGroup* young_regions_cset_group() { return &_young_regions_cset_group; }  G1CardSet* young_regions_cardset() { return _young_regions_cset_group.card_set(); };  G1MonotonicArenaMemoryStats young_regions_card_set_memory_stats() { return _young_regions_cset_group.card_set_memory_stats(); }  void prepare_group_cardsets_for_scan();  \/\/ After a collection pause, reset eden and the collection set.  void clear_eden();  void clear_collection_set();  \/\/ Abandon the current collection set without recording policy  \/\/ statistics or updating free lists.  void abandon_collection_set(G1CollectionSet* collection_set);  \/\/ The concurrent marker (and the thread it runs in.)  G1ConcurrentMark* _cm;  G1ConcurrentMarkThread* _cm_thread;  \/\/ The concurrent refiner.  G1ConcurrentRefine* _cr;  \/\/ Reusable parallel task queues and partial array manager.  G1ScannerTasksQueueSet* _task_queues;  PartialArrayStateManager* _partial_array_state_manager;  \/\/ (\"Weak\") Reference processing support.  \/\/  \/\/ G1 has 2 instances of the reference processor class.  \/\/  \/\/ One (_ref_processor_cm) handles reference object discovery and subsequent  \/\/ processing during concurrent marking cycles. Discovery is enabled\/disabled  \/\/ at the start\/end of a concurrent marking cycle.  \/\/  \/\/ The other (_ref_processor_stw) handles reference object discovery and  \/\/ processing during incremental evacuation pauses and full GC pauses.  \/\/  \/\/ ## Incremental evacuation pauses  \/\/  \/\/ STW ref processor discovery is enabled\/disabled at the start\/end of an  \/\/ incremental evacuation pause. No particular handling of the CM ref  \/\/ processor is needed, apart from treating the discovered references as  \/\/ roots; CM discovery does not need to be temporarily disabled as all  \/\/ marking threads are paused during incremental evacuation pauses.  \/\/  \/\/ ## Full GC pauses  \/\/  \/\/ We abort any ongoing concurrent marking cycle, disable CM discovery, and  \/\/ temporarily substitute a new closure for the STW ref processor's  \/\/ _is_alive_non_header field (old value is restored after the full GC). Then  \/\/ STW ref processor discovery is enabled, and marking & compaction  \/\/ commences.  \/\/ The (stw) reference processor...  ReferenceProcessor* _ref_processor_stw;  \/\/ During reference object discovery, the _is_alive_non_header  \/\/ closure (if non-null) is applied to the referent object to  \/\/ determine whether the referent is live. If so then the  \/\/ reference object does not need to be 'discovered' and can  \/\/ be treated as a regular oop. This has the benefit of reducing  \/\/ the number of 'discovered' reference objects that need to  \/\/ be processed.  \/\/  \/\/ Instance of the is_alive closure for embedding into the  \/\/ STW reference processor as the _is_alive_non_header field.  \/\/ Supplying a value for the _is_alive_non_header field is  \/\/ optional but doing so prevents unnecessary additions to  \/\/ the discovered lists during reference discovery.  G1STWIsAliveClosure _is_alive_closure_stw;  G1STWSubjectToDiscoveryClosure _is_subject_to_discovery_stw;  \/\/ The (concurrent marking) reference processor...  ReferenceProcessor* _ref_processor_cm;  \/\/ Instance of the concurrent mark is_alive closure for embedding  \/\/ into the Concurrent Marking reference processor as the  \/\/ _is_alive_non_header field. Supplying a value for the  \/\/ _is_alive_non_header field is optional but doing so prevents  \/\/ unnecessary additions to the discovered lists during reference  \/\/ discovery.  G1CMIsAliveClosure _is_alive_closure_cm;  G1CMSubjectToDiscoveryClosure _is_subject_to_discovery_cm;public:  G1ScannerTasksQueueSet* task_queues() const;  G1ScannerTasksQueue* task_queue(uint i) const;  PartialArrayStateManager* partial_array_state_manager() const;  \/\/ Create a G1CollectedHeap.  \/\/ Must call the initialize method afterwards.  \/\/ May not return if something goes wrong.  G1CollectedHeap();private:  jint initialize_concurrent_refinement();  jint initialize_service_thread();  void print_tracing_info() const override;  void stop() override;public:  \/\/ Initialize the G1CollectedHeap to have the initial and  \/\/ maximum sizes and remembered and barrier sets  \/\/ specified by the policy object.  jint initialize() override;  \/\/ Returns whether concurrent mark threads (and the VM) are about to terminate.  bool concurrent_mark_is_terminating() const;  void safepoint_synchronize_begin() override;  void safepoint_synchronize_end() override;  \/\/ Does operations required after initialization has been done.  void post_initialize() override;  \/\/ Initialize weak reference processing.  void ref_processing_init();  Name kind() const override {    return CollectedHeap::G1;  }  const char* name() const override {    return \"G1\";  }  const G1CollectorState* collector_state() const { return &_collector_state; }  G1CollectorState* collector_state() { return &_collector_state; }  \/\/ The current policy object for the collector.  G1Policy* policy() const { return _policy; }  \/\/ The heap sizing policy.  G1HeapSizingPolicy* heap_sizing_policy() const { return _heap_sizing_policy; }  \/\/ The remembered set.  G1RemSet* rem_set() const { return _rem_set; }  const G1MonotonicArenaFreePool* card_set_freelist_pool() const { return &_card_set_freelist_pool; }  G1MonotonicArenaFreePool* card_set_freelist_pool() { return &_card_set_freelist_pool; }  inline G1GCPhaseTimes* phase_times() const;  const G1CollectionSet* collection_set() const { return &_collection_set; }  G1CollectionSet* collection_set() { return &_collection_set; }  inline bool is_collection_set_candidate(const G1HeapRegion* r) const;  void initialize_serviceability() override;  MemoryUsage memory_usage() override;  GrowableArray<GCMemoryManager*> memory_managers() override;  GrowableArray<MemoryPool*> memory_pools() override;  void fill_with_dummy_object(HeapWord* start, HeapWord* end, bool zap) override;  static void start_codecache_marking_cycle_if_inactive(bool concurrent_mark_start);  static void finish_codecache_marking_cycle();  \/\/ The shared block offset table array.  G1BlockOffsetTable* bot() const { return _bot; }  \/\/ Reference Processing accessors  \/\/ The STW reference processor....  ReferenceProcessor* ref_processor_stw() const { return _ref_processor_stw; }  G1NewTracer* gc_tracer_stw() const { return _gc_tracer_stw; }  STWGCTimer* gc_timer_stw() const { return _gc_timer_stw; }  \/\/ The Concurrent Marking reference processor...  ReferenceProcessor* ref_processor_cm() const { return _ref_processor_cm; }  size_t unused_committed_regions_in_bytes() const;  size_t capacity() const override;  size_t used() const override;  \/\/ This should be called when we're not holding the heap lock. The  \/\/ result might be a bit inaccurate.  size_t used_unlocked() const;  size_t recalculate_used() const;  \/\/ These virtual functions do the actual allocation.  \/\/ Some heaps may offer a contiguous region for shared non-blocking  \/\/ allocation, via inlined code (by exporting the address of the top and  \/\/ end fields defining the extent of the contiguous allocation region.)  \/\/ But G1CollectedHeap doesn't yet support this.  \/\/ Returns true if an incremental GC should be upgrade to a full gc. This  \/\/ is done when there are no free regions and the heap can't be expanded.  bool should_upgrade_to_full_gc() const {    return num_available_regions() == 0;  }  \/\/ The number of inactive regions.  uint num_inactive_regions() const { return _hrm.num_inactive_regions(); }  \/\/ The current number of regions in the heap.  uint num_committed_regions() const { return _hrm.num_committed_regions(); }  \/\/ The max number of regions reserved for the heap.  uint max_num_regions() const { return _hrm.max_num_regions(); }  \/\/ The number of regions that are completely free.  uint num_free_regions() const { return _hrm.num_free_regions(); }  \/\/ The number of regions that are not completely free.  uint num_used_regions() const { return _hrm.num_used_regions(); }  \/\/ The number of regions that can be allocated into.  uint num_available_regions() const { return num_free_regions() + num_inactive_regions(); }  MemoryUsage get_auxiliary_data_memory_usage() const {    return _hrm.get_auxiliary_data_memory_usage();  }#ifdef ASSERT  bool is_on_master_free_list(G1HeapRegion* hr) {    return _hrm.is_free(hr);  }#endif \/\/ ASSERT  inline void old_set_add(G1HeapRegion* hr);  inline void old_set_remove(G1HeapRegion* hr);  size_t non_young_capacity_bytes() {    return (old_regions_count() + humongous_regions_count()) * G1HeapRegion::GrainBytes;  }  \/\/ Determine whether the given region is one that we are using as an  \/\/ old GC alloc region.  bool is_old_gc_alloc_region(G1HeapRegion* hr);  \/\/ Perform a collection of the heap; intended for use in implementing  \/\/ \"System.gc\".  This probably implies as full a collection as the  \/\/ \"CollectedHeap\" supports.  void collect(GCCause::Cause cause) override;  \/\/ Perform a collection of the heap with the given cause.  \/\/ Returns whether this collection actually executed.  bool try_collect(GCCause::Cause cause, const G1GCCounters& counters_before);  void start_concurrent_gc_for_metadata_allocation(GCCause::Cause gc_cause);  bool last_gc_was_periodic() { return _gc_lastcause == GCCause::_g1_periodic_collection; }  void remove_from_old_gen_sets(const uint old_regions_removed,                                const uint humongous_regions_removed);  void prepend_to_freelist(G1FreeRegionList* list);  void decrement_summary_bytes(size_t bytes);  bool is_in(const void* p) const override;  \/\/ Return \"TRUE\" iff the given object address is within the collection  \/\/ set. Assumes that the reference points into the heap.  inline bool is_in_cset(const G1HeapRegion* hr) const;  inline bool is_in_cset(oop obj) const;  inline bool is_in_cset(HeapWord* addr) const;  inline bool is_in_cset_or_humongous_candidate(const oop obj); private:  \/\/ This array is used for a quick test on whether a reference points into  \/\/ the collection set or not. Each of the array's elements denotes whether the  \/\/ corresponding region is in the collection set or not.  G1HeapRegionAttrBiasedMappedArray _region_attr; public:  inline G1HeapRegionAttr region_attr(const void* obj) const;  inline G1HeapRegionAttr region_attr(uint idx) const;  MemRegion reserved() const {    return _hrm.reserved();  }  bool is_in_reserved(const void* addr) const {    return reserved().contains(addr);  }  G1CardTable* card_table() const {    return _card_table;  }  \/\/ Iteration functions.  void object_iterate_parallel(ObjectClosure* cl, uint worker_id, G1HeapRegionClaimer* claimer);  \/\/ Iterate over all objects, calling \"cl.do_object\" on each.  void object_iterate(ObjectClosure* cl) override;  ParallelObjectIteratorImpl* parallel_object_iterator(uint thread_num) override;  \/\/ Keep alive an object that was loaded with AS_NO_KEEPALIVE.  void keep_alive(oop obj) override;  \/\/ Iterate over heap regions, in address order, terminating the  \/\/ iteration early if the \"do_heap_region\" method returns \"true\".  void heap_region_iterate(G1HeapRegionClosure* blk) const;  void heap_region_iterate(G1HeapRegionIndexClosure* blk) const;  \/\/ Return the region with the given index. It assumes the index is valid.  inline G1HeapRegion* region_at(uint index) const;  inline G1HeapRegion* region_at_or_null(uint index) const;  \/\/ Iterate over the regions that the humongous object starting at the given  \/\/ region and apply the given method with the signature f(G1HeapRegion*) on them.  template <typename Func>  void humongous_obj_regions_iterate(G1HeapRegion* start, const Func& f);  \/\/ Calculate the region index of the given address. Given address must be  \/\/ within the heap.  inline uint addr_to_region(const void* addr) const;  inline HeapWord* bottom_addr_for_region(uint index) const;  \/\/ Two functions to iterate over the heap regions in parallel. Threads  \/\/ compete using the G1HeapRegionClaimer to claim the regions before  \/\/ applying the closure on them.  \/\/ The _from_worker_offset version uses the G1HeapRegionClaimer and  \/\/ the worker id to calculate a start offset to prevent all workers to  \/\/ start from the point.  void heap_region_par_iterate_from_worker_offset(G1HeapRegionClosure* cl,                                                  G1HeapRegionClaimer* hrclaimer,                                                  uint worker_id) const;  void heap_region_par_iterate_from_start(G1HeapRegionClosure* cl,                                          G1HeapRegionClaimer* hrclaimer) const;  \/\/ Iterate over all regions in the collection set in parallel.  void collection_set_par_iterate_all(G1HeapRegionClosure* cl,                                      G1HeapRegionClaimer* hr_claimer,                                      uint worker_id);  \/\/ Iterate over all regions currently in the current collection set.  void collection_set_iterate_all(G1HeapRegionClosure* blk);  \/\/ Iterate over the regions in the current increment of the collection set.  \/\/ Starts the iteration so that the start regions of a given worker id over the  \/\/ set active_workers are evenly spread across the set of collection set regions  \/\/ to be iterated.  \/\/ The variant with the G1HeapRegionClaimer guarantees that the closure will be  \/\/ applied to a particular region exactly once.  void collection_set_iterate_increment_from(G1HeapRegionClosure *blk, uint worker_id) {    collection_set_iterate_increment_from(blk, nullptr, worker_id);  }  void collection_set_iterate_increment_from(G1HeapRegionClosure *blk, G1HeapRegionClaimer* hr_claimer, uint worker_id);  \/\/ Iterate over the array of region indexes, uint regions[length], applying  \/\/ the given G1HeapRegionClosure on each region. The worker_id will determine where  \/\/ to start the iteration to allow for more efficient parallel iteration.  void par_iterate_regions_array(G1HeapRegionClosure* cl,                                 G1HeapRegionClaimer* hr_claimer,                                 const uint regions[],                                 size_t length,                                 uint worker_id) const;  \/\/ Returns the G1HeapRegion that contains addr. addr must not be null.  inline G1HeapRegion* heap_region_containing(const void* addr) const;  \/\/ Returns the G1HeapRegion that contains addr, or null if that is an uncommitted  \/\/ region. addr must not be null.  inline G1HeapRegion* heap_region_containing_or_null(const void* addr) const;  \/\/ A CollectedHeap is divided into a dense sequence of \"blocks\"; that is,  \/\/ each address in the (reserved) heap is a member of exactly  \/\/ one block.  The defining characteristic of a block is that it is  \/\/ possible to find its size, and thus to progress forward to the next  \/\/ block.  (Blocks may be of different sizes.)  Thus, blocks may  \/\/ represent Java objects, or they might be free blocks in a  \/\/ free-list-based heap (or subheap), as long as the two kinds are  \/\/ distinguishable and the size of each is determinable.  \/\/ Returns the address of the start of the \"block\" that contains the  \/\/ address \"addr\".  We say \"blocks\" instead of \"object\" since some heaps  \/\/ may not pack objects densely; a chunk may either be an object or a  \/\/ non-object.  HeapWord* block_start(const void* addr) const;  \/\/ Requires \"addr\" to be the start of a block, and returns \"TRUE\" iff  \/\/ the block is an object.  bool block_is_obj(const HeapWord* addr) const;  \/\/ Section on thread-local allocation buffers (TLABs)  \/\/ See CollectedHeap for semantics.  size_t tlab_capacity(Thread* ignored) const override;  size_t tlab_used(Thread* ignored) const override;  size_t max_tlab_size() const override;  size_t unsafe_max_tlab_alloc(Thread* ignored) const override;  inline bool is_in_young(const oop obj) const;  inline bool requires_barriers(stackChunkOop obj) const override;  \/\/ Returns \"true\" iff the given word_size is \"very large\".  static bool is_humongous(size_t word_size) {    \/\/ Note this has to be strictly greater-than as the TLABs    \/\/ are capped at the humongous threshold and we want to    \/\/ ensure that we don't try to allocate a TLAB as    \/\/ humongous and that we don't allocate a humongous    \/\/ object in a TLAB.    return word_size > _humongous_object_threshold_in_words;  }  \/\/ Returns the humongous threshold for a specific region size  static size_t humongous_threshold_for(size_t region_size) {    return (region_size \/ 2);  }  \/\/ Returns the number of regions the humongous object of the given word size  \/\/ requires.  static size_t humongous_obj_size_in_regions(size_t word_size);  \/\/ Print the maximum heap capacity.  size_t max_capacity() const override;  size_t min_capacity() const;  Tickspan time_since_last_collection() const { return Ticks::now() - _collection_pause_end; }  \/\/ Convenience function to be used in situations where the heap type can be  \/\/ asserted to be this type.  static G1CollectedHeap* heap() {    return named_heap<G1CollectedHeap>(CollectedHeap::G1);  }  void set_region_short_lived_locked(G1HeapRegion* hr);  \/\/ add appropriate methods for any other surv rate groups  G1SurvivorRegions* survivor() { return &_survivor; }  inline uint eden_target_length() const;  uint eden_regions_count() const { return _eden.length(); }  uint eden_regions_count(uint node_index) const { return _eden.regions_on_node(node_index); }  uint survivor_regions_count() const { return _survivor.length(); }  uint survivor_regions_count(uint node_index) const { return _survivor.regions_on_node(node_index); }  size_t eden_regions_used_bytes() const { return _eden.used_bytes(); }  size_t survivor_regions_used_bytes() const { return _survivor.used_bytes(); }  uint young_regions_count() const { return _eden.length() + _survivor.length(); }  uint old_regions_count() const { return _old_set.length(); }  uint humongous_regions_count() const { return _humongous_set.length(); }#ifdef ASSERT  bool check_young_list_empty();#endif  bool is_marked(oop obj) const;  inline static bool is_obj_filler(const oop obj);  \/\/ Determine if an object is dead, given the object and also  \/\/ the region to which the object belongs.  inline bool is_obj_dead(const oop obj, const G1HeapRegion* hr) const;  \/\/ Determine if an object is dead, given only the object itself.  \/\/ This will find the region to which the object belongs and  \/\/ then call the region version of the same function.  \/\/ If obj is null it is not dead.  inline bool is_obj_dead(const oop obj) const;  inline bool is_obj_dead_full(const oop obj, const G1HeapRegion* hr) const;  inline bool is_obj_dead_full(const oop obj) const;  \/\/ Mark the live object that failed evacuation in the bitmap.  void mark_evac_failure_object(uint worker_id, oop obj, size_t obj_size) const;  G1ConcurrentMark* concurrent_mark() const { return _cm; }  \/\/ Refinement  G1ConcurrentRefine* concurrent_refine() const { return _cr; }  \/\/ Optimized nmethod scanning support routines  \/\/ Register the given nmethod with the G1 heap.  void register_nmethod(nmethod* nm) override;  \/\/ Unregister the given nmethod from the G1 heap.  void unregister_nmethod(nmethod* nm) override;  \/\/ No nmethod verification implemented.  void verify_nmethod(nmethod* nm) override {}  \/\/ Recalculate amount of used memory after GC. Must be called after all allocation  \/\/ has finished.  void update_used_after_gc(bool evacuation_failed);  \/\/ Rebuild the code root lists for each region  \/\/ after a full GC.  void rebuild_code_roots();  \/\/ Performs cleaning of data structures after class unloading.  void complete_cleaning(bool class_unloading_occurred);  void unload_classes_and_code(const char* description, BoolObjectClosure* cl, GCTimer* timer);  void bulk_unregister_nmethods();  \/\/ Verification  \/\/ Perform any cleanup actions necessary before allowing a verification.  void prepare_for_verify() override;  \/\/ Perform verification.  void verify(VerifyOption vo) override;  \/\/ WhiteBox testing support.  bool supports_concurrent_gc_breakpoints() const override;  WorkerThreads* safepoint_workers() override { return _workers; }  \/\/ The methods below are here for convenience and dispatch the  \/\/ appropriate method depending on value of the given VerifyOption  \/\/ parameter. The values for that parameter, and their meanings,  \/\/ are the same as those above.  bool is_obj_dead_cond(const oop obj,                        const G1HeapRegion* hr,                        const VerifyOption vo) const;  bool is_obj_dead_cond(const oop obj,                        const VerifyOption vo) const;  G1HeapSummary create_g1_heap_summary();  G1EvacSummary create_g1_evac_summary(G1EvacStats* stats);  \/\/ Printingprivate:  void print_heap_regions() const;  void print_regions_on(outputStream* st) const;public:  void print_heap_on(outputStream* st) const override;  void print_extended_on(outputStream* st) const;  void print_gc_on(outputStream* st) const override;  void gc_threads_do(ThreadClosure* tc) const override;  \/\/ Used to print information about locations in the hs_err file.  bool print_location(outputStream* st, void* addr) const override;};\/\/ Scoped object that performs common pre- and post-gc heap printing operations.class G1HeapPrinterMark : public StackObj {  G1CollectedHeap* _g1h;  G1HeapTransition _heap_transition;public:  G1HeapPrinterMark(G1CollectedHeap* g1h);  ~G1HeapPrinterMark();};\/\/ Scoped object that performs common pre- and post-gc operations related to\/\/ JFR events.class G1JFRTracerMark : public StackObj {protected:  STWGCTimer* _timer;  GCTracer* _tracer;public:  G1JFRTracerMark(STWGCTimer* timer, GCTracer* tracer);  ~G1JFRTracerMark();};#endif \/\/ SHARE_GC_G1_G1COLLECTEDHEAP_HPP * accompanied this code). * * You should have received a copy of the GNU General Public License version * 2 along with this work; if not, write to the Free Software Foundation, * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA. * * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA * or visit www.oracle.com if you need additional information or have any * questions. * *\/#ifndef SHARE_GC_G1_G1COLLECTEDHEAP_HPP#define SHARE_GC_G1_G1COLLECTEDHEAP_HPP#include \"gc\/g1\/g1BarrierSet.hpp\"#include \"gc\/g1\/g1BiasedArray.hpp\"#include \"gc\/g1\/g1CardSet.hpp\"#include \"gc\/g1\/g1CardTable.hpp\"#include \"gc\/g1\/g1CollectionSet.hpp\"#include \"gc\/g1\/g1CollectorState.hpp\"#include \"gc\/g1\/g1ConcurrentMark.hpp\"#include \"gc\/g1\/g1EdenRegions.hpp\"#include \"gc\/g1\/g1EvacStats.hpp\"#include \"gc\/g1\/g1GCPauseType.hpp\"#include \"gc\/g1\/g1HeapEvaluationTask.hpp\"#include \"gc\/g1\/g1HeapRegionAttr.hpp\"#include \"gc\/g1\/g1HeapRegionManager.hpp\"#include \"gc\/g1\/g1HeapRegionSet.hpp\"#include \"gc\/g1\/g1HeapTransition.hpp\"#include \"gc\/g1\/g1HeapVerifier.hpp\"#include \"gc\/g1\/g1MonitoringSupport.hpp\"#include \"gc\/g1\/g1MonotonicArenaFreeMemoryTask.hpp\"#include \"gc\/g1\/g1MonotonicArenaFreePool.hpp\"#include \"gc\/g1\/g1NUMA.hpp\"#include \"gc\/g1\/g1SurvivorRegions.hpp\"#include \"gc\/g1\/g1YoungGCAllocationFailureInjector.hpp\"#include \"gc\/shared\/barrierSet.hpp\"#include \"gc\/shared\/collectedHeap.hpp\"#include \"gc\/shared\/gcHeapSummary.hpp\"#include \"gc\/shared\/plab.hpp\"#include \"gc\/shared\/softRefPolicy.hpp\"#include \"gc\/shared\/taskqueue.hpp\"#include \"memory\/allocation.hpp\"#include \"memory\/iterator.hpp\"#include \"memory\/memRegion.hpp\"#include \"runtime\/mutexLocker.hpp\"#include \"runtime\/threadSMR.hpp\"#include \"utilities\/bitMap.hpp\"\/\/ A \"G1CollectedHeap\" is an implementation of a java heap for HotSpot.\/\/ It uses the \"Garbage First\" heap organization and algorithm, which\/\/ may combine concurrent marking with parallel, incremental compaction of\/\/ heap subsets that will yield large amounts of garbage.\/\/ Forward declarationsclass G1Allocator;class G1BatchedTask;class G1CardTableEntryClosure;class G1ConcurrentMark;class G1ConcurrentMarkThread;class G1ConcurrentRefine;class G1GCCounters;class G1GCPhaseTimes;class G1HeapSizingPolicy;class G1NewTracer;class G1RemSet;class G1ServiceTask;class G1ServiceThread;class GCMemoryManager;class G1HeapRegion;class MemoryPool;class nmethod;class PartialArrayStateManager;class ReferenceProcessor;class STWGCTimer;class WorkerThreads;typedef OverflowTaskQueue<ScannerTask, mtGC>           G1ScannerTasksQueue;typedef GenericTaskQueueSet<G1ScannerTasksQueue, mtGC> G1ScannerTasksQueueSet;typedef int RegionIdx_t;   \/\/ needs to hold [ 0..max_num_regions() )typedef int CardIdx_t;     \/\/ needs to hold [ 0..CardsPerRegion )\/\/ The G1 STW is alive closure.\/\/ An instance is embedded into the G1CH and used as the\/\/ (optional) _is_alive_non_header closure in the STW\/\/ reference processor. It is also extensively used during\/\/ reference processing during STW evacuation pauses.class G1STWIsAliveClosure : public BoolObjectClosure {  G1CollectedHeap* _g1h;public:  G1STWIsAliveClosure(G1CollectedHeap* g1h) : _g1h(g1h) {}  bool do_object_b(oop p) override;};class G1STWSubjectToDiscoveryClosure : public BoolObjectClosure {  G1CollectedHeap* _g1h;public:  G1STWSubjectToDiscoveryClosure(G1CollectedHeap* g1h) : _g1h(g1h) {}  bool do_object_b(oop p) override;};class G1RegionMappingChangedListener : public G1MappingChangedListener { private:  void reset_from_card_cache(uint start_idx, size_t num_regions); public:  void on_commit(uint start_idx, size_t num_regions, bool zero_filled) override;};\/\/ Helper to claim contiguous sets of JavaThread for processing by multiple threads.class G1JavaThreadsListClaimer : public StackObj {  ThreadsListHandle _list;  uint _claim_step;  volatile uint _cur_claim;  \/\/ Attempts to claim _claim_step JavaThreads, returning an array of claimed  \/\/ JavaThread* with count elements. Returns null (and a zero count) if there  \/\/ are no more threads to claim.  JavaThread* const* claim(uint& count);public:  G1JavaThreadsListClaimer(uint claim_step) : _list(), _claim_step(claim_step), _cur_claim(0) {    assert(claim_step > 0, \"must be\");  }  \/\/ Executes the given closure on the elements of the JavaThread list, chunking the  \/\/ JavaThread set in claim_step chunks for each caller to reduce parallelization  \/\/ overhead.  void apply(ThreadClosure* cl);  \/\/ Total number of JavaThreads that can be claimed.  uint length() const { return _list.length(); }};class G1CollectedHeap : public CollectedHeap {  friend class VM_G1CollectForAllocation;  friend class VM_G1CollectFull;  friend class VM_G1TryInitiateConcMark;  friend class VM_G1ShrinkHeap;  friend class VMStructs;  friend class MutatorAllocRegion;  friend class G1FullCollector;  friend class G1GCAllocRegion;  friend class G1HeapVerifier;  friend class G1YoungGCVerifierMark;  \/\/ Closures used in implementation.  friend class G1EvacuateRegionsTask;  friend class G1PLABAllocator;  \/\/ Other related classes.  friend class G1HeapPrinterMark;  friend class G1HeapRegionClaimer;  \/\/ Testing classes.  friend class G1CheckRegionAttrTableClosure;private:  G1ServiceThread* _service_thread;  G1ServiceTask* _periodic_gc_task;  G1MonotonicArenaFreeMemoryTask* _free_arena_memory_task;  WorkerThreads* _workers;  G1CardTable* _card_table;  Ticks _collection_pause_end;  static size_t _humongous_object_threshold_in_words;  \/\/ These sets keep track of old and humongous regions respectively.  G1HeapRegionSet _old_set;  G1HeapRegionSet _humongous_set;  \/\/ Young gen memory statistics before GC.  G1MonotonicArenaMemoryStats _young_gen_card_set_stats;  \/\/ Collection set candidates memory statistics after GC.  G1MonotonicArenaMemoryStats _collection_set_candidates_card_set_stats;  \/\/ The block offset table for the G1 heap.  G1BlockOffsetTable* _bot;  G1HeapEvaluationTask* _heap_evaluation_task;public:  void rebuild_free_region_list();  \/\/ Start a new incremental collection set for the next pause.  void start_new_collection_set();  void prepare_region_for_full_compaction(G1HeapRegion* hr);private:  \/\/ Rebuilds the region sets \/ lists so that they are repopulated to  \/\/ reflect the contents of the heap. The only exception is the  \/\/ humongous set which was not torn down in the first place. If  \/\/ free_list_only is true, it will only rebuild the free list.  void rebuild_region_sets(bool free_list_only);  \/\/ Callback for region mapping changed events.  G1RegionMappingChangedListener _listener;  \/\/ Handle G1 NUMA support.  G1NUMA* _numa;  \/\/ The sequence of all heap regions in the heap.  G1HeapRegionManager _hrm;  \/\/ Manages all allocations with regions except humongous object allocations.  G1Allocator* _allocator;  G1YoungGCAllocationFailureInjector _allocation_failure_injector;  \/\/ Manages all heap verification.  G1HeapVerifier* _verifier;  \/\/ Outside of GC pauses, the number of bytes used in all regions other  \/\/ than the current allocation region(s).  volatile size_t _summary_bytes_used;  void increase_used(size_t bytes);  void decrease_used(size_t bytes);  void set_used(size_t bytes);  \/\/ Number of bytes used in all regions during GC. Typically changed when  \/\/ retiring a GC alloc region.  size_t _bytes_used_during_gc;public:  size_t bytes_used_during_gc() const { return _bytes_used_during_gc; }private:  \/\/ GC allocation statistics policy for survivors.  G1EvacStats _survivor_evac_stats;  \/\/ GC allocation statistics policy for tenured objects.  G1EvacStats _old_evac_stats;  \/\/ Helper for monitoring and management support.  G1MonitoringSupport* _monitoring_support;  uint _num_humongous_objects; \/\/ Current amount of (all) humongous objects found in the heap.  uint _num_humongous_reclaim_candidates; \/\/ Number of humongous object eager reclaim candidates.public:  uint num_humongous_objects() const { return _num_humongous_objects; }  uint num_humongous_reclaim_candidates() const { return _num_humongous_reclaim_candidates; }  bool has_humongous_reclaim_candidates() const { return _num_humongous_reclaim_candidates > 0; }  void set_humongous_stats(uint num_humongous_total, uint num_humongous_candidates);  bool should_sample_collection_set_candidates() const;  void set_collection_set_candidates_stats(G1MonotonicArenaMemoryStats& stats);  void set_young_gen_card_set_stats(const G1MonotonicArenaMemoryStats& stats);  void update_perf_counter_cpu_time();private:  \/\/ Return true if an explicit GC should start a concurrent cycle instead  \/\/ of doing a STW full GC. A concurrent cycle should be started if:  \/\/ (a) cause == _g1_humongous_allocation,  \/\/ (b) cause == _java_lang_system_gc and +ExplicitGCInvokesConcurrent,  \/\/ (c) cause == _dcmd_gc_run and +ExplicitGCInvokesConcurrent,  \/\/ (d) cause == _wb_breakpoint,  \/\/ (e) cause == _g1_periodic_collection and +G1PeriodicGCInvokesConcurrent.  bool should_do_concurrent_full_gc(GCCause::Cause cause);  \/\/ Attempt to start a concurrent cycle with the indicated cause.  \/\/ precondition: should_do_concurrent_full_gc(cause)  bool try_collect_concurrently(GCCause::Cause cause,                                uint gc_counter,                                uint old_marking_started_before);  \/\/ indicates whether we are in young or mixed GC mode  G1CollectorState _collector_state;  \/\/ Keeps track of how many \"old marking cycles\" (i.e., Full GCs or  \/\/ concurrent cycles) we have started.  volatile uint _old_marking_cycles_started;  \/\/ Keeps track of how many \"old marking cycles\" (i.e., Full GCs or  \/\/ concurrent cycles) we have completed.  volatile uint _old_marking_cycles_completed;  \/\/ Create a memory mapper for auxiliary data structures of the given size and  \/\/ translation factor.  static G1RegionToSpaceMapper* create_aux_memory_mapper(const char* description,                                                         size_t size,                                                         size_t translation_factor);  void trace_heap(GCWhen::Type when, const GCTracer* tracer) override;  \/\/ These are macros so that, if the assert fires, we get the correct  \/\/ line number, file, etc.#define heap_locking_asserts_params(_extra_message_)                          \\  \"%s : Heap_lock locked: %s, at safepoint: %s, is VM thread: %s\",            \\  (_extra_message_),                                                          \\  BOOL_TO_STR(Heap_lock->owned_by_self()),                                    \\  BOOL_TO_STR(SafepointSynchronize::is_at_safepoint()),                       \\  BOOL_TO_STR(Thread::current()->is_VM_thread())#define assert_heap_locked()                                                  \\  do {                                                                        \\    assert(Heap_lock->owned_by_self(),                                        \\           heap_locking_asserts_params(\"should be holding the Heap_lock\"));   \\  } while (0)#define assert_heap_locked_or_at_safepoint(_should_be_vm_thread_)             \\  do {                                                                        \\    assert(Heap_lock->owned_by_self() ||                                      \\           (SafepointSynchronize::is_at_safepoint() &&                        \\             ((_should_be_vm_thread_) == Thread::current()->is_VM_thread())), \\           heap_locking_asserts_params(\"should be holding the Heap_lock or \"  \\                                        \"should be at a safepoint\"));         \\  } while (0)#define assert_heap_locked_and_not_at_safepoint()                             \\  do {                                                                        \\    assert(Heap_lock->owned_by_self() &&                                      \\                                    !SafepointSynchronize::is_at_safepoint(), \\          heap_locking_asserts_params(\"should be holding the Heap_lock and \"  \\                                       \"should not be at a safepoint\"));      \\  } while (0)#define assert_heap_not_locked()                                              \\  do {                                                                        \\    assert(!Heap_lock->owned_by_self(),                                       \\        heap_locking_asserts_params(\"should not be holding the Heap_lock\"));  \\  } while (0)#define assert_heap_not_locked_and_not_at_safepoint()                         \\  do {                                                                        \\    assert(!Heap_lock->owned_by_self() &&                                     \\                                    !SafepointSynchronize::is_at_safepoint(), \\      heap_locking_asserts_params(\"should not be holding the Heap_lock and \"  \\                                   \"should not be at a safepoint\"));          \\  } while (0)#define assert_at_safepoint_on_vm_thread()                                        \\  do {                                                                            \\    assert_at_safepoint();                                                        \\    assert(Thread::current_or_null() != nullptr, \"no current thread\");            \\    assert(Thread::current()->is_VM_thread(), \"current thread is not VM thread\"); \\  } while (0)#ifdef ASSERT#define assert_used_and_recalculate_used_equal(g1h)                           \\  do {                                                                        \\    size_t cur_used_bytes = g1h->used();                                      \\    size_t recal_used_bytes = g1h->recalculate_used();                        \\    assert(cur_used_bytes == recal_used_bytes, \"Used(%zu) is not\" \\           \" same as recalculated used(%zu).\",                    \\           cur_used_bytes, recal_used_bytes);                                 \\  } while (0)#else#define assert_used_and_recalculate_used_equal(g1h) do {} while(0)#endif  \/\/ The young region list.  G1EdenRegions _eden;  G1SurvivorRegions _survivor;  STWGCTimer* _gc_timer_stw;  G1NewTracer* _gc_tracer_stw;  \/\/ The current policy object for the collector.  G1Policy* _policy;  G1HeapSizingPolicy* _heap_sizing_policy;  G1CollectionSet _collection_set;  \/\/ Try to allocate a single non-humongous G1HeapRegion sufficient for  \/\/ an allocation of the given word_size. If do_expand is true,  \/\/ attempt to expand the heap if necessary to satisfy the allocation  \/\/ request. 'type' takes the type of region to be allocated. (Use constants  \/\/ Old, Eden, Humongous, Survivor defined in G1HeapRegionType.)  G1HeapRegion* new_region(size_t word_size,                           G1HeapRegionType type,                           bool do_expand,                           uint node_index = G1NUMA::AnyNodeIndex);  \/\/ Initialize a contiguous set of free regions of length num_regions  \/\/ and starting at index first so that they appear as a single  \/\/ humongous region.  HeapWord* humongous_obj_allocate_initialize_regions(G1HeapRegion* first_hr,                                                      uint num_regions,                                                      size_t word_size);  \/\/ Attempt to allocate a humongous object of the given size. Return  \/\/ null if unsuccessful.  HeapWord* humongous_obj_allocate(size_t word_size);  \/\/ The following two methods, allocate_new_tlab() and  \/\/ mem_allocate(), are the two main entry points from the runtime  \/\/ into the G1's allocation routines. They have the following  \/\/ assumptions:  \/\/  \/\/ * They should both be called outside safepoints.  \/\/  \/\/ * They should both be called without holding the Heap_lock.  \/\/  \/\/ * All allocation requests for new TLABs should go to  \/\/   allocate_new_tlab().  \/\/  \/\/ * All non-TLAB allocation requests should go to mem_allocate().  \/\/  \/\/ * If either call cannot satisfy the allocation request using the  \/\/   current allocating region, they will try to get a new one. If  \/\/   this fails, they will attempt to do an evacuation pause and  \/\/   retry the allocation.  \/\/  \/\/ * If all allocation attempts fail, even after trying to schedule  \/\/   an evacuation pause, allocate_new_tlab() will return null,  \/\/   whereas mem_allocate() will attempt a heap expansion and\/or  \/\/   schedule a Full GC.  \/\/  \/\/ * We do not allow humongous-sized TLABs. So, allocate_new_tlab  \/\/   should never be called with word_size being humongous. All  \/\/   humongous allocation requests should go to mem_allocate() which  \/\/   will satisfy them with a special path.  HeapWord* allocate_new_tlab(size_t min_size,                              size_t requested_size,                              size_t* actual_size) override;  HeapWord* mem_allocate(size_t word_size,                         bool*  gc_overhead_limit_was_exceeded) override;  \/\/ First-level mutator allocation attempt: try to allocate out of  \/\/ the mutator alloc region without taking the Heap_lock. This  \/\/ should only be used for non-humongous allocations.  inline HeapWord* attempt_allocation(size_t min_word_size,                                      size_t desired_word_size,                                      size_t* actual_word_size);  \/\/ Second-level mutator allocation attempt: take the Heap_lock and  \/\/ retry the allocation attempt, potentially scheduling a GC  \/\/ pause. This should only be used for non-humongous allocations.  HeapWord* attempt_allocation_slow(uint node_index, size_t word_size);  \/\/ Takes the Heap_lock and attempts a humongous allocation. It can  \/\/ potentially schedule a GC pause.  HeapWord* attempt_allocation_humongous(size_t word_size);  \/\/ Allocation attempt that should be called during safepoints (e.g.,  \/\/ at the end of a successful GC). expect_null_mutator_alloc_region  \/\/ specifies whether the mutator alloc region is expected to be null  \/\/ or not.  HeapWord* attempt_allocation_at_safepoint(size_t word_size,                                            bool expect_null_mutator_alloc_region);  \/\/ These methods are the \"callbacks\" from the G1AllocRegion class.  \/\/ For mutator alloc regions.  G1HeapRegion* new_mutator_alloc_region(size_t word_size, uint node_index);  void retire_mutator_alloc_region(G1HeapRegion* alloc_region,                                   size_t allocated_bytes);  \/\/ For GC alloc regions.  bool has_more_regions(G1HeapRegionAttr dest);  G1HeapRegion* new_gc_alloc_region(size_t word_size, G1HeapRegionAttr dest, uint node_index);  void retire_gc_alloc_region(G1HeapRegion* alloc_region,                              size_t allocated_bytes, G1HeapRegionAttr dest);  \/\/ - if clear_all_soft_refs is true, all soft references should be  \/\/   cleared during the GC.  \/\/ - if do_maximal_compaction is true, full gc will do a maximally  \/\/   compacting collection, leaving no dead wood.  \/\/ - if allocation_word_size is set, then this allocation size will  \/\/    be accounted for in case shrinking of the heap happens.  \/\/ - it returns false if it is unable to do the collection due to the  \/\/   GC locker being active, true otherwise.  void do_full_collection(bool clear_all_soft_refs,                          bool do_maximal_compaction,                          size_t allocation_word_size);  \/\/ Callback from VM_G1CollectFull operation, or collect_as_vm_thread.  void do_full_collection(bool clear_all_soft_refs) override;  \/\/ Helper to do a full collection that clears soft references.  void upgrade_to_full_collection();  \/\/ Callback from VM_G1CollectForAllocation operation.  \/\/ This function does everything necessary\/possible to satisfy a  \/\/ failed allocation request (including collection, expansion, etc.)  HeapWord* satisfy_failed_allocation(size_t word_size);  \/\/ Internal helpers used during full GC to split it up to  \/\/ increase readability.  bool abort_concurrent_cycle();  void verify_before_full_collection();  void prepare_heap_for_full_collection();  void prepare_for_mutator_after_full_collection(size_t allocation_word_size);  void abort_refinement();  void verify_after_full_collection();  void print_heap_after_full_collection();  \/\/ Helper method for satisfy_failed_allocation()  HeapWord* satisfy_failed_allocation_helper(size_t word_size,                                             bool do_gc,                                             bool maximal_compaction,                                             bool expect_null_mutator_alloc_region);  \/\/ Attempting to expand the heap sufficiently  \/\/ to support an allocation of the given \"word_size\".  If  \/\/ successful, perform the allocation and return the address of the  \/\/ allocated block, or else null.  HeapWord* expand_and_allocate(size_t word_size);  void verify_numa_regions(const char* desc);public:  \/\/ If during a concurrent start pause we may install a pending list head which is not  \/\/ otherwise reachable, ensure that it is marked in the bitmap for concurrent marking  \/\/ to discover.  void make_pending_list_reachable();  G1ServiceThread* service_thread() const { return _service_thread; }  WorkerThreads* workers() const { return _workers; }  \/\/ Run the given batch task using the workers.  void run_batch_task(G1BatchedTask* cl);  \/\/ Return \"optimal\" number of chunks per region we want to use for claiming areas  \/\/ within a region to claim.  \/\/ The returned value is a trade-off between granularity of work distribution and  \/\/ memory usage and maintenance costs of that table.  \/\/ Testing showed that 64 for 1M\/2M region, 128 for 4M\/8M regions, 256 for 16\/32M regions,  \/\/ and so on seems to be such a good trade-off.  static uint get_chunks_per_region();  G1Allocator* allocator() {    return _allocator;  }  G1YoungGCAllocationFailureInjector* allocation_failure_injector() { return &_allocation_failure_injector; }  G1HeapVerifier* verifier() {    return _verifier;  }  G1MonitoringSupport* monitoring_support() {    assert(_monitoring_support != nullptr, \"should have been initialized\");    return _monitoring_support;  }  void pin_object(JavaThread* thread, oop obj) override;  void unpin_object(JavaThread* thread, oop obj) override;  void resize_heap_if_necessary(size_t allocation_word_size);  \/\/ Check if there is memory to uncommit and if so schedule a task to do it.  void uncommit_regions_if_necessary();  \/\/ Immediately uncommit uncommittable regions.  uint uncommit_regions(uint region_limit);  bool has_uncommittable_regions();  G1NUMA* numa() const { return _numa; }  \/\/ Expand the garbage-first heap by at least the given size (in bytes!).  \/\/ Returns true if the heap was expanded by the requested amount;  \/\/ false otherwise.  \/\/ (Rounds up to a G1HeapRegion boundary.)  bool expand(size_t expand_bytes, WorkerThreads* pretouch_workers);  bool expand_single_region(uint node_index);  \/\/ Request an immediate heap contraction of (at most) the given number of bytes.  \/\/ Returns true if any pages were actually uncommitted.  bool request_heap_shrink(size_t shrink_bytes);  \/\/ Returns the PLAB statistics for a given destination.  inline G1EvacStats* alloc_buffer_stats(G1HeapRegionAttr dest);  \/\/ Determines PLAB size for a given destination.  inline size_t desired_plab_sz(G1HeapRegionAttr dest);  \/\/ Clamp the given PLAB word size to allowed values. Prevents humongous PLAB sizes  \/\/ for two reasons:  \/\/ * PLABs are allocated using a similar paths as oops, but should  \/\/   never be in a humongous region  \/\/ * Allowing humongous PLABs needlessly churns the region free lists  inline size_t clamp_plab_size(size_t value) const;  \/\/ Do anything common to GC's.  void gc_prologue(bool full);  void gc_epilogue(bool full);  \/\/ Does the given region fulfill remembered set based eager reclaim candidate requirements?  bool is_potential_eager_reclaim_candidate(G1HeapRegion* r) const;  inline bool is_humongous_reclaim_candidate(uint region);  \/\/ Remove from the reclaim candidate set.  Also remove from the  \/\/ collection set so that later encounters avoid the slow path.  inline void set_humongous_is_live(oop obj);  \/\/ Register the given region to be part of the collection set.  inline void register_humongous_candidate_region_with_region_attr(uint index);  void set_humongous_metadata(G1HeapRegion* first_hr,                              uint num_regions,                              size_t word_size,                              bool update_remsets);  \/\/ We register a region with the fast \"in collection set\" test. We  \/\/ simply set to true the array slot corresponding to this region.  void register_young_region_with_region_attr(G1HeapRegion* r) {    _region_attr.set_in_young(r->hrm_index(), r->has_pinned_objects());  }  inline void register_new_survivor_region_with_region_attr(G1HeapRegion* r);  inline void register_region_with_region_attr(G1HeapRegion* r);  inline void register_old_region_with_region_attr(G1HeapRegion* r);  inline void register_optional_region_with_region_attr(G1HeapRegion* r);  void clear_region_attr(const G1HeapRegion* hr) {    _region_attr.clear(hr);  }  void clear_region_attr() {    _region_attr.clear();  }  \/\/ Verify that the G1RegionAttr remset tracking corresponds to actual remset tracking  \/\/ for all regions.  void verify_region_attr_remset_is_tracked() PRODUCT_RETURN;  void clear_bitmap_for_region(G1HeapRegion* hr);  bool is_user_requested_concurrent_full_gc(GCCause::Cause cause);  \/\/ This is called at the start of either a concurrent cycle or a Full  \/\/ GC to update the number of old marking cycles started.  void increment_old_marking_cycles_started();  \/\/ This is called at the end of either a concurrent cycle or a Full  \/\/ GC to update the number of old marking cycles completed. Those two  \/\/ can happen in a nested fashion, i.e., we start a concurrent  \/\/ cycle, a Full GC happens half-way through it which ends first,  \/\/ and then the cycle notices that a Full GC happened and ends  \/\/ too. The concurrent parameter is a boolean to help us do a bit  \/\/ tighter consistency checking in the method. If concurrent is  \/\/ false, the caller is the inner caller in the nesting (i.e., the  \/\/ Full GC). If concurrent is true, the caller is the outer caller  \/\/ in this nesting (i.e., the concurrent cycle). Further nesting is  \/\/ not currently supported. The end of this call also notifies  \/\/ the G1OldGCCount_lock in case a Java thread is waiting for a full  \/\/ GC to happen (e.g., it called System.gc() with  \/\/ +ExplicitGCInvokesConcurrent).  \/\/ whole_heap_examined should indicate that during that old marking  \/\/ cycle the whole heap has been examined for live objects (as opposed  \/\/ to only parts, or aborted before completion).  void increment_old_marking_cycles_completed(bool concurrent, bool whole_heap_examined);  uint old_marking_cycles_started() const {    return _old_marking_cycles_started;  }  uint old_marking_cycles_completed() const {    return _old_marking_cycles_completed;  }  \/\/ Allocates a new heap region instance.  G1HeapRegion* new_heap_region(uint hrs_index, MemRegion mr);  \/\/ Frees a region by resetting its metadata and adding it to the free list  \/\/ passed as a parameter (this is usually a local list which will be appended  \/\/ to the master free list later or null if free list management is handled  \/\/ in another way).  \/\/ Callers must ensure they are the only one calling free on the given region  \/\/ at the same time.  void free_region(G1HeapRegion* hr, G1FreeRegionList* free_list);  \/\/ Add the given region to the retained regions collection set candidates.  void retain_region(G1HeapRegion* hr);  \/\/ It dirties the cards that cover the block so that the post  \/\/ write barrier never queues anything when updating objects on this  \/\/ block. It is assumed (and in fact we assert) that the block  \/\/ belongs to a young region.  inline void dirty_young_block(HeapWord* start, size_t word_size);  \/\/ Frees a humongous region by collapsing it into individual regions  \/\/ and calling free_region() for each of them. The freed regions  \/\/ will be added to the free list that's passed as a parameter (this  \/\/ is usually a local list which will be appended to the master free  \/\/ list later).  \/\/ The method assumes that only a single thread is ever calling  \/\/ this for a particular region at once.  void free_humongous_region(G1HeapRegion* hr,                             G1FreeRegionList* free_list);  \/\/ Execute func(G1HeapRegion* r, bool is_last) on every region covered by the  \/\/ given range.  template <typename Func>  void iterate_regions_in_range(MemRegion range, const Func& func);  \/\/ Commit the required number of G1 region(s) according to the size requested  \/\/ and mark them as 'old' region(s). Preferred address is treated as a hint for  \/\/ the location of the archive space in the heap. The returned address may or may  \/\/ not be same as the preferred address.  \/\/ This API is only used for allocating heap space for the archived heap objects  \/\/ in the CDS archive.  HeapWord* alloc_archive_region(size_t word_size, HeapWord* preferred_addr);  \/\/ Populate the G1BlockOffsetTable for archived regions with the given  \/\/ memory range.  void populate_archive_regions_bot(MemRegion range);  \/\/ For the specified range, uncommit the containing G1 regions  \/\/ which had been allocated by alloc_archive_regions. This should be called  \/\/ at JVM init time if the archive heap's contents cannot be used (e.g., if  \/\/ CRC check fails).  void dealloc_archive_regions(MemRegion range);private:  \/\/ Shrink the garbage-first heap by at most the given size (in bytes!).  \/\/ (Rounds down to a G1HeapRegion boundary.)  void shrink(size_t shrink_bytes);  void shrink_helper(size_t expand_bytes);  \/\/ Schedule the VM operation that will do an evacuation pause to  \/\/ satisfy an allocation request of word_size. *succeeded will  \/\/ return whether the VM operation was successful (it did do an  \/\/ evacuation pause) or not (another thread beat us to it or the GC  \/\/ locker was active). Given that we should not be holding the  \/\/ Heap_lock when we enter this method, we will pass the  \/\/ gc_count_before (i.e., total_collections()) as a parameter since  \/\/ it has to be read while holding the Heap_lock. Currently, both  \/\/ methods that call do_collection_pause() release the Heap_lock  \/\/ before the call, so it's easy to read gc_count_before just before.  HeapWord* do_collection_pause(size_t         word_size,                                uint           gc_count_before,                                bool*          succeeded,                                GCCause::Cause gc_cause);  \/\/ Perform an incremental collection at a safepoint, possibly  \/\/ followed by a by-policy upgrade to a full collection.  \/\/ precondition: at safepoint on VM thread  \/\/ precondition: !is_stw_gc_active()  void do_collection_pause_at_safepoint();  \/\/ Helper for do_collection_pause_at_safepoint, containing the guts  \/\/ of the incremental collection pause, executed by the vm thread.  void do_collection_pause_at_safepoint_helper();  void verify_before_young_collection(G1HeapVerifier::G1VerifyType type);  void verify_after_young_collection(G1HeapVerifier::G1VerifyType type);public:  \/\/ Start a concurrent cycle.  void start_concurrent_cycle(bool concurrent_operation_is_full_mark);  void prepare_for_mutator_after_young_collection();  void retire_tlabs();  \/\/ Update all region's pin counts from the per-thread caches and resets them.  \/\/ Must be called before any decision based on pin counts.  void flush_region_pin_cache();  void expand_heap_after_young_collection();  \/\/ Update object copying statistics.  void record_obj_copy_mem_stats();private:  \/\/ The g1 remembered set of the heap.  G1RemSet* _rem_set;  \/\/ Global card set configuration  G1CardSetConfiguration _card_set_config;  G1MonotonicArenaFreePool _card_set_freelist_pool;  \/\/ Group cardsets  G1CSetCandidateGroup _young_regions_cset_group;public:  G1CardSetConfiguration* card_set_config() { return &_card_set_config; }  G1CSetCandidateGroup* young_regions_cset_group() { return &_young_regions_cset_group; }  G1CardSet* young_regions_cardset() { return _young_regions_cset_group.card_set(); };  G1MonotonicArenaMemoryStats young_regions_card_set_memory_stats() { return _young_regions_cset_group.card_set_memory_stats(); }  void prepare_group_cardsets_for_scan();  \/\/ After a collection pause, reset eden and the collection set.  void clear_eden();  void clear_collection_set();  \/\/ Abandon the current collection set without recording policy  \/\/ statistics or updating free lists.  void abandon_collection_set(G1CollectionSet* collection_set);  \/\/ The concurrent marker (and the thread it runs in.)  G1ConcurrentMark* _cm;  G1ConcurrentMarkThread* _cm_thread;  \/\/ The concurrent refiner.  G1ConcurrentRefine* _cr;  \/\/ Reusable parallel task queues and partial array manager.  G1ScannerTasksQueueSet* _task_queues;  PartialArrayStateManager* _partial_array_state_manager;  \/\/ (\"Weak\") Reference processing support.  \/\/  \/\/ G1 has 2 instances of the reference processor class.  \/\/  \/\/ One (_ref_processor_cm) handles reference object discovery and subsequent  \/\/ processing during concurrent marking cycles. Discovery is enabled\/disabled  \/\/ at the start\/end of a concurrent marking cycle.  \/\/  \/\/ The other (_ref_processor_stw) handles reference object discovery and  \/\/ processing during incremental evacuation pauses and full GC pauses.  \/\/  \/\/ ## Incremental evacuation pauses  \/\/  \/\/ STW ref processor discovery is enabled\/disabled at the start\/end of an  \/\/ incremental evacuation pause. No particular handling of the CM ref  \/\/ processor is needed, apart from treating the discovered references as  \/\/ roots; CM discovery does not need to be temporarily disabled as all  \/\/ marking threads are paused during incremental evacuation pauses.  \/\/  \/\/ ## Full GC pauses  \/\/  \/\/ We abort any ongoing concurrent marking cycle, disable CM discovery, and  \/\/ temporarily substitute a new closure for the STW ref processor's  \/\/ _is_alive_non_header field (old value is restored after the full GC). Then  \/\/ STW ref processor discovery is enabled, and marking & compaction  \/\/ commences.  \/\/ The (stw) reference processor...  ReferenceProcessor* _ref_processor_stw;  \/\/ During reference object discovery, the _is_alive_non_header  \/\/ closure (if non-null) is applied to the referent object to  \/\/ determine whether the referent is live. If so then the  \/\/ reference object does not need to be 'discovered' and can  \/\/ be treated as a regular oop. This has the benefit of reducing  \/\/ the number of 'discovered' reference objects that need to  \/\/ be processed.  \/\/  \/\/ Instance of the is_alive closure for embedding into the  \/\/ STW reference processor as the _is_alive_non_header field.  \/\/ Supplying a value for the _is_alive_non_header field is  \/\/ optional but doing so prevents unnecessary additions to  \/\/ the discovered lists during reference discovery.  G1STWIsAliveClosure _is_alive_closure_stw;  G1STWSubjectToDiscoveryClosure _is_subject_to_discovery_stw;  \/\/ The (concurrent marking) reference processor...  ReferenceProcessor* _ref_processor_cm;  \/\/ Instance of the concurrent mark is_alive closure for embedding  \/\/ into the Concurrent Marking reference processor as the  \/\/ _is_alive_non_header field. Supplying a value for the  \/\/ _is_alive_non_header field is optional but doing so prevents  \/\/ unnecessary additions to the discovered lists during reference  \/\/ discovery.  G1CMIsAliveClosure _is_alive_closure_cm;  G1CMSubjectToDiscoveryClosure _is_subject_to_discovery_cm;public:  G1ScannerTasksQueueSet* task_queues() const;  G1ScannerTasksQueue* task_queue(uint i) const;  PartialArrayStateManager* partial_array_state_manager() const;  \/\/ Create a G1CollectedHeap.  \/\/ Must call the initialize method afterwards.  \/\/ May not return if something goes wrong.  G1CollectedHeap();private:  jint initialize_concurrent_refinement();  jint initialize_service_thread();public:  \/\/ Initialize the G1CollectedHeap to have the initial and  \/\/ maximum sizes and remembered and barrier sets  \/\/ specified by the policy object.  jint initialize() override;  \/\/ Returns whether concurrent mark threads (and the VM) are about to terminate.  bool concurrent_mark_is_terminating() const;  void stop() override;  void safepoint_synchronize_begin() override;  void safepoint_synchronize_end() override;  \/\/ Does operations required after initialization has been done.  void post_initialize() override;  \/\/ Initialize weak reference processing.  void ref_processing_init();  Name kind() const override {    return CollectedHeap::G1;  }  const char* name() const override {    return \"G1\";  }  const G1CollectorState* collector_state() const { return &_collector_state; }  G1CollectorState* collector_state() { return &_collector_state; }  \/\/ The current policy object for the collector.  G1Policy* policy() const { return _policy; }  \/\/ The heap sizing policy.  G1HeapSizingPolicy* heap_sizing_policy() const { return _heap_sizing_policy; }  \/\/ The remembered set.  G1RemSet* rem_set() const { return _rem_set; }  const G1MonotonicArenaFreePool* card_set_freelist_pool() const { return &_card_set_freelist_pool; }  G1MonotonicArenaFreePool* card_set_freelist_pool() { return &_card_set_freelist_pool; }  inline G1GCPhaseTimes* phase_times() const;  const G1CollectionSet* collection_set() const { return &_collection_set; }  G1CollectionSet* collection_set() { return &_collection_set; }  inline bool is_collection_set_candidate(const G1HeapRegion* r) const;  void initialize_serviceability() override;  MemoryUsage memory_usage() override;  GrowableArray<GCMemoryManager*> memory_managers() override;  GrowableArray<MemoryPool*> memory_pools() override;  void fill_with_dummy_object(HeapWord* start, HeapWord* end, bool zap) override;  static void start_codecache_marking_cycle_if_inactive(bool concurrent_mark_start);  static void finish_codecache_marking_cycle();  \/\/ The shared block offset table array.  G1BlockOffsetTable* bot() const { return _bot; }  \/\/ Reference Processing accessors  \/\/ The STW reference processor....  ReferenceProcessor* ref_processor_stw() const { return _ref_processor_stw; }  G1NewTracer* gc_tracer_stw() const { return _gc_tracer_stw; }  STWGCTimer* gc_timer_stw() const { return _gc_timer_stw; }  \/\/ The Concurrent Marking reference processor...  ReferenceProcessor* ref_processor_cm() const { return _ref_processor_cm; }  size_t unused_committed_regions_in_bytes() const;  size_t capacity() const override;  size_t used() const override;  \/\/ This should be called when we're not holding the heap lock. The  \/\/ result might be a bit inaccurate.  size_t used_unlocked() const;  size_t recalculate_used() const;  \/\/ These virtual functions do the actual allocation.  \/\/ Some heaps may offer a contiguous region for shared non-blocking  \/\/ allocation, via inlined code (by exporting the address of the top and  \/\/ end fields defining the extent of the contiguous allocation region.)  \/\/ But G1CollectedHeap doesn't yet support this.  \/\/ Returns true if an incremental GC should be upgrade to a full gc. This  \/\/ is done when there are no free regions and the heap can't be expanded.  bool should_upgrade_to_full_gc() const {    return num_available_regions() == 0;  }  \/\/ The number of inactive regions.  uint num_inactive_regions() const { return _hrm.num_inactive_regions(); }  \/\/ The current number of regions in the heap.  uint num_committed_regions() const { return _hrm.num_committed_regions(); }  \/\/ The max number of regions reserved for the heap.  uint max_num_regions() const { return _hrm.max_num_regions(); }  \/\/ The number of regions that are completely free.  uint num_free_regions() const { return _hrm.num_free_regions(); }  \/\/ The number of regions that are not completely free.  uint num_used_regions() const { return _hrm.num_used_regions(); }  \/\/ The number of regions that can be allocated into.  uint num_available_regions() const { return num_free_regions() + num_inactive_regions(); }  MemoryUsage get_auxiliary_data_memory_usage() const {    return _hrm.get_auxiliary_data_memory_usage();  }#ifdef ASSERT  bool is_on_master_free_list(G1HeapRegion* hr) {    return _hrm.is_free(hr);  }#endif \/\/ ASSERT  inline void old_set_add(G1HeapRegion* hr);  inline void old_set_remove(G1HeapRegion* hr);  size_t non_young_capacity_bytes() {    return (old_regions_count() + humongous_regions_count()) * G1HeapRegion::GrainBytes;  }  \/\/ Determine whether the given region is one that we are using as an  \/\/ old GC alloc region.  bool is_old_gc_alloc_region(G1HeapRegion* hr);  \/\/ Perform a collection of the heap; intended for use in implementing  \/\/ \"System.gc\".  This probably implies as full a collection as the  \/\/ \"CollectedHeap\" supports.  void collect(GCCause::Cause cause) override;  \/\/ Perform a collection of the heap with the given cause.  \/\/ Returns whether this collection actually executed.  bool try_collect(GCCause::Cause cause, const G1GCCounters& counters_before);  void start_concurrent_gc_for_metadata_allocation(GCCause::Cause gc_cause);  void remove_from_old_gen_sets(const uint old_regions_removed,                                const uint humongous_regions_removed);  void prepend_to_freelist(G1FreeRegionList* list);  void decrement_summary_bytes(size_t bytes);  bool is_in(const void* p) const override;  \/\/ Return \"TRUE\" iff the given object address is within the collection  \/\/ set. Assumes that the reference points into the heap.  inline bool is_in_cset(const G1HeapRegion* hr) const;  inline bool is_in_cset(oop obj) const;  inline bool is_in_cset(HeapWord* addr) const;  inline bool is_in_cset_or_humongous_candidate(const oop obj); private:  \/\/ This array is used for a quick test on whether a reference points into  \/\/ the collection set or not. Each of the array's elements denotes whether the  \/\/ corresponding region is in the collection set or not.  G1HeapRegionAttrBiasedMappedArray _region_attr; public:  inline G1HeapRegionAttr region_attr(const void* obj) const;  inline G1HeapRegionAttr region_attr(uint idx) const;  MemRegion reserved() const {    return _hrm.reserved();  }  bool is_in_reserved(const void* addr) const {    return reserved().contains(addr);  }  G1CardTable* card_table() const {    return _card_table;  }  \/\/ Iteration functions.  void object_iterate_parallel(ObjectClosure* cl, uint worker_id, G1HeapRegionClaimer* claimer);  \/\/ Iterate over all objects, calling \"cl.do_object\" on each.  void object_iterate(ObjectClosure* cl) override;  ParallelObjectIteratorImpl* parallel_object_iterator(uint thread_num) override;  \/\/ Keep alive an object that was loaded with AS_NO_KEEPALIVE.  void keep_alive(oop obj) override;  \/\/ Iterate over heap regions, in address order, terminating the  \/\/ iteration early if the \"do_heap_region\" method returns \"true\".  void heap_region_iterate(G1HeapRegionClosure* blk) const;  void heap_region_iterate(G1HeapRegionIndexClosure* blk) const;  \/\/ Return the region with the given index. It assumes the index is valid.  inline G1HeapRegion* region_at(uint index) const;  inline G1HeapRegion* region_at_or_null(uint index) const;  \/\/ Iterate over the regions that the humongous object starting at the given  \/\/ region and apply the given method with the signature f(G1HeapRegion*) on them.  template <typename Func>  void humongous_obj_regions_iterate(G1HeapRegion* start, const Func& f);  \/\/ Calculate the region index of the given address. Given address must be  \/\/ within the heap.  inline uint addr_to_region(const void* addr) const;  inline HeapWord* bottom_addr_for_region(uint index) const;  \/\/ Two functions to iterate over the heap regions in parallel. Threads  \/\/ compete using the G1HeapRegionClaimer to claim the regions before  \/\/ applying the closure on them.  \/\/ The _from_worker_offset version uses the G1HeapRegionClaimer and  \/\/ the worker id to calculate a start offset to prevent all workers to  \/\/ start from the point.  void heap_region_par_iterate_from_worker_offset(G1HeapRegionClosure* cl,                                                  G1HeapRegionClaimer* hrclaimer,                                                  uint worker_id) const;  void heap_region_par_iterate_from_start(G1HeapRegionClosure* cl,                                          G1HeapRegionClaimer* hrclaimer) const;  \/\/ Iterate over all regions in the collection set in parallel.  void collection_set_par_iterate_all(G1HeapRegionClosure* cl,                                      G1HeapRegionClaimer* hr_claimer,                                      uint worker_id);  \/\/ Iterate over all regions currently in the current collection set.  void collection_set_iterate_all(G1HeapRegionClosure* blk);  \/\/ Iterate over the regions in the current increment of the collection set.  \/\/ Starts the iteration so that the start regions of a given worker id over the  \/\/ set active_workers are evenly spread across the set of collection set regions  \/\/ to be iterated.  \/\/ The variant with the G1HeapRegionClaimer guarantees that the closure will be  \/\/ applied to a particular region exactly once.  void collection_set_iterate_increment_from(G1HeapRegionClosure *blk, uint worker_id) {    collection_set_iterate_increment_from(blk, nullptr, worker_id);  }  void collection_set_iterate_increment_from(G1HeapRegionClosure *blk, G1HeapRegionClaimer* hr_claimer, uint worker_id);  \/\/ Iterate over the array of region indexes, uint regions[length], applying  \/\/ the given G1HeapRegionClosure on each region. The worker_id will determine where  \/\/ to start the iteration to allow for more efficient parallel iteration.  void par_iterate_regions_array(G1HeapRegionClosure* cl,                                 G1HeapRegionClaimer* hr_claimer,                                 const uint regions[],                                 size_t length,                                 uint worker_id) const;  \/\/ Returns the G1HeapRegion that contains addr. addr must not be null.  inline G1HeapRegion* heap_region_containing(const void* addr) const;  \/\/ Returns the G1HeapRegion that contains addr, or null if that is an uncommitted  \/\/ region. addr must not be null.  inline G1HeapRegion* heap_region_containing_or_null(const void* addr) const;  \/\/ A CollectedHeap is divided into a dense sequence of \"blocks\"; that is,  \/\/ each address in the (reserved) heap is a member of exactly  \/\/ one block.  The defining characteristic of a block is that it is  \/\/ possible to find its size, and thus to progress forward to the next  \/\/ block.  (Blocks may be of different sizes.)  Thus, blocks may  \/\/ represent Java objects, or they might be free blocks in a  \/\/ free-list-based heap (or subheap), as long as the two kinds are  \/\/ distinguishable and the size of each is determinable.  \/\/ Returns the address of the start of the \"block\" that contains the  \/\/ address \"addr\".  We say \"blocks\" instead of \"object\" since some heaps  \/\/ may not pack objects densely; a chunk may either be an object or a  \/\/ non-object.  HeapWord* block_start(const void* addr) const;  \/\/ Requires \"addr\" to be the start of a block, and returns \"TRUE\" iff  \/\/ the block is an object.  bool block_is_obj(const HeapWord* addr) const;  \/\/ Section on thread-local allocation buffers (TLABs)  \/\/ See CollectedHeap for semantics.  size_t tlab_capacity(Thread* ignored) const override;  size_t tlab_used(Thread* ignored) const override;  size_t max_tlab_size() const override;  size_t unsafe_max_tlab_alloc(Thread* ignored) const override;  inline bool is_in_young(const oop obj) const;  inline bool requires_barriers(stackChunkOop obj) const override;  \/\/ Returns \"true\" iff the given word_size is \"very large\".  static bool is_humongous(size_t word_size) {    \/\/ Note this has to be strictly greater-than as the TLABs    \/\/ are capped at the humongous threshold and we want to    \/\/ ensure that we don't try to allocate a TLAB as    \/\/ humongous and that we don't allocate a humongous    \/\/ object in a TLAB.    return word_size > _humongous_object_threshold_in_words;  }  \/\/ Returns the humongous threshold for a specific region size  static size_t humongous_threshold_for(size_t region_size) {    return (region_size \/ 2);  }  \/\/ Returns the number of regions the humongous object of the given word size  \/\/ requires.  static size_t humongous_obj_size_in_regions(size_t word_size);  \/\/ Print the maximum heap capacity.  size_t max_capacity() const override;  Tickspan time_since_last_collection() const { return Ticks::now() - _collection_pause_end; }  \/\/ Convenience function to be used in situations where the heap type can be  \/\/ asserted to be this type.  static G1CollectedHeap* heap() {    return named_heap<G1CollectedHeap>(CollectedHeap::G1);  }  void set_region_short_lived_locked(G1HeapRegion* hr);  \/\/ add appropriate methods for any other surv rate groups  G1SurvivorRegions* survivor() { return &_survivor; }  uint eden_regions_count() const { return _eden.length(); }  uint eden_regions_count(uint node_index) const { return _eden.regions_on_node(node_index); }  uint survivor_regions_count() const { return _survivor.length(); }  uint survivor_regions_count(uint node_index) const { return _survivor.regions_on_node(node_index); }  size_t eden_regions_used_bytes() const { return _eden.used_bytes(); }  size_t survivor_regions_used_bytes() const { return _survivor.used_bytes(); }  uint young_regions_count() const { return _eden.length() + _survivor.length(); }  uint old_regions_count() const { return _old_set.length(); }  uint humongous_regions_count() const { return _humongous_set.length(); }#ifdef ASSERT  bool check_young_list_empty();#endif  bool is_marked(oop obj) const;  inline static bool is_obj_filler(const oop obj);  \/\/ Determine if an object is dead, given the object and also  \/\/ the region to which the object belongs.  inline bool is_obj_dead(const oop obj, const G1HeapRegion* hr) const;  \/\/ Determine if an object is dead, given only the object itself.  \/\/ This will find the region to which the object belongs and  \/\/ then call the region version of the same function.  \/\/ If obj is null it is not dead.  inline bool is_obj_dead(const oop obj) const;  inline bool is_obj_dead_full(const oop obj, const G1HeapRegion* hr) const;  inline bool is_obj_dead_full(const oop obj) const;  \/\/ Mark the live object that failed evacuation in the bitmap.  void mark_evac_failure_object(uint worker_id, oop obj, size_t obj_size) const;  G1ConcurrentMark* concurrent_mark() const { return _cm; }  \/\/ Refinement  G1ConcurrentRefine* concurrent_refine() const { return _cr; }  \/\/ Optimized nmethod scanning support routines  \/\/ Register the given nmethod with the G1 heap.  void register_nmethod(nmethod* nm) override;  \/\/ Unregister the given nmethod from the G1 heap.  void unregister_nmethod(nmethod* nm) override;  \/\/ No nmethod verification implemented.  void verify_nmethod(nmethod* nm) override {}  \/\/ Recalculate amount of used memory after GC. Must be called after all allocation  \/\/ has finished.  void update_used_after_gc(bool evacuation_failed);  \/\/ Rebuild the code root lists for each region  \/\/ after a full GC.  void rebuild_code_roots();  \/\/ Performs cleaning of data structures after class unloading.  void complete_cleaning(bool class_unloading_occurred);  void unload_classes_and_code(const char* description, BoolObjectClosure* cl, GCTimer* timer);  void bulk_unregister_nmethods();  \/\/ Verification  \/\/ Perform any cleanup actions necessary before allowing a verification.  void prepare_for_verify() override;  \/\/ Perform verification.  void verify(VerifyOption vo) override;  \/\/ WhiteBox testing support.  bool supports_concurrent_gc_breakpoints() const override;  WorkerThreads* safepoint_workers() override { return _workers; }  \/\/ The methods below are here for convenience and dispatch the  \/\/ appropriate method depending on value of the given VerifyOption  \/\/ parameter. The values for that parameter, and their meanings,  \/\/ are the same as those above.  bool is_obj_dead_cond(const oop obj,                        const G1HeapRegion* hr,                        const VerifyOption vo) const;  bool is_obj_dead_cond(const oop obj,                        const VerifyOption vo) const;  G1HeapSummary create_g1_heap_summary();  G1EvacSummary create_g1_evac_summary(G1EvacStats* stats);  \/\/ Printingprivate:  void print_heap_regions() const;  void print_regions_on(outputStream* st) const;public:  void print_heap_on(outputStream* st) const override;  void print_extended_on(outputStream* st) const;  void print_gc_on(outputStream* st) const override;  void gc_threads_do(ThreadClosure* tc) const override;  \/\/ Override  void print_tracing_info() const override;  \/\/ Used to print information about locations in the hs_err file.  bool print_location(outputStream* st, void* addr) const override;};\/\/ Scoped object that performs common pre- and post-gc heap printing operations.class G1HeapPrinterMark : public StackObj {  G1CollectedHeap* _g1h;  G1HeapTransition _heap_transition;public:  G1HeapPrinterMark(G1CollectedHeap* g1h);  ~G1HeapPrinterMark();};\/\/ Scoped object that performs common pre- and post-gc operations related to\/\/ JFR events.class G1JFRTracerMark : public StackObj {protected:  STWGCTimer* _timer;  GCTracer* _tracer;public:  G1JFRTracerMark(STWGCTimer* timer, GCTracer* tracer);  ~G1JFRTracerMark();};#endif \/\/ SHARE_GC_G1_G1COLLECTEDHEAP_HPP\n\\ No newline at end of file\n","filename":"src\/hotspot\/share\/gc\/g1\/g1CollectedHeap.hpp","additions":1,"deletions":1346,"binary":false,"changes":1347,"status":"modified"},{"patch":"@@ -0,0 +1,81 @@\n+\/*\n+ * Copyright (c) 2025, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#include \"gc\/g1\/g1CollectedHeap.hpp\"\n+#include \"gc\/g1\/g1CollectedHeap.inline.hpp\"\n+#include \"gc\/g1\/g1HeapEvaluationTask.hpp\"\n+#include \"gc\/g1\/g1HeapSizingPolicy.hpp\"\n+#include \"utilities\/globalDefinitions.hpp\"\n+#include \"logging\/log.hpp\"\n+#include \"memory\/resourceArea.hpp\"\n+#include \"runtime\/globals.hpp\"\n+#include \"utilities\/debug.hpp\"\n+#include \"utilities\/globalDefinitions.hpp\"\n+\n+G1HeapEvaluationTask::G1HeapEvaluationTask(G1CollectedHeap* g1h, G1HeapSizingPolicy* heap_sizing_policy) :\n+  PeriodicTask(G1TimeBasedEvaluationIntervalMillis),  \/\/ Use PeriodicTask with interval\n+  _g1h(g1h),\n+  _heap_sizing_policy(heap_sizing_policy) {\n+}\n+\n+void G1HeapEvaluationTask::task() {\n+  \/\/ This runs on WatcherThread during idle periods - perfect for time-based evaluation!\n+  log_debug(gc, sizing)(\"Starting heap evaluation\");\n+\n+  if (!G1UseTimeBasedHeapSizing) {\n+    return;\n+  }\n+\n+  \/\/ Ensure we're not running during GC activity\n+  if (_g1h->is_stw_gc_active()) {\n+    log_trace(gc, sizing)(\"GC active, skipping time-based evaluation\");\n+    return;\n+  }\n+\n+  ResourceMark rm; \/\/ Ensure temporary resources are released\n+\n+  bool should_expand = false;\n+  size_t resize_amount = _heap_sizing_policy->evaluate_heap_resize(should_expand);\n+\n+  if (resize_amount > 0) {\n+    \/\/ Time-based evaluation only handles uncommit\/shrinking, never expansion\n+    if (should_expand) {\n+      log_warning(gc, sizing)(\"Time-based evaluation unexpected expansion request ignored (resize_amount=%zuB)\", resize_amount);\n+      \/\/ This should not happen since time-based policy only handles uncommit\n+      assert(false, \"Time-based heap sizing should never request expansion\");\n+    } else {\n+      log_info(gc, sizing)(\"Time-based evaluation: shrinking heap by %zuMB\", resize_amount \/ M);\n+      log_debug(gc, sizing)(\"Time-based evaluation recommends shrinking by %zuB\", resize_amount);\n+      _g1h->request_heap_shrink(resize_amount);\n+    }\n+  } else {\n+    \/\/ Periodic info log for ongoing evaluation activity (less frequent)\n+    static int evaluation_count = 0;\n+    if (++evaluation_count % 10 == 0) { \/\/ Log every 10th evaluation when no action taken\n+      log_info(gc, sizing)(\"Time-based evaluation: no heap uncommit needed (evaluation #%d)\", evaluation_count);\n+    }\n+  }\n+\n+  \/\/ No need to schedule - PeriodicTask automatically reschedules itself!\n+}\n","filename":"src\/hotspot\/share\/gc\/g1\/g1HeapEvaluationTask.cpp","additions":81,"deletions":0,"binary":false,"changes":81,"status":"added"},{"patch":"@@ -0,0 +1,44 @@\n+\/*\n+ * Copyright (c) 2025, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef SHARE_GC_G1_G1HEAPEVALUATIONTASK_HPP\n+#define SHARE_GC_G1_G1HEAPEVALUATIONTASK_HPP\n+\n+#include \"runtime\/task.hpp\"  \/\/ Changed from g1ServiceThread.hpp\n+#include \"gc\/g1\/g1_globals.hpp\"\n+\n+class G1CollectedHeap;\n+class G1HeapSizingPolicy;\n+\n+\/\/ Time-based heap evaluation task that runs during idle periods\n+class G1HeapEvaluationTask : public PeriodicTask {  \/\/ Changed from G1ServiceTask\n+  G1CollectedHeap* _g1h;\n+  G1HeapSizingPolicy* _heap_sizing_policy;\n+\n+public:\n+  G1HeapEvaluationTask(G1CollectedHeap* g1h, G1HeapSizingPolicy* heap_sizing_policy);\n+  virtual void task() override;  \/\/ Changed from execute() to task()\n+};\n+\n+#endif \/\/ SHARE_GC_G1_G1HEAPEVALUATIONTASK_HPP\n","filename":"src\/hotspot\/share\/gc\/g1\/g1HeapEvaluationTask.hpp","additions":44,"deletions":0,"binary":false,"changes":44,"status":"added"},{"patch":"@@ -122,0 +122,1 @@\n+  record_activity(); \/\/ Record region initialization\n@@ -252,0 +253,1 @@\n+  _last_access_timestamp(os::javaTimeMillis()), \/\/ Initialize timestamp with milliseconds to match uncommit check\n","filename":"src\/hotspot\/share\/gc\/g1\/g1HeapRegion.cpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -1,1 +1,1 @@\n-\/*\n+\/*\n@@ -37,0 +37,2 @@\n+#include \"runtime\/os.hpp\"\n+#include \"utilities\/globalDefinitions.hpp\"\n@@ -73,0 +75,1 @@\n+  friend class G1Allocator;  \/\/ For access to record_activity()\n@@ -74,0 +77,1 @@\n+private:\n@@ -252,0 +256,3 @@\n+  \/\/ Time-based heap sizing: tracks last allocation\/access time\n+  jlong _last_access_timestamp;\n+\n@@ -553,0 +560,19 @@\n+  \/\/ Time-based heap sizing methods\n+  void record_activity() {\n+    _last_access_timestamp = os::javaTimeMillis();  \/\/ Use milliseconds to match uncommit check\n+  }\n+\n+  jlong last_access_time() const {\n+    return _last_access_timestamp;\n+  }\n+\n+  \/\/ Returns true if the region has been inactive for longer than the uncommit delay\n+  bool should_uncommit(uint64_t delay) const {\n+    if (!is_empty()) {\n+      return false;\n+    }\n+    jlong current_time = os::javaTimeMillis();\n+    jlong elapsed = current_time - _last_access_timestamp;\n+    return elapsed > (jlong)delay;\n+  }\n+\n","filename":"src\/hotspot\/share\/gc\/g1\/g1HeapRegion.hpp","additions":27,"deletions":1,"binary":false,"changes":28,"status":"modified"},{"patch":"@@ -1,438 +1,1 @@\n-\/*\n- * Copyright (c) 2016, 2025, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\n- *\/\n-\n-#include \"gc\/g1\/g1Analytics.hpp\"\n-#include \"gc\/g1\/g1CollectedHeap.inline.hpp\"\n-#include \"gc\/g1\/g1HeapSizingPolicy.hpp\"\n-#include \"gc\/shared\/gc_globals.hpp\"\n-#include \"logging\/log.hpp\"\n-#include \"runtime\/globals.hpp\"\n-#include \"utilities\/debug.hpp\"\n-#include \"utilities\/globalDefinitions.hpp\"\n-\n-G1HeapSizingPolicy* G1HeapSizingPolicy::create(const G1CollectedHeap* g1h, const G1Analytics* analytics) {\n-  return new G1HeapSizingPolicy(g1h, analytics);\n-}\n-\n-G1HeapSizingPolicy::G1HeapSizingPolicy(const G1CollectedHeap* g1h, const G1Analytics* analytics) :\n-  _g1h(g1h),\n-  _analytics(analytics),\n-  \/\/ Bias for expansion at startup; the +1 is to counter the first sample always\n-  \/\/ being 0.0, i.e. lower than any threshold.\n-  _gc_cpu_usage_deviation_counter((G1CPUUsageExpandThreshold \/ 2) + 1),\n-  _recent_cpu_usage_deltas(long_term_count_limit()),\n-  _long_term_count(0) {\n-}\n-\n-void G1HeapSizingPolicy::reset_cpu_usage_tracking_data() {\n-  _long_term_count = 0;\n-  _gc_cpu_usage_deviation_counter = 0;\n-  \/\/ Keep the recent GC CPU usage data.\n-}\n-\n-void G1HeapSizingPolicy::decay_cpu_usage_tracking_data() {\n-  _long_term_count = 0;\n-  _gc_cpu_usage_deviation_counter \/= 2;\n-  \/\/ Keep the recent GC CPU usage data.\n-}\n-\n-double G1HeapSizingPolicy::scale_with_heap(double gc_cpu_usage_target) {\n-  double target = gc_cpu_usage_target;\n-  \/\/ If the heap is at less than half its maximum size, scale the threshold down,\n-  \/\/ to a limit of 1%. Thus the smaller the heap is, the more likely it is to expand,\n-  \/\/ though the scaling code will likely keep the increase small.\n-  if (_g1h->capacity() <= _g1h->max_capacity() \/ 2) {\n-    target *= (double)_g1h->capacity() \/ (double)(_g1h->max_capacity() \/ 2);\n-    target = MAX2(target, 0.01);\n-  }\n-\n-  return target;\n-}\n-\n-static void log_resize(double short_term_cpu_usage,\n-                       double long_term_cpu_usage,\n-                       double lower_threshold,\n-                       double upper_threshold,\n-                       double cpu_usage_target,\n-                       bool at_limit,\n-                       size_t resize_bytes,\n-                       bool expand) {\n-\n-  log_debug(gc, ergo, heap)(\"Heap resize: \"\n-                            \"short term GC CPU usage %1.2f%% long term GC CPU usage %1.2f%% \"\n-                            \"lower threshold %1.2f%% upper threshold %1.2f%% GC CPU usage target %1.2f%% \"\n-                            \"at limit %s resize by %zuB expand %s\",\n-                            short_term_cpu_usage * 100.0,\n-                            long_term_cpu_usage * 100.0,\n-                            lower_threshold * 100.0,\n-                            upper_threshold * 100.0,\n-                            cpu_usage_target * 100.0,\n-                            BOOL_TO_STR(at_limit),\n-                            resize_bytes,\n-                            BOOL_TO_STR(expand));\n-}\n-\n-\/\/ Logistic function, returns values in the range [0,1]\n-static double sigmoid_function(double value) {\n-  \/\/ Sigmoid Parameters:\n-  double inflection_point = 1.0; \/\/ Inflection point (midpoint of the sigmoid).\n-  double steepness = 6.0;\n-  return 1.0 \/ (1.0 + exp(-steepness * (value - inflection_point)));\n-}\n-\n-\/\/ Computes a smooth scaling factor based on the relative deviation of actual gc_cpu_usage\n-\/\/ from the gc_cpu_usage_target, using a sigmoid function to transition between\n-\/\/ the specified minimum and maximum scaling factors.\n-\/\/\n-\/\/ The input cpu_usage_delta represents the relative deviation of the current gc_cpu_usage to the\n-\/\/ gc_cpu_usage_target. This value is passed through a sigmoid function that produces a smooth\n-\/\/ output between 0 and 1, which is then scaled to the range [min_scale_factor, max_scale_factor].\n-\/\/\n-\/\/ The sigmoid's inflection point is set at cpu_usage_delta = 1.0 (a 100% deviation), where the scaling\n-\/\/ response increases most rapidly.\n-\/\/\n-\/\/ The steepness parameter controls how sharply the scale factor changes near the inflection point.\n-\/\/  * Low steepness (1-3): gradual scaling over a wide range of deviations (more conservative).\n-\/\/  * High steepness (7-10): rapid scaling near the inflection point; small deviations result\n-\/\/                           in very low scaling, but larger deviations ramp up scaling quickly.\n-\/\/                           Steepness at 10 is nearly a step function.\n-\/\/\n-\/\/ In this case, we choose a steepness of 6.0:\n-\/\/ - For small deviations, the sigmoid output is close to 0, resulting in scale factors near the\n-\/\/   lower bound, preventing excessive resizing.\n-\/\/ - As cpu_usage_delta grows toward 1.0, the steepness value makes the transition sharper, enabling\n-\/\/   more aggressive scaling for large deviations.\n-\/\/\n-\/\/ This helps avoid overreacting to small gc_cpu_usage deviations but respond appropriately\n-\/\/ when necessary.\n-double G1HeapSizingPolicy::scale_cpu_usage_delta(double cpu_usage_delta,\n-                                                 double min_scale_factor,\n-                                                 double max_scale_factor) const {\n-  double sigmoid = sigmoid_function(cpu_usage_delta);\n-\n-  double scale_factor = min_scale_factor + (max_scale_factor - min_scale_factor) * sigmoid;\n-  return scale_factor;\n-}\n-\n-\/\/ Calculate the relative difference between a and b.\n-static double rel_diff(double a, double b) {\n-  return (a - b) \/ b;\n-}\n-\n-size_t G1HeapSizingPolicy::young_collection_expand_amount(double cpu_usage_delta) const {\n-  assert(cpu_usage_delta >= 0.0, \"must be\");\n-\n-  size_t reserved_bytes = _g1h->max_capacity();\n-  size_t committed_bytes = _g1h->capacity();\n-  size_t uncommitted_bytes = reserved_bytes - committed_bytes;\n-  size_t expand_bytes_via_pct = uncommitted_bytes * G1ExpandByPercentOfAvailable \/ 100;\n-  size_t min_expand_bytes = MIN2(G1HeapRegion::GrainBytes, uncommitted_bytes);\n-\n-  \/\/ Take the current size or G1ExpandByPercentOfAvailable % of\n-  \/\/ the available expansion space, whichever is smaller, as the base\n-  \/\/ expansion size. Then possibly scale this size according to how much the\n-  \/\/ GC CPU usage (on average) has exceeded the target.\n-  const double min_scale_factor = 0.2;\n-  const double max_scale_factor = 2.0;\n-\n-  double scale_factor = scale_cpu_usage_delta(cpu_usage_delta,\n-                                              min_scale_factor,\n-                                              max_scale_factor);\n-\n-  size_t resize_bytes = MIN2(expand_bytes_via_pct, committed_bytes);\n-\n-  resize_bytes = static_cast<size_t>(resize_bytes * scale_factor);\n-\n-  \/\/ Ensure the expansion size is at least the minimum growth amount\n-  \/\/ and at most the remaining uncommitted byte size.\n-  return clamp(resize_bytes, min_expand_bytes, uncommitted_bytes);\n-}\n-\n-size_t G1HeapSizingPolicy::young_collection_shrink_amount(double cpu_usage_delta, size_t allocation_word_size) const {\n-  assert(cpu_usage_delta >= 0.0, \"must be\");\n-\n-  const double max_scale_factor = G1ShrinkByPercentOfAvailable \/ 100.0;\n-  const double min_scale_factor = max_scale_factor \/ 10.0;\n-\n-  double scale_factor = scale_cpu_usage_delta(cpu_usage_delta,\n-                                              min_scale_factor,\n-                                              max_scale_factor);\n-  assert(scale_factor <= max_scale_factor, \"must be\");\n-\n-  \/\/ We are at the end of GC, so free regions are at maximum. Do not try to shrink\n-  \/\/ to have less than the reserve or the number of regions we are most certainly\n-  \/\/ going to use during this mutator phase.\n-  uint target_regions_to_shrink = _g1h->num_free_regions();\n-\n-  uint needed_for_allocation = _g1h->eden_target_length();\n-  if (_g1h->is_humongous(allocation_word_size)) {\n-    needed_for_allocation += (uint) _g1h->humongous_obj_size_in_regions(allocation_word_size);\n-  }\n-\n-  if (target_regions_to_shrink >= needed_for_allocation) {\n-    target_regions_to_shrink -= needed_for_allocation;\n-  } else {\n-    target_regions_to_shrink = 0;\n-  }\n-\n-  size_t resize_bytes = (double)G1HeapRegion::GrainBytes * target_regions_to_shrink * scale_factor;\n-\n-  log_debug(gc, ergo, heap)(\"Shrink log: scale factor %1.2f%% \"\n-                            \"total free regions %u \"\n-                            \"needed for alloc %u \"\n-                            \"base targeted for shrinking %u \"\n-                            \"resize_bytes %zd ( %zu regions)\",\n-                            scale_factor * 100.0,\n-                            _g1h->num_free_regions(),\n-                            needed_for_allocation,\n-                            target_regions_to_shrink,\n-                            resize_bytes,\n-                            (resize_bytes \/ G1HeapRegion::GrainBytes));\n-\n-  return resize_bytes;\n-}\n-\n-size_t G1HeapSizingPolicy::young_collection_resize_amount(bool& expand, size_t allocation_word_size) {\n-  assert(GCTimeRatio > 0, \"must be\");\n-  expand = false;\n-\n-  const double long_term_gc_cpu_usage = _analytics->long_term_pause_time_ratio();\n-  const double short_term_gc_cpu_usage = _analytics->short_term_pause_time_ratio();\n-\n-  double gc_cpu_usage_target = 1.0 \/ (1.0 + GCTimeRatio);\n-  gc_cpu_usage_target = scale_with_heap(gc_cpu_usage_target);\n-\n-  \/\/ Calculate gc_cpu_usage acceptable deviation thresholds:\n-  \/\/ - upper_threshold, do not want to exceed this.\n-  \/\/ - lower_threshold, we do not want to go below.\n-  const double gc_cpu_usage_margin = G1CPUUsageDeviationPercent \/ 100.0;\n-  const double upper_threshold = gc_cpu_usage_target * (1 + gc_cpu_usage_margin);\n-  const double lower_threshold = gc_cpu_usage_target * (1 - gc_cpu_usage_margin);\n-\n-  \/\/ Decide to expand\/shrink based on how far the current GC CPU usage deviates\n-  \/\/ from the target. This allows the policy to respond more quickly to GC pressure\n-  \/\/ when the heap is small relative to the maximum heap.\n-  const double long_term_delta = rel_diff(long_term_gc_cpu_usage, gc_cpu_usage_target);\n-  const double short_term_delta = rel_diff(short_term_gc_cpu_usage, gc_cpu_usage_target);\n-\n-  \/\/ If the short term GC CPU usage exceeds the upper threshold, increment the deviation\n-  \/\/ counter. If it falls below the lower_threshold, decrement the deviation counter.\n-  if (short_term_gc_cpu_usage > upper_threshold) {\n-    _gc_cpu_usage_deviation_counter++;\n-  } else if (short_term_gc_cpu_usage < lower_threshold) {\n-    _gc_cpu_usage_deviation_counter--;\n-  }\n-  \/\/ Ignore very first sample as it is garbage.\n-  if (_long_term_count != 0 || _recent_cpu_usage_deltas.num() != 0) {\n-    _recent_cpu_usage_deltas.add(short_term_delta);\n-  }\n-  _long_term_count++;\n-\n-  log_trace(gc, ergo, heap)(\"Heap resize triggers: long term count: %u \"\n-                            \"long term count limit: %u \"\n-                            \"short term delta: %1.2f \"\n-                            \"recent recorded short term deltas: %u\"\n-                            \"GC CPU usage deviation counter: %d\",\n-                            _long_term_count,\n-                            long_term_count_limit(),\n-                            short_term_delta,\n-                            _recent_cpu_usage_deltas.num(),\n-                            _gc_cpu_usage_deviation_counter);\n-\n-  \/\/ Check if there is a short- or long-term need for resizing, expansion first.\n-  \/\/\n-  \/\/ Short-term resizing need is detected by exceeding the upper or lower thresholds\n-  \/\/ multiple times, tracked in _gc_cpu_usage_deviation_counter. If it contains a large\n-  \/\/ positive or negative (larger than the respective thresholds), we trigger\n-  \/\/ resizing calculation.\n-  \/\/\n-  \/\/ Slowly occurring long-term changes to the actual GC CPU usage are checked\n-  \/\/ only every once in a while.\n-  \/\/\n-  \/\/ The _gc_cpu_usage_deviation_counter value is reset after each resize, or slowly\n-  \/\/ decayed if no resizing happens.\n-\n-  size_t resize_bytes = 0;\n-\n-  const bool use_long_term_delta = (_long_term_count == long_term_count_limit());\n-  const double avg_short_term_delta = _recent_cpu_usage_deltas.avg();\n-\n-  double delta;\n-  if (use_long_term_delta) {\n-    \/\/ For expansion, deltas are positive, and we want to expand aggressively.\n-    \/\/ For shrinking, deltas are negative, so the MAX2 below selects the least\n-    \/\/ aggressive one as we are using the absolute value for scaling.\n-    delta = MAX2(avg_short_term_delta, long_term_delta);\n-  } else {\n-    delta = avg_short_term_delta;\n-  }\n-  \/\/ Delta is negative when shrinking, but the calculation of the resize amount\n-  \/\/ always expects an absolute value. Do that here unconditionally.\n-  delta = fabsd(delta);\n-\n-  int count_threshold_for_shrink = (int)G1CPUUsageShrinkThreshold;\n-\n-  if ((_gc_cpu_usage_deviation_counter >= (int)G1CPUUsageExpandThreshold) ||\n-      (use_long_term_delta && (long_term_gc_cpu_usage > upper_threshold))) {\n-    expand = true;\n-\n-    \/\/ Short-cut calculation if already at maximum capacity.\n-    if (_g1h->capacity() == _g1h->max_capacity()) {\n-      log_resize(short_term_gc_cpu_usage, long_term_gc_cpu_usage,\n-                 lower_threshold, upper_threshold, gc_cpu_usage_target, true, 0, expand);\n-      reset_cpu_usage_tracking_data();\n-      return resize_bytes;\n-    }\n-\n-    log_trace(gc, ergo, heap)(\"expand deltas long %1.2f short %1.2f use long term %u delta %1.2f\",\n-                              long_term_delta, avg_short_term_delta, use_long_term_delta, delta);\n-\n-    resize_bytes = young_collection_expand_amount(delta);\n-\n-    reset_cpu_usage_tracking_data();\n-  } else if ((_gc_cpu_usage_deviation_counter <= -count_threshold_for_shrink) ||\n-             (use_long_term_delta && (long_term_gc_cpu_usage < lower_threshold))) {\n-    expand = false;\n-    \/\/ Short-cut calculation if already at minimum capacity.\n-    if (_g1h->capacity() == _g1h->min_capacity()) {\n-      log_resize(short_term_gc_cpu_usage, long_term_gc_cpu_usage,\n-                 lower_threshold, upper_threshold, gc_cpu_usage_target, true, 0, expand);\n-      reset_cpu_usage_tracking_data();\n-      return resize_bytes;\n-    }\n-\n-    log_trace(gc, ergo, heap)(\"expand deltas long %1.2f short %1.2f use long term %u delta %1.2f\",\n-                              long_term_delta, avg_short_term_delta, use_long_term_delta, delta);\n-\n-    resize_bytes = young_collection_shrink_amount(delta, allocation_word_size);\n-\n-    reset_cpu_usage_tracking_data();\n-  } else if (use_long_term_delta) {\n-    \/\/ A resize has not been triggered, but the long term counter overflowed.\n-    decay_cpu_usage_tracking_data();\n-    expand = false; \/\/ Does not matter.\n-  }\n-\n-  log_resize(short_term_gc_cpu_usage, long_term_gc_cpu_usage,\n-             lower_threshold, upper_threshold, gc_cpu_usage_target,\n-             false, resize_bytes, expand);\n-\n-  return resize_bytes;\n-}\n-\n-static size_t target_heap_capacity(size_t used_bytes, uintx free_ratio) {\n-  assert(free_ratio <= 100, \"precondition\");\n-  if (free_ratio == 100) {\n-    \/\/ If 100 then below calculations will divide by zero and return min of\n-    \/\/ resulting infinity and MaxHeapSize.  Avoid issues of UB vs is_iec559\n-    \/\/ and ubsan warnings, and just immediately return MaxHeapSize.\n-    return MaxHeapSize;\n-  }\n-\n-  const double desired_free_percentage = (double) free_ratio \/ 100.0;\n-  const double desired_used_percentage = 1.0 - desired_free_percentage;\n-\n-  \/\/ We have to be careful here as these two calculations can overflow\n-  \/\/ 32-bit size_t's.\n-  double used_bytes_d = (double) used_bytes;\n-  double desired_capacity_d = used_bytes_d \/ desired_used_percentage;\n-  \/\/ Let's make sure that they are both under the max heap size, which\n-  \/\/ by default will make it fit into a size_t.\n-  double desired_capacity_upper_bound = (double) MaxHeapSize;\n-  desired_capacity_d = MIN2(desired_capacity_d, desired_capacity_upper_bound);\n-  \/\/ We can now safely turn it into size_t's.\n-  return (size_t) desired_capacity_d;\n-}\n-\n-size_t G1HeapSizingPolicy::full_collection_resize_amount(bool& expand, size_t allocation_word_size) {\n-  \/\/ If the full collection was triggered by an allocation failure, we should account\n-  \/\/ for the bytes required for this allocation under used_after_gc. This prevents\n-  \/\/ unnecessary shrinking that would be followed by an expand call to satisfy the\n-  \/\/ allocation.\n-  size_t allocation_bytes = allocation_word_size * HeapWordSize;\n-  if (_g1h->is_humongous(allocation_word_size)) {\n-    \/\/ Humongous objects are allocated in entire regions, we must calculate\n-    \/\/ required space in terms of full regions, not just the object size.\n-    allocation_bytes = G1HeapRegion::align_up_to_region_byte_size(allocation_bytes);\n-  }\n-\n-  \/\/ Capacity, free and used after the GC counted as full regions to\n-  \/\/ include the waste in the following calculations.\n-  const size_t capacity_after_gc = _g1h->capacity();\n-  const size_t used_after_gc = capacity_after_gc + allocation_bytes -\n-                               _g1h->unused_committed_regions_in_bytes() -\n-                               \/\/ Discount space used by current Eden to establish a\n-                               \/\/ situation during Remark similar to at the end of full\n-                               \/\/ GC where eden is empty. During Remark there can be an\n-                               \/\/ arbitrary number of eden regions which would skew the\n-                               \/\/ results.\n-                               _g1h->eden_regions_count() * G1HeapRegion::GrainBytes;\n-\n-  size_t minimum_desired_capacity = target_heap_capacity(used_after_gc, MinHeapFreeRatio);\n-  size_t maximum_desired_capacity = target_heap_capacity(used_after_gc, MaxHeapFreeRatio);\n-\n-  \/\/ This assert only makes sense here, before we adjust them\n-  \/\/ with respect to the min and max heap size.\n-  assert(minimum_desired_capacity <= maximum_desired_capacity,\n-         \"minimum_desired_capacity = %zu, \"\n-         \"maximum_desired_capacity = %zu\",\n-         minimum_desired_capacity, maximum_desired_capacity);\n-\n-  \/\/ Should not be greater than the heap max size. No need to adjust\n-  \/\/ it with respect to the heap min size as it's a lower bound (i.e.,\n-  \/\/ we'll try to make the capacity larger than it, not smaller).\n-  minimum_desired_capacity = MIN2(minimum_desired_capacity, _g1h->max_capacity());\n-  \/\/ Should not be less than the heap min size. No need to adjust it\n-  \/\/ with respect to the heap max size as it's an upper bound (i.e.,\n-  \/\/ we'll try to make the capacity smaller than it, not greater).\n-  maximum_desired_capacity = MAX2(maximum_desired_capacity, _g1h->min_capacity());\n-\n-  \/\/ Don't expand unless it's significant; prefer expansion to shrinking.\n-  if (capacity_after_gc < minimum_desired_capacity) {\n-    size_t expand_bytes = minimum_desired_capacity - capacity_after_gc;\n-\n-    log_debug(gc, ergo, heap)(\"Heap resize. Attempt heap expansion (capacity lower than min desired capacity). \"\n-                              \"Capacity: %zuB occupancy: %zuB live: %zuB \"\n-                              \"min_desired_capacity: %zuB (%zu %%)\",\n-                              capacity_after_gc, used_after_gc, _g1h->used(), minimum_desired_capacity, MinHeapFreeRatio);\n-\n-    expand = true;\n-    return expand_bytes;\n-    \/\/ No expansion, now see if we want to shrink\n-  } else if (capacity_after_gc > maximum_desired_capacity) {\n-    \/\/ Capacity too large, compute shrinking size\n-    size_t shrink_bytes = capacity_after_gc - maximum_desired_capacity;\n-\n-    log_debug(gc, ergo, heap)(\"Heap resize. Attempt heap shrinking (capacity higher than max desired capacity). \"\n-                              \"Capacity: %zuB occupancy: %zuB live: %zuB \"\n-                              \"maximum_desired_capacity: %zuB (%zu %%)\",\n-                              capacity_after_gc, used_after_gc, _g1h->used(), maximum_desired_capacity, MaxHeapFreeRatio);\n-\n-    expand = false;\n-    return shrink_bytes;\n-  }\n-\n-  expand = true; \/\/ Does not matter.\n-  return 0;\n-}\n+\/* * Copyright (c) 2016, 2025, Oracle and\/or its affiliates. All rights reserved. * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER. * * This code is free software; you can redistribute it and\/or modify it * under the terms of the GNU General Public License version 2 only, as * published by the Free Software Foundation. * * This code is distributed in the hope that it will be useful, but WITHOUT * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License * version 2 for more details (a copy is included in the LICENSE file that * accompanied this code). * * You should have received a copy of the GNU General Public License version * 2 along with this work; if not, write to the Free Software Foundation, * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA. * * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA * or visit www.oracle.com if you need additional information or have any * questions. * *\/#include \"gc\/g1\/g1Analytics.hpp\"#include \"gc\/g1\/g1_globals.hpp\"  \/\/ For flag declarations#include \"gc\/g1\/g1CollectedHeap.hpp\"#include \"gc\/g1\/g1CollectedHeap.inline.hpp\"#include \"gc\/g1\/g1HeapSizingPolicy.hpp\"#include \"gc\/g1\/g1HeapRegionManager.inline.hpp\"#include \"gc\/shared\/gc_globals.hpp\"#include \"logging\/log.hpp\"#include \"memory\/resourceArea.hpp\"#include \"runtime\/globals.hpp\"#include \"runtime\/mutexLocker.hpp\"#include \"runtime\/os.hpp\"#include \"utilities\/debug.hpp\"#include \"utilities\/globalDefinitions.hpp\"\/\/ Initialize static memberjlong G1HeapSizingPolicy::_uncommit_delay_ms = 0;void G1HeapSizingPolicy::initialize() {  \/\/ Flag values are available at this point  _uncommit_delay_ms = (jlong)G1UncommitDelayMillis;}G1HeapSizingPolicy* G1HeapSizingPolicy::create(const G1CollectedHeap* g1h, const G1Analytics* analytics) {  return new G1HeapSizingPolicy(g1h, analytics);}G1HeapSizingPolicy::G1HeapSizingPolicy(const G1CollectedHeap* g1h, const G1Analytics* analytics) :  _g1h(g1h),  _analytics(analytics),  \/\/ Bias for expansion at startup; the +1 is to counter the first sample always  \/\/ being 0.0, i.e. lower than any threshold.  _gc_cpu_usage_deviation_counter((G1CPUUsageExpandThreshold \/ 2) + 1),  _recent_cpu_usage_deltas(long_term_count_limit()),  _long_term_count(0) {}void G1HeapSizingPolicy::reset_cpu_usage_tracking_data() {  _long_term_count = 0;  _gc_cpu_usage_deviation_counter = 0;  \/\/ Keep the recent GC CPU usage data.}void G1HeapSizingPolicy::decay_cpu_usage_tracking_data() {  _long_term_count = 0;  _gc_cpu_usage_deviation_counter \/= 2;  \/\/ Keep the recent GC CPU usage data.}double G1HeapSizingPolicy::scale_with_heap(double gc_cpu_usage_target) {  double target = gc_cpu_usage_target;  \/\/ If the heap is at less than half its maximum size, scale the threshold down,  \/\/ to a limit of 1%. Thus the smaller the heap is, the more likely it is to expand,  \/\/ though the scaling code will likely keep the increase small.  if (_g1h->capacity() <= _g1h->max_capacity() \/ 2) {    target *= (double)_g1h->capacity() \/ (double)(_g1h->max_capacity() \/ 2);    target = MAX2(target, 0.01);  }  return target;}static void log_resize(double short_term_cpu_usage,                       double long_term_cpu_usage,                       double lower_threshold,                       double upper_threshold,                       double cpu_usage_target,                       bool at_limit,                       size_t resize_bytes,                       bool expand) {  log_debug(gc, ergo, heap)(\"Heap resize: \"                            \"short term GC CPU usage %1.2f%% long term GC CPU usage %1.2f%% \"                            \"lower threshold %1.2f%% upper threshold %1.2f%% GC CPU usage target %1.2f%% \"                            \"at limit %s resize by %zuB expand %s\",                            short_term_cpu_usage * 100.0,                            long_term_cpu_usage * 100.0,                            lower_threshold * 100.0,                            upper_threshold * 100.0,                            cpu_usage_target * 100.0,                            BOOL_TO_STR(at_limit),                            resize_bytes,                            BOOL_TO_STR(expand));}\/\/ Logistic function, returns values in the range [0,1]static double sigmoid_function(double value) {  \/\/ Sigmoid Parameters:  double inflection_point = 1.0; \/\/ Inflection point (midpoint of the sigmoid).  double steepness = 6.0;  return 1.0 \/ (1.0 + exp(-steepness * (value - inflection_point)));}\/\/ Computes a smooth scaling factor based on the relative deviation of actual gc_cpu_usage\/\/ from the gc_cpu_usage_target, using a sigmoid function to transition between\/\/ the specified minimum and maximum scaling factors.\/\/\/\/ The input cpu_usage_delta represents the relative deviation of the current gc_cpu_usage to the\/\/ gc_cpu_usage_target. This value is passed through a sigmoid function that produces a smooth\/\/ output between 0 and 1, which is then scaled to the range [min_scale_factor, max_scale_factor].\/\/\/\/ The sigmoid's inflection point is set at cpu_usage_delta = 1.0 (a 100% deviation), where the scaling\/\/ response increases most rapidly.\/\/\/\/ The steepness parameter controls how sharply the scale factor changes near the inflection point.\/\/  * Low steepness (1-3): gradual scaling over a wide range of deviations (more conservative).\/\/  * High steepness (7-10): rapid scaling near the inflection point; small deviations result\/\/                           in very low scaling, but larger deviations ramp up scaling quickly.\/\/                           Steepness at 10 is nearly a step function.\/\/\/\/ In this case, we choose a steepness of 6.0:\/\/ - For small deviations, the sigmoid output is close to 0, resulting in scale factors near the\/\/   lower bound, preventing excessive resizing.\/\/ - As cpu_usage_delta grows toward 1.0, the steepness value makes the transition sharper, enabling\/\/   more aggressive scaling for large deviations.\/\/\/\/ This helps avoid overreacting to small gc_cpu_usage deviations but respond appropriately\/\/ when necessary.double G1HeapSizingPolicy::scale_cpu_usage_delta(double cpu_usage_delta,                                                 double min_scale_factor,                                                 double max_scale_factor) const {  double sigmoid = sigmoid_function(cpu_usage_delta);  double scale_factor = min_scale_factor + (max_scale_factor - min_scale_factor) * sigmoid;  return scale_factor;}\/\/ Calculate the relative difference between a and b.static double rel_diff(double a, double b) {  return (a - b) \/ b;}size_t G1HeapSizingPolicy::young_collection_expand_amount(double cpu_usage_delta) const {  assert(cpu_usage_delta >= 0.0, \"must be\");  size_t reserved_bytes = _g1h->max_capacity();  size_t committed_bytes = _g1h->capacity();  size_t uncommitted_bytes = reserved_bytes - committed_bytes;  size_t expand_bytes_via_pct = uncommitted_bytes * G1ExpandByPercentOfAvailable \/ 100;  size_t min_expand_bytes = MIN2(G1HeapRegion::GrainBytes, uncommitted_bytes);  \/\/ Take the current size or G1ExpandByPercentOfAvailable % of  \/\/ the available expansion space, whichever is smaller, as the base  \/\/ expansion size. Then possibly scale this size according to how much the  \/\/ GC CPU usage (on average) has exceeded the target.  const double min_scale_factor = 0.2;  const double max_scale_factor = 2.0;  double scale_factor = scale_cpu_usage_delta(cpu_usage_delta,                                              min_scale_factor,                                              max_scale_factor);  size_t resize_bytes = MIN2(expand_bytes_via_pct, committed_bytes);  resize_bytes = static_cast<size_t>(resize_bytes * scale_factor);  \/\/ Ensure the expansion size is at least the minimum growth amount  \/\/ and at most the remaining uncommitted byte size.  return clamp(resize_bytes, min_expand_bytes, uncommitted_bytes);}size_t G1HeapSizingPolicy::young_collection_shrink_amount(double cpu_usage_delta, size_t allocation_word_size) const {  assert(cpu_usage_delta >= 0.0, \"must be\");  const double max_scale_factor = G1ShrinkByPercentOfAvailable \/ 100.0;  const double min_scale_factor = max_scale_factor \/ 10.0;  double scale_factor = scale_cpu_usage_delta(cpu_usage_delta,                                              min_scale_factor,                                              max_scale_factor);  assert(scale_factor <= max_scale_factor, \"must be\");  \/\/ We are at the end of GC, so free regions are at maximum. Do not try to shrink  \/\/ to have less than the reserve or the number of regions we are most certainly  \/\/ going to use during this mutator phase.  uint target_regions_to_shrink = _g1h->num_free_regions();  uint needed_for_allocation = _g1h->eden_target_length();  if (_g1h->is_humongous(allocation_word_size)) {    needed_for_allocation += (uint) _g1h->humongous_obj_size_in_regions(allocation_word_size);  }  if (target_regions_to_shrink >= needed_for_allocation) {    target_regions_to_shrink -= needed_for_allocation;  } else {    target_regions_to_shrink = 0;  }  size_t resize_bytes = (double)G1HeapRegion::GrainBytes * target_regions_to_shrink * scale_factor;  log_debug(gc, ergo, heap)(\"Shrink log: scale factor %1.2f%% \"                            \"total free regions %u \"                            \"needed for alloc %u \"                            \"base targeted for shrinking %u \"                            \"resize_bytes %zd ( %zu regions)\",                            scale_factor * 100.0,                            _g1h->num_free_regions(),                            needed_for_allocation,                            target_regions_to_shrink,                            resize_bytes,                            (resize_bytes \/ G1HeapRegion::GrainBytes));  return resize_bytes;}size_t G1HeapSizingPolicy::young_collection_resize_amount(bool& expand, size_t allocation_word_size) {  assert(GCTimeRatio > 0, \"must be\");  expand = false;  const double long_term_gc_cpu_usage = _analytics->long_term_pause_time_ratio();  const double short_term_gc_cpu_usage = _analytics->short_term_pause_time_ratio();  double gc_cpu_usage_target = 1.0 \/ (1.0 + GCTimeRatio);  gc_cpu_usage_target = scale_with_heap(gc_cpu_usage_target);  \/\/ Calculate gc_cpu_usage acceptable deviation thresholds:  \/\/ - upper_threshold, do not want to exceed this.  \/\/ - lower_threshold, we do not want to go below.  const double gc_cpu_usage_margin = G1CPUUsageDeviationPercent \/ 100.0;  const double upper_threshold = gc_cpu_usage_target * (1 + gc_cpu_usage_margin);  const double lower_threshold = gc_cpu_usage_target * (1 - gc_cpu_usage_margin);  \/\/ Decide to expand\/shrink based on how far the current GC CPU usage deviates  \/\/ from the target. This allows the policy to respond more quickly to GC pressure  \/\/ when the heap is small relative to the maximum heap.  const double long_term_delta = rel_diff(long_term_gc_cpu_usage, gc_cpu_usage_target);  const double short_term_delta = rel_diff(short_term_gc_cpu_usage, gc_cpu_usage_target);  \/\/ If the short term GC CPU usage exceeds the upper threshold, increment the deviation  \/\/ counter. If it falls below the lower_threshold, decrement the deviation counter.  if (short_term_gc_cpu_usage > upper_threshold) {    _gc_cpu_usage_deviation_counter++;  } else if (short_term_gc_cpu_usage < lower_threshold) {    _gc_cpu_usage_deviation_counter--;  }  \/\/ Ignore very first sample as it is garbage.  if (_long_term_count != 0 || _recent_cpu_usage_deltas.num() != 0) {    _recent_cpu_usage_deltas.add(short_term_delta);  }  _long_term_count++;  log_trace(gc, ergo, heap)(\"Heap resize triggers: long term count: %u \"                            \"long term count limit: %u \"                            \"short term delta: %1.2f \"                            \"recent recorded short term deltas: %u\"                            \"GC CPU usage deviation counter: %d\",                            _long_term_count,                            long_term_count_limit(),                            short_term_delta,                            _recent_cpu_usage_deltas.num(),                            _gc_cpu_usage_deviation_counter);  \/\/ Check if there is a short- or long-term need for resizing, expansion first.  \/\/  \/\/ Short-term resizing need is detected by exceeding the upper or lower thresholds  \/\/ multiple times, tracked in _gc_cpu_usage_deviation_counter. If it contains a large  \/\/ positive or negative (larger than the respective thresholds), we trigger  \/\/ resizing calculation.  \/\/  \/\/ Slowly occurring long-term changes to the actual GC CPU usage are checked  \/\/ only every once in a while.  \/\/  \/\/ The _gc_cpu_usage_deviation_counter value is reset after each resize, or slowly  \/\/ decayed if no resizing happens.  size_t resize_bytes = 0;  const bool use_long_term_delta = (_long_term_count == long_term_count_limit());  const double avg_short_term_delta = _recent_cpu_usage_deltas.avg();  double delta;  if (use_long_term_delta) {    \/\/ For expansion, deltas are positive, and we want to expand aggressively.    \/\/ For shrinking, deltas are negative, so the MAX2 below selects the least    \/\/ aggressive one as we are using the absolute value for scaling.    delta = MAX2(avg_short_term_delta, long_term_delta);  } else {    delta = avg_short_term_delta;  }  \/\/ Delta is negative when shrinking, but the calculation of the resize amount  \/\/ always expects an absolute value. Do that here unconditionally.  delta = fabsd(delta);  int count_threshold_for_shrink = (int)G1CPUUsageShrinkThreshold;  if ((_gc_cpu_usage_deviation_counter >= (int)G1CPUUsageExpandThreshold) ||      (use_long_term_delta && (long_term_gc_cpu_usage > upper_threshold))) {    expand = true;    \/\/ Short-cut calculation if already at maximum capacity.    if (_g1h->capacity() == _g1h->max_capacity()) {      log_resize(short_term_gc_cpu_usage, long_term_gc_cpu_usage,                 lower_threshold, upper_threshold, gc_cpu_usage_target, true, 0, expand);      reset_cpu_usage_tracking_data();      return resize_bytes;    }    log_trace(gc, ergo, heap)(\"expand deltas long %1.2f short %1.2f use long term %u delta %1.2f\",                              long_term_delta, avg_short_term_delta, use_long_term_delta, delta);    resize_bytes = young_collection_expand_amount(delta);    reset_cpu_usage_tracking_data();  } else if ((_gc_cpu_usage_deviation_counter <= -count_threshold_for_shrink) ||             (use_long_term_delta && (long_term_gc_cpu_usage < lower_threshold))) {    expand = false;    \/\/ Short-cut calculation if already at minimum capacity.    if (_g1h->capacity() == _g1h->min_capacity()) {      log_resize(short_term_gc_cpu_usage, long_term_gc_cpu_usage,                 lower_threshold, upper_threshold, gc_cpu_usage_target, true, 0, expand);      reset_cpu_usage_tracking_data();      return resize_bytes;    }    log_trace(gc, ergo, heap)(\"expand deltas long %1.2f short %1.2f use long term %u delta %1.2f\",                              long_term_delta, avg_short_term_delta, use_long_term_delta, delta);    resize_bytes = young_collection_shrink_amount(delta, allocation_word_size);    reset_cpu_usage_tracking_data();  } else if (use_long_term_delta) {    \/\/ A resize has not been triggered, but the long term counter overflowed.    decay_cpu_usage_tracking_data();    expand = false; \/\/ Does not matter.  }  log_resize(short_term_gc_cpu_usage, long_term_gc_cpu_usage,             lower_threshold, upper_threshold, gc_cpu_usage_target,             false, resize_bytes, expand);  return resize_bytes;}static size_t target_heap_capacity(size_t used_bytes, uintx free_ratio) {  assert(free_ratio <= 100, \"precondition\");  if (free_ratio == 100) {    \/\/ If 100 then below calculations will divide by zero and return min of    \/\/ resulting infinity and MaxHeapSize.  Avoid issues of UB vs is_iec559    \/\/ and ubsan warnings, and just immediately return MaxHeapSize.    return MaxHeapSize;  }  const double desired_free_percentage = (double) free_ratio \/ 100.0;  const double desired_used_percentage = 1.0 - desired_free_percentage;  \/\/ We have to be careful here as these two calculations can overflow  \/\/ 32-bit size_t's.  double used_bytes_d = (double) used_bytes;  double desired_capacity_d = used_bytes_d \/ desired_used_percentage;  \/\/ Let's make sure that they are both under the max heap size, which  \/\/ by default will make it fit into a size_t.  double desired_capacity_upper_bound = (double) MaxHeapSize;  desired_capacity_d = MIN2(desired_capacity_d, desired_capacity_upper_bound);  \/\/ We can now safely turn it into size_t's.  return (size_t) desired_capacity_d;}size_t G1HeapSizingPolicy::full_collection_resize_amount(bool& expand, size_t allocation_word_size) {  \/\/ If the full collection was triggered by an allocation failure, we should account  \/\/ for the bytes required for this allocation under used_after_gc. This prevents  \/\/ unnecessary shrinking that would be followed by an expand call to satisfy the  \/\/ allocation.  size_t allocation_bytes = allocation_word_size * HeapWordSize;  if (_g1h->is_humongous(allocation_word_size)) {    \/\/ Humongous objects are allocated in entire regions, we must calculate    \/\/ required space in terms of full regions, not just the object size.    allocation_bytes = G1HeapRegion::align_up_to_region_byte_size(allocation_bytes);  }  \/\/ Capacity, free and used after the GC counted as full regions to  \/\/ include the waste in the following calculations.  const size_t capacity_after_gc = _g1h->capacity();  const size_t used_after_gc = capacity_after_gc + allocation_bytes -                               _g1h->unused_committed_regions_in_bytes() -                               \/\/ Discount space used by current Eden to establish a                               \/\/ situation during Remark similar to at the end of full                               \/\/ GC where eden is empty. During Remark there can be an                               \/\/ arbitrary number of eden regions which would skew the                               \/\/ results.                               _g1h->eden_regions_count() * G1HeapRegion::GrainBytes;  size_t minimum_desired_capacity = target_heap_capacity(used_after_gc, MinHeapFreeRatio);  size_t maximum_desired_capacity = target_heap_capacity(used_after_gc, MaxHeapFreeRatio);  \/\/ This assert only makes sense here, before we adjust them  \/\/ with respect to the min and max heap size.  assert(minimum_desired_capacity <= maximum_desired_capacity,         \"minimum_desired_capacity = %zu, \"         \"maximum_desired_capacity = %zu\",         minimum_desired_capacity, maximum_desired_capacity);  \/\/ Should not be greater than the heap max size. No need to adjust  \/\/ it with respect to the heap min size as it's a lower bound (i.e.,  \/\/ we'll try to make the capacity larger than it, not smaller).  minimum_desired_capacity = MIN2(minimum_desired_capacity, _g1h->max_capacity());  \/\/ Should not be less than the heap min size. No need to adjust it  \/\/ with respect to the heap max size as it's an upper bound (i.e.,  \/\/ we'll try to make the capacity smaller than it, not greater).  maximum_desired_capacity = MAX2(maximum_desired_capacity, _g1h->min_capacity());  \/\/ Don't expand unless it's significant; prefer expansion to shrinking.  if (capacity_after_gc < minimum_desired_capacity) {    size_t expand_bytes = minimum_desired_capacity - capacity_after_gc;    log_debug(gc, ergo, heap)(\"Heap resize. Attempt heap expansion (capacity lower than min desired capacity). \"                              \"Capacity: %zuB occupancy: %zuB live: %zuB \"                              \"min_desired_capacity: %zuB (%zu %%)\",                              capacity_after_gc, used_after_gc, _g1h->used(), minimum_desired_capacity, MinHeapFreeRatio);    expand = true;    return expand_bytes;    \/\/ No expansion, now see if we want to shrink  } else if (capacity_after_gc > maximum_desired_capacity) {    \/\/ Capacity too large, compute shrinking size    size_t shrink_bytes = capacity_after_gc - maximum_desired_capacity;    log_debug(gc, ergo, heap)(\"Heap resize. Attempt heap shrinking (capacity higher than max desired capacity). \"                              \"Capacity: %zuB occupancy: %zuB live: %zuB \"                              \"maximum_desired_capacity: %zuB (%zu %%)\",                              capacity_after_gc, used_after_gc, _g1h->used(), maximum_desired_capacity, MaxHeapFreeRatio);    expand = false;    return shrink_bytes;  }  expand = true; \/\/ Does not matter.  return 0;}void G1HeapSizingPolicy::get_uncommit_candidates(GrowableArray<G1HeapRegion*>* candidates) {  uint inactive_regions = 0;  \/\/ Check each heap region for inactivity  class UncommitCandidatesClosure : public G1HeapRegionClosure {    GrowableArray<G1HeapRegion*>* _candidates;    uint* _inactive_regions;    const G1HeapSizingPolicy* _policy;  public:    UncommitCandidatesClosure(GrowableArray<G1HeapRegion*>* candidates,                             uint* inactive_regions,                             const G1HeapSizingPolicy* policy) :      _candidates(candidates),      _inactive_regions(inactive_regions),      _policy(policy) {}    virtual bool do_heap_region(G1HeapRegion* r) {      if (r->is_empty() && _policy->should_uncommit_region(r)) {        _candidates->append(r);        (*_inactive_regions)++;      }      return false;    }  } cl(candidates, &inactive_regions, this);  _g1h->heap_region_iterate(&cl);  if (inactive_regions > 0) {    log_debug(gc, sizing)(\"Uncommit candidates found: %u inactive regions out of %u total regions\",                  inactive_regions, _g1h->max_num_regions());    log_debug(gc, sizing)(\"Region state transition: %u regions found eligible for uncommit after scan\",                  inactive_regions);  }  log_debug(gc, sizing)(\"Full region scan: found %u inactive regions out of %u total regions\",                       inactive_regions,                       _g1h->max_num_regions());}bool G1HeapSizingPolicy::should_uncommit_region(G1HeapRegion* hr) const {  \/\/ Note: Caller already guarantees hr->is_empty() is true  \/\/ Empty regions should always be free and not in collection set in normal operation  jlong current_time = os::javaTimeMillis();  jlong last_access = hr->last_access_time();  jlong elapsed = current_time - last_access;  log_trace(gc, sizing)(\"Region %u uncommit check: elapsed=\" JLONG_FORMAT \"ms threshold=\" JLONG_FORMAT \"ms last_access=\" JLONG_FORMAT \" now=\" JLONG_FORMAT \" empty=%s\",                     hr->hrm_index(), elapsed, (jlong)_uncommit_delay_ms, last_access, current_time,                     hr->is_empty() ? \"true\" : \"false\");  bool should_uncommit = elapsed > (jlong)_uncommit_delay_ms;  if (should_uncommit) {    log_debug(gc, sizing)(\"Region state transition: Region %u transitioning from active to inactive after \" JLONG_FORMAT \"ms idle\",                  hr->hrm_index(), elapsed);  }  return should_uncommit;}size_t G1HeapSizingPolicy::evaluate_heap_resize(bool& expand) {  expand = false; \/\/ Time-based sizing only handles uncommit, never expansion  if (!G1UseTimeBasedHeapSizing) {    return 0;  }  \/\/ Don't resize during GC  if (_g1h->is_stw_gc_active()) {    return 0;  }  \/\/ Must hold Heap_lock during heap resizing  MutexLocker ml(Heap_lock);  ResourceMark rm; \/\/ Ensure GrowableArray resources are properly released  \/\/ Find regions eligible for uncommit  GrowableArray<G1HeapRegion*> candidates;  get_uncommit_candidates(&candidates);  uint inactive_count = candidates.length();  uint total_regions = _g1h->max_num_regions();  \/\/ Need minimum number of inactive regions to proceed  if (inactive_count >= G1MinRegionsToUncommit) {    size_t region_size = G1HeapRegion::GrainBytes;    size_t current_heap = _g1h->capacity();    size_t min_heap = MAX2((size_t)InitialHeapSize, MinHeapSize);  \/\/ Never go below initial size    \/\/ Calculate maximum bytes we can uncommit while respecting min heap size    size_t max_shrink_bytes = current_heap > min_heap ? current_heap - min_heap : 0;    log_trace(gc, sizing)(\"Time-based evaluation details: current_heap=%zuB min_heap=%zuB \"                         \"region_size=%zuB max_shrink=%zuB initial_size=%zuB\",                         current_heap, min_heap, region_size, max_shrink_bytes, InitialHeapSize);    if (max_shrink_bytes > 0 && region_size > 0) {      size_t max_inactive_regions = max_shrink_bytes \/ region_size;      \/\/ Calculate maximum uncommit target as the smaller of:      \/\/ 1. No more than 25% of inactive regions      \/\/ 2. No more than 10% of total committed regions      \/\/ 3. No more than max_shrink_bytes worth of regions      size_t committed_regions = current_heap \/ region_size;      \/\/ Upper limits:      size_t by_inactive = static_cast<size_t>(inactive_count) \/ 4;    \/\/ 25%      size_t by_total = static_cast<size_t>(committed_regions) \/ 10;   \/\/ 10%      size_t regions_to_uncommit =          MIN2(by_total, MIN2(by_inactive, max_inactive_regions));      size_t shrink_bytes = regions_to_uncommit * region_size;      shrink_bytes = MIN2(shrink_bytes, current_heap - MinHeapSize);      if (current_heap - shrink_bytes < InitialHeapSize) {        log_info(gc, sizing)(\"Time-based uncommit skipped: would reduce heap below initial size (%zuMB < %zuMB)\",                            (current_heap - shrink_bytes) \/ M, InitialHeapSize \/ M);        log_debug(gc, sizing)(\"Skipping uncommit - would reduce heap below initial size: \"                             \"current=%zuB shrink=%zuB result=%zuB initial=%zuB min=%zuB\",                             current_heap, shrink_bytes, current_heap - shrink_bytes,                             InitialHeapSize, MinHeapSize);        return 0;      }      if (shrink_bytes > 0) {        log_info(gc, sizing)(\"Time-based uncommit: found %u inactive regions, uncommitting %zu regions (%zuMB)\",                            inactive_count, regions_to_uncommit, shrink_bytes \/ M);        log_debug(gc, sizing)(\"Time-based heap uncommit evaluation: Found %u inactive regions out of %u total regions, \"                             \"target shrink: %zuB (max allowed: %zuB)\",                             inactive_count, total_regions, shrink_bytes, max_shrink_bytes);        log_debug(gc, sizing)(\"Region state transition: %zu regions selected for uncommit\",                     regions_to_uncommit);      }      return shrink_bytes;    }  }  log_trace(gc, sizing)(\"Time-based heap evaluation: no uncommit needed \"                       \"(inactive=%u min_required=%zu heap=%zuB min=%zuB)\",                       inactive_count, G1MinRegionsToUncommit,                       _g1h->capacity(), MAX2((size_t)InitialHeapSize, MinHeapSize));  return 0;} * * You should have received a copy of the GNU General Public License version * 2 along with this work; if not, write to the Free Software Foundation, * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA. * * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA * or visit www.oracle.com if you need additional information or have any * questions. * *\/#include \"gc\/g1\/g1Analytics.hpp\"#include \"gc\/g1\/g1_globals.hpp\"  \/\/ For flag declarations#include \"gc\/g1\/g1CollectedHeap.hpp\"#include \"gc\/g1\/g1CollectedHeap.inline.hpp\"#include \"gc\/g1\/g1HeapSizingPolicy.hpp\"#include \"gc\/g1\/g1HeapRegionManager.inline.hpp\"#include \"gc\/shared\/gc_globals.hpp\"#include \"logging\/log.hpp\"#include \"memory\/resourceArea.hpp\"#include \"runtime\/globals.hpp\"#include \"runtime\/mutexLocker.hpp\"#include \"runtime\/os.hpp\"#include \"utilities\/debug.hpp\"#include \"utilities\/globalDefinitions.hpp\"\/\/ Initialize static memberjlong G1HeapSizingPolicy::_uncommit_delay_ms = 0;void G1HeapSizingPolicy::initialize() {  \/\/ Flag values are available at this point  _uncommit_delay_ms = (jlong)G1UncommitDelayMillis;}G1HeapSizingPolicy* G1HeapSizingPolicy::create(const G1CollectedHeap* g1h, const G1Analytics* analytics) {  return new G1HeapSizingPolicy(g1h, analytics);}G1HeapSizingPolicy::G1HeapSizingPolicy(const G1CollectedHeap* g1h, const G1Analytics* analytics) :  _g1h(g1h),  _analytics(analytics),  _num_prev_pauses_for_heuristics(analytics->number_of_recorded_pause_times()) {  assert(MinOverThresholdForGrowth < _num_prev_pauses_for_heuristics, \"Threshold must be less than %u\", _num_prev_pauses_for_heuristics);  clear_ratio_check_data();}void G1HeapSizingPolicy::clear_ratio_check_data() {  _ratio_over_threshold_count = 0;  _ratio_over_threshold_sum = 0.0;  _pauses_since_start = 0;}double G1HeapSizingPolicy::scale_with_heap(double pause_time_threshold) {  double threshold = pause_time_threshold;  \/\/ If the heap is at less than half its maximum size, scale the threshold down,  \/\/ to a limit of 1%. Thus the smaller the heap is, the more likely it is to expand,  \/\/ though the scaling code will likely keep the increase small.  if (_g1h->capacity() <= _g1h->max_capacity() \/ 2) {    threshold *= (double)_g1h->capacity() \/ (double)(_g1h->max_capacity() \/ 2);    threshold = MAX2(threshold, 0.01);  }  return threshold;}static void log_expansion(double short_term_pause_time_ratio,                          double long_term_pause_time_ratio,                          double threshold,                          double pause_time_ratio,                          bool fully_expanded,                          size_t resize_bytes) {  log_debug(gc, ergo, heap)(\"Heap expansion: \"                            \"short term pause time ratio %1.2f%% long term pause time ratio %1.2f%% \"                            \"threshold %1.2f%% pause time ratio %1.2f%% fully expanded %s \"                            \"resize by %zuB\",                            short_term_pause_time_ratio * 100.0,                            long_term_pause_time_ratio * 100.0,                            threshold * 100.0,                            pause_time_ratio * 100.0,                            BOOL_TO_STR(fully_expanded),                            resize_bytes);}size_t G1HeapSizingPolicy::young_collection_expansion_amount() {  assert(GCTimeRatio > 0, \"must be\");  double long_term_pause_time_ratio = _analytics->long_term_pause_time_ratio();  double short_term_pause_time_ratio = _analytics->short_term_pause_time_ratio();  const double pause_time_threshold = 1.0 \/ (1.0 + GCTimeRatio);  double threshold = scale_with_heap(pause_time_threshold);  size_t expand_bytes = 0;  if (_g1h->capacity() == _g1h->max_capacity()) {    log_expansion(short_term_pause_time_ratio, long_term_pause_time_ratio,                  threshold, pause_time_threshold, true, 0);    clear_ratio_check_data();    return expand_bytes;  }  \/\/ If the last GC time ratio is over the threshold, increment the count of  \/\/ times it has been exceeded, and add this ratio to the sum of exceeded  \/\/ ratios.  if (short_term_pause_time_ratio > threshold) {    _ratio_over_threshold_count++;    _ratio_over_threshold_sum += short_term_pause_time_ratio;  }  log_trace(gc, ergo, heap)(\"Heap expansion triggers: pauses since start: %u \"                            \"num prev pauses for heuristics: %u \"                            \"ratio over threshold count: %u\",                            _pauses_since_start,                            _num_prev_pauses_for_heuristics,                            _ratio_over_threshold_count);  \/\/ Check if we've had enough GC time ratio checks that were over the  \/\/ threshold to trigger an expansion. We'll also expand if we've  \/\/ reached the end of the history buffer and the average of all entries  \/\/ is still over the threshold. This indicates a smaller number of GCs were  \/\/ long enough to make the average exceed the threshold.  bool filled_history_buffer = _pauses_since_start == _num_prev_pauses_for_heuristics;  if ((_ratio_over_threshold_count == MinOverThresholdForGrowth) ||      (filled_history_buffer && (long_term_pause_time_ratio > threshold))) {    size_t min_expand_bytes = G1HeapRegion::GrainBytes;    size_t reserved_bytes = _g1h->max_capacity();    size_t committed_bytes = _g1h->capacity();    size_t uncommitted_bytes = reserved_bytes - committed_bytes;    size_t expand_bytes_via_pct =      uncommitted_bytes * G1ExpandByPercentOfAvailable \/ 100;    double scale_factor = 1.0;    \/\/ If the current size is less than 1\/4 of the Initial heap size, expand    \/\/ by half of the delta between the current and Initial sizes. IE, grow    \/\/ back quickly.    \/\/    \/\/ Otherwise, take the current size, or G1ExpandByPercentOfAvailable % of    \/\/ the available expansion space, whichever is smaller, as the base    \/\/ expansion size. Then possibly scale this size according to how much the    \/\/ threshold has (on average) been exceeded by. If the delta is small    \/\/ (less than the StartScaleDownAt value), scale the size down linearly, but    \/\/ not by less than MinScaleDownFactor. If the delta is large (greater than    \/\/ the StartScaleUpAt value), scale up, but adding no more than MaxScaleUpFactor    \/\/ times the base size. The scaling will be linear in the range from    \/\/ StartScaleUpAt to (StartScaleUpAt + ScaleUpRange). In other words,    \/\/ ScaleUpRange sets the rate of scaling up.    if (committed_bytes < InitialHeapSize \/ 4) {      expand_bytes = (InitialHeapSize - committed_bytes) \/ 2;    } else {      double const MinScaleDownFactor = 0.2;      double const MaxScaleUpFactor = 2;      double const StartScaleDownAt = pause_time_threshold;      double const StartScaleUpAt = pause_time_threshold * 1.5;      double const ScaleUpRange = pause_time_threshold * 2.0;      double ratio_delta;      if (filled_history_buffer) {        ratio_delta = long_term_pause_time_ratio - threshold;      } else {        ratio_delta = (_ratio_over_threshold_sum \/ _ratio_over_threshold_count) - threshold;      }      expand_bytes = MIN2(expand_bytes_via_pct, committed_bytes);      if (ratio_delta < StartScaleDownAt) {        scale_factor = ratio_delta \/ StartScaleDownAt;        scale_factor = MAX2(scale_factor, MinScaleDownFactor);      } else if (ratio_delta > StartScaleUpAt) {        scale_factor = 1 + ((ratio_delta - StartScaleUpAt) \/ ScaleUpRange);        scale_factor = MIN2(scale_factor, MaxScaleUpFactor);      }    }    expand_bytes = static_cast<size_t>(expand_bytes * scale_factor);    \/\/ Ensure the expansion size is at least the minimum growth amount    \/\/ and at most the remaining uncommitted byte size.    expand_bytes = clamp(expand_bytes, min_expand_bytes, uncommitted_bytes);    clear_ratio_check_data();  } else {    \/\/ An expansion was not triggered. If we've started counting, increment    \/\/ the number of checks we've made in the current window.  If we've    \/\/ reached the end of the window without resizing, clear the counters to    \/\/ start again the next time we see a ratio above the threshold.    if (_ratio_over_threshold_count > 0) {      _pauses_since_start++;      if (_pauses_since_start > _num_prev_pauses_for_heuristics) {        clear_ratio_check_data();      }    }  }  log_expansion(short_term_pause_time_ratio, long_term_pause_time_ratio,                threshold, pause_time_threshold, false, expand_bytes);  return expand_bytes;}static size_t target_heap_capacity(size_t used_bytes, uintx free_ratio) {  assert(free_ratio <= 100, \"precondition\");  if (free_ratio == 100) {    \/\/ If 100 then below calculations will divide by zero and return min of    \/\/ resulting infinity and MaxHeapSize.  Avoid issues of UB vs is_iec559    \/\/ and ubsan warnings, and just immediately return MaxHeapSize.    return MaxHeapSize;  }  const double desired_free_percentage = (double) free_ratio \/ 100.0;  const double desired_used_percentage = 1.0 - desired_free_percentage;  \/\/ We have to be careful here as these two calculations can overflow  \/\/ 32-bit size_t's.  double used_bytes_d = (double) used_bytes;  double desired_capacity_d = used_bytes_d \/ desired_used_percentage;  \/\/ Let's make sure that they are both under the max heap size, which  \/\/ by default will make it fit into a size_t.  double desired_capacity_upper_bound = (double) MaxHeapSize;  desired_capacity_d = MIN2(desired_capacity_d, desired_capacity_upper_bound);  \/\/ We can now safely turn it into size_t's.  return (size_t) desired_capacity_d;}size_t G1HeapSizingPolicy::full_collection_resize_amount(bool& expand, size_t allocation_word_size) {  \/\/ If the full collection was triggered by an allocation failure, we should account  \/\/ for the bytes required for this allocation under used_after_gc. This prevents  \/\/ unnecessary shrinking that would be followed by an expand call to satisfy the  \/\/ allocation.  size_t allocation_bytes = allocation_word_size * HeapWordSize;  if (_g1h->is_humongous(allocation_word_size)) {    \/\/ Humongous objects are allocated in entire regions, we must calculate    \/\/ required space in terms of full regions, not just the object size.    allocation_bytes = G1HeapRegion::align_up_to_region_byte_size(allocation_bytes);  }  \/\/ Capacity, free and used after the GC counted as full regions to  \/\/ include the waste in the following calculations.  const size_t capacity_after_gc = _g1h->capacity();  const size_t used_after_gc = capacity_after_gc + allocation_bytes -                               _g1h->unused_committed_regions_in_bytes() -                               \/\/ Discount space used by current Eden to establish a                               \/\/ situation during Remark similar to at the end of full                               \/\/ GC where eden is empty. During Remark there can be an                               \/\/ arbitrary number of eden regions which would skew the                               \/\/ results.                               _g1h->eden_regions_count() * G1HeapRegion::GrainBytes;  size_t minimum_desired_capacity = target_heap_capacity(used_after_gc, MinHeapFreeRatio);  size_t maximum_desired_capacity = target_heap_capacity(used_after_gc, MaxHeapFreeRatio);  \/\/ This assert only makes sense here, before we adjust them  \/\/ with respect to the min and max heap size.  assert(minimum_desired_capacity <= maximum_desired_capacity,         \"minimum_desired_capacity = %zu, \"         \"maximum_desired_capacity = %zu\",         minimum_desired_capacity, maximum_desired_capacity);  \/\/ Should not be greater than the heap max size. No need to adjust  \/\/ it with respect to the heap min size as it's a lower bound (i.e.,  \/\/ we'll try to make the capacity larger than it, not smaller).  minimum_desired_capacity = MIN2(minimum_desired_capacity, MaxHeapSize);  \/\/ Should not be less than the heap min size. No need to adjust it  \/\/ with respect to the heap max size as it's an upper bound (i.e.,  \/\/ we'll try to make the capacity smaller than it, not greater).  maximum_desired_capacity =  MAX2(maximum_desired_capacity, MinHeapSize);  \/\/ Don't expand unless it's significant; prefer expansion to shrinking.  if (capacity_after_gc < minimum_desired_capacity) {    size_t expand_bytes = minimum_desired_capacity - capacity_after_gc;    log_debug(gc, ergo, heap)(\"Attempt heap expansion (capacity lower than min desired capacity). \"                              \"Capacity: %zuB occupancy: %zuB live: %zuB \"                              \"min_desired_capacity: %zuB (%zu %%)\",                              capacity_after_gc, used_after_gc, _g1h->used(), minimum_desired_capacity, MinHeapFreeRatio);    expand = true;    return expand_bytes;    \/\/ No expansion, now see if we want to shrink  } else if (capacity_after_gc > maximum_desired_capacity) {    \/\/ Capacity too large, compute shrinking size    size_t shrink_bytes = capacity_after_gc - maximum_desired_capacity;    log_debug(gc, ergo, heap)(\"Attempt heap shrinking (capacity higher than max desired capacity). \"                              \"Capacity: %zuB occupancy: %zuB live: %zuB \"                              \"maximum_desired_capacity: %zuB (%zu %%)\",                              capacity_after_gc, used_after_gc, _g1h->used(), maximum_desired_capacity, MaxHeapFreeRatio);    expand = false;    return shrink_bytes;  }  expand = true; \/\/ Does not matter.  return 0;}void G1HeapSizingPolicy::get_uncommit_candidates(GrowableArray<G1HeapRegion*>* candidates) {  uint inactive_regions = 0;  \/\/ Check each heap region for inactivity  class UncommitCandidatesClosure : public G1HeapRegionClosure {    GrowableArray<G1HeapRegion*>* _candidates;    uint* _inactive_regions;    const G1HeapSizingPolicy* _policy;  public:    UncommitCandidatesClosure(GrowableArray<G1HeapRegion*>* candidates,                             uint* inactive_regions,                             const G1HeapSizingPolicy* policy) :      _candidates(candidates),      _inactive_regions(inactive_regions),      _policy(policy) {}    virtual bool do_heap_region(G1HeapRegion* r) {      if (r->is_empty() && _policy->should_uncommit_region(r)) {        _candidates->append(r);        (*_inactive_regions)++;      }      return false;    }  } cl(candidates, &inactive_regions, this);  _g1h->heap_region_iterate(&cl);  if (inactive_regions > 0) {    log_debug(gc, sizing)(\"Uncommit candidates found: %u inactive regions out of %u total regions\",                  inactive_regions, _g1h->max_num_regions());    log_debug(gc, sizing)(\"Region state transition: %u regions found eligible for uncommit after scan\",                  inactive_regions);  }  log_debug(gc, sizing)(\"Full region scan: found %u inactive regions out of %u total regions\",                       inactive_regions,                       _g1h->max_num_regions());}bool G1HeapSizingPolicy::should_uncommit_region(G1HeapRegion* hr) const {  \/\/ Note: Caller already guarantees hr->is_empty() is true  \/\/ Empty regions should always be free and not in collection set in normal operation  jlong current_time = os::javaTimeMillis();  jlong last_access = hr->last_access_time();  jlong elapsed = current_time - last_access;  log_trace(gc, sizing)(\"Region %u uncommit check: elapsed=\" JLONG_FORMAT \"ms threshold=\" JLONG_FORMAT \"ms last_access=\" JLONG_FORMAT \" now=\" JLONG_FORMAT \" empty=%s\",                     hr->hrm_index(), elapsed, (jlong)_uncommit_delay_ms, last_access, current_time,                     hr->is_empty() ? \"true\" : \"false\");  bool should_uncommit = elapsed > (jlong)_uncommit_delay_ms;  if (should_uncommit) {    log_debug(gc, sizing)(\"Region state transition: Region %u transitioning from active to inactive after \" JLONG_FORMAT \"ms idle\",                  hr->hrm_index(), elapsed);  }  return should_uncommit;}size_t G1HeapSizingPolicy::evaluate_heap_resize(bool& expand) {  expand = false; \/\/ Time-based sizing only handles uncommit, never expansion  if (!G1UseTimeBasedHeapSizing) {    return 0;  }  \/\/ Don't resize during GC  if (_g1h->is_stw_gc_active()) {    return 0;  }  \/\/ Must hold Heap_lock during heap resizing  MutexLocker ml(Heap_lock);  ResourceMark rm; \/\/ Ensure GrowableArray resources are properly released  \/\/ Find regions eligible for uncommit  GrowableArray<G1HeapRegion*> candidates;  get_uncommit_candidates(&candidates);  uint inactive_count = candidates.length();  uint total_regions = _g1h->max_num_regions();  \/\/ Need minimum number of inactive regions to proceed  if (inactive_count >= G1MinRegionsToUncommit) {    size_t region_size = G1HeapRegion::GrainBytes;    size_t current_heap = _g1h->capacity();    size_t min_heap = MAX2((size_t)InitialHeapSize, MinHeapSize);  \/\/ Never go below initial size    \/\/ Calculate maximum bytes we can uncommit while respecting min heap size    size_t max_shrink_bytes = current_heap > min_heap ? current_heap - min_heap : 0;    log_trace(gc, sizing)(\"Time-based evaluation details: current_heap=%zuB min_heap=%zuB \"                         \"region_size=%zuB max_shrink=%zuB initial_size=%zuB\",                         current_heap, min_heap, region_size, max_shrink_bytes, InitialHeapSize);    if (max_shrink_bytes > 0 && region_size > 0) {      size_t max_inactive_regions = max_shrink_bytes \/ region_size;      \/\/ Calculate maximum uncommit target as the smaller of:      \/\/ 1. No more than 25% of inactive regions      \/\/ 2. No more than 10% of total committed regions      \/\/ 3. No more than max_shrink_bytes worth of regions      size_t committed_regions = current_heap \/ region_size;      \/\/ Upper limits:      size_t by_inactive = static_cast<size_t>(inactive_count) \/ 4;    \/\/ 25%      size_t by_total = static_cast<size_t>(committed_regions) \/ 10;   \/\/ 10%      size_t regions_to_uncommit =          MIN2(by_total, MIN2(by_inactive, max_inactive_regions));      size_t shrink_bytes = regions_to_uncommit * region_size;      shrink_bytes = MIN2(shrink_bytes, current_heap - MinHeapSize);      if (current_heap - shrink_bytes < InitialHeapSize) {        log_info(gc, sizing)(\"Time-based uncommit skipped: would reduce heap below initial size (%zuMB < %zuMB)\",                            (current_heap - shrink_bytes) \/ M, InitialHeapSize \/ M);        log_debug(gc, sizing)(\"Skipping uncommit - would reduce heap below initial size: \"                             \"current=%zuB shrink=%zuB result=%zuB initial=%zuB min=%zuB\",                             current_heap, shrink_bytes, current_heap - shrink_bytes,                             InitialHeapSize, MinHeapSize);        return 0;      }      if (shrink_bytes > 0) {        log_info(gc, sizing)(\"Time-based uncommit: found %u inactive regions, uncommitting %zu regions (%zuMB)\",                            inactive_count, regions_to_uncommit, shrink_bytes \/ M);        log_debug(gc, sizing)(\"Time-based heap uncommit evaluation: Found %u inactive regions out of %u total regions, \"                             \"target shrink: %zuB (max allowed: %zuB)\",                             inactive_count, total_regions, shrink_bytes, max_shrink_bytes);        log_debug(gc, sizing)(\"Region state transition: %zu regions selected for uncommit\",                     regions_to_uncommit);      }      return shrink_bytes;    }  }  log_trace(gc, sizing)(\"Time-based heap evaluation: no uncommit needed \"                       \"(inactive=%u min_required=%zu heap=%zuB min=%zuB)\",                       inactive_count, G1MinRegionsToUncommit,                       _g1h->capacity(), MAX2((size_t)InitialHeapSize, MinHeapSize));  return 0;}\n\\ No newline at end of file\n","filename":"src\/hotspot\/share\/gc\/g1\/g1HeapSizingPolicy.cpp","additions":1,"deletions":438,"binary":false,"changes":439,"status":"modified"},{"patch":"@@ -1,1 +1,1 @@\n-\/*\n+\/*\n@@ -29,0 +29,2 @@\n+#include \"gc\/g1\/g1_globals.hpp\"\n+#include \"gc\/g1\/g1HeapRegion.hpp\"\n@@ -30,0 +32,3 @@\n+#include \"runtime\/globals.hpp\"\n+#include \"utilities\/debug.hpp\"\n+#include \"utilities\/globalDefinitions.hpp\"\n@@ -45,1 +50,1 @@\n-\/\/ gc_cpu_usage_target  [1 - d, 1 + d], where d = G1CPUUsageDeviationPercent \/ 100\n+\/\/ gc_cpu_usage_target  [1 - d, 1 + d], where d = G1CPUUsageDeviationPercent \/ 100\n@@ -69,0 +74,6 @@\n+  \/\/ MinOverThresholdForGrowth must be less than the number of recorded\n+  \/\/ pause times in G1Analytics, representing the minimum number of pause\n+  \/\/ time ratios that exceed GCTimeRatio before a heap expansion will be triggered.\n+  const static uint MinOverThresholdForGrowth = 4;\n+  static jlong _uncommit_delay_ms;  \/\/ Delay before uncommitting inactive regions\n+\n@@ -96,1 +107,0 @@\n-public:\n@@ -101,0 +111,6 @@\n+\n+  \/\/ Methods for time-based sizing\n+  void get_uncommit_candidates(GrowableArray<G1HeapRegion*>* candidates);\n+  bool should_uncommit_region(G1HeapRegion* hr) const;\n+\n+public:\n@@ -106,0 +122,4 @@\n+  \/\/ If an expansion would be appropriate, because recent GC overhead had\n+  \/\/ exceeded the desired limit, return an amount to expand by.\n+  size_t young_collection_expansion_amount();\n+\n@@ -110,0 +130,8 @@\n+  \/\/ Clear ratio tracking data used by expansion_amount().\n+  void clear_ratio_check_data();\n+\n+  \/\/ Time-based sizing methods\n+  static void initialize();\n+  static jlong uncommit_delay() { return _uncommit_delay_ms; }\n+  size_t evaluate_heap_resize(bool& expand);\n+\n","filename":"src\/hotspot\/share\/gc\/g1\/g1HeapSizingPolicy.hpp","additions":31,"deletions":3,"binary":false,"changes":34,"status":"modified"},{"patch":"@@ -51,0 +51,7 @@\n+\n+  \/\/ Print a message about time-based heap sizing configuration.\n+  if (G1UseTimeBasedHeapSizing) {\n+    log_info_p(gc, init)(\"G1 Time-Based Heap Sizing enabled (uncommit-only)\");\n+    log_info_p(gc, init)(\"  evaluation_interval=%zums, uncommit_delay=%zums, min_regions_to_uncommit=%zu\",\n+                         G1TimeBasedEvaluationIntervalMillis, G1UncommitDelayMillis, G1MinRegionsToUncommit);\n+  }\n","filename":"src\/hotspot\/share\/gc\/g1\/g1InitLogger.cpp","additions":7,"deletions":0,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -373,0 +373,19 @@\n+  product(bool, G1UseTimeBasedHeapSizing, false, EXPERIMENTAL,              \\\n+          \"Enable time-based heap sizing to uncommit memory from inactive \" \\\n+          \"regions independent of GC cycles\")                               \\\n+                                                                            \\\n+  product(uintx, G1TimeBasedEvaluationIntervalMillis, 60000, MANAGEABLE,    \\\n+          \"Interval in milliseconds between periodic heap-size evaluations \"\\\n+          \"when G1UseTimeBasedHeapSizing is enabled\")                       \\\n+          range(1000, LP64_ONLY(max_jlong) NOT_LP64(max_uintx \/ 2))         \\\n+                                                                            \\\n+  product(uintx, G1UncommitDelayMillis, 300000, MANAGEABLE,                 \\\n+          \"A region is considered inactive if it has not been accessed \"    \\\n+          \"within this many milliseconds\")                                  \\\n+          range(1000, LP64_ONLY(max_jlong) NOT_LP64(max_uintx \/ 2))         \\\n+                                                                            \\\n+  product(size_t, G1MinRegionsToUncommit, 10, EXPERIMENTAL,                 \\\n+          \"Minimum number of inactive regions required before G1 will \"     \\\n+          \"attempt to uncommit memory\")                                     \\\n+          range(1, max_uintx)                                               \\\n+                                                                            \\\n","filename":"src\/hotspot\/share\/gc\/g1\/g1_globals.hpp","additions":19,"deletions":0,"binary":false,"changes":19,"status":"modified"},{"patch":"@@ -181,0 +181,1 @@\n+  LOG_TAG(sizing) \\\n","filename":"src\/hotspot\/share\/logging\/logTag.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -61,0 +61,1 @@\n+  template(G1ShrinkHeap)                          \\\n","filename":"src\/hotspot\/share\/runtime\/vmOperation.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -31,0 +31,3 @@\n+#if INCLUDE_G1GC\n+#include \"gc\/g1\/g1CollectedHeap.hpp\"\n+#endif \/\/ INCLUDE_G1GC\n@@ -641,0 +644,6 @@\n+#if INCLUDE_G1GC\n+void VM_G1ShrinkHeap::doit() {\n+  _g1h->shrink(_bytes);\n+}\n+#endif \/\/ INCLUDE_G1GC\n+\n","filename":"src\/hotspot\/share\/runtime\/vmOperations.cpp","additions":9,"deletions":0,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -33,0 +33,2 @@\n+class G1CollectedHeap;\n+\n@@ -294,0 +296,15 @@\n+#if INCLUDE_G1GC\n+class VM_G1ShrinkHeap : public VM_Operation {\n+ private:\n+  G1CollectedHeap* _g1h;\n+  size_t _bytes;\n+ public:\n+  VM_G1ShrinkHeap(G1CollectedHeap* g1h, size_t bytes)\n+    : _g1h(g1h), _bytes(bytes) {}\n+  VMOp_Type type() const override { return VMOp_G1ShrinkHeap; }\n+  const char* name() const override { return \"G1ShrinkHeap\"; }\n+  bool is_gc_operation() const override { return true; }\n+  void doit() override;\n+};\n+#endif \/\/ INCLUDE_G1GC\n+\n","filename":"src\/hotspot\/share\/runtime\/vmOperations.hpp","additions":17,"deletions":0,"binary":false,"changes":17,"status":"modified"},{"patch":"@@ -0,0 +1,244 @@\n+\/*\n+ * Copyright (c) 2025, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+package gc.g1;\n+\n+\/**\n+ * @test TestG1RegionUncommit\n+ * @requires vm.gc.G1\n+ * @summary Test that G1 uncommits regions based on time threshold\n+ * @bug 8357445\n+ * @library \/test\/lib\n+ * @modules java.base\/jdk.internal.misc\n+ *          java.management\/sun.management\n+ * @run main\/othervm -XX:+UseG1GC -Xms8m -Xmx256m -XX:G1HeapRegionSize=1M\n+ *                   -XX:+UnlockExperimentalVMOptions -XX:+G1UseTimeBasedHeapSizing\n+ *                   -XX:G1UncommitDelayMillis=3000 -XX:G1TimeBasedEvaluationIntervalMillis=2000\n+ *                   -XX:G1MinRegionsToUncommit=2\n+ *                   -Xlog:gc*,gc+sizing*=debug\n+ *                   gc.g1.TestG1RegionUncommit\n+ *\/\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+import jdk.test.lib.process.OutputAnalyzer;\n+import jdk.test.lib.process.ProcessTools;\n+\n+public class TestG1RegionUncommit {\n+\n+    public static void main(String[] args) throws Exception {\n+        \/\/ If no args, run the subprocess with log analysis\n+        if (args.length == 0) {\n+            testTimeBasedEvaluation();\n+            testMinimumHeapBoundary();\n+            testConcurrentAllocationUncommit();\n+        } else if (\"subprocess\".equals(args[0])) {\n+            \/\/ This is the subprocess that does the actual allocation\/deallocation\n+            runTimeBasedUncommitTest();\n+        } else if (\"minheap\".equals(args[0])) {\n+            runMinHeapBoundaryTest();\n+        } else if (\"concurrent\".equals(args[0])) {\n+            runConcurrentTest();\n+        }\n+    }\n+\n+    static void testTimeBasedEvaluation() throws Exception {\n+        ProcessBuilder pb = ProcessTools.createTestJavaProcessBuilder(\n+            \"-XX:+UseG1GC\",\n+            \"-Xms8m\", \"-Xmx256m\", \"-XX:G1HeapRegionSize=1M\",\n+            \"-XX:+UnlockExperimentalVMOptions\", \"-XX:+G1UseTimeBasedHeapSizing\",\n+            \"-XX:G1UncommitDelayMillis=3000\", \"-XX:G1TimeBasedEvaluationIntervalMillis=2000\",\n+            \"-XX:G1MinRegionsToUncommit=2\",\n+            \"-Xlog:gc*,gc+sizing*=debug\",\n+            \"gc.g1.TestG1RegionUncommit\", \"subprocess\"\n+        );\n+\n+        OutputAnalyzer output = new OutputAnalyzer(pb.start());\n+\n+        \/\/ Verify the time-based evaluation logic is working\n+        output.shouldContain(\"G1 Time-Based Heap Sizing enabled (uncommit-only)\");\n+        output.shouldContain(\"Starting heap evaluation\");\n+        output.shouldContain(\"Region state transition:\");\n+        output.shouldContain(\"transitioning from active to inactive\");\n+        output.shouldContain(\"Uncommit candidates found:\");\n+        output.shouldContain(\"Time-based heap uncommit evaluation:\");\n+        output.shouldContain(\"target shrink:\");\n+\n+        output.shouldHaveExitValue(0);\n+        System.out.println(\"Test passed - time-based uncommit verified!\");\n+    }\n+\n+    static void runTimeBasedUncommitTest() throws Exception {\n+        final int allocSize = 64 * 1024 * 1024; \/\/ 64MB allocation - much larger than initial 8MB\n+        Object keepAlive;\n+        Object keepAlive2; \/\/ Keep some memory allocated to prevent full shrinkage\n+\n+        System.out.println(\"=== Testing G1 Time-Based Uncommit ===\");\n+\n+        \/\/ Phase 1: Allocate memory to force significant heap expansion\n+        System.out.println(\"Phase 1: Allocating large amount of memory\");\n+        keepAlive = new byte[allocSize];\n+\n+        \/\/ Phase 2: Keep some memory allocated, free the rest to create inactive regions\n+        \/\/ This ensures current_heap > min_heap so uncommit is possible\n+        System.out.println(\"Phase 2: Partially freeing memory, keeping some allocated\");\n+        keepAlive2 = new byte[24 * 1024 * 1024]; \/\/ Keep 24MB allocated\n+        keepAlive = null; \/\/ Free the 64MB, leaving regions available for uncommit\n+        System.gc();\n+        System.gc(); \/\/ Double GC to ensure the 64MB is cleaned up\n+\n+        \/\/ Phase 3: Wait for regions to become inactive and uncommit to occur\n+        System.out.println(\"Phase 3: Waiting for time-based uncommit...\");\n+\n+        \/\/ Wait long enough for:\n+        \/\/ 1. G1UncommitDelayMillis (3000ms) - regions to become inactive\n+        \/\/ 2. G1TimeBasedEvaluationIntervalMillis (2000ms) - evaluation to run\n+        \/\/ 3. Multiple evaluation cycles to ensure uncommit happens\n+        Thread.sleep(15000); \/\/ 15 seconds should be plenty\n+\n+        \/\/ Clean up remaining allocation\n+        keepAlive2 = null;\n+        System.gc();\n+\n+        System.out.println(\"=== Test completed ===\");\n+        Runtime.getRuntime().halt(0);\n+    }\n+\n+    static void testMinimumHeapBoundary() throws Exception {\n+        System.out.println(\"Testing minimum heap boundary conditions...\");\n+\n+        ProcessBuilder pb = ProcessTools.createTestJavaProcessBuilder(\n+            \"-XX:+UseG1GC\",\n+            \"-Xms32m\", \"-Xmx64m\",  \/\/ Small heap to test boundaries\n+            \"-XX:G1HeapRegionSize=1M\",\n+            \"-XX:+UnlockExperimentalVMOptions\", \"-XX:+G1UseTimeBasedHeapSizing\",\n+            \"-XX:G1UncommitDelayMillis=2000\", \/\/ Short delay\n+            \"-XX:G1TimeBasedEvaluationIntervalMillis=1000\",\n+            \"-XX:G1MinRegionsToUncommit=1\",\n+            \"-Xlog:gc+sizing=debug,gc+task=debug\",\n+            \"gc.g1.TestG1RegionUncommit\", \"minheap\"\n+        );\n+\n+        OutputAnalyzer output = new OutputAnalyzer(pb.start());\n+\n+        \/\/ Should not uncommit below initial heap size\n+        output.shouldHaveExitValue(0);\n+        System.out.println(\"Minimum heap boundary test passed!\");\n+    }\n+\n+    static void testConcurrentAllocationUncommit() throws Exception {\n+        System.out.println(\"Testing concurrent allocation and uncommit...\");\n+\n+        ProcessBuilder pb = ProcessTools.createTestJavaProcessBuilder(\n+            \"-XX:+UseG1GC\",\n+            \"-Xms64m\", \"-Xmx256m\",\n+            \"-XX:G1HeapRegionSize=1M\",\n+            \"-XX:+UnlockExperimentalVMOptions\", \"-XX:+G1UseTimeBasedHeapSizing\",\n+            \"-XX:G1TimeBasedEvaluationIntervalMillis=1000\", \/\/ Frequent evaluation\n+            \"-XX:G1UncommitDelayMillis=2000\",\n+            \"-XX:G1MinRegionsToUncommit=2\",\n+            \"-Xlog:gc+sizing=debug,gc+task=debug\",\n+            \"gc.g1.TestG1RegionUncommit\", \"concurrent\"\n+        );\n+\n+        OutputAnalyzer output = new OutputAnalyzer(pb.start());\n+\n+        \/\/ Should handle concurrent operations safely\n+        output.shouldHaveExitValue(0);\n+        System.out.println(\"Concurrent allocation\/uncommit test passed!\");\n+    }\n+\n+    static void runMinHeapBoundaryTest() throws Exception {\n+        System.out.println(\"=== Min Heap Boundary Test ===\");\n+\n+        List<byte[]> memory = new ArrayList<>();\n+\n+        \/\/ Allocate close to max\n+        for (int i = 0; i < 28; i++) { \/\/ 28MB, close to 32MB limit\n+            memory.add(new byte[1024 * 1024]);\n+        }\n+\n+        \/\/ Clear and wait for uncommit attempt\n+        memory.clear();\n+        System.gc();\n+        Thread.sleep(8000); \/\/ Wait longer than uncommit delay\n+\n+        System.out.println(\"MinHeapBoundaryTest completed\");\n+        Runtime.getRuntime().halt(0);\n+    }\n+\n+    static void runConcurrentTest() throws Exception {\n+        System.out.println(\"=== Concurrent Test ===\");\n+\n+        final List<byte[]> sharedMemory = new ArrayList<>();\n+        final boolean[] stopFlag = {false};\n+\n+        \/\/ Start allocation thread\n+        Thread allocThread = new Thread(() -> {\n+            int iterations = 0;\n+            while (!stopFlag[0] && iterations < 50) {\n+                try {\n+                    \/\/ Allocate\n+                    for (int j = 0; j < 5; j++) {\n+                        synchronized (sharedMemory) {\n+                            sharedMemory.add(new byte[1024 * 1024]); \/\/ 1MB\n+                        }\n+                        Thread.sleep(10);\n+                    }\n+\n+                    \/\/ Clear some\n+                    synchronized (sharedMemory) {\n+                        if (sharedMemory.size() > 10) {\n+                            for (int k = 0; k < 5; k++) {\n+                                if (!sharedMemory.isEmpty()) {\n+                                    sharedMemory.remove(0);\n+                                }\n+                            }\n+                        }\n+                    }\n+                    System.gc();\n+                    Thread.sleep(50);\n+                    iterations++;\n+                } catch (InterruptedException e) {\n+                    break;\n+                }\n+            }\n+        });\n+\n+        allocThread.start();\n+\n+        \/\/ Let it run for a while to trigger time-based evaluation\n+        Thread.sleep(8000);\n+\n+        stopFlag[0] = true;\n+        allocThread.join(2000);\n+\n+        synchronized (sharedMemory) {\n+            sharedMemory.clear();\n+        }\n+        System.gc();\n+\n+        System.out.println(\"ConcurrentTest completed\");\n+        Runtime.getRuntime().halt(0);\n+    }\n+}\n","filename":"test\/hotspot\/jtreg\/gc\/g1\/TestG1RegionUncommit.java","additions":244,"deletions":0,"binary":false,"changes":244,"status":"added"},{"patch":"@@ -0,0 +1,190 @@\n+\/*\n+ * Copyright (c) 2025, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+package gc.g1;\n+\n+\/**\n+ * @test TestTimeBasedHeapConfig\n+ * @bug 8357445\n+ * @summary Test configuration settings and error conditions for time-based heap sizing\n+ * @requires vm.gc.G1\n+ * @library \/test\/lib\n+ * @modules java.base\/jdk.internal.misc\n+ *          java.management\/sun.management\n+ * @run main\/othervm -XX:+UseG1GC -XX:+UnlockExperimentalVMOptions -XX:+G1UseTimeBasedHeapSizing\n+ *     -Xms16m -Xmx64m -XX:G1HeapRegionSize=1M\n+ *     -XX:G1TimeBasedEvaluationIntervalMillis=5000\n+ *     -XX:G1UncommitDelayMillis=10000\n+ *     -XX:G1MinRegionsToUncommit=2\n+ *     -Xlog:gc*,gc+sizing*=debug\n+ *     gc.g1.TestTimeBasedHeapConfig\n+ *\/\n+\n+import java.util.*;\n+import jdk.test.lib.process.OutputAnalyzer;\n+import jdk.test.lib.process.ProcessTools;\n+\n+public class TestTimeBasedHeapConfig {\n+\n+    public static void main(String[] args) throws Exception {\n+        testConfigurationParameters();\n+        testBoundaryValues();\n+        testEdgeCaseConfigurations();\n+    }\n+\n+    static void testConfigurationParameters() throws Exception {\n+        \/\/ Test default settings\n+        verifyVMConfig(new String[] {\n+            \"-XX:+UseG1GC\",\n+            \"-XX:+UnlockExperimentalVMOptions\",\n+            \"-XX:+G1UseTimeBasedHeapSizing\",\n+            \"-Xms16m\", \"-Xmx64m\",\n+            \"-XX:G1HeapRegionSize=1M\",\n+            \"-Xlog:gc*,gc+sizing*=debug\",\n+            \"gc.g1.TestTimeBasedHeapConfig$BasicTest\"\n+        });\n+    }\n+\n+    private static void verifyVMConfig(String[] opts) throws Exception {\n+        ProcessBuilder pb = ProcessTools.createTestJavaProcessBuilder(opts);\n+        OutputAnalyzer output = new OutputAnalyzer(pb.start());\n+        output.shouldHaveExitValue(0);\n+    }\n+\n+    public static class BasicTest {\n+        private static final int MB = 1024 * 1024;\n+        private static ArrayList<byte[]> arrays = new ArrayList<>();\n+\n+        public static void main(String[] args) throws Exception {\n+            \/\/ Initial allocation\n+            allocateMemory(8); \/\/ 8MB\n+            System.gc();\n+            Thread.sleep(1000);\n+\n+            \/\/ Clean up\n+            arrays.clear();\n+            System.gc();\n+            Thread.sleep(2000);\n+\n+            System.out.println(\"Basic configuration test completed successfully\");\n+            Runtime.getRuntime().halt(0);\n+        }\n+\n+        static void allocateMemory(int mb) throws InterruptedException {\n+            for (int i = 0; i < mb; i++) {\n+                arrays.add(new byte[MB]);\n+                if (i % 2 == 0) Thread.sleep(10);\n+            }\n+        }\n+    }\n+\n+    static void testBoundaryValues() throws Exception {\n+        \/\/ Test minimum values\n+        verifyVMConfig(new String[] {\n+            \"-XX:+UseG1GC\",\n+            \"-XX:+UnlockExperimentalVMOptions\",\n+            \"-XX:+G1UseTimeBasedHeapSizing\",\n+            \"-Xms8m\", \"-Xmx32m\",\n+            \"-XX:G1HeapRegionSize=1M\",\n+            \"-XX:G1TimeBasedEvaluationIntervalMillis=1000\", \/\/ 1 second minimum\n+            \"-XX:G1UncommitDelayMillis=1000\", \/\/ 1 second minimum\n+            \"-XX:G1MinRegionsToUncommit=1\", \/\/ 1 region minimum\n+            \"-Xlog:gc*,gc+sizing*=debug\",\n+            \"gc.g1.TestTimeBasedHeapConfig$BoundaryTest\"\n+        });\n+\n+        \/\/ Test maximum reasonable values\n+        verifyVMConfig(new String[] {\n+            \"-XX:+UseG1GC\",\n+            \"-XX:+UnlockExperimentalVMOptions\",\n+            \"-XX:+G1UseTimeBasedHeapSizing\",\n+            \"-Xms32m\", \"-Xmx256m\",\n+            \"-XX:G1HeapRegionSize=1M\",\n+            \"-XX:G1TimeBasedEvaluationIntervalMillis=300000\", \/\/ 5 minutes\n+            \"-XX:G1UncommitDelayMillis=300000\", \/\/ 5 minutes\n+            \"-XX:G1MinRegionsToUncommit=50\", \/\/ 50 regions\n+            \"-Xlog:gc*,gc+sizing*=debug\",\n+            \"gc.g1.TestTimeBasedHeapConfig$BoundaryTest\"\n+        });\n+    }\n+\n+    static void testEdgeCaseConfigurations() throws Exception {\n+        \/\/ Test with very small heap (should still work)\n+        verifyVMConfig(new String[] {\n+            \"-XX:+UseG1GC\",\n+            \"-XX:+UnlockExperimentalVMOptions\",\n+            \"-XX:+G1UseTimeBasedHeapSizing\",\n+            \"-Xms4m\", \"-Xmx8m\", \/\/ Very small heap\n+            \"-XX:G1HeapRegionSize=1M\",\n+            \"-XX:G1TimeBasedEvaluationIntervalMillis=2000\",\n+            \"-XX:G1UncommitDelayMillis=3000\",\n+            \"-XX:G1MinRegionsToUncommit=1\",\n+            \"-Xlog:gc*,gc+sizing*=debug\",\n+            \"gc.g1.TestTimeBasedHeapConfig$SmallHeapTest\"\n+        });\n+    }\n+\n+    public static class BoundaryTest {\n+        private static final int MB = 1024 * 1024;\n+        private static ArrayList<byte[]> arrays = new ArrayList<>();\n+\n+        public static void main(String[] args) throws Exception {\n+            System.out.println(\"BoundaryTest: Starting\");\n+\n+            \/\/ Test with boundary conditions\n+            allocateMemory(4); \/\/ 4MB\n+            Thread.sleep(2000);\n+\n+            arrays.clear();\n+            System.gc();\n+            Thread.sleep(5000); \/\/ Wait for evaluation\n+\n+            System.out.println(\"BoundaryTest: Completed\");\n+            Runtime.getRuntime().halt(0);\n+        }\n+\n+        static void allocateMemory(int mb) throws InterruptedException {\n+            for (int i = 0; i < mb; i++) {\n+                arrays.add(new byte[MB]);\n+                Thread.sleep(10);\n+            }\n+        }\n+    }\n+\n+    public static class SmallHeapTest {\n+        public static void main(String[] args) throws Exception {\n+            System.out.println(\"SmallHeapTest: Starting with very small heap\");\n+\n+            \/\/ With 4-8MB heap, just allocate a small amount\n+            byte[] smallAlloc = new byte[1024 * 1024]; \/\/ 1MB\n+            Thread.sleep(2000);\n+\n+            smallAlloc = null;\n+            System.gc();\n+            Thread.sleep(5000);\n+\n+            System.out.println(\"SmallHeapTest: Completed\");\n+            Runtime.getRuntime().halt(0);\n+        }\n+    }\n+}\n","filename":"test\/hotspot\/jtreg\/gc\/g1\/TestTimeBasedHeapConfig.java","additions":190,"deletions":0,"binary":false,"changes":190,"status":"added"},{"patch":"@@ -0,0 +1,261 @@\n+\/*\n+ * Copyright (c) 2025, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+package gc.g1;\n+\n+\/**\n+ * @test TestTimeBasedHeapSizing\n+ * @bug 8357445\n+ * @summary Test time-based heap sizing functionality in G1\n+ * @requires vm.gc.G1\n+ * @library \/test\/lib\n+ * @modules java.base\/jdk.internal.misc\n+ *          java.management\/sun.management\n+ * @run main\/othervm -XX:+UseG1GC -XX:+UnlockExperimentalVMOptions -XX:+G1UseTimeBasedHeapSizing\n+ *     -Xms32m -Xmx128m -XX:G1HeapRegionSize=1M\n+ *     -XX:G1TimeBasedEvaluationIntervalMillis=5000\n+ *     -XX:G1UncommitDelayMillis=10000\n+ *     -XX:G1MinRegionsToUncommit=2\n+ *     -Xlog:gc*,gc+sizing*=debug\n+ *     gc.g1.TestTimeBasedHeapSizing\n+ *\/\n+\n+import java.util.*;\n+import jdk.test.lib.process.OutputAnalyzer;\n+import jdk.test.lib.process.ProcessTools;\n+\n+public class TestTimeBasedHeapSizing {\n+\n+    private static final String TEST_VM_OPTS = \"-XX:+UseG1GC \" +\n+        \"-XX:+UnlockExperimentalVMOptions \" +\n+        \"-XX:+G1UseTimeBasedHeapSizing \" +\n+        \"-XX:G1TimeBasedEvaluationIntervalMillis=5000 \" +\n+        \"-XX:G1UncommitDelayMillis=10000 \" +\n+        \"-XX:G1MinRegionsToUncommit=2 \" +\n+        \"-XX:G1HeapRegionSize=1M \" +\n+        \"-Xmx128m -Xms32m \" +\n+        \"-Xlog:gc*,gc+sizing*=debug\";\n+\n+    public static void main(String[] args) throws Exception {\n+        testBasicFunctionality();\n+        testHumongousObjectHandling();\n+        testRapidAllocationCycles();\n+        testLargeHumongousObjects();\n+    }\n+\n+    static void testBasicFunctionality() throws Exception {\n+        String[] command = new String[TEST_VM_OPTS.split(\" \").length + 1];\n+        System.arraycopy(TEST_VM_OPTS.split(\" \"), 0, command, 0, TEST_VM_OPTS.split(\" \").length);\n+        command[command.length - 1] = \"gc.g1.TestTimeBasedHeapSizing$BasicFunctionalityTest\";\n+        ProcessBuilder pb = ProcessTools.createTestJavaProcessBuilder(command);\n+        OutputAnalyzer output = new OutputAnalyzer(pb.start());\n+\n+        output.shouldContain(\"G1 Time-Based Heap Sizing enabled (uncommit-only)\");\n+        output.shouldContain(\"Starting heap evaluation\");\n+        output.shouldContain(\"Full region scan:\");\n+\n+        output.shouldHaveExitValue(0);\n+    }\n+\n+    public static class BasicFunctionalityTest {\n+        private static final int MB = 1024 * 1024;\n+        private static ArrayList<byte[]> arrays = new ArrayList<>();\n+\n+        public static void main(String[] args) throws Exception {\n+            System.out.println(\"BasicFunctionalityTest: Starting heap activity\");\n+\n+            \/\/ Create significant heap activity\n+            for (int cycle = 0; cycle < 3; cycle++) {\n+                System.out.println(\"Allocation cycle \" + cycle);\n+                allocateMemory(25);  \/\/ 25MB per cycle\n+                Thread.sleep(200);   \/\/ Brief pause\n+                clearMemory();\n+                System.gc();\n+                Thread.sleep(200);\n+            }\n+\n+            System.out.println(\"BasicFunctionalityTest: Starting idle period\");\n+\n+            \/\/ Sleep to allow time-based evaluation\n+            Thread.sleep(18000);  \/\/ 18 seconds\n+\n+            System.out.println(\"BasicFunctionalityTest: Completed idle period\");\n+\n+            \/\/ Final cleanup\n+            clearMemory();\n+            Thread.sleep(500);\n+\n+            System.out.println(\"BasicFunctionalityTest: Test completed\");\n+            Runtime.getRuntime().halt(0);\n+        }\n+\n+        static void allocateMemory(int mb) throws InterruptedException {\n+            for (int i = 0; i < mb; i++) {\n+                arrays.add(new byte[MB]);\n+                if (i % 4 == 0) Thread.sleep(10);\n+            }\n+        }\n+\n+        static void clearMemory() {\n+            arrays.clear();\n+            System.gc();\n+        }\n+    }\n+\n+    static void testHumongousObjectHandling() throws Exception {\n+        String[] command = new String[TEST_VM_OPTS.split(\" \").length + 1];\n+        System.arraycopy(TEST_VM_OPTS.split(\" \"), 0, command, 0, TEST_VM_OPTS.split(\" \").length);\n+        command[command.length - 1] = \"gc.g1.TestTimeBasedHeapSizing$HumongousObjectTest\";\n+        ProcessBuilder pb = ProcessTools.createTestJavaProcessBuilder(command);\n+        OutputAnalyzer output = new OutputAnalyzer(pb.start());\n+\n+        output.shouldContain(\"Starting heap evaluation\");\n+        output.shouldHaveExitValue(0);\n+    }\n+\n+    static void testRapidAllocationCycles() throws Exception {\n+        String[] command = new String[TEST_VM_OPTS.split(\" \").length + 1];\n+        System.arraycopy(TEST_VM_OPTS.split(\" \"), 0, command, 0, TEST_VM_OPTS.split(\" \").length);\n+        command[command.length - 1] = \"gc.g1.TestTimeBasedHeapSizing$RapidCycleTest\";\n+        ProcessBuilder pb = ProcessTools.createTestJavaProcessBuilder(command);\n+        OutputAnalyzer output = new OutputAnalyzer(pb.start());\n+\n+        output.shouldContain(\"Starting heap evaluation\");\n+        output.shouldHaveExitValue(0);\n+    }\n+\n+    static void testLargeHumongousObjects() throws Exception {\n+        System.out.println(\"Testing large humongous object activity tracking...\");\n+\n+        ProcessBuilder pb = ProcessTools.createTestJavaProcessBuilder(\n+            \"-XX:+UseG1GC\",\n+            \"-XX:+UnlockExperimentalVMOptions\",\n+            \"-XX:+G1UseTimeBasedHeapSizing\",\n+            \"-Xms64m\", \"-Xmx256m\",\n+            \"-XX:G1HeapRegionSize=1M\",\n+            \"-XX:G1UncommitDelayMillis=5000\",\n+            \"-XX:G1MinRegionsToUncommit=1\",\n+            \"-Xlog:gc*,gc+sizing*=debug\",\n+            \"gc.g1.TestTimeBasedHeapSizing$LargeHumongousTest\"\n+        );\n+\n+        OutputAnalyzer output = new OutputAnalyzer(pb.start());\n+\n+        \/\/ Large humongous objects should not affect uncommit safety\n+        output.shouldContain(\"G1 Time-Based Heap Sizing enabled (uncommit-only)\");\n+        output.shouldHaveExitValue(0);\n+        System.out.println(\"Large humongous object test passed!\");\n+    }\n+\n+    public static class HumongousObjectTest {\n+        private static final int MB = 1024 * 1024;\n+        private static ArrayList<byte[]> humongousObjects = new ArrayList<>();\n+\n+        public static void main(String[] args) throws Exception {\n+            System.out.println(\"HumongousObjectTest: Starting\");\n+\n+            \/\/ Allocate humongous objects (> 512KB for 1MB regions)\n+            for (int i = 0; i < 8; i++) {\n+                humongousObjects.add(new byte[800 * 1024]); \/\/ 800KB humongous\n+                System.out.println(\"Allocated humongous object \" + (i + 1));\n+                Thread.sleep(200);\n+            }\n+\n+            \/\/ Keep them alive for a while\n+            Thread.sleep(3000);\n+\n+            \/\/ Clear and test uncommit behavior\n+            humongousObjects.clear();\n+            System.gc();\n+            Thread.sleep(12000); \/\/ Wait for uncommit delay\n+\n+            System.out.println(\"HumongousObjectTest: Test completed\");\n+            Runtime.getRuntime().halt(0);\n+        }\n+    }\n+\n+    public static class RapidCycleTest {\n+        private static final int MB = 1024 * 1024;\n+        private static ArrayList<byte[]> memory = new ArrayList<>();\n+\n+        public static void main(String[] args) throws Exception {\n+            System.out.println(\"RapidCycleTest: Starting\");\n+\n+            \/\/ Rapid allocation\/deallocation cycles\n+            for (int cycle = 0; cycle < 15; cycle++) {\n+                \/\/ Quick allocation\n+                for (int i = 0; i < 8; i++) {\n+                    memory.add(new byte[MB]); \/\/ 1MB\n+                }\n+\n+                \/\/ Quick deallocation\n+                memory.clear();\n+                System.gc();\n+\n+                \/\/ Brief pause\n+                Thread.sleep(100);\n+\n+                if (cycle % 5 == 0) {\n+                    System.out.println(\"Completed cycle \" + cycle);\n+                }\n+            }\n+\n+            \/\/ Final wait for time-based evaluation\n+            Thread.sleep(12000);\n+\n+            System.out.println(\"RapidCycleTest: Test completed\");\n+            Runtime.getRuntime().halt(0);\n+        }\n+    }\n+\n+    public static class LargeHumongousTest {\n+        public static void main(String[] args) throws Exception {\n+            System.out.println(\"=== Large Humongous Object Test ===\");\n+\n+            \/\/ Allocate several large humongous objects (multiple regions each)\n+            List<byte[]> humongousObjects = new ArrayList<>();\n+\n+            \/\/ Each region is 1MB, so allocate 2MB objects (humongous spanning multiple regions)\n+            for (int i = 0; i < 5; i++) {\n+                humongousObjects.add(new byte[2 * 1024 * 1024]);\n+                System.gc(); \/\/ Force potential region transitions\n+                Thread.sleep(100);\n+            }\n+\n+            \/\/ Hold some, release others to create mixed region states\n+            humongousObjects.remove(0);\n+            humongousObjects.remove(0);\n+            System.gc();\n+\n+            \/\/ Wait for time-based evaluation with humongous regions present\n+            Thread.sleep(8000);\n+\n+            \/\/ Clean up\n+            humongousObjects.clear();\n+            System.gc();\n+\n+            System.out.println(\"LargeHumongousTest: Test completed\");\n+            Runtime.getRuntime().halt(0);\n+        }\n+    }\n+}\n","filename":"test\/hotspot\/jtreg\/gc\/g1\/TestTimeBasedHeapSizing.java","additions":261,"deletions":0,"binary":false,"changes":261,"status":"added"},{"patch":"@@ -0,0 +1,342 @@\n+\/*\n+ * Copyright (c) 2025, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+package gc.g1;\n+\n+\/**\n+ * @test TestTimeBasedRegionTracking\n+ * @bug 8357445\n+ * @summary Test region activity tracking and state transitions for time-based heap sizing\n+ * @requires vm.gc.G1\n+ * @library \/test\/lib\n+ * @modules java.base\/jdk.internal.misc\n+ *          java.management\/sun.management\n+ * @run main\/othervm -XX:+UseG1GC -XX:+UnlockExperimentalVMOptions -XX:+G1UseTimeBasedHeapSizing\n+ *      -Xms32m -Xmx128m -XX:G1HeapRegionSize=1M\n+ *      -XX:G1TimeBasedEvaluationIntervalMillis=5000\n+ *      -XX:G1UncommitDelayMillis=10000\n+ *      -XX:G1MinRegionsToUncommit=2\n+ *      -Xlog:gc*,gc+sizing*=debug gc.g1.TestTimeBasedRegionTracking\n+ *\/\n+\n+import java.util.*;\n+import jdk.test.lib.process.OutputAnalyzer;\n+import jdk.test.lib.process.ProcessTools;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+\n+public class TestTimeBasedRegionTracking {\n+\n+    private static final String TEST_VM_OPTS = \"-XX:+UseG1GC \" +\n+        \"-XX:+UnlockExperimentalVMOptions \" +\n+        \"-XX:+G1UseTimeBasedHeapSizing \" +\n+        \"-XX:G1TimeBasedEvaluationIntervalMillis=5000 \" +\n+        \"-XX:G1UncommitDelayMillis=10000 \" +\n+        \"-XX:G1MinRegionsToUncommit=2 \" +\n+        \"-XX:G1HeapRegionSize=1M \" +\n+        \"-Xmx128m -Xms32m \" +\n+        \"-Xlog:gc*,gc+sizing*=debug\";\n+\n+    public static void main(String[] args) throws Exception {\n+        testRegionStateTransitions();\n+        testConcurrentRegionAccess();\n+        testRegionLifecycleEdgeCases();\n+        testSafepointRaceConditions();\n+    }\n+\n+    static void testRegionStateTransitions() throws Exception {\n+        String[] command = new String[TEST_VM_OPTS.split(\" \").length + 1];\n+        System.arraycopy(TEST_VM_OPTS.split(\" \"), 0, command, 0, TEST_VM_OPTS.split(\" \").length);\n+        command[command.length - 1] = \"gc.g1.TestTimeBasedRegionTracking$RegionTransitionTest\";\n+        ProcessBuilder pb = ProcessTools.createTestJavaProcessBuilder(command);\n+        OutputAnalyzer output = new OutputAnalyzer(pb.start());\n+\n+        \/\/ Verify region state changes\n+        output.shouldContain(\"Region state transition:\");\n+        output.shouldContain(\"Uncommit candidates found:\");\n+\n+        output.shouldHaveExitValue(0);\n+    }\n+\n+    public static class RegionTransitionTest {\n+        private static final int MB = 1024 * 1024;\n+        private static ArrayList<byte[]> arrays = new ArrayList<>();\n+\n+        public static void main(String[] args) throws Exception {\n+            \/\/ Phase 1: Active allocation\n+            allocateMemory(32); \/\/ 32MB\n+            System.gc();\n+\n+            \/\/ Phase 2: Idle period\n+            arrays.clear();\n+            System.gc();\n+            Thread.sleep(15000); \/\/ Wait for uncommit\n+\n+            \/\/ Phase 3: Reallocation\n+            allocateMemory(16); \/\/ 16MB\n+            System.gc();\n+\n+            \/\/ Clean up and wait for final uncommit evaluation\n+            arrays = null;\n+            System.gc();\n+            Thread.sleep(2000);\n+            Runtime.getRuntime().halt(0);\n+        }\n+\n+        static void allocateMemory(int mb) throws InterruptedException {\n+            for (int i = 0; i < mb; i++) {\n+                arrays.add(new byte[MB]);\n+                if (i % 4 == 0) Thread.sleep(10);\n+            }\n+        }\n+    }\n+\n+    static void testConcurrentRegionAccess() throws Exception {\n+        String[] command = new String[TEST_VM_OPTS.split(\" \").length + 1];\n+        System.arraycopy(TEST_VM_OPTS.split(\" \"), 0, command, 0, TEST_VM_OPTS.split(\" \").length);\n+        command[command.length - 1] = \"gc.g1.TestTimeBasedRegionTracking$ConcurrentAccessTest\";\n+        ProcessBuilder pb = ProcessTools.createTestJavaProcessBuilder(command);\n+        OutputAnalyzer output = new OutputAnalyzer(pb.start());\n+\n+        \/\/ Verify concurrent access is handled safely\n+        output.shouldHaveExitValue(0);\n+    }\n+\n+    static void testRegionLifecycleEdgeCases() throws Exception {\n+        String[] command = new String[TEST_VM_OPTS.split(\" \").length + 1];\n+        System.arraycopy(TEST_VM_OPTS.split(\" \"), 0, command, 0, TEST_VM_OPTS.split(\" \").length);\n+        command[command.length - 1] = \"gc.g1.TestTimeBasedRegionTracking$RegionLifecycleEdgeCaseTest\";\n+        ProcessBuilder pb = ProcessTools.createTestJavaProcessBuilder(command);\n+        OutputAnalyzer output = new OutputAnalyzer(pb.start());\n+\n+        \/\/ Verify region lifecycle edge cases are handled\n+        output.shouldHaveExitValue(0);\n+    }\n+\n+    static void testSafepointRaceConditions() throws Exception {\n+        System.out.println(\"Testing safepoint and allocation race conditions...\");\n+\n+        ProcessBuilder pb = ProcessTools.createTestJavaProcessBuilder(\n+            \"-XX:+UseG1GC\",\n+            \"-XX:+UnlockExperimentalVMOptions\",\n+            \"-XX:+G1UseTimeBasedHeapSizing\",\n+            \"-Xms64m\", \"-Xmx256m\",\n+            \"-XX:G1HeapRegionSize=1M\",\n+            \"-XX:G1TimeBasedEvaluationIntervalMillis=1000\", \/\/ Frequent evaluation (minimum allowed)\n+            \"-XX:G1UncommitDelayMillis=1000\", \/\/ Short delay\n+            \"-XX:G1MinRegionsToUncommit=1\",\n+            \"-Xlog:gc*,gc+sizing*=debug\",\n+            \"gc.g1.TestTimeBasedRegionTracking$SafepointRaceTest\"\n+        );\n+\n+        OutputAnalyzer output = new OutputAnalyzer(pb.start());\n+\n+        \/\/ Should handle safepoint races without errors\n+        output.shouldContain(\"G1 Time-Based Heap Sizing enabled (uncommit-only)\");\n+        output.shouldHaveExitValue(0);\n+        System.out.println(\"Safepoint race conditions test passed!\");\n+    }\n+\n+    public static class ConcurrentAccessTest {\n+        private static final int MB = 1024 * 1024;\n+        private static final List<byte[]> sharedMemory = new ArrayList<>();\n+        private static volatile boolean stopThreads = false;\n+\n+        public static void main(String[] args) throws Exception {\n+            System.out.println(\"ConcurrentAccessTest: Starting\");\n+\n+            \/\/ Start multiple allocation threads\n+            Thread[] threads = new Thread[3];\n+            for (int t = 0; t < threads.length; t++) {\n+                final int threadId = t;\n+                threads[t] = new Thread(() -> {\n+                    int iterations = 0;\n+                    while (!stopThreads && iterations < 30) {\n+                        try {\n+                            \/\/ Allocate\n+                            for (int i = 0; i < 3; i++) {\n+                                synchronized (sharedMemory) {\n+                                    sharedMemory.add(new byte[512 * 1024]); \/\/ 512KB\n+                                }\n+                                Thread.sleep(10);\n+                            }\n+\n+                            \/\/ Clear some memory\n+                            synchronized (sharedMemory) {\n+                                if (sharedMemory.size() > 15) {\n+                                    for (int i = 0; i < 5; i++) {\n+                                        if (!sharedMemory.isEmpty()) {\n+                                            sharedMemory.remove(0);\n+                                        }\n+                                    }\n+                                }\n+                            }\n+\n+                            if (iterations % 10 == 0) {\n+                                System.gc();\n+                            }\n+\n+                            iterations++;\n+                            Thread.sleep(50);\n+                        } catch (InterruptedException e) {\n+                            break;\n+                        }\n+                    }\n+                    System.out.println(\"Thread \" + threadId + \" completed \" + iterations + \" iterations\");\n+                });\n+                threads[t].start();\n+            }\n+\n+            \/\/ Let threads run for a while\n+            Thread.sleep(8000);\n+\n+            stopThreads = true;\n+            for (Thread t : threads) {\n+                t.join(2000);\n+            }\n+\n+            synchronized (sharedMemory) {\n+                sharedMemory.clear();\n+            }\n+            System.gc();\n+            Thread.sleep(3000);\n+\n+            System.out.println(\"ConcurrentAccessTest: Test completed\");\n+            Runtime.getRuntime().halt(0);\n+        }\n+    }\n+\n+    public static class RegionLifecycleEdgeCaseTest {\n+        private static final int MB = 1024 * 1024;\n+        private static List<Object> memory = new ArrayList<>();\n+\n+        public static void main(String[] args) throws Exception {\n+            System.out.println(\"RegionLifecycleEdgeCaseTest: Starting\");\n+\n+            \/\/ Phase 1: Mixed allocation patterns\n+            \/\/ Small objects\n+            for (int i = 0; i < 100; i++) {\n+                memory.add(new byte[8 * 1024]); \/\/ 8KB objects\n+            }\n+\n+            \/\/ Medium objects\n+            for (int i = 0; i < 20; i++) {\n+                memory.add(new byte[40 * 1024]); \/\/ 40KB objects\n+            }\n+\n+            \/\/ Large objects (but not humongous)\n+            for (int i = 0; i < 5; i++) {\n+                memory.add(new byte[300 * 1024]); \/\/ 300KB objects\n+            }\n+\n+            Thread.sleep(2000);\n+\n+            \/\/ Phase 2: Create fragmentation by selective deallocation\n+            for (int i = memory.size() - 1; i >= 0; i -= 2) {\n+                memory.remove(i);\n+            }\n+\n+            System.gc();\n+            Thread.sleep(3000);\n+\n+            \/\/ Phase 3: Add humongous objects\n+            for (int i = 0; i < 3; i++) {\n+                memory.add(new byte[900 * 1024]); \/\/ 900KB humongous\n+                Thread.sleep(500);\n+            }\n+\n+            Thread.sleep(2000);\n+\n+            \/\/ Phase 4: Final cleanup\n+            memory.clear();\n+            System.gc();\n+            Thread.sleep(12000); \/\/ Wait for multiple evaluation cycles\n+\n+            System.out.println(\"RegionLifecycleEdgeCaseTest: Test completed\");\n+            Runtime.getRuntime().halt(0);\n+        }\n+    }\n+\n+    public static class SafepointRaceTest {\n+        public static void main(String[] args) throws Exception {\n+            System.out.println(\"=== Safepoint Race Conditions Test ===\");\n+\n+            final AtomicBoolean stopFlag = new AtomicBoolean(false);\n+            final List<byte[]> sharedMemory = Collections.synchronizedList(new ArrayList<>());\n+\n+            \/\/ Start multiple threads to create allocation pressure\n+            Thread[] threads = new Thread[3];\n+            for (int i = 0; i < threads.length; i++) {\n+                final int threadId = i;\n+                threads[i] = new Thread(() -> {\n+                    int iteration = 0;\n+                    while (!stopFlag.get() && iteration < 20) {\n+                        try {\n+                            \/\/ Allocate and deallocate rapidly\n+                            for (int j = 0; j < 5; j++) {\n+                                sharedMemory.add(new byte[512 * 1024]); \/\/ 512KB\n+                            }\n+\n+                            \/\/ Force GC to trigger safepoints\n+                            if (iteration % 3 == 0) {\n+                                System.gc();\n+                            }\n+\n+                            \/\/ Clear some allocations\n+                            synchronized (sharedMemory) {\n+                                if (sharedMemory.size() > 10) {\n+                                    for (int k = 0; k < 3; k++) {\n+                                        if (!sharedMemory.isEmpty()) {\n+                                            sharedMemory.remove(0);\n+                                        }\n+                                    }\n+                                }\n+                            }\n+\n+                            Thread.sleep(100); \/\/ Brief pause\n+                            iteration++;\n+                        } catch (InterruptedException e) {\n+                            break;\n+                        }\n+                    }\n+                });\n+                threads[i].start();\n+            }\n+\n+            \/\/ Let threads run during time-based evaluation\n+            Thread.sleep(8000);\n+\n+            \/\/ Stop threads\n+            stopFlag.set(true);\n+            for (Thread thread : threads) {\n+                thread.join(2000);\n+            }\n+\n+            \/\/ Clean up\n+            sharedMemory.clear();\n+            System.gc();\n+\n+            System.out.println(\"SafepointRaceTest: Test completed\");\n+            Runtime.getRuntime().halt(0);\n+        }\n+    }\n+}\n","filename":"test\/hotspot\/jtreg\/gc\/g1\/TestTimeBasedRegionTracking.java","additions":342,"deletions":0,"binary":false,"changes":342,"status":"added"}]}