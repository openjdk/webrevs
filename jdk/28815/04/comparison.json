{"files":[{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2025, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2025, 2026, Oracle and\/or its affiliates. All rights reserved.\n@@ -67,0 +67,33 @@\n+ATTRIBUTE_ALIGNED(64) static const uint8_t kyberAvx512_12To16Dup[] = {\n+\/\/ 0 - 63\n+    0, 1, 1, 2, 3, 4, 4, 5, 6, 7, 7, 8, 9, 10, 10, 11, 12, 13, 13, 14, 15, 16,\n+    16, 17, 18, 19, 19, 20, 21, 22, 22, 23, 24, 25, 25, 26, 27, 28, 28, 29, 30,\n+    31, 31, 32, 33, 34, 34, 35, 36, 37, 37, 38, 39, 40, 40, 41, 42, 43, 43, 44,\n+    45, 46, 46, 47\n+  };\n+\n+static address kyberAvx512_12To16DupAddr() {\n+  return (address) kyberAvx512_12To16Dup;\n+}\n+\n+ATTRIBUTE_ALIGNED(64) static const uint16_t kyberAvx512_12To16Shift[] = {\n+\/\/ 0 - 31\n+    0, 4, 0, 4, 0, 4, 0, 4, 0, 4, 0, 4, 0, 4, 0, 4, 0, 4, 0, 4, 0, 4, 0, 4, 0,\n+    4, 0, 4, 0, 4, 0, 4\n+  };\n+\n+static address kyberAvx512_12To16ShiftAddr() {\n+  return (address) kyberAvx512_12To16Shift;\n+}\n+\n+ATTRIBUTE_ALIGNED(64) static const uint64_t kyberAvx512_12To16And[] = {\n+\/\/ 0 - 7\n+    0x0FFF0FFF0FFF0FFF, 0x0FFF0FFF0FFF0FFF, 0x0FFF0FFF0FFF0FFF,\n+    0x0FFF0FFF0FFF0FFF, 0x0FFF0FFF0FFF0FFF, 0x0FFF0FFF0FFF0FFF,\n+    0x0FFF0FFF0FFF0FFF, 0x0FFF0FFF0FFF0FFF\n+  };\n+\n+static address kyberAvx512_12To16AndAddr() {\n+  return (address) kyberAvx512_12To16And;\n+}\n+\n@@ -825,1 +858,1 @@\n-  Label Loop;\n+  Label Loop, VBMILoop;\n@@ -829,0 +862,55 @@\n+  if (VM_Version::supports_avx512_vbmi()) {\n+    \/\/ mask load for the first 48 bytes of each vector\n+    __ mov64(rax, 0x0000FFFFFFFFFFFF);\n+    __ kmovql(k1, rax);\n+\n+    __ lea(perms, ExternalAddress(kyberAvx512_12To16DupAddr()));\n+    __ evmovdqub(xmm20, Address(perms), Assembler::AVX_512bit);\n+\n+    __ lea(perms, ExternalAddress(kyberAvx512_12To16ShiftAddr()));\n+    __ evmovdquw(xmm21, Address(perms), Assembler::AVX_512bit);\n+\n+    __ lea(perms, ExternalAddress(kyberAvx512_12To16AndAddr()));\n+    __ evmovdquq(xmm22, Address(perms), Assembler::AVX_512bit);\n+\n+    __ align(OptoLoopAlignment);\n+    __ BIND(VBMILoop);\n+\n+      __ evmovdqub(xmm0, k1, Address(condensed, 0), false,\n+                   Assembler::AVX_512bit);\n+      __ evmovdqub(xmm1, k1, Address(condensed, 48), false,\n+                   Assembler::AVX_512bit);\n+      __ evmovdqub(xmm2, k1, Address(condensed, 96), false,\n+                   Assembler::AVX_512bit);\n+      __ evmovdqub(xmm3, k1, Address(condensed, 144), false,\n+                   Assembler::AVX_512bit);\n+\n+      __ evpermb(xmm4, k0, xmm20, xmm0, false, Assembler::AVX_512bit);\n+      __ evpermb(xmm5, k0, xmm20, xmm1, false, Assembler::AVX_512bit);\n+      __ evpermb(xmm6, k0, xmm20, xmm2, false, Assembler::AVX_512bit);\n+      __ evpermb(xmm7, k0, xmm20, xmm3, false, Assembler::AVX_512bit);\n+\n+      __ evpsrlvw(xmm4, xmm4, xmm21, Assembler::AVX_512bit);\n+      __ evpsrlvw(xmm5, xmm5, xmm21, Assembler::AVX_512bit);\n+      __ evpsrlvw(xmm6, xmm6, xmm21, Assembler::AVX_512bit);\n+      __ evpsrlvw(xmm7, xmm7, xmm21, Assembler::AVX_512bit);\n+\n+      __ evpandq(xmm0, xmm22, xmm4, Assembler::AVX_512bit);\n+      __ evpandq(xmm1, xmm22, xmm5, Assembler::AVX_512bit);\n+      __ evpandq(xmm2, xmm22, xmm6, Assembler::AVX_512bit);\n+      __ evpandq(xmm3, xmm22, xmm7, Assembler::AVX_512bit);\n+\n+      store4regs(parsed, 0, xmm0_3, _masm);\n+\n+      __ addptr(condensed, 192);\n+      __ addptr(parsed, 256);\n+      __ subl(parsedLength, 128);\n+      __ jcc(Assembler::greater, VBMILoop);\n+\n+    __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n+    __ mov64(rax, 0); \/\/ return 0\n+    __ ret(0);\n+\n+    return start;\n+  }\n+\n","filename":"src\/hotspot\/cpu\/x86\/stubGenerator_x86_64_kyber.cpp","additions":90,"deletions":2,"binary":false,"changes":92,"status":"modified"}]}