{"files":[{"patch":"@@ -16021,1 +16021,1 @@\n-    __ fast_lock_lightweight($object$$Register, $tmp$$Register, $tmp2$$Register, $tmp3$$Register);\n+    __ fast_lock_lightweight($object$$Register, $box$$Register, $tmp$$Register, $tmp2$$Register, $tmp3$$Register);\n@@ -16037,1 +16037,1 @@\n-    __ fast_unlock_lightweight($object$$Register, $tmp$$Register, $tmp2$$Register, $tmp3$$Register);\n+    __ fast_unlock_lightweight($object$$Register, $box$$Register, $tmp$$Register, $tmp2$$Register, $tmp3$$Register);\n","filename":"src\/hotspot\/cpu\/aarch64\/aarch64.ad","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -84,1 +84,1 @@\n-    lightweight_lock(obj, hdr, temp, rscratch2, slow_case);\n+    lightweight_lock(disp_hdr, obj, hdr, temp, rscratch2, slow_case);\n","filename":"src\/hotspot\/cpu\/aarch64\/c1_MacroAssembler_aarch64.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -227,1 +227,1 @@\n-void C2_MacroAssembler::fast_lock_lightweight(Register obj, Register t1,\n+void C2_MacroAssembler::fast_lock_lightweight(Register obj, Register box, Register t1,\n@@ -230,1 +230,1 @@\n-  assert_different_registers(obj, t1, t2, t3);\n+  assert_different_registers(obj, box, t1, t2, t3);\n@@ -239,0 +239,5 @@\n+  if (UseObjectMonitorTable) {\n+    \/\/ Clear cache in case fast locking succeeds.\n+    str(zr, Address(box, BasicLock::object_monitor_cache_offset_in_bytes()));\n+  }\n+\n@@ -247,0 +252,1 @@\n+  const Register t3_t = t3;\n@@ -254,1 +260,0 @@\n-    const Register t3_t = t3;\n@@ -292,3 +297,38 @@\n-    \/\/ mark contains the tagged ObjectMonitor*.\n-    const Register t1_tagged_monitor = t1_mark;\n-    const uintptr_t monitor_tag = markWord::monitor_value;\n+    const Register t1_monitor = t1;\n+\n+    if (!UseObjectMonitorTable) {\n+      assert(t1_monitor == t1_mark, \"should be the same here\");\n+    } else {\n+      Label monitor_found;\n+\n+      \/\/ Load cache address\n+      lea(t3_t, Address(rthread, JavaThread::om_cache_oops_offset()));\n+\n+      const int num_unrolled = 2;\n+      for (int i = 0; i < num_unrolled; i++) {\n+        ldr(t1, Address(t3_t));\n+        cmp(obj, t1);\n+        br(Assembler::EQ, monitor_found);\n+        increment(t3_t, in_bytes(OMCache::oop_to_oop_difference()));\n+      }\n+\n+      Label loop;\n+\n+      \/\/ Search for obj in cache.\n+      bind(loop);\n+\n+      \/\/ Check for match.\n+      ldr(t1, Address(t3_t));\n+      cmp(obj, t1);\n+      br(Assembler::EQ, monitor_found);\n+\n+      \/\/ Search until null encountered, guaranteed _null_sentinel at end.\n+      increment(t3_t, in_bytes(OMCache::oop_to_oop_difference()));\n+      cbnz(t1, loop);\n+      \/\/ Cache Miss, NE set from cmp above, cbnz does not set flags\n+      b(slow_path);\n+\n+      bind(monitor_found);\n+      ldr(t1_monitor, Address(t3_t, OMCache::oop_to_monitor_difference()));\n+    }\n+\n@@ -297,0 +337,5 @@\n+    const ByteSize monitor_tag = in_ByteSize(UseObjectMonitorTable ? 0 : checked_cast<int>(markWord::monitor_value));\n+    const Address owner_address(t1_monitor, ObjectMonitor::owner_offset() - monitor_tag);\n+    const Address recursions_address(t1_monitor, ObjectMonitor::recursions_offset() - monitor_tag);\n+\n+    Label monitor_locked;\n@@ -299,1 +344,1 @@\n-    lea(t2_owner_addr, Address(t1_tagged_monitor, (in_bytes(ObjectMonitor::owner_offset()) - monitor_tag)));\n+    lea(t2_owner_addr, owner_address);\n@@ -304,1 +349,1 @@\n-    br(Assembler::EQ, locked);\n+    br(Assembler::EQ, monitor_locked);\n@@ -311,1 +356,6 @@\n-    increment(Address(t1_tagged_monitor, in_bytes(ObjectMonitor::recursions_offset()) - monitor_tag), 1);\n+    increment(recursions_address, 1);\n+\n+    bind(monitor_locked);\n+    if (UseObjectMonitorTable) {\n+      str(t1_monitor, Address(box, BasicLock::object_monitor_cache_offset_in_bytes()));\n+    }\n@@ -334,2 +384,2 @@\n-void C2_MacroAssembler::fast_unlock_lightweight(Register obj, Register t1, Register t2,\n-                                                Register t3) {\n+void C2_MacroAssembler::fast_unlock_lightweight(Register obj, Register box, Register t1,\n+                                                Register t2, Register t3) {\n@@ -337,1 +387,1 @@\n-  assert_different_registers(obj, t1, t2, t3);\n+  assert_different_registers(obj, box, t1, t2, t3);\n@@ -340,1 +390,1 @@\n-  Label inflated, inflated_load_monitor;\n+  Label inflated, inflated_load_mark;\n@@ -352,0 +402,2 @@\n+    Label push_and_slow_path;\n+\n@@ -358,1 +410,1 @@\n-    br(Assembler::NE, inflated_load_monitor);\n+    br(Assembler::NE, inflated_load_mark);\n@@ -375,1 +427,4 @@\n-    tbnz(t1_mark, exact_log2(markWord::monitor_value), inflated);\n+    \/\/ Because we got here by popping (meaning we pushed in locked)\n+    \/\/ there will be no monitor in the box. So we need to push back the obj\n+    \/\/ so that the runtime can fix any potential anonymous owner.\n+    tbnz(t1_mark, exact_log2(markWord::monitor_value), UseObjectMonitorTable ? push_and_slow_path : inflated);\n@@ -384,0 +439,1 @@\n+    bind(push_and_slow_path);\n@@ -394,1 +450,1 @@\n-    bind(inflated_load_monitor);\n+    bind(inflated_load_mark);\n@@ -415,3 +471,4 @@\n-    \/\/ mark contains the tagged ObjectMonitor*.\n-    const Register t1_monitor = t1_mark;\n-    const uintptr_t monitor_tag = markWord::monitor_value;\n+    const Register t1_monitor = t1;\n+\n+    if (!UseObjectMonitorTable) {\n+      assert(t1_monitor == t1_mark, \"should be the same here\");\n@@ -419,2 +476,8 @@\n-    \/\/ Untag the monitor.\n-    sub(t1_monitor, t1_mark, monitor_tag);\n+      \/\/ Untag the monitor.\n+      add(t1_monitor, t1_mark, -(int)markWord::monitor_value);\n+    } else {\n+      ldr(t1_monitor, Address(box, BasicLock::object_monitor_cache_offset_in_bytes()));\n+      \/\/ null check with Flags == NE, no valid pointer below alignof(ObjectMonitor*)\n+      cmp(t1_monitor, checked_cast<uint8_t>(alignof(ObjectMonitor*)));\n+      br(Assembler::LO, slow_path);\n+    }\n","filename":"src\/hotspot\/cpu\/aarch64\/c2_MacroAssembler_aarch64.cpp","additions":84,"deletions":21,"binary":false,"changes":105,"status":"modified"},{"patch":"@@ -42,2 +42,2 @@\n-  void fast_lock_lightweight(Register object, Register t1, Register t2, Register t3);\n-  void fast_unlock_lightweight(Register object, Register t1, Register t2, Register t3);\n+  void fast_lock_lightweight(Register object, Register box, Register t1, Register t2, Register t3);\n+  void fast_unlock_lightweight(Register object, Register box, Register t1, Register t2, Register t3);\n","filename":"src\/hotspot\/cpu\/aarch64\/c2_MacroAssembler_aarch64.hpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -699,1 +699,1 @@\n-      lightweight_lock(obj_reg, tmp, tmp2, tmp3, slow_case);\n+      lightweight_lock(lock_reg, obj_reg, tmp, tmp2, tmp3, slow_case);\n@@ -755,9 +755,3 @@\n-    if (LockingMode == LM_LIGHTWEIGHT) {\n-      call_VM(noreg,\n-              CAST_FROM_FN_PTR(address, InterpreterRuntime::monitorenter_obj),\n-              obj_reg);\n-    } else {\n-      call_VM(noreg,\n-              CAST_FROM_FN_PTR(address, InterpreterRuntime::monitorenter),\n-              lock_reg);\n-    }\n+    call_VM(noreg,\n+            CAST_FROM_FN_PTR(address, InterpreterRuntime::monitorenter),\n+            lock_reg);\n","filename":"src\/hotspot\/cpu\/aarch64\/interp_masm_aarch64.cpp","additions":4,"deletions":10,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -6753,1 +6753,1 @@\n-void MacroAssembler::lightweight_lock(Register obj, Register t1, Register t2, Register t3, Label& slow) {\n+void MacroAssembler::lightweight_lock(Register basic_lock, Register obj, Register t1, Register t2, Register t3, Label& slow) {\n@@ -6755,1 +6755,1 @@\n-  assert_different_registers(obj, t1, t2, t3, rscratch1);\n+  assert_different_registers(basic_lock, obj, t1, t2, t3, rscratch1);\n@@ -6766,0 +6766,5 @@\n+  if (UseObjectMonitorTable) {\n+    \/\/ Clear cache in case fast locking succeeds.\n+    str(zr, Address(basic_lock, BasicObjectLock::lock_offset() + in_ByteSize((BasicLock::object_monitor_cache_offset_in_bytes()))));\n+  }\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/macroAssembler_aarch64.cpp","additions":7,"deletions":2,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -1642,1 +1642,1 @@\n-  void lightweight_lock(Register obj, Register t1, Register t2, Register t3, Label& slow);\n+  void lightweight_lock(Register basic_lock, Register obj, Register t1, Register t2, Register t3, Label& slow);\n","filename":"src\/hotspot\/cpu\/aarch64\/macroAssembler_aarch64.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -1814,1 +1814,1 @@\n-      __ lightweight_lock(obj_reg, swap_reg, tmp, lock_tmp, slow_path_lock);\n+      __ lightweight_lock(lock_reg, obj_reg, swap_reg, tmp, lock_tmp, slow_path_lock);\n","filename":"src\/hotspot\/cpu\/aarch64\/sharedRuntime_aarch64.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -988,9 +988,1 @@\n-    if (LockingMode == LM_LIGHTWEIGHT) {\n-      \/\/ Pass oop, not lock, in fast lock case. call_VM wants R1 though.\n-      push(R1);\n-      mov(R1, Robj);\n-      call_VM(noreg, CAST_FROM_FN_PTR(address, InterpreterRuntime::monitorenter_obj), R1);\n-      pop(R1);\n-    } else {\n-      call_VM(noreg, CAST_FROM_FN_PTR(address, InterpreterRuntime::monitorenter), Rlock);\n-    }\n+    call_VM(noreg, CAST_FROM_FN_PTR(address, InterpreterRuntime::monitorenter), Rlock);\n","filename":"src\/hotspot\/cpu\/arm\/interp_masm_arm.cpp","additions":1,"deletions":9,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -1046,5 +1046,1 @@\n-    if (LockingMode == LM_LIGHTWEIGHT) {\n-      call_VM(noreg, CAST_FROM_FN_PTR(address, InterpreterRuntime::monitorenter_obj), object);\n-    } else {\n-      call_VM(noreg, CAST_FROM_FN_PTR(address, InterpreterRuntime::monitorenter), monitor);\n-    }\n+    call_VM(noreg, CAST_FROM_FN_PTR(address, InterpreterRuntime::monitorenter), monitor);\n","filename":"src\/hotspot\/cpu\/ppc\/interp_masm_ppc_64.cpp","additions":1,"deletions":5,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -2807,26 +2807,33 @@\n-    \/\/ mark contains the tagged ObjectMonitor*.\n-    const Register tagged_monitor = mark;\n-    const uintptr_t monitor_tag = markWord::monitor_value;\n-    const Register owner_addr = tmp2;\n-\n-    \/\/ Compute owner address.\n-    addi(owner_addr, tagged_monitor, in_bytes(ObjectMonitor::owner_offset()) - monitor_tag);\n-\n-    \/\/ CAS owner (null => current thread).\n-    cmpxchgd(\/*flag=*\/flag,\n-            \/*current_value=*\/t,\n-            \/*compare_value=*\/(intptr_t)0,\n-            \/*exchange_value=*\/R16_thread,\n-            \/*where=*\/owner_addr,\n-            MacroAssembler::MemBarRel | MacroAssembler::MemBarAcq,\n-            MacroAssembler::cmpxchgx_hint_acquire_lock());\n-    beq(flag, locked);\n-\n-    \/\/ Check if recursive.\n-    cmpd(flag, t, R16_thread);\n-    bne(flag, slow_path);\n-\n-    \/\/ Recursive.\n-    ld(tmp1, in_bytes(ObjectMonitor::recursions_offset() - ObjectMonitor::owner_offset()), owner_addr);\n-    addi(tmp1, tmp1, 1);\n-    std(tmp1, in_bytes(ObjectMonitor::recursions_offset() - ObjectMonitor::owner_offset()), owner_addr);\n+    if (!UseObjectMonitorTable) {\n+      \/\/ mark contains the tagged ObjectMonitor*.\n+      const Register tagged_monitor = mark;\n+      const uintptr_t monitor_tag = markWord::monitor_value;\n+      const Register owner_addr = tmp2;\n+\n+      \/\/ Compute owner address.\n+      addi(owner_addr, tagged_monitor, in_bytes(ObjectMonitor::owner_offset()) - monitor_tag);\n+\n+      \/\/ CAS owner (null => current thread).\n+      cmpxchgd(\/*flag=*\/flag,\n+              \/*current_value=*\/t,\n+              \/*compare_value=*\/(intptr_t)0,\n+              \/*exchange_value=*\/R16_thread,\n+              \/*where=*\/owner_addr,\n+              MacroAssembler::MemBarRel | MacroAssembler::MemBarAcq,\n+              MacroAssembler::cmpxchgx_hint_acquire_lock());\n+      beq(flag, locked);\n+\n+      \/\/ Check if recursive.\n+      cmpd(flag, t, R16_thread);\n+      bne(flag, slow_path);\n+\n+      \/\/ Recursive.\n+      ld(tmp1, in_bytes(ObjectMonitor::recursions_offset() - ObjectMonitor::owner_offset()), owner_addr);\n+      addi(tmp1, tmp1, 1);\n+      std(tmp1, in_bytes(ObjectMonitor::recursions_offset() - ObjectMonitor::owner_offset()), owner_addr);\n+    } else {\n+      \/\/ OMCache lookup not supported yet. Take the slowpath.\n+      \/\/ Set flag to NE\n+      crxor(flag, Assembler::equal, flag, Assembler::equal);\n+      b(slow_path);\n+    }\n@@ -2946,43 +2953,50 @@\n-    \/\/ mark contains the tagged ObjectMonitor*.\n-    const Register monitor = mark;\n-    const uintptr_t monitor_tag = markWord::monitor_value;\n-\n-    \/\/ Untag the monitor.\n-    subi(monitor, mark, monitor_tag);\n-\n-    const Register recursions = tmp2;\n-    Label not_recursive;\n-\n-    \/\/ Check if recursive.\n-    ld(recursions, in_bytes(ObjectMonitor::recursions_offset()), monitor);\n-    addic_(recursions, recursions, -1);\n-    blt(CCR0, not_recursive);\n-\n-    \/\/ Recursive unlock.\n-    std(recursions, in_bytes(ObjectMonitor::recursions_offset()), monitor);\n-    crorc(CCR0, Assembler::equal, CCR0, Assembler::equal);\n-    b(unlocked);\n-\n-    bind(not_recursive);\n-\n-    Label release_;\n-    const Register t2 = tmp2;\n-\n-    \/\/ Check if the entry lists are empty.\n-    ld(t, in_bytes(ObjectMonitor::EntryList_offset()), monitor);\n-    ld(t2, in_bytes(ObjectMonitor::cxq_offset()), monitor);\n-    orr(t, t, t2);\n-    cmpdi(flag, t, 0);\n-    beq(flag, release_);\n-\n-    \/\/ The owner may be anonymous and we removed the last obj entry in\n-    \/\/ the lock-stack. This loses the information about the owner.\n-    \/\/ Write the thread to the owner field so the runtime knows the owner.\n-    std(R16_thread, in_bytes(ObjectMonitor::owner_offset()), monitor);\n-    b(slow_path);\n-\n-    bind(release_);\n-    \/\/ Set owner to null.\n-    release();\n-    \/\/ t contains 0\n-    std(t, in_bytes(ObjectMonitor::owner_offset()), monitor);\n+    if (!UseObjectMonitorTable) {\n+      \/\/ mark contains the tagged ObjectMonitor*.\n+      const Register monitor = mark;\n+      const uintptr_t monitor_tag = markWord::monitor_value;\n+\n+      \/\/ Untag the monitor.\n+      subi(monitor, mark, monitor_tag);\n+\n+      const Register recursions = tmp2;\n+      Label not_recursive;\n+\n+      \/\/ Check if recursive.\n+      ld(recursions, in_bytes(ObjectMonitor::recursions_offset()), monitor);\n+      addic_(recursions, recursions, -1);\n+      blt(CCR0, not_recursive);\n+\n+      \/\/ Recursive unlock.\n+      std(recursions, in_bytes(ObjectMonitor::recursions_offset()), monitor);\n+      crorc(CCR0, Assembler::equal, CCR0, Assembler::equal);\n+      b(unlocked);\n+\n+      bind(not_recursive);\n+\n+      Label release_;\n+      const Register t2 = tmp2;\n+\n+      \/\/ Check if the entry lists are empty.\n+      ld(t, in_bytes(ObjectMonitor::EntryList_offset()), monitor);\n+      ld(t2, in_bytes(ObjectMonitor::cxq_offset()), monitor);\n+      orr(t, t, t2);\n+      cmpdi(flag, t, 0);\n+      beq(flag, release_);\n+\n+      \/\/ The owner may be anonymous and we removed the last obj entry in\n+      \/\/ the lock-stack. This loses the information about the owner.\n+      \/\/ Write the thread to the owner field so the runtime knows the owner.\n+      std(R16_thread, in_bytes(ObjectMonitor::owner_offset()), monitor);\n+      b(slow_path);\n+\n+      bind(release_);\n+      \/\/ Set owner to null.\n+      release();\n+      \/\/ t contains 0\n+      std(t, in_bytes(ObjectMonitor::owner_offset()), monitor);\n+    } else {\n+      \/\/ OMCache lookup not supported yet. Take the slowpath.\n+      \/\/ Set flag to NE\n+      crxor(flag, Assembler::equal, flag, Assembler::equal);\n+      b(slow_path);\n+    }\n","filename":"src\/hotspot\/cpu\/ppc\/macroAssembler_ppc.cpp","additions":83,"deletions":69,"binary":false,"changes":152,"status":"modified"},{"patch":"@@ -326,19 +326,24 @@\n-    \/\/ mark contains the tagged ObjectMonitor*.\n-    const Register tmp1_tagged_monitor = tmp1_mark;\n-    const uintptr_t monitor_tag = markWord::monitor_value;\n-    const Register tmp2_owner_addr = tmp2;\n-    const Register tmp3_owner = tmp3;\n-\n-    \/\/ Compute owner address.\n-    la(tmp2_owner_addr, Address(tmp1_tagged_monitor, (in_bytes(ObjectMonitor::owner_offset()) - monitor_tag)));\n-\n-    \/\/ CAS owner (null => current thread).\n-    cmpxchg(\/*addr*\/ tmp2_owner_addr, \/*expected*\/ zr, \/*new*\/ xthread, Assembler::int64,\n-            \/*acquire*\/ Assembler::aq, \/*release*\/ Assembler::relaxed, \/*result*\/ tmp3_owner);\n-    beqz(tmp3_owner, locked);\n-\n-    \/\/ Check if recursive.\n-    bne(tmp3_owner, xthread, slow_path);\n-\n-    \/\/ Recursive.\n-    increment(Address(tmp1_tagged_monitor, in_bytes(ObjectMonitor::recursions_offset()) - monitor_tag), 1, tmp2, tmp3);\n+    if (!UseObjectMonitorTable) {\n+      \/\/ mark contains the tagged ObjectMonitor*.\n+      const Register tmp1_tagged_monitor = tmp1_mark;\n+      const uintptr_t monitor_tag = markWord::monitor_value;\n+      const Register tmp2_owner_addr = tmp2;\n+      const Register tmp3_owner = tmp3;\n+\n+      \/\/ Compute owner address.\n+      la(tmp2_owner_addr, Address(tmp1_tagged_monitor, (in_bytes(ObjectMonitor::owner_offset()) - monitor_tag)));\n+\n+      \/\/ CAS owner (null => current thread).\n+      cmpxchg(\/*addr*\/ tmp2_owner_addr, \/*expected*\/ zr, \/*new*\/ xthread, Assembler::int64,\n+              \/*acquire*\/ Assembler::aq, \/*release*\/ Assembler::relaxed, \/*result*\/ tmp3_owner);\n+      beqz(tmp3_owner, locked);\n+\n+      \/\/ Check if recursive.\n+      bne(tmp3_owner, xthread, slow_path);\n+\n+      \/\/ Recursive.\n+      increment(Address(tmp1_tagged_monitor, in_bytes(ObjectMonitor::recursions_offset()) - monitor_tag), 1, tmp2, tmp3);\n+    } else {\n+      \/\/ OMCache lookup not supported yet. Take the slowpath.\n+      j(slow_path);\n+    }\n@@ -456,43 +461,48 @@\n-    \/\/ mark contains the tagged ObjectMonitor*.\n-    const Register tmp1_monitor = tmp1_mark;\n-    const uintptr_t monitor_tag = markWord::monitor_value;\n-\n-    \/\/ Untag the monitor.\n-    sub(tmp1_monitor, tmp1_mark, monitor_tag);\n-\n-    const Register tmp2_recursions = tmp2;\n-    Label not_recursive;\n-\n-    \/\/ Check if recursive.\n-    ld(tmp2_recursions, Address(tmp1_monitor, ObjectMonitor::recursions_offset()));\n-    beqz(tmp2_recursions, not_recursive);\n-\n-    \/\/ Recursive unlock.\n-    addi(tmp2_recursions, tmp2_recursions, -1);\n-    sd(tmp2_recursions, Address(tmp1_monitor, ObjectMonitor::recursions_offset()));\n-    j(unlocked);\n-\n-    bind(not_recursive);\n-\n-    Label release;\n-    const Register tmp2_owner_addr = tmp2;\n-\n-    \/\/ Compute owner address.\n-    la(tmp2_owner_addr, Address(tmp1_monitor, ObjectMonitor::owner_offset()));\n-\n-    \/\/ Check if the entry lists are empty.\n-    ld(t0, Address(tmp1_monitor, ObjectMonitor::EntryList_offset()));\n-    ld(tmp3_t, Address(tmp1_monitor, ObjectMonitor::cxq_offset()));\n-    orr(t0, t0, tmp3_t);\n-    beqz(t0, release);\n-\n-    \/\/ The owner may be anonymous and we removed the last obj entry in\n-    \/\/ the lock-stack. This loses the information about the owner.\n-    \/\/ Write the thread to the owner field so the runtime knows the owner.\n-    sd(xthread, Address(tmp2_owner_addr));\n-    j(slow_path);\n-\n-    bind(release);\n-    \/\/ Set owner to null.\n-    membar(MacroAssembler::LoadStore | MacroAssembler::StoreStore);\n-    sd(zr, Address(tmp2_owner_addr));\n+    if (!UseObjectMonitorTable) {\n+      \/\/ mark contains the tagged ObjectMonitor*.\n+      const Register tmp1_monitor = tmp1_mark;\n+      const uintptr_t monitor_tag = markWord::monitor_value;\n+\n+      \/\/ Untag the monitor.\n+      sub(tmp1_monitor, tmp1_mark, monitor_tag);\n+\n+      const Register tmp2_recursions = tmp2;\n+      Label not_recursive;\n+\n+      \/\/ Check if recursive.\n+      ld(tmp2_recursions, Address(tmp1_monitor, ObjectMonitor::recursions_offset()));\n+      beqz(tmp2_recursions, not_recursive);\n+\n+      \/\/ Recursive unlock.\n+      addi(tmp2_recursions, tmp2_recursions, -1);\n+      sd(tmp2_recursions, Address(tmp1_monitor, ObjectMonitor::recursions_offset()));\n+      j(unlocked);\n+\n+      bind(not_recursive);\n+\n+      Label release;\n+      const Register tmp2_owner_addr = tmp2;\n+\n+      \/\/ Compute owner address.\n+      la(tmp2_owner_addr, Address(tmp1_monitor, ObjectMonitor::owner_offset()));\n+\n+      \/\/ Check if the entry lists are empty.\n+      ld(t0, Address(tmp1_monitor, ObjectMonitor::EntryList_offset()));\n+      ld(tmp3_t, Address(tmp1_monitor, ObjectMonitor::cxq_offset()));\n+      orr(t0, t0, tmp3_t);\n+      beqz(t0, release);\n+\n+      \/\/ The owner may be anonymous and we removed the last obj entry in\n+      \/\/ the lock-stack. This loses the information about the owner.\n+      \/\/ Write the thread to the owner field so the runtime knows the owner.\n+      sd(xthread, Address(tmp2_owner_addr));\n+      j(slow_path);\n+\n+      bind(release);\n+      \/\/ Set owner to null.\n+      membar(MacroAssembler::LoadStore | MacroAssembler::StoreStore);\n+      sd(zr, Address(tmp2_owner_addr));\n+    } else {\n+      \/\/ OMCache lookup not supported yet. Take the slowpath.\n+      j(slow_path);\n+    }\n","filename":"src\/hotspot\/cpu\/riscv\/c2_MacroAssembler_riscv.cpp","additions":72,"deletions":62,"binary":false,"changes":134,"status":"modified"},{"patch":"@@ -795,9 +795,3 @@\n-    if (LockingMode == LM_LIGHTWEIGHT) {\n-      call_VM(noreg,\n-              CAST_FROM_FN_PTR(address, InterpreterRuntime::monitorenter_obj),\n-              obj_reg);\n-    } else {\n-      call_VM(noreg,\n-              CAST_FROM_FN_PTR(address, InterpreterRuntime::monitorenter),\n-              lock_reg);\n-    }\n+    call_VM(noreg,\n+            CAST_FROM_FN_PTR(address, InterpreterRuntime::monitorenter),\n+            lock_reg);\n","filename":"src\/hotspot\/cpu\/riscv\/interp_masm_riscv.cpp","additions":3,"deletions":9,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -1075,10 +1075,3 @@\n-  if (LockingMode == LM_LIGHTWEIGHT) {\n-    \/\/ for lightweight locking we need to use monitorenter_obj, see interpreterRuntime.cpp\n-    call_VM(noreg,\n-            CAST_FROM_FN_PTR(address, InterpreterRuntime::monitorenter_obj),\n-            object);\n-  } else {\n-    call_VM(noreg,\n-            CAST_FROM_FN_PTR(address, InterpreterRuntime::monitorenter),\n-            monitor);\n-  }\n+  call_VM(noreg,\n+          CAST_FROM_FN_PTR(address, InterpreterRuntime::monitorenter),\n+          monitor);\n","filename":"src\/hotspot\/cpu\/s390\/interp_masm_s390.cpp","additions":3,"deletions":10,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -6221,20 +6221,27 @@\n-    \/\/ mark contains the tagged ObjectMonitor*.\n-    const Register tagged_monitor = mark;\n-    const Register zero           = tmp2;\n-\n-    \/\/ Try to CAS m->owner from null to current thread.\n-    \/\/ If m->owner is null, then csg succeeds and sets m->owner=THREAD and CR=EQ.\n-    \/\/ Otherwise, register zero is filled with the current owner.\n-    z_lghi(zero, 0);\n-    z_csg(zero, Z_thread, OM_OFFSET_NO_MONITOR_VALUE_TAG(owner), tagged_monitor);\n-    z_bre(locked);\n-\n-    \/\/ Check if recursive.\n-    z_cgr(Z_thread, zero); \/\/ zero contains the owner from z_csg instruction\n-    z_brne(slow_path);\n-\n-    \/\/ Recursive\n-    z_agsi(Address(tagged_monitor, OM_OFFSET_NO_MONITOR_VALUE_TAG(recursions)), 1ll);\n-    z_cgr(zero, zero);\n-    \/\/ z_bru(locked);\n-    \/\/ Uncomment above line in the future, for now jump address is right next to us.\n+    if (!UseObjectMonitorTable) {\n+      \/\/ mark contains the tagged ObjectMonitor*.\n+      const Register tagged_monitor = mark;\n+      const Register zero           = tmp2;\n+\n+      \/\/ Try to CAS m->owner from null to current thread.\n+      \/\/ If m->owner is null, then csg succeeds and sets m->owner=THREAD and CR=EQ.\n+      \/\/ Otherwise, register zero is filled with the current owner.\n+      z_lghi(zero, 0);\n+      z_csg(zero, Z_thread, OM_OFFSET_NO_MONITOR_VALUE_TAG(owner), tagged_monitor);\n+      z_bre(locked);\n+\n+      \/\/ Check if recursive.\n+      z_cgr(Z_thread, zero); \/\/ zero contains the owner from z_csg instruction\n+      z_brne(slow_path);\n+\n+      \/\/ Recursive\n+      z_agsi(Address(tagged_monitor, OM_OFFSET_NO_MONITOR_VALUE_TAG(recursions)), 1ll);\n+      z_cgr(zero, zero);\n+      \/\/ z_bru(locked);\n+      \/\/ Uncomment above line in the future, for now jump address is right next to us.\n+    } else {\n+      \/\/ OMCache lookup not supported yet. Take the slowpath.\n+      \/\/ Set flag to NE\n+      z_ltgr(obj, obj);\n+      z_bru(slow_path);\n+    }\n@@ -6367,2 +6374,3 @@\n-    \/\/ mark contains the tagged ObjectMonitor*.\n-    const Register monitor = mark;\n+    if (!UseObjectMonitorTable) {\n+      \/\/ mark contains the tagged ObjectMonitor*.\n+      const Register monitor = mark;\n@@ -6370,2 +6378,2 @@\n-    NearLabel not_recursive;\n-    const Register recursions = tmp2;\n+      NearLabel not_recursive;\n+      const Register recursions = tmp2;\n@@ -6373,3 +6381,3 @@\n-    \/\/ Check if recursive.\n-    load_and_test_long(recursions, Address(monitor, OM_OFFSET_NO_MONITOR_VALUE_TAG(recursions)));\n-    z_bre(not_recursive); \/\/ if 0 then jump, it's not recursive locking\n+      \/\/ Check if recursive.\n+      load_and_test_long(recursions, Address(monitor, OM_OFFSET_NO_MONITOR_VALUE_TAG(recursions)));\n+      z_bre(not_recursive); \/\/ if 0 then jump, it's not recursive locking\n@@ -6377,4 +6385,4 @@\n-    \/\/ Recursive unlock\n-    z_agsi(Address(monitor, OM_OFFSET_NO_MONITOR_VALUE_TAG(recursions)), -1ll);\n-    z_cgr(monitor, monitor); \/\/ set the CC to EQUAL\n-    z_bru(unlocked);\n+      \/\/ Recursive unlock\n+      z_agsi(Address(monitor, OM_OFFSET_NO_MONITOR_VALUE_TAG(recursions)), -1ll);\n+      z_cgr(monitor, monitor); \/\/ set the CC to EQUAL\n+      z_bru(unlocked);\n@@ -6382,1 +6390,1 @@\n-    bind(not_recursive);\n+      bind(not_recursive);\n@@ -6384,6 +6392,6 @@\n-    NearLabel not_ok;\n-    \/\/ Check if the entry lists are empty.\n-    load_and_test_long(tmp2, Address(monitor, OM_OFFSET_NO_MONITOR_VALUE_TAG(EntryList)));\n-    z_brne(not_ok);\n-    load_and_test_long(tmp2, Address(monitor, OM_OFFSET_NO_MONITOR_VALUE_TAG(cxq)));\n-    z_brne(not_ok);\n+      NearLabel not_ok;\n+      \/\/ Check if the entry lists are empty.\n+      load_and_test_long(tmp2, Address(monitor, OM_OFFSET_NO_MONITOR_VALUE_TAG(EntryList)));\n+      z_brne(not_ok);\n+      load_and_test_long(tmp2, Address(monitor, OM_OFFSET_NO_MONITOR_VALUE_TAG(cxq)));\n+      z_brne(not_ok);\n@@ -6391,2 +6399,2 @@\n-    z_release();\n-    z_stg(tmp2 \/*=0*\/, OM_OFFSET_NO_MONITOR_VALUE_TAG(owner), monitor);\n+      z_release();\n+      z_stg(tmp2 \/*=0*\/, OM_OFFSET_NO_MONITOR_VALUE_TAG(owner), monitor);\n@@ -6394,1 +6402,1 @@\n-    z_bru(unlocked); \/\/ CC = EQ here\n+      z_bru(unlocked); \/\/ CC = EQ here\n@@ -6396,1 +6404,1 @@\n-    bind(not_ok);\n+      bind(not_ok);\n@@ -6398,5 +6406,11 @@\n-    \/\/ The owner may be anonymous, and we removed the last obj entry in\n-    \/\/ the lock-stack. This loses the information about the owner.\n-    \/\/ Write the thread to the owner field so the runtime knows the owner.\n-    z_stg(Z_thread, OM_OFFSET_NO_MONITOR_VALUE_TAG(owner), monitor);\n-    z_bru(slow_path); \/\/ CC = NE here\n+      \/\/ The owner may be anonymous, and we removed the last obj entry in\n+      \/\/ the lock-stack. This loses the information about the owner.\n+      \/\/ Write the thread to the owner field so the runtime knows the owner.\n+      z_stg(Z_thread, OM_OFFSET_NO_MONITOR_VALUE_TAG(owner), monitor);\n+      z_bru(slow_path); \/\/ CC = NE here\n+    } else {\n+      \/\/ OMCache lookup not supported yet. Take the slowpath.\n+      \/\/ Set flag to NE\n+      z_ltgr(obj, obj);\n+      z_bru(slow_path);\n+    }\n","filename":"src\/hotspot\/cpu\/s390\/macroAssembler_s390.cpp","additions":61,"deletions":47,"binary":false,"changes":108,"status":"modified"},{"patch":"@@ -69,0 +69,1 @@\n+    lightweight_lock(disp_hdr, obj, hdr, thread, tmp, slow_case);\n@@ -70,2 +71,4 @@\n-    const Register thread = disp_hdr;\n-    get_thread(thread);\n+    \/\/ Implicit null check.\n+    movptr(hdr, Address(obj, oopDesc::mark_offset_in_bytes()));\n+    \/\/ Lacking registers and thread on x86_32. Always take slow path.\n+    jmp(slow_case);\n@@ -73,1 +76,0 @@\n-    lightweight_lock(obj, hdr, thread, tmp, slow_case);\n@@ -142,4 +144,2 @@\n-    \/\/ This relies on the implementation of lightweight_unlock being able to handle\n-    \/\/ that the reg_rax and thread Register parameters may alias each other.\n-    get_thread(disp_hdr);\n-    lightweight_unlock(obj, disp_hdr, disp_hdr, hdr, slow_case);\n+    \/\/ Lacking registers and thread on x86_32. Always take slow path.\n+    jmp(slow_case);\n","filename":"src\/hotspot\/cpu\/x86\/c1_MacroAssembler_x86.cpp","additions":7,"deletions":7,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -99,0 +99,1 @@\n+    __ bind(_slow_path);\n@@ -115,0 +116,4 @@\n+    const ByteSize monitor_tag = in_ByteSize(UseObjectMonitorTable ? 0 : checked_cast<int>(markWord::monitor_value));\n+    const Address succ_address(monitor, ObjectMonitor::succ_offset() - monitor_tag);\n+    const Address owner_address(monitor, ObjectMonitor::owner_offset() - monitor_tag);\n+\n@@ -116,1 +121,1 @@\n-    __ cmpptr(Address(monitor, OM_OFFSET_NO_MONITOR_VALUE_TAG(succ)), NULL_WORD);\n+    __ cmpptr(succ_address, NULL_WORD);\n@@ -120,1 +125,1 @@\n-    __ movptr(Address(monitor, OM_OFFSET_NO_MONITOR_VALUE_TAG(owner)), NULL_WORD);\n+    __ movptr(owner_address, NULL_WORD);\n@@ -127,1 +132,1 @@\n-    __ cmpptr(Address(monitor, OM_OFFSET_NO_MONITOR_VALUE_TAG(succ)), NULL_WORD);\n+    __ cmpptr(succ_address, NULL_WORD);\n@@ -136,1 +141,1 @@\n-    __ lock(); __ cmpxchgptr(_thread, Address(monitor, OM_OFFSET_NO_MONITOR_VALUE_TAG(owner)));\n+    __ lock(); __ cmpxchgptr(_thread, owner_address);\n","filename":"src\/hotspot\/cpu\/x86\/c2_CodeStubs_x86.cpp","additions":9,"deletions":4,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -593,0 +593,5 @@\n+  if (UseObjectMonitorTable) {\n+    \/\/ Clear cache in case fast locking succeeds.\n+    movptr(Address(box, BasicLock::object_monitor_cache_offset_in_bytes()), 0);\n+  }\n+\n@@ -606,1 +611,1 @@\n-    const Register top = box;\n+    const Register top = UseObjectMonitorTable ? rax_reg : box;\n@@ -633,0 +638,4 @@\n+    if (UseObjectMonitorTable) {\n+      \/\/ Need to reload top, clobbered by CAS.\n+      movl(top, Address(thread, JavaThread::lock_stack_top_offset()));\n+    }\n@@ -643,1 +652,44 @@\n-    const Register tagged_monitor = mark;\n+    const Register monitor = t;\n+\n+    if (!UseObjectMonitorTable) {\n+      assert(mark == monitor, \"should be the same here\");\n+    } else {\n+      \/\/ Uses ObjectMonitorTable.  Look for the monitor in the om_cache.\n+      \/\/ Fetch ObjectMonitor* from the cache or take the slow-path.\n+      Label monitor_found;\n+\n+      \/\/ Load cache address\n+      lea(t, Address(thread, JavaThread::om_cache_oops_offset()));\n+\n+      const int num_unrolled = 2;\n+      for (int i = 0; i < num_unrolled; i++) {\n+        cmpptr(obj, Address(t));\n+        jccb(Assembler::equal, monitor_found);\n+        increment(t, in_bytes(OMCache::oop_to_oop_difference()));\n+      }\n+\n+      Label loop;\n+\n+      \/\/ Search for obj in cache.\n+      bind(loop);\n+\n+      \/\/ Check for match.\n+      cmpptr(obj, Address(t));\n+      jccb(Assembler::equal, monitor_found);\n+\n+      \/\/ Search until null encountered, guaranteed _null_sentinel at end.\n+      cmpptr(Address(t), 1);\n+      jcc(Assembler::below, slow_path); \/\/ 0 check, but with ZF=0 when *t == 0\n+      increment(t, in_bytes(OMCache::oop_to_oop_difference()));\n+      jmpb(loop);\n+\n+      \/\/ Cache hit.\n+      bind(monitor_found);\n+      movptr(monitor, Address(t, OMCache::oop_to_monitor_difference()));\n+    }\n+    const ByteSize monitor_tag = in_ByteSize(UseObjectMonitorTable ? 0 : checked_cast<int>(markWord::monitor_value));\n+    const Address recursions_address(monitor, ObjectMonitor::recursions_offset() - monitor_tag);\n+    const Address owner_address(monitor, ObjectMonitor::owner_offset() - monitor_tag);\n+\n+    Label monitor_locked;\n+    \/\/ Lock the monitor.\n@@ -647,2 +699,2 @@\n-    lock(); cmpxchgptr(thread, Address(tagged_monitor, OM_OFFSET_NO_MONITOR_VALUE_TAG(owner)));\n-    jccb(Assembler::equal, locked);\n+    lock(); cmpxchgptr(thread, owner_address);\n+    jccb(Assembler::equal, monitor_locked);\n@@ -655,1 +707,7 @@\n-    increment(Address(tagged_monitor, OM_OFFSET_NO_MONITOR_VALUE_TAG(recursions)));\n+    increment(recursions_address);\n+\n+    bind(monitor_locked);\n+    if (UseObjectMonitorTable) {\n+      \/\/ Cache the monitor for unlock\n+      movptr(Address(box, BasicLock::object_monitor_cache_offset_in_bytes()), monitor);\n+    }\n@@ -697,1 +755,3 @@\n-  const Register top = reg_rax;\n+  const Register monitor = t;\n+  const Register top = UseObjectMonitorTable ? t : reg_rax;\n+  const Register box = reg_rax;\n@@ -709,0 +769,1 @@\n+  Label& slow_path = stub == nullptr ? dummy : stub->slow_path();\n@@ -715,2 +776,4 @@\n-    \/\/ Prefetch mark.\n-    movptr(mark, Address(obj, oopDesc::mark_offset_in_bytes()));\n+    if (!UseObjectMonitorTable) {\n+      \/\/ Prefetch mark.\n+      movptr(mark, Address(obj, oopDesc::mark_offset_in_bytes()));\n+    }\n@@ -733,0 +796,5 @@\n+    if (UseObjectMonitorTable) {\n+      \/\/ Load mark.\n+      movptr(mark, Address(obj, oopDesc::mark_offset_in_bytes()));\n+    }\n+\n@@ -754,0 +822,3 @@\n+    if (UseObjectMonitorTable) {\n+      movptr(mark, Address(obj, oopDesc::mark_offset_in_bytes()));\n+    }\n@@ -761,13 +832,14 @@\n-    \/\/ mark contains the tagged ObjectMonitor*.\n-    const Register monitor = mark;\n-\n-#ifndef _LP64\n-    \/\/ Check if recursive.\n-    xorptr(reg_rax, reg_rax);\n-    orptr(reg_rax, Address(monitor, OM_OFFSET_NO_MONITOR_VALUE_TAG(recursions)));\n-    jcc(Assembler::notZero, check_successor);\n-\n-    \/\/ Check if the entry lists are empty.\n-    movptr(reg_rax, Address(monitor, OM_OFFSET_NO_MONITOR_VALUE_TAG(EntryList)));\n-    orptr(reg_rax, Address(monitor, OM_OFFSET_NO_MONITOR_VALUE_TAG(cxq)));\n-    jcc(Assembler::notZero, check_successor);\n+    if (!UseObjectMonitorTable) {\n+      assert(mark == monitor, \"should be the same here\");\n+    } else {\n+      \/\/ Uses ObjectMonitorTable.  Look for the monitor in our BasicLock on the stack.\n+      movptr(monitor, Address(box, BasicLock::object_monitor_cache_offset_in_bytes()));\n+      \/\/ null check with ZF == 0, no valid pointer below alignof(ObjectMonitor*)\n+      cmpptr(monitor, alignof(ObjectMonitor*));\n+      jcc(Assembler::below, slow_path);\n+    }\n+    const ByteSize monitor_tag = in_ByteSize(UseObjectMonitorTable ? 0 : checked_cast<int>(markWord::monitor_value));\n+    const Address recursions_address{monitor, ObjectMonitor::recursions_offset() - monitor_tag};\n+    const Address cxq_address{monitor, ObjectMonitor::cxq_offset() - monitor_tag};\n+    const Address EntryList_address{monitor, ObjectMonitor::EntryList_offset() - monitor_tag};\n+    const Address owner_address{monitor, ObjectMonitor::owner_offset() - monitor_tag};\n@@ -775,3 +847,0 @@\n-    \/\/ Release lock.\n-    movptr(Address(monitor, OM_OFFSET_NO_MONITOR_VALUE_TAG(owner)), NULL_WORD);\n-#else \/\/ _LP64\n@@ -781,1 +850,1 @@\n-    cmpptr(Address(monitor, OM_OFFSET_NO_MONITOR_VALUE_TAG(recursions)), 0);\n+    cmpptr(recursions_address, 0);\n@@ -785,2 +854,2 @@\n-    movptr(reg_rax, Address(monitor, OM_OFFSET_NO_MONITOR_VALUE_TAG(cxq)));\n-    orptr(reg_rax, Address(monitor, OM_OFFSET_NO_MONITOR_VALUE_TAG(EntryList)));\n+    movptr(reg_rax, cxq_address);\n+    orptr(reg_rax, EntryList_address);\n@@ -790,1 +859,1 @@\n-    movptr(Address(monitor, OM_OFFSET_NO_MONITOR_VALUE_TAG(owner)), NULL_WORD);\n+    movptr(owner_address, NULL_WORD);\n@@ -795,1 +864,1 @@\n-    decrement(Address(monitor, OM_OFFSET_NO_MONITOR_VALUE_TAG(recursions)));\n+    decrement(recursions_address);\n@@ -797,1 +866,0 @@\n-#endif\n","filename":"src\/hotspot\/cpu\/x86\/c2_MacroAssembler_x86.cpp","additions":98,"deletions":30,"binary":false,"changes":128,"status":"modified"},{"patch":"@@ -1186,0 +1186,1 @@\n+      lightweight_lock(lock_reg, obj_reg, swap_reg, thread, tmp_reg, slow_case);\n@@ -1187,2 +1188,2 @@\n-      const Register thread = lock_reg;\n-      get_thread(thread);\n+      \/\/ Lacking registers and thread on x86_32. Always take slow path.\n+      jmp(slow_case);\n@@ -1190,1 +1191,0 @@\n-      lightweight_lock(obj_reg, swap_reg, thread, tmp_reg, slow_case);\n@@ -1252,9 +1252,3 @@\n-    if (LockingMode == LM_LIGHTWEIGHT) {\n-      call_VM(noreg,\n-              CAST_FROM_FN_PTR(address, InterpreterRuntime::monitorenter_obj),\n-              obj_reg);\n-    } else {\n-      call_VM(noreg,\n-              CAST_FROM_FN_PTR(address, InterpreterRuntime::monitorenter),\n-              lock_reg);\n-    }\n+    call_VM(noreg,\n+            CAST_FROM_FN_PTR(address, InterpreterRuntime::monitorenter),\n+            lock_reg);\n@@ -1309,4 +1303,2 @@\n-      \/\/ This relies on the implementation of lightweight_unlock being able to handle\n-      \/\/ that the reg_rax and thread Register parameters may alias each other.\n-      get_thread(swap_reg);\n-      lightweight_unlock(obj_reg, swap_reg, swap_reg, header_reg, slow_case);\n+      \/\/ Lacking registers and thread on x86_32. Always take slow path.\n+      jmp(slow_case);\n","filename":"src\/hotspot\/cpu\/x86\/interp_masm_x86.cpp","additions":8,"deletions":16,"binary":false,"changes":24,"status":"modified"},{"patch":"@@ -10279,1 +10279,1 @@\n-void MacroAssembler::lightweight_lock(Register obj, Register reg_rax, Register thread, Register tmp, Label& slow) {\n+void MacroAssembler::lightweight_lock(Register basic_lock, Register obj, Register reg_rax, Register thread, Register tmp, Label& slow) {\n@@ -10281,1 +10281,1 @@\n-  assert_different_registers(obj, reg_rax, thread, tmp);\n+  assert_different_registers(basic_lock, obj, reg_rax, thread, tmp);\n@@ -10290,0 +10290,5 @@\n+  if (UseObjectMonitorTable) {\n+    \/\/ Clear cache in case fast locking succeeds.\n+    movptr(Address(basic_lock, BasicObjectLock::lock_offset() + in_ByteSize((BasicLock::object_monitor_cache_offset_in_bytes()))), 0);\n+  }\n+\n@@ -10328,3 +10333,0 @@\n-\/\/\n-\/\/ x86_32 Note: reg_rax and thread may alias each other due to limited register\n-\/\/              availiability.\n@@ -10333,2 +10335,1 @@\n-  assert_different_registers(obj, reg_rax, tmp);\n-  LP64_ONLY(assert_different_registers(obj, reg_rax, thread, tmp);)\n+  assert_different_registers(obj, reg_rax, thread, tmp);\n@@ -10374,4 +10375,0 @@\n-  if (thread == reg_rax) {\n-    \/\/ On x86_32 we may lose the thread.\n-    get_thread(thread);\n-  }\n","filename":"src\/hotspot\/cpu\/x86\/macroAssembler_x86.cpp","additions":8,"deletions":11,"binary":false,"changes":19,"status":"modified"},{"patch":"@@ -2151,1 +2151,1 @@\n-  void lightweight_lock(Register obj, Register reg_rax, Register thread, Register tmp, Label& slow);\n+  void lightweight_lock(Register basic_lock, Register obj, Register reg_rax, Register thread, Register tmp, Label& slow);\n","filename":"src\/hotspot\/cpu\/x86\/macroAssembler_x86.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -65,3 +65,5 @@\n-    \/\/ check if monitor\n-    __ testptr(result, markWord::monitor_value);\n-    __ jcc(Assembler::notZero, slowCase);\n+    if (!UseObjectMonitorTable) {\n+      \/\/ check if monitor\n+      __ testptr(result, markWord::monitor_value);\n+      __ jcc(Assembler::notZero, slowCase);\n+    }\n","filename":"src\/hotspot\/cpu\/x86\/sharedRuntime_x86.cpp","additions":5,"deletions":3,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -1689,1 +1689,2 @@\n-      __ lightweight_lock(obj_reg, swap_reg, thread, lock_reg, slow_path_lock);\n+      \/\/ Lacking registers and thread on x86_32. Always take slow path.\n+      __ jmp(slow_path_lock);\n","filename":"src\/hotspot\/cpu\/x86\/sharedRuntime_x86_32.cpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -2269,1 +2269,1 @@\n-      __ lightweight_lock(obj_reg, swap_reg, r15_thread, rscratch1, slow_path_lock);\n+      __ lightweight_lock(lock_reg, obj_reg, swap_reg, r15_thread, rscratch1, slow_path_lock);\n","filename":"src\/hotspot\/cpu\/x86\/sharedRuntime_x86_64.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -40,0 +40,1 @@\n+#include \"runtime\/basicLock.inline.hpp\"\n@@ -47,0 +48,1 @@\n+#include \"utilities\/globalDefinitions.hpp\"\n@@ -334,13 +336,15 @@\n-    markWord disp = lockee->mark().set_unlocked();\n-    monitor->lock()->set_displaced_header(disp);\n-    bool call_vm = (LockingMode == LM_MONITOR);\n-    bool inc_monitor_count = true;\n-    if (call_vm || lockee->cas_set_mark(markWord::from_pointer(monitor), disp) != disp) {\n-      \/\/ Is it simple recursive case?\n-      if (!call_vm && thread->is_lock_owned((address) disp.clear_lock_bits().to_pointer())) {\n-        monitor->lock()->set_displaced_header(markWord::from_pointer(nullptr));\n-      } else {\n-        inc_monitor_count = false;\n-        CALL_VM_NOCHECK(InterpreterRuntime::monitorenter(thread, monitor));\n-        if (HAS_PENDING_EXCEPTION)\n-          goto unwind_and_return;\n+    bool success = false;\n+    if (LockingMode == LM_LEGACY) {\n+      markWord disp = lockee->mark().set_unlocked();\n+      monitor->lock()->set_displaced_header(disp);\n+      success = true;\n+      if (lockee->cas_set_mark(markWord::from_pointer(monitor), disp) != disp) {\n+        \/\/ Is it simple recursive case?\n+        if (thread->is_lock_owned((address) disp.clear_lock_bits().to_pointer())) {\n+          monitor->lock()->set_displaced_header(markWord::from_pointer(nullptr));\n+        } else {\n+          success = false;\n+        }\n+      }\n+      if (success) {\n+        THREAD->inc_held_monitor_count();\n@@ -349,2 +353,4 @@\n-    if (inc_monitor_count) {\n-      THREAD->inc_held_monitor_count();\n+    if (!success) {\n+      CALL_VM_NOCHECK(InterpreterRuntime::monitorenter(thread, monitor));\n+          if (HAS_PENDING_EXCEPTION)\n+            goto unwind_and_return;\n","filename":"src\/hotspot\/cpu\/zero\/zeroInterpreter_zero.cpp","additions":21,"deletions":15,"binary":false,"changes":36,"status":"modified"},{"patch":"@@ -760,2 +760,2 @@\n-  assert(LockingMode == LM_LIGHTWEIGHT || obj == lock->obj(), \"must match\");\n-  SharedRuntime::monitor_enter_helper(obj, LockingMode == LM_LIGHTWEIGHT ? nullptr : lock->lock(), current);\n+  assert(obj == lock->obj(), \"must match\");\n+  SharedRuntime::monitor_enter_helper(obj, lock->lock(), current);\n","filename":"src\/hotspot\/share\/c1\/c1_Runtime1.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -74,1 +74,1 @@\n-#include \"runtime\/synchronizer.hpp\"\n+#include \"runtime\/synchronizer.inline.hpp\"\n@@ -728,1 +728,0 @@\n-  assert(LockingMode != LM_LIGHTWEIGHT, \"Should call monitorenter_obj() when using the new lightweight locking\");\n@@ -743,17 +742,0 @@\n-\/\/ NOTE: We provide a separate implementation for the new lightweight locking to workaround a limitation\n-\/\/ of registers in x86_32. This entry point accepts an oop instead of a BasicObjectLock*.\n-\/\/ The problem is that we would need to preserve the register that holds the BasicObjectLock,\n-\/\/ but we are using that register to hold the thread. We don't have enough registers to\n-\/\/ also keep the BasicObjectLock, but we don't really need it anyway, we only need\n-\/\/ the object. See also InterpreterMacroAssembler::lock_object().\n-\/\/ As soon as legacy stack-locking goes away we could remove the other monitorenter() entry\n-\/\/ point, and only use oop-accepting entries (same for monitorexit() below).\n-JRT_ENTRY_NO_ASYNC(void, InterpreterRuntime::monitorenter_obj(JavaThread* current, oopDesc* obj))\n-  assert(LockingMode == LM_LIGHTWEIGHT, \"Should call monitorenter() when not using the new lightweight locking\");\n-  Handle h_obj(current, cast_to_oop(obj));\n-  assert(Universe::heap()->is_in_or_null(h_obj()),\n-         \"must be null or an object\");\n-  ObjectSynchronizer::enter(h_obj, nullptr, current);\n-  return;\n-JRT_END\n-\n","filename":"src\/hotspot\/share\/interpreter\/interpreterRuntime.cpp","additions":1,"deletions":19,"binary":false,"changes":20,"status":"modified"},{"patch":"@@ -56,0 +56,1 @@\n+#include \"runtime\/basicLock.inline.hpp\"\n@@ -57,0 +58,1 @@\n+#include \"runtime\/globals.hpp\"\n@@ -64,0 +66,1 @@\n+#include \"utilities\/globalDefinitions.hpp\"\n@@ -627,12 +630,16 @@\n-        \/\/ Traditional lightweight locking.\n-        markWord displaced = rcvr->mark().set_unlocked();\n-        mon->lock()->set_displaced_header(displaced);\n-        bool call_vm = (LockingMode == LM_MONITOR);\n-        bool inc_monitor_count = true;\n-        if (call_vm || rcvr->cas_set_mark(markWord::from_pointer(mon), displaced) != displaced) {\n-          \/\/ Is it simple recursive case?\n-          if (!call_vm && THREAD->is_lock_owned((address) displaced.clear_lock_bits().to_pointer())) {\n-            mon->lock()->set_displaced_header(markWord::from_pointer(nullptr));\n-          } else {\n-            inc_monitor_count = false;\n-            CALL_VM(InterpreterRuntime::monitorenter(THREAD, mon), handle_exception);\n+        bool success = false;\n+        if (LockingMode == LM_LEGACY) {\n+           \/\/ Traditional lightweight locking.\n+          markWord displaced = rcvr->mark().set_unlocked();\n+          mon->lock()->set_displaced_header(displaced);\n+          success = true;\n+          if (rcvr->cas_set_mark(markWord::from_pointer(mon), displaced) != displaced) {\n+            \/\/ Is it simple recursive case?\n+            if (THREAD->is_lock_owned((address) displaced.clear_lock_bits().to_pointer())) {\n+              mon->lock()->set_displaced_header(markWord::from_pointer(nullptr));\n+            } else {\n+              success = false;\n+            }\n+          }\n+          if (success) {\n+            THREAD->inc_held_monitor_count();\n@@ -641,2 +648,2 @@\n-        if (inc_monitor_count) {\n-          THREAD->inc_held_monitor_count();\n+        if (!success) {\n+            CALL_VM(InterpreterRuntime::monitorenter(THREAD, mon), handle_exception);\n@@ -644,0 +651,1 @@\n+\n@@ -726,12 +734,16 @@\n-      \/\/ traditional lightweight locking\n-      markWord displaced = lockee->mark().set_unlocked();\n-      entry->lock()->set_displaced_header(displaced);\n-      bool call_vm = (LockingMode == LM_MONITOR);\n-      bool inc_monitor_count = true;\n-      if (call_vm || lockee->cas_set_mark(markWord::from_pointer(entry), displaced) != displaced) {\n-        \/\/ Is it simple recursive case?\n-        if (!call_vm && THREAD->is_lock_owned((address) displaced.clear_lock_bits().to_pointer())) {\n-          entry->lock()->set_displaced_header(markWord::from_pointer(nullptr));\n-        } else {\n-          inc_monitor_count = false;\n-          CALL_VM(InterpreterRuntime::monitorenter(THREAD, entry), handle_exception);\n+      bool success = false;\n+      if (LockingMode == LM_LEGACY) {\n+        \/\/ traditional lightweight locking\n+        markWord displaced = lockee->mark().set_unlocked();\n+        entry->lock()->set_displaced_header(displaced);\n+        success = true;\n+        if (lockee->cas_set_mark(markWord::from_pointer(entry), displaced) != displaced) {\n+          \/\/ Is it simple recursive case?\n+          if (THREAD->is_lock_owned((address) displaced.clear_lock_bits().to_pointer())) {\n+            entry->lock()->set_displaced_header(markWord::from_pointer(nullptr));\n+          } else {\n+            success = false;\n+          }\n+        }\n+        if (success) {\n+          THREAD->inc_held_monitor_count();\n@@ -740,2 +752,2 @@\n-      if (inc_monitor_count) {\n-        THREAD->inc_held_monitor_count();\n+      if (!success) {\n+        CALL_VM(InterpreterRuntime::monitorenter(THREAD, entry), handle_exception);\n@@ -743,0 +755,1 @@\n+\n@@ -1656,12 +1669,16 @@\n-          \/\/ traditional lightweight locking\n-          markWord displaced = lockee->mark().set_unlocked();\n-          entry->lock()->set_displaced_header(displaced);\n-          bool call_vm = (LockingMode == LM_MONITOR);\n-          bool inc_monitor_count = true;\n-          if (call_vm || lockee->cas_set_mark(markWord::from_pointer(entry), displaced) != displaced) {\n-            \/\/ Is it simple recursive case?\n-            if (!call_vm && THREAD->is_lock_owned((address) displaced.clear_lock_bits().to_pointer())) {\n-              entry->lock()->set_displaced_header(markWord::from_pointer(nullptr));\n-            } else {\n-              inc_monitor_count = false;\n-              CALL_VM(InterpreterRuntime::monitorenter(THREAD, entry), handle_exception);\n+          bool success = false;\n+          if (LockingMode == LM_LEGACY) {\n+            \/\/ traditional lightweight locking\n+            markWord displaced = lockee->mark().set_unlocked();\n+            entry->lock()->set_displaced_header(displaced);\n+            success = true;\n+            if (lockee->cas_set_mark(markWord::from_pointer(entry), displaced) != displaced) {\n+              \/\/ Is it simple recursive case?\n+              if (THREAD->is_lock_owned((address) displaced.clear_lock_bits().to_pointer())) {\n+                entry->lock()->set_displaced_header(markWord::from_pointer(nullptr));\n+              } else {\n+                success = false;\n+              }\n+            }\n+            if (success) {\n+              THREAD->inc_held_monitor_count();\n@@ -1670,2 +1687,2 @@\n-          if (inc_monitor_count) {\n-            THREAD->inc_held_monitor_count();\n+          if (!success) {\n+            CALL_VM(InterpreterRuntime::monitorenter(THREAD, entry), handle_exception);\n@@ -1673,0 +1690,1 @@\n+\n@@ -1690,2 +1708,0 @@\n-            markWord header = lock->displaced_header();\n-            most_recent->set_obj(nullptr);\n@@ -1693,10 +1709,16 @@\n-            \/\/ If it isn't recursive we either must swap old header or call the runtime\n-            bool dec_monitor_count = true;\n-            bool call_vm = (LockingMode == LM_MONITOR);\n-            if (header.to_pointer() != nullptr || call_vm) {\n-              markWord old_header = markWord::encode(lock);\n-              if (call_vm || lockee->cas_set_mark(header, old_header) != old_header) {\n-                \/\/ restore object for the slow case\n-                most_recent->set_obj(lockee);\n-                dec_monitor_count = false;\n-                InterpreterRuntime::monitorexit(most_recent);\n+            bool success = false;\n+            if (LockingMode == LM_LEGACY) {\n+              \/\/ If it isn't recursive we either must swap old header or call the runtime\n+              most_recent->set_obj(nullptr);\n+              success = true;\n+              markWord header = lock->displaced_header();\n+              if (header.to_pointer() != nullptr) {\n+                markWord old_header = markWord::encode(lock);\n+                if (lockee->cas_set_mark(header, old_header) != old_header) {\n+                  \/\/ restore object for the slow case\n+                  most_recent->set_obj(lockee);\n+                  success = false;\n+                }\n+              }\n+              if (success) {\n+                THREAD->dec_held_monitor_count();\n@@ -1705,2 +1727,2 @@\n-            if (dec_monitor_count) {\n-              THREAD->dec_held_monitor_count();\n+            if (!success) {\n+              InterpreterRuntime::monitorexit(most_recent);\n@@ -3128,12 +3150,18 @@\n-          markWord header = lock->displaced_header();\n-          end->set_obj(nullptr);\n-\n-          \/\/ If it isn't recursive we either must swap old header or call the runtime\n-          bool dec_monitor_count = true;\n-          if (header.to_pointer() != nullptr) {\n-            markWord old_header = markWord::encode(lock);\n-            if (lockee->cas_set_mark(header, old_header) != old_header) {\n-              \/\/ restore object for the slow case\n-              end->set_obj(lockee);\n-              dec_monitor_count = false;\n-              InterpreterRuntime::monitorexit(end);\n+\n+          bool success = false;\n+          if (LockingMode == LM_LEGACY) {\n+            markWord header = lock->displaced_header();\n+            end->set_obj(nullptr);\n+\n+            \/\/ If it isn't recursive we either must swap old header or call the runtime\n+            success = true;\n+            if (header.to_pointer() != nullptr) {\n+              markWord old_header = markWord::encode(lock);\n+              if (lockee->cas_set_mark(header, old_header) != old_header) {\n+                \/\/ restore object for the slow case\n+                end->set_obj(lockee);\n+                success = false;\n+              }\n+            }\n+            if (success) {\n+              THREAD->dec_held_monitor_count();\n@@ -3142,2 +3170,2 @@\n-          if (dec_monitor_count) {\n-            THREAD->dec_held_monitor_count();\n+          if (!success) {\n+            InterpreterRuntime::monitorexit(end);\n@@ -3191,1 +3219,1 @@\n-          } else if (LockingMode == LM_MONITOR) {\n+          } else if (LockingMode != LM_LEGACY) {\n","filename":"src\/hotspot\/share\/interpreter\/zero\/bytecodeInterpreter.cpp","additions":99,"deletions":71,"binary":false,"changes":170,"status":"modified"},{"patch":"@@ -154,1 +154,1 @@\n-  volatile_nonstatic_field(BasicLock,          _displaced_header,                      markWord)                                     \\\n+  volatile_nonstatic_field(BasicLock,          _metadata,                              uintptr_t)                                    \\\n@@ -244,0 +244,1 @@\n+  nonstatic_field(JavaThread,                  _om_cache,                                     OMCache)                               \\\n@@ -534,0 +535,2 @@\n+  declare_constant_with_value(\"OMCache::oop_to_oop_difference\", OMCache::oop_to_oop_difference()) \\\n+  declare_constant_with_value(\"OMCache::oop_to_monitor_difference\", OMCache::oop_to_monitor_difference()) \\\n","filename":"src\/hotspot\/share\/jvmci\/vmStructs_jvmci.cpp","additions":4,"deletions":1,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -133,0 +133,1 @@\n+  LOG_TAG(monitortable) \\\n","filename":"src\/hotspot\/share\/logging\/logTag.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -27,0 +27,1 @@\n+#include \"runtime\/basicLock.inline.hpp\"\n@@ -70,1 +71,1 @@\n-    if (print_monitor_info) {\n+    if (print_monitor_info && !UseObjectMonitorTable) {\n","filename":"src\/hotspot\/share\/oops\/markWord.cpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -200,0 +200,1 @@\n+    assert(!UseObjectMonitorTable, \"Lightweight locking with OM table does not use markWord for monitors\");\n@@ -205,2 +206,5 @@\n-    return LockingMode == LM_LIGHTWEIGHT  ? lockbits == monitor_value   \/\/ monitor?\n-                                          : (lockbits & unlocked_value) == 0; \/\/ monitor | stack-locked?\n+    if (LockingMode == LM_LIGHTWEIGHT) {\n+      return !UseObjectMonitorTable && lockbits == monitor_value;\n+    }\n+    \/\/ monitor (0b10) | stack-locked (0b00)?\n+    return (lockbits & unlocked_value) == 0;\n@@ -226,0 +230,1 @@\n+    assert(!UseObjectMonitorTable, \"Lightweight locking with OM table does not use markWord for monitors\");\n@@ -230,0 +235,4 @@\n+  markWord set_has_monitor() const {\n+    return markWord((value() & ~lock_mask_in_place) | monitor_value);\n+  }\n+\n","filename":"src\/hotspot\/share\/oops\/markWord.hpp","additions":11,"deletions":2,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -106,0 +106,1 @@\n+  Label _slow_path;\n@@ -114,0 +115,1 @@\n+  Label& slow_path() { return _slow_path; }\n","filename":"src\/hotspot\/share\/opto\/c2_CodeStubs.hpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -4607,13 +4607,14 @@\n-  \/\/ Test the header to see if it is safe to read w.r.t. locking.\n-  Node *lock_mask      = _gvn.MakeConX(markWord::lock_mask_in_place);\n-  Node *lmasked_header = _gvn.transform(new AndXNode(header, lock_mask));\n-  if (LockingMode == LM_LIGHTWEIGHT) {\n-    Node *monitor_val   = _gvn.MakeConX(markWord::monitor_value);\n-    Node *chk_monitor   = _gvn.transform(new CmpXNode(lmasked_header, monitor_val));\n-    Node *test_monitor  = _gvn.transform(new BoolNode(chk_monitor, BoolTest::eq));\n-\n-    generate_slow_guard(test_monitor, slow_region);\n-  } else {\n-    Node *unlocked_val      = _gvn.MakeConX(markWord::unlocked_value);\n-    Node *chk_unlocked      = _gvn.transform(new CmpXNode(lmasked_header, unlocked_val));\n-    Node *test_not_unlocked = _gvn.transform(new BoolNode(chk_unlocked, BoolTest::ne));\n+  if (!UseObjectMonitorTable) {\n+    \/\/ Test the header to see if it is safe to read w.r.t. locking.\n+    Node *lock_mask      = _gvn.MakeConX(markWord::lock_mask_in_place);\n+    Node *lmasked_header = _gvn.transform(new AndXNode(header, lock_mask));\n+    if (LockingMode == LM_LIGHTWEIGHT) {\n+      Node *monitor_val   = _gvn.MakeConX(markWord::monitor_value);\n+      Node *chk_monitor   = _gvn.transform(new CmpXNode(lmasked_header, monitor_val));\n+      Node *test_monitor  = _gvn.transform(new BoolNode(chk_monitor, BoolTest::eq));\n+\n+      generate_slow_guard(test_monitor, slow_region);\n+    } else {\n+      Node *unlocked_val      = _gvn.MakeConX(markWord::unlocked_value);\n+      Node *chk_unlocked      = _gvn.transform(new CmpXNode(lmasked_header, unlocked_val));\n+      Node *test_not_unlocked = _gvn.transform(new BoolNode(chk_unlocked, BoolTest::ne));\n@@ -4621,1 +4622,2 @@\n-    generate_slow_guard(test_not_unlocked, slow_region);\n+      generate_slow_guard(test_not_unlocked, slow_region);\n+    }\n","filename":"src\/hotspot\/share\/opto\/library_call.cpp","additions":16,"deletions":14,"binary":false,"changes":30,"status":"modified"},{"patch":"@@ -59,0 +59,1 @@\n+#include \"runtime\/synchronizer.inline.hpp\"\n@@ -1468,1 +1469,0 @@\n-  ObjectMonitor *mon = nullptr;\n@@ -1498,2 +1498,5 @@\n-  if (mark.has_monitor()) {\n-    mon = mark.monitor();\n+  ObjectMonitor* mon = mark.has_monitor()\n+      ? ObjectSynchronizer::read_monitor(current_thread, hobj(), mark)\n+      : nullptr;\n+\n+  if (mon != nullptr) {\n","filename":"src\/hotspot\/share\/prims\/jvmtiEnvBase.cpp","additions":6,"deletions":3,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -1819,0 +1819,4 @@\n+  if (UseObjectMonitorTable) {\n+    FLAG_SET_CMDLINE(UseObjectMonitorTable, false);\n+    warning(\"UseObjectMonitorTable not supported on this platform\");\n+  }\n@@ -1821,0 +1825,6 @@\n+  if (UseObjectMonitorTable && LockingMode != LM_LIGHTWEIGHT) {\n+    \/\/ ObjectMonitorTable requires lightweight locking.\n+    FLAG_SET_CMDLINE(UseObjectMonitorTable, false);\n+    warning(\"UseObjectMonitorTable requires LM_LIGHTWEIGHT\");\n+  }\n+\n","filename":"src\/hotspot\/share\/runtime\/arguments.cpp","additions":10,"deletions":0,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -27,1 +27,2 @@\n-#include \"runtime\/basicLock.hpp\"\n+#include \"runtime\/basicLock.inline.hpp\"\n+#include \"runtime\/objectMonitor.hpp\"\n@@ -32,5 +33,12 @@\n-  markWord mark_word = displaced_header();\n-  if (mark_word.value() != 0) {\n-    \/\/ Print monitor info if there's an owning oop and it refers to this BasicLock.\n-    bool print_monitor_info = (owner != nullptr) && (owner->mark() == markWord::from_pointer((void*)this));\n-    mark_word.print_on(st, print_monitor_info);\n+  if (UseObjectMonitorTable) {\n+    ObjectMonitor* mon = object_monitor_cache();\n+    if (mon != nullptr) {\n+      mon->print_on(st);\n+    }\n+  } else if (LockingMode == LM_LEGACY) {\n+    markWord mark_word = displaced_header();\n+    if (mark_word.value() != 0) {\n+      \/\/ Print monitor info if there's an owning oop and it refers to this BasicLock.\n+      bool print_monitor_info = (owner != nullptr) && (owner->mark() == markWord::from_pointer((void*)this));\n+      mark_word.print_on(st, print_monitor_info);\n+    }\n@@ -85,0 +93,5 @@\n+  } else if (UseObjectMonitorTable) {\n+    \/\/ Preserve the ObjectMonitor*, the cache is cleared when a box is reused\n+    \/\/ and only read while the lock is held, so no stale ObjectMonitor* is\n+    \/\/ encountered.\n+    dest->set_object_monitor_cache(object_monitor_cache());\n@@ -88,1 +101,1 @@\n-    dest->set_displaced_header(markWord(badDispHeaderDeopt));\n+    dest->set_bad_metadata_deopt();\n","filename":"src\/hotspot\/share\/runtime\/basicLock.cpp","additions":20,"deletions":7,"binary":false,"changes":27,"status":"modified"},{"patch":"@@ -31,0 +31,1 @@\n+#include \"utilities\/globalDefinitions.hpp\"\n@@ -37,0 +38,3 @@\n+  \/\/ * For LM_MONITOR\n+  \/\/ Unused.\n+  \/\/ * For LM_LEGACY\n@@ -39,1 +43,9 @@\n-  volatile markWord _displaced_header;\n+  \/\/ * For LM_LIGHTWEIGHT\n+  \/\/ Used as a cache of the ObjectMonitor* used when locking. Must either\n+  \/\/ be nullptr or the ObjectMonitor* used when locking.\n+  volatile uintptr_t _metadata;\n+\n+  uintptr_t get_metadata() const { return Atomic::load(&_metadata); }\n+  void set_metadata(uintptr_t value) { Atomic::store(&_metadata, value); }\n+  static int metadata_offset_in_bytes() { return (int)offset_of(BasicLock, _metadata); }\n+\n@@ -41,3 +53,2 @@\n-  markWord displaced_header() const {\n-    return Atomic::load(&_displaced_header);\n-  }\n+  \/\/ LM_MONITOR\n+  void set_bad_metadata_deopt() { set_metadata(badDispHeaderDeopt); }\n@@ -45,3 +56,10 @@\n-  void set_displaced_header(markWord header) {\n-    Atomic::store(&_displaced_header, header);\n-  }\n+  \/\/ LM_LEGACY\n+  inline markWord displaced_header() const;\n+  inline void set_displaced_header(markWord header);\n+  static int displaced_header_offset_in_bytes() { return metadata_offset_in_bytes(); }\n+\n+  \/\/ LM_LIGHTWEIGHT\n+  inline ObjectMonitor* object_monitor_cache() const;\n+  inline void clear_object_monitor_cache();\n+  inline void set_object_monitor_cache(ObjectMonitor* mon);\n+  static int object_monitor_cache_offset_in_bytes() { return metadata_offset_in_bytes(); }\n@@ -53,2 +71,0 @@\n-\n-  static int displaced_header_offset_in_bytes() { return (int)offset_of(BasicLock, _displaced_header); }\n","filename":"src\/hotspot\/share\/runtime\/basicLock.hpp","additions":25,"deletions":9,"binary":false,"changes":34,"status":"modified"},{"patch":"@@ -0,0 +1,62 @@\n+\/*\n+ * Copyright (c) 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef SHARE_RUNTIME_BASICLOCK_INLINE_HPP\n+#define SHARE_RUNTIME_BASICLOCK_INLINE_HPP\n+\n+#include \"runtime\/basicLock.hpp\"\n+\n+inline markWord BasicLock::displaced_header() const {\n+  assert(LockingMode == LM_LEGACY, \"must be\");\n+  return markWord(get_metadata());\n+}\n+\n+inline void BasicLock::set_displaced_header(markWord header) {\n+  assert(LockingMode == LM_LEGACY, \"must be\");\n+  Atomic::store(&_metadata, header.value());\n+}\n+\n+inline ObjectMonitor* BasicLock::object_monitor_cache() const {\n+  assert(UseObjectMonitorTable, \"must be\");\n+#if defined(X86) || defined(AARCH64)\n+  return reinterpret_cast<ObjectMonitor*>(get_metadata());\n+#else\n+  \/\/ Other platforms does not make use of the cache yet,\n+  \/\/ and are not as careful with maintaining the invariant\n+  \/\/ that the metadata either is nullptr or ObjectMonitor*.\n+  return nullptr;\n+#endif\n+}\n+\n+inline void BasicLock::clear_object_monitor_cache() {\n+  assert(UseObjectMonitorTable, \"must be\");\n+  set_metadata(0);\n+}\n+\n+inline void BasicLock::set_object_monitor_cache(ObjectMonitor* mon) {\n+  assert(UseObjectMonitorTable, \"must be\");\n+  set_metadata(reinterpret_cast<uintptr_t>(mon));\n+}\n+\n+#endif \/\/ SHARE_RUNTIME_BASICLOCK_INLINE_HPP\n","filename":"src\/hotspot\/share\/runtime\/basicLock.inline.hpp","additions":62,"deletions":0,"binary":false,"changes":62,"status":"added"},{"patch":"@@ -66,0 +66,1 @@\n+#include \"runtime\/basicLock.inline.hpp\"\n@@ -78,0 +79,2 @@\n+#include \"runtime\/lightweightSynchronizer.hpp\"\n+#include \"runtime\/lockStack.inline.hpp\"\n@@ -87,1 +90,1 @@\n-#include \"runtime\/synchronizer.hpp\"\n+#include \"runtime\/synchronizer.inline.hpp\"\n@@ -1637,1 +1640,11 @@\n-              mon_info->lock()->set_displaced_header(markWord::unused_mark());\n+              if (LockingMode == LM_LEGACY) {\n+                mon_info->lock()->set_displaced_header(markWord::unused_mark());\n+              } else if (UseObjectMonitorTable) {\n+                mon_info->lock()->clear_object_monitor_cache();\n+              }\n+#ifdef ASSERT\n+              else {\n+                assert(LockingMode == LM_MONITOR || !UseObjectMonitorTable, \"must be\");\n+                mon_info->lock()->set_bad_metadata_deopt();\n+              }\n+#endif\n@@ -1643,0 +1656,1 @@\n+        BasicLock* lock = mon_info->lock();\n@@ -1645,3 +1659,7 @@\n-          \/\/ Inflate the locks instead. Enter then inflate to avoid races with\n-          \/\/ deflation.\n-          ObjectSynchronizer::enter_for(obj, nullptr, deoptee_thread);\n+          \/\/ Entering may create an invalid lock stack. Inflate the lock if it\n+          \/\/ was fast_locked to restore the valid lock stack.\n+          ObjectSynchronizer::enter_for(obj, lock, deoptee_thread);\n+          if (deoptee_thread->lock_stack().contains(obj())) {\n+            LightweightSynchronizer::inflate_fast_locked_object(obj(), deoptee_thread, thread,\n+                                                                ObjectSynchronizer::InflateCause::inflate_cause_vm_internal);\n+          }\n@@ -1649,2 +1667,3 @@\n-          ObjectMonitor* mon = ObjectSynchronizer::inflate_for(deoptee_thread, obj(), ObjectSynchronizer::inflate_cause_vm_internal);\n-          assert(mon->owner() == deoptee_thread, \"must be\");\n+          assert(obj->mark().has_monitor(), \"must be\");\n+          assert(!deoptee_thread->lock_stack().contains(obj()), \"must be\");\n+          assert(ObjectSynchronizer::read_monitor(thread, obj(), obj->mark())->owner() == deoptee_thread, \"must be\");\n@@ -1652,1 +1671,0 @@\n-          BasicLock* lock = mon_info->lock();\n","filename":"src\/hotspot\/share\/runtime\/deoptimization.cpp","additions":26,"deletions":8,"binary":false,"changes":34,"status":"modified"},{"patch":"@@ -1959,0 +1959,11 @@\n+  product(bool, UseObjectMonitorTable, false, DIAGNOSTIC,                   \\\n+          \"With Lightweight Locking mode, use a table to record inflated \"  \\\n+          \"monitors rather than the first word of the object.\")             \\\n+                                                                            \\\n+  product(int, LightweightFastLockingSpins, 13, DIAGNOSTIC,                 \\\n+          \"Specifies the number of time lightweight fast locking will \"     \\\n+          \"attempt to CAS the markWord before inflating. Between each \"     \\\n+          \"CAS it will spin for exponentially more time, resulting in \"     \\\n+          \"a total number of spins on the order of O(2^value)\")             \\\n+          range(1, 30)                                                      \\\n+                                                                            \\\n","filename":"src\/hotspot\/share\/runtime\/globals.hpp","additions":11,"deletions":0,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -507,1 +507,2 @@\n-  _lock_stack(this) {\n+  _lock_stack(this),\n+  _om_cache(this) {\n@@ -806,0 +807,2 @@\n+  om_clear_monitor_cache();\n+\n","filename":"src\/hotspot\/share\/runtime\/javaThread.cpp","additions":4,"deletions":1,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -64,0 +64,1 @@\n+class ObjectMonitor;\n@@ -1168,0 +1169,1 @@\n+  OMCache _om_cache;\n@@ -1179,0 +1181,7 @@\n+  static ByteSize om_cache_offset()        { return byte_offset_of(JavaThread, _om_cache); }\n+  static ByteSize om_cache_oops_offset()   { return om_cache_offset() + OMCache::entries_offset(); }\n+\n+  void om_set_monitor_cache(ObjectMonitor* monitor);\n+  void om_clear_monitor_cache();\n+  ObjectMonitor* om_get_from_monitor_cache(oop obj);\n+\n","filename":"src\/hotspot\/share\/runtime\/javaThread.hpp","additions":9,"deletions":0,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -39,0 +39,1 @@\n+#include \"runtime\/lockStack.inline.hpp\"\n@@ -40,0 +41,1 @@\n+#include \"runtime\/objectMonitor.inline.hpp\"\n@@ -242,0 +244,21 @@\n+inline void JavaThread::om_set_monitor_cache(ObjectMonitor* monitor) {\n+  assert(UseObjectMonitorTable, \"must be\");\n+  assert(monitor != nullptr, \"use om_clear_monitor_cache to clear\");\n+  assert(this == current() || monitor->owner_raw() == this, \"only add owned monitors for other threads\");\n+  assert(this == current() || is_obj_deopt_suspend(), \"thread must not run concurrently\");\n+\n+  _om_cache.set_monitor(monitor);\n+}\n+\n+inline void JavaThread::om_clear_monitor_cache() {\n+  if (UseObjectMonitorTable) {\n+    _om_cache.clear();\n+  }\n+}\n+\n+inline ObjectMonitor* JavaThread::om_get_from_monitor_cache(oop obj) {\n+  assert(obj != nullptr, \"do not look for null objects\");\n+  assert(this == current(), \"only get own thread locals\");\n+  return _om_cache.get_monitor(obj);\n+}\n+\n","filename":"src\/hotspot\/share\/runtime\/javaThread.inline.hpp","additions":23,"deletions":0,"binary":false,"changes":23,"status":"modified"},{"patch":"@@ -0,0 +1,1231 @@\n+\/*\n+ * Copyright (c) 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+\n+#include \"classfile\/vmSymbols.hpp\"\n+#include \"jfrfiles\/jfrEventClasses.hpp\"\n+#include \"logging\/log.hpp\"\n+#include \"memory\/resourceArea.hpp\"\n+#include \"nmt\/memflags.hpp\"\n+#include \"oops\/oop.inline.hpp\"\n+#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/basicLock.inline.hpp\"\n+#include \"runtime\/globals_extension.hpp\"\n+#include \"runtime\/interfaceSupport.inline.hpp\"\n+#include \"runtime\/javaThread.inline.hpp\"\n+#include \"runtime\/lightweightSynchronizer.hpp\"\n+#include \"runtime\/lockStack.inline.hpp\"\n+#include \"runtime\/mutexLocker.hpp\"\n+#include \"runtime\/objectMonitor.hpp\"\n+#include \"runtime\/objectMonitor.inline.hpp\"\n+#include \"runtime\/os.hpp\"\n+#include \"runtime\/perfData.inline.hpp\"\n+#include \"runtime\/safepointMechanism.inline.hpp\"\n+#include \"runtime\/safepointVerifiers.hpp\"\n+#include \"runtime\/synchronizer.inline.hpp\"\n+#include \"runtime\/timerTrace.hpp\"\n+#include \"runtime\/trimNativeHeap.hpp\"\n+#include \"utilities\/concurrentHashTable.inline.hpp\"\n+#include \"utilities\/concurrentHashTableTasks.inline.hpp\"\n+#include \"utilities\/globalDefinitions.hpp\"\n+\n+\n+\/\/\n+\/\/ Lightweight synchronization.\n+\/\/\n+\n+\/\/ ConcurrentHashTable storing links from objects to ObjectMonitors\n+class ObjectMonitorTable : public CHeapObj<MEMFLAGS::mtObjectMonitor> {\n+  struct Config {\n+    using Value = ObjectMonitor*;\n+    static uintx get_hash(Value const& value, bool* is_dead) {\n+      return (uintx)value->hash();\n+    }\n+    static void* allocate_node(void* context, size_t size, Value const& value) {\n+      reinterpret_cast<ObjectMonitorTable*>(context)->inc_items_count();\n+      return AllocateHeap(size, MEMFLAGS::mtObjectMonitor);\n+    };\n+    static void free_node(void* context, void* memory, Value const& value) {\n+      reinterpret_cast<ObjectMonitorTable*>(context)->dec_items_count();\n+      FreeHeap(memory);\n+    }\n+  };\n+  using ConcurrentTable = ConcurrentHashTable<Config, MEMFLAGS::mtObjectMonitor>;\n+\n+  ConcurrentTable _table;\n+  volatile size_t _items_count;\n+  size_t _table_size;\n+  volatile bool _resize;\n+\n+  class Lookup : public StackObj {\n+    oop _obj;\n+\n+  public:\n+    explicit Lookup(oop obj) : _obj(obj) {}\n+\n+    uintx get_hash() const {\n+      uintx hash = _obj->mark().hash();\n+      assert(hash != 0, \"should have a hash\");\n+      return hash;\n+    }\n+\n+    bool equals(ObjectMonitor** value) {\n+      assert(*value != nullptr, \"must be\");\n+      return (*value)->object_refers_to(_obj);\n+    }\n+\n+    bool is_dead(ObjectMonitor** value) {\n+      assert(*value != nullptr, \"must be\");\n+      return (*value)->object_is_cleared();\n+    }\n+  };\n+\n+  class LookupMonitor : public StackObj {\n+    ObjectMonitor* _monitor;\n+\n+  public:\n+    explicit LookupMonitor(ObjectMonitor* monitor) : _monitor(monitor) {}\n+\n+    uintx get_hash() const {\n+      return _monitor->hash();\n+    }\n+\n+    bool equals(ObjectMonitor** value) {\n+      return (*value) == _monitor;\n+    }\n+\n+    bool is_dead(ObjectMonitor** value) {\n+      assert(*value != nullptr, \"must be\");\n+      return (*value)->object_is_dead();\n+    }\n+  };\n+\n+  void inc_items_count() {\n+    Atomic::inc(&_items_count);\n+  }\n+\n+  void dec_items_count() {\n+    Atomic::inc(&_items_count);\n+  }\n+\n+  double get_load_factor() {\n+    return (double)_items_count\/(double)_table_size;\n+  }\n+\n+  size_t table_size(Thread* current = Thread::current()) {\n+    return ((size_t)1) << _table.get_size_log2(current);\n+  }\n+\n+  static size_t max_log_size() {\n+    \/\/ TODO[OMWorld]: Evaluate the max size.\n+    \/\/ TODO[OMWorld]: Need to fix init order to use Universe::heap()->max_capacity();\n+    \/\/                Using MaxHeapSize directly this early may be wrong, and there\n+    \/\/                are definitely rounding errors (alignment).\n+    const size_t max_capacity = MaxHeapSize;\n+    const size_t min_object_size = CollectedHeap::min_dummy_object_size() * HeapWordSize;\n+    const size_t max_objects = max_capacity \/ MAX2(MinObjAlignmentInBytes, checked_cast<int>(min_object_size));\n+    const size_t log_max_objects = log2i_graceful(max_objects);\n+\n+    return MAX2(MIN2<size_t>(SIZE_BIG_LOG2, log_max_objects), min_log_size());\n+  }\n+\n+  static size_t min_log_size() {\n+    \/\/ ~= log(AvgMonitorsPerThreadEstimate default)\n+    return 10;\n+  }\n+\n+  template<typename V>\n+  static size_t clamp_log_size(V log_size) {\n+    return MAX2(MIN2(log_size, checked_cast<V>(max_log_size())), checked_cast<V>(min_log_size()));\n+  }\n+\n+  static size_t initial_log_size() {\n+    const size_t estimate = log2i(MAX2(os::processor_count(), 1)) + log2i(MAX2(AvgMonitorsPerThreadEstimate, size_t(1)));\n+    return clamp_log_size(estimate);\n+  }\n+\n+  static size_t grow_hint () {\n+    return ConcurrentTable::DEFAULT_GROW_HINT;\n+  }\n+\n+public:\n+  ObjectMonitorTable()\n+  : _table(ConcurrentTable(initial_log_size(),\n+                           max_log_size(),\n+                           grow_hint(),\n+                           ConcurrentTable::DEFAULT_ENABLE_STATISTICS,\n+                           ConcurrentTable::DEFAULT_MUTEX_RANK,\n+                           this)),\n+    _items_count(0),\n+    _table_size(table_size()),\n+    _resize(false) {}\n+\n+  void verify_monitor_get_result(oop obj, ObjectMonitor* monitor) {\n+#ifdef ASSERT\n+    if (SafepointSynchronize::is_at_safepoint()) {\n+      bool has_monitor = obj->mark().has_monitor();\n+      assert(has_monitor == (monitor != nullptr),\n+          \"Inconsistency between markWord and OMW table has_monitor: %s monitor: \" PTR_FORMAT,\n+          BOOL_TO_STR(has_monitor), p2i(monitor));\n+    }\n+#endif\n+  }\n+\n+  ObjectMonitor* monitor_get(Thread* current, oop obj) {\n+    ObjectMonitor* result = nullptr;\n+    Lookup lookup_f(obj);\n+    auto found_f = [&](ObjectMonitor** found) {\n+      assert((*found)->object_peek() == obj, \"must be\");\n+      result = *found;\n+    };\n+    _table.get(current, lookup_f, found_f);\n+    verify_monitor_get_result(obj, result);\n+    return result;\n+  }\n+\n+  void try_notify_grow() {\n+    if (!_table.is_max_size_reached() && !Atomic::load(&_resize)) {\n+      Atomic::store(&_resize, true);\n+      if (Service_lock->try_lock()) {\n+        Service_lock->notify();\n+        Service_lock->unlock();\n+      }\n+    }\n+  }\n+\n+  bool should_shrink() {\n+    \/\/ No implemented;\n+    return false;\n+  }\n+\n+  static constexpr double GROW_LOAD_FACTOR = 0.75;\n+\n+  bool should_grow() {\n+    return get_load_factor() > GROW_LOAD_FACTOR && !_table.is_max_size_reached();\n+  }\n+\n+  bool should_resize() {\n+    return should_grow() || should_shrink() || Atomic::load(&_resize);\n+  }\n+\n+  template<typename Task, typename... Args>\n+  bool run_task(JavaThread* current, Task& task, const char* task_name, Args&... args) {\n+    if (task.prepare(current)) {\n+      log_trace(monitortable)(\"Started to %s\", task_name);\n+      TraceTime timer(task_name, TRACETIME_LOG(Debug, monitortable, perf));\n+      while (task.do_task(current, args...)) {\n+        task.pause(current);\n+        {\n+          ThreadBlockInVM tbivm(current);\n+        }\n+        task.cont(current);\n+      }\n+      task.done(current);\n+      return true;\n+    }\n+    return false;\n+  }\n+\n+  bool grow(JavaThread* current) {\n+    ConcurrentTable::GrowTask grow_task(&_table);\n+    if (run_task(current, grow_task, \"Grow\")) {\n+      _table_size = table_size(current);\n+      log_info(monitortable)(\"Grown to size: %zu\", _table_size);\n+      return true;\n+    }\n+    return false;\n+  }\n+\n+  bool clean(JavaThread* current) {\n+    ConcurrentTable::BulkDeleteTask clean_task(&_table);\n+    auto is_dead = [&](ObjectMonitor** monitor) {\n+      return (*monitor)->object_is_dead();\n+    };\n+    auto do_nothing = [&](ObjectMonitor** monitor) {};\n+    NativeHeapTrimmer::SuspendMark sm(\"omworld\");\n+    return run_task(current, clean_task, \"Clean\", is_dead, do_nothing);\n+  }\n+\n+  bool resize(JavaThread* current) {\n+    LogTarget(Info, monitortable) lt;\n+    bool success = false;\n+\n+    if (should_grow()) {\n+      lt.print(\"Start growing with load factor %f\", get_load_factor());\n+      success = grow(current);\n+    } else {\n+      if (!_table.is_max_size_reached() && Atomic::load(&_resize)) {\n+        lt.print(\"WARNING: Getting resize hints with load factor %f\", get_load_factor());\n+      }\n+      lt.print(\"Start cleaning with load factor %f\", get_load_factor());\n+      success = clean(current);\n+    }\n+\n+    Atomic::store(&_resize, false);\n+\n+    return success;\n+  }\n+\n+  ObjectMonitor* monitor_put_get(Thread* current, ObjectMonitor* monitor, oop obj) {\n+    \/\/ Enter the monitor into the concurrent hashtable.\n+    ObjectMonitor* result = monitor;\n+    Lookup lookup_f(obj);\n+    auto found_f = [&](ObjectMonitor** found) {\n+      assert((*found)->object_peek() == obj, \"must be\");\n+      result = *found;\n+    };\n+    bool grow;\n+    _table.insert_get(current, lookup_f, monitor, found_f, &grow);\n+    verify_monitor_get_result(obj, result);\n+    if (grow) {\n+      try_notify_grow();\n+    }\n+    return result;\n+  }\n+\n+  bool remove_monitor_entry(Thread* current, ObjectMonitor* monitor) {\n+    LookupMonitor lookup_f(monitor);\n+    return _table.remove(current, lookup_f);\n+  }\n+\n+  bool contains_monitor(Thread* current, ObjectMonitor* monitor) {\n+    LookupMonitor lookup_f(monitor);\n+    bool result = false;\n+    auto found_f = [&](ObjectMonitor** found) {\n+      result = true;\n+    };\n+    _table.get(current, lookup_f, found_f);\n+    return result;\n+  }\n+\n+  void print_on(outputStream* st) {\n+    auto printer = [&] (ObjectMonitor** entry) {\n+       ObjectMonitor* om = *entry;\n+       oop obj = om->object_peek();\n+       st->print(\"monitor \" PTR_FORMAT \" \", p2i(om));\n+       st->print(\"object \" PTR_FORMAT, p2i(obj));\n+       assert(obj->mark().hash() == om->hash(), \"hash must match\");\n+       st->cr();\n+       return true;\n+    };\n+    if (SafepointSynchronize::is_at_safepoint()) {\n+      _table.do_safepoint_scan(printer);\n+    } else {\n+      _table.do_scan(Thread::current(), printer);\n+    }\n+  }\n+};\n+\n+ObjectMonitorTable* LightweightSynchronizer::_omworld = nullptr;\n+\n+ObjectMonitor* LightweightSynchronizer::get_or_insert_monitor_from_table(oop object, JavaThread* current, bool* inserted) {\n+  assert(LockingMode == LM_LIGHTWEIGHT, \"must be\");\n+\n+  ObjectMonitor* monitor = get_monitor_from_table(current, object);\n+  if (monitor != nullptr) {\n+    *inserted = false;\n+    return monitor;\n+  }\n+\n+  ObjectMonitor* alloced_monitor = new ObjectMonitor(object);\n+  alloced_monitor->set_owner_anonymous();\n+\n+  \/\/ Try insert monitor\n+  monitor = add_monitor(current, alloced_monitor, object);\n+\n+  *inserted = alloced_monitor == monitor;\n+  if (!*inserted) {\n+    delete alloced_monitor;\n+  }\n+\n+  return monitor;\n+}\n+\n+static void log_inflate(Thread* current, oop object, ObjectSynchronizer::InflateCause cause) {\n+  if (log_is_enabled(Trace, monitorinflation)) {\n+    ResourceMark rm(current);\n+    log_trace(monitorinflation)(\"inflate: object=\" INTPTR_FORMAT \", mark=\"\n+                                INTPTR_FORMAT \", type='%s' cause %s\", p2i(object),\n+                                object->mark().value(), object->klass()->external_name(),\n+                                ObjectSynchronizer::inflate_cause_name(cause));\n+  }\n+}\n+\n+static void post_monitor_inflate_event(EventJavaMonitorInflate* event,\n+                                       const oop obj,\n+                                       ObjectSynchronizer::InflateCause cause) {\n+  assert(event != nullptr, \"invariant\");\n+  event->set_monitorClass(obj->klass());\n+  event->set_address((uintptr_t)(void*)obj);\n+  event->set_cause((u1)cause);\n+  event->commit();\n+}\n+\n+\n+ObjectMonitor* LightweightSynchronizer::get_or_insert_monitor(oop object, JavaThread* current, ObjectSynchronizer::InflateCause cause) {\n+  assert(UseObjectMonitorTable, \"must be\");\n+\n+  EventJavaMonitorInflate event;\n+\n+  bool inserted;\n+  ObjectMonitor* monitor = get_or_insert_monitor_from_table(object, current, &inserted);\n+\n+  if (inserted) {\n+    \/\/ Hopefully the performance counters are allocated on distinct\n+    \/\/ cache lines to avoid false sharing on MP systems ...\n+    OM_PERFDATA_OP(Inflations, inc());\n+    log_inflate(current, object, cause);\n+    if (event.should_commit()) {\n+      post_monitor_inflate_event(&event, object, cause);\n+    }\n+\n+    \/\/ The monitor has an anonymous owner so it is safe from async deflation.\n+    ObjectSynchronizer::_in_use_list.add(monitor);\n+  }\n+\n+  return monitor;\n+}\n+\n+\/\/ Add the hashcode to the monitor to match the object and put it in the hashtable.\n+ObjectMonitor* LightweightSynchronizer::add_monitor(JavaThread* current, ObjectMonitor* monitor, oop obj) {\n+  assert(UseObjectMonitorTable, \"must be\");\n+  assert(obj == monitor->object(), \"must be\");\n+\n+  intptr_t hash = obj->mark().hash();\n+  assert(hash != 0, \"must be set when claiming the object monitor\");\n+  monitor->set_hash(hash);\n+\n+  return _omworld->monitor_put_get(current, monitor, obj);\n+}\n+\n+bool LightweightSynchronizer::remove_monitor(Thread* current, oop obj, ObjectMonitor* monitor) {\n+  assert(UseObjectMonitorTable, \"must be\");\n+  assert(monitor->object_peek() == obj, \"must be, cleared objects are removed by is_dead\");\n+\n+  return _omworld->remove_monitor_entry(current, monitor);\n+}\n+\n+void LightweightSynchronizer::deflate_mark_word(oop obj) {\n+  assert(UseObjectMonitorTable, \"must be\");\n+\n+  markWord mark = obj->mark_acquire();\n+  assert(!mark.has_no_hash(), \"obj with inflated monitor must have had a hash\");\n+\n+  while (mark.has_monitor()) {\n+    const markWord new_mark = mark.clear_lock_bits().set_unlocked();\n+    mark = obj->cas_set_mark(new_mark, mark);\n+  }\n+}\n+\n+void LightweightSynchronizer::initialize() {\n+  if (!UseObjectMonitorTable) {\n+    return;\n+  }\n+  _omworld = new ObjectMonitorTable();\n+}\n+\n+bool LightweightSynchronizer::needs_resize() {\n+  if (!UseObjectMonitorTable) {\n+    return false;\n+  }\n+  return _omworld->should_resize();\n+}\n+\n+bool LightweightSynchronizer::resize_table(JavaThread* current) {\n+  if (!UseObjectMonitorTable) {\n+    return true;\n+  }\n+  return _omworld->resize(current);\n+}\n+\n+class LightweightSynchronizer::LockStackInflateContendedLocks : private OopClosure {\n+ private:\n+  oop _contended_oops[LockStack::CAPACITY];\n+  int _length;\n+\n+  void do_oop(oop* o) final {\n+    oop obj = *o;\n+    if (obj->mark_acquire().has_monitor()) {\n+      if (_length > 0 && _contended_oops[_length-1] == obj) {\n+        \/\/ Recursive\n+        return;\n+      }\n+      _contended_oops[_length++] = obj;\n+    }\n+  }\n+\n+  void do_oop(narrowOop* o) final {\n+    ShouldNotReachHere();\n+  }\n+\n+ public:\n+  LockStackInflateContendedLocks() :\n+    _contended_oops(),\n+    _length(0) {};\n+\n+  void inflate(JavaThread* current) {\n+    assert(current == JavaThread::current(), \"must be\");\n+    current->lock_stack().oops_do(this);\n+    for (int i = 0; i < _length; i++) {\n+      LightweightSynchronizer::\n+        inflate_fast_locked_object(_contended_oops[i], current, current, ObjectSynchronizer::inflate_cause_vm_internal);\n+    }\n+  }\n+};\n+\n+void LightweightSynchronizer::ensure_lock_stack_space(JavaThread* current) {\n+  assert(current == JavaThread::current(), \"must be\");\n+  LockStack& lock_stack = current->lock_stack();\n+\n+  \/\/ Make room on lock_stack\n+  if (lock_stack.is_full()) {\n+    \/\/ Inflate contented objects\n+    LockStackInflateContendedLocks().inflate(current);\n+    if (lock_stack.is_full()) {\n+      \/\/ Inflate the oldest object\n+      inflate_fast_locked_object(lock_stack.bottom(), current, current, ObjectSynchronizer::inflate_cause_vm_internal);\n+    }\n+  }\n+}\n+\n+class LightweightSynchronizer::CacheSetter : StackObj {\n+  JavaThread* const _thread;\n+  BasicLock* const _lock;\n+  ObjectMonitor* _monitor;\n+\n+  NONCOPYABLE(CacheSetter);\n+\n+public:\n+  CacheSetter(JavaThread* thread, BasicLock* lock) :\n+    _thread(thread),\n+    _lock(lock),\n+    _monitor(nullptr) {}\n+\n+  ~CacheSetter() {\n+    \/\/ Only use the cache if using the table.\n+    if (UseObjectMonitorTable) {\n+      if (_monitor != nullptr) {\n+        _thread->om_set_monitor_cache(_monitor);\n+        _lock->set_object_monitor_cache(_monitor);\n+      } else {\n+        _lock->clear_object_monitor_cache();\n+      }\n+    }\n+  }\n+\n+  void set_monitor(ObjectMonitor* monitor) {\n+    assert(_monitor == nullptr, \"only set once\");\n+    _monitor = monitor;\n+  }\n+\n+};\n+\n+class LightweightSynchronizer::VerifyThreadState {\n+  bool _no_safepoint;\n+\n+public:\n+  VerifyThreadState(JavaThread* locking_thread, JavaThread* current) : _no_safepoint(locking_thread != current) {\n+    assert(current == Thread::current(), \"must be\");\n+    assert(locking_thread == current || locking_thread->is_obj_deopt_suspend(), \"locking_thread may not run concurrently\");\n+    if (_no_safepoint) {\n+      DEBUG_ONLY(JavaThread::current()->inc_no_safepoint_count();)\n+    }\n+  }\n+  ~VerifyThreadState() {\n+    if (_no_safepoint){\n+      DEBUG_ONLY(JavaThread::current()->dec_no_safepoint_count();)\n+    }\n+  }\n+};\n+\n+inline bool LightweightSynchronizer::fast_lock_try_enter(oop obj, LockStack& lock_stack, JavaThread* current) {\n+  markWord mark = obj->mark();\n+  while (mark.is_unlocked()) {\n+    ensure_lock_stack_space(current);\n+    assert(!lock_stack.is_full(), \"must have made room on the lock stack\");\n+    assert(!lock_stack.contains(obj), \"thread must not already hold the lock\");\n+    \/\/ Try to swing into 'fast-locked' state.\n+    markWord locked_mark = mark.set_fast_locked();\n+    markWord old_mark = mark;\n+    mark = obj->cas_set_mark(locked_mark, old_mark);\n+    if (old_mark == mark) {\n+      \/\/ Successfully fast-locked, push object to lock-stack and return.\n+      lock_stack.push(obj);\n+      return true;\n+    }\n+  }\n+  return false;\n+}\n+\n+\n+bool LightweightSynchronizer::fast_lock_spin_enter(oop obj, LockStack& lock_stack, JavaThread* current, bool observed_deflation) {\n+  assert(UseObjectMonitorTable, \"must be\");\n+  \/\/ Will spin with exponential backoff with an accumulative O(2^spin_limit) spins.\n+  const int log_spin_limit = os::is_MP() ? LightweightFastLockingSpins : 1;\n+  const int log_min_safepoint_check_interval = 10;\n+\n+  markWord mark = obj->mark();\n+  const auto should_spin = [&]() {\n+    if (!mark.has_monitor()) {\n+      \/\/ Spin while not inflated.\n+      return true;\n+    } else if (observed_deflation) {\n+      \/\/ Spin while monitor is being deflated.\n+      ObjectMonitor* monitor = ObjectSynchronizer::read_monitor(current, obj, mark);\n+      return monitor == nullptr || monitor->is_being_async_deflated();\n+    }\n+    \/\/ Else stop spinning.\n+    return false;\n+  };\n+  \/\/ Always attempt to lock once even when safepoint synchronizing.\n+  bool should_process = false;\n+  for (int i = 0; should_spin() && !should_process && i < log_spin_limit; i++) {\n+    \/\/ Spin with exponential backoff.\n+    const int total_spin_count = 1 << i;\n+    const int inner_spin_count = MIN2(1 << log_min_safepoint_check_interval, total_spin_count);\n+    const int outer_spin_count = total_spin_count \/ inner_spin_count;\n+    for (int outer = 0; outer < outer_spin_count; outer++) {\n+      should_process = SafepointMechanism::should_process(current);\n+      if (should_process) {\n+        \/\/ Stop spinning for safepoint.\n+        break;\n+      }\n+      for (int inner = 1; inner < inner_spin_count; inner++) {\n+        SpinPause();\n+      }\n+    }\n+\n+    if (fast_lock_try_enter(obj, lock_stack, current)) return true;\n+  }\n+  return false;\n+}\n+\n+void LightweightSynchronizer::enter_for(Handle obj, BasicLock* lock, JavaThread* locking_thread) {\n+  assert(LockingMode == LM_LIGHTWEIGHT, \"must be\");\n+  JavaThread* current = JavaThread::current();\n+  VerifyThreadState vts(locking_thread, current);\n+\n+  if (obj->klass()->is_value_based()) {\n+    ObjectSynchronizer::handle_sync_on_value_based_class(obj, locking_thread);\n+  }\n+\n+  locking_thread->inc_held_monitor_count();\n+\n+  CacheSetter cache_setter(locking_thread, lock);\n+\n+  LockStack& lock_stack = locking_thread->lock_stack();\n+\n+  ObjectMonitor* monitor = nullptr;\n+  if (lock_stack.contains(obj())) {\n+    monitor = inflate_fast_locked_object(obj(), locking_thread, current, ObjectSynchronizer::inflate_cause_monitor_enter);\n+    bool entered = monitor->enter_for(locking_thread);\n+    assert(entered, \"recursive ObjectMonitor::enter_for must succeed\");\n+  } else {\n+    \/\/ It is assumed that enter_for must enter on an object without contention.\n+    monitor = inflate_and_enter(obj(), locking_thread, current, ObjectSynchronizer::inflate_cause_monitor_enter);\n+  }\n+\n+  assert(monitor != nullptr, \"LightweightSynchronizer::enter_for must succeed\");\n+  cache_setter.set_monitor(monitor);\n+}\n+\n+void LightweightSynchronizer::enter(Handle obj, BasicLock* lock, JavaThread* current) {\n+  assert(LockingMode == LM_LIGHTWEIGHT, \"must be\");\n+  assert(current == JavaThread::current(), \"must be\");\n+\n+  if (obj->klass()->is_value_based()) {\n+    ObjectSynchronizer::handle_sync_on_value_based_class(obj, current);\n+  }\n+\n+  current->inc_held_monitor_count();\n+\n+  CacheSetter cache_setter(current, lock);\n+\n+  \/\/ Used when deflation is observed. Progress here requires progress\n+  \/\/ from the deflator. After observing the that the deflator is not\n+  \/\/ making progress (after two yields), switch to sleeping.\n+  SpinYield spin_yield(0, 2);\n+  bool observed_deflation = false;\n+\n+  LockStack& lock_stack = current->lock_stack();\n+\n+  if (!lock_stack.is_full() && lock_stack.try_recursive_enter(obj())) {\n+    \/\/ Recursively fast locked\n+    return;\n+  }\n+\n+  if (lock_stack.contains(obj())) {\n+    ObjectMonitor* monitor = inflate_fast_locked_object(obj(), current, current, ObjectSynchronizer::inflate_cause_monitor_enter);\n+    bool entered = monitor->enter(current);\n+    assert(entered, \"recursive ObjectMonitor::enter must succeed\");\n+    cache_setter.set_monitor(monitor);\n+    return;\n+  }\n+\n+  while (true) {\n+    \/\/ Fast-locking does not use the 'lock' argument.\n+    \/\/ Fast-lock spinning to avoid inflating for short critical sections.\n+    \/\/ The goal is to only inflate when the extra cost of using ObjectMonitors\n+    \/\/ is worth it.\n+    \/\/ If deflation has been observed we also spin while deflation is ongoing.\n+    if (fast_lock_try_enter(obj(), lock_stack, current)) {\n+      return;\n+    } else if (UseObjectMonitorTable && fast_lock_spin_enter(obj(), lock_stack, current, observed_deflation)) {\n+      return;\n+    }\n+\n+    if (observed_deflation) {\n+      spin_yield.wait();\n+    }\n+\n+    ObjectMonitor* monitor = inflate_and_enter(obj(), current, current, ObjectSynchronizer::inflate_cause_monitor_enter);\n+    if (monitor != nullptr) {\n+      cache_setter.set_monitor(monitor);\n+      return;\n+    }\n+\n+    \/\/ If inflate_and_enter returns nullptr it is because a deflated monitor\n+    \/\/ was encountered. Fallback to fast locking. The deflater is responsible\n+    \/\/ for clearing out the monitor and transitioning the markWord back to\n+    \/\/ fast locking.\n+    observed_deflation = true;\n+  }\n+}\n+\n+void LightweightSynchronizer::exit(oop object, JavaThread* current) {\n+  assert(LockingMode == LM_LIGHTWEIGHT, \"must be\");\n+  assert(current == Thread::current(), \"must be\");\n+\n+  markWord mark = object->mark();\n+  assert(!mark.is_unlocked(), \"must be\");\n+\n+  LockStack& lock_stack = current->lock_stack();\n+  if (mark.is_fast_locked()) {\n+    if (lock_stack.try_recursive_exit(object)) {\n+      \/\/ This is a recursive exit which succeeded\n+      return;\n+    }\n+    if (lock_stack.is_recursive(object)) {\n+      \/\/ Must inflate recursive locks if try_recursive_exit fails\n+      \/\/ This happens for un-structured unlocks, could potentially\n+      \/\/ fix try_recursive_exit to handle these.\n+      inflate_fast_locked_object(object, current, current, ObjectSynchronizer::inflate_cause_vm_internal);\n+    }\n+  }\n+\n+  while (mark.is_fast_locked()) {\n+    markWord unlocked_mark = mark.set_unlocked();\n+    markWord old_mark = mark;\n+    mark = object->cas_set_mark(unlocked_mark, old_mark);\n+    if (old_mark == mark) {\n+      \/\/ CAS successful, remove from lock_stack\n+      size_t recursion = lock_stack.remove(object) - 1;\n+      assert(recursion == 0, \"Should not have unlocked here\");\n+      return;\n+    }\n+  }\n+\n+  assert(mark.has_monitor(), \"must be\");\n+  \/\/ The monitor exists\n+  ObjectMonitor* monitor = ObjectSynchronizer::read_monitor(current, object, mark);\n+  if (monitor->is_owner_anonymous()) {\n+    assert(current->lock_stack().contains(object), \"current must have object on its lock stack\");\n+    monitor->set_owner_from_anonymous(current);\n+    monitor->set_recursions(current->lock_stack().remove(object) - 1);\n+  }\n+\n+  monitor->exit(current);\n+}\n+\n+\/\/ LightweightSynchronizer::inflate_locked_or_imse is used to to get an inflated\n+\/\/ ObjectMonitor* with LM_LIGHTWEIGHT. It is used from contexts which requires\n+\/\/ an inflated ObjectMonitor* for a monitor, and expects to throw a\n+\/\/ java.lang.IllegalMonitorStateException if it is not held by the current\n+\/\/ thread. Such as notify\/wait and jni_exit. LM_LIGHTWEIGHT keeps it invariant\n+\/\/ that it only inflates if it is already locked by the current thread or the\n+\/\/ current thread is in the process of entering. To maintain this invariant we\n+\/\/ need to throw a java.lang.IllegalMonitorStateException before inflating if\n+\/\/ the current thread is not the owner.\n+\/\/ LightweightSynchronizer::inflate_locked_or_imse facilitates this.\n+ObjectMonitor* LightweightSynchronizer::inflate_locked_or_imse(oop obj, ObjectSynchronizer::InflateCause cause, TRAPS) {\n+  assert(LockingMode == LM_LIGHTWEIGHT, \"must be\");\n+  JavaThread* current = THREAD;\n+\n+  for(;;) {\n+    markWord mark = obj->mark_acquire();\n+    if (mark.is_unlocked()) {\n+      \/\/ No lock, IMSE.\n+      THROW_MSG_(vmSymbols::java_lang_IllegalMonitorStateException(),\n+                \"current thread is not owner\", nullptr);\n+    }\n+\n+    if (mark.is_fast_locked()) {\n+      if (!current->lock_stack().contains(obj)) {\n+        \/\/ Fast locked by other thread, IMSE.\n+        THROW_MSG_(vmSymbols::java_lang_IllegalMonitorStateException(),\n+                  \"current thread is not owner\", nullptr);\n+      } else {\n+        \/\/ Current thread owns the lock, must inflate\n+        return inflate_fast_locked_object(obj, current, current, cause);\n+      }\n+    }\n+\n+    assert(mark.has_monitor(), \"must be\");\n+    ObjectMonitor* monitor = ObjectSynchronizer::read_monitor(current, obj, mark);\n+    if (monitor != nullptr) {\n+      if (monitor->is_owner_anonymous()) {\n+        LockStack& lock_stack = current->lock_stack();\n+        if (lock_stack.contains(obj)) {\n+          \/\/ Current thread owns the lock but someone else inflated\n+          \/\/ fix owner and pop lock stack\n+          monitor->set_owner_from_anonymous(current);\n+          monitor->set_recursions(lock_stack.remove(obj) - 1);\n+        } else {\n+          \/\/ Fast locked (and inflated) by other thread, or deflation in progress, IMSE.\n+          THROW_MSG_(vmSymbols::java_lang_IllegalMonitorStateException(),\n+                    \"current thread is not owner\", nullptr);\n+        }\n+      }\n+      return monitor;\n+    }\n+  }\n+}\n+\n+ObjectMonitor* LightweightSynchronizer::inflate_into_object_header(Thread* current, JavaThread* inflating_thread, oop object, ObjectSynchronizer::InflateCause cause) {\n+\n+  \/\/ The JavaThread* inflating_thread parameter is only used by LM_LIGHTWEIGHT and requires\n+  \/\/ that the inflating_thread == Thread::current() or is suspended throughout the call by\n+  \/\/ some other mechanism.\n+  \/\/ Even with LM_LIGHTWEIGHT the thread might be nullptr when called from a non\n+  \/\/ JavaThread. (As may still be the case from FastHashCode). However it is only\n+  \/\/ important for the correctness of the LM_LIGHTWEIGHT algorithm that the thread\n+  \/\/ is set when called from ObjectSynchronizer::enter from the owning thread,\n+  \/\/ ObjectSynchronizer::enter_for from any thread, or ObjectSynchronizer::exit.\n+  EventJavaMonitorInflate event;\n+\n+  for (;;) {\n+    const markWord mark = object->mark_acquire();\n+\n+    \/\/ The mark can be in one of the following states:\n+    \/\/ *  inflated     - Just return if using stack-locking.\n+    \/\/                   If using fast-locking and the ObjectMonitor owner\n+    \/\/                   is anonymous and the inflating_thread owns the\n+    \/\/                   object lock, then we make the inflating_thread\n+    \/\/                   the ObjectMonitor owner and remove the lock from\n+    \/\/                   the inflating_thread's lock stack.\n+    \/\/ *  fast-locked  - Coerce it to inflated from fast-locked.\n+    \/\/ *  unlocked     - Aggressively inflate the object.\n+\n+    \/\/ CASE: inflated\n+    if (mark.has_monitor()) {\n+      ObjectMonitor* inf = mark.monitor();\n+      markWord dmw = inf->header();\n+      assert(dmw.is_neutral(), \"invariant: header=\" INTPTR_FORMAT, dmw.value());\n+      if (inf->is_owner_anonymous() &&\n+          inflating_thread != nullptr && inflating_thread->lock_stack().contains(object)) {\n+        inf->set_owner_from_anonymous(inflating_thread);\n+        size_t removed = inflating_thread->lock_stack().remove(object);\n+        inf->set_recursions(removed - 1);\n+      }\n+      return inf;\n+    }\n+\n+    \/\/ CASE: fast-locked\n+    \/\/ Could be fast-locked either by the inflating_thread or by some other thread.\n+    \/\/\n+    \/\/ Note that we allocate the ObjectMonitor speculatively, _before_\n+    \/\/ attempting to set the object's mark to the new ObjectMonitor. If\n+    \/\/ the inflating_thread owns the monitor, then we set the ObjectMonitor's\n+    \/\/ owner to the inflating_thread. Otherwise, we set the ObjectMonitor's owner\n+    \/\/ to anonymous. If we lose the race to set the object's mark to the\n+    \/\/ new ObjectMonitor, then we just delete it and loop around again.\n+    \/\/\n+    if (mark.is_fast_locked()) {\n+      ObjectMonitor* monitor = new ObjectMonitor(object);\n+      monitor->set_header(mark.set_unlocked());\n+      bool own = inflating_thread != nullptr && inflating_thread->lock_stack().contains(object);\n+      if (own) {\n+        \/\/ Owned by inflating_thread.\n+        monitor->set_owner_from(nullptr, inflating_thread);\n+      } else {\n+        \/\/ Owned by somebody else.\n+        monitor->set_owner_anonymous();\n+      }\n+      markWord monitor_mark = markWord::encode(monitor);\n+      markWord old_mark = object->cas_set_mark(monitor_mark, mark);\n+      if (old_mark == mark) {\n+        \/\/ Success! Return inflated monitor.\n+        if (own) {\n+          size_t removed = inflating_thread->lock_stack().remove(object);\n+          monitor->set_recursions(removed - 1);\n+        }\n+        \/\/ Once the ObjectMonitor is configured and object is associated\n+        \/\/ with the ObjectMonitor, it is safe to allow async deflation:\n+        ObjectSynchronizer::_in_use_list.add(monitor);\n+\n+        \/\/ Hopefully the performance counters are allocated on distinct\n+        \/\/ cache lines to avoid false sharing on MP systems ...\n+        OM_PERFDATA_OP(Inflations, inc());\n+        log_inflate(current, object, cause);\n+        if (event.should_commit()) {\n+          post_monitor_inflate_event(&event, object, cause);\n+        }\n+        return monitor;\n+      } else {\n+        delete monitor;\n+        continue;  \/\/ Interference -- just retry\n+      }\n+    }\n+\n+    \/\/ CASE: unlocked\n+    \/\/ TODO-FIXME: for entry we currently inflate and then try to CAS _owner.\n+    \/\/ If we know we're inflating for entry it's better to inflate by swinging a\n+    \/\/ pre-locked ObjectMonitor pointer into the object header.   A successful\n+    \/\/ CAS inflates the object *and* confers ownership to the inflating thread.\n+    \/\/ In the current implementation we use a 2-step mechanism where we CAS()\n+    \/\/ to inflate and then CAS() again to try to swing _owner from null to current.\n+    \/\/ An inflateTry() method that we could call from enter() would be useful.\n+\n+    assert(mark.is_unlocked(), \"invariant: header=\" INTPTR_FORMAT, mark.value());\n+    ObjectMonitor* m = new ObjectMonitor(object);\n+    \/\/ prepare m for installation - set monitor to initial state\n+    m->set_header(mark);\n+\n+    if (object->cas_set_mark(markWord::encode(m), mark) != mark) {\n+      delete m;\n+      m = nullptr;\n+      continue;\n+      \/\/ interference - the markword changed - just retry.\n+      \/\/ The state-transitions are one-way, so there's no chance of\n+      \/\/ live-lock -- \"Inflated\" is an absorbing state.\n+    }\n+\n+    \/\/ Once the ObjectMonitor is configured and object is associated\n+    \/\/ with the ObjectMonitor, it is safe to allow async deflation:\n+    ObjectSynchronizer::_in_use_list.add(m);\n+\n+    \/\/ Hopefully the performance counters are allocated on distinct\n+    \/\/ cache lines to avoid false sharing on MP systems ...\n+    OM_PERFDATA_OP(Inflations, inc());\n+    log_inflate(current, object, cause);\n+    if (event.should_commit()) {\n+      post_monitor_inflate_event(&event, object, cause);\n+    }\n+    return m;\n+  }\n+}\n+\n+ObjectMonitor* LightweightSynchronizer::inflate_fast_locked_object(oop object, JavaThread* locking_thread, JavaThread* current, ObjectSynchronizer::InflateCause cause) {\n+  assert(LockingMode == LM_LIGHTWEIGHT, \"only used for lightweight\");\n+  VerifyThreadState vts(locking_thread, current);\n+  assert(locking_thread->lock_stack().contains(object), \"locking_thread must have object on its lock stack\");\n+\n+  ObjectMonitor* monitor;\n+\n+  if (!UseObjectMonitorTable) {\n+    return inflate_into_object_header(current, locking_thread, object, cause);\n+  }\n+\n+  \/\/ Inflating requires a hash code\n+  ObjectSynchronizer::FastHashCode(current, object);\n+\n+  markWord mark = object->mark_acquire();\n+  assert(!mark.is_unlocked(), \"Cannot be unlocked\");\n+\n+  for (;;) {\n+    \/\/ Fetch the monitor from the table\n+    monitor = get_or_insert_monitor(object, current, cause);\n+\n+    \/\/ ObjectMonitors are always inserted as anonymously owned, this thread is\n+    \/\/ the current holder of the monitor. So unless the entry is stale and\n+    \/\/ contains a deflating monitor it must be anonymously owned.\n+    if (monitor->is_owner_anonymous()) {\n+      \/\/ The monitor must be anonymously owned if it was added\n+      assert(monitor == get_monitor_from_table(current, object), \"The monitor must be found\");\n+      \/\/ New fresh monitor\n+      break;\n+    }\n+\n+    \/\/ If the monitor was not anonymously owned then we got a deflating monitor\n+    \/\/ from the table. We need to let the deflator make progress and remove this\n+    \/\/ entry before we are allowed to add a new one.\n+    os::naked_yield();\n+    assert(monitor->is_being_async_deflated(), \"Should be the reason\");\n+  }\n+\n+  \/\/ Set the mark word; loop to handle concurrent updates to other parts of the mark word\n+  while (mark.is_fast_locked()) {\n+    mark = object->cas_set_mark(mark.set_has_monitor(), mark);\n+  }\n+\n+  \/\/ Indicate that the monitor now has a known owner\n+  monitor->set_owner_from_anonymous(locking_thread);\n+\n+  \/\/ Remove the entry from the thread's lock stack\n+  monitor->set_recursions(locking_thread->lock_stack().remove(object) - 1);\n+\n+  if (locking_thread == current) {\n+    \/\/ Only change the thread local state of the current thread.\n+    locking_thread->om_set_monitor_cache(monitor);\n+  }\n+\n+  return monitor;\n+}\n+\n+ObjectMonitor* LightweightSynchronizer::inflate_and_enter(oop object, JavaThread* locking_thread, JavaThread* current, ObjectSynchronizer::InflateCause cause) {\n+  assert(LockingMode == LM_LIGHTWEIGHT, \"only used for lightweight\");\n+  VerifyThreadState vts(locking_thread, current);\n+\n+  \/\/ Note: In some paths (deoptimization) the 'current' thread inflates and\n+  \/\/ enters the lock on behalf of the 'locking_thread' thread.\n+\n+  ObjectMonitor* monitor = nullptr;\n+\n+  if (!UseObjectMonitorTable) {\n+    \/\/ Do the old inflate and enter.\n+    monitor = inflate_into_object_header(current, locking_thread, object, cause);\n+\n+    bool entered;\n+    if (locking_thread == current) {\n+      entered = monitor->enter(locking_thread);\n+    } else {\n+      entered = monitor->enter_for(locking_thread);\n+    }\n+\n+    \/\/ enter returns false for deflation found.\n+    return entered ? monitor : nullptr;\n+  }\n+\n+  NoSafepointVerifier nsv;\n+\n+  \/\/ Lightweight monitors require that hash codes are installed first\n+  ObjectSynchronizer::FastHashCode(locking_thread, object);\n+\n+  \/\/ Try to get the monitor from the thread-local cache.\n+  \/\/ There's no need to use the cache if we are locking\n+  \/\/ on behalf of another thread.\n+  if (current == locking_thread) {\n+    monitor = current->om_get_from_monitor_cache(object);\n+  }\n+\n+  \/\/ Get or create the monitor\n+  if (monitor == nullptr) {\n+    monitor = get_or_insert_monitor(object, current, cause);\n+  }\n+\n+  if (monitor->try_enter(locking_thread)) {\n+    return monitor;\n+  }\n+\n+  \/\/ Holds is_being_async_deflated() stable throughout this function.\n+  ObjectMonitorContentionMark contention_mark(monitor);\n+\n+  \/\/\/ First handle the case where the monitor from the table is deflated\n+  if (monitor->is_being_async_deflated()) {\n+    \/\/ The MonitorDeflation thread is deflating the monitor. The locking thread\n+    \/\/ must spin until further progress have been made.\n+\n+    const markWord mark = object->mark_acquire();\n+\n+    if (mark.has_monitor()) {\n+      \/\/ Waiting on the deflation thread to remove the deflated monitor from the table.\n+      os::naked_yield();\n+\n+    } else if (mark.is_fast_locked()) {\n+      \/\/ Some other thread managed to fast-lock the lock, or this is a\n+      \/\/ recursive lock from the same thread; yield for the deflation\n+      \/\/ thread to remove the deflated monitor from the table.\n+      os::naked_yield();\n+\n+    } else {\n+      assert(mark.is_unlocked(), \"Implied\");\n+      \/\/ Retry immediately\n+    }\n+\n+    \/\/ Retry\n+    return nullptr;\n+  }\n+\n+  for (;;) {\n+    const markWord mark = object->mark_acquire();\n+    \/\/ The mark can be in one of the following states:\n+    \/\/ *  inflated     - If the ObjectMonitor owner is anonymous\n+    \/\/                   and the locking_thread thread owns the object\n+    \/\/                   lock, then we make the locking_thread thread\n+    \/\/                   the ObjectMonitor owner and remove the\n+    \/\/                   lock from the locking_thread thread's lock stack.\n+    \/\/ *  fast-locked  - Coerce it to inflated from fast-locked.\n+    \/\/ *  neutral      - Inflate the object. Successful CAS is locked\n+\n+    \/\/ CASE: inflated\n+    if (mark.has_monitor()) {\n+      LockStack& lock_stack = locking_thread->lock_stack();\n+      if (monitor->is_owner_anonymous() && lock_stack.contains(object)) {\n+        \/\/ The lock is fast-locked by the locking thread,\n+        \/\/ convert it to a held monitor with a known owner.\n+        monitor->set_owner_from_anonymous(locking_thread);\n+        monitor->set_recursions(lock_stack.remove(object) - 1);\n+      }\n+\n+      break; \/\/ Success\n+    }\n+\n+    \/\/ CASE: fast-locked\n+    \/\/ Could be fast-locked either by locking_thread or by some other thread.\n+    \/\/\n+    if (mark.is_fast_locked()) {\n+      markWord old_mark = object->cas_set_mark(mark.set_has_monitor(), mark);\n+      if (old_mark != mark) {\n+        \/\/ CAS failed\n+        continue;\n+      }\n+\n+      \/\/ Success! Return inflated monitor.\n+      LockStack& lock_stack = locking_thread->lock_stack();\n+      if (lock_stack.contains(object)) {\n+        \/\/ The lock is fast-locked by the locking thread,\n+        \/\/ convert it to a held monitor with a known owner.\n+        monitor->set_owner_from_anonymous(locking_thread);\n+        monitor->set_recursions(lock_stack.remove(object) - 1);\n+      }\n+\n+      break; \/\/ Success\n+    }\n+\n+    \/\/ CASE: neutral (unlocked)\n+\n+    \/\/ Catch if the object's header is not neutral (not locked and\n+    \/\/ not marked is what we care about here).\n+    assert(mark.is_neutral(), \"invariant: header=\" INTPTR_FORMAT, mark.value());\n+    markWord old_mark = object->cas_set_mark(mark.set_has_monitor(), mark);\n+    if (old_mark != mark) {\n+      \/\/ CAS failed\n+      continue;\n+    }\n+\n+    \/\/ Transitioned from unlocked to monitor means locking_thread owns the lock.\n+    monitor->set_owner_from_anonymous(locking_thread);\n+\n+    return monitor;\n+  }\n+\n+  if (current == locking_thread) {\n+    \/\/ One round of spinning\n+    if (monitor->spin_enter(locking_thread)) {\n+      return monitor;\n+    }\n+\n+    \/\/ Monitor is contended, take the time before entering to fix the lock stack.\n+    LockStackInflateContendedLocks().inflate(current);\n+  }\n+\n+  \/\/ enter can block for safepoints; clear the unhandled object oop\n+  PauseNoSafepointVerifier pnsv(&nsv);\n+  object = nullptr;\n+\n+  if (current == locking_thread) {\n+    monitor->enter_with_contention_mark(locking_thread, contention_mark);\n+  } else {\n+    monitor->enter_for_with_contention_mark(locking_thread, contention_mark);\n+  }\n+\n+  return monitor;\n+}\n+\n+void LightweightSynchronizer::deflate_monitor(Thread* current, oop obj, ObjectMonitor* monitor) {\n+  if (obj != nullptr) {\n+    deflate_mark_word(obj);\n+  }\n+  bool removed = remove_monitor(current, obj, monitor);\n+  if (obj != nullptr) {\n+    assert(removed, \"Should have removed the entry if obj was alive\");\n+  }\n+}\n+\n+ObjectMonitor* LightweightSynchronizer::get_monitor_from_table(Thread* current, oop obj) {\n+  assert(UseObjectMonitorTable, \"must be\");\n+  return _omworld->monitor_get(current, obj);\n+}\n+\n+bool LightweightSynchronizer::contains_monitor(Thread* current, ObjectMonitor* monitor) {\n+  assert(UseObjectMonitorTable, \"must be\");\n+  return _omworld->contains_monitor(current, monitor);\n+}\n+\n+bool LightweightSynchronizer::quick_enter(oop obj, JavaThread* current, BasicLock* lock) {\n+  assert(current->thread_state() == _thread_in_Java, \"must be\");\n+  assert(obj != nullptr, \"must be\");\n+  NoSafepointVerifier nsv;\n+\n+  CacheSetter cache_setter(current, lock);\n+\n+  LockStack& lock_stack = current->lock_stack();\n+  if (lock_stack.is_full()) {\n+    \/\/ Always go into runtime if the lock stack is full.\n+    return false;\n+  }\n+\n+  const markWord mark = obj->mark();\n+\n+#ifndef _LP64\n+  \/\/ Only for 32bit which have limited support for fast locking outside the runtime.\n+  if (lock_stack.try_recursive_enter(obj)) {\n+    \/\/ Recursive lock successful.\n+    current->inc_held_monitor_count();\n+    \/\/ Clears object monitor cache, because ?\n+    return true;\n+  }\n+\n+  if (mark.is_unlocked()) {\n+    markWord locked_mark = mark.set_fast_locked();\n+    if (obj->cas_set_mark(locked_mark, mark) == mark) {\n+      \/\/ Successfully fast-locked, push object to lock-stack and return.\n+      lock_stack.push(obj);\n+      current->inc_held_monitor_count();\n+      return true;\n+    }\n+  }\n+#endif\n+\n+  if (mark.has_monitor()) {\n+    ObjectMonitor* const monitor = UseObjectMonitorTable ? current->om_get_from_monitor_cache(obj) :\n+                                                           ObjectSynchronizer::read_monitor(mark);\n+\n+    if (monitor == nullptr) {\n+      \/\/ Take the slow-path on a cache miss.\n+      return false;\n+    }\n+\n+    if (monitor->try_enter(current)) {\n+      \/\/ ObjectMonitor enter successful.\n+      cache_setter.set_monitor(monitor);\n+      current->inc_held_monitor_count();\n+      return true;\n+    }\n+  }\n+\n+  \/\/ Slow-path.\n+  return false;\n+}\n","filename":"src\/hotspot\/share\/runtime\/lightweightSynchronizer.cpp","additions":1231,"deletions":0,"binary":false,"changes":1231,"status":"added"},{"patch":"@@ -0,0 +1,82 @@\n+\/*\n+ * Copyright (c) 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef SHARE_RUNTIME_LIGHTWEIGHTSYNCHRONIZER_HPP\n+#define SHARE_RUNTIME_LIGHTWEIGHTSYNCHRONIZER_HPP\n+\n+#include \"memory\/allStatic.hpp\"\n+#include \"runtime\/javaThread.hpp\"\n+#include \"runtime\/objectMonitor.hpp\"\n+#include \"runtime\/synchronizer.hpp\"\n+\n+class ObjectMonitorTable;\n+\n+class LightweightSynchronizer : AllStatic {\n+private:\n+  static ObjectMonitorTable* _omworld;\n+\n+  static ObjectMonitor* get_or_insert_monitor_from_table(oop object, JavaThread* current, bool* inserted);\n+  static ObjectMonitor* get_or_insert_monitor(oop object, JavaThread* current, ObjectSynchronizer::InflateCause cause);\n+\n+  static ObjectMonitor* add_monitor(JavaThread* current, ObjectMonitor* monitor, oop obj);\n+  static bool remove_monitor(Thread* current, oop obj, ObjectMonitor* monitor);\n+\n+  static void deflate_mark_word(oop object);\n+\n+  static void ensure_lock_stack_space(JavaThread* current);\n+\n+  class CacheSetter;\n+  class LockStackInflateContendedLocks;\n+  class VerifyThreadState;\n+\n+ public:\n+  static void initialize();\n+\n+  static bool needs_resize();\n+  static bool resize_table(JavaThread* current);\n+\n+private:\n+  static inline bool fast_lock_try_enter(oop obj, LockStack& lock_stack, JavaThread* current);\n+  static bool fast_lock_spin_enter(oop obj, LockStack& lock_stack, JavaThread* current, bool observed_deflation);\n+\n+public:\n+  static void enter_for(Handle obj, BasicLock* lock, JavaThread* locking_thread);\n+  static void enter(Handle obj, BasicLock* lock, JavaThread* current);\n+  static void exit(oop object, JavaThread* current);\n+\n+  static ObjectMonitor* inflate_into_object_header(Thread* current, JavaThread* inflating_thread, oop object, ObjectSynchronizer::InflateCause cause);\n+  static ObjectMonitor* inflate_locked_or_imse(oop object, ObjectSynchronizer::InflateCause cause, TRAPS);\n+  static ObjectMonitor* inflate_fast_locked_object(oop object, JavaThread* locking_thread, JavaThread* current, ObjectSynchronizer::InflateCause cause);\n+  static ObjectMonitor* inflate_and_enter(oop object, JavaThread* locking_thread, JavaThread* current, ObjectSynchronizer::InflateCause cause);\n+\n+  static void deflate_monitor(Thread* current, oop obj, ObjectMonitor* monitor);\n+\n+  static ObjectMonitor* get_monitor_from_table(Thread* current, oop obj);\n+\n+  static bool contains_monitor(Thread* current, ObjectMonitor* monitor);\n+\n+  static bool quick_enter(oop obj, JavaThread* current, BasicLock* Lock);\n+};\n+\n+#endif \/\/ SHARE_RUNTIME_LIGHTWEIGHTSYNCHRONIZER_HPP\n","filename":"src\/hotspot\/share\/runtime\/lightweightSynchronizer.hpp","additions":82,"deletions":0,"binary":false,"changes":82,"status":"added"},{"patch":"@@ -32,0 +32,1 @@\n+#include \"runtime\/javaThread.inline.hpp\"\n@@ -37,0 +38,1 @@\n+#include \"runtime\/synchronizer.inline.hpp\"\n@@ -43,0 +45,1 @@\n+#include \"utilities\/sizes.hpp\"\n@@ -117,0 +120,8 @@\n+\n+OMCache::OMCache(JavaThread* jt) : _entries() {\n+  STATIC_ASSERT(std::is_standard_layout<OMCache>::value);\n+  STATIC_ASSERT(std::is_standard_layout<OMCache::OMCacheEntry>::value);\n+  STATIC_ASSERT(offsetof(OMCache, _null_sentinel) == offsetof(OMCache, _entries) +\n+                offsetof(OMCache::OMCacheEntry, _oop) +\n+                OMCache::CAPACITY * in_bytes(oop_to_oop_difference()));\n+}\n","filename":"src\/hotspot\/share\/runtime\/lockStack.cpp","additions":11,"deletions":0,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -35,0 +35,1 @@\n+class ObjectMonitor;\n@@ -39,0 +40,1 @@\n+class Thread;\n@@ -126,0 +128,25 @@\n+class OMCache {\n+  friend class VMStructs;\n+public:\n+  static constexpr int CAPACITY = 8;\n+\n+private:\n+  struct OMCacheEntry {\n+    oop _oop = nullptr;\n+    ObjectMonitor* _monitor = nullptr;\n+  } _entries[CAPACITY];\n+  const oop _null_sentinel = nullptr;\n+\n+public:\n+  static ByteSize entries_offset() { return byte_offset_of(OMCache, _entries); }\n+  static constexpr ByteSize oop_to_oop_difference() { return in_ByteSize(sizeof(OMCacheEntry)); }\n+  static constexpr ByteSize oop_to_monitor_difference() { return in_ByteSize(sizeof(oop)); }\n+\n+  explicit OMCache(JavaThread* jt);\n+\n+  inline ObjectMonitor* get_monitor(oop o);\n+  inline void set_monitor(ObjectMonitor* monitor);\n+  inline void clear();\n+\n+};\n+\n","filename":"src\/hotspot\/share\/runtime\/lockStack.hpp","additions":27,"deletions":0,"binary":false,"changes":27,"status":"modified"},{"patch":"@@ -34,0 +34,2 @@\n+#include \"runtime\/lightweightSynchronizer.hpp\"\n+#include \"runtime\/objectMonitor.inline.hpp\"\n@@ -225,0 +227,50 @@\n+inline void OMCache::set_monitor(ObjectMonitor *monitor) {\n+  const int end = OMCache::CAPACITY - 1;\n+\n+  oop obj = monitor->object_peek();\n+  assert(obj != nullptr, \"must be alive\");\n+  assert(monitor == LightweightSynchronizer::get_monitor_from_table(JavaThread::current(), obj), \"must exist in table\");\n+\n+  OMCacheEntry to_insert = {obj, monitor};\n+\n+  for (int i = 0; i < end; ++i) {\n+    if (_entries[i]._oop == obj ||\n+        _entries[i]._monitor == nullptr ||\n+        _entries[i]._monitor->is_being_async_deflated()) {\n+      \/\/ Use stale slot.\n+      _entries[i] = to_insert;\n+      return;\n+    }\n+    \/\/ Swap with the most recent value.\n+    ::swap(to_insert, _entries[i]);\n+  }\n+  _entries[end] = to_insert;\n+}\n+\n+inline ObjectMonitor* OMCache::get_monitor(oop o) {\n+  for (int i = 0; i < CAPACITY; ++i) {\n+    if (_entries[i]._oop == o) {\n+      assert(_entries[i]._monitor != nullptr, \"monitor must exist\");\n+      if (_entries[i]._monitor->is_being_async_deflated()) {\n+        \/\/ Bad monitor\n+        \/\/ Shift down rest\n+        for (; i < CAPACITY - 1; ++i) {\n+          _entries[i] = _entries[i + 1];\n+        }\n+        \/\/ Clear end\n+        _entries[i] = {};\n+        return nullptr;\n+      }\n+      return _entries[i]._monitor;\n+    }\n+  }\n+  return nullptr;\n+}\n+\n+inline void OMCache::clear() {\n+  for (size_t i = 0; i < CAPACITY; ++i) {\n+    \/\/ Clear\n+    _entries[i] = {};\n+  }\n+}\n+\n","filename":"src\/hotspot\/share\/runtime\/lockStack.inline.hpp","additions":52,"deletions":0,"binary":false,"changes":52,"status":"modified"},{"patch":"@@ -46,0 +46,1 @@\n+#include \"runtime\/lightweightSynchronizer.hpp\"\n@@ -56,0 +57,1 @@\n+#include \"utilities\/debug.hpp\"\n@@ -249,1 +251,1 @@\n-  _header(markWord::zero()),\n+  _metadata(0),\n@@ -275,4 +277,0 @@\n-oop ObjectMonitor::object_peek() const {\n-  return _object.peek();\n-}\n-\n@@ -300,0 +298,6 @@\n+#define assert_mark_word_consistency()                                         \\\n+  assert(UseObjectMonitorTable || object()->mark() == markWord::encode(this),  \\\n+         \"object mark must match encoded this: mark=\" INTPTR_FORMAT            \\\n+         \", encoded this=\" INTPTR_FORMAT, object()->mark().value(),            \\\n+         markWord::encode(this).value());\n+\n@@ -303,1 +307,17 @@\n-bool ObjectMonitor::enter_for(JavaThread* locking_thread) {\n+bool ObjectMonitor::enter_is_async_deflating() {\n+  if (is_being_async_deflated()) {\n+    if (!UseObjectMonitorTable) {\n+      const oop l_object = object();\n+      if (l_object != nullptr) {\n+        \/\/ Attempt to restore the header\/dmw to the object's header so that\n+        \/\/ we only retry once if the deflater thread happens to be slow.\n+        install_displaced_markword_in_object(l_object);\n+      }\n+    }\n+    return true;\n+  }\n+\n+  return false;\n+}\n+\n+void ObjectMonitor::enter_for_with_contention_mark(JavaThread* locking_thread, ObjectMonitorContentionMark& contention_mark) {\n@@ -308,0 +328,2 @@\n+  assert(contention_mark._monitor == this, \"must be\");\n+  assert(!is_being_async_deflated(), \"must be\");\n@@ -309,2 +331,2 @@\n-  \/\/ Block out deflation as soon as possible.\n-  add_to_contentions(1);\n+\n+  void* prev_owner = try_set_owner_from(nullptr, locking_thread);\n@@ -313,2 +335,0 @@\n-  if (!is_being_async_deflated()) {\n-    void* prev_owner = try_set_owner_from(nullptr, locking_thread);\n@@ -316,25 +336,12 @@\n-    if (prev_owner == nullptr) {\n-      assert(_recursions == 0, \"invariant\");\n-      success = true;\n-    } else if (prev_owner == locking_thread) {\n-      _recursions++;\n-      success = true;\n-    } else if (prev_owner == DEFLATER_MARKER) {\n-      \/\/ Racing with deflation.\n-      prev_owner = try_set_owner_from(DEFLATER_MARKER, locking_thread);\n-      if (prev_owner == DEFLATER_MARKER) {\n-        \/\/ Cancelled deflation. Increment contentions as part of the deflation protocol.\n-        add_to_contentions(1);\n-        success = true;\n-      } else if (prev_owner == nullptr) {\n-        \/\/ At this point we cannot race with deflation as we have both incremented\n-        \/\/ contentions, seen contention > 0 and seen a DEFLATER_MARKER.\n-        \/\/ success will only be false if this races with something other than\n-        \/\/ deflation.\n-        prev_owner = try_set_owner_from(nullptr, locking_thread);\n-        success = prev_owner == nullptr;\n-      }\n-    } else if (LockingMode == LM_LEGACY && locking_thread->is_lock_owned((address)prev_owner)) {\n-      assert(_recursions == 0, \"must be\");\n-      _recursions = 1;\n-      set_owner_from_BasicLock(prev_owner, locking_thread);\n+  if (prev_owner == nullptr) {\n+    assert(_recursions == 0, \"invariant\");\n+    success = true;\n+  } else if (prev_owner == locking_thread) {\n+    _recursions++;\n+    success = true;\n+  } else if (prev_owner == DEFLATER_MARKER) {\n+    \/\/ Racing with deflation.\n+    prev_owner = try_set_owner_from(DEFLATER_MARKER, locking_thread);\n+    if (prev_owner == DEFLATER_MARKER) {\n+      \/\/ Cancelled deflation. Increment contentions as part of the deflation protocol.\n+      add_to_contentions(1);\n@@ -342,0 +349,7 @@\n+    } else if (prev_owner == nullptr) {\n+      \/\/ At this point we cannot race with deflation as we have both incremented\n+      \/\/ contentions, seen contention > 0 and seen a DEFLATER_MARKER.\n+      \/\/ success will only be false if this races with something other than\n+      \/\/ deflation.\n+      prev_owner = try_set_owner_from(nullptr, locking_thread);\n+      success = prev_owner == nullptr;\n@@ -343,13 +357,5 @@\n-    assert(success, \"Failed to enter_for: locking_thread=\" INTPTR_FORMAT\n-           \", this=\" INTPTR_FORMAT \"{owner=\" INTPTR_FORMAT \"}, observed owner: \" INTPTR_FORMAT,\n-           p2i(locking_thread), p2i(this), p2i(owner_raw()), p2i(prev_owner));\n-  } else {\n-    \/\/ Async deflation is in progress and our contentions increment\n-    \/\/ above lost the race to async deflation. Undo the work and\n-    \/\/ force the caller to retry.\n-    const oop l_object = object();\n-    if (l_object != nullptr) {\n-      \/\/ Attempt to restore the header\/dmw to the object's header so that\n-      \/\/ we only retry once if the deflater thread happens to be slow.\n-      install_displaced_markword_in_object(l_object);\n-    }\n+  } else if (LockingMode == LM_LEGACY && locking_thread->is_lock_owned((address)prev_owner)) {\n+    assert(_recursions == 0, \"must be\");\n+    _recursions = 1;\n+    set_owner_from_BasicLock(prev_owner, locking_thread);\n+    success = true;\n@@ -357,0 +363,6 @@\n+  assert(success, \"Failed to enter_for: locking_thread=\" INTPTR_FORMAT\n+          \", this=\" INTPTR_FORMAT \"{owner=\" INTPTR_FORMAT \"}, observed owner: \" INTPTR_FORMAT,\n+          p2i(locking_thread), p2i(this), p2i(owner_raw()), p2i(prev_owner));\n+}\n+\n+bool ObjectMonitor::enter_for(JavaThread* locking_thread) {\n@@ -358,1 +370,2 @@\n-  add_to_contentions(-1);\n+  \/\/ Block out deflation as soon as possible.\n+  ObjectMonitorContentionMark contention_mark(this);\n@@ -360,1 +373,4 @@\n-  assert(!success || owner_raw() == locking_thread, \"must be\");\n+  \/\/ Check for deflation.\n+  if (enter_is_async_deflating()) {\n+    return false;\n+  }\n@@ -362,1 +378,3 @@\n-  return success;\n+  enter_for_with_contention_mark(locking_thread, contention_mark);\n+  assert(owner_raw() == locking_thread, \"must be\");\n+  return true;\n@@ -365,7 +383,4 @@\n-bool ObjectMonitor::enter(JavaThread* current) {\n-  assert(current == JavaThread::current(), \"must be\");\n-  \/\/ The following code is ordered to check the most common cases first\n-  \/\/ and to reduce RTS->RTO cache line upgrades on SPARC and IA32 processors.\n-\n-  void* cur = try_set_owner_from(nullptr, current);\n-  if (cur == nullptr) {\n+bool ObjectMonitor::try_enter(JavaThread* current) {\n+  \/\/ TryLock avoids the CAS\n+  TryLockResult r = TryLock(current);\n+  if (r == TryLockResult::Success) {\n@@ -376,2 +391,1 @@\n-  if (cur == current) {\n-    \/\/ TODO-FIXME: check for integer overflow!  BUGID 6557169.\n+  if (r == TryLockResult::HasOwner && owner() == current) {\n@@ -382,1 +396,2 @@\n-  if (LockingMode != LM_LIGHTWEIGHT && current->is_lock_owned((address)cur)) {\n+  void* cur = owner_raw();\n+  if (LockingMode == LM_LEGACY && current->is_lock_owned((address)cur)) {\n@@ -389,0 +404,16 @@\n+  return false;\n+}\n+\n+bool ObjectMonitor::spin_enter(JavaThread* current) {\n+  assert(current == JavaThread::current(), \"must be\");\n+\n+  \/\/ Check for recursion.\n+  if (try_enter(current)) {\n+    return true;\n+  }\n+\n+  \/\/ Check for deflation.\n+  if (enter_is_async_deflating()) {\n+    return false;\n+  }\n+\n@@ -391,3 +422,1 @@\n-  \/\/ Try one round of spinning *before* enqueueing current\n-  \/\/ and before going through the awkward and expensive state\n-  \/\/ transitions.  The following spin is strictly optional ...\n+  \/\/ Do one round of spinning.\n@@ -399,4 +428,11 @@\n-    assert(object()->mark() == markWord::encode(this),\n-           \"object mark must match encoded this: mark=\" INTPTR_FORMAT\n-           \", encoded this=\" INTPTR_FORMAT, object()->mark().value(),\n-           markWord::encode(this).value());\n+    assert_mark_word_consistency();\n+    return true;\n+  }\n+\n+  return false;\n+}\n+\n+bool ObjectMonitor::enter(JavaThread* current) {\n+  assert(current == JavaThread::current(), \"must be\");\n+\n+  if (spin_enter(current)) {\n@@ -411,13 +447,5 @@\n-  \/\/ Keep track of contention for JVM\/TI and M&M queries.\n-  add_to_contentions(1);\n-  if (is_being_async_deflated()) {\n-    \/\/ Async deflation is in progress and our contentions increment\n-    \/\/ above lost the race to async deflation. Undo the work and\n-    \/\/ force the caller to retry.\n-    const oop l_object = object();\n-    if (l_object != nullptr) {\n-      \/\/ Attempt to restore the header\/dmw to the object's header so that\n-      \/\/ we only retry once if the deflater thread happens to be slow.\n-      install_displaced_markword_in_object(l_object);\n-    }\n-    add_to_contentions(-1);\n+  \/\/ Keep is_being_async_deflated stable across the rest of enter\n+  ObjectMonitorContentionMark contention_mark(this);\n+\n+  \/\/ Check for deflation.\n+  if (enter_is_async_deflating()) {\n@@ -427,0 +455,11 @@\n+  \/\/ At this point this ObjectMonitor cannot be deflated, finish contended enter\n+  enter_with_contention_mark(current, contention_mark);\n+  return true;\n+}\n+\n+void ObjectMonitor::enter_with_contention_mark(JavaThread *current, ObjectMonitorContentionMark &cm) {\n+  assert(current == JavaThread::current(), \"must be\");\n+  assert(owner_raw() != current, \"must be\");\n+  assert(cm._monitor == this, \"must be\");\n+  assert(!is_being_async_deflated(), \"must be\");\n+\n@@ -483,1 +522,0 @@\n-  add_to_contentions(-1);\n@@ -490,1 +528,1 @@\n-  assert(object()->mark() == markWord::encode(this), \"invariant\");\n+  assert_mark_word_consistency();\n@@ -519,1 +557,0 @@\n-  return true;\n@@ -552,1 +589,1 @@\n-bool ObjectMonitor::deflate_monitor() {\n+bool ObjectMonitor::deflate_monitor(Thread* current) {\n@@ -623,0 +660,1 @@\n+  }\n@@ -624,2 +662,7 @@\n-    \/\/ Install the old mark word if nobody else has already done it.\n-    install_displaced_markword_in_object(obj);\n+  if (UseObjectMonitorTable) {\n+    LightweightSynchronizer::deflate_monitor(current, obj, this);\n+  } else {\n+    if (obj != nullptr) {\n+      \/\/ Install the old mark word if nobody else has already done it.\n+      install_displaced_markword_in_object(obj);\n+    }\n@@ -639,0 +682,1 @@\n+  assert(!UseObjectMonitorTable, \"Lightweight has no dmw\");\n@@ -975,0 +1019,1 @@\n+  assert(current->thread_state() != _thread_blocked, \"invariant\");\n@@ -978,3 +1023,1 @@\n-  assert(object()->mark() == markWord::encode(this), \"invariant\");\n-\n-  assert(current->thread_state() != _thread_blocked, \"invariant\");\n+  assert_mark_word_consistency();\n@@ -1045,1 +1088,1 @@\n-  assert(object()->mark() == markWord::encode(this), \"invariant\");\n+  assert_mark_word_consistency();\n@@ -1671,1 +1714,1 @@\n-  assert(object()->mark() == markWord::encode(this), \"invariant\");\n+  assert_mark_word_consistency();\n@@ -2188,1 +2231,1 @@\n-\/\/   _header = 0x0000000000000001\n+\/\/   _metadata = 0x0000000000000001\n@@ -2217,1 +2260,1 @@\n-  st->print_cr(\"  _header = \" INTPTR_FORMAT, header().value());\n+  st->print_cr(\"  _metadata = \" INTPTR_FORMAT, _metadata);\n","filename":"src\/hotspot\/share\/runtime\/objectMonitor.cpp","additions":137,"deletions":94,"binary":false,"changes":231,"status":"modified"},{"patch":"@@ -36,0 +36,1 @@\n+class ObjectMonitorContentionMark;\n@@ -72,1 +73,1 @@\n-\/\/ - The _header field must be at offset 0 because the displaced header\n+\/\/ - The _metadata field must be at offset 0 because the displaced header\n@@ -78,1 +79,1 @@\n-\/\/ - The _header and _owner fields should be separated by enough space\n+\/\/ - The _metadata and _owner fields should be separated by enough space\n@@ -82,1 +83,1 @@\n-\/\/     _header\n+\/\/     _metadata\n@@ -86,0 +87,1 @@\n+\/\/     <optional padding>\n@@ -109,10 +111,9 @@\n-\/\/   - Separating _owner from the <remaining_fields> by enough space to\n-\/\/     avoid false sharing might be profitable. Given\n-\/\/     http:\/\/blogs.oracle.com\/dave\/entry\/cas_and_cache_trivia_invalidate\n-\/\/     we know that the CAS in monitorenter will invalidate the line\n-\/\/     underlying _owner. We want to avoid an L1 data cache miss on that\n-\/\/     same line for monitorexit. Putting these <remaining_fields>:\n-\/\/     _recursions, _EntryList, _cxq, and _succ, all of which may be\n-\/\/     fetched in the inflated unlock path, on a different cache line\n-\/\/     would make them immune to CAS-based invalidation from the _owner\n-\/\/     field.\n+\/\/ - Separating _owner from the <remaining_fields> by enough space to\n+\/\/   avoid false sharing might be profitable. Given that the CAS in\n+\/\/   monitorenter will invalidate the line underlying _owner. We want\n+\/\/   to avoid an L1 data cache miss on that same line for monitorexit.\n+\/\/   Putting these <remaining_fields>:\n+\/\/   _recursions, _EntryList, _cxq, and _succ, all of which may be\n+\/\/   fetched in the inflated unlock path, on a different cache line\n+\/\/   would make them immune to CAS-based invalidation from the _owner\n+\/\/   field.\n@@ -120,3 +121,3 @@\n-\/\/   - The _recursions field should be of type int, or int32_t but not\n-\/\/     intptr_t. There's no reason to use a 64-bit type for this field\n-\/\/     in a 64-bit JVM.\n+\/\/ - The _recursions field should be of type int, or int32_t but not\n+\/\/   intptr_t. There's no reason to use a 64-bit type for this field\n+\/\/   in a 64-bit JVM.\n@@ -134,3 +135,7 @@\n-  \/\/ The sync code expects the header field to be at offset zero (0).\n-  \/\/ Enforced by the assert() in header_addr().\n-  volatile markWord _header;        \/\/ displaced object header word - mark\n+  \/\/ The sync code expects the metadata field to be at offset zero (0).\n+  \/\/ Enforced by the assert() in metadata_addr().\n+  \/\/ * LM_LIGHTWEIGHT with UseObjectMonitorTable:\n+  \/\/ Contains the _object's hashCode.\n+  \/\/ * LM_LEGACY, LM_MONITOR, LM_LIGHTWEIGHT without UseObjectMonitorTable:\n+  \/\/ Contains the displaced object header word - mark\n+  volatile uintptr_t _metadata;     \/\/ metadata\n@@ -138,2 +143,2 @@\n-  \/\/ Separate _header and _owner on different cache lines since both can\n-  \/\/ have busy multi-threaded access. _header and _object are set at initial\n+  \/\/ Separate _metadata and _owner on different cache lines since both can\n+  \/\/ have busy multi-threaded access. _metadata and _object are set at initial\n@@ -141,2 +146,2 @@\n-  \/\/ its cache line with _header.\n-  DEFINE_PAD_MINUS_SIZE(0, OM_CACHE_LINE_SIZE, sizeof(volatile markWord) +\n+  \/\/ its cache line with _metadata.\n+  DEFINE_PAD_MINUS_SIZE(0, OM_CACHE_LINE_SIZE, sizeof(_metadata) +\n@@ -152,1 +157,2 @@\n-  #define DEFLATER_MARKER reinterpret_cast<void*>(2)\n+  static const uintptr_t DEFLATER_MARKER_VALUE = 2;\n+  #define DEFLATER_MARKER reinterpret_cast<void*>(DEFLATER_MARKER_VALUE)\n@@ -184,1 +190,1 @@\n- protected:\n+\n@@ -187,1 +193,0 @@\n- private:\n@@ -216,0 +221,1 @@\n+  static ByteSize metadata_offset()    { return byte_offset_of(ObjectMonitor, _metadata); }\n@@ -236,3 +242,9 @@\n-  markWord           header() const;\n-  volatile markWord* header_addr();\n-  void               set_header(markWord hdr);\n+  uintptr_t           metadata() const;\n+  void                set_metadata(uintptr_t value);\n+  volatile uintptr_t* metadata_addr();\n+\n+  markWord            header() const;\n+  void                set_header(markWord hdr);\n+\n+  intptr_t            hash() const;\n+  void                set_hash(intptr_t hash);\n@@ -309,0 +321,3 @@\n+  bool      object_is_cleared() const;\n+  bool      object_is_dead() const;\n+  bool      object_refers_to(oop obj) const;\n@@ -331,0 +346,2 @@\n+\n+  bool      enter_is_async_deflating();\n@@ -332,0 +349,1 @@\n+  void      enter_for_with_contention_mark(JavaThread* locking_thread, ObjectMonitorContentionMark& contention_mark);\n@@ -334,0 +352,3 @@\n+  bool      try_enter(JavaThread* current);\n+  bool      spin_enter(JavaThread* current);\n+  void      enter_with_contention_mark(JavaThread* current, ObjectMonitorContentionMark& contention_mark);\n@@ -367,1 +388,2 @@\n-  bool      deflate_monitor();\n+  bool      deflate_monitor(Thread* current);\n+private:\n@@ -371,0 +393,14 @@\n+\/\/ RAII object to ensure that ObjectMonitor::is_being_async_deflated() is\n+\/\/ stable within the context of this mark.\n+class ObjectMonitorContentionMark : StackObj {\n+  DEBUG_ONLY(friend class ObjectMonitor;)\n+\n+  ObjectMonitor* _monitor;\n+\n+  NONCOPYABLE(ObjectMonitorContentionMark);\n+\n+public:\n+  explicit ObjectMonitorContentionMark(ObjectMonitor* monitor);\n+  ~ObjectMonitorContentionMark();\n+};\n+\n","filename":"src\/hotspot\/share\/runtime\/objectMonitor.hpp","additions":66,"deletions":30,"binary":false,"changes":96,"status":"modified"},{"patch":"@@ -32,0 +32,1 @@\n+#include \"oops\/markWord.hpp\"\n@@ -33,0 +34,1 @@\n+#include \"runtime\/globals.hpp\"\n@@ -35,0 +37,2 @@\n+#include \"utilities\/checkedCast.hpp\"\n+#include \"utilities\/globalDefinitions.hpp\"\n@@ -52,2 +56,6 @@\n-inline markWord ObjectMonitor::header() const {\n-  return Atomic::load(&_header);\n+inline uintptr_t ObjectMonitor::metadata() const {\n+  return Atomic::load(&_metadata);\n+}\n+\n+inline void ObjectMonitor::set_metadata(uintptr_t value) {\n+  Atomic::store(&_metadata, value);\n@@ -56,2 +64,9 @@\n-inline volatile markWord* ObjectMonitor::header_addr() {\n-  return &_header;\n+inline volatile uintptr_t* ObjectMonitor::metadata_addr() {\n+  STATIC_ASSERT(std::is_standard_layout<ObjectMonitor>::value);\n+  STATIC_ASSERT(offsetof(ObjectMonitor, _metadata) == 0);\n+  return &_metadata;\n+}\n+\n+inline markWord ObjectMonitor::header() const {\n+  assert(!UseObjectMonitorTable, \"Lightweight locking with OM table does not use header\");\n+  return markWord(metadata());\n@@ -61,1 +76,12 @@\n-  Atomic::store(&_header, hdr);\n+  assert(!UseObjectMonitorTable, \"Lightweight locking with OM table does not use header\");\n+  set_metadata(hdr.value());\n+}\n+\n+inline intptr_t ObjectMonitor::hash() const {\n+  assert(UseObjectMonitorTable, \"Only used by lightweight locking with OM table\");\n+  return metadata();\n+}\n+\n+inline void ObjectMonitor::set_hash(intptr_t hash) {\n+  assert(UseObjectMonitorTable, \"Only used by lightweight locking with OM table\");\n+  set_metadata(hash);\n@@ -183,0 +209,31 @@\n+inline ObjectMonitorContentionMark::ObjectMonitorContentionMark(ObjectMonitor* monitor)\n+  : _monitor(monitor) {\n+  _monitor->add_to_contentions(1);\n+}\n+\n+inline ObjectMonitorContentionMark::~ObjectMonitorContentionMark() {\n+  _monitor->add_to_contentions(-1);\n+}\n+\n+inline oop ObjectMonitor::object_peek() const {\n+  if (_object.is_null()) {\n+    return nullptr;\n+  }\n+  return _object.peek();\n+}\n+\n+inline bool ObjectMonitor::object_is_dead() const {\n+  return object_peek() == nullptr;\n+}\n+\n+inline bool ObjectMonitor::object_is_cleared() const {\n+  return _object.is_null();\n+}\n+\n+inline bool ObjectMonitor::object_refers_to(oop obj) const {\n+  if (_object.is_null()) {\n+    return false;\n+  }\n+  return _object.peek() == obj;\n+}\n+\n","filename":"src\/hotspot\/share\/runtime\/objectMonitor.inline.hpp","additions":62,"deletions":5,"binary":false,"changes":67,"status":"modified"},{"patch":"@@ -734,0 +734,5 @@\n+\n+  \/\/ The oops in the monitor cache are cleared to prevent stale cache entries\n+  \/\/ from keeping dead objects alive. Because these oops are always cleared\n+  \/\/ before safepoint operations they are not visited in JavaThread::oops_do.\n+  _thread->om_clear_monitor_cache();\n","filename":"src\/hotspot\/share\/runtime\/safepoint.cpp","additions":5,"deletions":0,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -44,0 +44,1 @@\n+#include \"runtime\/lightweightSynchronizer.hpp\"\n@@ -97,0 +98,1 @@\n+    bool omworldtable_work = false;\n@@ -124,1 +126,2 @@\n-              (oopmap_cache_work = OopMapCache::has_cleanup_work())\n+              (oopmap_cache_work = OopMapCache::has_cleanup_work()) |\n+              (omworldtable_work = LightweightSynchronizer::needs_resize())\n@@ -186,0 +189,4 @@\n+\n+    if (omworldtable_work) {\n+      LightweightSynchronizer::resize_table(jt);\n+    }\n","filename":"src\/hotspot\/share\/runtime\/serviceThread.cpp","additions":8,"deletions":1,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -61,0 +61,1 @@\n+#include \"runtime\/basicLock.inline.hpp\"\n@@ -72,1 +73,1 @@\n-#include \"runtime\/synchronizer.hpp\"\n+#include \"runtime\/synchronizer.inline.hpp\"\n@@ -79,0 +80,1 @@\n+#include \"utilities\/globalDefinitions.hpp\"\n@@ -2948,0 +2950,2 @@\n+      } else if (UseObjectMonitorTable) {\n+        buf[i] = (intptr_t)lock->object_monitor_cache();\n","filename":"src\/hotspot\/share\/runtime\/sharedRuntime.cpp","additions":5,"deletions":1,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -38,0 +38,1 @@\n+#include \"runtime\/basicLock.inline.hpp\"\n@@ -44,0 +45,1 @@\n+#include \"runtime\/lightweightSynchronizer.hpp\"\n@@ -55,1 +57,1 @@\n-#include \"runtime\/synchronizer.hpp\"\n+#include \"runtime\/synchronizer.inline.hpp\"\n@@ -279,0 +281,4 @@\n+\n+  if (LockingMode == LM_LIGHTWEIGHT) {\n+    LightweightSynchronizer::initialize();\n+  }\n@@ -352,1 +358,5 @@\n-    ObjectMonitor* const mon = mark.monitor();\n+    ObjectMonitor* const mon = read_monitor(current, obj, mark);\n+    if (LockingMode == LM_LIGHTWEIGHT && mon == nullptr) {\n+      \/\/ Racing with inflation\/deflation go slow path\n+      return false;\n+    }\n@@ -379,0 +389,7 @@\n+static bool useHeavyMonitors() {\n+#if defined(X86) || defined(AARCH64) || defined(PPC64) || defined(RISCV64) || defined(S390)\n+  return LockingMode == LM_MONITOR;\n+#else\n+  return false;\n+#endif\n+}\n@@ -386,1 +403,1 @@\n-bool ObjectSynchronizer::quick_enter(oop obj, JavaThread* current,\n+bool ObjectSynchronizer::quick_enter_legacy(oop obj, JavaThread* current,\n@@ -389,2 +406,0 @@\n-  NoSafepointVerifier nsv;\n-  if (obj == nullptr) return false;       \/\/ Need to throw NPE\n@@ -392,2 +407,2 @@\n-  if (obj->klass()->is_value_based()) {\n-    return false;\n+  if (useHeavyMonitors()) {\n+    return false;  \/\/ Slow path\n@@ -397,10 +412,1 @@\n-    LockStack& lock_stack = current->lock_stack();\n-    if (lock_stack.is_full()) {\n-      \/\/ Always go into runtime if the lock stack is full.\n-      return false;\n-    }\n-    if (lock_stack.try_recursive_enter(obj)) {\n-      \/\/ Recursive lock successful.\n-      current->inc_held_monitor_count();\n-      return true;\n-    }\n+    return LightweightSynchronizer::quick_enter(obj, current, lock);\n@@ -409,0 +415,2 @@\n+  assert(LockingMode == LM_LEGACY, \"legacy mode below\");\n+\n@@ -412,1 +420,2 @@\n-    ObjectMonitor* const m = mark.monitor();\n+\n+    ObjectMonitor* const m = read_monitor(mark);\n@@ -432,12 +441,10 @@\n-    if (LockingMode != LM_LIGHTWEIGHT) {\n-      \/\/ This Java Monitor is inflated so obj's header will never be\n-      \/\/ displaced to this thread's BasicLock. Make the displaced header\n-      \/\/ non-null so this BasicLock is not seen as recursive nor as\n-      \/\/ being locked. We do this unconditionally so that this thread's\n-      \/\/ BasicLock cannot be mis-interpreted by any stack walkers. For\n-      \/\/ performance reasons, stack walkers generally first check for\n-      \/\/ stack-locking in the object's header, the second check is for\n-      \/\/ recursive stack-locking in the displaced header in the BasicLock,\n-      \/\/ and last are the inflated Java Monitor (ObjectMonitor) checks.\n-      lock->set_displaced_header(markWord::unused_mark());\n-    }\n+    \/\/ This Java Monitor is inflated so obj's header will never be\n+    \/\/ displaced to this thread's BasicLock. Make the displaced header\n+    \/\/ non-null so this BasicLock is not seen as recursive nor as\n+    \/\/ being locked. We do this unconditionally so that this thread's\n+    \/\/ BasicLock cannot be mis-interpreted by any stack walkers. For\n+    \/\/ performance reasons, stack walkers generally first check for\n+    \/\/ stack-locking in the object's header, the second check is for\n+    \/\/ recursive stack-locking in the displaced header in the BasicLock,\n+    \/\/ and last are the inflated Java Monitor (ObjectMonitor) checks.\n+    lock->set_displaced_header(markWord::unused_mark());\n@@ -511,8 +518,0 @@\n-static bool useHeavyMonitors() {\n-#if defined(X86) || defined(AARCH64) || defined(PPC64) || defined(RISCV64) || defined(S390)\n-  return LockingMode == LM_MONITOR;\n-#else\n-  return false;\n-#endif\n-}\n-\n@@ -527,0 +526,5 @@\n+\n+  if (LockingMode == LM_LIGHTWEIGHT) {\n+    return LightweightSynchronizer::enter_for(obj, lock, locking_thread);\n+  }\n+\n@@ -543,2 +547,1 @@\n-void ObjectSynchronizer::enter(Handle obj, BasicLock* lock, JavaThread* current) {\n-  assert(current == Thread::current(), \"must be\");\n+void ObjectSynchronizer::enter_legacy(Handle obj, BasicLock* lock, JavaThread* current) {\n@@ -564,0 +567,1 @@\n+  assert(LockingMode != LM_LIGHTWEIGHT, \"Use LightweightSynchronizer\");\n@@ -572,55 +576,1 @@\n-    if (LockingMode == LM_LIGHTWEIGHT) {\n-      \/\/ Fast-locking does not use the 'lock' argument.\n-      LockStack& lock_stack = locking_thread->lock_stack();\n-      if (lock_stack.is_full()) {\n-        \/\/ We unconditionally make room on the lock stack by inflating\n-        \/\/ the least recently locked object on the lock stack.\n-\n-        \/\/ About the choice to inflate least recently locked object.\n-        \/\/ First we must chose to inflate a lock, either some lock on\n-        \/\/ the lock-stack or the lock that is currently being entered\n-        \/\/ (which may or may not be on the lock-stack).\n-        \/\/ Second the best lock to inflate is a lock which is entered\n-        \/\/ in a control flow where there are only a very few locks being\n-        \/\/ used, as the costly part of inflated locking is inflation,\n-        \/\/ not locking. But this property is entirely program dependent.\n-        \/\/ Third inflating the lock currently being entered on when it\n-        \/\/ is not present on the lock-stack will result in a still full\n-        \/\/ lock-stack. This creates a scenario where every deeper nested\n-        \/\/ monitorenter must call into the runtime.\n-        \/\/ The rational here is as follows:\n-        \/\/ Because we cannot (currently) figure out the second, and want\n-        \/\/ to avoid the third, we inflate a lock on the lock-stack.\n-        \/\/ The least recently locked lock is chosen as it is the lock\n-        \/\/ with the longest critical section.\n-\n-        log_info(monitorinflation)(\"LockStack capacity exceeded, inflating.\");\n-        ObjectMonitor* monitor = inflate_for(locking_thread, lock_stack.bottom(), inflate_cause_vm_internal);\n-        assert(monitor->owner() == Thread::current(), \"must be owner=\" PTR_FORMAT \" current=\" PTR_FORMAT \" mark=\" PTR_FORMAT,\n-               p2i(monitor->owner()), p2i(Thread::current()), monitor->object()->mark_acquire().value());\n-        assert(!lock_stack.is_full(), \"must have made room here\");\n-      }\n-\n-      markWord mark = obj()->mark_acquire();\n-      while (mark.is_unlocked()) {\n-        \/\/ Retry until a lock state change has been observed. cas_set_mark() may collide with non lock bits modifications.\n-        \/\/ Try to swing into 'fast-locked' state.\n-        assert(!lock_stack.contains(obj()), \"thread must not already hold the lock\");\n-        const markWord locked_mark = mark.set_fast_locked();\n-        const markWord old_mark = obj()->cas_set_mark(locked_mark, mark);\n-        if (old_mark == mark) {\n-          \/\/ Successfully fast-locked, push object to lock-stack and return.\n-          lock_stack.push(obj());\n-          return true;\n-        }\n-        mark = old_mark;\n-      }\n-\n-      if (mark.is_fast_locked() && lock_stack.try_recursive_enter(obj())) {\n-        \/\/ Recursive lock successful.\n-        return true;\n-      }\n-\n-      \/\/ Failed to fast lock.\n-      return false;\n-    } else if (LockingMode == LM_LEGACY) {\n+    if (LockingMode == LM_LEGACY) {\n@@ -659,2 +609,2 @@\n-void ObjectSynchronizer::exit(oop object, BasicLock* lock, JavaThread* current) {\n-  current->dec_held_monitor_count();\n+void ObjectSynchronizer::exit_legacy(oop object, BasicLock* lock, JavaThread* current) {\n+  assert(LockingMode != LM_LIGHTWEIGHT, \"Use LightweightSynchronizer\");\n@@ -664,26 +614,1 @@\n-    if (LockingMode == LM_LIGHTWEIGHT) {\n-      \/\/ Fast-locking does not use the 'lock' argument.\n-      LockStack& lock_stack = current->lock_stack();\n-      if (mark.is_fast_locked() && lock_stack.try_recursive_exit(object)) {\n-        \/\/ Recursively unlocked.\n-        return;\n-      }\n-\n-      if (mark.is_fast_locked() && lock_stack.is_recursive(object)) {\n-        \/\/ This lock is recursive but is not at the top of the lock stack so we're\n-        \/\/ doing an unbalanced exit. We have to fall thru to inflation below and\n-        \/\/ let ObjectMonitor::exit() do the unlock.\n-      } else {\n-        while (mark.is_fast_locked()) {\n-          \/\/ Retry until a lock state change has been observed. cas_set_mark() may collide with non lock bits modifications.\n-          const markWord unlocked_mark = mark.set_unlocked();\n-          const markWord old_mark = object->cas_set_mark(unlocked_mark, mark);\n-          if (old_mark == mark) {\n-            size_t recursions = lock_stack.remove(object) - 1;\n-            assert(recursions == 0, \"must not be recursive here\");\n-            return;\n-          }\n-          mark = old_mark;\n-        }\n-      }\n-    } else if (LockingMode == LM_LEGACY) {\n+    if (LockingMode == LM_LEGACY) {\n@@ -711,1 +636,1 @@\n-            ObjectMonitor* m = mark.monitor();\n+            ObjectMonitor* m = read_monitor(mark);\n@@ -755,2 +680,10 @@\n-    ObjectMonitor* monitor = inflate(current, obj(), inflate_cause_jni_enter);\n-    if (monitor->enter(current)) {\n+    ObjectMonitor* monitor;\n+    bool entered;\n+    if (LockingMode == LM_LIGHTWEIGHT) {\n+      entered = LightweightSynchronizer::inflate_and_enter(obj(), current, current, inflate_cause_jni_enter) != nullptr;\n+    } else {\n+      monitor = inflate(current, obj(), inflate_cause_jni_enter);\n+      entered = monitor->enter(current);\n+    }\n+\n+    if (entered) {\n@@ -768,3 +701,8 @@\n-  \/\/ The ObjectMonitor* can't be async deflated until ownership is\n-  \/\/ dropped inside exit() and the ObjectMonitor* must be !is_busy().\n-  ObjectMonitor* monitor = inflate(current, obj, inflate_cause_jni_exit);\n+  ObjectMonitor* monitor;\n+  if (LockingMode == LM_LIGHTWEIGHT) {\n+    monitor = LightweightSynchronizer::inflate_locked_or_imse(obj, inflate_cause_jni_exit, CHECK);\n+  } else {\n+    \/\/ The ObjectMonitor* can't be async deflated until ownership is\n+    \/\/ dropped inside exit() and the ObjectMonitor* must be !is_busy().\n+    monitor = inflate(current, obj, inflate_cause_jni_exit);\n+  }\n@@ -803,0 +741,1 @@\n+\n@@ -808,4 +747,10 @@\n-  \/\/ The ObjectMonitor* can't be async deflated because the _waiters\n-  \/\/ field is incremented before ownership is dropped and decremented\n-  \/\/ after ownership is regained.\n-  ObjectMonitor* monitor = inflate(current, obj(), inflate_cause_wait);\n+\n+  ObjectMonitor* monitor;\n+  if (LockingMode == LM_LIGHTWEIGHT) {\n+    monitor = LightweightSynchronizer::inflate_locked_or_imse(obj(), inflate_cause_wait, CHECK_0);\n+  } else {\n+    \/\/ The ObjectMonitor* can't be async deflated because the _waiters\n+    \/\/ field is incremented before ownership is dropped and decremented\n+    \/\/ after ownership is regained.\n+    monitor = inflate(current, obj(), inflate_cause_wait);\n+  }\n@@ -828,3 +773,8 @@\n-  ObjectSynchronizer::inflate(THREAD,\n-                              obj(),\n-                              inflate_cause_wait)->wait(millis, false, THREAD);\n+\n+  ObjectMonitor* monitor;\n+  if (LockingMode == LM_LIGHTWEIGHT) {\n+    monitor = LightweightSynchronizer::inflate_locked_or_imse(obj(), inflate_cause_wait, CHECK);\n+  } else {\n+    monitor = inflate(THREAD, obj(), inflate_cause_wait);\n+  }\n+  monitor->wait(millis, false, THREAD);\n@@ -849,3 +799,9 @@\n-  \/\/ The ObjectMonitor* can't be async deflated until ownership is\n-  \/\/ dropped by the calling thread.\n-  ObjectMonitor* monitor = inflate(current, obj(), inflate_cause_notify);\n+\n+  ObjectMonitor* monitor;\n+  if (LockingMode == LM_LIGHTWEIGHT) {\n+    monitor = LightweightSynchronizer::inflate_locked_or_imse(obj(), inflate_cause_notify, CHECK);\n+  } else {\n+    \/\/ The ObjectMonitor* can't be async deflated until ownership is\n+    \/\/ dropped by the calling thread.\n+    monitor = inflate(current, obj(), inflate_cause_notify);\n+  }\n@@ -871,3 +827,9 @@\n-  \/\/ The ObjectMonitor* can't be async deflated until ownership is\n-  \/\/ dropped by the calling thread.\n-  ObjectMonitor* monitor = inflate(current, obj(), inflate_cause_notify);\n+\n+  ObjectMonitor* monitor;\n+  if (LockingMode == LM_LIGHTWEIGHT) {\n+    monitor = LightweightSynchronizer::inflate_locked_or_imse(obj(), inflate_cause_notify, CHECK);\n+  } else {\n+    \/\/ The ObjectMonitor* can't be async deflated until ownership is\n+    \/\/ dropped by the calling thread.\n+    monitor = inflate(current, obj(), inflate_cause_notify);\n+  }\n@@ -971,1 +933,1 @@\n-static inline intptr_t get_next_hash(Thread* current, oop obj) {\n+static intptr_t get_next_hash(Thread* current, oop obj) {\n@@ -1011,0 +973,21 @@\n+static intptr_t install_hash_code(Thread* current, oop obj) {\n+  assert(UseObjectMonitorTable && LockingMode == LM_LIGHTWEIGHT, \"must be\");\n+\n+  markWord mark = obj->mark_acquire();\n+  for(;;) {\n+    intptr_t hash = mark.hash();\n+    if (hash != 0) {\n+      return hash;\n+    }\n+\n+    hash = get_next_hash(current, obj);\n+    const markWord old_mark = mark;\n+    const markWord new_mark = old_mark.copy_set_hash(hash);\n+\n+    mark = obj->cas_set_mark(new_mark, old_mark);\n+    if (old_mark == mark) {\n+      return hash;\n+    }\n+  }\n+}\n+\n@@ -1012,0 +995,4 @@\n+  \/\/ Since the monitor isn't in the object header, it can simply be installed.\n+  if (UseObjectMonitorTable) {\n+    return install_hash_code(current, obj);\n+  }\n@@ -1105,1 +1092,1 @@\n-      uintptr_t v = Atomic::cmpxchg((volatile uintptr_t*)monitor->header_addr(), mark.value(), temp.value());\n+      uintptr_t v = Atomic::cmpxchg(monitor->metadata_addr(), mark.value(), temp.value());\n@@ -1117,1 +1104,1 @@\n-      if (monitor->is_being_async_deflated()) {\n+      if (monitor->is_being_async_deflated() && !UseObjectMonitorTable) {\n@@ -1148,1 +1135,15 @@\n-  if (mark.has_monitor()) {\n+  while (LockingMode == LM_LIGHTWEIGHT && mark.has_monitor()) {\n+    ObjectMonitor* monitor = read_monitor(current, obj, mark);\n+    if (monitor != nullptr) {\n+      return monitor->is_entered(current) != 0;\n+    }\n+    \/\/ Racing with inflation\/deflation, retry\n+    mark = obj->mark_acquire();\n+\n+    if (mark.is_fast_locked()) {\n+      \/\/ Some other thread fast_locked, current could not have held the lock\n+      return false;\n+    }\n+  }\n+\n+  if (LockingMode != LM_LIGHTWEIGHT && mark.has_monitor()) {\n@@ -1152,1 +1153,1 @@\n-    ObjectMonitor* monitor = mark.monitor();\n+    ObjectMonitor* monitor = read_monitor(mark);\n@@ -1176,1 +1177,15 @@\n-  if (mark.has_monitor()) {\n+  while (LockingMode == LM_LIGHTWEIGHT && mark.has_monitor()) {\n+    ObjectMonitor* monitor = read_monitor(Thread::current(), obj, mark);\n+    if (monitor != nullptr) {\n+      return Threads::owning_thread_from_monitor(t_list, monitor);\n+    }\n+    \/\/ Racing with inflation\/deflation, retry\n+    mark = obj->mark_acquire();\n+\n+    if (mark.is_fast_locked()) {\n+      \/\/ Some other thread fast_locked\n+      return Threads::owning_thread_from_object(t_list, h_obj());\n+    }\n+  }\n+\n+  if (LockingMode != LM_LIGHTWEIGHT && mark.has_monitor()) {\n@@ -1180,1 +1195,1 @@\n-    ObjectMonitor* monitor = mark.monitor();\n+    ObjectMonitor* monitor = read_monitor(mark);\n@@ -1256,1 +1271,1 @@\n-    size_t new_ceiling = ceiling + (size_t)((double)ceiling * remainder) + 1;\n+    size_t new_ceiling = ceiling \/ remainder + 1;\n@@ -1392,0 +1407,1 @@\n+  assert(LockingMode != LM_LIGHTWEIGHT, \"only inflate through enter\");\n@@ -1394,1 +1410,1 @@\n-    ObjectMonitor* monitor = mark.monitor();\n+    ObjectMonitor* monitor = read_monitor(mark);\n@@ -1404,4 +1420,2 @@\n-  if (LockingMode == LM_LIGHTWEIGHT && current->is_Java_thread()) {\n-    return inflate_impl(JavaThread::cast(current), obj, cause);\n-  }\n-  return inflate_impl(nullptr, obj, cause);\n+  assert(LockingMode != LM_LIGHTWEIGHT, \"only inflate through enter\");\n+  return inflate_impl(obj, cause);\n@@ -1412,12 +1426,6 @@\n-  return inflate_impl(thread, obj, cause);\n-}\n-\n-ObjectMonitor* ObjectSynchronizer::inflate_impl(JavaThread* inflating_thread, oop object, const InflateCause cause) {\n-  \/\/ The JavaThread* inflating_thread parameter is only used by LM_LIGHTWEIGHT and requires\n-  \/\/ that the inflating_thread == Thread::current() or is suspended throughout the call by\n-  \/\/ some other mechanism.\n-  \/\/ Even with LM_LIGHTWEIGHT the thread might be nullptr when called from a non\n-  \/\/ JavaThread. (As may still be the case from FastHashCode). However it is only\n-  \/\/ important for the correctness of the LM_LIGHTWEIGHT algorithm that the thread\n-  \/\/ is set when called from ObjectSynchronizer::enter from the owning thread,\n-  \/\/ ObjectSynchronizer::enter_for from any thread, or ObjectSynchronizer::exit.\n+  assert(LockingMode != LM_LIGHTWEIGHT, \"LM_LIGHTWEIGHT cannot use inflate_for\");\n+  return inflate_impl(obj, cause);\n+}\n+\n+ObjectMonitor* ObjectSynchronizer::inflate_impl(oop object, const InflateCause cause) {\n+  assert(LockingMode != LM_LIGHTWEIGHT, \"LM_LIGHTWEIGHT cannot use inflate_impl\");\n@@ -1430,7 +1438,1 @@\n-    \/\/ *  inflated     - Just return if using stack-locking.\n-    \/\/                   If using fast-locking and the ObjectMonitor owner\n-    \/\/                   is anonymous and the inflating_thread owns the\n-    \/\/                   object lock, then we make the inflating_thread\n-    \/\/                   the ObjectMonitor owner and remove the lock from\n-    \/\/                   the inflating_thread's lock stack.\n-    \/\/ *  fast-locked  - Coerce it to inflated from fast-locked.\n+    \/\/ *  inflated     - Just return it.\n@@ -1447,6 +1449,0 @@\n-      if (LockingMode == LM_LIGHTWEIGHT && inf->is_owner_anonymous() &&\n-          inflating_thread != nullptr && inflating_thread->lock_stack().contains(object)) {\n-        inf->set_owner_from_anonymous(inflating_thread);\n-        size_t removed = inflating_thread->lock_stack().remove(object);\n-        inf->set_recursions(removed - 1);\n-      }\n@@ -1456,65 +1452,9 @@\n-    if (LockingMode != LM_LIGHTWEIGHT) {\n-      \/\/ New lightweight locking does not use INFLATING.\n-      \/\/ CASE: inflation in progress - inflating over a stack-lock.\n-      \/\/ Some other thread is converting from stack-locked to inflated.\n-      \/\/ Only that thread can complete inflation -- other threads must wait.\n-      \/\/ The INFLATING value is transient.\n-      \/\/ Currently, we spin\/yield\/park and poll the markword, waiting for inflation to finish.\n-      \/\/ We could always eliminate polling by parking the thread on some auxiliary list.\n-      if (mark == markWord::INFLATING()) {\n-        read_stable_mark(object);\n-        continue;\n-      }\n-    }\n-\n-    \/\/ CASE: fast-locked\n-    \/\/ Could be fast-locked either by the inflating_thread or by some other thread.\n-    \/\/\n-    \/\/ Note that we allocate the ObjectMonitor speculatively, _before_\n-    \/\/ attempting to set the object's mark to the new ObjectMonitor. If\n-    \/\/ the inflating_thread owns the monitor, then we set the ObjectMonitor's\n-    \/\/ owner to the inflating_thread. Otherwise, we set the ObjectMonitor's owner\n-    \/\/ to anonymous. If we lose the race to set the object's mark to the\n-    \/\/ new ObjectMonitor, then we just delete it and loop around again.\n-    \/\/\n-    LogStreamHandle(Trace, monitorinflation) lsh;\n-    if (LockingMode == LM_LIGHTWEIGHT && mark.is_fast_locked()) {\n-      ObjectMonitor* monitor = new ObjectMonitor(object);\n-      monitor->set_header(mark.set_unlocked());\n-      bool own = inflating_thread != nullptr && inflating_thread->lock_stack().contains(object);\n-      if (own) {\n-        \/\/ Owned by inflating_thread.\n-        monitor->set_owner_from(nullptr, inflating_thread);\n-      } else {\n-        \/\/ Owned by somebody else.\n-        monitor->set_owner_anonymous();\n-      }\n-      markWord monitor_mark = markWord::encode(monitor);\n-      markWord old_mark = object->cas_set_mark(monitor_mark, mark);\n-      if (old_mark == mark) {\n-        \/\/ Success! Return inflated monitor.\n-        if (own) {\n-          size_t removed = inflating_thread->lock_stack().remove(object);\n-          monitor->set_recursions(removed - 1);\n-        }\n-        \/\/ Once the ObjectMonitor is configured and object is associated\n-        \/\/ with the ObjectMonitor, it is safe to allow async deflation:\n-        _in_use_list.add(monitor);\n-\n-        \/\/ Hopefully the performance counters are allocated on distinct\n-        \/\/ cache lines to avoid false sharing on MP systems ...\n-        OM_PERFDATA_OP(Inflations, inc());\n-        if (log_is_enabled(Trace, monitorinflation)) {\n-          ResourceMark rm;\n-          lsh.print_cr(\"inflate(has_locker): object=\" INTPTR_FORMAT \", mark=\"\n-                       INTPTR_FORMAT \", type='%s'\", p2i(object),\n-                       object->mark().value(), object->klass()->external_name());\n-        }\n-        if (event.should_commit()) {\n-          post_monitor_inflate_event(&event, object, cause);\n-        }\n-        return monitor;\n-      } else {\n-        delete monitor;\n-        continue;  \/\/ Interference -- just retry\n-      }\n+    \/\/ CASE: inflation in progress - inflating over a stack-lock.\n+    \/\/ Some other thread is converting from stack-locked to inflated.\n+    \/\/ Only that thread can complete inflation -- other threads must wait.\n+    \/\/ The INFLATING value is transient.\n+    \/\/ Currently, we spin\/yield\/park and poll the markword, waiting for inflation to finish.\n+    \/\/ We could always eliminate polling by parking the thread on some auxiliary list.\n+    if (mark == markWord::INFLATING()) {\n+      read_stable_mark(object);\n+      continue;\n@@ -1534,0 +1474,1 @@\n+    LogStreamHandle(Trace, monitorinflation) lsh;\n@@ -1535,1 +1476,0 @@\n-      assert(LockingMode != LM_LIGHTWEIGHT, \"cannot happen with new lightweight locking\");\n@@ -1667,0 +1607,1 @@\n+  Thread* current = Thread::current();\n@@ -1673,1 +1614,1 @@\n-    if (mid->deflate_monitor()) {\n+    if (mid->deflate_monitor(current)) {\n@@ -1691,0 +1632,5 @@\n+    if (thread->is_Java_thread()) {\n+      \/\/ Clear OM cache\n+      JavaThread* jt = JavaThread::cast(thread);\n+      jt->om_clear_monitor_cache();\n+    }\n@@ -1837,0 +1783,8 @@\n+#ifdef ASSERT\n+    if (UseObjectMonitorTable) {\n+      for (ObjectMonitor* monitor : delete_list) {\n+        assert(!LightweightSynchronizer::contains_monitor(current, monitor), \"Should have been removed\");\n+      }\n+    }\n+#endif\n+\n@@ -2045,1 +1999,2 @@\n-  if (n->header().value() == 0) {\n+\n+  if (n->metadata() == 0) {\n@@ -2047,1 +2002,1 @@\n-                  \"have non-null _header field.\", p2i(n));\n+                  \"have non-null _metadata (header\/hash) field.\", p2i(n));\n@@ -2050,0 +2005,1 @@\n+\n@@ -2051,17 +2007,21 @@\n-  if (obj != nullptr) {\n-    const markWord mark = obj->mark();\n-    if (!mark.has_monitor()) {\n-      out->print_cr(\"ERROR: monitor=\" INTPTR_FORMAT \": in-use monitor's \"\n-                    \"object does not think it has a monitor: obj=\"\n-                    INTPTR_FORMAT \", mark=\" INTPTR_FORMAT, p2i(n),\n-                    p2i(obj), mark.value());\n-      *error_cnt_p = *error_cnt_p + 1;\n-    }\n-    ObjectMonitor* const obj_mon = mark.monitor();\n-    if (n != obj_mon) {\n-      out->print_cr(\"ERROR: monitor=\" INTPTR_FORMAT \": in-use monitor's \"\n-                    \"object does not refer to the same monitor: obj=\"\n-                    INTPTR_FORMAT \", mark=\" INTPTR_FORMAT \", obj_mon=\"\n-                    INTPTR_FORMAT, p2i(n), p2i(obj), mark.value(), p2i(obj_mon));\n-      *error_cnt_p = *error_cnt_p + 1;\n-    }\n+  if (obj == nullptr) {\n+    return;\n+  }\n+\n+  const markWord mark = obj->mark();\n+  if (!mark.has_monitor()) {\n+    out->print_cr(\"ERROR: monitor=\" INTPTR_FORMAT \": in-use monitor's \"\n+                  \"object does not think it has a monitor: obj=\"\n+                  INTPTR_FORMAT \", mark=\" INTPTR_FORMAT, p2i(n),\n+                  p2i(obj), mark.value());\n+    *error_cnt_p = *error_cnt_p + 1;\n+    return;\n+  }\n+\n+  ObjectMonitor* const obj_mon = read_monitor(Thread::current(), obj, mark);\n+  if (n != obj_mon) {\n+    out->print_cr(\"ERROR: monitor=\" INTPTR_FORMAT \": in-use monitor's \"\n+                  \"object does not refer to the same monitor: obj=\"\n+                  INTPTR_FORMAT \", mark=\" INTPTR_FORMAT \", obj_mon=\"\n+                  INTPTR_FORMAT, p2i(n), p2i(obj), mark.value(), p2i(obj_mon));\n+    *error_cnt_p = *error_cnt_p + 1;\n@@ -2090,1 +2050,1 @@\n-        const markWord mark = monitor->header();\n+        const intptr_t hash = UseObjectMonitorTable ? monitor->hash() : monitor->header().hash();\n@@ -2093,1 +2053,1 @@\n-                   monitor->is_busy(), mark.hash() != 0, monitor->owner() != nullptr,\n+                   monitor->is_busy(), hash != 0, monitor->owner() != nullptr,\n","filename":"src\/hotspot\/share\/runtime\/synchronizer.cpp","additions":228,"deletions":268,"binary":false,"changes":496,"status":"modified"},{"patch":"@@ -32,0 +32,1 @@\n+#include \"runtime\/javaThread.hpp\"\n@@ -96,2 +97,3 @@\n-  static void enter(Handle obj, BasicLock* lock, JavaThread* current);\n-  static void exit(oop obj, BasicLock* lock, JavaThread* current);\n+  static inline void enter(Handle obj, BasicLock* lock, JavaThread* current);\n+  static inline void exit(oop obj, BasicLock* lock, JavaThread* current);\n+\n@@ -109,0 +111,3 @@\n+  static bool quick_enter_legacy(oop obj, JavaThread* current, BasicLock* Lock);\n+  static void enter_legacy(Handle obj, BasicLock* Lock, JavaThread* current);\n+  static void exit_legacy(oop obj, BasicLock* lock, JavaThread* current);\n@@ -121,1 +126,1 @@\n-  static bool quick_enter(oop obj, JavaThread* current, BasicLock* Lock);\n+  static inline bool quick_enter(oop obj, JavaThread* current, BasicLock* Lock);\n@@ -135,1 +140,1 @@\n-  static ObjectMonitor* inflate_impl(JavaThread* thread, oop obj, const InflateCause cause);\n+  static ObjectMonitor* inflate_impl(oop obj, const InflateCause cause);\n@@ -142,0 +147,3 @@\n+  inline static ObjectMonitor* read_monitor(markWord mark);\n+  inline static ObjectMonitor* read_monitor(Thread* current, oop obj, markWord mark);\n+\n@@ -203,0 +211,1 @@\n+  friend class LightweightSynchronizer;\n","filename":"src\/hotspot\/share\/runtime\/synchronizer.hpp","additions":13,"deletions":4,"binary":false,"changes":17,"status":"modified"},{"patch":"@@ -0,0 +1,83 @@\n+\/*\n+ * Copyright (c) 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef SHARE_RUNTIME_SYNCHRONIZER_INLINE_HPP\n+#define SHARE_RUNTIME_SYNCHRONIZER_INLINE_HPP\n+\n+#include \"runtime\/synchronizer.hpp\"\n+\n+#include \"runtime\/lightweightSynchronizer.hpp\"\n+#include \"runtime\/safepointVerifiers.hpp\"\n+\n+inline ObjectMonitor* ObjectSynchronizer::read_monitor(markWord mark) {\n+  return mark.monitor();\n+}\n+\n+inline ObjectMonitor* ObjectSynchronizer::read_monitor(Thread* current, oop obj, markWord mark) {\n+  if (!UseObjectMonitorTable) {\n+    return read_monitor(mark);\n+  } else {\n+    return LightweightSynchronizer::get_monitor_from_table(current, obj);\n+  }\n+}\n+\n+inline void ObjectSynchronizer::enter(Handle obj, BasicLock* lock, JavaThread* current) {\n+  assert(current == Thread::current(), \"must be\");\n+\n+  if (LockingMode == LM_LIGHTWEIGHT) {\n+    LightweightSynchronizer::enter(obj, lock, current);\n+  } else {\n+    enter_legacy(obj, lock, current);\n+  }\n+}\n+\n+inline bool ObjectSynchronizer::quick_enter(oop obj, JavaThread* current,\n+                                     BasicLock * lock) {\n+  assert(current->thread_state() == _thread_in_Java, \"invariant\");\n+  NoSafepointVerifier nsv;\n+  if (obj == nullptr) return false;       \/\/ Need to throw NPE\n+\n+  if (obj->klass()->is_value_based()) {\n+    return false;\n+  }\n+\n+  if (LockingMode == LM_LIGHTWEIGHT) {\n+    return LightweightSynchronizer::quick_enter(obj, current, lock);\n+  } else {\n+    return quick_enter_legacy(obj, current, lock);\n+  }\n+}\n+\n+inline void ObjectSynchronizer::exit(oop object, BasicLock* lock, JavaThread* current) {\n+  current->dec_held_monitor_count();\n+\n+  if (LockingMode == LM_LIGHTWEIGHT) {\n+    LightweightSynchronizer::exit(object, current);\n+  } else {\n+    exit_legacy(object, lock, current);\n+  }\n+}\n+\n+\n+#endif \/\/ SHARE_RUNTIME_SYNCHRONIZER_INLINE_HPP\n","filename":"src\/hotspot\/share\/runtime\/synchronizer.inline.hpp","additions":83,"deletions":0,"binary":false,"changes":83,"status":"added"},{"patch":"@@ -53,1 +53,1 @@\n-#include \"runtime\/synchronizer.hpp\"\n+#include \"runtime\/synchronizer.inline.hpp\"\n@@ -250,3 +250,6 @@\n-          if (mark.has_monitor() &&\n-              ( \/\/ we have marked ourself as pending on this monitor\n-                mark.monitor() == thread()->current_pending_monitor() ||\n+          if (mark.has_monitor()) {\n+            ObjectMonitor* mon = ObjectSynchronizer::read_monitor(current, monitor->owner(), mark);\n+            if (\/\/ if the monitor is null we must be in the process of locking\n+                mon == nullptr ||\n+                \/\/ we have marked ourself as pending on this monitor\n+                mon == thread()->current_pending_monitor() ||\n@@ -254,3 +257,3 @@\n-                !mark.monitor()->is_entered(thread())\n-              )) {\n-            lock_state = \"waiting to lock\";\n+                !mon->is_entered(thread())) {\n+              lock_state = \"waiting to lock\";\n+            }\n","filename":"src\/hotspot\/share\/runtime\/vframe.cpp","additions":10,"deletions":7,"binary":false,"changes":17,"status":"modified"},{"patch":"@@ -781,1 +781,1 @@\n-  volatile_nonstatic_field(ObjectMonitor,      _header,                                       markWord)                              \\\n+  volatile_nonstatic_field(ObjectMonitor,      _metadata,                                     uintptr_t)                             \\\n@@ -785,1 +785,1 @@\n-  volatile_nonstatic_field(BasicLock,          _displaced_header,                             markWord)                              \\\n+  volatile_nonstatic_field(BasicLock,          _metadata,                                     uintptr_t)                             \\\n","filename":"src\/hotspot\/share\/runtime\/vmStructs.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -413,0 +413,1 @@\n+  static const Mutex::Rank DEFAULT_MUTEX_RANK = static_cast<Mutex::Rank>(static_cast<int>(Mutex::nosafepoint) - 2);\n@@ -417,1 +418,1 @@\n-                      Mutex::Rank rank = Mutex::nosafepoint-2,\n+                      Mutex::Rank rank = DEFAULT_MUTEX_RANK,\n","filename":"src\/hotspot\/share\/utilities\/concurrentHashTable.hpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -159,0 +159,10 @@\n+    if (VM.getVM().getCommandLineFlag(\"UseObjectMonitorTable\").getBool()) {\n+      Iterator it = ObjectSynchronizer.objectMonitorIterator();\n+      while (it != null && it.hasNext()) {\n+        ObjectMonitor mon = (ObjectMonitor)it.next();\n+        if (getAddress().equals(mon.object())) {\n+          return mon;\n+        }\n+      }\n+      return null;\n+    }\n","filename":"src\/jdk.hotspot.agent\/share\/classes\/sun\/jvm\/hotspot\/oops\/Mark.java","additions":10,"deletions":0,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -46,1 +46,1 @@\n-    displacedHeaderField = type.getCIntegerField(\"_displaced_header\");\n+    displacedHeaderField = type.getCIntegerField(\"_metadata\");\n","filename":"src\/jdk.hotspot.agent\/share\/classes\/sun\/jvm\/hotspot\/runtime\/BasicLock.java","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -48,2 +48,2 @@\n-    sun.jvm.hotspot.types.Field f = type.getField(\"_header\");\n-    headerFieldOffset = f.getOffset();\n+    sun.jvm.hotspot.types.Field f = type.getField(\"_metadata\");\n+    metadataFieldOffset = f.getOffset();\n@@ -68,1 +68,1 @@\n-    return new Mark(addr.addOffsetTo(headerFieldOffset));\n+    return new Mark(addr.addOffsetTo(metadataFieldOffset));\n@@ -117,1 +117,1 @@\n-  private static long          headerFieldOffset;\n+  private static long          metadataFieldOffset;\n","filename":"src\/jdk.hotspot.agent\/share\/classes\/sun\/jvm\/hotspot\/runtime\/ObjectMonitor.java","additions":4,"deletions":4,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -58,0 +58,3 @@\n+      if (VM.getVM().getCommandLineFlag(\"UseObjectMonitorTable\").getBool()) {\n+        return mark.hash();\n+      }\n","filename":"src\/jdk.hotspot.agent\/share\/classes\/sun\/jvm\/hotspot\/runtime\/ObjectSynchronizer.java","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -30,1 +30,1 @@\n- uint cache_line_size = VM_Version::L1_data_cache_line_size();\n+  uint cache_line_size = VM_Version::L1_data_cache_line_size();\n@@ -32,9 +32,1 @@\n- if (cache_line_size != 0) {\n-   \/\/ We were able to determine the L1 data cache line size so\n-   \/\/ do some cache line specific sanity checks\n-   EXPECT_EQ((size_t) 0, sizeof (PaddedEnd<ObjectMonitor>) % cache_line_size)\n-        << \"PaddedEnd<ObjectMonitor> size is not a \"\n-        << \"multiple of a cache line which permits false sharing. \"\n-        << \"sizeof(PaddedEnd<ObjectMonitor>) = \"\n-        << sizeof (PaddedEnd<ObjectMonitor>)\n-        << \"; cache_line_size = \" << cache_line_size;\n+  if (cache_line_size != 0) {\n@@ -42,3 +34,11 @@\n-   EXPECT_GE((size_t) in_bytes(ObjectMonitor::owner_offset()), cache_line_size)\n-        << \"the _header and _owner fields are closer \"\n-        << \"than a cache line which permits false sharing.\";\n+    EXPECT_EQ(in_bytes(ObjectMonitor::metadata_offset()), 0)\n+         << \"_metadata at a non 0 offset. metadata_offset = \"\n+         << in_bytes(ObjectMonitor::metadata_offset());\n+\n+    EXPECT_GE((size_t) in_bytes(ObjectMonitor::owner_offset()), cache_line_size)\n+         << \"the _metadata and _owner fields are closer \"\n+         << \"than a cache line which permits false sharing.\";\n+\n+    EXPECT_GE((size_t) in_bytes(ObjectMonitor::recursions_offset() - ObjectMonitor::owner_offset()), cache_line_size)\n+         << \"the _owner and _recursions fields are closer \"\n+         << \"than a cache line which permits false sharing.\";\n","filename":"test\/hotspot\/gtest\/runtime\/test_objectMonitor.cpp","additions":13,"deletions":13,"binary":false,"changes":26,"status":"modified"},{"patch":"@@ -0,0 +1,240 @@\n+\/*\n+ * Copyright (c) 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+\/**\n+ * @test id=NormalDeflation\n+ * @summary A collection of small tests using synchronized, wait, notify to try\n+ *          and achieve good cheap coverage of UseObjectMonitorTable.\n+ * @run main\/othervm -XX:+UnlockDiagnosticVMOptions\n+ *                   -XX:+UseObjectMonitorTable\n+ *                   UseObjectMonitorTableTest\n+ *\/\n+\n+\/**\n+ * @test id=ExtremeDeflation\n+ * @summary Run the same tests but with deflation running constantly.\n+ * @run main\/othervm -XX:+UnlockDiagnosticVMOptions\n+ *                   -XX:GuaranteedAsyncDeflationInterval=1\n+ *                   -XX:+UseObjectMonitorTable\n+ *                   UseObjectMonitorTableTest\n+ *\/\n+\n+import java.lang.Runnable;\n+import java.util.concurrent.BrokenBarrierException;\n+import java.util.concurrent.CyclicBarrier;\n+import java.util.concurrent.Executors;\n+import java.util.concurrent.ThreadFactory;\n+import java.util.Random;\n+import java.util.stream.Stream;\n+\n+public class UseObjectMonitorTableTest {\n+    static final ThreadFactory TF = Executors.defaultThreadFactory();\n+\n+    static class WaitNotifyTest implements Runnable {\n+        static final int ITERATIONS = 10_000;\n+        static final int THREADS = 10;\n+        final WaitNotifySyncChannel startLatchChannel = new WaitNotifySyncChannel();\n+        final WaitNotifySyncChannel endLatchChannel = new WaitNotifySyncChannel();\n+        int count = 0;\n+\n+        static class WaitNotifyCountDownLatch {\n+            int latch;\n+            WaitNotifyCountDownLatch(int count) {\n+                latch = count;\n+            }\n+            synchronized void await() {\n+                while (latch != 0) {\n+                    try {\n+                        wait();\n+                    } catch (InterruptedException e) {\n+                        throw new RuntimeException(\"WaitNotifyTest: Unexpected interrupt\", e);\n+                    }\n+                }\n+            }\n+            synchronized void countDown() {\n+                if (latch != 0) {\n+                    latch--;\n+                    if (latch == 0) {\n+                        notifyAll();\n+                    }\n+                }\n+            }\n+        }\n+        static class WaitNotifySyncChannel extends WaitNotifyCountDownLatch {\n+            WaitNotifyCountDownLatch object;\n+            WaitNotifySyncChannel() { super(0); }\n+            synchronized void send(WaitNotifyCountDownLatch object, int count) {\n+                await();\n+                latch = count;\n+                this.object = object;\n+                notifyAll();\n+            }\n+            synchronized WaitNotifyCountDownLatch receive() {\n+                while (latch == 0) {\n+                    try {\n+                        wait();\n+                    } catch (InterruptedException e) {\n+                        throw new RuntimeException(\"WaitNotifyTest: Unexpected interrupt\", e);\n+                    }\n+                }\n+                countDown();\n+                return object;\n+            }\n+        }\n+        synchronized int getCount() {\n+            return count;\n+        }\n+        synchronized void increment() {\n+            count++;\n+        }\n+        public void run() {\n+            System.out.println(\"WaitNotifyTest started.\");\n+            for (int t = 0; t < THREADS; t++) {\n+                TF.newThread(() -> {\n+                    for (int i = 0; i < ITERATIONS; i++) {\n+                        startLatchChannel.receive().await();\n+                        increment();\n+                        endLatchChannel.receive().countDown();\n+                    }\n+                }).start();\n+            }\n+            for (int i = 0; i < ITERATIONS; i++) {\n+                WaitNotifyCountDownLatch startLatch = new WaitNotifyCountDownLatch(1);\n+                WaitNotifyCountDownLatch endLatch = new WaitNotifyCountDownLatch(THREADS);\n+                int count = getCount();\n+                if (count != i * THREADS) {\n+                    throw new RuntimeException(\"WaitNotifyTest: Invalid Count \" + count +\n+                                               \" pre-iteration \" + i);\n+                }\n+                startLatchChannel.send(startLatch, 10);\n+                startLatch.countDown();\n+                endLatchChannel.send(endLatch, 10);\n+                endLatch.await();\n+            }\n+            int count = getCount();\n+            if (count != ITERATIONS * THREADS) {\n+                throw new RuntimeException(\"WaitNotifyTest: Invalid Count \" + count);\n+            }\n+            System.out.println(\"WaitNotifyTest passed.\");\n+        }\n+    }\n+\n+    static class RandomDepthTest implements Runnable {\n+        static final int THREADS = 10;\n+        static final int ITERATIONS = 10_000;\n+        static final int MAX_DEPTH = 20;\n+        static final int MAX_RECURSION_COUNT = 10;\n+        static final double RECURSION_CHANCE = .25;\n+        final Random random = new Random();\n+        final Locker lockers[] = new Locker[MAX_DEPTH];\n+        final CyclicBarrier syncBarrier = new CyclicBarrier(THREADS + 1);\n+        int count = 0;\n+\n+        class Locker {\n+            final int depth;\n+            Locker(int depth) {\n+                this.depth = depth;\n+            }\n+            synchronized int getCount() {\n+                if (depth == MAX_DEPTH) {\n+                    return count;\n+                }\n+                return lockers[depth].getCount();\n+            }\n+            synchronized void increment(int recursion_count) {\n+                if (recursion_count != MAX_RECURSION_COUNT &&\n+                    random.nextDouble() < RECURSION_CHANCE) {\n+                    this.increment(recursion_count + 1);\n+                    return;\n+                }\n+                if (depth == MAX_DEPTH) {\n+                    count++;\n+                    return;\n+                }\n+                lockers[depth + random.nextInt(MAX_DEPTH - depth)].increment(recursion_count);\n+            }\n+            synchronized Locker create() {\n+                if (depth != MAX_DEPTH) {\n+                    lockers[depth] = (new Locker(depth + 1)).create();\n+                }\n+                return this;\n+            }\n+        }\n+        int getCount() {\n+            return lockers[0].getCount();\n+        }\n+        void increment() {\n+            lockers[random.nextInt(MAX_DEPTH)].increment(0);\n+        }\n+        void create() {\n+            lockers[0] = (new Locker(1)).create();\n+        }\n+        void syncPoint() {\n+            try {\n+                syncBarrier.await();\n+            } catch (InterruptedException e) {\n+                throw new RuntimeException(\"RandomDepthTest: Unexpected interrupt\", e);\n+            } catch (BrokenBarrierException e) {\n+                throw new RuntimeException(\"RandomDepthTest: Unexpected broken barrier\", e);\n+            }\n+        }\n+        public void run() {\n+            System.out.println(\"RandomDepthTest started.\");\n+            for (int t = 0; t < THREADS; t++) {\n+                TF.newThread(() -> {\n+                    syncPoint();\n+                    for (int i = 0; i < ITERATIONS; i++) {\n+                        increment();\n+                    }\n+                    syncPoint();\n+                }).start();\n+            }\n+            create();\n+            syncPoint();\n+            syncPoint();\n+            int count = getCount();\n+            if (count != THREADS * ITERATIONS) {\n+                throw new RuntimeException(\"RandomDepthTest: Invalid Count \" + count);\n+            }\n+            System.out.println(\"RandomDepthTest passed.\");\n+        }\n+    }\n+\n+    public static void main(String[] args) {\n+        Stream.of(\n+            TF.newThread(new WaitNotifyTest()),\n+            TF.newThread(new RandomDepthTest())\n+        ).map(t -> {\n+            t.start();\n+            return t;\n+        }).forEach(t -> {\n+            try {\n+                t.join();\n+            } catch (InterruptedException e) {\n+                throw new RuntimeException(\"UseObjectMonitorTableTest: Unexpected interrupt\", e);\n+            }\n+        });\n+\n+        System.out.println(\"UseObjectMonitorTableTest passed.\");\n+    }\n+}\n","filename":"test\/hotspot\/jtreg\/runtime\/Monitor\/UseObjectMonitorTableTest.java","additions":240,"deletions":0,"binary":false,"changes":240,"status":"added"},{"patch":"@@ -41,1 +41,1 @@\n-        output.shouldContain(\"inflate(has_locker):\");\n+        output.shouldContain(\"inflate:\");\n","filename":"test\/hotspot\/jtreg\/runtime\/logging\/MonitorInflationTest.java","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -57,0 +57,2 @@\n+    public volatile Object lockObject3Inflated;\n+    public volatile Object lockObject4Inflated;\n@@ -65,0 +67,15 @@\n+        lockObject3Inflated = new Object();\n+        lockObject4Inflated = new Object();\n+\n+        \/\/ Inflate the lock to use an ObjectMonitor\n+        try {\n+          synchronized (lockObject3Inflated) {\n+            lockObject3Inflated.wait(1);\n+          }\n+          synchronized (lockObject4Inflated) {\n+            lockObject4Inflated.wait(1);\n+          }\n+        } catch (InterruptedException e) {\n+          throw new RuntimeException(e);\n+        }\n+\n@@ -71,1 +88,1 @@\n-    public void testSimpleLockUnlock() {\n+    public void testBasicSimpleLockUnlockLocal() {\n@@ -81,0 +98,34 @@\n+    \/** Perform a synchronized on an object within a loop. *\/\n+    @Benchmark\n+    public void testBasicSimpleLockUnlock() {\n+        for (int i = 0; i < innerCount; i++) {\n+            synchronized (lockObject1) {\n+                dummyInt1++;\n+                dummyInt2++;\n+            }\n+        }\n+    }\n+\n+    \/** Perform a synchronized on a local object within a loop. *\/\n+    @Benchmark\n+    public void testInflatedSimpleLockUnlockLocal() {\n+        Object localObject = lockObject3Inflated;\n+        for (int i = 0; i < innerCount; i++) {\n+            synchronized (localObject) {\n+                dummyInt1++;\n+                dummyInt2++;\n+            }\n+        }\n+    }\n+\n+    \/** Perform a synchronized on an object within a loop. *\/\n+    @Benchmark\n+    public void testInflatedSimpleLockUnlock() {\n+        for (int i = 0; i < innerCount; i++) {\n+            synchronized (lockObject3Inflated) {\n+                dummyInt1++;\n+                dummyInt2++;\n+            }\n+        }\n+    }\n+\n@@ -83,1 +134,1 @@\n-    public void testRecursiveLockUnlock() {\n+    public void testBasicRecursiveLockUnlockLocal() {\n@@ -95,0 +146,13 @@\n+    \/** Perform a recursive synchronized on an object within a loop. *\/\n+    @Benchmark\n+    public void testBasicRecursiveLockUnlock() {\n+        for (int i = 0; i < innerCount; i++) {\n+            synchronized (lockObject1) {\n+                synchronized (lockObject1) {\n+                    dummyInt1++;\n+                    dummyInt2++;\n+                }\n+            }\n+        }\n+    }\n+\n@@ -97,1 +161,1 @@\n-    public void testSerialLockUnlock() {\n+    public void testBasicSerialLockUnlockLocal() {\n@@ -109,0 +173,120 @@\n+  \/** Perform two synchronized after each other on the same object. *\/\n+    @Benchmark\n+    public void testBasicSerialLockUnlock() {\n+        for (int i = 0; i < innerCount; i++) {\n+            synchronized (lockObject1) {\n+                dummyInt1++;\n+            }\n+            synchronized (lockObject1) {\n+                dummyInt2++;\n+            }\n+        }\n+    }\n+\n+    \/** Perform two synchronized after each other on the same local object. *\/\n+    @Benchmark\n+    public void testInflatedSerialLockUnlockLocal() {\n+        Object localObject = lockObject3Inflated;\n+        for (int i = 0; i < innerCount; i++) {\n+            synchronized (localObject) {\n+                dummyInt1++;\n+            }\n+            synchronized (localObject) {\n+                dummyInt2++;\n+            }\n+        }\n+    }\n+\n+  \/** Perform two synchronized after each other on the same object. *\/\n+    @Benchmark\n+    public void testInflatedSerialLockUnlock() {\n+        for (int i = 0; i < innerCount; i++) {\n+            synchronized (lockObject3Inflated) {\n+                dummyInt1++;\n+            }\n+            synchronized (lockObject3Inflated) {\n+                dummyInt2++;\n+            }\n+        }\n+    }\n+\n+    \/** Perform two synchronized after each other on the same object. *\/\n+    @Benchmark\n+    public void testInflatedMultipleSerialLockUnlock() {\n+        for (int i = 0; i < innerCount; i++) {\n+            synchronized (lockObject3Inflated) {\n+                dummyInt1++;\n+            }\n+            synchronized (lockObject4Inflated) {\n+                dummyInt2++;\n+            }\n+        }\n+    }\n+\n+    \/** Perform two synchronized after each other on the same object. *\/\n+    @Benchmark\n+    public void testInflatedMultipleRecursiveLockUnlock() {\n+        for (int i = 0; i < innerCount; i++) {\n+            synchronized (lockObject3Inflated) {\n+                dummyInt1++;\n+                synchronized (lockObject4Inflated) {\n+                    dummyInt2++;\n+                }\n+            }\n+        }\n+    }\n+\n+      \/** Perform a recursive-only synchronized on a local object within a loop. *\/\n+    @Benchmark\n+    public void testInflatedRecursiveOnlyLockUnlockLocal() {\n+        Object localObject = lockObject3Inflated;\n+        synchronized (localObject) {\n+            for (int i = 0; i < innerCount; i++) {\n+                synchronized (localObject) {\n+                    dummyInt1++;\n+                    dummyInt2++;\n+                }\n+            }\n+        }\n+    }\n+\n+    \/** Perform a recursive-only synchronized on an object within a loop. *\/\n+    @Benchmark\n+    public void testInflatedRecursiveOnlyLockUnlock() {\n+        synchronized (lockObject3Inflated) {\n+            for (int i = 0; i < innerCount; i++) {\n+                synchronized (lockObject3Inflated) {\n+                    dummyInt1++;\n+                    dummyInt2++;\n+                }\n+            }\n+        }\n+    }\n+\n+    \/** Perform a recursive-only synchronized on a local object within a loop. *\/\n+    @Benchmark\n+    public void testInflatedRecursiveLockUnlockLocal() {\n+        Object localObject = lockObject3Inflated;\n+        for (int i = 0; i < innerCount; i++) {\n+            synchronized (localObject) {\n+                synchronized (localObject) {\n+                    dummyInt1++;\n+                    dummyInt2++;\n+                }\n+            }\n+        }\n+    }\n+\n+    \/** Perform a recursive-only synchronized on an object within a loop. *\/\n+    @Benchmark\n+    public void testInflatedRecursiveLockUnlock() {\n+        for (int i = 0; i < innerCount; i++) {\n+            synchronized (lockObject3Inflated) {\n+                synchronized (lockObject3Inflated) {\n+                    dummyInt1++;\n+                    dummyInt2++;\n+                }\n+            }\n+        }\n+    }\n+\n","filename":"test\/micro\/org\/openjdk\/bench\/vm\/lang\/LockUnlock.java","additions":187,"deletions":3,"binary":false,"changes":190,"status":"modified"}]}