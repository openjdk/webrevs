{"files":[{"patch":"@@ -88,0 +88,1 @@\n+#include \"runtime\/lockStack.hpp\"\n@@ -1850,0 +1851,8 @@\n+WB_ENTRY(jint, WB_getLockStackCapacity(JNIEnv* env))\n+  return (jint) LockStack::CAPACITY;\n+WB_END\n+\n+WB_ENTRY(jboolean, WB_supportsRecursiveLightweightLocking(JNIEnv* env))\n+  return (jboolean) VM_Version::supports_recursive_lightweight_locking();\n+WB_END\n+\n@@ -2832,0 +2841,2 @@\n+  {CC\"getLockStackCapacity\", CC\"()I\",                 (void*)&WB_getLockStackCapacity },\n+  {CC\"supportsRecursiveLightweightLocking\", CC\"()Z\",  (void*)&WB_supportsRecursiveLightweightLocking },\n","filename":"src\/hotspot\/share\/prims\/whitebox.cpp","additions":11,"deletions":0,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -190,0 +190,3 @@\n+  \/\/ Is recursive lightweight locking implemented for this platform?\n+  constexpr static bool supports_recursive_lightweight_locking() { return false; }\n+\n","filename":"src\/hotspot\/share\/runtime\/abstract_vm_version.hpp","additions":4,"deletions":1,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -4,0 +4,1 @@\n+ * Copyright (c) 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -28,0 +29,1 @@\n+#include \"runtime\/globals.hpp\"\n@@ -34,0 +36,2 @@\n+#include \"utilities\/debug.hpp\"\n+#include \"utilities\/globalDefinitions.hpp\"\n@@ -36,0 +40,2 @@\n+#include <type_traits>\n+\n@@ -42,0 +48,5 @@\n+  \/\/ Make sure the layout of the object is compatible with the emitted code's assumptions.\n+  STATIC_ASSERT(sizeof(_bad_oop_sentinel) == oopSize);\n+  STATIC_ASSERT(sizeof(_base[0]) == oopSize);\n+  STATIC_ASSERT(std::is_standard_layout<LockStack>::value);\n+  STATIC_ASSERT(offsetof(LockStack, _bad_oop_sentinel) == offsetof(LockStack, _base) - oopSize);\n@@ -65,1 +76,1 @@\n-  assert((_top >= start_offset()), \"lockstack underflow: _top %d end_offset %d\", _top, start_offset());\n+  assert((_top >= start_offset()), \"lockstack underflow: _top %d start_offset %d\", _top, start_offset());\n@@ -70,0 +81,10 @@\n+      if (VM_Version::supports_recursive_lightweight_locking()) {\n+        oop o = _base[i];\n+        for (; i < top - 1; i++) {\n+          \/\/ Consecutive entries may be the same\n+          if (_base[i + 1] != o) {\n+            break;\n+          }\n+        }\n+      }\n+\n","filename":"src\/hotspot\/share\/runtime\/lockStack.cpp","additions":22,"deletions":1,"binary":false,"changes":23,"status":"modified"},{"patch":"@@ -4,0 +4,1 @@\n+ * Copyright (c) 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -38,0 +39,1 @@\n+  friend class LockStackTest;\n@@ -40,1 +42,1 @@\n-private:\n+public:\n@@ -42,0 +44,1 @@\n+private:\n@@ -54,0 +57,3 @@\n+  \/\/ The _bad_oop_sentinel acts as a sentinel value to elide underflow checks in generated code.\n+  \/\/ The correct layout is statically asserted in the constructor.\n+  const uintptr_t _bad_oop_sentinel = badOopVal;\n@@ -78,2 +84,2 @@\n-  \/\/ Return true if we have room to push onto this lock-stack, false otherwise.\n-  inline bool can_push() const;\n+  \/\/ Returns true if the lock-stack is full. False otherwise.\n+  inline bool is_full() const;\n@@ -84,0 +90,19 @@\n+  \/\/ Get the oldest oop from this lock-stack.\n+  \/\/ Precondition: This lock-stack must not be empty.\n+  inline oop bottom() const;\n+\n+  \/\/ Is the lock-stack empty.\n+  inline bool is_empty() const;\n+\n+  \/\/ Check if object is recursive.\n+  \/\/ Precondition: This lock-stack must contain the oop.\n+  inline bool is_recursive(oop o) const;\n+\n+  \/\/ Try recursive enter.\n+  \/\/ Precondition: This lock-stack must not be full.\n+  inline bool try_recursive_enter(oop o);\n+\n+  \/\/ Try recursive exit.\n+  \/\/ Precondition: This lock-stack must contain the oop.\n+  inline bool try_recursive_exit(oop o);\n+\n@@ -85,1 +110,3 @@\n-  inline void remove(oop o);\n+  \/\/ Precondition: This lock-stack must contain the oop.\n+  \/\/ Returns the number of oops removed.\n+  inline size_t remove(oop o);\n","filename":"src\/hotspot\/share\/runtime\/lockStack.hpp","additions":31,"deletions":4,"binary":false,"changes":35,"status":"modified"},{"patch":"@@ -4,0 +4,1 @@\n+ * Copyright (c) 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -29,0 +30,2 @@\n+#include \"runtime\/lockStack.hpp\"\n+\n@@ -31,1 +34,0 @@\n-#include \"runtime\/lockStack.hpp\"\n@@ -35,0 +37,2 @@\n+#include \"utilities\/align.hpp\"\n+#include \"utilities\/globalDefinitions.hpp\"\n@@ -37,0 +41,3 @@\n+  assert(is_aligned(offset, oopSize), \"Bad alignment: %u\", offset);\n+  assert((offset <= end_offset()), \"lockstack overflow: offset %d end_offset %d\", offset, end_offset());\n+  assert((offset >= start_offset()), \"lockstack underflow: offset %d start_offset %d\", offset, start_offset());\n@@ -45,2 +52,2 @@\n-inline bool LockStack::can_push() const {\n-  return to_index(_top) < CAPACITY;\n+inline bool LockStack::is_full() const {\n+  return to_index(_top) == CAPACITY;\n@@ -64,1 +71,1 @@\n-  assert(can_push(), \"must have room\");\n+  assert(!is_full(), \"must have room\");\n@@ -71,1 +78,94 @@\n-inline void LockStack::remove(oop o) {\n+inline oop LockStack::bottom() const {\n+  assert(to_index(_top) > 0, \"must contain an oop\");\n+  return _base[0];\n+}\n+\n+inline bool LockStack::is_empty() const {\n+  return to_index(_top) == 0;\n+}\n+\n+inline bool LockStack::is_recursive(oop o) const {\n+  if (!VM_Version::supports_recursive_lightweight_locking()) {\n+    return false;\n+  }\n+  verify(\"pre-is_recursive\");\n+\n+  \/\/ This will succeed iff there is a consecutive run of oops on the\n+  \/\/ lock-stack with a length of at least 2.\n+\n+  assert(contains(o), \"at least one entry must exist\");\n+  int end = to_index(_top);\n+  \/\/ Start iterating from the top because the runtime code is more\n+  \/\/ interested in the balanced locking case when the top oop on the\n+  \/\/ lock-stack matches o. This will cause the for loop to break out\n+  \/\/ in the first loop iteration if it is non-recursive.\n+  for (int i = end - 1; i > 0; i--) {\n+    if (_base[i - 1] == o && _base[i] == o) {\n+      verify(\"post-is_recursive\");\n+      return true;\n+    }\n+    if (_base[i] == o) {\n+      \/\/ o can only occur in one consecutive run on the lock-stack.\n+      \/\/ Only one of the two oops checked matched o, so this run\n+      \/\/ must be of length 1 and thus not be recursive. Stop the search.\n+      break;\n+    }\n+  }\n+\n+  verify(\"post-is_recursive\");\n+  return false;\n+}\n+\n+inline bool LockStack::try_recursive_enter(oop o) {\n+  if (!VM_Version::supports_recursive_lightweight_locking()) {\n+    return false;\n+  }\n+  verify(\"pre-try_recursive_enter\");\n+\n+  \/\/ This will succeed iff the top oop on the stack matches o.\n+  \/\/ When successful o will be pushed to the lock-stack creating\n+  \/\/ a consecutive run at least 2 oops that matches o on top of\n+  \/\/ the lock-stack.\n+\n+  assert(!is_full(), \"precond\");\n+\n+  int end = to_index(_top);\n+  if (end == 0 || _base[end - 1] != o) {\n+    \/\/ Topmost oop does not match o.\n+    verify(\"post-try_recursive_enter\");\n+    return false;\n+  }\n+\n+  _base[end] = o;\n+  _top += oopSize;\n+  verify(\"post-try_recursive_enter\");\n+  return true;\n+}\n+\n+inline bool LockStack::try_recursive_exit(oop o) {\n+  if (!VM_Version::supports_recursive_lightweight_locking()) {\n+    return false;\n+  }\n+  verify(\"pre-try_recursive_exit\");\n+\n+  \/\/ This will succeed iff the top two oops on the stack matches o.\n+  \/\/ When successful the top oop will be popped of the lock-stack.\n+  \/\/ When unsuccessful the lock may still be recursive, in which\n+  \/\/ case the locking is unbalanced. This case is handled externally.\n+\n+  assert(contains(o), \"entries must exist\");\n+\n+  int end = to_index(_top);\n+  if (end <= 1 || _base[end - 1] != o || _base[end - 2] != o) {\n+    \/\/ The two topmost oops do not match o.\n+    verify(\"post-try_recursive_exit\");\n+    return false;\n+  }\n+\n+  _top -= oopSize;\n+  DEBUG_ONLY(_base[to_index(_top)] = nullptr;)\n+  verify(\"post-try_recursive_exit\");\n+  return true;\n+}\n+\n+inline size_t LockStack::remove(oop o) {\n@@ -74,0 +174,1 @@\n+\n@@ -75,0 +176,1 @@\n+  int inserted = 0;\n@@ -76,4 +178,3 @@\n-    if (_base[i] == o) {\n-      int last = end - 1;\n-      for (; i < last; i++) {\n-        _base[i] = _base[i + 1];\n+    if (_base[i] != o) {\n+      if (inserted != i) {\n+        _base[inserted] = _base[i];\n@@ -81,5 +182,1 @@\n-      _top -= oopSize;\n-#ifdef ASSERT\n-      _base[to_index(_top)] = nullptr;\n-#endif\n-      break;\n+      inserted++;\n@@ -88,1 +185,10 @@\n-  assert(!contains(o), \"entries must be unique: \" PTR_FORMAT, p2i(o));\n+\n+#ifdef ASSERT\n+  for (int i = inserted; i < end; i++) {\n+    _base[i] = nullptr;\n+  }\n+#endif\n+\n+  uint32_t removed = end - inserted;\n+  _top -= removed * oopSize;\n+  assert(!contains(o), \"entry must have been removed: \" PTR_FORMAT, p2i(o));\n@@ -90,0 +196,1 @@\n+  return removed;\n","filename":"src\/hotspot\/share\/runtime\/lockStack.inline.hpp","additions":122,"deletions":15,"binary":false,"changes":137,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1998, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1998, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -298,0 +298,1 @@\n+  void      set_recursions(size_t recursions);\n","filename":"src\/hotspot\/share\/runtime\/objectMonitor.hpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1998, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1998, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -105,0 +105,6 @@\n+inline void ObjectMonitor::set_recursions(size_t recursions) {\n+  assert(_recursions == 0, \"must be\");\n+  assert(has_owner(), \"must be owned\");\n+  _recursions = checked_cast<intx>(recursions);\n+}\n+\n","filename":"src\/hotspot\/share\/runtime\/objectMonitor.inline.hpp","additions":7,"deletions":1,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1998, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1998, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -39,0 +39,1 @@\n+#include \"runtime\/globals.hpp\"\n@@ -63,0 +64,1 @@\n+#include \"utilities\/globalDefinitions.hpp\"\n@@ -394,0 +396,13 @@\n+  if (LockingMode == LM_LIGHTWEIGHT) {\n+    LockStack& lock_stack = current->lock_stack();\n+    if (lock_stack.is_full()) {\n+      \/\/ Always go into runtime if the lock stack is full.\n+      return false;\n+    }\n+    if (lock_stack.try_recursive_enter(obj)) {\n+      \/\/ Recursive lock successful.\n+      current->inc_held_monitor_count();\n+      return true;\n+    }\n+  }\n+\n@@ -520,14 +535,40 @@\n-      if (lock_stack.can_push()) {\n-        markWord mark = obj()->mark_acquire();\n-        while (mark.is_neutral()) {\n-          \/\/ Retry until a lock state change has been observed.  cas_set_mark() may collide with non lock bits modifications.\n-          \/\/ Try to swing into 'fast-locked' state.\n-          assert(!lock_stack.contains(obj()), \"thread must not already hold the lock\");\n-          const markWord locked_mark = mark.set_fast_locked();\n-          const markWord old_mark = obj()->cas_set_mark(locked_mark, mark);\n-          if (old_mark == mark) {\n-            \/\/ Successfully fast-locked, push object to lock-stack and return.\n-            lock_stack.push(obj());\n-            return;\n-          }\n-          mark = old_mark;\n+      if (lock_stack.is_full()) {\n+        \/\/ We unconditionally make room on the lock stack by inflating\n+        \/\/ the least recently locked object on the lock stack.\n+\n+        \/\/ About the choice to inflate least recently locked object.\n+        \/\/ First we must chose to inflate a lock, either some lock on\n+        \/\/ the lock-stack or the lock that is currently being entered\n+        \/\/ (which may or may not be on the lock-stack).\n+        \/\/ Second the best lock to inflate is a lock which is entered\n+        \/\/ in a control flow where there are only a very few locks being\n+        \/\/ used, as the costly part of inflated locking is inflation,\n+        \/\/ not locking. But this property is entirely program dependent.\n+        \/\/ Third inflating the lock currently being entered on when it\n+        \/\/ is not present on the lock-stack will result in a still full\n+        \/\/ lock-stack. This creates a scenario where every deeper nested\n+        \/\/ monitorenter must call into the runtime.\n+        \/\/ The rational here is as follows:\n+        \/\/ Because we cannot (currently) figure out the second, and want\n+        \/\/ to avoid the third, we inflate a lock on the lock-stack.\n+        \/\/ The least recently locked lock is chosen as it is the lock\n+        \/\/ with the longest critical section.\n+\n+        log_info(fastlock)(\"LockStack capacity exceeded, inflating.\");\n+        ObjectMonitor* monitor = inflate(current, lock_stack.bottom(), inflate_cause_vm_internal);\n+        assert(monitor->owner() == current, \"must be owner=\" PTR_FORMAT \" current=\" PTR_FORMAT \" mark=\" PTR_FORMAT,\n+               p2i(monitor->owner()), p2i(current), monitor->object()->mark_acquire().value());\n+        assert(!lock_stack.is_full(), \"must have made room here\");\n+      }\n+\n+      markWord mark = obj()->mark_acquire();\n+      while (mark.is_neutral()) {\n+        \/\/ Retry until a lock state change has been observed. cas_set_mark() may collide with non lock bits modifications.\n+        \/\/ Try to swing into 'fast-locked' state.\n+        assert(!lock_stack.contains(obj()), \"thread must not already hold the lock\");\n+        const markWord locked_mark = mark.set_fast_locked();\n+        const markWord old_mark = obj()->cas_set_mark(locked_mark, mark);\n+        if (old_mark == mark) {\n+          \/\/ Successfully fast-locked, push object to lock-stack and return.\n+          lock_stack.push(obj());\n+          return;\n@@ -535,0 +576,6 @@\n+        mark = old_mark;\n+      }\n+\n+      if (mark.is_fast_locked() && lock_stack.try_recursive_enter(obj())) {\n+        \/\/ Recursive lock successful.\n+        return;\n@@ -583,7 +630,21 @@\n-      while (mark.is_fast_locked()) {\n-        \/\/ Retry until a lock state change has been observed.  cas_set_mark() may collide with non lock bits modifications.\n-        const markWord unlocked_mark = mark.set_unlocked();\n-        const markWord old_mark = object->cas_set_mark(unlocked_mark, mark);\n-        if (old_mark == mark) {\n-          current->lock_stack().remove(object);\n-          return;\n+      LockStack& lock_stack = current->lock_stack();\n+      if (mark.is_fast_locked() && lock_stack.try_recursive_exit(object)) {\n+        \/\/ Recursively unlocked.\n+        return;\n+      }\n+\n+      if (mark.is_fast_locked() && lock_stack.is_recursive(object)) {\n+        \/\/ This lock is recursive but is not at the top of the lock stack so we're\n+        \/\/ doing an unbalanced exit. We have to fall thru to inflation below and\n+        \/\/ let ObjectMonitor::exit() do the unlock.\n+      } else {\n+        while (mark.is_fast_locked()) {\n+          \/\/ Retry until a lock state change has been observed. cas_set_mark() may collide with non lock bits modifications.\n+          const markWord unlocked_mark = mark.set_unlocked();\n+          const markWord old_mark = object->cas_set_mark(unlocked_mark, mark);\n+          if (old_mark == mark) {\n+            size_t recursions = lock_stack.remove(object) - 1;\n+            assert(recursions == 0, \"must not be recursive here\");\n+            return;\n+          }\n+          mark = old_mark;\n@@ -591,1 +652,0 @@\n-        mark = old_mark;\n@@ -1326,1 +1386,2 @@\n-        JavaThread::cast(current)->lock_stack().remove(object);\n+        size_t removed = JavaThread::cast(current)->lock_stack().remove(object);\n+        inf->set_recursions(removed - 1);\n@@ -1372,1 +1433,2 @@\n-          JavaThread::cast(current)->lock_stack().remove(object);\n+          size_t removed = JavaThread::cast(current)->lock_stack().remove(object);\n+          monitor->set_recursions(removed - 1);\n","filename":"src\/hotspot\/share\/runtime\/synchronizer.cpp","additions":87,"deletions":25,"binary":false,"changes":112,"status":"modified"},{"patch":"@@ -0,0 +1,427 @@\n+\/*\n+ * Copyright (c) 2023, 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"runtime\/interfaceSupport.inline.hpp\"\n+#include \"runtime\/lockStack.inline.hpp\"\n+#include \"runtime\/os.hpp\"\n+#include \"unittest.hpp\"\n+#include \"utilities\/globalDefinitions.hpp\"\n+\n+class LockStackTest : public ::testing::Test {\n+public:\n+  static void push_raw(LockStack& ls, oop obj) {\n+    ls._base[ls.to_index(ls._top)] = obj;\n+    ls._top += oopSize;\n+  }\n+\n+  static void pop_raw(LockStack& ls) {\n+    ls._top -= oopSize;\n+#ifdef ASSERT\n+    ls._base[ls.to_index(ls._top)] = nullptr;\n+#endif\n+  }\n+\n+  static oop at(LockStack& ls, int index) {\n+    return ls._base[index];\n+  }\n+\n+  static size_t size(LockStack& ls) {\n+    return ls.to_index(ls._top);\n+  }\n+};\n+\n+#define recursive_enter(ls, obj)             \\\n+  do {                                       \\\n+    bool ret = ls.try_recursive_enter(obj);  \\\n+    EXPECT_TRUE(ret);                        \\\n+  } while (false)\n+\n+#define recursive_exit(ls, obj)             \\\n+  do {                                      \\\n+    bool ret = ls.try_recursive_exit(obj);  \\\n+    EXPECT_TRUE(ret);                       \\\n+  } while (false)\n+\n+TEST_VM_F(LockStackTest, is_recursive) {\n+  if (LockingMode != LM_LIGHTWEIGHT || !VM_Version::supports_recursive_lightweight_locking()) {\n+    return;\n+  }\n+\n+  JavaThread* THREAD = JavaThread::current();\n+  \/\/ the thread should be in vm to use locks\n+  ThreadInVMfromNative ThreadInVMfromNative(THREAD);\n+\n+  LockStack& ls = THREAD->lock_stack();\n+\n+  EXPECT_TRUE(ls.is_empty());\n+\n+  oop obj0 = Universe::int_mirror();\n+  oop obj1 = Universe::float_mirror();\n+\n+  push_raw(ls, obj0);\n+\n+  \/\/ 0\n+  EXPECT_FALSE(ls.is_recursive(obj0));\n+\n+  push_raw(ls, obj1);\n+\n+  \/\/ 0, 1\n+  EXPECT_FALSE(ls.is_recursive(obj0));\n+  EXPECT_FALSE(ls.is_recursive(obj1));\n+\n+  push_raw(ls, obj1);\n+\n+  \/\/ 0, 1, 1\n+  EXPECT_FALSE(ls.is_recursive(obj0));\n+  EXPECT_TRUE(ls.is_recursive(obj1));\n+\n+  pop_raw(ls);\n+  pop_raw(ls);\n+  push_raw(ls, obj0);\n+\n+  \/\/ 0, 0\n+  EXPECT_TRUE(ls.is_recursive(obj0));\n+\n+  push_raw(ls, obj0);\n+\n+  \/\/ 0, 0, 0\n+  EXPECT_TRUE(ls.is_recursive(obj0));\n+\n+  pop_raw(ls);\n+  push_raw(ls, obj1);\n+\n+  \/\/ 0, 0, 1\n+  EXPECT_TRUE(ls.is_recursive(obj0));\n+  EXPECT_FALSE(ls.is_recursive(obj1));\n+\n+  push_raw(ls, obj1);\n+\n+  \/\/ 0, 0, 1, 1\n+  EXPECT_TRUE(ls.is_recursive(obj0));\n+  EXPECT_TRUE(ls.is_recursive(obj1));\n+\n+  \/\/ Clear stack\n+  pop_raw(ls);\n+  pop_raw(ls);\n+  pop_raw(ls);\n+  pop_raw(ls);\n+\n+  EXPECT_TRUE(ls.is_empty());\n+}\n+\n+TEST_VM_F(LockStackTest, try_recursive_enter) {\n+  if (LockingMode != LM_LIGHTWEIGHT || !VM_Version::supports_recursive_lightweight_locking()) {\n+    return;\n+  }\n+\n+  JavaThread* THREAD = JavaThread::current();\n+  \/\/ the thread should be in vm to use locks\n+  ThreadInVMfromNative ThreadInVMfromNative(THREAD);\n+\n+  LockStack& ls = THREAD->lock_stack();\n+\n+  EXPECT_TRUE(ls.is_empty());\n+\n+  oop obj0 = Universe::int_mirror();\n+  oop obj1 = Universe::float_mirror();\n+\n+  ls.push(obj0);\n+\n+  \/\/ 0\n+  EXPECT_FALSE(ls.is_recursive(obj0));\n+\n+  ls.push(obj1);\n+\n+  \/\/ 0, 1\n+  EXPECT_FALSE(ls.is_recursive(obj0));\n+  EXPECT_FALSE(ls.is_recursive(obj1));\n+\n+  recursive_enter(ls, obj1);\n+\n+  \/\/ 0, 1, 1\n+  EXPECT_FALSE(ls.is_recursive(obj0));\n+  EXPECT_TRUE(ls.is_recursive(obj1));\n+\n+  recursive_exit(ls, obj1);\n+  pop_raw(ls);\n+  recursive_enter(ls, obj0);\n+\n+  \/\/ 0, 0\n+  EXPECT_TRUE(ls.is_recursive(obj0));\n+\n+  recursive_enter(ls, obj0);\n+\n+  \/\/ 0, 0, 0\n+  EXPECT_TRUE(ls.is_recursive(obj0));\n+\n+  recursive_exit(ls, obj0);\n+  push_raw(ls, obj1);\n+\n+  \/\/ 0, 0, 1\n+  EXPECT_TRUE(ls.is_recursive(obj0));\n+  EXPECT_FALSE(ls.is_recursive(obj1));\n+\n+  recursive_enter(ls, obj1);\n+\n+  \/\/ 0, 0, 1, 1\n+  EXPECT_TRUE(ls.is_recursive(obj0));\n+  EXPECT_TRUE(ls.is_recursive(obj1));\n+\n+  \/\/ Clear stack\n+  pop_raw(ls);\n+  pop_raw(ls);\n+  pop_raw(ls);\n+  pop_raw(ls);\n+\n+  EXPECT_TRUE(ls.is_empty());\n+}\n+\n+TEST_VM_F(LockStackTest, contains) {\n+  if (LockingMode != LM_LIGHTWEIGHT) {\n+    return;\n+  }\n+\n+  const bool test_recursive = VM_Version::supports_recursive_lightweight_locking();\n+\n+  JavaThread* THREAD = JavaThread::current();\n+  \/\/ the thread should be in vm to use locks\n+  ThreadInVMfromNative ThreadInVMfromNative(THREAD);\n+\n+  LockStack& ls = THREAD->lock_stack();\n+\n+  EXPECT_TRUE(ls.is_empty());\n+\n+  oop obj0 = Universe::int_mirror();\n+  oop obj1 = Universe::float_mirror();\n+\n+  EXPECT_FALSE(ls.contains(obj0));\n+\n+  ls.push(obj0);\n+\n+  \/\/ 0\n+  EXPECT_TRUE(ls.contains(obj0));\n+  EXPECT_FALSE(ls.contains(obj1));\n+\n+  if (test_recursive) {\n+    push_raw(ls, obj0);\n+\n+    \/\/ 0, 0\n+    EXPECT_TRUE(ls.contains(obj0));\n+    EXPECT_FALSE(ls.contains(obj1));\n+  }\n+\n+  push_raw(ls, obj1);\n+\n+  \/\/ 0, 0, 1\n+  EXPECT_TRUE(ls.contains(obj0));\n+  EXPECT_TRUE(ls.contains(obj1));\n+\n+  if (test_recursive) {\n+    push_raw(ls, obj1);\n+\n+    \/\/ 0, 0, 1, 1\n+    EXPECT_TRUE(ls.contains(obj0));\n+    EXPECT_TRUE(ls.contains(obj1));\n+  }\n+\n+  pop_raw(ls);\n+  if (test_recursive) {\n+    pop_raw(ls);\n+    pop_raw(ls);\n+  }\n+  push_raw(ls, obj1);\n+\n+  \/\/ 0, 1\n+  EXPECT_TRUE(ls.contains(obj0));\n+  EXPECT_TRUE(ls.contains(obj1));\n+\n+  \/\/ Clear stack\n+  pop_raw(ls);\n+  pop_raw(ls);\n+\n+  EXPECT_TRUE(ls.is_empty());\n+}\n+\n+TEST_VM_F(LockStackTest, remove) {\n+  if (LockingMode != LM_LIGHTWEIGHT) {\n+    return;\n+  }\n+\n+  const bool test_recursive = VM_Version::supports_recursive_lightweight_locking();\n+\n+  JavaThread* THREAD = JavaThread::current();\n+  \/\/ the thread should be in vm to use locks\n+  ThreadInVMfromNative ThreadInVMfromNative(THREAD);\n+\n+  LockStack& ls = THREAD->lock_stack();\n+\n+  EXPECT_TRUE(ls.is_empty());\n+\n+  oop obj0 = Universe::int_mirror();\n+  oop obj1 = Universe::float_mirror();\n+  oop obj2 = Universe::short_mirror();\n+  oop obj3 = Universe::long_mirror();\n+\n+  push_raw(ls, obj0);\n+\n+  \/\/ 0\n+  {\n+    size_t removed = ls.remove(obj0);\n+    EXPECT_EQ(removed, 1u);\n+    EXPECT_FALSE(ls.contains(obj0));\n+  }\n+\n+  if (test_recursive) {\n+    push_raw(ls, obj0);\n+    push_raw(ls, obj0);\n+\n+    \/\/ 0, 0\n+    {\n+      size_t removed = ls.remove(obj0);\n+      EXPECT_EQ(removed, 2u);\n+      EXPECT_FALSE(ls.contains(obj0));\n+    }\n+  }\n+\n+  push_raw(ls, obj0);\n+  push_raw(ls, obj1);\n+\n+  \/\/ 0, 1\n+  {\n+    size_t removed = ls.remove(obj0);\n+    EXPECT_EQ(removed, 1u);\n+    EXPECT_FALSE(ls.contains(obj0));\n+    EXPECT_TRUE(ls.contains(obj1));\n+\n+    ls.remove(obj1);\n+    EXPECT_TRUE(ls.is_empty());\n+  }\n+\n+  push_raw(ls, obj0);\n+  push_raw(ls, obj1);\n+\n+  \/\/ 0, 1\n+  {\n+    size_t removed = ls.remove(obj1);\n+    EXPECT_EQ(removed, 1u);\n+    EXPECT_FALSE(ls.contains(obj1));\n+    EXPECT_TRUE(ls.contains(obj0));\n+\n+    ls.remove(obj0);\n+    EXPECT_TRUE(ls.is_empty());\n+  }\n+\n+  if (test_recursive) {\n+    push_raw(ls, obj0);\n+    push_raw(ls, obj0);\n+    push_raw(ls, obj1);\n+\n+    \/\/ 0, 0, 1\n+    {\n+      size_t removed = ls.remove(obj0);\n+      EXPECT_EQ(removed, 2u);\n+      EXPECT_FALSE(ls.contains(obj0));\n+      EXPECT_TRUE(ls.contains(obj1));\n+\n+      ls.remove(obj1);\n+      EXPECT_TRUE(ls.is_empty());\n+    }\n+\n+    push_raw(ls, obj0);\n+    push_raw(ls, obj1);\n+    push_raw(ls, obj1);\n+\n+    \/\/ 0, 1, 1\n+    {\n+      size_t removed = ls.remove(obj1);\n+      EXPECT_EQ(removed, 2u);\n+      EXPECT_FALSE(ls.contains(obj1));\n+      EXPECT_TRUE(ls.contains(obj0));\n+\n+      ls.remove(obj0);\n+      EXPECT_TRUE(ls.is_empty());\n+    }\n+\n+    push_raw(ls, obj0);\n+    push_raw(ls, obj1);\n+    push_raw(ls, obj1);\n+    push_raw(ls, obj2);\n+    push_raw(ls, obj2);\n+    push_raw(ls, obj2);\n+    push_raw(ls, obj2);\n+    push_raw(ls, obj3);\n+\n+    \/\/ 0, 1, 1, 2, 2, 2, 2, 3\n+    {\n+      EXPECT_EQ(size(ls), 8u);\n+\n+      size_t removed = ls.remove(obj1);\n+      EXPECT_EQ(removed, 2u);\n+\n+      EXPECT_TRUE(ls.contains(obj0));\n+      EXPECT_FALSE(ls.contains(obj1));\n+      EXPECT_TRUE(ls.contains(obj2));\n+      EXPECT_TRUE(ls.contains(obj3));\n+\n+      EXPECT_EQ(at(ls, 0), obj0);\n+      EXPECT_EQ(at(ls, 1), obj2);\n+      EXPECT_EQ(at(ls, 2), obj2);\n+      EXPECT_EQ(at(ls, 3), obj2);\n+      EXPECT_EQ(at(ls, 4), obj2);\n+      EXPECT_EQ(at(ls, 5), obj3);\n+      EXPECT_EQ(size(ls), 6u);\n+\n+      removed = ls.remove(obj2);\n+      EXPECT_EQ(removed, 4u);\n+\n+      EXPECT_TRUE(ls.contains(obj0));\n+      EXPECT_FALSE(ls.contains(obj1));\n+      EXPECT_FALSE(ls.contains(obj2));\n+      EXPECT_TRUE(ls.contains(obj3));\n+\n+      EXPECT_EQ(at(ls, 0), obj0);\n+      EXPECT_EQ(at(ls, 1), obj3);\n+      EXPECT_EQ(size(ls), 2u);\n+\n+      removed = ls.remove(obj0);\n+      EXPECT_EQ(removed, 1u);\n+\n+      EXPECT_FALSE(ls.contains(obj0));\n+      EXPECT_FALSE(ls.contains(obj1));\n+      EXPECT_FALSE(ls.contains(obj2));\n+      EXPECT_TRUE(ls.contains(obj3));\n+\n+      EXPECT_EQ(at(ls, 0), obj3);\n+      EXPECT_EQ(size(ls), 1u);\n+\n+      removed = ls.remove(obj3);\n+      EXPECT_EQ(removed, 1u);\n+\n+      EXPECT_TRUE(ls.is_empty());\n+      EXPECT_EQ(size(ls), 0u);\n+    }\n+  }\n+\n+  EXPECT_TRUE(ls.is_empty());\n+}\n","filename":"test\/hotspot\/gtest\/runtime\/test_lockStack.cpp","additions":427,"deletions":0,"binary":false,"changes":427,"status":"added"},{"patch":"@@ -2,1 +2,1 @@\n-# Copyright (c) 2013, 2023, Oracle and\/or its affiliates. All rights reserved.\n+# Copyright (c) 2013, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -145,0 +145,1 @@\n+  gtest\/LockStackGtests.java \\\n","filename":"test\/hotspot\/jtreg\/TEST.groups","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -0,0 +1,32 @@\n+\/*\n+ * Copyright (c) 2023, 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+\/* @test\n+ * @summary Run LockStack gtests with LockingMode=2\n+ * @library \/test\/lib\n+ * @modules java.base\/jdk.internal.misc\n+ *          java.xml\n+ * @requires vm.flagless\n+ * @run main\/native GTestWrapper --gtest_filter=LockStackTest* -XX:LockingMode=2\n+ *\/\n","filename":"test\/hotspot\/jtreg\/gtest\/LockStackGtests.java","additions":32,"deletions":0,"binary":false,"changes":32,"status":"added"},{"patch":"@@ -0,0 +1,108 @@\n+\/*\n+ * Copyright (c) 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+\/*\n+ * @test TestLockStackCapacity\n+ * @summary Tests the interaction between recursive lightweight locking and\n+ *          when the lock stack capacity is exceeded.\n+ * @requires vm.flagless\n+ * @library \/testlibrary \/test\/lib\n+ * @build jdk.test.whitebox.WhiteBox\n+ * @run driver jdk.test.lib.helpers.ClassFileInstaller jdk.test.whitebox.WhiteBox\n+ * @run main\/othervm -Xbootclasspath\/a:. -XX:+UnlockDiagnosticVMOptions -XX:+WhiteBoxAPI -Xint -XX:LockingMode=2 TestLockStackCapacity\n+ *\/\n+\n+import jdk.test.lib.Asserts;\n+import jdk.test.whitebox.WhiteBox;\n+import jtreg.SkippedException;\n+\n+public class TestLockStackCapacity {\n+    static final WhiteBox WB = WhiteBox.getWhiteBox();\n+    static final int LockingMode = WB.getIntVMFlag(\"LockingMode\").intValue();\n+    static final int LM_LIGHTWEIGHT = 2;\n+\n+    static class SynchronizedObject {\n+        static final SynchronizedObject OUTER = new SynchronizedObject();\n+        static final SynchronizedObject INNER = new SynchronizedObject();\n+        static final int LockStackCapacity = WB.getLockStackCapacity();\n+\n+        synchronized void runInner(int depth) {\n+            assertNotInflated();\n+            if (depth == 1) {\n+                return;\n+            } else {\n+                runInner(depth - 1);\n+            }\n+            assertNotInflated();\n+        }\n+\n+        synchronized void runOuter(int depth, SynchronizedObject inner) {\n+            assertNotInflated();\n+            if (depth == 1) {\n+                inner.runInner(LockStackCapacity);\n+            } else {\n+                runOuter(depth - 1, inner);\n+            }\n+            assertInflated();\n+        }\n+\n+        public static void runTest() {\n+            \/\/ Test Requires a capacity of at least 2.\n+            Asserts.assertGTE(LockStackCapacity, 2);\n+\n+            \/\/ Just checking\n+            OUTER.assertNotInflated();\n+            INNER.assertNotInflated();\n+\n+            synchronized(OUTER) {\n+                OUTER.assertNotInflated();\n+                INNER.assertNotInflated();\n+                OUTER.runOuter(LockStackCapacity - 1, INNER);\n+\n+                OUTER.assertInflated();\n+                INNER.assertNotInflated();\n+            }\n+        }\n+\n+        void assertNotInflated() {\n+            Asserts.assertFalse(WB.isMonitorInflated(this));\n+        }\n+\n+        void assertInflated() {\n+            Asserts.assertTrue(WB.isMonitorInflated(this));\n+        }\n+    }\n+\n+    public static void main(String... args) throws Exception {\n+        if (LockingMode != LM_LIGHTWEIGHT) {\n+            throw new SkippedException(\"Test only valid for LM_LIGHTWEIGHT\");\n+        }\n+\n+        if (!WB.supportsRecursiveLightweightLocking()) {\n+            throw new SkippedException(\"Test only valid is LM_LIGHTWEIGHT supports recursion\");\n+        }\n+\n+        SynchronizedObject.runTest();\n+    }\n+}\n","filename":"test\/hotspot\/jtreg\/runtime\/lockStack\/TestLockStackCapacity.java","additions":108,"deletions":0,"binary":false,"changes":108,"status":"added"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2012, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2012, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -122,0 +122,4 @@\n+  public native int getLockStackCapacity();\n+\n+  public native boolean supportsRecursiveLightweightLocking();\n+\n","filename":"test\/lib\/jdk\/test\/whitebox\/WhiteBox.java","additions":5,"deletions":1,"binary":false,"changes":6,"status":"modified"}]}