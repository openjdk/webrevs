{"files":[{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2020, 2023, SAP SE. All rights reserved.\n+ * Copyright (c) 2020, 2025, SAP SE. All rights reserved.\n@@ -38,10 +38,0 @@\n-bool ABIDescriptor::is_volatile_reg(Register reg) const {\n-  return _integer_argument_registers.contains(reg)\n-    || _integer_additional_volatile_registers.contains(reg);\n-}\n-\n-bool ABIDescriptor::is_volatile_reg(FloatRegister reg) const {\n-    return _float_argument_registers.contains(reg)\n-        || _float_additional_volatile_registers.contains(reg);\n-}\n-\n@@ -65,4 +55,0 @@\n-  objArrayOop volatileStorage = jdk_internal_foreign_abi_ABIDescriptor::volatileStorage(abi_oop);\n-  parse_register_array(volatileStorage, StorageType::INTEGER, abi._integer_additional_volatile_registers, as_Register);\n-  parse_register_array(volatileStorage, StorageType::FLOAT, abi._float_additional_volatile_registers, as_FloatRegister);\n-\n","filename":"src\/hotspot\/cpu\/ppc\/foreignGlobals_ppc.cpp","additions":1,"deletions":15,"binary":false,"changes":16,"status":"modified"},{"patch":"@@ -2,2 +2,2 @@\n- * Copyright (c) 2022, 2023, Oracle and\/or its affiliates. All rights reserved.\n- * Copyright (c) 2023 SAP SE. All rights reserved.\n+ * Copyright (c) 2022, 2025, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2023, 2025 SAP SE. All rights reserved.\n@@ -37,3 +37,0 @@\n-  GrowableArray<Register> _integer_additional_volatile_registers;\n-  GrowableArray<FloatRegister> _float_additional_volatile_registers;\n-\n","filename":"src\/hotspot\/cpu\/ppc\/foreignGlobals_ppc.hpp","additions":2,"deletions":5,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -2,2 +2,2 @@\n- * Copyright (c) 2000, 2024, Oracle and\/or its affiliates. All rights reserved.\n- * Copyright (c) 2012, 2024 SAP SE. All rights reserved.\n+ * Copyright (c) 2000, 2025, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2012, 2025 SAP SE. All rights reserved.\n@@ -137,50 +137,0 @@\n-  \/\/ non-volatile GPRs:\n-\n-  struct spill_nonvolatiles {\n-    uint64_t r14;\n-    uint64_t r15;                                 \/\/_16\n-    uint64_t r16;\n-    uint64_t r17;                                 \/\/_16\n-    uint64_t r18;\n-    uint64_t r19;                                 \/\/_16\n-    uint64_t r20;\n-    uint64_t r21;                                 \/\/_16\n-    uint64_t r22;\n-    uint64_t r23;                                 \/\/_16\n-    uint64_t r24;\n-    uint64_t r25;                                 \/\/_16\n-    uint64_t r26;\n-    uint64_t r27;                                 \/\/_16\n-    uint64_t r28;\n-    uint64_t r29;                                 \/\/_16\n-    uint64_t r30;\n-    uint64_t r31;                                 \/\/_16\n-\n-    double f14;\n-    double f15;\n-    double f16;\n-    double f17;\n-    double f18;\n-    double f19;\n-    double f20;\n-    double f21;\n-    double f22;\n-    double f23;\n-    double f24;\n-    double f25;\n-    double f26;\n-    double f27;\n-    double f28;\n-    double f29;\n-    double f30;\n-    double f31;\n-\n-    \/\/ aligned to frame::alignment_in_bytes (16)\n-  };\n-\n-  enum {\n-    spill_nonvolatiles_size = sizeof(spill_nonvolatiles)\n-  };\n-\n-  #define _spill_nonvolatiles_neg(_component) \\\n-     (int)(-frame::spill_nonvolatiles_size + offset_of(frame::spill_nonvolatiles, _component))\n@@ -233,0 +183,1 @@\n+  \/\/            [non-volatiles]\n@@ -295,1 +246,0 @@\n-    uint64_t r[spill_nonvolatiles_size\/sizeof(uint64_t)];\n","filename":"src\/hotspot\/cpu\/ppc\/frame_ppc.hpp","additions":3,"deletions":53,"binary":false,"changes":56,"status":"modified"},{"patch":"@@ -781,38 +781,22 @@\n-void MacroAssembler::save_nonvolatile_gprs(Register dst, int offset) {\n-  std(R14, offset, dst);   offset += 8;\n-  std(R15, offset, dst);   offset += 8;\n-  std(R16, offset, dst);   offset += 8;\n-  std(R17, offset, dst);   offset += 8;\n-  std(R18, offset, dst);   offset += 8;\n-  std(R19, offset, dst);   offset += 8;\n-  std(R20, offset, dst);   offset += 8;\n-  std(R21, offset, dst);   offset += 8;\n-  std(R22, offset, dst);   offset += 8;\n-  std(R23, offset, dst);   offset += 8;\n-  std(R24, offset, dst);   offset += 8;\n-  std(R25, offset, dst);   offset += 8;\n-  std(R26, offset, dst);   offset += 8;\n-  std(R27, offset, dst);   offset += 8;\n-  std(R28, offset, dst);   offset += 8;\n-  std(R29, offset, dst);   offset += 8;\n-  std(R30, offset, dst);   offset += 8;\n-  std(R31, offset, dst);   offset += 8;\n-\n-  stfd(F14, offset, dst);   offset += 8;\n-  stfd(F15, offset, dst);   offset += 8;\n-  stfd(F16, offset, dst);   offset += 8;\n-  stfd(F17, offset, dst);   offset += 8;\n-  stfd(F18, offset, dst);   offset += 8;\n-  stfd(F19, offset, dst);   offset += 8;\n-  stfd(F20, offset, dst);   offset += 8;\n-  stfd(F21, offset, dst);   offset += 8;\n-  stfd(F22, offset, dst);   offset += 8;\n-  stfd(F23, offset, dst);   offset += 8;\n-  stfd(F24, offset, dst);   offset += 8;\n-  stfd(F25, offset, dst);   offset += 8;\n-  stfd(F26, offset, dst);   offset += 8;\n-  stfd(F27, offset, dst);   offset += 8;\n-  stfd(F28, offset, dst);   offset += 8;\n-  stfd(F29, offset, dst);   offset += 8;\n-  stfd(F30, offset, dst);   offset += 8;\n-  stfd(F31, offset, dst);\n+void MacroAssembler::save_nonvolatile_registers(Register dst, int offset, bool include_fp_regs, bool include_vector_regs) {\n+  for (int i = 14; i < 32; i++) {\n+    std(as_Register(i), offset, dst);\n+    offset += 8;\n+  }\n+\n+  if (include_fp_regs) {\n+    for (int i = 14; i < 32; i++) {\n+      stfd(as_FloatRegister(i), offset, dst);\n+      offset += 8;\n+    }\n+  }\n+\n+  if (include_vector_regs) {\n+    assert(is_aligned(offset, StackAlignmentInBytes), \"should be\");\n+    Register spill_addr = R0;\n+    for (int i = 20; i < 32; i++) {\n+      addi(spill_addr, dst, offset);\n+      stxvd2x(as_VectorRegister(i)->to_vsr(), spill_addr);\n+      offset += 16;\n+    }\n+  }\n@@ -825,39 +809,22 @@\n-void MacroAssembler::restore_nonvolatile_gprs(Register src, int offset) {\n-  ld(R14, offset, src);   offset += 8;\n-  ld(R15, offset, src);   offset += 8;\n-  ld(R16, offset, src);   offset += 8;\n-  ld(R17, offset, src);   offset += 8;\n-  ld(R18, offset, src);   offset += 8;\n-  ld(R19, offset, src);   offset += 8;\n-  ld(R20, offset, src);   offset += 8;\n-  ld(R21, offset, src);   offset += 8;\n-  ld(R22, offset, src);   offset += 8;\n-  ld(R23, offset, src);   offset += 8;\n-  ld(R24, offset, src);   offset += 8;\n-  ld(R25, offset, src);   offset += 8;\n-  ld(R26, offset, src);   offset += 8;\n-  ld(R27, offset, src);   offset += 8;\n-  ld(R28, offset, src);   offset += 8;\n-  ld(R29, offset, src);   offset += 8;\n-  ld(R30, offset, src);   offset += 8;\n-  ld(R31, offset, src);   offset += 8;\n-\n-  \/\/ FP registers\n-  lfd(F14, offset, src);   offset += 8;\n-  lfd(F15, offset, src);   offset += 8;\n-  lfd(F16, offset, src);   offset += 8;\n-  lfd(F17, offset, src);   offset += 8;\n-  lfd(F18, offset, src);   offset += 8;\n-  lfd(F19, offset, src);   offset += 8;\n-  lfd(F20, offset, src);   offset += 8;\n-  lfd(F21, offset, src);   offset += 8;\n-  lfd(F22, offset, src);   offset += 8;\n-  lfd(F23, offset, src);   offset += 8;\n-  lfd(F24, offset, src);   offset += 8;\n-  lfd(F25, offset, src);   offset += 8;\n-  lfd(F26, offset, src);   offset += 8;\n-  lfd(F27, offset, src);   offset += 8;\n-  lfd(F28, offset, src);   offset += 8;\n-  lfd(F29, offset, src);   offset += 8;\n-  lfd(F30, offset, src);   offset += 8;\n-  lfd(F31, offset, src);\n+void MacroAssembler::restore_nonvolatile_registers(Register src, int offset, bool include_fp_regs, bool include_vector_regs) {\n+  for (int i = 14; i < 32; i++) {\n+    ld(as_Register(i), offset, src);\n+    offset += 8;\n+  }\n+\n+  if (include_fp_regs) {\n+    for (int i = 14; i < 32; i++) {\n+      lfd(as_FloatRegister(i), offset, src);\n+      offset += 8;\n+    }\n+  }\n+\n+  if (include_vector_regs) {\n+    assert(is_aligned(offset, StackAlignmentInBytes), \"should be\");\n+    Register spill_addr = R0;\n+    for (int i = 20; i < 32; i++) {\n+      addi(spill_addr, src, offset);\n+      lxvd2x(as_VectorRegister(i)->to_vsr(), spill_addr);\n+      offset += 16;\n+    }\n+  }\n@@ -1032,7 +999,0 @@\n-\/\/ Setup up a new C frame with a spill area for non-volatile GPRs and\n-\/\/ additional space for local variables.\n-void MacroAssembler::push_frame_reg_args_nonvolatiles(unsigned int bytes,\n-                                                      Register tmp) {\n-  push_frame(bytes + frame::native_abi_reg_args_size + frame::spill_nonvolatiles_size, tmp);\n-}\n-\n","filename":"src\/hotspot\/cpu\/ppc\/macroAssembler_ppc.cpp","additions":44,"deletions":84,"binary":false,"changes":128,"status":"modified"},{"patch":"@@ -302,2 +302,8 @@\n-  void save_nonvolatile_gprs(   Register dst_base, int offset);\n-  void restore_nonvolatile_gprs(Register src_base, int offset);\n+  int  save_nonvolatile_registers_size(bool include_fp_regs, bool include_vector_regs) {\n+    int size = (32 - 14) * 8; \/\/ GP regs\n+    if (include_fp_regs) size += (32 - 14) * 8;\n+    if (include_vector_regs) size += (32 - 20) * 16;\n+    return size;\n+  }\n+  void save_nonvolatile_registers(   Register dst_base, int offset, bool include_fp_regs, bool include_vector_regs);\n+  void restore_nonvolatile_registers(Register src_base, int offset, bool include_fp_regs, bool include_vector_regs);\n@@ -337,4 +343,0 @@\n-  \/\/ Setup up a new C frame with a spill area for non-volatile GPRs and additional\n-  \/\/ space for local variables\n-  void push_frame_reg_args_nonvolatiles(unsigned int bytes, Register tmp);\n-\n","filename":"src\/hotspot\/cpu\/ppc\/macroAssembler_ppc.hpp","additions":8,"deletions":6,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -261,32 +261,160 @@\n-  reg_def VSR0 ( SOC, SOC, Op_VecX, 0, VMRegImpl::Bad());\n-  reg_def VSR1 ( SOC, SOC, Op_VecX, 1, VMRegImpl::Bad());\n-  reg_def VSR2 ( SOC, SOC, Op_VecX, 2, VMRegImpl::Bad());\n-  reg_def VSR3 ( SOC, SOC, Op_VecX, 3, VMRegImpl::Bad());\n-  reg_def VSR4 ( SOC, SOC, Op_VecX, 4, VMRegImpl::Bad());\n-  reg_def VSR5 ( SOC, SOC, Op_VecX, 5, VMRegImpl::Bad());\n-  reg_def VSR6 ( SOC, SOC, Op_VecX, 6, VMRegImpl::Bad());\n-  reg_def VSR7 ( SOC, SOC, Op_VecX, 7, VMRegImpl::Bad());\n-  reg_def VSR8 ( SOC, SOC, Op_VecX, 8, VMRegImpl::Bad());\n-  reg_def VSR9 ( SOC, SOC, Op_VecX, 9, VMRegImpl::Bad());\n-  reg_def VSR10 ( SOC, SOC, Op_VecX, 10, VMRegImpl::Bad());\n-  reg_def VSR11 ( SOC, SOC, Op_VecX, 11, VMRegImpl::Bad());\n-  reg_def VSR12 ( SOC, SOC, Op_VecX, 12, VMRegImpl::Bad());\n-  reg_def VSR13 ( SOC, SOC, Op_VecX, 13, VMRegImpl::Bad());\n-  reg_def VSR14 ( SOC, SOE, Op_VecX, 14, VMRegImpl::Bad());\n-  reg_def VSR15 ( SOC, SOE, Op_VecX, 15, VMRegImpl::Bad());\n-  reg_def VSR16 ( SOC, SOE, Op_VecX, 16, VMRegImpl::Bad());\n-  reg_def VSR17 ( SOC, SOE, Op_VecX, 17, VMRegImpl::Bad());\n-  reg_def VSR18 ( SOC, SOE, Op_VecX, 18, VMRegImpl::Bad());\n-  reg_def VSR19 ( SOC, SOE, Op_VecX, 19, VMRegImpl::Bad());\n-  reg_def VSR20 ( SOC, SOE, Op_VecX, 20, VMRegImpl::Bad());\n-  reg_def VSR21 ( SOC, SOE, Op_VecX, 21, VMRegImpl::Bad());\n-  reg_def VSR22 ( SOC, SOE, Op_VecX, 22, VMRegImpl::Bad());\n-  reg_def VSR23 ( SOC, SOE, Op_VecX, 23, VMRegImpl::Bad());\n-  reg_def VSR24 ( SOC, SOE, Op_VecX, 24, VMRegImpl::Bad());\n-  reg_def VSR25 ( SOC, SOE, Op_VecX, 25, VMRegImpl::Bad());\n-  reg_def VSR26 ( SOC, SOE, Op_VecX, 26, VMRegImpl::Bad());\n-  reg_def VSR27 ( SOC, SOE, Op_VecX, 27, VMRegImpl::Bad());\n-  reg_def VSR28 ( SOC, SOE, Op_VecX, 28, VMRegImpl::Bad());\n-  reg_def VSR29 ( SOC, SOE, Op_VecX, 29, VMRegImpl::Bad());\n-  reg_def VSR30 ( SOC, SOE, Op_VecX, 30, VMRegImpl::Bad());\n-  reg_def VSR31 ( SOC, SOE, Op_VecX, 31, VMRegImpl::Bad());\n+  reg_def VSR0   (SOC, SOC, Op_VecX, 0, VMRegImpl::Bad());\n+  reg_def VSR0_H (SOC, SOC, Op_VecX, 0, VMRegImpl::Bad());\n+  reg_def VSR0_J (SOC, SOC, Op_VecX, 0, VMRegImpl::Bad());\n+  reg_def VSR0_K (SOC, SOC, Op_VecX, 0, VMRegImpl::Bad());\n+\n+  reg_def VSR1   (SOC, SOC, Op_VecX, 1, VMRegImpl::Bad());\n+  reg_def VSR1_H (SOC, SOC, Op_VecX, 1, VMRegImpl::Bad());\n+  reg_def VSR1_J (SOC, SOC, Op_VecX, 1, VMRegImpl::Bad());\n+  reg_def VSR1_K (SOC, SOC, Op_VecX, 1, VMRegImpl::Bad());\n+\n+  reg_def VSR2   (SOC, SOC, Op_VecX, 2, VMRegImpl::Bad());\n+  reg_def VSR2_H (SOC, SOC, Op_VecX, 2, VMRegImpl::Bad());\n+  reg_def VSR2_J (SOC, SOC, Op_VecX, 2, VMRegImpl::Bad());\n+  reg_def VSR2_K (SOC, SOC, Op_VecX, 2, VMRegImpl::Bad());\n+\n+  reg_def VSR3   (SOC, SOC, Op_VecX, 3, VMRegImpl::Bad());\n+  reg_def VSR3_H (SOC, SOC, Op_VecX, 3, VMRegImpl::Bad());\n+  reg_def VSR3_J (SOC, SOC, Op_VecX, 3, VMRegImpl::Bad());\n+  reg_def VSR3_K (SOC, SOC, Op_VecX, 3, VMRegImpl::Bad());\n+\n+  reg_def VSR4   (SOC, SOC, Op_VecX, 4, VMRegImpl::Bad());\n+  reg_def VSR4_H (SOC, SOC, Op_VecX, 4, VMRegImpl::Bad());\n+  reg_def VSR4_J (SOC, SOC, Op_VecX, 4, VMRegImpl::Bad());\n+  reg_def VSR4_K (SOC, SOC, Op_VecX, 4, VMRegImpl::Bad());\n+\n+  reg_def VSR5   (SOC, SOC, Op_VecX, 5, VMRegImpl::Bad());\n+  reg_def VSR5_H (SOC, SOC, Op_VecX, 5, VMRegImpl::Bad());\n+  reg_def VSR5_J (SOC, SOC, Op_VecX, 5, VMRegImpl::Bad());\n+  reg_def VSR5_K (SOC, SOC, Op_VecX, 5, VMRegImpl::Bad());\n+\n+  reg_def VSR6   (SOC, SOC, Op_VecX, 6, VMRegImpl::Bad());\n+  reg_def VSR6_H (SOC, SOC, Op_VecX, 6, VMRegImpl::Bad());\n+  reg_def VSR6_J (SOC, SOC, Op_VecX, 6, VMRegImpl::Bad());\n+  reg_def VSR6_K (SOC, SOC, Op_VecX, 6, VMRegImpl::Bad());\n+\n+  reg_def VSR7   (SOC, SOC, Op_VecX, 7, VMRegImpl::Bad());\n+  reg_def VSR7_H (SOC, SOC, Op_VecX, 7, VMRegImpl::Bad());\n+  reg_def VSR7_J (SOC, SOC, Op_VecX, 7, VMRegImpl::Bad());\n+  reg_def VSR7_K (SOC, SOC, Op_VecX, 7, VMRegImpl::Bad());\n+\n+  reg_def VSR8   (SOC, SOC, Op_VecX, 8, VMRegImpl::Bad());\n+  reg_def VSR8_H (SOC, SOC, Op_VecX, 8, VMRegImpl::Bad());\n+  reg_def VSR8_J (SOC, SOC, Op_VecX, 8, VMRegImpl::Bad());\n+  reg_def VSR8_K (SOC, SOC, Op_VecX, 8, VMRegImpl::Bad());\n+\n+  reg_def VSR9   (SOC, SOC, Op_VecX, 9, VMRegImpl::Bad());\n+  reg_def VSR9_H (SOC, SOC, Op_VecX, 9, VMRegImpl::Bad());\n+  reg_def VSR9_J (SOC, SOC, Op_VecX, 9, VMRegImpl::Bad());\n+  reg_def VSR9_K (SOC, SOC, Op_VecX, 9, VMRegImpl::Bad());\n+\n+  reg_def VSR10  (SOC, SOC, Op_VecX, 10, VMRegImpl::Bad());\n+  reg_def VSR10_H(SOC, SOC, Op_VecX, 10, VMRegImpl::Bad());\n+  reg_def VSR10_J(SOC, SOC, Op_VecX, 10, VMRegImpl::Bad());\n+  reg_def VSR10_K(SOC, SOC, Op_VecX, 10, VMRegImpl::Bad());\n+\n+  reg_def VSR11  (SOC, SOC, Op_VecX, 11, VMRegImpl::Bad());\n+  reg_def VSR11_H(SOC, SOC, Op_VecX, 11, VMRegImpl::Bad());\n+  reg_def VSR11_J(SOC, SOC, Op_VecX, 11, VMRegImpl::Bad());\n+  reg_def VSR11_K(SOC, SOC, Op_VecX, 11, VMRegImpl::Bad());\n+\n+  reg_def VSR12  (SOC, SOC, Op_VecX, 12, VMRegImpl::Bad());\n+  reg_def VSR12_H(SOC, SOC, Op_VecX, 12, VMRegImpl::Bad());\n+  reg_def VSR12_J(SOC, SOC, Op_VecX, 12, VMRegImpl::Bad());\n+  reg_def VSR12_K(SOC, SOC, Op_VecX, 12, VMRegImpl::Bad());\n+\n+  reg_def VSR13  (SOC, SOC, Op_VecX, 13, VMRegImpl::Bad());\n+  reg_def VSR13_H(SOC, SOC, Op_VecX, 13, VMRegImpl::Bad());\n+  reg_def VSR13_J(SOC, SOC, Op_VecX, 13, VMRegImpl::Bad());\n+  reg_def VSR13_K(SOC, SOC, Op_VecX, 13, VMRegImpl::Bad());\n+\n+  reg_def VSR14  (SOC, SOC, Op_VecX, 14, VMRegImpl::Bad());\n+  reg_def VSR14_H(SOC, SOC, Op_VecX, 14, VMRegImpl::Bad());\n+  reg_def VSR14_J(SOC, SOC, Op_VecX, 14, VMRegImpl::Bad());\n+  reg_def VSR14_K(SOC, SOC, Op_VecX, 14, VMRegImpl::Bad());\n+\n+  reg_def VSR15  (SOC, SOC, Op_VecX, 15, VMRegImpl::Bad());\n+  reg_def VSR15_H(SOC, SOC, Op_VecX, 15, VMRegImpl::Bad());\n+  reg_def VSR15_J(SOC, SOC, Op_VecX, 15, VMRegImpl::Bad());\n+  reg_def VSR15_K(SOC, SOC, Op_VecX, 15, VMRegImpl::Bad());\n+\n+  reg_def VSR16  (SOC, SOC, Op_VecX, 16, VMRegImpl::Bad());\n+  reg_def VSR16_H(SOC, SOC, Op_VecX, 16, VMRegImpl::Bad());\n+  reg_def VSR16_J(SOC, SOC, Op_VecX, 16, VMRegImpl::Bad());\n+  reg_def VSR16_K(SOC, SOC, Op_VecX, 16, VMRegImpl::Bad());\n+\n+  reg_def VSR17  (SOC, SOC, Op_VecX, 17, VMRegImpl::Bad());\n+  reg_def VSR17_H(SOC, SOC, Op_VecX, 17, VMRegImpl::Bad());\n+  reg_def VSR17_J(SOC, SOC, Op_VecX, 17, VMRegImpl::Bad());\n+  reg_def VSR17_K(SOC, SOC, Op_VecX, 17, VMRegImpl::Bad());\n+\n+  reg_def VSR18  (SOC, SOC, Op_VecX, 18, VMRegImpl::Bad());\n+  reg_def VSR18_H(SOC, SOC, Op_VecX, 18, VMRegImpl::Bad());\n+  reg_def VSR18_J(SOC, SOC, Op_VecX, 18, VMRegImpl::Bad());\n+  reg_def VSR18_K(SOC, SOC, Op_VecX, 18, VMRegImpl::Bad());\n+\n+  reg_def VSR19  (SOC, SOC, Op_VecX, 19, VMRegImpl::Bad());\n+  reg_def VSR19_H(SOC, SOC, Op_VecX, 19, VMRegImpl::Bad());\n+  reg_def VSR19_J(SOC, SOC, Op_VecX, 19, VMRegImpl::Bad());\n+  reg_def VSR19_K(SOC, SOC, Op_VecX, 19, VMRegImpl::Bad());\n+\n+  reg_def VSR20  (SOC, SOC, Op_VecX, 20, VMRegImpl::Bad());\n+  reg_def VSR20_H(SOC, SOC, Op_VecX, 20, VMRegImpl::Bad());\n+  reg_def VSR20_J(SOC, SOC, Op_VecX, 20, VMRegImpl::Bad());\n+  reg_def VSR20_K(SOC, SOC, Op_VecX, 20, VMRegImpl::Bad());\n+\n+  reg_def VSR21  (SOC, SOC, Op_VecX, 21, VMRegImpl::Bad());\n+  reg_def VSR21_H(SOC, SOC, Op_VecX, 21, VMRegImpl::Bad());\n+  reg_def VSR21_J(SOC, SOC, Op_VecX, 21, VMRegImpl::Bad());\n+  reg_def VSR21_K(SOC, SOC, Op_VecX, 21, VMRegImpl::Bad());\n+\n+  reg_def VSR22  (SOC, SOC, Op_VecX, 22, VMRegImpl::Bad());\n+  reg_def VSR22_H(SOC, SOC, Op_VecX, 22, VMRegImpl::Bad());\n+  reg_def VSR22_J(SOC, SOC, Op_VecX, 22, VMRegImpl::Bad());\n+  reg_def VSR22_K(SOC, SOC, Op_VecX, 22, VMRegImpl::Bad());\n+\n+  reg_def VSR23  (SOC, SOC, Op_VecX, 23, VMRegImpl::Bad());\n+  reg_def VSR23_H(SOC, SOC, Op_VecX, 23, VMRegImpl::Bad());\n+  reg_def VSR23_J(SOC, SOC, Op_VecX, 23, VMRegImpl::Bad());\n+  reg_def VSR23_K(SOC, SOC, Op_VecX, 23, VMRegImpl::Bad());\n+\n+  reg_def VSR24  (SOC, SOC, Op_VecX, 24, VMRegImpl::Bad());\n+  reg_def VSR24_H(SOC, SOC, Op_VecX, 24, VMRegImpl::Bad());\n+  reg_def VSR24_J(SOC, SOC, Op_VecX, 24, VMRegImpl::Bad());\n+  reg_def VSR24_K(SOC, SOC, Op_VecX, 24, VMRegImpl::Bad());\n+\n+  reg_def VSR25  (SOC, SOC, Op_VecX, 25, VMRegImpl::Bad());\n+  reg_def VSR25_H(SOC, SOC, Op_VecX, 25, VMRegImpl::Bad());\n+  reg_def VSR25_J(SOC, SOC, Op_VecX, 25, VMRegImpl::Bad());\n+  reg_def VSR25_K(SOC, SOC, Op_VecX, 25, VMRegImpl::Bad());\n+\n+  reg_def VSR26  (SOC, SOC, Op_VecX, 26, VMRegImpl::Bad());\n+  reg_def VSR26_H(SOC, SOC, Op_VecX, 26, VMRegImpl::Bad());\n+  reg_def VSR26_J(SOC, SOC, Op_VecX, 26, VMRegImpl::Bad());\n+  reg_def VSR26_K(SOC, SOC, Op_VecX, 26, VMRegImpl::Bad());\n+\n+  reg_def VSR27  (SOC, SOC, Op_VecX, 27, VMRegImpl::Bad());\n+  reg_def VSR27_H(SOC, SOC, Op_VecX, 27, VMRegImpl::Bad());\n+  reg_def VSR27_J(SOC, SOC, Op_VecX, 27, VMRegImpl::Bad());\n+  reg_def VSR27_K(SOC, SOC, Op_VecX, 27, VMRegImpl::Bad());\n+\n+  reg_def VSR28  (SOC, SOC, Op_VecX, 28, VMRegImpl::Bad());\n+  reg_def VSR28_H(SOC, SOC, Op_VecX, 28, VMRegImpl::Bad());\n+  reg_def VSR28_J(SOC, SOC, Op_VecX, 28, VMRegImpl::Bad());\n+  reg_def VSR28_K(SOC, SOC, Op_VecX, 28, VMRegImpl::Bad());\n+\n+  reg_def VSR29  (SOC, SOC, Op_VecX, 29, VMRegImpl::Bad());\n+  reg_def VSR29_H(SOC, SOC, Op_VecX, 29, VMRegImpl::Bad());\n+  reg_def VSR29_J(SOC, SOC, Op_VecX, 29, VMRegImpl::Bad());\n+  reg_def VSR29_K(SOC, SOC, Op_VecX, 29, VMRegImpl::Bad());\n+\n+  reg_def VSR30  (SOC, SOC, Op_VecX, 30, VMRegImpl::Bad());\n+  reg_def VSR30_H(SOC, SOC, Op_VecX, 30, VMRegImpl::Bad());\n+  reg_def VSR30_J(SOC, SOC, Op_VecX, 30, VMRegImpl::Bad());\n+  reg_def VSR30_K(SOC, SOC, Op_VecX, 30, VMRegImpl::Bad());\n+\n+  reg_def VSR31  (SOC, SOC, Op_VecX, 31, VMRegImpl::Bad());\n+  reg_def VSR31_H(SOC, SOC, Op_VecX, 31, VMRegImpl::Bad());\n+  reg_def VSR31_J(SOC, SOC, Op_VecX, 31, VMRegImpl::Bad());\n+  reg_def VSR31_K(SOC, SOC, Op_VecX, 31, VMRegImpl::Bad());\n+\n@@ -294,32 +422,159 @@\n-  reg_def VSR32 ( SOC, SOC, Op_VecX, 32, VSR32->as_VMReg());\n-  reg_def VSR33 ( SOC, SOC, Op_VecX, 33, VSR33->as_VMReg());\n-  reg_def VSR34 ( SOC, SOC, Op_VecX, 34, VSR34->as_VMReg());\n-  reg_def VSR35 ( SOC, SOC, Op_VecX, 35, VSR35->as_VMReg());\n-  reg_def VSR36 ( SOC, SOC, Op_VecX, 36, VSR36->as_VMReg());\n-  reg_def VSR37 ( SOC, SOC, Op_VecX, 37, VSR37->as_VMReg());\n-  reg_def VSR38 ( SOC, SOC, Op_VecX, 38, VSR38->as_VMReg());\n-  reg_def VSR39 ( SOC, SOC, Op_VecX, 39, VSR39->as_VMReg());\n-  reg_def VSR40 ( SOC, SOC, Op_VecX, 40, VSR40->as_VMReg());\n-  reg_def VSR41 ( SOC, SOC, Op_VecX, 41, VSR41->as_VMReg());\n-  reg_def VSR42 ( SOC, SOC, Op_VecX, 42, VSR42->as_VMReg());\n-  reg_def VSR43 ( SOC, SOC, Op_VecX, 43, VSR43->as_VMReg());\n-  reg_def VSR44 ( SOC, SOC, Op_VecX, 44, VSR44->as_VMReg());\n-  reg_def VSR45 ( SOC, SOC, Op_VecX, 45, VSR45->as_VMReg());\n-  reg_def VSR46 ( SOC, SOC, Op_VecX, 46, VSR46->as_VMReg());\n-  reg_def VSR47 ( SOC, SOC, Op_VecX, 47, VSR47->as_VMReg());\n-  reg_def VSR48 ( SOC, SOC, Op_VecX, 48, VSR48->as_VMReg());\n-  reg_def VSR49 ( SOC, SOC, Op_VecX, 49, VSR49->as_VMReg());\n-  reg_def VSR50 ( SOC, SOC, Op_VecX, 50, VSR50->as_VMReg());\n-  reg_def VSR51 ( SOC, SOC, Op_VecX, 51, VSR51->as_VMReg());\n-  reg_def VSR52 ( SOC, SOE, Op_VecX, 52, VSR52->as_VMReg());\n-  reg_def VSR53 ( SOC, SOE, Op_VecX, 53, VSR53->as_VMReg());\n-  reg_def VSR54 ( SOC, SOE, Op_VecX, 54, VSR54->as_VMReg());\n-  reg_def VSR55 ( SOC, SOE, Op_VecX, 55, VSR55->as_VMReg());\n-  reg_def VSR56 ( SOC, SOE, Op_VecX, 56, VSR56->as_VMReg());\n-  reg_def VSR57 ( SOC, SOE, Op_VecX, 57, VSR57->as_VMReg());\n-  reg_def VSR58 ( SOC, SOE, Op_VecX, 58, VSR58->as_VMReg());\n-  reg_def VSR59 ( SOC, SOE, Op_VecX, 59, VSR59->as_VMReg());\n-  reg_def VSR60 ( SOC, SOE, Op_VecX, 60, VSR60->as_VMReg());\n-  reg_def VSR61 ( SOC, SOE, Op_VecX, 61, VSR61->as_VMReg());\n-  reg_def VSR62 ( SOC, SOE, Op_VecX, 62, VSR62->as_VMReg());\n-  reg_def VSR63 ( SOC, SOE, Op_VecX, 63, VSR63->as_VMReg());\n+  reg_def VSR32  (SOC, SOC, Op_VecX, 32, VSR32->as_VMReg()         );\n+  reg_def VSR32_H(SOC, SOC, Op_VecX, 32, VSR32->as_VMReg()->next() );\n+  reg_def VSR32_J(SOC, SOC, Op_VecX, 32, VSR32->as_VMReg()->next(2));\n+  reg_def VSR32_K(SOC, SOC, Op_VecX, 32, VSR32->as_VMReg()->next(3));\n+\n+  reg_def VSR33  (SOC, SOC, Op_VecX, 33, VSR33->as_VMReg()         );\n+  reg_def VSR33_H(SOC, SOC, Op_VecX, 33, VSR33->as_VMReg()->next() );\n+  reg_def VSR33_J(SOC, SOC, Op_VecX, 33, VSR33->as_VMReg()->next(2));\n+  reg_def VSR33_K(SOC, SOC, Op_VecX, 33, VSR33->as_VMReg()->next(3));\n+\n+  reg_def VSR34  (SOC, SOC, Op_VecX, 34, VSR34->as_VMReg()         );\n+  reg_def VSR34_H(SOC, SOC, Op_VecX, 34, VSR34->as_VMReg()->next() );\n+  reg_def VSR34_J(SOC, SOC, Op_VecX, 34, VSR34->as_VMReg()->next(2));\n+  reg_def VSR34_K(SOC, SOC, Op_VecX, 34, VSR34->as_VMReg()->next(3));\n+\n+  reg_def VSR35  (SOC, SOC, Op_VecX, 35, VSR35->as_VMReg()         );\n+  reg_def VSR35_H(SOC, SOC, Op_VecX, 35, VSR35->as_VMReg()->next() );\n+  reg_def VSR35_J(SOC, SOC, Op_VecX, 35, VSR35->as_VMReg()->next(2));\n+  reg_def VSR35_K(SOC, SOC, Op_VecX, 35, VSR35->as_VMReg()->next(3));\n+\n+  reg_def VSR36  (SOC, SOC, Op_VecX, 36, VSR36->as_VMReg()         );\n+  reg_def VSR36_H(SOC, SOC, Op_VecX, 36, VSR36->as_VMReg()->next() );\n+  reg_def VSR36_J(SOC, SOC, Op_VecX, 36, VSR36->as_VMReg()->next(2));\n+  reg_def VSR36_K(SOC, SOC, Op_VecX, 36, VSR36->as_VMReg()->next(3));\n+\n+  reg_def VSR37  (SOC, SOC, Op_VecX, 37, VSR37->as_VMReg()         );\n+  reg_def VSR37_H(SOC, SOC, Op_VecX, 37, VSR37->as_VMReg()->next() );\n+  reg_def VSR37_J(SOC, SOC, Op_VecX, 37, VSR37->as_VMReg()->next(2));\n+  reg_def VSR37_K(SOC, SOC, Op_VecX, 37, VSR37->as_VMReg()->next(3));\n+\n+  reg_def VSR38  (SOC, SOC, Op_VecX, 38, VSR38->as_VMReg()         );\n+  reg_def VSR38_H(SOC, SOC, Op_VecX, 38, VSR38->as_VMReg()->next() );\n+  reg_def VSR38_J(SOC, SOC, Op_VecX, 38, VSR38->as_VMReg()->next(2));\n+  reg_def VSR38_K(SOC, SOC, Op_VecX, 38, VSR38->as_VMReg()->next(3));\n+\n+  reg_def VSR39  (SOC, SOC, Op_VecX, 39, VSR39->as_VMReg()         );\n+  reg_def VSR39_H(SOC, SOC, Op_VecX, 39, VSR39->as_VMReg()->next() );\n+  reg_def VSR39_J(SOC, SOC, Op_VecX, 39, VSR39->as_VMReg()->next(2));\n+  reg_def VSR39_K(SOC, SOC, Op_VecX, 39, VSR39->as_VMReg()->next(3));\n+\n+  reg_def VSR40  (SOC, SOC, Op_VecX, 40, VSR40->as_VMReg()         );\n+  reg_def VSR40_H(SOC, SOC, Op_VecX, 40, VSR40->as_VMReg()->next() );\n+  reg_def VSR40_J(SOC, SOC, Op_VecX, 40, VSR40->as_VMReg()->next(2));\n+  reg_def VSR40_K(SOC, SOC, Op_VecX, 40, VSR40->as_VMReg()->next(3));\n+\n+  reg_def VSR41  (SOC, SOC, Op_VecX, 41, VSR41->as_VMReg()         );\n+  reg_def VSR41_H(SOC, SOC, Op_VecX, 41, VSR41->as_VMReg()->next() );\n+  reg_def VSR41_J(SOC, SOC, Op_VecX, 41, VSR41->as_VMReg()->next(2));\n+  reg_def VSR41_K(SOC, SOC, Op_VecX, 41, VSR41->as_VMReg()->next(3));\n+\n+  reg_def VSR42  (SOC, SOC, Op_VecX, 42, VSR42->as_VMReg()         );\n+  reg_def VSR42_H(SOC, SOC, Op_VecX, 42, VSR42->as_VMReg()->next() );\n+  reg_def VSR42_J(SOC, SOC, Op_VecX, 42, VSR42->as_VMReg()->next(2));\n+  reg_def VSR42_K(SOC, SOC, Op_VecX, 42, VSR42->as_VMReg()->next(3));\n+\n+  reg_def VSR43  (SOC, SOC, Op_VecX, 43, VSR43->as_VMReg()         );\n+  reg_def VSR43_H(SOC, SOC, Op_VecX, 43, VSR43->as_VMReg()->next() );\n+  reg_def VSR43_J(SOC, SOC, Op_VecX, 43, VSR43->as_VMReg()->next(2));\n+  reg_def VSR43_K(SOC, SOC, Op_VecX, 43, VSR43->as_VMReg()->next(3));\n+\n+  reg_def VSR44  (SOC, SOC, Op_VecX, 44, VSR44->as_VMReg()         );\n+  reg_def VSR44_H(SOC, SOC, Op_VecX, 44, VSR44->as_VMReg()->next() );\n+  reg_def VSR44_J(SOC, SOC, Op_VecX, 44, VSR44->as_VMReg()->next(2));\n+  reg_def VSR44_K(SOC, SOC, Op_VecX, 44, VSR44->as_VMReg()->next(3));\n+\n+  reg_def VSR45  (SOC, SOC, Op_VecX, 45, VSR45->as_VMReg()         );\n+  reg_def VSR45_H(SOC, SOC, Op_VecX, 45, VSR45->as_VMReg()->next() );\n+  reg_def VSR45_J(SOC, SOC, Op_VecX, 45, VSR45->as_VMReg()->next(2));\n+  reg_def VSR45_K(SOC, SOC, Op_VecX, 45, VSR45->as_VMReg()->next(3));\n+\n+  reg_def VSR46  (SOC, SOC, Op_VecX, 46, VSR46->as_VMReg()         );\n+  reg_def VSR46_H(SOC, SOC, Op_VecX, 46, VSR46->as_VMReg()->next() );\n+  reg_def VSR46_J(SOC, SOC, Op_VecX, 46, VSR46->as_VMReg()->next(2));\n+  reg_def VSR46_K(SOC, SOC, Op_VecX, 46, VSR46->as_VMReg()->next(3));\n+\n+  reg_def VSR47  (SOC, SOC, Op_VecX, 47, VSR47->as_VMReg()         );\n+  reg_def VSR47_H(SOC, SOC, Op_VecX, 47, VSR47->as_VMReg()->next() );\n+  reg_def VSR47_J(SOC, SOC, Op_VecX, 47, VSR47->as_VMReg()->next(2));\n+  reg_def VSR47_K(SOC, SOC, Op_VecX, 47, VSR47->as_VMReg()->next(3));\n+\n+  reg_def VSR48  (SOC, SOC, Op_VecX, 48, VSR48->as_VMReg()         );\n+  reg_def VSR48_H(SOC, SOC, Op_VecX, 48, VSR48->as_VMReg()->next() );\n+  reg_def VSR48_J(SOC, SOC, Op_VecX, 48, VSR48->as_VMReg()->next(2));\n+  reg_def VSR48_K(SOC, SOC, Op_VecX, 48, VSR48->as_VMReg()->next(3));\n+\n+  reg_def VSR49  (SOC, SOC, Op_VecX, 49, VSR49->as_VMReg()         );\n+  reg_def VSR49_H(SOC, SOC, Op_VecX, 49, VSR49->as_VMReg()->next() );\n+  reg_def VSR49_J(SOC, SOC, Op_VecX, 49, VSR49->as_VMReg()->next(2));\n+  reg_def VSR49_K(SOC, SOC, Op_VecX, 49, VSR49->as_VMReg()->next(3));\n+\n+  reg_def VSR50  (SOC, SOC, Op_VecX, 50, VSR50->as_VMReg()         );\n+  reg_def VSR50_H(SOC, SOC, Op_VecX, 50, VSR50->as_VMReg()->next() );\n+  reg_def VSR50_J(SOC, SOC, Op_VecX, 50, VSR50->as_VMReg()->next(2));\n+  reg_def VSR50_K(SOC, SOC, Op_VecX, 50, VSR50->as_VMReg()->next(3));\n+\n+  reg_def VSR51  (SOC, SOC, Op_VecX, 51, VSR51->as_VMReg()         );\n+  reg_def VSR51_H(SOC, SOC, Op_VecX, 51, VSR51->as_VMReg()->next() );\n+  reg_def VSR51_J(SOC, SOC, Op_VecX, 51, VSR51->as_VMReg()->next(2));\n+  reg_def VSR51_K(SOC, SOC, Op_VecX, 51, VSR51->as_VMReg()->next(3));\n+\n+  reg_def VSR52  (SOC, SOE, Op_VecX, 52, VSR52->as_VMReg()         );\n+  reg_def VSR52_H(SOC, SOE, Op_VecX, 52, VSR52->as_VMReg()->next() );\n+  reg_def VSR52_J(SOC, SOE, Op_VecX, 52, VSR52->as_VMReg()->next(2));\n+  reg_def VSR52_K(SOC, SOE, Op_VecX, 52, VSR52->as_VMReg()->next(3));\n+\n+  reg_def VSR53  (SOC, SOE, Op_VecX, 53, VSR53->as_VMReg()         );\n+  reg_def VSR53_H(SOC, SOE, Op_VecX, 53, VSR53->as_VMReg()->next() );\n+  reg_def VSR53_J(SOC, SOE, Op_VecX, 53, VSR53->as_VMReg()->next(2));\n+  reg_def VSR53_K(SOC, SOE, Op_VecX, 53, VSR53->as_VMReg()->next(3));\n+\n+  reg_def VSR54  (SOC, SOE, Op_VecX, 54, VSR54->as_VMReg()         );\n+  reg_def VSR54_H(SOC, SOE, Op_VecX, 54, VSR54->as_VMReg()->next() );\n+  reg_def VSR54_J(SOC, SOE, Op_VecX, 54, VSR54->as_VMReg()->next(2));\n+  reg_def VSR54_K(SOC, SOE, Op_VecX, 54, VSR54->as_VMReg()->next(3));\n+\n+  reg_def VSR55  (SOC, SOE, Op_VecX, 55, VSR55->as_VMReg()         );\n+  reg_def VSR55_H(SOC, SOE, Op_VecX, 55, VSR55->as_VMReg()->next() );\n+  reg_def VSR55_J(SOC, SOE, Op_VecX, 55, VSR55->as_VMReg()->next(2));\n+  reg_def VSR55_K(SOC, SOE, Op_VecX, 55, VSR55->as_VMReg()->next(3));\n+\n+  reg_def VSR56  (SOC, SOE, Op_VecX, 56, VSR56->as_VMReg()         );\n+  reg_def VSR56_H(SOC, SOE, Op_VecX, 56, VSR56->as_VMReg()->next() );\n+  reg_def VSR56_J(SOC, SOE, Op_VecX, 56, VSR56->as_VMReg()->next(2));\n+  reg_def VSR56_K(SOC, SOE, Op_VecX, 56, VSR56->as_VMReg()->next(3));\n+\n+  reg_def VSR57  (SOC, SOE, Op_VecX, 57, VSR57->as_VMReg()         );\n+  reg_def VSR57_H(SOC, SOE, Op_VecX, 57, VSR57->as_VMReg()->next() );\n+  reg_def VSR57_J(SOC, SOE, Op_VecX, 57, VSR57->as_VMReg()->next(2));\n+  reg_def VSR57_K(SOC, SOE, Op_VecX, 57, VSR57->as_VMReg()->next(3));\n+\n+  reg_def VSR58  (SOC, SOE, Op_VecX, 58, VSR58->as_VMReg()         );\n+  reg_def VSR58_H(SOC, SOE, Op_VecX, 58, VSR58->as_VMReg()->next() );\n+  reg_def VSR58_J(SOC, SOE, Op_VecX, 58, VSR58->as_VMReg()->next(2));\n+  reg_def VSR58_K(SOC, SOE, Op_VecX, 58, VSR58->as_VMReg()->next(3));\n+\n+  reg_def VSR59  (SOC, SOE, Op_VecX, 59, VSR59->as_VMReg()         );\n+  reg_def VSR59_H(SOC, SOE, Op_VecX, 59, VSR59->as_VMReg()->next() );\n+  reg_def VSR59_J(SOC, SOE, Op_VecX, 59, VSR59->as_VMReg()->next(2));\n+  reg_def VSR59_K(SOC, SOE, Op_VecX, 59, VSR59->as_VMReg()->next(3));\n+\n+  reg_def VSR60  (SOC, SOE, Op_VecX, 60, VSR60->as_VMReg()         );\n+  reg_def VSR60_H(SOC, SOE, Op_VecX, 60, VSR60->as_VMReg()->next() );\n+  reg_def VSR60_J(SOC, SOE, Op_VecX, 60, VSR60->as_VMReg()->next(2));\n+  reg_def VSR60_K(SOC, SOE, Op_VecX, 60, VSR60->as_VMReg()->next(3));\n+\n+  reg_def VSR61  (SOC, SOE, Op_VecX, 61, VSR61->as_VMReg()         );\n+  reg_def VSR61_H(SOC, SOE, Op_VecX, 61, VSR61->as_VMReg()->next() );\n+  reg_def VSR61_J(SOC, SOE, Op_VecX, 61, VSR61->as_VMReg()->next(2));\n+  reg_def VSR61_K(SOC, SOE, Op_VecX, 61, VSR61->as_VMReg()->next(3));\n+\n+  reg_def VSR62  (SOC, SOE, Op_VecX, 62, VSR62->as_VMReg()         );\n+  reg_def VSR62_H(SOC, SOE, Op_VecX, 62, VSR62->as_VMReg()->next() );\n+  reg_def VSR62_J(SOC, SOE, Op_VecX, 62, VSR62->as_VMReg()->next(2));\n+  reg_def VSR62_K(SOC, SOE, Op_VecX, 62, VSR62->as_VMReg()->next(3));\n+\n+  reg_def VSR63  (SOC, SOE, Op_VecX, 63, VSR63->as_VMReg()         );\n+  reg_def VSR63_H(SOC, SOE, Op_VecX, 63, VSR63->as_VMReg()->next() );\n+  reg_def VSR63_J(SOC, SOE, Op_VecX, 63, VSR63->as_VMReg()->next(2));\n+  reg_def VSR63_K(SOC, SOE, Op_VecX, 63, VSR63->as_VMReg()->next(3));\n@@ -457,64 +712,64 @@\n-  VSR0,\n-  VSR1,\n-  VSR2,\n-  VSR3,\n-  VSR4,\n-  VSR5,\n-  VSR6,\n-  VSR7,\n-  VSR8,\n-  VSR9,\n-  VSR10,\n-  VSR11,\n-  VSR12,\n-  VSR13,\n-  VSR14,\n-  VSR15,\n-  VSR16,\n-  VSR17,\n-  VSR18,\n-  VSR19,\n-  VSR20,\n-  VSR21,\n-  VSR22,\n-  VSR23,\n-  VSR24,\n-  VSR25,\n-  VSR26,\n-  VSR27,\n-  VSR28,\n-  VSR29,\n-  VSR30,\n-  VSR31,\n-  VSR32,\n-  VSR33,\n-  VSR34,\n-  VSR35,\n-  VSR36,\n-  VSR37,\n-  VSR38,\n-  VSR39,\n-  VSR40,\n-  VSR41,\n-  VSR42,\n-  VSR43,\n-  VSR44,\n-  VSR45,\n-  VSR46,\n-  VSR47,\n-  VSR48,\n-  VSR49,\n-  VSR50,\n-  VSR51,\n-  VSR52,\n-  VSR53,\n-  VSR54,\n-  VSR55,\n-  VSR56,\n-  VSR57,\n-  VSR58,\n-  VSR59,\n-  VSR60,\n-  VSR61,\n-  VSR62,\n-  VSR63\n+  VSR0 , VSR0_H , VSR0_J , VSR0_K ,\n+  VSR1 , VSR1_H , VSR1_J , VSR1_K ,\n+  VSR2 , VSR2_H , VSR2_J , VSR2_K ,\n+  VSR3 , VSR3_H , VSR3_J , VSR3_K ,\n+  VSR4 , VSR4_H , VSR4_J , VSR4_K ,\n+  VSR5 , VSR5_H , VSR5_J , VSR5_K ,\n+  VSR6 , VSR6_H , VSR6_J , VSR6_K ,\n+  VSR7 , VSR7_H , VSR7_J , VSR7_K ,\n+  VSR8 , VSR8_H , VSR8_J , VSR8_K ,\n+  VSR9 , VSR9_H , VSR9_J , VSR9_K ,\n+  VSR10, VSR10_H, VSR10_J, VSR10_K,\n+  VSR11, VSR11_H, VSR11_J, VSR11_K,\n+  VSR12, VSR12_H, VSR12_J, VSR12_K,\n+  VSR13, VSR13_H, VSR13_J, VSR13_K,\n+  VSR14, VSR14_H, VSR14_J, VSR14_K,\n+  VSR15, VSR15_H, VSR15_J, VSR15_K,\n+  VSR16, VSR16_H, VSR16_J, VSR16_K,\n+  VSR17, VSR17_H, VSR17_J, VSR17_K,\n+  VSR18, VSR18_H, VSR18_J, VSR18_K,\n+  VSR19, VSR19_H, VSR19_J, VSR19_K,\n+  VSR20, VSR20_H, VSR20_J, VSR20_K,\n+  VSR21, VSR21_H, VSR21_J, VSR21_K,\n+  VSR22, VSR22_H, VSR22_J, VSR22_K,\n+  VSR23, VSR23_H, VSR23_J, VSR23_K,\n+  VSR24, VSR24_H, VSR24_J, VSR24_K,\n+  VSR25, VSR25_H, VSR25_J, VSR25_K,\n+  VSR26, VSR26_H, VSR26_J, VSR26_K,\n+  VSR27, VSR27_H, VSR27_J, VSR27_K,\n+  VSR28, VSR28_H, VSR28_J, VSR28_K,\n+  VSR29, VSR29_H, VSR29_J, VSR29_K,\n+  VSR30, VSR30_H, VSR30_J, VSR30_K,\n+  VSR31, VSR31_H, VSR31_J, VSR31_K,\n+  VSR32, VSR32_H, VSR32_J, VSR32_K,\n+  VSR33, VSR33_H, VSR33_J, VSR33_K,\n+  VSR34, VSR34_H, VSR34_J, VSR34_K,\n+  VSR35, VSR35_H, VSR35_J, VSR35_K,\n+  VSR36, VSR36_H, VSR36_J, VSR36_K,\n+  VSR37, VSR37_H, VSR37_J, VSR37_K,\n+  VSR38, VSR38_H, VSR38_J, VSR38_K,\n+  VSR39, VSR39_H, VSR39_J, VSR39_K,\n+  VSR40, VSR40_H, VSR40_J, VSR40_K,\n+  VSR41, VSR41_H, VSR41_J, VSR41_K,\n+  VSR42, VSR42_H, VSR42_J, VSR42_K,\n+  VSR43, VSR43_H, VSR43_J, VSR43_K,\n+  VSR44, VSR44_H, VSR44_J, VSR44_K,\n+  VSR45, VSR45_H, VSR45_J, VSR45_K,\n+  VSR46, VSR46_H, VSR46_J, VSR46_K,\n+  VSR47, VSR47_H, VSR47_J, VSR47_K,\n+  VSR48, VSR48_H, VSR48_J, VSR48_K,\n+  VSR49, VSR49_H, VSR49_J, VSR49_K,\n+  VSR50, VSR50_H, VSR50_J, VSR50_K,\n+  VSR51, VSR51_H, VSR51_J, VSR51_K,\n+  VSR52, VSR52_H, VSR52_J, VSR52_K,\n+  VSR53, VSR53_H, VSR53_J, VSR53_K,\n+  VSR54, VSR54_H, VSR54_J, VSR54_K,\n+  VSR55, VSR55_H, VSR55_J, VSR55_K,\n+  VSR56, VSR56_H, VSR56_J, VSR56_K,\n+  VSR57, VSR57_H, VSR57_J, VSR57_K,\n+  VSR58, VSR58_H, VSR58_J, VSR58_K,\n+  VSR59, VSR59_H, VSR59_J, VSR59_K,\n+  VSR60, VSR60_H, VSR60_J, VSR60_K,\n+  VSR61, VSR61_H, VSR61_J, VSR61_K,\n+  VSR62, VSR62_H, VSR62_J, VSR62_K,\n+  VSR63, VSR63_H, VSR63_J, VSR63_K\n@@ -913,22 +1168,32 @@\n-  \/\/ Attention: Only these ones are saved & restored at safepoint by RegisterSaver.\n-  VSR32,\n-  VSR33,\n-  VSR34,\n-  VSR35,\n-  VSR36,\n-  VSR37,\n-  VSR38,\n-  VSR39,\n-  VSR40,\n-  VSR41,\n-  VSR42,\n-  VSR43,\n-  VSR44,\n-  VSR45,\n-  VSR46,\n-  VSR47,\n-  VSR48,\n-  VSR49,\n-  VSR50,\n-  VSR51\n-  \/\/ VSR52-VSR63 \/\/ nv!\n+  VSR32, VSR32_H, VSR32_J, VSR32_K,\n+  VSR33, VSR33_H, VSR33_J, VSR33_K,\n+  VSR34, VSR34_H, VSR34_J, VSR34_K,\n+  VSR35, VSR35_H, VSR35_J, VSR35_K,\n+  VSR36, VSR36_H, VSR36_J, VSR36_K,\n+  VSR37, VSR37_H, VSR37_J, VSR37_K,\n+  VSR38, VSR38_H, VSR38_J, VSR38_K,\n+  VSR39, VSR39_H, VSR39_J, VSR39_K,\n+  VSR40, VSR40_H, VSR40_J, VSR40_K,\n+  VSR41, VSR41_H, VSR41_J, VSR41_K,\n+  VSR42, VSR42_H, VSR42_J, VSR42_K,\n+  VSR43, VSR43_H, VSR43_J, VSR43_K,\n+  VSR44, VSR44_H, VSR44_J, VSR44_K,\n+  VSR45, VSR45_H, VSR45_J, VSR45_K,\n+  VSR46, VSR46_H, VSR46_J, VSR46_K,\n+  VSR47, VSR47_H, VSR47_J, VSR47_K,\n+  VSR48, VSR48_H, VSR48_J, VSR48_K,\n+  VSR49, VSR49_H, VSR49_J, VSR49_K,\n+  VSR50, VSR50_H, VSR50_J, VSR50_K,\n+  VSR51, VSR51_H, VSR51_J, VSR51_K,\n+  VSR52, VSR52_H, VSR52_J, VSR52_K, \/\/ non-volatile\n+  VSR53, VSR53_H, VSR53_J, VSR53_K, \/\/ non-volatile\n+  VSR54, VSR54_H, VSR54_J, VSR54_K, \/\/ non-volatile\n+  VSR55, VSR55_H, VSR55_J, VSR55_K, \/\/ non-volatile\n+  VSR56, VSR56_H, VSR56_J, VSR56_K, \/\/ non-volatile\n+  VSR57, VSR57_H, VSR57_J, VSR57_K, \/\/ non-volatile\n+  VSR58, VSR58_H, VSR58_J, VSR58_K, \/\/ non-volatile\n+  VSR59, VSR59_H, VSR59_J, VSR59_K, \/\/ non-volatile\n+  VSR60, VSR60_H, VSR60_J, VSR60_K, \/\/ non-volatile\n+  VSR61, VSR61_H, VSR61_J, VSR61_K, \/\/ non-volatile\n+  VSR62, VSR62_H, VSR62_J, VSR62_K, \/\/ non-volatile\n+  VSR63, VSR63_H, VSR63_J, VSR63_K  \/\/ non-volatile\n@@ -1664,2 +1929,1 @@\n-  \/\/ We have 64 vector-scalar registers, starting at index 128.\n-  if (reg < 64+64+64) return rc_vs;\n+  assert(OptoReg::is_stack(reg) || reg >= 128+8, \"flags are not expected\");\n@@ -1667,2 +1931,2 @@\n-  \/\/ Between float regs & stack are the flags regs.\n-  assert(OptoReg::is_stack(reg) || reg < 64+64+64, \"blow up if spilling flags\");\n+  \/\/ We have 64 vector-scalar registers, starting at index 136.\n+  if (reg < 136+256) return rc_vs;\n@@ -1670,0 +1934,1 @@\n+  assert(OptoReg::is_stack(reg), \"what else is it?\");\n","filename":"src\/hotspot\/cpu\/ppc\/ppc.ad","additions":419,"deletions":154,"binary":false,"changes":573,"status":"modified"},{"patch":"@@ -217,0 +217,1 @@\n+  constexpr bool is_nonvolatile() const { return (14 <= _encoding && _encoding <= 31 ); }\n@@ -326,0 +327,1 @@\n+  constexpr bool is_nonvolatile() const { return (20 <= _encoding && _encoding <= 31 ); }\n@@ -483,1 +485,1 @@\n-    max_vsr = max_fpr + VectorSRegister::number_of_registers,\n+    max_vsr = max_fpr + VectorSRegister::number_of_registers * 4,\n","filename":"src\/hotspot\/cpu\/ppc\/register_ppc.hpp","additions":3,"deletions":1,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -246,1 +246,11 @@\n-  RegisterSaver_LiveVSReg( VSR51 )\n+  RegisterSaver_LiveVSReg( VSR51 ),\n+  RegisterSaver_LiveVSReg( VSR52 ),\n+  RegisterSaver_LiveVSReg( VSR53 ),\n+  RegisterSaver_LiveVSReg( VSR54 ),\n+  RegisterSaver_LiveVSReg( VSR55 ),\n+  RegisterSaver_LiveVSReg( VSR56 ),\n+  RegisterSaver_LiveVSReg( VSR57 ),\n+  RegisterSaver_LiveVSReg( VSR58 ),\n+  RegisterSaver_LiveVSReg( VSR59 ),\n+  RegisterSaver_LiveVSReg( VSR60 ),\n+  RegisterSaver_LiveVSReg( VSR61 )\n","filename":"src\/hotspot\/cpu\/ppc\/sharedRuntime_ppc.cpp","additions":11,"deletions":1,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -97,0 +97,2 @@\n+    int save_nonvolatile_registers_size = __ save_nonvolatile_registers_size(true, SuperwordUseVSX);\n+\n@@ -98,0 +100,1 @@\n+    STATIC_ASSERT(StackAlignmentInBytes == 16);\n@@ -100,1 +103,1 @@\n-    assert((sizeof(frame::spill_nonvolatiles) % 16) == 0,     \"unaligned\");\n+    assert((save_nonvolatile_registers_size % 16) == 0,       \"unaligned\");\n@@ -109,0 +112,2 @@\n+    Register r_arg_argument_addr            = R8;\n+    Register r_arg_argument_count           = R9;\n@@ -111,3 +116,3 @@\n-    Register r_temp                         = R24;\n-    Register r_top_of_arguments_addr        = R25;\n-    Register r_entryframe_fp                = R26;\n+    Register r_entryframe_fp                = R2; \/\/ volatile\n+    Register r_argument_size                = R11_scratch1; \/\/ volatile\n+    Register r_top_of_arguments_addr        = R21_tmp1;\n@@ -120,9 +125,1 @@\n-\n-      Register r_arg_argument_addr          = R8;\n-      Register r_arg_argument_count         = R9;\n-      Register r_frame_alignment_in_bytes   = R27;\n-      Register r_argument_addr              = R28;\n-      Register r_argumentcopy_addr          = R29;\n-      Register r_argument_size_in_bytes     = R30;\n-      Register r_frame_size                 = R23;\n-\n+      Register r_frame_size  = R12_scratch2; \/\/ volatile\n@@ -134,6 +131,0 @@\n-      \/\/ Zero extend arg_argument_count.\n-      __ clrldi(r_arg_argument_count, r_arg_argument_count, 32);\n-\n-      \/\/ Save non-volatiles GPRs to ENTRY_FRAME (not yet pushed, but it's safe).\n-      __ save_nonvolatile_gprs(R1_SP, _spill_nonvolatiles_neg(r14));\n-\n@@ -143,0 +134,20 @@\n+      \/\/ calculate frame size\n+      STATIC_ASSERT(Interpreter::logStackElementSize == 3);\n+\n+      \/\/ space for arguments aligned up: ((arg_count + 1) * 8) &~ 15\n+      __ addi(r_frame_size, r_arg_argument_count, 1);\n+      __ rldicr(r_frame_size, r_frame_size, 3, 63 - 4);\n+\n+      \/\/ this is the pure space for arguments\n+      __ sldi(r_argument_size, r_arg_argument_count, 3);\n+\n+      __ addi(r_frame_size, r_frame_size,\n+              save_nonvolatile_registers_size + frame::entry_frame_locals_size + frame::top_ijava_frame_abi_size);\n+\n+      \/\/ push ENTRY_FRAME\n+      __ push_frame(r_frame_size, R0);\n+\n+      \/\/ Save non-volatiles GPRs to ENTRY_FRAME (not yet pushed, but it's safe).\n+      __ save_nonvolatile_registers(r_entryframe_fp, -(frame::entry_frame_locals_size + save_nonvolatile_registers_size),\n+                                    true, SuperwordUseVSX);\n+\n@@ -149,0 +160,1 @@\n+      \/\/              [non-volatiles]\n@@ -153,24 +165,0 @@\n-      \/\/ calculate frame size\n-\n-      \/\/ unaligned size of arguments\n-      __ sldi(r_argument_size_in_bytes,\n-                  r_arg_argument_count, Interpreter::logStackElementSize);\n-      \/\/ arguments alignment (max 1 slot)\n-      \/\/ FIXME: use round_to() here\n-      __ andi_(r_frame_alignment_in_bytes, r_arg_argument_count, 1);\n-      __ sldi(r_frame_alignment_in_bytes,\n-              r_frame_alignment_in_bytes, Interpreter::logStackElementSize);\n-\n-      \/\/ size = unaligned size of arguments + top abi's size\n-      __ addi(r_frame_size, r_argument_size_in_bytes,\n-              frame::top_ijava_frame_abi_size);\n-      \/\/ size += arguments alignment\n-      __ add(r_frame_size,\n-             r_frame_size, r_frame_alignment_in_bytes);\n-      \/\/ size += size of call_stub locals\n-      __ addi(r_frame_size,\n-              r_frame_size, frame::entry_frame_locals_size);\n-\n-      \/\/ push ENTRY_FRAME\n-      __ push_frame(r_frame_size, r_temp);\n-\n@@ -178,6 +166,3 @@\n-      __ std(r_arg_call_wrapper_addr,\n-             _entry_frame_locals_neg(call_wrapper_address), r_entryframe_fp);\n-      __ std(r_arg_result_addr,\n-             _entry_frame_locals_neg(result_address), r_entryframe_fp);\n-      __ std(r_arg_result_type,\n-             _entry_frame_locals_neg(result_type), r_entryframe_fp);\n+      __ std(r_arg_call_wrapper_addr, _entry_frame_locals_neg(call_wrapper_address), r_entryframe_fp);\n+      __ std(r_arg_result_addr, _entry_frame_locals_neg(result_address), r_entryframe_fp);\n+      __ std(r_arg_result_type, _entry_frame_locals_neg(result_type), r_entryframe_fp);\n@@ -186,1 +171,0 @@\n-\n@@ -191,5 +175,3 @@\n-      \/\/ FIXME: why not simply use SP+frame::top_ijava_frame_size?\n-      __ addi(r_top_of_arguments_addr,\n-              R1_SP, frame::top_ijava_frame_abi_size);\n-      __ add(r_top_of_arguments_addr,\n-             r_top_of_arguments_addr, r_frame_alignment_in_bytes);\n+      __ addi(r_top_of_arguments_addr, r_entryframe_fp,\n+              -(save_nonvolatile_registers_size + frame::entry_frame_locals_size));\n+      __ sub(r_top_of_arguments_addr, r_top_of_arguments_addr, r_argument_size);\n@@ -203,0 +185,2 @@\n+        Register r_argument_addr     = R22_tmp2;\n+        Register r_argumentcopy_addr = R23_tmp3;\n@@ -210,2 +194,1 @@\n-        __ add(r_argument_addr,\n-                   r_arg_argument_addr, r_argument_size_in_bytes);\n+        __ add(r_argument_addr, r_arg_argument_addr, r_argument_size);\n@@ -219,1 +202,1 @@\n-          __ ld(r_temp, 0, r_argument_addr);\n+          __ ld(R0, 0, r_argument_addr);\n@@ -222,1 +205,1 @@\n-          __ std(r_temp, 0, r_argumentcopy_addr);\n+          __ std(R0, 0, r_argumentcopy_addr);\n@@ -237,5 +220,1 @@\n-      Register r_new_arg_entry = R14;\n-      assert_different_registers(r_new_arg_entry, r_top_of_arguments_addr,\n-                                 r_arg_method, r_arg_thread);\n-\n-      __ mr(r_new_arg_entry, r_arg_entry);\n+      assert_different_registers(r_arg_entry, r_top_of_arguments_addr, r_arg_method, r_arg_thread);\n@@ -265,1 +244,1 @@\n-      __ load_const_optimized(R25_templateTableBase, (address)Interpreter::dispatch_table((TosState)0), R11_scratch1);\n+      __ load_const_optimized(R25_templateTableBase, (address)Interpreter::dispatch_table((TosState)0), R0);\n@@ -271,0 +250,1 @@\n+      \/\/              [non-volatiles]\n@@ -277,1 +257,1 @@\n-      __ load_const_optimized(R29_TOC, MacroAssembler::global_toc(), R11_scratch1);\n+      __ load_const_optimized(R29_TOC, MacroAssembler::global_toc(), R0);\n@@ -284,1 +264,1 @@\n-      \/\/ Do a light-weight C-call here, r_new_arg_entry holds the address\n+      \/\/ Do a light-weight C-call here, r_arg_entry holds the address\n@@ -287,3 +267,3 @@\n-      assert(r_new_arg_entry != tos && r_new_arg_entry != R19_method && r_new_arg_entry != R16_thread,\n-             \"trashed r_new_arg_entry\");\n-      return_address = __ call_stub(r_new_arg_entry);\n+      assert(r_arg_entry != tos && r_arg_entry != R19_method && r_arg_entry != R16_thread,\n+             \"trashed r_arg_entry\");\n+      return_address = __ call_stub(r_arg_entry);\n@@ -301,0 +281,1 @@\n+      \/\/              [non-volatiles]\n@@ -313,3 +294,2 @@\n-      Register r_entryframe_fp = R30;\n-      Register r_lr            = R7_ARG5;\n-      Register r_cr            = R8_ARG6;\n+      Register r_lr = R11_scratch1;\n+      Register r_cr = R12_scratch2;\n@@ -321,1 +301,1 @@\n-      __ ld(r_entryframe_fp, _abi0(callers_sp), R1_SP);\n+      __ ld(r_entryframe_fp, _abi0(callers_sp), R1_SP); \/\/ restore after call\n@@ -323,4 +303,2 @@\n-      __ ld(r_arg_result_addr,\n-            _entry_frame_locals_neg(result_address), r_entryframe_fp);\n-      __ ld(r_arg_result_type,\n-            _entry_frame_locals_neg(result_type), r_entryframe_fp);\n+      __ ld(r_arg_result_addr, _entry_frame_locals_neg(result_address), r_entryframe_fp);\n+      __ ld(r_arg_result_type, _entry_frame_locals_neg(result_type), r_entryframe_fp);\n@@ -329,6 +307,2 @@\n-\n-      \/\/ pop frame and restore non-volatiles, LR and CR\n-      __ mr(R1_SP, r_entryframe_fp);\n-      __ pop_cont_fastpath();\n-      __ mtcr(r_cr);\n-      __ mtlr(r_lr);\n+      __ mtcr(r_cr); \/\/ restore CR\n+      __ mtlr(r_lr); \/\/ restore LR\n@@ -338,4 +312,7 @@\n-      __ cmpwi(CR0, r_arg_result_type, T_OBJECT);\n-      __ cmpwi(CR1, r_arg_result_type, T_LONG);\n-      __ cmpwi(CR5, r_arg_result_type, T_FLOAT);\n-      __ cmpwi(CR6, r_arg_result_type, T_DOUBLE);\n+      \/\/ Using volatile CRs.\n+      __ cmpwi(CR1, r_arg_result_type, T_OBJECT);\n+      __ cmpwi(CR5, r_arg_result_type, T_LONG);\n+      __ cmpwi(CR6, r_arg_result_type, T_FLOAT);\n+      __ cmpwi(CR7, r_arg_result_type, T_DOUBLE);\n+\n+      __ pop_cont_fastpath(); \/\/ kills CR0, uses R16_thread\n@@ -344,1 +321,2 @@\n-      __ restore_nonvolatile_gprs(R1_SP, _spill_nonvolatiles_neg(r14));\n+      __ restore_nonvolatile_registers(r_entryframe_fp, -(frame::entry_frame_locals_size + save_nonvolatile_registers_size),\n+                                       true, SuperwordUseVSX);\n@@ -346,0 +324,2 @@\n+      \/\/ pop frame\n+      __ mr(R1_SP, r_entryframe_fp);\n@@ -354,7 +334,4 @@\n-      \/\/ All non-volatiles have been restored at this point!!\n-      assert(R3_RET == R3, \"R3_RET should be R3\");\n-\n-      __ beq(CR0, ret_is_object);\n-      __ beq(CR1, ret_is_long);\n-      __ beq(CR5, ret_is_float);\n-      __ beq(CR6, ret_is_double);\n+      __ beq(CR1, ret_is_object);\n+      __ beq(CR5, ret_is_long);\n+      __ beq(CR6, ret_is_float);\n+      __ beq(CR7, ret_is_double);\n@@ -367,4 +344,0 @@\n-      __ bind(ret_is_object);\n-      __ std(R3_RET, 0, r_arg_result_addr);\n-      __ blr(); \/\/ return to caller\n-\n@@ -372,0 +345,1 @@\n+      __ bind(ret_is_object);\n","filename":"src\/hotspot\/cpu\/ppc\/stubGenerator_ppc.cpp","additions":74,"deletions":100,"binary":false,"changes":174,"status":"modified"},{"patch":"@@ -122,0 +122,1 @@\n+  int save_nonvolatile_registers_size = __ save_nonvolatile_registers_size(false, false);\n@@ -124,1 +125,1 @@\n-  __ save_nonvolatile_gprs(R1_SP, _spill_nonvolatiles_neg(r14));\n+  __ save_nonvolatile_registers(R1_SP, -save_nonvolatile_registers_size, false, false);\n@@ -127,1 +128,1 @@\n-  __ push_frame_reg_args_nonvolatiles(0, R11_scratch1);\n+  __ push_frame(frame::native_abi_reg_args_size + save_nonvolatile_registers_size, R11_scratch1);\n@@ -312,1 +313,1 @@\n-  __ restore_nonvolatile_gprs(R1_SP, _spill_nonvolatiles_neg(r14));\n+  __ restore_nonvolatile_registers(R1_SP, -save_nonvolatile_registers_size, false, false);\n","filename":"src\/hotspot\/cpu\/ppc\/templateInterpreterGenerator_ppc.cpp","additions":4,"deletions":3,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -3,1 +3,1 @@\n- * Copyright (c) 2023 SAP SE. All rights reserved.\n+ * Copyright (c) 2023, 2025 SAP SE. All rights reserved.\n@@ -38,22 +38,0 @@\n-\/\/ for callee saved regs, according to the caller's ABI\n-static int compute_reg_save_area_size(const ABIDescriptor& abi) {\n-  int size = 0;\n-  for (int i = 0; i < Register::number_of_registers; i++) {\n-    Register reg = as_Register(i);\n-    \/\/ R1 saved\/restored by prologue\/epilogue, R13 (system thread) won't get modified!\n-    if (reg == R1_SP || reg == R13) continue;\n-    if (!abi.is_volatile_reg(reg)) {\n-      size += 8; \/\/ bytes\n-    }\n-  }\n-\n-  for (int i = 0; i < FloatRegister::number_of_registers; i++) {\n-    FloatRegister reg = as_FloatRegister(i);\n-    if (!abi.is_volatile_reg(reg)) {\n-      size += 8; \/\/ bytes\n-    }\n-  }\n-\n-  return size;\n-}\n-\n@@ -61,6 +39,0 @@\n-  \/\/ 1. iterate all registers in the architecture\n-  \/\/     - check if they are volatile or not for the given abi\n-  \/\/     - if NOT, we need to save it here\n-\n-  int offset = reg_save_area_offset;\n-\n@@ -68,18 +40,1 @@\n-  for (int i = 0; i < Register::number_of_registers; i++) {\n-    Register reg = as_Register(i);\n-    \/\/ R1 saved\/restored by prologue\/epilogue, R13 (system thread) won't get modified!\n-    if (reg == R1_SP || reg == R13) continue;\n-    if (!abi.is_volatile_reg(reg)) {\n-      __ std(reg, offset, R1_SP);\n-      offset += 8;\n-    }\n-  }\n-\n-  for (int i = 0; i < FloatRegister::number_of_registers; i++) {\n-    FloatRegister reg = as_FloatRegister(i);\n-    if (!abi.is_volatile_reg(reg)) {\n-      __ stfd(reg, offset, R1_SP);\n-      offset += 8;\n-    }\n-  }\n-\n+  __ save_nonvolatile_registers(R1_SP, reg_save_area_offset, true, SuperwordUseVSX);\n@@ -90,6 +45,0 @@\n-  \/\/ 1. iterate all registers in the architecture\n-  \/\/     - check if they are volatile or not for the given abi\n-  \/\/     - if NOT, we need to restore it here\n-\n-  int offset = reg_save_area_offset;\n-\n@@ -97,18 +46,1 @@\n-  for (int i = 0; i < Register::number_of_registers; i++) {\n-    Register reg = as_Register(i);\n-    \/\/ R1 saved\/restored by prologue\/epilogue, R13 (system thread) won't get modified!\n-    if (reg == R1_SP || reg == R13) continue;\n-    if (!abi.is_volatile_reg(reg)) {\n-      __ ld(reg, offset, R1_SP);\n-      offset += 8;\n-    }\n-  }\n-\n-  for (int i = 0; i < FloatRegister::number_of_registers; i++) {\n-    FloatRegister reg = as_FloatRegister(i);\n-    if (!abi.is_volatile_reg(reg)) {\n-      __ lfd(reg, offset, R1_SP);\n-      offset += 8;\n-    }\n-  }\n-\n+  __ restore_nonvolatile_registers(R1_SP, reg_save_area_offset, true, SuperwordUseVSX);\n@@ -143,1 +75,2 @@\n-  int reg_save_area_size = compute_reg_save_area_size(abi);\n+  MacroAssembler* _masm = new MacroAssembler(&buffer);\n+  int reg_save_area_size = __ save_nonvolatile_registers_size(true, SuperwordUseVSX);\n@@ -150,0 +83,3 @@\n+  if (SuperwordUseVSX) { \/\/ VectorRegisters want alignment\n+    reg_save_area_offset = align_up(reg_save_area_offset, StackAlignmentInBytes);\n+  }\n@@ -205,1 +141,0 @@\n-  MacroAssembler* _masm = new MacroAssembler(&buffer);\n","filename":"src\/hotspot\/cpu\/ppc\/upcallLinker_ppc.cpp","additions":8,"deletions":73,"binary":false,"changes":81,"status":"modified"},{"patch":"@@ -2,2 +2,2 @@\n- * Copyright (c) 2001, 2022, Oracle and\/or its affiliates. All rights reserved.\n- * Copyright (c) 2012, 2022 SAP SE. All rights reserved.\n+ * Copyright (c) 2001, 2025, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2012, 2025 SAP SE. All rights reserved.\n@@ -65,1 +65,1 @@\n-  return ::as_VectorSRegister(value() - ConcreteRegisterImpl::max_fpr);\n+  return ::as_VectorSRegister((value() - ConcreteRegisterImpl::max_fpr) >> 2);\n","filename":"src\/hotspot\/cpu\/ppc\/vmreg_ppc.hpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -2,2 +2,2 @@\n- * Copyright (c) 2002, 2022, Oracle and\/or its affiliates. All rights reserved.\n- * Copyright (c) 2012, 2022 SAP SE. All rights reserved.\n+ * Copyright (c) 2002, 2025, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2012, 2025 SAP SE. All rights reserved.\n@@ -44,1 +44,2 @@\n-  return VMRegImpl::as_VMReg((encoding()) + ConcreteRegisterImpl::max_fpr);\n+  \/\/ Four halves, multiply by 4.\n+  return VMRegImpl::as_VMReg((encoding() << 2) + ConcreteRegisterImpl::max_fpr);\n","filename":"src\/hotspot\/cpu\/ppc\/vmreg_ppc.inline.hpp","additions":4,"deletions":3,"binary":false,"changes":7,"status":"modified"}]}