{"files":[{"patch":"@@ -2,2 +2,2 @@\n- * Copyright (c) 2002, 2024, Oracle and\/or its affiliates. All rights reserved.\n- * Copyright (c) 2012, 2024 SAP SE. All rights reserved.\n+ * Copyright (c) 2002, 2025, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2012, 2025 SAP SE. All rights reserved.\n@@ -541,0 +541,2 @@\n+    LXVP_OPCODE    = ( 6u << OPCODE_SHIFT             ),\n+    STXVP_OPCODE   = ( 6u << OPCODE_SHIFT |    1u     ),\n@@ -1246,0 +1248,5 @@\n+  static int vsrtp(     int         x)  {\n+    assert((x & 1) == 0, \"must be even\");\n+    return opp_u_field((x & 0x1F) >> 1, 9, 6) | opp_u_field((x & 0x20) >> 5, 10, 10);\n+  }\n+  static int vsrsp(     int         x)  { return  vsrtp(x); }\n@@ -1254,0 +1261,2 @@\n+  static int vsrtp(  VectorSRegister r)  { return  vsrtp(r->encoding());}\n+  static int vsrsp(  VectorSRegister r)  { return  vsrsp(r->encoding());}\n@@ -2361,0 +2370,2 @@\n+  inline void lxvp(     VectorSRegister d, int si16, Register a);\n+  inline void stxvp(    VectorSRegister d, int si16, Register a);\n","filename":"src\/hotspot\/cpu\/ppc\/assembler_ppc.hpp","additions":13,"deletions":2,"binary":false,"changes":15,"status":"modified"},{"patch":"@@ -865,2 +865,4 @@\n-inline void Assembler::lxv(     VectorSRegister d, int ui16, Register a)     { assert(is_aligned(ui16, 16), \"displacement must be a multiple of 16\"); emit_int32( LXV_OPCODE  | vsrt_dq(d) | ra0mem(a) | uimm(ui16, 16)); }\n-inline void Assembler::stxv(    VectorSRegister d, int ui16, Register a)     { assert(is_aligned(ui16, 16), \"displacement must be a multiple of 16\"); emit_int32( STXV_OPCODE  | vsrs_dq(d) | ra0mem(a) | uimm(ui16, 16)); }\n+inline void Assembler::lxv(     VectorSRegister d, int si16, Register a)     { assert(is_aligned(si16, 16), \"displacement must be a multiple of 16\"); emit_int32( LXV_OPCODE   | vsrt_dq(d) | ra0mem(a) | simm(si16, 16)); }\n+inline void Assembler::stxv(    VectorSRegister d, int si16, Register a)     { assert(is_aligned(si16, 16), \"displacement must be a multiple of 16\"); emit_int32( STXV_OPCODE  | vsrs_dq(d) | ra0mem(a) | simm(si16, 16)); }\n+inline void Assembler::lxvp(    VectorSRegister d, int si16, Register a)     { assert(is_aligned(si16, 16), \"displacement must be a multiple of 16\"); emit_int32( LXVP_OPCODE  | vsrtp(d)   | ra0mem(a) | simm(si16, 16)); }\n+inline void Assembler::stxvp(   VectorSRegister d, int si16, Register a)     { assert(is_aligned(si16, 16), \"displacement must be a multiple of 16\"); emit_int32( STXVP_OPCODE | vsrsp(d)   | ra0mem(a) | simm(si16, 16)); }\n","filename":"src\/hotspot\/cpu\/ppc\/assembler_ppc.inline.hpp","additions":4,"deletions":2,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2020, 2023, SAP SE. All rights reserved.\n+ * Copyright (c) 2020, 2025, SAP SE. All rights reserved.\n@@ -38,10 +38,0 @@\n-bool ABIDescriptor::is_volatile_reg(Register reg) const {\n-  return _integer_argument_registers.contains(reg)\n-    || _integer_additional_volatile_registers.contains(reg);\n-}\n-\n-bool ABIDescriptor::is_volatile_reg(FloatRegister reg) const {\n-    return _float_argument_registers.contains(reg)\n-        || _float_additional_volatile_registers.contains(reg);\n-}\n-\n@@ -65,4 +55,0 @@\n-  objArrayOop volatileStorage = jdk_internal_foreign_abi_ABIDescriptor::volatileStorage(abi_oop);\n-  parse_register_array(volatileStorage, StorageType::INTEGER, abi._integer_additional_volatile_registers, as_Register);\n-  parse_register_array(volatileStorage, StorageType::FLOAT, abi._float_additional_volatile_registers, as_FloatRegister);\n-\n","filename":"src\/hotspot\/cpu\/ppc\/foreignGlobals_ppc.cpp","additions":1,"deletions":15,"binary":false,"changes":16,"status":"modified"},{"patch":"@@ -2,2 +2,2 @@\n- * Copyright (c) 2022, 2023, Oracle and\/or its affiliates. All rights reserved.\n- * Copyright (c) 2023 SAP SE. All rights reserved.\n+ * Copyright (c) 2022, 2025, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2023, 2025 SAP SE. All rights reserved.\n@@ -37,3 +37,0 @@\n-  GrowableArray<Register> _integer_additional_volatile_registers;\n-  GrowableArray<FloatRegister> _float_additional_volatile_registers;\n-\n","filename":"src\/hotspot\/cpu\/ppc\/foreignGlobals_ppc.hpp","additions":2,"deletions":5,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -2,2 +2,2 @@\n- * Copyright (c) 2000, 2024, Oracle and\/or its affiliates. All rights reserved.\n- * Copyright (c) 2012, 2024 SAP SE. All rights reserved.\n+ * Copyright (c) 2000, 2025, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2012, 2025 SAP SE. All rights reserved.\n@@ -137,50 +137,0 @@\n-  \/\/ non-volatile GPRs:\n-\n-  struct spill_nonvolatiles {\n-    uint64_t r14;\n-    uint64_t r15;                                 \/\/_16\n-    uint64_t r16;\n-    uint64_t r17;                                 \/\/_16\n-    uint64_t r18;\n-    uint64_t r19;                                 \/\/_16\n-    uint64_t r20;\n-    uint64_t r21;                                 \/\/_16\n-    uint64_t r22;\n-    uint64_t r23;                                 \/\/_16\n-    uint64_t r24;\n-    uint64_t r25;                                 \/\/_16\n-    uint64_t r26;\n-    uint64_t r27;                                 \/\/_16\n-    uint64_t r28;\n-    uint64_t r29;                                 \/\/_16\n-    uint64_t r30;\n-    uint64_t r31;                                 \/\/_16\n-\n-    double f14;\n-    double f15;\n-    double f16;\n-    double f17;\n-    double f18;\n-    double f19;\n-    double f20;\n-    double f21;\n-    double f22;\n-    double f23;\n-    double f24;\n-    double f25;\n-    double f26;\n-    double f27;\n-    double f28;\n-    double f29;\n-    double f30;\n-    double f31;\n-\n-    \/\/ aligned to frame::alignment_in_bytes (16)\n-  };\n-\n-  enum {\n-    spill_nonvolatiles_size = sizeof(spill_nonvolatiles)\n-  };\n-\n-  #define _spill_nonvolatiles_neg(_component) \\\n-     (int)(-frame::spill_nonvolatiles_size + offset_of(frame::spill_nonvolatiles, _component))\n@@ -233,0 +183,1 @@\n+  \/\/            [non-volatiles]\n@@ -295,1 +246,0 @@\n-    uint64_t r[spill_nonvolatiles_size\/sizeof(uint64_t)];\n","filename":"src\/hotspot\/cpu\/ppc\/frame_ppc.hpp","additions":3,"deletions":53,"binary":false,"changes":56,"status":"modified"},{"patch":"@@ -340,1 +340,1 @@\n-        reg_save_index += 2;\n+        reg_save_index += (2 + (reg_save_index & 1)); \/\/ 2 slots + alignment if needed\n@@ -343,0 +343,1 @@\n+        int spill_offset = offset - reg_save_index * BytesPerWord;\n@@ -344,2 +345,6 @@\n-          _masm->addi(spill_addr, R1_SP, offset - reg_save_index * BytesPerWord);\n-          _masm->stxvd2x(vs_reg, spill_addr);\n+          if (PowerArchitecturePPC64 >= 9) {\n+            _masm->stxv(vs_reg, spill_offset, R1_SP);\n+          } else {\n+            _masm->addi(spill_addr, R1_SP, spill_offset);\n+            _masm->stxvd2x(vs_reg, spill_addr);\n+          }\n@@ -347,2 +352,6 @@\n-          _masm->addi(spill_addr, R1_SP, offset - reg_save_index * BytesPerWord);\n-          _masm->lxvd2x(vs_reg, spill_addr);\n+          if (PowerArchitecturePPC64 >= 9) {\n+            _masm->lxv(vs_reg, spill_offset, R1_SP);\n+          } else {\n+            _masm->addi(spill_addr, R1_SP, spill_offset);\n+            _masm->lxvd2x(vs_reg, spill_addr);\n+          }\n","filename":"src\/hotspot\/cpu\/ppc\/gc\/shared\/barrierSetAssembler_ppc.cpp","additions":14,"deletions":5,"binary":false,"changes":19,"status":"modified"},{"patch":"@@ -777,42 +777,33 @@\n-\/\/ Uses ordering which corresponds to ABI:\n-\/\/    _savegpr0_14:  std  r14,-144(r1)\n-\/\/    _savegpr0_15:  std  r15,-136(r1)\n-\/\/    _savegpr0_16:  std  r16,-128(r1)\n-void MacroAssembler::save_nonvolatile_gprs(Register dst, int offset) {\n-  std(R14, offset, dst);   offset += 8;\n-  std(R15, offset, dst);   offset += 8;\n-  std(R16, offset, dst);   offset += 8;\n-  std(R17, offset, dst);   offset += 8;\n-  std(R18, offset, dst);   offset += 8;\n-  std(R19, offset, dst);   offset += 8;\n-  std(R20, offset, dst);   offset += 8;\n-  std(R21, offset, dst);   offset += 8;\n-  std(R22, offset, dst);   offset += 8;\n-  std(R23, offset, dst);   offset += 8;\n-  std(R24, offset, dst);   offset += 8;\n-  std(R25, offset, dst);   offset += 8;\n-  std(R26, offset, dst);   offset += 8;\n-  std(R27, offset, dst);   offset += 8;\n-  std(R28, offset, dst);   offset += 8;\n-  std(R29, offset, dst);   offset += 8;\n-  std(R30, offset, dst);   offset += 8;\n-  std(R31, offset, dst);   offset += 8;\n-\n-  stfd(F14, offset, dst);   offset += 8;\n-  stfd(F15, offset, dst);   offset += 8;\n-  stfd(F16, offset, dst);   offset += 8;\n-  stfd(F17, offset, dst);   offset += 8;\n-  stfd(F18, offset, dst);   offset += 8;\n-  stfd(F19, offset, dst);   offset += 8;\n-  stfd(F20, offset, dst);   offset += 8;\n-  stfd(F21, offset, dst);   offset += 8;\n-  stfd(F22, offset, dst);   offset += 8;\n-  stfd(F23, offset, dst);   offset += 8;\n-  stfd(F24, offset, dst);   offset += 8;\n-  stfd(F25, offset, dst);   offset += 8;\n-  stfd(F26, offset, dst);   offset += 8;\n-  stfd(F27, offset, dst);   offset += 8;\n-  stfd(F28, offset, dst);   offset += 8;\n-  stfd(F29, offset, dst);   offset += 8;\n-  stfd(F30, offset, dst);   offset += 8;\n-  stfd(F31, offset, dst);\n+void MacroAssembler::save_nonvolatile_registers(Register dst, int offset, bool include_fp_regs, bool include_vector_regs) {\n+  BLOCK_COMMENT(\"save_nonvolatile_registers {\");\n+\n+  for (int i = 14; i < 32; i++) {\n+    std(as_Register(i), offset, dst);\n+    offset += 8;\n+  }\n+\n+  if (include_fp_regs) {\n+    for (int i = 14; i < 32; i++) {\n+      stfd(as_FloatRegister(i), offset, dst);\n+      offset += 8;\n+    }\n+  }\n+\n+  if (include_vector_regs) {\n+    assert(is_aligned(offset, StackAlignmentInBytes), \"should be\");\n+    if (PowerArchitecturePPC64 >= 10) {\n+      for (int i = 20; i < 32; i += 2) {\n+        stxvp(as_VectorRegister(i)->to_vsr(), offset, dst);\n+        offset += 32;\n+      }\n+    } else {\n+      Register spill_addr = R0;\n+      for (int i = 20; i < 32; i++) {\n+        addi(spill_addr, dst, offset);\n+        stxvd2x(as_VectorRegister(i)->to_vsr(), spill_addr);\n+        offset += 16;\n+      }\n+    }\n+  }\n+\n+  BLOCK_COMMENT(\"} save_nonvolatile_registers \");\n@@ -821,43 +812,33 @@\n-\/\/ Uses ordering which corresponds to ABI:\n-\/\/    _restgpr0_14:  ld   r14,-144(r1)\n-\/\/    _restgpr0_15:  ld   r15,-136(r1)\n-\/\/    _restgpr0_16:  ld   r16,-128(r1)\n-void MacroAssembler::restore_nonvolatile_gprs(Register src, int offset) {\n-  ld(R14, offset, src);   offset += 8;\n-  ld(R15, offset, src);   offset += 8;\n-  ld(R16, offset, src);   offset += 8;\n-  ld(R17, offset, src);   offset += 8;\n-  ld(R18, offset, src);   offset += 8;\n-  ld(R19, offset, src);   offset += 8;\n-  ld(R20, offset, src);   offset += 8;\n-  ld(R21, offset, src);   offset += 8;\n-  ld(R22, offset, src);   offset += 8;\n-  ld(R23, offset, src);   offset += 8;\n-  ld(R24, offset, src);   offset += 8;\n-  ld(R25, offset, src);   offset += 8;\n-  ld(R26, offset, src);   offset += 8;\n-  ld(R27, offset, src);   offset += 8;\n-  ld(R28, offset, src);   offset += 8;\n-  ld(R29, offset, src);   offset += 8;\n-  ld(R30, offset, src);   offset += 8;\n-  ld(R31, offset, src);   offset += 8;\n-\n-  \/\/ FP registers\n-  lfd(F14, offset, src);   offset += 8;\n-  lfd(F15, offset, src);   offset += 8;\n-  lfd(F16, offset, src);   offset += 8;\n-  lfd(F17, offset, src);   offset += 8;\n-  lfd(F18, offset, src);   offset += 8;\n-  lfd(F19, offset, src);   offset += 8;\n-  lfd(F20, offset, src);   offset += 8;\n-  lfd(F21, offset, src);   offset += 8;\n-  lfd(F22, offset, src);   offset += 8;\n-  lfd(F23, offset, src);   offset += 8;\n-  lfd(F24, offset, src);   offset += 8;\n-  lfd(F25, offset, src);   offset += 8;\n-  lfd(F26, offset, src);   offset += 8;\n-  lfd(F27, offset, src);   offset += 8;\n-  lfd(F28, offset, src);   offset += 8;\n-  lfd(F29, offset, src);   offset += 8;\n-  lfd(F30, offset, src);   offset += 8;\n-  lfd(F31, offset, src);\n+void MacroAssembler::restore_nonvolatile_registers(Register src, int offset, bool include_fp_regs, bool include_vector_regs) {\n+  BLOCK_COMMENT(\"restore_nonvolatile_registers {\");\n+\n+  for (int i = 14; i < 32; i++) {\n+    ld(as_Register(i), offset, src);\n+    offset += 8;\n+  }\n+\n+  if (include_fp_regs) {\n+    for (int i = 14; i < 32; i++) {\n+      lfd(as_FloatRegister(i), offset, src);\n+      offset += 8;\n+    }\n+  }\n+\n+  if (include_vector_regs) {\n+    assert(is_aligned(offset, StackAlignmentInBytes), \"should be\");\n+    if (PowerArchitecturePPC64 >= 10) {\n+      for (int i = 20; i < 32; i += 2) {\n+        lxvp(as_VectorRegister(i)->to_vsr(), offset, src);\n+        offset += 32;\n+      }\n+    } else {\n+      Register spill_addr = R0;\n+      for (int i = 20; i < 32; i++) {\n+        addi(spill_addr, src, offset);\n+        lxvd2x(as_VectorRegister(i)->to_vsr(), spill_addr);\n+        offset += 16;\n+      }\n+    }\n+  }\n+\n+  BLOCK_COMMENT(\"} restore_nonvolatile_registers\");\n@@ -1032,7 +1013,0 @@\n-\/\/ Setup up a new C frame with a spill area for non-volatile GPRs and\n-\/\/ additional space for local variables.\n-void MacroAssembler::push_frame_reg_args_nonvolatiles(unsigned int bytes,\n-                                                      Register tmp) {\n-  push_frame(bytes + frame::native_abi_reg_args_size + frame::spill_nonvolatiles_size, tmp);\n-}\n-\n","filename":"src\/hotspot\/cpu\/ppc\/macroAssembler_ppc.cpp","additions":66,"deletions":92,"binary":false,"changes":158,"status":"modified"},{"patch":"@@ -302,2 +302,8 @@\n-  void save_nonvolatile_gprs(   Register dst_base, int offset);\n-  void restore_nonvolatile_gprs(Register src_base, int offset);\n+  int  save_nonvolatile_registers_size(bool include_fp_regs, bool include_vector_regs) {\n+    int size = (32 - 14) * 8; \/\/ GP regs\n+    if (include_fp_regs) size += (32 - 14) * 8;\n+    if (include_vector_regs) size += (32 - 20) * 16;\n+    return size;\n+  }\n+  void save_nonvolatile_registers(   Register dst_base, int offset, bool include_fp_regs, bool include_vector_regs);\n+  void restore_nonvolatile_registers(Register src_base, int offset, bool include_fp_regs, bool include_vector_regs);\n@@ -337,4 +343,0 @@\n-  \/\/ Setup up a new C frame with a spill area for non-volatile GPRs and additional\n-  \/\/ space for local variables\n-  void push_frame_reg_args_nonvolatiles(unsigned int bytes, Register tmp);\n-\n","filename":"src\/hotspot\/cpu\/ppc\/macroAssembler_ppc.hpp","additions":8,"deletions":6,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -261,32 +261,160 @@\n-  reg_def VSR0 ( SOC, SOC, Op_VecX, 0, VMRegImpl::Bad());\n-  reg_def VSR1 ( SOC, SOC, Op_VecX, 1, VMRegImpl::Bad());\n-  reg_def VSR2 ( SOC, SOC, Op_VecX, 2, VMRegImpl::Bad());\n-  reg_def VSR3 ( SOC, SOC, Op_VecX, 3, VMRegImpl::Bad());\n-  reg_def VSR4 ( SOC, SOC, Op_VecX, 4, VMRegImpl::Bad());\n-  reg_def VSR5 ( SOC, SOC, Op_VecX, 5, VMRegImpl::Bad());\n-  reg_def VSR6 ( SOC, SOC, Op_VecX, 6, VMRegImpl::Bad());\n-  reg_def VSR7 ( SOC, SOC, Op_VecX, 7, VMRegImpl::Bad());\n-  reg_def VSR8 ( SOC, SOC, Op_VecX, 8, VMRegImpl::Bad());\n-  reg_def VSR9 ( SOC, SOC, Op_VecX, 9, VMRegImpl::Bad());\n-  reg_def VSR10 ( SOC, SOC, Op_VecX, 10, VMRegImpl::Bad());\n-  reg_def VSR11 ( SOC, SOC, Op_VecX, 11, VMRegImpl::Bad());\n-  reg_def VSR12 ( SOC, SOC, Op_VecX, 12, VMRegImpl::Bad());\n-  reg_def VSR13 ( SOC, SOC, Op_VecX, 13, VMRegImpl::Bad());\n-  reg_def VSR14 ( SOC, SOE, Op_VecX, 14, VMRegImpl::Bad());\n-  reg_def VSR15 ( SOC, SOE, Op_VecX, 15, VMRegImpl::Bad());\n-  reg_def VSR16 ( SOC, SOE, Op_VecX, 16, VMRegImpl::Bad());\n-  reg_def VSR17 ( SOC, SOE, Op_VecX, 17, VMRegImpl::Bad());\n-  reg_def VSR18 ( SOC, SOE, Op_VecX, 18, VMRegImpl::Bad());\n-  reg_def VSR19 ( SOC, SOE, Op_VecX, 19, VMRegImpl::Bad());\n-  reg_def VSR20 ( SOC, SOE, Op_VecX, 20, VMRegImpl::Bad());\n-  reg_def VSR21 ( SOC, SOE, Op_VecX, 21, VMRegImpl::Bad());\n-  reg_def VSR22 ( SOC, SOE, Op_VecX, 22, VMRegImpl::Bad());\n-  reg_def VSR23 ( SOC, SOE, Op_VecX, 23, VMRegImpl::Bad());\n-  reg_def VSR24 ( SOC, SOE, Op_VecX, 24, VMRegImpl::Bad());\n-  reg_def VSR25 ( SOC, SOE, Op_VecX, 25, VMRegImpl::Bad());\n-  reg_def VSR26 ( SOC, SOE, Op_VecX, 26, VMRegImpl::Bad());\n-  reg_def VSR27 ( SOC, SOE, Op_VecX, 27, VMRegImpl::Bad());\n-  reg_def VSR28 ( SOC, SOE, Op_VecX, 28, VMRegImpl::Bad());\n-  reg_def VSR29 ( SOC, SOE, Op_VecX, 29, VMRegImpl::Bad());\n-  reg_def VSR30 ( SOC, SOE, Op_VecX, 30, VMRegImpl::Bad());\n-  reg_def VSR31 ( SOC, SOE, Op_VecX, 31, VMRegImpl::Bad());\n+  reg_def VSR0   (SOC, SOC, Op_RegF, 0, VMRegImpl::Bad());\n+  reg_def VSR0_H (SOC, SOC, Op_RegF, 0, VMRegImpl::Bad());\n+  reg_def VSR0_J (SOC, SOC, Op_RegF, 0, VMRegImpl::Bad());\n+  reg_def VSR0_K (SOC, SOC, Op_RegF, 0, VMRegImpl::Bad());\n+\n+  reg_def VSR1   (SOC, SOC, Op_RegF, 1, VMRegImpl::Bad());\n+  reg_def VSR1_H (SOC, SOC, Op_RegF, 1, VMRegImpl::Bad());\n+  reg_def VSR1_J (SOC, SOC, Op_RegF, 1, VMRegImpl::Bad());\n+  reg_def VSR1_K (SOC, SOC, Op_RegF, 1, VMRegImpl::Bad());\n+\n+  reg_def VSR2   (SOC, SOC, Op_RegF, 2, VMRegImpl::Bad());\n+  reg_def VSR2_H (SOC, SOC, Op_RegF, 2, VMRegImpl::Bad());\n+  reg_def VSR2_J (SOC, SOC, Op_RegF, 2, VMRegImpl::Bad());\n+  reg_def VSR2_K (SOC, SOC, Op_RegF, 2, VMRegImpl::Bad());\n+\n+  reg_def VSR3   (SOC, SOC, Op_RegF, 3, VMRegImpl::Bad());\n+  reg_def VSR3_H (SOC, SOC, Op_RegF, 3, VMRegImpl::Bad());\n+  reg_def VSR3_J (SOC, SOC, Op_RegF, 3, VMRegImpl::Bad());\n+  reg_def VSR3_K (SOC, SOC, Op_RegF, 3, VMRegImpl::Bad());\n+\n+  reg_def VSR4   (SOC, SOC, Op_RegF, 4, VMRegImpl::Bad());\n+  reg_def VSR4_H (SOC, SOC, Op_RegF, 4, VMRegImpl::Bad());\n+  reg_def VSR4_J (SOC, SOC, Op_RegF, 4, VMRegImpl::Bad());\n+  reg_def VSR4_K (SOC, SOC, Op_RegF, 4, VMRegImpl::Bad());\n+\n+  reg_def VSR5   (SOC, SOC, Op_RegF, 5, VMRegImpl::Bad());\n+  reg_def VSR5_H (SOC, SOC, Op_RegF, 5, VMRegImpl::Bad());\n+  reg_def VSR5_J (SOC, SOC, Op_RegF, 5, VMRegImpl::Bad());\n+  reg_def VSR5_K (SOC, SOC, Op_RegF, 5, VMRegImpl::Bad());\n+\n+  reg_def VSR6   (SOC, SOC, Op_RegF, 6, VMRegImpl::Bad());\n+  reg_def VSR6_H (SOC, SOC, Op_RegF, 6, VMRegImpl::Bad());\n+  reg_def VSR6_J (SOC, SOC, Op_RegF, 6, VMRegImpl::Bad());\n+  reg_def VSR6_K (SOC, SOC, Op_RegF, 6, VMRegImpl::Bad());\n+\n+  reg_def VSR7   (SOC, SOC, Op_RegF, 7, VMRegImpl::Bad());\n+  reg_def VSR7_H (SOC, SOC, Op_RegF, 7, VMRegImpl::Bad());\n+  reg_def VSR7_J (SOC, SOC, Op_RegF, 7, VMRegImpl::Bad());\n+  reg_def VSR7_K (SOC, SOC, Op_RegF, 7, VMRegImpl::Bad());\n+\n+  reg_def VSR8   (SOC, SOC, Op_RegF, 8, VMRegImpl::Bad());\n+  reg_def VSR8_H (SOC, SOC, Op_RegF, 8, VMRegImpl::Bad());\n+  reg_def VSR8_J (SOC, SOC, Op_RegF, 8, VMRegImpl::Bad());\n+  reg_def VSR8_K (SOC, SOC, Op_RegF, 8, VMRegImpl::Bad());\n+\n+  reg_def VSR9   (SOC, SOC, Op_RegF, 9, VMRegImpl::Bad());\n+  reg_def VSR9_H (SOC, SOC, Op_RegF, 9, VMRegImpl::Bad());\n+  reg_def VSR9_J (SOC, SOC, Op_RegF, 9, VMRegImpl::Bad());\n+  reg_def VSR9_K (SOC, SOC, Op_RegF, 9, VMRegImpl::Bad());\n+\n+  reg_def VSR10  (SOC, SOC, Op_RegF, 10, VMRegImpl::Bad());\n+  reg_def VSR10_H(SOC, SOC, Op_RegF, 10, VMRegImpl::Bad());\n+  reg_def VSR10_J(SOC, SOC, Op_RegF, 10, VMRegImpl::Bad());\n+  reg_def VSR10_K(SOC, SOC, Op_RegF, 10, VMRegImpl::Bad());\n+\n+  reg_def VSR11  (SOC, SOC, Op_RegF, 11, VMRegImpl::Bad());\n+  reg_def VSR11_H(SOC, SOC, Op_RegF, 11, VMRegImpl::Bad());\n+  reg_def VSR11_J(SOC, SOC, Op_RegF, 11, VMRegImpl::Bad());\n+  reg_def VSR11_K(SOC, SOC, Op_RegF, 11, VMRegImpl::Bad());\n+\n+  reg_def VSR12  (SOC, SOC, Op_RegF, 12, VMRegImpl::Bad());\n+  reg_def VSR12_H(SOC, SOC, Op_RegF, 12, VMRegImpl::Bad());\n+  reg_def VSR12_J(SOC, SOC, Op_RegF, 12, VMRegImpl::Bad());\n+  reg_def VSR12_K(SOC, SOC, Op_RegF, 12, VMRegImpl::Bad());\n+\n+  reg_def VSR13  (SOC, SOC, Op_RegF, 13, VMRegImpl::Bad());\n+  reg_def VSR13_H(SOC, SOC, Op_RegF, 13, VMRegImpl::Bad());\n+  reg_def VSR13_J(SOC, SOC, Op_RegF, 13, VMRegImpl::Bad());\n+  reg_def VSR13_K(SOC, SOC, Op_RegF, 13, VMRegImpl::Bad());\n+\n+  reg_def VSR14  (SOC, SOC, Op_RegF, 14, VMRegImpl::Bad());\n+  reg_def VSR14_H(SOC, SOC, Op_RegF, 14, VMRegImpl::Bad());\n+  reg_def VSR14_J(SOC, SOC, Op_RegF, 14, VMRegImpl::Bad());\n+  reg_def VSR14_K(SOC, SOC, Op_RegF, 14, VMRegImpl::Bad());\n+\n+  reg_def VSR15  (SOC, SOC, Op_RegF, 15, VMRegImpl::Bad());\n+  reg_def VSR15_H(SOC, SOC, Op_RegF, 15, VMRegImpl::Bad());\n+  reg_def VSR15_J(SOC, SOC, Op_RegF, 15, VMRegImpl::Bad());\n+  reg_def VSR15_K(SOC, SOC, Op_RegF, 15, VMRegImpl::Bad());\n+\n+  reg_def VSR16  (SOC, SOC, Op_RegF, 16, VMRegImpl::Bad());\n+  reg_def VSR16_H(SOC, SOC, Op_RegF, 16, VMRegImpl::Bad());\n+  reg_def VSR16_J(SOC, SOC, Op_RegF, 16, VMRegImpl::Bad());\n+  reg_def VSR16_K(SOC, SOC, Op_RegF, 16, VMRegImpl::Bad());\n+\n+  reg_def VSR17  (SOC, SOC, Op_RegF, 17, VMRegImpl::Bad());\n+  reg_def VSR17_H(SOC, SOC, Op_RegF, 17, VMRegImpl::Bad());\n+  reg_def VSR17_J(SOC, SOC, Op_RegF, 17, VMRegImpl::Bad());\n+  reg_def VSR17_K(SOC, SOC, Op_RegF, 17, VMRegImpl::Bad());\n+\n+  reg_def VSR18  (SOC, SOC, Op_RegF, 18, VMRegImpl::Bad());\n+  reg_def VSR18_H(SOC, SOC, Op_RegF, 18, VMRegImpl::Bad());\n+  reg_def VSR18_J(SOC, SOC, Op_RegF, 18, VMRegImpl::Bad());\n+  reg_def VSR18_K(SOC, SOC, Op_RegF, 18, VMRegImpl::Bad());\n+\n+  reg_def VSR19  (SOC, SOC, Op_RegF, 19, VMRegImpl::Bad());\n+  reg_def VSR19_H(SOC, SOC, Op_RegF, 19, VMRegImpl::Bad());\n+  reg_def VSR19_J(SOC, SOC, Op_RegF, 19, VMRegImpl::Bad());\n+  reg_def VSR19_K(SOC, SOC, Op_RegF, 19, VMRegImpl::Bad());\n+\n+  reg_def VSR20  (SOC, SOC, Op_RegF, 20, VMRegImpl::Bad());\n+  reg_def VSR20_H(SOC, SOC, Op_RegF, 20, VMRegImpl::Bad());\n+  reg_def VSR20_J(SOC, SOC, Op_RegF, 20, VMRegImpl::Bad());\n+  reg_def VSR20_K(SOC, SOC, Op_RegF, 20, VMRegImpl::Bad());\n+\n+  reg_def VSR21  (SOC, SOC, Op_RegF, 21, VMRegImpl::Bad());\n+  reg_def VSR21_H(SOC, SOC, Op_RegF, 21, VMRegImpl::Bad());\n+  reg_def VSR21_J(SOC, SOC, Op_RegF, 21, VMRegImpl::Bad());\n+  reg_def VSR21_K(SOC, SOC, Op_RegF, 21, VMRegImpl::Bad());\n+\n+  reg_def VSR22  (SOC, SOC, Op_RegF, 22, VMRegImpl::Bad());\n+  reg_def VSR22_H(SOC, SOC, Op_RegF, 22, VMRegImpl::Bad());\n+  reg_def VSR22_J(SOC, SOC, Op_RegF, 22, VMRegImpl::Bad());\n+  reg_def VSR22_K(SOC, SOC, Op_RegF, 22, VMRegImpl::Bad());\n+\n+  reg_def VSR23  (SOC, SOC, Op_RegF, 23, VMRegImpl::Bad());\n+  reg_def VSR23_H(SOC, SOC, Op_RegF, 23, VMRegImpl::Bad());\n+  reg_def VSR23_J(SOC, SOC, Op_RegF, 23, VMRegImpl::Bad());\n+  reg_def VSR23_K(SOC, SOC, Op_RegF, 23, VMRegImpl::Bad());\n+\n+  reg_def VSR24  (SOC, SOC, Op_RegF, 24, VMRegImpl::Bad());\n+  reg_def VSR24_H(SOC, SOC, Op_RegF, 24, VMRegImpl::Bad());\n+  reg_def VSR24_J(SOC, SOC, Op_RegF, 24, VMRegImpl::Bad());\n+  reg_def VSR24_K(SOC, SOC, Op_RegF, 24, VMRegImpl::Bad());\n+\n+  reg_def VSR25  (SOC, SOC, Op_RegF, 25, VMRegImpl::Bad());\n+  reg_def VSR25_H(SOC, SOC, Op_RegF, 25, VMRegImpl::Bad());\n+  reg_def VSR25_J(SOC, SOC, Op_RegF, 25, VMRegImpl::Bad());\n+  reg_def VSR25_K(SOC, SOC, Op_RegF, 25, VMRegImpl::Bad());\n+\n+  reg_def VSR26  (SOC, SOC, Op_RegF, 26, VMRegImpl::Bad());\n+  reg_def VSR26_H(SOC, SOC, Op_RegF, 26, VMRegImpl::Bad());\n+  reg_def VSR26_J(SOC, SOC, Op_RegF, 26, VMRegImpl::Bad());\n+  reg_def VSR26_K(SOC, SOC, Op_RegF, 26, VMRegImpl::Bad());\n+\n+  reg_def VSR27  (SOC, SOC, Op_RegF, 27, VMRegImpl::Bad());\n+  reg_def VSR27_H(SOC, SOC, Op_RegF, 27, VMRegImpl::Bad());\n+  reg_def VSR27_J(SOC, SOC, Op_RegF, 27, VMRegImpl::Bad());\n+  reg_def VSR27_K(SOC, SOC, Op_RegF, 27, VMRegImpl::Bad());\n+\n+  reg_def VSR28  (SOC, SOC, Op_RegF, 28, VMRegImpl::Bad());\n+  reg_def VSR28_H(SOC, SOC, Op_RegF, 28, VMRegImpl::Bad());\n+  reg_def VSR28_J(SOC, SOC, Op_RegF, 28, VMRegImpl::Bad());\n+  reg_def VSR28_K(SOC, SOC, Op_RegF, 28, VMRegImpl::Bad());\n+\n+  reg_def VSR29  (SOC, SOC, Op_RegF, 29, VMRegImpl::Bad());\n+  reg_def VSR29_H(SOC, SOC, Op_RegF, 29, VMRegImpl::Bad());\n+  reg_def VSR29_J(SOC, SOC, Op_RegF, 29, VMRegImpl::Bad());\n+  reg_def VSR29_K(SOC, SOC, Op_RegF, 29, VMRegImpl::Bad());\n+\n+  reg_def VSR30  (SOC, SOC, Op_RegF, 30, VMRegImpl::Bad());\n+  reg_def VSR30_H(SOC, SOC, Op_RegF, 30, VMRegImpl::Bad());\n+  reg_def VSR30_J(SOC, SOC, Op_RegF, 30, VMRegImpl::Bad());\n+  reg_def VSR30_K(SOC, SOC, Op_RegF, 30, VMRegImpl::Bad());\n+\n+  reg_def VSR31  (SOC, SOC, Op_RegF, 31, VMRegImpl::Bad());\n+  reg_def VSR31_H(SOC, SOC, Op_RegF, 31, VMRegImpl::Bad());\n+  reg_def VSR31_J(SOC, SOC, Op_RegF, 31, VMRegImpl::Bad());\n+  reg_def VSR31_K(SOC, SOC, Op_RegF, 31, VMRegImpl::Bad());\n+\n@@ -294,32 +422,159 @@\n-  reg_def VSR32 ( SOC, SOC, Op_VecX, 32, VSR32->as_VMReg());\n-  reg_def VSR33 ( SOC, SOC, Op_VecX, 33, VSR33->as_VMReg());\n-  reg_def VSR34 ( SOC, SOC, Op_VecX, 34, VSR34->as_VMReg());\n-  reg_def VSR35 ( SOC, SOC, Op_VecX, 35, VSR35->as_VMReg());\n-  reg_def VSR36 ( SOC, SOC, Op_VecX, 36, VSR36->as_VMReg());\n-  reg_def VSR37 ( SOC, SOC, Op_VecX, 37, VSR37->as_VMReg());\n-  reg_def VSR38 ( SOC, SOC, Op_VecX, 38, VSR38->as_VMReg());\n-  reg_def VSR39 ( SOC, SOC, Op_VecX, 39, VSR39->as_VMReg());\n-  reg_def VSR40 ( SOC, SOC, Op_VecX, 40, VSR40->as_VMReg());\n-  reg_def VSR41 ( SOC, SOC, Op_VecX, 41, VSR41->as_VMReg());\n-  reg_def VSR42 ( SOC, SOC, Op_VecX, 42, VSR42->as_VMReg());\n-  reg_def VSR43 ( SOC, SOC, Op_VecX, 43, VSR43->as_VMReg());\n-  reg_def VSR44 ( SOC, SOC, Op_VecX, 44, VSR44->as_VMReg());\n-  reg_def VSR45 ( SOC, SOC, Op_VecX, 45, VSR45->as_VMReg());\n-  reg_def VSR46 ( SOC, SOC, Op_VecX, 46, VSR46->as_VMReg());\n-  reg_def VSR47 ( SOC, SOC, Op_VecX, 47, VSR47->as_VMReg());\n-  reg_def VSR48 ( SOC, SOC, Op_VecX, 48, VSR48->as_VMReg());\n-  reg_def VSR49 ( SOC, SOC, Op_VecX, 49, VSR49->as_VMReg());\n-  reg_def VSR50 ( SOC, SOC, Op_VecX, 50, VSR50->as_VMReg());\n-  reg_def VSR51 ( SOC, SOC, Op_VecX, 51, VSR51->as_VMReg());\n-  reg_def VSR52 ( SOC, SOE, Op_VecX, 52, VSR52->as_VMReg());\n-  reg_def VSR53 ( SOC, SOE, Op_VecX, 53, VSR53->as_VMReg());\n-  reg_def VSR54 ( SOC, SOE, Op_VecX, 54, VSR54->as_VMReg());\n-  reg_def VSR55 ( SOC, SOE, Op_VecX, 55, VSR55->as_VMReg());\n-  reg_def VSR56 ( SOC, SOE, Op_VecX, 56, VSR56->as_VMReg());\n-  reg_def VSR57 ( SOC, SOE, Op_VecX, 57, VSR57->as_VMReg());\n-  reg_def VSR58 ( SOC, SOE, Op_VecX, 58, VSR58->as_VMReg());\n-  reg_def VSR59 ( SOC, SOE, Op_VecX, 59, VSR59->as_VMReg());\n-  reg_def VSR60 ( SOC, SOE, Op_VecX, 60, VSR60->as_VMReg());\n-  reg_def VSR61 ( SOC, SOE, Op_VecX, 61, VSR61->as_VMReg());\n-  reg_def VSR62 ( SOC, SOE, Op_VecX, 62, VSR62->as_VMReg());\n-  reg_def VSR63 ( SOC, SOE, Op_VecX, 63, VSR63->as_VMReg());\n+  reg_def VSR32  (SOC, SOC, Op_RegF, 32, VSR32->as_VMReg()         );\n+  reg_def VSR32_H(SOC, SOC, Op_RegF, 32, VSR32->as_VMReg()->next() );\n+  reg_def VSR32_J(SOC, SOC, Op_RegF, 32, VSR32->as_VMReg()->next(2));\n+  reg_def VSR32_K(SOC, SOC, Op_RegF, 32, VSR32->as_VMReg()->next(3));\n+\n+  reg_def VSR33  (SOC, SOC, Op_RegF, 33, VSR33->as_VMReg()         );\n+  reg_def VSR33_H(SOC, SOC, Op_RegF, 33, VSR33->as_VMReg()->next() );\n+  reg_def VSR33_J(SOC, SOC, Op_RegF, 33, VSR33->as_VMReg()->next(2));\n+  reg_def VSR33_K(SOC, SOC, Op_RegF, 33, VSR33->as_VMReg()->next(3));\n+\n+  reg_def VSR34  (SOC, SOC, Op_RegF, 34, VSR34->as_VMReg()         );\n+  reg_def VSR34_H(SOC, SOC, Op_RegF, 34, VSR34->as_VMReg()->next() );\n+  reg_def VSR34_J(SOC, SOC, Op_RegF, 34, VSR34->as_VMReg()->next(2));\n+  reg_def VSR34_K(SOC, SOC, Op_RegF, 34, VSR34->as_VMReg()->next(3));\n+\n+  reg_def VSR35  (SOC, SOC, Op_RegF, 35, VSR35->as_VMReg()         );\n+  reg_def VSR35_H(SOC, SOC, Op_RegF, 35, VSR35->as_VMReg()->next() );\n+  reg_def VSR35_J(SOC, SOC, Op_RegF, 35, VSR35->as_VMReg()->next(2));\n+  reg_def VSR35_K(SOC, SOC, Op_RegF, 35, VSR35->as_VMReg()->next(3));\n+\n+  reg_def VSR36  (SOC, SOC, Op_RegF, 36, VSR36->as_VMReg()         );\n+  reg_def VSR36_H(SOC, SOC, Op_RegF, 36, VSR36->as_VMReg()->next() );\n+  reg_def VSR36_J(SOC, SOC, Op_RegF, 36, VSR36->as_VMReg()->next(2));\n+  reg_def VSR36_K(SOC, SOC, Op_RegF, 36, VSR36->as_VMReg()->next(3));\n+\n+  reg_def VSR37  (SOC, SOC, Op_RegF, 37, VSR37->as_VMReg()         );\n+  reg_def VSR37_H(SOC, SOC, Op_RegF, 37, VSR37->as_VMReg()->next() );\n+  reg_def VSR37_J(SOC, SOC, Op_RegF, 37, VSR37->as_VMReg()->next(2));\n+  reg_def VSR37_K(SOC, SOC, Op_RegF, 37, VSR37->as_VMReg()->next(3));\n+\n+  reg_def VSR38  (SOC, SOC, Op_RegF, 38, VSR38->as_VMReg()         );\n+  reg_def VSR38_H(SOC, SOC, Op_RegF, 38, VSR38->as_VMReg()->next() );\n+  reg_def VSR38_J(SOC, SOC, Op_RegF, 38, VSR38->as_VMReg()->next(2));\n+  reg_def VSR38_K(SOC, SOC, Op_RegF, 38, VSR38->as_VMReg()->next(3));\n+\n+  reg_def VSR39  (SOC, SOC, Op_RegF, 39, VSR39->as_VMReg()         );\n+  reg_def VSR39_H(SOC, SOC, Op_RegF, 39, VSR39->as_VMReg()->next() );\n+  reg_def VSR39_J(SOC, SOC, Op_RegF, 39, VSR39->as_VMReg()->next(2));\n+  reg_def VSR39_K(SOC, SOC, Op_RegF, 39, VSR39->as_VMReg()->next(3));\n+\n+  reg_def VSR40  (SOC, SOC, Op_RegF, 40, VSR40->as_VMReg()         );\n+  reg_def VSR40_H(SOC, SOC, Op_RegF, 40, VSR40->as_VMReg()->next() );\n+  reg_def VSR40_J(SOC, SOC, Op_RegF, 40, VSR40->as_VMReg()->next(2));\n+  reg_def VSR40_K(SOC, SOC, Op_RegF, 40, VSR40->as_VMReg()->next(3));\n+\n+  reg_def VSR41  (SOC, SOC, Op_RegF, 41, VSR41->as_VMReg()         );\n+  reg_def VSR41_H(SOC, SOC, Op_RegF, 41, VSR41->as_VMReg()->next() );\n+  reg_def VSR41_J(SOC, SOC, Op_RegF, 41, VSR41->as_VMReg()->next(2));\n+  reg_def VSR41_K(SOC, SOC, Op_RegF, 41, VSR41->as_VMReg()->next(3));\n+\n+  reg_def VSR42  (SOC, SOC, Op_RegF, 42, VSR42->as_VMReg()         );\n+  reg_def VSR42_H(SOC, SOC, Op_RegF, 42, VSR42->as_VMReg()->next() );\n+  reg_def VSR42_J(SOC, SOC, Op_RegF, 42, VSR42->as_VMReg()->next(2));\n+  reg_def VSR42_K(SOC, SOC, Op_RegF, 42, VSR42->as_VMReg()->next(3));\n+\n+  reg_def VSR43  (SOC, SOC, Op_RegF, 43, VSR43->as_VMReg()         );\n+  reg_def VSR43_H(SOC, SOC, Op_RegF, 43, VSR43->as_VMReg()->next() );\n+  reg_def VSR43_J(SOC, SOC, Op_RegF, 43, VSR43->as_VMReg()->next(2));\n+  reg_def VSR43_K(SOC, SOC, Op_RegF, 43, VSR43->as_VMReg()->next(3));\n+\n+  reg_def VSR44  (SOC, SOC, Op_RegF, 44, VSR44->as_VMReg()         );\n+  reg_def VSR44_H(SOC, SOC, Op_RegF, 44, VSR44->as_VMReg()->next() );\n+  reg_def VSR44_J(SOC, SOC, Op_RegF, 44, VSR44->as_VMReg()->next(2));\n+  reg_def VSR44_K(SOC, SOC, Op_RegF, 44, VSR44->as_VMReg()->next(3));\n+\n+  reg_def VSR45  (SOC, SOC, Op_RegF, 45, VSR45->as_VMReg()         );\n+  reg_def VSR45_H(SOC, SOC, Op_RegF, 45, VSR45->as_VMReg()->next() );\n+  reg_def VSR45_J(SOC, SOC, Op_RegF, 45, VSR45->as_VMReg()->next(2));\n+  reg_def VSR45_K(SOC, SOC, Op_RegF, 45, VSR45->as_VMReg()->next(3));\n+\n+  reg_def VSR46  (SOC, SOC, Op_RegF, 46, VSR46->as_VMReg()         );\n+  reg_def VSR46_H(SOC, SOC, Op_RegF, 46, VSR46->as_VMReg()->next() );\n+  reg_def VSR46_J(SOC, SOC, Op_RegF, 46, VSR46->as_VMReg()->next(2));\n+  reg_def VSR46_K(SOC, SOC, Op_RegF, 46, VSR46->as_VMReg()->next(3));\n+\n+  reg_def VSR47  (SOC, SOC, Op_RegF, 47, VSR47->as_VMReg()         );\n+  reg_def VSR47_H(SOC, SOC, Op_RegF, 47, VSR47->as_VMReg()->next() );\n+  reg_def VSR47_J(SOC, SOC, Op_RegF, 47, VSR47->as_VMReg()->next(2));\n+  reg_def VSR47_K(SOC, SOC, Op_RegF, 47, VSR47->as_VMReg()->next(3));\n+\n+  reg_def VSR48  (SOC, SOC, Op_RegF, 48, VSR48->as_VMReg()         );\n+  reg_def VSR48_H(SOC, SOC, Op_RegF, 48, VSR48->as_VMReg()->next() );\n+  reg_def VSR48_J(SOC, SOC, Op_RegF, 48, VSR48->as_VMReg()->next(2));\n+  reg_def VSR48_K(SOC, SOC, Op_RegF, 48, VSR48->as_VMReg()->next(3));\n+\n+  reg_def VSR49  (SOC, SOC, Op_RegF, 49, VSR49->as_VMReg()         );\n+  reg_def VSR49_H(SOC, SOC, Op_RegF, 49, VSR49->as_VMReg()->next() );\n+  reg_def VSR49_J(SOC, SOC, Op_RegF, 49, VSR49->as_VMReg()->next(2));\n+  reg_def VSR49_K(SOC, SOC, Op_RegF, 49, VSR49->as_VMReg()->next(3));\n+\n+  reg_def VSR50  (SOC, SOC, Op_RegF, 50, VSR50->as_VMReg()         );\n+  reg_def VSR50_H(SOC, SOC, Op_RegF, 50, VSR50->as_VMReg()->next() );\n+  reg_def VSR50_J(SOC, SOC, Op_RegF, 50, VSR50->as_VMReg()->next(2));\n+  reg_def VSR50_K(SOC, SOC, Op_RegF, 50, VSR50->as_VMReg()->next(3));\n+\n+  reg_def VSR51  (SOC, SOC, Op_RegF, 51, VSR51->as_VMReg()         );\n+  reg_def VSR51_H(SOC, SOC, Op_RegF, 51, VSR51->as_VMReg()->next() );\n+  reg_def VSR51_J(SOC, SOC, Op_RegF, 51, VSR51->as_VMReg()->next(2));\n+  reg_def VSR51_K(SOC, SOC, Op_RegF, 51, VSR51->as_VMReg()->next(3));\n+\n+  reg_def VSR52  (SOC, SOE, Op_RegF, 52, VSR52->as_VMReg()         );\n+  reg_def VSR52_H(SOC, SOE, Op_RegF, 52, VSR52->as_VMReg()->next() );\n+  reg_def VSR52_J(SOC, SOE, Op_RegF, 52, VSR52->as_VMReg()->next(2));\n+  reg_def VSR52_K(SOC, SOE, Op_RegF, 52, VSR52->as_VMReg()->next(3));\n+\n+  reg_def VSR53  (SOC, SOE, Op_RegF, 53, VSR53->as_VMReg()         );\n+  reg_def VSR53_H(SOC, SOE, Op_RegF, 53, VSR53->as_VMReg()->next() );\n+  reg_def VSR53_J(SOC, SOE, Op_RegF, 53, VSR53->as_VMReg()->next(2));\n+  reg_def VSR53_K(SOC, SOE, Op_RegF, 53, VSR53->as_VMReg()->next(3));\n+\n+  reg_def VSR54  (SOC, SOE, Op_RegF, 54, VSR54->as_VMReg()         );\n+  reg_def VSR54_H(SOC, SOE, Op_RegF, 54, VSR54->as_VMReg()->next() );\n+  reg_def VSR54_J(SOC, SOE, Op_RegF, 54, VSR54->as_VMReg()->next(2));\n+  reg_def VSR54_K(SOC, SOE, Op_RegF, 54, VSR54->as_VMReg()->next(3));\n+\n+  reg_def VSR55  (SOC, SOE, Op_RegF, 55, VSR55->as_VMReg()         );\n+  reg_def VSR55_H(SOC, SOE, Op_RegF, 55, VSR55->as_VMReg()->next() );\n+  reg_def VSR55_J(SOC, SOE, Op_RegF, 55, VSR55->as_VMReg()->next(2));\n+  reg_def VSR55_K(SOC, SOE, Op_RegF, 55, VSR55->as_VMReg()->next(3));\n+\n+  reg_def VSR56  (SOC, SOE, Op_RegF, 56, VSR56->as_VMReg()         );\n+  reg_def VSR56_H(SOC, SOE, Op_RegF, 56, VSR56->as_VMReg()->next() );\n+  reg_def VSR56_J(SOC, SOE, Op_RegF, 56, VSR56->as_VMReg()->next(2));\n+  reg_def VSR56_K(SOC, SOE, Op_RegF, 56, VSR56->as_VMReg()->next(3));\n+\n+  reg_def VSR57  (SOC, SOE, Op_RegF, 57, VSR57->as_VMReg()         );\n+  reg_def VSR57_H(SOC, SOE, Op_RegF, 57, VSR57->as_VMReg()->next() );\n+  reg_def VSR57_J(SOC, SOE, Op_RegF, 57, VSR57->as_VMReg()->next(2));\n+  reg_def VSR57_K(SOC, SOE, Op_RegF, 57, VSR57->as_VMReg()->next(3));\n+\n+  reg_def VSR58  (SOC, SOE, Op_RegF, 58, VSR58->as_VMReg()         );\n+  reg_def VSR58_H(SOC, SOE, Op_RegF, 58, VSR58->as_VMReg()->next() );\n+  reg_def VSR58_J(SOC, SOE, Op_RegF, 58, VSR58->as_VMReg()->next(2));\n+  reg_def VSR58_K(SOC, SOE, Op_RegF, 58, VSR58->as_VMReg()->next(3));\n+\n+  reg_def VSR59  (SOC, SOE, Op_RegF, 59, VSR59->as_VMReg()         );\n+  reg_def VSR59_H(SOC, SOE, Op_RegF, 59, VSR59->as_VMReg()->next() );\n+  reg_def VSR59_J(SOC, SOE, Op_RegF, 59, VSR59->as_VMReg()->next(2));\n+  reg_def VSR59_K(SOC, SOE, Op_RegF, 59, VSR59->as_VMReg()->next(3));\n+\n+  reg_def VSR60  (SOC, SOE, Op_RegF, 60, VSR60->as_VMReg()         );\n+  reg_def VSR60_H(SOC, SOE, Op_RegF, 60, VSR60->as_VMReg()->next() );\n+  reg_def VSR60_J(SOC, SOE, Op_RegF, 60, VSR60->as_VMReg()->next(2));\n+  reg_def VSR60_K(SOC, SOE, Op_RegF, 60, VSR60->as_VMReg()->next(3));\n+\n+  reg_def VSR61  (SOC, SOE, Op_RegF, 61, VSR61->as_VMReg()         );\n+  reg_def VSR61_H(SOC, SOE, Op_RegF, 61, VSR61->as_VMReg()->next() );\n+  reg_def VSR61_J(SOC, SOE, Op_RegF, 61, VSR61->as_VMReg()->next(2));\n+  reg_def VSR61_K(SOC, SOE, Op_RegF, 61, VSR61->as_VMReg()->next(3));\n+\n+  reg_def VSR62  (SOC, SOE, Op_RegF, 62, VSR62->as_VMReg()         );\n+  reg_def VSR62_H(SOC, SOE, Op_RegF, 62, VSR62->as_VMReg()->next() );\n+  reg_def VSR62_J(SOC, SOE, Op_RegF, 62, VSR62->as_VMReg()->next(2));\n+  reg_def VSR62_K(SOC, SOE, Op_RegF, 62, VSR62->as_VMReg()->next(3));\n+\n+  reg_def VSR63  (SOC, SOE, Op_RegF, 63, VSR63->as_VMReg()         );\n+  reg_def VSR63_H(SOC, SOE, Op_RegF, 63, VSR63->as_VMReg()->next() );\n+  reg_def VSR63_J(SOC, SOE, Op_RegF, 63, VSR63->as_VMReg()->next(2));\n+  reg_def VSR63_K(SOC, SOE, Op_RegF, 63, VSR63->as_VMReg()->next(3));\n@@ -457,64 +712,64 @@\n-  VSR0,\n-  VSR1,\n-  VSR2,\n-  VSR3,\n-  VSR4,\n-  VSR5,\n-  VSR6,\n-  VSR7,\n-  VSR8,\n-  VSR9,\n-  VSR10,\n-  VSR11,\n-  VSR12,\n-  VSR13,\n-  VSR14,\n-  VSR15,\n-  VSR16,\n-  VSR17,\n-  VSR18,\n-  VSR19,\n-  VSR20,\n-  VSR21,\n-  VSR22,\n-  VSR23,\n-  VSR24,\n-  VSR25,\n-  VSR26,\n-  VSR27,\n-  VSR28,\n-  VSR29,\n-  VSR30,\n-  VSR31,\n-  VSR32,\n-  VSR33,\n-  VSR34,\n-  VSR35,\n-  VSR36,\n-  VSR37,\n-  VSR38,\n-  VSR39,\n-  VSR40,\n-  VSR41,\n-  VSR42,\n-  VSR43,\n-  VSR44,\n-  VSR45,\n-  VSR46,\n-  VSR47,\n-  VSR48,\n-  VSR49,\n-  VSR50,\n-  VSR51,\n-  VSR52,\n-  VSR53,\n-  VSR54,\n-  VSR55,\n-  VSR56,\n-  VSR57,\n-  VSR58,\n-  VSR59,\n-  VSR60,\n-  VSR61,\n-  VSR62,\n-  VSR63\n+  VSR0 , VSR0_H , VSR0_J , VSR0_K ,\n+  VSR1 , VSR1_H , VSR1_J , VSR1_K ,\n+  VSR2 , VSR2_H , VSR2_J , VSR2_K ,\n+  VSR3 , VSR3_H , VSR3_J , VSR3_K ,\n+  VSR4 , VSR4_H , VSR4_J , VSR4_K ,\n+  VSR5 , VSR5_H , VSR5_J , VSR5_K ,\n+  VSR6 , VSR6_H , VSR6_J , VSR6_K ,\n+  VSR7 , VSR7_H , VSR7_J , VSR7_K ,\n+  VSR8 , VSR8_H , VSR8_J , VSR8_K ,\n+  VSR9 , VSR9_H , VSR9_J , VSR9_K ,\n+  VSR10, VSR10_H, VSR10_J, VSR10_K,\n+  VSR11, VSR11_H, VSR11_J, VSR11_K,\n+  VSR12, VSR12_H, VSR12_J, VSR12_K,\n+  VSR13, VSR13_H, VSR13_J, VSR13_K,\n+  VSR14, VSR14_H, VSR14_J, VSR14_K,\n+  VSR15, VSR15_H, VSR15_J, VSR15_K,\n+  VSR16, VSR16_H, VSR16_J, VSR16_K,\n+  VSR17, VSR17_H, VSR17_J, VSR17_K,\n+  VSR18, VSR18_H, VSR18_J, VSR18_K,\n+  VSR19, VSR19_H, VSR19_J, VSR19_K,\n+  VSR20, VSR20_H, VSR20_J, VSR20_K,\n+  VSR21, VSR21_H, VSR21_J, VSR21_K,\n+  VSR22, VSR22_H, VSR22_J, VSR22_K,\n+  VSR23, VSR23_H, VSR23_J, VSR23_K,\n+  VSR24, VSR24_H, VSR24_J, VSR24_K,\n+  VSR25, VSR25_H, VSR25_J, VSR25_K,\n+  VSR26, VSR26_H, VSR26_J, VSR26_K,\n+  VSR27, VSR27_H, VSR27_J, VSR27_K,\n+  VSR28, VSR28_H, VSR28_J, VSR28_K,\n+  VSR29, VSR29_H, VSR29_J, VSR29_K,\n+  VSR30, VSR30_H, VSR30_J, VSR30_K,\n+  VSR31, VSR31_H, VSR31_J, VSR31_K,\n+  VSR32, VSR32_H, VSR32_J, VSR32_K,\n+  VSR33, VSR33_H, VSR33_J, VSR33_K,\n+  VSR34, VSR34_H, VSR34_J, VSR34_K,\n+  VSR35, VSR35_H, VSR35_J, VSR35_K,\n+  VSR36, VSR36_H, VSR36_J, VSR36_K,\n+  VSR37, VSR37_H, VSR37_J, VSR37_K,\n+  VSR38, VSR38_H, VSR38_J, VSR38_K,\n+  VSR39, VSR39_H, VSR39_J, VSR39_K,\n+  VSR40, VSR40_H, VSR40_J, VSR40_K,\n+  VSR41, VSR41_H, VSR41_J, VSR41_K,\n+  VSR42, VSR42_H, VSR42_J, VSR42_K,\n+  VSR43, VSR43_H, VSR43_J, VSR43_K,\n+  VSR44, VSR44_H, VSR44_J, VSR44_K,\n+  VSR45, VSR45_H, VSR45_J, VSR45_K,\n+  VSR46, VSR46_H, VSR46_J, VSR46_K,\n+  VSR47, VSR47_H, VSR47_J, VSR47_K,\n+  VSR48, VSR48_H, VSR48_J, VSR48_K,\n+  VSR49, VSR49_H, VSR49_J, VSR49_K,\n+  VSR50, VSR50_H, VSR50_J, VSR50_K,\n+  VSR51, VSR51_H, VSR51_J, VSR51_K,\n+  VSR52, VSR52_H, VSR52_J, VSR52_K,\n+  VSR53, VSR53_H, VSR53_J, VSR53_K,\n+  VSR54, VSR54_H, VSR54_J, VSR54_K,\n+  VSR55, VSR55_H, VSR55_J, VSR55_K,\n+  VSR56, VSR56_H, VSR56_J, VSR56_K,\n+  VSR57, VSR57_H, VSR57_J, VSR57_K,\n+  VSR58, VSR58_H, VSR58_J, VSR58_K,\n+  VSR59, VSR59_H, VSR59_J, VSR59_K,\n+  VSR60, VSR60_H, VSR60_J, VSR60_K,\n+  VSR61, VSR61_H, VSR61_J, VSR61_K,\n+  VSR62, VSR62_H, VSR62_J, VSR62_K,\n+  VSR63, VSR63_H, VSR63_J, VSR63_K\n@@ -913,22 +1168,32 @@\n-  \/\/ Attention: Only these ones are saved & restored at safepoint by RegisterSaver.\n-  VSR32,\n-  VSR33,\n-  VSR34,\n-  VSR35,\n-  VSR36,\n-  VSR37,\n-  VSR38,\n-  VSR39,\n-  VSR40,\n-  VSR41,\n-  VSR42,\n-  VSR43,\n-  VSR44,\n-  VSR45,\n-  VSR46,\n-  VSR47,\n-  VSR48,\n-  VSR49,\n-  VSR50,\n-  VSR51\n-  \/\/ VSR52-VSR63 \/\/ nv!\n+  VSR32, VSR32_H, VSR32_J, VSR32_K,\n+  VSR33, VSR33_H, VSR33_J, VSR33_K,\n+  VSR34, VSR34_H, VSR34_J, VSR34_K,\n+  VSR35, VSR35_H, VSR35_J, VSR35_K,\n+  VSR36, VSR36_H, VSR36_J, VSR36_K,\n+  VSR37, VSR37_H, VSR37_J, VSR37_K,\n+  VSR38, VSR38_H, VSR38_J, VSR38_K,\n+  VSR39, VSR39_H, VSR39_J, VSR39_K,\n+  VSR40, VSR40_H, VSR40_J, VSR40_K,\n+  VSR41, VSR41_H, VSR41_J, VSR41_K,\n+  VSR42, VSR42_H, VSR42_J, VSR42_K,\n+  VSR43, VSR43_H, VSR43_J, VSR43_K,\n+  VSR44, VSR44_H, VSR44_J, VSR44_K,\n+  VSR45, VSR45_H, VSR45_J, VSR45_K,\n+  VSR46, VSR46_H, VSR46_J, VSR46_K,\n+  VSR47, VSR47_H, VSR47_J, VSR47_K,\n+  VSR48, VSR48_H, VSR48_J, VSR48_K,\n+  VSR49, VSR49_H, VSR49_J, VSR49_K,\n+  VSR50, VSR50_H, VSR50_J, VSR50_K,\n+  VSR51, VSR51_H, VSR51_J, VSR51_K,\n+  VSR52, VSR52_H, VSR52_J, VSR52_K, \/\/ non-volatile\n+  VSR53, VSR53_H, VSR53_J, VSR53_K, \/\/ non-volatile\n+  VSR54, VSR54_H, VSR54_J, VSR54_K, \/\/ non-volatile\n+  VSR55, VSR55_H, VSR55_J, VSR55_K, \/\/ non-volatile\n+  VSR56, VSR56_H, VSR56_J, VSR56_K, \/\/ non-volatile\n+  VSR57, VSR57_H, VSR57_J, VSR57_K, \/\/ non-volatile\n+  VSR58, VSR58_H, VSR58_J, VSR58_K, \/\/ non-volatile\n+  VSR59, VSR59_H, VSR59_J, VSR59_K, \/\/ non-volatile\n+  VSR60, VSR60_H, VSR60_J, VSR60_K, \/\/ non-volatile\n+  VSR61, VSR61_H, VSR61_J, VSR61_K, \/\/ non-volatile\n+  VSR62, VSR62_H, VSR62_J, VSR62_K, \/\/ non-volatile\n+  VSR63, VSR63_H, VSR63_J, VSR63_K  \/\/ non-volatile\n@@ -1664,2 +1929,1 @@\n-  \/\/ We have 64 vector-scalar registers, starting at index 128.\n-  if (reg < 64+64+64) return rc_vs;\n+  assert(OptoReg::is_stack(reg) || reg >= 128+8, \"flags are not expected\");\n@@ -1667,2 +1931,2 @@\n-  \/\/ Between float regs & stack are the flags regs.\n-  assert(OptoReg::is_stack(reg) || reg < 64+64+64, \"blow up if spilling flags\");\n+  \/\/ We have 64 vector-scalar registers, starting at index 136.\n+  if (reg < 136+256) return rc_vs;\n@@ -1670,0 +1934,2 @@\n+  \/\/ Special purpose registers are not allocated. We only accept stack from here.\n+  assert(OptoReg::is_stack(reg), \"what else is it?\");\n@@ -1746,3 +2012,12 @@\n-      if (masm) {\n-        __ addi(R0, R1_SP, dst_offset);\n-        __ stxvd2x(Rsrc, R0);\n+      assert(is_aligned(dst_offset, StackAlignmentInBytes), \"should be\");\n+      if (PowerArchitecturePPC64 >= 9) {\n+        if (masm) {\n+          __ stxv(Rsrc, dst_offset, R1_SP);\n+        }\n+        size += 4;\n+      } else {\n+        if (masm) {\n+          __ addi(R0, R1_SP, dst_offset);\n+          __ stxvd2x(Rsrc, R0);\n+        }\n+        size += 8;\n@@ -1750,1 +2025,0 @@\n-      size += 8;\n@@ -1756,3 +2030,12 @@\n-      if (masm) {\n-        __ addi(R0, R1_SP, src_offset);\n-        __ lxvd2x(Rdst, R0);\n+      assert(is_aligned(src_offset, StackAlignmentInBytes), \"should be\");\n+      if (PowerArchitecturePPC64 >= 9) {\n+        if (masm) {\n+          __ lxv(Rdst, src_offset, R1_SP);\n+        }\n+        size += 4;\n+      } else {\n+        if (masm) {\n+          __ addi(R0, R1_SP, src_offset);\n+          __ lxvd2x(Rdst, R0);\n+        }\n+        size += 8;\n@@ -1760,1 +2043,0 @@\n-      size += 8;\n@@ -2268,39 +2550,0 @@\n-\/\/ Constants for c2c and c calling conventions.\n-\n-const MachRegisterNumbers iarg_reg[8] = {\n-  R3_num, R4_num, R5_num, R6_num,\n-  R7_num, R8_num, R9_num, R10_num\n-};\n-\n-const MachRegisterNumbers farg_reg[13] = {\n-  F1_num, F2_num, F3_num, F4_num,\n-  F5_num, F6_num, F7_num, F8_num,\n-  F9_num, F10_num, F11_num, F12_num,\n-  F13_num\n-};\n-\n-const MachRegisterNumbers vsarg_reg[64] = {\n-  VSR0_num, VSR1_num, VSR2_num, VSR3_num,\n-  VSR4_num, VSR5_num, VSR6_num, VSR7_num,\n-  VSR8_num, VSR9_num, VSR10_num, VSR11_num,\n-  VSR12_num, VSR13_num, VSR14_num, VSR15_num,\n-  VSR16_num, VSR17_num, VSR18_num, VSR19_num,\n-  VSR20_num, VSR21_num, VSR22_num, VSR23_num,\n-  VSR24_num, VSR23_num, VSR24_num, VSR25_num,\n-  VSR28_num, VSR29_num, VSR30_num, VSR31_num,\n-  VSR32_num, VSR33_num, VSR34_num, VSR35_num,\n-  VSR36_num, VSR37_num, VSR38_num, VSR39_num,\n-  VSR40_num, VSR41_num, VSR42_num, VSR43_num,\n-  VSR44_num, VSR45_num, VSR46_num, VSR47_num,\n-  VSR48_num, VSR49_num, VSR50_num, VSR51_num,\n-  VSR52_num, VSR53_num, VSR54_num, VSR55_num,\n-  VSR56_num, VSR57_num, VSR58_num, VSR59_num,\n-  VSR60_num, VSR61_num, VSR62_num, VSR63_num\n-};\n-\n-const int num_iarg_registers = sizeof(iarg_reg) \/ sizeof(iarg_reg[0]);\n-\n-const int num_farg_registers = sizeof(farg_reg) \/ sizeof(farg_reg[0]);\n-\n-const int num_vsarg_registers = sizeof(vsarg_reg) \/ sizeof(vsarg_reg[0]);\n-\n@@ -2312,2 +2555,0 @@\n-  \/\/ We return true for all registers contained in iarg_reg[] and\n-  \/\/ farg_reg[] and their virtual halves.\n","filename":"src\/hotspot\/cpu\/ppc\/ppc.ad","additions":444,"deletions":203,"binary":false,"changes":647,"status":"modified"},{"patch":"@@ -217,0 +217,1 @@\n+  constexpr bool is_nonvolatile() const { return (14 <= _encoding && _encoding <= 31 ); }\n@@ -326,0 +327,1 @@\n+  constexpr bool is_nonvolatile() const { return (20 <= _encoding && _encoding <= 31 ); }\n@@ -483,1 +485,1 @@\n-    max_vsr = max_fpr + VectorSRegister::number_of_registers,\n+    max_vsr = max_fpr + VectorSRegister::number_of_registers * 4,\n","filename":"src\/hotspot\/cpu\/ppc\/register_ppc.hpp","additions":3,"deletions":1,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -246,1 +246,11 @@\n-  RegisterSaver_LiveVSReg( VSR51 )\n+  RegisterSaver_LiveVSReg( VSR51 ),\n+  RegisterSaver_LiveVSReg( VSR52 ),\n+  RegisterSaver_LiveVSReg( VSR53 ),\n+  RegisterSaver_LiveVSReg( VSR54 ),\n+  RegisterSaver_LiveVSReg( VSR55 ),\n+  RegisterSaver_LiveVSReg( VSR56 ),\n+  RegisterSaver_LiveVSReg( VSR57 ),\n+  RegisterSaver_LiveVSReg( VSR58 ),\n+  RegisterSaver_LiveVSReg( VSR59 ),\n+  RegisterSaver_LiveVSReg( VSR60 ),\n+  RegisterSaver_LiveVSReg( VSR61 )\n@@ -339,1 +349,1 @@\n-      map->set_callee_saved(VMRegImpl::stack2reg(offset>>2),\n+      map->set_callee_saved(VMRegImpl::stack2reg(offset >> 2),\n@@ -341,1 +351,1 @@\n-      map->set_callee_saved(VMRegImpl::stack2reg((offset + half_reg_size)>>2),\n+      map->set_callee_saved(VMRegImpl::stack2reg((offset + half_reg_size) >> 2),\n@@ -347,10 +357,36 @@\n-  for (int i = 0; i < vsregstosave_num; i++) {\n-    int reg_num  = RegisterSaver_LiveVSRegs[i].reg_num;\n-    int reg_type = RegisterSaver_LiveVSRegs[i].reg_type;\n-\n-    __ li(R30, offset);\n-    __ stxvd2x(as_VectorSRegister(reg_num), R30, R1_SP);\n-\n-    if (generate_oop_map) {\n-      map->set_callee_saved(VMRegImpl::stack2reg(offset>>2),\n-                            RegisterSaver_LiveVSRegs[i].vmreg);\n+  \/\/ Note that generate_oop_map in the following loops is only used for the\n+  \/\/ polling_page_vectors_safepoint_handler_blob.\n+  \/\/ The order in which the vector contents are stored depends on Endianess and\n+  \/\/ the utilized instructions (PowerArchitecturePPC64).\n+  assert(is_aligned(offset, StackAlignmentInBytes), \"should be\");\n+  if (PowerArchitecturePPC64 >= 10) {\n+    for (int i = 0; i < vsregstosave_num; i += 2) {\n+      int reg_num = RegisterSaver_LiveVSRegs[i].reg_num;\n+      assert(RegisterSaver_LiveVSRegs[i + 1].reg_num == reg_num + 1, \"or use other instructions!\");\n+\n+      __ stxvp(as_VectorSRegister(reg_num), offset, R1_SP);\n+\n+      if (generate_oop_map) {\n+        VMReg vsr = RegisterSaver_LiveVSRegs[i].vmreg;\n+        for (int j = 0; j < 8; j++) {\n+          map->set_callee_saved(VMRegImpl::stack2reg((offset >> 2) + j), vsr);\n+          vsr = vsr->next();\n+        }\n+      }\n+      offset += (2 * vs_reg_size);\n+    }\n+  } else {\n+    for (int i = 0; i < vsregstosave_num; i++) {\n+      int reg_num = RegisterSaver_LiveVSRegs[i].reg_num;\n+\n+      __ li(R31, offset);\n+      __ stxvd2x(as_VectorSRegister(reg_num), R31, R1_SP);\n+\n+      if (generate_oop_map) {\n+        VMReg vsr = RegisterSaver_LiveVSRegs[i].vmreg;\n+        for (int j = 0; j < 4; j++) {\n+          map->set_callee_saved(VMRegImpl::stack2reg((offset >> 2) + j), vsr);\n+          vsr = vsr->next();\n+        }\n+      }\n+      offset += vs_reg_size;\n@@ -358,1 +394,0 @@\n-    offset += vs_reg_size;\n@@ -421,3 +456,7 @@\n-  for (int i = 0; i < vsregstosave_num; i++) {\n-    int reg_num  = RegisterSaver_LiveVSRegs[i].reg_num;\n-    int reg_type = RegisterSaver_LiveVSRegs[i].reg_type;\n+  assert(is_aligned(offset, StackAlignmentInBytes), \"should be\");\n+  if (PowerArchitecturePPC64 >= 10) {\n+    for (int i = 0; i < vsregstosave_num; i += 2) {\n+      int reg_num  = RegisterSaver_LiveVSRegs[i].reg_num;\n+      assert(RegisterSaver_LiveVSRegs[i + 1].reg_num == reg_num + 1, \"or use other instructions!\");\n+\n+      __ lxvp(as_VectorSRegister(reg_num), offset, R1_SP);\n@@ -425,2 +464,8 @@\n-    __ li(R31, offset);\n-    __ lxvd2x(as_VectorSRegister(reg_num), R31, R1_SP);\n+      offset += (2 * vs_reg_size);\n+    }\n+  } else {\n+    for (int i = 0; i < vsregstosave_num; i++) {\n+      int reg_num  = RegisterSaver_LiveVSRegs[i].reg_num;\n+\n+      __ li(R31, offset);\n+      __ lxvd2x(as_VectorSRegister(reg_num), R31, R1_SP);\n@@ -428,1 +473,2 @@\n-    offset += vs_reg_size;\n+      offset += vs_reg_size;\n+    }\n","filename":"src\/hotspot\/cpu\/ppc\/sharedRuntime_ppc.cpp","additions":66,"deletions":20,"binary":false,"changes":86,"status":"modified"},{"patch":"@@ -97,0 +97,2 @@\n+    int save_nonvolatile_registers_size = __ save_nonvolatile_registers_size(true, SuperwordUseVSX);\n+\n@@ -98,0 +100,1 @@\n+    STATIC_ASSERT(StackAlignmentInBytes == 16);\n@@ -100,1 +103,1 @@\n-    assert((sizeof(frame::spill_nonvolatiles) % 16) == 0,     \"unaligned\");\n+    assert((save_nonvolatile_registers_size % 16) == 0,       \"unaligned\");\n@@ -109,0 +112,2 @@\n+    Register r_arg_argument_addr            = R8;\n+    Register r_arg_argument_count           = R9;\n@@ -111,3 +116,3 @@\n-    Register r_temp                         = R24;\n-    Register r_top_of_arguments_addr        = R25;\n-    Register r_entryframe_fp                = R26;\n+    Register r_entryframe_fp                = R2; \/\/ volatile\n+    Register r_argument_size                = R11_scratch1; \/\/ volatile\n+    Register r_top_of_arguments_addr        = R21_tmp1;\n@@ -120,9 +125,1 @@\n-\n-      Register r_arg_argument_addr          = R8;\n-      Register r_arg_argument_count         = R9;\n-      Register r_frame_alignment_in_bytes   = R27;\n-      Register r_argument_addr              = R28;\n-      Register r_argumentcopy_addr          = R29;\n-      Register r_argument_size_in_bytes     = R30;\n-      Register r_frame_size                 = R23;\n-\n+      Register r_frame_size  = R12_scratch2; \/\/ volatile\n@@ -134,6 +131,0 @@\n-      \/\/ Zero extend arg_argument_count.\n-      __ clrldi(r_arg_argument_count, r_arg_argument_count, 32);\n-\n-      \/\/ Save non-volatiles GPRs to ENTRY_FRAME (not yet pushed, but it's safe).\n-      __ save_nonvolatile_gprs(R1_SP, _spill_nonvolatiles_neg(r14));\n-\n@@ -143,0 +134,20 @@\n+      \/\/ calculate frame size\n+      STATIC_ASSERT(Interpreter::logStackElementSize == 3);\n+\n+      \/\/ space for arguments aligned up: ((arg_count + 1) * 8) &~ 15\n+      __ addi(r_frame_size, r_arg_argument_count, 1);\n+      __ rldicr(r_frame_size, r_frame_size, 3, 63 - 4);\n+\n+      \/\/ this is the pure space for arguments (excluding alignment padding)\n+      __ sldi(r_argument_size, r_arg_argument_count, 3);\n+\n+      __ addi(r_frame_size, r_frame_size,\n+              save_nonvolatile_registers_size + frame::entry_frame_locals_size + frame::top_ijava_frame_abi_size);\n+\n+      \/\/ push ENTRY_FRAME\n+      __ push_frame(r_frame_size, R0);\n+\n+      \/\/ Save non-volatiles registers to ENTRY_FRAME.\n+      __ save_nonvolatile_registers(r_entryframe_fp, -(frame::entry_frame_locals_size + save_nonvolatile_registers_size),\n+                                    true, SuperwordUseVSX);\n+\n@@ -149,0 +160,1 @@\n+      \/\/              [non-volatiles]\n@@ -153,24 +165,0 @@\n-      \/\/ calculate frame size\n-\n-      \/\/ unaligned size of arguments\n-      __ sldi(r_argument_size_in_bytes,\n-                  r_arg_argument_count, Interpreter::logStackElementSize);\n-      \/\/ arguments alignment (max 1 slot)\n-      \/\/ FIXME: use round_to() here\n-      __ andi_(r_frame_alignment_in_bytes, r_arg_argument_count, 1);\n-      __ sldi(r_frame_alignment_in_bytes,\n-              r_frame_alignment_in_bytes, Interpreter::logStackElementSize);\n-\n-      \/\/ size = unaligned size of arguments + top abi's size\n-      __ addi(r_frame_size, r_argument_size_in_bytes,\n-              frame::top_ijava_frame_abi_size);\n-      \/\/ size += arguments alignment\n-      __ add(r_frame_size,\n-             r_frame_size, r_frame_alignment_in_bytes);\n-      \/\/ size += size of call_stub locals\n-      __ addi(r_frame_size,\n-              r_frame_size, frame::entry_frame_locals_size);\n-\n-      \/\/ push ENTRY_FRAME\n-      __ push_frame(r_frame_size, r_temp);\n-\n@@ -178,6 +166,3 @@\n-      __ std(r_arg_call_wrapper_addr,\n-             _entry_frame_locals_neg(call_wrapper_address), r_entryframe_fp);\n-      __ std(r_arg_result_addr,\n-             _entry_frame_locals_neg(result_address), r_entryframe_fp);\n-      __ std(r_arg_result_type,\n-             _entry_frame_locals_neg(result_type), r_entryframe_fp);\n+      __ std(r_arg_call_wrapper_addr, _entry_frame_locals_neg(call_wrapper_address), r_entryframe_fp);\n+      __ std(r_arg_result_addr, _entry_frame_locals_neg(result_address), r_entryframe_fp);\n+      __ std(r_arg_result_type, _entry_frame_locals_neg(result_type), r_entryframe_fp);\n@@ -186,1 +171,0 @@\n-\n@@ -191,5 +175,3 @@\n-      \/\/ FIXME: why not simply use SP+frame::top_ijava_frame_size?\n-      __ addi(r_top_of_arguments_addr,\n-              R1_SP, frame::top_ijava_frame_abi_size);\n-      __ add(r_top_of_arguments_addr,\n-             r_top_of_arguments_addr, r_frame_alignment_in_bytes);\n+      __ addi(r_top_of_arguments_addr, r_entryframe_fp,\n+              -(save_nonvolatile_registers_size + frame::entry_frame_locals_size));\n+      __ sub(r_top_of_arguments_addr, r_top_of_arguments_addr, r_argument_size);\n@@ -203,0 +185,2 @@\n+        Register r_argument_addr     = R22_tmp2;\n+        Register r_argumentcopy_addr = R23_tmp3;\n@@ -210,2 +194,1 @@\n-        __ add(r_argument_addr,\n-                   r_arg_argument_addr, r_argument_size_in_bytes);\n+        __ add(r_argument_addr, r_arg_argument_addr, r_argument_size);\n@@ -219,1 +202,1 @@\n-          __ ld(r_temp, 0, r_argument_addr);\n+          __ ld(R0, 0, r_argument_addr);\n@@ -222,1 +205,1 @@\n-          __ std(r_temp, 0, r_argumentcopy_addr);\n+          __ std(R0, 0, r_argumentcopy_addr);\n@@ -237,5 +220,1 @@\n-      Register r_new_arg_entry = R14;\n-      assert_different_registers(r_new_arg_entry, r_top_of_arguments_addr,\n-                                 r_arg_method, r_arg_thread);\n-\n-      __ mr(r_new_arg_entry, r_arg_entry);\n+      assert_different_registers(r_arg_entry, r_top_of_arguments_addr, r_arg_method, r_arg_thread);\n@@ -265,1 +244,1 @@\n-      __ load_const_optimized(R25_templateTableBase, (address)Interpreter::dispatch_table((TosState)0), R11_scratch1);\n+      __ load_const_optimized(R25_templateTableBase, (address)Interpreter::dispatch_table((TosState)0), R0);\n@@ -271,0 +250,1 @@\n+      \/\/              [non-volatiles]\n@@ -277,1 +257,1 @@\n-      __ load_const_optimized(R29_TOC, MacroAssembler::global_toc(), R11_scratch1);\n+      __ load_const_optimized(R29_TOC, MacroAssembler::global_toc(), R0);\n@@ -284,1 +264,1 @@\n-      \/\/ Do a light-weight C-call here, r_new_arg_entry holds the address\n+      \/\/ Do a light-weight C-call here, r_arg_entry holds the address\n@@ -287,3 +267,3 @@\n-      assert(r_new_arg_entry != tos && r_new_arg_entry != R19_method && r_new_arg_entry != R16_thread,\n-             \"trashed r_new_arg_entry\");\n-      return_address = __ call_stub(r_new_arg_entry);\n+      assert(r_arg_entry != tos && r_arg_entry != R19_method && r_arg_entry != R16_thread,\n+             \"trashed r_arg_entry\");\n+      return_address = __ call_stub(r_arg_entry);\n@@ -301,0 +281,1 @@\n+      \/\/              [non-volatiles]\n@@ -313,3 +294,2 @@\n-      Register r_entryframe_fp = R30;\n-      Register r_lr            = R7_ARG5;\n-      Register r_cr            = R8_ARG6;\n+      Register r_lr = R11_scratch1;\n+      Register r_cr = R12_scratch2;\n@@ -321,1 +301,1 @@\n-      __ ld(r_entryframe_fp, _abi0(callers_sp), R1_SP);\n+      __ ld(r_entryframe_fp, _abi0(callers_sp), R1_SP); \/\/ restore after call\n@@ -323,4 +303,2 @@\n-      __ ld(r_arg_result_addr,\n-            _entry_frame_locals_neg(result_address), r_entryframe_fp);\n-      __ ld(r_arg_result_type,\n-            _entry_frame_locals_neg(result_type), r_entryframe_fp);\n+      __ ld(r_arg_result_addr, _entry_frame_locals_neg(result_address), r_entryframe_fp);\n+      __ ld(r_arg_result_type, _entry_frame_locals_neg(result_type), r_entryframe_fp);\n@@ -329,6 +307,2 @@\n-\n-      \/\/ pop frame and restore non-volatiles, LR and CR\n-      __ mr(R1_SP, r_entryframe_fp);\n-      __ pop_cont_fastpath();\n-      __ mtcr(r_cr);\n-      __ mtlr(r_lr);\n+      __ mtcr(r_cr); \/\/ restore CR\n+      __ mtlr(r_lr); \/\/ restore LR\n@@ -338,4 +312,7 @@\n-      __ cmpwi(CR0, r_arg_result_type, T_OBJECT);\n-      __ cmpwi(CR1, r_arg_result_type, T_LONG);\n-      __ cmpwi(CR5, r_arg_result_type, T_FLOAT);\n-      __ cmpwi(CR6, r_arg_result_type, T_DOUBLE);\n+      \/\/ Using volatile CRs.\n+      __ cmpwi(CR1, r_arg_result_type, T_OBJECT);\n+      __ cmpwi(CR5, r_arg_result_type, T_LONG);\n+      __ cmpwi(CR6, r_arg_result_type, T_FLOAT);\n+      __ cmpwi(CR7, r_arg_result_type, T_DOUBLE);\n+\n+      __ pop_cont_fastpath(); \/\/ kills CR0, uses R16_thread\n@@ -344,1 +321,2 @@\n-      __ restore_nonvolatile_gprs(R1_SP, _spill_nonvolatiles_neg(r14));\n+      __ restore_nonvolatile_registers(r_entryframe_fp, -(frame::entry_frame_locals_size + save_nonvolatile_registers_size),\n+                                       true, SuperwordUseVSX);\n@@ -346,0 +324,2 @@\n+      \/\/ pop frame\n+      __ mr(R1_SP, r_entryframe_fp);\n@@ -354,7 +334,4 @@\n-      \/\/ All non-volatiles have been restored at this point!!\n-      assert(R3_RET == R3, \"R3_RET should be R3\");\n-\n-      __ beq(CR0, ret_is_object);\n-      __ beq(CR1, ret_is_long);\n-      __ beq(CR5, ret_is_float);\n-      __ beq(CR6, ret_is_double);\n+      __ beq(CR1, ret_is_object);\n+      __ beq(CR5, ret_is_long);\n+      __ beq(CR6, ret_is_float);\n+      __ beq(CR7, ret_is_double);\n@@ -367,4 +344,0 @@\n-      __ bind(ret_is_object);\n-      __ std(R3_RET, 0, r_arg_result_addr);\n-      __ blr(); \/\/ return to caller\n-\n@@ -372,0 +345,1 @@\n+      __ bind(ret_is_object);\n","filename":"src\/hotspot\/cpu\/ppc\/stubGenerator_ppc.cpp","additions":74,"deletions":100,"binary":false,"changes":174,"status":"modified"},{"patch":"@@ -122,0 +122,1 @@\n+  int save_nonvolatile_registers_size = __ save_nonvolatile_registers_size(false, false);\n@@ -124,1 +125,1 @@\n-  __ save_nonvolatile_gprs(R1_SP, _spill_nonvolatiles_neg(r14));\n+  __ save_nonvolatile_registers(R1_SP, -save_nonvolatile_registers_size, false, false);\n@@ -127,1 +128,1 @@\n-  __ push_frame_reg_args_nonvolatiles(0, R11_scratch1);\n+  __ push_frame(frame::native_abi_reg_args_size + save_nonvolatile_registers_size, R11_scratch1);\n@@ -312,1 +313,1 @@\n-  __ restore_nonvolatile_gprs(R1_SP, _spill_nonvolatiles_neg(r14));\n+  __ restore_nonvolatile_registers(R1_SP, -save_nonvolatile_registers_size, false, false);\n","filename":"src\/hotspot\/cpu\/ppc\/templateInterpreterGenerator_ppc.cpp","additions":4,"deletions":3,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -3,1 +3,1 @@\n- * Copyright (c) 2023 SAP SE. All rights reserved.\n+ * Copyright (c) 2023, 2025 SAP SE. All rights reserved.\n@@ -38,80 +38,0 @@\n-\/\/ for callee saved regs, according to the caller's ABI\n-static int compute_reg_save_area_size(const ABIDescriptor& abi) {\n-  int size = 0;\n-  for (int i = 0; i < Register::number_of_registers; i++) {\n-    Register reg = as_Register(i);\n-    \/\/ R1 saved\/restored by prologue\/epilogue, R13 (system thread) won't get modified!\n-    if (reg == R1_SP || reg == R13) continue;\n-    if (!abi.is_volatile_reg(reg)) {\n-      size += 8; \/\/ bytes\n-    }\n-  }\n-\n-  for (int i = 0; i < FloatRegister::number_of_registers; i++) {\n-    FloatRegister reg = as_FloatRegister(i);\n-    if (!abi.is_volatile_reg(reg)) {\n-      size += 8; \/\/ bytes\n-    }\n-  }\n-\n-  return size;\n-}\n-\n-static void preserve_callee_saved_registers(MacroAssembler* _masm, const ABIDescriptor& abi, int reg_save_area_offset) {\n-  \/\/ 1. iterate all registers in the architecture\n-  \/\/     - check if they are volatile or not for the given abi\n-  \/\/     - if NOT, we need to save it here\n-\n-  int offset = reg_save_area_offset;\n-\n-  __ block_comment(\"{ preserve_callee_saved_regs \");\n-  for (int i = 0; i < Register::number_of_registers; i++) {\n-    Register reg = as_Register(i);\n-    \/\/ R1 saved\/restored by prologue\/epilogue, R13 (system thread) won't get modified!\n-    if (reg == R1_SP || reg == R13) continue;\n-    if (!abi.is_volatile_reg(reg)) {\n-      __ std(reg, offset, R1_SP);\n-      offset += 8;\n-    }\n-  }\n-\n-  for (int i = 0; i < FloatRegister::number_of_registers; i++) {\n-    FloatRegister reg = as_FloatRegister(i);\n-    if (!abi.is_volatile_reg(reg)) {\n-      __ stfd(reg, offset, R1_SP);\n-      offset += 8;\n-    }\n-  }\n-\n-  __ block_comment(\"} preserve_callee_saved_regs \");\n-}\n-\n-static void restore_callee_saved_registers(MacroAssembler* _masm, const ABIDescriptor& abi, int reg_save_area_offset) {\n-  \/\/ 1. iterate all registers in the architecture\n-  \/\/     - check if they are volatile or not for the given abi\n-  \/\/     - if NOT, we need to restore it here\n-\n-  int offset = reg_save_area_offset;\n-\n-  __ block_comment(\"{ restore_callee_saved_regs \");\n-  for (int i = 0; i < Register::number_of_registers; i++) {\n-    Register reg = as_Register(i);\n-    \/\/ R1 saved\/restored by prologue\/epilogue, R13 (system thread) won't get modified!\n-    if (reg == R1_SP || reg == R13) continue;\n-    if (!abi.is_volatile_reg(reg)) {\n-      __ ld(reg, offset, R1_SP);\n-      offset += 8;\n-    }\n-  }\n-\n-  for (int i = 0; i < FloatRegister::number_of_registers; i++) {\n-    FloatRegister reg = as_FloatRegister(i);\n-    if (!abi.is_volatile_reg(reg)) {\n-      __ lfd(reg, offset, R1_SP);\n-      offset += 8;\n-    }\n-  }\n-\n-  __ block_comment(\"} restore_callee_saved_regs \");\n-}\n-\n@@ -143,1 +63,2 @@\n-  int reg_save_area_size = compute_reg_save_area_size(abi);\n+  MacroAssembler* _masm = new MacroAssembler(&buffer);\n+  int reg_save_area_size = __ save_nonvolatile_registers_size(true, SuperwordUseVSX);\n@@ -150,0 +71,3 @@\n+  if (SuperwordUseVSX) { \/\/ VectorRegisters want alignment\n+    reg_save_area_offset = align_up(reg_save_area_offset, StackAlignmentInBytes);\n+  }\n@@ -204,1 +128,0 @@\n-  MacroAssembler* _masm = new MacroAssembler(&buffer);\n@@ -215,1 +138,1 @@\n-  preserve_callee_saved_registers(_masm, abi, reg_save_area_offset);\n+  __ save_nonvolatile_registers(R1_SP, reg_save_area_offset, true, SuperwordUseVSX);\n@@ -313,1 +236,1 @@\n-  restore_callee_saved_registers(_masm, abi, reg_save_area_offset);\n+  __ restore_nonvolatile_registers(R1_SP, reg_save_area_offset, true, SuperwordUseVSX);\n","filename":"src\/hotspot\/cpu\/ppc\/upcallLinker_ppc.cpp","additions":8,"deletions":85,"binary":false,"changes":93,"status":"modified"},{"patch":"@@ -2,2 +2,2 @@\n- * Copyright (c) 2001, 2022, Oracle and\/or its affiliates. All rights reserved.\n- * Copyright (c) 2012, 2022 SAP SE. All rights reserved.\n+ * Copyright (c) 2001, 2025, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2012, 2025 SAP SE. All rights reserved.\n@@ -65,1 +65,1 @@\n-  return ::as_VectorSRegister(value() - ConcreteRegisterImpl::max_fpr);\n+  return ::as_VectorSRegister((value() - ConcreteRegisterImpl::max_fpr) >> 2);\n@@ -70,1 +70,6 @@\n-  return is_even(value());\n+  if (is_Register() || is_FloatRegister()) return is_even(value());\n+  if (is_VectorSRegister()) {\n+    int base = value() - ConcreteRegisterImpl::max_fpr;\n+    return (base & 3) == 0;\n+  }\n+  return true;\n","filename":"src\/hotspot\/cpu\/ppc\/vmreg_ppc.hpp","additions":9,"deletions":4,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -2,2 +2,2 @@\n- * Copyright (c) 2002, 2022, Oracle and\/or its affiliates. All rights reserved.\n- * Copyright (c) 2012, 2022 SAP SE. All rights reserved.\n+ * Copyright (c) 2002, 2025, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2012, 2025 SAP SE. All rights reserved.\n@@ -44,1 +44,2 @@\n-  return VMRegImpl::as_VMReg((encoding()) + ConcreteRegisterImpl::max_fpr);\n+  \/\/ Four halves, multiply by 4.\n+  return VMRegImpl::as_VMReg((encoding() << 2) + ConcreteRegisterImpl::max_fpr);\n","filename":"src\/hotspot\/cpu\/ppc\/vmreg_ppc.inline.hpp","additions":4,"deletions":3,"binary":false,"changes":7,"status":"modified"}]}