{"files":[{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -359,0 +359,2 @@\n+  template(jdk_internal_foreign_NativeMemorySegmentImpl,             \"jdk\/internal\/foreign\/NativeMemorySegmentImpl\") \\\n+                                                                                                  \\\n","filename":"src\/hotspot\/share\/classfile\/vmSymbols.hpp","additions":3,"deletions":1,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -2862,2 +2862,2 @@\n-  bool is_trace_pointer() const {\n-    return is_trace(TraceMergeStores::Tag::POINTER);\n+  bool is_trace_pointer_parsing() const {\n+    return is_trace(TraceMergeStores::Tag::POINTER_PARSING);\n@@ -2866,2 +2866,2 @@\n-  bool is_trace_aliasing() const {\n-    return is_trace(TraceMergeStores::Tag::ALIASING);\n+  bool is_trace_pointer_aliasing() const {\n+    return is_trace(TraceMergeStores::Tag::POINTER_ALIASING);\n@@ -2870,2 +2870,2 @@\n-  bool is_trace_adjacency() const {\n-    return is_trace(TraceMergeStores::Tag::ADJACENCY);\n+  bool is_trace_pointer_adjacency() const {\n+    return is_trace(TraceMergeStores::Tag::POINTER_ADJACENCY);\n@@ -2942,3 +2942,4 @@\n-  const TraceMemPointer trace(is_trace_pointer(),\n-                              is_trace_aliasing(),\n-                              is_trace_adjacency());\n+  const TraceMemPointer trace(is_trace_pointer_parsing(),\n+                              is_trace_pointer_aliasing(),\n+                              is_trace_pointer_adjacency(),\n+                              true);\n@@ -2946,2 +2947,2 @@\n-  const MemPointer pointer_use(use_store NOT_PRODUCT( COMMA trace ));\n-  const MemPointer pointer_def(def_store NOT_PRODUCT( COMMA trace ));\n+  const MemPointer pointer_use(use_store NOT_PRODUCT(COMMA trace));\n+  const MemPointer pointer_def(def_store NOT_PRODUCT(COMMA trace));\n","filename":"src\/hotspot\/share\/opto\/memnode.cpp","additions":12,"deletions":11,"binary":false,"changes":23,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2024, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -27,0 +27,1 @@\n+#include \"opto\/addnode.hpp\"\n@@ -28,0 +29,10 @@\n+#include \"classfile\/vmSymbols.hpp\"\n+\n+MemPointerParserCallback MemPointerParserCallback::_empty;\n+\n+MemPointer::MemPointer(const MemNode* mem,\n+                       MemPointerParserCallback& callback\n+                       NOT_PRODUCT(COMMA const TraceMemPointer& trace)) :\n+  MemPointer(MemPointerParser::parse(mem,\n+                                     callback\n+                                     NOT_PRODUCT(COMMA trace))) {}\n@@ -31,1 +42,2 @@\n-MemPointerDecomposedForm MemPointerDecomposedFormParser::parse_decomposed_form() {\n+MemPointer MemPointerParser::parse(MemPointerParserCallback& callback\n+                                   NOT_PRODUCT(COMMA const TraceMemPointer& trace)) {\n@@ -36,0 +48,1 @@\n+  const jint size = _mem->memory_size();\n@@ -45,2 +58,4 @@\n-    if (traversal_count++ > 1000) { return MemPointerDecomposedForm::make_trivial(pointer); }\n-    parse_sub_expression(_worklist.pop());\n+    if (traversal_count++ > 1000) {\n+      return MemPointer::make_trivial(pointer, size NOT_PRODUCT(COMMA trace));\n+    }\n+    parse_sub_expression(_worklist.pop(), callback);\n@@ -50,1 +65,3 @@\n-  if (_con.is_NaN()) { return MemPointerDecomposedForm::make_trivial(pointer); }\n+  if (_con.is_NaN()) {\n+    return MemPointer::make_trivial(pointer, size NOT_PRODUCT(COMMA trace));\n+  }\n@@ -70,1 +87,1 @@\n-      return MemPointerDecomposedForm::make_trivial(pointer);\n+      return MemPointer::make_trivial(pointer, size NOT_PRODUCT(COMMA trace));\n@@ -79,1 +96,1 @@\n-  return MemPointerDecomposedForm::make(pointer, _summands, _con);\n+  return MemPointer::make(pointer, _summands, _con, size NOT_PRODUCT(COMMA trace));\n@@ -85,1 +102,1 @@\n-void MemPointerDecomposedFormParser::parse_sub_expression(const MemPointerSummand& summand) {\n+void MemPointerParser::parse_sub_expression(const MemPointerSummand& summand, MemPointerParserCallback& callback) {\n@@ -111,0 +128,1 @@\n+        callback.callback(n);\n@@ -124,0 +142,1 @@\n+        callback.callback(n);\n@@ -158,0 +177,1 @@\n+        callback.callback(n);\n@@ -160,0 +180,10 @@\n+      case Op_CastX2P:\n+        \/\/ A CastX2P indicates that we are pointing to native memory, where some long is cast to\n+        \/\/ a pointer. In general, we have no guarantees about this long, and just take it as a\n+        \/\/ terminal summand. A CastX2P can also be a good candidate for a native-memory \"base\".\n+        if (!sub_expression_has_native_base_candidate(n->in(1))) {\n+          \/\/ General case: take CastX2P as a terminal summand, it is a candidate for the \"base\".\n+          break;\n+        }\n+        \/\/ Fall-through: we can find a more precise native-memory \"base\". We further decompose\n+        \/\/ the CastX2P to find this \"base\" and any other offsets from it.\n@@ -162,1 +192,0 @@\n-      case Op_CastX2P:\n@@ -164,14 +193,15 @@\n-      \/\/ On 32bit systems we can also look through ConvL2I, since the final result will always\n-      \/\/ be truncated back with ConvL2I. On 64bit systems we cannot decompose ConvL2I because\n-      \/\/ such int values will eventually be expanded to long with a ConvI2L:\n-      \/\/\n-      \/\/   valL = max_jint + 1\n-      \/\/   ConvI2L(ConvL2I(valL)) = ConvI2L(min_jint) = min_jint != max_jint + 1 = valL\n-      \/\/\n-      NOT_LP64( case Op_ConvL2I: )\n-      {\n-        \/\/ Decompose: look through.\n-        Node* a = n->in(1);\n-        _worklist.push(MemPointerSummand(a, scale));\n-        return;\n-      }\n+        \/\/ On 32bit systems we can also look through ConvL2I, since the final result will always\n+        \/\/ be truncated back with ConvL2I. On 64bit systems we cannot decompose ConvL2I because\n+        \/\/ such int values will eventually be expanded to long with a ConvI2L:\n+        \/\/\n+        \/\/   valL = max_jint + 1\n+        \/\/   ConvI2L(ConvL2I(valL)) = ConvI2L(min_jint) = min_jint != max_jint + 1 = valL\n+        \/\/\n+        NOT_LP64( case Op_ConvL2I: )\n+        {\n+          \/\/ Decompose: look through.\n+          Node* a = n->in(1);\n+          _worklist.push(MemPointerSummand(a, scale));\n+          callback.callback(n);\n+          return;\n+        }\n@@ -189,0 +219,61 @@\n+bool MemPointerParser::sub_expression_has_native_base_candidate(Node* start) {\n+  \/\/ BFS over the expression.\n+  \/\/ Allocate sufficient space in worklist for 100 limit below.\n+  ResourceMark rm;\n+  GrowableArray<Node*> worklist(102);\n+  worklist.append(start);\n+  for (int i = 0; i < worklist.length(); i++) {\n+    Node* n = worklist.at(i);\n+    switch(n->Opcode()) {\n+      case Op_AddL:\n+        \/\/ Traverse to both inputs.\n+        worklist.append(n->in(1));\n+        worklist.append(n->in(2));\n+        break;\n+      case Op_SubL:\n+      case Op_CastLL:\n+        \/\/ Traverse to the first input. The base cannot be on the rhs of a sub.\n+        worklist.append(n->in(1));\n+        break;\n+      default:\n+        if (is_native_memory_base_candidate(n)) { return true; }\n+        break;\n+    }\n+    \/\/ This is a heuristic, so we are allowed to bail out early if the graph\n+    \/\/ is too deep. The constant is chosen arbitrarily, not too large but big\n+    \/\/ enough for all normal cases.\n+    if (worklist.length() > 100) { return false; }\n+  }\n+  \/\/ Parsed over the whole expression, nothing found.\n+  return false;\n+}\n+\n+\/\/ Check if the node is a candidate to be a memory segment \"base\".\n+\/\/ (1) CastX2P: some arbitrary long that is cast to a pointer.\n+\/\/ (2) LoadL from field jdk.internal.foreign.NativeMemorySegmentImpl.min\n+\/\/     Holds the address() of a native memory segment.\n+bool MemPointerParser::is_native_memory_base_candidate(Node* n) {\n+  \/\/ (1) CastX2P\n+  if (n->Opcode() == Op_CastX2P) { return true; }\n+\n+  \/\/ (2) LoadL from field jdk.internal.foreign.NativeMemorySegmentImpl.min\n+  if (n->Opcode() != Op_LoadL) { return false; }\n+  LoadNode* load = n->as_Load();\n+\n+  const TypeInstPtr* inst_ptr = load->adr_type()->isa_instptr();\n+  if (inst_ptr == nullptr) { return false; }\n+\n+  ciInstanceKlass* klass = inst_ptr->instance_klass();\n+  int offset = inst_ptr->offset();\n+  ciField* field = klass->get_field_by_offset(offset, false);\n+  if (field == nullptr) { return false; }\n+\n+  Symbol* field_symbol = field->name()->get_symbol();\n+  Symbol* holder_symbol = field->holder()->name()->get_symbol();\n+  if (holder_symbol != vmSymbols::jdk_internal_foreign_NativeMemorySegmentImpl() ||\n+      field_symbol != vmSymbols::min_name()) {\n+    return false;\n+  }\n+  return true;\n+}\n+\n@@ -191,1 +282,1 @@\n-bool MemPointerDecomposedFormParser::is_safe_to_decompose_op(const int opc, const NoOverflowInt& scale) const {\n+bool MemPointerParser::is_safe_to_decompose_op(const int opc, const NoOverflowInt& scale) const {\n@@ -301,4 +392,43 @@\n-\/\/ Compute the aliasing between two MemPointerDecomposedForm. We use the \"MemPointer Lemma\" to\n-\/\/ prove that the computed aliasing also applies for the underlying pointers. Note that the\n-\/\/ condition (S0) is already given, because the MemPointerDecomposedForm is always constructed\n-\/\/ using only safe decompositions.\n+MemPointer::Base MemPointer::Base::make(Node* pointer, const GrowableArray<MemPointerSummand>& summands) {\n+  \/\/ Bad form -> unknown.\n+  AddPNode* adr = pointer->isa_AddP();\n+  if (adr == nullptr) { return Base(); }\n+\n+  \/\/ Non-TOP base -> object.\n+  Node* maybe_object_base = adr->in(AddPNode::Base);\n+  bool is_object_base = !maybe_object_base->is_top();\n+\n+  Node* base = find_base(is_object_base ? maybe_object_base : nullptr, summands);\n+\n+  if (base == nullptr) {\n+    \/\/ Not found -> unknown.\n+    return Base();\n+  } else if (is_object_base) {\n+    assert(base == maybe_object_base, \"we confirmed that it is in summands\");\n+    return Base(Object, base);\n+  } else {\n+    return Base(Native, base);\n+  }\n+}\n+\n+Node* MemPointer::Base::find_base(Node* object_base, const GrowableArray<MemPointerSummand>& summands) {\n+  for (int i = 0; i < summands.length(); i++) {\n+    const MemPointerSummand& s = summands.at(i);\n+    assert(s.variable() != nullptr, \"no empty summands\");\n+    \/\/ Object base.\n+    if (object_base != nullptr && s.variable() == object_base && s.scale().is_one()) {\n+      return object_base;\n+    }\n+    \/\/ Native base.\n+    if (object_base == nullptr &&\n+        s.scale().is_one() &&\n+        MemPointerParser::is_native_memory_base_candidate(s.variable())) {\n+      return s.variable();\n+    }\n+  }\n+  return nullptr;\n+}\n+\n+\/\/ Compute the aliasing between two MemPointer. We use the \"MemPointer Lemma\" to prove that the\n+\/\/ computed aliasing also applies for the underlying pointers. Note that the condition (S0) is\n+\/\/ already given, because the MemPointer is always constructed using only safe decompositions.\n@@ -310,2 +440,2 @@\n-MemPointerAliasing MemPointerDecomposedForm::get_aliasing_with(const MemPointerDecomposedForm& other\n-                                                               NOT_PRODUCT( COMMA const TraceMemPointer& trace) ) const {\n+MemPointerAliasing MemPointer::get_aliasing_with(const MemPointer& other\n+                                                 NOT_PRODUCT( COMMA const TraceMemPointer& trace) ) const {\n@@ -314,1 +444,1 @@\n-    tty->print_cr(\"MemPointerDecomposedForm::get_aliasing_with:\");\n+    tty->print_cr(\"MemPointer::get_aliasing_with:\");\n@@ -321,4 +451,10 @@\n-  for (uint i = 0; i < SUMMANDS_SIZE; i++) {\n-    const MemPointerSummand s1 = summands_at(i);\n-    const MemPointerSummand s2 = other.summands_at(i);\n-    if (s1 != s2) {\n+  bool has_same_base = false;\n+  if (has_different_object_base_but_otherwise_same_summands_as(other)) {\n+    \/\/ At runtime, the two object bases can be:\n+    \/\/   (1) different: we have no aliasing, pointers point to different memory objects.\n+    \/\/   (2) the same:  implies that all summands are the same, (S2) holds.\n+    has_same_base = false;\n+  } else if (has_same_summands_as(other)) {\n+    \/\/ (S2) holds. If all summands are the same, also the base must be the same.\n+    has_same_base = true;\n+  } else {\n@@ -326,5 +462,2 @@\n-      if (trace.is_trace_aliasing()) {\n-        tty->print_cr(\"  -> Aliasing unknown, differ on summand %d.\", i);\n-      }\n-#endif\n-      return MemPointerAliasing::make_unknown();\n+    if (trace.is_trace_aliasing()) {\n+      tty->print_cr(\"  -> Aliasing unknown, summands are not the same.\");\n@@ -332,0 +465,2 @@\n+#endif\n+    return MemPointerAliasing::make_unknown();\n@@ -349,9 +484,10 @@\n-  \/\/ \"MemPointer Lemma\" condition (S1):\n-  \/\/   Given that all summands are the same, we know that both pointers point into the\n-  \/\/   same memory object. With the Pre-Condition, we know that both pointers are in\n-  \/\/   bounds of that same memory object.\n-\n-  \/\/ Hence, all 4 conditions of the \"MemoryPointer Lemma\" are established, and hence\n-  \/\/ we know that the distance between the underlying pointers is equal to the distance\n-  \/\/ we computed for the MemPointers:\n-  \/\/   p_other - p_this = distance = other.con - this.con\n+  if (has_same_base) {\n+    \/\/ \"MemPointer Lemma\" condition (S1):\n+    \/\/   Given that all summands are the same, we know that both pointers point into the\n+    \/\/   same memory object. With the Pre-Condition, we know that both pointers are in\n+    \/\/   bounds of that same memory object.\n+    \/\/\n+    \/\/ Hence, all 4 conditions of the \"MemPointer Lemma\" are established, and hence\n+    \/\/ we know that the distance between the underlying pointers is equal to the distance\n+    \/\/ we computed for the MemPointers:\n+    \/\/   p_other - p_this = distance = other.con - this.con\n@@ -359,3 +495,40 @@\n-    if (trace.is_trace_aliasing()) {\n-      tty->print_cr(\"  -> Aliasing always, distance = %d.\", distance.value());\n-    }\n+      if (trace.is_trace_aliasing()) {\n+        tty->print_cr(\"  -> Aliasing always at distance = %d.\", distance.value());\n+      }\n+#endif\n+    return MemPointerAliasing::make_always_at_distance(distance.value());\n+  } else {\n+    \/\/ At runtime, the two object bases can be:\n+    \/\/   (1) different: pointers do not alias.\n+    \/\/   (2) the same:  implies that (S2) holds. The summands are all the same, and with\n+    \/\/                  the Pre-Condition, we know that both pointers are in bounds of the\n+    \/\/                  same memory object, i.e. (S1) holds. We have already proven (S0)\n+    \/\/                  and (S3), so all 4 conditions for \"MemPointer Lemma\" are given.\n+#ifndef PRODUCT\n+      if (trace.is_trace_aliasing()) {\n+        tty->print_cr(\"  -> Aliasing not or at distance = %d.\", distance.value());\n+      }\n+#endif\n+    return MemPointerAliasing::make_not_or_at_distance(distance.value());\n+  }\n+}\n+\n+bool MemPointer::has_same_summands_as(const MemPointer& other, uint start) const {\n+  for (uint i = start; i < SUMMANDS_SIZE; i++) {\n+    if (summands_at(i) != other.summands_at(i)) { return false; }\n+  }\n+  return true;\n+}\n+\n+bool MemPointer::has_different_object_base_but_otherwise_same_summands_as(const MemPointer& other) const {\n+  if (!base().is_object() ||\n+      !other.base().is_object() ||\n+      base().object() == other.base().object()) {\n+    \/\/ The base is the same, or we do not know if the base is different.\n+    return false;\n+  }\n+\n+#ifdef ASSERT\n+  const MemPointerSummand base1(base().object(),       NoOverflowInt(1));\n+  const MemPointerSummand base2(other.base().object(), NoOverflowInt(1));\n+  assert(summands_at(0) == base1 && other.summands_at(0) == base2, \"bases in 0th element\");\n@@ -363,1 +536,3 @@\n-  return MemPointerAliasing::make_always(distance.value());\n+\n+  \/\/ Check if all other summands are the same.\n+  return has_same_summands_as(other, 1);\n@@ -366,0 +541,15 @@\n+\/\/ Examples:\n+\/\/   p1 = MemPointer[size=1, base + i + 16]\n+\/\/   p2 = MemPointer[size=1, base + i + 17]\n+\/\/   -> Always at distance 1\n+\/\/   -> p1 always adjacent and before p2 -> return true\n+\/\/\n+\/\/   p1 = MemPointer[size=4, x + y + z + 4L * i + 16]\n+\/\/   p2 = MemPointer[size=4, x + y + z + 4L * i + 20]\n+\/\/   -> Always at distance 4\n+\/\/   -> p1 always adjacent and before p2 -> return true\n+\/\/\n+\/\/   p1 = MemPointer[size=4, base1 + 4L * i1 + 16]\n+\/\/   p2 = MemPointer[size=4, base2 + 4L * i2 + 20]\n+\/\/   -> Have differing summands, distance is unknown\n+\/\/   -> Unknown if adjacent at runtime -> return false\n@@ -367,5 +557,2 @@\n-  const MemPointerDecomposedForm& s1 = decomposed_form();\n-  const MemPointerDecomposedForm& s2 = other.decomposed_form();\n-  const MemPointerAliasing aliasing = s1.get_aliasing_with(s2 NOT_PRODUCT( COMMA _trace ));\n-  const jint size = mem()->memory_size();\n-  const bool is_adjacent = aliasing.is_always_at_distance(size);\n+  const MemPointerAliasing aliasing = get_aliasing_with(other NOT_PRODUCT( COMMA _trace ));\n+  const bool is_adjacent = aliasing.is_always_at_distance(_size);\n@@ -376,1 +563,1 @@\n-               is_adjacent ? \"true\" : \"false\", size);\n+               is_adjacent ? \"true\" : \"false\", _size);\n@@ -384,0 +571,54 @@\n+\n+\/\/ Examples:\n+\/\/   p1 = MemPointer[size=1, base + i + 16]\n+\/\/   p2 = MemPointer[size=1, base + i + 17]\n+\/\/   -> Always at distance 1\n+\/\/   -> Can never overlap -> return true\n+\/\/\n+\/\/   p1 = MemPointer[size=1, base + i + 16]\n+\/\/   p2 = MemPointer[size=1, base + i + 16]\n+\/\/   -> Always at distance 0\n+\/\/   -> Always have exact overlap -> return false\n+\/\/\n+\/\/   p1 = MemPointer[size=4, x + y + z + 4L * i + 16]\n+\/\/   p2 = MemPointer[size=4, x + y + z + 4L * i + 56]\n+\/\/   -> Always at distance 40\n+\/\/   -> Can never overlap -> return true\n+\/\/\n+\/\/   p1 = MemPointer[size=8, x + y + z + 4L * i + 16]\n+\/\/   p2 = MemPointer[size=8, x + y + z + 4L * i + 20]\n+\/\/   -> Always at distance 4\n+\/\/   -> Always have partial overlap -> return false\n+\/\/\n+\/\/   p1 = MemPointer[size=4, base1 + 4L * i1 + 16]\n+\/\/   p2 = MemPointer[size=4, base2 + 4L * i2 + 20]\n+\/\/   -> Have differing summands, distance is unknown\n+\/\/   -> Unknown if overlap at runtime -> return false\n+bool MemPointer::never_overlaps_with(const MemPointer& other) const {\n+  const MemPointerAliasing aliasing = get_aliasing_with(other NOT_PRODUCT( COMMA _trace ));\n+\n+  \/\/ The aliasing tries to compute:\n+  \/\/   distance = other - this\n+  \/\/\n+  \/\/ We know that we have no overlap if we can prove:\n+  \/\/   this >= other + other.size      ||  this + this.size <= other\n+  \/\/\n+  \/\/ Which we can restate as:\n+  \/\/   distance <= -other.size    ||  this.size <= distance\n+  \/\/\n+  const jint distance_lo = -other.size();\n+  const jint distance_hi = size();\n+  bool is_never_overlap = aliasing.is_never_in_distance_range(distance_lo, distance_hi);\n+\n+#ifndef PRODUCT\n+  if (_trace.is_trace_overlap()) {\n+    tty->print(\"Never Overlap: %s, distance_lo: %d, distance_hi: %d, aliasing: \",\n+               is_never_overlap ? \"true\" : \"false\", distance_lo, distance_hi);\n+    aliasing.print_on(tty);\n+    tty->cr();\n+  }\n+#endif\n+\n+  return is_never_overlap;\n+}\n+\n","filename":"src\/hotspot\/share\/opto\/mempointer.cpp","additions":300,"deletions":59,"binary":false,"changes":359,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2024, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -31,2 +31,12 @@\n-\/\/ The MemPointer is a shared facility to parse pointers and check the aliasing of pointers,\n-\/\/ e.g. checking if two stores are adjacent.\n+\/\/ The MemPointer is a shared facility to parse pointers and check the aliasing of pointers.\n+\/\/\n+\/\/ A MemPointer points to a region in memory, starting at a \"pointer\", and extending for \"size\" bytes:\n+\/\/   [pointer, pointer + size)\n+\/\/\n+\/\/ We can check if two loads \/ two stores:\n+\/\/  - are adjacent               -> pack multiple memops into a single memop\n+\/\/  - never overlap              -> independent, can swap order\n+\/\/\n+\/\/ Other use-cases:\n+\/\/  - alignment                  -> find an alignment solution for all memops in a vectorized loop\n+\/\/  - detect partial overlap     -> indicates store-to-load-forwarding failures\n@@ -146,1 +156,1 @@\n-\/\/ MemPointerDecomposedForm:\n+\/\/ MemPointer:\n@@ -164,11 +174,0 @@\n-\/\/ MemPointerAliasing:\n-\/\/   The decomposed form allows us to determine the aliasing between two pointers easily. For\n-\/\/   example, if two pointers are identical, except for their constant:\n-\/\/\n-\/\/     pointer1 = SUM(summands) + con1\n-\/\/     pointer2 = SUM(summands) + con2\n-\/\/\n-\/\/   then we can easily compute the distance between the pointers (distance = con2 - con1),\n-\/\/   and determine if they are adjacent.\n-\/\/\n-\/\/ MemPointerDecomposedFormParser:\n@@ -197,5 +196,49 @@\n-\/\/   Hence, in MemPointerDecomposedFormParser::parse_decomposed_form, we start with the pointer as\n-\/\/   a trivial summand. A summand can either be decomposed further or it is terminal (cannot\n-\/\/   be decomposed further). We decompose the summands recursively until all remaining summands\n-\/\/   are terminal, see MemPointerDecomposedFormParser::parse_sub_expression. This effectively parses\n-\/\/   the pointer expression recursively.\n+\/\/   Hence, in MemPointerParser::parse, we start with the pointer as a trivial summand. A summand can either\n+\/\/   be decomposed further or it is terminal (cannot be decomposed further). We decompose the summands\n+\/\/   recursively until all remaining summands are terminal, see MemPointerParser::parse_sub_expression.\n+\/\/   This effectively parses the pointer expression recursively.\n+\/\/\n+\/\/ MemPointerAliasing:\n+\/\/   The decomposed form allows us to determine the aliasing between two pointers easily. For\n+\/\/   example, if two pointers are identical, except for their constant:\n+\/\/\n+\/\/     pointer1 = SUM(summands) + con1\n+\/\/     pointer2 = SUM(summands) + con2\n+\/\/\n+\/\/   then we can easily compute the distance between the pointers (distance = con2 - con1),\n+\/\/   and determine if they are adjacent.\n+\/\/\n+\/\/ MemPointer::Base\n+\/\/   The MemPointer is decomposed like this:\n+\/\/     pointer = SUM(summands) + con\n+\/\/\n+\/\/   This is sufficient for simple adjacency checks and we do not need to know if the pointer references\n+\/\/   native (off-heap) or object (heap) memory. However, in some cases it is necessary or useful to know\n+\/\/   the object base, or the native pointer's base.\n+\/\/\n+\/\/   - Object (heap) base (MemPointer::base().is_object()):\n+\/\/     Is the base of the Java object, which resides on the Java heap.\n+\/\/     Guarantees:\n+\/\/       - Always has an alignment of ObjectAlignmentInBytes.\n+\/\/       - A MemPointer with a given object base always must point into the memory of that object. Thus,\n+\/\/         if we have two pointers with two different bases at runtime, we know the two pointers do not\n+\/\/         alias.\n+\/\/\n+\/\/   - Native (off-heap) base (MemPointer::base().is_native()):\n+\/\/     When we decompose a pointer to native memory, it is at first not clear that there is a base address.\n+\/\/     Even if we could know that there is some base address to which we add index offsets, we cannot know\n+\/\/     if this reference address points to the beginning of a native memory allocation or into the middle,\n+\/\/     or outside it. We also have no guarantee for alignment with such a base address.\n+\/\/     Still: we would like to find such a base if possible, and if two pointers are similar (i.e. have the\n+\/\/     same summands), we would like to find the same base. Further, it is reasonable to speculatively\n+\/\/     assume that such base addresses are aligned (need to add this speculative check in  JDK-8323582).\n+\/\/     A base pointer must have scale = 1, and be accepted byMemPointer::is_native_memory_base_candidate.\n+\/\/     It can thus be one of these:\n+\/\/      (1) CastX2P\n+\/\/          This is simply some arbitrary long cast to a pointer. It may be computed as an addition of\n+\/\/          multiple long and even int values. In some cases this means that we could have further\n+\/\/          decomposed the CastX2P, but at that point it is even harder to tell what should be a good\n+\/\/          candidate for a native memory base.\n+\/\/      (2) LoadL from field jdk.internal.foreign.NativeMemorySegmentImpl.min\n+\/\/          This would be preferrable over CastX2P, because it holds the address() of a native\n+\/\/          MemorySegment, i.e. we know it points to the beginning of that MemorySegment.\n@@ -262,2 +305,2 @@\n-\/\/    Note: MemPointerDecomposedForm::get_aliasing_with relies on this MemPointer Lemma to\n-\/\/          prove the correctness of its aliasing computation between two MemPointers.\n+\/\/    Note: MemPointer::get_aliasing_with relies on this MemPointer Lemma to prove the correctness of its\n+\/\/          aliasing computation between two MemPointers.\n@@ -266,2 +309,1 @@\n-\/\/    Note: MemPointerDecomposedFormParser::is_safe_to_decompose_op checks that all\n-\/\/          decompositions we apply are safe.\n+\/\/    Note: MemPointerParser::is_safe_to_decompose_op checks that all decompositions we apply are safe.\n@@ -344,1 +386,0 @@\n-\n@@ -348,1 +389,1 @@\n-  const bool _is_trace_pointer;\n+  const bool _is_trace_parsing;\n@@ -351,0 +392,1 @@\n+  const bool _is_trace_overlap;\n@@ -353,1 +395,1 @@\n-  TraceMemPointer(const bool is_trace_pointer,\n+  TraceMemPointer(const bool is_trace_parsing,\n@@ -355,2 +397,3 @@\n-                  const bool is_trace_adjacency) :\n-    _is_trace_pointer(  is_trace_pointer),\n+                  const bool is_trace_adjacency,\n+                  const bool is_trace_overlap) :\n+    _is_trace_parsing(  is_trace_parsing),\n@@ -358,1 +401,2 @@\n-    _is_trace_adjacency(is_trace_adjacency)\n+    _is_trace_adjacency(is_trace_adjacency),\n+    _is_trace_overlap(is_trace_overlap)\n@@ -361,1 +405,1 @@\n-  bool is_trace_pointer()   const { return _is_trace_pointer; }\n+  bool is_trace_parsing()   const { return _is_trace_parsing; }\n@@ -364,0 +408,1 @@\n+  bool is_trace_overlap()   const { return _is_trace_overlap; }\n@@ -369,9 +414,0 @@\n-public:\n-  enum Aliasing {\n-    Unknown, \/\/ Distance unknown.\n-             \/\/   Example: two \"int[]\" with different variable index offsets.\n-             \/\/            e.g. \"array[i]  vs  array[j]\".\n-             \/\/            e.g. \"array1[i] vs  array2[j]\".\n-    Always}; \/\/ Constant distance = p1 - p2.\n-             \/\/   Example: The same address expression, except for a constant offset\n-             \/\/            e.g. \"array[i]  vs  array[i+1]\".\n@@ -379,0 +415,16 @@\n+  enum Aliasing {\n+    Unknown,          \/\/ Distance unknown.\n+                      \/\/   Example: two \"int[]\" (unknown if the same) with different variable index offsets:\n+                      \/\/            e.g. \"array[i]  vs  array[j]\".\n+                      \/\/            e.g. \"array1[i] vs  array2[j]\".\n+    AlwaysAtDistance, \/\/ Constant distance = p2 - p1.\n+                      \/\/   Example: The same address expression, except for a constant offset:\n+                      \/\/            e.g. \"array[i]  vs  array[i+1]\".\n+    NotOrAtDistance}; \/\/ At compile-time, we know that at run-time it is either of these:\n+                      \/\/   (1) Not: The pointers belong to different memory objects. Distance unknown.\n+                      \/\/   (2) AtConstDistance: distance = p2 - p1.\n+                      \/\/   Example: two \"int[]\" (unknown if the same) with indices that only differ by a\n+                      \/\/            constant offset:\n+                      \/\/            e.g. \"array1[i] vs array2[i+4]\":\n+                      \/\/                 if \"array1 == array2\": distance = 4.\n+                      \/\/                 if \"array1 != array2\": different memory objects.\n@@ -394,2 +446,6 @@\n-  static MemPointerAliasing make_always(const jint distance) {\n-    return MemPointerAliasing(Always, distance);\n+  static MemPointerAliasing make_always_at_distance(const jint distance) {\n+    return MemPointerAliasing(AlwaysAtDistance, distance);\n+  }\n+\n+  static MemPointerAliasing make_not_or_at_distance(const jint distance) {\n+    return MemPointerAliasing(NotOrAtDistance, distance);\n@@ -400,1 +456,8 @@\n-    return _aliasing == Always && _distance == distance;\n+    return _aliasing == AlwaysAtDistance && _distance == distance;\n+  }\n+\n+  \/\/ Use case: overlap.\n+  \/\/ Note: the bounds are exclusive: lo < element < hi\n+  bool is_never_in_distance_range(const jint distance_lo, const jint distance_hi) const {\n+    return (_aliasing == AlwaysAtDistance || _aliasing == NotOrAtDistance) &&\n+           (_distance <= distance_lo || distance_hi <= _distance);\n@@ -406,2 +469,3 @@\n-      case Unknown: st->print(\"Unknown\");               break;\n-      case Always:  st->print(\"Always(%d)\", _distance); break;\n+      case Unknown:           st->print(\"Unknown\");                         break;\n+      case AlwaysAtDistance:  st->print(\"AlwaysAtDistance(%d)\", _distance); break;\n+      case NotOrAtDistance:   st->print(\"NotOrAtDistance(%d)\",  _distance); break;\n@@ -414,1 +478,1 @@\n-\/\/ Summand of a MemPointerDecomposedForm:\n+\/\/ Summand of a MemPointer:\n@@ -440,3 +504,7 @@\n-    if (p1->variable() == nullptr) {\n-      return (p2->variable() == nullptr) ? 0 : 1;\n-    } else if (p2->variable() == nullptr) {\n+    return cmp_by_variable_idx(*p1, *p2);\n+  }\n+\n+  static int cmp_by_variable_idx(const MemPointerSummand& p1, const MemPointerSummand& p2) {\n+    if (p1.variable() == nullptr) {\n+      return (p2.variable() == nullptr) ? 0 : 1;\n+    } else if (p2.variable() == nullptr) {\n@@ -446,1 +514,8 @@\n-    return p1->variable()->_idx - p2->variable()->_idx;\n+    return p1.variable()->_idx - p2.variable()->_idx;\n+  }\n+\n+  static int cmp(const MemPointerSummand& p1, const MemPointerSummand& p2) {\n+    int cmp = cmp_by_variable_idx(p1, p2);\n+    if (cmp != 0) { return cmp; }\n+\n+    return NoOverflowInt::cmp(p1.scale(), p2.scale());\n@@ -464,1 +539,0 @@\n-    st->print(\"Summand[\");\n@@ -466,1 +540,1 @@\n-    tty->print(\" * [%d %s]]\", _variable->_idx, _variable->Name());\n+    tty->print(\" * [%d %s]\", _variable->_idx, _variable->Name());\n@@ -471,1 +545,22 @@\n-\/\/ Decomposed form of the pointer sub-expression of \"pointer\".\n+\/\/ Parsing calls the callback on every decomposed node. These are all the\n+\/\/ nodes on the paths from the pointer to the summand variables, i.e. the\n+\/\/ \"inner\" nodes of the pointer expression. This callback is for example\n+\/\/ used in SuperWord::unrolling_analysis to collect all inner nodes of a\n+\/\/ pointer expression.\n+class MemPointerParserCallback : public StackObj {\n+private:\n+  static MemPointerParserCallback _empty;\n+\n+public:\n+  virtual void callback(Node* n) { \/* do nothing by default *\/ }\n+\n+  \/\/ Singleton for default arguments.\n+  static MemPointerParserCallback& empty() { return _empty; }\n+};\n+\n+\/\/ A MemPointer points to a region in memory, starting at a \"pointer\", and extending\n+\/\/ for \"size\" bytes:\n+\/\/\n+\/\/   [pointer, pointer + size)\n+\/\/\n+\/\/ Where the \"pointer\" is decomposed into the following form:\n@@ -474,0 +569,1 @@\n+\/\/   pointer = SUM(scale_i * variable_i) + con\n@@ -475,2 +571,11 @@\n-class MemPointerDecomposedForm : public StackObj {\n-private:\n+\/\/ Where SUM() adds all \"scale_i * variable_i\" for each i together.\n+\/\/\n+\/\/ Node: if the base is known, then it is in the 0th summand. A base can be:\n+\/\/       - on-heap  \/ object: base().object()\n+\/\/       - off-heap \/ native: base().native()\n+\/\/\n+\/\/   pointer = scale_0 * variable_0 + scale_1 * scale_1 + ... + con\n+\/\/   pointer =       1 * base       + scale_1 * scale_1 + ... + con\n+\/\/\n+class MemPointer : public StackObj {\n+public:\n@@ -483,1 +588,14 @@\n-  Node* _pointer; \/\/ pointer node associated with this (sub)pointer\n+  \/\/ A base can be:\n+  \/\/ - Known:\n+  \/\/   - On-heap: Object\n+  \/\/   - Off-heap: Native\n+  \/\/ - Unknown\n+  class Base : public StackObj {\n+  private:\n+    enum Kind { Unknown, Object, Native };\n+    Kind _kind;\n+    Node* _base;\n+\n+    Base(Kind kind, Node* base) : _kind(kind), _base(base) {\n+      assert((kind == Unknown) == (base == nullptr), \"known base\");\n+    }\n@@ -485,2 +603,3 @@\n-  MemPointerSummand _summands[SUMMANDS_SIZE];\n-  NoOverflowInt _con;\n+  public:\n+    Base() : Base(Unknown, nullptr) {}\n+    static Base make(Node* pointer, const GrowableArray<MemPointerSummand>& summands);\n@@ -488,3 +607,28 @@\n-public:\n-  \/\/ Empty\n-  MemPointerDecomposedForm() : _pointer(nullptr), _con(NoOverflowInt::make_NaN()) {}\n+    bool is_known()          const { return _kind != Unknown; }\n+    bool is_object()         const { return _kind == Object; }\n+    bool is_native()         const { return _kind == Native; }\n+    Node* object()           const { assert(is_object(), \"\"); return _base; }\n+    Node* native()           const { assert(is_native(), \"\"); return _base; }\n+    Node* object_or_native() const { assert(is_known(),  \"\"); return _base; }\n+    Node* object_or_native_or_null() const { return _base; }\n+\n+#ifndef PRODUCT\n+    void print_on(outputStream* st) const {\n+      switch (_kind) {\n+      case Object:\n+          st->print(\"object  \");\n+          st->print(\"%d %s\", _base->_idx, _base->Name());\n+          break;\n+      case Native:\n+          st->print(\"native  \");\n+          st->print(\"%d %s\", _base->_idx, _base->Name());\n+          break;\n+      default:\n+          st->print(\"unknown \");\n+      };\n+    }\n+#endif\n+\n+  private:\n+    static Node* find_base(Node* object_base, const GrowableArray<MemPointerSummand>& summands);\n+  };\n@@ -493,0 +637,6 @@\n+  MemPointerSummand _summands[SUMMANDS_SIZE];\n+  const NoOverflowInt _con;\n+  const Base _base;\n+  const jint _size;\n+  NOT_PRODUCT( const TraceMemPointer& _trace; )\n+\n@@ -494,1 +644,8 @@\n-  MemPointerDecomposedForm(Node* pointer) : _pointer(pointer), _con(NoOverflowInt(0)) {\n+  MemPointer(Node* pointer,\n+             const jint size\n+             NOT_PRODUCT(COMMA const TraceMemPointer& trace)) :\n+    _con(NoOverflowInt(0)),\n+    _base(Base()),\n+    _size(size)\n+    NOT_PRODUCT(COMMA _trace(trace))\n+  {\n@@ -497,0 +654,1 @@\n+    assert(1 <= _size && _size <= 2048 && is_power_of_2(_size), \"sanity: no vector is expected to be larger\");\n@@ -499,2 +657,11 @@\n-  MemPointerDecomposedForm(Node* pointer, const GrowableArray<MemPointerSummand>& summands, const NoOverflowInt& con)\n-    : _pointer(pointer), _con(con) {\n+  \/\/ pointer = SUM(SUMMANDS) + con\n+  MemPointer(Node* pointer,\n+             const GrowableArray<MemPointerSummand>& summands,\n+             const NoOverflowInt& con,\n+             const jint size\n+             NOT_PRODUCT(COMMA const TraceMemPointer& trace)) :\n+    _con(con),\n+    _base(Base::make(pointer, summands)),\n+    _size(size)\n+    NOT_PRODUCT(COMMA _trace(trace))\n+  {\n@@ -503,0 +670,1 @@\n+#ifdef ASSERT\n@@ -504,1 +672,1 @@\n-      MemPointerSummand s = summands.at(i);\n+      const MemPointerSummand& s = summands.at(i);\n@@ -507,1 +675,35 @@\n-      _summands[i] = s;\n+    }\n+#endif\n+\n+    \/\/ Put the base in in the 0th summand.\n+    Node* base = _base.object_or_native_or_null();\n+    int pos = 0;\n+    if (base != nullptr) {\n+      MemPointerSummand b(base, NoOverflowInt(1));\n+      _summands[0] = b;\n+      pos++;\n+    }\n+    \/\/ Put all other summands afterward.\n+    for (int i = 0; i < summands.length(); i++) {\n+      const MemPointerSummand& s = summands.at(i);\n+      if (s.variable() == base && s.scale().is_one()) { continue; }\n+      _summands[pos++] = summands.at(i);\n+    }\n+    assert(pos == summands.length(), \"copied all summands\");\n+\n+    assert(1 <= _size && _size <= 2048 && is_power_of_2(_size), \"sanity: no vector is expected to be larger\");\n+  }\n+\n+  \/\/ Mutated copy.\n+  \/\/   The new MemPointer is identical, except it has a different size and con.\n+  MemPointer(const MemPointer& old,\n+             const NoOverflowInt new_con,\n+             const jint new_size) :\n+    _con(new_con),\n+    _base(old.base()),\n+    _size(new_size)\n+    NOT_PRODUCT(COMMA _trace(old._trace))\n+  {\n+    assert(!_con.is_NaN(), \"non-NaN constant\");\n+    for (int i = 0; i < SUMMANDS_SIZE; i++) {\n+      _summands[i] = old.summands_at(i);\n@@ -512,2 +714,16 @@\n-  static MemPointerDecomposedForm make_trivial(Node* pointer) {\n-    return MemPointerDecomposedForm(pointer);\n+  \/\/ Parse pointer of MemNode. Delegates to MemPointerParser::parse.\n+  \/\/ callback: receives a callback for every decomposed (inner) node\n+  \/\/           of the pointer expression.\n+  MemPointer(const MemNode* mem,\n+             MemPointerParserCallback& callback\n+             NOT_PRODUCT(COMMA const TraceMemPointer& trace));\n+\n+  \/\/ Parse pointer of MemNode. Delegates to MemPointerParser::parse.\n+  MemPointer(const MemNode* mem\n+             NOT_PRODUCT(COMMA const TraceMemPointer& trace)) :\n+    MemPointer(mem, MemPointerParserCallback::empty() NOT_PRODUCT(COMMA trace)) {}\n+\n+  static MemPointer make_trivial(Node* pointer,\n+                                 const jint size\n+                                 NOT_PRODUCT(COMMA const TraceMemPointer& trace)) {\n+    return MemPointer(pointer, size NOT_PRODUCT(COMMA trace));\n@@ -516,1 +732,5 @@\n-  static MemPointerDecomposedForm make(Node* pointer, const GrowableArray<MemPointerSummand>& summands, const NoOverflowInt& con) {\n+  static MemPointer make(Node* pointer,\n+                         const GrowableArray<MemPointerSummand>& summands,\n+                         const NoOverflowInt& con,\n+                         const jint size\n+                         NOT_PRODUCT(COMMA const TraceMemPointer& trace)) {\n@@ -518,1 +738,1 @@\n-      return MemPointerDecomposedForm(pointer, summands, con);\n+      return MemPointer(pointer, summands, con, size NOT_PRODUCT(COMMA trace));\n@@ -520,1 +740,1 @@\n-      return MemPointerDecomposedForm::make_trivial(pointer);\n+      return MemPointer::make_trivial(pointer, size NOT_PRODUCT(COMMA trace));\n@@ -524,2 +744,7 @@\n-  MemPointerAliasing get_aliasing_with(const MemPointerDecomposedForm& other\n-                                       NOT_PRODUCT( COMMA const TraceMemPointer& trace) ) const;\n+  MemPointer make_with_size(const jint new_size) const {\n+    return MemPointer(*this, this->con(), new_size);\n+  };\n+\n+  MemPointer make_with_con(const NoOverflowInt new_con) const {\n+    return MemPointer(*this, new_con, this->size());\n+  };\n@@ -527,1 +752,19 @@\n-  const MemPointerSummand summands_at(const uint i) const {\n+private:\n+  MemPointerAliasing get_aliasing_with(const MemPointer& other\n+                                       NOT_PRODUCT(COMMA const TraceMemPointer& trace)) const;\n+\n+  bool has_same_summands_as(const MemPointer& other, uint start) const;\n+  bool has_same_summands_as(const MemPointer& other) const { return has_same_summands_as(other, 0); }\n+  bool has_different_object_base_but_otherwise_same_summands_as(const MemPointer& other) const;\n+\n+public:\n+  bool has_same_non_base_summands_as(const MemPointer& other) const {\n+    if (!base().is_known() || !other.base().is_known()) {\n+      assert(false, \"unknonw base case is not answered optimally\");\n+      return false;\n+    }\n+    \/\/ Known base at 0th summand: all other summands are non-base summands.\n+    return has_same_summands_as(other, 1);\n+  }\n+\n+  const MemPointerSummand& summands_at(const uint i) const {\n@@ -533,0 +776,25 @@\n+  const Base& base() const { return _base; }\n+  jint size() const { return _size; }\n+\n+  static int cmp_summands(const MemPointer& a, const MemPointer& b) {\n+    for (int i = 0; i < SUMMANDS_SIZE; i++) {\n+      const MemPointerSummand& s_a = a.summands_at(i);\n+      const MemPointerSummand& s_b = b.summands_at(i);\n+      int cmp = MemPointerSummand::cmp(s_a, s_b);\n+      if (cmp != 0) { return cmp;}\n+    }\n+    return 0;\n+  }\n+\n+  template<typename Callback>\n+  void for_each_non_empty_summand(Callback callback) const {\n+    for (int i = 0; i < SUMMANDS_SIZE; i++) {\n+      const MemPointerSummand& s = summands_at(i);\n+      if (s.variable() != nullptr) {\n+        callback(s);\n+      }\n+    }\n+  }\n+\n+  bool is_adjacent_to_and_before(const MemPointer& other) const;\n+  bool never_overlaps_with(const MemPointer& other) const;\n@@ -535,3 +803,3 @@\n-  void print_on(outputStream* st) const {\n-    if (_pointer == nullptr) {\n-      st->print_cr(\"MemPointerDecomposedForm empty.\");\n+  void print_form_on(outputStream* st) const {\n+    if (_con.is_NaN()) {\n+      st->print_cr(\"empty\");\n@@ -540,1 +808,0 @@\n-    st->print(\"MemPointerDecomposedForm[%d %s:  con = \", _pointer->_idx, _pointer->Name());\n@@ -545,1 +812,1 @@\n-        st->print(\", \");\n+        st->print(\" + \");\n@@ -549,1 +816,9 @@\n-    st->print_cr(\"]\");\n+  }\n+\n+  void print_on(outputStream* st, bool end_with_cr = true) const {\n+    st->print(\"MemPointer[size: %2d, base: \", size());\n+    _base.print_on(st);\n+    st->print(\", form: \");\n+    print_form_on(st);\n+    st->print(\"]\");\n+    if (end_with_cr) { st->cr(); }\n@@ -554,1 +829,1 @@\n-class MemPointerDecomposedFormParser : public StackObj {\n+class MemPointerParser : public StackObj {\n@@ -564,22 +839,1 @@\n-  MemPointerDecomposedForm _decomposed_form;\n-\n-public:\n-  MemPointerDecomposedFormParser(const MemNode* mem) : _mem(mem), _con(NoOverflowInt(0)) {\n-    _decomposed_form = parse_decomposed_form();\n-  }\n-\n-  const MemPointerDecomposedForm decomposed_form() const { return _decomposed_form; }\n-\n-private:\n-  MemPointerDecomposedForm parse_decomposed_form();\n-  void parse_sub_expression(const MemPointerSummand& summand);\n-\n-  bool is_safe_to_decompose_op(const int opc, const NoOverflowInt& scale) const;\n-};\n-\n-\/\/ Facility to parse the pointer of a Load or Store, so that aliasing between two such\n-\/\/ memory operations can be determined (e.g. adjacency).\n-class MemPointer : public StackObj {\n-private:\n-  const MemNode* _mem;\n-  const MemPointerDecomposedForm _decomposed_form;\n+  MemPointer _mem_pointer;\n@@ -587,1 +841,6 @@\n-  NOT_PRODUCT( const TraceMemPointer& _trace; )\n+  MemPointerParser(const MemNode* mem,\n+                   MemPointerParserCallback& callback\n+                   NOT_PRODUCT(COMMA const TraceMemPointer& trace)) :\n+    _mem(mem),\n+    _con(NoOverflowInt(0)),\n+    _mem_pointer(parse(callback NOT_PRODUCT(COMMA trace))) {}\n@@ -590,5 +849,7 @@\n-  MemPointer(const MemNode* mem NOT_PRODUCT( COMMA const TraceMemPointer& trace)) :\n-    _mem(mem),\n-    _decomposed_form(init_decomposed_form(_mem))\n-    NOT_PRODUCT( COMMA _trace(trace) )\n-  {\n+  static MemPointer parse(const MemNode* mem,\n+                          MemPointerParserCallback& callback\n+                          NOT_PRODUCT(COMMA const TraceMemPointer& trace)) {\n+    assert(mem->is_Store() || mem->is_Load(), \"only stores and loads are allowed\");\n+    ResourceMark rm;\n+    MemPointerParser parser(mem, callback NOT_PRODUCT(COMMA trace));\n+\n@@ -596,5 +857,5 @@\n-    if (_trace.is_trace_pointer()) {\n-      tty->print_cr(\"MemPointer::MemPointer:\");\n-      tty->print(\"mem: \"); mem->dump();\n-      _mem->in(MemNode::Address)->dump_bfs(5, nullptr, \"d\");\n-      _decomposed_form.print_on(tty);\n+    if (trace.is_trace_parsing()) {\n+      tty->print_cr(\"\\nMemPointerParser::parse:\");\n+      tty->print(\"  mem: \"); mem->dump();\n+      parser.mem_pointer().print_on(tty);\n+      mem->in(MemNode::Address)->dump_bfs(7, nullptr, \"d\");\n@@ -603,0 +864,2 @@\n+\n+    return parser.mem_pointer();\n@@ -605,3 +868,1 @@\n-  const MemNode* mem() const { return _mem; }\n-  const MemPointerDecomposedForm decomposed_form() const { return _decomposed_form; }\n-  bool is_adjacent_to_and_before(const MemPointer& other) const;\n+  static bool is_native_memory_base_candidate(Node* n);\n@@ -610,6 +871,9 @@\n-  static const MemPointerDecomposedForm init_decomposed_form(const MemNode* mem) {\n-    assert(mem->is_Store(), \"only stores are supported\");\n-    ResourceMark rm;\n-    MemPointerDecomposedFormParser parser(mem);\n-    return parser.decomposed_form();\n-  }\n+  const MemPointer& mem_pointer() const { return _mem_pointer; }\n+\n+  MemPointer parse(MemPointerParserCallback& callback\n+                   NOT_PRODUCT(COMMA const TraceMemPointer& trace));\n+\n+  void parse_sub_expression(const MemPointerSummand& summand, MemPointerParserCallback& callback);\n+  static bool sub_expression_has_native_base_candidate(Node* n);\n+\n+  bool is_safe_to_decompose_op(const int opc, const NoOverflowInt& scale) const;\n","filename":"src\/hotspot\/share\/opto\/mempointer.hpp","additions":387,"deletions":123,"binary":false,"changes":510,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2024, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -55,0 +55,1 @@\n+  bool is_one() const { return !is_NaN() && value() == 1; }\n@@ -103,0 +104,17 @@\n+  \/\/ This \"cmp\" is used for sort only.\n+  \/\/ Note: the NaN semantics are different from floating arithmetic NaNs!\n+  \/\/ - Smaller non-NaN are before larger non-NaN.\n+  \/\/ - Any non-NaN are before NaN.\n+  \/\/ - NaN is equal to NaN.\n+  \/\/ Note: NaN indicate overflow, uninitialized, etc.\n+  static int cmp(const NoOverflowInt& a, const NoOverflowInt& b) {\n+    if (a.is_NaN()) {\n+      return b.is_NaN() ? 0 : 1;\n+    } else if (b.is_NaN()) {\n+      return -1;\n+    }\n+    if (a.value() < b.value()) { return -1; }\n+    if (a.value() > b.value()) { return  1; }\n+    return 0;\n+  }\n+\n","filename":"src\/hotspot\/share\/opto\/noOverflowInt.hpp","additions":19,"deletions":1,"binary":false,"changes":20,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2007, 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2007, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -28,0 +28,1 @@\n+#include \"opto\/memnode.hpp\"\n@@ -51,0 +52,46 @@\n+\/\/ Collect ignored loop nodes during VPointer parsing.\n+class SuperWordUnrollingAnalysisIgnoredNodes : public MemPointerParserCallback {\n+private:\n+  const VLoop&     _vloop;\n+  const Node_List& _body;\n+  bool*            _ignored;\n+\n+public:\n+  SuperWordUnrollingAnalysisIgnoredNodes(const VLoop& vloop) :\n+    _vloop(vloop),\n+    _body(_vloop.lpt()->_body),\n+    _ignored(NEW_RESOURCE_ARRAY(bool, _body.size()))\n+  {\n+    for (uint i = 0; i < _body.size(); i++) {\n+      _ignored[i] = false;\n+    }\n+  }\n+\n+  virtual void callback(Node* n) override { set_ignored(n); }\n+\n+  void set_ignored(uint i) {\n+    assert(i < _body.size(), \"must be in bounds\");\n+    _ignored[i] = true;\n+  }\n+\n+  void set_ignored(Node* n) {\n+    \/\/ Only consider nodes in the loop.\n+    Node* ctrl = _vloop.phase()->get_ctrl(n);\n+    if (_vloop.lpt()->is_member(_vloop.phase()->get_loop(ctrl))) {\n+      \/\/ Find the index in the loop.\n+      for (uint j = 0; j < _body.size(); j++) {\n+        if (n == _body.at(j)) {\n+          set_ignored(j);\n+          return;\n+        }\n+      }\n+      assert(false, \"must find\");\n+    }\n+  }\n+\n+  bool is_ignored(uint i) const {\n+    assert(i < _vloop.lpt()->_body.size(), \"must be in bounds\");\n+    return _ignored[i];\n+  }\n+};\n+\n@@ -57,0 +104,1 @@\n+  SuperWordUnrollingAnalysisIgnoredNodes ignored_nodes(vloop);\n@@ -58,8 +106,0 @@\n-  size_t ignored_size = lpt->_body.size();\n-  int *ignored_loop_nodes = NEW_RESOURCE_ARRAY(int, ignored_size);\n-  Node_Stack nstack((int)ignored_size);\n-\n-  \/\/ First clear the entries\n-  for (uint i = 0; i < lpt->_body.size(); i++) {\n-    ignored_loop_nodes[i] = -1;\n-  }\n@@ -80,1 +120,1 @@\n-      ignored_loop_nodes[i] = n->_idx;\n+      ignored_nodes.set_ignored(i);\n@@ -88,1 +128,1 @@\n-          ignored_loop_nodes[i] = n->_idx;\n+          ignored_nodes.set_ignored(i);\n@@ -106,1 +146,1 @@\n-      ignored_loop_nodes[i] = n->_idx;\n+      ignored_nodes.set_ignored(i);\n@@ -124,1 +164,1 @@\n-      ignored_loop_nodes[i] = n->_idx;\n+      ignored_nodes.set_ignored(i);\n@@ -135,24 +175,3 @@\n-        \/\/ Process the memory expression\n-        int stack_idx = 0;\n-        bool have_side_effects = true;\n-        if (adr->is_AddP() == false) {\n-          nstack.push(adr, stack_idx++);\n-        } else {\n-          \/\/ Mark the components of the memory operation in nstack\n-          VPointer p1(current, vloop, &nstack);\n-          have_side_effects = p1.node_stack()->is_nonempty();\n-        }\n-\n-        \/\/ Process the pointer stack\n-        while (have_side_effects) {\n-          Node* pointer_node = nstack.node();\n-          for (uint j = 0; j < lpt->_body.size(); j++) {\n-            Node* cur_node = lpt->_body.at(j);\n-            if (cur_node == pointer_node) {\n-              ignored_loop_nodes[j] = cur_node->_idx;\n-              break;\n-            }\n-          }\n-          nstack.pop();\n-          have_side_effects = nstack.is_nonempty();\n-        }\n+        \/\/ Parse the address expression with VPointer, and mark the internal\n+        \/\/ nodes of the address expression in ignore_nodes.\n+        VPointer p(current, vloop, ignored_nodes);\n@@ -168,1 +187,1 @@\n-      if (ignored_loop_nodes[i] != -1) continue;\n+      if (ignored_nodes.is_ignored(i)) continue;\n@@ -480,0 +499,24 @@\n+int SuperWord::MemOp::cmp_by_group(MemOp* a, MemOp* b) {\n+  \/\/ Opcode\n+  int c_Opcode = cmp_code(a->mem()->Opcode(), b->mem()->Opcode());\n+  if (c_Opcode != 0) { return c_Opcode; }\n+\n+  \/\/ VPointer summands\n+  return MemPointer::cmp_summands(a->vpointer().mem_pointer(),\n+                                  b->vpointer().mem_pointer());\n+}\n+\n+int SuperWord::MemOp::cmp_by_group_and_con_and_original_index(MemOp* a, MemOp* b) {\n+  \/\/ Group\n+  int cmp_group = cmp_by_group(a, b);\n+  if (cmp_group != 0) { return cmp_group; }\n+\n+  \/\/ VPointer con\n+  jint a_con = a->vpointer().mem_pointer().con().value();\n+  jint b_con = b->vpointer().mem_pointer().con().value();\n+  int c_con = cmp_code(a_con, b_con);\n+  if (c_con != 0) { return c_con; }\n+\n+  return cmp_code(a->original_index(), b->original_index());\n+}\n+\n@@ -483,12 +526,15 @@\n-  GrowableArray<const VPointer*> vpointers;\n-\n-  collect_valid_vpointers(vpointers);\n-\n-  \/\/ Sort the VPointers. This does 2 things:\n-  \/\/  - Separate the VPointer into groups: all memops that have the same opcode and the same\n-  \/\/    VPointer, except for the offset. Adjacent memops must have the same opcode and the\n-  \/\/    same VPointer, except for a shift in the offset. Thus, two memops can only be adjacent\n-  \/\/    if they are in the same group. This decreases the work.\n-  \/\/  - Sort by offset inside the groups. This decreases the work needed to determine adjacent\n-  \/\/    memops inside a group.\n-  vpointers.sort(VPointer::cmp_for_sort);\n+  GrowableArray<MemOp> memops;\n+\n+  collect_valid_memops(memops);\n+\n+  \/\/ Sort the MemOps by group, and inside a group by VPointer con:\n+  \/\/  - Group: all memops with the same opcode, and the same VPointer summands. Adjacent memops\n+  \/\/           have the same opcode and the same VPointer summands, only the VPointer con is\n+  \/\/           different. Thus, two memops can only be adjacent if they are in the same group.\n+  \/\/           This decreases the work.\n+  \/\/  - VPointer con: Sorting by VPointer con inside the group allows us to perform a sliding\n+  \/\/                  window algorithm, to determine adjacent memops efficiently.\n+  \/\/ Since GrowableArray::sort relies on qsort, the sort is not stable on its own. This can lead\n+  \/\/ to worse packing in some cases. To make the sort stable, our last cmp criterion is the\n+  \/\/ original index, i.e. the position in the memops array before sorting.\n+  memops.sort(MemOp::cmp_by_group_and_con_and_original_index);\n@@ -502,1 +548,1 @@\n-  create_adjacent_memop_pairs_in_all_groups(vpointers);\n+  create_adjacent_memop_pairs_in_all_groups(memops);\n@@ -512,3 +558,4 @@\n-\/\/ Collect all memops vpointers that could potentially be vectorized.\n-void SuperWord::collect_valid_vpointers(GrowableArray<const VPointer*>& vpointers) {\n-  for_each_mem([&] (const MemNode* mem, int bb_idx) {\n+\/\/ Collect all memops that could potentially be vectorized.\n+void SuperWord::collect_valid_memops(GrowableArray<MemOp>& memops) {\n+  int original_index = 0;\n+  for_each_mem([&] (MemNode* mem, int bb_idx) {\n@@ -516,1 +563,1 @@\n-    if (p.valid() &&\n+    if (p.is_valid() &&\n@@ -519,1 +566,1 @@\n-      vpointers.append(&p);\n+      memops.append(MemOp(mem, &p, original_index++));\n@@ -525,1 +572,1 @@\n-void SuperWord::create_adjacent_memop_pairs_in_all_groups(const GrowableArray<const VPointer*> &vpointers) {\n+void SuperWord::create_adjacent_memop_pairs_in_all_groups(const GrowableArray<MemOp>& memops) {\n@@ -527,3 +574,3 @@\n-  while (group_start < vpointers.length()) {\n-    int group_end = find_group_end(vpointers, group_start);\n-    create_adjacent_memop_pairs_in_one_group(vpointers, group_start, group_end);\n+  while (group_start < memops.length()) {\n+    int group_end = find_group_end(memops, group_start);\n+    create_adjacent_memop_pairs_in_one_group(memops, group_start, group_end);\n@@ -534,2 +581,2 @@\n-\/\/ Step forward until we find a VPointer of another group, or we reach the end of the array.\n-int SuperWord::find_group_end(const GrowableArray<const VPointer*>& vpointers, int group_start) {\n+\/\/ Step forward until we find a MemOp of another group, or we reach the end of the array.\n+int SuperWord::find_group_end(const GrowableArray<MemOp>& memops, int group_start) {\n@@ -537,4 +584,4 @@\n-  while (group_end < vpointers.length() &&\n-         VPointer::cmp_for_sort_by_group(\n-           vpointers.adr_at(group_start),\n-           vpointers.adr_at(group_end)\n+  while (group_end < memops.length() &&\n+         MemOp::cmp_by_group(\n+           memops.adr_at(group_start),\n+           memops.adr_at(group_end)\n@@ -549,1 +596,1 @@\n-void SuperWord::create_adjacent_memop_pairs_in_one_group(const GrowableArray<const VPointer*>& vpointers, const int group_start, const int group_end) {\n+void SuperWord::create_adjacent_memop_pairs_in_one_group(const GrowableArray<MemOp>& memops, const int group_start, const int group_end) {\n@@ -554,1 +601,3 @@\n-      const VPointer* p = vpointers.at(i);\n+      const MemOp& memop = memops.at(i);\n+      tty->print(\"  \");\n+      memop.mem()->dump();\n@@ -556,1 +605,1 @@\n-      p->print();\n+      memop.vpointer().print_on(tty);\n@@ -561,2 +610,2 @@\n-  MemNode* first = vpointers.at(group_start)->mem();\n-  int element_size = data_size(first);\n+  MemNode* first = memops.at(group_start).mem();\n+  const int element_size = data_size(first);\n@@ -566,2 +615,2 @@\n-    const VPointer* p1 = vpointers.at(i);\n-    MemNode* mem1 = p1->mem();\n+    const VPointer& p1  = memops.at(i).vpointer();\n+    MemNode* mem1 = memops.at(i).mem();\n@@ -572,2 +621,2 @@\n-      const VPointer* p2 = vpointers.at(j);\n-      MemNode* mem2 = p2->mem();\n+      const VPointer& p2  = memops.at(j).vpointer();\n+      MemNode* mem2 = memops.at(j).mem();\n@@ -579,3 +628,3 @@\n-      assert(p1->offset_in_bytes() <= p2->offset_in_bytes(), \"must be sorted by offset\");\n-      if (p1->offset_in_bytes() + element_size > p2->offset_in_bytes()) { continue; }\n-      if (p1->offset_in_bytes() + element_size < p2->offset_in_bytes()) { break; }\n+      assert(p1.con() <= p2.con(), \"must be sorted by offset\");\n+      if (p1.con() + element_size > p2.con()) { continue; }\n+      if (p1.con() + element_size < p2.con()) { break; }\n@@ -596,1 +645,1 @@\n-        p1->print();\n+        p1.print_on(tty);\n@@ -598,1 +647,1 @@\n-        p2->print();\n+        p2.print_on(tty);\n@@ -726,2 +775,0 @@\n-  \/\/ Adjacent memory references must have the same base, be comparable\n-  \/\/ and have the correct distance between them.\n@@ -730,3 +777,1 @@\n-  if (p1.base() != p2.base() || !p1.comparable(p2)) return false;\n-  int diff = p2.offset_in_bytes() - p1.offset_in_bytes();\n-  return diff == data_size(s1);\n+  return p1.is_adjacent_to_and_before(p2);\n@@ -1435,1 +1480,2 @@\n-  AlignmentSolver solver(pack->at(0)->as_Mem(),\n+  AlignmentSolver solver(mem_ref_p,\n+                         pack->at(0)->as_Mem(),\n@@ -1437,5 +1483,0 @@\n-                         mem_ref_p.base(),\n-                         mem_ref_p.offset_in_bytes(),\n-                         mem_ref_p.invar(),\n-                         mem_ref_p.invar_factor(),\n-                         mem_ref_p.scale_in_bytes(),\n@@ -2614,1 +2655,1 @@\n-    VTransformVectorNode* vtn = vtnodes.at(i)->isa_Vector();\n+    VTransformMemVectorNode* vtn = vtnodes.at(i)->isa_MemVector();\n@@ -2616,2 +2657,1 @@\n-    MemNode* p0 = vtn->nodes().at(0)->isa_Mem();\n-    if (p0 == nullptr) { continue; }\n+    MemNode* p0 = vtn->nodes().at(0)->as_Mem();\n@@ -2663,2 +2703,2 @@\n-  const VPointer& align_to_ref_p = vpointer(align_to_ref);\n-  assert(align_to_ref_p.valid(), \"sanity\");\n+  const VPointer& p = vpointer(align_to_ref);\n+  assert(p.is_valid(), \"sanity\");\n@@ -2672,1 +2712,1 @@\n-  \/\/   adr = base + offset + invar + scale * iv                               (1)\n+  \/\/   adr = base + invar + iv_scale * iv + con                               (1)\n@@ -2689,2 +2729,2 @@\n-  \/\/   iv = new_limit = old_limit + adjust_pre_iter                           (3a, stride > 0)\n-  \/\/   iv = new_limit = old_limit - adjust_pre_iter                           (3b, stride < 0)\n+  \/\/   iv = new_limit = old_limit + adjust_pre_iter                           (3a, iv_stride > 0)\n+  \/\/   iv = new_limit = old_limit - adjust_pre_iter                           (3b, iv_stride < 0)\n@@ -2692,1 +2732,1 @@\n-  \/\/ We define boi as:\n+  \/\/ We define bic as:\n@@ -2694,1 +2734,1 @@\n-  \/\/   boi = base + offset + invar                                            (4)\n+  \/\/   bic = base + invar + con                                               (4)\n@@ -2698,3 +2738,3 @@\n-  \/\/   adr = boi + scale * new_limit\n-  \/\/   adr = boi + scale * (old_limit + adjust_pre_iter)                      (5a, stride > 0)\n-  \/\/   adr = boi + scale * (old_limit - adjust_pre_iter)                      (5b, stride < 0)\n+  \/\/   adr = bic + iv_scale * new_limit\n+  \/\/   adr = bic + iv_scale * (old_limit + adjust_pre_iter)                   (5a, iv_stride > 0)\n+  \/\/   adr = bic + iv_scale * (old_limit - adjust_pre_iter)                   (5b, iv_stride < 0)\n@@ -2704,2 +2744,2 @@\n-  \/\/   (boi + scale * (old_limit + adjust_pre_iter) % aw = 0                  (6a, stride > 0)\n-  \/\/   (boi + scale * (old_limit - adjust_pre_iter) % aw = 0                  (6b, stride < 0)\n+  \/\/   (bic + iv_scale * (old_limit + adjust_pre_iter) % aw = 0               (6a, iv_stride > 0)\n+  \/\/   (bic + iv_scale * (old_limit - adjust_pre_iter) % aw = 0               (6b, iv_stride < 0)\n@@ -2707,1 +2747,1 @@\n-  \/\/ In most cases, scale is the element size, for example:\n+  \/\/ In most cases, iv_scale is the element size, for example:\n@@ -2711,1 +2751,1 @@\n-  \/\/ It is thus reasonable to assume that both abs(scale) and abs(stride) are\n+  \/\/ It is thus reasonable to assume that both abs(iv_scale) and abs(iv_stride) are\n@@ -2716,2 +2756,2 @@\n-  \/\/ Further, if abs(scale) >= aw, then adjust_pre_iter has no effect on alignment, and\n-  \/\/ we are not able to affect the alignment at all. Hence, we require abs(scale) < aw.\n+  \/\/ Further, if abs(iv_scale) >= aw, then adjust_pre_iter has no effect on alignment, and\n+  \/\/ we are not able to affect the alignment at all. Hence, we require abs(iv_scale) < aw.\n@@ -2719,1 +2759,1 @@\n-  \/\/ Moreover, for alignment to be achievable, boi must be a multiple of scale. If strict\n+  \/\/ Moreover, for alignment to be achievable, bic must be a multiple of iv_scale. If strict\n@@ -2723,1 +2763,1 @@\n-  \/\/ In many cases boi will be a multiple of scale, but if it is not, then the adjustment\n+  \/\/ In many cases bic will be a multiple of iv_scale, but if it is not, then the adjustment\n@@ -2726,2 +2766,2 @@\n-  \/\/ Hence, in what follows we assume that boi is a multiple of scale, and in fact all\n-  \/\/ terms in (6) are multiples of scale. Therefore we divide all terms by scale:\n+  \/\/ Hence, in what follows we assume that bic is a multiple of iv_scale, and in fact all\n+  \/\/ terms in (6) are multiples of iv_scale. Therefore we divide all terms by iv_scale:\n@@ -2729,2 +2769,2 @@\n-  \/\/   AW = aw \/ abs(scale)            (power of 2)                           (7)\n-  \/\/   BOI = boi \/ abs(scale)                                                 (8)\n+  \/\/   AW = aw \/ abs(iv_scale)            (power of 2)                        (7)\n+  \/\/   BIC = bic \/ abs(iv_scale)                                              (8)\n@@ -2732,1 +2772,1 @@\n-  \/\/ and restate (6), using (7) and (8), i.e. we divide (6) by abs(scale):\n+  \/\/ and restate (6), using (7) and (8), i.e. we divide (6) by abs(iv_scale):\n@@ -2734,2 +2774,2 @@\n-  \/\/   (BOI + sign(scale) * (old_limit + adjust_pre_iter) % AW = 0           (9a, stride > 0)\n-  \/\/   (BOI + sign(scale) * (old_limit - adjust_pre_iter) % AW = 0           (9b, stride < 0)\n+  \/\/   (BIC + sign(iv_scale) * (old_limit + adjust_pre_iter) % AW = 0         (9a, iv_stride > 0)\n+  \/\/   (BIC + sign(iv_scale) * (old_limit - adjust_pre_iter) % AW = 0         (9b, iv_stride < 0)\n@@ -2737,1 +2777,1 @@\n-  \/\/   where: sign(scale) = scale \/ abs(scale) = (scale > 0 ? 1 : -1)\n+  \/\/   where: sign(iv_scale) = iv_scale \/ abs(iv_scale) = (iv_scale > 0 ? 1 : -1)\n@@ -2747,3 +2787,3 @@\n-  \/\/ Case A: scale > 0 && stride > 0 (i.e. sign(scale) =  1)\n-  \/\/   (BOI + old_limit + adjust_pre_iter) % AW = 0\n-  \/\/   adjust_pre_iter = (-BOI - old_limit) % AW                              (11a)\n+  \/\/ Case A: iv_scale > 0 && iv_stride > 0 (i.e. sign(iv_scale) =  1)\n+  \/\/   (BIC + old_limit + adjust_pre_iter) % AW = 0\n+  \/\/   adjust_pre_iter = (-BIC - old_limit) % AW                              (11a)\n@@ -2751,3 +2791,3 @@\n-  \/\/ Case B: scale < 0 && stride > 0 (i.e. sign(scale) = -1)\n-  \/\/   (BOI - old_limit - adjust_pre_iter) % AW = 0\n-  \/\/   adjust_pre_iter = (BOI - old_limit) % AW                               (11b)\n+  \/\/ Case B: iv_scale < 0 && iv_stride > 0 (i.e. sign(iv_scale) = -1)\n+  \/\/   (BIC - old_limit - adjust_pre_iter) % AW = 0\n+  \/\/   adjust_pre_iter = (BIC - old_limit) % AW                               (11b)\n@@ -2755,3 +2795,3 @@\n-  \/\/ Case C: scale > 0 && stride < 0 (i.e. sign(scale) =  1)\n-  \/\/   (BOI + old_limit - adjust_pre_iter) % AW = 0\n-  \/\/   adjust_pre_iter = (BOI + old_limit) % AW                               (11c)\n+  \/\/ Case C: iv_scale > 0 && iv_stride < 0 (i.e. sign(iv_scale) =  1)\n+  \/\/   (BIC + old_limit - adjust_pre_iter) % AW = 0\n+  \/\/   adjust_pre_iter = (BIC + old_limit) % AW                               (11c)\n@@ -2759,3 +2799,3 @@\n-  \/\/ Case D: scale < 0 && stride < 0 (i.e. sign(scale) = -1)\n-  \/\/   (BOI - old_limit + adjust_pre_iter) % AW = 0\n-  \/\/   adjust_pre_iter = (-BOI + old_limit) % AW                              (11d)\n+  \/\/ Case D: iv_scale < 0 && iv_stride < 0 (i.e. sign(iv_scale) = -1)\n+  \/\/   (BIC - old_limit + adjust_pre_iter) % AW = 0\n+  \/\/   adjust_pre_iter = (-BIC + old_limit) % AW                              (11d)\n@@ -2765,2 +2805,2 @@\n-  \/\/   OP:   (stride         > 0) ? SUB   : ADD\n-  \/\/   XBOI: (stride * scale > 0) ? -BOI  : BOI\n+  \/\/   OP:   (iv_stride            > 0) ?  SUB  : ADD\n+  \/\/   XBIC: (iv_stride * iv_scale > 0) ? -BIC  : BIC\n@@ -2770,1 +2810,1 @@\n-  \/\/   adjust_pre_iter = (XBOI OP old_limit) % AW                             (12)\n+  \/\/   adjust_pre_iter = (XBIC OP old_limit) % AW                             (12)\n@@ -2772,1 +2812,1 @@\n-  \/\/ We can construct XBOI by additionally defining:\n+  \/\/ We can construct XBIC by additionally defining:\n@@ -2774,1 +2814,1 @@\n-  \/\/   xboi = (stride * scale > 0) ? -boi              : boi                  (13)\n+  \/\/   xbic = (iv_stride * iv_scale > 0) ? -bic                 : bic         (13)\n@@ -2778,3 +2818,3 @@\n-  \/\/   XBOI = (stride * scale > 0) ? -BOI              : BOI\n-  \/\/        = (stride * scale > 0) ? -boi \/ abs(scale) : boi \/ abs(scale)\n-  \/\/        = xboi \/ abs(scale)                                               (14)\n+  \/\/   XBIC = (iv_stride * iv_scale > 0) ? -BIC                 : BIC\n+  \/\/        = (iv_stride * iv_scale > 0) ? -bic \/ abs(iv_scale) : bic \/ abs(iv_scale)\n+  \/\/        = xbic \/ abs(iv_scale)                                            (14)\n@@ -2789,1 +2829,1 @@\n-  \/\/                   = MIN(new_limit,                   orig_limit)         (15a, stride > 0)\n+  \/\/                   = MIN(new_limit,                   orig_limit)         (15a, iv_stride > 0)\n@@ -2791,1 +2831,1 @@\n-  \/\/                   = MAX(new_limit,                   orig_limit)         (15a, stride < 0)\n+  \/\/                   = MAX(new_limit,                   orig_limit)         (15a, iv_stride < 0)\n@@ -2793,5 +2833,5 @@\n-  const int stride   = iv_stride();\n-  const int scale    = align_to_ref_p.scale_in_bytes();\n-  const int offset   = align_to_ref_p.offset_in_bytes();\n-  Node* base         = align_to_ref_p.adr();\n-  Node* invar        = align_to_ref_p.invar();\n+  const int iv_stride = this->iv_stride();\n+  const int iv_scale  = p.iv_scale();\n+  const int con       = p.con();\n+  Node* base          = p.mem_pointer().base().object_or_native();\n+  bool is_base_native = p.mem_pointer().base().is_native();\n@@ -2804,4 +2844,6 @@\n-    tty->print_cr(\"  aw:       %d\", aw);\n-    tty->print_cr(\"  stride:   %d\", stride);\n-    tty->print_cr(\"  scale:    %d\", scale);\n-    tty->print_cr(\"  offset:   %d\", offset);\n+    tty->print(\"  \");\n+    p.print_on(tty);\n+    tty->print_cr(\"  aw:        %d\", aw);\n+    tty->print_cr(\"  iv_stride: %d\", iv_stride);\n+    tty->print_cr(\"  iv_scale:  %d\", iv_scale);\n+    tty->print_cr(\"  con:       %d\", con);\n@@ -2810,2 +2852,2 @@\n-    if (invar == nullptr) {\n-      tty->print_cr(\"  invar:     null\");\n+    if (!p.has_invar_summands()) {\n+      tty->print_cr(\"  invar:     none\");\n@@ -2813,2 +2855,6 @@\n-      tty->print(\"  invar:\");\n-      invar->dump();\n+      tty->print_cr(\"  invar_summands:\");\n+      p.for_each_invar_summand([&] (const MemPointerSummand& s) {\n+        tty->print(\"   -> \");\n+        s.print_on(tty);\n+      });\n+      tty->cr();\n@@ -2823,3 +2869,3 @@\n-  if (stride == 0 || !is_power_of_2(abs(stride)) ||\n-      scale  == 0 || !is_power_of_2(abs(scale))  ||\n-      abs(scale) >= aw) {\n+  if (iv_stride == 0 || !is_power_of_2(abs(iv_stride)) ||\n+      iv_scale  == 0 || !is_power_of_2(abs(iv_scale))  ||\n+      abs(iv_scale) >= aw) {\n@@ -2829,1 +2875,1 @@\n-      tty->print_cr(\" stride or scale are not power of 2, or abs(scale) >= aw.\");\n+      tty->print_cr(\" iv_stride or iv_scale are not power of 2, or abs(iv_scale) >= aw.\");\n@@ -2836,3 +2882,3 @@\n-  assert(stride != 0 && is_power_of_2(abs(stride)) &&\n-         scale  != 0 && is_power_of_2(abs(scale))  &&\n-         abs(scale) < aw, \"otherwise we cannot affect alignment with pre-loop\");\n+  assert(iv_stride != 0 && is_power_of_2(abs(iv_stride)) &&\n+         iv_scale  != 0 && is_power_of_2(abs(iv_scale))  &&\n+         abs(iv_scale) < aw, \"otherwise we cannot affect alignment with pre-loop\");\n@@ -2840,1 +2886,1 @@\n-  const int AW = aw \/ abs(scale);\n+  const int AW = aw \/ abs(iv_scale);\n@@ -2844,1 +2890,1 @@\n-    tty->print_cr(\"  AW = aw(%d) \/ abs(scale(%d)) = %d\", aw, scale, AW);\n+    tty->print_cr(\"  AW = aw(%d) \/ abs(iv_scale(%d)) = %d\", aw, iv_scale, AW);\n@@ -2849,11 +2895,14 @@\n-  \/\/    xboi = -boi = (-base - offset - invar)         (stride * scale > 0)\n-  \/\/    xboi = +boi = (+base + offset + invar)         (stride * scale < 0)\n-  const bool is_sub = scale * stride > 0;\n-\n-  \/\/ 1.1: offset\n-  Node* xboi = igvn().intcon(is_sub ? -offset : offset);\n-  TRACE_ALIGN_VECTOR_NODE(xboi);\n-\n-  \/\/ 1.2: invar (if it exists)\n-  if (invar != nullptr) {\n-    if (igvn().type(invar)->isa_long()) {\n+  \/\/    xbic = -bic = (-base - invar - con)         (iv_stride * iv_scale > 0)\n+  \/\/    xbic = +bic = (+base + invar + con)         (iv_stride * iv_scale < 0)\n+  const bool is_sub = iv_scale * iv_stride > 0;\n+\n+  \/\/ 1.1: con\n+  Node* xbic = igvn().intcon(is_sub ? -con : con);\n+  TRACE_ALIGN_VECTOR_NODE(xbic);\n+\n+  \/\/ 1.2: invar = SUM(invar_summands)\n+  \/\/      We iteratively add \/ subtract all invar_summands, if there are any.\n+  p.for_each_invar_summand([&] (const MemPointerSummand& s) {\n+    Node* invar_variable = s.variable();\n+    jint  invar_scale    = s.scale().value();\n+    if (igvn().type(invar_variable)->isa_long()) {\n@@ -2863,4 +2912,8 @@\n-      invar = new ConvL2INode(invar);\n-      phase()->register_new_node(invar, pre_ctrl);\n-      TRACE_ALIGN_VECTOR_NODE(invar);\n-   }\n+      invar_variable = new ConvL2INode(invar_variable);\n+      phase()->register_new_node(invar_variable, pre_ctrl);\n+      TRACE_ALIGN_VECTOR_NODE(invar_variable);\n+    }\n+    Node* invar_scale_con = igvn().intcon(invar_scale);\n+    Node* invar_summand = new MulINode(invar_variable, invar_scale_con);\n+    phase()->register_new_node(invar_summand, pre_ctrl);\n+    TRACE_ALIGN_VECTOR_NODE(invar_summand);\n@@ -2868,1 +2921,1 @@\n-      xboi = new SubINode(xboi, invar);\n+      xbic = new SubINode(xbic, invar_summand);\n@@ -2870,1 +2923,1 @@\n-      xboi = new AddINode(xboi, invar);\n+      xbic = new AddINode(xbic, invar_summand);\n@@ -2872,3 +2925,3 @@\n-    phase()->register_new_node(xboi, pre_ctrl);\n-    TRACE_ALIGN_VECTOR_NODE(xboi);\n-  }\n+    phase()->register_new_node(xbic, pre_ctrl);\n+    TRACE_ALIGN_VECTOR_NODE(xbic);\n+  });\n@@ -2877,12 +2930,24 @@\n-  if (aw > ObjectAlignmentInBytes || align_to_ref_p.base()->is_top()) {\n-    \/\/ The base is only aligned with ObjectAlignmentInBytes with arrays.\n-    \/\/ When the base() is top, we have no alignment guarantee at all.\n-    \/\/ Hence, we must now take the base into account for the calculation.\n-    Node* xbase = new CastP2XNode(nullptr, base);\n-    phase()->register_new_node(xbase, pre_ctrl);\n-    TRACE_ALIGN_VECTOR_NODE(xbase);\n-#ifdef _LP64\n-    xbase  = new ConvL2INode(xbase);\n-    phase()->register_new_node(xbase, pre_ctrl);\n-    TRACE_ALIGN_VECTOR_NODE(xbase);\n-#endif\n+  if (aw > ObjectAlignmentInBytes || is_base_native) {\n+    \/\/ For objects, the base is ObjectAlignmentInBytes aligned.\n+    \/\/ For native memory, we simply have a long that was cast to\n+    \/\/ a pointer via CastX2P, or if we parsed through the CastX2P\n+    \/\/ we only have a long. There is no alignment guarantee, and\n+    \/\/ we must always take the base into account for the calculation.\n+    \/\/\n+    \/\/ Computations are done % (vector width\/element size) so it's\n+    \/\/ safe to simply convert invar to an int and loose the upper 32\n+    \/\/ bit half. The base could be ptr, long or int. We cast all\n+    \/\/ to int.\n+    Node* xbase = base;\n+    if (igvn().type(xbase)->isa_ptr()) {\n+      \/\/ ptr -> int\/long\n+      xbase = new CastP2XNode(nullptr, xbase);\n+      phase()->register_new_node(xbase, pre_ctrl);\n+      TRACE_ALIGN_VECTOR_NODE(xbase);\n+    }\n+    if (igvn().type(xbase)->isa_long()) {\n+      \/\/ long -> int\n+      xbase  = new ConvL2INode(xbase);\n+      phase()->register_new_node(xbase, pre_ctrl);\n+      TRACE_ALIGN_VECTOR_NODE(xbase);\n+    }\n@@ -2890,1 +2955,1 @@\n-      xboi = new SubINode(xboi, xbase);\n+      xbic = new SubINode(xbic, xbase);\n@@ -2892,1 +2957,1 @@\n-      xboi = new AddINode(xboi, xbase);\n+      xbic = new AddINode(xbic, xbase);\n@@ -2894,2 +2959,2 @@\n-    phase()->register_new_node(xboi, pre_ctrl);\n-    TRACE_ALIGN_VECTOR_NODE(xboi);\n+    phase()->register_new_node(xbic, pre_ctrl);\n+    TRACE_ALIGN_VECTOR_NODE(xbic);\n@@ -2899,1 +2964,1 @@\n-  \/\/    XBOI = xboi \/ abs(scale)\n+  \/\/    XBIC = xbic \/ abs(iv_scale)\n@@ -2901,5 +2966,5 @@\n-  Node* log2_abs_scale = igvn().intcon(exact_log2(abs(scale)));\n-  Node* XBOI = new URShiftINode(xboi, log2_abs_scale);\n-  phase()->register_new_node(XBOI, pre_ctrl);\n-  TRACE_ALIGN_VECTOR_NODE(log2_abs_scale);\n-  TRACE_ALIGN_VECTOR_NODE(XBOI);\n+  Node* log2_abs_iv_scale = igvn().intcon(exact_log2(abs(iv_scale)));\n+  Node* XBIC = new URShiftINode(xbic, log2_abs_iv_scale);\n+  phase()->register_new_node(XBIC, pre_ctrl);\n+  TRACE_ALIGN_VECTOR_NODE(log2_abs_iv_scale);\n+  TRACE_ALIGN_VECTOR_NODE(XBIC);\n@@ -2908,1 +2973,1 @@\n-  \/\/    adjust_pre_iter = (XBOI OP old_limit) % AW\n+  \/\/    adjust_pre_iter = (XBIC OP old_limit) % AW\n@@ -2910,4 +2975,4 @@\n-  \/\/ 3.1: XBOI_OP_old_limit = XBOI OP old_limit\n-  Node* XBOI_OP_old_limit = nullptr;\n-  if (stride > 0) {\n-    XBOI_OP_old_limit = new SubINode(XBOI, old_limit);\n+  \/\/ 3.1: XBIC_OP_old_limit = XBIC OP old_limit\n+  Node* XBIC_OP_old_limit = nullptr;\n+  if (iv_stride > 0) {\n+    XBIC_OP_old_limit = new SubINode(XBIC, old_limit);\n@@ -2915,1 +2980,1 @@\n-    XBOI_OP_old_limit = new AddINode(XBOI, old_limit);\n+    XBIC_OP_old_limit = new AddINode(XBIC, old_limit);\n@@ -2917,2 +2982,2 @@\n-  phase()->register_new_node(XBOI_OP_old_limit, pre_ctrl);\n-  TRACE_ALIGN_VECTOR_NODE(XBOI_OP_old_limit);\n+  phase()->register_new_node(XBIC_OP_old_limit, pre_ctrl);\n+  TRACE_ALIGN_VECTOR_NODE(XBIC_OP_old_limit);\n@@ -2921,3 +2986,3 @@\n-  \/\/    adjust_pre_iter = (XBOI OP old_limit) % AW\n-  \/\/                    = XBOI_OP_old_limit % AW\n-  \/\/                    = XBOI_OP_old_limit AND (AW - 1)\n+  \/\/    adjust_pre_iter = (XBIC OP old_limit) % AW\n+  \/\/                    = XBIC_OP_old_limit % AW\n+  \/\/                    = XBIC_OP_old_limit AND (AW - 1)\n@@ -2927,1 +2992,1 @@\n-  Node* adjust_pre_iter = new AndINode(XBOI_OP_old_limit, mask_AW);\n+  Node* adjust_pre_iter = new AndINode(XBIC_OP_old_limit, mask_AW);\n@@ -2940,2 +3005,2 @@\n-  \/\/    pre-loop limit can only be increased (for stride > 0), but an add\n-  \/\/    overflow might decrease it, or decreased (for stride < 0), but a sub\n+  \/\/    pre-loop limit can only be increased (for iv_stride > 0), but an add\n+  \/\/    overflow might decrease it, or decreased (for iv_stride < 0), but a sub\n@@ -2955,2 +3020,2 @@\n-  \/\/    new_limit = old_limit + adjust_pre_iter     (stride > 0)\n-  \/\/    new_limit = old_limit - adjust_pre_iter     (stride < 0)\n+  \/\/    new_limit = old_limit + adjust_pre_iter     (iv_stride > 0)\n+  \/\/    new_limit = old_limit - adjust_pre_iter     (iv_stride < 0)\n@@ -2959,1 +3024,1 @@\n-  if (stride < 0) {\n+  if (iv_stride < 0) {\n@@ -2970,2 +3035,2 @@\n-    (stride > 0) ? (Node*) new MinLNode(phase()->C, new_limit, orig_limit)\n-                 : (Node*) new MaxLNode(phase()->C, new_limit, orig_limit);\n+    (iv_stride > 0) ? (Node*) new MinLNode(phase()->C, new_limit, orig_limit)\n+                    : (Node*) new MaxLNode(phase()->C, new_limit, orig_limit);\n","filename":"src\/hotspot\/share\/opto\/superword.cpp","additions":290,"deletions":225,"binary":false,"changes":515,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2007, 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2007, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -59,2 +59,0 @@\n-class VPointer;\n-\n@@ -477,1 +475,1 @@\n-  int data_size(Node* n) const {\n+  int data_size(const Node* n) const {\n@@ -566,0 +564,32 @@\n+  class MemOp : public StackObj {\n+  private:\n+    MemNode* _mem;\n+    const VPointer* _vpointer;\n+    int _original_index;\n+\n+  public:\n+    \/\/ Empty, for GrowableArray\n+    MemOp() :\n+      _mem(nullptr),\n+      _vpointer(nullptr),\n+      _original_index(-1) {}\n+    MemOp(MemNode* mem, const VPointer* vpointer, int original_index) :\n+      _mem(mem),\n+      _vpointer(vpointer),\n+      _original_index(original_index) {}\n+\n+    MemNode* mem() const { return _mem; }\n+    const VPointer& vpointer() const { return *_vpointer; }\n+    int original_index() const { return _original_index; }\n+\n+    static int cmp_by_group(MemOp* a, MemOp* b);\n+    static int cmp_by_group_and_con_and_original_index(MemOp* a, MemOp* b);\n+\n+    \/\/ We use two comparisons, because a subtraction could underflow.\n+    template <typename T>\n+    static int cmp_code(T a, T b) {\n+      if (a < b) { return -1; }\n+      if (a > b) { return  1; }\n+      return 0;\n+    }\n+  };\n@@ -567,4 +597,4 @@\n-  void collect_valid_vpointers(GrowableArray<const VPointer*>& vpointers);\n-  void create_adjacent_memop_pairs_in_all_groups(const GrowableArray<const VPointer*>& vpointers);\n-  static int find_group_end(const GrowableArray<const VPointer*>& vpointers, int group_start);\n-  void create_adjacent_memop_pairs_in_one_group(const GrowableArray<const VPointer*>& vpointers, const int group_start, int group_end);\n+  void collect_valid_memops(GrowableArray<MemOp>& memops);\n+  void create_adjacent_memop_pairs_in_all_groups(const GrowableArray<MemOp>& memops);\n+  static int find_group_end(const GrowableArray<MemOp>& memops, int group_start);\n+  void create_adjacent_memop_pairs_in_one_group(const GrowableArray<MemOp>& memops, const int group_start, int group_end);\n","filename":"src\/hotspot\/share\/opto\/superword.hpp","additions":38,"deletions":8,"binary":false,"changes":46,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2024, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -142,1 +142,3 @@\n-    vtn = new (_vtransform.arena()) VTransformLoadVectorNode(_vtransform, pack_size);\n+    const VPointer& scalar_p = _vloop_analyzer.vpointers().vpointer(p0->as_Load());\n+    const VPointer vector_p(scalar_p.make_with_size(scalar_p.size() * pack_size));\n+    vtn = new (_vtransform.arena()) VTransformLoadVectorNode(_vtransform, pack_size, vector_p);\n@@ -144,1 +146,3 @@\n-    vtn = new (_vtransform.arena()) VTransformStoreVectorNode(_vtransform, pack_size);\n+    const VPointer& scalar_p = _vloop_analyzer.vpointers().vpointer(p0->as_Store());\n+    const VPointer vector_p(scalar_p.make_with_size(scalar_p.size() * pack_size));\n+    vtn = new (_vtransform.arena()) VTransformStoreVectorNode(_vtransform, pack_size, vector_p);\n","filename":"src\/hotspot\/share\/opto\/superwordVTransformBuilder.cpp","additions":7,"deletions":3,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2024, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -32,1 +32,4 @@\n-  flags(POINTER_ANALYSIS,     \"Trace VPointer (verbose)\") \\\n+  flags(POINTER_PARSING,      \"Trace VPointer\/MemPointer parsing\") \\\n+  flags(POINTER_ALIASING,     \"Trace VPointer\/MemPointer aliasing\") \\\n+  flags(POINTER_ADJACENCY,    \"Trace VPointer\/MemPointer adjacency\") \\\n+  flags(POINTER_OVERLAP,      \"Trace VPointer\/MemPointer overlap\") \\\n","filename":"src\/hotspot\/share\/opto\/traceAutoVectorizationTag.hpp","additions":5,"deletions":2,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2024, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -34,3 +34,3 @@\n-    flags(POINTER,              \"Trace pointer IR\") \\\n-    flags(ALIASING,             \"Trace MemPointerSimpleForm::get_aliasing_with\") \\\n-    flags(ADJACENCY,            \"Trace adjacency\") \\\n+    flags(POINTER_PARSING,      \"Trace pointer IR\") \\\n+    flags(POINTER_ALIASING,     \"Trace MemPointerSimpleForm::get_aliasing_with\") \\\n+    flags(POINTER_ADJACENCY,    \"Trace adjacency\") \\\n","filename":"src\/hotspot\/share\/opto\/traceMergeStoresTag.hpp","additions":4,"deletions":4,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2023, 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2023, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -33,13 +33,0 @@\n-#ifndef PRODUCT\n-void VPointer::print_con_or_idx(const Node* n) {\n-  if (n == nullptr) {\n-    tty->print(\"(   0)\");\n-  } else if (n->is_ConI()) {\n-    jint val = n->as_ConI()->get_int();\n-    tty->print(\"(%4d)\", val);\n-  } else {\n-    tty->print(\"[%4d]\", n->_idx);\n-  }\n-}\n-#endif\n-\n@@ -198,2 +185,2 @@\n-  uint bytes = _vpointers_length * sizeof(VPointer);\n-  _vpointers = (VPointer*)_arena->Amalloc(bytes);\n+  uint bytes2 = _vpointers_length * sizeof(VPointer);\n+  _vpointers = (VPointer*)_arena->Amalloc(bytes2);\n@@ -227,1 +214,1 @@\n-    p.print();\n+    p.print_on(tty);\n@@ -268,1 +255,1 @@\n-        if (!VPointer::not_equal(p1.cmp(p2))) {\n+        if (!p1.never_overlaps_with(p2)) {\n@@ -409,951 +396,2 @@\n-int VPointer::Tracer::_depth = 0;\n-#endif\n-\n-VPointer::VPointer(MemNode* const mem, const VLoop& vloop,\n-                   Node_Stack* nstack, bool analyze_only) :\n-  _mem(mem), _vloop(vloop),\n-  _base(nullptr), _adr(nullptr), _scale(0), _offset(0), _invar(nullptr),\n-#ifdef ASSERT\n-  _debug_invar(nullptr), _debug_negate_invar(false), _debug_invar_scale(nullptr),\n-#endif\n-  _has_int_index_after_convI2L(false),\n-  _int_index_after_convI2L_offset(0),\n-  _int_index_after_convI2L_invar(nullptr),\n-  _int_index_after_convI2L_scale(0),\n-  _nstack(nstack), _analyze_only(analyze_only), _stack_idx(0)\n-#ifndef PRODUCT\n-  , _tracer(vloop.is_trace_pointer_analysis())\n-#endif\n-{\n-  NOT_PRODUCT(_tracer.ctor_1(mem);)\n-\n-  Node* adr = mem->in(MemNode::Address);\n-  if (!adr->is_AddP()) {\n-    assert(!valid(), \"too complex\");\n-    return;\n-  }\n-  \/\/ Match AddP(base, AddP(ptr, k*iv [+ invariant]), constant)\n-  Node* base = adr->in(AddPNode::Base);\n-  \/\/ The base address should be loop invariant\n-  if (is_loop_member(base)) {\n-    assert(!valid(), \"base address is loop variant\");\n-    return;\n-  }\n-  \/\/ unsafe references require misaligned vector access support\n-  if (base->is_top() && !Matcher::misaligned_vectors_ok()) {\n-    assert(!valid(), \"unsafe access\");\n-    return;\n-  }\n-\n-  NOT_PRODUCT(if(_tracer._is_trace_alignment) _tracer.store_depth();)\n-  NOT_PRODUCT(_tracer.ctor_2(adr);)\n-\n-  int i;\n-  for (i = 0; ; i++) {\n-    NOT_PRODUCT(_tracer.ctor_3(adr, i);)\n-\n-    if (!scaled_iv_plus_offset(adr->in(AddPNode::Offset))) {\n-      assert(!valid(), \"too complex\");\n-      return;\n-    }\n-    adr = adr->in(AddPNode::Address);\n-    NOT_PRODUCT(_tracer.ctor_4(adr, i);)\n-\n-    if (base == adr || !adr->is_AddP()) {\n-      NOT_PRODUCT(_tracer.ctor_5(adr, base, i);)\n-      break; \/\/ stop looking at addp's\n-    }\n-  }\n-  if (!invariant(adr)) {\n-    \/\/ The address must be invariant for the current loop. But if we are in a main-loop,\n-    \/\/ it must also be invariant of the pre-loop, otherwise we cannot use this address\n-    \/\/ for the pre-loop limit adjustment required for main-loop alignment.\n-    assert(!valid(), \"adr is loop variant\");\n-    return;\n-  }\n-\n-  if (!base->is_top() && adr != base) {\n-    assert(!valid(), \"adr and base differ\");\n-    return;\n-  }\n-\n-  NOT_PRODUCT(if(_tracer._is_trace_alignment) _tracer.restore_depth();)\n-  NOT_PRODUCT(_tracer.ctor_6(mem);)\n-\n-  \/\/ In the pointer analysis, and especially the AlignVector, analysis we assume that\n-  \/\/ stride and scale are not too large. For example, we multiply \"scale * stride\",\n-  \/\/ and assume that this does not overflow the int range. We also take \"abs(scale)\"\n-  \/\/ and \"abs(stride)\", which would overflow for min_int = -(2^31). Still, we want\n-  \/\/ to at least allow small and moderately large stride and scale. Therefore, we\n-  \/\/ allow values up to 2^30, which is only a factor 2 smaller than the max\/min int.\n-  \/\/ Normal performance relevant code will have much lower values. And the restriction\n-  \/\/ allows us to keep the rest of the autovectorization code much simpler, since we\n-  \/\/ do not have to deal with overflows.\n-  jlong long_scale  = _scale;\n-  jlong long_stride = _vloop.iv_stride();\n-  jlong max_val = 1 << 30;\n-  if (abs(long_scale) >= max_val ||\n-      abs(long_stride) >= max_val ||\n-      abs(long_scale * long_stride) >= max_val) {\n-    assert(!valid(), \"adr stride*scale is too large\");\n-    return;\n-  }\n-\n-  if (!is_safe_to_use_as_simple_form(base, adr)) {\n-    assert(!valid(), \"does not have simple form\");\n-    return;\n-  }\n-\n-  _base = base;\n-  _adr  = adr;\n-  assert(valid(), \"Usable\");\n-}\n-\n-\/\/ Following is used to create a temporary object during\n-\/\/ the pattern match of an address expression.\n-VPointer::VPointer(VPointer* p) :\n-  _mem(p->_mem), _vloop(p->_vloop),\n-  _base(nullptr), _adr(nullptr), _scale(0), _offset(0), _invar(nullptr),\n-#ifdef ASSERT\n-  _debug_invar(nullptr), _debug_negate_invar(false), _debug_invar_scale(nullptr),\n-#endif\n-  _has_int_index_after_convI2L(false),\n-  _int_index_after_convI2L_offset(0),\n-  _int_index_after_convI2L_invar(nullptr),\n-  _int_index_after_convI2L_scale(0),\n-  _nstack(p->_nstack), _analyze_only(p->_analyze_only), _stack_idx(p->_stack_idx)\n-#ifndef PRODUCT\n-  , _tracer(p->_tracer._is_trace_alignment)\n-#endif\n-{}\n-\n-\/\/ Biggest detectable factor of the invariant.\n-int VPointer::invar_factor() const {\n-  Node* n = invar();\n-  if (n == nullptr) {\n-    return 0;\n-  }\n-  int opc = n->Opcode();\n-  if (opc == Op_LShiftI && n->in(2)->is_Con()) {\n-    return 1 << n->in(2)->get_int();\n-  } else if (opc == Op_LShiftL && n->in(2)->is_Con()) {\n-    return 1 << n->in(2)->get_int();\n-  }\n-  \/\/ All our best-effort has failed.\n-  return 1;\n-}\n-\n-\/\/ We would like to make decisions about aliasing (i.e. removing memory edges) and adjacency\n-\/\/ (i.e. which loads\/stores can be packed) based on the simple form:\n-\/\/\n-\/\/   s_pointer = adr + offset + invar + scale * ConvI2L(iv)\n-\/\/\n-\/\/ However, we parse the compound-long-int form:\n-\/\/\n-\/\/   c_pointer = adr + long_offset + long_invar + long_scale * ConvI2L(int_index)\n-\/\/   int_index =       int_offset  + int_invar  + int_scale  * iv\n-\/\/\n-\/\/ In general, the simple and the compound-long-int form do not always compute the same pointer\n-\/\/ at runtime. For example, the simple form would give a different result due to an overflow\n-\/\/ in the int_index.\n-\/\/\n-\/\/ Example:\n-\/\/   For both forms, we have:\n-\/\/     iv = 0\n-\/\/     scale = 1\n-\/\/\n-\/\/   We now account the offset and invar once to the long part and once to the int part:\n-\/\/     Pointer 1 (long offset and long invar):\n-\/\/       long_offset = min_int\n-\/\/       long_invar  = min_int\n-\/\/       int_offset  = 0\n-\/\/       int_invar   = 0\n-\/\/\n-\/\/     Pointer 2 (int offset and int invar):\n-\/\/       long_offset = 0\n-\/\/       long_invar  = 0\n-\/\/       int_offset  = min_int\n-\/\/       int_invar   = min_int\n-\/\/\n-\/\/   This gives us the following pointers:\n-\/\/     Compound-long-int form pointers:\n-\/\/       Form:\n-\/\/         c_pointer   = adr + long_offset + long_invar + long_scale * ConvI2L(int_offset + int_invar + int_scale * iv)\n-\/\/\n-\/\/       Pointers:\n-\/\/         c_pointer1  = adr + min_int     + min_int    + 1          * ConvI2L(0          + 0         + 1         * 0)\n-\/\/                     = adr + min_int + min_int\n-\/\/                     = adr - 2^32\n-\/\/\n-\/\/         c_pointer2  = adr + 0           + 0          + 1          * ConvI2L(min_int    + min_int   + 1         * 0)\n-\/\/                     = adr + ConvI2L(min_int + min_int)\n-\/\/                     = adr + 0\n-\/\/                     = adr\n-\/\/\n-\/\/     Simple form pointers:\n-\/\/       Form:\n-\/\/         s_pointer  = adr + offset                     + invar                     + scale                    * ConvI2L(iv)\n-\/\/         s_pointer  = adr + (long_offset + int_offset) + (long_invar  + int_invar) + (long_scale * int_scale) * ConvI2L(iv)\n-\/\/\n-\/\/       Pointers:\n-\/\/         s_pointer1 = adr + (min_int     + 0         ) + (min_int     + 0        ) + 1                        * 0\n-\/\/                    = adr + min_int + min_int\n-\/\/                    = adr - 2^32\n-\/\/         s_pointer2 = adr + (0           + min_int   ) + (0           + min_int  ) + 1                        * 0\n-\/\/                    = adr + min_int + min_int\n-\/\/                    = adr - 2^32\n-\/\/\n-\/\/   We see that the two addresses are actually 2^32 bytes apart (derived from the c_pointers), but their simple form look identical.\n-\/\/\n-\/\/ Hence, we need to determine in which cases it is safe to make decisions based on the simple\n-\/\/ form, rather than the compound-long-int form. If we cannot prove that using the simple form\n-\/\/ is safe (i.e. equivalent to the compound-long-int form), then we do not get a valid VPointer,\n-\/\/ and the associated memop cannot be vectorized.\n-bool VPointer::is_safe_to_use_as_simple_form(Node* base, Node* adr) const {\n-#ifndef _LP64\n-  \/\/ On 32-bit platforms, there is never an explicit int_index with ConvI2L for the iv. Thus, the\n-  \/\/ parsed pointer form is always the simple form, with int operations:\n-  \/\/\n-  \/\/   pointer = adr + offset + invar + scale * iv\n-  \/\/\n-  assert(!_has_int_index_after_convI2L, \"32-bit never has an int_index with ConvI2L for the iv\");\n-  return true;\n-#else\n-\n-  \/\/ Array accesses that are not Unsafe always have a RangeCheck which ensures that there is no\n-  \/\/ int_index overflow. This implies that the conversion to long can be done separately:\n-  \/\/\n-  \/\/   ConvI2L(int_index) = ConvI2L(int_offset) + ConvI2L(int_invar) + ConvI2L(scale) * ConvI2L(iv)\n-  \/\/\n-  \/\/ And hence, the simple form is guaranteed to be identical to the compound-long-int form at\n-  \/\/ runtime and the VPointer is safe\/valid to be used.\n-  const TypeAryPtr* ary_ptr_t = _mem->adr_type()->isa_aryptr();\n-  if (ary_ptr_t != nullptr) {\n-    if (!_mem->is_unsafe_access()) {\n-      return true;\n-    }\n-  }\n-\n-  \/\/ We did not find the int_index. Just to be safe, reject this VPointer.\n-  if (!_has_int_index_after_convI2L) {\n-    return false;\n-  }\n-\n-  int int_offset  = _int_index_after_convI2L_offset;\n-  Node* int_invar = _int_index_after_convI2L_invar;\n-  int int_scale   = _int_index_after_convI2L_scale;\n-  int long_scale  = _scale \/ int_scale;\n-\n-  \/\/ If \"int_index = iv\", then the simple form is identical to the compound-long-int form.\n-  \/\/\n-  \/\/   int_index = int_offset + int_invar + int_scale * iv\n-  \/\/             = 0            0           1         * iv\n-  \/\/             =                                      iv\n-  if (int_offset == 0 && int_invar == nullptr && int_scale == 1) {\n-    return true;\n-  }\n-\n-  \/\/ Intuition: What happens if the int_index overflows? Let us look at two pointers on the \"overflow edge\":\n-  \/\/\n-  \/\/              pointer1 = adr + ConvI2L(int_index1)\n-  \/\/              pointer2 = adr + ConvI2L(int_index2)\n-  \/\/\n-  \/\/              int_index1 = max_int + 0 = max_int  -> very close to but before the overflow\n-  \/\/              int_index2 = max_int + 1 = min_int  -> just enough to get the overflow\n-  \/\/\n-  \/\/            When looking at the difference of pointer1 and pointer2, we notice that it is very large\n-  \/\/            (almost 2^32). Since arrays have at most 2^31 elements, chances are high that pointer2 is\n-  \/\/            an actual out-of-bounds access at runtime. These would normally be prevented by range checks\n-  \/\/            at runtime. However, if the access was done by using Unsafe, where range checks are omitted,\n-  \/\/            then an out-of-bounds access constitutes undefined behavior. This means that we are allowed to\n-  \/\/            do anything, including changing the behavior.\n-  \/\/\n-  \/\/            If we can set the right conditions, we have a guarantee that an overflow is either impossible\n-  \/\/            (no overflow or range checks preventing that) or undefined behavior. In both cases, we are\n-  \/\/            safe to do a vectorization.\n-  \/\/\n-  \/\/ Approach:  We want to prove a lower bound for the distance between these two pointers, and an\n-  \/\/            upper bound for the size of a memory object. We can derive such an upper bound for\n-  \/\/            arrays. We know they have at most 2^31 elements. If we know the size of the elements\n-  \/\/            in bytes, we have:\n-  \/\/\n-  \/\/              array_element_size_in_bytes * 2^31 >= max_possible_array_size_in_bytes\n-  \/\/                                                 >= array_size_in_bytes                      (ARR)\n-  \/\/\n-  \/\/            If some small difference \"delta\" leads to an int_index overflow, we know that the\n-  \/\/            int_index1 before overflow must have been close to max_int, and the int_index2 after\n-  \/\/            the overflow must be close to min_int:\n-  \/\/\n-  \/\/              pointer1 =        adr + long_offset + long_invar + long_scale * ConvI2L(int_index1)\n-  \/\/                       =approx  adr + long_offset + long_invar + long_scale * max_int\n-  \/\/\n-  \/\/              pointer2 =        adr + long_offset + long_invar + long_scale * ConvI2L(int_index2)\n-  \/\/                       =approx  adr + long_offset + long_invar + long_scale * min_int\n-  \/\/\n-  \/\/            We realize that the pointer difference is very large:\n-  \/\/\n-  \/\/              difference =approx  long_scale * 2^32\n-  \/\/\n-  \/\/            Hence, if we set the right condition for long_scale and array_element_size_in_bytes,\n-  \/\/            we can prove that an overflow is impossible (or would imply undefined behaviour).\n-  \/\/\n-  \/\/ We must now take this intuition, and develop a rigorous proof. We start by stating the problem\n-  \/\/ more precisely, with the help of some definitions and the Statement we are going to prove.\n-  \/\/\n-  \/\/ Definition:\n-  \/\/   Two VPointers are \"comparable\" (i.e. VPointer::comparable is true, set with VPointer::cmp()),\n-  \/\/   iff all of these conditions apply for the simple form:\n-  \/\/     1) Both VPointers are valid.\n-  \/\/     2) The adr are identical, or both are array bases of different arrays.\n-  \/\/     3) They have identical scale.\n-  \/\/     4) They have identical invar.\n-  \/\/     5) The difference in offsets is limited: abs(offset1 - offset2) < 2^31.                 (DIFF)\n-  \/\/\n-  \/\/ For the Vectorization Optimization, we pair-wise compare VPointers and determine if they are:\n-  \/\/   1) \"not comparable\":\n-  \/\/        We do not optimize them (assume they alias, not assume adjacency).\n-  \/\/\n-  \/\/        Whenever we chose this option based on the simple form, it is also correct based on the\n-  \/\/        compound-long-int form, since we make no optimizations based on it.\n-  \/\/\n-  \/\/   2) \"comparable\" with different array bases at runtime:\n-  \/\/        We assume they do not alias (remove memory edges), but not assume adjacency.\n-  \/\/\n-  \/\/        Whenever we have two different array bases for the simple form, we also have different\n-  \/\/        array bases for the compound-long-form. Since VPointers provably point to different\n-  \/\/        memory objects, they can never alias.\n-  \/\/\n-  \/\/   3) \"comparable\" with the same base address:\n-  \/\/        We compute the relative pointer difference, and based on the load\/store size we can\n-  \/\/        compute aliasing and adjacency.\n-  \/\/\n-  \/\/        We must find a condition under which the pointer difference of the simple form is\n-  \/\/        identical to the pointer difference of the compound-long-form. We do this with the\n-  \/\/        Statement below, which we then proceed to prove.\n-  \/\/\n-  \/\/ Statement:\n-  \/\/   If two VPointers satisfy these 3 conditions:\n-  \/\/     1) They are \"comparable\".\n-  \/\/     2) They have the same base address.\n-  \/\/     3) Their long_scale is a multiple of the array element size in bytes:\n-  \/\/\n-  \/\/          abs(long_scale) % array_element_size_in_bytes = 0                                     (A)\n-  \/\/\n-  \/\/   Then their pointer difference of the simple form is identical to the pointer difference\n-  \/\/   of the compound-long-int form.\n-  \/\/\n-  \/\/   More precisely:\n-  \/\/     Such two VPointers by definition have identical adr, invar, and scale.\n-  \/\/     Their simple form is:\n-  \/\/\n-  \/\/       s_pointer1 = adr + offset1 + invar + scale * ConvI2L(iv)                                 (B1)\n-  \/\/       s_pointer2 = adr + offset2 + invar + scale * ConvI2L(iv)                                 (B2)\n-  \/\/\n-  \/\/     Thus, the pointer difference of the simple forms collapses to the difference in offsets:\n-  \/\/\n-  \/\/       s_difference = s_pointer1 - s_pointer2 = offset1 - offset2                               (C)\n-  \/\/\n-  \/\/     Their compound-long-int form for these VPointer is:\n-  \/\/\n-  \/\/       c_pointer1 = adr + long_offset1 + long_invar1 + long_scale1 * ConvI2L(int_index1)        (D1)\n-  \/\/       int_index1 = int_offset1 + int_invar1 + int_scale1 * iv                                  (D2)\n-  \/\/\n-  \/\/       c_pointer2 = adr + long_offset2 + long_invar2 + long_scale2 * ConvI2L(int_index2)        (D3)\n-  \/\/       int_index2 = int_offset2 + int_invar2 + int_scale2 * iv                                  (D4)\n-  \/\/\n-  \/\/     And these are the offset1, offset2, invar and scale from the simple form (B1) and (B2):\n-  \/\/\n-  \/\/       offset1 = long_offset1 + long_scale1 * ConvI2L(int_offset1)                              (D5)\n-  \/\/       offset2 = long_offset2 + long_scale2 * ConvI2L(int_offset2)                              (D6)\n-  \/\/\n-  \/\/       invar   = long_invar1 + long_scale1 * ConvI2L(int_invar1)\n-  \/\/               = long_invar2 + long_scale2 * ConvI2L(int_invar2)                                (D7)\n-  \/\/\n-  \/\/       scale   = long_scale1 * ConvI2L(int_scale1)\n-  \/\/               = long_scale2 * ConvI2L(int_scale2)                                              (D8)\n-  \/\/\n-  \/\/     The pointer difference of the compound-long-int form is defined as:\n-  \/\/\n-  \/\/       c_difference = c_pointer1 - c_pointer2\n-  \/\/\n-  \/\/   Thus, the statement claims that for the two VPointer we have:\n-  \/\/\n-  \/\/     s_difference = c_difference                                                                (Statement)\n-  \/\/\n-  \/\/ We prove the Statement with the help of a Lemma:\n-  \/\/\n-  \/\/ Lemma:\n-  \/\/   There is some integer x, such that:\n-  \/\/\n-  \/\/     c_difference = s_difference + array_element_size_in_bytes * x * 2^32                       (Lemma)\n-  \/\/\n-  \/\/ From condition (DIFF), we can derive:\n-  \/\/\n-  \/\/   abs(s_difference) < 2^31                                                                     (E)\n-  \/\/\n-  \/\/ Assuming the Lemma, we prove the Statement:\n-  \/\/   If \"x = 0\" (intuitively: the int_index does not overflow), then:\n-  \/\/     c_difference = s_difference\n-  \/\/     and hence the simple form computes the same pointer difference as the compound-long-int form.\n-  \/\/   If \"x != 0\" (intuitively: the int_index overflows), then:\n-  \/\/     abs(c_difference) >= abs(s_difference + array_element_size_in_bytes * x * 2^32)\n-  \/\/                       >= array_element_size_in_bytes * 2^32 - abs(s_difference)\n-  \/\/                                                               --  apply (E)  --\n-  \/\/                       >  array_element_size_in_bytes * 2^32 - 2^31\n-  \/\/                       >= array_element_size_in_bytes * 2^31\n-  \/\/                              --  apply (ARR)  --\n-  \/\/                       >= max_possible_array_size_in_bytes\n-  \/\/                       >= array_size_in_bytes\n-  \/\/\n-  \/\/     This shows that c_pointer1 and c_pointer2 have a distance that exceeds the maximum array size.\n-  \/\/     Thus, at least one of the two pointers must be outside of the array bounds. But we can assume\n-  \/\/     that out-of-bounds accesses do not happen. If they still do, it is undefined behavior. Hence,\n-  \/\/     we are allowed to do anything. We can also \"safely\" use the simple form in this case even though\n-  \/\/     it might not match the compound-long-int form at runtime.\n-  \/\/ QED Statement.\n-  \/\/\n-  \/\/ We must now prove the Lemma.\n-  \/\/\n-  \/\/ ConvI2L always truncates by some power of 2^32, i.e. there is some integer y such that:\n-  \/\/\n-  \/\/   ConvI2L(y1 + y2) = ConvI2L(y1) + ConvI2L(y2) + 2^32 * y                                  (F)\n-  \/\/\n-  \/\/ It follows, that there is an integer y1 such that:\n-  \/\/\n-  \/\/   ConvI2L(int_index1) =  ConvI2L(int_offset1 + int_invar1 + int_scale1 * iv)\n-  \/\/                          -- apply (F) --\n-  \/\/                       =  ConvI2L(int_offset1)\n-  \/\/                        + ConvI2L(int_invar1)\n-  \/\/                        + ConvI2L(int_scale1) * ConvI2L(iv)\n-  \/\/                        + y1 * 2^32                                                         (G)\n-  \/\/\n-  \/\/ Thus, we can write the compound-long-int form (D1) as:\n-  \/\/\n-  \/\/   c_pointer1 =   adr + long_offset1 + long_invar1 + long_scale1 * ConvI2L(int_index1)\n-  \/\/                  -- apply (G) --\n-  \/\/              =   adr\n-  \/\/                + long_offset1\n-  \/\/                + long_invar1\n-  \/\/                + long_scale1 * ConvI2L(int_offset1)\n-  \/\/                + long_scale1 * ConvI2L(int_invar1)\n-  \/\/                + long_scale1 * ConvI2L(int_scale1) * ConvI2L(iv)\n-  \/\/                + long_scale1 * y1 * 2^32                                                    (H)\n-  \/\/\n-  \/\/ And we can write the simple form as:\n-  \/\/\n-  \/\/   s_pointer1 =   adr + offset1 + invar + scale * ConvI2L(iv)\n-  \/\/                  -- apply (D5, D7, D8) --\n-  \/\/              =   adr\n-  \/\/                + long_offset1\n-  \/\/                + long_scale1 * ConvI2L(int_offset1)\n-  \/\/                + long_invar1\n-  \/\/                + long_scale1 * ConvI2L(int_invar1)\n-  \/\/                + long_scale1 * ConvI2L(int_scale1) * ConvI2L(iv)                            (K)\n-  \/\/\n-  \/\/ We now compute the pointer difference between the simple (K) and compound-long-int form (H).\n-  \/\/ Most terms cancel out immediately:\n-  \/\/\n-  \/\/   sc_difference1 = c_pointer1 - s_pointer1 = long_scale1 * y1 * 2^32                        (L)\n-  \/\/\n-  \/\/ Rearranging the equation (L), we get:\n-  \/\/\n-  \/\/   c_pointer1 = s_pointer1 + long_scale1 * y1 * 2^32                                         (M)\n-  \/\/\n-  \/\/ And since long_scale1 is a multiple of array_element_size_in_bytes, there is some integer\n-  \/\/ x1, such that (M) implies:\n-  \/\/\n-  \/\/   c_pointer1 = s_pointer1 + array_element_size_in_bytes * x1 * 2^32                         (N)\n-  \/\/\n-  \/\/ With an analogue equation for c_pointer2, we can now compute the pointer difference for\n-  \/\/ the compound-long-int form:\n-  \/\/\n-  \/\/   c_difference =  c_pointer1 - c_pointer2\n-  \/\/                   -- apply (N) --\n-  \/\/                =  s_pointer1 + array_element_size_in_bytes * x1 * 2^32\n-  \/\/                 -(s_pointer2 + array_element_size_in_bytes * x2 * 2^32)\n-  \/\/                   -- where \"x = x1 - x2\" --\n-  \/\/                =  s_pointer1 - s_pointer2 + array_element_size_in_bytes * x * 2^32\n-  \/\/                   -- apply (C) --\n-  \/\/                =  s_difference            + array_element_size_in_bytes * x * 2^32\n-  \/\/ QED Lemma.\n-  if (ary_ptr_t != nullptr) {\n-    BasicType array_element_bt = ary_ptr_t->elem()->array_element_basic_type();\n-    if (is_java_primitive(array_element_bt)) {\n-      int array_element_size_in_bytes = type2aelembytes(array_element_bt);\n-      if (abs(long_scale) % array_element_size_in_bytes == 0) {\n-        return true;\n-      }\n-    }\n-  }\n-\n-  \/\/ General case: we do not know if it is safe to use the simple form.\n-  return false;\n-#endif\n-}\n-\n-bool VPointer::is_loop_member(Node* n) const {\n-  Node* n_c = phase()->get_ctrl(n);\n-  return lpt()->is_member(phase()->get_loop(n_c));\n-}\n-\n-bool VPointer::invariant(Node* n) const {\n-  NOT_PRODUCT(Tracer::Depth dd;)\n-  bool is_not_member = !is_loop_member(n);\n-  if (is_not_member) {\n-    CountedLoopNode* cl = lpt()->_head->as_CountedLoop();\n-    if (cl->is_main_loop()) {\n-      \/\/ Check that n_c dominates the pre loop head node. If it does not, then\n-      \/\/ we cannot use n as invariant for the pre loop CountedLoopEndNode check\n-      \/\/ because n_c is either part of the pre loop or between the pre and the\n-      \/\/ main loop (Illegal invariant happens when n_c is a CastII node that\n-      \/\/ prevents data nodes to flow above the main loop).\n-      Node* n_c = phase()->get_ctrl(n);\n-      return phase()->is_dominator(n_c, _vloop.pre_loop_head());\n-    }\n-  }\n-  return is_not_member;\n-}\n-\n-\/\/ Match: k*iv + offset\n-\/\/ where: k is a constant that maybe zero, and\n-\/\/        offset is (k2 [+\/- invariant]) where k2 maybe zero and invariant is optional\n-bool VPointer::scaled_iv_plus_offset(Node* n) {\n-  NOT_PRODUCT(Tracer::Depth ddd;)\n-  NOT_PRODUCT(_tracer.scaled_iv_plus_offset_1(n);)\n-\n-  if (scaled_iv(n)) {\n-    NOT_PRODUCT(_tracer.scaled_iv_plus_offset_2(n);)\n-    return true;\n-  }\n-\n-  if (offset_plus_k(n)) {\n-    NOT_PRODUCT(_tracer.scaled_iv_plus_offset_3(n);)\n-    return true;\n-  }\n-\n-  int opc = n->Opcode();\n-  if (opc == Op_AddI) {\n-    if (offset_plus_k(n->in(2)) && scaled_iv_plus_offset(n->in(1))) {\n-      NOT_PRODUCT(_tracer.scaled_iv_plus_offset_4(n);)\n-      return true;\n-    }\n-    if (offset_plus_k(n->in(1)) && scaled_iv_plus_offset(n->in(2))) {\n-      NOT_PRODUCT(_tracer.scaled_iv_plus_offset_5(n);)\n-      return true;\n-    }\n-  } else if (opc == Op_SubI || opc == Op_SubL) {\n-    if (offset_plus_k(n->in(2), true) && scaled_iv_plus_offset(n->in(1))) {\n-      \/\/ (offset1 + invar1 + scale * iv) - (offset2 + invar2)\n-      \/\/ Subtraction handled via \"negate\" flag of \"offset_plus_k\".\n-      NOT_PRODUCT(_tracer.scaled_iv_plus_offset_6(n);)\n-      return true;\n-    }\n-    VPointer tmp(this);\n-    if (offset_plus_k(n->in(1)) && tmp.scaled_iv_plus_offset(n->in(2))) {\n-      \/\/ (offset1 + invar1) - (offset2 + invar2 + scale * iv)\n-      \/\/ Subtraction handled explicitly below.\n-      assert(_scale == 0, \"shouldn't be set yet\");\n-      \/\/ _scale = -tmp._scale\n-      if (!try_MulI_no_overflow(-1, tmp._scale, _scale)) {\n-        return false; \/\/ mul overflow.\n-      }\n-      \/\/ _offset -= tmp._offset\n-      if (!try_SubI_no_overflow(_offset, tmp._offset, _offset)) {\n-        return false; \/\/ sub overflow.\n-      }\n-      \/\/ _invar -= tmp._invar\n-      if (tmp._invar != nullptr) {\n-        maybe_add_to_invar(tmp._invar, true);\n-#ifdef ASSERT\n-        _debug_invar_scale = tmp._debug_invar_scale;\n-        _debug_negate_invar = !tmp._debug_negate_invar;\n-#endif\n-      }\n-\n-      \/\/ Forward info about the int_index:\n-      assert(!_has_int_index_after_convI2L, \"no previous int_index discovered\");\n-      _has_int_index_after_convI2L = tmp._has_int_index_after_convI2L;\n-      _int_index_after_convI2L_offset = tmp._int_index_after_convI2L_offset;\n-      _int_index_after_convI2L_invar  = tmp._int_index_after_convI2L_invar;\n-      _int_index_after_convI2L_scale  = tmp._int_index_after_convI2L_scale;\n-\n-      NOT_PRODUCT(_tracer.scaled_iv_plus_offset_7(n);)\n-      return true;\n-    }\n-  }\n-\n-  NOT_PRODUCT(_tracer.scaled_iv_plus_offset_8(n);)\n-  return false;\n-}\n-\n-\/\/ Match: k*iv where k is a constant that's not zero\n-bool VPointer::scaled_iv(Node* n) {\n-  NOT_PRODUCT(Tracer::Depth ddd;)\n-  NOT_PRODUCT(_tracer.scaled_iv_1(n);)\n-\n-  if (_scale != 0) { \/\/ already found a scale\n-    NOT_PRODUCT(_tracer.scaled_iv_2(n, _scale);)\n-    return false;\n-  }\n-\n-  if (n == iv()) {\n-    _scale = 1;\n-    NOT_PRODUCT(_tracer.scaled_iv_3(n, _scale);)\n-    return true;\n-  }\n-  if (_analyze_only && (is_loop_member(n))) {\n-    _nstack->push(n, _stack_idx++);\n-  }\n-\n-  int opc = n->Opcode();\n-  if (opc == Op_MulI) {\n-    if (n->in(1) == iv() && n->in(2)->is_Con()) {\n-      _scale = n->in(2)->get_int();\n-      NOT_PRODUCT(_tracer.scaled_iv_4(n, _scale);)\n-      return true;\n-    } else if (n->in(2) == iv() && n->in(1)->is_Con()) {\n-      _scale = n->in(1)->get_int();\n-      NOT_PRODUCT(_tracer.scaled_iv_5(n, _scale);)\n-      return true;\n-    }\n-  } else if (opc == Op_LShiftI) {\n-    if (n->in(1) == iv() && n->in(2)->is_Con()) {\n-      if (!try_LShiftI_no_overflow(1, n->in(2)->get_int(), _scale)) {\n-        return false; \/\/ shift overflow.\n-      }\n-      NOT_PRODUCT(_tracer.scaled_iv_6(n, _scale);)\n-      return true;\n-    }\n-  } else if (opc == Op_ConvI2L && !has_iv()) {\n-    \/\/ So far we have not found the iv yet, and are about to enter a ConvI2L subgraph,\n-    \/\/ which may be the int index (that might overflow) for the memory access, of the form:\n-    \/\/\n-    \/\/   int_index = int_offset + int_invar + int_scale * iv\n-    \/\/\n-    \/\/ If we simply continue parsing with the current VPointer, then the int_offset and\n-    \/\/ int_invar simply get added to the long offset and invar. But for the checks in\n-    \/\/ VPointer::is_safe_to_use_as_simple_form() we need to have explicit access to the\n-    \/\/ int_index. Thus, we must parse it explicitly here. For this, we use a temporary\n-    \/\/ VPointer, to pattern match the int_index sub-expression of the address.\n-\n-    NOT_PRODUCT(Tracer::Depth dddd;)\n-    VPointer tmp(this);\n-    NOT_PRODUCT(_tracer.scaled_iv_8(n, &tmp);)\n-\n-    if (tmp.scaled_iv_plus_offset(n->in(1)) && tmp.has_iv()) {\n-      \/\/ We successfully matched an integer index, of the form:\n-      \/\/   int_index = int_offset + int_invar + int_scale * iv\n-      \/\/ Forward scale.\n-      assert(_scale == 0 && tmp._scale != 0, \"iv only found just now\");\n-      _scale = tmp._scale;\n-      \/\/ Accumulate offset.\n-      if (!try_AddI_no_overflow(_offset, tmp._offset, _offset)) {\n-        return false; \/\/ add overflow.\n-      }\n-      \/\/ Accumulate invar.\n-      if (tmp._invar != nullptr) {\n-        maybe_add_to_invar(tmp._invar, false);\n-      }\n-      \/\/ Set info about the int_index:\n-      assert(!_has_int_index_after_convI2L, \"no previous int_index discovered\");\n-      _has_int_index_after_convI2L = true;\n-      _int_index_after_convI2L_offset = tmp._offset;\n-      _int_index_after_convI2L_invar  = tmp._invar;\n-      _int_index_after_convI2L_scale  = tmp._scale;\n-\n-      NOT_PRODUCT(_tracer.scaled_iv_7(n);)\n-      return true;\n-    }\n-  } else if (opc == Op_ConvI2L || opc == Op_CastII) {\n-    if (scaled_iv_plus_offset(n->in(1))) {\n-      NOT_PRODUCT(_tracer.scaled_iv_7(n);)\n-      return true;\n-    }\n-  } else if (opc == Op_LShiftL && n->in(2)->is_Con()) {\n-    if (!has_iv()) {\n-      \/\/ Need to preserve the current _offset value, so\n-      \/\/ create a temporary object for this expression subtree.\n-      \/\/ Hacky, so should re-engineer the address pattern match.\n-      NOT_PRODUCT(Tracer::Depth dddd;)\n-      VPointer tmp(this);\n-      NOT_PRODUCT(_tracer.scaled_iv_8(n, &tmp);)\n-\n-      if (tmp.scaled_iv_plus_offset(n->in(1))) {\n-        int shift = n->in(2)->get_int();\n-        \/\/ Accumulate scale.\n-        if (!try_LShiftI_no_overflow(tmp._scale, shift, _scale)) {\n-          return false; \/\/ shift overflow.\n-        }\n-        \/\/ Accumulate offset.\n-        int shifted_offset = 0;\n-        if (!try_LShiftI_no_overflow(tmp._offset, shift, shifted_offset)) {\n-          return false; \/\/ shift overflow.\n-        }\n-        if (!try_AddI_no_overflow(_offset, shifted_offset, _offset)) {\n-          return false; \/\/ add overflow.\n-        }\n-        \/\/ Accumulate invar.\n-        if (tmp._invar != nullptr) {\n-          BasicType bt = tmp._invar->bottom_type()->basic_type();\n-          assert(bt == T_INT || bt == T_LONG, \"\");\n-          maybe_add_to_invar(register_if_new(LShiftNode::make(tmp._invar, n->in(2), bt)), false);\n-#ifdef ASSERT\n-          _debug_invar_scale = n->in(2);\n-#endif\n-        }\n-\n-        \/\/ Forward info about the int_index:\n-        assert(!_has_int_index_after_convI2L, \"no previous int_index discovered\");\n-        _has_int_index_after_convI2L = tmp._has_int_index_after_convI2L;\n-        _int_index_after_convI2L_offset = tmp._int_index_after_convI2L_offset;\n-        _int_index_after_convI2L_invar  = tmp._int_index_after_convI2L_invar;\n-        _int_index_after_convI2L_scale  = tmp._int_index_after_convI2L_scale;\n-\n-        NOT_PRODUCT(_tracer.scaled_iv_9(n, _scale, _offset, _invar);)\n-        return true;\n-      }\n-    }\n-  }\n-  NOT_PRODUCT(_tracer.scaled_iv_10(n);)\n-  return false;\n-}\n-\n-\/\/ Match: offset is (k [+\/- invariant])\n-\/\/ where k maybe zero and invariant is optional, but not both.\n-bool VPointer::offset_plus_k(Node* n, bool negate) {\n-  NOT_PRODUCT(Tracer::Depth ddd;)\n-  NOT_PRODUCT(_tracer.offset_plus_k_1(n);)\n-\n-  int opc = n->Opcode();\n-  if (opc == Op_ConI) {\n-    if (!try_AddSubI_no_overflow(_offset, n->get_int(), negate, _offset)) {\n-      return false; \/\/ add\/sub overflow.\n-    }\n-    NOT_PRODUCT(_tracer.offset_plus_k_2(n, _offset);)\n-    return true;\n-  } else if (opc == Op_ConL) {\n-    \/\/ Okay if value fits into an int\n-    const TypeLong* t = n->find_long_type();\n-    if (t->higher_equal(TypeLong::INT)) {\n-      jlong loff = n->get_long();\n-      jint  off  = (jint)loff;\n-      if (!try_AddSubI_no_overflow(_offset, off, negate, _offset)) {\n-        return false; \/\/ add\/sub overflow.\n-      }\n-      NOT_PRODUCT(_tracer.offset_plus_k_3(n, _offset);)\n-      return true;\n-    }\n-    NOT_PRODUCT(_tracer.offset_plus_k_4(n);)\n-    return false;\n-  }\n-  assert((_debug_invar == nullptr) == (_invar == nullptr), \"\");\n-\n-  if (_analyze_only && is_loop_member(n)) {\n-    _nstack->push(n, _stack_idx++);\n-  }\n-  if (opc == Op_AddI) {\n-    if (n->in(2)->is_Con() && invariant(n->in(1))) {\n-      maybe_add_to_invar(n->in(1), negate);\n-      if (!try_AddSubI_no_overflow(_offset, n->in(2)->get_int(), negate, _offset)) {\n-        return false; \/\/ add\/sub overflow.\n-      }\n-      NOT_PRODUCT(_tracer.offset_plus_k_6(n, _invar, negate, _offset);)\n-      return true;\n-    } else if (n->in(1)->is_Con() && invariant(n->in(2))) {\n-      if (!try_AddSubI_no_overflow(_offset, n->in(1)->get_int(), negate, _offset)) {\n-        return false; \/\/ add\/sub overflow.\n-      }\n-      maybe_add_to_invar(n->in(2), negate);\n-      NOT_PRODUCT(_tracer.offset_plus_k_7(n, _invar, negate, _offset);)\n-      return true;\n-    }\n-  }\n-  if (opc == Op_SubI) {\n-    if (n->in(2)->is_Con() && invariant(n->in(1))) {\n-      maybe_add_to_invar(n->in(1), negate);\n-      if (!try_AddSubI_no_overflow(_offset, n->in(2)->get_int(), !negate, _offset)) {\n-        return false; \/\/ add\/sub overflow.\n-      }\n-      NOT_PRODUCT(_tracer.offset_plus_k_8(n, _invar, negate, _offset);)\n-      return true;\n-    } else if (n->in(1)->is_Con() && invariant(n->in(2))) {\n-      if (!try_AddSubI_no_overflow(_offset, n->in(1)->get_int(), negate, _offset)) {\n-        return false; \/\/ add\/sub overflow.\n-      }\n-      maybe_add_to_invar(n->in(2), !negate);\n-      NOT_PRODUCT(_tracer.offset_plus_k_9(n, _invar, !negate, _offset);)\n-      return true;\n-    }\n-  }\n-\n-  if (!is_loop_member(n)) {\n-    \/\/ 'n' is loop invariant. Skip ConvI2L and CastII nodes before checking if 'n' is dominating the pre loop.\n-    if (opc == Op_ConvI2L) {\n-      n = n->in(1);\n-    }\n-    if (n->Opcode() == Op_CastII) {\n-      \/\/ Skip CastII nodes\n-      assert(!is_loop_member(n), \"sanity\");\n-      n = n->in(1);\n-    }\n-    \/\/ Check if 'n' can really be used as invariant (not in main loop and dominating the pre loop).\n-    if (invariant(n)) {\n-      maybe_add_to_invar(n, negate);\n-      NOT_PRODUCT(_tracer.offset_plus_k_10(n, _invar, negate, _offset);)\n-      return true;\n-    }\n-  }\n-\n-  NOT_PRODUCT(_tracer.offset_plus_k_11(n);)\n-  return false;\n-}\n-\n-Node* VPointer::maybe_negate_invar(bool negate, Node* invar) {\n-#ifdef ASSERT\n-  _debug_negate_invar = negate;\n-#endif\n-  if (negate) {\n-    BasicType bt = invar->bottom_type()->basic_type();\n-    assert(bt == T_INT || bt == T_LONG, \"\");\n-    Node* zero = phase()->zerocon(bt);\n-    Node* sub = SubNode::make(zero, invar, bt);\n-    invar = register_if_new(sub);\n-  }\n-  return invar;\n-}\n-\n-Node* VPointer::register_if_new(Node* n) const {\n-  PhaseIterGVN& igvn = phase()->igvn();\n-  Node* prev = igvn.hash_find_insert(n);\n-  if (prev != nullptr) {\n-    n->destruct(&igvn);\n-    n = prev;\n-  } else {\n-    Node* c = phase()->get_early_ctrl(n);\n-    phase()->register_new_node(n, c);\n-  }\n-  return n;\n-}\n-\n-void VPointer::maybe_add_to_invar(Node* new_invar, bool negate) {\n-  new_invar = maybe_negate_invar(negate, new_invar);\n-  if (_invar == nullptr) {\n-    _invar = new_invar;\n-#ifdef ASSERT\n-    _debug_invar = new_invar;\n-#endif\n-    return;\n-  }\n-#ifdef ASSERT\n-  _debug_invar = NodeSentinel;\n-#endif\n-  BasicType new_invar_bt = new_invar->bottom_type()->basic_type();\n-  assert(new_invar_bt == T_INT || new_invar_bt == T_LONG, \"\");\n-  BasicType invar_bt = _invar->bottom_type()->basic_type();\n-  assert(invar_bt == T_INT || invar_bt == T_LONG, \"\");\n-\n-  BasicType bt = (new_invar_bt == T_LONG || invar_bt == T_LONG) ? T_LONG : T_INT;\n-  Node* current_invar = _invar;\n-  if (invar_bt != bt) {\n-    assert(bt == T_LONG && invar_bt == T_INT, \"\");\n-    assert(new_invar_bt == bt, \"\");\n-    current_invar = register_if_new(new ConvI2LNode(current_invar));\n-  } else if (new_invar_bt != bt) {\n-    assert(bt == T_LONG && new_invar_bt == T_INT, \"\");\n-    assert(invar_bt == bt, \"\");\n-    new_invar = register_if_new(new ConvI2LNode(new_invar));\n-  }\n-  Node* add = AddNode::make(current_invar, new_invar, bt);\n-  _invar = register_if_new(add);\n-}\n-\n-bool VPointer::try_AddI_no_overflow(int offset1, int offset2, int& result) {\n-  jlong long_offset = java_add((jlong)(offset1), (jlong)(offset2));\n-  jint  int_offset  = java_add(        offset1,          offset2);\n-  if (long_offset != int_offset) {\n-    return false;\n-  }\n-  result = int_offset;\n-  return true;\n-}\n-\n-bool VPointer::try_SubI_no_overflow(int offset1, int offset2, int& result) {\n-  jlong long_offset = java_subtract((jlong)(offset1), (jlong)(offset2));\n-  jint  int_offset  = java_subtract(        offset1,          offset2);\n-  if (long_offset != int_offset) {\n-    return false;\n-  }\n-  result = int_offset;\n-  return true;\n-}\n-\n-bool VPointer::try_AddSubI_no_overflow(int offset1, int offset2, bool is_sub, int& result) {\n-  if (is_sub) {\n-    return try_SubI_no_overflow(offset1, offset2, result);\n-  } else {\n-    return try_AddI_no_overflow(offset1, offset2, result);\n-  }\n-}\n-\n-bool VPointer::try_LShiftI_no_overflow(int offset, int shift, int& result) {\n-  if (shift < 0 || shift > 31) {\n-    return false;\n-  }\n-  jlong long_offset = java_shift_left((jlong)(offset), shift);\n-  jint  int_offset  = java_shift_left(        offset,  shift);\n-  if (long_offset != int_offset) {\n-    return false;\n-  }\n-  result = int_offset;\n-  return true;\n-}\n-\n-bool VPointer::try_MulI_no_overflow(int offset1, int offset2, int& result) {\n-  jlong long_offset = java_multiply((jlong)(offset1), (jlong)(offset2));\n-  jint  int_offset  = java_multiply(        offset1,          offset2);\n-  if (long_offset != int_offset) {\n-    return false;\n-  }\n-  result = int_offset;\n-  return true;\n-}\n-\n-\/\/ We use two comparisons, because a subtraction could underflow.\n-#define RETURN_CMP_VALUE_IF_NOT_EQUAL(a, b) \\\n-  if (a < b) { return -1; }                 \\\n-  if (a > b) { return  1; }\n-\n-\/\/ To be in the same group, two VPointers must be the same,\n-\/\/ except for the offset.\n-int VPointer::cmp_for_sort_by_group(const VPointer** p1, const VPointer** p2) {\n-  const VPointer* a = *p1;\n-  const VPointer* b = *p2;\n-\n-  RETURN_CMP_VALUE_IF_NOT_EQUAL(a->base()->_idx,     b->base()->_idx);\n-  RETURN_CMP_VALUE_IF_NOT_EQUAL(a->mem()->Opcode(),  b->mem()->Opcode());\n-  RETURN_CMP_VALUE_IF_NOT_EQUAL(a->scale_in_bytes(), b->scale_in_bytes());\n-\n-  int a_inva_idx = a->invar() == nullptr ? 0 : a->invar()->_idx;\n-  int b_inva_idx = b->invar() == nullptr ? 0 : b->invar()->_idx;\n-  RETURN_CMP_VALUE_IF_NOT_EQUAL(a_inva_idx,          b_inva_idx);\n-\n-  return 0; \/\/ equal\n-}\n-\n-\/\/ We compare by group, then by offset, and finally by node idx.\n-int VPointer::cmp_for_sort(const VPointer** p1, const VPointer** p2) {\n-  int cmp_group = cmp_for_sort_by_group(p1, p2);\n-  if (cmp_group != 0) { return cmp_group; }\n-\n-  const VPointer* a = *p1;\n-  const VPointer* b = *p2;\n-\n-  RETURN_CMP_VALUE_IF_NOT_EQUAL(a->offset_in_bytes(), b->offset_in_bytes());\n-  RETURN_CMP_VALUE_IF_NOT_EQUAL(a->mem()->_idx,       b->mem()->_idx);\n-  return 0; \/\/ equal\n-}\n-\n-#ifndef PRODUCT\n-\/\/ Function for printing the fields of a VPointer\n-void VPointer::print() const {\n-  tty->print(\"VPointer[mem: %4d %10s, \", _mem->_idx, _mem->Name());\n+void VPointer::print_on(outputStream* st, bool end_with_cr) const {\n+  st->print(\"VPointer[\");\n@@ -1361,2 +399,2 @@\n-  if (!valid()) {\n-    tty->print_cr(\"invalid]\");\n+  if (!is_valid()) {\n+    st->print_cr(\"invalid]\");\n@@ -1366,28 +404,2 @@\n-  tty->print(\"base: %4d, \", _base != nullptr ? _base->_idx : 0);\n-  tty->print(\"adr: %4d, \", _adr != nullptr ? _adr->_idx : 0);\n-\n-  tty->print(\" base\");\n-  VPointer::print_con_or_idx(_base);\n-\n-  tty->print(\" + offset(%4d)\", _offset);\n-\n-  tty->print(\" + invar\");\n-  VPointer::print_con_or_idx(_invar);\n-\n-  tty->print_cr(\" + scale(%4d) * iv]\", _scale);\n-}\n-#endif\n-\n-\/\/ Following are functions for tracing VPointer match\n-#ifndef PRODUCT\n-void VPointer::Tracer::print_depth() const {\n-  for (int ii = 0; ii < _depth; ++ii) {\n-    tty->print(\"  \");\n-  }\n-}\n-\n-void VPointer::Tracer::ctor_1(const Node* mem) {\n-  if (_is_trace_alignment) {\n-    print_depth(); tty->print(\" %d VPointer::VPointer: start alignment analysis\", mem->_idx); mem->dump();\n-  }\n-}\n+  st->print(\"size: %2d, %s, \", size(),\n+            _mem_pointer.base().is_object() ? \"object\" : \"native\");\n@@ -1395,9 +407,5 @@\n-void VPointer::Tracer::ctor_2(Node* adr) {\n-  if (_is_trace_alignment) {\n-    \/\/store_depth();\n-    inc_depth();\n-    print_depth(); tty->print(\" %d (adr) VPointer::VPointer: \", adr->_idx); adr->dump();\n-    inc_depth();\n-    print_depth(); tty->print(\" %d (base) VPointer::VPointer: \", adr->in(AddPNode::Base)->_idx); adr->in(AddPNode::Base)->dump();\n-  }\n-}\n+  Node* base = _mem_pointer.base().object_or_native();\n+  tty->print(\"base(%d %s) + con(%3d) + iv_scale(%3d) * iv + invar(\",\n+             base->_idx, base->Name(),\n+             _mem_pointer.con().value(),\n+             _iv_scale);\n@@ -1405,22 +413,4 @@\n-void VPointer::Tracer::ctor_3(Node* adr, int i) {\n-  if (_is_trace_alignment) {\n-    inc_depth();\n-    Node* offset = adr->in(AddPNode::Offset);\n-    print_depth(); tty->print(\" %d (offset) VPointer::VPointer: i = %d: \", offset->_idx, i); offset->dump();\n-  }\n-}\n-\n-void VPointer::Tracer::ctor_4(Node* adr, int i) {\n-  if (_is_trace_alignment) {\n-    inc_depth();\n-    print_depth(); tty->print(\" %d (adr) VPointer::VPointer: i = %d: \", adr->_idx, i); adr->dump();\n-  }\n-}\n-\n-void VPointer::Tracer::ctor_5(Node* adr, Node* base, int i) {\n-  if (_is_trace_alignment) {\n-    inc_depth();\n-    if (base == adr) {\n-      print_depth(); tty->print_cr(\"  \\\\ %d (adr) == %d (base) VPointer::VPointer: breaking analysis at i = %d\", adr->_idx, base->_idx, i);\n-    } else if (!adr->is_AddP()) {\n-      print_depth(); tty->print_cr(\"  \\\\ %d (adr) is NOT Addp VPointer::VPointer: breaking analysis at i = %d\", adr->_idx, i);\n+  int count = 0;\n+  for_each_invar_summand([&] (const MemPointerSummand& s) {\n+    if (count > 0) {\n+      st->print(\" + \");\n@@ -1428,0 +418,5 @@\n+    s.print_on(tty);\n+    count++;\n+  });\n+  if (count == 0) {\n+    st->print(\"0\");\n@@ -1429,227 +424,2 @@\n-}\n-\n-void VPointer::Tracer::ctor_6(const Node* mem) {\n-  if (_is_trace_alignment) {\n-    \/\/restore_depth();\n-    print_depth(); tty->print_cr(\" %d (adr) VPointer::VPointer: stop analysis\", mem->_idx);\n-  }\n-}\n-\n-void VPointer::Tracer::scaled_iv_plus_offset_1(Node* n) {\n-  if (_is_trace_alignment) {\n-    print_depth(); tty->print(\" %d VPointer::scaled_iv_plus_offset testing node: \", n->_idx);\n-    n->dump();\n-  }\n-}\n-\n-void VPointer::Tracer::scaled_iv_plus_offset_2(Node* n) {\n-  if (_is_trace_alignment) {\n-    print_depth(); tty->print_cr(\" %d VPointer::scaled_iv_plus_offset: PASSED\", n->_idx);\n-  }\n-}\n-\n-void VPointer::Tracer::scaled_iv_plus_offset_3(Node* n) {\n-  if (_is_trace_alignment) {\n-    print_depth(); tty->print_cr(\" %d VPointer::scaled_iv_plus_offset: PASSED\", n->_idx);\n-  }\n-}\n-\n-void VPointer::Tracer::scaled_iv_plus_offset_4(Node* n) {\n-  if (_is_trace_alignment) {\n-    print_depth(); tty->print_cr(\" %d VPointer::scaled_iv_plus_offset: Op_AddI PASSED\", n->_idx);\n-    print_depth(); tty->print(\"  \\\\ %d VPointer::scaled_iv_plus_offset: in(1) is scaled_iv: \", n->in(1)->_idx); n->in(1)->dump();\n-    print_depth(); tty->print(\"  \\\\ %d VPointer::scaled_iv_plus_offset: in(2) is offset_plus_k: \", n->in(2)->_idx); n->in(2)->dump();\n-  }\n-}\n-\n-void VPointer::Tracer::scaled_iv_plus_offset_5(Node* n) {\n-  if (_is_trace_alignment) {\n-    print_depth(); tty->print_cr(\" %d VPointer::scaled_iv_plus_offset: Op_AddI PASSED\", n->_idx);\n-    print_depth(); tty->print(\"  \\\\ %d VPointer::scaled_iv_plus_offset: in(2) is scaled_iv: \", n->in(2)->_idx); n->in(2)->dump();\n-    print_depth(); tty->print(\"  \\\\ %d VPointer::scaled_iv_plus_offset: in(1) is offset_plus_k: \", n->in(1)->_idx); n->in(1)->dump();\n-  }\n-}\n-\n-void VPointer::Tracer::scaled_iv_plus_offset_6(Node* n) {\n-  if (_is_trace_alignment) {\n-    print_depth(); tty->print_cr(\" %d VPointer::scaled_iv_plus_offset: Op_%s PASSED\", n->_idx, n->Name());\n-    print_depth(); tty->print(\"  \\\\  %d VPointer::scaled_iv_plus_offset: in(1) is scaled_iv: \", n->in(1)->_idx); n->in(1)->dump();\n-    print_depth(); tty->print(\"  \\\\ %d VPointer::scaled_iv_plus_offset: in(2) is offset_plus_k: \", n->in(2)->_idx); n->in(2)->dump();\n-  }\n-}\n-\n-void VPointer::Tracer::scaled_iv_plus_offset_7(Node* n) {\n-  if (_is_trace_alignment) {\n-    print_depth(); tty->print_cr(\" %d VPointer::scaled_iv_plus_offset: Op_%s PASSED\", n->_idx, n->Name());\n-    print_depth(); tty->print(\"  \\\\ %d VPointer::scaled_iv_plus_offset: in(2) is scaled_iv: \", n->in(2)->_idx); n->in(2)->dump();\n-    print_depth(); tty->print(\"  \\\\ %d VPointer::scaled_iv_plus_offset: in(1) is offset_plus_k: \", n->in(1)->_idx); n->in(1)->dump();\n-  }\n-}\n-\n-void VPointer::Tracer::scaled_iv_plus_offset_8(Node* n) {\n-  if (_is_trace_alignment) {\n-    print_depth(); tty->print_cr(\" %d VPointer::scaled_iv_plus_offset: FAILED\", n->_idx);\n-  }\n-}\n-\n-void VPointer::Tracer::scaled_iv_1(Node* n) {\n-  if (_is_trace_alignment) {\n-    print_depth(); tty->print(\" %d VPointer::scaled_iv: testing node: \", n->_idx); n->dump();\n-  }\n-}\n-\n-void VPointer::Tracer::scaled_iv_2(Node* n, int scale) {\n-  if (_is_trace_alignment) {\n-    print_depth(); tty->print_cr(\" %d VPointer::scaled_iv: FAILED since another _scale has been detected before\", n->_idx);\n-    print_depth(); tty->print_cr(\"  \\\\ VPointer::scaled_iv: _scale (%d) != 0\", scale);\n-  }\n-}\n-\n-void VPointer::Tracer::scaled_iv_3(Node* n, int scale) {\n-  if (_is_trace_alignment) {\n-    print_depth(); tty->print_cr(\" %d VPointer::scaled_iv: is iv, setting _scale = %d\", n->_idx, scale);\n-  }\n-}\n-\n-void VPointer::Tracer::scaled_iv_4(Node* n, int scale) {\n-  if (_is_trace_alignment) {\n-    print_depth(); tty->print_cr(\" %d VPointer::scaled_iv: Op_MulI PASSED, setting _scale = %d\", n->_idx, scale);\n-    print_depth(); tty->print(\"  \\\\ %d VPointer::scaled_iv: in(1) is iv: \", n->in(1)->_idx); n->in(1)->dump();\n-    print_depth(); tty->print(\"  \\\\ %d VPointer::scaled_iv: in(2) is Con: \", n->in(2)->_idx); n->in(2)->dump();\n-  }\n-}\n-\n-void VPointer::Tracer::scaled_iv_5(Node* n, int scale) {\n-  if (_is_trace_alignment) {\n-    print_depth(); tty->print_cr(\" %d VPointer::scaled_iv: Op_MulI PASSED, setting _scale = %d\", n->_idx, scale);\n-    print_depth(); tty->print(\"  \\\\ %d VPointer::scaled_iv: in(2) is iv: \", n->in(2)->_idx); n->in(2)->dump();\n-    print_depth(); tty->print(\"  \\\\ %d VPointer::scaled_iv: in(1) is Con: \", n->in(1)->_idx); n->in(1)->dump();\n-  }\n-}\n-\n-void VPointer::Tracer::scaled_iv_6(Node* n, int scale) {\n-  if (_is_trace_alignment) {\n-    print_depth(); tty->print_cr(\" %d VPointer::scaled_iv: Op_LShiftI PASSED, setting _scale = %d\", n->_idx, scale);\n-    print_depth(); tty->print(\"  \\\\ %d VPointer::scaled_iv: in(1) is iv: \", n->in(1)->_idx); n->in(1)->dump();\n-    print_depth(); tty->print(\"  \\\\ %d VPointer::scaled_iv: in(2) is Con: \", n->in(2)->_idx); n->in(2)->dump();\n-  }\n-}\n-\n-void VPointer::Tracer::scaled_iv_7(Node* n) {\n-  if (_is_trace_alignment) {\n-    print_depth(); tty->print_cr(\" %d VPointer::scaled_iv: Op_ConvI2L PASSED\", n->_idx);\n-    print_depth(); tty->print_cr(\"  \\\\ VPointer::scaled_iv: in(1) %d is scaled_iv_plus_offset: \", n->in(1)->_idx);\n-    inc_depth(); inc_depth();\n-    print_depth(); n->in(1)->dump();\n-    dec_depth(); dec_depth();\n-  }\n-}\n-\n-void VPointer::Tracer::scaled_iv_8(Node* n, VPointer* tmp) {\n-  if (_is_trace_alignment) {\n-    print_depth(); tty->print(\" %d VPointer::scaled_iv: Op_LShiftL, creating tmp VPointer: \", n->_idx); tmp->print();\n-  }\n-}\n-\n-void VPointer::Tracer::scaled_iv_9(Node* n, int scale, int offset, Node* invar) {\n-  if (_is_trace_alignment) {\n-    print_depth(); tty->print_cr(\" %d VPointer::scaled_iv: Op_LShiftL PASSED, setting _scale = %d, _offset = %d\", n->_idx, scale, offset);\n-    print_depth(); tty->print_cr(\"  \\\\ VPointer::scaled_iv: in(1) [%d] is scaled_iv_plus_offset, in(2) [%d] used to scale: _scale = %d, _offset = %d\",\n-    n->in(1)->_idx, n->in(2)->_idx, scale, offset);\n-    if (invar != nullptr) {\n-      print_depth(); tty->print_cr(\"  \\\\ VPointer::scaled_iv: scaled invariant: [%d]\", invar->_idx);\n-    }\n-    inc_depth(); inc_depth();\n-    print_depth(); n->in(1)->dump();\n-    print_depth(); n->in(2)->dump();\n-    if (invar != nullptr) {\n-      print_depth(); invar->dump();\n-    }\n-    dec_depth(); dec_depth();\n-  }\n-}\n-\n-void VPointer::Tracer::scaled_iv_10(Node* n) {\n-  if (_is_trace_alignment) {\n-    print_depth(); tty->print_cr(\" %d VPointer::scaled_iv: FAILED\", n->_idx);\n-  }\n-}\n-\n-void VPointer::Tracer::offset_plus_k_1(Node* n) {\n-  if (_is_trace_alignment) {\n-    print_depth(); tty->print(\" %d VPointer::offset_plus_k: testing node: \", n->_idx); n->dump();\n-  }\n-}\n-\n-void VPointer::Tracer::offset_plus_k_2(Node* n, int _offset) {\n-  if (_is_trace_alignment) {\n-    print_depth(); tty->print_cr(\" %d VPointer::offset_plus_k: Op_ConI PASSED, setting _offset = %d\", n->_idx, _offset);\n-  }\n-}\n-\n-void VPointer::Tracer::offset_plus_k_3(Node* n, int _offset) {\n-  if (_is_trace_alignment) {\n-    print_depth(); tty->print_cr(\" %d VPointer::offset_plus_k: Op_ConL PASSED, setting _offset = %d\", n->_idx, _offset);\n-  }\n-}\n-\n-void VPointer::Tracer::offset_plus_k_4(Node* n) {\n-  if (_is_trace_alignment) {\n-    print_depth(); tty->print_cr(\" %d VPointer::offset_plus_k: FAILED\", n->_idx);\n-    print_depth(); tty->print_cr(\"  \\\\ \" JLONG_FORMAT \" VPointer::offset_plus_k: Op_ConL FAILED, k is too big\", n->get_long());\n-  }\n-}\n-\n-void VPointer::Tracer::offset_plus_k_5(Node* n, Node* _invar) {\n-  if (_is_trace_alignment) {\n-    print_depth(); tty->print_cr(\" %d VPointer::offset_plus_k: FAILED since another invariant has been detected before\", n->_idx);\n-    print_depth(); tty->print(\"  \\\\ %d VPointer::offset_plus_k: _invar is not null: \", _invar->_idx); _invar->dump();\n-  }\n-}\n-\n-void VPointer::Tracer::offset_plus_k_6(Node* n, Node* _invar, bool _negate_invar, int _offset) {\n-  if (_is_trace_alignment) {\n-    print_depth(); tty->print_cr(\" %d VPointer::offset_plus_k: Op_AddI PASSED, setting _debug_negate_invar = %d, _invar = %d, _offset = %d\",\n-    n->_idx, _negate_invar, _invar->_idx, _offset);\n-    print_depth(); tty->print(\"  \\\\ %d VPointer::offset_plus_k: in(2) is Con: \", n->in(2)->_idx); n->in(2)->dump();\n-    print_depth(); tty->print(\"  \\\\ %d VPointer::offset_plus_k: in(1) is invariant: \", _invar->_idx); _invar->dump();\n-  }\n-}\n-\n-void VPointer::Tracer::offset_plus_k_7(Node* n, Node* _invar, bool _negate_invar, int _offset) {\n-  if (_is_trace_alignment) {\n-    print_depth(); tty->print_cr(\" %d VPointer::offset_plus_k: Op_AddI PASSED, setting _debug_negate_invar = %d, _invar = %d, _offset = %d\",\n-    n->_idx, _negate_invar, _invar->_idx, _offset);\n-    print_depth(); tty->print(\"  \\\\ %d VPointer::offset_plus_k: in(1) is Con: \", n->in(1)->_idx); n->in(1)->dump();\n-    print_depth(); tty->print(\"  \\\\ %d VPointer::offset_plus_k: in(2) is invariant: \", _invar->_idx); _invar->dump();\n-  }\n-}\n-\n-void VPointer::Tracer::offset_plus_k_8(Node* n, Node* _invar, bool _negate_invar, int _offset) {\n-  if (_is_trace_alignment) {\n-    print_depth(); tty->print_cr(\" %d VPointer::offset_plus_k: Op_SubI is PASSED, setting _debug_negate_invar = %d, _invar = %d, _offset = %d\",\n-    n->_idx, _negate_invar, _invar->_idx, _offset);\n-    print_depth(); tty->print(\"  \\\\ %d VPointer::offset_plus_k: in(2) is Con: \", n->in(2)->_idx); n->in(2)->dump();\n-    print_depth(); tty->print(\"  \\\\ %d VPointer::offset_plus_k: in(1) is invariant: \", _invar->_idx); _invar->dump();\n-  }\n-}\n-\n-void VPointer::Tracer::offset_plus_k_9(Node* n, Node* _invar, bool _negate_invar, int _offset) {\n-  if (_is_trace_alignment) {\n-    print_depth(); tty->print_cr(\" %d VPointer::offset_plus_k: Op_SubI PASSED, setting _debug_negate_invar = %d, _invar = %d, _offset = %d\", n->_idx, _negate_invar, _invar->_idx, _offset);\n-    print_depth(); tty->print(\"  \\\\ %d VPointer::offset_plus_k: in(1) is Con: \", n->in(1)->_idx); n->in(1)->dump();\n-    print_depth(); tty->print(\"  \\\\ %d VPointer::offset_plus_k: in(2) is invariant: \", _invar->_idx); _invar->dump();\n-  }\n-}\n-\n-void VPointer::Tracer::offset_plus_k_10(Node* n, Node* _invar, bool _negate_invar, int _offset) {\n-  if (_is_trace_alignment) {\n-    print_depth(); tty->print_cr(\" %d VPointer::offset_plus_k: PASSED, setting _debug_negate_invar = %d, _invar = %d, _offset = %d\", n->_idx, _negate_invar, _invar->_idx, _offset);\n-    print_depth(); tty->print_cr(\"  \\\\ %d VPointer::offset_plus_k: is invariant\", n->_idx);\n-  }\n-}\n-\n-void VPointer::Tracer::offset_plus_k_11(Node* n) {\n-  if (_is_trace_alignment) {\n-    print_depth(); tty->print_cr(\" %d VPointer::offset_plus_k: FAILED\", n->_idx);\n-  }\n+  st->print(\")]\");\n+  if (end_with_cr) { st->cr(); }\n@@ -1659,1 +429,0 @@\n-\n@@ -1670,3 +439,3 @@\n-  \/\/ Out of simplicity: non power-of-2 scale not supported.\n-  if (abs(_scale) == 0 || !is_power_of_2(abs(_scale))) {\n-    return new EmptyAlignmentSolution(\"non power-of-2 scale not supported\");\n+  \/\/ Out of simplicity: non power-of-2 iv_scale not supported.\n+  if (abs(iv_scale()) == 0 || !is_power_of_2(abs(iv_scale()))) {\n+    return new EmptyAlignmentSolution(\"non power-of-2 iv_scale not supported\");\n@@ -1681,1 +450,1 @@\n-  \/\/   adr = base + offset + invar + scale * iv\n+  \/\/   adr = base + invar + iv_scale * iv + con\n@@ -1697,8 +466,8 @@\n-  \/\/          Simple form           Expansion of iv variable                  Reshaped with constants   Comments for terms\n-  \/\/          -----------           ------------------------                  -----------------------   ------------------\n-  \/\/   adr =  base               =  base                                   =  base                      (base % aw = 0)\n-  \/\/        + offset              + offset                                  + C_const                   (sum of constant terms)\n-  \/\/        + invar               + invar_factor * var_invar                + C_invar * var_invar       (term for invariant)\n-  \/\/                          \/   + scale * init                            + C_init  * var_init        (term for variable init)\n-  \/\/        + scale * iv   -> |   + scale * pre_stride * pre_iter           + C_pre   * pre_iter        (adjustable pre-loop term)\n-  \/\/                          \\   + scale * main_stride * main_iter         + C_main  * main_iter       (main-loop term)\n+  \/\/          Simple form             Expansion of iv variable                  Reshaped with constants   Comments for terms\n+  \/\/          -----------             ------------------------                  -----------------------   ------------------\n+  \/\/   adr =  base                 =  base                                   =  base                      (assume: base % aw = 0)\n+  \/\/        + invar                 + invar_factor * var_invar                + C_invar * var_invar       (term for invariant)\n+  \/\/                            \/   + iv_scale * init                         + C_init  * var_init        (term for variable init)\n+  \/\/        + iv_scale * iv  -> |   + iv_scale * pre_stride * pre_iter        + C_pre   * pre_iter        (adjustable pre-loop term)\n+  \/\/                            \\   + iv_scale * main_stride * main_iter      + C_main  * main_iter       (main-loop term)\n+  \/\/        + con                   + con                                     + C_const                   (sum of constant terms)\n@@ -1714,2 +483,5 @@\n-  \/\/   2) The \"C_const\" term is the sum of all constant terms. This is \"offset\",\n-  \/\/      plus \"scale * init\" if it is constant.\n+  \/\/      Note: we have been assuming that this also holds for native memory base\n+  \/\/            addresses. This is incorrect, see JDK-8323582.\n+  \/\/\n+  \/\/   2) The \"C_const\" term is the sum of all constant terms. This is \"con\",\n+  \/\/      plus \"iv_scale * init\" if it is constant.\n@@ -1721,1 +493,1 @@\n-  \/\/   4) The \"C_init * var_init\" is the factorization of \"scale * init\" into a\n+  \/\/   4) The \"C_init * var_init\" is the factorization of \"iv_scale * init\" into a\n@@ -1725,2 +497,2 @@\n-  \/\/        scale * init = C_init * var_init + scale * C_const_init                 (FAC_INIT)\n-  \/\/        C_init       = (init is constant) ? 0    : scale\n+  \/\/        iv_scale * init = C_init * var_init + iv_scale * C_const_init           (FAC_INIT)\n+  \/\/        C_init       = (init is constant) ? 0    : iv_scale\n@@ -1739,1 +511,1 @@\n-  const int C_const =      _offset + C_const_init * _scale;\n+  const int C_const =      _vpointer.con() + C_const_init * iv_scale();\n@@ -1742,1 +514,1 @@\n-  const int C_invar = (_invar == nullptr) ? 0 : abs(_invar_factor);\n+  const int C_invar = _vpointer.compute_invar_factor();\n@@ -1744,3 +516,3 @@\n-  const int C_init = _init_node->is_ConI() ? 0 : _scale;\n-  const int C_pre =  _scale * _pre_stride;\n-  const int C_main = _scale * _main_stride;\n+  const int C_init = _init_node->is_ConI() ? 0 : iv_scale();\n+  const int C_pre =  iv_scale() * _pre_stride;\n+  const int C_main = iv_scale() * _main_stride;\n@@ -1753,0 +525,1 @@\n+  \/\/ Note: the following assumption is incorrect for native memory bases, see JDK-8323582.\n@@ -2031,1 +804,1 @@\n-  \/\/                    = mx2 * q - C_const \/ (scale * pre_stride)                                  (11a)\n+  \/\/                    = mx2 * q - C_const \/ (iv_scale * pre_stride)                               (11a)\n@@ -2039,1 +812,1 @@\n-  \/\/                    = my2 * q - invar \/ (scale * pre_stride)                                    (11b, with invar)\n+  \/\/                    = my2 * q - invar \/ (iv_scale * pre_stride)                                 (11b, with invar)\n@@ -2045,1 +818,1 @@\n-  \/\/ If init is variable (i.e. C_init = scale, init = var_init):\n+  \/\/ If init is variable (i.e. C_init = iv_scale, init = var_init):\n@@ -2047,5 +820,5 @@\n-  \/\/   pre_iter_C_init  = mz2 * q - sign(C_pre) * Z       * var_init\n-  \/\/                    = mz2 * q - sign(C_pre) * C_init  * var_init  \/ abs(C_pre)\n-  \/\/                    = mz2 * q - sign(C_pre) * scale   * init      \/ abs(C_pre)\n-  \/\/                    = mz2 * q - scale * init \/ C_pre\n-  \/\/                    = mz2 * q - scale * init \/ (scale * pre_stride)\n+  \/\/   pre_iter_C_init  = mz2 * q - sign(C_pre) * Z          * var_init\n+  \/\/                    = mz2 * q - sign(C_pre) * C_init     * var_init  \/ abs(C_pre)\n+  \/\/                    = mz2 * q - sign(C_pre) * iv_scale   * init      \/ abs(C_pre)\n+  \/\/                    = mz2 * q - iv_scale * init \/ C_pre\n+  \/\/                    = mz2 * q - iv_scale * init \/ (iv_scale * pre_stride)\n@@ -2062,3 +835,3 @@\n-  \/\/            =   mx2 * q  - C_const \/ (scale * pre_stride)\n-  \/\/              + my2 * q [- invar \/ (scale * pre_stride) ]\n-  \/\/              + mz2 * q [- init \/ pre_stride            ]\n+  \/\/            =   mx2 * q  - C_const \/ (iv_scale * pre_stride)\n+  \/\/              + my2 * q [- invar \/ (iv_scale * pre_stride) ]\n+  \/\/              + mz2 * q [- init \/ pre_stride               ]\n@@ -2067,3 +840,3 @@\n-  \/\/              - C_const \/ (scale * pre_stride)        (align constant term)\n-  \/\/             [- invar \/ (scale * pre_stride)   ]      (align invariant term, if present)\n-  \/\/             [- init \/ pre_stride              ]      (align variable init term, if present)    (12)\n+  \/\/              - C_const \/ (iv_scale * pre_stride)        (align constant term)\n+  \/\/             [- invar \/ (iv_scale * pre_stride)   ]      (align invariant term, if present)\n+  \/\/             [- init \/ pre_stride                 ]      (align variable init term, if present)    (12)\n@@ -2073,1 +846,1 @@\n-  \/\/   r = (-C_const \/ (scale * pre_stride)) % q                                                    (13)\n+  \/\/   r = (-C_const \/ (iv_scale * pre_stride)) % q                                                    (13)\n@@ -2075,1 +848,1 @@\n-  const int r = AlignmentSolution::mod(-C_const \/ (_scale * _pre_stride), q);\n+  const int r = AlignmentSolution::mod(-C_const \/ (iv_scale() * _pre_stride), q);\n@@ -2078,2 +851,2 @@\n-  \/\/                   [- invar \/ (scale * pre_stride)  ]\n-  \/\/                   [- init \/ pre_stride             ]                                           (14)\n+  \/\/                   [- invar \/ (iv_scale * pre_stride)  ]\n+  \/\/                   [- init \/ pre_stride                ]                                           (14)\n@@ -2083,1 +856,1 @@\n-  \/\/   q (periodicity), r (constant alignment), invar, scale, pre_stride, init\n+  \/\/   q (periodicity), r (constant alignment), invar, iv_scale, pre_stride, init\n@@ -2090,1 +863,1 @@\n-  return new ConstrainedAlignmentSolution(_mem_ref, q, r, _invar, _scale);\n+  return new ConstrainedAlignmentSolution(_mem_ref, q, r, _vpointer \/* holds invar and iv_scale *\/);\n@@ -2098,1 +871,1 @@\n-  \/\/   (base + offset + invar + scale * iv) % aw =\n+  \/\/   (base + invar + iv_scale * iv + con) % aw =\n@@ -2101,1 +874,1 @@\n-  \/\/   (base + offset + invar + scale * (init + pre_stride * pre_iter + main_stride * main_iter)) % aw =\n+  \/\/   (base + con + invar + iv_scale * (init + pre_stride * pre_iter + main_stride * main_iter)) % aw =\n@@ -2104,4 +877,4 @@\n-  \/\/   (base + offset + invar\n-  \/\/         + scale * init\n-  \/\/         + scale * pre_stride * pre_iter\n-  \/\/         + scale * main_stride * main_iter)) % aw =\n+  \/\/   (base + con + invar\n+  \/\/         + iv_scale * init\n+  \/\/         + iv_scale * pre_stride * pre_iter\n+  \/\/         + iv_scale * main_stride * main_iter)) % aw =\n@@ -2110,2 +883,3 @@\n-  \/\/   -> main-loop iterations aligned (2): C_main % aw = (scale * main_stride) % aw = 0\n-  \/\/   (offset + invar + scale * init + scale * pre_stride * pre_iter) % aw =\n+  \/\/        Note: this assumption is incorrect for native memory bases, see JDK-8323582.\n+  \/\/   -> main-loop iterations aligned (2): C_main % aw = (iv_scale * main_stride) % aw = 0\n+  \/\/   (con + invar + iv_scale * init + iv_scale * pre_stride * pre_iter) % aw =\n@@ -2114,4 +888,4 @@\n-  \/\/   (offset + invar + scale * init\n-  \/\/           + scale * pre_stride * (m * q - C_const \/ (scale * pre_stride)\n-  \/\/                                        [- invar \/ (scale * pre_stride) ]\n-  \/\/                                        [- init \/ pre_stride            ]\n+  \/\/   (con + invar + iv_scale * init\n+  \/\/        + iv_scale * pre_stride * (m * q - C_const \/ (iv_scale * pre_stride)\n+  \/\/                                        [- invar \/ (iv_scale * pre_stride) ]\n+  \/\/                                        [- init \/ pre_stride               ]\n@@ -2121,6 +895,6 @@\n-  \/\/   -> expand C_const = offset [+ init * scale]  (if init const)\n-  \/\/   (offset + invar + scale * init\n-  \/\/           + scale * pre_stride * (m * q - offset \/ (scale * pre_stride)\n-  \/\/                                        [- init \/ pre_stride            ]             (if init constant)\n-  \/\/                                        [- invar \/ (scale * pre_stride) ]             (if invar present)\n-  \/\/                                        [- init \/ pre_stride            ]             (if init variable)\n+  \/\/   -> expand C_const = con [+ init * iv_scale]  (if init const)\n+  \/\/   (con + invar + iv_scale * init\n+  \/\/        + iv_scale * pre_stride * (m * q - con \/ (iv_scale * pre_stride)\n+  \/\/                                        [- init \/ pre_stride               ]          (if init constant)\n+  \/\/                                        [- invar \/ (iv_scale * pre_stride) ]          (if invar present)\n+  \/\/                                        [- init \/ pre_stride               ]          (if init variable)\n@@ -2132,2 +906,2 @@\n-  \/\/   -> apply (8): q = aw \/ (abs(C_pre)) = aw \/ abs(scale * pre_stride)\n-  \/\/   -> and hence: (scale * pre_stride * q) % aw = 0\n+  \/\/   -> apply (8): q = aw \/ (abs(C_pre)) = aw \/ abs(iv_scale * pre_stride)\n+  \/\/   -> and hence: (iv_scale * pre_stride * q) % aw = 0\n@@ -2135,5 +909,5 @@\n-  \/\/   (offset + invar + scale * init\n-  \/\/           + scale * pre_stride * m * q                             -> aw aligned\n-  \/\/           - scale * pre_stride * offset \/ (scale * pre_stride)     -> = offset\n-  \/\/           - scale * pre_stride * init \/ pre_stride                 -> = scale * init\n-  \/\/           - scale * pre_stride * invar \/ (scale * pre_stride)      -> = invar\n+  \/\/   (con + invar + iv_scale * init\n+  \/\/        + iv_scale * pre_stride * m * q                              -> aw aligned\n+  \/\/        - iv_scale * pre_stride * con   \/ (iv_scale * pre_stride)    -> = con\n+  \/\/        - iv_scale * pre_stride * init  \/ pre_stride                 -> = iv_scale * init\n+  \/\/        - iv_scale * pre_stride * invar \/ (iv_scale * pre_stride)    -> = invar\n@@ -2150,2 +924,3 @@\n-    tty->print_cr(\"  vector_width = vector_length(%d) * element_size(%d) = %d\",\n-                  _vector_length, _element_size, _vector_width);\n+    tty->print(\"  VPointer: \");\n+    _vpointer.print_on(tty);\n+    tty->print_cr(\"  vector_width = %d\", _vector_width);\n@@ -2160,3 +935,11 @@\n-    if (_invar != nullptr) {\n-      tty->print(\"  invar:\");\n-      _invar->dump();\n+    tty->print_cr(\"  invar = SUM(invar_summands), invar_summands:\");\n+    int invar_count = 0;\n+    _vpointer.for_each_invar_summand([&] (const MemPointerSummand& s) {\n+      tty->print(\"   \");\n+      s.print_on(tty);\n+      tty->print(\" -> \");\n+      s.variable()->dump();\n+      invar_count++;\n+    });\n+    if (invar_count == 0) {\n+      tty->print_cr(\"   No invar_summands.\");\n@@ -2165,1 +948,2 @@\n-    tty->print_cr(\"  invar_factor = %d\", _invar_factor);\n+    const jint invar_factor = _vpointer.compute_invar_factor();\n+    tty->print_cr(\"  invar_factor = %d\", invar_factor);\n@@ -2169,1 +953,5 @@\n-    VPointer::print_con_or_idx(_init_node);\n+    if (_init_node->is_ConI()) {\n+      tty->print(\"(%4d)\", _init_node->as_ConI()->get_int());\n+    } else {\n+      tty->print(\"[%4d]\", _init_node->_idx);\n+    }\n@@ -2172,7 +960,3 @@\n-\n-    \/\/ adr = base + offset + invar + scale * iv\n-    tty->print(\"  adr = base\");\n-    VPointer::print_con_or_idx(_base);\n-    tty->print(\" + offset(%d) + invar\", _offset);\n-    VPointer::print_con_or_idx(_invar);\n-    tty->print_cr(\" + scale(%d) * iv\", _scale);\n+    \/\/ adr = base + con + invar + iv_scale * iv\n+    tty->print(\"  adr = base[%d]\", base().object_or_native()->_idx);\n+    tty->print(\" + con(%d) + invar + iv_scale(%d) * iv\", _vpointer.con(), iv_scale());\n@@ -2190,1 +974,1 @@\n-    tty->print(\"      = base[%d] + \", _base->_idx);\n+    tty->print(\"      = base[%d] + \", base().object_or_native()->_idx);\n@@ -2200,1 +984,1 @@\n-      tty->print_cr(\"    C_init = abs(scale)= %d\", C_init);\n+      tty->print_cr(\"    C_init = abs(iv_scale)= %d\", C_init);\n@@ -2202,1 +986,1 @@\n-    if (_invar != nullptr) {\n+    if (C_invar != 0) {\n@@ -2204,1 +988,1 @@\n-      tty->print_cr(\"    C_invar = abs(invar_factor) = %d\", C_invar);\n+      tty->print_cr(\"    C_invar = invar_factor = %d\", C_invar);\n@@ -2209,6 +993,6 @@\n-    tty->print_cr(\"  C_const = offset(%d) + scale(%d) * C_const_init(%d) = %d\",\n-                  _offset, _scale, C_const_init, C_const);\n-    tty->print_cr(\"  C_pre   = scale(%d) * pre_stride(%d) = %d\",\n-                  _scale, _pre_stride, C_pre);\n-    tty->print_cr(\"  C_main  = scale(%d) * main_stride(%d) = %d\",\n-                  _scale, _main_stride, C_main);\n+    tty->print_cr(\"  C_const = con(%d) + iv_scale(%d) * C_const_init(%d) = %d\",\n+                  _vpointer.con(), iv_scale(), C_const_init, C_const);\n+    tty->print_cr(\"  C_pre   = iv_scale(%d) * pre_stride(%d) = %d\",\n+                  iv_scale(), _pre_stride, C_pre);\n+    tty->print_cr(\"  C_main  = iv_scale(%d) * main_stride(%d) = %d\",\n+                  iv_scale(), _main_stride, C_main);\n@@ -2283,2 +1067,2 @@\n-    tty->print_cr(\"  r = (-C_const(%d) \/ (scale(%d) * pre_stride(%d)) %% q(%d) = %d\",\n-                  C_const, _scale, _pre_stride, q, r);\n+    tty->print_cr(\"  r = (-C_const(%d) \/ (iv_scale(%d) * pre_stride(%d)) %% q(%d) = %d\",\n+                  C_const, iv_scale(), _pre_stride, q, r);\n@@ -2287,3 +1071,3 @@\n-    if (_invar != nullptr) {\n-      tty->print_cr(\"                                 - invar \/ (scale(%d) * pre_stride(%d))\",\n-                    _scale, _pre_stride);\n+    if (C_invar != 0) {\n+      tty->print_cr(\"                                 - invar \/ (iv_scale(%d) * pre_stride(%d))\",\n+                    iv_scale(), _pre_stride);\n","filename":"src\/hotspot\/share\/opto\/vectorization.cpp","additions":138,"deletions":1354,"binary":false,"changes":1492,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2023, 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2023, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -31,0 +31,1 @@\n+#include \"opto\/mempointer.hpp\"\n@@ -88,0 +89,1 @@\n+  NOT_PRODUCT(TraceMemPointer _mptrace;)\n@@ -105,1 +107,12 @@\n-    _pre_loop_end (nullptr) {}\n+    _pre_loop_end (nullptr)\n+#ifndef PRODUCT\n+    COMMA\n+    _mptrace(TraceMemPointer(\n+      _vtrace.is_trace(TraceAutoVectorizationTag::POINTER_PARSING),\n+      _vtrace.is_trace(TraceAutoVectorizationTag::POINTER_ALIASING),\n+      _vtrace.is_trace(TraceAutoVectorizationTag::POINTER_ADJACENCY),\n+      _vtrace.is_trace(TraceAutoVectorizationTag::POINTER_OVERLAP)\n+    ))\n+#endif\n+    {}\n+\n@@ -136,1 +149,2 @@\n-  const VTrace& vtrace()      const { return _vtrace; }\n+  const VTrace& vtrace()           const { return _vtrace; }\n+  const TraceMemPointer& mptrace() const { return _mptrace; }\n@@ -165,4 +179,0 @@\n-\n-  bool is_trace_pointer_analysis() const {\n-    return _vtrace.is_trace(TraceAutoVectorizationTag::POINTER_ANALYSIS);\n-  }\n@@ -178,0 +188,21 @@\n+  \/\/ Some nodes must be pre-loop invariant, so that they can be used for conditions\n+  \/\/ before or inside the pre-loop. For example, alignment of main-loop vector\n+  \/\/ memops must be achieved in the pre-loop, via the exit check in the pre-loop.\n+  bool is_pre_loop_invariant(Node* n) const {\n+    \/\/ Must be in the main-loop, otherwise we can't access the pre-loop.\n+    \/\/ This fails during SuperWord::unrolling_analysis, but that is ok.\n+    if (!cl()->is_main_loop()) {\n+      return false;\n+    }\n+\n+    Node* ctrl = phase()->get_ctrl(n);\n+\n+    \/\/ Quick test: is it in the main-loop?\n+    if (lpt()->is_member(phase()->get_loop(ctrl))) {\n+      return false;\n+    }\n+\n+    \/\/ Is it before the pre-loop?\n+    return phase()->is_dominator(ctrl, pre_loop_head());\n+  }\n+\n@@ -430,1 +461,1 @@\n-  int data_size(Node* s) const {\n+  int data_size(const Node* s) const {\n@@ -670,3 +701,1 @@\n-\/\/ A vectorization pointer (VPointer) has information about an address for\n-\/\/ dependence checking and vector alignment. It's usually bound to a memory\n-\/\/ operation in a counted loop for vectorizable analysis.\n+\/\/ Reminder: MemPointer have the form:\n@@ -674,1 +703,1 @@\n-\/\/ We parse and represent pointers of the simple form:\n+\/\/   pointer = SUM(summands) + con\n@@ -676,1 +705,1 @@\n-\/\/   pointer   = adr + offset + invar + scale * ConvI2L(iv)\n+\/\/ Where every summand in summands has the form:\n@@ -678,10 +707,1 @@\n-\/\/ Where:\n-\/\/\n-\/\/   adr: the base address of an array (base = adr)\n-\/\/        OR\n-\/\/        some address to off-heap memory (base = TOP)\n-\/\/\n-\/\/   offset: a constant offset\n-\/\/   invar:  a runtime variable, which is invariant during the loop\n-\/\/   scale:  scaling factor\n-\/\/   iv:     loop induction variable\n+\/\/   summand = scale * variable\n@@ -689,1 +709,2 @@\n-\/\/ But more precisely, we parse the composite-long-int form:\n+\/\/ The VPointer wraps a MemPointer for the use in loops. A \"valid\" VPointer has\n+\/\/ the form:\n@@ -691,1 +712,1 @@\n-\/\/   pointer   = adr + long_offset + long_invar + long_scale * ConvI2L(int_offset + inv_invar + int_scale * iv)\n+\/\/   pointer = base + invar + iv_scale * iv + con\n@@ -693,2 +714,1 @@\n-\/\/   pointer   = adr + long_offset + long_invar + long_scale * ConvI2L(int_index)\n-\/\/   int_index =       int_offset  + int_invar  + int_scale  * iv\n+\/\/   invar = SUM(invar_summands)\n@@ -696,7 +716,11 @@\n-\/\/ However, for aliasing and adjacency checks (e.g. VPointer::cmp()) we always use the simple form to make\n-\/\/ decisions. Hence, we must make sure to only create a \"valid\" VPointer if the optimisations based on the\n-\/\/ simple form produce the same result as the compound-long-int form would. Intuitively, this depends on\n-\/\/ if the int_index overflows, but the precise conditions are given in VPointer::is_safe_to_use_as_simple_form().\n-\/\/\n-\/\/   ConvI2L(int_index) = ConvI2L(int_offset  + int_invar  + int_scale  * iv)\n-\/\/                      = Convi2L(int_offset) + ConvI2L(int_invar) + ConvI2L(int_scale) * ConvI2L(iv)\n+\/\/ Where:\n+\/\/   - base: is the known base of the MemPointer.\n+\/\/       on-heap (object base) or off-heap (native base address)\n+\/\/   - iv and iv_scale: i.e. the iv_summand = iv * iv_scale.\n+\/\/       If we find a summand where the variable is the iv, we set iv_scale to the\n+\/\/       corresponding scale. If there is no such summand, then we know that the\n+\/\/       pointer does not depend on the iv, since otherwise there would have to be\n+\/\/       a summand where its variable is main-loop variant.\n+\/\/   - invar_summands: all other summands except base and iv_summand.\n+\/\/       All variables must be pre-loop invariant. This is important when we need\n+\/\/       to memory align a pointer using the pre-loop limit.\n@@ -704,3 +728,10 @@\n-\/\/   scale  = long_scale * ConvI2L(int_scale)\n-\/\/   offset = long_offset + long_scale * ConvI2L(int_offset)\n-\/\/   invar  = long_invar  + long_scale * ConvI2L(int_invar)\n+\/\/ A VPointer can be marked \"invalid\", if some of these conditions are not met, or\n+\/\/ it is unknown if they are met. If a VPointer is marked \"invalid\", it always\n+\/\/ returns conservative answers to aliasing queries, which means that we do not\n+\/\/ optimize in these cases. For example:\n+\/\/    - is_adjacent_to_and_before: returning true would allow optimizations such as\n+\/\/                                 packing into vectors. So for \"invalid\" VPointers\n+\/\/                                 we always return false (i.e. unknown).\n+\/\/    - never_overlaps_with: returning true would allow optimizations such as\n+\/\/                           swapping the order of memops. So for \"invalid\" VPointers\n+\/\/                           we always return false (i.e. unknown).\n@@ -708,1 +739,8 @@\n-\/\/   pointer   = adr + offset + invar + scale * ConvI2L(iv)\n+\/\/ These are examples where a VPointer becomes \"invalid\":\n+\/\/    - If the MemPointer does not have the required form for VPointer,\n+\/\/      i.e. if one of these conditions is not met (see init_is_valid):\n+\/\/      - Base must be known.\n+\/\/      - All summands except the iv-summand must be pre-loop invariant.\n+\/\/      - Some restrictions on iv_scale and iv_stride, to avoid overflow in\n+\/\/        alignment computations.\n+\/\/    - If the new con computed in make_with_iv_offset overflows.\n@@ -711,16 +749,3 @@\n- protected:\n-  MemNode* const  _mem;      \/\/ My memory reference node\n-  const VLoop&    _vloop;\n-\n-  \/\/ Components of the simple form:\n-  Node* _base;               \/\/ Base address of an array OR null if some off-heap memory.\n-  Node* _adr;                \/\/ Same as _base if an array pointer OR some off-heap memory pointer.\n-  int   _scale;              \/\/ multiplier for iv (in bytes), 0 if no loop iv\n-  int   _offset;             \/\/ constant offset (in bytes)\n-\n-  Node* _invar;              \/\/ invariant offset (in bytes), null if none\n-#ifdef ASSERT\n-  Node* _debug_invar;\n-  bool  _debug_negate_invar; \/\/ if true then use: (0 - _invar)\n-  Node* _debug_invar_scale;  \/\/ multiplier for invariant\n-#endif\n+private:\n+  const VLoop& _vloop;\n+  const MemPointer _mem_pointer;\n@@ -728,33 +753,2 @@\n-  \/\/ The int_index components of the compound-long-int form. Used to decide if it is safe to use the\n-  \/\/ simple form rather than the compound-long-int form that was parsed.\n-  bool  _has_int_index_after_convI2L;\n-  int   _int_index_after_convI2L_offset;\n-  Node* _int_index_after_convI2L_invar;\n-  int   _int_index_after_convI2L_scale;\n-\n-  Node_Stack* _nstack;       \/\/ stack used to record a vpointer trace of variants\n-  bool        _analyze_only; \/\/ Used in loop unrolling only for vpointer trace\n-  uint        _stack_idx;    \/\/ Used in loop unrolling only for vpointer trace\n-\n-  PhaseIdealLoop* phase() const { return _vloop.phase(); }\n-  IdealLoopTree*  lpt() const   { return _vloop.lpt(); }\n-  PhiNode*        iv() const    { return _vloop.iv(); }\n-\n-  bool is_loop_member(Node* n) const;\n-  bool invariant(Node* n) const;\n-\n-  \/\/ Match: k*iv + offset\n-  bool scaled_iv_plus_offset(Node* n);\n-  \/\/ Match: k*iv where k is a constant that's not zero\n-  bool scaled_iv(Node* n);\n-  \/\/ Match: offset is (k [+\/- invariant])\n-  bool offset_plus_k(Node* n, bool negate = false);\n-\n- public:\n-  enum CMP {\n-    Less          = 1,\n-    Greater       = 2,\n-    Equal         = 4,\n-    NotEqual      = (Less | Greater),\n-    NotComparable = (Less | Greater | Equal)\n-  };\n+  \/\/ Derived, for quicker use.\n+  const jint  _iv_scale;\n@@ -762,37 +756,12 @@\n-  VPointer(MemNode* const mem, const VLoop& vloop) :\n-    VPointer(mem, vloop, nullptr, false) {}\n-  VPointer(MemNode* const mem, const VLoop& vloop, Node_Stack* nstack) :\n-    VPointer(mem, vloop, nstack, true) {}\n- private:\n-  VPointer(MemNode* const mem, const VLoop& vloop,\n-           Node_Stack* nstack, bool analyze_only);\n-  \/\/ Following is used to create a temporary object during\n-  \/\/ the pattern match of an address expression.\n-  VPointer(VPointer* p);\n-  NONCOPYABLE(VPointer);\n-\n-  bool is_safe_to_use_as_simple_form(Node* base, Node* adr) const;\n-\n- public:\n-  bool valid()             const { return _adr != nullptr; }\n-  bool has_iv()            const { return _scale != 0; }\n-\n-  Node* base()             const { return _base; }\n-  Node* adr()              const { return _adr; }\n-  MemNode* mem()           const { return _mem; }\n-  int   scale_in_bytes()   const { return _scale; }\n-  Node* invar()            const { return _invar; }\n-  int   offset_in_bytes()  const { return _offset; }\n-  int   memory_size()      const { return _mem->memory_size(); }\n-  Node_Stack* node_stack() const { return _nstack; }\n-\n-  \/\/ Biggest detectable factor of the invariant.\n-  int   invar_factor() const;\n-\n-  \/\/ Comparable?\n-  bool invar_equals(const VPointer& q) const {\n-    assert(_debug_invar == NodeSentinel || q._debug_invar == NodeSentinel ||\n-           (_invar == q._invar) == (_debug_invar == q._debug_invar &&\n-                                    _debug_invar_scale == q._debug_invar_scale &&\n-                                    _debug_negate_invar == q._debug_negate_invar), \"\");\n-    return _invar == q._invar;\n+  const bool _is_valid;\n+\n+  VPointer(const VLoop& vloop,\n+           const MemPointer& mem_pointer,\n+           const bool must_be_invalid = false) :\n+    _vloop(vloop),\n+    _mem_pointer(mem_pointer),\n+    _iv_scale(init_iv_scale()),\n+    _is_valid(!must_be_invalid && init_is_valid()) {}\n+\n+  VPointer make_invalid() const {\n+    return VPointer(_vloop, mem_pointer(), true \/* must be invalid*\/);\n@@ -801,42 +770,14 @@\n-  \/\/ We compute if and how two VPointers can alias at runtime, i.e. if the two addressed regions of memory can\n-  \/\/ ever overlap. There are essentially 3 relevant return states:\n-  \/\/  - NotComparable:  Synonymous to \"unknown aliasing\".\n-  \/\/                    We have no information about how the two VPointers can alias. They could overlap, refer\n-  \/\/                    to another location in the same memory object, or point to a completely different object.\n-  \/\/                    -> Memory edge required. Aliasing unlikely but possible.\n-  \/\/\n-  \/\/  - Less \/ Greater: Synonymous to \"never aliasing\".\n-  \/\/                    The two VPointers may point into the same memory object, but be non-aliasing (i.e. we\n-  \/\/                    know both address regions inside the same memory object, but these regions are non-\n-  \/\/                    overlapping), or the VPointers point to entirely different objects.\n-  \/\/                    -> No memory edge required. Aliasing impossible.\n-  \/\/\n-  \/\/  - Equal:          Synonymous to \"overlap, or point to different memory objects\".\n-  \/\/                    The two VPointers either overlap on the same memory object, or point to two different\n-  \/\/                    memory objects.\n-  \/\/                    -> Memory edge required. Aliasing likely.\n-  \/\/\n-  \/\/ In a future refactoring, we can simplify to two states:\n-  \/\/  - NeverAlias:     instead of Less \/ Greater\n-  \/\/  - MayAlias:       instead of Equal \/ NotComparable\n-  \/\/\n-  \/\/ Two VPointer are \"comparable\" (Less \/ Greater \/ Equal), iff all of these conditions apply:\n-  \/\/   1) Both are valid, i.e. expressible in the compound-long-int or simple form.\n-  \/\/   2) The adr are identical, or both are array bases of different arrays.\n-  \/\/   3) They have identical scale.\n-  \/\/   4) They have identical invar.\n-  \/\/   5) The difference in offsets is limited: abs(offset0 - offset1) < 2^31.\n-  int cmp(const VPointer& q) const {\n-    if (valid() && q.valid() &&\n-        (_adr == q._adr || (_base == _adr && q._base == q._adr)) &&\n-        _scale == q._scale   && invar_equals(q)) {\n-      jlong difference = abs(java_subtract((jlong)_offset, (jlong)q._offset));\n-      jlong max_diff = (jlong)1 << 31;\n-      if (difference >= max_diff) {\n-        return NotComparable;\n-      }\n-      bool overlap = q._offset <   _offset +   memory_size() &&\n-                       _offset < q._offset + q.memory_size();\n-      return overlap ? Equal : (_offset < q._offset ? Less : Greater);\n-    } else {\n-      return NotComparable;\n+public:\n+  VPointer(const MemNode* mem,\n+           const VLoop& vloop,\n+           MemPointerParserCallback& callback = MemPointerParserCallback::empty()) :\n+    VPointer(vloop,\n+             MemPointer(mem,\n+                        callback\n+                        NOT_PRODUCT(COMMA vloop.mptrace())))\n+  {\n+#ifndef PRODUCT\n+    if (vloop.mptrace().is_trace_parsing()) {\n+      tty->print_cr(\"VPointer::VPointer:\");\n+      tty->print(\"mem: \"); mem->dump();\n+      print_on(tty);\n@@ -844,0 +785,1 @@\n+#endif\n@@ -846,10 +788,7 @@\n-  bool overlap_possible_with_any_in(const GrowableArray<Node*>& nodes) const {\n-    for (int i = 0; i < nodes.length(); i++) {\n-      MemNode* mem = nodes.at(i)->as_Mem();\n-      VPointer p_mem(mem, _vloop);\n-      \/\/ Only if we know that we have Less or Greater can we\n-      \/\/ be sure that there can never be an overlap between\n-      \/\/ the two memory regions.\n-      if (!not_equal(p_mem)) {\n-        return true;\n-      }\n+  VPointer make_with_size(const jint new_size) const {\n+    const VPointer p(_vloop, mem_pointer().make_with_size(new_size));\n+#ifndef PRODUCT\n+    if (_vloop.mptrace().is_trace_parsing()) {\n+      tty->print_cr(\"VPointer::make_with_size:\");\n+      tty->print(\"  old: \"); print_on(tty);\n+      tty->print(\"  new: \"); p.print_on(tty);\n@@ -857,1 +796,2 @@\n-    return false;\n+#endif\n+    return p;\n@@ -860,15 +800,6 @@\n-  bool not_equal(const VPointer& q)  const { return not_equal(cmp(q)); }\n-  bool equal(const VPointer& q)      const { return equal(cmp(q)); }\n-  bool comparable(const VPointer& q) const { return comparable(cmp(q)); }\n-  static bool not_equal(int cmp)  { return cmp <= NotEqual; }\n-  static bool equal(int cmp)      { return cmp == Equal; }\n-  static bool comparable(int cmp) { return cmp < NotComparable; }\n-\n-  \/\/ We need to be able to sort the VPointer to efficiently group the\n-  \/\/ memops into groups, and to find adjacent memops.\n-  static int cmp_for_sort_by_group(const VPointer** p1, const VPointer** p2);\n-  static int cmp_for_sort(const VPointer** p1, const VPointer** p2);\n-\n-  NOT_PRODUCT( void print() const; )\n-  NOT_PRODUCT( static void print_con_or_idx(const Node* n); )\n-\n+  \/\/ old_pointer = base + invar + iv_scale *  iv              + con\n+  \/\/ new_pointer = base + invar + iv_scale * (iv + iv_offset) + con\n+  \/\/             = base + invar + iv_scale * iv               + (con + iv_scale * iv_offset)\n+  VPointer make_with_iv_offset(const jint iv_offset) const {\n+    NoOverflowInt new_con = NoOverflowInt(con()) + NoOverflowInt(iv_scale()) * NoOverflowInt(iv_offset);\n+    if (new_con.is_NaN()) {\n@@ -876,61 +807,5 @@\n-  class Tracer {\n-    friend class VPointer;\n-    bool _is_trace_alignment;\n-    static int _depth;\n-    int _depth_save;\n-    void print_depth() const;\n-    int  depth() const    { return _depth; }\n-    void set_depth(int d) { _depth = d; }\n-    void inc_depth()      { _depth++; }\n-    void dec_depth()      { if (_depth > 0) _depth--; }\n-    void store_depth()    { _depth_save = _depth; }\n-    void restore_depth()  { _depth = _depth_save; }\n-\n-    class Depth {\n-      friend class VPointer;\n-      Depth()      { ++_depth; }\n-      Depth(int x) { _depth = 0; }\n-      ~Depth()     { if (_depth > 0) --_depth; }\n-    };\n-    Tracer(bool is_trace_alignment) : _is_trace_alignment(is_trace_alignment) {}\n-\n-    \/\/ tracing functions\n-    void ctor_1(const Node* mem);\n-    void ctor_2(Node* adr);\n-    void ctor_3(Node* adr, int i);\n-    void ctor_4(Node* adr, int i);\n-    void ctor_5(Node* adr, Node* base,  int i);\n-    void ctor_6(const Node* mem);\n-\n-    void scaled_iv_plus_offset_1(Node* n);\n-    void scaled_iv_plus_offset_2(Node* n);\n-    void scaled_iv_plus_offset_3(Node* n);\n-    void scaled_iv_plus_offset_4(Node* n);\n-    void scaled_iv_plus_offset_5(Node* n);\n-    void scaled_iv_plus_offset_6(Node* n);\n-    void scaled_iv_plus_offset_7(Node* n);\n-    void scaled_iv_plus_offset_8(Node* n);\n-\n-    void scaled_iv_1(Node* n);\n-    void scaled_iv_2(Node* n, int scale);\n-    void scaled_iv_3(Node* n, int scale);\n-    void scaled_iv_4(Node* n, int scale);\n-    void scaled_iv_5(Node* n, int scale);\n-    void scaled_iv_6(Node* n, int scale);\n-    void scaled_iv_7(Node* n);\n-    void scaled_iv_8(Node* n, VPointer* tmp);\n-    void scaled_iv_9(Node* n, int _scale, int _offset, Node* _invar);\n-    void scaled_iv_10(Node* n);\n-\n-    void offset_plus_k_1(Node* n);\n-    void offset_plus_k_2(Node* n, int _offset);\n-    void offset_plus_k_3(Node* n, int _offset);\n-    void offset_plus_k_4(Node* n);\n-    void offset_plus_k_5(Node* n, Node* _invar);\n-    void offset_plus_k_6(Node* n, Node* _invar, bool _negate_invar, int _offset);\n-    void offset_plus_k_7(Node* n, Node* _invar, bool _negate_invar, int _offset);\n-    void offset_plus_k_8(Node* n, Node* _invar, bool _negate_invar, int _offset);\n-    void offset_plus_k_9(Node* n, Node* _invar, bool _negate_invar, int _offset);\n-    void offset_plus_k_10(Node* n, Node* _invar, bool _negate_invar, int _offset);\n-    void offset_plus_k_11(Node* n);\n-  } _tracer; \/\/ Tracer\n+      if (_vloop.mptrace().is_trace_parsing()) {\n+        tty->print_cr(\"VPointer::make_with_iv_offset:\");\n+        tty->print(\"  old: \"); print_on(tty);\n+        tty->print_cr(\"  new con overflow (iv_offset: %d) -> invalid VPointer.\", iv_offset);\n+      }\n@@ -938,0 +813,12 @@\n+      return make_invalid();\n+    }\n+    const VPointer p(_vloop, mem_pointer().make_with_con(new_con));\n+#ifndef PRODUCT\n+    if (_vloop.mptrace().is_trace_parsing()) {\n+      tty->print_cr(\"VPointer::make_with_iv_offset:\");\n+      tty->print(\"  old: \"); print_on(tty);\n+      tty->print(\"  new: \"); p.print_on(tty);\n+    }\n+#endif\n+    return p;\n+  }\n@@ -939,1 +826,6 @@\n-  Node* maybe_negate_invar(bool negate, Node* invar);\n+  \/\/ Accessors\n+  bool is_valid()                 const { return _is_valid; }\n+  const MemPointer& mem_pointer() const { assert(_is_valid, \"must be valid\"); return _mem_pointer; }\n+  jint size()                     const { assert(_is_valid, \"must be valid\"); return mem_pointer().size(); }\n+  jint iv_scale()                 const { assert(_is_valid, \"must be valid\"); return _iv_scale; }\n+  jint con()                      const { return mem_pointer().con().value(); }\n@@ -941,1 +833,10 @@\n-  void maybe_add_to_invar(Node* new_invar, bool negate);\n+  template<typename Callback>\n+  void for_each_invar_summand(Callback callback) const {\n+    mem_pointer().for_each_non_empty_summand([&] (const MemPointerSummand& s) {\n+      Node* variable = s.variable();\n+      if (variable != mem_pointer().base().object_or_native() &&\n+          _vloop.is_pre_loop_invariant(variable)) {\n+        callback(s);\n+      }\n+    });\n+  }\n@@ -943,5 +844,15 @@\n-  static bool try_AddI_no_overflow(int offset1, int offset2, int& result);\n-  static bool try_SubI_no_overflow(int offset1, int offset2, int& result);\n-  static bool try_AddSubI_no_overflow(int offset1, int offset2, bool is_sub, int& result);\n-  static bool try_LShiftI_no_overflow(int offset1, int offset2, int& result);\n-  static bool try_MulI_no_overflow(int offset1, int offset2, int& result);\n+  \/\/ Greatest common factor among the scales of the invar_summands.\n+  \/\/ Out of simplicity, we only factor out positive powers-of-2,\n+  \/\/ between (inclusive) 1 and ObjectAlignmentInBytes. If the invar\n+  \/\/ is empty, i.e. there is no summand in invar_summands, we return 0.\n+  jint compute_invar_factor() const {\n+    jint factor = ObjectAlignmentInBytes;\n+    int invar_count = 0;\n+    for_each_invar_summand([&] (const MemPointerSummand& s) {\n+      invar_count++;\n+      while (!s.scale().is_multiple_of(NoOverflowInt(factor))) {\n+        factor = factor \/ 2;\n+      }\n+    });\n+    return invar_count > 0 ? factor : 0;\n+  }\n@@ -949,2 +860,13 @@\n-  Node* register_if_new(Node* n) const;\n-};\n+  bool has_invar_summands() const {\n+    int invar_count = 0;\n+    for_each_invar_summand([&] (const MemPointerSummand& s) {\n+      invar_count++;\n+    });\n+    return invar_count > 0;\n+  }\n+\n+  \/\/ If we have the same invar_summands, and the same iv summand with the same iv_scale,\n+  \/\/ then all summands except the base must be the same.\n+  bool has_same_invar_summands_and_iv_scale_as(const VPointer& other) const {\n+    return mem_pointer().has_same_non_base_summands_as(other.mem_pointer());\n+  }\n@@ -953,6 +875,13 @@\n-\/\/ Vector element size statistics for loop vectorization with vector masks\n-class VectorElementSizeStats {\n- private:\n-  static const int NO_SIZE = -1;\n-  static const int MIXED_SIZE = -2;\n-  int* _stats;\n+  \/\/ Delegate to MemPointer::is_adjacent_to_and_before, but guard for invalid cases\n+  \/\/ where we must return a conservative answer: unknown adjacency, return false.\n+  bool is_adjacent_to_and_before(const VPointer& other) const {\n+    if (!is_valid() || !other.is_valid()) {\n+#ifndef PRODUCT\n+      if (_vloop.mptrace().is_trace_overlap()) {\n+        tty->print_cr(\"VPointer::is_adjacent_to_and_before: invalid VPointer, adjacency unknown.\");\n+      }\n+#endif\n+      return false;\n+    }\n+    return mem_pointer().is_adjacent_to_and_before(other.mem_pointer());\n+  }\n@@ -960,3 +889,12 @@\n- public:\n-  VectorElementSizeStats(Arena* a) : _stats(NEW_ARENA_ARRAY(a, int, 4)) {\n-    clear();\n+  \/\/ Delegate to MemPointer::never_overlaps_with, but guard for invalid cases\n+  \/\/ where we must return a conservative answer: unknown overlap, return false.\n+  bool never_overlaps_with(const VPointer& other) const {\n+    if (!is_valid() || !other.is_valid()) {\n+#ifndef PRODUCT\n+      if (_vloop.mptrace().is_trace_overlap()) {\n+        tty->print_cr(\"VPointer::never_overlaps_with: invalid VPointer, overlap unknown.\");\n+      }\n+#endif\n+      return false;\n+    }\n+    return mem_pointer().never_overlaps_with(other.mem_pointer());\n@@ -965,1 +903,1 @@\n-  void clear() { memset(_stats, 0, sizeof(int) * 4); }\n+  NOT_PRODUCT( void print_on(outputStream* st, bool end_with_cr = true) const; )\n@@ -967,3 +905,11 @@\n-  void record_size(int size) {\n-    assert(1 <= size && size <= 8 && is_power_of_2(size), \"Illegal size\");\n-    _stats[exact_log2(size)]++;\n+private:\n+  jint init_iv_scale() const {\n+    for (uint i = 0; i < MemPointer::SUMMANDS_SIZE; i++) {\n+      const MemPointerSummand& summand = _mem_pointer.summands_at(i);\n+      Node* variable = summand.variable();\n+      if (variable == _vloop.iv()) {\n+        return summand.scale().value();\n+      }\n+    }\n+    \/\/ No summand with variable == iv.\n+    return 0;\n@@ -972,3 +918,5 @@\n-  int count_size(int size) {\n-    assert(1 <= size && size <= 8 && is_power_of_2(size), \"Illegal size\");\n-    return _stats[exact_log2(size)];\n+  \/\/ Check the conditions for a \"valid\" VPointer.\n+  bool init_is_valid() const {\n+    return init_is_base_known() &&\n+           init_are_non_iv_summands_pre_loop_invariant() &&\n+           init_are_scale_and_stride_not_too_large();\n@@ -977,3 +925,9 @@\n-  int smallest_size() {\n-    for (int i = 0; i <= 3; i++) {\n-      if (_stats[i] > 0) return (1 << i);\n+  \/\/ VPointer needs to know if it is native (off-heap) or object (on-heap).\n+  \/\/ We may for example have failed to fully decompose the MemPointer, possibly\n+  \/\/ because such a decomposition is not considered safe.\n+  bool init_is_base_known() const {\n+    if (_mem_pointer.base().is_known()) { return true; }\n+\n+#ifndef PRODUCT\n+    if (_vloop.mptrace().is_trace_parsing()) {\n+      tty->print_cr(\"VPointer::init_is_valid: base not known.\");\n@@ -981,1 +935,2 @@\n-    return NO_SIZE;\n+#endif\n+    return false;\n@@ -984,3 +939,17 @@\n-  int largest_size() {\n-    for (int i = 3; i >= 0; i--) {\n-      if (_stats[i] > 0) return (1 << i);\n+  \/\/ All summands, except the iv-summand must be pre-loop invariant. This is necessary\n+  \/\/ so that we can use the variables in checks inside or before the pre-loop, e.g. for\n+  \/\/ alignment.\n+  bool init_are_non_iv_summands_pre_loop_invariant() const {\n+    for (uint i = 0; i < MemPointer::SUMMANDS_SIZE; i++) {\n+      const MemPointerSummand& summand = _mem_pointer.summands_at(i);\n+      Node* variable = summand.variable();\n+      if (variable != nullptr && variable != _vloop.iv() && !_vloop.is_pre_loop_invariant(variable)) {\n+#ifndef PRODUCT\n+        if (_vloop.mptrace().is_trace_parsing()) {\n+          tty->print(\"VPointer::init_is_valid: summand is not pre-loop invariant: \");\n+          summand.print_on(tty);\n+          tty->cr();\n+        }\n+#endif\n+        return false;\n+      }\n@@ -988,1 +957,1 @@\n-    return NO_SIZE;\n+    return true;\n@@ -991,4 +960,24 @@\n-  int unique_size() {\n-    int small = smallest_size();\n-    int large = largest_size();\n-    return (small == large) ? small : MIXED_SIZE;\n+  \/\/ In the pointer analysis, and especially the AlignVector analysis, we assume that\n+  \/\/ stride and scale are not too large. For example, we multiply \"iv_scale * iv_stride\",\n+  \/\/ and assume that this does not overflow the int range. We also take \"abs(iv_scale)\"\n+  \/\/ and \"abs(iv_stride)\", which would overflow for min_int = -(2^31). Still, we want\n+  \/\/ to at least allow small and moderately large stride and scale. Therefore, we\n+  \/\/ allow values up to 2^30, which is only a factor 2 smaller than the max\/min int.\n+  \/\/ Normal performance relevant code will have much lower values. And the restriction\n+  \/\/ allows us to keep the rest of the autovectorization code much simpler, since we\n+  \/\/ do not have to deal with overflows.\n+  bool init_are_scale_and_stride_not_too_large() const {\n+    jlong long_iv_scale  = _iv_scale;\n+    jlong long_iv_stride = _vloop.iv_stride();\n+    jlong max_val = 1 << 30;\n+    if (abs(long_iv_scale) >= max_val ||\n+        abs(long_iv_stride) >= max_val ||\n+        abs(long_iv_scale * long_iv_stride) >= max_val) {\n+#ifndef PRODUCT\n+      if (_vloop.mptrace().is_trace_parsing()) {\n+        tty->print_cr(\"VPointer::init_is_valid: scale or stride too large.\");\n+      }\n+#endif\n+      return false;\n+    }\n+    return true;\n@@ -1001,3 +990,3 @@\n-\/\/   adr = base + offset + invar + scale * init\n-\/\/                               + scale * pre_stride * pre_iter\n-\/\/                               + scale * main_stride * main_iter\n+\/\/   adr = base + invar + iv_scale * init                      + con\n+\/\/                      + iv_scale * pre_stride * pre_iter\n+\/\/                      + iv_scale * main_stride * main_iter\n@@ -1012,3 +1001,3 @@\n-\/\/   pre_iter = m * q + r                                    (for any integer m)\n-\/\/                   [- invar \/ (scale * pre_stride)  ]      (if there is an invariant)\n-\/\/                   [- init \/ pre_stride             ]      (if init is variable)\n+\/\/   pre_iter = m * q + r                                       (for any integer m)\n+\/\/                   [- invar \/ (iv_scale * pre_stride)  ]      (if there is an invariant)\n+\/\/                   [- init \/ pre_stride                ]      (if init is variable)\n@@ -1040,1 +1029,1 @@\n-  virtual void print() const = 0;\n+  DEBUG_ONLY( virtual void print() const = 0; )\n@@ -1074,0 +1063,1 @@\n+#ifndef PRODUCT\n@@ -1077,0 +1067,1 @@\n+#endif\n@@ -1097,0 +1088,1 @@\n+#ifndef PRODUCT\n@@ -1100,0 +1092,1 @@\n+#endif\n@@ -1107,2 +1100,2 @@\n-  const Node* _invar;\n-  const int _scale;\n+  \/\/ Use VPointer for invar and iv_scale\n+  const VPointer& _vpointer;\n@@ -1113,2 +1106,1 @@\n-                               const Node* invar,\n-                               int scale) :\n+                               const VPointer& vpointer) :\n@@ -1118,2 +1110,2 @@\n-      _invar(invar),\n-      _scale(scale) {\n+      _vpointer(vpointer)\n+  {\n@@ -1130,0 +1122,1 @@\n+  const VPointer& vpointer() const { return _vpointer; }\n@@ -1153,2 +1146,2 @@\n-    \/\/                     [- invar1 \/ (scale1 * pre_stride)  ]\n-    \/\/                     [- init \/ pre_stride               ]\n+    \/\/                     [- invar1 \/ (iv_scale1 * pre_stride)  ]\n+    \/\/                     [- init \/ pre_stride                  ]\n@@ -1157,2 +1150,2 @@\n-    \/\/                     [- invar2 \/ (scale2 * pre_stride)  ]\n-    \/\/                     [- init \/ pre_stride               ]\n+    \/\/                     [- invar2 \/ (iv_scale2 * pre_stride)  ]\n+    \/\/                     [- init \/ pre_stride                  ]\n@@ -1167,1 +1160,1 @@\n-    \/\/   - both mem_refs have the same invariant and the same scale.\n+    \/\/   - both mem_refs have the same invariant and the same iv_scale.\n@@ -1169,5 +1162,7 @@\n-    if (s1->_invar != s2->_invar) {\n-      return new EmptyAlignmentSolution(\"invar not identical\");\n-    }\n-    if (s1->_invar != nullptr && s1->_scale != s2->_scale) {\n-      return new EmptyAlignmentSolution(\"has invar with different scale\");\n+    \/\/ Use VPointer to do checks on invar and iv_scale:\n+    const VPointer& p1 = s1->vpointer();\n+    const VPointer& p2 = s2->vpointer();\n+    bool both_no_invar = !p1.has_invar_summands() &&\n+                         !p2.has_invar_summands();\n+    if(!both_no_invar && !p1.has_same_invar_summands_and_iv_scale_as(p2)) {\n+      return new EmptyAlignmentSolution(\"invar alignment term not identical\");\n@@ -1212,0 +1207,1 @@\n+#ifndef PRODUCT\n@@ -1214,2 +1210,11 @@\n-    if (_invar != nullptr) {\n-      tty->print(\" - invar[%d] \/ (scale(%d) * pre_stride)\", _invar->_idx, _scale);\n+    if (_vpointer.has_invar_summands()) {\n+      tty->print(\" - invar(\");\n+      int count = 0;\n+      _vpointer.for_each_invar_summand([&] (const MemPointerSummand& s) {\n+        if (count > 0) {\n+          tty->print(\" + \");\n+        }\n+        s.print_on(tty);\n+        count++;\n+      });\n+      tty->print(\") \/ (iv_scale(%d) * pre_stride)\", _vpointer.iv_scale());\n@@ -1219,0 +1224,1 @@\n+#endif\n@@ -1233,2 +1239,2 @@\n-\/\/   adr = base + offset + invar + scale * iv\n-\/\/   adr = base + offset + invar + scale * (init + i * pre_stride)\n+\/\/   adr = base + invar + iv_scale * iv                      + con\n+\/\/   adr = base + invar + iv_scale * (init + i * pre_stride) + con\n@@ -1248,1 +1254,1 @@\n-\/\/   adr = base + offset + invar + scale * iv \/\/ must be aligned\n+\/\/   adr = base + invar + iv_scale * iv + con \/\/ must be aligned\n@@ -1260,0 +1266,2 @@\n+  const VPointer& _vpointer;\n+\n@@ -1261,2 +1269,0 @@\n-  const uint     _vector_length; \/\/ number of elements in vector\n-  const int      _element_size;\n@@ -1268,1 +1274,1 @@\n-  \/\/ aligned. For example, the relative offset between two arrays is only guaranteed to\n+  \/\/ aligned. For example, the relative distance between two arrays is only guaranteed to\n@@ -1278,1 +1284,1 @@\n-  \/\/   adr = base + offset + invar + scale * iv\n+  \/\/   adr = base + invar + iv_scale * iv + con\n@@ -1287,5 +1293,0 @@\n-  const Node*    _base;           \/\/ base of address (e.g. Java array object, aw-aligned)\n-  const int      _offset;\n-  const Node*    _invar;\n-  const int      _invar_factor;   \/\/ known constant factor of invar\n-  const int      _scale;\n@@ -1304,1 +1305,2 @@\n-  AlignmentSolver(const MemNode* mem_ref,\n+  AlignmentSolver(const VPointer& vpointer,\n+                  const MemNode* mem_ref,\n@@ -1306,5 +1308,0 @@\n-                  const Node* base,\n-                  const int offset,\n-                  const Node* invar,\n-                  const int invar_factor,\n-                  const int scale,\n@@ -1316,0 +1313,1 @@\n+      _vpointer(          vpointer),\n@@ -1317,3 +1315,1 @@\n-      _vector_length(     vector_length),\n-      _element_size(      _mem_ref->memory_size()),\n-      _vector_width(      _vector_length * _element_size),\n+      _vector_width(      vector_length * vpointer.size()),\n@@ -1321,5 +1317,0 @@\n-      _base(              base),\n-      _offset(            offset),\n-      _invar(             invar),\n-      _invar_factor(      invar_factor),\n-      _scale(             scale),\n@@ -1339,0 +1330,3 @@\n+  MemPointer::Base base() const { return _vpointer.mem_pointer().base();}\n+  jint iv_scale() const { return _vpointer.iv_scale(); }\n+\n","filename":"src\/hotspot\/share\/opto\/vectorization.hpp","additions":335,"deletions":341,"binary":false,"changes":676,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2024, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -147,5 +147,0 @@\n-\/\/ We use two comparisons, because a subtraction could underflow.\n-#define RETURN_CMP_VALUE_IF_NOT_EQUAL(a, b) \\\n-  if (a < b) { return -1; }                 \\\n-  if (a > b) { return  1; }\n-\n@@ -153,2 +148,5 @@\n-\/\/ It represents a memory region: [ptr, ptr + memory_size)\n-class VMemoryRegion : public StackObj {\n+\/\/ It wraps a VPointer. The VPointer have an iv_offset applied, which\n+\/\/ simulates a virtual unrolling. They represent the memory region:\n+\/\/   [adr, adr + size)\n+\/\/   adr = base + invar + iv_scale * (iv + iv_offset) + con\n+class VMemoryRegion : public ResourceObj {\n@@ -156,5 +154,5 @@\n-  Node* _base;        \/\/ ptr = base + offset + invar + scale * iv\n-  int _scale;\n-  Node* _invar;\n-  int _offset;\n-  uint _memory_size;\n+  \/\/ Note: VPointer has no default constructor, so we cannot use VMemoryRegion\n+  \/\/       in-place in a GrowableArray. Hence, we make VMemoryRegion a resource\n+  \/\/       allocated object, so the GrowableArray of VMemoryRegion* has a default\n+  \/\/       nullptr element.\n+  const VPointer _vpointer;\n@@ -165,8 +163,3 @@\n-  VMemoryRegion() {} \/\/ empty constructor for GrowableArray\n-  VMemoryRegion(const VPointer& vpointer, int iv_offset, int vector_length, uint schedule_order) :\n-    _base(vpointer.base()),\n-    _scale(vpointer.scale_in_bytes()),\n-    _invar(vpointer.invar()),\n-    _offset(vpointer.offset_in_bytes() + _scale * iv_offset),\n-    _memory_size(vpointer.memory_size() * vector_length),\n-    _is_load(vpointer.mem()->is_Load()),\n+  VMemoryRegion(const VPointer& vpointer, bool is_load, uint schedule_order) :\n+    _vpointer(vpointer),\n+    _is_load(is_load),\n@@ -175,5 +168,1 @@\n-    Node* base()          const { return _base; }\n-    int scale()           const { return _scale; }\n-    Node* invar()         const { return _invar; }\n-    int offset()          const { return _offset; }\n-    uint memory_size()    const { return _memory_size; }\n+    const VPointer& vpointer() const { return _vpointer; }\n@@ -184,6 +173,3 @@\n-      RETURN_CMP_VALUE_IF_NOT_EQUAL(r1->base()->_idx, r2->base()->_idx);\n-      RETURN_CMP_VALUE_IF_NOT_EQUAL(r1->scale(),      r2->scale());\n-      int r1_invar_idx = r1->invar() == nullptr ? 0 : r1->invar()->_idx;\n-      int r2_invar_idx = r2->invar() == nullptr ? 0 : r2->invar()->_idx;\n-      RETURN_CMP_VALUE_IF_NOT_EQUAL(r1_invar_idx,      r2_invar_idx);\n-      return 0; \/\/ equal\n+      \/\/ Sort by mem_pointer (base, invar, iv_scale), except for the con.\n+      return MemPointer::cmp_summands(r1->vpointer().mem_pointer(),\n+                                      r2->vpointer().mem_pointer());\n@@ -192,2 +178,2 @@\n-    static int cmp_for_sort(VMemoryRegion* r1, VMemoryRegion* r2) {\n-      int cmp_group = cmp_for_sort_by_group(r1, r2);\n+    static int cmp_for_sort(VMemoryRegion** r1, VMemoryRegion** r2) {\n+      int cmp_group = cmp_for_sort_by_group(*r1, *r2);\n@@ -196,2 +182,6 @@\n-      RETURN_CMP_VALUE_IF_NOT_EQUAL(r1->offset(),     r2->offset());\n-      return 0; \/\/ equal\n+      \/\/ We use two comparisons, because a subtraction could underflow.\n+      jint con1 = (*r1)->vpointer().con();\n+      jint con2 = (*r2)->vpointer().con();\n+      if (con1 < con2) { return -1; }\n+      if (con1 > con2) { return  1; }\n+      return 0;\n@@ -207,4 +197,4 @@\n-      jlong offset1 = p1->offset();\n-      jlong offset2 = p2->offset();\n-      jlong memory_size1 = p1->memory_size();\n-      jlong memory_size2 = p2->memory_size();\n+      jlong con1 = p1->vpointer().con();\n+      jlong con2 = p2->vpointer().con();\n+      jlong size1 = p1->vpointer().size();\n+      jlong size2 = p2->vpointer().size();\n@@ -212,3 +202,3 @@\n-      if (offset1 >= offset2 + memory_size2) { return AFTER; }\n-      if (offset2 >= offset1 + memory_size1) { return BEFORE; }\n-      if (offset1 == offset2 && memory_size1 == memory_size2) { return EXACT_OVERLAP; }\n+      if (con1 >= con2 + size2) { return AFTER; }\n+      if (con2 >= con1 + size1) { return BEFORE; }\n+      if (con1 == con2 && size1 == size2) { return EXACT_OVERLAP; }\n@@ -220,7 +210,4 @@\n-    tty->print(\"VMemoryRegion[%s %dbytes, schedule_order(%4d), base\",\n-               _is_load ? \"load \" : \"store\", _memory_size, _schedule_order);\n-    VPointer::print_con_or_idx(_base);\n-    tty->print(\" + offset(%4d)\", _offset);\n-    tty->print(\" + invar\");\n-    VPointer::print_con_or_idx(_invar);\n-    tty->print_cr(\" + scale(%4d) * iv]\", _scale);\n+    tty->print(\"VMemoryRegion[%s schedule_order(%4d), \",\n+               _is_load ? \"load, \" : \"store,\", _schedule_order);\n+    vpointer().print_on(tty, false);\n+    tty->print_cr(\"]\");\n@@ -332,1 +319,2 @@\n-  GrowableArray<VMemoryRegion> memory_regions;\n+  \/\/ Use pointers because no default constructor for elements available.\n+  GrowableArray<VMemoryRegion*> memory_regions;\n@@ -354,1 +342,1 @@\n-        if (p.valid()) {\n+        if (p.is_valid()) {\n@@ -356,2 +344,7 @@\n-          uint vector_length = vector != nullptr ? vector->nodes().length() : 1;\n-          memory_regions.push(VMemoryRegion(p, iv_offset, vector_length, schedule_order++));\n+          bool is_load = vtn->is_load_in_loop();\n+          const VPointer iv_offset_p(p.make_with_iv_offset(iv_offset));\n+          if (iv_offset_p.is_valid()) {\n+            \/\/ The iv_offset may lead to overflows. This is a heuristic, so we do not\n+            \/\/ care too much about those edge cases.\n+            memory_regions.push(new VMemoryRegion(iv_offset_p, is_load, schedule_order++));\n+          }\n@@ -372,1 +365,1 @@\n-      VMemoryRegion& region = memory_regions.at(i);\n+      VMemoryRegion& region = *memory_regions.at(i);\n@@ -380,1 +373,1 @@\n-    VMemoryRegion& region1 = memory_regions.at(i);\n+    VMemoryRegion& region1 = *memory_regions.at(i);\n@@ -383,1 +376,1 @@\n-      VMemoryRegion& region2 = memory_regions.at(j);\n+      VMemoryRegion& region2 = *memory_regions.at(j);\n@@ -578,0 +571,1 @@\n+  const VPointer& load_p = vpointer(vloop_analyzer);\n@@ -579,4 +573,2 @@\n-    VPointer p_store(mem->as_Mem(), vloop_analyzer.vloop());\n-    if (p_store.overlap_possible_with_any_in(nodes())) {\n-      break;\n-    } else {\n+    VPointer store_p(mem->as_Mem(), vloop_analyzer.vloop());\n+    if (store_p.never_overlaps_with(load_p)) {\n@@ -584,0 +576,2 @@\n+    } else {\n+      break;\n","filename":"src\/hotspot\/share\/opto\/vtransform.cpp","additions":55,"deletions":61,"binary":false,"changes":116,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2024, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -69,0 +69,1 @@\n+class VTransformMemVectorNode;\n@@ -317,0 +318,1 @@\n+  virtual VTransformMemVectorNode* isa_MemVector() { return nullptr; }\n@@ -320,0 +322,1 @@\n+  virtual bool is_load_in_loop() const { return false; }\n@@ -345,0 +348,1 @@\n+  virtual bool is_load_in_loop() const override { return _node->is_Load(); }\n@@ -361,0 +365,1 @@\n+  virtual bool is_load_in_loop() const override { return false; }\n@@ -481,1 +486,15 @@\n-class VTransformLoadVectorNode : public VTransformVectorNode {\n+class VTransformMemVectorNode : public VTransformVectorNode {\n+private:\n+  const VPointer _vpointer; \/\/ with size of the vector\n+\n+public:\n+  VTransformMemVectorNode(VTransform& vtransform, const uint req, uint number_of_nodes, const VPointer& vpointer) :\n+    VTransformVectorNode(vtransform, req, number_of_nodes),\n+    _vpointer(vpointer) {}\n+\n+  virtual VTransformMemVectorNode* isa_MemVector() override { return this; }\n+  virtual bool is_load_or_store_in_loop() const override { return true; }\n+  virtual const VPointer& vpointer(const VLoopAnalyzer& vloop_analyzer) const override { return _vpointer; }\n+};\n+\n+class VTransformLoadVectorNode : public VTransformMemVectorNode {\n@@ -484,2 +503,2 @@\n-  VTransformLoadVectorNode(VTransform& vtransform, uint number_of_nodes) :\n-    VTransformVectorNode(vtransform, 3, number_of_nodes) {}\n+  VTransformLoadVectorNode(VTransform& vtransform, uint number_of_nodes, const VPointer& vpointer) :\n+    VTransformMemVectorNode(vtransform, 3, number_of_nodes, vpointer) {}\n@@ -488,2 +507,1 @@\n-  virtual bool is_load_or_store_in_loop() const override { return true; }\n-  virtual const VPointer& vpointer(const VLoopAnalyzer& vloop_analyzer) const override { return vloop_analyzer.vpointers().vpointer(nodes().at(0)->as_Mem()); }\n+  virtual bool is_load_in_loop() const override { return true; }\n@@ -495,1 +513,1 @@\n-class VTransformStoreVectorNode : public VTransformVectorNode {\n+class VTransformStoreVectorNode : public VTransformMemVectorNode {\n@@ -498,2 +516,2 @@\n-  VTransformStoreVectorNode(VTransform& vtransform, uint number_of_nodes) :\n-    VTransformVectorNode(vtransform, 4, number_of_nodes) {}\n+  VTransformStoreVectorNode(VTransform& vtransform, uint number_of_nodes, const VPointer& vpointer) :\n+    VTransformMemVectorNode(vtransform, 4, number_of_nodes, vpointer) {}\n@@ -501,2 +519,1 @@\n-  virtual bool is_load_or_store_in_loop() const override { return true; }\n-  virtual const VPointer& vpointer(const VLoopAnalyzer& vloop_analyzer) const override { return vloop_analyzer.vpointers().vpointer(nodes().at(0)->as_Mem()); }\n+  virtual bool is_load_in_loop() const override { return false; }\n","filename":"src\/hotspot\/share\/opto\/vtransform.hpp","additions":28,"deletions":11,"binary":false,"changes":39,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2024, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -1042,3 +1042,3 @@\n-    @IR(counts = {IRNode.LOAD_VECTOR_B, \"= 0\",\n-                  IRNode.AND_VB, \"= 0\",\n-                  IRNode.STORE_VECTOR, \"= 0\"},\n+    @IR(counts = {IRNode.LOAD_VECTOR_B, IRNode.VECTOR_SIZE + \"min(max_byte, 4)\", \"> 0\",\n+                  IRNode.AND_VB,        IRNode.VECTOR_SIZE + \"min(max_byte, 4)\", \"> 0\",\n+                  IRNode.STORE_VECTOR,                                           \"> 0\"},\n@@ -1046,0 +1046,1 @@\n+        applyIf = {\"AlignVector\", \"false\"},\n@@ -1049,1 +1050,1 @@\n-            \/\/ Currently does not vectorize at all\n+            \/\/ Non-power-of-2 stride. Vectorization of 4 bytes, then 2-bytes gap.\n","filename":"test\/hotspot\/jtreg\/compiler\/loopopts\/superword\/TestAlignVector.java","additions":6,"deletions":5,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -0,0 +1,951 @@\n+\/*\n+ * Copyright (c) 2025, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+package compiler.loopopts.superword;\n+\n+import compiler.lib.ir_framework.*;\n+import jdk.test.lib.Utils;\n+import jdk.internal.misc.Unsafe;\n+import java.lang.reflect.Array;\n+import java.util.Map;\n+import java.util.HashMap;\n+import java.util.Random;\n+import java.lang.foreign.*;\n+\n+\/*\n+ * @test\n+ * @bug 8343685 8330274\n+ * @summary Test vectorization with various invariants that are equivalent, but not trivially so,\n+ *          i.e. where the invariants have the same summands, but in a different order.\n+ * @modules java.base\/jdk.internal.misc\n+ * @library \/test\/lib \/\n+ * @run driver compiler.loopopts.superword.TestEquivalentInvariants\n+ *\/\n+\n+public class TestEquivalentInvariants {\n+    static int RANGE = 1024*64;\n+    private static final Unsafe UNSAFE = Unsafe.getUnsafe();\n+    private static final Random RANDOM = Utils.getRandomInstance();\n+\n+    \/\/ Inputs\n+    byte[] aB;\n+    byte[] bB;\n+    int[] aI;\n+    int[] bI;\n+    long[] aL;\n+    long[] bL;\n+\n+    \/\/ List of tests\n+    Map<String,TestFunction> tests = new HashMap<String,TestFunction>();\n+\n+    \/\/ List of gold, the results from the first run before compilation\n+    Map<String,Object[]> golds = new HashMap<String,Object[]>();\n+\n+    interface TestFunction {\n+        Object[] run();\n+    }\n+\n+    public static void main(String[] args) {\n+        TestFramework.runWithFlags(\"--add-modules\", \"java.base\", \"--add-exports\", \"java.base\/jdk.internal.misc=ALL-UNNAMED\",\n+                                   \"-XX:-AlignVector\");\n+        TestFramework.runWithFlags(\"--add-modules\", \"java.base\", \"--add-exports\", \"java.base\/jdk.internal.misc=ALL-UNNAMED\",\n+                                   \"-XX:+AlignVector\");\n+    }\n+\n+    public TestEquivalentInvariants() {\n+        \/\/ Generate input once\n+        aB = generateB();\n+        bB = generateB();\n+        aI = generateI();\n+        bI = generateI();\n+        aL = generateL();\n+        bL = generateL();\n+\n+        \/\/ Add all tests to list\n+        tests.put(\"testArrayBB\", () -> {\n+          return testArrayBB(aB.clone(), bB.clone());\n+        });\n+        tests.put(\"testArrayBBInvar3\", () -> {\n+          return testArrayBBInvar3(aB.clone(), bB.clone(), 0, 0, 0);\n+        });\n+        tests.put(\"testMemorySegmentB\", () -> {\n+          MemorySegment data = MemorySegment.ofArray(aB.clone());\n+          return testMemorySegmentB(data);\n+        });\n+        tests.put(\"testMemorySegmentBInvarI\", () -> {\n+          MemorySegment data = MemorySegment.ofArray(aB.clone());\n+          return testMemorySegmentBInvarI(data, 101, RANGE-200);\n+        });\n+        tests.put(\"testMemorySegmentBInvarL\", () -> {\n+          MemorySegment data = MemorySegment.ofArray(aB.clone());\n+          return testMemorySegmentBInvarL(data, 101, RANGE-200);\n+        });\n+        tests.put(\"testMemorySegmentBInvarIAdr\", () -> {\n+          MemorySegment data = MemorySegment.ofArray(aB.clone());\n+          return testMemorySegmentBInvarIAdr(data, 101, RANGE-200);\n+        });\n+        tests.put(\"testMemorySegmentBInvarLAdr\", () -> {\n+          MemorySegment data = MemorySegment.ofArray(aB.clone());\n+          return testMemorySegmentBInvarLAdr(data, 101, RANGE-200);\n+        });\n+        tests.put(\"testMemorySegmentBInvarI3a\", () -> {\n+          MemorySegment data = MemorySegment.ofArray(aB.clone());\n+          return testMemorySegmentBInvarI3a(data, 1, 2, 3, RANGE-200);\n+        });\n+        tests.put(\"testMemorySegmentBInvarI3b\", () -> {\n+          MemorySegment data = MemorySegment.ofArray(aB.clone());\n+          return testMemorySegmentBInvarI3b(data, 1, 2, 3, RANGE-200);\n+        });\n+        tests.put(\"testMemorySegmentBInvarI3c\", () -> {\n+          MemorySegment data = MemorySegment.ofArray(aB.clone());\n+          return testMemorySegmentBInvarI3c(data, 1, 2, 3, RANGE-200);\n+        });\n+        tests.put(\"testMemorySegmentBInvarI3d\", () -> {\n+          MemorySegment data = MemorySegment.ofArray(aB.clone());\n+          return testMemorySegmentBInvarI3d(data, 1, 2, 3, RANGE-200);\n+        });\n+        tests.put(\"testMemorySegmentBInvarI3e\", () -> {\n+          MemorySegment data = MemorySegment.ofArray(aB.clone());\n+          return testMemorySegmentBInvarI3e(data, 1, 2, 3, RANGE-200);\n+        });\n+        tests.put(\"testMemorySegmentBInvarI3f\", () -> {\n+          MemorySegment data = MemorySegment.ofArray(aB.clone());\n+          return testMemorySegmentBInvarI3f(data, 1, 2, 3, RANGE-200);\n+        });\n+        tests.put(\"testMemorySegmentBInvarL3g\", () -> {\n+          MemorySegment data = MemorySegment.ofArray(aB.clone());\n+          return testMemorySegmentBInvarL3g(data, 1, 2, 3, RANGE-200);\n+        });\n+        tests.put(\"testMemorySegmentBInvarL3h\", () -> {\n+          MemorySegment data = MemorySegment.ofArray(aB.clone());\n+          return testMemorySegmentBInvarL3h(data, -1, -2, -3, RANGE-200);\n+        });\n+        tests.put(\"testMemorySegmentBInvarL3k\", () -> {\n+          MemorySegment data = MemorySegment.ofArray(aB.clone());\n+          return testMemorySegmentBInvarL3k(data, 1, 2, 3, RANGE-200);\n+        });\n+        tests.put(\"testMemorySegmentIInvarL3a\", () -> {\n+          MemorySegment data = MemorySegment.ofArray(aI.clone());\n+          return testMemorySegmentIInvarL3a(data, 1, 2, 3, RANGE-200);\n+        });\n+        tests.put(\"testMemorySegmentIInvarL3b\", () -> {\n+          MemorySegment data = MemorySegment.ofArray(aI.clone());\n+          return testMemorySegmentIInvarL3b(data, -1, -2, -3, RANGE-200);\n+        });\n+        tests.put(\"testMemorySegmentIInvarL3c\", () -> {\n+          MemorySegment data = MemorySegment.ofArray(aI.clone());\n+          return testMemorySegmentIInvarL3c(data, 1, 2, 3, RANGE-200);\n+        });\n+        tests.put(\"testMemorySegmentIInvarL3d\", () -> {\n+          MemorySegment data = MemorySegment.ofArray(aI.clone());\n+          return testMemorySegmentIInvarL3d(data, 1, 2, 3, RANGE-200);\n+        });\n+        tests.put(\"testMemorySegmentIInvarL3d2\", () -> {\n+          MemorySegment data = MemorySegment.ofArray(aI.clone());\n+          return testMemorySegmentIInvarL3d2(data, 1, 2, 3, RANGE-200);\n+        });\n+        tests.put(\"testMemorySegmentIInvarL3d3\", () -> {\n+          MemorySegment data = MemorySegment.ofArray(aI.clone());\n+          return testMemorySegmentIInvarL3d3(data, RANGE-200);\n+        });\n+        tests.put(\"testMemorySegmentIInvarL3e\", () -> {\n+          MemorySegment data = MemorySegment.ofArray(aI.clone());\n+          return testMemorySegmentIInvarL3e(data, 1, 2, 3, RANGE-200);\n+        });\n+        tests.put(\"testMemorySegmentIInvarL3f\", () -> {\n+          MemorySegment data = MemorySegment.ofArray(aI.clone());\n+          return testMemorySegmentIInvarL3f(data, 1, 2, 3, RANGE-200);\n+        });\n+        tests.put(\"testMemorySegmentLInvarL3a\", () -> {\n+          MemorySegment data = MemorySegment.ofArray(aL.clone());\n+          return testMemorySegmentLInvarL3a(data, 1, 2, 3, RANGE-200);\n+        });\n+        tests.put(\"testMemorySegmentLInvarL3b\", () -> {\n+          MemorySegment data = MemorySegment.ofArray(aL.clone());\n+          return testMemorySegmentLInvarL3b(data, -1, -2, -3, RANGE-200);\n+        });\n+        tests.put(\"testMemorySegmentLInvarL3c\", () -> {\n+          MemorySegment data = MemorySegment.ofArray(aL.clone());\n+          return testMemorySegmentLInvarL3c(data, 1, 2, 3, RANGE-200);\n+        });\n+        tests.put(\"testMemorySegmentLInvarL3d\", () -> {\n+          MemorySegment data = MemorySegment.ofArray(aL.clone());\n+          return testMemorySegmentLInvarL3d(data, 1, 2, 3, RANGE-200);\n+        });\n+        tests.put(\"testMemorySegmentLInvarL3d2\", () -> {\n+          MemorySegment data = MemorySegment.ofArray(aL.clone());\n+          return testMemorySegmentLInvarL3d2(data, 1, 2, 3, RANGE-200);\n+        });\n+        tests.put(\"testMemorySegmentLInvarL3d3\", () -> {\n+          MemorySegment data = MemorySegment.ofArray(aL.clone());\n+          return testMemorySegmentLInvarL3d3(data, RANGE-200);\n+        });\n+        tests.put(\"testMemorySegmentLInvarL3e\", () -> {\n+          MemorySegment data = MemorySegment.ofArray(aL.clone());\n+          return testMemorySegmentLInvarL3e(data, 1, 2, 3, RANGE-200);\n+        });\n+        tests.put(\"testMemorySegmentLInvarL3f\", () -> {\n+          MemorySegment data = MemorySegment.ofArray(aL.clone());\n+          return testMemorySegmentLInvarL3f(data, 1, 2, 3, RANGE-200);\n+        });\n+        tests.put(\"testLargeInvariantSum\", () -> {\n+          return testLargeInvariantSum(aB.clone(), 0, 0, 0, RANGE-200);\n+        });\n+\n+        \/\/ Compute gold value for all test methods before compilation\n+        for (Map.Entry<String,TestFunction> entry : tests.entrySet()) {\n+            String name = entry.getKey();\n+            TestFunction test = entry.getValue();\n+            Object[] gold = test.run();\n+            golds.put(name, gold);\n+        }\n+    }\n+\n+    @Warmup(100)\n+    @Run(test = {\"testArrayBB\",\n+                 \"testArrayBBInvar3\",\n+                 \"testMemorySegmentB\",\n+                 \"testMemorySegmentBInvarI\",\n+                 \"testMemorySegmentBInvarL\",\n+                 \"testMemorySegmentBInvarIAdr\",\n+                 \"testMemorySegmentBInvarLAdr\",\n+                 \"testMemorySegmentBInvarI3a\",\n+                 \"testMemorySegmentBInvarI3b\",\n+                 \"testMemorySegmentBInvarI3c\",\n+                 \"testMemorySegmentBInvarI3d\",\n+                 \"testMemorySegmentBInvarI3e\",\n+                 \"testMemorySegmentBInvarI3f\",\n+                 \"testMemorySegmentBInvarL3g\",\n+                 \"testMemorySegmentBInvarL3h\",\n+                 \"testMemorySegmentBInvarL3k\",\n+                 \"testMemorySegmentIInvarL3a\",\n+                 \"testMemorySegmentIInvarL3b\",\n+                 \"testMemorySegmentIInvarL3c\",\n+                 \"testMemorySegmentIInvarL3d\",\n+                 \"testMemorySegmentIInvarL3d2\",\n+                 \"testMemorySegmentIInvarL3d3\",\n+                 \"testMemorySegmentIInvarL3e\",\n+                 \"testMemorySegmentIInvarL3f\",\n+                 \"testMemorySegmentLInvarL3a\",\n+                 \"testMemorySegmentLInvarL3b\",\n+                 \"testMemorySegmentLInvarL3c\",\n+                 \"testMemorySegmentLInvarL3d\",\n+                 \"testMemorySegmentLInvarL3d2\",\n+                 \"testMemorySegmentLInvarL3d3\",\n+                 \"testMemorySegmentLInvarL3e\",\n+                 \"testMemorySegmentLInvarL3f\",\n+                 \"testLargeInvariantSum\"})\n+    public void runTests() {\n+        for (Map.Entry<String,TestFunction> entry : tests.entrySet()) {\n+            String name = entry.getKey();\n+            TestFunction test = entry.getValue();\n+            \/\/ Recall gold value from before compilation\n+            Object[] gold = golds.get(name);\n+            \/\/ Compute new result\n+            Object[] result = test.run();\n+            \/\/ Compare gold and new result\n+            verify(name, gold, result);\n+        }\n+    }\n+\n+    static byte[] generateB() {\n+        byte[] a = new byte[RANGE];\n+        for (int i = 0; i < a.length; i++) {\n+            a[i] = (byte)RANDOM.nextInt();\n+        }\n+        return a;\n+    }\n+\n+    static short[] generateS() {\n+        short[] a = new short[RANGE];\n+        for (int i = 0; i < a.length; i++) {\n+            a[i] = (short)RANDOM.nextInt();\n+        }\n+        return a;\n+    }\n+\n+    static int[] generateI() {\n+        int[] a = new int[RANGE];\n+        for (int i = 0; i < a.length; i++) {\n+            a[i] = RANDOM.nextInt();\n+        }\n+        return a;\n+    }\n+\n+    static long[] generateL() {\n+        long[] a = new long[RANGE];\n+        for (int i = 0; i < a.length; i++) {\n+            a[i] = RANDOM.nextLong();\n+        }\n+        return a;\n+    }\n+\n+    static void verify(String name, Object[] gold, Object[] result) {\n+        if (gold.length != result.length) {\n+            throw new RuntimeException(\"verify \" + name + \": not the same number of outputs: gold.length = \" +\n+                                       gold.length + \", result.length = \" + result.length);\n+        }\n+        for (int i = 0; i < gold.length; i++) {\n+            Object g = gold[i];\n+            Object r = result[i];\n+            if (g == r) {\n+                throw new RuntimeException(\"verify \" + name + \": should be two separate objects (with identical content):\" +\n+                                           \" gold[\" + i + \"] == result[\" + i + \"]\");\n+            }\n+\n+            \/\/ Wrap everything in MemorySegments, this allows simple value verification of Array as well as MemorySegment.\n+            MemorySegment mg = null;\n+            MemorySegment mr = null;\n+            if (g.getClass().isArray()) {\n+                if (g.getClass() != r.getClass() || !g.getClass().isArray() || !r.getClass().isArray()) {\n+                    throw new RuntimeException(\"verify \" + name + \": must both be array of same type:\" +\n+                                               \" gold[\" + i + \"].getClass() = \" + g.getClass().getSimpleName() +\n+                                               \" result[\" + i + \"].getClass() = \" + r.getClass().getSimpleName());\n+                }\n+                if (Array.getLength(g) != Array.getLength(r)) {\n+                    throw new RuntimeException(\"verify \" + name + \": arrays must have same length:\" +\n+                                           \" gold[\" + i + \"].length = \" + Array.getLength(g) +\n+                                           \" result[\" + i + \"].length = \" + Array.getLength(r));\n+                }\n+                Class c = g.getClass().getComponentType();\n+                if (c == byte.class) {\n+                    mg = MemorySegment.ofArray((byte[])g);\n+                    mr = MemorySegment.ofArray((byte[])r);\n+                } else if (c == int.class) {\n+                    mg = MemorySegment.ofArray((int[])g);\n+                    mr = MemorySegment.ofArray((int[])r);\n+                } else if (c == long.class) {\n+                    mg = MemorySegment.ofArray((long[])g);\n+                    mr = MemorySegment.ofArray((long[])r);\n+                } else {\n+                    throw new RuntimeException(\"verify \" + name + \": array type not supported for verify:\" +\n+                                           \" gold[\" + i + \"].getClass() = \" + g.getClass().getSimpleName() +\n+                                           \" result[\" + i + \"].getClass() = \" + r.getClass().getSimpleName());\n+                }\n+            } else if (g instanceof MemorySegment) {\n+                mg = (MemorySegment)g;\n+                if (!(r instanceof MemorySegment)) {\n+                    throw new RuntimeException(\"verify \" + name + \": was not both MemorySegment:\" +\n+                                           \" gold[\" + i + \"].getClass() = \" + g.getClass().getSimpleName() +\n+                                           \" result[\" + i + \"].getClass() = \" + r.getClass().getSimpleName());\n+                }\n+                mr = (MemorySegment)r;\n+            }\n+\n+            if (mg.byteSize() != mr.byteSize()) {\n+                throw new RuntimeException(\"verify \" + name + \": memory segment must have same length:\" +\n+                                       \" gold[\" + i + \"].length = \" + mg.byteSize() +\n+                                       \" result[\" + i + \"].length = \" + mr.byteSize());\n+            }\n+            verifyMS(name, i, mg, mr);\n+        }\n+    }\n+\n+    static void verifyMS(String name, int i, MemorySegment g, MemorySegment r) {\n+        for (long j = 0; j < g.byteSize(); j++) {\n+            byte vg = g.get(ValueLayout.JAVA_BYTE, j);\n+            byte vr = r.get(ValueLayout.JAVA_BYTE, j);\n+            if (vg != vr) {\n+                throw new RuntimeException(\"verify \" + name + \": arrays must have same content:\" +\n+                                           \" gold[\" + i + \"][\" + j + \"] = \" + vg +\n+                                           \" result[\" + i + \"][\" + j + \"] = \" + vr);\n+            }\n+        }\n+    }\n+\n+    @Test\n+    @IR(counts = {IRNode.LOAD_VECTOR_B, \"> 0\",\n+                  IRNode.ADD_VB,        \"> 0\",\n+                  IRNode.STORE_VECTOR,  \"> 0\"},\n+        applyIfPlatform = {\"64-bit\", \"true\"},\n+        applyIfCPUFeatureOr = {\"sse4.1\", \"true\", \"asimd\", \"true\"})\n+    static Object[] testArrayBB(byte[] a, byte[] b) {\n+        for (int i = 0; i < a.length; i++) {\n+            b[i+0] = (byte)(a[i] + 1);\n+        }\n+        return new Object[]{ a, b };\n+    }\n+\n+    @Test\n+    @IR(counts = {IRNode.LOAD_VECTOR_B, \"> 0\",\n+                  IRNode.ADD_VB,        \"> 0\",\n+                  IRNode.STORE_VECTOR,  \"> 0\"},\n+        applyIfPlatform = {\"64-bit\", \"true\"},\n+        applyIfCPUFeatureOr = {\"sse4.1\", \"true\", \"asimd\", \"true\"})\n+    \/\/ Same int invariant summands, but added in a different order.\n+    static Object[] testArrayBBInvar3(byte[] a, byte[] b, int invar1, int invar2, int invar3) {\n+        int i1 = invar1 + invar2 + invar3;\n+        int i2 = invar2 + invar3 + invar1;\n+        for (int i = 0; i < a.length; i++) {\n+            b[i + i1] = (byte)(a[i + i2] + 1);\n+        }\n+        return new Object[]{ a, b };\n+    }\n+\n+    @Test\n+    @IR(counts = {IRNode.LOAD_VECTOR_B, \"> 0\",\n+                  IRNode.ADD_VB,        \"> 0\",\n+                  IRNode.STORE_VECTOR,  \"> 0\"},\n+        applyIfPlatform = {\"64-bit\", \"true\"},\n+        applyIfCPUFeatureOr = {\"sse4.1\", \"true\", \"asimd\", \"true\"})\n+    \/\/ Just a simple pattern, without any (explicit) invariant.\n+    static Object[] testMemorySegmentB(MemorySegment m) {\n+        for (int i = 0; i < (int)m.byteSize(); i++) {\n+            byte v = m.get(ValueLayout.JAVA_BYTE, i);\n+            m.set(ValueLayout.JAVA_BYTE, i, (byte)(v + 1));\n+        }\n+        return new Object[]{ m };\n+    }\n+\n+    @Test\n+    @IR(counts = {IRNode.LOAD_VECTOR_B, \"= 0\",\n+                  IRNode.STORE_VECTOR,  \"= 0\"},\n+        applyIfPlatform = {\"64-bit\", \"true\"},\n+        applyIfCPUFeatureOr = {\"sse4.1\", \"true\", \"asimd\", \"true\"})\n+    \/\/ Does not vectorize: RangeChecks are not eliminated.\n+    \/\/ Filed RFE: JDK-8327209\n+    static Object[] testMemorySegmentBInvarI(MemorySegment m, int invar, int size) {\n+        for (int i = 0; i < size; i++) {\n+            byte v = m.get(ValueLayout.JAVA_BYTE, i + invar);\n+            m.set(ValueLayout.JAVA_BYTE, i + invar, (byte)(v + 1));\n+        }\n+        return new Object[]{ m };\n+    }\n+\n+    @Test\n+    @IR(counts = {IRNode.LOAD_VECTOR_B, \"> 0\",\n+                  IRNode.ADD_VB,        \"> 0\",\n+                  IRNode.STORE_VECTOR,  \"> 0\"},\n+        applyIfPlatform = {\"64-bit\", \"true\"},\n+        applyIfCPUFeatureOr = {\"sse4.1\", \"true\", \"asimd\", \"true\"})\n+    \/\/ Has different invariants, before sorting:\n+    \/\/\n+    \/\/   3125 AddL = ((CastLL(Param 11) + ConvI2L(1460  Phi)) + 530  LoadL)\n+    \/\/   3127 AddL = (ConvI2L(1460  Phi) + (11 Param + 530  LoadL))\n+    \/\/\n+    static Object[] testMemorySegmentBInvarL(MemorySegment m, long invar, int size) {\n+        for (int i = 0; i < size; i++) {\n+            byte v = m.get(ValueLayout.JAVA_BYTE, i + invar);\n+            m.set(ValueLayout.JAVA_BYTE, i + invar, (byte)(v + 1));\n+        }\n+        return new Object[]{ m };\n+    }\n+\n+    @Test\n+    @IR(counts = {IRNode.LOAD_VECTOR_B, \"= 0\",\n+                  IRNode.STORE_VECTOR,  \"= 0\"},\n+        applyIfPlatform = {\"64-bit\", \"true\"},\n+        applyIfCPUFeatureOr = {\"sse4.1\", \"true\", \"asimd\", \"true\"})\n+    \/\/ Does not vectorize: RangeChecks are not eliminated.\n+    \/\/ Filed RFE: JDK-8327209\n+    static Object[] testMemorySegmentBInvarIAdr(MemorySegment m, int invar, int size) {\n+        for (int i = 0; i < size; i++) {\n+            long adr = i + invar;\n+            byte v = m.get(ValueLayout.JAVA_BYTE, adr);\n+            m.set(ValueLayout.JAVA_BYTE, adr, (byte)(v + 1));\n+        }\n+        return new Object[]{ m };\n+    }\n+\n+    @Test\n+    @IR(counts = {IRNode.LOAD_VECTOR_B, \"> 0\",\n+                  IRNode.ADD_VB,        \"> 0\",\n+                  IRNode.STORE_VECTOR,  \"> 0\"},\n+        applyIfPlatform = {\"64-bit\", \"true\"},\n+        applyIfCPUFeatureOr = {\"sse4.1\", \"true\", \"asimd\", \"true\"})\n+    \/\/ Since we add \"i + invar\", the invariant is already equivalent without sorting.\n+    static Object[] testMemorySegmentBInvarLAdr(MemorySegment m, long invar, int size) {\n+        for (int i = 0; i < size; i++) {\n+            long adr = i + invar;\n+            byte v = m.get(ValueLayout.JAVA_BYTE, adr);\n+            m.set(ValueLayout.JAVA_BYTE, adr, (byte)(v + 1));\n+        }\n+        return new Object[]{ m };\n+    }\n+\n+    @Test\n+    @IR(counts = {IRNode.LOAD_VECTOR_B, \"> 0\",\n+                  IRNode.ADD_VB,        \"> 0\",\n+                  IRNode.STORE_VECTOR,  \"> 0\"},\n+        applyIfPlatform = {\"64-bit\", \"true\"},\n+        applyIfCPUFeatureOr = {\"sse4.1\", \"true\", \"asimd\", \"true\"})\n+    static Object[] testMemorySegmentBInvarI3a(MemorySegment m, int invar1, int invar2, int invar3, int size) {\n+        long i1 = (long)(invar1 + invar2 + invar3);\n+        long i2 = (long)(invar2 + invar3 + invar1); \/\/ equivalent\n+        for (int i = 0; i < size; i++) {\n+            byte v = m.get(ValueLayout.JAVA_BYTE, i + i1);\n+            m.set(ValueLayout.JAVA_BYTE, i + i2, (byte)(v + 1));\n+        }\n+        return new Object[]{ m };\n+    }\n+\n+    @Test\n+    @IR(counts = {IRNode.LOAD_VECTOR_B, \"> 0\",\n+                  IRNode.ADD_VB,        \"> 0\",\n+                  IRNode.STORE_VECTOR,  \"> 0\"},\n+        applyIfPlatform = {\"64-bit\", \"true\"},\n+        applyIf = {\"AlignVector\", \"false\"},\n+        applyIfCPUFeatureOr = {\"sse4.1\", \"true\", \"asimd\", \"true\"})\n+    static Object[] testMemorySegmentBInvarI3b(MemorySegment m, int invar1, int invar2, int invar3, int size) {\n+        long i1 = (long)(invar1 + invar2 + invar3);\n+        long i2 = (long)(invar2 + invar3 + invar1); \/\/ equivalent\n+        for (int i = 0; i < size; i+=2) {\n+            byte v0 = m.get(ValueLayout.JAVA_BYTE, i + i1 + 0);\n+            byte v1 = m.get(ValueLayout.JAVA_BYTE, i + i2 + 1);\n+            m.set(ValueLayout.JAVA_BYTE, i + i1 + 0, (byte)(v0 + 1));\n+            m.set(ValueLayout.JAVA_BYTE, i + i2 + 1, (byte)(v1 + 1));\n+        }\n+        return new Object[]{ m };\n+    }\n+\n+    @Test\n+    \/\/ Currently, we don't vectorize. But we may vectorize this, once we implement something like aliasing analysis,\n+    \/\/ though in this particular case we know that the values at runtime will alias.\n+    static Object[] testMemorySegmentBInvarI3c(MemorySegment m, int invar1, int invar2, int invar3, int size) {\n+        long i1 = (long)(invar1 + invar2 + invar3);\n+        long i2 = (long)(invar2 + invar3) + (long)(invar1); \/\/ not equivalent!\n+        for (int i = 0; i < size; i++) {\n+            byte v = m.get(ValueLayout.JAVA_BYTE, i + i1);\n+            m.set(ValueLayout.JAVA_BYTE, i + i2, (byte)(v + 1));\n+        }\n+        return new Object[]{ m };\n+    }\n+\n+    @Test\n+    @IR(counts = {IRNode.LOAD_VECTOR_B, \"> 0\",\n+                  IRNode.STORE_VECTOR,  \"> 0\"},\n+        applyIfPlatform = {\"64-bit\", \"true\"},\n+        applyIf = {\"AlignVector\", \"false\"},\n+        applyIfCPUFeatureOr = {\"sse4.1\", \"true\", \"asimd\", \"true\"})\n+    static Object[] testMemorySegmentBInvarI3d(MemorySegment m, int invar1, int invar2, int invar3, int size) {\n+        long i1 = (long)(invar1 + invar2 + invar3);\n+        long i2 = (long)(invar2 + invar3) + (long)(invar1);\n+        for (int i = 0; i < size; i+=2) {\n+            byte v0 = m.get(ValueLayout.JAVA_BYTE, i + i1 + 0);\n+            byte v1 = m.get(ValueLayout.JAVA_BYTE, i + i2 + 1);\n+            m.set(ValueLayout.JAVA_BYTE, i + i1 + 0, (byte)(v0 + 1));\n+            m.set(ValueLayout.JAVA_BYTE, i + i2 + 1, (byte)(v1 + 1));\n+        }\n+        return new Object[]{ m };\n+    }\n+\n+    @Test\n+    @IR(counts = {IRNode.LOAD_VECTOR_B, \"> 0\",\n+                  IRNode.ADD_VB,        \"> 0\",\n+                  IRNode.STORE_VECTOR,  \"> 0\"},\n+        applyIfPlatform = {\"64-bit\", \"true\"},\n+        applyIfCPUFeatureOr = {\"sse4.1\", \"true\", \"asimd\", \"true\"})\n+    static Object[] testMemorySegmentBInvarI3e(MemorySegment m, int invar1, int invar2, int invar3, int size) {\n+        long i1 = (long)(invar1 + invar2 - invar3);\n+        long i2 = (long)(invar2 - invar3 + invar1); \/\/ equivalent\n+        for (int i = 0; i < size; i++) {\n+            byte v = m.get(ValueLayout.JAVA_BYTE, i + i1);\n+            m.set(ValueLayout.JAVA_BYTE, i + i2, (byte)(v + 1));\n+        }\n+        return new Object[]{ m };\n+    }\n+\n+    @Test\n+    @IR(counts = {IRNode.LOAD_VECTOR_B, \"> 0\",\n+                  IRNode.ADD_VB,        \"> 0\",\n+                  IRNode.STORE_VECTOR,  \"> 0\"},\n+        applyIfPlatform = {\"64-bit\", \"true\"},\n+        applyIfCPUFeatureOr = {\"sse4.1\", \"true\", \"asimd\", \"true\"})\n+    static Object[] testMemorySegmentBInvarI3f(MemorySegment m, int invar1, int invar2, int invar3, int size) {\n+        long i1 = (long)(invar1 - (invar2 - invar3));\n+        long i2 = (long)(-invar2 + invar3 + invar1); \/\/ equivalent\n+        for (int i = 0; i < size; i++) {\n+            byte v = m.get(ValueLayout.JAVA_BYTE, i + i1);\n+            m.set(ValueLayout.JAVA_BYTE, i + i2, (byte)(v + 1));\n+        }\n+        return new Object[]{ m };\n+    }\n+\n+    @Test\n+    @IR(counts = {IRNode.LOAD_VECTOR_B, \"> 0\",\n+                  IRNode.ADD_VB,        \"> 0\",\n+                  IRNode.STORE_VECTOR,  \"> 0\"},\n+        applyIfPlatform = {\"64-bit\", \"true\"},\n+        applyIfCPUFeatureOr = {\"sse4.1\", \"true\", \"asimd\", \"true\"})\n+    static Object[] testMemorySegmentBInvarL3g(MemorySegment m, long invar1, long invar2, long invar3, int size) {\n+        long i1 = invar1 - (invar2 - invar3);\n+        long i2 = -invar2 + invar3 + invar1; \/\/ equivalent\n+        for (int i = 0; i < size; i++) {\n+            byte v = m.get(ValueLayout.JAVA_BYTE, i + i1);\n+            m.set(ValueLayout.JAVA_BYTE, i + i2, (byte)(v + 1));\n+        }\n+        return new Object[]{ m };\n+    }\n+\n+    @Test\n+    @IR(counts = {IRNode.LOAD_VECTOR_B, \"> 0\",\n+                  IRNode.ADD_VB,        \"> 0\",\n+                  IRNode.STORE_VECTOR,  \"> 0\"},\n+        applyIfPlatform = {\"64-bit\", \"true\"},\n+        applyIfCPUFeatureOr = {\"sse4.1\", \"true\", \"asimd\", \"true\"})\n+    static Object[] testMemorySegmentBInvarL3h(MemorySegment m, long invar1, long invar2, long invar3, int size) {\n+        long i1 = -invar1 - invar2 - invar3;\n+        long i2 = -invar2 - invar3 - invar1; \/\/ equivalent\n+        for (int i = 0; i < size; i++) {\n+            byte v = m.get(ValueLayout.JAVA_BYTE, i + i1);\n+            m.set(ValueLayout.JAVA_BYTE, i + i2, (byte)(v + 1));\n+        }\n+        return new Object[]{ m };\n+    }\n+\n+    @Test\n+    @IR(counts = {IRNode.LOAD_VECTOR_B, \"> 0\",\n+                  IRNode.ADD_VB,        \"> 0\",\n+                  IRNode.STORE_VECTOR,  \"> 0\"},\n+        applyIfPlatform = {\"64-bit\", \"true\"},\n+        applyIfCPUFeatureOr = {\"sse4.1\", \"true\", \"asimd\", \"true\"})\n+    static Object[] testMemorySegmentBInvarL3k(MemorySegment m, long invar1, long invar2, long invar3, int size) {\n+        long i1 = -invar1 + invar2 + invar3;\n+        long i2 = invar2 + invar3 - invar1; \/\/ equivalent\n+        for (int i = 0; i < size; i++) {\n+            byte v = m.get(ValueLayout.JAVA_BYTE, i + i1);\n+            m.set(ValueLayout.JAVA_BYTE, i + i2, (byte)(v + 1));\n+        }\n+        return new Object[]{ m };\n+    }\n+\n+    @Test\n+    @IR(counts = {IRNode.LOAD_VECTOR_I, \"> 0\",\n+                  IRNode.ADD_VI,        \"> 0\",\n+                  IRNode.STORE_VECTOR,  \"> 0\"},\n+        applyIfPlatform = {\"64-bit\", \"true\"},\n+        applyIf = {\"AlignVector\", \"false\"},\n+        applyIfCPUFeatureOr = {\"sse4.1\", \"true\", \"asimd\", \"true\"})\n+    static Object[] testMemorySegmentIInvarL3a(MemorySegment m, long invar1, long invar2, long invar3, int size) {\n+        long i1 = invar1 + invar2 + invar3;\n+        long i2 = invar2 + invar3 + invar1; \/\/ equivalent\n+        for (int i = 0; i < size; i++) {\n+            int v = m.getAtIndex(ValueLayout.JAVA_INT, i + i1);\n+            m.setAtIndex(ValueLayout.JAVA_INT, i + i2, v + 1);\n+        }\n+        return new Object[]{ m };\n+    }\n+\n+    @Test\n+    @IR(counts = {IRNode.LOAD_VECTOR_I, \"> 0\",\n+                  IRNode.ADD_VI,        \"> 0\",\n+                  IRNode.STORE_VECTOR,  \"> 0\"},\n+        applyIfPlatform = {\"64-bit\", \"true\"},\n+        applyIf = {\"AlignVector\", \"false\"},\n+        applyIfCPUFeatureOr = {\"sse4.1\", \"true\", \"asimd\", \"true\"})\n+    static Object[] testMemorySegmentIInvarL3b(MemorySegment m, long invar1, long invar2, long invar3, int size) {\n+        long i1 = -invar1 - invar2 - invar3;\n+        long i2 = -invar2 - invar3 - invar1; \/\/ equivalent\n+        for (int i = 0; i < size; i++) {\n+            int v = m.getAtIndex(ValueLayout.JAVA_INT, i + i1);\n+            m.setAtIndex(ValueLayout.JAVA_INT, i + i2, v + 1);\n+        }\n+        return new Object[]{ m };\n+    }\n+\n+    @Test\n+    @IR(counts = {IRNode.LOAD_VECTOR_I, \"> 0\",\n+                  IRNode.ADD_VI,        \"> 0\",\n+                  IRNode.STORE_VECTOR,  \"> 0\"},\n+        applyIfPlatform = {\"64-bit\", \"true\"},\n+        applyIf = {\"AlignVector\", \"false\"},\n+        applyIfCPUFeatureOr = {\"sse4.1\", \"true\", \"asimd\", \"true\"})\n+    static Object[] testMemorySegmentIInvarL3c(MemorySegment m, long invar1, long invar2, long invar3, int size) {\n+        long i1 = -invar1 + invar2 + invar3;\n+        long i2 = invar2 + invar3 - invar1; \/\/ equivalent\n+        for (int i = 0; i < size; i++) {\n+            int v = m.getAtIndex(ValueLayout.JAVA_INT, i + i1);\n+            m.setAtIndex(ValueLayout.JAVA_INT, i + i2, v + 1);\n+        }\n+        return new Object[]{ m };\n+    }\n+\n+    @Test\n+    @IR(counts = {IRNode.LOAD_VECTOR_I, \"= 0\",\n+                  IRNode.STORE_VECTOR,  \"= 0\"},\n+        applyIfPlatform = {\"64-bit\", \"true\"},\n+        applyIfCPUFeatureOr = {\"sse4.1\", \"true\", \"asimd\", \"true\"})\n+    \/\/ Would be nice if it vectorized.\n+    \/\/ Fails because of control flow. Somehow the \"offsetPlain\" check (checks for alignment) is not folded away.\n+    static Object[] testMemorySegmentIInvarL3d(MemorySegment m, int invar1, int invar2, int invar3, int size) {\n+        long i1 = (long)(-invar1 + invar2 + invar3);\n+        long i2 = (long)(invar2 + invar3 - invar1); \/\/ equivalent\n+        for (int i = 0; i < size; i+=2) {\n+            int v0 = m.getAtIndex(ValueLayout.JAVA_INT, i + i1 + 0);\n+            int v1 = m.getAtIndex(ValueLayout.JAVA_INT, i + i2 + 1);\n+            m.setAtIndex(ValueLayout.JAVA_INT, i + i1 + 0, v0 + 1);\n+            m.setAtIndex(ValueLayout.JAVA_INT, i + i2 + 1, v1 + 1);\n+        }\n+        return new Object[]{ m };\n+    }\n+\n+    @Test\n+    @IR(counts = {IRNode.LOAD_VECTOR_I, \"= 0\",\n+                  IRNode.STORE_VECTOR,  \"= 0\"},\n+        applyIfPlatform = {\"64-bit\", \"true\"},\n+        applyIfCPUFeatureOr = {\"sse4.1\", \"true\", \"asimd\", \"true\"})\n+    \/\/ Would be nice if it vectorized.\n+    \/\/ Fails because of control flow. Somehow the \"offsetPlain\" check (checks for alignment) is not folded away.\n+    static Object[] testMemorySegmentIInvarL3d2(MemorySegment m, int invar1, int invar2, int invar3, int size) {\n+        long i1 = (long)(-invar1 + invar2 + invar3);\n+        for (int i = 0; i < size; i+=2) {\n+            int v0 = m.getAtIndex(ValueLayout.JAVA_INT, i + i1 + 0);\n+            int v1 = m.getAtIndex(ValueLayout.JAVA_INT, i + i1 + 1);\n+            m.setAtIndex(ValueLayout.JAVA_INT, i + i1 + 0, v0 + 1);\n+            m.setAtIndex(ValueLayout.JAVA_INT, i + i1 + 1, v1 + 1);\n+        }\n+        return new Object[]{ m };\n+    }\n+\n+    @Test\n+    @IR(counts = {IRNode.LOAD_VECTOR_I, \"> 0\",\n+                  IRNode.ADD_VI,        \"> 0\",\n+                  IRNode.STORE_VECTOR,  \"> 0\"},\n+        applyIfPlatform = {\"64-bit\", \"true\"},\n+        applyIf = {\"AlignVector\", \"false\"},\n+        applyIfCPUFeatureOr = {\"sse4.1\", \"true\", \"asimd\", \"true\"})\n+    \/\/ But here the \"offsetPlain\" is folded away\n+    static Object[] testMemorySegmentIInvarL3d3(MemorySegment m, int size) {\n+        for (int i = 0; i < size; i+=2) {\n+            int v0 = m.getAtIndex(ValueLayout.JAVA_INT, i + 0);\n+            int v1 = m.getAtIndex(ValueLayout.JAVA_INT, i + 1);\n+            m.setAtIndex(ValueLayout.JAVA_INT, i + 0, v0 + 1);\n+            m.setAtIndex(ValueLayout.JAVA_INT, i + 1, v1 + 1);\n+        }\n+        return new Object[]{ m };\n+    }\n+\n+    @Test\n+    @IR(counts = {IRNode.LOAD_VECTOR_I, \"= 0\",\n+                  IRNode.STORE_VECTOR,  \"= 0\"},\n+        applyIfPlatform = {\"64-bit\", \"true\"},\n+        applyIfCPUFeatureOr = {\"sse4.1\", \"true\", \"asimd\", \"true\"})\n+    \/\/ Should never vectorize, since i1 and i2 are not guaranteed to be adjacent\n+    \/\/ invar2 + invar3 could overflow, and the address be valid with and without overflow.\n+    \/\/ So both addresses are valid, and not adjacent.\n+    static Object[] testMemorySegmentIInvarL3e(MemorySegment m, int invar1, int invar2, int invar3, int size) {\n+        long i1 = (long)(-invar1 + invar2 + invar3);\n+        long i2 = (long)(invar2 + invar3) - (long)(invar1); \/\/ not equivalent\n+        for (int i = 0; i < size; i+=2) {\n+            int v0 = m.getAtIndex(ValueLayout.JAVA_INT, i + i1 + 0);\n+            int v1 = m.getAtIndex(ValueLayout.JAVA_INT, i + i2 + 1);\n+            m.setAtIndex(ValueLayout.JAVA_INT, i + i1 + 0, v0 + 1);\n+            m.setAtIndex(ValueLayout.JAVA_INT, i + i2 + 1, v1 + 1);\n+        }\n+        return new Object[]{ m };\n+    }\n+\n+    @Test\n+    @IR(counts = {IRNode.LOAD_VECTOR_I, \"> 0\",\n+                  IRNode.ADD_VI,        \"> 0\",\n+                  IRNode.STORE_VECTOR,  \"> 0\"},\n+        applyIfPlatform = {\"64-bit\", \"true\"},\n+        applyIf = {\"AlignVector\", \"false\"},\n+        applyIfCPUFeatureOr = {\"sse4.1\", \"true\", \"asimd\", \"true\"})\n+    static Object[] testMemorySegmentIInvarL3f(MemorySegment m, long invar1, long invar2, long invar3, int size) {\n+        long i1 = -invar1 + invar2 + invar3;\n+        long i2 = invar2 + invar3 - invar1; \/\/ equivalent\n+        for (int i = 0; i < size; i++) {\n+            \/\/ Scale the index manually\n+            int v = m.get(ValueLayout.JAVA_INT, 4 * (i + i1));\n+            m.set(ValueLayout.JAVA_INT, 4 * (i + i2), v + 1);\n+        }\n+        return new Object[]{ m };\n+    }\n+\n+    @Test\n+    @IR(counts = {IRNode.LOAD_VECTOR_L, \"> 0\",\n+                  IRNode.ADD_VL,        \"> 0\",\n+                  IRNode.STORE_VECTOR,  \"> 0\"},\n+        applyIfPlatform = {\"64-bit\", \"true\"},\n+        applyIf = {\"AlignVector\", \"false\"},\n+        applyIfCPUFeatureOr = {\"sse4.1\", \"true\", \"asimd\", \"true\"})\n+    static Object[] testMemorySegmentLInvarL3a(MemorySegment m, long invar1, long invar2, long invar3, int size) {\n+        long i1 = invar1 + invar2 + invar3;\n+        long i2 = invar2 + invar3 + invar1; \/\/ equivalent\n+        for (int i = 0; i < size; i++) {\n+            long v = m.getAtIndex(ValueLayout.JAVA_LONG, i + i1);\n+            m.setAtIndex(ValueLayout.JAVA_LONG, i + i2, v + 1);\n+        }\n+        return new Object[]{ m };\n+    }\n+\n+    @Test\n+    @IR(counts = {IRNode.LOAD_VECTOR_L, \"> 0\",\n+                  IRNode.ADD_VL,        \"> 0\",\n+                  IRNode.STORE_VECTOR,  \"> 0\"},\n+        applyIfPlatform = {\"64-bit\", \"true\"},\n+        applyIf = {\"AlignVector\", \"false\"},\n+        applyIfCPUFeatureOr = {\"sse4.1\", \"true\", \"asimd\", \"true\"})\n+    static Object[] testMemorySegmentLInvarL3b(MemorySegment m, long invar1, long invar2, long invar3, int size) {\n+        long i1 = -invar1 - invar2 - invar3;\n+        long i2 = -invar2 - invar3 - invar1; \/\/ equivalent\n+        for (int i = 0; i < size; i++) {\n+            long v = m.getAtIndex(ValueLayout.JAVA_LONG, i + i1);\n+            m.setAtIndex(ValueLayout.JAVA_LONG, i + i2, v + 1);\n+        }\n+        return new Object[]{ m };\n+    }\n+\n+    @Test\n+    @IR(counts = {IRNode.LOAD_VECTOR_L, \"> 0\",\n+                  IRNode.ADD_VL,        \"> 0\",\n+                  IRNode.STORE_VECTOR,  \"> 0\"},\n+        applyIfPlatform = {\"64-bit\", \"true\"},\n+        applyIf = {\"AlignVector\", \"false\"},\n+        applyIfCPUFeatureOr = {\"sse4.1\", \"true\", \"asimd\", \"true\"})\n+    static Object[] testMemorySegmentLInvarL3c(MemorySegment m, long invar1, long invar2, long invar3, int size) {\n+        long i1 = -invar1 + invar2 + invar3;\n+        long i2 = invar2 + invar3 - invar1; \/\/ equivalent\n+        for (int i = 0; i < size; i++) {\n+            long v = m.getAtIndex(ValueLayout.JAVA_LONG, i + i1);\n+            m.setAtIndex(ValueLayout.JAVA_LONG, i + i2, v + 1);\n+        }\n+        return new Object[]{ m };\n+    }\n+\n+    @Test\n+    @IR(counts = {IRNode.LOAD_VECTOR_L, \"= 0\",\n+                  IRNode.STORE_VECTOR,  \"= 0\"},\n+        applyIfPlatform = {\"64-bit\", \"true\"},\n+        applyIfCPUFeatureOr = {\"sse4.1\", \"true\", \"asimd\", \"true\"})\n+    \/\/ Would be nice if it vectorized.\n+    \/\/ Fails because of control flow. Somehow the \"offsetPlain\" check (checks for alignment) is not folded away.\n+    static Object[] testMemorySegmentLInvarL3d(MemorySegment m, int invar1, int invar2, int invar3, int size) {\n+        long i1 = (long)(-invar1 + invar2 + invar3);\n+        long i2 = (long)(invar2 + invar3 - invar1); \/\/ equivalent\n+        for (int i = 0; i < size; i+=2) {\n+            long v0 = m.getAtIndex(ValueLayout.JAVA_LONG, i + i1 + 0);\n+            long v1 = m.getAtIndex(ValueLayout.JAVA_LONG, i + i2 + 1);\n+            m.setAtIndex(ValueLayout.JAVA_LONG, i + i1 + 0, v0 + 1);\n+            m.setAtIndex(ValueLayout.JAVA_LONG, i + i2 + 1, v1 + 1);\n+        }\n+        return new Object[]{ m };\n+    }\n+\n+    @Test\n+    @IR(counts = {IRNode.LOAD_VECTOR_L, \"= 0\",\n+                  IRNode.STORE_VECTOR,  \"= 0\"},\n+        applyIfPlatform = {\"64-bit\", \"true\"},\n+        applyIfCPUFeatureOr = {\"sse4.1\", \"true\", \"asimd\", \"true\"})\n+    \/\/ Would be nice if it vectorized.\n+    \/\/ Fails because of control flow. Somehow the \"offsetPlain\" check (checks for alignment) is not folded away.\n+    static Object[] testMemorySegmentLInvarL3d2(MemorySegment m, int invar1, int invar2, int invar3, int size) {\n+        long i1 = (long)(-invar1 + invar2 + invar3);\n+        for (int i = 0; i < size; i+=2) {\n+            long v0 = m.getAtIndex(ValueLayout.JAVA_LONG, i + i1 + 0);\n+            long v1 = m.getAtIndex(ValueLayout.JAVA_LONG, i + i1 + 1);\n+            m.setAtIndex(ValueLayout.JAVA_LONG, i + i1 + 0, v0 + 1);\n+            m.setAtIndex(ValueLayout.JAVA_LONG, i + i1 + 1, v1 + 1);\n+        }\n+        return new Object[]{ m };\n+    }\n+\n+    @Test\n+    @IR(counts = {IRNode.LOAD_VECTOR_L, \"> 0\",\n+                  IRNode.ADD_VL,        \"> 0\",\n+                  IRNode.STORE_VECTOR,  \"> 0\"},\n+        applyIfPlatform = {\"64-bit\", \"true\"},\n+        applyIf = {\"AlignVector\", \"false\"},\n+        applyIfCPUFeatureOr = {\"sse4.1\", \"true\", \"asimd\", \"true\"})\n+    \/\/ But here the \"offsetPlain\" is folded away\n+    static Object[] testMemorySegmentLInvarL3d3(MemorySegment m, int size) {\n+        for (int i = 0; i < size; i+=2) {\n+            long v0 = m.getAtIndex(ValueLayout.JAVA_LONG, i + 0);\n+            long v1 = m.getAtIndex(ValueLayout.JAVA_LONG, i + 1);\n+            m.setAtIndex(ValueLayout.JAVA_LONG, i + 0, v0 + 1);\n+            m.setAtIndex(ValueLayout.JAVA_LONG, i + 1, v1 + 1);\n+        }\n+        return new Object[]{ m };\n+    }\n+\n+    @Test\n+    @IR(counts = {IRNode.LOAD_VECTOR_L, \"= 0\",\n+                  IRNode.STORE_VECTOR,  \"= 0\"},\n+        applyIfPlatform = {\"64-bit\", \"true\"},\n+        applyIfCPUFeatureOr = {\"sse4.1\", \"true\", \"asimd\", \"true\"})\n+    \/\/ FAILS: should be ok to vectorize, but does not. Investigate in JDK-8330274.\n+    static Object[] testMemorySegmentLInvarL3e(MemorySegment m, int invar1, int invar2, int invar3, int size) {\n+        long i1 = (long)(-invar1 + invar2 + invar3);\n+        long i2 = (long)(invar2 + invar3) - (long)(invar1); \/\/ not equivalent\n+        for (int i = 0; i < size; i+=2) {\n+            long v0 = m.getAtIndex(ValueLayout.JAVA_LONG, i + i1 + 0);\n+            long v1 = m.getAtIndex(ValueLayout.JAVA_LONG, i + i2 + 1);\n+            m.setAtIndex(ValueLayout.JAVA_LONG, i + i1 + 0, v0 + 1);\n+            m.setAtIndex(ValueLayout.JAVA_LONG, i + i2 + 1, v1 + 1);\n+        }\n+        return new Object[]{ m };\n+    }\n+\n+    @Test\n+    @IR(counts = {IRNode.LOAD_VECTOR_L, \"> 0\",\n+                  IRNode.ADD_VL,        \"> 0\",\n+                  IRNode.STORE_VECTOR,  \"> 0\"},\n+        applyIfPlatform = {\"64-bit\", \"true\"},\n+        applyIf = {\"AlignVector\", \"false\"},\n+        applyIfCPUFeatureOr = {\"sse4.1\", \"true\", \"asimd\", \"true\"})\n+    static Object[] testMemorySegmentLInvarL3f(MemorySegment m, long invar1, long invar2, long invar3, int size) {\n+        long i1 = -invar1 + invar2 + invar3;\n+        long i2 = invar2 + invar3 - invar1; \/\/ equivalent\n+        for (int i = 0; i < size; i++) {\n+            \/\/ Scale the index manually\n+            long v = m.get(ValueLayout.JAVA_LONG, 8 * (i + i1));\n+            m.set(ValueLayout.JAVA_LONG, 8 * (i + i2), v + 1);\n+        }\n+        return new Object[]{ m };\n+    }\n+\n+    @Test\n+    \/\/ Traversal through AddI would explode in exponentially many paths, exhausing the node limit.\n+    \/\/ For this, we have a traversal size limit.\n+    static Object[] testLargeInvariantSum(byte[] a, int invar1, int invar2, int invar3, int size) {\n+        int e = invar1;\n+        e = ((e + invar2) + (e + invar3));\n+        e = ((e + invar2) + (e + invar3));\n+        e = ((e + invar2) + (e + invar3));\n+        e = ((e + invar2) + (e + invar3));\n+        e = ((e + invar2) + (e + invar3));\n+        e = ((e + invar2) + (e + invar3));\n+        e = ((e + invar2) + (e + invar3));\n+        e = ((e + invar2) + (e + invar3));\n+        e = ((e + invar2) + (e + invar3));\n+        e = ((e + invar2) + (e + invar3));\n+        e = ((e + invar2) + (e + invar3));\n+        e = ((e + invar2) + (e + invar3));\n+        e = ((e + invar2) + (e + invar3));\n+        e = ((e + invar2) + (e + invar3));\n+        e = ((e + invar2) + (e + invar3));\n+        e = ((e + invar2) + (e + invar3));\n+        e = ((e + invar2) + (e + invar3));\n+        e = ((e + invar2) + (e + invar3));\n+        e = ((e + invar2) + (e + invar3));\n+        e = ((e + invar2) + (e + invar3));\n+        e = ((e + invar2) + (e + invar3));\n+        e = ((e + invar2) + (e + invar3));\n+        e = ((e + invar2) + (e + invar3));\n+        for (int i = 0; i < size; i++) {\n+            a[i + e] += 1;\n+        }\n+        return new Object[]{ a };\n+    }\n+}\n","filename":"test\/hotspot\/jtreg\/compiler\/loopopts\/superword\/TestEquivalentInvariants.java","additions":951,"deletions":0,"binary":false,"changes":951,"status":"added"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2024, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -41,0 +41,8 @@\n+\/*\n+ * @test id=StoreToLoadForwardingFailureDetection\n+ * @bug 8328938\n+ * @modules java.base\/jdk.internal.misc\n+ * @library \/test\/lib \/\n+ * @run main\/othervm -XX:+IgnoreUnrecognizedVMOptions -XX:+UnlockDiagnosticVMOptions -XX:SuperWordStoreToLoadForwardingFailureDetection=4096 compiler.loopopts.superword.TestLargeScaleAndStride\n+ *\/\n+\n@@ -51,0 +59,1 @@\n+        byte[] b = new byte[RANGE];\n@@ -60,0 +69,1 @@\n+        byte[] gold4  = b.clone();\n@@ -67,0 +77,1 @@\n+        test4(gold4);\n@@ -109,0 +120,6 @@\n+\n+        for (int i = 0; i < 100; i++) {\n+            byte[] c = b.clone();\n+            test4(c);\n+            verify(c, gold4);\n+        }\n@@ -252,0 +269,13 @@\n+\n+    \/\/ VPointer con overflow possible with large SuperWordStoreToLoadForwardingFailureDetection\n+    static final int test4_BIG = (1 << 31)-1000;\n+    static       int test4_big = (1 << 31)-1000;\n+    static void test4(byte[] a) {\n+        long zero = test4_BIG - test4_big;\n+        for (int i = 0; i < RANGE; i++) {\n+            long base = UNSAFE.ARRAY_INT_BASE_OFFSET;\n+            long adr = base + zero + i;\n+            byte v0 = UNSAFE.getByte(a, adr);\n+            UNSAFE.putByte(a, adr, (byte)(v0 + 1));\n+        }\n+    }\n","filename":"test\/hotspot\/jtreg\/compiler\/loopopts\/superword\/TestLargeScaleAndStride.java","additions":31,"deletions":1,"binary":false,"changes":32,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2024, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -459,3 +459,3 @@\n-    @IR(counts = {IRNode.LOAD_VECTOR_B, \"= 0\",\n-                  IRNode.ADD_VB,        \"= 0\",\n-                  IRNode.STORE_VECTOR,  \"= 0\"},\n+    @IR(counts = {IRNode.LOAD_VECTOR_B, \"> 0\",\n+                  IRNode.ADD_VB,        \"> 0\",\n+                  IRNode.STORE_VECTOR,  \"> 0\"},\n@@ -464,2 +464,0 @@\n-    \/\/ FAILS: invariants are sorted differently, because of differently inserted Cast.\n-    \/\/ See: JDK-8330274\n@@ -477,3 +475,3 @@\n-    @IR(counts = {IRNode.LOAD_VECTOR_B, \"= 0\",\n-                  IRNode.ADD_VB,        \"= 0\",\n-                  IRNode.STORE_VECTOR,  \"= 0\"},\n+    @IR(counts = {IRNode.LOAD_VECTOR_B, \"> 0\",\n+                  IRNode.ADD_VB,        \"> 0\",\n+                  IRNode.STORE_VECTOR,  \"> 0\"},\n@@ -482,2 +480,0 @@\n-    \/\/ FAILS: invariants are sorted differently, because of differently inserted Cast.\n-    \/\/ See: JDK-8330274\n@@ -559,3 +555,3 @@\n-    @IR(counts = {IRNode.LOAD_VECTOR_I, \"= 0\",\n-                  IRNode.ADD_VI,        \"= 0\",\n-                  IRNode.STORE_VECTOR,  \"= 0\"},\n+    @IR(counts = {IRNode.LOAD_VECTOR_I, \"> 0\",\n+                  IRNode.ADD_VI,        \"> 0\",\n+                  IRNode.STORE_VECTOR,  \"> 0\"},\n@@ -564,2 +560,0 @@\n-    \/\/ FAILS: invariants are sorted differently, because of differently inserted Cast.\n-    \/\/ See: JDK-8330274\n@@ -577,3 +571,3 @@\n-    @IR(counts = {IRNode.LOAD_VECTOR_I, \"= 0\",\n-                  IRNode.ADD_VI,        \"= 0\",\n-                  IRNode.STORE_VECTOR,  \"= 0\"},\n+    @IR(counts = {IRNode.LOAD_VECTOR_I, \"> 0\",\n+                  IRNode.ADD_VI,        \"> 0\",\n+                  IRNode.STORE_VECTOR,  \"> 0\"},\n@@ -582,2 +576,0 @@\n-    \/\/ FAILS: invariants are sorted differently, because of differently inserted Cast.\n-    \/\/ See: JDK-8330274\n@@ -656,5 +648,0 @@\n-    @IR(counts = {IRNode.LOAD_VECTOR_B, \"= 0\",\n-                  IRNode.ADD_VB,        \"= 0\",\n-                  IRNode.STORE_VECTOR,  \"= 0\"},\n-        applyIfPlatform = {\"64-bit\", \"true\"},\n-        applyIfCPUFeatureOr = {\"sse4.1\", \"true\", \"asimd\", \"true\"})\n@@ -663,0 +650,1 @@\n+    \/\/ Interestingly, it now vectorizes for native, but not for arrays.\n@@ -674,5 +662,0 @@\n-    @IR(counts = {IRNode.LOAD_VECTOR_B, \"= 0\",\n-                  IRNode.ADD_VB,        \"= 0\",\n-                  IRNode.STORE_VECTOR,  \"= 0\"},\n-        applyIfPlatform = {\"64-bit\", \"true\"},\n-        applyIfCPUFeatureOr = {\"sse4.1\", \"true\", \"asimd\", \"true\"})\n@@ -681,0 +664,1 @@\n+    \/\/ Interestingly, it now vectorizes for native, but not for arrays.\n","filename":"test\/hotspot\/jtreg\/compiler\/loopopts\/superword\/TestMemorySegment.java","additions":15,"deletions":31,"binary":false,"changes":46,"status":"modified"}]}