{"files":[{"patch":"@@ -76,0 +76,2 @@\n+#include \"opto\/phase.hpp\"\n+#include \"opto\/phaseloadfolding.hpp\"\n@@ -2405,0 +2407,13 @@\n+  {\n+    \/\/ This phase is much faster than EA, so doing it before EA reduces the work of EA by reducing\n+    \/\/ the number of loads. It also helps EA terminate sooner because folded loads may expose\n+    \/\/ further EA opportunities, and it is better if an EA opportunity is revealed from the\n+    \/\/ beginning than if it is only revealed after some rounds of EA.\n+    TracePhase tp(_t_loadFolding);\n+    PhaseLoadFolding load_folding(igvn);\n+    load_folding.optimize();\n+    if (failing()) {\n+      return;\n+    }\n+  }\n+\n","filename":"src\/hotspot\/share\/opto\/compile.cpp","additions":15,"deletions":0,"binary":false,"changes":15,"status":"modified"},{"patch":"@@ -71,0 +71,1 @@\n+    tty->print_cr (\"         Load Folding:        %7.3f s\", timers[_t_loadFolding].seconds());\n@@ -103,0 +104,1 @@\n+       timers[_t_loadFolding].seconds() +\n","filename":"src\/hotspot\/share\/opto\/phase.cpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -53,0 +53,1 @@\n+    LoadFolding,                      \/\/ Aggressively look through loads\n@@ -76,0 +77,1 @@\n+    f(     _t_loadFolding,           \"loadFolding\")              \\\n","filename":"src\/hotspot\/share\/opto\/phase.hpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -0,0 +1,445 @@\n+\/*\n+ * Copyright (c) 2025, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#include \"libadt\/vectset.hpp\"\n+#include \"memory\/resourceArea.hpp\"\n+#include \"opto\/addnode.hpp\"\n+#include \"opto\/callnode.hpp\"\n+#include \"opto\/cfgnode.hpp\"\n+#include \"opto\/compile.hpp\"\n+#include \"opto\/memnode.hpp\"\n+#include \"opto\/mulnode.hpp\"\n+#include \"opto\/node.hpp\"\n+#include \"opto\/phaseloadfolding.hpp\"\n+#include \"utilities\/globalDefinitions.hpp\"\n+\n+void PhaseLoadFolding::optimize() {\n+  ciEnv* env = C->env();\n+  if (env->should_retain_local_variables() || env->jvmti_can_walk_any_space()) {\n+    \/\/ Give up because JVMTI can do wonders\n+    return;\n+  }\n+\n+  \/\/ This phase is very fast, but it is still preferable not to allow potential unbounded\n+  \/\/ iterations\n+  for (int i = 0; i < 10; i++) {\n+    bool progress = do_optimize();\n+    if (!progress) {\n+      return;\n+    }\n+    _igvn.optimize();\n+    if (C->failing()) {\n+      return;\n+    }\n+  }\n+}\n+\n+\/\/ The escape status of a node is visible in the memory graph. That is, at runtime, if a load 'l'\n+\/\/ from an object 'o' must be executed after an action 'a' that allows 'o' to escape, and in the\n+\/\/ IR graph, the node 'L' corresponding to 'l' consumes the address 'O' + c, with 'O' being the\n+\/\/ node corresponding to the newly allocated object 'o', then there must be a path along the\n+\/\/ use-def edges from 'L' to the node 'A' that corresponds to 'a'.\n+\/\/\n+\/\/ - If 'a' is a method invocation that receives 'o' as an argument, then in the graph, 'A' kills\n+\/\/   all memory. As a result, the memory input of 'L' must be a transitive use of 'A'. This is\n+\/\/   because in a well-behave memory graph, there is always a path of use-def edges from a memory\n+\/\/   node to the previous bottom memory node. This is important as it ensures memory fences can\n+\/\/   serialize memory operations by imposing use-def dependencies between the fence and the\n+\/\/   surrounding memory nodes.\n+\/\/   Example:\n+\/\/       Integer o = new Integer(v);\n+\/\/       int x = o.value;\n+\/\/       if (flag) {\n+\/\/         consume(o);\n+\/\/         int y = o.value;\n+\/\/       } else {\n+\/\/         consume(null);\n+\/\/         int z = o.value;\n+\/\/       }\n+\/\/       int t = o.value;\n+\/\/   The memory graph will then look like:\n+\/\/        NarrowMemProj (Integer.value)\n+\/\/          |          |\n+\/\/          |          |\n+\/\/     CallJava(o) CallJava(null)\n+\/\/          |          |\n+\/\/          |          |\n+\/\/        Proj1      Proj2\n+\/\/           \\       \/\n+\/\/            \\     \/\n+\/\/              Phi\n+\/\/   We can see that the object can be considered non-escape at NarrowMemProj, CallJava(null), and\n+\/\/   Proj2, while it is considered escape at CallJava(o), Proj1, Phi. The loads x and z will be\n+\/\/   from NarrowMemProj and Proj2, respectively, which means they can be considered loads from an\n+\/\/   object that has not escaped, and we can fold them to v. On the other hand, the loads y and t\n+\/\/   are from Proj1 and Phi, respectively, which means we cannot assume that the only value they\n+\/\/   can see is v.\n+\/\/\n+\/\/ - If 'a' is a store of 'o' into the memory, then 'l' must be executed after a iff:\n+\/\/   + There is a memory fence that prevents 'l' from being executed before 'a'. Since a memory\n+\/\/     fence kills all memory, the node 'F' corresponding to that fence must be a transitive use of\n+\/\/     'A', and the memory input of 'L' must be a transitive use of 'F', similar to case 1.\n+\/\/   + There is a data dependency between 'l' and 'a'. In this case, there must be a path of\n+\/\/     use-def edges from 'L' to 'A'.\n+\/\/     For example:\n+\/\/       Integer o = new Integer(v);\n+\/\/       *p = o;\n+\/\/       Integer o_cloned = *p;\n+\/\/       o_clone.value = u;\n+\/\/       int x = o.value;\n+\/\/     Then, there is a path of use-def edges:\n+\/\/            Load(x = o.value)\n+\/\/                   | (MemNode::Memory)\n+\/\/                   v\n+\/\/         Store(o_clone.value = u)\n+\/\/                   | (MemNode::Address)\n+\/\/                   V\n+\/\/           Load(o_clone = *p)\n+\/\/                   | (MemNode::Memory)\n+\/\/                   v\n+\/\/              Store(*p = o)\n+\/\/     We can see that, we cannot fold x to v, because it must observe the value u, and we can\n+\/\/     correcly detect that the object O has escaped by following the outputs of the store that\n+\/\/     allows o to escape.\n+\/\/\n+\/\/   It is important to remind that even if 'l' is scheduled after the store 'a', unless there is a\n+\/\/   memory fence between 'l' and 'a', it is generally not required that 'l' is executed after 'a'.\n+\/\/   For example:\n+\/\/     1.  Integer o = new Integer(v);\n+\/\/         *p = o;\n+\/\/         int x = o.value;\n+\/\/     In this case, even if the load x = o.value is declared after the store of o to p that allows o\n+\/\/     to escape, it is valid for the load to actually happen before the store. If the developer\n+\/\/     wants to ensure that the order in which the memory accesses appear in the program is the same\n+\/\/     as the order they are executed, memory barriers (e.g. a store-load barrier) must be placed\n+\/\/     between them. As a result, we can consider x = o.value to be a load from an object that has\n+\/\/     not escaped, and fold it to v.\n+\/\/     2.  boolean b1, b2;\n+\/\/         Point o = new Point(v1, v2);\n+\/\/         int r;\n+\/\/         if (b1) {\n+\/\/           *p = o;\n+\/\/         } else {\n+\/\/           *q = o;\n+\/\/         }\n+\/\/         if (b2) {\n+\/\/           r = o.x;\n+\/\/         } else {\n+\/\/           r = o.y;\n+\/\/         }\n+\/\/     In this case, even if the control flow forces the loads to be scheduled after the stores\n+\/\/     that allow o to escape, without actual memory barriers, the JMM does not require the CPU to\n+\/\/     execute the loads after the stores (e.g. the loads are in cache so they can be executed\n+\/\/     sooner while the stores need to wait for the acquisition of the corresponding cache lines).\n+\/\/     As a result, we can consider those loads to be from an object that has not escaped, and fold\n+\/\/     o.x to v1 and o.y to v2.\n+bool PhaseLoadFolding::do_optimize() {\n+  bool progress = false;\n+  for (int macro_idx = 0; macro_idx < C->macro_count(); macro_idx++) {\n+    Node* macro = C->macro_node(macro_idx);\n+    if (!macro->is_Allocate()) {\n+      continue;\n+    }\n+\n+    AllocateNode* alloc = macro->as_Allocate();\n+    Node* oop = alloc->result_cast();\n+    if (oop == nullptr) {\n+      continue;\n+    }\n+\n+    if (process_allocate_result(oop)) {\n+      progress = true;\n+    }\n+  }\n+  return progress;\n+}\n+\n+\/\/ Find all loads from oop that have not observed the escape of oop, and try to find their\n+\/\/ corresponding stores\n+bool PhaseLoadFolding::process_allocate_result(Node* oop) {\n+  ResourceMark rm;\n+  Unique_Node_List candidates;\n+  VectorSet candidate_set;\n+\n+  collect_loads(candidates, candidate_set, oop);\n+  if (candidate_set.is_empty()) {\n+    return false;\n+  }\n+\n+  WorkLists work_lists;\n+  process_candidates(candidate_set, work_lists, oop);\n+  if (candidate_set.is_empty()) {\n+    return false;\n+  }\n+\n+  bool progress = false;\n+  for (uint candidate_idx = 0; candidate_idx < candidates.size(); candidate_idx++) {\n+    LoadNode* candidate = candidates.at(candidate_idx)->as_Load();\n+    if (!candidate_set.test(candidate->_idx)) {\n+      continue;\n+    }\n+\n+    work_lists.results.clear();\n+    Node* folded_value = try_fold_recursive(oop, candidate, candidate->in(MemNode::Memory), work_lists);\n+    if (folded_value != nullptr) {\n+      progress = true;\n+      _igvn.replace_node(candidate, folded_value);\n+    }\n+  }\n+  return progress;\n+}\n+\n+\/\/ Collect all loads from oop\n+void PhaseLoadFolding::collect_loads(Unique_Node_List& candidates, VectorSet& candidate_set, Node* oop) {\n+  assert(candidates.size() == 0 && candidate_set.is_empty(), \"must start with no candidates\");\n+  for (DUIterator_Fast oop_out_max, oop_out_idx = oop->fast_outs(oop_out_max); oop_out_idx < oop_out_max; oop_out_idx++) {\n+    Node* out = oop->fast_out(oop_out_idx);\n+    if (!out->is_AddP()) {\n+      continue;\n+    }\n+\n+    if (out->in(AddPNode::Base) != oop || out->in(AddPNode::Address) != oop || !out->in(AddPNode::Offset)->is_Con()) {\n+      \/\/ Only try to fold loads in the form of oop + C\n+      continue;\n+    }\n+\n+    for (DUIterator_Fast addp_out_max, addp_out_idx = out->fast_outs(addp_out_max); addp_out_idx < addp_out_max; addp_out_idx++) {\n+      Node* addp_out = out->fast_out(addp_out_idx);\n+      if (addp_out->is_Load() && !addp_out->as_Load()->is_mismatched_access()) {\n+        candidates.push(addp_out);\n+      }\n+    }\n+  }\n+\n+  for (uint i = 0; i < candidates.size(); i++) {\n+    candidate_set.set(candidates.at(i)->_idx);\n+  }\n+}\n+\n+\/\/ Find all nodes that observe the escape of oop. This function also finds stores that may store\n+\/\/ into oop. This is tricky, for example:\n+\/\/     Integer o = new Integer(v);\n+\/\/     Integer phi = o;\n+\/\/     if (b) {\n+\/\/       phi = new Integer(0);\n+\/\/     }\n+\/\/     phi.value = 1;\n+\/\/ Then, the store phi.value = 1 may or may not modify o, this cannot be known at compile time. As\n+\/\/ a result, when we walk the memory graph from a load, if we encounter such a store, we cannot\n+\/\/ know if it is the value we are looking for, and must give up.\n+void PhaseLoadFolding::process_candidates(VectorSet& candidate_set, WorkLists& work_lists, Node* oop) {\n+  assert(work_lists.may_alias.is_empty() && work_lists.escapes.size() == 0 && work_lists.work_list.size() == 0, \"must start with empty work lists\");\n+  work_lists.work_list.push(oop);\n+  for (uint wl_idx = 0; wl_idx < work_lists.work_list.size(); wl_idx++) {\n+    \/\/ At runtime, n may be the same as oop, or may be a different value\n+    Node* n = work_lists.work_list.at(wl_idx);\n+    for (DUIterator_Fast out_max, out_idx = n->fast_outs(out_max); out_idx < out_max; out_idx++) {\n+      Node* out = n->fast_out(out_idx);\n+      if (out->is_ConstraintCast() || out->is_DecodeN() || out->is_EncodeP() ||\n+          out->is_Phi() || out->is_CMove()) {\n+        \/\/ All things that can alias n\n+        work_lists.work_list.push(out);\n+      } else if (out->is_AddP()) {\n+        AddPNode* addp = out->as_AddP();\n+        assert(addp->base_node() == n, \"unexpected base of an AddP\");\n+\n+        \/\/ A store that may or may not modify a field of oop (e.g. a store into a Phi which has oop\n+        \/\/ as one input, or a store into an element of oop at a variable index). This is\n+        \/\/ conservative, that is it must be true if the store may modify a field of oop but is not\n+        \/\/ in the form oop + C\n+        bool may_alias = false;\n+        if (out->in(AddPNode::Base) != oop || out->in(AddPNode::Address) != oop || !out->in(AddPNode::Offset)->is_Con()) {\n+          \/\/ Not an oop + C pointer\n+          may_alias = true;\n+        }\n+\n+        for (DUIterator_Fast addp_out_max, addp_out_idx = addp->fast_outs(addp_out_max); addp_out_idx < addp_out_max; addp_out_idx++) {\n+          Node* addp_out = addp->fast_out(addp_out_idx);\n+          if ((addp_out->is_Store() || addp_out->is_LoadStore())) {\n+            assert(addp == addp_out->in(MemNode::Address), \"store a derived pointer?\");\n+            if (may_alias) {\n+              work_lists.may_alias.set(addp_out->_idx);\n+            }\n+\n+            if (addp_out->is_LoadStore() || addp_out->as_Store()->is_mismatched_access()) {\n+              \/\/ Mismatched accesses are especially hard because they may lie in a different alias\n+              \/\/ class, so we may not encounter them when walking the memory graph. As a result, be\n+              \/\/ conservative and give up on all loads that may observe this store. LoadStores are\n+              \/\/ also lumped here because there is no LoadStoreNode::is_mismatched_access.\n+              work_lists.escapes.push(addp_out);\n+            }\n+          } else if (addp_out->is_Mem()) {\n+            \/\/ A load, does not affect the memory\n+          } else if (addp_out->is_AddP()) {\n+            \/\/ Another AddP, it should share the base with the current addp, so it will be visited\n+            \/\/ later\n+            assert(addp_out->in(AddPNode::Base) == n, \"must have the same base\");\n+          } else {\n+            \/\/ Some runtime calls receive the pointer without the base\n+            work_lists.escapes.push(addp_out);\n+          }\n+        }\n+      } else if (out->is_Mem()) {\n+        \/\/ A store that may allow oop to escape\n+        if (out->req() > MemNode::ValueIn && n == out->in(MemNode::ValueIn)) {\n+          work_lists.escapes.push(out);\n+        }\n+      } else if (out->is_Call()) {\n+        \/\/ A call that may allow oop to escape\n+        if (!out->is_AbstractLock() && out->as_Call()->has_non_debug_use(n)) {\n+          work_lists.escapes.push(out);\n+        }\n+      } else if (out->is_SafePoint()) {\n+        \/\/ Non-call safepoints are pure control nodes\n+        continue;\n+      } else {\n+        \/\/ Be conservative with everything else\n+        work_lists.escapes.push(out);\n+      }\n+    }\n+  }\n+\n+  \/\/ Propagate the escape status, if a node observes oop escaping, then all of its users also\n+  \/\/ observe that oop escapes\n+  for (uint idx = 0; idx < work_lists.escapes.size(); idx++) {\n+    Node* n = work_lists.escapes.at(idx);\n+    candidate_set.remove(n->_idx);\n+    if (candidate_set.is_empty()) {\n+      return;\n+    }\n+\n+    for (DUIterator_Fast imax, i = n->fast_outs(imax); i < imax; i++) {\n+      Node* out = n->fast_out(i);\n+      if (!out->is_Root()) {\n+        work_lists.escapes.push(out);\n+      }\n+    }\n+  }\n+}\n+\n+\/\/ Try to find the store that a load observes. Since we know that oop has not escaped, we can\n+\/\/ inspect the graph aggressively, ignoring calls and memory barriers.\n+Node* PhaseLoadFolding::try_fold_recursive(Node* oop, LoadNode* candidate, Node* mem, WorkLists& work_lists) {\n+  \/\/ An arbitrary int can be the input to a StoreB or a StoreC, the load needs to do the\n+  \/\/ normalization\n+  auto extract_store_value = [&](StoreNode* store) {\n+    assert(store->Opcode() == candidate->store_Opcode(), \"must match %s - %s\", store->Name(), candidate->Name());\n+    Node* res = store->in(MemNode::ValueIn);\n+    if (candidate->Opcode() == Op_LoadUB) {\n+      res = new AndINode(res, _igvn.intcon(0xFF));\n+      _igvn.register_new_node_with_optimizer(res);\n+    } else if (candidate->Opcode() == Op_LoadB) {\n+      res = new LShiftINode(res, _igvn.intcon(24));\n+      _igvn.register_new_node_with_optimizer(res);\n+      res = new RShiftINode(res, _igvn.intcon(24));\n+      _igvn.register_new_node_with_optimizer(res);\n+    } else if (candidate->Opcode() == Op_LoadUS) {\n+      res = new AndINode(res, _igvn.intcon(0xFFFF));\n+      _igvn.register_new_node_with_optimizer(res);\n+    } else if (candidate->Opcode() == Op_LoadS) {\n+      res = new LShiftINode(res, _igvn.intcon(16));\n+      _igvn.register_new_node_with_optimizer(res);\n+      res = new RShiftINode(res, _igvn.intcon(16));\n+      _igvn.register_new_node_with_optimizer(res);\n+    }\n+    return res;\n+  };\n+\n+  Node* ptr = candidate->in(MemNode::Address);\n+  int alias_idx = C->get_alias_index(_igvn.type(ptr)->is_ptr());\n+  while (true) {\n+    \/\/ We may encounter a memory loop, so recording Phis are necessary\n+    if (work_lists.results.length() > int(mem->_idx)) {\n+      Node* res = work_lists.results.at(mem->_idx);\n+      if (res != nullptr) {\n+        return res;\n+      }\n+    }\n+\n+    \/\/ If we encounter a store that we cannot decide if it modifies the memory candidate loads\n+    \/\/ from, give up\n+    if (work_lists.may_alias.test(mem->_idx)) {\n+      return nullptr;\n+    }\n+\n+    if (mem->is_MergeMem()) {\n+      mem = mem->as_MergeMem()->memory_at(alias_idx);\n+    } else if (mem->is_Phi()) {\n+      \/\/ Create a Phi for the result and store it in work_lists.results, this allows working with\n+      \/\/ cycles\n+      PhiNode* res = new PhiNode(mem->in(0), candidate->bottom_type());\n+      _igvn.register_new_node_with_optimizer(res);\n+      work_lists.results.at_put_grow(mem->_idx, res);\n+      for (uint i = 1; i < mem->req(); i++) {\n+        Node* phi_in = try_fold_recursive(oop, candidate, mem->in(i), work_lists);\n+        if (phi_in == nullptr) {\n+          return nullptr;\n+        }\n+\n+        res->init_req(i, phi_in);\n+      }\n+      return res;\n+    } else if (mem->is_Proj()) {\n+      mem = mem->in(0);\n+    } else if (mem->is_MemBar()) {\n+      \/\/ Look through MemBars, only stop at the InitializeNode of oop\n+      if (!mem->is_Initialize() || mem != oop->in(0)->in(0)) {\n+        mem = mem->in(TypeFunc::Memory);\n+        continue;\n+      }\n+\n+      InitializeNode* init = mem->as_Initialize();\n+      assert(ptr->is_AddP() && ptr->in(AddPNode::Base) == oop && ptr->in(AddPNode::Address) == oop && ptr->in(AddPNode::Offset)->is_Con(),\n+             \"invalid pointer\");\n+\n+#ifdef _LP64\n+      Node* res = init->find_captured_store(ptr->in(AddPNode::Offset)->get_long(), candidate->memory_size(), &_igvn);\n+#else \/\/ _LP64\n+      Node* res = init->find_captured_store(ptr->in(AddPNode::Offset)->get_int(), candidate->memory_size(), &_igvn);\n+#endif \/\/ _LP64\n+      if (res == nullptr) {\n+        return nullptr;\n+      } else if (res->is_Proj() && res->in(0) == init->allocation()) {\n+        \/\/ Failure to find a captured store will return the memory output of the AllocateNode\n+        return _igvn.zerocon(candidate->value_basic_type());\n+      } else if (res->Opcode() == candidate->store_Opcode()) {\n+        return extract_store_value(res->as_Store());\n+      } else {\n+        return nullptr;\n+      }\n+    } else if (mem->is_SafePoint()) {\n+      mem = mem->in(TypeFunc::Memory);\n+    } else if (mem->is_Store()) {\n+      \/\/ We discarded all stores that may write into this field but does not have the form oop + C,\n+      \/\/ so a simple comparison of the address input is enough\n+      if (ptr == mem->in(MemNode::Address)) {\n+        return extract_store_value(mem->as_Store());\n+      } else {\n+        mem = mem->in(MemNode::Memory);\n+      }\n+    } else {\n+      return nullptr;\n+    }\n+  }\n+}\n","filename":"src\/hotspot\/share\/opto\/phaseloadfolding.cpp","additions":445,"deletions":0,"binary":false,"changes":445,"status":"added"},{"patch":"@@ -0,0 +1,66 @@\n+\/*\n+ * Copyright (c) 2025, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef SHARE_OPTO_PHASELOADFOLDING_HPP\n+#define SHARE_OPTO_PHASELOADFOLDING_HPP\n+\n+#include \"libadt\/vectset.hpp\"\n+#include \"opto\/node.hpp\"\n+#include \"opto\/phase.hpp\"\n+#include \"utilities\/growableArray.hpp\"\n+\n+class AllocateNode;\n+class PhaseIterGVN;\n+\n+\/\/ Try to fold loads by finding the corresponding stores. The transformations here inspect the\n+\/\/ graph more aggressively than during IterGVN, so it is a separate phase in the compilation\n+\/\/ process. The loads taken into consideration are:\n+\/\/\n+\/\/ 1. If an object has not escaped, then all modification must be visible in the graph. As a\n+\/\/    result, we can follow the memory input, skip through calls and memory fences to find a\n+\/\/    corresponding store.\n+class PhaseLoadFolding : public Phase {\n+private:\n+  PhaseIterGVN& _igvn;\n+\n+  class WorkLists {\n+  public:\n+    VectorSet may_alias;\n+    Unique_Node_List escapes;\n+    Unique_Node_List work_list;\n+    GrowableArray<Node*> results;\n+  };\n+\n+  bool do_optimize();\n+  bool process_allocate_result(Node* oop);\n+  void collect_loads(Unique_Node_List& candidates, VectorSet& candidate_mems, Node* oop);\n+  void process_candidates(VectorSet& candidate_mems, WorkLists& work_lists, Node* oop);\n+  Node* try_fold_recursive(Node* oop, LoadNode* candidate, Node* mem, WorkLists& work_lists);\n+\n+public:\n+  PhaseLoadFolding(PhaseIterGVN& igvn) : Phase(LoadFolding), _igvn(igvn) {}\n+  void optimize();\n+};\n+\n+#endif \/\/ SHARE_OPTO_PHASELOADFOLDING_HPP\n","filename":"src\/hotspot\/share\/opto\/phaseloadfolding.hpp","additions":66,"deletions":0,"binary":false,"changes":66,"status":"added"},{"patch":"@@ -0,0 +1,258 @@\n+\/*\n+ * Copyright (c) 2025, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+package compiler.escapeAnalysis;\n+\n+import compiler.lib.ir_framework.*;\n+\n+import java.lang.invoke.VarHandle;\n+\n+\/**\n+ * @test\n+ * @bug 8373495\n+ * @summary Test that loads from a newly allocated object are aggressively folded if the object has not escaped\n+ * @library \/test\/lib \/\n+ * @run driver ${test.main.class}\n+ *\/\n+public class TestLoadFolding {\n+    public static class Point {\n+        int x;\n+        int y;\n+\n+        Point() {\n+            x = 1;\n+            y = 2;\n+        }\n+\n+        static final Point DEFAULT = new Point();\n+    }\n+\n+    static Point staticField;\n+\n+    public static void main(String[] args) {\n+        TestFramework.run();\n+    }\n+\n+    @Run(test = {\"test11\", \"test12\", \"test13\", \"test14\", \"test15\", \"test16\", \"test17\", \"test18\"})\n+    public void runPositiveTests() {\n+        test11();\n+        test12(false);\n+        test12(true);\n+        test13(false);\n+        test13(true);\n+        test14();\n+        test15(1, 16);\n+        test16(1, 16, false);\n+        test16(1, 16, true);\n+        test17(0);\n+        test18(0);\n+    }\n+\n+    @Run(test = {\"test01\", \"test02\", \"test03\", \"test04\", \"test05\"})\n+    public void runNegativeTests() {\n+        test01();\n+        test02(false);\n+        test02(true);\n+        test03(false);\n+        test03(true);\n+        test04(1, 16);\n+        test05(0);\n+    }\n+\n+    @DontInline\n+    static void escape(Object o) {}\n+\n+    @Test\n+    @IR(failOn = IRNode.LOAD_I, counts = {IRNode.ALLOC, \"1\"})\n+    public Point test11() {\n+        \/\/ p only escapes at return\n+        Point p = new Point();\n+        escape(null);\n+        p.x += p.y;\n+        return p;\n+    }\n+\n+    @Test\n+    @IR(failOn = IRNode.LOAD_I, counts = {IRNode.ALLOC, \"1\"})\n+    public Point test12(boolean b) {\n+        \/\/ p escapes in another branch\n+        Point p = new Point();\n+        if (b) {\n+            escape(p);\n+        } else {\n+            escape(null);\n+            p.x += p.y;\n+        }\n+        return p;\n+    }\n+\n+    @Test\n+    @IR(failOn = IRNode.LOAD_I, counts = {IRNode.ALLOC, \"1\"})\n+    public Point test13(boolean b) {\n+        \/\/ A Phi of p1 and Point.DEFAULT, but a store to Phi is after all the loads from p1\n+        Point p1 = new Point();\n+        Point p = b ? p1 : Point.DEFAULT;\n+        escape(null);\n+        p.x = p1.x + p1.y;\n+        return p;\n+    }\n+\n+    @Test\n+    @IR(failOn = IRNode.LOAD_I, counts = {IRNode.ALLOC, \"1\"})\n+    public int test14() {\n+        \/\/ Even if p escapes before the loads, if it is legal to execute the loads before the\n+        \/\/ store, then we can fold the loads\n+        Point p = new Point();\n+        escape(null);\n+        staticField = p;\n+        return p.x + p.y;\n+    }\n+\n+    @Test\n+    @IR(failOn = IRNode.LOAD_I, counts = {IRNode.ALLOC, \"1\"})\n+    public Point test15(int begin, int end) {\n+        \/\/ Fold the load that is a part of a cycle\n+        Point p = new Point();\n+        for (int i = begin; i < end; i *= 2) {\n+            p.x++;\n+            escape(null); \/\/ Force a memory Phi\n+        }\n+        p.x += p.y;\n+        return p;\n+    }\n+\n+    @Test\n+    @IR(failOn = IRNode.LOAD_I, counts = {IRNode.ALLOC, \"1\"})\n+    public Point test16(int begin, int end, boolean b) {\n+        \/\/ A cycle and a Phi, this time the store is at a different\n+        Point p1 = new Point();\n+        \/\/ This store is not on a Phi involving p1, so it does not interfere\n+        Point.DEFAULT.y = 3;\n+        Point p = p1;\n+        for (int i = begin; i < end; i += 2) {\n+            if (b) {\n+                p = p1;\n+            } else {\n+                p = Point.DEFAULT;\n+            }\n+            b = !b;\n+\n+            p.x = p1.y + 3;\n+            escape(null); \/\/ Force a memory Phi\n+        }\n+        p1.x = p1.y;\n+        return p;\n+    }\n+\n+    @Test\n+    @IR(counts = {IRNode.LOAD_I, \"1\", IRNode.ALLOC_ARRAY, \"1\"})\n+    public int test17(int idx) {\n+        \/\/ Array\n+        int[] a = new int[2];\n+        a[0] = 1;\n+        a[1] = 2;\n+        int res = a[idx & 1];\n+        escape(null);\n+        res += a[0] + a[1];\n+        escape(a);\n+        return res;\n+    }\n+\n+    @Test\n+    @IR(failOn = IRNode.LOAD_I, counts = {IRNode.ALLOC_ARRAY, \"1\"})\n+    public int test18(int idx) {\n+        \/\/ Array, even if we will give up if we encounter a[idx & 1] = 3, we meet a[0] = 4 first,\n+        \/\/ so the load int res = a[0] can still be folded\n+        int[] a = new int[2];\n+        a[0] = 1;\n+        a[1] = 2;\n+        escape(null);\n+        a[idx & 1] = 3;\n+        a[0] = 4;\n+        escape(null);\n+        int res = a[0];\n+        escape(a);\n+        return res;\n+    }\n+\n+    @Test\n+    @IR(counts = {IRNode.LOAD_I, \"2\", IRNode.ALLOC, \"1\"})\n+    public int test01() {\n+        Point p = new Point();\n+        staticField = p;\n+        \/\/ Actually, the only fence that requires the following loads to be executed after the\n+        \/\/ store is a fullFence\n+        VarHandle.fullFence();\n+        return p.x + p.y;\n+    }\n+\n+    @Test\n+    @IR(counts = {IRNode.LOAD_I, \"1\", IRNode.ALLOC, \"1\"})\n+    public int test02(boolean b) {\n+        Point p = new Point();\n+        if (b) {\n+            escape(p);\n+            \/\/ p escaped, so the load must not be removed\n+            return p.x;\n+        } else {\n+            escape(null);\n+            return 0;\n+        }\n+    }\n+\n+    @Test\n+    @IR(counts = {IRNode.LOAD_I, \"1\", IRNode.ALLOC, \"1\"})\n+    public int test03(boolean b) {\n+        Point p = new Point();\n+        if (b) {\n+            escape(p);\n+        }\n+        \/\/ p escaped, so the load must not be removed\n+        return p.x;\n+    }\n+\n+    @Test\n+    @IR(counts = {IRNode.LOAD_I, \"> 0\", IRNode.ALLOC, \"1\"})\n+    public Point test04(int begin, int end) {\n+        Point p = new Point();\n+        for (int i = begin; i < end; i *= 2) {\n+            \/\/ p escaped here because this is a loop\n+            p.x++;\n+            escape(p);\n+        }\n+        return p;\n+    }\n+\n+    @Test\n+    @IR(counts = {IRNode.LOAD_I, \"2\", IRNode.ALLOC_ARRAY, \"1\"})\n+    public int test05(int idx) {\n+        int[] a = new int[2];\n+        a[0] = 1;\n+        a[1] = 2;\n+        escape(null);\n+        a[idx & 1] = 3;\n+        \/\/ Cannot fold the loads because we do not know which element is written to by\n+        \/\/ a[idx & 1] = 3\n+        return a[0] + a[1];\n+    }\n+}\n","filename":"test\/hotspot\/jtreg\/compiler\/escapeAnalysis\/TestLoadFolding.java","additions":258,"deletions":0,"binary":false,"changes":258,"status":"added"}]}