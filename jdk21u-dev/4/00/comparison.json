{"files":[{"patch":"@@ -2638,0 +2638,7 @@\n+  \/\/ There might be a volatile load before this Unsafe CAS.\n+  if (support_IRIW_for_not_multiple_copy_atomic_cpu) {\n+    __ sync();\n+  } else {\n+    __ lwsync();\n+  }\n+\n@@ -2999,3 +3006,18 @@\n-  Register Rco = noreg;\n-  if (UseCompressedOops && data->is_oop()) {\n-    Rco = __ encode_heap_oop(Rtmp, data->as_register());\n+  Register Robj = noreg;\n+  if (data->is_oop()) {\n+    if (UseCompressedOops) {\n+      Robj = __ encode_heap_oop(Rtmp, data->as_register());\n+    } else {\n+      Robj = data->as_register();\n+      if (Robj == dest->as_register()) { \/\/ May happen with ZGC.\n+        __ mr(Rtmp, Robj);\n+        Robj = Rtmp;\n+      }\n+    }\n+  }\n+\n+  \/\/ There might be a volatile load before this Unsafe OP.\n+  if (support_IRIW_for_not_multiple_copy_atomic_cpu) {\n+    __ sync();\n+  } else {\n+    __ lwsync();\n@@ -3021,0 +3043,1 @@\n+    assert_different_registers(Rptr, Rold, Robj);\n@@ -3022,1 +3045,0 @@\n-      assert_different_registers(Rptr, Rold, Rco);\n@@ -3024,1 +3046,1 @@\n-      __ stwcx_(Rco, Rptr);\n+      __ stwcx_(Robj, Rptr);\n@@ -3026,7 +3048,0 @@\n-      Register Robj = data->as_register();\n-      assert_different_registers(Rptr, Rold, Rtmp);\n-      assert_different_registers(Rptr, Robj, Rtmp);\n-      if (Robj == Rold) { \/\/ May happen with ZGC.\n-        __ mr(Rtmp, Robj);\n-        Robj = Rtmp;\n-      }\n@@ -3060,0 +3075,6 @@\n+\n+  if (support_IRIW_for_not_multiple_copy_atomic_cpu) {\n+    __ isync();\n+  } else {\n+    __ sync();\n+  }\n","filename":"src\/hotspot\/cpu\/ppc\/c1_LIRAssembler_ppc.cpp","additions":33,"deletions":12,"binary":false,"changes":45,"status":"modified"},{"patch":"@@ -642,7 +642,0 @@\n-  \/\/ Volatile load may be followed by Unsafe CAS.\n-  if (support_IRIW_for_not_multiple_copy_atomic_cpu) {\n-    __ membar();\n-  } else {\n-    __ membar_release();\n-  }\n-\n@@ -673,8 +666,0 @@\n-\n-  \/\/ Volatile load may be followed by Unsafe CAS.\n-  if (support_IRIW_for_not_multiple_copy_atomic_cpu) {\n-    __ membar();\n-  } else {\n-    __ membar_release();\n-  }\n-\n@@ -682,6 +667,0 @@\n-\n-  if (support_IRIW_for_not_multiple_copy_atomic_cpu) {\n-    __ membar_acquire();\n-  } else {\n-    __ membar();\n-  }\n@@ -697,8 +676,0 @@\n-\n-  \/\/ Volatile load may be followed by Unsafe CAS.\n-  if (support_IRIW_for_not_multiple_copy_atomic_cpu) {\n-    __ membar(); \/\/ To be safe. Unsafe semantics are unclear.\n-  } else {\n-    __ membar_release();\n-  }\n-\n@@ -706,6 +677,0 @@\n-\n-  if (support_IRIW_for_not_multiple_copy_atomic_cpu) {\n-    __ membar_acquire();\n-  } else {\n-    __ membar();\n-  }\n","filename":"src\/hotspot\/cpu\/ppc\/c1_LIRGenerator_ppc.cpp","additions":0,"deletions":35,"binary":false,"changes":35,"status":"modified"},{"patch":"@@ -2,2 +2,2 @@\n- * Copyright (c) 2018, 2021, Red Hat, Inc. All rights reserved.\n- * Copyright (c) 2012, 2021 SAP SE. All rights reserved.\n+ * Copyright (c) 2018, 2023, Red Hat, Inc. All rights reserved.\n+ * Copyright (c) 2012, 2023 SAP SE. All rights reserved.\n@@ -56,2 +56,7 @@\n-  \/\/ Due to the memory barriers emitted in ShenandoahBarrierSetC1::atomic_cmpxchg_at_resolved,\n-  \/\/ there is no need to specify stronger memory semantics.\n+  \/\/ There might be a volatile load before this Unsafe CAS.\n+  if (support_IRIW_for_not_multiple_copy_atomic_cpu) {\n+    __ sync();\n+  } else {\n+    __ lwsync();\n+  }\n+\n@@ -66,0 +71,6 @@\n+  if (support_IRIW_for_not_multiple_copy_atomic_cpu) {\n+    __ isync();\n+  } else {\n+    __ sync();\n+  }\n+\n@@ -83,8 +94,0 @@\n-    if (ShenandoahCASBarrier) {\n-      if (support_IRIW_for_not_multiple_copy_atomic_cpu) {\n-        __ membar();\n-      } else {\n-        __ membar_release();\n-      }\n-    }\n-\n@@ -107,6 +110,0 @@\n-      if (support_IRIW_for_not_multiple_copy_atomic_cpu) {\n-        __ membar_acquire();\n-      } else {\n-        __ membar();\n-      }\n-\n@@ -128,6 +125,0 @@\n-  if (support_IRIW_for_not_multiple_copy_atomic_cpu) {\n-    __ membar();\n-  } else {\n-    __ membar_release();\n-  }\n-\n@@ -155,6 +146,0 @@\n-  if (support_IRIW_for_not_multiple_copy_atomic_cpu) {\n-    __ membar_acquire();\n-  } else {\n-    __ membar();\n-  }\n-\n","filename":"src\/hotspot\/cpu\/ppc\/gc\/shenandoah\/c1\/shenandoahBarrierSetC1_ppc.cpp","additions":15,"deletions":30,"binary":false,"changes":45,"status":"modified"},{"patch":"@@ -343,1 +343,2 @@\n-                MacroAssembler::MemBarNone, MacroAssembler::cmpxchgx_hint_atomic_update());\n+                MacroAssembler::MemBarNone, MacroAssembler::cmpxchgx_hint_atomic_update(),\n+                noreg, need_restore ? nullptr : &slow_path);\n@@ -346,0 +347,1 @@\n+      __ bne(CCR0, slow_path);\n@@ -347,1 +349,0 @@\n-    __ bne(CCR0, slow_path);\n","filename":"src\/hotspot\/cpu\/ppc\/gc\/z\/zBarrierSetAssembler_ppc.cpp","additions":3,"deletions":2,"binary":false,"changes":5,"status":"modified"}]}