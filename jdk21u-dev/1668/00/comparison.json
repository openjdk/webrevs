{"files":[{"patch":"@@ -3558,0 +3558,8 @@\n+void Assembler::vmovsd(XMMRegister dst, XMMRegister src, XMMRegister src2) {\n+  assert(UseAVX > 0, \"Requires some form of AVX\");\n+  InstructionMark im(this);\n+  InstructionAttr attributes(AVX_128bit, \/* rex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n+  int encode = vex_prefix_and_encode(src2->encoding(), src->encoding(), dst->encoding(), VEX_SIMD_F2, VEX_OPCODE_0F, &attributes);\n+  emit_int16(0x11, (0xC0 | encode));\n+}\n+\n@@ -6550,0 +6558,23 @@\n+void Assembler::evfnmadd213sd(XMMRegister dst, XMMRegister src1, XMMRegister src2, EvexRoundPrefix rmode) { \/\/ Need to add rmode for rounding mode support\n+  assert(VM_Version::supports_evex(), \"\");\n+  InstructionAttr attributes(rmode, \/* vex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n+  attributes.set_extended_context();\n+  attributes.set_is_evex_instruction();\n+  int encode = vex_prefix_and_encode(dst->encoding(), src1->encoding(), src2->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int16((unsigned char)0xAD, (0xC0 | encode));\n+}\n+\n+void Assembler::vfnmadd213sd(XMMRegister dst, XMMRegister src1, XMMRegister src2) {\n+  assert(VM_Version::supports_fma(), \"\");\n+  InstructionAttr attributes(AVX_128bit, \/* vex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n+  int encode = vex_prefix_and_encode(dst->encoding(), src1->encoding(), src2->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int16((unsigned char)0xAD, (0xC0 | encode));\n+}\n+\n+void Assembler::vfnmadd231sd(XMMRegister dst, XMMRegister src1, XMMRegister src2) {\n+  assert(VM_Version::supports_fma(), \"\");\n+  InstructionAttr attributes(AVX_128bit, \/* vex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n+  int encode = vex_prefix_and_encode(dst->encoding(), src1->encoding(), src2->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int16((unsigned char)0xBD, (0xC0 | encode));\n+}\n+\n@@ -6911,0 +6942,16 @@\n+void Assembler::vroundsd(XMMRegister dst, XMMRegister src, XMMRegister src2, int32_t rmode) {\n+  assert(VM_Version::supports_avx(), \"\");\n+  assert(rmode <= 0x0f, \"rmode 0x%x\", rmode);\n+  InstructionAttr attributes(AVX_128bit, \/* vex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n+  int encode = vex_prefix_and_encode(dst->encoding(), src->encoding(), src2->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_3A, &attributes);\n+  emit_int24(0x0B, (0xC0 | encode), (rmode));\n+}\n+\n+void Assembler::vrndscalesd(XMMRegister dst,  XMMRegister src1, XMMRegister src2, int32_t rmode) {\n+  assert(VM_Version::supports_evex(), \"requires EVEX support\");\n+  InstructionAttr attributes(AVX_128bit, \/* vex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n+  attributes.set_is_evex_instruction();\n+  int encode = vex_prefix_and_encode(dst->encoding(), src1->encoding(), src2->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_3A, &attributes);\n+  emit_int24(0x0B, (0xC0 | encode), (rmode));\n+}\n+\n@@ -8876,0 +8923,13 @@\n+void Assembler::extractps(Register dst, XMMRegister src, uint8_t imm8) {\n+  assert(VM_Version::supports_sse4_1(), \"\");\n+  assert(imm8 <= 0x03, \"imm8: %u\", imm8);\n+  InstructionAttr attributes(AVX_128bit, \/* vex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n+  int encode = simd_prefix_and_encode(src, xnoreg, as_XMMRegister(dst->encoding()), VEX_SIMD_66, VEX_OPCODE_0F_3A, &attributes);\n+  \/\/ imm8:\n+  \/\/ 0x00 - extract from bits 31:0\n+  \/\/ 0x01 - extract from bits 63:32\n+  \/\/ 0x02 - extract from bits 95:64\n+  \/\/ 0x03 - extract from bits 127:96\n+  emit_int24(0x17, (0xC0 | encode), imm8 & 0x03);\n+}\n+\n@@ -9550,0 +9610,9 @@\n+void Assembler::evdivsd(XMMRegister dst, XMMRegister nds, XMMRegister src, EvexRoundPrefix rmode) {\n+  assert(VM_Version::supports_evex(), \"\");\n+  InstructionAttr attributes(rmode, \/* vex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n+  attributes.set_extended_context();\n+  attributes.set_is_evex_instruction();\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_F2, VEX_OPCODE_0F, &attributes);\n+  emit_int16(0x5E, (0xC0 | encode));\n+}\n+\n","filename":"src\/hotspot\/cpu\/x86\/assembler_x86.cpp","additions":69,"deletions":0,"binary":false,"changes":69,"status":"modified"},{"patch":"@@ -531,0 +531,7 @@\n+  enum EvexRoundPrefix {\n+    EVEX_RNE = 0x0,\n+    EVEX_RD  = 0x1,\n+    EVEX_RU  = 0x2,\n+    EVEX_RZ  = 0x3\n+  };\n+\n@@ -889,0 +896,2 @@\n+  void vmovsd(XMMRegister dst, XMMRegister src, XMMRegister src2);\n+\n@@ -2248,0 +2257,1 @@\n+  void evdivsd(XMMRegister dst, XMMRegister nds, XMMRegister src, EvexRoundPrefix rmode);\n@@ -2251,0 +2261,3 @@\n+  void vfnmadd213sd(XMMRegister dst, XMMRegister nds, XMMRegister src);\n+  void evfnmadd213sd(XMMRegister dst, XMMRegister nds, XMMRegister src, EvexRoundPrefix rmode);\n+  void vfnmadd231sd(XMMRegister dst, XMMRegister src1, XMMRegister src2);\n@@ -2340,0 +2353,1 @@\n+  void vrndscalesd(XMMRegister dst,  XMMRegister src1,  XMMRegister src2, int32_t rmode);\n@@ -2342,0 +2356,2 @@\n+  void vroundsd(XMMRegister dst, XMMRegister src, XMMRegister src2, int32_t rmode);\n+  void vroundsd(XMMRegister dst, XMMRegister src, Address src2, int32_t rmode);\n@@ -2725,0 +2741,2 @@\n+  void extractps(Register dst, XMMRegister src, uint8_t imm8);\n+\n@@ -2958,0 +2976,2 @@\n+  void set_extended_context(void) { _is_extended_context = true; }\n+\n","filename":"src\/hotspot\/cpu\/x86\/assembler_x86.hpp","additions":20,"deletions":0,"binary":false,"changes":20,"status":"modified"},{"patch":"@@ -971,1 +971,1 @@\n-      __ call_runtime_leaf(StubRoutines::dpow(), getThreadTemp(), result_reg, cc->args());\n+        __ call_runtime_leaf(StubRoutines::dpow(), getThreadTemp(), result_reg, cc->args());\n","filename":"src\/hotspot\/cpu\/x86\/c1_LIRGenerator_x86.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -98,0 +98,2 @@\n+  const bool is_LP64 = LP64_ONLY(true) NOT_LP64(false);\n+  if (!is_LP64 || UseAVX < 1 || !UseFMA) {\n@@ -108,0 +110,8 @@\n+  } else {\n+    assert(StubRoutines::fmod() != nullptr, \"\");\n+    jdouble (*addr)(jdouble, jdouble) = (double (*)(double, double))StubRoutines::fmod();\n+    jdouble dx = (jdouble) x;\n+    jdouble dy = (jdouble) y;\n+\n+    retval = (jfloat) (*addr)(dx, dy);\n+  }\n@@ -113,0 +123,2 @@\n+  const bool is_LP64 = LP64_ONLY(true) NOT_LP64(false);\n+  if (!is_LP64 || UseAVX < 1 || !UseFMA) {\n@@ -123,0 +135,6 @@\n+  } else {\n+    assert(StubRoutines::fmod() != nullptr, \"\");\n+    jdouble (*addr)(jdouble, jdouble) = (double (*)(double, double))StubRoutines::fmod();\n+\n+    retval = (*addr)(x, y);\n+  }\n","filename":"src\/hotspot\/cpu\/x86\/sharedRuntime_x86.cpp","additions":18,"deletions":0,"binary":false,"changes":18,"status":"modified"},{"patch":"@@ -4031,0 +4031,4 @@\n+\n+  if ((UseAVX >= 1) && (VM_Version::supports_avx512vlbwdq() || VM_Version::supports_fma())) {\n+    StubRoutines::_fmod = generate_libmFmod(); \/\/ from stubGenerator_x86_64_fmod.cpp\n+  }\n","filename":"src\/hotspot\/cpu\/x86\/stubGenerator_x86_64.cpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -490,0 +490,1 @@\n+  address generate_libmFmod();\n","filename":"src\/hotspot\/cpu\/x86\/stubGenerator_x86_64.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -0,0 +1,524 @@\n+\/*\n+ * Copyright (c) 2023, Intel Corporation. All rights reserved.\n+ * Intel Math Library (LIBM) Source Code\n+ *\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"macroAssembler_x86.hpp\"\n+#include \"stubGenerator_x86_64.hpp\"\n+\n+\/******************************************************************************\/\n+\/\/                     ALGORITHM DESCRIPTION - FMOD()\n+\/\/                     ---------------------\n+\/\/\n+\/\/ If either value1 or value2 is NaN, the result is NaN.\n+\/\/\n+\/\/ If neither value1 nor value2 is NaN, the sign of the result equals the sign of the dividend.\n+\/\/\n+\/\/ If the dividend is an infinity or the divisor is a zero or both, the result is NaN.\n+\/\/\n+\/\/ If the dividend is finite and the divisor is an infinity, the result equals the dividend.\n+\/\/\n+\/\/ If the dividend is a zero and the divisor is finite, the result equals the dividend.\n+\/\/\n+\/\/ In the remaining cases, where neither operand is an infinity, a zero, or NaN, the floating-point\n+\/\/ remainder result from a dividend value1 and a divisor value2 is defined by the mathematical\n+\/\/ relation result = value1 - (value2 * q), where q is an integer that is negative only if\n+\/\/ value1 \/ value2 is negative, and positive only if value1 \/ value2 is positive, and whose magnitude\n+\/\/ is as large as possible without exceeding the magnitude of the true mathematical quotient of value1 and value2.\n+\/\/\n+\/******************************************************************************\/\n+\n+#define __ _masm->\n+\n+ATTRIBUTE_ALIGNED(32) static const uint64_t CONST_NaN[] = {\n+    0x7FFFFFFFFFFFFFFFULL, 0x7FFFFFFFFFFFFFFFULL   \/\/ NaN vector\n+};\n+ATTRIBUTE_ALIGNED(32) static const uint64_t CONST_1p260[] = {\n+    0x5030000000000000ULL,    \/\/ 0x1p+260\n+};\n+\n+ATTRIBUTE_ALIGNED(32) static const uint64_t CONST_MAX[] = {\n+    0x7FEFFFFFFFFFFFFFULL,    \/\/ Max\n+};\n+\n+ATTRIBUTE_ALIGNED(32) static const uint64_t CONST_INF[] = {\n+    0x7FF0000000000000ULL,    \/\/ Inf\n+};\n+\n+ATTRIBUTE_ALIGNED(32) static const uint64_t CONST_e307[] = {\n+    0x7FE0000000000000ULL\n+};\n+\n+address StubGenerator::generate_libmFmod() {\n+  StubCodeMark mark(this, \"StubRoutines\", \"libmFmod\");\n+  address start = __ pc();\n+  __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n+\n+  if (VM_Version::supports_avx512vlbwdq()) {     \/\/ AVX512 version\n+\n+    \/\/ Source used to generate the AVX512 fmod assembly below:\n+    \/\/\n+    \/\/ #include <ia32intrin.h>\n+    \/\/ #include <emmintrin.h>\n+    \/\/ #pragma float_control(precise, on)\n+    \/\/\n+    \/\/ #define UINT32 unsigned int\n+    \/\/ #define SINT32 int\n+    \/\/ #define UINT64 unsigned __int64\n+    \/\/ #define SINT64 __int64\n+    \/\/\n+    \/\/ #define DP_FMA(a, b, c)    __fence(_mm_cvtsd_f64(_mm_fmadd_sd(_mm_set_sd(a), _mm_set_sd(b), _mm_set_sd(c))))\n+    \/\/ #define DP_FMA_RN(a, b, c)    _mm_cvtsd_f64(_mm_fmadd_round_sd(_mm_set_sd(a), _mm_set_sd(b), _mm_set_sd(c), (_MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC)))\n+    \/\/ #define DP_FMA_RZ(a, b, c) __fence(_mm_cvtsd_f64(_mm_fmadd_round_sd(_mm_set_sd(a), _mm_set_sd(b), _mm_set_sd(c), (_MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC))))\n+    \/\/\n+    \/\/ #define DP_ROUND_RZ(a)   _mm_cvtsd_f64(_mm_roundscale_sd(_mm_setzero_pd(), _mm_set_sd(a), (_MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC)))\n+    \/\/\n+    \/\/ #define DP_CONST(C)    _castu64_f64(0x##C##ull)\n+    \/\/ #define DP_AND(X, Y)   _mm_cvtsd_f64(_mm_and_pd(_mm_set_sd(X), _mm_set_sd(Y)))\n+    \/\/ #define DP_XOR(X, Y)   _mm_cvtsd_f64(_mm_xor_pd(_mm_set_sd(X), _mm_set_sd(Y)))\n+    \/\/ #define DP_OR(X, Y)    _mm_cvtsd_f64(_mm_or_pd(_mm_set_sd(X), _mm_set_sd(Y)))\n+    \/\/ #define DP_DIV_RZ(a, b) __fence(_mm_cvtsd_f64(_mm_div_round_sd(_mm_set_sd(a), _mm_set_sd(b), (_MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC))))\n+    \/\/ #define DP_FNMA(a, b, c)    __fence(_mm_cvtsd_f64(_mm_fnmadd_sd(_mm_set_sd(a), _mm_set_sd(b), _mm_set_sd(c))))\n+    \/\/ #define DP_FNMA_RZ(a, b, c) __fence(_mm_cvtsd_f64(_mm_fnmadd_round_sd(_mm_set_sd(a), _mm_set_sd(b), _mm_set_sd(c), (_MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC))))\n+    \/\/\n+    \/\/ #define D2L(x)  _mm_castpd_si128(x)\n+    \/\/ \/\/ transfer highest 32 bits (of low 64b) to GPR\n+    \/\/ #define TRANSFER_HIGH_INT32(X)   _mm_extract_epi32(D2L(_mm_set_sd(X)), 1)\n+    \/\/\n+    \/\/ double fmod(double x, double y)\n+    \/\/ {\n+    \/\/ double a, b, sgn_a, q, bs, bs2;\n+    \/\/ unsigned eq;\n+\n+    Label L_5280, L_52a0, L_5256, L_5300, L_5320, L_52c0, L_52d0, L_5360, L_5380, L_53b0, L_5390;\n+    Label L_53c0, L_52a6, L_53d0, L_exit;\n+\n+    __ movdqa(xmm2, xmm0);\n+    \/\/     \/\/ |x|, |y|\n+    \/\/     a = DP_AND(x, DP_CONST(7fffffffffffffff));\n+    __ movq(xmm0, xmm0);\n+    __ mov64(rax, 0x7FFFFFFFFFFFFFFFULL);\n+    __ evpbroadcastq(xmm3, rax, Assembler::AVX_128bit);\n+    __ vpand(xmm6, xmm0, xmm3, Assembler::AVX_128bit);\n+    \/\/     b = DP_AND(y, DP_CONST(7fffffffffffffff));\n+    __ vpand(xmm4, xmm1, xmm3, Assembler::AVX_128bit);\n+    \/\/     \/\/ sign(x)\n+    \/\/     sgn_a = DP_XOR(x, a);\n+    __ vpxor(xmm3, xmm6, xmm0, Assembler::AVX_128bit);\n+    \/\/     q = DP_DIV_RZ(a, b);\n+    __ movq(xmm5, xmm4);\n+    __ evdivsd(xmm0, xmm6, xmm5, Assembler::EVEX_RZ);\n+    \/\/     q = DP_ROUND_RZ(q);\n+    __ movq(xmm0, xmm0);\n+    \/\/     a = DP_AND(x, DP_CONST(7fffffffffffffff));\n+    __ vxorpd(xmm7, xmm7, xmm7, Assembler::AVX_128bit);\n+    \/\/     q = DP_ROUND_RZ(q);\n+    __ vroundsd(xmm0, xmm7, xmm0, 0xb);\n+    \/\/     eq = TRANSFER_HIGH_INT32(q);\n+    __ extractps(rax, xmm0, 1);\n+    \/\/     if (!eq)  return x + sgn_a;\n+    __ testl(rax, rax);\n+    __ jcc(Assembler::equal, L_5280);\n+    \/\/     if (eq >= 0x7fefffffu) goto SPECIAL_FMOD;\n+    __ cmpl(rax, 0x7feffffe);\n+    __ jcc(Assembler::belowEqual, L_52a0);\n+    __ vpxor(xmm2, xmm2, xmm2, Assembler::AVX_128bit);\n+    \/\/ SPECIAL_FMOD:\n+    \/\/\n+    \/\/     \/\/ y==0 or x==Inf?\n+    \/\/     if ((b == 0.0) || (!(a <= DP_CONST(7fefffffffffffff))))\n+    __ ucomisd(xmm4, xmm2);\n+    __ jcc(Assembler::notEqual, L_5256);\n+    __ jcc(Assembler::noParity, L_5300);\n+    __ bind(L_5256);\n+    __ movsd(xmm2, ExternalAddress((address)CONST_MAX), rax);\n+    __ ucomisd(xmm2, xmm6);\n+    __ jcc(Assembler::below, L_5300);\n+    __ movsd(xmm0, ExternalAddress((address)CONST_INF), rax);\n+    \/\/         return DP_FNMA(b, q, a);    \/\/ NaN\n+    \/\/     \/\/ y is NaN?\n+    \/\/     if (!(b <= DP_CONST(7ff0000000000000))) return y + y;\n+    __ ucomisd(xmm0, xmm4);\n+    __ jcc(Assembler::aboveEqual, L_5320);\n+    __ vaddsd(xmm0, xmm1, xmm1);\n+    __ jmp(L_exit);\n+    \/\/     if (!eq)  return x + sgn_a;\n+    __ align32();\n+    __ bind(L_5280);\n+    __ vaddsd(xmm0, xmm3, xmm2);\n+    __ jmp(L_exit);\n+    \/\/     a = DP_FNMA_RZ(b, q, a);\n+    __ align(8);\n+    __ bind(L_52a0);\n+    __ evfnmadd213sd(xmm0, xmm4, xmm6, Assembler::EVEX_RZ);\n+    \/\/     while (b <= a)\n+    __ bind(L_52a6);\n+    __ ucomisd(xmm0, xmm4);\n+    __ jcc(Assembler::aboveEqual, L_52c0);\n+    \/\/     a = DP_XOR(a, sgn_a);\n+    __ vpxor(xmm0, xmm3, xmm0, Assembler::AVX_128bit);\n+    __ jmp(L_exit);\n+    __ bind(L_52c0);\n+    __ movq(xmm6, xmm0);\n+    \/\/         q = DP_ROUND_RZ(q);\n+    __ vpxor(xmm1, xmm1, xmm1, Assembler::AVX_128bit);\n+    __ align32();\n+    __ bind(L_52d0);\n+    \/\/         q = DP_DIV_RZ(a, b);\n+    __ evdivsd(xmm2, xmm6, xmm5, Assembler::EVEX_RZ);\n+    \/\/         q = DP_ROUND_RZ(q);\n+    __ movq(xmm2, xmm2);\n+    __ vroundsd(xmm2, xmm1, xmm2, 0xb);\n+    \/\/     a = DP_FNMA_RZ(b, q, a);\n+    __ evfnmadd213sd(xmm2, xmm4, xmm0, Assembler::EVEX_RZ);\n+    \/\/     while (b <= a)\n+    __ ucomisd(xmm2, xmm4);\n+    __ movq(xmm6, xmm2);\n+    __ movapd(xmm0, xmm2);\n+    __ jcc(Assembler::aboveEqual, L_52d0);\n+    \/\/     a = DP_XOR(a, sgn_a);\n+    __ vpxor(xmm0, xmm3, xmm2, Assembler::AVX_128bit);\n+    __ jmp(L_exit);\n+    \/\/         return DP_FNMA(b, q, a);    \/\/ NaN\n+    __ bind(L_5300);\n+    __ vfnmadd213sd(xmm0, xmm4, xmm6);\n+    __ jmp(L_exit);\n+    \/\/     bs = b * DP_CONST(7fe0000000000000);\n+    __ bind(L_5320);\n+    __ vmulsd(xmm1, xmm4, ExternalAddress((address)CONST_e307), rax);\n+    \/\/     q = DP_DIV_RZ(a, bs);\n+    __ movq(xmm2, xmm1);\n+    __ evdivsd(xmm0, xmm6, xmm2, Assembler::EVEX_RZ);\n+    \/\/     q = DP_ROUND_RZ(q);\n+    __ movq(xmm0, xmm0);\n+    __ vroundsd(xmm7, xmm7, xmm0, 0xb);\n+    \/\/     eq = TRANSFER_HIGH_INT32(q);\n+    __ extractps(rax, xmm7, 1);\n+    \/\/     if (eq >= 0x7fefffffu)\n+    __ cmpl(rax, 0x7fefffff);\n+    __ jcc(Assembler::below, L_5360);\n+    \/\/         \/\/ b* 2*1023 * 2^1023\n+    \/\/         bs2 = bs * DP_CONST(7fe0000000000000);\n+    __ vmulsd(xmm0, xmm1, ExternalAddress((address)CONST_e307), rax);\n+    \/\/         while (bs2 <= a)\n+    __ ucomisd(xmm6, xmm0);\n+    __ jcc(Assembler::aboveEqual, L_5380);\n+    __ movapd(xmm7, xmm6);\n+    __ jmp(L_53b0);\n+    \/\/         a = DP_FNMA_RZ(b, q, a);\n+    __ bind(L_5360);\n+    __ evfnmadd213sd(xmm7, xmm1, xmm6, Assembler::EVEX_RZ);\n+    __ jmp(L_53b0);\n+    \/\/             q = DP_ROUND_RZ(q);\n+    __ bind(L_5380);\n+    __ vxorpd(xmm8, xmm8, xmm8, Assembler::AVX_128bit);\n+    \/\/             q = DP_DIV_RZ(qa, bs2);\n+    __ align32();\n+    __ bind(L_5390);\n+    __ evdivsd(xmm7, xmm6, xmm0, Assembler::EVEX_RZ);\n+    \/\/             q = DP_ROUND_RZ(q);\n+    __ movq(xmm7, xmm7);\n+    __ vroundsd(xmm7, xmm8, xmm7, 0xb);\n+    \/\/             a = DP_FNMA_RZ(bs2, q, a);\n+    __ evfnmadd213sd(xmm7, xmm0, xmm6, Assembler::EVEX_RZ);\n+    \/\/         while (bs2 <= a)\n+    __ ucomisd(xmm7, xmm0);\n+    __ movapd(xmm6, xmm7);\n+    __ jcc(Assembler::aboveEqual, L_5390);\n+    \/\/     while (bs <= a)\n+    __ bind(L_53b0);\n+    __ ucomisd(xmm7, xmm1);\n+    __ jcc(Assembler::aboveEqual, L_53c0);\n+    __ movapd(xmm0, xmm7);\n+    __ jmp(L_52a6);\n+    \/\/         q = DP_ROUND_RZ(q);\n+    __ bind(L_53c0);\n+    __ vxorpd(xmm6, xmm6, xmm6, Assembler::AVX_128bit);\n+    \/\/         q = DP_DIV_RZ(a, bs);\n+    __ align32();\n+    __ bind(L_53d0);\n+    __ evdivsd(xmm0, xmm7, xmm2, Assembler::EVEX_RZ);\n+    \/\/         q = DP_ROUND_RZ(q);\n+    __ movq(xmm0, xmm0);\n+    __ vroundsd(xmm0, xmm6, xmm0, 0xb);\n+    \/\/         a = DP_FNMA_RZ(bs, q, a);\n+    __ evfnmadd213sd(xmm0, xmm1, xmm7, Assembler::EVEX_RZ);\n+    \/\/     while (bs <= a)\n+    __ ucomisd(xmm0, xmm1);\n+    __ movapd(xmm7, xmm0);\n+    __ jcc(Assembler::aboveEqual, L_53d0);\n+    __ jmp(L_52a6);\n+\n+    __ bind(L_exit);\n+\n+\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n+\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n+\/\/                         AVX2 code\n+\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n+\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n+  } else if (VM_Version::supports_fma()) {       \/\/ AVX2 version\n+\n+    Label L_104a, L_11bd, L_10c1, L_1090, L_11b9, L_10e7, L_11af, L_111c, L_10f3, L_116e, L_112a;\n+    Label L_1173, L_1157, L_117f, L_11a0;\n+\n+    \/\/   double fmod(double x, double y)\n+    \/\/ {\n+    \/\/ double a, b, sgn_a, q, bs, bs2, corr, res;\n+    \/\/ unsigned eq;\n+    \/\/ unsigned mxcsr, mxcsr_rz;\n+\n+    \/\/   __asm { stmxcsr DWORD PTR[mxcsr] }\n+    \/\/   mxcsr_rz = 0x7f80 | mxcsr;\n+    __ push(rax);\n+    __ stmxcsr(Address(rsp, 0));\n+    __ movl(rax, Address(rsp, 0));\n+    __ movl(rcx, rax);\n+    __ orl(rcx, 0x7f80);\n+    __ movl(Address(rsp, 0x04), rcx);\n+\n+    \/\/     \/\/ |x|, |y|\n+    \/\/     a = DP_AND(x, DP_CONST(7fffffffffffffff));\n+    __ movq(xmm2, xmm0);\n+    __ vmovdqu(xmm3, ExternalAddress((address)CONST_NaN), rcx);\n+    __ vpand(xmm4, xmm2, xmm3, Assembler::AVX_128bit);\n+    \/\/     b = DP_AND(y, DP_CONST(7fffffffffffffff));\n+    __ vpand(xmm3, xmm1, xmm3, Assembler::AVX_128bit);\n+    \/\/   \/\/ sign(x)\n+    \/\/   sgn_a = DP_XOR(x, a);\n+    __ mov64(rcx, 0x8000000000000000ULL);\n+    __ movq(xmm5, rcx);\n+    __ vpand(xmm2, xmm2, xmm5, Assembler::AVX_128bit);\n+\n+    \/\/   if (a < b)  return x + sgn_a;\n+    __ ucomisd(xmm3, xmm4);\n+    __ jcc(Assembler::belowEqual, L_104a);\n+    __ vaddsd(xmm0, xmm2, xmm0);\n+    __ jmp(L_11bd);\n+\n+    \/\/   if (((mxcsr & 0x6000)!=0x2000) && (a < b * 0x1p+260))\n+    __ bind(L_104a);\n+    __ andl(rax, 0x6000);\n+    __ cmpl(rax, 0x2000);\n+    __ jcc(Assembler::equal, L_10c1);\n+    __ vmulsd(xmm0, xmm3, ExternalAddress((address)CONST_1p260), rax);\n+    __ ucomisd(xmm0, xmm4);\n+    __ jcc(Assembler::belowEqual, L_10c1);\n+    \/\/   {\n+    \/\/     q = DP_DIV(a, b);\n+    __ vdivpd(xmm0, xmm4, xmm3, Assembler::AVX_128bit);\n+    \/\/     corr = DP_SHR(DP_FNMA(b, q, a), 63);\n+    __ movapd(xmm1, xmm0);\n+    __ vfnmadd213sd(xmm1, xmm3, xmm4);\n+    __ movq(xmm5, xmm1);\n+    __ vpxor(xmm1, xmm1, xmm1, Assembler::AVX_128bit);\n+    __ vpcmpgtq(xmm5, xmm1, xmm5, Assembler::AVX_128bit);\n+    \/\/     q = DP_PSUBQ(q, corr);\n+    __ vpaddq(xmm0, xmm5, xmm0, Assembler::AVX_128bit);\n+    \/\/     q = DP_TRUNC(q);\n+    __ vroundsd(xmm0, xmm0, xmm0, 3);\n+    \/\/     a = DP_FNMA(b, q, a);\n+    __ vfnmadd213sd(xmm0, xmm3, xmm4);\n+    __ align32();\n+    \/\/     while (b <= a)\n+    __ bind(L_1090);\n+    __ ucomisd(xmm0, xmm3);\n+    __ jcc(Assembler::below, L_11b9);\n+    \/\/     {\n+    \/\/       q = DP_DIV(a, b);\n+    __ vdivsd(xmm4, xmm0, xmm3);\n+    \/\/       corr = DP_SHR(DP_FNMA(b, q, a), 63);\n+    __ movapd(xmm5, xmm4);\n+    __ vfnmadd213sd(xmm5, xmm3, xmm0);\n+    __ movq(xmm5, xmm5);\n+    __ vpcmpgtq(xmm5, xmm1, xmm5, Assembler::AVX_128bit);\n+    \/\/       q = DP_PSUBQ(q, corr);\n+    __ vpaddq(xmm4, xmm5, xmm4, Assembler::AVX_128bit);\n+    \/\/       q = DP_TRUNC(q);\n+    __ vroundsd(xmm4, xmm4, xmm4, 3);\n+    \/\/       a = DP_FNMA(b, q, a);\n+    __ vfnmadd231sd(xmm0, xmm3, xmm4);\n+    __ jmp(L_1090);\n+    \/\/     }\n+    \/\/     return DP_XOR(a, sgn_a);\n+    \/\/   }\n+\n+    \/\/   __asm { ldmxcsr DWORD PTR [mxcsr_rz] }\n+    __ bind(L_10c1);\n+    __ ldmxcsr(Address(rsp, 0x04));\n+\n+    \/\/   q = DP_DIV(a, b);\n+    __ vdivpd(xmm0, xmm4, xmm3, Assembler::AVX_128bit);\n+    \/\/   q = DP_TRUNC(q);\n+    __ vroundsd(xmm0, xmm0, xmm0, 3);\n+\n+    \/\/   eq = TRANSFER_HIGH_INT32(q);\n+    __ extractps(rax, xmm0, 1);\n+\n+    \/\/   if (__builtin_expect((eq >= 0x7fefffffu), (0==1))) goto SPECIAL_FMOD;\n+    __ cmpl(rax, 0x7feffffe);\n+    __ jcc(Assembler::above, L_10e7);\n+\n+    \/\/   a = DP_FNMA(b, q, a);\n+    __ vfnmadd213sd(xmm0, xmm3, xmm4);\n+    __ jmp(L_11af);\n+\n+    \/\/ SPECIAL_FMOD:\n+\n+    \/\/   \/\/ y==0 or x==Inf?\n+    \/\/   if ((b == 0.0) || (!(a <= DP_CONST(7fefffffffffffff))))\n+    __ bind(L_10e7);\n+    __ vpxor(xmm5, xmm5, xmm5, Assembler::AVX_128bit);\n+    __ ucomisd(xmm3, xmm5);\n+    __ jcc(Assembler::notEqual, L_10f3);\n+    __ jcc(Assembler::noParity, L_111c);\n+\n+    __ bind(L_10f3);\n+    __ movsd(xmm5, ExternalAddress((address)CONST_MAX), rax);\n+    __ ucomisd(xmm5, xmm4);\n+    __ jcc(Assembler::below, L_111c);\n+    \/\/     return res;\n+    \/\/   }\n+    \/\/   \/\/ y is NaN?\n+    \/\/   if (!(b <= DP_CONST(7ff0000000000000))) {\n+    __ movsd(xmm0, ExternalAddress((address)CONST_INF), rax);\n+    __ ucomisd(xmm0, xmm3);\n+    __ jcc(Assembler::aboveEqual, L_112a);\n+    \/\/     res = y + y;\n+    __ vaddsd(xmm0, xmm1, xmm1);\n+    \/\/     __asm { ldmxcsr DWORD PTR[mxcsr] }\n+    __ ldmxcsr(Address(rsp, 0));\n+    __ jmp(L_11bd);\n+    \/\/   {\n+    \/\/     res = DP_FNMA(b, q, a);    \/\/ NaN\n+    __ bind(L_111c);\n+    __ vfnmadd213sd(xmm0, xmm3, xmm4);\n+    \/\/     __asm { ldmxcsr DWORD PTR[mxcsr] }\n+    __ ldmxcsr(Address(rsp, 0));\n+    __ jmp(L_11bd);\n+    \/\/     return res;\n+    \/\/   }\n+\n+    \/\/   \/\/ b* 2*1023\n+    \/\/   bs = b * DP_CONST(7fe0000000000000);\n+    __ bind(L_112a);\n+    __ vmulsd(xmm1, xmm3, ExternalAddress((address)CONST_e307), rax);\n+\n+    \/\/   q = DP_DIV(a, bs);\n+    __ vdivsd(xmm0, xmm4, xmm1);\n+    \/\/   q = DP_TRUNC(q);\n+    __ vroundsd(xmm0, xmm0, xmm0, 3);\n+\n+    \/\/   eq = TRANSFER_HIGH_INT32(q);\n+    __ extractps(rax, xmm0, 1);\n+\n+    \/\/   if (eq >= 0x7fefffffu)\n+    __ cmpl(rax, 0x7fefffff);\n+    __ jcc(Assembler::below, L_116e);\n+    \/\/   {\n+    \/\/     \/\/ b* 2*1023 * 2^1023\n+    \/\/     bs2 = bs * DP_CONST(7fe0000000000000);\n+    __ vmulsd(xmm0, xmm1, ExternalAddress((address)CONST_e307), rax);\n+    \/\/     while (bs2 <= a)\n+    __ ucomisd(xmm4, xmm0);\n+    __ jcc(Assembler::below, L_1173);\n+    \/\/     {\n+    \/\/       q = DP_DIV(a, bs2);\n+    __ bind(L_1157);\n+    __ vdivsd(xmm5, xmm4, xmm0);\n+    \/\/       q = DP_TRUNC(q);\n+    __ vroundsd(xmm5, xmm5, xmm5, 3);\n+    \/\/       a = DP_FNMA(bs2, q, a);\n+    __ vfnmadd231sd(xmm4, xmm0, xmm5);\n+    \/\/     while (bs2 <= a)\n+    __ ucomisd(xmm4, xmm0);\n+    __ jcc(Assembler::aboveEqual, L_1157);\n+    __ jmp(L_1173);\n+    \/\/     }\n+    \/\/   }\n+    \/\/   else\n+    \/\/   a = DP_FNMA(bs, q, a);\n+    __ bind(L_116e);\n+    __ vfnmadd231sd(xmm4, xmm1, xmm0);\n+\n+    \/\/   while (bs <= a)\n+    __ bind(L_1173);\n+    __ ucomisd(xmm4, xmm1);\n+    __ jcc(Assembler::aboveEqual, L_117f);\n+    __ movapd(xmm0, xmm4);\n+    __ jmp(L_11af);\n+    \/\/   {\n+    \/\/     q = DP_DIV(a, bs);\n+    __ bind(L_117f);\n+    __ vdivsd(xmm0, xmm4, xmm1);\n+    \/\/     q = DP_TRUNC(q);\n+    __ vroundsd(xmm0, xmm0, xmm0, 3);\n+    \/\/     a = DP_FNMA(bs, q, a);\n+    __ vfnmadd213sd(xmm0, xmm1, xmm4);\n+\n+    \/\/   while (bs <= a)\n+    __ ucomisd(xmm0, xmm1);\n+    __ movapd(xmm4, xmm0);\n+    __ jcc(Assembler::aboveEqual, L_117f);\n+    __ jmp(L_11af);\n+    __ align32();\n+    \/\/   {\n+    \/\/     q = DP_DIV(a, b);\n+    __ bind(L_11a0);\n+    __ vdivsd(xmm1, xmm0, xmm3);\n+    \/\/     q = DP_TRUNC(q);\n+    __ vroundsd(xmm1, xmm1, xmm1, 3);\n+    \/\/     a = DP_FNMA(b, q, a);\n+    __ vfnmadd231sd(xmm0, xmm3, xmm1);\n+\n+    \/\/ FMOD_CONT:\n+    \/\/   while (b <= a)\n+    __ bind(L_11af);\n+    __ ucomisd(xmm0, xmm3);\n+    __ jcc(Assembler::aboveEqual, L_11a0);\n+    \/\/   }\n+\n+    \/\/   __asm { ldmxcsr DWORD PTR[mxcsr] }\n+    __ ldmxcsr(Address(rsp, 0));\n+    __ bind(L_11b9);\n+    __ vpxor(xmm0, xmm2, xmm0, Assembler::AVX_128bit);\n+    \/\/   }\n+\n+    \/\/   goto FMOD_CONT;\n+\n+    \/\/ }\n+    __ bind(L_11bd);\n+    __ pop(rax);\n+\n+  } else {                                       \/\/ SSE version\n+    assert(false, \"SSE not implemented\");\n+  }\n+\n+  __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n+  __ ret(0);\n+\n+  return start;\n+}\n+\n+#undef __\n","filename":"src\/hotspot\/cpu\/x86\/stubGenerator_x86_64_fmod.cpp","additions":524,"deletions":0,"binary":false,"changes":524,"status":"added"},{"patch":"@@ -165,0 +165,1 @@\n+address StubRoutines::_fmod = nullptr;\n","filename":"src\/hotspot\/share\/runtime\/stubRoutines.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -252,0 +252,1 @@\n+  static address _fmod;\n@@ -433,0 +434,1 @@\n+  static address fmod()                { return _fmod; }\n","filename":"src\/hotspot\/share\/runtime\/stubRoutines.hpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -560,0 +560,1 @@\n+     static_field(StubRoutines,                _fmod,                                         address)                               \\\n","filename":"src\/hotspot\/share\/runtime\/vmStructs.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -0,0 +1,131 @@\n+\/*\n+ * Copyright (c) 2023, Intel Corporation. All rights reserved.\n+ * Intel Math Library (LIBM) Source Code\n+ *\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+\/**\n+ * @test\n+ * @bug 8308966\n+ * @summary Add intrinsic for float\/double modulo for x86 AVX2 and AVX512\n+ * @run main compiler.floatingpoint.DmodTest\n+ *\/\n+\n+ package compiler.floatingpoint;\n+\n+ import java.lang.Double;\n+\n+ public class DmodTest {\n+   static double [] op1 = { 1.2345d, 0.0d, -0.0d, 1.0d\/0.0d, -1.0d\/0.0d, 0.0d\/0.0d };\n+   static double [] op2 = { 1.2345d, 0.0d, -0.0d, 1.0d\/0.0d, -1.0d\/0.0d, 0.0d\/0.0d };\n+   static double [][] res = {\n+      {\n+        0.0d,\n+        Double.NaN,\n+        Double.NaN,\n+        1.2345d,\n+        1.2345d,\n+        Double.NaN,\n+      },\n+      {\n+        0.0d,\n+        Double.NaN,\n+        Double.NaN,\n+        0.0d,\n+        0.0d,\n+        Double.NaN,\n+      },\n+      {\n+        -0.0d,\n+        Double.NaN,\n+        Double.NaN,\n+        -0.0d,\n+        -0.0d,\n+        Double.NaN,\n+      },\n+      {\n+        Double.NaN,\n+        Double.NaN,\n+        Double.NaN,\n+        Double.NaN,\n+        Double.NaN,\n+        Double.NaN,\n+      },\n+      {\n+        Double.NaN,\n+        Double.NaN,\n+        Double.NaN,\n+        Double.NaN,\n+        Double.NaN,\n+        Double.NaN,\n+      },\n+      {\n+        Double.NaN,\n+        Double.NaN,\n+        Double.NaN,\n+        Double.NaN,\n+        Double.NaN,\n+        Double.NaN,\n+      },\n+   };\n+   public static void main(String[] args) throws Exception {\n+     double f1, f2, f3;\n+     boolean failure = false;\n+     boolean print_failure = false;\n+     for (int i = 0; i < 100_000; i++) {\n+       for (int j = 0; j < op1.length; j++) {\n+         for (int k = 0; k < op2.length; k++) {\n+           f1 = op1[j];\n+           f2 = op2[k];\n+           f3 = f1 % f2;\n+\n+           if (Double.isNaN(res[j][k])) {\n+             if (!Double.isNaN(f3)) {\n+               failure = true;\n+               print_failure = true;\n+             }\n+           } else if (Double.isNaN(f3)) {\n+             failure = true;\n+             print_failure = true;\n+           } else if (f3 != res[j][k]) {\n+             failure = true;\n+             print_failure = true;\n+           }\n+\n+           if (print_failure) {\n+             System.out.println( \"Actual   \" + f1 + \" % \" + f2 + \" = \" + f3);\n+             System.out.println( \"Expected \" + f1 + \" % \" + f2 + \" = \" + res[j][k]);\n+             print_failure = false;\n+           }\n+         }\n+       }\n+     }\n+\n+    if (failure) {\n+      throw new RuntimeException(\"Test Failed\");\n+    } else {\n+      System.out.println(\"Test passed.\");\n+    }\n+ }\n+}\n+\n","filename":"test\/hotspot\/jtreg\/compiler\/floatingpoint\/DmodTest.java","additions":131,"deletions":0,"binary":false,"changes":131,"status":"added"},{"patch":"@@ -0,0 +1,130 @@\n+\/*\n+ * Copyright (c) 2023, Intel Corporation. All rights reserved.\n+ * Intel Math Library (LIBM) Source Code\n+ *\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+\/**\n+ * @test\n+ * @bug 8308966\n+ * @summary Add intrinsic for float\/double modulo for x86 AVX2 and AVX512\n+ * @run main compiler.floatingpoint.FmodTest\n+ *\/\n+\n+ package compiler.floatingpoint;\n+\n+ import java.lang.Float;\n+\n+ public class FmodTest {\n+   static float [] op1 = { 1.2345f, 0.0f, -0.0f, 1.0f\/0.0f, -1.0f\/0.0f, 0.0f\/0.0f };\n+   static float [] op2 = { 1.2345f, 0.0f, -0.0f, 1.0f\/0.0f, -1.0f\/0.0f, 0.0f\/0.0f };\n+   static float [][] res = {\n+      {\n+        0.0f,\n+        Float.NaN,\n+        Float.NaN,\n+        1.2345f,\n+        1.2345f,\n+        Float.NaN,\n+      },\n+      {\n+        0.0f,\n+        Float.NaN,\n+        Float.NaN,\n+        0.0f,\n+        0.0f,\n+        Float.NaN,\n+      },\n+      {\n+        -0.0f,\n+        Float.NaN,\n+        Float.NaN,\n+        -0.0f,\n+        -0.0f,\n+        Float.NaN,\n+      },\n+      {\n+        Float.NaN,\n+        Float.NaN,\n+        Float.NaN,\n+        Float.NaN,\n+        Float.NaN,\n+        Float.NaN,\n+      },\n+      {\n+        Float.NaN,\n+        Float.NaN,\n+        Float.NaN,\n+        Float.NaN,\n+        Float.NaN,\n+        Float.NaN,\n+      },\n+      {\n+        Float.NaN,\n+        Float.NaN,\n+        Float.NaN,\n+        Float.NaN,\n+        Float.NaN,\n+        Float.NaN,\n+      },\n+   };\n+   public static void main(String[] args) throws Exception {\n+     float f1, f2, f3;\n+     boolean failure = false;\n+     boolean print_failure = false;\n+     for (int i = 0; i < 100_000; i++) {\n+       for (int j = 0; j < op1.length; j++) {\n+         for (int k = 0; k < op2.length; k++) {\n+           f1 = op1[j];\n+           f2 = op2[k];\n+           f3 = f1 % f2;\n+\n+           if (Float.isNaN(res[j][k])) {\n+             if (!Float.isNaN(f3)) {\n+               failure = true;\n+               print_failure = true;\n+             }\n+           } else if (Float.isNaN(f3)) {\n+             failure = true;\n+             print_failure = true;\n+           } else if (f3 != res[j][k]) {\n+             failure = true;\n+             print_failure = true;\n+           }\n+\n+           if (print_failure) {\n+             System.out.println( \"Actual   \" + f1 + \" % \" + f2 + \" = \" + f3);\n+             System.out.println( \"Expected \" + f1 + \" % \" + f2 + \" = \" + res[j][k]);\n+             print_failure = false;\n+           }\n+         }\n+       }\n+     }\n+\n+     if (failure) {\n+       throw new RuntimeException(\"Test Failed\");\n+     } else {\n+       System.out.println(\"Test passed.\");\n+     }\n+   }\n+ }\n","filename":"test\/hotspot\/jtreg\/compiler\/floatingpoint\/FmodTest.java","additions":130,"deletions":0,"binary":false,"changes":130,"status":"added"}]}