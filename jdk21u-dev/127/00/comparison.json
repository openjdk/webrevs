{"files":[{"patch":"@@ -289,16 +289,4 @@\n-static void atomic_copy64(const volatile void *src, volatile void *dst) {\n-#if defined(PPC32)\n-  double tmp;\n-  asm volatile (\"lfd  %0, 0(%1)\\n\"\n-                \"stfd %0, 0(%2)\\n\"\n-                : \"=f\"(tmp)\n-                : \"b\"(src), \"b\"(dst));\n-#elif defined(S390) && !defined(_LP64)\n-  double tmp;\n-  asm volatile (\"ld  %0, 0(%1)\\n\"\n-                \"std %0, 0(%2)\\n\"\n-                : \"=r\"(tmp)\n-                : \"a\"(src), \"a\"(dst));\n-#else\n-  *(jlong *) dst = *(const jlong *) src;\n-#endif\n+inline void atomic_copy64(const volatile void *src, volatile void *dst) {\n+  int64_t tmp;\n+  __atomic_load(reinterpret_cast<const volatile int64_t*>(src), &tmp, __ATOMIC_RELAXED);\n+  __atomic_store(reinterpret_cast<volatile int64_t*>(dst), &tmp, __ATOMIC_RELAXED);\n@@ -311,3 +299,3 @@\n-  volatile int64_t dest;\n-  atomic_copy64(reinterpret_cast<const volatile int64_t*>(src), reinterpret_cast<volatile int64_t*>(&dest));\n-  return PrimitiveConversions::cast<T>(dest);\n+  T dest;\n+  __atomic_load(const_cast<T*>(src), &dest, __ATOMIC_RELAXED);\n+  return dest;\n@@ -321,1 +309,1 @@\n-  atomic_copy64(reinterpret_cast<const volatile int64_t*>(&store_value), reinterpret_cast<volatile int64_t*>(dest));\n+  __atomic_store(dest, &store_value, __ATOMIC_RELAXED);\n","filename":"src\/hotspot\/os_cpu\/bsd_zero\/atomic_bsd_zero.hpp","additions":8,"deletions":20,"binary":false,"changes":28,"status":"modified"},{"patch":"@@ -137,39 +137,3 @@\n-#if defined(PPC32) && !defined(__SPE__)\n-  double tmp;\n-  asm volatile (\"lfd  %0, %2\\n\"\n-                \"stfd %0, %1\\n\"\n-                : \"=&f\"(tmp), \"=Q\"(*(volatile double*)dst)\n-                : \"Q\"(*(volatile double*)src));\n-#elif defined(PPC32) && defined(__SPE__)\n-  long tmp;\n-  asm volatile (\"evldd  %0, %2\\n\"\n-                \"evstdd %0, %1\\n\"\n-                : \"=&r\"(tmp), \"=Q\"(*(volatile long*)dst)\n-                : \"Q\"(*(volatile long*)src));\n-#elif defined(S390) && !defined(_LP64)\n-  double tmp;\n-  asm volatile (\"ld  %0, %2\\n\"\n-                \"std %0, %1\\n\"\n-                : \"=&f\"(tmp), \"=Q\"(*(volatile double*)dst)\n-                : \"Q\"(*(volatile double*)src));\n-#elif defined(__ARM_ARCH_7A__)\n-  \/\/ The only way to perform the atomic 64-bit load\/store\n-  \/\/ is to use ldrexd\/strexd for both reads and writes.\n-  \/\/ For store, we need to have the matching (fake) load first.\n-  \/\/ Put clrex between exclusive ops on src and dst for clarity.\n-  uint64_t tmp_r, tmp_w;\n-  uint32_t flag_w;\n-  asm volatile (\"ldrexd %[tmp_r], [%[src]]\\n\"\n-                \"clrex\\n\"\n-                \"1:\\n\"\n-                \"ldrexd %[tmp_w], [%[dst]]\\n\"\n-                \"strexd %[flag_w], %[tmp_r], [%[dst]]\\n\"\n-                \"cmp    %[flag_w], 0\\n\"\n-                \"bne    1b\\n\"\n-                : [tmp_r] \"=&r\" (tmp_r), [tmp_w] \"=&r\" (tmp_w),\n-                  [flag_w] \"=&r\" (flag_w)\n-                : [src] \"r\" (src), [dst] \"r\" (dst)\n-                : \"cc\", \"memory\");\n-#else\n-  *(jlong *) dst = *(const jlong *) src;\n-#endif\n+  int64_t tmp;\n+  __atomic_load(reinterpret_cast<const volatile int64_t*>(src), &tmp, __ATOMIC_RELAXED);\n+  __atomic_store(reinterpret_cast<volatile int64_t*>(dst), &tmp, __ATOMIC_RELAXED);\n@@ -182,3 +146,3 @@\n-  volatile int64_t dest;\n-  atomic_copy64(reinterpret_cast<const volatile int64_t*>(src), reinterpret_cast<volatile int64_t*>(&dest));\n-  return PrimitiveConversions::cast<T>(dest);\n+  T dest;\n+  __atomic_load(const_cast<T*>(src), &dest, __ATOMIC_RELAXED);\n+  return dest;\n@@ -192,1 +156,1 @@\n-  atomic_copy64(reinterpret_cast<const volatile int64_t*>(&store_value), reinterpret_cast<volatile int64_t*>(dest));\n+  __atomic_store(dest, &store_value, __ATOMIC_RELAXED);\n","filename":"src\/hotspot\/os_cpu\/linux_zero\/atomic_linux_zero.hpp","additions":7,"deletions":43,"binary":false,"changes":50,"status":"modified"}]}